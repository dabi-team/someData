Optimal Counterfactual Explanations in Tree Ensembles

Axel Parmentier 1 Thibaut Vidal 2 3

1
2
0
2

n
u
J

5
2

]

G
L
.
s
c
[

2
v
1
3
6
6
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Counterfactual explanations are usually generated
through heuristics that are sensitive to the search’s
initial conditions. The absence of guarantees of
performance and robustness hinders trustworthi-
ness. In this paper, we take a disciplined approach
towards counterfactual explanations for tree en-
sembles. We advocate for a model-based search
aiming at “optimal” explanations and propose ef-
ﬁcient mixed-integer programming approaches.
We show that isolation forests can be modeled
within our framework to focus the search on plau-
sible explanations with a low outlier score. We
provide comprehensive coverage of additional
constraints that model important objectives, het-
erogeneous data types, structural constraints on
the feature space, along with resource and action-
ability restrictions. Our experimental analyses
demonstrate that the proposed search approach
requires a computational effort that is orders of
magnitude smaller than previous mathematical
programming algorithms. It scales up to large
data sets and tree ensembles, where it provides,
within seconds, systematic explanations grounded
on well-deﬁned models solved to optimality.

1. Introduction

Accountability in machine learning is quickly rising as a ma-
jor concern as learning algorithms take over tasks that have
a major impact on human lives. With the increasing use of
proﬁling and automated decision-making systems, new legal
provisions are being set up to protect rights to transparency.
The recent interpretation of the EU General Data Protection
Regulation (GDPR) by Article 29 Data Protection Work-
ing Party (2017) refers to a “right to explanations” and has

1CERMICS, ´Ecole des Ponts Paristech; 2CIRRELT & SCALE-
AI Chair in Data-Driven Supply Chains, Department of Mathemat-
ics and Industrial Engineering, Polytechnique Montreal, Canada;
3Department of Computer Science, Pontiﬁcal Catholic University
of Rio de Janeiro (PUC-Rio), Brazil. Correspondence to: Thibaut
Vidal <thibaut.vidal@cirrelt.ca>.

Accepted for publication in the Proceedings of the 38 th Interna-
tional Conference on Machine Learning, PMLR 139, 2021.

triggered extensive research on algorithmic recourse and
counterfactual explanations (see, e.g., Wachter et al., 2018;
Karimi et al., 2020b; Verma et al., 2020).

Counterfactual explanations are contrastive arguments of
the type: “To obtain this loan, you need $40,000 of annual
revenue instead of the current $30,000”. They correspond to
small perturbations of an example that permit to modify the
classiﬁcation outcome, in a similar fashion as adversarial ex-
amples, though typically restricted by additional constraints
ensuring actionability and plausibility (Barocas et al., 2020;
Venkatasubramanian & Alfano, 2020).

Despite their conceptual simplicity, counterfactual explana-
tions pose signiﬁcant challenges related to data protection,
intellectual property, ethics, along with fundamental com-
putational tractability issues. Indeed, the scale of machine
learning models has tremendously increased over a few
decades. Even when restricted to the vicinity of an exam-
ple, a systematic inspection of all possible explanations is
intractable, and therefore most studies on counterfactual
explanations rely on ad-hoc algorithms or heuristics (e.g.,
gradient descent in a non-convex space in Wachter et al.
2018). This poses at least three main issues:

(a) Heuristics can fail to identify the most natural and
insightful explanation, and therefore do not necessarily
give a trustworthy cause (Karimi et al., 2020a). This is
especially true when the search involves combinatorial
spaces (e.g., tree ensembles) with binary or integer
features bound together by plausibility constraints.

(b) They can be sensitive to the initial conditions of the
search, leading to unstable results —even for the same
subject.

(c) Finally, these methods are not readily extensible to in-
clude additional constraints and domain knowledge re-
garding actionability and plausibility. A small change
of problem formulation due to a speciﬁc application do-
main can very well require signiﬁcant methodological
adaptations.

To circumvent these issues, we advocate for a disciplined
analysis of counterfactual explanations through mathemat-
ical programming lenses. We focus on tree ensembles (in-
cluding random forests and gradient boosting), a popular

 
 
 
 
 
 
Optimal Counterfactual Explanations in Tree Ensembles

family of models with good empirical performance which
is often sought as a more transparent replacement to neural
networks (Rudin, 2019). We opt for a solution approach
grounded on mixed integer linear and quadratic program-
ming (MILP and MIQP) since we believe that it solves the
three aforementioned issues. Firstly, the search for an op-
timal solution of a well-deﬁned model permits to control
the quality of the solution and ensures the stability of the
results, solving key issues (a) and (b). Moreover, as seen
later in this paper, the modeling capacities of MILP permit
to seamlessly integrate domain information as well as many
forms of plausibility and actionability constraints, there-
fore making meaningful progress on issue (c). Finally, the
tremendous progress of MILP solution approaches over the
years (estimated to a 1011 reduction of computational effort
on the same problems – Bixby 2012) now permits to use
such a modeling and search approach to its full potential.

We make the following contributions:

1. We propose the ﬁrst efﬁcient mathematical models to
search for counterfactual explanations in tree ensem-
bles with a number of binary variables that is loga-
rithmic in the number of vertices, and therefore scales
to large trees while retaining its ability to ﬁnd an op-
timal solution. This is exponentially fewer variables
than the previous models used in Cui et al. (2015) and
Kanamori et al. (2020). Our approach is applicable to
heterogeneous datasets with numerical, ordinal, binary,
and categorical features, with possible oblique splits on
numerical features, and considering single or multiple
classes. In contrast with previous works, it does not
require binary variables to model the numerical feature
levels and has a sparse constraint matrix. Consequently,
solution performance remains stable as the number of
features increases.

2. We demonstrate how to integrate plausibility in our
mathematical framework from an isolation forest view-
point. Isolation forests are an effective and distribution-
agnostic way to associate a plausibility score for the
different regions of the feature space. This is, to our
knowledge, the ﬁrst time that this framework is used
for counterfactual explanations, through an integration
which is only possible due to our signiﬁcant model-
tractability improvements.

3. We discuss extensions of the model capturing impor-
tant constraints regarding plausibility and actionability.
We therefore provide a ﬂexible and modular toolset
that can be adapted to each speciﬁc situation.

4. Finally, we conduct an extensive and reproducible ex-
perimental campaign, which can be executed from a
single self-contained Python script. Our source code
is openly accessible at https://github.com/

vidalt/OCEAN under a MIT license. We demon-
strate that the approaches proposed in this work are
efﬁcient and scalable, producing optimal counterfac-
tual explanations in a matter of a few seconds on data
sets with over ﬁfty features, using tree ensembles with
hundreds of trees. We evaluate the impact of our plau-
sibility constraints via isolation forests, demonstrating
the ﬂexibility of the approach and showing that these
extra constraints do not signiﬁcantly impact the per-
formance of the solution process while signiﬁcantly
boosting the usefulness of the explanations.

2. Background

2.1. Mixed Integer Programming

MIQPs can be cast into the following standard form:

min f (x)

s.t. Ax ≤ b

x(cid:124)Qix + cix ≤ bi
x ∈ Za × Rb,

i ∈ {1, . . . , m}

(1)

(2)
(3)

(4)

where a represents the number of variables taking integer
values and b is the number of continuous variables. The fea-
sibility region of the problem is deﬁned as the intersection
of a polytope (Constraint 2) along with a set of quadratic
restrictions (Constraint 3). State-of-the-art solvers (e.g.,
CPLEX and Gurobi) can handle separable quadratic objec-
tives of the form f (x) = x(cid:124)Qx + cx and therefore model a
wide range of objectives (regularization terms through l0, l1
and squared l2 norms, squared Euclidean and Mahalanobis
distances, and variations thereof). MILP and MIQP are
NP-hard in general, though astonishing progress in solution
methods has permitted to handle increasingly large prob-
lems. Solver performance is, however, dependent on the
quality of the problem formulation (i.e., the model). Ideally,
a good model should have few binary variables, limited sym-
metry, and a strong continuous relaxation, i.e., a small gap
between its optimal solution value and that of the same prob-
lem in which variables x are relaxed to the domain Ra+b,
as this permits to quickly prune regions of the search space
during the branch-and-bound process (Wolsey, 2020).

2.2. Counterfactual Explanations in Tree Ensembles

k=1 be a training set in which each xk ∈ Rp
Let {xk, ck}n
corresponds to a sample characterized by a p-dimensional
feature vector and a class ck ∈ C. In the most general form,
a tree ensemble T learns a set of trees t ∈ T returning class
probabilities Ftc : X → [0, 1]. For any sample x, the tree
ensemble returns the class c that maximizes the weighted
(cid:80) wtFtc(x).
sum of the probabilities: FT (x) = arg maxc
Given an origin point ˆx and a desired prediction class c∗,
searching for a plausible and actionable counterfactual ex-

Optimal Counterfactual Explanations in Tree Ensembles

planation consists in locating a new data point x ∈ X that
solves the following problem:

min fˆx(x)
s.t. FT (x) = c∗

x ∈ X P ∩ X A.

(5)

(6)

(7)

In this problem, fˆx is a separable convex cost that represents
how difﬁcult it is to move from ˆx to x. This generic cost
function includes distance metrics (e.g., squared Euclidean
or Mahalanobis distance) as a special case. Moreover, it
allows possible cost asymmetry (Ustun et al., 2019; Karimi
et al., 2020b) and can include additional penalization terms
if needed. Polytopes X P and X A represent the space of
plausible and actionable counterfactual explanations, and
will be discussed in the next paragraphs in connection with
recent works.

Related studies As reviewed in Guidotti et al. (2018),
Karimi et al. (2020b) and Verma et al. (2020), early studies
on counterfactual explanations were conducted in majority
in a model-agnostic context through enumeration and heuris-
tic search approaches (Wachter et al., 2018). Dedicated work
has been later conducted on speciﬁc models, such as tree
ensembles, which presented additional challenges due to
their combinatorial and non-differentiable nature. To handle
this case, Lucic et al. (2019) proposed a gradient algorithm
that approximates the splits of the decision trees through
sigmoid functions. Tolomei et al. (2017) designed a feature
tweaking algorithm that enumerates alternative paths in each
tree to change the decision of the ensemble. The method
is shown to provide useful counterfactual explanations on
several application cases, though it does not always deliver
an optimal (or even a feasible) counterfactual explanation
for tree ensembles. To circumvent these issues, Karimi et al.
(2020a) reformulated the search for counterfactual expla-
nations in heterogeneous domains (with binary, numerical
and categorical features) as a satisﬁability problem and em-
ployed specialized solvers. The authors reported promising
results on three data sets involving up to 14 features.

Cui et al. (2015) and Kanamori et al. (2020) proposed mixed-
integer linear programming approaches for optimal explana-
tions in tree ensembles. The former work considers Maha-
lanobis distance and introduced decision logic constraints
to ensure the consistency of the split decisions through the
forest (ensuring that there is exists a counterfactual example
satisfying them). The later work demonstrated how to ex-
pand the formulation to consider an l1-norm distance and
plausibility constraints grounded on the Local Outlier Factor
(LOF) score. Both models use a discretization of the feature
space, and therefore need a large number of binary vari-
ables to express continuous features (one for each possible
feature level, and one for each leaf of each decision tree).
Good results were still achieved on a variety of data sets.

As demonstrated in our work, better formulations relying on
exponentially fewer variables can be designed, leading to re-
ductions of CPU time by some orders of magnitude. Finally,
a few other works related to mathematical programming
do not necessarily consider tree ensembles explanations but
provide useful additional modeling strategies. In particular,
Russell (2019) modeled the choice of a feature level among
multiple intervals (a special case of disjunctive constraint
discussed in Jeroslow & Lowe 1984) and suggest strategies
to ﬁnd multiple explanations. Moreover, Ustun et al. (2019)
propose to improve actionability through the deﬁnition of
extra constraints representing a partial causal model.

Plausibility and actionability. Constraint (7) aims at en-
suring plausibility and actionability of the counterfactual
explanations. These important requirements constitute one
of the cruces of recent research on counterfactual explana-
tions (Barocas et al., 2020). Plausibility constraints (poly-
tope X P) should ensure that the explanation x respects the
structure of the data and that it is located in a region that has
a sufﬁciently large density of samples. To capture this no-
tion, we will rely on the information of isolation forests (Liu
et al., 2008) within our framework to restrict the search to
plausible regions. In contrast, actionability constraints (poly-
tope X A) concern the trajectory between ˆx and x. At the
very least, they ensure that immutable features remain ﬁxed
and that features that are bound to evolve unilaterally are
constrained to remain within a half-space. A ﬁner-grained
knowledge of correlations (or even a partial causal model as
discussed in Mahajan et al. 2019, Ustun et al. 2019, Mothilal
et al. 2020 and Karimi et al. 2020c) can also be integrated
into the formulation through additional linear and logical
constraints, as discussed in Section 3.4.

3. Methodology

Our mathematical model is presented in two main stages.
First, we describe the variables and constraints that char-
acterize the branches taken by the counterfactual example.
Next, we include additional variables and constraints model-
ing the counterfactual example’s feature values and ensuring
compatibility with all the branch choices.

Our formulation relies on two main pillars. First, it uses the
natural disjunctive structure of the trees to model branch
choices using exponentially fewer binary variables than pre-
vious models by Cui et al. (2015) and Kanamori et al. (2020).
Second, it includes continuous variables organized as order
simplices (Grotzinger & Witzgall, 1984) to represent the
values of numerical or ordinal features of the counterfac-
tual example and to connect them with the branch choices.
This effectively leads to a formulation requiring only O(Nv)
non-zero terms in the constraint matrix instead of O(N 2
v ),
where Nv stands for the overall number of internal nodes
in the tree ensemble. Notably, this formulation complexity

Optimal Counterfactual Explanations in Tree Ensembles

does not depend on the number of features of the data set.
Our model is especially suitable for large data sets with
many numerical features, and permits us to rely on isolation
forests to model plausibility without sacriﬁcing numerical
tractability.

3.1. Branch Choices

For each tree t ∈ T , let V I
t be the set of internal vertices
associated with the splits, and let V L
t be the set of terminal
vertices (leaves). Let Dt represent the possible depths values
and deﬁne V I
td as the set of internal nodes at depth d. Let l(v)
and r(v) be the left and right children of each internal vertex
v ∈ V I
t . Finally, let ptvc be the class probability of c ∈ C
in each leaf v ∈ V L
t . Class probabilities can be deﬁned
in {0, 1} in the hard voting model (as in the random forest
algorithm initially proposed by Breiman 2001), or in [0, 1]
in the general soft voting model (e.g., as in scikit-learn).

For each tree t and depth d in Dt, we use a binary decision
variable λtd which will take value 1 if the counterfactual
example descends towards the left branch, and 0 otherwise.
This value is free if the path does not attain this depth. We
also use continuous variables ytv ∈ [0, 1] for each v ∈ V I
t ∪
V L
t to represent the ﬂow of the counterfactual example in the
decision tree. These variables are not explicitly deﬁned as
binary but will effectively take value 1 if the counterfactual
example passes through vertex v, and 0 otherwise. This
behavior is ensured with the following set of constraints:

yt1 = 1
ytv = ytl(v) + ytr(v)
(cid:88)

ytl(v) ≤ λtd

t ∈ T
t ∈ T , v ∈ V I
t

(8)

(9)

t ∈ T , d ∈ Dt

(10a)

v∈V I
td
(cid:88)

v∈V I
td

ytr(v) ≤ 1 − λtd

t ∈ T , d ∈ Dt

(10b)

ytv ∈ [0, 1]
λtd ∈ {0, 1}

t ∈ T , v ∈ V I

t ∪ V I
t
t ∈ T , d ∈ Dt.

(11)

(12)

Theorem 1 Formulation (8–12) guarantees the integrality
of the y variables.

Proof of this theorem is provided in the supplementary mate-
rial. From these variables, ﬁnding the desired counterfactual
class through majority vote can be expressed as:

(cid:88)

(cid:88)

zc =

wtptvcytv

c ∈ C

(13)

t∈T

v∈V L
t

zc∗ > zc

c ∈ C, c (cid:54)= c∗.

(14)

3.2. Feature Consistency with the Splits

The previous variables and constraints deﬁne the counter-
factual example’s paths through each tree. Additional con-

straints are needed to ensure that there exist feature values
that are consistent with all these branching decisions. To
that extent, we propose efﬁcient formulations for each main
data type (numerical, binary, and categorical), which can be
combined in the case of heterogeneous data sets. We will
refer to IN, IB, and IC as the index sets of each feature type.

Numerical Features. We can assume, w.l.o.g., that contin-
uous features have been scaled into the interval [0, 1]. For
each numerical (continuous or discrete) feature i ∈ IN, let ki
be the overall number of distinct split levels in the forest.
Moreover, let xj
i be the jth split level for j ∈ {1, . . . , ki},
and deﬁne x0
= 1.

i = 0 as well as xki+1

i

For each tree t, let V I
tij be the set of internal nodes involving
a split on feature i with level xj
i , such that samples with
feature values xi ≤ xj
i descend to the left branch, whereas
others values satisfying xi > xj
i descend to the right branch.
With these deﬁnitions, the consistency of a feature i through
the forest can be modeled with the help of auxiliary contin-
uous variables µj
i for j ∈ {0, . . . , ki}, constrained in such
a way that µj
i = 1 implies
xi ∈ [xj
i + (cid:15), 1], where (cid:15) is a small constant. These condi-
tions are ensured through the following set of constraints:

i = 0 implies xi ∈ [0, xj

i ] and µj

i

i ≥ µj
µj−1
µj
i ≤ 1 − ytl(v)
µj−1
i ≥ ytr(v)
µj
i ≥ (cid:15)ytr(v)
µj
i ∈ [0, 1]

j ∈ {1, . . . , ki} (15)

j ∈ {1, . . . , ki}, t ∈ T , v ∈ V I
tij
j ∈ {1, . . . , ki}, t ∈ T , v ∈ V I
tij
j ∈ {1, . . . , ki}, t ∈ T , v ∈ V I
tij
j ∈ {0, . . . , ki},

(16)

(17)

(18)

(19)

and the feature level xi can be derived from these auxiliary
variables (if needed) as:

xi =

ki(cid:88)

j=0

(xj+1

i − xj

i )µj
i .

(20)

Binary Features. We assume, w.l.o.g., that all splits on
binary features send values 0 to the left branch and values 1
to the right branch. Let V I
ti be the set of all vertices splitting
on a binary feature i ∈ IB. The consistency of this feature
value can be ensured as follows:

xi ≤ 1 − ytl(v)
xi ≥ ytr(v)
xi ∈ {0, 1}.

t ∈ T , v ∈ V I
ti
t ∈ T , v ∈ V I
ti

(21)

(22)

(23)

Categorical Features. Let ki be the number of possible
categories for feature i ∈ IC. Let νj
i be a variable that
will take value 1 if xi belongs to category j ∈ {1, . . . , ki}
and 0 otherwise. Decision trees usually handle categorical
variables through one-vs-all splits, sending the samples of
a given category j to the right branch and the rest of the

Optimal Counterfactual Explanations in Tree Ensembles

samples to the left. Let v ∈ V I
tij be the set of vertices
splitting category j of feature i. The consistency of this
feature through the forest is modeled as:

νj
i ≤ 1 − ytl(v)
νj
i ≥ ytr(v)
νj
i ∈ {0, 1}
(cid:88)
νj
i = 1.

j∈Ci

j ∈ Ci, t ∈ T , v ∈ V I
tij
j ∈ Ci, t ∈ T , v ∈ V I
tij
j ∈ Ci

(24)

(25)

(26)

(27)

Numerical features can be directly accessed through the x
variables deﬁned in Equation (20), or indirectly through
the µ variables. Most classical objectives used in counter-
factual explanations can be directly expressed from x, but
modeling via µ can in some cases lead to a better linear re-
laxation (e.g., for l0). To that end, we add the origin level ˆxi
(with index denoted as ˆji) to the list of hyperplane levels
deﬁning the µi variables. The l0, l1 and l2 objectives with
asymmetric and feature-dependent weights {c−
i } can
then be expressed as:

i , c+

Theorem 2 Formulation (8–27)

(i) yields feature values that are consistent with all splits

in the forest;

(ii) involves only O(Nv) non-zero terms in the constraint

matrix overall;

(iii) achieves an equal or tighter linear relaxation than the
decision logic constraints used in Cui et al. (2015).

Using a reduced number of integer variables is usually ben-
eﬁcial for computational performance. As discussed in the
supplementary material, as a consequence of the integrality
of the λ and y variables, the domain of the x, ν variables in
Formulation (8–27) can be relaxed to the continuous inter-
val [0, 1] while retaining integrality of the linear-relaxation
solutions. We also show in the supplementary material how
to efﬁciently handle ordinal features (or, generally, any or-
dered feature in which the open intervals between successive
levels bear no meaning), multivariate splits on numerical
features (Brodley & Utgoff, 1995), and combinatorial splits
involving several categories for categorical features.

3.3. Objective Function

In a similar fashion as Gower’s distance (Gower, 1971), we
use a general objective f (x, µ, ν) = f N(µ)+f B(x)+f C(ν)
which contains different terms to model the objective con-
tributions of the numerical, binary, and categorical features.
Moreover, as actionability depends on each feature and di-
rection of action, we will use asymmetric extensions of
common distances metrics with feature-dependent weights.

The model presented in the previous sections gives a direct
access to the values of the binary and categorical features
through x and ν. We can therefore generally associate any
discrete cost value for each of these choices:

f B(x) =

f C(ν) =

(cid:88)

i∈IB
(cid:88)

(cTRUE
i

xi + cFALSE
i

(1 − xi))

(cid:88)

i νj
cj
i .

i∈IC

j∈Ci

(28)

(29)

The cost coefﬁcients corresponding to the origin state ˆx
typically have zero cost, though negative values could be
used if needed to model possible feature states that appear
more desirable.














l0 :

l1 :

l2 :

f N
0 (µ) =

(cid:88)

i∈IN

(c−

i z−

i + c+

i z+
i )

i ≥ 1 − µj−1
z−
i ∈ {0, 1}, z+
z−

i ≥ µj
i
i ∈ {0, 1}

, z+

i

(30)

i ∈ IN, j = ˆji
i ∈ IN

f N
1 (µ) =

ki(cid:88)

(φj+1

i − φj

i )µj

i

j=0

with parameter φj

i = c−
+ c+

i max(ˆxi − xj
i max(xj

i , 0)
i − ˆxi, 0)

(31)

f N
2 (x) =

(cid:88)

i∈IN

(cid:0)c−

i (z−

i )2 + c+

i (z+

i )2(cid:1)

i = xi − ˆxi

z+
i + z−
i ∈ {0, 1}, z−
z+

i ∈ [0, 1]

(32)

i ∈ IN
i ∈ IN

Finally, observe that Equation (31) can be extended to model
any objective expressed as a piecewise-linear convex func-
tion by deﬁning different φ parameters and introducing
extra µ variables for any additional breakpoint needed.

Overall, our model gives an extensible framework for ef-
ﬁciently modeling most existing data types, decision-tree
structures, and objectives. As it mathematically represents
the space of all feasible counterfactual explanations, solving
it to optimality using state-of-the-art MILP solvers for the
objective of choice permits to locate optimal explanations.

3.4. Domain Knowledge and Actionability

As seen in the previous sections, Formulation (8–27) ac-
counts for different data types (e.g., numerical, binary and
categorical) without transformation. For categorical vari-
ables in particular, this effectively ensures that counterfac-
tual explanations respect the structure of the data (select
exactly one category). Our model’s ﬂexibility also permits
us to integrate, as needed, additional domain knowledge
and actionability requirements through linear and logical
constraints. Table 1 summarizes several of these constraints
based on recent proposals from the literature. In the next sec-
tion, we will detail our proposal to exploit isolation forests

Optimal Counterfactual Explanations in Tree Ensembles

Table 1. Domain knowledge and actionability constraints

Domain Knowledge

Fixed features
Monotonic features

Known linear relations between features
(i.e., joint actionability – Venkatasubramanian & Alfano 2020)

Constraints

xi = ˆxi, µi = ˆµi, νi = ˆνi
xi ≥ ˆxi, µi ≥ ˆµi, νi ≥ ˆνi

A(xi − ˆxi) ≤ b

Known logical implications between features,
Example for binary features (x1 = TRUE) ⇒ (x2 = TRUE)
Example for categorical features x1 ∈ {CAT1, CAT2} ⇒ x2 ∈ {CAT3, CAT4}

Resource constraints (e.g., time) as modeled by additional functions gi(x, ν, µ)

2 + ν4
ν3

x2 ≥ x1
1 + ν2
2 ≥ ν1
1
gi(x, ν, µ) ≤ bi

to ensure counterfactual explanations plausibility within our
mathematical framework.

our counterfactual explanations relatively to previous
algorithms on a common objective (l1 distance).

3.5. Isolation Forests for Plausibility

We propose to rely on isolation forests (Liu et al., 2008)
for a ﬁne-grained representation of explanation plausibility.
Isolation forests are trained to return an outlier score for
any sample, inversely proportional to its average path depth
within a set of randomized trees grown to full extent on
random sample subsets. Therefore, constraining this aver-
age depth controls the outlier score (and consequently the
plausibility) of the counterfactual explanation.

To include this constraint, we train the isolation forest TI
on the training samples from the target class. Then, we
mathematically express it through Formulation (8–12) and
collect the µ levels from the union of the two forests in
Equations (15–19). Lastly, we constrain the average depth
in TI as follows:

(cid:88)

(cid:88)

t∈TI

v∈V L
t

dtvytv ≥ δ|TI|,

(33)

where dtv represents the depth of a vertex v in tree t, and δ
is a ﬁxed threshold deﬁning the average depth under which
samples are declared as outliers. In our experiments, we will
set this threshold to capture 10% of the training data as an
outlier, and therefore we seek a counterfactual explanation
typical of the 90% most common cases of the target class.

• Assessing the impact of the plausibility constraints
obtained from isolation forests on the tractability of the
model and the quality of the counterfactuals.

Our algorithm, referred to as OCEAN (Optimal Counter-
factual Explanations) in the remainder of this paper, has
been developed in Python 3.8 and can be readily executed
from a single script that builds the most suitable mathe-
matical model for the data set at hand. The complete data
and source code needed to reproduce our experiments is
provided at https://github.com/vidalt/OCEAN.
The supplementary material of this paper also includes ad-
ditional detailed results.

We use scikit-learn v0.23.0 for training random forests and
Gurobi 9.1 (via gurobipy) for solving the mathematical
models. All experiments have been run on four threads
of an Intel Core i9-9880H 2.30GHz CPU with 64GB of
available RAM, running Ubuntu 20.04.1 LTS.

We now discuss the preparation of the data and describe
each experiment. We limit the scope of our experiments to
the search of a single —optimal— explanation for each sub-
ject. If needed, diverse explanations could be generated by
iteratively applying our framework, collecting its solution,
and excluding it in subsequent iterations via an additional
linear constraint or penalty term.

4. Computational Experiments

4.1. Data Preparation

We conduct an extensive experimental campaign to fulﬁll
two main goals:

• Evaluating the performance of our approach in terms
of CPU time and solution quality. We measure the time
needed to ﬁnd optimal solutions on different data sets
and evaluate the impact of the number of trees in the en-
semble and their depth. We also compare the quality of

We conduct our experiments on eight data sets representa-
tive of diverse applications such as loan approval, socio-
economical studies, pretrial bail, news performance predic-
tion, and malware detection. Table 2 reports their number of
samples (n), number of features (total = p, numerical = pN,
binary = pB, and categorical = pC), and source of origin.

All these data sets include heterogeneous feature types. For
data sets AD, CC and CP, we used the same preprocessing

Optimal Counterfactual Explanations in Tree Ensembles

Data set

Table 2. Characteristics of the data sets
n p pN pB pC

Src.

AD: Adult
CC: Credit Card Default
CP: COMPAS
GC: German Credit
ON: Online News
PH: Data Phishing
SP: Spambase
ST: Students Performance

5
9

45222 11
5
29623 14 11
2
5278
1000
5
39644 47 43
11055 30
4601 57 57

2
3
3
1
2
8 22
0
395 30 13 13

UCI
4
0
UCI
0 ProPublica
UCI
3
UCI
2
UCI
0
UCI
0
UCI
4

as indicated in Karimi et al. (2020a). GC preprocessing was
done as suggested in German credit preprocessing (2017).
Finally, for ON we set “data channel” and “weekday” as
categorical. The three remaining data sets are used in their
original form. Each data set has been randomly split into
80% training and 20% test set.

To standardize the analyses between different data sets, we
opted to set actionability constraints on two columns wher-
ever applicable: “age” is constrained to be non-decreasing,
and “sex” always stays ﬁxed.

4.2. Performance and Scalability

i and c+

In a ﬁrst analysis, we evaluate the CPU time of OCEAN on
each data sets for the asymmetric and weighted l0, l1, and l2
objectives, without the plausibility restrictions. To simulate
differences of actionability among features, the marginal
weights c−
i of each feature in the objective have been
independently drawn in the uniform distribution U (0.5, 2).
For each data set, we generated a single random forest with
100 trees limited at depth 5. We selected 20 different nega-
tive samples from the test set to serve as origin points for the
counterfactual explanations. Figure 1 reports the CPU time
needed to ﬁnd an optimal counterfactual explanation in each
case. Each boxplot represents 20 CPU time measurements,
one for each counterfactual.

As visible in this experiment, OCEAN can locate optimal
counterfactual explanations in a matter of seconds, even for
data sets including over ﬁfty numerical features with a large
number of levels. Finding counterfactual explanations with
variants of the l1 or l2 norms also appears slightly faster
than with l0. For small data sets with a few dozen features,
CPU times of the order of a second are typically achieved,
making our framework applicable even in time-constrained
environments, e.g., for interactive tasks. Finally, as coun-
terfactual search has a decomposable geometrical structure,
CPU time could be even further reduced by additional par-
allel computing if the need arises.

Next, we compare the performance of OCEAN with pre-
vious approaches: the heuristic feature tweaking (FT) al-
gorithm of Tolomei et al. (2017), the exact model-agnostic

Figure 1. CPU time to ﬁnd an optimal counterfactual explanations,
considering different data sets and objectives

counterfactual explanations (MACE) algorithm from Karimi
et al. (2020a) and the optimal action extraction (OAE)
approach proposed in Cui et al. (2015) and extended in
Kanamori et al. (2020). We used the implementation of
FT and MACE (with precision 10−3) provided at https:
//github.com/amirhk/mace, and we provide a re-
implementation of OAE within the same code base as
OCEAN. For this comparative analysis, we use the l1 ob-
jective with homogeneous weights, as this is a common
objective handled by all the considered methods. For a fair
comparison, all the random forests and origin points for the
counterfactual explanations have been saved in a serialized
format, and identically loaded for each method.

We start from a baseline size of 100 trees with a maximum
depth of 5 for the random forest and then extend our analysis
to a varying number of trees in {10, 20, 50, 100, 200, 500}
and depth limit in {3, 4, 5, 6, 7, 8}. Figures 2 and 3 report,
for each method, the mean CPU time with its 95% conﬁ-
dence interval as a function of these parameters for data sets
AD and CC. The same ﬁgures are provided in the supple-
mentary material for the other data sets.

As seen in these experiments, OCEAN locates optimal coun-
terfactual explanations in a CPU time orders of magnitude
smaller than MACE and OEA. Even in the most complex
conﬁgurations (e.g., ST with 500 trees), OCEAN completed
the optimization within two minutes, whereas MACE and
OEA ran for more than ﬁve hours without terminating.

Next, we evaluate the quality of the counterfactual expla-
nations produced by these methods. We measure, for each
method and dataset, the average CPU time per explanation
and the ratio R = D/DOPT between the sum of the l1 dis-
tances of the 20 counterfactual explanations produced by
the method (D) and that of optimal explanations (DOPT).
Table 3 provides these values for the baseline setting (100
trees in the random forest limited at depth 5), and similar

llllllllllllllllldroplevels(interaction(Objective, Case))Time(s)ADCCCPGCONPHSPST0.20.512510L0L1L2CPU Time (s)Optimal Counterfactual Explanations in Tree Ensembles

guarantee optimality and effectively reached, on average,
distance values that are 1.4 to 31.7 times greater than the
optima. It also regularly failed to ﬁnd feasible counterfac-
tual explanations (i.e., attaining the desired class) for SP
and ST. These observations conﬁrm the fact that heuristic
techniques for counterfactual search can provide explana-
tions that are considerably more complex than needed, and
sensitive to the subject or subject group. In contrast, optimal
counterfactual explanations are deterministic and fully spec-
iﬁed through their mathematical deﬁnition —independently
of the search approach designed to ﬁnd them— providing
us greater control and accountability. Arguably, “optimal”
counterfactual explanations should be gradually established
as a requirement for transparency and trustworthiness.

Finally, the computational efﬁciency of OCEAN’s can be
explained, in part, by the sparsity of its mathematical model.
For data set AD with baseline parameters, OEA’s formu-
lation included 163,605 non-zero terms, whereas OCEAN
included only 18,330. This difference becomes even more
marked as the maximum depth increases. With a maxi-
mum tree depth of 8, the number of non-zero terms rises
to 3,223,586 for OEA compared to 127,755 for OCEAN.
This permits integrating additional linear constraints, logi-
cal constraints, and a wide range of actionability deﬁnitions
(Table 1). The next section will show how to proﬁt from
this performance gain to model ﬁne-grained plausibility
restrictions through isolation forests.

4.3. Isolation Forests for Plausibility

We ﬁnally evaluate how isolation forests can ensure better
plausibility in OCEAN’s counterfactual explanations. Fig-
ure 4 ﬁrst provides an illustrative example of our method on
a small case. As observed in this example, random forests
tend to make arbitrary class choices in low-density regions.
Without any plausibility constraint, the counterfactual al-
gorithm exploits the proximity to one such region to ﬁnd
a nearby counterfactual explanation. However, this expla-
nation is not plausible as it represents an outlier among the
samples of the desired class.

Introducing additional isolation forest constraints in our
mathematical model as suggested in Section 3.5 permits us
to circumvent this issue, as it successfully restricts the search
for counterfactual examples to the core of the distribution
of the desired class. Our restriction, therefore, builds on the
same logic as the convex density constraints based on Gaus-
sian mixtures proposed in (Artelt & Hammer, 2020), though
it has the general advantage to be distribution-agnostic and
applicable to most data types.

To evaluate the impact of these plausibility constraints in
our model, we conduct a ﬁnal experiment which consists of
measuring the prevalence of plausible explanations (P), the
cost of the explanations (C), and the computational effort (T)

Figure 2. Comparative analysis of CPU time as a function of the
maximum depth of the trees. Number of trees ﬁxed to 100.

Figure 3. Comparative analysis of CPU time as a function of the
number of trees in the ensemble. Maximum depth ﬁxed to 5.

Table 3. Time and solution quality comparison

Data

FT

T(s)

R

MACE
T(s) R

OAE
T(s) R

OCEAN
T(s) R

AD
3.03 15.9
CC
29.44 10.2
CP
22.68 4.5
GC 16.26 4.8
ON 10.05 31.7
PH 10.95 1.4
SP
NA —
ST
NA —

1.22 1.0
28.37 1.0
20.60 1.1
1.34 1.0
5.52 1.0
41.25 1.2
0.52 1.0
0.38 1.0
15.82 1.0
19.03 1.0
1.16 1.0
5.08 1.0
>900 — >900 — 2.97 1.0
>900 —
0.52 1.0
0.94 1.0
>900 — >900 — 2.73 1.0
>900 — 69.64 1.0
1.10 1.0

results are provided in the supplementary material for the
other settings. A value of “NA” means that no feasible
counterfactual explanation has been found, whereas “>900”
means that the CPU time limit of 900 seconds was exceeded.

As observed in this experiment, it is notable that the CPU
time of OCEAN is shorter or comparable to that of the
FT heuristic. Yet, despite its similar speed, FT does not

0.11.010100 3 4 5 6 7 8ADT(s)MACEOCEAN0.11.010100 3 4 5 6 7 8CCT(s)OAEFTMax DepthMax Depth0.11.010100102050100200500ADT(s)0.11.010100102050100200500CCT(s)Nb TreesNb TreesMACEOCEANOAEFTOptimal Counterfactual Explanations in Tree Ensembles

through careful integration of isolation forests and mathe-
matical programming. By doing so, we have circumvented
a major trustworthiness and accountability issue faced by
heuristic approaches, and provided mathematical guaran-
tees for tree ensembles interpretation. Our contribution also
includes a modeling toolkit that can be used as a building
block for a disciplined evaluation of counterfactual search
models, plausibility constraints, and actionability paradigms
in various applications.

The research perspectives are numerous. From a methodol-
ogy standpoint, one can always challenge tractability limits
and attempt to apply the OCEAN methodology to increas-
ingly larger data sets and tree ensembles. To that end, we
suggest to investigate new formulations and valid inequal-
ities, as well as model compression and geometrical de-
composition strategies (see, e.g., Vidal & Schiffer, 2020)
which have the potential to speed up the solution process.
We also recommend pursuing a disciplined evaluation of
compression and explanation of white-box models through
mathematical programming lenses, as performance guaran-
tees are critical for a fair access to algorithmic recourse.

Acknowledgements

This research has been partially supported by CAPES, CNPq
[grant number 308528/2018-2] and FAPERJ [grant number
E-26/202.790/2019] in Brazil.

References

Artelt, A. and Hammer, B. Convex density constraints
for computing plausible counterfactual explanations. In
International Conference on Artiﬁcial Neural Networks,
pp. 353–365. Springer Cham, 2020.

Article 29 Data Protection Working Party. Guidelines on
Automated individual decision-making and proﬁling for
the purposes of Regulation 2016/679, 2017.

Barocas, S., Selbst, A., and Raghavan, M. The hidden
assumptions behind counterfactual explanations and prin-
cipal reasons. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency, pp. 80–89.
ACM, 2020.

Bixby, R. A brief history of linear and mixed-integer pro-
gramming computation. Documenta Mathematica, pp.
107–121, 2012.

Breiman, L. Random forests. Machine Learning, 45(1):

5–32, 2001.

Figure 4. Isolation forests in counterfactual explanations for the
Iris data set: x-axis = “sepal length”, y-axis = “sepal width”

of OCEAN with and without isolation-forest constraints.
The results of this analysis are reported in Table 4, with
additional details in the supplementary material.

Table 4. Impact of the plausibility constraints in OCEAN

Data set

OCEAN-noIF
C
P

T(s)

OCEAN-IF
C

P

AD
CC
CP
GC
ON
PH
SP
ST

55% 0.21
0%
0.03
25% 0.12
25% 0.09
100% 0.01
35% 0.78
100% 0.02
40% 1.18

0.75
0.99
0.44
0.60
1.34
0.36
2.71
0.62

100% 0.40
100% 0.56
100% 0.57
100% 1.13
100% 0.01
100% 2.40
100% 0.02
100% 1.63

T(s)

1.82
1.54
0.85
1.71
1.64
1.81
4.43
1.36

As seen in this experiment, plausibility comes with extra
costs that depend on the data set. Nevertheless, one cannot
refer to a trade-off between cost and plausibility since mis-
leading targets do not necessarily help to fulﬁll any given
goal and may even trigger additional losses. Finding plau-
sible and actionable explanations should therefore be the
over-arching goal in counterfactual search. Our experiments
also demonstrate that unconstrained solutions are rarely
plausible and that dedicated constraints (e.g., through iso-
lation forests) should be set up. Finally, the computational
effort of OCEAN has roughly doubled due to the addition
of the isolation forests in the model. This appears to be a
reasonable increase given the current efﬁciency of OCEAN
and the high practical importance of ﬁne-grained plausibility
constraints.

5. Discussion

We have shown that it is possible to generate optimal plausi-
ble counterfactual explanations at scale for tree ensembles

Brodley, C. and Utgoff, P. Multivariate decision trees. Ma-

chine Learning, 19(1):45–77, 1995.

Optimal Counterfactual Explanations in Tree Ensembles

Cui, Z., Chen, W., He, W., and Chen, Y. Optimal action
extraction for random forests and boosted trees. Proceed-
ings of the 21th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 179–188,
2015.

German credit preprocessing, 2017. URL https://www.

kaggle.com/uciml/german-credit.

Gower, J. A general coefﬁcient of similarity and some of its

properties. Biometrics, 27(4):857–871, 1971.

Mothilal, R., Sharma, A., and Tan, C. Explaining machine
learning classiﬁers through diverse counterfactual expla-
nations. In Proceedings of the 2020 Conference on Fair-
ness, Accountability, and Transparency, FAT*’20, pp.
607–617, New York, NY, USA, 2020. Association for
Computing Machinery.

Rudin, C. Stop explaining black box machine learning
models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence, 1(5):206–
215, 2019.

Grotzinger, S. and Witzgall, C. Projections onto order sim-
plexes. Applied Mathematics and Optimization, 270(12):
247–270, 1984.

Russell, C. Efﬁcient search for diverse coherent explana-
tions. Proceedings of the Conference on Fairness, Ac-
countability, and Transparency, pp. 20–28, 2019.

Tolomei, G., Silvestri, F., Haines, A., and Lalmas, M. In-
terpretable predictions of tree-based ensembles via ac-
tionable feature tweaking. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 465–474, New York, NY,
2017.

Ustun, B., Spangher, A., and Liu, Y. Actionable recourse in
linear classiﬁcation. In Proceedings of the Conference on
Fairness, Accountability, and Transparency, pp. 10–19,
2019.

Venkatasubramanian, S. and Alfano, M. The philosoph-
ical basis of algorithmic recourse. Proceedings of the
2020 Conference on Fairness, Accountability, and Trans-
parency, pp. 284–293, 2020.

Verma, S., Dickerson, J., and Hines, K.

Counter-
factual explanations for machine learning: A review.
arXiv:2010.10596, 2020.

Vidal, T. and Schiffer, M. Born-again tree ensembles. In
III, H. D. and Singh, A. (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume
119, pp. 9743–9753, Virtual, 2020. PMLR.

Wachter, S., Mittelstadt, B., and Russell, C. Counterfactual
explanations without opening the black box: Automated
decisions and the GDPR. Harvard Journal of Law &
Technology, 31:841, 2018.

Wolsey, L. A. Integer Programming. John Wiley & Sons,

Hoboken, NJ, 2020. ISBN 9781119606475.

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Gian-
notti, F., and Pedreschi, D. A survey of methods for
explaining black box models. ACM Computing Surveys,
51(5):93:1–93:42, 2018.

Jeroslow, R. and Lowe, J. Modelling with integer variables.
Mathematical Programming Study, 22:167–184, 1984.

Kanamori, K., Takagi, T., Kobayashi, K., and Arimura, H.
DACE: Distribution-aware counterfactual explanation by
mixed-integer linear optimization. Proceedings of the
Twenty-Ninth International Joint Conference on Artiﬁcial
Intelligence, IJCAI-20, pp. 2855–2862, 2020.

Karimi, A.-H., Barthe, G., Balle, B., and Valera, I. Model-
agnostic counterfactual explanations for consequential
decisions. In Chiappa, S. and Calandra, R. (eds.), Pro-
ceedings of the Twenty Third International Conference
on Artiﬁcial Intelligence and Statistics, volume 108 of
Proceedings of Machine Learning Research, pp. 895–905.
PMLR, 2020a.

Karimi, A.-H., Barthe, G., Sch¨olkopf, B., and Valera, I. A
survey of algorithmic recourse: deﬁnitions, formulations,
solutions, and prospects. arXiv:2010.04050, 2020b.

Karimi, A. H., Sch¨olkopf, B., and Valera, I. Algorithmic
recourse: From counterfactual explanations to interven-
tions. arXiv:2002.06278, 2020c.

Liu, F., Ting, K., and Zhou, Z.-H. Isolation forest. In 2008
Eighth IEEE International Conference on Data Mining,
pp. 413–422, 2008.

Lucic, A., Oosterhuis, H., Haned, H., and de Rijke, M. FO-
CUS: Flexible optimizable counterfactual explanations
for tree ensembles. arXiv:1911.12199, 2019.

Mahajan, D., Tan, C., and Sharma, A. Preserving causal
constraints in counterfactual explanations for machine
learning classiﬁers. arXiv:1912.03277, 2019.

Supplementary Material – Proofs

Optimal Counterfactual Explanations in Tree Ensembles

Proof of Theorem 1.
Let λ, y be a solution of Equations (8–12). We will prove by induction on the depth d that every node v of depth d in every
tree t ∈ T is such that the variables yvt belong to {0, 1} and that there is a unique node v in t at depth d such that yv = 1.
Equation (8) gives the result for d = 1. Now, suppose that the result is true up to depth d and let v be a node of depth
d + 1 in some tree t of the forest. Let u be the parent of v. By induction hypothesis, yu belongs to {0, 1}. If yu = 0, then
Equation (9) implies that yv = 0. If yu = 1, then yv = 1 if λtd = 1 (resp. 0) and v is the left (resp. right) child of u, and 0
otherwise. In all cases, we have yv ∈ {0, 1}. Therefore, there is a unique node v in t of depth d such that yv = 1. This
concludes the induction step and proves the theorem.

Proof of Theorem 2.

(i) Let λ, y, x, µ be a feasible solution.

We will ﬁrst prove that the numeric features of this solution are consistent with all splits.
Theorem 1 ensures that y is integral and that in each tree t there is a unique leaf l such that yl = 1. Let P be the path
from the root of t to l. A backward induction along P using Equation (9) shows that yv = 1 for every vertex v in P
and also implies that yv = 0 for all nodes that are not in P .
Let v be a non-leaf node along P . Let i be the feature of v and j be its split level (i.e., such that v ∈ V I
to the left child of v, then Equation (16) ensures that µj
Equation (20) then ensures that xi ≤ xj
i = 1 for any k < j. Equation (18) ensures that µj
ensure that µk
by disjunction of cases, xi is consistent with the split at node v.
If i is a binary feature, then Equations (21–23) immediately ensure that xi = 0 if P goes to the left and xi = 1
otherwise, which gives the consistency. The same reasoning on the ν gives the consistency for the categorical features.

tij). If P goes
i = 0 for any k > j.
i . Otherwise, if P goes to the right child of v, then Equations (17) and (15)
i . Therefore,

i > 0, and Equation (20) then gives xi > xj

i = 0 and Equation (15) ensures that µk

(ii) This result immediately follows from the fact that the number of variables and constraints is in O(Nv) and that the only
constraints with more than two non-zero coefﬁcients are constraints (10), (13), (20), and (27) with O(Nv) non-zero
terms overall in each case.

(iii) We will use Cui et al. (2015) notations when referring to the OAE formulation. The proof reconstructs a solution of the
linear relaxation of OAE from the optimal solution of the linear relaxation of OCEAN. We prove this result when all
the features are numeric. The other cases are simpler and derive from a similar proof scheme.

Let λ, y, x, µ be an optimal solution of the linear relaxation of our formulation. We reconstruct a solution of the linear
relaxation of OAE as follows. First, remark that OAE’s variables φ and OCEAN’s variables y corresponding to tree
leaves represent the same quantities. Therefore, given a tree t and a leaf k of t, deﬁne

where the φ refer to the variables in OAE. The ﬂow constraints (8)-(9) of OCEAN then immediately imply

φtk = ytk

mt(cid:88)

k=1

φtk = 1.

Let i be a numerical feature and let ˆji be the index associated with the original value ˆxi in OCEAN (see Section 3.3).
OAE has the same splits as OCEAN except for the split level ˆji corresponding to the original value of ˆxi. With this,
OCEAN deﬁnes variables {µ0

i } and OAE deﬁnes variables {vi1, . . . , vini} with ni = ki.

i , . . . , µki

Now, let us set the following values:

vij = ˜µj−1

i − ˜µj

i where

˜µj
i =

(cid:40)

µj
i
0

if µj
i > (cid:15)
otherwise.

(vars)

Summing the values of vij as deﬁned in Equation (vars), we obtain (cid:80)
does not intervene in constraints (16) of OCEAN, and µki

i . Moreover, observe that µ0
i
i does not intervene in constraints (17). The convexity of

j vij = µ0

i − ˜µki

Optimal Counterfactual Explanations in Tree Ensembles

the cost and the optimality of the solution of OCEAN considered then imply that µ0
solution of the linear relaxation, and therefore ˜µki

i = 0, in such a way that:

i = 1 and µki

i ≤ (cid:15) in an optimal

Let us now prove that:

ni(cid:88)

j=1

vij = 1.

φtk ≤

(cid:88)

v∈Skp

v, ∀t, ∀k, ∀p ∈ πtk,

which implies (by aggregation) the following constraints of OAE:

φtk ≤

1
|πtk|

(cid:88)

(cid:88)

p∈πtk

v∈Skp

v, ∀t, ∀k.

(dis)

(agg)

For this, consider a tree t, a leaf k, and a node p on the path to leaf k. Let jp be the index of the split corresponding
to p. Suppose that the path goes to the left at p. Constraint (9) of OCEAN implies that φtk = ytk ≤ ytl(p). Hence,

(cid:88)

v∈Skp

v = µ0

i − µjp−1

i

= 1 − µjp−1

i

,

and since ytl(p) ≤ 1 − µjp−1
to the right at p. Constraint (9) of OCEAN implies that φtk = ytk ≤ ytr(p). Hence,

i

by constraint (15) and (16) of OCEAN, we obtain (dis). Suppose now that the path goes

(cid:88)

v∈Skp

v = µjp−1
i

− µni

i = µjp−1

i

,

and since ytr(p) ≤ µjp−1
Moreover, the majority vote constraints (13) and (14) of OCEAN ensure that the majority vote constraint of OAE is
satisﬁed, and thus the solution built is a feasible solution of OAE.

by constraint (17) of OCEAN, we obtain (dis).

i

We ﬁnally evaluate the cost of this solution. We place ourselves in the general context of a piecewise linear convex loss
function, as in Cui et al. (2015). Deﬁne (cid:96)j
i ), the value of the loss for feature i if the counterfactual value for
feature i is xj
i , where xj
i = 0 and
xni+1
i

i is the coordinate of split j for feature i in OCEAN and 1 ≤ j ≤ ni. Deﬁning x0

= 1, the objective of OCEAN is:

i = (cid:96)(ˆxi, xj

(cid:96)0
i +

((cid:96)j+1

i − (cid:96)j

i )µj
i .

ni(cid:88)

j=0

Reindexing the sum and using the fact that ˜µ0
than the numerical precision (cid:15) as:

i = 1 and ˜µni

i = 0, we can rewrite this objective with an error non-greater

ni(cid:88)

j=1

i (˜µj−1
(cid:96)j

i − ˜µj

i ).

i = xj

Let bj
We have bj
correspond to (cid:96)(ˆxi, ˜xj
extremity of the cell if j < ˆji, the left extremity if j > ˆji and ˆxi if j = ˆji. Given the index shift between xj
in ˆxi, we get Cij(xc) = (cid:96)(ˆxi, xj

i be the coordinate of the splits for OAE. Finally, let ˆji be the index of the split of OCEAN corresponding to ˆxi.
for j ≥ ˆji. Consider now the costs Cij(xc) in the objective of OAE, which
i is the right
i and bj
i
i . The value of the objective of OAE for the solution reconstructed is therefore

i is the nearest point of ˆxi in the cell j of OAE. By convexity of the loss, ˜xj

i for j < ˆji, and bj
i ) where ˜xj

i = xj+1

i

i ) = (cid:96)j
ni(cid:88)

vijCij =

ni(cid:88)

j=1

vij(cid:96)j

i =

ni(cid:88)

j=1

i (˜µj−1
(cid:96)j

i − ˜µj

i ),

j=1

which is the value of the optimal solution of OCEAN.

In summary, we have built a feasible solution of the linear relaxation OAE whose objective is equal to the value of the
linear relaxation of OCEAN. This concludes the proof.

Optimal Counterfactual Explanations in Tree Ensembles

Implied integrality of the x variables for binary features and ν variables for categorical features.
Consider an optimal solution of the continuous relaxation of variables x for binary features and ν for categorical features.
Fix all the variables but those corresponding to one of these features. Since the variables y are integer in an optimal solution,
the feasible set of the resulting problem is a simplex, and a basic optimal solution is therefore integer.

Supplementary Material – Model Reﬁnements

In this section, we discuss some additional formulation reﬁnement and extensions. Firstly, we provide improved formulations
for ordinal features. Next, we discuss multivariate splits for numerical features and combinatorial splits for categorical
features.

Ordinal features. These features involve ordered levels in a similar fashion as numerical features, but open intervals
between successive levels bear no meaning or should be prohibited. In this case, no fractional value should arise due to (e.g.,
actionability or plausibility) side constraints, and the associated costs are typically discretized over the ordinal levels. To
efﬁciently model these features, we use specialized constraints that represent a simpliﬁcation of the formulation used for
numerical features.

Let ki be the number of possible categories for ordinal feature i ∈ IO. For each tree t, let V I
tij be the set of internal nodes
involving a split on feature i at level j, such that samples with feature level smaller or equal to j descend to the left branch,
whereas other samples descend to the right branch. Consistency of feature i can be ensured through auxiliary variables ωj
i
for j ∈ {1, . . . , ki − 1} which take value 0 if feature i has a level smaller or equal to j and 1 otherwise. These conditions
can be ensured as follows:

i

i ≥ ωj
ωj−1
ωj
i ≤ 1 − ytl(v)
ωj
i ≥ ytr(v)
ωj
i ∈ {0, 1}

j ∈ {2, . . . , ki − 1}
j ∈ {1, . . . , ki −1}, t ∈ T , v ∈ V I
tij
j ∈ {1, . . . , ki −1}, t ∈ T , v ∈ V I
tij
j ∈ {1, . . . , ki − 1}.

(34)

(35)

(36)

(37)

Multivariate splits on numerical features. If the need arises, one can also model the search for counterfactual explanations
in contexts where some splits of the decision trees (or isolation trees – e.g., as in extended isolation forests) involve linear
combinations of the features. To that end, we can rely on big-M constraints as follows:

atvx ≤ btv − (cid:15) + M +
atvx ≥ btv + (cid:15) − M −

tv(1 − ytL(v))
tv(1 − ytR(v))

t ∈ T , v ∈ V I
t
t ∈ T , v ∈ V I
t .

(38)

(39)

It is important to use the smallest possible value for the big-M constants to achieve a good linear relaxation. Given that
numerical features are normalized in the interval [0, 1], we can use:

M +

tv =

p
(cid:88)

k=1

max{0, atvp} − btv

M −

tv = btv −

p
(cid:88)

k=1

min{0, atvp}.

(40)

(41)

Combinatorial splits on categorical features. As usual for categorical features, ki will be used to denote the number of
possible categories for feature i ∈ IC, and let νj
i be a variable that will take value 1 if xi belongs to category j ∈ {1, . . . , ki}
and 0 otherwise. Combinatorial splits on categorical features involve sending certain categories towards the right branch,
and the rest on the left. For each split at an internal node v of tree t, let C +
ivt be the set of categories that descend towards the
right branch. Then, the consistency of categorical feature i through the forest with combinatorial splits can be modeled as:

νj
i ≤ 1 − ytl(v)

νj
i ≥ ytr(v)

(cid:88)

j∈C+
ivt
(cid:88)

j∈C+
ivt

t ∈ T , v ∈ V I
t

t ∈ T , v ∈ V I
t

(42)

(43)

Optimal Counterfactual Explanations in Tree Ensembles

νj
i ∈ {0, 1}
(cid:88)
νj
i = 1.

j∈Ci

j ∈ Ci

(44)

(45)

Supplementary Material – Detailed Numerical Results

5.1. Detailed Performance Comparisons

Figures F1 and F2 extends Figures 2 and 3 in the main body of the paper with an analysis on all datasets and methods. In
those ﬁgures, and missing data point for a method means that no feasible counterfactual has been found (possible for FT due
to the way the search process is conducted) or that the CPU time limit of 900 seconds has been exceeded.

The left section of Table A1 extends the results of Table 3 in the main paper for a varying number of trees “#T” in
{10, 20, 50, 100, 200, 500} with a maximum depth ﬁxed to 5. In a similar fashion, the right section of the table extends
these results for varying depth “#D” in {3, 4, 5, 6, 7, 8} with a number of trees ﬁxed to 100.

5.2. Detailed Performance with Plausibility Constraints

This section provides additional detailed results concerning the performance of OCEAN with plausibility constraints via
isolation forests.

Firstly, Figure F3 displays the same computational time analysis as Figure 1 in the main paper, when considering the
additional plausibility constraints through isolation forests. As visible on this ﬁgure, OCEAN maintains a good scalability
for all objectives even with the plausibility restrictions.

Finally, Table A2 reports the average computational time in seconds needed to ﬁnd optimal counterfactual explanations
with (OCEAN-IF) and without (OCEAN-noIF) plausibility constraints as a function of the maximum depth of the trees (in
{5, 6, 7, 8}, corresponding to a maximum of {32, 64, 128, 256} leaves). In these experiments, we use the l1 objective, the
number of trees is set to the baseline value of 100, and the isolation forest has the same depth limit as the random forest.

As visible in these experiments, the use of the plausibility restrictions through isolation forests roughly doubles the time
needed to locate optimal explanations. This increase directly relates to the fact that considering both the random forest and
isolation forest simultaneously involves considering twice the number of trees. Despite this increase of model complexity,
optimal explanations are found in less than one minute, even when considering a maximum depth of 8 (i.e., with up to 256
leaves per tree and 51, 200 leaves overall in both forests).

Supplementary Material – Open Source Code

All the material (source code and data sets) needed to reproduce our experiments is accessible at https://github.
com/vidalt/OCEAN under a MIT license.

Optimal Counterfactual Explanations in Tree Ensembles

Figure F1. Comparative analysis of CPU time as a function of the maximum depth of the trees. Number of trees set to 100. Results
provided for all data sets.

Figure F2. Comparative analysis of CPU time as a function of the number of trees in the ensemble. Maximum depth ﬁxed to 5. Results
provided for all data sets.

0.11.010100ADT(s)CCCPGC0.11.010100 3 4 5 6 7 8ONT(s) 3 4 5 6 7 8PH 3 4 5 6 7 8SP 3 4 5 6 7 8STMax DepthMax DepthMax DepthMax DepthMACEOCEANOAEFT0.11.010100ADT(s)CCCPGC0.11.010100102050100200500ONT(s)102050100200500PH102050100200500SP102050100200500STMACEOCEANOAEFTNb TreesNb TreesNb TreesNb TreesOptimal Counterfactual Explanations in Tree Ensembles

Table A1. Time and solution quality comparison, considering all conﬁgurations

#T Data

FT

10 AD
CC
CP
GC
ON
PH
SP
ST

20 AD
CC
CP
GC
ON
PH
SP
ST

50 AD
CC
CP
GC
ON
PH
SP
ST

100 AD
CC
CP
GC
ON
PH
SP
ST

200 AD
CC
CP
GC
ON
PH
SP
ST

T(s)

R

NA —
23.0
0.51
18.5
0.42
17.4
0.23
106.0
0.13
0.23
1.7
NA —
NA —

NA —
22.2
1.62
1.27
10.6
10.0
0.81
NA —
0.63
1.4
NA —
NA —

NA —
5.9
8.06
5.0
6.19
6.2
4.44
2.10
97.7
1.6
2.72
NA —
NA —

15.9
3.03
10.2
29.44
4.5
22.68
4.8
16.26
31.7
10.05
10.95
1.4
NA —
NA —

14.8
10.69
8.3
106.37
3.4
80.53
4.9
59.37
18.4
38.18
1.6
36.60
40.71
8.5
NA —

MACE
R
T(s)

1.0
6.64
1.3
5.99
1.0
5.85
5.17
1.0
>900 —
67.13 1.0
7.6
2.80
>900 —

7.92
1.0
7.56
1.3
8.07
1.0
1.0
7.69
>900 —
92.15 1.0
10.51 4.9
>900 —

12.80 1.1
24.97 1.1
10.65 1.0
11.51 1.0
>900 —
>900 —
>900 —
>900 —

20.60 1.1
41.25 1.2
15.82 1.0
19.03 1.0
>900 —
>900 —
>900 —
>900 —

39.18 1.0
120.31 1.2
26.61 1.0
40.91 1.0
>900 —
>900 —
>900 —
>900 —

OAE

T(s)

0.05
0.08
0.03
0.04
0.13
0.02
0.14
0.12

0.44
0.47
0.07
0.29
0.89
0.07
0.88
0.35

R

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

8.71
1.0
3.02
1.0
0.20
1.0
0.81
1.0
9.29
1.0
1.0
0.38
15.31 1.0
1.0
6.75

28.37 1.0
1.0
5.52
1.0
0.38
5.08
1.0
>900 —
0.94
1.0
>900 —
69.64 1.0

>900 —
21.89 1.0
1.21
1.0
13.58 1.0
>900 —
3.44
1.0
>900 —
>900 —

OCEAN
T(s) R

0.07 1.0
0.05 1.0
0.02 1.0
0.05 1.0
0.07 1.0
0.02 1.0
0.07 1.0
0.02 1.0

0.13 1.0
0.11 1.0
0.05 1.0
0.13 1.0
0.21 1.0
0.05 1.0
0.16 1.0
0.05 1.0

0.57 1.0
0.63 1.0
0.17 1.0
0.28 1.0
0.82 1.0
0.26 1.0
0.57 1.0
0.23 1.0

1.22 1.0
1.34 1.0
0.52 1.0
1.16 1.0
2.97 1.0
0.52 1.0
2.73 1.0
1.10 1.0

4.30 1.0
4.12 1.0
1.97 1.0
3.59 1.0
7.34 1.0
1.99 1.0
9.13 1.0
4.70 1.0

500 AD
18.0
56.39
CC
4.9
640.08
CP
2.9
479.78
GC 365.30
4.2
ON 265.22 33.0
PH 261.96
1.5
SP
7.7
262.35
ST
NA —

>900 — 22.96 1.0
120.25 1.1
>900 — 145.36 1.0
15.98 1.0
7.74 1.0
3.45
59.36 1.0
1.0
41.22 1.0
214.64 1.0
13.82 1.0
>900 — 47.35 1.0
>900 —
>900 —
10.96 1.0
13.05 1.0
>900 — 83.05 1.0
>900 —
>900 — 55.95 1.0
>900 —

#D Data

FT

3

4

5

6

7

8

T(s)

R

22.6
2.27
128.2
9.13
5.9
4.86
1.9
9.14
38.5
3.35
4.82
1.3
NA —
NA —

22.5
2.07
26.5
14.40
4.9
10.00
5.2
9.92
52.1
4.74
6.02
1.5
NA —
NA —

3.03
15.9
29.44
10.2
22.68
4.5
16.26
4.8
10.05
31.7
1.4
10.95
NA —
NA —

21.6
4.11
11.5
53.13
9.1
48.20
6.9
22.60
24.7
25.94
20.30
1.3
NA —
NA —

8.38
85.39
71.92
30.99
49.16
31.33
21.22
13.07

22.3
13.6
8.2
7.7
70.9
1.3
7.8
3.3

10.5
15.55
139.40 18.2
9.1
104.49
7.5
41.25
60.3
84.72
1.3
45.32
6.7
28.16
3.0
15.07

AD
CC
CP
GC
ON
PH
SP
ST

AD
CC
CP
GC
ON
PH
SP
ST

AD
CC
CP
GC
ON
PH
SP
ST

AD
CC
CP
GC
ON
PH
SP
ST

AD
CC
CP
GC
ON
PH
SP
ST

AD
CC
CP
GC
ON
PH
SP
ST

MACE
R
T(s)

1.0
9.76
6.1
7.69
7.59
1.0
18.27 1.0
>900 —
84.46 1.0
>900 —
>900 —

15.19 1.0
15.66 1.4
12.68 1.0
15.21 1.0
>900 —
164.22 1.0
>900 —
>900 —

20.60 1.1
41.25 1.2
15.82 1.0
19.03 1.0
>900 —
>900 —
>900 —
>900 —

OAE

T(s)

0.25
0.19
0.05
1.35
0.58
0.16
2.74
0.65

R

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.0
6.71
1.0
1.16
1.0
0.12
1.0
2.25
1.0
5.86
0.28
1.0
40.31 1.0
1.0
8.76

28.37 1.0
1.0
5.52
1.0
0.38
5.08
1.0
>900 —
1.0
0.94
>900 —
69.64 1.0

>900 —
29.08 1.1
28.33 1.0
62.36 1.2
1.0
0.66
21.96 1.0
6.15
1.0
20.41 1.0
>900 —
>900 —
0.98
1.0
178.61 1.0
>900 —
>900 —
>900 — 128.52 1.0

OCEAN
T(s) R

0.13 1.0
0.08 1.0
0.05 1.0
0.25 1.0
0.22 1.0
0.08 1.0
0.28 1.0
0.11 1.0

0.35 1.0
0.38 1.0
0.19 1.0
0.59 1.0
0.94 1.0
0.19 1.0
1.19 1.0
0.35 1.0

1.22 1.0
1.34 1.0
0.52 1.0
1.16 1.0
2.97 1.0
0.52 1.0
2.73 1.0
1.10 1.0

2.03 1.0
4.45 1.0
1.17 1.0
1.28 1.0
7.08 1.0
1.36 1.0
5.42 1.0
1.47 1.0

37.84 1.1
92.11 1.2
28.14 1.0
26.50 1.0
>900 —
>900 —
>900 —
>900 —

59.30 1.1
154.47 1.3
33.11 1.0
27.45 1.0
>900 —
236.58 1.0
>900 —
>900 —

>900 —
4.24 1.0
11.13 1.0
92.18 1.0
2.41 1.0
1.18
1.0
20.41 1.0
3.49 1.0
>900 — 15.41 1.0
1.61
3.72 1.0
>900 — 18.26 1.0
>900 —
2.91 1.0

1.0

>900 — 12.35 1.0
>900 — 21.96 1.0
5.65 1.0
2.17
1.0
36.89 1.0
5.11 1.0
>900 — 72.87 1.0
2.21
8.70 1.0
>900 — 27.94 1.0
>900 —
4.93 1.0

1.0

Optimal Counterfactual Explanations in Tree Ensembles

Figure F3. CPU time to ﬁnd an optimal counterfactual explanations, considering different data sets and objectives

Table A2. CPU time (s) of OCEAN with isolation forests for ensuring plausibility

Data set
Max-Depth
Max-Leaves

AD
CC
CP
GC
ON
PH
SP
ST

OCEAN-noIF

5
32

0.75
0.99
0.44
0.60
1.34
0.36
2.71
0.62

6
64

1.43
4.70
1.18
1.15
3.19
1.16
6.60
1.47

7
128

2.70
9.14
2.74
2.05
9.61
2.25
15.92
2.22

8
256

5.36
24.50
5.32
3.50
36.81
5.09
34.02
3.13

OCEAN-IF
6
64

7
128

3.83
5.42
1.35
3.59
4.29
5.86
7.18
3.63

7.26
12.83
3.18
11.04
11.79
15.16
27.63
8.13

5
32

1.82
1.54
0.85
1.71
1.64
1.81
4.43
1.36

8
256

14.10
32.83
7.06
18.16
41.87
43.12
37.98
12.31

lllllllllllllllllllllllllllldroplevels(interaction(Objective, Case))Time(s)ADCCCPGCONPHSPST0.10.512510L0L1L2CPU Time (s)0.2