1
2
0
2

y
a
M
0
2

]
T
E
.
s
c
[

1
v
3
4
9
9
0
.
5
0
1
2
:
v
i
X
r
a

Prospects and applications of photonic neural networks

Chaoran Huang a, Volker J. Sorgerb, Mario Miscugliob , Mohammed Al-Qadasic,
Avilash Mukherjeec, Sudip Shekharc, Lukas Chrostowskic, Lutz Lampec, Mitchell
Nicholsc, Mable P. Fokd, Daniel Brunnere, Alexander N. Taitf, Thomas Ferreira de
Limaa, Bicky A. Marquezg, Paul R. Prucnal a, Bhavin J. Shastrig.

aPrinceton University, Princeton, NJ 08542, USA;
bThe George Washington University, Washington DC, DC, USA;
c The University of British Columbia, Vancouver, BC V6T 1Z4, Canada;
d University of Georgia, Athens, GA 30602, USA;
d Institut FEMTO-ST, Universit´e Bourgogne-Franche-Comt´e CNRS UMR 6174, Besan¸con,
France;
f National Institute of Standards and Technology, 325 Broadway, Boulder, Colorado 80305,
USA;
g Queen’s University, Kingston, ON K7L 3N6, Canada

ARTICLE HISTORY
Compiled 24th May 2021

Abstract
Neural networks have enabled applications in artiﬁcial intelligence through machine
learning, and neuromorphic computing. Software implementations of neural net-
works on conventional computers that have separate memory and processor (and
that operate sequentially) are limited in speed and energy eﬃciency. Neuromorphic
engineering aims to build processors in which hardware mimics neurons and syn-
apses in the brain for distributed and parallel processing. Neuromorphic engineering
enabled by photonics (optical physics) can oﬀer sub-nanosecond latencies and high
bandwidth with low energies to extend the domain of artiﬁcial intelligence and
neuromorphic computing applications to machine learning acceleration, nonlinear
programming, intelligent signal processing, etc. Photonic neural networks have been
demonstrated on integrated platforms and free-space optics depending on the class
of applications being targeted. Here, we discuss the prospects and demonstrated
applications of these photonic neural networks.

KEYWORDS
Neuromorphic computing; photonic neural networks; neuromorphic photonics;
silicon photonics; machine learning.

1. Primer on artiﬁcial intelligence, machine learning, neuromorphic

computing, and neuromorphic photonics

Creating a machine that can process information like human brains has been a driving
force of innovations throughout history. Artiﬁcial intelligence (AI) has been coined
as an academic discipline since the 1950s (McCarthy, Minsky, Rochester, & Shannon,
2006). This ﬁeld underwent the ﬁrst surge of optimism from the 1950s to 1970s, how-

CONTACT Bhavin J. Shastri. Email: bhavin.shastri@queensu.ca

 
 
 
 
 
 
ever, followed by decades of setbacks. The biggest obstacle at that time was the lack
of computing power (The History of Artiﬁcial Intelligence, 2017). In the last decade,
AI has experienced explosive growth. Three sources fuel the advancement of AI: (1)
substantial development of AI algorithms, especially in machine learning and neural
network models (alias for “deep learning” (LeCun, Bengio, & Hinton, 2015)); (2) the
abundant amount of available information in the “big data” era; and (3) the rise of
computing power as predicted by Moore’s Law, together with new hardware (e.g.,
graphics processing unit (GPU)) and infrastructures (e.g., cloud-based servers).

State-of-the-art AI algorithms, by and large, are implemented using neural net-
works, a computing model inspired by the brain’s neuro-synaptic framework. Today,
nearly all AI algorithms are running on digital computers based on von Neumann
architecture, a computing architecture that has dominated computing design since it
was invented but is nothing like the brain. This architecture consists of a central-
ized processing unit (CPU) that performs all operations speciﬁed by the program’s
instructions and a separate memory that stores data and instructions. It processes
information sequentially in a serialized manner. However, neural network models are
radically diﬀerent from von Neumann architecture in some key features. First, neural
networks are highly parallel and distributed, whereas von Neumann architecture is
inherently sequential (or, in the best case: sequential-parallel with multi-processors).
Second, in neural networks, computing units (neurons) and storage units (synapses)
are co-located. In contrast, computing units (CPUs) and storage units (dynamic ran-
dom access memories (DRAMs)) are physically separate chips in digital computers.
The sharp contrast between the two architectures slows down the computing speed
and increases the power consumption, which, as a result, necessitates reinventing con-
ventional computers for eﬃcient information processing.

Neuromorphic (i.e. neuron isomorphic) computing promises to solve these problems
by creating radical new hardware platforms that can emulate the underlying neural
structure of the brain. The general idea is to build circuits composed of physical
devices that mimic the neuron biophysics interconnected by massive physical inter-
connects with co-integrated non-volatile memories. In doing so, neuromorphic hard-
ware could break performance limitations inherent in von Neumann architectures and
gain advantages in speed and eﬃciency in solving intellectual tasks. Achieving this
goal requires signiﬁcant advances in a wide range of technologies, including materi-
als, devices, device fabrication, system integration, platform co-integration, packaging
etc (Berggren et al., 2020; Schuman et al., 2017; Shastri et al., 2021).

Neuromorphic hardware has been built in electronics on various platforms, includ-
ing traditional digital CMOS (Furber, Galluppi, Temple, & Plana, 2014; Merolla et
al., 2014; Schemmel et al., 2010) and hybrid CMOS-memristive technologies (discussed
next) (Govoreanu et al., 2011; Yang, Strukov, & Stewart, 2013). Neural network mod-
els highlight the essential needs of high-degree physical interconnections, which, in
electronic neuromorphic hardware, is achieved by incorporating a dense mesh of wires
overlaying the semiconductor substrate as crossbar arrays. Unfortunately, electronic
connections fundamentally suﬀer harsh trade-oﬀs between bandwidth and intercon-
nectivity (Miller, 2009; Nahmias et al., 2019). A major limitation for neuromorphic
electronics is interconnect density, thus conﬁning the neuromorphic processing speed
and associated application space within the MHz regime.

Photonics has unmatched feats for interconnects and communications in terms of
bandwidth, which can negate the bandwidth and interconnectivity trade-oﬀs (Ahmed
et al., 2020; Ahmed, Sharkia, Casper, Mirabbasi, & Shekhar, 2016; Shastri et al.,
2021). The advantages of photonics for neural networks were recognized decades ago.

2

The photonic neural network research was pioneered by Psaltis and others who adopted
spatial multiplexing techniques enabling all-to-all interconnection (Psaltis & Farhat,
1985). However, low-level photonic integration and packaging technologies hindered
the practical applications of photonic neural networks at that time. Nevertheless, the
landscape of photonic neural networks has changed tremendously with the emergence
of large-scale photonic fabrication and integration techniques (Feldmann, Youngblood,
Wright, Bhaskaran, & Pernice, 2019; X. Lin et al., 2018; Shastri et al., 2021; Thomson
et al., 2016). For example, silicon photonics provides an unprecedented platform to
produce large-scale and low-cost optical systems (Shekhar, 2021; Sun, Timurdogan,
Yaacobi, Hosseini, & Watts, 2013; Thomson et al., 2016). In parallel, a broad domain
of emerging applications (such as solving nonlinear optimization problems or real-time
processing of multichannel, gigahertz analog signals) is also looking for new computing
platforms to fulﬁll their computing demands (De Lima et al., 2019; Han, Jentzen, &
Weinan, 2018; Huang, Fujisawa, et al., 2020; Khan, Fan, Lu, & Lau, 2019; Ma et
al., 2020). All these changes have shed light on new opportunities and directions for
photonic neural networks (Prucnal & Shastri, 2017).

This paper is intended to provide an intuitive understanding of photonic neural
networks and why, where, and how photonic neural networks can play a unique role in
enabling new domains of applications. First, we discuss the challenges of digital versus
analog approaches in implementing neural networks. Next, we provide a rationale for
photonic neural networks as a compelling alternative for neuromorphic computing
compared to electronic platforms. Then, we outline the primary technology required
for evolving neuromorphic photonic processors, review existing approaches, and discuss
challenges. In the subsequent sections, we provide a survey of new applications enabled
by photonic neural networks and highlight the role of photonic neural networks in
addressing the challenges in these applications.

2. Digital vs. analog neural networks

In this Section, we brieﬂy compare the state-of-the-art electronic implementations
of neural networks in digital and analog domain. We establish the advantages and
limitations of analog implementations in general to then make the case for analog
photonic implementations in the following Sections.

Deep neural networks (DNN) model complex nonlinear functions by composing lay-
ers of linear matrix operations with non-linear activation functions. Computationally,
DNNs are mostly matrix-multiplication, with matrix-multiplications taking more than
90% of the total computations in a DNN (Cong & Xiao, 2014). Due to the underlying
array-based operation in the matrix multiplication, digital electronic neural network
hardware is usually composed of basic units, referred to as processing elements (PEs),
in a 2D array structure (Chen, Krishna, Emer, & Sze, 2016). Such a structure enables
the matrix multiplication operation to be N× faster than CPUs, where N is the input
vector length. Usually, PEs are composed of digital multipliers and adders, with pre-
cision up to 32 bits, to perform a single multiply and accumulate (MAC) operation,
similar to the arithmetic logic unit in the CPU core.

Since DNNs consume a huge chunk of energy in data movement, many digital neural
network hardware focus on optimizing dataﬂow to save energy. Based on the connection
of PEs and the interconnects, various dataﬂows can be described. An output-stationary
dataﬂow performs all the MAC operations for a single output before moving to the
next. All the inputs and weights required are fetched from the memory, multiplied, and

3

added to the partial sum, which is stored inside PE (Chen, Emer, & Sze, 2016). On
the other hand, a weight-stationary dataﬂow holds the weights inside PE to maximize
weight reuse. The partial sum accumulation occurs across multiple PEs while the input
vector is fed in a staggered style allowing the PEs to perform MAC operation with the
internally stored weights. Further dataﬂow optimizations combining diﬀerent types
of data reuse are also possible to reduce energy further (Chen, Emer, & Sze, 2016;
Gudaparthi et al., 2019).

Implementing the MAC operations in analog domain can help in reducing the en-
ergy consumption. Analog electronic elements, such as charge, current and time can
be used to represent the data values. An inherent advantage in analog techniques is the
built-in addition operation without requiring additional circuits. To perform the MAC
operation with analog electronics, switched-capacitor techniques charge a capacitance
sized proportionally to the weight with a current sized proportionally to the input
(Bankman & Murmann, 2016), current-steering techniques control the magnitude of
current ﬂowing through transistors (Skrzyniarz et al., 2016), and time-domain tech-
niques modulate the pulse width of a signal using controlled oscillators (Cao, Chang,
& Raychowdhury, 2020). Such analog techniques have shown to decrease energy signi-
ﬁcantly for small DNN models: 4× using switched-capacitor on BinaryNet, 67× using
current steering on Matched Filter, and 1.4× using time-domain techniques on mobile
reinforcement learning. The shortcomings of analog techniques include: limited size of
DNN models, low bit precision (< 4 b) and associated accuracy loss, analog-to-digital
converter/digital-to-analog converter (ADC/DAC) overhead, susceptance to noise and
process, voltage and temperature (PVT) variations.

Analog implementations have a direct consequence for noise and noise propagation
(Sarpeshkar, 1998). Since the probability of corrupting a symbol usually is identical
for all bits in a sequence, the impact of a noise-induced Boolean symbol modiﬁca-
tion can be dramatic. Compared to that the corruption of an analog signal is usually
more subtle as signal perturbations are mostly proportional to noise amplitude. Digital
encoding therefore requires that thresholding levels signiﬁcantly exceed all noise amp-
litudes; however, the signal propagation is then practically noiseless as the noiseless
symbolic representation is continuously re-established. Furthermore, increasing a di-
gital signal’s resolution is comparatively economic as the number of digitization levels
grows exponentially.

Well-designed circuits readily approach the thermodynamic noise limit to better
than one order of magnitude (Wattanapanitch, Fee, & Sarpeshkar, 2007). Thermody-
namics, therefore, establishes the link between such information centered arguments
and the energy fundamentally required for a certain SNR. Digital encoding is penal-
ized with a large constant energy penalty due to the required high encoding ﬁdelity.
However, its superior scaling means that digital becomes more energy eﬃcient than
analog, roughly beyond SNR < 104 (Boahen, 2017). Recently digital implementations
of neural networks signiﬁcantly reduced their bit-precision, with several systems today
running with 8 or less bit resolution during inference. Finally, spiking NNs occupy a
middle ground and potentially are superior in energy eﬃciency to analog for SNR
> 102 and to digital for SNR < 107 (Boahen, 2017).

Finally, the accumulation of noise can be strongly managed using the connections of
a neural network. Studies based on linear, symmetric, i.e. untrained networks of noisy
linear neurons show that neural network analog in and output neurons are the chief
noise source, while in particular noise uncorrelated across neurons is essentially fully
suppressed through the network’s connections (Semenova et al., 2019). New studies in
fully trained networks of noisy nonlinear units show that nonlinearity also eﬃciently

4

decorrelates noise from correlated noise-populations (Semenova, Larger, & Brunner,
2021). This is important as such noise can for example be induced by a common power
supply. Finally, rather weak requirements allow to fully freeze the propagation of noise
through a network, and an analog photonic neural network’s output can, therefore,
approach the SNR of a single neuron (Semenova et al., 2021).

Another possible method to reduce data movement energy is moving the computing
inside the memory modules itself. Such architectures are referred to as In-Memory
computing (IMC), and use the memory cells as an analog circuit to perform the MAC
operations, generally in a weight-stationary dataﬂow. The inputs are analog currents
or voltages on the wordlines, while their weights are either binary, ternary or digitally
stored over multiple memory cells. The accumulation happens inherently in the bitlines
in the memory array, resulting in an analog output (Biswas & Chandrakasan, 2018;
Jintao Zhang, Zhuo Wang, & Verma, 2016). IMC architectures, while signiﬁcantly
enhancing the throughput of the system, operate in analog which call for adding more
system level design considerations to meet the output signal to noise ratio (SNR)
requirements and size.

IMC can also be performed in crossbar arrays of emerging memories, such as res-
istive RAM (ReRAM), conductive bridging, magnetic tunnel junctions, and phase
change memories (Yu, Sun, Peng, & Huang, 2020). Explicit multipliers, adders and
PE interconnects are not needed in IMC. Rather, the equivalent PE array in digital is
implemented in IMC in just the area required to implement the memory array, plus the
ADC and DAC at the array periphery (Yu et al., 2020). Increased area eﬃciency al-
lows packing far more parallel units, hence processing operations can be accelerated by
more than an order of magnitude. For comparison, the area required to implement the
PE in digital implementations is 13.4 × 106 F2 (Chen, Krishna, et al., 2016), whereas
the average memory cell sizes in IMC are less than 100 F2 (Liu et al., 2020), where F
is the minimum feature size of the technology. Furthermore, energy consumption can
also be reduced by an order of magnitude over the equivalent digital systems, since
several MAC operations are performed almost at the cost of a single read operation
of the memory array.

IMC suﬀers from constraints similar to the analog MAC implementations. Further-
more, the nonidealities of analog memory cells and their interconnects pose limitations
for achieving high accuracy and scaling (Marinella et al., 2018). For example, the non-
linearities of memory cells and the resistance of the interconnect in ReRAM IMC was
shown to degrade the accuracy of computation with scaling (Peng Gu et al., 2015).
Noise limitations have shown to saturate the computing accuracy of analog crossbars
to 8 bits (Hu et al., 2016). In comparison with digital implementations, the energy,
area and latency advantages from IMC have been shown to reduce with the increased
precision (Agarwal et al., 2019). But precision reduction techniques such as variable
layer precisions and nonuniform quantization (Cong & Xiao, 2014; Judd, Albericio,
Hetherington, Aamodt, & Moshovos, 2016), have shown equivalent accuracy to 32-bit
digital implementations even after reducing precision down to 2-bit (Choi et al., 2019).

3. The case for photonics for neuromorphic processors

In an artiﬁcial neural network, neurons are interconnected by synaptic weights (a
memory element). Signals from many neurons are weighted before being summed by
the receiving neuron. This many-to-one (N:1) connection is called fan-in, and the
weighted sum–a linear operation–is the dot product of the output from connected

5

neurons attenuated by a weight vector. A neuron then performs a nonlinear operation
on the weighted sum to implement a thresholding eﬀect which is output to many
neurons. This one-to-many (1:N) connection is called fan-out.

Electronic and photonic neuromorphic approaches to implement neural networks
face diﬀerent challenges. Physical laws that restrain electronics do not necessarily ap-
ply to optics. For high speed data transfer, compared to electronic wires, optical wave-
guides have a lower attenuation, have no inductance (minimal frequency-dependent
signal distortions), and photons hardly interact with other photons (unless intermedi-
ated by matter) which allows for wavelength multiplexing. While these fundamental
properties have proven to be advantageous in optical communications, they are also im-
portant for neural networks including interconnectivity (Goodman, Leonberger, Sun-
Yuan Kung, & Athale, 1984; Miller, 2000) (i.e. neuron-to-neuron, neurons-to-neurons
and neurons-to-memory communications), and parallel and linear processing i.e. mat-
rix multiplication (Miller, 2000). However, these same properties make it challenging
to implement nonlinear operations (Keyes, 1985). We refer the reader to the quant-
itative analyses comparing electronic and optical interconnects, and electronic and
photonic computing performed by Miller (Miller, 2009; Miller, 2000) and Nahmias et
al (de Lima et al., 2020; Nahmias et al., 2019), respectively. Here, we qualitatively
summarize these concepts comparing and contrasting optics and electronics.

Interconnects: Conventional electronic processors rely on point-to-point memory
processor communication and can take advantage of state-of-the-art transmission line
and active buﬀering techniques (Shastri et al., 2021). However, a neuromorphic pro-
cessor typically requires a large number of interconnects (i.e., hundreds of many-to-one
fan-in per processor) (Hasler & Marr, 2013) where it is not practical to use line and
active buﬀering techniques for each connection at high speed. This creates a commu-
nication burden which in turn, introduces fundamental performance challenges that
result from RC and radiative physics in electronic links, in addition to the typical
bandwidth-distance-energy limits of point-to-point connections (Miller, 2000). While
some electronic neuromorphic processors incorporate a dense mesh of wires overlay-
ing the semiconductor substrate as crossbar arrays, large-scale systems employ time-
multiplexing or packet switching, notably, address-event representation (AER). These
virtual interconnectivities allow electronic approaches to exceed wire density by a
factor related to the sacriﬁced bandwidth. As argued by Shastri et al. (2021), for
many applications, however, bandwidth and low latency are paramount, and these
applications can be met only by direct, non-digital photonic broadcast (that is, many-
to-many) interconnects.

Apart from bandwidth, power dissipation in interconnects is another major chal-
lenge in neuromorphic processors. Data movement has posed severe challenges to the
power consumption of today’s digital computers as parallel computing is widely de-
ployed. A large amount of data has to move among processors and memory units,
especially in those distributed programming models like neural networks. In these
models, most energy is lost in data movement (Jouppi et al., 2017b) due to capacitive
charge-discharge events at high frequencies. In contrast, optics is more energy eﬃcient
in data movement at high speeds.

Linear operations and weighting: Optical physics is very well suited for linear
operations. The electric ﬁeld or intensity of the light can also be used as an analog met-
ric for neuromorphic computations. The analog input in the electrical domain (voltage)
can be used to modulate components such as Mach Zehnder (MZ) modulators (MZMs)
or ring resonator (RR) modulators (RRMs). The weights can be implemented by scal-
ing the ﬁeld with MZ interferometers (MZIs) (Shen et al., 2017) or the intensity with

6

RRs (Tait, Jayatilleka, et al., 2018; Tait, Nahmias, Shastri, & Prucnal, 2014). Optical
signals can be added through wavelength-division multiplexing (WDM) by accumu-
lation of carriers in semiconductors (Tait et al., 2014), electronic currents (Bangari
et al., 2019; Shainline, Buckley, Mirin, & Nam, 2017) or changes in the crystal struc-
ture of a material induced by photons (Feldmann et al., 2019). Larger vector matrix
multiplications (VMM) can thus be implemented with such MZ (Shen et al., 2017) or
RR (Tait et al., 2014) based circuits. Such photonic neural networks, being analog in
nature, share the advantages and constraints of their analog electronic counterparts.
In addition, modulations can be done at 10s of GHz, addition can be done at the speed
of light in coherent MZI networks and at 10s of GHz in RR networks, and weight mul-
tiplication can be done at the speed of light, all of which provides much lower latency
than analog or digital electronic MAC operations.

Dynamic power consumption is also much lower, since multiplication does not con-
sume any energy and energy eﬃciency of the E/O modulation can be reduced to as
low as 10 fJ per operation (Nozaki et al., 2019). However, static power consump-
tion is dependent on laser wall plug eﬃciency, waveguide losses, energy consumed in
maintaining the weights, etc, and must be minimized. Assuming an estimated energy
consumption of > 100 fJ per operation for digital implementation (Lin et al., 2020),
analog IMC brings 2.1-7.8× reduction (Liu et al., 2020; Xue et al., 2019). Photonic
implementations can achieve energy consumption similar to IMC, but with orders of
magnitude reduction in latency. Like their IMC counterparts, photonic neural net-
works can operate up to 8 bits of precision (Bangari et al., 2019; Huang, Bilodeau, et
al., 2020; Ramey, 2020).

Nonlinear operations: The same properties that allow optoelectronic components
to excel at linear operations and interconnectivity are at odds with the requirements
of nonlinear operations for computing (Shastri et al., 2021). The implementation of
photonic neurons relies on the nonlinear response of optical devices. The approaches
fall into two major categories based on the physical representation of signals within
the neuron: optical-electrical-optical (O/E/O) vs. all-optical.

O/E/O neurons involve the transduction of optical power into electrical current and
back within the signal pathway. Their nonlinearities occur in the electronic domain
and in the E/O conversion stage using lasers or saturated modulators (Amin et al.,
2019; George et al., 2019; Tait et al., 2019). In the authors’ recent work (Tait et al.,
2019), neurons are implemented using silicon modulators that exploit the nonlinearity
of the electro-optic transfer function. Modulation mechanisms can change the real
and imaginary parts of the refractive index of the material and subsequently alters
the index (termed EO modulators) and loss (termed electro-absorptive modulators
(EAMs)) of the optical propagating mode. Research on high speed and power-eﬃcient
modulators are very active, with a general focus on maximizing the interaction between
the active material and the light. Such approaches include lithium niobate (LiNbO3)
modulators based on the Pockels eﬀect (C. Wang et al., 2018), III-V semiconductor-
based quantum-conﬁned Stark eﬀect modulators (Kuo et al., 2005), silicon modulators
based on plasma dispersion eﬀect (Dong et al., 2009; Q. Xu, Schmidt, Pradhan, &
Lipson, 2005), and hybrid modulators incorporating novel materials (such as ITO and
graphene) to silicon-based modulators (Amin et al., 2019, 2018; Komljenovic et al.,
2016; M. Liu et al., 2011). With the foundry-compatible silicon-on-insulator (SOI)
technology for on-chip integrated photonics, OEO neurons are demonstrated by Tait
et al. (2019) using a silicon microring resonator (MRR) with embedded PN modulator,
and Williamson et al. (2019) with a Mach-Zehnder type modulator.

All-optical neurons rely on the semiconductor carriers or optical susceptibility that

7

occur in many materials. A perceived advantage of all-optical neuron implementa-
tions is that they are inherently faster than O/E/O approaches due to relatively slow
carrier drift and/or current ﬂow stages in the latter. All-optical perceptron has been
demonstrated based on single-carrier optical nonlinearities, including through carrier
eﬀect in MRR (Huang et al., 2019; Jha, Huang, & Prucnal, 2020), changing a material
state (Chakraborty, Saha, Sengupta, & Roy, 2018; Feldmann et al., 2019), such as
via a structural phase transition, and saturable absorbers and quantum assemblies
heterogeneously integrated in PICs (Miscuglio et al., 2018).

Memory: As dicussed in Section 2, nonvolatile materials allow implementing
memory elements directly on the computing devices for IMC. On-chip nonvolatile
memories that can be written, erased, and accessed optically are rapidly bridging a
gap toward on-chip photonic computing (R´ıos et al., 2019); however, they cannot usu-
ally be written to and read from at high frequencies. As described by Shastri et al.
(2021), future scalable neuromorphic photonic processors will need to have a tight
co-integration of electronics with potentially hybrid electronic and optical memory
architectures, and take advantage of the memory type (volatile versus non-volatile) in
either digital or analog domains depending on the application and the computation
been performed.

Current approaches with photonic neural networks are driven by electronic circuits
or micro-controllers to load matrices. The integration and packaging of large-scale op-
tical and electronic circuits can be challenging in terms of cost and power. In some ma-
chine learning applications (such as deep learning inference) the weights, once trained,
do not have to be updated often or at all. In these cases, the integration of novel
photonic memory technologies can limit the need to read from and write to electronic
memories with DACs and ADCs. All-optical memories have been demonstrated us-
ing various optical components such as nonlinear switches Dorren, Lenstra, Liu, Hill,
and Khoe (2003), MZIs (Hill, De Waardt, Khoe, & Dorren, 2001b), laser diodes (Hill,
De Waardt, Khoe, & Dorren, 2001a), semiconductor optical ampliﬁers (SOAs), band-
pass ﬁlters (BPFs) and isolators (J. Wang et al., 2008). The access time of an optical
memory cell is small and attractive for photonic neural networks (Alexoudi, Kanellos,
& Pleros, 2020). Non-volatile photonic memories with phase-change materials (PCMs)
set and retain the weights without further holding power after being set (Feldmann et
al., 2019), resulting in almost zero power consumption in performing matrix multiplica-
tion operations. Here a crystalline phase transition (from amorphous to crystalline and
back) constitutes reversible (WRITE & RESET) memory programmability, therefore
enabling dynamic synaptic plasticity and online learning in photonic neural networks.
For online training of the photonic neural network, a high-speed WRITE would be
ideal, and experimentally MHz speeds are possible for known PCMs such as GST or
GST alloys, just limited by the heat-capacitance of these photonic RAM. The READ
speed, interestingly, is on the order of ps, and simply given by the time-of-ﬂight of the
signal photon through such photonic RAM. However,signiﬁcant improvements must
be made in reducing the energy consumption (taking into account optical losses) and
size, reliability and ease of interfacing for their practical deployment in photonic neural
networks. The current prominent material GST has prohibitively high optical losses
even for the lower-loss amorphous state with an extinction coeﬃcient (kappa = 0.2),
thus leading to high insertion loss. Future research should focus on low-optical loss
solutions. Emerging P-RAMs would also eliminate the long-standing ‘memory access
bottleneck’ known from electronics, and a successful P-RAM integration constitutes
a similar shift in optics from centralized to decentralized computing, known as in-
memory computing, one of the very active research ﬁelds left in circuit design, after

8

transistor scaling stopped.

4. Architectures of neuromorphic photonic processors

Neuromorphic processors and photonic neural networks alike require a number of fun-
damental building blocks; i) synaptic MAC operation, ii) nonlinear activation function
(NLAF); iii) state-retention, i.e. memory, iv) data input/output (I/O) ports and, de-
pending on the applications, data domain crossings such as v) photonic to electronic
and/or vi) analog-to-digital, with the latter including DAC and the ADC counterparts.
However, a generic architecture of a current state-of-art photonic neural network sys-
tem is given in Figure. 1

Figure 1. Generic system schematic of an analog photonic neural network or photonic tensor core accelerator.
The digital-to-analog domain crossings are power costly and ideally a photonic DAC without leaving the
optical domain, e.g. Ref (Meng et al., 2019) would be used instead, thus reducing system complexity and hence
allowing for extended scaling laws. The optical processor itself performs the MAC operation such as for VMM,
convolutions, or of a perceptron, but could also include nonlinearity (neuron thresholding) for higher biological
plausibility (e.g. spiking neurons where event-driven scheme play a role)(Jha et al., 2020; Nahmias et al., 2020;
Peng et al., 2018; Tait et al., 2019).

Photonic synaptic MAC operations and VMMs: The most common form
of photonic neural networks focuses on accelerating the mathematical computation
of multiplications using optics; this is not surprising, since photonic programmable
circuits allow for a non-iterative mathematical multiplication by simple preparing the
state of the programmable element followed by sending the optical signal through this
element. Then, the multiplication is performed ‘on-the-ﬂy’ at pico-second delays in
PICs enabling a notion of ‘real-time’ (i.e. zero-delay) computing, which is incidentally
not to be confused with computing schemes that ‘expect’ a system to complete a task
at a pre-determined and deterministic time. Options for VMM are plenty, but the most
common ones are a mesh network of cascaded MZIs, or MRR ﬁlters, or a photonic
tensor core (PTC) processor (Miscuglio, Hu, et al., 2020; Shen et al., 2017; Tait et
al., 2017). Accelerating VMMs is a worthy aim, since the mathematical computational

9

complexity of VMMs scales with N3, where N is the matrix size (assuming a square
matrix). However, the mathematical complexity is technically speaking irrelevant (at
ﬁrst order), since for hardware implementations considered here, the runtime com-
plexity is actually of interest. And it is this runtime complexity that is non-iterative
in analog photonic neural networks (assuming the neuron’s weights are set, i.e. pro-
grammed).

NLAF, or threshold and training neural networks: Depending on the under-
lying algorithm model of the neural network, the NLAF can be as simple as a step-
function such as used in binary classiﬁcation problems, or a more elaborate function
such as an sigmoidal, tangent hyperbolic, or population growth functions. However,
when performing neural network training using gradient descent (GD) backpropaga-
tion (BP) methods, the problem of vanishing (or exploding) gradients can occur, where
during each training cycle (called epoch) the available gradient on which a diﬀer-
entiation is performed is becoming consecutively smaller to the point of noise-level
dominated and training would seize. To prevent this, rectifying linear units (ReLU)
or Gaussian error linear units (GELU) could be used instead. In a ReLU, for ex-
ample, the output is ‘zero’ up until a certain input level, upon which, the output
is simply a linear function. This linearity ensures a constant and non-zero gradient
during GD BG training cycles. Since diﬀerentiation at a step is mathematically not
deﬁned, a Soft-ReLU is often used instead to ensure continued loss-function minimiz-
ation and neural network performance improvement with training. Note, the known
problem of overﬁtting neural networks does also apply to photonic neural networks.
However, pruning techniques and hyperparameter adjustments during training are
an known and interesting method (yet time consuming) to improve artiﬁcial neural
network performance. Interestingly, for the context of photonic neural networks, the
network interconnectivity sparseness created in pruning steps, saves fan-out (fan-in)
connections as well as waveguide connections. This is a blessing, since the bulkiness
of photonic components (in contrast to electronic counterparts on a per-unit basis)
increases functional density and thus performance per unit chip footprint. Indeed,
future work should explore pruning techniques further for photonic neural networks.
An initial study was, for instance, performed on the MZI NxN mesh network that
was originally developed by the MIT groups (Shen et al., 2017), where it was shown
that the number of required MZIs can be reduced from a brute-force N×N (Gu et al.,
2020). Incidentally, in performance-optimized DNNs, the speciﬁc shape of the NLAF
should be adjusted with layer depth; for instance, a ReLU that is not saturating is
more useful in the upstream layers, while a saturating sigmoidal function supports a
‘decision-making’ process at the fully-connected (FC) layer at the output in classi-
ﬁcation problems. In most neural networks demonstrations, the primary performance
driver is power eﬃciency, while speed is a secondary consideration. Therefore, early
photonic neural network demonstrations implement NLAF in the digital electronic
domain, limiting the speed of neural network to the clock speed (hundreds of MHz to
a few GHz). Nevertheless, optical NLAF, as discussed in Section 3 oﬀer a unmatched
speed over ten GHz in performing NLAF, thus becoming essential elements in order to
solve many compelling applications that requires online (i.e., real-time) learning and
inference or for neural networks with gigahertz bandwidths.

Memories: The memory is usually implemented electronically and various choices
exist. Static RAM (SRAM) and dynamic RAM (DRAM) are used in neural network
architectures to store inputs, weights, training parameters and look up table (LUT)
values. At the cell level, SRAM typically uses 6 transistors (6T) to store a single bit,
whereas DRAMs use a single transistor and a single capacitor (1T1C). SRAMs are

10

indispensable for neural network implementations given their faster access time; in
addition, they can be leveraged for data reuse to reduce data fetch energy (Sze, Chen,
Yang, & Emer, 2017). DRAM with its larger storage capacity is used to store all
activations and training weights. However, owing to oﬀ-chip implementation, DRAM
is slower with higher energy consumption than SRAM. Depending on the size of an
SRAM and its location, the energy for read and write can scale from sub-pJ to 10
pJ per byte for SRAMs (Gudaparthi et al., 2019; Horowitz, 2014). With an estimated
SRAM size of 64KB used for DNN, the corresponding read and write latencies are
∼1 ns (Imani, Patil, & Rosing, 2016), which can be projected to reduce to 0.25 ns in
7-nm CMOS. Given the size diﬀerence and oﬀ-chip implementation in comparison to
SRAMs, oﬀ-chip DRAMs consume more than two orders of magnitude of energy for
data fetch (Sze et al., 2017). High bandwidth memories (HBM) are used to reduce
the energy consumption of DRAM’s data fetch (Tran, 2016), by moving DRAM mod-
ules closer to the chip. Alternatives to DRAM include resistive memories and PCMs.
Utilizing 1T1R in ReRAM crossbar topology was demonstrated with low energy con-
sumption and write latency (Xiao, Bennett, Feinberg, Agarwal, & Marinella, 2020).
On the other hand, using PCMs as unit cells requires high energy to write, and has
poor resolution and linearity (Mukherjee, Saurav, Nair, Shekhar, & Lis, 2021).

An analog memory cell made up of a capacitor has drawbacks such as need for
ADC/DACs to interact with digital cells, low density compared to SRAM cell, need
for refresh, and sensitivity to noise and crosstalk. However, for photonic computing,
placing a capacitor memory cell next to photonic elements is attractive. This is because
the computing is analog in nature, and photonic compute elements are much bigger
than digital compute elements, so the large size of the capacitor memory cell in com-
parison to SRAM is not a major concern. Such an architecture can eliminate the data
movement bottleneck, signiﬁcantly reducing the access time and energy consumption.
Monolithic photonic processes supporting metal capacitors are ideally suited for such
implementations.

For most applications, training the neural network is time consuming, spanning
hours to days or even weeks. Thus, it is realistic to assume that for most applications,
the neurons’ weights or the kernel of a PTC is ﬁxed and does not change often in time.
For this reason, a non-volatile solution that retains information-of-state (i.e. memory),
is of high interest. If achieved, a (near) zero static power consumption can be achieved
in photonic neural networks, allowing them to be rather eﬃcient. Note, ‘static’ refers
here to the MAC operation and not to possible signal modulation, which would be
considered part of the I/O of the system. Fortunately, such state-retention is recently
achieved in electro-thermal programmable PCM.

Data I/Os: Photonic accelerators such as photonic neural networks and PTCs
allow for high-throughputs approaching P-OPS (peta operations per second). Such
a photonic ‘highway’, while promising, may not demonstrate its full potential, if the
to-be-processed input data is not provided at a suﬃciently high data rate to the
optical accelerator. This can be assured in two ways; either the I/O data bandwidth is
suﬃciently high such as provided by a FPGA or the data is already prevailing in the
optical domain (such as of an optical aperture from a camera system, for example).
The latter is elegant, since it not only eliminates the needs to drive power-costly EO
modulators for signal encoding, but more importantly eliminates the requirements for
DACs/ADCs (see next paragraph). Indeed, high-speed DACs would consume about a
third of the total photonic neural network system’s power. For the case of optical data
as the input, some PTCs show a dramatic power drop from about 80W down to 2W
when DACs are not needed (Miscuglio & Sorger, 2020).

11

Domain crossings: digital/analog domain crossings: DACs and ADCs are
required to interface a photonic neural network with digital signal processing (DSP)
units (typically back-end) or when receiving data input data digitally such as from a
server/computer etc. High sampling rate DACs used to drive input modulators mostly
utilize current steering schemes and dissipate >5.5 pJ of energy for 6b-8b of conversion
resolution (Kim, Cho, Kim, & Kwon, 2010; Sedighi, Khafaji, & Scheytt, 2011). The
contribution of such converters to the overall energy eﬃciency of the neural network is
reduced by 1/N as the network is scaled with N. Given the reusability of the weights
over a given batch size, low speed capacitive DACs can be used. These DACs are
usually adopted in SAR architectures and typically contribute to most of the ADCs’
energy. Charge average switching, merge and split, charge recycling, and common
mode voltage (Vcm) based charge recovery are some of the techniques used to reduce
the DAC capacitance and consequently its switching energy (Lin & Hsieh, 2015; Liou
& Hsieh, 2013; Zhu et al., 2010). In the Vcm-based scheme, the diﬀerential DAC arrays
are connected to a common mode voltage Vcm which reduces the DAC’s switching
energy. The power consumption of high speed ADCs scales exponentially with the
resolution and linearly with the conversion rate (Murmann, n.d.). For photonic neural
network applications where the sampling frequency of the analog frontend is between
1-10 GS/s, and the resolution requirement is < 8b, the energy consumption can be
estimated as ∼1 pJ for state of the art ADCs (Murmann, n.d.). The architecture for
ADC depends upon the photonic neural network. In recurrent neural network and long
short-term memory (LSTM) networks, or in training back-propagation, the ADC is
in a feedback network. Hence, a low-latency ADC architecture must be chosen. The
lowest achievable latency is achieved in Flash ADCs, approximately ∼100 ps (with
an estimated delay of 80 ps for dynamic comparators and 30 ps for the encoding
gates). Therefore, latency-dependent photonic neural networks are limited to ∼10
GS/s of operation. However, high-speed Flash ADCs also consume large power. On
the other hand, neural network architectures such as convolutional neural networks
which do not rely on feedback relax the low latency constraints for the ADC. In such
implementations, pipeline and time interleaved SAR ADCs are better suited given
their low energy consumption and high conversion rate.

5. Applications of photonic neural networks

Integrated optical neural networks will be smaller (hundreds of neurons) than elec-
tronic implementations (tens of millions of neurons). But the bandwidth and inter-
connect density in optics is signiﬁcantly superior to that in electronics. This raises a
question: what are the applications where sub-nanosecond latencies and energy eﬃ-
ciency trump the sheer size of processor? These may include applications 1) where the
same task needs to be done over and over and needs to be done quickly; 2) where the
signals to be processed are already in the analog domain (optical, wireless); and 3)
where the same hardware can be used in a reconﬁgurable way. This section will discuss
potential applications of photonic neural networks in computing, communication, and
signal processing.

12

5.1. High-speed and low-latency signal processing for ﬁber optical

communications and wireless communications

5.1.1. Fiber nonlinearity compensation

The world is witnessing an explosion of internet traﬃc. The global internet traﬃc
has reached 5.3 exabytes per day in 2020 and will continue doubling approximately
every 18 months. Innovations in ﬁber communication technologies are required to sus-
tain the long-term exponential growth of data traﬃc (Cisco, n.d.). Increasing data
rate and system crosstalk has imposed signiﬁcant challenges on the DSP chips in
terms of analog-to-digital converters (ADCs) performances, circuit complexity, and
power consumption. A key to advancing the deployment of DSP relies on the consist-
ent improvement in CMOS technology (Pillai et al., 2014). However, the exponential
hardware scaling of ASIC based DSP chips, which is embodied in Moore’s law as other
digital electronic hardware, is fundamentally unsustainable. In parallel, many eﬀorts
are focused on developing new DSP algorithms to minimize computational complexity,
but usually at the expense of reducing transmission link performances (Agrell et al.,
2016).

Instead of embracing such a complexity-performance trade-oﬀ, an alternative ap-
proach is to explore new hardware platforms that intrinsically oﬀer high bandwidth,
speed, and low power consumption (Argyris, Bueno, & Fischer, 2018a; De Lima et al.,
2019; Huang, Fujisawa, et al., 2020). Machine learning algorithms, especially neural
networks, have been found eﬀective in performing many functions in optical networks,
including dispersion and nonlinearity impairments compensation, channel equaliza-
tion, optical performance monitoring, traﬃc prediction, etc (Khan et al., 2019).

PNNs are well suited for optical communications because the optical signals are
processed directly in the optical domain. This innovation avoids prohibitive energy
consumption overhead and speed reduction in ADCs, especially in data center ap-
plications. In parallel, many PNN approaches are inspired by optical communication
systems, making PNNs naturally suitable for processing optical communication sig-
nals. For example, we proposed synaptic weights and neuron networking architecture
based on the concept of WDM to enable fan-in and weighted addition (Tait et al.,
2014). This architecture can provide a seamless interface between PNNs and WDM
systems, which can be applied as a front-end processor to address inter-wavelength or
inter-mode crosstalks problems that DSP usually lacks the bandwidth or computing
power to process (e.g., ﬁber nonlinearity compensation in WDM systems). Moreover,
PNNs combine high-quality waveguides and photonic devices that have been initially
developed for telecommunications. Therefore, PNNs, by default, can support ﬁber op-
tic communication rates and enables real-time processing. For example, The a scalable
silicon PNN proposed by the authors is composed of microring resonator (MRR) banks
for synaptic weighting and O/E/O neurons to produce standard machine learning ac-
tivation functions. The MRR weight bank is inspired by WDM ﬁlters, and the O/E/O
neurons use typical silicon photodetector and modulator. Therefore, the optimization
of associated devices in PNNs can utilize the fruits of the entire silicon photonic ecosys-
tem that is paramountly driven by telecommunications and data center applications.
In order to truly demonstrate photonics can excel over DSP, careful considerations
are required to identify diﬀerent application scenarios (i.e., long-haul, short-reach)
and system requirements (i.e., performances, energy). Continuous research is needed
to improve photonic hardware and to develop hardware-compatible algorithms. Here,
we discuss several approaches to train and apply PNNs for optical communications.

13

Neuromorphic approach Long-haul communication systems prioritize high per-
formances in terms of distance reach and spectral eﬃciency. This requirement allows
the use of coherent technology, along with dense wavelength multiplexing and polar-
ization multiplexing schemes, to maximize the ﬁber capacity. In long-haul ﬁber optic
transmission systems, ﬁber nonlinearity remains a challenge to the achievable capacity
and transmission distance. One reason is that the nonlinear interplay between signal,
noises, and optical ﬁbers negates the accuracy of conventional nonlinear compensation
algorithms based on digital backpropagation. Another reason is, the implementation of
most nonlinear compensation algorithms in DSP chips demands excessive resources.
In contrast, the neural network approach can learn and approximate the nonlinear
perturbation from the abundant training data, rather than solely relying on the phys-
ical ﬁber model (known as stochastic nonlinear Schrodinger equation). Based on the
perturbation methods, the derived neural network algorithm has enabled compens-
ating the nonlinear distortion in a 10800 km ﬁber transmission link with 32 Gbaud
signals (Zhang et al., 2019). Tait et al. (2017) developed a photonic neural network
platform based on the so-called ”neuromorphic” approach, aiming to map physical
models of optoelectronic systems to abstract models of neural networks (which dif-
fers from the reservoir approaches discussed next). By doing so, the photonic neural
network system can leverage existing machine learning algorithms (i.e., backpropaga-
tion) and map training results from simulations to heterogeneous photonic hardware.
The concept is shown in Figure 2. A proof-of-concept experiment demonstrates the
real-time implementation of a trained neural network model using an integrated silicon
photonic neural network chip (Huang, Fujisawa, et al., 2020). In this work, the authors
experimentally demonstarted that the silicon photonic neural network can produce a
similar Q factor improvement compared to the simulated neural network for nonlinear
compensation as shown in Figure 2, but it promises to process the communication
data in real-time and with high bandwidth and low latency.

We also proposed a photonic architecture enabling all-to-all continuous-time re-
current neural networks (RNN) (Tait et al., 2017). Recurrent neural networks can
resemble optical ﬁber transmission systems: the linear neuron-to-neuron connections
with internal feedback is analog to linear multiple-input multiple-output (MIMO)
ﬁber channel with dispersive memory. With neuron nonlinearity, RNNs can be ideally
used to approximate all types of linear and nonlinear eﬀects in a ﬁber transmission
system and compensate for diﬀerent transmission impairments. RNNs, consisting of
many feedback connections, are considered to be computationally expensive for digital
hardware and require at least milliseconds to conduct a single inference. Contrarily,
in photonic RNN, the feedback operations are simply done by busing the signals on
photonic waveguides, allowing photonic hardware capable of converging to the solu-
tion within microseconds. This architecture also adopts the neuromorphic approach
and thus allows to train PNNs externally using standard machine learning algorithms.

5.1.2. Channel and/or predistortion equalization

Reservoir computing Short-reach ﬁber-optic communication systems (FOCS) have
recently seen increasing demand driven largely by the proliferation of cloud-based
computing architectures and fronthauling in cloud radio access networks (C-RAN).
DSP-based coherent transceivers are optimized for reach and capacity and gener-
ally considered commercially unviable for short-reach optical ﬁber links due to their
high cost, footprint, and latency. Legacy systems employing intensity modulation
and direct detection (IM/DD) with limited signal processing capabilities can provide

14

Figure 2.
(left) Concept of training and implementing photonic neural networks. Inset shows a transfer
function of the photonic neural network measured with real-time signals. (right) Constellations of X-polarization
of a 32 Gbaud PM-16QAM, with the ANN-NLC gain of 0.57 dB in Q-factor and with the PNN-NLC gain of
0.51 dB in Q-factor. Huang, Fujisawa, et al. (2020)

low-cost solutions for inter-datacenter applications, however they cannot scale with
the ever-increasing capacity requirements in modern communication networks. This
has motivated renewed research interest in low-cost and low-complexity transceivers
with bitrates optimized over short transmission distances (10-100 km) where channel
impairments are dominated by dispersion with some nonlinear distortion (Chagnon,
2019).

Photonic neural networks based on reservoir computing techniques have shown
promising results for channel equalization in ﬁber-optic links. Reservoir computers
(RC) are a class of recurrent neural networks that consist of a reservoir of sparsely
connected neurons with randomized ﬁxed weights. Contrary to feed-forward recurrent
neural networks which are trained using backpropagation or Hessian-free optimiza-
tion, reservoir computers only require the output weights to be trained, which can be
achieved by linear regression. Optical RCs have attracted signiﬁcant research interest
as the reservoir can be realized by a single nonlinear element with a delayed feedback
loop (Appeltant et al., 2011) which has size- and cost-eﬃcient implementations in
photonic circuits. The ﬁrst demonstrations of this technology for signal equalization
tasks in FOCS used a semiconductor laser as a nonlinear element with a ﬁber delayed-
feedback line and showed results competitive with DSP-based techniques (Argyris
et al., 2018a). This approach is illustrated in Figure 3. Real-time operation of this
photonic RC faces challenges, however, as the input layer is time-multiplexed by elec-
tronically masking each bit before injection into the reservoir, which incurs a speed
penalty as the bit time must be stretched to match the delay of the feedback loop
(Sorokina, Sergeyev, & Turitsyn, 2019). An all-optical implementation of a dual quad-
rature RC has also been proposed that could enable high bandwidth signal processing
for coherent optical receivers (Sorokina et al., 2019). In this design, nonlinear trans-
formation of the input signal is achieved through the Kerr eﬀect in a highly nonlinear
ﬁber with a modulated pump to select the desired signal quadrature. Optoelectronic
approaches have also been considered, with optical pre-processing via spectral slicing
to improve the dynamics of a digital reservoir computer. This architecture addresses
the system losses incurred in all-optical RCs and achieves signiﬁcant reach extension
compared to legacy IM/DD systems at the cost of higher complexity as the number
of photodetectors scales linearly with the number of spectral slices (Da Ros, Ranzini,
B¨ulow, & Zibar, 2020). Despite the SNR penalties, photonic RCs have merit over con-

15

ventional DSP-based linear techniques when there is signiﬁcant nonlinear distortion
in the channel. Increasing the launch power at the transmitter may oﬀset the system
losses to achieve a higher OSNR with improved performance in the presence of non-
linear impairments compared to linear equalizers. This hypothesis is supported by (Li
& Pachnicke, 2020) which compared the BER vs OSNR trade-oﬀ for a state-of-the-art
DSP based equalizer against a photonic RC in a 100 km 56 Gbd DWDM transmis-
sion system. As expected, the DSP equalizer is the better choice at low OSNR values,
however the results show the RC-based equalization outperforms the digital receiver
at high OSNR where the nonlinear perturbations are strong.

Figure 3.
(a) Generic model of a reservoir computer (b) Illustration of the RC equalization scheme in Argyris
et al. (2018b) based on a single nonlinear node with time delayed feedback. The input ai is a vector of N samples
representing a single bit or symbol. The mask vector m , which deﬁnes the input weights, is multiplied by the
elements in ai and injected sequentially into the reservoir. The output is a linear combination of the virtual
nodes in the reservoir with weights optimized to produce an estimate bi of the bit or symbol value.

5.1.3. Jamming avoidance response

The dramatic increased demand in mobile RF systems has signiﬁcantly worsened the
spectral scarcity issue, the overcrowded RF spectrum increases the likelihood of in-
advertent jamming (Poisel, 2011; Wilhelm, Martinovic, Schmitt, & Lenders, 2011).
Inadvertent jamming is one type of jamming that comes from a friendly source, that
is usually aimless and unforeseeable. However, inadvertent jamming could easily cor-
rupt the transmission channel if not being mitigated properly. In fact, inadvertent
jamming does not only happen in our communication systems. Eigenmannia, a genus
of glass knifeﬁshes uses electric discharge to communicate with their own species and
to recognize diﬀerent species. Since Eigenmannia does not have FCC to regulate their
frequency allocation, their frequency usage is dynamic. To avoid jamming, the Ei-
genmannia has an eﬀective Jamming Avoidance Response (JAR) that helps the ﬁsh
to identify potential jamming and automatically move their electric discharge fre-
quency away from the potential jamming frequency (Bullock, Hamstra, & Scheich,
1972; Scheich, 1977). The JAR in Eigenmannia mainly consists of four functional
blocks (Figure 4(a), they are (i) Zero-crossing point detection unit, (ii) Phase unit,
(iii) Amplitude unit, and (iv) Logic unit.

First, the ZeroX unit identiﬁes the positive zero crossing points in the reference
signal. Then, phase comparison between the reference signal and the beat signal takes
place at the Phase unit. Amplitude unit takes the envelope of the beat signal and marks
the rising and falling amplitudes diﬀerently. Lastly, the Logic unit takes the phase and
amplitude information obtained and determines if the electric discharge frequency
should be increased or decreased to avoid jamming, and if there is no potential jamming
threat.

This powerful JAR can be implemented using photonics to allow the JAR to be used

16

in our communication frequency range (Fok & Toole, 2018; R. Lin et al., 2018; Toole
& Fok, 2016). The major device to achieve JAR is a semiconductor optical ampliﬁer
(SOA), where self-phase modulation is used in the Zero-crossing point detection unit,
cross-gain modulation is used in both the Phase unit and amplitude units (Fok & Toole,
2018; R. Lin et al., 2018). As the jamming frequency is moving closer to the jamming
range, the JAR will be activated and move the emitting frequency away gradually until
it is out of the jamming range, as illustrated in the spectral waterfall measurement in
Figure 4(b). The photonic JAR works well for frequencies from hundreds of MHz to
tens of GHz, that provides an adaptive and intelligent solution to inadvertent jamming
in emerging communication systems.

Figure 4.
(a) Illustration of the JAR design and the four functional units. (b) Spectral waterfall measurement
of the photonic JAR in action with sinusoidal reference signal fR and jamming signals fJ = 150 MHz. (i) fJ
is approaching fR from the low frequency side and triggers the JAR, (ii) fJ is approaching fR from the low
frequency side and triggers the JAR, and then is moved away.

5.1.4. Multivariate photonics - PCA, BSS, MIMO

Multi-antenna systems provide an orthogonal dimension with which to share the elec-
tromagnetic spectrum. In many cases, interference can originate from sources that are
either broad spectrum or frequency hopping. They cannot be eliminated by changing
center frequency or ﬁltering frequency. This interference can be separated by analyz-
ing correlations in signals received by diﬀerent antennas in an array. Photonic devices
excel at processing radio-frequency (RF) signals. The radio signal is multiplexed onto
an optical carrier that eﬀectively serves as a much higher intermediate frequency, as
shown in Figure 5. The resulting signal has a small fractional bandwidth, even for
radio signals spanning 10s of GHz, which means that linear processing elements have
low dispersion across the band. Once processed in the optical domain, the signal is
converted back to RF by photodetection.

5.1.5. Compressive sensing

Compressive sensing is a paradigm for signal acquisition that exploits the inherent
sparsity in many natural and man-made signals to enable sampling at rates far below
the Nyquist frequency with limited or no loss of information. CS theory relies on linear
dimensionality reduction to produce an eﬃcient representation of an input signal by
projecting it onto a rank-deﬁcient sensing matrix. Provided that the input signal is
sparse under some basis and the sensing matrix satisﬁes the requisite properties for
sparse signal recovery, the input signal can be reconstructed by linear programming
with polynomial complexity (M. F. Duarte & Eldar, 2011). Compressive sensing has

17

Figure 5.
(Comparison of multi-antenna radio front-ends followed by dimensionality reduction. a) Dimen-
sionality reduction with electronic DSP in which each antenna requires an ADC. b) Dimensionality reduction
in the analog domain (a.k.a physical layer) in which only one ADC is required. A photonic implementation of
weighted addition is pictured, consisting of electrooptic modulation, WDM ﬁltering, and photodetection. From
Tait, De Lima, et al. (2018)

motivated the design of converters that can sense a wideband spectrum at sub-Nyquist
sampling rates by performing the linear projections in the analog domain before digit-
ization. The analog front end can be implemented by modulating the input signal with
one or more periodic chip sequences followed by a low pass ﬁlter. This approach relaxes
the bandwidth requirements of the ADC, however the RF front end must still operate
at the Nyquist frequency to avoid aliasing. When the observation bandwidth is large,
non-idealities in the chip sequences such as timing oﬀset and insuﬃcient slew rate
can cause measurement errors similar to the clock jitter and aperture error that can
occur in high speed ADCs (Abari, Lim, Chen, & Stojanovi´c, 2013). Several photonic
implementations of a compressive sensing receiver have been proposed as a solution
to the bandwidth limitations of analog electronics (Shmel & Pace, 2017; S. Wang,
Wu, Sun, & Chen, 2019). The high bandwidth oﬀered by photonic pseudo random
bit sequence generators and modulators enables analog domain compression of input
signals with extremely wide bandwidths of 40 GHz or more that would be infeasible or
impossible with electronic hardware. Furthermore, wavelength division multiplexing
can be used to dynamically conﬁgure parallel sensing channels that share the same
waveguide and modulator, which may oﬀer cost and size beneﬁts over electronic im-
plementations(Shaver & Nichols, 2016).

5.2. AI/Machine learning

5.2.1. Vector-matrix multipliers

With an ongoing trend in computing hardware towards increased heterogeneity,
domain-speciﬁc coprocessors are emerging as alternatives to centralized paradigms.
The tensor core unit has shown to outperform graphic process units by almost 3-orders
of magnitude enabled by higher signal throughout and energy eﬃciency. In this con-
text, photons bear several synergistic physical properties while PCMs allow for local
nonvolatile mnemonic functionality in these emerging distributed non von-Neumann
architectures. While several photonic neural network designs have been explored, a
photonic tensor core to perform matrix vector multiplication and summation is yet to
be implemented. An integrated photonics-based tensor core unit can be designed by

18

strategically utilizing i) a photonic parallelism via wavelength division multiplexing,
ii) high 2 Peta-operations-per second throughputs enabled by 10s of picosecond-short
delays from optoelectronics and compact photonic integrated circuitry, and iii) near-
zero power-consuming novel photonic multi-state memories based on PCMs featuring
vanishing losses in the amorphous state. Combining these physical synergies of ma-
terial, function, and system, we show, supported by numerical simulations, that the
performance of this 4-bit photonic tensor core unit can be one order of magnitude
higher for electrical data, whilst the full potential of this photonic tensor processor
is delivered for optical data being processed, where we ﬁnd a 2-3 orders higher per-
formance (operations per joule) as compared to an electrical tensor core unit whilst
featuring similar chip areas. This work shows that photonic specialized processors
have the potential to augment electronic systems and may perform exceptionally well
in network-edge devices in the looming 5G networks and beyond. (Figure. 6)

Figure 6. Photonic Tensor Core (PTC) featuring POPS throughput and 10s ps-short latency to process
VMM in PICs. (left) schematic of a modular exemplary PTC composed of 4×4 photonic dot-product engines
using RRs for WDM. Data entered via (exemplary) EO modulators at 50 Gbps are MUXed then dropped at
passive RRs. Being ’passive’ for the RR is importantly beneﬁcial since it allows for reduced complexity, less
real-estate used on the chip, and does not consume power unlike thermally-tuned approaches to perform the
optical multiplication. The latter can be elegantly executed using nonvolatile programmable P-RAM elements
using non-GST with a very low optical loss in the amorphous state (Miscuglio, Meng, et al., 2020)

5.2.2. Convolutions (inference accelerator)

Convolutions of an image with a ﬁlter, i.e. the convolution kernel, are among the
most heavily employed operations in computational imaging. During convolution, a
kernel slides across an image and at each position the overlap integral between image
and kernel provides the convolution’s value. This technique is extremely eﬃcient in
identifying entire objects (Ambs, 2010) or only local features. It therefore comes at not
too much of a surprise that functional topologies implementing convolutions similar to
Gabor ﬁlters have been identiﬁed in the visual cortex of mammals (Olshausen & Field,
1996). Convolutional neural networks (CNNs) leverage these concepts in a modern
information processing context, and an image classiﬁcation performance superior to
humans makes CNNs relevant for a wide range of applications (LeCun et al., 2015).

Free-space optical convolution: A fundamental aspect of computational ima-
ging is that the primary information is distributed in two dimensions (2D). Free-space
optical convolution setups inherently respect this encoding principle. Task-speciﬁc ac-

19

celerators based on free-space optics bear fundamental homomorphism for massively
parallel and real-time information processing given the wave-nature of light. How-
ever, initial results are frustrated by data handling challenges and slow optical pro-
grammability. Sorger’s group recently introduced an amplitude-only Fourier-optical
processor paradigm capable of processing large-scale ∼(1000 × 1000) matrices in a
single time-step and 100 microsecond short latency (Miscuglio, Hu, et al., 2020) (Fig-
ure 7). Conceptually, the information-ﬂow direction is orthogonal to the two dimen-
sional programmable-network, which leverages 106 -parallel channels of display tech-
nology, and enables a prototype demonstration performing convolutions as pixel-wise
multiplications in the Fourier domain reaching peta operations per second through-
puts. The required real-to-Fourier domain transformations are performed passively by
optical lenses at zero-static power. We exemplary realize a convolutional neural net-
work (CNN) performing classiﬁcation tasks on 2-Megapixel large matrices at 10 kHz
rates, which latency outperforms current GPU and phase-based display technology
by one and two orders of magnitude, respectively. Training this optical convolutional
layer on image classiﬁcation tasks and utilizing it in a hybrid optical-electronic CNN,
shows classiﬁcation accuracy of 98% (MNIST) and 54% (CIFAR-10). Interestingly,
the amplitude-only CNN is inherently robust against coherence noise in contrast to
phase-based paradigms and features an over 2 orders of magnitude lower delay than
liquid crystal-based systems. Beyond contributing to novel accelerator technology, sci-
entiﬁcally this amplitude-only massively-parallel optical compute-paradigm can be far-
reaching as it de-validates the assumption that phase information outweighs amplitude
in optical processors for machine-intelligence, such as for information processing at the
network-edge, in data centers, or for pre-processing information or ﬁltering towards
near real-time decision making.

Figure 7. Example of an Optical Convolutional Neural Network (CNN) accelerator exploiting the massive
(106 parallel channel) parallelism of free-space optics. The convolutional ﬁltering is executed as point-wise dot-
product multiplication in the Fourier domain. The conversion into and out-of the Fourier domain is performed
elegantly and completely passively at zero power While SLM-based systems can perform such Fourier ﬁltering
in the frequency domain, the slow update rates does not allow them to outperform GPUs. However, replacing
SLMs with fast 10s kHz programmable digital micromirror display (DMD) units, gives such optician CNNs
an edge over the top-performing GPUs. Interestingly, the lack-of-phase information of these amplitude-only
DMD-based optical CNNs can be accounted for during the NN training process. For details refer to Miscuglio,
Hu, et al. (2020)

2D integrated optical convolution : Deep neural networks are based on CNNs
which are powerful and highly ubiquitous tools for extracting features from large data-
sets for applications such as computer vision and natural language processing. The suc-
cess of CNNs for large-scale image recognition has stimulated research in developing

20

faster and more accurate algorithms for their use. However, CNNs are computation-
ally intensive and therefore results in long processing latency. One of the primary
bottlenecks is computing the matrix multiplication required for forward propagation.
In fact, over 80% of the total processing time is spent on the convolution (Li, Zhang,
Huang, Wang, & Zheng, 2016). Therefore, techniques that improve the eﬃciency of
even forward-only propagation are in high demand and researched extensively (Good-
fellow, Bengio, & Courville, 2016; Jaderberg, Vedaldi, & Zisserman, 2014). Recently,
there has been much investigation of implementing convolution operations with integ-
rated optics (Bangari et al., 2019; Feldmann et al., 2021; Miscuglio & Sorger, 2020;
X. Xu et al., 2021). These approaches can speed up convolution operations by orders
of magnitude (over current electronic processors such as GPUs and TPUs) by imple-
menting fast and parallel vector-matrix multiplications with wavelength multiplexing
techniques.

Figure 8. Schematic illustration of a convolution. At the top of the ﬁgure, an input image is represented as
a matrix of numbers with dimensionality H × W × D where H, W and D are the height, width and depth of
the image, respectively. Each element Ai,j of A represents the intensity of a pixel at that particular spatial
location. The kernel F is a matrix with dimensionality R × R × D, where each element Fi,j is deﬁned as a
real number. The kernel is slid over the image by using a stride S equal to one. As the image has multiple
channels (or depth) D, the same kernel is applied to each channel. Assuming H = W , the overall output
dimensionality is (H − R + 1)2. The bottom of the ﬁgure shows how a convolution operation generalized into
a single matrix-matrix multiplication. where the kernel F is transformed into a vector F with DR2 elements,
and the image A is transformed into a matrix A of dimensionality DR2 × (H − R + 1)2. Therefore, the output
is represented by a vector with (H − R + 1) elements.

A convolution is a weighted summation of two discrete domain functions f and
g: (f ∗ g) = (cid:80)∞
t=−∞ f [τ ]g[t − τ ] where (f ∗ g) represents a weighted average of the
function f [τ ] when it is weighted by g[−τ ] shifted by t. The weighting function g[−τ ]
emphasizes diﬀerent parts of the input function f [τ ] as t changes. Convolutions are
well known to perform a highly eﬃcient and parallel matrix multiplication using ker-
nels (Goodfellow et al., 2016). As depicted in Fig. 8 convolution of an image A with
a kernel F that produces a convolved image O. An image is represented as a mat-
rix of numbers with dimensionality H × W × D, where H and W are the height
and width of the image, respectively; and D refers to the number of channels within
the input image. Each element of a matrix A represents the intensity of a pixel at
that particular spatial location. A kernel is a matrix F of real numbers with di-
mensionality R × R × D. The value of a particular convolved pixel is deﬁned by:

21

1,11,21,31,41,51,61,71,81,9{1,11,21,51,41,51,61,71,81,91,21,31,41,51,51,61,81,11,21,31,41,11,21,31,4{2,12,22,32,43,13,23,33,4imagekernelDR2RRHWDD{{AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFFFFFFFFFFFFFFFF1234oooo1,11,21,31,4FFFF2,12,22,32,4FFFF3,13,23,33,4FFFFDR2output1234ooooFoA=*2,12,22,52,42,52,62,72,82,92,22,32,42,52,52,62,8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3,13,23,53,43,53,63,73,83,93,23,33,43,53,53,63,8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2,12,22,32,42,52,62,72,82,9AAAAAAAAAAAAAAAAAA3,13,23,33,43,53,63,73,83,9AAAAAAAAAAAAAAAAAA1,11,21,31,41,51,61,71,81,9AAAAAAAAAAAAAAAAAAh=1

(cid:80)jS+R

(cid:80)iS+R
q=0

Oi,j = (cid:80)D
p=0 Bq,p,hAiS+q,jS+p,h, where S is the stride” of the convolu-
tion. The dimensionality of the output feature is: (cid:100) H−R
S +1(cid:101)×K, where K
is the number of diﬀerent kernels of dimensionality R×R×D applied to an image, and
(cid:100)·(cid:101) is the ceiling function. The eﬃciency of convolutions for image processing is based
on the fact that they lower the dimensions of the outputted convolved features. Since
kernels are typically smaller than the input images, the feature extraction operation
allows eﬃcient edge detection, therefore reducing the amount of memory required to
store those features.

S +1(cid:101)×(cid:100) W −R

In 2014, Tait et al. (Tait et al., 2014) a proposed a scalable silicon photonic
neural network called ”broadcast-and-weight” (B&W) which was demonstrated in
2017 (Tait et al., 2017) concurrently with other silicon photonic neuromorphic ar-
chitectures (Shainline et al., 2017; Shen et al., 2017). Since the B&W architecture
is based on wavelength mutiplexing, parallel matrix multiplication can be leveraged
to perform operations such as convolutions. In B&W architecture, WDM signals are
weighted in parallel by a bank of MRRs used as tunable ﬁlters Huang, Bilodeau, et al.
(2020); Tait, Jayatilleka, et al. (2018), summed with balanced photodiodes (BPD), and
nonlinear activation functions implemented with MRR modulators. In 2020, based on
this architecture, Bangari et al. (Bangari et al., 2019) introduced a digital electronic
and analog photonic (DEAP) architecture capable of performing highly eﬃcient CNNs.

Figure 9. Photonic architecture for producing a single convolved pixel. Input images are encoded in intensities
Al,h, where the pixel inputs Am,n,k with m ∈ [i, i + Rm], n ∈ [j, j + Rm], k ∈ [1, Dm] are represented as Al,h,
l = 1, . . . , D and h = 1, . . . , R2. Considering the boundary parameters, we set D = Dm and R = Rm. Likewise,
the ﬁlter values Fm,n,k are represented as are represented as Fl,h under the same conditions. We use an array
of R2 lasers with diﬀerent wavelengths λh to feed the MRRs. The input and kernel values, Al,h and Fl,h
modulate the MRRs via electrical currents proportional to those values. Then, the photonic weight banks will
perform the dot products on these modulated signals in parallel. Finally, the voltage adder with resistance R
adds all signals from the weight banks, resulting in the convolved feature.

Fig. 9 shows the silicon photonic implementation of DEAP for performing convolu-
tion operations. To handle convolutions for kernels with dimensionality up to R×R×D,
R2 lasers are required with unique wavelengths since a particular convolved pixel can
be represented as the dot product of two 1 × R2 vectors. To represent the values of
each pixel, DR2 add-drop modulators (one per kernel value) are required where each
modulator keeps the intensity of the corresponding carrier wave proportional to the
normalized input pixel value. The R2 lasers are multiplexed together using WDM,
which is then split into D separate lines. On every line, there are R2 add-drop MRRs
(where only input and through ports are being used), resulting in DR2 MRRs in total.
Each WDM line will modulate the signals corresponding to a subset of R2 pixels on

22

***MUXWDMAA21,R1,2A1,11λ2λλ2RFD,1FD,2F2D,RF2,1F2,2F22,RF1,1F1,2F21,RRRRoutputRTIATIATIAInputKernellaser diodesmodulation weight banksvoltage adderAA22,R2,2A2,1AA2D,RD,2AD,1channel k, meaning that the modulated wavelengths on a particular line corresponds
to the pixel inputs (Am,n,k); m ∈ [i, i + R], n ∈ [j, j + R] where k ∈ [1, D], and S = 1.
The D WDM lines are then be fed into an array of D photonic weight banks (WB).
Each WB contains R2 MRRs with the weights corresponding to the kernel values
at a particular channel. Each MRR within a WB is tuned to a unique wavelength
within the multiplexed signal. The outputs of the weight bank array are electrical sig-
nals, each proportional to the dot product (Fm,n,k) · (Ap,q,k); m ∈ [1, R2], n ∈ [1, R2],
p ∈ [i, i + R2], q ∈ [j, j + R2], where k ∈ [1, D], and S = 1. Finally, the signals from the
weight banks are added together. This can be achieved using a passive voltage adder.
The output from this adder will therefore be the value of a single convolved pixel.

Recently, Feldman et al. (Feldmann et al., 2021) experimentally demonstrated this
approach with a photonic tensor core for parallel convolutional processing achieving
bandwidths of 2 TMACs/s and compute densities of 555 GMACs/s/mm2with 5-bits
of precision. Their system consists of a 3 × 3 ﬁlter, 4 channels and ﬁlters and 14 GHz
modulation and detection bandwidth, with a MAC cell area of 285 µm ×354 µm.
With improved devices, eﬃciency, bandwidth, and integration densities, such a tensor
core could feature a computational bandwidth of 1 PMACs/s and compute density of
15.6 TMACs/s/mm2 with 50 GHz modulation and detection bandwidths, tensor core
size of 50×50, and 187 wavelength channels, and MAC cell area of 30 µm × 30 µm.
For comparison, the Google TPU (Jouppi et al., 2017a) has a compute density of 150
GMACs/s/mm2 with 8-bits of precision.

3D integrated optical convolution: Information is kept within 2D spatial en-
coding, while light propagation through passive components along a third dimension
implements the required operation, i.e. Fourier and inverse Fourier transforms as well
as multiplication with a ﬁlter kernel. Novel 3D nano-fabrication (Deubel et al., 2004)
can now create intricate photonic waveguide circuits (Moughames et al., 2020) and
holograms (Dinc, Lim, Kakkava, Moser, & Psaltis, 2020) that equally leverage the
primary encoding space of images. Figure 10(a) shows how Boolean convolutional to-
pologies can be ’hard-wired’ in 3D. This intricate 3D routing can then be realized using
3D photonic waveguides, fabricated using two-photon polymerization of femto-second
laser pulses (Deubel et al., 2004), and Figure 10(b) shows a SEM micrograph, and
the resulting convolution ﬁlter’s transfer function, Figure 10(c), agrees well with the
target. Importantly, 3D integration makes such interconnects scalable in size (Dinc,
Psaltis, & Brunner, 2020), and motivated by identical scalability arguments similar
3D integration is already explored in electronics (P. Lin et al., 2020). However, such
electronics chips will face severe thermal management challenges due to capacitive
energy deposition into a volumetric circuit, and photonics therefore has an inherent
advantage towards scalable integration of parallel convolutional ﬁlters.

5.2.3. Edge or fog computing: image processing and super resolution ﬁltering

Photonic neural networks (e.g. Miscuglio and Sorger (2020)) and free-space Fourier-
optics 4f processors (Miscuglio, Hu, et al., 2020) can be particularly beneﬁcial to tasks
that involve, for instance, Super-Resolution on Object Detection Performance in Satel-
lite Imagery. Some modern camera sensors, present in everyday electronic devices like
digital cameras, phones, and tablets, are able to produce reasonably high-resolution
(HR) images and videos. The resolution in the images and videos produced by these
devices is in many cases acceptable for general use. However, there are situations where
the image or video is considered low resolution (LR). Examples include the following
situations: (i) Device resolution limitation (as in some surveillance systems and satellite

23

Figure 10. Fully parallel and passive convolutions integrated in 3D. (a) 9 convolutional Haar ﬁlters and their
3D integration topology using photonic waveguides. (b) SEM micrograph of the 3D printed convolutional ﬁlter
unit, fabricated with direct laser writing. (c) The ﬁlter’s optical transfer function agrees well with the target
deﬁned in (a). Figure reproduced from Moughames et al. (2020).

images). (ii) Object relatively small in a larger context; e.g. faces or vehicle located far
away from the camera. (iii) Blurred or noisy images. being the device mounted on mov-
ing automated device (e.g. drones). (iv) Improving the resolution as a pre-processing
step improves the performance of other algorithms that use the images; pattern recog-
nition and target tracking. Super-resolution is a technique to obtain a high resolution
(HR) image from one or several LR images. In this case one can use a training set to
train (oﬄine) a convolutional neural network (CNN) to learn to map between a low-
resolution image and the high-resolution image within the training set; for instance,
using a 4f-based system one can obtain pixel-wise dot product in the Fourier domain
between the large input matrix (2MP×) with a reprogrammable kernel that has been
pre-patterned/written according to the training. The re-programmability, in a ﬁrst
instance, can be achieved using micromirror devices, which by shining light on top of
the ﬁlm, can locally change the phase of the PCM generating a 2D pattern. Although,
this solution is limited by the speed at which the ﬁlm can be written, therefore, in
future implementations one can independently and simultaneously tune each pixel by
changing their phases electrothermally in µs-time scale (requires additional circuitry),
thus acting as 2D space ﬁlter in the Fourier domain or as layer of a convolutional net-
work. Providing a forward looking view, if one would substitute the liquid crystal from
the SLMs and the micromirrors from the DMDs with GHz-fast updating elements, but
keep the same 1000 x 1000 pixel real-estate, the system would yet improve by a factor
of 1010 to 108 from SLM’s and DMD’s, respectively. Such electro-optic modulation,
however, must be ultra-compact and regular photonics modulators based on carrier
injection or depletion using Silicon or using the Pockel’s eﬀect in Lithium Niobate
C. Wang et al. (2018) are non-usable due to millimeter to centimeter-large modulator
device footprints. An alternative approach is to utilize higher index-changing emerging
EO materials and heterogeneously integrate them with photonic waveguides. ITO and
its ability to produce epsilon-near-zero Amin et al. (2017), a nonlinearity enhance-
ments (including EO nonlinearity) can be used to demonstrate micrometer-compact
modulators, for example. Sorger, Lanzillotti-Kimura, Ma, and Zhang (2012), Amin et
al. (2018), Amin, Maiti, George, et al. (2020), Amin, Maiti, Gui, et al. (2020).

5.2.4. Ultrafast learning and STDP

What makes neuron fascinating is its ability to learn and adapt, which is a power-
ful capability that governs our actions, thoughts, and memories. These important
functions of humans are relying on the synaptic plasticity between neurons, which is

24

self-adjusted based on the information being processed and the response of the neuron
itself. Among diﬀerent synaptic weight plasticity models, spike timing dependent plas-
ticity (STDP) is the most popular one, which is a biological process that adjusts the
interconnection strength between neurons based on the temporal relationship (i.e. tim-
ing and sequence) between pre-synaptic and post-synaptic activities (Song, Miller, &
Abbott, 2000). The more you are running a certain neural circuit, the stronger the
circuit becomes.

In STDP, the interconnection strength between two neurons (N1 and N2) is de-
termined by the relative timing and sequence between the presynaptic (red) and post-
synaptic spikes (blue), as illustrated in Figure 11(a). If N2 spikes shortly after the
stimulation from the presynaptic spike, the interconnection strength will be signiﬁc-
antly increased and results in long-term potentiation (LTP) of the connection strength,
as illustrated by the shaded purple region in Figure 11(b). However, if N2 spikes be-
fore the stimulation from the pre-synaptic spike, the interconnection strength will be
signiﬁcantly decreased, resulting in long-term depression (LTD) of the synaptic con-
nection, as illustrated by the shaded brown region in Figure 11(b). The exact amount
of synaptic connection strength increment/decrement depending on the precise timing
diﬀerence between the pre-synaptic and post-synaptic spikes of N2.

To enable ultrafast learning in photonic neuron network, STDP has to be imple-
mented using photonics and integrated into the photonic neuron network (Fok, Tian,
Rosenbluth, & Prucnal, 2013; Toole et al., 2015). One promising solution to implement
STDP in photonic neuron network is using semiconductor optical ampliﬁer (SOA) (Fok
et al., 2013; Toole & Fok, 2015; Toole et al., 2015). SOA has a unique gain dynamic
that is sensitive to the timing and sequence of the input stimulations, which is similar
to the STDP in neurons. One example is to use both cross-gain modulation and cross-
polarization modulation in SOA to mimic the LTP and LTD responses in biological
neurons (Toole & Fok, 2015), the resultant photonic STDP is shown in Figure 11(c)
that reassemble the biological STDP. It has been shown that supervised learning can
be achieved using a SOA based STDP and two SOAs based neurons (Fok et al., 2013;
Toole et al., 2015).

Figure 11.
(a) Illustration of pre-synaptic and post-synaptic spikes from two neurons N1 and N2. (b) Illustra-
tion of a STDP response. Right purple region: long-term potentiation; Left brown region: long-term depression.
tpost-tpre: time diﬀerence between the ﬁring of the post- and pre-synaptic spikes. (c) Experimentally measured
STDP curves from the photonic based STDP circuit.

5.2.5. Boolean learning via coordinate descent

Current neural network optimization is largely based on back-propagating error gradi-
ents, and today’s boom in neural network applications is closely linked to the successful

25

implementation of this concept in digital hardware. However, in hardware with uni-
directional, i.e. forward ﬂow of information, its implementation requires calculating the
error-gradient for each network connection according to the chain rule of diﬀerenti-
ation. In digital hardware this creates an enormous overhead, while in analog networks
each weight and neuron parameter needs to be probed and stored, which ultimately
is prohibitively complex in most settings. In systems where information can symmet-
rically propagate forward as well as backward, such as often the case in photonics, a
regulatory error signal could be sent backwards. However, in order to physically im-
plement weight optimization according to error back propagation using hardware rules
such as STDP, a neuron’s nonlinearity in backward direction needs to be the gradi-
ent of its nonlinearity in forward direction. Such asymmetric neurons are a challenge
that remains largely out of reach until the current day, and the only viable photonic
concepts rely on phase conjugation (Psaltis & Wagner, 1987; Zhou et al., 2020).

Coordinate descent is a practical as well as eﬃcient alternative to error back
propagation and currently is heavily explored in the ﬁeld of machine learning. In-
dividual or sets of weights i.e. coordinates, usually drawn at random, are modiﬁed in
order to probe the error landscape’s local gradient. Probing is followed by updating
weights opposite to the gradient’s direction and with a weighting factor dubbed the
learning rate. In photonic hardware Boolean neural network connection weights have
been realized via a digital micro-mirror device (DMDs) (Bueno Moragues et al., 2018),
and in a Boolean context the error gradient is probed by inverting a set of weights.
Should the associated error gradient be negative, then the last modiﬁcation is kept,
otherwise it is discarded and the connections revert back to the previous conﬁguration.
Boolean weights are currently explored in the general context of neural networks
(Courbariaux, Hubara, Soudry, El-Yaniv, & Bengio, 2016) as well as in special-
purpose electronic hardware (Hirtzlin et al., 2020). In photonic reservoir comput-
ing (Bueno Moragues et al., 2018) it was shown that Boolean coordinate descent
converges exponentially and achieves chaotic signal prediction accuracy only slightly
below a comparable photonic reservoir (Bueno, Brunner, Soriano, & Fischer, 2017)
where double-precision weights were optimized oﬄine. Furthermore, instead of a fully
random, i.e. Makovian selection of descent coordinates, leveraging a greedy selection
strategy allowed the system to converge twice as fast.

Boolean photonic weights implemented via a DMD enable programmable photonic
neural networks comprising thousands of connections. In digitally emulated (Courbar-
iaux et al., 2016), electronically (Hirtzlin et al., 2020) and photonically implemented
(Bueno Moragues et al., 2018) neural networks, such binarized weights resulted only
in slight performance penalties. DMD based Boolean coordinate descent in photonic
neural networks therefore harbors great prospects for future, practical yet high per-
formance neural networks, which noteworthy can be readily programmed based on
classical software tools.

5.3. Nonlinear programming

5.3.1. Solving optimization problems (model predictive control)

Solving mathematical optimization problems lies at the heart of various applications
present in modern technology such as machine learning, resource optimization in wire-
less networks, and drug discovery. Many optimization problems can be written as a
quadratic program. For example, the least squares regression method can be math-
ematically mapped to a relatively easy quadratic program (with a positive deﬁnite

26

quadratic matrix). Quadratic Programming refers to algorithms related to solving the
optimization problem of ﬁnding the extremes of a quadratic objective function subject
to linear constraints. The general formulation of a quadratic program is usually solved
iteratively, often requiring many time steps to reach the desired solution. The diﬃculty
of quadratic programming grows exponentially with the dimension of the problem. Al-
gorithms that can deal with large dimensions involve more computationally intensive
techniques such as genetic algorithms or particle swarm optimization. As a result, con-
ventional digital computers must either be limited to solving quadratic programs of
very few variables, or to applications where the computation time is non-critical. This
is why traditional computers are not appropriate to implement algorithms depending
on QP for high-speed applications such as signal processing and control systems. In
machine learning, many algorithms, such as SVM, require oﬄine training because of
the computational complexity of QP, but would be much more eﬀective were they
trained online.

A number of high-speed control problems, e.g. controlling plasma in aircraft actu-
ations, fusion power plants, guiding of drones etc., are currently bottlenecked by the
speed and latency of the control algorithms. Model predictive control (MPC) is an
advanced technique to control complex systems, outperforming traditional PID con-
trol methods because it is able to predict control violations, rather than react to it.
However, its control loop involves solving a quadratic problem at every control step,
and therefore it is not computationally tractable for systems requiring speeds higher
than kHz. Photonic neural networks help overcome this tradeoﬀ by using techniques
such as wavelength division multiplexing, which enables hundreds of high bandwidth
signals (20 GHz) to be guided through a single optical waveguide.

Neural networks were demonstrated to solve general-purpose quadratic optimization
problems by Hopﬁeld and Tank (Cichocki, Unbehauen, & Swiniarski, 1993; Hopﬁeld
& Tank, 1986). Until now, Hopﬁeld networks have not been commonly implemented
in hardware due to its all-to-all connectivity, which creates an undesirable tradeoﬀ
between neuron speed and neural network size — in an electronic circuit, as the num-
ber of connections increases, the bandwidth at which the system can operate decreases
Tait et al. (2014). This means a photonic Hopﬁeld network implementation can sim-
ultaneously tackle quadratic programs with large dimensions and converge in nano-
seconds (De Lima et al., 2019). Implemented in photonic hardware, model predictive
control can be employed in systems operating in the MHz regime.

5.3.2. Ordinary diﬀerential equation solving with a neural compiler

While neural networks are often used for their learning properties, they can also be
programmed directly. Direct programming of analog systems has a major pitfall in
that the components are unreliable and subject to parameter variation. One approach
is to represent variables as population states that are robust to parameter variations.
The neural engineering framework (NEF) (Stewart & Eliasmith, 2014) provides an
algorithm to program ensembles of imperfect analog devices to perform operations on
population coded variables.

Tait et al. (2017) demonstrated a programmable network of two photonic neurons.
The NEF algorithm was fed the responses of photonic devices, resulting in a weight
matrix that allows the network to approximate variables, operations, and diﬀeren-
tial equations. It was shown in simulation how 24 neurons could emulate the Lorenz
attractor. The approximation improves with more neurons.

The example of the Lorenz attractor is an example of a task-based benchmark for

27

Figure 12. Schematic ﬁgure of the procedure to implement the MPC algorithm on a neuromorphic photonic
processor. Firstly, map the MPC problem to QP. Then, construct a QP solver with continuous-time recurrent
neural networks (CT-RNN) (Cichocki et al., 1993). Finally, build a neuromorphic photonic processor to imple-
ment the CT-RNN. The details of how to map MPC to QP, and how to construct a QP solver with CT-RNN
are given in De Lima et al. (2019). Adapted from De Lima et al. (2019).

a 24-neuron network. Task-based benchmarks play an important role in validating
experimental systems as they begin to incorporate more neurons and parameters.
The example also demonstrated the compatibility between photonic neural networks
and the NEF, including all of the NEF’s key principles and consequent functionality.
Using this neural compiler provides a route to a variety of known applications and
other benchmarks (Stewart, DeWolf, Kleinhans, & Eliasmith, 2015).

Figure 13. Photonic neural network benchmarking against a CPU. (a,b) Phase diagrams of the Lorenz at-
tractor simulated by a conventional CPU (a) and a photonic neural network (b). (c,d) Time traces of simulation
variables for a conventional CPU (c) and a photonic CTRNN (d). The horizontal axes are labeled in physical
real time, and cover equal intervals of virtual simulation time, as benchmarked by γCPU and γPho. The ratio
of real-time values of γ’s indicates a 294-fold acceleration. From Tait et al. (2017)

28

5.4. Cryptography and security

5.4.1. Optical steganography (bio-inspired by Marine Hatchetﬁsh camouﬂage

strategies)

Communication systems have been integrated deeply in our daily lives, supporting ap-
plications including online banking, augmented reality experience, and telemedicine.
Therefore, it is expected that the massive amount of sensitive and personal information
are needed to be protected against attacks. To provide information security, sophist-
icated encryption schemes are used at the higher layer of the communication system,
i.e. media access control (MAC) layer. However, a physical layer without property
security measures makes a communication system vulnerable to attack, resulting in
total exposure of sensitive information (Skorin-Kapov, Furdek, Zsigmond, & Wosinska,
2016).

Eﬀective physical cryptography requires both encryption and steganography. En-
cryption scrambles the sensitive information so that it is unreadable without the key,
while steganography hides the sensitive information in plain sight so that the attacker
will not even know there is sensitive information to look for. It is like hiding valuables
in a locked safe (encryption) behind a secret bookcase door (steganography). Physical
encryption schemes have been intensively studied, but physical layer steganography is
still underdeveloped (Fok, Wang, Deng, & Prucnal, 2011; Z. Wang & Prucnal, 2010;
Wu et al., 2013).

Turning to nature for an eﬀective solution, Marine Hatchetﬁsh is a master of hiding
its appearance in the deep ocean using unique ocean camouﬂage techniques (Rosenthal,
Holt, & Sweeney, 2017). Firstly, Marine Hatcherﬁsh has microstructured skin on the
sides so that only light that is similar to its surrounding is constructively interfered,
while colors that could disclose its presence are destructively interfered, this technique
is called silvering (Figure 14(a)). Secondly, Marine Hatchetﬁsh also generates and
directs light to the bottom part of its body to illuminate itself so that its color and
brightness is the same as its surrounding when seen from below, this technique is called
counter-illumination (Figure 14(b)). The two camouﬂage strategies allow the Marine
Hatchetﬁsh to conceal its appearance in all directions.

Borrowing the camouﬂage strategies from Marine Hatchetﬁsh and applying it in
RF signal transmission in optical ﬁber would be a natural and an eﬀective way to
achieve steganography. Silvering can be achieved using photonic FIR to make the
sensitive signal disappear in the eyes of the attacker through destructive interference
(Q. Liu & Fok, 2021). Figure 14(c) shows the transformation of the FIR response at
(i) the stealth transmitter, (ii) during and after signal transmission in single mode
ﬁber, (iii) after dispersion compensation, and (iv) at the designated stealth receiver.
The stealth signal is invisible at any point during the transmission and can only
be retrieval with a precise stealth receiver at the designated location, Furthermore,
counter-illumination can be achieved using a noise-like optical carrier that has the same
spectral content and intensity as the system noise, similar to how Marine Hatchetﬁsh
illuminate itself (Q. Liu & Fok, 2021). Steganography using bio-inspired silvering and
counter-illumination techniques allow the sensitive signal to be concealed in all possible
domains that the attacker could be looking at. Figure 14(d)i and ii shows the measured
RF spectra and constellation diagrams of the stealth signal during transmission and
at the designated stealth receiver. It is proven that the stealth signal is disappeared
in the eyes of the attacker.

29

Figure 14. a) Side view (i) no camouﬂage - ﬁsh is visible (ii) silvering - ﬁsh is destructively interfered at
colors that could indicate the presence of the ﬁsh; (b) Bottom view (i) no camouﬂage – ﬁsh appears darker
against the bright water surface when seen from below (ii) counterillumination – ﬁsh illuminates itself to the
same color and intensity as the background. (c) Silvering (i) photonic RF FIR creates destructive interference
condition at the stealth signal frequency (fs); (ii) Transmission in optical ﬁber will only push the constructive
interference condition to a much higher frequency (fc+); (iii) Dispersion compensation ﬁber at the last section
of the transmission will move the constructive interference condition back to fc; (iv) Correct dispersion at
the stealth receiver allows constructive interference condition to occur at the stealth signal frequency fs. (d)
Measured RF spectra and constellation diagrams (i) during transmission without a correct stealth receiver (ii)
at the designated stealth receiver with correct location and dispersion.

5.4.2. Encryption and decryption

Movement of massive data traﬃc is rapidly growing in this big-data era, i.e. the 4th in-
dustrial revolution of ’digitalization’, spurred by the emergence of machine intelligence.
Since the compute capacity at any one physical location is limited by the bounds of
power and cooling requirements, it is naturally inevitable that data movement will be
necessary. At the same time, it is critical to maintain information security during data
transit between distributed computing locations. Encryption-in-transit mechanisms
protect the integrity and conﬁdentiality of sensitive information transmitted over In-
ternet infrastructure between physical computing locations. However, due to the scale
of data volumes (∼Terabits/sec), eﬃcient (en/de)cryption mechanisms are paramount
for eﬀectiveness of encryption-in-transit — exactly what optical communication links
provide. Since the data is already in the optical domain, thence, it is natural to con-
sider (pre)processing information in the same optical and analog-domain (Figure 15).
That is, avoiding domain crossings such as optical-to-electrical (OE, vise-versa) and
eliminating digital-to-analog (and vise-versa) not only saves energy-per-compute, but
also improves system delay and reduces system complexity. The latter, is important
for scaling vectors such as volume, reliability and ultimate cost of the system and
hence the application.

Data security applications include three main areas: Authentication, which is the
veriﬁcation of data sources and destinations. Such functions are carried out by mod-
ules embedded within Trusted Computing Platforms, which ensure that only legitim-
ate users can send and receive data. Modern advances in multi-factor authentication
mechanisms (Bhargav-Spantzel et al., 2007) have incorporated several aspects such as
knowledge factors, ownership factors and inherence factors. Integrity, which considers
ensuring that the transmitted data arrives at the destination in an unmodiﬁed man-
ner. Data integrity is usually guaranteed with checksum mechanisms that include error
correcting codes. Such integrity-preserving mechanisms have been incorporated even
inside modern hardware- for instance, some CPU architectures employ transparent
checks to detect and mitigate data corruption in CPU caches, buﬀers and instruction

30

pipelines as evidenced in Intel Instruction Replay mechanism in its Itanium processor
family (Bostian, 2013). Data Privacy – involving transformations applied to legible
data (plaintext user data) with the intent of making sure that it is only available to
users that are authorized by the data owners. Typically, data encryption algorithms are
used for achieving privacy guarantees, where the plaintext is transformed into cipher
text before transmission and the keys needed for decrypting the encrypted cipher text
are kept private. In order to secure web applications and systems, networks have to be
able to promptly discern potential menaces and unwanted connections. Systems like
intrusion detection (IDS) and intrusion prevention (IPS) are used for this purpose.
Intrusion detection systems are divided in two groups: misuse detection (traditional
IDS) and anomaly detection. Misuse detection systems are signature based, have high
accuracy in detecting many kinds of known attacks but cannot detect unknown and
emerging attacks. Our PTC, when properly trained according to previous knowledge
of attacks, can be used as an intelligent comparator for the fast detection of mis-
use of the systems compared (performing convolution on string of data) to stored
signatures of known exploits or attacks which are learnt in the photonic memories.
This can be supplemented with anomaly-based intrusion detection as a prevention
system. In fact, due to matrix multiplication and comparison performed at high-speed
in optical accelerators, it can be used not just as smart pattern matcher, but as an
evolving fast pre-screening of malicious activities, by collecting normal behaviors and
detecting intrusion based on that, since new intrusion model can be implemented in
the reprogrammable photonic memories thanks to the newly acquired and updating
‘knowledge’. Optical processing of high parallelism, inherent to several cryptographic
operations, is enabled by taking advantage of various attributes of light waves such as
the wavelength, phase, polarization, and amplitude. As such, energy-eﬃcient, ultra-
low latency encryption-in-transit using optical accelerators can help address the grand
challenges surrounding security in big data movement between computing systems.

Figure 15.
(left) Photonic machine-learning accelerators such as photonic tensor core (PTU) processors
(Miscuglio & Sorger, 2020) enabled a higher level of data security with reduced overhead (e.g. time and power
consumption). This opens possibilities for not only securing ’secure’ and ’conﬁdential’ data, but also meta-data.
(right) Flow-chart for detection of anomalies and misuses exploiting rapidly updating photonic neural network
and trained photonic neural network which embeds on chip photonic memory, respectively.

31

5.5. Physics experiments

5.5.1. Intelligent Preﬁltering in Astronomy and Scientiﬁc Applications

Astronomical radio observation and study on the galaxy formation have become ex-
tremely accurate thanks to the use of a plurality of telescopes arranged in an array,
operating as a one single giant telescope. In this way, like all synthetic arrays, due to
an enlarged equivalent aperture of 22 miles, the very large array (VLA), is sensitive
and able to resolve a range of angular scales between the diﬀraction limit (Figure 16).
A tremendous leap in this established technology is represented by the way the vast
data obtained is collected and processed; data is ﬁber-optically fed to a supercom-
puter (WIDAR (The WIDAR Supercomputer , n.d.)) turning the VLA into a sensitive
instrument. It is possible to obtain important information regarding star formation
in ’interferometric’ computing techniques, where the supercomputer correlates the
hyper-spectral (wide band) signals from pairs of dishes obtaining a much sharper im-
age than a single dish could produce. WIDAR uses FPGAs to perform correlations
on the radiofrequency signal and only then sends relevant data to the cluster for fur-
ther processing. Ultimately the cluster output is sent to an image-processing system.
However, electronic data processing is limited by FPGA-setup times and fundamental
electronic capacitive delay, resulting in delayed processing. To mitigate such processing
limitations, photonic neural networks can be used as preprocessing unit to work syn-
ergistically with the WIDAR supercomputer on the vast data, in order to intelligently
sorting and correlating the signal looking for speciﬁc chunks of radio-patterns (e.g.
hydrogen gas moves into galaxies to fuel star formation) in near real time (∼ps).
Besides the increased speeds and bandwidths that can come from working directly
in the optical domain, leveraging on the intrinsic optical nature of signal captured
by the dish-antennas travelling in optical ﬁbers, the advantage of using the photonic
architectures consists in exploiting the wave-nature of the input signals to perform
inherent correlation detection or convolution using pre-stored/programmable trained
weights, without active power consumption and burdensome electro-optic conversions,
as discussed above. In this way, the total amount of information to be handled by the
supercomputer and consequently by the cluster unit is reduced, saving resources for
useful data, favored by intelligent pre-screening, towards resolving the evolution of the
universe (VLA Begins Huge Project of Cosmic Discovery, n.d.).

Figure 16. Photonic tensor core and neural networks enable intelligent preﬁltering and correlation for sci-
entiﬁc discovery, such as between electromagnetic signals collected by Very-Large-Array telescope systems
performing intelligent pre-ﬁltering, thus reducing computation load in supercomputers.

32

5.5.2. Emerging applications: Quantum computer auxiliary systems and High-energy

particle classiﬁcation

Scalable quantum computing depends on classical auxiliary technologies for state re-
construction, calibration, and control. Neural networks have been used for quantum
state reconstruction (Flurin, Martin, Hacohen-Gourgy, & Siddiqi, 2020), tomography
(Torlai et al., 2018), and control (Niu, Boixo, Smelyanskiy, & Neven, 2019). In some
cases, the characteristic time constants of the state or instability are slow enough for a
conventional computer to perform the task. In other cases, in particular for microwave
qubits, the system is changing faster than a conventional computer can react. This
means that a control and/or reconstruction task cannot occur in real-time. Photonic
neural networks could reduce the latency of these operations, potentially opening up
new opportunities to better monitor and stabilize quantum processing systems.

Photonic neural networks can exhibit latencies lower than electronic processors,
whether neuromorphic, FPGAs, or ASICs. This low latency can make the crucial
diﬀerence in certain applications where a decision must be made before the time to
act passes. This critical time constraint exists in particle detectors such as CMS.
Not all collisions are salient, so a trigger must classify the collision as salient or not
before the next collision occurs. Hardware limitations dictate that the existing trigger
uses rudimentary, non-adaptive algorithms that potentially overlook particular physics
signatures. Recent work to improve the sophistication of particle classiﬁcations has
adopted a neural network algorithms, implemented on FPGAs (J. Duarte et al., 2018).
There is a potential for photonic neural networks to improve the performance of the
time-critical triggering task, thus preserving physics signatures and enabling a higher
collision rate.

6. Conclusion

The emergence of neural network models has highlighted the importance of inter-
connects and parallel processing, which is inherently advantageous to implement in
photonics. The research community has built bridges between photonic device physics
and neural networks. The performance improvement of photonic neural networks is
expected to continue as new devices (e.g., modulators or lasers) based on new materi-
als and nanostructures demonstrate their potential of further increasing the eﬃciency.
The next generation of photonic devices could consume only hundreds of aJs of en-
ergy per time slot, allowing analog photonic MAC-based processors to consume even
less per operation (de Lima et al., 2020; Nahmias et al., 2019). Meanwhile, advanced
integration and fabrication techniques (e.g., silicon photonics) have provided an unpre-
cedented platform to produce large-scale and low-cost photonic systems. The increased
optical component density signiﬁcantly extends the spectrum of information processing
capabilities. Finally, monolithic fabrication, which integrates electronics and photon-
ics on the same substrate, entails a tight co-integration of electronics and photonics,
resulting in hybrid neuromorphic processors that can take the best advantages of both
electronics and photonics depending on diﬀerent applications.

In light of these developments, photonic neural networks have found places in many
applications unreachable by conventional computing technology. Examples of applic-
ations explored in this paper include intelligent signal processing, high-performance
computing, nonlinear programming, and control, enabling fundamental physics break-
throughs, etc. These applications particularly require low latency, high bandwidth,

33

and low energies. To march ahead, we envisage a huge interest in developing the fun-
damental technologies (i.e., devices, fabrication/integration platforms, etc.) enabling
large-scale photonic neural networks. In parallel, more applications will be identiﬁed
and demonstrated, along with the photonic platform development, promising to ex-
pand the application space of AI and information processing.

References

Abari, O., Lim, F., Chen, F., & Stojanovi´c, V. (2013). Why analog-to-information converters
suﬀer in high-bandwidth sparse signal applications. IEEE Transactions on Circuits and
Systems I: Regular Papers, 60 (9), 2273–2284.

Agarwal, S., Jacobs-Gedrim, R. B., Bennett, C., Hsia, A., Van Heukelom, M. S., Hughart,
D., . . . Marinella, M. J. (2019). Designing and modeling analog neural network training
accelerators. In 2019 international symposium on vlsi technology, systems and application
(vlsi-tsa) (p. 1-2).

Agrell, E., Karlsson, M., Chraplyvy, A., Richardson, D. J., Krummrich, P. M., Winzer, P., . . .
others (2016). Roadmap of optical communications. Journal of Optics, 18 (6), 063002.
Ahmed, A. H., El Moznine, A., Lim, D., Ma, Y., Rylyakov, A., & Shekhar, S. (2020). A
dual-polarization silicon-photonic coherent transmitter supporting 552 Gb/s/wavelength.
IEEE Journal of Solid-State Circuits, 55 (9), 2597-2608.

Ahmed, A. H., Sharkia, A., Casper, B., Mirabbasi, S., & Shekhar, S. (2016). Silicon-photonics
microring links for datacenters—challenges and opportunities. IEEE Journal of Selected
Topics in Quantum Electronics, 22 (6), 194-203.

Alexoudi, T., Kanellos, G. T., & Pleros, N.

(2020). Optical ram and integrated optical

memories: a survey. Light: Science & Applications, 9 (1), 1–16.

Ambs, P. (2010). Optical computing: A 60-year adventure. Advances in Optical Technologies.
Amin, R., George, J., Sun, S., Ferreira de Lima, T., Tait, A. N., Khurgin, J., . . . others
Ito-based electro-absorption modulator for photonic neural activation function.

(2019).
APL Materials, 7 (8), 081112.

Amin, R., Maiti, R., Carfano, C., Ma, Z., Tahersima, M. H., Lilach, Y., . . . Sorger, V. J.
(2018). 0.52 v mm ito-based mach-zehnder modulator in silicon photonics. APL Photonics,
3 (12), 126104.

Amin, R., Maiti, R., George, J. K., Ma, X., Ma, Z., Dalir, H., . . . Sorger, V. J. (2020). A
lateral mos-capacitor-enabled ito mach–zehnder modulator for beam steering. Journal of
Lightwave Technology, 38 (2), 282–290.

Amin, R., Maiti, R., Gui, Y., Suer, C., Miscuglio, M., Heidari, E., . . . Sorger, V. J. (2020). Sub-
wavelength ghz-fast broadband ito mach–zehnder modulator on silicon photonics. Optica,
7 (4), 333–335.

Amin, R., Suer, C., Ma, Z., Sarpkaya, I., Khurgin, J. B., Agarwal, R., & Sorger, V. J. (2017).
Active material, optical mode and cavity impact on nanoscale electro-optic modulation
performance. Nanophotonics, 7 (2), 455–472.

Appeltant, L., Soriano, M. C., Van der Sande, G., Danckaert, J., Massar, S., Dambre, J., . . .
Fischer, I. (2011). Information processing using a single dynamical node as complex system.
Nature communications, 2 (1), 1–6.

Argyris, A., Bueno, J., & Fischer, I. (2018a). Photonic machine learning implementation for

signal recovery in optical communications. Scientiﬁc reports, 8 (1), 1–13.

Argyris, A., Bueno, J., & Fischer, I. (2018b). Photonic machine learning implementation for

signal recovery in optical communications. Scientiﬁc reports, 8 (1), 1–13.

Bangari, V., Marquez, B. A., Miller, H., Tait, A. N., Nahmias, M. A., De Lima, T. F., . . .
Shastri, B. J. (2019). Digital electronics and analog photonics for convolutional neural
networks (deap-cnns). IEEE Journal of Selected Topics in Quantum Electronics, 26 (1),
1–13.

34

Bankman, D., & Murmann, B.

(2016). An 8-bit, 16 input, 3.2 pj/op switched-capacitor
dot product circuit in 28-nm fdsoi cmos. In 2016 ieee asian solid-state circuits conference
(a-sscc) (pp. 21–24).

Berggren, K., Xia, Q., Likharev, K. K., Strukov, D. B., Jiang, H., Mikolajick, T., . . . others
(2020). Roadmap on emerging hardware and technology for machine learning. Nanotech-
nology, 32 (1), 012002.

Bhargav-Spantzel, A., Squicciarini, A. C., Modi, S., Young, M., Bertino, E., & Elliott, S. J.
(2007). Privacy preserving multi-factor authentication with biometrics. Journal of Com-
puter Security, 15 (5), 529–560.

Biswas, A., & Chandrakasan, A. P. (2018). Conv-ram: An energy-eﬃcient sram with embedded
convolution computation for low-power cnn-based machine learning applications. In 2018
ieee international solid - state circuits conference - (isscc) (p. 488-490).

Boahen, K. (2017). A neuromorph’s prospectus. Computing in Science Engineering, 19 (2),

14-28.

Bostian, S. (2013). Rachet up reliability for mission-critical applications: Intel® instruction

replay technology. White Paper , 48 .

Bueno, J., Brunner, D., Soriano, M. C., & Fischer, I. (2017). Conditions for reservoir computing
performance using semiconductor lasers with delayed optical feedback. Optics express,
25 (3), 2401–2412.

Bueno Moragues, J., Maktoobi, S., Froehly, L., Fischer, I., Jacquot, M., Larger, L., & Brunner,
D. (2018). Reinforcement learning in a large-scale photonic recurrent neural network.
Bullock, T. H., Hamstra, R. H., & Scheich, H. (1972). The jamming avoidance response of

high frequency electric ﬁsh. In How do brains work? (pp. 509–534). Springer.

Cao, N., Chang, M., & Raychowdhury, A. (2020, Jan). A 65-nm 8-to-3-b 1.0–0.36-v 9.1–1.1-
tops/w hybrid-digital-mixed-signal computing platform for accelerating swarm robotics.
IEEE Journal of Solid-State Circuits, 55 (1), 49-59.

Chagnon, M. (2019). Optical communications for short reach. Journal of Lightwave Techno-

logy, 37 (8), 1779–1797.

Chakraborty, I., Saha, G., Sengupta, A., & Roy, K. (2018). Toward fast neural computing

using all-photonic phase change spiking neurons. Scientiﬁc reports, 8 (1), 1–9.

Chen, Y.-H., Emer, J., & Sze, V. (2016). Eyeriss: A spatial architecture for energy-eﬃcient
dataﬂow for convolutional neural networks. ACM SIGARCH Computer Architecture News,
44 (3), 367–379.

Chen, Y.-H., Krishna, T., Emer, J. S., & Sze, V. (2016). Eyeriss: An energy-eﬃcient recon-
ﬁgurable accelerator for deep convolutional neural networks. IEEE journal of solid-state
circuits, 52 (1), 127–138.

Choi, J., Venkataramani, S., Srinivasan, V., Gopalakrishnan, K., Wang, Z., & Chuang, P.
(2019). Accurate and eﬃcient 2-bit quantized neural networks. In Proceedings of the 2nd
sysml conference (Vol. 2019).

Cichocki, A., Unbehauen, R., & Swiniarski, R. W. (1993). Neural networks for optimization

and signal processing (Vol. 253). wiley New York.

Cisco. (n.d.). Cisco annual internet report - cisco annual internet report (2018–2023) white pa-
https://www.cisco.com/c/en/us/solutions/collateral/executive-perspectives/annual-

per.
internet-report/white-paper-c11-741490.html. (Accessed: 2021-02-17)

Cong, J., & Xiao, B. (2014). Minimizing computation in convolutional neural networks. In

International conference on artiﬁcial neural networks (pp. 281–290).

Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., & Bengio, Y. (2016). Binarized neural
networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830 .

Da Ros, F., Ranzini, S. M., B¨ulow, H., & Zibar, D.

(2020). Reservoir-computing based
equalization with optical pre-processing for short-reach optical transmission. IEEE Journal
of Selected Topics in Quantum Electronics, 26 (5), 1–12.

De Lima, T. F., Peng, H.-T., Tait, A. N., Nahmias, M. A., Miller, H. B., Shastri, B. J., &
Prucnal, P. R. (2019). Machine learning with neuromorphic photonics. Journal of Lightwave

35

Technology, 37 (5), 1515–1534.

de Lima, T. F., Tait, A. N., Mehrabian, A., Nahmias, M. A., Huang, C., Peng, H.-T., . . .
Prucnal, P. R. (2020). Primer on silicon neuromorphic photonic processors: architecture and
compiler. Nanophotonics, 9 (13), 4055–4073. Retrieved from https://doi.org/10.1515/
nanoph-2020-0172

Deubel, M., Von Freymann, G., Wegener, M., Pereira, S., Busch, K., & Soukoulis, C. M. (2004).
Direct laser writing of three-dimensional photonic-crystal templates for telecommunications.
Nature materials, 3 (7), 444–447.

Dinc, N. U., Lim, J., Kakkava, E., Moser, C., & Psaltis, D. (2020). Computer generated optical

volume elements by additive manufacturing. Nanophotonics, 1 (ahead-of-print).

Dinc, N. U., Psaltis, D., & Brunner, D. (2020). Optical neural networks: The 3d connection.

Photoniques(104), 34–38.

Dong, P., Liao, S., Feng, D., Liang, H., Zheng, D., Shaﬁiha, R., . . . others (2009). Low v
pp, ultralow-energy, compact, high-speed silicon electro-optic modulator. Optics express,
17 (25), 22484–22490.

Dorren, H., Lenstra, D., Liu, Y., Hill, M. T., & Khoe, G.-D. (2003). Nonlinear polarization
rotation in semiconductor optical ampliﬁers: theory and application to all-optical ﬂip-ﬂop
memories. IEEE Journal of Quantum Electronics, 39 (1), 141–148.

Duarte, J., Han, S., Harris, P., Jindariani, S., Kreinar, E., Kreis, B., . . . others (2018). Fast
inference of deep neural networks in fpgas for particle physics. Journal of Instrumentation,
13 (07), P07027.

Duarte, M. F., & Eldar, Y. C. (2011). Structured compressed sensing: From theory to applic-

ations. IEEE Transactions on signal processing, 59 (9), 4053–4085.

Feldmann, J., Youngblood, N., Karpov, M., Gehring, H., Li, X., Stappers, M., . . . Bhas-
karan, H. (2021, jan). Parallel convolutional processing using an integrated photonic tensor
core. Nature, 589 (7840), 52–58. Retrieved from http://www.nature.com/articles/
s41586-020-03070-1

Feldmann, J., Youngblood, N., Wright, C. D., Bhaskaran, H., & Pernice, W. (2019). All-
optical spiking neurosynaptic networks with self-learning capabilities. Nature, 569 (7755),
208–214.

Flurin, E., Martin, L. S., Hacohen-Gourgy, S., & Siddiqi, I. (2020). Using a recurrent neural
network to reconstruct quantum dynamics of a superconducting qubit from physical obser-
vations. Physical Review X , 10 (1), 011006.

Fok, M. P., Tian, Y., Rosenbluth, D., & Prucnal, P. R. (2013). Pulse lead/lag timing detection
for adaptive feedback and control based on optical spike-timing-dependent plasticity. Optics
letters, 38 (4), 419–421.

Fok, M. P., & Toole, R. (2018). Photonic implementation of jamming avoidance response.

Google Patents. (US Patent 9,954,619)

Fok, M. P., Wang, Z., Deng, Y., & Prucnal, P. R. (2011). Optical layer security in ﬁber-optic

networks. IEEE Transactions on Information Forensics and Security, 6 (3), 725–736.

Furber, S. B., Galluppi, F., Temple, S., & Plana, L. A.

(2014). The spinnaker project.

Proceedings of the IEEE , 102 (5), 652–665.

George, J. K., Mehrabian, A., Amin, R., Meng, J., De Lima, T. F., Tait, A. N., . . . Sorger,
V. J. (2019). Neuromorphic photonics with electro-absorption modulators. Optics express,
27 (4), 5181–5191.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. The MIT Press.
Goodman, J. W., Leonberger, F. J., Sun-Yuan Kung, & Athale, R. A. (1984). Optical inter-

connections for vlsi systems. Proceedings of the IEEE , 72 (7), 850-866.

Govoreanu, B., Kar, G., Chen, Y., Paraschiv, V., Kubicek, S., Fantini, A., . . . others (2011).
10× 10nm 2 hf/hfo x crossbar resistive ram with excellent performance, reliability and
low-energy operation. In 2011 international electron devices meeting (pp. 31–6).

Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., & Pan, D. Z. (2020). Eﬃcient on-chip
learning for optical neural networks through power-aware sparse zeroth-order optimization.
arXiv preprint arXiv:2012.11148 .

36

Gudaparthi, S., Narayanan, S., Balasubramonian, R., Giacomin, E., Kambalasubramanyam,
H., & Gaillardon, P.-E. (2019). Wire-aware architecture and dataﬂow for cnn accelerators.
In Proceedings of the 52nd annual ieee/acm international symposium on microarchitecture
(pp. 1–13).

Han, J., Jentzen, A., & Weinan, E.

(2018). Solving high-dimensional partial diﬀerential
equations using deep learning. Proceedings of the National Academy of Sciences, 115 (34),
8505–8510.

Hasler, J., & Marr, H. (2013). Finding a roadmap to achieve large neuromorphic hardware
systems. Frontiers in Neuroscience, 7 , 118. Retrieved from https://www.frontiersin
.org/article/10.3389/fnins.2013.00118

Hill, M. T., De Waardt, H., Khoe, G., & Dorren, H. (2001a). All-optical ﬂip-ﬂop based on

coupled laser diodes. IEEE Journal of quantum electronics, 37 (3), 405–413.

Hill, M. T., De Waardt, H., Khoe, G., & Dorren, H. (2001b). Fast optical ﬂip-ﬂop by use of
mach–zehnder interferometers. Microwave and optical technology letters, 31 (6), 411–415.
Hirtzlin, T., Bocquet, M., Penkovsky, B., Klein, J.-O., Nowak, E., Vianello, E., . . . Querlioz,
D. (2020). Digital biologically plausible implementation of binarized neural networks with
diﬀerential hafnium oxide resistive memory arrays. Frontiers in neuroscience, 13 , 1383.
The History of Artiﬁcial Intelligence. (2017, August). Retrieved 2021-03-08, from https://

sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/

Hopﬁeld, J. J., & Tank, D. W. (1986). Computing with neural circuits: A model. Science,

233 (4764), 625–633.

Horowitz, M. (2014). 1.1 computing’s energy problem (and what we can do about it). In
2014 ieee international solid-state circuits conference digest of technical papers (isscc) (pp.
10–14).

Hu, M., Strachan, J. P., Li, Z., Grafals, E. M., Davila, N., Graves, C., . . . Williams, R. S.
(2016). Dot-product engine for neuromorphic computing: Programming 1t1m crossbar to
In 2016 53nd acm/edac/ieee design automation
accelerate matrix-vector multiplication.
conference (dac) (p. 1-6).

Huang, C., Bilodeau, S., Ferreira de Lima, T., Tait, A. N., Ma, P. Y., Blow, E. C., . . . Prucnal,
(2020). Demonstration of scalable microring weight bank control for large-scale

P. R.
photonic integrated circuits. APL Photonics, 5 (4), 040803.

Huang, C., De Lima, T. F., Jha, A., Abbaslou, S., Tait, A. N., Shastri, B. J., & Prucnal, P. R.
(2019). Programmable silicon photonic optical thresholder. IEEE Photonics Technology
Letters, 31 (22), 1834–1837.

Huang, C., Fujisawa, S., De Lima, T. F., Tait, A. N., Blow, E., Tian, Y., . . . others (2020).
Demonstration of photonic neural network for ﬁber nonlinearity compensation in long-haul
transmission systems. In 2020 optical ﬁber communications conference and exhibition (ofc)
(pp. 1–3).

Imani, M., Patil, S., & Rosing, T. (2016). Low power data-aware stt-ram based hybrid cache
architecture. In 2016 17th international symposium on quality electronic design (isqed) (pp.
88–94).

Jaderberg, M., Vedaldi, A., & Zisserman, A. (2014). Speeding up convolutional neural networks
with low rank expansions. CoRR, abs/1405.3866 . Retrieved from http://arxiv.org/abs/
1405.3866

Jha, A., Huang, C., & Prucnal, P. R. (2020). Reconﬁgurable all-optical nonlinear activation

functions for neuromorphic photonics. Optics Letters, 45 (17), 4819–4822.

Jintao Zhang, Zhuo Wang, & Verma, N. (2016). A machine-learning classiﬁer implemented in
a standard 6t sram array. In 2016 ieee symposium on vlsi circuits (vlsi-circuits) (p. 1-2).
Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., . . . Yoon, D. H.
(2017a). In-datacenter performance analysis of a tensor processing unit. In Proceedings of the
44th annual international symposium on computer architecture (p. 1–12). New York, NY,
USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/
3079856.3080246

Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., . . . Yoon, D. H.

37

(2017b, June). In-datacenter performance analysis of a tensor processing unit. SIGARCH
Comput. Archit. News, 45 (2), 1–12. Retrieved from https://doi.org/10.1145/3140659
.3080246

Judd, P., Albericio, J., Hetherington, T., Aamodt, T. M., & Moshovos, A. (2016). Stripes:
In 2016 49th annual ieee/acm international

Bit-serial deep neural network computing.
symposium on microarchitecture (micro) (p. 1-12).

Keyes, R. W. (1985). Optical logic-in the light of computer technology. Optica Acta: Inter-
national Journal of Optics, 32 (5), 525-535. Retrieved from https://doi.org/10.1080/
713821757

Khan, F. N., Fan, Q., Lu, C., & Lau, A. P. T. (2019). An optical communication’s perspective
on machine learning and its applications. Journal of Lightwave Technology, 37 (2), 493–516.
Kim, B., Cho, M.-H., Kim, Y.-G., & Kwon, J.-K. (2010). A 1 v 6-bit 2.4 gs/s nyquist cmos

dac for uwb systems. , 912-915.

Komljenovic, T., Davenport, M., Hulme, J., Liu, A. Y., Santis, C. T., Spott, A., . . . Bowers,
(2016). Heterogeneous silicon photonic integrated circuits. Journal of Lightwave

J. E.
Technology, 34 (1), 20–35.

Kuo, Y.-H., Lee, Y. K., Ge, Y., Ren, S., Roth, J. E., Kamins, T. I., . . . Harris, J. S. (2005).
Strong quantum-conﬁned stark eﬀect in germanium quantum-well structures on silicon.
Nature, 437 (7063), 1334–1336.

LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521 (7553), 436–444.
Li, S., & Pachnicke, S. (2020). Photonic reservoir computing in optical transmission systems.

In 2020 ieee photonics society summer topicals meeting series (sum) (pp. 1–2).

Li, X., Zhang, G., Huang, H. H., Wang, Z., & Zheng, W. (2016, Aug). Performance analysis of
GPU-based convolutional neural networks. In 2016 45th international conference on parallel
processing (icpp) (p. 67-76).

Lin, C.-H., Cheng, C.-C., Tsai, Y.-M., Hung, S.-J., Kuo, Y.-T., Wang, P. H., . . . others (2020).
7.1 a 3.4-to-13.3 tops/w 3.6 tops dual-core deep-learning accelerator for versatile ai applic-
ations in 7nm 5g smartphone soc. In 2020 ieee international solid-state circuits conference-
(isscc) (pp. 134–136).

Lin, J., & Hsieh, C. (2015). A 0.3 v 10-bit 1.17 f sar adc with merge and split switching in 90
nm cmos. IEEE Transactions on Circuits and Systems I: Regular Papers, 62 (1), 70-79.
Lin, P., Li, C., Wang, Z., Li, Y., Jiang, H., Song, W., . . . others (2020). Three-dimensional

memristor circuits as complex neural networks. Nature Electronics, 3 (4), 225–232.

Lin, R., Ge, J., Tran, P., Perea, L. A., Toole, R., & Fok, M. P. (2018). Biomimetic photonics:

jamming avoidance system in eigenmannia. Optics express, 26 (10), 13349–13360.

Lin, X., Rivenson, Y., Yardimci, N. T., Veli, M., Luo, Y., Jarrahi, M., & Ozcan, A. (2018).
All-optical machine learning using diﬀractive deep neural networks. Science, 361 (6406),
1004–1008.

Liou, C., & Hsieh, C. (2013). A 2.4-to-5.2fj/conversion-step 10b 0.5-to-4ms/s sar adc with
charge-average switching dac in 90nm cmos. In 2013 ieee international solid-state circuits
conference digest of technical papers (p. 280-281).

Liu, M., Yin, X., Ulin-Avila, E., Geng, B., Zentgraf, T., Ju, L., . . . Zhang, X. (2011). A

graphene-based broadband optical modulator. Nature, 474 (7349), 64–67.

Liu, Q., & Fok, M. P. (2021). Bio-inspired photonics–marine hatchetﬁsh camouﬂage strategies

for rf steganography. Optics Express, 29 (2), 2587–2596.

Liu, Q., Gao, B., Yao, P., Wu, D., Chen, J., Pang, Y., . . . Wu, H. (2020). 33.2 a fully integrated
analog reram based 78.4tops/w compute-in-memory chip with fully parallel mac computing.
In 2020 ieee international solid- state circuits conference - (isscc) (p. 500-502).

Ma, P. Y., Tait, A. N., Zhang, W., Karahan, E. A., de Lima, T. F., Huang, C., . . . Prucnal,
P. R. (2020). Blind source separation with integrated photonics and reduced dimensional
statistics. Optics Letters, 45 (23), 6494–6497.

Marinella, M. J., Agarwal, S., Hsia, A., Richter, I., Jacobs-Gedrim, R., Niroula, J., . . . James,
C. D.
(2018). Multiscale co-design analysis of energy, latency, area, and accuracy of a
reram analog neural training accelerator. IEEE Journal on Emerging and Selected Topics

38

in Circuits and Systems, 8 (1), 86-101.

McCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E. (2006). A proposal for the
dartmouth summer research project on artiﬁcial intelligence, august 31, 1955. AI magazine,
27 (4), 12–12.

Meng, J., Miscuglio, M., George, J. K., Babakhani, A., & Sorger, V. J. (2019). Electronic bot-
tleneck suppression in next-generation networks with integrated photonic digital-to-analog
converters. Advanced Photonics Research, 2000033.

Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., . . .
others (2014). A million spiking-neuron integrated circuit with a scalable communication
network and interface. Science, 345 (6197), 668–673.

Miller, D. A. (2009). Device requirements for optical interconnects to silicon chips. Proceedings

of the IEEE , 97 (7), 1166–1185.

Miller, D. A. B. (2000). Rationale and challenges for optical interconnects to electronic chips.

Proceedings of the IEEE , 88 (6), 728-749.

Miscuglio, M., Hu, Z., Li, S., George, J. K., Capanna, R., Dalir, H., . . . Sorger, V. J. (2020).

Massively parallel amplitude-only fourier neural network. Optica, 7 (12), 1812–1819.

Miscuglio, M., Mehrabian, A., Hu, Z., Azzam, S. I., George, J., Kildishev, A. V., . . . Sorger,
V. J. (2018). All-optical nonlinear activation function for photonic neural networks. Optical
Materials Express, 8 (12), 3851–3863.

Miscuglio, M., Meng, J., Yesiliurt, O., Zhang, Y., Prokopeva, L. J., Mehrabian, A., . . . Sorger,
V. J. (2020). Artiﬁcial synapse with mnemonic functionality using gsst-based photonic
integrated memory. In 2020 international applied computational electromagnetics society
symposium (aces) (pp. 1–3).

Miscuglio, M., & Sorger, V. J. (2020). Photonic tensor cores for machine learning. Applied

Physics Reviews, 7 (3), 031404.

Moughames, J., Porte, X., Thiel, M., Ulliac, G., Larger, L., Jacquot, M., . . . Brunner, D.
(2020). Three-dimensional waveguide interconnects for scalable integration of photonic
neural networks. Optica, 7 (6), 640–646.

Mukherjee, I., Saurav, K., Nair, P., Shekhar, S., & Lis, M. (2021). A case for emerging memories
in dnn accelerators. In Design, automation & test in europe conference & exhibition (date).
(n.d.). Retrieved from http://web.stanford.edu/~murmann/adcsurvey

Murmann, B.

.html

Nahmias, M. A., De Lima, T. F., Tait, A. N., Peng, H.-T., Shastri, B. J., & Prucnal, P. R.
(2019). Photonic multiply-accumulate operations for neural networks. IEEE Journal of
Selected Topics in Quantum Electronics, 26 (1), 1–18.

Nahmias, M. A., Peng, H.-T., de Lima, T. F., Huang, C., Tait, A. N., Shastri, B. J., &
(2020). A laser spiking neuron in a photonic integrated circuit. arXiv

Prucnal, P. R.
preprint arXiv:2012.08516 .

Niu, M. Y., Boixo, S., Smelyanskiy, V. N., & Neven, H. (2019). Universal quantum control

through deep reinforcement learning. npj Quantum Information, 5 (1), 1–8.

Nozaki, K., Matsuo, S., Fujii, T., Takeda, K., Shinya, A., Kuramochi, E., & Notomi, M. (2019).
Femtofarad optoelectronic integration demonstrating energy-saving signal conversion and
nonlinear functions. Nature Photonics, 13 (7), 454–459.

Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by

learning a sparse code for natural images. Nature, 381 (6583), 607–609.

Peng, H.-T., Nahmias, M. A., De Lima, T. F., Tait, A. N., & Shastri, B. J. (2018). Neur-
omorphic photonic integrated circuits. IEEE Journal of Selected Topics in Quantum Elec-
tronics, 24 (6), 1–15.

Peng Gu, Boxun Li, Tianqi Tang, Yu, S., Yu Cao, Wang, Y., & Yang, H. (2015). Technological
exploration of rram crossbar array for matrix-vector multiplication. In The 20th asia and
south paciﬁc design automation conference (p. 106-111).

Pillai, B. S. G., Sedighi, B., Guan, K., Anthapadmanabhan, N. P., Shieh, W., Hinton, K. J.,
& Tucker, R. S. (2014). End-to-end energy modeling and analysis of long-haul coherent
transmission systems. Journal of Lightwave Technology, 32 (18), 3093–3111.

39

Poisel, R. (2011). Modern communications jamming principles and techniques. Artech House.
Prucnal, P. R., & Shastri, B. J. (2017). Neuromorphic photonics. CRC Press.
Psaltis, D., & Farhat, N. (1985). Optical information processing based on an associative-
memory model of neural nets with thresholding and feedback. Optics Letters, 10 (2), 98–
100.

Psaltis, D., & Wagner, K.

(1987). Multilayer optical learning networks. Applied Optics,

26 (ARTICLE), 5061–5076.

Ramey, C. (2020). Silicon photonics for artiﬁcial intelligence acceleration: Hotchips 32. In

2020 ieee hot chips 32 symposium (hcs) (pp. 1–26).

R´ıos, C., Youngblood, N., Cheng, Z., Le Gallo, M., Pernice, W. H., Wright, C. D., . . . Bhas-
karan, H. (2019). In-memory computing on a photonic platform. Science advances, 5 (2),
eaau5759.

Rosenthal, E. I., Holt, A. L., & Sweeney, A. M. (2017). Three-dimensional midwater camouﬂage
from a novel two-component photonic structure in hatchetﬁsh skin. Journal of The Royal
Society Interface, 14 (130), 20161034.

Sarpeshkar, R.

(1998). Analog versus digital: Extrapolating from electronics to neurobio-
logy. Neural Computation, 10 (7), 1601-1638. Retrieved from https://doi.org/10.1162/
089976698300017052

Scheich, H. (1977). Neural basis of communication in the high frequency electric ﬁsh, eigen-
mannia virescens (jamming avoidance response). Journal of comparative physiology, 113 (2),
229–255.

Schemmel, J., Br¨uderle, D., Gr¨ubl, A., Hock, M., Meier, K., & Millner, S. (2010). A wafer-scale
neuromorphic hardware system for large-scale neural modeling. In 2010 ieee international
symposium on circuits and systems (iscas) (pp. 1947–1950).

Schuman, C. D., Potok, T. E., Patton, R. M., Birdwell, J. D., Dean, M. E., Rose, G. S., &
Plank, J. S. (2017). A survey of neuromorphic computing and neural networks in hardware.
arXiv preprint arXiv:1705.06963 .

Sedighi, B., Khafaji, M., & Scheytt, J. C. S. (2011). 8-bit 5gs/s d/a converter for multi-gigabit

wireless transceivers. , 192-195.

Semenova, N., Larger, L., & Brunner, D. (2021). Noise in trained deep neural networks.
Semenova, N., Porte, X., Andreoli, L., Jacquot, M., Larger, L., & Brunner, D. (2019). Fun-
damental aspects of noise in analog-hardware neural networks. Chaos: An Interdisciplinary
Journal of Nonlinear Science, 29 (10), 103128. Retrieved from https://doi.org/10.1063/
1.5120824

Shainline, J. M., Buckley, S. M., Mirin, R. P., & Nam, S. W.

(2017). Superconducting
optoelectronic circuits for neuromorphic computing. Physical Review Applied , 7 (3), 034013.
Shastri, B. J., Tait, A. N., de Lima, T. F., Pernice, W. H., Bhaskaran, H., Wright, C. D., &
Prucnal, P. R. (2021). Photonics for artiﬁcial intelligence and neuromorphic computing.
Nature Photonics, 15 (2), 102–114.

Shaver, J., & Nichols, T. (2016, September 20). Photonic compressive sensing receiver. Google

Patents. (US Patent 9,450,696)

Shekhar, S. (2021). Tutorial: Silicon photonics - from basics to ASICs. In 2021 ieee interna-

tional solid-state circuits conference (isscc).

Shen, Y., Harris, N. C., Skirlo, S., Prabhu, M., Baehr-Jones, T., Hochberg, M., . . . others
(2017). Deep learning with coherent nanophotonic circuits. Nature Photonics, 11 (7), 441.
Shmel, R. N., & Pace, P. (2017). Photonic compressed sensing nyquist folding receiver. In

2017 ieee photonics conference (ipc) (pp. 633–634).

Skorin-Kapov, N., Furdek, M., Zsigmond, S., & Wosinska, L. (2016). Physical-layer security

in evolving optical networks. IEEE Communications Magazine, 54 (8), 110–117.

Skrzyniarz, S., Fick, L., Shah, J., Kim, Y., Sylvester, D., Blaauw, D., . . . Henry, M. B. (2016).
24.3 a 36.8 2b-tops/w self-calibrating gps accelerator implemented using analog calculation
In 2016 ieee international solid-state circuits conference (isscc) (pp.
in 65nm lp cmos.
420–422).

Song, S., Miller, K. D., & Abbott, L. F. (2000). Competitive hebbian learning through spike-

40

timing-dependent synaptic plasticity. Nature neuroscience, 3 (9), 919–926.

Sorger, V. J., Lanzillotti-Kimura, N. D., Ma, R.-M., & Zhang, X. (2012). Ultra-compact silicon

nanophotonic modulator with broadband response. Nanophotonics, 1 (1), 17–22.

Sorokina, M., Sergeyev, S., & Turitsyn, S.

(2019). Fiber echo state network analogue for

high-bandwidth dual-quadrature signal processing. Optics express, 27 (3), 2387–2395.

Stewart, T. C., DeWolf, T., Kleinhans, A., & Eliasmith, C. (2015). Closed-loop neuromorphic

benchmarks. Frontiers in neuroscience, 9 , 464.

Stewart, T. C., & Eliasmith, C. (2014). Large-scale synthesis of functional spiking neural

circuits. Proceedings of the IEEE , 102 (5), 881–898.

Sun, J., Timurdogan, E., Yaacobi, A., Hosseini, E. S., & Watts, M. R. (2013). Large-scale

nanophotonic phased array. Nature, 493 (7431), 195–199.

Sze, V., Chen, Y.-H., Yang, T.-J., & Emer, J. S. (2017). Eﬃcient processing of deep neural

networks: A tutorial and survey. Proceedings of the IEEE , 105 (12), 2295–2329.

Tait, A. N., De Lima, T. F., Ma, P. Y., Chang, M. P., Nahmias, M. A., Shastri, B. J., . . .
Prucnal, P. R. (2018). Blind source separation in the physical layer. In 2018 52nd annual
conference on information sciences and systems (ciss) (pp. 1–6).

Tait, A. N., De Lima, T. F., Nahmias, M. A., Miller, H. B., Peng, H.-T., Shastri, B. J., &
Prucnal, P. R. (2019). Silicon photonic modulator neuron. Physical Review Applied , 11 (6),
064043.

Tait, A. N., De Lima, T. F., Zhou, E., Wu, A. X., Nahmias, M. A., Shastri, B. J., & Prucnal,
P. R. (2017). Neuromorphic photonic networks using silicon photonic weight banks. Sci-
entiﬁc reports, 7 (1), 1–10.

Tait, A. N., Jayatilleka, H., Lima, T. F. D., Ma, P. Y., Nahmias, M. A., Shastri, B. J., . . .
Prucnal, P. R. (2018, Oct). Feedback control for microring weight banks. Opt. Express,
26 (20), 26422–26443.

Tait, A. N., Nahmias, M. A., Shastri, B. J., & Prucnal, P. R. (2014). Broadcast and weight: an
integrated network for scalable photonic spike processing. Journal of Lightwave Technology,
32 (21), 4029–4041.

Thomson, D., Zilkie, A., Bowers, J. E., Komljenovic, T., Reed, G. T., Vivien, L., . . . others

(2016). Roadmap on silicon photonics. Journal of Optics, 18 (7), 073003.

Toole, R., & Fok, M. P. (2015). Photonic implementation of a neuronal learning algorithm
based on spike timing dependent plasticity. In Optical ﬁber communication conference (pp.
W1K–6).

Toole, R., & Fok, M. P. (2016). A photonic rf jamming avoidance response system bio-inspired
by eigenmannia. In 2016 optical ﬁber communications conference and exhibition (ofc) (pp.
1–3).

Toole, R., Tait, A. N., De Lima, T. F., Nahmias, M. A., Shastri, B. J., Prucnal, P. R., & Fok,
M. P. (2015). Photonic implementation of spike-timing-dependent plasticity and learning
algorithms of biological neural systems. Journal of Lightwave Technology, 34 (2), 470–476.
Torlai, G., Mazzola, G., Carrasquilla, J., Troyer, M., Melko, R., & Carleo, G. (2018). Neural-

network quantum state tomography. Nature Physics, 14 (5), 447–450.

Tran, K. (2016). The era of high bandwidth memory. In 2016 ieee hot chips 28 symposium

(hcs) (pp. 1–22).

VLA Begins Huge Project of Cosmic Discovery. (n.d.). Retrieved 2021-03-07, from https://

public.nrao.edu/news/vla-begins-huge-project/

Wang, C., Zhang, M., Chen, X., Bertrand, M., Shams-Ansari, A., Chandrasekhar, S., . . .
Lonˇcar, M. (2018). Integrated lithium niobate electro-optic modulators operating at cmos-
compatible voltages. Nature, 562 (7725), 101–104.

Wang, J., Zhang, Y., Malacarne, A., Yao, M., Pot`ı, L., & Bogoni, A. (2008). Soa ﬁber ring
laser-based three-state optical memory. IEEE Photonics Technology Letters, 20 (20), 1697–
1699.

Wang, S., Wu, G., Sun, Y., & Chen, J. (2019). Photonic compressive receiver for multiple

microwave frequency measurement. Optics express, 27 (18), 25364–25374.

Wang, Z., & Prucnal, P. R. (2010). Optical steganography over a public dpsk channel with

41

asynchronous detection. IEEE Photonics Technology Letters, 23 (1), 48–50.

Wattanapanitch, W., Fee, M., & Sarpeshkar, R. (2007). An energy-eﬃcient micropower neural
recording ampliﬁer. IEEE Transactions on Biomedical Circuits and Systems, 1 (2), 136-
147.

The WIDAR Supercomputer. (n.d.). Retrieved 2021-03-07, from https://public.nrao.edu/

gallery/the-widar-supercomputer/

Wilhelm, M., Martinovic, I., Schmitt, J. B., & Lenders, V.

(2011). Short paper: reactive
jamming in wireless networks: how realistic is the threat? In Proceedings of the fourth acm
conference on wireless network security (pp. 47–52).

Williamson, I. A., Hughes, T. W., Minkov, M., Bartlett, B., Pai, S., & Fan, S. (2019). Repro-
grammable electro-optic nonlinear activation functions for optical neural networks. IEEE
Journal of Selected Topics in Quantum Electronics, 26 (1), 1–12.

Wu, B., Wang, Z., Tian, Y., Fok, M. P., Shastri, B. J., Kanoﬀ, D. R., & Prucnal, P. R. (2013).
Optical steganography based on ampliﬁed spontaneous emission noise. Optics express,
21 (2), 2065–2071.

Xiao, T. P., Bennett, C. H., Feinberg, B., Agarwal, S., & Marinella, M. J. (2020). Analog
architectures for neural network acceleration based on non-volatile memory. Applied Physics
Reviews, 7 (3), 031301.

Xu, Q., Schmidt, B., Pradhan, S., & Lipson, M. (2005). Micrometre-scale silicon electro-optic

modulator. nature, 435 (7040), 325–327.

Xu, X., Tan, M., Corcoran, B., Wu, J., Boes, A., Nguyen, T. G., . . . Moss, D. J. (2021, jan). 11
TOPS photonic convolutional accelerator for optical neural networks. Nature, 589 (7840),
44–51. Retrieved from http://dx.doi.org/10.1038/s41586-020-03063-0http://www
.nature.com/articles/s41586-020-03063-0

Xue, C.-X., Chen, W.-H., Liu, J.-S., Li, J.-F., Lin, W.-Y., Lin, W.-E., . . . others (2019). 24.1 a
1mb multibit reram computing-in-memory macro with 14.6 ns parallel mac computing time
for cnn based ai edge processors. In 2019 ieee international solid-state circuits conference-
(isscc) (pp. 388–390).

Yang, J. J., Strukov, D. B., & Stewart, D. R. (2013). Memristive devices for computing.

Nature nanotechnology, 8 (1), 13–24.

Yu, S., Sun, X., Peng, X., & Huang, S. (2020). Compute-in-memory with emerging nonvolatile-
In 2020 ieee custom integrated circuits conference

memories: Challenges and prospects.
(cicc) (p. 1-4).

Zhang, S., Yaman, F., Nakamura, K., Inoue, T., Kamalov, V., Jovanovski, L., . . . Wang, T.
(2019). Field and lab experimental demonstration of nonlinear impairment compensation
using neural networks. Nature communications, 10 (1), 1–8.

Zhou, T., Fang, L., Yan, T., Wu, J., Li, Y., Fan, J., . . . Dai, Q.

In situ optical
backpropagation training of diﬀractive optical neural networks. Photonics Research, 8 (6),
940–953.

(2020).

Zhu, Y., Chan, C., Chio, U., Sin, S., U, S., Martins, R. P., & Maloberti, F. (2010). A 10-bit
100-ms/s reference-free sar adc in 90 nm cmos. IEEE Journal of Solid-State Circuits, 45 (6),
1111-1121.

42

