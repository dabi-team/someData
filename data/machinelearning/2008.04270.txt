Sketching semideﬁnite programs for faster clustering

Dustin G. Mixon∗

Kaiying Xie†

0
2
0
2

g
u
A
0
1

]
T
I
.
s
c
[

1
v
0
7
2
4
0
.
8
0
0
2
:
v
i
X
r
a

Abstract

Many clustering problems enjoy solutions by semideﬁnite programming. Theoreti-
cal results in this vein frequently consider data with a planted clustering and a notion
of signal strength such that the semideﬁnite program exactly recovers the planted clus-
tering when the signal strength is suﬃciently large. In practice, semideﬁnite programs
are notoriously slow, and so speedups are welcome. In this paper, we show how to
sketch a popular semideﬁnite relaxation of a graph clustering problem known as mini-
mum bisection, and our analysis supports a meta-claim that the clustering task is less
computationally burdensome when there is more signal.

1

Introduction

In many data science applications, one is tasked with partitioning objects into clusters so
that members of a common cluster are more similar than members of diﬀerent clusters.
For example, given a graph, one might cluster the vertices in such a way that most edges
are between vertices from a common cluster. At the same time, one ought to ensure that
clusters are appropriately balanced in size, since otherwise a cluster could degenerate to a
single vertex. There are a variety of graph clustering objectives that simultaneously penalize
edges across clusters and varying sizes of clusters, and for each objective, there are families
of graphs for which ﬁnding an optimal clustering appears to be computationally diﬃcult.
For example, given a graph with an even number of vertices, one might bisect the vertex set
in such a way that minimizes the number of edges across clusters. This minimum bisection
problem is known to be NP-hard [5].

Recently, it has been popular to demonstrate instances of the meta-claim that clustering
is only diﬃcult when it doesn’t matter. For example, while one can encode hard instances of
the traveling salesman problem as instances of minimum bisection, these sorts of instances
would never appear in the context of real-world data science. Rather, a data scientist will
cluster data that is meant to be clustered. With this perspective in mind, researchers have
studied how a variety of clustering algorithms perform for datasets with a planted cluster-
ing structure. For example, one might attempt to solve the minimum bisection problem
for random graphs drawn from the stochastic block model, in which edges are drawn within
planted communities at a higher rate than edges across planted communities. In this setting,

∗Department of Mathematics, The Ohio State University, Columbus, OH
†Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH

1

 
 
 
 
 
 
it was shown in [1, 3] that a slight modiﬁcation of the Goemans–Williamson semideﬁnite
program [6] exactly recovers the planted clustering whenever it is information theoretically
feasible to do so; see Proposition 1 for details. Similar approaches have treated other semidef-
inite programming–based clustering algorithms in various settings [2, 9, 12, 10, 11].

Many of these results take the following form: “Given enough signal, the semideﬁnite
program exactly recovers the planted clusters.” In the context of graph clustering, “signal”
refers to the extent to which there are more edges within clusters than across clusters (in an
appropriate quantitative sense). In this paper, we pose a subtler meta-claim:

Clustering is easier when there is more signal.

Indeed, while previous results determined how much signal is necessary and suﬃcient for
clustering to be computationally feasible, the above meta-claim suggests that the computa-
tional burden should decline gracefully with additional signal. This makes intuitive sense
considering it’s easier to ﬁnd a needle in a haystack when the haystack contains more nee-
dles. Such behavior is particularly welcome in the context of semideﬁnite programming, as
solvers are notoriously slow for large datasets despite having polynomial runtime. The goal
of this paper is to demonstrate this meta-claim in the special case of minimum bisection by
semideﬁnite programming under the stochastic block model. In other words, we provide a
method to systematically decrease runtime for instances with more signal.

We start by formally deﬁning the stochastic block model. Let G ∼ SBM(n1, n2, p, q)
denote a random graph with vertex set V (G) = S1 (cid:116) S2 such that |S1| = n1 and |S2| = n2
and whose edges are independent Bernoulli random variables. For every pair of vertices, if
they reside in the same community Si, we draw an edge between them with probability p,
and otherwise the edge probability is q. If p > q, then this models how a social network
exhibits more connections within a community than between communities. In our problem,
we do not have access to the partition {S1, S2}. One might randomize the communities to
model this lack of information, but we do not bother with this formality here.

In the special case where n1 = n2, one may exactly recover {S1, S2} from G provided
p is appropriately large compared to q. To do so, we follow [1, 3] by encoding G with the
matrix B deﬁned by Bij = 1 if i ↔ j, Bii = 0, and otherwise Bij = −1, and we then solve
the program

maximize x(cid:62)Bx subject to 1(cid:62)x = 0,

x ∈ {±1}n.

(1)

This corresponds to ﬁnding the minimum bisection of G, and one may show that the optimiz-
), where { ˆS1, ˆS2} is the maximum
ers of this combinatorial program take the form ±(1 ˆS1
likelihood estimator of {S1, S2}. In pursuit of a computationally eﬃcient alternative, one is
inclined to consider a semideﬁnite program obtained by lifting X := xx(cid:62) and relaxing:

−1 ˆS2

maximize

tr(BX)

subject to

diag(X) = 1, X (cid:23) 0.

(2)

Impressively, this relaxation exactly recovers {S1, S2} in the regime in which it is information
theoretically feasible to do so:

Proposition 1 ([1, 3]). Select α > β > 0 and for each n ∈ 2N, draw G ∼ SBM(n/2, n/2, p, q)
with p = (α log n)/n and q = (β log n)/n and with planted communities {S1, S2}.

(a) If

√

α −

√

β >

√

2, then (2) recovers {S1, S2} from G with probability 1 − o(1).

2

(b) If

√

α −

√

β <

√

2, then it is impossible to recover {S1, S2} from G.

This phase transition partitions the set of all (α, β) into two regimes: one in which
{S1, S2} can be recovered from G in polynomial time by semideﬁnite programming, and
another in which no algorithm exists (not even an ineﬃcient one) that recovers {S1, S2}
from G. Recalling our meta-claim, we would like to show that in the former case, a larger
choice of α for a ﬁxed β makes it more computationally eﬃcient to recover {S1, S2} from G.
In this spirit, we ﬁrst consider Figure 1. This ﬁgure illustrates that the SDP solver empirically
behaves according to our meta-claim, taking less time to identify the clustering when there
is more signal available. In this paper, we do not explain this speciﬁc phenomenon. Instead,
we analyze a modiﬁcation to the SDP algorithm using an idea that might be transferred
more easily to other settings. In fact, Figure 1 also illustrates that our modiﬁcation provides
a substantial speedup over the original SDP.

Recall the general sketch-and-solve approach: Given a large problem, we

1. randomly project to a smaller version of the same problem,

2. solve the smaller version of the problem, and then

3. use the solution to the smaller problem to (approximately) solve the original problem.

Notice that step 2 promises to be faster since the size of the problem is smaller. This approach
has been particularly eﬀective in approximately solving large least squares problems [15], and
some work has been done to transfer these ideas to the setting of semideﬁnite programs [16, 4].
We will apply the sketch-and-solve approach to systematically decrease the computational
burden of clustering given more signal. In the next section, we show how to perform steps 1
and 3 above, thereby reducing our task to solving step 2. Speciﬁcally, we sketch by passing
to the subgraph induced by a random subset of vertices, but in doing so, we no longer
have communities of equal size. This motivates the study of the unbalanced stochastic
block model, and in Section 3, we show a slight modiﬁcation of (2) exactly recovers the
communities in this setting. In Section 4, we combine the ideas and results in Sections 2
and 3 to state and prove our main result. As we will see, we can aﬀord to sketch down
to a smaller subgraph when there is more signal, which corroborates our meta-claim. We
conclude in Section 5 with a discussion.

2 Exact recovery from a sketch oracle

As discussed in the previous section, our approach is to sketch the original graph to a smaller
graph, solve the clustering problem for the smaller graph, and then use these small clusters
to determine a clustering for the entire graph. For the last step, we will assign each vertex
to the small cluster that it shares more edges with. The following lemma indicates that
this sketch-and-solve approach identiﬁes the planted clusters provided (1) the small graph
we sketch to is not too small relative to the signal (measured in terms of the sketching
parameter γ relative to the edge density parameters α and β) and (2) we correctly identify
the planted small clusters R1 and R2. We discuss how to accomplish (2) in the next section.

3

Figure 1:
For each α ∈ {2, 4, . . . , 50} and β ∈ {1, 2, . . . , 10}, perform the follow-
ing experiment 10 times and plot the results. Draw a random graph with distribution
SBM(n1 = 150, n2 = 150, p = (α log n)/n, q = (β log n)/n). Attempt to exactly recover
the planted communities by solving the Abbe–Bandeira–Hall SDP (2) in CVX [7]. The
proportion of recovery is displayed in (top left), while the average runtime (in seconds) is
displayed in (top right). Next, attempt to exactly recover the planted communities using
the sketch-and-solve method described in this paper; see Section 5 for details. The recovery
rate and average runtime are displayed in (bottom left) and (bottom right), respectively.
The red curve depicts the phase transition from Proposition 1. For both approaches, the
runtime is smaller when α (cid:29) β, and more dramatically so for the sketch-and-solve method.

4

Lemma 2. Draw G ∼ SBM(n/2, n/2, p, q) with planted communities {S1, S2} and with p =
(α log n)/n and q = (β log n)/n, where α > β > 0. Draw vertices V at random according to
a Bernoulli process with rate γ and put Ri := Si ∩ V for both i ∈ {1, 2}. Let e(v, S) denote
the number of edges in G between v and S ⊆ V (G) and take

ˆSi := Ri ∪ {v ∈ V (G) \ V : e(v, Ri) > e(v, R3−i)}.

Then ( ˆS1, ˆS2) = (S1, S2) with probability 1 − o(1) provided

γ >

8
3

·

2α + β
(α − β)2 .

Proof. Let E denote the success event. After conditioning on V , the union bound gives

P(E c|V ) ≤

(cid:88)

(cid:88)

i∈{1,2}

v∈Si

1{v∈V (G)\V } · P({e(v, Ri) ≤ e(v, R3−i)}|V ).

Conditioned on V , then for each v ∈ Si ∩ (V (G) \ V ), the quantity e(v, Ri) − e(v, R3−i) takes
the form

Ki(cid:88)

K3−i
(cid:88)

B(p)

j −

B(q)
j

,

j=1

j=1

where Ki := |Ri| and the terms in the sums are independent Bernoulli random variables with
rate indicated by the superscript. The mean of this sum is Kip − K3−iq. After centering,
each term has absolute value at most 1 almost surely, and the variance of the sum is

Kip(1 − p) + K3−iq(1 − q) ≤ Kip + K3−iq.

If in addition to {v ∈ Si ∩ (V (G) \ V )} we restrict to the event Ei := {Kip − K3−iq ≥ 0},
then we may apply Bernstein’s inequality for bounded variables (see Theorem 2.8.4 in [14]):

P({e(v, Ri) − e(v, R3−i) ≤ 0}|V ) ≤ exp

−

(cid:18)

(cid:18)

≤ exp

−

(cid:19)

(Kip − K3−iq)2/2
Kip + K3−iq + (Kip − K3−iq)/3
(K(p − q) − J(p + q))2/2
3q) + J( 4
K( 4
3q)

3p − 2

3p + 2

(cid:19)
,

where K := K1+K2
selected later. Then on the event {v ∈ Si ∩ (V (G) \ V )} ∩ Ei ∩ E3, it further holds that

|. Denote E3 := {J ≤ (cid:15)K} for some (cid:15) ∈ (0, α−β

and J := | K1−K2

α+β ) to be

2

2

P({e(v, Ri) − e(v, R3−i) ≤ 0}|V ) ≤ exp

−

(cid:18)

(cid:18)

≤ exp

−

5

3p + 2

(K(p − q) − J(p + q))2/2
K( 4
3p − 2
3q) + J( 4
3q)
((p − q) − (cid:15)(p + q))2/2
3q) + (cid:15)( 4
3p + 2
( 4
3q)

3p − 2

(cid:19)

(cid:19)

· K

=: ep,q,(cid:15)(K).

Overall, we may bound the failure probability:

P(E c) = E[P(E c|V )] ≤ E

(cid:20) (cid:88)

(cid:88)

1{v∈V (G)\V } · P({e(v, Ri) ≤ e(v, R3−i)}|V )

(cid:21)

i∈{1,2}

v∈Si

(cid:20) (cid:88)

≤ E

(cid:88)

1{v∈V (G)\V } ·

(cid:16)

1(Ei∩E3)c + 1Ei∩E3 · ep,q,(cid:15)(K)

(cid:17)(cid:21)

i∈{1,2}

v∈Si

(cid:20) (cid:88)

≤ E

(cid:88)

(cid:16)

1(Ei∩E3)c + ep,q,(cid:15)(K)

(cid:17)(cid:21)

=

n
2

v∈Si
i∈{1,2}
(cid:17)
(cid:16)
(cid:88)
P((Ei ∩ E3)c) + E[ep,q,(cid:15)(K)]

,

i∈{1,2}

where the last inequality uses ep,q,(cid:15)(K) ≥ 0. We will ﬁnd (cid:15) ∈ (0, α−β

α+β ) such that

P(E c

1) = e−Ω(n),

P(E c

2) = e−Ω(n),

P(E c

3) = e−Ω(n),

E[ep,q,(cid:15)(K)] = o(1/n),

from which it follows that P(E c) = o(1), as desired.

For the ﬁrst three estimates, it will be helpful to ﬁrst consider independent Bernoulli

variables X1, . . . , Xn/2 and Y1 . . . , Yn/2, all with rate γ. Given a > b > 0, we bound

f (a, b) := P

(cid:26) n/2
(cid:88)

j=1

aXj −

(cid:27)

bYj < 0

.

n/2
(cid:88)

j=1

The expectation of the sum is n
absolute value at most a almost surely. The variance of the sum is n
such, Bernstein’s inequality for bounded variables gives

2 (a − b)γ, and after centering, each term in the sum has
2 (a2 + b2)γ(1 − γ). As

(cid:18)

f (a, b) ≤ exp

−

( n
2 (a − b)γ)2/2
2 (a2 + b2)γ(1 − γ) + a · n

n

2 (a − b)γ/3

(cid:19)

.

Notice that P(E c

1) = P(E c

2) = f (p, q) = f ( α log n

n

, β log n
n

). Simplifying then gives

P(E c

i ) ≤ exp

(cid:18)

−

( 1
2(α − β)γ)2/2
2(α2 + β2)γ(1 − γ) + α · 1

1

2(α − β)γ/3

(cid:19)
,

· n

which is e−Ω(n) since α > β. Next,

P(E c

3) = P{J > (cid:15)K}

= P{|K1 − K2| > (cid:15)(K1 + K2)}
≤ P{K1 − K2 > (cid:15)(K1 + K2)} + P{K2 − K1 > (cid:15)(K1 + K2)}
= P{(1 + (cid:15))K2 − (1 − (cid:15))K1 < 0} + P{(1 + (cid:15))K1 − (1 − (cid:15))K2 < 0}
= 2 · f (1 + (cid:15), 1 − (cid:15)),

which is e−Ω(n) since (cid:15) ∈ (0, 1).

6

It remains to show that E[ep,q,(cid:15)(K)] = o(1/n). Since K = |V |/2, we may write

(cid:18)

ep,q,(cid:15)(K) = exp

((p − q) − (cid:15)(p + q))2/4
3q) + (cid:15)( 4
3p + 2
( 4
3q)
We may interpret E[ep,q,(cid:15)(K)] as the moment generating function of |V | evaluated at a point.
Since |V | has binomial distribution over n trials with rate γ, this gives

3p − 2

· |V |

−

.

(cid:19)

E[ep,q,(cid:15)(K)] =

(cid:20)

(cid:18)

1 − γ + γ · exp

−

((p − q) − (cid:15)(p + q))2/4
( 4
3q) + (cid:15)( 4
3p + 2
3q)

3p − 2

(cid:19)(cid:21)n

=: ((1 − γ) + γe−c(log n)/n)n,

where c := ((α−β)−(cid:15)(α+β))2/4
3 α− 2
3 α+ 2
( 4
3 β)

3 β)+(cid:15)( 4

. Next, we compare to linear approximations to obtain

((1 − γ) + γe−c(log n)/n)n = exp(n log((1 − γ) + γe−c(log n)/n))
≤ exp(n((1 − γ) + γe−c(log n)/n − 1))
= exp(−γn(1 − e−c(log n)/n)) = exp(−γn · (1 − o(1)) · c log n

n ).

Finally, by our assumption on γ, it holds that γ > 1/c for every suﬃciently small (cid:15) > 0.
Taking any such (cid:15) gives that the above quantity is o(1/n), as desired.

3 The unbalanced stochastic block model

Lemma 2 reduces our problem of exact recovery of balanced communities in the stochastic
block model to a smaller problem. Unfortunately, by sketching to a random subgraph, the
planted communities are no longer balanced, and so we cannot naively apply Proposition 1.
Still, considering the overwhelming success of (2) in the balanced case, one is inclined to try
some version of it in the more general case.

To this end, we ﬁrst note that the choice of B = 2A − J + I in (1) as an encoding of G is
somewhat arbitrary; here, A denotes the adjacency matrix of G, and J denotes the all-ones
matrix. Suppose we replaced B with any ˜B = aA + bJ + cI with a > 0. Then the constraints
1(cid:62)x = 0 and x ∈ {±1}n together ensure that

x(cid:62) ˜Bx = x(cid:62)(aA + bJ + cI)x = a · x(cid:62)Ax + c · n.

Observe that the maximizer in (1) is the same for all such objectives. However, we can
expect diﬀerent choices of ˜B to produce diﬀerent optimizers once we relax to the SDP. Of
course, there is no change to the SDP if we change c since X is constrained to have all-ones
diagonal, and so we take c = 0 for simplicity. Next, we can rescale ˜B so that a = 1 without
changing the SDP. Overall, we are interested in encodings of the form ˜B = A − µJ. The
choice µ = 1
2 corresponds to the Abbe–Bandeira–Hall encoding [1]. Alternatively, we could
run the Goemans–Williamson relaxation of maximum cut on the complement of G, which
corresponds to taking µ = 1 [6]. These two SDPs behave similarly for G ∼ SBM(n1, n2, p, q)
when n1 = n2, but as Figure 2 illustrates, the Abbe–Bandeira–Hall SDP performs better in
the unbalanced case. Curiously, it appears that the choice µ = (p + q)/2 exhibits the phase
transition from Proposition 1.

7

Figure 2:
Succes rates for exact recovery under SBM(n1 = 100, n2 = 200, p =
(α log n)/n, q = (β log n)/n). (left) Goemans–Williamson SDP. (center) Abbe–Bandeira–
Hall SDP. (right) Proposed SDP, which assumes access to the value of (p + q)/2. The red
2, which by Proposition 1 is the phase transition for
curve depicts the curve
exact recovery in the balanced case. It appears that the same phase transition holds for the
proposed SDP in the unbalanced case.

β =

α −

√

√

√

In this paper, we consider the family semideﬁnite programs

maximize

tr((A − µJ)X)

subject to

diag(X) = 1, X (cid:23) 0.

((A, µ)-SDP)

Similar to [1], we pass to the dual program to obtain an optimality condition:

Lemma 3. Let A denote the adjacency matrix of a simple graph G on n vertices. Partition
partition the vertices S1 (cid:116) S2 = V (G) and put n1 := |S1|, n2 := |S2|, and g := 1S1 − 1S2.
Let G+ denote the subgraph of G with edge set E(S1, S1) ∪ E(S2, S2), and let G− denote the
subgraph with edge set E(S1, S2). Finally, let D+ and D− denote the diagonal matrices of
vertex degrees in G+ and G−. If the matrix

D+ − D− − µ(n1 − n2) diag(g) − A + µJ

is positive semideﬁnite with rank n − 1, then gg(cid:62) is the unique solution to (A, µ)-SDP.

Proof. The dual of (A, µ)-SDP is given by

minimize

tr(Y )

subject to Y (cid:23) A − µJ,

Y diagonal.

To verify weak duality, we have

tr((A − µJ)X) ≤ tr(Y X) = tr(Y ),

where the ﬁrst inequality follows from the semideﬁnite constraints, and the equality follows
from the diagonal constraints. By the hypotheses of the lemma, we see that

X0 := gg(cid:62),

Y0 := D+ − D− − µ(n1 − n2) diag(g)

8

are primal- and dual-feasible, respectively. As such, by rearranging the above inequality, it
suﬃces to show that equality in

tr((Y0 − A + µJ)X) ≥ 0

holds with primal-feasible X if and only if X = X0. For the “if” direction, note that

tr(Y0) = tr(D+) − tr(D−) − µ(n1 − n2) tr(diag(g)) = g(cid:62)Ag − µ(n1 − n2)2,

and so

tr((Y0 − A + µJ)X0) = g(cid:62)(Y0 − A + µJ)g = tr(Y0) − g(cid:62)Ag + µ(g(cid:62)1)2 = 0.

For the “only if” direction, put Z := Y0 − A + µJ. Then by assumption, Z is positive
semideﬁnite, and we just showed that g(cid:62)Zg = 0. It follows that Zg = 0. Furthermore, Z
has rank n − 1 by assumption, and so the nullspace of Z equals the span of g. Suppose X is
primal-feasible with tr(ZX) = 0, and consider the spectral decompositions X = (cid:80)
i λixix(cid:62)
i
and Z = (cid:80)

j . Note that 0 = µ1 < µ2 ≤ µn and z1 = n−1/2 · g. Then

j µjzjz(cid:62)

0 = tr(ZX) =

(cid:88)

λi

(cid:88)

i

j>1

µj|(cid:104)xi, zj(cid:105)|2.

Since each term in the outer sum is nonnegative, it must hold that for each i, either λi = 0
or |(cid:104)xi, zj(cid:105)|2 = 0 for every j > 1. Considering {zj}j∈[n] is an orthonormal basis for Rn, we see
that |(cid:104)xi, zj(cid:105)|2 = 0 for every j > 1 only if xi is a scalar multiple of z1. On the other hand,
{xi}i∈[n] is also an orthonormal basis, and so there is at most one such xi. It follows that X
has rank at most 1, and in particular, X is a scalar multiple of gg(cid:62). The diagonal constraint
on the primal-feasible X then implies that X = gg(cid:62) = X0, as desired.

Next, we apply Lemma 3 to the stochastic block model to show when the SDP exactly

recovers the planted clusters. Notice the appearance of p+q

2 as a threshold on µ:

Lemma 4. Take n1, n2 ≥ 1 and p > q > 0, put n := n1 + n2 and m := |n1 − n2|, draw
G ∼ SBM(n1, n2, p, q) with planted communities {S1, S2}, let A denote the adjacency matrix
of G, and put g = 1S1 − 1S2. If µ > q, then gg(cid:62) is the unique solution to (A, µ)-SDP with
probability at least






((µ−q)n)2
1 − 2n exp(− 3
(3p+q+2µ)n+3(p−q)m )
2 ·
16 · ((p−q)n−(2µ−(p+q))m)2
(2p+q)n+(2p−q−µ)m )

1 − 2n exp(− 3

if µ < p+q
2

if µ ≥ p+q
2 .

Proof. By Lemma 3, it suﬃces to show that with high probability, the random matrix

Z := D+ − D− − µ(n1 − n2) diag(g) − A + µJ

is positive semideﬁnite with rank n − 1. As established in the proof of Lemma 3, it holds
that Zg = 0 almost surely. It remains to show that the second-smallest eigenvalue λ2(Z) of
Z is strictly positive with high probability. Weyl’s inequality (see Theorem 4.3.1 in [8]) gives

λ2(Z) = λ2(EZ + Z − EZ) ≥ λ2(EZ) − (cid:107)Z − EZ(cid:107)2→2.

9

To continue, we determine the exact value of λ2(EZ), and then we use matrix Bernstein to
bound (cid:107)Z − EZ(cid:107)2→2 in a high-probability event.

To compute λ2(EZ),

it is helpful to assume (without loss of generality) that S1 =

{1, . . . , n1} and S2 = {n1 + 1, . . . , n}. We ﬁrst write the matrix in block form:

EZ =

(cid:20) (a1 − b)In1 + b1n11(cid:62)
c1n21(cid:62)
n1

n1

c1n11(cid:62)
n2
(a2 − b)In2 + b1n21n(cid:62)

2

(cid:21)

,

where the matrix entries are given by

a1 = p(n1 − 1) − qn2 − µ(n1 − n2) + µ,
a2 = p(n2 − 1) − qn1 + µ(n1 − n2) + µ,

b = −p + µ,
c = −q + µ.

From this block form, we see that for each i ∈ {1, 2}, every vector supported on Si that is
orthogonal to 1Si is an eigenvector with eigenvalue ai − b. One may simplify to obtain

a1 − b = (p − µ)n1 + (µ − q)n2,

a2 − b = (p − µ)n2 + (µ − q)n1.

Considering g = 1S1 − 1S2 is an eigenvector with eigenvalue 0, the remaining eigenvector h
must reside in the span of 1S1 and 1S2 and simultaneously be orthogonal to 1S1 − 1S2. As
such, we may take h = n21S1 + n11S2, and multiplying by EZ reveals that the corresponding
eigenvalue is (µ − q)n. Observe that this is the second-smallest eigenvalue provided it is
nonnegative and p − µ > µ − q, i.e., q ≤ µ < (p + q)/2. On the other hand, if µ ≥ (p + q)/2,
then the second smallest eigenvalue is the smaller of a1 − b and a2 − b. Overall, we have
λ2(EZ) ≤ 0 if µ < q and otherwise
(cid:26)

(µ − q)n
(p − µ) max(n1, n2) + (µ − q) min(n1, n2)

if q ≤ µ < (p + q)/2,
if µ ≥ (p + q)/2.

λ2(EZ) =

Notice that in the case µ ≥ (p + q)/2, we we may simplify this expression:

λ2(EZ) = (p − µ) n+m

2 + (µ − q) n−m

2 = p−q

2

· n − (µ − p+q

2 ) · m.

It remains to obtain a high-probability upper bound on (cid:107)Z − EZ(cid:107)2→2. To accomplish

this, we will apply matrix Bernstein. First, we express Z − EZ as a random series:

Z − EZ =

Mij :=

(cid:88)

i,j∈[n]
i<j

ij − p)(ei − ej)(ei − ej)(cid:62) −

(cid:88)

(B+
1 ∪S2
2

(i,j)∈S2
i<j

(B−

ij − q)(ei + ej)(ei + ej)(cid:62).

(cid:88)

i∈S1
j∈S2

Then EMij = 0 and (cid:107)Mij(cid:107)2→2 ≤ 2 almost surely for every i, j ∈ [n] with i < j. Next,
(cid:26) 2p(1 − p)(ei − ej)(ei − ej)(cid:62) if (i, j) ∈ S2

1 ∪ S2
2q(1 − q)(ei + ej)(ei + ej)(cid:62) if (i, j) ∈ S1 × S2,

2, i < j

EM 2

ij =

and so we may write the sum in block form:

EM 2

ij =

(cid:20) c1In1 − 2p(1 − p)1n11(cid:62)
2q(1 − q)1n21(cid:62)
n1

n1

2q(1 − q)1n11(cid:62)
n2
c2In2 − 2p(1 − p)1n21(cid:62)
n2

(cid:21)

,

(cid:88)

i,j∈[n]
i<j

10

where c1 = 2p(1 − p)n1 + 2q(1 − q)n2 and c2 = 2p(1 − p)n2 + 2q(1 − q)n1. As before, every
vector supported on Si that is orthogonal to 1Si is an eigenvector with eigenvalue ci. The
other two eigenvectors reside in the span of 1S1 and 1S2. Since g is in the nullspace, the
remaining eigenvector is h = n21S1 + n11S2. All together, the eigenvalues are






2p(1 − p)n1 + 2q(1 − q)n2 with multiplicity n1 − 1
2p(1 − p)n2 + 2q(1 − q)n1 with multiplicity n2 − 1

0
2q(1 − q)n

with multiplicity 1
with multiplicity 1.

As expected, all of these eigenvalues are nonnegative. Furthermore, since p > q, the largest
eigenvalue is

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

EM 2
ij

i,j∈[n]
i<j

(cid:13)
(cid:13)
(cid:13)
(cid:13)2→2

= 2p(1 − p) max(n1, n2) + 2q(1 − q) min(n1, n2)

≤ 2p max(n1, n2) + 2q min(n1, n2) = (p + q)n + (p − q)m.

We are now ready to apply the matrix Bernstein inequality (see Theorem 1.6.2 in [13]):

P{(cid:107)Z − EZ(cid:107)2→2 ≥ t} ≤ 2n exp(−

t2/2
(p+q)n+(p−q)m+2t/3).

The result then follows by taking t := λ2(EZ) and simplifying.

As an aside, we point out the following corollary: If we know p+q

2 , then we can solve
the unbalanced case by semideﬁnite programming, though more signal is required when the
communities are less balanced (at least for this result).

Corollary 5. Select α > β > 0 and δ ≥ 0. For each n1, n2 ∈ N with |n1 − n2| ≤ δ(n1 + n2),
put n = n1+n2, p = (α log n)/n and q = (β log n)/n, draw G ∼ SBM(n1, n2, p, q) with planted
communities {S1, S2}, let A denote the adjacency matrix of G, and put g = 1S1 − 1S2. Then
gg(cid:62) is the unique solution to (A, p+q

2 )-SDP with probability 1 − o(1) provided

3(α − β)2 > 16(2α + β) + 24(α − β)δ.

Proof. Put m = |n1 − n2|. By Lemma 4, the success probability is at least

1 − 2n exp(− 3

16 · ((p−q)n−(2µ−(p+q))m)2

(2p+q)n+(2p−q−µ)m ) = 1 − 2n exp(− 3
≥ 1 − 2n exp(− 3

16 ·
16 ·
16 ·

((p−q)n)2
(2p+q)n+(3/2)(p−q)m )
((p−q)n)2
(2p+q)n+(3/2)(p−q)δn )
(2α+β)+(3/2)(α−β)δ − 1) · log n),

(α−β)2

= 1 − 2 exp(−( 3

which is 1 − o(1) by our assumption on (α, β, δ).

In the special case where δ = 0, this threshold matches the guarantee provided in [1].
Perhaps surprisingly, the true phase transition appears to be independent of δ; see Figure 2.

11

4 Main result

We are now ready to state our main result, which combines our sketching approach with our
solver for the sketched problem:

2

Theorem 6 (main result). Draw G ∼ SBM(n/2, n/2, p, q) with planted communities {S1, S2}
and with p = (α log n)/n and q = (β log n)/n, where α > β > 0. Consider the random
(cid:1), where E denotes the random edge set of G. Next, draw vertices
variable µ := |E|/(cid:0)n
V ⊆ V (G) at random according to a Bernoulli process with rate γ, and let A denote the
adjacency matrix of the random subgraph induced by V . In the event that the solution to
(A, µ)-SDP is unique and takes the form gg(cid:62) for some g ∈ {±1}V , select ˆR1 (cid:116) ˆR2 = V
, and otherwise let { ˆR1, ˆR2} be a random partition of V . Next, let
such that g = 1 ˆR1
ˆSi denote ˆRi union the vertices in V (G) \ V that share more edges with ˆRi than with ˆR3−i.
Provided

− 1 ˆR2

γ >

16
3

·

2α + β
(α − β)2 ,

it holds that { ˆS1, ˆS2} = {S1, S2} with probability 1 − o(1).

In words, if α (cid:29) β, then for small choices of γ, we can sketch down to a subgraph with
approximately γn vertices before solving the SDP. Since the runtime of the SDP is sensitive
to the size of the problem instance, this promises to provide a substantial speedup. To prove
Theorem 6, we ﬁrst need a version of Lemma 4 that accounts for the randomness in our
sketching process:

Lemma 7. Draw G ∼ SBM(n/2, n/2, p, q) with planted communities {S1, S2} and with p =
(α log n)/n and q = (β log n)/n, where α > β > 0. Draw vertices V ⊆ V (G) at random
according to a Bernoulli process with rate γ. Let A denote the adjacency matrix of the random
subgraph induced by V . Suppose there exists (cid:15) such that β < (cid:15) < (n/ log n)µ < 2α − β, and
put η := ( α+β
2 − (cid:15))+. Then with probability 1 − o(1), the solution to (A, µ)-SDP is unique
and identiﬁes {S1 ∩ V, S2 ∩ V } provided

γ >

16
3

·

2α + β − η
(α − β − 2η)2 .

(3)

Before proving this lemma, we provide the idea of the proof. There are two sources
of randomness, namely, the problem instance G ∼ SBM(n/2, n/2, p, q) and the sketching
variables B1, . . . , Bn ∼ Bernoulli(γ), all of which are independent. Consider the subgraph
G[V ] of G induced by V := {i ∈ [n] : Bi = 1}. Conditioned on the event {V = V0}, we have

G[V0] ∼ SBM(|S1 ∩ V0|, |S2 ∩ V0|, p, q)

with planted communities {S1 ∩V0, S2 ∩V0}. By virtue of this conditioning, we may appeal to
Lemma 4. Since our bound on the success probability only depends on the sizes of S1 ∩V and
S2 ∩ V , we can simply condition on these sizes. The sizes are similar with high probability,
in which case the bound in Lemma 4 gives that we succeed with high probability. Since
Lemma 4 is broken up into cases, it is a somewhat technical exercise to make this last
statement rigorous:

12

Proof of Lemma 7. Put g := 1S1∩V − 1S2∩V , and let S denote the event that gg(cid:62) is the
unique solution to (A, µ)-SDP. We condition on K1 := |S1 ∩ V | and K2 := |S2 ∩ V |, which
are independent of each other:

P(S c) =

n/2
(cid:88)

n/2
(cid:88)

k1=0

k2=0

P(S c|{K1 = k1} ∩ {K2 = k2}) · P{K1 = k1} · P{K2 = k2}.

(4)

Notice that K1, K2 ∼ Binomial( n
at γn

2 . With this intuition, we take δ > 0 (to be selected later) and denote

2 , γ), and so we expect these random variables to concentrate

I := {k ∈ N : (γ−δ)n

2 ≤ k ≤ (γ+δ)n

2

}.

We use this interval to split the sum (4) into two parts:

P(S c) =

(cid:88)

+

(cid:88)

(k1,k2)∈I×I

k1,k2∈{0,...,n/2}
(k1,k2)(cid:54)∈I×I

(cid:88)

≤

(k1,k2)∈I×I

P(S c|{K1 = k1} ∩ {K2 = k2}) · P{K1 = k1} · P{K2 = k2}

+ 2 · P{|K − γn

2 | > δn
2 }

(cid:16)

≤

P{K ∈ I}

(cid:17)2

· max
(k1,k2)∈I×I

P(S c|{K1 = k1} ∩ {K2 = k2}) + 2 · P{|K − γn

2 | > δn
2 }

≤ max

(k1,k2)∈I×I

P(S c|{K1 = k1} ∩ {K2 = k2}) + 2 · P{|K − γn

2 | > δn
2 }

(5)

where K ∼ Binomial( n
bound the second term using Bernstein’s inequality.
For the ﬁrst term, ﬁrst assume that µ ≥ p+q

2 , γ). We bound the ﬁrst term above by applying Lemma 4, and we

2 . Then Lemma 4 gives

P(S c|{K1 = k1} ∩ {K2 = k2}) ≤ 2(k1 + k2) exp(− 3

16 · ((p−q)(k1+k2)−(2µ−(p+q))|k1−k2|)2
(2p+q)(k1+k2)+(2p−q−µ)|k1−k2| ).

We will select δ > 0 small enough so that

(α − β)(γ − δ) ≥ (2(2α − β) − (α + β))δ,

which in turn implies that every k1, k2 ∈ I satisﬁes

(p − q)(k1 + k2) ≥ (α − β) log n

n · (γ − δ)n

≥ (2(2α − β) − (α + β))δ · log n ≥ (2µ − (p + q))|k1 − k2|.

With this information, we may bound the exponent:

(2p+q)(k1+k2)+(2p−q−µ)|k1−k2| ≥ ((α−β)(γ−δ)−(2(2α−β)−(α+β))δ)2
((p−q)(k1+k2)−(2µ−(p+q))|k1−k2|)2

(2α+β)(γ+δ)+(2α−β−(cid:15))δ

· log n.

All together, for every k1, k2 ∈ I, we have

P(S c|{K1 = k1} ∩ {K2 = k2}) ≤ 2(γ + δ) exp((1 − 3

16 · ((α−β)(γ−δ)−(2(2α−β)−(α+β))δ)2

(2α+β)(γ+δ)+(2α−β−(cid:15))δ

) log n).

13

Thanks to our assumption

γ > 16(2α+β−η)

3(α−β−2η)2 = 16(2α+β)
3(α−β)2 ,

this bound is o(1) for every suﬃciently small δ > 0. Next, we consider the case in which
µ < p+q

2 . Put ζ := µn/ log n. Then Lemma 4 gives

P(S c|{K1 = k1} ∩ {K2 = k2}) ≤ 2(k1 + k2) exp(− 3
2 ·
≤ 2(γ + δ) exp((1 − 3
2 ·
≤ 2(γ + δ) exp((1 − 3
2 ·

((µ−q)(k1+k2))2
(3p+q+2µ)(k1+k2)+3(p−q)|k1−k2| )
(3α+β+2ζ)(γ+δ)+3(α−β)δ ) log n)
(3α+β+2(cid:15))(γ+δ)+3(α−β)δ ) log n),

((ζ−β)(γ−δ))2

(((cid:15)−β)(γ−δ))2

where the last step applies ζ > (cid:15) and the fact that the map ζ (cid:55)→
increasing over ζ ∈ [β, α+β

2 ]. Thanks to the assumption
3(α−β−2η)2 = 16(2α+β−( α+β

γ > 16(2α+β−η)

3(α−β−2( α+β

2 −(cid:15)))
2 −(cid:15)))2 = 2

3 · 3α+β+2(cid:15)
((cid:15)−β)2 ,

((ζ−β)(γ−δ))2
(3α+β+2ζ)(γ+δ)+3(α−β)δ is

this bound is o(1) for every suﬃciently small δ > 0.

At this point, we know that the ﬁrst term in (5) is o(1). For the second term, note that
K − γn
2 is a sum of independent, mean zero random variables Xi such that |Xi| ≤ 1 almost
surely and Var(Xi) = γ(1 − γ) ≤ γ. As such, Bernstein’s inequality for bounded variables
gives P{|K − γn

nγ2+(δn/2)/3) = o(1), as desired.

2 | > δn

2 } ≤ 2 exp(− (δn/2)2/2
Next, we provide a way of estimating p+q
2

so as to select µ:

Lemma 8. Draw G ∼ SBM(n/2, n/2, p, q) with planted communities {S1, S2} and with p =
(α log n)/n and q = (β log n)/n, and consider the random variable µ := |E|/(cid:0)n
(cid:1), where E
denotes the random edge set of G. Then for any ﬁxed c > 0, it holds that

2

(cid:12)
(cid:12)
(cid:12)µ −

p + q
2

(cid:12)
(cid:12)
(cid:12) ≤

c log n
n3/2

with probability 1 − o(1).

Proof. Let A denote the adjacency matrix of G.
generality) that S1 = {1, . . . , n1} and S2 = {n1 + 1, . . . , n}. Then

It is helpful to assume (without loss of

EA =

(cid:20) p(J − I)
qJ

qJ
p(J − I)

(cid:21)

,

where J and I denote the all-ones and identity matrices of order n/2. Then

Eµ = 1

n(n−1)

n
(cid:88)

i,j=1

EAij = n−2

2(n−1) · p + n

2(n−1) · q = p+q

2 − p−q

2(n−1).

Also, the random variable n(n−1)
independent Bernoulli random variables,
2 − 1) with mean p and ( n
2 )( n
( n
2 )2 with mean q. After subtracting the mean, each of these
random variables has absolute value at most 1 almost surely, and the variance of the sum is

2 µ is a sum of n(n−1)

2

Var( n(n−1)

2 µ) = ( n

2 )( n

2 − 1)p(1 − p) + ( n

2 )2q(1 − q) ≤ n(n−1)

2 max(p, q).

14

We may therefore apply Bernstein’s inequality for bounded variables:

P{|µ − Eµ| > t} = P{| n(n−1)

≤ 2 exp

−

Taking t = c log n

2n3/2 then gives

2 µ − E n(n−1)
(cid:18)

2 µ| > n(n−1)
2
( n(n−1)
t)2/2
2
2 max(p, q) + ( n(n−1)

n(n−1)

t}

2

(cid:19)

t)/3

= 2 exp(− n(n−1)

2

·

t2/2
max(p,q)+t/3).

P{|µ − Eµ| > t} ≤ 2 exp(−(1 − o(1)) ·

c2

16 max(α,β) · log n) = o(1).

All together, with probability 1 − o(1), it holds that

|µ − p+q

2 | ≤ |µ − Eµ| + |Eµ − p+q

2 | ≤ c log n

2n3/2 + |α−β| log n

2n(n−1) ≤ c log n
n3/2 .

Proof of Theorem 6. Select η > 0 such that (3) is satisﬁed and put (cid:15) := α+β
2 − η. By
Lemma 8, it holds that (n/ log n)µ ∈ ((cid:15), 2α − β) with probability 1 − o(1). As such, by
Lemma 7, we have { ˆR1, ˆR2} = {S1 ∩ V, S2 ∩ V } with probability 1 − o(1). The result then
follows from Lemma 2.

5 Discussion

In this paper, we applied the sketch-and-solve approach to systematically reduce the com-
putational burden of solving minimum bisection by semideﬁnite programming when the
problem instance exhibits more signal. Figure 1 illustrates that our approach provides a
substantial speedup over the original semideﬁnite programming approach. For this ﬁgure,
we selected γ assuming access to α and β using the following rule:

γ = min

(cid:110)

1,

4
α −

√
(

√

β)2

(cid:111)
.

Based on our experiments, we expect that our sketch-and-solve approach will succeed for
large n in the regime

γ >

√
(

2
α −

√

,

β)2

(6)

which corresponds to Proposition 1 in the special case where γ = 1.

Our investigation suggests a few opportunities for future work. First, while Theorem 6
provides a suﬃcient condition on γ, the apparent sketching threshold (6) warrants an expla-
nation. Also, we did not provide a method for selecting the sketching parameter γ when α
and β are unknown. In practice, one can test whether a given γ is too small by running the
algorithm multiple times and observing whether substantially diﬀerent clusterings emerge.
Presumably, one can identify a good choice of γ by performing this test for increasing values
of γ, but it would be nice to have a principled approach to select γ. Next, Figure 2 indicates
that the phase transition from Proposition 1 emerges when solving exact recovery under
the unbalanced stochastic block model. A more detailed analysis is required to explain this
phenomenon. To obtain this phase transition empirically, we put µ = p+q
2 , which suggests

15

another question: Given G ∼ SBM(n1, n2, p, q) with unknown n1, n2, p, q, how does one esti-
2 ? In our setting, the fact that n1 = n2 before sketching helped us to estimate p+q
mate p+q
in Lemma 8. Finally, it would be interesting if sketching SDPs in other settings (such as
geometric clustering) also supports our meta-claim that clustering is easier in the presence
of more signal.

2

Acknowledgments

DGM was partially supported by AFOSR FA9550-18-1-0107 and NSF DMS 1829955.

References

[1] E. Abbe, A. S. Bandeira, G. Hall, Exact recovery in the stochastic block model, IEEE

Trans. Inform. Theory 62 (2015) 471–487.

[2] P. Awasthi, A. S. Bandeira, M. Charikar, R. Krishnaswamy, S. Villar, R. Ward, Relax,

no need to round: Integrality of clustering formulations, ITCS 2015, 191–200.

[3] A. S. Bandeira, Random Laplacian matrices and convex relaxations, Found. Comput.

Math. 18 (2018) 345–379.

[4] A. Bluhm, D. Stilck Fran¸ca, Dimensionality reduction of SDPs through sketching,

Linear Algebra Appl. 563 (2019) 461–475.

[5] M. R. Garey, D. S. Johnson, Computers and intractability: A guide to the theory of

NP-completeness, Vol. 174, San Francisco: Freeman, 1979.

[6] M. X. Goemans, D. P. Williamson, Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming, JACM 42 (1995)
1115–1145.

[7] M. Grant, S. Boyd, CVX: Matlab software for disciplined convex programming, http:

//cvxr.com/cvx

[8] R. A. Horn, C. R. Johnson, Matrix Analysis, Cambridge University Press, 1985.

[9] T. Iguchi, D. G. Mixon, J. Peterson, S. Villar, Probably certiﬁably correct k-means

clustering, Math. Program. 165 (2017) 605–642.

[10] X. Li, Y. Li, S. Ling, T. Strohmer, K. Wei, When do birds of a feather ﬂock together?

k-means, proximity, and conic programming, Math. Program. 179 (2020) 295–341.

[11] S. Ling, T. Strohmer, Certifying global optimality of graph cuts via semideﬁnite
relaxation: A performance guarantee for spectral clustering, Found. Comput. Math.
(2019) 1–55.

[12] D. G. Mixon, S. Villar, R. Ward, Clustering subgaussian mixtures by semideﬁnite

programming, Inform. Inference 6 (2017) 389–415.

16

[13] J. A. Tropp, An Introduction to Matrix Concentration Inequalities, Found. Trends

Theor. Comput. Sci. 8 (2015) 1–230.

[14] R. Vershynin, High-dimensional probability: An introduction with applications in

data science, Vol. 47, Cambridge University Press, 2018.

[15] D. P. Woodruﬀ, Sketching as a tool for numerical linear algebra, Found. Trends Theor.

Comput. Sci. 10 (2014) 1–157.

[16] A. Yurtsever, M. Udell, J. A. Tropp, V. Cevher, Sketchy decisions: Convex low-rank

matrix optimization with optimal storage, arXiv:1702.06838

17

