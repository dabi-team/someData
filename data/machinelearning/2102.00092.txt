REINFORCEMENT LEARNING FOR FREIGHT BOOKING CONTROL
PROBLEMS

2
2
0
2

g
u
A
9

]

C
O
.
h
t
a
m

[

2
v
2
9
0
0
0
.
2
0
1
2
:
v
i
X
r
a

Justin Dumouchelle
University of Toronto
justin.dumouchelle@mail.utoronto.ca

Emma Frejinger
Université de Montréal
emma.frejinger@umontreal.ca

Andrea Lodi
Jacobs Technion-Cornell Institute, Cornell Tech and Technion - IIT
andrea.lodi@cornell.edu

ABSTRACT

Booking control problems are sequential decision-making problems that occur in the domain of
revenue management. More precisely, freight booking control focuses on the problem of deciding to
accept or reject bookings: given a limited capacity, accept a booking request or reject it to reserve
capacity for future bookings with potentially higher revenue. This problem can be formulated as a
ﬁnite-horizon stochastic dynamic program, where accepting a set of requests results in a proﬁt at the
end of the booking period that depends on the cost of fulﬁlling the accepted bookings. For many
freight applications, the cost of fulﬁlling requests is obtained by solving an operational decision-
making problem, which often requires the solutions to mixed-integer linear programs. Routinely
solving such operational problems when deploying reinforcement learning algorithms that require
large number of simulations may be too time consuming. The majority of booking control policies are
obtained by solving problem-speciﬁc mathematical programming relaxations that are often non-trivial
to generalize to new problems and, in some cases, provide quite crude approximations.
In this work, we propose a two-phase approach: we ﬁrst train a supervised learning model to predict
the objective of the operational problem, and then we deploy the model within simulation-based
reinforcement learning algorithms to compute control policies. This approach is general: it can be
used every time the objective function of the end-of-horizon operational problem can be predicted
with reasonable accuracy, and it is particularly suitable to those cases where such problems are
computationally hard. Furthermore, it allows one to leverage the recent advances in reinforcement
learning as routinely solving the operational problem is replaced with a single prediction. Our
methodology is evaluated on two booking control problems in the literature, namely, distributional
logistics and airline cargo management.

Keywords revenue management; booking control; reinforcement learning; supervised learning; freight transportation

1

Introduction

Revenue management (RM) is of importance in many commercial applications such as airline cargo, hospitality, and
attended home delivery (see, e.g., the survey Klein et al. 2020). In general, RM examines the decisions regarding the
distribution of products or services with the goal of maximizing the total proﬁt or revenue. The focus of this work is on
quantity decisions in the context of freight booking control problems. Such problems are of high practical relevance.
This has been particularly manifest since the beginning of the pandemic, when we have experienced fast moving
changes in freight transportation demand simultaneously as major disruptions in global supply chains occurred. This
resulted in a highly uncertain environment with shifts in demand distributions over time. In this context, effective use of
transport capacity is of the essence. Moreover, when faced with such a challenging environment, models and solution
algorithms need to scale to large problems and should be ﬂexible, i.e., it should be possible to adapt them regularly at a
reasonable cost.

 
 
 
 
 
 
RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

j and the probability that no request occurs at period t is given by pt

The booking control problem we consider has a relatively simple structure, and as we describe in the following,
can straightforwardly be formulated as a Markov Decision Process (MDP). We present a formulation similar to that
delineated in Talluri and Ryzin (2004). Let N denote a set of requests with cardinality |N | = n. The decision-making
problem is deﬁned over T periods indexed by t ∈ {1, . . . , T }. The probability that request j ∈ N is made at time t is
given by pt
0. The time intervals are deﬁned to be
small enough such that at most one request is received in each period.
j denote the number of requests of type j accepted before time t and st ∈ Rn, t =
To formulate this as an MDP, let st
1, . . . , T + 1, denote the state vector where the j-th index is given by st
j. Furthermore, let St denote the set of all
possible states at period t. Since a given request can only be accepted or rejected, the actions are denoted aj ∈ {0, 1},
where aj = 1 if an offer for request j is accepted. The deterministic transition is given by st+1 = st + ajej, with
ej ∈ Rn having a j-th component equal to 1 and every other equal to 0. The revenue associated with accepting a
request of type j is given by rj, and the reward function is given by R : N × {0, 1} → R, where R(j, aj) = rjaj.
An operational decision-making problem occurs at the end of the booking period and is related to the fulﬁllment of the
accepted requests, henceforth referred to as the end-of-horizon problem (EoHP). In the two applications we consider,
they respectively correspond to a capacitated vehicle routing problem (VRP) and a two-dimensional knapsack problem.
The EoHP only depends on the state sT +1 and we denote its cost by Γ : Rn → R−. With this, the value function is
given by

Vt(st) = pt

0Vt+1(st) +

pt
j max
aj ∈{0,1}

{rjaj + Vt+1(st + ajej)}, t = 1, . . . , T,

(cid:88)

j∈N

(1)

(2)

with s1 = 0.

VT +1(sT +1) = Γ(sT +1),

Due to the curse of dimensionality, solving this recursive formulation is intractable for problems of practical relevance
(e.g., Adelman 2007). Furthermore, for reinforcement learning (RL) algorithms that typically require a large number of
simulated system trajectories, the computational burden of learning a policy may be prohibitively high if hard EoHPs
are solved routinely to compute (2).

The objective of our work is to design an approach that (i) is broadly applicable to freight booking control problems
with different EoHP structures, (ii) scales to large-size problems, and (iii) can learn from data. RL is appealing in this
context, deep RL algorithms in particular as they have high capacity, and are known to produce well-performing policies
without an explicit model of system dynamics (e.g., Mnih et al. 2015). The main challenge associated with using RL
for our problem class pertains to the EoHPs as they can be relatively costly to solve and good relaxations may not exist.

In this paper, we precisely address this issue. In brief, we propose to embed fast predictions from a supervised learning
algorithm tasked with handling (2) within an RL approach. In particular, we focus our attention on the case where
the EoHP is a mixed-integer linear program (MILP) and compare our approach against the state-of-the-art in two
applications, namely, distributional logistics (Giallombardo et al. 2022) and airline cargo management (Barz and Gartner
2016).

Next, we provide an overview of the related work. We outline our contributions in Section 1.2, and close with a
description of the remainder paper structure (Section 1.3).

1.1 Related Work

In this section, we discuss three areas of related work. First, we provide a brief background on the use of deterministic
linear programming (DLP) to compute booking policies and we describe related approaches for solving the two booking
control problems that we consider here. Second, we review existing work on RL for booking control. Third, we discuss
machine learning (ML) for discrete optimization problems.

1.1.1 Deterministic Linear Programming and Booking Control Problems.

In RM, DLP is by far the most widely adopted algorithmic approach for computing policies. The primary reason for this
is that DLP can be used to compute both booking-limit and bid-price policies through a linear program (LP) that directly
models capacity constraints and the cost associated with fulﬁlling the requests. The LP is formulated to maximize the
quantity of each type of request that can be accepted without violating capacity constraints.

Speciﬁcally, the DLP is given by

max

(cid:88)

j∈N

rj · xj

2

(3)

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

subject to Ax ≤ b,

0 ≤ x ≤ µ,

(4)
(5)

where µ ∈ Rn is the vector of expected demand for each request type, A ∈ Rm×n and b ∈ Rm denote the capacity
constraints, and x ∈ Rn is the variable that provides the quantity of each type of request to be accepted. Booking-limit
control can be implemented directly through the primal solution by not accepting more requests than an upper limit
x∗ for each type. Bid-price control can be imposed by requiring that the revenue from accepting a request is greater
than a function of the dual variables and constraints. This speciﬁc formulation does not account for exact times and
realizations of the requests. However, the LP can be re-solved throughout the booking period to provide updated primal
and dual solutions. Although DLP generally works well and is widely used in practice (e.g., Williamson 1992, Talluri
and Ryzin 2004), it hinges on problem speciﬁc assumptions and may be difﬁcult to apply depending on the structure
of the EoHP. As we discuss next, several studies have examined a broad diversity of speciﬁc problem settings and
solutions methods.

Airline cargo management was introduced by Kasilingam (1997) and has become a major focus in freight booking
control over the recent years. Generally, weight and volume of each request are uncertain until the departure. At the
end of the booking period, the set of accepted requests needs to be packed on an aircraft and if there is insufﬁcient
capacity available, the unpacked items incur an off-loading penalty. By only considering weight and volume of the
goods to be transported, the EoHP (technically a three-dimensional knapsack problem, see the discussion in Section
3.2) is formulated and solved as a vector-packing problem. Airline cargo management can be formulated as a single-leg
or a network problem, where the former focuses on control for a single ﬂight and the latter is concerned with contexts
in which cargo may need to be loaded on multiple ﬂights to reach a ﬁnal destination.

Amaruchkul et al. (2007) propose a variety of heuristics based on DLP for single-leg airline cargo management, where
the off-loading penalty is simpliﬁed to a linear function of the excess weight and volume. Levin et al. (2012) introduce
a Lagrangian relaxation of the capacity constraints in the off-loading problem that are updated during the booking
period to compute a policy. Levina et al. (2011) introduce an inﬁnite horizon network cargo management problem with
a solution based on approximate dynamic programming (ADP). Barz and Gartner (2016) examine several approaches
derived from DLP and ADP for single-leg and network problems while considering off-loading costs modeled as a
vector packing problem (EoHP).

For distributional logistics, the problem was only recently introduced by Giallombardo et al. (2022). In this problem,
a set of pick-up requests are received from a set of locations. At the end of the booking period the requests must be
fulﬁlled, resulting in a capacitated VRP. This is a signiﬁcantly more challenging EoHP as the exact solution to even a
single VRP can be intractable. Giallombardo et al. (2022) propose a MILP formulation, similar to a DLP formulation,
but with the inclusion of integer variables to model the underlying routing problem. A booking-limit policy is then given
by the primal solutions of the MILP. Unfortunately, solving the MILP exactly is intractable for even small instances,
hence the approach in Giallombardo et al. (2022) terminates after a small number of incumbent solutions are found in
the solving process.

1.1.2 Reinforcement Learning for Booking Control.

In recent years, RL has been successful in a variety of challenging tasks (Silver et al. 2016, Mnih et al. 2015). However,
its application is currently foremost restricted to simulation environments (such as games), or real-world settings where
system failure is not costly and of limited risk (Donti et al. 2021). As we discuss below, its application is still limited
within RM, in particular in the context of freight applications. There is an extensive and fast evolving literature on
RL so a comprehensive overview is out of the scope of this paper. Instead, we refer the reader to excellent textbooks
(Bertsekas 2019, Sutton and Barto 2018) and surveys (e.g., Gosavi 2009, Buconiu et al. 2018).

Within RM, RL has been applied to solve pricing problems under various market assumptions (e.g., Kephart and Tesauro
2000, Rana and Oliveira 2014, Shukla et al. 2020, Kastius and Schlosser 2022), and to airline seat inventory control
problems (e.g., Gosavi et al. 2002, Lawhead and Gosavi 2019, Bondoux et al. 2020, Shihab et al. 2019). Both contexts
have important distinguishing features compared to freight booking control problems. The former problems have a
fundamentally different structure, whereas the latter are more closely related but involve a trivial determination of
capacity usage. Indeed, one passenger occupies exactly one seat and the capacity is deﬁned as a given number of seats.
On the contrary, in the airline cargo and distributional logistics applications discussed above, deploying RL is more
involved because of the potentially hard EoHP.

We are only aware of one study that uses RL for freight booking control in a realistic setting. Seo et al. (2021) consider
a sea cargo booking control problem. Instead of proﬁt maximization, they maximize revenue, hence ignoring both the
cost associated with transporting accepted demand, and constraints related to loading the cargo. In other words, they
ignore the operational planning problem at the end of the horizon and RL can be straightforwardly applied.

3

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

1.1.3 Machine Learning for Discrete Optimization.

The relatively recent success of ML in a wide range of applications has led to a fast-growing interest within the
optimization community. In particular, the area at the intersection of ML and discrete optimization has been expanding
from an impressive surge of contributions (see, e.g., Bengio et al. 2021, for a methodological analysis of the literature).

ML approaches for discrete optimization problems have been developed to either predict the entire solution of a problem,
i.e., replacing the discrete optimization techniques by ML ones, or to accelerate discrete optimization methods (referred
to as ML-augmented combinatorial optimization in the survey Kotary et al. 2021). We refer to Bengio et al. (2021)
for references on these topics. Instead, we concentrate on using ML to predict the objective value of a (hard) discrete
optimization problem, a task that has received less attention with a few notable exceptions: Fischetti and Fraccaro
(2017) use supervised learning to predict optimal objective value of MILP problems for a wind energy application.
Prates et al. (2019) propose a combination of supervised learning and binary search to predict optimal solution values
to the traveling salesperson problem (TSP). Larsen et al. (2022b) focus on predicting aggregate solutions (objective
function value is a special case) to stochastic discrete optimization problems in the context of two-stage stochastic
programs.

Closest to our work are heuristic methods that integrate predictions from supervised learning to gain speed. Larsen
et al. (2022a) propose a matheuristic version of a Benders decomposition method for solving two-stage stochastic
programs. Dumouchelle et al. (2022) propose easy-to-solve surrogate models for two-stage stochastic programs based
on neural networks trained to predict the second-stage objective. For multi-stage stochastic-programming, Dai et al.
(2022) propose a supervised approach for learning piecewise linear convex value function approximations. Whereas the
combination of supervised learning and RL is not new (e.g., Ye et al. 2003, Henderson et al. 2005, Finn et al. 2017),
we are unaware of any works that introduce predictions of optimal discrete optimization problem values within an RL
algorithm.

1.2 Contributions

In this work, we propose a two-phase approach, wherein we ﬁrst train a supervised learning model to predict the value
of the EoHP solution, and then we integrate predictions within simulation-based RL algorithms to compute control
policies. This approach is general as it can be applied to any EoHP, regardless of its structure, as long as the objective
value can be computed or approximated. In addition, as the prediction signiﬁcantly reduces the time required for
simulation, this can be scaled to more complex EoHPs and allow for the use of RL algorithms requiring extensive
simulation.

We make the following contributions:

1. We propose a methodology that (i) formulates an approximate MDP by replacing the (discrete) optimization
model of the EoHP with a prediction function yielded by ofﬂine supervised learning, and (ii) uses RL techniques
to obtain approximate control policies, both of which can be implemented with existing off-the-shelf libraries.

2. We apply the proposed methodology to distribution logistics and airline cargo management problems from the
literature and we show computationally that we achieve high-quality control policies that can be deployed
efﬁciently. The EoHP is harder for the distributional logistics application. In this case the percent improvement
in mean proﬁt compared to the algorithms presented in Giallombardo et al. (2022) is at least 26.6% with
an average online computing time of less than 4.5 seconds. Notably, we achieve a performance close to the
strong state-of-the-art baseline for the well-studied airline cargo management problems. In this case, the
percent improvement in mean proﬁt compared to the baseline method ranges between -2.2% (baseline is
slightly better) to 0.7% with an average online computing time of less than 1.5 seconds. In all cases, the total
ofﬂine computing time is less than six hours, orders of magnitude smaller than it would be without leveraging
predictions from a supervised learning model.

We note that by approximating the value of the solution of the EoHP through supervised learning, we do not need to
either simplify the EoHP so as to embed it in an algorithmic framework that requires simulation, or to exploit the special
structure of the problem to devise effective booking control policies. This is the distinctive advantage of our approach,
and its advantages will be discussed in both distributional logistics and airline cargo management applications, where
simpliﬁcations to the EoHP and exploitation of the problem structure are usually required.

1.3 Structure of the Paper

The remainder of the paper is organized as follows. In Section 2, we introduce our methodology. In Section 3, we
discuss two freight booking control problems, their state-of-the-art baselines, and how our supervised prediction task is

4

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

formulated. In Section 4, we compare our methodology to the problem speciﬁc baselines. Finally, Section 5 draws
conclusions and highlights avenues for future work.

2 Methodology

We divide the description of our methodology into three parts: In Section 2.1, we propose an approximation of the
MDP (1)–(2), and, in Section 2.2, we specify the ML ingredients of that approximation. Finally, in Section 2.3, we
motivate the use of RL to solve the approximate MDP of Section 2.1.

2.1 Approximate MDP Formulation

Solving the MDP (1)–(2) can be intractable even for small instances. A possible approach to deal with this is to
approximate (2). This is especially important in the context of booking control problems, where it may be costly to
compute. Speciﬁcally, the approximation of Γ is denoted as φ : Rm → R− and given by

where g : Rn → Rm is a mapping from the state at the end of the horizon to an m-dimensional representation. The
approximate MDP is then given by

Γ(sT +1) ≈ φ(g(sT +1)),

(6)

˜Vt(st) = pt
0

˜Vt+1(st) +

(cid:88)

j∈N

pt
j max
aj ∈{0,1}

{rjaj + ˜Vt+1(st + ajej)}, t = 1, . . . , T,

˜VT +1(sT +1) = φ(g(sT +1)).

(7)

(8)

Note that in the system (7)–(8), the only change compared to (1)–(2) is in the value function at the end of the time
horizon. However, the value will be recursively propagated to the entire value function.

As we further motivate in Section 2.3, we focus on solving the approximate MDP in (7)-(8) with RL. Such algorithms
are based on fast evaluations of system trajectories. In turn, this requires evaluating (8) and a related fast approximation
scheme is the topic of the following section.

2.2 Approximating the End-of-Horizon Cost

The approximation of Γ(·) in (8) can be computed in various ways, for instance by using a problem-speciﬁc heuristic,
solving a MILP (or its LP relaxation) to a given time limit or optimality gap or, as proposed in this paper, it can be
predicted by ML. In this work, we focus our attention on prediction through supervised learning, i.e., we develop a
mapping from the end-of-horizon state to the associated operational cost. Speciﬁcally, a supervised learning model is
trained ofﬂine to separate the problem of accurately predicting Γ(·) from that of solving (7)–(8).

The choice of using supervised learning is motivated by the three following considerations. First, predicting the
end-of-horizon cost is problem agnostic as long as a set of features and labels can be constructed. This allows our
methodology to be easily adapted to new applications, which is not the case for problem-speciﬁc heuristics, where
minor changes in the problem formulation often require non-trivial – if at all possible – adaptations to be made to the
heuristic. Second, the use of supervised learning is motivated by efﬁciency considerations. As we only require a single
prediction for each simulated trajectory, this can be made in very short computing time (milliseconds), often orders of
magnitude faster than heuristics for mathematical programming problems, especially when considering challenging
EoHP. Third, the sample size required to learn the EoHP cost separately from the booking policy is likely much smaller
than that required if they were learned jointly. We analyze this numerically in Section 4.

The aim of the supervised learning algorithm is to accurately predict the value of the EoHP for any feasi-
ble (and relevant) state sT +1. For this purpose, we use simulation to collect N labeled data points D =
{(s1
T +1, Γ(sN
T +1)), . . . , (sN
T +1))} and an associated feature mapping g(·). Next, we describe the steps of
the data generation process in detail.

T +1, Γ(s1

Sampling End-of-Horizon States sT +1. We use a stationary random policy that accepts a request with a given
probability p to simulate trajectories. At the end of the time horizon, a state sT +1 is obtained. This process is repeated
for different values of p. The objective is to generate a distribution of feasible ﬁnal states in the data (optimal and
sub-optimal ones) that will lead to high-quality generalization of the ML algorithm.

5

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Feature Mapping. The feature mapping is problem speciﬁc. It is represented by the function g(·) in (8). In general,
for booking-control problems we will have accepted a set of requests. Since a set is a permutation-invariant structure, it
is desirable to make use of a permutation-invariant learning model similar to those described in Zaheer et al. (2017) to
capture the features of the set of accepted requests. Hence, the vectors of features, say xr, characterizing the individual
requests are independently passed through an encoding feed forward network, Ψ1. The outputs of this network are then
aggregated with a permutation-invariant transformation (a sum in our case). The resulting permutation-invariant output
is passed through a decoding feed forward network, Ψ2, thus yielding the permutation-invariant embedding vector of
all requests, as advocated in Zaheer et al. (2017). Next, the concatenation of this embedding vector with the vector
characterizing the features of the carrying vessels and global information, say xc, is passed through a third feed forward
network, Φ, yielding a prediction for the EoHP cost. Figure 1 illustrates this process.

As an alternative, one could replace the embedding with a vector that directly aggregates the individual statistics of the
accepted requests. However, as evidenced in Section 4.2, models which use the aggregated features suffer from notably
high prediction errors.

T +1, n = 1, . . . , N , an “expert” computes a label
T +1) by solving the EoHP. The expert can correspond to an exact method, or a heuristic depending on the required

Computing Labels. For each sampled end-of-horizon state sn
Γ(sn
precision and computing time budget.

An important aspect of this data generation procedure is that solving the EoHP can be parallelized up to the number of
available threads. This stands in contrast with online approaches for estimating the EoHP cost (such as a problem-speciﬁc
heuristic or LP-relaxation) as they involve signiﬁcantly more costly simulation time when training RL algorithms. In
addition, since most RL algorithms rely on sequential simulation, parallelization is usually more difﬁcult.

Figure 1: Pictorial representation of permutation-invariant model for booking control. Adapted from Figure 2 in Dumouchelle et al.
(2022).

2.3 Policies for the Approximate MDP

Once supplied with an ofﬂine supervised learning model trained to approximate the EoHP cost, we can leverage a
broad range of RL techniques to compute control policies by solving (7)-(8). We select the Q-learning algorithm. Its
performance has been well documented through extensive applications on a broad range of tasks and it is known to
feature typically short computing times. More speciﬁcally, we focus on Deep Q-learning (DQN).

We implement two variants of DQN. The ﬁrst is DQN-L which simply considers a linear state representation containing
the number of accepted requests for each class along with additional information about the utilized capacity and period.
DQN-L employs a fully connected network to approximate the value function. Similar to the supervised learning case,
we have a set of accepted requests during simulation, so once again permutation-invariant approaches are a natural
choice for the DQN value-function estimator. This motivates the choice of the second model, DQN-S, which is similar
in architecture to Figure 1 as it accounts for the features of each of the accepted requests from previous periods.

6

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

3 Applications

In this section, we introduce two speciﬁc booking control problems, describe their respective state-of-the-art baselines,
and examine considerations pertaining to the prediction of the EoHP costs.

3.1 A Distributional Logistics Problem

We start by describing the formulation of the EoHP proposed by Giallombardo et al. (2022) for a distributional logistics
problem. We then outline their solution approach and follow with a description of the associated supervised prediction
task (Section 3.1.3). We close with a discussion of RL training in Section 3.1.4.

3.1.1 EoHP Formulation.

In this problem, booking requests correspond to pickup activities and the EoHP to a capacitated VRP. Each pickup
request has an associated location and revenue. The cost incurred at the end of the booking period is the cost of the
VRP solution and hence depends on all the accepted requests.

The problem can be formulated as (1)–(2) with the boundary function described below. The end-of-horizon VRP
consists of a ﬁxed set of K0 vehicles, each with capacity Q ∈ R+. The depot location is indexed by location 0. Each
request is associated with a location, so we use N to denote the set of request locations. The set of all nodes V is given
by N ∪ {0}, i.e., the union of the request locations and the depot. The set of arcs is given by V × V and denoted as A.
The cost of an arc is given by cij ∈ R+, for (i, j) ∈ A. The optimal objective value is denoted by z∗(s, K), where K
denotes the number of vehicles used. If more than K0 vehicles are required, then additional outsourcing vehicles are
allowed to be used at an additional ﬁxed cost of C ∈ R+ each. Giallombardo et al. (2022) do not consider the inclusion
of outsourcing vehicles and instead assign an inﬁnite operational cost if the requests cannot be fulﬁlled. To accomplish
the same objective, C can be chosen sufﬁciently large enough such that the cost of adding an additional vehicle is larger
than any potential revenue from the requests it can fulﬁll. The operational cost is given by

Γ(s) =

max
K≥K0,K∈Z+

{−z∗(s, K) − C(K − K0)},

(9)

where z∗(s, K) denotes the cost of the routing problem given K vehicles, and C(K − K0) denotes the cumulative
cost of the K − K0 outsourcing vehicles used. The routing problem can be represented as a MILP and, following
Giallombardo et al. (2022), is given by

z∗(s, K) = min

(cid:88)

(cid:88)

cijαk
ij

k∈K

(i,j)∈A

s.t.

(cid:88)

ij = βk
αk
i

(i,j)∈A

(cid:88)

(i,j)∈A

ji = βk
αk
i

(cid:88)

k∈K

(cid:88)

k∈K

βk
j ≤ 1

βk
0 ≤ K,

∀i ∈ V, ∀k ∈ {1, . . . , K},

∀i ∈ V, ∀k ∈ {1, . . . , K},

∀j ∈ N ,

ij ≥ βk
αk
h

∀S ⊂ V : 0 ∈ S, ∀h ∈ V\S, ∀k ∈ {1, . . . , K},

(cid:88)

(cid:88)

i∈S

j∈V\S

j ≤ Qβk
qk
j

(cid:88)

j∈N

qk
j ≤ Q

∀j ∈ N , k ∈ {1, . . . , K},

∀k ∈ {1, . . . , K},

7

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

(cid:88)

k∈K

qk
j = sj

αk

ij ∈ {0, 1}

βk
i ∈ {0, 1}

qk
j ≥ 0

∀j ∈ N ,

∀(i, j) ∈ A, ∀k ∈ {1, . . . , K},

∀i ∈ V, ∀k ∈ {1, . . . , K},

∀j ∈ N , ∀k ∈ {1, . . . , K}.

(18)

(19)

(20)

(21)

ij, βk

i , and qk

j : The binary variable αk

The decision variables are αk
ij equals 1 if vehicle k uses arc (i, j). The binary
variable βk
i equals 1 if vehicle k visits node i. The quantity of accepted requests from vehicle k at location j is given by
the non-negative variable qk
j . Objective function (10) minimizes the routing cost. Constraints (11) and (12) ensure that
each vehicle goes in and out of the visited nodes. Constraints (13) assert that each location must be visited at most once,
and (14) limits the number of vehicles to at most K. Constraints (15) ensure that every route traveled by a vehicle is
connected, and each constraint in (16) restricts vehicle k from collecting anything from location j if it does not visit the
location (i.e., if βk
j = 0). Finally, constraints (17) guarantee that the capacity Q of each vehicle is not exceeded, and
(18) ensure that the accepted requests at each location are fulﬁlled. It is well-known that VRP formulations with arc
variables duplicated for each vehicle, i.e., αk
ij in (10)–(21) above, are not the most efﬁcient ones (Toth and Vigo 2002).
However, this formulation is suitable for dealing with the booking control version of the problem (see Giallombardo
et al. 2022, for details).

3.1.2 State-of-the-art Solution Method.

The DLP-based approach of Giallombardo et al. (2022) constitutes the state-of-the-art for the distributional logistics
booking control problem. They formulate a proﬁt-maximizing MILP which is given by

max

(cid:88)

j∈N

(cid:88)

(cid:88)

rjyj −

cijαk
ij

k∈K

(i,j)∈A

s.t.

(11) − (17), (19) − (21),
(cid:88)

qk
j = sj + yj

k∈K
0 ≤ yj ≤ µj

(22)

(23)

(24)

(25)

∀j ∈ N ,

∀j ∈ N ,

where yj speciﬁes a threshold on the number of requests to accept for class j. Giallombardo et al. (2022) provide two
baselines, namely, a booking-limit policy (BLP) and a booking-limit policy with reoptimization (BLPR). The BLP
simply solves the MILP before any requests have been accepted to provide a ﬁxed threshold for requests of each class.
The BLPR is similar, but allows for an additional MILP solution halfway through the booking period to update the
thresholds based on the set of accepted requests. These approaches are then applied by accepting requests until the
threshold for a given class is met. Because the DLP-type formulation is not, in fact, a LP but a MILP, the dual solution
is not available, so it cannot be used to compute bid-price policies.

Considering complexity of the EoHP, solving the MILP is difﬁcult and Giallombardo et al. (2022) limit the process
to two incumbent solutions for the proﬁt maximization problem and four incumbent solutions for the end-of-horizon
MILP. Essentially, the MILP run is aborted after two (or four) primal solutions have been found, independently of their
quality. This formulation additionally suffers from the fact that low probability-high reward requests may be rejected if
their expected demand is quite small, which is perhaps the largest limitation of the BLP/BLPR in this context.

3.1.3 Prediction Task.

The objective is to produce an accurate approximation of (9) that is fast to compute. In general, one can predict the sum
of the MILP objective function value and the outsourcing cost, i.e. (9). However, this is likely to be detrimental as the
outsourcing cost may result in a large change in the label whereas only a minor change occurs in the features. For this
reason, ML is used to predict z∗(s, K) while the outsourcing cost C(K − K0) is computed separately by solving the
associated bin-packing problem of determining the minimum number of vehicles needed to do all pickup activities. The
bin-packing is solved to optimality by the solver MTP (Martello and Toth 1990).

For this distributional logistics application, even solving the EoHP to optimalily to collect labeled data is prohibitively
expensive. For this reason, we leverage a VRP-speciﬁc heuristic that allows us to compute high-quality solutions within

8

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

reasonable computing times. Speciﬁcally, we use the heuristic FILO (Accorsi and Vigo 2021) to compute a label (10).
One minor consideration we had to make when using FILO was to ensure the number of vehicles was minimized which
is not done explicitly in FILO as it is an unlimited ﬂeet solver. To bias FILO, we simply add a prohibitively expensive
cost for leaving the depot which in turn minimizes the number of vehicles leaving the depot. The additional cost is then
subtracted after a solution is computed.

We use a permutation-invariant model and compare it to a ﬁxed-size input model. For the permutation-invariant model,
the request features, xr, are given by the location index, x and y coordinate, and demand. The carrier features, xc, are
given by the number of vehicles, the number of locations with demand, the x and y coordinate of the depot, and the
capacity. For the directly aggregated input structure, the capacity, depot location, total number of accepted requests per
location and aggregate statistics of the locations are computed, namely, the distance between locations and the depot,
and relative distances between locations are used. For each of these, we compute the min, max, mean, median, standard
deviation, and 1st/3rd quartiles.

3.1.4 RL Training.

For RL, we have two distinct state space representations. For DQN-L, we consider a linear state space with the following
features: the incoming request index, the number of accepted requests at each location, and the period. For DQN-S, we
consider a permutation-invariant model similar to that in Section 2.2 with request features, xr, given by the location
index and the x and y coordinates. The carrier features, xc, are given by the total capacity, utilized capacity, number of
vehicles, and the period. The time period can be indicated through either an integer or a one-hot vector. One of these
hyperparameter settings is selected at the time of model selection.

3.2 An Airline Cargo Problem

We introduce the airline cargo management problem described in Barz and Gartner (2016) (originally proposed by
Amaruchkul et al. 2007), and discuss the details required for building a predictor for the EoHP. Generally, an airline
cargo management problem can be formulated as either a single-leg or a network problem. A single-leg problem is
limited to cargo traveling between an origin-destination pair, while a network problem allows for cargo to travel on
multiple ﬂights within a network. In this work, we focus on the single-leg problem as it is more rigorously studied
and the generalization to the network problem is straightforward with the methodology presented here. The MILP
formulation of the EoHP available in Barz and Gartner (2016) can be specialized to the single-leg context as follows.

3.2.1 EoHP Formulation.

In the single-leg cargo-management application, booking requests correspond to shipments that are being transported
on commercial airline ﬂights. Typically, commercial airlines have a non-negligible quantity of space remaining after
passenger luggages are stored. With the excess capacity, commercial shipments are often transported between the origin
and destination, resulting in a signiﬁcant source of additional revenue. Each request is considered to be of a particular
class, such as electronics, fresh food, or raw materials, which share a joint weight and volume distribution.

The EoHP incurred at the end of the booking period is given by a vector packing problem (see, e.g., Caprara and Toth
2001). The EoHP cost is given by the set of accepted requests which cannot be loaded due to the capacity of the aircraft
and is referred to as the off-loading cost. More precisely, each request has an associated weight and volume and the
ﬂight has both a weight and volume capacity. Whereas all of these quantities are uncertain until the time of departure,
their expected values are assumed known.

We now deﬁne the MILP formulation for the EoHP. The realized weight and volume capacities of the ﬂight are given
by Qw ∈ R+ and Qv ∈ R+, respectively. The set of shipping classes or request types is given by N . The realized
weight and volume associated with the i-th request of the j-th shipping class are denoted by wji and vji, respectively.
For airline cargo, it is often convenient to express the proﬁt and off-loading costs as a function of the weight and
volume. This is done through the concept of chargeable weight, which is deﬁned by a weighted maximum of the weight
and volume for a given request. The chargeable weight allows requests to be charged in terms of the predominant
capacity dimension. Speciﬁcally, the chargeable weight is given by max{wji, vij/η}, where η ∈ R+ is a constant
representing the weight-volume ratio of a shipment. Using the chargeable weight, the off-loading cost is then given by
cj · max{wji, vij/η}. Finally, the operational cost is given by

where z∗(s) denotes the off-loading cost associated with the vector packing problem

Γ(s) = −z∗(s),

z∗(s) = min

(cid:88)

sj
(cid:88)

j∈N

i=1

cj max{wji, vji/η}(1 − xji)

(26)

(27)

9

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

s.t.

(cid:88)

sj
(cid:88)

j∈N

i=1

(cid:88)

sj
(cid:88)

j∈N

i=1

xjiwji ≤ Qw,

xjivji ≤ Qv,

xji ∈ {0, 1}

(28)

(29)

∀j ∈ N , ∀i ∈ {1, . . . , sj}.

(30)

The binary variables xji are equal to 1 if the i-th accepted request of the j-th shipping class is loaded, and 0 otherwise.
Objective function (27) minimizes the sum of the off-loading costs of the shipments multiplied by a constant cj
associated with each shipping class j. Constraints (28) and (29) ensure that the weight and volume of the loaded items
respect the capacity of the ﬂight.

Although modeling the loading requirements as the two independent knapsack constraints on weight and volume
(28)–(29) is an established practice in the literature, it is worth mentioning that this is clearly a crude approximation. In
fact, the real loading problem would require to consider three-dimensional objects (instead of approximations with their
volume) together with their weight (see, e.g., the extensive discussion in Brandt and Nickel 2019). The corresponding
optimization problem would be a generalization of the already very complex three-dimensional knapsack.

3.2.2 State-of-the-art Solution Method.

For airline cargo management, the set of available algorithms is signiﬁcantly larger than for distributional logistics.
Barz and Gartner (2016) provide an in-depth numerical analysis of algorithms, but the top-performing algorithms are all
based on the DLP. As the weight and volume are unknown until the end of the booking period, the DLP is formulated
using their expectations. Speciﬁcally, the expected weight and volume capacities are denoted by E[Qw] and E[Qv],
respectively. The expected weight and volume of an item of class j are denoted by E[wj] and E[vj], respectively. With
these assumptions and deﬁnitions, the DLP is then given by

max

s.t.

(cid:88)

j∈N

(cid:88)

j∈N

(cid:88)

j∈N

pjyj − cjzj

xj ≤ E[Qw],

xj ≤ E[Qv],

zj + xj ≥ yjE[wj]

zj +

xj
η

≥ yj

E[vj]
η

xj, zj ≥ 0

0 ≤ yj ≤ µj

(31)

(32)

(33)

(34)

(35)

(36)

(37)

∀j ∈ N ,

∀j ∈ N ,

∀j ∈ N ,

∀j ∈ N ,

where yj models the accepted requests for a class j. Variables xj and zj model the expected weight and volume of a
class with xj representing the quantity that can be loaded and zj representing the quantity that cannot. The objective
(31) models the proﬁt from accepting requests, minus the ofﬂoading costs. Constraints (32)-(33) ensure that the capacity
is not exceeded. Constraints (34)-(35) model the amount that is loaded/ofﬂoaded. Finally, constraints (36)-(37) deﬁne
the variable restrictions.

It is important to note that this formulation is an LP relaxation to the original vector packing problem. However, since
the LP relaxation of the vector packing is relatively tight (Caprara and Toth 2001), this will likely not have a large
impact on the objective value. Furthermore, since we have an LP, duality can be used to compute bid-price policies. In
Barz and Gartner (2016), the DLP algorithm is deﬁned by using the dual variables of constraints (32)-(33) to model
opportunity cost from accepting a given request. Several algorithms are then built on the idea of using the DLP solution,
with the best performing algorithm being the dynamic programming decomposition (DPD). This algorithm decomposes

10

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

the weight and volume into a single quantity using the dual variables, then formulates a tractable one-dimensional MDP
that can be solved via backtracking (see Barz and Gartner 2016, for details).

The practicality of this approach is highly dependent on the simpliﬁcation of the EoHP discussed in the previous section.
Solving the real generalized three-dimensional knapsack would have two major consequences: a computationally much
more demanding problem to be solved repeatedly, and the loss of duality to derive price-bid policies.

3.2.3 Prediction Task.

The objective is to produce an accurate approximation of (26) that is fast to compute. For this purpose, ML is used to
predict z∗(s). In contrast to the previous application where the EoHP required a heuristic to label the data, for airline
cargo the EoHP can be solved exactly within a reasonable computing time, so we use the optimal objective in this case.

Similar to the ﬁrst application, we use a permutation-invariant model as well as a model with ﬁxed-size input model.
For the permutation-invariant model, the request features, xr, are given by the weight, volume, and cost of each request.
The carrier features, xc, are given by the weight and volume capacities and the number of requests. For the directly
aggregated input structure, the features are derived from the capacities, number of items, and aggregate statistics of the
items, namely, the weights, volumes, and off-loading costs are used. For each of these, the min, max, mean, median,
standard deviation, and 1st/3rd quartiles are computed.

3.2.4 RL Training.

For RL, we have two distinct state space representations. For DQN-L, we consider a linear state space with the following
features: the incoming request index and price ratio, the number of accepted requests at each request class, the expected
remaining weight and volume capacities, and the period. For DQN-S, we consider a permutation-invariant model
similar to the previous section with request features, xr, given by the item class, the item class, price ratio, expected
weight, expected volume, expected cost, expected price, and expected chargeable weight. For the carrier features, xc,
we consider the previously stated request features for the incoming request, the expected weight and volume capacities,
the expected utilized weight and volume, and the period. Similar to the distributional logistics case, the period can
either be encoded with an integer or a one-hot vector.

One important note for the training of DQN is that for the airline cargo management instances the EoHP is uncertain
until the end of the booking period. This results in a reward with high-variance depending on the realization of weights
and volumes and not the trajectory itself. To mitigate this we sample 50 realizations of the weight and volume then
take the average over the approximate EoHP costs as predicted by the supervised learning model. This highlights an
important area where the supervised learning provides a signiﬁcant reduction, even for the easier EoHP, as computing
the 50 end-of-horizon costs can be done with efﬁcient parallel matrix operations, whereas other approaches would
need to access signiﬁcantly more computing resources in order to parallelize a heuristic or exact algorithm across the
different realizations. Without additional sampling of EoHP costs, training is less stable and achieves signiﬁcantly
worse results.

4 Computational Results

This section reports on the results of our computational experiments. It is structured as follows. First, Section 4.1
provides details about the computational setup and instances. Then, Section 4.2, presents the supervised learning results,
that is, the ﬁrst phase of our methodology. Section 4.3 outlines the speciﬁc parameters of the control algorithms and the
baselines. Finally, we report and discuss our main results in Section 4.4.

4.1 Computational Setup and Instances

All experiments were run on a computing cluster equipped with an Intel Xeon E5-2683 CPU and an Nvidia Tesla
P100 GPU operating with 64GB of RAM. Gurobi 9.1.2 (Gurobi Optimization, LLC 2021) handles MILP solving,
Scikit-learn 1.0.1 (Pedregosa et al. 2011) and Pytorch 1.10.0 (Paszke et al. 2019) handles supervised learning. We build
our implementation of RL based on the DQN implementation in RLLib (Liang et al. 2018).

In our experiments we include eight and four instances from the distributional logistics and airline cargo management
applications, respectively. Here, an instance refers to an MDP, i.e. (1)-(2), and we refer to a single realization of the
uncertainty as an episode or trajectory. We analyze results for 1,000 trajectories of each problem instance.

For distributional logistics, we denote an instance as VRP_X_Y, where X ∈ {4, 10, 15, 50} is the number of locations
and Y ∈ {L, H} is the load factor deﬁning the capacity in terms of the expected demand. A value of L indicates a low

11

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Instance

Training MAE

Validation MAE

Times (seconds)

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H

LR

1.38
2.08
1.81
8.44
2.38
3.20
2.83
11.31

RF

0.59
0.62
0.49
2.58
1.11
1.04
0.83
3.46

NN

0.03
0.11
0.14
1.08
0.05
0.18
0.30
1.24

LR

1.33
2.21
1.83
8.44
2.38
3.19
2.86
11.71

RF

0.62
1.41
1.31
6.69
1.28
2.31
2.18
9.07

NN

0.11
1.07
1.22
7.62
0.10
1.57
2.21
8.04

CM_0.5_0.5
CM_1.0_0.5
CM_0.5_1.0
CM_1.0_1.0

1988.40
1957.81
2019.76
1896.39

118.97
67.31
72.10
74.90

53.26
28.70
29.59
27.85

1926.60
2006.23
1980.19
1955.22

316.41
170.43
183.12
219.76

57.41
36.06
32.94
39.47

LR

0.36
0.23
0.21
0.23
0.27
0.17
0.32
0.59

0.08
0.08
0.36
0.07

RF

1.12
4.23
4.52
6.94
1.25
4.13
5.23
8.75

8.46
8.63
9.52
10.94

NN

Data

803.17
221.81
449.47
332.57
284.26
498.34
877.92
470.63

457.37
692.58
830.33
252.01

184.75
307.77
486.34
798.65
212.47
398.85
575.57
831.12

51.65
50.63
57.02
58.01

Table 1: Training and validation MAEs, training times and data generation times.

load factor, which results in a higher capacity, while a value of H indicates the opposite. For each instance, locations are
generated uniformly at random. The number of time periods is 20, 30, 50, and 100 for the instances with 4, 10, 15,
and 50 locations, respectively. We choose the proﬁts and probability of requests such that high-value requests have a
higher probability of occurring later in the booking period. A more detailed description of all the instances is available
in Appendix A.1.

For airline cargo management, we denote an instance as CM_X_Y, where X ∈ {0.5, 1.0} is the expected weight
capacity divided by the expected total weight of all requests received during the booking period and Y ∈ {0.5, 1.0} is
the expected volume capacity divided by the expected total volume of all requests received during the booking period.
These instances are all generated with exactly the same parameters as the single-leg experiments in Barz and Gartner
(2016). They have 60 time periods and 24 item classes. Similar to the distributional logistics instances, the proﬁts and
probabilities of request are such that high-value requests have a higher probability of occurring later in the booking
period. A more detailed description of all the instances is available in Appendix A.2.

4.2 Supervised Learning Results

This section presents the numerical results of the supervised learning component of our methodology. Speciﬁcally,
Table 1 reports the mean absolute error (MAE) on both the train and validation sets for our permutation-invariant neural
model (NN) described in Section 2.2 as well as the time dedicated to training models and generating data. We compare
with two baselines that use linear features derived from aggregating statistics of the requests, namely linear regression
(LR) and random forests (RF) (Breiman 2001).

To generate datasets we simulate random policies as described in Section 2.2 to obtain 5,000 training and 1,000
validation samples of end-of-horizon state-cost pairs. This process is done for each instance. The choice of dataset size
is motivated by the results in Appendix B.1, which show diminishing returns when generating samples beyond these
cardinalities. All data generation was done in parallel with 32 threads which is reﬂected in the reported computing times.
The last column of Table 1 indicates that the time to generate data is quite small for all problem instances, ranging
between 51 and 831 seconds. As the datasets are generated ofﬂine before policies are computed, we consider these
ﬁgures to be small.

For LR and RF we apply the default hyperparameters values in Scikit-learn. However, we note that changes in
hyperparameters values do not result in a meaningful difference in validation performance. For the NN, we perform
random searches over 50 conﬁgurations for each instance. Detailed results of the best conﬁgurations from random
search and the variance between conﬁgurations are reported in Appendix B.2. In summary, the variance in performance
is relatively small, so it does not appear that hyperparameter conﬁguration is particularly needed in this case.

The results in Table 1 show that NN typically outperforms the baselines in both training and validation MAE, often by
an order of magnitude. Whereas this does come at the cost of increasing training times, we argue that, considering that
the training is performed ofﬂine, the considerable improvement in performance justiﬁes the use of NN. Hence, we use
NN as the EoHP cost predictor for all instances.

12

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Instance

Mean Proﬁts

Online Time

Ofﬂine Time

DQN-L DQN-S

BLP

BLPR

FCFS DQN-L DQN-S BLP BLPR FCFS

DQN-L

DQN-S BLP BLPR FCFS

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H

101.26
247.30
465.62
1405.62
73.96
155.04
313.19
864.70

100.10
248.03
468.09
1420.89
73.26
154.89
316.93
907.77

77.12
125.57
142.86
190.92
52.72
102.15
142.86
165.91

78.68
127.69
141.39
186.30
57.85
107.05
141.39
178.51

70.23
243.45
451.04
1372.16
32.44
129.75
248.36
647.29

0.79
2.25
2.61
4.17
0.77
2.00
2.17
2.94

0.73
2.35
3.55
3.49
0.74
1.99
2.27
2.29

0.01
0.08
0.21
0.88
0.01
0.11
0.39
0.93

0.10
0.22
0.33
2.13
0.12
0.45
0.32
2.16

0.75
2.08
2.49
3.11
0.75
1.88
2.24
2.97

2727.62
3369.42
5352.19
12753.77
2039.01
3164.38
5176.40
12275.82

3195.24
4070.65
10169.04
19141.25
3298.18
4379.66
10328.45
17392.24

0.11
0.12
0.19
1.09
0.48
0.57
0.20
1.11

0.08
0.12
0.20
1.10
0.30
0.59
0.20
1.14

-
-
-
-
-
-
-
-

Table 2: Distributional logistics control results. Proﬁt and online time averaged over 1,000 trajectories. All times are in
seconds.

Instance

Mean Proﬁts

Online Time

Ofﬂine Time

DQN-L DQN-S

DPD

DLP

FCFS DQN-L DQN-S DPD DLP

FCFS DQN-L

DQN-S

DPD DLP

FCFS

CM_0.5_0.5
CM_1.0_0.5
CM_0.5_1.0
CM_1.0_1.0

6078.84
6449.63
6475.36
9923.42

6106.74
6414.37
6497.96
9860.28

6111.81
6559.94
6538.72
9851.77

6100.18
6433.58
6421.70
9306.05

3816.06
4341.09
4282.58
9306.05

0.16
0.14
0.15
0.28

1.31
0.18
0.20
0.21

0.03
0.03
0.03
0.04

0.03
0.02
0.03
0.04

0.04
0.03
0.03
0.04

6120.83
6165.87
5836.42
6072.19

13006.05
7653.07
11153.71
11014.95

266.06
446.40
371.40
615.32

0.05
0.05
0.05
0.05

0.00
0.00
0.00
0.00

Table 3: Cargo management control results. Proﬁt and online time averaged over 1,000 trajectories. All times are in
seconds.

4.3 Control Algorithms and Baselines

For our control algorithms, we implement two variants of DQN, DQN-L and DQN-S as described in Section 2.3. The
DQN control results reported in the following section are obtained using random search over 100 conﬁgurations and
selecting the model which obtains the highest mean reward over 100 validation trajectories. Detailed results of the
model selection and variance are reported in Appendix B.3. In addition, we report the mean reward on the validation
trajectories throughout the DQN training in Appendix C.

When deploying the trained DQN on the evaluation instances we additionally impose the capacity restrictions by simply
rejecting requests which will violate the constraints (in expectation for the cargo management case). Whereas this is not
strictly necessary, given that a violation of constraints results in a signiﬁcant penalty, we did empirically notice a slight
improvement when imposing constraints during evaluation. This may be directly handled by more sophisticated RL
algorithms which can explicitly impose hard constraints (Donti et al. 2021, Ryu et al. 2020).

In terms of baselines, for distributional logistics we compare to BLP and BLPR as described in Section 3.1.2, and
for airline cargo management we compare to DLP and DPD as described in Section 3.2.2. In our implementation of
BLP/BLPR we use a branch-and-cut procedure for the subtour elimination constraints (15) to solve the problems to
optimality quite efﬁciently rather than relying on a ﬁxed number on incumbent solutions as is done in Giallombardo
et al. (2022). In addition, we implement a naive ﬁrst-come-ﬁrst-serve (FCFS) baseline, which simply accepts requests
until a capacity limit is reached.

4.4 Control Results

We now turn our attention to the main results of the paper. We compare the quality and speed of the control policies
computed with our methodology to the baselines. In Tables 2 and 3, we report mean proﬁts, as well as online and ofﬂine
computing times for the distributional logistics and airline cargo management applications, respectively. The proﬁt and
online computing times are averaged over 1,000 trajectories, while the ofﬂine time is incurred once before any policy
evaluation is done. Note that the ofﬂine computing time does not include the time to generate the data and train the
supervised learning models.

In addition to the tables, we report trajectory-speciﬁc gaps to the best known solutions as box plots in Figures 2-4. The
gap is computed as 100 · Best Reward−Algorithm Reward
, where Best Reward is the best known solution between all of the
algorithms for a given trajectory, and Algorithm Reward is the reward for a speciﬁc algorithm on the same trajectory.
Whereas this is a non-standard way of reporting the gaps, we use Best Reward as opposed to Algorithm Reward in
the denominator in view of the signiﬁcant difference in performance between our approach and the baselines for the
distributional logistics application.

Best Reward

Based on these results, a number of ﬁndings emerge. For the distributional logistics application, both DQN-L and
DQN-S perform signiﬁcantly better than BLP and BLPR in both the low and high-load factor settings. The primary

13

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Figure 2: Gap to best known solution for distributional logisitics (VRP_X_L).

Figure 3: Gap to best known solution for distributional logisitics (VRP_X_H).

reason for this is that booking-limit policies from BLP and BLPR are simply not adequate in the case where locations
are not expected to receive a large number of requests. In fact, BLP and BLPR are signiﬁcantly worse than FCFS in this
case, since for the majority of the locations the booking-limit policy provides a threshold of zero.

DQN is able to achieve a control of better quality than FCFS that improves with increased load factor. This is in line
with our expectations as the value of making strategic decisions increases as less requests can be accepted. In terms of
our policies, DQN-S generally outperforms DQN-L, especially as the number of locations increases. We hypothesize
that this is due to DQN-S having a better representation of the locations and features of the previously accepted requests,
especially as the number of locations increases.

For the airline cargo management application, we can see that DQN-L and DQN-S are able to achieve policies which
are quite close to state-of-the-art. Speciﬁcally when compared to DPD, the percentage improvement in mean proﬁt
ranges between -2.2% and 0.7% (i.e, DQN provides a slight improvement compared to DPD). Since DPD is a very
strong baseline that leverages problem-speciﬁc structure, we did not expect to outperform it. The fact that our general
methodology achieves a comparable performance is evidence of the potential of RL for booking control. Moreover,
compared to DLP, which is less problem-speciﬁc than DPD, we can see that we are able to obtain higher-quality policies
in three out of the four instances for both DQN-L and DQN-S. Comparing DQN-L to DQN-S, we can see that there is
no signiﬁcant difference between the two for this instance size.

14

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Figure 4: Gap to best known solution for airline cargo management.

As expected, the ofﬂine computing time is much larger for DQN-L and DQN-S than the other algorithms. Nevertheless,
they are still reasonable for an end-to-end RL approach as they take less than six hours in the worst case when combining
data generation, supervised learning training, and DQN training. The algorithm can hence be retrained in an overnight
computing time budget.

We close this section by discussing the reduction in computing time resulting from the supervised learning phase of our
approach. If the supervised learning prediction were unavailable, solving or approximating 30,000 and 1,500,000 EoHPs
would be required to train a single DQN conﬁguration for the distributional logistics and airline cargo management
applications, respectively. The larger number – 1,500,000 EoHPs – is required as we sample 50 EoHP problems for
each trajectory in the airline cargo management application.

Let us analyze the time spent evaluating the EoHP within a DQN training run. We compare them in a completely
sequential manner. We take VRP_50_H as an example. Solving a single EoHP takes 4.43 seconds on average with
FILO whereas our ML approximation with bin-packing takes 0.02 seconds. We extrapolate to compute the total time
by multiplying these ﬁgures with the number of EoHPs evaluated during DQN training (30,000), which results in
132,979.20 seconds (∼ 37 hours) for FILO and 729.05 seconds for our approximation.

Similarly for CM_1.0_1.0, solving a single EoHP takes 0.31 seconds on average while our ML approximation takes 0.5
milliseconds. Multiplying these ﬁgures with the number of EoHPs evaluated during DQN training (1,500,000) results
in 464070.00 seconds (∼ 129 hours) for the exact algorithm and 753.05 seconds for our approximation. We note that in
our actual implementation this cost is roughly 50 times lower, as all 50 EoHP realizations are fed through the neural
network in a single batch.

Overall, the time spent approximating the EoHP cost during simulation with ML is several orders of magnitude
smaller than with the algorithms used to compute the labels. It is very unlikely that more efﬁcient implementations or
approximations of the EoHPs would achieve a time close to the nearly negligible amount of time required by our ML
approximation. Importantly, this time is expected to be nearly invariant with respect to the speciﬁc EoHP structure.
This hence opens up the possibility to use less crude approximations of the end-of-horizon operational decision-making
problem. In turn, this may result in policies that better reﬂect real operations (e.g., a three-dimensional knapsack instead
of a vector packing problem formulation in the airline cargo case). The latter is not evaluated in the performance metrics
reported here as it would require a ﬁeld study.

5 Conclusion

In this work, we focused our attention on freight booking control and proposed a two-phase approach, which ﬁrst
estimates the cost of the EoHP via supervised learning, then deploys the trained model to signiﬁcantly speed up
simulation for RL algorithms. Our approach can be extended to any freight booking control problem, regardless of the
EoHP, as long as one can adequately compute an exact or relevant approximation to the objective function value. In
addition, since a trained model can provide fast estimates for any EoHP, our approach can be scaled to problems with

15

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

hard EoHPs. Through our computational experiments, we demonstrated that our approach is able to achieve control
policies that surpass the state-of-the-art by a signiﬁcant margin in a distributional logistics application and a policy
comparable to a strong problem-speciﬁc algorithm in airline cargo management.

In terms of future work, we identify ﬁve potential directions. First, as this work uses DQN for computing control
policies, the extension to other RL algorithms may lead to even better performing control. In addition, with the capacity
constraint structure of the problem, one can consider RL algorithms which can explicitly enforce these constraints such
as in Donti et al. (2021) and Ryu et al. (2020).

Second, determining better baselines or bounds (e.g., Brown et al. 2010, 2022) would be useful in understanding the
performance of our approach in regard to the application to distributional logistics since the policies computed by BLP
and BLPR were not of high enough quality to provide a meaningful comparison.

Fourth, it may be worth exploring an iterative improvement of the supervised learning model during the training of
the DQN. As the DQN trajectories improve with training, having a representative sample of those EoHPs may further
improve the EoHP predictor. We note that this process could be performed in parallel with the training of the DQN, so
that the computing cost associated with these improvements would be negligible.

Lastly, the broadest direction for future work is the extension of our methodology to other freight booking control
problems and more generally other sequential decision-making problems which involve solving discrete optimization
problems during simulation.

Acknowledgments

We are grateful to Luca Accorsi and Daniele Vigo who provided us access to their VRP code FILO Accorsi and
Vigo (2021), which has been very useful for our research. We thank Francesca Guerriero for sharing details on the
implementation of the approach in Giallombardo et al. (2022). We would also like to thank Pierre-Luc Bacon for
valuable comments on this work, and we express our gratitude to Eric Larsen and Rahul Patel for their comments that
helped us improve the manuscript. Computations were made on the clusters managed by Calcul Québec and Compute
Canada.

16

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

References

Accorsi L, Vigo D (2021) A fast and scalable heuristic for the solution of large-scale capacitated vehicle routing problems.

Transportation Science 55(4):832–856.

Adelman D (2007) Dynamic bid prices in revenue management. Operations Research 55(4):647–661.
Amaruchkul K, Cooper WL, Gupta D (2007) Single-leg air-cargo revenue management. Transportation science 41(4):457–469.
Barz C, Gartner D (2016) Air cargo network revenue management. Transportation Science 50(4):1206–1222.
Bengio Y, Lodi A, Prouvost A (2021) Machine learning for combinatorial optimization: a methodological tour d’horizon. European

Journal of Operational Research 290(2):405–421.

Bertsekas DP (2019) Reinforcement learning and optimal control (Athena Scientiﬁc Belmont, MA).
Bondoux N, Nguyen AQ, Fiig T, Acuna-Agost R (2020) Reinforcement learning applied to airline revenue management. Journal of

Revenue and Pricing Management 19(5):332–348.

Brandt F, Nickel S (2019) The air cargo load planning problem - a consolidated problem deﬁnition and literature review on related

problems. European Journal of Operational Research 275:399–410.

Breiman L (2001) Random forests. Machine Learning 45(1):5–32.
Brown DB, Smith JE, Sun P (2010) Information relaxations and duality in stochastic dynamic programs. Operations research

58(4-part-1):785–801.

Brown DB, Smith JE, et al. (2022) Information relaxations and duality in stochastic dynamic programs: A review and tutorial.

Foundations and Trends in Optimization 5(3):246–339.

Buconiu L, de Bruin T, Tolic D, Kober J, Palunko I (2018) Reinforcement learning for control: Performance, stability, and deep

approximators. Annual Reviews in Control 46:8–28.

Caprara A, Toth P (2001) Lower bounds and algorithms for the 2-dimensional vector packing problem. Discrete Applied Mathematics

111(3):231–262.

Dai H, Xue Y, Syed Z, Schuurmans D, Dai B (2022) Neural stochastic dual dynamic programming. International Conference on

Learning Representations.

Donti PL, Roderick M, Fazlyab M, Kolter JZ (2021) Enforcing robust control guarantees within neural network policies. In

International Conference on Learning Representations.

Dumouchelle J, Patel R, Khalil EB, Bodur M (2022) Neur2SP: Neural two-stage stochastic programming. ArXiv preprint

arXiv:2205.12006.

Finn C, Yu T, Fu J, Abbeel P, Levine S (2017) Generalizing skills with semi-supervised reinforcement learning. International

Conference on Learning Representations.

Fischetti M, Fraccaro M (2017) Using OR + AI to predict the optimal production of offshore wind parks: a preliminary study.

International Conference on Optimization and Decision Science, 203–211 (Springer).

Giallombardo G, Guerriero F, Miglionico G (2022) Proﬁt maximization via capacity control for distribution logistics problems.

Computers & Industrial Engineering 171:108466.

Gosavi A (2009) Reinforcement learning: A tutorial survey and recent advances. INFORMS Journal on Computing 21(2):178–192.
Gosavi A, Bandla N, Das TK (2002) A reinforcement learning approach to a single leg airline revenue management problem with

multiple fare classes and overbooking. IIE Transactions 34(9):729–742.

Gurobi Optimization, LLC (2021) Gurobi Optimizer Reference Manual. URL https://www.gurobi.com/documentation/9.5/

refman/index.html.

Henderson J, Lemon O, Georgila K (2005) Hybrid reinforcement/supervised learning for dialogue policies from communicator data.

IJCAI workshop on knowledge and reasoning in practical dialogue systems.

Kasilingam RG (1997) Air cargo revenue management: Characteristics and complexities. European Journal of Operational Research

96(1):36–44.

Kastius A, Schlosser R (2022) Dynamic pricing under competition using reinforcement learning. Journal of Revenue and Pricing

Management 21(1):50–63.

Kephart JO, Tesauro GJ (2000) Pseudo-convergent Q-learning by competitive pricebots. Proceedings 17th International Conference

Machine Learning, 463–470 (Morgan Kaufmann).

Klein R, Koch S, Steinhardt C, Strauss AK (2020) A review of revenue management: Recent generalizations and advances in industry

applications. European Journal of Operational Research 284(2):397–412.

Kotary J, Fioretto F, Van Hentenryck P, Wilder B (2021) End-to-end constrained optimization learning: A survey. Proceedings of the

Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI-21, 4475–4482.

Larsen E, Frejinger E, Gendron B, Lodi A (2022a) Fast continuous and integer L-shaped heuristics through supervised learning.

ArXiv preprint arXiv:2205.00897v2.

Larsen E, Lachapelle S, Bengio Y, Frejinger E, Lacoste-Julien S, Lodi A (2022b) Predicting tactical solutions to operational planning

problems under imperfect information. INFORMS Journal on Computing 34(1):227–242.

17

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Lawhead RJ, Gosavi A (2019) A bounded actor–critic reinforcement learning algorithm applied to airline revenue management.

Engineering Applications of Artiﬁcial Intelligence 82:252 – 262.

Levin Y, Nediak M, Topaloglu H (2012) Cargo capacity management with allotments and spot market demand. Operations Research

60(2):351–365.

Levina T, Levin Y, McGill J, Nediak M (2011) Network cargo capacity management. Operations Research 59(4):1008–1023.
Liang E, Liaw R, Nishihara R, Moritz P, Fox R, Goldberg K, Gonzalez J, Jordan M, Stoica I (2018) Rllib: Abstractions for distributed

reinforcement learning. International Conference on Machine Learning, 3053–3062.

Martello S, Toth P (1990) Knapsack problems: Algorithms and computer implementations (USA: John Wiley & Sons Ltd.).
Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G,
Petersen S, Beattie C, Sadik A, Antonoglou I, King H, Kumaran D, Wierstra D, Legg S, Hassabis D (2015) Human-level
control through deep reinforcement learning. Nature 518(7540):529–533.

Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang
E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S (2019) Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, volume 32, 8026–8037.
Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas
J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E (2011) Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research 12:2825–2830.

Prates M, Avelar PH, Lemos H, Lamb LC, Vardi MY (2019) Learning to solve NP-complete problems: A graph neural network for

decision tsp. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, 4731–4738.

Rana R, Oliveira FS (2014) Real-time dynamic pricing in a non-stationary environment using model-free reinforcement learning.

Omega 47:116–126.

Ryu M, Chow Y, Anderson R, Tjandraatmadja C, Boutilier C (2020) CAQL: Continuous action Q-learning. International Conference

on Learning Representations.

Seo DW, Chang K, Cheong T, Baek JG (2021) A reinforcement learning approach to distribution-free capacity allocation for sea

cargo revenue management. Information Sciences 571:623–648.

Shihab SAM, Logemann C, Thomas DG, Wei P (2019) Autonomous airline revenue management: A deep reinforcement learning

approach to seat inventory control and overbooking. ArXiv preprint arXiv:1902.06824.

Shukla N, Kolbeinsson A, Marla L, Yellepeddi K (2020) From average customer to individual traveler: A ﬁeld experiment in airline

ancillary pricing. Available at SSRN URL http://dx.doi.org/10.2139/ssrn.3518854.

Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot
M, Dieleman S, Grewe D, Nham J, Kalchbrenner N, Sutskever I, Lillicrap T, Leach M, Kavukcuoglu K, Graepel T, Hassabis
D (2016) Mastering the game of Go with deep neural networks and tree search. Nature 529(7587):484–489.
Sutton RS, Barto AG (2018) Reinforcement Learning: An Introduction (Cambridge, MA, USA: A Bradford Book).
Talluri KT, Ryzin GJV (2004) The Theory and Practice of Revenue Management (Springer US).
Toth P, Vigo D (2002) The vehicle routing problem (SIAM).
Van Hasselt H, Guez A, Silver D (2016) Deep reinforcement learning with double Q-learning 30(1).
Williamson EL (1992) Airline network seat inventory control: Methodologies and revenue impacts. Ph.D. thesis, Massachusetts

Institute of Technology.

Ye C, Yung NH, Wang D (2003) A fuzzy controller with supervised learning assisted reinforcement learning algorithm for obstacle

avoidance. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 33(1):17–27.

Zaheer M, Kottur S, Ravanbakhsh S, Poczos B, Salakhutdinov RR, Smola AJ (2017) Deep sets. Guyon I, Luxburg UV, Bengio
S, Wallach H, Fergus R, Vishwanathan S, Garnett R, eds., Advances in Neural Information Processing Systems, volume 30
(Curran Associates, Inc.).

18

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

A Description of Instances

In this section, we provide detailed descriptions of how the instances were generated for both the distributional logistics
and airline cargo management applications.

A.1 Distributional Logistics

We detail the parameters that deﬁne the 8 sets of instances in the distributional logistics application. In each of the
instances the capacity is determined to be inversely proportional to the demand outstanding at the locations. Speciﬁcally,
a load factor, LF , is used, and the capacity is given by

(cid:106)

Q =

(cid:80)

j∈N ∪{0}

(cid:80)T

t=1 pt
j

K0 · LF

(cid:107)
.

(38)

This load factor is used to determine capacity differences between the instances ending in L and H, where L corresponds
to a low load factor and H corresponds to a high load factor. This is done to create two distinct instances where
accept/reject decisions become more crucial as the load factor is increased. The locations are determined by sampling
2D coordinates uniformly at random in a box. For each instance, we specify the number of locations, periods, load
factors, number of no cost vehicles, additional vehicle costs, and location bounds in Table 4.

Instance

# locations

# periods Load factor

# No cost vehicles Additional vehicle cost

Location bounds

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H

4
10
15
50
4
10
15
50

20
30
50
100
20
30
50
100

1.1
1.1
1.1
1.1
1.8
1.8
1.8
1.8

2
3
3
3
2
3
3
3

100
100
250
600
100
100
250
600

[0, 10]
[0, 10]
[0, 10]
[0, 50]
[0, 10]
[0, 10]
[0, 10]
[0, 50]

Table 4: Summary of instances generating parameters for distributional logistics.

For the proﬁt we aim to generate instances which have a higher probability of high proﬁt requests later in the booking
period. We now detail how the probability and pricing parameters are deﬁned to generate each instance in the
distributional logistics application.

VRP_4_{L,H}. The proﬁts for accepting a request from each location are deﬁned by r1 = 4, r2 = 8, r3 = 12, r4 = 16.
The probability of no request is pt
0 = 0.10, for t ∈ {1, . . . , T }. The initial probability of a request for each location is
2 = 0.40, p1
1 = 0.45, p1
p1
4 = 0.05. For the remainder of the periods, the probability of a request for each
location is given by pt+1

j − 0.01 for j ∈ {1, 2} and pt+1

j + 0.01 for j ∈ {3, 4}.

3 = 0.10, p1

j = pt

j = pt

VRP_10_{L,H}. The proﬁts for accepting a request from each location are deﬁned by rj = 10 for j ∈ {1, . . . , 4},
rj = 12 for j ∈ {5, . . . , 8}, and rj = 20 for j ∈ {9, 10}. The probability of no request is pt
0 = 0.10, for t ∈ {1, . . . , T }.
j = 0.125 for j ∈ {1, . . . 4}, p1
The initial probability of a request for each location is p1
j = 0.075 for j ∈ {5, . . . , 8},
and p1
j = 0.05 for j ∈ {9, 10}. For the remainder of the periods, the probability of a request for each location is given
by pt+1
j = pt

j − 0.001 for j ∈ {1, . . . 4}, pt+1

j for j ∈ {5, . . . , 8}, and pt+1

j + 0.002 for j ∈ {9, 10}.

j = pt

j = pt

VRP_15_{L,H}. The proﬁts for accepting a request from each location are deﬁned by rj = 10 for j ∈ {1, . . . , 5},
rj = 12 for j ∈ {6, . . . , 10}, and rj = 20 for j ∈ {11, . . . , 15}. The probability of no request is pt
0 = 0.10, for
t ∈ {1, . . . , T }. The initial probability of a request for each location is p1
j = 0.10 for j ∈ {1, . . . , 5}, p1
j = 0.06 for
j ∈ {6, . . . , 10}, and p1
j = 0.02 for j ∈ {11, . . . , 15}. For the remainder of the periods, the probability of a request for
each location is given by pt+1
j = pt
j + 0.001
for j ∈ {11, . . . , 15}.

j − 0.001 for j ∈ {1, . . . , 5}, pt+1

j for j ∈ {6, . . . , 10}, and pt+1

j = pt

j = pt

VRP_50_{L,H}. The proﬁts for accepting a request from each location are deﬁned by rj = 15 for j ∈ {1, . . . , 30},
rj = 22 for j ∈ {31, . . . , 40}, and rj = 30 for j ∈ {41, . . . , 50}. The probability of no request is λt
0 = 0.10, for
t ∈ {1, . . . , T }. The initial probability of a request for each location is p1
j = 0.03
for j ∈ {31, . . . , 40}, and p1
j = 0.01 for j ∈ {41, . . . , 50}. For the remainder of the periods, the probability of a
request at for each location is given by pt+1
j for j ∈ {31, . . . , 40}, and
pt+1
j = pt
j + 0.0003 for j ∈ {41, . . . , 50}.

j − 0.0001 for j ∈ {1, . . . , 30}, pt+1

j = 0.0166 for j ∈ {1, . . . , 30}, p1

j = pt

j = pt

19

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

A.2 Airline Cargo Instance

We describe the parameters deﬁning the airline cargo instance in the second application. These instances are taken
directly from Barz and Gartner (2016). In our setting the number of periods is given by T = 60. The parameter which
determines the chargeable weight is given by η = 0.6. The number of shipment classes is n = 24. As mentioned
previously, whereas the realizations of the weight and volume are given at the end of the booking period, the expectations
of each class are known and provided in Table 5, where ¯wj and ¯vj denote the expected weight and volume of class
j. The realized weight and volume of a request of class j at the end of a booking period are given by sampling a
multivariate normal distribution with mean µ = [ ¯wj, ¯vj] and covariance matrix

Σ =

(cid:18) (σw
j )2
j σv
0.8σw
j

0.8σw
j σv
j
j )2
(σv

(cid:19)

,

(39)

where σw

j = 0.25 ¯wj and σv

j = 0.25¯vj.

Class

¯wj (kg)
¯vj (1e4 cm3)

1

50
30

2

50
29

3

50
27

4

50
25

5

100
59

6

100
58

7

100
55

8

100
52

9

200
125

10

200
119

11

250
100

12

240
147

13

300
138

14

400
179

15

500
235

16

500
277

17

1000
598

18

1500
898

19

2500
1488

20

3500
2083

21

70
244

22

70
17

23

210
700

24

210
52

Table 5: Airline Cargo: Expected weights and volumes.

Let wj and vj be the realized weight and volume of a request of class j. The off-loading cost of the request is
given by cj · max{wj, vj/η}, where cj = 2.4 for j = 1, . . . , 24. The proﬁt is given by α · max{wj, vj/η}, where
α ∈ {0.7, 1.0, 1.4}. Here, α is used in order to ensure that more proﬁtable requests occur later in the booking period.
Table 6 gives the probability that α takes each value over the time horizon as well as the probability that no request
occurs. Given that a request occurs, the probability of the request for each class is provided in Table 7.

Request

Periods Range

1-20

21-40

41-60

P r(α = 0.7)
P r(α = 1.0)
P r(α = 1.4)
P r(No request)

0.7
0.2
0.0
0.1

0.4
0.2
0.4
0.0

0.0
0.2
0.7
0.1

Table 6: Airline Cargo: Proﬁt per chargeable weight probabilities.

Class

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

Probability

0.072

0.072

0.072

0.072

0.072

0.072

0.072

0.072

0.072

0.072

0.040

0.040

0.040

0.040

0.040

0.040

0.009

0.009

0.009

0.009

0.001

0.001

0.001

0.001

Table 7: Airline Cargo: Class request probabilities.

Similar to the weight and volume of a request, the capacities are realized at the end of the booking period. The expected
weight and volume capacities are given as a ratio of the total expected weight and volume of the requests. In the context
of this work, we consider all single-leg instances from Barz and Gartner (2016). The expected weight capacity and
volume capacity are denoted by ¯Qw and ¯Qv, respectively. At the end of the booking period, the realized weight and
volume capacities are given by sampling a multivariate normal distribution with mean µ = [ ¯Qw, ¯Qv] and covariance
matrix

Σ =

(cid:18) (σQw )2

0.8σQw σQv

0.8σQw σQv
(σQv )2

(cid:19)

,

(40)

where σQw = 0.25 ¯Qw and σQv = 0.25 ¯Qv.
Finally, we detail the differences between the instances. For instance CM_X_Y, X speciﬁes the ratio of expected weight
capacity in comparison to the expected weight of all requests, while Y speciﬁes the same for volume. For example for
the instance CM_0.5_1.0, the expected total weight of all requests is twice as large as the expected weight capacity, and
the expected total volume of all requests is equal to the expected volume capacity.

20

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

B Model Selection and Dataset Sizing

In this section, we report on the effects of variations in dataset sizes and hyperparameter conﬁgurations for both the
supervised learning model for predicting the EoHP cost and the DQN models for control.

B.1 Dataset Sizing

As supervised learning is a core component of our methodology, determining an estimate of a good number of samples
is important. Ideally this would be done for every instance under consideration. However, this would impose a large
computational burden, so we restrict our attention to a single instance setting, namely, CM_0.5_0.5 and generate training
datasets with 500, 1,000, 5,000, 10,000, and 20,000 samples. We then train LR, RF, and NN on each training set size
and evaluate every model on the same 5,000 validation samples. The validation MAEs for each model type and dataset
size are reported in Figure 5. From this ﬁgure, we can see that with LR and NN, there is no signiﬁcant improvement
beyond 5,000 samples, so for this reason we use 5,000 training samples when generating data for all of the instances.

Figure 5: Dataset sizing in airline cargo management (CM_0.5_0.5).

B.2 Supervised Learning Model Selection

For the supervised learning models, we perform a hyperparameter search only for the NN models as they signiﬁcantly
outperform both LR and RF. For both LR and RF we use the defaults provided in Scikit-learn, while for the permutation-
invariant model, we perform a small random search over the hyperparameters according to Table 8. The batch
size, learning rate, activation function, and # of epochs are all relatively standard hyperparameters, so we omit their
description. However, the remainder are speciﬁc to our permutation-invariant model. To simplify the hyperparameter
search for the NN models, we consider a ﬁxed-depth network and allow the width to be a conﬁgurable hyperparameter.
Embed hidden dimension 1 and Embed dimension 2 indicate the width of the two layers of Ψ1, Embed hidden dimension
3 is the width of Ψ2 and Concat is the width of the single layer in Φ. Ψ1, Ψ2 and Φ are illustrated in Figure 1.

Within the search space speciﬁed in Table 8, we perform random search over 50 conﬁgurations. The best hyperparameters
for each instance are reported in Table 9.
In addition, we provide box plots showing the validation MAE over
hyperparameter conﬁgurations in Figures 6 and 7 for the distributional logistics and airline cargo management,
respectively. Each box plot shows the variations in MAE over the hyperparameter conﬁgurations for a particular
instance. We conclude that we are generally able to achieve trained models of good quality reliably for the majority of
the instances, with the exception of the larger distributional logistics instances with 50 nodes. It is worth noting here
that even the worst performing models in the distributional logistics case actually achieve a very low MAE, especially
when compared to the target distribution, which is in the order 102.

We note that due to an implementation error, the best hyperparameters reported for all distributional logistics were
chosen based on the minimal training MAE, not validation MAE. The same minimal training MAE model was used

21

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Hyper parameter name

Parameter space

Batch size
Learning rate
Activation Function
# Epochs
Embed hidden dimension 1
Embed hidden dimension 2
Embed hidden dimension 3
Concat hidden dimension

{16, 32, 64, 128}
[1e−5, 1e−2]
{ReLU, Leaky Relu}
1000
{64, 128, 256}
{64, 128, 256}
{64, 128, 256}
{256, 512, 1024}

Table 8: Random search hyperparameter space for the neural network used in supervised learning. For values in {}, we
sample with equal probability for each discrete choice. For values in [], we sample a uniform distribution within the
given bounds. For single values, we keep it ﬁxed across all conﬁgurations.

Instance

Learning rate Activation function Batch size Embed hidden dimension 1 Embed hidden dimension 2 Embed hidden dimension 3 Concat hidden dimension

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H
CM_0.5_0.5
CM_1.0_0.5
CM_0.5_1.0
CM_1.0_1.0

0.00013
0.00229
0.00055
0.00075
0.00125
0.00071
0.00051
0.00062
0.00043
0.00023
0.00046
0.00032

Leaky ReLU
Leaky ReLU
Leaky ReLU
ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
ReLU
ReLU
Leaky ReLU

16
128
32
64
64
32
16
32
32
16
16
64

64
64
256
256
128
256
128
64
128
128
128
128

256
128
128
256
256
128
128
128
256
128
64
64

128
256
256
256
256
256
128
128
128
64
128
64

1024
512
1024
512
256
512
256
512
1024
1024
512
1024

Table 9: Supervised learning best conﬁgurations from random search.

within the DQN experiments. Due to the minimal differences in validation MAE, the impact is likely insigniﬁcant and
if anything would only further improve our control results. Hence, we did not rerun all of the DQN experiments before
submitting the manuscript. This does not affect the results reported in Figure 6.

Figure 6: Distributional Logistics: Box plots of MAE over random search conﬁgurations.

B.3 DQN Model Selection

For DQN, we perform random search over the hyperparameters for both the linear and set-based value function
estimators. The hyperparameters for which a search is conducted are listed in Table 10. We omit the description of
the batch size, learning rate, and activation function as they are standard. One-hot period is a binary decision, which
if true encodes the period as a one-hot vector in the state representation, and if false encodes the period as an integer.
Double-DQN is a binary decision which if true uses double Q-learning (Van Hasselt et al. 2016), and if false uses
standard Q-learning. Hidden dimensions are only applicable to DQN-L and indicate the hidden dimensions for the
value function approximator. Embed hidden dimension {1,2,3} and Concat hidden dimension are the dimensions for
the permutation-invariant structure of DQN-S, and are similar to those deﬁned in Appendix B.2.

22

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

Figure 7: Airline Cargo Management: Box plots of MAE over random search conﬁgurations.

Hyper parameter name

Batch size
One-hot period
Learning rate
Activation Function
# Episodes
Double DQN
Hidden dimensions
Embed hidden dimension 1
Embed hidden dimension 2
Embed hidden dimension 3
Concat hidden dimension

DQN-L

{16, 32, 64, 128}
{True, False}
[1e−5, 1e−2]
{ReLU, LeakyReLU}
15000
{True, False}
{[128], [256], [512], [128, 128], [256, 256], [512, 512], [128, 128, 128], [256, 256, 256], [512, 512, 512]}
-
-
-
-

DQN-S

{16, 32, 64, 128}
{True, False}
[1e−5, 1e−2]
{ReLU, LeakyReLU}
15000
{True, False}
-
{64, 128, 256, 512}
{16, 32, 64, 128}
{8, 16, 32, 64}
{64, 128, 256, 512}

Table 10: Random search hyperparameter space for DQN-L and DQN-S. For values in {}, we sample with equal
probability for each discrete choice. For values in [], we sample a uniform distribution within the given bounds. For
single values, we keep it ﬁxed across all conﬁgurations. Note that for the hidden dimension, the value contained in []
are a list of the hidden dimension. A value of - indicates that hyperparameter is not applicable for the given model.

Within the search space speciﬁed in Table 10, we perform random search over 100 conﬁgurations for each of DQN-L
and DQN-S. The conﬁguration which achieves the best mean reward over 100 validation trajectories is reported in
Tables 11 and 12 for the DQN-L and SQN-S, respectively. Box plots of the variations in performance over the various
hyperparameter conﬁgurations appear in Figures 8-9. Here, we report the best mean reward over the same 100 validation
instances obtained throughout the training for each conﬁguration. We conclude that hyperparameter search leads to
marginal improvements. For most hyperparameter conﬁgurations, high-quality control is achieved.

Figure 8: Distributional Logistics: Box plots of reward over random search conﬁgurations.

23

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H
CM_0.5_0.5
CM_1.0_0.5
CM_0.5_1.0
CM_1.0_1.0

One-hot period Learning rate Activation function Batch size Dueling DQN Double DQN Hidden dimensions

True
False
False
False
True
True
True
False
True
True
True
False

0.00273
0.00084
0.0053
0.00257
0.0019
0.00045
0.00058
0.00173
0.00407
0.00098
0.00404
0.00874

LeakyReLU
ReLU
LeakyReLU
LeakyReLU
ReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
ReLU

32
128
64
32
32
128
32
64
64
64
128
32

False
False
False
False
False
False
False
False
False
False
False
False

True
False
True
True
False
True
False
True
False
True
False
False

Table 11: DQN-L best conﬁgurations from random search.

[512, 512]
[256]
[128]
[128, 128]
[256]
[128]
[128]
[256]
[512]
[512]
[128]
[128]

One-hot period Learning rate Activation function Batch size Dueling DQN Double DQN Embed hidden dimensions 1 Embed hidden dimensions 2 Embed hidden dimensions 3 Concat hidden dimensions 1

VRP_4_L
VRP_10_L
VRP_15_L
VRP_50_L
VRP_4_H
VRP_10_H
VRP_15_H
VRP_50_H
CM_0.5_0.5
CM_1.0_0.5
CM_0.5_1.0
CM_1.0_1.0

True
False
True
True
True
True
True
True
False
False
False
True

0.00034
0.00264
0.00087
0.00059
0.00162
0.00679
0.00112
0.00085
0.0026
0.0005
0.00043
0.00102

ReLU
ReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
ReLU

64
32
64
32
128
32
64
128
64
64
128
16

0
0
0
0
0
0
0
0
0
0
0
0

False
True
True
False
False
False
True
True
False
False
True
False

256
128
512
64
64
128
256
128
512
128
64
128

128
128
128
64
128
256
256
128
256
64
128
256

128
64
128
256
512
256
64
64
128
64
512
64

1024
128
512
1024
1024
256
1024
256
256
256
1024
1024

Table 12: DQN-S best conﬁgurations from random search.

Figure 9: Airline Cargo Management: Box plots of reward over random search conﬁgurations.

C Reinforcement Learning Training

For completeness, we provide in Figures 10 to 12 the training curves for the best DQN conﬁguration for each instance.
These curves report the mean reward over the 100 validation trajectories for each of the instances over the 15,000
episodes. We notice a relatively high variance between episodes. This is expected, given that we are only plotting the
reward curve for 1 seed, rather than multiple which is standard in RL. The curves indicate that convergence occurs in
most cases and that the difference between DQN-L and DQN-S is often negligible, with the exception of the larger
VRP instances. It is useful to note that convergence occurs typically in less than 5,000 episodes so that training with
15,000 may not be required.

24

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

(a) VRP_4_L

(b) VRP_10_L

(c) VRP_15_L

(d) VRP_50_L

Figure 10: DQN training curves for VRP_X_L.

25

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

(a) VRP_4_H

(b) VRP_10_H

(c) VRP_15_H

(d) VRP_50_H

Figure 11: DQN training curves for VRP_X_H.

26

RL for Freight Booking Control

Dumouchelle, Frejinger and Lodi

(a) CM_0.5_0.5

(b) CM_0.5_1.0

(c) CM_1.0_0.5

(d) CM_1.0_1.0

Figure 12: DQN training curves for CM_X_Y.

27

