1
2
0
2

r
p
A
5
1

]

G
L
.
s
c
[

3
v
7
3
1
0
0
.
4
0
1
2
:
v
i
X
r
a

Achieving Transparency Report Privacy in Linear Time

CHIEN-LUN CHEN and LEANA GOLUBCHIK, University of Southern California
RANJAN PAL, University of Michigan

An accountable algorithmic transparency report (ATR) should ideally investigate the (a) transparency of the
underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data sub-
jects’ privacy. However, a provably formal study of the impact to data subjects’ privacy caused by the utility
of releasing an ATR (that investigates transparency and fairness), is yet to be addressed in the literature. The
far-fetched beneﬁt of such a study lies in the methodical characterization of privacy-utility trade-oﬀs for
release of ATRs in public, and their consequential application-speciﬁc impact on the dimensions of society,
politics, and economics. In this paper, we ﬁrst investigate and demonstrate potential privacy hazards brought
on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects’ pri-
vacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming
(LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation
on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-oﬀs induced by
our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of
our knowledge, this is the ﬁrst analytical work that simultaneously addresses trade-oﬀs between the triad of
privacy, utility, and fairness, applicable to algorithmic transparency reports.

Additional Key Words and Phrases: privacy; algorithmic transparency; fairness; linear fractional program-
ming

1 INTRODUCTION
In the era of big data and machine learning (ML), automated data processing algorithms are widely
adopted in many ﬁelds for classiﬁcation, prediction, or decision-making tasks due to huge volumes
of input data and successful performance of ML approaches. Ongoing concerns and social uproar
about the transparency and fairness of such decision-making have been raised by the media, gov-
ernment agencies, foundations, and academics over the past decade [14, 68]. On a technical note,
it has been shown in example studies that ML algorithms can be biased when (i) a dataset used
to train ML models reﬂects society’s historical biases [86], e.g., only a few female presidential
nominees in the U.S. history, or (ii) because ML algorithms have much better understanding of
the majority groups and poor understanding of the minority groups [12], and so on. Thus, as we
rapidly move forward to a data-driven age where a signiﬁcant amount of day-day decision making
in personal and professional spheres might be automation-driven, it would make great sense to
often know the reasons behind certain decisions in order to understand if they are being treated
fairly. Unfortunately, most decision processes today are often opaque, making it diﬃcult to ratio-
nalize why certain decisions are made and whether they favor or disfavor certain individuals or
groups.

Providing an algorithmic transparency report (ATR) by data controllers and third party regu-
latory agencies to decision-facing individuals is one way to investigate whether decisions made
in a blackbox are fair and transparent [26, 36, 71] - an immediate application area of considerable
social impact being explainable AI for medical diagnoses [50, 77] to enable medical personnel bet-
ter understand and interpret diagnostic reports, and justify vulnerabilities of deployed AI models
through domain expertise. This is a popular topic in research and there have been works in the last
decade that have developed methodologies to reduce opaqueness in decision making [28, 46] and

Manuscript submitted to ACM

 
 
 
 
 
 
2

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Transparency
of ML models

Security & privacy
in ML

ATR

Fairness in ML

Fig. 1. A depiction of the realm of accountable ATRs

improve on its fairness relative to certain protected attributes1 [63]. The notion of transparency
has also made its way into recently implemented policies for data protection such as in the EU
General Data Protection Regulation (GDPR), and the California Consumer Privacy Act of 2018
(CCPA or AB-375) - both of which regulates the processing of collected personal or non-personal
data of any data subject (the natural person to whom the data and the decision process relate) [44].
More speciﬁcally, any data controller shall inform data subjects before collecting their data, and
is required to clearly explain the purpose of collecting data and how data will be processed, upon
data subjects’ requests (“right to explanation” and “right to non-discrimination”) [44]. However, a
major side eﬀect of providing transparency and fairness guarantees to the decision-facing clients is
an unwanted risk to the privacy of other clients in a database. To this end, there exists a substan-
tial literature pointing out potential privacy threats in ML [69], including membership attacks
[80], training data extraction (model inversion attack) [39, 40], model extraction [87], and so on.
However, for ATRs, although it has been pointed out that transparency, proposed by legislature
to protect people’s rights, may hurt privacy [8, 23], it is yet to be made methodically clear how
transparency can hurt privacy.

Goal - An accountable ATR, especially for automated ML decision processes, should ideally
include transparency of the underlying algorithm, ability to inspect fairness of the algorithmic de-
cisions, and most importantly, preserve data subjects’ privacy (“right to privacy” [20]), as depicted
in Fig. 1. Our goal in this paper is to work towards this goal and study the corresponding trade-oﬀs
between the triad elements.

In this paper, we investigate this problem and explicitly show that data subjects’ private infor-
mation can be inferred via various transparency schemes and fairness measures in announced
ATRs.

Research Contributions - We make the following contributions in this research paper:

• We explicitly demonstrate inference attacks on data subjects’ private information using a
synthetic and a real dataset and show that such attacks can be performed on various trans-
parency schemes without strong assumptions of adversaries’ knowledge. These instances ex-
pose the possible aspects of algorithmic transparency that could hurt data subjects’ privacy
and subsequently have negative socio-political implications. (See Section 3 and Appendix B)
• To protect data subjects’ privacy in an ATR, we propose a privacy-aware mechanism per-
turbing the unveiled rationale of opaque decision-making algorithms to control the amount
of disclosed information in ATRs, at the same time providing suﬃcient utility. Speciﬁcally,
given a released privacy-preserving ATR, for honest-but-curious adversaries which may know

1Protected attributes form a subset of attributes, to which any decision process should not show preference, in any instance.
It may contain public attributes (gender, race, etc.) and/or private/sensitive attributes (health conditions, gene, etc.).

Achieving Transparency Report Privacy in Linear Time

3

(i) the target individuals’ public information used as inputs to the decision model, (ii) the tar-
get individuals’ received decisions (model outputs), and (iii) side-information from auxiliary
sources, the maximum conﬁdence of inferring any sensitive information about any data sub-
ject is guaranteed not exceeding a predetermined privacy threshold (See Section 4 and 5).
• We study the trade-oﬀ between privacy and utility of ATRs. Speciﬁcally, we aim to under-
stand the minimum required perturbation/distortion in order to provide a certain privacy
guarantee, or the maximum privacy that can be provided/guaranteed subject to utility con-
straints, which can be formulated as an optimization problem for privacy-utility trade-oﬀ in
ATRs. In addition, we analyze the impact of privacy perturbations on fairness measures. In
this regard, our work provides useful quantitative trade-oﬀs and inﬂuences between privacy,
transparency, and fairness measures in ATRs (See Sections 5 and 6).

• We deduce that our privacy-utility optimization problem equivalent to a generalized linear
fractional programming problem (LFP) [16, 94]. Such a problem can in general be solved
as a sequence of linear programming feasibility problems, each with pseudo-polynomial
time complexity with respect to the number of problem variables (the number of diﬀerent
decision regions2 in our problem), which, however, in the worst case, grows exponentially
with the number of input attributes - hence lending the said optimization problem intractable
for large record sizes. However, on a closer investigation, we ﬁgure out that the region of
interest in the solution space can be decomposed into disjoint “subspaces” leading to multiple
independent sub-problems - each bearing important properties and amenable to propose
closed-form solutions. Subject to utility constraints, the optimal-privacy protection scheme
can thus be solved from the optimization problem eﬃciently in linear time (See Sections 7
and 8).

2 APPLICABILITY AND ETHICALITY OF THE PROPOSED PRIVACY SCHEME
In this section, we ﬁrst describe a range of possible application domains for the proposed ATRs
privacy protection scheme. We then discuss potential ethical concerns related to announcing a
perturbed “transparency” report, speciﬁcally, the conﬂict between principles of transparency and
perturbation for preserving data subjects’ privacy.

2.1 Applicability

The proposed privacy protection scheme, based on perturbing of decision mappings (Deﬁnition 1),
can be applied to numerous applications with characteristic that the decision regions of the appli-
cation decision process are disjoint ﬁnite sets of the input attribute space, i.e., the application decision
process can be represented by a ﬁnite number of decision mappings3. Although, to our knowledge,
to date there are no current instantiations of an algorithmic transparency report, we believe that
potential applicable domains for our scheme include the following (among others):

• University admissions and job recruitment: Fairness in university/college admissions has be-
come a signiﬁcant public concern, attracting more and more attention from the public as
well as the media [37], even though the fact that the deﬁnition of “fairness” is still contro-
versial, e.g., whether race should be used in admissions decisions to reﬂect racial diversity
[65, 90]. To respond to the public’s concerns, some governments [2] and universities [1] ini-
tiated work on admissions transparency, providing statistical data from applications (inputs)

2The regions of input attributes partitioned by decision rules; see Section 3 for examples.
3Our scheme may not be a good ﬁt for some application domains where it may not be possible to represent the decision
process using a ﬁnite number of decision mappings, e.g., applications of natural language processing, such as speech/music
recognition, speech/text understanding, and text/intent classiﬁcation, or applications of image and video processing, such
as text recognition, item detection and alert, and image classiﬁcation.

4

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

and admissions (outputs). Currently, we are not aware of instantiations disclosing the admis-
sion decision process; however, a negligent report could leak applications’ data or records,
e.g., (range of) SAT scores, competition records and ranks, extracurricular activities, or vol-
unteer work. Similar circumstances apply to applications and corresponding decisions in job
recruitment and other related domains.

• Credit scores and the associated domains: When it comes to evaluating and identifying every-
one’s ﬁnancial creditworthiness based on credit scores, people have the following concerns:
are their credit scores computed/treated fairly [49] and whether they have the ability to iden-
tify and contest any (potentially) unfair credit decisions [52]. Similar circumstances apply
to credit card applications [27] and a variety of loans, i.e., domains where credit scores may
be taken into account. In this paper, we demonstrate potential privacy hazards that could
be brought on by a bluntly disclosed ATR in credit card applications via various types of
transparency schemes and fairness measures using a synthetic (Section 3) and a real dataset
(Appendix B).

• Medical or pharmocogenetic models: As noted in [53], a pharmocogenetic model has been
built to predict proper dosages for patients based on their clinical histories, demographics,
and genotypes. However, it has been shown in [40] that once accurate information about the
model is leaked (or obtained through hacking), it can be utilized by an attacker to identify
patients’ genotypes, which could be exploited to further infer other private information, e.g.,
risk of getting a particular disease or someone’s family ancestry. In the ATR setting, we focus
on a related scenario where an adversary has no ability to access the pharmocogenetic model
internals but can merely gain model information from an announced ATR. In such a case,
our proposed privacy protection scheme can be applied to preserve patients’ privacy and
their genotype information.

• Open Government: It has been reported in [36] that nowadays governments utilize algorithms
to detect or to determine a variety of issues, such as illegal insider trading, eligibility for pub-
lic health beneﬁts, and tax evasion. In this regard, the Open Government organization [3]
aims to bring transparency to the data and the algorithms used by governments, aiming for
people and society to supervise governments’ actions and decisions. However, opening a
government blackbox can be very dangerous and can bring catastrophic results to society if
the released information is not carefully treated, and hence it is crucial to have a provable
privacy-preserving scheme for any planned-to-disclose information to protect people’s pri-
vacy and secret information (tax data, health/medical records, banking information, business
processes, and so on).

• Online advertising: ML algorithms can be biased [12, 86], and it has been shown that bias
also appears in online advertising, one of the ML applications that we probably experience
daily. In [78] authors indicate a bias and a privacy issue associated with online advertising
settings, particularly when users select the "Rather Not Say" category of gender. In addition,
[24] also found that setting the gender to female results in receiving fewer instances of
advertisements related to high paying jobs as compared to setting it to male. With concerns
about ML bias and the purpose of the collected data, GDPR and CCPA stipulate rights to
explanation and non-discrimination for data subjects; ad providers are required to respond
to data subjects’ requests regarding what data has been collected, how data is used, and if
the applied ML models treat them fairly. Thus, all the disclosed information in an ATR may
need to be further processed to protect data subjects’ privacy.

Achieving Transparency Report Privacy in Linear Time

5

2.2 Ethicality
When our proposed privacy protection scheme is applied to an ATR, the announced information
regarding the opaque decision process may be more or less distorted, and the announced measured
fairness/bias may also deviate from the true one. This may raise concerns about the ethicality
(manner of being ethical) of the process, i.e., whether the perturbed information could mislead the
public into trusting or believing that a biased decision process is fair, and vice versa.

Similarly to privacy preservation in data-mining (PPDM) and data-publishing (PPDP), a com-
mon theme is to ﬁnd an optimal trade-oﬀ between utility and privacy, subject to a certain degree
of privacy guarantee for data subjects. In the context of ATRs - although both transparency and
privacy are major principles in data ethics [60, 82] - we believe that data subjects’ privacy should
have higher priority [6]. Similarly to PPDM or PPDP, an auxiliary note could be appended with
the announced information indicating that some listed information might be anonymized or per-
turbed for data subjects’ privacy, which could help the public understand how to interpret the
disclosed information appropriately. Moreover, in light of this, in this work, we propose a ﬁdelity
measure (Section 5.2) for the announced decision mappings and characterize the inﬂuence of pri-
vacy perturbation on the measured fairness (Section 5.3). This information can also be disclosed
with the announced ATR in order to further assist the information recipients in understanding the
range of true measures.

3 DEMONSTRATING PRIVACY LEAKAGE VIA AN ATR

As a necessary and important step, we ﬁrst motivate our research by comprehensively demonstrat-
ing via an example consumer database of how a data subject’s (i.e., consumer’s) private information
can be leaked via an announced algorithmic transparency report (ATR). In this work we only focus
on reports that provide a rationale on the use of ML models to process individual records. As sec-
tion structure, we start by brieﬂy reviewing transparency approaches on which privacy leakages
can be induced, and follow it up with a speciﬁc example of privacy leakage on each transparency
approach.

3.1 Algorithmic Transparency Report (ATR) in a Nutshell

ML models used to make decision on consumer individuals are often opaque to the latter, and act
as blackboxes. A survey of popularity-gaining transparency schemes to explain ML blackboxes is
provided in [46]. A common representative (from the survey) transparency approach collects both
input data and labeled outputs (decision outcomes) as a training dataset, to train an ML surrogate
model (e.g., linear model, logistic regression, decision tree, decision rules) to mimic the behavior of
the blackbox. Popular methods include Anchors [74] and PALM [57]. The output of such learned
behavior must be interpret-able (understandable) by humans. Another common approach (e.g., [18,
38]) extracts certain important “properties” from blackbox models, such as contributions of input
features, to model outputs. Speciﬁcally, these transparency schemes measure feature importance
(based on the underlying 𝐷), using both amplitude and sign to represent importance/inﬂuence of
input features, where larger amplitude represents greater inﬂuence, and the sign indicates positive
or negative eﬀect on the output. Popular methods include LIME [73], FIRM [93], QII [23], and
Shapley Value [56], PDP [41], ICE [43], and ALEPlot [10]. In addition to transparency schemes,
an ATR may also provide information regarding whether a decision algorithm or ML model is
biased against a certain group or individual - in other words, an ATR may measure individual or
group fairness of ML-based decisions based on the diﬀerent desired metrics discussed in existing
literature [15, 22, 30, 35, 54, 55, 92]. We refer readers to Appendix A for detailed deﬁnitions of
various individual and group fairness measures. In what follows, we investigate and demonstrate

6

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 1. A Synthetic Credit Card Application Scenario

Gender

Input Attributes
Annual
Income
< 100k
100k∼200k
> 200k
< 100k
100k∼200k
> 200k

F
F
F
M
M
M

Adversaries’ Knowledge
ATR
Decision
Rule
0
0
1
0
0.5
1

Side-Info
Census
Statistics
93.1%
5.7%
1.2%
84.2%
12.3%
3.5%

Popu-
lation
139
9
2
117
18
5

privacy leakage instances via various kinds of transparency schemes and fairness measures, given
honest-but-curious adversaries.

3.2 Privacy Leakage via Interpretable Surrogate Models

As noted, transparency schemes can interpret a blackbox’s rules in a human-understandable man-
ner, such as decision rules or decision trees. Here, we explain how such transparent information
can hurt a data subject’s privacy. Without loss of representativity, here we set up a synthetic sce-
nario, in which we consider the existence of a perfect interpretable surrogate model4, to illustrate
the possibility of causing a catastrophic privacy leak.

Consider the following synthetic credit card application scenario (summarized in Table 1). A
credit card application takes several input attributes from applicants, while the bank’s decision
process only depends on two input attributes: the applicants’ annual income and their gender
(which, depending on the country, may be illegal and in those cases should not be used in any
decision process). Due to the suspicious diﬀerences in approval rates between male and female
applicants, a third-party regulatory agency actively takes action. It collects all applicants’ data and
their received decisions, and trains an (assumed perfect) interpretable surrogate model, disclosing
the decision rules used in the credit card application to all past applicants, as follows
𝑑 ({Income} > 200k) = 1,
𝑑 ({Income} ∈ 100k∼200k, Male) = 0.5,

where 𝑑 (·) is decision rule representing the probability of receiving a positive decision given the
condition. An equivalent if-then decision rule form is the following

if Income > 200k, then Positive Decision;
if 100k ≤ Income ≤ 200k ∧ Male, then Random;
otherwise, then Negative Decision.





Note that other interpretable surrogate models such as a decision tree or logistic regression can
also be equivalently expressed by decision rule 𝑑 (·).

Next, we demonstrate how data subjects’ sensitive information (annual income in this scenario)
could be leaked. Revisit Table 1 in which the key input attributes, population, and decision rule of
the credit card application are listed. Population of applicants are aggregated according to decision

4We consider the most privacy-catastrophic case, a perfect interpretation, which has the most accurate information in an
ATR.

Achieving Transparency Report Privacy in Linear Time

7

regions, i.e., the regions of input attributes partitioned by decision rules. Here the population pro-
portion among decision regions refers to the U.S. census data, and adversaries assumed blind to
population of applicants utilize the U.S. census data as side-information to estimate, for each de-
cision region, the percentage of the total number of male/female applicants (listed in the “Census
Statistics” attribute; for instance, the value 93.1% in Table 1 represents the following: given that the
decision region is {Annual Income < 100k; Gender=Female}, 93.1% of female applicants belong to
this region). Adversaries know public information of targeted applicants and also know decision
rules from an announced ATR.

When an ATR containing such decision rule is negligently announced, as it reveals strong depen-
dencies between annual income and decisions, any female using such a credit card in public in-
stantly tells anyone who has ever seen the report that her annual income is above 200k, which
not only results in a privacy hazard to her, but may also result in unexpected safety concerns. In
such a case, an adversary does not even require auxiliary information to be able to infer someone’s
secret.

Male credit card owners are also at risk, although not as much. For a male credit card owner, the
conﬁdence of an adversary believing that his income is above 200k is only around 36%, compared
with 100% in the case of a female owner, while based on census statistics, the conﬁdence of an
adversary believing that his income is above 200k is merely 3.5%. In other words, once such a
negligent algorithmic transparency report is announced to the public, a high-income (>200k) male
credit card owner’s risk of exposing annual income information is increased 10 fold.

In summary, releasing precise information of interpretable surrogate models (that can be equiv-
alently expressed by decision rules) can be harmful to data subjects’ privacy, as such information
gives adversaries a clear mapping between input records and received decision. With assistance
from public information and/or side-information, adversaries can abuse algorithmic transparency
to undermine people’s privacy. The same privacy leakage concern applies when precise infor-
mation of transparency scheme is released in the form of feature importance/interaction, which,
however, in the interest of space, is explicitly demonstrated in Appendix B, using a real dataset.

3.3 Privacy Leakage via Fairness Measures
Recall that one of the main motivation for algorithmic transparency is to understand if a decision-
making algorithm is fair and complies with regulations/law, e.g., the U.S. equal employment op-
portunity commission (EEOC) regulates the ratio of the hiring rates between women and men,
which should not be lower than 80% (80%-rule). In an algorithmic transparency report, such fair-
ness measures may be required upon data subjects’ demands (e.g., GDPR, Article 22).

To this end, consider again the credit card application in Table 1, in which the bank is under sus-
picion of discriminating against female applicants. Upon female applicants’ demands, a regulation
agency gets involved and discloses the following fairness measures for gender: (i) bias in statistical
parity (SP) (Deﬁnition 9) for male and female applicants, (ii) bias in conditional statistical parity
(CSP) (Deﬁnition 10) for male and female applicants who have the same level of income. An ATR
listing all the above fairness measures w.r.t. the credit card application is shown in Table 2 (see
Remark 1 for details), which can be announced to the public in an electronic form, e.g., through a
website (e.g., GDPR, Recital 58, information related to the public’s concerns).

Moreover, a data subject, which is a credit card applicant in our scenario, has the right to inquire
about the decision principle w.r.t. his or her personal data. Mary, a low-income (<100k) female who
would like to know why her applications are always denied, demands information regarding the
decision processing for her record. The response indicates that the approval rate for a low-income
female is 0. If we let 𝑑𝑖,𝑗 be the decision rule for people in {Y𝑖, W𝑗 } in Table 2, by utilizing the census

8

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 2. Fairness Measures for Table 1 in an ATR

Y1 = {F}, Y2 = {M}
W1 = {Annual Income ≤ 100k}
W2 = {100k ≤ Annual Income ≤ 200k}
W3 = {Annual Income ≥ 200k}
Overall approval rate for female (Y1) = 1.33%;
Overall approval rate for male (Y2) = 10%;
Bias in SP for Y1 and Y2 = 0.0866;
Bias in CSP for {Y1, W1} and {Y2, W1} = 0;
Bias in CSP for {Y1, W2} and {Y2, W2} = 0.5;
Bias in CSP for {Y1, W3} and {Y2, W3} = 0.

statistics as shown in Table 1, and based on the deﬁnitions of SP and CSP for binary decisions in
(30) and (31), respectively, the information provided in Table 2 tells us the following:

(1)

(2)

(3)

(4)

Overall approval rate for female(Y1) = 0.0133 ≈ 0.931𝑑1,1 + 0.057𝑑1,2 + 0.012𝑑1,3
Overall approval rate for male(Y2) = 0.1 ≈ 0.842𝑑2,1 + 0.123𝑑2,2 + 0.035𝑑2,3
Bias in SP for {Y1, W1} and {Y2, W1} = 0 = |𝑑1,1 − 𝑑2,1|
Bias in SP for {Y1, W2} and {Y2, W2} = 0.5 = |𝑑1,2 − 𝑑2,2|
Bias in SP for {Y1, W3} and {Y2, W3} = 0 = |𝑑1,3 − 𝑑2,3|.

(5)
Since Mary just got a reply indicating 𝑑1,1 = 0, from (3) and (5), Mary then knows that 𝑑1,1 =
𝑑2,1 = 0, 𝑑1,3 = 𝑑2,3, and from (4), either 𝑑1,2 = 𝑑2,2 + 0.5 or 𝑑1,2 = 𝑑2,2 − 0.5. She can ﬁrst assume
𝑑1,2 = 𝑑2,2 + 0.5, by plugging the values of 𝑑1,1 into (1) and 𝑑2,1 into (2), and replacing 𝑑2,2 and 𝑑2,3
by 𝑑1,2 − 0.5 and 𝑑1,3 in (1) and (2), respectively, she gets 0.057𝑑1,2 + 0.012𝑑1,3 = 0.0133 from (1)
and 0.123𝑑1,2 + 0.035𝑑1,3 = 0.1615 from (2). Since 𝑑𝑖,𝑗 are probabilities, ∀𝑖, 𝑗 , 𝑑1,2 and 𝑑1,3 can not be
grater than 1, and thus the obtained equation from (2) is infeasible, which implies the assumption
is wrong. She then knows 𝑑1,2 = 𝑑2,2 − 0.5. Repeat the same steps and she will obtain 𝑑1,2 = 0.0088
and 𝑑1,3 = 1.0692. By understanding that any 𝑑𝑖,𝑗 can not be greater than 1 and this is probably
caused by the mismatch between the census statistics and the true distribution, she would thus
update 𝑑1,3 = 1 and thus obtain 𝑑1,2 = 0.0013 ≈ 0; these estimates are very close to the true
the values. In addition, Mary can use the obtained 𝑑1,2 and 𝑑1,3 to further acquire 𝑑2,2 and 𝑑2,3.
Therefore, by utilizing the decision processing rule for her record and the publicly announced
fairness measures, she can obtain accurate decision rules for the credit card application. As in
Section 3.2, we know that a privacy disaster can happen when accurate decision rules are released
or hacked. The adversary Mary now can utilize her hacked decision rules to infer other applicants’
income.

From the above demonstrations, we have seen that a negligent ATR can result in a serious hazard
to data subjects’ privacy. In the following sections, we formalize the privacy leakage problem,
and propose the corresponding properties and solutions. We will revisit the examples demonstrated
above again in Section 8, with our proposed solutions applied.

Remark 1. Here we demonstrate how the numbers in Table 2 are calculated based on Table 1. The
deﬁnitions of SP and CSP for binary decisions can be found in (30) and (31), respectively.

Overall approval rate for female (Y1) = (2 × 1)/(139 + 9 + 2) = 1.33%;
Overall approval rate for male (Y2) = (18 × 0.5 + 5 × 1)/(117 + 18 + 5) = 10%;
Bias in SP for Y1 and Y2 = |1.33% − 10%| = 0.0866;
Bias in CSP for {Y1, W1} and {Y2, W1} = |0 − 0| = 0;

Achieving Transparency Report Privacy in Linear Time

9

Set of 
Input 
Attributes

…

…

Decision Blackbox

Decision
Outcomes

Fig. 2. A representative illustration of a decision blackbox

Bias in CSP for {Y1, W2} and {Y2, W2} = |0 − 0.5| = 0.5;
Bias in CSP for {Y1, W3} and {Y2, W3} = |1 − 1| = 0.

4 PROBLEM SETUP

In the following sections, we formalize and analyze the privacy leakage problem in ATR. To be-
gin with, in this section, we provide essential notations listed in Table 3 and useful deﬁnitions
for problem setup, followed by adversarial settings and deﬁnition of privacy violation in ATRs
formally.

4.1 Decision Mapping
Fig. 2 illustrates an opaque decision-making blackbox, which is essentially an unknown decision
mapping function deﬁned as follows.

Deﬁnition 1. (Decision Mapping [30]) Consider a decision process as illustrated in Fig. 2, where
𝑋 = {𝑋𝑘 | 𝑘 = 1, . . . , 𝐾 } is a set of input attributes, 𝐴 the output attribute (decision outcomes), and
A the range of 𝐴. Recall that Δ(𝑆) is a set of probability distributions over 𝑆. A decision mapping
𝐷 A : R𝑋 → Δ(A) is a function mapping from the range of input attributes to a set of probability
distributions over the range of decision outcomes. Formally,

𝐷 A (𝑋 ) = {𝑃𝐴 |𝑋 (𝐴 = 𝑎|𝑋 ) | ∀𝑎 ∈ A} = {𝐷𝑎 (𝑋 ) | ∀𝑎 ∈ A}.

Particularly, for binary decisions (0 =‘negative’ and 1 =‘positive’), we let

𝐷 A (𝑋 ) =

𝐷1 (𝑋 ) = 𝑑 (𝑋 )
𝐷0 (𝑋 ) = 1 − 𝑑 (𝑋 )

(

, for 𝑎 = 1
, for 𝑎 = 0,

(6)

(7)

where 𝑑 (𝑋 ) is decision rule [22] representing probabilities of mapping from input space to the positive
decision outcome.

Clearly, decision mapping is more comprehensive, while decision rule is more concise and con-

venient for an ATR, e.g., decision rule in Table 1.

As noted in Section 3.1, an ATR opens an opaque decision blackbox via transparency schemes
such as an interpretable surrogate model (a surrogate of 𝐷 A) or feature importance/interaction (a
function of 𝐷 A). In addition, an ATR may also contain fairness measures (functions of 𝐷 A, see
Appendix A). Clearly, an ATR is in general a function of decision mapping 𝐷 A (when there is
no confusion, we omit the subscript and simply write 𝐷 in the rest of the paper for conciseness);
while released, the mapping from decision inputs to outputs are made public, and thus it is very
crucial to ensure the reverse inference is not possible, or limited with low conﬁdence. To explicitly
characterize the reverse inference, we ﬁrst need to understand the capability of the adversaries.

is a legitimate participant in a communication protocol who will not deviate from the deﬁned

protocol but will attempt to learn all possible information from legitimately received messages.

10

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

4.2 Adversarial Settings
For the privacy leakage problem brought on by releasing ATRs, we consider honest-but-curious (or
curious-but-not-malicious) adversaries, i.e., adversaries who only perform legitimate actions and
will not deviate from the deﬁned protocol but would like to learn as much as possible (including
others’ secrets); in our ATR setting, this implies that an adversary will not hack into the system
and steal information but only acquires as much as possible information that is made public or
is widely-available. For example, the adversaries may know public information about his friends,
e.g., gender, race, ZIP code, age, etc; the adversaries may also have knowledge about the census
data [4] providing side-information (with week inference) between public and private attributes,
e.g., joint distributions between age, race, marriage status, household size, and income. Such kind
of adversaries are ubiquitous, making privacy leakage via released ATRs omnipresent.

It’s worth noting that the adversaries do not have access to all the input features and all the
output responses (decision outcomes), and thus are not able to extract any information about the
blackbox from the limited knowledge. Instead, the adversaries may just know the public informa-
tion and the received decisions of the targeted several individuals. The reason that we particularly
focus on honest-but-curious adversaries in this paper is that we would like to convey an important
message that candidly releasing an ATR could result in privacy hazards even for weak adversaries
who are not able to probe or hack into the system but possess some public information and/or widely-
available side-information. Moreover, more powerful adversaries who may have access to all input
features and output responses can train a more powerful surrogate model (as it need not be an
interpretable model) to mimic the original model, and thus can obtain more accurate information
w.r.t. the decision blackbox, as compared to what is provided in an announced ATR. In such a case,
the privacy hazard is not due to the ATR, as adversaries have already obtained something more
powerful (resulting in stronger inference), and thus such an adversarial setting is not meaningful
for ATR.

In practice, since honest-but-curious adversaries can be ubiquitous, the background knowledge
that adversaries may possess could be diverse and unknown to agencies in charge of ATRs. There-
fore, it is important that agencies should consider the worse-case scenario, i.e., the most information
that an honest-but-curious adversary can possess (which is the worst-case weak adversary). Hence,
the agencies should assume that an adversary could possess precise and full knowledge of

• the range R𝑋 and the joint distribution 𝑃𝑋 (x) of all inputs x;
• all public records (a.k.a. quasi-identiﬁer (QID) [5, 83, 84]) xU of speciﬁc individuals;
• the received decisions 𝑎 of the targeted individuals;
• the internal privacy parameters (e.g., the predeﬁned required privacy level) of the privacy

protection scheme M used for an ATR, if any.

The following information is assumed in general unknown (or known with little conﬁdence) by
adversaries before seeing an ATR: (i) data subjects’ private records xS and (ii) the decision mapping
𝐷 of the black-box. Given the above adversarial settings, we clearly deﬁne privacy violation in
releasing ATRs in the following.

Deﬁnition 2. The release of an ATR is privacy violating if any private or conﬁdential information
of any data subject to whom decision algorithms, disclosed in the ATR, have been applied can be
(unintentionally) inferred by any honest-but-curious unauthorized individual or entity to whom the
ATR is released, with conﬁdence exceeding a tolerable threshold, due to the release of the ATR.

Remark 2. Given Deﬁnition 2, inferring attribute values due to high correlations between at-
tributes, e.g., knowing people who have ovarian cancer are female, should not be mistaken as
privacy breach (not private information; not via an ATR). Similarly, releasing ATRs to a doctor for

Achieving Transparency Report Privacy in Linear Time

11

Table 3. Notation

Set of all public attributes
Set of all private attributes
Random variable (r.v.) of attribute 𝑘
= {𝑋𝑘 | ∀𝑘 ∈ U}; collection of r.v.’s of all public attributes
= {𝑋𝑘 | ∀𝑘 ∈ S}; collection of r.v.’s of all private attributes
= (𝑋 U, 𝑋 S); collection of r.v.’s of all attributes
Range of 𝑋 ; the universe of inputs; R𝑋 = R𝑋U × R𝑋S
An instance of 𝑋 U
An instance of 𝑋 S
= (xU, xS), an instance of 𝑋
= {x′ ∈ R𝑋 | x′
The r.v. of decision outcome
Range of 𝐴
Aleatory probability; chance

U = xU } = range of (xU, 𝑋 S)

Epistemic probability; credence or belief
= {𝑃 (𝐴 = 𝑎|𝑋 ) | ∀𝑎 ∈ A}; decision mapping (Deﬁnition 1)
= { ˜𝑃 (𝐴 = 𝑎|𝑋 ) | ∀𝑎 ∈ A}; announced decision mapping
= 𝑃 (𝐴 = 1|𝑋 ); decision rules (Deﬁnition 1)
= ˜𝑃 (𝐴 = 1|𝑋 ); announced decision rules
A privacy protection scheme for an ATR

U
S
𝑋𝑘
𝑋 U
𝑋 S
𝑋
R𝑋
xU
xS
x
𝑇xU
𝐴
A
𝑃 (·)
˜𝑃 (·)
𝐷 (𝑋 )
˜𝐷 (𝑋 )
𝑑 (𝑋 )
˜𝑑 (𝑋 )
M

the ML-assist diagnoses of his patients should not be considered as privacy violation (an authorized
personal).

4.3 Comparison with PPDM and PPDP
The main diﬀerences between privacy preservation in ATRs and privacy preservation in data-
mining (PPDM) and data-publishing (PPDP) are their adversarial settings.

More speciﬁcally, in the PPDM setting, a dataset is not published; instead, users or data analysts
send queries (a set of pre-deﬁned/allowed deterministic functions, e.g., average, count, median,
max, and min) to the curator, and the curator generates the corresponding query outputs based
on the dataset. In such a setting, if the pre-deﬁned queries are carefully designed, an adversary (a
malicious user), in general, is not able to determine the direct mappings between public and private
attribute values of a record nor any private information of any individual from any single query out-
put. However, since query functions are known in advance, an adversary can send multiple queries
and compare the obtained results to extract data subjects’ private information from the outputs. In
this regard, diﬀerential privacy (DP) [21, 31] is usually adopted to preserve privacy in PPDM. In
summary, the main diﬀerences between the settings in PPDM and ATRs are (i) in PPDM, mappings
between public and private attributes are in general not available, or may be known only partially,
while these could be known statistically in the ATRs setting; (ii) an adversary can send multiple
(deterministic) queries and/or collude with other adversaries to extract data subjects’ private infor-
mation, while an ATR is a one-shot announcement, and the announced decision mappings from
the inputs to the outputs could be probabilistic (i.e., random decisions).

In the PPDP setting, a dataset is published. Therefore, the mappings between public and private
attributes are clearly known to an adversary (much stronger than auxiliary or side-information).
When the published dataset shows uniqueness of a public record or unique relationship between

12

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

certain public and private attributes, an adversary can utilize such uniqueness to identify data sub-
jects’ private information. Therefore, several techniques (𝑘-anonymity [75, 76], 𝑙-diversity [61, 62],
etc.) are proposed in the literature to obfuscate such uniqueness in order to preserve data subjects’
privacy. In summary, the main diﬀerences between the settings in PPDP and ATRs are (i) unlike
in PPDP where privacy is leaked due to strong inference between attributes, in ATR, as we have
emphasized in Section 4.2, we only consider the case of weak correlation/inference between public
and private attributes, i.e., an adversary is not able to identify any private information with high
conﬁdence before seeing an ATR, while the conﬁdence could be dramatically enhanced after an
ATR is released; (ii) in PPDP, depending on the application, there may or may not exist output
attributes. When there exist output attributes, as the dataset is published, all output attribute val-
ues are available to an adversary, which could provide strong inference between some sensitive
attributes and the output attributes (for learning purposes), and thus we need to guarantee that
any output attribute value is not directly associated with any individual; while in ATRs, we con-
sider the case that a decision outcome could be directly associated with an individual (credit card
applications, university admissions, etc.), but an adversary knows a few decision outcomes only.

5 PRIVACY, UTILITY, AND MEASURED FAIRNESS

Given clear context of adversarial settings and deﬁnition of privacy violation for releasing ATRs,
we next formulate privacy leakage caused by inference attacks, and propose a privacy-preserving
mechanism for ATRs. To this end, in this section, we provide a privacy measure to mathematically
characterize and formulate the degree of privacy leakage. Based on the proposed privacy measure,
we formulate the requirements for a privacy-preserving mechanism for ATRs, and introduce a util-
ity measure to characterize the inﬂuences caused by the proposed privacy-preserving mechanism;
similarly, we address the inﬂuence of the proposed privacy mechanism on fairness measures.

5.1 Privacy Measure and Privacy-Preserving Mechanism
Recall in Section 3 we have seen privacy leakage disasters when decision rules were divulged. The
fundamental problem is that transparency schemes as well as fairness measures are closely related
to, or functions of, decision mapping 𝐷, and more importantly, if 𝐷 provides strong inference from
public knowledge to sensitive records, once it is utilized in an ATR and obtained by an adversary,
the adversary can utilize it to further acquire data subjects’ secrets with high conﬁdence.

In light of this, here we propose the following: a carefully processed 𝐷, denoted by ˜𝐷, should
be adopted as a substitute for 𝐷 in an ATR for preserving data subjects’ privacy. ˜𝐷 should satisfy
certain privacy requirements and can be safely announced (if an ATR chooses to release an inter-
pretable surrogate model) or utilized by transparency schemes and fairness measures provided in
an ATR.

In such a case, when an ATR is released, an adversary acquires information about ˜𝐷, and could

𝑃𝑋 , ˜𝐷
−−−−→ 𝑋 Si which maps from inference source
further utilize it in an inference channel h𝑋 U, 𝐴
𝑋 U and 𝐴 to sensitive attribute values 𝑋 S. (When the context is clear, we will omit 𝑃𝑋 and ˜𝐷
above the arrow for simplicity.) Based on the adversarial settings in Section 4.2, one reasonable
privacy measure to characterize the above inference (caused by ˜𝐷) is the maximum conﬁdence
of an adversary in inferring any data subject’s sensitive value 𝑋 S, which is also known as the
worst-case posterior vulnerability [34]. In other words, even if an adversary knows ˜𝐷 and further
utilizes it to perform inference attacks, the maximal conﬁdence that the adversary can have is
carefully controlled in advance to prevent privacy violation. In this regard, privacy measures of
the announced version of decision mapping ˜𝐷 used in an ATR should reﬂect the maximal degree of

Achieving Transparency Report Privacy in Linear Time

13

an adversary’s conﬁdence in inferring any data subject’s secret via ˜𝐷. Consider the case in which
S is a singleton set, we deﬁne maximum conﬁdence formally in the following.

Deﬁnition 3. (Maximum Conﬁdence) Given the adversarial settings in Section 4.2 and an inference
channel h𝑋 U, 𝐴 → 𝑋 Si, the conﬁdence of inferring a certain sensitive attribute value 𝑥 S from a
certain inference source (xU, 𝑎), denoted by conf (xU, 𝑎 → 𝑥 S), is the posterior epistemic probability
of 𝑥 S given xU and 𝑎 as follow

conf (xU, 𝑎 → 𝑥 S) = ˜𝑃𝑋S |𝑋U,𝐴 (𝑥 S |xU, 𝑎).
The maximum conﬁdence of inferring a speciﬁc sensitive attribute value 𝑥 S from any inference sources,
denoted by Conf (𝑋 U, 𝐴 → 𝑥 S), is deﬁned as

Conf (𝑋 U, 𝐴 → 𝑥 S) , max
xU,𝑎
Accordingly, the maximum conﬁdence of inferring any sensitive attribute value from any inference
channel is

{conf (xU, 𝑎 → 𝑥 S)}.

Conf (𝑋 U, 𝐴 → 𝑋 S) , max
xU,𝑎,𝑥S

{conf (xU, 𝑎 → 𝑥 S)}.

The privacy requirement, similar to conﬁdence bounding [88, 89], 𝛽-likeness [19], and privacy
enforcement in [58], restricts the maximum conﬁdence on inferring any sensitive attribute by a
conﬁdence threshold, a pre-determined privacy parameter 𝛽.
Deﬁnition 4. (𝛽-Maximum Conﬁdence) In an algorithmic transparency report, ˜𝐷 satisﬁes the pri-
vacy requirement 𝛽-Maximum Conﬁdence if Conf (𝑋 U, 𝐴 → 𝑋 S) ≤ 𝛽.

Lemma 1. The privacy requirement 𝛽-Maximum Conﬁdence imposes the following constraints to
the announced decision mapping ˜𝐷, ∀x ∈ R𝑋 , ∀𝑎 ∈ A,
˜𝐷𝑎 (x)𝑃𝑋 (x)

Proof. Please refer to Appendix D for detailed proof.

˜𝐷𝑎 (x′)𝑃𝑋 (x′)

≤ 𝛽.

x′ ∈𝑇xU
Í

(8)

(cid:3)

Remark 3. Note that a privacy requirement which only prevents an adversary from correctly infer-
ring any sensitive attribute value of any data subject is insuﬃcient. The reason is that an adversary
can possess the knowledge of the privacy protection scheme and its internal privacy parameters. If
the privacy requirement allows an adversary to incorrectly infer wrong sensitive values with arbi-
trary high conﬁdence, since an adversary may know the privacy requirement, he/she may perceive
that any sensitive attribute value which can be inferred with conﬁdence higher than the threshold is
an incorrect one; this could become additional side-information for the adversary. An adversary
can further utilize such extra side-information to narrow down the range of conjectures, which
enhances the conﬁdence of correctly guessing the right sensitive value. The enhanced conﬁdence
could result in exceeding the privacy threshold, and thus cause a privacy hazard.

The advantage of using maximum conﬁdence as a privacy measure is that it results in intuitive
understanding of 𝛽. This could be important when a privacy scheme is used for an ATR, the regula-
tion may require a plain explanation for the adopted privacy scheme as well as the corresponding
settings and meanings of its parameters. Alternatively, one can use other privacy measures, e.g.,
minimum uncertainty (Appendix C), which is essentially conveying the same concept as maximum
conﬁdence, but the privacy parameter 𝛾 grows with the strength of privacy.

A privacy protection scheme M takes the original/true decision mapping 𝐷 as the input and
generates a privacy-preserving decision mapping ˜𝐷 safe for announcement with careful processing

14

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

based on privacy requirements. Inevitably, the original 𝐷 would diﬀer from the generated ˜𝐷, which
is a distorted/perturbed but private version of 𝐷. In the next section, we introduce a utility measure
to characterize the distortion.

5.2 Utility Measure: Fidelity
In this section, we introduce an appropriate utility measure for our problem. Given proposed
M
−−→ ˜𝐷, an appropriate utility measure should characterize the distortion from ˜𝐷 to 𝐷, or quan-
𝐷
tify the quality of faithfulness of ˜𝐷 (compared with 𝐷), and hence, particularly, is named ﬁdelity
measure hereafter. By imposing ﬁdelity constraints to M, the maximal distortion between ˜𝐷 and
𝐷 is guaranteed to be bounded accordingly.

Deﬁnition 5. (𝛿-Fidelity) A privacy perturbation method M : Δ(A) → Δ(A) satisﬁes 𝛿-ﬁdelity,
𝛿 ∈ [0, 1], if ∀x ∈ R𝑋 and ∀𝑎 ∈ A, we have

| ˜𝐷𝑎 (x) − 𝐷𝑎 (x)| ≤ 1 − 𝛿.
Deﬁnition 6. (𝛼-Fidelity) A privacy perturbation method M : Δ(A) → Δ(A) satisﬁes 𝛼-ﬁdelity,
𝛼 ∈ [0, 1], if ∀x ∈ R𝑋 and ∀𝑎 ∈ A, we have

(9)

𝛼 ≤ ˜𝐷𝑎 (x)/𝐷𝑎 (x) ≤ 1/𝛼.

(10)

In the most general form, deﬁnition of ﬁdelity can be

˜𝐷𝑎 (x)min ≤ ˜𝐷𝑎 (x) ≤ ˜𝐷𝑎 (x)max,
which describes the restriction (the allowed range) of distortion of ˜𝐷 in a very general manner.
The corresponding equivalent representations for 𝛿- and 𝛼-ﬁdelity are
𝐷𝑎 (x) − (1 − 𝛿) ≤ ˜𝐷𝑎 (x) ≤ 𝐷𝑎 (x) + (1 − 𝛿),

(11)

(12)

𝛼𝐷𝑎 (x) ≤ ˜𝐷𝑎 (x) ≤

1
𝛼

𝐷𝑎 (x),

(13)

in which the upper and the lower bounds ˜𝐷𝑎 (x)max and ˜𝐷𝑎 (x)min are functions of 𝐷 and 𝛿, or 𝛼.
In practice, 𝛿 and 𝛼 should not be far from 1.

5.3 Influence of Privacy on Measured Fairness
As demonstrated in Section 3.3, since fairness measures are functions of decision mapping/rule,
releasing precise fairness measures could bring privacy hazards. On account of this, the released
fairness measures should be computed based on privacy-preserving ˜𝐷, which, however, would in-
ﬂuence and distort the measured bias 𝜀. In this section, we show that by knowing the ﬁdelity con-
straints to M, we are able to characterize and bound the distortion of the measured fairness/bias.
Fig. 3 is a representative illustration of the true bias 𝜀 F and the measured bias ˜𝜀 F inﬂuenced by
M, where F denotes a (general or speciﬁc) set of fairness deﬁnitions on which bias is computed.
Since the true decision mapping 𝐷 should not be released and utilized to compute fairness mea-
sures, the true bias computed based on 𝐷 is generally unknown. A natural question may arise: by
knowing ˜𝜀 F, and the degree of ﬁdelity of ˜𝐷 (𝛿 or 𝛼), what can we know about 𝜀 F? The following
lemma answers the question: if the maximum distortion from ˜𝐷 to 𝐷 is known, the maximum
distortion from ˜𝜀 F to 𝜀 F can be known, and thus the range of 𝜀 F can be known.

Lemma 2. Let Ftv denote the set of all total-variation-based fairness deﬁnitions, and Frm the set of all
M
−−→ ˜𝐷, if M satisﬁes 𝛿-ﬁdelity,
relative-metric-based fairness deﬁnitions (see Appendix A). Given 𝐷

Achieving Transparency Report Privacy in Linear Time

15

Privacy Parameter:

Fidelity Parameter:

or

or

-fidelity/
-fidelity

(cid:34)

(cid:88)(cid:81)(cid:78)(cid:81)(cid:82)(cid:90)(cid:81)

Fig. 3. A depiction of how fidelity of ˜𝐷 can aid in characterizing the diﬀerence between the measured bias
˜𝜀 F and the true bias 𝜀 F, where F denotes a (general or specific) set of fairness definitions on which bias is
computed. In Section 3, we have seen that releasing 𝐷 and 𝜀 F can cause privacy leakage and thus should be
prohibited, so that 𝜀 F is unknown to the public. However, if the fidelity parameter used in M is known, we
are able to characterize 𝜀 F by ˜𝜀 F based on Lemma 2.

we can guarantee the measured bias ˜𝜀 Ftv satisﬁes

| ˜𝜀 Ftv − 𝜀 Ftv | ≤ min{2(1 − 𝛿), 1}.

If M satisﬁes 𝛼-ﬁdelity, we can guarantee the measured bias ˜𝜀 Frm satisﬁes

| ˜𝜀 Frm − 𝜀 Frm | ≤ min{−2 log 𝛼, 1}.

Proof. By applying the reverse triangle inequality [85], the results trivially follow.

(14)

(15)

(cid:3)

6 PRIVACY-FIDELITY TRADE-OFF
Strong privacy perturbation could cause serious distortion on the announced information includ-
ing decision rules and fairness measures, and thus a privacy protection scheme should preserve
privacy while guaranteeing a certain degree of ﬁdelity to the announced information; this turns
out a privacy-ﬁdelity trade-oﬀ problem. In this section, we mathematically formulate the trade-oﬀ
problem, and revisit existing algorithms that can eﬃciently solve this problem.

6.1 Optimization Formulation
We describe the privacy-ﬁdelity trade-oﬀ problem in the following: given ﬁdelity constraints, we
would like to ﬁnd the greatest privacy (the smallest 𝛽) that we can achieve. The problem is mathe-
matically formulated as follow. For conciseness, we omit the subscript of all probability measures
and simply write, e.g., 𝑃 (x) instead of 𝑃𝑋 (x).

OPT(R𝑋 ×A) :

min
˜𝐷

𝛽

s.t.

𝑃 (x) ˜𝐷𝑎 (x)

𝑃 (x′) ˜𝐷𝑎 (x′)

x′ ∈𝑇xU
Í

≤ 𝛽 , ∀x ∈ R𝑋 , ∀𝑎 ∈ A

˜𝐷𝑎 (x) ≤ ˜𝐷𝑎 (x)max , ∀x ∈ R𝑋 , ∀𝑎 ∈ A
˜𝐷𝑎 (x) ≥ ˜𝐷𝑎 (x)min , ∀x ∈ R𝑋 , ∀𝑎 ∈ A
˜𝐷𝑎 (x) ≥ 0 , ∀x ∈ R𝑋 , ∀𝑎 ∈ A
˜𝐷𝑎 (x) = 1 , ∀x ∈ R𝑋 .

Õ𝑎 ∈A

(OPT)

(16a)

(16b)

(16c)

(16d)

(16e)

(16f)

16

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

The ﬁrst constraint in (16b) is the privacy constraint 𝛽-Maximum Conﬁdence deﬁned in Deﬁnition
4 and Lemma 1, and the last two constraints in (16e) and (16f) are probability distribution condi-
tions. The second and the third constraints in (16c) and (16d) are ﬁdelity constraints introduced
in (11). Its corresponding representations for 𝛿- or 𝛼-ﬁdelity can be found in (12) and (13), respec-
tively. The objective in (16a) is to ﬁnd the minimal 𝛽 subject to the feasibility of ˜𝐷 based on the
above-mentioned constraints. A careful observation of the optimization problem (OPT) will reveal
that it is an equivalent formulation of a generalized linear fractional programming (LFP) problem
[94].

6.2 Drawbacks of Existing Methods to Solve Generalized LFP Problems
It has been known that a generalized LFP is quasi-convex and not reducible to a linear program-
ming (LP) problem; however, it can be solved as a sequence of LP feasibility problems [16], i.e.,
solving numerous sub-level LP problems iteratively according to bisection method. By eﬃcient
algorithms such as interior point method, the solution of a LP problem can be obtained in pseudo-
polynomial time 𝑂 ( 𝑛3
log 𝑛 𝐿) [9], where 𝑛 is the number of variables, 𝐿 is the input size, i.e., the length
of the binary coding of the input data to represent the problem, which is roughly proportional to
the number of constraints. For our problem, based on (16b)–(16f), it is clear that the number of
constraints is proportional to |R𝑋 ×A|, which is the number of variables 𝑛 = | ˜𝐷𝑎 (x)|, exponential
in the number of input attributes 𝐾. For example, suppose the cardinality for each input attribute
is consistent, e.g., |𝑋𝑘 | = 𝑙, ∀𝑘 = 1, . . . , 𝐾, we have 𝑛 = |R𝑋 ×A| = 2 · 𝑙 𝐾 and roughly 4𝑛 = 8 · 𝑙 𝐾
constraints. Even for a relatively small example, e.g., a binary decision process with 𝐾 = 10 input
attributes, each with 𝑙 = 5 possible values, we have 𝑛 ≈ 2 · 107 variables and ≈ 8 · 107 constraints
for each sub-level LP problem, with time complexity 𝑂 ( 𝑛4
log 𝑛 ), i.e., not tractable on typical ma-
chines (e.g., as reported in [64], ‘spal_004’ with 10203 rows (w.r.t. constrains) and 321696 columns
(w.r.t. variables) can encounter out of memory or timeout (> 25, 000 seconds) issue on a Linux-
PC with a 4GHz i7-4790K CPU and 32GB RAM). To solve a generalized LFP problem, we need to
solve such a huge sub-level LP problem iteratively. In practice, ML algorithms may require non-
trivial amounts of attributes to aid in decision-making; therefore, without an eﬃcient solver, the
privacy-ﬁdelity trade-oﬀ problem could be intractable and the feasibility of the associated privacy
protection scheme could be dramatically reduced. In this regard, it is crucial and essential to propose
a more eﬃcient method to solve the proposed privacy-ﬁdelity trade-oﬀ optimization problem.

7 LINEAR-TIME OPTIMAL PRIVACY SOLUTION
In this section, we analyze the optimization problem (OPT), reveal its important properties, and
propose eﬃcient methods to solve it. We ﬁrst investigate the decomposability of the optimization
problem, i.e, whether the problem can be decomposed into several smaller sub-problems for eﬃ-
cient solving. We ﬁnd (OPT) decomposable and can be solved using divide-and-conquer approach.
In addition, we propose closed-form solutions for each optimization sub-problem. The optimization
problem can thus be solved very eﬃciently by solving multiple independent sub-problems in linear
time. Moreover, analysis insights into the optimal solutions are also provided in this section.

7.1 Decomposability

In the following, we show that the optimization problem can actually be decomposed into nu-
merous small sub-problems and thus can be solved more eﬃciently. An optimization problem is
separable or trivially parallelizable if the variables can be partitioned into disjoint subvectors and
each constraint involves only variables from one of the subvectors [17]. By observing (i) each con-
straint in (16c), (16d), and (16e) involves only a single variable ˜𝐷𝑎 (x), (ii) each constraint in (16f)

Achieving Transparency Report Privacy in Linear Time

17

involves a set of variables { ˜𝐷𝑎 (x) | ∀𝑎 ∈ A}, and (iii) each constraint in (16b) involves a set of
variables { ˜𝐷𝑎 (x) | ∀x ∈ 𝑇xU }, we notice that any variable ˜𝐷𝑎 (x) is a complicating variable in
𝑇xU ×A but is irrelevant to any other variables outside the QID group 𝑇xU . Hence, (16b)–(16f) are
complicating constraints within a tuple but separable constraints among tuples. (OPT) can thus be
decomposed into multiple smaller sub-problems; each focuses on a particular QID group only. Let
ℎ( ˜𝐷𝑎 (x), 𝛽) ≥ 0 be the aﬃne function representing all linear inequality constraints (16b)–(16e).
An optimization sub-problem can thus be expressed as follow.

OPT-SUB(𝑇xU ×A) :

𝛽

min
˜𝐷
s.t. ℎ( ˜𝐷𝑎 (x), 𝛽) ≥ 0 , ∀x ∈ 𝑇xU , ∀𝑎 ∈ A

˜𝐷𝑎 (x) = 1 , ∀x ∈ 𝑇xU .

(OPT) is then equivalent to the master problem below.

Õ𝑎 ∈A

OPT-MASTER(R𝑋 ×A) :

min
˜𝐷
s.t.

𝛽

(INEQ-Sub(𝑇xU ×A)) , ∀𝑇xU ⊆ R𝑋
(EQ-Sub(𝑇xU ×A)) , ∀𝑇xU ⊆ R𝑋 .

(OPT-Sub)

(OBJ-Sub)

(INEQ-Sub)

(EQ-Sub)

(OPT-MS)

(OBJ-MS)

(INEQ-MS)

(EQ-MS)

Lemma 3. Let 𝛽∗
(OPT). We have 𝛽∗ = max
𝑇xU ⊆ R𝑋

𝑇xU

𝛽∗
𝑇xU

.

denote the optimal value of a sub-problem (OPT-Sub), 𝛽∗ the optimal value of

Proof. Since (OPT) is a generalized LFP (in an equivalent formulation), according to OPT-MS,
(cid:3)

the result trivially follows.

The Lemma above basically tells us that given the same ﬁdelity constraints, the overall highest
among all sub-problems, i.e., the weakest optimal privacy

privacy guarantee 𝛽∗ is the largest 𝛽∗
guarantee among all QID groups.

𝑇xU

7.2 Solution Properties

According to the decomposability of the optimization problem, in the following, we only need
to focus on solving an optimization sub-problem (OPT-Sub). To characterize the privacy-ﬁdelity
trade-oﬀ, we are particularly interested in where the trade-oﬀ starts and ends. In this section, we
propose lemmas addressing the above question.

Before introducing the lemmas, we ﬁrst deﬁne a useful quantity which will be further utilized

to characterize the trade-oﬀ.

Deﬁnition 7. (Maximum Posterior Conﬁdence) Given an optimization sub-problem (OPT-Sub) and
1-ﬁdelity (100% faithfulness) requirement, i.e., 𝛼 = 𝛿 = 1 and ˜𝐷 = 𝐷, the highest conﬁdence that an
adversary can have on inferring any sensitive information from any decision outcome, denoted by 𝐶∗,
is 𝐶∗ , Conf (𝑋 U = xU, 𝐴

{conf (xU, 𝑎 → 𝑥 S)}.

𝑃𝑋 ,𝐷
−−−−→ 𝑋 S) = max
𝑎,𝑥S

Lemma 4. An (OPT-Sub) has the 1-ﬁdelity solution ˜𝐷𝑎 (x) = 𝐷𝑎 (x), ∀x ∈ 𝑇xU , ∀𝑎 ∈ A, iﬀ 𝛽 ≥ 𝐶∗.
Proof. Please refer to Appendix E for detailed proof. We provide intuitive explanation as proof
sketch here. Since the highest conﬁdence that an adversary can have (𝐶∗) is lower than the privacy

18

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

requirement (the conﬁdence threshold 𝛽), it is safe to release 𝐷 directly, i.e., ˜𝐷 = 𝐷 with perfect
ﬁdelity. On the other hand, as long as 𝐶∗ is greater than 𝛽, releasing 𝐷 violates privacy requirement
(cid:3)
and cannot be a feasible solution.

Lemma Insight - Lemma 4 tells us when 𝛽 ≥ 𝐶∗, there is no trade-oﬀ between privacy and
ﬁdelity: as long as 𝛽 is greater than 𝐶∗, increasing the strength of privacy (decreasing 𝛽) would
not cause degradation in ﬁdelity. In other words, alone the strength of privacy (from low to high),
the trade-oﬀ between privacy and ﬁdelity starts when 𝛽 is right below 𝐶∗. The next lemma will
tell us the end of this trade-oﬀ region.

Lemma 5. For 𝛼 = 𝛿 = 0, i.e., ﬁdelity constraints are trivialized or not presented, an (OPT-Sub)
has feasible solutions if and only if (iﬀ) 𝛽 ≥ 𝛽min , maxx∈𝑇xU
𝑃 (x|𝑇xU ). In other words, there exists
privacy limit, the strongest privacy that we can have, based on the adversarial settings in Section 4.2.

Proof. Please refer to Appendix F for detailed proof. We provide intuitive behind this lemma
𝑃 (x|𝑇xU ) is the greatest conditional probability
as proof sketch here. The privacy limit maxx∈𝑇xU
over the tuple5, which is actually the highest possible inference conﬁdence of an adversary before
releasing an ATR. It is the baseline conﬁdence, which merely utilizes knowledge of public record xU
𝑃𝑋−−→ 𝑥 Si. Since an ATR does not contribute
and side-information 𝑃 (x) in an inference channel hxU
to such an inference channel, an associated privacy protection scheme is not able to help further
reduce this baseline conﬁdence. While achieving such a privacy limit, an ATR basically reveals
(cid:3)
zero useful information to the public.

Lemma Insight - Lemma 4 and 5 tell us the start and the end of the privacy-ﬁdelity trade-oﬀ

region along 𝛽. Next, we show that the end point can never happen before the starting point.

Lemma 6. 𝐶∗ ≥ 𝛽min.

Proof. Please refer to Appendix G for detailed proof. We ﬁrst provide the intuition of the lemma
as a proof sketch. The intuition here is very straightforward: the maximum posterior conﬁdence
can never be lower than the maximum prior conﬁdence (prior vulnerability cannot exceed posterior
(cid:3)
vulnerability in [66]). Equality holds when the revealed information is completely useless.

Lemma Insight - According to Lemma 4, when 𝛽 ∈ [𝐶∗, 1], the true decision mapping 𝐷 can
be safely released without perturbation (1-ﬁdelity). Lemma 5 tells us when ﬁdelity constraints are
not imposed (0-ﬁdelity), the feasible privacy region is 𝛽 ∈ [𝛽min, 1]. Moreover, based on Lemma
6, the region [𝛽min, 𝐶∗] is always non-empty. Clearly, this is the region where we trade oﬀ ﬁdelity
for privacy. Next, we propose our main theorem to characterize the trade-oﬀ in this region.

7.3 Optimal Privacy and Solutions
In the following, we propose our main theorem, which provides the optimal-privacy solutions to
the optimization sub-problem (OPT-Sub) in a closed-form expression, in terms of ﬁdelity. Given
ﬁdelity constraints, the proposed closed-form solution yields the optimal privacy value, and thus
can be utilized to analytically characterize privacy-ﬁdelity trade-oﬀ.

5Since ∀x ∈ 𝑇xU , xU is the same, 𝑃 (x |𝑇xU ) is also 𝑃 (𝑥S |𝑇xU ), the conditional distribution over all sensitive records.

Achieving Transparency Report Privacy in Linear Time

19

Theorem 1. Consider an optimization sub-problem (OPT-Sub) for a QID group, in which we seek
for the strongest privacy guarantee given ﬁdelity constraints. For a decision outcome 𝑎, deﬁne

x𝑎 , arg max

𝑃 (x) ˜𝐷𝑎 (x)min,

𝑃 (x′),

x′ ∈𝑇xU

x∈𝑇xU
𝑏 (x) = 𝑃 (x) − 𝛽
˜𝐷𝑎 (x)max′ , 1
˜𝐷𝑎 (x)min′ , 1

Í

𝑃 (x) min{𝑃 (x) ˜𝐷𝑎 (x)max, 𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min},
𝑃 (x) max{𝑃 (x) ˜𝐷𝑎 (x)min, 𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min + 𝑏 (x)}.

For binary decisions, i.e., 𝑎 ∈ A = {0, 1}, the optimal privacy 𝛽∗

𝑇xU

= max{𝛽0, 𝛽1, 𝛽𝑝 }, where

𝛽0 =

𝛽1 =

𝛽𝑝 =

𝑃 (x0) ˜𝐷0 (x0)min
x≠x0,x∈𝑇xU
𝑃 (x1) ˜𝐷1 (x1)min
x≠x1,x∈𝑇xU

Í

𝑃 (x0) ˜𝐷0 (x0)min +

𝑃 (x) ˜𝐷0 (x)max′

𝑃 (x1) ˜𝐷1 (x1)min +
𝑃 (x1) ˜𝐷1 (x1)min + 𝑃 (x0) ˜𝐷0 (x0)min
x∈𝑇xU

Í
𝑃 (x)

,

𝑃 (x) ˜𝐷1 (x)max′

,

,

and the corresponding optimal privacy solutions are
When 𝛽∗

= 𝛽0 :

𝑇xU

When 𝛽∗

𝑇xU

When 𝛽∗

𝑇xU

= 𝛽1 :

= 𝛽𝑝 :

Í
˜𝐷0 (x) = ˜𝐷0 (x)max′, ∀x ∈ 𝑇xU
˜𝐷1 (x) = ˜𝐷1 (x)max′, ∀x ∈ 𝑇xU
˜𝐷𝑎 (x𝑎) = ˜𝐷𝑎 (x𝑎)min, ∀𝑎 ∈ A
𝑃 (x) ˜𝐷𝑎 (x) = 1
𝛽𝑝

x∈𝑇xU

𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min, ∀𝑎 ∈ A

When 𝛽∗

𝑇xU

= 𝛽𝑝 and |𝑇xU | > 3, we have multiple solutions.

Proof. We refer readers to Appendix H for the detailed proof.

(cid:3)

˜𝐷𝑎 (x)min′ ≤ ˜𝐷𝑎 (x) ≤ ˜𝐷𝑎 (x)max′, ∀x ∈ 𝑇xU , ∀𝑎 ∈ A.
Í

Practical Implications of Theorem - Based on Theorem 1, the optimal privacy guarantee 𝛽∗

for
each QID group can be computed analytically (in closed form), and based on Lemma 3, the overall
strongest privacy guarantee is the largest 𝛽∗
among all QID groups. This is particularly useful
and practical in releasing ATRs - given any value of tolerable distortion, we can now easily obtain
the optimal privacy value without the need of solving an optimization problem, which can then
be applied to aid in determining the desired trade-oﬀ between privacy and ﬁdelity.

𝑇xU

𝑇xU

Linear Time Justiﬁcation of Algorithm 1 - The net time to achieve a solution to (OPT) is a function
of the number of sub-problems - each of which is solved via Algorithm 1 in linear-time in the
number of records 𝑛. Given an optimization sub-problem, the number of records x ∈ 𝑇xU , i.e.,
|𝑇xU | , 𝑛 is equal to the number of sensitive attribute values, as all records in a QID group 𝑇xU
have the same public record xU. To see that Algorithm 1 is in 𝑂 (𝑛), it is ﬁrst worth noting that,
based on Theorem 1, the time complexity of computing x𝑎 and 𝑏 (x) are in 𝑂 (𝑛); consequently,
the time complexity of computing ˜𝐷𝑎 (x)max′ and ˜𝐷𝑎 (x)min′ are in 𝑂 (1). Given these complexities,
it is clear that 1, lines 1 to 4 in Algorithm 1 is in 𝑂 (𝑛); all lines from line 5 to line 15, except
line 13, are in 𝑂 (1); function Allocation called in line 13 is in 𝑂 (𝑛) - since lines 18 to 19, as
well as line 20, are in 𝑂 (𝑛), line 21 and 22 are in 𝑂 (1), and lines 23 to 27 is in 𝑂 (𝑛)). Therefore,

20

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Algorithm 1 Optimal Privacy Protection Scheme
Input: 𝑃 (x), 𝑇xU , ˜𝐷𝑎 (x)min, ˜𝐷𝑎 (x)max
Output: ˜𝐷𝑎 (x), ∀𝑎, ∀x ∈ 𝑇xU
1: for 𝑎 ∈ {0, 1} do
ﬁnd x𝑎
2:
for all x ∈ 𝑇xU do

3:

← max{𝛽0, 𝛽1, 𝛽𝑝 }

𝑇xU

compute ˜𝐷𝑎 (x)max′
4:
5: compute 𝛽0, 𝛽1, 𝛽𝑝, and 𝛽∗
6: if 𝛽∗
= 𝛽0 then
𝑇xU
˜𝐷0(x) ← ˜𝐷0 (x)max′
˜𝐷1(x) ← 1 − ˜𝐷0(x)max′

7:

8:
9: else if 𝛽∗

𝑇xU

= 𝛽1 then
˜𝐷1(x) ← ˜𝐷1 (x)max′
˜𝐷0(x) ← 1 − ˜𝐷1(x)max′

10:

11:
12: else if 𝛽∗

= 𝛽𝑝 then

𝑇xU

13:

˜𝐷1(x) ← Allocation()
˜𝐷0(x) ← 1 − ˜𝐷1(x)
14:
15: return ˜𝐷𝑎 (x), ∀𝑎, ∀x ∈ 𝑇xU
16:
17: function Allocation( )
18:

x≠x1,x0 𝑃 (x) ˜𝐷1 (x)min′

for all x ∈ 𝑇xU , x ≠ x1, x0 do
compute ˜𝐷1 (x)min′
resid ← RHS of (22) −
˜𝐷1(x1) ← ˜𝐷1(x1)min
˜𝐷1(x0) ← 1 − ˜𝐷0 (x0)min
for all x ∈ 𝑇xU , x ≠ x1, x0 do

Í

capacity ← ˜𝐷1 (x)max′ − ˜𝐷1 (x)min′
allocation ← min{ resid
˜𝐷1 (x) ← ˜𝐷1 (x)min′ + allocation
resid ← resid − 𝑃 (x) · allocation

𝑃 (x) , capacity}

return ˜𝐷1(x), ∀x ∈ 𝑇xU

19:

20:

21:

22:

23:

24:

25:

26:
27:

28:

the time complexity of Algorithm 1 is in 𝑂 (𝑛). By using multi-threaded coding structures solving
“parallelizable” sub-problems via Algorithm 1, (OPT) can be solved in 𝑂 (𝑛).

7.4 Theorem Insights on Achieving Solution Optimality

In this section, we provide some insights into the optimal-privacy solutions subject to ﬁdelity
constraints in Theorem 1.

An important observation is that the optimal-privacy candidate values (𝛽0, 𝛽1, and 𝛽𝑝) the in-
ference conﬁdence (left-hand-side of (16b)) are fully characterized by 𝑃 (x) ˜𝐷𝑎 (x) pairs of prod-
uct, which are the announced joint probabilities ˜𝑃𝑋,𝐴 (x, 𝑎) representing the portion of population
with input record x receiving decision 𝑎, bounded within ranges [𝑃 (x) ˜𝐷𝑎 (x)min, 𝑃 (x) ˜𝐷𝑎 (x)max]

Achieving Transparency Report Privacy in Linear Time

21

Fig. 4. An representative illustration for changes of joint probabilities caused by the optimal-privacy scheme.

due to ﬁdelity constraints. Solving (OPT-Sub) to obtain optimal-privacy solution is thus equiva-
lent to “tune” those pairs of product within the allowed range, for all inputs x ∈ 𝑇xU and outputs
𝑎 ∈ A, to minimize the maximal possible inference conﬁdence 𝛽 of an adversary. Particularly, from
Theorem 1, it turns out that for each decision outcome instance 𝑎, the term 𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min =
𝑃 (x) ˜𝐷𝑎 (x)min, the maximum of the lower bounds of the allowed ranges over x ∈ 𝑇xU ,
maxx∈𝑇xU
plays a crucial role in solving (OPT-Sub). Next, we show that the optimal-solution for x = x𝑎 can
only be the minimum of its allowed range.

Corollary 1.

˜𝐷𝑎 (x𝑎)min = ˜𝐷𝑎 (x𝑎)min′ = ˜𝐷𝑎 (x𝑎)max′, ∀𝑎 ∈ {0, 1}.

Proof. By deﬁnitions of x𝑎 and ˜𝐷𝑎 (x)max′, the result ˜𝐷𝑎 (x𝑎)min = ˜𝐷𝑎 (x𝑎)max′ trivially follows.
x′ 𝑃 (x′) ≤ 0, and thus by plugging x = x𝑎 into
(cid:3)

Based on Lemma 5, we have 𝑏 (x) = 𝑃 (x) − 𝛽𝑝
˜𝐷𝑎 (x)min′, we obtain ˜𝐷𝑎 (x𝑎)min = ˜𝐷𝑎 (x𝑎)min′.

Í
The eﬀective lower limits 𝑃 (x) ˜𝐷𝑎 (x)min′ and the eﬀective upper limits 𝑃 (x) ˜𝐷𝑎 (x)max′ represent
the feasible region where the ﬁdelity constraints and privacy constraints intersect. Based on Corol-
lary 1, the eﬀective upper and lower limit of x𝑎 are equal, which implies that when x = x𝑎, the
only possible value for the optimal-privacy solution is ˜𝐷𝑎 (x𝑎)min. From Theorem 1, we can see
that this is true for all the three cases. It is worth noting that the cases 𝛽∗
= 𝛽1 are
equivalent (by swapping 0’s and 1’s), so we only have two representative cases: 𝛽∗
= 𝛽𝑎, 𝑎 ∈ {0, 1},
and 𝛽∗

= 𝛽0 and 𝛽∗

𝑇xU

𝑇xU

𝑇xU

= 𝛽𝑝 .

𝑇xU

𝑇xU

= 𝛽𝑎, 𝑎 ∈ {0, 1}, the allowed ranges of all 𝑃 (x) ˜𝐷𝑎 (x)
7.4.1 Representative Case 1: When 𝛽∗
pairs are imposed by an additional upper limit 𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min caused by privacy constraints. In
these cases, eﬀective upper limits are the minimum of the original upper limits 𝑃 (x) ˜𝐷𝑎 (x)max
and the threshold, formally, 𝑃 (x) ˜𝐷𝑎 (x)max′ = min{𝑃 (x) ˜𝐷𝑎 (x)max, 𝑃 (x𝑎 ) ˜𝐷𝑎 (x𝑎)min}. According to
Theorem 1, when 𝛽∗
= 𝛽𝑎, the corresponding optimal-privacy solution is simply the eﬀective
𝑇xU
upper limit ˜𝐷𝑎 (x) = ˜𝐷𝑎 (x)max′, ∀x ∈ 𝑇xU .

An illustration that aids in understanding the intuition behind Theorem 1 is shown in Fig. 4,
in which joint probabilities for 𝑎 = 1 and ∀x ∈ 𝑇xU are depicted for the case 𝛽∗
= 𝛽1. For
6, 𝑝𝑖 = 𝑃 (x𝑖 ), and ˜𝑑𝑖 = ˜𝑑 (x𝑖 ) = ˜𝐷1(x𝑖 ), ∀𝑖 = 1, . . . , 𝑚. The yellow
conciseness, let 𝑚 denote
spots in Fig. 4 denote the true joint probabilities 𝑝𝑖𝑑𝑖, and the regions indicated by grey arrows

𝑇xU

𝑇xU

6Since 𝑇xU is the range of (xU, 𝑋S ), 𝑚 represents the number of distinct sensitive records in the tuple.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

22

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

interpret the allowed perturbation ranges [𝑝𝑖 ˜𝑑𝑖min, 𝑝𝑖 ˜𝑑𝑖max]. In this example, the maximum of the
lower limits is 𝑝6 ˜𝑑6min, i.e., x1 = x6. The value 𝑝6 ˜𝑑6min serves as a threshold (the blue dash line),
imposing an upper limit on all perturbation ranges. The output of the optimal-privacy solution is
denoted by the red spots, which take values from the eﬀective upper bounds min{𝑝𝑖 ˜𝑑𝑖max, 𝑝6 ˜𝑑6min},
∀𝑖. Here we get a clear insight into the optimal-privacy solutions for a QID sub-group 𝑇{xU,𝑎 }: the
optimality is achieved by ﬂattening the joint distribution 𝑃 (x) ˜𝐷𝑎 (x) over all x ∈ 𝑇xU as much as
possible subject to the allowed perturbation range imposed by ﬁdelity constraints. By ﬂattening
the joint distribution, the (Bayesian) posterior distribution over distinct sensitive values seen by
an adversary becomes more uniform, and hence the maximal inference conﬁdence is reduced.

Deﬁne 𝑎 the complement of 𝑎, e.g., 𝑎 = 0 if 𝑎 = 1. For binary decisions, the optimal-privacy
scheme for a QID group 𝑇xU is also privacy optimal for a sub-group 𝑇{xU,𝑎 } when the joint distri-
bution of 𝑇{xU,𝑎 } is much “ﬂatter” (more private) than 𝑇{xU,𝑎 }. In other words, the optimal-privacy
solution ﬂattens the least private distribution as much as possible; although this might inﬂuence
the other (the much more private) one and cause it to be less private7, as long as its maximal infer-
ence conﬁdence is less than 𝛽𝑎, the optimal privacy for the entire QID group is dominated by 𝛽𝑎,
and thus the optimal privacy scheme for the sub-group 𝑇{xU,𝑎 } is the optimal privacy scheme for
the entire QID group.

7.4.2 Representative Case 2: When neither distribution is much ﬂatter than the other one, making
one sub-group highly private causes the other one’s privacy to degrade, i.e., none of the optimal
schemes for any QID sub-group can be optimal for the entire QID group. In such a case, both
sub-groups need to ﬁnd a “balanced point” at which both sub-groups are equally private. Such
a balanced privacy value for the maximal inference conﬁdence for two sub-groups is denoted by
𝛽𝑝 in Theorem 1, representing the optimal privacy for the QID group. As shown in Theorem 1, in
general, we have multiple solutions to achieve this balanced privacy value. This is because, in such
a case, the optimality of privacy is guaranteed if (i) ˜𝐷𝑎 (x𝑎) = ˜𝐷𝑎 (x𝑎)min, ∀𝑎, and (ii) the following
two equalities hold

x 𝑃 (x) ˜𝐷0(x) = 1
𝛽𝑝
x 𝑃 (x) ˜𝐷1(x) = 1
𝛽𝑝

Í

𝑃 (x0) ˜𝐷0 (x0)min,
𝑃 (x1) ˜𝐷1 (x1)min,

(19)

(20)

While in the following, we show that the above two equalities are equivalent, i.e., one implies the
other.

Í

Corollary 2. When 𝛽∗

= 𝛽𝑝, (19) implies (20), and vice versa.

𝑇xU
Proof. Recall 𝛽𝑝 from Theorem 1, we have

x𝑃 (x) = 1
𝛽𝑝

𝑃 (x1) ˜𝐷1 (x1)min + 1
𝛽𝑝

𝑃 (x0) ˜𝐷0(x0)min.

(21)

Since ˜𝐷0(x) + ˜𝐷1 (x) = 1, subtract (19) from (21), we obtain (20). Similarly, subtract (20) from (21),
(cid:3)
we obtain (19).

Í

Therefore, to compute an optimal solution when 𝛽∗

= 𝛽𝑝 , we only need to solve (20). Since
˜𝐷0 (x) + ˜𝐷1 (x) = 1, and ˜𝐷𝑎 (x𝑎) = ˜𝐷𝑎 (x𝑎)min, ∀𝑎, in general we only have 𝑚 − 2 variables (see
Remark 4), and based on (21), equality (20) is equivalent to

𝑇xU

𝑃 (x) ˜𝐷1 (x) =

1−2𝛽𝑝
𝛽𝑝

𝑃 (x1) ˜𝐷1 (x1)min − 𝑏 (x0).

(22)

7Based on (EQ-Sub), any changes made to ˜𝐷𝑎 (x) will also change ˜𝐷𝑎 (x).

Õx≠x0,x1

(cid:0)

(cid:1)

Achieving Transparency Report Privacy in Linear Time

23

Table 4. Detailed Inputs and Computations of the Provided Numerical Example

0

1

1

x 𝑃 (x) 𝐷1 (x) 𝐷0 (x)
x1 0.3
0
x2 0.125
x3 0.075
x4 0.225
x5 0.175
x6 0.1

1
0.5
0

0
0.5
1

0

1

Inputs

˜𝐷1 (x) min
0

0
0.9
0
0.4
0.9

˜𝐷1 (x) max
0.1
0.1
1
0.1
0.6
1

˜𝐷0 (x) min
0.9
0.9
0
0.9
0.4
0

Computations

˜𝐷0 (x) max 𝑃 (x) ˜𝐷1 (x) min 𝑃 (x) ˜𝐷1 (x) max 𝑃 (x) ˜𝐷1 (x) max′ 𝑃 (x) ˜𝐷0 (x) min 𝑃 (x) ˜𝐷0 (x) max 𝑃 (x) ˜𝐷0 (x) max′

1

1
0.1
1
0.6
0.1

0

0
0.0675
0
0.07
0.09

0.03
0.0125
0.075
0.0225
0.105
0.1

0.03
0.0125
0.0675
0.0225
0.09
0.09

0.27
0.1125
0
0.2025
0.07
0

0.3
0.125
0.0075
0.225
0.105
0.01

0.27
0.125
0.0075
0.2025
0.105
0.01

𝑇xU

When 𝛽∗

x≠x0,x1 𝑃 (x) ˜𝐷1 (x)max′

= 𝛽𝑝, the right-hand-side (RHS) of (22) is strictly bounded by

x≠x0,x1 𝑃 (x) ˜𝐷1(x)min′,
, which implies there always exists a feasible solution for (20). When 𝑚 >
3, since the number of variables to solve (𝑚 − 2) is greater than the number of equation (one, which
Í
is (22)), an optimal solution, in general, is not unique.

(cid:2) Í

(cid:3)

Remark 4. For the special case x0 = x1, we have 𝑚 − 1 variables. Such a case can happen when the
population of a certain record dominates its corresponding QID group. When this is the case, the
prior (distribution) knowledge provides very high (baseline) conﬁdence on inferring this record.
In particular, for such a case, we must have 𝛽𝑝 = 𝛽min. If 𝛽𝑝 > 𝛽𝑎, ∀𝑎, i.e., 𝛽∗
= 𝛽𝑝, this becomes
trivial: according to Lemma 5 and its following discussion, the announced ATR can only provide
trivial information to achieve this lowest-possible baseline conﬁdence.

𝑇xU

8 NUMERICAL EXAMPLES

In the previous section, we proposed lemmas characterizing important properties about privacy-
ﬁdelity trade-oﬀ, a theorem providing closed-form optimal solutions for the trade-oﬀ problem, and
insights into the optimal solutions for both the representative cases. In this section, we provide
numerical examples to demonstrate privacy-ﬁdelity trade-oﬀ regions, and aid in understanding the
properties of the trade-oﬀ regions and the insights into the optimal solutions for both the repre-
sentative cases. Without loss of generality, we reuse the same examples demonstrated in Section 3
showcasing how the proposed linear-time optimal-privacy scheme (Algorithm 1) can be applied in
practice to solve the problem, as long as there is no privacy preference among sensitive attribute
values.

Consider Table 1 again but for a smaller size population {12, 5, 3, 9, 7, 4} (ﬁrst column of the table)
for ease of demonstration, and let x𝑖 denote the record of the 𝑖-th row, 𝑖 = 1, . . . , 6. Suppose an
announced ATR needs to satisfy a pre-determined ﬁdelity constraint 𝛿 = 0.9 (90%-ﬁdelity), and we
would like to preserve data subjects’ privacy as much as possible subject to the ﬁdelity constraint.
First, consider the QID group of female 𝑇xU ={F}, i.e., the tuple of records 𝑇{F} = {x1, x2, x3}. Based
on lines 1 to 4 in Algorithm 1, we ﬁrst need to determine x𝑎 and ˜𝐷𝑎 (x)max′ ∀𝑎 ∈ {0, 1}, ∀x ∈ 𝑇{F}.
Detailed computations are demonstrated in Remark 5, and the computed results are presented in
Table 4; from which, we observe that x1 = x3 and x0 = x1 (see Remark 5 for details as well).

Proceeding to line 5, we compute 𝛽0, 𝛽1, and 𝛽𝑝 as follows:

𝛽1 =

𝑃 (x1) ˜𝐷1 (x1)min +

𝑃 (x) ˜𝐷1 (x)max′

𝑃 (x1) ˜𝐷1 (x1)min
x≠x1,x∈𝑇xU

=

0.0675
0.0675 + 0.03 + 0.0125

≈ 0.6136,

Í

24

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

𝛽0 =

𝛽𝑝 =

𝑃 (x0) ˜𝐷0(x0)min
x≠x0,x∈𝑇xU

𝑃 (x0) ˜𝐷0(x0)min +
𝑃 (x1) ˜𝐷1(x1)min + 𝑃 (x0) ˜𝐷0 (x0)min
x∈𝑇xU

Í
𝑃 (x)

𝑃 (x) ˜𝐷0 (x)max′

=

0.27
0.27 + 0.125 + 0.0075

≈ 0.6708,

=

0.0675 + 0.27
0.5

= 0.675,

and obtain 𝛽∗
𝑇{F}
to call function Allocation in line 17. Based on lines 18 and 19, we ﬁrst need to compute

, max{𝛽0, 𝛽1, 𝛽𝑝 } = 𝛽𝑝 = 0.675. Proceeding to lines 12 and 13, in this case we need

Í

˜𝐷1 (x2)min′ = 1
1
0.125

=

𝑃 (x2) max{𝑃 (x2) ˜𝐷1 (x2)min, 𝑃 (x1) ˜𝐷1(x1)min + 𝑏 (x2)}

max{0, 0.0675 + 0.125 − (0.675) (0.5)} = 0.

Proceeding to line 20, since ˜𝐷1(x2)min′ = 0, we have

resid = RHS of (22) =

𝑃 (x1) ˜𝐷1 (x1)min − 𝑏 (x0)

1−2𝛽𝑝
𝛽𝑝
= ( −0.35
0.675 ) (0.075) (0.9) + (0.675) (0.5) − 0.3 = 0.0025.
(cid:0)

(cid:1)

Based on lines 21 and 22, we obtain

˜𝐷1 (x3) = ˜𝐷1(x1) = ˜𝐷1 (x1)min = ˜𝐷1(x3)min = 0.9,
˜𝐷1 (x1) = ˜𝐷1(x0) = 1 − ˜𝐷0 (x0)min = 1 − ˜𝐷0 (x1)min = 0.1.

Moreover, proceeding to lines 23 to 27, we obtain

capacity = ˜𝐷1 (x2)max′ − ˜𝐷1 (x2)min′ = 0.0125
allocation = min{ 0.0025
˜𝐷1(x2) = ˜𝐷1 (x2)min′ + allocation = 0 + 0.02 = 0.02.

0.125 , 0.1} = 0.02,

0.125 − 0 = 0.1,

We therefore obtain the optimal solution ˜𝐷1 ({x1, x2, x3}) = [0.1, 0.02, 0.9] for the QID group
of female, which yields maximum conﬁdence of 67.5% for an adversary inferring any sensitive
information from any female data subject.

We then consider the QID group for male 𝑇xU ={M}, i.e., the tuple of records 𝑇{M} = {x4, x5, x6}.

Similarly, based on Table 4, we obtain x1 = x6, x0 = x4, and

𝛽1 =

𝛽0 =

𝛽𝑝 =

0.09
0.0225 + 0.09 + 0.09
0.2025
0.2025 + 0.105 + 0.01
0.09 + 0.2025
0.5

= 0.585,

≈ 0.4444,

≈ 0.6378,

𝑇{M}

and we get 𝛽∗
= max{𝛽0, 𝛽1, 𝛽𝑝 } = 𝛽0 ≈ 0.6378. Based on lines 6 to 8, we obtain the optimal solu-
tion for this group ˜𝐷1 ({x4, x5, x6}) = 1− ˜𝐷0({x4, x5, x6})max′ = 1− [ 0.2025
0.1 ] = [0.1, 0.4, 0.9]
for QID group of male, which yields maximum conﬁdence of 63.78% for an adversary inferring any
sensitive information from any male data subject. Based on Lemma 3, the optimal-privacy 𝛽∗ for
the entire dataset is max{𝛽∗
} = max{0.675, 0.6378} = 0.675, which is the maximum conﬁ-
𝑇{F}
dence for an adversary inferring any sensitive information from any data subject from this dataset,
based on the announced ATR.

0.225 , 0.105

0.175, 0.01

, 𝛽∗

𝑇{M}

The optimal solution for the QID group of female is a “balanced point” between inferring the
annual income of x1 and x3 correctly, i.e., Conf (F, 𝐴 → Annual Income) = conf (F, 𝐴 = 0 →

Achieving Transparency Report Privacy in Linear Time

25

(a) P-F Tradeoﬀ for the QID Group of Female

(b) P-F Tradeoﬀ for the QID Group of Male

1

0.9

0.8

0.7

0.6

0.5

(cid:39)(cid:70)(cid:66)(cid:84)(cid:74)(cid:67)(cid:77)(cid:70)(cid:1)(cid:51)(cid:70)(cid:72)(cid:74)(cid:80)(cid:79)
*
T
{F}

=0.6

min

C*=1

 = 0.675

(cid:39)(cid:70)(cid:66)(cid:84)(cid:74)(cid:67)(cid:77)(cid:70)(cid:1)(cid:51)(cid:70)(cid:72)(cid:74)(cid:80)(cid:79)

C*=0.72

 = 0.6378

*
T
{M}

=0.45

min

0.8

0.7

0.6

0.5

0.4

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Fig. 5. Privacy-Fidelity (P-F) Tradeoﬀs for QID Groups of Female and Male

< 100k) = conf (F, 𝐴 = 1 → > 200k), where 𝐴 is deﬁned in Table 3, the random variable of
decision outcome (0: negative; 1: positive), and

conf (F, 0 → < 100k) =

conf (F, 1 → > 200k) =

𝑃 (x1) ˜𝐷0(x1)
𝑃 (x1) ˜𝐷0 (x1) + 𝑃 (x2) ˜𝐷0(x2) + 𝑃 (x3) ˜𝐷0 (x3)
𝑃 (x3) ˜𝐷1(x3)
𝑃 (x1) ˜𝐷1 (x1) + 𝑃 (x2) ˜𝐷1(x2) + 𝑃 (x3) ˜𝐷1 (x3)

=

0.3×0.9
0.3×0.9+0.125×0.98+0.075×0.1

= 0.675,

=

0.075×0.9
0.3×0.1+0.125×0.02+0.075×0.9

= 0.675.

Making either inference more private will cause the other one less private and hence degrades
the overall privacy guarantee as discussed in Section 7.4. In contrast, the optimal solution for the
male group tries to minimize the conﬁdence of correctly inferring the annual income of x4, i.e.,
Conf (M, 𝐴 → Annual Income) = conf (M, 𝐴 = 0 → < 100k), and

conf (M, 0 → < 100k) =

𝑃 (x4) ˜𝐷0 (x4)
𝑃 (x4) ˜𝐷0(x4) + 𝑃 (x5) ˜𝐷0 (x5) + 𝑃 (x6) ˜𝐷0 (x6)

=

0.225×0.9+0.175×0.6+0.1×0.1 ≈ 0.6378.

0.225×0.9

From the above equation, it is not hard to see that the optimal solution maximizes the denominator
while minimizing the numerator in order to minimize the ratio for optimal privacy.

From the above example, we demonstrated that subject to ﬁdelity constraints, how an optimal-
privacy ATR can be obtained eﬃciently using Algorithm 1. The maximum conﬁdence of an adver-
sary, which is conf (F, 1 → > 200k), drops from 100% to 67.5% by setting a 10%-distortion tolerance
for the announced ATR.

Figure 5 depicts the privacy-ﬁdelity tradeoﬀs for both QID groups in this numerical example.
Given any ﬁdelity requirement 𝛿, the optimal privacy (i.e., the smallest possible 𝛽) that we can
achieve is the boundary of the trade-oﬀ region (the blue curve). The tradeoﬀ region for 𝛽, as dis-
cussed in Section 7.2, should be within the range [𝛽𝑚𝑖𝑛, 𝐶∗], which can be easily computed based
on Deﬁnition 7 and Lemma 5:
For the QID group of female: [𝛽𝑚𝑖𝑛, 𝐶∗] =

[

𝑃 (x1)
𝑃 (x1)+𝑃 (x2)+𝑃 (x3) ,

𝑃 (x1)𝐷1 (x1)+𝑃 (x2)𝐷1 (x2)+𝑃 (x3)𝐷1 (x3) ] = [ 0.3

𝑃 (x1)𝐷1 (x1)

0.5, 0.075×1

0.075×1 ] = [0.6, 1].

For the QID group of male: [𝛽𝑚𝑖𝑛, 𝐶∗] =

[

𝑃 (x4)
𝑃 (x4)+𝑃 (x5)+𝑃 (x6) ,

𝑃 (x4)𝐷1 (x4)+𝑃 (x5)𝐷1 (x5)+𝑃 (x6)𝐷1 (x6) ] = [ 0.225
0.5 ,

𝑃 (x4)𝐷1 (x4)

0.225×1

0.225×1+0.175×0.5 ] = [0.45, 0.72].

Both results show consistency with Figure 5 : in Figure 5a, the range of 𝛽 is within [0.6, 1]; in
Figure 5b, the range of 𝛽 is within [0.45, 0.72]. Note that based on Lemma 5, any privacy require-
ment with 𝛽 < max{0.6, 0.45} = 0.6 is not feasible for this dataset, and based on Lemma 4, any
privacy requirement with 𝛽 > 0.72 can have 1-ﬁdelity solution for the QID group of male, i.e., no
perturbation is needed. How much ﬁdelity should be sacriﬁced in order to achieve a certain level
of privacy can thus be known based on the tradeoﬀ curves.

26

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Remark 5. Here we demonstrate how the values presented in Table 4 are computed. Note that,
we only demonstrate the computation of values in the ﬁrst row (i.e., for x1), as computations for
values in all other rows (for all other xi’s) follow similar steps. In the following, we start from the
left-most value and then move to the right.

For x = x1, xU = {F} (Female), and since the total population is 12 + 5 + 3 + 9 + 7 + 4 = 40,
𝑃 (x) = 12/40 = 0.3. Based on Table 1, since the decision rule represents the probability of receiving
a positive decision, 𝐷1(x) is basically the decision rule in Table 1, and 𝐷0 (x) is simply 1 − 𝐷1(x).
The pre-deﬁned ﬁdelity parameter 𝛿 is 0.9, i.e., the announced decision mapping ˜𝐷𝑎 (x) can diﬀer
from the true decision mapping 𝐷𝑎 (x) by at most 10%, ∀𝑎 = 0, 1. Therefore, | ˜𝐷1(x) − 0| ≤ 0.1, and
we get ˜𝐷1 (x)min = 0 and ˜𝐷1 (x)max = 0.1. Similarly, | ˜𝐷0 (x) − 1| ≤ 0.1, and we get ˜𝐷0 (x)min = 0.9
and ˜𝐷0 (x)max = 1. The values of the above terms are based on their deﬁnitions (refer to Theorem 1)
and the input parameters. Since now we have values for 𝑃 (x), ˜𝐷1 (x)min, ˜𝐷1 (x)max, ˜𝐷0 (x)min, and
˜𝐷0 (x)max, the values for the terms 𝑃 (x) ˜𝐷1 (x)min, 𝑃 (x) ˜𝐷1 (x)max, 𝑃 (x) ˜𝐷0(x)min, and 𝑃 (x) ˜𝐷0 (x)max
are just simple multiplications.

Next, we show how the values for the terms 𝑃 (x) ˜𝐷1(x)max′ (third column in the “Computations”
˜𝐷𝑎 (x)max′ ,
𝑃 (x) ˜𝐷𝑎 (x)min, ∀𝑎 = 0, 1, we
𝑃 (x) ˜𝐷0 (x)min = arg maxx∈{x1,x2,x3 }{0.27, 0.1125, 0} = x1 and x1 =
0.3 min{0.03, 0.0675} =
0.3 min{0.3, 0.27} = 0.27/0.3. Therefore, we obtain 𝑃 (x) ˜𝐷1 (x)max′ = 0.03

category) and 𝑃 (x) ˜𝐷0(x)max′ (the last column) are obtained. Based on Theorem 1,
𝑃 (x) min{𝑃 (x) ˜𝐷𝑎 (x)max, 𝑃 (x𝑎) ˜𝐷𝑎 (x𝑎)min}, where x𝑎 , arg maxx∈𝑇xU
1
thus have x0 , arg maxx∈𝑇{F}
arg maxx∈{x1,x2,x3 }{0, 0, 0.0675} = x3. Hence, for x = x1,
0.03/0.3 and ˜𝐷0 (x)max′ = 1
and 𝑃 (x) ˜𝐷0(x)max′ = 0.27.

˜𝐷1 (x)max′ = 1

9 RELATED WORK

There is a huge literature on transparency [28, 46] and fairness [63] for ML. [59] provides a detailed
survey on techniques proposed for enhancing transparency and fairness for ML models. However,
the perspectives of transparency and fairness in ML models may not be completely in sync with
those in algorithmic transparency, e.g., the philosophy of fairness in ML is to train fair ML mod-
els or algorithms, while the philosophy of fairness in accountable algorithmic transparency is to
verify or to demonstrate whether the examined ML algorithms comply with certain fairness re-
quirements.

There are a number of studies on transparency and fairness and several addressing privacy in
data transparency, e.g., [72, 82, 91]; however, there is little eﬀort in considering the potential impact
on privacy brought on by algorithmic transparency schemes and/or fairness measures. [24] provides
transparency in the interaction between Google Ads, users’ Ad privacy settings, and user behav-
iors, showing the disparate impact that female gender setting has (vs. male gender setting) on
results, e.g., with fewer instances of ads related to high paying jobs; while whether users’ privacy
could be leaked from Google Ads or the associated transparency report is unclear without fur-
ther investigation. [8] investigates the limitations of transparency and its impact on society and
notes that transparency can threaten privacy, but it is yet to be made clear what possible aspects
of transparency can hurt privacy, and by what privacy-preserving techniques could remedy the situ-
ation. Here, we show that data subjects’ privacy can be leaked via various kinds of transparency
schemes and fairness measures in an announced ATR and propose a privacy protection scheme
yielding privacy preserving information on an ATR. Motivated by transparency and fairness, [33]
raises questions regarding fair privacy for all participating users, as it is considered discriminatory
when diﬀerent users are protected by diﬀerent levels of privacy; however, on what notion of privacy
should be fair, by what methodology to protect such privacy and to achieve it fairly are still unclear.

Achieving Transparency Report Privacy in Linear Time

27

However, in contrast, numerical examples in Section 8 show that the optimal privacy for diﬀerent
QID groups, subject to the same ﬁdelity constraints, is in general diﬀerent due to the disparity of
prior distributions, prior vulnerabilities [66], side-information, and associated decision mapping
between groups. [81] studies the problem of providing transparency to consumers while preserv-
ing information privacy for them, and proposes informational norms to constrain the collection,
use, and distribution of transparent information in role-appropriate manners to fulﬁll the goal.
However, the deﬁnition of role-appropriate manners is yet to be more speciﬁc, and it is also unclear
how fairness measures, which compare decision rules between two individuals or groups (likely in
diﬀerent roles), should be announced under such a norm. In contrast, our privacy protection scheme
does not make any assumptions on informational norm and does not rely on any norm (poten-
tially hard to accomplish) to protect users’ privacy, and thus can be applied generally. In addition,
informational norm may still not be adequate to protect users’ privacy: individuals belonging to
the same role may still be able to infer private information of others, e.g., in Table 1, any female
credit card owner can infer other female credit card owners’ income range.

There exist a couple of works using diﬀerential privacy (DP) to remedy the privacy leakage/attack
issue in algorithmic transparency or model explanations. A recent work [79] demonstrates mem-
bership inference attacks [80] on training datasets of ML models by utilizing information from
the corresponding featured-based model explanations (i.e., feature importance/interaction trans-
parency schemes). To address this issue, in [70], DP is applied to the gradient descent algorithm
for generating feature-based model explanations. [23], arguably the only previous work that ad-
dresses transparency, fairness, and privacy in an accountable ATR, proposes a feature-based mea-
sure, named quantitative input inﬂuence (QII); based on which, the authors propose public and
personalized transparency reports, as well as a fairness measure, named group disparity, to mea-
sure potential disparate impacts on diﬀerent groups of people. DP is adopted to the above measures
in order to prevent potential privacy leaks caused by the provided QII and group disparity in an
announced ATR. However, applying DP solely does not result in prevention of inference attacks, in
particular, attribute inference attacks: once strong correlations between attribute values are known,
sensitive attribute values can be inferred no matter whether a privacy victim belongs to a speciﬁc
dataset or not, and thus DP cannot help in such a scenario (see [32], Section 2.3.2, the smoking-
cause-cancer example). In light of this, here, we propose a privacy-preserving scheme to prevent
attribute inference attacks by limiting the attribute inference conﬁdence from public/known at-
tribute values, via an announced ATR with assistance of side-information, to any data subjects’
private attribute values.

10 SUMMARY

In this work, we demonstrated how a honest-but-curious adversary can utilize widely-available
information provided in an algorithmic transparency report, to obtain data subjects’ private infor-
mation. From this we glean which potential aspects of transparency and fairness measures can
hurt privacy. We then propose a privacy scheme that perturbs the information to be announced,
to remedy the privacy leaks. We systematically study the impact of such perturbation on fair-
ness measures and the ﬁdelity of the announced information, formulated as an optimization for
optimal privacy subject to ﬁdelity constraints. To eﬃciently solve the optimization problem, we
reveal important properties and provide closed-form solutions, based on which we propose a pri-
vacy protection scheme. Given ﬁdelity requirements, the proposed scheme can eﬃciently produce
optimal-privacy ATRs in linear time. In addition, we provide insight into our proposed optimal pri-
vacy scheme. Our proposed methodology is suited to more general problems beyond algorithmic
transparency, where the release of the model information is controlled and the input data cannot
be modiﬁed - an example being in the setting of model inversion attacks in [40] where the model

28

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

owner has no authority to modify the input data (patents’ clinical history and genomic data) but
has the control of the amount of information about the (dose-suggesting) model to be released. In
such a scenario, our scheme can help privately release information of a model to pharmacists for
better understanding of suggesting personalized dosage.

REFERENCES
[1] [n.d.]. Admissions Transparency Data, New College of the Humanities, London, United Kingdom. https://t.ly/9bo0.
[2] [n.d.]. Admissions Transparency Implementation Working Group, Department of Education, Skills, and Employment,

Australian Government. https://t.ly/qaaZ. Accessed: 2021-02-19.

[3] [n.d.]. Open Government. https://www.oecd.org/gov/open-government/. Accessed: 2021-02-19.
[4] [n.d.]. U.S. Census Bureau Historical Income Tables: People. https://goo.gl/UDoF64. Accessed: 2018-10-01.
[5] Alessandro Acquisti and Ralph Gross. 2009. Predicting social security numbers from public data. Proceedings of the

National Academy of Sciences (2009), PNAS–0904891106.

[6] Anita L Allen. 2016. Protecting one’s own privacy in a big data economy. Harv. L. Rev. F. 130 (2016), 71.
[7] Mário S Alvim, Miguel E Andrés, Konstantinos Chatzikokolakis, Pierpaolo Degano, and Catuscia Palamidessi. 2011.
Diﬀerential Privacy: On the Trade-Oﬀ between Utility and Information Leakage. Formal Aspects in Security and Trust
7140 (2011), 39–54.

[8] Mike Ananny and Kate Crawford. 2018. Seeing without knowing: Limitations of the transparency ideal and its appli-

cation to algorithmic accountability. New Media & Society 20, 3 (2018), 973–989.

[9] Kurt M Anstreicher. 1999. Linear programming in 𝑂 ( 𝑛

3

ln 𝑛 𝐿) operations. SIAM Journal on Optimization 9, 4 (1999),

803–812.

[10] Daniel W Apley. 2016. Visualizing the Eﬀects of Predictor Variables in Black Box Supervised Learning Models. arXiv

preprint arXiv:1612.08468 (2016).

[11] Michael Barbaro, Tom Zeller, and Saul Hansell. 2006. A face is exposed for AOL searcher no. 4417749. New York Times

9, 2008 (2006), 8.

[12] Solon Barocas and Andrew D Selbst. 2016. Big data’s disparate impact. Cal. L. Rev. 104 (2016), 671.
[13] Gilles Barthe and Boris Kopf. 2011. Information-theoretic bounds for diﬀerentially private mechanisms. In Computer

Security Foundations Symposium (CSF), 2011 IEEE 24th. IEEE, 191–204.

[14] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. 2017. Fairness in Criminal Justice Risk

Assessments: The State of the Art. arXiv preprint arXiv:1703.09207 (2017).

[15] Dan Biddle. 2006. Adverse Impact and Test Validation: A Practitioner’s Guide to Valid and Defensible Employment

Testing (2 ed.). Gower Publishing, Ltd.

[16] Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press.
[17] Stephen Boyd, Lin Xiao, Almir Mutapcic, and Jacob Mattingley. 2007. Notes on decomposition methods. Notes for

EE364B, Stanford University (2007), 1–36.

[18] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[19] Jianneng Cao and Panagiotis Karras. 2012. Publishing microdata with a robust privacy guarantee. Proceedings of the

VLDB Endowment 5, 11 (2012), 1388–1399.

[20] Fred H Cate, D Annette Fields, and James K McBain. 1994. The right to privacy and the public’s right to know: The

central purpose of the Freedom of Information Act. Admin. L. Rev. 46 (1994), 41.

[21] Chien-Lun Chen, Ranjan Pal, and Leana Golubchik. 2016. Oblivious mechanisms in diﬀerential privacy: experiments,

conjectures, and open questions. In 2016 IEEE Security and Privacy Workshops (SPW). IEEE, 41–48.

[22] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and

the cost of fairness. arXiv preprint arXiv:1701.08230 (2017).

[23] A. Datta, S. Sen, and Y. Zick. 2016. Algorithmic Transparency via Quantitative Input Inﬂuence: Theory and Experi-

ments with Learning Systems. In 2016 IEEE Symposium on Security and Privacy (SP). 598–617.

[24] Amit Datta, Michael Carl Tschantz, and Anupam Datta. 2015. Automated experiments on ad privacy settings. Pro-

ceedings on Privacy Enhancing Technologies 2015, 1 (2015), 92–112.

[25] Dua Dheeru and Eﬁ Karra Taniskidou. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml
[26] Nicholas Diakopoulos. 2014. Algorithmic accountability reporting: On the investigation of black boxes. (2014).
[27] Kelly Dilworth. [n.d.]. We still don’t know a lot about how credit card applications are evaluated. https://t.ly/ifSo.
[28] Filip Karlo Došilović, Mario Brčić, and Nikica Hlupić. 2018. Explainable artiﬁcial intelligence: A survey. In 2018 41st
International convention on information and communication technology, electronics and microelectronics. IEEE, 0210–
0215.

[29] Flávio du Pin Calmon and Nadia Fawaz. 2012. Privacy against statistical inference. In Communication, Control, and

Computing (Allerton), 2012 50th Annual Allerton Conference on. IEEE, 1401–1408.

Achieving Transparency Report Privacy in Linear Time

29

[30] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Aware-
ness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (Cambridge, Massachusetts)
(ITCS ’12). ACM, New York, NY, USA, 214–226.

[31] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private

data analysis. In Theory of Cryptography Conference. Springer, 265–284.

[32] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of diﬀerential privacy. Foundations and Trends

in Theoretical Computer Science 9, 3–4 (2014), 211–407.

[33] Michael D Ekstrand, Rezvan Joshaghani, and Hoda Mehrpouyan. 2018. Privacy for All: Ensuring Fair and Equitable

Privacy Protections. In Conference on Fairness, Accountability and Transparency. 35–47.

[34] Barbara Espinoza and Geoﬀrey Smith. 2013. Min-entropy as a resource.

Information and Computation 226 (2013),

57–75.

[35] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certi-
fying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. ACM, 259–268.

[36] Katherine Fink. 2018. Opening the government’s black boxes: freedom of information and algorithmic accountability.

Information, Communication & Society 21, 10 (2018), 1453–1471.

[37] Kate Finman. [n.d.]. CA state auditor report alleges UC admissions are biased, unfair. https://t.ly/7VcF. Accessed:

2021-02-19.

[38] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2018. Model Class Reliance: Variable Importance Measures for
any Machine Learning Model Class, from the" Rashomon" Perspective. arXiv preprint arXiv:1801.01489 (2018).
[39] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM, 1322–1333.

[40] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. Privacy in Phar-
macogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.. In USENIX Security Symposium. 17–32.

[41] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001),

1189–1232.

[42] Jerome H Friedman, Bogdan E Popescu, et al. 2008. Predictive learning via rule ensembles. The Annals of Applied

Statistics 2, 3 (2008), 916–954.

[43] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. Peeking inside the black box: Visualizing statis-
tical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics 24, 1
(2015), 44–65.

[44] Bryce Goodman and Seth Flaxman. 2016. European Union regulations on algorithmic decision-making and a "right

to explanation". arXiv preprint arXiv:1606.08813 (2016).

[45] Brandon M Greenwell, Bradley C Boehmke, and Andrew J McCarthy. 2018. A Simple and Eﬀective Model-Based

Variable Importance Measure. arXiv preprint arXiv:1805.04755 (2018).

[46] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A

survey of methods for explaining black box models. ACM Computing Surveys (CSUR) 51, 5 (2018), 93.

[47] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in supervised learning. In Advances in

Neural Information Processing Systems. 3315–3323.

[48] Harold V Henderson and Shayle R Searle. 1981. On deriving the inverse of a sum of matrices. Siam Review 23, 1 (1981),

53–60.

[49] Tamara E. Holmes. [n.d.]. How race aﬀects your credit score. https://t.ly/6gf V. Accessed: 2021-02-19.
[50] Andreas Holzinger, Chris Biemann, Constantinos S Pattichis, and Douglas B Kell. 2017. What do we need to build

explainable AI systems for the medical domain? arXiv preprint arXiv:1712.09923 (2017).

[51] Giles Hooker. 2004. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD

international conference on Knowledge discovery and data mining. ACM, 575–580.

[52] Mikella Hurley and Julius Adebayo. 2016. Credit scoring in the era of big data. Yale JL & Tech. 18 (2016), 148.
[53] Farhad Kamali and Hilary Wynne. 2010. Pharmacogenetics of warfarin. Annual review of medicine 61 (2010), 63–75.
[54] Faisal Kamiran, Indr˙e Žliobait˙e, and Toon Calders. 2013. Quantifying explainable discrimination and removing illegal

discrimination in automated decision making. Knowledge and Information Systems 35, 3 (2013), 613–644.

[55] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classiﬁer with prejudice

remover regularizer. Machine Learning and Knowledge Discovery in Databases (2012), 35–50.

[56] Igor Kononenko et al. 2010. An eﬃcient explanation of individual classiﬁcations using game theory. Journal of

Machine Learning Research 11, Jan (2010), 1–18.

[57] Sanjay Krishnan and Eugene Wu. 2017. PALM: Machine learning explanations for iterative debugging. In Proceedings

of the 2nd Workshop on Human-In-the-Loop Data Analytics. ACM, 4.

30

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

[58] Martin Kučera, Petar Tsankov, Timon Gehr, Marco Guarnieri, and Martin Vechev. 2017. Synthesis of probabilistic
privacy enforcement. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.
ACM, 391–408.

[59] Bruno Lepri, Nuria Oliver, Emmanuel Letouzé, Alex Pentland, and Patrick Vinck. 2018. Fair, transparent, and account-

able algorithmic decision-making processes. Philosophy & Technology 31, 4 (2018), 611–627.

[60] Floridi Luciano and Taddeo Mariarosaria. 2016. What is data ethics? Phil. Trans. R. Soc. A.37420160360 (2016).
[61] Ashwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan Venkitasubramaniam. 2006.

𝑙-
diversity: Privacy beyond 𝑘-anonymity. In Data Engineering, 2006. ICDE’06. Proceedings of the 22nd International Con-
ference on. IEEE, 24–24.

[62] Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubramaniam. 2007.
diversity: Privacy beyond 𝑘-anonymity. ACM Transactions on Knowledge Discovery from Data 1, 1 (2007), 3.

𝑙-

[63] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. A survey on bias and

fairness in machine learning. arXiv preprint arXiv:1908.09635 (2019).

[64] H. Mittelmann. [n.d.]. Benchmark of commercial LP solvers. http://plato.asu.edu/ftp/lpcom.html. Accessed: 2021-03-

6.

[65] Tomas Monarrez and Kelia Washington. 2020. Racial and Ethnic Representation in Postsecondary Education. Research

Report. Urban Institute (2020).

[66] S Alvim M’rio, Kostas Chatzikokolakis, Catuscia Palamidessi, and Geoﬀrey Smith. 2012. Measuring information
leakage using generalized gain functions. In 2012 IEEE 25th Computer Security Foundations Symposium. IEEE, 265–
279.

[67] Arvind Narayanan and Vitaly Shmatikov. 2008. Robust de-anonymization of large sparse datasets. In Security and

Privacy, 2008. SP 2008. IEEE Symposium on. IEEE, 111–125.

[68] Cathy O’neil. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway

Books.

[69] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016. Towards the science of security and

privacy in machine learning. arXiv preprint arXiv:1611.03814 (2016).

[70] Neel Patel, Reza Shokri, and Yair Zick. 2020. Model explanations with diﬀerential privacy. arXiv:2006.09129 (2020).
[71] Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as mechanisms for supporting algorithmic trans-

parency. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1–13.

[72] Joel R Reidenberg and Florian Schaub. 2018. Achieving big data privacy in education. Theory and Research in Education

16, 3 (2018), 263–279.

[73] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why should I trust you?”: Explaining the predictions
of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining. ACM, 1135–1144.

[74] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explanations.

In AAAI Conference on Artiﬁcial Intelligence.

[75] Pierangela Samarati and Latanya Sweeney. 1998. Generalizing data to provide anonymity when disclosing informa-

tion. In PODS, Vol. 98. Citeseer, 188.

[76] Pierangela Samarati and Latanya Sweeney. 1998. Protecting privacy when disclosing information: k-anonymity and its

enforcement through generalization and suppression. Technical Report. Technical report, SRI International.

[77] Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert Müller. 2019. Explainable

AI: interpreting, explaining and visualizing deep learning. Vol. 11700. Springer Nature.

[78] Nisha Shekhawat, Aakanksha Chauhan, and Sakthi Balan Muthiah. 2019. Algorithmic Privacy and Gender Bias Issues

in Google Ad Settings. In Proceedings of the 10th ACM Conference on Web Science. 281–285.

[79] Reza Shokri, Martin Strobel, and Yair Zick. 2020. On the Privacy Risks of Model Explanations. arXiv preprint

arXiv:1907.00164v5 (2020).

[80] Reza Shokri, Marco Stronati, and Vitaly Shmatikov. 2016. Membership inference attacks against machine learning

models. arXiv preprint arXiv:1610.05820 (2016).

[81] Robert H Sloan and Richard Warner. 2018. When is an algorithm transparent? Predictive analytics, privacy, and public

policy. IEEE Security & Privacy 16, 3 (2018), 18–25.

[82] Bernd Carsten Stahl and David Wright. 2018. Ethics and privacy in AI and big data: Implementing responsible research

and innovation. IEEE Security & Privacy 16, 3 (2018), 26–33.

[83] Latanya Sweeney. 1997. Weaving technology and policy together to maintain conﬁdentiality. The Journal of Law,

Medicine & Ethics 25, 2-3 (1997), 98–110.

[84] Latanya Sweeney. 2000. Simple demographics often identify people uniquely. Health 671 (2000), 1–34.
[85] RC Thompson. 1978. Matrix type metric inequalities. Linear and Multilinear Algebra 5, 4 (1978), 303–319.

Achieving Transparency Report Privacy in Linear Time

31

[86] Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition

(CVPR), 2011 IEEE Conference on. IEEE, 1521–1528.

[87] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning

Models via Prediction APIs.. In USENIX Security Symposium. 601–618.

[88] Ke Wang, Benjamin CM Fung, and Guozhu Dong. 2005. Integrating private databases for data analysis. In International

Conference on Intelligence and Security Informatics. Springer, 171–182.

[89] Ke Wang, Benjamin CM Fung, and S Yu Philip. 2007. Handicapping attacker’s conﬁdence: an alternative to 𝑘-

anonymization. Knowledge and Information Systems 11, 3 (2007), 345–368.

[90] Teresa Watanabe. [n.d.]. UCLA professor wants to see data on whether UC illegally uses race in admissions decisions.

https://t.ly/ny6t.

[91] Meg Young, Luke Rodriguez, Emily Keller, Feiyang Sun, Boyang Sa, Jan Whittington, and Bill Howe. 2019. Beyond
open vs. closed: Balancing individual privacy and public accountability in data sharing. In Proceedings of the Conference
on Fairness, Accountability, and Transparency. 191–200.

[92] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning fair representations. In Proceed-

ings of the 30th International Conference on Machine Learning (ICML-13). 325–333.

[93] Alexander Zien, Nicole Krämer, Sören Sonnenburg, and Gunnar Rätsch. 2009. The feature importance ranking mea-
sure. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 694–709.
[94] Stanley Zionts. 1968. Programming with linear fractional functionals. Naval Research Logistics Quarterly 15, 3 (1968),

449–451.

32

APPENDIX

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

A FAIRNESS MEASURES
Another important motivation of providing algorithmic transparency is to understand if a decision-
making algorithm is fair. GDPR Article 5 regulation indicates that personal data should be pro-
cessed fairly and in a transparent manner. Many researchers are committed to providing proper
measures for fairness and making ML algorithms fair [15, 22, 30, 35, 54, 55, 92]. In general, there are
two main categories of fairness: (i) individual fairness, and (ii) group fairness. Popular deﬁnitions
of group fairness includes statistical parity (SP), conditional statistical parity (CSP), and p%-rule
(PR) (see Appendix A for detailed deﬁnitions).

A.1 Measures for Individual Fairness
Deﬁnition 8. ((𝔇, D)-Individual Fairness [30]) Given a distance measure D : R𝑋 × R𝑋 → R+ ,
[0, ∞) on individuals’ records, a decision mapping 𝐷 : R𝑋 → Δ(A) satisﬁes individual fairness if it
complies with the (𝔇, D)-Lipschitz property for every two individuals’ records x1, x2 ∈ R𝑋 , i.e.,

𝔇(𝐷 (x1), 𝐷 (x2)) ≤ D(x1, x2),
(23)
where 𝔇 : Δ(A) × Δ(A) → R+ is a distance measure on distributions over A. Moreover, we deﬁne
𝐷 satisfying individual fairness up to bias 𝜀 if for all x1, x2 ∈ R𝑋 , we have

𝔇(𝐷 (x1), 𝐷 (x2)) ≤ D(x1, x2) + 𝜀.

(24)

Individual fairness ensures a decision mapping maps similar people similarly. When two indi-
viduals’ records x1 and x2 are similar, i.e., D(x1, x2) (cid:27) 0, the Lipschitz condition in equation (23)
ensures that both records map to similar distributions over A. Candidates for distance measure 𝔇
include (but not limited to) statistical distance and relative 𝑙∞ metric. The relative 𝑙∞ metric ( a.k.a.
relative inﬁnity norm) of two distributions 𝑍1 and 𝑍2, deﬁned as follow

𝔇∞ (𝑍1, 𝑍2) = sup
𝑎 ∈A

log

max

𝑍1(𝑎)
𝑍2(𝑎)

,

𝑍2(𝑎)
𝑍1(𝑎)

,

(25)

(cid:18)
is considered a potential better choice in the aspect that it does not require the distance measure
D to be re-scaled within [0, 1] 8. However, it has the shortcoming that it is sensitive to small
probability values. The statistical distance, or the total variation norm, of two distributions 𝑍1 and
𝑍2, deﬁned as follow

(cid:27)(cid:19)

(cid:26)

𝔇tv (𝑍1, 𝑍2) =

1
|A|

is a more stable measure in this aspect.

𝑍1(𝑎) − 𝑍2(𝑎)

,

(26)

Õ𝑎 ∈A (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

A.2 Measures for Group Fairness
Popular measures for group fairness include (but not limited to) statistical parity (SP) (a.k.a. demo-
graphic parity) [30, 35, 55, 92], conditional statistical parity (CSP) [22, 54], 𝑝-% rule (PR) [15, 35],
accuracy parity (a.k.a. equalized odds) [47], and true positive parity (a.k.a. equal opportunity) [47].
However, the last two measures require knowledge of labeled outputs and is thus particular used
to train fair ML algorithms in supervised learning. For algorithmic transparency, we use the former
three measures for group fairness.

Deﬁne 𝑔(𝑋 ) a projection function from input attributes 𝑋 onto a group in protected attributes,
𝑣 (𝑋 ) a score/valuation function from 𝑋 onto a set scores, and 𝑇Y , {x ∈ R𝑋 | 𝑔(x) ∈ Y} the

8The normalization could bring non-trivial burden, especially when the maximal distance can be arbitrarily large.

Achieving Transparency Report Privacy in Linear Time

33

set/tuple in which records belong to a protected group Y. We summarize deﬁnitions of measures
for group fairness in the following:

Deﬁnition 9. (Statistical Parity (SP)) A decision mapping 𝐷 : R𝑋 → Δ(A) satisﬁes statistical
parity for two groups Y1 and Y2 up to bias 𝜀 if for every decision outcome 𝑎 ∈ A, we have the
following property

𝔇tv

E

𝐷𝑎 (𝑋 )|𝑇Y1

, E

𝐷𝑎 (𝑋 )|𝑇Y2

≤ 𝜀.

(27)

(cid:16)

(cid:2)

(cid:3)

(cid:2)

(cid:3) (cid:17)

Deﬁnition 10. (Conditional Statistical Parity (CSP)) Given a score/valuation function 𝑣 (𝑋 ) based
on input attributes 𝑋 , deﬁne 𝑇Y,V , {x ∈ R𝑋 | 𝑔(x) ∈ Y, 𝑣 (x) ∈ V} the set/tuple in which records
belong to a protected group Y having scores in a set V. A decision mapping 𝐷 : R𝑋 → Δ(A) satisﬁes
conditional statistical parity given the same score conditions V for two groups Y1 and Y2 up to bias
𝜀 if for every decision outcome 𝑎 ∈ A, we have the following property

𝔇tv

E

𝐷𝑎 (𝑋 )|𝑇Y1,V

, E

𝐷𝑎 (𝑋 )|𝑇Y2,V

≤ 𝜀.

(28)

Deﬁnition 11. (𝑝-% Rule (PR)) A decision mapping 𝐷 : R𝑋 → Δ(A) satisﬁes 𝑝-% rule for two
groups Y1 and Y2 if for every decision outcome 𝑎 ∈ A, we have the following property

(cid:2)

(cid:3)

(cid:2)

(cid:3) (cid:17)

≤ − log 𝑝.

(29)

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

log

E

E

𝐷𝑎 (𝑋 )|𝑇Y1
𝐷𝑎 (𝑋 )|𝑇Y2

(cid:2)

(cid:3)

(cid:18)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3)

In particular, for binary decisions, we say a decision rule 𝑑 satisﬁes SP, CSP, or PR for two groups

(cid:2)

Y1 and Y2 up to bias 𝜀 (SP and CSP only) if

SP:

E

𝑑 (𝑋 )|𝑇Y1

− E

𝑑 (𝑋 )|𝑇Y2

≤ 𝜀

(cid:2)
(cid:3)
𝑑 (𝑋 )|𝑇Y1,V

(cid:2)
− E

𝑑 (𝑋 )|𝑇Y2,V

(cid:3) (cid:12)
(cid:12)
(cid:12)

CSP:

(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
(cid:2)
(cid:12)
PR: 𝑝 ≤
(cid:12)

E

E

𝑑 (𝑋 )|𝑇Y1
(cid:3)
𝑑 (𝑋 )|𝑇Y2
(cid:2)

≤

(cid:2)
1
𝑝

.

≤ 𝜀

(cid:3) (cid:12)
(cid:12)
(cid:12)

(30)

(31)

(32)

(cid:3)
Note that all fairness deﬁnitions are based on the distance between the decision of two groups9,
(cid:3)
speciﬁcally, total variation (26) and relative metric (25). Let F denote the set of all fairness deﬁni-
tions. Based on the use of distance metrics, F can be classiﬁed as follows:

(cid:2)

• Total-variation-based fairness deﬁnitions (Ftv): Deﬁnitions include (𝔇tv, D)-individual fair-

ness, statistical parity, and conditional statistical parity.

• Relative-metric-based fairness deﬁnitions (Frm): Deﬁnitions include (𝔇∞, D)- individual fair-

ness and p%-rule.

B PRIVACY LEAKAGE VIA FEATURE IMPORTANCE/INTERACTION

Feature (value) importance, or feature (value) interaction, measures the importance (or inﬂuence) of
input attributes (or attribute values) to the decision outcomes. The importance of an input attribute
(value) is measured based on the corresponding change of output due to change of that certain input.
By changing an input, if the change of output is signiﬁcant, it implies the input is important (has
signiﬁcant inﬂuence) to the output. On the other hand, if the output changes very little, the input
contributes very little to the output.

9More precisely, from (27), (28), and (29), the decision distribution of a group is the expected decision mapping among the
group, over all decision outcomes 𝑎 ∈ A.

34

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 5. Attribute Information of the Credit Approval Dataset

A1: b, a.
A2: continuous.
A3: continuous.
A4: u, y, l, t.
A5: g, p, gg.
A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ﬀ. A14: continuous.
A15: continuous.
A7: v, h, bb, j, n, z, dd, ﬀ, o.
A16: +,- (class attribute)
A8: continuous.

t, f.
A9:
A10: t, f.
A11: continuous.
A12: t, f.
A13: g, p, s.

Diﬀerent works may propose diﬀerent measures, but their philosophies are almost the same (as
stated above). For example, the measures for change of an input can be (i) removing the presence
of an input attribute, or (ii) permuting attribute values on an input attribute. The measures of
outputs are many, e.g., (i) accuracy of the (predicted) outputs [18, 38], (ii) probability of receiving a
certain outcome [23], (iii) statistics measures, such as partial dependence [41, 45], H-statistic [42],
or variable interaction networks [51], or (iv) a self-deﬁned quantity or a score/gain function. The
measures for the change of outputs can be (i) diﬀerence (i.e., subtraction), (ii) ratio, or (iii) averaged
diﬀerence/contribution, e.g., the Shapley value [56], of the measured outputs. In this regard, it is
impractical for us to demonstrate the privacy leakage issue for all present methods. However,
since the philosophies of all these methods are similar, it is reasonable for us to demonstrate the
privacy hacking procedures via a representative one. The principles of hacking procedures can be
transferred and applied to other methods.

We investigate potential privacy leakage via the quantitative input inﬂuence (QII) proposed in
the most pioneering work [23] in accountable ATR. For QII, the measure for change of an input is
permuting attribute values (called intervention in the paper) on an input attribute. The measure of
output can be user-speciﬁed, called quantity of interest, denoted by 𝑄. The measure for change of
output is diﬀerence between (subtraction of) two measured outputs. Formally, the QII of an input
attribute 𝑘 for a quantity of interest 𝑄 is deﬁned as

I 𝑄 (𝑘) = 𝑄 (𝑋 ) − 𝑄 (𝑋−𝑘𝑈𝑘 ),

(33)

in which 𝑋−𝑘𝑈𝑘 , meaning that attribute 𝑘 is (removed from input 𝑋 and) replaced by a permuted
version 𝑈𝑘, represents intervention on attribute 𝑘. In particular, for 𝑄 (𝑋 ) = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W },
the fraction of records belonging to a set 𝑇W (e.g., women) with positive classiﬁcation, the QII of
an input attribute 𝑘 is

I (𝑘) = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W } − 𝑃 {𝑐 (𝑋−𝑘𝑈𝑘 ) = 1|𝑋 ∈ 𝑇W },

(34)

where 𝑐 (·) is a classiﬁer (decision-maker). The QII of a set of input attributes K is deﬁned similarly,
using K instead of 𝑘.

In the following, we conduct an experiment to demonstrate the hacking of decision rules via
provided QII’s on an ATR for a real dataset, and utilize the hacked decision rules to further infer
private records as what we did in Section 3.2. We use the Australian credit approval dataset from
UCI machine learning repository [25] in our experiment10. The dataset has 690 instances, with
15 input attributes and 1 output attribute. All attribute information can be found in Table 5. In
order to protect conﬁdentiality of the data, all attribute names and values have been changed to

10Since we are demonstrating stealing private information from a real dataset, the chosen dataset needs to contain critical
information, and its size needs to be adequate: on the one hand, it should not be too large for ease of demonstration; on
the other hand, it should not be too small for the accuracy of the trained classiﬁer.

Achieving Transparency Report Privacy in Linear Time

35

Table 6. A Snapshot of the QID Group 𝑇xU={y, p, k, v}
A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 A14 A15 A16
y
y
y
y
y
y
y
y
y
y
y

160
0
224
0
200
0
280
0
140
4
70
200
216 2100
80
280
20
180
320
0
380 2732

v 0.125
0.25
v
0.29
v
v
1.25
v 0.125
v 0.125
v 0.085
v 0.415
2.5
v
0.25
v
0.25
v

p k
p k
p k
p k
p k
p k
p k
p k
p k
p k
p k

0
0
0
0
0
0
0
1
1
10
11

-
-
-
-
-
-
-
-
-
+
+

g
g
s
g
g
g
g
g
g
g
g

f
f
f
f
f
f
f
f
t
t
t

f
f
f
t
f
f
f
t
f
t
f

f
f
f
f
f
f
f
t
t
t
t

meaningless symbols by the dataset provider. Based on the dataset, with adequate data cleaning
and pre-processing, we train a classiﬁer based on a fully-connected neural network with one input
layer (36 inputs, after one-hot encoding for categorical attribute values), two hidden layers (147
and 85 neurons, respectively), and one output layer (binary outputs), with dropout rate 0.5. The
averaged testing accuracy of the trained classiﬁer is 89.5%.

The trained classiﬁer is served as the knowledge of a trust-worthy 3rd-party regulation agency
which feeds both inputs and outputs of the dataset to a ML model in order to learn the unknown
decision-making rules of this Australian credit card company. Since QII is a data-mining based
approach [23], the regulation agency provides information regarding input inﬂuences (QII) in an
ATR upon users’ demand. Since the access control is still an open question, we assume a user is
able to request such information in a reasonable manner.

Based on the above experimental settings, we ﬁrst construct a scenario to demonstrate the hack-

ing.

Scenario:
• Let U = {A4, A5, A6, A7} be public attributes and all other attributes are private and unknown

to adversaries (See Remark 6).

• Alice has public record xU = {y, p, k, v}. She gets a positive decision (+) and receives a credit

card.

• Tom also has the same public record xU = {y, p, k, v}. He gets a negative decision (-).
• An adversary is a friend of both, knowing their public records, knowing that Alice owns such
a credit card but Tom doesn’t. The adversary also has the knowledge of joint distribution of
A4∼A7, A9, and A11, e.g., demographic statistics of age, marriage status, race, and annual in-
come.

A snapshot of the QID group 𝑇xU ={y, p, k, v} is shown in Table 611, in which public attributes are
marked as grey, class attribute (decision outcome) is marked as light blue, and for those attributes
that an adversary has associated side-information (joint distribution) are marked as bold.

We next demonstrate privacy hacking procedures in the following. Let W0 ={A4=y, A5=p, A6=k,

A7=v, A11∈[0,1]}, and W1 ={A4=y, A5=p, A6=k, A7=v, A11∈[10,11]}.

Privacy Hacking:
(1) Since the input to QII can be a set of attributes, i.e., the joint inﬂuence of a set of input attributes.
Let S be the collection of all private attributes as denoted in Table 3, which is {A1∼A3, A8∼A15}
in our scenario. The adversary then sends the following QII query to the regulation agency:

11We remove attributes A1∼A3 in the interest of space

36

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

• Input Attribute: S
• Quantity of Interest: 𝑄 (𝑋 ) = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 }

(2) The adversary gets a response I (S) = 0.66475333, which indicates the degree of inﬂuence of

all private input attributes S to the group W1.

(3) The adversary sends the following QII query to the regulation agency:

• Input Attribute: S
• Quantity of Interest: 𝑄 (𝑋 ) = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W0 }

(4) The adversary gets a response I (S) = −0.33524666, which indicates the degree of inﬂuence
of all private input attributes S to the group W0. Note that negative sign stands for negative
impact as mentioned in Section 3.1.

(5) From the above two query responses, the adversary has

0.66475333 = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } − 𝑃 {𝑐 (𝑋−S𝑈 S) = 1|𝑋 ∈ 𝑇W1 }
−0.33524666 = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W0 } − 𝑃 {𝑐 (𝑋−S𝑈 S) = 1|𝑋 ∈ 𝑇W0 }
(6) Since W1 and W0 have the same public record xU = {y, p, k, v}, for the same classiﬁer, we must

have

𝑃 {𝑐 (𝑋−S𝑈 S) = 1|𝑋 ∈ 𝑇W1 } = 𝑃 {𝑐 (𝑋−S𝑈 S) = 1|𝑋 ∈ 𝑇W0 }.

(7) Utilize the above equality, the adversary obtains

𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } − 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W0 } = 1.

Since probabilities are always within [0, 1], the adversary thus obtains decision rules

𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } = 1,
𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W0 } = 0.

It is worth mentioning that the attack may not be unique. As shown in the following, there could
exist many ways to obtain decision rules, and thus it seems hopeless to cease the attack simply by
access control.

Privacy Hacking (Method 2):
(1) The adversary sends the following QII query to the regulation agency:

• Input Attribute: A9
• Quantity of Interest: 𝑄 (𝑋 ) = 𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 }

(2) The adversary gets a response I (A9) = 0.45142778, which indicates the degree of inﬂuence of

input attribute A9 to the group W1.

(3) The adversary analyzes the response I (A9).Deﬁne 𝑃𝑡 = 𝑃 {𝑐 (𝑋−A9𝑈A9) = 1|𝑋 ∈ 𝑇W1, 𝑈A9 = t}

and 𝑃𝑓 = 𝑃 {𝑐 (𝑋−A9𝑈A9) = 1|𝑋 ∈ 𝑇W1, 𝑈A9 = f}. He gets

0.45142778 =𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } − 𝑃 {𝑐 (𝑋−A9𝑈A9) = 1|𝑋 ∈ 𝑇W1 }
=𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } − 𝑃 {𝑈A9 = t}𝑃𝑡 − 𝑃 {𝑈A9 = f}𝑃𝑓

(4) The adversary realizes the fact that, for the same classiﬁer, we must have

𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } =𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1, 𝐴9 = t}

=𝑃 {𝑐 (𝑋−𝐴9𝑈𝐴9) = 1|𝑋 ∈ 𝑇W1, 𝑈𝐴9 = t} = 𝑃𝑡

(5) Since the adversary has joint distribution knowledge as mentioned in the scenario, he knows

the marginal distribution 𝑃 {𝑈𝐴9 = f} = 1 − 𝑃 {𝑈𝐴9 = t} = 0.45142857, he then gets

𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } − 𝑃𝑓 =

0.45142778
0.45142857

≈ 1

Achieving Transparency Report Privacy in Linear Time

37

(6) Since probabilities are always within [0, 1], the adversary knows 𝑃𝑓 ≈ 0, and

𝑃 {𝑐 (𝑋 ) = 1|𝑋 ∈ 𝑇W1 } ≈ 1.

The adversary obtains very accurate information regarding decision rule for W1.
Based on the hacked decision rules above, the adversary has 100% conﬁdence that Alice’s record
belongs to 𝑇W1 and Tom’s record belongs to 𝑇W0. Based on Table 6, he then knows that Alice’s
A11 attribute value is either 10 or 11, and Tom’s is either 0 or 1. If the adversary has richer side-
information, e.g., joint distribution including A8 and A14, then the adversary has 100% conﬁdence
that Alice’s A8 attribute value is 0.25, her A14 attribute value is in the range between 300 and 400,
and Tom’s A14 attribute value is in the range between 100 and 300.

It is worth mentioning that, based on our investigation, we do not ﬁnd a general attack method
that can be applied to all datasets and decision rules. However, this does not mean the attacks
demonstrated above are cherry-picked. As we have shown, there could exist many feasible attack
approaches. Adversaries can simply try multiple diﬀerent attempts and/or collude their test results
so that eventually acquire a successful attack result. Moreover, similar to the privacy incidents of
AOL search data leak [11] and de-anonymization of the Netﬂix Price dataset [67], although there is
no guarantee that the attacks can always succeed in all the cases, as long as the attack can succeed,
there exists a privacy breach which can result in a catastrophic disaster.

In fact, the authors of the pioneering work, i.e., [23], had already noticed the potential privacy
issue in algorithmic transparency and added noise to make the measures diﬀerentially private.
Unfortunately, adding diﬀerentially private noise [31] solely cannot mitigate the demonstrated
privacy leakage issue. The fundamental reason is that diﬀerential privacy only guarantees a small
amount of information leakage when an individual participates the survey or opts into a data-
base. Diﬀerential privacy itself does not guarantee information leakage due to strong statistical
inference between attributes; this has been noted in many previous works such as [7, 13, 29], and
section 2.3.2 in [32]. The most classic example is the study of “smoking causes cancer”, in which
no matter whether a person opts into the survey or not, once we know that he is a smoker, we
know he has a certain high chance of getting lung cancer. What can be guaranteed in the proposed
diﬀerentially private perturbation for an ATR is that an adversary can only gain very little infor-
mation by comparing two ATRs of which the training data to train the classiﬁers are diﬀer in only
one data subject’s record. When the size of dataset is very large, the required variance of DP noise
is very small. This is why they claimed only very little noise needs to be added.

Remark 6. Although all attribute names in the dataset are removed, we are still able to reason-
ably conjecture public and private attributes based on their inﬂuences to the decision outcome.
Attributes with high inﬂuences are more likely to be private attributes such as income or credit
score, and attributes with low inﬂuences are likely to be public ones. Observing that attribute A9,
A11, and A15 are the most inﬂuential ones and others are less signiﬁcant (from experiments). For
ease of demonstration, we choose 4 adjacent categorical attributes from insigniﬁcant ones, A4 to
A7, to serve as public attributes.

C MINIMUM UNCERTAINTY
Deﬁnition 12. (Minimum Uncertainty) Given an inference channel h𝑋 U, 𝐴 → 𝑋 Si, the uncertainty
of inferring a certain sensitive attribute value 𝑥 S from a certain inference source {xU, 𝑎} is deﬁned
as ucrt (xU, 𝑎 → 𝑥 S) = − log
. The minimum uncertainty of inferring any

conf (xU, 𝑎 → 𝑥 S)

(cid:0)

(cid:1)

38

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

sensitive value from any inference channel is

Ucrt(𝑋 U, 𝐴 → 𝑋 S) = min
xU,𝑎,𝑥S
= − log

{− log

conf (xU, 𝑎 → 𝑥 S)

}

(cid:0)
(cid:1)
{conf (xU, 𝑎 → 𝑥 S)}
max
xU,𝑎,𝑥S
Conf (𝑋 U, 𝐴 → 𝑋 S)

.

(cid:1)

= − log

(cid:0)

(cid:0)
Similarly, the corresponding privacy requirement for minimal uncertainty is the following.

(cid:1)

Deﬁnition 13. (𝛾-Minimum Uncertainty) In an algorithmic transparency report, ˜𝐷 satisﬁes 𝛾-Minimum
Uncertainty if Ucrt (𝑋 U, 𝐴 → 𝑋 S) ≥ 𝛾.

The above privacy requirement is basically saying that an adversary’s uncertainty on inferring
any sensitive value from any inference channel cannot be too low and should be lower-bounded
by a threshold 𝛾; the larger the 𝛾, the higher the minimum uncertainty, and thus the stronger
the privacy. From deﬁnition (12), it is clear that 𝛾-Minimum Uncertainty implies 𝑒−𝛾 -Maximum
Conﬁdence, and 𝛽-Maximum Conﬁdence implies − log 𝛽-Minimum Uncertainty.

Lemma 7. The privacy requirement 𝛾-Minimum Uncertainty imposes the following constraints to
the announced decision mapping ˜𝐷, ∀x ∈ R𝑋 , ∀𝑎 ∈ A,

log

˜𝐷𝑎 (x′)𝑃𝑋 (x′)

− log

˜𝐷𝑎 (x)𝑃𝑋 (x)

≥ 𝛾 .

(35)

(cid:0)Õx′

(cid:1)

(cid:0)

(cid:1)

D PROOF OF LEMMA 1

Proof. Recall that conf (xU, 𝑎 → 𝑥 S), the conﬁdence of inferring a sensitive attribute value 𝑥 S,

is a posterior epistemic probability which can be expressed as

conf (xU, 𝑎 → 𝑥 S) = ˜𝑃𝑋S |𝑋U,𝐴 (𝑥 S |xU, 𝑎) =

˜𝑃𝐴 |𝑋U,𝑋S (𝑎|xU, 𝑥 S)𝑃𝑋U,𝑋S (xU, 𝑥 S)

˜𝑃𝐴 |𝑋U ,𝑋S (𝑎|xU, 𝑥 ′

S)𝑃𝑋U,𝑋S (xU, 𝑥 ′
S)

.

(36)

𝑥 ′
S ∈R𝑋S
Í
, {x′ ∈ R𝑋 | x′
U = xU } to denote the tuple in which records

Let x = (xU, 𝑥 S) and deﬁne 𝑇xU
having the same QID xU. We have a more comprehensive expression

conf (xU, 𝑎 → 𝑥 S) =

˜𝑃𝐴 |𝑋 (𝑎|x)𝑃𝑋 (x)

˜𝑃𝐴 |𝑋 (𝑎|x′)𝑃𝑋 (x′)

=

˜𝐷𝑎 (x)𝑃𝑋 (x)

˜𝐷𝑎 (x′)𝑃𝑋 (x′)

.

(37)

x′ ∈𝑇xU
Í

x′ ∈𝑇xU
Í

Therefore, based on Deﬁnitions 3 and 4, the privacy requirement 𝛽-Maximum Conﬁdence imposes
the following constraints for all x = (xU, 𝑥 S) ∈ R𝑋 , ∀𝑎 ∈ A.

˜𝐷𝑎 (x)𝑃𝑋 (x)

˜𝐷𝑎 (x′)𝑃𝑋 (x′)

x′ ∈𝑇xU
Í

≤ 𝛽.

(38)

E PROOF OF LEMMA 4

Proof. We ﬁrst prove that if 𝛽 ≥ 𝐶∗, ˜𝐷 = 𝐷 is a feasible solution. We then prove its converse:

if ˜𝐷 = 𝐷 is a feasible solution, we must have 𝛽 ≥ 𝐶∗.

We ﬁrst prove that if 𝛽 ≥ 𝐶∗, the 1-ﬁdelity solution ˜𝐷 = 𝐷 is a feasible solution, i.e., it satisﬁes all
constraints. Obviously, the solution ˜𝐷 = 𝐷 satisﬁes probability distribution conditions and ﬁdelity

Achieving Transparency Report Privacy in Linear Time

39

constraints. Based on deﬁnition 7, ˜𝐷 = 𝐷 yields

𝑃 (x) ˜𝐷𝑎 (x)

𝑃 (x′) ˜𝐷𝑎 (x′)

x′ ∈𝑇xU
Í

= 𝐶∗ ≤ 𝛽 , ∀x ∈ 𝑇xU , ∀𝑎 ∈ A.

Therefore, it also satisﬁes privacy constraints, and hence when 𝛽 ≥ 𝐶∗, the 1-ﬁdelity solution is a
feasible solution.

Next, we prove the converse by proving its contrapositive, i.e., if 𝛽 < 𝐶∗, ˜𝐷 = 𝐷 is not a feasible
solution. Apparently when ˜𝐷 = 𝐷, the highest conﬁdence that an adversary can have exceeds 𝛽,
and hence it violates privacy requirements and cannot be a feasible solution. We therefore prove
(cid:3)
the converse.

F PROOF OF LEMMA 5

Proof. We ﬁrst prove that if an (OPT-Sub) has feasible solutions, 𝛽 ≥ 𝛽min. We then prove its

converse: if 𝛽 ≥ 𝛽min, an (OPT-Sub) must have feasible solutions.

We ﬁrst prove the conditional statement by proving its contrapositive, i.e., if 𝛽 < 𝛽min, there
exists no feasible solution for an (OPT-Sub). Since ˜𝐷 is non-negative, we can rewrite the privacy
constraints as follow.

𝑃 (x) ˜𝐷𝑎 (x) − 𝛽

𝑃 (x′) ˜𝐷𝑎 (x′) ≤ 0 ,

(39)

Õx′ ∈𝑇xU
which has to be satisﬁed ∀x ∈ 𝑇xU and ∀𝑎 ∈ A. Sum (39) over all 𝑎 ∈ A, by (EQ-Sub), we have

𝑃 (x) − 𝛽

𝑃 (x′) ≤ 0 , ∀x ∈ 𝑇xU ,

(40)

We then prove the converse. If 𝛽 ≥ maxx∈𝑇xU

Õx′ ∈𝑇xU
𝑃 (x|𝑇xU ). Therefore, if there exists any x ∈ 𝑇xU such that
which is equivalent to 𝛽 ≥ maxx∈𝑇xU
𝛽 < 𝑃 (x|𝑇xU ), then (39) cannot be satisﬁed for all x ∈ 𝑇xU , and hence no feasible solution exists.
𝑃 (x|𝑇xU ), there always exists a feasible solution
˜𝐷𝑎 (x′) = 1/|A|, ∀x ∈ 𝑇xU , ∀𝑎 ∈ A. To see this, we only need to verify if it satisﬁes all constraints.
It is very obvious that the solution satisﬁes probability distribution conditions. Since ﬁdelity con-
straints are trivialized, we then only need to verify if the solution satisﬁes privacy constraints.
Since ˜𝐷𝑎 (x′) is a constant for all 𝑎 and x, the left hand side of (39) becomes 𝑃 (x|𝑇xU ), and thus the
privacy constraints are also satisﬁed. Hence ˜𝐷𝑎 (x′) = 1/|A| is a feasible solution and we proved
(cid:3)
the converse.

G PROOF OF LEMMA 6

Proof. We prove this by contradiction. Assume that 𝐶∗ < 𝛽min. By their deﬁnitions in Lemma

4 and 5, it follows that

𝑃 (x)𝐷𝑎 (x)

𝑃 (x′)𝐷𝑎 (x′)

< max
x∈𝑇xU

𝑃 (x)

𝑃 (x′)

.

(41)

max
x∈𝑇xU ,
𝑎 ∈A

x′ ∈𝑇xU
Í

x′ ∈𝑇xU
Í

Let x† = arg maxx∈𝑇xU
If inequality (41) holds, the following inequalities must hold

𝑃 (x|𝑇xU ). The right hand side of (41) is equivalent to 𝑃 (x†)/

𝑃 (x′).

x′ ∈𝑇xU

Í

, ∀𝑎 ∈ A,

(42)

𝑃 (x†)𝐷𝑎 (x†)

𝑃 (x′)𝐷𝑎 (x′)

x′ ∈𝑇xU
Í

<

𝑃 (x†)

𝑃 (x′)

x′ ∈𝑇xU
Í

40

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

since the maximum of the left hand side of (42) over all 𝑎 ∈ A is not greater than the left hand side
of (41). Therefore, if there exists any 𝑎 ∈ A for which the corresponding inequality in (42) does
not hold, it implies our assumption 𝐶∗ < 𝛽min is not true, and, if so, we are done with the proof.

If there exists no such an 𝑎 and (42) holds, by eliminating 𝑃 (x†) from both sides of (42) and

cross-multiplying (as all terms are non-negative), (42) is equivalent to the following

𝐷𝑎 (x†)

𝑃 (x′) <

𝑃 (x′)𝐷𝑎 (x′) , ∀𝑎 ∈ A.

(43)

Õx′ ∈𝑇xU

Õx′ ∈𝑇xU
𝑃 (x′),
Sum (43) over 𝑎 ∈ A for both sides, based on (EQ-Sub), we obtain
which is obviously not true. Therefore, it implies the inequality (43) (and (42), equivalently) cannot
be true for all 𝑎 ∈ A, i.e., there must exist some 𝑎 for which the left hand side is not smaller than
the right hand side of (42), so that both sides are equal when summed over all 𝑎. Therefore, the
(cid:3)
initial assumption is incorrect and the lemma is proved.

𝑃 (x′) <

x′ ∈𝑇xU

x′ ∈𝑇xU

Í

Í

H PROOF OF THEOREM 1

For the convenience and conciseness of the proof, as long as there is no confusion, we abuse
some notations in this section and the following Appendix sections. All notations in the following
Appendix sections only follow their deﬁnitions in this section.

Recall that an optimization subproblem in (OPT-Sub) is formulated over a quasi-identiﬁer (QID)
group 𝑇xU in which all public records are equal to xU. Let 𝑚 = |𝑇xU | be the cardinality of the QID
group, or equivalently, the number of rows of this tuple. Let x𝑘 be the unique record of row 𝑘 in
the tuple, 𝑘 = 1, . . . , 𝑚, and deﬁne 𝑝𝑘 , 𝑃 (x𝑘 ), 𝑥𝑘 , ˜𝐷1 (x𝑘 ), and 𝑦𝑘 , ˜𝐷0 (x𝑘 ) = 1 −𝑥𝑘. The privacy
constraints can thus be re-written as

which can be combined as

𝑝𝑘𝑥𝑘
𝑚
𝑖=1 𝑝𝑖𝑥𝑖
𝑝𝑘𝑦𝑘
𝑚
𝑖=1 𝑝𝑖𝑦𝑖

Í

Í

𝑚

≤ 𝛽, ∀𝑘 = 1, . . . , 𝑚,

≤ 𝛽, ∀𝑘 = 1, . . . , 𝑚,

𝑚

𝑝𝑘 − 𝛽

𝑝𝑖 ≤ 𝑝𝑘𝑥𝑘 − 𝛽

𝑝𝑖𝑥𝑖 ≤ 0,

Õ𝑖=1

Õ𝑖=1

(44)

∀𝑘 = 1, . . . , 𝑚. Moreover, let x = [𝑥1, 𝑥2, · · · , 𝑥𝑚]𝑇 , where 𝑇 represents the transpose operator.
Deﬁne A as

A =

(1 − 𝛽)𝑝1
−𝛽𝑝1
...
−𝛽𝑝1

©

−𝛽𝑝2
(1 − 𝛽)𝑝2
...
−𝛽𝑝2

· · ·
· · ·
. . .
· · ·

−𝛽𝑝𝑚
−𝛽𝑝𝑚
...
(1 − 𝛽)𝑝𝑚

,

(45)

ª
®
®
®
®
¬

and let b = [𝑏1, 𝑏2, · · · , 𝑏𝑚]𝑇 , in which 𝑏𝑘 = 𝑝𝑘 − 𝛽
constraints as

«

𝑚
𝑖=1 𝑝𝑖 . We can further simplify the privacy

Í
b (cid:22) Ax (cid:22) 0,

(46)

where 0 is an 𝑚 × 1 zero vector.

Remark 7. Note that 𝑏𝑘 = 𝑝𝑘 − 𝛽

𝑚
𝑖=1 𝑝𝑖 ≤ 0 due to Lemma 5, or (40), equivalently.

Í

 
 
 
 
Achieving Transparency Report Privacy in Linear Time

41

Similarly, the ﬁdelity constraints can be re-written as

𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, ∀𝑘 = 1, . . . , 𝑚,
𝑦𝑘 min ≤ 𝑦𝑘 ≤ 𝑦𝑘 max, ∀𝑘 = 1, . . . , 𝑚.
However, since for binary decision, 𝑦𝑘 = 1 − 𝑥𝑘 , the above two constraints are basically equivalent
(to see this, simply let 𝑦𝑘 min = 1 − 𝑥𝑘 max and 𝑦𝑘 max = 1 − 𝑥𝑘 min), so we obtain the following ﬁdelity
constraints

𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, ∀𝑘 = 1, . . . , 𝑚.
Note that the 2𝑚 privacy constraints in (44) (or their equivalent vectorized form in (46)) form(s) a
parallelotope in the 𝑚-dimensional space, and the 2𝑚 ﬁdelity constraints in (47) form a hypercube
in the 𝑚-dimensional space. Let P denote the parallelotope and H denote the hypercube. Moreover,
deﬁne I , P
H be the intersection of P and H . I = ∅ if and only if P and H are disjoint,
where ∅ denotes the empty set. We have the following fact.

(47)

Fact 1. An optimization subproblem has feasible solutions if and only if I ≠ ∅, i.e., P and H
intersect/collide with each other.

Ñ

To prove Theorem 1, based on the above fact, it is hence equivalent to show that P and H
, max{𝛽0, 𝛽1, 𝛽𝑝 }. Let 𝜋 , arg max𝑘𝑝𝑘𝑥𝑘 min and

collide with each other if and only if 𝛽 ≥ 𝛽*
𝜃 , arg max𝑘𝑝𝑘𝑦𝑘 min, we can re-write 𝛽0, 𝛽1, and 𝛽𝑝 in the following

𝑇xU

𝛽0 =

𝛽1 =

𝛽𝑝 =

,

,

𝑝𝜃𝑦𝜃 min +

𝑝𝑖𝑦𝑖 max′

𝑝𝜃𝑦𝜃 min
𝑚
𝑖=1
𝑖≠𝜃
𝑝𝜋 𝑥𝜋 min
Í

𝑝𝑖𝑥𝑖 max′

𝑚
𝑖=1
𝑖≠𝜋

𝑝𝜋 𝑥𝜋 min +
𝑝𝜋 𝑥𝜋 min + 𝑝𝜃𝑦𝜃 min
Í
𝑚
𝑖=1 𝑝𝑖

,

where

Í

𝑥𝑖 max′ , min{𝑥𝑖 max,

𝑦𝑖 max′ , min{𝑦𝑖 max,

𝑝𝜋
𝑝𝑖
𝑝𝜃
𝑝𝑖

𝑥𝜋 min},

𝑦𝜃 min}.

(48)

(49)

(50)

(51)

(52)

Consider the following two optimization problems for 𝑥 𝑗 , where 𝑗 is an arbitrary index, 1 ≤ 𝑗 ≤

𝑚:

minimize

𝑥 𝑗

s.t. b (cid:22) Ax (cid:22) 0,

𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, for 𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 .

maximize

s.t.

𝑥 𝑗
b (cid:22) Ax (cid:22) 0,
𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, for 𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 .

(OPT-1)

(OPT-2)

The above two problems have exactly the same constraints. The ﬁrst line constraint forms the
parallelotope P, and let H ′
𝑗 denote the hypercube formed by the second line constraints, i.e.,
𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, for 𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 . In addition, deﬁne I ′
𝑗 be the intersection of
𝑗

, P

H ′

Ñ

42

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

P and H ′
tion problems. Moreover, if I ′
𝑗
let 𝑥 †
the following lemma.

𝑗 and 𝑥 ‡

𝑗 , interpreting the geometric space formed by the constraints of the above two optimiza-
≠ ∅, (i.e., there exist feasible solutions for (OPT-1) and (OPT-2)), we
𝑗 denote the optimal objective values of (OPT-1) and (OPT-2), respectively. We have

𝑗

𝑘

If I ′
Lemma 8.
𝑘
> 𝑥 𝑗 max or 𝑥 ‡
𝑥 †
𝑗
𝑥 †
𝑘 ≤ 𝑥𝑘 max, and 𝑥 ‡
Proof. Apparently, since H = H ′
𝑗
implies if there exists any 𝑗 such that I′
𝑗
Ñ
for every 𝑗 , if x ∉ I′

≠ ∅ for all 𝑘 = 1, · · · , 𝑚, P and H are disjoint (I = ∅) if and only if either
< 𝑥 𝑗 min. In other words, P and H collide with each other if and only if I ′
≠ ∅,
𝑘 ≥ 𝑥𝑘 min, ∀𝑘 = 1, · · · , 𝑚.
𝑘 for any 𝑘 ≠ 𝑗 , we have I ⊆ I′
H ′
𝑗 true for any 𝑗 , which
= ∅, I = ∅, and P and H must be disjoint. Since I ⊆ I′
𝑗

𝑗 for any 𝑗 , then x ∉ I. Moreover, for any point x ∈ I ′

𝑗 ≤ 𝑥 𝑗 ≤ 𝑥 ‡
𝑗 .
< 𝑥 𝑗 min, since for any x ∈ I′
𝑗 ,
𝑗 , which implies either 𝑥 𝑗 < 𝑥 𝑗 min or 𝑥 𝑗 > 𝑥 𝑗 max, and thus either I = ∅, or I * I′

If I ′
𝑘
𝑗 ≤ 𝑥 𝑗 ≤ 𝑥 ‡
𝑥 †
(which violates the truth). Therefore, P and H are disjoint.
𝑘 ≤ 𝑥𝑘 max, and 𝑥 ‡

𝑘 ≥ 𝑥𝑘 min are true for all 𝑘 = 1, · · · , 𝑚,
since for any x ∈ I ′
𝑘 , we have 𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, ∀𝑘, which implies
x ∈ I, so that I ≠ ∅, P and H collide with each other. We thus prove the converse and the proof
(cid:3)
is done.

≠ ∅ for all 𝑘 = 1, · · · , 𝑚, and either 𝑥 †
𝑗

We next prove the converse. If I ′
𝑘

≠ ∅, 𝑥 †
𝑘 ≤ 𝑥𝑘 ≤ 𝑥 ‡

𝑘 , ∀𝑘 = 1, · · · , 𝑚, 𝑥 †

> 𝑥 𝑗 max or 𝑥 ‡

𝑗 , 𝑥 †

𝑗

𝑗

Based on Fact 1 and Lemma 8, following statements are equivalent.

(S1) An optimization sub-problem has feasible solutions.

⇐⇒ (S2) P and H intersect/collide with each other.
⇐⇒ (S3) (OPT-1) and (OPT-2) have feasible solutions for all 𝑗 .
⇐⇒ (S4) I′

𝑗 ≥ 𝑥 𝑗 min, ∀𝑗 = 1, · · · , 𝑚.

𝑗 ≤ 𝑥 𝑗 max and 𝑥 ‡

𝑗 ≠ ∅, 𝑥 †

Our next goal is to show that (S1)∼(S4) are true if and only if 𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 }. To show this,

we need the following lemma.

Lemma 9. Consider the optimization problem (OPT-1) for some (arbitrary) 𝑗 . If (S1)∼(S4) are true,
we have 𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 𝑗 }, where

𝛽0 =

𝛽1 =

=

𝛽𝑝 𝑗

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃
𝑝𝜋 𝑥𝜋 min
Í
𝑚
𝑘=1
𝑘≠𝜋

𝑝𝜃𝑦𝜃 min +

𝑝𝑘𝑦𝑘 max′

𝑝𝜋 𝑥𝜋 min +

𝑝𝑘𝑥𝑘 max′

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
Í
𝑚
𝑘=1 𝑝𝑘

.

,

,

(53)

(54)

(55)

For each of the above cases, i.e., 𝛽 = 𝛽0, 𝛽1, or 𝛽𝑝 𝑗 , the corresponding optimal objective value 𝑥 †

𝑗

and its corresponding optimal solutions are

Í

Achieving Transparency Report Privacy in Linear Time

43

𝛽 = 𝛽0 ⇐⇒ 𝑥 †
𝑗

=

1
𝑝 𝑗

𝑝 𝑗 −
n

𝛽
1 − 𝛽

𝑚

Õ𝑘=1
𝑘≠𝑗

𝑝𝑘𝑦𝑘 max′

, 𝑥 †0
𝑗

o

⇐⇒ 𝑦 𝑗 = 𝑦𝜃 = 𝑦𝜃 min

𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃 ,

𝛽 = 𝛽1 ⇐⇒ 𝑥 †
𝑗

=

1 − 𝛽
𝛽

1
𝑝 𝑗

n

𝑝𝜋 𝑥𝜋 min −

𝑚

Õ𝑘=1
𝑘≠𝑗,𝜋

𝑝𝑘𝑥𝑘 max′

, 𝑥 †1
𝑗

o

⇐⇒ 𝑥𝜋 = 𝑥𝜋 min

𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗, 𝜋,

𝛽 = 𝛽𝑝 𝑗 ⇐⇒ 𝑥 †

𝑗

=

1
𝑝 𝑗

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 − 𝛽
n

𝑚

𝑝𝑘

, 𝑥 †𝑝
𝑗

Õ𝑘=1

o

⇐⇒ 𝑥𝜋 = 𝑥𝜋 min
𝑚

𝑝𝑘𝑥𝑘 =

Õ𝑘=1
𝑘≠𝑗,𝜋

1 − 2𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑝 𝑗 + 𝛽

𝑝𝑘 .

𝑚

Õ𝑘=1

Proof. Please refer to Appendix I for the proofs.

(cid:3)

When (S1)∼(S4) are true, I ′
𝑗

≠ ∅ and 𝑥 †
Lemma 9, it implies that 𝛽 ≥ 𝛽0, 𝛽 ≥ 𝛽1, and

𝑗 ≤ 𝑥 𝑗 max need to be met for all 𝑗 = 1, . . . , 𝑚. Based on

𝛽 ≥ 𝛽𝑝 𝑗

=

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
𝑚
𝑘=1 𝑝𝑘

, ∀𝑗 = 1, . . . , 𝑚,

Í

which is equivalent to

𝛽 ≥ max

𝑗

𝑝𝜋𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
𝑚
𝑘=1 𝑝𝑘

=

𝑝𝜋 𝑥𝜋 min + 𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1 𝑝𝑘

= 𝛽𝑝 .

Í

Í

We then obtain 𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 } = 𝛽*
𝑦 𝑗 = 𝑦𝜃 = 𝑦𝜃 min. Based on (54) and (55), we have

𝑇xU

. Since when 𝛽*

= 𝛽0, based on Lemma 9, we have

𝑇xU

𝛽*
𝑇xU
𝛽*
𝑇xU

= 𝛽1 ⇐⇒ 𝑥 𝑗 = 𝑥 𝑗 max′,
= 𝛽𝑝 ⇐⇒ 𝑦 𝑗 = 𝑦𝜃 = 𝑦𝜃 min.

44

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Combining the above results with Lemma 9, we thus have

𝛽*
𝑇xU

𝛽*
𝑇xU

𝛽*
𝑇xU

= 𝛽0 ⇐⇒ 𝑦𝜃 = 𝑦𝜃 min

𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃

= 𝛽1 ⇐⇒ 𝑥𝜋 = 𝑥𝜋 min

𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜋

= 𝛽𝑝 ⇐⇒ 𝑥𝜋 = 𝑥𝜋 min
𝑦𝜃 = 𝑦𝜃 min
1 − 2𝛽
𝛽

𝑝𝑘𝑥𝑘 =

𝑚

Õ𝑘=1
𝑘≠𝜃,𝜋

𝑝𝜋 𝑥𝜋 min − 𝑝 𝑗 + 𝛽

𝑝𝑘 .

𝑚

Õ𝑘=1

Similarly, if (S1)∼(S4) are true, I′
𝑗

𝑗 ≥ 𝑥 𝑗 min need to be met for all 𝑗 = 1, . . . , 𝑚. By
letting 𝑦𝑘 = 1 − 𝑥𝑘 , 𝑦𝑘 min = 1 − 𝑥𝑘 max, and 𝑦𝑘 max = 1 − 𝑥𝑘 min, the optimization problem (OPT-2) is
essentially equivalent to the following optimization problem:

≠ ∅ and 𝑥 ‡

minimize 𝑦 𝑗

s.t. b (cid:22) Ay (cid:22) 0,

𝑦𝑘 min ≤ 𝑦𝑘 ≤ 𝑦𝑘 max, for 𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 .

(OPT-3)

𝑗 be the optimal objective value of the above optimization problem. Clearly, 𝑦†
𝑗 ≥ 𝑥 𝑗 min is equivalent to 𝑦†

Let 𝑦†
Therefore, for all 𝑗 = 1, . . . , 𝑚, 𝑥 ‡
in Lemma 9, we will obtain exactly the same conditions for 𝛽, i.e., 𝛽 ≥ 𝛽*
are true, we have 𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 }.

= 1 − 𝑥 ‡
𝑗 .
𝑗 ≤ 𝑦 𝑗 max. By applying results from 𝑥 †
𝑗
. Therefore, if (S1)∼(S4)

𝑇xU

𝑗

𝑗 ≤ 𝑦 𝑗 max, which is equivalent to 𝑥 ‡

We next prove the converse. If 𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 }, which, based on (53), (54), and (55), implies
𝑗 ≤ 𝑥 𝑗 max and 𝑦†
𝑥 †
𝑗 ≥ 𝑥 𝑗 min, ∀𝑗 = 1, · · · , 𝑚. Therefore, based
on Lemma 9, 𝑥 𝑗 is feasible, which implies I ′
≠ ∅, ∀𝑗 = 1, · · · , 𝑚, and thus (S4) is true. Since
𝑗
(S1)∼(S4) are equivalent, an optimization sub-problem has feasible solutions if and only if 𝛽 ≥
𝛽*
𝑇xU

= max{𝛽0, 𝛽1, 𝛽𝑝 }. We thus ﬁnish the proof.

I PROOF OF LEMMA 9
Here we demonstrate the proof of Lemma 9, which shows the optimal objective value of the opti-
mization problem (OPT-1).

H ′

If I ′
𝑗
= P

𝑗 , any x ∈ I ′

≠ ∅, there exists (at least one or some) x ∈ I′

𝑗 . Since
I ′
𝑗 also belongs to P and H ′. Since P is a 𝑚-dimensional parallelotope,
𝑗
and 0 ∈ P is a vertex of P, any point x ∈ P can be uniquely represented by a linear combination of
𝑚
𝑚 linear independent edge vectors emitted from 0, denoted by L𝑘, 𝑘 = 1, . . . , 𝑚, and x =
𝑘=1 𝛼𝑘 L𝑘 ,
0 ≤ 𝛼𝑘 ≤ 1, ∀𝑘 = 1, . . . , 𝑚. Let L be the collection of these 𝑚 vectors; speciﬁcally, L , [L1L2 · · · L𝑚],
where L𝑘 is an 𝑚 × 1 column vector and L is an 𝑚 × 𝑚 matrix. L can be obtained by

𝑗 , and for all x, 𝑥 †

𝑗 ≤ 𝑥 𝑗 ≤ 𝑥 ‡

Ñ

Í

L = A−1B,

(56)

where A is deﬁned in (45) and B = dg(b) where dg(b) denotes a diagonal matrix with elements of
b = (𝑏1, 𝑏2, · · · , 𝑏𝑚) along the diagonal. To ﬁnd A−1, note that since A can be represented by

A = dg(p) + (−𝛽)1𝑚p𝑇 ,

(57)

Achieving Transparency Report Privacy in Linear Time

45

where p = [𝑝1, 𝑝2, · · · , 𝑝𝑚]𝑇 and 1𝑚 is an all-one vector with 𝑚 elements, we can thus apply the
following matrix inversion formula [48]

(Z + 𝑐uv𝑇 )−1 = Z−1 −

1
1 + 𝑐v𝑇 Z−1u

Z−1uv𝑇 Z−1

(58)

to compute A−1 and obtain L as follow

L =

1
1 − 𝑚𝛽 ©

𝑏1
𝑝1 [1−(𝑚−1)𝛽 ]
𝑏1
𝑝2 𝛽
...

𝑏1
𝑝𝑚

𝛽

𝑏2
𝑝1 𝛽

· · ·
𝑏2
𝑝2 [1−(𝑚−1)𝛽 ] · · ·
...
. . .
· · · 𝑏𝑚
𝑝𝑚

𝑏2
𝑝𝑚

𝛽

𝑏𝑚
𝑝1 𝛽
𝑏𝑚
𝑝2 𝛽
...
[1−(𝑚−1)𝛽 ]

.

(59)

ª
®
®
®
®
®
¬

, ( 𝑏1

Deﬁne b
p
«
to see that L can be represented as the following equivalent form

𝑝2 , · · · , 𝑏𝑚
𝑝𝑚

𝑝1 , 𝑏2

) as the element-wise division operation of two vectors. It is not hard

L = dg

b
p

+

𝛽
1 − 𝑚𝛽

1
p

b𝑇 ,

(60)

(cid:16)

(cid:17)

which implies that its inverse can also be found by applying the matrix inversion formula in (58).
We will utilize this property in the later of the proof.

Recall that if x ∈ I ′

𝑗 , x ∈ P as well. Therefore, any x ∈ I′

𝑗 can be uniquely represented by

𝑚

x =

𝛼𝑘 L𝑘 ,

(61)

Õ𝑘=1
in which 0 ≤ 𝛼𝑘 ≤ 1, ∀𝑘 = 1, . . . , 𝑚. Recall that we are solving the optimization problem (OPT-1)
for some 𝑗 , 1 ≤ 𝑗 ≤ 𝑚. We ﬁrst take out the 𝑗 -th row from (61),

𝑚

𝑥 𝑗 =

𝛼𝑘𝐿𝑘,𝑗 ,

(62)

Õ𝑘=1
and for the rest 𝑚 − 1 equalities, we move the term 𝛼 𝑗 L𝑗 from the right-hand-side (RHS) to the
left-hand-side (LHS). We then obtain

=

− 𝛼 𝑗

𝐿1,1
𝐿2,1
...

𝐿1,2
𝐿2,2
...

𝐿1,𝑚
𝐿2,𝑚
...

𝐿1,𝑗−1
𝐿2,𝑗−1
...

𝑥1
𝑥2
...



𝑥 𝑗−1


𝑥 𝑗+1


...



𝑥𝑚







· · ·
· · ·
. . .
· · · 𝐿𝑗−1,𝑗−1
· · · 𝐿𝑗+1,𝑗−1
. . .
· · ·

𝐿1,𝑗
𝐿2,𝑗
...



𝐿𝑗−1,𝑗


𝐿𝑗+1,𝑗


...



𝐿𝑚,𝑗



′
= Lsub𝜶 ′ be the corresponding vector form of (63), in which, based on (59),
and let x′ − 𝛼 𝑗 L


𝑗

𝐿𝑘,𝑘 = 1−(𝑚−1)𝛽
, ∀𝑘 = 1, · · · , 𝑚, and 𝐿𝑘,𝑖 = 𝛽
𝑏𝑘
, ∀𝑘, 𝑖 = 1, · · · , 𝑚, 𝑘 ≠ 𝑖. Note that Lsub is an
1−𝑚𝛽
𝑝𝑘
(𝑚 − 1) × (𝑚 − 1) square sub-matrix of L by removing the 𝑗 -th row and the 𝑗 -th column. Therefore,
it has the similar form as shown in (60) by removing the 𝑗 -th row/element of b and p, and thus its
inverse L−1




𝐿𝑗−1,1 𝐿𝑗−1,2


𝐿𝑗+1,2
𝐿𝑗+1,1


...
...



𝐿𝑚,2
𝐿𝑚,1







· · ·
· · ·
. . .
· · · 𝐿𝑗−1,𝑚
· · · 𝐿𝑗+1,𝑚
. . .
· · ·

𝐿1,𝑗+1
𝐿2,𝑗+1
...
𝐿𝑗−1,𝑗+1
𝐿𝑗+1,𝑗+1
...
𝐿𝑚,𝑗+1

𝛼1
𝛼2
...



𝛼 𝑗−1


𝛼 𝑗+1


...



𝛼𝑚























1−𝑚𝛽

sub can also be found by (58). By applying L−1

sub to both sides of (63), we have

...
𝐿𝑚,𝑗−1

...
𝐿𝑚,𝑚




















































(63)

𝑏𝑖
𝑝𝑘

,

𝜶 ′ = L−1

subx′ − 𝛼 𝑗 L−1

subL

′

𝑗 .

(64)

 
 
 
 
 
46

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Let u , L−1

subx′ and v , 𝛼 𝑗 L−1

subL

′

𝑗 , we obtain

𝑢𝑘 =

1
1 − 𝛽

1
𝑏𝑘

h

𝑚

(1 − 𝛽)𝑝𝑘𝑥𝑘 − 𝛽

𝑝𝑖𝑥𝑖

,

Õ𝑖=1
𝑖≠𝑗

i

𝑣𝑘 =

𝛽
1 − 𝛽

1
𝑏𝑘

𝛼 𝑗𝑏 𝑗 ,

𝛼𝑘 = 𝑢𝑘 − 𝑣𝑘 =

𝛽
1 − 𝛽

1
𝑏𝑘

h

for all 𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗 . Similarly, if we deﬁne 𝛼𝑘

′ =

𝛼𝑘

𝛽
1 − 𝛽

1
𝑏𝑘

1 − 𝛽
𝛽

h

Substituting the 𝛼𝑘 in (65) into (62), we obtain

𝑚

𝑥 𝑗 =

𝛼𝑘𝐿𝑘,𝑗 =

Õ𝑘=1

1
1 − 𝛽

1
𝑝 𝑗

1 − 𝛽
𝛽

𝑚

𝑝𝑘𝑥𝑘 −

𝑝𝑖𝑥𝑖 − 𝛼 𝑗𝑏 𝑗

,

Õ𝑖=1
𝑖≠𝑗
′ , 1 − 𝛼𝑘 , we have

i

𝑚

𝑝𝑘𝑦𝑘 −

𝑝𝑖𝑦𝑖 − 𝛼 𝑗

′𝑏 𝑗

.

i

Õ𝑖=1
𝑖≠𝑗

𝑚

𝛽

𝑝𝑘𝑥𝑘 + 𝛼 𝑗𝑏 𝑗

.

h

Õ𝑘=1
𝑘≠𝑗

i

(65)

(66)

(67)

Based on (67), we are looking for the values of 𝛼 𝑗 and 𝑥𝑘 ’s (𝑘 ≠ 𝑗 ) yielding the minimum of 𝑥 𝑗 .

Since 0 ≤ 𝛼𝑘 ≤ 1, ∀𝑘 = 1, . . . , 𝑚, which implies 𝑢𝑘 ≥ 𝑣𝑘 , ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 , and

We then have

𝑢𝑘
𝑣𝑘

=

1
𝛽𝛼 𝑗𝑏 𝑗

h

𝑚

(1 − 𝛽)𝑝𝑘𝑥𝑘 − 𝛽

𝑝𝑖𝑥𝑖

≥ 1.

Õ𝑖=1
𝑖≠𝑗

i

𝛼 𝑗 ≤

1
𝛽𝑏 𝑗

h

𝑚

(1 − 𝛽)𝑝𝑘𝑥𝑘 − 𝛽

𝑝𝑖𝑥𝑖

, 𝑅𝑘
𝑗 ,

Õ𝑖=1
𝑖≠𝑗

i

for all 𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝑗 . Let 𝑅∗
𝑗

, min
𝑘

𝑅𝑘
𝑗 . Combining with the fact that 𝛼 𝑗 ≤ 1, we obtain

𝛼 𝑗 ≤ min(𝑅∗

𝑗 , 1).

(68)

(69)

(70)

Case 1:
We ﬁrst consider the case 𝑅∗

𝑗 ≤ 1. Please refer to Case 2 for 𝑅∗

𝑗 ≥ 1.

When 𝑅∗

𝑗 ≤ 1, based on (70), we have 𝛼 𝑗 = 𝑅∗

𝑗 . Therefore, based on (69), there must exist an 𝑘

(denoted by 𝜅) such that

𝛼 𝑗 =

1
𝛽𝑏 𝑗

h

𝑚

(1 − 𝛽)𝑝𝜅𝑥𝜅 − 𝛽

𝑝𝑖𝑥𝑖

= 𝑅∗

𝑗 ≤ 1.

Õ𝑖=1
𝑖≠𝑗

i

Because 𝑏 𝑗 ≤ 0 (see Remark 7), from the LHS of (71), we have

𝑚

𝛽

𝑝𝑖𝑥𝑖 + 𝛼 𝑗𝑏 𝑗 = (1 − 𝛽)

Õ𝑖=1
𝑖≠𝑗

1
𝛽

h

𝑚

𝑝𝜅𝑥𝜅 −

𝑝𝑖𝑥𝑖

.

Õ𝑖=1
𝑖≠𝑗

i

(71)

(72)

Achieving Transparency Report Privacy in Linear Time

Substitute the LHS of (72) into (67). We obtain

or equivalently,

𝑝 𝑗𝑥 𝑗 =

1
𝛽

𝑚

𝑝𝜅𝑥𝜅 −

𝑝𝑖𝑥𝑖 =

Õ𝑘=1
𝑘≠𝑗

1 − 𝛽
𝛽

𝑚

𝑝𝜅𝑥𝜅 −

𝑝𝑖𝑥𝑖,

Õ𝑘=1
𝑘≠𝑗,𝜅

𝛽 =

𝑝𝜅𝑥𝜅
𝑚
𝑘=1𝑝𝑘𝑥𝑘

=

𝑝𝜅𝑥𝜅 +

𝑝𝜅𝑥𝜅
𝑚
𝑘=1
𝑘≠𝜅

.

𝑝𝑘𝑥𝑘

47

(73)

(74)

Therefore, 𝑥 𝑗 is minimized if the RHS of (73) is minimized. In addition, in Case 1, based on (74), 𝛽
achieves its minimum when (i) minimizing 𝑝𝜅𝑥𝜅 and (ii) maximizing

𝑚
𝑘=1
𝑘≠𝜅
We next ﬁnd the minimum of the RHS of (73). Since from (71), we have

𝑝𝑘𝑥𝑘 .

Í

Í

Í

and from (73), we have

𝑚

𝑚

𝑝𝑖𝑥𝑖 =

𝑝𝑘𝑥𝑘 =

Õ𝑖=1
𝑖≠𝑗

Õ𝑘=1
𝑘≠𝑗

1 − 𝛽
𝛽

𝑝𝜅𝑥𝜅 − 𝛼 𝑗𝑏 𝑗,

𝑚

𝛽

𝑝𝑘𝑥𝑘 = 𝑝𝜅𝑥𝜅 − 𝛽𝑝 𝑗𝑥 𝑗 .

Õ𝑘=1
𝑘≠𝑗

Substituting the LHS of (75) into (65), we obtain

𝛼𝑘 =

1
𝑏𝑘

(𝑝𝑘𝑥𝑘 − 𝑝𝜅𝑥𝜅), ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗,

and substituting the LHS of (75) into (76), we obtain

Combining (77) and (78), we have

𝛼 𝑗 =

1
𝑏 𝑗

(𝑝 𝑗𝑥 𝑗 − 𝑝𝜅𝑥𝜅 ).

(75)

(76)

(77)

(78)

𝛼𝑘 =

1
𝑏𝑘
Since 𝛼𝑘 ≥ 0 and 𝑏𝑘 ≤ 0, we have 𝑝𝜅𝑥𝜅 ≥ 𝑝𝑘𝑥𝑘, ∀𝑘, i.e., 𝑝𝜅𝑥𝜅 = max
𝑝𝑘𝑥𝑘 . Moreover, from (73),
since 𝛽 is non-negative and 𝑥𝑘 ≥ 0 for all 𝑘, in order to minimize 𝑥 𝑗 , we need to (i) minimize 𝑝𝜅𝑥𝜅
𝑝𝑘𝑥𝑘 . Note that both (i) and (ii) minimize 𝛽 as well, which implies that the
and (ii) maximize

(𝑝𝑘𝑥𝑘 − 𝑝𝜅𝑥𝜅), ∀𝑘 = 1, · · · , 𝑚.

(79)

𝑘

𝑚
𝑘=1
𝑘≠𝑗,𝜅

optimal solutions that minimize 𝑥 𝑗 also minimize 𝛽.

Í

To minimize 𝑝𝜅𝑥𝜅, since 𝑝𝜅𝑥𝜅 = max

𝑝𝑘𝑥𝑘 , and 𝑝𝑘𝑥𝑘 ≥ 𝑝𝑘𝑥𝑘 min, ∀𝑘 (including 𝜅), the minimal
𝑝𝜅𝑥𝜅 , i.e., 𝑝𝜅𝑥𝜅 min, is therefore the largest eﬀective lower limit 𝑝𝑘𝑥𝑘 min over all 𝑘, i.e., 𝑝𝜅𝑥𝜅 min =
max
𝑘
To maximize

𝑝𝑘𝑥𝑘 min = 𝑝𝜋 𝑥𝜋 min by deﬁnition, and thus we get 𝜅 = 𝜋 and 𝑥𝜋 = 𝑥𝜋 min.

𝑝𝑘𝑥𝑘 , we need to ﬁnd the maximum of each 𝑥𝑘. Since 0 ≤ 𝛼𝑘 ≤ 1, by

𝑘

Í

substituting 𝑝𝜅𝑥𝜅 = 𝑝𝜋 𝑥𝜋 min into (79), we have 𝑝𝜋 𝑥𝜋 min + 𝑏𝑘 ≤ 𝑝𝑘𝑥𝑘 ≤ 𝑝𝜋 𝑥𝜋 min, ∀𝑘 = 1, · · · , 𝑚
(including 𝑗 ). By deﬁnitions, 𝑥𝑘 max′ , min{𝑥𝑘 max, 𝑝𝜋
𝑥𝜋 min +𝑏𝑘 }.
𝑝𝑘
Combining with the constraints 𝑥𝑘 min ≤ 𝑥𝑘 ≤ 𝑥𝑘 max, we obtain 𝑥𝑘 min′ ≤ 𝑥𝑘 ≤ 𝑥𝑘 max′, ∀𝑘 =
𝑝𝑘𝑥𝑘 is maximized when 𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘, 𝑘 ≠ 𝑗, 𝜋.
1, · · · , 𝑚, and thus

𝑥𝜋 min} and 𝑥𝑘 min′ , max{𝑥𝑘 min, 𝑝𝜋
𝑝𝑘

𝑚
𝑘=1
𝑘≠𝑗,𝜅

𝑚
𝑘=1
𝑘≠𝑗,𝜅

Í

48

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

By substituting the 𝑥𝑘 ’s we obtained above into (73), based on which, the optimal objective value
𝑥 †
𝑗 , the minimum of 𝑥 𝑗 , is thus

𝑥 †
𝑗

=

1 − 𝛽
𝛽

1
𝑝 𝑗

n

𝑝𝜋 𝑥𝜋 min −

𝑚

Õ𝑘=1
𝑘≠𝑗,𝜋

𝑝𝑖𝑥𝑖 max′

, 𝑥 †1
𝑗

.

o

(80)

If (S1)∼(S4) are true, 𝑥 †
𝑝𝜋 𝑥𝜋 min, and thus from (51), we get 𝑥 𝑗 max
that (OPT-1) has feasible solution, for Case 1, is

𝑗 ≤ 𝑥 𝑗 max. In addition, based on (78), we have 𝑝 𝑗𝑥 𝑗 = 𝑝𝜋 𝑥𝜋 min + 𝛼 𝑗𝑏 𝑗 ≤
= 𝑥 𝑗 max′. Therefore, based on (80), the minimum 𝛽 such

𝛽 =

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 𝑥 †

𝑝𝜋 𝑥𝜋 min
𝑚
𝑗 +
𝑘=1
𝑘≠𝑗,𝜋

𝑝𝑘𝑥𝑘 max′

𝑝𝜋 𝑥𝜋 min
𝑚
𝑘=1
𝑘≠𝜋
We next prove the converse. If (81) and (80) holds, i.e., 𝑥𝜅 = 𝑥𝜋 = 𝑥𝜋 min, and 𝑥𝑘 = 𝑥𝑘 max′,

Í
∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗, 𝜋, since 𝑥𝜋 min = 𝑥𝜋 max′, we have 𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗 , and

Í
𝑝𝑘𝑥𝑘 max′

𝑝𝜋 𝑥𝜋 min +

, 𝛽1.

≥

(81)

𝑝 𝑗𝑥 †
𝑗

=

1 − 𝛽
𝛽

𝑚

𝑝𝜋 𝑥𝜋 min −

𝑝𝑘𝑥𝑘 max′

Õ𝑘=1
𝑘≠𝑗,𝜋

𝑚

(82)

=

1
𝛽

𝑝𝜋 𝑥𝜋 min −

𝑝𝑘𝑥𝑘 max′.

Õ𝑘=1
𝑘≠𝑗

Given the above 𝑥𝑘 ’s and (82), since 𝑥𝜋 min = 𝑥𝜋 max′ and 𝑏 𝑗 ≤ 0, and by deﬁnition in (51), 𝑝𝜋 𝑥𝜋 min ≥
𝑝𝑘𝑥𝑘 max′, ∀𝑘, we have

𝑅∗
𝑗

, min
𝑘

𝑅𝑘
𝑗

= min
𝑘

1
𝛽𝑏 𝑗

h

1
𝛽𝑏 𝑗

max
𝑘

h

1
𝛽𝑏 𝑗

h

=

=

=

(1 − 𝛽)𝑝𝑘𝑥𝑘 max′ − 𝛽

𝑝𝑖𝑥𝑖 max′

𝑚

Õ𝑖=1
𝑖≠𝑗
𝑚

i

(1 − 𝛽)𝑝𝑘𝑥𝑘 max′ − 𝛽

𝑝𝑖𝑥𝑖 max′

Õ𝑖=1
𝑖≠𝑗

i

𝑚

(1 − 𝛽)𝑝𝜋𝑥𝜋 min − 𝛽

𝑝𝑖𝑥𝑖 max′

Õ𝑖=1
𝑖≠𝑗

i

1
𝑏 𝑗

(𝑝 𝑗𝑥 †

𝑗 − 𝑝𝜋 𝑥𝜋 min).

Besides, by substituting 𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗 , into (67), we obtain

𝑝 𝑗𝑥 †
𝑗

=

1
1 − 𝛽

𝑚

𝛽

𝑝𝑘𝑥𝑘 max′ + 𝛼 𝑗𝑏 𝑗

.

h

Õ𝑘=1
𝑘≠𝑗

i

(83)

(84)

Achieving Transparency Report Privacy in Linear Time

By substituting the RHS of (82) into (84), we get

𝛼 𝑗 =

1
𝑏 𝑗

(𝑝 𝑗𝑥 †

𝑗 − 𝑝𝜋𝑥𝜋 min).

49

(85)

Since 𝛼 𝑗 ≤ 1, and according to (83), we obtain 𝑅∗
𝑗
converse.

= 𝛼 𝑗 ≤ 1, and thus ﬁnish the proof of the

We summarize Case 1 in the following:

𝑅∗

𝑗 ≤ 1
1 − 𝛽
𝛽

⇐⇒

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗 ≥

𝑚

𝑝𝑖𝑥𝑖 max′

Õ𝑘=1
𝑘≠𝑗
⇐⇒ 𝑥𝜋 = 𝑥𝜋 min and 𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗, 𝜋

⇐⇒ 𝑥 †
𝑗

=

1 − 𝛽
𝛽

1
𝑝 𝑗

n

𝑝𝜋 𝑥𝜋 min −

𝑚

Õ𝑘=1
𝑘≠𝑗,𝜋

𝑝𝑘𝑥𝑘 max′

, 𝑥 †1
𝑗

o

⇐⇒ 𝛽 ≥

𝑝𝜋 𝑥𝜋 min +

𝑝𝑘𝑥𝑘 max′

𝑝𝜋𝑥𝜋 min
𝑚
𝑘=1
𝑘≠𝜋

, 𝛽1.

Í

Case 2:
Now consider the case 𝑅∗
(67) thus becomes

𝑗 ≥ 1. In this case, according to (70), we have 𝛼 𝑗 = min(𝑅∗

𝑗 , 1) = 1, and

𝑝 𝑗𝑥 𝑗 =

1
1 − 𝛽

𝑚

𝛽

𝑝𝑘𝑥𝑘 + 𝑏 𝑗

.

h

Õ𝑘=1
𝑘≠𝑗

i

Moreover, based on (69), there must exist an 𝑘 (denoted by 𝜅) such that

𝛼 𝑗 = 1 ≤

1
𝛽𝑏 𝑗

h

𝑚

(1 − 𝛽)𝑝𝜅𝑥𝜅 − 𝛽

𝑝𝑖𝑥𝑖

= 𝑅∗
𝑗 ,

Õ𝑖=1
𝑖≠𝑗

i

which, since 𝑥𝑖 ≤ 𝑥𝑖 max′, ∀𝑖, yields

𝑚

𝑚

𝑝𝑖𝑥𝑖 max′ ≥

𝑝𝑖𝑥𝑖 ≥

Õ𝑖=1
𝑖≠𝑗

Õ𝑖=1
𝑖≠𝑗

1 − 𝛽
𝛽

𝑝𝜅𝑥𝜅 − 𝑏 𝑗 .

(86)

(87)

(88)

Since

𝑝𝑖𝑥𝑖 ≥

𝑚
𝑖=1
𝑖≠𝑗
to proceed the proof.

𝑚
𝑖=1
𝑖≠𝑗

Í

Í

𝑝𝑖𝑥𝑖 min′ as well, we need to consider diﬀerent cases in the following in order

Case 2.1:
First, we consider the case that the RHS of (88) is greater or equal to the sum of the equivalent
lower limits, i.e.,

1 − 𝛽
𝛽

𝑚

𝑝𝜅𝑥𝜅 − 𝑏 𝑗 ≥

𝑝𝑖𝑥𝑖 min′.

Õ𝑖=1
𝑖≠𝑗

(89)

50

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

In such a case, the equality of the RHS of (88) can hold. Since, based on (86), in order to minimize
𝑥 𝑗 , we need to minimize
𝑝𝑘𝑥𝑘 , from the RHS of (88), which is

𝑚
𝑘=1
𝑘≠𝑗

Í

𝑚

𝑝𝑘𝑥𝑘 =

Õ𝑘=1
𝑘≠𝑗

1 − 𝛽
𝛽

𝑝𝜅𝑥𝜅 − 𝑏 𝑗,

(90)

and thus we need to minimize 𝑝𝜅𝑥𝜅 . By substituting the LHS of (90) into (86) and (65), we obtain

and

𝑝 𝑗𝑥 𝑗 = 𝑝𝜅𝑥𝜅 + 𝑏 𝑗

𝛼𝑘 =

1
𝑏𝑘

(𝑝𝑘𝑥𝑘 − 𝑝𝜅𝑥𝜅), ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗,

(91)

(92)

respectively. Based on (92), similar to Case 1, since 𝛼𝑘 ≥ 0 and 𝑏𝑘 ≤ 0, we have 𝑝𝜅𝑥𝜅 ≥ 𝑝𝑘𝑥𝑘, ∀𝑘, i.e.,
𝑝𝜅𝑥𝜅 = max
𝑝𝑘𝑥𝑘 . Moreover, in order to minimize 𝑥 𝑗 , we need to minimize 𝑝𝜅𝑥𝜅. To minimize 𝑝𝜅𝑥𝜅 ,
𝑘
since 𝑝𝜅𝑥𝜅 = max
𝑝𝑘𝑥𝑘, and 𝑝𝑘𝑥𝑘 ≥ 𝑝𝑘𝑥𝑘 min, ∀𝑘 (including 𝜅), the minimal 𝑝𝜅𝑥𝜅 , i.e., 𝑝𝜅𝑥𝜅 min, is
therefore the largest eﬀective lower limit 𝑝𝑘𝑥𝑘 min over all 𝑘, i.e., 𝑝𝜅𝑥𝜅 min = max
𝑝𝑘𝑥𝑘 min = 𝑝𝜋 𝑥𝜋 min
by deﬁnition, and thus we get 𝜅 = 𝜋 and 𝑥𝜋 = 𝑥𝜋 min. Substituting which into (91), we thus obtain
the minimum of 𝑥 𝑗

𝑘

𝑘

𝑥 †
𝑗

=

=

1
𝑝 𝑗

1
𝑝 𝑗

𝑝𝜋 𝑥𝜋 min + 𝑏 𝑗
n
𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 − 𝛽
n

o

𝑚

𝑝𝑘

, 𝑥 †𝑝
𝑗

.

Õ𝑘=1

o

For the rest of 𝑘, 𝑘 ≠ 𝑗, 𝜋, according to (90), we have

𝑝𝑘𝑥𝑘 =

1 − 2𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗

𝑚

Õ𝑘=1
𝑘≠𝑗,𝜋

=

1 − 2𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑝 𝑗 + 𝛽

𝑝𝑘 .

𝑚

Õ𝑘=1

(93)

(94)

If (S1)∼(S4) are true, 𝑥 †
and thus from (51), we get 𝑥 𝑗 max
(OPT-1) has feasible solution, for Case 2.1, is

𝑗 ≤ 𝑥 𝑗 max. In addition, based on (91), we have 𝑝 𝑗 𝑥 𝑗 = 𝑝𝜋 𝑥𝜋 min +𝑏 𝑗 ≤ 𝑝𝜋 𝑥𝜋 min,
= 𝑥 𝑗 max′. Therefore, based on (93), the minimum 𝛽 such that

𝛽 =

≥

=

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 (1 − 𝑥 †
𝑗 )
𝑚
𝑘=1 𝑝𝑘
𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 (1 − 𝑥 𝑗 max)
𝑚
𝑘=1 𝑝𝑘

Í

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
Í
𝑚
𝑘=1 𝑝𝑘

, 𝛽𝑝 𝑗 .

Í

(95)

Achieving Transparency Report Privacy in Linear Time

51

We next prove the converse. If 𝑥𝜅 = 𝑥𝜋 = 𝑥𝜋 min, and (93), (94), and (95) hold, from (67), we have
𝑚

𝑝 𝑗 𝑥 𝑗 =

1
1 − 𝛽

𝛽

𝑝𝑘𝑥𝑘 + 𝛼 𝑗𝑏 𝑗

,

h

Õ𝑘=1
𝑘≠𝑗

i

and, since

𝑝𝑘𝑥𝑘 ≥

𝑚
𝑘=1
𝑘≠𝑗

𝑚
𝑘=1
𝑘≠𝑗

𝑝𝑘𝑥𝑘 min′, from (94), we have

Í

Í

𝑚

𝑝𝑘𝑥𝑘 =

Õ𝑘=1
𝑘≠𝑗

1 − 𝛽
𝛽

𝑚

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗 ≥

𝑝𝑘𝑥𝑘 min′.

Õ𝑘=1
𝑘≠𝑗

(96)

(97)

Since (94) and (97) hold when 𝑥 𝑗 = 𝑥 †
get

𝑗 , i.e., (93) holds, by substituting the LHS of (97) into (96), we

𝑝 𝑗𝑥 †
𝑗

= 𝑝𝜋𝑥𝜋 min +

𝑏 𝑗 .

(98)

𝛼 𝑗 − 𝛽
1 − 𝛽

(cid:16)

(cid:17)

= 1, and therefore 𝛼 𝑗 = 1. Based on (70), we thus obtain

Comparing (98) with (93), we have 𝛼 𝑗 −𝛽
1−𝛽
𝑅∗
𝑗 ≥ 1 and ﬁnish the proof of the converse.
We summarize Case 2.1 in the following:

𝑅∗

𝑗 ≥ 1 and

1 − 𝛽
𝛽

𝑚

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗 ≥

𝑝𝑘𝑥𝑘 min′

Õ𝑘=1
𝑘≠𝑗

𝑚

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗 ≥

𝑝𝑘𝑥𝑘 min′

Õ𝑘=1
𝑘≠𝑗

𝑚

⇐⇒

𝑝𝑘𝑥𝑘 max′ ≥

Õ𝑘=1
𝑘≠𝑗

1 − 𝛽
𝛽

𝑚

⇐⇒ 𝑥𝜋 = 𝑥𝜋 min and

𝑝𝑘𝑥𝑘 =

Õ𝑘=1
𝑘≠𝑗,𝜋

⇐⇒ 𝑥 †
𝑗

=

⇐⇒ 𝛽 ≥

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 − 𝛽
n

1
𝑝 𝑗
𝑝𝜋𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
𝑚
𝑘=1 𝑝𝑘

Õ𝑘=1
, 𝛽𝑝 𝑗 .

1 − 2𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑝 𝑗 + 𝛽

𝑝𝑘

𝑚

Õ𝑘=1

𝑚

𝑝𝑘

, 𝑥 †𝑝
𝑗

o

𝑚

Case 2.2:
Next, we consider the case that

Í

1 − 𝛽
𝛽

Recall that 𝑦𝑘 , 1 − 𝑥𝑘, 𝑦𝑘 max
𝛼 𝑗

′ = 1 − 𝛼 𝑗 = 0, based on (66), we have

, 1 − 𝑥𝑘 min, 𝑦𝑘 min

𝑝𝜋𝑥𝜋 min − 𝑏 𝑗 ≤

𝑝𝑘𝑥𝑘 min′.

(99)

Õ𝑘=1
𝑘≠𝑗

, 1 − 𝑥𝑘 max, and 𝛼𝑘

′ , 1 − 𝛼𝑘, ∀𝑘. Since from (87),

′ =

𝛼𝑘

𝛽
1 − 𝛽

1
𝑏𝑘

1 − 𝛽
𝛽

h

𝑚

𝑝𝑘𝑦𝑘 −

𝑝𝑖𝑦𝑖

, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗 .

(100)

Õ𝑖=1
𝑖≠𝑗

i

52

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Moreover, by substituting 𝑥𝑘 = 1 − 𝑦𝑘 , ∀𝑘, into (86), we get

𝑝 𝑗𝑥 𝑗 = 𝑝 𝑗 −

𝛽
1 − 𝛽

𝑚

𝑝𝑘𝑦𝑘 ,

Õ𝑘=1
𝑘≠𝑗

𝑝 𝑗𝑦 𝑗 =

𝛽
1 − 𝛽

𝑚

𝑝𝑘𝑦𝑘 ,

Õ𝑘=1
𝑘≠𝑗

or equivalently,

and

𝛽 =

𝑝 𝑗𝑦 𝑗
𝑚
𝑘=1𝑝𝑘𝑦𝑘

=

𝑝 𝑗𝑦 𝑗 +

𝑝 𝑗𝑦 𝑗
𝑚
𝑘=1
𝑘≠𝑗

.

𝑝𝑘𝑦𝑘

Substituting the RHS of (102) into (100), we obtain

Í

Í

′ =

𝛼𝑘

1
𝑏𝑘

(𝑝𝑘𝑦𝑘 − 𝑝 𝑗𝑦 𝑗 ), ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗,

(101)

(102)

(103)

(104)

′ ≥ 0 and 𝑏𝑘 ≤ 0, we have 𝑝 𝑗𝑦 𝑗 ≥ 𝑝𝑘𝑦𝑘 , ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗 , i.e., 𝑝 𝑗𝑦 𝑗 = max

𝑝𝑘𝑦𝑘 .
Since 𝛼𝑘
Moreover, from (101), since 𝛽 is non-negative and 𝑥𝑘 ≥ 0 for all 𝑘, in order to minimize 𝑥 𝑗 , we
𝑝𝑘𝑦𝑘 . In addition, based on (103), in Case 2.2, 𝛽 achieves its minimum when
need to maximize

𝑘

(i) minimizing 𝑝 𝑗𝑦 𝑗 and (ii) maximizing
Í

𝑝𝑘𝑦𝑘 .

𝑚
𝑘=1
𝑘≠𝑗

To minimize 𝑝 𝑗𝑦 𝑗 , since 𝑝 𝑗𝑦 𝑗 = max

𝑝 𝑗𝑦 𝑗 , i.e., 𝑝 𝑗𝑦 𝑗 min, is therefore the largest eﬀective lower limit 𝑝𝑘𝑦𝑘 min over all 𝑘, i.e., 𝑝 𝑗𝑦 𝑗 min
max
𝑘
To maximize

𝑝𝑘𝑦𝑘 min = 𝑝𝜃𝑦𝜃 min by deﬁnition, and thus we get 𝑗 = 𝜃 and 𝑦𝜃 = 𝑦𝜃 min.

𝑝𝑘𝑦𝑘 , we need to ﬁnd the maximum of each 𝑦𝑘 . Since 0 ≤ 𝛼𝑘

′ ≤ 1, by substi-

𝑘

𝑝𝑘𝑦𝑘 , and 𝑝𝑘𝑦𝑘 ≥ 𝑝𝑘𝑦𝑘 min, ∀𝑘 (including 𝑗 ), the minimal
Í
=

Í

tuting 𝑝 𝑗𝑦 𝑗 = 𝑝𝜃𝑦𝜃 min into (104), we have 𝑝𝜃𝑦𝜃 min + 𝑏𝑘 ≤ 𝑝𝑘𝑦𝑘 ≤ 𝑝𝜃𝑦𝜃 min, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃 .
By deﬁnitions, 𝑦𝑘 max′ , min{𝑦𝑘 max, 𝑝𝜃
𝑦𝜃 min + 𝑏𝑘 }. Combining
𝑝𝑘
with the constraints 𝑦𝑘 min ≤ 𝑦𝑘 ≤ 𝑦𝑘 max, we obtain 𝑦𝑘 min′ ≤ 𝑦𝑘 ≤ 𝑦𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, and thus

𝑦𝜃 min} and 𝑦𝑘 min′ , max{𝑦𝑘 min, 𝑝𝜃
𝑝𝑘

𝑝𝑘𝑦𝑘 is maximized when 𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘, 𝑘 ≠ 𝑗 .

𝑚
𝑘=1
𝑘≠𝑗
By substituting the 𝑦𝑘 ’s we obtained above into (101), based on which, the optimal objective

Í
value 𝑥 †

𝑗 , the minimum of 𝑥 𝑗 , is thus

𝑥 †
𝑗

=

𝑝 𝑗 −

𝛽
1 − 𝛽

1
𝑝 𝑗

n

𝑚

𝑝𝑘𝑦𝑘 max′

, 𝑥 †0
𝑗

.

Õ𝑘=1
𝑘≠𝑗

o

(105)

If (S1)∼(S4) are true, 𝑥 †
solution, for Case 2.2, is

𝑗 ≤ 𝑥 𝑗 max. Based on (103), the minimum 𝛽 such that (OPT-1) has feasible

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃
We next prove the converse. Before proving the converse, we ﬁrst need the following lemma.

𝑝𝜃𝑦𝜃 min +

𝑝𝑘𝑦𝑘 max′

, 𝛽0.

𝛽 ≥

Í

(106)

Lemma 10. If 𝑝𝜃𝑦𝜃 min ≤ 𝑝𝑘𝑦𝑘 − 𝑏𝑘, ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃 , we have 𝑝𝑘𝑥𝑘 min′ ≤ 𝑝𝑘
∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃 .

1 − 𝑦𝑘 max′

,

(cid:0)

(cid:1)

𝑚
𝑘=1
𝑘≠𝑗

𝑚
𝑘=1
𝑘≠𝑗

Achieving Transparency Report Privacy in Linear Time

Proof. Recall that ∀𝑘 = 1, . . . , 𝑚, by deﬁnitions we have

𝑝𝑘𝑥𝑘 min′ , max{𝑝𝜋 𝑥𝜋 min + 𝑏𝑘, 𝑝𝑘𝑥𝑘 min},

𝑝𝑘

1 − 𝑦𝑘 max′

(cid:0)

= 𝑝𝑘 − 𝑝𝑘𝑦𝑘 max′
, max{𝑝𝑘 − 𝑝𝜃𝑦𝜃 min, 𝑝𝑘 − 𝑝𝑘𝑦𝑘 max}
= max{𝑝𝑘 − 𝑝𝜃𝑦𝜃 min, 𝑝𝑘𝑥𝑘 min}.

(cid:1)

Given the conditions that 𝑝𝜃𝑦𝜃 min ≤ 𝑝𝑘𝑦𝑘 − 𝑏𝑘, ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃 , from which we get

𝑝𝜃𝑦𝜃 min ≤ 𝑝𝑘𝑦𝑘 − 𝑏𝑘, ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃,

=⇒ 𝑝𝜃𝑦𝜃 min ≤ 𝑝𝜋𝑦𝜋 − 𝑏𝜋

=⇒ 𝑝𝜃𝑦𝜃 min ≤ 𝑝𝜋

1 − 𝑥𝜋

− 𝑝𝜋 + 𝛽

𝑝𝑘

𝑚

(cid:0)
=⇒ 𝑝𝜋 𝑥𝜋 = 𝑝𝜋 𝑥𝜋 min ≤ −𝑝𝜃𝑦𝜃 min + 𝛽

(cid:1)

Õ𝑘=1
𝑚

𝑝𝑘

=⇒ 𝑝𝜋 𝑥𝜋 min + 𝑏𝑘 ≤ −𝑝𝜃𝑦𝜃 min + 𝛽

=⇒ 𝑝𝜋 𝑥𝜋 min + 𝑏𝑘 ≤ 𝑝𝑘 − 𝑝𝜃𝑦𝜃 min.

𝑚

Õ𝑘=1
𝑝𝑘 + 𝑝𝑘 − 𝛽

Õ𝑘=1

𝑝𝑘

𝑚

Õ𝑘=1

53

(107)

(108)

(109)

Therefore, since the above inequalities hold for all 𝑘 except 𝑘 = 𝜃 , by comparing the RHS of (107)
, ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃 . We thus ﬁnish the
and (108), we obtain that 𝑝𝑘𝑥𝑘 min′ ≤ 𝑝𝑘
1 − 𝑦𝑘 max′
(cid:3)
proof.

(cid:0)

(cid:1)

Now we start proving the converse. If 𝑦 𝑗 , 1 − 𝑥 𝑗 = 𝑦𝜃 = 𝑦𝜃 min, 𝑦𝑘 , 1 − 𝑥𝑘 = 𝑦𝑘 max′, ∀𝑘, 𝑘 ≠ 𝑗 ,
′ = 0, by

(105) and the equality in (106) hold, since from (87) we have 𝛼 𝑗 = 1, or equivalently, 𝛼 𝑗
′ into (66), we obtain
substituting the above 𝑦𝑘 ’s and 𝛼 𝑗
𝑚

′ =

𝛼𝑘

𝛽
1 − 𝛽

1
𝑏𝑘

1 − 𝛽
𝛽

h

𝑝𝑘𝑦𝑘 −

𝑝𝑖𝑦𝑖 max′

, ∀𝑘, 𝑘 ≠ 𝑗 ,

(110)

Õ𝑖=1
𝑖≠𝑗

i

and by substituting the 𝑥𝑘’s transformed from the above 𝑦𝑘 ’s (including 𝑘 = 𝑗 = 𝜃 ) into (86), we
have

𝑚

𝑚

𝑝𝑘𝑥𝑘 =

𝑝𝑘

1 − 𝑦𝑘 max′

=

Õ𝑘=1
𝑘≠𝜃

Õ𝑘=1
𝑘≠𝜃

(cid:0)

(cid:1)

1 − 𝛽
𝛽

𝑝𝜃𝑥𝜃 −

1
𝛽

𝑏𝜃 .

From (110), since 𝑗 = 𝜃 , by substituting (106) into (110), we obtain
1
𝑏𝑘

(𝑝𝑘𝑦𝑘 − 𝑝𝜃𝑦𝜃 min), ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃 .

′ =

𝛼𝑘

(111)

(112)

′ ≤ 1, from (112), we obtain 𝑝𝜃𝑦𝜃 min + 𝑏𝑘 ≤ 𝑝𝑘𝑦𝑘 ≤ 𝑝𝜃𝑦𝜃 min, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃 .

Since 0 ≤ 𝛼𝑘
Therefore, based on Lemma 10, we have 𝑝𝑘𝑥𝑘 min′ ≤ 𝑝𝑘
Recall that based on (108), for each 𝑘, 𝑘 ≠ 𝜃 , 𝑝𝑘

is the maximum of 𝑝𝑘 − 𝑝𝜃𝑦𝜃 min
(cid:1)
and 𝑝𝑘𝑥𝑘 min. Deﬁne Φ the set of 𝑘’s yielding 𝑝𝑘
= 𝑝𝑘𝑥𝑘 min ≥ 𝑝𝑘 − 𝑝𝜃𝑦𝜃 min, 𝑘 ∈ Φ, and
1 − 𝑦𝑘 max′
(cid:0)
> 𝑝𝑘𝑥𝑘 min,
deﬁne Ω the complement of Φ, i.e., the set of 𝑘’s yielding 𝑝𝑘
= 𝑝𝑘 − 𝑝𝜃𝑦𝜃 min
1 − 𝑦𝑘 max′
(cid:0)
(cid:1)
𝑘 ∈ Ω. In addition, let 𝜙 , |Φ| and 𝜔 , |Ω|. Note that 𝜙 + 𝜔 = 𝑚 − 1. We have the following lemma.
Lemma 11. If 𝑦𝜃 = 𝑦𝜃 min, 𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘, 𝑘 ≠ 𝜃 , and the equality in (106) holds, we have 𝜔 ≤ 1−𝛽
𝛽 .

1 − 𝑦𝑘 max′

(cid:1)

(cid:0)

(cid:1)

(cid:0)

1 − 𝑦𝑘 max′

, ∀𝑘 = 1, . . . , 𝑚, 𝑘 ≠ 𝜃 .

54

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Proof. If 𝑦𝜃 = 𝑦𝜃 min, 𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘, 𝑘 ≠ 𝜃 , and the equality in (106) holds, from (106) we have

𝛽
1 − 𝛽

=

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃

𝑝𝑘𝑦𝑘 max′

.

(113)

Í
1 − 𝑦𝑘 max′
1 − 𝑦𝑘 max′
(cid:1)

= 𝑝𝑘 −𝑝𝜃𝑦𝜃 min, or equivalently, 𝑝𝑘𝑦𝑘 max′ =
= 𝑝𝑘𝑥𝑘 min, or equivalently, 𝑝𝑘𝑦𝑘 max′ =

Note that since for those 𝑘’s in Ω, we have 𝑝𝑘
𝑝𝜃𝑦𝜃 min, and for those 𝑘’s in Φ, we have 𝑝𝑘
(cid:0)
𝑝𝑘𝑦𝑘 max, (113) thus becomes

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃

𝑝𝑘𝑦𝑘 max′

=

Í

We thus ﬁnish the proof.

(cid:0)

(cid:1)

𝑝𝜃𝑦𝜃 min

𝜔𝑝𝜃𝑦𝜃 min +

𝑘 ∈Φ 𝑝𝑘𝑦𝑘 max

Í

≤

1
𝜔

.

(114)

(cid:3)

Deﬁne X ,

𝑝𝑘
addition, by deﬁnitions of Φ and Ω, we have
(cid:0)
Í

1 − 𝑦𝑘 max′

and Y ,

𝑚
𝑘=1
𝑘≠𝜃

Í

(cid:1)

𝑚
𝑘=1
𝑘≠𝜃

𝑝𝑘𝑥𝑘 min′. Based on Lemma 10, we have X ≥ Y. In

𝑚

X ,

𝑝𝑘

1 − 𝑦𝑘 max′

Õ𝑘=1
𝑘≠𝜃

(cid:0)

(cid:1)

=

=

𝑝𝑘𝑥𝑘 min +

𝑝𝑘 − 𝑝𝜃𝑦𝜃 min

(115)

Õ𝑘 ∈Φ
1 − 𝛽
𝛽

𝑝𝜃𝑥𝜃 −

(cid:0)

Õ𝑘 ∈Ω
1
𝑏𝜃 . (Based on (111))
𝛽

(cid:1)

Similarly to the Φ and Ω in X, deﬁne Φ’ the set of 𝑘’s yielding 𝑝𝑘𝑥𝑘 min′ = 𝑝𝑘𝑥𝑘 min ≥ 𝑝𝜋𝑥𝜋 min +𝑏𝑘,
𝑘 ∈ Φ’, for Y. Since based on (109) in Lemma 10, we have 𝑝𝜋 𝑥𝜋 min+𝑏𝑘 ≤ 𝑝𝑘 −𝑝𝜃𝑦𝜃 min for all 𝑘 except
𝜃 . Therefore, for those 𝑘’s belonging to Φ (in X), based on (108), we get 𝑝𝑘𝑥𝑘 min ≥ 𝑝𝑘 − 𝑝𝜃𝑦𝜃 min ≥
𝑝𝜋 𝑥𝜋 min + 𝑏𝑘, and based on (107), we ﬁnd that those 𝑘’s belonging to Φ (in X) also belong to Φ’ (in
Y), i.e., Φ ⊆ Φ’. Therefore, Y can be interpreted by Φ as follow.

𝑚

Y ,

𝑝𝑘𝑥𝑘 min′

Õ𝑘=1
𝑘≠𝜃

=

𝑝𝑘𝑥𝑘 min +

max

Õ𝑘 ∈Φ

Õ𝑘 ∈Ω

𝑝𝜋 𝑥𝜋 min + 𝑏𝑘, 𝑝𝑘𝑥𝑘 min
n

o

.

In addition, similarly to Y, we deﬁne Z as follow.

Z ,

𝑝𝑘𝑥𝑘 min +

𝑝𝜋 𝑥𝜋 min + 𝑏𝑘

.

Õ𝑘 ∈Φ

Õ𝑘 ∈Ω

(cid:0)

(cid:1)

(116)

(117)

Achieving Transparency Report Privacy in Linear Time

55

Clearly, the RHS of Z is not greater than the RHS of Y, and thus we have X ≥ Y ≥ Z. Deﬁne
W , X − Z, based on (115) and (117), we have

W , X − Z

=

=

=

𝑝𝑘 − 𝑝𝜃𝑦𝜃 min

−

𝑝𝜋 𝑥𝜋 min + 𝑏𝑘

Õ𝑘 ∈Ω

h (cid:0)

(cid:1)

𝑝𝑘 − 𝑝𝜃𝑦𝜃 min

−

(cid:1) i

(cid:0)
𝑝𝜋 𝑥𝜋 min + 𝑝𝑘 − 𝛽

Õ𝑘 ∈Ω

h (cid:0)
𝛽

𝑚

(cid:1)

(cid:0)
𝑝𝑖 − 𝑝𝜃𝑦𝜃 min − 𝑝𝜋 𝑥𝜋 min

𝑚

𝑝𝑖

Õ𝑖=1

(cid:1)i

h

Õ𝑘 ∈Ω
𝛽

= 𝜔

Õ𝑖=1
𝑚
𝑝𝑖 − 𝑝𝜃𝑦𝜃 min − 𝑝𝜋𝑥𝜋 min

h

Õ𝑖=1

i

.

i

(118)

Deﬁne C , 𝛽
𝑚
𝑖=1 𝑝𝑖 − 𝑝𝜃𝑦𝜃 min − 𝑝𝜋 𝑥𝜋 min, the constant term in (118). Since X ≥ Z, we have
W = 𝜔C ≥ 0, and because 𝜔 is the cardinality of Ω, it is non-negative, and thus C ≥ 0. Since
C ≥ 0, and from Lemma 11, 𝜔 ≤ 1−𝛽

Í

𝛽 , we thus have

Y ≥ Z = X −

X − Z

= X − W = X − 𝜔C ≥ X −

1 − 𝛽
𝛽

C.

(119)

Note that since 𝑦𝜃 = 𝑦𝜃 min, the RHS of (119) becomes

(cid:0)

(cid:1)

X −

1 − 𝛽
𝛽

C

1 − 𝛽
𝛽

𝑝𝜃𝑦𝜃 min +

1 − 𝛽
𝛽

𝑝𝜋𝑥𝜋 min

𝑝𝑖 +

1 − 𝛽
𝛽

𝑝𝜋 𝑥𝜋 min

(120)

=

=

1 − 𝛽
𝛽

𝑝𝜃𝑥𝜃 −

1
𝛽

h
1 − 𝛽
𝛽

𝑝𝜃𝑥𝜃 −

1
𝛽

𝑏𝜃

−

i

1 − 𝛽
𝛽

C

𝑚

𝑏𝜃 − (1 − 𝛽)

𝑝𝑖 +

Õ𝑖=1
𝑏𝜃 − (1 − 𝛽)

𝑚

𝑝𝜃 (𝑥𝜃 + 𝑦𝜃 ) −

1
𝛽

=

1 − 𝛽
𝛽

=

1 − 𝛽
𝛽

=

=

1 − 𝛽
𝛽
1 − 𝛽
𝛽

𝑝𝜃 −

𝑏𝜃 −

1 − 𝛽
𝛽

1 − 𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑏𝜃 .

𝑏𝜃 − (1 − 𝛽)

𝑝𝜋 𝑥𝜋 min − 𝑏𝜃

Õ𝑖=1
𝑝𝑖 +

1 − 𝛽
𝛽

𝑚

Õ𝑖=1

𝑏𝜃 +

1 − 𝛽
𝛽

𝑝𝜋𝑥𝜋 min − 𝑏𝜃

Therefore, we obtain

𝑚

𝑝𝑘𝑥𝑘 min′ = Y ≥ X −

Õ𝑘=1
𝑘≠𝜃

1 − 𝛽
𝛽

C =

1 − 𝛽
𝛽

𝑝𝜋𝑥𝜋 min − 𝑏𝜃 .

(121)

Since 𝑗 = 𝜃 , by replacing 𝜃 in (121) by 𝑗 , we obtain (99) and ﬁnish the proof of the converse.

56

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

We summarize Case 2.2 in the following:

𝑅∗

𝑗 ≥ 1 and

1 − 𝛽
𝛽

𝑚

𝑝𝜋 𝑥𝜋 min − 𝑏 𝑗 ≤

𝑝𝑘𝑥𝑘 min′

Õ𝑘=1
𝑘≠𝑗

⇐⇒ 𝑦𝜃 = 𝑦𝜃 min and 𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃
𝑚

𝑝𝑘𝑦𝑘 max′

, 𝑥 †0
𝑗

⇐⇒ 𝑥 †
𝑗

=

⇐⇒ 𝛽 ≥

1
𝑝 𝑗

𝑝 𝑗 −

n

𝛽
1 − 𝛽

Õ𝑘=1
𝑘≠𝑗

𝑝𝜃𝑦𝜃 min +

𝑝𝑘𝑦𝑘 max′

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃

o

, 𝛽0.

Í

Therefore, if (S1) (S4) are true, (OPT-1) has feasible solutions for arbitrary 𝑥𝑘 min and 𝑥𝑘 max, ∀𝑘 =
1, . . . , 𝑚, 𝑘 ≠ 𝑗 , which implies (OPT-1) has feasible solutions for all the cases (Case 1, Case 2.1,
and Case 2.2), which requires 𝛽 to be greater than the minimum 𝛽 in each of the above cases, i.e.,
𝛽 ≥ max{𝛽0, 𝛽1, 𝛽𝑝 𝑗 }, where

𝛽0 =

𝛽1 =

=

𝛽𝑝 𝑗

,

,

𝑝𝜃𝑦𝜃 min
𝑚
𝑘=1
𝑘≠𝜃
𝑝𝜋 𝑥𝜋 min
Í
𝑚
𝑘=1
𝑘≠𝜋

𝑝𝜃𝑦𝜃 min +

𝑝𝑘𝑦𝑘 max′

𝑝𝜋𝑥𝜋 min +

𝑝𝑘𝑥𝑘 max′

𝑝𝜋𝑥𝜋 min + 𝑝 𝑗𝑦 𝑗 min
Í
𝑚
𝑘=1 𝑝𝑘

,

and the corresponding optimal objective value 𝑥 †

Í

𝑗 and its corresponding optimal solutions are

𝛽 = 𝛽0 ⇐⇒ 𝑥 †
𝑗

=

1
𝑝 𝑗

𝑝 𝑗 −
n

𝛽
1 − 𝛽

𝑚

Õ𝑘=1
𝑘≠𝑗

𝑝𝑘𝑦𝑘 max′

, 𝑥 †0
𝑗

o

⇐⇒ 𝑦 𝑗 = 𝑦𝜃 = 𝑦𝜃 min

𝑦𝑘 = 𝑦𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝜃 ,

𝛽 = 𝛽1 ⇐⇒ 𝑥 †
𝑗

=

1 − 𝛽
𝛽

1
𝑝 𝑗

n

𝑝𝜋 𝑥𝜋 min −

𝑚

Õ𝑘=1
𝑘≠𝑗,𝜋

𝑝𝑘𝑥𝑘 max′

, 𝑥 †1
𝑗

o

⇐⇒ 𝑥𝜋 = 𝑥𝜋 min

𝑥𝑘 = 𝑥𝑘 max′, ∀𝑘 = 1, · · · , 𝑚, 𝑘 ≠ 𝑗, 𝜋,

𝛽 = 𝛽𝑝 𝑗 ⇐⇒ 𝑥 †

𝑗

=

1
𝑝 𝑗

𝑝𝜋 𝑥𝜋 min + 𝑝 𝑗 − 𝛽
n

𝑚

𝑝𝑘

, 𝑥 †𝑝
𝑗

Õ𝑘=1

o

⇐⇒ 𝑥𝜋 = 𝑥𝜋 min
𝑚

1 − 2𝛽
𝛽

𝑝𝜋 𝑥𝜋 min − 𝑝 𝑗 + 𝛽

𝑝𝑘 .

𝑚

Õ𝑘=1

Õ𝑘=1
𝑘≠𝑗,𝜋
We thus ﬁnish the proof of Lemma 9.

𝑝𝑘𝑥𝑘 =

