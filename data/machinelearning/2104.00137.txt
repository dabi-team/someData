1
2
0
2

r
p
A
5
1

]

G
L
.
s
c
[

3
v
7
3
1
0
0
.
4
0
1
2
:
v
i
X
r
a

Achieving Transparency Report Privacy in Linear Time

CHIEN-LUN CHEN and LEANA GOLUBCHIK, University of Southern California
RANJAN PAL, University of Michigan

An accountable algorithmic transparency report (ATR) should ideally investigate the (a) transparency of the
underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data sub-
jects‚Äô privacy. However, a provably formal study of the impact to data subjects‚Äô privacy caused by the utility
of releasing an ATR (that investigates transparency and fairness), is yet to be addressed in the literature. The
far-fetched beneÔ¨Åt of such a study lies in the methodical characterization of privacy-utility trade-oÔ¨Äs for
release of ATRs in public, and their consequential application-speciÔ¨Åc impact on the dimensions of society,
politics, and economics. In this paper, we Ô¨Årst investigate and demonstrate potential privacy hazards brought
on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects‚Äô pri-
vacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming
(LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation
on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-oÔ¨Äs induced by
our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of
our knowledge, this is the Ô¨Årst analytical work that simultaneously addresses trade-oÔ¨Äs between the triad of
privacy, utility, and fairness, applicable to algorithmic transparency reports.

Additional Key Words and Phrases: privacy; algorithmic transparency; fairness; linear fractional program-
ming

1 INTRODUCTION
In the era of big data and machine learning (ML), automated data processing algorithms are widely
adopted in many Ô¨Åelds for classiÔ¨Åcation, prediction, or decision-making tasks due to huge volumes
of input data and successful performance of ML approaches. Ongoing concerns and social uproar
about the transparency and fairness of such decision-making have been raised by the media, gov-
ernment agencies, foundations, and academics over the past decade [14, 68]. On a technical note,
it has been shown in example studies that ML algorithms can be biased when (i) a dataset used
to train ML models reÔ¨Çects society‚Äôs historical biases [86], e.g., only a few female presidential
nominees in the U.S. history, or (ii) because ML algorithms have much better understanding of
the majority groups and poor understanding of the minority groups [12], and so on. Thus, as we
rapidly move forward to a data-driven age where a signiÔ¨Åcant amount of day-day decision making
in personal and professional spheres might be automation-driven, it would make great sense to
often know the reasons behind certain decisions in order to understand if they are being treated
fairly. Unfortunately, most decision processes today are often opaque, making it diÔ¨Écult to ratio-
nalize why certain decisions are made and whether they favor or disfavor certain individuals or
groups.

Providing an algorithmic transparency report (ATR) by data controllers and third party regu-
latory agencies to decision-facing individuals is one way to investigate whether decisions made
in a blackbox are fair and transparent [26, 36, 71] - an immediate application area of considerable
social impact being explainable AI for medical diagnoses [50, 77] to enable medical personnel bet-
ter understand and interpret diagnostic reports, and justify vulnerabilities of deployed AI models
through domain expertise. This is a popular topic in research and there have been works in the last
decade that have developed methodologies to reduce opaqueness in decision making [28, 46] and

Manuscript submitted to ACM

 
 
 
 
 
 
2

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Transparency
of ML models

Security & privacy
in ML

ATR

Fairness in ML

Fig. 1. A depiction of the realm of accountable ATRs

improve on its fairness relative to certain protected attributes1 [63]. The notion of transparency
has also made its way into recently implemented policies for data protection such as in the EU
General Data Protection Regulation (GDPR), and the California Consumer Privacy Act of 2018
(CCPA or AB-375) - both of which regulates the processing of collected personal or non-personal
data of any data subject (the natural person to whom the data and the decision process relate) [44].
More speciÔ¨Åcally, any data controller shall inform data subjects before collecting their data, and
is required to clearly explain the purpose of collecting data and how data will be processed, upon
data subjects‚Äô requests (‚Äúright to explanation‚Äù and ‚Äúright to non-discrimination‚Äù) [44]. However, a
major side eÔ¨Äect of providing transparency and fairness guarantees to the decision-facing clients is
an unwanted risk to the privacy of other clients in a database. To this end, there exists a substan-
tial literature pointing out potential privacy threats in ML [69], including membership attacks
[80], training data extraction (model inversion attack) [39, 40], model extraction [87], and so on.
However, for ATRs, although it has been pointed out that transparency, proposed by legislature
to protect people‚Äôs rights, may hurt privacy [8, 23], it is yet to be made methodically clear how
transparency can hurt privacy.

Goal - An accountable ATR, especially for automated ML decision processes, should ideally
include transparency of the underlying algorithm, ability to inspect fairness of the algorithmic de-
cisions, and most importantly, preserve data subjects‚Äô privacy (‚Äúright to privacy‚Äù [20]), as depicted
in Fig. 1. Our goal in this paper is to work towards this goal and study the corresponding trade-oÔ¨Äs
between the triad elements.

In this paper, we investigate this problem and explicitly show that data subjects‚Äô private infor-
mation can be inferred via various transparency schemes and fairness measures in announced
ATRs.

Research Contributions - We make the following contributions in this research paper:

‚Ä¢ We explicitly demonstrate inference attacks on data subjects‚Äô private information using a
synthetic and a real dataset and show that such attacks can be performed on various trans-
parency schemes without strong assumptions of adversaries‚Äô knowledge. These instances ex-
pose the possible aspects of algorithmic transparency that could hurt data subjects‚Äô privacy
and subsequently have negative socio-political implications. (See Section 3 and Appendix B)
‚Ä¢ To protect data subjects‚Äô privacy in an ATR, we propose a privacy-aware mechanism per-
turbing the unveiled rationale of opaque decision-making algorithms to control the amount
of disclosed information in ATRs, at the same time providing suÔ¨Écient utility. SpeciÔ¨Åcally,
given a released privacy-preserving ATR, for honest-but-curious adversaries which may know

1Protected attributes form a subset of attributes, to which any decision process should not show preference, in any instance.
It may contain public attributes (gender, race, etc.) and/or private/sensitive attributes (health conditions, gene, etc.).

Achieving Transparency Report Privacy in Linear Time

3

(i) the target individuals‚Äô public information used as inputs to the decision model, (ii) the tar-
get individuals‚Äô received decisions (model outputs), and (iii) side-information from auxiliary
sources, the maximum conÔ¨Ådence of inferring any sensitive information about any data sub-
ject is guaranteed not exceeding a predetermined privacy threshold (See Section 4 and 5).
‚Ä¢ We study the trade-oÔ¨Ä between privacy and utility of ATRs. SpeciÔ¨Åcally, we aim to under-
stand the minimum required perturbation/distortion in order to provide a certain privacy
guarantee, or the maximum privacy that can be provided/guaranteed subject to utility con-
straints, which can be formulated as an optimization problem for privacy-utility trade-oÔ¨Ä in
ATRs. In addition, we analyze the impact of privacy perturbations on fairness measures. In
this regard, our work provides useful quantitative trade-oÔ¨Äs and inÔ¨Çuences between privacy,
transparency, and fairness measures in ATRs (See Sections 5 and 6).

‚Ä¢ We deduce that our privacy-utility optimization problem equivalent to a generalized linear
fractional programming problem (LFP) [16, 94]. Such a problem can in general be solved
as a sequence of linear programming feasibility problems, each with pseudo-polynomial
time complexity with respect to the number of problem variables (the number of diÔ¨Äerent
decision regions2 in our problem), which, however, in the worst case, grows exponentially
with the number of input attributes - hence lending the said optimization problem intractable
for large record sizes. However, on a closer investigation, we Ô¨Ågure out that the region of
interest in the solution space can be decomposed into disjoint ‚Äúsubspaces‚Äù leading to multiple
independent sub-problems - each bearing important properties and amenable to propose
closed-form solutions. Subject to utility constraints, the optimal-privacy protection scheme
can thus be solved from the optimization problem eÔ¨Éciently in linear time (See Sections 7
and 8).

2 APPLICABILITY AND ETHICALITY OF THE PROPOSED PRIVACY SCHEME
In this section, we Ô¨Årst describe a range of possible application domains for the proposed ATRs
privacy protection scheme. We then discuss potential ethical concerns related to announcing a
perturbed ‚Äútransparency‚Äù report, speciÔ¨Åcally, the conÔ¨Çict between principles of transparency and
perturbation for preserving data subjects‚Äô privacy.

2.1 Applicability

The proposed privacy protection scheme, based on perturbing of decision mappings (DeÔ¨Ånition 1),
can be applied to numerous applications with characteristic that the decision regions of the appli-
cation decision process are disjoint Ô¨Ånite sets of the input attribute space, i.e., the application decision
process can be represented by a Ô¨Ånite number of decision mappings3. Although, to our knowledge,
to date there are no current instantiations of an algorithmic transparency report, we believe that
potential applicable domains for our scheme include the following (among others):

‚Ä¢ University admissions and job recruitment: Fairness in university/college admissions has be-
come a signiÔ¨Åcant public concern, attracting more and more attention from the public as
well as the media [37], even though the fact that the deÔ¨Ånition of ‚Äúfairness‚Äù is still contro-
versial, e.g., whether race should be used in admissions decisions to reÔ¨Çect racial diversity
[65, 90]. To respond to the public‚Äôs concerns, some governments [2] and universities [1] ini-
tiated work on admissions transparency, providing statistical data from applications (inputs)

2The regions of input attributes partitioned by decision rules; see Section 3 for examples.
3Our scheme may not be a good Ô¨Åt for some application domains where it may not be possible to represent the decision
process using a Ô¨Ånite number of decision mappings, e.g., applications of natural language processing, such as speech/music
recognition, speech/text understanding, and text/intent classiÔ¨Åcation, or applications of image and video processing, such
as text recognition, item detection and alert, and image classiÔ¨Åcation.

4

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

and admissions (outputs). Currently, we are not aware of instantiations disclosing the admis-
sion decision process; however, a negligent report could leak applications‚Äô data or records,
e.g., (range of) SAT scores, competition records and ranks, extracurricular activities, or vol-
unteer work. Similar circumstances apply to applications and corresponding decisions in job
recruitment and other related domains.

‚Ä¢ Credit scores and the associated domains: When it comes to evaluating and identifying every-
one‚Äôs Ô¨Ånancial creditworthiness based on credit scores, people have the following concerns:
are their credit scores computed/treated fairly [49] and whether they have the ability to iden-
tify and contest any (potentially) unfair credit decisions [52]. Similar circumstances apply
to credit card applications [27] and a variety of loans, i.e., domains where credit scores may
be taken into account. In this paper, we demonstrate potential privacy hazards that could
be brought on by a bluntly disclosed ATR in credit card applications via various types of
transparency schemes and fairness measures using a synthetic (Section 3) and a real dataset
(Appendix B).

‚Ä¢ Medical or pharmocogenetic models: As noted in [53], a pharmocogenetic model has been
built to predict proper dosages for patients based on their clinical histories, demographics,
and genotypes. However, it has been shown in [40] that once accurate information about the
model is leaked (or obtained through hacking), it can be utilized by an attacker to identify
patients‚Äô genotypes, which could be exploited to further infer other private information, e.g.,
risk of getting a particular disease or someone‚Äôs family ancestry. In the ATR setting, we focus
on a related scenario where an adversary has no ability to access the pharmocogenetic model
internals but can merely gain model information from an announced ATR. In such a case,
our proposed privacy protection scheme can be applied to preserve patients‚Äô privacy and
their genotype information.

‚Ä¢ Open Government: It has been reported in [36] that nowadays governments utilize algorithms
to detect or to determine a variety of issues, such as illegal insider trading, eligibility for pub-
lic health beneÔ¨Åts, and tax evasion. In this regard, the Open Government organization [3]
aims to bring transparency to the data and the algorithms used by governments, aiming for
people and society to supervise governments‚Äô actions and decisions. However, opening a
government blackbox can be very dangerous and can bring catastrophic results to society if
the released information is not carefully treated, and hence it is crucial to have a provable
privacy-preserving scheme for any planned-to-disclose information to protect people‚Äôs pri-
vacy and secret information (tax data, health/medical records, banking information, business
processes, and so on).

‚Ä¢ Online advertising: ML algorithms can be biased [12, 86], and it has been shown that bias
also appears in online advertising, one of the ML applications that we probably experience
daily. In [78] authors indicate a bias and a privacy issue associated with online advertising
settings, particularly when users select the "Rather Not Say" category of gender. In addition,
[24] also found that setting the gender to female results in receiving fewer instances of
advertisements related to high paying jobs as compared to setting it to male. With concerns
about ML bias and the purpose of the collected data, GDPR and CCPA stipulate rights to
explanation and non-discrimination for data subjects; ad providers are required to respond
to data subjects‚Äô requests regarding what data has been collected, how data is used, and if
the applied ML models treat them fairly. Thus, all the disclosed information in an ATR may
need to be further processed to protect data subjects‚Äô privacy.

Achieving Transparency Report Privacy in Linear Time

5

2.2 Ethicality
When our proposed privacy protection scheme is applied to an ATR, the announced information
regarding the opaque decision process may be more or less distorted, and the announced measured
fairness/bias may also deviate from the true one. This may raise concerns about the ethicality
(manner of being ethical) of the process, i.e., whether the perturbed information could mislead the
public into trusting or believing that a biased decision process is fair, and vice versa.

Similarly to privacy preservation in data-mining (PPDM) and data-publishing (PPDP), a com-
mon theme is to Ô¨Ånd an optimal trade-oÔ¨Ä between utility and privacy, subject to a certain degree
of privacy guarantee for data subjects. In the context of ATRs - although both transparency and
privacy are major principles in data ethics [60, 82] - we believe that data subjects‚Äô privacy should
have higher priority [6]. Similarly to PPDM or PPDP, an auxiliary note could be appended with
the announced information indicating that some listed information might be anonymized or per-
turbed for data subjects‚Äô privacy, which could help the public understand how to interpret the
disclosed information appropriately. Moreover, in light of this, in this work, we propose a Ô¨Ådelity
measure (Section 5.2) for the announced decision mappings and characterize the inÔ¨Çuence of pri-
vacy perturbation on the measured fairness (Section 5.3). This information can also be disclosed
with the announced ATR in order to further assist the information recipients in understanding the
range of true measures.

3 DEMONSTRATING PRIVACY LEAKAGE VIA AN ATR

As a necessary and important step, we Ô¨Årst motivate our research by comprehensively demonstrat-
ing via an example consumer database of how a data subject‚Äôs (i.e., consumer‚Äôs) private information
can be leaked via an announced algorithmic transparency report (ATR). In this work we only focus
on reports that provide a rationale on the use of ML models to process individual records. As sec-
tion structure, we start by brieÔ¨Çy reviewing transparency approaches on which privacy leakages
can be induced, and follow it up with a speciÔ¨Åc example of privacy leakage on each transparency
approach.

3.1 Algorithmic Transparency Report (ATR) in a Nutshell

ML models used to make decision on consumer individuals are often opaque to the latter, and act
as blackboxes. A survey of popularity-gaining transparency schemes to explain ML blackboxes is
provided in [46]. A common representative (from the survey) transparency approach collects both
input data and labeled outputs (decision outcomes) as a training dataset, to train an ML surrogate
model (e.g., linear model, logistic regression, decision tree, decision rules) to mimic the behavior of
the blackbox. Popular methods include Anchors [74] and PALM [57]. The output of such learned
behavior must be interpret-able (understandable) by humans. Another common approach (e.g., [18,
38]) extracts certain important ‚Äúproperties‚Äù from blackbox models, such as contributions of input
features, to model outputs. SpeciÔ¨Åcally, these transparency schemes measure feature importance
(based on the underlying ùê∑), using both amplitude and sign to represent importance/inÔ¨Çuence of
input features, where larger amplitude represents greater inÔ¨Çuence, and the sign indicates positive
or negative eÔ¨Äect on the output. Popular methods include LIME [73], FIRM [93], QII [23], and
Shapley Value [56], PDP [41], ICE [43], and ALEPlot [10]. In addition to transparency schemes,
an ATR may also provide information regarding whether a decision algorithm or ML model is
biased against a certain group or individual - in other words, an ATR may measure individual or
group fairness of ML-based decisions based on the diÔ¨Äerent desired metrics discussed in existing
literature [15, 22, 30, 35, 54, 55, 92]. We refer readers to Appendix A for detailed deÔ¨Ånitions of
various individual and group fairness measures. In what follows, we investigate and demonstrate

6

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 1. A Synthetic Credit Card Application Scenario

Gender

Input Attributes
Annual
Income
< 100k
100k‚àº200k
> 200k
< 100k
100k‚àº200k
> 200k

F
F
F
M
M
M

Adversaries‚Äô Knowledge
ATR
Decision
Rule
0
0
1
0
0.5
1

Side-Info
Census
Statistics
93.1%
5.7%
1.2%
84.2%
12.3%
3.5%

Popu-
lation
139
9
2
117
18
5

privacy leakage instances via various kinds of transparency schemes and fairness measures, given
honest-but-curious adversaries.

3.2 Privacy Leakage via Interpretable Surrogate Models

As noted, transparency schemes can interpret a blackbox‚Äôs rules in a human-understandable man-
ner, such as decision rules or decision trees. Here, we explain how such transparent information
can hurt a data subject‚Äôs privacy. Without loss of representativity, here we set up a synthetic sce-
nario, in which we consider the existence of a perfect interpretable surrogate model4, to illustrate
the possibility of causing a catastrophic privacy leak.

Consider the following synthetic credit card application scenario (summarized in Table 1). A
credit card application takes several input attributes from applicants, while the bank‚Äôs decision
process only depends on two input attributes: the applicants‚Äô annual income and their gender
(which, depending on the country, may be illegal and in those cases should not be used in any
decision process). Due to the suspicious diÔ¨Äerences in approval rates between male and female
applicants, a third-party regulatory agency actively takes action. It collects all applicants‚Äô data and
their received decisions, and trains an (assumed perfect) interpretable surrogate model, disclosing
the decision rules used in the credit card application to all past applicants, as follows
ùëë ({Income} > 200k) = 1,
ùëë ({Income} ‚àà 100k‚àº200k, Male) = 0.5,

where ùëë (¬∑) is decision rule representing the probability of receiving a positive decision given the
condition. An equivalent if-then decision rule form is the following

if Income > 200k, then Positive Decision;
if 100k ‚â§ Income ‚â§ 200k ‚àß Male, then Random;
otherwise, then Negative Decision.

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥
Ô£≥

Note that other interpretable surrogate models such as a decision tree or logistic regression can
also be equivalently expressed by decision rule ùëë (¬∑).

Next, we demonstrate how data subjects‚Äô sensitive information (annual income in this scenario)
could be leaked. Revisit Table 1 in which the key input attributes, population, and decision rule of
the credit card application are listed. Population of applicants are aggregated according to decision

4We consider the most privacy-catastrophic case, a perfect interpretation, which has the most accurate information in an
ATR.

Achieving Transparency Report Privacy in Linear Time

7

regions, i.e., the regions of input attributes partitioned by decision rules. Here the population pro-
portion among decision regions refers to the U.S. census data, and adversaries assumed blind to
population of applicants utilize the U.S. census data as side-information to estimate, for each de-
cision region, the percentage of the total number of male/female applicants (listed in the ‚ÄúCensus
Statistics‚Äù attribute; for instance, the value 93.1% in Table 1 represents the following: given that the
decision region is {Annual Income < 100k; Gender=Female}, 93.1% of female applicants belong to
this region). Adversaries know public information of targeted applicants and also know decision
rules from an announced ATR.

When an ATR containing such decision rule is negligently announced, as it reveals strong depen-
dencies between annual income and decisions, any female using such a credit card in public in-
stantly tells anyone who has ever seen the report that her annual income is above 200k, which
not only results in a privacy hazard to her, but may also result in unexpected safety concerns. In
such a case, an adversary does not even require auxiliary information to be able to infer someone‚Äôs
secret.

Male credit card owners are also at risk, although not as much. For a male credit card owner, the
conÔ¨Ådence of an adversary believing that his income is above 200k is only around 36%, compared
with 100% in the case of a female owner, while based on census statistics, the conÔ¨Ådence of an
adversary believing that his income is above 200k is merely 3.5%. In other words, once such a
negligent algorithmic transparency report is announced to the public, a high-income (>200k) male
credit card owner‚Äôs risk of exposing annual income information is increased 10 fold.

In summary, releasing precise information of interpretable surrogate models (that can be equiv-
alently expressed by decision rules) can be harmful to data subjects‚Äô privacy, as such information
gives adversaries a clear mapping between input records and received decision. With assistance
from public information and/or side-information, adversaries can abuse algorithmic transparency
to undermine people‚Äôs privacy. The same privacy leakage concern applies when precise infor-
mation of transparency scheme is released in the form of feature importance/interaction, which,
however, in the interest of space, is explicitly demonstrated in Appendix B, using a real dataset.

3.3 Privacy Leakage via Fairness Measures
Recall that one of the main motivation for algorithmic transparency is to understand if a decision-
making algorithm is fair and complies with regulations/law, e.g., the U.S. equal employment op-
portunity commission (EEOC) regulates the ratio of the hiring rates between women and men,
which should not be lower than 80% (80%-rule). In an algorithmic transparency report, such fair-
ness measures may be required upon data subjects‚Äô demands (e.g., GDPR, Article 22).

To this end, consider again the credit card application in Table 1, in which the bank is under sus-
picion of discriminating against female applicants. Upon female applicants‚Äô demands, a regulation
agency gets involved and discloses the following fairness measures for gender: (i) bias in statistical
parity (SP) (DeÔ¨Ånition 9) for male and female applicants, (ii) bias in conditional statistical parity
(CSP) (DeÔ¨Ånition 10) for male and female applicants who have the same level of income. An ATR
listing all the above fairness measures w.r.t. the credit card application is shown in Table 2 (see
Remark 1 for details), which can be announced to the public in an electronic form, e.g., through a
website (e.g., GDPR, Recital 58, information related to the public‚Äôs concerns).

Moreover, a data subject, which is a credit card applicant in our scenario, has the right to inquire
about the decision principle w.r.t. his or her personal data. Mary, a low-income (<100k) female who
would like to know why her applications are always denied, demands information regarding the
decision processing for her record. The response indicates that the approval rate for a low-income
female is 0. If we let ùëëùëñ,ùëó be the decision rule for people in {Yùëñ, Wùëó } in Table 2, by utilizing the census

8

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 2. Fairness Measures for Table 1 in an ATR

Y1 = {F}, Y2 = {M}
W1 = {Annual Income ‚â§ 100k}
W2 = {100k ‚â§ Annual Income ‚â§ 200k}
W3 = {Annual Income ‚â• 200k}
Overall approval rate for female (Y1) = 1.33%;
Overall approval rate for male (Y2) = 10%;
Bias in SP for Y1 and Y2 = 0.0866;
Bias in CSP for {Y1, W1} and {Y2, W1} = 0;
Bias in CSP for {Y1, W2} and {Y2, W2} = 0.5;
Bias in CSP for {Y1, W3} and {Y2, W3} = 0.

statistics as shown in Table 1, and based on the deÔ¨Ånitions of SP and CSP for binary decisions in
(30) and (31), respectively, the information provided in Table 2 tells us the following:

(1)

(2)

(3)

(4)

Overall approval rate for female(Y1) = 0.0133 ‚âà 0.931ùëë1,1 + 0.057ùëë1,2 + 0.012ùëë1,3
Overall approval rate for male(Y2) = 0.1 ‚âà 0.842ùëë2,1 + 0.123ùëë2,2 + 0.035ùëë2,3
Bias in SP for {Y1, W1} and {Y2, W1} = 0 = |ùëë1,1 ‚àí ùëë2,1|
Bias in SP for {Y1, W2} and {Y2, W2} = 0.5 = |ùëë1,2 ‚àí ùëë2,2|
Bias in SP for {Y1, W3} and {Y2, W3} = 0 = |ùëë1,3 ‚àí ùëë2,3|.

(5)
Since Mary just got a reply indicating ùëë1,1 = 0, from (3) and (5), Mary then knows that ùëë1,1 =
ùëë2,1 = 0, ùëë1,3 = ùëë2,3, and from (4), either ùëë1,2 = ùëë2,2 + 0.5 or ùëë1,2 = ùëë2,2 ‚àí 0.5. She can Ô¨Årst assume
ùëë1,2 = ùëë2,2 + 0.5, by plugging the values of ùëë1,1 into (1) and ùëë2,1 into (2), and replacing ùëë2,2 and ùëë2,3
by ùëë1,2 ‚àí 0.5 and ùëë1,3 in (1) and (2), respectively, she gets 0.057ùëë1,2 + 0.012ùëë1,3 = 0.0133 from (1)
and 0.123ùëë1,2 + 0.035ùëë1,3 = 0.1615 from (2). Since ùëëùëñ,ùëó are probabilities, ‚àÄùëñ, ùëó , ùëë1,2 and ùëë1,3 can not be
grater than 1, and thus the obtained equation from (2) is infeasible, which implies the assumption
is wrong. She then knows ùëë1,2 = ùëë2,2 ‚àí 0.5. Repeat the same steps and she will obtain ùëë1,2 = 0.0088
and ùëë1,3 = 1.0692. By understanding that any ùëëùëñ,ùëó can not be greater than 1 and this is probably
caused by the mismatch between the census statistics and the true distribution, she would thus
update ùëë1,3 = 1 and thus obtain ùëë1,2 = 0.0013 ‚âà 0; these estimates are very close to the true
the values. In addition, Mary can use the obtained ùëë1,2 and ùëë1,3 to further acquire ùëë2,2 and ùëë2,3.
Therefore, by utilizing the decision processing rule for her record and the publicly announced
fairness measures, she can obtain accurate decision rules for the credit card application. As in
Section 3.2, we know that a privacy disaster can happen when accurate decision rules are released
or hacked. The adversary Mary now can utilize her hacked decision rules to infer other applicants‚Äô
income.

From the above demonstrations, we have seen that a negligent ATR can result in a serious hazard
to data subjects‚Äô privacy. In the following sections, we formalize the privacy leakage problem,
and propose the corresponding properties and solutions. We will revisit the examples demonstrated
above again in Section 8, with our proposed solutions applied.

Remark 1. Here we demonstrate how the numbers in Table 2 are calculated based on Table 1. The
deÔ¨Ånitions of SP and CSP for binary decisions can be found in (30) and (31), respectively.

Overall approval rate for female (Y1) = (2 √ó 1)/(139 + 9 + 2) = 1.33%;
Overall approval rate for male (Y2) = (18 √ó 0.5 + 5 √ó 1)/(117 + 18 + 5) = 10%;
Bias in SP for Y1 and Y2 = |1.33% ‚àí 10%| = 0.0866;
Bias in CSP for {Y1, W1} and {Y2, W1} = |0 ‚àí 0| = 0;

Achieving Transparency Report Privacy in Linear Time

9

Set of 
Input 
Attributes

‚Ä¶

‚Ä¶

Decision Blackbox

Decision
Outcomes

Fig. 2. A representative illustration of a decision blackbox

Bias in CSP for {Y1, W2} and {Y2, W2} = |0 ‚àí 0.5| = 0.5;
Bias in CSP for {Y1, W3} and {Y2, W3} = |1 ‚àí 1| = 0.

4 PROBLEM SETUP

In the following sections, we formalize and analyze the privacy leakage problem in ATR. To be-
gin with, in this section, we provide essential notations listed in Table 3 and useful deÔ¨Ånitions
for problem setup, followed by adversarial settings and deÔ¨Ånition of privacy violation in ATRs
formally.

4.1 Decision Mapping
Fig. 2 illustrates an opaque decision-making blackbox, which is essentially an unknown decision
mapping function deÔ¨Åned as follows.

DeÔ¨Ånition 1. (Decision Mapping [30]) Consider a decision process as illustrated in Fig. 2, where
ùëã = {ùëãùëò | ùëò = 1, . . . , ùêæ } is a set of input attributes, ùê¥ the output attribute (decision outcomes), and
A the range of ùê¥. Recall that Œî(ùëÜ) is a set of probability distributions over ùëÜ. A decision mapping
ùê∑ A : Rùëã ‚Üí Œî(A) is a function mapping from the range of input attributes to a set of probability
distributions over the range of decision outcomes. Formally,

ùê∑ A (ùëã ) = {ùëÉùê¥ |ùëã (ùê¥ = ùëé|ùëã ) | ‚àÄùëé ‚àà A} = {ùê∑ùëé (ùëã ) | ‚àÄùëé ‚àà A}.

Particularly, for binary decisions (0 =‚Äònegative‚Äô and 1 =‚Äòpositive‚Äô), we let

ùê∑ A (ùëã ) =

ùê∑1 (ùëã ) = ùëë (ùëã )
ùê∑0 (ùëã ) = 1 ‚àí ùëë (ùëã )

(

, for ùëé = 1
, for ùëé = 0,

(6)

(7)

where ùëë (ùëã ) is decision rule [22] representing probabilities of mapping from input space to the positive
decision outcome.

Clearly, decision mapping is more comprehensive, while decision rule is more concise and con-

venient for an ATR, e.g., decision rule in Table 1.

As noted in Section 3.1, an ATR opens an opaque decision blackbox via transparency schemes
such as an interpretable surrogate model (a surrogate of ùê∑ A) or feature importance/interaction (a
function of ùê∑ A). In addition, an ATR may also contain fairness measures (functions of ùê∑ A, see
Appendix A). Clearly, an ATR is in general a function of decision mapping ùê∑ A (when there is
no confusion, we omit the subscript and simply write ùê∑ in the rest of the paper for conciseness);
while released, the mapping from decision inputs to outputs are made public, and thus it is very
crucial to ensure the reverse inference is not possible, or limited with low conÔ¨Ådence. To explicitly
characterize the reverse inference, we Ô¨Årst need to understand the capability of the adversaries.

is a legitimate participant in a communication protocol who will not deviate from the deÔ¨Åned

protocol but will attempt to learn all possible information from legitimately received messages.

10

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

4.2 Adversarial Settings
For the privacy leakage problem brought on by releasing ATRs, we consider honest-but-curious (or
curious-but-not-malicious) adversaries, i.e., adversaries who only perform legitimate actions and
will not deviate from the deÔ¨Åned protocol but would like to learn as much as possible (including
others‚Äô secrets); in our ATR setting, this implies that an adversary will not hack into the system
and steal information but only acquires as much as possible information that is made public or
is widely-available. For example, the adversaries may know public information about his friends,
e.g., gender, race, ZIP code, age, etc; the adversaries may also have knowledge about the census
data [4] providing side-information (with week inference) between public and private attributes,
e.g., joint distributions between age, race, marriage status, household size, and income. Such kind
of adversaries are ubiquitous, making privacy leakage via released ATRs omnipresent.

It‚Äôs worth noting that the adversaries do not have access to all the input features and all the
output responses (decision outcomes), and thus are not able to extract any information about the
blackbox from the limited knowledge. Instead, the adversaries may just know the public informa-
tion and the received decisions of the targeted several individuals. The reason that we particularly
focus on honest-but-curious adversaries in this paper is that we would like to convey an important
message that candidly releasing an ATR could result in privacy hazards even for weak adversaries
who are not able to probe or hack into the system but possess some public information and/or widely-
available side-information. Moreover, more powerful adversaries who may have access to all input
features and output responses can train a more powerful surrogate model (as it need not be an
interpretable model) to mimic the original model, and thus can obtain more accurate information
w.r.t. the decision blackbox, as compared to what is provided in an announced ATR. In such a case,
the privacy hazard is not due to the ATR, as adversaries have already obtained something more
powerful (resulting in stronger inference), and thus such an adversarial setting is not meaningful
for ATR.

In practice, since honest-but-curious adversaries can be ubiquitous, the background knowledge
that adversaries may possess could be diverse and unknown to agencies in charge of ATRs. There-
fore, it is important that agencies should consider the worse-case scenario, i.e., the most information
that an honest-but-curious adversary can possess (which is the worst-case weak adversary). Hence,
the agencies should assume that an adversary could possess precise and full knowledge of

‚Ä¢ the range Rùëã and the joint distribution ùëÉùëã (x) of all inputs x;
‚Ä¢ all public records (a.k.a. quasi-identiÔ¨Åer (QID) [5, 83, 84]) xU of speciÔ¨Åc individuals;
‚Ä¢ the received decisions ùëé of the targeted individuals;
‚Ä¢ the internal privacy parameters (e.g., the predeÔ¨Åned required privacy level) of the privacy

protection scheme M used for an ATR, if any.

The following information is assumed in general unknown (or known with little conÔ¨Ådence) by
adversaries before seeing an ATR: (i) data subjects‚Äô private records xS and (ii) the decision mapping
ùê∑ of the black-box. Given the above adversarial settings, we clearly deÔ¨Åne privacy violation in
releasing ATRs in the following.

DeÔ¨Ånition 2. The release of an ATR is privacy violating if any private or conÔ¨Ådential information
of any data subject to whom decision algorithms, disclosed in the ATR, have been applied can be
(unintentionally) inferred by any honest-but-curious unauthorized individual or entity to whom the
ATR is released, with conÔ¨Ådence exceeding a tolerable threshold, due to the release of the ATR.

Remark 2. Given DeÔ¨Ånition 2, inferring attribute values due to high correlations between at-
tributes, e.g., knowing people who have ovarian cancer are female, should not be mistaken as
privacy breach (not private information; not via an ATR). Similarly, releasing ATRs to a doctor for

Achieving Transparency Report Privacy in Linear Time

11

Table 3. Notation

Set of all public attributes
Set of all private attributes
Random variable (r.v.) of attribute ùëò
= {ùëãùëò | ‚àÄùëò ‚àà U}; collection of r.v.‚Äôs of all public attributes
= {ùëãùëò | ‚àÄùëò ‚àà S}; collection of r.v.‚Äôs of all private attributes
= (ùëã U, ùëã S); collection of r.v.‚Äôs of all attributes
Range of ùëã ; the universe of inputs; Rùëã = RùëãU √ó RùëãS
An instance of ùëã U
An instance of ùëã S
= (xU, xS), an instance of ùëã
= {x‚Ä≤ ‚àà Rùëã | x‚Ä≤
The r.v. of decision outcome
Range of ùê¥
Aleatory probability; chance

U = xU } = range of (xU, ùëã S)

Epistemic probability; credence or belief
= {ùëÉ (ùê¥ = ùëé|ùëã ) | ‚àÄùëé ‚àà A}; decision mapping (DeÔ¨Ånition 1)
= { ÀúùëÉ (ùê¥ = ùëé|ùëã ) | ‚àÄùëé ‚àà A}; announced decision mapping
= ùëÉ (ùê¥ = 1|ùëã ); decision rules (DeÔ¨Ånition 1)
= ÀúùëÉ (ùê¥ = 1|ùëã ); announced decision rules
A privacy protection scheme for an ATR

U
S
ùëãùëò
ùëã U
ùëã S
ùëã
Rùëã
xU
xS
x
ùëáxU
ùê¥
A
ùëÉ (¬∑)
ÀúùëÉ (¬∑)
ùê∑ (ùëã )
Àúùê∑ (ùëã )
ùëë (ùëã )
Àúùëë (ùëã )
M

the ML-assist diagnoses of his patients should not be considered as privacy violation (an authorized
personal).

4.3 Comparison with PPDM and PPDP
The main diÔ¨Äerences between privacy preservation in ATRs and privacy preservation in data-
mining (PPDM) and data-publishing (PPDP) are their adversarial settings.

More speciÔ¨Åcally, in the PPDM setting, a dataset is not published; instead, users or data analysts
send queries (a set of pre-deÔ¨Åned/allowed deterministic functions, e.g., average, count, median,
max, and min) to the curator, and the curator generates the corresponding query outputs based
on the dataset. In such a setting, if the pre-deÔ¨Åned queries are carefully designed, an adversary (a
malicious user), in general, is not able to determine the direct mappings between public and private
attribute values of a record nor any private information of any individual from any single query out-
put. However, since query functions are known in advance, an adversary can send multiple queries
and compare the obtained results to extract data subjects‚Äô private information from the outputs. In
this regard, diÔ¨Äerential privacy (DP) [21, 31] is usually adopted to preserve privacy in PPDM. In
summary, the main diÔ¨Äerences between the settings in PPDM and ATRs are (i) in PPDM, mappings
between public and private attributes are in general not available, or may be known only partially,
while these could be known statistically in the ATRs setting; (ii) an adversary can send multiple
(deterministic) queries and/or collude with other adversaries to extract data subjects‚Äô private infor-
mation, while an ATR is a one-shot announcement, and the announced decision mappings from
the inputs to the outputs could be probabilistic (i.e., random decisions).

In the PPDP setting, a dataset is published. Therefore, the mappings between public and private
attributes are clearly known to an adversary (much stronger than auxiliary or side-information).
When the published dataset shows uniqueness of a public record or unique relationship between

12

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

certain public and private attributes, an adversary can utilize such uniqueness to identify data sub-
jects‚Äô private information. Therefore, several techniques (ùëò-anonymity [75, 76], ùëô-diversity [61, 62],
etc.) are proposed in the literature to obfuscate such uniqueness in order to preserve data subjects‚Äô
privacy. In summary, the main diÔ¨Äerences between the settings in PPDP and ATRs are (i) unlike
in PPDP where privacy is leaked due to strong inference between attributes, in ATR, as we have
emphasized in Section 4.2, we only consider the case of weak correlation/inference between public
and private attributes, i.e., an adversary is not able to identify any private information with high
conÔ¨Ådence before seeing an ATR, while the conÔ¨Ådence could be dramatically enhanced after an
ATR is released; (ii) in PPDP, depending on the application, there may or may not exist output
attributes. When there exist output attributes, as the dataset is published, all output attribute val-
ues are available to an adversary, which could provide strong inference between some sensitive
attributes and the output attributes (for learning purposes), and thus we need to guarantee that
any output attribute value is not directly associated with any individual; while in ATRs, we con-
sider the case that a decision outcome could be directly associated with an individual (credit card
applications, university admissions, etc.), but an adversary knows a few decision outcomes only.

5 PRIVACY, UTILITY, AND MEASURED FAIRNESS

Given clear context of adversarial settings and deÔ¨Ånition of privacy violation for releasing ATRs,
we next formulate privacy leakage caused by inference attacks, and propose a privacy-preserving
mechanism for ATRs. To this end, in this section, we provide a privacy measure to mathematically
characterize and formulate the degree of privacy leakage. Based on the proposed privacy measure,
we formulate the requirements for a privacy-preserving mechanism for ATRs, and introduce a util-
ity measure to characterize the inÔ¨Çuences caused by the proposed privacy-preserving mechanism;
similarly, we address the inÔ¨Çuence of the proposed privacy mechanism on fairness measures.

5.1 Privacy Measure and Privacy-Preserving Mechanism
Recall in Section 3 we have seen privacy leakage disasters when decision rules were divulged. The
fundamental problem is that transparency schemes as well as fairness measures are closely related
to, or functions of, decision mapping ùê∑, and more importantly, if ùê∑ provides strong inference from
public knowledge to sensitive records, once it is utilized in an ATR and obtained by an adversary,
the adversary can utilize it to further acquire data subjects‚Äô secrets with high conÔ¨Ådence.

In light of this, here we propose the following: a carefully processed ùê∑, denoted by Àúùê∑, should
be adopted as a substitute for ùê∑ in an ATR for preserving data subjects‚Äô privacy. Àúùê∑ should satisfy
certain privacy requirements and can be safely announced (if an ATR chooses to release an inter-
pretable surrogate model) or utilized by transparency schemes and fairness measures provided in
an ATR.

In such a case, when an ATR is released, an adversary acquires information about Àúùê∑, and could

ùëÉùëã , Àúùê∑
‚àí‚àí‚àí‚àí‚Üí ùëã Si which maps from inference source
further utilize it in an inference channel hùëã U, ùê¥
ùëã U and ùê¥ to sensitive attribute values ùëã S. (When the context is clear, we will omit ùëÉùëã and Àúùê∑
above the arrow for simplicity.) Based on the adversarial settings in Section 4.2, one reasonable
privacy measure to characterize the above inference (caused by Àúùê∑) is the maximum conÔ¨Ådence
of an adversary in inferring any data subject‚Äôs sensitive value ùëã S, which is also known as the
worst-case posterior vulnerability [34]. In other words, even if an adversary knows Àúùê∑ and further
utilizes it to perform inference attacks, the maximal conÔ¨Ådence that the adversary can have is
carefully controlled in advance to prevent privacy violation. In this regard, privacy measures of
the announced version of decision mapping Àúùê∑ used in an ATR should reÔ¨Çect the maximal degree of

Achieving Transparency Report Privacy in Linear Time

13

an adversary‚Äôs conÔ¨Ådence in inferring any data subject‚Äôs secret via Àúùê∑. Consider the case in which
S is a singleton set, we deÔ¨Åne maximum conÔ¨Ådence formally in the following.

DeÔ¨Ånition 3. (Maximum ConÔ¨Ådence) Given the adversarial settings in Section 4.2 and an inference
channel hùëã U, ùê¥ ‚Üí ùëã Si, the conÔ¨Ådence of inferring a certain sensitive attribute value ùë• S from a
certain inference source (xU, ùëé), denoted by conf (xU, ùëé ‚Üí ùë• S), is the posterior epistemic probability
of ùë• S given xU and ùëé as follow

conf (xU, ùëé ‚Üí ùë• S) = ÀúùëÉùëãS |ùëãU,ùê¥ (ùë• S |xU, ùëé).
The maximum conÔ¨Ådence of inferring a speciÔ¨Åc sensitive attribute value ùë• S from any inference sources,
denoted by Conf (ùëã U, ùê¥ ‚Üí ùë• S), is deÔ¨Åned as

Conf (ùëã U, ùê¥ ‚Üí ùë• S) , max
xU,ùëé
Accordingly, the maximum conÔ¨Ådence of inferring any sensitive attribute value from any inference
channel is

{conf (xU, ùëé ‚Üí ùë• S)}.

Conf (ùëã U, ùê¥ ‚Üí ùëã S) , max
xU,ùëé,ùë•S

{conf (xU, ùëé ‚Üí ùë• S)}.

The privacy requirement, similar to conÔ¨Ådence bounding [88, 89], ùõΩ-likeness [19], and privacy
enforcement in [58], restricts the maximum conÔ¨Ådence on inferring any sensitive attribute by a
conÔ¨Ådence threshold, a pre-determined privacy parameter ùõΩ.
DeÔ¨Ånition 4. (ùõΩ-Maximum ConÔ¨Ådence) In an algorithmic transparency report, Àúùê∑ satisÔ¨Åes the pri-
vacy requirement ùõΩ-Maximum ConÔ¨Ådence if Conf (ùëã U, ùê¥ ‚Üí ùëã S) ‚â§ ùõΩ.

Lemma 1. The privacy requirement ùõΩ-Maximum ConÔ¨Ådence imposes the following constraints to
the announced decision mapping Àúùê∑, ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A,
Àúùê∑ùëé (x)ùëÉùëã (x)

Proof. Please refer to Appendix D for detailed proof.

Àúùê∑ùëé (x‚Ä≤)ùëÉùëã (x‚Ä≤)

‚â§ ùõΩ.

x‚Ä≤ ‚ààùëáxU
√ç

(8)

(cid:3)

Remark 3. Note that a privacy requirement which only prevents an adversary from correctly infer-
ring any sensitive attribute value of any data subject is insuÔ¨Écient. The reason is that an adversary
can possess the knowledge of the privacy protection scheme and its internal privacy parameters. If
the privacy requirement allows an adversary to incorrectly infer wrong sensitive values with arbi-
trary high conÔ¨Ådence, since an adversary may know the privacy requirement, he/she may perceive
that any sensitive attribute value which can be inferred with conÔ¨Ådence higher than the threshold is
an incorrect one; this could become additional side-information for the adversary. An adversary
can further utilize such extra side-information to narrow down the range of conjectures, which
enhances the conÔ¨Ådence of correctly guessing the right sensitive value. The enhanced conÔ¨Ådence
could result in exceeding the privacy threshold, and thus cause a privacy hazard.

The advantage of using maximum conÔ¨Ådence as a privacy measure is that it results in intuitive
understanding of ùõΩ. This could be important when a privacy scheme is used for an ATR, the regula-
tion may require a plain explanation for the adopted privacy scheme as well as the corresponding
settings and meanings of its parameters. Alternatively, one can use other privacy measures, e.g.,
minimum uncertainty (Appendix C), which is essentially conveying the same concept as maximum
conÔ¨Ådence, but the privacy parameter ùõæ grows with the strength of privacy.

A privacy protection scheme M takes the original/true decision mapping ùê∑ as the input and
generates a privacy-preserving decision mapping Àúùê∑ safe for announcement with careful processing

14

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

based on privacy requirements. Inevitably, the original ùê∑ would diÔ¨Äer from the generated Àúùê∑, which
is a distorted/perturbed but private version of ùê∑. In the next section, we introduce a utility measure
to characterize the distortion.

5.2 Utility Measure: Fidelity
In this section, we introduce an appropriate utility measure for our problem. Given proposed
M
‚àí‚àí‚Üí Àúùê∑, an appropriate utility measure should characterize the distortion from Àúùê∑ to ùê∑, or quan-
ùê∑
tify the quality of faithfulness of Àúùê∑ (compared with ùê∑), and hence, particularly, is named Ô¨Ådelity
measure hereafter. By imposing Ô¨Ådelity constraints to M, the maximal distortion between Àúùê∑ and
ùê∑ is guaranteed to be bounded accordingly.

DeÔ¨Ånition 5. (ùõø-Fidelity) A privacy perturbation method M : Œî(A) ‚Üí Œî(A) satisÔ¨Åes ùõø-Ô¨Ådelity,
ùõø ‚àà [0, 1], if ‚àÄx ‚àà Rùëã and ‚àÄùëé ‚àà A, we have

| Àúùê∑ùëé (x) ‚àí ùê∑ùëé (x)| ‚â§ 1 ‚àí ùõø.
DeÔ¨Ånition 6. (ùõº-Fidelity) A privacy perturbation method M : Œî(A) ‚Üí Œî(A) satisÔ¨Åes ùõº-Ô¨Ådelity,
ùõº ‚àà [0, 1], if ‚àÄx ‚àà Rùëã and ‚àÄùëé ‚àà A, we have

(9)

ùõº ‚â§ Àúùê∑ùëé (x)/ùê∑ùëé (x) ‚â§ 1/ùõº.

(10)

In the most general form, deÔ¨Ånition of Ô¨Ådelity can be

Àúùê∑ùëé (x)min ‚â§ Àúùê∑ùëé (x) ‚â§ Àúùê∑ùëé (x)max,
which describes the restriction (the allowed range) of distortion of Àúùê∑ in a very general manner.
The corresponding equivalent representations for ùõø- and ùõº-Ô¨Ådelity are
ùê∑ùëé (x) ‚àí (1 ‚àí ùõø) ‚â§ Àúùê∑ùëé (x) ‚â§ ùê∑ùëé (x) + (1 ‚àí ùõø),

(11)

(12)

ùõºùê∑ùëé (x) ‚â§ Àúùê∑ùëé (x) ‚â§

1
ùõº

ùê∑ùëé (x),

(13)

in which the upper and the lower bounds Àúùê∑ùëé (x)max and Àúùê∑ùëé (x)min are functions of ùê∑ and ùõø, or ùõº.
In practice, ùõø and ùõº should not be far from 1.

5.3 Influence of Privacy on Measured Fairness
As demonstrated in Section 3.3, since fairness measures are functions of decision mapping/rule,
releasing precise fairness measures could bring privacy hazards. On account of this, the released
fairness measures should be computed based on privacy-preserving Àúùê∑, which, however, would in-
Ô¨Çuence and distort the measured bias ùúÄ. In this section, we show that by knowing the Ô¨Ådelity con-
straints to M, we are able to characterize and bound the distortion of the measured fairness/bias.
Fig. 3 is a representative illustration of the true bias ùúÄ F and the measured bias ÀúùúÄ F inÔ¨Çuenced by
M, where F denotes a (general or speciÔ¨Åc) set of fairness deÔ¨Ånitions on which bias is computed.
Since the true decision mapping ùê∑ should not be released and utilized to compute fairness mea-
sures, the true bias computed based on ùê∑ is generally unknown. A natural question may arise: by
knowing ÀúùúÄ F, and the degree of Ô¨Ådelity of Àúùê∑ (ùõø or ùõº), what can we know about ùúÄ F? The following
lemma answers the question: if the maximum distortion from Àúùê∑ to ùê∑ is known, the maximum
distortion from ÀúùúÄ F to ùúÄ F can be known, and thus the range of ùúÄ F can be known.

Lemma 2. Let Ftv denote the set of all total-variation-based fairness deÔ¨Ånitions, and Frm the set of all
M
‚àí‚àí‚Üí Àúùê∑, if M satisÔ¨Åes ùõø-Ô¨Ådelity,
relative-metric-based fairness deÔ¨Ånitions (see Appendix A). Given ùê∑

Achieving Transparency Report Privacy in Linear Time

15

Privacy Parameter:

Fidelity Parameter:

or

or

-fidelity/
-fidelity

(cid:34)

(cid:88)(cid:81)(cid:78)(cid:81)(cid:82)(cid:90)(cid:81)

Fig. 3. A depiction of how fidelity of Àúùê∑ can aid in characterizing the diÔ¨Äerence between the measured bias
ÀúùúÄ F and the true bias ùúÄ F, where F denotes a (general or specific) set of fairness definitions on which bias is
computed. In Section 3, we have seen that releasing ùê∑ and ùúÄ F can cause privacy leakage and thus should be
prohibited, so that ùúÄ F is unknown to the public. However, if the fidelity parameter used in M is known, we
are able to characterize ùúÄ F by ÀúùúÄ F based on Lemma 2.

we can guarantee the measured bias ÀúùúÄ Ftv satisÔ¨Åes

| ÀúùúÄ Ftv ‚àí ùúÄ Ftv | ‚â§ min{2(1 ‚àí ùõø), 1}.

If M satisÔ¨Åes ùõº-Ô¨Ådelity, we can guarantee the measured bias ÀúùúÄ Frm satisÔ¨Åes

| ÀúùúÄ Frm ‚àí ùúÄ Frm | ‚â§ min{‚àí2 log ùõº, 1}.

Proof. By applying the reverse triangle inequality [85], the results trivially follow.

(14)

(15)

(cid:3)

6 PRIVACY-FIDELITY TRADE-OFF
Strong privacy perturbation could cause serious distortion on the announced information includ-
ing decision rules and fairness measures, and thus a privacy protection scheme should preserve
privacy while guaranteeing a certain degree of Ô¨Ådelity to the announced information; this turns
out a privacy-Ô¨Ådelity trade-oÔ¨Ä problem. In this section, we mathematically formulate the trade-oÔ¨Ä
problem, and revisit existing algorithms that can eÔ¨Éciently solve this problem.

6.1 Optimization Formulation
We describe the privacy-Ô¨Ådelity trade-oÔ¨Ä problem in the following: given Ô¨Ådelity constraints, we
would like to Ô¨Ånd the greatest privacy (the smallest ùõΩ) that we can achieve. The problem is mathe-
matically formulated as follow. For conciseness, we omit the subscript of all probability measures
and simply write, e.g., ùëÉ (x) instead of ùëÉùëã (x).

OPT(Rùëã √óA) :

min
Àúùê∑

ùõΩ

s.t.

ùëÉ (x) Àúùê∑ùëé (x)

ùëÉ (x‚Ä≤) Àúùê∑ùëé (x‚Ä≤)

x‚Ä≤ ‚ààùëáxU
√ç

‚â§ ùõΩ , ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A

Àúùê∑ùëé (x) ‚â§ Àúùê∑ùëé (x)max , ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A
Àúùê∑ùëé (x) ‚â• Àúùê∑ùëé (x)min , ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A
Àúùê∑ùëé (x) ‚â• 0 , ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A
Àúùê∑ùëé (x) = 1 , ‚àÄx ‚àà Rùëã .

√ïùëé ‚ààA

(OPT)

(16a)

(16b)

(16c)

(16d)

(16e)

(16f)

16

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

The Ô¨Årst constraint in (16b) is the privacy constraint ùõΩ-Maximum ConÔ¨Ådence deÔ¨Åned in DeÔ¨Ånition
4 and Lemma 1, and the last two constraints in (16e) and (16f) are probability distribution condi-
tions. The second and the third constraints in (16c) and (16d) are Ô¨Ådelity constraints introduced
in (11). Its corresponding representations for ùõø- or ùõº-Ô¨Ådelity can be found in (12) and (13), respec-
tively. The objective in (16a) is to Ô¨Ånd the minimal ùõΩ subject to the feasibility of Àúùê∑ based on the
above-mentioned constraints. A careful observation of the optimization problem (OPT) will reveal
that it is an equivalent formulation of a generalized linear fractional programming (LFP) problem
[94].

6.2 Drawbacks of Existing Methods to Solve Generalized LFP Problems
It has been known that a generalized LFP is quasi-convex and not reducible to a linear program-
ming (LP) problem; however, it can be solved as a sequence of LP feasibility problems [16], i.e.,
solving numerous sub-level LP problems iteratively according to bisection method. By eÔ¨Écient
algorithms such as interior point method, the solution of a LP problem can be obtained in pseudo-
polynomial time ùëÇ ( ùëõ3
log ùëõ ùêø) [9], where ùëõ is the number of variables, ùêø is the input size, i.e., the length
of the binary coding of the input data to represent the problem, which is roughly proportional to
the number of constraints. For our problem, based on (16b)‚Äì(16f), it is clear that the number of
constraints is proportional to |Rùëã √óA|, which is the number of variables ùëõ = | Àúùê∑ùëé (x)|, exponential
in the number of input attributes ùêæ. For example, suppose the cardinality for each input attribute
is consistent, e.g., |ùëãùëò | = ùëô, ‚àÄùëò = 1, . . . , ùêæ, we have ùëõ = |Rùëã √óA| = 2 ¬∑ ùëô ùêæ and roughly 4ùëõ = 8 ¬∑ ùëô ùêæ
constraints. Even for a relatively small example, e.g., a binary decision process with ùêæ = 10 input
attributes, each with ùëô = 5 possible values, we have ùëõ ‚âà 2 ¬∑ 107 variables and ‚âà 8 ¬∑ 107 constraints
for each sub-level LP problem, with time complexity ùëÇ ( ùëõ4
log ùëõ ), i.e., not tractable on typical ma-
chines (e.g., as reported in [64], ‚Äòspal_004‚Äô with 10203 rows (w.r.t. constrains) and 321696 columns
(w.r.t. variables) can encounter out of memory or timeout (> 25, 000 seconds) issue on a Linux-
PC with a 4GHz i7-4790K CPU and 32GB RAM). To solve a generalized LFP problem, we need to
solve such a huge sub-level LP problem iteratively. In practice, ML algorithms may require non-
trivial amounts of attributes to aid in decision-making; therefore, without an eÔ¨Écient solver, the
privacy-Ô¨Ådelity trade-oÔ¨Ä problem could be intractable and the feasibility of the associated privacy
protection scheme could be dramatically reduced. In this regard, it is crucial and essential to propose
a more eÔ¨Écient method to solve the proposed privacy-Ô¨Ådelity trade-oÔ¨Ä optimization problem.

7 LINEAR-TIME OPTIMAL PRIVACY SOLUTION
In this section, we analyze the optimization problem (OPT), reveal its important properties, and
propose eÔ¨Écient methods to solve it. We Ô¨Årst investigate the decomposability of the optimization
problem, i.e, whether the problem can be decomposed into several smaller sub-problems for eÔ¨É-
cient solving. We Ô¨Ånd (OPT) decomposable and can be solved using divide-and-conquer approach.
In addition, we propose closed-form solutions for each optimization sub-problem. The optimization
problem can thus be solved very eÔ¨Éciently by solving multiple independent sub-problems in linear
time. Moreover, analysis insights into the optimal solutions are also provided in this section.

7.1 Decomposability

In the following, we show that the optimization problem can actually be decomposed into nu-
merous small sub-problems and thus can be solved more eÔ¨Éciently. An optimization problem is
separable or trivially parallelizable if the variables can be partitioned into disjoint subvectors and
each constraint involves only variables from one of the subvectors [17]. By observing (i) each con-
straint in (16c), (16d), and (16e) involves only a single variable Àúùê∑ùëé (x), (ii) each constraint in (16f)

Achieving Transparency Report Privacy in Linear Time

17

involves a set of variables { Àúùê∑ùëé (x) | ‚àÄùëé ‚àà A}, and (iii) each constraint in (16b) involves a set of
variables { Àúùê∑ùëé (x) | ‚àÄx ‚àà ùëáxU }, we notice that any variable Àúùê∑ùëé (x) is a complicating variable in
ùëáxU √óA but is irrelevant to any other variables outside the QID group ùëáxU . Hence, (16b)‚Äì(16f) are
complicating constraints within a tuple but separable constraints among tuples. (OPT) can thus be
decomposed into multiple smaller sub-problems; each focuses on a particular QID group only. Let
‚Ñé( Àúùê∑ùëé (x), ùõΩ) ‚â• 0 be the aÔ¨Éne function representing all linear inequality constraints (16b)‚Äì(16e).
An optimization sub-problem can thus be expressed as follow.

OPT-SUB(ùëáxU √óA) :

ùõΩ

min
Àúùê∑
s.t. ‚Ñé( Àúùê∑ùëé (x), ùõΩ) ‚â• 0 , ‚àÄx ‚àà ùëáxU , ‚àÄùëé ‚àà A

Àúùê∑ùëé (x) = 1 , ‚àÄx ‚àà ùëáxU .

(OPT) is then equivalent to the master problem below.

√ïùëé ‚ààA

OPT-MASTER(Rùëã √óA) :

min
Àúùê∑
s.t.

ùõΩ

(INEQ-Sub(ùëáxU √óA)) , ‚àÄùëáxU ‚äÜ Rùëã
(EQ-Sub(ùëáxU √óA)) , ‚àÄùëáxU ‚äÜ Rùëã .

(OPT-Sub)

(OBJ-Sub)

(INEQ-Sub)

(EQ-Sub)

(OPT-MS)

(OBJ-MS)

(INEQ-MS)

(EQ-MS)

Lemma 3. Let ùõΩ‚àó
(OPT). We have ùõΩ‚àó = max
ùëáxU ‚äÜ Rùëã

ùëáxU

ùõΩ‚àó
ùëáxU

.

denote the optimal value of a sub-problem (OPT-Sub), ùõΩ‚àó the optimal value of

Proof. Since (OPT) is a generalized LFP (in an equivalent formulation), according to OPT-MS,
(cid:3)

the result trivially follows.

The Lemma above basically tells us that given the same Ô¨Ådelity constraints, the overall highest
among all sub-problems, i.e., the weakest optimal privacy

privacy guarantee ùõΩ‚àó is the largest ùõΩ‚àó
guarantee among all QID groups.

ùëáxU

7.2 Solution Properties

According to the decomposability of the optimization problem, in the following, we only need
to focus on solving an optimization sub-problem (OPT-Sub). To characterize the privacy-Ô¨Ådelity
trade-oÔ¨Ä, we are particularly interested in where the trade-oÔ¨Ä starts and ends. In this section, we
propose lemmas addressing the above question.

Before introducing the lemmas, we Ô¨Årst deÔ¨Åne a useful quantity which will be further utilized

to characterize the trade-oÔ¨Ä.

DeÔ¨Ånition 7. (Maximum Posterior ConÔ¨Ådence) Given an optimization sub-problem (OPT-Sub) and
1-Ô¨Ådelity (100% faithfulness) requirement, i.e., ùõº = ùõø = 1 and Àúùê∑ = ùê∑, the highest conÔ¨Ådence that an
adversary can have on inferring any sensitive information from any decision outcome, denoted by ùê∂‚àó,
is ùê∂‚àó , Conf (ùëã U = xU, ùê¥

{conf (xU, ùëé ‚Üí ùë• S)}.

ùëÉùëã ,ùê∑
‚àí‚àí‚àí‚àí‚Üí ùëã S) = max
ùëé,ùë•S

Lemma 4. An (OPT-Sub) has the 1-Ô¨Ådelity solution Àúùê∑ùëé (x) = ùê∑ùëé (x), ‚àÄx ‚àà ùëáxU , ‚àÄùëé ‚àà A, iÔ¨Ä ùõΩ ‚â• ùê∂‚àó.
Proof. Please refer to Appendix E for detailed proof. We provide intuitive explanation as proof
sketch here. Since the highest conÔ¨Ådence that an adversary can have (ùê∂‚àó) is lower than the privacy

18

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

requirement (the conÔ¨Ådence threshold ùõΩ), it is safe to release ùê∑ directly, i.e., Àúùê∑ = ùê∑ with perfect
Ô¨Ådelity. On the other hand, as long as ùê∂‚àó is greater than ùõΩ, releasing ùê∑ violates privacy requirement
(cid:3)
and cannot be a feasible solution.

Lemma Insight - Lemma 4 tells us when ùõΩ ‚â• ùê∂‚àó, there is no trade-oÔ¨Ä between privacy and
Ô¨Ådelity: as long as ùõΩ is greater than ùê∂‚àó, increasing the strength of privacy (decreasing ùõΩ) would
not cause degradation in Ô¨Ådelity. In other words, alone the strength of privacy (from low to high),
the trade-oÔ¨Ä between privacy and Ô¨Ådelity starts when ùõΩ is right below ùê∂‚àó. The next lemma will
tell us the end of this trade-oÔ¨Ä region.

Lemma 5. For ùõº = ùõø = 0, i.e., Ô¨Ådelity constraints are trivialized or not presented, an (OPT-Sub)
has feasible solutions if and only if (iÔ¨Ä) ùõΩ ‚â• ùõΩmin , maxx‚ààùëáxU
ùëÉ (x|ùëáxU ). In other words, there exists
privacy limit, the strongest privacy that we can have, based on the adversarial settings in Section 4.2.

Proof. Please refer to Appendix F for detailed proof. We provide intuitive behind this lemma
ùëÉ (x|ùëáxU ) is the greatest conditional probability
as proof sketch here. The privacy limit maxx‚ààùëáxU
over the tuple5, which is actually the highest possible inference conÔ¨Ådence of an adversary before
releasing an ATR. It is the baseline conÔ¨Ådence, which merely utilizes knowledge of public record xU
ùëÉùëã‚àí‚àí‚Üí ùë• Si. Since an ATR does not contribute
and side-information ùëÉ (x) in an inference channel hxU
to such an inference channel, an associated privacy protection scheme is not able to help further
reduce this baseline conÔ¨Ådence. While achieving such a privacy limit, an ATR basically reveals
(cid:3)
zero useful information to the public.

Lemma Insight - Lemma 4 and 5 tell us the start and the end of the privacy-Ô¨Ådelity trade-oÔ¨Ä

region along ùõΩ. Next, we show that the end point can never happen before the starting point.

Lemma 6. ùê∂‚àó ‚â• ùõΩmin.

Proof. Please refer to Appendix G for detailed proof. We Ô¨Årst provide the intuition of the lemma
as a proof sketch. The intuition here is very straightforward: the maximum posterior conÔ¨Ådence
can never be lower than the maximum prior conÔ¨Ådence (prior vulnerability cannot exceed posterior
(cid:3)
vulnerability in [66]). Equality holds when the revealed information is completely useless.

Lemma Insight - According to Lemma 4, when ùõΩ ‚àà [ùê∂‚àó, 1], the true decision mapping ùê∑ can
be safely released without perturbation (1-Ô¨Ådelity). Lemma 5 tells us when Ô¨Ådelity constraints are
not imposed (0-Ô¨Ådelity), the feasible privacy region is ùõΩ ‚àà [ùõΩmin, 1]. Moreover, based on Lemma
6, the region [ùõΩmin, ùê∂‚àó] is always non-empty. Clearly, this is the region where we trade oÔ¨Ä Ô¨Ådelity
for privacy. Next, we propose our main theorem to characterize the trade-oÔ¨Ä in this region.

7.3 Optimal Privacy and Solutions
In the following, we propose our main theorem, which provides the optimal-privacy solutions to
the optimization sub-problem (OPT-Sub) in a closed-form expression, in terms of Ô¨Ådelity. Given
Ô¨Ådelity constraints, the proposed closed-form solution yields the optimal privacy value, and thus
can be utilized to analytically characterize privacy-Ô¨Ådelity trade-oÔ¨Ä.

5Since ‚àÄx ‚àà ùëáxU , xU is the same, ùëÉ (x |ùëáxU ) is also ùëÉ (ùë•S |ùëáxU ), the conditional distribution over all sensitive records.

Achieving Transparency Report Privacy in Linear Time

19

Theorem 1. Consider an optimization sub-problem (OPT-Sub) for a QID group, in which we seek
for the strongest privacy guarantee given Ô¨Ådelity constraints. For a decision outcome ùëé, deÔ¨Åne

xùëé , arg max

ùëÉ (x) Àúùê∑ùëé (x)min,

ùëÉ (x‚Ä≤),

x‚Ä≤ ‚ààùëáxU

x‚ààùëáxU
ùëè (x) = ùëÉ (x) ‚àí ùõΩ
Àúùê∑ùëé (x)max‚Ä≤ , 1
Àúùê∑ùëé (x)min‚Ä≤ , 1

√ç

ùëÉ (x) min{ùëÉ (x) Àúùê∑ùëé (x)max, ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min},
ùëÉ (x) max{ùëÉ (x) Àúùê∑ùëé (x)min, ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min + ùëè (x)}.

For binary decisions, i.e., ùëé ‚àà A = {0, 1}, the optimal privacy ùõΩ‚àó

ùëáxU

= max{ùõΩ0, ùõΩ1, ùõΩùëù }, where

ùõΩ0 =

ùõΩ1 =

ùõΩùëù =

ùëÉ (x0) Àúùê∑0 (x0)min
x‚â†x0,x‚ààùëáxU
ùëÉ (x1) Àúùê∑1 (x1)min
x‚â†x1,x‚ààùëáxU

√ç

ùëÉ (x0) Àúùê∑0 (x0)min +

ùëÉ (x) Àúùê∑0 (x)max‚Ä≤

ùëÉ (x1) Àúùê∑1 (x1)min +
ùëÉ (x1) Àúùê∑1 (x1)min + ùëÉ (x0) Àúùê∑0 (x0)min
x‚ààùëáxU

√ç
ùëÉ (x)

,

ùëÉ (x) Àúùê∑1 (x)max‚Ä≤

,

,

and the corresponding optimal privacy solutions are
When ùõΩ‚àó

= ùõΩ0 :

ùëáxU

When ùõΩ‚àó

ùëáxU

When ùõΩ‚àó

ùëáxU

= ùõΩ1 :

= ùõΩùëù :

√ç
Àúùê∑0 (x) = Àúùê∑0 (x)max‚Ä≤, ‚àÄx ‚àà ùëáxU
Àúùê∑1 (x) = Àúùê∑1 (x)max‚Ä≤, ‚àÄx ‚àà ùëáxU
Àúùê∑ùëé (xùëé) = Àúùê∑ùëé (xùëé)min, ‚àÄùëé ‚àà A
ùëÉ (x) Àúùê∑ùëé (x) = 1
ùõΩùëù

x‚ààùëáxU

ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min, ‚àÄùëé ‚àà A

When ùõΩ‚àó

ùëáxU

= ùõΩùëù and |ùëáxU | > 3, we have multiple solutions.

Proof. We refer readers to Appendix H for the detailed proof.

(cid:3)

Àúùê∑ùëé (x)min‚Ä≤ ‚â§ Àúùê∑ùëé (x) ‚â§ Àúùê∑ùëé (x)max‚Ä≤, ‚àÄx ‚àà ùëáxU , ‚àÄùëé ‚àà A.
√ç

Practical Implications of Theorem - Based on Theorem 1, the optimal privacy guarantee ùõΩ‚àó

for
each QID group can be computed analytically (in closed form), and based on Lemma 3, the overall
strongest privacy guarantee is the largest ùõΩ‚àó
among all QID groups. This is particularly useful
and practical in releasing ATRs - given any value of tolerable distortion, we can now easily obtain
the optimal privacy value without the need of solving an optimization problem, which can then
be applied to aid in determining the desired trade-oÔ¨Ä between privacy and Ô¨Ådelity.

ùëáxU

ùëáxU

Linear Time JustiÔ¨Åcation of Algorithm 1 - The net time to achieve a solution to (OPT) is a function
of the number of sub-problems - each of which is solved via Algorithm 1 in linear-time in the
number of records ùëõ. Given an optimization sub-problem, the number of records x ‚àà ùëáxU , i.e.,
|ùëáxU | , ùëõ is equal to the number of sensitive attribute values, as all records in a QID group ùëáxU
have the same public record xU. To see that Algorithm 1 is in ùëÇ (ùëõ), it is Ô¨Årst worth noting that,
based on Theorem 1, the time complexity of computing xùëé and ùëè (x) are in ùëÇ (ùëõ); consequently,
the time complexity of computing Àúùê∑ùëé (x)max‚Ä≤ and Àúùê∑ùëé (x)min‚Ä≤ are in ùëÇ (1). Given these complexities,
it is clear that 1, lines 1 to 4 in Algorithm 1 is in ùëÇ (ùëõ); all lines from line 5 to line 15, except
line 13, are in ùëÇ (1); function Allocation called in line 13 is in ùëÇ (ùëõ) - since lines 18 to 19, as
well as line 20, are in ùëÇ (ùëõ), line 21 and 22 are in ùëÇ (1), and lines 23 to 27 is in ùëÇ (ùëõ)). Therefore,

20

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Algorithm 1 Optimal Privacy Protection Scheme
Input: ùëÉ (x), ùëáxU , Àúùê∑ùëé (x)min, Àúùê∑ùëé (x)max
Output: Àúùê∑ùëé (x), ‚àÄùëé, ‚àÄx ‚àà ùëáxU
1: for ùëé ‚àà {0, 1} do
Ô¨Ånd xùëé
2:
for all x ‚àà ùëáxU do

3:

‚Üê max{ùõΩ0, ùõΩ1, ùõΩùëù }

ùëáxU

compute Àúùê∑ùëé (x)max‚Ä≤
4:
5: compute ùõΩ0, ùõΩ1, ùõΩùëù, and ùõΩ‚àó
6: if ùõΩ‚àó
= ùõΩ0 then
ùëáxU
Àúùê∑0(x) ‚Üê Àúùê∑0 (x)max‚Ä≤
Àúùê∑1(x) ‚Üê 1 ‚àí Àúùê∑0(x)max‚Ä≤

7:

8:
9: else if ùõΩ‚àó

ùëáxU

= ùõΩ1 then
Àúùê∑1(x) ‚Üê Àúùê∑1 (x)max‚Ä≤
Àúùê∑0(x) ‚Üê 1 ‚àí Àúùê∑1(x)max‚Ä≤

10:

11:
12: else if ùõΩ‚àó

= ùõΩùëù then

ùëáxU

13:

Àúùê∑1(x) ‚Üê Allocation()
Àúùê∑0(x) ‚Üê 1 ‚àí Àúùê∑1(x)
14:
15: return Àúùê∑ùëé (x), ‚àÄùëé, ‚àÄx ‚àà ùëáxU
16:
17: function Allocation( )
18:

x‚â†x1,x0 ùëÉ (x) Àúùê∑1 (x)min‚Ä≤

for all x ‚àà ùëáxU , x ‚â† x1, x0 do
compute Àúùê∑1 (x)min‚Ä≤
resid ‚Üê RHS of (22) ‚àí
Àúùê∑1(x1) ‚Üê Àúùê∑1(x1)min
Àúùê∑1(x0) ‚Üê 1 ‚àí Àúùê∑0 (x0)min
for all x ‚àà ùëáxU , x ‚â† x1, x0 do

√ç

capacity ‚Üê Àúùê∑1 (x)max‚Ä≤ ‚àí Àúùê∑1 (x)min‚Ä≤
allocation ‚Üê min{ resid
Àúùê∑1 (x) ‚Üê Àúùê∑1 (x)min‚Ä≤ + allocation
resid ‚Üê resid ‚àí ùëÉ (x) ¬∑ allocation

ùëÉ (x) , capacity}

return Àúùê∑1(x), ‚àÄx ‚àà ùëáxU

19:

20:

21:

22:

23:

24:

25:

26:
27:

28:

the time complexity of Algorithm 1 is in ùëÇ (ùëõ). By using multi-threaded coding structures solving
‚Äúparallelizable‚Äù sub-problems via Algorithm 1, (OPT) can be solved in ùëÇ (ùëõ).

7.4 Theorem Insights on Achieving Solution Optimality

In this section, we provide some insights into the optimal-privacy solutions subject to Ô¨Ådelity
constraints in Theorem 1.

An important observation is that the optimal-privacy candidate values (ùõΩ0, ùõΩ1, and ùõΩùëù) the in-
ference conÔ¨Ådence (left-hand-side of (16b)) are fully characterized by ùëÉ (x) Àúùê∑ùëé (x) pairs of prod-
uct, which are the announced joint probabilities ÀúùëÉùëã,ùê¥ (x, ùëé) representing the portion of population
with input record x receiving decision ùëé, bounded within ranges [ùëÉ (x) Àúùê∑ùëé (x)min, ùëÉ (x) Àúùê∑ùëé (x)max]

Achieving Transparency Report Privacy in Linear Time

21

Fig. 4. An representative illustration for changes of joint probabilities caused by the optimal-privacy scheme.

due to Ô¨Ådelity constraints. Solving (OPT-Sub) to obtain optimal-privacy solution is thus equiva-
lent to ‚Äútune‚Äù those pairs of product within the allowed range, for all inputs x ‚àà ùëáxU and outputs
ùëé ‚àà A, to minimize the maximal possible inference conÔ¨Ådence ùõΩ of an adversary. Particularly, from
Theorem 1, it turns out that for each decision outcome instance ùëé, the term ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min =
ùëÉ (x) Àúùê∑ùëé (x)min, the maximum of the lower bounds of the allowed ranges over x ‚àà ùëáxU ,
maxx‚ààùëáxU
plays a crucial role in solving (OPT-Sub). Next, we show that the optimal-solution for x = xùëé can
only be the minimum of its allowed range.

Corollary 1.

Àúùê∑ùëé (xùëé)min = Àúùê∑ùëé (xùëé)min‚Ä≤ = Àúùê∑ùëé (xùëé)max‚Ä≤, ‚àÄùëé ‚àà {0, 1}.

Proof. By deÔ¨Ånitions of xùëé and Àúùê∑ùëé (x)max‚Ä≤, the result Àúùê∑ùëé (xùëé)min = Àúùê∑ùëé (xùëé)max‚Ä≤ trivially follows.
x‚Ä≤ ùëÉ (x‚Ä≤) ‚â§ 0, and thus by plugging x = xùëé into
(cid:3)

Based on Lemma 5, we have ùëè (x) = ùëÉ (x) ‚àí ùõΩùëù
Àúùê∑ùëé (x)min‚Ä≤, we obtain Àúùê∑ùëé (xùëé)min = Àúùê∑ùëé (xùëé)min‚Ä≤.

√ç
The eÔ¨Äective lower limits ùëÉ (x) Àúùê∑ùëé (x)min‚Ä≤ and the eÔ¨Äective upper limits ùëÉ (x) Àúùê∑ùëé (x)max‚Ä≤ represent
the feasible region where the Ô¨Ådelity constraints and privacy constraints intersect. Based on Corol-
lary 1, the eÔ¨Äective upper and lower limit of xùëé are equal, which implies that when x = xùëé, the
only possible value for the optimal-privacy solution is Àúùê∑ùëé (xùëé)min. From Theorem 1, we can see
that this is true for all the three cases. It is worth noting that the cases ùõΩ‚àó
= ùõΩ1 are
equivalent (by swapping 0‚Äôs and 1‚Äôs), so we only have two representative cases: ùõΩ‚àó
= ùõΩùëé, ùëé ‚àà {0, 1},
and ùõΩ‚àó

= ùõΩ0 and ùõΩ‚àó

ùëáxU

ùëáxU

ùëáxU

= ùõΩùëù .

ùëáxU

ùëáxU

= ùõΩùëé, ùëé ‚àà {0, 1}, the allowed ranges of all ùëÉ (x) Àúùê∑ùëé (x)
7.4.1 Representative Case 1: When ùõΩ‚àó
pairs are imposed by an additional upper limit ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min caused by privacy constraints. In
these cases, eÔ¨Äective upper limits are the minimum of the original upper limits ùëÉ (x) Àúùê∑ùëé (x)max
and the threshold, formally, ùëÉ (x) Àúùê∑ùëé (x)max‚Ä≤ = min{ùëÉ (x) Àúùê∑ùëé (x)max, ùëÉ (xùëé ) Àúùê∑ùëé (xùëé)min}. According to
Theorem 1, when ùõΩ‚àó
= ùõΩùëé, the corresponding optimal-privacy solution is simply the eÔ¨Äective
ùëáxU
upper limit Àúùê∑ùëé (x) = Àúùê∑ùëé (x)max‚Ä≤, ‚àÄx ‚àà ùëáxU .

An illustration that aids in understanding the intuition behind Theorem 1 is shown in Fig. 4,
in which joint probabilities for ùëé = 1 and ‚àÄx ‚àà ùëáxU are depicted for the case ùõΩ‚àó
= ùõΩ1. For
6, ùëùùëñ = ùëÉ (xùëñ ), and Àúùëëùëñ = Àúùëë (xùëñ ) = Àúùê∑1(xùëñ ), ‚àÄùëñ = 1, . . . , ùëö. The yellow
conciseness, let ùëö denote
spots in Fig. 4 denote the true joint probabilities ùëùùëñùëëùëñ, and the regions indicated by grey arrows

ùëáxU

ùëáxU

6Since ùëáxU is the range of (xU, ùëãS ), ùëö represents the number of distinct sensitive records in the tuple.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

22

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

interpret the allowed perturbation ranges [ùëùùëñ Àúùëëùëñmin, ùëùùëñ Àúùëëùëñmax]. In this example, the maximum of the
lower limits is ùëù6 Àúùëë6min, i.e., x1 = x6. The value ùëù6 Àúùëë6min serves as a threshold (the blue dash line),
imposing an upper limit on all perturbation ranges. The output of the optimal-privacy solution is
denoted by the red spots, which take values from the eÔ¨Äective upper bounds min{ùëùùëñ Àúùëëùëñmax, ùëù6 Àúùëë6min},
‚àÄùëñ. Here we get a clear insight into the optimal-privacy solutions for a QID sub-group ùëá{xU,ùëé }: the
optimality is achieved by Ô¨Çattening the joint distribution ùëÉ (x) Àúùê∑ùëé (x) over all x ‚àà ùëáxU as much as
possible subject to the allowed perturbation range imposed by Ô¨Ådelity constraints. By Ô¨Çattening
the joint distribution, the (Bayesian) posterior distribution over distinct sensitive values seen by
an adversary becomes more uniform, and hence the maximal inference conÔ¨Ådence is reduced.

DeÔ¨Åne ùëé the complement of ùëé, e.g., ùëé = 0 if ùëé = 1. For binary decisions, the optimal-privacy
scheme for a QID group ùëáxU is also privacy optimal for a sub-group ùëá{xU,ùëé } when the joint distri-
bution of ùëá{xU,ùëé } is much ‚ÄúÔ¨Çatter‚Äù (more private) than ùëá{xU,ùëé }. In other words, the optimal-privacy
solution Ô¨Çattens the least private distribution as much as possible; although this might inÔ¨Çuence
the other (the much more private) one and cause it to be less private7, as long as its maximal infer-
ence conÔ¨Ådence is less than ùõΩùëé, the optimal privacy for the entire QID group is dominated by ùõΩùëé,
and thus the optimal privacy scheme for the sub-group ùëá{xU,ùëé } is the optimal privacy scheme for
the entire QID group.

7.4.2 Representative Case 2: When neither distribution is much Ô¨Çatter than the other one, making
one sub-group highly private causes the other one‚Äôs privacy to degrade, i.e., none of the optimal
schemes for any QID sub-group can be optimal for the entire QID group. In such a case, both
sub-groups need to Ô¨Ånd a ‚Äúbalanced point‚Äù at which both sub-groups are equally private. Such
a balanced privacy value for the maximal inference conÔ¨Ådence for two sub-groups is denoted by
ùõΩùëù in Theorem 1, representing the optimal privacy for the QID group. As shown in Theorem 1, in
general, we have multiple solutions to achieve this balanced privacy value. This is because, in such
a case, the optimality of privacy is guaranteed if (i) Àúùê∑ùëé (xùëé) = Àúùê∑ùëé (xùëé)min, ‚àÄùëé, and (ii) the following
two equalities hold

x ùëÉ (x) Àúùê∑0(x) = 1
ùõΩùëù
x ùëÉ (x) Àúùê∑1(x) = 1
ùõΩùëù

√ç

ùëÉ (x0) Àúùê∑0 (x0)min,
ùëÉ (x1) Àúùê∑1 (x1)min,

(19)

(20)

While in the following, we show that the above two equalities are equivalent, i.e., one implies the
other.

√ç

Corollary 2. When ùõΩ‚àó

= ùõΩùëù, (19) implies (20), and vice versa.

ùëáxU
Proof. Recall ùõΩùëù from Theorem 1, we have

xùëÉ (x) = 1
ùõΩùëù

ùëÉ (x1) Àúùê∑1 (x1)min + 1
ùõΩùëù

ùëÉ (x0) Àúùê∑0(x0)min.

(21)

Since Àúùê∑0(x) + Àúùê∑1 (x) = 1, subtract (19) from (21), we obtain (20). Similarly, subtract (20) from (21),
(cid:3)
we obtain (19).

√ç

Therefore, to compute an optimal solution when ùõΩ‚àó

= ùõΩùëù , we only need to solve (20). Since
Àúùê∑0 (x) + Àúùê∑1 (x) = 1, and Àúùê∑ùëé (xùëé) = Àúùê∑ùëé (xùëé)min, ‚àÄùëé, in general we only have ùëö ‚àí 2 variables (see
Remark 4), and based on (21), equality (20) is equivalent to

ùëáxU

ùëÉ (x) Àúùê∑1 (x) =

1‚àí2ùõΩùëù
ùõΩùëù

ùëÉ (x1) Àúùê∑1 (x1)min ‚àí ùëè (x0).

(22)

7Based on (EQ-Sub), any changes made to Àúùê∑ùëé (x) will also change Àúùê∑ùëé (x).

√ïx‚â†x0,x1

(cid:0)

(cid:1)

Achieving Transparency Report Privacy in Linear Time

23

Table 4. Detailed Inputs and Computations of the Provided Numerical Example

0

1

1

x ùëÉ (x) ùê∑1 (x) ùê∑0 (x)
x1 0.3
0
x2 0.125
x3 0.075
x4 0.225
x5 0.175
x6 0.1

1
0.5
0

0
0.5
1

0

1

Inputs

Àúùê∑1 (x) min
0

0
0.9
0
0.4
0.9

Àúùê∑1 (x) max
0.1
0.1
1
0.1
0.6
1

Àúùê∑0 (x) min
0.9
0.9
0
0.9
0.4
0

Computations

Àúùê∑0 (x) max ùëÉ (x) Àúùê∑1 (x) min ùëÉ (x) Àúùê∑1 (x) max ùëÉ (x) Àúùê∑1 (x) max‚Ä≤ ùëÉ (x) Àúùê∑0 (x) min ùëÉ (x) Àúùê∑0 (x) max ùëÉ (x) Àúùê∑0 (x) max‚Ä≤

1

1
0.1
1
0.6
0.1

0

0
0.0675
0
0.07
0.09

0.03
0.0125
0.075
0.0225
0.105
0.1

0.03
0.0125
0.0675
0.0225
0.09
0.09

0.27
0.1125
0
0.2025
0.07
0

0.3
0.125
0.0075
0.225
0.105
0.01

0.27
0.125
0.0075
0.2025
0.105
0.01

ùëáxU

When ùõΩ‚àó

x‚â†x0,x1 ùëÉ (x) Àúùê∑1 (x)max‚Ä≤

= ùõΩùëù, the right-hand-side (RHS) of (22) is strictly bounded by

x‚â†x0,x1 ùëÉ (x) Àúùê∑1(x)min‚Ä≤,
, which implies there always exists a feasible solution for (20). When ùëö >
3, since the number of variables to solve (ùëö ‚àí 2) is greater than the number of equation (one, which
√ç
is (22)), an optimal solution, in general, is not unique.

(cid:2) √ç

(cid:3)

Remark 4. For the special case x0 = x1, we have ùëö ‚àí 1 variables. Such a case can happen when the
population of a certain record dominates its corresponding QID group. When this is the case, the
prior (distribution) knowledge provides very high (baseline) conÔ¨Ådence on inferring this record.
In particular, for such a case, we must have ùõΩùëù = ùõΩmin. If ùõΩùëù > ùõΩùëé, ‚àÄùëé, i.e., ùõΩ‚àó
= ùõΩùëù, this becomes
trivial: according to Lemma 5 and its following discussion, the announced ATR can only provide
trivial information to achieve this lowest-possible baseline conÔ¨Ådence.

ùëáxU

8 NUMERICAL EXAMPLES

In the previous section, we proposed lemmas characterizing important properties about privacy-
Ô¨Ådelity trade-oÔ¨Ä, a theorem providing closed-form optimal solutions for the trade-oÔ¨Ä problem, and
insights into the optimal solutions for both the representative cases. In this section, we provide
numerical examples to demonstrate privacy-Ô¨Ådelity trade-oÔ¨Ä regions, and aid in understanding the
properties of the trade-oÔ¨Ä regions and the insights into the optimal solutions for both the repre-
sentative cases. Without loss of generality, we reuse the same examples demonstrated in Section 3
showcasing how the proposed linear-time optimal-privacy scheme (Algorithm 1) can be applied in
practice to solve the problem, as long as there is no privacy preference among sensitive attribute
values.

Consider Table 1 again but for a smaller size population {12, 5, 3, 9, 7, 4} (Ô¨Årst column of the table)
for ease of demonstration, and let xùëñ denote the record of the ùëñ-th row, ùëñ = 1, . . . , 6. Suppose an
announced ATR needs to satisfy a pre-determined Ô¨Ådelity constraint ùõø = 0.9 (90%-Ô¨Ådelity), and we
would like to preserve data subjects‚Äô privacy as much as possible subject to the Ô¨Ådelity constraint.
First, consider the QID group of female ùëáxU ={F}, i.e., the tuple of records ùëá{F} = {x1, x2, x3}. Based
on lines 1 to 4 in Algorithm 1, we Ô¨Årst need to determine xùëé and Àúùê∑ùëé (x)max‚Ä≤ ‚àÄùëé ‚àà {0, 1}, ‚àÄx ‚àà ùëá{F}.
Detailed computations are demonstrated in Remark 5, and the computed results are presented in
Table 4; from which, we observe that x1 = x3 and x0 = x1 (see Remark 5 for details as well).

Proceeding to line 5, we compute ùõΩ0, ùõΩ1, and ùõΩùëù as follows:

ùõΩ1 =

ùëÉ (x1) Àúùê∑1 (x1)min +

ùëÉ (x) Àúùê∑1 (x)max‚Ä≤

ùëÉ (x1) Àúùê∑1 (x1)min
x‚â†x1,x‚ààùëáxU

=

0.0675
0.0675 + 0.03 + 0.0125

‚âà 0.6136,

√ç

24

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

ùõΩ0 =

ùõΩùëù =

ùëÉ (x0) Àúùê∑0(x0)min
x‚â†x0,x‚ààùëáxU

ùëÉ (x0) Àúùê∑0(x0)min +
ùëÉ (x1) Àúùê∑1(x1)min + ùëÉ (x0) Àúùê∑0 (x0)min
x‚ààùëáxU

√ç
ùëÉ (x)

ùëÉ (x) Àúùê∑0 (x)max‚Ä≤

=

0.27
0.27 + 0.125 + 0.0075

‚âà 0.6708,

=

0.0675 + 0.27
0.5

= 0.675,

and obtain ùõΩ‚àó
ùëá{F}
to call function Allocation in line 17. Based on lines 18 and 19, we Ô¨Årst need to compute

, max{ùõΩ0, ùõΩ1, ùõΩùëù } = ùõΩùëù = 0.675. Proceeding to lines 12 and 13, in this case we need

√ç

Àúùê∑1 (x2)min‚Ä≤ = 1
1
0.125

=

ùëÉ (x2) max{ùëÉ (x2) Àúùê∑1 (x2)min, ùëÉ (x1) Àúùê∑1(x1)min + ùëè (x2)}

max{0, 0.0675 + 0.125 ‚àí (0.675) (0.5)} = 0.

Proceeding to line 20, since Àúùê∑1(x2)min‚Ä≤ = 0, we have

resid = RHS of (22) =

ùëÉ (x1) Àúùê∑1 (x1)min ‚àí ùëè (x0)

1‚àí2ùõΩùëù
ùõΩùëù
= ( ‚àí0.35
0.675 ) (0.075) (0.9) + (0.675) (0.5) ‚àí 0.3 = 0.0025.
(cid:0)

(cid:1)

Based on lines 21 and 22, we obtain

Àúùê∑1 (x3) = Àúùê∑1(x1) = Àúùê∑1 (x1)min = Àúùê∑1(x3)min = 0.9,
Àúùê∑1 (x1) = Àúùê∑1(x0) = 1 ‚àí Àúùê∑0 (x0)min = 1 ‚àí Àúùê∑0 (x1)min = 0.1.

Moreover, proceeding to lines 23 to 27, we obtain

capacity = Àúùê∑1 (x2)max‚Ä≤ ‚àí Àúùê∑1 (x2)min‚Ä≤ = 0.0125
allocation = min{ 0.0025
Àúùê∑1(x2) = Àúùê∑1 (x2)min‚Ä≤ + allocation = 0 + 0.02 = 0.02.

0.125 , 0.1} = 0.02,

0.125 ‚àí 0 = 0.1,

We therefore obtain the optimal solution Àúùê∑1 ({x1, x2, x3}) = [0.1, 0.02, 0.9] for the QID group
of female, which yields maximum conÔ¨Ådence of 67.5% for an adversary inferring any sensitive
information from any female data subject.

We then consider the QID group for male ùëáxU ={M}, i.e., the tuple of records ùëá{M} = {x4, x5, x6}.

Similarly, based on Table 4, we obtain x1 = x6, x0 = x4, and

ùõΩ1 =

ùõΩ0 =

ùõΩùëù =

0.09
0.0225 + 0.09 + 0.09
0.2025
0.2025 + 0.105 + 0.01
0.09 + 0.2025
0.5

= 0.585,

‚âà 0.4444,

‚âà 0.6378,

ùëá{M}

and we get ùõΩ‚àó
= max{ùõΩ0, ùõΩ1, ùõΩùëù } = ùõΩ0 ‚âà 0.6378. Based on lines 6 to 8, we obtain the optimal solu-
tion for this group Àúùê∑1 ({x4, x5, x6}) = 1‚àí Àúùê∑0({x4, x5, x6})max‚Ä≤ = 1‚àí [ 0.2025
0.1 ] = [0.1, 0.4, 0.9]
for QID group of male, which yields maximum conÔ¨Ådence of 63.78% for an adversary inferring any
sensitive information from any male data subject. Based on Lemma 3, the optimal-privacy ùõΩ‚àó for
the entire dataset is max{ùõΩ‚àó
} = max{0.675, 0.6378} = 0.675, which is the maximum conÔ¨Å-
ùëá{F}
dence for an adversary inferring any sensitive information from any data subject from this dataset,
based on the announced ATR.

0.225 , 0.105

0.175, 0.01

, ùõΩ‚àó

ùëá{M}

The optimal solution for the QID group of female is a ‚Äúbalanced point‚Äù between inferring the
annual income of x1 and x3 correctly, i.e., Conf (F, ùê¥ ‚Üí Annual Income) = conf (F, ùê¥ = 0 ‚Üí

Achieving Transparency Report Privacy in Linear Time

25

(a) P-F TradeoÔ¨Ä for the QID Group of Female

(b) P-F TradeoÔ¨Ä for the QID Group of Male

1

0.9

0.8

0.7

0.6

0.5

(cid:39)(cid:70)(cid:66)(cid:84)(cid:74)(cid:67)(cid:77)(cid:70)(cid:1)(cid:51)(cid:70)(cid:72)(cid:74)(cid:80)(cid:79)
*
T
{F}

=0.6

min

C*=1

 = 0.675

(cid:39)(cid:70)(cid:66)(cid:84)(cid:74)(cid:67)(cid:77)(cid:70)(cid:1)(cid:51)(cid:70)(cid:72)(cid:74)(cid:80)(cid:79)

C*=0.72

 = 0.6378

*
T
{M}

=0.45

min

0.8

0.7

0.6

0.5

0.4

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Fig. 5. Privacy-Fidelity (P-F) TradeoÔ¨Äs for QID Groups of Female and Male

< 100k) = conf (F, ùê¥ = 1 ‚Üí > 200k), where ùê¥ is deÔ¨Åned in Table 3, the random variable of
decision outcome (0: negative; 1: positive), and

conf (F, 0 ‚Üí < 100k) =

conf (F, 1 ‚Üí > 200k) =

ùëÉ (x1) Àúùê∑0(x1)
ùëÉ (x1) Àúùê∑0 (x1) + ùëÉ (x2) Àúùê∑0(x2) + ùëÉ (x3) Àúùê∑0 (x3)
ùëÉ (x3) Àúùê∑1(x3)
ùëÉ (x1) Àúùê∑1 (x1) + ùëÉ (x2) Àúùê∑1(x2) + ùëÉ (x3) Àúùê∑1 (x3)

=

0.3√ó0.9
0.3√ó0.9+0.125√ó0.98+0.075√ó0.1

= 0.675,

=

0.075√ó0.9
0.3√ó0.1+0.125√ó0.02+0.075√ó0.9

= 0.675.

Making either inference more private will cause the other one less private and hence degrades
the overall privacy guarantee as discussed in Section 7.4. In contrast, the optimal solution for the
male group tries to minimize the conÔ¨Ådence of correctly inferring the annual income of x4, i.e.,
Conf (M, ùê¥ ‚Üí Annual Income) = conf (M, ùê¥ = 0 ‚Üí < 100k), and

conf (M, 0 ‚Üí < 100k) =

ùëÉ (x4) Àúùê∑0 (x4)
ùëÉ (x4) Àúùê∑0(x4) + ùëÉ (x5) Àúùê∑0 (x5) + ùëÉ (x6) Àúùê∑0 (x6)

=

0.225√ó0.9+0.175√ó0.6+0.1√ó0.1 ‚âà 0.6378.

0.225√ó0.9

From the above equation, it is not hard to see that the optimal solution maximizes the denominator
while minimizing the numerator in order to minimize the ratio for optimal privacy.

From the above example, we demonstrated that subject to Ô¨Ådelity constraints, how an optimal-
privacy ATR can be obtained eÔ¨Éciently using Algorithm 1. The maximum conÔ¨Ådence of an adver-
sary, which is conf (F, 1 ‚Üí > 200k), drops from 100% to 67.5% by setting a 10%-distortion tolerance
for the announced ATR.

Figure 5 depicts the privacy-Ô¨Ådelity tradeoÔ¨Äs for both QID groups in this numerical example.
Given any Ô¨Ådelity requirement ùõø, the optimal privacy (i.e., the smallest possible ùõΩ) that we can
achieve is the boundary of the trade-oÔ¨Ä region (the blue curve). The tradeoÔ¨Ä region for ùõΩ, as dis-
cussed in Section 7.2, should be within the range [ùõΩùëöùëñùëõ, ùê∂‚àó], which can be easily computed based
on DeÔ¨Ånition 7 and Lemma 5:
For the QID group of female: [ùõΩùëöùëñùëõ, ùê∂‚àó] =

[

ùëÉ (x1)
ùëÉ (x1)+ùëÉ (x2)+ùëÉ (x3) ,

ùëÉ (x1)ùê∑1 (x1)+ùëÉ (x2)ùê∑1 (x2)+ùëÉ (x3)ùê∑1 (x3) ] = [ 0.3

ùëÉ (x1)ùê∑1 (x1)

0.5, 0.075√ó1

0.075√ó1 ] = [0.6, 1].

For the QID group of male: [ùõΩùëöùëñùëõ, ùê∂‚àó] =

[

ùëÉ (x4)
ùëÉ (x4)+ùëÉ (x5)+ùëÉ (x6) ,

ùëÉ (x4)ùê∑1 (x4)+ùëÉ (x5)ùê∑1 (x5)+ùëÉ (x6)ùê∑1 (x6) ] = [ 0.225
0.5 ,

ùëÉ (x4)ùê∑1 (x4)

0.225√ó1

0.225√ó1+0.175√ó0.5 ] = [0.45, 0.72].

Both results show consistency with Figure 5 : in Figure 5a, the range of ùõΩ is within [0.6, 1]; in
Figure 5b, the range of ùõΩ is within [0.45, 0.72]. Note that based on Lemma 5, any privacy require-
ment with ùõΩ < max{0.6, 0.45} = 0.6 is not feasible for this dataset, and based on Lemma 4, any
privacy requirement with ùõΩ > 0.72 can have 1-Ô¨Ådelity solution for the QID group of male, i.e., no
perturbation is needed. How much Ô¨Ådelity should be sacriÔ¨Åced in order to achieve a certain level
of privacy can thus be known based on the tradeoÔ¨Ä curves.

26

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Remark 5. Here we demonstrate how the values presented in Table 4 are computed. Note that,
we only demonstrate the computation of values in the Ô¨Årst row (i.e., for x1), as computations for
values in all other rows (for all other xi‚Äôs) follow similar steps. In the following, we start from the
left-most value and then move to the right.

For x = x1, xU = {F} (Female), and since the total population is 12 + 5 + 3 + 9 + 7 + 4 = 40,
ùëÉ (x) = 12/40 = 0.3. Based on Table 1, since the decision rule represents the probability of receiving
a positive decision, ùê∑1(x) is basically the decision rule in Table 1, and ùê∑0 (x) is simply 1 ‚àí ùê∑1(x).
The pre-deÔ¨Åned Ô¨Ådelity parameter ùõø is 0.9, i.e., the announced decision mapping Àúùê∑ùëé (x) can diÔ¨Äer
from the true decision mapping ùê∑ùëé (x) by at most 10%, ‚àÄùëé = 0, 1. Therefore, | Àúùê∑1(x) ‚àí 0| ‚â§ 0.1, and
we get Àúùê∑1 (x)min = 0 and Àúùê∑1 (x)max = 0.1. Similarly, | Àúùê∑0 (x) ‚àí 1| ‚â§ 0.1, and we get Àúùê∑0 (x)min = 0.9
and Àúùê∑0 (x)max = 1. The values of the above terms are based on their deÔ¨Ånitions (refer to Theorem 1)
and the input parameters. Since now we have values for ùëÉ (x), Àúùê∑1 (x)min, Àúùê∑1 (x)max, Àúùê∑0 (x)min, and
Àúùê∑0 (x)max, the values for the terms ùëÉ (x) Àúùê∑1 (x)min, ùëÉ (x) Àúùê∑1 (x)max, ùëÉ (x) Àúùê∑0(x)min, and ùëÉ (x) Àúùê∑0 (x)max
are just simple multiplications.

Next, we show how the values for the terms ùëÉ (x) Àúùê∑1(x)max‚Ä≤ (third column in the ‚ÄúComputations‚Äù
Àúùê∑ùëé (x)max‚Ä≤ ,
ùëÉ (x) Àúùê∑ùëé (x)min, ‚àÄùëé = 0, 1, we
ùëÉ (x) Àúùê∑0 (x)min = arg maxx‚àà{x1,x2,x3 }{0.27, 0.1125, 0} = x1 and x1 =
0.3 min{0.03, 0.0675} =
0.3 min{0.3, 0.27} = 0.27/0.3. Therefore, we obtain ùëÉ (x) Àúùê∑1 (x)max‚Ä≤ = 0.03

category) and ùëÉ (x) Àúùê∑0(x)max‚Ä≤ (the last column) are obtained. Based on Theorem 1,
ùëÉ (x) min{ùëÉ (x) Àúùê∑ùëé (x)max, ùëÉ (xùëé) Àúùê∑ùëé (xùëé)min}, where xùëé , arg maxx‚ààùëáxU
1
thus have x0 , arg maxx‚ààùëá{F}
arg maxx‚àà{x1,x2,x3 }{0, 0, 0.0675} = x3. Hence, for x = x1,
0.03/0.3 and Àúùê∑0 (x)max‚Ä≤ = 1
and ùëÉ (x) Àúùê∑0(x)max‚Ä≤ = 0.27.

Àúùê∑1 (x)max‚Ä≤ = 1

9 RELATED WORK

There is a huge literature on transparency [28, 46] and fairness [63] for ML. [59] provides a detailed
survey on techniques proposed for enhancing transparency and fairness for ML models. However,
the perspectives of transparency and fairness in ML models may not be completely in sync with
those in algorithmic transparency, e.g., the philosophy of fairness in ML is to train fair ML mod-
els or algorithms, while the philosophy of fairness in accountable algorithmic transparency is to
verify or to demonstrate whether the examined ML algorithms comply with certain fairness re-
quirements.

There are a number of studies on transparency and fairness and several addressing privacy in
data transparency, e.g., [72, 82, 91]; however, there is little eÔ¨Äort in considering the potential impact
on privacy brought on by algorithmic transparency schemes and/or fairness measures. [24] provides
transparency in the interaction between Google Ads, users‚Äô Ad privacy settings, and user behav-
iors, showing the disparate impact that female gender setting has (vs. male gender setting) on
results, e.g., with fewer instances of ads related to high paying jobs; while whether users‚Äô privacy
could be leaked from Google Ads or the associated transparency report is unclear without fur-
ther investigation. [8] investigates the limitations of transparency and its impact on society and
notes that transparency can threaten privacy, but it is yet to be made clear what possible aspects
of transparency can hurt privacy, and by what privacy-preserving techniques could remedy the situ-
ation. Here, we show that data subjects‚Äô privacy can be leaked via various kinds of transparency
schemes and fairness measures in an announced ATR and propose a privacy protection scheme
yielding privacy preserving information on an ATR. Motivated by transparency and fairness, [33]
raises questions regarding fair privacy for all participating users, as it is considered discriminatory
when diÔ¨Äerent users are protected by diÔ¨Äerent levels of privacy; however, on what notion of privacy
should be fair, by what methodology to protect such privacy and to achieve it fairly are still unclear.

Achieving Transparency Report Privacy in Linear Time

27

However, in contrast, numerical examples in Section 8 show that the optimal privacy for diÔ¨Äerent
QID groups, subject to the same Ô¨Ådelity constraints, is in general diÔ¨Äerent due to the disparity of
prior distributions, prior vulnerabilities [66], side-information, and associated decision mapping
between groups. [81] studies the problem of providing transparency to consumers while preserv-
ing information privacy for them, and proposes informational norms to constrain the collection,
use, and distribution of transparent information in role-appropriate manners to fulÔ¨Åll the goal.
However, the deÔ¨Ånition of role-appropriate manners is yet to be more speciÔ¨Åc, and it is also unclear
how fairness measures, which compare decision rules between two individuals or groups (likely in
diÔ¨Äerent roles), should be announced under such a norm. In contrast, our privacy protection scheme
does not make any assumptions on informational norm and does not rely on any norm (poten-
tially hard to accomplish) to protect users‚Äô privacy, and thus can be applied generally. In addition,
informational norm may still not be adequate to protect users‚Äô privacy: individuals belonging to
the same role may still be able to infer private information of others, e.g., in Table 1, any female
credit card owner can infer other female credit card owners‚Äô income range.

There exist a couple of works using diÔ¨Äerential privacy (DP) to remedy the privacy leakage/attack
issue in algorithmic transparency or model explanations. A recent work [79] demonstrates mem-
bership inference attacks [80] on training datasets of ML models by utilizing information from
the corresponding featured-based model explanations (i.e., feature importance/interaction trans-
parency schemes). To address this issue, in [70], DP is applied to the gradient descent algorithm
for generating feature-based model explanations. [23], arguably the only previous work that ad-
dresses transparency, fairness, and privacy in an accountable ATR, proposes a feature-based mea-
sure, named quantitative input inÔ¨Çuence (QII); based on which, the authors propose public and
personalized transparency reports, as well as a fairness measure, named group disparity, to mea-
sure potential disparate impacts on diÔ¨Äerent groups of people. DP is adopted to the above measures
in order to prevent potential privacy leaks caused by the provided QII and group disparity in an
announced ATR. However, applying DP solely does not result in prevention of inference attacks, in
particular, attribute inference attacks: once strong correlations between attribute values are known,
sensitive attribute values can be inferred no matter whether a privacy victim belongs to a speciÔ¨Åc
dataset or not, and thus DP cannot help in such a scenario (see [32], Section 2.3.2, the smoking-
cause-cancer example). In light of this, here, we propose a privacy-preserving scheme to prevent
attribute inference attacks by limiting the attribute inference conÔ¨Ådence from public/known at-
tribute values, via an announced ATR with assistance of side-information, to any data subjects‚Äô
private attribute values.

10 SUMMARY

In this work, we demonstrated how a honest-but-curious adversary can utilize widely-available
information provided in an algorithmic transparency report, to obtain data subjects‚Äô private infor-
mation. From this we glean which potential aspects of transparency and fairness measures can
hurt privacy. We then propose a privacy scheme that perturbs the information to be announced,
to remedy the privacy leaks. We systematically study the impact of such perturbation on fair-
ness measures and the Ô¨Ådelity of the announced information, formulated as an optimization for
optimal privacy subject to Ô¨Ådelity constraints. To eÔ¨Éciently solve the optimization problem, we
reveal important properties and provide closed-form solutions, based on which we propose a pri-
vacy protection scheme. Given Ô¨Ådelity requirements, the proposed scheme can eÔ¨Éciently produce
optimal-privacy ATRs in linear time. In addition, we provide insight into our proposed optimal pri-
vacy scheme. Our proposed methodology is suited to more general problems beyond algorithmic
transparency, where the release of the model information is controlled and the input data cannot
be modiÔ¨Åed - an example being in the setting of model inversion attacks in [40] where the model

28

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

owner has no authority to modify the input data (patents‚Äô clinical history and genomic data) but
has the control of the amount of information about the (dose-suggesting) model to be released. In
such a scenario, our scheme can help privately release information of a model to pharmacists for
better understanding of suggesting personalized dosage.

REFERENCES
[1] [n.d.]. Admissions Transparency Data, New College of the Humanities, London, United Kingdom. https://t.ly/9bo0.
[2] [n.d.]. Admissions Transparency Implementation Working Group, Department of Education, Skills, and Employment,

Australian Government. https://t.ly/qaaZ. Accessed: 2021-02-19.

[3] [n.d.]. Open Government. https://www.oecd.org/gov/open-government/. Accessed: 2021-02-19.
[4] [n.d.]. U.S. Census Bureau Historical Income Tables: People. https://goo.gl/UDoF64. Accessed: 2018-10-01.
[5] Alessandro Acquisti and Ralph Gross. 2009. Predicting social security numbers from public data. Proceedings of the

National Academy of Sciences (2009), PNAS‚Äì0904891106.

[6] Anita L Allen. 2016. Protecting one‚Äôs own privacy in a big data economy. Harv. L. Rev. F. 130 (2016), 71.
[7] M√°rio S Alvim, Miguel E Andr√©s, Konstantinos Chatzikokolakis, Pierpaolo Degano, and Catuscia Palamidessi. 2011.
DiÔ¨Äerential Privacy: On the Trade-OÔ¨Ä between Utility and Information Leakage. Formal Aspects in Security and Trust
7140 (2011), 39‚Äì54.

[8] Mike Ananny and Kate Crawford. 2018. Seeing without knowing: Limitations of the transparency ideal and its appli-

cation to algorithmic accountability. New Media & Society 20, 3 (2018), 973‚Äì989.

[9] Kurt M Anstreicher. 1999. Linear programming in ùëÇ ( ùëõ

3

ln ùëõ ùêø) operations. SIAM Journal on Optimization 9, 4 (1999),

803‚Äì812.

[10] Daniel W Apley. 2016. Visualizing the EÔ¨Äects of Predictor Variables in Black Box Supervised Learning Models. arXiv

preprint arXiv:1612.08468 (2016).

[11] Michael Barbaro, Tom Zeller, and Saul Hansell. 2006. A face is exposed for AOL searcher no. 4417749. New York Times

9, 2008 (2006), 8.

[12] Solon Barocas and Andrew D Selbst. 2016. Big data‚Äôs disparate impact. Cal. L. Rev. 104 (2016), 671.
[13] Gilles Barthe and Boris Kopf. 2011. Information-theoretic bounds for diÔ¨Äerentially private mechanisms. In Computer

Security Foundations Symposium (CSF), 2011 IEEE 24th. IEEE, 191‚Äì204.

[14] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. 2017. Fairness in Criminal Justice Risk

Assessments: The State of the Art. arXiv preprint arXiv:1703.09207 (2017).

[15] Dan Biddle. 2006. Adverse Impact and Test Validation: A Practitioner‚Äôs Guide to Valid and Defensible Employment

Testing (2 ed.). Gower Publishing, Ltd.

[16] Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press.
[17] Stephen Boyd, Lin Xiao, Almir Mutapcic, and Jacob Mattingley. 2007. Notes on decomposition methods. Notes for

EE364B, Stanford University (2007), 1‚Äì36.

[18] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5‚Äì32.
[19] Jianneng Cao and Panagiotis Karras. 2012. Publishing microdata with a robust privacy guarantee. Proceedings of the

VLDB Endowment 5, 11 (2012), 1388‚Äì1399.

[20] Fred H Cate, D Annette Fields, and James K McBain. 1994. The right to privacy and the public‚Äôs right to know: The

central purpose of the Freedom of Information Act. Admin. L. Rev. 46 (1994), 41.

[21] Chien-Lun Chen, Ranjan Pal, and Leana Golubchik. 2016. Oblivious mechanisms in diÔ¨Äerential privacy: experiments,

conjectures, and open questions. In 2016 IEEE Security and Privacy Workshops (SPW). IEEE, 41‚Äì48.

[22] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and

the cost of fairness. arXiv preprint arXiv:1701.08230 (2017).

[23] A. Datta, S. Sen, and Y. Zick. 2016. Algorithmic Transparency via Quantitative Input InÔ¨Çuence: Theory and Experi-

ments with Learning Systems. In 2016 IEEE Symposium on Security and Privacy (SP). 598‚Äì617.

[24] Amit Datta, Michael Carl Tschantz, and Anupam Datta. 2015. Automated experiments on ad privacy settings. Pro-

ceedings on Privacy Enhancing Technologies 2015, 1 (2015), 92‚Äì112.

[25] Dua Dheeru and EÔ¨Å Karra Taniskidou. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml
[26] Nicholas Diakopoulos. 2014. Algorithmic accountability reporting: On the investigation of black boxes. (2014).
[27] Kelly Dilworth. [n.d.]. We still don‚Äôt know a lot about how credit card applications are evaluated. https://t.ly/ifSo.
[28] Filip Karlo Do≈°iloviƒá, Mario Brƒçiƒá, and Nikica Hlupiƒá. 2018. Explainable artiÔ¨Åcial intelligence: A survey. In 2018 41st
International convention on information and communication technology, electronics and microelectronics. IEEE, 0210‚Äì
0215.

[29] Fl√°vio du Pin Calmon and Nadia Fawaz. 2012. Privacy against statistical inference. In Communication, Control, and

Computing (Allerton), 2012 50th Annual Allerton Conference on. IEEE, 1401‚Äì1408.

Achieving Transparency Report Privacy in Linear Time

29

[30] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Aware-
ness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (Cambridge, Massachusetts)
(ITCS ‚Äô12). ACM, New York, NY, USA, 214‚Äì226.

[31] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private

data analysis. In Theory of Cryptography Conference. Springer, 265‚Äì284.

[32] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of diÔ¨Äerential privacy. Foundations and Trends

in Theoretical Computer Science 9, 3‚Äì4 (2014), 211‚Äì407.

[33] Michael D Ekstrand, Rezvan Joshaghani, and Hoda Mehrpouyan. 2018. Privacy for All: Ensuring Fair and Equitable

Privacy Protections. In Conference on Fairness, Accountability and Transparency. 35‚Äì47.

[34] Barbara Espinoza and GeoÔ¨Ärey Smith. 2013. Min-entropy as a resource.

Information and Computation 226 (2013),

57‚Äì75.

[35] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certi-
fying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. ACM, 259‚Äì268.

[36] Katherine Fink. 2018. Opening the government‚Äôs black boxes: freedom of information and algorithmic accountability.

Information, Communication & Society 21, 10 (2018), 1453‚Äì1471.

[37] Kate Finman. [n.d.]. CA state auditor report alleges UC admissions are biased, unfair. https://t.ly/7VcF. Accessed:

2021-02-19.

[38] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2018. Model Class Reliance: Variable Importance Measures for
any Machine Learning Model Class, from the" Rashomon" Perspective. arXiv preprint arXiv:1801.01489 (2018).
[39] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit conÔ¨Ådence informa-
tion and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM, 1322‚Äì1333.

[40] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. Privacy in Phar-
macogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.. In USENIX Security Symposium. 17‚Äì32.

[41] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001),

1189‚Äì1232.

[42] Jerome H Friedman, Bogdan E Popescu, et al. 2008. Predictive learning via rule ensembles. The Annals of Applied

Statistics 2, 3 (2008), 916‚Äì954.

[43] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. Peeking inside the black box: Visualizing statis-
tical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics 24, 1
(2015), 44‚Äì65.

[44] Bryce Goodman and Seth Flaxman. 2016. European Union regulations on algorithmic decision-making and a "right

to explanation". arXiv preprint arXiv:1606.08813 (2016).

[45] Brandon M Greenwell, Bradley C Boehmke, and Andrew J McCarthy. 2018. A Simple and EÔ¨Äective Model-Based

Variable Importance Measure. arXiv preprint arXiv:1805.04755 (2018).

[46] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A

survey of methods for explaining black box models. ACM Computing Surveys (CSUR) 51, 5 (2018), 93.

[47] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in supervised learning. In Advances in

Neural Information Processing Systems. 3315‚Äì3323.

[48] Harold V Henderson and Shayle R Searle. 1981. On deriving the inverse of a sum of matrices. Siam Review 23, 1 (1981),

53‚Äì60.

[49] Tamara E. Holmes. [n.d.]. How race aÔ¨Äects your credit score. https://t.ly/6gf V. Accessed: 2021-02-19.
[50] Andreas Holzinger, Chris Biemann, Constantinos S Pattichis, and Douglas B Kell. 2017. What do we need to build

explainable AI systems for the medical domain? arXiv preprint arXiv:1712.09923 (2017).

[51] Giles Hooker. 2004. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD

international conference on Knowledge discovery and data mining. ACM, 575‚Äì580.

[52] Mikella Hurley and Julius Adebayo. 2016. Credit scoring in the era of big data. Yale JL & Tech. 18 (2016), 148.
[53] Farhad Kamali and Hilary Wynne. 2010. Pharmacogenetics of warfarin. Annual review of medicine 61 (2010), 63‚Äì75.
[54] Faisal Kamiran, IndrÀôe ≈ΩliobaitÀôe, and Toon Calders. 2013. Quantifying explainable discrimination and removing illegal

discrimination in automated decision making. Knowledge and Information Systems 35, 3 (2013), 613‚Äì644.

[55] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classiÔ¨Åer with prejudice

remover regularizer. Machine Learning and Knowledge Discovery in Databases (2012), 35‚Äì50.

[56] Igor Kononenko et al. 2010. An eÔ¨Écient explanation of individual classiÔ¨Åcations using game theory. Journal of

Machine Learning Research 11, Jan (2010), 1‚Äì18.

[57] Sanjay Krishnan and Eugene Wu. 2017. PALM: Machine learning explanations for iterative debugging. In Proceedings

of the 2nd Workshop on Human-In-the-Loop Data Analytics. ACM, 4.

30

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

[58] Martin Kuƒçera, Petar Tsankov, Timon Gehr, Marco Guarnieri, and Martin Vechev. 2017. Synthesis of probabilistic
privacy enforcement. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.
ACM, 391‚Äì408.

[59] Bruno Lepri, Nuria Oliver, Emmanuel Letouz√©, Alex Pentland, and Patrick Vinck. 2018. Fair, transparent, and account-

able algorithmic decision-making processes. Philosophy & Technology 31, 4 (2018), 611‚Äì627.

[60] Floridi Luciano and Taddeo Mariarosaria. 2016. What is data ethics? Phil. Trans. R. Soc. A.37420160360 (2016).
[61] Ashwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan Venkitasubramaniam. 2006.

ùëô-
diversity: Privacy beyond ùëò-anonymity. In Data Engineering, 2006. ICDE‚Äô06. Proceedings of the 22nd International Con-
ference on. IEEE, 24‚Äì24.

[62] Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubramaniam. 2007.
diversity: Privacy beyond ùëò-anonymity. ACM Transactions on Knowledge Discovery from Data 1, 1 (2007), 3.

ùëô-

[63] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. A survey on bias and

fairness in machine learning. arXiv preprint arXiv:1908.09635 (2019).

[64] H. Mittelmann. [n.d.]. Benchmark of commercial LP solvers. http://plato.asu.edu/ftp/lpcom.html. Accessed: 2021-03-

6.

[65] Tomas Monarrez and Kelia Washington. 2020. Racial and Ethnic Representation in Postsecondary Education. Research

Report. Urban Institute (2020).

[66] S Alvim M‚Äôrio, Kostas Chatzikokolakis, Catuscia Palamidessi, and GeoÔ¨Ärey Smith. 2012. Measuring information
leakage using generalized gain functions. In 2012 IEEE 25th Computer Security Foundations Symposium. IEEE, 265‚Äì
279.

[67] Arvind Narayanan and Vitaly Shmatikov. 2008. Robust de-anonymization of large sparse datasets. In Security and

Privacy, 2008. SP 2008. IEEE Symposium on. IEEE, 111‚Äì125.

[68] Cathy O‚Äôneil. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway

Books.

[69] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016. Towards the science of security and

privacy in machine learning. arXiv preprint arXiv:1611.03814 (2016).

[70] Neel Patel, Reza Shokri, and Yair Zick. 2020. Model explanations with diÔ¨Äerential privacy. arXiv:2006.09129 (2020).
[71] Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as mechanisms for supporting algorithmic trans-

parency. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1‚Äì13.

[72] Joel R Reidenberg and Florian Schaub. 2018. Achieving big data privacy in education. Theory and Research in Education

16, 3 (2018), 263‚Äì279.

[73] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ‚ÄúWhy should I trust you?‚Äù: Explaining the predictions
of any classiÔ¨Åer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining. ACM, 1135‚Äì1144.

[74] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explanations.

In AAAI Conference on ArtiÔ¨Åcial Intelligence.

[75] Pierangela Samarati and Latanya Sweeney. 1998. Generalizing data to provide anonymity when disclosing informa-

tion. In PODS, Vol. 98. Citeseer, 188.

[76] Pierangela Samarati and Latanya Sweeney. 1998. Protecting privacy when disclosing information: k-anonymity and its

enforcement through generalization and suppression. Technical Report. Technical report, SRI International.

[77] Wojciech Samek, Gr√©goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M√ºller. 2019. Explainable

AI: interpreting, explaining and visualizing deep learning. Vol. 11700. Springer Nature.

[78] Nisha Shekhawat, Aakanksha Chauhan, and Sakthi Balan Muthiah. 2019. Algorithmic Privacy and Gender Bias Issues

in Google Ad Settings. In Proceedings of the 10th ACM Conference on Web Science. 281‚Äì285.

[79] Reza Shokri, Martin Strobel, and Yair Zick. 2020. On the Privacy Risks of Model Explanations. arXiv preprint

arXiv:1907.00164v5 (2020).

[80] Reza Shokri, Marco Stronati, and Vitaly Shmatikov. 2016. Membership inference attacks against machine learning

models. arXiv preprint arXiv:1610.05820 (2016).

[81] Robert H Sloan and Richard Warner. 2018. When is an algorithm transparent? Predictive analytics, privacy, and public

policy. IEEE Security & Privacy 16, 3 (2018), 18‚Äì25.

[82] Bernd Carsten Stahl and David Wright. 2018. Ethics and privacy in AI and big data: Implementing responsible research

and innovation. IEEE Security & Privacy 16, 3 (2018), 26‚Äì33.

[83] Latanya Sweeney. 1997. Weaving technology and policy together to maintain conÔ¨Ådentiality. The Journal of Law,

Medicine & Ethics 25, 2-3 (1997), 98‚Äì110.

[84] Latanya Sweeney. 2000. Simple demographics often identify people uniquely. Health 671 (2000), 1‚Äì34.
[85] RC Thompson. 1978. Matrix type metric inequalities. Linear and Multilinear Algebra 5, 4 (1978), 303‚Äì319.

Achieving Transparency Report Privacy in Linear Time

31

[86] Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition

(CVPR), 2011 IEEE Conference on. IEEE, 1521‚Äì1528.

[87] Florian Tram√®r, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning

Models via Prediction APIs.. In USENIX Security Symposium. 601‚Äì618.

[88] Ke Wang, Benjamin CM Fung, and Guozhu Dong. 2005. Integrating private databases for data analysis. In International

Conference on Intelligence and Security Informatics. Springer, 171‚Äì182.

[89] Ke Wang, Benjamin CM Fung, and S Yu Philip. 2007. Handicapping attacker‚Äôs conÔ¨Ådence: an alternative to ùëò-

anonymization. Knowledge and Information Systems 11, 3 (2007), 345‚Äì368.

[90] Teresa Watanabe. [n.d.]. UCLA professor wants to see data on whether UC illegally uses race in admissions decisions.

https://t.ly/ny6t.

[91] Meg Young, Luke Rodriguez, Emily Keller, Feiyang Sun, Boyang Sa, Jan Whittington, and Bill Howe. 2019. Beyond
open vs. closed: Balancing individual privacy and public accountability in data sharing. In Proceedings of the Conference
on Fairness, Accountability, and Transparency. 191‚Äì200.

[92] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning fair representations. In Proceed-

ings of the 30th International Conference on Machine Learning (ICML-13). 325‚Äì333.

[93] Alexander Zien, Nicole Kr√§mer, S√∂ren Sonnenburg, and Gunnar R√§tsch. 2009. The feature importance ranking mea-
sure. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 694‚Äì709.
[94] Stanley Zionts. 1968. Programming with linear fractional functionals. Naval Research Logistics Quarterly 15, 3 (1968),

449‚Äì451.

32

APPENDIX

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

A FAIRNESS MEASURES
Another important motivation of providing algorithmic transparency is to understand if a decision-
making algorithm is fair. GDPR Article 5 regulation indicates that personal data should be pro-
cessed fairly and in a transparent manner. Many researchers are committed to providing proper
measures for fairness and making ML algorithms fair [15, 22, 30, 35, 54, 55, 92]. In general, there are
two main categories of fairness: (i) individual fairness, and (ii) group fairness. Popular deÔ¨Ånitions
of group fairness includes statistical parity (SP), conditional statistical parity (CSP), and p%-rule
(PR) (see Appendix A for detailed deÔ¨Ånitions).

A.1 Measures for Individual Fairness
DeÔ¨Ånition 8. ((ùîá, D)-Individual Fairness [30]) Given a distance measure D : Rùëã √ó Rùëã ‚Üí R+ ,
[0, ‚àû) on individuals‚Äô records, a decision mapping ùê∑ : Rùëã ‚Üí Œî(A) satisÔ¨Åes individual fairness if it
complies with the (ùîá, D)-Lipschitz property for every two individuals‚Äô records x1, x2 ‚àà Rùëã , i.e.,

ùîá(ùê∑ (x1), ùê∑ (x2)) ‚â§ D(x1, x2),
(23)
where ùîá : Œî(A) √ó Œî(A) ‚Üí R+ is a distance measure on distributions over A. Moreover, we deÔ¨Åne
ùê∑ satisfying individual fairness up to bias ùúÄ if for all x1, x2 ‚àà Rùëã , we have

ùîá(ùê∑ (x1), ùê∑ (x2)) ‚â§ D(x1, x2) + ùúÄ.

(24)

Individual fairness ensures a decision mapping maps similar people similarly. When two indi-
viduals‚Äô records x1 and x2 are similar, i.e., D(x1, x2) (cid:27) 0, the Lipschitz condition in equation (23)
ensures that both records map to similar distributions over A. Candidates for distance measure ùîá
include (but not limited to) statistical distance and relative ùëô‚àû metric. The relative ùëô‚àû metric ( a.k.a.
relative inÔ¨Ånity norm) of two distributions ùëç1 and ùëç2, deÔ¨Åned as follow

ùîá‚àû (ùëç1, ùëç2) = sup
ùëé ‚ààA

log

max

ùëç1(ùëé)
ùëç2(ùëé)

,

ùëç2(ùëé)
ùëç1(ùëé)

,

(25)

(cid:18)
is considered a potential better choice in the aspect that it does not require the distance measure
D to be re-scaled within [0, 1] 8. However, it has the shortcoming that it is sensitive to small
probability values. The statistical distance, or the total variation norm, of two distributions ùëç1 and
ùëç2, deÔ¨Åned as follow

(cid:27)(cid:19)

(cid:26)

ùîátv (ùëç1, ùëç2) =

1
|A|

is a more stable measure in this aspect.

ùëç1(ùëé) ‚àí ùëç2(ùëé)

,

(26)

√ïùëé ‚ààA (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

A.2 Measures for Group Fairness
Popular measures for group fairness include (but not limited to) statistical parity (SP) (a.k.a. demo-
graphic parity) [30, 35, 55, 92], conditional statistical parity (CSP) [22, 54], ùëù-% rule (PR) [15, 35],
accuracy parity (a.k.a. equalized odds) [47], and true positive parity (a.k.a. equal opportunity) [47].
However, the last two measures require knowledge of labeled outputs and is thus particular used
to train fair ML algorithms in supervised learning. For algorithmic transparency, we use the former
three measures for group fairness.

DeÔ¨Åne ùëî(ùëã ) a projection function from input attributes ùëã onto a group in protected attributes,
ùë£ (ùëã ) a score/valuation function from ùëã onto a set scores, and ùëáY , {x ‚àà Rùëã | ùëî(x) ‚àà Y} the

8The normalization could bring non-trivial burden, especially when the maximal distance can be arbitrarily large.

Achieving Transparency Report Privacy in Linear Time

33

set/tuple in which records belong to a protected group Y. We summarize deÔ¨Ånitions of measures
for group fairness in the following:

DeÔ¨Ånition 9. (Statistical Parity (SP)) A decision mapping ùê∑ : Rùëã ‚Üí Œî(A) satisÔ¨Åes statistical
parity for two groups Y1 and Y2 up to bias ùúÄ if for every decision outcome ùëé ‚àà A, we have the
following property

ùîátv

E

ùê∑ùëé (ùëã )|ùëáY1

, E

ùê∑ùëé (ùëã )|ùëáY2

‚â§ ùúÄ.

(27)

(cid:16)

(cid:2)

(cid:3)

(cid:2)

(cid:3) (cid:17)

DeÔ¨Ånition 10. (Conditional Statistical Parity (CSP)) Given a score/valuation function ùë£ (ùëã ) based
on input attributes ùëã , deÔ¨Åne ùëáY,V , {x ‚àà Rùëã | ùëî(x) ‚àà Y, ùë£ (x) ‚àà V} the set/tuple in which records
belong to a protected group Y having scores in a set V. A decision mapping ùê∑ : Rùëã ‚Üí Œî(A) satisÔ¨Åes
conditional statistical parity given the same score conditions V for two groups Y1 and Y2 up to bias
ùúÄ if for every decision outcome ùëé ‚àà A, we have the following property

ùîátv

E

ùê∑ùëé (ùëã )|ùëáY1,V

, E

ùê∑ùëé (ùëã )|ùëáY2,V

‚â§ ùúÄ.

(28)

DeÔ¨Ånition 11. (ùëù-% Rule (PR)) A decision mapping ùê∑ : Rùëã ‚Üí Œî(A) satisÔ¨Åes ùëù-% rule for two
groups Y1 and Y2 if for every decision outcome ùëé ‚àà A, we have the following property

(cid:2)

(cid:3)

(cid:2)

(cid:3) (cid:17)

‚â§ ‚àí log ùëù.

(29)

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

log

E

E

ùê∑ùëé (ùëã )|ùëáY1
ùê∑ùëé (ùëã )|ùëáY2

(cid:2)

(cid:3)

(cid:18)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3)

In particular, for binary decisions, we say a decision rule ùëë satisÔ¨Åes SP, CSP, or PR for two groups

(cid:2)

Y1 and Y2 up to bias ùúÄ (SP and CSP only) if

SP:

E

ùëë (ùëã )|ùëáY1

‚àí E

ùëë (ùëã )|ùëáY2

‚â§ ùúÄ

(cid:2)
(cid:3)
ùëë (ùëã )|ùëáY1,V

(cid:2)
‚àí E

ùëë (ùëã )|ùëáY2,V

(cid:3) (cid:12)
(cid:12)
(cid:12)

CSP:

(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
(cid:2)
(cid:12)
PR: ùëù ‚â§
(cid:12)

E

E

ùëë (ùëã )|ùëáY1
(cid:3)
ùëë (ùëã )|ùëáY2
(cid:2)

‚â§

(cid:2)
1
ùëù

.

‚â§ ùúÄ

(cid:3) (cid:12)
(cid:12)
(cid:12)

(30)

(31)

(32)

(cid:3)
Note that all fairness deÔ¨Ånitions are based on the distance between the decision of two groups9,
(cid:3)
speciÔ¨Åcally, total variation (26) and relative metric (25). Let F denote the set of all fairness deÔ¨Åni-
tions. Based on the use of distance metrics, F can be classiÔ¨Åed as follows:

(cid:2)

‚Ä¢ Total-variation-based fairness deÔ¨Ånitions (Ftv): DeÔ¨Ånitions include (ùîátv, D)-individual fair-

ness, statistical parity, and conditional statistical parity.

‚Ä¢ Relative-metric-based fairness deÔ¨Ånitions (Frm): DeÔ¨Ånitions include (ùîá‚àû, D)- individual fair-

ness and p%-rule.

B PRIVACY LEAKAGE VIA FEATURE IMPORTANCE/INTERACTION

Feature (value) importance, or feature (value) interaction, measures the importance (or inÔ¨Çuence) of
input attributes (or attribute values) to the decision outcomes. The importance of an input attribute
(value) is measured based on the corresponding change of output due to change of that certain input.
By changing an input, if the change of output is signiÔ¨Åcant, it implies the input is important (has
signiÔ¨Åcant inÔ¨Çuence) to the output. On the other hand, if the output changes very little, the input
contributes very little to the output.

9More precisely, from (27), (28), and (29), the decision distribution of a group is the expected decision mapping among the
group, over all decision outcomes ùëé ‚àà A.

34

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Table 5. Attribute Information of the Credit Approval Dataset

A1: b, a.
A2: continuous.
A3: continuous.
A4: u, y, l, t.
A5: g, p, gg.
A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, Ô¨Ä. A14: continuous.
A15: continuous.
A7: v, h, bb, j, n, z, dd, Ô¨Ä, o.
A16: +,- (class attribute)
A8: continuous.

t, f.
A9:
A10: t, f.
A11: continuous.
A12: t, f.
A13: g, p, s.

DiÔ¨Äerent works may propose diÔ¨Äerent measures, but their philosophies are almost the same (as
stated above). For example, the measures for change of an input can be (i) removing the presence
of an input attribute, or (ii) permuting attribute values on an input attribute. The measures of
outputs are many, e.g., (i) accuracy of the (predicted) outputs [18, 38], (ii) probability of receiving a
certain outcome [23], (iii) statistics measures, such as partial dependence [41, 45], H-statistic [42],
or variable interaction networks [51], or (iv) a self-deÔ¨Åned quantity or a score/gain function. The
measures for the change of outputs can be (i) diÔ¨Äerence (i.e., subtraction), (ii) ratio, or (iii) averaged
diÔ¨Äerence/contribution, e.g., the Shapley value [56], of the measured outputs. In this regard, it is
impractical for us to demonstrate the privacy leakage issue for all present methods. However,
since the philosophies of all these methods are similar, it is reasonable for us to demonstrate the
privacy hacking procedures via a representative one. The principles of hacking procedures can be
transferred and applied to other methods.

We investigate potential privacy leakage via the quantitative input inÔ¨Çuence (QII) proposed in
the most pioneering work [23] in accountable ATR. For QII, the measure for change of an input is
permuting attribute values (called intervention in the paper) on an input attribute. The measure of
output can be user-speciÔ¨Åed, called quantity of interest, denoted by ùëÑ. The measure for change of
output is diÔ¨Äerence between (subtraction of) two measured outputs. Formally, the QII of an input
attribute ùëò for a quantity of interest ùëÑ is deÔ¨Åned as

I ùëÑ (ùëò) = ùëÑ (ùëã ) ‚àí ùëÑ (ùëã‚àíùëòùëàùëò ),

(33)

in which ùëã‚àíùëòùëàùëò , meaning that attribute ùëò is (removed from input ùëã and) replaced by a permuted
version ùëàùëò, represents intervention on attribute ùëò. In particular, for ùëÑ (ùëã ) = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW },
the fraction of records belonging to a set ùëáW (e.g., women) with positive classiÔ¨Åcation, the QII of
an input attribute ùëò is

I (ùëò) = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW } ‚àí ùëÉ {ùëê (ùëã‚àíùëòùëàùëò ) = 1|ùëã ‚àà ùëáW },

(34)

where ùëê (¬∑) is a classiÔ¨Åer (decision-maker). The QII of a set of input attributes K is deÔ¨Åned similarly,
using K instead of ùëò.

In the following, we conduct an experiment to demonstrate the hacking of decision rules via
provided QII‚Äôs on an ATR for a real dataset, and utilize the hacked decision rules to further infer
private records as what we did in Section 3.2. We use the Australian credit approval dataset from
UCI machine learning repository [25] in our experiment10. The dataset has 690 instances, with
15 input attributes and 1 output attribute. All attribute information can be found in Table 5. In
order to protect conÔ¨Ådentiality of the data, all attribute names and values have been changed to

10Since we are demonstrating stealing private information from a real dataset, the chosen dataset needs to contain critical
information, and its size needs to be adequate: on the one hand, it should not be too large for ease of demonstration; on
the other hand, it should not be too small for the accuracy of the trained classiÔ¨Åer.

Achieving Transparency Report Privacy in Linear Time

35

Table 6. A Snapshot of the QID Group ùëáxU={y, p, k, v}
A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 A14 A15 A16
y
y
y
y
y
y
y
y
y
y
y

160
0
224
0
200
0
280
0
140
4
70
200
216 2100
80
280
20
180
320
0
380 2732

v 0.125
0.25
v
0.29
v
v
1.25
v 0.125
v 0.125
v 0.085
v 0.415
2.5
v
0.25
v
0.25
v

p k
p k
p k
p k
p k
p k
p k
p k
p k
p k
p k

0
0
0
0
0
0
0
1
1
10
11

-
-
-
-
-
-
-
-
-
+
+

g
g
s
g
g
g
g
g
g
g
g

f
f
f
f
f
f
f
f
t
t
t

f
f
f
t
f
f
f
t
f
t
f

f
f
f
f
f
f
f
t
t
t
t

meaningless symbols by the dataset provider. Based on the dataset, with adequate data cleaning
and pre-processing, we train a classiÔ¨Åer based on a fully-connected neural network with one input
layer (36 inputs, after one-hot encoding for categorical attribute values), two hidden layers (147
and 85 neurons, respectively), and one output layer (binary outputs), with dropout rate 0.5. The
averaged testing accuracy of the trained classiÔ¨Åer is 89.5%.

The trained classiÔ¨Åer is served as the knowledge of a trust-worthy 3rd-party regulation agency
which feeds both inputs and outputs of the dataset to a ML model in order to learn the unknown
decision-making rules of this Australian credit card company. Since QII is a data-mining based
approach [23], the regulation agency provides information regarding input inÔ¨Çuences (QII) in an
ATR upon users‚Äô demand. Since the access control is still an open question, we assume a user is
able to request such information in a reasonable manner.

Based on the above experimental settings, we Ô¨Årst construct a scenario to demonstrate the hack-

ing.

Scenario:
‚Ä¢ Let U = {A4, A5, A6, A7} be public attributes and all other attributes are private and unknown

to adversaries (See Remark 6).

‚Ä¢ Alice has public record xU = {y, p, k, v}. She gets a positive decision (+) and receives a credit

card.

‚Ä¢ Tom also has the same public record xU = {y, p, k, v}. He gets a negative decision (-).
‚Ä¢ An adversary is a friend of both, knowing their public records, knowing that Alice owns such
a credit card but Tom doesn‚Äôt. The adversary also has the knowledge of joint distribution of
A4‚àºA7, A9, and A11, e.g., demographic statistics of age, marriage status, race, and annual in-
come.

A snapshot of the QID group ùëáxU ={y, p, k, v} is shown in Table 611, in which public attributes are
marked as grey, class attribute (decision outcome) is marked as light blue, and for those attributes
that an adversary has associated side-information (joint distribution) are marked as bold.

We next demonstrate privacy hacking procedures in the following. Let W0 ={A4=y, A5=p, A6=k,

A7=v, A11‚àà[0,1]}, and W1 ={A4=y, A5=p, A6=k, A7=v, A11‚àà[10,11]}.

Privacy Hacking:
(1) Since the input to QII can be a set of attributes, i.e., the joint inÔ¨Çuence of a set of input attributes.
Let S be the collection of all private attributes as denoted in Table 3, which is {A1‚àºA3, A8‚àºA15}
in our scenario. The adversary then sends the following QII query to the regulation agency:

11We remove attributes A1‚àºA3 in the interest of space

36

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

‚Ä¢ Input Attribute: S
‚Ä¢ Quantity of Interest: ùëÑ (ùëã ) = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 }

(2) The adversary gets a response I (S) = 0.66475333, which indicates the degree of inÔ¨Çuence of

all private input attributes S to the group W1.

(3) The adversary sends the following QII query to the regulation agency:

‚Ä¢ Input Attribute: S
‚Ä¢ Quantity of Interest: ùëÑ (ùëã ) = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW0 }

(4) The adversary gets a response I (S) = ‚àí0.33524666, which indicates the degree of inÔ¨Çuence
of all private input attributes S to the group W0. Note that negative sign stands for negative
impact as mentioned in Section 3.1.

(5) From the above two query responses, the adversary has

0.66475333 = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚àí ùëÉ {ùëê (ùëã‚àíSùëà S) = 1|ùëã ‚àà ùëáW1 }
‚àí0.33524666 = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW0 } ‚àí ùëÉ {ùëê (ùëã‚àíSùëà S) = 1|ùëã ‚àà ùëáW0 }
(6) Since W1 and W0 have the same public record xU = {y, p, k, v}, for the same classiÔ¨Åer, we must

have

ùëÉ {ùëê (ùëã‚àíSùëà S) = 1|ùëã ‚àà ùëáW1 } = ùëÉ {ùëê (ùëã‚àíSùëà S) = 1|ùëã ‚àà ùëáW0 }.

(7) Utilize the above equality, the adversary obtains

ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚àí ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW0 } = 1.

Since probabilities are always within [0, 1], the adversary thus obtains decision rules

ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } = 1,
ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW0 } = 0.

It is worth mentioning that the attack may not be unique. As shown in the following, there could
exist many ways to obtain decision rules, and thus it seems hopeless to cease the attack simply by
access control.

Privacy Hacking (Method 2):
(1) The adversary sends the following QII query to the regulation agency:

‚Ä¢ Input Attribute: A9
‚Ä¢ Quantity of Interest: ùëÑ (ùëã ) = ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 }

(2) The adversary gets a response I (A9) = 0.45142778, which indicates the degree of inÔ¨Çuence of

input attribute A9 to the group W1.

(3) The adversary analyzes the response I (A9).DeÔ¨Åne ùëÉùë° = ùëÉ {ùëê (ùëã‚àíA9ùëàA9) = 1|ùëã ‚àà ùëáW1, ùëàA9 = t}

and ùëÉùëì = ùëÉ {ùëê (ùëã‚àíA9ùëàA9) = 1|ùëã ‚àà ùëáW1, ùëàA9 = f}. He gets

0.45142778 =ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚àí ùëÉ {ùëê (ùëã‚àíA9ùëàA9) = 1|ùëã ‚àà ùëáW1 }
=ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚àí ùëÉ {ùëàA9 = t}ùëÉùë° ‚àí ùëÉ {ùëàA9 = f}ùëÉùëì

(4) The adversary realizes the fact that, for the same classiÔ¨Åer, we must have

ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } =ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1, ùê¥9 = t}

=ùëÉ {ùëê (ùëã‚àíùê¥9ùëàùê¥9) = 1|ùëã ‚àà ùëáW1, ùëàùê¥9 = t} = ùëÉùë°

(5) Since the adversary has joint distribution knowledge as mentioned in the scenario, he knows

the marginal distribution ùëÉ {ùëàùê¥9 = f} = 1 ‚àí ùëÉ {ùëàùê¥9 = t} = 0.45142857, he then gets

ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚àí ùëÉùëì =

0.45142778
0.45142857

‚âà 1

Achieving Transparency Report Privacy in Linear Time

37

(6) Since probabilities are always within [0, 1], the adversary knows ùëÉùëì ‚âà 0, and

ùëÉ {ùëê (ùëã ) = 1|ùëã ‚àà ùëáW1 } ‚âà 1.

The adversary obtains very accurate information regarding decision rule for W1.
Based on the hacked decision rules above, the adversary has 100% conÔ¨Ådence that Alice‚Äôs record
belongs to ùëáW1 and Tom‚Äôs record belongs to ùëáW0. Based on Table 6, he then knows that Alice‚Äôs
A11 attribute value is either 10 or 11, and Tom‚Äôs is either 0 or 1. If the adversary has richer side-
information, e.g., joint distribution including A8 and A14, then the adversary has 100% conÔ¨Ådence
that Alice‚Äôs A8 attribute value is 0.25, her A14 attribute value is in the range between 300 and 400,
and Tom‚Äôs A14 attribute value is in the range between 100 and 300.

It is worth mentioning that, based on our investigation, we do not Ô¨Ånd a general attack method
that can be applied to all datasets and decision rules. However, this does not mean the attacks
demonstrated above are cherry-picked. As we have shown, there could exist many feasible attack
approaches. Adversaries can simply try multiple diÔ¨Äerent attempts and/or collude their test results
so that eventually acquire a successful attack result. Moreover, similar to the privacy incidents of
AOL search data leak [11] and de-anonymization of the NetÔ¨Çix Price dataset [67], although there is
no guarantee that the attacks can always succeed in all the cases, as long as the attack can succeed,
there exists a privacy breach which can result in a catastrophic disaster.

In fact, the authors of the pioneering work, i.e., [23], had already noticed the potential privacy
issue in algorithmic transparency and added noise to make the measures diÔ¨Äerentially private.
Unfortunately, adding diÔ¨Äerentially private noise [31] solely cannot mitigate the demonstrated
privacy leakage issue. The fundamental reason is that diÔ¨Äerential privacy only guarantees a small
amount of information leakage when an individual participates the survey or opts into a data-
base. DiÔ¨Äerential privacy itself does not guarantee information leakage due to strong statistical
inference between attributes; this has been noted in many previous works such as [7, 13, 29], and
section 2.3.2 in [32]. The most classic example is the study of ‚Äúsmoking causes cancer‚Äù, in which
no matter whether a person opts into the survey or not, once we know that he is a smoker, we
know he has a certain high chance of getting lung cancer. What can be guaranteed in the proposed
diÔ¨Äerentially private perturbation for an ATR is that an adversary can only gain very little infor-
mation by comparing two ATRs of which the training data to train the classiÔ¨Åers are diÔ¨Äer in only
one data subject‚Äôs record. When the size of dataset is very large, the required variance of DP noise
is very small. This is why they claimed only very little noise needs to be added.

Remark 6. Although all attribute names in the dataset are removed, we are still able to reason-
ably conjecture public and private attributes based on their inÔ¨Çuences to the decision outcome.
Attributes with high inÔ¨Çuences are more likely to be private attributes such as income or credit
score, and attributes with low inÔ¨Çuences are likely to be public ones. Observing that attribute A9,
A11, and A15 are the most inÔ¨Çuential ones and others are less signiÔ¨Åcant (from experiments). For
ease of demonstration, we choose 4 adjacent categorical attributes from insigniÔ¨Åcant ones, A4 to
A7, to serve as public attributes.

C MINIMUM UNCERTAINTY
DeÔ¨Ånition 12. (Minimum Uncertainty) Given an inference channel hùëã U, ùê¥ ‚Üí ùëã Si, the uncertainty
of inferring a certain sensitive attribute value ùë• S from a certain inference source {xU, ùëé} is deÔ¨Åned
as ucrt (xU, ùëé ‚Üí ùë• S) = ‚àí log
. The minimum uncertainty of inferring any

conf (xU, ùëé ‚Üí ùë• S)

(cid:0)

(cid:1)

38

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

sensitive value from any inference channel is

Ucrt(ùëã U, ùê¥ ‚Üí ùëã S) = min
xU,ùëé,ùë•S
= ‚àí log

{‚àí log

conf (xU, ùëé ‚Üí ùë• S)

}

(cid:0)
(cid:1)
{conf (xU, ùëé ‚Üí ùë• S)}
max
xU,ùëé,ùë•S
Conf (ùëã U, ùê¥ ‚Üí ùëã S)

.

(cid:1)

= ‚àí log

(cid:0)

(cid:0)
Similarly, the corresponding privacy requirement for minimal uncertainty is the following.

(cid:1)

DeÔ¨Ånition 13. (ùõæ-Minimum Uncertainty) In an algorithmic transparency report, Àúùê∑ satisÔ¨Åes ùõæ-Minimum
Uncertainty if Ucrt (ùëã U, ùê¥ ‚Üí ùëã S) ‚â• ùõæ.

The above privacy requirement is basically saying that an adversary‚Äôs uncertainty on inferring
any sensitive value from any inference channel cannot be too low and should be lower-bounded
by a threshold ùõæ; the larger the ùõæ, the higher the minimum uncertainty, and thus the stronger
the privacy. From deÔ¨Ånition (12), it is clear that ùõæ-Minimum Uncertainty implies ùëí‚àíùõæ -Maximum
ConÔ¨Ådence, and ùõΩ-Maximum ConÔ¨Ådence implies ‚àí log ùõΩ-Minimum Uncertainty.

Lemma 7. The privacy requirement ùõæ-Minimum Uncertainty imposes the following constraints to
the announced decision mapping Àúùê∑, ‚àÄx ‚àà Rùëã , ‚àÄùëé ‚àà A,

log

Àúùê∑ùëé (x‚Ä≤)ùëÉùëã (x‚Ä≤)

‚àí log

Àúùê∑ùëé (x)ùëÉùëã (x)

‚â• ùõæ .

(35)

(cid:0)√ïx‚Ä≤

(cid:1)

(cid:0)

(cid:1)

D PROOF OF LEMMA 1

Proof. Recall that conf (xU, ùëé ‚Üí ùë• S), the conÔ¨Ådence of inferring a sensitive attribute value ùë• S,

is a posterior epistemic probability which can be expressed as

conf (xU, ùëé ‚Üí ùë• S) = ÀúùëÉùëãS |ùëãU,ùê¥ (ùë• S |xU, ùëé) =

ÀúùëÉùê¥ |ùëãU,ùëãS (ùëé|xU, ùë• S)ùëÉùëãU,ùëãS (xU, ùë• S)

ÀúùëÉùê¥ |ùëãU ,ùëãS (ùëé|xU, ùë• ‚Ä≤

S)ùëÉùëãU,ùëãS (xU, ùë• ‚Ä≤
S)

.

(36)

ùë• ‚Ä≤
S ‚ààRùëãS
√ç
, {x‚Ä≤ ‚àà Rùëã | x‚Ä≤
U = xU } to denote the tuple in which records

Let x = (xU, ùë• S) and deÔ¨Åne ùëáxU
having the same QID xU. We have a more comprehensive expression

conf (xU, ùëé ‚Üí ùë• S) =

ÀúùëÉùê¥ |ùëã (ùëé|x)ùëÉùëã (x)

ÀúùëÉùê¥ |ùëã (ùëé|x‚Ä≤)ùëÉùëã (x‚Ä≤)

=

Àúùê∑ùëé (x)ùëÉùëã (x)

Àúùê∑ùëé (x‚Ä≤)ùëÉùëã (x‚Ä≤)

.

(37)

x‚Ä≤ ‚ààùëáxU
√ç

x‚Ä≤ ‚ààùëáxU
√ç

Therefore, based on DeÔ¨Ånitions 3 and 4, the privacy requirement ùõΩ-Maximum ConÔ¨Ådence imposes
the following constraints for all x = (xU, ùë• S) ‚àà Rùëã , ‚àÄùëé ‚àà A.

Àúùê∑ùëé (x)ùëÉùëã (x)

Àúùê∑ùëé (x‚Ä≤)ùëÉùëã (x‚Ä≤)

x‚Ä≤ ‚ààùëáxU
√ç

‚â§ ùõΩ.

(38)

E PROOF OF LEMMA 4

Proof. We Ô¨Årst prove that if ùõΩ ‚â• ùê∂‚àó, Àúùê∑ = ùê∑ is a feasible solution. We then prove its converse:

if Àúùê∑ = ùê∑ is a feasible solution, we must have ùõΩ ‚â• ùê∂‚àó.

We Ô¨Årst prove that if ùõΩ ‚â• ùê∂‚àó, the 1-Ô¨Ådelity solution Àúùê∑ = ùê∑ is a feasible solution, i.e., it satisÔ¨Åes all
constraints. Obviously, the solution Àúùê∑ = ùê∑ satisÔ¨Åes probability distribution conditions and Ô¨Ådelity

Achieving Transparency Report Privacy in Linear Time

39

constraints. Based on deÔ¨Ånition 7, Àúùê∑ = ùê∑ yields

ùëÉ (x) Àúùê∑ùëé (x)

ùëÉ (x‚Ä≤) Àúùê∑ùëé (x‚Ä≤)

x‚Ä≤ ‚ààùëáxU
√ç

= ùê∂‚àó ‚â§ ùõΩ , ‚àÄx ‚àà ùëáxU , ‚àÄùëé ‚àà A.

Therefore, it also satisÔ¨Åes privacy constraints, and hence when ùõΩ ‚â• ùê∂‚àó, the 1-Ô¨Ådelity solution is a
feasible solution.

Next, we prove the converse by proving its contrapositive, i.e., if ùõΩ < ùê∂‚àó, Àúùê∑ = ùê∑ is not a feasible
solution. Apparently when Àúùê∑ = ùê∑, the highest conÔ¨Ådence that an adversary can have exceeds ùõΩ,
and hence it violates privacy requirements and cannot be a feasible solution. We therefore prove
(cid:3)
the converse.

F PROOF OF LEMMA 5

Proof. We Ô¨Årst prove that if an (OPT-Sub) has feasible solutions, ùõΩ ‚â• ùõΩmin. We then prove its

converse: if ùõΩ ‚â• ùõΩmin, an (OPT-Sub) must have feasible solutions.

We Ô¨Årst prove the conditional statement by proving its contrapositive, i.e., if ùõΩ < ùõΩmin, there
exists no feasible solution for an (OPT-Sub). Since Àúùê∑ is non-negative, we can rewrite the privacy
constraints as follow.

ùëÉ (x) Àúùê∑ùëé (x) ‚àí ùõΩ

ùëÉ (x‚Ä≤) Àúùê∑ùëé (x‚Ä≤) ‚â§ 0 ,

(39)

√ïx‚Ä≤ ‚ààùëáxU
which has to be satisÔ¨Åed ‚àÄx ‚àà ùëáxU and ‚àÄùëé ‚àà A. Sum (39) over all ùëé ‚àà A, by (EQ-Sub), we have

ùëÉ (x) ‚àí ùõΩ

ùëÉ (x‚Ä≤) ‚â§ 0 , ‚àÄx ‚àà ùëáxU ,

(40)

We then prove the converse. If ùõΩ ‚â• maxx‚ààùëáxU

√ïx‚Ä≤ ‚ààùëáxU
ùëÉ (x|ùëáxU ). Therefore, if there exists any x ‚àà ùëáxU such that
which is equivalent to ùõΩ ‚â• maxx‚ààùëáxU
ùõΩ < ùëÉ (x|ùëáxU ), then (39) cannot be satisÔ¨Åed for all x ‚àà ùëáxU , and hence no feasible solution exists.
ùëÉ (x|ùëáxU ), there always exists a feasible solution
Àúùê∑ùëé (x‚Ä≤) = 1/|A|, ‚àÄx ‚àà ùëáxU , ‚àÄùëé ‚àà A. To see this, we only need to verify if it satisÔ¨Åes all constraints.
It is very obvious that the solution satisÔ¨Åes probability distribution conditions. Since Ô¨Ådelity con-
straints are trivialized, we then only need to verify if the solution satisÔ¨Åes privacy constraints.
Since Àúùê∑ùëé (x‚Ä≤) is a constant for all ùëé and x, the left hand side of (39) becomes ùëÉ (x|ùëáxU ), and thus the
privacy constraints are also satisÔ¨Åed. Hence Àúùê∑ùëé (x‚Ä≤) = 1/|A| is a feasible solution and we proved
(cid:3)
the converse.

G PROOF OF LEMMA 6

Proof. We prove this by contradiction. Assume that ùê∂‚àó < ùõΩmin. By their deÔ¨Ånitions in Lemma

4 and 5, it follows that

ùëÉ (x)ùê∑ùëé (x)

ùëÉ (x‚Ä≤)ùê∑ùëé (x‚Ä≤)

< max
x‚ààùëáxU

ùëÉ (x)

ùëÉ (x‚Ä≤)

.

(41)

max
x‚ààùëáxU ,
ùëé ‚ààA

x‚Ä≤ ‚ààùëáxU
√ç

x‚Ä≤ ‚ààùëáxU
√ç

Let x‚Ä† = arg maxx‚ààùëáxU
If inequality (41) holds, the following inequalities must hold

ùëÉ (x|ùëáxU ). The right hand side of (41) is equivalent to ùëÉ (x‚Ä†)/

ùëÉ (x‚Ä≤).

x‚Ä≤ ‚ààùëáxU

√ç

, ‚àÄùëé ‚àà A,

(42)

ùëÉ (x‚Ä†)ùê∑ùëé (x‚Ä†)

ùëÉ (x‚Ä≤)ùê∑ùëé (x‚Ä≤)

x‚Ä≤ ‚ààùëáxU
√ç

<

ùëÉ (x‚Ä†)

ùëÉ (x‚Ä≤)

x‚Ä≤ ‚ààùëáxU
√ç

40

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

since the maximum of the left hand side of (42) over all ùëé ‚àà A is not greater than the left hand side
of (41). Therefore, if there exists any ùëé ‚àà A for which the corresponding inequality in (42) does
not hold, it implies our assumption ùê∂‚àó < ùõΩmin is not true, and, if so, we are done with the proof.

If there exists no such an ùëé and (42) holds, by eliminating ùëÉ (x‚Ä†) from both sides of (42) and

cross-multiplying (as all terms are non-negative), (42) is equivalent to the following

ùê∑ùëé (x‚Ä†)

ùëÉ (x‚Ä≤) <

ùëÉ (x‚Ä≤)ùê∑ùëé (x‚Ä≤) , ‚àÄùëé ‚àà A.

(43)

√ïx‚Ä≤ ‚ààùëáxU

√ïx‚Ä≤ ‚ààùëáxU
ùëÉ (x‚Ä≤),
Sum (43) over ùëé ‚àà A for both sides, based on (EQ-Sub), we obtain
which is obviously not true. Therefore, it implies the inequality (43) (and (42), equivalently) cannot
be true for all ùëé ‚àà A, i.e., there must exist some ùëé for which the left hand side is not smaller than
the right hand side of (42), so that both sides are equal when summed over all ùëé. Therefore, the
(cid:3)
initial assumption is incorrect and the lemma is proved.

ùëÉ (x‚Ä≤) <

x‚Ä≤ ‚ààùëáxU

x‚Ä≤ ‚ààùëáxU

√ç

√ç

H PROOF OF THEOREM 1

For the convenience and conciseness of the proof, as long as there is no confusion, we abuse
some notations in this section and the following Appendix sections. All notations in the following
Appendix sections only follow their deÔ¨Ånitions in this section.

Recall that an optimization subproblem in (OPT-Sub) is formulated over a quasi-identiÔ¨Åer (QID)
group ùëáxU in which all public records are equal to xU. Let ùëö = |ùëáxU | be the cardinality of the QID
group, or equivalently, the number of rows of this tuple. Let xùëò be the unique record of row ùëò in
the tuple, ùëò = 1, . . . , ùëö, and deÔ¨Åne ùëùùëò , ùëÉ (xùëò ), ùë•ùëò , Àúùê∑1 (xùëò ), and ùë¶ùëò , Àúùê∑0 (xùëò ) = 1 ‚àíùë•ùëò. The privacy
constraints can thus be re-written as

which can be combined as

ùëùùëòùë•ùëò
ùëö
ùëñ=1 ùëùùëñùë•ùëñ
ùëùùëòùë¶ùëò
ùëö
ùëñ=1 ùëùùëñùë¶ùëñ

√ç

√ç

ùëö

‚â§ ùõΩ, ‚àÄùëò = 1, . . . , ùëö,

‚â§ ùõΩ, ‚àÄùëò = 1, . . . , ùëö,

ùëö

ùëùùëò ‚àí ùõΩ

ùëùùëñ ‚â§ ùëùùëòùë•ùëò ‚àí ùõΩ

ùëùùëñùë•ùëñ ‚â§ 0,

√ïùëñ=1

√ïùëñ=1

(44)

‚àÄùëò = 1, . . . , ùëö. Moreover, let x = [ùë•1, ùë•2, ¬∑ ¬∑ ¬∑ , ùë•ùëö]ùëá , where ùëá represents the transpose operator.
DeÔ¨Åne A as

A =

(1 ‚àí ùõΩ)ùëù1
‚àíùõΩùëù1
...
‚àíùõΩùëù1

¬©

‚àíùõΩùëù2
(1 ‚àí ùõΩ)ùëù2
...
‚àíùõΩùëù2

¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
. . .
¬∑ ¬∑ ¬∑

‚àíùõΩùëùùëö
‚àíùõΩùëùùëö
...
(1 ‚àí ùõΩ)ùëùùëö

,

(45)

¬™
¬Æ
¬Æ
¬Æ
¬Æ
¬¨

and let b = [ùëè1, ùëè2, ¬∑ ¬∑ ¬∑ , ùëèùëö]ùëá , in which ùëèùëò = ùëùùëò ‚àí ùõΩ
constraints as

¬´

ùëö
ùëñ=1 ùëùùëñ . We can further simplify the privacy

√ç
b (cid:22) Ax (cid:22) 0,

(46)

where 0 is an ùëö √ó 1 zero vector.

Remark 7. Note that ùëèùëò = ùëùùëò ‚àí ùõΩ

ùëö
ùëñ=1 ùëùùëñ ‚â§ 0 due to Lemma 5, or (40), equivalently.

√ç

 
 
 
 
Achieving Transparency Report Privacy in Linear Time

41

Similarly, the Ô¨Ådelity constraints can be re-written as

ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, ‚àÄùëò = 1, . . . , ùëö,
ùë¶ùëò min ‚â§ ùë¶ùëò ‚â§ ùë¶ùëò max, ‚àÄùëò = 1, . . . , ùëö.
However, since for binary decision, ùë¶ùëò = 1 ‚àí ùë•ùëò , the above two constraints are basically equivalent
(to see this, simply let ùë¶ùëò min = 1 ‚àí ùë•ùëò max and ùë¶ùëò max = 1 ‚àí ùë•ùëò min), so we obtain the following Ô¨Ådelity
constraints

ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, ‚àÄùëò = 1, . . . , ùëö.
Note that the 2ùëö privacy constraints in (44) (or their equivalent vectorized form in (46)) form(s) a
parallelotope in the ùëö-dimensional space, and the 2ùëö Ô¨Ådelity constraints in (47) form a hypercube
in the ùëö-dimensional space. Let P denote the parallelotope and H denote the hypercube. Moreover,
deÔ¨Åne I , P
H be the intersection of P and H . I = ‚àÖ if and only if P and H are disjoint,
where ‚àÖ denotes the empty set. We have the following fact.

(47)

Fact 1. An optimization subproblem has feasible solutions if and only if I ‚â† ‚àÖ, i.e., P and H
intersect/collide with each other.

√ë

To prove Theorem 1, based on the above fact, it is hence equivalent to show that P and H
, max{ùõΩ0, ùõΩ1, ùõΩùëù }. Let ùúã , arg maxùëòùëùùëòùë•ùëò min and

collide with each other if and only if ùõΩ ‚â• ùõΩ*
ùúÉ , arg maxùëòùëùùëòùë¶ùëò min, we can re-write ùõΩ0, ùõΩ1, and ùõΩùëù in the following

ùëáxU

ùõΩ0 =

ùõΩ1 =

ùõΩùëù =

,

,

ùëùùúÉùë¶ùúÉ min +

ùëùùëñùë¶ùëñ max‚Ä≤

ùëùùúÉùë¶ùúÉ min
ùëö
ùëñ=1
ùëñ‚â†ùúÉ
ùëùùúã ùë•ùúã min
√ç

ùëùùëñùë•ùëñ max‚Ä≤

ùëö
ùëñ=1
ùëñ‚â†ùúã

ùëùùúã ùë•ùúã min +
ùëùùúã ùë•ùúã min + ùëùùúÉùë¶ùúÉ min
√ç
ùëö
ùëñ=1 ùëùùëñ

,

where

√ç

ùë•ùëñ max‚Ä≤ , min{ùë•ùëñ max,

ùë¶ùëñ max‚Ä≤ , min{ùë¶ùëñ max,

ùëùùúã
ùëùùëñ
ùëùùúÉ
ùëùùëñ

ùë•ùúã min},

ùë¶ùúÉ min}.

(48)

(49)

(50)

(51)

(52)

Consider the following two optimization problems for ùë• ùëó , where ùëó is an arbitrary index, 1 ‚â§ ùëó ‚â§

ùëö:

minimize

ùë• ùëó

s.t. b (cid:22) Ax (cid:22) 0,

ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, for ùëò = 1, . . . , ùëö, ùëò ‚â† ùëó .

maximize

s.t.

ùë• ùëó
b (cid:22) Ax (cid:22) 0,
ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, for ùëò = 1, . . . , ùëö, ùëò ‚â† ùëó .

(OPT-1)

(OPT-2)

The above two problems have exactly the same constraints. The Ô¨Årst line constraint forms the
parallelotope P, and let H ‚Ä≤
ùëó denote the hypercube formed by the second line constraints, i.e.,
ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, for ùëò = 1, . . . , ùëö, ùëò ‚â† ùëó . In addition, deÔ¨Åne I ‚Ä≤
ùëó be the intersection of
ùëó

, P

H ‚Ä≤

√ë

42

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

P and H ‚Ä≤
tion problems. Moreover, if I ‚Ä≤
ùëó
let ùë• ‚Ä†
the following lemma.

ùëó and ùë• ‚Ä°

ùëó , interpreting the geometric space formed by the constraints of the above two optimiza-
‚â† ‚àÖ, (i.e., there exist feasible solutions for (OPT-1) and (OPT-2)), we
ùëó denote the optimal objective values of (OPT-1) and (OPT-2), respectively. We have

ùëó

ùëò

If I ‚Ä≤
Lemma 8.
ùëò
> ùë• ùëó max or ùë• ‚Ä°
ùë• ‚Ä†
ùëó
ùë• ‚Ä†
ùëò ‚â§ ùë•ùëò max, and ùë• ‚Ä°
Proof. Apparently, since H = H ‚Ä≤
ùëó
implies if there exists any ùëó such that I‚Ä≤
ùëó
√ë
for every ùëó , if x ‚àâ I‚Ä≤

‚â† ‚àÖ for all ùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, P and H are disjoint (I = ‚àÖ) if and only if either
< ùë• ùëó min. In other words, P and H collide with each other if and only if I ‚Ä≤
‚â† ‚àÖ,
ùëò ‚â• ùë•ùëò min, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö.
ùëò for any ùëò ‚â† ùëó , we have I ‚äÜ I‚Ä≤
H ‚Ä≤
ùëó true for any ùëó , which
= ‚àÖ, I = ‚àÖ, and P and H must be disjoint. Since I ‚äÜ I‚Ä≤
ùëó

ùëó for any ùëó , then x ‚àâ I. Moreover, for any point x ‚àà I ‚Ä≤

ùëó ‚â§ ùë• ùëó ‚â§ ùë• ‚Ä°
ùëó .
< ùë• ùëó min, since for any x ‚àà I‚Ä≤
ùëó ,
ùëó , which implies either ùë• ùëó < ùë• ùëó min or ùë• ùëó > ùë• ùëó max, and thus either I = ‚àÖ, or I * I‚Ä≤

If I ‚Ä≤
ùëò
ùëó ‚â§ ùë• ùëó ‚â§ ùë• ‚Ä°
ùë• ‚Ä†
(which violates the truth). Therefore, P and H are disjoint.
ùëò ‚â§ ùë•ùëò max, and ùë• ‚Ä°

ùëò ‚â• ùë•ùëò min are true for all ùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö,
since for any x ‚àà I ‚Ä≤
ùëò , we have ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, ‚àÄùëò, which implies
x ‚àà I, so that I ‚â† ‚àÖ, P and H collide with each other. We thus prove the converse and the proof
(cid:3)
is done.

‚â† ‚àÖ for all ùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, and either ùë• ‚Ä†
ùëó

We next prove the converse. If I ‚Ä≤
ùëò

‚â† ‚àÖ, ùë• ‚Ä†
ùëò ‚â§ ùë•ùëò ‚â§ ùë• ‚Ä°

ùëò , ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùë• ‚Ä†

> ùë• ùëó max or ùë• ‚Ä°

ùëó , ùë• ‚Ä†

ùëó

ùëó

Based on Fact 1 and Lemma 8, following statements are equivalent.

(S1) An optimization sub-problem has feasible solutions.

‚áê‚áí (S2) P and H intersect/collide with each other.
‚áê‚áí (S3) (OPT-1) and (OPT-2) have feasible solutions for all ùëó .
‚áê‚áí (S4) I‚Ä≤

ùëó ‚â• ùë• ùëó min, ‚àÄùëó = 1, ¬∑ ¬∑ ¬∑ , ùëö.

ùëó ‚â§ ùë• ùëó max and ùë• ‚Ä°

ùëó ‚â† ‚àÖ, ùë• ‚Ä†

Our next goal is to show that (S1)‚àº(S4) are true if and only if ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù }. To show this,

we need the following lemma.

Lemma 9. Consider the optimization problem (OPT-1) for some (arbitrary) ùëó . If (S1)‚àº(S4) are true,
we have ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù ùëó }, where

ùõΩ0 =

ùõΩ1 =

=

ùõΩùëù ùëó

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ
ùëùùúã ùë•ùúã min
√ç
ùëö
ùëò=1
ùëò‚â†ùúã

ùëùùúÉùë¶ùúÉ min +

ùëùùëòùë¶ùëò max‚Ä≤

ùëùùúã ùë•ùúã min +

ùëùùëòùë•ùëò max‚Ä≤

ùëùùúã ùë•ùúã min + ùëù ùëóùë¶ ùëó min
√ç
ùëö
ùëò=1 ùëùùëò

.

,

,

(53)

(54)

(55)

For each of the above cases, i.e., ùõΩ = ùõΩ0, ùõΩ1, or ùõΩùëù ùëó , the corresponding optimal objective value ùë• ‚Ä†

ùëó

and its corresponding optimal solutions are

√ç

Achieving Transparency Report Privacy in Linear Time

43

ùõΩ = ùõΩ0 ‚áê‚áí ùë• ‚Ä†
ùëó

=

1
ùëù ùëó

ùëù ùëó ‚àí
n

ùõΩ
1 ‚àí ùõΩ

ùëö

√ïùëò=1
ùëò‚â†ùëó

ùëùùëòùë¶ùëò max‚Ä≤

, ùë• ‚Ä†0
ùëó

o

‚áê‚áí ùë¶ ùëó = ùë¶ùúÉ = ùë¶ùúÉ min

ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ ,

ùõΩ = ùõΩ1 ‚áê‚áí ùë• ‚Ä†
ùëó

=

1 ‚àí ùõΩ
ùõΩ

1
ùëù ùëó

n

ùëùùúã ùë•ùúã min ‚àí

ùëö

√ïùëò=1
ùëò‚â†ùëó,ùúã

ùëùùëòùë•ùëò max‚Ä≤

, ùë• ‚Ä†1
ùëó

o

‚áê‚áí ùë•ùúã = ùë•ùúã min

ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó, ùúã,

ùõΩ = ùõΩùëù ùëó ‚áê‚áí ùë• ‚Ä†

ùëó

=

1
ùëù ùëó

ùëùùúã ùë•ùúã min + ùëù ùëó ‚àí ùõΩ
n

ùëö

ùëùùëò

, ùë• ‚Ä†ùëù
ùëó

√ïùëò=1

o

‚áê‚áí ùë•ùúã = ùë•ùúã min
ùëö

ùëùùëòùë•ùëò =

√ïùëò=1
ùëò‚â†ùëó,ùúã

1 ‚àí 2ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëù ùëó + ùõΩ

ùëùùëò .

ùëö

√ïùëò=1

Proof. Please refer to Appendix I for the proofs.

(cid:3)

When (S1)‚àº(S4) are true, I ‚Ä≤
ùëó

‚â† ‚àÖ and ùë• ‚Ä†
Lemma 9, it implies that ùõΩ ‚â• ùõΩ0, ùõΩ ‚â• ùõΩ1, and

ùëó ‚â§ ùë• ùëó max need to be met for all ùëó = 1, . . . , ùëö. Based on

ùõΩ ‚â• ùõΩùëù ùëó

=

ùëùùúã ùë•ùúã min + ùëù ùëóùë¶ ùëó min
ùëö
ùëò=1 ùëùùëò

, ‚àÄùëó = 1, . . . , ùëö,

√ç

which is equivalent to

ùõΩ ‚â• max

ùëó

ùëùùúãùë•ùúã min + ùëù ùëóùë¶ ùëó min
ùëö
ùëò=1 ùëùùëò

=

ùëùùúã ùë•ùúã min + ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1 ùëùùëò

= ùõΩùëù .

√ç

√ç

We then obtain ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù } = ùõΩ*
ùë¶ ùëó = ùë¶ùúÉ = ùë¶ùúÉ min. Based on (54) and (55), we have

ùëáxU

. Since when ùõΩ*

= ùõΩ0, based on Lemma 9, we have

ùëáxU

ùõΩ*
ùëáxU
ùõΩ*
ùëáxU

= ùõΩ1 ‚áê‚áí ùë• ùëó = ùë• ùëó max‚Ä≤,
= ùõΩùëù ‚áê‚áí ùë¶ ùëó = ùë¶ùúÉ = ùë¶ùúÉ min.

44

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Combining the above results with Lemma 9, we thus have

ùõΩ*
ùëáxU

ùõΩ*
ùëáxU

ùõΩ*
ùëáxU

= ùõΩ0 ‚áê‚áí ùë¶ùúÉ = ùë¶ùúÉ min

ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ

= ùõΩ1 ‚áê‚áí ùë•ùúã = ùë•ùúã min

ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúã

= ùõΩùëù ‚áê‚áí ùë•ùúã = ùë•ùúã min
ùë¶ùúÉ = ùë¶ùúÉ min
1 ‚àí 2ùõΩ
ùõΩ

ùëùùëòùë•ùëò =

ùëö

√ïùëò=1
ùëò‚â†ùúÉ,ùúã

ùëùùúã ùë•ùúã min ‚àí ùëù ùëó + ùõΩ

ùëùùëò .

ùëö

√ïùëò=1

Similarly, if (S1)‚àº(S4) are true, I‚Ä≤
ùëó

ùëó ‚â• ùë• ùëó min need to be met for all ùëó = 1, . . . , ùëö. By
letting ùë¶ùëò = 1 ‚àí ùë•ùëò , ùë¶ùëò min = 1 ‚àí ùë•ùëò max, and ùë¶ùëò max = 1 ‚àí ùë•ùëò min, the optimization problem (OPT-2) is
essentially equivalent to the following optimization problem:

‚â† ‚àÖ and ùë• ‚Ä°

minimize ùë¶ ùëó

s.t. b (cid:22) Ay (cid:22) 0,

ùë¶ùëò min ‚â§ ùë¶ùëò ‚â§ ùë¶ùëò max, for ùëò = 1, . . . , ùëö, ùëò ‚â† ùëó .

(OPT-3)

ùëó be the optimal objective value of the above optimization problem. Clearly, ùë¶‚Ä†
ùëó ‚â• ùë• ùëó min is equivalent to ùë¶‚Ä†

Let ùë¶‚Ä†
Therefore, for all ùëó = 1, . . . , ùëö, ùë• ‚Ä°
in Lemma 9, we will obtain exactly the same conditions for ùõΩ, i.e., ùõΩ ‚â• ùõΩ*
are true, we have ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù }.

= 1 ‚àí ùë• ‚Ä°
ùëó .
ùëó ‚â§ ùë¶ ùëó max. By applying results from ùë• ‚Ä†
ùëó
. Therefore, if (S1)‚àº(S4)

ùëáxU

ùëó

ùëó ‚â§ ùë¶ ùëó max, which is equivalent to ùë• ‚Ä°

We next prove the converse. If ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù }, which, based on (53), (54), and (55), implies
ùëó ‚â§ ùë• ùëó max and ùë¶‚Ä†
ùë• ‚Ä†
ùëó ‚â• ùë• ùëó min, ‚àÄùëó = 1, ¬∑ ¬∑ ¬∑ , ùëö. Therefore, based
on Lemma 9, ùë• ùëó is feasible, which implies I ‚Ä≤
‚â† ‚àÖ, ‚àÄùëó = 1, ¬∑ ¬∑ ¬∑ , ùëö, and thus (S4) is true. Since
ùëó
(S1)‚àº(S4) are equivalent, an optimization sub-problem has feasible solutions if and only if ùõΩ ‚â•
ùõΩ*
ùëáxU

= max{ùõΩ0, ùõΩ1, ùõΩùëù }. We thus Ô¨Ånish the proof.

I PROOF OF LEMMA 9
Here we demonstrate the proof of Lemma 9, which shows the optimal objective value of the opti-
mization problem (OPT-1).

H ‚Ä≤

If I ‚Ä≤
ùëó
= P

ùëó , any x ‚àà I ‚Ä≤

‚â† ‚àÖ, there exists (at least one or some) x ‚àà I‚Ä≤

ùëó . Since
I ‚Ä≤
ùëó also belongs to P and H ‚Ä≤. Since P is a ùëö-dimensional parallelotope,
ùëó
and 0 ‚àà P is a vertex of P, any point x ‚àà P can be uniquely represented by a linear combination of
ùëö
ùëö linear independent edge vectors emitted from 0, denoted by Lùëò, ùëò = 1, . . . , ùëö, and x =
ùëò=1 ùõºùëò Lùëò ,
0 ‚â§ ùõºùëò ‚â§ 1, ‚àÄùëò = 1, . . . , ùëö. Let L be the collection of these ùëö vectors; speciÔ¨Åcally, L , [L1L2 ¬∑ ¬∑ ¬∑ Lùëö],
where Lùëò is an ùëö √ó 1 column vector and L is an ùëö √ó ùëö matrix. L can be obtained by

ùëó , and for all x, ùë• ‚Ä†

ùëó ‚â§ ùë• ùëó ‚â§ ùë• ‚Ä°

√ë

√ç

L = A‚àí1B,

(56)

where A is deÔ¨Åned in (45) and B = dg(b) where dg(b) denotes a diagonal matrix with elements of
b = (ùëè1, ùëè2, ¬∑ ¬∑ ¬∑ , ùëèùëö) along the diagonal. To Ô¨Ånd A‚àí1, note that since A can be represented by

A = dg(p) + (‚àíùõΩ)1ùëöpùëá ,

(57)

Achieving Transparency Report Privacy in Linear Time

45

where p = [ùëù1, ùëù2, ¬∑ ¬∑ ¬∑ , ùëùùëö]ùëá and 1ùëö is an all-one vector with ùëö elements, we can thus apply the
following matrix inversion formula [48]

(Z + ùëêuvùëá )‚àí1 = Z‚àí1 ‚àí

1
1 + ùëêvùëá Z‚àí1u

Z‚àí1uvùëá Z‚àí1

(58)

to compute A‚àí1 and obtain L as follow

L =

1
1 ‚àí ùëöùõΩ ¬©

ùëè1
ùëù1 [1‚àí(ùëö‚àí1)ùõΩ ]
ùëè1
ùëù2 ùõΩ
...

ùëè1
ùëùùëö

ùõΩ

ùëè2
ùëù1 ùõΩ

¬∑ ¬∑ ¬∑
ùëè2
ùëù2 [1‚àí(ùëö‚àí1)ùõΩ ] ¬∑ ¬∑ ¬∑
...
. . .
¬∑ ¬∑ ¬∑ ùëèùëö
ùëùùëö

ùëè2
ùëùùëö

ùõΩ

ùëèùëö
ùëù1 ùõΩ
ùëèùëö
ùëù2 ùõΩ
...
[1‚àí(ùëö‚àí1)ùõΩ ]

.

(59)

¬™
¬Æ
¬Æ
¬Æ
¬Æ
¬Æ
¬¨

, ( ùëè1

DeÔ¨Åne b
p
¬´
to see that L can be represented as the following equivalent form

ùëù2 , ¬∑ ¬∑ ¬∑ , ùëèùëö
ùëùùëö

ùëù1 , ùëè2

) as the element-wise division operation of two vectors. It is not hard

L = dg

b
p

+

ùõΩ
1 ‚àí ùëöùõΩ

1
p

bùëá ,

(60)

(cid:16)

(cid:17)

which implies that its inverse can also be found by applying the matrix inversion formula in (58).
We will utilize this property in the later of the proof.

Recall that if x ‚àà I ‚Ä≤

ùëó , x ‚àà P as well. Therefore, any x ‚àà I‚Ä≤

ùëó can be uniquely represented by

ùëö

x =

ùõºùëò Lùëò ,

(61)

√ïùëò=1
in which 0 ‚â§ ùõºùëò ‚â§ 1, ‚àÄùëò = 1, . . . , ùëö. Recall that we are solving the optimization problem (OPT-1)
for some ùëó , 1 ‚â§ ùëó ‚â§ ùëö. We Ô¨Årst take out the ùëó -th row from (61),

ùëö

ùë• ùëó =

ùõºùëòùêøùëò,ùëó ,

(62)

√ïùëò=1
and for the rest ùëö ‚àí 1 equalities, we move the term ùõº ùëó Lùëó from the right-hand-side (RHS) to the
left-hand-side (LHS). We then obtain

=

‚àí ùõº ùëó

ùêø1,1
ùêø2,1
...

ùêø1,2
ùêø2,2
...

ùêø1,ùëö
ùêø2,ùëö
...

ùêø1,ùëó‚àí1
ùêø2,ùëó‚àí1
...

ùë•1
ùë•2
...
Ô£Æ
Ô£Ø
Ô£Ø
ùë• ùëó‚àí1
Ô£Ø
Ô£Ø
ùë• ùëó+1
Ô£Ø
Ô£Ø
...
Ô£Ø
Ô£Ø
Ô£Ø
ùë•ùëö
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
. . .
¬∑ ¬∑ ¬∑ ùêøùëó‚àí1,ùëó‚àí1
¬∑ ¬∑ ¬∑ ùêøùëó+1,ùëó‚àí1
. . .
¬∑ ¬∑ ¬∑

ùêø1,ùëó
ùêø2,ùëó
...
Ô£Æ
Ô£Ø
Ô£Ø
ùêøùëó‚àí1,ùëó
Ô£Ø
Ô£Ø
ùêøùëó+1,ùëó
Ô£Ø
Ô£Ø
...
Ô£Ø
Ô£Ø
Ô£Ø
ùêøùëö,ùëó
Ô£Ø
Ô£Ø
Ô£Ø
‚Ä≤
= Lsubùú∂ ‚Ä≤ be the corresponding vector form of (63), in which, based on (59),
and let x‚Ä≤ ‚àí ùõº ùëó L
Ô£Ø
Ô£Ø
ùëó
Ô£∞
ùêøùëò,ùëò = 1‚àí(ùëö‚àí1)ùõΩ
, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, and ùêøùëò,ùëñ = ùõΩ
ùëèùëò
, ‚àÄùëò, ùëñ = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëñ. Note that Lsub is an
1‚àíùëöùõΩ
ùëùùëò
(ùëö ‚àí 1) √ó (ùëö ‚àí 1) square sub-matrix of L by removing the ùëó -th row and the ùëó -th column. Therefore,
it has the similar form as shown in (60) by removing the ùëó -th row/element of b and p, and thus its
inverse L‚àí1

Ô£Æ
Ô£Ø
Ô£Ø
ùêøùëó‚àí1,1 ùêøùëó‚àí1,2
Ô£Ø
Ô£Ø
ùêøùëó+1,2
ùêøùëó+1,1
Ô£Ø
Ô£Ø
...
...
Ô£Ø
Ô£Ø
Ô£Ø
ùêøùëö,2
ùêøùëö,1
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
. . .
¬∑ ¬∑ ¬∑ ùêøùëó‚àí1,ùëö
¬∑ ¬∑ ¬∑ ùêøùëó+1,ùëö
. . .
¬∑ ¬∑ ¬∑

ùêø1,ùëó+1
ùêø2,ùëó+1
...
ùêøùëó‚àí1,ùëó+1
ùêøùëó+1,ùëó+1
...
ùêøùëö,ùëó+1

ùõº1
ùõº2
...
Ô£Æ
Ô£Ø
Ô£Ø
ùõº ùëó‚àí1
Ô£Ø
Ô£Ø
ùõº ùëó+1
Ô£Ø
Ô£Ø
...
Ô£Ø
Ô£Ø
Ô£Ø
ùõºùëö
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Ô£π
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª
1‚àíùëöùõΩ

sub can also be found by (58). By applying L‚àí1

sub to both sides of (63), we have

...
ùêøùëö,ùëó‚àí1

...
ùêøùëö,ùëö

Ô£π
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£π
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£π
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

(63)

ùëèùëñ
ùëùùëò

,

ùú∂ ‚Ä≤ = L‚àí1

subx‚Ä≤ ‚àí ùõº ùëó L‚àí1

subL

‚Ä≤

ùëó .

(64)

 
 
 
 
 
46

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Let u , L‚àí1

subx‚Ä≤ and v , ùõº ùëó L‚àí1

subL

‚Ä≤

ùëó , we obtain

ùë¢ùëò =

1
1 ‚àí ùõΩ

1
ùëèùëò

h

ùëö

(1 ‚àí ùõΩ)ùëùùëòùë•ùëò ‚àí ùõΩ

ùëùùëñùë•ùëñ

,

√ïùëñ=1
ùëñ‚â†ùëó

i

ùë£ùëò =

ùõΩ
1 ‚àí ùõΩ

1
ùëèùëò

ùõº ùëóùëè ùëó ,

ùõºùëò = ùë¢ùëò ‚àí ùë£ùëò =

ùõΩ
1 ‚àí ùõΩ

1
ùëèùëò

h

for all ùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó . Similarly, if we deÔ¨Åne ùõºùëò

‚Ä≤ =

ùõºùëò

ùõΩ
1 ‚àí ùõΩ

1
ùëèùëò

1 ‚àí ùõΩ
ùõΩ

h

Substituting the ùõºùëò in (65) into (62), we obtain

ùëö

ùë• ùëó =

ùõºùëòùêøùëò,ùëó =

√ïùëò=1

1
1 ‚àí ùõΩ

1
ùëù ùëó

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùëòùë•ùëò ‚àí

ùëùùëñùë•ùëñ ‚àí ùõº ùëóùëè ùëó

,

√ïùëñ=1
ùëñ‚â†ùëó
‚Ä≤ , 1 ‚àí ùõºùëò , we have

i

ùëö

ùëùùëòùë¶ùëò ‚àí

ùëùùëñùë¶ùëñ ‚àí ùõº ùëó

‚Ä≤ùëè ùëó

.

i

√ïùëñ=1
ùëñ‚â†ùëó

ùëö

ùõΩ

ùëùùëòùë•ùëò + ùõº ùëóùëè ùëó

.

h

√ïùëò=1
ùëò‚â†ùëó

i

(65)

(66)

(67)

Based on (67), we are looking for the values of ùõº ùëó and ùë•ùëò ‚Äôs (ùëò ‚â† ùëó ) yielding the minimum of ùë• ùëó .

Since 0 ‚â§ ùõºùëò ‚â§ 1, ‚àÄùëò = 1, . . . , ùëö, which implies ùë¢ùëò ‚â• ùë£ùëò , ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùëó , and

We then have

ùë¢ùëò
ùë£ùëò

=

1
ùõΩùõº ùëóùëè ùëó

h

ùëö

(1 ‚àí ùõΩ)ùëùùëòùë•ùëò ‚àí ùõΩ

ùëùùëñùë•ùëñ

‚â• 1.

√ïùëñ=1
ùëñ‚â†ùëó

i

ùõº ùëó ‚â§

1
ùõΩùëè ùëó

h

ùëö

(1 ‚àí ùõΩ)ùëùùëòùë•ùëò ‚àí ùõΩ

ùëùùëñùë•ùëñ

, ùëÖùëò
ùëó ,

√ïùëñ=1
ùëñ‚â†ùëó

i

for all ùëò = 1, . . . , ùëö, ùëò ‚â† ùëó . Let ùëÖ‚àó
ùëó

, min
ùëò

ùëÖùëò
ùëó . Combining with the fact that ùõº ùëó ‚â§ 1, we obtain

ùõº ùëó ‚â§ min(ùëÖ‚àó

ùëó , 1).

(68)

(69)

(70)

Case 1:
We Ô¨Årst consider the case ùëÖ‚àó

ùëó ‚â§ 1. Please refer to Case 2 for ùëÖ‚àó

ùëó ‚â• 1.

When ùëÖ‚àó

ùëó ‚â§ 1, based on (70), we have ùõº ùëó = ùëÖ‚àó

ùëó . Therefore, based on (69), there must exist an ùëò

(denoted by ùúÖ) such that

ùõº ùëó =

1
ùõΩùëè ùëó

h

ùëö

(1 ‚àí ùõΩ)ùëùùúÖùë•ùúÖ ‚àí ùõΩ

ùëùùëñùë•ùëñ

= ùëÖ‚àó

ùëó ‚â§ 1.

√ïùëñ=1
ùëñ‚â†ùëó

i

Because ùëè ùëó ‚â§ 0 (see Remark 7), from the LHS of (71), we have

ùëö

ùõΩ

ùëùùëñùë•ùëñ + ùõº ùëóùëè ùëó = (1 ‚àí ùõΩ)

√ïùëñ=1
ùëñ‚â†ùëó

1
ùõΩ

h

ùëö

ùëùùúÖùë•ùúÖ ‚àí

ùëùùëñùë•ùëñ

.

√ïùëñ=1
ùëñ‚â†ùëó

i

(71)

(72)

Achieving Transparency Report Privacy in Linear Time

Substitute the LHS of (72) into (67). We obtain

or equivalently,

ùëù ùëóùë• ùëó =

1
ùõΩ

ùëö

ùëùùúÖùë•ùúÖ ‚àí

ùëùùëñùë•ùëñ =

√ïùëò=1
ùëò‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúÖùë•ùúÖ ‚àí

ùëùùëñùë•ùëñ,

√ïùëò=1
ùëò‚â†ùëó,ùúÖ

ùõΩ =

ùëùùúÖùë•ùúÖ
ùëö
ùëò=1ùëùùëòùë•ùëò

=

ùëùùúÖùë•ùúÖ +

ùëùùúÖùë•ùúÖ
ùëö
ùëò=1
ùëò‚â†ùúÖ

.

ùëùùëòùë•ùëò

47

(73)

(74)

Therefore, ùë• ùëó is minimized if the RHS of (73) is minimized. In addition, in Case 1, based on (74), ùõΩ
achieves its minimum when (i) minimizing ùëùùúÖùë•ùúÖ and (ii) maximizing

ùëö
ùëò=1
ùëò‚â†ùúÖ
We next Ô¨Ånd the minimum of the RHS of (73). Since from (71), we have

ùëùùëòùë•ùëò .

√ç

√ç

√ç

and from (73), we have

ùëö

ùëö

ùëùùëñùë•ùëñ =

ùëùùëòùë•ùëò =

√ïùëñ=1
ùëñ‚â†ùëó

√ïùëò=1
ùëò‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëùùúÖùë•ùúÖ ‚àí ùõº ùëóùëè ùëó,

ùëö

ùõΩ

ùëùùëòùë•ùëò = ùëùùúÖùë•ùúÖ ‚àí ùõΩùëù ùëóùë• ùëó .

√ïùëò=1
ùëò‚â†ùëó

Substituting the LHS of (75) into (65), we obtain

ùõºùëò =

1
ùëèùëò

(ùëùùëòùë•ùëò ‚àí ùëùùúÖùë•ùúÖ), ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó,

and substituting the LHS of (75) into (76), we obtain

Combining (77) and (78), we have

ùõº ùëó =

1
ùëè ùëó

(ùëù ùëóùë• ùëó ‚àí ùëùùúÖùë•ùúÖ ).

(75)

(76)

(77)

(78)

ùõºùëò =

1
ùëèùëò
Since ùõºùëò ‚â• 0 and ùëèùëò ‚â§ 0, we have ùëùùúÖùë•ùúÖ ‚â• ùëùùëòùë•ùëò, ‚àÄùëò, i.e., ùëùùúÖùë•ùúÖ = max
ùëùùëòùë•ùëò . Moreover, from (73),
since ùõΩ is non-negative and ùë•ùëò ‚â• 0 for all ùëò, in order to minimize ùë• ùëó , we need to (i) minimize ùëùùúÖùë•ùúÖ
ùëùùëòùë•ùëò . Note that both (i) and (ii) minimize ùõΩ as well, which implies that the
and (ii) maximize

(ùëùùëòùë•ùëò ‚àí ùëùùúÖùë•ùúÖ), ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö.

(79)

ùëò

ùëö
ùëò=1
ùëò‚â†ùëó,ùúÖ

optimal solutions that minimize ùë• ùëó also minimize ùõΩ.

√ç

To minimize ùëùùúÖùë•ùúÖ, since ùëùùúÖùë•ùúÖ = max

ùëùùëòùë•ùëò , and ùëùùëòùë•ùëò ‚â• ùëùùëòùë•ùëò min, ‚àÄùëò (including ùúÖ), the minimal
ùëùùúÖùë•ùúÖ , i.e., ùëùùúÖùë•ùúÖ min, is therefore the largest eÔ¨Äective lower limit ùëùùëòùë•ùëò min over all ùëò, i.e., ùëùùúÖùë•ùúÖ min =
max
ùëò
To maximize

ùëùùëòùë•ùëò min = ùëùùúã ùë•ùúã min by deÔ¨Ånition, and thus we get ùúÖ = ùúã and ùë•ùúã = ùë•ùúã min.

ùëùùëòùë•ùëò , we need to Ô¨Ånd the maximum of each ùë•ùëò. Since 0 ‚â§ ùõºùëò ‚â§ 1, by

ùëò

√ç

substituting ùëùùúÖùë•ùúÖ = ùëùùúã ùë•ùúã min into (79), we have ùëùùúã ùë•ùúã min + ùëèùëò ‚â§ ùëùùëòùë•ùëò ‚â§ ùëùùúã ùë•ùúã min, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö
(including ùëó ). By deÔ¨Ånitions, ùë•ùëò max‚Ä≤ , min{ùë•ùëò max, ùëùùúã
ùë•ùúã min +ùëèùëò }.
ùëùùëò
Combining with the constraints ùë•ùëò min ‚â§ ùë•ùëò ‚â§ ùë•ùëò max, we obtain ùë•ùëò min‚Ä≤ ‚â§ ùë•ùëò ‚â§ ùë•ùëò max‚Ä≤, ‚àÄùëò =
ùëùùëòùë•ùëò is maximized when ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò, ùëò ‚â† ùëó, ùúã.
1, ¬∑ ¬∑ ¬∑ , ùëö, and thus

ùë•ùúã min} and ùë•ùëò min‚Ä≤ , max{ùë•ùëò min, ùëùùúã
ùëùùëò

ùëö
ùëò=1
ùëò‚â†ùëó,ùúÖ

ùëö
ùëò=1
ùëò‚â†ùëó,ùúÖ

√ç

48

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

By substituting the ùë•ùëò ‚Äôs we obtained above into (73), based on which, the optimal objective value
ùë• ‚Ä†
ùëó , the minimum of ùë• ùëó , is thus

ùë• ‚Ä†
ùëó

=

1 ‚àí ùõΩ
ùõΩ

1
ùëù ùëó

n

ùëùùúã ùë•ùúã min ‚àí

ùëö

√ïùëò=1
ùëò‚â†ùëó,ùúã

ùëùùëñùë•ùëñ max‚Ä≤

, ùë• ‚Ä†1
ùëó

.

o

(80)

If (S1)‚àº(S4) are true, ùë• ‚Ä†
ùëùùúã ùë•ùúã min, and thus from (51), we get ùë• ùëó max
that (OPT-1) has feasible solution, for Case 1, is

ùëó ‚â§ ùë• ùëó max. In addition, based on (78), we have ùëù ùëóùë• ùëó = ùëùùúã ùë•ùúã min + ùõº ùëóùëè ùëó ‚â§
= ùë• ùëó max‚Ä≤. Therefore, based on (80), the minimum ùõΩ such

ùõΩ =

ùëùùúã ùë•ùúã min + ùëù ùëó ùë• ‚Ä†

ùëùùúã ùë•ùúã min
ùëö
ùëó +
ùëò=1
ùëò‚â†ùëó,ùúã

ùëùùëòùë•ùëò max‚Ä≤

ùëùùúã ùë•ùúã min
ùëö
ùëò=1
ùëò‚â†ùúã
We next prove the converse. If (81) and (80) holds, i.e., ùë•ùúÖ = ùë•ùúã = ùë•ùúã min, and ùë•ùëò = ùë•ùëò max‚Ä≤,

√ç
‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó, ùúã, since ùë•ùúã min = ùë•ùúã max‚Ä≤, we have ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó , and

√ç
ùëùùëòùë•ùëò max‚Ä≤

ùëùùúã ùë•ùúã min +

, ùõΩ1.

‚â•

(81)

ùëù ùëóùë• ‚Ä†
ùëó

=

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúã ùë•ùúã min ‚àí

ùëùùëòùë•ùëò max‚Ä≤

√ïùëò=1
ùëò‚â†ùëó,ùúã

ùëö

(82)

=

1
ùõΩ

ùëùùúã ùë•ùúã min ‚àí

ùëùùëòùë•ùëò max‚Ä≤.

√ïùëò=1
ùëò‚â†ùëó

Given the above ùë•ùëò ‚Äôs and (82), since ùë•ùúã min = ùë•ùúã max‚Ä≤ and ùëè ùëó ‚â§ 0, and by deÔ¨Ånition in (51), ùëùùúã ùë•ùúã min ‚â•
ùëùùëòùë•ùëò max‚Ä≤, ‚àÄùëò, we have

ùëÖ‚àó
ùëó

, min
ùëò

ùëÖùëò
ùëó

= min
ùëò

1
ùõΩùëè ùëó

h

1
ùõΩùëè ùëó

max
ùëò

h

1
ùõΩùëè ùëó

h

=

=

=

(1 ‚àí ùõΩ)ùëùùëòùë•ùëò max‚Ä≤ ‚àí ùõΩ

ùëùùëñùë•ùëñ max‚Ä≤

ùëö

√ïùëñ=1
ùëñ‚â†ùëó
ùëö

i

(1 ‚àí ùõΩ)ùëùùëòùë•ùëò max‚Ä≤ ‚àí ùõΩ

ùëùùëñùë•ùëñ max‚Ä≤

√ïùëñ=1
ùëñ‚â†ùëó

i

ùëö

(1 ‚àí ùõΩ)ùëùùúãùë•ùúã min ‚àí ùõΩ

ùëùùëñùë•ùëñ max‚Ä≤

√ïùëñ=1
ùëñ‚â†ùëó

i

1
ùëè ùëó

(ùëù ùëóùë• ‚Ä†

ùëó ‚àí ùëùùúã ùë•ùúã min).

Besides, by substituting ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó , into (67), we obtain

ùëù ùëóùë• ‚Ä†
ùëó

=

1
1 ‚àí ùõΩ

ùëö

ùõΩ

ùëùùëòùë•ùëò max‚Ä≤ + ùõº ùëóùëè ùëó

.

h

√ïùëò=1
ùëò‚â†ùëó

i

(83)

(84)

Achieving Transparency Report Privacy in Linear Time

By substituting the RHS of (82) into (84), we get

ùõº ùëó =

1
ùëè ùëó

(ùëù ùëóùë• ‚Ä†

ùëó ‚àí ùëùùúãùë•ùúã min).

49

(85)

Since ùõº ùëó ‚â§ 1, and according to (83), we obtain ùëÖ‚àó
ùëó
converse.

= ùõº ùëó ‚â§ 1, and thus Ô¨Ånish the proof of the

We summarize Case 1 in the following:

ùëÖ‚àó

ùëó ‚â§ 1
1 ‚àí ùõΩ
ùõΩ

‚áê‚áí

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó ‚â•

ùëö

ùëùùëñùë•ùëñ max‚Ä≤

√ïùëò=1
ùëò‚â†ùëó
‚áê‚áí ùë•ùúã = ùë•ùúã min and ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó, ùúã

‚áê‚áí ùë• ‚Ä†
ùëó

=

1 ‚àí ùõΩ
ùõΩ

1
ùëù ùëó

n

ùëùùúã ùë•ùúã min ‚àí

ùëö

√ïùëò=1
ùëò‚â†ùëó,ùúã

ùëùùëòùë•ùëò max‚Ä≤

, ùë• ‚Ä†1
ùëó

o

‚áê‚áí ùõΩ ‚â•

ùëùùúã ùë•ùúã min +

ùëùùëòùë•ùëò max‚Ä≤

ùëùùúãùë•ùúã min
ùëö
ùëò=1
ùëò‚â†ùúã

, ùõΩ1.

√ç

Case 2:
Now consider the case ùëÖ‚àó
(67) thus becomes

ùëó ‚â• 1. In this case, according to (70), we have ùõº ùëó = min(ùëÖ‚àó

ùëó , 1) = 1, and

ùëù ùëóùë• ùëó =

1
1 ‚àí ùõΩ

ùëö

ùõΩ

ùëùùëòùë•ùëò + ùëè ùëó

.

h

√ïùëò=1
ùëò‚â†ùëó

i

Moreover, based on (69), there must exist an ùëò (denoted by ùúÖ) such that

ùõº ùëó = 1 ‚â§

1
ùõΩùëè ùëó

h

ùëö

(1 ‚àí ùõΩ)ùëùùúÖùë•ùúÖ ‚àí ùõΩ

ùëùùëñùë•ùëñ

= ùëÖ‚àó
ùëó ,

√ïùëñ=1
ùëñ‚â†ùëó

i

which, since ùë•ùëñ ‚â§ ùë•ùëñ max‚Ä≤, ‚àÄùëñ, yields

ùëö

ùëö

ùëùùëñùë•ùëñ max‚Ä≤ ‚â•

ùëùùëñùë•ùëñ ‚â•

√ïùëñ=1
ùëñ‚â†ùëó

√ïùëñ=1
ùëñ‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëùùúÖùë•ùúÖ ‚àí ùëè ùëó .

(86)

(87)

(88)

Since

ùëùùëñùë•ùëñ ‚â•

ùëö
ùëñ=1
ùëñ‚â†ùëó
to proceed the proof.

ùëö
ùëñ=1
ùëñ‚â†ùëó

√ç

√ç

ùëùùëñùë•ùëñ min‚Ä≤ as well, we need to consider diÔ¨Äerent cases in the following in order

Case 2.1:
First, we consider the case that the RHS of (88) is greater or equal to the sum of the equivalent
lower limits, i.e.,

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúÖùë•ùúÖ ‚àí ùëè ùëó ‚â•

ùëùùëñùë•ùëñ min‚Ä≤.

√ïùëñ=1
ùëñ‚â†ùëó

(89)

50

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

In such a case, the equality of the RHS of (88) can hold. Since, based on (86), in order to minimize
ùë• ùëó , we need to minimize
ùëùùëòùë•ùëò , from the RHS of (88), which is

ùëö
ùëò=1
ùëò‚â†ùëó

√ç

ùëö

ùëùùëòùë•ùëò =

√ïùëò=1
ùëò‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëùùúÖùë•ùúÖ ‚àí ùëè ùëó,

(90)

and thus we need to minimize ùëùùúÖùë•ùúÖ . By substituting the LHS of (90) into (86) and (65), we obtain

and

ùëù ùëóùë• ùëó = ùëùùúÖùë•ùúÖ + ùëè ùëó

ùõºùëò =

1
ùëèùëò

(ùëùùëòùë•ùëò ‚àí ùëùùúÖùë•ùúÖ), ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó,

(91)

(92)

respectively. Based on (92), similar to Case 1, since ùõºùëò ‚â• 0 and ùëèùëò ‚â§ 0, we have ùëùùúÖùë•ùúÖ ‚â• ùëùùëòùë•ùëò, ‚àÄùëò, i.e.,
ùëùùúÖùë•ùúÖ = max
ùëùùëòùë•ùëò . Moreover, in order to minimize ùë• ùëó , we need to minimize ùëùùúÖùë•ùúÖ. To minimize ùëùùúÖùë•ùúÖ ,
ùëò
since ùëùùúÖùë•ùúÖ = max
ùëùùëòùë•ùëò, and ùëùùëòùë•ùëò ‚â• ùëùùëòùë•ùëò min, ‚àÄùëò (including ùúÖ), the minimal ùëùùúÖùë•ùúÖ , i.e., ùëùùúÖùë•ùúÖ min, is
therefore the largest eÔ¨Äective lower limit ùëùùëòùë•ùëò min over all ùëò, i.e., ùëùùúÖùë•ùúÖ min = max
ùëùùëòùë•ùëò min = ùëùùúã ùë•ùúã min
by deÔ¨Ånition, and thus we get ùúÖ = ùúã and ùë•ùúã = ùë•ùúã min. Substituting which into (91), we thus obtain
the minimum of ùë• ùëó

ùëò

ùëò

ùë• ‚Ä†
ùëó

=

=

1
ùëù ùëó

1
ùëù ùëó

ùëùùúã ùë•ùúã min + ùëè ùëó
n
ùëùùúã ùë•ùúã min + ùëù ùëó ‚àí ùõΩ
n

o

ùëö

ùëùùëò

, ùë• ‚Ä†ùëù
ùëó

.

√ïùëò=1

o

For the rest of ùëò, ùëò ‚â† ùëó, ùúã, according to (90), we have

ùëùùëòùë•ùëò =

1 ‚àí 2ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó

ùëö

√ïùëò=1
ùëò‚â†ùëó,ùúã

=

1 ‚àí 2ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëù ùëó + ùõΩ

ùëùùëò .

ùëö

√ïùëò=1

(93)

(94)

If (S1)‚àº(S4) are true, ùë• ‚Ä†
and thus from (51), we get ùë• ùëó max
(OPT-1) has feasible solution, for Case 2.1, is

ùëó ‚â§ ùë• ùëó max. In addition, based on (91), we have ùëù ùëó ùë• ùëó = ùëùùúã ùë•ùúã min +ùëè ùëó ‚â§ ùëùùúã ùë•ùúã min,
= ùë• ùëó max‚Ä≤. Therefore, based on (93), the minimum ùõΩ such that

ùõΩ =

‚â•

=

ùëùùúã ùë•ùúã min + ùëù ùëó (1 ‚àí ùë• ‚Ä†
ùëó )
ùëö
ùëò=1 ùëùùëò
ùëùùúã ùë•ùúã min + ùëù ùëó (1 ‚àí ùë• ùëó max)
ùëö
ùëò=1 ùëùùëò

√ç

ùëùùúã ùë•ùúã min + ùëù ùëóùë¶ ùëó min
√ç
ùëö
ùëò=1 ùëùùëò

, ùõΩùëù ùëó .

√ç

(95)

Achieving Transparency Report Privacy in Linear Time

51

We next prove the converse. If ùë•ùúÖ = ùë•ùúã = ùë•ùúã min, and (93), (94), and (95) hold, from (67), we have
ùëö

ùëù ùëó ùë• ùëó =

1
1 ‚àí ùõΩ

ùõΩ

ùëùùëòùë•ùëò + ùõº ùëóùëè ùëó

,

h

√ïùëò=1
ùëò‚â†ùëó

i

and, since

ùëùùëòùë•ùëò ‚â•

ùëö
ùëò=1
ùëò‚â†ùëó

ùëö
ùëò=1
ùëò‚â†ùëó

ùëùùëòùë•ùëò min‚Ä≤, from (94), we have

√ç

√ç

ùëö

ùëùùëòùë•ùëò =

√ïùëò=1
ùëò‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó ‚â•

ùëùùëòùë•ùëò min‚Ä≤.

√ïùëò=1
ùëò‚â†ùëó

(96)

(97)

Since (94) and (97) hold when ùë• ùëó = ùë• ‚Ä†
get

ùëó , i.e., (93) holds, by substituting the LHS of (97) into (96), we

ùëù ùëóùë• ‚Ä†
ùëó

= ùëùùúãùë•ùúã min +

ùëè ùëó .

(98)

ùõº ùëó ‚àí ùõΩ
1 ‚àí ùõΩ

(cid:16)

(cid:17)

= 1, and therefore ùõº ùëó = 1. Based on (70), we thus obtain

Comparing (98) with (93), we have ùõº ùëó ‚àíùõΩ
1‚àíùõΩ
ùëÖ‚àó
ùëó ‚â• 1 and Ô¨Ånish the proof of the converse.
We summarize Case 2.1 in the following:

ùëÖ‚àó

ùëó ‚â• 1 and

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó ‚â•

ùëùùëòùë•ùëò min‚Ä≤

√ïùëò=1
ùëò‚â†ùëó

ùëö

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó ‚â•

ùëùùëòùë•ùëò min‚Ä≤

√ïùëò=1
ùëò‚â†ùëó

ùëö

‚áê‚áí

ùëùùëòùë•ùëò max‚Ä≤ ‚â•

√ïùëò=1
ùëò‚â†ùëó

1 ‚àí ùõΩ
ùõΩ

ùëö

‚áê‚áí ùë•ùúã = ùë•ùúã min and

ùëùùëòùë•ùëò =

√ïùëò=1
ùëò‚â†ùëó,ùúã

‚áê‚áí ùë• ‚Ä†
ùëó

=

‚áê‚áí ùõΩ ‚â•

ùëùùúã ùë•ùúã min + ùëù ùëó ‚àí ùõΩ
n

1
ùëù ùëó
ùëùùúãùë•ùúã min + ùëù ùëóùë¶ ùëó min
ùëö
ùëò=1 ùëùùëò

√ïùëò=1
, ùõΩùëù ùëó .

1 ‚àí 2ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëù ùëó + ùõΩ

ùëùùëò

ùëö

√ïùëò=1

ùëö

ùëùùëò

, ùë• ‚Ä†ùëù
ùëó

o

ùëö

Case 2.2:
Next, we consider the case that

√ç

1 ‚àí ùõΩ
ùõΩ

Recall that ùë¶ùëò , 1 ‚àí ùë•ùëò, ùë¶ùëò max
ùõº ùëó

‚Ä≤ = 1 ‚àí ùõº ùëó = 0, based on (66), we have

, 1 ‚àí ùë•ùëò min, ùë¶ùëò min

ùëùùúãùë•ùúã min ‚àí ùëè ùëó ‚â§

ùëùùëòùë•ùëò min‚Ä≤.

(99)

√ïùëò=1
ùëò‚â†ùëó

, 1 ‚àí ùë•ùëò max, and ùõºùëò

‚Ä≤ , 1 ‚àí ùõºùëò, ‚àÄùëò. Since from (87),

‚Ä≤ =

ùõºùëò

ùõΩ
1 ‚àí ùõΩ

1
ùëèùëò

1 ‚àí ùõΩ
ùõΩ

h

ùëö

ùëùùëòùë¶ùëò ‚àí

ùëùùëñùë¶ùëñ

, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó .

(100)

√ïùëñ=1
ùëñ‚â†ùëó

i

52

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Moreover, by substituting ùë•ùëò = 1 ‚àí ùë¶ùëò , ‚àÄùëò, into (86), we get

ùëù ùëóùë• ùëó = ùëù ùëó ‚àí

ùõΩ
1 ‚àí ùõΩ

ùëö

ùëùùëòùë¶ùëò ,

√ïùëò=1
ùëò‚â†ùëó

ùëù ùëóùë¶ ùëó =

ùõΩ
1 ‚àí ùõΩ

ùëö

ùëùùëòùë¶ùëò ,

√ïùëò=1
ùëò‚â†ùëó

or equivalently,

and

ùõΩ =

ùëù ùëóùë¶ ùëó
ùëö
ùëò=1ùëùùëòùë¶ùëò

=

ùëù ùëóùë¶ ùëó +

ùëù ùëóùë¶ ùëó
ùëö
ùëò=1
ùëò‚â†ùëó

.

ùëùùëòùë¶ùëò

Substituting the RHS of (102) into (100), we obtain

√ç

√ç

‚Ä≤ =

ùõºùëò

1
ùëèùëò

(ùëùùëòùë¶ùëò ‚àí ùëù ùëóùë¶ ùëó ), ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó,

(101)

(102)

(103)

(104)

‚Ä≤ ‚â• 0 and ùëèùëò ‚â§ 0, we have ùëù ùëóùë¶ ùëó ‚â• ùëùùëòùë¶ùëò , ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó , i.e., ùëù ùëóùë¶ ùëó = max

ùëùùëòùë¶ùëò .
Since ùõºùëò
Moreover, from (101), since ùõΩ is non-negative and ùë•ùëò ‚â• 0 for all ùëò, in order to minimize ùë• ùëó , we
ùëùùëòùë¶ùëò . In addition, based on (103), in Case 2.2, ùõΩ achieves its minimum when
need to maximize

ùëò

(i) minimizing ùëù ùëóùë¶ ùëó and (ii) maximizing
√ç

ùëùùëòùë¶ùëò .

ùëö
ùëò=1
ùëò‚â†ùëó

To minimize ùëù ùëóùë¶ ùëó , since ùëù ùëóùë¶ ùëó = max

ùëù ùëóùë¶ ùëó , i.e., ùëù ùëóùë¶ ùëó min, is therefore the largest eÔ¨Äective lower limit ùëùùëòùë¶ùëò min over all ùëò, i.e., ùëù ùëóùë¶ ùëó min
max
ùëò
To maximize

ùëùùëòùë¶ùëò min = ùëùùúÉùë¶ùúÉ min by deÔ¨Ånition, and thus we get ùëó = ùúÉ and ùë¶ùúÉ = ùë¶ùúÉ min.

ùëùùëòùë¶ùëò , we need to Ô¨Ånd the maximum of each ùë¶ùëò . Since 0 ‚â§ ùõºùëò

‚Ä≤ ‚â§ 1, by substi-

ùëò

ùëùùëòùë¶ùëò , and ùëùùëòùë¶ùëò ‚â• ùëùùëòùë¶ùëò min, ‚àÄùëò (including ùëó ), the minimal
√ç
=

√ç

tuting ùëù ùëóùë¶ ùëó = ùëùùúÉùë¶ùúÉ min into (104), we have ùëùùúÉùë¶ùúÉ min + ùëèùëò ‚â§ ùëùùëòùë¶ùëò ‚â§ ùëùùúÉùë¶ùúÉ min, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ .
By deÔ¨Ånitions, ùë¶ùëò max‚Ä≤ , min{ùë¶ùëò max, ùëùùúÉ
ùë¶ùúÉ min + ùëèùëò }. Combining
ùëùùëò
with the constraints ùë¶ùëò min ‚â§ ùë¶ùëò ‚â§ ùë¶ùëò max, we obtain ùë¶ùëò min‚Ä≤ ‚â§ ùë¶ùëò ‚â§ ùë¶ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, and thus

ùë¶ùúÉ min} and ùë¶ùëò min‚Ä≤ , max{ùë¶ùëò min, ùëùùúÉ
ùëùùëò

ùëùùëòùë¶ùëò is maximized when ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò, ùëò ‚â† ùëó .

ùëö
ùëò=1
ùëò‚â†ùëó
By substituting the ùë¶ùëò ‚Äôs we obtained above into (101), based on which, the optimal objective

√ç
value ùë• ‚Ä†

ùëó , the minimum of ùë• ùëó , is thus

ùë• ‚Ä†
ùëó

=

ùëù ùëó ‚àí

ùõΩ
1 ‚àí ùõΩ

1
ùëù ùëó

n

ùëö

ùëùùëòùë¶ùëò max‚Ä≤

, ùë• ‚Ä†0
ùëó

.

√ïùëò=1
ùëò‚â†ùëó

o

(105)

If (S1)‚àº(S4) are true, ùë• ‚Ä†
solution, for Case 2.2, is

ùëó ‚â§ ùë• ùëó max. Based on (103), the minimum ùõΩ such that (OPT-1) has feasible

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ
We next prove the converse. Before proving the converse, we Ô¨Årst need the following lemma.

ùëùùúÉùë¶ùúÉ min +

ùëùùëòùë¶ùëò max‚Ä≤

, ùõΩ0.

ùõΩ ‚â•

√ç

(106)

Lemma 10. If ùëùùúÉùë¶ùúÉ min ‚â§ ùëùùëòùë¶ùëò ‚àí ùëèùëò, ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ , we have ùëùùëòùë•ùëò min‚Ä≤ ‚â§ ùëùùëò
‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ .

1 ‚àí ùë¶ùëò max‚Ä≤

,

(cid:0)

(cid:1)

ùëö
ùëò=1
ùëò‚â†ùëó

ùëö
ùëò=1
ùëò‚â†ùëó

Achieving Transparency Report Privacy in Linear Time

Proof. Recall that ‚àÄùëò = 1, . . . , ùëö, by deÔ¨Ånitions we have

ùëùùëòùë•ùëò min‚Ä≤ , max{ùëùùúã ùë•ùúã min + ùëèùëò, ùëùùëòùë•ùëò min},

ùëùùëò

1 ‚àí ùë¶ùëò max‚Ä≤

(cid:0)

= ùëùùëò ‚àí ùëùùëòùë¶ùëò max‚Ä≤
, max{ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min, ùëùùëò ‚àí ùëùùëòùë¶ùëò max}
= max{ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min, ùëùùëòùë•ùëò min}.

(cid:1)

Given the conditions that ùëùùúÉùë¶ùúÉ min ‚â§ ùëùùëòùë¶ùëò ‚àí ùëèùëò, ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ , from which we get

ùëùùúÉùë¶ùúÉ min ‚â§ ùëùùëòùë¶ùëò ‚àí ùëèùëò, ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ,

=‚áí ùëùùúÉùë¶ùúÉ min ‚â§ ùëùùúãùë¶ùúã ‚àí ùëèùúã

=‚áí ùëùùúÉùë¶ùúÉ min ‚â§ ùëùùúã

1 ‚àí ùë•ùúã

‚àí ùëùùúã + ùõΩ

ùëùùëò

ùëö

(cid:0)
=‚áí ùëùùúã ùë•ùúã = ùëùùúã ùë•ùúã min ‚â§ ‚àíùëùùúÉùë¶ùúÉ min + ùõΩ

(cid:1)

√ïùëò=1
ùëö

ùëùùëò

=‚áí ùëùùúã ùë•ùúã min + ùëèùëò ‚â§ ‚àíùëùùúÉùë¶ùúÉ min + ùõΩ

=‚áí ùëùùúã ùë•ùúã min + ùëèùëò ‚â§ ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min.

ùëö

√ïùëò=1
ùëùùëò + ùëùùëò ‚àí ùõΩ

√ïùëò=1

ùëùùëò

ùëö

√ïùëò=1

53

(107)

(108)

(109)

Therefore, since the above inequalities hold for all ùëò except ùëò = ùúÉ , by comparing the RHS of (107)
, ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ . We thus Ô¨Ånish the
and (108), we obtain that ùëùùëòùë•ùëò min‚Ä≤ ‚â§ ùëùùëò
1 ‚àí ùë¶ùëò max‚Ä≤
(cid:3)
proof.

(cid:0)

(cid:1)

Now we start proving the converse. If ùë¶ ùëó , 1 ‚àí ùë• ùëó = ùë¶ùúÉ = ùë¶ùúÉ min, ùë¶ùëò , 1 ‚àí ùë•ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò, ùëò ‚â† ùëó ,
‚Ä≤ = 0, by

(105) and the equality in (106) hold, since from (87) we have ùõº ùëó = 1, or equivalently, ùõº ùëó
‚Ä≤ into (66), we obtain
substituting the above ùë¶ùëò ‚Äôs and ùõº ùëó
ùëö

‚Ä≤ =

ùõºùëò

ùõΩ
1 ‚àí ùõΩ

1
ùëèùëò

1 ‚àí ùõΩ
ùõΩ

h

ùëùùëòùë¶ùëò ‚àí

ùëùùëñùë¶ùëñ max‚Ä≤

, ‚àÄùëò, ùëò ‚â† ùëó ,

(110)

√ïùëñ=1
ùëñ‚â†ùëó

i

and by substituting the ùë•ùëò‚Äôs transformed from the above ùë¶ùëò ‚Äôs (including ùëò = ùëó = ùúÉ ) into (86), we
have

ùëö

ùëö

ùëùùëòùë•ùëò =

ùëùùëò

1 ‚àí ùë¶ùëò max‚Ä≤

=

√ïùëò=1
ùëò‚â†ùúÉ

√ïùëò=1
ùëò‚â†ùúÉ

(cid:0)

(cid:1)

1 ‚àí ùõΩ
ùõΩ

ùëùùúÉùë•ùúÉ ‚àí

1
ùõΩ

ùëèùúÉ .

From (110), since ùëó = ùúÉ , by substituting (106) into (110), we obtain
1
ùëèùëò

(ùëùùëòùë¶ùëò ‚àí ùëùùúÉùë¶ùúÉ min), ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ .

‚Ä≤ =

ùõºùëò

(111)

(112)

‚Ä≤ ‚â§ 1, from (112), we obtain ùëùùúÉùë¶ùúÉ min + ùëèùëò ‚â§ ùëùùëòùë¶ùëò ‚â§ ùëùùúÉùë¶ùúÉ min, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ .

Since 0 ‚â§ ùõºùëò
Therefore, based on Lemma 10, we have ùëùùëòùë•ùëò min‚Ä≤ ‚â§ ùëùùëò
Recall that based on (108), for each ùëò, ùëò ‚â† ùúÉ , ùëùùëò

is the maximum of ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min
(cid:1)
and ùëùùëòùë•ùëò min. DeÔ¨Åne Œ¶ the set of ùëò‚Äôs yielding ùëùùëò
= ùëùùëòùë•ùëò min ‚â• ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min, ùëò ‚àà Œ¶, and
1 ‚àí ùë¶ùëò max‚Ä≤
(cid:0)
> ùëùùëòùë•ùëò min,
deÔ¨Åne Œ© the complement of Œ¶, i.e., the set of ùëò‚Äôs yielding ùëùùëò
= ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min
1 ‚àí ùë¶ùëò max‚Ä≤
(cid:0)
(cid:1)
ùëò ‚àà Œ©. In addition, let ùúô , |Œ¶| and ùúî , |Œ©|. Note that ùúô + ùúî = ùëö ‚àí 1. We have the following lemma.
Lemma 11. If ùë¶ùúÉ = ùë¶ùúÉ min, ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò, ùëò ‚â† ùúÉ , and the equality in (106) holds, we have ùúî ‚â§ 1‚àíùõΩ
ùõΩ .

1 ‚àí ùë¶ùëò max‚Ä≤

(cid:1)

(cid:0)

(cid:1)

(cid:0)

1 ‚àí ùë¶ùëò max‚Ä≤

, ‚àÄùëò = 1, . . . , ùëö, ùëò ‚â† ùúÉ .

54

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

Proof. If ùë¶ùúÉ = ùë¶ùúÉ min, ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò, ùëò ‚â† ùúÉ , and the equality in (106) holds, from (106) we have

ùõΩ
1 ‚àí ùõΩ

=

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ

ùëùùëòùë¶ùëò max‚Ä≤

.

(113)

√ç
1 ‚àí ùë¶ùëò max‚Ä≤
1 ‚àí ùë¶ùëò max‚Ä≤
(cid:1)

= ùëùùëò ‚àíùëùùúÉùë¶ùúÉ min, or equivalently, ùëùùëòùë¶ùëò max‚Ä≤ =
= ùëùùëòùë•ùëò min, or equivalently, ùëùùëòùë¶ùëò max‚Ä≤ =

Note that since for those ùëò‚Äôs in Œ©, we have ùëùùëò
ùëùùúÉùë¶ùúÉ min, and for those ùëò‚Äôs in Œ¶, we have ùëùùëò
(cid:0)
ùëùùëòùë¶ùëò max, (113) thus becomes

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ

ùëùùëòùë¶ùëò max‚Ä≤

=

√ç

We thus Ô¨Ånish the proof.

(cid:0)

(cid:1)

ùëùùúÉùë¶ùúÉ min

ùúîùëùùúÉùë¶ùúÉ min +

ùëò ‚ààŒ¶ ùëùùëòùë¶ùëò max

√ç

‚â§

1
ùúî

.

(114)

(cid:3)

DeÔ¨Åne X ,

ùëùùëò
addition, by deÔ¨Ånitions of Œ¶ and Œ©, we have
(cid:0)
√ç

1 ‚àí ùë¶ùëò max‚Ä≤

and Y ,

ùëö
ùëò=1
ùëò‚â†ùúÉ

√ç

(cid:1)

ùëö
ùëò=1
ùëò‚â†ùúÉ

ùëùùëòùë•ùëò min‚Ä≤. Based on Lemma 10, we have X ‚â• Y. In

ùëö

X ,

ùëùùëò

1 ‚àí ùë¶ùëò max‚Ä≤

√ïùëò=1
ùëò‚â†ùúÉ

(cid:0)

(cid:1)

=

=

ùëùùëòùë•ùëò min +

ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min

(115)

√ïùëò ‚ààŒ¶
1 ‚àí ùõΩ
ùõΩ

ùëùùúÉùë•ùúÉ ‚àí

(cid:0)

√ïùëò ‚ààŒ©
1
ùëèùúÉ . (Based on (111))
ùõΩ

(cid:1)

Similarly to the Œ¶ and Œ© in X, deÔ¨Åne Œ¶‚Äô the set of ùëò‚Äôs yielding ùëùùëòùë•ùëò min‚Ä≤ = ùëùùëòùë•ùëò min ‚â• ùëùùúãùë•ùúã min +ùëèùëò,
ùëò ‚àà Œ¶‚Äô, for Y. Since based on (109) in Lemma 10, we have ùëùùúã ùë•ùúã min+ùëèùëò ‚â§ ùëùùëò ‚àíùëùùúÉùë¶ùúÉ min for all ùëò except
ùúÉ . Therefore, for those ùëò‚Äôs belonging to Œ¶ (in X), based on (108), we get ùëùùëòùë•ùëò min ‚â• ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min ‚â•
ùëùùúã ùë•ùúã min + ùëèùëò, and based on (107), we Ô¨Ånd that those ùëò‚Äôs belonging to Œ¶ (in X) also belong to Œ¶‚Äô (in
Y), i.e., Œ¶ ‚äÜ Œ¶‚Äô. Therefore, Y can be interpreted by Œ¶ as follow.

ùëö

Y ,

ùëùùëòùë•ùëò min‚Ä≤

√ïùëò=1
ùëò‚â†ùúÉ

=

ùëùùëòùë•ùëò min +

max

√ïùëò ‚ààŒ¶

√ïùëò ‚ààŒ©

ùëùùúã ùë•ùúã min + ùëèùëò, ùëùùëòùë•ùëò min
n

o

.

In addition, similarly to Y, we deÔ¨Åne Z as follow.

Z ,

ùëùùëòùë•ùëò min +

ùëùùúã ùë•ùúã min + ùëèùëò

.

√ïùëò ‚ààŒ¶

√ïùëò ‚ààŒ©

(cid:0)

(cid:1)

(116)

(117)

Achieving Transparency Report Privacy in Linear Time

55

Clearly, the RHS of Z is not greater than the RHS of Y, and thus we have X ‚â• Y ‚â• Z. DeÔ¨Åne
W , X ‚àí Z, based on (115) and (117), we have

W , X ‚àí Z

=

=

=

ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min

‚àí

ùëùùúã ùë•ùúã min + ùëèùëò

√ïùëò ‚ààŒ©

h (cid:0)

(cid:1)

ùëùùëò ‚àí ùëùùúÉùë¶ùúÉ min

‚àí

(cid:1) i

(cid:0)
ùëùùúã ùë•ùúã min + ùëùùëò ‚àí ùõΩ

√ïùëò ‚ààŒ©

h (cid:0)
ùõΩ

ùëö

(cid:1)

(cid:0)
ùëùùëñ ‚àí ùëùùúÉùë¶ùúÉ min ‚àí ùëùùúã ùë•ùúã min

ùëö

ùëùùëñ

√ïùëñ=1

(cid:1)i

h

√ïùëò ‚ààŒ©
ùõΩ

= ùúî

√ïùëñ=1
ùëö
ùëùùëñ ‚àí ùëùùúÉùë¶ùúÉ min ‚àí ùëùùúãùë•ùúã min

h

√ïùëñ=1

i

.

i

(118)

DeÔ¨Åne C , ùõΩ
ùëö
ùëñ=1 ùëùùëñ ‚àí ùëùùúÉùë¶ùúÉ min ‚àí ùëùùúã ùë•ùúã min, the constant term in (118). Since X ‚â• Z, we have
W = ùúîC ‚â• 0, and because ùúî is the cardinality of Œ©, it is non-negative, and thus C ‚â• 0. Since
C ‚â• 0, and from Lemma 11, ùúî ‚â§ 1‚àíùõΩ

√ç

ùõΩ , we thus have

Y ‚â• Z = X ‚àí

X ‚àí Z

= X ‚àí W = X ‚àí ùúîC ‚â• X ‚àí

1 ‚àí ùõΩ
ùõΩ

C.

(119)

Note that since ùë¶ùúÉ = ùë¶ùúÉ min, the RHS of (119) becomes

(cid:0)

(cid:1)

X ‚àí

1 ‚àí ùõΩ
ùõΩ

C

1 ‚àí ùõΩ
ùõΩ

ùëùùúÉùë¶ùúÉ min +

1 ‚àí ùõΩ
ùõΩ

ùëùùúãùë•ùúã min

ùëùùëñ +

1 ‚àí ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min

(120)

=

=

1 ‚àí ùõΩ
ùõΩ

ùëùùúÉùë•ùúÉ ‚àí

1
ùõΩ

h
1 ‚àí ùõΩ
ùõΩ

ùëùùúÉùë•ùúÉ ‚àí

1
ùõΩ

ùëèùúÉ

‚àí

i

1 ‚àí ùõΩ
ùõΩ

C

ùëö

ùëèùúÉ ‚àí (1 ‚àí ùõΩ)

ùëùùëñ +

√ïùëñ=1
ùëèùúÉ ‚àí (1 ‚àí ùõΩ)

ùëö

ùëùùúÉ (ùë•ùúÉ + ùë¶ùúÉ ) ‚àí

1
ùõΩ

=

1 ‚àí ùõΩ
ùõΩ

=

1 ‚àí ùõΩ
ùõΩ

=

=

1 ‚àí ùõΩ
ùõΩ
1 ‚àí ùõΩ
ùõΩ

ùëùùúÉ ‚àí

ùëèùúÉ ‚àí

1 ‚àí ùõΩ
ùõΩ

1 ‚àí ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëèùúÉ .

ùëèùúÉ ‚àí (1 ‚àí ùõΩ)

ùëùùúã ùë•ùúã min ‚àí ùëèùúÉ

√ïùëñ=1
ùëùùëñ +

1 ‚àí ùõΩ
ùõΩ

ùëö

√ïùëñ=1

ùëèùúÉ +

1 ‚àí ùõΩ
ùõΩ

ùëùùúãùë•ùúã min ‚àí ùëèùúÉ

Therefore, we obtain

ùëö

ùëùùëòùë•ùëò min‚Ä≤ = Y ‚â• X ‚àí

√ïùëò=1
ùëò‚â†ùúÉ

1 ‚àí ùõΩ
ùõΩ

C =

1 ‚àí ùõΩ
ùõΩ

ùëùùúãùë•ùúã min ‚àí ùëèùúÉ .

(121)

Since ùëó = ùúÉ , by replacing ùúÉ in (121) by ùëó , we obtain (99) and Ô¨Ånish the proof of the converse.

56

Chien-Lun Chen, Leana Golubchik, and Ranjan Pal

We summarize Case 2.2 in the following:

ùëÖ‚àó

ùëó ‚â• 1 and

1 ‚àí ùõΩ
ùõΩ

ùëö

ùëùùúã ùë•ùúã min ‚àí ùëè ùëó ‚â§

ùëùùëòùë•ùëò min‚Ä≤

√ïùëò=1
ùëò‚â†ùëó

‚áê‚áí ùë¶ùúÉ = ùë¶ùúÉ min and ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ
ùëö

ùëùùëòùë¶ùëò max‚Ä≤

, ùë• ‚Ä†0
ùëó

‚áê‚áí ùë• ‚Ä†
ùëó

=

‚áê‚áí ùõΩ ‚â•

1
ùëù ùëó

ùëù ùëó ‚àí

n

ùõΩ
1 ‚àí ùõΩ

√ïùëò=1
ùëò‚â†ùëó

ùëùùúÉùë¶ùúÉ min +

ùëùùëòùë¶ùëò max‚Ä≤

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ

o

, ùõΩ0.

√ç

Therefore, if (S1) (S4) are true, (OPT-1) has feasible solutions for arbitrary ùë•ùëò min and ùë•ùëò max, ‚àÄùëò =
1, . . . , ùëö, ùëò ‚â† ùëó , which implies (OPT-1) has feasible solutions for all the cases (Case 1, Case 2.1,
and Case 2.2), which requires ùõΩ to be greater than the minimum ùõΩ in each of the above cases, i.e.,
ùõΩ ‚â• max{ùõΩ0, ùõΩ1, ùõΩùëù ùëó }, where

ùõΩ0 =

ùõΩ1 =

=

ùõΩùëù ùëó

,

,

ùëùùúÉùë¶ùúÉ min
ùëö
ùëò=1
ùëò‚â†ùúÉ
ùëùùúã ùë•ùúã min
√ç
ùëö
ùëò=1
ùëò‚â†ùúã

ùëùùúÉùë¶ùúÉ min +

ùëùùëòùë¶ùëò max‚Ä≤

ùëùùúãùë•ùúã min +

ùëùùëòùë•ùëò max‚Ä≤

ùëùùúãùë•ùúã min + ùëù ùëóùë¶ ùëó min
√ç
ùëö
ùëò=1 ùëùùëò

,

and the corresponding optimal objective value ùë• ‚Ä†

√ç

ùëó and its corresponding optimal solutions are

ùõΩ = ùõΩ0 ‚áê‚áí ùë• ‚Ä†
ùëó

=

1
ùëù ùëó

ùëù ùëó ‚àí
n

ùõΩ
1 ‚àí ùõΩ

ùëö

√ïùëò=1
ùëò‚â†ùëó

ùëùùëòùë¶ùëò max‚Ä≤

, ùë• ‚Ä†0
ùëó

o

‚áê‚áí ùë¶ ùëó = ùë¶ùúÉ = ùë¶ùúÉ min

ùë¶ùëò = ùë¶ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùúÉ ,

ùõΩ = ùõΩ1 ‚áê‚áí ùë• ‚Ä†
ùëó

=

1 ‚àí ùõΩ
ùõΩ

1
ùëù ùëó

n

ùëùùúã ùë•ùúã min ‚àí

ùëö

√ïùëò=1
ùëò‚â†ùëó,ùúã

ùëùùëòùë•ùëò max‚Ä≤

, ùë• ‚Ä†1
ùëó

o

‚áê‚áí ùë•ùúã = ùë•ùúã min

ùë•ùëò = ùë•ùëò max‚Ä≤, ‚àÄùëò = 1, ¬∑ ¬∑ ¬∑ , ùëö, ùëò ‚â† ùëó, ùúã,

ùõΩ = ùõΩùëù ùëó ‚áê‚áí ùë• ‚Ä†

ùëó

=

1
ùëù ùëó

ùëùùúã ùë•ùúã min + ùëù ùëó ‚àí ùõΩ
n

ùëö

ùëùùëò

, ùë• ‚Ä†ùëù
ùëó

√ïùëò=1

o

‚áê‚áí ùë•ùúã = ùë•ùúã min
ùëö

1 ‚àí 2ùõΩ
ùõΩ

ùëùùúã ùë•ùúã min ‚àí ùëù ùëó + ùõΩ

ùëùùëò .

ùëö

√ïùëò=1

√ïùëò=1
ùëò‚â†ùëó,ùúã
We thus Ô¨Ånish the proof of Lemma 9.

ùëùùëòùë•ùëò =

