Chordal Sparsity for Lipschitz Constant Estimation
of Deep Neural Networks

Anton Xue, Lars Lindemann, Alexander Robey, Hamed Hassani, George J. Pappas, and Rajeev Alur †

2
2
0
2

r
p
A
2

]

G
L
.
s
c
[

1
v
6
4
8
0
0
.
4
0
2
2
:
v
i
X
r
a

Abstract— Lipschitz constants of neural networks allow for
guarantees of robustness in image classiﬁcation, safety in
controller design, and generalizability beyond the training
data. As calculating Lipschitz constants is NP-hard, techniques
for estimating Lipschitz constants must navigate the trade-off
between scalability and accuracy. In this work, we signiﬁcantly
push the scalability frontier of a semideﬁnite programming
technique known as LipSDP while achieving zero accuracy loss.
We ﬁrst show that LipSDP has chordal sparsity, which allows us
to derive a chordally sparse formulation that we call Chordal-
LipSDP. The key beneﬁt
the main computational
is that
bottleneck of LipSDP, a large semideﬁnite constraint, is now
decomposed into an equivalent collection of smaller ones —
allowing Chordal-LipSDP to outperform LipSDP particularly
as the network depth grows. Moreover, our formulation uses
a tunable sparsity parameter that enables one to gain tighter
estimates without incurring a signiﬁcant computational cost.
We illustrate the scalability of our approach through extensive
numerical experiments.

I. INTRODUCTION

Neural networks are arguably the most common choice
of function approximators used in machine learning and
artiﬁcial intelligence. Their success is well documented in
the literature and showcased in various applications, e.g., in
solving the game Go [1] and in handwritten character recog-
nition [2]. However, neural networks have shown to be non-
robust, i.e., their outputs may be sensitive to small changes
in the inputs and result in large deviations in the outputs [3],
[4]. It is hence often unclear what a neural network exactly
learns and how it can generalize to previously unseen data.
This is a particular concern in safety critical applications like
perception in autonomous driving, where one would like to
be robust and even obtain a robustness certiﬁcate. A way to
measure the robustness of a neural network f : Rn1 → Rm
is to calculate its Lipschitz constant L that satisﬁes

(cid:107)f (x) − f (y)(cid:107) ≤ L(cid:107)x − y(cid:107)

for all x, y ∈ Rn1 .

The exact calculation of L is NP-hard and hence poses
computational challenges [5], [6]. Therefore, past effort has
been on estimating upper bounds on the Lipschitz constant
L in computationally efﬁcient ways. A key difﬁculty here
is to appropriately model the nonlinear activation functions
within a neural network. For feedforward neural networks,
the authors in [7] abstract activation functions using incre-
mental quadratic constraints [8]. These are then formed into
a convex semideﬁnite program, referred to as LipSDP, whose

† The authors are with the School of Engineering and Applied
Sciences, University
PA, USA.
Pennsylvania,
{antonxue,larsl,arobey1,hassani,pappasg,alur}@
seas.upenn.edu

Philadelphia,

of

solution yields tight upper bounds on L. As the size of the
neural network grows, however, the general formulation of
LipSDP becomes computationally intractable. One may par-
tially alleviate this issue by selectively reducing the number
of optimization variables, which will induce sparsity into
LipSDP at the cost of a looser bound. Still, this does not
address the core computational bottleneck of LipSDP, which
is that the solver must process a large semideﬁnite constraint
whose dimension scales with the number of neurons.

In this paper we study computationally efﬁcient formu-
lations of LipSDP. In particular, we introduce a variant
of LipSDP that exhibits chordal sparsity [9], [10], which
allows us to decompose a large semideﬁnite constraint into
an equivalent collection of smaller ones. Moreover, our
formulation has a tunable sparsity parameter, enabling one
to trade-off between efﬁciency and accuracy. We call our de-
composed semideﬁnite program Chordal-LipSDP, and study
its theoretical properties and computational performance in
this paper. The contributions of our work are as follows:

• We introduce a variant of LipSDP formulated in terms
of a sparsity parameter τ and precisely characterize its
chordal sparsity pattern. This allows us to decompose
LipSDP, which is a large semideﬁnite constraint, into
a collection of smaller ones, yielding an equivalent
problem that we call Chordal-LipSDP.

• We present numerical evaluations and observe that
Chordal-LipSDP is signiﬁcantly faster than LipSDP,
especially for deeper networks, without accuracy loss
relative to LipSDP. Furthermore, adjusting τ allows
Chordal-LipSDP to obtain rapidly tightening bounds on
L without incurring a high performance penalty.

• We make an open-source implementation available at

github.com/AntonXue/chordal-lipsdp.

A. Related Work

There has been a great interest in the machine learning
and control communities towards efﬁciently and accurately
estimating Lipschitz constants of neural networks. Indeed,
it has been shown that there is a close connection between
the Lipschitz constant of a neural network and its ability
to generalize [11]. The authors in [12] were among the
ﬁrst to normalize weights of a neural network based on the
Lipschitz constant. In control, Lipschitz constants of neural
network-based control laws can be used to obtain stability
or safety guarantees [13], [14]. Training neural networks
with a desired Lipschitz constant is, however, difﬁcult. In
practice, one has to either solve constrained optimization

 
 
 
 
 
 
problems, e.g., [15], or iteratively bootstrap training pa-
rameters. As a consequence, one is interested in obtaining
Lipschitz certiﬁcates of neural networks. In [5], [6], it was
shown that exact calculation of the Lipschitz constant is NP-
hard. As estimating Lipschitz constants is computationally
challenging, we are here motivated to efﬁciently estimate
Lipschitz constants of neural networks.

Broadly, there are two ways for estimating Lipschitz con-
stants of general nonlinear functions, either sampling-based
as in [16] and [17], or using optimization techniques [7],
[18]. A naive approach is to calculate the product of the norm
of the weights of each individual layer. The authors in [5]
follow a similar idea, and obtain tighter Lipschitz constants
using singular value decomposition and maximization over
the unit cube. This, however, still becomes quickly com-
putationally intractable for large neural networks. Tighter
bounds have been obtained in [19] capturing cross-layer
dependencies using compositions of nonexpansive averaged
operators. However, again this approach does not scale well
with the number of layers. While these works estimate global
Lipschitz constants, it was shown in [20] that estimating local
Lipschitz can be done more efﬁciently.

In this paper, we build on the LipSDP framework pre-
sented in [7], which amounts to solving a semideﬁnite
optimization program. LipSDP abstracts activation functions
into quadratic constraints and allows to encode rich layer-to-
layer relations allowing to trade-off accuracy and efﬁciency.
While LipSDP considers the l2-norm, general lp-norms on
the input output relation of a neural network can be con-
servatively obtained using the equivalence of norms. The
authors in [18] present LiPopt, which is a polynomial op-
timization framework that allows to calculate tight estimates
of Lipschitz constants for l2 and l∞-norms. However, for
l2-norms LipSDP empirically shows to have tighter bounds.
Exact computation of the Lipschitz constant under l1 and
l∞ norms was presented in [6] by solving a mixed integer
linear program. Lipschitz continuity of a neural network with
respect to its training parameters has been analyzed in [21].
We show that a particular formulation of LipSDP satisﬁes
chordal sparsity [9], [10], from which we apply chordal
decomposition to obtain Chordal-LipSDP. Applications of
chordal sparsity has also been explored in other domains
[22], [23], [24], [25]. The key beneﬁt of exploiting chordal
sparsity is that a large semideﬁnite constraint is decomposed
into an equivalent collection of smaller ones, in particular
allowing us to scale to deeper networks. This equivalence
also means that LipSDP and Chordal-LipSDP will compute
identical estimates of the Lipschitz constant.

II. BACKGROUND AND PROBLEM FORMULATION

In this section, we state the problem formulation and

provide background on LipSDP and chordal sparsity.

A. Lipschitz Constant Estimation of Neural Networks

We consider feedforward neural networks f : Rn1 → Rm
with K ≥ 2 layers, i.e., K − 1 hidden layers and one linear
output layer. From now on, let x1 ∈ Rn1 denote the input

of the neural network. The output of the neural network is
recursively computed for layers k = 1, . . . , K − 1 as

f (x1) := WKxK + bK,

xk+1 := φ(Wkxk + bk),

(1)

where Wk and bk are the weight matrices and bias vec-
tors of the kth layer, respectively,
that are assumed to
be of appropriate size. We denote the dimensions of
x2, . . . , xK by n2, . . . , nK ∈ N. The function φ(u) :=
vcat(ϕ(u1), ϕ(u2) . . .) is the stack vector of activation func-
tions ϕ, e.g., ReLU or tanh activation functions, that are
applied element-wise. We assume throughout the paper that
the same type of activation function is used across all layers.

B. LipSDP

We now present LipSDP [7] in a way that enables us later
to conveniently characterize the chordal sparsity pattern of
LipSDP. First, let x := vcat(x1, . . . , xK) ∈ RN be a stack
of the state vectors with N := (cid:80)K

A :=






W1
...
0

· · ·
. . .
· · · WK−1

0
...





k=1 nk. By deﬁning
In2
...
0

· · ·
. . .
· · ·

0
...
0




0
...

 , B :=
0






0
...
InK

and b := vcat(b1, b2, . . . , bK−1) we can rewrite the dynam-
ics of (1) as Bx = φ(Ax + b), where φ : RNf → RNf is
a Nf -height stack of ϕ with Nf := n2 + · · · + nK. To deal
with the nonlinear activation function φ in an efﬁcient way,
the key idea in LipSDP is to abstract φ using incremental
quadratic constraints [8]. In particular, LipSDP considers a
family of symmetric indeﬁnite matrices Q such that any
matrix Q ∈ Q satisﬁes

(cid:20)

u − v
φ(u) − φ(v)

(cid:21)(cid:62)

(cid:20)

Q

(cid:21)

u − v
φ(u) − φ(v)

≥ 0

(2)

for all u, v ∈ RNf . In the case where each element ϕ of φ
is [s, s]-sector-bounded, i.e., where its subgradients satisfy
∂ϕ ⊆ [s, s], then one possible parameterization of Q is

Q :=

(cid:40)(cid:20)A
B

(cid:21)(cid:62) (cid:20) −2ssT
(s + s)T

(s + s)T
−2T

(cid:21)

(cid:21) (cid:20)A
B

(cid:41)

: γα ≥ 0

where T is a dense matrix that is parametrized by γα. In this
paper, we ﬁx an integer τ ≥ 0 and deﬁne T as follows1

T :=

Nf
(cid:88)

i=1

(γα)iieie(cid:62)

i +

(cid:88)

(i,j)∈Iτ

(γα)ij(ei − ej)(ei − ej)(cid:62),

Iτ := {(i, j) = 1 ≤ i < j ≤ Nf , j − i ≤ τ }.

By tuning the value of τ , we obtain different formulations
of Q that all provide over-approximations of φ as in (2)
while allowing us to trade-off on the spectrum of sparsity
and accuracy. In the sparsest case, i.e., τ = 0, the matrix T
is a nonnegative diagonal matrix and γα ∈ RNf
+ , while in the
densest case, i.e., τ = Nf − 1, the matrix T is fully dense
and parameterized by γα ∈ R1+···+Nf

.

+

1We specialize T to be τ -banded whereas LipSDP permits T to be dense.

However this restriction induces a chordally sparse structure in LipSDP.

To formulate our variant of LipSDP, we deﬁne the linearly-

Zα(γα) :=

parametrized matrix-valued functions
(cid:20)A
(cid:21)(cid:62) (cid:20) −2ssT
(s + s)T
(s + s)T
−2T
B
K WK)EK − γ(cid:96)E(cid:62)
K(W (cid:62)
Z(cid:96)(γ(cid:96)) := E(cid:62)
Ek := (cid:2)· · ·
0
0

1 E1,
· · ·(cid:3) ∈ Rnk×N ,

(cid:21)

(cid:21) (cid:20)A
B

,

Ink
where T is deﬁned as above, γ(cid:96) ∈ R+, and Ek is the kth
block-index selector such that xk = Ekx. Now combine the
above terms as

By decomposing a chordally sparse matrix with respect to
its maximal cliques, a key result in sparse matrix analysis
allows us to deduce the semideﬁniteness of a large matrix
with respect to a collection of smaller matrices.

Lemma 1 (Theorem 2.10 [10]). Let G(V, E) be a chordal
graph and let {C1, . . . , Cp} be the set of its maximal cliques.
Then X ∈ Sn(E) and X (cid:23) 0 if and only if there exists
Xk ∈ S|Ck| such that each Xk (cid:23) 0 and

X =

p
(cid:88)

k=1

E(cid:62)
Ck

XkECk .

(6)

Z(γ) := Zα(γα) + Z(cid:96)(γ(cid:96)) ∈ SN ,

then LipSDP is the following semideﬁnite program:

minimize
γ≥0

γ(cid:96)

subject to Z(γ) (cid:22) 0

(3)

(4)

We say that (6) is a chordal decomposition of X by
C1, . . . , Cp, and such a decomposition allows us to solve a
large semideﬁnite constraint using an equivalent collection
of smaller ones.

(cid:96) is the optimal value of (4), then the Lipschitz constant

If γ(cid:63)
of f is upper-bounded by (γ(cid:63)

(cid:96) )1/2, see [7]. That is,

(cid:107)f (x) − f (y)(cid:107) ≤ (γ(cid:63)

(cid:96) )1/2(cid:107)x − y(cid:107)

for all x, y ∈ Rn1.

C. Chordal Sparsity

Chordal sparsity establishes a connection between graph
theory and sparse matrix decomposition [26], [9]. In the con-
text of this paper, we aim to solve the potentially large-scale
semideﬁnite program (4) using chordal sparsity in Z(γ). This
is done by decomposing the semideﬁnite constraint Z(γ) (cid:22) 0
into an equivalent collection of smaller Zk (cid:22) 0 constraints,
which we demonstrate in Section III.

1) Chordal Graphs and Sparse Matrices: A graph
G(V, E) consists of vertices V := {1, . . . , n} and edges
E ⊆ V × V. We assume that E is symmetric, i.e. (i, j) ∈ E
implies (j, i) ∈ E, and so G(V, E) is an undirected graph.
We say that the vertices C ⊆ V form a clique if u, v ∈ C
implies (u, v) ∈ E, and let C(i) be the ith vertex of C under
the natural ordering. A maximal clique is a clique that is not
strictly contained within another clique. A cycle of length
l is a sequence of vertices v1, . . . , vl with (vl, v1) ∈ E and
adjacent connections (vi, vi+1) ∈ E. A chord is any edge
that connects two nonadjacent vertices in a cycle, and we
say that a graph is chordal if every cycle of length four has
at least one chord [9].

An edge set E can dually describe the sparsity pattern of
a matrix. Given a graph G(V, E), deﬁne the set of symmetric
matrices of size n with sparsity pattern E as

Sn(E) := {X ∈ Sn : Xij = Xji = 0 if (i, j) (cid:54)∈ E}.
If in addition G(V, E) is chordal and X ∈ Sn(E), then we
say that X has chordal sparsity or is chordally sparse. For
X with sparsity E, we say that Xij is dense if (i, j) ∈ E,
and that it is sparse otherwise.

(5)

2) Chordal Decomposition of Sparse Matrices: For a
chordally sparse X ∈ Sn(E), useful decompositions can
be analyzed through the cliques of G(V, E). Given a clique
Ck ⊆ V, deﬁne its block-index matrix as follows:

(ECk )ij = 1 if Ck(i) = j else 0, ECk ∈ R|Ck|×n.

III. CHORDAL DECOMPOSITION OF LIPSDP

In this section, we present Chordal-LipSDP which is a
chordally sparse formulation of LipSDP. We ﬁrst identify
the sparsity pattern for Z(γ) in Theorem 1 and then present
Chordal-LipSDP in Theorem 2 as a chordal decomposition of
LipSDP. An equivalence result is then stated in Theorem 3.
The proofs of our results can be found in the appendix.

Our goal is to construct the edge set E of a chordal graph
G(V, E) with vertices V := {1, . . . , N } such that Z(γ) ∈
SN (E). To gain intuition for E, we plot the dense entries of
Z(γ) in Figure 1 where the (i, j) square is dark if (Z(γ))ij
is dense, i.e., (i, j) ∈ E.

Fig. 1. The sparsity of Z(γ) for τ = 0, 2, 4 with dimensions (3, 3, 3, 3, 3).
For each increment of τ , each block grows by one unit on the bottom and
right, and corresponds to a maximal clique of G(V, E). As τ increases the
number of blocks (maximal cliques) will decrease as the lower-right blocks
become overshadowed. At τ = 0 we have what [7] refers to as “LipSDP-
neuron”; at τ = Nf − 1 we have the completely dense “LipSDP-network”.

In order to compactly present our results, we ﬁrst deﬁne

a notation for summation as follows:

S(k) :=

k
(cid:88)

l=1

nl, nK+1 := m, S(0) := 0, S(K) := N.

Our main results are then stated in the following theorems.

Theorem 1. Let Z(γ) be deﬁned as in (3). It holds that
Z(γ) ∈ SN (E), where E := (cid:83)K−1

k=1 Ek such that

Ek := (cid:8)(i, j) : S(k − 1) + 1 ≤ i, j ≤ S(k + 1) + τ (cid:9).

Note that

the set E deﬁned in Theorem 1 is already
illustrated in Fig. 1. Also, we implicitly assume that all
(i, j) in the deﬁnition of E are within 1 ≤ i, j ≤ N . From
this construction of E it is then straightforward to prove
chordality of G(V, E) and identify its maximal cliques.

The runtimes (seconds) of Chordal-LipSDP, LipSDP, and CP-Lip on a subset of the networks. The times for Naive-Lip are omitted because it
Fig. 2.
ﬁnishes in < 0.1 seconds on all instances. We ran Chordal-LipSDP and LipSDP for τ = 0, . . . , 6. Because CP-Lip is independent of τ , it is a constant
line. Moreover, due to the scaling exponentially with respect to the number of layers, we only ran CP-Lip for networks of depth ≤ 25.

Theorem 2. Let V := {1, . . . , N } and deﬁne E as in
Theorem 1. Then G(V, E) is chordal and the set of
its
maximal cliques is {C1, . . . , Cp}, where

Theorem 3. The semideﬁnite programs (4) and (7) are
equivalent: γ is a solution for (4) iff γ, Z1, . . . , Zp is a
solution for (7). Moreover, their optimal values are identical.

p := min {k : S(k + 1) + τ ≥ N }

IV. EXPERIMENTS

and each clique Ck for k < p has size and elements

|Ck| := nk + nk+1 + τ,

Ck(i) := S(k − 1) + i

for 1 ≤ i ≤ |Ck|. The ﬁnal clique Cp has elements

Cp(i) := S(p − 1) + i,

1 ≤ i ≤ N − S(p − 1).

Using Lemma 1, the maximal cliques {C1, . . . , Cp} from
Theorem 2 now give a chordal decomposition of Z(γ), and
lets us formulate the following semideﬁnite program that we
call Chordal-LipSDP:

minimize
γ≥0,Z1,...,Zp

γ(cid:96)

subject to Z(γ) =

p
(cid:88)

k=1

E(cid:62)
Ck

ZkECk ,

Zk (cid:22) 0 for k = 1, . . . , p,

We remark that solving Chordal-LipSDP is typically much
faster than solving LipSDP, especially for deep neural net-
works as we impose a set of smaller semideﬁnite matrix con-
straints instead of one large semideﬁnite matrix constraint. In
other words, the computational beneﬁt of (7) over (4) is that
each Zk (cid:22) 0 constraint is a signiﬁcantly smaller LMI than
Z(γ) (cid:22) 0, which is especially the case for deeper networks.
In the next theorem, we show that LipSDP and Chordal-
LipSDP compute Lipschitz constants that are in fact identi-
cal, i.e., a chordal decomposition of LipSDP gives no loss
of accuracy over the original formulation.

In this section we evaluate the effectiveness of Chordal-

LipSDP. Our aim is to answer the following questions:

(Q1) How well does Chordal-LipSDP scale in compar-
ison to the baseline methods?
(Q2) How does the computed Lipschitz constant vary
as the sparsity parameter τ increases?

(Dataset) We use a randomly generated batch of neural
networks with random weights from N (0, 1/2), with depth
K = d, widths n2 = · · · = nK = w, and input-output
n1 = m = 2 for w ∈ {10, . . . , 50}, and d ∈ {5, 10, . . . , 50}.
As a naming convention, for instance, W30-D20 would be
the random network with w = 30 and d = 20. In total there
are 50 such random networks.

(7)

(Baseline Methods) We compare Chordal-LipSDP against

the following baselines:

• LipSDP: as in (4), using the same values of τ
• Naive-Lip: by taking L = (cid:81)K
• CP-Lip [19], which scales exponentially with depth.
To the best of our knowledge this is the only2 other
method that can handle general activation functions
while yielding a non-trivial bound.

k=1 (cid:107)Wk(cid:107)2

(System) All experiments were run on an Intel i9-9940X
with 28 cores and 125 GB of RAM. Our codebase was
implemented with Julia 1.7.2 and we used MOSEK 9.3 as
our convex solver with a tolerance of ε = 10−6.

2The method of [5] is not a true upper-bound of the Lipschitz constant,
although it is often such in practice as demonstrated in [7]. The method
of [6] assumes piecewise linear activations.

A. (Q1) Runtime of Chordal-LipSDP vs Baselines

We ﬁrst evaluate the runtime of Chordal-LipSDP against
the baselines of LipSDP, Naive-Lip, and CP-Lip. For each
random network we ran both Chordal-LipSDP and LipSDP
with sparsity parameter values of τ = 0, . . . , 6 and record
their respective runtimes in Figure 2. Because Naive-Lip and
CP-Lip do not depend on the sparsity parameter τ , they
therefore appear as constant times for all sparsities; we omit
plotting the Naive-Lip times because they are < 0.1 seconds
for all networks. Moreover, because the runtime of CP-Lip
scales exponentially with the number of layers, we only ran
CP-Lip for networks of depth ≤ 25.

Figure 2 gives a general comparison for scalability be-
tween LipSDP and Chordal-LipSDP. We further record the
runtimes of these two methods when the width of the network
is ﬁxed and the depth is varied in Figure 3, as well as when
the depth is ﬁxed and the width is varied in Figure 4.

LipSDP signiﬁcantly out-scales LipSDP, especially for net-
works of depth ≥ 20. Moreover, Chordal-LipSDP also
achieves better scaling for higher values of τ compared
to LipSDP. In general, Naive-Lip is consistently the fastest
method, while CP-Lip is initially fast, but quickly falls off
on deep networks due to exponential scaling with depth.

B. (Q2) Lipschitz Constant vs Sparsity Parameter

We also studied how the value of τ affects the resulting
Lipschitz constant and plot
the results in Figure 5. In
particular, as τ increases, the estimate rapidly improves by at
least an order of magnitude. Moreover, the Lipschitz constant
estimate is also better than Naive-Lip and CP-Lip — when
the runtime would be reasonable (depth ≤ 25).

Fig. 3.
The runtimes (seconds) of LipSDP and Chordal-LipSDP as the
depth varies. Each plot shows networks that share the same width, but whose
depths are varied on the x-axis. Each curve shows the runtimes for a different
value of τ = 0, . . . , 6, with higher curves corresponding to higher runtimes
— and in this case also higher values of τ . We shade the region between
the τ = 0 and τ = 6 curves for each method.

Fig. 4.
The runtimes (seconds) of LipSDP and Chordal-LipSDP as the
width varies. Similar to Figure 3, but the x-axis now shows varying widths.

We see that as the network depth increases, Chordal-

Fig. 5.
The Lipschitz constant estimate given by Chordal-LipSDP (the
same as LipSDP) on some networks, with τ on the x-axis. On the left we
also plot the estimates given by CP-Lip (green) and Naive-Lip (purple).

C. Discussion

Our experiments show that Chordal-LipSDP out-scales
LipSDP on deeper networks, but this is not necessarily the
case for shallower networks, e.g. when depth ≤ 10. This
is likely because the overhead of creating many smaller
constraints of the form Zk (cid:22) 0, as well as a large equality
constraint Z(γ) = (cid:80)p
k=1 E(cid:62)
ZkECk may only be worth-
Ck
while when there are sufﬁciently many maximal cliques, i.e.,
when the network is deep. This also means that Chordal-
LipSDP is likely more resource intensive than LipSDP.

For LipSDP we found that solving the dual problem was
almost always signiﬁcantly faster than solving the primal.
This dualization did not yield noticeable beneﬁts for Chordal-
LipSDP, however, and so all instances of Chordal-LipSDP
were solved for the primal.

Additionally, we found it helpful to scale the weights
of Wk in order to make sure that the solver receives a
sufﬁciently well-conditioned problem, especially for larger
problem instances. To ensure scaling correctness, we require
that the scaled network (cid:98)f must satisfy f (x) = c1 (cid:98)f (c0x) for
all x ∈ Rn1 for some known c0, c1 ∈ R.
V. CONCLUSIONS
We present Chordal-LipSDP, a chordally sparse variant of
LipSDP for estimating the Lipschitz constant of a feedfor-

ward neural network. We give a precise characterization of
the sparsity structure present, and using this we decompose a
large semideﬁnite constraint of LipSDP — which is its main
computational bottleneck — into an equivalent collection of
smaller constraints.

Our numerical experiments show that Chordal-LipSDP
signiﬁcantly out-scales LipSDP, especially on deeper net-
works. Moreover, our formulation introduces a tunable spar-
sity parameter that allows the user to ﬁnely trade-off accuracy
and scalability: in fact it is often possible to gain rapidly
tightening estimates of the Lipschitz constant without incur-
ring a major performance penalty.

REFERENCES

[1] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “A general
reinforcement learning algorithm that masters chess, shogi, and go
through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.
[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[3] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-
ing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[4] J. Su, D. V. Vargas, and K. Sakurai, “One Pixel Attack for Fooling
Deep Neural Networks,” IEEE Transactions on Evolutionary Compu-
tation, vol. 23, no. 5, pp. 828–841, 2019.

[5] A. Virmaux and K. Scaman, “Lipschitz regularity of Deep Neural
Networks: Analysis and Efﬁcient Estimation,” in Proceedings of
the Advances in Neural Information Processing Systems, vol. 31,
Montreal, Canada, December 2018, p. 3839–3848.

[6] M. Jordan and A. G. Dimakis, “Exactly Computing the Local Lipschitz
Constant of ReLU Networks,” in Proceedings of the Advances in
Neural Information Processing Systems, vol. 33, December 2020, pp.
7344–7353.

[7] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas, “Efﬁ-
cient and Accurate Estimation of Lipschitz Constants for Deep Neural
Networks,” in Proceedings of the Advances in Neural Information
Processing Systems, vol. 32, Vancouver, Canada, December 2019, pp.
11 427–11 438.

[8] B. Ac¸ıkmes¸e and M. Corless, “Observers for systems with nonlineari-
ties satisfying incremental quadratic constraints,” Automatica, vol. 47,
no. 7, pp. 1339–1348, 2011.

[9] L. Vandenberghe and M. S. Andersen, “Chordal graphs and semidef-
inite optimization,” Foundations and Trends in Optimization, vol. 1,
no. 4, pp. 241–433, 2015.

[10] Y. Zheng, “Chordal Sparsity in Control and Optimization of Large-
scale Systems,” Ph.D. dissertation, University of Oxford, 2019.
[11] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky, “Spectrally-normalized
margin bounds for neural networks,” in Proceedings of the Conference
on Neural Information Processing Systems, vol. 30, Long Beach,
California, USA, December 2017.

[12] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral
normalization for generative adversarial networks,” arXiv preprint
arXiv:1802.05957, 2018.

[13] M. Jin and J. Lavaei, “Stability-Certiﬁed Reinforcement Learning:
A Control-Theoretic Perspective,” IEEE Access, vol. 8, pp. 229 086–
229 100, 2020.

[14] L. Lindemann, A. Robey, L. Jiang, S. Tu, and N. Matni, “Learning
Robust Output Control Barrier Functions from Safe Expert Demon-
strations,” arXiv preprint arXiv:2111.09971, 2021.

[15] H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree, “Regularisation of
neural networks by enforcing lipschitz continuity,” Machine Learning,
vol. 110, no. 2, pp. 393–416, 2021.

[16] G. Wood and B. Zhang, “Estimation of the Lipschitz constant of a
function,” Journal of Global Optimization, vol. 8, no. 1, pp. 91–103,
1996.

[17] A. Chakrabarty, D. K. Jha, G. T. Buzzard, Y. Wang, and K. G.
Vamvoudakis, “Safe Approximate Dynamic Programming via Kernel-
ized Lipschitz Estimation,” IEEE Transactions On Neural Networks
and Learning Systems, vol. 32, no. 1, pp. 405–419, 2020.

[18] F. Latorre, P. T. Y. Rolland, and V. Cevher, “Lipschitz constant
estimation for Neural Networks via sparse polynomial optimization,”
in Proceedings of the International Conference on Learning Repre-
sentations, April 2020.

[19] P. L. Combettes and J.-C. Pesquet, “Lipschitz Certiﬁcates for Layered
Network Structures Driven by Averaged Activation Operators,” SIAM
Journal on Mathematics of Data Science, vol. 2, no. 2, pp. 529–557,
2020.

[20] T. Avant and K. A. Morgansen, “Analytical bounds on the local Lips-
chitz constants of ReLU networks,” arXiv preprint arXiv:2104.14672,
2021.

[21] C. Herrera, F. Krach, and J. Teichmann, “Estimating Full Lip-
schitz Constants of Deep Neural Networks,” arXiv preprint
arXiv:2004.13135, 2020.

[22] R. P. Mason and A. Papachristodoulou, “Chordal Sparsity, Decompos-
ing SDPs and the Lyapunov Equation,” in Proceedings of the American
Control Conference, Portland, Oregon, USA, June 2014, pp. 531–537.
[23] L. P. Ihlenfeld and G. H. Oliveira, “A Faster Passivity Enforcement
Method via Chordal Sparsity,” Electric Power Systems Research, vol.
204, p. 107706, 2022.

[24] H. Chen, H.-T. D. Liu, A. Jacobson, and D. I. Levin, “Chordal De-
composition for Spectral Coarsening,” ACM Transactions on Graphics,
vol. 39, no. 6, pp. 1–16, 2020.

[25] M. Newton and A. Papachristodoulou, “Exploiting Sparsity for Neural
Network Veriﬁcation,” in Proceedings of Learning for Dynamics and
Control, June 2021, pp. 715–727.

[26] A. Griewank and P. L. Toint, “On the existence of convex decompo-
sitions of partially separable functions,” Mathematical Programming,
vol. 28, no. 1, pp. 25–49, 1984.

A. Proof of Theorem 1

APPENDIX

To simplify and formalize the proof, we ﬁrst need to
introduce some useful notation. We extend the deﬁnition of
sparsity patterns to general matrices. Let E ⊆ {1, . . . , m} ×
{1, . . . , n} and deﬁne analogously to (5):

Mm×n(E) := {M ∈ Rm×n : Mij = 0 if (i, j) (cid:54)∈ E},

(8)

and for (i, j) ∈ E associated with an m × n matrix we will
assume that 1 ≤ i ≤ m and 1 ≤ j ≤ n. When m = n, we
simply write Mn. Let E (cid:62) be the transpositioned (inverse)
pairs of E, and note that E = E (cid:62) iff E is symmetric. We will
explicitly distinguish between symmetric and nonsymmetric
E when necessary. Whenever we write Sn(E) it is implied
that E is symmetric, and that the undirected graph G(V, E)
is therefore well-deﬁned.

In the remainder, we also use the following notation

ki := min{k : S(k) ≥ i},

1 ≤ i ≤ N.

There are a few useful properties for ki that we remark:

• ki is the index of (n1, . . . , nK) that 1 ≤ i ≤ N falls in.
• If i ≤ j, then ki ≤ kj.
• S(ki − 1) ≤ i ≤ S(ki) for all 1 ≤ i ≤ N .

Also, some rules of sparse matrix arithmetics are as follows:

A ∈ Mm×n(E) =⇒ A(cid:62) ∈ Mn×m(E (cid:62))

A ∈ Mn(EA), B ∈ Mn(EB) =⇒ A + B ∈ Mn(EA ∪ EB)
A ∈ Mn(E) =⇒ A + A(cid:62) ∈ Sn(E ∪ E (cid:62))

To prove Theorem 1, we need to show that Z(γ) ∈ SN (E).

Note ﬁrst that Z(γ) can be expressed as:

Z(γ) = A(cid:62)T A + B(cid:62)T B + A(cid:62)T B + B(cid:62)T A

+ E(cid:62)

KW (cid:62)

K WKEK − γlE(cid:62)

1 E1.

The proof of Theorem 1 follows ﬁve steps and analyzes
sparsity of each term in Z(γ) separately. For better readabil-
ity, we summarize these ﬁve steps next and provide detailed
proofs for each step in separate lemmas.

Step 1. We construct the edge set EB := (cid:83)K−1
EB,k := (cid:8)(i, j) : S(k − 1) + 1 ≤ j ≤ S(k),

k=1 EB,k where

S(k) − τ + 1 ≤ i ≤ S(k + 1) + τ (cid:9).

In Lemma 2, we show that B(cid:62)T A ∈ MN (EB). By symme-
try, it then also holds that A(cid:62)T B ∈ MN (E (cid:62)

B ).

Step 2. By construction, each EB,k has dense entries only
in the column range S(k − 1) + 1 ≤ j ≤ S(k), which means
that EB,k ∩ EB,k(cid:48) = ∅ when k (cid:54)= k(cid:48). The goal now is to
show that B(cid:62)T A + A(cid:62)T B is in a sense the “frontier” of
growth for Z(γ) as τ increases, as seen in Figure 1. To more
easily analyze the growth pattern of EB ∪ E (cid:62)
B , we deﬁne an
over-approximation EC := (cid:83)K

k=1 EC,k ⊇ EB where
EC,k := (cid:8)(i, j) : S(k − 1) + 1 ≤ j ≤ S(k),

1 ≤ i ≤ S(k + 1) + τ (cid:9).

Each EC,k is similar to EB,k, but with the i index range
relaxed. In addition the union is up to K, which is the depth
of the neural network. EC is then a stair-case like sparsity
pattern where the top side is dense (resp. the left side of E (cid:62)
C
is dense), and so EC ∩ E (cid:62)
C is an overlapping block diagonal
structure. Our goal is now to show that each term of Z(γ)
C , beginning with B(cid:62)T A + A(cid:62)T B. In
has sparsity EC ∩ E (cid:62)
Lemma 3, we show that EB ∪ E (cid:62)
B ⊆ EC ∩ E (cid:62)
C . By Step 1, it
consequently follows that

B(cid:62)T A + A(cid:62)T B ∈ SN (EB ∪ E (cid:62)

B ) ⊆ SN (EC ∩ E (cid:62)

C ).

Step 3. Let us next deﬁne the edge set EA as

EA := (cid:8)(i, j) : S(kj) − τ + 1 ≤ S(ki + 1),
S(ki) − τ + 1 ≤ S(kj + 1)(cid:9).

In Lemma 4, we show that

A(cid:62)T A + E(cid:62)

KW (cid:62)

K WKEK − γ(cid:96)E(cid:62)

1 E1 ∈ SN (EA),

while we show that EA ⊆ EC ∩ E (cid:62)

C in Lemma 5.

Step 4. For the remaining term B(cid:62)T B of Z(γ), we show

that B(cid:62)T B ∈ SN (EC ∩ E (cid:62)

C ) in Lemma 6.

Step 5. The previous steps imply that Z(γ) ∈ SN (EC ∩
E (cid:62)
C ). Particularly, by Lemmas 3, 5, and 6, each term has
sparsity EC ∩ E (cid:62)
C , and therefore so does their sum. Finally,
we show that EC ∩ E (cid:62)
C ⊆ E in Lemma 7. This therefore
(cid:3)
means that Z(γ) ∈ SN (E) and concludes the proof.

B. Statement and proof of Lemma 2
Lemma 2. It holds that B(cid:62)T A ∈ MN (EB).
Proof. We analyze the action of B(cid:62)T on each block column
Ak ∈ RNf ×nk of A separately, where,

A = (cid:2)A1

· · · AK−1

0(cid:3) , A =

K−1
(cid:88)

k=1

AkEk.

Since Wk ∈ Rnk+1×nk , the entry (Ak)ij is dense iff

S(k) − n1 + 1 ≤ i ≤ S(k + 1) − n1,

and there is no condition on j because each column of Ak
has at least one dense entry. Observe that T is a τ -banded
matrix, and therefore has the same sparsity as R+R(cid:62), where

R := I + U + · · · + U τ , U is the upper shift matrix.

Thus (T Ak)ij is dense iff

S(k) − n1 − τ + 1 ≤ i ≤ S(k + 1) − n1 + τ.

Finally, left-multiplication by B(cid:62) pads a zero block of height
n1 at the top, and so (B(cid:62)T Ak)ij is dense iff

S(k) − τ + 1 ≤ i ≤ S(k + 1) + τ.

Right-multiplication by Ek puts Ak into the kth block
column of (n1, . . . , nK), so (B(cid:62)T AkEk)ij is dense iff

S(k − 1) + 1 ≤ j ≤ S(k),

S(k) − τ + 1 ≤ i ≤ S(k + 1) + τ

which shows that B(cid:62)T AkEk has sparsity EB,k. Thus,

B(cid:62)T A =

K−1
(cid:88)

k=1

B(cid:62)T AkEk ∈ MN

(cid:33)

EB,k

= MN (EB).

(cid:32)K−1
(cid:91)

k=1

By symmetry we also have that A(cid:62)T B ∈ MN (E (cid:62)

B ); the
dense blocks of B(cid:62)T A grow vertically with τ , and those of
A(cid:62)T B grow horizontally.

C. Statement and proof of Lemma 3

Lemma 3. It holds that EB ∪ E (cid:62)

B ⊆ EC ∩ E (cid:62)
C .

Proof. We show that EB,k ⊆ EC and EB,k ⊆ E (cid:62)
C for any
1 ≤ k ≤ K − 1. It sufﬁces to consider only EB,k because
C is symmetric, and would therefore also contain E (cid:62)
EC ∩E (cid:62)
B,k.
To show that EB,k ⊆ EC, observe that EB,k ⊆ EC,k. To
C , consider (i, j) ∈ EB,k, and we claim

show that EB,k ⊆ E (cid:62)
that (i, j) ∈ E (cid:62)

C,ki

, for which we need to satisfy

S(ki − 1) + 1 ≤ i ≤ S(ki),

1 ≤ j ≤ S(ki + 1) + τ.

The LHS inequalities follow from the previously stated prop-
erties of ki. For the RHS inequalities deduce the following
from the deﬁnition of EB,k:

j ≤ S(k), S(k) − τ + 1 ≤ i =⇒ j ≤ S(k) ≤ i + τ − 1,

and since i ≤ S(ki + 1) we have

j ≤ i + τ − 1 ≤ S(ki + 1) + τ,

meaning that (i, j) ∈ E (cid:62)

C,ki

⊆ E (cid:62)
C .

D. Statement and proof of Lemma 4
Lemma 4. It holds that
KW (cid:62)

A(cid:62)T A + E(cid:62)

K WKEK − γ(cid:96)E(cid:62)

1 E1 ∈ SN (EA).

Proof. Note that (γ(cid:96)E(cid:62)
(A(cid:62)T A)ij is dense, and therefore it sufﬁces to show that

1 E1)ij being dense implies that

A(cid:62)T A + E(cid:62)

KW (cid:62)

K WKEK = W (cid:62) (cid:98)T W ∈ SN (EA),

(cid:98)T := blockdiag(T, I), W := blockdiag(W1, . . . , Wk),
where it is assumed that each Wk ∈ Rnk+1×nk is dense.
Because (cid:98)T has more sparse entries than a τ -banded matrix
of the same size, it is therefore less dense than R(cid:62)R, where
R := I + U + · · · + U τ , U is the upper shift matrix.
Let V := RW , it then sufﬁces to show that V (cid:62)V ∈ SN (EA).
Observe that (V (cid:62)V )ij = (cid:80)
l VliVlj is dense iff the ith and
jth columns of V share a row (cid:96) at which Vli and Vlj are
both dense. Let Vk ∈ R(n2+···+nK +m)×nk be the kth block
column of V , then (Vk)ij is dense iff

S(k) − n1 − τ + 1 ≤ i ≤ S(k + 1) − n1.

Thus Vk and Vk(cid:48) have rows at which they are both dense iff
S(k) − n1 − τ + 1 ≤ S(k(cid:48) + 1) − n1
S(k(cid:48)) − n1 − τ + 1 ≤ S(k + 1) − n1,

which are equivalent to the conditions described in EA.

E. Statement and proof of Lemma 5
Lemma 5. It holds that EA ⊆ EC ∩ E (cid:62)
C .

Proof. By symmetry of EA, it sufﬁces to prove that EA ⊆ EC.
Consider (i, j) ∈ EA, we claim that (i, j) ∈ EC,kj — for
which a sufﬁcient condition is

S(kj − 1) + 1 ≤ j ≤ S(kj),

1 ≤ i ≤ S(kj + 1) + τ,

The LHS inequalities follow from the properties of kj. For
the RHS inequalities, recall that i ≤ S(ki), and rewrite the
second condition of EA to yield

1 ≤ i ≤ S(ki) ≤ S(kj + 1) + τ − 1,

and so (i, j) ∈ EC,kj ⊆ EC.

F. Statement and proof of Lemma 6
Lemma 6. It holds that B(cid:62)T B ∈ SN (EC ∩ E (cid:62)
Proof. By symmetry, it sufﬁces to show B(cid:62)T B ∈ MN (EC).
Because left (resp. right) multiplication by B(cid:62) (resp. B)
consists of padding zeros on the top (resp. left), we may
treat B(cid:62)T B as a τ -banded matrix. First suppose that j ≤ i,
then (B(cid:62)T B)ij is dense iff i ≤ j + τ . Since j ≤ S(kj + 1),

C ).

1 ≤ i ≤ j + τ ≤ S(kj + 1) + τ,

which shows that (i, j) ∈ EC,kj ⊆ EC.

Now suppose that i ≤ j, then (B(cid:62)T B)ij is dense iff

j ≤ i + τ . Furthermore, S(ki) ≤ S(kj) ≤ S(kj + 1), so

1 ≤ j ≤ i + τ ≤ S(ki) + τ ≤ S(kj + 1) + τ

which again shows that (i, j) ∈ EC,kj ⊆ EC.

G. Statement and proof of Lemma 7
Lemma 7. It holds that EC ∩ E (cid:62)
Proof. Consider (i, j) ∈ EC ∩E (cid:62)
generality that i ≤ j. Then (i, j) ∈ EC,kj and (i, j) ∈ E (cid:62)
meaning that the following conditions hold:

C and suppose without loss of
,
C,ki

C ⊆ E.

S(kj − 1) + 1 ≤ j ≤ S(kj), 1 ≤ i ≤ S(kj + 1) + τ,
S(ki − 1) + 1 ≤ i ≤ S(ki), 1 ≤ j ≤ S(ki + 1) + τ.

Tightening the bounds on i and by monotonicity of S,

S(ki − 1) + 1 ≤ i ≤ S(ki) ≤ S(ki + 1) + τ,

and also for j we have

S(ki − 1) + 1 ≤ S(kj − 1) + 1 ≤ j ≤ S(ki + 1) + τ,

which together imply that (i, j) ∈ Eki ⊆ E.

H. Proof of Theorem 2

The structure of E results in a lengthy proof of Theorem 1.
However, it is easy to guess each Ek by simple experiments,
and leads to a straightforward proof of Theorem 2.

Observe that SN (Ek) is the set of block diagonal matrices

whose (i, j) entry is dense iff

S(k − 1) + 1 ≤ i, j ≤ S(k + 1) + τ.

Because E is a union of the Ek sparsities, SN (E) is therefore
the set of matrices with overlapping block diagonals, which
are known to be chordal [9, Section 8.2].

It remains to identify the maximal cliques of G(V, E). First
consider k < p with k ≥ 1, and observe that (N, N ) (cid:54)∈ Ek.
By construction each Ek is the edges of a clique, and when
k < p such Ek is also not contained by any other Ek(cid:48) because

(cid:40)

S(k − 1) + 1 < S(k(cid:48) − 1) + 1,
S(k(cid:48) + 1) + τ < S(k + 1) + τ,

if k < k(cid:48)
if k > k(cid:48),

meaning that there exists (i, j) ∈ Ek \ Ek(cid:48) with

(cid:40)

i = j = S(k − 1) + 1

i = j = S(k + 1) + τ

if k < k(cid:48)
if k > k(cid:48).

Consequently, Ek is in fact the edges of a maximal clique
containing indices i that satisfy

S(k − 1) + 1 ≤ i ≤ S(k + 1) + τ,

which are exactly the conditions of Ck for k < p.

Now consider k > p with k ≤ K − 1, and observe that

Ek ⊆ Ep because any (i, j) ∈ Ek will satisfy

S(p − 1) + 1 < S(k − 1) + 1 ≤ i, j ≤ N ≤ S(p + 1) + τ,

and so (i, j) ∈ Ep as well. Ep is therefore the edges of a
clique that contains all other cliques that contain N , and is
thus maximal — corresponding to the description of Cp. (cid:3)

I. Proof of Theorem 3

Because Z(γ) ∈ SN (E) and G(V, E) is chordal with
maximal cliques {C1, . . . , Cp}, conclude from Lemma 1 that
(cid:3)
Z(γ) (cid:22) 0 iff each Zk (cid:22) 0.

