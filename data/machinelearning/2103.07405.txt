Optimal sequential decision making with probabilistic digital twins

Christian Agrella,b, Kristina Rognlien Dahlb, Andreas Hafvera

aGroup Research and Development, DNV, Norway
bDepartment of Mathematics, University of Oslo, Norway

1
2
0
2

r
a

M
2
1

]
L
M

.
t
a
t
s
[

1
v
5
0
4
7
0
.
3
0
1
2
:
v
i
X
r
a

Abstract

Digital twins are emerging in many industries, typically consisting of simulation models and data associ-
ated with a speciﬁc physical system. One of the main reasons for developing a digital twin, is to enable
the simulation of possible consequences of a given action, without the need to interfere with the physical
system itself. Physical systems of interest, and the environments they operate in, do not always behave
deterministically. Moreover, information about the system and its environment is typically incomplete or
imperfect. Probabilistic representations of systems and environments may therefore be called for, especially
to support decisions in application areas where actions may have severe consequences. A probabilistic digital
twin is a digital twin, with the added capability of proper treatment of uncertainties associated with the
consequences, enabling better decision support and management of risks.

In this paper we introduce the probabilistic digital twin (PDT). We will start by discussing how epistemic
uncertainty can be treated using measure theory, by modelling epistemic information via σ-algebras. Based
on this, we give a formal deﬁnition of how epistemic uncertainty can be updated in a PDT. We then study
the problem of optimal sequential decision making. That is, we consider the case where the outcome of
each decision may inform the next. Within the PDT framework, we formulate this optimization problem.
We discuss how this problem may be solved (at least in theory) via the maximum principle method or the
dynamic programming principle. However, due to the curse of dimensionality, these methods are often not
tractable in practice. To mend this, we propose a generic approximate solution using deep reinforcement
learning together with neural networks deﬁned on sets. We illustrate the method on a practical problem,
considering optimal information gathering for the estimation of a failure probability.

Keywords: Probabilistic digital twin. Epistemic uncertainty. Sequential decision making. Partially
observable Markov decision process. Deep reinforcement learning.

1. Introduction

The use of digital twins has emerged as one of the
major technology trends the last couple of years.
In essence, a digital twin (DT) is a digital repre-
sentation of some physical system, including data
from observations of the physical system, which can
be used to perform forecasts, evaluate the conse-
quences of potential actions, simulate possible fu-
ture scenarios, and in general inform decision mak-
ing without requiring interference with the physical
system. From a theoretical perspective, a digital

pre-print: This is a pre-print version of this article

twin may be regraded to consist of the following
two components:

• A set of assumptions regarding the physical
system (e.g. about the behaviour or relation-
ships among system components and between
the system and its environment), often given
in the form of a physics-based numerical simu-
lation model.

• A set of information, usually in the form of a
set of observations, or records of the relevant
actions taken within the system.

In some cases, a digital twin may be desired for a
system which attributes and behaviours are not de-
terministic, but stochastic. For example, the degra-
dation and failure of physical structures or machin-

 
 
 
 
 
 
ery is typically described as stochastic processes. A
systems performance may be impacted by weather
or ﬁnancial conditions, which also may be most
appropriately modelled as stochastic. Sometimes
the functioning of the system itself is stochastic,
such as supply chain or production chains involving
stochastic variation in demand and performance of
various system components.

Even for systems or phenomena that are deter-
ministic in principle, a model will never give a per-
fect rendering of reality. There will typically be
uncertainty about the model’s structure and pa-
rameters (i.e. epistemic uncertainty), and if conse-
quences of actions can be critical, such uncertainties
need to be captured and handled appropriately by
the digital twin. In general, the system of interest
will have both stochastic elements (aleatory uncer-
tainty) and epistemic uncertainty.

If we want to apply digital twins to inform deci-
sions in systems where the analysis of uncertainty
and risk is important, certain properties are re-
quired:

1. The digital twin must capture uncertainties:
This could be done by using a probabilistic rep-
resentation for uncertain system attributes.
2. It should be possible to update the digital twin
as new information becomes available: This
could be from new evidence in the form of data,
or underlying assumptions about the system
that have changed.

3. For the digital twin to be informative in de-
cision making, it should be possible to query
the model suﬃciently fast: This could mean
making use of surrogate models or emulators,
which introduces additional uncertainties.

These properties are paraphrased from Hafver et
al. [1], which provides a detailed discussion on the
use of digital twins for on-line risk assessment. In
this paper we propose a mathematical framework
for deﬁning digital twins that comply with these
properties. As Hafver et al.
[1], we will refer to
these as probabilistic digital twins (PDTs), and we
will build on the Bayesian probabilistic framework
which is a natural choice to satisfy (1)-(2).

A numerical model of a complex physical sys-
tem can often be computationally expensive, for
instance if it involves numerical solution of nontriv-
ial partial diﬀerential equations. In a probabilistic
setting this is prohibitive, as a large number of eval-
uations (e.g. PDE solves) is needed for tasks involv-
ing uncertainty propagation, such as prediction and

2

inference. Applications towards real-time decision
making also sets natural restrictions with respect to
the runtime of such queries. This is why property
(3) is important, and why probabilistic models of
complex physical phenomena often involve the use
of approximate alternatives, usually obtained by
”ﬁtting” a computationally cheap model to the out-
put of a few expensive model runs. These compu-
tationally cheap approximations are often referred
to as response surface models, surrogate models or
emulators in the literature.

Introducing this kind of approximation for com-
putational eﬃciency also means that we introduce
additional epistemic uncertainty into our modelling
framework. By epistemic uncertainty we mean, in
short, any form of uncertainty that can be reduced
by gathering more information (to be discussed fur-
ther later on). In our context, uncertainty may in
principle be reduced by running the expensive nu-
merical modes instead of the cheaper approxima-
tions.

Many interesting sequential decision making
problems arise from the property that our knowl-
edge about the system we operate changes as we
learn about the outcomes. That is, each decision
may aﬀect the epistemic uncertainty which the next
decision will be based upon. We are motivated by
this type of scenario, in combination with the chal-
lenge of ﬁnding robust decisions in safety-critical
systems, where a decision should be robust with re-
spect to what we do not know, i.e. with respect
to epistemic uncertainty. Although we will not re-
strict the framework presented in this paper to any
speciﬁc type of sequential decision making objec-
tives, we will mainly focus on problems related to
optimal information gathering. That is, where the
decisions we consider are related to acquiring infor-
mation (e.g., by running an experiment) in order
to reduce the epistemic uncertainty with respect
to some speciﬁed objective (e.g., estimating some
quantity of interest).

A very relevant example of such a task, is the
problem of optimal experimental design for struc-
tural reliability analysis. This involves deciding
which experiments to run in order to build a surro-
gate model that can be used to estimate a failure
probability with suﬃcient level of conﬁdence. This
is a problem that has received considerable atten-
tion (see e.g. [2, 3, 4, 5, 6, 7, 8]). These methods all
make use of a myopic (one-step lookahead) criterion
to determine the ”optimal” experiment, as a multi-
step or full dynamic programming formulation of

the optimization problem becomes numerically in-
feasible. In Agrell and Dahl [2], they consider the
case where there are diﬀerent types of experiments
to choose from. Here, the myopic (one-step looka-
head) assumption can still be justiﬁed, but if the
diﬀerent types of experiments are associated with
diﬀerent costs, then it can be diﬃcult to apply in
practice (e.g., if a feasible solution requires expen-
sive experiments with delayed reward).

We will review the mathematical framework of
sequential decision making, and connect this to the
deﬁnition of a PDT. Traditionally, there are two
main solution strategies for solving discrete time se-
quential decision making problems: Maximum prin-
ciples, and dynamic programming. We review these
two solution methods, and conclude that the PDT
framework is well suited for a dynamic program-
ming approach. However, dynamic programming
suﬀers from the curse of dimensionality, i.e. pos-
sible sequences of decisions and state realizations
grow exponentially with the size of the state space.
Hence, we are typically not able to solve a PDT
sequential decision making problem in practice di-
rectly via dynamic programming.

As a generic solution to the problem of optimal
sequential decision making we instead propose an
alternative based on reinforcement learning. This
means that when we consider the problem of ﬁnd-
ing an optimal decision policy, instead of truncating
the theoretical optimal solution (from the Bellman
equation) by e.g., looking only one step ahead, we
try to approximate the optimal policy. This approx-
imation can be done by using e.g. a neural network.
Here we will frame the sequential decision making
setup as a Markov decision process (MDP), in gen-
eral as a partially observed MDP (POMDP), where
a state is represented by the information available
at any given time. This kind of state speciﬁcation is
often referred to as the information state-space. As
a generic approach to deep reinforcement learning
using PDTs, we propose an approach using neu-
ral networks that operate on the information state-
space directly.

Main contributions. In this paper we will:

(i) Propose a mathematical framework for mod-
elling epistemic uncertainty based on measure
theory, and deﬁne epistemic conditioning.
(ii) Present a mathematical deﬁnition of the prob-
abilistic digital twin (PDT). This is a mathe-
matical framework for modelling physical sys-
tems with aleatory and epistemic uncertainty.

(iii) Introduce the problem of sequential decision
making in the PDT, and illustrate how this
problem can be solved (at least in theory) via
maximum principle methods or the dynamic
programming principle.

(iv) Discuss the curse of dimensionality for these
solution methods, and illustrate how the se-
quential decision making problem in the PDT
can be viewed as a partially observable Markov
decision process.

(v) Explain how reinforcement learning (RL) can
be applied to ﬁnd approximate optimal strate-
gies for sequential decision making in the PDT,
and propose a generic approach using a deep
sets architecture that enables RL directly on
the information state-space. We end with a
numerical example to illustrate this approach.

The paper is structured as follows:

In Section
2 we introduce epistemic uncertainty and suggest
modeling this via σ-algebras. We also deﬁne epis-
temic conditioning.
In Section 3, we present the
mathematical framework, as well as a formal def-
inition, of a probabilistic digital twin (PDT), and
discuss how such PDTs are used in practice.

Then, in Section 4, we introduce the problem
of stochastic sequential decision making. We dis-
cuss the traditional solution approaches, in partic-
ular dynamic programming which is theoretically a
suitable approach for decision problems that can be
modelled using a PDT. However, due to the curse of
dimensionality, using the dynamic programming di-
rectly is typically not tractable. We therefore turn
to reinforcement learning using function approxi-
mation as a practical alternative. In Section 5, we
show how an approximate optimal strategy can be
achieved using deep reinforcement learning, and we
illustrate the approach with a numerical example.
Finally, in Section 6 we conclude and sketch some
future works in this direction.

2. A measure-theoretic treatment of epis-

temic uncertainty

In this section, we review the concepts of epis-
temic and aleatory uncertainty, and introduce a
measure-theoretic framework for modelling epis-
temic uncertainty. We will also deﬁne epistemic
conditioning.

2.1. Motivation

In uncertainty quantiﬁcation (UQ), it is com-
mon to consider two diﬀerent kinds of uncertainty:

3

Aleatory (stochastic) and epistemic (knowledge-
based) uncertainty. We say that uncertainty is epis-
temic if we foresee the possibility of reducing it
through gathering more or better information. For
instance, uncertainty related to a parameter that
has a ﬁxed but unknown value is considered epis-
temic. Aleatory uncertainty, on the other hand,
is the uncertainty which cannot (in the modellers
perspective) be aﬀected by gathering information
alone. Note that the characterization of aleatory
and epistemic uncertainty has to depend on the
modelling context. For instance, the result of a
coin ﬂip may be viewed as epistemic, if we imag-
ine a physics-based model that could predict the
outcome exactly (given all initial conditions etc.).
However, under most circumstances it is most natu-
ral to view a coin ﬂip as aleatory, or that it contains
both aleatory and epistemic uncertainty (e.g. if the
bias of the coin us unknown). Der Kiureghian et al.
[9] provides a detailed discussion of the diﬀerences
between aleatory and epistemic uncertainty.

In this paper, we have two main reasons for dis-
tinguishing between epistemic and aleatory uncer-
tainty. First, we would like to make decisions that
are robust with respect to epistemic uncertainty.
Secondly, we are interested in studying the eﬀect of
gathering information. Modelling epistemic uncer-
tainty is a natural way of doing this.

In the UQ literature, aleatory uncertainty is typ-
ically modelled via probability theory. However,
epistemic uncertainty is represented in many diﬀer-
ent ways. For instance, Helton [10] considers four
diﬀerent ways of modelling epistemic uncertainty:
Interval analysis, possibility theory, evidence theory
(Dempster–Shafer theory) and probability theory.
In this paper we take a measure-theoretic ap-
proach. This provides a framework that is rela-
tively ﬂexible with respect to the types of assump-
tions that underlay the epistemic uncertainty. As a
motivating example, consider the following typical
setup used in statistics:

Example 2.1. (A parametric model)
Let X = (Y, θ) where Y is a random variable repre-
senting some stochastic phenomenon, and assume
Y is modelled using a given probability distribu-
tion, P (Y |θ), that depends on a parameter θ (e.g.
Y ∼ N (µ, σ) with θ = (µ, σ)). Assume that we
do not know the value of θ, and we therefore con-
sider θ as a (purely) epistemic parameter. For some
ﬁxed value of θ, the random variable Y is (purely)
aleatory, but in general, as the true value of θ is

4

not known, Y is associated with both epistemic and
aleatory uncertainty.

The model X in Example 2.1 can be decoupled
into an aleatory component Y |θ and an epistemic
component θ. Any property of the aleatory uncer-
tainty in X is determined by P (Y |θ), and is there-
fore a function of θ. For instance, the probabil-
ity P (Y ∈ A|θ) and the expectation E[f (Y )|θ], are
both functions of θ. There are diﬀerent ways in
which we can choose to address the epistemic un-
certainty in θ. We could consider intervals, for in-
stance the minimum and maximum of P (Y ∈ A|θ)
over any plausible value of θ, or assign probabilities,
or some other measure of belief, to the possible val-
ues θ may take. However, in order for this to be
well-deﬁned mathematically, we need to put some
requirements on A, f and θ. By using probability
theory to represent the aleatory uncertainty, we im-
plicitly assume that the set A and function f are
measurable, and we will assume that the same holds
for θ. We will describe in detail what is meant by
measurable in Section 2.2 below. Essentially, this is
just a necessity for deﬁning properties such as dis-
tance, volume or probability in the space where θ
resides.

In this paper we will rely on probability the-
ory for handling both aleatory and epistemic un-
certainty. This means that, along with the measur-
ability requirement on θ, we have the familiar setup
for Bayesian inference:

Example 2.2. (A parametric model – Inference
and prediction) If θ from Example 2.1 is a random
variable with distribution P (θ), then X = (Y, θ) de-
notes a complete probabilistic model (capturing both
aleatory and epistemic uncertainty). X is a random
variable with distribution

P (X) = P (Y |θ)P (θ).

Let I be some piece of information from which
Bayesian inference is possible, i.e. P (X|I) is well
deﬁned. We may then deﬁne the updated joint dis-
tribution

Pnew(X) = P (Y |θ)P (θ|I),

and the updated marginal (predictive) distribution
for Y becomes

(cid:90)

Pnew(Y ) =

P (Y |θ)dP (θ|I).

Note that the distribution Pnew(X) in Example
2.2 is obtained by only updating the belief with
respect to epistemic uncertainty, and that

Pnew(X) (cid:54)= P (X|I) = P (Y |I, θ)P (θ|I).

For instance, if I corresponds to an observation of
Y , e.g. I = {Y = y}, then P (Y |I) = δ(y), the
Dirac delta at y, whereas P (θ|I) is the updated
distribution for θ having observed one realization
In the following, we will refer to the kind
of Y .
of Bayesian updating in Example 2.2 as epistemic
updating.

This epistemic updating of the model considered
in Example 2.1 and Example 2.2 should be fairly
intuitive, if

1. All epistemic uncertainty is represented by a

single parameter θ, and

2. θ is a familiar object like a number or a vector

in Rn.

But what can we say in a more general setting? It
is common that epistemic uncertainty comes from
lack of knowledge related to functions. This is
the case with probabilistic emulators and surrogate
models. The input to these functions may contain
epistemic and/or aleatory uncertainty as well. Can
we talk about isolating and modifying the epistemic
uncertainty in such a model, without making refer-
ence to the speciﬁc details of how the model has
been created? In the following we will show that
with the measure-theoretic framework, we can still
make use of a simple formulation like the one in
Example 2.2.

2.2. The probability space

Let X be a random variable containing both
aleatory and epistemic uncertainty. In order to de-
scribe how X can be treated like in Example 2.1
and Example 2.2, but for the general setting, we
will ﬁrst recall some of the basic deﬁnitions from
measure theory and measure-theoretic probability.
To say that X is a random variable, means that X
is deﬁned on some measurable space (Ω, F). Here,
Ω is a set, and if X takes values in Rn (or some other
measurable space), then X is a so-called measurable
function, X(ω) : Ω → Rn (to be deﬁned precisely
later). Any randomness or uncertainty about X is
just a consequence of uncertainty regarding ω ∈ Ω.
As an example, X could relate to a some 1-year
extreme value, whose uncertainty comes from day

to day ﬂuctuations, or some fundamental stochas-
tic phenomenon represented by ω ∈ Ω. Examples
of natural sources of uncertainty are weather or
human actions in large scale. Therefore, whether
modeling weather, option prices, structural safety
at sea or traﬃc networks, stochastic models should
be used.

The probability of the event {X ∈ E},
for
some subset E ⊂ Rn, is really the probability of
{ω ∈ X−1(E)}. Technically, we need to ensure that
{ω ∈ X−1(E)} is something that we can compute
the probability of, and for this we need F. F is a
collection of subsets of Ω, and represents all possible
events (in the ”Ω-world”). When F is a σ-algebra 1
the pair (Ω, F) becomes a measurable space.

So, when we deﬁne X as a random variable taking
values in Rn, this means that there exists some mea-
surable space (Ω, F), such that any event {X ∈ E}
in the ”Rn-world” (which has its own σ-algebra)
has a corresponding event {ω ∈ X−1(E)} ∈ F in
the ”Ω-world”.
It also means that we can deﬁne
a probability measure on (Ω, F) that gives us the
probability of each event, but before we introduce
any speciﬁc probability measure, X will just be a
measurable function 2.

- We start with assuming that there exists some
measurable space (Ω, F) where X is a measur-
able function.

The natural way to make X into a random variable
is then to introduce some probability measure3 P
on F, giving us the probability space (Ω, F, P ).

- Given a probability measure P on (Ω, F) we
obtain the probability space (Ω, F, P ) on which
X is deﬁned as a random variable.

We have considered here, for familiarity, that X
takes values in Rn. When no measure and σ-algebra
is stated explicitly, one can assume that Rn is en-
dowed with the Lebesgue measure (which under-

1This means that 1) Ω ∈ F , 2) if S ∈ F then also the
complement Ω \ S ∈ F, and 3) if S1, S2, · · · is a countable
set of events then also the union S1 ∪ S2 ∪ . . . is in F . Note
that if these properties hold, many other types of events
(e.g. countable intersections) will have to be included as a
consequence.

2By deﬁnition, given two measure spaces (Ω, F ) and
(X, X ), the function X : Ω → X is measurable if and only if
X−1(A) ∈ F ∀A ∈ X .

3A function P : F → [0, 1] such that 1) P (Ω) = 1 and 2)
P (∪Ei) = (cid:80) P (Ei) for any countable collection of pairwise
disjoint events Ei.

5

lies the standard notion of length, area and vol-
ume etc.) and the Borel σ-algebra (the smallest σ-
algebra containing all open sets). Generally, X can
take values in any measurable space. For example,
X can map from Ω to a space of functions. This is
important in the study of stochastic processes.

2.3. The epistemic sub σ-algebra E

In the probability space (Ω, F, P ), recall that the
σ-algebra F contains all possible events. For any
random variable X deﬁned on (Ω, F, P ), the knowl-
edge that some event has occurred provides infor-
mation about X. This information may relate to
X in a way that it only aﬀects epistemic uncer-
tainty, only aleatory uncertainty, or both. We are
interested in specifying the events e ∈ F that are
associated with epistemic information alone. It is
the probability of these events we want to update
as new information is obtained. The collection E of
such sets is itself a σ-algebra, and we say that

E ⊆ F

(1)

is the sub σ-algebra of F representing epistemic in-
formation.

We illustrate this in the following examples. In
Example 2.3, we consider the simplest possible sce-
nario represented by the ﬂip of a biased coin, and
in Example 2.4 a familiar scenario from uncertainty
quantiﬁcation involving uncertainty with respect to
functions.

Example 2.3. (Coin ﬂip)
Deﬁne X = (Y, θ) as in Example 2.1, and let
Y ∈ {0, 1} denote the outcome of a coin ﬂip where
”heads” is represented by Y = 0 and ”tails” by
Y = 1. Assume that P (Y = 0) = θ for some ﬁxed
but unknown θ ∈ [0, 1]. For simplicity we assume
that θ can only take two values, θ ∈ {θ1, θ2} (e.g.
there are two coins but we do not know which one
is being used).

Then Ω = {0, 1} × {θ1, θ2}, F = 2Ω and E =

{∅, Ω, {(0, θ1), (1, θ1)}, {(0, θ2), (1, θ2)}}.

Example 2.4. (UQ)
Let X = (x, y) where x is an aleatory random vari-
able, and y is the result of a ﬁxed but unknown
function applied to x. We let y = ˆf (x) where ˆf
is a function-valued epistemic random variable.

If x is deﬁned on a probability space (Ωx, Fx, Px)
a
on
process
then (Ω, F, P ) can be deﬁned as

ˆf
and
(Ωf , Ff , Pf ),

stochastic

deﬁned

is

the product of
projection E = {Ωx × A | A ∈ Ff }.

the two spaces and E as the

In the following, we assume that the epistemic

sub σ-algebra E has been identiﬁed.

Given a random variable X, we say that X is E-
measurable if X is measurable as a function deﬁned
on (Ω, E). We say that X is independent of E, if the
conditional probability P (X|e) is equal to P (X) for
any event e ∈ E. With our deﬁnition of E, we then
have for any random variable X on (Ω, F, P ) that

- X is purely epistemic if and only if X is E-

measurable,

- X is purely aleatory if and only if X is inde-

pendent of E.

2.4. Epistemic conditioning

Let X be a random variable on (Ω, F, P ) that
may contain both epistemic and aleatory uncer-
tainty, and assume that the epistemic sub σ-algebra
E is given. By epistemic conditioning, we want to
update the epistemic part of the uncertainty in X
using some set of information I.
In Example 2.3
this means updating the probabilities P (θ = θ1)
and P (θ = θ2), and in Example 2.4 this means
updating Pf . In order to achieve this in the gen-
eral setting, we ﬁrst need a way to decouple epis-
temic and aleatory uncertainty. This can actually
be made fairly intuitive, if we rely on the following
assumption:

Assumption 2.5. There exists a random variable
θ : Ω → Θ that generates4 E.

If this generator θ exists, then for any ﬁxed value
θ ∈ Θ, we have that X|θ is independent of E. Hence
X|θ is purely aleatory and θ is purely epistemic.

We will call θ the epistemic generator, and we
can interpret θ as a signal that reveals all epis-
temic information when known. That is, if θ could
be observed, then knowing the value of θ would
remove all epistemic uncertainty from our model.
As it turns out, under fairly mild conditions one
can always assume existence of this generator. One
suﬃcient condition is that (Ω, F, P ) is a standard
probability space, and then the statement holds up

4There exists some measurable space (Θ, T ) and a F -
measurable function θ : Ω → Θ such that E = σ(θ), the
smallest σ-algebra containing all of the sets θ−1(T ) for T ∈
T .

6

to sets of measure zero. This is a technical re-
quirement to avoid pathological cases, and does not
provide any new intuition that we see immediately
useful, so we postpone further explanation to Ap-
pendix A.

Example 2.6. (Coin ﬂip – epistemic generator)
In the coin ﬂip example, the variable θ ∈ {θ1, θ2}
which generates E is already speciﬁed.

Example 2.7. (UQ – epistemic generator)
In this example, when (Ω, F, P ) is the product of an
aleatory space (Ωx, Fx, Px) and an epistemic space
(Ωf , Ff , Pf ), we could let θ : Ω = Ωx × Ωf → Ωf be
the projection θ(ωx, ωf ) = ωf .

Alternatively, given only the space (Ω, F, P )
where both x and ˆf are deﬁned, assume that ˆf is
a Gaussian process (or some other stochastic pro-
cess for which the Karhunen–Lo´eve theorem holds).
Then there exists a sequence of deterministic func-
tions φ1, φ2, . . . and an inﬁnite-dimensional vari-
able θ = (θ1, θ2, . . . ) such that ˆf (x) = (cid:80)∞
i=1 θiφi(x),
and we can let E be generated by θ.

The decoupling of epistemic and aleatory uncer-
tainty is then obtained by considering the joint vari-
able (X, θ) instead of X alone, because

P (X, θ) = P (X | θ)P (θ).

(2)

From (2) we see how the probability measure P be-
comes the product of the epistemic probability P (θ)
and the aleatory probability P (X|θ) when applied
to (X, θ).

Given new information, I, we will update our
beliefs about θ, P (θ) → P (θ|I), and we deﬁne the
epistemic conditioning as follows:

Pnew(X, θ) = P (X | θ)P (θ | I).

(3)

2.5. Two types of assumptions

Consider the probability space (Ω, F, P ), with
epistemic sub σ-algebra E. Here E represents epis-
temic information, which is the information asso-
ciated with assumptions. In other words, an epis-
temic event e ∈ E represents an assumption.
In
fact, given a class of assumptions, the following
Remark 2.8, shows why σ-algebras are appropriate
structures.

Remark 2.8. Let E be a collection of assumptions.
If e ∈ E, this means that it is possible to assume that
e is true. If it is also possible to assume that that

7

e is false, then. ¯e ∈ E as well. It may then also
be natural to require that e1, e2 ∈ E ⇒ e1 ∩ e2 ∈ E,
and so on. These are the deﬁning properties of a
σ-algebra.

For any random variable X deﬁned on (Ω, F, P ),
when E is a sub σ-algebra of F, X|e for e ∈ E is
well deﬁned, and represents the random variable
under the assumption e. In particular, given any
ﬁxed epistemic event e ∈ E we have a correspond-
ing aleatory distribution P (X|e) over X, and the
conditional P (X|E) is the random measure corre-
sponding to P (X|e) when e is a random epistemic
event in E. Here, the global probability measure P
when applied to e, P (e), is the belief that e is true.
In Section 2.4 we discussed updating the part of P
associated with epistemic uncertainty. We also in-
troduced the epistemic generator θ in order to asso-
ciate the event e with an outcome θ(e), and make
use of P (X|θ) in place of P (X|E). This provides
a more intuitive interpretation of the assumptions
that are measurable, i.e. those whose belief we may
specify through P .

Of course, the measure P is also based on as-
sumptions. For instance, if we in Example 2.1 as-
sume that Y follows a normal distribution. One
could in principle specify a (measurable) space of
probability distributions, from which the normal
distribution is one example. Otherwise, we view
the normality assumption as a structural assump-
tion related to the probabilistic model for X, i.e.
the measure P . These kinds of assumptions cannot
be treated the same way as assumptions related to
measurable events. For instance, the consequence
of the complement assumption ”Y does not follow
a normal distribution” is not well deﬁned.

In order to avoid any confusion, we split the as-

sumptions into two types:

1. The measurable assumptions represented by

the σ-algebra E, and

2. the set M of structural assumptions underlying

the probability measure P .

This motivates the following deﬁnition.

Deﬁnition 2.9 (Structural assumptions). We let
M denote the set of structural assumptions that de-
ﬁnes a probability measure on (Ω, F), which we may
write PM (·) or P (· | M ).

We may also refer to M as the non-measurable
assumptions, to emphasize that M contains all the

assumptions not covered by E. When there is no
risk of confusion we will also suppress the depen-
dency on M and just write P (·). Stating the set
M explicitly is typically only relevant for scenar-
ios where we consider changes being made to the
actual system that is being modelled, or for evalu-
ating diﬀerent candidate models, e.g. through the
marginal likelihood P (I|M ). In practice one would
also state M so that decision makers can determine
their level of trust in the probabilistic model, and
the appropriate level of caution when applying the
model.

As we will see in the upcoming section, making
changes to M and making changes to how PM acts
on events in E are the two main ways in which we
update a probabilistic digital twin.

3. The Probabilistic Digital Twin

The object that we will call probabilistic digital
twin, PDT for short, is a probabilistic model of a
physical system. It is essentially a (possibly degen-
erate) probability distribution of a vector X, rep-
resenting the relevant attributes of the system, but
where we in addition require the speciﬁcation of
epistemic uncertainty (assumptions) and how this
uncertainty may be updated given new information.
Before presenting the formal deﬁnition of a prob-
abilistic digital twin, we start with an example
showing why the identiﬁcation of epistemic uncer-
tainty is important.

3.1. Why distinguish between aleatory and epis-

temic uncertainty?

The decoupling of epistemic and aleatory uncer-
tainty (as described in Section 2.4) is central in the
PDT framework. There are two good reasons for
doing this:

1. We want to make decisions that are robust with

respect to epistemic uncertainty.

2. We want to study the eﬀect of gathering infor-

mation.

Item 1.

relates to the observation that deci-
sion theoretic approaches based on expectation may
if we marginalize out
not be robust. That is,
the epistemic uncertainty (and considering only
Eθ[P (X|θ)] = (cid:82) P (X|θ)dPθ). We give two exam-
ples of this below, see Example 3.1 and Example
3.2.

8

Item 2. means that by considering the eﬀect of
information on epistemic uncertainty, we can eval-
uate the value of gathering information. This is
discussed in further detail in Section 4.7.

Example 3.1. (Coin ﬂip – robust decisions)
Continuing from the coin ﬂip example (see Example
2.3), we let θ1 = 0.5, θ2 = 0.99. Assume that you
are given the option to guess the outcome of X.
If you guess correct, you collect a reward of R =
106 $, otherwise you have to pay L = 106 $. A
priori your belief about the bias of the coin is that
P (θ = 0.5) = P (θ = 0.99) = 0.5. If you consider
betting on X = 0, then the expected return, obtained
by marginalizing over θ, becomes P (θ = 0.5)(0.5R−
0.5L) + P (θ = 0.99)(0.99R − 0.01L) = 490.000$.

This is a scenario where decisions supported by
taking the expectation with respect to epistemic un-
certainty is not robust, as we believe that θ = 0.5
and θ = 0.99 are equally likely, and if θ = 0.5 we
will lose 106 $ 50% of the time by betting on X = 0.

Example 3.2. (UQ – robust decisions)
This example is a continuation of Example 2.4 and
Example 2.7.

In structural reliability analysis, we are dealing
with an unknown function g with the property that
the event {y = g(x) < 0} corresponds to failure.
When g is represented by a random function ˆg with
epistemic uncertainty, the failure probability is also
uncertain. Or in other words, if ˆg is epistemic then
ˆg is a function of the generator θ. Hence, the failure
probability is a function of θ. We want to make use
of a conservative estimate of the failure probability,
i.e., use a conservative value of θ. P (θ) tells us how
conservative a given value of θ is.

3.2. The attributes X

To deﬁne a PDT, we start by considering a vector
X consisting of the attributes of some system. This
means that X is a representation of the physical ob-
ject or asset that we are interested in. In general, X
describes the physical system. In addition, X must
contain attributes related to any type of informa-
tion that we want to make use of. For instance,
if the information consists of observations, the rel-
evant observable quantities, as well as attributes
related to measurement errors or noise, may be in-
cluded in X. In general, we will think of a model
of a system as a set of assumptions that describes
how the components of X are related or behave.

The canonical example here is where some physi-
cal quantity is inferred from observations including
errors and noise, in which case a model of the phys-
ical quantity (physical system) is connected with a
model of the data generating process (observational
system). We are interested in modelling dependen-
cies with associated uncertainty related to the com-
ponents of X, and treat X as a random variable.

The attributes X characterise the state of the
system and the processes that the PDT represents.
X may for instance include:

• System parameters representing quantities that
have a ﬁxed, but possibly uncertain, value. For
instance, these parameters may be related to
the system conﬁguration.

• System variables that may vary in time, and

which value may be uncertain.

• System events i.e., the occurrence of deﬁned

state transitions.

In risk analysis, one is often concerned with risk
measures given as quantiﬁed properties of X, usu-
ally in terms of expectations. For instance, if X con-
tains some extreme value (e.g. the 100-year wave)
or some speciﬁed event of failure (using a binary
variable), the expectations of these may be com-
pared against risk acceptance criteria to determine
compliance.

3.3. The PDT deﬁnition

Based on the concepts introduced so far, we de-

ﬁne the PDT as follows:

Deﬁnition 3.3 (Probabilistic Digital Twin). A
Probabilistic Digital Twin (PDT) is a triplet

(X, A, I)

the likelihood P (o|X, d) is well deﬁned. For brevity,
we will write this likelihood as P (I|X) when I con-
tains multiple events of this sort.

When M is understood, and there is no risk on
confusion, we will drop stating the dependency on
M explicitly and just refer to the probability space
(Ω, F, P ).

It is important to note that consistency between
I and P (X) is required. That is, when using the
probabilistic model for X, it should be possible to
simulate the type of observations given by I.
In
this case the likelihood P (I|X) is well deﬁned, and
the epistemic updating of X can be obtained from
Bayes’ theorem.

Finally, we note that with this setup the informa-
tion I may contain observations made under dif-
ferent conditions than what is currently speciﬁed
through M . The information I is generally deﬁned
as a set of events, given as pairs (d, o), where the
d encodes the relevant action leading to observing
o, as well as a description of the conditions under
which o was observed. Here d may relate to mod-
iﬁcations of the structural assumptions M , for in-
stance if the the causal relationships that describes
the model of X under observation of o is not the
same as what is currently represented by M . This
is the scenario when we perform controlled experi-
ments. Alternatively, (d, o) may represent a passive
observation, e.g. d = ”measurement taken from
sensor 1 at time 01:02:03”, o = 1.7 mm. We il-
lustrate this in the following example.

Example 3.4. (Parametric regression)
Let (x1, x2) denote two physical quantities where x2
depends on x1, and let (y, ε) represent an observable
quantity where y corresponds to observing x2 to-
gether with additive noise ε. Set X = (x1, x2, y, ε).

where X is a vector of attributes of a system,
A contains the assumptions needed to specify a
probabilistic model, and I contains information
regarding actions and observations:

x1

x2

y

ε

Figure 1: A standard regression model as a PDT.

A = ((Ω, F), E, M ), where (Ω, F) is a measure
space where X is measurable, and E is the sub
σ-algebra representing epistemic information. M
contains the structural assumptions that deﬁnes a
probability measure PM on (Ω, F).

I is a set consisting of events of the form (d, o),
where d encodes a description of the conditions un-
der which the observation o was made, and where

We deﬁne a model M corresponding to x1 ∼
px1 (x1|θ1), x2 = f (x1, θ2), y = x2 + ε and ε ∼ pε,
where px1 is a probability density depending on the
parameter θ1 and f (·, θ2) is a deterministic function
depending on the parameter θ2.

θ1 and θ2 are epistemic parameters for which we

deﬁne a joint density pθ.

Assume that I = {(d(1), o(1)), . . . , (d(n), o(n))}
is a set of controlled experiments, where d(i) =

9

(set x1 = x(i)
1 ) and o(i) is a corresponding obser-
vation of y|(x1 = x(i)
1 , ε = ε(i)) for a selected set of
inputs x(i)
1 , . . . , x(n)
and unknown i.i.d. ε(i) ∼ pε.
In this scenario, regression is performed by updating
the distribution pθ to agree with the observations:

1

pθ(θ|I) = pθ(θ1|θ2)pθ(θ2|I)

=

1
Z

(cid:32) n
(cid:89)

i=1

(cid:16)
o(i) − f (x(i)

1 , θ2)

pε

(cid:33)

(cid:17)

pθ(θ),

(4)

where Z is a constant ensuring that the updated
density integrates to one.

If instead I corresponds to direct observations,
d(i) = (observe y(i)), o(i) = y(i), then pθ(θ|I) cor-
responds to using x1 instead of x(i)
1 and multiplying
with px1 (x1|θ1) in (4).

Note that the scenario with controlled experi-
ments in Example 3.4 corresponds to a diﬀerent
model than the one in Figure 1. This is a famil-
iar scenario in the study of causal inference, where
actively setting the value of x1 is the do-operator
(see Pearl [11]) which breaks link between x1 and
x2.

3.4. Corroded pipeline example

To give a concrete example of a system where the
PDT framework is relevant, we consider the follow-
ing model from Agrell and Dahl [2]. This is based
on a probabilistic structural reliability model which
is recommended for engineering assessment of oﬀ-
shore pipelines with corrosion (DNV GL RP-F101
[12]).
It is a model of a physical failure mecha-
nism called pipeline burst, which may occur when
the pipeline’s ability to withstand the high inter-
nal pressure has been reduced as a consequence of
corrosion. We will describe just a general overview
of this model, and refer to [2, Example 4] for spe-
ciﬁc details regarding probability distributions etc.
Later, in Section 5.6, we will revisit this example
and make use of reinforcement learning to search
for an optimal way of updating the PDT.

Figure 2 shows a graphical representation of the
structural reliability model. Here, a steel pipeline
is characterised by the outer diameter D, the wall
thickness t and the ultimate tensile strength s. The
pipeline contains a rectangular shaped defect with
a given depth d and length l. Given a pipeline
(D, t, s) with a defect (d, l), we can determine the
pipeline’s pressure resistance capacity (the max-
imum diﬀerential pressure the pipeline can with-

stand before bursting). We let pFE denote the ca-
pacity coming from a Finite Element simulation of
the physical phenomenon. From the theoretical ca-
pacity pFE, we model the true pipeline capacity as
a function of pFE and Xm, where Xm is the model
discrepancy. For simplicity we have assumed that
Xm does not depend on the type of pipeline and de-
fect, and we will also assume that σm is ﬁxed, and
only the mean µm can be inferred from observa-
tions. Finally, given the pressure load pd, the limit
state representing the transition to failure is then
given as g = pc − pd, and the probability of failure
is deﬁned as P (g ≤ 0).

Pipeline

Defect

D

t

s

d

l

Model discrepancy

µm

σm

Xm

y
t
i
c
a
p
a
C

pFE

pc

Load

pd

g

Figure 2: Graphical representation of the corroded pipeline
structural reliability model. The shaded nodes d, pFE and
µm have associated epistemic uncertainty.

If we let X be the random vector containing all
of the nodes in Figure 2, then X represents a prob-
abilistic model of the physical system. In this ex-
ample, we want to model some of the uncertainty
related to the defect size, the model uncertainty,
and the capacity as epistemic. We assume that
the defect depth d has a ﬁxed but unknown value,
that can be inferred through observations that in-
clude noise. Similarly, the model uncertainty Xm
can be determined from experiments. Uncertainty
with respect to pFE comes from the fact that eval-
uating the true value of pFE|(D, t, s, d, l) involves
a time-consuming numerical computation. Hence,
pFE can only be known for a ﬁnite, and relatively
small set of input combinations. We can let ˆpFE
denote a stochastic process that models our uncer-
tainty about pFE. To construct a PDT from X we
will let ˆpFE take the place of pFE, and specify that
d, µm and ˆpFE are epistemic, i.e. E = σ(d, µm, ˆpFE).
If we want a way to update the epistemic uncer-

10

tainty based on observations, we also need to spec-
In this
ify the relevant data generating process.
example, we assume that there are three diﬀerent
ways of collecting data:

1. Defect measurement: We assume that noise
perturbed observations of the relative depth,
d/t + ε, can be made.

2. Computer experiment: Evaluate pFE at

some selected input (D, t, s, d, l).

3. Lab experiment: Obtain one observation of

Xm.

As the defect measurements requires speciﬁcation
of an additional random variable, we have to include
ε or (d/t)obs = d/t + ε in X as part of the complete
probabilistic model. This would then deﬁne a PDT
where epistemic updating is possible.

The physical system that the PDT represents
in this example is rarely viewed in isolation. For
instance, the random variables representing the
pipeline geometry and material are the result of
uncertainty or variations in how the pipeline has
been manufactured, installed and operated. And
the size of the defect is the result of a chemical pro-
cess, where scientiﬁc models are available. It could
therefore be natural to view the PDT from this
example as a component of a bigger PDT, where
probabilistic models of the manufacturing, operat-
ing conditions and corrosion process etc. are con-
nected. This form of modularity is often empha-
sized in the discussion of digital twins, and likewise
for the kind of Bayesian network type of models as
considered in this example.

4. Sequential decision making

We now consider how the PDT framework may
be adopted in real-world applications. As with any
statistical model of this form, the relevant type of
applications are related to prediction and inference.
Since the PDT is supposed to provide a one-to-one
correspondence (including uncertainty) with a real
physical system, we are interested in using the PDT
to understand the consequences of actions that we
have the option to make. In particular, we will con-
sider the discrete sequential decision making sce-
nario, where get the opportunity to make a decision,
receive information related to the consequences of
this decision, and use this to inform the next deci-
sion, and so on.

In this kind of scenario, we want to use the PDT
to determine an action or policy for how to act opti-
mally (with respect to some case-speciﬁc criterion).
By a policy here we mean the instructions for how
to select among multiple actions given the infor-
mation available at each discrete time step. We
describe this in more detail in Section 4.4 where we
discuss how the PDT is used for planning. When we
make use of the PDT in this way, we consider the
PDT as a ”mental model” of the real physical sys-
tem, which an agent uses to evaluate the potential
consequences of actions. The agent then decides on
some action to make, observes the outcome, and
updates her beliefs about the true system, as illus-
trated in Figure 3.

PDT

Apply

p

olic

y

Simulate
and
plan

(cid:140)

∆M
∆I

el
d
o
m

e
t
a
d

p

U

(cid:140)

i n f o r m ation

∆M
∆I

Co l l e c

t

Figure 3: A PDT as a mental model of an agent taking ac-
tions in the real world. As new experience is gained, the PDT
may be updated by changing the structural assumptions M
that deﬁned the probability measure P , or updating belief
with respect to epistemic events through conditioning on the
new set of information I. The changes in structural assump-
tions and epistemic information are represented by ∆M and
∆I respectively. As part of the planning process, the PDT
may simulate possible scenarios as indicated by the inner
circle.

Whether the agent applies a policy or just an
action (the ﬁrst in the policy) before collecting in-
formation and updating the probabilistic model de-
pends on the type of application at hand. In general
it is better to update the model as often as possi-
ble, preferably between each action, but the actual
computational time needed to perform this updat-
ing might make it impossible to achieve in practice.

11

4.1. Mathematical framework of sequential decision

making

In this section, we brieﬂy recap the mathematical
framework of stochastic, sequential decision making
in discrete time. We ﬁrst recall the general frame-
work, and in the following Section 4.2, we show how
this relates to our deﬁnition of a PDT.

Let t = 0, 1, 2, . . . , N − 1 and consider a discrete
time system where the state of the system, {xt}t≥1,
is given by

xt+1 = ft(xt, ut, wt),

t = 0, 1, 2, . . . , N − 1.

(5)

Here, xt is the state of the system at time t, ut
is a control and wt is a noise, or random parameter
at time t. Note that the control, ut, is a decision
which can be made by an agent (the controller ) at
time t. This control is to be chosen from a set of ad-
missible controls At (possibly, but not necessarily
depending on time). Also, ft, t = 0, 1, 2, . . . , N − 1
are functions mapping from the space of state vari-
ables (state space), controls and noise into the set
of possible states of {xt}t≥0. The precise structure
of the state space, set of admissible controls and the
random parameter space depends on the particular
problem under consideration. Note that due to the
randomness in wt, t = 0, 1, 2, . . . , N − 1, the sys-
tem state xt and control ut, t = 1, 2, . . . , N − 1 also
become random variables.

We remark that because of this, the state equa-

tion is sometimes written in the following form,

xt+1(ω) = ft(xt(ω), ut(ω), ω)

(6)

where ω ∈ Ω is a scenario in a scenario space Ω (rep-
resenting the randomness). Sometimes, the ran-
domness is suppressed for notational convenience,
so the state equation becomes xt+1 = ft(xt, ut),
t = 0, 1, 2, . . . , N − 1.

Note that in the state equation (5) (alternatively,
equation (6)), xt+1 only depends on the previ-
ous time step, i.e., xt, ut, wt. This is the Markov
property (as long as we assume that the distribu-
tion of wt does not depend on past values of ws,
s = 0, 1, . . . t − 1, only xt, ut). That is, the next
system state only depends on the previous one.
Since this Markovian framework is what will be
used throughout this paper as we move on to rein-
forcement learning for a probabilistic digital twin,
we focus on this. However, we remark that there is
a large theory of sequential decision making which
is not based on this Markovianity. This theory is

12

based around maximum principles instead of dy-
namic programming, see the following Section 4.3
for more on this.

The aim of the agent is to minimize a cost func-
tion under the state constraint (5) (or alternatively,
(6)). We assume that this cost function is of the fol-
lowing, additive form,

E(cid:2)g(xN ) +

N −1
(cid:88)

t=0

ht(xt, ut, wt)(cid:3)

(7)

where the expectation is taken with respect to an a
priori given probability measure. That is, we sum
over all instantaneous rewards ht(xt, ut, wt), t =
0, 1, . . . , N − 1 which depend on the state of the
system, the control and the randomness and add a
terminal reward g(xN ) which only depends on the
system state at the terminal time t = N . This
function is called the objective function.

Hence, the stochastic sequential decision making
problem of the agent is to choose admissible con-
trols ut, t = 0, 1, 2, . . . , N − 1 in order to,

min
ut∈At,t≥0

E(cid:2)g(xN ) +

N −1
(cid:88)

t=0

ht(xt, ut, wt)(cid:3)

(8)

such that

xt+1 = ft(xt, ut, wt),

t = 0, 1, 2, . . . , N − 1.

Typically, we assume that the agent has full in-
formation in the sense that they can choose the con-
trol at time t based on (fully) observing the state
process up until this time, but that they are not
able to use any more information than this (future
information, such as inside information).

This problem formulation is very similar to that
of continuous time stochastic optimal control prob-
lem.

Remark 4.1. (A note on continuous time) This
framework is parallel to that of stochastic optimal
control in continuous time. The main diﬀerences in
the framework in the continuous time case is that
the state equation is typically a stochastic diﬀeren-
tial equation, and the sum is replaced by an integral
in the objective function. For a detailed introduc-
tion to continuous time control, see e.g., Øksendal
[13].

Other versions of sequential decision making
problems include inside information optimal con-
inﬁnite
trol, partial information optimal control,

time horizon optimal control and control with var-
ious delay and memory eﬀects. One can also con-
sider problems where further constraints, either on
the control or the state, is added to problem (8).

In Bertsekas [14], the sequential decision making
problem (8) is studied via the dynamic program-
ming algorithm. This algorithm is based on the
Bellman optimality principle, which says that an
optimal policy chosen at some initial time, must
be optimal when the problem is re-solved at a
later stage given the state resulting from the ini-
tial choice.

4.2. Sequential decision making in the PDT

Now, we show how the sequential decision mak-
ing framework from the previous section can be
used to solve sequential decision making problems
in the PDT.

We may apply this sequential decision making

framework to our PDT by letting

xt := Xt.

That is, the state process for the PDT sequential
decision making problem is the random vector of
attributes Xt. Note that in Deﬁnition 3.3, there
is no time-dependency in the attributes X. How-
ever, since we are interested in considering sequen-
tial decision making in the PDT, we need to assume
that there is some sort of development over time (or
some indexed set, e.g. information) of the PDT.

Hence, the stochastic sequential decision making
problem of the PDT-agent is to choose admissible
controls ut, t = 0, 1, 2, . . . , N − 1 in order to,

min
ut∈At,t≥0

E(cid:2)g(XN ) +

N −1
(cid:88)

t=0

ht(Xt, ut, wt)(cid:3)

(9)

such that

Xt+1 = ft(Xt, ut, wt),

t = 0, 1, 2, . . . , N − 1.

Here, the set of admissible controls, {ut}t≥0 ∈ A,
are problem speciﬁc. So are the functions ht, gt
and ft for t ≥ 0. Given a particular problem, these
functions are deﬁned based on the goal of the PDT-
agent as well as the updating of the PDT given new
input.

4.3. Two solution methods for sequential decision

making

In the literature on (discrete time) stochastic se-
quential decision making, there are two main ap-
proaches:

13

• The dynamic programming principle (DPP).

• The Pontyagrin maximum principle (MP).

The continuous time analogues are the Hamilton-
Jacobi-Bellman equations (a particular kind of par-
tial diﬀerential equation) and the stochastic maxi-
mum principle, respectively.

The DPP is based on the Bellman optimality
principle, and the resulting Bellman equation which
can be derived from this. Dynamic programming
has a few important advantages. It is tractable from
an algorithmic perspective because of the backprop-
agation algorithm naturally resulting from the Bell-
man equation. Furthermore, the method is always
well deﬁned since it is based on working with the
value function (the function that maps states to the
optimal value given by (9), assuming that we start
from the given state). However, there are some
downsides to the DPP method as well. Firstly, DPP
requires a Markovian framework (or that we can
transform the problem to a Markovian framework).
Also, the DPP requires that the Bellman equation
holds. This may not be the case if we have problems
with for example non-exponential discounting (with
respect to time). In this case, we say that there are
problems with time-inconsistency, see e.g., Rudloﬀ
[15]. For instance, traditional risk measures
et al.
such as value-at-risk (VaR) and conditional-value at
risk (CVaR) are time-inconsistent in this sense, see
Cheridito and Stadje [16], and Artzner et al.
[17]
respectively. Hence, we run into time-inconsistency
issues when e.g., minimizing the conditional-value-
at-risk of some ﬁnancial position, if we are using
the DPP method. Finally, we cannot have state
constraints when using the DPP method, since this
causes discontinuity of the value function, see Hao
and Li [18].

The alternative approach to solving stochastic se-
quential decision making problems is via the Pon-
tryagin maximum principle. This method does not
require Markovianity or depend on Bellman equa-
tion. Hence, there are no problems with time-
inconsistency. However, the MP approach is less
tractable from an algorithmic point of view. Fur-
thermore, the MP approach requires existence of
a minimizing (or maximizing) control. This may
not be the case, since it is possible that only lim-
iting control processes converging to the minimum
(maximum) exist.

The pros and cons of dynamic programming and
the maximum principle approach carry over in con-
tinuous time.

From a computational point of view, the dynamic
programming method suﬀers from the curse of di-
mensionality. When doing numerical backward in-
duction in the DPP, the objective function must
be computed for each combination of values. This
makes the method too computationally demanding
to be applicable in practice for problems where the
state space is large, see Agrell and Dahl [2] for a
discussion of this. Until recently, numerical algo-
rithms based on the maximum principle were not
frequently studied in the literature, an exception is
Bonnans [19]. However, the MP approach leads to
systems of backward diﬀerential equations, in the
continuous case, which are often computationally
demanding and also less tractable from an algorith-
mic point of view than the DPP method. However,
with the advances of machine learning over the past
decade, some new approaches based on the MP ap-
proach using deep learning have been introduced,
see Li et al. [20].

Actually, reinforcement learning (RL) is essen-
tially the DPP method. Hence, RL algorithms also
suﬀer from the curse of dimensionality, see Sutton
and Barto [21]. This means that most RL algo-
rithms become less eﬃcient when the dimension of
the state space increases. However, by using func-
tion approximation the curse of dimensionality can
often be eﬃciently handled, see Arulkumaran et al.
[22].

The purpose of this paper is to build on this lit-
erature by connecting deep reinforcement learning
(so essentially, the dynamic programming method)
to probabilistic digital twins in order to do planning
with respect to the PDT. This is the topic of the
following section.

4.4. Planning in the PDT

In this section, we discuss how the PDT can be
used for planning. That is, how we use the PDT
to identify an optimal policy, without acting in the
real world, but by instead simulating what will hap-
pen in the real world given that the agent chooses
speciﬁc actions (or controls, as they are called in
the sequential decision making literature, see Sec-
tion 4.1). We use the PDT as a tool to ﬁnd a plan
(policy), or a single action (ﬁrst action of policy),
to perform in the real world.

In order to solve our sequential decision making
problem in the PDT, we have chosen to use a re-
inforcement learning formulation. As remarked in
Section 4.3, this essentially corresponds to choos-
ing the dynamic programming method for solving

14

the optimal control problem (as opposed to a max-
imum principle approach). Because we will use a
DPP approach, we need all the assumptions that
come with this, see the discussion in Section 4.3: A
Markovian framework, or the possibility of trans-
forming the problem to something Markovian. We
need the Bellman equation to hold in order to avoid
issues with time-inconsistency. In order to ensure
this, we for example need to use exponential dis-
counting and not have e.g., conditional expectation
of state process in a non-linear way in the objec-
tive function. Finally, our planning problem cannot
have state constraints.

Remark 4.2. Instead of using the DPP to solve the
planning problem, we could use a maximum prin-
ciple approach. One possible way of doing this in
practice, is by using one of the MP based algorithms
[20], instead of using reinforce-
found in Li et al.
ment learning. By this alternative approach, we
avoid the Markovianity requirement, possible time-
inconsistency issues and can allow for state con-
straints (via a Lagrange multiplier method - see
e.g., Dahl and Stokkereit [23]). This topic is be-
yond the scope of this paper, but is a current work
in progress.

Starting with an initial PDT as a digital represen-
tation of a physical system given our current knowl-
edge, we assume that there are two ways to update
the PDT:

1. Changing or updating the structural assump-
tions M , and hence the probability measure
PM .

2. Updating the information I.

The structural assumptions M are related to the
probabilistic model for X. Recall from Section 2.4,
that these assumptions deﬁne the probability mea-
sure PM . Often, this probability measure is taken
as given in stochastic modeling. However, in prac-
tice, probability measures are not given to us, but
decided by analysts based on previous knowledge.
Hence, the structural assumptions M may be up-
dated because of new knowledge, external to the
model, or for other reasons the analysts view as
important.

Updating the information is our main concern in
this paper, since this is related to the agent making
costly decisions in order to gather more informa-
tion. An update of the information also means (po-
tentially) reducing the epistemic uncertainty in the

PDT. Optimal information gathering in the PDT
will be discussed in detail in the following Section
4.7.

4.5. MDP, POMDP and its relation to DPP

In this section, we brieﬂy recall the deﬁnitions
of Markov decision processes, partially observable
Markov decision processes and explain how these
relate to the seuqntial decision making framework
of Section 4.1.

Markov decision processes (MDP) are discrete-
time stochastic control processes of a speciﬁc form.
An MDP is a tuple

(S, A, Pa, Ra),

where S is a set of states (the state space) and A is
a set of actions (action space). Also,

Pa(s, s(cid:48)) = Pa(st+1 = s(cid:48) | at = a, st = s)

is the probability of going from state s at time t
to state s(cid:48) at time t + 1 if we do action a at time
t. Finally, Ra(s, s(cid:48)) is the instantaneous reward of
transitioning from state s at time t to state s(cid:48) at
time t + 1 by doing action a (at time t).

An MDP satisﬁes the Markov property, so given
that the process is in state s and will be doing a at
time t, the next state st+1 is conditionally indepen-
dent of all other previous states and actions.

Remark 4.3. (MDP and DPP)
Note that this deﬁnition of an MDP is essentially
the same as our DPP framework of Section 4.1. In
the MDP notation, we say actions, while in the con-
trol notation, it is common to use the word control.
In Section 4.1, we talked about instantaneous cost
functions, but here we talk about instantaneous re-
wards. Since minimization and maximization prob-
lems are equivalent (since inf{·} = − sup{−·}), so
are these two concepts. Furthermore, the deﬁni-
tion of the transition probabilities Pa in the MDP
framework corresponding to the Markov assumption
of the DPP method. In both frameworks, we talk
about the system states, though in the DPP frame-
work we model this directly via equation (5).

A generalization of MDP are partially observable
Markov decision processes (POMDPs). While an
MDP is a 4-tuple, a POMDP is a 6-tuple,

(S, A, Pa, Ra, ¯Ω, O).

15

Here (like before), S is the state space, A is the ac-
tion space, Pa give the conditional transition prob-
abilities between the diﬀerent states in S and Ra
give the instantaneous rewards of the transitions
for a particular action a.

In addition, we have ¯Ω, which is a set of obser-
vations. In contrast to the MDP framework, with
POMDP, the agent no longer observes the state s
directly, but only an observation o ∈ ¯Ω. Further-
more, the agent knows O which is a set of condi-
tional observation probabilities. That is,

O(o | s(cid:48), a)
is the probability of observing o ∈ ¯Ω given that we
do action a from state s(cid:48).

The objective of the agent in the POMDP se-
quential decision problem is to choose a policy, that
is actions at each time, in order to

E(cid:2)

max
{at}∈A

T
(cid:88)

t=0

(cid:3)

λtrt

(10)

where rt is the reward earned at time t (depend-
ing on st, at and st+1), and λ ∈ [0, 1] is a number
called the discount factor. The discount factor can
be used to introduce a preference for immediate re-
wards as opposed to more distant rewards, which
may be relevant for the problem at hand, or used
just for numerical eﬃciency. Hence, the agent aims
to maximize their expected discounted reward over
all future times. Note that is it also possible to con-
sider problem (10) over an inﬁnite time horizon or
with a separate terminal reward function as well.
This is similar to the DPP sequential decision mak-
ing framework of Section 4.1.

In order to solve a POMDP, it is necessary to in-
clude memory of past actions and observations. Ac-
tually, the inclusion of partial observations means
that the problem is no longer Markovian. How-
ever, there is a way to Markovianize the POMDP by
transforming the POMDP into a belief-state MDP.
In this case, the agent summarizes all information
about the past in a belief vector b(t), which is up-
dated as time passes. See [24], Chapter 12.2.3 for
details.

4.6. MDP (and POMDP) in the PDT framework

In this section, we show how the probabilistic dig-
ital twin can be incorporated in a reinforcement
learning framework, in order to solve sequential de-
cision problems in the PDT.

In Section 4.2, we showed how we can use
the mathematical framework of sequential decision
making to solve optimal control problems for a
PDT-agent. Also, in Section 4.5, we saw (in Re-
mark 4.3) that the MDP (or POMDP in general)
framework essentially corresponds to that of the
DPP. In theory, we could use the sequential deci-
sion making framework and the DPP to solve opti-
mal control problems in the PDT. However, due to
the curse of dimensionality, this will typically not
be practically tractable (see Section 4.3). In order
to resolve this, we cast the PDT sequential decision
making problem into a reinforcement learning, in
particular a MDP, framework. This will enable us
to solve the PDT optimal control problem via deep
reinforcement learning, in which there are suitable
tools to overcome the curse of dimensionality.

To deﬁne a decision making process in the PDT
as a MDP, we need to determine our state space,
action space, (Markovian) transition probabilities
and a reward function.

• The action space A: These are the possible ac-
tions within the PDT. These may depend on
the problem at hand. In the next Section 4.7,
we will discuss optimal information gathering,
where the agent can choose between diﬀerent
types of experiments, at diﬀerent costs, in or-
der to gain more information. In this case, the
action space is the set of possible decisions that
the agent can choose between in order to attain
more information.

• The state space S: We deﬁne a state as a
PDT (or equivalently a version of a PDT that
evolves in discrete time t = 0, 1, . . . ). A PDT
represents our belief about the current phys-
ical state of a system, and it is deﬁned by
some initial assumptions together with the in-
formation acquired through time. In practice,
if the structural assumptions are not changed,
we may let the information available at the cur-
rent time represent a state.

This means that our MDP will consist of belief-
states, represented by information, from which
inference about the true physical state can be
made. This is a standard way of creating a
MDP from a POMDP, so we can view the PDT
state-space as a space of beliefs about some un-
derlying partially observable physical state.

Starting from a PDT, we deﬁne the state space
as all updated PDTs we can reach by taking

16

actions in the action space A.

• The transition probabilities Pa: Based on our
chosen deﬁnition of the state space, the transi-
tion probabilities are the probabilities of going
from one level of information to another, given
the action chosen by the agent. For example, if
the agent chooses to make decision (action) d,
what is the probability of going from the cur-
rent level of information to another (equal or
better) level. This is given by epistemic condi-
tioning of the PDT with respect to the given
information set I = {(d, o)} based on the deci-
sions d the new observation o. When it comes
to updates of the structural assumptions M ,
we consider this as deterministic transitions.

• The reward Ra: The reward function, or equiv-
alently, cost function, will depend on the spe-
ciﬁc problem at hand. To each action a ∈ A,
we assume that we have an associated reward
Ra.
In the numerical examples in Section 5,
we give speciﬁc examples of how these rewards
can be deﬁned.

As mentioned in Section 4.4, there are two ways
to update the PDT: Updating the structural as-
sumptions M and updating the information I. If
we update the PDT by (only) adding to the infor-
mation set I, we always have the Markov property.
If we also update M , then the preservation of the
Markov property is not given. In this case, using a
maximum principle deep learning algorithm instead
of the DPP based deep RL is a possibility, see [20].

Remark 4.4. Note that in the case where we have
a very simple PDT with only discrete variables and
only a few actions, then the RL approach is not
necessary. In this case, the DPP method as done in
traditional optimal control works well, and we can
apply a planning algorithm to the PDT in order to
derive an optimal policy. However, in general, the
state-action space of the PDT will be too large for
this. Hence, traditional planning algorithms, and
even regular RL may not be feasible due to the curse
of dimensionality. In this paper, we will consider
deep reinforcement learning as an approach to deal
with this. We discuss this further in Section 5.

Note that what determines an optimal action or
policy will of course depend on what objective the
outcomes are measured against. That is, what do
we want to achieve in the real world? There are

many diﬀerent objectives we could consider. In the
following we present one generic objective related
to optimal information gathering, where the PDT
framework is suitable.

4.7. Optimal information gathering

A generic, but relevant, objective in optimal se-
quential decision making is simply to ”improve it-
self”. That is, to reduce epistemic uncertainty with
respect to some quantity of interest. Another op-
tion, is to consider maximizing the Kullback-Leibler
divergence with respect to epistemic uncertainty as
a general objective. This would mean that we aim
to collect the information that ”will surprise us the
most”.

By deﬁnition, a PDT contains an observational
model related to the data generating process (the
epistemic conditioning relies on this). This means
that we can simulate the eﬀect of gathering infor-
mation, and we can study how to do this optimally.
In order to deﬁne what we mean by an optimal
strategy for gathering information, we then have to
specify the following,

• Objective: What we need the information for.
For example, what kind of decision do we in-
tend to support using the PDT? Is it something
we want to estimate? What is the required ac-
curacy needed? For instance, we might want
to reduce epistemic uncertainty with respect
to some quantity, e.g., a risk metric such as
a failure probability, expected extreme values
etc.

• Cost:

The cost

related to the relevant

information-gathering activities.

Then, from the PDT together with a speciﬁed
objective and cost, one alternative is to deﬁne the
optimal strategy as the strategy that minimizes
the (discounted) expected cost needed to achieve
the objective (or equivalently achieves the objective
while maximizing reward).

Example 4.5. (Coin ﬂip – information gathering)
Continuing from Example 3.1, imagine that before
making your ﬁnal bet, you can ﬂip the coin as many
times as you like in order to learn about θ. Each of
these test ﬂips will cost 10.000 $. You also get the
opportunity to replace the coin with a new one, at
the cost of 100.000 $.

An interesting problem is now how to select an
optimal strategy for when to test, bet or replace in

17

this game. And will such a strategy be robust? What
if there is a limit on the total number of actions
than can be performed? In Section 5.5 we illustrate
how reinforcement learning can be applied to study
this problem, where the coin represents a component
with reliability θ, that we may test, use or replace.

5. Deep Reinforcement Learning with PDTs

In this section we give an example of how rein-
forcement learning can be used for planning, i.e.
ﬁnding an optimal action or policy, with a PDT.
The reinforcement learning paradigm is especially
relevant for problems where the state and/or action
space is large, or dynamical models where speciﬁc
transition probabilities are not easily attainable but
where eﬃcient sampling is still feasible. In proba-
bilistic modelling of complex physical phenomena,
we often ﬁnd ourselves in this kind of setting.

5.1. Reinforcement Learning (RL)

Reinforcement learning, in short, aims to opti-
mize sequential decision problems through sampling
from a MDP (Sutton and Barto [21]). We think of
this as an agent taking actions within an environ-
ment, following some policy π(a|s), which gives the
probability of taking action a if the agent is cur-
rently at state s. Generally, π(a|s) represents a
(possibly degenerate) probability distribution over
actions a ∈ A for each s ∈ S. The agent’s objective
is to maximize the amount of reward it receives over
time, and a policy π that achieves this is called an
optimal policy.

Given a policy π we can deﬁne the value of a

state s ∈ S as

vπ(s) = E(cid:2)

T
(cid:88)

t=0

λtrt | s0 = s(cid:3)

(11)

where rt is the reward earned at time t (depend-
ing on st, at and st+1), given that the agent fol-
lows policy π starting from s0 = s. That is,
for Pa and Ra given by the MDP, at ∼ π(at|st),
st+1 ∼ Pat(st, st+1) and rt ∼ Rat(st, st+1). Here
we make use of a discount factor λ ∈ [0, 1] in the
deﬁnition of cumulative reward. If we want to con-
sider T = ∞ (continuing tasks) instead of T < ∞
(episodic task), then λ < 1 is generally necessary.

The optimal value function is deﬁned as the one
that maximises (11) over all policies π. The opti-
mal action at each state s ∈ S then corresponds to

acting greedily with respect to this value function,
i.e. selecting the action at that in expectation max-
imises the value of st+1. Likewise, it is common to
deﬁne the action-value function qπ(s, a), which cor-
responds to the expected cumulative return of ﬁrst
taking action a in state s and following π there-
after. RL generally involves some form of Monte
Carlo simulation, where a large number of episodes
are sampled from the MDP, with the goal of esti-
mating or approximating the optimal value of sates,
state-action pairs, or an optimal policy directly.

Theoretically this is essentially equivalent to the
DPP framework, but with RL we are mostly con-
cerned with problems where optimal solutions can-
not be found and some form of approximation is
needed. By the use of ﬂexible approximation meth-
ods combined with adaptive sampling strategies,
RL makes it possible to deal with large and complex
state- and action spaces.

5.2. Function approximation

One way of using function approximation in RL
is to deﬁne a parametric function ˆv(s, w) ≈ vπ(s),
given by a set of weights w ∈ Rd, and try to learn
the value function of an optimal policy by ﬁnd-
ing an appropriate value for w. Alternatively, we
could approximate the value of a state-action pair,
ˆq(s, a, w) ≈ qπ(s, a), or a policy ˆπ(a|s, w) ≈ π(a|s).
The general goal is then to optimize w, using data
generated by sampling from the MDP, and the RL
literature contains many diﬀerent algorithms de-
signed for this purpose. In the case where a neural
network is used for function approximation, it is of-
ten referred to as deep reinforcement learning. One
alternative, which we will make use of in an example
later on, is the deep Q-learning (DQN) approach as
introduced by van Hasselt et al. [25], which repre-
sents the value of a set of m actions at a state s
using a multi-layered neural network

ˆq(s, w) : S → Rm.

(12)

Note here that ˆq(s, w) is a function deﬁned on the
state space S. In general, any approximation of the
value functions v or q, or the policy π are deﬁned on
S or S × A. A question that then arises, is how can
we deﬁne parametric functions on the state space S
when we are dealing with PDTs? We can assume
that we have control over the set of admissible ac-
tions A, in the sense that this is something we de-
ﬁne, and creating parametric functions deﬁned on
A should not be a problem. But as discussed in
Section 4.6, S will consist of belief -states.

5.3. Deﬁning the state space

We are interested in an MDP where the transi-
tion probabilities Pa(s, s(cid:48)) corresponds to updating
a PDT as a consequence of action a. In that sense,
s and s(cid:48) are PDTs. Given a well-deﬁned set of ad-
missible actions, the state space S is then the set of
all PDTs that can be obtained starting from some
initial state s0, within some deﬁned horizon.

Recall that going from s to s(cid:48) then means keep-
ing track of any changes made to the structural as-
sumptions M and the information I, as illustrated
in Figure 3. From now on, we will for simplicity
assume that updating the PDT only involves epis-
temic conditioning with respect to the information
I. This is a rather generic situation. Also, ﬁnding
a way to represent changes in M will have to be
handled for the speciﬁc use case under considera-
tion. Assuming some initial PDT s0 is given, any
state st at a later time t is then uniquely deﬁned by
the set of information It available at time t. Rep-
resenting states by information in this way is some-
thing that is often done to transform a POMDP to
a MDP. That is, although the true state st at time
t is unknown in a POMDP, the information It, and
consequently our belief about st, is always know at
time t. Inspired by the POMDP terminology, we
may therefore view a PDT as a belief-state, which
seems natural as the PDT is essentially a way to
encode our beliefs about some real physical system.
Hence, we will proceed with deﬁning the state
space S as the information state-space, which is the
set of all sets of information I. Although this is
a very generic approach, we will show that there
is a way of deﬁning a ﬂexible parametric class of
functions on S. But we must emphasize that that if
there are other ways of compressing the information
I, for instance due to conjugacy in the epistemic
updating, then this is probably much more eﬃcient.
Example 5.1 below shows exactly what we mean by
this.

Example 5.1. (Coin ﬂip – information state-
space) In the coin ﬂip example (Example 2.3), all
of our belief with respect to epistemic uncertainty is
represented by the number ψ = P (θ = θ1). Given
some observation Y = y ∈ {0, 1}, the epistemic
conditioning corresponds to

ψ →

β1(y)ψ
β1(y)ψ + β2(y)(1 − ψ)

,

where, for j = 1, 2, βj(y) = θj if y = 0 and βj(y) =
1 − θj if y = 1.

18

In this example, the information state-space con-
sists of all sets of the form It = {y1, . . . , yt} where
each yi is binary. However, if the goal is to let It be
the representation of a PDT, we could just as well
use ψt, i.e. deﬁne S = [0, 1] as the state space. Al-
ternatively, the number of heads and tails (0s and
1s) provides the same information, so we could also
make use of S = {0, . . . , N } × {0, . . . , N } where N
is an upper limit on the total number of ﬂips we
consider.

5.4. Deep learning on the information state-space
Let S be a set of sets I ⊂ Rd. We will assume
that each set I ∈ S consists of a ﬁnite number of
elements y ∈ Rd, but we do not require that all sets
I have the same size. We are interested in functions
deﬁned on S.

An important property of any function f that
takes a set I as input, is permutation invariance.
I.e.
f ({y1, . . . , yN }) = f ({yκ(1), . . . , yκ(N )}) for
any permutation κ. It can been shown that under
fairly mild assumptions, that such functions have
the following decomposition

f (I) = ρ



φ(y)

 .



(cid:88)



y∈I

(13)

These sum decompositions were studied by Zaheer
et al. [26] and later by Wagstaﬀ et al. [27], which
showed that if |I| ≤ p for all I ∈ S, then any
continuous function f : S → R can be written as
(13) for some suitable functions φ : Rd → Rp and
ρ : Rp → R. The motivation in [26, 27] was to en-
able supervised learning of permutation invariant
and set-valued functions, by replacing ρ and φ with
ﬂexible function approximators, such as Gaussian
processes or neural networks. Other forms of de-
composition, by replacing the summation in (13)
with something else that can be learned, has also
been considered by Soelch et al.
[28]. For rein-
forcement learning, we will make use of the form
(13) to represent functions deﬁned on the informa-
tion states space S, such as ˆv(s, w), ˆq(s, a, w), or
ˆπ(a|s, w), using a neural network with parameter
w. In the remaining part of this paper we present
two examples showing how this works in practice.

by introducing a game where the player has to se-
lect whether to bet on, test or replace the coin. As a
simple illustration we will show how reinforcement
learning can be applied in this setting.

But now, we will imagine that the coin Y repre-
sents a component in some physical system, where
Y = 0 corresponds to the component functioning
and Y = 1 represents failure. The probability
P (Y = 1) = 1 − θ is then the components failure
probability, and we say that θ is the reliability.

For simplicity we assume that θ ∈ {0.5, 0.99},
and that our initial belief is P (θ = 0.5) = 0.5. That
is, when we buy a new component, there is a 50 %
chance of getting a ”bad” component (that fails 50
% of the time), and consequently a 50 % probability
of getting a ”good” component (that fails 1 % of the
time).

We consider a project going over N = 10 days.
Each day we will decide between one of the follow-
ing 4 actions:

1. Test the component (ﬂip the coin once). Cost

r = −10.000$.

2. Replace the component (buy a new coin). Cost

r = −100.000$.

3. Use the component (bet on the outcome). Ob-
tain a reward of r = 106$ if the component
works (Y = 0) and a cost of r = −106$ if the
component fails (Y = 1).

4. Terminate the project (set t = N ), r = 0.

We will ﬁnd a deterministic policy π : S → A
that maps from the information state-space to one
of the four actions. The information state-space S
is here represented by the number of days left of the
project, n = N − t, and the set It of observations
of the component that is currently in use at time
t.
If we let SY contain all sets of the form I =
{Y1, . . . , Yt}, for Yt ∈ {0, 1} and t < N , then

S = SY × {1, . . . , N }

(14)

represents the information state-space. In this ex-
ample we made use of the deep Q-learning (DQN)
approach described by van Hasselt et al. [25], where
we deﬁne a neural network

ˆq(s, w) : S → R4,

5.5. The ”coin ﬂip” example

Throughout this paper we have presented a se-
ries of small examples involving a biased coin, rep-
resented by X = (Y, θ). In Example 4.5 we ended

that represents the action-value of each of the four
actions. The optimal policy is then obtained by at
each state s selecting the action corresponding to
the maximal component of ˆq.

19

We start by ﬁnding a policy that optimizes the
cumulative reward over the 10 days (without dis-
counting). As it turns out, this policy prefers to
”gamble” that the component works rather than
performing tests.
In the case where the starting
component is reliable (which happens 50 % of the
time), a high reward can be obtained by selection
action 3 at every opportunity. The general ”idea”
with this policy, is that if action 3 results in failure,
the following action is to replace the component (ac-
tion 2), unless there are few days left of the project
in which case action 0 is selected. We call this the
”unconstrained” policy.

Although the unconstrained policy givens the
largest expected reward, there is an approximately
50 % chance that it will produce a failure, i.e. that
action 3 is selected with Y = 1 as the resulting
outcome. One way to reduce this failure probabil-
ity, is to introduce the constraint that action 3 (us-
ing the component) is not allowed unless we have
a certain level of conﬁdence in that the component
is reliable. We introduced this type of constraint
by requiring that P (θ = 0.99) > 0.9 (a constraint
on epistemic uncertainty). The optimal policy un-
der this constraint will start with running exper-
iments (action 1), before deciding whether to re-
place (action 2), use the component (action 3), or
terminate the project (action 0). Figure 4 shows
a histogram of the cumulative reward over 1000
simulated episodes, for the constrained and uncon-
strained policies obtained by RL, together with a
completely random policy for comparison.

Figure 4: Total reward after 1000 episodes for a random
policy, the unconstrained policy, and the agent which is sub-
jected to the constraint that action 3 is not allowed unless
P (θ = 0.99) > 0.9.

In this example,

the information state-space

20

could also be deﬁned in a simpler way, as explained
in Example 5.1. As a result the reinforcement learn-
ing task will be simpliﬁed. Using the diﬀerent state-
space representations, we obtained the same results
shown in Figure 4. Finally, we should note that in
the case where deﬁning the state space as in (14) is
necessary, the constraint P (θ = 0.99) > 0.9 is not
practical. That is, if we could estimate this prob-
ability eﬃciently, then we also have access to the
compressed information state-space. One alterna-
tive could then be to instead consider the uncertain
failure probability pf (θ) = P (Y = 1 | θ), and set
a limit on e.g. E[pf ] + 2 · Std(pf ). This is the ap-
proach taken in the following example concerning
failure probability estimation.

5.6. Corroded pipeline example

Here we revisit the corroded pipeline example
from Agrell and Dahl [2] which we introduced in
Section 3.4. In this example, we have speciﬁed epis-
temic uncertainty with respect to model discrep-
ancy, the size of a defect, and the capacity pFE com-
ing from a Finite Element simulation. If we let θ
be the epistemic generator, we can write the failure
probability conditioned on epistemic information as
pf (θ) = P (g ≤ 0 | θ).
In [2] the following ob-
jective was considered: Determine with conﬁdence
whether pf (θ) < 10−3. That is, when we consider
pf as a purely epistemic random variable, we want
to either conﬁrm that the failure probability is less
than the target 10−3 (in which case we can con-
tinue operations as normal), or to detect with con-
ﬁdence that the target is exceeded (and we have to
intervene). Will say the the objective is achieved
if we obtain either E[pf ] + 2 · Std(pf ) < 10−3 or
E[pf ]−2·Std(pf ) > 10−3 (where E[pf ] and Std(pf )
can be eﬃciently approximated using the method
developed in [2]). There are three ways in which
we can reduce epistemic uncertainty:

1. Defect measurement: Noise perturbed mea-
surement that reduces uncertainty in the defect
size d

2. Computer experiment: Evaluate pFE at
some selected input (D, t, s, d, l), to reduce un-
certainty in the surrogate ˆpFE used to approx-
imate pFE.

3. Lab experiment: Obtain one observation of

Xm, which reduces uncertainty in µm.

The set of information corresponding to defect
measurements is IMeasure ⊂ R as each measurement

42024681012Total reward (10^6 $)0.00.10.20.30.40.5RandomUnconstrainedAgentis a real valued number. Similarly, ILab ⊂ R as well,
and IFE ⊂ R6 when we consider a vector y ∈ R6
as an experiment [D, t, s, d, l, pFE]. Actually, in this
example we may exploit some conjugacy in in the
representation of IMeasure and ILab as discussed in
Example 5.1 (see [2] for details), so we can deﬁne
the information state-space as S = SFE ×R2, where
SFE consists of ﬁnite subsets of R6.

We will use RL to determine which of the three
types of experiment to perform, and deﬁne the ac-
tion space A = {Measurement, FE, Lab}. Note that
when we decide to run a computer experiment, we
also have to specify the input (D, t, s, d, l). This is
a separate decision making problem regarding de-
sign of experiments. For this we make use of the
myopic (one-step lookahead) method developed in
[2], although one could in principle use RL for this
as well. This kind of decision making, where one
ﬁrst decides between diﬀerent types of task to per-
form, and then proceed to ﬁnd a way to perform
the selected task optimally, is often referred to as
hierarchical RL in the reinforcement learning lit-
erature. Actually, [2] considers a myopic alterna-
tive for also selecting between the diﬀerent types of
experiments, and it was observed that this might
be challenging in practice if there are large diﬀer-
ences in cost between the experiments. This was
the motivation for studying the current example,
where we now deﬁne the reward (cost) r as a di-
rect consequence of a ∈ A as follows: r = −10
for a = Measurement, r = −1 for a = Lab and
r = −0.1 for a = FE.

In this example we also made use of the DQN
approach of van Hasselt et al. [25], where we deﬁne
a neural network

that the agent has succeeded and we report the sum
of the cost of all performed experiments. We also
set a maximum limit of 40 experiments. I.e. after
40 tries the agent has failed. To compare the policy
obtained by RL, we consider the random policy that
selects between the three actions uniformly at ran-
dom. We also consider a more ”human like” bench-
mark policy, that corresponds to ﬁrst running 10
computer experiments, followed by one lab exper-
iment then one defect measurement, then 10 new
computer experiments, and so on.

The ﬁnal results from simulating 100 episodes
with each of the three policies is shown in Figure 5.

Figure 5: Total cost (negative reward) after 100 successful
episodes. For the random and benchmark policy, the success
rate was around 60% (to achieve the objective within 40
experiments in total), whereas 94 % was successful for the
RL agent.

6. Concluding remarks

ˆq(s, w) : S = SFE × R2 → R3,

To conclude our discussion, we recall that in this

paper, we have:

that gives, for each state s, the (near optimal) value
of each of the three actions. We refrain from de-
scribing all details regarding the neural network and
the speciﬁc RL algorithm, as the main purpose with
this example is for illustration. But we note that
two important innovations in the DQN algorithm,
the use of a target network and experience replay
as proposed in [29], was necessary for this to work.
The objective in this RL example is to estimate
a failure probability using as little resources as pos-
sible.
If an agent achieves the criterion on epis-
temic uncertainty reduction, that the expected fail-
ure probability plus/minus two standard deviations
is either above or below the target value, we say

• Given a measure-theoretic discussion of epis-
temic uncertainty and formally deﬁned epis-
temic conditioning.

• Provided a mathematical deﬁnition of a prob-

abilistic digital twin (PDT).

• Connected PDTs with sequential decision
making problems, and discussed several solu-
tion approaches (maximum principle, dynamic
programming, MDP and POMDP).

• Argued that using (deep) RL to solve sequen-
tial decision making problems in the PDT is a
good choice for practical applications today.

21

020406080100120Total cost0.000.010.020.030.040.050.060.07Probability densityRandomBenchmarkAgent• For the speciﬁc use-case of optimal information
gathering, we proposed a generic solution using
deep RL on the information state-space.

Further research in this direction includes looking
at alternative solution methods and RL algorithms
in order to handle diﬀerent PDT frameworks. A
possible idea is to use a maximum principle ap-
proach instead of a DPP approach (as is done in
RL). By using one of the MP based algorithms in
[20], we may avoid the Markovianity requirement,
possible time-inconsistency issues and can also al-
low for state constraints. For instance, this is of in-
terest when the objective of the sequential decision
making problem in the PDT is to minimize a risk
measure such as CVaR or VaR. Both of these risk
measures are known to cause time-inconsistency in
the Bellman equation, and hence, the DPP (and
also RL) cannot be applied in a straightforward
manner. This is work in progress.

Acknowledgements

This work has been supported by grant 276282
from the Research Council of Norway (RCN) and
DNV Group Research and Development (Christian
Agrell and Andreas Hafver). The work has also
been supported by grant 29989 from the Research
Council of Norway as part of the SCROLLER
project (Kristina Rognlien Dahl). The main ideas
presented in this paper is the culmination of input
from recent research activities related to risk, un-
certainty, machine learning and probabilistic mod-
elling in DNV and at the University of Oslo.
In
particular we want to thank Simen Eldevik, Frank
Børre Pedersen and Carla Ferreira for valuable in-
put to this paper.

References

[1] A. Hafver, S. Eldevik, F. B. Pedersen, Probabilistic
Digital Twins (2018). [Online position paper by DNV
GL Group Technology and Research; posted 28-August-
2018;
https://ai-and-safety.dnvgl.com/probabilistic-
twin/index.html].

[2] C. Agrell, K. R. Dahl, Sequential Bayesian optimal
experimental design for structural reliability analysis,
Statistics and Computing 31 (2021).

[3] J. Bect, D. Ginsbourger, L. Li, V. Picheny, E. Vazquez,
Sequential design of computer experiments for the esti-
mation of a probability of failure, Statistics and Com-
puting 22 (2012) 773–793.

[4] B. Echard, N. Gayton, M. Lemaire, AK-MCS: An ac-
tive learning reliability method combining Kriging and
Monte Carlo Simulation, Structural Safety 33 (2011)
145 – 154.

[5] B. Bichon, M. Eldred, L. Swiler, S. Mahadevan, J. Mc-
Farland, Eﬃcient Global Reliability Analysis for Non-
linear Implicit Performance Functions, AIAA Journal
46 (2008) 2459–2468.

[6] Z. Sun, J. Wang, R. Li, C. Tong, LIF: A new Kriging
based learning function and its application to structural
reliability analysis, Reliability Engineering & System
Safety 157 (2017) 152 – 165.

[7] W. Jian, S. Zhili, Y. Qiang, L. Rui, Two accuracy
measures of the Kriging model for structural reliability
analysis, Reliability Engineering & System Safety 167
(2017) 494 – 505.

[8] G. Perrin, Active learning surrogate models for the
conception of systems with multiple failure modes, Re-
liability Engineering & System Safety 149 (2016) 130 –
136.

[9] A. D. Kiureghian, O. Ditlevsen, Aleatory or epistemic?
Does it matter?, Structural Safety 31 (2009) 105 – 112.
Risk Acceptance and Risk Communication.

[10] J. Helton, J. Johnson, W. Oberkampf, C. Sallaberry,
Representation of analysis results involving aleatory
and epistemic uncertainty,
International Journal of
General Systems 39 (2010) 605–646.

[11] J. Pearl,

Causal diagrams for empirical research,

Biometrika 82 (1995) 669–688.

[12] DNV GL, Recommended Practice: Corroded pipelines

DNVGL-RP-F101, DNV GL, Høvik, Norway (2017).

[13] B. Øksendal,

in:
Stochastic diﬀerential equations, Springer, 2003, pp.
65–84.

Stochastic diﬀerential equations,

[14] D. P. Bertsekas, Dynamic programming and optimal
control, volume 1, Athena scientiﬁc Belmont, MA, 1995.
[15] B. Rudloﬀ, A. Street, D. M. Vallad˜ao, Time consistency
and risk averse dynamic decision models: Deﬁnition,
interpretation and practical consequences, European
Journal of Operational Research 234 (2014) 743–750.

[16] P. Cheridito, M. Stadje, Time-inconsistency of var and
time-consistent alternatives, Finance Research Letters
6 (2009) 40–46.

[17] P. Artzner, F. Delbaen, J.-M. Eber, D. Heath, H. Ku,
Coherent multiperiod risk adjusted values and bell-
man’s principle, Annals of Operations Research 152
(2007) 5–22.

[18] Q. Li, S. Hao, An optimal control approach to deep
learning and applications to discrete-weight neural net-
works, in: International Conference on Machine Learn-
ing, PMLR, 2018, pp. 2985–2994.

[19] J. F. Bonnans, On an algorithm for optimal control
using pontryagin’s maximum principle, SIAM Journal
on Control and Optimization 24 (1986) 579–588.
[20] Q. Li, L. Chen, C. Tai, et al., Maximum principle
arXiv preprint

based algorithms for deep learning,
arXiv:1710.09513 (2017).

[21] R. S. Sutton, A. G. Barto, Reinforcement Learning: An

Introduction, second ed., The MIT Press, 2018.

[22] K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A.
Bharath, A brief survey of deep reinforcement learning,
arXiv preprint arXiv:1708.05866 (2017).

[23] K. R. Dahl, E. Stokkereit, Stochastic maximum princi-
ple with Lagrange multipliers and optimal consumption
with L´evy wage, Afrika Matematika 27 (2016) 555–572.
[24] M. Wiering, M. Van Otterlo, Reinforcement learning,
Adaptation, learning, and optimization 12 (2012).
[25] H. van Hasselt, A. Guez, D. Silver, Deep reinforcement
learning with double q-learning, in: Proceedings of the

22

AAAI Conference on Artiﬁcial Intelligence, volume 30,
2016.

[26] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R.
Salakhutdinov, A. J. Smola, Deep sets,
in: I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, R. Garnett (Eds.), Advances in Neu-
ral Information Processing Systems, volume 30, Curran
Associates, Inc., 2017.

[27] E. Wagstaﬀ, F. Fuchs, M. Engelcke, I. Posner, M. A. Os-
borne, On the limitations of representing functions on
sets, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Pro-
ceedings of the 36th International Conference on Ma-
chine Learning, volume 97 of Proceedings of Machine
Learning Research, PMLR, 2019, pp. 6487–6494.
[28] M. Soelch, A. Akhundov, P. van der Smagt, J. Bayer,
On deep set learning and the choice of aggregations,
in: I. V. Tetko, V. Kurkov´a, P. Karpov, F. J. Theis
(Eds.), Artiﬁcial Neural Networks and Machine Learn-
ing - ICANN 2019, volume 11727, Springer, 2019, pp.
444–457.

[29] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,
J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,
A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beat-
tie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, D. Hassabis, Human-level con-
trol through deep reinforcement learning, Nature 518
(2015) 529–533.

[30] M. Greinecker, Every countably generated sigma-
algebra is generated by a real random variable, Math-
ematics Stack Exchange, 2014. URL: https://math.
stackexchange.com/q/816337.

Appendices

A. Existence of the epistemic generator

The purpose of this section, is to explain why
Assumption 2.5 (of the existence of a generating
random variable for the epistemic σ-algebra) hold
under some very mild assumptions.

In order to do this, we consider a standard prob-
ability space. Roughly, this is a probability space
consisting of an interval and/or a countable (or
ﬁnite) number of atoms. Formally, a probability
space is standard if it is isomorphic (up to P-null
sets) with an interval equipped with the Lebesgue
measure, a countable (or ﬁnite) set of atoms, or a
disjoint union of both of these types of sets.

The following proposition says that in a standard
probability space, any sub σ-algebra is generated by
a random variable up to P -null sets. For a proof,
see e.g. Greinecker [30].

Proposition A.1. Let (Ω, F, P ) be a standard
probability space and E ⊆ F a sub σ-algebra.

Then there exists an F-measurable random vari-

able θ such that

E = σ(θ) mod 0.

Hence, as long as our probability space is stan-
dard (which is a mild assumption), we can assume
that our sub σ-algebra of epistemic information, E,
is generated (up to P -null sets) by a random vari-
able θ without loss of generality. Note that for the
purpose of this paper, the mod 0 (i.e., up to P -null
sets) is not a problem. Since we are only considering
conditional expectations (or in particular, expecta-
tions), the P -null sets disappear.

Actually, this generating random variable, θ, can
always be modiﬁed to another random variable, ˆθ,
which is E-measurable (purely epistemic) by aug-
menting the P -null sets. This means that θ and ˆθ
are the same with respect to conditional expecta-
tions.

Furthermore, if X is a random variable on this
standard probability space, X|θ, is purely aleatory,
i.e., independent of E. This follows because X|θ
is independent of σ(θ) and independence holds P -
almost surely, so mod 0 does not aﬀect this.

23

