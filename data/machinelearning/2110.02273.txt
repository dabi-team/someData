1
2
0
2

t
c
O
5

]

C
O
.
h
t
a
m

[

1
v
3
7
2
2
0
.
0
1
1
2
:
v
i
X
r
a

Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity
Constraints

Juan Carlos De los Reyes∗ and David Villac´ıs∗ †

Abstract. We investigate a family of bilevel imaging learning problems where the lower-level instance corresponds
to a convex variational model involving ﬁrst- and second-order nonsmooth regularizers. By using geo-
metric properties of the primal-dual reformulation of the lower-level problem and introducing suitable
changes of variables, we are able to reformulate the original bilevel problems as Mathematical Programs
with Complementarity Constraints (MPCC). For the latter, we prove tight constraint qualiﬁcation condi-
tions (MPCC-MFCQ and partial MPCC-LICQ) and derive Mordukovich (M-) and Strong (S-) stationarity
conditions. The S-stationarity system for the MPCC turns also into S-stationarity conditions for the
original formulation. Second-order suﬃcient optimality conditions are derived as well. The proposed refor-
mulation may be extended to problems in function spaces, leading to MPCC’s with additional constraints
on the gradient of the state. Finally, we report on some numerical results obtained by using the proposed
MPCC reformulations together with available large-scale nonlinear programming solvers.

Key words. Bilevel optimization, machine learning, MPCC, variational models.

AMS subject classiﬁcations. 49K99, 90C33, 68U10, 68T99, 65K10

1. Introduction. Bilevel imaging learning problems have been introduced in [38] for learning
Markov random ﬁelds models, and in [15, 30] for optimally learning variational models and non-
smooth regularizers in denoising problems. Thereafter, several other imaging applications have
been successfully considered within this framework, e.g., noise model learning [5, 6], higher order
regularizers [14, 12, 11, 27], blind deconvolution problems [28], nonlocal models [10, 2].

The main diﬃculty of these problems relies on the nonsmooth structure of the lower-level in-
stance, which has prevented the application of standard bilevel programming theory. In classical
bilevel optimization, whenever the lower-level cost function is convex and diﬀerentiable and no
additional inequality constraints are present, the problems may be equivalently written as Math-
ematical Programs with Complementarity Constraints (MPCC), which allows the use of the rich
analytical toolbox developed for MPCC in the last decades to derive sharp optimality conditions
[31, 22, 34]. Since in most of the cases, however, this equivalence does not hold [18], much of the
research carried out in this ﬁeld consists precisely in proving under which circumstances an MPCC
formulation is possible.

In the case of bilevel imaging learning problems, due to the diﬃculties already mentioned,
optimality conditions have been previously obtained using a local regularization of the nonsmooth
terms and performing an asymptotic analysis thereafter [15, 30, 40], yielding a C-stationarity
system (see also [25, 26] for a related approach based on a dual reformulation of the lower-level
problem). Alternatively, a direct nonsmooth approach was considered in [28] and [17] to learn
point spread functions in blind deconvolution models and the weight in front of the ﬁdelity term
in denoising models, respectively.
In both such cases, the parameter aﬀects the ﬁdelity term
and, based on variational analysis tools, M-stationarity systems were derived. Recently [16], M-
stationarity conditions were also obtained for total variation bilevel learning problems, when the
scale-dependent parameter appears within the regularizer.
In summary, so far, M-stationarity
systems are the sharpest ones that have been obtained for the type of problems in question.

∗Research Center

for Mathematical Modelling (MODEMAT), Escuela Polit´ecnica Nacional, Quito, Ecuador

(juan.delosreyes@epn.edu.ec, david.villacis01@epn.edu.ec, http://www.modemat.epn.edu.ec/).

†D. Villac´ıs contributed with the Pyomo code and experiments in Section 5.

1

 
 
 
 
 
 
2

J.C. DE LOS REYES AND D. VILLAC´ıS

In this paper we improve previous results and provide sharper optimality conditions for bilevel
imaging learning problems by means of an MPCC reformulation. By restricting our attention to
ﬁrst and second order regularizers, and exploiting the geometric nature of the primal-dual reformu-
lation of the lower-level problem, we are indeed able, after performing suitable changes of variables,
to reformulate the bilevel instances as MPCC. This reformulation opens the door to a detailed char-
acterization of stationarity conditions; we are able to demonstrate S-stationarity (Theorem 3.4 and
Theorem 3.7) under suitable assumptions. Moreover, also second-order suﬃcient optimality condi-
tions may be derived in this manner (Theorem 3.6) and an inﬁnite-dimensional bilevel counterpart
may be stated as well (Section 4).

Thanks to the equivalent MPCC reformulation and the targeted strong stationary points, it is
possible to use diﬀerent MPCC solvers which are already available. In the last part of the paper
we report on the performance of these solvers for the bilevel learning problems under study and
provide some insights about their behaviour.

The outline of the paper is as follows. We will present the general bilevel problem and its
MPCC reformulation in Section 2, illustrating diﬀerent particular instances such as Total Variation
and Second-Order Total Generalized Variation denoising. The analysis of the bilevel problems
will be presented in Section 3, where the MPCC-MFCQ and the partial MPCC-LICQ constraint
qualiﬁcation conditions will be veriﬁed, and corresponding strong stationarity systems derived.
An extension of the obtained reformulation to the inﬁnite-dimensional setting will be explored in
Section 4, highlighting the main diﬃculties and required assumptions. In Section 5 we report on a
set of experiments with isotropic and anisotropic total variation regularizers, using available MPCC
solvers.

2. Problem Statement and Reformulation. In this work we are interested in bilevel learning

problems of the following form:

(2.1a)

(2.1b)

(2.1c)

minimize
(u,λ,σ,α,β)

J (u)

subject to u = arg min

E(v, λ, σ, α, β),

v∈Rd

P (λ), R(σ), Q(α), S(β) ≥ 0,

where the lower-level energy is given by

(2.2) E(v, λ, σ, α, β) :=

k1(cid:88)

i=1

D1(v)i +

K
(cid:88)

kj
(cid:88)

j=2

i=1

Pj(λj)iDj(v)i +

L
(cid:88)

lj
(cid:88)

j=1

i=1

Rj(σj)i|(Bjv)i|

+

M
(cid:88)

mj
(cid:88)

j=1

i=1

Qj(αj)i(cid:107)(Kjv)i(cid:107)2 +

N
(cid:88)

nj
(cid:88)

j=1

i=1

Sj(βj)i(cid:107)(Ejv)i(cid:107)F .

(cid:55)→ Rkj , Rj : R|σj |

(cid:55)→ Rmj and Sj : R|βj |

(cid:55)→ Rlj , Qj :
with vector parameters λj, σj, αj, βj. The operators Pj : R|λj |
(cid:55)→ Rnj are assumed to be twice continuous diﬀerentiable, while the
R|αj |
operators inside the norms, Bj : Rd (cid:55)→ Rlj , Kj : Rd (cid:55)→ Rmj ×2 and Ej : Rd (cid:55)→ (Sym2)nj , are
assumed to be linear. The notation | · |, (cid:107) · (cid:107) and (cid:107) · (cid:107)F stands for the absolute value, the Euclidean
norm and the Frobenius norm, respectively. Similarly, (cid:104)·, ·(cid:105) and (cid:104)·, ·(cid:105)F denote the Euclidean and
Frobenius scalar products, respectively. Sym2 denotes the space of 2 × 2 symmetric matrices. The
operators Dj : Rd → Rkj correspond to the diﬀerent data ﬁdelity terms considered in the model.
The energy (2.2) considered in this manuscript encompasses, therefore, the main ﬁrst and second
order regularizers used within variational image processing.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

3

Assuming existence of a training set of clean and noisy images, or some information about
noise statistics, the upper-level loss function J may incorporate this information. This occurs,
for instance, through a quadratic loss using ground-truth images [15], quality measures aimed at
preserving jumps [14] or deviation of image residuals from an estimated variance corridor [27].

Next, we will reformulate problem (2.1) using the duality properties of the corresponding lower-
level problem. To do so, we assume along the paper the following condition on the diﬀerent data
ﬁdelity terms.

Assumption 2.1. For each j = 1, . . . , K, the corresponding data ﬁdelity function Dj is convex

and diﬀerentiable.

2.1. Bilevel Total Variation. A typical lower-level example is the classical Rudin-Osher-Fatemi

denoising model given by the energy minimization problem:

(2.3)

u = arg min

v∈Rd

d
(cid:88)

i=1

D(v; f )i +

m
(cid:88)

i=1

Q(α)i(cid:107)(Kv)i(cid:107),

where K : Rd → Rm×2 stands for the discrete gradient operator. In this case the choice of regularizer
is known as isotropic total variation, which has been widely adopted in the image processing com-
munity due to its edge-preserving properties, even though it may induce other unwanted artifacts
such as staircasing (see, e.g., [36] and the references therein).

Given the assumptions on the ﬁdelity terms, described in Assumption 2.1, we can guarantee
existence of a unique minimizer for problem (2.3). Moreover, its necessary and suﬃcient optimality
condition is given by the following variational inequality of the second kind [13]:

(2.4)

(cid:104)∇uD(u; f ), v − u(cid:105) +

m
(cid:88)

i=1

Q(α)i(cid:107)(Kv)i(cid:107) −

m
(cid:88)

i=1

Q(α)i(cid:107)(Ku)i(cid:107) ≥ 0,

∀v ∈ Rd.

By using Fenchel duality theory [19], it is possible to write an equivalent primal-dual optimality
condition for the lower level problem (2.4) as follows:

(2.5a)

(2.5b)

(2.5c)

∇uD(u; f ) + K(cid:124)q = 0,

(cid:104)qi, (Ku)i(cid:105) = Q(α)i(cid:107)(Ku)i(cid:107),
(cid:107)qi(cid:107) ≤ Q(α)i,

i = 1, . . . , m,

i = 1, . . . , m,

(cid:124)
where q stands for the dual multiplier and K(cid:124)q := K
y q(2) corresponds to the discrete
divergence operator. Now, by replacing the lower-level problem with the optimality condition
(2.5), we get a new single level optimization problem given by:

(cid:124)
xq(1) + K

(2.6a)

(2.6b)

(2.6c)

(2.6d)

(2.6e)

minimize
u,λ,α

J (u)

subject to ∇uD(u; f ) + K(cid:124)q = 0,

(cid:104)qi, (Ku)i(cid:105) = Q(α)i(cid:107)(Ku)i(cid:107),
(cid:107)qi(cid:107) ≤ Q(α)i,
Q(α) ≥ 0.

i = 1, . . . , m,

i = 1, . . . , m,

This alternative optimization problem presents the nonstandard complementarity constraint (2.6c),
which resembles a componentwise cosinus formula. To gain further insight, let us reformulate, using

4

J.C. DE LOS REYES AND D. VILLAC´ıS

a polar type change of variables, the components of the dual variable and the gradient of the primal
variable, qi and (Ku)i, as follows:

qi = δi[cos(φi), sin(φi)](cid:124),
(Ku)i = ri[cos(θi), sin(θi)](cid:124),

and let us also introduce the following inactive, active and biactive index sets

I(u) := {i ∈ {1, . . . , m} : (Ku)i (cid:54)= 0},
A(u) := {i ∈ {1, . . . , m} : (Ku)i = 0, (cid:107)qi(cid:107)2 < Q(α)i},
B(u) := {i ∈ {1, . . . , m} : (Ku)i = 0, (cid:107)qi(cid:107)2 = Q(α)i}.

As depicted in Figure 1 we can see that for the case i ∈ A(u) ∪ B(u) the equality in (2.6c) holds
trivially as (Ku)i = 0 in this set. For the case i ∈ I(u) the equality holds only if both vectors qi
and (Ku)i are colinear, implying that the angles are the same, i.e., θi = φi. Moreover, in this case
the dual variable can be uniquely determined, which is not the case in the active and biactive sets,
where it is contained in the unit ball.

(Ku)i
qi

qi

(Ku)i

qi

(Ku)i

(a) i ∈ I(u)

(b) i ∈ A(u)

(c) i ∈ B(u)

Figure 1: Geometric interpretation of the primal-dual system for the diﬀerent index sets.

This insight allow us to reformulate the terms in (2.6c) as follows

(cid:104)qi, (Ku)i(cid:105) = ri cos(θi)δi cos(θi) + ri sin(θi)δi sin(θi) = riδi(cos2(θi) + sin2(θi)) = riδi,
Q(α)i(cid:107)(Ky)i(cid:107) = Q(α)i(r2

i sin2(θi))1/2 = Q(α)i|ri|,

i cos2(θi) + r2

which yields

riδi − Q(α)i|ri| = 0, ∀i = 1, . . . , m.

Constraint (2.6d) can then be rewritten as

(cid:107)qi(cid:107) = (δ2

i cos2(φi) + δ2

i sin2(φi))1/2 = |δi| ≤ Q(α)i.

It is clear that whenever ri = δi = 0, the angle θi may be arbitrarily chosen as it does not aﬀect the
representation of the primal and dual variables. Furthermore, we know that riδi ≥ 0 and δi ≥ 0,
implying 0 ≤ Q(α)iri = riδi and 0 ≤ δi ≤ Q(α)i.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

5

Altogether, we can rewrite problem (2.6) as the following MPCC:

(2.7)

minimize
u,q,α,r,δ,θ

J (u)

subject to
∇uD(u; f )) + K(cid:124)q = 0,
(Ku)i = ri[cos(θi), sin(θi)](cid:124),
qi = δi[cos(θi), sin(θi)](cid:124),
Q(α) ≥ 0,
δi ≥ 0,
0 ≤ ri ⊥ (Q(α)i − δi) ≥ 0,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1, . . . , m.

Problem (2.7) has classical equality and inequality constraints, along with standard complementar-
ity constraints (see, e.g., [35]).

2.2. Bilevel Second Order Total Generalized Variation. Higher order regularizers were intro-
duced as a remedy to some shortcomings of nonsmooth ﬁrst order regularizers, such as isotropic
or anisotropic Total Variation. Arguably the most well-known artifact introduced by TV is the so-
called staircasing, which leads to a piecewise constant reconstruction of smooth intensity variations
in the image. Indeed, one possibility to counteract such artifacts is the introduction of higher-order
derivatives in the image regularization terms. Chambolle and Lions [8], for instance, proposed a
higher-order method by means of an inﬁmal convolution of the TV and the TV of the image gradi-
ent, called Inﬁmal Convolution Total Variation (ICTV) model. Other approaches to combine ﬁrst-
and second-order regularization originate, for instance, from Chan et al. [9] who consider total vari-
ation minimization together with weighted versions of the Laplacian, the Euler-elastica functional
[33], which combines total variation regularization with curvature penalisation, and many more.

Bredies et al. [4] proposed Total Generalized Variation (TGV) as a higher-order variant of TV
regularization. This family of regularizers has gained a lot of popularity thanks to its frequently su-
perior performance compared with TV and ICTV, and has been studied in depth both theoretically
and numerically [29, 32, 3, 25].

Bilevel problems with TGV lower-level instances have also been investigated in the last years,
yielding results on existence and approximability of optimal parameters [14], ﬁrst-order necessary
optimality conditions for scalar and scale-dependent parameters [40, 7], dualization approaches [27,
26], and optimization algorithms for their numerical solution [7, 14].

In the case of the Second Order Total Generalized Variation (T GV 2) regularizer, the lower-level

denoising problem is given by

(2.8a)

minimize
u = (v, w)

k
(cid:88)

i=1

D(v; f )i +

m
(cid:88)

i=1

Q(α)i(cid:107)(Kv − w)i(cid:107) +

n
(cid:88)

i=1

S(β)i(cid:107)(Ew)i(cid:107)F ,

where K stands for the discrete gradient operator and E for the discrete symmetrized gradient
tensor. Thanks to the convexity of the energy and using duality theory, a necessary and suﬃcient
optimality condition for the denoising problem is given by the primal-dual system:

(2.9a)

(2.9b)

(2.9c)

(2.9d)

∇vD(v; f ) + K(cid:124)q = 0,
Q(α)iqi = −Div Λi,
(cid:104)qi, (Kv − w)i(cid:105) = Q(α)i(cid:107)(Kv − w)i(cid:107),
(cid:107)qi(cid:107) ≤ Q(α)i,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1, . . . , m

6

(2.9e)

(2.9f)

(cid:104)Λi, (Ew)i(cid:105)F = S(β)i(cid:107)(Ew)i(cid:107)F ,
(cid:107)Λi(cid:107)F ≤ S(β)i,

i = 1, . . . , n,

i = 1, . . . , n,

J.C. DE LOS REYES AND D. VILLAC´ıS

where Div stands for the discrete row-wise divergence operator. Since each qi and each (Kv − w)i
are elements of R2, we proceed as in the TV case and use the change of variables

(Kv − w)i = [ri cos(θi), ri sin(θi)](cid:124),
qi = [δi cos(θi), δi sin(θi)](cid:124),

which implies that

Also

which yields

(cid:107)qi(cid:107) = (δ2

i cos2(θi) + δ2

i sin2(θi))1/2 = |δi| ≤ Q(α)i.

(cid:104)qi, (Kv − w)i(cid:105) = ri cos(θi)δi cos(θi) + ri sin(θi)δi sin(θi),

= riδi(cos2(θi) + sin2(θi)) = riδi.

(cid:107)(Kv − w)i(cid:107) = (r2

i cos2(θi) + r2

i sin2(θi))1/2 = |ri|,

riδi − Q(α)i|ri| = 0, ∀i = 1, . . . , m.

(cid:21)

1,2

, that their Frobenius norms are given by (cid:107)Λi(cid:107)F =

Concerning the second-order term, we obtain, thanks to the symmetry of each matrix Λi =
(cid:20)λi
1,1 λi
2,2)2, which
2,1 λi
λi
2,2](cid:124).
turn out to be equivalent to the Euclidean norm of the transformed vectors ˜Λi = [λi
1,2, λi
(cid:3)(cid:124)
In a similar manner, we introduce the notation (˜Ew)i := (cid:2)(Ew)i
. As a
consequence, we may transform the matrix constraints to vector ones and obtain

1,1,
1,2, (Ew)i

1,2)2 + (λi
√

1,1)2 + 2(λi

2(Ew)i

1,1,

2λi

(λi

(cid:113)

√

2,2

2,2

(2.10)

(2.11)

(cid:104)˜Λi, (˜Ew)i(cid:105) = S(β)i(cid:107)(˜Ew)i(cid:107),
(cid:107)˜Λi(cid:107) ≤ S(β)i,

i = 1, . . . , n,

i = 1, . . . , n.

Equation (2.10) then implies collinearity of the vectors ˜Λi and (˜Ew)i, for each i. Hence, using

in this case a spherical coordinates type of transformation, we get the representation

(cid:124)
(˜Ew)i = ρi [sin(φi) cos(ϕi), sin(φi) sin(ϕi), cos(φi)]
˜Λi = τi[sin(φi) cos(ϕi), sin(φi) sin(ϕi), cos(φi)](cid:124),

,

which implies that

(cid:104)˜Λi, (˜Ew)i(cid:105) = ρiτi(sin2(φi) cos2(ϕi) + sin2(φi) sin2(ϕi) + cos2(φi)) = ρiτi,

and, also,

(cid:107)˜Λi(cid:107) = |τi|

(cid:113)

sin2(φi) cos2(ϕi) + sin2(φi) sin2(ϕi) + cos2(φi) = |τi|.

Consequently, equations (2.9e) and (2.9f) may be rewritten as

ρiτi = S(β)i|ρi|

and

|τi| ≤ S(β)i,

∀i = 1, . . . , n,

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

7

respectively.

Altogether, we arrive at the following equivalent MPCC reformulation of the second-order TGV

bilevel problem:

(2.12)

minimize
v,w,q, ˜Λ,α,β,r,δ,ρ,τ,θ,φ,ϕ

J (v)

subject to
∇vD(v; f ) + K(cid:124)q = 0,
Q(α)iqi = −Div Λi
(Kv − w)i = ri[cos(θi), sin(θi)](cid:124),
qi = δi[cos(θi), sin(θi)](cid:124),
(cid:124)
(˜Ew)i = ρi [sin(φi) cos(ϕi), sin(φi) sin(ϕi), cos(φi)]
˜Λi = τi[sin(φi) cos(ϕi), sin(φi) sin(ϕi), cos(φi)](cid:124)
Q(α)i, δi ≥ 0
S(β)i, τi ≥ 0,
0 ≤ ri ⊥ (Q(α)i − δi) ≥ 0,
0 ≤ ρi ⊥ (S(β)i − τi) ≥ 0,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1 . . . , n,

i = 1 . . . , n,

i = 1 . . . , m,

i = 1 . . . , n,

i = 1 . . . , m,

i = 1 . . . , n.

2.3. General Bilevel Problem. For the general problem (2.1), a primal-dual reformulation of
the lower-level instance may be carried out in a similar manner as for the TV and TGV cases. The
additional diﬃculty is related with the absolute value terms |(Bjv)i|, which may be handled also
using duality. Indeed, introducing the dual variables cj,i we get the relations

(2.13)

(2.14)

cj,i(Bju)i = Rj(σj)i|(Bju)i|,
|cj,i| ≤ Rj(σj)i,

j = 1, . . . , L, i = 1, . . . , lj,
j = 1, . . . , L, i = 1, . . . , lj.

The constraints (2.14) are clearly box ones for each cj,i. For the constraints (2.13), due to the
positivity of Rj(σj)i and the Cauchy-Schwarz inequality, the less or equal direction always holds.
Therefore, the constraints can be formulated as

cj,i(Bju)i ≥ Rj(σj)i|(Bju)i|,

j = 1, . . . , L, i = 1, . . . , lj.

Splitting the absolute value, we get equivalently

−cj,i(Bju)i ≤ Rj(σj)i(Bju)i ≤ cj,i(Bju)i,

cj,i(Bju)i ≥ 0,

j = 1, . . . , L, i = 1, . . . , lj,
j = 1, . . . , L, i = 1, . . . , lj.

Thus, using bilateral constraints for the absolute value terms and changes of variables for the
Euclidean and the Frobenius norm terms, as in Equation (2.7) and Equation (2.12), respectively,
the MPCC reformulation of the general bilevel learning problem is given by:

(2.15a)

minimize
u,c,q,Λ,λ,σ,α,β,r,δ,ρ,τ,θ,φ,ϕ

J (u)

subject to the primal-dual denoising problem:

(2.15b)

L
(cid:88)

lj
(cid:88)

j=1

i=1

(B(cid:63)

j cj)i +

M
(cid:88)

mj
(cid:88)

j=1

i=1

(K(cid:63)

j qj)i +

N
(cid:88)

nj
(cid:88)

j=1

i=1

(E(cid:63)

j Λj)i

8

J.C. DE LOS REYES AND D. VILLAC´ıS

= −

k1(cid:88)

i=1

∇uD1(u)i −

K
(cid:88)

kj
(cid:88)

j=2

i=1

Pj(λj)i∇uDj(u)i,

the changes of variables of primal and dual variables of the Euclidean and Frobenius norm terms:

(2.15c)

(2.15d)

(2.15e)

(2.15f)

(Kju)i = rj,i[cos(θj,i), sin(θj,i)](cid:124),
qj,i = δj,i[cos(θj,i), sin(θj,i)](cid:124),

(cid:124)
(Eju)i = ρj,i [sin(φj,i) cos(ϕj,i), sin(φj,i) sin(ϕj,i), cos(φj,i)]
(cid:124)
Λj,i = τj,i [sin(φj,i) cos(ϕj,i), sin(φj,i) sin(ϕj,i), cos(φj,i)]

,

,

j = 1, . . . , M, i = 1, . . . , mj
j = 1, . . . , M, i = 1, . . . , mj
j = 1, . . . , N, i = 1, . . . , nj
j = 1, . . . , N, i = 1, . . . , nj

the inequality constraints for the absolute value terms:

(2.15g)

(2.15h)

(2.15i)

−cj,i(Bju)i ≤ Rj(σj)i(Bju)i ≤ cj,i(Bju)i,

−Rj(σj)i ≤ cj,i ≤ Rj(σj)i,

0 ≤ cj,i(Bju)i,

j = 1, . . . , L, i = 1, . . . , lj,
j = 1, . . . , L, i = 1, . . . , lj,
j = 1, . . . , L, i = 1, . . . , lj,

the positivity constraints

(2.15j)

(2.15k)

(2.15l)

(2.15m)

Rj(σj) ≥ 0,
Pj(λj)i ≥ 0,
δj,i, Qj(αj)i ≥ 0,
τj,i, Sj(βj)i ≥ 0,

and the complementarity constraints

j = 1, . . . , L, i = 1, . . . , lj
j = 2, . . . , L, i = 1, . . . , kj,
j = 1, . . . , M, i = 1, . . . , mj,
j = 1, . . . , N, i = 1, . . . , nj,

(2.15n)

(2.15o)

0 ≤ rj,i ⊥ (Qj(αj)i − δj,i) ≥ 0,
0 ≤ ρj,i ⊥ (Sj(βj)i − τj,i) ≥ 0,

j = 1, . . . , M, i = 1, . . . , mj,
j = 1, . . . , N, i = 1, . . . , nj.

3. Optimality Conditions. For mathematical programs with complementarity constraints, it is
of particular importance to characterize local optimal solutions by means of an optimality system
as sharp as possible, as diﬀerent stationarity concepts coexist for this type of problems. To do
so, taylored constraint qualiﬁcation conditions are required to hold in order to get existence of
Lagrange multipiers and establish sign conditions on the so-called biactive set.

Consider the general MPCC given by:

(3.1a)

(3.1b)

(3.1c)

(3.1d)

minimize
x

(cid:96)(x)

subject to h(x) = 0,

g(x) ≤ 0,

0 ≤ M (x) ⊥ N (x) ≥ 0,

where (cid:96) : Rn → R, h : Rn → Rp, g : Rn → Rm and M, N : Rn → Rq are continuosly diﬀerentiable
functions. We denote the feasible set by

X = {x ∈ Rn : g(x) ≤ 0, h(x) = 0, 0 ≤ M (x) ⊥ N (x) ≥ 0}

and introduce the following index sets for a given feasible point x∗ ∈ X:

A(x∗) := {i : Mi(x∗) = 0, Ni(x∗) > 0},

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

9

B(x∗) := {i : Mi(x∗) = 0, Ni(x∗) = 0},
I(x∗) := {i : Mi(x∗) > 0, Ni(x∗) = 0}.

The set B(z∗) is called the biactive set. Moreover, for a given index set, where a set of relations
hold, we introduce the short notation {e(x)i = 0} := {i ∈ {1, . . . , n} : e(x)i = 0}.

To prove existence of Lagrange multipliers for (3.1), a suitable constraint qualiﬁcation condition
has to be satisﬁed. For this type of problems, however, it can be shown that standard nonlinear
programming constraint qualiﬁcation conditions such as LICQ and MFCQ do not hold, even for
very simple problem instances. Therefore, tailor-made constraint qualiﬁcation conditions have
been proposed in last years for MPCC, along with diﬀerent notions of stationarity such as Clarke-
stationarity, Mordukovich-stationarity and Strong-stationarity [22, 21].

To formulate the MPCC constraint qualiﬁcation conditions, let us start by deﬁning the tangent

cone TX (x) and the MPCC-linearized tangent cone LM P CC

X

(x).

Deﬁnition 3.1. Let x∗ ∈ X be feasible for (3.1).

• The tangent (Bouligand) cone to a set X at the point x∗ is given by

TX (x∗) := {d ∈ Rn : ∃xk

X−→ x∗, tk ≥ 0, tk(xk − x∗) → d}.

• The MPCC-Linearized Tangent Cone at a feasible point x∗ ∈ X is given by

LM P CC

X

(x∗) := {d ∈ Rn : ∇gi(x∗)(cid:124)d ≤ 0,
∇hi(x∗)(cid:124)d = 0,
∇Mi(x∗)(cid:124)d = 0,
∇Ni(x∗)(cid:124)d = 0,
∇Mi(x∗)(cid:124)d ≥ 0,
∇Ni(x∗)(cid:124)d ≥ 0,
(∇Mi(x∗)(cid:124)d)(∇Ni(x∗)(cid:124)d) = 0,

∀i ∈ {gi(x∗) = 0},
∀i = 1, . . . , p,
∀i ∈ A(x∗),
∀i ∈ I(x∗),
∀i ∈ B(x∗),
∀i ∈ B(x∗),
∀i ∈ B(x∗)}.

Depending on the relation between these two cones, or their polars, the diﬀerent MPCC con-

straint qualiﬁcation conditions are established.

Deﬁnition 3.2. Let x∗ ∈ X be feasible for (3.1).
a) MPCC-Guignard Constraint Qualiﬁcation (MPCC-GCQ) holds at x∗ if

b) MPCC-Abadie Constraint Qualiﬁcation (MPCC-ACQ) holds at x∗ if

TX (x∗)◦ = LM P CC

X

(x∗)◦.

TX (x∗) = LM P CC

X

(x∗).

c) MPCC-Mangasarian Fromowitz Constraint Qualiﬁcation (MPCC-MFCQ) holds at x∗ if the

vectors ∇gi(x∗), i ∈ {gi(x∗) = 0}, and

∇hi(x∗), i = 1, . . . , p, ∇Mi(x∗), i ∈ A(x∗) ∪ B(x∗), ∇Ni(x∗), i ∈ I(x∗) ∪ B(x∗)

are positive-linearly independent.

d) Partial MPCC-Linear Independence Constraint Qualiﬁcation holds at x∗ if

10

J.C. DE LOS REYES AND D. VILLAC´ıS

p
(cid:88)

i=1

λi∇hi(x∗) +

(cid:88)

µi∇gi(x∗)

i∈{gi(x∗)=0}

(cid:88)

−

γi∇Mi(x∗) −

(cid:88)

νi∇Ni(x∗) = 0

i∈A(x∗)∪B(x∗)

i∈I(x∗)∪B(x∗)

implies that γi = 0 and νi = 0, for all i ∈ B(x∗).

e) MPCC-Linear Independence Constraint Qualiﬁcation (MPCC-LICQ) holds at x∗ if the vec-

tors

∇hi(x∗),
∇Mi(x∗), i ∈ A(x∗) ∪ B(x∗),

∇gi(x∗), i ∈ {gi(x∗) = 0},
∇Ni(x∗), i ∈ I(x∗) ∪ B(x∗)

are linearly independent.

A feasible point x∗ ∈ X is called Clarke stationary for (3.1) if there exist multipliers λ ∈

Rp, µ ∈ Rm and γ, ν ∈ Rq, such that the following system is satisﬁed:

(3.2a)

(3.2b)

(3.2c)

(3.2d)

(3.2e)

(3.2f)

(3.2g)

(3.2h)

∇(cid:96)(x∗) + ∇h(x∗)λ + ∇g(x∗)µ − ∇M (x∗)γ − ∇N (x∗)ν = 0,
0 ≥ g(x∗) ⊥ µ ≥ 0,
h(x∗) = 0,
M (x∗) ≥ 0,
N (x∗) ≥ 0,
νi = 0,
γi = 0,
γiνi ≥ 0,

A constraint qualiﬁcation condition for (3.2) to hold is the MPCC-GCQ.

If, the last condition in (3.2) is replaced by the sharper characterization:

γiνi = 0 ∨ γi ≥ 0, νi ≥ 0,

for all i ∈ B(x∗),

∀i ∈ A(x∗),
∀i ∈ I(x∗),
∀i ∈ B(x∗).

then the system is called M-stationary, which also holds under MPCC-GCQ [42].

The sharpest optimality condition is called S-stationary (for strong), where, in addition to

(3.2), the following sign condition on the biactive set holds

γi ≥ 0, νi ≥ 0,

for all i ∈ B(x∗).

Theorem 3.3 (Flegel, Kanzow [22]). Let x∗ ∈ X be a local optimal solution of problem (3.1). If

both the MPCC-ACQ and the partial MPCC-LICQ hold at x∗, then x∗ is strongly stationary.

The strong stationarity system is the sharpest to characterize minima of MPCC problems. In
the case of an empty biactive set, all stationarity concepts presented previously coincide. For this
type of stationary points also second-order optimality conditions can be obtained.

3.1. Bilevel Total Variation. Concerning bilevel imaging learning problems with Total Vari-
ation, Clarke stationarity has been previously obtained for inﬁnite-dimensional problems in [15]
and Mordukovich stationarity was proved for blind point deconvolution in [28] and for denoising
problems in [17, 16]. Next, we will investigate under which conditions strong stationarity also holds
for this family of problems.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

11

Let us recall the reformulated bilevel learning problem with a Total Variation regularizer,

(3.3a)

(3.3b)

(3.3c)

(3.3d)

(3.3e)

(3.3f)

(3.3g)

J (u)

minimize
x=(u,q,α,r,δ,θ)
subject to ∇uD(u; f ) + K(cid:124)q = 0,

(Ku)i = ri[cos(θi), sin(θi)](cid:124),
qi = δi[cos(θi), sin(θi)](cid:124),
Q(α) ≥ 0,
δi ≥ 0,
0 ≤ ri ⊥ (Q(α)i − δi) ≥ 0,

i = 1, . . . , m,

i = 1, . . . , m,

i = 1 . . . , m,

i = 1 . . . , m.

and let us introduce the following active, inactive and biactive sets:

A(x∗) := {i : ri = 0, Q(α)i > δi},
I(x∗) := {i : ri > 0, Q(α)i = δi},
B(x∗) := {i : ri = 0, Q(α)i = δi}.

Theorem 3.4. Assume that ∇αQ(α)(cid:124) is a full-rank matrix and let x∗ = (u∗, q∗, α∗, r∗, δ∗, θ∗) be a
local optimal solution to (3.3). There exist Lagrange multipliers p ∈ Rd and λx, λy, ϕx, ϕy, ρ, σ, γ, ν ∈
Rm such that, together with equations (3.3b)-(3.3g), the following M-stationarity system is sat-
isﬁed:

(3.4a)

(3.4b)

(3.4c)

(3.4d)

(3.4e)

(3.4f)

(3.4g)

(3.4h)

(3.4i)

(3.4j)

(3.4k)

(3.4l)

∇uJ (u∗) − ∇2

uuD(u∗)(cid:124)p − K(cid:124)ϕ = 0,
∇αQ(α∗)(cid:124) (ρ + ν) = 0
−Kxp + λx = 0
−Kyp + λy = 0
cos(θ∗) ◦ ϕx + sin(θ∗) ◦ ϕy − γ = 0
− cos(θ∗) ◦ λx − sin(θ∗) ◦ λy − σ + ν = 0
sin(θ∗) ◦ (−r∗ ◦ ϕx + δ∗ ◦ λx) − cos(θ∗) ◦ (−r∗ ◦ ϕy + δ∗ ◦ λy) = 0
0 ≤ Q(α∗) ⊥ ρ ≥ 0,
0 ≤ δ∗ ⊥ σ ≥ 0,
νi = 0,
γi = 0,
γiνi = 0 ∨ γi ≥ 0, νi ≥ 0,

∀i ∈ A(x∗),
∀i ∈ I(x∗),
∀i ∈ B(x∗).

Moreover, if the triactive set T (x∗) := B(x∗) ∩ {i : Q(α∗)i = 0} is empty, then x∗ is S-stationary,
i.e., Equation (3.4l) is replaced by

(3.5)

γi ≥ 0, νi ≥ 0,

∀i ∈ B(x∗).

Proof. We start by writing (3.3) in the form of (3.1). To do so, we introduce the vector

x = (u, q, α, r, δ, θ)(cid:124) ∈ Rd+|α|+5m and deﬁne h : Rd+|α|+5m → Rd+4m by









h(x) =

−∇uD(u) − K(cid:124)q
−Kxu + r ◦ cos(θ)
−Kyu + r ◦ sin(θ)
qx − δ ◦ cos(θ)
qy − δ ◦ sin(θ)









,

12

J.C. DE LOS REYES AND D. VILLAC´ıS

where we used the structure of the TV problem to rewrite the constraints in terms of the partial
derivative operators. In addition, we deﬁne g : Rd+|α|+5m → R2m by

g(x) =

(cid:20) −Q(α)

(cid:21)

−δ

and M : Rd+|α|+5m → Rm, N : Rd+|α|+5m → Rm by M (x) = r and N (x) = Q(α) − δ, respectively.

The gradients of the constraints are then given by

[∇hi(x)] =








−∇2

uuD(u)(cid:124)
−Kx
−Ky
0
0
0
0

(cid:124)
x

−K
0
0
0
diag(cos(θ))
0

(cid:124)
y

−K
0
0
0
diag(sin(θ))
0

0
I
0
0
0

0
0
I
0
0

− diag(cos(θ)) − diag(sin(θ))





,



− diag(r◦sin(θ)) diag(r◦cos(θ)) diag(δ◦sin(θ)) − diag(δ◦cos(θ))

[∇gi(x)] =






0
0
0
0
0
0
−∇αQ(α)(cid:124) 0
0
0
−I
0
0
0




,

[∇Mi(x)] =









,

0
0
0
0
I
0
0

[∇Ni(x)] =









.

0
0
0
∇αQ(α)(cid:124)
0
−I
0

To verify the MPCC-MFCQ, let us start with the block columns of the equality constraints matrix
[∇hi(x)].
It can be easily veriﬁed that linear independence holds, even for indexes i such that
δi = ri = 0. Moreover, linear independence with respect to the block columns of the matrices
[∇gi(x)], [∇Mi(x)] and [∇Ni(x)] can be easily veriﬁed as well.

Let us now introduce the index set Z := {i = 1, . . . , m : δi = Q(α)i = 0} and consider the

linear system

(3.6)

d+4m
(cid:88)

i=1

λi∇hi(x∗) +

(cid:88)

µi∇gi(x∗) +

(cid:88)

µm+i∇gm+i(x∗)

i∈{gi(x∗)=0}

i∈{gm+i(x∗)=0}

(cid:88)

−

γi∇Mi(x∗) −

(cid:88)

νi∇Ni(x∗) = 0

i∈A(x∗)∪B(x∗)

i∈I(x∗)∪B(x∗)

Since the columns of the matrices [∇gi(x)]i∈{gi(x∗)=0}\Z , [∇gm+i(x)]i∈{gm+i(x∗)=0}\Z , [∇Mi(x)] and
[∇Ni(x)]i /∈Z are linearly independent, it follows that (3.6) is solvable with

λi = 0,
µi = 0,
µm+i = 0,
γi = 0,
νi = 0,

∀i
∀i ∈ {gi(x∗) = 0}\Z
∀i ∈ {gm+i(x∗) = 0}\Z
∀i ∈ A(x∗) ∪ B(x∗)
∀i ∈ (I(x∗) ∪ B(x∗)) \Z

In the index set Z we get, from the structure of [∇gi(x)]i∈Z , [∇gm+i(x)]i∈Z and [∇Ni(x)]i∈Z , that
µm+i = νi, and also that ∇αQ(α)(cid:124)(µi + νi) = 0, which, since by hipothesis ∇αQ(α)(cid:124) is full-rank,
implies that µi = −νi. Consequently, positive linear dependency does not hold and MPCC-MFCQ
is satisﬁed. Therefore, there exist Lagrange multipliers such that system (3.4a)-(3.4k) is satisﬁed,
together with

(3.7)

γiνi = 0 ∨ γi ≥ 0, νi ≥ 0,

∀i ∈ B(x∗).

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

13

To prove the partial MPCC-LICQ condition, let us consider system (3.6) again. If B(x∗) ∩ {i :

Q(α∗)i = 0} = ∅, it follows that δi = Q(α)i > 0, for all i ∈ B(x∗). Consequently,

(3.8)

{gi(x) = 0} ∩ B(x∗) = ∅,

which implies, by similar arguments as above, that the solution to (3.6) necessarily satisﬁes γi =
νi = 0, for all i ∈ B(x∗). Thus partial MPCC-LICQ holds. By applying Theorem 3.3 the results
follows.

System (3.4) may also be written using solely the original variables, leading to stationary

systems sharper than the ones obtained previously.

Theorem 3.5. Let x∗ = (u∗, q∗, α∗) be an optimal solution to (2.6). Assume that ∇αQ(α∗)(cid:124) is a
full-rank matrix and that T (x∗) = ∅. Then there exist Lagrange multipliers p ∈ Rd, ϕ ∈ R2m, and
ρ, σ, ν ∈ Rm such that the following S-stationarity system is satisﬁed:

(3.9a)

(3.9b)

(3.9c)

(3.9d)

(3.9e)

(3.9f)

(3.9g)

(3.9h)

(3.9i)

(3.9j)

(3.9k)

(3.9l)

i , (Ku∗)i(cid:105) = Q(α∗)i(cid:107)(Ku∗)i(cid:107),
i (cid:107) ≤ Q(α∗)i,
uuD(u∗)(cid:124)p + K(cid:124)ϕ = ∇uJ (u∗),

∇uD(u∗) + K(cid:124)q∗ = 0
(cid:104)q∗
(cid:107)q∗
∇2
∇αQ(α∗)(cid:124) (ρ + ν) = 0
0 ≤ Q(α∗) ⊥ ρ ≥ 0
0 ≤ (cid:107)qi(cid:107) ⊥ σi ≥ 0

(cid:107)(Ku∗)i(cid:107) ϕi = Q(α∗)i

(cid:18)

I −

(cid:124)
(Ku)i(Ku)
i
(cid:107)(Ku∗)i(cid:107)2

(cid:19)

(Kp)i,

(Kp)i = 0,
i , (Kp)i(cid:105) = Q(α∗)i(cid:107)(Kp)i(cid:107),
(cid:104)q∗
(cid:104)ϕi, q∗
i (cid:105) ≥ 0,

1

(cid:107)(Ku)i(cid:107)
0,

(cid:107)(Kp)i(cid:107),

νi =

(cid:104)(Ku)i, (Kp)i(cid:105) + σi,

i ∈ I(x∗)

i ∈ A(x∗)
i ∈ B(x∗)

i = 1, . . . , m

i = 1, . . . , m

i = 1, . . . , m

i ∈ I(x∗)

i ∈ A(x∗)
i ∈ B(x∗)
i ∈ B(x∗).

Proof. Since the optimal solution x∗ satisﬁes the optimality system (3.4), we start from there.
Equations (3.9a)-(3.9c) are just the constraints in (2.6), while equations (3.9d)-(3.9e) follow im-
mediatelly from (3.4a) and (3.4b). The same occurs with (3.9f), which is obtained directly from
(3.4h).

Condition (3.9k) follows directly from (3.4e) by taking into account that δi = Q(α)i, for all

i ∈ B(x∗), and using the sign condition (3.5) and the constraints (3.3d). Indeed,

(cid:104)ϕi, q∗

i (cid:105) = Q(α)i cos(θi)ϕx

i + Q(α)i sin(θi)ϕy

i = Q(α)iγi ≥ 0,

∀i ∈ B(x∗).

To verify (3.9i) let us note that

ri = 0,

νi = 0,

∀i ∈ A(x∗).

Consequently, equations (3.4f) and (3.4g) may be written as the linear system

(3.10)

(cid:18) cos(θi)
−δi sin(θi)

sin(θi)
δi cos(θi)

(cid:19) (cid:18)λx
i
λy
i

(cid:19)

(cid:19)

(cid:18)−σi
0

,

=

∀i ∈ A(x∗).

14

J.C. DE LOS REYES AND D. VILLAC´ıS

For all i such that δi > 0, the system matrix becomes orthogonal (after dividing the second row by
δi) and, hence, invertible. Consequently, since σi = 0, for all i ∈ A(x∗) ∩ {i : δi > 0}, we get that
i = 0 and λy
λx

i = 0, for all i ∈ A(x∗) ∩ {i : δi > 0}, which implies that

In addition, on A(x∗) ∩ {i : δi = 0}, it holds ri = 0 and Q(α)i > 0. Consequently, we get that

(Kp)i = 0,

∀i ∈ A(x∗) ∩ {i : δi > 0}.

0 ≤ σi = − cos(θi)λx

i − sin(θi)λy

i = − cos(θi)Kxp − sin(θi)Kyp.

Since this holds for arbitrary angles θi, it follows that (Kp)i = 0 and σi = 0, for all A(x∗) ∩ {i :
δi = 0}.

In a similar manner, on the biactive set B(x∗), ri = 0 and δi = Q(α∗)i, and, thus, we get from

equations (3.4f) and (3.4g)the system

(3.11)

(cid:18) cos(θi)
−δi sin(θi)

sin(θi)
δi cos(θi)

(cid:19) (cid:18)λx
i
λy
i

(cid:19)

(cid:19)

(cid:18)νi − σi
0

,

=

∀i ∈ B(x∗).

Since for any orthogonal matrix U , (cid:107)U x(cid:107)2 = (cid:107)x(cid:107)2, it then follows, using again the sign condition
(3.5), that

0 ≤ νi = (cid:107)λi(cid:107),

∀i ∈ B(x∗) ∩ {i : δi > 0}.

Replacing the latter in (3.4f) and using the representation (3.4c)-(3.4d), we then get that

(cid:104)qi, (Kp)i(cid:105) = Q(α∗)i(cid:107) (Kp)i (cid:107),

∀i ∈ B(x∗) ∩ {i : Q(α∗)i > 0}.

For the characterization of the multiplier ϕ on the inactive set, we consider the system resulting

from equations (3.4e) and (3.4g). Since δi = Q(α∗)i, ri > 0 and γi = 0 on I(x∗), we get

(3.12)

(cid:18) cos(θi)
− sin(θi)

sin(θi)
cos(θi)

(cid:19) (cid:18)riϕx
i
riϕy
i

(cid:19)

(cid:18)

=

−δi sin(θi)λx

0
i + δi cos(θi)λy

i

(cid:19)

,

∀i ∈ I(x∗).

Owing to the orthogonality of the system matrix, we get that

(cid:19)

(cid:18)riϕx
i
riϕy
i

=

(cid:18)cos(θi) − sin(θi)
cos(θi)

(cid:19) (cid:18)

0
i + δi cos(θi)λy
−δi sin(θi)λx
(cid:19)

i

(cid:19)

i

(cid:18)

sin(θi)
(cid:18) sin2(θi)λx
− sin(θi) cos(θi)λx
sin2(θi)
− sin(θi) cos(θi)
(cid:18) cos2(θi)
sin(θi) cos(θi)

i − sin(θi) cos(θi)λy
i + cos2(θi)λy
− sin(θi) cos(θi)
cos2(θi)
sin(θi) cos(θi)
sin2(θi)

I −

(cid:20)

i

= δi

= δi

= δi

(cid:19)

(cid:19) (cid:18)λx
i
λy
i
(cid:19)(cid:21) (cid:18)λx
i
λy
i

(cid:19)

,

∀i ∈ I(x∗).

Since, for all i ∈ I(x∗),

(Ku)i(Ku)

(cid:124)
i = r2
i

(cid:19)
(cid:18)cos(θi)
sin(θi)

(cid:0)cos(θi)

sin(θi)(cid:1) = r2

i

(cid:18) cos2(θi)
sin(θi) cos(θi)

(cid:19)

sin(θi) cos(θi)
sin2(θi)

,

we then get that

(cid:18)

riϕi = δi

I −

(cid:19)

(cid:124)
(Ku)i(Ku)
i
r2
i

λi,

∀i ∈ I(x∗),

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

15

which can also be written as

(cid:107)(Ku∗)i(cid:107) ϕj = Q(α∗)i

(cid:18)

I −

(cid:124)
(Ku)i(Ku)
i
(cid:107)(Ku∗)i(cid:107)2

(cid:19)

(Kp)i,

∀i ∈ I(x∗).

For the characterization of ν, let us ﬁrst notice that, from (3.4j), νi = 0, for all i ∈ A(x∗). On

the inactive set, on the other hand, we know that ri = (cid:107)(Ku)i(cid:107) > 0. Consequently, from (3.4f)

(3.13)

νi = cos(θi) ◦ λx

i + sin(θi) ◦ λy

i + σi,

∀i ∈ I(x∗).

Multiplying with ri both sides of the last equation, we obtain

riνi = ri (cos(θi)λx

i + sin(θi)λy

i ) + riσi = (cid:104)(Ku)i, (Kp)i(cid:105) + riσi,

∀i ∈ I(x∗),

which implies that

νi =

1
(cid:107)(Ku∗)i(cid:107)

(cid:104)(Ku)i, (Kp)i(cid:105) + σi,

∀i ∈ I(x∗).

On B(x∗), since by hypothesis B(x∗)∩{i : Q(α)i = 0} = ∅, we know that σi = 0 and δi = Q(α)i > 0.
Multiplying with δi both sides of (3.13), we get that

δiνi = δi (cos(θi)λx

i + sin(θi)λy

i ) = (cid:104)qi, (Kp)i(cid:105).

Since we know, in addition, that

(cid:104)qi, (Kp)i(cid:105) = Q(α∗)i(cid:107) (Kp)i (cid:107),

∀i ∈ B(x∗),

it follows that νi = (cid:107) (Kp)i (cid:107), for all i ∈ B(x∗).

For MPCC it is well-known that the strong stationarity system (3.4) corresponds to the Karush-
Kuhn-Tucker system of a locally around x∗ relaxed problem [35], which in the case of problem (3.3)
takes the following form:

J (u)

minimize
u,q,α,r,δ,θ
subject to ∇uD(u; f )) + K(cid:124)q = 0,

(Ku)i = ri[cos(θi), sin(θi)](cid:124),
qi = δi[cos(θi), sin(θi)](cid:124),
Q(α) ≥ 0,
δi ≥ 0,
ri = 0,
ri ≥ 0,
ri ≥ 0,

(Q(α)i − δi) ≥ 0,
(Q(α)i − δi) = 0,
(Q(α)i − δi) ≥ 0,

i = 1, . . . , n,

i = 1, . . . , n,

i = 1 . . . , n,
i ∈ A(x∗),
i ∈ I(x∗),
i ∈ B(x∗),

In the case of problem (2.6), using solely the original variables, a similar result, locally around x∗,
can be veriﬁed with the relaxed problem:

(3.14a)

(3.14b)

J (u)

minimize
u,q,α
subject to ∇uD(u; f ) + K(cid:124)q = 0,

16

(3.14c)

(3.14d)

(3.14e)

(3.14f)

(3.14g)

J.C. DE LOS REYES AND D. VILLAC´ıS

qi = Q(α)i

(Ku)i
(cid:107)(Ku)i(cid:107)

,

i ∈ I(x∗),

(Ku)i = 0,
(cid:104)qi, (Ku)i(cid:105) ≥ 0,
(cid:107)qi(cid:107) ≤ Q(α)i,
Q(α) ≥ 0.

i ∈ A(x∗),
i ∈ B(x∗),
i ∈ (A(x∗) ∪ B(x∗)) ∩ {q∗

i (cid:54)= 0},

(cid:54)= 0 and (K¯u)i (cid:54)= 0
Indeed, if ¯x = (¯u, ¯q, ¯α) is an optimal solution to (3.14) such that ¯qi (cid:54)= 0 if q∗
i
if (Ku∗)i (cid:54)= 0, then the KKT system of the relaxed problem coincides with the strong stationarity
system (3.9).

Let us now turn to second-order suﬃcient conditions and let us introduce the Lagrangian for

the MPCC problem (3.3) as follows:

(3.15) L(x∗) = J (u) + (p, −∇uD(u) − K(cid:124)q) + (ϕx, −Kxu + r ◦ cos(θ))

+ (ϕy, −Kyu + r ◦ sin(θ)) + (λx, qx − δ ◦ cos(θ)) + (λy, qy − δ ◦ sin(θ))

− ρ(cid:124)Q(α) − σ(cid:124)δ − γ(cid:124)r − ν(cid:124)(Q(α) − δ).

Since strong stationarity conditions hold under suitable assumptions (see Theorem 3.4), second-
order suﬃcient conditions may be veriﬁed under strong convexity of the Lagrangian for critical
directions [35, 23].
In the next theorem we study under which conditions suﬃciency holds for
strong stationary points of (2.7).

Theorem 3.6. Let x∗ be a strongly stationary point of (2.7) that satisﬁes the hypotheses of

Theorem 3.4. If

(cid:104)∇2

uuJ (u)du, du(cid:105) − (cid:104)∇u(∇2

uuD(u)(cid:124)p)du, du(cid:105) − (cid:107)ϕ(cid:107)2
(cid:124)
i dα(cid:107)2
− (cid:107)Kp(cid:107)2
I(cid:107)∇Q(α)

I∪B(cid:107)K(cid:107)2(cid:107)du(cid:107)2 − (cid:104)∇α(∇Q(α))dα, dα(cid:105)
I +

(δiνi − 2) d2

(δiνi − 1) d2

θ,i +

(cid:88)

(cid:88)

θ,i > 0,

for every non-vanishing critical direction d ∈ LM P CC
strict local minimizer.

X

B
I
(x∗) ∩ {d : ∇J (u∗)(cid:124)du ≤ 0}, then x∗ is a

Proof. Since x∗ satisﬁes the hypotheses of Theorem 3.4, it follows from the same result that x∗

fulﬁlls the MPCC-MFCQ. Consequently, MPCC-RCPLD holds as well (see [23]).

To prove the result, we need to verify that for every non-vanishing critical direction d ∈

LM P CC

X

(x∗) ∩ {d : ∇(cid:96)(x∗)(cid:124)d ≤ 0} and every set of M-multipliers (p, ϕ, λ, ρ, σ, γ, ν),

d(cid:124)∇2

xxL(x∗, p, ϕ, λ, ρ, σ, γ, ν)d > 0.

The critical directions satisfy the following equations for the equality constraints
uuD(u)(cid:124)du − K(cid:124)

xdqx − K(cid:124)

y dqy = 0,

− ∇2

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

− Kxdu + cos(θ) ◦ dr − r ◦ sin(θ) ◦ dθ = 0,
− Kydu + sin(θ) ◦ dr + r ◦ cos(θ) ◦ dθ = 0,
dqx − cos(θ) ◦ dδ + δ ◦ sin(θ) ◦ dθ = 0,
dqy − sin(θ) ◦ dδ − δ ◦ cos(θ) ◦ dθ = 0,

the relations for the inequality restrictions

(3.21)

(cid:124)
i dα ≥ 0,
∇Q(α)

if Q(α)i = 0,

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

17

(3.22)

dδ,i ≥ 0,

the complementarity conditions

if δi = 0,

(3.23)

(3.24)

(3.25)

and

(3.26)

(cid:124)
∇Q(α)
i dα = dδ,i,
dr,i = 0,
(cid:124)
i dα − dδ,i) ⊥ dr,i ≥ 0,
0 ≤ (∇Q(α)

if i ∈ I(x∗),
if i ∈ A(x∗)
if i ∈ B(x∗),

∇uJ (u∗)(cid:124)du ≤ 0.

For the ﬁrst and second derivatives of the Lagrangian with respect to u, in direction du, we

obtain

∇uL(z∗)[du]2 = (cid:104)∇uJ (u) − ∇2
∇2

uuD(u)(cid:124)p − K(cid:124)
uuJ (u)du, du(cid:105) − (cid:104)∇u(∇2

uuL(z∗)[du]2 = (cid:104)∇2

xϕx − K(cid:124)
y ϕy, du(cid:105),
uuD(u)(cid:124)p)(cid:124)du, du(cid:105).

The ﬁrst and second derivatives with respect to α are given by

∇αL(z∗)[dα] = −(cid:104)∇αQ(α)(ρ + ν), dα(cid:105),
∇2

ααL(z∗)[dα]2 = −(cid:104)∇α(∇Q(α))(cid:124)dα, dα(cid:105),

The ﬁrst derivatives of the Lagrangian with respect to r and δ are given by

∇rL(z)[dr] = (cid:104)ϕx ◦ cos(θ) + ϕy ◦ sin(θ) − γ, dr(cid:105)
∇δL(z)[dδ] = −(cid:104)λx ◦ cos(θ) + λy ◦ sin(θ) + σ − ν, dδ(cid:105),

respectively. Deriving both with respect to θ we get

(3.27)

(3.28)

∇2
∇2

rθL[dr, dθ] = (cid:104)(−ϕx ◦ sin(θ) + ϕy ◦ cos(θ)) ◦ dθ, dr(cid:105),
δθL[dδ, dθ] = (cid:104)(λx ◦ sin(θ) − λy ◦ cos(θ)) ◦ dθ, dδ(cid:105).

In addition we get that ∇2

θrL[dθ, dr] = ∇2

rθL[dr, dθ] and ∇2

θδL[dθ, dδ] = ∇2

δθL[dδ, dθ].

From Equation (3.27) we obtain, thanks to equations (3.17), (3.18) and (3.24), that

∇2

rθL[dr, dθ] = (cid:104)r ◦ (ϕx ◦ cos(θ) + ϕy ◦ sin(θ)) ◦ dθ, dθ(cid:105)I + (cid:104)−ϕx ◦ Kydu + ϕy ◦ Kxdu, dθ(cid:105)I∪B.

Using additionally Equation (3.4e) and Equation (3.4k) we then get that

(3.29)

∇2

rθL[dr, dθ] = (cid:104)−ϕx ◦ Kydu + ϕy ◦ Kxdu, dθ(cid:105)I∪B.

From Equation (3.28) we get, using the fact that λx|A = 0, λy|A = 0 and equations (3.4g) and

(3.23), that

(3.30)

∇2

(cid:124)
δθL[dδ, dθ] = (cid:104)(λx ◦ sin(θ) − λy ◦ cos(θ)) ◦ ∇Q(α)
i dα, dθ(cid:105)I.

The ﬁrst and second derivatives with respect to θ are given by

∇θL(z∗)[dθ] = (cid:104)−ϕx ◦ r ◦ sin(θ) + ϕy ◦ r ◦ cos(θ) + λx ◦ δ ◦ sin(θ) − λy ◦ δ ◦ cos(θ), dθ(cid:105) ,
∇2

θθL(z∗)[dθ]2 = (cid:104)(−ϕx ◦ r ◦ cos(θ) − ϕy ◦ r ◦ sin(θ) + λx ◦ δ ◦ cos(θ) + λy ◦ δ ◦ sin(θ)) ◦ dθ, dθ(cid:105) ,

18

J.C. DE LOS REYES AND D. VILLAC´ıS

which, using Equation (3.4e) and Equation (3.4f), implies that

∇2

θθL(z∗)[dθ]2 = (cid:104)(−γ ◦ r + (ν − σ) ◦ δ) ◦ dθ, dθ(cid:105) .

Since γi = 0, for all i ∈ I(x∗), and ri = 0, for all i ∈ A(x∗) ∪ B(x∗), the ﬁrst term on the right
hand side vanishes. Moreover, since 0 ≤ δ ⊥ σ ≥ 0, we obtain

∇2

θθL(z∗)[dθ]2 =

(cid:88)

i∈I∪B

νiδid2

θ,i ≥ 0.

All remaining second partial derivatives are equal to zero.

Altogether we then obtain that

dT ∇2

xxL(x∗, p, ϕ, λ, ρ, σ, γ, ν)d = (cid:104)∇2

uuJ (u)du, du(cid:105) − (cid:104)∇u(∇2

uuD(u)(cid:124)p)du, du(cid:105)

− (cid:104)∇α(∇Q(α))dα, dα(cid:105) + 2(cid:104)−ϕx ◦ Kydu + ϕy ◦ Kxdu, dθ(cid:105)I∪B
(cid:124)
+ 2(cid:104)(λx ◦ sin(θ) − λy ◦ cos(θ)) ◦ ∇αQ(α)
i dα, dθ(cid:105)I + (cid:104)δ ◦ ν ◦ dθ, dθ(cid:105)I∪B.

Using Young’s inequality for the mixed terms, we then obtain

dT ∇2

xxL(x∗, p, ϕ, λ, ρ, σ, γ, ν)d ≥ (cid:104)∇2
(cid:88)
− (cid:104)∇α(∇αQ(α))dα, dα(cid:105) +

uuJ (u)du, du(cid:105) − (cid:104)∇u(∇2
θ,i − (cid:107)ϕ(cid:107)2
δiνid2

I∪B(cid:107)K(cid:107)2(cid:107)du(cid:107)2 − (cid:107)dθ(cid:107)2

I∪B

uuD(u)(cid:124)p)du, du(cid:105)

− (cid:107)Kp(cid:107)2

(cid:124)
i dα(cid:107)2
I(cid:107)∇Q(α)

I∪B
I − (cid:107)dθ(cid:107)2
I,

which completes the proof.

3.1.1. Case with scalar parameter. In the case of a scalar weight, it is known, under weak
data conditions [14], that the optimal parameter α∗ > 0 and, therefore, ρ = 0. Since in this case
Q(α) = α1, it then follows that σi = 0, for all i ∈ I(x∗) ∪ B(x∗). Consequently,

νi =

1
α∗ (cid:104)qi, (Kp)i(cid:105),

∀i ∈ I(x∗) ∪ B(x∗).

The strong stationary system then becomes

(3.31a)

(3.31b)

(3.31c)

(3.31d)

(3.31e)

(3.31f)

(3.31g)

(3.31h)

(3.31i)

∇uD(u∗) + K(cid:124)q∗ = 0
(cid:104)q∗
(cid:107)q∗
∇2

i , (Ku∗)i(cid:105) = α∗(cid:107)(Ku∗)i(cid:107),
i (cid:107) ≤ α∗,
uuD(u∗)(cid:124)p + K(cid:124)ϕ = ∇uJ (u∗),

(cid:88)

(cid:104)qi, (Kp)i(cid:105) = 0,

i∈I(x∗)∪B(x∗)

(cid:107)(Ku∗)i(cid:107) ϕi = α∗

(cid:18)

I −

(cid:124)
(Ku)i(Ku)
i
(cid:107)(Ku∗)i(cid:107)2

(cid:19)

(Kp)i,

(Kp)i = 0,
i , (Kp)i(cid:105) = α∗(cid:107)(Kp)i(cid:107),
(cid:104)q∗
(cid:104)ϕi, q∗

i (cid:105) ≥ 0,

i = 1, . . . , n

i = 1, . . . , n

i ∈ I(x∗),

i ∈ A(x∗),
i ∈ B(x∗),
i ∈ B(x∗).

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

19

3.1.2. Case with scale-dependent parameter. In the case of a scale dependent parameter
α ∈ Rn, the parameter function Q is just the identity matrix. Consequently, Equation (3.9e)
becomes

νi + ρi = 0,

∀i.

On the set A(x∗), both νi = 0 and ρi = 0, and Equation (3.9e) becomes redundant there. Finally,
on the set (I(x∗)∪B(x∗))∩{αi > 0} it follows that ρi = σi = 0 and, therefore, from Equation (3.9e)
and Equation (3.9l),

(cid:104)qi, (Kp)i(cid:105) = 0,

i ∈ (I(x∗) ∪ B(x∗)) ∩ {αi > 0}.

As a consequence, assuming B(x∗)∩{αi = 0} = ∅, the strong stationarity system takes the following
form:

(3.32a)

(3.32b)

(3.32c)

(3.32d)

(3.32e)

(3.32f)

(3.32g)

(3.32h)

(3.32i)

(3.32j)

(3.32k)

(3.32l)

(3.32m)

i (cid:107)(Ku∗)i(cid:107),

∇uD(u∗) + K(cid:124)q∗ = 0
i , (Ku∗)i(cid:105) = α∗
(cid:104)q∗
i (cid:107) ≤ α∗
(cid:107)q∗
i ,
uuD(u∗)(cid:124)p + K(cid:124)ϕ = ∇uJ (u∗),
∇2
0 ≤ α∗ ⊥ ρ ≥ 0
0 ≤ (cid:107)qi(cid:107) ⊥ σi ≥ 0

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

(cid:107)(Ku∗)i(cid:107) ϕi = α∗
i

(cid:18)

I −

(cid:124)
(Ku)i(Ku)
i
(cid:107)(Ku∗)i(cid:107)2

(cid:19)

(Kp)i,

i ∈ I(x∗)

i ∈ A(x∗)
i ∈ B(x∗)

i (cid:107)(Kp)i(cid:107),

(cid:29)

, (Kp)i

0 =

(Kp)i = 0,
i , (Kp)i(cid:105) = α∗
(cid:104)q∗
(cid:28) (Ku)i
(cid:107)(Ku)i(cid:107)
(cid:104)qi, (Kp)i(cid:105) = 0,
σi, ρi = 0,
(cid:104)ϕi, q∗

i (cid:105) ≥ 0,

+ σi + ρi,

i ∈ I(x∗) ∩ {αi = 0}

i ∈ (I(x∗) ∪ B(x∗)) ∩ {αi > 0}
i ∈ B(x∗)
i ∈ B(x∗).

For Gaussian denoising and tracking type loss function, if a condition on the total variation of
the noisy and ground truth images is fulﬁlled, we also know from [14] that αi > 0, for all i, in which
case the optimality system simpliﬁes even fruther. Indeed, in such case, ρ = 0 and Equation (3.32j)
can be dismissed.

3.2. General Bilevel Problem. The ideas developed before for the bilevel total variation case
can be extended to the general problem (2.15), involving several data ﬁdelity terms, as well as
diﬀerent regularizers. In the next result, existence of Lagrange multipliers for the general problem
(2.15) is veriﬁed, and M- and S-stationarity conditions are derived.

To formulate the result, let us introduce the following active, inactive and biactive sets for the

Euclidean norm regularizers:

j (x∗) := {i = 1, . . . , mj : rj,i = 0, Qj(αj)i > δj,i},
Aα
j (x∗) := {i = 1, . . . , mj : rj,i > 0, Qj(αj)i = δj,i},
I α
j (x∗) := {i = 1, . . . , mj : rj,i = 0, Qj(αj)i = δj,i},
Bα

j = 1, . . . , M,

j = 1, . . . , M,

j = 1, . . . , M.

20

J.C. DE LOS REYES AND D. VILLAC´ıS

and the active, inactive and biactive sets for the Frobenius norm regularizers:

Aβ
j (x∗) := {i = 1, . . . , nj : ρj,i = 0, Sj(βj)i > τj,i},
I β
j (x∗) := {i = 1, . . . , nj : ρj,i > 0, Sj(βj)i = τj,i},
Bβ
j (x∗) := {i = 1, . . . , nj : ρj,i = 0, Sj(βj)i = τj,i},

j = 1, . . . , N,

j = 1, . . . , N,

j = 1, . . . , N.

Theorem 3.7. Let x∗ = (u∗, c∗, q∗, Λ∗, λ∗, σ∗, α∗, β∗, r∗, δ∗, ρ∗, τ ∗, θ∗, φ∗, ϕ∗) be a local optimal
solution to (2.15). Assume that ∇Qj(α∗
j ), j = 1, . . . , N, are full-rank.
There exist Lagrange multipliers such that, together with equations (2.15b)-(2.15o), the adjoint
equation:

j ), j = 1, . . . , M, and ∇Sj(β∗

(3.33) ∇J (u∗) −

K
(cid:88)

j=1

Pj(λj) ◦ ∇2

uuDj(u)T pj −

M
(cid:88)

j=1

K(cid:124)

j π −

N
(cid:88)

j=1

E(cid:124)
j ζ

L
(cid:88)

(−cj − Rj(σj)) ◦ Bjµ−

B,j +

L
(cid:88)

(−cj + Rj(σj)) ◦ Bjµ+

B,j −

+

j=1

j=1

the relation between the adjoint state p and the dual variables’ multipliers:

(3.34)

(3.35)

(3.36)

B,j + µ+

B,j) − µ−

σ,j + µ+

σ,j − (Bju) ◦ µc,j = 0,

− Bjpj − (Bju) ◦ (µ−
ϑj = Kjpj,
ηj = Ejpj,

the gradient type equations:

(3.37)

(3.38)

(3.39)

(3.40)

∇Dj(u)(cid:124)∇Pj(λj)pj − ∇Pj(λj)µP,j = 0,
Bju ◦ ∇Rj(σj) ◦ (µ+
∇Qj(αj) ◦ (µQ,j + να
∇Sj(βj) ◦ (µS,j + νβ

B,j − µ−
j ) = 0,
j ) = 0,

B,j) − ∇Rj(σj) ◦ (µ−

σ,j + µ+

σ,j + µR,j) = 0,

the relations between change of variables’ multipliers:

(3.41)

(3.42)

(3.43)

(3.44)

cos(θj) ◦ πjx + sin(θj) ◦ πy
j + sin(θj) ◦ ϑy
cos(θj) ◦ ϑx
(cid:16)
cos(ϕj) ◦ ζ x
(cid:16)

sin(φj) ◦

j − γα
j = 0,
j + µδ,j − να
j = 0,
(cid:17)
j + sin(ϕj) ◦ ζ y

j

cos(ϕj) ◦ ηx

j + sin(ϕj) ◦ ηy

j

− sin(φj) ◦

+ cos(φj) ◦ ζ z
(cid:17)

j − γβ

j = 0,

(3.45)

− rj ◦

j − µτ,j + νβ
j − cos(θj) ◦ πy

j = 0,
(cid:17)

j

− cos(φj) ◦ ηz
(cid:16)

sin(θj) ◦ πx
(cid:16)

+ δj ◦

sin(θj) ◦ ϑx
(cid:16)

cos(ϕj) ◦ ζ x

j − cos(θj) ◦ ϑy
j + sin(ϕj) ◦ ζ y

j

j

(cid:17)

= 0,
(cid:17)

(3.46)

ρj ◦ cos(φj) ◦

− τj ◦ cos(φj) ◦

(cid:16)

cos(ϕj) ◦ ηx

j + sin(ϕj) ◦ ηy

j

+ τj ◦ sin(φj) ◦ ηz

j = 0,

− ρj ◦ sin(φj) ◦ ζ z
j
(cid:17)

L
(cid:88)

j=1

cj ◦ Bjµc,j = 0,

j = 1, . . . , L,

j = 1, . . . , M,

j = 1, . . . , N,

j = 1, . . . , K,

j = 1, . . . , L,

j = 1, . . . , M,

j = 1, . . . , N,

j = 1, . . . , M,

j = 1, . . . , M,

j = 1, . . . , N,

j = 1, . . . , N,

j = 1, . . . , M,

j = 1, . . . , N,

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

21

(3.47)

− ρj ◦ sin(φj) ◦

(cid:16)

j − cos(ϕj) ◦ ζ y

j

(cid:17)

sin(ϕj) ◦ ζ x
(cid:16)

+ τj ◦ sin(φj) ◦

sin(ϕj) ◦ ηx

j − cos(ϕj) ◦ ηy

j

(cid:17)

= 0,

j = 1, . . . , N,

the complementarity conditions for the absolute value terms:

(3.48)

(3.49)

(3.50)

(3.51)

(3.52)

(3.53)

B,j ≥ 0,
B,j ≥ 0,

0 ≥ (−cj − Rj(σj)) ◦ (Bju) ⊥ µ−
0 ≥ (−cj + Rj(σj)) ◦ (Bju) ⊥ µ+
0 ≥ (−cj − Rj(σj)) ⊥ µ−
0 ≥ (−cj + Rj(σj)) ⊥ µ+
0 ≤ cj(Bju)◦ ⊥ µc,j ≥ 0,
0 ≤ Rj(σj) ⊥ µR,j ≥ 0,

σ,j ≥ 0,
σ,j ≥ 0,

the complementarity relations for the positivity constraints

(3.54)

(3.55)

(3.56)

(3.57)

(3.58)

0 ≤ Qj(αj) ⊥ µQ,j ≥ 0,
0 ≤ δj ⊥ µδ,j ≥ 0,
0 ≤ Sj(βj) ⊥ µS,j ≥ 0,
0 ≤ τj ⊥ µτ,j ≥ 0,
0 ≤ Pj(λj) ⊥ µP,j ≥ 0,

and the M-stationarity conditions

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , M,

j = 1, . . . , M,

j = 1, . . . , N,

j = 1, . . . , N,

j = 1, . . . , K,

(3.59)

(3.60)

(3.61)

(3.62)

(3.63)

(3.64)

να
j,i = 0,
γα
j,i = 0,
j,iνα
γα
νβ
j,i = 0,
γβ
j,i = 0,
j,iνβ
γβ

j,i = 0 ∨ γα

j,i ≥ 0, να

j,i ≥ 0,

j,i = 0 ∨ γβ

j,i ≥ 0, νβ

j,i ≥ 0,

∀i ∈ Aα
∀i ∈ I α
∀i ∈ Bα
∀i ∈ Aβ
∀i ∈ I β
∀i ∈ Bβ

j (x∗),
j (x∗),
j (x∗),
j (x∗),
j (x∗),
j (x∗),

j = 1, . . . , M

j = 1, . . . , M

j = 1, . . . , M

j = 1, . . . , N

j = 1, . . . , N

j = 1, . . . , N.

hold. Moreover, if the triactive sets

M
(cid:91)

j=1

(cid:0)Bα

j (x∗) ∩ {i : Qj(αj)i = 0}(cid:1) = ∅

and

N
(cid:91)

(cid:16)

j=1

Bβ
j (x∗) ∩ {i : Sj(βj)i = 0}

(cid:17)

= ∅,

then equations (3.61)-(3.64) may be replaced by

(3.65)

(3.66)

γα
j,i ≥ 0, να
j,i ≥ 0, νβ
γβ
and x∗ corresponds to a S-stationary point.

j,i ≥ 0,
j,i ≥ 0,

∀i ∈ Bα
∀i ∈ Bβ

j (x∗),
j (x∗),

j = 1, . . . , M

j = 1, . . . , N

Proof. Let us introduce the equality constraints functions by

hD(x) = −





L
(cid:88)

lj
(cid:88)

j=1

i=1

(B(cid:63)

j cj)i +

M
(cid:88)

mj
(cid:88)

j=1

i=1

(K(cid:63)

j qj)i +

N
(cid:88)

nj
(cid:88)

j=1

i=1

(E(cid:63)

j Λj)i +

K
(cid:88)

kj
(cid:88)

j=1

i=1



Pj(λj)i∇uDj(u)i

 ,

22

J.C. DE LOS REYES AND D. VILLAC´ıS

hK(x) =










−(Kx
−(Ky

1u) + r1 ◦ cos(θ1)
1u) + r1 ◦ sin(θ1)

...

−(Kx
−(Ky

M u) + rM ◦ cos(θM )
M u) + rM ◦ sin(θM )










,

hq(x) =










qx
1 − δ1 ◦ cos(θ1)
qy
1 − δ1 ◦ sin(θ1)
...
qx
M − δM ◦ cos(θM )
qy
M − δM ◦ sin(θM )










,

hE(x) =














−(Ex
−(Ey

1u) + ρ1 ◦ sin(φ1) ◦ cos(ϕ1)
1u) + ρ1 ◦ sin(φ1) ◦ sin(ϕ1)
−(Ez
1u) + ρ1 ◦ cos(φ1)

...

−(Ex
−(Ey

N u) + ρN ◦ sin(φN ) ◦ cos(ϕN )
N u) + ρN ◦ sin(φN ) ◦ sin(ϕN )
−(Ez
N u) + ρN ◦ cos(φN )














,

hΛ(x) =














Λz

Λx
1u − τ1 ◦ sin(φ1) ◦ cos(ϕ1)
Λy
1u − τ1 ◦ sin(φ1) ◦ sin(ϕ1)
1u − τ1 ◦ cos(φ1)
...
N u − τN ◦ sin(φN ) ◦ cos(ϕN )
N u − τN ◦ sin(φN ) ◦ sin(ϕN )
N u − τN ◦ cos(φN )

Λz

Λx
Λy














,

In a similar manner, we introduce the inequality constraints functions

g±
B,j(x) = −cj ◦ (Bju) ± Rj(σj) ◦ (Bju),
g±
σ,j(x) = −Rj(σj) ± cj,
gc,j(x) = −cj ◦ (Bju),
gR,j(x) = −Rj(σj),
gQ,j(x) = −Qj(αj),
gδ,j(x) = −δj,
gS,j(x) = −Sj(βj),
gτ,j(x) = −τj,
gP,j(x) = −Pj(λj),

and the complementarity constraints functions

Mr(x) =




 , Mρ(x) =






r1
...
rM






ρ1
...
ρM

j = 1, . . . , L

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , L,

j = 1, . . . , M,

j = 1, . . . , M,

j = 1, . . . , N,

j = 1, . . . , N,

j = 1, . . . , K,




 ,

NQ(x) =






Q1(α1) − δ1
...
QM (αM ) − δM




 , NS(x) =






S1(α1) − τ1
...
SN (βN ) − τN






Similar to the proof of Theorem 3.4 it can be veriﬁed that the columns of the gradients of
the equality constraints ∇hD(x), ∇hK(x), ∇hq(x), ∇hE(x), ∇hΛ(x) are linearly independent, even
when δj = rj = 0 or when ρj = τj = 0. The detailed computed matrices are provided in Section 6,
from where this statement can be easily veriﬁed. Moreover, linear independence with respect to the
block columns of the matrices [∇g±
σ,j(x)], [∇gc,j(x)], [∇gR,j(x)], [∇gQ,j(x)], [∇gδ,j(x)],
[∇gS,j(x)], [∇gτ,j(x)], [∇gP,j(x)], [∇Mr,j(x)], [∇Mρ,j(x)], [∇NQ,j(x)] and [∇NS,j(x)] can be veriﬁed
form the structure of the corresponding matrices.

B,j(x)], [∇g±

Let us now introduce the index sets

Zj := {i = 1, . . . , mj : δj,i = Qj(αj)i = 0} = {Qj(αj)i = 0},

j = 1, . . . , M,

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

23

Yj := {i = 1, . . . , nj : τj,i = Sj(βj)i = 0} = {Sj(βj)i = 0},

j = 1, . . . , N,

and consider the linear system

∇hD(x∗)vD + ∇hK(x∗)vK + ∇hq(x∗)vq + ∇hE(x∗)vE + ∇hΛ(x∗)vΛ

(cid:16)

(cid:16)

+

+

∇g+

∇g+

(cid:17)

B,j(x∗)
(cid:17)
σ,j(x∗)

{g+

B,j,i(x∗)=0}

{g+

σ,j,i(x∗)=0}

(cid:16)

w+

B,j +

∇g−

(cid:16)

w+

σ,j +

∇g−

(cid:17)

B,j(x∗)
(cid:17)
σ,j(x∗)

{g−

B,j,i(x∗)=0}

{g−

σ,j,i(x∗)=0}

w−
B,j

w−
σ,j

(3.67)

+ (∇gc,j(x∗)){gc,j,i(x∗)=0} wc,j + (∇gR,j(x∗)){gR,j,i(x∗)=0} wR,j
+ (∇gQ,j(x∗)){gQ,j,i(x∗)=0} wQ,j + (∇gδ,j(x∗)){gδ,j,i(x∗)=0} wδ,j
+ (∇gS,j(x∗)){gS,j,i(x∗)=0} wS,j + (∇gτ,j(x∗)){gτ,j,i(x∗)=0} wτ,j
+ (∇gP,j(x∗)){gP,j,i(x∗)=0} wP,j − (∇Mr,j(x∗))Aα
j (x∗) γρ,j − (∇NQ,j(x∗))Iα
− (∇Mρ,j(x∗))Aβ
− (∇NS,j(x∗))Iβ
j (x∗) νS,j = 0,

j (x∗)∪Bβ

j (x∗)∪Bβ

j (x∗)∪Bα

j (x∗)∪Bα

j (x∗) γr,j
j (x∗) νQ,j

where we used the Einstein summation convention. From the structure of the matrices, the solution
to Equation (3.67) then satisﬁes:

vD, vK, vq, vE, vΛ = 0,

(cid:17)

(cid:16)

w±
B,j

{g+

B,j,i(x∗)=0}

= 0,

j = 1, . . . , L,

(cid:17)

(cid:16)

w±
σ,j

{g+

σ,j,i(x∗)=0}

= 0,

j = 1, . . . , L,

(wc,j){gc,j,i(x∗)=0} = 0,

j = 1, . . . , L,

(wR,j){gR,j,i(x∗)=0} = 0,
(wτ,j){gτ,j,i(x∗)=0}\Yj
(cid:0)γα
j

= 0,

(cid:1)

j (x∗)∪Bα

j (x∗)

Aα

= 0,

j = 1, . . . , L,

j = 1, . . . , N,

j = 1, . . . , M,

= 0,

(wδ,j){gδ,j,i(x∗)=0}\Zj
(wP,j){gP,j,i(x∗)=0} = 0,
(cid:16)
γβ
j

= 0,

(cid:17)

Aβ

j (x∗)∪Bβ

j (x∗)

j = 1, . . . , M,

j = 1, . . . , L,

j = 1, . . . , N,

(cid:1)

(cid:0)να
j

(Iα

j (x∗)∪Bα

j (x∗))\Zj

= 0,

j = 1, . . . , M,

(cid:17)

(cid:16)

νβ
j

(Iβ

j (x∗)∪Bβ

j (x∗))\Yj

= 0,

j = 1, . . . , N.

Additionally, on each Zj we get

(3.68)

− ∇Qj(αj)(wQ,j + να

j ) = 0

and wδ,j − να

j = 0

Since ∇Qj(αj) is full-rank, the unique possible non-negative solution is having all multipliers equal
to zero. Similarly, on each Yj we obtain

(3.69)

− ∇Sj(βj)(wS,j + νβ

j ) = 0

and wρ,j − νβ

j = 0

and, also from the fact that each ∇Sj(βj) is full-rank, wQ,j = wδ,j = να
j = 0 is the unique possibility.
Consequently, positive linear independence holds and, thus, MPCC-MFCQ is satisﬁed. Therefore,
there exist Lagrange multipliers such that the M-stationary system (3.33)-(3.64) is fulﬁlled.

Let us consider system (3.67) again. If Bα

Qj(α∗)j,i > 0, for all i ∈ Bα
j = 0, for all i ∈ Bα
j = να
γα
j,i = Sj(β∗)j,i > 0, for all i ∈ Bβ
τ ∗
we get that partial MPCC-LICQ holds and Theorem 3.3 may be applied.

j,i =
j (x∗). Consequently, the solution to Equation (3.67) necessarily satisﬁes
j )i = 0} = ∅, it follows that
j (x∗). Altogether,

j (x∗) ∩ {i : Sj(β∗
j = νβ

j (x∗) and, therefore, γβ

j )i = 0} = ∅, it follows that δ∗

j (x∗). Similarly, if Bβ

j = 0, for all i ∈ Bβ

j (x∗) ∩ {i : Qj(α∗

24

J.C. DE LOS REYES AND D. VILLAC´ıS

4. Remark on the Inﬁnite-Dimensional Case. Let us now consider the inﬁnite-dimensional

Total Variation Gaussian denoising model given by
(cid:90)

(cid:90)

Ω

µ
2

(4.1a)

|∇u|2 dx +

minimize
u

λ
2
where f ∈ L∞(Ω), Ω ⊂ R2 is a convex domain and 0 < µ (cid:28) 1 is an elliptic regularization
parameter. In this case the unique solution to the denoising problem belongs to H 1(Ω). Moreover,
the solution is characterized by the existence of a dual multiplier q ∈ L2(Ω; R2), such that the
following extremality conditions are satisﬁed:
−µ∆u + λ(u − f ) + ∇(cid:124)q = 0,

|u − f |2 dx +

|∇u| dx,

(4.2a)

Ω

Ω

(cid:90)

(4.2b)

(4.2c)

(cid:104)q(x), ∇u(x)(cid:105) = (cid:107)∇u(x)(cid:107),

(cid:107)q(x)(cid:107) ≤ 1,

a.e. in Ω

a.e. in Ω

Thanks to the convexity of Ω, extra regularity results hold for this problem. In particular we

get that q ∈ [L∞(Ω)]2 and ∇u ∈ [L∞(Ω)]2.

Although, intuitively, a similar change of variables as in problem 2.7 may be used in this case, a
careful treatment must be carried out due to the inﬁnite-dimensional structure of the problem. In
the following result, the change of variables is justiﬁed by means of the inverse mapping theorem.
Theorem 4.1. Let (cid:15) > 0 and let δ∗ ∈ L∞(Ω), θ∗ ∈ L∞(Ω) satisfy δ∗(x) ≥ (cid:15) > 0. Then there
exists an open neighbourhood V of q∗(x) := δ∗(x)[cos(φ∗(x)), sin(φ∗(x))](cid:124) in [L∞(Ω)]2 , an open
neighbourhood W of (δ∗, θ∗) in [L∞(Ω)]2 and an implicit function G ∈ C1([L∞(Ω)]2; [L∞(Ω)]2)
such that G(V ) ⊂ W and

{(δ, θ, q) ∈ V × W : q = δ[cos(φ, sin(φ](cid:124)} = {(δ, θ, q) ∈ V × W : (δ, θ) = G(q)}.

Proof. Let F denote the operator

F : L∞(Ω) × L∞(Ω) → L∞(Ω) × L∞(Ω)

(δ, θ) (cid:55)→ δ[cos(θ), sin(θ)]

In a neighbourhood of (δ∗, θ∗), F (δ, θ) is well-deﬁned. Moreover, since it is a Nemytskii superpo-
sition operator, and the functions sin(·) and cos(·) are C∞, F is continuously diﬀerentiable [39,
Lemma 4.12]. The derivative of F with respect to (δ, θ), in direction (dδ, dθ), is given by

∇δ,θF [dδ, dθ] = −

(cid:18)cos(θ) − sin(θ)
cos(θ)

sin(θ)

(cid:19) (cid:18) dδ
δdθ

(cid:19)

.

The matrix is orthogonal for a.a. x ∈ Ω and also uniformly bounded. Since by hypothesis δ(x) >
(cid:15) ≥ 0 a.e. in Ω, it follows that for any (ϕ1, ϕ2), there exists a unique solution (h1, h2) to the system

(cid:18)cos(θ∗) − sin(θ∗)
cos(θ∗)

sin(θ∗)

(cid:19) (cid:18) h1
δ∗h2

(cid:19)

=

(cid:19)

.

(cid:18)ϕ1
ϕ2

Applying the inverse mapping theorem [1, Thm. 1.2], there are constants r, r0 > 0 and an
implicit function G such that for each q with (cid:107)q − q∗(cid:107)L∞(Ω) ≤ r0 there exist δ, θ ∈ L∞(Ω), with
(cid:107)(δ, θ) − (δ∗, θ∗)(cid:107)[L∞(Ω)]2 ≤ r such that (δ, θ) = G(q). Moreover,
(cid:18)h1
h2

(cid:18) cos(θ)
−δ sin(θ)

sin(θ)
δ cos(θ)

(cid:19) (cid:18)ϕ1
ϕ2

=

(cid:19)

(cid:19)

and G is continuously diﬀerentiable.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

25

A similar result holds for the change of variables

∇u(x) = r(x)[cos(φ(x)), sin(φ(x))](cid:124).

in Ω, we obtain, similarly as for the ﬁnite-
By using the collinearity condition θ(x) = φ(x) a.e.
dimensional case, that the following complementarity conditions are fulﬁlled by the auxiliary vari-
ables:

δ(x) ≥ 0,

0 ≤ r(x) ⊥ (1 − δ(x)) ≥ 0,

a.e. in Ω,

a.e. in Ω.

With this reformulation it is possible to write the bilevel problem as:

(4.3)

minimize
u,q,λ,r,δ,θ

J (u)

subject to
− µ∆u + λ(u − f )) + ∇(cid:124)q = 0,
∇u(x) = r(x)[cos(θ(x)), sin(θ(x))](cid:124),
q(x) = δ(x)[cos(θ(x)), sin(θ(x))](cid:124),
λ ≥ 0,

δ(x) ≥ 0,

0 ≤ r(x) ⊥ (1 − δ(x)) ≥ 0,

a.e. in Ω,

a.e. in Ω,

a.e. in Ω,

a.e. in Ω,

a.e. in Ω.

The constraints in (4.3) involve pointwise constraints on the Euclidean norm of the gradient of u
and also on the Euclidean norm of the dual multiplier q, which are in addition in complementarity
to each other. Although there are several contributions on PDE-constrained optimization problems
involving state constraints, pointwise constraints on the gradient of the state are particularly diﬃ-
cult to handle; even more so if the state variables are also in pointwise complementarity. Problem
(4.3) is indeed a very challenging one that requires a detailed treatment, and is a matter of future
research.

5. Experiments. In this section we report on some experimental results from the implementa-
tion of the reformulated optimization problem (2.7), using the python library Pyomo [24], which is
based on the NLP solver IPOPT [41].

For solving this problem, we consider a BFGS update of the second-order matrix and rely
on the nonlinear problem transformation for complementarity problems provided in Pyomo. This
transformation is based on the work of Ferris et al. [20] where the complementarity conditions

0 ≤ ri ⊥ Q(α)i − δi ≥ 0, ∀i = 1, . . . , m,

are relaxed using a tolerance (cid:15) > 0 in the following form:

Q(α)i − δi ≥ 0,
ri ≥ 0,
rivi ≤ (cid:15),

∀i = 1, . . . , m,

∀i = 1, . . . , m,

∀i = 1, . . . , m.

The value of (cid:15) allows for the speciﬁcation of a relaxed problem which may be easier to optimize.

26

J.C. DE LOS REYES AND D. VILLAC´ıS

5.1. Optimal parameter learning for total variation image denoising. We ﬁrst explore the
case of a scalar regularization parameter α, which implies Q(α)i = α, for all i = 1, . . . , m. For this
purpose, we will make use of the Cameraman dataset. It is a training pair of the classic 128 by
128 pixels cameraman image and a synthetic added noisy image corrupted by zero-mean gaussian
noise with standard deviation 0.05 and 0.1.

We start by testing the inﬂuence of the relaxation parameter on the computed solution. Search-
ing for a scalar regularization parameter α, we can see a degradation of the quality of the recon-
structed image for the cameraman dataset as (cid:15) is chosen smaller (see Table 1).

(cid:15)

α∗

l2

PSNR

SSIM

0.5
0.1
0.05
0.03
0.02
0.01
0.001
0.0001

0.578 35
0.174 63
0.121 95
0.100 58
0.087 63
0.079 15
0.063 71
0.060 78

11.070 00
11.878 09
14.466 02
17.475 19
19.306 25
23.914 11
37.846 35
38.670 26

31.702 72
31.396 73
30.540 71
29.719 98
29.287 22
28.357 66
26.363 96
26.270 43

0.955 44
0.956 13
0.950 38
0.944 43
0.937 31
0.923 72
0.880 32
0.869 22

Table 1: Reconstruction quality comparison for diﬀerent relaxation parameters (cid:15)

Since the method used for solving this problem is iterative, it is worth exploring the eﬀects of
the initial values, in particular our choice of the initial parameter α0. In Table 2 we observe that the
method is robust regarding the choice of the initial condition, i.e., the quality of the reconstruction
remains consistent across diﬀerent initializations.

α0

10.000
1.000
0.100
0.010
0.001

α∗

0.063 48
0.064 71
0.065 08
0.064 70
0.065 64

l2

PSNR

SSIM

37.496 88
36.265 76
37.971 00
35.755 87
36.343 50

26.404 25
26.549 23
26.349 68
26.610 73
26.539 93

0.881 48
0.881 21
0.874 18
0.881 35
0.880 24

Table 2: Reconstruction quality comparison of diﬀerent initialization parameter α0

The initial condition chosen to solve this complementarity problem plays, however, an important
role in the convergence speed of the method, specially when dealing with a large number of variables
and constraints. For instance, in the case of the cameraman dataset, we deal with 81921 variables
and 98304 constraints. To speed up the computation, we make use of a warm-start strategy
described in Algorithm 5.1

Now, as a generalization of the scalar problem, we implement a patch-dependent version. It
implies that we now deﬁne a patch operator Q : Rd → Rn that takes a parameter α ∈ Rd with
d << n to a new parameter that aﬀects all pixels. The eﬀect of this mapping on an image is shown
in Figure 3.

In Figure 4 the eﬀect of increasing the number of patches in the image on the quality of the
reconstruction can be veriﬁed. In particular, when moving from 4x4 to 32x32 patches. Even though
giving the parameter more degrees of freedom allows it to adjust the regularization parameter to

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

27

Algorithm 5.1 Warm Start Strategy
1: Choose initial parameter α0 and tol > 0
2: Setup the resolution of the original pair R
3: Obtain the value of the reconstruction u by solving the variational problem with α = α0
4: Resize training image pair to three diﬀerent smaller resolutions: r1 < r2 < r3 = R
5: for r ∈ {r1, r2, r3} do
6:

Solve the complementarity problem (2.7) for r with initial conditions αr−1 and ur−1, then
obtain the solution αr and ur
Set α = αr and u = ur

7:
8: end for
9: return Optimal values for α and u.

Original

SSIM=0.7721

SSIM=0.5463

SSIM=0.9386
α∗ = 0.0261

SSIM=0.8692
α∗ = 0.0607

Figure 2: Cameraman Dataset
Optimal reconstructions using a scalar regularization parameter with (cid:15) = 1e−4.

speciﬁc regions in the image, it may result in an overﬁtted learned parameter. By restricting
the dimension of this patch parameter we are able to obtain better generalization properties as
described in [17].

As a ﬁnal note, it is worth mentioning the size of the generated Pyomo model. As detailed in
Table 3, we can see that the lowest number of variables appear for models with scalar parameters and
grows along with the size of the patch parameter. Likewise, the execution time increases accordingly.
Furthermore, the increasing size of the image leads to large-scale optimization problems that are
much harder to deal with computationally.

28

J.C. DE LOS REYES AND D. VILLAC´ıS

α1

α3

α2

α4

α1

α2

α3

α4

Original

Parameter

Patch Parameter 2x2

Figure 3: Patch Operator
Mapping of a patch parameter.

Model

Variables Constraints Execution Time (s)

cameraman scalar
cameraman 4
cameraman 8
cameraman 16
cameraman 32

81921
81036
81984
82176
82944

98304
98304
98304
98304
98304

787
449
942
513
953

Table 3: Size of the diﬀerent total variation models and its execution time for noise with standard deviation 0.1.

5.2. Optimal parameter learning for joint isotropic and anisotropic total variation. In this
experiment we consider an image denoising model consisting of a combination of the anisotropic
and isotropic total variation regularizers, i.e., we solve

(5.1a)

minimize
α,σ∈R+

(cid:107)u − utrue(cid:107)2

1
2

(5.1b)

subject to u = arg min

v

(cid:107)v − f (cid:107)2 +

1
2

m
(cid:88)

i=1

R(σ)i(|(Kxv)i| + |(Kyv)i|) +

m
(cid:88)

i=1

Q(α)i(cid:107)(Kv)i(cid:107),

where Kx and Ky are ﬁnite diﬀerences discretizations of the partial derivatives along the horizontal
and vertical coordinates, respectively. This problem can be seen as a special case of the general
formulation and it may be reformulated according to subsection 2.3.

For testing this model we chose the Woven Silk dataset. This single image dataset was built
using a scanning electron micrograph of woven silk fabric described in [37]. This grayscale image
is 128x128 pixels and, as done previously, the training pair is built by adding zero mean gaussian
noise with standard deviation 0.05 and 0.1. It is worth noting that this model includes several new
constraints and variables when compared to the pure isotropic total variation model, as shown in
Table 4.

Using the same workﬂow as in the previous experiment, let us start considering scalar regular-
ization parameters α and σ. In Figure 5 we can see the values for the optimal parameters α∗, σ∗,
its corresponding optimal reconstruction along with their SSIM quality reconstruction measures for
both noise levels.

Now, changing our choice of parameter to patch-dependent parameters α and σ, we can ﬁnd
optimal parameters for both the anisotropic and isotropic total variations that are scale-dependent.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

29

SSIM=0.9482

α

SSIM=0.8797

α

SSIM=0.9544

α

SSIM=0.8938

α

SSIM=0.9566

α

SSIM=0.8992

α

SSIM=0.9603

α

SSIM=0.9087

α

Figure 4: Cameraman Dataset
Optimal reconstructions using a patch-based regularization parameter.

Figure 6 shows the image reconstructions and the optimal parameter structure for α∗ and σ∗
along with their respective SSIM quality reconstruction metrics. We can point out that the spatial
adaptation of both regularization parameters lead to a better reconstruction quality, specially when
one type of regularizer is preferred for a speciﬁc portion of the image, as shown in the structure of
both parameters displayed in the Figure 6.

Finally, a depiction of the model sizes and the corresponding execution time is presented in

30

J.C. DE LOS REYES AND D. VILLAC´ıS

Original

SSIM=0.9290

SSIM=0.7956

SSIM=0.9683
α∗ = 0.0434
σ∗ = 2.2e−6

SSIM=0.9141
α∗ = 0.0708
σ∗ = 0.0018

Figure 5: Woven Silk Dataset
Optimal reconstructions using a scalar regularization parameter.

Table 4. Here, we can see that this model contains a larger number of variables and constraints
when compared to the pure total variation model, with a growing number of variables as the size
of the patch parameter grows. Again, the large number of variables appearing even in the case of
a 128x128 image, leads to a very large-scale optimization problem.

Model

Variables Constraints Execution Time (s)

woven silk scalar
woven silk 4
woven silk 8

114690
114720
114720

262144
262144
262144

40912
16421
16421

Table 4: Size of the diﬀerent implemented models and its execution time for noise with standard deviation 0.1.

6. Appendix. Gradient matrices, proof of Theorem 3.7. To simplify the presentation, we
introduce the notation 0a to denote a matrix of zeros with |a| rows and full column width. Similarly
we denote by Ia the identity matrix of size |a|. Further, we denote by tdiag([aj, bj, cj], [m, n, p]) the
tridiagonal matrix with the vectors aj, bj and cj, of the same size, located in the diagonals m, n
and p, respectively, and introduce the following:

bdiag([a1, b1], [0, n]) :=



a11




. . .

b11

. . .






a1M

b1M

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

31

SSIM=0.9356

SSIM=0.9642

α

SSIM=0.8055

SSIM=0.9285

α

σ

σ

Figure 6: Patch Dependent Woven Silk Dataset
Optimal reconstructions using a patch-based regularization parameter.

tdiag([a1, b1, c1], [0, n, p]) :=



a11




. . .

b11

c11

. . .

. . .






a1M

b1M

c1M

We also deﬁne the following block diagonal versions of the bidiagonal and tridiagonal matrices:

Bdiag1:N ([aj, bj], [0, mj]) =









bdiag([a1, b1], [0, m1])
0
...
0

0
bdiag([a2, b2], [0, m2])

. . .
. . .
. . .

0
0









bdiag([aN , bN ], [0, mN ])

Tdiag1:N ([aj, bj, cj], [0, nj, pj]) =




. . .



tdiag([a1, b1, c1], [0, nj, pj])






tdiag([aN , bN , cN ], [0, nj, pj])

32

J.C. DE LOS REYES AND D. VILLAC´ıS

The columns of the equality constraints gradients are then given by



− (cid:80)K

j=1 Pj(λj) ◦ ∇uuDj(u)(cid:124)

∇hD(x) =
































−B1
...
−BL
−K1
...
−KM
−E1
...
−EN

−∇D1(u)(cid:124)P (cid:48)

1(λ1)

...

−∇DK (u)(cid:124)P (cid:48)
K (λK )
0σ,α,β,r,δ,ρ,τ,θ,φ,ϕ


































, ∇hK(x) =





















−(K x

M )(cid:124)

1 )(cid:124)

1 )(cid:124) −(K x

M )(cid:124) −(K x

. . . −(K x
0c,q,Λ
0λ,σ,α,β
Bdiag1:N ([cos(θj), sin(θj)], [0, mj])
0δ,ρ,τ
Bdiag1:M ([−rj sin(θj), rj cos(θj)], [0, mj])
0φ,ϕ





















,

∇hq(x) =





















0u,c
Iq
0Λ,λ,σ,α,β,r
Bdiag1:M ([− cos(θj), − sin(θj)], [0, mj])
0ρ,τ
Bdiag1:M ([−δj sin(θj), −δj cos(θj)], [0, mj])
0φ,ϕ





















,

∇hE(x) =

















N )(cid:124)

−(Ex

1 )(cid:124) −(Ey

1 )(cid:124) −(Ez

N )(cid:124) −(Ey

N )(cid:124) −(Ez

1 )(cid:124)
. . . −(Ex
0c,q,Λ,λ,σ,α,β,r,δ
Tdiag1:N ([sin(φj) cos(ϕj), sin(φj) sin(ϕj), cos(φj)], [0, nj, 2nj])
0τ,θ
Tdiag1:N ([ρj cos(φj) cos(ϕj), ρj cos(φj) sin(ϕj), −ρj sin(φj)], [0, nj, 2nj])
Bdiag1:N ([−ρj sin(φj) sin(ϕj), ρj sin(φj) cos(ϕj)], [0, nj])

















,

∇hΛ(x) =





















0u,c,q
IΛ
0λ,σ,α,β,r,δ,ρ
Tdiag1:N ([− sin(φj) cos(ϕj), − sin(φj) sin(ϕj), − cos(φj)], [0, nj, 2nj])
0θ
Tdiag1:N ([−τj cos(φj) cos(ϕj), −τj cos(φj) sin(ϕj), τj sin(φj)], [0, nj, 2nj])
Bdiag1:N ([τj sin(φj) sin(ϕj), −τj sin(φj) cos(ϕj)], [0, nj])





















,

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

33

The gradients of the inequality constraints are given by

(−c1 ± R1(σ1))B1
−diag(B1u)

0
0

±diag(B1u ◦ R(cid:48)

1(σ1))

∇g±

B (x) =





















(−cL ± RL(σL))BL
0

0
−diag(BLu)

0

0

±diag(BLu ◦ R(cid:48)

L(σL))





















,

. . .
. . .
. . .
. . .
0q,Λ,λ
. . .
. . .
. . .
0

, ∇gP (x) =













−diag(P (cid:48)

1(λ1))

0
0

0

0u,c,q,Λ
. . .
. . .
0
. . . −diag(P (cid:48)
K (λK ))
0













,




















0
0

0

0
−diag(R(cid:48)

L(σL))

0
0



diag(cL ◦ BL)
0

. . .
. . .
. . .
. . . −diag(BLu)
0

0












, ∇gR(x) =












−diag(R(cid:48)

1(σ1))

0u,c,q,Λ,λ
. . .
. . .
. . .
0

0

0
−diag(R(cid:48)

L(σL))

0u,c,q,Λ,λ,σ
. . .
. . .
0
. . . −diag(Q(cid:48)
M (αM ))

0













, ∇gS(x) =











−diag(S(cid:48)

1(β1))

0

0u,c,q,Λ,λ,σ,α
. . .
. . .
0
. . . −diag(S(cid:48)
N (βN ))
0

0

∇gδ(x) =






0
−Iδ
0ρ,τ,θ,φ,ϕ




 , ∇gτ (x) =






0
−Iτ
0θ,φ,ϕ

0
0




 .












,













,

0u
±Ic
0q,Λ,λ
. . .
. . .
. . .
0




















∇g±

σ (x) =

−diag(R(cid:48)

1(σ1))

0
0

0
0

diag(c1 ◦ B1)
−diag(B1u)

0
0














−diag(Q(cid:48)

1(α1))

∇gc(x) =

∇gQ(x) =











For the complementarity functions, the gradients are given by

∇Mr(x) =






0
−Ir
0ρ,τ,θ,φ,ϕ






 , ∇Mρ(x) =









0
−Iρ
0θ,φ,ϕ

34

and

∇NQ(x) =




















J.C. DE LOS REYES AND D. VILLAC´ıS

diag(Q(cid:48)

1(α1))

0

−diag(S(cid:48)

1(β1))

0
0

0
diag(Q(cid:48)
M (αM ))

0
0




















, ∇NS(x) =




















0

0u,c,q,Λ,λ,σ,α
. . .
. . .
0
. . . −diag(S(cid:48)
N (βN ))
0r,δ,ρ
−Iτ
0θ,φ,ϕ




















.

0u,c,q,Λ,λ,σ
. . .
. . .
. . .
0β,r
−Iδ
0ρ,τ,θ,φ,ϕ

References.

[1] Antonio Ambrosetti and Giovanni Prodi. A primer of nonlinear analysis. 34. Cambridge

University Press, 1995.

[2] S¨oren Bartels and Nico Weber. “Parameter learning and fractional diﬀerential operators:
application in image regularization and decomposition”. In: arXiv preprint arXiv:2001.03394
(2020).

[3] Kristian Bredies, Yiqiu Dong, and Michael Hinterm¨uller. “Spatially dependent regularization
parameter selection in total generalized variation models for image restoration”. In: Interna-
tional Journal of Computer Mathematics 90.1 (2013), pp. 109–123.

[4] Kristian Bredies, Karl Kunisch, and Thomas Pock. “Total generalized variation”. In: SIAM

Journal on Imaging Sciences 3.3 (2010), pp. 492–526.

[5] Luca Calatroni, Juan Carlos De Los Reyes, and Carola-Bibiane Sch¨onlieb. “Dynamic sam-
pling schemes for optimal noise learning under multiple nonsmooth constraints”. In: IFIP
Conference on System Modeling and Optimization. Springer. 2013, pp. 85–95.

[6] Luca Calatroni and Kostas Papaﬁtsoros. “Analysis and automatic parameter selection of a
variational model for mixed Gaussian and salt-and-pepper noise removal”. In: Inverse Prob-
lems 35.11 (2019), p. 114001.

[7] Luca Calatroni et al. “Bilevel approaches for learning of variational imaging models”. In:

Variational Methods. Walter de Gruyter GmbH, 2017, pp. 252–290.

[8] Antonin Chambolle and Pierre-Louis Lions. “Image recovery via total variation minimization

and related problems”. In: Numerische Mathematik 76.2 (1997), pp. 167–188.

[9] T. Chan, A. Marquina, and P. Mulet. “High-Order Total Variation-Based Image Restoration”.

In: 22.2 (2000), pp. 503–516. doi: 10.1137/S1064827598344169.

[10] M D’Elia, JC De Los Reyes, and A Miniguano-Trujillo. “Bilevel Parameter Learning for
Nonlocal Image Denoising Models”. In: Journal of Mathematical Imaging and Vision (2021),
pp. 1–23.

[11] Elisa Davoli, Irene Fonseca, and Pan Liu. “Adaptive image processing: ﬁrst order PDE
constraint regularizers and a bilevel training scheme”. In: arXiv preprint arXiv:1902.01122
(2019).

[12] Elisa Davoli and Pan Liu. “One dimensional fractional order TGV: gamma convergence and
bilevel training scheme”. In: Communications in Mathematical Sciences 16.1 (2018), pp. 213–
237.

[13] Juan Carlos De los Reyes. Numerical PDE-Constrained Optimization. Springer, 2015.
[14] Juan Carlos De los Reyes, C-B Sch¨onlieb, and Tuomo Valkonen. “The structure of opti-
mal parameters for image restoration problems”. In: Journal of Mathematical Analysis and
Applications 434.1 (2016), pp. 464–500.

BILEVEL IMAGING LEARNING PROBLEMS AS MPCC

35

[15] Juan Carlos De los Reyes and Carola-Bibiane Sch¨onlieb. “Image denoising: learning the noise
model via nonsmooth PDE-constrained optimization”. In: Inverse Problems & Imaging 7.4
(2013), pp. 1183–1214.

[16] Juan Carlos De los Reyes and David Villacis. “Optimality Conditions for Bilevel Imaging
Learning Problems with Total Variation Regularization”. In: arXiv preprint arXiv:2107.08100
(2021).

[17] Juan Carlos De los Reyes and David Villac´ıs. “Bilevel Optimization Methods in Imaging”.
In: Handbook of Mathematical Models and Algorithms in Computer Vision and Imaging 33.7
(2021), p. 074005.

[18] Stephan Dempe. Foundations of bilevel programming. Springer Science & Business Media,

2002.
I. Ekeland and R. Temam. Convex analysis and variational problems. SIAM, 1999.

[19]
[20] Michael C Ferris, Steven P Dirkse, and Alexander Meeraus. “Mathematical programs with
equilibrium constraints: Automatic reformulation and solution via constrained optimization”.
In: Frontiers in applied general equilibrium modeling (2005), pp. 67–93.

[21] Michael Flegel, Christian Kanzow, and Jiri Outrata. “Optimality conditions for disjunctive
programs with application to mathematical programs with equilibrium constraints”. In: Set-
Valued Analysis 15.2 (2007), pp. 139–162.

[22] Michael L Flegel and Christian Kanzow. “On the Guignard constraint qualiﬁcation for math-
ematical programs with equilibrium constraints”. In: Optimization 54.6 (2005), pp. 517–534.
[23] Lei Guo, Gui-Hua Lin, and J Ye Jane. “Second-order optimality conditions for mathematical
programs with equilibrium constraints”. In: Journal of Optimization Theory and Applications
158.1 (2013), pp. 33–64.

[24] William E Hart, Jean-Paul Watson, and David L Woodruﬀ. “Pyomo: modeling and solving
mathematical programs in Python”. In: Mathematical Programming Computation 3.3 (2011),
pp. 219–260.

[25] Michael Hinterm¨uller, Konstantinos Papaﬁtsoros, and Carlos N Rautenberg. “Analytical as-
pects of spatially adapted total variation regularisation”. In: Journal of Mathematical Analysis
and Applications 454.2 (2017), pp. 891–935.

[26] Michael Hinterm¨uller and Kostas Papaﬁtsoros. “Generating structured nonsmooth priors and
associated primal-dual methods”. In: Handbook of Numerical Analysis. Vol. 20. Elsevier, 2019,
pp. 437–502.

[27] Michael Hinterm¨uller and Carlos N Rautenberg. “Optimal selection of the regularization
function in a weighted total variation model. Part I: Modelling and theory”. In: Journal of
Mathematical Imaging and Vision 59.3 (2017), pp. 498–514.

[28] Michael Hinterm¨uller and Tao Wu. “Bilevel optimization for calibrating point spread functions

in blind deconvolution”. In: Inverse Problems & Imaging 9.4 (2015).

[29] Florian Knoll et al. “Second order total generalized variation (TGV) for MRI”. In: Magnetic

resonance in medicine 65.2 (2011), pp. 480–491.

[30] Karl Kunisch and Thomas Pock. “A bilevel optimization approach for parameter learning in
variational models”. In: SIAM Journal on Imaging Sciences 6.2 (2013), pp. 938–983.
[31] Z.-Q. Luo, J.-S. Pang, and D. Ralph. Mathematical programs with equilibrium constraints.

Cambridge University Press, 1996.

[32] Adri´an Mart´ın and Emanuele Schiavi. “Automatic Total Generalized Variation-Based DTI
Rician Denoising”. In: Image Analysis and Recognition. Vol. 7950. Lecture Notes in Computer
Science. Springer Berlin Heidelberg, 2013, pp. 581–588. isbn: 978-3-642-39093-7. doi: 10 .
1007/978-3-642-39094-4 66.

36

J.C. DE LOS REYES AND D. VILLAC´ıS

[33] Simon Masnou and J-M Morel. “Level lines based disocclusion”. In: Image Processing, 1998.

ICIP 98. Proceedings. 1998 International Conference on. IEEE. 1998, pp. 259–263.

[34] Jiri V Outrata. “A generalized mathematical program with equilibrium constraints”. In:

SIAM Journal on Control and Optimization 38.5 (2000), pp. 1623–1638.

[35] Holger Scheel and Stefan Scholtes. “Mathematical programs with complementarity constraints:
Stationarity, optimality, and sensitivity”. In: Mathematics of Operations Research 25.1 (2000),
pp. 1–22.

[36] Otmar Scherzer. Handbook of mathematical methods in imaging. Springer Science & Business

Media, 2010.

[37] Sericin and Fibroin SPECIFIC Stain used to characterize SILK FILAMENTS. url: https:

//vartest.com/sericin-and-ﬁbroin-speciﬁc-stain-used-to-characterize-silk-ﬁlaments.

[38] Marshall F Tappen. “Utilizing variational optimization to learn markov random ﬁelds”. In:
2007 IEEE Conference on Computer Vision and Pattern Recognition. IEEE. 2007, pp. 1–8.
[39] Fredi Tr¨oltzsch. Optimal control of partial diﬀerential equations: theory, methods, and appli-

cations. Vol. 112. American Mathematical Soc., 2010.

[40] Cao Van Chung, JC De los Reyes, and CB Sch¨onlieb. “Learning optimal spatially-dependent
regularization parameters in total variation image denoising”. In: Inverse Problems 33.7
(2017), p. 074005.

[41] Andreas W¨achter and Lorenz T Biegler. “On the implementation of an interior-point ﬁlter
line-search algorithm for large-scale nonlinear programming”. In: Mathematical programming
106.1 (2006), pp. 25–57.

[42] Jane J. Ye. “Necessary and suﬃcient optimality conditions for mathematical programs with
equilibrium constraints”. In: Journal of Mathematical Analysis and Applications 307.1 (2005),
pp. 350–369.

