0
2
0
2

t
c
O
4
2

]
L
M

.
t
a
t
s
[

3
v
8
5
3
2
1
.
0
1
9
1
:
v
i
X
r
a

Dual Instrumental Variable Regression

Krikamol Muandet
Max Planck Institute for Intelligent Systems
T¨ubingen, Germany
krikamol@tuebingen.mpg.de

Arash Mehrjou
Max Planck Institute for Intelligent Systems
ETH Z¨urich, Z¨urich, Switzerland
arash.mehrjou@inf.ethz.ch

Si Kai Lee
Booth School of Business
University of Chicago, USA
sikai.lee@chicagobooth.edu

Anant Raj
Max Planck Institute for Intelligent Systems
T¨ubingen, Germany
anant.raj@tuebingen.mpg.de

Abstract

We present a novel algorithm for non-linear instrumental variable (IV) regression,
DualIV, which simpliﬁes traditional two-stage methods via a dual formulation.
Inspired by problems in stochastic programming, we show that two-stage pro-
cedures for non-linear IV regression can be reformulated as a convex-concave
saddle-point problem. Our formulation enables us to circumvent the ﬁrst-stage
regression which is a potential bottleneck in real-world applications. We develop
a simple kernel-based algorithm with an analytic solution based on this formula-
tion. Empirical results show that we are competitive to existing, more complicated
algorithms for non-linear instrumental variable regression.

1 Introduction

Inferring causal relationships under the inﬂuence of unobserved confounders remains one of the
most challenging problems in economics, health care, and social sciences [1, 2]. A typical example
in economics is the study of returns from schooling [3], which attempts to measure the causal effect
of education on labor market earnings. For each individual, the treatment variable X represents
the level of education and the outcome Y represents how much they earn. However, one’s level
of education and income is likely confounded by the socioeconomic status or other unobserved
confounding factors H [1, Ch. 4].

Since randomized control trials are often infeasible in most economic studies, economists have
turned to instrumental variables (IVs) or instruments derived from naturally occurring random ex-
periments to overcome unobserved confounding. Informally, instrumental variables Z are deﬁned
as variables that are associated with the treatment X, affect the outcome Y only through X and do
not share common causes with Y . For instance, the season-of-birth was used as an instrument in
[4] to estimate the impact of compulsory schooling on earnings. Because of the compulsory school
attendance laws, an individual’s season-of-birth, which is likely to be random, affects how long they
actually remain in school, but not their earnings. Figure 1 illustrates this example. Finding valid
instruments for speciﬁc problems is an essential task in econometrics [1] and epidemiology [5].

Although IV analysis is widely used, the statistical tools employed for estimating causal effect are
fairly rudimentary. Most applications of instrumental variables utilise a two-stage procedure [1, 6–
8]. For instance, the two-stage least squares (2SLS) relies on the assumption that the relationship
between X and Y is linear [9].
Z = z] via linear
regression and then regresses Y on the estimate of E[X
Z = z] to obtain an estimate of the causal
effect. Since the ﬁrst-stage estimate is by construction independent from confounders, the resultant

It ﬁrst estimates the conditional mean E[X

|

|

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
season of birth

education

income

socioeconomic status

Z

X

Y

H

Figure 1: A data generating process (DGP) with a hidden confounder H and an instrument Z. A
variation in X comes from both H and Z. Intuitively speaking, the external source of variation from
Z can help improve an estimation by removing the effect of H on X.

causal estimate is therefore free from hidden confounding. In the non-linear setting, however, a
poorly-ﬁtted ﬁrst-stage regression may result in inaccurate second-stage estimates [1, Ch. 4.6].

In this paper, we propose a novel procedure, DualIV, to directly estimate the structural causal func-
tion. Unlike previous works which extend 2SLS by employing non-linear models in place of their
linear counterparts [7, 8], we solve the dual problem which can be expressed as a convex-concave
saddle-point problem. Based on this framework, we develop a consistent reproducing kernel Hilbert
spaces-based (RKHS) algorithm. Our formulation was inspired by the mathematical resemblance of
non-linear IV to two-stage problems in stochastic programming [10–12].

The rest of the paper is organized in the following manner. Section 2 introduces the IV regression
problem, reviews related work and identiﬁes current limitations. We present our formulation in
Section 3, followed by the kernelized estimation method in Section 4. Then, Section 5 reports
empirical comparisons between DualIV and existing algorithms. Finally, we discuss the limitations
of our procedure and suggest future directions in Section 6. All proofs can be found in Appendix E.

2 Instrumental variable regression

, respectively. In this work, we assume that

Let X, Y , and Z be treatment, outcome, and instrumental variable(s) taking values in

, and
are Polish spaces. We also
Z
Y ∈
assume that Y is bounded, i.e.,
almost surely. Moreover, we denote unobserved
∞
confounder(s) by H. The underlying data generating process (DGP) is described by the causal
graph in Figure 1 equipped with the following structural causal model (SCM):

< M <

R, and

and

Z

X

X

Y

Y

|

|

,

Y = f (X) + ε, E[ε] = 0,

(1)

where f is an unknown, potentially non-linear continuous function and ε denotes the additive noise
which depends on the hidden confounder(s) H. If E[ε
X] = 0, we can estimate f consistently from
observational data via the standard least-square regression. This allows us to identify E[Y
do(X =
x)] where do(X = x) represents an intervention on X where its value is set to x [13].

|

|

For non-expert readers, we elaborate that do(X = x) here denotes a mathematical operator which
simulates physical interventions by setting the value of X to x, while keeping the rest of the model
unchanged [14, Sec. 3.2.1]. That is, the conditional expectation E[Y
do(X = x)] is computed with
respect to the interventional distribution P(Y
do(X = x)) if
it is possible to directly manipulate X and then observe the resulting outcome Y . In Figure 1, for
instance, one may assign different levels of education to people and then observe their subsequent
levels of income in the labor market. Unfortunately, such experiment is not always possible and
we only have access to an observational distribution P(Y
X = x), which can be different from
P(Y
do(X = x)). The discrepancy between interventional and observational distributions may
result from the unobserved socioeconomic status, as illustrated in Figure 1.

do(X = x)). We can estimate P(Y

|

|

|

|

|

|

When hidden confounders exist between X and Y , the error term ε in (1) is generally correlated
with X. Hence, E[ε

= 0 and it follows from (1) that

X]

E[Y

X = x] = f (x) + E[ε

X = x],

(2)

|

|

|

= E[Y

do(X = x)]

which implies that E[Y
X = x]. Thus, standard least-square regression no
longer provides a valid estimate of f for making a prediction about the outcome of an intervention
on X [7, 8, 15]. To handle hidden confounders, we assume access to an instrumental variable(s)
Z which satisﬁes the following assumptions: (i) Relevance: Z has a causal inﬂuence on X. (ii)
Exclusion restriction: Z affects Y only through X, i.e., Y
X, ε. (iii) Unconfounded instru-
ment(s): Z is independent of the error, i.e., ε

⊥⊥

Z.

Z

|

|

⊥⊥

2

6
6
The properties of Z imply that E[ε
Z yields the following integral equation

|

Z] = 0. Taking the expectation of (1) w.r.t. Y conditioned on

E[Y

Z] =

|

Z

X

f (x) dP(x
|

Z),

(3)

which is a Fredholm integral equation of the ﬁrst kind. Recent works in nonparametric IV regression
have adopted this perspective [7, 8, 15, 16] despite the fact that solving (3) directly is an ill-posed
problem as it involves inverting linear compact operators [15, 17, 18].

To illustrate the role of an instrument, we consider two special cases. When X is perfectly correlated
with Z, the treatment is uncorrelated with the hidden confounder. In other words, we recover the
strong ignorability assumption [19, 20] required for causal inference. When Z is independent of X,
the instrument is useless as it has no predictive power over treatment so the structural function f is
unidentiﬁable from the data. Therefore, the most interesting cases lie between these two extremes,
especially when X and Z are weakly correlated, see, e.g., [21, 22][1, pp. 205–216].

2.1 Previous work

Early applications of instrumental variables often assume linear relationships between Z and X as
well as X and Y [1, 23]. When there is a single endogeneous variable and instrument, the structural
parameter can be estimated consistently by the instrumental variable (IV) estimator [23]. Interest-
ingly, we can obtain this estimate using a two-stage procedure: regress X on Z using ordinary
least square (OLS) to calculate the predicted value of X and used that as an explanatory variable
in the structural equation to estimate the structural parameter using OLS. When there are multiple
instruments, the two-stage least squares (2SLS) estimator is obtained by using all the instruments
simultaneously in the ﬁrst-stage regression. Wooldridge [24, Theorem 5.3] asserts that the 2SLS
estimator is the most efﬁcient IV estimator; see, e.g., [1, 24] for a detailed exposition.

Recently, several extensions of 2SLS have been proposed to overcome the linearity constraint. The
ﬁrst line of work replaces linear regression by a linear projection onto a set of known basis functions
[15, 16, 25, 26]. Chen and Christensen [27] provides a uniform convergence rate of this approach.
However, there exists no principled way of choosing the appropriate set of basis functions. The
second line of work replaces the ﬁrst-stage regression by a conditional density estimate of P(X
Z)
[28, 29]. Despite being more ﬂexible, such approaches are known to suffer from the curse of dimen-
sionality [30, Ch. 1]. Other extensions of 2SLS are DeepIV [8] and KernelIV [7] algorithms. In [8],
(3) is solved by ﬁrst estimating P(X
Z) with a mixture of deep generative models on which f is
|
learned using another deep neural network. Instead of neural networks, Singh et al. [7] proposes to
model the ﬁrst-stage regression using the conditional mean embedding of P(X
Z) [31–33] which is
then used in the second-stage kernel ridge regression.

|

|

The curse of two-stage methods. Two-stage procedures have two fundamental issues. First, such
procedures violate Vapnik’s principle [34]: “[...] when solving a problem of interest, do not solve
a more general problem as an intermediate step [...]”. Speciﬁcally, estimating the conditional den-
sity [8] or the conditional mean embedding [7] via regression in the ﬁrst stage can be harder than
estimating the parameter of interest in the second stage. The ﬁrst stage is even referred as the “for-
bidden regression” in econometrics [1, Ch. 4.6]. On top of that, we usually only observe a single
sample from each P(X
Z = z), which further increases the difﬁculty of the task. Second, although
two-stage procedures are asymptotically consistent, the ﬁrst-stage estimate creates a ﬁnite-sample
bias in the second-stage estimate [1, Sec. 4.6.4]. This bias can be alleviated through sample splitting
[35] which is also used in [7, 8]. Thus, two-stage procedures are less sample efﬁcient and could
yield biased estimates when run on the smaller datasets common in economics and social sciences.

|

The generalized method of moments (GMM) framework provides another set of popular approaches
for estimating f [36, 37]. Unlike two-stage procedures, GMM-based algorithms ﬁnd a function
f that satisﬁes the orthogonality condition E[ε
Z] = 0 directly. Speciﬁcally, if g1, g2, . . . , gm are
arbitrary real-valued functions, the orthogonality condition implies that E[(Y
f (X))gj(Z)] = 0
for j = 1, . . . , m. The GMM estimate of f can then be obtained by minimizing the quadratic form
1
f (X))g(Z)]. This estimator can be interpreted as a
2
generalization of the 2SLS estimator in the linear setting [6]. Recently, extensions of GMM-based
methods where both f and g are parameterized by deep neural networks have successfully been

j=1 ψ(f, gj)2 where ψ(f, g) := E[(Y

P

−

−

m

|

3

used to solve non-linear IV regression [38, 39]. In contrast, Muandet et al. [40] considers the set of
RKHS functions which allow for an analytic formulation of the orthogonality condition.

3 Dual IV

In this section, we reformulate the integral equation (3) as an empirical risk minimization problem
and present DualIV algorithm.

3.1 Empirical risk minimization

R

Let ℓ : R
×
its ﬁrst argument.1 Let
that fulﬁlls the integral equation (3). Then, we can formulate (3) as

R+ be a proper, convex, and lower semi-continuous loss function for any value in
be an arbitrary class of continuous functions which we assume contains f

→

F

min
f
∈F

R(f ) := EYZ

ℓ(Y, EX
(cid:2)
where R(f ) denotes the expected risk of f . To understand how (3) and (4) are related, let us consider
z[f (X)]. Then, the solution to (4) is the
the squared loss ℓ(y, y′) = (y
−
minimum mean square error (MMSE) estimator h∗(z) := E[Y
z], which is exactly the LHS of (3).
z], we use h∗(z) as the best MMSE approximation.
If there exists no f

y′)2 and deﬁne h(z) := EX

for which h∗(z) = E[Y

Z[f (X)])
(cid:3)

(4)

,

|

|

|

∈ F

|

The key challenge is if f is noncontinuous in h(z), it is not guaranteed to be consistently estimated
even if h(z) is estimated correctly [15]. We defer further discussion to Section 3.4. In addition, it
remains cumbersome to solve (4) directly because of the inner expectation. To circumvent this, we
look to similar two-stage problems in stochastic programming [10, 11]. For example, in [11], the
problem of learning from conditional distributions was formulated in a similar fashion to (4). More-
over, [12] proposes the deconditional mean embedding (DME) which solves the integral equation
(3) by performing a closed-form “inversion” of the conditional mean embedding of P(X
Z) (see
[33, 42] for a review). In contrast, we solve the equivalent dual formulation of (4) instead of (3).

|

3.2 Dual formulation

To derive the dual of (4), we employ two existing results, interchangeability and Fenchel duality,
which we review; see, e.g., [11, Lemma 1], [43, Ch. 14], and [10, Ch. 7] for more details.
Theorem 1 (Interchangeability). Let ω be a random variable on Ω and, for any ω
f (
·

) is proper and upper semi-continuous concave function. Then,

Ω, the function

,
(
−∞

, ω) : R

∞

→

∈

Eω

f (u, ω)

max
R
u
∈
is the entire space of functions deﬁned on the support Ω.

= max
u(
)
∈U
·

Eω[f (u(ω), ω)],

(Ω)

(cid:21)

(cid:20)
R

{

U

) : Ω

(Ω) :=

where
u(
·
Deﬁnition 2 (Fenchel duality). Let ℓ : R
R+ be a proper, convex, and lower semi-continuous
loss function for any value in its ﬁrst argument and ℓ⋆
)
·
which is also proper, convex, and lower semi-continuous w.r.t. the second argument. Then, ℓy(v) =
maxu

) a convex conjugate of ℓy := ℓ(y,

. The maximum is achieved at v

∂ℓ⋆(u), or equivalently u

y := ℓ⋆(y,

∂ℓ(v).

uv

→

→

×

R

}

·

{

∈
Applying the interchangeability and Fenchel duality to (4) yields the expected loss

−

∈

ℓ⋆
y(u)
}

(5)

EX
R(f ) = EYZ [max
R {
u
∈
EYZ [EX
|
EXYZ [f (X)u(Y, Z)]

|
Z [f (X)]u(Y, Z)

Z[f (X)]u

−

= max
u
∈U
= max
u
∈U

−

]

ℓ⋆
Y (u)
}
ℓ⋆
Y (u(Y, Z))]

−
EYZ [ℓ⋆

Y (u(Y, Z))]

where

U

is the space of continuous functions over

min
f
∈F

max
u
∈U

EXYZ [f (X)u(Y, Z)]

Y × Z

−

. Hence, (4) can be reformulated as
EYZ [ℓ⋆

Y (u(Y, Z))] .

(6)

1The function f is proper if dom f 6= ∅ and f (x) > −∞, ∀x ∈ X . It is lower (upper) semi-continuous
at x0 ∈ X if for ε > 0 there exists a neighborhood N (x0) of x0 such that ε < (>)f (x) − f (x0) for all
x ∈ N (x0) [41].

4

Following [11], we will refer to u
the outcome Y and the instrument Z, but not the treatment X.

∈ U

as the dual function. Note that this function depends on only

The advantages of our formulation (6) over (3) and (4) are twofold. First, there is no need to estimate
EX
Z[f (X)] or P(X
Z) explicitly. Second, the target function f appears linearly in (6) which makes
it convex in f . Since ℓ⋆
y is also convex, (6) is concave in the dual function u. Hence, (6) is essentially
a convex-concave saddle-point problem for which efﬁcient solvers exist [11].

|

|

y′)2, we have ℓ⋆
For the squared loss ℓ(y, y′) = (y
derivation) and the saddle-point problem (6) reduces to

−

y(w) = wy + 1

2 w2 (see Appendix A for the

min
f
∈F

max
u
∈U

Ψ(f, u) := EXYZ [(f (X)

Y )u(Y, Z)]

−

1
2

−

EYZ

u(Y, Z)2
(cid:2)

(cid:3)

.

(7)

To solve (7), one can adopt an SGD-based algorithm developed by Dai et al. [11]. Alternatively, we
propose in Section 4 a simple algorithm that can solve (7) in closed form.

3.3 Interpreting the dual function

The dual function u(y, z) plays an important role in our framework. To understand its role, we
consider the minimization and maximization problems in (7) separately. For any f
, the maxi-
∈ F
2
L2(PYZ ) where the ﬁrst term
mization problem is maxu
can be viewed, loosely speaking, as a loss function and the second as a regularizer. Intuitively,
we are seeking u∗
that is least orthogonal to the residual. Given u∗, the outer minimization
Y )u∗(Y, Z)] ﬁnds the function f that yields the most orthogonal
problem minf
residual to u∗. Our procedure clearly differs from previous two-stage methods as the minimization
and maximization stages are interdependent.

∈ U
EXYZ [(f (X)

EXYZ [(f (X)

u(Y, Z)
k

Y )u(Y, Z)]

1
2 k

∈F

−

−

−

∈U

Examining the formulation in the context of instrumental variable regression, the residual contains
the variation that cannot be explained by the current estimate of f due to hidden confounding.
We select u that maximally reweights the residuals according to how inconsistent they are w.r.t.
the unconfounded joint distribution of Y and Z. Given u, we then select f that minimizes the
inconsistencies between the residuals and u. Hence, at the equilibrium, we are left with residuals
uncorrelated with Y and Z which can be attributed to noise due to unobserved confounding.

∈U

−

Y × Z

Ψ(f, u) = 1

and ψ(f, g) := E[(Y

f (X))g(Y, Z)]. When
2 ψ⊤Λ−

Lastly, we draw a connection between (7) and GMM. Let g1, g2, . . . , gm be real-valued func-
tions on
, it
}
g] with
is not difﬁcult to show that maxu
g := (g1(Y, Z), . . . , gm(Y, Z))⊤; see Appendix B. That is, minimizing the above over f yields
a formulation that strongly resembles the GMM objective, with the dual function u(Y, Z) playing a
role similar to that of an instrument. However, we must clarify that u cannot act as an instrument
since it depends on Y and thereby violates the exclusion restriction assumption. Liao et al. [44,
Appendix F], using an alternative formulation similar to (4) and (7), showed that one can obtain a
dual function u(Z) that can act as an instrument. Furthermore, we also note that AGMM [38] and
DeepGMM [39] rely on minimax optimization, similar to (7), but were formulated based on the
GMM framework.

1ψ where Λ := EYZ [g

= span
{

g1, . . . , gm

⊗

U

3.4 Theoretical analysis

This section provides the conditions for which the true structural function f ∗ can be identiﬁed by the
optimum of the saddle-point problem (7). We lay out the assumptions needed for the optimal dual
function u∗ to be unique and continuous, show that the saddle-point formulation (7) is equivalent to
the problem (4) under the squared loss and prove that the solution of (7) given u∗ is indeed f ∗.
Assumption 1. (i) P(X
Z) is continuous in Z for any values of X. (ii) The function class
|
correctly speciﬁed, i.e., f ∗

F

is

.

∈ F

Following [11], we deﬁne the optimal dual function for any pair (y, z)
arg maxu
z[f (X)
−
{
u∗(y, z) takes the form EX
|
continuously differentiable, it follows from [11, Proposition 1] that u∗ is unique and continuous.

∈
. Since this is an unconstrained quadratic program,
y. Given Assumption 1 and the loss function ℓ is convex and

y]u
−
z[f (X)]

as u∗(y, z)

∈ Y × Z

(1/2)u2

EX

−

}

∈

R

|

5

5

0

-5

4

2

0

-2

-4

Data
True
OLS
Estimated

-6

-4

-2

0

2

-6

-4

Data
True

-2

5

0

-5

5

0

-5

-4

-2

0

Figure 2: The dual function u w.r.t. the current estimate f in the linear setting (8). For each y and z,
u can directly measures the discrepancy between y and EX

z[f (X)].

|

Next, we shows that if (f ∗, u∗) is the saddle-point of (7), f ∗ minimizes the original objective (4).
The result follows from plugging u∗ = EX
y into the dual loss Ψ(f, u) in (7); see
z[f (X)]
Appendix E.1 for the detailed proof.
Proposition 3. Let ℓ(y, y′) = 1

y′)2. Then, for any ﬁxed f , we have R(f ) = maxu Ψ(f, u).

−

|

2 (y

−

By Proposition 3 and the convexity of the loss ℓ(y, y′), we obtain the following result.
Theorem 4. Let ℓ(y, y′) = 1
saddle-point of a minimax problem minf

y′)2 and assume that Assumption 1 holds. Then, (f ∗, u∗) is the

Ψ(f, u).

maxu

2 (y

−

∈F

∈U

|

|
∈ F

By virtue of Theorem 4, we can identify the true function f ∗ under relatively weak assumptions.
In contrast, previous work usually require stronger assumptions such as the completeness condition
[7, 15] which speciﬁes that the ﬁrst-stage conditional expectation EX
z[f (X)] is injective, or h(z) =
EX
z[f (X)] is a smooth function of z [7, 26, 27]. Since we do not perform ﬁrst-stage regression,
we only require P(X
Z) is continuous in Z for any value of X. The assumption that (4) is correctly
speciﬁed, i.e., f ∗
, is standard in the literature [7, 15, 16].
As we can see, the optimal dual function u∗(y, z) = EX
y acts as a residual function
measuring the discrepancy between y and EX
z[f (X)] [11]. Remarkably, this makes it possible to
approximate R(f ) in (4) without computing the expectation EX
z[f (X)] explicitly. We later exploit
this property when performing hyperparameter selection. Moreover, since EX
z[f (X)] allows X
and Z to have a non-linear relationship, u can be non-linear even when the true structural function
f ∗ is linear. This ﬂexibility enables u to accommodate a larger class of functions that maps Z to X.
Figure 2 illustrates this given the following generative process:

z[f (X)]

−

|

|

|

|

|

Y = Xβ + e + ǫ, X = (1

ρ)Z1 + ρe + η

−
where e
(0, 0.1). The parameter ρ controls
(0, 0.1), and η
∼ N
the strength of the instrument w.r.t. hidden confounder e. Here, we set n = 300, β = 0.7, ˆβ = 0.4,
and ρ = 0.2 where ˆβ is an OLS estimate of β. Under this model, we have u∗(y, z)
y.

(0, 2), Z1 ∼ N

(0, 2), ǫ

∼ N

∼ N

ˆβ(1

ρ)z

(8)

≈

−

−

4 Kernelized DualIV

To demonstrate the effectiveness of our framework, we develop a simple kernel-based algorithm
using the new formulation (7). To simplify notation, we denote by W := (Y, Z) a random variable
to be reproducing kernel Hilbert spaces (RKHSs)
taking value in
R, respectively. Let
associated with positive deﬁnite kernels k :
φ : x
F
are universal and hence are dense in the space of bounded continuous functions [46, Ch. 4].

) be the canonical feature maps [45]. We assume both

W
) and ϕ : w

W × W →

R and l :

X × X →

. We pick

Y × Z

k(x,

l(w,

and

and

:=

7→

7→

F

U

U

·

·

Then, for any f

and u

, we can rewrite the objective in (7) as

∈ F
Ψ(f, u) = EXW [f (X)u(W )]

∈ U

EYZ [Y u(Y, Z)]

1
2

−

EW [u(W )2]

WX f

=

hC

−

b, u

iU −

u,

W u

,

iU

C

(9)

−
1
2 h

6

i=1, kernel functions k, l, and a parameter grid Γ.

Algorithm 1 Kernelized DualIV
Input: Data (xi, yi, zi)n
1: Compute kernel matrices Kij = k(xi, xj) and Lij = l((yi, zi), (yj, zj)).
2: (λ1, λ2)
3: M
←
4: β
←
Output: f (x) =

SelectParams(K, L, Γ).
1L.
1My.

K(L + nλ1I)−
(MK + nλ2K)−

n
i=1 βik(xi, x).

←

P

,
C
∈ U
φ(X)]
∈ U ⊗ F
C
1
W (
−
C

b,

−

C

WX f

W u∗ =

⊗
WX f

C

−

where b := EYZ [Y ϕ(Y, Z)]
WX := EWX [ϕ(W )
and
C). Since (9) is quadratic in u, we have

⊗

C

W := EW [ϕ(W )

is a covariance operator,
is a cross-covariance operator [47, 48] (see Appendix

∈ U ⊗ U

ϕ(W )]

b. Substituting u∗ back into (9) yields

f ∗ = arg min
∈F

f

1
2

C
(cid:10)

WX f

−

b)

(cid:11)U

= (

C

XW

1
−
W

C

C

WX )−

1

XW

C

C

1
W b.
−

(10)

C

)−

1
−
W

1 where

WX + λ2I

W and
−
1 may not exist in general, we replace them with regularized versions (
W +
C
is the identity operator and λ1, λ2 > 0 are regular-

We can view (10) as a generalized least squares solution in RKHS. Since
1
WX )−
(
XW
−
W
C
C
C
1 and (
λ1I
)−
XW
C
C
ization parameters.
Given an i.i.d. sample (xi, yi, zi)n
[ϕ(y1, z1), . . . , ϕ(yn, zn)], and y := [y1, . . . , yn]⊤. Then, we can estimate b,
their empirical counterparts ˆb := n−
1
1ΦΥ⊤ and
ϕ(yi, zi) = n−
W = n−
empirical version of (9) by

i=1 from P(X, Y, Z), we deﬁne Φ := [φ(x1), . . . , φ(xn)], Υ :=
XW with

n
i=1 yiϕ(yi, zi) = n−
n
i=1 ϕ(yi, zi)

1Υy,
ϕ(yi, zi) = n−

W , and
n
1
i=1 φ(xi)

⊗
1ΥΥ⊤. We denote the

XW := n−

1
P
P

C
b

P

⊗

I

C

C

C

1

C
Ψ(f, u) and the estimate of f ∗ by ˆf .
b
b
and u

Next, we show that the representer theorem [49] for
Lemma 5. For any f
for some α, β

, there exist fβ =
b
Ψ(fβ, uα).

∈ F
Rn such that

∈ U
Ψ(f, u) =

P

Ψ(f, u) holds for both f and u.
) and uα =

n
i=1 βik(xi,

·

n
i=1 αil(wi,

)

·

P

∈

b

By virtue of Lemma 5, the solution to (10) can be expressed as f (x) =
coefﬁcients β are given by the following proposition.
P
i=1 from P(X, Y, Z), let K := Φ⊤Φ and L :=
Proposition 6. Given an i.i.d. sample (xi, yi, zi)n
Υ⊤Υ be the Gram matrices such that Kij = k(xi, xj) and Lij = l(wi, wj) where wi := (yi, zi).
Then, ˆf = Φβ where β = (MK + nλ2K)−

1My and M := K(L + nλ1I)−

n
i=1 βik(xi, x) where the

1L.

b

Compared to previous work which involved conditional density estimation [8, 28, 29] and vector-
valued regression [7] as ﬁrst-stage regression, estimating the dual function u, a real-valued function,
is arguably easier. This is especially so when X and Z are high-dimensional.

Hyperparameter selection. Our estimator depends on two hyper-parameters, λ1 and λ2. Given a
dataset (xi, yi, zi)2n
i=1 of size 2n, we provide a simple heuristic to determine the values of (λ1, λ2).
Ideally, if we know the optimal dual function u∗, we can interpret u∗(y, z)2 as a loss function
of f ∗ at (y, z), as discussed in Section 3.4. To this end, we ﬁrst estimate ˆf via Proposition 6 and
WX ˆf
i=1. Next, the out-of-sample
ˆuλ := (
loss of ˆf is evaluated on the second half (xi, yi, zi)2n

ˆb) on the ﬁrst half of the data (xi, yi, zi)n

W + λ

)−

−

I

i=n+1 by

1(
C
b

C
b

R( ˆf ) =

1
n

b

2n

Xi=n+1

(EX

|

zi[ ˆf (X)]

−

yi)2

≈

1
n

2n

Xi=n+1

ˆuλ(yi, zi)2.

(11)

×

1(Kβ

n are kernel matrices evaluated on (xi, yi, zi)n

Note that ˆuλ = Υ(L + nλI)−
∈
Rn
Lij =
l((yi, zi), (yj , zj)) for i = 1, . . . , n and j = n + 1, . . . , 2n and 1 is the all-ones column vector. In
practice, we ﬁx λ in ˆuλ to a small constant to stabilize the loss (11) and only optimize (λ1, λ2) that
appear in β. Note that this procedure differs from the two-stage causal validation procedures used in
[7, 8]. Alternatively, one may choose the hyperparameters by cross-validation with respect to (11).

y) = Υα where α := (L + nλI)−
i=1. Hence,

L1/n where

y) and K, L

R( ˆf )

1(Kβ
α⊤

≈

−

−

e

e

b

7

Table 1: Comparisons of IV regression methods in small (top) and medium (bottom) sample size
regimes. We report the log10 mean squared error (MSE) and its standard deviations over 20 trials.

n = 50

2SLS
DeepIV
KernelIV
DeepGMM
DualIV

n = 1000
2SLS
DeepIV
KernelIV
DeepGMM
DualIV

ρ = 0.1

5.814
5.127
4.481
3.848
4.257

8.236
4.613
4.189
4.090
4.143

±
±
±
±
±

±
±
±
±
±

1.214
0.043
0.134
1.096
0.108

0.117
0.052
0.046
0.691
0.117

Log10 Mean Squared Error (MSE)
ρ = 0.5

ρ = 0.25

ρ = 0.75

6.013
5.131
4.460
2.899
4.210

7.242
4.618
4.209
3.953
4.221

±
±
±
±
±

±
±
±
±
±

0.827
0.031
0.095
1.638
0.126

1.232
0.048
0.040
1.076
0.185

5.895
5.133
4.438
3.952
4.285

8.290
4.614
4.199
4.392
4.104

±
±
±
±
±

±
±
±
±
±

0.718
0.072
0.132
0.900
0.170

1.132
0.068
0.043
0.561
0.102

5.625
5.130
4.433
4.148
4.286

8.371
4.701
4.195
4.272
4.142

±
±
±
±
±

±
±
±
±
±

1.182
0.124
0.100
0.556
0.126

0.865
0.040
0.045
0.595
0.105

ρ = 0.9

5.308
5.127
4.462
3.738
4.232

8.544
4.731
4.194
4.415
4.127

±
±
±
±
±

±
±
±
±
±

1.031
0.061
0.114
0.587
0.152

1.109
0.032
0.055
0.522
0.106

Algorithm 1 outlines the kernelized DualIV method whose consistency is studied in Appendix D.
(n3)) which become the primary com-
We note that above algorithm involves matrix inversions (
putational bottlenecks when scaling to large datasets. To improve the scalability of our algorithm,
we can leverage the rich literature on large-scale kernel machines such as random Fourier features
and Nystr¨om method; see, e.g., Yang et al. [50] and references therein. Alternatively, we can em-
ploy stochastic gradient descent-based (SGD) algorithms similar to those proposed in Dai et al. [11,
Algorithm 1] to solve the dual formulation (7) directly. This would also allow us to employ ﬂexi-
ble models such as neural networks to parameterize the function classes
. Recently, Liao
et al. [44] has taken an important step in this direction and provided convergence analysis for neural
networks under a similar formulation.

and

O

F

U

5 Experiments

In this section, we compare kernelized DualIV2 with: (i) vanilla two-stage least squares (2SLS)
[23], (ii) DeepIV [8], (iii) KernelIV [7] and (iv) DeepGMM [39]. To provide a fair comparison, we
adhered to the provided hyperparameter settings. Given the low-dimensional nature of our experi-
ments, we used DeepGMM’s settings for low-dimensional scenarios in [39, Appendix B.2.1]. We
ran 20 simulations of each algorithm for sample sizes of 50 and 1000 and calculated the log10 mean
squared error and its standard deviations w.r.t. the true function f for 2800 out-of-sample test points.

Demand design. We consider the same simulation as in [7, 8]: Y = f (X) + ε where Y is
outcome, X = (P, T, S) are inputs, and Z = (C, T, S) are instruments. Speciﬁcally, Y is sales, P
is price, which is endogeneous, C is a supply cost shifter (instrument), and (T, S) are time of year
and customer sentiment acting as exogeneous variables. The aim is to estimate the demand function
2].
2p where ψ(t) = 2[(t
f (p, t, s) = 100 + (10 + p)sψ(t)
−
−
(0, I2),
Training data is sampled according to S
Unif[0, 10], (C, V )
Unif
{
ε
0.1, 0.25, 0.5, 0.75, 0.9
}
controls the extent to which price P is confounded with outcome Y by supply-side market forces.
In our notation, X = (P, T, S), Z = (C, T, S) and W = (Y, Z) = (Y, C, T, S).

∼
ρ2), and P = 25 + (C + 3)ψ(T ) + V . The parameter ρ

5)4/600 + exp(
, T

−
1, . . . , 7
}

5)2) + t/10

(ρV, 1

∼ N

∼ N

∈ {

4(t

−

−

−

∼

For DualIV, we used the Gaussian RBF kernel for both k and l. In the experiments, the kernels
on X and Z are product kernels, i.e., k(xi, xj) = kp(pi, pj)kt(ti, tj)ks(si, sj) and k(zi, zj) =
kc(ci, cj)kt(ti, tj)ks(si, sj), and l([yi, zi], [yj, zj]) = exp([yi
zj])
where Vyz is a symmetric bandwidth matrix. The values of all bandwidth parameters are determined
via the median heuristic. We choose (λ1, λ2) from
via cross-validation.
Once (λ1, λ2) is chosen, we reﬁt ˆf on the entire training set.
Table 1 reports the results of different methods evaluated on the test data. First, we observe that 2SLS
achieves the largest MSE in both regimes as expected because the linearity assumption is violated

−
9, . . . , 10−

−
10, 10−

zj]⊤V −

1
yz [yi

yj, zi

yj, zi

10−

−

−

{

}

1

2Our implementation is available at https://github.com/krikamol/DualIV-NeurIPS2020.

8

here. Second, in the small sample size regime, DeepIV achieves relatively larger MSE than the
other non-linear methods. KernelIV, DeepGMM, and DualIV, on the other hand, have comparable
performance, with DeepGMM having the lowest MSE. However, we note that the results attained
by DeepGMM were unstable out of the box and we had to reduce the variance of the initialization
of the neural networks to 0.1 to obtain some degree of stability which is reﬂected in the standard
deviations. We can fully attribute this variability to initialization as DeepGMM’s default batch
size of 1024 is larger than that of both training datasets so there is no sampling variability in the
optimization process. This suggests that DeepGMM, like DeepIV, is relatively brittle compared to
kernel-based methods in the small sample size regime. Furthermore, DeepGMM comes with an
extensive hyperparameter selection process, which highlights its need for ﬁne-tuning. Last but not
least, DualIV is competitive to KernelIV across ρ with slightly smaller MSE, which lends weight to
our hypothesis that estimating the real-valued dual function is easier than vector-valued regression.

In the medium sample size regime, we observe that performance of DeepIV is in the same ballpark
as the rest of the non-linear IV regression methods and the variance of DeepGMM is reduced, albeit
still highest among the non-linear methods. The results of DualIV, KernelIV and DeepGMM are
almost indistinguishable with DualIV having an edge as ρ increases. This could mean accounting for
both Y and Z is perhaps slightly more effective than Z alone in the presence of greater confounding.

6 Conclusion

This paper proposes a general framework for non-linear IV regression called DualIV. Unlike pre-
vious work, DualIV does not require the ﬁrst-stage regression which is the critical bottleneck of
modern two-stage procedures. By exploiting tools in stochastic programming, we were able to re-
formulate the two-stage problem as the convex-concave saddle-point problem which is relatively
simpler to solve. Instead of ﬁrst-stage regression, DualIV requires the dual function u(y, z) to be
estimated, which is arguably easier than ﬁrst-stage regression, especially when the instruments and
treatments are high-dimensional. We demonstrate the validity of our framework with a kernel-based
algorithm. Results show the competitiveness of our algorithm with respect to existing ones. Finally,
potential directions for future work include (i) a minimax convergence analysis which could provide
additional insight into the beneﬁts of our framework, (ii) more ﬂexible and scalable models such as
deep neural networks as dual functions with stochastic gradient descent (SGD) [11], and (iii) appli-
cations to other two-stage problems in causal inference such as double ML [51].

Broader impact

This work provides a new framework for non-linear instrumental variable regression which allows
one to perform causal analysis under the presence of unobserved confounders. This could have a
profound impact in other ﬁelds such as economics, social science, and epidemiology, among oth-
ers. Understanding the role of instruments in the context of learning theory may also pave the way
towards creating more robust and trustworthy machine learning algorithms that are capable of sur-
viving in the world full of hidden biases.

Acknowledgments and Disclosure of Funding

We are indebted to Rahul Singh and Arthur Gretton for their help with the KernelIV code used in
our experiments. We thank Victor Chernozhukov, Elias Bareinboim, Sorawit Saengkyongam, Uri
Shalit, Konrad Kording, Rahul Singh, Arthur Gretton, and You-Lin Chen for fruitful discussions as
well as anonymous reviewers for the helpful feedback on our initial submission.

This work was funded by the federal and state governments of Germany through the Max Planck
Society (MPG).

References

[1] Joshua D. Angrist and J¨orn-Steffen Pischke. Mostly Harmless Econometrics: An Empiricist’s

Companion. Princeton University Press, 2008.

9

[2] Guido W. Imbens and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical

Sciences: An Introduction. Cambridge University Press, New York, NY, USA, 2015.

[3] David Card. The causal effect of education on earnings. In Handbook of Labor Economics,
volume 3 of Handbook of Labor Economics, chapter 30, pages 1801–1863. Elsevier, 1999.

[4] Joshua D. Angrist and Alan B. Keueger. Does Compulsory School Attendance Affect School-

ing and Earnings? The Quarterly Journal of Economics, 106(4):979–1014, 1991.

[5] Stephen Burgess, Dylan S Small, and Simon G Thompson. A review of instrumental vari-
able estimators for Mendelian randomization. Statistical Methods in Medical Research, 26(5):
2333–2355, 2017.

[6] Halbert White. Instrumental variables regression with independent observations. Economet-

rica, 50(2):483–499, 1982.

[7] Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression.

In Advances in Neural Information Processing Systems 32, pages 4593–4605. 2019.

[8] Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep IV: A ﬂexible ap-
proach for counterfactual prediction. In Proceedings of the 34th International Conference on
Machine Learning, volume 70, pages 1414–1423. PMLR, 2017.

[9] Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal inference: foun-

dations and learning algorithms. 2017.

[10] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic
Programming: Modeling and Theory, Second Edition. Society for Industrial and Applied
Mathematics, Philadelphia, PA, USA, 2014.

[11] Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from Conditional Dis-
tributions via Dual Embeddings. In Proceedings of the 20th International Conference on Arti-
ﬁcial Intelligence and Statistics, volume 54, pages 1458–1467. PMLR, 2017.

[12] Kelvin Hsu and Fabio Ramos. Bayesian deconditional kernel mean embeddings. In Proceed-
ings of the 36th International Conference on Machine Learning, volume 97, pages 2830–2838.
PMLR, 2019.

[13] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.

[14] Judea Pearl. Causal inference in statistics: An overview. Statistics Surveys, 3:96–146, 2009.

[15] Whitney K. Newey and James L. Powell. Instrumental variable estimation of nonparametric

models. Econometrica, 71(5):1565–1578, 2003.

[16] Joel L Horowitz. Applied nonparametric instrumental variables estimation. Econometrica, 79

(2):347–394, 2011.

[17] Reiner Kress. Linear Integral Equations, volume 3. Springer, 1989.

[18] M. Nashed and G. Wahba. Generalized inverses in reproducing kernel spaces: An approach
to regularization of linear operator equations. SIAM Journal on Mathematical Analysis, 5(6):
974–987, 1974.

[19] D.B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies.

Journal of Educational Psychology, 66(5):688–701, 1974.

[20] Donald Rubin. Causal inference using potential outcomes. Journal of the American Statistical

Association, 100(469):322–331, 2005.

[21] John Bound, David A. Jaeger, and Regina M. Baker. Problems with instrumental variables
estimation when the correlation between the instruments and the endogeneous explanatory
variable is weak. Journal of the American Statistical Association, 90(430):443–450, 1995.

[22] Douglas Staiger and James H. Stock. Instrumental variables regression with weak instruments.

Econometrica, 65(3):557–586, 1997.

[23] Joshua D. Angrist, Guido W. Imbens, and Donald B. Rubin. Identiﬁcation of causal effects
using instrumental variables. Journal of the American Statistical Association, 91(434):444–
455, 1996.

[24] Jeffrey M. Wooldridge. Econometric Analysis of Cross Section and Panel Data, volume 1 of

MIT Press Books. The MIT Press, March 2001.

10

[25] Richard Blundell, Xiaohong Chen, and Dennis Kristensen. Semi-nonparametric IV estimation

of shape-invariant Engel curves. Econometrica, 75(6):1613–1669, 2007.

[26] Xiaohong Chen and Demian Pouzo. Estimation of nonparametric conditional moment models

with possibly nonsmooth generalized residuals. Econometrica, 80(1):277–321, 2012.

[27] Xiaohong Chen and Timothy M. Christensen. Optimal sup-norm rates and uniform inference
on nonlinear functionals of nonparametric IV regression. Quantitative Economics, 9(1):39–84,
2018.

[28] Peter Hall and Joel L. Horowitz. Nonparametric methods for inference in the presence of

instrumental variables. The Annals of Statistics, 33(6):2904–2929, 12 2005.

[29] S. Darolles, Y. Fan, J. P. Florens, and E. Renault. Nonparametric instrumental regression.

Econometrica, 79(5):1541–1565, 2011.

[30] Alexandre Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Com-

pany, Incorporated, 1st edition, 2008.

[31] Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu. Hilbert space embeddings of
conditional distributions with applications to dynamical systems. In Proceedings of the 26th
International Conference on Machine Learning (ICML), June 2009.

[32] Le Song, Kenji Fukumizu, and Arthur Gretton. Kernel embeddings of conditional distributions:
IEEE Signal

A uniﬁed kernel framework for nonparametric inference in graphical models.
Processing Magazine, 30(4):98–111, 2013.

[33] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Sch¨olkopf. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends in Machine
Learning, 10(1-2):1–141, 2017.

[34] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.
[35] Joshua D. Angrist and Alan B. Krueger. Split sample instrumental variables. Technical Report
699, Princeton University, Department of Economics, Industrial Relations Section., October
1993.

[36] Lars Peter Hansen. Large sample properties of generalized method of moments estimators.

Econometrica, 50(4):1029–1054, 1982.

[37] A.R. Hall. Generalized Method of Moments. Advanced texts in econometrics. Oxford Univer-

sity Press, 2005.

[38] Greg Lewis and Vasilis Syrgkanis. Adversarial generalized method of moments. 03 2018.
[39] Andrew Bennett, Nathan Kallus, and Tobias Schnabel. Deep generalized method of moments
for instrumental variable analysis. In Advances in Neural Information Processing Systems 32,
pages 3564–3574. 2019.

[40] Krikamol Muandet, Wittawat Jitkrittum, and Jonas K¨ubler. Kernel conditional moment test
via maximum moment restriction. In Proceedings of the 36th Conference on Uncertainty in
Artiﬁcial Intelligence, volume 124, pages 41–50. PMLR, 2020.

[41] R. Tyrrell Rockafellar. Convex analysis. Princeton Mathematical Series. Princeton University

Press, 1970.

[42] Le Song, Kenji Fukumizu, and Arthur Gretton. Kernel embeddings of conditional distributions:
IEEE Signal

A uniﬁed kernel framework for nonparametric inference in graphical models.
Processing Magazine, 30:98–111, 07 2013.

[43] R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational Analysis. Springer Verlag, Heidelberg,

Berlin, New York, 1998.

[44] Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Zhaoran Wang, and Mladen Kolar. Prov-
ably efﬁcient neural estimation of structural equation model: An adversarial approach.
In
Advances in Neural Information Processing Systems 33. 2020.

[45] Bernhard Sch¨olkopf and Alexander Smola. Learning with Kernels: Support Vector Machines,

Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2002.

[46] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
[47] Charles R. Baker. Joint measures and cross-covariance operators. Transactions of the American

Mathematical Society, 186:pp. 273–289, 1973.

11

[48] Kenji Fukumizu, Francis Bach, and Michael Jordan. Dimensionality reduction for supervised
learning with reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5:
73–99, 2004.

[49] B. Sch¨olkopf, R. Herbrich, and AJ. Smola. A generalized representer theorem. In Lecture

Notes in Computer Science, Vol. 2111, number 2111 in LNCS, pages 416–426, 2001.

[50] Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr¨om method
vs random Fourier features: A theoretical and empirical comparison. In Advances in Neural
Information Processing Systems 25, pages 476–484. 2012.

[51] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen,
Whitney Newey, and James Robins. Double/debiased machine learning for treatment and
structural parameters. The Econometrics Journal, 21(1):C1–C68, 2018.

[52] K. Fukumizu, L. Song, and A. Gretton. Kernel Bayes’ rule: Bayesian inference with positive

deﬁnite kernels. Journal of Machine Learning Research, 14:3753–3783, 2013.

12

A Conjugate loss function

R. It
Let ℓy(v) := 1
follows from the deﬁnition of Fenchel conjugate (see, e.g., Deﬁnition 2 or [43, Ch. 14] and [10, Ch.
7]) that for any y

v)2 be a proper, convex, and lower semi-continuous function for all y

2 (y

R,

−

∈

∈

ℓ⋆
y(u) := sup

uv
{

−

ℓy(v) : v

R

}

∈

= sup

uv

(cid:26)

−

1
2

(y

−

v)2 : v

R

.

(cid:27)

∈

(12)

Hence, ℓ⋆
1
2 (y
uv
−
Since uv

y(u) is also a proper, concave, and upper semi-continuous function. Taking a derivative of
R.
v)2 w.r.t. v and setting it to zero yield a critical point v∗ = u + y for any u, y
−
1
2 (y
−

∈
v)2 is a concave function in v, we can substituting v∗ back into (12) to obtain

−

ℓ⋆
y(u) = u(u + y)

1
2

(y

−

−

(u + y))2 = u2 + uy

1
2

−

u2 = uy +

1
2

u2,

as required.

B Connection to generalized method of moments (GMM)

To understand the connection between DualIV and GMM, let us consider
where g1, . . . , gm are arbitrary real-valued functions on
u =

m
j=1 αjgj for some (α1, . . . , αm)⊤

Y × Z
Rm. Then, we have

. That is, for any u

:= span(g1, . . . , gm)
, we have

U

∈ U

∈

P
J(f )

:= max
u
∈U

Ψ(f, u)

= max
Rm
α

∈

EXYZ 

(f (X)

−



Y ) 

αjgj(Y, Z)



Xj=1







m

1
2

−

m

EYZ 



αjgj(Y, Z)

Xj=1






m

EYZ 



αjgj(Y, Z)

Xj=1








2








2






= max
Rm
α

∈

m

Xj=1

αjEXYZ [(f (X)

Y )gj(Y, Z)]

−

1
2

−

= max
Rm
α

∈

α⊤ψ

1
2

−

α⊤Λα,

where we deﬁne α := (α1, . . . , αm)⊤, ψ := (ψ(f, g1), . . . , ψ(f, gm))⊤, and Λ := EYZ [g(Y, Z)
⊗
g(Y, Z)] with g(Y, Z) := (g1(Y, Z), . . . , gm(Y, Z))⊤. Taking the derivative w.r.t. α and setting it
to zero yield

ψ⊤Λ−

J(f ) =

1
2
In this case, the DualIV objective can be expressed in a quadratic form. In the language of GMM,
ψ acts as a vector of moment conditions and Λ acts as a weighting matrix [36, 37]. However,
we reiterate that there is a fundamental difference here: the dual function u
cannot act as an
instrument since it depends on Y and thereby violates the exclusion restriction assumption. Recently,
Liao et al. [44, Appendix F] provided a clariﬁcation on this connection by employing an alternative
formulation.

1ψ.

∈ U

(13)

C Dual formulation in RKHS

W × W →

R. Let φ : x

In this section, we provide a detailed derivation of the dual formulation (9) when
are both
F
R
reproducing kernel Hilbert spaces (RKHSs) associated with positive deﬁnite kernels k :
and l :
) be the canonical feature maps
7→
of k and l, respectively [45]. We assume throughout that both
are universal such that they
are both dense in the space of bounded continuous functions (see, e.g., [46, Ch. 4]). Furthermore,
let HS(
with an inner
product

) be a Hilbert space of Hilbert-Schmidt operators mapping from
,
U
·iHS (see, e.g., [33, Sec. 2.3]).
,

) and ϕ : w

X ×X →

F
h·

·
and

k(x,

l(w,

and

7→

to

F

F

U

U

U

·

13

Then, for any f

and u

∈ F
Ψ(f, u) = EXW [f (X)u(W )]

∈ U

, we can rewrite the objective in (7) as a functional

EYZ [Y u(Y, Z)]

−

EW [u(W )2]

1
2

−
EYZ [Y

u, ϕ(Y, Z)
]
iU
h
u, EYZ [Y ϕ(Y, Z)]

iU

u, ϕ(W )

]

= EXW [
= EXW [

f, φ(X)
h
f
h
EW [

⊗
u
h

⊗

iF h
u, φ(X)

⊗
u, ϕ(W )

1
2

−

iU
−
HS]
ϕ(W )
− h
i
HS]
ϕ(W )
i

⊗

XW , f

u

HS
i

⊗

hC

u, b

1
2 hC

W , u

u

HS
i

⊗

=

=

XW u

f,
h

C

iF − h

where b := EYZ [Y ϕ(Y, Z)]
XW := EXW [φ(X)
and
⊗
the reproducing property of
U
property of the rank-one operator, i.e.,

,
C
∈ U
ϕ(W )]
and

F

C

− h

u, b

iU −
1
2 h
W := EW [ϕ(W )

iU −

W u

u,

C

,

iU

is a covariance operator,
∈ U ⊗ U
is a cross-covariance operator [47, 48]. We used
∈ F ⊗ U
in the second equality. The third equality follows from the

ϕ(W )]

⊗

1
2

−

EW [

2
u, ϕ(W )
i
h
U

]

f, g
h
We then used the deﬁnition of
from the fact that
f

C
iHS(

, and u

⊗

u

L, f
h
.
∈ U

∈ F

U

,

W ,

v, u

=

f
h

⊗

iU

iF h

v, u

g

iHS(

⊗

).

,

U
XW , and b to get the fourth equation. The last equation follows
C
),
for any Hilbert-Schmidt operator L
) =
F

HS(

f, Lu
h

iF

F

∈

U

F

,

D Consistency

1

1

C

C

C

C

C

XW

XW

XW

1
−
W

1
−
W

WX )−

WX )−

W and (
−
C

the kernelized DualIV estimator is asymptotically consistent
In this section, we show that
1
1 exist. Under these assumptions, we
under the assumption that
C
show in (10) that the solution f ∗ of the saddle-point problem (9) can be expressed as f ∗ =
W b. For simplicity, we assume further that the operator norm of the in-
(
−
C
verse covariance functions are bounded from below. The following theorem shows the consistency
for the estimator ˆf obtained via Proposition 6.
Theorem 7. Let ˆfλ be an empirical estimator of f ∗ obtained from Proposition 6 with the regular-
1 exist and the operator
ization parameters λ := (λ1, λ2). Assume that
W and (
−
C
C
norm of the inverse are bounded. Then, for sufﬁciently slow decay of regularization parameters λ1
and λ2, ˆfλ is a consistent estimator of f ∗ in RKHS norm, i.e.,

WX )−

C
ˆfλ

0 as n

1
−
W

f ∗

XW

C

C

1

k

−

kF →

.
→ ∞

The proof of this theorem can be found in Appendix E.4.

The critical drawback of Theorem 7 is that it assumes the existence of
WX )−
which may not hold in general. Similar assumption was also made in Fukumizu et al. [52] who
provided counterexamples of cases in which such an assumption does not hold; see, also, [31, 33, 52]
for the detailed discussion. One potential direction for future work is thus to provide the consistency
result of DualIV under weaker assumptions.

W and (
−
C

XW

C

C

C

1

1
−
W

1

E Proofs

This section contains the detailed proofs.

E.1 Proof of Proposition 3

Proposition 3. Let ℓ(y, y′) = 1

2 (y

Proof. Taking ℓ in (4) to be 1

2 (y

−

y′)2. Then, for any ﬁxed f , we have R(f ) = maxu Ψ(f, u).

−
y′)2, plugging u∗(y, z) = EX

z[f (X)]

|

−

y into (7) yields

Ψ(f, u∗) = EXYZ [(f (X)

Y )u∗(Y, Z)]

−

EYZ [u∗(Y, Z)2]

1
2

−

14

= EXYZ [(f (X)

−

Y )(EX

Z [f (X)]

Y )]

−

1
2

−

= EYZ [(EX

Z [f (X)]

|

= EYZ [(EX

Z [f (X)]

|
EYZ [(EX

Z [f (X)]

|

−

|
Y )(EX

|

Z [f (X)]

Y )]

−

−

−
EYZ [(EX

Y )2]

1
2
Y )2] = R(f ),

−

Z [f (X)]

EYZ [(EX
1
2

−

|
EYZ [(EX

Y )2]

−

Z [f (X)]

|

−

Y )2]

Z [f (X)]

|

−

Y )2]

=

1
2

as required.

E.2 Proof of Lemma 5

Lemma 5. For any f
for some α, β

∈ F
Rn such that

and u

∈ U
Ψ(f, u) =

, there exist fβ =
Ψ(fβ, uα).

∈

n
i=1 βik(xi,

·

) and uα =

n
i=1 αil(wi,

)

·

P

P

Proof. Given a ﬁxed sample (xi, yi, zi)n
as
and
α
⊕ F⊥
functions of the following forms:

⊕ U⊥

=

=

F

F

U

U

β

b

b
i=1 of size n, any RKHSes
where

can be decomposed
U
α are respectively subspaces consisting of

β and

and

F

F

U

n

n

for some β
∈
are orthogonal to
we have
fβ, f
h
as f = fβ + f

Rn and α
∈
β and
F
U
= 0 and
⊥iF
and u = uα + u
⊥
Next, recall that

uα, u
h

⊥

fβ =

βik(xi,

),

uα =

αil(wi,

·

Xi=1

Xi=1
Rn. The orthogonal subspaces
α, respectively, i.e., for any fβ

),

·

= 0. Any elements f

⊥iU
where fβ

β, f

∈ F

F⊥
∈ F

and
β, f

U⊥
⊥ ∈ F⊥
and u
∈ U
α, and u

consist of elements which
,
α, u
, uα
⊥ ∈ U⊥
can thus be expressed
.

∈ U

∈ F
, uα

⊥ ∈ F⊥

∈ U

⊥ ∈ U⊥

Ψ(f, u) =

XW u

f,
h
iF − h
C
1ΥΥ⊤, ˆb = n−
b
W = n−

u, ˆb

u,

W u

iU −
C
iU
b
1Υy, Φ = [k(x1,

b
1ΦΥ⊤,
), . . . , k(xn,
)], and y = [y1, . . . , yn]⊤. Using the above decomposition, we have

·

1
2 h

)], Υ =

·

where
[l(w1,

XW = n−
C
), . . . , l(wn,
b
·

C
b

·
Ψ(f, u) =

b

=

=

XW u

f,
C
(cid:10)
b
fβ + f
(cid:10)

n

u, ˆb

n

(cid:11)F −
,

(cid:10)
β′ik(xi,

⊥

Xi=1

fβ,
(cid:10)

Xi=1

β′ik(xi,

·

)
(cid:11)F −

1
2

·

(cid:11)U −
)
(cid:11)F −
u, ˆb
(cid:10)

W u

u,
(cid:10)

C
b
u, ˆb
(cid:10)

(cid:11)U

(cid:11)U −
1
2

u,
(cid:10)

(cid:11)U −

1
2

u,
(cid:10)

W u

C
b

(cid:11)U

W u

,

(cid:11)U

C
b

. Since the choice of u is arbitrary, the minimizer of

Ψ(f, u) with

b

1
where β′i := n−
respect to f lives in the subspace

l(wi,
h

), u

iU

·

β.

F

Similarly, we can write

Ψ(f, u) for any f

∈ F

as a function of u

Ψ(f, u) =

b

=

=

WX f, u

b
C
n
b

(cid:10)

u, ˆb
(cid:10)

(cid:11)U −

(cid:11)U −

1
2

u,
(cid:10)

α′il(wi,

), uα + u

⊥

·

W u

C
(cid:11)U
b
uα + u

⊥

,

W (uα + u

(cid:10)

(cid:11)U −
)
(cid:11)U
n

⊥

(cid:10)

−

Xi=1
1
uα + u
2
(cid:10)
n

α′il(wi,

·

Xi=1

(cid:10)

⊥

C
b
), uα

(cid:11)U −
=

as

∈ U

n

,

α′′i l(wi,

Xi=1

)
(cid:11)U

·

uα,
(cid:10)

Xi=1

α′′i l(wi,

)
(cid:11)U −

·

1
2

uα,
(cid:10)

W uα

.

(cid:11)U

C
b

The ﬁrst equality follows from
Since the choice of f is arbitrary, the maximizer of
subspace

C
h
b

XW u

f,
h

C
b

iF

α.

WX f, u

as

XW is an adjoint operator of

WX .
Ψ(f, u) with respect to u also lives in the

iU

C
b

C
b

U

b

15

Consequently,

Ψ(f, u) =

Ψ(fβ, uα) for some β

Rn and α

∈

∈

Rn. This completes the proof.

b
E.3 Proof of Proposition 6

b

i=1 from P(X, Y, Z), let K := Φ⊤Φ and L :=
Proposition 6. Given an i.i.d. sample (xi, yi, zi)n
Υ⊤Υ be the Gram matrices such that Kij = k(xi, xj) and Lij = l(wi, wj) where wi := (yi, zi).
Then, ˆf = Φβ where β = (MK + nλ2K)−

1My and M := K(L + nλ1I)−

1L.

Proof. It follows from (10) that the structural function f ∗
W + λ1I

C
Replacing the population quantities with the empirical counterparts
and ˆb = 1

WX + λ2I

)f ∗ =

XW (

XW (

∈ F

(
C

)−

C

C

C

1

n Υy yields

satisﬁes

C
b

)−

1b.

W + λ1I
XW = 1

(ΦΥ⊤(ΥΥ⊤ + nλ1I

)−

1ΥΦ⊤ + nλ2I

)f ∗ = ΦΥ⊤(ΥΥ⊤ + nλ1I

)−

C
b
1Υy.

n ΦΥ⊤,

W = 1

n ΥΥ⊤,

Using the identity Υ⊤(ΥΥ⊤ + nλ1I
rewritten as

)−

1 = (Υ⊤Υ + nλ1I)−

1Υ⊤, the above equation can be

(Φ(Υ⊤Υ + nλ1I)−

(Φ(L + nλ1I)−

1Υ⊤ΥΦ⊤ + nλ2I
1LΦ⊤ + nλ2I
∈

)f ∗ = Φ(Υ⊤Υ + nλ1I)−
1Ly.
)f ∗ = Φ(L + nλ1I)−

1Υ⊤Υy

By Lemma 5, f ∗ = Φβ for some β

Rn. Substituting this back into the equation above yields

Φ(L + nλ1I)−

Φ(L + nλ1I)−

1LΦ⊤Φβ + nλ2Φβ = Φ(L + nλ1I)−
1LKβ + nλ2Φβ = Φ(L + nλ1I)−

1Ly
1Ly.

Multiplying both sides of the equation by Φ⊤ gives

Φ⊤Φ(L + nλ1I)−

K(L + nλ1I)−
(K(L + nλ1I)−

1LKβ + nλ2Φ⊤Φβ = Φ⊤Φ(L + nλ1I)−
1Ly
1LKβ + nλ2Kβ = K(L + nλ1I)−
1Ly.
1LK + nλ2K)β = K(L + nλ1I)−

1Ly

Setting M = K(L + nλ1I)−

1L yields the result.

E.4 Proof of Theorem 7

Theorem 7. Let ˆfλ be an empirical estimator of f ∗ obtained from Proposition 6 with the regular-
1 exist and the operator
W and (
ization parameters λ := (λ1, λ2). Assume that
−
C
C
norm of the inverse are bounded. Then, for sufﬁciently slow decay of regularization parameters λ1
and λ2, ˆfλ is a consistent estimator of f ∗ in RKHS norm, i.e.,

C
ˆfλ

WX )−

0 as n

1
−
W

f ∗

XW

C

1

k

−

kF →

.
→ ∞

For ease of understanding, we will use the following notation throughout the proof:

:=

XW

1
−
W

WX

C

λ1

R

:=
:=

R
λ1
C

C
W + λ1I

C
XW (
C
W + λ1I

R
b
R
b
λ1
C
b
The following identity will be used heavily in our proof:
1(A

1) = B−

(B−

A−

C
C

)−

WX

C

1

1

λ1

−

B)A−

1.

−

:=

:=

:=

XW

1
−
W

WX

C
C
C
W + λ1I
XW (
b
b
b
C
C
.
W + λ1I
b
b
C
b

1

)−

WX

C
b

Proof. First, it follows from the assumption that
δ−
2

(
C
k
for some δ1, δ2 > 0. Moreover, we can write the empirical estimate as

and

δ−
1

kC

≤

op

k

1

1

λ1 + λ2I
R
Similarly, under our assumption, the true population function can be expressed as
b

C
b

C
b

)−

XW

ˆfλ = (

1
−
λ1

ˆb.

1
−
W

1

(14)

XW

1
−
W

C

C

⊤WX )−

1

op

k

≤

f ∗ =

1

C

−

R

XW

C

1
W b.
−

16

The goal is then to bound the difference of ˆfλ and f ∗ in RKHS norm, i.e.,

ˆfλ
(cid:13)
(cid:13)
(cid:13)

f ∗

−

(cid:13)
F
(cid:13)
(cid:13)

)−

=

λ1 + λ2I
(
R
(cid:13)
(cid:13)
b
(cid:13)
λ1 + λ2I
(
≤ (cid:13)
R
(cid:13)
b
λ1 + λ2I
(
+
(cid:13)
R
(cid:13)
=: T1 + T2.
(cid:13)

)−

1

XW

1

C
b
XW
C
1
b
)−

C
b
1
−
λ1
C
b
XW

C

C

1
−
λ1

ˆb

−

− R

1

C

XW

C

1
W b
−

ˆb

−
1
λ1 b
−

+ λ2I)−

1

(
R

−

− R

1

C

XW

C

F

(cid:13)
(cid:13)
(cid:13)
XW

C
1
W b
−

1
λ1 b
−

C

F

(cid:13)
(cid:13)

Bounding T2: Let us ﬁrst consider the second term T2 in (15):

T2

≤

:=

λ1 + λ2I
(
R
(cid:13)
λ1 + λ2I
(
(cid:13)
R
(cid:13)
1
+
(cid:13)
−
λ1 C
R
(cid:13)
=: T21 + T22.
(cid:13)

XW

1

)−

1

)−

XW

C

XW

C
1
λ1 b
−

C

C

C

1
λ1 b
−
1
λ1 b
−
1

−

C

− R

− R
XW

C

− R

XW

XW

1

−

C
1
−
λ1 C
1
W b
−

1
W b
−
1
λ1 b
−

C

C

F

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)

Bounding T21: Let us consider T21 in (16) ﬁrst.

F

(cid:13)
(cid:13)
(cid:13)

(15)

(16)

1

XW

1
λ1 b
−
C
1
−
λ1 C

XW

R

1
−
λ1 C
− R
1
λ1 b
−

XW

1
λ1 b
−

C

F

(cid:13)
(cid:13)

op

F
(cid:13)
(cid:13)
XW
C
(cid:13)
(cid:13)
WX )

op

(cid:13)
(cid:13)

1
λ1 b
−

C

(cid:13)
(cid:13)

F
2

op

(cid:13)
(cid:13)
1
−
λ1
(cid:13)
(cid:13)

R

XW

1
λ1 b
−

C

C

(cid:13)
(cid:13)

F

(cid:13)
(cid:13)

T21 =

C
)−

1

1

)−

)−

XW

≤

λ2

)−
λ1 + λ2I
(
R
(cid:13)
λ1 + λ2I
(
= λ2
(cid:13)
R
(cid:13)
λ1 + λ2I
(
(cid:13)
R
(cid:13)
(cid:13)
λ1 + λ2I
(
R
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
−
λ1

λ2

λ2

R

≤

≤

op

2

C
(cid:13)
(cid:13)
1
λ1 b
−

(cid:13)
(cid:13)
XW
C

≤

λ2
δ2
C
2 (cid:13)
(cid:13)

F ≤

(cid:13)
(cid:13)

C

1
−
λ1

op

R

(cid:13)
(cid:13)
1
−
λ1 C

C

(cid:13)
(cid:13)
XW

(cid:13)
(cid:13)
1(
C
1
λ1 b
−
(cid:13)
λ2
(cid:13)
δ1δ2
2 kC

C

F

XW

op

k

k

b

.

kF

(17)

The second equality in (17) follows from the identity in (14). Hence, we have T21 =
From the above argument,
max(
XW

(λ2).
it is also clear that there exists a positive constant C such that
1
λ1 b
−

1
W b
−

C.

XW

O

)

,

kC

C

kF

kC

C

kF

≤

Bounding T22: Let us now consider the term T22 in (16).

T22 =

≤
=

≤

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
C
(cid:13)

C

+

R

R
(
R

C

XW

XW

1
−
λ1 C
1
−
λ1 C
C
1
−
−
λ1 − R
1
−
λ1

1
λ1 b
−
1
λ1 b
−
1)
C
−

op

R

R

R

(cid:13)
(cid:13)
(cid:13)
(cid:13)
R
(cid:13)
(cid:13)

1
−
λ1

1

−

op

(cid:13)
(cid:13)
(cid:13)
(cid:13)
op
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
C
(cid:13)
(cid:13)

−

R
XW

1

1

−

− R

1

1

−

C
1
λ1 b
−

− R
XW

C

C

F

F

C

XW

XW

1
W b
−
(cid:13)
1
λ1 b
(cid:13)
−
C
F
(cid:13)
1
+
(cid:13)
−
R
(cid:13)
(cid:13)
λ1 − Rkop +
(cid:13)
(cid:13)
1
XW (
−
λ1 − C
b
.
(cid:1)

C
1
−
W

F

(cid:13)
(cid:13)

op

op kR
(cid:13)
(cid:13)
(cid:13)
(cid:13)
C
(cid:0)

C
(cid:13)
1
(cid:13)
−
λ1 − C

Further, we have

1

−

XW

R
(cid:13)
(cid:13)
XW

C
1
λ1 b
−

1
λ1 b
−

C

XW

1

−

1
W b
−

XW

C

C

F

(cid:13)
(cid:13)

− R
1
W b
−
C
1
−
λ1 − C

− C
XW

C
(cid:0)

F
(cid:1)(cid:13)
1
(cid:13)
−
W

b
(cid:1)

F

(cid:13)
(cid:13)

+

C

(cid:0)

C
1

−

(cid:13)
(cid:13)
WX

R
(cid:13)
1
(cid:13)
W )
−
C

op

C

(cid:13)
(cid:13)
op

(cid:13)
(cid:13)

(18)

T22 ≤

λC
δ2 (cid:13)
(cid:13)

1
−
λ1

R

op

(cid:13)
(cid:13)

(cid:13)
(cid:13)

XW

1
−
λ1 C

C

1
−
W

C

WX

C

op +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

1

−

R

XW

C
(cid:0)

C

(cid:13)
(cid:13)

1
−
λ1 − C

1
−
W

.

b
(cid:1)

F

(cid:13)
(cid:13)

(19)

op

(cid:13)
(cid:13)

Let us consider now the following term in the above inequality:

1
−
λ1

R

(cid:13)
(cid:13)

op ≤

(cid:13)
(cid:13)

≤

≤

1

−

R
(cid:13)
1
(cid:13)
δ2
1
δ2

+

+

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1

R

op +
(cid:13)
(cid:13)
R

(cid:13)
(cid:13)
−
λ1 − R

1

−
λ1 − R
1

−

op

−

1

op

(cid:13)
(cid:13)

1
−
λ1

R

R

op

(cid:13)
(cid:13)

(cid:13)
(cid:13)

λ1 − Rkop

op kR
(cid:13)
(cid:13)

(cid:13)
(cid:13)
1
−

17

=

=

1
δ2
1
δ2

1
−
λ1

R

+

(cid:13)
(cid:13)
+ λ1

1

−

op

(cid:13)
(cid:13)
1
−
λ1
R

R

(cid:13)
(cid:13)
op

op

(cid:13)
(cid:13)
−
R

1

C

(cid:13)
(cid:13)
op

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 and similarly

(cid:13)
(cid:13)

(cid:13)
(cid:13)

XW (

C

1
−
λ1 − C

1
W )
−
C

WX

XW

1
−
λ1 C

C

1
−
W

C

WX

C

(cid:13)
(cid:13)

(cid:13)
(cid:13)

op

(cid:13)
(cid:13)
op .

1
Now,
−
W
positive real numbers ˆc and ˜c. Hence,

1
−
λ1 C

kC

XW

WX

≤

˜c/δ2

op

C

C

k

XW

kC

C
(cid:0)

1

−
λ1 − C

1
−
W

b
(cid:1)

kF ≤

ˆc/δ2

1 for some

1
−
λ1

R

(cid:13)
(cid:13)

op ≤

(cid:13)
(cid:13)

1
δ2

+

λ1˜c
δ2
1δ2 (cid:13)
(cid:13)

1
−
λ1

R

op ⇒

(cid:13)
(cid:13)

1
−
λ1

R

(cid:13)
(cid:13)

1/δ2

λ1 ˜c
δ2
1 δ2

−

.

op ≤

1

(cid:13)
(cid:13)

Hence, if λ1 →

0 sufﬁciently fast, then

T22 ≤
for a positive real number ˜C. This implies T22 =

(λ1).

O

λ1 ˜C
δ2
1δ2

(20)

Bounding T1: We now consider the ﬁrst term in (15).

T1 =

)−

(cid:13)
(cid:13)
(cid:13)
≤ (cid:13)
(cid:13)
(cid:13)
+

λ1 + λ2I
(
R
b
λ1 + λ2I
(
R
b
λ1 + λ2I
(
R
(cid:13)
(cid:13)
=: T11 + T12.
(cid:13)

)−

1

XW

1

C
b
XW
C
b
1
)−

C
b
1
−
λ1
C
b
XW

C
b

C
b

1
−
λ1

ˆb

−

ˆb

−
ˆb
1
−
λ1

)−

λ1 + λ2I
(
R
λ1 + λ2I
(
R
λ1 + λ2I
(
R

−

)−

C

1

1

C
b
1
)−

1
λ1 b
−

XW

C

XW

1
−
λ1

C
b
XW

C

C

F

ˆb

(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
1
(cid:13)
λ1 b
−

(cid:13)
(cid:13)
(cid:13)

F

(21)

Bounding T11: Consider the ﬁrst term in (21):

1

1
−
λ1

ˆb

XW

)−

1
−
λ1

T11 =

λ1 + λ2I
(
R
(cid:13)
(cid:13)
b
ˆb
(cid:13)
XW
≤ (cid:13)
C
(cid:13)
(cid:13) b
C
λ1λ2

−
C
C
b
b
λ1 + λ2I
(
R
F (cid:13)
(cid:13)
(cid:13)
(cid:13)
b
(cid:13)
(cid:13)
λ1
λ1 − R

C
b
R
2 (cid:13)
(cid:13)
(cid:13) b
for some positive constant C. Next, we have

(cid:13)
(cid:13)
(cid:13)

≤

op

)−

1

)−

λ1 + λ2I
(
C
R
b
1
λ1 + λ2I
(
R
(cid:13)
(cid:13)

op

(cid:13)
(cid:13)
(cid:13)

XW

1
−
λ1

ˆb

C
b
1
)−

F

(cid:13)
(cid:13)
(cid:13)
R
(cid:13)
(cid:13)
(cid:13) b

λ1 − R

λ1

(22)

op

(cid:13)
(cid:13)
(cid:13)

op

(cid:13)
(cid:13)

XW

1
−
λ1 C

C

WX

op

(cid:13)
(cid:13)
(cid:13)
1
−
λ1

XW

C
(cid:13)
(cid:13)
(cid:13) b

1
−
λ1

C
b
≤ (cid:13)
C
(cid:13)
(cid:13) b
C
≤ (cid:13)
(cid:13)
(cid:13) b
+

WX

C
b
XW

− C
1
−
λ1

WX

C
C
b
b
1
XW (
−
λ1 − C
C
b
1
XW
−
λ1

C

C

XW

WX

C
−
b
1
λ1 )
−
C
b
− C

WX

op

(cid:13)
(cid:13)
(cid:13)
XW

WX

C
b
+

op

(cid:13)
(cid:13)
(cid:13)
XW
C
(cid:13)
(cid:13)
(cid:13) b
1
−
λ1 C

WX

C

XW

C

WX

+

C
(cid:13)
(cid:13)
(cid:13) b
1
−
λ1
C
b

C

1
−
λ1

WX

C
b
XW
− C

− C
1
−
λ1

C

XW

1
−
λ1 C

C

WX

WX

C
b

op

(cid:13)
(cid:13)
(cid:13)

op

(cid:13)
(cid:13)
(cid:13)

(23)

C
≤ (cid:13)
(cid:13)
(cid:13) b
+

C

(cid:13)
(cid:13)
(cid:13)
XW

C
b
− C

1
−
λ1

W )

C

WX

− C

W

C
b
XW
C

1
λ1 (
−
C
b
1
λ1 (
−
C
C
b

WX

C
b
WX )

op

.

(cid:13)
(cid:13)
(cid:13)
op

op

(cid:13)
(cid:13)
(cid:13)
+
(
C
(cid:13)
(cid:13)
b
(cid:13)

XW

XW )

C

1
−
λ1

− C

WX

C
b

op

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
From the √n-consistency of covariance and cross-covariance operators [47, 48], we have

(cid:13)
(cid:13)
(cid:13)

XW

WX

1
−
λ1

C
b

C
b

C
(cid:13)
(cid:13)
(cid:13) b

XW

− C

1
−
λ1 C

C

WX

=

O (cid:18)

1
λ1√n (cid:19)

.

op

(cid:13)
(cid:13)
(cid:13)

Hence, if (λ1λ2)2 converges to zero slower than 1/√n, then T11 converges to zero asymptotically.

18

Bounding T12: Let us now consider the second term in (21):

1

)−

1
λ1 b
−

XW

C

C

λ1 + λ2I
(
R
ˆb

XW

−
1
−
λ1

− C

1
λ1 b
−

C

F

(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)

T12 =

≤

XW

λ1 + λ2I
(
R
(cid:13)
(cid:13)
(cid:13)
λ1 + λ2I
(
≤ (cid:13)
R
(cid:13)
1
(cid:13)
λ2 (cid:13)
C
(cid:13)
(cid:13) b
1
λ2 h(cid:13)
C
(cid:13)
(cid:13) b
+
(cid:13)
(cid:13)
1
(cid:13)
(
XW
λ2 h(cid:13)
C
(cid:13)
b
(cid:13)
+

C
b
XW
C
b
XW

XW

≤

≤

C

1

)−

1

)−

1
−
λ1

ˆb

XW

C
b
op
k

C
b
XW
C
b
XW

k

C
b
1
λ1 b
−

C

1
−
λ1

ˆb

1
−
λ1

− C
ˆb

− C
ˆb
1
−
λ1

XW

1
−
λ1

C
b
XW

C

− C

XW )

1
−
λ1

ˆb

− C

1
−
λ1

ˆb

C

XW

− C

C
b

C

(cid:13)
(cid:13)
(cid:13)

+

XW

F

(cid:13)
(cid:13)
(cid:13)
ˆb
(cid:13)
F
(cid:13)
(cid:13)
1
λ1 b
−

C

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
F i
(cid:13)
(cid:13)
XW (
C
(cid:13)
F
(cid:13)
(cid:13)
1
λ1 b
−
(cid:13)
G
(cid:13)
(cid:13)

(cid:21)

.

C

(cid:13)
(cid:13)
(cid:13)
C

XW

− C

1
−
λ1

ˆb

C

ˆb

1
−
λ1

C
b

F

(cid:13)
(cid:13)
(cid:13)

(24)

1
−
λ1 − C
C
b

1

λ1 )ˆb

−

F

(cid:13)
(cid:13)
(cid:13)

1
λ1λ2√n

O (cid:16)

(cid:17)

XW

By the √n-consistency of mean element and covariance operator in RKHS [47, 48], we have that
. Moreover, it follows from what we have shown so far that
T12 =

(
C
(cid:13)
(cid:13)
b
(cid:13)

1

1
−
λ1

WX + λ2I
C
C
b
b
T1 + T2 ≤

XW

)−

(
C
T11 + T12 + T13 + T14.

C
b

C
b

−

1
−
λ1

ˆb

XW

1
−
W

C

C

WX )−

1

XW

C

C

1
W b
−

(25)

F

(cid:13)
(cid:13)
(cid:13)

≤

Hence, if λ1 and λ2 converge to zero with the sample size n such that

1
2√n also converges to
1λ2
λ2

zero, then

ˆfλ

k

−

f ∗

kF →

0. That is, our estimator is consistent in RKHS norm.

19

