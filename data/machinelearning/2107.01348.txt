2
2
0
2

p
e
S
1

]

G
L
.
s
c
[

2
v
8
4
3
1
0
.
7
0
1
2
:
v
i
X
r
a

Examining average and discounted reward optimality
criteria in reinforcement learning

Vektor Dewanto, Marcus Gallagher
School of Information Technology and Electrical Engineering
University of Queensland, Australia
v.dewanto@uqconnect.edu.au, marcusg@uq.edu.au

Abstract

In reinforcement learning (RL), the goal is to obtain an optimal policy, for which the
optimality criterion is fundamentally important. Two major optimality criteria are
average and discounted rewards. While the latter is more popular, it is problematic
to apply in environments without an inherent notion of discounting. This motivates
us to revisit a) the progression of optimality criteria in dynamic programming,
b) justiﬁcation for and complication of an artiﬁcial discount factor, and c) beneﬁts
of directly maximizing the average reward criterion, which is discounting-free. Our
contributions include a thorough examination of the relationship between average
and discounted rewards, as well as a discussion of their pros and cons in RL. We
emphasize that average-reward RL methods possess the ingredient and mechanism
for applying a family of discounting-free optimality criteria (Veinott, 1969) to RL.

1

Introduction

Reinforcement learning (RL) is concerned with sequential decision making, where a decision maker
has to choose an action based on its current state. Determining the best actions amounts to ﬁnding
an optimal mapping from every state to a probability distribution over actions available at that state.
Thus, one fundamental component of any RL method is the optimality criterion, by which we deﬁne
what we mean by such an optimal mapping.

The most popular optimality criterion in RL is the discounted reward (Duan et al., 2016; Machado
et al., 2018; Henderson et al., 2018). On the other hand, there is growing interest in the average reward
optimality, as surveyed by Mahadevan (1996a); Dewanto et al. (2020). In this paper, we discuss
both criteria in order to obtain a comprehensive understanding of their properties, relationships, and
differences. This is important because the choice of optimality criteria affects almost every aspect
of RL methods, including the policy evaluation function, the policy gradient formulation, and the
resulting optimal policy, where the term policy refers to the above-mentioned mapping. Thus, the
choice of optimality criteria eventually impacts the performance of an RL system (and the choices
made within, e.g. approximation techniques and hyperparameters).

This paper presents a thorough examination of the connection between average and discounted
rewards, as well as a discussion of their pros and cons in RL. Our examination here is devised through
broader lens of reﬁned optimality criteria (which generalize average and discounted rewards), inspired
by the seminal work of Mahadevan (1996b). It is also broader in the sense of algorithmic styles:
value- and policy-iteration, as well as of tabular and function approximation settings in RL. In this
paper, we also attempt to compile and unify various justiﬁcations for deviating from the common
practice of maximizing the discounted reward criterion in RL.

There are a number of papers that speciﬁcally compare and contrast average to discounted rewards in
RL. For example, Mahadevan (1994) empirically investigated average versus discounted reward Q-

Preprint. Under review.

 
 
 
 
 
 
learning (in the context of value-iteration RL). Tsitsiklis and Van Roy (2002) theoretically compared
average versus discounted reward temporal-difference (TD) methods for policy evaluation (in the
context of policy-iteration RL). We provide updates and extensions to those existing comparison
works in order to obtain a comprehensive view on discounting and discounting-free RL.

Since RL is approximate dynamic programming (DP), we begin with reviewing optimality criteria
in DP, model classiﬁcation (which plays an important role in average-reward optimality), and the
interpretation of discounting in Sec 2. We then provide justiﬁcations for discounting for environments
without any inherent notion of discounting (Sec 3). This is followed by the difﬁculties that arise
from introducing an artiﬁcial discount factor (Sec 4). We discuss the beneﬁts from maximizing the
average-reward criterion in Sec 5, as well as our ﬁnding and viewpoint in Sec 6.

2 Preliminaries

Sequential decision making is often formulated as a Markov decision process (MDP) with a state
set S, an action set A, a reward set R, and a decision-epoch set T . Here, all but T are ﬁnite
sets, yielding an inﬁnite-horizon ﬁnite MDP. At the current decision-epoch t ∈ T , a decision maker
(henceforth, an agent) is in a current state st ∈ S, and chooses to then execute a current action at ∈ A.
Consequently at the next decision-epoch t + 1, it arrives in the next state st+1 ∈ S and earns an
(immediate) scalar reward rt+1 ∈ R. The decision-epochs occur every single time unit (i.e. timestep)
from t = 0 till the maximum timestep (denoted as tmax); hence, we have a discrete-time MDP.

For t = 0, 1, . . . , tmax = ∞, the agent experiences a sequence (trajectory) of states st, actions at and
rewards rt+1. That is, s0, a0, r1, s1, a1, r2, . . . , stmax . The initial state, the next state, and the next
reward are governed by the environment dynamics that is fully speciﬁed by three time-homogenous
(time-invariant) entities: i) the initial state distribution ˚p, from which St=0 ∼ ˚p, ii) the one-(time)step
state transition distribution p(·|st, at), from which St+1|St = st, At = at ∼ p(·|st, at), and iii) the
r∈R Pr{r|st, at, St+1} · r(cid:3) =: rt+1, where St and At
reward function r(st, at) = Ep(·|st,at)
denote the state and action random variables, respectively. The existence of a probability space
that holds this inﬁnite sequence of random variables (S0, A0, S1, A1, . . .) can be shown using the
Ionescu-Tulcea theorem (Lattimore and Szepesvári, 2020, p513). We assume that the rewards are
bounded, i.e. |r(s, a)| ≤ rmax < ∞, ∀(s, a) ∈ S × A, and the MDP is unichain and aperiodic (see
Sec 2.2 for MDP classiﬁcation).

(cid:2)(cid:80)

The solution to a sequential decision making problem is an optimal mapping from every state to a
probability distribution over the (available) action set A in that state.1 It is optimal with respect to
some optimality criterion, as discussed later in Sec 2.1. Any of such a mapping (regardless whether it
is optimal) is called a policy and generally depends on timestep t. It is denoted as πt : S (cid:55)→ [0, 1]|A|,
or alternatively πt : S × A (cid:55)→ [0, 1], where πt(at|st) := Pr{At = at|St = st} indicating the
probability of selecting action at ∈ A given the state st ∈ S at timestep t ∈ T . Thus, each action is
sampled from a conditional action distribution, i.e. At|St = st ∼ πt(·|st).

The most speciﬁc solution space is the stationary and deterministic policy set ΠSD whose policies
π ∈ ΠSD are stationary (time-invariant), i.e. π := π0 = π1 = . . . = πtmax−1, as well as deterministic,
i.e. π(·|st) has a single action support (hence the mapping is reduced to π : S (cid:55)→ A). In this work,
we consider a more general policy set, that is the stationary policy set ΠS. It includes the stationary
randomized (stochastic) set ΠSR and its degenerate counterpart: the stationary deterministic set ΠSD.

2.1 Optimality criteria

In a basic notion of optimality, a policy with the largest value is optimal. That is,

vx(π∗

x) ≥ vx(π),

∀π ∈ ΠS.

(1)

Here, the function vx measures the value (utility) of a policy π based on the inﬁnite reward sequence
that is earned by an agent following π. The subscript x indicates the speciﬁc type of value functions,
which induces a speciﬁc x-optimality criterion, hence the x-optimal policy, denoted as π∗
x.

1In general, different states may have different action sets.

2

One intuitive2 value function is the expected total reward. That is,

vtot(π, s) := lim

tmax→∞

EAt∼π(·|st),St+1∼p(·|st,at)

(cid:34)tmax−1
(cid:88)

(cid:12)
(cid:12)
(cid:12)S0 = s, π
r(St, At)

(cid:35)
,

∀π ∈ ΠS, ∀s ∈ S.

t=0

(2)
However, vtot may be inﬁnite (unbounded, divergent, non-summable).3 Howard (1960) therefore,
examined the expected average reward (also referred to as the gain) deﬁned as

vg(π, s) := lim

tmax→∞

1
tmax

EAt∼π(·|st),St+1∼p(·|st,at)

(cid:34)tmax−1
(cid:88)

t=0

r(St, At)

(cid:12)
(cid:12)
(cid:12)S0 = s, π

(cid:35)
,

(3)

which is ﬁnite for all π ∈ ΠS and all s ∈ S. For more details, including interpretation about the gain,
we refer the reader to (Dewanto et al., 2020).

Alternatively, Blackwell (1962, Sec 4) attempted tackling the inﬁniteness of (2) through the expected
total discounted reward4, which was studied before by Bellman (1957). That is,

vγ(π, s) := lim

tmax→∞

EAt∼π(·|st),St+1∼p(·|st,at)

(cid:34)tmax−1
(cid:88)

γtr(St, At)

(cid:12)
(cid:12)
(cid:12)S0 = s, π

(cid:35)
,

∀π ∈ ΠS, ∀s ∈ S,

t=0

(4)
with a discount factor γ ∈ [0, 1). In particular, according to what is later known as the truncated
Laurent series expansion (7), Blackwell suggested ﬁnding policies that are γ-discounted optimal
for all discount factors γ sufﬁciently close to 1. He also established their existence in ﬁnite MDPs.
Subsequently, Smallwood (1966) identiﬁed that the discount factor interval can be divided into a
ﬁnite number of intervals5, i.e. [0 = γm, γm−1), [γm−1, γm−2), . . . , [γ0, γ−1 = 1), in such a way
that there exist policies π∗
γi for 0 ≤ i ≤ m, that are γ-discounted optimal for all γ ∈ [γi, γi−1). This
leads to the concept of Blackwell optimality. A policy π∗
Bw is Blackwell optimal if there exists a
critical6 discount factor γBw ∈ [0, 1) such that

vγ(π∗

Bw, s) ≥ vγ(π, s),

for γBw ≤ γ < 1, and for all π ∈ ΠS and all s ∈ S.

(5)

Note that whenever the policy value function v depends not only on policies π but also on states s
from which the value is measured, the basic optimality (1) requires that the optimal policy has a value
greater than or equal to the other policies’ values in all state s ∈ S.

In order to obtain Blackwell optimal policies, Veinott (1969, Eqn 27) introduced a family of new
optimality criteria. That is, a policy π∗

n is n-discount optimal for n = −1, 0, . . ., if

lim
γ→1

(cid:16)

1
(1 − γ)n

vγ(π∗

n, s) − vγ(π, s)

(cid:17)

≥ 0,

∀π ∈ ΠS, ∀s ∈ S.

He showed that (n = |S|)-discount optimality7 is equivalent to Blackwell optimality because the
selectivity increases with n (hence, ni-discount optimality implies nj-discount optimalities for all
i > j). Then, he developed a policy-iteration algorithm (for ﬁnding n-discount optimal policies) that
utilizes the full Laurent series expansion of vγ(π) for any π ∈ ΠS, where the policy value vector

2This intuition seems to lead to the so-called reward hypothesis (Sutton and Barto, 2018, p53).
3If a limit is equal to inﬁnity, then we assert that such a limit does not exist. No comparison can be made

between ﬁnite and inﬁnite policy values, as well as between inﬁnite policy values.

4There are other discounting schemes, e.g. 1/(t2 + t), 1/(1 + κt) for κ > 0 (hyperbolic), and t−κ for κ > 1
(Hutter, 2006; Lattimore and Hutter, 2014), but they do not exhibit advantageous properties as the geometric
discounting (4), such as its decomposition via (6). Hence, those other schemes are not considered here.

5See also Hordijk et al. (1985) and Feinberg and Shwartz (2002, Thm 2.16).
6It is critical in that it speciﬁes the sufﬁciency of being close to 1 for attaining Blackwell optimal policies.
7This is eventually reﬁned to (n = |S| − nre(π∗
Bw) is the number of

Bw))-discount optimality, where nre(π∗

recurrent classes under the Blackwell optimal policy (Feinberg and Shwartz, 2002, Thm 8.3, Sec 8.1.4).

3

vx(π) ∈ R|S| is obtained by stacking the state-wise values vx(π, s) altogether for all s ∈ S. That is,

vγ(π) =

1
γ

(cid:16) γ

1 − γ

v−1(π) + v0(π) +

∞
(cid:88)

n=1

(cid:17)n

(cid:16) 1 − γ
γ

(cid:17)

vn(π)

(Full expansion)

(6)

v−1(π) + v0(π) +

(cid:40)

1 − γ
γ

v0(π) +

v−1(π) + v0(π) +

e(π, γ)

,

(cid:26)

(cid:27)

=

1
1 − γ

=

1
1 − γ

(cid:41)

∞
(cid:88)

(cid:17)n

(cid:16) 1 − γ
γ

1
γ
n=1
(Using the additive identity: v0 − v0 = 0)

vn(π)

(Truncated expansion)

(7)

where vn ∈ R|S| for n = −1, 0, . . . denotes the expansion coefﬁcients. The truncated form ﬁrst
appeared (before the full one) in Blackwell (1962, Thm 4a), where v−1 is equivalent to the gain
from all states vg ∈ R|S|, v0 is equivalent to the so-called bias, and e(π, γ) converges to 0 as γ
approaches 1, that is limγ→1 O(1 − γ) = 0.

2.2 MDP classiﬁcation

A stationary policy π of a ﬁnite MDP induces a stationary ﬁnite Markov chain (MC) with a |S|-by-|S|
one-step transition stochastic matrix Pπ, whose [s, s(cid:48)]-th entry indicates

pπ(s(cid:48)|s) := Pr{St+1 = s(cid:48)|St = s; π} =

(cid:88)

a∈A

p(s(cid:48)|s, a)π(a|s),

∀s, s(cid:48) ∈ S.

(8)

Therefore, an MDP can be classiﬁed on the basis of the set of MCs induced by its stationary policies.
To this end, we need to review the classiﬁcation of states, then of MCs below; mainly following
Puterman (1994, Appendix A) and Douc et al. (2018, Ch 6.2).

π(s(cid:48)|s) > 0 for some t ≥ 0.

π denote the t-th power of Pπ of a Markov chain. This P t

π is another
π(s(cid:48)|s) := Pr{St = s(cid:48)|S0 = s; π},

State classiﬁcation: Let P t
|S|-by-|S| stochastic matrix, whose [s, s(cid:48)]-entry indicates pt
i.e. the probability of being in state s(cid:48) ∈ S in t timesteps starting from a state s ∈ S.
A state s(cid:48) ∈ S is accessible from state s ∈ S, denoted as s → s(cid:48), if pt
Furthermore, s and s(cid:48) communicate if s → s(cid:48) and s(cid:48) → s.
A subset ˜S of S is called a closed set if no state outside ˜S is accessible from any state in ˜S.
Furthermore, a subset ˜S of S is called a recurrent class (or a recurrent chain) if all states in ˜S
communicate and ˜S is closed. In a ﬁnite MC, there exists at least one recurrent class.
vis := (cid:80)∞
Let ns,π
vis denote the number of visits to a state s under a policy π. That is, ns,π
I[St = s|π],
where I is an identity (indicator) operator. Then, the expected number of visits to a state s ∈ S
starting from the state s itself under a policy π is given by E[N s,π
A state s ∈ S is recurrent if and only if E[N s,π
vis ] = ∞. This means that a recurrent state will be
re-visited inﬁnitely often. Equivalently, starting from a recurrent state s ∈ S, the probability of
returning to s itself for the ﬁrst time in ﬁnite time is 1.
A state s ∈ S is transient if and only if E[N s,π
vis ] < ∞. This means that a transient state will never
be re-visited again after some point in time. Equivalently, starting from a transient state s ∈ S, the
probability of returning to s itself for the ﬁrst time in ﬁnite time is less than 1. A transient state is a
non-recurrent state (hence, every state is either recurrent or transient).

vis ] = (cid:80)∞

π(s|s).

t=0 pt

t=0

The period of a state s ∈ S is deﬁned as the greatest common divisors tgcd of all t ≥ 1 for which
pt
π(St = s|S0 = s) > 0. Whenever the period tgcd > 1, we clasify s as periodic. Otherwise, s
is aperiodic (not periodic). Intuitively, if returning to a state s occurs at irregular times, then s is
aperiodic. Note that periodicity is a class property, implying that all states in a recurrent class have
the same period.

An ergodic class (or an ergodic chain) is a class that is both recurrent and aperiodic. Its recurrent and
aperiodic states are referred to as ergodic states. Note however, that the term “ergodic” is also used in
the ergodic theory (mathematics), in which its deﬁnition does not involve the notion of aperiodicity.

4

Markov chain (MC) classiﬁcation: An MC is irreducible if the set of all its states forms a single
recurrent class. An irreducible MC is aperiodic if all its states are aperiodic. Otherwise, it is periodic.
An MC is reducible if it has both recurrent states and transient states. The state set of a reducible MC
can be partitioned into one or more disjoint recurrent classes, plus a set of transient states.

An MC is unichain if it consists of a single (uni-) recurrent class (chain), plus a (possibly empty) set
of transient states. Otherwise, it is multichain.

MDP classiﬁcation based on the pattern of MCs induced by all stationary policies: An MDP
is unichain if the induced MC corresponding to every stationary policy is unichain. Otherwise, it is
multichain (i.e. at least one induced MC is multichain, containing two or more recurrent classes).
This is a restrictive deﬁnition of a multichain MDP. In the literature, its loose deﬁnition is also used,
where it simply means a general MDP.

A recurrent MDP is a special case of unichain MDPs, whenever the MC corresponding to every
stationary policy is irreducible. In other words, a recurrent MDP is a unichain MDP whose all induced
MCs have an empty transient state set.

MDP classiﬁcation based on the pattern of state accessibility under some stationary policy:
An MDP is communicating (also termed strongly connected) if, for every pair of states s and s(cid:48) in
the state set S, there exists a stationary policy (which may depend on s and s(cid:48)) under which s(cid:48) is
accessible from s. A recurrent MDP is always communicating, but a communicating MDP may not
be recurrent, it may be unichain or multichain.

An MDP is weakly communicating (also termed simply connected) if there exists a closed state
set ˜S ⊆ S, for which there exists a stationary policy under which s is accessible from s(cid:48) for
every pair of states s, s(cid:48) ∈ ˜S, plus a (possibly empty) set of transient states under every stationary
policy. This weakly communicating classiﬁcation is more general than the communicating in that
any communicating MDP is weakly communicating without any state that is transient under every
stationary policy. Unichain MDPs are always weakly communicating.

There exists an MDP that is not weakly communicating. Such a non-weakly-communicating MDP is
always multichain, but a multichain MDP may also be weakly communicating (or communicating).

2.3 Discounting in environments with an inherent notion of discounting

A environment with an inherent notion of discounting has a discount factor γ that encodes one of the
following entities. Thus, γ is part of the environment speciﬁcation (deﬁnition, description).
Firstly, the time value of rewards, i.e. the value of a unit reward t timesteps in the future is γt:
This is related to psychological concepts. For example, some people prefer rewards now rather
than latter (Mischel et al., 1972), hence they assign greater values to early rewards through a small
γ (being shortsighted). It is also natural to believe that there is more certainty about near- than
far-future, because immediate rewards are (exponentially more) likely due to recent actions. The
time preference is also well-motivated in economics (Samuelson, 1937). This includes γ for taking
account of the decreasing value of money (because of inﬂation), as well as the interpretation of
(1 − γ)/γ as a positive interest rate. Moreover, commercial activities have failure (abandonment) risk
due to changing government regulation and consumer preferences over time.

Secondly, the uncertainty about random termination independent of the agent’s actions: Such
termination comes from external control beyond the agent, e.g. someone shutting down a robot,
engine failure (due to weather/natural disaster), or death of any living organisms.

In particular, whenever the random termination time Tmax follows a geometric distribution
Geo(p = 1 − γ), we have the following identity between the total and discounted rewards,

vTmax(π, s) := ESt,At

(cid:34)
ETmax

(cid:34)Tmax−1
(cid:88)

t=0

r(St, At)

(cid:12)
(cid:12)
(cid:12)S0 = s, π

(cid:35)(cid:35)

= vγ(π, s)
,
(cid:124) (cid:123)(cid:122) (cid:125)
See (4)

∀π ∈ ΠS, ∀s ∈ S,

(9)
where the discount factor γ plays the role of the geometric distribution parameter (Puterman, 1994,
Prop 5.3.1). This discounting implies that at every timestep (for any state-action pair), the agent has

5

a probability of (1 − γ) for entering the 0-reward absorbing terminal state, see Fig 4a. Note that
because γ is invariant to states and actions (as well as time), this way of capturing the randomness
of Tmax may be inaccurate in cases where termination depends on states, actions, or both.

3 Discounting without an inherent notion of discounting

From now on, we focus on environments without inherent notion of discounting, where γ is not part
of the environment speciﬁcation (cf. Sec 2.3). We emphasize the qualiﬁcation “inherent” since any
MDP can always be thought of having some notion of discounting from the Blackwell optimality
point of view (5). This is because a Blackwell optimal policy is guaranteed to exist in ﬁnite MDPs
(Puterman, 1994, Thm 10.1.4); implying the existence of its discount factor γBw ∈ (0, 1] and of a
(potentially very long) ﬁnite-horizon MDP model that gives exactly the same Blackwell-optimal
policy as its inﬁnite-horizon counterpart (Chang et al., 2013, Ch 1.3).

When there is no inherent notion of discounting, the discount factor γ is imposed for bounded sums
(Sec 2.1) and becomes part of the solution method (algorithm). This is what we refer to as artiﬁcial
discounting, which induces artiﬁcial interpretation as for instance, those described in Sec 2.3. The
γBw (mentioned in the previous paragraph) is one of such artiﬁcial discount factors. In addition to
bounding the sum, we observe other justiﬁcations that have been made for introducing an artiﬁcial γ.
They are explained in the following Secs 3.1, 3.2 and 3.3.

3.1 Approximation to the average reward (the gain) as γ approaches 1

For recurrent MDPs, the gain optimality is the most selective8 because there are no transient states
(Feinberg and Shwartz, 2002, Sec 3.1). This implies that a gain optimal policy is also Blackwell
optimal in recurrent MDPs, for which one should target the gain optimality criterion. Nonetheless,
the following relationships exist between the average reward vg and discounted reward vγ value
functions of a policy π ∈ ΠS.

Firstly, for every state s0 ∈ S,

vg(π, s0) = lim
γ→1

(1 − γ) vγ(π, s0)

(Puterman, 1994, Corollary 8.2.5)

(10)

= (1 − γ)

(cid:88)

s∈S

p(cid:63)
π(s|s0) vγ(π, s),

∀γ ∈ [0, 1),

(Singh et al., 1994, Sec 5.3)

(11)

where p(cid:63)
π(s|s0) denotes the stationary probability of a state s, i.e. the long-run (steady-state) proba-
bility of being in state s when the MC begins in s0. Here, (10) is obtained by multiplying the left and
right hand sides of (7) by (1 − γ), then taking the limit of both sides as γ → 1. The derivation of (11)
begins with taking the expectation of vγ(π, S) with respect to p(cid:63)
π, then utilizes the discounted-reward
Bellman (expectation) equation, and the stationarity of p(cid:63)

π. That is,

(cid:88)

s∈S

p(cid:63)
π(s|s0)

(cid:26)

(cid:27)

vγ(π, s)

=

(cid:26)

p(cid:63)
π(s|s0)

rπ(s) + γ

(cid:88)

s∈S

(cid:88)

s(cid:48)∈S

pπ(s(cid:48)|s)vγ(π, s(cid:48))

(cid:27)

(cid:124)

(cid:123)(cid:122)
vγ (π, s) via the Bellman equation

(cid:125)

(cid:88)

=

p(cid:63)
π(s|s0)rπ(s)

+ γ

(cid:88)

(cid:88)

π(s|s0)pπ(s(cid:48)|s)
p(cid:63)

vγ(π, s(cid:48)),

s∈S
(cid:124)

(cid:123)(cid:122)
Gain vg(π, s0) in (3)

(cid:125)

s(cid:48)∈S

s∈S
(cid:124)
(cid:125)
(cid:123)(cid:122)
π(s(cid:48)|s0) due to stationarity
p(cid:63)

which can be re-arranged to obtain (11). Here, rπ(s) := (cid:80)
a∈A π(a|s)r(s, a) whereas pπ(s(cid:48)|s) is
deﬁned in (8). It is interesting that any discount factor γ ∈ [0, 1) maintains the equality in (11), which
was also proved by Sutton and Barto (2018, p254).

The second relationship pertains to the gradient of the gain when a parameterized policy π(θ) is
used. By notationally suppressing the policy parameterization θ and the dependency on s0, as well as

8For an intuitive explanation about the selectivity of optimality criteria, refer to Fig 1.

6

using ∇ := ∂/∂θ, this relation can be expressed as

∇vg(π) = lim
γ→1

(cid:110) (cid:88)

(cid:88)

s∈S

s(cid:48)∈S

p(cid:63)
π(s)

(cid:88)

a∈A
(cid:124)

p(s(cid:48)|s, a)π(a|s)∇ log π(a|s)

(cid:111)

vγ(π, s(cid:48))

(12)

(cid:123)(cid:122)
∇pπ(s(cid:48)|s)

(cid:125)

(cid:88)

(cid:88)

=

p(cid:63)
π(s)π(a|s)

(cid:104)
qγ(π, s, a)∇ log π(a|s)

(cid:105)

+ (1 − γ)

a∈A

s∈S
(cid:124)

(cid:123)(cid:122)
involving the discounted state-action value qγ

(cid:125)

(cid:124)

(cid:123)(cid:122)
involving the discounted state value vγ

(13)

(cid:104)
p(cid:63)
π(s)

(cid:88)

s∈S

vγ(π, s)∇ log p(cid:63)

π(s)

(cid:105)
,

(cid:125)

for all γ ∈ [0, 1). Notice that the right hand sides (RHS’s) of (12) and (13) involve the
discounted-reward value functions, i.e. the state value function vγ(π, s), ∀s ∈ S in (4) and the
corresponding (state-)action value function qγ(π, s, a), ∀(s, a) ∈ S × A. They are related via
vγ(π, s) = EA∼π[qγ(π, s, A)]. The identity (12) was shown by Baxter and Bartlett (2001, Thm 2),
whereas (13) was derived from (11) by Morimura et al. (2010, Appendix A).

Thus for attaining average-reward optimality, one can maximize vγ but merely as an approximation to
vg because the equality in (10) is only in the limit, i.e. setting γ exactly to 1 is prohibited by deﬁnition
(4). The similiar limiting behaviour applies to (12), where vγ is used to approximately compute
the gain gradient ∇vg, yielding approximately gain-optimal policies. In particular, Jin and Sidford
(2021, Lemma 3) proved that an (cid:15)-gain-optimal policy is equivalent to an
3(1−γ) -discounted-optimal
policy with a certain γ value that depends on (cid:15) and a parameter describing some property of the
target MDP. Here, a policy π is said to be ε-x-optimal for any positive ε and a criterion x if its values
vx(π, s) ≥ vx(π∗
The aforementioned approximation to vg by vγ with γ → 1 seems to justify the use of stationary state
distribution p(cid:63)
π in (14) below) to weight
the errors in recurrent states in approximate discounted policy evaluation, see e.g. Dann et al. (2014,
Eqn 4). In fact, the identity in (10) implies the following connection,

π (instead of the (improper) discounted state distribution pγ

x, s) − ε, for all s ∈ S.

(cid:15)

P (cid:63)
π rπ
(cid:124) (cid:123)(cid:122) (cid:125)
vg(π)

= lim
γ→1

(1 − γ) P γ

,
π rπ
(cid:124) (cid:123)(cid:122) (cid:125)
vγ (π)

such that P (cid:63)

π = lim
γ→1

(1 − γ)P γ

π , hence p(cid:63)

π = lim
γ→1

where P (cid:63)

π = lim

tmax→∞

1
tmax

tmax−1
(cid:88)

t=0

P t
π ,

and P γ

π = lim

tmax→∞

tmax−1
(cid:88)

(γPπ)t.

t=0

(1 − γ)pγ
π
(cid:125)
(cid:123)(cid:122)
(cid:124)
a proper distribution

,

(14)

(15)

Here, the s0-th row of P (cid:63)
On the other hand, the s0-th row of P γ
tribution pγ
this connection suggests that p(cid:63)
function only when γ → 1. Otherwise, pγ

π contains the probability values of the stationary state distribution p(cid:63)

π(·|s0).
π contains the values of the improper discounted state dis-
π(s(cid:48)|s0) = 1/(1 − γ) (cid:54)= 1. Importantly,
π is suitable as weights in the discounted value approximator’s error

π(·|s0), which is improper because (cid:80)

π may be more suitable.

s(cid:48)∈S pγ

It is also an approximation in (11) whenever vγ is weighted by some initial state distribution ˚p (such
as in (18)) or transient state-distributions pt
π. In (13), the second
RHS term is typically ignored since calculating ∇ log p(cid:63)
π(s) is difﬁcult in RL settings.9 Consequently,
∇vg(π) is approximated (closely whenever γ is close to 1) by the ﬁrst RHS term of (13), then by
sampling the state S ∼ p(cid:63)

π (after the agent interacts long “enough” with its environment).

π, which generally differs from p(cid:63)

Moreover, approximately maximizing the average reward via discounting is favourable because
discounting formulation has several mathematical virtues, as described in Sec 3.3.

3.2 A technique for attaining the most selective optimality with γ ∈ [γBw, 1)

As discussed in the previous Sec 3.1, γ-discounted optimality approximates the gain optimality as
γ → 1. This is desirable since the gain optimality is the most selective in recurrent MDPs. In
unichain MDPs however, the gain optimality is generally underselective since the gain ignores the

9The difﬁculty of computing the gradient of state distributions, such as ∇ log p(cid:63)

π(s), in RL motivates the

development of the policy gradient theorem (Sutton and Barto, 2018, Ch 13.2).

7

reward = 2

s0

s1

s2

reward = 0

reward = 1

reward = 1

Figure 1: A symbolic diagram of a unichain MDP with S = {s0, s1, s2} and deterministic transitions.
There are two stationary policies that differ in their blue or red action selection in the left-most
state s0, hence we call them: red and blue policies. Under both policies, the right-most state s2
is a 0-reward (absorbing) recurrent state szrat, hence this unichain MDP is a szrat-model. Both
policies are gain-optimal and (n = 0)-discount optimal, but only the blue policy is (n = 1)-discount
optimal. Therefore, (n = 1 = |S| − 2)-discount optimality is the most selective, which is equivalent
to Blackwell optimality. In particular for this simple environment, the Blackwell discount factor
is trivially γBw = 0. Note that the total reward and the (n = 0)-discount values of both policies
are equal, i.e. vtot(red, s0) = vtot(blue, s0) = vn=0(red, s0) = vn=0(blue, s0) = 2 such that the
total reward and (n = 0)-discount criteria are underselective. This example MDP is adopted from
Puterman (1994, Fig 10.1.1) and Mahadevan (1996b, Fig 1).

rewards earned in transient states (for an illustrative example, see Fig 1). Consequently, multiple
gain-optimal policies prescribe different action selections (earning different rewards) in transient
states. The underselectiveness of gain optimality (equivalent to (n = −1)-discount optimality) can
be reﬁned up to the most selective optimality by increasing the value of n from −1 to 0 (or higher if
needed up to n = (|S| − 2) for unichain MDPs) in the family of n-discount optimality (Sec 2.1).

Interestingly, such a remedy towards the most selective criterion can also be achieved by specifying
a discount factor γ that lies in the Blackwell’s interval, i.e. γ ∈ [γBw, 1), for the γ-discounted
optimality (5). This is because the resulting π∗
γ∈[γBw,1), which is also called a Blackwell optimal
policy, is also optimal for all n = −1, 0, . . . in n-discount optimality (Puterman, 1994, Thm 10.1.5).
Moreover, Blackwell optimality is always the most selective regardless of the MDP classiﬁcation,
inheriting the classiﬁcation invariance property of γ-discounted optimality. Thus, artiﬁcial discounting
can be interpreted as a technique to attain the most selective criterion (i.e. the Blackwell optimality)
whenever γ ∈ [γBw, 1) not only in recurrent but also unichain MDPs, as well as the most general
multichain MDPs.

Targetting the Blackwell optimality (instead of gain optimality) is imperative, especially for episodic
environments10 that are commonly modelled as inﬁnite-horizon MDPs (so that the stationary policy
set ΠS is a sufﬁcient space to look at for optimal policies).11 Such modelling is carried out by
augmenting the state set with a 0-reward absorbing terminal state (denoted by szrat), as shown in
Fig 4a. This yields a unichain MDP with a non-empty set of transient states. For such szrat-models,
the gain is trivially 0 for all stationary policies so that gain optimality is underselective. The (n = 0)-
discount optimality improves the selectivity. It may be the most selective (hence, it is equivalent
to Blackwell optimality) in some cases. Otherwise, it is underselective as well, hence some higher
n-discount optimality criterion should be used, e.g. n = 1 for an MDP shown in Fig 1. It is also worth
noting that in szrat-models, (n = 0)-discount optimality is equivalent to the total reward optimality
whose vtot (2) is ﬁnite (Puterman, 1994, Prop 10.4.2). The total reward optimality therefore may also
be underselective in some cases.

10 Episodic environments are those with at least one terminal state. Once the agent enters the terminal state,

the agent-environment interaction terminates.

11In ﬁnite-horizon MDPs, the optimal policy is generally non-stationary, where it depends on the number of
remaining timesteps (in addition to the state). Intuitively, if we (approximately) had only 1 week to live, would
we act the same way as if we (approximately) had 10 years to live? In basketball, a long shot attempt (from
beyond half court) is optimal only at the ﬁnal seconds of the game.

8

Towards obtaining Blackwell optimal policies in unichain MDPs, the relationship between maximiz-
ing γ-discounted and n-discount criteria can be summarized as follows (see also Fig 6).

argmax
π∈ΠS

vγ∈[γBw,1)(π, s)

(cid:124)

=




(cid:123)(cid:122)
Blackwell-optimal policies


(cid:104)

(cid:125)

argmax
π∈ΠS

(1 − γ)vγ(π, s)

v−1(π, s) = lim
γ→1
(cid:123)(cid:122)
(cid:125)
Based on (10)
(1 − γ)vγ(π, s) − v−1(π, s)
1 − γ

v0(π, s) = lim
γ→1

(cid:124)
(cid:104)

(cid:2)vn(π, s)(cid:3) for n = 1, 2, . . . , (|S| − 3)

(cid:123)(cid:122)
Based on (7)

(cid:2)vn=(|S|−2)(π, s)(cid:3)

if MDPs are unichain,

argmax
π∈Π∗
n=−1

(cid:124)

argmax
π∈Π∗
n−1
argmax

π∈Π∗

n=(|S|−3)

(cid:105)

if MDPs are recurrent (see Sec 3.1),

(cid:105)

if MDPs are unichain
and (n = 0)-discount
is the most selective,

(16)

(cid:125)

if MDPs are unichain and n-
discount is the most selective,

for all s ∈ S, where Π∗

n denotes the n-discount optimal policy set for n = −1, 0, . . ..

From the second case in the RHS of (16), we know that v0 can be computed by taking the limit of a
function involving vγ and v−1(= vg) as γ approaches 1. This means that for unichain MDPs, the
Blackwell optimal policies may be obtained by setting γ very close to 1, similar to that for recurrent
MDPs (the ﬁrst case) but not the same since the function of which the limit is taken differs.

In practice, the limits in the ﬁrst and second cases in (16) are computed approximately using a discount
factor γ close to unity, which is likely in the Blackwell’s interval, i.e. γBw ≤ (γ ≈ 1) < 1. Paradoxi-
cally however, this does not necessarily attain the Blackwell optimality because the ﬁnite-precision
computation involving γ ≈ 1 yields quite accurate estimation to the limit values: maximizing the ﬁrst
and second cases in the RHS of (16) with a high γ ≈ 1 attains (approximately) gain (n = −1) and
bias (n = 0) optimality respectively, which may be underselective in unichain MDPs. Consequently
in practice, the most selective Blackwell optimality can always be achieved using γ that is at least
as high as γBw but not too close to 1. That is, γBw ≤ γ ≤ ¯γ < 1 for some practical upper bound ¯γ,
which is computation (hence, implementation) dependent.

3.3 Contraction, variance reduction, and independence from MDP classiﬁcation

Discounted reward optimality is easier to deal with than its average reward counterpart. This can be
attributed to three main factors as follows.

Firstly, the discounted-reward theory holds regardless of the classiﬁcation of the induced MCs
(Sec 2.2), whereas that of the average reward involves such classiﬁcation. Because in RL settings,
the transition probability p(s(cid:48)|s, a) is unknown, average reward algorithms require estimation or
assumption about the chain classiﬁcation, speciﬁcally whether unichain or multichain. Nevertheless,
note that such (assumed) classiﬁcation is needed in order to apply a speciﬁc (simpler) class of average-
reward algorithms: leveraging the fact that a unichain MDP has a single scalar gain (associated with
its single chain) that is constant across all states, whereas a multichain MDP generally has different
gain values associated with its multiple chains.
Secondly, the discounted-reward Bellman optimality operator B∗
factor γ serves as the contraction modulus (hence, B∗

γ is contractive, where the discount

γ is a γ-contraction). That is,

(cid:107)B∗

for any vectors v, v(cid:48) ∈ R|S|,

γ[v(cid:48)](cid:107)∞ ≤ γ(cid:107)v − v(cid:48)(cid:107)∞,
γ[v](s) := maxa∈A{r(s, a) + γ (cid:80)

γ[v] − B∗
where in state-wise form, B∗
s(cid:48)∈S p(s(cid:48)|s, a)v(s(cid:48))}, ∀s ∈ S, and
(cid:107)v(cid:107)∞ := maxs∈S |v(s)| denotes the maximum norm. This means that B∗
γ makes v and v(cid:48) closer
by at least γ such that the sequence of iterates vk+1 ← B∗
γ[vk] converges to the unique ﬁxed point
γ as k → ∞ from any initial vk=0 ∈ R|S|. This is based on the Banach’s ﬁxed-point theorem
of B∗
Szepesvári (2010, Thm A.9). In particular as k → ∞, we obtain B∗
γ, where v∗
γ
denotes the optimal discounted value, i.e. vγ(π∗
γ(cid:107)∞ = 0. In the absense
of γ, the contraction no longer holds. This is the case for the average-reward Bellman optimality

γ). Thus, limk→∞ (cid:107)vk − v∗

γ[vk] = vk = v∗

9

operator B∗
value iteration based on B∗

g[v](s) := maxa∈A{r(s, a) + (cid:80)

g is not guaranteed to converge (Mahadevan, 1996a, Sec 2.4.3).

s(cid:48)∈S p(s(cid:48)|s, a)v(s(cid:48))}, ∀s ∈ S. As a result, the basic

(cid:2)r(s, A) + γ (cid:80)

γ [v](s) := EA∼π

The aforementioned contraction property also applies to the discounted-reward Bellman expectation
s(cid:48)∈S p(s(cid:48)|s, A)v(s(cid:48))(cid:3), ∀s ∈ S. As a consequence
operator, i.e. Bπ
γ [vk]
of Banach’s ﬁxed-point theorem, we have limk→∞ (cid:107)vk − vπ
is iteratively applied on any initial vk=0 ∈ R|S| for k = 0, 1, . . ., whereas vπ
γ := vγ(π) is the
discounted policy value of a policy π. In other words, vπ
γ such that
γ = Bπ
vπ
γ ], which is known as the discounted-reward Bellman evaluation equation (γ-BEE). This
γ-BEE is further utilized to derive a TD-based parametric value approximator whose convergence
depends on the fact that γ ∈ [0, 1). That is, the approximator’s error minimizer formula involves the
inverse (I − γPπ)−1, which exists (Sutton and Barto, 2018, p206). This is in contrast to the matrix
(I − Pπ) whose inverse does not exist (Puterman, 1994, p596).

γ is the unique ﬁxed point of Bπ

γ (cid:107)∞ = 0, where vk+1 ← Bπ

γ [vπ

In addition, the contractive nature induced by γ is also utilized for computing state similarity metrics
(Castro, 2020; Ferns et al., 2006). There, γ guarantees the convergence of the metric operator to its
ﬁxed point (when such an operator is iteratively applied). Note that the notion of state similarity plays
an important role in for example, state aggregation and state representation learning.

Third, discounting can be used to reduce the variance of, for example policy gradient estimates,
at the cost of bias-errors (Baxter and Bartlett, 2001, Thm 3; Kakade, 2001, Thm 1). In particular,
the variance (the bias-error) increases (decreases) as a function of 1/(1 − γ) = (cid:80)∞
t=0 γt. This is
because the effective number of timesteps (horizon) can be controlled by γ. This is also related to the
fact that the inﬁnite-horizon discounted reward vγ (4) can be (cid:15)-approximated by a ﬁnite horizon τ
proportional to logγ(1 − γ), as noted by Tang and Abbeel (2010, Footnote 1). That is,

(cid:34) ∞
(cid:88)

(cid:35)
γtr(St, At)

(cid:34)τ −1
(cid:88)

(cid:35)
γtr(St, At)

− E

(cid:34) ∞
(cid:88)

(cid:35)
γtr(St, At)

≤

= E

(cid:15) := E

t=0

t=0

t=τ

∞
(cid:88)

t=τ

γtrmax =

γτ rmax
1 − γ

.

This is then re-arranged to obtain γτ ≥ ((1 − γ)(cid:15))/rmax, whose both sides are taken to the logarithm
with base γ to yield

τ ≥ logγ

(1 − γ)(cid:15)
rmax

,

hence, the smallest of such a ﬁnite horizon is τ =

(cid:108)

logγ

(1 − γ)(cid:15)
rmax

(cid:109)
,

where (cid:100)x(cid:101) indicates the smallest integer greater than or equal to x ∈ R.

4 Artiﬁcial discount factors are sensitive and troublesome

The artiﬁcial discount factor γ (which is part of the solution method) is said to be sensitive because
the performance of RL methods often depends largely on γ. Fig 2a illustrates this phenomenon using
Qγ-learning with various γ values. As can be seen, higher γ leads to slower convergence, whereas
lower γ leads to suboptimal policies (with respect to the most selective criterion, which in this case, is
the gain optimality since the MDP is recurrent). This trade-off is elaborated more in Secs 4.1 and 4.2.
The sensitivity to γ has also been observed in the error (and even whether convergence or divergence)
of approximate policy evaluation methods with function approximators (Scherrer, 2010, Fig 1; Sutton
and Barto, 2018, Example 11.1).

Additionally, Fig 3 shows the effect of various γ values on the optimization landscapes on which
policy gradient methods look for the maximizer of the discounted value vγ. It is evident that different
γ values induce different maximizing policy parameters. From the 2D visualization in Fig 3, we can
observe that the landscape of vγ begins to look like that of the gain vg when γ is around γBw, then
becomes more and more look like it as γ approaches 1. When γ is far less than γBw, the maximizer
of vγ does not coincide with that of vg. In such cases, some local maximizer of vγ may be desirable
(instead of the global one) because of its proximity to the maximizer of vg, which represents the
optimal point of the most selective criterion for recurrent MDPs examined in Fig 3.

The artiﬁcial γ is troublesome because its critical value, i.e. γBw, is difﬁcult to determine, even in DP
where the transition and reward functions are known (Hordijk et al., 1985). This is exacerbated by
the fact that γBw is speciﬁc to each environment instance (even from the same environment family, as
shown in Fig 2b). Nevertheless, knowing this critical value γBw is always desirable. For example,

10

(a) Learning curves of Qγ-learning with varying
γ values on GridNav-25, which is episodic but
modelled using srst (Sec 5.5). Empirically, the
critical discount factor is γBw ≈ 0.83 (yellowish).

(b) The critical γBw as a (non-trivial) function of
number of states, and of some constant in the reward
function on three environment families.

Figure 2: Empirical results illustrating the sensitivity and troublesomeness of artiﬁcial discount
factors γ. In the left sub-ﬁgure (a), the gain optimality is the most selective (since the MDP is
recurrent), where the optimal gain vg(π∗
g ) is indicated by the black solid horizontal line on top of the
plot. For experimental setup about the environments and learning methods, see Sec 7.

despite the gain optimality can be attained by having γ very close to 1, setting γ ← γBw (or some
value around it) leads to not only convergence to the optimal gain (or close to it) but also faster
convergence, as demonstrated by Qγ-learning (Fig 2a). We can also observe visually in Fig 3 that
the discounted vγ≈γBw -landscape already resembles the gain vg-landscape. Thus, for obtaining the
gain-optimal policy in recurrent MDPs, γ does not need to be too close to 1 (as long as it is larger
than or equal to γBw); see also (16).

Apart from that, γ is troublesome because some derivation involving it demands extra care, e.g. for
handling the improper discounted state distribution pγ
π (14) in discounted-reward policy gradient
algorithms (Nota and Thomas, 2020; Thomas, 2014).

4.1 Higher discount factors lead to slower convergence

According to (10), increasing the discount factor γ closer to 1 makes the scaled discounted reward
(1 − γ)vγ approximate the average reward vg more closely. This means that a discounted-reward
method with such a setting obtains more accurate estimates of gain-optimal policies. However,
it suffers from a lower rate of convergence (to the approximate gain optimality), as well as from
some numerical issue (since for example, it involves the term 1/(1 − γ) that explodes as γ → 1).
This becomes unavoidable whenever γBw is indeed very close to unity because the most selective
Blackwell optimality (equivalent to gain optimality in recurrent MDPs) requires that γ ≥ γBw.

The slow convergence can be explained by examining the effect of the effective horizon induced by γ.
That is, as γ approaches 1, the reward information is propagated to more states (Beleznay et al., 1999,
Fig 1). From discounted policy gradient methods, we also know that an i.i.d state sample from the
discounted state distribution pγ
π in (14) is the last state of a trajectory whose length is drawn from
a geometric distribution Geo(p = 1 − γ), see Kumar et al. (2020, Algo 3). Evidently, the closer γ
to 1, the longer the required trajectory. Also recall that such a geometric distribution has a mean of
1/(1 − γ) and a variance of γ/(1 − γ)2, which blow up as γ → 1.

There are numerous works that prove and demonstrate slow convergence due to higher γ. From them,
we understand that the error (hence, iteration/sample complexity) essentially grows as a function of
1/(1 − γ). Those works include (Thrun and Schwartz, 1993, Fig 3; Melo and Ribeiro, 2007, Thm 1;
Fan et al., 2020, Thm 4.4) for Q-learning with function approximators, (Sutton and Barto, 2018,
Eqn 9.14, 12.8) for TD learning, and (Agarwal et al., 2019, Table 1, 2) for policy gradient methods.
We note that for a speciﬁc environment type and with some additional hyperparameter, Devraj and
Meyn (2020) proposed a variant of Q-learning whose sample complexity is independent of γ.

11

4.2 Lower discount factors likely lead to suboptimal policies

Setting γ further from 1 such that γ < γBw yields γ-discounted optimal policies that are suboptimal
with respect to the most selective criterion (see Fig 2a). From the gain optimality standpoint, lower γ
makes (1 − γ)vγ deviate from vg in the order of O(1 − γ) as shown by (Baxter and Bartlett, 2001).
More generally based on (16), γ < γBw induces an optimal policy π∗
γ<γBw that is not Blackwell
optimal (hence, π∗
γ<γBw is also not gain optimal in recurrent MDPs). This begs the question: is it
ethical to run a suboptimal policy (due to misspecifying the optimality criterion) in perpetuity?

For a parameterized policy in recurrent MDPs, γ < γBw induces a discounted vγ-landscape that
is different form the gain vg-landscape. Fig 3 shows such a discrepancy, which becomes more
signiﬁcant as γ is set further below 1. Therefore, the maximizing parameters do not coincide,
i.e. argmaxθ vγ<γBw (π(θ)) (cid:54)= argmaxθ vg(π(θ)) =: θ∗
g , where θ ∈ Θ denotes the policy parameter
in some parameter set. Interestingly, vγ<γBw (π(θ∗
g )) is a local maximum in vγ-landscape so that the
(γ < γBw)-discounted-reward optimization is ill-posed in that the (global) maximum is not what we
desire in terms of obtaining an optimal policy with respect to the most selective criterion (that is, the
gain optimal policy π(θ∗

g ) in recurrent MDPs).

Petrik and Scherrer (2008, Thm 2) established the following error bound due to misspecifying a
discount factor γ < γBw. That is,

(cid:107)v∗

γBw

−v∗

γ(cid:107)∞ ≤

(γBw − γ) rmax
(1 − γ)(1 − γBw)

,

for v∗

γBw

, v∗

γ ∈ R|S|, and v∗

γ := vγ(π∗

γ) for any γ ∈ [0, 1).

Subsequently, Jiang et al. (2016) reﬁned the above error bound by taking into account the transition
and reward functions. They also highlighted that such an error as a function of γ is not monotonically
decreasing (with increasing γ). This is consistent with what Smallwood (1966) observed, i.e. multiple
disconnected γ-intervals that share the same γ-discounted optimal policy. We note that speciﬁcally
for sparse-reward environments (where non-zero rewards are not received in every timestep), a lower
discount factor γ < γBw is likely to improve the performance of RL algorithms (Petrik and Scherrer,
γ − ˆv∗
2008, Thm 10). They argue that lowering γ decreases the value approximation error (cid:107)v∗
γ(cid:107)∞
more signiﬁcantly than it increases the γ-misspeciﬁcation error (cid:107)v∗
γ denotes an
approximation to v∗
γ.

γ(cid:107)∞. Here, ˆv∗

− v∗

γBw

12

γ = 0.00

γ = 0.35

γ = 0.50

γ = 0.70

γ = 0.85 ≈ γBw

γ = 0.95

Gain

γ = 0.00

γ = 0.35

γ = 0.50

γ = 0.80 ≈ γBw

γ = 0.85

γ = 0.95

Gain

1
3

γ = 0.35

γ = 0.50

γ = 0.55 ≈ γBw

γ = 0.60

γ = 0.75

γ = 0.95

Gain

Figure 3: Policy-value landscapes as a function of two policy parameters θ ∈ R2 (i.e. horizontal and vertical axes) on three environments (from top to bottom rows:
Chain, Taxicab, and Torus, which all induce recurrent MDPs where the gain optimality is the most selective). All columns, except the rightmost (the gain vg),
correspond to the scaled discounted reward (1 − γ)vγ of randomized policies π(θ), measured from a single initial state. The color maps the lowest value to dark-blue,
and the highest value to yellow. These lowest and highest values are respectively of the “worst” and of the optimal deterministic stationary policies with respect to the
corresponding criterion (such that different columns (hence, subplots) have different lowest and highest values). As anticipated, the scaled vγ-landscape becomes
more and more similar to its gain counterpart as γ approaches 1. Particularly, γ ≥ γBw induces a scaled vγ-landscape, whose global maximum coordinate coincides
with that of the gain. Note that the gain landspace in the bottom row only has a yellow-ish region because there is a signiﬁcant difference between the maximum gain
g of the optimal deterministic policy and the maximum gain ˆv∗
v∗
g across the shown landscape (which is only of a portion of the policy parameter space). Their
absolute difference, i.e. (cid:12)
(cid:12)
(cid:12), is of 7.070%, where v∗

g = 0.208. For experimental setup, see Sec 7.

g ≈ v∗
g = 0.224 and ˆv∗

g − v∗

g )/v∗
g

(cid:12)(ˆv∗

5 Beneﬁts of maximizing the average reward in recurrent MDPs

In this Section, we enumerate the beneﬁts of directly maximizing the average reward (gain) optimality
criterion in recurrent MDPs. Loosely speaking, such a recurrent structure is found in continuing
environments12 with cyclical events across all states. For episodic environments, we can obtain
their recurrent MDPs by explicitly modelling the episode repetition, as explained in Sec 5.5. For a
non-exhaustive list of continuing and episodic environments, refer to (Dewanto et al., 2020).

The combination of recurrent MDPs and gain optimality is advantageous. There is no discount
factor γ involved (as a by-product), removing the difﬁculties due to artiﬁcial discounting (Sec 4).
Other beneﬁts are described in the following sub-sections.

5.1 Unconditionally the most selective criterion

Gain optimality is the most selective criterion for recurrent MDPs. This is because all states are
recurrent so that the gain is all that is needed to quantitatively measure the quality of any stationary
policy from those states (such a gain quantity turns out to be constant for all states in recurrent or
more generally unichain MDPs). Recall that the gain is concerned with the long-run rewards (3), and
recurrent states are states that are re-visited inﬁnitely many times in the long-run (Sec 2.2).

Gain optimality therefore is equivalent to Blackwell optimality unconditionally, as well as instan-
taneously in that there is no need for hyperparameter tuning for the optimality criterion (which
fundamentally determines the optimization objective function, hence the overall optimization). This
is in contrast to γ-discounted optimality, where it is equivalent to Blackwell optimality if γ ≥ γBw as
in (5). Moreover since γBw is unknown, tuning γ is necessary (Sec 3.2).

5.2 Uniformly optimal policies

Since recurrent (up to unichain) MDPs have only a single recurrent class (chain), the gain of any
policy is constant across all states. As a result, average-reward policy gradient methods maximize
an objective (17) that is independent of initial states, or generally of initial state distributions. The
resulting gain-optimal policies are said to be uniformly optimal because they are optimal for all initial
states or all initial state distributions (Altman, 1999, Def 2.1).

In contrast, the discounted-reward counterpart maximizes an objective (18) that is deﬁned with
respect to some initial state distribution ˚p (since the discounted-reward value vγ depends on the
state from which the value is measured, as in (4)). Consequently, the resulting γ-discounted optimal
policies may not be optimal for all initial states; they are said to be non-uniformly optimal, as noted
by Bacon (2018, p41). This non-uniform optimality can be interpreted as a relaxation of the uniform
optimality in DP, which requires that the superiority of optimal policies π∗
γ holds in every state,
i.e. vγ(π∗
The objectives of average- and discounted-reward policy gradient methods are as follows.

γ, s0) ≥ vγ(π, s0), for all states s0 ∈ S and all policies π ∈ ΠS, see Sec 2.1.

Average-reward policy gradient objective:

Discounted-reward policy gradient objective:

argmax
θ∈Θ
argmax
θ∈Θ

vg(π(θ)),

E˚p[vγ(π(θ), S0)],

(17)

(18)

where θ ∈ Θ = Rdim(θ) is the policy parameter and S0 ∼ ˚p is the initial state random variable.

5.3 Potentially higher convergence rates

Without delving into speciﬁc algorithms, there are at least two reasons for faster convergence of
average-reward methods (compared to their discounted-reward counterparts), as hinted by Schwartz
(1993, Sec 6) and Van Roy (1998, p114).

First is the common gain across states. Such commonality eases the gain approximation in that no
generalization is required. That is, a single gain estimation is all that is needed for one true gain of all
states in unichain MDPs.

12Continuing environments are those with no terminal state; cf. episodic environments (Footnote 10).

14

Second, average-reward methods optimize solely the gain term in the Laurent series expansion
of vγ in (6). On the other hand, their discounted-reward counterparts optimize the gain, bias, and
higher-order terms altogether simultaneously, whose implication becomes more substantial as γ is set
further below 1, as can be observed in (7).

5.4 Less discrepancy between the learning objective and the performance metric

Since there is a ﬁnite number of timesteps (as training budget) and no inherent notion of discounting,
the ﬁnal learning performance metric ¯ψﬁnal
in RL is typically measured by the average return over
tot
several last experiment-episodes13 (Machado et al., 2018, Sec 5.2; Henderson et al., 2018, Fig 2).
That is,

¯ψﬁnal
tot

:=

1
nlast
xep

nxep
(cid:88)

(cid:34) ˆtxep(i)
max(cid:88)

r(i)
t+1

i=j+1

t=0
(cid:124)

(cid:123)(cid:122)
Return

(cid:35)

(cid:12)
(cid:12)ˆπ∗
(cid:12)
(cid:125)

(cid:34) ˆtxep
max(cid:88)

≈ E

(cid:35)

(cid:12)
(cid:12)ˆπ∗
(cid:12)

∝

Rt+1

t=0

(cid:124)
(cid:125)
(cid:123)(cid:122)
Finite-horizon total reward

1
ˆtxep
max + 1
(cid:124)

(cid:34) ˆtxep
max(cid:88)

E

Rt+1

(cid:12)
(cid:12)ˆπ∗
(cid:12)

t=0
(cid:123)(cid:122)
Finite-horizon average reward

(with j = nxep − nlast
xep)

(cid:35)

(cid:125)

≈ vg(ˆπ∗)
,
(cid:124) (cid:123)(cid:122) (cid:125)
Gain

(19)

where nxep denotes the number of i.i.d experiment-episodes, from which we pick the last nlast
xep
experiment-episodes once the learning is deemed converged to a ﬁnal learned policy ˆπ∗. Each
i-th experiment-episode runs from t = 0 to a ﬁnite ˆtxep(i)
max ≈ tmax, where the reward realization
at every timestep t is denoted by r(i)
t+1. The expectation in (19) of the reward random variable
Rt+1 := r(St, At) is with respect to S0 ∼ ˚p, At ∼ ˆπ∗(·|st) and St+1 ∼ p(·|st, at).
We argue that the learning performance metric ¯ψﬁnal
tot has less discrepancy with respect to the gain vg
than to the discounted reward vγ. In other words, whenever the criterion is vg, what is measured during
training (i.e. the learning performance metric) is more aligned to what the agent learns/optimizes
(i.e. the learning objective).14 There are three arguments for this as follows.
First, since the experiment-episodes are i.i.d, the expectation of ¯ψﬁnal
xep → ∞
tot
to the expected ﬁnite-horizon total reward. This is proportional to the expected ﬁnite-horizon average
reward, which is an approximation to the gain vg(ˆπ∗) due to the ﬁniteness of ˆtxep
max.
Second is the fact that ˆtxep
max is typically ﬁxed for all experiment-episodes during training.15 It is
not randomly sampled from a geometric distribution Geo(p = 1 − γ) even when maximizing vγ
(seemingly because there is no inherent notion of discounting). If it was sampled from such a
geometric distribution, then the expectation of ¯ψﬁnal
tot would converge to vγ, following the identity in
(9). This however would inherently pose a risk of miss-specifying γ (Sec 4.2). This metric gap due
to artiﬁcial discounting is highlighted by Schwartz (1993, Sec 3), van Hasselt (2011, p25), Dabney
(2014, p47), and Van Seijen et al. (2019, Sec 2).
Third is because the induced Markov chain may reach stationarity (or close to it) within ˆtxep
max < ∞
in some environments. That is, P t
π = P t
max, see (15).
If this happens, then some number of rewards are generated from states sampled from the stationary
state distribution p(cid:63)
π (although they are not independent, but Markovian state samples). This makes
the approximation to the gain vg more accurate (although it is inherently biased due to non-i.i.d state
samples) than to the discounted vγ since vg(π) = ES∼p(cid:63)
[rπ(S) := EA∼π[r(S, A)]], which can be
shown to be equivalent to (3).

π for several timesteps t ≤ ˆtxep

in (19) converges as nlast

π Pπ = P t+1

π = P (cid:63)

π

13The term “experiment-episode” refers to one trial (run, roll-out, or simulation), which produces one sample
of trajectory (of states, actions, and rewards). The term “experiment-episode” is applicable to both episodic and
continuing environments. In contrast, the term “episode” only makes sense for episodic environments.
14In some cases, this alignment may be traded-off for more tractable computation/training/analysis.
15One common practice is to set ˆtxep

max to some constant far less than the training budget to obtain
multiple experiment-episodes. This is implemented as for instance, the ‘max_episode_steps’ variable at
https://github.com/openai/gym/blob/master/gym/envs/__init__.py of a seminal and popular RL
environment codebase: OpenAI Gym (Brockman et al., 2016).

15

s0

s2

0

s1

s0

s1

0

0

0

szrat

s2

srst

(a) A unichain szrat-model

(b) A recurrent srst-model

Figure 4: Diagrams of a unichain szrat-model and a recurrent srst-model of an episodic environment
with an (original) state set S = {s0, s1, s2}. The red edge indicates a deterministic transition via
a single self-loop action azrat in a 0-reward absorbing terminal state szrat. On the other hand, the
blue edges indicate possible transitions via a single reset action arst in a resetting state srst. This arst
leads to a next state that is distributed according to the initial state distribution ˚p (which here has all
states s ∈ S as its supports). Transitions via azrat and arst yield a zero reward as indicated by the red
and blue edge labels. The other black unlabeled edges represent examples of transitions with positive
probabilities and any reward values. For a formal szrat- to srst-model conversion, refer to Sec 5.5.

5.5 Modelling the episode repetition explicitly in episodic environments

In order to obtain an inﬁnite-horizon recurrent MDP of an episodic environment, we can model
its episode repetition explicitly. This is in contrast to modelling an episodic environment as an
inﬁnite-horizon MDP with a 0-reward absorbing terminal state (denoted as szrat, which is recurrent
under every policy). This modelling (called szrat-modelling) induces a unichain MDP, where all
states but szrat are transient. In szrat-model, the gain is trivially 0 for all stationary policies, rendering
the gain optimality underselective (Sec 3.2). Fig 4a shows the diagram of a szrat-model.

To explicitly model episode repetition, we augment the original state set with a resetting terminal
state srst (instead of szrat) that has a single available reset action arst (instead of a self-loop action
azrat). This action arst is responsible for a transition from srst to an initial state S0 ∼ ˚p, which yields
a reward of 0. We call this approach srst-modelling, whose example diagram is shown in Fig 4b. The
conversion from a unichain szrat-model to a recurrent srst-model is as follows.

Let S and A denote the original state and action sets of an episodic environment, respectively, before
the augmentation of a 0-reward absorbing terminal state szrat. Given a unichain szrat-model with a
state set S +
zrat = A ∪ {azrat}, an initial state distribution ˚pzrat,
a state transition distribution pzrat, and a reward function rzrat, then the corresponding recurrent
srst-model has the following components.

zrat = S ∪ {szrat}, an action set A+

• A state set S +

rst = S ∪{srst}, an action set A+

˚prst = ˚pzrat, where ˚prst(srst) = ˚pzrat(szrat) = 0.

rst = A∪{arst}, and an initial state distribution

• A state transition distribution prst and a reward function rrst, where

• prst(s(cid:48)|s, a) = pzrat(s(cid:48)|s, a) and rrst(s, a, s(cid:48)) = rzrat(s, a, s(cid:48)), for all s, s(cid:48) ∈ S and

for all a ∈ A,

• prst(srst|s, a) = pzrat(szrat|s, a) and rrst(s, a, srst) = rzrat(s, a, szrat), for all

s, s(cid:48) ∈ S and for all a ∈ A, as well as

• prst(s|srst, arst) = ˚pzrat(s) and rrst(srst, arst, s) = 0, for all s ∈ S.

The above conversion requires that i) reaching szrat is inevitable with probability 1 under all stationary
policies (this is equivalent to inevitable termination in an episodic environment), and ii) there is no
inherent notion of discounting that operates from t = 0 until the end of an episode. For example
diagrams of both szrat- and srst-models, refer to Fig 4.

The srst-modelling suits the fact that in practice, an RL-agent is expected to run in multiple episodes,
e.g. to play some game repeatedly. By using srst-modelling, we can train an agent operating on

16

(a) Episodic: GridNav-25

(b) Episodic: Taxi-15

Figure 5: Learning curves of Qtot-learning on szrat-model and Qb-learning on srst-model, evaluated
on two episodic environments. Qtot-learning maximizes the total reward criterion, whereas Qb-
learning maximizes the average-reward criterion. For experimental setup, see Sec 7.

episodic environments in the same practical way as if it were operating on continuing environments.
Similar to the zero state-value of the terminal state (i.e. vπ
γ (szrat) = 0) in discounted-reward
szrat-modelling, we may have a zero state-value of the resetting state (i.e. vπ
b (srst) = 0) in the
average-reward srst-modelling, where vb denotes the relative value of the bias (7).

We compare szrat- and srst-modelling by running experiments with two training schemes as follows.

• Scheme-A uses an szrat-model and the total reward criterion (hence, Qtot-learning).
• Scheme-B uses an srst-model and the average reward criterion (hence, Qb-learning).

Fig 5 depicts the learning curves of Qx-learning trained under Scheme-A and Scheme-B on two
episodic environments. Both schemes are trained with the same experiment-episode length16, and
gauged by the same performance metric, i.e. the ﬁnite-time average reward (19). As can be observed,
both schemes converge to the same value, but Scheme-B empirically leads to a higher rate of
convergence. This indicates that both szrat- and srst-models induce the same optimal policy (in
the original states s ∈ S), and that the szrat- to srst-model conversion is sound. More importantly,
conversion to an srst-model enables obtaining the optimal policy using an average-reward method
since such a model is recurrent, for which gain optimality is the most selective.

Mahadevan (1996a, Sec 3.1) mentioned the idea about episode repetitions in an episodic grid-
navigation environment. He however, did not provide a formal conversion. Pardo et al. (2018)
also used a similar conception to the episode repetition for proposing a technique that bootstraps
from the value of the state at the end of each experiment-episode for random-horizon episodic
environments. Another important related work is of White (2017) who introduced a uniﬁcation of
episodic and continuing environment speciﬁcations. Such a uniﬁcation is carried out via transition-
based discounting, where the discount factor from the terminal to initial states is set to 0. She noted
that the transition-based discounting breaks the Laurent-expansion-based connection between the
discounted- and average-reward criteria in (6).

6 Discussions

The route of using the discounted reward to approximately maximize the average reward in RL seems
to follow Blackwell’s γ-discounted approach (1962) to Howard’s average reward (1960) in DP.17 As
explained in Sec 2.1, the major successive development of such Blackwell’s approach came from
Veinott who introduced a family of n-discount optimality criteria (1969), which is discounting-free.

16During training on episodic environments, an agent is always transported back to the initial state after an
episode ends, regardless of the training scheme. This transportation to initial states is captured by srst-modelling,
but not by szrat-modelling (see Fig 4).

17Recall that both Blackwell’s γ-discounted and Howard’s average-reward approaches were aimed to deal

with the inﬁniteness of the total reward in inﬁnite-horizon MDPs, refer to Sec 2.1.

17

ΠS

Π∗

0≤γ<γ2

may be
empty

Π∗

γ1≤γ<γBw

ΠS

Π∗
−1 = Π∗
Bw
(if recurrent)

Π∗
0 = Π∗
Bw
(may be)

Π∗
1 = Π∗
Bw
(may be)

Π∗

... = Π∗
(may be)

Bw

Π∗

γ2≤γ<γ1

Π∗

γBw≤γ<1 := Π∗

Bw

Π∗

|S|−2 = Π∗

Bw

(a) γ-discounted optimality for γ ∈ [0, 1), whose
optimal policy sets are denoted as Π∗
γ. Theoretically,
there are a ﬁnite number of intervals. For simplicity
here, we assume that there are four intervals, namely:
[0 = γ3, γ2), [γ2, γ1), [γ1, γ0 = γBw), [γBw, γ−1 = 1).

(b) n-discount optimality for n = −1, 0, . . . , |S| − 2,
whose optimal policy sets are denoted by Π∗
n. In re-
current MDPs, Π∗
−1 = Π∗
Bw. When transient states
are present, such an equivalency to Π∗
Bw may not be
achieved till n = |S| − 2 at most in unichain MDPs.

Figure 6: Two Venn diagrams of optimal policy sets Π∗ of γ-discounted and n-discount optimality
criteria, applied to unichain MDPs. There is only one green circle that is present in both Venn
diagrams. It indicates a subset Π∗
Bw containing stationary policies that are guaranteed to be optimal
with respect to the most selective Blackwell criterion. Note that the sizes of circles in both diagrams
are not to scale. The left Venn diagram of γ-discounted optimality is invariant to MDP classiﬁcation
and is based on (Blackwell, 1962; Smallwood, 1966; Jiang et al., 2016), particularly we conjecture
that Π∗
γBw≤γ<1 is always disjoint with the other optimal policy subsets. The right Venn diagram of
n-discount optimality is based on (Puterman, 1994, Thm 10.1.5, 10.3.6 ; Mahadevan, 1996b, Fig 2).

Then, he developed an algorithm for obtaining n-discount optimal policies, from which policies that
are considered nearly-optimal and optimal by Blackwell emerge as special cases, namely (n = 0)-
and (n = ∞)-discount optimal policies, respectively. Both imply the average reward optimality.

Like in DP, the route in RL should be completed by devising methods based on Veinott optimality
criteria. To this end, average reward RL methods constitute the ﬁrst step in the following senses. First,
they yield a set of approximately (n = −1)-discount optimal policies, from which higher n-discount
optimal policies are sought, as illustrated in Fig 6b. This follows from the hierarchical property
of n-discount optimality, whose selectivity increases as n increases. Note that such a property is
exploited in the n-discount policy iteration in DP (Puterman, 1994, Ch 10.3). Second, average reward
RL methods evaluate the gain vg ≡ v−1 and the bias v0 that are likely to be useful for attaining
higher n-discount criteria. This is because the expansion coefﬁcients vn in (6) are interconnected.
For instance, the ﬁrst three coefﬁcients satisfy three nested equations below,

i) v−1 = P v−1,

ii) v0 = r − v−1 + P v0,

iii) v1 = −v0 + P v1,

(20)

where r is the reward vector, P is the one-step transition matrix, and v1 is the policy evaluation entity
for (n = 1)-discount optimality. Thus, advancement on gain and bias estimations is transferable to
higher n-discount methods towards obtaining Blackwell optimal policies.

Directly maximizing the average reward brings several beneﬁts, as described in Sec 5. It is also
evidently free from any γ-related difﬁculties (Sec 4). It is interesting now to discuss whether those
beneﬁts outweigh the loss of all the virtues of γ (Sec 3).

Approximating the average reward via discounting (Sec 3.1) is not without caveats. Taking γ really
close to 1 slows the convergence, as well as increases the variance of, for example, policy gradient
estimates. On the other hand, lowering γ poses the risk of suboptimality (with respect to the most
selective criterion). This trade-off can be potentially guided by the critical discount factors γBw
(Sec 3.2). Speciﬁcally in RL, we conjecture that it is not the exact value of γBw that is needed, but

18

some value around it because of the interplay among γ-dependent approximation layers, such as those
related to policy value and gradient estimations. Thus, ﬁne-tuning γ is always a crucial yet non-trivial
task, dealing with possibly non-concave and non-differentiable learning performance functions with
respect to γ (even though the domain of such a univariate function is limited to [0, 1)). Research on
this front has been going on with some progress (Zahavy et al., 2020; Paul et al., 2019; Xu et al.,
2018). In that regard, Veinott’s discounting-free criteria (including average-reward optimality) can be
viewed as alternatives towards achieving the most selective Blackwell optimality in RL.

There are several ways to compensate for the other merits of discounting (Sec 3.3), which are missed
due to directly maximizing the average reward. We describe some of them as follows.

The chain-classiﬁcation independence merit: The need for chain-type determination (or assump-
tion) can be thought of as a way to exploit the chain structural property to be able to apply more
simple average-reward methods. That is, for a constant gain across all states, we impose a unichain
assumption, which is relatively not restrictive in that it already generalizes the recurrent type. Never-
theless, the ultimate goal remains: an average-reward RL method that can handle non-constant gains
across states as in multichain MDPs. Because of its generality (hence, sophistication), it does not
require prior determination of the chain type.

The chain-type assumption can also be interpreted as an attempt to break down the difﬁculty in
attaining Blackwell-optimal policies in one go (regardless of the chain structure) as in the γ-discounted
optimality, see Fig 6a. Recall that in such a criterion, only (γ ≥ γBw)-discounted optimal policies are
Blackwell optimal (however the critical γBw is generally unknown, otherwise this one-go technique
would be favourable). On the other hand, the average reward ((n = −1)-discount) optimality is
already equivalent to the Blackwell optimality whenever the MDPs are recurrent. For more general
unichain MDPs, (n = 0)-discount optimality is equivalent to the Blackwell optimality but only for
some transition and reward conﬁguration (for other conﬁgurations, higher n-discount criteria should
be applied, as shown in Fig 6b).

The contractive merit: The relative value-iteration (VI) has been shown to alleviate the non-
contractive nature of basic VI in average rewards (Abounadi et al., 2001). There are also policy
gradient methods whose convergence generally follows that of stochastic gradient ascents. Their
policy evaluation part can be carried out via stochastic gradient descents. That is, by minimizing
some estimation loss derived for instance, from the average-reward Bellman evaluation equation,
which involves the substraction of gain (similar to that of the relative VI).

The variance reduction merit: The variance of trajectory-related estimation can be controlled by
directly varying (truncating) the experiment-episode length (as an alternative to varying the artiﬁcial
discount factor). One may also apply baseline substraction techniques for variance reduction. In
general however, lower variance induces higher bias-errors, leading to a trade-off.

We conclude that directly maximizing the average reward has a number of beneﬁts that make it
worthwhile to use and to investigate further. It is the root for approaching Blackwell optimality
through Veinott’s criteria, which are discounting-free (eliminating any complication due to artiﬁcial
discounting). Future works include examination about exploration strategies: to what extent strategies
developed for the discounted rewards applies to RL aiming at discounting-free criteria.

7 Experimental setup

In this Section, we describe the experimental setups used to produce Figs 2, 3, and 5. They are about
environments (Sec 7.1) and learning methods (Sec 7.2).

7.1 Environments

GridNav-n (episodic)
refers to an n-by-n grid-world with no obstacle. An agent has to navigate
from an initial state to the goal state. There are n2 − 1 states, excluding the goal state. Every state
has four available actions, i.e. moving in North, East, South and West compass directions. The
state-transition stochasticity is governed by an action-slip parameter. That is, the agent actually
moves according to the chosen (intended) directional action with a probability q. Otherwise, it stays
or moves in one of three other directions; hence there are four alternatives due to slipping, each is

19

with probability (1 − q)/4. If an action brings the agent beyond the grid-world, then the agent stays at
the current grid. There is a reward of +1 for transitioning to the goal; any other movement costs −1.

Taxi-n (episodic)
refers to the n-by-n grid Taxi environment, which is adopted from OpenAI Gym
(Brockman et al., 2016). The number of obstacles is increased accordingly with respect to n, whereas
the number of passengers and pickup/drop-off locations is kept the same.

The environment families 1, 2, and 3 are adopted from the access-control queueing task (Sutton
and Barto, 2018, p252), the Chain problem (Strens, 2000, Fig 1), and the torus MDP (Morimura et al.,
2010, Fig 2), respectively. In Fig 2b, the ﬁrst two were used as Families 1 and 2, whose numbers of
states are varied, whereas the third was used as Family 3, whose reward constant is varied.

7.2 Learning methods

The learning method used for experiments in Figs 2 and 5 is Q-learning. It is an iterative method
that relies on the Bellman optimality equation (BOE) to produce iterates approximating the optimal
action value (denoted by q∗). Different optimality criteria have different BOEs, inducing different
types of Q-learning as follows.
Discounted rewards (Qγ-learning): ˆq∗

Average rewards (Qb-learning): ˆq∗

Total rewards (Qtot-learning): ˆq∗

γ(st, at) ← (1 − αt)ˆq∗
b (st, at) ← (1 − αt)ˆq∗
tot(st, at) ← (1 − αt)ˆq∗

ˆq∗
γ(st+1, a)},

γ(st, at) + αt{rt+1 + γ max
a∈A
b (st, at) + αt{rt+1 − ˆv∗
g + max
a∈A
ˆq∗
tot(st+1, a)},

tot(st, at) + αt{rt+1 + max
a∈A

ˆq∗
b (st+1, a)},

with a positive learning rate αt (here, we used a ﬁne-tuned constant α). The estimate ˆq∗ is initialized
optimistically to large values to encourage exploration in the outset of learning. For Qb-learning,
we set the optimal gain estimate ˆv∗
b (sref , a) with a prescribed (arbitrary but ﬁxed)
reference state sref , following the relative-VI (RVI) technique by Abounadi et al. (2001, Sec 2.2).
Note that Qtot-learning converges as long as the total reward is ﬁnite (Schwartz, 1993, p2).

g ← maxa∈A ˆq∗

References

Abounadi, J., Bertsekas, D., and Borkar, V. S. (2001). Learning algorithms for Markov decision
processes with average cost. SIAM Journal on Control and Optimization, 40(3). (pages 19 and 20)

Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. (2019). On the theory of policy gradient

methods: Optimality, approximation, and distribution shift. arXiv: 1908.00261. (page 11)

Altman, E. (1999). Constrained Markov Decision Processes. Taylor & Francis. (page 14)

Bacon, P.-L. (2018). Temporal Representation Learning. PhD thesis, School of Computer Science,

McGill University. (page 14)

Baxter, J. and Bartlett, P. L. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial

Intelligence Research, 15(1). (pages 7, 10, and 12)

Beleznay, F., Grobler, T., and Szepesvari, C. (1999). Comparing value-function estimation algorithms

in undiscounted problems. Technical report. (page 11)

Bellman, R. (1957). Dynamic Programming. Princeton University Press. (page 3)

Blackwell, D. (1962). Discrete dynamic programming. The Annals of Mathematical Statistics, 33(2).

(pages 3, 4, and 18)

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.

(2016). OpenAI Gym. arXiv:1606.01540. (pages 15 and 20)

Castro, P. S. (2020). Scalable methods for computing state similarity in deterministic Markov decision

processes. Proceedings of the AAAI Conference on Artiﬁcial Intelligence. (page 10)

Chang, H. S., Hu, J., Fu, M. C., and Marcus, S. I. (2013). Simulation-Based Algorithms for Markov

Decision Processes. Springer, 2nd edition. (page 6)

20

Dabney, W. C. (2014). Adaptive step-sizes for reinforcement learning. PhD thesis, Computer Science

Department, University of Massachusetts Amherst. (page 15)

Dann, C., Neumann, G., and Peters, J. (2014). Policy evaluation with temporal differences: A survey

and comparison. Journal of Machine Learning Research, 15(24). (page 7)

Devraj, A. M. and Meyn, S. P. (2020). Q-learning with uniformly bounded variance: Large discounting

is not a barrier to fast learning. arXiv: 2002.10301. (page 11)

Dewanto, V., Dunn, G., Eshragh, A., Gallagher, M., and Roosta, F. (2020). Average-reward model-
free reinforcement learning: a systematic review and literature mapping. arXiv: 2010.08920.
(pages 1, 3, and 14)

Douc, R., Moulines, E., Priouret, P., and Soulier, P. (2018). Markov Chains. Springer Series in
Operations Research and Financial Engineering. Springer International Publishing. (page 4)

Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep
reinforcement learning for continuous control. In Proceedings of The 33rd International Conference
on Machine Learning. (page 1)

Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep Q-learning. In

Proceedings of the 2nd Conference on Learning for Dynamics and Control. (page 11)

Feinberg, E. A. and Shwartz, A. (2002). Handbook of Markov Decision Processes: Methods and

Applications, volume 40. Springer US. (pages 3 and 6)

Ferns, N., Castro, P. S., Precup, D., and Panangaden, P. (2006). Methods for computing state similarity
in Markov decision processes. In Conference in Uncertainty in Artiﬁcial Intelligence. (page 10)

Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Deep rein-
forcement learning that matters. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
(pages 1 and 15)

Hordijk, A., Dekker, R., and Kallenberg, L. C. M. (1985). Sensitivity analysis in discounted

Markovian decision problems. Operations Research Spektrum, 7. (pages 3 and 10)

Howard, R. A. (1960). Dynamic Programming and Markov Processes. Technology Press of the

Massachusetts Institute of Technology. (page 3)

Hutter, M. (2006). General discounting versus average reward. In Algorithmic Learning Theory.

Springer Berlin Heidelberg. (page 3)

Jiang, N., Singh, S., and Tewari, A. (2016). On structural properties of MDPs that bound loss due to
shallow planning. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence. (pages 12 and 18)

Jin, Y. and Sidford, A. (2021). Towards tight bounds on the sample complexity of average-reward
MDPs. In Proceedings of the 38th International Conference on Machine Learning. (page 7)

Kakade, S. (2001). Optimizing average reward using discounted rewards. In Proceedings of the 14th

Annual Conference on Computational Learning Theory. (page 10)

Kumar, H., Kalogerias, D. S., Pappas, G. J., and Ribeiro, A. (2020). Zeroth-order deterministic policy

gradient. Arxiv:2006.07314. (page 11)

Lattimore, T. and Hutter, M. (2014). General time consistent discounting. Theoretical Computer

Science, 519. (page 3)

Lattimore, T. and Szepesvári, C. (2020). Bandit Algorithms. Cambridge University Press. (page 2)

Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M. J., and Bowling, M.
(2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for
general agents. Journal of Artiﬁcial Intelligence Research. (pages 1 and 15)

21

Mahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case study
comparing R-learning and Q-learning. In Proceedings of the 11th International Conference on
Machine Learning. (page 1)

Mahadevan, S. (1996a). Average reward reinforcement learning: Foundations, algorithms, and

empirical results. Machine Learning. (pages 1, 10, and 17)

Mahadevan, S. (1996b). Sensitive discount optimality: Unifying discounted and average reward
reinforcement learning. In Proceedings of the 13th International Conference on Machine Learning.
(pages 1, 8, and 18)

Melo, F. S. and Ribeiro, M. I. (2007). Q-learning with linear function approximation. In Computa-

tional Learning Theory. (page 11)

Mischel, W., Ebbesen, E. B., and Zeiss, A. R. (1972). Cognitive and attentional mechanisms in delay

of gratiﬁcation. Journal of personality and social psychology, 21(2). (page 5)

Morimura, T., Uchibe, E., Yoshimoto, J., Peters, J., and Doya, K. (2010). Derivatives of logarithmic
stationary distributions for policy gradient reinforcement learning. Neural Computation, 22(2).
(pages 7 and 20)

Nota, C. and Thomas, P. S. (2020). Is the policy gradient a gradient? Proceedings of the 19th

International Conference on Autonomous Agents and MultiAgent Systems. (page 11)

Pardo, F., Tavakoli, A., Levdik, V., and Kormushev, P. (2018). Time limits in reinforcement learning.

In Proceedings of the 35th International Conference on Machine Learning. (page 17)

Paul, S., Kurin, V., and Whiteson, S. (2019). Fast efﬁcient hyperparameter tuning for policy gradient

methods. In Advances in Neural Information Processing Systems 32. (page 19)

Petrik, M. and Scherrer, B. (2008). Biasing approximate dynamic programming with a lower discount

factor. In Advances in Neural Information Processing Systems 21. (page 12)

Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons, Inc., 1st edition. (pages 4, 5, 6, 8, 10, and 18)

Samuelson, P. A. (1937). A note on measurement of utility. Review of Economic Studies, 4. (page 5)

Scherrer, B. (2010). Should one compute the Temporal Difference ﬁx point or minimize the Bellman
Residual? The uniﬁed oblique projection view. In Proceedings of the 27th International Conference
on Machine Learning. (page 10)

Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In
Proceedings of the 10th International Conference on Machine Learning. (pages 14, 15, and 20)

Singh, S. P., Jaakkola, T. S., and Jordan, M. I. (1994). Learning without state-estimation in partially
observable Markovian decision processes. In Proceedings of the 11th International Conference on
Machine Learning. (page 6)

Smallwood, R. D. (1966). Optimum policy regions for Markov processes with discounting. Opera-

tions Research, 14. (pages 3, 12, and 18)

Strens, M. J. A. (2000). A Bayesian framework for reinforcement learning. In Proceedings of the

27th International Conference on Machine Learning. (page 20)

Sutton, R. S. and Barto, A. G. (2018). Introduction to Reinforcement Learning. MIT Press. (pages 3,

6, 7, 10, 11, and 20)

Szepesvári, C. (2010). Algorithms for Reinforcement Learning. Morgan & Claypool. (page 9)

Tang, J. and Abbeel, P. (2010). On a connection between importance sampling and the likelihood

ratio policy gradient. In Advances in Neural Information Processing Systems 23. (page 10)

Thomas, P. (2014). Bias in natural actor-critic algorithms. In Proceedings of the 31st International

Conference on Machine Learning. (page 11)

22

Thrun, S. and Schwartz, A. (1993). Issues in using function approximation for reinforcement learning.

In Proceedings of Connectionist Models Summer School. (page 11)

Tsitsiklis, J. N. and Van Roy, B. (2002). On average versus discounted reward temporal-difference

learning. Machine Learning, 49. (page 2)

van Hasselt, H. P. (2011).

Insights in Reinforcement Learning: formal analysis and empirical

evaluation of temporal-difference learning algorithms. PhD thesis, Univ. Utrecht. (page 15)

Van Roy, B. (1998). Learning and Value Function Approximation in Complex Decision Processes.

PhD thesis, MIT. (page 14)

Van Seijen, H., Fatemi, M., and Tavakoli, A. (2019). Using a logarithmic mapping to enable lower
discount factors in reinforcement learning. In Advances in Neural Information Processing Systems
32. (page 15)

Veinott, A. F. (1969). Discrete Dynamic Programming with Sensitive Discount Optimality Criteria.

The Annals of Mathematical Statistics, 40(5). (pages 1 and 3)

White, M. (2017). Unifying task speciﬁcation in reinforcement learning. In Proceedings of the 34th

International Conference on Machine Learning. (page 17)

Xu, Z., van Hasselt, H. P., and Silver, D. (2018). Meta-gradient reinforcement learning. In Advances

in Neural Information Processing Systems 31. (page 19)

Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H. P., Silver, D., and Singh, S. (2020).
A self-tuning actor-critic algorithm. In Advances in Neural Information Processing Systems 33.
(page 19)

23

