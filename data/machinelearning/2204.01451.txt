2
2
0
2

g
u
A
9
1

]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[

2
v
1
5
4
1
0
.
4
0
2
2
:
v
i
X
r
a

Atomistic Global Optimization X: A Python package for optimization of
atomistic structures

Mads-Peter V. Christiansen,1 Nikolaj Rønne,1 and Bjørk Hammer1, a)
Center for Interstellar Catalysis, Department of Physics and Astronomy, Aarhus University, DK-8000 Aarhus,
Denmark

Modelling and understanding properties of materials from ﬁrst principles require knowledge of the underlying
atomistic structure. This entails knowing the individual chemical identity and position of all atoms involved.
Obtaining such information for macro-molecules, nano-particles, clusters, and for the surface, interface, and
bulk phases of amorphous and solid materials represents a diﬃcult high-dimensional global optimization
problem. The rise of machine learning techniques in materials science has, however, led to many compelling
developments that may speed up structure searches. The complexity of such new methods has prompted a
need for an eﬃcient way of assembling them into global optimization algorithms that can be experimented
with. In this paper, we introduce the Atomistic Global Optimization X (AGOX) framework and code, as
a customizable approach that enables eﬃcient building and testing of global optimization algorithms. A
modular way of expressing global optimization algorithms is described and modern programming practices
are used to enable that modularity in the freely available AGOX python package. A number of examples
of global optimization approaches are implemented and analyzed. This ranges from random search and
basin-hopping to machine learning aided approaches with on-the-ﬂy learnt surrogate energy landscapes. The
methods are show-cased on problems ranging from supported clusters over surface reconstructions to large
carbon clusters and metal-nitride clusters incorporated into graphene sheets.

I.

INTRODUCTION

Global optimization is a prerequisite for the computa-
tional treatment of materials at the atomic level. The
application of quantum mechanics to predict the proper-
ties of materials requires knowledge of the positions of all
the atoms that make up the material. The positions that
occur in reality will often be very close to those that min-
imize the total quantum mechanical energy, whereby the
importance of global optimization for atomistic structure
is evident. The problem may be visualized as ﬁnding the
lowest valley in a high-dimensional landscape, the poten-
tial energy surface (PES). This surface may be described
at diﬀerent levels of theory, from crude distance-based
pair-potentials to sophisticated calculations treating the
particles at the quantum level. With increased accuracy
comes increased computational cost, therefore global op-
timization methods that eﬃciently search the PES for
the lowest energy minima are necessary. Computational
approaches that utilize global optimization have received
signiﬁcant attention and led to impressive results.1–5

A multitude of optimization algorithms have been
proposed from simulated annealing6
through basin-
hopping7 and minima-hopping8 methods to evolution-
ary algorithms9–14 to ab-initio-random structure search15
and particle swarm algorithms16,17 to mention a few of
the most successful methods. In recent years, machine
learning has become a central topic in computational ma-
terials science. A prominent example of which is the ever
increasing accuracy of machine learned potential energy
surfaces, colloquially referred to simply as machine learn-

a)Electronic mail: hammer@phys.au.dk

ing potentials.18–33 Machine learning has also led to sub-
stantial improvements for simulation tasks in computa-
tional material science, such as machine learning poten-
tial based molecular-dynamics simulations for investigat-
ing properties of materials,34–41 optimization algorithms
exploiting machine learning potentials42–62 or machine
learning methods for guided exploration of the PES.63–74

With the speed of advancement in the materials sci-
ence and machine learning communities it is essential
that software tools are available that allow quick experi-
mentation. This is especially true for global optimization
(GO), an open-ended subject with room for new and im-
proved algorithms.
In GO, the aim is to identify the
optimum solution for a complex target function. Being
goal-driven, any GO method that does so is legitimate if
the goal is eventually reached, and hence experimenta-
tion with the computational strategy is welcomed. This
contrasts other tasks in materials science, such as solving
the Kohn-Sham equations and calculating forces in den-
sity functional theory calculations, or propagating atomic
positions in molecular dynamics simulations, where the
methods are governed by well-established deﬁning equa-
tions, and where eﬃcient algorithms and codes have been
developed.

In this work, we introduce the Atomistic Global Op-
timization X (AGOX) framework and python package.
AGOX is modular and ﬂexible such that many popular
global optimization algorithms can be formulated in the
framework and realized in the code. The package builds
on the atomistic simulation environment75 (ASE) and
can thus be used with a multitude of electronic struc-
ture codes, with a similar focus on eﬀortless scripting
as ASE enables. An overview of the framework is pre-
sented, and the goals of the code are discussed in detail.
We present applications of the code on several systems,

 
 
 
 
 
 
2

FIG. 1: Overview of several popular global optimization algorithms. Random structure search (RSS) represents the
simplest method. In each iteration of RSS, a structural candidate is generated and locally optimized. In
basin-hopping (BH), the current position in the search-space is kept track of and updated using the Metropolis
criterion. The position is then used by the generation mechanism. In the evolutionary algorithm (EA), a population
of candidates is maintained of and serves as input to the generation of a new candidate in each iteration which is
then optimized. In GOFEE, a number of candidates are generated and are locally optimized in a computationally
inexpensive surrogate potential before deciding on which candidate to evaluate in the true potential using a lower
conﬁdence bound acquisition function. The orange boxes will for many problems involve computationally expensive
potentials such as DFT.

starting with a cluster on a metal surface described by a
cheap potential that is solved using four diﬀerent global
optimization algorithms. A second application is used
to highlight the ability of AGOX to surgically make al-
gorithmic changes for a tin-oxide surface system, where
basin-hopping and a machine learning enhanced basin-
hopping algorithm are compared. The third example
is used to discuss parallelization options, where parallel
tempering76 is used to solve a two-dimensional carbon
cluster show casing how it is possible to take advantage
of computational resources to reduce waiting time for re-
sults. The fourth and ﬁnal example documents consider-
ations taken when solving a computationally demanding
problem, in this case a metal-nitride cluster embedded
in a graphene sheet where also spin polarization must be
taken into account. The AGOX code is freely available
on gitlab https://gitlab.com/agox/agox.

II. METHOD

A. AGOX Framework

A large number of global optimization methods have
been proposed, some of which diﬀer only slightly while
others diﬀer signiﬁcantly, however, all of them involve
two essential steps. The ﬁrst step is the generation of
a candidate structure and the second is the evaluation
In
of the generated candidate in the target potential.
Figure 1 examples of global optimization methods hav-
ing these two steps are given. The ﬁrst example given
is random structure search (RSS), that consists of just

candidate generation and local optimization. The next
examples are basin-hopping (BH) and evolutionary al-
gorithms (EA) that both use previous structures as the
starting point for the generation, i.e. a perturbation of
the atomic positions or as a crossover mutation that com-
bines two or more structures. They therefore need to
keep track of which previous structures are used in this
way. The ﬁnal example is the global optimization with
ﬁrst-principles energy expressions (GOFEE) method,57
in whih the expensive local optimization in the target
potential is replaced by local optimization in an on-the-
ﬂy trained surrogate potential. Due to the much reduced
cost, several candidates may be optimized per iteration
and a small number of them may be selected for evalua-
tion in the target potential.

AGOX is a framework that allows all of these algo-
rithms, and more, to be used within a single codebase.
This is achieved by deﬁning a number of modules that
can be used to build this wide range of optimization al-
gorithms. At present, we have identiﬁed the need for two
data-type modules and eight action-type modules. The
data-type modules enable the handling of the candidates
and are as follows:

The environment module handles the simulation
cell, any already present atoms (which we call a template)
and the number and species of the atoms the search algo-
rithm should place. This is the fundamental module that
deﬁnes the properties of the global optimization problem.
The candidate module manages all the informa-
tion about the structural candidates, e.g. position of the
atoms by inheriting functionality from the ASE Atoms
object, and by what means the candidate originated.

Update w.MetropolisGeneratecandidateIterationBHOptimizecandidateUpdate populationGenerate NcandidatesOptimize NcandidatesIterationEAGOFEEDraw sampleGenerate NcandidatesSurrogateoptimizeLCB Prioritize N candidatesEvaluate M candidatesIteration GeneratecandidateOptimizecandidateRSSIteration3

FIG. 2: Pictorial illustration some of the action-type AGOX modules. generators generate candidate structures,
a random generator places atoms at random with the only requirement that bond lengths are within are not too
short or too long, whereas a rattle generator perturbs a previously evaluated structure. A collector may be
used to collect several generated candidates. postprocessors may be used to improve generated candidates
regardless of which generator they originate from, such as local optimization in a model or moving to the center of a
cell. An acquisitor can be used to select the most promising candidate. evaluators calculate key properties
of candidates, e.g. the energy and the evaluated structure is stored in a database .

The action-type modules perform actions based on
the structural information in objects from the data-type
modules and do in some cases update one or several can-
didate objects. The eight action-type objects are:

The database module stores the proposed solutions,
that obey the conditions deﬁned by the environment.
These may be candidates whose properties have not yet
been calculated in the target potential, or structures for
which such properties are indeed available. The database
represents all the knowledge gathered about the target
potential that can be analyzed at the end of the search
to determine the global minimum energy structure. Fur-
thermore, several other modules can leverage the infor-
mation stored in the database during the search, e.g. to
build machine learning potentials, to maintain a popula-
tion, or to extract a sample.

The model module builds machine learning poten-
tials, such as the gaussian process regression model em-
ployed by GOFEE, based on the structures stored in the
database.

The sampler module provides one or more struc-
tures to be used by other modules of the search algo-
rithm to further exploitation of structurally unique areas
of search space, or to serve as input in the formation of
new candidates causing further exploration of non-visited
regions of search space. Depending on the implementa-
tion, the module can do so based on all previously studied
structures (or even proposed candidates) or it can main-
tain a population of selected structures, Figure 3 depicts

three diﬀerent ways that a sampler may function.
In
basin-hopping, the sampler would use the metropolis ac-
ceptance criterion to decide whether or not to update the
sample according to the most recently evaluated candi-
date. In an evolutionary algorithm, the sampler would
maintain a population of candidates that are structurally
diverse which can be used as parents for subsequent can-
didates. In general, the sampler can be dependent on the
order in which the data is obtained or it can be a func-
tion of the gathered data, as is the case for the K-means
sampling technique77 we employ for GOFEE searches in
this paper.

The generator module produces new candidates ei-
ther by manipulating structures taken from the sampler
module or through stochastic process that somehow pro-
poses a set of coordinates, e.g. by placing the atoms ran-
domly in the simulation cell. Generators can be biased
in a number of ways that may improve the performance
of the search for a set of problems.

The collector module manages the generation of
In general, more than one candidate
new candidates.
may be generated at a time and the collector module
deﬁnes how many candidates are generated using each
type of generator, e.g. a preset number of each, a prob-
ability for each - or even as a function of the number of
iterations.

The postprocessor module performs actions on the
generated candidates. This could be local optimization
of the candidate in a machine learning model potential.

Random Generator××××××44{},××××New candidateEnvironmentRandom insertionsRattle Generator××××××××××××Sample memberNew candidateRandom perturbationsModel Optimization Postprocessor××××Local optimization××××××××OptimizationEvaluator××××Local optimization××××××××E××××Acquisitor××××××××××××3××××2××××Prioritize1×××× Collector××××××××GeneratorsRattle Generator××××××××××××RandomGenerator××××××××DatabaseEvaluated candidateStorage××××EnergySingle-point Evaluator××××E4

FIG. 3: The starting point of candidate generation can
be decided by a sampler . This decision can taken in
a number of ways, such as based on the collected data
as depicted in (a). It may also depend only on the most
recently evaluated structure as depicted in (b). A
population-based sampling scheme is depicted in (c).
Here, parent and child structures are put under
selection pressure where only the ﬁttest structures are
allowed to survive to the next generation. The
modularity of AGOX makes experimentation using
diﬀerent sampling strategies easy.

The postprocessor may also discard a candidate if it con-
sists of multiple unbonded fragments or if it has some
unfavourable atomic arrangements, e.g. an expected too
short bond, an unphysical local coordination, or a too
high local energy according to a machine learning model.
The acquisitor module decides which candidates
are evaluated in the target potential. This concept
is based on Bayesian approaches, such as BOSS or
GOFEE,53,57 but it is implicitly a part of all global op-
timization algorithms as the choice of which candidates
to evaluate in the target potential is central to the global
optimization task. Some popular global optimization al-
gorithms, such as BH or RSS, implicitly use an acquisitor
that accepts all generated candidates. GOFEE on the
other hand leverages Bayesian statistics to intelligently
decide which candidate out of a collection of candidates
is most likely to further progress the search. So, while
the term acquisitor is derived from Bayesian methods,
it is in fact something all global optimization methods
apply.

The evaluator module evaluates the property of
interest for the next candidate. This may be the single
candidate produced in a RSS or BH search or the most
promissing candidate in a GOFEE search. In this work,
the total energy in the target potential is the property
of interest, but evaluators for other properties can be
added such that these properties may be optimized for.
Two types of evaluators are used in this work, one that

FIG. 4: (a) Flow diagram of a random-structure search
algorithm shown in terms of the involved action-type
AGOX modules. (b)(c)(d) Illustration of the actions
within the modules.

performs a local geometry optimization used with the
RSS, BH and EA algorithms and one that just performs a
single-point calculation used with GOFEE. Another pos-
sibility is to do a limited number, Ns, of relaxation steps
in the target potential. In GOFEE it has been suggested
to use Ns = 1 meaning that a total of two single-point
DFT calculations are performed, an approach dubbed
”dual-point evaluation”.57

In Figure 4 we show the modules involved in a random-
structure search. Figure 4(a) depicts the ﬂow diagram for
the three action-type modules while Figure 4(b)(c)(d)
details it further with the pictorial illustrations of the
modules.

The framework does not require that the modules are
used in any speciﬁc order or that all of them are used.
This makes the framework very ﬂexible. It is enabled by
an observer-pattern, which is a software design technique
that will be discussed further in the next section along
with the goals of the code.

B. Code

The modules described in section II A are abstractions
that we believe are useful when thinking about and de-
scribing atomistic global search algorithms. In order to
use them in practice they need to be translated into code.
It is, however, worth discussing brieﬂy what makes a
codebase useful. Our focus has been on three goals

• Ease of development.

• Ease of use.

• Performance (where necessary).

××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××ClusterstructuresExtract sampleIf acceptedStochasticcriteriaP(Ei,Es)EiEs(b) Iteration based(c) Population based(a) Database basedSelection Pressure Fittest surviveSurvivalthresholdParentsChildrenStructureFitnessDatabaseEvaluated candidateStorage××××EOptimizationEvaluator××××Local optimization××××××××E××××Random Generator××××××44{},××××New candidateEnvironmentRandom insertions(a)(b)(c)(d)Optimization EvaluatorHere, ease of development covers everything from testing
new ideas to fully implementing new algorithms. Ease
of use enables new users to eﬃciently utilize the code
even though they may not be experts in the ﬁeld. Fi-
nally, any code needs to be performant, however whereas
the previous points largely go hand in hand, performance
can come at the cost of increased complexity. For many
atomistic search problems, the most time-consuming step
will be the quantum mechanical calculation of the total
energy in the target potential, e.g. density functional the-
ory (DFT). In order to leverage the many eﬃcient elec-
tronic structure codes available, AGOX uses the atom-
istic simulation environment package75 (ASE) which has
Python interfaces to a large collection of codes. Com-
pared to the time of a DFT calculation, the generation
of a candidate structure is very fast, even without ex-
tremely optimized code. Therefore, for such modules it
is more important that the code is easy to understand
and fast to develop further. Even though local optimiza-
tion in a machine learning potential is many orders of
magnitude faster than in DFT, the computational time
spent in optimizing many candidates may become com-
parable to that spent in a single-point DFT calculation.
Therefore, it is worth some additional complexity to en-
sure that such optimizations are done eﬃciently.

In order to ensure both ease of use and ease of de-
velopment, we take advantage of the objective-oriented
programming (OOP) capabilities of the Python language.
An abstract base class (ABC) has been implemented for
each of the modules of the framework. These are essen-
tially code templates that deﬁne the required methods
and attributes of an actual speciﬁc implementation of any
of the modules. The ABC may also deﬁne a number of
functions that are generally convenient for that particu-
lar module, such as checking whether a set of coordinates
have chemically valid bond lengths. This improves both
the readability of the code by hiding the details behind
a method-call when those details are not necessary and
the reliability by only having one common implementa-
tion for each such method. The OOP design also allows
us to leverage inheritance when developing new features
in two ways. The ﬁrst is as mentioned when a new ver-
sion of one of the modules of the framework inherits from
the ABC and the second when an implementation inher-
its from another speciﬁc implementation. Inheritance is
depicted in Figure 5.

To make the framework as ﬂexible as possible, we do
not wish to impose any restrictions on how the modules
are used to design a search algorithm. This means, that it
has to be possible to leave out a module, change the order
of modules and use multiple versions of the same module.
This is accomplished by building an observer-pattern into
the core iterative loop.78 An observer-pattern is a soft-
ware design technique that entails a subject maintaining
a list of its observers that it will notify when changing
its state.
In AGOX, this state is the iteration counter
and observers, which are AGOX action-type modules,
will be notiﬁed in a pre-speciﬁed order in each iteration.

5

FIG. 5: Illustration of inheritance from an abstract base
class shown on the left. The ABC deﬁnes two abstract
methods and a single default method, a speciﬁc version
of the class (middle) requires real implementations of
the two abstract methods. Experiments with the
functionality of this speciﬁc version can be performed
by inheriting from the it and surgically replacing only
the relevant method.

By changing the list of observers one may alter what the
program actually does.
Importantly, this can be done
without altering the code of any of the modules or the
core iterative loop. Each module acts as an observer that
can read and write to a shared data collection, such that
there is no hardcoded order of execution. A valid AGOX
program can range from a for-loop that does nothing each
iteration to a GOFEE search and beyond.

Examples of such observer-pattern based algorithms
are depicted in Figure 6. The RSS method, shown in
Figure 6(a), has three observer-type modules attached
to the iteration loop. Similarly, the BH method may be
implemented with four observer type modules attached
to the iteration loop, as shown in Figure 6(b). Since
the observer-pattern is also built into the database
module, an alternative implementation of the BH meth-
ods can be layed out as shown in Figure 6(c). Here,
the sampler module is moved from being an observer
on the main iteration loop to being an observer on
the database module.
In this implementation, the
sampler module is invoked whenever new DFT-level
data has been dealt with by the database module.
The observer pattern on the database module is also
exploited in the implementation of the GOFEE method
presented in Figure 6(d). Here, it is the update of the
model that is invoked whenever the database mod-
ule has handled new DFT level data. To allow decoupling
of modules, such that e.g. a postprocessor does not
expect input from a generator or any other instance
of hard coded interdependence, the communication be-
tween modules is handled by a shared cache or through
the database, as depicted in Figure 7.

A useful way of thinking about an AGOX program is
to consider it a set of functions that are executed in an
order speciﬁed in a script, rather than by the underly-
ing core code and without any predeﬁned relations be-

AbstractClassDefaultmethod1Abstractmethod2Abstractmethod1ClassDefaultmethod1Method2Method1Exp.ClassDefaultmethod1Method2Exp.Method16

FIG. 6: Several global optimization algorithms
programmed in diﬀerent ways. The observer-pattern
loop allows diﬀerent algorithm to be expressed using
the same modules. The numbers note the order of
execution, which the observer-pattern takes into
account. A module may also be an observer to the
database and be notiﬁed whenever new data is added.
This is depicted for the sampler module of
basin-hopping and the model training function in
GOFEE.

tween the set of functions. AGOX is designed to work
based on deﬁnition and assembly of such functions via
deﬁnition of modules at the scripting level since it pro-
vides an easy and logical way of handling algorithms de-
ﬁcient of certain actions. Imagine, by the contrary, that
connections between the modules were indeed hardcoded
to e.g. match the GOFEE layout presented in Figure 1,
then the RSS algorithm could still be formulated hav-
ing versions of certain modules that would do essentially
nothing, e.g. the acquisitor, the sampler and the post-
processors. With the observer-pattern these modules can
be left out entirely. Another advantage granted by the
observer-pattern is that new modules may be included in
the future, again because the order of execution and the
communication between modules is not predeﬁned.

FIG. 7: Depiction of the program ﬂow and data access
of an AGOX algorithm. Here, an abstract algorithm is
depicted as a series of observers, each of which have the
ability to get or set data in the shared cache.
Additionally, modules can have access to the database.
At the end of an iteration the shared cache is cleared,
whereas data saved to the database is saved
permanently. Observers may also communicate with
each other through connections deﬁned in the AGOX
python script, whenever modules depend on other
modules to perform their function. For instance, a
generator may be given access to the sampler in order
to retrieve sample members.

C. Code example: An acquisitor

As an example of the beneﬁts of the described coding
scheme, we will take a look at an acquisitor. The basic
acquisitor used for GOFEE type searches is the lower
conﬁdence bound (LCB) acquisitor, see Section X A for
details. The implementation of LCB inherits from the
acquisitor ABC, in the way depicted in Figure 5. We can
easily experiment with alternative acquisition functions,
and depending on the type, it can either inherit from
the base-class or a speciﬁc implementation, e.g. the LCB
acquisitor. As an example, we can imagine we would like
to try an acquisition function deﬁned by

xa = argmin

x∈X

[E(x) − κσ(x)ρ] ,

(1)

where X is a set of candidate coordinates, E(x) and σ(x)
are surrogate energy and uncertainty functions and κ and
ρ are chosen parameters. For ρ = 1 this reduces to the
LCB expression. This can be implemented in very little
code by inheriting from the LCB class.

(a): Observer random structure searchRandom GeneratorDatabase123Observer-loopIteration(b): Observer basin hoppingObserver-loopIterationDatabaseMetropolis Sampler134Rattle Generator2Optimization Evaluator(c): Alt. observer basin hoppingOptimization EvaluatorDatabaseMetropolis Sampler123Rattle GeneratorObserver-loopIteration1*(d): Observer GOFEEObserver-loopIterationCollectorGeneratorsModel Relaxation PostprocessorLCB AcquisitorDatabaseKMeans Sampler213456Model Training1*Single-point EvaluatorOptimization EvaluatorShared cacheObserver 1Observer 2Observer 3Observer ...Observer N1 class P o w e r L o w e r C o n f i d e n c e B o u n d A c q u i s i t o r (
L o w e r C o n f i d e n c e B o u n d A c q u i s i t o r ) :

2
3 name = ’ P o w e r L o w e r C o n f i n d e n c e B o u n d A c q u i s i t o r ’
4
5 def __init__ ( self , rho =1 , * args , ** kwargs ) :
super () . __init__ (* args , ** kwargs )
6
self . rho = rho

7

8
9 def a c q u i s i t i o n _ f u n c t i o n ( self , E , sigma ) :
10

return E - self . kappa * sigma ** self . rho

11
12 def acq u i s iti on_ for ce ( self , E , F , sigma ,

13

sigma_force ) :
return F - self . kappa * self . rho * sigma **(
self . rho -1) * sigma_force

Thus, in less than 15 lines of code, experiments can be
made with a diﬀerent acquisition function without any
risk of causing issues with existing code due to the use of
inheritance.

D. Success curves

One key metric to judge the performance of any global
optimization algorithm is a success curve, which is a
statistical property that measures the percentage of in-
dependent search runs or searches that are ’successful’
against the number of single-point calculations,
itera-
tions or timing metrics. In the used terminology, a search
refers to the execution of an algorithm typically for a pre-
determined number of iterations - although other stop-
ping criteria are also possible. A search produces a num-
ber of structures, e.g. a random-structure search of 1000
iterations will produce 1000, not necessarily unique, local
minimum energy structures. However, given the stochas-
tic nature of most global optimization algorithms there
is no guarantee that a search ﬁnds the global minimum
structure. Whether or not a single search ﬁnds the global
minimum also tells us very little about the ability of the
algorithm to solve the problem, again due to the stochas-
tic nature, a second search may lead to diﬀerent results.
To compare searches done with diﬀerent algorithms, or
the same algorithm with diﬀerent parameters, we there-
fore need a statistical measure that averages over several
independent searches, which is exactly what a success
curve does. Since the underlying physics is typically not
only governed by the global minimum energy structure,
but by a collection of low-energy structures, we count
structures within some small energy of the best structure
found among all searches to be successful in this work.
For applications where ﬁnding a speciﬁc structure is con-
sidered the success criterion, a graph-based method for
identifying that speciﬁc structure can be employed, see
Sec. X C. With either success criterion, success curves are
informative about the performance of the used algorithm.
As such, they can be used as a measure of conﬁdence in
the found solution, with high success being required in
order to be conﬁdent in the solution and the ability of the
method to solve more diﬃcult problems. The process of

obtaining a success curve is illustrated in Figure 8.

7

FIG. 8: (a) The energy per single-point calculation for
two diﬀerent searches, the dashed black-line indicates
the success criterion. The two-colored dots show when
the two searches ﬁnd a candidate that is considered
successful for the ﬁrst time. In (b) the number of
single-point calculations until ﬁrst success is presented
as a histogram, green and orange bars indicate the
contributions from the searches in (a). A success curve
is obtained by integrating the histogram (with a bin
size of 1) and normalizing according to the number of
searches, as shown in (c). With more searches the curve
becomes smoother and the uncertainty decreases as
evidenced in the diﬀerence between the gray curve
based on 20 searches and the blue based on 100 searches
in (c).

III. APPLICATION: GLOBAL OPTIMIZATION
ALGORITHMS

As an example of an application of the AGOX frame-
work, we study a system consisting of a platinum clus-
ter on a gold surface described by the simple eﬀective
medium theory (EMT) potential79 as implemented in
ASE. This potential is chosen as it works out of the box
having installed just ASE and AGOX. We present the
results of the application of the AGOX framework using

0246810Energy[eV](a)0123Count[#](b)0100200300400500Single-pointcalculations[#]020406080100Success[%](c)20searches100searchesused in the four methods were set up in the following
way and with the following choices for the adjustable pa-
rameters:

8

Random-structure search

Generator : One candidate is generated per itera-

tion using the random generator.

Evaluator : The generated candidate is relaxed in
the EMT potential until the forces on all Pt atoms are
below 0.05 eV/˚A, template atoms are ﬁxed, and Pt atoms
are not allowed to leave the conﬁnement cell depicted in
Figure 9.

Basin-hopping

Sampler : A new structure is accepted or rejected
using the Metropolis criterion with probability of accep-
tance given by

A = min

1, exp [β(Ek−1 − Ek)]

(2)

(cid:111)
,

(cid:110)

With β = 1/kBT with kBT = 1 eV and Ek is the energy
of the structure found in iteration k.
If accepted, the
structure replaces the previously accepted structure as
the starting point of the rattle generator in the next
iteration. Skipped in the ﬁrst iteration.

Generator : Initialized with a random generator,
and all subsequent iterations generate one candidate per
iteration using a rattle generator applied to the latest
structure accepted by the sampler.

Evaluator : Same as for RSS.

Evolutionary Algorithm

Sampler : A population of 10 structures with
diversity enforced using ﬁngerprint feature and parent
selection using the algorithm proposed by Vilhelmsen
and Hammer.13

Generators : Initialized with 10 random generator
candidates and with subsequent iterations using 10
candidates generated by rattling structures from the
population.

Evaluator : Same as RSS for all 10 candidates

generated per iteration.

FIG. 9: Gold surface slab used as template for the
Pt14/Au(100) search. The computational super cell and
conﬁnement cell are depicted in the xy- and xz-planes
in black and red, respectively.

four diﬀerent search algorithms, namely RSS, BH, EA
and GOFEE.

In order for an optimization algorithm to be eﬃcient
at solving any particular problem it must impose search
biases that favour ﬁnding the global optimum solution
for that particular problem. We distinguish between two
kinds of biases. Those that are imposed from the outset
and those that are learned by the algorithm. A some-
what trivial example of an imposed bias is the number of
atoms, having decided that only 14 Pt atoms are present,
the search space is limited to solutions that involve 14 Pt
atoms. A less trivial imposed bias, is to constrain the
physical space that the algorithm is allowed to use, this
is useful as it limits the number of symmetry related so-
lutions, e.g. translations of the cluster along the surface.
Learned biases arise from how the algorithm uses the
data it gathers during a search. The surrogate potential
will impose a learned bias and so will the sampling tech-
nique used to decide which previous structures are used
to generate new candidates.

In AGOX, the search can be conﬁned to a cell which
may diﬀer from the periodic/computational cell used,
such that both the generators and the relaxation post-
processing will not result in any geometries that have
atoms outside of the speciﬁed cell. For the Pt14/Au(100)
system this is essential to ﬁnd physical solutions where
Pt atoms are only present on one side of the surface slab.
This cell is depicted along with the surface slab in Figure
9.

The system-dependent choices, that is the decisions

that deﬁne the search problem, can be summarized as

• The number of and species of the atoms that are
directly involved in the search. (The 14 Pt atoms)

• The template, that is the number and positions of
atoms already present in the cell. (The gold surface
and cell depicted in Figure 9)

• The conﬁnements, if any.

(The conﬁnement cell

depicted in red in Figure 9)

The speciﬁc choices made here are listed in parenthe-
sis. Each global optimization algorithm has an additional
number of options. The action-type observer modules

xyxzGOFEE

Sampler : We use the K-means clustering based
sampling method reported by Merte et al.77 with a
total sample size of 10 structures and an energy re-
quirement such that considered structures are within
25 eV of the best structure discovered so far. The
method works similarly to the depictions in Figure
3(a) by applying K-means clustering to all evaluated
candidates in feature space and selecting structures from
each cluster enforcing diversity of the selected structures.

Generators : We use a random generator, a rattle
generator and a generation mechanism described by
Palecio et. al.56 that favours perturbing atoms far from
the center of geometry of the cluster, which we call
center-of-geometry generator. A total of 30 candidates
are generated per iteration with 10 candidates from the
random generator, 15 from the rattle generator, and ﬁve
from the center-of-geometry generator.

Postprocessing : All candidates are relaxed in
the LCB expression evaluated using the surrogate
GPR model until the maximum LCB force (κ = 2) of
non-template atoms is below 0.2 eV/˚A. This is done in
parallel utilizing all available CPU cores. The relaxation
is constrained such that atoms are kept within the
conﬁnement cell.

Acquisitor : Among the 30 candidates the one
with the lowest lower conﬁdence bound (κ = 2) value is
chosen.

Evaluator : One single-point calculation is done

for the candidate picked by the acquisitor.

Model : Details of the GPR model are given in

Sections X A and X B.

Results

Four diﬀerent success curves are presented in Fig-
ure 10 that originate from sets of AGOX searches us-
ing the four diﬀerent search algorithms, RSS, BH, EA
and GOFEE. The least biased algorithm, random struc-
ture search, explores the search space in the least di-
rected way which means it will not become stuck but
at an increased computational cost. Basin-hopping, that
reuses previous structures in the generation mechanism,
is directed towards exploring low-energy regions which
results in higher success with fewer single-point calcula-
tions. The EA is able to evolve a number of structures,
making it more resistant towards getting stuck in a lo-
cal minimum and enabling slightly better performance
compared to the BH in agreement with similar studies.80
Note that the EA could also use a crossover generation

9

mechanisms that would involve combining two or more
members of its population, but that is not taken advan-
tage of here. Finally, the GOFEE algorithm replaces lo-
cal optimization in the true potential (EMT) with local
optimization in a surrogate GPR model and only does
single-point calculations for favorable structures chosen
by the LCB acquisition function. This results in orders of
magnitude fewer single-point calculations while reaching
much higher success. It should be noted that the param-
eters of each algorithm have not been optimized, as that
requires a very large computational eﬀort as a single suc-
cess curve requires at least tens of searches, likely more to
resolve subtle diﬀerences for fairly unresponsive param-
eters. Regardless, the objective here is to showcase that
all of the algorithms can be handled within the AGOX
framework. Figure 10 does show that the GOFEE algo-
rithm heavily outperforms the other three algorithms in
terms of single-point calculations, without the prospect
of the others improving enough with optimal parameters
to compete with it.

This is not surprising given that GOFEE only performs
one single-point calculation per iteration while relying on
the surrogate model to get locally optimized structures,
which is an algorithmic change rather than due to spe-
ciﬁc parameter choices. As an EMT potential is used
here, it is in fact more computationally demanding to
run GOFEE for this problem, as the surrogate potential
is not faster to evaluate than the potential, but GOFEE
needs to query the potential much fewer times which is
the key beneﬁt for problems with a more computation-
ally demanding potential.

FIG. 10: Success curves for four diﬀerent global
optimization algorithms.

In Figure 11 we report the 15 structures with the low-
est energy for the Pt14/Au(100) system. These have been
extracted by ﬁrst obtaining all structures found that are
within 0.5 eV of the best structure. Among these, those
that have unique graph spectra, according to the method
described in Sec. X C are identiﬁed and locally opti-
mized, after which the graph spectra are compared once
again. It is apparent, that the methods are capable of

010000200003000040000Single-pointcalculations[#]0102030405060708090Success[%]GOFEERSSBHEAﬁnding many distinct low-energy structures and that the
graph-based sorting criterion enables the distinction of
the diﬀerent low-energy structures.

10

FIG. 12: Basin-hopping search algorithm with a model
inserted.

the ability of the algorithm to solve any given problem
that would constitute a more computational eﬃcient
algorithm.

In AGOX such algorithmic changes can be made eas-
ily and surgically requiring only changes to the script and
not to the core of the code. These speciﬁc changes in-
volve inserting a surrogate model local optimization step
between the steps performing candidate generation and
candidate optimization as for BH in Figure 1 or in the ter-
minology of AGOX a postprocessor in between the rattle-
generator and local optimization evaluator observers of
Figure 6(a). The resulting ﬂowchart of the algorithm is
shown in Figure 12. The changes compared to a standard
basin-hopping search script amount to, deﬁning a model

model = ModelGPR . default ( environment ,
database )

and giving that model to a postprocessor

relaxer = R elax Pos tp roces s (
model = model , start_relax =10 ,
optimizer = BFGS , o p t i m i z e r _ r u n _ k w a r g s =

{ ’ fmax ’ :0.05 , ’ steps ’ :100} ,

constraints = environment . g et_co ns traints () )

and giving those additional modules to the AGOX class

agox = AGOX ( database , generator , sampler ,
evaluator , relaxer , wrapper )

1

1

2

3

4

5

1

scripts

The
are
//gitlab.com/agox/agox_data.

available

in

full

at

https:

FIG. 11: Low-energy structures of Pt14 on Au(100),
with the energies relative to the structure with the
lowest energy. The structures have been locally
optimized after the search such that the maximum force
is 0.01 eV/˚A.

IV. APPLICATION: MACHINE LEARNING-ENHANCED
BASIN-HOPPING

As an example of the ability of AGOX to facilitate
the design of global optimization algorithms, we present
a comparison of a basin-hopping algorithm and a
machine learning-enhanced basin-hopping algorithm.
As discussed previously, the most expensive part of a
basin-hopping type search is the local optimization,
often involving many dozens of electronic structure
calculations to obtain the forces on each atom. If some
of these calculations can be omitted while retaining

We applied the ML-assisted basin-hopping algorithm
to optimizing a rutile SnO2(110)-(4×1) system where 6
Sn and 6 O atoms are arranged on the ﬁxed surface
described at the DFT level using an LCAO basis set81
and the PBE exchange-correlation functional82 as imple-
mented in GPAW.83,84 The Γ-point is used for sampling
the Brillouin zone and for Sn a 4-valence electron PAW
setup is used. The system is shown in Figure 13. The
settings of the modules were as follows:

E = 0.0 eVE = 0.017 eVE = 0.025 eVE = 0.16 eVE = 0.18 eVE = 0.19 eVE = 0.22 eVE = 0.23 eVE = 0.24 eVE = 0.24 eVE = 0.25 eVE = 0.26 eVE = 0.26 eVE = 0.26 eVE = 0.26 eVDatabaseMetropolis Sampler14Model Training1*5Rattle Generator2Local Optimization EvaluatorModel Relaxation Postprocessor3Observer-loopIteration11

FIG. 13: Left: Template comprising a one-layer rutile
SnO2(110)-(4×1) slab. Right: Global minimum energy
structure for Sn6O6 on SnO2(110)-(4×1). Sn atoms are
shown in gray, oxygen atoms are shown in red. The
super cell is drawn as a dashed black rectangle. The
conﬁnement in the xy-plane is indicated as a red
colored rectangle.

ML-enhanced basin-hopping

Sampler : Same as for BH in Section III.

Generator : Same as for BH in Section III.

Postprocessor : Invoking the BFGS optimizer in
ASE using the model calculator for up to 100 episodes
or until the forces of non-template atoms are below 0.05
eV/˚A.

Evaluator : Same as for RSS in Section III, except
that only a limited number of relaxation steps were done
(Ns = 3 or Ns = 10).

Model : Same as for GOFEE in Section III.

In Figure 14, we show success curves for solving this
problem with and without locally optimizing in the model
prior to doing DFT optimization. For both methods
searches, are done with a diﬀerent maximum allowed
number of DFT gradient steps, as that is an important
parameter for the ability of standard basin-hopping to
solve a problem.
In Figure 14(a) we see that using a
model much fewer DFT steps are required, which is the
same trend observed for the previous system. As the
DFT calculation time is the dominating factor a similar
trend is observed when plotted against CPU time in Fig-
ure 14(b), however the curves using a model are slightly
less steep because of the scaling of training and using
the model. Finally, in Figure 14(c) the success is plotted
against the number of basin-hopping iterations, present-
ing a remarkable result that even though the model is
learning the potential on-the-ﬂy, there is no diﬀerence in
the number of iterations required to solve the problem.
Developments such as the one presented here are enabled
by AGOX as algorithms can be altered entirely through
a script.

FIG. 14: Success measured against (a) single-point
DFT calculations, (b) CPU time and (c) number of
basin-hopping iterations. The colored numbers indicate
the maximum number of DFT gradient steps allowed in
each basin-hopping iteration. The CPU time is derived
as 24 times the wall time passed, since the runs were
performed on 24 CPU cores in order to speed up the
DFT calculations that parallelize well. In (c) the
iteration axis has been limited to 1000, the standard
basin-hopping searches performing 10 and 20
relaxations steps have run for a total of 2700 and 1400
iterations, respectively.

V. APPLICATION: PARALLEL TEMPERING

An important consideration when building software
tools is the ability to take advantage of the ever-
increasing number of processor cores in modern comput-
ers, be it a desktop computer with a handful of cores or
a high-performance computing server with many thou-
sands.

Within a single search run AGOX takes advantage of
ASE to run electronic-structure calculations in parallel
on the number of processes allotted. Furthermore, in the
GOFEE setting, where several candidates are produced
per iteration, model relaxations are also parallelized such

0500010000150002000025000Single-pointcalculations[#]020406080100Success[%]310102050200NomodelModel(a)05001000150020002500300035004000CPUtime[hour]020406080100Success[%]310102050200NomodelModel(b)02004006008001000Iteration[#]020406080100Success[%]310102050200NomodelModel(c)that the same number of cores are utilized as for elec-
tronic structure calculations.

In general, it is also an advantage to perform several
instances of the same search with the same settings in
order to have several independent search runs of the al-
gorithm, in order to more thoroughly explore the search-
space. Due to the independence of each search, with no
communication to any other searches, this is an embar-
rassingly parallel task, and parallelization is as simple as
running the same script on several computers - or us-
ing a workload manager, such as Slurm commonly avail-
able on HPC facilities. This is what is done to make
success-curves that require a large number of indepen-
dent searches as discussed previously, and what we rec-
ommend doing when applying AGOX to solve a search
problem.

AGOX can, however, also be used to build algorithms
that beneﬁt from having several workers. An example
of such an algorithm is parallel tempering.76 In parallel
tempering, basin-hopping searches are performed simul-
taneously at diﬀerent temperatures. Structures accepted
at diﬀerent temperatures may be swapped to avoid stag-
nation by promoting exploration at high temperatures
and exploitation at low temperatures.
In this setting,
a search consists of a number of workers using diﬀer-
ent processors with each worker running a basin-hopping
search at one temperature and all workers sharing a sin-
gle database .

Structures are swapped between workers with adjacent

temperatures every Nt episodes with probability

12

FIG. 15: Success curves for parallel tempering with
Nw = {1, 2, 4, 8} as a function of (a) CPU time and (b)
wall time. In both cases it is the time per independent
search. The inset in (a) shows the global minimum
structure.

Evaluator : Local optimization until forces on all

(cid:110)

(cid:111)

P = min

1, exp (βi − βj)(Ei − Ej)

(3)

atoms are below 0.2 eV/˚A.

where βi = 1/kBTi. Temperatures are chosen according
to kBTi = kBT0·(3/2)i where i ranges from zero to the to-
tal number of workers Nw minus one in integer steps and
kBT0 = 0.05 eV. Every Nt iterations each worker waits
for all other workers of the same search to reach that iter-
ation before reading the database from disk, this is done
prior to swapping structures to ensure all workers are
synchronized. Compared to a standard basin-hopping
search this type of parallel tempering search requires only
changing the database module and the sampling module.

Synchronized parallel tempering

Sampler : Parallel tempering using the Metropolis
criterion of Eq. (2) to decide whether or not to accept a
candidate and attempting swaps between candidates at
adjacent temperatures every Nt = 10 episode according
to Eq. (3).

Generator : Same as for BH in Section III.

Postprocessor : Generated structure is moved to

the center of the cell.

Database :

Structures are synchronized among

workers every Nt iteration.

As an example, we apply this algorithm to a 24-
atom carbon cluster constrained to two-dimensions in the
search described by the semi-empirical extended tight-
binding method (GFN2-xTB).85,86 To fairly compare
searches with diﬀerent number of workers each search is
run for 2000/Nw iterations, such that the same number
of candidates are generated and locally optimized. Suc-
cess curves for this system are shown in Figure 15 both
as a function of CPU time and wall time in (a) and (b)
respectively. The parallel-tempering scheme helps allevi-
ate stagnation, as evidenced by searches with more work-
ers reaching higher success rates for the same amount of
CPU time. The overhead introduced by workers having
to wait for each other to synchronize the database does
make it more expensive in CPU time to do searches with
more workers. However, as each worker only has to do
a fraction of the work, the wall time, that is the waiting
time between starting search runs and achieving results,
is decreased signiﬁcantly.

This example show-cases one example of how AGOX
modules may be used to parallelize algorithms. The
shared database module can also be used to parallelize

050100150200CPUtime[min]020406080100Success[%]Nw=1Nw=2Nw=4Nw=8(a)020406080Walltime[min]020406080100Success[%]Nw=1Nw=2Nw=4Nw=8(b)other algorithms implemented in AGOX due to the mod-
ularity of the framework. That could for example be ran-
dom structure search runs, which parallelize trivially, or
the model-enhanced basin-hopping example of the pre-
vious section, where the model for each search may be
updated based on the collected data from all searches.

VI. APPLICATION: EMBEDDED METAL CLUSTER

FIG. 16: Template used for Ru3N4C4 with the
conﬁnement cell shown in red.

Finally, we have applied AGOX to a system that con-
sists of three Ru, four Ni and four carbon atoms that
are embedded in a hole in a graphene sheet. This sys-
tem is inspired by a combined experimental and theoret-
ical study of the properties of such graphene embedded
Ru3N4 clusters.87 Here, we only employ the GOFEE al-
gorithm as described in Section III, except the 30 can-
didates per iteration are generated only with a random
generator and a rattle generator, producing 10 and 20
candidates, respectively. The choices of parameters and
analysis procedure are not speciﬁc to GOFEE and can be
used with any global optimization algorithm. Only the
three Ru, four Ni and four carbon atoms are allowed to
move during the search. To accurately rank the most sta-
ble structures, we apply a procedure where all atoms are
involved in a local optimization after the global optimiza-
tion algorithm has ﬁnished, this procedure is described
in detail below.

The template and conﬁnement used for the search is
illustrated in Figure 16, the periodic cell is 16 ˚A in the
z-direction. For this system the search is performed at
the DFT level using GPAW83,84 with a plane wave basis
set using an energy cutoﬀ of 300 eV and only a single k-
point with the Perdew-Burke-Ernzerhof (PBE) exchange-
correlation functional.82 Further analysis of the found
structures is performed with a plane wave energy cutoﬀ
of 400 eV and a (3, 3, 1) Monkhorst-Pack k-point grid
again with the PBE functional, but now also including
the eﬀects of spin polarization. Henceforth, we call the
ﬁrst set of settings the ’rough’ settings and the latter set
the ’ﬁne’ settings.

For studying the physical properties of a system using
DFT, it is important that the computational settings are
chosen at a suﬃcient level of precision in order for the

13

relevant properties to converge. This is typically done by
performing convergence checks were each setting is var-
ied until the property of interest is converged within a
speciﬁed tolerance. For global optimization, the target
property is the geometry of low-energy structures, but
running the entire search with multiple sets of settings is
computationally costly so that cannot generally be done.
Furthermore, it is often the case that the geometries con-
verge before their total energies do. It is thus suﬃcient
to perform the search at relatively rough settings with
the beneﬁt of decreasing the computational cost. To en-
sure that correct results are obtained, a number of the
most stable solutions can then be investigated using more
accurate settings.

The procedure we employ for this Ru3N4C4 search

problem is

• Run the GOFEE search for a number of indepen-

dent searches with rough settings.

• Identify the most stable geometries.

• Select those with unique graphs.

• Locally optimize these, without constraining the
template atoms, with the rough settings followed
by the ﬁne settings.

• Select again those with unique graphs.

• Obtain total energies at higher level of theory, in-

cluding spin polarization.

In the steps involving local relaxations with either rough
or ﬁne settings after the GOFEE search, all atoms are
included, i.e. both template atoms and the atoms placed
in the search.

In the present work, we have performed 25 indepen-
dent searches for 1000 iterations, resulting in 25000
structures among which 8685 are within 2 eV of the
most stable structure found during the search. These
8685 structures share 120 unique graphs, when using
the spectral graph technique described in Section X C.
The best structures of each graph is then relaxed in the
target potential. Before performing the spin-polarized
calculations the graph comparison method was employed
again, now with the 120 locally optimized structures,
resulting in a ﬁnal total of 28 unique structures. Spin
polarized calculations were performed with the total
magnetic moment ﬁxed at 0, 1, 2 and 3 and the initial
magnetic moments distributed evenly among the Ru
atoms for each of these 28 structures.

The 10 most stable structures found by employing this
procedure are depicted in Figure 17. For the search, the
most stable structure found is structure two, however af-
ter local optimization with the same settings as used for
the search, without ﬁxing the graphene sheet, structure
three becomes the most stable. Structures four and ﬁve

xyxz14

FIG. 17: Most stable structures found for search of Ru3N4 in a graphene sheet. The bar plot shows the energy
relative to the most stable structure at that step of the procedure. The most stable structure identiﬁed by the
search is structure two, but at the higher level of accuracy structure one has a lower energy. Note that because
structure 0 lowers its energy more by the inclusion of spin than the other structures, the relative energy of the other
structures including spin is higher than when spin is not included even if their energy also decrease. The total
magnetic moment M that leads to the lowest total energy is reported along with each structure.

both decrease their total energy by over 1 eV as a re-
sult of the local optimization, going from being uncom-
petitive structures to being possible candidates for the
global minimum. This can be attributed to local opti-
mization of the graphene sheet being particularly favor-
able for these two structures. With more accurate DFT
settings structure 1 becomes the most stable structure
and the inclusion of spin further decreases its energy rel-
ative to the other structures. Structure one, two, three
and ﬁve all show the triangular arrangement of the Ru
atoms that is expected from the experimental results pre-
sented by Shufang et. al.87 We note that in their work,
a structural model several eV above the ones shown in
Figure 17 was proposed. This highlights the need for ef-
ﬁcient and easy to use global optimization algorithms, as
even with experimental evidence as a guide, guessing the
global minimum structure for such complex systems is
practically impossible.

This analysis shows that some degree of reordering of
the stability hierarchy of structures must be accounted

for when using less accurate settings for the potential em-
ployed in the search compared to the ﬁnal desired level of
accuracy. This begs the question of why not performing
the search at the desired level of accuracy? An answer
to that question is shown in Figure 18 which shows that
at least twice the computational time budget would be
required to do so. The post analysis procedure of local
optimization and spin-polarized calculations is relatively
cheap as it is only done for the structures identiﬁed by
the spectral clustering technique.

VII. CONCLUSION

Global optimization is an essential part of the com-
putational treatment of materials. However,
its suc-
cessful application requires choosing an appropriate al-
gorithm according to the diﬃculty of the problem and
the computational demand of the chosen potential. Fur-
thermore, computational modeling of materials in gen-

0.00.51.01.52.0Energy[eV]EsearchEroughEﬁneEspin1:M=12:M=03:M=14:M=15:M=16:M=07:M=18:M=19:M=110:M=115

the Danish National Research Foundation through the
Center of Excellence “InterCat” (Grant agreement no:
DNRF150).

IX. DATA AVAILABILITY

Version 1.1.0 of the code is publically available at
https://gitlab.com/agox/agox under a GNU GPLv3
license. Documentation available at https://agox.
gitlab.io/agox. Data supporting the ﬁndings pre-
sented in this paper available at https://gitlab.com/
agox/agox_data.

X. METHODS

A. Gaussian Process Regression

When employing a machined learned model in AGOX
we follow Ref. 57 and use a gaussian process regression
(GPR) model. With a GPR model, the energy prediction
for a structure with feature x∗ is made as:

E(x∗) = K(x∗, X)[K(X, X) + σ2

nI]−1(E − µ(X)) + µ(x∗)

where µ is the prior, and E are the energies of training
structures that are described by their feature represen-
tations X that are compared by the kernel K for which
we use a double Gaussian, as in Bisbo et. al.,57 where an
element is calculated as

K(x, x∗) = θ0

(cid:20)
(1 − β) exp

(cid:18)

−

(cid:18)

+β exp

−

(cid:19)

(cid:19)(cid:21)

(x − x∗)2
2l2
1
(x − x∗)2
2l2
2

where the length-scales l1 and l2 are chosen such that
l1 > l2 with β = 0.01. The GPR model also allows the
calculation of the model uncertainty for a query structure

σ(x∗) = K(x∗, x∗)−K(X, x∗)T [K(X, X)+σ2

nI]−1K(X, x∗),

with σn being a noise parameter.

Since the GPR model can estimate the uncertainty, it
allows for the use of the lower-conﬁdence-bound (LCB)
acquisition function. This is done in GOFEE where can-
didates are relaxed in the LCB and where the next struc-
ture to be evaluated in the target potential is chosen ac-
cording to

xa = argmin

x∈X

[E(x) − κσ(x)]

(4)

where X is a set of candidate coordinates and κ is a pa-
rameter decided upon prior to starting the search. This
procedure is depicted in 19, here three candidate coor-
dinates are generated and locally optimized in the LCB
expression with the most promising one, that is the one
with the lowest LCB value, picked for evaluation.

FIG. 18: Breakdown of CPU for the searches performed
for Ru3N4C4-graphene system. The top bar shows the
time for the search performed with fast DFT settings,
whereas the bottom bar reports the time if it had been
performed with the more accurate DFT settings. A
total of 28 × 4 = 112 spin-polarized calculations were
performed at roughly 3 CPU hours per calculations,
compared to the roughly 10 and 40 CPU minutes
required for at the low and high unpolarized DFT
settings. The hatched area is an estimate based on a
single search run with the ﬁne DFT settings.

eral is a rapidly developing ﬁeld, largely due to the ad-
vent of machine learning techniques, it is therefore neces-
sary for software tools to enable these developments. We
have introduced the Atomistic Global Optimization X
framework and accompanying Python code for the global
optimization of atomistic structures that leverage mod-
ern programming principles and state of the art machine
learning techniques to eﬃciently solve these tasks. The
code is ﬂexible and allows for the rapid development and
testing of global optimization algorithms. The applica-
tion of the package to four examples of global optimiza-
tion tasks has been documented, one using a simple ef-
fective medium theory potential that allows for repro-
duction of the results with a fairly small computational
budget and without installation of additional software.
The second example documents how AGOX allows sur-
gically changing algorithms to reduce the computational
demand. We also present an application using parallel-
tempering that shows how AGOX can take advantage of
computational resources through parallelization. Finally,
an example show-cases the use of AGOX for a real-world
atomistic optimization problem.

VIII. ACKNOWLEDGEMENTS

This work has been supported by VILLUM FONDEN
through Investigator grant, project no. 16562, and by

0500010000150002000025000CPUtime[h]GenerationOtherDFTRelaxroughRelaxﬁneSpinpolarized16

by search algorithms, in order to ﬁlter out the unique
structures found. We do this by building the adjacency
matrix of each structure

Aij =






0
1
0

if i = j
if Dij < 1.3 dcov(ti, tj)
else

(8)

with ti being the atomic number of atom i and where Dij
is the distance between atoms i and j, and dcov(ti, tj) is
the sum of covalent radii for atoms of type ti and tj.
From this a Laplacian matrix may be constructed as

(cid:88)

Lij = (

Aij)δij − Aij

j

(9)

where δij is the Kronecker delta. Finally the eigenval-
ues of the Laplacian λ = λ1, λ2, .., λn may be computed
and two structures may be compared by checking that
their eigenvalue spectra are equal. Because this proce-
dure is based on the adjacency matrix small variations in
bonds lengths do not change the eigenvalues (unless that
change causes a bond to break according to Eq.
(8)),
which is a helpful property when using this feature to
ﬁnd unique structures. While the eigenspectrum of dis-
tinct graphs can be equal, this is unlikely to be the case
in practice.89,90 We use this feature to ﬁnd distinct struc-
tures by grouping together all those structures that have
equal eigen spectra. Whereas with a more usual con-
tinuous atomistic feature, such as the ﬁngerprint feature
used for the GPR model, a distance threshold param-
eter must be chosen. The ﬁngerprint feature describes
minute changes in the structure that do not correspond
to conﬁgurational diﬀerences, whereas the graph eigen
spectrum for structures with variations in bond lengths
is equivalent and therefore a distance threshold is not
necessary.

XI. REFERENCES

1J. Greeley, T. F. Jaramillo, J. Bonde, I. Chorkendorﬀ, and J. K.
Nørskov, Nat. Mater. 5, 909 (2006).
2C. J. Pickard and R. J. Needs, Phys. Rev. Lett. 97, 045504
(2006).
3Z. A. Piazza, H.-S. Hu, W.-L. Li, Y.-F. Zhao, J. Li, and L.-S.
Wang, Nat. Commun. 5, 3113 (2014).
4A. Jain, Y. Shin, and K. A. Persson, Nat. Rev Mater. 1, 15004
(2016).
5A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Nat. Rev.
Mater. 4, 331 (2019).
6S. Kirkpatrick, C. D. Gelatt, and C. D. Vecchi, Science 220, 671
(1983).
7D. J. Wales and J. P. K. Doye, J. Chem. Phys. A 101, 5111
(1997).
8S. Goedecker, J. Chem. Phys 120, 9911 (2004).
9D. M. Deaven and K. M. Ho, Phys. Rev. Lett. 75, 288 (1995).
10R. L. Johnston, Dalton T. 22, 4193 (2003).
11A. R. Oganov and C. W. Glass, J. Chem. Phys. 124, 244704

(2006).

12S. Q. Wu, M. Ji, C. Z. Wang, M. C. Nguyen, X. Zhao, K. Umem-
oto, R. M. Wentzcovitch, and K. M. Ho, J. Phys. Condens. Mat.
26, 035402 (2013).

FIG. 19: Example of GPR model and LCB sampling.
Orange is the real function from which the orange
training points have been gathered, this leads to the
blue GPR model where the shaded area represents the
uncertainty of the model. By sampling a number of
points and relaxing in the LCB surface the next point
to evaluate and add to training data may be chosen

B. Fingerprint feature

The representation we employ for the GPR model is
the Oganov-Valle ﬁngerprint feature,88 where the radial
components between species A and B are given by

FAB(R) ∝




(cid:88)

ij

0

1
R2
ij

(cid:18)

exp

−

(cid:19)

(R − Rij)2
2σ2

, R < RR,

, R ≥ RR,

(5)
where RR (6 ˚A) is a hard cut-oﬀ and σ (0.2 ˚A) is a
smearing parameter. A feature vector is constructed by
sampling the feature at intervals of ∆ (0.2 ˚A), for mul-
tiple species the vectors can are appended together. We
also employ angular components, given by

FABC(θ) ∝

fc(rij)fc(rik) exp

−

(cid:18)

(cid:19)

(θ − θijk)2
2l2
σ

(6)

(cid:88)

ijk

where lσ (0.2 rad) is a smearing parameter and fc(r) is
a cut-oﬀ function that ensures that the feature smoothly
goes to zero at Rθ (4 ˚A) as controlled by the parameter
γ (2), in particular it is

fc(r) = 1 + γ(

r
Rθ

)γ+1 − (γ + 1)(

r
Rθ

)γ.

(7)

The values given in parenthesis next to each parameter
is the value used in this work.

C. Spectral clustering of atomistic structures

Graphs are a natural way of describing atomistic struc-
tures, which ball-stick depictions of molecules illustrates
clearly. For our purposes graphs are particularly useful
in order to analyse the vast amount of data generated

FeatureEnergy13L. B. Vilhelmsen and B. Hammer, J. Chem. Phys. 141, 044711

(2014).

14J. Roberts, J. R. S. Bursten, and C. Risko, Chem. Mater. 33,

6589 (2021).

15C. J. Pickard and R. J. Needs, J. Phys. Condens. Matter 23,

053201 (2011).

16Y. Wang, J. Lv, L. Zhu, and Y. Ma, Phys. Rev. B 82, 094116

(2010).

17J. Lv, Y. Wang, L. Zhu, and Y. Ma, J. Chem. Phys 137, 084104

(2012).

18J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401 (2007).
19A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi, Phys.

Rev. Lett. 104, 136403 (2010).

20J. Behler, J. Chem. Phys. 134, 074106 (2011).
21M. Rupp, A. Tkatchenko, K.-R. M¨uller, and O. A. von Lilienfeld,

Phys. Rev. Lett. 108, 058301 (2012).

22A. P. Bart´ok, R. Kondor, and G. Cs´anyi, Phys. Rev. B 87,

184115 (2013).

23K. Hansen, F. Biegler, R. Ramakrishnan, W. Pronobis, O. A.
von Lilienfeld, K.-R. M¨uller, and A. Tkatchenko, J. Phys. Chem
6, 2326 (2015).

24F. A. Faber, A. S. Christensen, B. Huang, and O. A. von Lilien-

feld, J. Chem. Phys. 148, 241717 (2018).

25J. S. Smith, O. Isayev, and A. E. Roitberg, Chem. Sci. 8, 3192

(2017).

26V. L. Deringer and G. Cs´anyi, Phys. Rev. B 95, 094203 (2017).
27K. T. Sch¨utt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko,

and K.-R. M¨uller, J. Chem. Phys. 148, 241722 (2018).

28N. Lubbers, J. S. Smith, and K. Barros, J. Chem. Phys. 148,

241715 (2018).

29V. L. Deringer, N. Bernstein, A. P. Bart´ok, M. J. Cliﬀe, R. N.
Kerber, L. E. Marbella, C. P. Grey, S. R. Elliott, and G. Cs´anyi,
J. Phys. Chem. Lett. 9, 2879 (2018).

30L. Li, H. Li, I. D. Seymour, L. Koziol, and G. Henkelman, J.

Chem. Phys 152, 224102 (2020).

31V. Zaverkin, D. Holzm¨uller, I. Steinwart, and J. K¨astner, J.

Chem. Theory Comput 17, 6658 (2021).

32J. Timmermann, Y. Lee, C. G. Staacke, J. T. Margraf,
C. Scheurer, and K. Reuter, J. Chem. Phys 155, 244107 (2021).
33J. Xu, X.-M. Cao, and P. Hu, J. Chem. Theory Comput 17,

4465 (2021).

34Z. Li, J. R. Kermode, and A. De Vita, Phys. Rev. Lett. 114,

17

47A. Denzel and J. K¨astner, J. Chem. Phys. 148, 094114 (2018).
48E. G. del R´ıo, J. J. Mortensen, and K. W. Jacobsen, Phys. Rev.

B. 100, 104103 (2019).

49G. Schmitz and O. Christiansen, J. Chem. Phys. 148, 241704

(2018).

50Q. Tong, L. Xue, J. Lv, Y. Wang, and Y. Ma, Faraday Discuss.

211, 31 (2018).

51E. L. Kolsbjerg, A. A. Peterson, and B. Hammer, Phys. Rev. B.

97, 195424 (2018).

52T. L. Jacobsen, M. S. Jørgensen, and B. Hammer, Phys. Rev.

Lett. 120, 026102 (2018).

53M. Todorovi´c, M. U. Gutmann, J. Corander, and P. Rinke, Npj

Comput. Mater. 5, 35 (2019).

54P. C. Jennings, S. Lysgaard, J. S. Hummelshøj, T. Vegge, and

T. Bligaard, Npj Comput. Mater. 5, 46 (2019).

55E. V. Podryabinkin, E. V. Tikhonov, A. V. Shapeev, and A. R.

Oganov, Phys. Rev. B 99, 064114 (2019).

56M. L. Palecio and J. Behler, J. Chem. Phys 153, 054704 (2020).
57M. K. Bisbo and B. Hammer, Phys. Rev. Lett. 124, 086102

(2020).

58S. Kaappa, E. G. del R´ıo, and K. W. Jacobsen, Phys. Rev. B

103, 174114 (2021).

59M. Arrigoni and G. K. H. Madsen, npj Computational Materials

7, 71 (2021).

60Y. Yang, O. A. Jim´enez-Negr´on, and J. R. Kitchin, J. Chem.

Phys 154, 234704 (2021).

61V. Sumaria and P. Sautet, Chem. Sci. 12, 15543 (2021).
62E. Musa, F. Doherty, and B. R. Goldsmith, Curr. Opin. Chem.

Eng. 35, 100771 (2022).

63M. S. Jørgensen, M. N. Groves, and B. Hammer, J. Chem 13,

1486 (2017).

64S. A. Meldgaard, E. L. Kolsbjerg, and B. Hammer, J. Chem.

Phys 149, 134104 (2018).

65M. S. Jørgensen, U. F. Larsen, K. W. Jacobsen, and B. Hammer,

J. Chem. Phys. A 122, 1504 (2018).

66K. H. Sørensen, M. S. Jørgensen, A. Bruix, and B. Hammer, J.

Chem. Phys. 148, 241734 (2018).

67M. S. Jørgensen, H. L. Mortensen, S. A. Meldgaard, E. L. Kolsb-
jerg, T. L. Jacobsen, K. H. Sørensen, and B. Hammer, J. Chem.
Phys 151, 054111 (2019).

68C. J. Pickard, Phys. Rev. B 99, 054102 (2019).
69S. Chiriki, M.-P. Christiansen, and B. Hammer, Phys. Rev B.

096405 (2015).

35M. Gastegger, J. Behler, and P. Marquetand, Chem. Sci. 8, 6924

100, 235436 (2019).

70Z. Zhou, S. Kearnes, L. Li, R. N. Zare, and P. Riley, Sci. Rep 9,

(2017).

36V. L. Deringer, M. A. Caro, R. Jana, A. Aarva, S. R. Elliott,
T. Laurila, G. Cs´anyi, and L. Pastewka, Chem. Mater. 30, 7438
(2018).

37V. L. Deringer, N. Bernstein, A. P. Bart´ok, M. J. Cliﬀe, R. N.
Kerber, L. E. Marbella, C. P. Grey, S. R. Elliott, and G. Cs´anyi,
J. Phys. Chem. Lett. 9, 2879 (2018).

38R. Jinnouchi, J. Lahnsteiner, F. Karsai, G. Kresse, and M. Bok-

dam, Phys. Rev. Lett. 122, 225701 (2019).

39F. No´e, A. Tkatchenko, K.-R. M¨uller, and C. Clementi, Annu.

Rev. Phys. Chem. 71, 361 (2020).

40J. S. Lim, J. Vandermause, M. A. van Spronsen, A. Musaelian,
Y. Xie, L. Sun, C. R. O’Connor, T. Egle, N. Molinari, J. Flo-
rian, K. Duanmu, R. J. Madix, P. Sautet, C. M. Friend, and
B. Kozinsky, J. Am. Chem. Soc. 142, 15907 (2020).

41L. B¨oselt, M. Th¨urlemann, and S. Riniker, J. Chem. Theory

Comput 17, 2641 (2021).

42R. Ouyang, Y. Xie, and D. Jiang, Nanoscale 7, 14817 (2015).
43T. K. Patra, V. Meenakshiundaram, J.-H. Hung, and D. S. Sim-

mons, ACS Comb. Sci. 19, 96 (2017).

44H. Zhai and A. Alexandrova, J. Chem. Theory Comput. 12, 6213

(2016).

45S. Jindal, S. Chiriki, and S. S. Bulusu, J. Chem. Phys. 146,

204301 (2017).

46V. L. Deringer, D. M. Proserpio, G. Cs´anyi, and C. J. Pickard,

Faraday Discuss. 211, 45 (2018).

10752 (2019).

71S. A. Meldgaard, H. L. Mortensen, M. S. Jørgensen, and M. S.

Hammer, J. Condens. Matter Phys. 32, 404005 (2020).

72G. N. C. Simm, R. Pinsler, and J. M. Hern´andez-Lobato, “Rein-
forcement Learning for Molecular Design Guided by Quantum
Mechanics,”
(2020), in International Conference on Machine
Learning, 2020, arXiv:2002.07717.

73G. N. C. Simm, R. Pinsler, G. Cs´anyi, and J. M. Hern´andez-
Lobato, “Symmetry-Aware Actor-Critic for 3D Molecular De-
sign,” (2020), in International Conference on Learning Repre-
sentations 2021, arXiv:2011.12747.

74S. Kaappa, C. Larsen, and K. W. Jacobsen, Phys. Rev. Lett.

127, 166001 (2021).

75A. H. Larsen, J. J. Mortensen, J. Blomqvist, I. E. Castelli,
R. Christensen, M. Du(cid:32)lak, J. Friis, M. N. Groves, B. Ham-
mer, C. Hargus, E. D. Hermes, P. C. Jennings, P. B. Jensen,
J. Kermode, J. R. Kitchin, E. L. Kolsbjerg, J. Kubal, K. Kaas-
bjerg, S. Lysgaard, J. B. Maronsson, T. Maxson, T. Olsen,
L. Pastewka, A. Peterson, C. Rostgaard, J. Schiøtz, O. Sch¨utt,
M. Strange, K. S. Thygesen, T. Vegge, L. Vilhelmsen, M. Wal-
ter, Z. Zeng, and K. W. Jacobsen, J. Condens. Matter Phys. 29,
273002 (2017).

76D. A. Kofke, J. Chem. Phys. 117, 6911 (2002).
77L. R. Merte, M. K. Bisbo, I. Sokolovi´c, M. Setv´ın, B. Hagman,
M. Shipilin, M. Schmid, U. Diebold, E. Lundgren, and B. Ham-
mer, Angew. Chem., Int. Ed. Engl. 61, e202204244 (2022).

78E. Gamma, R. Helm, J. Vlissides, and R. Johnson, Design Pat-
terns: Elements of Reusable Object-Oriented Software (Addison-
Wesley Professional, 1994).

79K. W. Jacobsen, J. K. Norskov, and M. J. Puska, Phys. Rev. B

35, 7423 (1987).

80M. N. Bauer, M. I. J. Probert, and C. Panosetti, J. Phys. Chem.

A 126, 3043 (2022).

81A. H. Larsen, M. Vanin, J. J. Mortensen, K. S. Thygesen, and

K. W. Jacobsen, Phys. Rev. B 80, 195112 (2009).

82J. P. Perdew, K. Burke, and M. Ernzerhof, Phys. Rev. Lett. 77,

3865 (1996).

83J. J. Mortensen, L. B. Hansen, and K. W. Jacobsen, Phys. Rev.

B 71, 035109 (2005).

84J. J. Enkovaara, C. Rostgaard, J. J. Mortensen, and et al., J.

Phys. Condens. Matter 22, 253202 (2010).

85C. Bannwarth, S. Ehlert, and S. Grimme, Journal of Chemical

18

Theory and Computation 15, 1652 (2019).

86C. Bannwarth, E. Caldeweyher, S. Ehlert, A. Hansen, P. Pracht,
J. Seibert, S. Spicher, and S. Grimme, WIREs Computational
Molecular Science 11, e1493 (2021).

87S. Ji, Y. Chen, Q. Fu, Y. Chen, J. Dong, W. Chen, Z. Li,
Y. Wang, L. Gu, W. He, C. Chen, Q. Peng, Y. Huang, X. Duan,
D. Wang, C. Draxl, and Y. Li, J. Am. Chem. Soc 139, 9795
(2017).

88M. Valle and A. R. Oganov, Acta Crystallogr. A. 66, 507 (2010).
89R. C. Wilson and P. Zhu, Pattern Recognit 41, 2833 (2008).
90P. Wills and F. G. Meyer, PLOS ONE 15, 1 (2020).

