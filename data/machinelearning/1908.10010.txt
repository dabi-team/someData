Research on Autonomous Maneuvering Decision of UCAV 
Based on Approximate Dynamic Programming  

Zhencai Hu, Peng Gao, Fei Wang

Harbin Institute of Technology, Shenzhen 

ABSTRACT  

9
1
0
2

g
u
A
8
2

]

O
R
.
s
c
[

2
v
0
1
0
0
1
.
8
0
9
1
:
v
i
X
r
a

Unmanned aircraft systems can  perform some more dangerous and difficult missions than manned aircraft systems. In 
some highly complicated and changeable tasks, such as air combat, the maneuvering decision mechanism is required to 
sense the combat situation accurately and make the optimal strategy in real time. This paper presents a formulation of a 
3-D one-on-one air combat maneuvering problem and an approximate dynamic programming approach for computing an 
optimal policy on autonomous maneuvering decision making. The aircraft learns combat strategies in a Reinforcement 
Leaning  method,  while  sensing  the  environment,  taking  available  maneuvering  actions  and  getting  feedback  reward 
signals. To solve the problem of dimensional explosion in the air combat, the proposed method is implemented through 
feature  selection,  trajectory  sampling,  function  approximation  and  Bellman  backup  operation  in  the  air  combat 
simulation  environment.  This  approximate  dynamic  programming  approach  provides  a  fast  response  to  a  rapidly 
changing tactical situation, learns in a long-term planning, without any explicitly coded air combat rule base. 

Keywords: Air Combat, Maneuvering Decision, Approximate Dynamic Programming, Reinforcement Learning 

1. INTRODUCTION

Unmanned combat aerial vehicle (UCAV) is a highly efficient military aircraft that carries kinds of weapons and perform 
their  missions  automatically,  UCAVs  have  been  successfully  employed  to  replace  manned  aircrafts  in  a  variety  of 
military aerial missions for their high mobility and large overloaded maneuvers. It is foreseeable that UCAV will become 
the primary choice in the future air combat field. Autonomous maneuvering decision is a mechanism with respect to how 
the UCAV choose tactical plans or maneuvers during  the process. The performance of this mechanism plays a crucial 
role  in  the  complex  and  constantly  changing  air-to-air  combat  missions.  The  goal  of  autonomous  maneuvering  in  air 
combat  games  is  to  develop  a  method  that  can  make  effective  maneuvering  decisions  online,  and  compute  desirable 
sequentially maneuvers without directly expert pilot inputs. 

Air  combat  has  been  explored  by  several  researchers  in  the  past.  The  autonomous  maneuvering  decision  methods  for 
UCAV  can be divided  into two categories. One is the non-learning method,  such as  influence diagram[1,2], differential 
game[3] and expert system[4]. There is no training and optimizing process in the non-learning strategies. Moreover, these 
maneuvering  strategies  are  fixed  and  limited  for  situational  awareness,  and  it  is  difficult  to  completely  cover  all  air 
combat  situations.  The  other  category  is  the  self-learning  method,  including  genetic  algorithm[5],  artificial  immune 
system[6]  and  reinforcement  learning[7],  and  so  forth.  These  methods  adopt  their  own  experiences,  and  the  models  are 
optimized  to  cope  with  complex  and  changeable  environments.  The  reinforcement  learning  (RL)  methods  have  been 
successfully applied to solve many difficult decision making tasks[8,9]. 

Unfortunately,  in  the  large-scale  air  combat  problems,  RL-based  dynamic  programming  (DP)  method  are  intractable 
because of the problem of exponential dimensional explosion, which indicates that it is impossible to recode information 
over all the continuous air combat states. Function approximation[10] is capable of producing superior results in a finite 
time by approximating state value functions or state-action value functions.  

In this paper, we create a 3-D air combat adversarial simulation environment and train an automatic online maneuvering 
decision model based on the approximate dynamic programming (ADP) method. First, a simplified combat environment 
is  modeled  to  simulate  the  dynamic  process.  Then  a  learning  mechanism  based  on  ADP  method  is  built.  Finally,  the 
effectiveness of automatic maneuvering decision-making strategy is verified by simulation test. 

 
 
 
 
 
 
2.  APPROXIMATE DYNAMIC PROGRAMMING TO AIR COMBAT 

Given  a  model  of  UCAV  dynamics  and  an  approximate  objective  function,  the  dynamic  programming  provides  the 
efficient  means  to  precisely  compute  an  optimal  maneuvering  policy  for  the  air  combat  game.  The  optimal  policy 
provides the best sequentially actions at any state, eliminating the massive online computation. This section gives a brief 
review of the dynamic programming, and describes the proposed approximate solution in detail. 

2.1  Dynamic Programming 

There  are  some  terminologies  will  be  used  in  the  DP  method.  The  UCAV  air  combat  simulation  is  a  process  of 
continuous  decision  making  on  UCAV,  which  can  be  abstracted  into  a  Markov  decision  process[11]  (MDP).  State 
=  , and the whole state space is consisted of all the single 
vector is  represents the combat situation at each state 
[
discrete  states  as 
ia  from  action 
s
1
 at each step. And a scalar feedback reward signal r  is released from the environment. Given 

.  The  UCAV  agent 

take  actions

is  allowed 

set

to 



1,

A

=

a

S

n

]

s

s

i

,

n

2

[

a
1

a
2



=
]

N

the  current  state  and  action  input,  we  use  the  state  transition  function 
dynamics of UCAV movement.  

f s a  to  compute  the  next  state  based  on  the 

( , )

( )V s  is 
The optimal policy is accomplished by the optimal future reward value of each state 
defined at each state, representing the long-term future reward from this state, which is used to evaluate the goodness or 
badness of the state. This optimal long-term reward function can be computed by executing the value iteration, which is 
repeatedly performing a Bellman backup[12] operation on each state, defined as 

V s . A value function 

* ( )

+

k

V

1( ) max
=

s

∈
a A

{

a
r
s

+

γ

k

V f s a

}
( , )]

[

(1) 

γ∈

[0,1]

 is the discount factor applied at each one-step look forward prediction, and 

a
where
sr  is the immediate reward 
by  taking  action  a  at  state  s .  The  Bellman  backup  optimization  can  be  implemented  on  many  states  simultaneously. 
V s . Then an optimal policy *π  can be determined 
After  adequate iteration, 
from * ( )

( )V s  will  converge to the optimal value * ( )
V s . By playing greedy strategy[13,14], the optimal action at each time step is defined as 

2.2  Function approximation 

a
i

=

*
π

(

s
i

)

=

arg max
∈
a A

{

a
r
s
i

+

*

γ

(
V f s a

[

}
, )]

i

(2) 

A continuous function approximator eliminates the need to represent and compute the future reward for every available 
state is [15]. Some correlated features are chosen to approximate the value function of a high-dimensional state space. The 
components of state vector is  can take any continuous value within the boundary. The original discrete look-up table of 
values 
s  which is initialized to be zero at all locations. And the 
( )

approx
state space S  can be determined by some manageable state sampling approach. The Bellman backup operation is applied 

( )V s  is now replaced by the approximated value

V

to each state sample, and the iteration values are stored in target vector

^

kV

S+
1( )

: 

^
V

k

+

1( ) max
=

S

∈
a A

{

a
r
S

+

γ
V

k
approx

[

}
( , )]

f S a

(3) 

The  linear  estimator  uses  a  set  of  descriptive  features

( )sφ  and  the  target  vector  to  estimate  the  future  reward 

function

V

+
k
approx

1 ( )
s

 by standard least-square estimation.  

V

+
k
1
approx

s
( )

=

φ

s
( )(

T

Φ Φ Φ
)

−
1

T

^
V

k

+
1

s
( )

Φ =

[
φ φ
2

φ
m

T

]

where

 stores  the  features  set  computed  from  all  available  states is
relieves some of the difficulty associated with dimensional curse in classical DP techniques. 



1

(4) 

S∈ .  The  ADP  architecture 

 
 
 
 
 
 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
3.  AIR COMBAT ENVIRONMENT DESIGN 

In  this  section,  the  system  states,  control  actions,  aircraft  dynamics,  goal  and  reward  function  are  described  for  the 
specific one-on-one air combat simulation. 

3.1  State, Actions and Dynamics 

Assuming the aircraft as a particle, the combat state is defined by the speed, position and the flight attitude angles of a 
red UCAV (denoted by the  r  subscript) and a blue UCAV (denoted by the  b  subscript) at any step. The state vector is 

s

=

[

v x y z
,
r
r

,

,

r

r

θ ψ ϕ
,
r
r

,

,

r

,

v x y z
,
b
b

,

,

b

θ ψ ϕ
,
b
b

,

,

b

b

]

(5) 

where x , y and z indicate the coordinates of the craft in the three-dimensional spatial inertial system, x represents the east 
axis, y represents the north axis and z represents the height axis.  v  represents the flying velocity, θ,ψ  andϕ refer to the 
pitch angle, the yaw angle and the roll angle of the aircraft, respectively. 

According  to  the  kinematic  principles,  the  flight  dynamics  of  both  sides  follow  the  differential  equations  shown  as 
formula (6). 


v

=

g N
(

x

−

θ
sin )


ψ

=


θ

=

gN

z

θ
sin / ( cos )
v

ϕ

(

N

z

cos

ϕ

−

θ
cos )

g v
/


x


y


z

=

v

cos

θ ψ
sin

=

v

cos cos

θ ψ

=

v

sin

θ

(6) 

z

x

,

,

]

zN  is  the  normal  overload  and ϕ is  the  roll  angle  of  the  aircraft. Relevantly,  the  list  of 
xN is  the  tangential overload, 
N N ϕ  is  regarded  as  the  control  commands  input  to  the  system.  The  dynamics  explain  the  state  transition 
[
t∆ , 
mechanism.  Given  the  current  state  values,  successively  UCAV  maneuvers  and  a  suitable  simulation  time  interval
f s a  using  the  fourth  order  Runge-Kutta[16] 
the  following  states  can  be  solved  with  the  state  transaction  function ( , )
method. The state samples for ADP training process are sampled by the trajectory sampling to ensure that the areas most 
likely to be seen during combat were sampled sufficiently. The simulation was initialized again at a randomly generated 
state. This process continued until all needed points were generated.  

The establishment of the UCAV maneuver library can refer to the pilot commands. NASA have designed seven typical 
flight  maneuver  models  that  can  be  used  in  air  combat,  which  are  continued  flight,  maximum  acceleration,  maximum 
N N ϕ  of these maneuvers 
deceleration, turn left, turn right, pull up and push down. The corresponding control inputs  [
are shown in Table 1.  

]

,

,

x

z

Table 1. Control inputs for basic UCAV maneuver Commands 

Maneuvering 

Air Combat Maneuvers Library 

inputs 

continued  Acceleration  Deceleration  Turn left 

Turn right 

Pull up 

Push down 

xN  

zN  

ϕ 

0 

1 

0 

2 

1 

0 

0 

1 

0 

0 

5 

0 

5 

/ 3π−

/ 3π  

0 

5 

0 

0 

-5 

0 

 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
  
3.2  Goal and Reward function  

The goal of UCAV is to attain and maintain a position of advantage over the other aircraft and shot the enemy aircraft, 
while  minimizing  the  risk  to  its  own  body.  The  reward  function  decides  the  direction  of  model  optimization  and  is 
determined through the real-time combat situation features at every step. Figure 1 shows the spatial relative relationships 
between the two aircrafts. 

Z

AA

Y

→
d

→
bV

h∆

ATA

X

→
rV

Figure1 Spatial relative relationship between the red and blue aircraft 

The angle factors in air combat are mainly Aspect Angle (AA) and Antenna Train Angle (ATA). From the perspective of 
the red aircraft, the AA represents the angle between the distance vector and the tail of the enemy's speed vector. The 
ATA is the angle between the red speed vector and the radar guideline. Both AA and ATA can be calculated using the 
opponents of the state vector as formula (7). 

→
v
r
→
v
b
→
d

=

=

[cos

ψ θ
cos
r

r

       sin

ψ θ
cos
r

r

[cos

ψ θ
cos
b

b

       sin

ψ θ
cos
b

b

=

[

x
r

−

x
b

AA

=

arccos(

ATA

=

arccos(

r

−

y
y
b
→ → →
.
|)
/ |
d
v d
b
→ → →
.
/ |
d
v d
r

z

r

−

z

b

]

|)

      sin

θ
r

]

      sin

θ
b

]

(7) 

The  extracted  features  include  velocity  advantage  reward
distance-angle reward, as Eq. 8. All the three rewards are scaled to the interval [ 1,1]−
every step is the weighted sum of them, as Eq.9. 

∆ =
v

v
r

−

v
b

,  height  advantage  reward

z
,  and  the 
, and the total reward function at 

∆ =
h

−

z

b

r

−
(1 |

AA

| /

π
)

R
3


= 


ATA

| /

π
)

+ −
(1 |
2

−

(

|

−
d R
|
d
π
k

)





e



R

=

(

+
R R
2

1

+

R
3

) / 3

(8) 

(9) 

The reward function for the blue UCAV can be computed in the same way. The feature vector chosen to approximate the 

value function is ( )
s

φ

=

[

AA ATA z

∆ ∆
,

,

,

→
v d
,|

|]

. 

4.  EXPERIMENTS 

As we have talked all the environment design and ADP application in air combat. In this section we perform simulations 
and demonstrate the capability of UCAV to make intelligent successively maneuvering decisions.  

 
 
 
 
 
 
 
      
     
 
 
 
 
 
 
 
 
   
 
 
 
 
 
 
We  set  the  red  and  the  blue  UCAV  flying  oppositely.  The  original  positional  component  values  are  sampled  from  a 
10σ = m, and means  µ listed in Table 2. The pitch angle and yaw angle 
Gaussian distribution with standard deviation 
are  sampled  from  a  narrow  range  ( 3± ° )  of  uniform  distribution  around  the values  in  Table 2.  The  states  space  in  the 
ADP  model  includes  100,000  discrete  samples  obtained  by  trajectory  sampling,  and  the  number  of  dynamic 
programming learning iterations is set as 40. 

Table 2. Initial state values used for simulation 

_S init  

rv (m/s) 

rx (m) 

ry (m) 

rz (m) 

rθ (°) 

rψ (°) 

bv (m/s) 

bx (m) 

by (m) 

bz (m) 

bθ (°) 

bψ (°) 

values 

250. 

0. 

0. 

2900. 

0. 

45. 

204. 

3000. 

3000. 

2800. 

0. 

-135. 

The time interval in the whole simulation process is 
, that is, every maneuvering decision is made after 0.25s. 
0.25s
Once a UCAV enter the dominated area of the opponent enemy or the total number of steps in current episode reaches a 
certain  predefined  maximum  number,  the  combat  episode  ends  immediately.  The  dominated  area  is  defined  as 
, and the maximum simulation length of each air combat episode is defined as 200 to avoid 
|
excessive spatial scales in the 3-D simulation. 

ATA <

 and  |

AA <

| 0.6

| 1.1

t∆ =

We present two combat situations. In the first case, the red side UCAV makes online maneuvering decisions according to 
the policy learned from ADP method, while the blue side UCAV keeps the continued flight maneuvering and flies in a 
straight  line.  As  we  can  see  in  Figure  2,  when  the  blue  side  flies  straightly,  the  red  UCAV  learns  an  effective  tactic 
strategy that pull up the aircraft and gains the advantage of height firstly, and then turn the direction around and dive to 
the back side of the enemy aircraft immediately to obtain the relative dominative area. In the second case, we let both the 
red side and blue side UCAV make maneuvering decisions through the ADP decision model, which is also known as a 
self-play process. Figure 3 shows some typical combat episodes.  

Figure 2. Typical combat result against continued flying opponent 

Figure 3. Typical combat result of self-play fight 

In this paper, we successfully apply the approximate dynamic programming method to the air combat which avoids the 
problem of dimensional explosion problem by leading in the function approximation and dynamic programming method. 

5.  CONCLUSION 

 
 
 
 
 
 
   
   
 
   
   
 
 
Specifically, we create a simplified 3-D one-on-one adversarial air combat simulation environment, design the UCAV 
available maneuvers, and specify the reward function of the UCAV which directs the optimization direction. Moreover, a 
training  and  testing  model  is  constructed.  Without  the  involvement  of  experienced  air  combat  pilot,  the  UCAV  can 
intelligently  implement  an  effective  tactic  strategy  after  a  certain  amount  of  training.  Experimental  evaluations 
demonstrate that our agent UCAV can learn the offensive and defensive tactics in the highly flexible air combat game. 
Due to the limitation of the computing power, we constrain the air combat scene in a relatively simplified occasion and 
decide the action choice as discrete values. The continuous action value maneuvering model would be more complicated 
and  likely  to  the  real  pilot  maneuvering.  In  the  future,  more  factors  can  be  considered,  such  as  expanding  the  spatial 
scales of air combat, adding different weapons, and even many-to-many UCAV battle scenarios. The air combat problem 
is still a highly complex issue. With the development of intensive learning technology and artificial general intelligence, 
both flighting and decision-making capabilities of UCAV will reach a new level of intelligence.  

REFERENCES 

[1] Virtanen K, Karelahti J, Raivio T J J O G, Control,, et al. Modeling air combat by a moving horizon influence 

diagram game[J], 2006, 29(5): 1080-1091. 

[2]  Virtanen  K,  Raivio  T,  Hamalainen  R  P  J  J  O  G,  Control,,  et  al.  Modeling  pilot's  sequential  maneuvering 

decisions by a multistage influence diagram[J], 2004, 27(4): 665-677. 

[3]  Park  H,  Lee  B-Y,  Tahk  M-J,  et  al.  Differential  game  based  air  combat  maneuver  generation  using  scoring 

function matrix[J], 2016, 17(2): 204-213. 

[4]  Mcmanus  J  W,  Chappell  A  R,  Arbuckle  P  D.  Situation  assessment  in  the  Paladin  tactical  decision  generation 

system[J], 2003. 

[5]  Mulgund  S,  Harper  K,  Zacharias  G  J  J  O  G,  Control,, et  al.  Large-scale  air  combat tactics  optimization  using 

genetic algorithms[J], 2001, 24(1): 140-142. 

[6]  Kaneshige  J,  Krishnakumar  K.  Artificial  immune  system  approach  for  air  combat  maneuvering[C].  Intelligent 

Computing: Theory and Applications V, 2007: 656009. 

[7] Mcgrew J S, How J P, Williams B, et al. Air-combat strategy using approximate dynamic programming[J], 2010, 

33(5): 1641-1654. 

[8] Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J], 

2016, 529(7587): 484-489. 

[9] Mnih V, Kavukcuoglu K, Silver D, et al. Playing Atari with Deep Reinforcement Learning[J], 2013. 
[10] Bhatnagar S, Precup D, Silver D, et al. Convergent temporal-difference learning with arbitrary smooth function 

approximation[C]. Advances in Neural Information Processing Systems, 2009: 1204-1212. 

[11] Puterman M L. Markov decision processes: discrete stochastic dynamic programming[M].  John Wiley & Sons, 

2014. 

[12] Bellman R J P O T N a O S O T U S O A. On the theory of dynamic programming[J], 1952, 38(8): 716. 
[13] Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement learning[J], 2015. 
[14]  Lange  S,  Riedmiller  M.  Deep  auto-encoder  neural  networks  in  reinforcement  learning[C].  The  2010 

International Joint Conference on Neural Networks (IJCNN), 2010: 1-8. 

[15] Powell W B J a O O R. Perspectives of approximate dynamic programming[J], 2016, 241(1-2): 319-356. 
[16] Zhang M, Qin H, Lan M, et al. A high fidelity simulator for a quadrotor uav using ros and gazebo[C]. IECON 

2015-41st Annual Conference of the IEEE Industrial Electronics Society, 2015: 002846-002851. 

 
 
 
 
 
 
 
