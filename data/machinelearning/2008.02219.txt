0
2
0
2

t
c
O
9

]

G
L
.
s
c
[

2
v
9
1
2
2
0
.
8
0
0
2
:
v
i
X
r
a

Under review as a Conference paper at ICLR 2021

META CONTINUAL LEARNING VIA DYNAMIC PRO-
GRAMMING

R. Krishnan1 and Prasanna Balaprakash1,2
1Mathematics and Computer Science Division
2Leadership Computing Facility
Argonne National Laboratory
kraghavan,pbalapra@anl.gov

ABSTRACT

Meta continual learning algorithms seek to train a model when faced with sim-
ilar tasks observed in a sequential manner. Despite promising methodological
advancements, there is a lack of theoretical frameworks that enable analysis of
learning challenges such as generalization and catastrophic forgetting. To that
end, we develop a new theoretical approach for meta continual learning (MCL)
where we mathematically model the learning dynamics using dynamic program-
ming, and we establish conditions of optimality for the MCL problem. Moreover,
using the theoretical framework, we derive a new dynamic-programming-based
MCL method that adopts stochastic-gradient-driven alternating optimization to
balance generalization and catastrophic forgetting. We show that, on MCL bench-
mark data sets, our theoretically grounded method achieves accuracy better than
or comparable to that of existing state-of-the-art methods.

1

INTRODUCTION

The central theme of meta continual learning
(MCL) is to learn on similar tasks revealed
sequentially.
In this process, two fundamen-
tal challenges must be addressed: catastrophic
forgetting of the previous tasks and general-
ization to new tasks (Finn et al., 2017; Javed
and White, 2019).
In order to address these
challenges, several approaches (Beaulieu et al.,
2020; Finn et al., 2017; Javed and White, 2019)
have been proposed in the literature that build
on the second-order, derivative-driven approach
introduced in Finn et al. (2017).

Despite the promising prior methodological ad-
vancements, existing MCL methods suffer from
three key issues: (1) there is a lack of a theo-
retical framework to systematically design and
analyze MCL methods; (2) data samples repre-
senting the complete task distribution must be
known in advance (Finn et al., 2017; Javed and White, 2019; Beaulieu et al., 2020), often, an im-
practical requirement in real-world environments as the tasks are observed sequentially; and (3) the
use of ﬁxed representations (Javed and White, 2019; Beaulieu et al., 2020) limits the ability to han-
dle signiﬁcant changes in the input data distribution, as demonstrated in Caccia et al. (2020). We
focus on a supervised learning paradigm within MCL, and our key contributions are (1) a dynamic-
programming-based theoretical framework for MCL and (2) a theoretically grounded MCL approach
with convergence properties that compare favorably with the existing MCL methods.

Figure 1: Illustration of DPMCL to learn parameters
ˆθ1, ˆθ2 (the copy of ˆθ2 is ˆθB).

Dynamic-programming-based theoretical framework for MCL: In our approach, the problem is
ﬁrst posed as the minimization of a cost function that is integrated over the lifetime of the model.

1

 
 
 
 
 
 
Under review as a Conference paper at ICLR 2021

Nevertheless, at any time t, the future tasks are not available, and the integral calculation becomes
intractable. Therefore, we use the Bellman’s principle of optimality (Bellman, 2015) to recast the
MCL problem to minimize the sum of catastrophic forgetting cost on the previous tasks and gen-
eralization cost on the new task. Next, we theoretically analyze the impact of these costs on the
MCL problem using tools from the optimal control literature (Lewis et al., 2012). Furthermore, we
demonstrate that the MCL approaches proposed in (Finn et al., 2017; Beaulieu et al., 2020; Javed
and White, 2019) can be derived from the proposed framework.

Theoretically grounded MCL approach: We derive a theoretically grounded dynamic
programming-based meta continual learning (DPMCL) approach. In our approach, the generaliza-
tion cost is computed by training and evaluating the model on given new task data. The catastrophic
forgetting cost is computed by evaluating the model on the task memory (previous tasks) after the
model is trained on the new task. We alternately minimize the generalization and catastrophic forget-
ting costs for a predeﬁned number of iterations to achieve a balance between the two. Our approach
is illustrated in Fig. 1. We analyze the performance of the DPMCL approach experimentally on
classiﬁcation and regression benchmark data sets.

2 PROBLEM FORMULATION

We focus on the widely studied supervised MCL setting where we let R denote the set of real num-
bers and use boldface to denote vectors and matrices. We use (cid:107).(cid:107) to denote the Euclidean norm for
vectors and the Frobenius norm for matrices. The lifetime of the model is given by [0, Γ] : Γ ∈ R,
where Γ is the maximum lifetime of the model. We let p(T ) be the distribution over all the tasks in
the interval [0, Γ]. Based on underlying processes that generate the tasks, the task arrival can be con-
tinuous time (CT) or discrete time (DT). For example, consider the system identiﬁcation problem in
processes modeled by ordinary differential equations (ODEs) or partial differential equations (PDE)
where the tasks represented by the states of the process x(t) are generated in CT through the ODE.
On the other hand, in the typical supervised learning setting, consider an image classiﬁcation prob-
lem where each task comprises a set of images sampled from a discrete process and the tasks arrive
in DT. As in many previous MCL works, we focus on DT MCL. However, we develop our theory
for the CT MCL setting ﬁrst because a CT MCL approach is broadly applicable to many domains.
In Section 3.3 we provide a DPMCL approach for DT MCL setting by discretizing our theory.

A task T (t) is a tuple of input-output pairs {X (t), Y(t)} provided in the interval [t, t + ∆t]∀t ∈
[0, Γ), ∆t ∈ R. We denote (x(t), y(t)) ∈ {X (t), Y(t)}. We deﬁne a parametric model g(.) with
parameters ˆθ such that ˆy(t) = g(x(t); ˆθ(t)). Although we will use neural networks, any parametric
model can be utilized with our framework. The catastrophic forgetting cost measures the error of
the model on all the previous tasks (typically known as the meta learning phase (Finn et al., 2017));
the generalization cost measures the error of the model on the new task (typically known as the meta
testing phase (Finn et al., 2017)). The goal in MCL is to minimize both the catastrophic forgetting
cost and generalization cost for every t ∈ [0, Γ). Let us split the interval [0, Γ) as [0, t] ∪ (t, Γ),
where the intervals [0, t] and (t, Γ) comprise previous tasks and new task (i.e., the collection of all
the task that can be observed in the interval, (t, Γ)), respectively. To take all the previous tasks into
account, we deﬁne the instantaneous catastrophic forgetting cost J(t; ˆθ(t)) to be the integral of the
loss function (cid:96)(τ ) at any t ∈ [0, Γ) as

J(t; ˆθ(t)) =

(cid:90) t

γ(τ )(cid:96)(τ )dτ,

(1)

τ =0
where (cid:96)(τ ) is computed on task T (τ ) with γ(τ ) being a parameter describing the contribution of
this task to the integral. The value of γ(τ ) is critical for the integral to be bounded (details are
provided in Lemma 1). Given a new task, the goal is to perform well on the new task as well as
maintain the performance on the previous tasks. To this end, we write V (t; ˆθ(t)), as the cumulative
cost (combination of catastrophic cost and generalization cost) that is integrated over [t, Γ). We
therefore seek to minimize V (t; ˆθ(t)) and obtain the optimal value, V ∗(t), by solving the problem

V ∗(t) = minˆθ(τ )∈Ω:t≤τ ≤Γ

(cid:90) Γ

τ =t

J(τ ; ˆθ(τ ))d τ,

(2)

where Ω is the compact set that implies that the parameters are initialized appropriately. Note from
Eq. (2) that V ∗(t) is the optimal cost value over the complete lifetime of the model [0, Γ). Since we

2

Under review as a Conference paper at ICLR 2021

have only the data corresponding to all the tasks in the interval [0, t], solving Eq. (2) in its current
form is intractable. To circumvent this issue, we take a dynamic programming view of the MCL
problem. We introduce a new theoretical framework where we model the learning process as a
dynamical system. Furthermore, we derive conditions under which the learning process is stable
and optimal using tools from the optimal control literature (Lewis et al., 2012).

3 THEORETICAL META CONTINUAL LEARNING FRAMEWORK

We will recast the problem deﬁned in Eq. (2) using ideas from dynamic programming, speciﬁcally
Bellman’s principle of optimality (Lewis et al., 2012). We treat the MCL problem as a dynamical
system and describe the system using the following PDE:

−

∂V ∗(t)
∂t

= minˆθ(t)∈Ω

(cid:2)J(t; ˆθ(t)) + JN (t; ˆθ(t)) + (cid:0)V ∗

ˆθ(t)

(cid:1)T

∆ˆθ(cid:3) + (cid:0)V ∗

x(t)

(cid:1)T

∆x(t),

(3)

ˆθ(t)

= ∂V ∗(t;ˆθ(t))
∂ ˆθ(t)

where V ∗(t) describes the optimal cost (the left-hand side of Eq. (2)) and (.)T refers to the transpose
operator. The notation A(.) denotes the partial derivative of A with respect to (.), for instance,
. The solution to the MCL problem is the parameter ˆθ that minimizes the right-
V ∗
hand side of Eq. (3). This involves minimizing the impact of introducing a new task on the optimal
cost. The impact is quantiﬁed by the four terms in the right-hand side of Eq. (3): the cost contribution
from all the previous tasks J(t; ˆθ(t)); the cost due to the new task JN (t; ˆθ(t)); the change in the
optimal cost due to the change in the parameters (cid:0)V ∗
∆ˆθ; and the change in the optimal cost
due to change in the input (introduction of new task) (cid:0)V ∗
function of x(t), the changes in the optimal cost due to y(t) are captured by (cid:0)V ∗
∆x(t). The
full derivation for Eq. (3) from Eq. (2) is provided in Appendix A.1. Eq. (3) is also known as the
Hamilton-Jacobi Bellman equation in optimal control (Lewis et al., 2012) with the key difference
that there is an extra term to quantify the changes due to the new task.

∆x(t). Note that since y(t) is a

ˆθ(t)

x(t)

x(t)

(cid:1)T

(cid:1)T

(cid:1)T

3.1 ANALYSIS

Our formalism has two critical elements: γ(t), which quantiﬁes the contribution of each task to
catastrophic forgetting cost, and the impact of the change in the input data distribution ∆x on learn-
ing, speciﬁcally while adapting to new tasks. We will analyze them next.

Impact of γ(t) on catastrophic forgetting cost: In Eq. (1), the integral is indeﬁnite if Γ → ∞.
If this indeﬁnite integral does not have a convergence point, our MCL problem cannot be solved.
The existence of the convergence point depends directly on the contribution of each task determined
through γ(t). In the next lemma and corollary, we will impose conditions on γ(t) under which the
cost J(t; ˆθ(t)) has a converging point.
Lemma 1. Consider t ∈ [0, Γ) : Γ → ∞ and J(t; ˆθ(t)) = (cid:82) t
τ =0 γ(τ )(cid:96)(τ )dτ, and let (cid:15) ≤ (cid:96)(τ ) ≤
L, ∀(cid:15) > 0 and let (cid:96) be a continuous ∀τ ∈ [0, t] and bounded function. Let γ(t) be a monotonically
decreasing sequence such that γ(t) → 0, as t → ∞ and (cid:82) ∞
τ =0 γ(τ ) < M, M ∈ R Under these
assumptions, J(t; ˆθ(t)) is convergent. Proof: See Appendix A.2.

The following corollary is immediate.
Corollary 1. Let J(t; ˆθ(t)) = (cid:82) t
τ =0 γ(τ )(cid:96)(τ )dτ , and let (cid:96) be continuous and L ≥ (cid:96)(τ ) ≥ (cid:15), ∀τ, (cid:15) >
0. Consider two cases for γ(t) as γ(t) > 0 : γ(t) = c or limt→∞γ(t) = ∞. Then J(t; ˆθ(t)) is
divergent. Proof: See Appendix A.2.

In Lemma 1 and Corollary 1, we consider only the scenario in which each task contributes nonzero
cost to the integral. Corollary 1 implies that no methodology can perform perfectly on all the previ-
ous tasks, as has been observed empirically (Lin, 1992). By choosing γ(t), however, we can control
the catastrophic forgetting by choosing which tasks to forget. In a typical setting larger weights are
given to the recently observed tasks, and smaller weights are given to the older tasks. However, any
choice of γ(t) that will keep J(t; ˆθ(t)) bounded is reasonable.

3

Under review as a Conference paper at ICLR 2021

Impact of ∆x(t) on learning: We ﬁrst present the following theorem.

(cid:1)T

(cid:1)T

∂
∂(θ(t))

Theorem 1. Let V (t; ˆθ(t)) = (cid:82) Γ
τ =t J(τ ; ˆθ(τ ))dτ , and let Lemma 1 hold. Let the ﬁrst derivative
of V (t; ˆθ(t)) be −(cid:2)J(t; ˆθ(t))) + (cid:0)Vˆθ(t)
∆x(t)(cid:3). Let there be a compact set Ω
∆ˆθ(t) + (cid:0)Vx
such that ˆθ(t) ∈ Ω. Consider the assumptions ||Jˆθ(t)||, > 0, ||Jx||(cid:107)∆x(t)(cid:107) < 1, (cid:107)Jx(cid:107) > 0 and
(cid:82) c
τ =t J(τ, ˆθ(τ ))d τ = J(t, ˆθ(t)), c ∈ [t, t + ∆t], ∀t ∈ [0, Γ] Let the update for the model be
provided as α(t)Vˆθ(t) with α(t) > 0 being the learning rate. Choose α(t) = 1−||Jx||(cid:107)∆x(t)(cid:107)
. The
ﬁrst derivative of V (t; ˆθ(t)) is negative semi-deﬁnite and V (t; ˆθ(t)) is ultimately bounded with the
bound on J(t; ˆθ(t)) given as J(t; ˆθ(t)) ≤ β, where β is a user deﬁned threshold on the cost. Proof:
See Appendix A.

β(cid:107)Jˆθ(t)(cid:107)

In Theorem 1, there are three main assumptions. First is a consequence of Lemma 1 where the
contributions of each task to the cost must be chosen such that the cost is bounded and conver-
gent. Second is the assumption of a compact set Ω. This assumption implies that if a weight value
initialized from within the compact set, there will be a convergence to local minima. Third is the
assumption that (cid:107)Jˆθ(t)(cid:107) > 0 and (cid:107)Jx(cid:107) > 0 are important to the proof and reﬂect through the choice
of the learning rate, α(t) = 1−||Jx||(cid:107)∆x(t)(cid:107)
. The condition ||Jˆθ(t)|| = 0 is well known in the lit-
erature as the vanishing gradient problem (Pascanu et al., 2013). On the other hand, intuitively, if
(cid:107)Jx(cid:107) = 0, then the value of the cost J will not change due to change in the input, and the learning
process will stagnate. Nevertheless, a large change in the input data distribution presents issues in
the learning process. Note that for Theorem 1 to hold, α(t) > 0, therefore, (cid:107)Jx(cid:107)(cid:107)∆x(t)(cid:107) < 1. Let
(cid:107)∆x(t)(cid:107) ≤ bx, where bx is the upper bound on the change in the input data distribution. If bx is
large (going from predicting on images to understanding texts), the condition (cid:107)Jx(cid:107)(cid:107)∆x(t)(cid:107) < 1
will be violated, and our approach will be unstable.

β(cid:107)Jˆθ(t)(cid:107)

We can, however, adapt our model to the change in the input data-distribution ∆x(t) exactly if
we can explicitly track the change in the input. This type of adaptation can be done easily when
the process generating x(t) can be described by using an ODE or PDE. In traditional supervised
learning settings, however, such a description is not possible. The issue highlights the need for strong
representation learning methodologies where a good representation over all the tasks can minimize
the impact of changes in ∆x(t) on the performance of the MCL problem (Javed and White, 2019;
Beaulieu et al., 2020). Currently, in the literature, it is common to control the magnitude of ∆x(t)
through normalization procedures under the assumption that all tasks are sampled from the same
distribution. Therefore, for all practical purposes, we can choose α(t) ≤ (β ||Jˆθ(t)||)−1.
Connection to MAML, OML, and their variants: The optimization problem in MAML (Finn
et al., 2017) and OML (Finn et al., 2019) can be obtained from Eq. (3) by setting the third and the
fourth terms to zero, which provides − ∂V ∗(t)
∂t = minˆθ(t)∈Ω[J(t; ˆθ(t)) + JN (t; ˆθ(t)]. MAML and
OML have two loops. In the meta training step the goal is to learn a prior on all the previous tasks
by performing multiple updates with data from the previous task. This can be done by optimizing
the ﬁrst term on the right-hand side in the equation above. In the meta testing phase, the goal is to
generalize to new tasks, which is akin to optimizing the second term by using repeated updates. Fur-
thermore, in MAML, the optimization in the meta testing phase is performed with the second-order
derivatives. With the choice of different architectures for the neural network, all of the approaches
that build on MAML and OML such as (Javed and White, 2019) and (Beaulieu et al., 2020) can be
directly derived. All these methods do not adopt the PDE formalism; the third and fourth terms in
Eq. (3) are not explicitly included in MAML and OML. On the other hand, the works in (Javed and
White, 2019) and (Beaulieu et al., 2020) use a well-learned representation to implicitly minimize the
impact of the last two terms (impact of change in input distribution). In our DPMCL approach, we
explicitly consider these extra terms from Eq. (3) in the design of the weight update rule, providing
us with a much more methodical process of addressing the impact of these terms and, by extension,
the impact of key challenges in the MCL setting.

4

Under review as a Conference paper at ICLR 2021

3.2 DYNAMIC PROGRAMMING-BASED META CONTINUAL LEARNING (DPMCL)

∗

∂t

(cid:1)T

ˆθ(t)

∆ˆθ + (cid:0)V ∗

= minˆθ(t)∈Ω

As a consequence of Theorem 1, the update for the parameters is provided by α(t)Vˆθ(t)). Since
V (t; ˆθ(t)) is not completely known, we have to approximate this gradient. To derive this approx-
imation, we ﬁrst rewrite Eq. (3) as − ∂V ∗(t;ˆθ(t))
(cid:2)H(t; ˆθ(t))(cid:3), which provides the
(cid:2)H(t; ˆθ(t)))(cid:3), where H(t; ˆθ(t))) = J(t; ˆθ(t)) +
optimization problem as ˆθ
(t) = arg minˆθ(t)∈Ω
JN (t; ˆθ(t)) + (cid:0)V ∗
(cid:1)T
∆x(t) is the CT Hamiltonian. The solution for this opti-
mization problem (the updates for the parameters in the network) is provided when the derivative of
the Hamiltonian is set to zero. First, we discretize the MCL problem setting. Let k be the discrete
sampling instant such that t = k∆t, where ∆t is the sampling interval. Let a task T k at instant k be
sampled from p(T ). Let T k = (X k, Y k) be a tuple, where X k ∈ Rn×p denotes the input data and
Y k ∈ Rn×1 denotes the target labels (output). Let n be the number of samples and p be the number
of dimensions. Let the parametric model be given as ˆy = g(h(x; ˆθ1); ˆθ2), where the inner map h(.)
is treated as a representation learning network and g(.) is the prediction network. Let ˆθ = [ˆθ1
ˆθ2]
be the learnable model parameters and the weight updates be given as

x(t)

ˆθ(k + 1) = ˆθ(k) − α(k)

∂
∂ ˆθ(k)

(cid:2)JN (k) + JP (k) + (cid:0)JP N (k) − JP N (k; ˆθ(k + ζ))(cid:1)(cid:3)

(4)

Our update rule has three terms (the terms inside the bracket). The ﬁrst term depends on JN , which
is calculated on the new task; the second term depends on JP and is calculated on all the previous
tasks; the third term comprises JP N and is evaluated on a combination of the previous tasks and
the new task. The ﬁrst term minimizes the generalization cost. Together, the second and the third
terms minimize the catastrophic forgetting cost. The second term minimizes the cost evaluated on
just the previous tasks, whereas the third term reduces the impact of change in input and the weights
introduced by the presence of the new task (product of the third and the fourth terms in Eq. (3)). If
we set the third term to zero, we can achieve the update rule for MAML and OML (the ﬁrst-order
β(cid:107)J ˆθ(k))(cid:107)2+(cid:15) , where β is a user-deﬁned parameter and (cid:15) > 0 is
approximation). In Eq. (4) α(k) ≤
a small value to ensure that the denominator does not go to zero. The derivation of the update rule
and the discretization are presented in Appendix A.3.

1

Equipped with the gradient updates, we now describe the DPMCL algorithm. We deﬁne a new
task sample, DN (k) = {Xk, Yk}, and a task memory (samples from all the previous tasks)
DP (k) ⊂ ∪k−1
τ =0T τ . We can approximate the required terms in our update rule Eq. 4 using samples
(batches) from DP (k) and DN (k). The overall algorithm consists of two steps: generalization and
catastrophic forgetting (see Algorithm 1 in Appendix B.4). DPMCL comprises representation and
prediction neural networks parameterized by ˆθ1 and ˆθ2, respectively. For each batch bN ∈ DN (k),
DPMCL alternatively performs generalization and catastrophic forgetting cost updates κ times. The
generalization cost update consists of computing the cost JN and using that to update ˆθ1 and ˆθ2;
the catastrophic forgetting cost update comprises the following steps. First we create a batch that
combines the new task data with samples from the previous tasks bP N = bP ∪ bN (k), where
bP ∈ DP (k). Second, to approximate the term (cid:0)JP N (k; ˆθ(k + ζ))),, we copy ˆθ2 (prediction
network) into a temporary network parameterized by ˆθB. We then perform ζ updates on ˆθB while
keeping ˆθ1 ﬁxed. Third, using ˆθB(k + ζ), we compute JP N (k; ˆθB(k + ζ)) and update ˆθ1, ˆθ2
with JP (k) + (JP N (k) − JP N (k; ˆθB(k + ζ))). The inner loop with ζ is purely for the purpose of
approximating the optimal cost.
The rationale behind repeated updates to approximate JP N (k; ˆθ(k + ζ)), is as follows. At every
instant k, JP N (k) (the cost on all the previous tasks and the new task) is the boundary value for the
optimal cost as the optimal cost can only be less than JP N (k). Therefore, if we start from JP N (k)
and execute a Markov chain for ζ steps, the end point of this Markov chain is the optimal value of
JP N (k) at instant k, provided ζ is large enough. Furthermore, the difference between the cost at
the starting point and the end point of this chain provides us with a value that should be minimized
such that the cost JP N (k) will reach the optimal value for the MCL problem at instant k. To execute
this Markov chain, we perform repeated updates on a copy of ˆθ2 (prediction network) denoted as
ˆθB. The goal of the representation network is to learn a robust representation across all tasks. If

5

Under review as a Conference paper at ICLR 2021

the representation network is updated multiple times with respect to each batch of data, it might get
biased toward the data present in the batch. Therefore, we keep the representation network ﬁxed and
update only a copy of ˆθ2. We repeat this alternative update process for each data batch in the new
task. Once all the data from the new task is exhausted, we move to the next task.

3.3 RELATED WORK

Existing MCL methods can be grouped into three classes: (1) dynamic architectures and ﬂexible
knowledge representation (Sutton, 1990; Rusu et al., 2016; Yoon et al., 2017); (2) regularization
approaches, ((Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018)) and (3) memo-
ry/experience replay, (Lin, 1992; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017). Flexible
knowledge representations maintain a state of the whole dataset with the advent of a new tasks that
require computationally expensive mechanisms (Yoon et al., 2017). Regularization approaches (Sut-
ton, 1990; Lin, 1992; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017) attempt to minimize the
impact of new tasks (changes in the input data-distribution) on the parameters of the model involv-
ing a signiﬁcant trial-and-error process. Memory/experience replay-driven approaches (Lin, 1992),
(Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017) can address catastrophic forgetting but do
not generalize well to new tasks

More recently, MCL has been investigated in Finn et al. (2017) and Finn et al. (2019). In Finn et al.
(2017), the authors presented a method where an additional term is introduced into the cost function
(the gradient of the cost function with respect to the previous tasks). However, this method requires
all the data to be known prior to the start of the learning procedure. To obviate this constraint, an
online meta-learning approach was introduced in Finn et al. (2017). The approach does not ex-
plicitly minimize catastrophic forgetting but focuses on fast online learning. In contrast with Finn
et al. (2017), our method can learn sequentially as the new tasks are observed. Although sequential
learning was possible in Finn et al. (2019), the work highlighted the trade-off between memory re-
quirements and catastrophic forgetting, which must to be addressed by learning a representation over
all the tasks as in Javed and White (2019); Beaulieu et al. (2020). Similar to Javed and White (2019);
Beaulieu et al. (2020), our method allows a representation to be learned over the distribution of all
the tasks p(T ). However, both the representation and the training model are learned sequentially in
DPCML, and we do not observe a pretraining step.

Our approach is the ﬁrst comprehensive theoretical framework based on dynamic programming that
is model agnostic and can adapted to different MCL setting in both CT and DT. Although theoretical
underpinnings were provided in Finn et al. (2019) and Flennerhag et al. (2019), the focus was to
provide structure for parameter updates but not attempt to holistically model the overall learning
dynamics as is done in our theoretical framework. The key ideas in this paper have been adapted
from dynamic programming and optimal control theory; additional details can be found in Lewis
and Vrabie (2009).

4 EXPERIMENTS

We use four continual learning data sets: incremental sine wave (regression on 50 tasks (SINE));
split-Omniglot (classiﬁcation on 50 tasks, (OMNI)); continuous MNIST (classiﬁcation on 10 tasks
and (MNIST)); and CIFAR10 (classiﬁcation on 10 tasks, (CIFAR10)). All these data sets have been
used in (Finn et al., 2019; 2017). We compare DPMCL with Naive (training is always performed
on the new task without any explicit catastrophic forgetting minimization); Experience-Replay (ER)
(Lin, 1992) (training is performed by sampling batches of data from all the tasks (previous and
the new)); online-meta learning (OML) (Finn et al., 2019), online meta-continual learning (CML)
(Javed and White, 2019), neuro-modulated meta learning (ANML) (Beaulieu et al., 2020). To keep
consistency with our computing environment and the task structure, we implement a sequential
online version (Finn et al., 2019) (where each task is exposed to the model sequentially) of all
these algorithms in our environment. For any particular data set, we set the same model hyper-
parameters (number of hidden layers, number of hidden layer units, activation functions, learning
rate, etc.) across all implementations. For fair comparisons, for any given task we also ﬁx the total
number of gradient updates a method can perform (See Appendix B.1,2,3 for details on data-set and
hyper-parameters).

6

Under review as a Conference paper at ICLR 2021

Figure 2: Top row: Cumulative error (CME) trends with respect to tasks; Bottom row: new task error (NTE)
trends with respect to tasks. The error bars describe the area between µ + σerror and µ − σerror over 50
repetitions. Gaussian smoothing ﬁlter with standard deviation of 2 is applied on each trajectory.

For each task, we split the given data into training (60%), validation (20%), and testing (20%) data
sets. Methods such as ANML, OML, and CML follow the two loop training strategy from OML:
the inner loop and the outer loop. The training data for each task is used for the inner loop, whereas
the validation data is used in the outer loop. The testing data is used to report accuracy metrics. We
measure generalization and catastrophic forgetting through cumulative error (CME) given by the
average error on all the previous tasks and the new task error (NTE) given by the average error on
the new task, respectively. For regression problems, they are computed from mean squared error;
for classiﬁcation problems, given as (1 − Acc
100 ), where Acc refers to the classiﬁcation accuracy.
For cost function, we use the mean squared error for regression and categorical cross-entropy for
classiﬁcation. We use a total of 50 runs (repetition) with different random seeds and report the mean
µ and standard error of the mean (σerror = σ/
50, where σ is the standard deviation with 50 being
the number of repetition). We report only the σerror when it is greater than 10−3; otherwise, we
indicate a 0 (See Appendix B.4,5 for implementations details).

√

4.1 RESULTS

We ﬁrst analyze the CME and NTE as each task is incrementally shown to the model. We record
the CME and NTE on the testing data (averaged over 50 repetitions) at each instant when a task is
observed. The results are shown in Fig. 2. Unlike the other methods, DPMCL achieves low error
with respect to CME and NTE. ANML and CML perform poorly on CME because of the lack of a
learned representation (as we do not have a pretraining phase to learn an encoder). The performance
of DPMCL is better than OML and ER in all data sets except CIFAR-10, where ER is comparable to
DPMCL. The poor performance of Naive is expected because it is trained only on the new task data
and thus incurs catastrophic forgetting. DPMCL generalizes well to new tasks, and consequently the
performance of DPMCL is better than OML, ER, and ANML in all the data sets except CIFAR10. As
expected, Naive has the lowest NTE. In the absence of a well-learned representation, CML exhibits
behavior similar to Naive’s and is able to quickly generalize to new task. For CIFAR10, OML
achieves NTE that is lower than that of DPMCL. ER struggles to generalize to a new task; however,
the performance is better than ANML’s, which exhibits the poorest performance due to the absence
of a well-learned representation.

The CME and NTE values for all the data sets and methods are summarized in Table 1 in Appendix
B.6. DPMCL achieves CME [µ(σerr)] of 10−5(0) for SINE, 0.171(0.007) for OMNI, 0.020(0.001)
for MNIST, and 0.496(0.003) for CIFAR10. We note that the CME for DPMCL are the best among
all the methods and all the data sets except OML, ER for SINE, and ER for CIFAR10. In SINE, the
CME of DPMCL are comparable to OML and ER. In CIFAR10, ER demonstrates a 3% improvement
in accuracy ((0.496 − 0.464) × 100). On the NTE [µ (σerr)] scale, DPMCL achieves 10−7(0) for
SINE, 0.283(0.091) for OMNI, 0.003(0) for MNIST and 0.231(0.008) for CIFAR10. On the NTE
scale, the best-performing methods are CML and Naive; this behavior can be observed in the trends
from Fig. 2. DPMCL is better than all the other methods in all the data sets except CIFAR 10,

7

020400.00.20.4CMEOMNI050.000.020.04MNIST050.00.20.4CIFAR100204010-710-4SINE020400.00.20.4NTE050.000.020.04050.00.20.40204010-710-4NaiveDPMCLOMLCMLANMLERUnder review as a Conference paper at ICLR 2021

Figure 3: Cumulative error (CME) and new task error (NTE) trends with respect to (a,b) ζ with κ = 200 and
(c, d) κ with ζ = 2. . For each value of κ or ζ, we learn a total of 50 tasks incrementally. We perform 50
repetitions of this learning. Next, we calculate the µ and σerr value of cumulative error and new task error
over these 50 runs and record them. After we have computed these values for each value of κ or ζ, we plot the
mean values with error bars from σerr as a function of ζ or κ. We apply a Gaussian smoothing ﬁlter with a
standard deviation of 2.

where OML is better (observed earlier in Fig. 2) by 10% ((0.231 − 0.108) × 100). However,
this 10% improvement for OML comes at the expense of 18 % ((0.676 − 0.496) × 100) drop in
performance on the CME scale. Similarly, although ER exhibits a 3 % improvement on CME scale,
DPMCL outperforms ER on the NTE scale by a 22.7 % ((0.458 − 0.231) × 100) improvement. The
results show that DPMCL achieves a balance between CME and NTE, thus performing better than or
comparable to the state of the art in the MCL setting.

The improved performance of DPMCL is because of the third term in Eq. (4). Therefore, we analyze
the impact of the term and show that without the third term, DPMCL suffers from poor generaliza-
tion and catastrophic large forgetting. In Fig. 3, we plot the variation in CME and NTE for the 50th
task in the OMNI data set with respect to hyperparameter ζ. In DPMCL, the value of ζ controls
the magnitude of the third term in the update rule, namely, (cid:0)JP N (k) − JP N (k; ˆθ(k + ζ)))(cid:1), where
JP N (k; ˆθ(k + ζ))) is an approximation of the optimal cost. Therefore, the larger the value of ζ, the
greater the value of (cid:0)JP N (k) − JP N (k; ˆθ(k + ζ)))(cid:1), and the third term is zero when ζ = 0. We
observe from Figs. 3(a) and (b) that when the value of ζ is zero, DPMCL generalizes poorly to a new
task (low NTE) and incurs catastrophic forgetting (low CME). As the value of ζ is increased from
zero, however, forgetting reduces (decreasing CME in Fig. 3(a)) and generalization improves (de-
creasing NTE in Fig. 3(b)). Furthermore, the best value of forgetting with generalization is achieved
when ζ = 20. We know from our theoretical analysis that the larger the value of ζ, the closer
JP N (k; ˆθ(k + ζ)) is to the optimal cost. Minimizing the difference (cid:0)JP N (k) − JP N (k; ˆθ(k + ζ)))(cid:1)
would push the model toward optimal generalization and catastrophic forgetting. This behavior is
observed in Fig. 3(a, b).

Next, we analyzed the impact of κ, the parameter controlling the total number of alternative updates.
From our theory, we know that an appropriate number of κ allows DPMCL to balance forgetting and
generalization. We observe this behavior from Figs. 3(c) and (d). Note that with an increase in κ
from zero, forgetting keeps on reducing (decreasing CME on Fig. 3(c).) When κ > 250, however, we
observe that while forgetting does improve, generalization no longer improves (Fig. 3(d)). In fact,
we observe an inﬂection point on NTE between κ = 200 and κ = 250. where a balance between
CME and NTE is achieved. DPMCL is designed in such a way that with choice of κ, this balance
point can be engineered.

5 CONCLUSIONS

We introduced a dynamic-programming-based theoretical framework for meta continual learn-
ing.Within this framework, catastrophic forgetting and generalization, the two central challenges
of the meta continual learning, can be studied and analyzed methodically. Furthermore, the frame-
work also allowed us to provide theoretical justiﬁcation for intuitive and empirically proven ideas
about generalization and catastrophic forgetting. We then introduced DPMCL which was able to
systematically model and compensate for the trade-off between the catastrophic forgetting and gen-

8

01020(a)ζ0.120.150.180.200.230.25CME01020(b)ζ0.000.200.400.60NTE0200400(c)0.200.400.60CME0200400(d)0.200.400.600.80NTEUnder review as a Conference paper at ICLR 2021

eralization. We also provided experimental results in a sequential learning setting that show that the
framework is practical with comparable performance to state of the art in meta continual learning.

In the future, we plan to extend this approach to reinforcement and unsupervised learning. Moreover,
we plan to study different architectures such as convolutional neural networks and graph neural
networks.

AUTHOR CONTRIBUTIONS

If you’d like to, you may include a section for author contributions as is done in many journals. This
is optional and at the discretion of the authors.

ACKNOWLEDGMENTS

Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.

A APPENDIX -DERIVATION AND PROOFS

First we will derive our PDE.

A.1 DERIVATION OF HAMILTON-JACOBI BELLMAN FOR THE META CONTINUAL LEARNING

SETTING

Let the optimal cost be given as

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤Γ

(cid:20) (cid:90) Γ

τ =t

J(τ ; ˆθ(τ ))d τ

(cid:21)
.

(5)

We split the interval [t, Γ] as [t, t + ∆t], and [t + ∆t, Γ]. With this split, we rewrite the cost function
as

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤Γ

J(τ ; ˆθ(τ ))d τ

(cid:20) (cid:90) t+∆t

τ =t

(cid:90) Γ

+

J(τ ; ˆθ(τ ))d τ

(cid:21)
.

τ =t+∆t

(6)

With V (t; ˆθ(t)) = (cid:82) Γ
deﬁned as V (t + ∆t; ˆθ(t + ∆t)), which provides

τ =t J(τ ; ˆθ(τ ))d τ, note that (cid:82) Γ

τ =t+∆t J(τ ; ˆθ(τ ))d τ is V at t + ∆t and can be

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤Γ

(cid:20) (cid:90) t+∆t

τ =t

J(τ ; ˆθ(τ ))d τ

+V (t + ∆t; ˆθ(t + ∆t))

(cid:21)
.

(7)

Suppose now that all information for τ ≥ t + ∆t is known, and also suppose that all optimal
conﬁgurations of parameters are known. With this information, we can narrow our search to just the
optimal costs for the interval [t, t + ∆t] and write

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤t+∆t

(cid:20) (cid:90) t+∆t

τ =t

J(τ ; ˆθ(τ ))d τ

+V ∗(t + ∆t; ˆθ(t + ∆t))

(cid:21)
.

(8)

Now, the only parameters to be obtained are for the sequence t ≤ τ ≤ t + ∆t. We approximate
the V ∗(t + ∆t; ˆθ(t + ∆t)) using the information provided in the interval [t, t + ∆t]. To do so, we
further simplify this framework by writing the ﬁrst-order Taylor series expansion. However, V ∗(t +
∆t; ˆθ(t + ∆t)) is a function of y(t), that is the model. Since y(t) is a function of (t, x(t), ˆθ(t)),

9

Under review as a Conference paper at ICLR 2021

all changes in y(t), can be summarized through (t, x(t), ˆθ(t)). Therefore, we evaluate the Taylor
series around (t, x(t), ˆθ(t)),

V ∗(t + ∆t; ˆθ(t + ∆t)) = V ∗(t; ˆθ(t)) + (cid:0)V ∗
∆ˆθ + (cid:0)V ∗

+ (cid:0)V ∗

(cid:1)T

t

(cid:1)T

x(t)

ˆθ(t)

∆t
(cid:1)T

∆x(t),

(9)

where we use the notation V ∗

(.) to denote the partial derivative with respect to (.). For instance,

V ∗

ˆθ(t)

= ∂V ∗((t;ˆθ(t))
∂ ˆθ(t)

. Substituting into the original equation, we have

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤t+∆t
(cid:1)T

+ V ∗(t; ˆθ(t)) + (cid:0)V ∗
(cid:21)

t

(cid:20) (cid:90) t+∆t

τ =t

J(τ ; ˆθ(τ ))d τ

∆t + (cid:0)V ∗

(cid:1)T

∆ˆθ

(10)

ˆθ(t)

+ (cid:0)V ∗

x(t)

(cid:1)T

∆x(t)

.

The terms V ∗(t; ˆθ(t)) + (cid:0)V ∗
pendent of τ , the sequence being selected. Therefore,

(cid:1)T

t

∆t can be brought outside the minimization because they are inde-

V ∗(t; ˆθ(t)) = minˆθ(τ )∈Ω:t≤τ ≤t+∆t

(cid:20) (cid:90) t+∆t

τ =t

+ (cid:0)V ∗

ˆθ(t)

(cid:1)T

∆ˆθ + (cid:0)V ∗

x(t)

(cid:1)T

∆x(t)

J(τ ; ˆθ(τ ))d τ

(cid:21)

+ V ∗(t; ˆθ(t))

+ (cid:0)V ∗

t

(cid:1)T

∆t

.

Upon cancellation of common terms we have

−(cid:0)V ∗

t

(cid:1)T

∆t = minˆθ(τ )∈Ω:t≤τ ≤t+∆t

(cid:20) (cid:90) t+∆t

τ =t

J(τ ; ˆθ(τ ))d τ

+ (cid:0)V ∗

ˆθ(t)

(cid:1)T

∆ˆθ + (cid:0)V ∗

x(t)

(cid:1)T

∆x(t)

(cid:21)
.

(11)

(12)

Observe that (cid:82) t+∆t
all the previous tasks and (cid:82) t+∆t
PDE as

τ =t J(τ ; ˆθ(τ ))d τ = J(t; ˆθ(t)) + (cid:82) t+∆t

τ =t+ γ(τ )(cid:96)(τ )dτ, where J(t; ˆθ(t)) represents
τ =t+ γ(τ )(cid:96)(τ )dτ represents the new task. Therefore, we get our ﬁnal

−(cid:0)V ∗

t

(cid:1)T

∆t = minˆθ(τ )∈Ω:t≤τ ≤t+∆t

(cid:20)
J(τ ; ˆθ(τ ))

+

(cid:90) t+∆t

τ =t+

γ(τ )(cid:96)(τ )dτ

+ (cid:0)V ∗

ˆθ(t)

(cid:1)T

∆ˆθ + (cid:0)V ∗

x(t)

(cid:1)T

(cid:21)
.

∆x(t)

Let us push ∆t → 0 and denote lim∆t→0

(cid:82) t+∆t
τ =t+ γ(τ )(cid:96)(τ )dτ as JN (τ ; ˆθ(τ )). We get

−(cid:0)V ∗

t

(cid:1)T

∆t = minˆθ(t)∈Ω

(cid:20)
J(t; ˆθ(t)) + JN (t; ˆθ(t))

+ (cid:0)V ∗

ˆθ(t)

(cid:1)T

∆ˆθ + (cid:0)V ∗

x(t)

(cid:1)T

(cid:21)
.

∆x(t)

(13)

(14)

This is the Hamilton-Jacobi Bellman equation that is speciﬁc to the meta continual learning problem.

10

Under review as a Conference paper at ICLR 2021

A.2 PROOF OF LEMMA AND COROLLARY 1
Proof of Lemma 1. Consider J(t; ˆθ(t)) = (cid:82) t
τ =0 γ(τ )(cid:96)(τ )dτ. Consider t1, t2 ∈ [0, t) : t2 > t1. By
γ(t1) | < 1 and (cid:82) t
the ratio test | γ(t2)
τ =0 γ(τ )dτ is convergent. Invoking the condition (cid:96)(τ ) ≤ L, ∀τ , we
get J(t; ˆθ(t)) ≤ (cid:82) t
τ =0 γ(τ )Ldτ. is conver-
gent. Therefore, J(t; ˆθ(t)) is upper bounded by a sequence that is convergent. Thus J(t; ˆθ(t)) is
convergent.

τ =0 γ(τ )Ldτ. Since γ(τ ) is convergent, by linearity (cid:82) t

(cid:82) ∞
t=0 c(cid:15)dt, where limt→∞

Proof of Corollary 1. We know that the cost is given as J(t; ˆθ(t)) = (cid:82) t
τ =0 γ(τ )(cid:96)(τ )dτ . We consider
two cases. In the ﬁrst case γ(t) = c,, where c is a constant. One can easily see that J(t; ˆθ(t)) ≥
(cid:82) ∞
t=0 c(cid:15)dt = limt→∞[c(cid:15)t]Γ
limt→∞
In the second case, J(t; ˆθ(t)) ≥ (cid:82) ∞
[0, ∞) : t2 > t1. By the ratio test, | γ(t2)
lower bounded by a function that is divergent.

t=0 (cid:15)γ(t)dt, ∀(cid:15) > 0. Consider limt→∞γ(t) = ∞ and let t1, t2 ∈
γ(t1) | > 1 and (cid:82) ∞
t=0 γ(t)dt is divergent. Therefore, J(t; ˆθ(t)) is

0 = ∞.

A.3 PROOF OF THEOREM 1

Proof of Theorem 1. To show that the cumulative cost will achieve a bound, we need only to show
that the ﬁrst derivative of the cumulative cost is negative semi-deﬁnite and bounded. This idea stems
from the principles of Lyapunov, which we discuss below.

Lyapunov Principles The basic idea is to demonstrate stability of the system described by the PDE
in the sense of Lyapunov.

Deﬁnition 1 (Deﬁnition of stability in the Lyapunov sense ((Lewis et al., 2012))). Let V (x, t) be a
non-negative function with derivative ˙V (x, t) along the system. The following is then true:

1. If V (x, t) is locally positive deﬁnite and ˙V (x, t) ≤ 0 locally in x and for all t, then the

equilibrium point is locally stable (in the sense of Lyapunov).

2. If V (x, t) is locally positive deﬁnite and decresent and ˙V (x, t) ≤ 0 locally in x and for all

t, then the equilibrium point is uniformly locally stable (in the sense of Lyapunov).

3. If V (x, t) is locally positive deﬁnite and decresent and ˙V (x, t) < a in x and for all t, then
the equilibrium point is Lyapunov stable, and the equillibrium point is ultimately bounded.
Refer to Deﬁnition 2.

4. If V (x, t) is locally positive deﬁnite and decresent and ˙V (x, t) < 0 locally in x and for all

t, then the equilibrium point is locally asymptotically stable.

5. If V (x, t) is locally positive deﬁnite and decresent and ˙V (x, t) < 0 in x and for all t, then

the equilibrium point is globally asymptotically stable.

In our analysis we seek to determine the behavior of cumulative cost with respect to change in the
parameters (controllable by choice of the parameter update) and change in the input (uncontrollable).
We assume that the changes due to input and parameter update are both bounded, and we conclude
that V is ultimately bounded and stable in the sense of Lyapunov. The idea of ultimately bounded is
described by the next deﬁnition.

Deﬁnition 2. The solution of a differential equation ˙x = f (t, x) is uniformly ultimately bounded
with ultimate bound b if b and c and for every 0 < a < c, ∃T = T (a, b) ≥ 0 such that (cid:107)x(t0)(cid:107) ≤
a =⇒ (cid:107)x(t)(cid:107) ≤ b, ∀t ≥ t0 + T.

The full proof is as follows. Let the Lyapunov function be given as

J(τ, ˆθ(τ )))d τ.

(15)

V (t; ˆθ(t)) =

(cid:90) Γ

τ =t

11

Under review as a Conference paper at ICLR 2021

The ﬁrst step is to observe that cumulative cost is a suitable function to summarize the state of the
learning at any time t. The integral is indeﬁnite and therefore represents a family of non-negative
functions, parameterized by ˆθ(τ ) and x(t). The non-negative nature of the function is guaranteed
by the construction of the cost and the choice of the cost. Furthermore, the function is zero when
both ˆθ(τ ) and x(t). are zero. The function is continuously differentiable by construction. Lemma
1 describes boundedness and existence of the convergence point for J(τ, ˆθ(τ ))) for all τ ∈ [t, Γ).
We will also assume that there exists a bounded compact set (search space) for ˆθ(τ ) (the weight
initialization procedure ensures this) and that input x(t) is always numerically bounded (this can be
achieved through data normalization methods). With all these conditions being true, we can observe
that Eq. 15 is a reasonable candidate for this analysis (Athalye, 2015) and also explains the behavior
of the system completely.

Note that we can write the ﬁrst derivative of the cost function (this was derived as part of the HJB
derivation) under the assumption that the boundary condition for the optimal cost V ∗ is V such that

∂V (t; ˆθ(t))
∂t

(cid:20)
J(t; ˆθ(t))) +

(cid:18)

= −

(cid:19)T

∆ˆθ(t)

Vˆθ(t)

(cid:18)

(cid:19)T

+

Vx

∆x(t)

(cid:21)
.

(16)

Consider the update as ∆ˆθ(t) = αVˆθ(t), with α > 0, and write by the fundamental theorem of
calculus and chain rule

Vˆθ(t) =

= −

= −

∂
∂ ˆθ(t)
(cid:90) t

τ =Γ

(cid:90) c
[

τ =Γ

∂
∂ ˆθ(t)
∂
∂ ˆθ(t)

(cid:90) Γ

τ =t

J(τ, ˆθ(τ )))d τ

J(τ, y(τ ; ˆθ(τ )))d τ

J(τ, y(τ ; ˆθ(τ )))d τ

+

=

=

=

(cid:90) t

τ =c

J(τ, y(τ ; ˆθ(τ )))d τ ]

−J(t, y(t; ˆθ(t)))

∂J(t; ˆθ(t)
∂ ˆθ(t)

−J(t; ˆθ(t))Jˆθ(t)(t; ˆθ(t)))
−J(t; ˆθ(t))Jˆθ(t).

Similarly, simplify Vx, and write by the fundamental theorem of calculus and chain rule

Vx =

= −

=

+

=

=

=

∂
∂x

(cid:90) t

τ =Γ

(cid:90) c

τ =Γ

∂
∂x
∂
∂x

[

(cid:90) Γ

τ =t

J(τ, ˆθ(τ )))d τ

J(τ, y(τ ; ˆθ(τ )))d τ

J(τ, y(τ ; ˆθ(τ )))d τ

(cid:90) t

τ =c

J(τ, y(τ ; ˆθ(τ )))d τ ]

−J(t, y(t; ˆθ(t)))

∂J(t; ˆθ(t)
∂x

−J(t; ˆθ(t))Jx(t; ˆθ(t)))
−J(t; ˆθ(t))Jx.

12

(17)

(18)

Under review as a Conference paper at ICLR 2021

Substituting Eqs. (17) and (18) into (16), we can write

∂V (t; ˆθ(t))
∂t

(cid:20)
J(t; ˆθ(t)))

= −

(cid:19)T

(cid:18)

(cid:18)

−

−

J(t; ˆθ(t))Jˆθ(t)
(cid:19)T

J(t; ˆθ(t))Jx

(cid:21)
,

∆x(t)

(α(t)J(t; ˆθ(t))Jˆθ(t))

(19)

which when simpliﬁed provides

∂V (t; ˆθ(t))
∂t

(cid:20)
J(t; ˆθ(t))) − α(t)J(t; ˆθ(t))2||Jˆθ(t)||2

= −

(cid:18)

− J(t; ˆθ(t))

(cid:19)T

Jx

∆x(t)

(cid:21)
.

Pulling J(t; ˆθ(t)) out of the bracket provides

∂V (t; ˆθ(t))
∂t

(cid:20)
= −J(t; ˆθ(t)))

(cid:18)

(cid:19)T

−

Jx

∆x(t)

1 − α(t)J(t; ˆθ(t))||Jˆθ(t)||2
(cid:21)
.

The ﬁrst term J(t; ˆθ(t))) ≥ 0 and bounded by construction of the cost function. Equation (21) is
negative as long as the terms in the bracket are greater than or equal to zero, which is possible if and
only if

(cid:20)
α(t)J(t; ˆθ(t))||Jˆθ(t)||2 +

(cid:18)

(cid:19)T

(cid:21)

Jx

∆x(t)

< 1.

Hence, by Cauchy’s inequality,

(cid:21)
(cid:20)
αJ(t; ˆθ(t))||Jˆθ(t)||2 + (cid:107)Jx(cid:107)(cid:107)∆x(t)(cid:107)

< 1,

J(t; ˆθ(t)) <

1 − (cid:107)Jx(cid:107)(cid:107)∆x(t)(cid:107)
α(t)||Jˆθ(t)||2

.

Choose α(t) = 1−(cid:107)Jx(cid:107)(cid:107)∆x(t)(cid:107)

β||Jˆθ(t)||2

with β > 0, and get the bound on (cid:107)J(t; ˆθ(t))(cid:107) as

J(t; ˆθ(t)) < β.

As a consequence, V (t; ˆθ(t)) is ultimately bounded (Lewis et al., 2012) with (cid:107)J(t; ˆθ(t))(cid:107) < β.

A.4 DERIVATION OF THE UPDATE THROUGH FINITE APPROXIMATION

From Theorem 1, the update for the network is chosen as the derivative of the cumulative cost, that
is, ∂V (t,y(t;ˆθ(t)))
∂ ˆθ(t)

providing

−

∂V ∗(t; ˆθ(t))
∂t

= minˆθ(t)∈Ω

(cid:21)
(cid:20)
H(t; ˆθ(t)))
,

(25)

where H(t; ˆθ(t))) is the CT Hamiltonian, which we will discretize and approximate. Under the
assumption that the boundary condition for the optimal cost is the cumulative cost itself, we may
write

H(t; ˆθ(t))) = J(t; ˆθ(t))
∆ˆθ + (cid:0)Vx(t)
+ (cid:0)Vˆθ(t)

(cid:1)T

(cid:1)T

∆x(t).

(26)

13

(20)

(21)

(22)

(23)

(24)

Under review as a Conference paper at ICLR 2021

Upon Euler’s disretization, we achieve

1
∆t

H(k; ˆθ(k))) =

1
J(k; ˆθ(k))
∆t
+ (cid:0)V (k; ˆθ(k + 1)) − V (k; ˆθ(k))
+ (cid:0)V (k + 1; ˆθ(k)) − V (k; ˆθ(k))

Simpliﬁcation provides

H(k; ˆθ(k))) = J(k; ˆθ(k))
+ ∆t(cid:0)V (k; ˆθ(k + 1)) − V (k; ˆθ(k))
+ ∆t(cid:0)V (k + 1; ˆθ(k)) − V ∗(k; ˆθ(k))

Taking the derivative and setting it to zero, we get

0 =

J(k; ˆθ(k))

∂
∂ ˆθ(k)
∂
∂ ˆθ(k)
∂
∂ ˆθ(k)

∆t(cid:0)V (k; ˆθ(k + 1)) − V (k; ˆθ(k))

∆t(cid:0)V (k + 1; ˆθ(k)) − V (k; ˆθ(k)).

+

+

Rearranging, we get

∂V (k; ˆθ(k))
∂ ˆθ(k)
∂
∂ ˆθ(k)
∂
∂ ˆθ(k)

+

+

(cid:20)

=

∂
∂ ˆθ(k)

J(k; ˆθ(k))
∆t

(cid:0)V (k; ˆθ(k + 1))) − V (k; ˆθ(k))(cid:1)

V (k + 1; ˆθ(k))

(cid:21)
.

(27)

(28)

(29)

(30)

Since we have no information about the data from the future, we will think of the last term
(cid:82) c
τ =t J(τ, ˆθ(τ )))d τ ∝ J(t, ˆθ(t))).

(cid:0)V (k + 1; ˆθ(k)) as 0. Under the assumption that

∂
∂ ˆθ(k)
From the fundamental theorem of calculus we write

∂
∂ ˆθ(t)

Vˆθ(t) =

(cid:90) Γ

∂
∂ ˆθ(t)
∂
∂ ˆθ(t)

= −

J(τ, ˆθ(τ )))d τ

τ =t
(cid:90) t

τ =Γ

J(τ, y(τ ; ˆθ(τ )))d τ

∝ −J(t; ˆθ(t))

∂J(t; ˆθ(t)
∂ ˆθ(t)

∝ −J(t; ˆθ(t))).

We now achieve our update as

∂V (k; ˆθ(k))
∂ ˆθ(k)
∂
∂ ˆθ(k)

−

(cid:20)

∝

∂
∂ ˆθ(k)

J(k; ˆθ(k))
∆t

(cid:0)J(k; ˆθ(k + 1)) − J(k; ˆθ(k)))(cid:1)

(cid:21)
.

(31)

(32)

To obtain the ﬁrst term, we replace 1/∆t with a small value η and write J(k; ˆθ(k)). =
JN (k; ˆθ(k)) + JP (k; ˆθ(k)), where JP (k; ˆθ(k)) refers to the cost on all the previous tasks and
JN (k; ˆθ(k)) refers to the cost on the new task. For the second term, we replace the difference
(cid:0)J(k; ˆθ(k + 1))) − J(k; ˆθ(k))(cid:1) with (cid:0)J(k; ˆθ(k + ζ))) − J(k; ˆθ(k))(cid:1) with ζ being the ﬁnite number

14

Under review as a Conference paper at ICLR 2021

of steps for the approximation. We get the gradient

∂V (k; ˆθ(k))
∂ ˆθ(k)
∂
∂ ˆθ(k)

+

(cid:20)

∝

∂
∂ ˆθ(k)

η[JN (k; ˆθ(k)) + JP (k; ˆθ(k))]

(cid:0)J(k; ˆθ(k)) − J(k; ˆθ(k + ζ)))(cid:1),

(cid:21)
,

(33)

where ζ is a predeﬁned number of iterations for the parameters. To simplify notation, we write
JN (k; ˆθ(k)) as JN (k), JN (k; ˆθ(k)) as JP (k), J(k; ˆθ(k)) as JP N (k), and J(k; ˆθ(k + ζ)) as
JP N (k; ˆθ(k + ζ)) and

∂V (k; ˆθ(k))
∂ ˆθ(k)
∂
∂ ˆθ(k)

+

(cid:20)

∝

∂
∂ ˆθ(k)

η[JN (k) + JP (k)]

(cid:0)JP N (k) − JP N (k; ˆθ(k + ζ)))(cid:1),

(cid:21)

..

Our update rule with η = 1 then is

ˆθ(k + 1) = ˆθ(k) − α(t) ×

(cid:20)

∂
∂ ˆθ(k)

[JN (k) + JP (k)]

+

∂
∂ ˆθ(k)

(cid:0)JP N (k) − JP N (k; ˆθ(k + ζ)))(cid:1),

(cid:21)
.

(34)

(35)

B APPENDIX - RESULTS AND IMPLEMENTATION DETAILS

Our implementations are done in Python with the Pytorch library (version 1.4). First, we will discuss
the datasets

B.1 DATA SET

Incremental Sine Waves (regression problem): An incremental sine wave problem is deﬁned by ﬁfty
(randomly generated) sine functions where each sine wave is considered a task and is incrementally
shown to the model. Each sine function is generated by randomly selecting an amplitude in the range
[0.1, 5] and phase in [0, π]. For training, we generate 40 minibatches from the ﬁrst sine function in
the sequence (each minibatch has eight elements) and then 40 from the second and so on. We
use a single regression head to predict these tasks. For the time, t ∈ {0, 0.001, · · · , 0.01}. We
generate a sine wave data set consisting of 50 tasks. Each task is shown sequentially to the model
and is described by its amplitude, phase, frequency, and time t ∈ {0, 0.001, · · · , 0.01}. Each task is
generated by making incremental changes to amplitude, phase, and frequency while following the
protocol in (Finn et al., 2017).

Split-Omniglot data set): We choose the ﬁrst ﬁfty classes to constitute our problem. Each character
has 20 handwritten images. The data set is divided into two parts. These classes are shown incre-
mentally to all the approaches. We choose 12 images to be part of the training data for each task, 3
images for validation, and 5 images for the testing.

MNIST & CIFAR10 data set): These data-sets are comprised of ten classes. We incremently show
each of these classes to our model. Therefore, each task in our problem is comprised of exactly one
class. We choose 60 % of the data from each task to constitute our training data and the rest is split
into validation and test.

B.2 MODEL SETUP

SINE: DPMCL uses two neural networks: one for the representation and one for the prediction.
Both have a three-layer feed-forward network (one input, one hidden, and one output layer) with
100 hidden layer neurons and relu activation function.The input vector is 3 × 1m and the output of
the prediction network is a 100 × 1 vector with a linear activation function at the output layer. For

15

Under review as a Conference paper at ICLR 2021

the implementations of Naive, ER, and OML, a single six-layer network (100 hidden units, relu ac-
tivation) is utilized. For CML, we use two networks: representation learning network and prediction
learning networks. For ANML, the representation learning network becomes the neuromodulatory
network (Beaulieu et al., 2020). For CML and ANML implementations, we use 100 hidden units
(same as DPMCL).

B.2.1 SINE MODEL DEFINITIONS

# Model F e a t u r e
s e l f . model F = t o r c h . nn . S e q u e n t i a l (

e x t r a c t i o n s .

t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’ D in ’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D in ’ ] )

)

t h e same

# The g model and t h e b u f f e r model a r e
# Model g
s e l f . model P = t o r c h . nn . S e q u e n t i a l (
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’ D in ’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

# Model b u f f e r
s e l f . m o d e l b u f f e r = t o r c h . nn . S e q u e n t i a l (
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’ D in ’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

MNIST, OMNI, CIFAR10: For all these data sets, we use a combination of convolutional neural net-
work (two convolutional layers, max pooling, relu-activation function) and a feed-forward network
(2 feed-forward layer, relu activation function, softmax output). For CIFAR10, we use three chan-
nels, of which only one channel is used with OMNI and MNIST. For the implementations of Naive,
ER, and OML, a single four-layer network (2 convolutional layers, 2 feed-forward layers) is utilized.
For CML (Javed and White, 2019), we use two networks: a representation learning network (con-
volutional neural network) and a prediction learning network (feed-forward network), respectively.
For ANML, the representation learning network becomes the neuromodulatory network (Beaulieu
et al., 2020).

B.2.2 OMNI

# F e a t u r e E x t r a c t o r
s e l f . model F = t o r c h . nn . S e q u e n t i a l (
t o r c h . nn . Conv2d ( 1 , 3 2 , k e r n e l s i z e =5 ,
t o r c h . nn . MaxPool2d ( k e r n e l s i z e =2 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . Conv2d ( 3 2 , 6 4 , k e r n e l s i z e =5 ,
t o r c h . nn . MaxPool2d ( k e r n e l s i z e =2 ,
t o r c h . nn . ReLU ( ) ,
)

s t r i d e =1 , p a d d i n g = 2 ) ,

s t r i d e = 2 ) ,

s t r i d e =1 , p a d d i n g = 2 ) ,

s t r i d e = 2 ) ,

16

Under review as a Conference paper at ICLR 2021

# F e e d f o r w a r d L a y e r
s e l f . model P = t o r c h . nn . S e q u e n t i a l (
t o r c h . nn . L i n e a r ( 7 * 7 * 6 4 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

# B u f f e r L a y e r
s e l f . m o d e l b u f f e r = t o r c h . nn . S e q u e n t i a l (
t o r c h . nn . L i n e a r ( 7 * 7 * 6 4 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

B.2.3 CIFAR10 AND MNIST

# F e a t u r e E x t r a c t o r

s e l f . model F = t o r c h . nn . S e q u e n t i a l (

t o r c h . nn . Conv2d ( 3 , 6 , 5 ) ,
t o r c h . nn . MaxPool2d ( k e r n e l s i z e =2 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . Conv2d ( 6 , 1 6 , 5 ) ,
t o r c h . nn . MaxPool2d ( k e r n e l s i z e =2 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . D r o p o u t ( )
)

s t r i d e = 2 ) ,

s t r i d e = 2 ) ,

# Feed F o r w a r d L a y e r

s e l f . model P = t o r c h . nn . S e q u e n t i a l (

s e l f . c o n f i g [ ’H ’ ] ) ,

t o r c h . nn . L i n e a r ( 2 5 6 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

# B u f f e r L a y e r

s e l f . m o d e l b u f f e r = t o r c h . nn . S e q u e n t i a l (

s e l f . c o n f i g [ ’H ’ ] ) ,

t o r c h . nn . L i n e a r ( 2 5 6 ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
t o r c h . nn . ReLU ( ) ,
t o r c h . nn . L i n e a r ( s e l f . c o n f i g [ ’H’ ] ,
)

s e l f . c o n f i g [ ’H ’ ] ) ,

s e l f . c o n f i g [ ’ D out ’ ] )

B.3 HYPERPARAMETERS

Since, DPMCL update rule involves division by the norm of the gradient, we use the Adagrad
optimizer throughout.

17

Under review as a Conference paper at ICLR 2021

Parameters
Learning rate
total runs
num tasks
Num of Hidden Layers.
Input size
Output Size
κ
ζ
Nmeta
Ngrad
N
length of DP
β
batch size
activation function
optimizer
Loss function

Sine
1e − 03
50
50
100
3
100
300
2
150
150
300
1000
1000
64
relu, output-linear
Adagrad
MSE

Omniglot
1e − 04
50
50
100
28 × 28
50
200
2
100
100
200
20000
10000
8
relu,output-softmax
Adagrad
Cross Entropy

MNIST
1e − 04
50
10
512
28 × 28
10
300
5
150
150
300
20000
10000
32
relu, output-softmax
Adagrad
Cross Entropy

CIFAR10
1e − 04
50
10
512
28 × 28
10
600
5
300
300
600
100000
10000
512
relu, output-softmax
Adagrad
Cross Entropy

Next, we will discuss the different methods that have been used in the study. We start by describing
the algorithm for DPMCL.

B.4 DPMCL

We deﬁne a new task sample, DN (k) = {Xk, Yk}, and a task memory (samples from all the pre-
vious tasks) DP (k) ⊂ ∪k−1
τ =0T τ . We can approximate the required terms in our update rule Eq. 4
using samples (batches) from DP (k) and DN (k). The overall algorithm consists of two steps:
generalization and catastrophic forgetting (see Algorithm 1 in Appendix C). DPMCL comprises
representation and prediction neural networks parameterized by ˆθ1 and ˆθ2, respectively. For each
batch bN ∈ DN (k), DPMCL alternatively performs generalization and catastrophic forgetting cost
updates κ times. The generalization cost update consists of computing the cost JN and using that to
update ˆθ1 and ˆθ2; the catastrophic forgetting cost update comprises the following steps. First we cre-
ate a batch that combines the new task data with samples from the previous tasks bP N = bP ∪bN (k),
where bP ∈ DP (k). Second, to approximate the term (cid:0)JP N (k; ˆθ(k + ζ))),, we copy ˆθ2 (prediction
network) into a temporary network parameterized by ˆθB. We then perform ζ updates on ˆθB while
keeping ˆθ1 ﬁxed. Third, using ˆθB(k + ζ), we compute JP N (k; ˆθB(k + ζ)) and update ˆθ1, ˆθ2 with
JP (k) + (JP N (k) − JP N (k; ˆθB(k + ζ))) (lines 16-17 in Alg. 1).

Initialize ˆθ1, ˆθ2, DP , DN
while k = 1, 2, 3, ...k × Γ do
i = 0 while i < κ do

Step 1: Generalization Get bN ∈ DN (k) Update ˆθ1(k), ˆθ2(k) with JN (k)
Step 2: Catastrophic Forgetting Get JP (k) with bP ∈ DP (k) Get bP N = bP ∪ bN
and copy ˆθ2 into ˆθB
j = 0
while j + 1 <= ζ do

Update ˆθB(k) with JP N (k; ˆθB(k)).
j = j+1

end
Update ˆθ2(k), ˆθ1(k) with JP (k) + (JP N (k) − JP N (k; ˆθB(k + ζ))).
i = i+1

end
Update DP (k) with DN (k).

end

B.5 COMPARATIVE METHODS

The ﬁve methods are Naive, ER, OML, CML and ANML. Naive For the naive implementation,
we use the training data for each task to train our approach. The core idea is to greedily learn

18

Under review as a Conference paper at ICLR 2021

any new task. We run gradient updates for each task data for a predetermined number of epochs.

Algorithm 1: Naive algorithm.
Initialize θ(k).
while j < num tasks do
Initialize task data DN
k =0
while k < N do
bN ∈ DN
Update θ
k = k+1

end

end

Experience-Replay (ER (Lin, 1992)): This approach aims at maintaining the performance
the tasks till now. We therefore deﬁne a task memory array. We store sam-
of all
At
ples from each new task into the experience replay array.
the start of every new
task, we use the samples from the task memory array for
training the network multi-
This method focuses on minimizing catastrophic forgetting.
ple epochs through the data.

Algorithm 2: Experience Replay Algorithm.
Initialize θ(k) and DP N
while j < num tasks do

Initialize task data DN and append to DP N
k =0
while k < N do
bP N ∈ DP N
Update θ
k = k+1

end

end

Online Meta Learning (OML, (Finn et al., 2019)): We follow the meta training testing procedures
described in (Finn et al., 2019) for this implementation. The process is composed of two loops. In
the inner loop, the training is performed on the new task; in the outer loop, the training is performed
on the buffer (task memory). We ﬁrst save samples from each task into the buffer data. Both the
inner loop and the outer loop updates are performed by using the gradients of the cost function.

Algorithm 3: Our OML implementation.
Initialize θ(t).
Initialize DP N
while j < num tasks do

Initialize DN and append to DP N for Nmeta samples in DP N do

Update θ(k) to obtain ˜θ(k)

end
for Ngrad samples in DN do

Update θ(k) using cost calculated with ˜θ.

end

end

Online Meta Continual Learning (CML, (Javed and White, 2019)): The learning process of this
method is the same as that of the one in (Finn et al., 2019) with the key difference being the use of
the representation network. The algorithm is provided in (Javed and White, 2019). This approach
is composed of a prelearned representation. We do not train a representation but try to learn it
while the tasks are being observed sequentially. This protocol is followed to highlight the idea

19

Under review as a Conference paper at ICLR 2021

that although good representations are necessary, no data is available for training a representation.

Algorithm 4: Our CML Implementation.
Initialize θ1(t), θ2(t).
Initialize DP N
while j < num tasks do

Initialize DN and append to DP N for Nmeta samples in DP N do

Update θ2(k) to obtain ˜θ2(k)

end
for Ngrad samples in DN do

Update θ(k) using cost calculated with ˜θ2 and θ1

end

end

Neuromodulated Meta Learning (ANML (Beaulieu et al., 2020)): Similar to the CML
case,
the learning process is the same as that of OML with the key difference be-
ing the neuromodulatory network which is in addition to the representation learning net-
to the earlier
work.
scenario, a the neuromodulatory network is learned while the tasks are being observed.

The algorithm is provided in (Beaulieu et al., 2020).

Similar

Algorithm 5: Our CML Implementation.
Initialize Network 1 with parameters θ1(t)
Initialize Network 2 with θ2(t)
Initialize DP N
while j < num tasks do

Initialize DN and append to DP N
for Nmeta samples in DP N do

Get output by multiplying Network 1 and Network 2 outputs(similar to gating process
in Beaulieu et al. (2020)).
Update θ2(k)

end
for Ngrad samples in DN do

Get output by multiplying Network 1 and Network 2 outputs(similar to gating process
in Beaulieu et al. (2020)).
Update θ1(k) using cost calculated with θ2 and θ1

end

end

B.6 ADDITIONAL RESULTS

Table 1: cumulative error (CME) and new task error (NTE)value for different data sets. The mean and
standard error of the mean are reported. These values are calculated by averaging across 50 repetitions.

SINE

OMNI

MNIST

CIFAR10

CME
NTE
CME
NTE
CME
NTE
CME
NTE

Naive
10−4(0)
10−7(0)
0.979(0)
0.001(0)
0.912(0)
0.001(0)
0.949(2)
0.01(0)

DPMCL
10−5(0)
10−7(0)
0.171(0.007)
0.283(0.091)
0.020(0.001)
0.003(0)
0.496(0.003)
0.231(0.008)

OML
10−5(0)
10−05(0)
0.224(0.010)
0.512(0.098)
0.023(0.001)
0.011(0)
0.676(0.006)
0.108(0.005)

CML
0.0005(0)
10−7(0)
0.706(0.245)
0.077(0.053)
0.659(0.004)
0.001(0)
0.651(0.004)
0.055(0.003)

ANML
10−4(0)
10−7(7)
0.977(0.001)
0.945(0)
0.873(0.002)
0.884(0)
0.918(0.016)
0.689(0.140)

ER
10−5(0)
10−5(0)
0.194(0.008)
0.291(0.071)
0.030(0.001)
0.024(0.001)
0.464(0.002)
0.458(0.008)

REFERENCES

R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses:
Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 139–154, 2018.

20

Under review as a Conference paper at ICLR 2021

C. D. Athalye. Necessary condition on Lyapunov functions corresponding to the globally asymptot-
ically stable equilibrium point. In 2015 54th IEEE Conference on Decision and Control (CDC),
pages 1168–1173. IEEE, 2015.

S. Beaulieu, L. Frati, T. Miconi, J. Lehman, K. O. Stanley, J. Clune, and N. Cheney. Learning to

continually learn. arXiv preprint arXiv:2002.09571, 2020.

R. E. Bellman. Adaptive control processes: a guided tour. Princeton university press, 2015.

M. Caccia, P. Rodr´ıguez, O. Ostapenko, F. Normandin, M. Lin, L. Caccia, I. H. Laradji, I. Rish,
A. Lacoste, D. V´azquez, and L. Charlin. Online fast adaptation and knowledge accumulation: A
new approach to continual learning. ArXiv, abs/2003.05856, 2020.

A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato.

Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019.

C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep net-
works. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 1126–1135. JMLR. org, 2017.

C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online meta-learning.

arXiv preprint

arXiv:1902.08438, 2019.

S. Flennerhag, A. A. Rusu, R. Pascanu, F. Visin, H. Yin, and R. Hadsell. Meta-learning with warped

gradient descent, 2019.

K. Javed and M. White. Meta-learning representations for continual learning. In Advances in Neural

Information Processing Systems, pages 1818–1828, 2019.

J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan,
T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.
Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

F. L. Lewis and D. Vrabie. Reinforcement learning and adaptive dynamic programming for feedback

control. IEEE Circuits and Systems Magazine, 9(3):32–50, 2009.

F. L. Lewis, D. Vrabie, and V. L. Syrmos. Optimal control. John Wiley & Sons, 2012.

L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.

Machine Learning, 8(3-4):293–321, 1992.

D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In Advances in

Neural Information Processing Systems, pages 6467–6476, 2017.

R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks. In

International Conference on Machine Learning, pages 1310–1318, 2013.

A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu,

and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine learning proceedings 1990, pages 216–224. Elsevier, 1990.

J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks.

arXiv preprint arXiv:1708.01547, 2017.

F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pages 3987–3995. JMLR.
org, 2017.

21

