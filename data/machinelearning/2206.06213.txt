2
2
0
2

n
u
J

3
1

]
I

A
.
s
c
[

1
v
3
1
2
6
0
.
6
0
2
2
:
v
i
X
r
a

Symbolic Regression for Space Applications:
Diﬀerentiable Cartesian Genetic Programming
Powered by Multi-objective Memetic Algorithms

Marcus M¨artens[0000−0003−1950−7111] and Dario Izzo[0000−0002−9846−8423]

European Space Agency, Noordwijk, 2201 AZ, The Netherlands
{marcus.maertens, dario.izzo}@esa.int

Abstract. Interpretable regression models are important for many ap-
plication domains, as they allow experts to understand relations between
variables from sparse data. Symbolic regression addresses this issue by
searching the space of all possible free form equations that can be con-
structed from elementary algebraic functions. While explicit mathemat-
ical functions can be rediscovered this way, the determination of un-
known numerical constants during search has been an often neglected
issue. We propose a new multi-objective memetic algorithm that ex-
ploits a diﬀerentiable Cartesian Genetic Programming encoding to learn
constants during evolutionary loops. We show that this approach is com-
petitive or outperforms machine learned black box regression models or
hand-engineered ﬁts for two applications from space: the Mars express
thermal power estimation and the determination of the age of stars by
gyrochronology.

Keywords: Cartesian Genetic Programming · Symbolic Regression ·
Memetic Algorithms.

1

Introduction

Regression models, frequently based on machine learning concepts [7] like sup-
port vector machines [4], decision trees [14], random forests [3] or neural net-
works [13] are common methods to predict or estimate properties of interest.
However, a diﬃcult balance needs to be found when regression arises as part
of a real-world engineering problem: while complex models may provide a high
accuracy, they can be opaque. For example, while certain neural network models
are powerful enough to approximate any function [8], their nonlinear and heavily
parameterized layered structure is prohibitive to interpretation. For many space-
related problems, a lack of interpretability is a show stopper, as predictability
and robustness of the systems are of critical importance.

Consequently, simpler models like linear regressors [24] are favored in such
cases, as they allow to study the behavior of the model analytically. The price
of assuming a linear model is often a suboptimal accuracy if non-linear rela-
tionships are dominant in the underlying data. A human expert in the loop

 
 
 
 
 
 
2

M. M¨artens and D. Izzo

might instead impose certain types of functions (e.g., logarithmic, polynomial,
etc.) to improve the ﬁt, but this pragmatic approach introduces human bias and
scalalibity problems.

Symbolic regression approaches aim to substitute this expert by an impar-
tial system that will discover the best function by evolutionary recombination
of basic mathematical operators, like addition, multiplication, trigonometrical
functions, exponentials and similar. This allows various approaches [22,20,15]
the rediscovery of explicit mathematical functions from sparse samples. In con-
trast, work that applies the same techniques to practical engineering problems is,
putting a few noteworthy industrial exceptions [23] aside, seemingly rare, raising
the question why this is the case?

A potential explanation could be that numerical constants are essential to
describe any reasonably noisy and complex application. Constants are, however,
rarely found in the expressions deployed to test the eﬀectiveness of symbolic
regression techniques. While those techniques excel at recombining the basic
operators, they are poorly equipped to invent constants, either relying on pro-
viding the correct constants already as a ﬁxed (sometimes mutatable) input or
producing them by idiosyncratic combinations of operators.1

In this report, we introduce a new multi-objective memetic algorithm based
on diﬀerentiable Cartesian Genetic Programming (dCGP) [11,10] that is capa-
ble of ﬁnding interpretable mathematical expressions for two diﬀerent real world
problems coming from the space domain without the need for speciﬁc expert
knowledge. We show that by exploiting the diﬀerentiability of a dCGP expres-
sion, the best ﬁtting constants can be learned directly. The evolved expressions
are shown to be either competitive or outperforming state of the art reference
models for the tasks under consideration, including machine learned regression
models. Due to its multi-objective nature, a decision maker is able to balance
interpretability (i.e. expression complexity) with model accuracy.

Our report is structured as follows: Section 2 provides some context of previ-
ous symbolic regression works and relates them to ours. Section 3 brieﬂy explains
dCGP and introduces our memetic and multi-objective algorithm. Section 4 and
Section 5 describe two case studies, one from the Mars express mission and one
from star dating, to which our algorithm is applied and evaluated. We conclude
our work in Section 6.

2 Related Work

Early works on symbolic regression were inspired by the Genetic Programming
approach of Koza [12] and deployed encodings based on arithmetic trees or gram-
mars to evolve mathematical expressions. The encoding of our chromosomes is
based on an extension of Cartesian Genetic Programming (CGP), which was
originally developed by Miller [16].A comprehensive overview about recent ge-
netic symbolic regression techniques and their applications is given by Wang et.

1 For example, the constant 3 might be constructed from an arbitrary input variable

x by evolving the expression x

x + x

x + x
x .

Symbolic Regression for Space Applications

3

al [23]. Most inﬂuential is the seminal work of Michael Schmidt and Hod Lip-
son [19] who extracted conservation laws from observing physical systems like
the double pendulum. Their evolutionary technique is available in the commer-
cial “Eureqa” software. Eureqa uses an evolutionary technique to determine the
numerical value of constants.

There also exist approaches (the deterministic FFX algorithm [15]) that do
not rely on genetic programming. Most recently, Udrescu and Tegmark [22] intro-
duced the “AI Feynman algorithm” which relies on a series of regressions tech-
niques, brute-forcing and neural networks instead of evolutionary approaches.
However, AI Feynman requires constants to be provided as input a priori or
must otherwise construct them by operator recombination.

3 Symbolic Regression with dCGP

3.1 dCPG

Diﬀerentiable Cartesian Genetic Programming (dCGP) is a recent development
in the ﬁeld of Genetic Programming (GP) adding to genetic programs their any-
order diﬀerential representation to be used in learning tasks [10]. The use of
low-order diﬀerentials, i.e. gradients, to learn parameters in Genetic Programs
is rare in previous works and can be found, for example, in [21] and [5]. In
past research, attempts to use genetic operators such as crossover and muta-
tion [9] as well as meta-heuristics such as simulated annealing [6] were made to
adapt such parameters. It is clear how access to the model’s diﬀerential infor-
mation, when available, may bring substantial gains if used during learning as
proved, for example, by the enormous success of stochastic gradient descent in
machine learning and in particular in learning deep neural network parameters.
Key to the successful use of the diﬀerential information is its availability at a low
computational cost, a feature that for ﬁrst order derivatives, i.e. the gradient,
is guaranteed by the backward automated diﬀerentiation technique, while for
higher order derivatives is more diﬃcult to obtain. Thanks to the built-in eﬃ-
cient implementation of the algebra of Taylor truncated polynomials, evaluating
a dCGP program allows seamless access to gradients and Hessians (as well as
higher order diﬀerential information). In the following, we describe how our new
Multi Objective Memetic Evolutionary Strategy (MOMES) leverages the loss
gradient and its Hessian to evolve the dCGP program and learn the ephemeral
constants simultaneously.

3.2 Multi Objective Memetic Evolutionary Strategy (MOMES)

Let us denote with η = (ηu, c) a generic chromosome encoding the symbolic
expression ˆy = fηu (x, c). We recognize in η two distinct parts: ηu is the integer
part of the chromosome making use of the classical Cartesian Genetic Program-
ming [16] encoding. Thus, ηu represents the mathematical form of f , while c

4

M. M¨artens and D. Izzo

represents the actual values of the ephemeral constants in a continuous direct
encoding. We may then introduce the gradient and Hessian as:

∇ˆy = [ ∂ ˆy
∂c1

, ..., ∂ ˆy
∂cn

]

∇2 ˆy =







∂ ˆy2
∂c1∂c1

...

∂ ˆy2
∂cn∂c1

. . .
. . .







∂ ˆy2
∂c1∂cn

∂ ˆy2
∂cn∂cn

Assuming to work on labelled data denoted by D = (xi, yi), i = 1..N we may
thus compute, for any chromosome η, the mean square error (MSE) loss:

(cid:88)

(cid:96) =

(yi − ˆyi)2

i

its full Hessian H and its full gradient G. We also deﬁne the complexity µ of the
chromosome η as the number of active nodes in its CGP. Let us now introduce
the concept of active constants ˜c as the selection from c of all components with
nonzero entries in the gradient G. Zero entries correspond, most of the time, to
ephemeral constants that are not expressed by the corresponding CGP and are
thus inactive. Similarly, we call ˜H and ˜G, respectively, the loss active Hessian and
the loss active gradient when considering only the active constants. Following
these notations and deﬁnitions, we may now describe the MOMES algorithm.

MOMES is an evolutionary strategy where all members in a population of
N P individuals, each represented by a chromosome ηi, i = 1..N P , undergo a
mutation step at each generation, acting on all the genes (active and inactive)
in ηu. The mutated chromosome is then subject to a lifelong learning process
consisting of one single step of Newton’s method and acting on the (active)
continuous part ˜c of the chromosome:

˜cnew = ˜cold − ˜H−1 ˜G

The new individual ﬁtness is then evaluated and, to preserve diversity, the chro-
mosome is added to the N P parents only if its ﬁtness is not already present
in the candidate pool. Non-dominated sorting over the objectives ((cid:96), µ) is then
applied to the candidate pool selecting N P individuals to be carried over to the
new generation. The exact implementation details of MOMES are documented
in the open source project dCGP [10].

The design of MOMES required the use of a few ideas that were the results
of experiments and iterations that are worth discussing brieﬂy at this point.
Firstly, the mutation of the CGP expression encoded in ηu acts randomly on all
genes, also the ones that are not expressed by the resulting expression. This type
of mutation, already noted as beneﬁcial when used in the standard evolutionary
strategy technique used for CGP [16], has an added beneﬁt here as it allows
for a mutated chromosome to express the same formula as the parent. In these
cases, the subsequent lifelong learning ensures the possibility to apply further
iterations of the Newton’s method to the same expression, and the mechanism
will thus ensure that, eventually, a full local descent and not only one single

Symbolic Regression for Space Applications

5

iteration is made over the most promising individuals. Additionally, this sim-
ple trick acts as a necessary diversity preservation mechanism as otherwise the
very same expression (possibly having a diﬀerent complexity as counted by the
number of active nodes in the CGP) would ﬂood the non-dominated front and
lead to a degraded convergence. Secondly, the choice of using a single step of a
second order method for the memetic part of MOMES was found to be most
eﬀective when coupled with MSE loss. It has been noted (see [11]) that, when-
ever constants appear linearly in the expressed CGP, one single step of a second
order method is enough to get to the optimal choice if the loss is expressed as
MSE. This mechanism adds a small, but beneﬁcial bias towards simple expres-
sions during evolution, since these are evaluated with optimal constants values
at each generation, while more complex appearances of constants would require
more generations to reach their optimal values. Lastly, the introduction of active
constants, Hessians and gradients allows the Hessian matrix to admit an inverse
and thus enable an eﬀective lifelong learning.

4 Case 1: Mars Express Thermal Data

4.1 Background and Dataset

Mars Express (MEX) is a spacecraft of the European Space Agency (ESA) that
has been monitoring the “red planet” continuously since 2004. To remain op-
erable in the challenging environment of space, the probe is equipped with a
thermal regulation system to which a certain power has to be allocated. An ac-
curate power budget is essential for the mission, as underestimates might lead
to malfunctions and overestimates will waste energy that should have been al-
located to scientiﬁc tasks instead. The MEX data for this work [18] is split in
a training set MEX1 and a test set MEX2. MEX1 consists of 5795 completed
orbits in the time frame from 01.01.2015 to 18.08.2019 and MEX2 to 1507 or-
bits from 18.08.2019 to 31.10.2020. Measurements of potential relevance for the
prediction of the thermal power budget have been aggregated for each orbit into
several thermal contributors. In particular, the task is to ﬁnd the thermal power
demand Pth as a function

Pth(LV AH, SH, Decl, T X, F O, N S).

(1)

A reference model based on linear regression

Pth = c · LV AH + d · SH + e · Decl + f · T X + g · F O + h · N S + i

(2)

was ﬁtted to the MEX1 data. Table 1 gives a short description of each thermal
contributor and the value of the ﬁtted coeﬃcients. Figure 1 shows the MEX data
in relation to orbital conditions and actual power consumption. The evaluation
metric for all MEX data is the Root Mean Square Error (RMSE).

6

M. M¨artens and D. Izzo

thermal
contri-
butor

LV AH
SH
Decl
T X
F O
N S
-

description

related
coeﬃcient

dCGP
variable

Launch Vehicle Adapter heating
solar heating
heating due to eclipses
transmitter activities
ﬂag indicating power oﬀ
guidance ﬂag
(baseline)

c = −7.666 · 101
d = −1.764 · 10−1
e = −3.387 · 10−3
f = −6.898 · 100
g = +1.107 · 101
h = +6.820 · 100
i = +2.267 · 102

x0
x1
x2
x3
x4
x5

Table 1. The reference model THP for MEX data is a linear regression model with
the highlighted coeﬃcients (compare Equation 2). Last column shows corresponding
variables for dCGP expressions.

4.2 Results

For this experiment, MEX1 data is used for training and MEX2 data for addi-
tional testing. Our features are the thermal contributors of the reference THP
model (see Table 1), with two diﬀerences: SH and Decl are standardized by
subtracting the mean and dividing by the standard deviation of the training set.
The determination of optimal hyperparameters for dCGP and MOMES is not
within the scope of this work, as it would require us to thoroughly explain, char-
acterize and statistically analyze each of them. In practice, evolution converges
fast enough to perform grid searches over some reasonable parameter guesses
that can be further calibrated over a few iterations of the optimization pipeline.
Table 2 reports the hyperparameters that we found to be eﬀective enough to
make our point with this experiment.

parameter

basic operators
rows
columns
levels back
maximum number of constants
maximum mutations
generations
population size

value

+, −, ×, ÷, log
2
20
20
5
4
50000
40

Table 2. Hyperparameters deployed for dCGP and MOMES for the MEX1 dataset.

As common with evolutionary techniques, we multi-start MOMES 200 times
to account for its stochasticity. Out of those 200 results, we select the one that has
the strongest extreme point (i.e. lowest RMSE) and show the complete Pareto

Symbolic Regression for Space Applications

7

Fig. 1. MEX data, including orbital and operative parameters. Goal is to predict the
average power consumption from thermal contributors, compare Equation 1.

front in Figure 2. For further comparison with the THP reference model, we
select three diﬀerent formulas: M1 is at the extreme point, M2 is the expression
with minimum complexity that is still above the reference and M3 is a low
complexity expression. Table 3 shows the corresponding RMSE and the average
over- and underestimations.

We observe that the RMSE for MEX2 is considerably higher than for MEX1
for each of the models, with Expression M1 generalizing slightly better to the
unseen conditions of MEX2 than THP. We show the actual predictions of both,
THP and Expression M1 in Figure 2, which highlights the diﬀerences between
the models. From a mathematical point of view, M1 includes several products
of variables, resulting in a degree 2 polynomial. Despite having access to the
division and logarithm operator, evolution did not favor them in the population.
However, other non-dominated fronts from the 200 runs (not shown) included
sometimes division, constructing rational functions of similar RMSE. M3 is a
linear model without the thermal contributors x2 and x3 that is worse than
THP but could provide an interesting alternative if certain measurements would
not be readily available. While some of the numerical constants are similar across
diﬀerent expression, it is clear that the memetic algorithm is optimizing them
for each expression independently to further reduce the error. Furthermore, the
maximum of 5 constants is not (always) expressed, highlighting that MOMES
is selective and parsimonious. In comparison, the simple linear regression model
THP relies already on 7 explicit constants.

8

M. M¨artens and D. Izzo

Fig. 2. Top: Non-dominated front found by MOMES, 18 out of 40 individuals, MEX1
data. Blue line is the reference model THP. The expressions M1, M2 and M3 are
highlighted for further analysis. Bottom: Predictions of THP and dCGP expression
M1 in direct comparison with target power consumption. Shaded region is test data
MEX2.

5 Case 2: Star Dating Gyrochronology

5.1 Background and Dataset

This dataset was released by Moya et al. [17] as a benchmark for AI research on
star dating. Many physical models for astronomical problems (e.g. the search for
habitable exoplanets) depend on the age of a star, which is however not directly
measurable. Consequently, the age of a star needs to be inferred from observable
features. The public benchmark describes 1464 stars whose accurate ages have
been determined by asteroseismology and association to star clusters, providing
for each star the stellar features described in Table 4.

In this context an approach termed “gyrochronology” is of particular interest,
which hypothesizes a functional relationship between star age, stellar mass and

Symbolic Regression for Space Applications

9

1
X
E
M

2
X
E
M

Model

RMSE

complx.

avg. over-
estimate

avg. under-
estimate

THP
M1
M2
M3

8.672
8.478
8.647
9.223

-
61
34
22

6.665
6.657
6.826
7.184

6.774
6.442
6.619
7.232

Model

RMSE

complx.

avg. over-
estimate

avg. under-
estimate

THP
M1
M2
M3

12.454
11.390
12.828
14.991

-
61
34
22

10.684
9.753
10.924
12.928

4.549
4.461
4.998
6.745

Table 3. RMSE for 3 selected expression from Figure 2, complexities and values for
average over- and underestimation of thermal power consumption. THP is the linear
regression reference model.

rotational period. Some empirical relations (assuming a ﬁxed functional form)
have been proposed [1] but the accuracy of gyrochronology remains controversial,
with some works suggesting that linear relationships might not be suﬃcient at all
or just applicable for stars of a certain age group [2]. Consequently, the authors
of the dataset propose to study star dating directly as regression problem using
supervised machine learning models.

Feature

Description

dCGP
variable

M
R
Tef f
L
[F e/H]
log g
Prot

Stellar Mass
Stellar Radius
Stellar eﬀective temperature
Luminosity
Metallicity
Logarithm of surface gravity
Stellar rotational period
Table 4. Features available for every star in the dataset.

x0
x1
x2
x3
x4
x5
x6

Several reference models including linear regression, decision tree regressor,
random forest regressor, support vector regressor, Gaussian process, kNN, neural
networks and ensembles are provided within the benchmark. Additionally, four
diﬀerent splits of the data for training and test are available:

– A a random 80/20 split
– B1 stars of age [0.4, 4.2] Gyr are used for training, ages [4.2, 13.8] for testing
– B2 stars dated by cluster belonging used for training, remainder for testing

10

M. M¨artens and D. Izzo

– C independent control with 32 new stars for testing, including our own sun

The evaluation metric for all gyrochronology models is the Mean Absolute
Error (MAE). Additionally, the dataset provides error bounds on the age of the
stars allowing to deﬁne a precision metric by the fraction of star age predictions
that fall within the corresponding conﬁdence interval.

5.2 Results

For this experiment, we deployed MOMES on all training sets of the given bench-
marks and re-evaluated the found expressions on the test sets. We selected the
6 features as described in Table 4 and scaled them by dividing each of them by
the standard deviation of the corresponding training set. Similar to Section 4, a
grid-search was used to ﬁnd suitable hyperparameters for dCGP and MOMES,
reported in Table 5.

parameter

value

basic operators
rows
columns
levels back
number of constants
maximum mutations
generations
population size

+, −, ×, ÷, log, sin
2
16
16
5
6
500000
40

Table 5. Hyperparameters deployed for dCGP and MOMES for star dating.

A multi-start setup of 400 independent runs (100 for each benchmark) has
been deployed to our servers and completed in about 12 hours. We selected the
run that resulted in the lowest MAE (extreme point) on the training set for
further analysis. Table 6 shows the MAE and the precision of the lowest MAE
expression found in the non-dominated front of MOMES in comparison with
the reference models. To exemplify the diversity and complexity trade-oﬀs in
the found expressions, we show the approximated Pareto fronts in Figure 3 for
Benchmark A and B1 (for reasons of space).

Interpreting the results, the dCGP based models are competitive with some
of the machine learning models for benchmark A, but have a higher error than
neural networks, Gaussian processes, kNN and random forests. Benchmark B1
represents a domain gap study, which is (without the deployment of additional
techniques or assumptions) hard for most machine learning models to bridge.
Since dCGP looks to capture the physical relations between the diﬀerent features
in algebraic expressions, it leads itself to better generalization.

Investigating the Pareto front of dCGP on the test-set for B2, the simple
expression x6 − sin(x1 − x4) + 6.987 could be shown to outperforms all other

Symbolic Regression for Space Applications

11

machine learning models by some margin. On a closer look however, it turned
out that the correlation between the training error and test error in this bench-
mark was rather low when looking at the entirety of our MOMES runs. Although
this particular result for B2 has to be taken with a grain of salt, it neverthe-
less demonstrates the existence of surprisingly simple expressions that provide
an accurate explanation of the data. Thanks to the interpretability of dCGP
expressions, these type of insights may uncover potential issues related to over-
ﬁtting of machine learning models or issues related to the data split that would
go unnoticed otherwise. Lastly, on benchmark C the best dCGP expressions
achieves a low MAE, only surpassed by the Gaussian process model.

Fig. 3. Non-dominated front found by MOMES for Benchmark A (top) and B1
(bottom). Shown is the non-dominated front after the complete population was re-
evaluated on the test-set of its corresponding benchmark. Blue lines represent reference
machine learned models as described by Moya et al. [17]

12

M. M¨artens and D. Izzo

E
A
M

n
o
i
s
i
c
e
r
P

A
B1
B2
C

A
B1
B2
C

dcgp

nnet

0.62
1.41
1.39
0.97

0.41
1.48
1.85
1.10

lr

0.95
1.92
1.56
1.30

dcgp

nnet

lr

50.39
51.97
42.26
15.63

73.23
59.84
28.45
34.37

35.43
29.92
30.96
21.87

dtr

0.70
1.62
1.90
2.11

dtr

63.78
51.18
21.34
9.37

rf

0.57
1.71
1.85
1.01

svm bayes

knn

0.68
1.72
1.51
1.28

0.95
1.94
1.48
1.30

0.53
1.61
1.84
1.73

gp

0.45
1.67
1.62
0.84

rf

svm bayes

knn

gp

60.63
48.03
25.94
21.87

46.46
37.79
33.05
12.50

32.28
28.35
29.71
21.87

77.95
55.91
25.52
15.62

62.20
51.97
30.96
40.62

Table 6. MAE and precision on test set for all star dating benchmarks. Each model
was trained for each benchmark separately. The dCGP expressions used are always the
ones that are at the extreme end of the MAE objective of the non-dominated front
created by MOMES.

6 Conclusions

Following our experiments on the Mars express and star dating datasets, we have
demonstrated that MOMES is capable of ﬁnding algebraic expressions that ex-
plain unseen data and have the capability to generalize well to it. The fact that
the numerical constants for each expression are learned at each step of evolu-
tion guides the memetic algorithm towards expressions that are both accurate
and of low complexity. The expressions with the lowest error notably contain
products of sub-expressions or non-linear operators like the sin-function and are
thus more expressive than linear regressors while still amenable to mathematical
analysis. Consequently, no expert is required to make potentially biased guesses
in ﬁnding non-linear expressions. The task that still remains for the expert is
the engineering and scaling of features (e.g. the thermal contributors in case of
MEX) to obtain the best possible results, as well as the determination of suitable
basic operators and eﬃcient hyperparameters.

As shown in the case of benchmark B2 for the star dating case study, deploy-
ing symbolic regression in general is beneﬁcial as sometimes simple expressions
appear to be more general than complex black box models. Given the practical
beneﬁts of interpretability that comes with algebraic expressions together with
the low requirements for their inference make algorithms like MOMES valuable
methods of knowledge discovery. In particular, applications with sparse data in
extreme environments like space are likely to beneﬁt from it.

Acknowledgements The authors are grateful to Thomas Dreßler, who helped
making the MEX data public and explained it to us. Similarly, the authors want
to thank Roberto J. L´opez-Sastre for providing support and insight into the
gyrochronology data.

Symbolic Regression for Space Applications

13

References

1. Angus, R., Aigrain, S., Foreman-Mackey, D., McQuillan, A.: Calibrat-
targets. Monthly No-
the Royal Astronomical Society 450(2), 1787–1798 (04 2015).

ing
tices of
https://doi.org/10.1093/mnras/stv423

gyrochronology

using Kepler

asteroseismic

2. Barnes, S.A.: A simple nonlinear model for the rotation of main-sequence cool
stars. i. introduction, implications for gyrochronology, and color–period diagrams.
The Astrophysical Journal 722(1),
222 (2010). https://doi.org/10.1088/0004-
637X/722/1/222

3. Breiman, L.: Random forests. Machine
https://doi.org/10.1023/A:1010933404324

learning 45(1),

5–32

(2001).

4. Chang, C.C., Lin, C.J.: Libsvm: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST) 2(3), 1–27 (2011).
https://doi.org/10.1145/1961189.1961199

5. Emigdio, Z., Trujillo, L., Sch¨utze, O., Legrand, P., et al.: A local search ap-
proach to genetic programming for binary classiﬁcation. In: Proceedings of the
2015 on Genetic and Evolutionary Computation Conference-GECCO’15 (2015).
https://doi.org/10.1145/2739480.2754797

6. Esparcia-Alcazar, A.I., Sharman, K.C.: Genetic programming techniques that
evolve recurrent neural network architectures for signal processing. In: Neu-
the 1996
ral Networks
IEEE Signal Processing Society Workshop. pp. 139–148.
IEEE (1996).
https://doi.org/10.1109/NNSP.1996.548344

for Signal Processing [1996] VI. Proceedings of

7. Fern´andez-Delgado, M., Sirsat, M.S., Cernadas, E., Alawadi, S., Barro, S., Febrero-
Bande, M.: An extensive experimental survey of regression methods. Neural Net-
works 111, 11–34 (2019). https://doi.org/10.1016/j.neunet.2018.12.010

8. Hornik, K., Stinchcombe, M., White, H.: Multilayer

feedforward net-
works are universal approximators. Neural networks 2(5), 359–366 (1989).
https://doi.org/10.1016/0893-6080(89)90020-8

9. Howard, L.M., D’Angelo, D.J.: The GA-P: A genetic algorithm and genetic pro-

gramming hybrid. IEEE Expert 10(3), 11–15 (1995)

10. Izzo, D., Biscani, F.: dcgp: Diﬀerentiable cartesian genetic programming
(2020).

of Open Source Software 5(51),

2290

easy.

made
Journal
https://doi.org/10.21105/joss.02290

11. Izzo, D., Biscani, F., Mereta, A.: Diﬀerentiable genetic programming.

In:
European conference on genetic programming. pp. 35–51. Springer (2017).
https://doi.org/10.1007/978-3-319-55696-3 3

12. Koza, J.R.: Genetic programming as a means

for programming comput-
selection. Statistics and computing 4(2), 87–112 (1994).

ers by natural
https://doi.org/10.1007/BF00175355

13. Lathuili`ere, S., Mesejo, P., Alameda-Pineda, X., Horaud, R.: A comprehensive
analysis of deep regression. IEEE transactions on pattern analysis and machine in-
telligence 42(9), 2065–2081 (2019). https://doi.org/10.1109/TPAMI.2019.2910523
interdisciplinary
(2011).
14–23

14. Loh, W.Y.: Classiﬁcation and regression trees. Wiley
1(1),
and
reviews:
https://doi.org/10.1002/widm.8

data mining

knowledge

discovery

15. McConaghy, T.: FFX: fast, scalable, deterministic symbolic regression technology.

In: Genetic Programming Theory and Practice IX, pp. 235–260. Springer (2011)

14

M. M¨artens and D. Izzo

16. Miller, J.F.: Cartesian genetic programming. In: Cartesian Genetic Programming,

pp. 17–34. Springer (2011). https://doi.org/10.1007/978-3-642-17310-3 2

17. Moya, A., Recio-Martinez, J., L´opez-Sastre, R.J.: Ai for dating stars: a bench-
marking study for gyrochronology. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition. pp. 1971–1981 (2021).
https://doi.org/10.1109/CVPRW53098.2021.00225

18. M¨artens, M., Dreßler, T.: Mars express thermal power dataset (Apr 2022).

https://doi.org/10.5281/zenodo.6417900

19. Schmidt, M., Lipson, H.: Distilling free-form natural laws from experimental data.

science 324(5923), 81–85 (2009). https://doi.org/10.1126/science.1165893

20. Schmidt, M.D., Lipson, H.: Solving iterated functions using genetic programming.
In: Proceedings of the 11th Annual Conference Companion on Genetic and Evo-
lutionary Computation Conference: Late Breaking Papers. pp. 2149–2154 (2009).
https://doi.org/10.1145/1570256.1570292

21. Topchy, A., Punch, W.F.: Faster genetic programming based on local gradient
search of numeric leaf values. In: Proceedings of the 3rd Annual Conference on Ge-
netic and Evolutionary Computation. pp. 155–162. Morgan Kaufmann Publishers
Inc. (2001)

22. Udrescu, S.M., Tegmark, M.: AI Feynman: A physics-inspired method for symbolic
regression. Science Advances 6(16) (2020). https://doi.org/10.1126/sciadv.aay2631
23. Wang, Y., Wagner, N., Rondinelli, J.M.: Symbolic regression in materials science.
MRS Communications 9(3), 793–805 (2019). https://doi.org/10.1557/mrc.2019.85

24. Weisberg, S.: Applied linear regression, vol. 528. John Wiley & Sons (2005)

