Value Function Based Difference-of-Convex Algorithm for Bilevel
Hyperparameter Selection Problems

Lucy Gao 1 Jane J. Ye 2 Haian Yin 3 Shangzhi Zeng 2 Jin Zhang 3 4

2
2
0
2

n
u
J

3
1

]

C
O
.
h
t
a
m

[

1
v
6
7
9
5
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Gradient-based optimization methods for hyper-
parameter tuning guarantee theoretical conver-
gence to stationary solutions when for ﬁxed upper-
level variable values, the lower level of the bilevel
program is strongly convex (LLSC) and smooth
(LLS). This condition is not satisﬁed for bilevel
programs arising from tuning hyperparameters in
many machine learning algorithms. In this work,
we develop a sequentially convergent Value Func-
tion based Difference-of-Convex Algorithm with
inexactness (VF-iDCA). We show that this algo-
rithm achieves stationary solutions without LLSC
and LLS assumptions for bilevel programs from a
broad class of hyperparameter tuning applications.
Our extensive experiments conﬁrm our theoreti-
cal ﬁndings and show that the proposed VF-iDCA
yields superior performance when applied to tune
hyperparameters.

1. Introduction

Virtually all machine learning algorithms require hyperpa-
rameter tuning. For example, in regression and classiﬁcation
problems, regularization is often used to induce structured
solutions and to control model complexity. The degree of
regularization is typically controlled by hyperparameters.
Careful tuning of these hyperparameters is critical to the
predictive accuracy of the ﬁtted model.

An effective hyperparameter selection strategy is to choose
the hyperparameters that minimize a loss function on a

1Department of Statistics and Actuarial Science, University
of Waterloo, Waterloo, Ontario, Canada 2Department of Math-
ematics and Statistics, University of Victoria, Victoria, British
Columbia, Canada 3Department of Mathematics, SUSTech Inter-
national Center for Mathematics, Southern University of Science
and Technology, Shenzhen, Guangdong, China 4National Center
for Applied Mathematics Shenzhen, Shenzhen, Guangdong, China.
Correspondence to: Jin Zhang <zhangj9@sustech.edu.cn>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

held-out validation set, which amounts to solving a bilevel
program (BLP). We consider the following BLP framework:

min
x∈Rn,λ∈RJ
+

L(x)

s.t. x ∈ arg min
x(cid:48)∈Rn

(cid:40)

l(x(cid:48)) +

J
(cid:88)

i=1

(cid:41)

(1)

λiPi(x(cid:48))

,

where L, l : Rn → R and Pi : Rn → R+, i = 1, . . . , J
are merely convex (possibly non-smooth) functions. In BLP
Eq. (1), λ is a vector of hyperparameters, the upper-level
(UL) problem minimizes the validation error in terms of the
hyperparameters, and the lower-level (LL) problem ﬁts a
model to training data for a given choice of hyperparameters.
Note that in BLP Eq. (1), we assume that for each λ ∈ RJ
+,
the LL problem has solutions.

Table 1 presents examples of bilevel hyperparameter se-
lection problems of the form (1) with non-strongly convex
and/or non-smooth LL objective functions. In Section 4,
we will study a generalization of BLP Eq. (1) with LL
constraints, which captures the support vector machine clas-
siﬁcation example in Table 1.

1.1. Related Work

The simplest hyperparameter selection approach is brute-
force grid search, which is seldom applied when there are
more than two hyperparameters due to its unfavourable
computational complexity. The current gold standard in
the presence of more than two hyperparameters is Bayesian
optimization, which models the conditional probability of
the performance on some metric given hyperparameters and
a dataset. However, these gradient-free methods often scale
poorly beyond 10 to 20 hyperparameters.

Gradient-based optimization methods (GM) can handle up
to a few hundred hyperparameters. Existing gradient-based
methods can be roughly divided into two categories based
on the strategy used for calculating the hypergradient (the
gradient of the UL objective). Implicit gradient methods
(IGM), also known as implicit differentiation (Pedregosa,
2016; Rajeswaran et al., 2019; Lorraine et al., 2020), replace
the LL sub-problem with an implicit equation. These IGM
apply the implicit function theorem (IFT) to the optimal-

 
 
 
 
 
 
Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Table 1. Examples of bilevel hyperparameter selection problems of the form (1) and its generalization. These examples were also explored
in Kunapuli et al. (2008) and Feng & Simon (2018).

Machine learning algorithm

elastic net

sparse group lasso

x

β

β

low-rank matrix completion

θ, β, Γ

λ

λ1, λ2

λ ∈ RM +1

+

λ ∈ R2G+1
+

support vector machine

w, c

λ, ¯w

L(x)/l(x)

1
2

1
2

(cid:80)

i∈Ival/i∈Itr

|bi − β(cid:62)ai|2

(cid:80)

i∈Ival/i∈Itr

|bi − β(cid:62)ai|2

(cid:80)

(i,j)∈Ωval/(i,j)∈Ωtr

1

2 |Mij − xiθ − zjβ − Γij|2

(cid:80)

j∈Ival/j∈Itr

max(1 − bj(w(cid:62)aj − c), 0)

The detailed descriptions of these problems are presented in the Supplemental Material.

(cid:80)J

i=1 λiPi(x)

LL Constraints

λ1(cid:107)β(cid:107)1 + λ2

2 (cid:107)β(cid:107)2

2

(cid:80)M

m=1 λm(cid:107)β(m)(cid:107)2 + λM +1(cid:107)β(cid:107)1

λ0(cid:107)Γ(cid:107)∗ + (cid:80)G

g=1 λg(cid:107)θ(g)(cid:107)2 + (cid:80)G
2 (cid:107)w(cid:107)2

λ

g=1 λg+G(cid:107)β(g)(cid:107)2

-

-

-

− ¯w ≤ w ≤ ¯w

ity conditions of the LL problem, and hence derive their
hypergradients by solving a linear system. As it involves
computing a Hessian matrix and its inverse, in practice, the
conjugate gradient method (CG) or Neumann method are
used for fast inverse computation. Explicit gradient methods
(EGM) replace the LL problem with a gradient-descent-type
dynamics iteration (Franceschi et al., 2017; 2018; Finn et al.,
2017; Nichol et al., 2018; Lorraine & Duvenaud, 2018).
Speciﬁcally, Franceschi et al. (2017) and Franceschi et al.
(2018) ﬁrst calculate gradient representations of the LL ob-
jective and then perform either reverse or forward gradient
computations (called Reverse Hyper-Gradient (RHG) and
Forward Hyper-Gradient (FHG)) for the UL objective. In
order to reduce the amount of computation, Shaban et al.
(2019) proposes truncated reverse hypergradient to truncate
the gradient trajectory, and Liu et al. (2019a) uses the dif-
ference of vectors to approximate the gradient. Recently, Ji
et al. (2020a) and Ji et al. (2021) analyze the convergence
rate and complexity of IGM and EGM, respectively. Ji et al.
(2022) analyze the effect of inner loop step for IGM and
EGM. Besides, some work (Liu et al., 2020; 2021a; Sow
et al., 2022; Liu et al., 2022) study the case where LL prob-
lem is convex but not strongly convex; Liu et al. (2021b)
proposed an initialization auxiliary method for the setting
where the LL problem is generally nonconvex.

1.2. Our Motivations and Contributions

Some theoretical progress has been made in both gradient-
free and gradient-based methods. However, existing GM
require restrictive assumptions to ensure the convergence
towards stationarity: both IGM and EGM require the LL
strong convexity (LLSC) and smoothness (LLS), which is
often violated in applications. For IGM, the subsequential
convergence of stationary solutions can be found in Pe-
dregosa (2016). Without the LLS assumption, some IGM
variants study the computation of hypergradients, where
the convergence analysis of stationarity of such methods is
still lacking; see Implicit Gradient-based Joint Optimization
(IGJO) in Feng & Simon (2018) and Implicit Forward Dif-
ferentiation Method (IFDM) in Bertrand et al. (2020) and
Bertrand et al. (2021). For EGM, when the LL objective
is strongly convex uniformly for λ and its gradient w.r.t.
x is uniformly Lipschitz continuous, the LL gradient iter-

ates converge linearly in a uniform rate. This further leads
to a uniformly linear convergence of the hypergradient of
approximated UL objective to the true hypergradient. The
subsequential convergence towards stationary solutions for
EGM thus follows easily; see (Grazzi et al., 2020). Focus-
ing on stationarity convergence, under LLSC and LLS, Ji
et al. (2020b) and Ji & Liang (2021) give a comprehensive
characterization on the nonasymptotic convergence rate and
complexity bounds for both IGM and EGM.

Many applications involve BLP where LLSC and LLS are
violated; see the hyperparameter selection applications in
BLP Eq. (1) and Table 1. This motivates us to investigate
algorithms with provable convergence towards stationary
solutions without assuming LLSC and LLS.

Table 2. Comparing theoretical results among BLP algorithms.

Category

Methods

LLSC LLS

Conv. Type

Sol. Quality

EGM

IGM

FHG/RHG

CG/Neumann
IGJO/IFDM

w/

w/
w/

DCA

VF-iDCA

w/o

w/

w/
w/o

w/o

subsequential

stationary

subsequential
-

stationary
-

sequential

stationary

A striking feature of our study is the decoupling of the hy-
perparameters from the regularization term, which results
in an equivalent BLP reformulation with fully convex LL
problem. Critically, since the reformulated LL problem is
fully convex, the value function (VF) of the LL problem
is convex. Subsequently, we can further reformulate BLP
Eq. (1) as a DC program. By using the main idea of DC
algorithm (DCA) which linearizes the concave part of the
DC structure, we propose a sequential convex programming
scheme with subproblem inexactness, named VF-iDCA. For
a wide range of applications, we justify the sequential con-
vergence towards stationary solutions, which is an unusual
ﬁnding in BLP optimization. Recently, a DC-type algo-
rithm, namely, iP-DCA for solving bilevel support vector
classiﬁcation problem was proposed in Ye et al. (2021).
In comparison with iP-DCA, the new hyperparameter de-
coupling mechanism of VF-iDCA allows us to efﬁciently
handle more hyperparameter selection BLPs where the LL
problems are equipped with complex regularization terms.
Our main contributions are as follows:

• By decoupling hyperparameters from the regulariza-

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

tion, based on the value function approach, VF-iDCA
constructs a series of tractable convex subproblems.

• To our best knowledge, we establish the ﬁrst strict se-
quential convergence towards stationary solutions for
an important class of hyperparameter selection applica-
tions without assuming LLSC and LLS for LL tasks.

• We conduct experiments to verify our theoretical ﬁnd-
ings and evaluate VF-iDCA on various hyperparameter
selection BLPs. Our results compare favourably to
existing gradient-free and gradient-based methods.

In Table 2, we summarize our contributions to the theory of
convergence of BLP algorithms by comparing our results to
some existing results in the literature.

2. Value Function Based Difference-of-Convex

Algorithm with Inexactness

2.1. Hyperparameter Decoupling and Fully Convex LL

The LL problem in Eq. (1) is a penalized problem with
regularization term involving coupled UL and LL variables:

l(x(cid:48)) +

min
x(cid:48)

J
(cid:88)

i=1

λiPi(x(cid:48)).

(2)

We decouple the hyperparameter variables λ from the regu-
larization term by introducing a new variable r, which yields
the following constrained optimization problem:

min
x(cid:48)

l(x(cid:48)) s.t. Pi(x(cid:48)) ≤ ri, i = 1, . . . , J.

(3)

Denote their solution sets by Sp(λ) and Sc(r), respectively.
Thanks to the LL convexity w.r.t. x, there is a one-to-one
relationship between regularized problem Eq. (2) and con-
strained problem Eq. (3). That is, for any λ ≥ 0, there
is r ≥ 0 such that Sp(λ) = Sc(r) and vice versa. This
suggests working with the following BLP:

min
x,r∈RJ
+

L(x)

to Eq. (1), because Eq. (4) is a non-convex optimization
problem. Theoretical analysis of the relationship between
the local solutions is substantially more challenging than
theoretical analysis of the relationship between the global
solutions. Fortunately, we are able to overcome this chal-
lenge in Proposition 3.1 of Section 3.1, where we show that
local minima of Eq. (4) are also local minima of Eq. (1).

2.2. Single-level DC Reformulation and Algorithm

We now consider solving (4). The value function of the LL
problem governed by r is denoted by

v(r) := min {l(x) s.t. Pi(x) ≤ ri, i = 1, . . . , J} .

Throughout the paper, we assume that the following condi-
tion holds.
Assumption 2.1. For each r belonging to any open subset
of RJ
+, F(r) := {x s.t. Pi(x) ≤ ri, i = 1, . . . , J} (cid:54)= ∅ and
l(x) is bounded below.

Thanks to full convexity, v(r) induced by a partial minimiza-
tion is convex and locally Lipschitz continuous around every
>0 := {r ∈ RJ |rj > 0} (see e.g. Lemma 3 in Ye
point in RJ
et al. 2021). Using the value function, we now reformulate
BLP (4) as the following DC program:

min
x,r∈RJ
+

L(x)

(5)

s.t.

l(x) − v(r) ≤ 0, Pi(x) ≤ ri, i = 1, . . . , J.

We next propose VF-iDCA to solve BLP (4). By using the
main idea of DCA which linearizes the concave part of the
DC structure, we propose a sequential convex programming
scheme as follows. Given a current iteration (xk, rk) for
each k, solving the LL problem parameterized by rk

min
x

l(x) s.t. Pi(x) ≤ rk

i , i = 1, . . . , J,

(6)

leads to a solution ˜xk ∈ Sc(rk) and a corresponding Karush-
Kuhn-Tucker (KKT) multiplier γk ∈ M(˜xk, rk), where
M(x, r) denotes the set of KKT multipliers of LL problem

x(cid:48)

s.t. x ∈ arg min

{l(x(cid:48)) s.t. Pi(x(cid:48)) ≤ ri, i = 1, . . . , J}.
(4)
In the section 2.2, we will use the fact that the LL of Eq. (4)
is fully convex w.r.t the joint variable (x, r) to rewrite Eq. (4)
as a single-level problem and design an algorithm to solve
it.

However, we must ask: is Eq. (4) equivalent to Eq. (1)? It is
relatively straightforward to show that their global solutions
coincide, using the equivalence between their LL problems.
However, just because their global solutions coincide, does
not necessarily mean their local solutions coincide. If we
are to work with Eq. (4) in place of Eq. (1), then it is crit-
ical that the local solutions to Eq. (4) are local solutions

M(x, r) :=

γ ∈ RJ

+ | 0 ∈ ∂l(x) +

(cid:110)

J
(cid:88)

i=1

γi∂Pi(x),

γi(Pi(x) − ri) = 0, i = 1, . . . , J

(cid:111)
.

Then by sensitivity analysis, we have −γk ∈ ∂v(rk); see
Proposition 3.3. Compute zk+1 := (xk+1, rk+1) as an
approximate minimizer of the strongly convex subproblem
ρ
2

φk(x, r) :=L(x) +

(cid:107)z − zk(cid:107)2

min
x,r∈RJ
+

+ αk max
i=1,...,J

{0, Vk(x, r), Pi(x) − ri},

(7)

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

where ρ > 0, and αk represents the adaptive penalty param-
eter, z := (x, r), zk := (xk, rk) and

0, λ ∈ RJ

+ such that

Vk(x, r) := l(x) − l(˜xk) + (cid:104)γk, r − rk(cid:105).

0 ∈ ∂L(¯x) + η∂l(¯x) +

J
(cid:88)

i=1

λi∂Pi(¯x),

Denoting Σ := Rn × RJ
for choosing zk+1:

+, we introduce an inexact condition

dist(0, ∂φk(zk+1)+NΣ(zk+1)) ≤

√

2
2

ρ(cid:107)zk−zk−1(cid:107), (8)

where dist(x, Ω) is the distance from x to Ω and NΣ denotes
the normal cone to Σ. Using above constructions and letting

tk+1 = max
i=1,...,J

{0, Vk(xk+1, rk+1), Pi(xk+1) − rk+1

i

},

(9)

we are ready to present VF-iDCA in Algorithm 1.

Algorithm 1 VF-iDCA
1: Take an initial point (x0, r0) ∈ Rn × RJ

+; cα, δα > 0;
an initial penalty parameter α0 > 0; tolerance tol > 0.

2: for k = 0, 1, . . . do
3:

Solve LL problem Eq. (6). Find ˜xk ∈ Sc(rk) and a
KKT multiplier γk.
Solve problem Eq. (7) up to tolerance in (8). Find an
approximate solution zk+1 = (xk+1, rk+1).
Stopping test: Stop if max{(cid:107)zk+1 − zk(cid:107), tk+1} <
tol.
Adaptive penalty parameter update: Set

4:

5:

6:

αk+1 =




αk + δα,

if max

(cid:110)

αk,

(cid:111)

1
tk+1

<

cα
∆k+1 ,



αk,

otherwise,

(10)
for tk+1 deﬁned in (9) and ∆k+1 := (cid:107)zk+1 − zk(cid:107).

7: end for

Remark 2.1. When l(x) is differentiable with a Lipschitz
continuous gradient, we can derive a linearized version of
VF-iDCA where we not only linearize the concave part but
also the convex smooth part of the DC structure.

3. Theoretical Investigations

With the purpose of deriving a provably convergent algo-
rithm for BLP Eq. (1) without LLSC and LLS, we involve
two signature features in our algorithmic design, i.e., hy-
perparameter decoupling and single-level DC reformula-
tion. In this section, we provide convergence analysis of the
proposed VF-iDCA towards “good” quality solutions, i.e.
towards KKT stationarity solutions, deﬁned as follows.

λ ∈ −η∂v(¯r) + NRJ

+

(¯r), λi (Pi(¯x) − ¯ri) = 0, i = 1, . . . , J.

3.1. Solutions Recovery for Hyperparameter

Decoupling

Before analyzing the convergence properties of Algorithm
1, we ﬁrst need to justify the validity of the hyperparame-
ter decoupling, i.e. we need to show that BLP Eq. (1) is
equivalent to BLP Eq. (4).

The following lemmas show a one-to-one relationship be-
tween the LL problems Eq. (2) and Eq. (3).
Lemma 3.1. Let x ∈ Sp(λ) with λ ∈ RJ
where r = P (x) ∈ RJ
+.
Lemma 3.2. For any x ∈ Sc(r) with r ∈ RJ
λ ∈ M(x, r). Then x ∈ Sp(λ).

+. Then x ∈ Sc(r)

+, suppose

Note that the nonemptyness of the multiplier set M(x, r)
can be acheived by imposing certain constraint qualiﬁca-
tions such as the Slater condition.

The next theorem shows that the x components of globally
optimal solutions for BLPs Eq. (1) and Eq. (4) coincide. We
denote by P (x) := (P1(x), . . . , PJ (x)).
Theorem 3.1 (Global solutions recovery). If (¯x, ¯r) ∈ Rn ×
RJ
+ is a global optimal solution of problem Eq. (4) and
¯λ ∈ M(¯x, ¯r), then (¯x, ¯λ) is a global optimal solution of
BLP Eq. (1). Conversely, suppose there exists a dense subset
D of Rn × RJ
+ such that M(x, r) (cid:54)= ∅ for all (x, r) ∈ D.
Let (¯x, ¯λ) be a global optimal solution of problem Eq. (1).
Then (¯x, ¯r) with ¯r := P (¯x) is a global optimal solution of
BLP Eq. (4).

Theorem 3.1 says that the global solutions of the fully con-
vex BLP Eq. (4) are the same as the global solutions of the
original BLP Eq. (1).

We now ask an arguably more important question: are the
local solutions of BLP Eq. (4) the same as the local solutions
of BLP Eq. (1)? First, we show in the following proposition
that any local optimal solution of BLP Eq. (4) must be
locally optimal for BLP Eq. (1).
Proposition 3.1 (Local solution recovery-I). If (¯x, ¯r) is a
local solution of BLP Eq. (4) with ¯r = P (¯x) and ¯λ ∈
M(¯x, ¯r), then (¯x, ¯λ) is a local solution of BLP Eq. (1).

This is the key result that justiﬁes our choice in this paper
to solve the tractable fully convex BLP Eq. (4) in place of
the substantially more difﬁcult BLP Eq. (1).

Deﬁnition 3.1. A feasible point (¯x, ¯r) of problem Eq. (5)
is a KKT stationary solution if there exist multipliers η ≥

The reverse direction is less practically relevant, because
we are unlikely to attempt to solve BLP Eq. (4) by solving

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

the more difﬁcult BLP Eq. (1). Nevertheless, the following
result provides conditions under which local solutions to
BLP Eq. (1) are local solutions to BLP Eq. (4).
Proposition 3.2 (Local solution recovery-II). Suppose that
there exists a dense subset D of Rn × RJ
+ such that
M(x, r) (cid:54)= ∅ for all (x, r) ∈ D. Let (¯x, ¯λ) be a local
optimal solution of BLP Eq. (1), locally w.r.t. x and globally
w.r.t. λ. Then (¯x, ¯r) with ¯r := P (¯x) is an optimal solution
of BLP Eq. (4) locally w.r.t. x and globally w.r.t. r.
Furthermore, let (¯x, ¯λ) be a locally optimal solution of BLP
Eq. (1) and ¯r := P (¯x).
If there exist a bounded set Λ
and (cid:15) > 0 such that Λ ∩ M(x, r) (cid:54)= ∅ for any (x, r) ∈
D ∩ B(cid:15)(¯x, ¯r), M(¯x, ¯r) is a singleton, then (¯x, ¯r) must be
locally optimal for BLP Eq. (4).
Remark 3.2. If both l, P are differentiable, then the exis-
tence of dense set and the uniqueness of the multipliers can
be simply implied by the linear independence of the gradient
vectors {∇Pi(¯x)}J

i=1.

3.2. Sequential Convergence Towards Stationary

Solutions

Assumption 3.1. Throughout this section, we assume the
following conditions.

(a) For all the rk generated by the algorithm, the lower

level solution set Sc(rk) is nonempty.

(b) For all the rk and ˜xk ∈ Sc(rk) generated by the algo-

rithm, M(˜xk, rk) (cid:54)= ∅.

Under Assumption 2.1, by Lemma 3 in Ye et al. (2021),
the value function is locally Lipschitz continuous in RJ
>0.
By Theorem 3 in Ye et al. (2021), we have a characteriza-
tion for the subdifferential of v(r) in the sense of convex
analysis (Rockafellar, 2015).
Proposition 3.3. For any r ∈ RJ

+, x ∈ Sc(r), we have that

∂v(r) ⊇ −M(x, r).

In addition, if the Slater condition holds for the LL problem,
then the equality holds.

The subsequential convergence of VF-iDCA towards station-
ary solutions follows from Theorem 1 in Ye et al. (2021).
Theorem 3.2. Suppose that L(x) is bounded below and the
sequences {zk} and {αk} generated by the VF-iDCA are
bounded. Then any accumulation point (¯x, ¯r) of {zk} is a
KKT stationary solution of problem Eq. (5) provided that
¯r ∈ RJ

>0.

Next, as the main contribution of this part, we show that the
subsequential convergence can be further enhanced to se-
quential convergence under the Kurdyka-Łojasiewicz prop-
erty (Attouch & Bolte, 2009; Attouch et al., 2010; 2013;

Bolte et al., 2014). Let η ∈ [0, +∞] and Φη denote the
class of all concave and continuous functions ϕ : [0, η) →
[0, +∞) satisfying the conditions: (a) ϕ(0) = 0, (b) ϕ is
C 1 on (0, η) and continuous at 0, (c) ϕ(cid:48)(s) > 0 for all
s ∈ (0, η).

Deﬁnition 3.2 (Kurdyka-Łojasiewicz property). Let σ :
Rd → (−∞, +∞] be proper and lower semicontinuous.
The function σ is said to have the Kurdyka-Łojasiewicz (KL)
property at ¯y ∈ dom ∂σ := {y ∈ Rd | ∂σ(y) (cid:54)= ∅}, if
there exist η ∈ (0, +∞], a neighborhood Y of ¯y and a func-
tion ϕ ∈ Φη, such that for all y ∈ Y ∩ {y ∈ Rd | σ(¯y) <
σ(y) < σ(¯y) + η}, the following inequality holds

ϕ(cid:48)(σ(y) − σ(¯y))dist(0, ∂σ(y)) ≥ 1.

If σ satisfy the KL property at each point of dom ∂σ then σ
is called a KL function.

In addition, given the KL property around any points in a
compact set, the uniformized KL property holds; see Lemma
6 in Bolte et al. (2014).

Lemma 3.3 (Uniformized KL property). Given a compact
set C and a proper and lower semicontinuous function σ :
Rd → (−∞, +∞], suppose that σ is constant on C and
satisﬁes the KL property at each point of C. Then, there
exist (cid:15), η and ϕ ∈ Φη such that for all ¯y ∈ C and y ∈ {y ∈
Rd | dist(y, C) < (cid:15), σ(¯y) < σ(y) < σ(¯y) + η}, it holds

ϕ(cid:48)(σ(y) − σ(¯y))dist(0, ∂σ(y)) ≥ 1.

Inspired by Liu et al. (2019b), we deﬁne the following merit
function for convergence analysis:

Eα(z, z0, γ) :=L(x) +

ρ
4

(cid:107)z − z0(cid:107)2 + δΣ(z)

+ α max
i=1,...,J

{0, l(x) + (cid:104)γ, r(cid:105) + v∗(−γ), Pi(x) − ri},

where z := (x, r), z0 := (x0, r0), δΣ(z) is the indicator
function of Σ and v∗ denotes the conjugate function of v,
that is, v∗(ξ) := supr{(cid:104)ξ, r(cid:105) − v(r)}.
Let {(xk, rk)} be iterates generated by VF-iDCA. Before
we can analyze the sequential convergence of {(xk, rk)},
we need the sufﬁcient decrease property and the relative
error condition of the merit function Eα, which are summa-
rized in the following lemma.
Lemma 3.4. Let {(xk, rk)} be iterates generated by VF-
iDCA, then zk := (xk, rk) satisﬁes

Eαk (zk, zk−1, γk−1)
≥ Eαk (zk+1, zk, γk) +

ρ
4

(cid:107)zk+1 − zk(cid:107)2,

and

dist (cid:0)0, ∂Eαk (zk+1, zk, γk)(cid:1)
√

≤

ρ(cid:107)zk − zk−1(cid:107) + (αk + ρ)(cid:107)zk+1 − zk(cid:107).

2
2

(11)

(12)

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Theorem 3.3. Suppose that L(x) is bounded below and
the sequences {zk} and {αk} generated by VF-iDCA are
bounded, the merit function Eα is a KL function and there
exists δ > 0 such that rk
i ≥ δ for all k and i = 1, . . . , J.
Then the sequence {zk} converges to a KKT stationary
solution of problem Eq. (5).

3.3. Further Justiﬁcation of Validity in Applications

To further justify the validity of our sequential convergence
theory in real-world applications, we next discuss the con-
ditions, especially the KL property imposed on the merit
function Eα in Theorem 3.3. First, part (b) of Assump-
tion 3.1 holds automatically, for example if the Slater con-
dition holds, i.e., there exists x0 such that Pi(x0) < rk
i for
all i = 1, . . . , J. Thus, a large class of functions automati-
cally satisﬁes the KL property, see e.g. Bolte et al. (2010),
Attouch et al. (2010), Attouch et al. (2013), and Bolte et al.
(2014). As shown in Bolte et al. (2007a) and Bolte et al.
(2007b), any semi-algebraic function meets the KL property.
Deﬁnition 3.3 (Semi-algebraic sets and functions). A subset
S ⊆ Rd is called a real semi-algebraic set if there exists a
ﬁnite number of real polynomial functions gij, hij : Rd →
R such that

et al. (2013), and Bolte et al. (2014). In particular, all the
functions involved in our applications in the experiments
section are semi-algebraic.

4. Extension to General Setting

Our valued function based DC algorithm as well as its con-
vergence analysis can be straightforwardly extended to BLP
in a more general setting with LL constraints.

min
x∈X,u∈U,λ∈RJ
+

L(x, u)

(cid:26)

s.t. x ∈ argmin

x(cid:48)∈X

l(x(cid:48), u) +

J
(cid:88)

i=1

λiPi(x(cid:48), u),

s.t. g(x(cid:48), u) ≤ 0

(cid:27)

,

(13)
where X ⊆ Rn, U ⊆ Rd are closed convex sets, gi(i =
1, . . . , m), L, l, Pi : Rn × Rd → R+(i = 1, . . . , J) are
convex functions deﬁning on an open convex set containing
X×U and g(x, u) := (g1(x, u), . . . , gJ (x, u)). We provide
a detailed description of VF-iDCA for BLP Eq. (13) in the
Supplemental Material.

p
(cid:91)

q
(cid:92)

S =

{y ∈ Rd | gij(y) = 0, hij(y) < 0}.

5. Experiments

j=1

i=1

A function h : Rd → (−∞, +∞] is called semi-algebraic
if its graph {(y, s) ∈ Rd+1 | h(y) = s} is a semi-algebraic
subset of Rd+1.
Lemma 3.5. Let σ : Rd → (−∞, +∞] be proper and
lower semicontinuous. If σ is semi-algebraic then it satisﬁes
the KL property at any point of its domain.

The class of semi-algebraic functions is closed under vari-
ous operations, see, e.g., (Attouch et al., 2010; 2013; Bolte
et al., 2014). In particular, the indicator functions of semi-
algebraic sets, ﬁnite sum and product of semi-algebraic
functions, composition of semi-algebraic functions and par-
tial minimization of semi-algebraic function over semi-
algebraic set are all semi-algebraic functions. Applying
this to BLP Eq. (1), when the LL functions l(x) and P (x)
are both semi-algebraic, v(r) is a semi-algebraic function.
Moreover, if L(x) is a semi-algebraic function, the merit
function Eα is also semi-algebraic and hence a KL function.
Theorem 3.4. Assume that L(x), l(x) and P (x) are semi-
algebraic functions. Suppose that {zk := (xk, rk)} and
{αk} generated by VF-iDCA are bounded, L(x) is bounded
below and there exists δ > 0 such that rk
i ≥ δ for all k
and i = 1, . . . , J. Then {zk} converges to a KKT point of
problem Eq. (5).

A wide range of functions appearing in applications are
semi-algebraic, see, e.g., Attouch et al. (2010), Attouch

In this section we will compare the performance of our
VF-iDCA to widely-used competitors in hyperparameter
optimization for a number of machine learning algorithms.
Detailed descriptions of these problems are presented in
the Supplemental Material. All algorithms were imple-
mented in Python and the software package used for repro-
duce our experiments is available at https://github.
com/SUSTech-Optimization/VF-iDCA. The com-
petitors are as follows:

• Implicit Differentiation: We consider IGJO in Feng
& Simon (2018) and IFDM in Bertrand et al. (2020).

• Grid Search: We perform a brute-force grid search

over a 10× 10 uniformly-spaced grid.

• Random Search: We apply uniform random sampling

100 times at each direction of hyperparameters.

• TPE: Tree-structured Parzen Estimator approach
(Bergstra et al., 2013) is a Bayesian optimization
method based on Gaussian mixture models which can
handle a large number of hyperparameters.

All experiments run on a computer with Intel(R) Core(TM)
i9-9900K CPU @ 3.60GHz and 16.00 GB memory. We
solve the strongly convex subproblem (7) at each iteration
of VF-iDCA using the CVXPY package. IGJO and IFDM
are implemented using code from https://github.
com/jjfeng/nonsmooth-joint-opt and https:

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

//github.com/QB3/sparse-ho, respectively. We
only use IFDM for elastic net, as its code can only deal
with elastic net among our tested problems. Each subprob-
lem in grid search and random search is solved using the
CVXPY package. TPE is implemented using code from
https://github.com/hyperopt/hyperopt and
its subproblem is solved using the CVXPY package. When-
ever used, the CVXPY package is applied with the open
source solvers ECOS and SCS only.

As for the paramters δα and cα in VF-iDCA, we used δα = 5
for all the experiments, and we adopted cα = 0.1 in sparse
group lasso while we used cα = 1 for the other applications.
When δα is too small, the algorithm may converge slower
since the algorithm takes more steps to reach the exact
penalty. However, in our experience, the performance of
Algorithm 1 is not sensitive to the choice of δα. The choice
of cα is more important. We tested various ratios for ∆k and
tk to adjust cα. When cα is large, the updating frequency of
the penalty parameter will be high, which may slow down
the convergence speed.

5.1. Numerical experiments on synthetic data

We consider hyperparameter selection for elastic net and
variations on sparse group lasso, low-rank matrix comple-
tion, and support vector machines previously explored in
Feng & Simon (2018) and Kunapuli et al. (2008); see Sec-
tion C of the Supplemental Materials for details. Detailed
descriptions of the synthetic data generation settings are in
Section D of the Supplemental Materials.

5.1.1. ELASTIC NET

The numerical results on elastic net averaged over 30 data
sets are in Table 3. Overall, VF-iDCA attains the highest-
quality solutions relatively quickly. In the ﬁrst two settings,
the gradient-based methods (IGJO/IFDM and VF-iDCA)
perform better than the gradient-free methods. Furthermore,
our VF-iDCA achieves signiﬁcantly lower validation and
test errors than all other methods in the third setting, where
p = 2500. This suggests that our method is especially
competitive when the dimension of the problem is large.

5.1.2. SPARSE GROUP LASSO

Table 4 records the numerical results of sparse group lasso
averaged over 30 repetitions. Not only are the solutions of
VF-iDCA are better than the gradient free methods, they
are also better than the gradient-based IGJO. Furthermore,
VF-iDCA is much faster than IGJO.

5.1.3. LOW-RANK MATRIX COMPLETION

For this problem, we present the numerical results on 60×60
matrices in Table 5.

Table 3. Elastic net problems on synthetic data.
Here,
|Itr|; |Ival|; |Itest|, and p represent the number of training examples,
the number of validation examples, the number of test examples,
and the number of features, respectively.

Settings

Method

Time

Val. Err.

Test Err.

|Itr| = 100
|Ival| = 20
|Itest| = 250
p = 250

|Itr| = 100
|Ival| = 100
|Itest| = 250
p = 250

|Itr| = 100
|Ival| = 100
|Itest| = 100
p = 2500

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

3.10 ± 0.44
3.55 ± 0.58
5.41 ± 0.75
2.04 ± 1.46
1.33 ± 0.55
0.91 ± 0.19

3.17 ± 0.43
5.29 ± 0.60
5.40 ± 0.84
2.42 ± 1.30
1.30 ± 0.41
1.37 ± 0.29

19.05 ± 1.63
35.42 ± 3.55
32.17 ± 7.40
16.12 ± 40.95
4.38 ± 2.53
19.97 ± 5.17

6.16 ± 2.35
5.98 ± 2.24
6.05 ± 2.30
4.43 ± 1.77
4.41 ± 0.96
1.95 ± 0.81

6.51 ± 1.53
6.44 ± 1.53
6.44 ± 1.53
4.71 ± 1.32
4.78 ± 1.12
3.04 ± 0.74

7.95 ± 1.10
7.90 ± 1.09
7.89 ± 1.11
7.99 ± 1.18
7.97 ± 0.83
1.61 ± 1.85

6.68 ± 1.16
6.67 ± 1.15
6.77 ± 1.04
5.13 ± 1.37
4.77 ± 1.46
3.99 ± 0.69

6.82 ± 1.10
6.77 ± 1.14
6.76 ± 1.06
4.88 ± 1.30
4.61 ± 1.12
3.58 ± 0.60

8.54 ± 0.81
8.52 ± 0.79
8.60 ± 0.87
8.41 ± 0.86
8.53 ± 1.53
5.10 ± 1.07

Table 4. Sparse group lasso problems on synthetic data. Recall
that #λ is the dimension of hyperparameters, p is the number of
features and M is the number of groups. Each group contains
p/M features.

Settings

Method

p = 600
M = 30

p = 600
M = 300

p = 1200
M = 300

Grid
Random
TPE
IGJO
VF-iDCA

#λ

2
31
31
31
31

2
Grid
301
Random
301
TPE
IGJO
301
VF-iDCA 301

2
Grid
301
Random
301
TPE
301
IGJO
VF-iDCA 301

Time

Val. Err.

Test Err.

30.38 ± 1.82
28.54 ± 1.51
47.07 ± 4.01
69.62 ± 47.76
8.13 ± 1.20

20.84 ± 1.04
18.94 ± 1.09
76.82 ± 2.55
160.85 ± 71.50
56.73 ± 92.48

87.20 ± 5.85
73.75 ± 4.28
117.07 ± 5.66
98.35 ± 47.47
23.41 ± 1.31

42.45 ± 7.67
39.27 ± 7.32
35.69 ± 5.92
30.16 ± 7.41
0.01 ± 0.00

41.88 ± 7.64
43.92 ± 8.77
39.22 ± 6.26
20.37 ± 4.46
19.61 ± 8.33

44.56 ± 7.33
43.00 ± 8.83
40.59 ± 6.67
39.28 ± 6.56
38.50 ± 6.00

44.90 ± 7.02
47.90 ± 8.55
42.93 ± 8.00
38.52 ± 6.78
33.55 ± 4.71

49.56 ± 10.76
53.65 ± 12.03
45.94 ± 9.30
20.70 ± 4.70
17.90 ± 3.47

51.85 ± 12.90
55.84 ± 14.25
51.67 ± 12.29
38.90 ± 7.20
36.90 ± 7.48

5.2. Application to real data

In this part, we will conduct numerical experiments on real
datasets. All datasets are from the LIBSVM repository
(Chang & Lin, 2011)1. For each dataset, we will repeat the
experiment 30 times. For each repetition of experiments,
we applied a random shufﬂe to the original dataset before
partition, and ran all the algorithms on the same partition.

5.2.1. ELASTIC NET

This section we will apply our method to choose hyperpa-
rameters for the elastic net in real data. We will test on three
moderate scale datasets, including gisette (Guyon et al.,
2004), duke breast-cancer (West et al., 2001) and sensit
(Duarte & Hu, 2004). For datasets gisette, duke breast-

1http://www.csie.ntu.edu.tw/˜cjlin/

libsvmtools/datasets/.

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Table 5. Low-rank matrix completion problems on synthetic data.
#λ is the dimension of hyperparameters.

Method

#λ

Time

Val. Err.

Test Err.

2
Grid
25
Random
25
TPE
IGJO
25
VF-iDCA 25

20.67 ± 0.90
32.49 ± 1.84
35.05 ± 9.37
1268.65 ± 365.99
51.55 ± 10.43

0.71 ± 0.21
0.73 ± 0.21
0.68 ± 0.20
0.68 ± 0.21
0.06 ± 0.07

0.76 ± 0.20
0.80 ± 0.20
0.76 ± 0.18
0.72 ± 0.18
0.70 ± 0.16

cancer, sensit, we randomly extracted 50, 11, 25 examples
as training set, respectively; 50, 11, 25 examples as valida-
tion set, respectively; and the remaining for testing.

Table 6. Elastic net on madelon, gisette and duke breast-cancer.
p, |Itr|, |Ival|, |Itest| denote features number, the size of train data,
validation data and test data.

Dataset

gisette
p = 5000
|Itr| = 50
|Ival| = 50
|Itest| = 5900

duke breast-cancer
p = 7129
|Itr| = 11
|Ival| = 11
|Itest| = 22

sensit
p = 78823
|Itr| = 25
|Ival| = 25
|Itest| = 50

Method

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

Grid
Random
TPE
IGJO
IFDM
VF-iDCA

Time

Val. Err.

Test Err.

31.85 ± 3.51
55.04 ± 9.05
39.12 ± 6.96
6.10 ± 3.74
191.18 ± 202.30
5.39 ± 1.62

23.75 ± 4.42
39.68 ± 7.89
36.22 ± 22.16
3.51 ± 2.06
26.79 ± 73.39
12.23 ± 15.48

1.26 ± 0.11
1.29 ± 0.11
1.76 ± 0.09
0.49 ± 0.74
6.60 ± 3.49
0.16 ± 0.08

0.22 ± 0.04
0.22 ± 0.05
0.22 ± 0.05
0.24 ± 0.05
0.22 ± 0.02
0.00 ± 0.00

0.29 ± 0.06
0.28 ± 0.07
0.27 ± 0.07
0.30 ± 0.06
0.32 ± 0.05
0.00 ± 0.00

1.30 ± 0.82
1.52 ± 1.15
1.38 ± 0.96
0.52 ± 0.20
0.57 ± 0.10
0.23 ± 0.11

0.23 ± 0.01
0.23 ± 0.02
0.24 ± 0.02
0.25 ± 0.03
0.23 ± 0.03
0.19 ± 0.01

0.36 ± 0.08
0.41 ± 0.22
0.41 ± 0.24
0.37 ± 0.08
0.36 ± 0.08
0.29 ± 0.07

1.31 ± 0.48
1.45 ± 0.60
1.39 ± 0.55
0.61 ± 0.11
0.61 ± 0.23
0.51 ± 0.06

5.2.2. SUPPORT VECTOR MACHINE

We apply our experiments to moderate scale datasets, liver-
disorders, diabetes, breast-cancer, sonar, a1a (Asuncion
& Newman, 2007), w1a (Platt, 1999). On each dataset,
we choose hyperparameters for the SVM via 3/6-fold
cross-validation 30 times with the following data partition
rule. For a dataset which contains N examples, we ex-
tracted 3(cid:98)N/6(cid:99) examples as the training set used for cross-
validation Ω and took the remaining part as the test set Ωtest.
Since the bilevel program for cross-validated SVM involves
LL constraints, IGJO cannot be applied. We implement
VF-iDCA with two different tolerances. We denote the one
with tol = 0.01 by VF-iDCA, and the one with tol = 0.1
by VF-iDCA-t. We set ¯wlb = 10−6 and ¯wub = 10.

Table 7 and Table 8 record the numerical results of 3-fold
SVM and 6-fold SVM on 6 datasets with 30 repeatations,
respectively. Fig. 1 shows how the validation error and test
error rate changed over time for each algorithm. For such a
problem, VF-iDCA shows superiority in both computation
time and solution quality. By comparing the numerical
results of VF-iDCA and VF-iDCA-t, it can be observed that
VF-iDCA can obtain a satisfactory result with a moderate
algorithmic tolerance.

The boundedness assumption for the sequence of adaptive
penalty parameters, which plays an important role in the
convergence analysis, appeared to be satisﬁed in our exten-
sive experiments. In fact, as discussed in Ye et al. (2021),
if we relax the constraint in Eq. (5) to be l(x) − v(r) ≤ (cid:15)
for any small ﬁxed positive (cid:15), then the penalty parameters
are guaranteed to be bounded. Thus, a small modiﬁcation
of Step 3 of Algorithm 1 would solve this alternative “(cid:15)-
problem”, and results analogous to those in Section 3 would
hold without assuming boundedness. In the future, we may
also try to derive sufﬁcient conditions that directly guaran-
tee the boundedness of the adaptive penalty parameters for
Algorithm 1.

Table 7. 3-fold SVM on datasets liver-disorders scale, dia-
betes scale , breast-cancer scale, sonar, a1a, w1a. #λ is the dimen-
sion of hyperparameters.

Dataset

Method

#λ

Time

Val. Err.

Test Err.

liver-disorders scale
p = 5
|Ω| = 72
|Ωtest| = 73

diabetes scale
p = 8
|Ω| = 384
|Ωtest| = 384

breast-cancer scale
p = 14
|Ω| = 338
|Ωtest| = 345

sonar
p = 60
|Ω| = 102
|Ωtest| = 106

a1a
p = 123
|Ω| = 801
|Ωtest| = 804

w1a
p = 300
|Ω| = 1236
|Ωtest| = 1241

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

2
6
2
6
6
6

2
9
2
9
9
9

2
11
2
11
11
11

2
61
2
61
61
61

2
124
2
124
124
124

2
301
2
301
301
301

0.53 ± 0.01
0.56 ± 0.03
2.88 ± 1.16
0.37 ± 0.29
0.20 ± 0.06
0.09 ± 0.02

1.70 ± 0.11
1.83 ± 0.09
18.67 ± 7.84
6.64 ± 4.30
0.28 ± 0.03
0.18 ± 0.02

1.63 ± 0.04
1.80 ± 0.03
14.72 ± 6.02
9.14 ± 4.55
1.12 ± 0.59
0.14 ± 0.01

3.19 ± 0.10
3.23 ± 0.06
18.47 ± 6.84
40.77 ± 7.12
2.22 ± 1.50
0.48 ± 0.09

8.04 ± 0.15
8.62 ± 0.30
65.51 ± 16.24
176.59 ± 17.38
10.17 ± 5.47
1.10 ± 0.07

20.21 ± 0.82
20.44 ± 1.10
86.10 ± 28.19
299.62 ± 78.72
27.49 ± 7.31
4.87 ± 0.51

0.64 ± 0.08
0.58 ± 0.06
0.61 ± 0.07
0.65 ± 0.07
0.53 ± 0.09
0.53 ± 0.07

0.55 ± 0.03
0.56 ± 0.04
0.54 ± 0.03
0.55 ± 0.03
0.48 ± 0.02
0.48 ± 0.02

0.08 ± 0.01
0.09 ± 0.01
0.07 ± 0.01
0.09 ± 0.01
0.05 ± 0.01
0.09 ± 0.01

0.58 ± 0.08
0.54 ± 0.06
0.57 ± 0.08
0.64 ± 0.10
0.00 ± 0.00
0.03 ± 0.02

0.41 ± 0.02
0.41 ± 0.02
0.41 ± 0.02
0.42 ± 0.03
0.27 ± 0.02
0.27 ± 0.02

0.06 ± 0.01
0.06 ± 0.01
0.06 ± 0.01
0.06 ± 0.01
0.01 ± 0.00
0.01 ± 0.00

0.34 ± 0.06
0.32 ± 0.05
0.33 ± 0.06
0.34 ± 0.05
0.28 ± 0.05
0.27 ± 0.03

0.33 ± 0.05
0.30 ± 0.06
0.32 ± 0.06
0.29 ± 0.05
0.23 ± 0.01
0.23 ± 0.01

0.12 ± 0.06
0.08 ± 0.09
0.09 ± 0.10
0.10 ± 0.11
0.03 ± 0.01
0.04 ± 0.01

0.40 ± 0.12
0.34 ± 0.10
0.37 ± 0.13
0.41 ± 0.12
0.24 ± 0.04
0.24 ± 0.04

0.24 ± 0.02
0.22 ± 0.03
0.24 ± 0.02
0.23 ± 0.03
0.18 ± 0.01
0.17 ± 0.01

0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.02 ± 0.00
0.02 ± 0.00

6. Conclusion

In this paper, we develop VF-iDCA, a new algorithm for
hyperparameter tuning that can be applied for a single train-
ing/validation split or for cross-validation. This algorithm
heavily exploits the structure of the hyperparameter tuning
BLP in order to develop a tailored and effective solution.

Critically, unlike existing gradient-based BLP algorithms,
we are able to rigorously establish the ﬁrst result for se-
quential convergence towards stationary solutions in the
absence of LLSC and LLS simpliﬁcations. Furthermore, in
our numerical experiments we ﬁnd that VF-iDCA is able

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Figure 1. Comparison of the algorithms on SVM problem (validation error and test error versus time) for 6 datasets: liver-disorders scale,
diabetes scale, breast-cancer scale, sonar, a1a, w1a

Table 8. 6-fold SVM on datasets liver-disorders scale, dia-
betes scale , breast-cancer scale, sonar, a1a, w1a. #λ is the dimen-
sion of hyperparameters.

Dataset

Method

#λ

Time

Val. Err.

Test Err.

liver-disorders scale
p = 5
|Ω| = 72
|Ωtest| = 73

diabetes scale
p = 8
|Ω| = 384
|Ωtest| = 384

breast-cancer scale
p = 14
|Ω| = 336
|Ωtest| = 347

sonar
p = 60
|Ω| = 102
|Ωtest| = 106

a1a
p = 123
|Ω| = 798
|Ωtest| = 807

w1a
p = 300
|Ω| = 1236
|Ωtest| = 1241

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

Grid
Random
TPE2
TPE
VF-iDCA
VF-iDCA-t

2
6
2
6
6
6

2
9
2
9
9
9

2
11
2
11
11
11

2
61
2
61
61
61

2
124
2
124
124
124

2
301
2
301
301
301

0.78 ± 0.02
0.79 ± 0.04
6.88 ± 4.15
1.06 ± 1.04
0.33 ± 0.14
0.18 ± 0.03

3.18 ± 0.14
3.63 ± 0.21
51.85 ± 20.49
29.52 ± 13.13
0.56 ± 0.08
0.35 ± 0.01

3.38 ± 0.25
3.92 ± 0.29
38.69 ± 16.27
25.96 ± 12.95
2.01 ± 0.17
0.29 ± 0.08

6.57 ± 0.32
6.44 ± 0.28
58.19 ± 29.60
97.65 ± 31.37
0.92 ± 0.02
0.92 ± 0.02

17.60 ± 0.36
18.59 ± 0.42
161.68 ± 42.67
312.63 ± 60.60
63.01 ± 186.14
4.22 ± 0.37

44.29 ± 1.39
61.80 ± 2.91
190.04 ± 39.00
703.72 ± 82.75
97.50 ± 35.99
26.74 ± 3.67

0.63 ± 0.08
0.62 ± 0.07
0.62 ± 0.07
0.63 ± 0.08
0.41 ± 0.08
0.41 ± 0.08

0.55 ± 0.03
0.56 ± 0.03
0.55 ± 0.03
0.55 ± 0.03
0.43 ± 0.02
0.43 ± 0.02

0.08 ± 0.02
0.09 ± 0.02
0.07 ± 0.02
0.09 ± 0.01
0.03 ± 0.01
0.08 ± 0.01

0.59 ± 0.08
0.54 ± 0.06
0.57 ± 0.08
0.60 ± 0.07
0.00 ± 0.00
0.00 ± 0.00

0.40 ± 0.02
0.40 ± 0.02
0.40 ± 0.02
0.41 ± 0.03
0.18 ± 0.02
0.19 ± 0.01

0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.01
0.01 ± 0.00
0.01 ± 0.00

0.33 ± 0.07
0.31 ± 0.05
0.32 ± 0.06
0.34 ± 0.05
0.27 ± 0.05
0.27 ± 0.04

0.32 ± 0.05
0.31 ± 0.05
0.33 ± 0.05
0.27 ± 0.06
0.23 ± 0.01
0.23 ± 0.01

0.15 ± 0.06
0.07 ± 0.08
0.08 ± 0.09
0.11 ± 0.13
0.03 ± 0.01
0.04 ± 0.01

0.39 ± 0.11
0.32 ± 0.08
0.36 ± 0.12
0.39 ± 0.12
0.23 ± 0.04
0.23 ± 0.04

0.25 ± 0.02
0.21 ± 0.03
0.24 ± 0.02
0.23 ± 0.03
0.17 ± 0.01
0.17 ± 0.01

0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.02 ± 0.00
0.02 ± 0.00

to match or outperform the state-of-the-art hyperparameter
tuning approaches in terms of validation and test error for
a number of machine learning algorithms, especially when
the number of hyperparameters is large. In many cases,
VF-iDCA also enjoys considerable gains in computational
efﬁciency compared to the state-of-the-art.

The boundedness assumption of the sequence of adaptive
penalty parameters, which plays an important role in the
convergence analysis, appeared to be satisﬁed in our exten-
sive experiments. In fact, as discussed in Ye et al. (2021),
if we relax the constraint in Eq. (5) to be l(x) − v(r) ≤ (cid:15)
for any small ﬁxed positive (cid:15), then the penalty parameters
are guaranteed to be bounded. Thus, a small modiﬁcation
of Step 3 of Algorithm 1 would solve this alternative “(cid:15)-
problem”, and results analogous to those in Section 3 would
hold without assuming boundedness. In the future, we may
also try to derive sufﬁcient conditions that directly guaran-
tee the boundedness of the adaptive penalty parameters for
Algorithm 1.

Acknowledgements
The alphabetical order of the authors indicates the equal
contribution to the paper. This work is partially supported
by NSERC, the National Natural Science Foundation of
China (No. 11971220), the Shenzhen Science and Tech-
nology Program (No. RCYX20200714114700072), the
Guangdong Basic and Applied Basic Research Foundation
(No. 2022B1515020082) and the Paciﬁc Institute for the
Mathematical Sciences (PIMS).

Figure 2. Adaptive penalty parameter αk (deﬁned in Eq. (10))
of the VF-iDCA on SVM problem for 6 datasets:
liver-
disorders scale, diabetes scale, breast-cancer scale, sonar, a1a,
w1a

    0.50.60.70.80.91.0Validation errorliver-disorders_scale   0.50.60.70.80.91.0diabetes_scale   0.00.20.40.60.81.0breast-cancer_scaleVF-iDCAGrid SearchRandom SearchTPE   0.00.20.40.60.81.0sonar    0.20.40.60.81.0a1a    0.00.20.40.60.81.0w1a0.00.10.20.3Time (s)26283032343638Test error rate (%)0.000.250.500.751.00Time (s)242628303234360.000.250.500.751.00Time (s)1020300.00.51.01.52.0Time (s)2530354045500246Time (s)18202224051015Time (s)2.53.03.50102030405060123αkliver-disorders_scale (100%)01020304050601diabetes_scale (100%)01020304050601breast-cancer_scale (100%)0102030405060# iterations1αksonar (100%)0102030405060# iterations1a1a (90%)0102030405060# iterations1w1a (90%)Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

References

Clarke, F. H. Optimization and nonsmooth analysis. SIAM,

Asuncion, A. and Newman, D. UCI machine learning repos-

1990.

itory, 2007.

Attouch, H. and Bolte, J. On the convergence of the prox-
imal algorithm for nonsmooth functions involving ana-
lytic features. Mathematical Programming, 116(1):5–16,
2009.

Attouch, H., Bolte, J., Redont, P., and Soubeyran, A. Proxi-
mal alternating minimization and projection methods for
nonconvex problems: An approach based on the kurdyka-
łojasiewicz inequality. Mathematics of Operations Re-
search, 35(2):438–457, 2010.

Attouch, H., Bolte, J., and Svaiter, B. F. Convergence of
descent methods for semi-algebraic and tame problems:
proximal algorithms, forward–backward splitting, and
regularized gauss–seidel methods. Mathematical Pro-
gramming, 137(1):91–129, 2013.

Bergstra, J., Yamins, D., and Cox, D. Making a science of
model search: Hyperparameter optimization in hundreds
of dimensions for vision architectures. In ICML, pp. 115–
123. PMLR, 2013.

Bertrand, Q., Klopfenstein, Q., Blondel, M., Vaiter, S.,
Implicit differentiation
Gramfort, A., and Salmon, J.
of lasso-type models for hyperparameter optimization. In
ICML, pp. 810–821. PMLR, 2020.

Bertrand, Q., Klopfenstein, Q., Massias, M., Blondel, M.,
Vaiter, S., Gramfort, A., and Salmon, J. Implicit differen-
tiation for fast hyperparameter selection in non-smooth
convex learning. arXiv preprint arXiv:2105.01637, 2021.

Bolte, J., Daniilidis, A., and Lewis, A. The łojasiewicz in-
equality for nonsmooth subanalytic functions with appli-
cations to subgradient dynamical systems. SIAM Journal
on Optimization, 17(4):1205–1223, 2007a.

Bolte, J., Daniilidis, A., Lewis, A., and Shiota, M. Clarke
subgradients of stratiﬁable functions. SIAM Journal on
Optimization, 18(2):556–572, 2007b.

Bolte, J., Daniilidis, A., Ley, O., and Mazet, L. Charac-
terizations of łojasiewicz inequalities: subgradient ﬂows,
talweg, convexity. Transactions of the American Mathe-
matical Society, 362(6):3319–3363, 2010.

Bolte, J., Sabach, S., and Teboulle, M. Proximal alternating
linearized minimization for nonconvex and nonsmooth
problems. Mathematical Programming, 146(1):459–494,
2014.

Chang, C.-C. and Lin, C.-J. Libsvm: a library for sup-
port vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST), 2(3):1–27, 2011.

Cortes, C. and Vapnik, V. Support-vector networks. Ma-

chine learning, 20(3):273–297, 1995.

Duarte, M. F. and Hu, Y. H. Vehicle classiﬁcation in dis-
tributed sensor networks. Journal of Parallel and Dis-
tributed Computing, 64(7):826–838, 2004.

Feng, J. and Simon, N. Gradient-based regularization param-
eter selection for problems with nonsmooth penalty func-
tions. Journal of Computational and Graphical Statistics,
27(2):426–435, 2018.

Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML,
pp. 1126–1135. PMLR, 2017.

Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
Forward and reverse gradient-based hyperparameter opti-
mization. In ICML, pp. 1165–1173. PMLR, 2017.

Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil,
M. Bilevel programming for hyperparameter optimization
In ICML, pp. 1563–1572. PMLR,
and meta-learning.
2018.

Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S. On the
iteration complexity of hypergradient computation. In
ICML, pp. 3748–3758. PMLR, 2020.

Guyon, I., Gunn, S., Ben-Hur, A., and Dror, G. Result analy-
sis of the nips 2003 feature selection challenge. NeurIPS,
17, 2004.

Ji, K. and Liang, Y. Lower bounds and accelerated
arXiv preprint

algorithms for bilevel optimization.
arXiv:2102.03926v2, 2021.

Ji, K., Lee, J. D., Liang, Y., and Poor, H. V. Convergence of
meta-learning with task-speciﬁc adaptation over partial
parameters. NeurIPS, 33:11490–11500, 2020a.

Ji, K., Yang, J., and Liang, Y. Bilevel optimization:
Nonasymptotic analysis and faster algorithms. arXiv
preprint arXiv:2010.07962v2, 2020b.

Ji, K., Yang, J., and Liang, Y. Bilevel optimization: Con-
vergence analysis and enhanced design. In ICML, pp.
4882–4892. PMLR, 2021.

Ji, K., Liu, M., Liang, Y., and Ying, L. Will bilevel optimiz-
ers beneﬁt from loops. arXiv preprint arXiv:2205.14224,
2022.

Kunapuli, G. A bilevel optimization approach to machine

learning. Rensselaer Polytechnic Institute, 2008.

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Kunapuli, G., Bennett, K. P., Hu, J., and Pang, J.-S. Classi-
ﬁcation model selection via bilevel programming. Opti-
mization Methods & Software, 23(4):475–489, 2008.

Simon, N., Friedman, J., Hastie, T., and Tibshirani, R. A
sparse-group lasso. Journal of Computational and Graph-
ical Statistics, 22(2):231–245, 2013.

Sow, D., Ji, K., Guan, Z., and Liang, Y. A constrained opti-
mization approach to bilevel optimization with multiple
inner minima. arXiv preprint arXiv:2203.01123, 2022.

West, M., Blanchette, C., Dressman, H., Huang, E., Ishida,
S., Spang, R., Zuzan, H., Olson, J. A., Marks, J. R., and
Nevins, J. R. Predicting the clinical status of human breast
cancer by using gene expression proﬁles. Proceedings of
the National Academy of Sciences, 98(20):11462–11467,
2001.

Ye, J. J., Yuan, X., Zeng, S., and Zhang, J. Difference
of convex algorithms for bilevel programs with appli-
arXiv preprint
cations in hyperparameter selection.
arXiv:2102.09006, 2021.

Zou, H. and Hastie, T. Regression shrinkage and selection
via the elastic net, with applications to microarrays. Jour-
nal of the Royal Statistical Society: Series B (Statistical
Methodology), 67:301–20, 2003.

Liu, H., Simonyan, K., and Yang, Y. Darts: Differentiable

architecture search. In ICLR, 2019a.

Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J. A generic
ﬁrst-order algorithmic framework for bi-level program-
ming beyond lower-level singleton. In ICML, pp. 6305–
6315. PMLR, 2020.

Liu, R., Liu, X., Yuan, X., Zeng, S., and Zhang, J. A value-
function-based interior-point method for non-convex bi-
In ICML, pp. 6882–6892. PMLR,
level optimization.
2021a.

Liu, R., Liu, Y., Zeng, S., and Zhang, J. Towards gradient-
based bilevel optimization with non-convex followers and
beyond. NeurIPS, 34, 2021b.

Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J. A gen-
eral descent aggregation framework for gradient-based
bi-level optimization. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 2022.

Liu, T., Pong, T. K., and Takeda, A. A reﬁned convergence
analysis of pDCAe pdca e with applications to simultane-
ous sparse recovery and outlier detection. Computational
Optimization and Applications, 73(1):69–100, 2019b.

Lorraine, J. and Duvenaud, D.

Stochastic hyperpa-
rameter optimization through hypernetworks. CoRR,
abs/1802.09419, 2018.

Lorraine, J., Vicol, P., and Duvenaud, D. Optimizing mil-
lions of hyperparameters by implicit differentiation. In
AISTATS, pp. 1540–1552. PMLR, 2020.

Nichol, A., Achiam, J., and Schulman, J. On ﬁrst-order
meta-learning algorithms. CoRR, abs/1803.02999, 2018.

Pedregosa, F. Hyperparameter optimization with approxi-
mate gradient. In ICML, pp. 737–746. PMLR, 2016.

Platt, J. C. Fast training of support vector machines us-
ing sequential minimal optimization, advances in kernel
methods. 1999.

Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S.
Meta-learning with implicit gradients. NeurIPS, 32, 2019.

Rockafellar, R. T. Convex analysis. Princeton university

press, 2015.

Shaban, A., Cheng, C., Hatch, N., and Boots, B. Truncated
back-propagation for bilevel optimization. In AISTATS,
2019.

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

The supplemental materials are organized as follows. In
Appendix A, we present all the detailed proofs of the theo-
retical results given in Section 3. And in Appendix B, the
detailed description of the VF-iDCA for bilevel problem
in general setting is presented. Appendix C is devoted to
the illustration of hyperparameter decoupling for BLP ap-
plications. Additional information on how the numerical
experiments were run are given in Appendix D.

A. Detailed Proofs

A.1. Proof of Lemma 3.1

By convexity, x is an optimal solution for problem Eq. (2)
if and only if its ﬁrst-order optimality condition

0 ∈ ∂l(x) +

J
(cid:88)

i=1

λi∂Pi(x)

(14)

holds. Assuming the KKT condition for problem Eq. (3)

∃λ ∈ RJ

+ s.t. 0 ∈ ∂l(x) +

J
(cid:88)

i=1

λi∂Pi(x),

λi(Pi(x) − ri) = 0, i = 1, . . . , J,
Pi(x) − ri ≤ 0, i = 1, . . . , J,

(15)

holds, then x ∈ Sc(r). Since the difference between the
KKT conditions Eq. (14) and Eq. (15) is the complementary
slackness conditions λi(Pi(x) − ri) = 0, Pi(x) − ri ≤
0, i = 1, . . . , J, which holds automatically if P (x) = r, the
conclusion follows immediately.

A.2. Proof of Lemma 3.2

Since λ ∈ M(x, r), we have

0 ∈ ∂l(x) +

J
(cid:88)

i=1

λi∂Pi(x).

By convexity we have x ∈ Sp(λ).

A.3. Proof of Theorem 3.1

Problems Eq. (1) and Eq. (4) can be equivalently rewritten
in the following form

min L(x)

s.t. x ∈ Sp(λ), λ ∈ RJ
+,

min L(x)

s.t. x ∈ Sc(r), r ∈ RJ
+,

(16)

(17)

and

respectively.

(1) Suppose that (¯x, ¯r) ∈ Rn × RJ
+ is a global optimal solu-
tion of problem Eq. (17) and ¯λ ∈ M(¯x, ¯r). We want to show
that (¯x, ¯λ) is a global optimal solution of problem Eq. (16).
Let (x, λ) be a feasible solution of problem Eq. (16) which
means that x ∈ Sp(λ), λ ∈ RJ
+. By Lemma 3.1, x ∈ Sc(r)
with r = P (x) ∈ RJ
+ and hence (x, r) is a feasible solution
of problem Eq. (17). Hence by the optimality of (¯x, ¯r) to
problem Eq. (17), we have L(¯x) ≤ L(x). This shows that
(¯x, ¯λ) is a global optimal solution of problem Eq. (16).
(2) Suppose that (¯x, ¯λ) ∈ Rn × RJ
+ is a global optimal solu-
tion of problem Eq. (16). We want to show that (¯x, ¯r) with
¯r := P (¯x) is a global optimal solution of problem Eq. (17).
Since ¯x ∈ Sp(¯λ), by Lemma 3.1, ¯x ∈ Sc(¯r) with ¯r = P (¯x).
Hence (¯x, ¯r) is a feasible solution of problem Eq. (17). Let
(x, r) be a feasible solution of problem Eq. (17) which
means that x ∈ Sc(r), r ∈ RJ
+. As D is a dense sub-
set of X × RJ
+, there exists a sequence {(xk, rk)} ⊆ D
satisfying (xk, rk) → (x, r). Since M(xk, rk) (cid:54)= ∅ for
all (xk, rk) ∈ D, we can pick λk ∈ M(xk, rk). By
Lemma 3.2, xk ∈ Sp(λk) and hence (xk, λk) is a feasi-
ble solution of problem Eq. (16). By the optimality of (¯x, ¯λ)
to problem Eq. (16), we have L(¯x) ≤ L(xk). By taking
k → ∞ and the continuity of L, this shows that (¯x, ¯r) is a
global optimal solution of problem Eq. (17).

A.4. Proof of Proposition 3.1

Since (¯x, ¯r) is a local optimal solution of problem Eq. (17),
there is a (cid:15)0 > 0 such that ∀x ∈ Sc(r), r ∈ RJ
+, and
(x, r) ∈ B(cid:15)0 (¯x, ¯r),

L(¯x) ≤ L(x),

(18)

where B(cid:15)0(¯x, ¯r) denotes the closed ball centered at (¯x, ¯r)
with radius (cid:15)0. Let (x, λ) be a feasible solution of prob-
lem Eq. (16) which means that x ∈ Sp(λ), λ ∈ RJ
+. By
Lemma 3.1, x ∈ Sc(r) with r = P (x) ∈ RJ
+ and hence
(x, r) is a feasible solution of problem Eq. (17). Moreover
since P is continuous, we can ﬁnd 0 < (cid:15)1 < (cid:15)0 such that
as x ∈ B(cid:15)1(¯x), r = P (x) ∈ B(cid:15)0(¯r). Hence by Eq. (18),
the local optimality of (¯x, ¯r) to problem Eq. (17), we have
L(¯x) ≤ L(x). This shows that (¯x, ¯λ) is a local optimal
solution of problem Eq. (16).

A.5. Proof of Proposition 3.2
Since (¯x, ¯λ) is a local optimal solution of problem Eq. (16)
locally with respect to variable x and globally with respect to
variable λ, there is a (cid:15)0 > 0 such that ∀x ∈ Sp(λ), λ ∈ RJ
+,
and x ∈ B(cid:15)0(¯x),

L(¯x) ≤ L(x).
(19)
Since ¯x ∈ Sp(¯λ), by Lemma 3.1, ¯x ∈ Sc(¯r) with ¯r = P (¯x).
Hence (¯x, ¯r) is a feasible solution of problem Eq. (17). Let
(x, r) be a feasible solution of problem Eq. (17)) which

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

means that x ∈ Sc(r), r ∈ RJ
+. As D is a dense subset of
X × RJ
+, there exists a sequence {(xk, rk)} ⊆ D satisfying
(xk, rk) → (x, r). Since M(xk, rk) (cid:54)= ∅ for all (xk, rk) ∈
D, we can pick λk ∈ M(xk, rk). By Lemma 3.2, xk ∈
Sp(λk) and hence (xk, λk) is a feasible solution of problem
Eq. (16). Moreover suppose that x and {xk} lie in B(cid:15)0(¯x, ¯u).
Hence by (19), we have L(¯x) ≤ L(xk). Taking k → ∞
implies L(¯x) ≤ L(x). This shows that (¯x, ¯r) is a local
optimal solution of problem Eq. (17) locally with respect to
variable x and globally with respect to variable r.
Now suppose that M(¯x, ¯r) = {¯λ} is a singleton. Let (¯x, ¯λ)
be a local optimal solution of problem Eq. (16). Then there
is a (cid:15)1 > 0 such that ∀x ∈ Sp(λ), λ ∈ RJ
+ and (x, λ) ∈
B(cid:15)1(¯x, ¯λ),

L(¯x) ≤ L(x).

(20)

Let (x, r) be a feasible solution of problem Eq. (17) which
lies in B(cid:15)2(¯x, ¯r) for some 0 < (cid:15)2 < (cid:15)1. It implies that x ∈
Sc(r), r ∈ RJ
+ and (x, r) ∈ B(cid:15)2(¯x, ¯r). By the assumption
of the uniqueness of the multipliers, we can show through
proof by contradiction that when (cid:15)2 is sufﬁciently small,
for x ∈ B(cid:15)2 (¯x), Λ ∩ M(x, r) ⊆ B(cid:15)1(¯λ). We can ﬁnd a
sequence {(xk, rk)} ⊆ D ∩ B(cid:15)(¯x, ¯r) satisfying (xk, rk) →
(x, r). Since Λ ∩ M(xk, rk) (cid:54)= ∅ for all (xk, rk) ∈ D ∩
B(cid:15)(¯x, ¯r), we can pick λk ∈ Λ ∩M(xk, rk). We can assume
without loss of generality that (xk) ∈ B(cid:15)2 (¯x) and thus
λk ∈ B(cid:15)1(¯λ). By Lemma 3.2, xk ∈ Sp(λk) and hence
(xk, λk) is a feasible solution of problem Eq. (16). Hence
by Eq. (20), we have L(¯x) ≤ L(xk). Taking k → ∞ gives
us L(¯x) ≤ L(x). This shows that (¯x, ¯r) is a local optimal
solution of problem Eq. (17).

A.6. Proof of Lemma 3.4

For convenience, in the proof we denote by gk(z) :=

max
i=1,...,J

{0, Vk(z), Pi(x) − ri}. Then

Combining Eq. (21), Eq. (22) and Eq. (23), and taking into
account the fact that zk ∈ Σ, we obtain

φk(zk+1) ≤ Eαk (zk, zk−1, γk−1).

(24)

Again, as v∗ is the conjugate function of v, and −γk ∈
∂v(rk), there holds that

−v(rk) − (cid:104)γk, rk(cid:105) = v∗(−γk),

which implies

φk(zk+1) = Eαk (zk+1, zk, γk) +

ρ
4

(cid:107)zk+1 − zk(cid:107)2. (25)

Combining with Eq. (22) and Eq. (24) gives us Eq. (11).

It remains to prove Eq. (12). Since l(x) and Pi(x) − ri,
i = 1, . . . , J are all convex and continuous and thus regular,
by the calculus rule for the pointwise maximum (see e.g.
Proposition 2.3.12 in (Clarke, 1990)), we have that gk(z) is
regular and

∂gk(z) =
(cid:40)

η(∂l(x) × {γk}) +

J
(cid:88)

λi∂Pi(x) × {−λiei}

i=1
s.t. λi ∈ [0, 1], λi (Pi(x) − ri − gk(z)) = 0,
η ∈ [0, 1], η(l(x) − v(rk) + (cid:104)γk, r − rk(cid:105) − gk(z)) = 0,

gk(z)(1 − η −

J
(cid:88)

λi) = 0, η +

J
(cid:88)

(cid:41)

λi ≤ 1

,

i=1

i=1

(26)
where ei denotes the unit vector with the ith component
equal to 1.

Similarly, ˜g(z, γ) is regular and

∂z ˜g(z, γk) =
(cid:40)

φk(z) = L(x) +

ρ
2

(cid:107)z − zk(cid:107)2 + αkgk(z),

(21)

η(∂l(x) × {γk}) +

J
(cid:88)

i=1

λi∂Pi(x) × {−λiei}

where Vk(z) = l(x) − v(rk) + (cid:104)γk, r − rk(cid:105). By using the
same arguments as in the proof of Lemma 1 in (Ye et al.,
2021), we can have following decrease result on φk,

s.t. λi ∈ [0, 1], λi
η ∈ [0, 1], η(l(x) + (cid:104)γk, r(cid:105) + v∗(−γk) − ˜g(z, γk)) = 0,

(cid:0)Pi(x) − ri − ˜g(z, γk)(cid:1) = 0,

φk(zk+1) ≤ φk(zk) +

ρ
4

(cid:107)zk − zk−1(cid:107)2.

(22)

˜g(z, γk)(1 − η −

Next since v∗ is the conjugate function of v, by deﬁnition

− v(rk) ≤ (cid:104)γk−1, rk(cid:105) + v∗(−γk−1).

(23)

Denote by

˜g(z, γ) := max
i=1,...,J

{0, l(x) + (cid:104)γ, r(cid:105) + v∗(−γ), Pi(x) − ri}.

Then

Eα(z, z0, γ) := L(x) +

ρ
4

J
(cid:88)

i=1

λi) = 0, η +

(cid:41)

λi ≤ 1

.

J
(cid:88)

i=1

(27)

We also have

∂γ ˜g(zk+1, γ) = (cid:8)η(rk+1 − ∂v∗(−γ)) s.t. η ∈ [0, 1],
η (cid:0)l(x) + (cid:104)γ, rk+1(cid:105) + v∗(−γ) − ˜g(zk+1, γ)(cid:1) = 0(cid:9) .

(28)

Since v∗ is the conjugate function of v, and −γk ∈ ∂v(rk),
there holds that

(cid:107)z − z0(cid:107)2 + δΣ(z) + αk ˜g(z, γ).

−v(rk) − (cid:104)γk, rk(cid:105) = v∗(−γk).

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Hence

Vk(zk+1) = l(xk+1) − v(rk) + (cid:104)γk, rk+1 − rk(cid:105)
= l(xk+1) + (cid:104)γk, rk+1(cid:105) + v∗(−γk),

Next, using the fact that −γk ∈ ∂v(rk) which is equivalent
to rk ∈ ∂v∗(−γk), we have

∂Eαk (zk+1, zk, γk) (cid:51)





ek − ρ
ρ

2 (zk+1 − zk)

2 (zk − zk+1)
αkηk+1(rk+1 − rk)



 .

and thus

and

gk(zk+1) = ˜g(zk+1, γk),

Eq. (12) then follows immediately.

∂gk(zk+1) = ∂z ˜g(zk+1, γk).

(29)

A.7. Proof of Theorem 3.3

Since L(x) and gk(z) are all convex and continuous and
thus regular, according to the the subdifferential sum rule
we have (see e.g. Proposition 2.3.3 in Clarke (1990)), we
have

∂φk(z) = ∂L(x) × {0} + ρ(z − zk) + αk∂gk(z).

Similarly we have

∂zEαk (z, zk, γk) = ∂L(x) × {0} +

ρ
2

(z − zk)

+ NΣ(zk+1) + αk∂z ˜g(z, γk).

Hence by Eq. (29), we have

∂zEαk (zk+1, zk, γk)

= ∂φk(zk+1) + NΣ(zk+1) −

ρ
2

(zk+1 − zk).

(30)

By the partial subdifferential formula (see e.g. by Propo-
sition 2 in (Ye et al., 2021)) and Eq. (28), there exists
ηk+1 ∈ [0, 1] such that

∂Eαk (zk+1, zk, γk)

= ∂zEαk (zk+1, zk, γk) × ∂z0Eαk (zk+1, zk, γk)

×∂γEαk (zk+1, zk, γk)
= ∂zEαk (zk+1, zk, γk) × {

×∂γ ˜g(zk+1, γk)

⊇ ∂zEαk (zk+1, zk, γk) × {

ρ
2

ρ
2

(zk − zk+1)}

(zk − zk+1)}

×{αkηk+1(rk+1 − ∂v∗(−γk))}.

Since zk+1 is an approximate solution to problem Eq. (5)
satisfying inexact criteria Eq. (8), there exists a vector ek
such that

ek ∈ ∂φk(zk+1) + NΣ(zk+1)

satisfying

(cid:107)ek(cid:107) ≤

√

2
2

ρ(cid:107)zk − zk−1(cid:107).

It follows from Eq. (30) that

ek −

ρ
2

(zk+1 − zk) ∈ ∂zEαk (zk+1, zk, γk).

By the assumption that the adaptive penalty sequence {αk}
is bounded, αk = ¯α for all sufﬁciently large k and we can
assume without loss of generality that αk = ¯α for all k.
Since L(x) is assumed to be bounded below, E ¯α(z, z0, γ)
is also bounded below. Then, according to Lemma 3.4, we
have that limk→∞ (cid:107)zk+1 − zk(cid:107)2 = 0.

Let C denote the set of all limit points of the sequence
{(zk, zk−1, γk−1)}. Then we have that C is a closed set
and limk→∞ dist((zk, zk−1, γk−1), C) = 0. And by as-
sumption that there exists δ > 0 such that rk
i ≥ δ for all k
and all i = 1, . . . , J and Theorem 3.2, any accumulation
point of the sequence {zk} corresponds to those in the set C
is a KKT stationary point of problem Eq. (5). According to
Assumption 2.1, the value function is locally Lipschitz con-
tinuous around any point in RJ
>0. Then by the assumptions
that sequence {zk} is bounded, rk
i ≥ δ for some δ > 0
and −γk−1 ∈ ∂v(zk−1), we get the boundedness of the
sequence {γk} and thus C is a compact set.

Since by Lemma 3.4, E ¯α(zk, zk−1, γk−1) is decreas-
ing, E ¯α(z, z0, γ) is bounded below and continuous on
Σ and limk→∞ (cid:107)zk+1 − zk(cid:107) = 0, we have that
for any subsequence {(zl, zl−1, γl−1)} of the sequence
{(zk, zk−1, γk−1)},

¯E = lim
l→∞

E ¯α(zl, zl−1, γl−1) = lim
k→∞

E ¯α(zk, zk−1, γk−1),

and thus E ¯α is constant on C. We can assume that
E ¯α(zk, zk−1, γk−1) > ¯E for all k. Otherwise, if there
exists k > 0 such that E ¯α(zk, zk−1, γk−1) = ¯E, then by
the above equation and Lemma 3.4, we get (cid:107)zk+1 −zk(cid:107) = 0
when k is sufﬁciently large, which implies the convergence
of sequence {zk} and the conclusion follows immediately.

Since limk→∞ dist((zk, zk−1, γk−1), C) = 0 and
limk→∞ E ¯α(zk, zk−1, ζ k−1) = ¯E, for any (cid:15), η > 0, there
exists k0 such that dist((zk, zk−1, ζ k−1), C) < (cid:15) and
¯E < E ¯α(zk, zk−1, ζ k−1) < ¯E + η for k ≥ k0. Since
E ¯α(z, z0, γ) satisﬁes the Kurdyka-Łojasiewicz property at
each point in C, and E ¯α is a ﬁnite constant on C, we can
apply Lemma 3.3 to obtain a continuous concave function
ϕ such that for any k ≥ k0,
ϕ(cid:48)(E ¯α(zk, zk−1, γk−1) − ¯E)dist (cid:0)0, ∂E ¯α(zk, zk−1, γk−1)(cid:1)
≥ 1.

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Combining with Eq. (12) yields

Taking k → ∞ in the above inequality shows that

ϕ(cid:48) (cid:0)E ¯α(zk, zk−1, γk−1) − ¯E(cid:1)
(cid:32) √

(cid:33)

·

ρ(cid:107)zk−1 − zk−2(cid:107) + (¯α + ρ)(cid:107)zk − zk−1(cid:107)

≥ 1.

2
2

The concavity of ϕ and Eq. (11) implies

ϕ(cid:48)(E ¯α(zk, zk−1, γk−1) − ¯E) ·
≤ ϕ(cid:48)(E ¯α(zk, zk−1, γk−1) − ¯E)

ρ
4

(cid:107)zk+1 − zk(cid:107)2

· (cid:0)E ¯α(zk, zk−1, γk−1) − E ¯α(zk+1, zk, γk)(cid:1)

≤ ϕ (cid:0)E ¯α(zk, zk−1, γk−1) − ¯E(cid:1)
− ϕ (cid:0)E ¯α(zk+1, zk, γk) − ¯E(cid:1) .

Combining the above two inequalities, we obtain

(cid:32) √

2
2

4
ρ

ρ(cid:107)zk−1 − zk−2(cid:107) + (¯α + ρ)(cid:107)zk − zk−1(cid:107)

(cid:33)

(cid:34)
ϕ (cid:0)E ¯α(zk, zk−1, γk−1) − ¯E(cid:1)

·

− ϕ (cid:0)E ¯α(zk+1, zk, γk) − ¯E(cid:1)

(cid:35)

≥ (cid:107)zk+1 − zk(cid:107)2.

Multiplying both sides of this inequality by 4 and taking the
square root, and by 2ab ≤ a2 + b2, we have

4(cid:107)zk+1 − zk(cid:107) ≤ (cid:107)zk − zk−1(cid:107) + (cid:107)zk−1 − zk−2(cid:107)

+

16(¯α + ρ)
ρ

(cid:104)

ϕ (cid:0)E ¯α(zk, zk−1, γk−1) − ¯E(cid:1)

− ϕ (cid:0)E ¯α(zk+1, zk, γk) − ¯E(cid:1) (cid:105)

.

Summing up the above inequality for i = k0 + 1, . . . , k, we
have

k
(cid:88)

i=k0

4(cid:107)zi+1 − zi(cid:107) ≤

k
(cid:88)

(cid:0)(cid:107)zi − zi−1(cid:107) + (cid:107)zi−1 − zi−2(cid:107)(cid:1)

+

16(¯α + ρ)
ρ

i=k0
(cid:104)
ϕ (cid:0)E ¯α(zk0, zk0−1, γk0−1) − ¯E(cid:1)
− ϕ (cid:0)E ¯α(zk+1, zk, γk) − ¯E(cid:1) (cid:105)

and since ϕ ≥ 0, we get

∞
(cid:88)

k=1

(cid:107)zk+1 − zk(cid:107) < ∞,

and thus the sequence {zk} is a Cauchy sequence. Hence
the sequence {zk} is convergent and we get the conclusion.

B. Detailed description of VF-iDCA in the

general setting

Our value function based DC algorithm as well as its con-
vergence analysis can be straightforwardly extended to BLP
in a more general setting with LL constraints.

min
x∈X,u∈U,λ∈RJ
+

L(x, u)

s.t. x ∈ argmin

x(cid:48)∈X

s.t.

(cid:26)

l(x(cid:48), u) +

J
(cid:88)

λiPi(x(cid:48), u)

i=1
(cid:27)

g(x(cid:48), u) ≤ 0

,

(31)
where X ⊆ Rn, U ⊆ Rd are closed convex sets, L, l :
Rn × Rd → R, Pi : Rn × Rd → R+, i = 1, . . . , J, gi :
Rn × Rd → R, i = 1, . . . , m are convex functions deﬁned
on an open convex set containing X ×U and g : Rn ×Rd →
Rm is deﬁned as g(x, u) = (g1(x, u), . . . , gm(x, u)). To
ensure the convergence, we assume that function L(x, u) is
bounded below on an open convex set containing X × U .

Decoupling the hyperparameter variable λ from the regular-
ization term, and introducing a new variable r result in the
following BLP:

min
x∈X,u∈U,λ∈RJ
+

L(x, u)

s.t. x ∈ argmin

(cid:26)

l(x(cid:48), u)

x(cid:48)∈X
s.t.

g(x(cid:48), u) ≤ 0,

(32)

Pi(x(cid:48), u) ≤ ri, i = 1, . . . , J

(cid:27)

.

Denote the solution sets of the lower level programs in
problem Eq. (31) and Eq. (32) by Sp(λ, u) and Sc(r, u),
respectively.

,

BLP Eq. (32) is then reformulated as the following single-
level DC program:

k
(cid:88)

i=k0

2(cid:107)zi+1 − zi(cid:107) ≤2(cid:107)zk0 − zk0−1(cid:107) + (cid:107)zk0−1 − zk0−2(cid:107)

min
x∈X,u∈U,λ∈RJ
+

L(x, u)

+

16(¯α + ρ)
ρ

ϕ (cid:0)E ¯α(zk0, zk0−1, γk0−1) − ¯E(cid:1) .

s.t.

l(x, u) − v(r, u) ≤ 0, g(x, u) ≤ 0,
Pi(x, u) ≤ ri, i = 1, . . . , J,

(33)

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

where v(r, u) is the value function of the LL problem gov-
erned by (r, u)

Denoting Σ := X × U × RJ
condition for choosing zk+1

+, we may introduce an inexact

v(r, u) := min
x(cid:48)∈X

{l(x(cid:48), u) s.t. g(x(cid:48), u) ≤ 0,

Pi(x(cid:48), u) ≤ ri, i = 1, . . . , J}.

dist(0, ∂φk(zk+1) + NΣ(zk+1)) ≤

√

2
2

ρ(cid:107)zk − zk−1(cid:107).

(37)

To ensure the convergence, assume that for each r belonging
to any open subset of RJ
+, the feasible region F(r, u) :=
{x ∈ Xs.t.g(x, u) ≤ 0, Pi(x, u) ≤ ri, i = 1, . . . , J} (cid:54)=
∅ and l(x, u) is bounded on F(r, u). Thanks to the full
convexity after hyperparameter variable decoupling, v(r, u)
is indeed convex and locally Lipschitz continuous around
every point in set RJ
>0 × U (see, e.g., Lemma 3 in (Ye et al.,
2021)).

Given a current iteration (xk, uk, rk) for each k, solving the
LL problem parameterized by uk and rk

min
x∈X

l(x, uk) s.t. g(x, uk) ≤ 0,

Pi(x, uk) ≤ rk

i , i = 1, . . . , J,

(34)

leads to a solution ˜xk ∈ Sc(rk, uk) and a correspond-
ing KKT multiplier (γk, ζ k) ∈ M(˜xk, uk, rk), where
M(x, u, r) denotes the set of multipliers of the lower level
problem,

M(x, u, r) :=

(cid:110)

λ ∈ RJ

+ | 0 ∈ ∂xl(x, u) + NX (x)

+, µ ∈ Rm
m
(cid:88)

λi∂xPi(x, u) +

µi∂xgi(x, u), (cid:104)µ, g(x, u)(cid:105) = 0,

i=1

λi(Pi(x, u) − ri) = 0, i = 1, . . . , J

(cid:111)
.

+

J
(cid:88)

i=1

Select

ξk ∈ ∂ul(˜xk, uk) +

J
(cid:88)

i=1

i ∂uPi(˜xk, uk)
γk

m
(cid:88)

+

i ∂ugi(˜xk, uk).
ζ k

i=1

(35)
Then by sensitivity analysis (see, e.g., Theorem 3 in (Ye
et al., 2021)), it can be easily checked that (−γk, ξk) ∈
∂v(rk, uk). Compute zk+1 := (xk+1, uk+1, rk+1) as an
approximate minimizer of the strongly convex subproblem

min
x∈X,u∈U,r∈RJ
+

φk(x, u, r) :=L(x, u) +

ρ
2

(cid:107)z − zk(cid:107)2

+ αk max
j=1,...,J
i=1,...,m

{0, Vk(x, u, r), Pj(x, u) − rj, gi(x, u)},

(36)

where z := (x, u, r), zk := (xk, uk, rk) and

Vk(x, u, r) := l(x, u)−l(˜xk, uk)−(cid:104)ξk, u−uk(cid:105)+(cid:104)γk, r−rk(cid:105).

Using above constructions, letting

tk+1 := max
j=1,...,J
i=1,...,m

(cid:110)

0,Vk(zk+1, uk+1, rk+1),

Pj(xk+1, uk+1) − rk+1

j

, gi(xk+1, uk+1)

(cid:111)
,

we are ready to present VF-iDCA in Algorithm 2.

Algorithm 2 VF-iDCA
1: Take an initial point (x0, u0, r0) ∈ X × U × RJ
+;
cα, δα > 0; an initial penalty parameter α0 > 0; toler-
ance tol > 0.

2: for k = 0, 1, . . . do
3:

Solve LL problem Eq. (34). Find ˜xk ∈ Sc(rk, uk)
and KKT multiplier γk, ζ k.
Solve problem Eq. (36) up to tolerance in Eq. (37).
Find an approximate solution (xk+1, uk+1, rk+1).
Stopping test.
Stop if max{(cid:107)zk+1 − zk(cid:107), tk+1} < tol.
Adaptive penalty parameter update. Set

4:

5:

6:

αk+1 =




αk + δα,

if max{αk, 1/tk+1} <

cα
∆k+1 ,



αk,

otherwise,

where ∆k+1 := (cid:107)zk+1 − zk(cid:107).

7: end for

Obviously similar convergence results for the general prob-
lem as in Theorems 3.2 and 3.3 can be obtained.

C. Hyperparameter Decoupling Illustration

We illustrate how to adopt the hyperparameter decoupling
technique and cast the BLPs listed in Table 1 in the formu-
lation of Eq. (4) with fully convex LL problem.

Elastic net (Zou & Hastie, 2003)
is a regularized linear
regression method that combines the ridge and lasso penal-
ties. The hyperparameter optimization problem for elastic

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

net can be formulated as the following BLP:

rameter decoupling and thus work on the following BLP:

min
β∈Rp,λ∈R2
+

s.t. β ∈ arg min

ˆβ∈Rp

1
2
(cid:26) 1
2

i∈Ival
(cid:88)

(cid:88)

|bi − β(cid:62)ai|2

(38)

min
β∈Rp,r∈RM +1

+

(cid:107)bi − β(cid:62)ai(cid:107)2

(41)

|bi − ˆβ(cid:62)ai|2

s.t. β ∈ arg min

ˆβ∈Rp

(cid:107)bi − ˆβ(cid:62)ai(cid:107)2

i∈Itr

+ λ1(cid:107) ˆβ(cid:107)1 +

(cid:27)

.

(cid:107) ˆβ(cid:107)2
2

λ2
2

s.t. (cid:107) ˆβ(m)(cid:107)2 ≤ rm, m = 1, . . . , M, (cid:107) ˆβ(cid:107)1 ≤ rM +1

(cid:27)

.

(cid:88)

1
2
(cid:26) 1
2

i∈Ival
(cid:88)

i∈Itr

For the implementation of VF-iDCA, we adopt the hyperpa-
rameter decoupling and thus work on the following BLP:

min
β∈Rp,r∈R2
+

1
2

(cid:88)

i∈Ival

|bi − β(cid:62)ai|2

s.t. β ∈ arg min

ˆβ∈Rp

(cid:26) 1
2

(cid:88)

i∈Itr

|bi − ˆβ(cid:62)ai|2

(39)

s.t. (cid:107) ˆβ(cid:107)1 ≤ r1,

(cid:107) ˆβ(cid:107)2

2 ≤ r2

(cid:27)

.

1
2

Sparse group lasso (Simon et al., 2013)
is a regularized
linear regression method well-suited for cases where the
features have a natural grouping. The sparse group lasso
penalty involves both (cid:107) · (cid:107)2 and (cid:107) · (cid:107)1 norms, and encourages
sparsity both on the group level and on the within group
level.

Suppose the p features are divided into M groups. Denoting
the corresponding coefﬁcients of m-th group as β(m) and
the corresponding regularization parameter as λm (rm for
the VF-iDCA), we can write the hyperparameter optimiza-
tion problem in the following way:

Low-rank matrix completion is a ﬂexible framework for
reduced-rank modelling of matrix-valued data. We consider
a variation explored in Feng & Simon (2018), where we
have access to additional information corresponding to each
row and column. Speciﬁcally, suppose that for matrix M ∈
Rn×n, we observe some entries Mij where (i, j) ∈ Ω and
do not have access to the rest. We denote the row features
X ∈ Rn×p and column features as Z ∈ Rn×p. We model
the matrix as the sum of a low rank effect Γ and a linear
combination of the row features and the column features.
Denote the coefﬁcients of row features and column features
by θ and β, respectively. Furthermore, suppose that there is
a natural grouping of the row and column features, which
partitions the corresponding coefﬁcients as {θ(g)}G
g=1 and
{β(g)}G
g=1. So with the nuclear norm penalty on Γ and the
group lasso penalty on the linear part, we have the following
model:

(cid:88)

(i,j)∈Ωval

1
2

min
θ∈Rp, β∈Rp,
Γ∈Rn×n, λ∈R2G+1

+

s.t. θ, β, Γ ∈

|Mij − xiθ − zjβ − Γij|2

(cid:26) (cid:88)

(i,j)∈Ωtr

1
2

arg min
ˆθ∈Rp, ˆβ∈Rp,
ˆΓ∈Rn×n

|Mij − xi

ˆθ − zj

ˆβ − ˆΓij|2

+ λ0(cid:107)ˆΓ(cid:107)∗ +

G
(cid:88)

g=1

λg(cid:107) ˆθ(g)(cid:107)2 +

λg+G(cid:107) ˆβ(g)(cid:107)2

(cid:27)

,

G
(cid:88)

g=1

min
β∈Rp,λ∈RM +1

+

1
2

(cid:88)

i∈Ival

|bi − β(cid:62)ai|2

s.t. β ∈ arg min

ˆβ∈Rp

(cid:26) 1
2

(cid:88)

i∈Itr

|bi − ˆβ(cid:62)ai|2

λm(cid:107) ˆβ(m)(cid:107)2 + λM +1(cid:107) ˆβ(cid:107)1

(cid:27)

.

+

M
(cid:88)

m=1

Note that in the above, we consider a variation on (Simon
et al., 2013) where we use a separate regularization parame-
ter for each of the groups, rather than a single regularization
parameter for the sum of the penalties across all groups,
along the lines of (Feng & Simon, 2018).

For the implementation of VF-iDCA, we adopt the hyperpa-

(40)

where Ω = Ωval ∪ Ωtr, Ωval ∩ Ωtr = ∅.

For the implementation of VF-iDCA, we adopt the hyperpa-
rameter decoupling ﬁrst and obtain the following BLP:

(cid:88)

(i,j)∈Ωval

1
2

min
θ∈Rp, β∈Rp,
Γ∈Rn×n, r∈R2G+1

+

s.t. θ, β, Γ ∈

|Mij − xiθ − zjβ − Γij|2

(cid:26) (cid:88)

(i,j)∈Ωtr

1
2

arg min
ˆθ∈Rp, ˆβ∈Rp,
ˆΓ∈Rn×n

|Mij − xi

ˆθ − zj

ˆβ − ˆΓij|2

s.t. (cid:107)ˆΓ(cid:107)∗ ≤ r0, (cid:107) ˆθ(g)(cid:107)2 ≤ rg, g = 1, . . . , G
(cid:27)

(cid:107) ˆβ(g)(cid:107)2 ≤ rg+G, g = 1, . . . , G

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

Support vector machines (Cortes & Vapnik, 1995)
are
supervised learning methods for binary classiﬁcation. Sup-
port vector machines solve

D. Numerical experiments

D.1. Numerical experiments on synthetic data

min
− ¯w≤w≤ ¯w
c∈R

(cid:26) λ
2

(cid:107)w(cid:107)2 +

(cid:88)

j∈Ωtr

max(1 − bj(w(cid:62)aj − c), 0)

(cid:27)

.

We note that this is a variant of support vector machines
proposed by Kunapuli 2008 which incorporates feature se-
lection through the box constraints on w.

Recent work (Kunapuli et al., 2008) considered the T -fold
cross validation method for selecting the hyperparameters
λ and ¯w, and treated it as a bilevel optimization problem.
A given data set Ω is randomly partitioned into T pairwise
disjoint subset called the validation sets {Ωt
t=1. For
each validation set Ωt
val, the corresponding training set is
Ωt
val. We select the best hyperparameters via
tr
minimizing the hinge loss based on the validation set and
its partition by following BLP:

:= Ω\Ωt

val}T

min
λ, ¯w,w1,...,wT ,c

Θ(w1, . . . , wT , c)

s.t. λ ≥ 0,

¯wlb ≤ ¯w ≤ ¯wub,

and for t = 1, . . . , T :

(wt, ct) ∈ argmin

− ¯w ≤ w ≤ ¯w
c ∈ R

(cid:40)

λ
2

(cid:107)w(cid:107)2

max(1 − bj(aT

j w − c), 0)

,

(cid:41)

(cid:88)

+

j∈Ωt

trn

with

Θ(w1, . . . , wT , c)

:=

1
T

T
(cid:88)

t=1

1
|Ωt
val|

(cid:88)

j∈Ωt

val

max(1 − bj(aT

j wt − ct), 0),

where |Ω| denotes the number of elements in set Ω.

For the implementation of VF-iDCA, we adopt the hyperpa-
rameter decoupling ﬁrst and obtain the following BLP:

min
r, ¯w,w1,...,wT ,c

Θ(w1, . . . , wT , c)

s.t. r ≥ 0,

¯wlb ≤ ¯w ≤ ¯wub,

(wt, ct) ∈

arg min
w∈Rp,c∈R

(cid:40)

(cid:88)

j∈Ωt

trn

max(1 − bj(aT

j w − c), 0)

(cid:107)w(cid:107)2 ≤ r, − ¯w ≤ w ≤ ¯w

(cid:41)
.

s.t.

1
2

D.1.1. ELASTIC NET

We simulate data in a similar manner as Feng & Simon
(2018) as follows. We draw ai ∈ Rp from a N (0, I)
distribution with cor(aij, aik) = 0.5|j−k|. We draw the
response b according to bi = β(cid:62)ai + σ(cid:15)i, where β
is randomly generated such that βi is either 0 or 1 and
(cid:80)p
i=1 βi = 15; (cid:15) is sampled from the standard Gaussian
distribution, and σ was chosen such that the signal-to-noise
ratio SNR ∆= (cid:107)Aβ(cid:107)/(cid:107)b − Aβ(cid:107) was 2.

For grid search, random search and TPE, we deﬁned u1 =
log10(λ1) and u2 = log10(λ2), and searched over u1, u2 ∈
[−5, −2], as in (Feng & Simon, 2018). Grid search was
performed on a 10 × 10 uniformly-spaced grid. Random
search was performed with 100 uniform random samples.
The space used in TPE for both u1 and u2 was a uniform
distribution on [−5, 2]. For IGJO, the initial guesses for
λ1 and λ2 were 0.01 and 0.01, respectively. For IFDM,
the inital guesses for λ1 and λ2 were were 0.01λmax as in
(Bertrand et al., 2021), where λmax = 1/|Itr| max(A(cid:62)
tr btr).
The maximum number of iterations of IFDM was set to be
50. For VF-iDCA, the initial guesses for r1 and r2 were 10
and 5, respectively. VF-iDCA was stopped when

(cid:40)

max

(cid:107)zk+1 − zk(cid:107)
(cid:112)1 + (cid:107)zk(cid:107)2

(cid:41)

, tk+1

< tol,

(42)

and we set tol = 0.1.

D.1.2. SPARSE GROUP LASSO

The data generation method was mainly referred to Feng &
Simon (2018) as follows. Each dataset contains 100 training
data, 100 validation data and 100 test data. Each ai ∈ Rp
were sampled from the standard normal distribution. The
response b was generated by bi = β(cid:62)ai + σ(cid:15)i, where
β = (cid:2)β(1), β(2), β(3)(cid:3), β(i) = (1, 2, 3, 4, 5, 0, . . . , 0), for
i = 1, 2, 3. (cid:15) are generated from the standard normal distri-
bution, and σ was chosen such that the SNR is 2.

For grid search, we did the search on two hyperparame-
ters µ1, µ2 such that λm = 10µ1, for m = 1, 2, . . . , M
and λM +1 = 10µ2, we use a 10×10 uniform-spaced
grid on [−3, 1] × [−3, 1]. For random search and TPE
method, we search over the transformed variables um,
where um = log10(λm), for m = 1, 2, . . . , M + 1, and
the space of um is deﬁned as a uniform distribution on
[−3, 1]. For implicit differentiation method, we use the ini-
tial guess [0.01, 0.01, . . . , 0.01] for λ. For VF-iDCA, we
use the initial guess [10, 10, . . . , 10] for r. The stopping cri-
terion used for VF-iDCA here was Eq. (42) with tol = 0.05.
In all experiments, the features are grouped by order, i.e.,

Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems

λmax = 1/|Itr| max(A(cid:62)
tr btr). The maximum number of iter-
ations of IFDM was set to be 10. For VF-iDCA, the initial
guesses for r1 and r2 are 10, 5. VF-iDCA was stopped
when Eq. (42) was satisﬁed with tol = 0.1.

D.2.2. SUPPORT VECTOR MACHINE

For grid search and random search, we did the search over
two hyperparameters µ1 and µ2, where λ = 10µ1, ¯w =
(10µ2, . . . , 10µ2 )(cid:62), the range of µ1 is -4 to 4, and the range
of µ2 is -6 to 2. For TPE method, we search the log10(λ)
in [−4, 4], and log10( ¯wi) in [−6, 2]. However, since TPE
is slow when the dimension is high, we set the maximum
number of iteration to be 10. And we also tested TPE
method on the simpliﬁed model, i.e., the same setting as the
search methods, and denote such a method as TPE2. For the
simpliﬁed version, we set the maximum number of iterations
to be 100. For VF-iDCA, the initial guess of r is 10 and the
initial guess of ¯w is (10−6, · · · , 10−6)(cid:62). We implement
VF-iDCA with two different stopping criteria, i.e., Eq. (42)
with tol = 0.01 and tol = 0.1. We denote the one with
tol = 0.01 by VF-iDCA, and the one with tol = 0.1 by
VF-iDCA-t. We set ¯wlb = 10−6 and ¯wub = 10 in the
model.

the ﬁrst p/M features are grouped as the ﬁrst group, the
next p/M features are grouped as the second group, etc.

D.1.3. LOW-RANK MATRIX COMPLETION

The data generation method was also referred to Feng &
Simon (2018) as follows. We took two entries per row and
column as the training set Ωtr and one entry per row and
column as the validation set Ωval. The rest entries were
collected as test set Ωtest. The row features are grouped into
12 groups of 3 covariates each, and same for the columns
features, i.e., p = 36, G = 12.

The true coefﬁcients chosen as α(g) = g13 for g = 1, . . . , 4
and β(g) = g13 for g = 1, 2. The rest of the coefﬁcients
were zero. We generated rank-one effect matrices Γ =
uv(cid:62), where u and v were sampled from a standard normal
distribution. The row features and column features X and
Z were sampled from a standard normal distribution, and
scaled such that the Frobenius norm of Xα1(cid:62) + (Zβ1(cid:62))(cid:62)
was the same as Γ. The matrix data were generated by
Mij = xiα + zjβ + Γij + σ(cid:15)ij, where (cid:15)ij were generated
from the standard normal distribution, and σ was chosen
such that the SNR is 2.

For grid search, we did the search on two hyperparame-
ters µ1, µ2 such that λ0 = 10µ1 and λg = 10µ2, for
g = 1, . . . , 2G, we use a 10 × 10 uniform-spaced grid
on [−3.5, −1] × [−3.5, −1] as Feng & Simon (2018) did.
For random search and TPE method, we search over the
transformed variables ug, where ug = log10(λm), for
m = 0, 1, 2, . . . , 2G, and the space of ug is deﬁned as a
uniform distribution on [−3.5, −1]. For IGJO, we use the
initial guess [0.005, 0.005, . . . , 0.005] for λ. For VF-iDCA,
we use the initial guess [1, 0.1, 0.1, . . . , 0.1] for r. The stop-
ping criterion used for VF-iDCA here was Eq. (42) with
tol = 0.05. In all experiments, the features are grouped by
order, i.e., the ﬁrst 3 features are grouped as the ﬁrst group,
the next 3 features are grouped as the second group, etc.

D.2. Application to real data

D.2.1. ELASTIC NET

The numerical settings for these method are kept the same
as in Section 5.1.1. For grid search, random search and TPE
method, we did the search on the logarithmic hyperparam-
eters u1 = log10(λ1) and u2 = log10(λ2), and the range
of both u1 and u2 is [−5, 2] as in Feng & Simon (2018).
Grid search was performed on a 10 × 10 uniform-spaced
grid. Random search was performed with 100 uniformly
random sampling. The space used in TPE method for both
u1 and u2 is deifned as a uniform distribution on [−5, 2].
For IGJO, the initial guesses for λ1 and λ2 were 0.01 and
0.01, respectively. For IFDM, the inital guesses for λ1
and λ2 were 0.01λmax as in Bertrand et al. (2021), where

