Predictive Synthesis of API-Centric Code

Daye Nam
Carnegie Mellon University†
U.S.A.

Baishakhi Ray
Columbia University†
U.S.A.

Seohyun Kim
Meta
U.S.A.

Xianshan Qu
Meta
U.S.A.

Satish Chandra
Meta
U.S.A.

2
2
0
2

y
a
M
8
1

]
E
S
.
s
c
[

2
v
8
5
7
3
0
.
1
0
2
2
:
v
i
X
r
a

Abstract
Today’s programmers, especially data science practitioners,
make heavy use of data-processing libraries (APIs) such as
PyTorch, Tensorflow, NumPy, and the like. Program syn-
thesizers can provide significant coding assistance to this
community of users; however program synthesis also can be
slow due to enormous search spaces.

In this work, we examine ways in which machine learning
can be used to accelerate enumerative program synthesis.
We present a deep-learning-based model to predict the se-
quence of API functions that would be needed to go from a
given input to a desired output, both being numeric vectors.
Our work is based on two insights. First, it is possible to
learn, based on a large number of input-output examples, to
predict the likely API function needed. Second, and impor-
tantly, it is also possible to learn to compose API functions
into a sequence, given an input and the desired final output,
without explicitly knowing the intermediate values.

We show that we can speed up an enumerative synthe-
sizer by using predictions from our model variants. These
speedups significantly outperform previous ways (e.g. Deep-
Coder [2]) in which researchers have used ML models in
enumerative synthesis.

CCS Concepts: • Software and its engineering → Pro-
gramming by example; Automatic programming; API lan-
guages.

Keywords: Program Synthesis, Programming By Example,
PyTorch, Tensor Manipulation

† Work done at Facebook as an intern.
† Work done at Facebook as visiting scientist; equal contribution as the first
author.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
MAPS ’22, June 13, 2022, San Diego, CA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9273-0/22/06. . . $15.00
https://doi.org/10.1145/3520312.3534866

ACM Reference Format:
Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish
Chandra. 2022. Predictive Synthesis of API-Centric Code. In Pro-
ceedings of the 6th ACM SIGPLAN International Symposium on Ma-
chine Programming (MAPS ’22), June 13, 2022, San Diego, CA, USA.
ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/3520312.
3534866

1 Introduction
One of the cherished dreams of the programming languages
research community is to enable automated synthesis of pro-
grams based on a specification. Synthesis approaches have
been designed around several different forms of specification,
e.g. a formal specification, or natural language description,
or input-output examples (aka demonstration), or a combi-
nation thereof. Just as well, several different approaches to
synthesis have been researched; see Related Work.

Our focus is on coding assistance for users of numeric
libraries such as PyTorch, Tensorflow, Numpy, Pandas, and
the like, each of which provide powerful data manipulation
routines behind an API, and the API functions are gener-
ally side-effect free. We assume a specification in the form
of a single input-output example, and we are looking for a
straight-line program consisting of calls to API functions.
We choose enumerative synthesis (explained in the next sub-
section) as the underlying synthesis approach. Our research
goal—shared with recent works such as DeepCoder [2], TF-
Coder [26], Autopandas [4], and others—is to speed up plain
enumerative synthesis using machine learning (ML).

Here is an input matrix as well as the desired output matrix,
and the synthesis problem is to come up with a sequence of
function calls that would convert the input to output. We
will use the PyTorch API for this purpose.
in = [[5., 2.], [1., 3.], [0., -1.]]
out = [[[5., 5.], [1., 1.], [0., 0.]],

[[2., 2.], [3., 3.], [-1., -1.]]]
The desired code fragment for this example is:

transpose(stack((in, in), 2), 0, 1)

The goal of program synthesis is to arrive at this expres-
sion, given only the input and output. Keep in mind that it
is unlikely that random guessing of an expression will work:
there are tens if not hundreds of available functions, and
each function might take more than one argument. Thus, a
systematic search is necessary.

 
 
 
 
 
 
MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

Figure 1. Overview of ML guided enumerative search algorithms. (a) weighted enumerative synthesis without ML model
incorporation [26], (b) weighted enumerative synthesis with one-time ML-based prioritization [2, 26], (c) incorporation of
Full-Seq prediction mode, (d) incorporation of First-Of-Seq prediction mode. Red-highlight indicates the API functions predicted
by ML models. The underscore is a placeholder for argument values. Numbers (in (a),(b)) on the left side are the costs assigned
to the values and API functions.

Basic enumerative synthesis Refer to Figure 1, part (a),
where we illustrate an enumerative synthesis in the style of
Transit [31] and TF-Coder [26]. The idea is to organize the
search in the order of increasingly complex expression trees,
where the complexity is approximated by a cost. We assign
a cost to each available value, and to each operation, which
here are API functions. (The cost of a function is assigned
heuristically, e.g. based on a global frequency of usage.) At
each step, we work with a budget, which grows in successive
steps. Expressions that can be formed from existing values
within the budget are added to a pool of values. For instance,
the expression stack((in,in),2) would cost the two times
the cost of in plus the costs of the value 2 and the function
stack. In the figure, this cost comes to 48, based on the cost
of stack being 36. The value computed by this expression is
added to the pool of values, along with the expression that
computes them and its cost. The process continues, with
increasing budget at each step, until the desired output value
is found. The expressions added to the pool of values are
shown in the figure.

Trying likely functions first The enumerative search
presented above is slow, and gets exponentially slower if a
larger expression is needed to get the job done. A reason
for this slowness is that the turn of the actually needed API
function might come in quite late, as enumerative synthesis
makes its way through the smaller cost budget and cheaper
functions. Balog et al [2], in their seminal work DeepCoder,
described a machine learning based strategy to accelerate

enumerative program synthesis. DeepCoder’s insight is to re-
assign costs to functions—based on a machine learning model
over the given input and output—such that the function(s)
more likely to be needed in a given situation are prioritized.
See part (b) of Figure 1. Here, given the specific input and
output, DeepCoder’s machine learning model, adapted to
our setting, correctly deems transpose, and stack as likely to
be needed. Operationally, an enumerative synthesis process
(e.g. as implemented in TF-Coder [26]) can lower costs of
these operations by some factor, so they are likely to be tried
in preference to other API functions. The hope is that if the
ML prediction is accurate, and the discounted costs work
out, the process of enumerative synthesis can be sped up
considerably.

This paper: Predicting function sequences Our thesis
is that ML can be used in the setting of enumerative syn-
thesis of API-centric code in a more powerful way: not for
prioritization, but instead to directly predict the sequence of
API functions that required to go from input(s) to the desired
output. We describe two ways in which such a predictive
model can be used to accelerate enumerative synthesis.

The first way in which we use this prediction model is to
just let it predict the entire sequence of API functions in one
shot, given the input and the output. In our running example,
the model will predict stack, transpose as the sequence.
See Figure 1, part (c). Given this sequence, the enumerative
synthesizer will only look for values to fill into the function
call arguments (shown by "_"). If the model predicts correctly,

(a)(b)(d)(c)transpose(stack((in,in),0),0,1)transpose(stack((in,in),1),0,1)Valuesin, 0, 1, -1, 3, 21st Function FOS(in1, out)stack((_,_),_)transpose(stack((in,in),2),_,_)×transpose(stack((in,in),2),0,1)1st Interm Valsstack((in,in),0)stack((in,in),1)stack((in,in),2)×transpose(in,0,1)⋮stack((in,in),2) ⋮transpose(transpose (in,0,1),0,1)⋮×API Functions (Prioritized with Classiﬁcation Model)24*0.536*0.52424transposestackeqmax⋮50|transpose(stack((in,in),2),0,1)in, 0, 1, -1, 3, 2424⋮30⋮44⋮Valuesin, 0, 1, -1, 3, 2API Sequence Full(in1, out)transpose(stack((_,_),_),_,_)transpose(stack((in,in),0),0,1)transpose(stack((in,in),1),0,1)×transpose(stack((in,in),2),0,1)ValuesAPI Functionseqmax transposestack⋮24242436eq(in,0)eq(in,1)⋮transpose(in,0,1)⋮stack((in,in),1)stack((in,in),2)⋮eq(in,eq(in,0)⋮transpose(eq(in,0),0,1)⋮in, 0, 1, -1, 3, 2×80|transpose(stack((in,in),2),0,1)3232⋮36⋮4848⋮60⋮64⋮4ValuesBASEEXPLOREDBASEEXPLORED2nd Function FOS(1st_Interm_val, out)Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

Table 1. Sample of synthesized programs with Full-Seq
model guided enumerative synthesis and the synthesis time
comparison. More examples can be found in the supplemen-
tary material (Appendix B.1).

Synthesized Program
eq(in1,unsqueeze(in1,1))
tensordot(in1,transpose(in2,0,1),1)

Full-Seq (s)
0.18
0.32

no ML (s)
0.8
2.06

the search space that an enumerative synthesizer faces is
vastly reduced, leading to possibly significant speedups.

A second way in which we use this predictive model is to
use it as a “first-of-sequence” (FOS) predictor. See Figure 1,
part (d). Given the input and the desired output, the FOS
predictor only predicts the first function in the sequence
needed. Say it predicts that function is stack. The synthe-
sizer tries out a set of concrete arguments for stack from
the values pool. The result of evaluating stack on each of
these sets of arguments is added to the values pool; these
are intermediate values in the desired computation. Next,
for each intermediate value thus obtained, the synthesizer
invokes the model again, this time giving it the intermediate
value (in place of the input) and the desired output value.
Say the model now predicts that the first function in the
remaining sequence needed is transpose. The synthesizer
then looks for appropriate arguments for transpose. At this
point, one of the argument choices would provide the desired
output. Compared to the full prediction, the point of this FOS
mode is that it gets to predict on the basis on known interme-
diate values, a bit akin to teacher forcing [32] in sequence
prediction, and can be successful more often than the full
prediction mode; but it can be less efficient than one-shot
prediction of the entire sequence.

On the running example, here are the comparative times
to a successful solution: plain enumerative synthesis, 54.79
seconds; DeepCoder-style ML-based prioritization, 34.71 sec-
onds; our API sequence prediction, FOS mode, 0.49 seconds;
and API sequence prediction, Full mode, 1.45 seconds. In this
example, full sequence prediction mode took a tad longer
than the FOS mode: this is because the correct full sequence
was in top-3 but not top-1, whereas in the FOS model the cor-
rect choice were at top-1. In general, we have found the full
sequence mode to be faster than FOS mode. Other examples
of Full-Seq guided synthesis are available in Table 1.

Contributions We make two contributions in this work.
First, we present a way to incorporate powerful predictive
models in the context of enumerative program synthesis.
On a suite of benchmarks (adapted from Stack Overflow)
for PyTorch, using our ML models reduces the (mean, max)
synthesis time from (10.01, 96.53) to (1.04, 9.58). By con-
trast, an adaptation of the idea of DeepCoder [2] reduces
the (mean,max) synthesis time only to (7.44, 77.00). (See
Section 5.3, Table 4.)

Second, our main technical advancement is in being able
to carry out prediction of a sequence of API functions, given
the input and the final desired output. Specifically, our model
predicts one API function at a time and executes each pre-
dicted API function to convert the (intermediate) input state
into another intermediate state until it becomes the target
output state. Here, the intermediate states are not given to
the model, but the model learns to represent what would be
concrete intermediate values in the latent space during the
training time. The ability to execute the API functions in the
latent space indicates that the model learns the API function
semantics (i.e., the relation between the input and output
states) rather than the sequence distribution of the train-
ing dataset, and allows the model to generalize to unseen
sequences or lengths. See Sec 6.

2 Learning to predict API sequences
Our technique works based on supervised learning over a
large number of input and output examples, trained over
individual API functions, or on sequences of API functions.
Since the availability of real training data is a pervasive prob-
lem in ML, we use synthetic data generation similar to prior
program synthesis work [28]. We pipe randomly-generated
diverse inputs through sequences of API functions and col-
lect resulting outputs (see Section 3.4). This helps capture
the behavior of a single or a sequence of API functions in
terms of how it transforms its input to the output.

Once trained, the model is able to predict a sequence of
API functions. It can predict for input-output pairs that were
never seen in the training data; thus it generalizes in the
data space as long as the query input-output pairs are in
distribution. More interestingly, it can predict sequences of
API functions that were not seen in the training set either.
This latter point is crucial, because the way we train the
model, it learns to compose new, previously unseen sequences
from the behaviors learned from training sequences.

Before we present operational details (Sec 3 onwards), we
would like to present some intuitions behind our proposed
ideas. We start with a basic classification model designed to
predict one API function, given an input and desired out-
put; and then build over it a compositional model that is
designed to predict a sequence of API functions. The impor-
tance of examining the classification model on its own was
crucial in our own journey, because it helped overcome sev-
eral challenges in synthetic data generation for training. (In
actual synthesis application, we use the model that predicts
function sequences, described after this.)

Predicting a function from input-output data The
first intuition we use is that for many common API functions,
their behavior—the relationship of output to inputs—has sim-
ple patterns. Moreover, the behavior of a function is discrim-
inable from behaviors of other functions based on simple
clues. Many functions simply move around elements of a

MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

function. (The correspondence of the internal representa-
tions to intermediate values is further explored in Section 6.)
This representation, along with the final output, can then
be passed to a recurrent invocation of the model, to make it
predict the next API function in the sequence. In this way,
we can train a compositional model for API sequences.

In our running example, the model first predicts stack
based on in and out. Importantly, it also computes an internal
representation of the intermediate value stack((in,in),2).
It then predicts transpose based on this internal represen-
tation and out. This is the principle by which the model is
able to compose even longer previously-unseen sequences.
Here we emphasize that the model is not predicting the
next API token (e.g. transpose) based on the tokens that
came before (e.g. stack), as is done in code completion mod-
els [14]. At each step, the prediction is based only on (an
internal representation of) the program state, as opposed to
on program text. This is a new capability, which could option-
ally be combined with additional signals such as previous
tokens, if desired.

3 Technical Details
In this section, we explain our models, the training and the
inference. We will use Figure 3 to show details using an
example.

3.1 Notations

We will work with following entities:
• T for domain values, tensors or vectors (or lists thereof)
• E for embeddings, which are vector representations inter-

nal to a neural network

• D for distributions, which are probability distributions
over names of API functions. For 𝑑 ∈ D, 𝑑 (𝑓 ) is the proba-
bility of function 𝑓 .

We will use some auxiliary operators:
• embedding, denoted by ⟦.⟧ : encoding(T) → E
• concatenation, denoted by .#. : T × T → T

3.2 The encoding function
Before passing the input and output tensors to the models, we
encode them into a fixed-length vector (Figure 3-Encoding).
We extract three different pieces of information from the
tensors: (i) tensor values, (ii) tensor shapes, and (iii) tensor
types, and combine them as a sequence separated by a special
separator < 𝑠 >, i.e., 𝑋 = type <s> shape <s> value , such that, the
models can learn from all the three modalities together.

To manage the wide range of tensor values in the model,
we normalize the values as follows: we encoded the values
greater than 100 into 100, values greater than 1000 into 101,
and similarly for the negative values. The intuition is based
on how developers recognize patterns: when a value becomes
large enough, the importance of the least significant digit
decreases in pattern recognition.

Figure 2. Visualization of embedding space of input-output
pairs.

data structure (e.g. transpose or reverse) in easy to recognize
patterns. In other cases, the operation is a simple element-
wise computation.1 This suggests that a feed-forward neural
network can be trained to predict likely functions—as in a
multi-classification problem—from a representation of the
input and output data. Such a network would have to be
trained on large amounts of input-output examples and their
known (ground truth) functions.

Figure 2 shows the tSNE plot for 5000 input-output pairs.
The classification model was trained over synthetic data gen-
erated to classify among one of 33 API calls from PyTorch.
The figure shows that the input-output pairs – or rather,
their embeddings – map to visually distinct clusters, corre-
sponding to the function calls that would be needed to go
from the input to the output. The reason this clusters cleanly
is that the network learn to pick up the essential patterns
that appear in the input and corresponding outputs.

Predicting sequences of functions The case of predict-
ing an API sequence, such as stack followed by transpose,
is harder. The intermediate values that flow between API
calls are not known ahead of time, so it is not possible to
reconstitute this sequence simply by invoking the classifica-
tion model (for one API method name) over successive pairs
of inputs and outputs. Moreover, learning to recognize the
intended sequence from among all possible sequences, based
on an input and the final output, can be difficult, for reasons
for computational cost, for a classification model that pre-
dicts over a fixed collection of sequences of API function
names.

This is where a second intuition comes into play. Given
an input and a final output, we can imagine a model that
predicts the first function in the intended sequence of the
API functions that would process the input and eventually
produce the (final) output. Crucially, we train this model
as a recurrent unit, such that it not only predicts the first
API function needed, but additionally produces a representa-
tion (in the embedding space) of the output of that first API

1There do exist operations with more complex behaviors, but here we limit
ourselves to simple ones (see Appendix A.

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

Finally, all domain inputs and output encoding are con-
catenated together. We support up to 3 inputs and one output.
Dummy inputs are added when there are less then 3 inputs
to keep the model input size same for all examples.

3.3 Compositional Model

We train a model to predict the sequence of API functions
𝑠𝑓 = [𝑓1, ..., 𝑓𝑛], given a task specification 𝜙 = {𝑖𝑛𝑝, 𝑜𝑢𝑡 },
where 𝑖𝑛𝑝 is a list of input tensors that have gone through
the sequence of API operations 𝑠𝑓 , and 𝑜𝑢𝑡 is the final output
tensor.

In this description, we assume all functions take two in-
puts: first, the result of previous computation, and second,
a “local” input, e.g. 𝑖𝑛𝑝𝑖 here that comes from 𝑖𝑛𝑝. Define
𝑎𝑟𝑔𝑠𝑖 = (𝑓𝑖−1(𝑎𝑟𝑔𝑠𝑖−1), 𝑖𝑛𝑝𝑖 ), for 𝑖 > 1, and 𝑎𝑟𝑔𝑠1 = (_, 𝑖𝑛𝑝1).
We train a model 𝐺, such that for 𝑖 = 1..𝑛:

𝑓𝑖 = 𝐺 (⟦𝑎𝑟𝑔𝑠𝑖, 𝑜𝑢𝑡⟧)

(1)
The embedding ⟦.⟧ of the encoded inputs is obtained using
feed-forward networks (FFN), whereas 𝐺 is rendered by em-
ploying recurrent neural networks (RNNs)2. The operation
of one cell of an RNN has the type: RNN : E1 × E2 → (E3, D)
where,
• E1 is the hidden state coming from previous cell, or a zero

value;

• E2 is the embedding of the local input and the final output;
we permit each function to have an optional additional
input;

• E3 is the output hidden state being passed to the next cell;
• D is the prediction of API function from this cell; techni-
cally it is a distribution from which we take the argmax.

Then,

h𝑖, 𝑑𝑖 = RNN 𝑖 (h𝑖−1, ⟦𝑖𝑛𝑝𝑖 #𝑜𝑢𝑡⟧)
where hi−1 and hi are incoming and outgoing hidden
states, respectively, and 𝑑𝑖 the predicted distribution. We
expect 𝑓𝑖 = argmax (𝑑𝑖 ).

(2)

Figure 3 Compositional Model shows three units of the
model for an example. In each unit, the encoding passed to
the feed forward network is similar to the one used before
to create ⟦𝑖𝑛𝑝𝑖 #out⟧. When 𝑓𝑖 needs to use 𝑓𝑖−1(𝑎𝑟𝑔𝑠𝑖−1), we
mask the position as empty ("<p>" in the figure) so that the
model exploits h𝑖−1. Embedded encodings are passed to RNN
units, and each unit further projects the input embedding
into the RNN embedding space to generate ℎ𝑖 , using infor-
mation flowed from adjacent units, ℎ𝑖−1. Finally, the output
of each unit is passed to a softmax layer (not shown here) to
produce a probability distribution over API functions.

3.4 Synthetic Data Generation

To train a neural model so that it can understand the be-
havior of API functions, a large number of corresponding

2Technically, bi-directional RNNs [24].

input-output pairs is necessary. Unlike other problems ex-
ploiting ML models, collecting real-world data from code
repositories (e.g., GitHub) is not applicable here because we
need runtime values, not static information such as static
code. Therefore, we randomly generate input/output values,
and use the synthetic dataset for model training.

For each API function, we randomly generate input ten-
sors, run the API functions with them, and capture the cor-
responding outputs. In other words, we create a set of in-
put/output values in a black-box manner: we do not assume
API functions’ implementation details or internal behaviors.
As it does not require understanding internal program struc-
tures, it is easy to generate a large number of input-output
pairs without much manual effort and can be easily paral-
lelized.

Listing 1. Example data generation code for torch.sum

def g e n e r a t e _ s u m _ I O ( ) :

t e n s o r _ s i z e = r a n d o m _ t e n s o r ( )

i n _ t e n s o r ,
dim = random_dimension ( 0 ,
i f dim == len ( t e n s o r _ s i z e ) :

t e n s o r _ s i z e )

o u t _ t e n s o r = t o r c h . sum ( i n _ t e n s o r )

e l s e :

o u t _ t e n s o r = t o r c h . sum ( i n _ t e n s o r , dim )

return ( i n _ t e n s o r , o u t _ t e n s o r )

However, as even a simple API operation in modern li-
braries (e.g., PyTorch) imposes many constraints, inputting
random values will generate many runtime errors due to
the constraints violations. To reduce such errors, we exploit
API specification, and generate a set of inputs with the valid
combinations (see Listing 1 for an example of generating dat-
apoints for torch.sum). By excluding invalid combinations
of arguments to each API function, we can speed up the data
generation and generate a large synthetic dataset that can
capture API function input/output benign behavior.

4 Incorporating ML in Enumerative Syn.
Here we formally describe how the ML models were incorpo-
rated into the enumerative synthesis. Please refer to Figure 1
and Section 1 for a walk through of these on an example.
Detailed description of our implementation and the pseudo
code for each synthesis approach can be found in the sup-
plementary material (Appendix C, D).

Basic enumerative synthesis. As a baseline, we imple-
ment an enumerative synthesizer without any ML models.
Basic enumerative search starts with a set of base values and
enumerates over combinations of operations and the values.
The list of base values includes inp, other basic constants
such as 0, 1, -1, or heuristically-chosen values such as the di-
mensions of the given variables (e.g., 3). Then, starting with
the base values, the search enumerates ways of applying
operations to previously-explored values and expand the set
of known values. There are various ways of iterating the
operations and the values (e.g. based on syntactic size as

MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

Figure 3. Illustration of Compositional Model on an example. The inputs are in the Tensor Values box, and the expected
prediction is shown in the Sequence box.

in Transit [31]), but we use weighted enumerative search,
which is the approach of and TF-Coder [26]. It does so in the
order of increasing cost. Operations and values are assigned
costs based on their complexity: less common and more com-
plex operations are assigned higher cost, and the common
and simple operations are assigned lower cost. Costs are
additive, so common operations and simpler expressions are
explored earlier. The costs are manually set by the synthe-
sizer developers, but only needed to be set once, and it will
be used for all tasks.

Prioritizing likely functions with an ML model. As
the needed operations for a specific problem are not known
to the synthesizer ahead of time, the costs seeded in it will not
always be ideally suited for all problems. TF-Coder [26] and
DeepCoder [2] address this problem using an ML model to
re-weigh all operations before the enumerative search starts.
Given a task specification (i.e., input/output examples), it
invokes a multi-label classification model to predict the prob-
ability of each needed operation and re-weighs them accord-
ingly with the goal of encountering the needed operations
earlier in the search. We trained multi-label classification
model following DeepCoder [2].

Compositional Model - Full-Sequence. In this mode,
compositional model predicts a sequence of API functions
𝑠𝑓 = [𝑓1, 𝑓2, ..., 𝑓𝑛] given the final output 𝑜𝑢𝑡, and the inputs
to each API function [𝑖𝑛𝑝1, 𝑖𝑛𝑝2, .., 𝑖𝑛𝑝𝑛]3. The synthesizer
invokes the compositional model with the specification, pre-
dicts a sequence of operations, and searches only the param-
eter values (e.g., dimension) that were not provided in the
specification.

The Full-Seq mode completely bypasses enumerative search
over operations. Instead, the compositional model predicts
the API functions needed in a synthesis instance as well as
the order of those APIs in the synthesized code. Thus, the
synthesizer does not need to search the operation space, but
only needs to search the combinations of base values.

3As the program synthesis task specification only provides an order agnostic
list of inputs, the synthesizer needs to search through different combinations
of them to generate a list of input tensors to each API call to invoke the
compositional model. In this section, we assume that the list of input tensors
to each API call is provided.

Compositional Model - First-Of-Sequence. In the First-
Of-Seq mode, given an input and a final output, composi-
tional model predicts the most probable API function needs
to come in the sequence. As enumerative search keeps track
of the intermediate output value, we can iteratively invoke
compositional model, and compute the intermediate values
using the predicted API functions, which can be used to
predict the next API function.

5 Evaluation
In this section, we first describe the training and evaluation
dataset ( Section 5.1), and evaluate the trained API function
sequence prediction model ( Section 5.2). Then, we investi-
gate the prediction-guided synthesis ( Section 5.3). Finally,
we show the generalizability and the compositional property
of our model ( Section 5.4).

5.1 Dataset
Program synthesis benchmarks. We evaluated the effec-
tiveness of our approaches with a subset of TF-Coder’s SO
benchmarks [26]. These benchmarks contain 50 tensor ma-
nipulation examples collected from SO, each containing in-
put and output tensor values and the desired solutions in
Tensorflow. To evaluate our approach that supports PyTorch,
we first translated them into PyTorch and excluded tasks
that we could not translate by hand. Among the 33 API
functions needed to for remaining 36 benchmarks, we se-
lected 16 functions covering 18 benchmarks (Table 2-Stack
Overflow) from core utility that modify values (e.g., add) or
shapes (e.g., transpose) of tensors, create them, or manip-
ulate them in similar ways. These operations were chosen
because the model can clearly observe the behavior of each
API function solely from input and output pairs (i.e., no side
effects). The full benchmarks we support are available in the
supplementary material (Appendix B).
Synthetic data generation. To train our sequence predic-
tion model that work for the SO benchmarks, we synthesized
a dataset as per Section 3.4. We synthesized 202 unique API
functions sequences by using the exhaustive combination of
16 API functions, with 1 or 2-length sequences. From the 272
(16 + 16*16) possible sequences, 70 sequences were removed
due to the constraints.

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

Table 2. Statistics of the dataset used in this study. Numbers
in parentheses indicate the length of the sequences.

Table 4. End-to-end program synthesis results; our models
in bold. Time, Max, and Median show the average, max,
median synthesis time of found programs.

Synthetic

Stack Overflow

Train

Valid.

Test

Test

# of unique seqs (len)
# of in/out values

5.5M

16 (1) + 186 (2)
10K

10K

8 (1) + 7 (2)
18

Table 3. Model accuracy for unseen input/output values.

Model

Full-Seq
First-Of-Seq

Synthetic-Test
Top-1

Stack Overflow
Top-3
Top-1

79.36%
66.88%

35.29%
52.38%

76.47%
76.19%

For each API function sequence in the training dataset,
say 𝑓1, 𝑓2, 𝑓3, we ran 𝑓1 with randomly generated input and
other parameter values (e.g., dimension, mode, etc.). Then,
𝑓2 takes 𝑓1’s output as input and takes other random input
tensors, if necessary. We treat 𝑓3 similarly by propagating
𝑓2’s output.

To generate diverse and unbiased input-output pairs, we
cover different properties of the functions, such that the
model can explore the broad data space of input-output pairs.
It took 1-person week to encode the API specifications
to write valid data generation code by reading the PyTorch
documentation. To avoid expansion to large input values
and to let the model learn the patterns sufficiently, we used
a fixed range of values (from 0 to 20) and the size of tensors
(up to 3 dimensions, and up to 5 elements in each dimension),
to prevent the tensors to be dispersed too much.

We created the dataset with 100,000 input-output pairs
for each unique API sequence (Table 2-Synthetic). We split
the dataset into training, validation, and test sets. The train-
ing, validation, and test sets included all 202 API sequences,
but the input/output values were not overlapped across the
datasets.

5.2 Sequence Prediction Model

We trained both Full-Seq and First-Of-Seq variants using the
training set of the synthetic data, and evaluated it with (1)
the test set of the synthetic data, and (2) SO benchmarks.
Observation. Table 3 shows the result. Model’s top-1 testing
accuracies of the 10K synthetic test set are ∼79%. Among
18 SO benchmarks, the Full-Seq model found 13 sequences
are in top-3 (72.22%), among them 6 are in top-1 (33.33%). In
comparison to the Full-Seq model, the First-Of-Seq model’s
top-1 accuracy is better. This is not surprising as First-Of-Seq
model has more information (actual values of the intermedi-
ate inputs) than the Full-Seq variant. However, surprisingly,
the top-3 accuracies of both are almost similar. These results
indicate that the compositional model perhaps learned a rep-
resentation of the intermediate states of the API operation

Time

Found Not Found Mean Max Median

Enumerative
Multi-label
First-Of-Seq
Full-Seq

18
18
17
14

0
0
1
4

10.01
7.44
5.87
1.04

96.53
77.00
59.93
9.58

0.46
0.32
0.39
0.25

sequence: even without passing the true intermediate values,
the Full-Seq model behaves at per with the Full-Seq model
at top-3.

5.3 Prediction-guided enumerative synthesis

Using the trained Full-Seq and First-Of-Seq variants, we first
evaluate our approach, against vanilla enumerative synthesis
(similar to [26]). We further compared with a multi-label pre-
diction model, a setting inspired by DeepCoder. [2]. Table 4
shows the results.

Existing synthesizers vs. compositional model. Among

the 18 tasks, both vanilla enumerative search and a synthe-
sizer prioritized with multi-label classification model could
synthesize all tasks. The new variants incorporating our mod-
els synthesized 17 (First-Of-Seq) and 14 (Full-Seq) correctly,
out of 18 and 17 respectively. However, although they synthe-
sized fewer solutions, they required less time to synthesize
the solutions: 5.87 seconds (First-Of-Seq) and 1.04 seconds
(Full-Seq) on average, whereas the existing synthesizers took
10.01 and 7.44 respectively.

We see this speed up because predicting the sequence re-
duces the search space. As the compositional models return a
sequence of API functions, the enumerative search can focus
on the argument values instead of iterating over the API
function sequences. Note that the multi-label classification
model also suggests potential API functions, but instead of
function sequences, it provides us with a set of functions.
Thus, in the worst case, the associated enumerative search
has to explore all the possible combinations increasing the
synthesis time.

The difference in synthesis time between the composi-
tional models and the baselines is not big in simple tasks
(e.g., any(in,-1)), which is why the difference in median in
Table 4 is not significant enough. However, when it comes to
more complex tasks like in Figure 1, the difference becomes
significant: 54.79 seconds with plain enumerative synthesis
vs. 0.49 with First-Of-Seq mode.

One caveat of compositional models is that the model pre-
diction is not the bottom-up method but a one-shot approach.
Therefore, when the model fails to predict the sequence cor-
rectly, it cannot synthesize the program. However, as the
whole search can be done quickly, the time overhead is not

MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

high even when one tries with the compositional model and
employ other approaches once it fails.

First-Of-Seq vs. Full-Seq. Between the two variants,
First-Of-Seq was able to synthesize more programs. For ex-
ample, First-Of-Seq successfully synthesized the desired pro-
gram where(lt (in,1),in,1) by predicting lt and where cor-
rectly in top-3. However, Full-Seq failed, and predicted [[eq,
where],[eq,mul],[gt,where]] as top-3, which are close enough,
but not entirely correct. This is expected. First-Of-Seq only
has to get the first element of the sequence right; the next ele-
ment is in fact the first element of the result of the subsequent
prediction, which in turn, is based on the actual intermediate
value computed by the first function predicted. Whereas, the
Full-Seq gets only one chance to get the entire sequence right
without knowing the intermediate values. When a sequence
is correctly predicted, Full-Seq model could synthesize the
solutions faster; it only needs to be invoked once, without
the intermediate values computation.

5.4 Evaluating Generalization

To see whether the model truly learned the functionality
of the API functions and learned their compositions, we
tested whether the model can generate new API sequences
that were not present in the training data. From the original
synthetic dataset 2, we removed the data of 7 API function se-
quences with length-2 that were included in Stack Overflow
benchmarks, and trained the First-Of-Seq and Full-Seq mod-
els. Our hypothesis was that if the models are able to learn
the compositional property, instead of learning the distribu-
tions of sequences, they should be able to generate unseen
sequences by composing API functions into a sequence.
Observation. Not surprisingly, the accuracy drops from the
Section 5.2 result. Nevertheless, out of 8 benchmarks with
2 sequence, we can still predict 4 sequences at top-5 (50%
accuracy) with Full-Seq, and 6 sequences (75% accuracy) with
First-Of-Seq. In this setting, we sometimes narrowly miss
some function sequences. For example, we miss a benchmark
[lt, where], however it predicts [eq, where], and [gt, where]
instead. Note that gt and eq have very similar functionalities
to the intended API function lt. Both the models can cor-
rectly predict sequences like [unsqueeze, eq], [matmul, add],
etc. As expected, the First-Of-Seq model works much better
than Full-Seq model.

To further check the model’s ability to generalize to un-
seen 3-length sequence, we randomly picked 71 unique 3-
length sequences made out of 16 API functions and collected
100 instances of them with different input/output values.
This gives a total of 7100 test samples. We used the model
trained with only sequences with length 2. Overall, at top-
5, model’s accuracy is ∼34% when queried with unknown
sequences and unknown values. However, the model can
predict 69 out of 71 sequences correctly at least with one
input-output. The only two sequences the model missed are
[add, mul, any] and [add, unsqueeze, ne]. In contrast, [where,

Table 5. Model accuracy for unseen input/output values,
trained with a dataset covering all SO benchmarks.

Model

Full-Seq
First-Of-Seq

Synthetic-Test
Top-1

Stack Overflow
Top-3
Top-1

88.15%
65.44%

68.57%
51.61%

91.42%
79.03%

expand, matmul] was predicted correctly around 97% time.
These results indicate the model’s ability to generalize.

5.5 Scaling to a larger set of API functions
First-Of-Seq vs. Full-Seq. We created a dataset covering
all 36 SO benchmarks (1- to 3-length) in section 5.1 with
33 API functions (up from 16 before). Synthesizing a train-
ing data covering all exhaustive combinations of the 33 API
functions, up to 3-length sequences, gives us a huge number
of combinations (33x33x33=35,937). Generating the corre-
sponding training data (>35B samples) and train the model
accordingly is challenging. Therefore, as a start, we selected
65 unique sequences that can be a solution to one of the 36
benchmarks.

Observation. Table 5 shows the result. Compared to the
model trained with the exhaustive combinations of 16 API
functions (Section 3), the Full-Seq model trained with this
dataset actually performed even better for (91.42% top-3
accuracy in SO benchmark data). We conjecture that this is
because the model could focus its learning on the semantics
of API functions sequences that are more likely to be in the
test set, rather than learning semantics of all combinations.
Unlike the accuracy difference in Full-Seq models, First-Of-
Seq variant achieved slightly lower top-1 accuracy with this
dataset. This is because the number of API functions it needs
to learn has increased from 16 to 33. For First-Of-Seq, as it
predicts each API function independent from others, it could
not benefit from having a training set with the targeted
sequences.

Generalizability. We also tested the generalizability of
our model when it is trained with a dataset covering all
33 API functions needed to synthesize the solutions for 36
SO benchmarks. Due to the aforementioned challenge, we
could not synthesize a dataset like in Section 5.4; instead,
we synthesized a dataset with 598 random combinations of
1 or 2-length sequences of 33 API functions, that has not
appeared in SO benchmark.

Observation. 7 out of 18 (39%) two-length unseen sequences
were predicted correctly with Full-Seq, and the first API of
a sequence correctly predicted by First-Of-Seq with 69% ac-
curacy. They both achieved lower accuracy than the ones
trained with the exhaustive combinations, as this model had
fewer sequences to learn from, which is critical in generating
unseen sequences.

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

successive hidden states contain information analogous to
the results of concrete computations: 𝑓1(𝑖𝑛𝑝1), 𝑓2(𝑓1(𝑖𝑛𝑝1)),
and so on. (In the actual model, these functions need not be
unary, as implied here.)

Figure 4. Illustration of compositional learning. (a): Two
units of the compositional model predicting a sequence
[𝑓1, 𝑓2]. (b): Single unit model predicting 𝑓2 given 𝑓1(𝑖𝑛𝑝1)
instead of ℎ1. We show the compositional property of the
model by showing ℎ2 ≈ ℎ′
2.

6 Why Composition works?
We show that a unit of our compositional model has an
interesting property: it learns to convert its incoming hidden
vector to its outgoing hidden vector in a way consistent
with the semantics of the API function it predicts, albeit in
embedding space. This property is crucial for predicting a
sequence compositionally.

In Fig 4(a), we show two units of the compositional model,
where the first one predicts function 𝑓1, on the basis of 𝑖𝑛𝑝1,
𝑜𝑢𝑡 and the previous hidden vector, if any. That unit also
produces a hidden vector ℎ1. The second unit produces hid-
den vector ℎ2. It may also consume further local input (such
as 𝑖𝑛𝑝2). Fig 4(b) shows an alternate situation in which we
give the result of 𝑓1(𝑖𝑛𝑝1) directly as input to the first unit,
which then produces ℎ′
2.

Figure 5. Proximity of ℎ2 and ℎ′
2 pairs for some inputs
(in white and black respectively), against a backdrop of ℎ2
(crosses) and ℎ′

2 (dots).

The interesting property is that ℎ2 and ℎ′

2 are close to-
gether in the representational space. Figure 5 shows a part of
a tSNE plot of ℎ2 (crosses) and ℎ′
2 (dots), as described above,
for inputs drawn from our benchmarks. As we expect, black
and white markers in the figure show that ℎ2 and ℎ2′ for the
same inputs are arranged close to each other in the embed-
ding space. In producing ℎ2 (or ℎ′
2), the RNN unit did not care
whether it was given ℎ1, the representation produced by the
previous RNN unit, or directly given 𝑓1(𝑖𝑛𝑝1). In this manner,

Composition property formally. Refer to the notation
introduced in Sec 3.3. If instead of ℎ𝑖−1, RNN 𝑖 is given (the
embedding of) the concrete intermediate value computed by
the computation so far, its observed behavior is the same in
both cases. Formally,

h𝑖, 𝑑𝑖 = RNN 𝑖 (⟦𝑓𝑖−1(𝑎𝑟𝑔𝑠𝑖−1)#out⟧, ⟦𝑖𝑛𝑝𝑖 #out⟧)

(3)

≈ RNN 𝑖 (h𝑖−1, ⟦𝑖𝑛𝑝𝑖 #𝑜𝑢𝑡⟧)

Thus, the hidden vectors capture an abstraction of interme-
diate values that would have arisen in concrete computation
𝑓1(𝑖1), 𝑓2(𝑓1(𝑖1)), and so on. The composition takes place as
each unit of RNN make a local decision based on incoming
hidden vector, which is set up to capture the result of the
corresponding concrete computation so far.

7 Limitations
Our results are promising, yet preliminary in many ways,
and we have not established generality in several dimensions.
First, we support a small set of API functions and have car-
ried out a limited evaluation. As the number increases, the
training data size also increases, and training the model well
becomes harder due to computational needs. The robustness
of training is a challenge in general.

Second, the model’s ability to generalize to unseen se-
quences is crucially dependent on training over a broad di-
versity of API sequences. This is challenging as we go to a
larger number of API functions, because we cannot cover all
permutations exhaustively. However, as seen in Section 5.5,
the model can still learn the semantics reasonably well if the
training data covers the sequences in the test set. Thus, the
future works may benefit from creating a training dataset
containing a distribution of sequences representing the real-
world API usage patterns, through API usage mining [18, 36].
Third, we have explored the model’s training and infer-
ence on relatively short tensors, with small data ranges, and
have generally worked only with integer data. In a real ap-
plication, tensors can be out-of-distribution with respect to
the model.

Fourth, we have worked only with PyTorch. We believe the
work can be replicated easily to NumPy and Tensorflow API
functions, because of their similar nature (acting over arrays
of numbers.). Farther out, we may need to invent additional
techniques. For example, Pandas is designed to deal with
records (or “dataframes”) containing labeled axes (rows and
columns); so it supports not only numeric manipulation, but
also label-based slicing. Here we may be able to borrow some
insights from the Autopandas work [4]. Finding ways to deal
with the above threats is in our future work.

28(a)(b)RNN1prob(f2)FFN10h’2(f1(inp1),inp2)  outh2RNN2RNN1prob(f1)prob(f2)FFN2FFN1h10 inp1 out inp2 outMAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

8 Related Work
Program Synthesis. It has a rich literature, including ex-
ample driven LISP code generation [13, 25], deductive syn-
thesis from decades ago [17], sketch completion using satis-
fiability [30], bit-vector manipulations [15], string process-
ing [12, 22], data processing [29, 33], syntax transforma-
tions [23], database queries [34], data wrangling [10, 11, 16],
and the highly successful programming-by-examples system
FlashFill [12]. FlashFill uses enumerative synthesis, where a
space of programs are explored in some order, until one that
fulfils a requirement—typically one or more examples—is
found. Transit [31] is another well-known work in enumera-
tive synthesis, where the exploration is arranged in terms of
finding sub-expressions in order of their costs. Increasingly
more costly expressions are attempted, using expressions
previously computed.

ML for Program Synthesis. With advances in ML, re-
searchers tried to adopt ML on top of the enumerative search
for more efficient program synthesis [2, 4, 19, 21, 26]. Our
work is closely related to DeepCoder [2], TF-Coder [26] and
BUSTLE [21]. Using prediction-guided enumerative synthe-
sizer, they show the benefits of predicting API functions that
are needed somewhere given a synthesis instance. However,
they all use a explicit featurization over these the input-
output values, which is not easy to generalize to other pro-
gramming languages. Also, they only predict presence or
absence of API functions, the prediction was only used to
prioritize operations in the enumerative search, rather than
directly predicting the API function(s) in sequence. With the
ML model guiding the search, BUSTLE takes an approach
similar to ours, which gives feedback to search iteratively,
whereas the models of DeepCoder or TF-Coder only give
feedback in the beginning of the search. However, BUSTLE
and DeepCoder only support simple DSL tasks, which may
not be generalized for real-world API-based synthesis.

Neural Program Synthesis. Approaches like [1, 3, 5, 8,
22, 22, 35] directly use neural networks for end-to-end syn-
thesis [3, 5, 8, 22] to generate string transformation programs
from examples. These works generally use encoder-decoder
model. In particular, the encoder embeds the input/output

strings, and the decoder generates the program sequences
conditioned on the input embedding. However, these ap-
proaches are mostly built and evaluated with simple DSL
tasks, mostly with simple string transformation. In this work,
we worked on the real-world tensor manipulation library
PyTorch. Although our evaluation does not cover the full
range of PyTorch, we found several challenges in expanding
these work into more complex programs, such as the scala-
bility issue in training data generation and diversity of the
API parameters especially in the tensor domain.

Execution-guided Program Synthesis. Recent works
have tried to exploit program execution to learn better repre-
sentations for the neural program synthesis [5–7, 9, 20, 27].
Some of the approaches [5, 27] use program interpreters
to provide the actual intermediate execution results, and a
more recent approach [7] learns the latent representation to
approximate the execution of partial programs using a sepa-
rate “Latent Executor”. We also learn the representation of
the execution of partial programs and demonstrate that (see
Section 4). However, we capture the intermediate execution
results as part of the main recurrent model, without needing
to use the separate module to approximate or execute the
program.

9 Conclusion
In this paper, we proposed a new machine learning technique
to speed up enumerative program synthesis. Our idea is to
use an ML model to predict the sequence of API function
calls required to go from an input to the final desired output,
in our case, both numeric vectors. Our model is trained on
randomly generated data. It is able to predict API sequences
for previously unseen inputs and outputs. Moreover, it can
predict API sequences that were not seen during training
either. The model does so by learning to compose API se-
quences, by learning how to keep track of values in the
hidden states of an RNN. We showed that our model can
predict sequences of lengths 1 to 3 fairly well. In terms of
effectiveness, we showed that our technique accelerates enu-
merative synthesis more effectively than related previous
works DeepCoder [2] and TF-Coder [26].

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

References
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
Chang. 2021. Unified Pre-training for Program Understanding and
Generation. In 2021 Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL).

[2] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian
Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to write
programs. arXiv preprint arXiv:1611.01989 (2016).

[3] Matej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton. 2020.
Neural program synthesis with a differentiable fixer. arXiv preprint
arXiv:2006.10924 (2020).

[4] Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen, and Ion Stoica.
2019. AutoPandas: neural-backed generators for program synthesis.
Proceedings of the ACM on Programming Languages 3, OOPSLA (2019),
1–27.

[5] Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and
Pushmeet Kohli. 2018. Leveraging grammar and reinforcement learn-
ing for neural program synthesis. arXiv preprint arXiv:1805.04276
(2018).

[6] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Execution-guided
neural program synthesis. In International Conference on Learning
Representations.

[7] Xinyun Chen, Dawn Song, and Yuandong Tian. 2021. Latent execu-
tion for neural program synthesis beyond domain-specific languages.
Advances in Neural Information Processing Systems 34 (2021).

[8] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh,
Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neu-
ral program learning under noisy i/o. In International conference on
machine learning. PMLR, 990–998.

[9] Kevin M Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Joshua Tenenbaum,
and Armando Solar-Lezama. 2019. Write, execute, assess: Program
synthesis with a repl. (2019).

[10] Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig. 2018. Program
synthesis using conflict-driven learning. ACM SIGPLAN Notices 53, 4
(2018), 420–435.

[11] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat
Chaudhuri. 2017. Component-based synthesis of table consolidation
and transformation tasks from examples. ACM SIGPLAN Notices 52, 6
(2017), 422–436.

[12] Sumit Gulwani. 2011. Automating string processing in spreadsheets
using input-output examples. ACM Sigplan Notices 46, 1 (2011), 317–
330.

[13] Steven Hardy. 1974. Automatic induction of LISP functions. In Pro-
ceedings of the 1st Summer Conference on Artificial Intelligence and
Simulation of Behaviour. 50–62.

[14] Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar

Devanbu. 2016. On the Naturalness of Software. 59, 5 (2016).

[15] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010.
Oracle-guided component-based program synthesis. In 2010 ACM/IEEE
32nd International Conference on Software Engineering, Vol. 1. IEEE,
215–224.

[16] Vu Le and Sumit Gulwani. 2014. Flashextract: A framework for data
extraction by examples. In Proceedings of the 35th ACM SIGPLAN
Conference on Programming Language Design and Implementation. 542–
553.

[17] Zohar Manna and Richard Waldinger. 1980. A deductive approach to
program synthesis. ACM Transactions on Programming Languages and
Systems (TOPLAS) 2, 1 (1980), 90–121.

[18] Daye Nam, Amber Horvath, Andrew Macvean, Brad Myers, and Bog-
dan Vasilescu. 2019. Marble: Mining for boilerplate code to identify
API usability problems. In 2019 34th IEEE/ACM International Conference

on Automated Software Engineering (ASE). IEEE, 615–627.

[19] Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-
Lezama. 2019. Learning to infer program sketches. In International
Conference on Machine Learning. PMLR, 4861–4870.

[20] Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B
Tenenbaum, and Armando Solar-Lezama. 2020. Representing Par-
arXiv preprint
tial Programs with Blended Abstract Semantics.
arXiv:2012.12964 (2020).

[21] Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles
Sutton, and Hanjun Dai. 2020. BUSTLE: Bottom-Up program synthesis
through learning-guided exploration. arXiv preprint arXiv:2007.14381
(2020).

[22] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li,
Dengyong Zhou, and Pushmeet Kohli. 2016. Neuro-symbolic program
synthesis. arXiv preprint arXiv:1611.01855 (2016).

[23] Reudismam Rolim, Gustavo Soares, Loris D’Antoni, Oleksandr Polozov,
Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Björn Hartmann. 2017.
Learning syntactic program transformations from examples. In 2017
IEEE/ACM 39th International Conference on Software Engineering (ICSE).
IEEE, 404–415.

[24] Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent
neural networks. IEEE transactions on Signal Processing 45, 11 (1997),
2673–2681.

[25] David E Shaw, William R Swartout, and C Cordell Green. 1975. Infer-
ring LISP Programs From Examples.. In IJCAI, Vol. 75. 260–267.
[26] Kensen Shi, David Bieber, and Rishabh Singh. 2020. TF-Coder: Program
Synthesis for Tensor Manipulations. arXiv preprint arXiv:2003.09040
(2020).

[27] Eui Chul Shin, Illia Polosukhin, and Dawn Song. 2018.

Improving
neural program synthesis with inferred execution traces. Advances in
Neural Information Processing Systems 31 (2018).

[28] Richard Shin, Neel Kant, Kavi Gupta, Christopher Bender, Brandon
Trabucco, Rishabh Singh, and Dawn Song. 2019. Synthetic datasets
for neural program synthesis. arXiv preprint arXiv:1912.12345 (2019).
[29] Calvin Smith and Aws Albarghouthi. 2016. MapReduce program syn-

thesis. Acm Sigplan Notices 51, 6 (2016), 326–340.

[30] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia,
and Vijay Saraswat. 2006. Combinatorial sketching for finite programs.
In Proceedings of the 12th international conference on Architectural
support for programming languages and operating systems. 404–415.

[31] Abhishek Udupa, Arun Raghavan, Jyotirmoy V Deshmukh, Sela Mador-
Haim, Milo MK Martin, and Rajeev Alur. 2013. TRANSIT: specifying
protocols with concolic snippets. ACM SIGPLAN Notices 48, 6 (2013),
287–296.

[32] Ronald J Williams and David Zipser. 1989. A learning algorithm for
continually running fully recurrent neural networks. Neural computa-
tion 1, 2 (1989), 270–280.

[33] Navid Yaghmazadeh, Xinyu Wang, and Isil Dillig. 2018. Automated
migration of hierarchical data to relational tables using programming-
by-example. Proceedings of the VLDB Endowment 11, 5 (2018), 580–593.
[34] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig.
2017. SQLizer: query synthesis from natural language. Proceedings of
the ACM on Programming Languages 1, OOPSLA (2017), 1–26.
[35] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model
for General-Purpose Code Generation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), Vol. 1. 440–450.

[36] Tianyi Zhang, Ganesha Upadhyaya, Anastasia Reinhardt, Hridesh
Rajan, and Miryung Kim. 2018. Are code examples on an online
q&a forum reliable?: a study of api misuse on stack overflow. In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE).
IEEE, 886–896.

MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

A Supported operations of PyTorch
Below is the list of 33 PyTorch operations. 16 operations used in the original dataset (described in Section 5.1) are highlighted.

• add
• any
• arange
• argmax
• bincount
• cdist
• div
• eq
• expand
• eye
• gather
• gt
• lt
• masked_select
• matmul
• max
• minimum
• mul
• ne
• one_hot
• repeat_interleave
• reshape
• roll
• searchsorted
• square
• squeeze
• stack
• sum
• tensordot
• tile
• transpose
• unsqueeze
• where

B Stack Overflow Benchmarks
As mentioned in Section 5.1, we adapted the Stack Overflow benchmarks created for TF-Coder [26]. The examples were
collected from Stack Overflow posts and the benchmarks were inspired by those posts. However, the input/output values were
replaced by the TF-Coder authors for licensing reasons. The input/output values created by TF-Coder authors, and we updated
some values to fit into our scope.

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

B.1 Input/output and Desired Code

InputsOutputCode & Function SequenceExpected SequenceSO01in1 = [[5, 2], [1, 3], [0, -1]][[[5, 5], [1, 1], [0, 0]],[[2, 2], [3, 3], [-1, -1]]]torch.transpose(in1, expand((2, 3, 2)), 0, 2)expand, transposeSO02in1 = [5, 1, 0, 3, 0, 0, 2, 0, 2]in2 = 1[1, 1, 0, 1, 0, 0, 1, 0, 1]torch.where(torch.lt(in1, 1), in1, 1)lt, whereSO05in1 = [[4, 3, 1], [6, 5, 2]]in2 = [[[5, 5]], [[1, 5]], [[6, 0]]][[[29, 35]], [[47, 55]]]torch.tensordot(in1, in2, 1)tensordotSO06in1 = [3, 5, 0, 2, 3, 3, 0][[1, 0, 0, 0, 1, 1, 0],[0, 1, 0, 0, 0, 0, 0],[0, 0, 1, 0, 0, 0, 1],[0, 0, 0, 1, 0, 0, 0],[1, 0, 0, 0, 1, 1, 0],[1, 0, 0, 0, 1, 1, 0],[0, 0, 1, 0, 0, 0, 1]]torch.eq(in1, torch.unsqueeze(in1, 1))unsqueeze, eqSO07in1 = [[[8, 4, 6], [2, 12, 3]],[[11, 12, 5], [9, 12, 12]],[[9, 2, 13], [7, 0, 7]],[[2, 10, 5], [7, 1, 2]]],[[[8, 4, 6], [11, 12, 5], [9, 2, 13], [2, 10, 5]],[[2, 12, 3], [9, 12, 12], [7, 0, 7], [7, 1, 2]]]torch.transpose(in1, 0, 1)transposeSO08in1 = [1, 0, 0, 2, 1, 3, 5, 0, 1, 2, 10],in2 = [12, 3, 45, 6, 7, 8, 9, 87, 65, 4, 32],in3 = 1[6, 8, 9, 4, 32]torch.masked_select(in2, torch.gt(in1, 1))gt, masked_selectSO11in1 = [4, 0, 1, 1, 0, 4, 0, 0, 3, 4, 1][4, 3, 0, 1, 3]torch.bincount(in1)bincountSO13in1 = [[3, 5], [10, 2]],in2 = [[[1, 0], [5, 4]], [[3, 10], [2, 0]]][[[28, 20], [19, 30]], [[20, 8], [34, 100]]]torch.transpose(torch.matmul(in1, in2), 0, 1)matmul, transposeSO14in1 = [[[0, 0, 1],           [0, 0, 0],           [1, 0, 1],           [0, 1, 0],           [0, 0, 0],           [1, 1, 1],           [1, 1, 0]]][[1, 0, 1, 1, 0, 1, 1]]torch.any(in1, -1)anySO15in1 = [3, 1, 2, 0, 1, 0, 10, 1, 0]3, 0, 2, 0, 0, 0, 10, 0, 0]torch.mul(in1, torch.ne(in1, 1))ne, mulSO16in1 = [[2, 5], [3, 0], [8, 7]],in2 = [4, 10, 6][[8, 20], [30, 0], [48, 42]]torch.mul(in1, torch.unsqueeze(in2, 1))unsqueez, mulSO17in1 = [17, 32, 99][[17, 17], [32, 32], [99, 99]]torch.stack((in1, in1), 1)stackSO18in1 = [[[1, 1, 1], [1, 0, 1]],          [[1, 2, 3], [4, 5, 6]]],in2 = [[1, 1, 1, 1], [1, 2, 3, 4], [5, 6, 7, 8]],in3 = [100, 200, 300, 400],[[[107, 209, 311, 413], [106, 207, 308, 409]],[[118, 223, 328, 433], [139, 250, 361, 472]]]torch.add(in3, torch.matmul(in1, in2))matmul, addSO20in1 = [[7, 2, 1],          [4, 5, 1],          [4, 4, 2],          [3, 4, 3],          [0, 0, 1]][[1, 0, 0],[0, 1, 0],[1, 0, 0],[0, 1, 0],[0, 0, 1]]torch.nn.functional.one_hot(torch.argmax(in1, 1), 3)argmax, one_hotSO21in1 = [[2], [0], [1], [0]],in2 = [[2, 5, 3], [1, 3, 6], [1, 6, 3], [7, 0, 3]][[3], [1], [6], [7]]torch.gather(in2, 1, in1)gatherSO22in1 = [3, 1, 10],in2 = [[6, 4], [5, 1], [3, 4]][53, 53]torch.tensordot(in1, in2, 1)tensordotSO23in1 = [[0, 5, 2], [3, 1, 4], [5, 1, 5]],[[1, 0, 1, 0, 0, 1, 0, 0, 0],[0, 1, 0, 1, 1, 0, 0, 0, 0],[0, 1, 0, 0, 0, 1, 0, 0, 0]]torch.sum(torch.nn.functional.one_hot(in1, 9), 1)one_hot, sumMAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

SO24in1 = [3, 1, 4, 5, 2, 8, 6, 7],in2 = [1, 0, 2, 0, 1, 1, 0, 2],in3 = 0[3, 1, 2, 5, 2, 8, 6, 3.5]torch.where(torch.ne(in2, in3), torch.div(in1, in2), in1)div, whereSO25in1 = 3in2 = 4[[1, 0, 0],[0, 1, 0],[0, 0, 1],[1, 0, 0],[0, 1, 0],[0, 0, 1],[1, 0, 0],[0, 1, 0],[0, 0, 1],[1, 0, 0],[0, 1, 0],[0, 0, 1]]torch.tile(torch.eye(in1), (in2, 1))eye, tileSO26in1 = [[[[3, 4], [1, 2]], [[5, 2], [10, 3]], [[10, 20], [4, 7]]]][10, 20, 41]torch.sum(torch.flatten(in1, 1), 1)sum, sumSO27in1 = [0, 3, 5, 6],in2 = 8[1, 0, 0, 1, 0, 1, 1, 0]torch.sum(torch.nn.functional.one_hot(in1, in2), 0)one_hot, sumSO29in1 = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],in2 = [12, 0, 10, 23, 16],[6, 0, 5, 11, 8]torch.searchsorted(in1, in2)searchsortedSO30in1 = [[1., 2.], [3., 4.], [5., 6.]],in2 = [[9., 4.], [8., 5.], [7., 6.]],[[math.sqrt(68), math.sqrt(58), math.sqrt(52)],[math.sqrt(36), math.sqrt(26), math.sqrt(20)],[math.sqrt(20), math.sqrt(10), math.sqrt(4)]]torch.cdist(in1, in2)cdistSO32[[1, 6, 2, 1], [3, 1, 4, 2], [2, 1, 2, 5]][1.3, 1.5, 2.0]torch.tensordot(in1, torch.arange(4), 1)arange, tensordotSO34in1 = [[[1, 2], [3, 4]],          [[5, 6], [7, 8]],          [[10, 20], [30, 40]]],in2 = [3, 5, 10],[[128, 236], [344, 452]]torch.tensordot(in2, in1, 1)tensordotSO36in1 = [1, 0, 1, 1, 0, 1, 0, 1],[1., 0., 0.333333, 0.25, 0., 0.166667, 0., 0.125]torch.div(in1, torch.add(in1, torch.arange(8)))arange, add, divSO37in1 = [[[[10, 20, 30], [40, 50, 60]],           [[12, 34, 56], [78, 98, 76]]]],in2 = [5, 10, 20],[[[850, 1900], [1520, 2890]]]torch.tensordot(in1, in2, 1)tensordotSO39in1 = [[15, 10, 9, 20], [11, 0, 1, 9], [10, 1, 11, 25]][[225, 100, 81, 400],[121, 0, 1, 81],[100, 1, 121, 625]]torch.square(torch.mul(in1, in1))mul, squareSO41in1 = [5, 2, 8, 2, 4, 1, 1, 0, 2, 1],in2 = 3[5, 2, 8, 4, 1, 1, 0, 2, 1]torch.masked_select(in1, torch.ne(torch.arange(10), in2))arange, ne, masked_selectSO42in1 = [4, 6, 2, 6, 7, 3, 3]in2 = 7[0, 0, 0, 0, 1, 0, 0]torch.eq(in1, 7)eqSO44in1 = [[3, 5, 2],          [6, 2, 3],          [8, 7, 1],          [0, 3, 5],          [4, 7, 3],          [2, 1, 6],          [10, 20, 30],          [4, 5, 6]][[9, 7, 5], [8, 19, 6], [6, 8, 9], [14, 25, 36]]torch.sum(torch.reshape(in1, (-1, 2, 8)), 1)reshape, sumSO45in1 =  [[[12, 34], [56, 78], [23, 54], [76, 78], [42, 24]]],in2 = [1, 0, 1, 0, 1],[[[34, 12], [56, 78], [54, 23], [76, 78], [24, 42]]]torch.where(torch.unsqueeze(in2, 1), torch.roll(in1, 1, -1), in1)roll, unsqueeze, whereSO46in1 = [3, 4, 1],[0, 0, 0, 1, 1, 1, 1, 2]torch.repeat_interleave(torch.arange(3), in1, 0)arange, repeat_interleaveSO48in1 = [32, 53, 45, 38, 29, 89, 64, 23],in2 = [38, 53, 89, 38, 32, 64],[3, 1, 5, 3, 0, 6]torch.argmax(torch.eq(in1, torch.unsqueeze(in2, 1)).float(), 1)unsqueez, eq, argmaxPredictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

SO49in1 = [[[[1, 2, 3], [4, 5, 6]]],          [[[8, 10, 0], [6, 4, 2]]],          [[[9, 8, 7], [1, 2, 3]]],],in2 = [20, 5, 10],[[[[20, 40, 60], [80, 100, 120]]],[[[40, 50, 0], [30, 20, 10]]],[[[90, 80, 70], [10, 20, 30]]]]torch.transpose(torch.mul(torch.transpose(in1, 0, 3), in2), 0, 3)transpose, mul, transposeSO50in1 = [3][[0, 0, 0, 1, 0, 0],[0, 0, 0, 1, 0, 0],[0, 0, 0, 1, 0, 0],[0, 0, 0, 1, 0, 0],[0, 0, 0, 1, 0, 0]]torch.nn.functional.one_hot(in1.expand(in1,)) in2)expand, one_hotMAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

B.2 Links to Original StackOverflow Posts

• https://stackoverflow.com/questions/40441503/tensorflow-tensor-reshape
• https://stackoverflow.com/questions/46408839/tensorflow-trim-values-in-tensor
• https://stackoverflow.com/questions/43067338/tensor-multiplication-in-tensorflow
• https://stackoverflow.com/questions/47816231/create-binary-tensor-from-vector-in-tensorflow
• https://stackoverflow.com/questions/38212205/swap-tensor-axes-in-tensorflow
• https://stackoverflow.com/questions/33769041/tensorflow-indexing-with-boolean-tensor
• https://stackoverflow.com/questions/45194672/how-to-count-elements-in-tensorflow-tensor
• https://stackoverflow.com/questions/50777704/n-d-tensor-matrix-multiplication-with-tensorflow
• https://stackoverflow.com/questions/35657003/aggregate-each-element-of-tensor-in-tensorflow
• https://stackoverflow.com/questions/39045797/conditional-assignment-of-tensor-values-in-tensorflow
• https://stackoverflow.com/questions/46240646/tensor-multiply-along-axis-in-tensorflow
• https://stackoverflow.com/questions/51761353/about-tensor-of-tensorflow
• https://stackoverflow.com/questions/38222126/tensorflow-efficient-way-for-tensor-multiplication
• https://stackoverflow.com/questions/44834739/argmax-on-a-tensor-and-ceiling-in-tensorflow
• https://stackoverflow.com/questions/51690095/how-to-gather-element-with-index-in-tensorflow
• https://stackoverflow.com/questions/43284897/how-can-i-multiply-a-vector-and-a-matrix-in-tensorflow-without-reshaping
• https://stackoverflow.com/questions/53414433/tensorflow-tensor-binarization
• https://stackoverflow.com/questions/53643339/tensorflow-overriding-tf-divide-to-return-the-numerator-when-dividing-

by-0

• https://stackoverflow.com/questions/53602691/duplicate-a-tensor-n-times
• https://stackoverflow.com/questions/54294780/how-to-perform-reduce-op-on-multiple-dimensions-at-once
• https://stackoverflow.com/questions/54225704/how-do-i-get-a-tensor-representing-the-on-positions-in-the-original-tensor
• https://stackoverflow.com/questions/54155085/bucketing-continous-value-tensors-in-tensorflow
• https://stackoverflow.com/questions/54147780/tensorflow-how-to-calculate-the-euclidean-distance-between-two-tensor
• https://stackoverflow.com/questions/48659449/how-to-compute-the-weighted-sum-of-a-tensor-in-tensorflow
• https://stackoverflow.com/questions/49532371/compute-a-linear-combination-of-tensors-in-tensorflow
• https://stackoverflow.com/questions/43306788/divide-elements-of-1-d-tensor-by-the-corrispondent-index
• https://stackoverflow.com/questions/49206051/multiply-4-d-tensor-with-1-d-tensor
• https://stackoverflow.com/questions/37912161/how-can-i-compute-element-wise-conditionals-on-batches-in-tensorflow
• https://stackoverflow.com/questions/54499051/elegant-way-to-access-python-list-and-tensor-in-tensorflow
• https://stackoverflow.com/questions/54493814/binary-vector-of-max
• https://stackoverflow.com/questions/54402389/sum-the-columns-for-each-two-consecutive-rows-of-a-tensor-of-3-dimensions
• https://stackoverflow.com/questions/54337925/reverse-order-of-some-elements-in-tensorflow
• https://stackoverflow.com/questions/58652161/how-to-convert-2-3-4-to-0-0-1-1-1-2-2-2-2-to-utilize-tf-math-segment-

sum

• https://stackoverflow.com/questions/58481332/getting-the-indices-of-several-elements-in-a-tensorflow-at-once
• https://stackoverflow.com/questions/58466562/given-a-batch-of-n-images-how-to-scalar-multiply-each-image-by-a-different-

scal

• https://stackoverflow.com/questions/58537495/tensorflow-initialize-a-sparse-tensor-with-only-one-line-column-not-zero

Predictive Synthesis of API-Centric Code

MAPS ’22, June 13, 2022, San Diego, CA, USA

C Implementation of ML Models
We implement the model in Python using the PyTorch. We describe some implementation details here, but the code will be
shared after anonymous period is over.

Encoding. Encoding of a tensor requires three encodings, separated by a separator: value, size, and type. The max sizes of
the encodings are 150, 5, 3, respectively. Our approach supports up to 3 input tensors and one output tensor, and each tensor is
separated by a separator, which makes the the size of input encoding to be 640 (4 ∗ (150 + 1 + 5 + 1 + 2 + 1)).

Compositional Model. For the embedding, we use the feed forward network, that is identical to the classification model,
which is trained jointly with a bi-RNN model. The embedded input-output pair is passed to bi-RNN, having 1 hidden layer. To
evaluate the model, we use a beam size of 3. We use the compositional model in two modes: “full sequence” mode, which
returns the predicted API function sequence, and a “first-of-sequence” mode that returns only the first API function from the
predicted sequence.

Multi-label Classification Model. For the weighted enumerative search with prioritization (Figure 1 (b)), we trained a
simple multi-label classification model following DeepCoder, instead of TF-Coder which uses manually defined features (e.g.,
whether a value is a primitive) which we found less generalizable. We used the same model architecture with the classification
model, but changed the last activation function into sigmoid for the multi-label classification. We trained this model with
input-output of sequences of API functions.

Evaluation Metrics. We evaluate our models on the accuracy of their predictions. For the model accuracy with synthetic
data, we check whether the model correctly predict the ground-truth APIs, and for the Stack Overflow benchmarks evaluation,
we measure the rank of the correct prediction and extract top-1, top-3 and top-10 accuracy metrics.

D Algorithms

Algorithm 1 Weighted Enumerative Synthesis
Input: A task specification input/output, (𝐼, 𝑂)
Output: A program 𝑃 such that 𝑃 (𝐼 ) = 𝑂

𝑣.𝑐𝑜𝑠𝑡 ← AssignValCost (𝑣)

1: 𝐵 ← {𝐼, 0, −1, 1, ...}
2: 𝐸 ← 𝐵
3: 𝑂𝑝𝑠 ← AssignOpCost (𝐼, 𝑂)
4: for all 𝑣 ∈ 𝐶 do
5:
6: for 𝐶 = 1 −→ 𝑚𝑎𝑥_𝑐𝑜𝑠𝑡 do
7:
8:
9:
10:
11:
12:
13:
14:

for 𝑖 = 1, ..., 𝑛 do

for all 𝑜𝑝 ∈ 𝑂𝑝𝑠 do
𝑐 ← 𝑜𝑝.𝑐𝑜𝑠𝑡
𝑛 ← 𝑜𝑝.𝑎𝑟𝑖𝑡𝑦
⊲ Partition cost budgets into 𝑛 arguments
for all [𝑐1, ..., 𝑐𝑛] ∈ 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛(𝐶 − 𝑐, 𝑛) do

⊲ Collect values satisfying i-th arg cost budget
𝐴𝑖 ← {𝑒 ∈ 𝐸|𝑒.𝑐𝑜𝑠𝑡 = 𝑐𝑖 }

15:
16:
17:

for all 𝑎𝑟𝑔𝑠 ∈ Π𝑖𝐴𝑖 do

𝑉 ← 𝐸𝑥𝑒𝑐𝑢𝑡𝑒 (𝑜𝑝, 𝑎𝑟𝑔𝑠)
if 𝑉 = 𝑂 then return V .expr
if 𝑉 ∉ 𝐸 then
𝑉 .𝑐𝑜𝑠𝑡 ← 𝐶
𝐸 ← 𝐸 ∪ {𝑉 }

18:
19:
20:
21: return "Fail: reached maximum cost"

⊲ Base values
⊲ Pool of values

⊲ Cost Budget

⊲ Run 𝑜𝑝 w/ 𝑎𝑟𝑔𝑠

MAPS ’22, June 13, 2022, San Diego, CA, USA

Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra

Algorithm 2 AssignOpCost
Input: A task specification input/output, (𝐼, 𝑂)
Output: List of operations with costs 𝑂𝑝𝑠

1: for all 𝑜𝑝 ∈ 𝑂𝑝𝑠 do
𝑜𝑝 ← 𝑝𝑟𝑒𝑠𝑒𝑡_𝑐𝑜𝑠𝑡
2:
3: if doModelPrioritization then
4:
5:
6:
7: return 𝑂𝑝𝑠

𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒_𝑜𝑝𝑠 ← MultiClassificationModel(𝐼, 𝑂)
for all 𝑜𝑝 ∈ candidate_ops do

𝑜𝑝.𝑐𝑜𝑠𝑡 ← 𝑜𝑝.𝑐𝑜𝑠𝑡 ∗ 𝑟𝑒𝑤𝑒𝑖𝑔ℎ𝑡_𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑖𝑒𝑟

Algorithm 3 Compositional Model - Full-Sequence
Input: A task specification input/output, (𝐼, 𝑂)
Output: A program 𝑃 such that 𝑃 (𝐼 ) = 𝑂

1: 𝐵 ← {𝐼, 0, −1, 1, ...}
2: 𝑜𝑝_𝑠𝑒𝑞 ← CompositionalModel(𝐼, 𝑂)
3: 𝑛 ← (cid:205) 𝑜𝑝𝑖 .𝑎𝑟𝑖𝑡𝑦
4: 𝑎𝑟𝑔𝑠_𝑙𝑖𝑠𝑡 ← Π𝑛𝐵
5: for all 𝑎𝑟𝑔𝑠 ∈ 𝑎𝑟𝑔𝑠_𝑙𝑖𝑠𝑡 do
6:
7:
8: return "Fail" or start "Enumerative Search"

𝑉 ← 𝐸𝑥𝑒𝑐𝑢𝑡𝑒 (𝑜𝑝_𝑠𝑒𝑞, 𝑎𝑟𝑔𝑠)
if 𝑉 = 𝑂 then return V .expr

Algorithm 4 Compositional Model - First-Of-Sequence
Input: A task specification input/output, (𝐼, 𝑂)
Output: A program 𝑃 such that 𝑃 (𝐼 ) = 𝑂

1: 𝐵 ← {𝐼, 0, −1, 1, ...}
2: for 𝑖 = 0 −→ 𝑘 do
3:
4:
5:
6:
7:
8:
9: return "Fail" or start "Enumerative Search"

𝑜𝑝𝑖 ← 𝐶𝑜𝑚𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙𝑀𝑜𝑑𝑒𝑙 (𝑉𝑖−1, 𝐼𝑖, 𝑂)
𝑛 ← 𝑜𝑝𝑖 .𝑎𝑟𝑖𝑡𝑦
𝑎𝑟𝑔𝑠_𝑙𝑖𝑠𝑡 ← Π𝑛𝐵
for all 𝑎𝑟𝑔𝑠 ∈ 𝑎𝑟𝑔𝑠_𝑙𝑖𝑠𝑡 do
𝑉𝑖 ← 𝐸𝑥𝑒𝑐𝑢𝑡𝑒 (𝑜𝑝𝑖, 𝑎𝑟𝑔𝑠)
if 𝑉𝑖 = 𝑂 then return V .expr

⊲ Sequence of 𝑘 operations

