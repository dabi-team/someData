2
2
0
2

r
a

M
9
2

]
L
P
.
s
c
[

1
v
6
2
4
5
1
.
3
0
2
2
:
v
i
X
r
a

On Reinforcement Learning, Eﬀect Handlers, and the State
Monad

Ugo Dal Lago

Francesco Gavazzo

Alexis Ghyselen

Abstract

We study the algebraic eﬀects and handlers as a way to support decision-making abstractions
in functional programs, whereas a user can ask a learning algorithm to resolve choices without
implementing the underlying selection mechanism, and give a feedback by way of rewards. Diﬀerently
from some recently proposed approach to the problem based on the selection monad [2], we express
the underlying intelligence as a reinforcement learning algorithm implemented as a set of handlers
for some of these algebraic operations, including those for choices and rewards. We show how we can
in practice use algebraic operations and handlers — as available in the programming language EFF
— to clearly separate the learning algorithm from its environment, thus allowing for a good level
of modularity. We then show how the host language can be taken as a λ-calculus with handlers,
this way showing what the essential linguistic features are. We conclude by hinting at how type and
eﬀect systems could ensure safety properties, at the same time pointing at some directions for further
work.

1 Introduction

Machine learning is having, and will likely have more and more, a tremendous impact on the way
computational problems (e.g. classiﬁcation or clustering) are solved. Learning techniques, however, turn
out to be very fruitful when solving control problems, too. There, in fact, an agent’s goal is to learn
how to maximize its reward while interacting with the environment rather than while computing a mere
function. From this point of view, the so-called reinforcement learning techniques [34] are proving to be
particularly appropriate in many contexts where exploration and optimization have to be interleaved.

Prompted by that, in the past decade there has been an incredible eﬀort to develop programming
languages and programming language techniques oriented to the design of machine learning systems.
The outcome of such an eﬀort is well-known and gave birth to new programming language paradigms,
such as Bayesian [18, 39] and diﬀerentiable programming [1, 25, 33], as well as to the ﬂourishing ﬁeld
of programming languages for inference.1 Despite the incredible strides made, machine learning support
from programming languages is still in its infancy and its deliverables mostly consist of a set of general-
purpose programming language libraries (such as Theano [7], TensorFlow [3], and Edward [36, 37], just to
mention but a few) and domain-speciﬁc languages (such as Anglican [35], Pyro [9], and Stan [11], just to
mention but a few).

In this work, we deal with a further programming language paradigm oriented to machine learning:
choice-based programming. The latter moves from the observation that many machine learning sys-
tems — especially those pertaining the realm of reinforcement learning — can be described in terms of
choices, costs, and rewards. Prompted by that, choice-based programming languages2 extend traditional
programming languages with high-level decision-making abstractions that allow for the modular design
of programs in terms of choices and rewards. While in a probabilistic language programs are structured
in terms of sampling and observing — leaving the actual inference process to the interpreter — in a
choice-based language the code is structured by specifying where a choice should be made and what its
associated cost (or, dually, its reward) is, leaving the actual decision-making process to the interpreter.
All of that considerably changes the way one writes and thinks about software, at the same time
raising new — and challenging — questions both from the point of view of programming language
theory and of machine learning. In the former case, in fact, we have to deal with the introduction of

1See, e.g., the dedicated POPL Workshop LAFI https://popl21.sigplan.org/home/lafi-2021.
2Such as SmartChoice [10].

1

 
 
 
 
 
 
decision-making abstractions, their implementation, and their semantics. In the latter case, instead, we
have just began to realise how to tackle modularity of machine learning systems using programming
language-based techniques.

The latter point is the main topic of this paper. We move from the recent work by Abadi and Plotkin
[2], where a monadic [22] approach to choice-based programming is developed. There, the authors show
how the so-called selection monad [14, 16, 15] can be used to model (and to give semantics to) programs
written in a choice-based language as eﬀectful programs, and how it is possible to manage choices as
algebraic operations [27, 26, 28], delegating the task of solving these choices to those who implement the
selection mechanism. In this work, we are concerned with further developing this idea, although in a
diﬀerent direction. In fact, even if we still deal with monadic and algebraic approaches to choice-based
programming, we explore the use of the state — rather than selection — monad in the framework of
reinforcement learning systems. In particular, we show how the use of the state monad allows for a high
level of modularity in the construction of systems based on reinforcement learning.

The reader, at this point, may wonder why one should consider a further kind of monadic programming
— viz. one based on the state monad — when dealing with reinforcement learning systems . The next two
sections are dedicated to answer this question. There, we will study a simple example coming from the
reinforcement learning literature, highlighting some drawbacks of the selection monad, on the one hand,
and the strengths of our state-based approach, on the other hand. That will also allow us to illustrate
the practical advantages of functional programming techniques in reinforcement learning modelling.

For the moment, we simply remark that at the very heart of our approach lies a clear separation
between the part of the program dealing with the learning algorithm and the one that handles the
interaction with the environment. Such a separation can be naturally structured in the form of a state
monad with choice and reward operations acting as algebraic operations. However — and here comes the
main diﬀerence between our approach and the one based on the selection monad — the action of such
operations is not determined by the monad (as de facto happens when working with the selection monad),
but it is ultimately given by the reinforcement learning algorithm used. As a consequence, one can
model reinforcement learning systems modularly as monadic, state-based programs written using choice
and reward operations, and then view reinforcement learning algorithms as algebraic interpretations
of such operations. Concretely, that means that we can view (and implement) reinforcement learning
algorithms as handlers [29, 5, 31] giving interpretations to choice and reward operations. Diﬀerent
learning algorithms then give diﬀerent handlers — and thus diﬀerent interpretations of choices and
rewards — so that it is possible to instantiate the very same program with diﬀerent learning algorithms
by simply changing the way its algebraic operations are handled.

Summing up, the main contributions of this paper are the following:

• The development of a modular approach to reinforcement learning systems throughout functional
programming language techniques. Among such techniques, the main role is played by the state
monad and its associated algebraic operations for performing choices and rewards. Crucially,
the use of the state monad (as opposed to the selection monad) allows us to consider diﬀerent
interpretations of choices and rewards, such interpretations ultimately being the reinforcement
learning algorithm used. We make use of handlers to implement these interpretations in a modular
fashion.

• The analysis of our approach in a core λ-calculus with handlers and algebraic operations. This
allows us to isolate the exact features a language needs to have in order to support our functional
approach to reinforcement learning systems.

• A preliminary study of how to use semantic-based techniques (notably polymorphic and graded
type systems) to ensure correct behaviours of reinforcement learning systems when implemented
in a functional way.

2 Structuring Reinforcement Learning Applications Through Func-

tional Programming

To begin with, we are going to illustrate our approach on a simple running example, common in the
reinforcement learning literature [34], namely the so called multi-armed bandit problem, denoted MAB.

2

Environment

observe

Learner

ORL : Abstract Observations
ARL : Abstract Actions

choice

do

User
User

reward

OE : Observations
OE : Observations
AE : Actions
AE : Actions

Abstract Interface

Figure 1: Our approach

In this setting, a gambler faces a row of k slot machines, each of which distributes rewards in the
form of winnings according to a probabilistic model unknown to the gambler. The gambler, it goes
without saying, wants to maximize its gains over a ﬁxed number of rounds. At each round, the gambler
chooses a machine and obtains a reward depending on its reward distribution. On the one hand, then, the
gambler wants to maximize her gains, but on the other she knows nothing about the inner working of the
machines. This example, indeed, expresses the need of a trade-oﬀ between exploration and optimization:
the gambler wants to play on the best machine as much as possible, but in order to ﬁnd this best
machine, she needs to try every machine. Moreover, since rewards are random, playing once on each
machine, then playing only on the one with the best observed reward, may not be optimal in the long run.
Reinforcement learning indeed focuses in oﬀering techniques which explores this trade-oﬀ in a meaningful
way.

What we are interested in doing here is deriving a strategy for the agent in the aformentioned
problem using reinforcement learning techniques, and with the help of functional programming. We
are not interested at devising new algorithms, but rather at showing that functional programming can
help in giving structure to programs which comprise both the proper learning algorithm, but also the
interaction with the environment.

Conceptually, it is natural to see a program solving MAB by way of reinforcement learning as struc-

tured into three parts:

• The ﬁrst one, the environment, serves as an interface with the outside world, making it visible to
the program. Here, this part comprises the k slot machines, possibly through a system of sensors
and functions querying those sensors.

• The second one, the learner, which provides generic reinforcement learning algorithms, possibly
through libraries. There are many algorithms the literature oﬀers, for example gradient learning,
Q-learning, expected Sarsa or other TD-methods [34].

• The user, namely some code letting the environment and the learner exchange information, thus
acting as a bridge between the two. Here, the user is supposed to turn events and data coming
from the environment into a form which can be understood by the learner.

One of the key ingredients of our proposal consists in structuring these three parts and their interac-
tion by stipulating that the user proceeds by invoking some algebraic operations provided by both the
environment and the learner. This is schematized in Figure 1. The main algebraic operations are the
four mentioned in the ﬁgure, two of them provided by the environment and two provided by the learner.
The two algebraic operations provided by the environment are observe and do. The former, having
type 1   OE allows the user to retrieve some data from the environment. In our multi-armed bandit
example, it corresponds to observing the gain on the current slot machine. The operation do : AE   1
executes an action in the environment. In our running example, an action would be the choice of one
of the k slot machines. Those operations depend the set of possible observations and the set of possible
actions in the environment, those sets being represented by two types OE and AE in the user code. In
our example, the observations are the gains, thus we can give the type OE = Real. As for actions, they
stand for choices of a slot machine, and thus AE = {1, . . . , k}.

3

Then, there are two other operations for the learner, namely choice and reward. The operation
choice : 1   AE asks the learner to take an action for us.
In our setting it means that a call to
choice returns an integer between 1 and k designing the slot machine the user should play on. The
reward : Real   1 operation allows the user to give a feedback to the learner depending on its previous
choice. In our example, the typical reward would be the gain obtained from the slot machine chosen
by the learner. There is also an additional bridge between the user and the learner, that we call the
abstract interface: the learner may need more information than the feedback given by reward, and
this information will be transmitted using the abstract interface by way of some additional algebraic
operations invoked by the learner but handled by the user.

One advantage of the just described approach is to show that this learner part of the program, i.e.,
the handler for the operations choice and reward, can be implemented independently from the details of
the environment, once and for all. One could even conceive to have a library of handlers, where each set
of handlers corresponds to a reinforcement learning algorithm, in such a way that the user could choose
one of these handlers for each of the environments he has access to, implementing the abstract interface
but without delving into the details of the underlying algorithms. Similarly, implementing any new RL
algorithm would boil down to just write a new set of handlers for choice and reward.

In which order should the user invoke the various algebraic operations? A typical sequence of in-
teractions would be choice → do → observe → reward, meaning intuitively that it asks for a choice,
and then does explicitly this action in the environment, observes the results of this action, and ﬁnally
produces a reward. As we will soon see, it is preferable that the user indeed respects this order, e.g.,
when handling the reward operation, the learner may need to observe the environment (abstracted by
the interface) since a valuation function typically depends on the state reached after an action.

In order to have a learner independent from the environment, it must not make explicit reference to
the types AE and OE, as we saw on MAB. Thus, we introduce two ﬁnite sets ORL and ARL corresponding
to the abstract sets of observations and actions, respectively. Except for their sizes, those ﬁnite sets must
be totally independent form the environment. It means that in practice, the learner only has access to
some elements of those sets, and should make choices in ARL depending only on the abstract observations
from ORL it receives from the user. Typically, both the policy to choose actions and the learning part are
built by constructing and updating a value function mapping pairs in ORL × ARL to a reward estimation,
for example an element of Real.

2.1 A Naive RL Algorithm in EFF

We present our approach practically with a very basic algorithm for reinforcement learning, which works
by only remembering an evaluation function keeping track of the expectation of the immediate rewards
it receives for any of the actions, and chooses an action by way of a so-called ε-greedy policy [34]:
with probability (1 − ε), it makes the best possible choice based on the current valuation, and with
probability ε it explores by choosing an action uniformly at random. To implement this algorithm, we
use the language EFF [31], an OCAML-based language for eﬀects and handlers. The syntax of EFF should
be understandable to anyone with some basic knowledge on OCAML, eﬀects and handlers. We use lists for
the sake of simplicity, although other kinds of data structures would enable better performances. The
source code as well as other examples can be found in [13].

Declaring The Abstract Interface. The ﬁrst step towards implementing the RL algorithm consists
in declaring the sets ORL and ARL together with the abstract interface which will allow us to recover
some information on the sets ORL and ARL:

ty pe r l _ a c t
ty pe r l _ o b s
e f f e c t
e f f e c t

r l _ o b s e r v e : u n i t
r l _ g e t a v a i l a b l e a c t

7→ r l _ o b s

:

r l _ o b s 7→ r l _ a c t

l i s t

Here, the ﬁrst eﬀect allows the learner to observe the environment (after the abstraction) and to get,
for each observation, the list of available actions it has to choose from. As we stated before, the learner
does not have access to the interpretation of those two eﬀects nor to the actual types, and only knows
that those sets of abstract actions and observations are ﬁnite, this being enough to implement the
reinforcement learning algorithm.

4

Handling Choices and Rewards. The handler makes essential use of the state monad, where the
state represents the memory of the learner. For the speciﬁc RL algorithm we are targeting now, this
memory consists of an element of this type:

ty pe memory = ( ( r l _ o b s ∗ ( ( r l _ a c t ∗ i n t ∗ f l o a t )

l i s t ) )

l i s t ) ∗ i n t ∗ i n t

The left type of the internal memory corresponds to a value function, for each pair of an observable and
an action, we give an estimation of the immediate reward we can obtain. It is computed as the average
reward, and in order to do this average incrementally, it is common to also remember the number of
times a choice has been made. Then, the internal memory also remembers the last choice made using a
pair of integers denoting indexes in the evaluation function (we usually denote na the index of an action
and no the index of an observation). Then, we can implement the ε-greedy policy of the learner (we only
describe the important functions and not the simple intermediate one).

i n p u t a p r o b a b i l i t y and a l i s t o f

reward e s t i m a t i o n

f u n c t i o n t a k e s a s

( ∗ This
and r e t u r n s
l e t g r e e d y p o l i c y ( ǫ , l ) =

t h e s e l e c t e d a c t i o n and i t s

( ( randomfloat 1 . ) <= ǫ )

i f
( ∗ Uniform c h o i c e i n t h i s c a s e ∗ )
b e g i n

then

l e t na = randomint ( l i s t _ l e n g t h l )
( ∗ Find t h e a c t i o n with i n d e x na ∗ )
l e t a = f i n d a c t
( na , a )

l na i n

i n d e x i n t h e

l i s t ∗ )

i n

end
e l s e

( ∗ S e l e c t
argmax l

; ;

t h e a c t i o n with t h e maximal e s t i m a t e d reward ∗ )

And with this, and some other auxiliary functions, we can deﬁne the handler for the basic RL algorithm.
The only non-standard clause for the handler is the last one, starting with finally, in this setting with
a state monad, it should be understood as the initial state for a computation.

( ∗ The f i r s t
t h e s e c o n d i s
l e t

i n p u t
t h e
r l _ n a i v e ǫ v = h a n d l e r

i s
i n i t i a l

t h e p r o b a b i l i t y o f e x p l o r i n g ,

e s t i m a t i o n ∗ )

l i s t o f e s t i m a t i o n s q ∗ )

( ∗ d e c l a r e a s t a t e monad , with t h e ty pe d e s c r i b e d above ∗ )
| y 7→ ( fun (_: memory )
|

7→ y )

7→

t h e a c t i o n a with t h e g r e e d y p o l i c y ∗ )

i n

l o v i n

( na , a ) = g r e e d y p o l i c y ( ǫ , q )

( l ’ , no , q ) = g e t s t a t e e s t i m a t e

t h e i n d e x no f o r o , with i t s

e f f e c t Choice k 7→ fun ( l , _,_)
( ∗ u s e t h e i n t e r f a c e t o g e t an o b s e r v a t i o n ∗ )
l e t o = r l _ o b s e r v e ( )
( ∗ e x t r a c t
l e t
( ∗ s e l e c t
l e t
( ∗ update t h e e s t i m a t i o n s ∗ )
l ’ ’ = u p d a t e s t a t e
l e t
( ∗ r e t u r n a c t i o n a , with t h e updated memory ∗ )
( c o n t i n u e k a )
e f f e c t
( ∗ update e s t i m a t i o n s
l ’ = u p d a t e s t a t e
l e t
( ∗ g i v e t h e new memory t o t h e c o n t i n u a t i o n ∗ )
( c o n t i n u e k ( ) )
( l ’ , no , na )
( ∗ i n i t i a l memory ∗ )
|

(Reward r ) k 7→ fun ( l , no , na )

l ’ no ( fun l l

( l ’ ’ , no , na )

f i n a l l y f

( [ ] , 0 , 0 )

7→ f

7→

i n

|

t h e p r e v i o u s c h o i c e ( no , na ) ∗ )

f o r
l no ( fun q 7→ updatereward q na r )

i n

7→ u p d a t e c h o i c e

l l na )

i n

; ;

Note that we initialize the estimations lists only when we see an element of ORL for the ﬁrst time. This
is standard in RL because there could be an extremely large set of states, and it may well be that not all
of them are reached — it may be better to give an initial estimation to a state only when we actually see

5

this state (in this program, this is done by the geststateestimate function). We do not give the code
of the update functions, which is anyway easy to write. The nice thing of this handler is that it does not
depend on the environment, it can of course interact with its environment (using rl_observe) but it
does so in a modular way, so that this program can be used in any environment in which we would like
to experiment this (admittedly naive) algorithm.

2.2 The Multi-Armed Bandit in EFF

We show how to make use of the RL algorithm described in the previous section on the environment
coming from MAB. The ﬁrst step consists in declaring the types for OE and AE. Since rewards are
earnings, and our actions are nothing more than a choice of a speciﬁc machine, we can proceed as
follows:

ty pe env_obs = f l o a t
ty pe env_act = i n t

Modeling the Environment We model MAB as a program, where we take a very simple distribution
of rewards for the sake of the example. As stated before, the environment correspond to the handling of
observe and do.

ty pe e n v _ s t a t e = f l o a t

( ∗ The random reward o f
In a r e a l c a s e ,
by o b s e r v i n g t h e r e s u l t o f
l e t g e t r e w a r d ( a : a c t ) = ( f l o a t _ o f _ i n t a ) +. ( randomfloat 1 0 . )

reward s h o u l d be o b t a i n e d
t h e s l o t machine ∗ )

t h e machine a .

t h i s

( ∗max c o r r e s p o n d s
l e t MAB_handler max = h a n d l e r
| y 7→ ( fun (_: e n v _ s t a t e )
|

(Do a ) k 7→ fun _ 7→

t o t h e number o f

7→ y )

s l o t machines ∗ )

e f f e c t
( ∗ When s e e i n g a v a l i d a c t i o n a , we compute t h e g a i n f o r
s l o t machine a and s t o r e t h e r e s u l t
i f
then ( c o n t i n u e k ( ) )
e l s e r a i s e " This a c t i o n i s not a v a i l a b l e ! \n"

( a > 0 ) && ( a <= max )

i n t h e memory ∗ )

( g e t r e w a r d a )

; ;

t h e

( ∗An o b s e r v a t i o n c o r r e s p o n d s
e f f e c t Observe k 7→ fun r
|
( ∗ I n i t i a l
|

f i n a l l y f

7→ f 0 .

env ironment , no r e w a r d s o b s e r v e d ∗ )

t o showing t h e s t o r e d r e s u l t ∗)

7→ ( c o n t i n u e k r )

r

; ;

Implementing the Abstract Interface The abstract interface is a handler that implements the
types ARL and ORL, and handles the algebraic operations declared by the learner:

t h e l e a r n e r has no i n f o r m a t i o n on t h e env ironment ∗ )

t h a t

f o r o b s e r v a t i o n s

( ∗ A b s t r a c t i o n s Types . The ty pe u n i t
means
ty pe r l _ o b s = u n i t
ty pe r l _ a c t = i n t
e f f e c t
e f f e c t
( ∗ Transform a s t a n d a r d o b s e r v a t i o n i n t o an a b s t r a c t one ∗ )
l e t a b s t r a c t o b s

r l _ o b s e r v e :
r l _ g e t a v a i l a b l e a c t

r l _ o b s 7→ r l _ a c t

( o : env_obs )

: obs = ( )

r l _ o b s

l i s t

; ;

; ;

; ;

:

( ∗ The h a n d l e r d e s c r i b e s :

− t h e a b s t r a c t i o n o f o b s e r v a t i o n s
− t h e a c t i o n s a v a i l a b l e t o t h e l e a r n e r ∗ )

l e t abs_MAB max =
l e t
h a n d l e r

l = l i s t _ e n u m e r a t e 1 (max + 1 )

i n

6

r l _ o b s e r v e k 7→ l e t o = observe ( )

i n

e f f e c t
c o n t i n u e k ( a b s t r a c t o b s o )
e f f e c t

( r l _ g e t a v a i l a b l e a c t o ) k 7→ c o n t i n u e k l

|

|

; ;

The Main Program With this, we have everything we need to handle the four main operations. In
order to use the learner described above, we can just open the ﬁle in which the RL algorithm is deﬁned
and use it as a handler, with the interface described above. For example, we can write the main program:

#u s e "MAB_Environment. e f f " ; ;
#u s e "RL_Naive . e f f " ; ;

t h e i n t e r f a c e t o t h e l e a r n e r ∗ )

( ∗ Multi −Armed Bandit with 6 machines ∗ )
with ( MAB_handler 6 ) h a n d l e
( ∗ P r o v i d e s
with (abs_MAB 6 ) h a n d l e
( ∗ C a l l
with ( r l _ n a i v e 0 . 0 5 1 0 . ) h a n d l e
( ∗ S t a r t w r i t i n g y our progam with a l g e b r a i c o p e r a t i o n s
Here , we do 500 rounds ∗ )
r e c run n r =

t h e B a s i c RL a l g o r i t h m d e s c r i b e d p r e v i o u s l y ∗ )

l e t

i f n = 0 then r e l s e
l e t a = choice ( )
l e t
reward r ’ ;
i n run 500 0 .

r ’ = observe ( )

i n (do a ) ;

i n
run ( n−1) ( r +. r ’ )

; ;

And the point is that if we want to use another learner we only have to load a diﬀerent handler for
the learner, and this naive learner can be used in any environment as long as the abstract interface is
handled.

3 The Selection Monad, and Why it is Not an Answer

To gain a better understanding of the diﬀerences between our state-based approach and the selection-
based approach by Abadi and Plotkin [2], let us illustrate some of the main drawbacks exhibited by the
selection monad when applied to MAB. To do so, let us ﬁrst shortly recap the underlying mechanism
behind such a monad.

The selection monad is deﬁned through the functor S(X) = (X → R) → X, where R is a (usually
ordered) set of rewards. Intuitively, a computation in S(X) takes in input a reward function f : X → R
associating to each element in X a reward in R (we can think about f (x) as a measure of the goodness
of x), and chooses an element in X that, intuitively, is optimal for f . By its very deﬁnition, even if the
set R of rewards is a parameter of the selection monad, the latter does not have direct access to it: the
only way to interact with rewards is through a reward function, meaning that the selection monad, by
itself, does not handle rewards directly. Abadi and Plotkin [2] overcome this problem by combining the
selection monad with another monad T giving direct access to rewards, this way obtaining (a monad
whose carrier is) the functor

ST (X) = (X → R) → T (X).

Here, T can for example be the R-based writer monad T (X) = R × X — this way, rewards can be found
in the ﬁrst component of the result whenever a choice is made. One can even go beyond that and take
T (X) = Dﬁn (R × X), where Dﬁn is the ﬁnite distribution monad, this way modeling stochastic rewards.
Using this strategy — i.e. working with ST rather than with S alone — we can indeed model
(stochastic) rewards as prescribed by the multi-armed bandit problem. For that, we just consider the
monad (X → R) → Dﬁn (R × X). The last ingredient needed to attempt a selection monad-based
model to MAB is to give choice operations, which ultimately constitute the way to construct selection
computations. As the reader may guess, this is the most delicate point.

7

The selection monad comes with a choice operation choice which given two selection computations
a and b in S(X), returns a new computation choice(a, b) belonging to S(X) working as follow: given
a reward function f : X → R, choice(a, b) passes f to both a and b, this way obtaining two candidate
optimal elements a(f ) and b(f ), and then chooses between the latter on the basis of f . More precisely,
given a(f ) and b(f ), we can obtain a reward for each of them as the elements f (a(f )) and f (b(f )) in R.
Assuming R to come with a binary relation (cid:22) ranking rewards, we then let choice(a, b)(f ) to return
the best one between a(f ) and b(f ) according to (cid:22).

At this point, we can already start perceiving the main issue behind choice operations: to choose
between two computations, we have to simulate both of them, and then take the optimal one. In any
case, this concerns the selection monad S alone. What about the monad ST , i.e. the combination of the
selection monad with a monad T giving access to rewards? Here, the situation is slightly more compli-
cated, but the basic mechanism behind choice operations is essentially the same one for S. Proceeding
as for the latter, we obtain elements a(f ) and b(f ) belonging to T (X): this time, however, we cannot
directly apply the reward function f on them. The solution proposed by Abadi and Plotkin [2] is to
require to have a T -algebra α : T (R) → R, so that we can apply T (f ) — rather than f — on a(f ) and
b(f ), this way obtaining monadic rewards T (f )(a(f )), T (f )(b(f )) in T (R); map them into R using α;
and then choose the optimal one according to the order (cid:22) on R. For instance, taking real numbers as
rewards, we can take, e.g., α : R × R → R as real-number by addition.

Notice that dealing with choice operations this way, we not only still have the same problem seen for
S — namely that choosing between computations require to simulate all of them — but we also have
to compute monadic applications of f . That is, the application of T (f ) to an element φ ∈ T (X) usually
requires to compute the reward f (x) for each element x which is, intuitively, part of φ. For instance,
if we think about φ as a distribution, then computing T (f )(φ) requires to compute the reward of each
element in the support of φ. All of that does not ﬁt well with the very essence of reinforcement learning,
as we are going to see.

Let us now come to reinforcement learning systems and to the multi-armed bandit example, high-
lighting the main problem behind the semantic mechanism of choice operations. First, let us notice that
the selection monad seems to provide an adequate setting for reinforcement learning. In fact, taking the
monad (X → R) → Dﬁn (R × X), we obtain choices and (stochastic) rewards, and the former can in prin-
ciple depend on the latter. In such a setting, the choice operation is deﬁned parametrically with respect
to an algebra α : Dﬁn (R × R) → R, which can be naturally deﬁned as follows: given a distribution φ, we
ﬁrst apply addition to all the elements in the support of φ, and then compute the resulting expectation.
Given such an algebra α, to perform a choice operation choose between two computations a and b with
a reward function f , we ﬁrst compute the probability distributions a(f ) and b(f ), and then compute the
expected reward associated to each such a distribution, which means ﬁrst computing the reward of each
element in its support.

The latter passage is not reasonable from reinforcement learning perspective. In fact, when dealing
with reinforcement learning systems, implementing choices operations this way forces us to (i) simulate
all actions; (ii) to obtain a perfect knowledge of the environment; (iii) and to make (optimal) choices
based on that. In MAB, that means (i) simulating playing on each machine; (ii) observing the whole
distributions associated to each machine; (iii) and then select the one with the best expectation, which
in turn requires to compute the rewards of each element in the support of the distribution of a machine.
All of that is simply to strong to give a reasonable model of reinforcement learning systems. Notice that
all of that is essentially independent of the choice of the algebra α, meaning that the problem lies at
the very hearth of he selection monad (and its choice operations) rather than on the concrete way one
aggregates monadic rewards.

4 From Practice to Theory: Eﬀects and Handlers

In this section, we try to inject the ideas we developed in Section 2 into a paradigmatic programming
language, so as to be able to isolate the features we deem necessary.

8

(Values) V ::= x | λx.C | (V, V ) | () | c | f | H

(Handlers) H ::= handler {return x 7→ C, op1(x; k) 7→ C, . . . , opn(x; k) 7→ C}

(Computations) C ::= return V | πi(V ) | op(V ; x.C) | let x = C in C | V V | with V handle C

(Types) T ::= B | Unit | T × T | T → T | T ⇒ T

Figure 2: Syntax and Types of Terms

4.1 A Core Language with Eﬀects and Handlers

For the sake of simplicity, we take the core language described in [31] with additional base types cor-
responding to observations and actions and a simple type system without type eﬀects. We will not
talk about the details of the underlying eﬀect, such as randomness, I/O, or exceptions. Those eﬀects
are obviously needed in practice, but adding them to the theory would be standard, so for the sake of
simplicity we ignore those, and when we write a type T → T ′, this function should be understood as a
function that can use those standard eﬀects.

The grammar for terms and types is in Figure 2. In other words, we work with a λ-calculus with pairs
base types (ranged over by B), constants and functions for those base types, denoted by c and f . Any
symbol c ∈ C is associated to a base type B, while any symbol f ∈ F comes equipped with a function
type B1 × · · · × Bn → B. Moreover, we have algebraic operations, each operation symbol op coming with
a type op : Tp   Ta ∈ Σ where Tp is the type of parameters and Ta is the arity. In a computation
op(V ; x.C), the variable x is bound in C. Those operations are handled by handlers. An handler has
the following form

handler {return x 7→ Cr, op1(x; k) 7→ C1, . . . , opn(x; k) 7→ Cn}

Here, the computations C1, . . . , Cn are pieces of code meant to handle the corresponding algebraic
operation, while Cr is meant to handle a return clause. We use the arrow ⇒ to denote handler types,
contrary to the function type that uses →. When one wants to use the aforementioned handler for the
purpose of managing some algebraic operations, we do by way of a term in the form

with V handle C

in which C is executed in a protected environment such that any algebraic operations produced by C is
handled by the handler V , provided it is one among those declared in it.

The typing rules for this language are given in Figure 3. In a computation op(V ; x.C), C can be
seen as a continuation for the computation, with type Ta → T , this is why in the typing of handler, the
second parameter k has this type. The typing rule for the handler with simple types looks like a function
application, where the handler transforms a computation of type T2 to a computation of type T1.

Then, the dynamic semantics is given in Figure 4. Algebraic operations can commute with the let
constructor and handlers for other operations. As for handlers, the return computation is handled by the
return clause, and an algebraic operation is handled by the corresponding computation in the handler,
where the continuation k is replaced by the actual continuation C with the same handler.

With this, we have a complete description of a core language with eﬀects, handlers and base types.
In practice, we also want other standard constructors for booleans (if then else), lists or other data
structures, but this can be easily added to the core language without any theoretical diﬃculties. It is
then relatively standard to prove the subject reduction of this type system, proving some simple kind of
safety:

Theorem 1 (Subject Reduction). If Γ ⊢ C : T and C → C′ then Γ ⊢ C′ : T

Proof. The proof is standard. We start by proving a weakening lemma (if Γ ⊢ C : T then Γ, x : T ′ ⊢ C : T )
and a value substitution lemma (if Γ, x : T ⊢ C : T ′ and Γ ⊢ V : T then Γ ⊢ C[V /x] : T ′) by induction on

9

Γ, x : T ⊢ x : T

Γ, x : T1 ⊢ C : T2
Γ ⊢ λx.C : T1 → T2

Γ ⊢ V1 : T1

Γ ⊢ V2 : T2

Γ ⊢ (V1, V2) : T1 × T2

Γ ⊢ () : Unit

c : B ∈ C
Γ ⊢ c : B

f : B1 × · · · Bn → B ∈ F
Γ ⊢ f : B1 × · · · Bn → B

{(opi : T p

i

  T a

i ) ∈ Σ

Γ, x : T p

i , k : T a

i → Tc ⊢ Ci : Tc}1≤i≤n

Γ, x : Tv ⊢ Cr : Tc

Γ ⊢ handler {return x 7→ Cr, op1(x; k) 7→ C1, . . . , opn(x; k) 7→ Cn} : Tv ⇒ Tc

Γ ⊢ V : T
Γ ⊢ return V : T

Γ ⊢ V : T1 × T2
Γ ⊢ πi(V ) : Ti

(op : Tp   Ta) ∈ Σ

Γ ⊢ V : Tp

Γ, x : Ta ⊢ C : T

Γ ⊢ op(V ; x.C) : T

Γ ⊢ C1 : T1

Γ, x : T1 ⊢ C2 : T2

Γ ⊢ V1 : T2 → T1

Γ ⊢ V2 : T2

Γ ⊢ let x = C1 in C2 : T2

Γ ⊢ V1 V2 : T1

Γ ⊢ V : T2 ⇒ T1

Γ ⊢ C : T2

Γ ⊢ with V handle C : T1

Figure 3: Typing Rules

πi(V1, V2) → Vi

C1 → C

′
1

let x = C1 in C2 → let x = C

′
1 in C2

let x = op(V ; y.C1) in C2 → op(V ; y.let x = C1 in C2)

let x = return V in C2 → C2[V /x]

(λx.C) V → C[V /x]

f V → c

V = (c1, . . . , cn)

c = f (c1, . . . , cn)

For the following rules, we denote H = handler {return x 7→ Cr, op1(x; k) 7→ C1, . . . , opn(x; k) 7→ Cn}

C → C
with H handle C → with H handle C

′

′

op /∈ {op1, . . . , opn}
with H handle op(V ; x.C) → op(V ; x.with H handle C)

with H handle return V → Cr[V /x]

with H handle opi(V ; x.C) → Ci[V /x][λx.with H handle C/k]

Figure 4: Semantics

10

judgment, and then we can prove subject reduction by induction on the relation C → C′. The weakening
lemma is useful for the cases when an algebraic operation op commute with a let or an handle, and the
substitution lemma is useful for substitutions, which are always for values as one can see in Figure 4.

However, with those simple types it is not possible to prove that any typable computation reduces
to a computation of the shape return V , because a non handled operation op cannot be reduced. To
have those kind of safety theorems, we need type eﬀects, as we will see in the next section.

4.2 Setting the Stage: Types and Algebraic Operations for RL

Let us now instantiate more clearly the set of base types and operations for our approach. We consider
that:

• Base types should contain at least Bool, Real, AE, OE, ARL, ORL, respectively the types for booleans,

real number for rewards, actions, observations and their abstract counterparts.

• Constants for booleans and real numbers are standard. For AE, ARL and ORL, we consider ﬁnite
sets of constants, and AE, ARL should have the same size. As for OE, the set of constants depends
on the environment so we have no ﬁxed choice.

• We have standard functions for booleans and real numbers, and equality for all those base types.

We may also have additional functions for OE depending on the environment.

• For the set of operations symbols, we suppose that we have at least the main four operations

described before

choice : Unit   AE
reward : Real   Unit
observe : Unit   OE
do : AE   Unit

And, in order to make the abstraction more formal, we also add

choiceRL : Unit   ARL
rewardRL : Real   Unit

Moreover, we also need to add eﬀects corresponding to the abstract interface. As this interface is
not ﬁxed, we chose for the sake of the example the one we used on the naive algorithm. So we add
those operations:

observeRL : Unit   ORL
getactionsRL : ORL   ARL List

This describes all we need in order to formalize our approach. However, in practice, we do not want
all the actors (environment, user and learner) to have access to all those operations at all time. A typical
example is that the learner must not use observe or have access to OE as it would break abstraction,
and thus modularity. As a ﬁrst approach, we deﬁne subsets of this language, by deﬁning subsets of base
types, functions and algebraic operation, so that we can deﬁne clearly which actor has access to which
constructors. In the next section on safety, we will formalize this with a type system.

• The whole language described above, with all base types and operations is denoted λI

eﬀ . This cal-
culus will be used to deﬁne the interface, as an interface should be able to see both the constructors
for the environment and for the learner in order to make the bridge.

• We denote by λE

eﬀ the subset of this whole language without types and operations related to
the learner (all types and operations with RL in the name, such as ARL, choiceRL, . . . ). This
language will be used for the main program and the handler for do and observe, as it is basically
the language with no concrete information about the learner.

11

• Dually, we denote by λRL

eﬀ the language with only the operations related to the learner. Also, in
λRL
eﬀ , we consider that we do not have any functions nor constants for ORL and ARL except equality
and the operations in the abstract interface. Indeed, this language will correspond to the learner,
and as explained before, we want the learner to be independent from its environment. An important
point to make this possible, is that the learner should be modular (or ideally, polymorphic) in the
types ORL and ARL, and so having access to the constants of those types would break this principle.
In particular, with this deﬁnition of λRL
eﬀ , changing the size of the ﬁnite sets ORL and ARL does
not modify the language, so whatever the size may be, the learner still uses the same language.

4.3 RL Algorithms and Environments as Handlers

We start with the main program. As showed in the previous section, the main program is just an usual
functional program that has access to the types OE and AE as well as the four main operations choice,
reward, observe and do. Thus, it corresponds to the language we denote λE
eﬀ . This is the main focus
of our work, to make it possible to program within this language. In order to do this we still need to
handle those four operations, and for this we design several handlers.

4.3.1 The Learner

The learner is a handler with the state monad for the operations choiceRL and rewardRL, written in
λRL
eﬀ . So, formally, the learner is a handler

HRL ≡ handler {return x 7→ λm.return (m, x), choiceRL(k) 7→ Cc, rewardRL(r; k) 7→ Cr}

such that

k : ARL → SM (T ) ⊢ Cc : SM (T )

r : Real, k : Unit → SM (T ) ⊢ Cr : SM (T )

where T is any type and SM (T ) ≡ M → (M × T ) is the type for the state monad with state M , that
represents the memory of the learner. With this, we obtain a handler HRL with type T ⇒ SM (T ) for
any T .

In this handler, we can indeed encode a RL algorithm, as we did for the naive algorithm for MAB,

because, informally:

• The memory M can contain a value function, associating an heuristic to pairs of states and actions
(ORL × ARL). It can also be used to log information if needed, typically the previous choice, the
number of times choice was called . . .

• The term for choice Cc has a link with the policy of the learner. Indeed, a policy can be seen as
a function of type M → ARL × M , where, from an internal memory of the learner, we chose an
action in ARL and we can also modify the memory if needed, for example to log this choice. With
this policy, it is easy to obtain a computation Cc with the type described above by composing with
the continuation k.

• The term for Cr has a link with the update of the value function after a choice. Indeed, learner
usually modify their value function after a choice (or a sequence of choices). This can be seen as
an update function of type Real × M → M , when we modify the memory (and thus the value
function) according to the reward. Here, using a log in the memory can come in handy for most
reinforcement learning algorithm to remember which choice is being rewarded. It is then easy to
see that, from this update function, we can obtain a computation Cr with the type described above
by composing with the continuation k.

However, the learner will also need additional information from the environment, typically the current
observable state or the list of available actions for this state, that is why we can also use the operations
of the abstract interface in the computations Cc and Cr.

12

4.3.2 Hiding the Memory of the Learner

The concept of this handler for the learner, which is typically a handler with the state monad, is standard
but it is not very practical. Indeed, in the type T ⇒ SM (T ) we can see that the handled computation
needs an initial memory, and the ﬁnal memory is returned at the end of the computation. However, this
handled computation should be done in the main program, and the user cannot provide an initial memory
since it is not supposed to know the actual type of memory. Similarly, there is no reason for the user to
have direct access to the memory of the learner, so this type M should be hidden in the computation
type. So, in practice, we want a handler for the learner of type T ⇒ T for any T . Fortunately, it
is possible to do this from the previous handler, and it is a standard way to make the state invisible.
Suppose that the learner provides an initial memory mi. Then, it can deﬁne the following handler (with
the empty set of handled operations):

Hhide ≡ handler {return f 7→ let x = f mi in π2(x)}

with type SM (T ) ⇒ T for any T . So, by composing this handler with the previous one, we obtain a
handler of type T ⇒ T , and the memory becomes totally hidden from the user. This construction is
a way to mimic the ﬁnally clause of the EFF language, that we used in Section 2, using the standard
syntax of handler.

4.3.3 The Interface

As the previous handler uses additional algebraic operations (the one from the interface), we need to
handle them. Also, the previous handler is for the operations choiceRL and rewardRL and we need to
make the bridge between those operations and the one for the main program: choice and reward. We
do this by deﬁning two handlers in λI

eﬀ .

The ﬁrst handler is a simple one, mainly abstracting the set of actions. For this, we need to deﬁne a
bijection f : ARL → AE, which is easy to do as they are both ﬁnite sets with the same size. Then, the
handler for abstracting actions is given by:

HAct ≡ handler {return x 7→ return x,

choice(k) 7→ choiceRL((); x.k (f x))

reward(r; k) 7→ rewardRL(r; x.k x)}

With this handler HAct , with type T ⇒ T for any T , we can go from the operations from the main
program to the operation for the learner. And now the only thing to do in order to successfully use
the handler deﬁned by the learner is to deﬁne the handler for abstract interface. With the interface we
deﬁned for this example, the two operations observeRL and getactionsRL, then the interface handler
would look like:

HI ≡ handler {return x 7→ return x, observeRL(k) 7→ Co, getactionsRL(o; k) 7→ Ca}

with

k : ORL → T ⊢ Co : T

o : ORL, k : (ARL List) → T ⊢ Ca : T

so that the handler HI has type T ⇒ T for any T . Those computations may use the observe operation,
and so with this handler, that can depend on the environment, we can handle the computations coming
from the handler for the learner HRL. Now, only two operations remain to be handled do and observe

4.3.4 The Environment

To handle the environment, we only need the types and functions of λE
The handler for the environment should have the shape:

eﬀ , without reward and choice.

HE ≡ handler {return x 7→ Cr, observe(k) 7→ Co, do(a; k) 7→ Ca}

such that this handler is typable with type T ⇒ F (T ) for any type T where F (T ) a transformation of
T . The actual computations for this handler depend strongly on the environment and so we cannot give

13

additional information for the general case. However, in order to illustrate this handler, we show how to
implement it in the case where we have a speciﬁc model of the environment.

Suppose that we can model the environment by a type E and two functions: N extE : AE × E → E
and ObserveE : E → O. This may seem ad-hoc but it is in fact close to a Markov Decision Process
which is a common model for the environment in RL algorithm [34]. Indeed, in this case the ObserveE
function corresponds to observing a reward and the current state of the Markov Decision Process, and
the N extE function corresponds to moving in the MDP after an action in AE. With those functions, we
can deﬁne the following handler for the environment:

HE ≡ handler {return x 7→ λe.return (e, x),

observe(k) 7→ λe.k (ObserveE e) e
do(a; k) 7→ k () (N extE (a, e))}

with type T ⇒ SE(T ) for any T . As we saw with the learner, it is possible in this case to hide the type
E for the main program. This is what we did for example in the description of the MAB environment
in Section 2.

4.3.5 The Main Program with Handlers

And now, we can interpret all the four main operations for the main program. Thus, given a computation
C in λE

eﬀ , we can handle all those operations with the computation:

with HE handle (with HI handle (with HRL handle (with HAct handle C)))

With this composition, that we could see in the main program of Section 2, we obtain a handler of type
T ⇒ F (T ) if the learner hides its memory as explained before. And, with a complete model of the
environment, by hiding we can then obtain a type T ⇒ T .

5 Type Safety

In the previous section, we have introduced a core simply-typed calculus and used it to implement our
running example. The choice of the language was driven by a simple goal: highlighting in the simplest way
the essential features needed to implement our functional approach to reinforcement learning systems.
The price we have to pay for such a simplicity is the lack of several desirable guarantees on program
behavior. In fact, functional programming languages usually come endowed with expressive type systems
— such as polymorphic [32], linear [40, 6, 17], and graded [24] type systems — ensuring the validity of
nontrivial program properties at static time. Due to its simple type discipline, our calculus oﬀers only but
a few guarantees on program correctness, especially if one takes into account that the simplicity of the
type system is not reﬂected at the operational level, which is instead characterised by highly expressive
constructs, such as algebraic operations and handlers.

In light of that, it is desirable to strengthen the power of the type system deﬁned in the previous
section. Clearly, there are several possible extensions we may look for, and it thus natural to ask what
In this section, we ﬁrst identify three program properties well-known in the
path we should choose.
ﬁeld functional programming that we believe to be particularly relevant when modelling reinforcement
learning systems functionally, and then outline how such properties could ensured by way of expressive
type systems. Let us begin with the target program properties.

1. Locality of Operations. As a ﬁrst property, we would like to ensure (families of) algebraic
operations to be used only in speciﬁc parts of programs. In MAB, for instance, we would like the
code describing the environment, i.e. the slot machines, not to be able to perform the choice
operation.

2. Polymorphism. Secondly, we would like to ensure code to be as modular as possible, this way
enhancing its (re)usability. Concretely, that means having some form of polymorphism at disposal.
This way, algorithms may be written just once, the same code being called in many diﬀerent
environments, possibly within the same program. In other words, the learner should use the types

14

ORL and ARL in a restricted way, the only relevant information about them being that they are
ﬁnite types, whose values can thus be enumerated. As an example, the naive algorithm we have
presented in Section 2 could be used in any context. In the particular case of MAB, we have deﬁned
ORL as the unit type Unit. However, we could very well replaced Unit with any other ﬁnite type,
meaning that our strategy scales to environments with more information.

3. Linearity and Order. Finally, we would like force algebraic operations to be performed in a
speciﬁed order, this way ensuring the learner to have all the information it needs. Let us clarify
this point with an example. In the main program presented in Section 2, we see that the ﬂow
for each iteration, we make a choice; we perform such a
of information follows a certain logic:
choice in the environment; we observe its results; and, ﬁnally, we give the reward. Such a logic is
reﬂected in a speciﬁc execution order of algebraic operation, and breaking such an order may lead
the learner to obtain false information.

Having isolated our target properties, we spend the rest of this section outlining some possible ways
to force such properties by means of type systems. In particular, we focus on the use of type and eﬀect
systems [23], polymorphism [32], and graded modal types [24].

5.1 Algebraic Operations and Eﬀect Typing

Type and eﬀect systems [23] endowed traditional type systems with annotations giving information on
what eﬀects are produced during program execution. For our purposes, we can build upon well-known
eﬀect typing systems keeping track of which operations are handled during a computation [19]. Extending
the simple type system of Section 4 in this way, we can then ensure well-typed programs to handle all
their algebraic operations.

We now deﬁne eﬀect typing for our core calculus and show how to take advantage of such a typing
in the context of a reinforcement learning problem. We deﬁne an eﬀect signature as a collection of
operations and annotate the type of handlers with those signatures. Formally, leave the the syntax of
terms as in Figure 2, but we replace the one of types as follows:

T ::= B | Unit | T × T | T →E T | T E⇒ET

E ::= {op : T   T } ⊔ E | ∅

Typing judgments for computations now are of the form Γ ⊢E C : T , with the informal reading that
that C has type T in a context where the only unhandled algebraic operations C can possibly perform
are included in E. Typing judgments for values, instead, remain the same (i.e. Γ ⊢ V : T ), as values not
being executed, they cannot perform any algebraic operation at all. The inference system for our new
typing judgment is given in Figure 5.

The most important rule in Figure 5 is the one for handlers (cf. the typing rule for open handlers by
Kammar et al. [19]): it states that if a handler can handle a subset of the available operations (denoted
by {opi : T p
| 1 ≤ i ≤ n} in Figure 5) and introduce some new operations in their computations
(E′
2), then the ﬁnal set of free operations contains the unhandled ones in the original set (Ef ) together
with the new operations introduced by the handler.

  T a
i

i

This new type system satisﬁes better safety properties than the previous one. In particular, it enjoys

preservation (as the type system of previous section) and progress.

Theorem 2. Progress and preservation hold for the type system in Figure 5. That is:

1. Preservation. If Γ ⊢E C : T and C → C′ then Γ ⊢E C′ : T .

2. Progress. Suppose that all functions for base types are total for the constants in the language. If
⊢E C : T , then either there exists C′ such that C → C′, or C has the shape return V for some
V , or C has the shape op(V, x.C′) with op ∈ E.

Proof. We prove progress by induction on C. The use of typed eﬀects is important to keep track of what
are exactly the set of operations that can appear, and the commutation of op with let and handler is
also essential in this proof. The hypothesis on base type functions ensures that an application of a base

15

Γ, x : T ⊢ x : T

Γ, x : T1 ⊢E C : T2
Γ ⊢ λx.C : T1 →E T2

Γ ⊢ V1 : T1

Γ ⊢ V2 : T2

Γ ⊢ (V1, V2) : T1 × T2

Γ ⊢ () : Unit

c : B ∈ C
Γ ⊢ c : B

f : B1 × · · · Bn → B ∈ F
Γ ⊢ f : B1 × · · · Bn → B

E1 = {opi : T p
i , k : T a

| 1 ≤ i ≤ n} ⊔ Ef
i →E2 Tc ⊢E2 Ci : Tc)1≤i≤n
Γ ⊢ handler {return x 7→ Cr, op1(x; k) 7→ C1, . . . , opn(x; k) 7→ Cn} : Tv

(Γ, x : T p

Γ, x : Tv ⊢E2 Cr : Tc

E2 = E′

  T a
i

2 ⊔ Ef

i

E1⇒E2Tc

Γ ⊢ V : T
Γ ⊢E return V : T

Γ ⊢ V : T1 × T2
Γ ⊢E πi(V ) : Ti

(op : Tp   Ta) ∈ E

Γ ⊢ V : Tp
Γ ⊢E op(V ; x.C) : T

Γ, x : Ta ⊢E C : T

Γ ⊢E C1 : T1

Γ, x : T1 ⊢E C2 : T2

Γ ⊢ V1 : T2 →E T1

Γ ⊢ V2 : T2

Γ ⊢E let x = C1 in C2 : T2

Γ ⊢E V1 V2 : T1

Γ ⊢ V : T2

E1 ⇒E2T1

Γ ⊢E1 C : T2

Γ ⊢E2 with V handle C : T1

Figure 5: Typing Rules with Type Eﬀects

type function can always be reduced. The case of with H handle C is the most interesting one, by
induction hypothesis and typing, there are four cases for the computation C, either it can be reduced to
C′, and then we use the ﬁrst rule for handlers of Figure 4, either it is an operation op ∈ Ef and we use
the second rule, either it is a return V and we use the third rule, or it is an operation op ∈ E1/Ef and
we use the fourth rule.

Remark. Even if we have focused on progress and preservation, it is worth mentioning that that termi-
nation of the reduction relation can be proved using standard methods [19].

We now have all the ingredients needed to encode the informal policies we could build the MAB

example upon. Let us deﬁne the following sets of eﬀects:

EE = {observe : Unit   OE ; do : AE   Unit}
ERL = {choice : Unit   AE ; reward : Real   Unit}
RL = {choiceRL : Unit   ARL ; rewardRL : Real   Unit}
EAbs
I = {observeRL : Unit   ORL ; getactions : ORL   ARL List ;
EAbs

· · ·}

These are, respectively, the set of eﬀects for the environment, the learner seen by the environment, the
learner seen by the learner, and the interface used by the learner.

The Learner The handler HRL for the learner should have the type

⊢ HRL : T EAbs

RL ⇒EAbs

I SM (T ),

where T is any type and SM (T ) ≡ M → M × T . This means that the learner handles the operations
choiceRL and rewardRL using only the operations in the interface EAbs
. Thus, with this type, we
ensure that the learner does not have a direct access to the environment, and it only sees eﬀects for the
abstract types ARL and ORL and not the concrete types OE and AE. Then, with hiding, we can obtain
a handler of type T EAbs
I T as long as the learner provides an initial memory. Notice that in this
hiding handler of type T EAbs
I T , the initial memory can depend on the abstract interface, this being
useful in those cases in which some parameters in the initial memory have to be exposed.

I ⇒EAbs

RL ⇒EAbs

I

16

The Interface The interface is separated into two handlers: HAct and HI . This ﬁrst handler is only
a bridge between abstract actions and concrete actions. It is not diﬃcult to see that the former should
have the type:

⊢ HAct : T ERL ⇒EAbs

RL T,

for any type T , thus handling the operations choice and reward seen from the exterior to the internal
one of the learner. As for the interface, it should be a bridge between the environment and learner
abstracted by the user, so it should have the type:

where the ﬁxed set of operations (Ef in the rule) would be EE.

⊢ HI : T EAbs

I ⊔EE ⇒EE T,

The Environment As for the environment, the handler only has to handle the two operations observe
and do, without using the learner nor the interface. So, it should have the type:

⊢ HE : T EE ⇒∅F (T ).

5.1.1 Composition of Handlers and Main Program

Now that we have types for each of the four handlers needed, we compose them. An important point of
our eﬀect typing is that it supports weakening: in the handler rule, we can always take a larger set Ef ,
so that we are licensed to use the handler in diﬀerent contexts.3 In our case, it means that we can give
the following types to handlers:

I ⊔EE T

RL ⊔EE T

RL ⊔EE ⇒EAbs
⊢ HRL : T EAbs
⊢ HAct : T ERL⊔EE ⇒EAbs
I ⊔EE ⇒EE T

⊢ HI : T EAbs
⊢ HE : T EE ⇒∅F (T ).

Consequently, we can compose them to obtain:

HE ◦ HI ◦ HRL ◦ HAct : T ERL⊔EE ⇒∅F (T ).

When we have a model of the environment, with hiding, we have

HE ◦ HI ◦ HRL ◦ HAct : T ERL⊔EE ⇒∅T.

This shows that the main program can use the four main eﬀects, as expected. Without hiding, we would
have a type which is essentially the composition of the two state monads on E and M , meaning that the
program should be understood in a context with both an environment and a memory for the learner.

5.2 Polymorphism

The typing rule for handlers allows us to assign handlers arbitrary types T . To ensure such types not
to be inspected by programs, and to ensure uniqueness of typing derivation of handlers, it is natural to
extend our type system with polymorphic types. Moreover, the base types we presented for the language,
ARL, ORL, AE and OE all depend on the actual environment, whereas we expect the handler HRL for
the learner not to do so. Consequently, we may rely on polymorphism to enforce ARL and ORL not to be
inspected, this way giving a unique polymorphic type to the handler HRL and thus ensuring the latter
to be usable in any environment.

Languages, however, can be polymorphic in may ways: especially in presence of eﬀects. We now
outline what kind of polymorphism is needed for our purposes. As a ﬁrst requirement, we need to be
able to declare polymorphic handlers; write their implementation once and for all; and then use them
in diﬀerent contexts. To do that, we believe having handlers as ﬁrst-class citizen of the language, as we

3This is one of the main reasons why this rule is very useful to achieve polymorphism.

17

presented in Figure 2, is necessary. Secondly, we also want to declare polymorphic eﬀects, so to give a
sense to those polymorphic handlers.

We take as an inspiration the language by Biernacki et al. [8]. However, since the aforementioned
language does not consider handlers as ﬁrst-class citizens, we cannot directly rely on that. In the rest of
this section, we informally outline a possible polymorphic type assignment for our language, leaving its
formal deﬁnition and analysis (such as type soundness) as future work.

We consider diﬀerent kinds, starting from the base kinds of types, eﬀects, and rows, and building

complex kinds using a functional arrow. Formally, kinds are generated by the following grammar:

κ := T | E | R | κ → κ.
A row ρ is a sequence of eﬀects hE1 | ρ′i, with intuitively the same meaning as eﬀect signature in our
last type system, the main diﬀerence being that we can have polymorphism on row, and an eﬀect can
appear several time in a row (but possibly with diﬀerent instantiation of their polymorphic types). We
would like to write, as in [8], the following computation:

eﬀect EAbs

RL = ∀αA :: T.{choiceRL : Unit   αA; rewardRL : Real   Unit} in · · ·

declaring a new eﬀect, called EAbs
RL , waiting for a type αA representing the type (of kind T ) of actions,
and then declare the types of those two algebraic operations. The kind of EAbs
RL would then be T → E,
so given a type of kind T this indeed gives an eﬀect of kind E. Similarly, the abstract interface would
be declared with:
eﬀect EAbs

I = ∀αA :: T, αO :: T.{observeRL : Unit   αO; getactionsRL : αO   αA List; · · ·}

And then, in this context, the type of the handler for the learner, HRL would be, after hiding:

HRL : ∀αA :: T, αO :: T, αT :: T, ρ :: R.αT

hEAbs

RL αA|ρi⇒hEAbs

I

αA αO |ρiαT

RL αA and introducing the new operations of EAbs

meaning that for any type of actions αA, any type of observations αO, any type of computation αT and
any eﬀect environment ρ, the handler for the learner has an identity type on the computation, handling
the operations in EAbs
αA αO. In practice, we would
like αA and αO to always represent base types, and especially ﬁnite sets, so we would need some more
restriction on this polymorphism. But informally, this is the shape of type we would like for the handler
written by the learner, as it ensures that it can be used in any environment without modiﬁcations.
Notice that in order to do this, we want handlers as ﬁrst-class citizens but we do not need the full power
of polymorphism since the algebraic operations themselves do not need to be polymorphic. This kind
of polymorphism that we need diﬀers slightly from the polymorphism mainly studied in the literature:
usually, it allows the programmer to use diﬀerent handlers for the same operation, however what we
want in our case is to use a unique handler for diﬀerent contexts.

I

5.3 Imposing an Order on Operations

Our last property of interest focuses on linearity and order of algebraic operations. For instance, we
would like to ensure that when a call to choice is made, it is always followed by a do and a reward
(observe can be useful to give the reward but it is not mandatory as it should have no eﬀect on the
environment). We now outline how such a goal can be achieved in presence of algebraic operations
relying on suitable graded type systems [20]. Such an approach, however, is not readily extendable to
eﬀect handlers. As far as we know, graded types have not been extended to handlers.

Informally, we deﬁne a monoid on algebraic operations and associate each computation to an element
of this monoid, the latter giving information on the operations executed during program evaluation.
Sequential composition of computations corresponds to monoid multiplication, basic computations to
the unit of the monoid, and algebraic operations to the corresponding elements of the monoid. A typing
rule (for computations) then has shape Γ ⊢E C : T ; m, where m is an element of the monoid with, for
example, the following rules:

Γ ⊢ V : T
Γ ⊢E return V : T ; 1

Γ ⊢E C1 : T1; m1

Γ, x : T1 ⊢E C2 : T2; m2

Γ ⊢E let x = C1 in C2 : T2 : m1 · m2

(op : Tp   Ta) ∈ E

Γ ⊢ V : Tp
Γ ⊢E op(V ; x.C) : T ; mop · m

Γ, x : Ta ⊢E C : T ; m

18

where 1 is the unit of the monoid, m1 · m2 is monoid multiplication and mop is the element of the monoid
corresponding to the operation op.

In our case, the monoid would be the free monoid on the three element associated to the algebraic
operations choice, do and reward, with observe confounded with the unit of the monoid, with the
following equation

mchoice · mdo · mreward = 1

and then, we would like the main program to be associated to an element of this monoid with the shape
mn
do. With this, we can ensure that, if we forget about observe since it has no eﬀect at all, then any
choice is always followed by exactly one do and one reward. Moreover, only the do operation can be
done outside of this loop, because essentially the user can make choices without calling the learner, and
for the point of view of the learner then the user would just be a part of the transition function of the
environment.

However, this method, which is standard for eﬀects, does not generalise well in presence of handlers.
Indeed, we need to take into account the potentially new operations introduced by a handler, and
intuitively the handler would then represent a map from the new operations to an element of the monoid.
For a very simple example, consider the operation choicedoandobserve : 1   AE × OE with the handler

handler {return x 7→ x; choicedoandobserve(k) 7→ choice(a.do(a; _.observe(o.k (a, o))))}

doing the three operations choice, do and observe in sequence. Then, this handler should be typed
with the information that the new operation choicedoandobserve is associated to the element mchoice ·
mdo · mobserve.

We believe such a type system should be feasible, but we have no concrete formalization of this. So, as
for polymorphism, those kind of type system would be desirable for safety but we leave the formalization
to future work.

6 Related Work

The interaction between programming language theory and machine learning is an active and ﬂourishing
research area. Arguably, the current, most well-established products of such an interaction are the so-
called Bayesian [36, 37, 35, 9, 11] and diﬀerentiable [7, 3] programming languages, and their associated
theories [18, 39, 1, 25, 33].

Choice-based operations are not new in programming language theory, as they ﬁrst appeared in the
context of computational eﬀects (more speciﬁcally, choice operations has been ﬁrst studied as nondeter-
ministic choices [21]). However, their integration with reward constructs as a way to structure machine
learning systems as data-driven, decision-making processes is quite recent. As an example of that,
the programming language SmartChoice [10] integrates choice and rewards operations with traditional
programming constructs. To the best of the authors’ knowledge, the ﬁrst work dealing with semantic
foundations for choice-based programming languages is the recent work by Abadi and Plotkin [2]. There,
a modular approach to higher-order choice-based programming languages is developed in terms of the
selection monad [14, 16, 15] and its algebraic operations [27, 26, 28], both at the level of operational and
denotational semantics. As such, that work is the closest to ours.

The results presented in this paper has been obtained starting from an investigation of possible appli-
cations of the aforementioned work by Abadi and Plotkin to modelling reinforcement learning systems.
We have explained in Section 3 why, when dealing with reinforcement learning, moving from the selection
to the state monad is practically beneﬁcial. From a theoretical point of view, such a shift can be seen
as obtained by looking at comodels [30] of choice operations, which directly leads to view reinforcement
learning algorithms as ultimately deﬁning stateful runners [38, 4] for such operations, and thus as generic
eﬀects [26] for the state monad. Following this path, we have consequently relied on handlers [29, 5, 31]
to deﬁne modular implementations of such generic eﬀects, and thus of reinforcement learning algorithms.
Even if handlers and algebraic operations are well-known tools in functional programming, to the best of
our knowledge the present work is the ﬁrst one investigating their application to modelling reinforcement
learning systems.

Finally, we mention that applications of functional programming techniques in the context of machine
learning similar in spirit to ones investigated in this work, as well as in the work by Abadi and Plotkin

19

[2], have been proposed [12] in terms of induction and abduction constructs, rather than in terms of
choices and rewards

7 Conclusion

In this article, we address the problem of implementing reinforcement learning algorithms within a
functional programming language. The starting idea is to manage the interaction between the three
involved agents (namely the learner, the user, and the environment) through a set of algebraic operations
and handlers for them. This way, a certain degree of modularity is guaranteed.

The path we followed starts from practice, i.e. from the implementation of these ideas in a concrete
language such as EFF, and then progressively moves towards theory, i.e. towards the study of a paradig-
matic language for eﬀects and handlers in which these ideas can be formalized, thus becoming an object
of study. Technically, the most important contribution of this work lies in highlighting where the state
of the art is deﬁcient with respect to the type of type safety and code reuse properties that one would
like.

Some ideas for future works can be sought precisely in the direction just mentioned and, in particular,
in the deﬁnition of systems of types suﬃciently powerful to guarantee that the algebraic operations
involved are actually carried out in the good order, or that they allow the right level of polymorphism.
An alternative to powerful type systems for some safety properties could also be the use of some
advanced features of programming language for abstraction. For example, we presented the abstract
interface as a handler, with a learner polymorphic in the types of actions and observation. In OCAML,
an alternative could be to use modules and signatures: an abstract interface would be represented by a
signature, with abstract data types for actions and observations, and additional functions to replace the
operations. The learner would then have access to elements of this signature, and the user would need
to implement an actual module respecting this signature in order to use the learning algorithm. This
way, the fact that the learner can be written independently from the environment would come from the
OCAML abstraction, instead of polymorphism in a type and eﬀect system.

References

[1] M. Abadi and G. D. Plotkin. A simple diﬀerentiable programming language. Proc. ACM Program.

Lang., 4(POPL):38:1–38:28, 2020.

[2] M. Abadi and G. D. Plotkin. Smart choices and the selection monad. In 36th Annual ACM/IEEE
Symposium on Logic in Computer Science, LICS 2021, Rome, Italy, June 29 - July 2, 2021, pages
1–14, 2021. doi: 10.1109/LICS52264.2021.9470641.

[3] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker,
V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorﬂow: A system for large-scale
machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation,
OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 265–283, 2016.

[4] D. Ahman and A. Bauer. Runners in action.

In Programming Languages and Systems - 29th
European Symposium on Programming, ESOP 2020, Held as Part of the European Joint Conferences
on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25-30, 2020, Proceedings,
pages 29–55, 2020. doi: 10.1007/978-3-030-44914-8\_2.

[5] A. Bauer and M. Pretnar. Programming with algebraic eﬀects and handlers. J. Log. Algebr. Meth.

Program., 84(1):108–123, 2015.

[6] P. N. Benton and P. Wadler. Linear logic, monads and the lambda calculus. In Proc. of LICS 1996,

pages 420–431, 1996. doi: 10.1109/LICS.1996.561458.

[7] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-
farley, and Y. Bengio. Theano: A cpu and gpu math compiler in python. In Proceedings of the 9th
Python in Science Conference, pages 3–10, 2010.

20

[8] D. Biernacki, M. Piróg, P. Polesiuk, and F. Sieczkowski. Abstracting algebraic eﬀects. Proc. ACM

Program. Lang., 3(POPL), 2019.

[9] E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh, P. A.
Szerlip, P. Horsfall, and N. D. Goodman. Pyro: Deep universal probabilistic programming. J. Mach.
Learn. Res., 20:28:1–28:6, 2019.

[10] V. Carbune, T. Coppey, A. Daryin, T. Deselaers, N. Sarda, and J. Yagnik. Predicted variables in

programming, 2019.

[11] B. Carpenter, A. Gelman, M. D. Hoﬀman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker,
J. Guo, P. Li, and A. Riddell. Stan: A probabilistic programming language. Journal of Statistical
Software, 76(1):1–32, 2017. doi: 10.18637/jss.v076.i01.

[12] S. Cheung, V. Darvariu, D. R. Ghica, K. Muroya, and R. N. S. Rowe. A functional perspective on
machine learning via programmable induction and abduction. In J. P. Gallagher and M. Sulzmann,
editors, Proc. of FLOPS, volume 10818 of Lecture Notes in Computer Science, pages 84–98. Springer,
2018. doi: 10.1007/978-3-319-90686-7\_6.

[13] U. Dal

Lago,

F. Gavazzo,

and A. Ghyselen.

Source Code

in

EFF.

https://github.com/GhyselenAlexis/GhyselenAlexis.github.io/tree/main/EFF, 2022.

[14] M. H. Escardó and P. Oliva. The peirce translation and the double negation shift. In Proc. of CiE

2010, pages 151–161, 2010. doi: 10.1007/978-3-642-13962-8\_17.

[15] M. H. Escardó and P. Oliva. The peirce translation. Ann. Pure Appl. Log., 163(6):681–692, 2012.

doi: 10.1016/j.apal.2011.11.002.

[16] M. H. Escardó, P. Oliva, and T. Powell. System T and the product of selection functions. In Proc.

of CSL 2011, pages 233–247, 2011. doi: 10.4230/LIPIcs.CSL.2011.233.

[17] D. R. Ghica and A. I. Smith. Bounded linear types in a resource semiring. In Proc. of ESOP 2014,

pages 331–350, 2014. doi: 10.1007/978-3-642-54833-8\_18.

[18] N. D. Goodman. The principles and practice of probabilistic programming. In Proc. of POPL ’13,

pages 399–402, 2013. doi: 10.1145/2429069.2429117.

[19] O. Kammar, S. Lindley, and N. Oury. Handlers in action. In Proceedings of the 18th ACM SIGPLAN
International Conference on Functional Programming, ICFP ’13, page 145–158, New York, NY,
USA, 2013. Association for Computing Machinery.

[20] S.-y. Katsumata. Parametric eﬀect monads and semantics of eﬀect systems. In Proceedings of the
41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 633–
645, 2014.

[21] J. McCarthy. A basis for a mathematical theory of computation, preliminary report. In Papers
presented at the 1961 western joint IRE-AIEE-ACM computer conference, IRE-AIEE-ACM 1961
(Western), Los Angeles, California, USA, May 9-11, 1961, pages 225–238, 1961. doi: 10.1145/
1460690.1460715.

[22] E. Moggi. Computational Lambda-calculus and Monads. University of Edinburgh, Department of

Computer Science, 1988.

[23] F. Nielson and H. R. Nielson. Type and eﬀect systems. In Correct System Design, Recent Insight
and Advances, (to Hans Langmaack on the occasion of his retirement from his professorship at the
University of Kiel), pages 114–136, 1999.

[24] D. Orchard, V.-B. Liepelt, and H. Eades III. Quantitative program reasoning with graded modal

types. Proc. ACM Program. Lang., 3(ICFP):110:1–110:30, 2019. doi: 10.1145/3341714.

[25] B. A. Pearlmutter and J. M. Siskind. Reverse-mode AD in a functional framework: Lambda the

ultimate backpropagator. ACM Trans. Program. Lang. Syst., 30(2):7:1–7:36, 2008.

21

[26] G. Plotkin and J. Power. Algebraic operations and generic eﬀects. Applied Categorical Structures,

11:69–94, 2003. https://doi.org/10.1023/A:1023064908962.

[27] G. D. Plotkin and J. Power. Adequacy for algebraic eﬀects. In Proc. of FOSSACS 2001, pages 1–24,

2001. doi: 10.1007/3-540-45315-6\_1.

[28] G. D. Plotkin and J. Power. Notions of computation determine monads. In Proc. of FOSSACS

2002, pages 342–356, 2002.

[29] G. D. Plotkin and M. Pretnar. Handling algebraic eﬀects. Logical Methods in Computer Science, 9

(4), 2013.

[30] A. J. Power and O. Shkaravska. From comodels to coalgebras: State and arrays. In Proceedings of
the Workshop on Coalgebraic Methods in Computer Science, CMCS 2004, Barcelona, Spain, March
27-29, 2004, pages 297–314, 2004. doi: 10.1016/j.entcs.2004.02.041.

[31] M. Pretnar. An introduction to algebraic eﬀects and handlers. invited tutorial paper. Electronic

notes in theoretical computer science, 319:19–35, 2015.

[32] J. Reynolds. Types, abstraction and parametric polymorphism. In IFIP Congress, pages 513–523,

1983.

[33] J. M. Siskind and B. A. Pearlmutter. Nesting forward-mode AD in a functional framework. Higher-

Order and Symbolic Computation, 21(4):361–376, 2008.

[34] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

[35] D. Tolpin, J.-W. van de Meent, H. Yang, and F. Wood. Design and implementation of probabilistic
programming language anglican. In Proceedings of the 28th Symposium on the Implementation and
Application of Functional Programming Languages, IFL 2016, pages 6:1–6:12, 2016.

[36] D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Edward: A library

for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:1610.09787, 2016.

[37] D. Tran, M. D. Hoﬀman, R. A. Saurous, E. Brevdo, K. Murphy, and D. M. Blei. Deep probabilistic

programming. In International Conference on Learning Representations, 2017.

[38] T. Uustalu. Stateful runners of eﬀectful computations. In The 31st Conference on the Mathematical
Foundations of Programming Semantics, MFPS 2015, Nijmegen, The Netherlands, June 22-25,
2015, pages 403–421, 2015. doi: 10.1016/j.entcs.2015.12.024.

[39] J. van de Meent, B. Paige, H. Yang, and F. Wood. An introduction to probabilistic programming.

CoRR, abs/1809.10756, 2018. URL http://arxiv.org/abs/1809.10756.

[40] P. Wadler. Linear types can change the world! In Programming concepts and methods: Proceedings
of the IFIP Working Group 2.2, 2.3 Working Conference on Programming Concepts and Methods,
Sea of Galilee, Israel, 2-5 April, 1990, page 561, 1990.

22

