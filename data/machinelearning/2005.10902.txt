Global optimization of Gaussian processes ∗

Artur M. Schweidtmann1
Tim Kerkenhoﬀ1

Dominik Bongartz1

Daniel Grothe1

Xiaopeng Lin1
Alexander Mitsos1,2,3

Jaromi(cid:32)l Najman1

0
2
0
2

y
a
M
1
2

]

C
O
.
h
t
a
m

[

1
v
2
0
9
0
1
.
5
0
0
2
:
v
i
X
r
a

1 Process Systems Engineering (AVT.SVT), RWTH Aachen University,
Aachen, Germany.
2 JARA-CSD, 52056 Aachen, Germany.
3 Institute of Energy and Climate Research, Energy Systems Engineering
(IEK-10), Forschungszentrum J¨ulich GmbH, 52425 J¨ulich, Germany.
(amitsos@alum.mit.edu).

May 25, 2020

Abstract

Gaussian processes (Kriging) are interpolating data-driven models that are fre-
quently applied in various disciplines. Often, Gaussian processes are trained on
datasets and are subsequently embedded as surrogate models in optimization prob-
lems. These optimization problems are nonconvex and global optimization is de-
sired. However, previous literature observed computational burdens limiting de-
terministic global optimization to Gaussian processes trained on few data points.
We propose a reduced-space formulation for deterministic global optimization with
trained Gaussian processes embedded. For optimization, the branch-and-bound
solver branches only on the degrees of freedom and McCormick relaxations are
propagated through explicit Gaussian process models. The approach also leads
to signiﬁcantly smaller and computationally cheaper subproblems for lower and
upper bounding. To further accelerate convergence, we derive envelopes of com-
mon covariance functions for GPs and tight relaxations of acquisition functions
used in Bayesian optimization including expected improvement, probability of im-
provement, and lower conﬁdence bound. In total, we reduce computational time
by orders of magnitude compared to state-of-the-art methods, thus overcoming
previous computational burdens. We demonstrate the performance and scaling
of the proposed method and apply it to Bayesian optimization with global opti-
mization of the acquisition function and chance-constrained programming. The

∗This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foun-
dation) under Germany’s Excellence Strategy - Cluster of Excellence 2186 “The Fuel Science Center”
and the project “Improved McCormick Relaxations for the eﬃcient Global Optimization in the Space of
Degrees of Freedom” MI 1851/4-1.

1

 
 
 
 
 
 
Gaussian process models, acquisition functions, and training scripts are available
open-source within the “MeLOn - Machine Learning Models for Optimization”
toolbox (https://git.rwth-aachen.de/avt.svt/public/MeLOn).
Keywords: Kriging; Machine learning; Bayesian optimization; Acquisition func-
tion; Chance-constrained programming; Reduced-space; Expected improvement

1

Introduction

A Gaussian process (GP) is a stochastic process where any ﬁnite collection of random
variables has a multivariate Gaussian distribution; they can be understood as an inﬁnite-
dimensional generalization of multivariate Gaussian distributions [57]. This means that
predictions of GPs are Gaussian distributions that provide not only an estimate but also
a variance. GPs originate from geostatistics [39] and gained popularity for the design
and analysis of computer experiments (DACE) since 1989 [58]. Furthermore, GPs are
commonly applied as interpolating surrogate models across various disciplines including
biotechnology [26, 71, 45, 12, 23], chemical engineering [43, 13, 20, 21, 22, 24, 30], chem-
istry [59, 1], and deep-learning [64]. Note that GP regression is also often referred to
as Kriging. In many applications, GPs are trained on a data set and are subsequently
embedded in an optimization problem, e.g., to identify an optimal operating point of a
process. Moreover, many derivative-free solvers for expensive-to-evaluate black-box func-
tions actually train GPs and optimize their predictions (e.g., Bayesian optimization algo-
rithms [62, 33, 71, 11] and other adaptive sampling approaches [20, 21, 22, 18, 24, 9, 8]).
In Bayesian optimization, the optimum of an acquisition function determines the next
sampling point [62]. The vast majority of these optimizations have been performed by
local solution approaches [37, 76] and a few by stochastic global [11]. Our contribution
focuses on the deterministic global solution of optimization problems with trained GPs
embedded and on applications in process systems engineering.

GPs are commonly used to learn the input-output behavior of unit operations [14, 13,
55, 54, 40, 35, 36], complete ﬂowsheets [29], or thermodynamic property relations from
data [42]. Subsequently, the trained GPs are often combined with nonlinear mechanistic
process models leading to hybrid mechanistic / data-driven models [72, 34, 28, 51] which
are optimized. Most of the previous works on optimization with GPs embedded rely on
local optimization techniques. Caballero and Grossmann, for instance, train GPs on data
obtained from a rigorous divided wall column simulation. Then, they iteratively optimize
the operation of the column (modeled by GPs) using SNOPT [27], sample new data at the
solution point, and update the GPs [14, 13]. Later, Caballero and co-workers extend this
work to distillation sequence superstructure problems [55, 54]. In [54], the authors solve
the resulting mixed-integer nonlinear programs (MINLPs) using a local solver in GAMS.
Therein, the GP estimate is computed via an external function in Matlab which leads
to a reduced optimization problem size visible to the local solver in GAMS. However,
all these local methods have the drawback that they can lead to suboptimal solutions,
because the resulting optimization problems are nonconvex. This nonconvexity is induced
by the covariance functions of the GPs as well as often the mechanistic part of the hybrid
models.

Deterministic global optimization can guarantee to identify globally optimal solutions

2

In a few previous studies, deter-
within ﬁnite time to a given nonzero tolerance [31].
ministic global optimization with GPs embedded was done using general-purpose global
solvers. For instance, in the black-box optimization algorithms ALAMO [18] and ARG-
ONAUT [8], GPs are included as surrogate models and are optimized using BARON [68]
and ANTIGONE [48], respectively. However, computational burdens were observed that
limit applicability, e.g., in terms of the number of training points. Cozad et al. [18] state
that GPs are accurate but “diﬃcult to solve using provable derivative-based optimization
software”. Similarly, Boukouvala and Floudas [8] state that the computational cost be-
comes a limiting factor because the number of nonlinear terms of GPs equals the product
of the number of interpolated points (N ) and the dimensionality (D) of the input domain.
More recently, Keßler et al. [35, 36] optimized the design of nonideal distillation columns
by a trust-region approach with GPs embedded. Therein, optimization problems with
GPs embedded are solved globally using BARON [68] within relatively long CPU times
(102 ´ 105 CPU seconds on a personal computer).

As mentioned earlier, Quirante et al. [54] call an external Matlab function to compute
GP estimates with a local solver in GAMS. As an alternative approach, they also solve
the problem globally using BARON in GAMS by providing the full set of GP equations
as equality constraints. This leads to additional intermediate optimization variables be-
sides the degrees of freedom of the problem. Similar to other studies, they observe that
their formulation is only practical for a small number of GP surrogates and training data
points to avoid large numbers of variables and constraints [54]. We refer to the problem
formulation where the GP is described by equality constraints and additional optimization
variables as a full-space (FS) formulation. It is commonly used in modeling environments,
e.g., GAMS, that interface with state-of-the-art global solvers such as ANTIGONE [48],
BARON [68], and SCIP [41].

An alternative to the FS is a reduced-space (RS) formulation where some optimization
variables are eliminated using explicit constraints. This reduced problem size leads to a
lower number of variables for branching as well as potentially smaller subproblems. The
former has some similarity to selective branching [25] (c.f. discussion in [3]). The exact
size of the subproblems for lower bounding and bound tightening depends on the method
for constructing relaxations. In particular, when constructing relaxations in the RS using
McCormick [44], alphaBB [2] or natural interval extensions, the resulting lower bounding
problems are much smaller compared to the auxiliary variable method (AVM) [63, 69].
Therefore, any global solver can in principle handle RS but some methods for constructing
relaxations appear more promising to beneﬁt from the RS [3]. We have recently released
the open-source global solver MAiNGO [6] which uses the MC++ library [15] for automatic
propagation of McCormick relaxations through computer code [49]. We have shown that
the RS formulation can be advantageous for ﬂowsheet optimization problems [4, 5] and
problems with artiﬁcial neural networks embedded [60, 56, 32]. In the context of Bayesian
optimization, Jones et at. [33] develop valid overestimators of the expected improvement
(EI) acquisition function in the RS. However, their relaxations rely on interval extensions
and optimization-based relaxations limited to a speciﬁc covariance function; they do not
derive envelopes. Furthermore, they do not provide convex underestimators which are in
general necessary to embed GPs in optimization problems.

In this work, we show that the proposed RS outperforms a FS formulation for problems

3

with GPs embedded by speedup factors of several magnitudes. Moreover, this speedup
increases with the number of training points. This is mainly due to the fact that the num-
ber of variables and constraints in the FS scales linearly with the number of data points
OpN ` Dq while in the RS it depends only on the input dimensionality, i.e., OpDq. To
further accelerate convergence, we derive envelopes of covariance functions for GPs and
tight relaxations of acquisition functions, which are commonly used in Bayesian optimiza-
tion. Finally, we solve a chance-constrained optimization problem with GPs embedded
and we perform global optimization of an acquisition function. The GP training methods
and models are provided as an open-source toolbox called “MeLOn - Machine Learning
Models for Optimization” under the Eclipse public license [61]. The resulting optimiza-
tion problems are solved using our global solver MAiNGO [6]. Note that the MeLOn
toolbox is also automatically included as a submodule in our new MAiNGO release.

2 Gaussian Processes

In this section, GPs are brieﬂy introduced (c.f. [57]). We ﬁrst describe the GP prior distri-
bution, i.e., the probability distribution before any data is taken into account. Then, we
describe the posterior distribution, which results from conditioning the prior on training
data. Finally, we describe how hyperparameters of the GP can be adapted to data by a
maximum a posteriori (MAP) estimate.

2.1 Prior

A GP prior is fully described by its mean function mpxq and positive semi-deﬁnite covari-
ance function kpx, x1q (also known as kernel function). We consider a noisy observation
y from a function ˜f pxq with ypxq :“ ˜f pxq ` εnoise, whereby the output noise εnoise is
independent and identically distributed (i.i.d.) with εnoise „ N p0, σ2
noiseq. We say y is
distributed as a GP, i.e., y „ GPpmpxq, kpx, x1qq with

“
mpxq :“ IE
“
kpx, x1q :“ IE

‰
,

˜f pxq
p ypxq ´ mpxq q p ypx1q ´ mpxq qT

‰

.

Without loss of generality, we assume that the prior mean function is mpxq “ 0. This
implies that we train the GP on scaled data such that the mean of the training outputs
is zero. A common class of covariance functions is the Mat´ern class.

kMat´ernpx, x1q :“ σ2
f

´?

2νr

¯
ν

Kν

´?

2νr

¯

,

i , ¨ ¨ ¨ λ2

1, ¨ ¨ ¨ , λ2

f is the output variance, r :“

where σ2
px ´ x1qTΛ px ´ x1q is a weighted Euclidean
nxq is a length-scale matrix with λi P R, Γp¨q is the
distance, Λ :“ diagpλ2
gamma function, and Kνp¨q is the modiﬁed Bessel function of the second kind. The
smoothness of Mat´ern covariance functions can be adjusted by the positive parameter
ν. When ν is a half-integer value, the Mat´ern covariance function becomes a product of
a polynomial and an exponential [57]. Common values for ν are 1{2, 3{2, 5{2, and 8,
i.e., the most widely-used squared exponential covariance function. We derive envelopes

21´ν
Γpνq
a

4

n ¨ δpx, x1q, can be added to any covariance function where σ2

of these covariance functions in Section 4.1 and implement them within MeLOn [61].
Also, a noise term, σ2
n is
the noise variance and δpx, x1q is the Kronecker delta function. The hyperparameters
of the covariance function are adjusted during training and are jointly noted as θ “
rλ1, .., λd, σf , σns. Herein, a log-transformation is common to prevent negative values
during training.

2.2 Posterior

The GP posterior is obtained by conditioning the prior on observations. We consider
, ..., xpDq
a set of N training inputs X “ txpDq
i,D sT is a D-
dimensional vector. Note that we use the superscript pDq to denote the training data.
The corresponding set of scalar observations is given by Y “ typDq
N u. Furthermore,
we deﬁne the vector of scalar observations y “ rypDq
N sT P RN . The posterior GP
is obtained by Bayes’ theorem:

N u where xpDq

i “ rxpDq

i,1 , ..., xpDq

, ..., ypDq

, ..., ypDq

1

1

1

˜f pxq „ GPpmpxq, kpx, x1q|X , Yq “ N pmDpxq, kDpx, x1qq

with

mDpxq “ K x,X pK X ,X q´1 y,
kDpxq “ Kx,x ´ K x,X pK X ,X q´1 K X ,x,

(1)

(2)

”

where the covariance matrix of the training data is given by K X ,X :“ rkpxi, xjqs P RN ˆN ,
the covariance vector between the candidate point x and the training data is given by
T , and Kx,x :“ kpx, xq. Equa-
K x,X :“
tions (1), (2) describe essentially the predictions of a GP and are implemented within
MeLOn.

ı
q, .., kpx, xpDq
N q

P R1ˆN , K X ,x “ K x,X

kpx, xpDq

1

2.3 Maximum A Posteriori

In order to ﬁnd appropriate hyperparameters θ for a given problem, we use a MAP
estimate which is known to be advantageous compared to the maximum likelihood esti-
mation (MLE) on small data sets [67]. Using the MAP estimate, the hyperparameters
are identiﬁed by maximizing the probability that the GP ﬁts the training data, i.e.,
θopt :“ argmaxθ P pθ|X , Yq. Analytical expressions for P pθ|X , Yq and its derivatives
w.r.t. the hyperparameters can be found in the literature [57]. We provide a Matlab
training script in MeLOn that is based on our previous work [11]. Therein, we assume an
independent Gaussian distribution as a prior distribution on the log-transformed hyper-
parameters, i.e., θi „ N pµi, σ2

i q.

3 Optimization Problem Formulations

In the simplest and common in the literature case, the (scaled) inputs of a GP are
degrees of freedom of the optimization problem with x P ˜X “ rxL, xU s. For given

5

x, the dependent (or intermediate) variables z can be computed by the solution of
hpx, zq “ 0, h : ˜X ˆ Rnz Ñ Rnz . This corresponds to Equations (1) and (2) where the
estimate (mD) and variance (kD) of the GP are computed respectively. Note that in this
case, we can solve explicitly for mD and kD.

The realization of the objective function f depends on the application. In many ap-
plications, it depends on the estimate of the GP, i.e., f pmDq (c.f. Subsection 5.1). In
Bayesian optimization, the objective function is called acquisition function and usually
depends on the estimate and variance of the GP, i.e., f pmD, kDq (c.f. Subsection 5.3).
Finally, additional constraints might depend on the inputs of the GP, its estimate, and
variance, i.e., gpx, mD, kDq ď 0. In more complex cases, multiple GPs can be combined
in one optimization problem (c.f. Subsection 5.2).

In the following, we describe two optimization problem formulations for problems with
trained GPs embedded: the commonly used FS formulation in Subsection 3.1 and the RS
formulation in Subsection 3.2. Both problem formulations are equivalent reformulations in
the sense that they have the same optimal solution. However, the formulation signiﬁcantly
aﬀects problem size and performance of global optimization solvers.

3.1 Full-Space formulation

In the FS formulation, the nonlinear equations hpx, zq “ 0 are provided as equality
constraints and the intermediate dependent variables z P Z are optimization variables. A
general FS problem formulation is:

min
xP ˜X,zPZ
s.t.

f px, zq

(FS)

hpx, zq “ 0,

gpx, zq ď 0

In general, there exist multiple valid FS formulations for optimization problems. In Sec-
tion 3 of the electronic supplementary information (ESI), we provide a representative FS
formulation for the case where the estimate of a GP is minimized. This is also the FS
formulation that we use in our numerical examples (c.f., Section 5.1).

3.2 Reduced-Space Formulation

In the RS formulation, the equality constraints are solved for the intermediate variables
and substituted in the optimization problem (c.f. [4]). A general RS problem formulation
in the context of optimization with a GP embedded is:

min
xP ˜X
s.t.

f pmDpxq, kDpxqq

gpx, mDpxq, kDpxqq ď 0

(RS)

Herein, the B&B solver operates only on the degrees of freedom x and no equality con-
straints are visible to the solver. In GPs, the estimate and variance are explicit functions
of the input (Equations (1), (2)). Thus, we can directly formulate a RS formulation.
The RS formulation eﬀectively combines those equations and hides them from the B&B
algorithm. This results in a total number of D optimization variables, zero equality con-
straints, and no additional optimization variables z. Thus, the RS formulation requires

6

only bounds on x.

Note that the direct substitution of all equality constraints is not always possible
when multiple GPs are combined with mechanistic models, e.g., in the presence of recycle
streams. Here, a small number of additional optimization variables and corresponding
equality constraints can remain in the RS formulation [4]. As an alternative, relaxations
for implicit functions can also be derived [66, 75]. Moreover, we have previously observed
that a hybrid between RS and FS formulation can be optimal for some optimization prob-
lems [5]. In this work, we compare the RS and the FS formulation and do not consider
any hybrid problem formulations.

4 Convex and Concave Relaxations

The construction of relaxations, i.e., convex function underestimators (F cv) and concave
function overestimators (F cc), is essential for B&B algorithms. In our open-source solver
MAiNGO, we use the (multivariate) McCormick method [44, 70] to propagate relaxations
and their subgradients [49] through explicit functions using the MC++ library [15]. How-
ever, the McCormick method often does not provide the tightest possible relaxations,
i.e., the envelopes. In this section, we derive tight relaxations or envelopes of functions
that are relevant for GPs and Bayesian optimization. The functions and their relaxations
are implemented in MC++. When using these intrinsic functions and their relaxations in
MAiNGO, the (multivariate) McCormick method is only used for the remaining parts of
the model. Note that the derived relaxations are used within MAiNGO while BARON
does not allow for implementation of custom relaxations or piecewise deﬁned functions.

4.1 Covariance Functions

The covariance function is a key element of GPs that occurs N times within the optimiza-
tion of estimate or variance. Thus, tight relaxations are highly desirable. In this subsec-
tion, we derive envelopes for common Mat´ern covariance functions. We consider univariate
covariance functions, i.e., kν : R Ñ R, with input d “ px ´ x1qTΛ px ´ x1q ě 0. This is
possible because we consider stationary covariance functions that are invariant to trans-
lations in the input space. Common Mat´ern covariance functions use ν “ 1{2, 3{2, 5{2
and 8 and are given by:

kν“1{2pdq :“ exp

ˆ

kν“5{2pdq :“

1 `

´

¯

?

´

d

,

?
5

?

d `

5
3

kν“3{2pdq :“

˙

´

d

¨ exp

´

1 `
?
?
5

d

´

¯

?

d

¨ exp

´

´

¯

?
3

?

d
ˆ

?
3

¯

,

kSEpdq :“ exp

´

˙

d

,

1
2

where kSE is the squared exponential covariance function with ν Ñ 8. We ﬁnd that these
four covariance functions are convex because their Hessian is positive semideﬁnite. Thus,
the convex envelope is given by F cvpdq “ kpdq and the concave envelope by the secant
F ccpdq “ sctpdq where sctpdq “ kpdU q´kpdLq
on a given interval rdL, dU s.
As the McCormick composition and product theorems provide weak relaxations of kν“3{2
and kν“5{2 (c.f. ESI Section 1), we implement these functions and their envelopes in our
library of intrinsic functions in MC++. Furthermore, natural interval extensions are not

d ` dU kpdLq´dLkpdU q

dU ´dL

dU ´dL

7

exact for kν“3{2 and kν“5{2. Thus, we also provide exact interval bounds based on the
monotonicity.

?

It should be noted that covariance functions are commonly given as a function of
d. However, we chose to use d instead for three
the weighted Euclidean distance r “
main reasons: (1) x is usually a degree of freedom. Thus, the computation of r would
lead to potentially weaker relaxations for kν“5{2 and kSE. (2) The derivative of kν“3{2p¨q,
kν“5{2p¨q, and kSEp¨q is deﬁned at d “ 0 while the derivative of the square root function
is not. (3) The covariance functions ˆkv“3{2 : r ÞÑ kv“3{2pr2q, ˆkv“5{2 : r ÞÑ kv“5{2pr2q, and
ˆkSE : r ÞÑ kSEpr2q are nonconvex in r, so deriving the envelopes would be nontrivial.

Finally, it can be noted that we did not derive envelopes of kMat´ernpx, x1q, because the
variable input dimensions poses diﬃculties in implementation and the multidimensionality
is a challenge for derivation of envelopes. Nevertheless, the McCormick composition
theorem applied to kνpdpx, x1qq yields relaxations that are exact at the minimum of kMat´ern
because the natural interval extensions of the weighted squared distance d are exact
(c.f. [53]). This means that the relaxations are exact in Hausdorﬀ metric.

4.2 Gaussian Probability Density Function

The PDF is an auxiliary for the EI acquisition function and is given by φ : R Ñ R with

φpxq :“

1
?
2π

¨ exp

˙

ˆ

´x2
2

(3)

The Gaussian probability density function (PDF) is a nonconvex function for which the
McCormick composition rule does not provide its envelopes. For one-dimensional func-
tions, McCormick [44] also provides a method to construct envelopes. We construct the
envelopes of PDF using this method and implement them in our library of intrinsic func-
tions. The envelope of the PDF is illustrated in Figure 1 and derived in Appendix A.1.

Figure 1: Illustration of the envelope of the Gaussian PDF

8

-10123x00.10.20.30.4(x)Concave envelopeFunctionConvex envelope4.3 Gaussian Cumulative Distribution Function
The Gaussian cumulative distribution function (CDF) is given by Φ : R Ñ R with

ż

x

Φpxq :“

φptq dt “

´8

´

¯

?

2x
2

1 ` erf

2

.

(4)

The envelopes of the error function are already available in MC++ as an intrinsic function
and consequently the McCormick technique provides envelopes of the CDF (see Figure 2a
in ESI). In contrast, the error function is not available as an intrinsic function in BARON
and a closed-form expression does not exist. Thus, a numerical approximation is required
for optimization in BARON. Common numerical approximations of the error function
are only valid for x ě 0 and use point symmetry of the error function. To overcome this
technical diﬃculty in BARON, a big-M formulation with additional binary and continuous
variables is a possible workaround. However, this workaround leads to potentially weaker
relaxations (see Section 2 in the ESI).

4.4 Lower Conﬁdence Bound Acquisition Function

The lower conﬁdence bound (LCB) (upper conﬁdence bound when considering maxi-
mization) is an acquisition function with strong theoretical foundation. For instance,
a bound on its cumulative regret, i.e., a convergence rate for Bayesian optimization,
for relatively mild assumptions on the black-box function is known [65]. It is given by
LCB : R ˆ Rě0 Ñ R with

LCBpµ, σq :“ µ ´ κ ¨ σ
with a parameter κ P Rą0. LCB has not been popular in engineering applications as it
requires an additional tuning parameter κ and leads to heavy exploration when a rigorous
value for κ is chosen [65]. Recently, LCB has gained more popularity through application
as policy in deep reinforcement learning, e.g., by DeepMind [50]. LCB is a linear function
and thus McCormick relaxations are exact.

4.5 Probability of Improvement Acquisition Function

Probability of improvement (PI) computes the probability that a prediction at x is below
a given target fmin, i.e., ˜PIpxq “ P pf pxq ď fminq. When the underlying function is
distributed as a GP with mean µ and variance σ, the PI is given by PI : R ˆ Rě0 Ñ R
with
˘

`

fmin´µ
σ

$
’&

Φ
0,
1,

PIpµ, σq :“

’%

,

σ ą 0,
σ “ 0, fmin ď µ,
σ “ 0, fmin ą µ.

(5)

Tight relaxations of the PI acquisition function are derived in Section A.2 based on
a combination of McCormick relaxations, tailored relaxations based on componentwise
monononicity, and tailored relaxations based on componentwise convexity/concavity [47,
52]. Examples for the resulting relaxations on two diﬀerent subsets of the domain of PI
are shown in Figure 2.

9

Concave relaxation

PI

Convex
relaxation

(a)

(b)

Figure 2: Graph of the probability of improvement acquisition function (PI) as in Equation
(5) for fmin “ 0 along with the developed convex and concave relaxations. (a) On the
interval r´2, 2s ˆ r0, 10s, the relaxations are constructed on the basis of monotonicity
properties of PI. (b) On the interval r1, 2s ˆ r0, 1s, the relaxations are constructed on the
basis of componentwise convexity properties via the methods of Meyer and Floudas [47]
and Najman et al. [52]

4.6 Expected Improvement Acquisition Function

EI is the acquisition function that is most commonly used in Bayesian optimization [33]. It
‰
“
is deﬁned as ˜EIpxq “ IE
. When the underlying function is distributed
maxpfmin´f pxq, 0q
as a GP, EI : R ˆ Rě0 Ñ R is given by
$
’&

` σ ¨ φ

˘

˘

`

`

,

fmin´µ
σ

fmin´µ
σ

EIpµ, σq :“

pfmin ´ µq ¨ Φ
fmin ´ µ,
’%
0

σ ą 0
σ “ 0, µ ă fmin
σ “ 0, µ ě fmin

(6)

As noted by Jones et al. [33], EI is componentwise monotonic and thus, exact interval
bounds can easily be derived. In Section A.3, we show that EI is convex and we provide
its envelopes. As EI is not available as an intrinsic function in BARON, an algebraic
reformulation is necessary that uses Equation (6) where Φ is substituted from Equation (4)
with Equation (1) in ESI and φ from Equation (3). In addition, some workaround would
be necessary for σ “ 0 (e.g., additional binary variable and big-M formulation).

5 Numerical Results

We now investigate the numerical performance of the proposed method on one core of
an Intel Xeon CPU with 2.60 GHz, 128 GB RAM and Windows Server 2016 operating
system. We use MAiNGO version v0.2.1 and BARON v19.12.7 through GAMS v30.2.0
to solve diﬀerent optimization problems with GPs embedded. We use SLSQP [38] within
the pre-processing of MAiNGO as a local solver. The GP models, acquisition functions,
and training scripts are available open-source within the MeLOn toolbox [61] and the

10

-202051000.51µσ11.5200.5100.040.080.120.16σµrelaxations of the corresponding functions are available through the MC++ library used
by MAiNGO. We present three case studies. First, we illustrate the scaling of the method
w.r.t.
the number of training data points on a representative test function. Herein,
the estimate of the GP is optimized. Second, we consider a chemical engineering case
study with a chance constraint, which utilizes the variance prediction of a GP. Third, we
optimize an acquisition function that is commonly used in Bayesian optimization on a
chemical engineering dataset.

5.1 Illustrative Example & Scaling of the Algorithm

In the ﬁrst illustrative example, the peaks function is learned by GPs. Then, the GP
predictions are optimized on ˜X “ tx1, x2 P R : ´ 3 ď x1, x2 ď 3u. The peaks function is
given by fpeaks : R2 Ñ R with
fpeakspx1, x2q :“

3 p1 ´ x1q2 ¨ e´x2

1´ px2`1q2

´ 10 ¨ p

x1
5

´ x3

1 ´ x5

2q ¨ e´x2

1´x2

2 ´

e´px1`1q2´x2
3

2

The two-dimensional function has multiple suboptimal local optima and one unique global
minimizer at x˚ « r0.228, ´1.626sT with fpeakspx˚q « ´6.551.

We generate various training data on ˜X using a Latin hypercube sampling of sizes
10, 20, 30, ..., 500. Then, we train GPs with kν“1{2pdq, kν“3{2pdq, kν“5{2pdq, and kSEpdq
covariance functions on the data. After training, the predictions of the GPs are minimized
using the RS and FS formulation to locate an approximation of the minimum of fpeaks.
We run optimizations in MAiNGO once using the developed envelopes and once using
standard McCormick relaxations. Due to long CPU times, we run optimizations for the
FS formulations only for up to 250 data points in MAiNGO. The whole data generation,
training, and optimization procedure is repeated 50 times for each data set. Thus, we
train a total of 10, 000 GPs and run 90, 000 optimization problems in MAiNGO. We also
solve the FS and RS formulation in BARON by automatically parsing the problem from
our C++ implementation to GAMS. This is particularly important in the RS as equations
with several thousand characters are generated. We solve the RS problem for up to 360
and the FS for up to 210 data points in BARON due to the high computational eﬀort.
The optimality tolerances are set to (cid:15)abs. tol. “ 10´3 and (cid:15)rel. tol. “ 10´3 and the maximum
CPU time is set to 1, 000 CPU seconds. The feasibility tolerances are set to 10´6. The
analysis in this section is based on results for the kν“5{2 covariance function. The detailed
results for the other covariance functions show qualitatively similar results (c.f. ESI Sec-
tion 4).

In the FS, this problem has D ` 2 ¨ N ` 2 equality constraints and 2 ¨ D ` 2 ¨ N ` 2
optimization variables while the RS has D optimization variables and no equality con-
straints. Note that for practical applications the number of training data points is usually
much larger than the dimension of the inputs, i.e., N " D. The full problem formulation
is also provided in ESI Section 3.

Figure 3 shows a comparison of the CPU time for optimization of GPs. For the
solver MAiNGO, Figure 3a shows that RS formulation outperforms the FS formulation
by more than one order of magnitude and shows a more favorable scaling with the num-
ber of training data points. For example, the speedup increases to a factor of 778 for

11

(a) MAiNGO

(b) GAMS BARON

Figure 3: Comparison of the total CPU time for optimization, i.e., the sum of prepro-
cessing time and B&B time, of GPs with kν“5{2 covariance function. The plots show the
median of 50 repetitions of data generation, GP training, and optimization. Note that
#points are incremented in steps of 10 and the lines are interpolations between them

250 data points. Notably, the achieved speedup increases drastically with the number
of training data points (c.f. ESI Section 4). This is mainly due to the fact that the
CPU times for the FS formulations scale approximately cubically with the data points
(CPUF S w{ envpN q “ 1.053 ¨ 10´4N 2.958 sec with R2 “ 0.993) while the ones for the RS
scale almost linearly (CPURS w{ envpN q “ 0.0022 ¨ N 1.156 sec with R2 “ 0.995).

In general, the number of optimization variables can lead to an exponential growth of
the worst-case B&B iterations and thus runtime. In this particular case, the number of
B&B iterations is very similar for the FS and RS formulation (see Figure 4a). Instead,
for the present problems the number of B&B iterations is more inﬂuenced by the use of
tight relaxations. Figure 4b shows that the CPU time per iteration increases drastically
with problem size in the FS while it increases only moderately in the RS. This indicates
that the solution time of the lower bounding, upper bounding, and bound tightening sub-
problems scales favorably in the RS and that this is the main reason for speedup of the
RS formulation in MAiNGO. This is probably due to the smaller subproblem sizes when
using McCormick relaxations in the RS formulation (c.f. discussion in Section 1).

The use of envelopes of covariance functions also improves computational performance
(see Figure 3a). However, this eﬀect is approximately constant over the problem size (c.f.
Figure 3 in ESI Section 4). In other words, the CPU time shows a similar trend for the
cases with and without envelopes in Figure 3a. In the RS, the CPU time with envelopes
takes on average 7.1% of the CPU time without envelopes (« 14 times less). In the FS,
the impact of the envelopes is less pronounced, i.e., the CPU time w/ envelopes is on
average 15.1% of the CPU time w/o envelopes (« 6.6 times less). Figure 4a shows that
the envelopes considerably reduce the number of necessary B&B iterations. However,
the relaxations do not show a signiﬁcant inﬂuence on the CPU time per iteration (see
Figure 4b).

The results of this numerical example show clearly that the development of tight re-
laxations is more important for the RS formulation than for the FS. As shown in Section

12

1050100250500# training data points (-)10-2100102104CPU time (s)time limitFS w/o envelopesFS w/ envelopesRS w/o envelopesRS w/ envelopes1050100250500# training data points (-)10-2100102104CPU time (s)time limitFS BARONRS BARON(a) B&B iterations in MAiNGO

(b) CPU time per B&B iteration in MAiNGO

Figure 4: Comparison of number of B&B iterations of optimization problems with GPs
embedded with kν“5{2 covariance function. The plots show the median of 50 repetitions
of data generation, GP training, and optimization. Note that #points are incremented in
steps of 10 and the lines are interpolations between them

3.4.2 of [3], this eﬀect can be explained by the fact that in RS, it is more likely to have re-
occurring nonlinear factors which can cause the McCormick relaxations to become weaker
(c.f. also the relationship to the AVM in this case explored in [70]). However, in this study,
the improvement in relaxations is outweighed by the increase of CPU time per iteration
when additional variables are introduced in the FS.

The RS formulation also performs favorably compared to the FS formulation in the
solver BARON (see Figure 3b). However, the diﬀerences between the CPU times are less
pronounced. In contrast to MAiNGO, the number of B&B iterations in the FS and RS
drastically increase with increasing number of training data points when using BARON
(c.f. Figure 4 in ESI). Also, the time per B&B iteration is similar between RS and FS.
This is probably due to the AVM method for the construction of relaxations. The AVM
method introduces auxiliary variables for some factorable terms. Thus, the size of the
subproblems in BARON increases with the number of training data points regardless
which of the two formulations is used.

5.2 Chance-Constrained Programming

Probabilistic constraints are relevant in engineering and science [17] and GPs have been
used in the previous literature to formulate chance constraints, e.g., in model predictive
control [10].

As a second case study, we consider the N-benzylation reaction of α methylbenzy-
lamine with benzylbromide to form desired secondary (2˝) amine and undesired tertiary
(3˝) amine. We utilize an experimental data set consisting of 78 data points from a robotic
chemical reaction platform [59]. We aim to maximize the expected space-time yield of
2˝ amine (2˝-STY) and ensure that the probability of a product quality constraint sat-
isfaction is above 95%. The 2˝-STY and yield of 3˝ amine impurity (3˝-Y) are modeled
by individual GPs. Thus, we solve optimization problems with two GPs embedded. The

13

1050100250500# training data points (-)101102103# branch-and-bound iterations (-)RS w/o envelopesFS w/o envelopesFS w/ envelopesRS w/ envelopes1050100250500# training data points (-)10-310-210-1100101time per iteration (s)FS w/o envelopesFS w/ envelopesRS w/o envelopesRS w/ envelopeschance-constrained optimization problem is formulated as follows

min
xPE
s.t.

“
´ IE

fSTYpxq

‰

P pfimpuritypxq ď cq ě 95%

Here, the objective is to minimize the negative of the expected STY. This corresponds to
minimizing the negative prediction of the GP, i.e., ´mD,2˝´STY. The chance constraint
ensures that the impurity is below a parameter c with a probability of 95%. This corre-
sponds to the constraint mD,3˝´Y ` 1.96 ¨

kD,3˝´Y ď c with c “ 5.

?

The optimization is conducted with respect to four optimization variables: (1) the
primary (1˝) amine ﬂow rate of the feed varying between 0.2 and 0.4 mL min´1, (2) the
ratio between benzyl bromide and 1˝ amine varying between 1.0 and 5.0, (3) the ratio be-
tween solvent and 1˝ amine varying between 0.5 and 1.0, and (4) the temperature varying
between 110 and 150˝C.

As this problem is highly multimodal and diﬃcult to solve, we increase the number
of local searches in pre-processing in MAiNGO to 500 and increase the maximum CPU
time to 24 hours. The computational performance of the diﬀerent methods is given in
Table 1. The results show that none of the considered methods converged to the desired
tolerance within the time limit. The RS formulation in MAiNGO that uses the proposed
envelopes outperforms the other formulations and BARON solver as it yields the smallest
optimality gap. Note that the considered SLSQP solver does not ﬁnd any valid solution
point in the FS in MAiNGO while feasible points are found in the RS. This demonstrates
that the RS formulation can also be advantageous for local solvers. Note that when using
IPOPT [73] with 500 multistart points in the FS formulation in MAiNGO, it identiﬁes
a local optimum with f ˚ “ ´226.5 in the pre-processing. In the ESI, we provide a brief
comparison of a few pre-processing settings for this case study.

The best solution of the optimization problem that we found is x1 “ 0.40 min´1,

Table 1: Numerical results of the N-benzylation reaction optimization with chance con-
straint (Subsection 5.2). The FS formulation has 330 optimization variables, 326 equality
constraints, and 1 inequality constraint. The RS formulation has 4 optimization variables,
0 equality constraints, and 1 inequality constraint. The CPU time limit is 86,400 seconds

Solver
MAiNGO w/ Env.
MAiNGO w/o Env.
BARON
MAiNGO w/ Env.
MAiNGO w/o Env.
BARON

CPU [s]
86,400
86,400
86,400
86,400
86,400
86,400

Iter.
26,761
20,795
219,239
1.6 ¨ 106
1.1 ¨ 106
7,927

UB
N/A
N/A
-226.5
-226.5
-226.5
-226.5

LB Abs. gap
N/A
N/A
53,548.5
18.3
346.7
56,167.5

-379,2
-1,704.1
-53,775.0
-244.8
-573.2
-56,394.0

(FS)

(RS)

x2 “ 1.0, x3 “ 0.5, and x4 “ 123.5˝C. At the optimal point, the predicted 2˝-STY is
226.5 kg m´3 h´1 with a variance of 17.1 while the predicted amine impurity is 4.2 %
with a variance of 0.17. The result shows that the probability constraint ensures a safety
margin between the predicted impurity and c “ 5. Note that the chance constraint is
active at the optimal solution point.

14

5.3 Bayesian Optimization

In the third case study, we consider the synthesis of layer-by-layer membranes. Membrane
development is a prerequisite for sustainable supply of safe drinking water. However,
synthesis of membranes is often based on try-and-error leading to extensive experimental
eﬀorts, i.e., building and measuring a membrane in the development phase usually takes
several weeks per synthesis protocol. In this case study, we plan to improve the retention
of Na2SO4 salt of a recently developed layer-by-layer nanoﬁltration membrane system.
The optimization variables are the sodium chloride concentration in the polyelectrolyte
solution cN aCl P r0, 0.5s gL´1, the deposited polyelectrolyte mass mP E P r0, 5s gm´2,
and the number of layers Nlayer P t1, 2, 3, .., 10u. The detailed description of the setup is
given in the literature [46, 56]. Overall, we utilize 63 existing data points from previous
literature [46]. We identify a promising synthesis protocol based on the EI acquisition
function. Thus, this numerical example corresponds to one step of a Bayesian optimization
setup for this experiment. Global optimization of the acquisition function is particularly
relevant due to inherent multimodality of the acquisition functions [37] and high cost of
experiments. Note that the experimental validation of this data point is not within the
scope of this work.

Table 2: Numerical results of the membrane synthesis optimization (Subsection 5.3). The
FS formulation has 136 optimization variables and 133 equality constraints. The RS
formulation has 3 optimization variables

Solver

(FS) MAiNGO w/ Env.
(RS) MAiNGO w/ Env.

CPU [s]
3,802
405

Iter.
25,995
14,331

UB
-2.025
-2.025

LB Abs. gap
0.002
0.002

-2.027
-2.027

The computational performance of the proposed method is summarized in Table 2.
Using the solver MAiNGO, the RS formulation converges approximately 9 times faster to
the desired tolerance compared to the FS formulation. Herein, we use the derived tailored
relaxations of the EI acquisition function and envelopes of the covariance functions in both
cases. Notably, the FS requires approximately 1.8 times the number of B&B iterations
compared to the RS formulation, which is much less than the overall speedup. Thus, the
results are in good agreement with the previous examples showing that the reduction of
CPU time per iteration in the RS has a major contribution to the overall speedup. For
this example, a comparison to BARON is omitted due to necessary workarounds including
several integer variables and function approximations for CDF and EI (c.f., Section 4.3,
4.6).

The optimal solution point of the optimization problem is cN aCl “ 0.362 gL´1, mP E “
0 gm´2, and Nlayer “ 4. The expected retention is 85.32 with a standard deviation
σ “ 14.8. The expected retention is actually worse than the best retention in the training
data of 96.1. However, Bayesian optimization takes also the high variance of the solution
into account, i.e., it is also exploring the space. EI identiﬁes an optimal trade-oﬀ between
exploration and exploitation.

15

6 Conclusions

We propose a RS formulation for the deterministic global solution of problems with trained
GPs embedded. Also, we derive envelopes of common covariance functions or tight relax-
ations of acquisition functions leading to tight overall problem relaxations.

The computational performance is demonstrated on illustrative and engineering case
studies using our open-source global solver MAiNGO. The results show that the number
of optimization variables and equality constraints are reduced signiﬁcantly compared to
the FS formulation.
In particular, the RS formulation results in smaller subproblems
whose size does not scale with the number of training data points when using McCormick
relaxations. This leads to tractable solution times and overcomes previous computational
limitations. For example, we archive a speedup factor of 778 for a GP trained on 250 data
points. The GP training methods and models are provided as an open-source module
called “MeLOn - Machine Learning Models for Optimization” toolbox [61].

We thus demonstrate a high potential for future research and industrial applications.
For instance, global optimization of the acquisition function can improve the eﬃciency
of Bayesian optimization in various applications. It also allows to easily include integer
decisions and nonlinear constraints in Bayesian optimization. Furthermore, the proposed
method could be extended to various related concepts such as multi-task GPs [7], deep
GPs [19], global model-predictive control with dynamic GPs [74, 12], and Thompson sam-
pling [16, 11]. Finally, the proposed work demonstrates that the RS formulation may be
advantageous for a wide variety of problems that have a similar structure, including var-
ious machine-learning models, model ensembles, Monte-Carlo simulation, and two-stage
stochastic programming problems.

A Derivations of Convex and Concave Relaxations

For the sake of simplicity, we use the same symbols in each subsection for the correspond-
ing convex (F cv) and concave (F cc) relaxations. To solve the one-dimensional nonlinear
equation that arise multiple times in the following, we use Newton’s method with 100
iterations and a tolerance of 10´9. If this is not successful, we run a golden section search
as a backup.

A.1 Probability Density Function of Gaussian Distribution

In this subsection, the envelopes of the PDF are derived on a compact interval D “
rxL, xU s. As the probability density function is one-dimensional, McCormick [44] gives a
method to construct its envelopes. The PDF is convex on s ´ 8, ´1s and r1, 8r and it is
concave on r´1, 1s. Its convex envelope, F cv : R Ñ R, and concave envelope, F cc : R Ñ R,

16

are given by

F cvpxq “

F ccpxq “

$

’’’’’’’’&
’’’’’’’’%

φpxq,
F cv
2 pxq,
sctpxq,
F cv
4 pxq,
F cv
5 pxq,
φpxq,

xU ď ´1,
xL ď ´1, ´1 ď xU ď 1,
´1 ď xL,
´1 ď xL,
xL ď ´1,
xL ě 1

xU ď 1,
xU ě 1,
xU ě 1,

$

’’’’’’’’&
’’’’’’’’%

sctpxq,
F cc
2 pxq,
φpxq,
F cc
4 pxq,
F cc
5 pxq,
sctpxq,

xU ď ´1,
xL ď ´1, ´1 ď xU ď 1,
´1 ď xL,
´1 ď xL ď 1,
xL ď ´1,
xL ě 1

xU ě 1,

xU ě 1,

xU ď 1,

where sctpxq “ φpxU q´φpX Lq

xU ´xL

: R Ñ R is given by:

¨ px ´ xLq ` φpxLq. F cc
2
$
&

c,2q´φpxLq
φpxU
xU
c,2´xL
φpxq,

%

¨ px ´ xLq ` φpxLq,

F cc

2 pxq “

x ď xU
x ą xU

c,2,
c,2,

where xU
F cv
2

c,2 “ minpxU ˚
: R Ñ R is given by:

c,2 , xU q and xU ˚

c,2 is the solution of dφ

dx

ˇ
ˇ

x “ φpxq´φpxLq

x´xL

,

x P r´1, 0s.

F cv

2 pxq “

$
&

%

φpxq,
φpxU ´φpxL
xU ´xL
c,2

c,2qq

¨ px ´ xL

c,2q ` φpxL

c,2q,

x ď xL
x ą xL

c,2,
c,2,

where xL
F cc
4

c,2 “ maxpxL˚
: R Ñ R is given by:

c,2, xLq and xL˚

c,2 is the solution of dφ

dx

ˇ
ˇ

x “ φpxq´φpxLq

x´xL

,

x P rxL, ´1s.

F cc

4 pxq “

$
&

%

φpxq,
φpxU q´φpxL
xU ´xL
c,4

c,4q

¨ px ´ xL

c,4q ` φpxL

c,4q,

x ă xL
x ě xL

c,4,
c,4,

where xL
F cv
4

c,4 “ maxpxL˚
: R Ñ R is given by:

c,4, xLq and xL˚

c,4 is the solution of dφ

dx

ˇ
ˇ

x “ φpxq´φpxLq

x´xL

,

x P r0, xU s.

F cv

4 pxq “

$
&

%

φpxU
c,4q´φpxLq
xU
c,4´xL
φpxq,

¨ px ´ xLq ` φpxLq,

x ď xU
x ą xU

c,4,
c,4,

on rxL, ´1s if
where xU
xL ` xU ă 0 or r1, xU s if xL ` xU ą 0 The case xL ` xU “ 0 is symmetrical and handled

c,4 is the solution of dφ

c,4 , xU q and xU ˚

c,4 “ minpxU ˚

x “ φpxq´φpxLq

x´xL

dx

ˇ
ˇ

17

separately to avoid numerical issues in Newton. F cc
5
of F cc

2 and F cc
4 :

: R Ñ R is given by a combination

F cc

5 pxq “

$
’’&
’’%

c,2q´φpxLq
φpxU
xU
c,2´xL
φpxq,
φpxU q´φpxL
xU ´xL
c,4

c,4q

F cv
5

: R Ñ R is given by:

¨ px ´ xLq ` φpxLq,

¨ px ´ xL

c,4q ` φpxL

c,4q,

F cv

5 pxq “

$

’’’’’&
’’’’’%

φpxU
c,5q´φpxLq
xU
c,5´xL
φpxq,
φpxq,
φpφpxU ´xL
xU ´xL
c,5

c,5qq

¨ px ´ xLq ` φpxLq,

¨ px ´ xL

c,5q ` φpxL

c,5q,

x ď xU
c,2,
c,2 ă x ă xL
xU
x ě xL
c,4,

c,4,

xL ` xU ě 0,
xL ` xU ě 0,
xL ` xU ă 0,
xL ` xU ă 0,

x ď xU
x ą xU
x ď xL
x ą xL

c,5,
c,5,
c,5,
c,5,

c,5 “ minpxU ˚

c,5 , xU q and xU ˚

c,5 is the solution of dφ

on rxL, 0s. Further,

where xU
c,5 “ maxpxL˚
xL

c,5, xLq and xL˚

c,5 is the solution of dφ

dx

ˇ
ˇ
x “ φpxq´φpxLq

x´xL

dx

x “ φpxq´φpxLq

x´xL

ˇ
ˇ

on r0, xU s.

A.2 Probability of Improvement Acquisition Function

In this section, a tight relaxation of the PI acquisition function is derived. PI is continuous
for all pµ, σq P R ˆ r0, 8r ztp0, 0qu, since limxÑ`8 Φpxq “ 1 and limxÑ´8 Φpxq “ 0.

A.2.1 Monotonicity

From the gradient of PI on R ˆ p0, 8q,

∇PIpµ, σq “ ´

1
?

2π

σ2 ¨

¨ exp

´

pfmin ´ µq2
2 ¨ σ2

˜

¸ „



,

σ
pfmin ´ µq

where fmin is a given target, we identify the following monotonicity properties:

• PI is monotonically decreasing with respect to µ.

• If µ ă fmin then PI is monotonically decreasing with respect to σ.

• If µ ě fmin then PI is monotonically increasing with respect to σ (recall that

PIpfmin, 0q “ 0, and PIpfmin, σq “ 0.5 @σ P s0, 8r ).

These properties can be used to obtain exact interval bounds on the function values of
PI. Furthermore, they can be exploited to construct relaxations as described in Section
A.2.3.

18

A.2.2 Componentwise Convexity

The Hessian of PI on R ˆ p0, 8r is given by
«

∇2PIpµ, σq “

´ fmin´µ
σ3
´ pfmin´µq2´σ2
σ4

´ pfmin´µq2´σ2
σ4
pfmin´µq¨pp2σ2´fmin´µq2
σ5

ﬀ

q

¨

1
?
2π

pfmin´µq2
2σ2

.

¨ e´

The Hessian is indeﬁnite and PI is therefore neither convex nor concave on its whole
domain. However, we ﬁnd componentwise convexity properties on certain parts of the
domain, i.e., convexity with respect to one variable when the other is ﬁxed. To this end,
we divide the domain into the following four sets:

• I1 :“ tpµ, σq | µ ď fmin ^ µ ´ fmin ě ´

2σu,

• I2 :“ tpµ, σq | µ ě fmin ^ µ ´ fmin ď `

2σu,

• I3 :“ tpµ, σq | µ ď fmin ^ µ ´ fmin ď ´

2σu,

• I4 :“ tpµ, σq | µ ě fmin ^ µ ´ fmin ě `

2σu.

?

?

?

?

On these sets, PI has the componentwise convexity properties listed in Table 3.

Table 3: componentwise convexity properties of PI over subsets of its domain

Subset
I1
I2
I3
I4

componentwise property
concave w.r.t. µ convex w.r.t. σ
convex w.r.t. µ concave w.r.t. σ
concave w.r.t. µ concave w.r.t. σ
convex w.r.t. µ convex w.r.t. σ

A.2.3 Relaxations

σ

We construct relaxations of PI over a given subset X “ rµL, µUs ˆ rσL, σUs of its domain
depending on which of the four sets I1-I4 contains the set X . If X Ă I1 Y I2, we use the
McCormick relaxations obtained by applying the multivariate composition theorem [70]
to the composition of the rational function fmin´µ
with Φ (c.f. Equation (5)), since these
are already very tight. If X does not fully lie within I1 Y I2, the McCormick relaxations
get increasingly weaker and we thus resort to other methods as described in the following.
If X Ă I4, PI is componentwise convex with respect to both variables. Therefore, the
concave envelope of PI over X consists of two planes anchored at the four corner points of
X and can be calculated as described by [47]. A tight convex relaxation can be obtained
using the method by [52]. Since the oﬀ-diagonal entries of the Hessian have a constant
sign over I4, a suﬃcient condition for this method is fulﬁlled (c.f. Corollary 1 in [52]).
An example for the resulting relaxation is shown in Figure 2b. Similarly, if X Ă I3, PI is
componentwise concave and we obtain its convex envelope using the method by [47] and
a tight concave relaxation using the method by [52].

If X Ă I2 Y I4, we construct relaxations exploiting the monotonicity properties of PI.

19

Since for all pµ, σq P I2 YI4 we have µ ě fmin, PI is thus monotonically decreasing in µ and
increasing in σ over X . Therefore, we can construct a convex relaxation PIcv
2,4 : X Ñ r0, 1s
as

PIcv

2,4pµ, σq :“ max

σLpµq, f cv
f cv

µUpσq

,

`

˘

where f cv

σL and f cv

µU are the convex envelopes of the univariate functions

and

fσL : rµL, µUs Ñ r0, 1s, µ ÞÑ PIpµ, σLq

fµU : rσL, σUs Ñ r0, 1s, σ ÞÑ PIpµU, σq,

(7)

(8)

σLpµq and f cv

respectively, i.e., they correspond to the function PI restricted to one-dimensional facets
of X at σL and µU. Both f cv
µUpσq are valid relaxations of PI because of the
monotonicity of PI over I2 Y I4. By taking the pointwise maximum in (7), we obtain
a tighter relaxation while preserving convexity. To compute f cv
µU, we can use
the method described in Section 4 of [44] because they are one-dimensional functions
with a known inﬂection point. To apply this method, we typically need to solve a one-
dimensional nonlinear equation, which we do via Newton’s method. A concave relaxation
can be obtained analogously using concave envelopes of PI over one-dimensional facets of
X at σU and µL. If X Ă I1YI3, an analogous method can be used since PI is monotonically
increasing in both µ and σ.

σL and f cv

In the most general case, X contains parts of all four sets I1-I4. In this case, we can
still obtain relaxations by exploiting monotonicity properties. In particular, we compute
a convex relaxation PIcv

1´4 : X Ñ r0, 1s as

PIcv

1´4pµ, σq :“ max

´
˜f cv
σLpµq, f cv

µUpσq

¯

,

(9)

where f cv
µU is again the convex relaxation of the univariate function fµU as in (8), which
is still valid because PI is decreasing with respect to µ on its entire domain. In contrast,
the convex relaxation of the univariate function at σL as in (7) is not valid because PI is
not monotonic with respect to σ. Instead, in (9) it is replaced by the convex relaxation
σL of the univariate function ˜fσL : rµL, µUs Ñ r0, 1s with
˜f cv

#

˜fσLpµq :“

PIpµ, σLq,
PIpfmin, σLq ` PIpfmin,σLq´PIpµL,σUq

fmin´µL

µ ě fmin,
pµ ´ fminq , otherwise.

(10)

To see that ˜f cv
σL is a valid relaxation of PI, we ﬁrst note that by deﬁnition it is a relaxation
of ˜fσL, so it suﬃces to show that ˜fσL is in turn a relaxation of PI. The latter is established
in the following Lemma.
Lemma: Let PI be deﬁned as in (5) and ˜fσL as in (10). Then ˜fσLpµq ď PIpµ, σq @pµ, σq P
X :“ rµL, µUs ˆ rσL, σUs.
Proof : Consider ﬁrst any ﬁxed ˆµ such that ˆµ ě fmin. In this case, we have ˜fσLpˆµq “
PIpˆµ, σLq ď PIpˆµ, σq @σ P rσL, σUs because of the monotonicity w.r.t σ (c.f. Section A.2.1).
Next, consider any ˜µ such that ˜µ ă fmin. Note that this implies µL ă fmin. In this case,

20

we have

˜fσLp˜µq “ PIpfmin, σLq `

PIpfmin, σLq ´ PIpµL, σUq
fmin ´ µL

p˜µ ´ fminq

“ PIpfmin, σLq

ď PIpfmin, σUq

˜µ ´ µL
fmin ´ µL ` PIpµL, σUq
˜µ ´ µL
fmin ´ µL ` PIpµL, σUq

fmin ´ ˜µ
fmin ´ µL
fmin ´ ˜µ
fmin ´ µL

ď PIp˜µ, σUq
ď PIp˜µ, σq @σ P rσL, σUs,

where the inequalities follow, in this order, from the monotonicity of PI with respect to
σ for µ ě fmin, its componentwise concavity with respect to µ for µ ă fmin, and its
monotonicity with respect to σ for µ ă fmin.

A.3 Expected Improvement Acquisition Function

We now show that the EI acquisition function is convex. From the Hessian matrix of EI
on R ˆ p0, 8q

HessEIpµ, σq “

„

1
σ
´ µ´fmin
σ2

´ µ´fmin
σ2
pµ´fminq2
σ3



ˆ

¨ φ

´

˙

,

µ ´ fmin
σ

we ﬁnd the eigenvalues 0 and pµ´fminq2`σ2
the envelopes can be constructed directly.

σ3

`

¨ φ

´ µ´fmin
σ

˘

. As σ ě 0, EIp¨, ¨q is convex and

Contributions of authors AMS designed the research concept, ran simulations, and
wrote the manuscript. AMS and DB interpreted the results. AMS, DB, JN, and AM
edited the manuscript. AMS, XL, DG implemented the GP models and parser. AMS,
DB, JN and TK derived the function relaxations. JN and DB implemented the function
relaxations. AM is principle investigator and guided the eﬀort.
Acknowledgements: We are grateful to Benoˆıt Chachuat for providing MC++. We are
grateful to Daniel Menne, Deniz Rall and Matthias Wessling for providing the experimen-
tal membrane data for Example 3.

Conﬂict of interest

The authors declare that they have no conﬂict of interest.

References

[1] Amar, Y., Schweidtmann, A.M., Deutsch, P., Cao, L., Lapkin, A.: Machine learning
and molecular descriptors enable rational solvent selection in asymmetric catalysis.
Chemical science 10(27), 6697–6706 (2019). DOI 10.1039/C9SC01844A

21

[2] Androulakis, I.P., Maranas, C.D., Floudas, C.A.: αBB: A global optimization
method for general constrained nonconvex problems. Journal of Global Optimization
7(4), 337–363 (1995). DOI 10.1007/BF01099647

[3] Bongartz, D.: Deterministic global ﬂowsheet optimization for the design of energy

conversion processes. Ph.D. thesis, RWTH Aachen University (2020)

[4] Bongartz, D., Mitsos, A.: Deterministic global optimization of process ﬂowsheets in
a reduced space using McCormick relaxations. Journal of Global Optimization 20(9),
419 (2017). DOI 10.1007/s10898-017-0547-4

[5] Bongartz, D., Mitsos, A.: Deterministic global ﬂowsheet optimization: Between
equation-oriented and sequential-modular methods. AIChE Journal 65(3), 1022–
1034 (2019). DOI 10.1002/aic.16507

[6] Bongartz, D., Najman, J., Sass, S., Mitsos, A.: MAiNGO: McCormick-based Algo-
rithm for mixed integer Nonlinear Global Optimization. Technical report, Process
Systems Engineering (AVT.SVT), RWTH Aachen University (2018). URL https:
//www.avt.rwth-aachen.de/global/show_document.asp?id=aaaaaaaaabclahw

[7] Bonilla, E.V., Chai, K.M., Williams, C.: Multi-task Gaussian process prediction. In:

Advances in neural information processing systems, pp. 153–160 (2008)

[8] Boukouvala, F., Floudas, C.A.: Argonaut: Algorithms for global optimization of
constrained grey-box computational problems. Optimization Letters 11(5), 895–913
(2017). DOI 10.1007/s11590-016-1028-2

[9] Boukouvala, F., Misener, R., Floudas, C.A.: Global optimization advances in mixed-
integer nonlinear programming, MINLP, and constrained derivative-free optimiza-
tion, CDFO. European Journal of Operational Research 252(3), 701–727 (2016).
DOI 10.1016/j.ejor.2015.12.018

[10] Bradford, E., Imsland, L., Zhang, D., Chanona, E.A.d.R.: Stochastic data-driven
model predictive control using Gaussian processes. arXiv preprint arXiv:1908.01786
(2019)

[11] Bradford, E., Schweidtmann, A.M., Lapkin, A.: Eﬃcient multiobjective optimization
employing Gaussian processes, spectral sampling and a genetic algorithm. Journal
of Global Optimization 71(2), 407–438 (2018). DOI 10.1007/s10898-018-0609-2

[12] Bradford, E., Schweidtmann, A.M., Zhang, D., Jing, K., del Rio-Chanona, E.A.:
Dynamic modeling and optimization of sustainable algal production with uncertainty
using multivariate Gaussian processes. Computers & Chemical Engineering 118,
143–158 (2018). DOI 10.1016/j.compchemeng.2018.07.015

[13] Caballero, J.A., Grossmann, I.E.: An algorithm for the use of surrogate models in
modular ﬂowsheet optimization. AIChE Journal 54(10), 2633–2650 (2008). DOI
10.1002/aic.11579

22

[14] Caballero, J.A., Grossmann, I.E.: Rigorous ﬂowsheet optimization using process
simulators and surrogate models. In: Computer Aided Chemical Engineering, vol. 25,
pp. 551–556. Elsevier (2008)

[15] Chachuat, B., Houska, B., Paulen, R., Peric, N., Rajyaguru, J., Villanueva, M.E.:
Set-theoretic approaches in analysis, estimation and control of nonlinear systems.
IFAC-PapersOnLine 48(8), 981–995 (2015). DOI 10.1016/j.ifacol.2015.09.097

[16] Chapelle, O., Li, L.: An empirical evaluation of Thompson sampling.

In:
J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, K.Q. Weinberger
Information Processing Systems 24, pp. 2249–
(eds.) Advances
2257. Curran Associates,
Inc. (2011). URL http://papers.nips.cc/paper/
4321-an-empirical-evaluation-of-thompson-sampling.pdf

in Neural

[17] Charnes, A., Cooper, W.W.: Chance-constrained programming. Management science

6(1), 73–79 (1959). DOI 10.1287/mnsc.6.1.73

[18] Cozad, A., Sahinidis, N.V., Miller, D.C.: Learning surrogate models for simulation-
based optimization. AIChE Journal 60(6), 2211–2227 (2014). DOI 10.1002/aic.14418

[19] Damianou, A., Lawrence, N.: Deep Gaussian processes. In: Artiﬁcial Intelligence

and Statistics, pp. 207–215 (2013)

[20] Davis, E., Ierapetritou, M.: A Kriging method for the solution of nonlinear programs
with black-box functions. AIChE Journal 53(8), 2001–2012 (2007). DOI 10.1002/
aic.11228

[21] Davis, E., Ierapetritou, M.: A kriging-based approach to MINLP containing black-
box models and noise. Industrial & Engineering Chemistry Research 47(16), 6101–
6125 (2008). DOI 10.1021/ie800028a

[22] Davis, E., Ierapetritou, M.: A centroid-based sampling strategy for Kriging global
modeling and optimization. AIChE journal 56(1), 220–240 (2010). DOI 10.1002/aic.
11881

[23] Del Rio-Chanona, E.A., Cong, X., Bradford, E., Zhang, D., Jing, K.: Review of
advanced physical and data-driven models for dynamic bioprocess simulation: Case
study of algae–bacteria consortium wastewater treatment. Biotechnology and bio-
engineering 116(2), 342–353 (2019). DOI 10.1002/bit.26881

[24] Eason, J.P., Biegler, L.T.: A trust region ﬁlter method for glass box/black box
optimization. AIChE Journal 62(9), 3124–3136 (2016). DOI 10.1002/aic.15325

[25] Epperly, T.G.W., Pistikopoulos, E.N.: A reduced space branch and bound algorithm
for global optimization. Journal of Global Optimization 11(3), 287–311 (1997). DOI
10.1023/A:1008212418949

23

[26] Freier, L., Hemmerich, J., Sch¨oler, K., Wiechert, W., Oldiges, M., von Lieres, E.:
Framework for Kriging-based iterative experimental analysis and design: Optimiza-
tion of secretory protein production in corynebacterium glutamicum. Engineering in
Life Sciences 16(6), 538–549 (2016). DOI 10.1002/elsc.201500171

[27] Gill, P.E., Murray, W., Saunders, M.A.: SNOPT: An SQP algorithm for large-
scale constrained optimization. SIAM review 47(1), 99–131 (2005). DOI 10.1137/
S0036144504446096

[28] Glassey, J., Von Stosch, M.: Hybrid Modeling in Process Industries. CRC Press

(2018)

[29] Hasan, M.F., Baliban, R.C., Elia, J.A., Floudas, C.A.: Modeling, simulation, and
optimization of postcombustion CO2 capture for variable feed concentration and
ﬂow rate. 2. pressure swing adsorption and vacuum swing adsorption processes.
Industrial & Engineering Chemistry Research 51(48), 15665–15682 (2012). DOI
10.1021/ie301572n

[30] Helmdach, D., Yaseneva, P., Heer, P.K., Schweidtmann, A.M., Lapkin, A.A.: A
multiobjective optimization including results of life cycle assessment in developing
biorenewables-based processes. ChemSusChem 10(18), 3632–3643 (2017). DOI
10.1002/cssc.201700927

[31] Horst, R., Tuy, H.: Global Optimization: Deterministic Approaches, 3 edn. Springer,

Berlin, Heidelberg (1996). DOI 10.1007/978-3-662-03199-5

[32] H¨ullen, G., Zhai, J., Kim, S.H., Sinha, A., Realﬀ, M.J., Boukouvala, F.: Managing
uncertainty in data-driven simulation-based optimization. Computers & Chemical
Engineering p. 106519 (2019). DOI 10.1016/j.compchemeng.2019.106519

[33] Jones, D.R., Schonlau, M., Welch, W.J.: Eﬃcient global optimization of expensive
black-box functions. Journal of Global optimization 13(4), 455–492 (1998). DOI
10.1023/A:1008306431147

[34] Kahrs, O., Marquardt, W.: The validity domain of hybrid models and its application
in process optimization. Chemical Engineering and Processing: Process Intensiﬁca-
tion 46(11), 1054–1066 (2007). DOI 10.1016/j.cep.2007.02.031

[35] Keßler, T., Kunde, C., McBride, K., Mertens, N., Michaels, D., Sundmacher, K.,
Kienle, A.: Global optimization of distillation columns using explicit and implicit
surrogate models. Chemical Engineering Science 197, 235–245 (2019). DOI 10.1016/
j.ces.2018.12.002

[36] Keßler, T., Kunde, C., Mertens, N., Michaels, D., Kienle, A.: Global optimization
of distillation columns using surrogate models. SN Applied Sciences 1(1), 11 (2019).
DOI 10.1007/s42452-018-0008-9

[37] Kim, J., Choi, S.: On local optimizers of acquisition functions in bayesian optimiza-

tion. arXiv preprint arXiv:1901.08350 (2019)

24

[38] Kraft, D.: Algorithm 733: TOMP–fortran modules for optimal control calculations.
ACM Transactions on Mathematical Software (TOMS) 20(3), 262–281 (1994). DOI
10.1145/192115.192124

[39] Krige, D.G.: A statistical approach to some basic mine valuation problems on the
witwatersrand. Journal of the Southern African Institute of Mining and Metallurgy
52(6), 119–139 (1951)

[40] Lin, Z., Wang, J., Nikolakis, V., Ierapetritou, M.: Process ﬂowsheet optimization of
chemicals production from biomass derived glucose solutions. Computers & Chemical
Engineering 102, 258–267 (2017). DOI 10.1016/j.compchemeng.2016.09.012

[41] Maher, S.J., Fischer, T., Gally, T., Gamrath, G., Gleixner, A., Gottwald, R.L., Hen-
del, G., Koch, T., L¨ubbecke, M.E., Miltenberger, M., M¨uller, B., Pfetsch, M.E.,
Puchert, C., Rehfeldt, D., Schenker, S., Schwarz, R., Serrano, F., Shinano, Y.,
Weninger, D., Witt, J.T., Witzig, J.: The SCIP optimization suite (version 4.0)

[42] McBride, K., Kaiser, N.M., Sundmacher, K.: Integrated reaction–extraction process
for the hydroformylation of long-chain alkenes with a homogeneous catalyst. Com-
puters & Chemical Engineering 105, 212–223 (2017). DOI 10.1016/j.compchemeng.
2016.11.019

[43] McBride, K., Sundmacher, K.: Overview of surrogate modeling in chemical process
engineering. Chemie Ingenieur Technik 91(3), 228–239 (2019). DOI 10.1002/cite.
201800091

[44] McCormick, G.P.: Computability of global solutions to factorable nonconvex pro-
grams: Part I — convex underestimating problems. Mathematical Programming
10(1), 147–175 (1976). DOI 10.1007/BF01580665

[45] Mehrian, M., Guyot, Y., Papantoniou, I., Olofsson, S., Sonnaert, M., Misener, R.,
Geris, L.: Maximizing neotissue growth kinetics in a perfusion bioreactor: An in
silico strategy using model reduction and bayesian optimization. Biotechnology and
bioengineering 115(3), 617–629 (2018). DOI 10.1002/bit.26500

[46] Menne, D., Kamp, J., Wong, J.E., Wessling, M.: Precise tuning of salt retention
of backwashable polyelectrolyte multilayer hollow ﬁber nanoﬁltration membranes.
Journal of membrane science 499, 396–405 (2016). DOI 10.1016/j.memsci.2015.10.
058

[47] Meyer, C.A., Floudas, C.A.: Convex envelopes for edge-concave functions. Mathe-
matical programming 103(2), 207–224 (2005). DOI 10.1007/s10107-005-0580-9

[48] Misener, R., Floudas, C.A.: ANTIGONE: Algorithms for continuous / integer global
optimization of nonlinear equations. Journal of Global Optimization 59(2), 503–526
(2014). DOI 10.1007/s10898-014-0166-2

[49] Mitsos, A., Chachuat, B., Barton, P.I.: McCormick-based relaxations of algorithms.
SIAM Journal on Optimization 20(2), 573–601 (2009). DOI 10.1137/080717341

25

[50] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 (2013)

[51] Mogk, G., Mrziglod, T., Schuppert, A.: Application of hybrid models in chemical
industry. In: Computer Aided Chemical Engineering, vol. 10, pp. 931–936. Elsevier
(2002). DOI 10.1016/S1570-7946(02)80183-3

[52] Najman, J., Bongartz, D., Mitsos, A.: Convex relaxations of componentwise convex
functions. Computers & Chemical Engineering 130, 106527 (2019). DOI 10.1016/j.
compchemeng.2019.106527

[53] Najman, J., Mitsos, A.: On tightness and anchoring of McCormick and other
Journal of Global Optimization pp. 1–27 (2017). DOI 10.1007/

relaxations.
s10898-017-0598-6

[54] Quirante, N., Javaloyes, J., Caballero, J.A.: Rigorous design of distillation columns
using surrogate models based on Kriging interpolation. AIChE Journal 61(7), 2169–
2187 (2015). DOI 10.1002/aic.14798

[55] Quirante, N., Javaloyes, J., Ruiz-Femenia, R., Caballero, J.A.: Optimization of
chemical processes using surrogate models based on a Kriging interpolation.
In:
Computer Aided Chemical Engineering, vol. 37, pp. 179–184. Elsevier (2015). DOI
10.1016/B978-0-444-63578-5.50025-6

[56] Rall, D., Menne, D., Schweidtmann, A.M., Kamp, J., von Kolzenberg, L., Mitsos, A.,
Wessling, M.: Rational design of ion separation membranes. Journal of Membrane
Science 569, 209–219 (2019). DOI 10.1016/j.memsci.2018.10.013

[57] Rasmussen, C.E.: Gaussian processes in machine learning. In: Advanced lectures on

machine learning, pp. 63–71. Springer (2004)

[58] Sacks, J., Welch, W.J., Mitchell, T.J., Wynn, H.P.: Design and analysis of computer
experiments. Statistical science pp. 409–423 (1989). DOI 10.1214/ss/1177012413

[59] Schweidtmann, A.M., Clayton, A.D., Holmes, N., Bradford, E., Bourne, R.A., Lap-
kin, A.A.: Machine learning meets continuous ﬂow chemistry: Automated optimiza-
tion towards the pareto front of multiple objectives. Chemical Engineering Journal
(2018). DOI 10.1016/j.cej.2018.07.031

[60] Schweidtmann, A.M., Mitsos, A.: Deterministic global optimization with artiﬁcial
neural networks embedded. Journal of Optimization Theory and Applications 180(3),
925–948 (2019). DOI 10.1007/s10957-018-1396-0

[61] Schweidtmann, A.M., Netze, L., Mitsos, A.: Melon: Machine learning models for
optimization. https://git.rwth-aachen.de/avt.svt/public/MeLOn/ (2020)

[62] Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: Taking the human
out of the loop: A review of bayesian optimization. Proceedings of the IEEE 104(1),
148–175 (2016). DOI 10.1109/JPROC.2015.2494218

26

[63] Smith, E.M., Pantelides, C.C.: Global optimisation of nonconvex MINLPs. Com-
puters & Chemical Engineering 21, S791–S796 (1997). DOI 10.1016/S0098-1354(97)
87599-0

[64] Snoek, J., Larochelle, H., Adams, R.P.: Practical bayesian optimization of machine
In: Advances in neural information processing systems, pp.

learning algorithms.
2951–2959 (2012)

[65] Srinivas, N., Krause, A., Kakade, S.M., Seeger, M.: Gaussian process optimiza-
arXiv preprint

tion in the bandit setting: No regret and experimental design.
arXiv:0912.3995 (2009)

[66] Stuber, M.D., Scott, J.K., Barton, P.I.: Convex and concave relaxations of implicit
functions. Optimization Methods and Software 30(3), 424–460 (2015). DOI 10.1080/
10556788.2014.924514

[67] Sundararajan, S., Keerthi, S.S.: Predictive approaches for choosing hyperparameters
in Gaussian processes. In: Advances in neural information processing systems, pp.
631–637 (2000)

[68] Tawarmalani, M., Sahinidis, N.V.: A polyhedral branch-and-cut approach to global
optimization. Mathematical Programming 103(2), 225–249 (2005). DOI 10.1007/
s10107-005-0581-8

[69] Tawarmalani, M., Sahinidis, N.V., Pardalos, P.: Convexiﬁcation and Global Opti-
mization in Continuous and Mixed-Integer Nonlinear Programming: Theory, Algo-
rithms, Software, and Applications, Nonconvex Optimization and Its Applications,
vol. 65. Springer, Boston, MA (2002). DOI 10.1007/978-1-4757-3532-1

[70] Tsoukalas, A., Mitsos, A.: Multivariate McCormick relaxations. Journal of Global

Optimization 59(2-3), 633–662 (2014). DOI 10.1007/s10898-014-0176-0

[71] Ulmasov, D., Baroukh, C., Chachuat, B., Deisenroth, M.P., Misener, R.: Bayesian
optimization with dimension scheduling: Application to biological systems. In: Com-
puter Aided Chemical Engineering, vol. 38, pp. 1051–1056. Elsevier (2016). DOI
10.1016/B978-0-444-63428-3.50180-6

[72] Von Stosch, M., Oliveira, R., Peres, J., de Azevedo, S.F.: Hybrid semi-parametric
modeling in process systems engineering: Past, present and future. Computers &
Chemical Engineering 60, 86–101 (2014). DOI 10.1016/j.compchemeng.2013.08.008

[73] W¨achter, A., Biegler, L.T.: On the implementation of an interior-point ﬁlter line-
search algorithm for large-scale nonlinear programming. Mathematical programming
106(1), 25–57 (2006)

[74] Wang, J., Hertzmann, A., Fleet, D.J.: Gaussian process dynamical models.
Advances in neural information processing systems, pp. 1441–1448 (2006)

In:

27

[75] Wechsung, A., Scott, J.K., Watson, H.A.J., Barton, P.I.: Reverse propagation of
McCormick relaxations. Journal of Global Optimization 63(1), 1–36 (2015). DOI
10.1007/s10898-015-0303-6

[76] Wilson, J., Hutter, F., Deisenroth, M.: Maximizing acquisition functions for bayesian
optimization. In: Advances in Neural Information Processing Systems, pp. 9884–9895
(2018)

28

