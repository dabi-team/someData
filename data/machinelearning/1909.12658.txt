Quantum Algorithm for Finding the Optimal Variable Ordering
for Binary Decision Diagrams

Seiichiro Tani
NTT Communication Science Laboratories, NTT Corporation.
seiichiro.tani.cs@hco.ntt.co.jp

Abstract

An ordered binary decision diagram (OBDD) is a directed acyclic graph that represents a Boolean
function. OBDDs are also known as special cases of oblivious read-once branching programs in the
ï¬eld of complexity theory. Since OBDDs have many nice properties as data structures, they have
been extensively studied for decades in both theoretical and practical ï¬elds, such as VLSI design,
formal veriï¬cation, machine learning, and combinatorial problems. Arguably, the most crucial prob-
lem in using OBDDs is that they may vary exponentially in size depending on their variable ordering
(i.e., the order in which the variables are to read) when they represent the same function. Indeed,
it is NP hard to ï¬nd an optimal variable ordering that minimizes an OBDD for a given function.
Hence, numerous studies have sought heuristics to ï¬nd an optimal variable ordering. From practi-
cal as well as theoretical points of view, it is also important to seek algorithms that output optimal
solutions with lower (exponential) time complexity than trivial brute-force algorithms do. Friedman
and Supowit provided a clever deterministic algorithm with time/space complexity Oâˆ—(3n), where n
is the number of variables of the function, which is much better than the trivial brute-force bound
Oâˆ—(n!2n). This paper shows that a further speedup is possible with quantum computers by demon-
strating the existence of a quantum algorithm that produces a minimum OBDD together with the
corresponding variable ordering in Oâˆ—(2.77286n) time and space with an exponentially small error.
Moreover, this algorithm can be adapted to constructing other minimum decision diagrams such as
zero-suppressed BDDs, which provide compact representations of sparse sets and are often used in
the ï¬eld of discrete optimization and enumeration.

0
2
0
2

b
e
F
7
1

]
h
p
-
t
n
a
u
q
[

2
v
8
5
6
2
1
.
9
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
1 Introduction

1.1 Background

Ordered binary decision diagrams. The ordered binary decision diagram (OBDD) [Lee59, Ake78]
is one of the data structures that have been most often used for decades to represent Boolean functions in
practical situations, such as VLSI design, formal veriï¬cation, optimization of combinatorial problems,
and machine learning, and it has been extensively studied from both theoretical and practical standpoints
(see standard textbooks and surveys, e.g., Refs. [Bry92, MT98, DB98, Weg00, Knu09, Bry18]). More-
over, many variants of OBDDs have been invented to more efï¬ciently represent data with properties
observed frequently in speciï¬c applications (e.g., Refs. [Min93, BC95, BFG+97, CMZ+97, Min11]).
More technically speaking, OBDDs are directed acyclic graphs that represent Boolean functions and
also known as special cases of oblivious read-once branching programs in the ï¬eld of complexity the-
ory. The reason for OBDDsâ€™ popularity lies in their nice properties â€” they can be uniquely determined
up to the isomorphism for each function once variable ordering (i.e., the order in which to read the vari-
ables) is ï¬xed and, thanks to this property, the equivalence of functions can be checked by just testing
the isomorphism between the OBDDs representing the functions. In addition, binary operations such
as AND and OR between two functions can be performed efï¬ciently over the OBDDs representing
those functions [Bry86]. Since these properties are essential in many applications, OBDDs have gath-
ered much attention from various research ï¬elds. To enjoy these nice properties, however, we actually
need to address a crucial problem, which is that OBDDs may vary exponentially in size depending on
their variable ordering. For instance, a Boolean function f (x1, . . . , x2n) = x1x2 + x3x4 + Â· Â· Â· + x2nâˆ’1x2n
has the a (2n + 2)-sized OBDD for the ordering (x1, . . . , x2n) and a 2n+1-sized OBDD for the ordering
(x1, x3, . . . , x2nâˆ’1, x2, x4, . . . , x2n) [MT98, Sec. 8.1] (see Fig. 1 for the case where n = 6). This is not a
rare phenomenon; it could happen in many concrete functions that one encounters. Thus, since the early
stages of OBDD research, one of the most central problems has been how to ï¬nd an optimal variable
ordering, i.e., one that minimizes OBDDs. Since there are n! permutations over n variables, the brute-
force search requires at least n! = 2â„¦(n log n) time to ï¬nd an optimal variable ordering. Indeed, ï¬nding an
optimal variable ordering for a given function is an NP hard problem.

To tackle this high complexity, many heuristics have been proposed to ï¬nd an optimal variable
ordering or a relatively good one. These heuristics work well for Boolean functions appearing in speciï¬c
applications since they are based on very insightful observations, but they do not guarantee a worst-case
time complexity lower than that achievable with the brute-force search. The only algorithm with a much
lower worst-case time complexity bound, Oâˆ—(3n) time,1 than the brute-force bound Oâˆ—(n!2n) for all
Boolean functions with n variables was provided by Friedman and Supowit [FS90], and that was almost
thirty years ago!

In practice, it is often too costly to construct a minimum OBDD and the optimal variable order-
ing may change as the function changes during a procedure, say, by imposing additional constraints.
Nevertheless, theoretically sound methods for ï¬nding an optimal variable ordering are worth studying
for several reasons, such as to judge the optimization quality of heuristics and to be able to apply such
methods at least to parts of the OBDDs within a heuristics procedure [MT98, Sec. 9.22].

Quantum Speedups of Dynamic Programming Groverâ€™s quantum search algorithm [Gro96] and its
variants achieve quadratic speedups over any classical algorithm for the very fundamental problem of
exhaustive search. Thus, one of the merits of the quantum search is its wide applicability. However, it
does not immediately mean quantum speedups for all problems to which the quantum search is appli-
cable, since there may exist better classical algorithms than simple exhaustive search. Indeed, quantum
search for an optimal variable ordering of the OBDD from among n! candidates takes approximately
âˆš
2 n log n time, while the best classical algorithm takes only Oâˆ—(3n) = Oâˆ—(2(log2 3)n). These clas-
sical algorithms often employ powerful algorithmic techniques such as dynamic programming, divide-

n! â‰ˆ 2

1

1Oâˆ—(Â·) hides a polynomial factor. This hidden polynomial factor can be improved [BW96, Knu09].

1

and-conquer, and branch-and-bound. One typical strategy to gain quantum speedups would be to ï¬nd
exhaustive search (often implicitly) performed within such classical algorithms and apply the quantum
search to that part. For instance, DÂ¨urr et al. [DHHM06] provided quantum algorithms for some graph
problems, among which the quantum algorithm for the single-source shortest path problem achieves a
quantum speedup by applying a variant of Groverâ€™s search algorithm to select the cheapest border edge
in Dijkstraâ€™s algorithm. However, applying the quantum search in this way does not work when the
number of states in a dynamic programming algorithm is much larger than the number of predecessors
of each state. For instance, the Traveling Salesman Problem (TSP) can be solved in Oâˆ—(2n) time by a
classical dynamic programming algorithm, but locally applying the quantum search can attain at most
a polynomial-factor improvement. Recently, Ambainis et al. [ABI+19] has introduced break-through
techniques to speed up dynamic programming approaches. They provide quantum algorithms that solve
a variety of vertex ordering problems on graphs in Oâˆ—(1.817n) time, graph bandwidth in Oâˆ—(2.946n) time,
and TSP and minimum set cover in Oâˆ—(1, 728n) time, where n is the number of vertices in the graphs.

1.2 Our Results

In this paper, we show that quantum speedup is possible for the problem of ï¬nding an optimal variable
ordering of the OBDD for a given function. This is the ï¬rst quantum speedup for the OBDD-related
problems. Our algorithms assume the quantum random access memory (QRAM) model [GLM08],
which is commonly used in the literature concerned with quantum algorithms. In the model, one can
read contents from or write them into quantum memory in a superposition.

We provide our main result in the following theorem.

Theorem 1 (Informal) There exists a quantum algorithm that, for a function f : {0, 1}n â†’ {0, 1} given
as its truth table, produces a minimum OBDD representing f together with the corresponding variable
ordering in Oâˆ—(Î³n) time and space with an exponentially small error with respect to n, where the constant
Î³ is at most 2.77286. Moreover, the OBDD produced by our algorithm is always a valid one for f ,
although it is not minimum with an exponentially small probability.

This improves upon the classical best bound Oâˆ—(3n) [FS90] on time/space complexity. The classical
algorithm achieving this bound is a deterministic one. However, there are no randomized algorithms that
compute an optimal variable ordering in asymptotically less time complexity as far as we know.

It may seem somewhat restricted to assume that the function f is given as its truth table, since
there are other common representations of Boolean functions such as DNFs, CNFs, Boolean circuits
and OBDDs. However, this is not the case. Our algorithm actually works in more general settings
where the input function f is given as any representation such that the value of f on any speciï¬ed
assignment can be computed over the representation in polynomial time in n, such as polynomial-size
DNFs/CNFs/circuits and OBDDs of any size. This is because, in such cases, the truth table of f can be
prepared in Oâˆ—(2n) time and the minimum OBDD is computable from that truth table with our algorithm.
We restate Theorem 1 in a more general form as follows.

Corollary 2 Let R( f ) be any representation of a Boolean function f with n variables such that the value
of f (x) on any given assignment x âˆˆ {0, 1}n can be computed on R( f ) in polynomial time with respect to
n. Then, there exists a quantum algorithm that, for a function f : {0, 1}n â†’ {0, 1} given as R( f ), produces
a minimum OBDD representing f together with the corresponding variable ordering in Oâˆ—(Î³n) time and
space with an exponentially small error with respect to n, where the constant Î³ is at most 2.77286.
Possible representations as R( f ) are polynomial-size DNFs/CNFs/circuits and OBDDs of any size for
function f .

There are many variants of OBDDs, among which the zero-suppressed BDDs (ZDDs or ZBDDs)
introduced by Minato [Min93] have been shown to be very powerful in dealing with combinatorial
problems (see Knuthâ€™s famous textbook [Knu09] for how to apply ZDDs to such problems). With
just two-line modiï¬cations, our algorithm can construct a minimum ZDD with the same time/space

2

Figure 1: The OBDDs represent the function f (x1, x2, x3, x4, x5, x6) = x1x2 + x3x4 + x5x6 under two
variable orderings: (x1, x2, x3, x4, x5, x6) (left) and (x1, x3, x5, x2, x4, x6) (right), where the solid and dot-
ted arcs express 1-edges and 0-edges, respectively, and the terminal nodes for true and false are labeled
In general, the function f (x1, . . . , x2n) = x1x2 + x3x4 + Â· Â· Â· + x2nâˆ’1x2n
with T and F, respectively.
has the a (2n + 2)-sized OBDD for the ordering (x1, . . . , x2n) and a 2n+1-sized OBDD for the ordering
(x1, x3, . . . , x2nâˆ’1, x2, x4, . . . , x2n). As deï¬ned in Sec. 2.2 , for each OBDD, {V0, . . . , V6} is the partition
of the node set, the top node is identiï¬ed with r, and the bottom two nodes are identiï¬ed with t and f. A
more detailed explanation of these OBDDs is provided in Example 1 in Sec. 2.2 .

complexity. We believe that similar speedups are possible for many other variants of OBDDs (adapting
our algorithm to multiterminal BDDs (MTBDDs) [BFG+97, CMZ+97] is almost trivial).

1.3 Technical Outline

The ï¬rst step to take is to somehow adapt the dynamic programming approach of the classical algo-
rithm [FS90] (called, FS) to the framework provided by Ambainis et al. [ABI+19]. Consider a Boolean
function f over n variables: x1, . . . , xn. Intuitively, FS determines the variable ordering of the minimum
OBDD for f by performing dynamic programming from the variable to be read last toward that to be
read ï¬rst. More concretely, let (xÏ€[1], . . . , xÏ€[n]) be the variable ordering from the one read last (xÏ€[1])
to the one read ï¬rst (xÏ€[n]), where Ï€ = (Ï€[1], . . . , Ï€[n]) is a permutation over [n] := {1, . . . , n}. For
k = 1, . . . , n in this order, and for every subset K âŠ† [n] of cardinality k, the algorithm FS computes
a lower bound on OBDD size when {Ï€[1], . . . , Ï€[k]} = K from the lower bounds on OBDD size when
{Ï€[1], . . . , Ï€[k âˆ’ 1]} = K \ {h} for all h âˆˆ K. Thus, by thinking of each node z âˆˆ {0, 1}n of weight k in
a Boolean hypercube as the characteristic vector of K, the algorithm FS can be seen as solving a kind
of shortest path problem on a Boolean hypercube. Hence, Ambainis et al.â€™s framework seems appli-
cable. Their results depend on the property that a large problem can be divided into the same kind of
subproblems or, in other words, symmetric subproblems in the sense that they can be solved with the
same algorithm. This property naturally holds in many graph problems. In our case, ï¬rstly, it is unclear
whether the problem can be divided into subproblems. Secondly, subproblems would be to optimize
the ordering of variable starting from the middle variable or even from the opposite end, i.e., from the
variable to be read ï¬rst, toward the one to be read last. Such subproblems cannot be solved with the
algorithm FS, and, in particular, optimizing in the latter case essentially requires the equivalence check
of subfunctions of f , which is very costly.

Our technical contribution is to ï¬nd, by carefully observing the unique properties of OBDDs, that
it is actually possible to even recursively divide the original problem into asymmetric subproblems, to
generalize the algorithm FS so that it can solve the subproblems, and to use the quantum minimum

3

ğ‘¥"ğ‘¥#ğ‘¥$ğ‘¥%ğ‘¥&ğ‘¥â€™TFğ‘¥"ğ‘¥$ğ‘¥$ğ‘¥&ğ‘¥&ğ‘¥&ğ‘¥&ğ‘¥#ğ‘¥#ğ‘¥#ğ‘¥#ğ‘¥%ğ‘¥%ğ‘¥â€™TFğ‘‰)ğ‘‰â€™ğ‘‰&ğ‘‰%ğ‘‰$ğ‘‰#ğ‘‰"ï¬nding algorithm in order to efï¬ciently select the subproblems that essentially contribute to the optimal
variable ordering.

More concretely, we show that, for any k âˆˆ [n], it is possible to divide the problem into two collec-

tions of subproblems as follows: for all K âŠ† [n] of cardinality k,

â€¢ problems of ï¬nding the ordering (Ï€[1], . . . , Ï€[k]) that minimizes the size of the bottom k-layers of

the corresponding OBDD, assuming that the set {Ï€[1], . . . , Ï€[k]} equals K,

â€¢ problems of ï¬nding the ordering (Ï€[k + 1], . . . , Ï€[n]) that minimizes the size of the upper (n âˆ’ k)-
layers of the corresponding OBDD, assuming that the set {Ï€[k + 1], . . . , Ï€[n]} equals [n] \ K.

Then, taking the minimum of the OBDD size over all K and k with the quantum minimum ï¬nding pro-
vides a minimum OBDD and the corresponding variable ordering. To obtain a better bound, a straight-
forward strategy is to consider m division points (0 < k1 < Â· Â· Â· < km < n) and optimize each of the
(m + 1) suborderings (Ï€[1], . . . , Ï€[k1]), (Ï€[k1 + 1], . . . , Ï€[k2]), . . . , (Ï€[km + 1], . . . , Ï€[n]). However, this
makes subproblems even more asymmetric. To deal with this asymmetry, we generalize the algorithm
FS so that it can cover all the subproblems. Then, by applying it to each subproblem, we optimize
the suborderings with the quantum minimum ï¬nding so that the OBDD size is minimized. To improve
the complexity bound further, a simple idea would be to replace the generalized FS with the quantum
algorithm we have just obtained. However, the latter algorithm works only for the original problem.
Thus, we generalize the quantum algorithm so that it can be applied to the asymmetric subproblems. By
repeating this composition and generalization, we obtain the ï¬nal algorithm.

1.4 Related Work

The studies related to minimizing OBDDs are so numerous that we cannot cover all of them. We thus
pick up some of purely theoretical work.

Meinel and SlobodovÂ´a [MS94] proved that it is NP hard to construct an optimal OBDD for a Boolean
function given by a logical circuit, a DNF, a CNF, or an OBDD, even if the optimal OBDD is of constant
size. Tani, Hamaguchi and Yajima [THY96] proved that it is NP hard to improve the variable ordering
(and thus, to ï¬nd an optimal variable ordering) for a given multi-rooted OBDD, where the NP hardness
is proved by a reduction from Optimal Linear Arrangement [GJ79]. Bollig and Wegener [BW96] ï¬nally
proved the NP hardness for a given single-rooted OBDD. This is still true if the input function is re-
stricted to monotone functions [INY98]. Minimizing the width of an OBDD is also NP hard [Bol16].
As for approximation hardness, Sieling [Sie02a, Sie02b] proved that if there exists a polynomial-time
approximation scheme for computing the size of the minimum OBDD for a given OBDD, it then holds
that NP = P.

It would be nice if, for every function, there exists at least one variable ordering under which the
OBDD for the function is of a size bounded by a polynomial. As one may expect, this is not the case.
It can be proved by a counting argument that there exists a function for which the OBDD size grows
exponentially in the number of variables under any variable ordering [Lee59, HC92, HM94]. Examples
of such functions include practical ones such as the multiplication function [Bry91], a threshold func-
tion [HTKY97], and the division function [HY97]. The sizes of OBDDs for several classes of Boolean
functions are investigated [STY94, Hea93, HM00]. The OBDD size is also studied from the viewpoint
of computational learning and knowledge-bases [TY00, HI02].

In applying OBDDs to graph problems, it is possible to ï¬nd variable orderings for which OBDD size
is nontrivially upper-bounded in terms of certain measures characterizing graph structures [TI94, SIT95].
A similar concept was discoved for ZDDs [Min93] by Knuth [Knu09]. This concept is now called the
frontier method, and lots of work is based on it.

4

2 Preliminaries

2.1 Basic Terminology

Let N, Z and R be the sets of natural numbers, integers, and real numbers, respectively. For each n âˆˆ N,
let [n] be the set {1, . . . , n}, and Sn be the permutation group over [n]. We may denote a singleton set {k}
by k for notational simplicity if it is clear from the context; for instance, I \ {k} may be denoted by I \ k,
if we know I is a set. For any subset I âŠ† [n], let Î n(I) be the set of Ï€ âˆˆ Sn such that the ï¬rst |I| members
{Ï€[1], . . . , Ï€[|I|]} constitutes I, i.e.,

Î n(I) := {Ï€ âˆˆ Sn : {Ï€[1], . . . , Ï€[|I|]} = I} âŠ† Sn.

For simplicity, we omit the subscript n and write Î (I). More generally, for any two disjoint subsets
I, J âŠ† [n], let

Î n((cid:104)I, J(cid:105)) := {Ï€ âˆˆ Sn : {Ï€[1], . . . , Ï€[|I|]} = I, {Ï€[|I| + 1], . . . , Ï€[|I| + |J|]} = J} âŠ† Sn.

For any disjoint subsets I1, . . . , Im âŠ† [n] for m âˆˆ [n], Î n((cid:104)I1, . . . , Im(cid:105)) is deï¬ned similarly. For simplicity,
we may denote (cid:104)I(cid:105) by I, if it is clear from the context.

We denote the union operation over disjoint sets by (cid:116) (instead of âˆª) when we emphasize the disjoint-
ness of the sets. For example, the union of the disjoint sets {1, 2} and {5, 6} is denoted by {1, 2} (cid:116) {5, 6}.
For n Boolean variables x1, . . . , xn, any set I âŠ† [n], and any vector b = (b1, . . . , b|I|) âˆˆ {0, 1}|I|, xI
denotes the ordered set (x j1, . . . , x j|I|), where { j1, . . . , j|I|} = I and j1 < Â· Â· Â· < j|I|, and xI = b denotes
= bi for each i = [|I|]. For any Boolean function f : {0, 1}n â†’ {0, 1} with variables x1, . . . , xn, we
x ji
denote by f |xI =b the function obtained by restricting f with xI = b. If I is a singleton set, say, I = {i}, we
may write xi and f |xi=b to mean x{i} and f |x{i}=b, respectively, for notational simplicity. We say that g is a
subfunction of f if g is equivalent to the function f |xI =b for some I âŠ† [n] and b âˆˆ {0, 1}|I|.

For any function g(n) in n, we use the notation Oâˆ—(g(n)) to hide a polynomial factor in n. For

instance, n32n and 2n are both in Oâˆ—(2n). We further denote X = Oâˆ—(Y) by X (cid:47) Y.

We use the following upper bound many times in this paper. For n âˆˆ N and k âˆˆ [n] âˆª {0}, it holds that
â‰¤ 2n H(k/n), where H(Â·) represents the binary entropy function H(Î´) := âˆ’Î´ log2 Î´ âˆ’ (1 âˆ’ Î´) log2(1 âˆ’ Î´).

(cid:17)
(cid:16)n
k

2.2 Ordered Binary Decision Diagrams

This subsection brieï¬‚y introduces ordered binary decision diagrams (OBDDs). For interested readers,
see standard textbooks and survey papers (e.g., Refs. [Bry92, MT98, DB98, Weg00, Bry18]).

OBDDs are a special case of read-once oblivious branching programs in complexity-theoretic terms
that is, branching programs that satisfy the following conditions: each variable is read at most once on
each directed path from the root to a terminal node, and the orderings of variables to be read on all such
paths are consistent with a certain ï¬xed ordering.

In the following, we formally deï¬ne OBDDs in graph-theoretic terms and provide several notations.
For any Boolean function f : {0, 1}n â†’ {0, 1} over variables x1, . . . , xn and any permutation Ï€ âˆˆ Sn
(called a variable ordering), an OBDD B( f, Ï€) is a directed acyclic graph G(V, E) deï¬ned as follows:

1. The node set V is the union of two disjoint sets N and T of non-terminal nodes with out-degree
two and terminal nodes with out-degree zero, respectively, where T contains exactly two nodes:
T = {f, t}. The set N contains a unique source node r, called the root.

2. B( f, Ï€) is a leveled graph with n + 1 levels. Namely, the node set can be partitioned into n subsets:
V := V0 (cid:116) V1 (cid:116) Â· Â· Â· (cid:116) Vn, where Vn = {r} and V0 = T = {t, f}, such that each directed edge (u, v) âˆˆ E
is in Vi Ã— V j for a pair (i, j) âˆˆ [n] Ã— ({0} (cid:116) [n âˆ’ 1]) with i > j. For each i âˆˆ [n], subset Vi (called the
level i) is associated with the variable xÏ€[i], or alternatively, each node in Vi is labeled with xÏ€[i].2
For convenience, we deï¬ne a map var : N â†’ [n] such that if v âˆˆ Vi then var = Ï€[i].

2In the standard deï¬nition, Vi is associated with the variable xÏ€[nâˆ’i]. Our deï¬nition follows the one given in [FS90] to avoid

complicated subscripts of variables.

5

Figure 2: Examples of a redundant node (left) and equivalent nodes (right), and their removal rules,
where the solid and dotted arcs express 1-edges and 0-edges, respectively,

3. The two edges emanating from every non-terminal node v are called the 0-edge and the 1-edge,
which are labeled with 0 and 1, respectively. For every u âˆˆ N, let u0 and u1 be the destinations of
the 0-edge and 1-edge of u, respectively.

4. Let F ( f ) be the set of all subfunctions of f . Deï¬ne a bijective map F : V â†’ F ( f ) as follows:
(a) F(r) = f for r âˆˆ Vn; (b) F(t) = true and F(f) = false for t, f âˆˆ V0; (c) For every u âˆˆ N
and b âˆˆ {0, 1}, F(ub) is the subfunction obtained from F(u) by substituting xvar(u) with b, i.e.,
F(ub) = F(u)|xvar(u)=b.

5. B( f, Ï€) must be minimal in the sense that the following reduction rules cannot be applied. In other

words, B( f, Ï€) is obtained by maximally applying the following rules (Fig. 2):

(a) if there exists a redundant node u âˆˆ N, then remove u and its outgoing edges, and redirect
all the incoming edges of u to u0, where a node u is redundant if u0 is the same node as u1.
(b) if there exist equivalent nodes {u, v} âŠ‚ N, then remove v (i.e., any one of them) and its
outgoing edges, and redirect all incoming edges of v to u, where u and v are equivalent if (1)
var(u) is equal to var(v), and (2) u0 and u1 are the same nodes as v0 and v1, respectively.

Example 1 For ease of understanding the above notations, let us consider the OBDD on the right side
in Fig. 1. The root r is the uppermost node labeled with x6. The ordering Ï€ is (1, 3, 5, 2, 4, 6). Every node
in Vi (i = 1, . . . , 6) is represented by a circle labeled with xÏ€[i]. For instance, V3 consists of all the four
nodes labeled with x5. For each node v âˆˆ V3, it holds that var(v) = 5. Let u be the left node labeled with
x3. Since the path from r to u consists of three edges labeled with 0, 1, and 0 in this order from the root
side, F(u) is represented as F(r)|x6=0,x4=1,x2=0 = f |x6=0,x4=1,x2=0 = x3.

For each j âˆˆ [n], Cost j( f, Ï€) denotes the width at the level associated with the variable x j, namely,
the number of nodes in the level Ï€âˆ’1[ j] (Fig. 3). For I âŠ† [n], let Ï€I be a permutation Ï€ in Î (I) that
minimizes the number of nodes in level 1 to level |I|:

Ï€I := arg min

ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³

|I|(cid:88)

j=1

CostÏ€[ j]( f, Ï€) : Ï€ âˆˆ Î (I)

ï£¼
ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£¾

.

(1)

j=1 CostÏ€[ j]( f, Ï€) = (cid:80)

Note that (cid:80)|I|
iâˆˆI Costi( f, Ï€) for Ï€ âˆˆ Î (I). More generally, for disjoint subsets
I1, . . . , Im âŠ† [n], Ï€(cid:104)I1,...,Im(cid:105) is a permutation in Î ((cid:104)I1, . . . , Im(cid:105)) that minimizes the number of the nodes in
level 1 to level |I1| + Â· Â· Â· + |Im| over all Ï€ âˆˆ Î ((cid:104)I1, . . . , Im(cid:105)):

Ï€(cid:104)I1,...,Im(cid:105) := arg min

ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³

|I1|+Â·Â·Â·+|Im|(cid:88)

j=1

CostÏ€[ j]( f, Ï€) : Ï€ âˆˆ Î ((cid:104)I1, . . . , Im(cid:105))

ï£¼
ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£¾

.

(2)

6

ğ‘¥"ğ‘¥"#$removeğ‘¥"#$ğ‘¥"ğ‘¥"ğ‘¥"#$ğ‘¥"#$removeğ‘¥"ğ‘¥"#$ğ‘¥"#$Redundant nodeEquivalent nodesFigure 3: Schematic expression of Cost j( f, Ï€)

j=1

CostÏ€[ j]( f, Ï€) = (cid:80)

Note that min (cid:80)|I1|+Â·Â·Â·+|Im|
iâˆˆI1(cid:116)Â·Â·Â·(cid:116)Im Costi( f, Ï€) for any Ï€ âˆˆ Î ((cid:104)I1, . . . , Im(cid:105)). The follow-
ing well-known lemma captures the essential property of OBDDs. It states that the number of nodes at
level i âˆˆ [n] is constant over all Ï€, provided that the two sets {Ï€[1], . . . , Ï€[i âˆ’ 1]} and {Ï€[i + 1], . . . , Ï€[n]}
are ï¬xed.

Lemma 3 ([FS90]) For any non-empty subset I âŠ† [n] and any i âˆˆ I, there exists a constant c f such that,
for each Ï€ âˆˆ Î ((cid:104)I \ {i}, {i}(cid:105)), CostÏ€[|I|]( f, Ï€) â‰¡ Costi( f, Ï€) = c f .

For convenience, we deï¬ne shorthand for the minimums of the sums in Eqs. (1) and (2). For I(cid:48) âŠ†
I âŠ† [n], MINCOSTI[I(cid:48)] is deï¬ned as the number of nodes in the levels associated with variables indexed
by elements in I(cid:48) under permutation Ï€I, namely, MINCOSTI[I(cid:48)] := (cid:80)
iâˆˆI(cid:48) Costi( f, Ï€I). More generally, for
disjoint subsets I1, . . . , Im âŠ† [n] and I(cid:48) âŠ† I1 (cid:116) Â· Â· Â· (cid:116) Im,

MINCOST(cid:104)I1,...,Im(cid:105)[I(cid:48)] :=

(cid:88)

iâˆˆI(cid:48)

Costi( f, Ï€(cid:104)I1,...,Im(cid:105)).

As a special case, we denote MINCOST(cid:104)I1,...,Im(cid:105)[I1 (cid:116) Â· Â· Â· (cid:116) Im] by MINCOST(cid:104)I1,...,Im(cid:105). We deï¬ne MINCOSTâˆ…
as 0.

2.3 The Algorithm by Friedman and Supowit

This subsection reviews the algorithm by Friedman and Supowit [FS90]. We will generalize their idea
later and heavily use the generalized form in our quantum algorithm. Hereafter, we call their algorithm
FS.

2.3.1 Key Lemma and Data Structures

The following lemma is the basis of the dynamic programming approach used in the algorithm.

Lemma 4 For any non-empty subset I âŠ† [n] and any Boolean function f : {0, 1}n â†’ {0, 1}, the following
holds: MINCOSTI = minkâˆˆI

MINCOSTI\k + Costk( f, Ï€(cid:104)I\k,k(cid:105))(cid:1) = minkâˆˆI

MINCOST(cid:104)I\k,k(cid:105)

(cid:1) .

(cid:0)

(cid:0)

The proof is given in Appendix A.

Before sketching algorithm FS, we provide several deï¬nitions. For any I âŠ† [n], TABLEI is an array
Intuitively, for b âˆˆ {0, 1}nâˆ’|I|, the cell
with 2nâˆ’|I| cells each of which stores a non-negative integer.
TABLEI[b] stores (the pointer to) the unique node of B( f, Ï€I) associated via F with function f |x[n]\I =b.
Hence, we may write TABLEI[x[n]\I = b] instead of TABLEI[b] to clearly indicate the value assigned to
each variable x j for j âˆˆ [n]\I. The purpose of TABLEI is to relate all subfunctions f |x[n]\I =b (b âˆˆ {0, 1}nâˆ’|I|)

7

aTFğ‘¥"ğ‘¥"ğ‘¥"ğ‘¥"ğ‘“ğ‘‰%ğœ‹ğ‘˜=ğ‘—=Cost.%ğ‘“,ğœ‹Cost"ğ‘“,ğœ‹Figure 4: Schematic expression of Lemma 3: For any two permutations Ï€, Ï€(cid:48) âˆˆ Sn such that
{Ï€[1], . . . , Ï€[|I| âˆ’ 1]} = {Ï€(cid:48)[1], . . . , Ï€(cid:48)[|I| âˆ’ 1]} and Ï€[|I|] = Ï€(cid:48)[|I|], it holds that the number of nodes
labeled with xi is equal to that of nodes labeled with xi, where it is assumed that Ï€[|I|] = Ï€(cid:48)[|I|].

to the corresponding nodes of B( f, Ï€I). We assume without loss of generality that the pointers to nodes
of B( f, Ï€I) are non-negative integers and, in particular, those to the two terminal nodes corresponding to
false and true are the integers 0 and 1, respectively. Thus, TABLEâˆ… is merely the truth table of f .

Algorithm FS computes TABLEI together with Ï€I, MINCOSTI, and another data structure, NODEI
for all I âŠ† [n], starting from TABLEâˆ… via dynamic programming. NODEI is the set of all triples of (the
pointers to) nodes, (u, u0, u1) âˆˆ N Ã— (N (cid:116) T ) Ã— (N (cid:116) T ), in B( f, Ï€I), where var(u) = Ï€I[|I|], and (u, u0) and
(u, u1) are the 0-edge and 1-edge of u, respectively. Thus, NODEI contains the structure of the subgraph of
B( f, Ï€I) induced by V|I|. The purpose of the NODEI is to prevent the algorithm from duplicating existing
nodes, i.e., creating nodes associated with the same subfunctions as those with which the existing nodes
are associated. By the deï¬nition, NODEâˆ… is the empty set. We assume that NODEI is implemented with an
appropriate data structure, such as a balanced tree, so that the time complexity required for membership
testing and insertion is the order of logarithm in the number of triples stored in NODEI. An example of
TABLEI and NODEI is shown in Fig. 5.

More generally, for disjoint subset I1, . . . , Im âŠ† [n], TABLE(cid:104)I1,...,Im(cid:105) is an array with 2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im| cells
such that, for b âˆˆ {0, 1}nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im|, TABLE(cid:104)I1,...,Im(cid:105)[b] stores the nodes of B( f, Ï€(cid:104)I1,...,Im(cid:105)) associated with the
function f |x[n]\I1(cid:116)Â·Â·Â·(cid:116)Im
=b. NODE(cid:104)I1,...,Im(cid:105) is deï¬ned similarly for B( f, Ï€(cid:104)I1,...,Im(cid:105)). For simplicity, we hereafter
denote by F S((cid:104)I1, . . . , Im(cid:105)) the quadruplet (Ï€(cid:104)I1,...,Im(cid:105), MINCOST(cid:104)I1,...,Im(cid:105), TABLE(cid:104)I1,...,Im(cid:105), NODE(cid:104)I1,...,Im(cid:105)).

2.3.2 Sketch of Algorithm FS [FS90]

Algorithm FS performs the following operations for k = 1, . . . , n in this order. For each k-element
subset I âŠ† [n], compute F S((cid:104)I \ i, i(cid:105)) from F S((cid:104)I \ i(cid:105)) for each i âˆˆ I in the manner described later (note
that, since the cardinality of the set I \ i is k âˆ’ 1, F S((cid:104)I \ i(cid:105)) has already been computed). Then set
F S(I) â†âˆ’ F S((cid:104)I \ iâˆ—, iâˆ—(cid:105)), where iâˆ— is the index i âˆˆ I that minimizes MINCOST(cid:104)I\i,i(cid:105), implying that Ï€I is
Ï€(cid:104)I\iâˆ—,iâˆ—(cid:105). This is justiï¬ed by Lemma 4. A schematic view of the algorithm is shown in Fig. 6.

To compute F S((cid:104)I \ i, i(cid:105)) from F S((cid:104)I \ i(cid:105)), do the following. First set NODE(cid:104)I\i,i(cid:105) â† âˆ… and

MINCOST(cid:104)I\i,i(cid:105) â† MINCOSTI\i as their initial values. Then, for each b âˆˆ {0, 1}nâˆ’|I|, set

u0 â† TABLEI\i[x[n]\I = b, xi = 0],

u1 â† TABLEI\i[x[n]\I = b, xi = 1].

If u0 = u1, then store u0 in TABLE(cid:104)I\i,i(cid:105)[b]. Otherwise, test whether (u, u0, u1) for some u is a mem-
ber of NODEI\i. If it is, store u in the TABLE(cid:104)I\i,i(cid:105)[b]; otherwise create a new triple (u(cid:48), u0, u1), insert
it to NODE(cid:104)I\i,i(cid:105) and increment MINCOST(cid:104)I\i,i(cid:105). Since u(cid:48) is the pointer to the new node, u(cid:48) must be dif-
ferent from any pointer already included in NODE(cid:104)I\i,i(cid:105) and from any pointer to a node in V1 (cid:116) Â· Â· Â· (cid:116)

8

aTFğ‘¥"ğ‘¥"ğ‘¥"ğ‘¥"ğ‘“Cost"ğ‘“,ğœ‹=Cost"ğ‘“,ğœ‹+ğœ‹1ğœ‹ğ¼âˆ’1â‹®ğœ‹ğ¼ğœ‹ğ¼+1â‹®ğœ‹ğ‘›aTFğ‘¥"ğ‘¥"ğ‘¥"ğ‘¥"ğ‘“ğœ‹+1ğœ‹+ğ¼âˆ’1â‹®ğœ‹+ğ¼ğœ‹+ğ¼+1â‹®ğœ‹+ğ‘›ğ¼ğ¼ğ‘–==ğ‘–Figure 5: An example of data structure used in Algorithm FS. TABLEI and NODEI for the OBDD (rhs)
are shown, where the pointers (integers) to the nodes labeled with x1, x3, x5 are each shown at the top-left
positions of the nodes.

Vkâˆ’1 in B( f, Ï€(cid:104)I\i(cid:105)), where k = |I|. Such u(cid:48) can be easily chosen by setting u(cid:48) to two plus the value
of MINCOST(cid:104)I\i,i(cid:105) before the increment, since the MINCOST(cid:104)I\i,i(cid:105) is exactly the number of triples in
NODE(cid:104)I\i,i(cid:105) plus |V1 (cid:116) Â· Â· Â· (cid:116) Vkâˆ’1|, and the numbers 0 and 1 are reserved for the terminal nodes. We
call the above procedure table folding with respect to xi, because it halves the size of TABLE(cid:104)I\i(cid:105). We
also mean it by â€œfolding TABLE(cid:104)I\i(cid:105) with respect to xiâ€.

The complexity analysis is fairly simple. For each k, we need to compute F S(I) for

possible
Iâ€™s with |I| = k. For each I, it takes Oâˆ—(2nâˆ’k) time since the the size of TABLEI\i is 2nâˆ’k+1 and each
(cid:17) = 3n, up to a
operation to NODEI\i takes a polynomial time in n. Thus, the total time is (cid:80)n
polynomial factor. The point is that computing each F S(I) takes time linear to the size of TABLEI\i up
to a polynomial factor. The space required by Algorithm FS during the process for k is dominated by
that for TABLEI, TABLEI\i and NODEI for all I and i âˆˆ I, which is Oâˆ— (cid:16)
. The space complexity is
(cid:17)(cid:17) = Oâˆ—(3n).
thus Oâˆ— (cid:16)

maxkâˆˆ{0}âˆª[n] 2nâˆ’k(cid:16)n

k=0 2nâˆ’k(cid:16)n

(cid:17)(cid:17)
2nâˆ’k(cid:16)n

k

k

k

(cid:17)
(cid:16)n
k

Theorem 5 (Friedman and Supowit [FS90]) Suppose that the truth table of
given as input. Algorithm FS produces F S([n]) in Oâˆ—(3n) time and space.

f : {0, 1}n â†’ {0, 1} is

2.4 Quantum Computation

We assume that readers have a basic knowledge of quantum computing (e.g., Refs. [NC00, KSV02,
KLM07]). We provide only a lemma used to obtain our results.

The quantum search algorithm discovered by Grover [Gro96] has been generalized in many ways.
For instance, Buhrman et al. [BCdWZ99] provided a small error version of quantum search, while DÂ¨urr

9

[ğ‘›]âˆ’ğ¼ğ‘¥â€™ğ‘¥(ğ‘¥(ğ‘¥)ğ‘¥)ğ‘¥)ğ‘¥)ğ‘¥*ğ‘¥*ğ‘¥*ğ‘¥*ğ‘¥+ğ‘¥+ğ‘¥,TFğ¼013425678ğ‘¢ğ‘¢7ğ‘¢ğŸ531601721841NODE=ğ‘¥â€™ğ‘¥(ğ‘¥)pointer00000012010301141006101711051118Table=ğ¼={1,3,5}ğœ‹==(1,3,5,2,4,6)Figure 6: Schematic view of Friedman-Supowit Algorithm. The algorithm goes from the left to the
(cid:17)
(cid:16)n
dots, each of which corresponds to a subset I âŠ† [n]
right. On the vertical line indicated by k, there are
k
of size k (or, more strictly speaking, F S(I)). FS (I) is computed from the collection of F S((cid:104)I \ i(cid:105)) over
all i âˆˆ I, which are arranged as dots on the line indicated by k âˆ’ 1 and have already been computed.

and HÃ¸yer [DH96] devised a quantum algorithm that ï¬nds the minimum of a function. By combining
these two algorithms, we obtain the following lemma (an adaptation of Corollary 2.3 in Ref. [LGM18]).

Lemma 6 (Quantum Minimum Finding [DH96, BCdWZ99, LGM18]) For every Îµ > 0 there exists
a quantum algorithm that, for a function f : [N] â†’ Z given as an oracle, ï¬nd an element x âˆˆ [N] at
which f (x) achieves the minimum, with error probability at most Îµ by making O(
N log(1/Îµ)) queries.

(cid:112)

In this paper, the search space N is exponentially large in n and we are interested in exponential com-
plexities, ignoring polynomial factors in them. We can thus safely assume Îµ = 1/2p(n) for a polynomial
p(n), so that the overhead is polynomially bounded. Since our algorithms use Lemma 6 constant times,
their overall error probabilities are exponentially small. In the following proofs, we thus assume that Îµ
is exponentially small whenever we use Lemma 6, and do not explicitly analyze the error probability for
simplicity.

Our algorithms assume that the quantum random access memory (QRAM) model [GLM08], which
is commonly used in the literature when considering quantum algorithms. In the model, one can read
contents from or write them into quantum memory in a superposition.

3 Quantum Algorithm with Divide-and-Conquer

We generalize Lemma 4 and Theorem 5 and use them in our quantum algorithm.
Lemma 7 For any disjoint subsets I1, . . . , Im, J âŠ† [n] with J (cid:44) âˆ… and any Boolean function f : {0, 1}n â†’
{0, 1}, the following holds:

MINCOST(cid:104)I1,...,Im,J(cid:105) = min
kâˆˆJ
= min
kâˆˆJ

(cid:0)

(cid:0)

MINCOST(cid:104)I1,...,Im,J\{k}(cid:105) + Costk( f, Ï€(cid:104)I1,...,Im,J\{k},{k}(cid:105))(cid:1)
(cid:1) .

MINCOST(cid:104)I1,...,Im,J\{k},{k}(cid:105)

The proof of this lemma is very similar to that of Lemma 4 and deferred to Appendix A. Based on
Lemma 7, we generalize Theorem 5 to obtain algorithm FSâˆ— (its pseudo code is given in Appendix D).
Recall that F S((cid:104)I1, . . . , Im(cid:105)) denotes the quadruplet (Ï€(cid:104)I1,...,Im(cid:105), MINCOST(cid:104)I1,...,Im(cid:105), TABLE(cid:104)I1,...,Im(cid:105), NODE(cid:104)I1,...,Im(cid:105)).
A schematic view of FSâˆ— is shown in Fig. 7.

10

âˆ…"Each dot corresponds to a subset #âŠ‚"of size %%%âˆ’1(âˆ—=argmin1mincost6âŸ¨8âˆ–:,âŸ©:0">?=>âŸ¨?âˆ–1âˆ—,âŸ©1âˆ—Figure 7: Schematic view of FSâˆ—. This view corresponds to the case where m = 1 and J âŠ‚ [n] \ I in
Lemma 8. The shaded area is the one that FSâˆ— sweeps to produce F S((cid:104)I, J(cid:105)).

Lemma 8 (Classical Composition Lemma) For disjoint subsets I1, . . . , Im, J âŠ† [n] with J (cid:44) âˆ…, there
exists a deterministic algorithm FSâˆ— that produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)) for an un-
derlying function f : {0, 1}n â†’ {0, 1} in Oâˆ— (cid:16)
2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· 3|J|(cid:17)
time and space. More generally, for each
k âˆˆ [|J|], the algorithm produces the set {F S((cid:104)I1, . . . , Im, K(cid:105)) : K âŠ† J, |K| = k} from F S((cid:104)I1, . . . , Im(cid:105)) in
Oâˆ— (cid:16)

2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| (cid:80)k

time and space.

(cid:17)(cid:17)
j=0 2|J|âˆ’ j(cid:16)|J|

j

Note that if I1 (cid:116) Â· Â· Â· (cid:116) Im = âˆ… and J = [n], then we obtain Theorem 5.
Proof. We focuses on the simplest case of m = 1, for which our goal is to show an algorithm that
produces F S((cid:104)I, J(cid:105)) from F S(I). It is straightforward to generalize the proof to the case of m â‰¥ 2.
Starting from F S(I), the algorithm ï¬rst folds TABLEI with respect to each variable in {x j : j âˆˆ J} to
obtain F S((cid:104)I, j(cid:105)) for every j âˆˆ J, then fold TABLE(cid:104)I, j1(cid:105) with respect to x j2 and TABLE(cid:104)I, j2(cid:105) with respect
to x j1 to obtain F S((cid:104)I, { j1, j2}(cid:105)) by taking the minimum of MINCOST(cid:104)I, j1, j2(cid:105) and MINCOST(cid:104)I, j2, j1(cid:105) for
every j1, j2 âˆˆ J, and repeat this to ï¬nally obtain F S((cid:104)I, J(cid:105)). This algorithms is justiï¬ed by Lemma 7.
The details of algorithm FSâˆ— are shown in Appendix D. For each j âˆˆ [|J|], K âŠ† J with |K| = j, and h âˆˆ K,
the time complexity of computing F S((cid:104)I, K(cid:105)) from F S((cid:104)I, K âˆ’ h(cid:105)) is linear to the size of TABLE(cid:104)I,K(cid:105),
i.e., 2nâˆ’|I|âˆ’ j, up to a polynomial factor. The total time is thus, up to a polynomial factor,

2nâˆ’|I|âˆ’ j

(cid:33)

(cid:32)|J|
j

|J|(cid:88)

j=1

< 2nâˆ’|I|âˆ’|J|

2|J|âˆ’ j

(cid:33)

(cid:32)|J|
j

|J|(cid:88)

j=0

= 2nâˆ’|I(cid:116)J| Â· 3|J|.

If we stop the algorithm at j = k, then the algorithm produces the set {F S((cid:104)I, K(cid:105)) : K âŠ† J, |K| = k}.

The time complexity in this case is at most 2nâˆ’|I|âˆ’|J| (cid:80)k

(cid:17)
j=0 2|J|âˆ’ j(cid:16)|J|

j

, up to a polynomial factor.

Since the space complexity is trivially upper-bounded by the time complexity, we complete the
(cid:3)

proof.

Remark 1 One may think that the actual space complexity could be much less than the time complexity.
However, this is not the case. The size of TABLE(cid:104)I,K(cid:105) is also the dominant factor determining the space
complexity. When computing TABLE(cid:104)I,K(cid:105) with |K| = j, it sufï¬ces to keep TABLE(cid:104)I,K(cid:105) and TABLE(cid:104)I,Kâˆ’h(cid:105)
for every h âˆˆ K in memory. The space complexity is thus, up to a polynomial factor, the maximum of
2nâˆ’|I|âˆ’ j(cid:16)|J|

over all j âˆˆ [|J|], which is the same order as the time complexity.

(cid:17) + 2nâˆ’|I|âˆ’( jâˆ’1)(cid:16) |J|

(cid:17)
jâˆ’1

j

11

LemmaThere is a deterministic algorithm FS* that, given the partial OBDD for !"and #âŠ†%âˆ–', produces the partial OBDD for !"âŠ”)in time *âˆ—2-.".)3)time/space.âˆ…%11âŠ”#1|1âŠ”#|Remark 2 It is not difï¬cult to see that the algorithm FSâˆ— works even when the function f has a multival-
ued function: f : {0, 1}n â†’ Z. The only difference from the Boolean case is that the truth table maps each
Boolean assignment to a value in Z. In this case, the algorithm produces a variant of an OBDD (called
a multi-terminal BDD, MTBDD) of minimum size. In addition, our algorithm with two-line modiï¬ca-
tions to the table folding rule in FSâˆ— can construct a minimum zero-suppressed BDD (ZDD) [Min93]
for a given Boolean function. The details are described in Appendix D. These modiï¬cations are also
possible for the quantum algorithms described later, since they perform table folding by running FSâˆ— as
a subroutine.

The following theorem is the basis of our quantum algorithms.

Lemma 9 (Divide-and-Conquer) For any disjoint subsets I1, . . . , Im, J âŠ† [n] with J (cid:44) âˆ… and any k âˆˆ
[|J|], it holds that

MINCOST(cid:104)I1,...,Im,J(cid:105)[J] =

min
K : KâŠ†J,|K|=k

(cid:0)

MINCOST(cid:104)I1,...,Im,K(cid:105)[K] + MINCOST(cid:104)I1,...,Im,K,J\K(cid:105)[J \ K](cid:1) .

(3)

In particular, when I1 (cid:116) Â· Â· Â· (cid:116) Im = âˆ… and J = [n], it holds that

MINCOST[n] = min

KâŠ†[n],|K|=k

(cid:0)

MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K](cid:1) .

(4)

Proof. We ï¬rst prove the special case of I1 (cid:116) Â· Â· Â· (cid:116) Im = âˆ… and J = [n]. By the deï¬nition, we have

MINCOST[n] =

n(cid:88)

j=1

CostÏ€[ j]( f, Ï€) =

k(cid:88)

j=1

CostÏ€[ j]( f, Ï€) +

n(cid:88)

j=k+1

CostÏ€[ j]( f, Ï€),

for the optimal permutation Ï€ = Ï€[n]. Let K = {Ï€[1], . . . , Ï€[k]}. By Lemma 3, the ï¬rst sum is independent
of how Ï€ maps {k + 1, . . . , n} to [n] \ K. Thus, it is equal to the minimum of (cid:80)k
j=1 CostÏ€1[ j]( f, Ï€) over all
Ï€1 âˆˆ Î (K), i.e., MINCOSTK. Similarly, the second sum is independent of how Ï€ maps [k] to K. Thus, it is
equal to the minimum of (cid:80)n
j=k+1 CostÏ€2[ j]( f, Ï€) over all Ï€2 âˆˆ Î ((cid:104)K, [n] \ K(cid:105)), i.e., MINCOST(cid:104)K,[n]\K(cid:105)[[n] \
K]. This completes the proof of Eq. (4).

We can generalize this in a straightforward manner. Let Ï€ = Ï€(cid:104)I1,...,Im,J(cid:105) and (cid:96) = |I1 (cid:116) Â· Â· Â· (cid:116) Im|. Then,

we have

MINCOST(cid:104)I1,...,Im,J(cid:105)[J] =

k(cid:88)

j=1

CostÏ€[(cid:96)+ j]( f, Ï€) +

|J|(cid:88)

j=k+1

CostÏ€[(cid:96)+ j]( f, Ï€).

By deï¬ning K := {Ï€[(cid:96) + 1], . . . , Ï€[(cid:96) + k]}, the same argument as the special case of (cid:96) = 0 implies that
the ï¬rst and second sums are MINCOST(cid:104)I1,...,Im,K(cid:105)[K] and MINCOST(cid:104)I1,...,Im,K,J\K(cid:105)[J \ K], respectively. This
(cid:3)
completes the proof of Eq. (3).

A schematic view of the above lemma is shown in Fig. 8.

3.1 Simple Case

(cid:17)
(cid:16)n
k

We provide simple quantum algorithms on the basis of Lemma 9. The lemma states that, for any k âˆˆ [n],
MINCOST[n] is the minimum of MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K] over all K âŠ† [n] with |K| = k. To
ï¬nd K from among
possibilities that minimizes this amount, we use the quantum minimum ï¬nding
(Lemma 6). To compute MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K] = MINCOST(cid:104)K,[n]\K(cid:105), it sufï¬ces to
ï¬rst compute F S(K) (including MINCOSTK), and then F S((cid:104)K, [n] \ K(cid:105)) (including MINCOST(cid:104)K,[n]\K(cid:105))
from F S(K). The time complexity for computing F S(K) from F S(âˆ…) is Oâˆ—(2nâˆ’k3k) by Lemma 8 with
I1 (cid:116) Â· Â· Â· (cid:116) Im = âˆ… and J = K, while that for computing F S((cid:104)K, [n] \ K(cid:105)) from F S(K) is Oâˆ—(3nâˆ’k) by
Lemma 8 with m = 1, I1 = K, and J = [n]\K. Thus, the time complexity for computing F S((cid:104)K, [n]\K(cid:105))

12

Figure 8: Schematic view of Eq. (4) in Lemma 9.
Intuitively, the lemma says that it is possible to
decompose FSâˆ— into the parts each of which goes through the dot corresponding to a subset I âŠ† [n] of
some ï¬xed size, and the optimal variable ordering is induced by one of the parts.

from F S(âˆ…) is Oâˆ—(2nâˆ’k3k + 3nâˆ’k). Thus, for k = Î±n with Î± âˆˆ (0, 1) ï¬xed later, the total time complexity
up to a polynomial factor is

T (n) =

(cid:33) (cid:16)

(cid:115)(cid:32) n
Î±n

2(1âˆ’Î±)n3Î±n + 3(1âˆ’Î±)n(cid:17)

â‰¤ 2

1

2 H(Î±)n (cid:110)

2[(1âˆ’Î±)+Î± log2 3]n + 2[(1âˆ’Î±) log2 3]n(cid:111)

.

To balance the both terms, we set (1 âˆ’ Î±) + Î± log2 3 = (1 âˆ’ Î±) log2 3 and obtain Î± = Î±âˆ—, where Î±âˆ— =
log2 3âˆ’1
2 log2 3âˆ’1 â‰ˆ 0.269577. We have minÎ±âˆˆ[0,1] T (n) = O
0), where Î³0 =
2.98581 . . . . This slightly improves the classical best bound Oâˆ—(3n) on the time complexity. To improve
the bound further, we introduce a preprocess that classically computes F S(K) for every K with |K| =
Î±n (Î± âˆˆ (0, 1)) by using algorithm FSâˆ—. By Lemma 7, the preprocessing time is then

2 H(Î±âˆ—)n+(1âˆ’Î±âˆ—)n+Î±âˆ—(log2 3)n(cid:17) = O(Î³n

2

(cid:16)

1

2nâˆ’ j Â·

(cid:32)n
(cid:33)
j

Î±n(cid:88)

j=1

â‰¤ Î±n Â· max
jâˆˆ[Î±n]

2nâˆ’ j

(cid:33)
(cid:32)n
j

(cid:47)

(cid:40) 2(1âˆ’Î±)n+H(Î±)n
3 n+H(1/3)n

2

2

(Î± < 1/3)
(Î± â‰¥ 1/3),

(5)

(cid:17)
since 2nâˆ’ j(cid:16)n
increases when j < n/3 and decreases otherwise. Note that once this preprocess is com-
pleted, we can use F S(K) for free and assume that the cost for accessing F S(K) is polynomially
bounded for all K âŠ† [n] with |K| = Î±n.

j

Then, assuming that Î± < 1/3, the total time complexity up to a polynomial factor is

T (n) =

2nâˆ’ j Â·

(cid:33)
(cid:32)n
j

+

Î±n(cid:88)

j=1

(cid:33) (cid:16)

(cid:115)(cid:32) n
Î±n

nO(1) + 3(1âˆ’Î±)n(cid:17) (cid:47) 2[(1âˆ’Î±)+H(Î±)]n + 2[ 1

2 H(Î±)+(1âˆ’Î±) log2 3]n.

2 H(Î±) + (1 âˆ’ Î±) log2 3 and obtain the solution
To balance the both terms, we set (1 âˆ’ Î±) + H(Î±) = 1
Î± = Î±âˆ—, where Î±âˆ— := 0.274863 . . . , which is less than 1/3 as we assumed. At Î± = Î±âˆ—, we have
T (n) (cid:47) 2[(1âˆ’Î±âˆ—)+H(Î±âˆ—)]n = Oâˆ—(Î³n
1), where Î³1 is at most 2.97625 (< Î³0). Thus, introducing the preprocess
improves the complexity bound. A schematic view of the above algorithm is shown in Fig. 9.

Appendix B shows that, by using Lemma 9 twice, we have a better complexity bound Oâˆ—(Î³n

2), where

Î³2 is at most 2.8569 (< Î³1).

13

âˆ…"This corresponds to #âŠ‚".|#|0"Output '(,*âˆ–('*='-,*âˆ–-such thatK achieves the minimum of mincost56,7âˆ–6Figure 9: Schematic view of our algorithm in the simplest case (one-parameter case). The dotted area
is computed in the classical preprocess, which is realized by truncating the process of FSâˆ— as stated in
Lemma 8. The shaded area is computed by using FSâˆ—. The actual algorithm runs the quantum minimum
ï¬nding, which calls FSâˆ— to coherently compute the shaded area corresponding to every dot on the vertical
line indicated by k.

3.2 General Case

We can improve this bound further by applying Lemma 9 k times. We denote the resulting algorithm
with parameters k and Î± := (Î±1, . . . , Î±k) by OptOBDD(k, Î±) where 0 < Î±1 < Â· Â· Â· < Î±k < 1. Its pseudo
code is given in Appendix D. In addition, we assume Î±1 < 1/3 and Î±k+1 = 1 in the following complexity
analysis.

To simplify notations, deï¬ne two function as follows: for x, y âˆˆ R such that 0 < x < y < 1,

f (x, y) := 1
2

y Â· H

(cid:33)

(cid:32) x
y

+ g(x, y),

g(x, y) := (1 âˆ’ y) + (y âˆ’ x) log2(3).

The time required for the preprocess is (cid:80)Î±1n
complexity can be described as the following recurrence:

(cid:96)=1 2nâˆ’(cid:96) Â·

(cid:17)

(cid:16)n
(cid:96)

up to a polynomial factor. Thus, the total time

T (n) =

Î±1n(cid:88)

2nâˆ’(cid:96) Â·

(cid:33)

+ Lk+1(n),

(cid:32)n
(cid:96)
(cid:33) (cid:16)

(cid:96)=1
(cid:115)(cid:32)

Î± j+1n
Î± jn

L j+1(n) =

L j(n) + 2(1âˆ’Î± j+1)n3(Î± j+1âˆ’Î± j)n(cid:17) =

L1(n) = Oâˆ—(1),

(cid:115)(cid:32)

(cid:33) (cid:16)

Î± j+1n
Î± jn

L j(n) + 2g(Î± j,Î± j+1)n(cid:17)

,

(6)

(7)

(8)

for each j âˆˆ [k]. Intuitively, L j(n) is the time required for producing F S((cid:104)K1, K2 \ K1, . . . , K j \ K jâˆ’1(cid:105))
such that MINCOST(cid:104)K1,K2\K1,...,K j\K jâˆ’1(cid:105) is minimum over all K1, . . . , K jâˆ’1 satisfying |K(cid:96)| = Î±(cid:96)n for every
(cid:96) âˆˆ [k + 1] and K(cid:96) âŠ‚ K(cid:96)+1 for every (cid:96) âˆˆ [k].
Since L1(n) = Oâˆ—(1), we have L2(n) (cid:47)

Â·2g(Î±1,Î±2)n (cid:47) 2 f (Î±1,Î±2)n. By setting f (Î±1, Î±2) = g(Î±2, Î±3),

(cid:113)(cid:16)Î±2n
(cid:17)
Î±1n

we have

L3(n) =

(cid:115)(cid:32)

(cid:33)
Î±3n
Î±2n

Â· (L2(n) + 2g(Î±2,Î±3)n) (cid:47)

(cid:115)(cid:32)

(cid:33)
Î±3n
Î±2n

Â· 2g(Î±2,Î±3)n (cid:47) 2 f (Î±2,Î±3)n.

In general, for j = 2, . . . , k, setting f (Î± jâˆ’1, Î± j) = g(Î± j, Î± j+1) yields L j+1(n) (cid:47) 2 f (Î± j,Î± j+1)n. Therefore, the

14

âˆ…ğ‘›ğ‘˜Quantumly find the minimumFS*Computed by classical preprocess (truncation of FS*)total complexity [Eq. (6)] is

T (n) (cid:47)

2nâˆ’(cid:96) Â·

(cid:33)

(cid:32)n
(cid:96)

Î±1n(cid:88)

(cid:96)=1

+ 2 f (Î±k,Î±k+1)n (cid:47) 2(1âˆ’Î±1)n+H(Î±1)n + 2 f (Î±k,1)n,

where we use Î±1 < 1/3, Î±k+1 = 1, and Eq. (5). To optimize the right-hand side, we set parameters so
that 1 âˆ’ Î±1 + H(Î±1) = f (Î±k, 1).

In summary, we need to ï¬nd the values of parameters Î±1, . . . , Î±k that satisfy the following system of

equations and Î±1 < 1/3:

1 âˆ’ Î±1 + H(Î±1) = f (Î±k, 1),

f (Î± jâˆ’1, Î± j) = g(Î± j, Î± j+1)

( j = 2, . . . , k).

(9)

(10)

By numerically solving this system of equations, we obtain T (n) = O(Î³n
k), where Î³k and the correspond-
ing Î±iâ€™s for k = 1, . . . , 6 are shown in Tab. 1 in Appendix C (Î±1 < 1/3 is satisï¬ed as we assumed). The
value of Î³k becomes smaller as k increases. However, incrementing k beyond 6 provides only negligible
improvement of Î³k. The value of Î³6 is at most 2.83728. Since the space complexity is trivially upper-
bounded by the time complexity, we have the following theorem. Note that the values of Î±iâ€™s are not
symmetric with respect to 1/2. This reï¬‚ects the fact that optimizing cost is not symmetric with respect
to 1/2, contrasting with many other combinatorial problems.

Theorem 10 There exists a quantum algorithm that, for the truth table of f : {0, 1}n â†’ {0, 1} given as
input, produces F S([n]) with probability 1 âˆ’ exp(âˆ’â„¦(n)) in Oâˆ—(Î³n) time and space, where the constant
Î³ is at most 2.83728, which is achieved by OptOBDD(k, Î±) with k = 6 and

Î± = (0.183791, 0.183802, 0.183974, 0.186131, 0.206480, 0.343573).

4 Quantum Algorithm with Composition

4.1 Quantum Composition Lemma

Recall that the classical composition lemma (Lemma 8) states that algorithm FS can be generalized to
compute F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)). By generalizing the quantum algorithm given in
Theorem 10, we now provide a quantum version of Lemma 8, called the quantum composition lemma.

Lemma 11 (Quantum Composition: Base Part) For any disjoint subsets I1, . . . , Im, J âŠ† [n], there
exists a quantum algorithm that, with probability 1 âˆ’ exp(âˆ’â„¦(n)), produces F S((cid:104)I1, . . . , Im, J(cid:105)) from
F S((cid:104)I1, . . . , Im(cid:105)) for an underlying function f : {0, 1}n â†’ {0, 1} in Oâˆ— (cid:16)
time and
space, where Î³ is the constant deï¬ned in Theorem 10.

2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· Î³|J|(cid:17)

A pseudo code of the algorithm provided in Lemma 11 is shown as OptOBDDâˆ—

Î“(k, Î±) in Appendix D,
where the subscript Î“, which represents the subroutine appearing in line 16, will be set to the determin-
istic algorithm FSâˆ—, and k and Î± will be set to the values speciï¬ed in Theorem 10.
Proof. Since the space complexity is trivially upper-bounded by the time complexity, we analyze the
time complexity in the following. For simplicity, we assume m = 1 and write just I instead of I1. It is
straightforward to generalize to the case of m â‰¥ 2.
We now provide the algorithm OptOBDDâˆ—

Î“(k, Î±) that produces F S((cid:104)I, J(cid:105)) from F S((cid:104)I(cid:105)), where the
subroutine Î“ used in line 16 is set to algorithm FSâˆ—. As parameters, the algorithm has an integer k âˆˆ N
and a vector Î± := (Î±1, . . . , Î±k) âˆˆ Rk such that 0 < Î±1 < Â· Â· Â· < Î±k < 1. Set k and Î± to the same values
assumed in the algorithm in Theorem 10. In addition, we assume Î±k+1 = 1 in the following.

15

Let n(cid:48) = |J|. In the preprocess, the algorithm computes the collection {F S((cid:104)I, K(cid:105)) : K âŠ† J, |K| =
up to a poly-

Î±1n(cid:48)} based on Lemma 8 for given F S(I) with the time complexity 2nâˆ’|I|âˆ’n(cid:48) (cid:80)Î±1n(cid:48)
nomial factor. Thus, the total time complexity is expressed as

(cid:96)=1 2n(cid:48)âˆ’(cid:96)(cid:16)n(cid:48)

(cid:17)

(cid:96)

T (cid:48)(n, n(cid:48)) = 2nâˆ’|I|âˆ’n(cid:48)

Î±1n(cid:48)
(cid:88)

(cid:96)=1

2n(cid:48)âˆ’(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ L(cid:48)

k+1(n, n(cid:48)),

(11)

where L(cid:48)

k+1(n, n(cid:48)) is the time taken to perform all but the preprocess.

Based on Lemma 9, the algorithm proceeds in a way similar to the one given in Theorem 10, which
k+1(n, n(cid:48)) is then expressed by the

corresponds to the special case of I = âˆ… and J = [n]. The complexity L(cid:48)
following recurrence:

j+1(n, n(cid:48)) =
L(cid:48)

(cid:115)(cid:32)

Î± j+1n(cid:48)
Î± jn(cid:48)

(cid:33) (cid:16)

1(n, n(cid:48)) = Oâˆ—(1).
L(cid:48)

j(n, n(cid:48)) + 2nâˆ’|I|âˆ’Î± j+1n(cid:48)
L(cid:48)

3(Î± j+1âˆ’Î± j)n(cid:48)(cid:17)

[ j âˆˆ [k]],

In the following, we prove by induction that T (cid:48)(n, n(cid:48)) (cid:47) 2nâˆ’|I|âˆ’n(cid:48)

T (n(cid:48)), where the function T (Â·) is

deï¬ned in Eq. (6). Since n(cid:48) = |J| and T (n(cid:48)) = Oâˆ—(Î³n(cid:48)

), this completes the proof for m = 1.

Since L(cid:48)

1(n, n(cid:48)) = Oâˆ—(1), we have

2(n, n(cid:48)) (cid:47) 2nâˆ’|I|âˆ’n(cid:48)
L(cid:48)

(cid:33)

(cid:115)(cid:32)

Î±2n(cid:48)
Î±1n(cid:48)

2(1âˆ’Î±2)n(cid:48)

3(Î±2âˆ’Î±1)n(cid:48) = 2nâˆ’|I|âˆ’n(cid:48)

L2(n(cid:48)),

where the function L2(Â·) is deï¬ned in Eq. (7) for j = 1. This is the base case of the induction. Then,
assuming that L(cid:48)

L j(n(cid:48)), we have

j(n, n(cid:48)) (cid:47) 2nâˆ’|I|âˆ’n(cid:48)
(cid:115)(cid:32)

Î± j+1n(cid:48)
Î± jn(cid:48)

j+1(n, n(cid:48)) (cid:47)
L(cid:48)

(cid:33) (cid:16)

2nâˆ’|I|âˆ’n(cid:48)

L j(n(cid:48)) + 2nâˆ’|I|âˆ’Î± j+1n(cid:48)

3(Î± j+1âˆ’Î± j)n(cid:48)(cid:17)

= 2nâˆ’|I|âˆ’n(cid:48)

(cid:115)(cid:32)

(cid:33) (cid:16)

Î± j+1n(cid:48)
Î± jn(cid:48)

L j(n(cid:48)) + 2n(cid:48)âˆ’Î± j+1n(cid:48)

3(Î± j+1âˆ’Î± j)n(cid:48)(cid:17) = 2nâˆ’|I|âˆ’n(cid:48)

L j+1(n(cid:48)).

Therefore, it holds that L(cid:48)

k+1(n, n(cid:48)) (cid:47) 2nâˆ’|I|âˆ’n(cid:48)

Lk+1(n(cid:48)) by induction. Then, it follows from Eq. (11) that

T (cid:48)(n, n(cid:48)) (cid:47) 2nâˆ’|I|âˆ’n(cid:48)

Î±1n(cid:48)
(cid:88)

(cid:96)=1

2n(cid:48)âˆ’(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ 2nâˆ’|I|âˆ’n(cid:48)

Lk+1(n(cid:48)) = 2nâˆ’|I|âˆ’n(cid:48)

T (n(cid:48)).

Since T (n(cid:48)) = T (|J|) = Oâˆ—(Î³|J|) by Theorem 10, the lemma follows.

(cid:3)

Lemma 12 (Quantum Composition: Induction Part) Suppose that Î“ is a quantum algorithm that, for
any disjoint subsets I1, . . . , Im, J âŠ† [n] with J (cid:44) âˆ…, produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105))
2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· Î³|J|(cid:17)
with probability 1 âˆ’ exp(âˆ’â„¦(n)) in Oâˆ— (cid:16)
time and space for an underlying function
f : {0, 1}n â†’ {0, 1}. Then, for any possible k âˆˆ N and Î± âˆˆ Rk and for any disjoint subsets I1, . . . , Im, J âŠ†
[n] with J (cid:44) âˆ…, OptOBDDâˆ—
Î“(k, Î±) produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)) with probability
1 âˆ’ exp(âˆ’â„¦(n)) in Oâˆ— (cid:16)
time and space for the function f , where Î²n
k upper-bounds,
up to a polynomial factor, the time complexity required for OptOBDDâˆ—
Î“(k, Î±) to compute F S([n]) from

2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· Î²|J|
k

(cid:17)

16

F S(âˆ…), that is, T (n) = Oâˆ—(Î²n

k) for T (n) that satisï¬es the following recurrence:

T (n) =

Î±1n(cid:88)

2nâˆ’(cid:96)

(cid:32)n
(cid:33)
(cid:96)
(cid:33) (cid:16)

(cid:96)=1
(cid:115)(cid:32)

Î± j+1n
Î± jn

L j+1 =

+ Lk+1,

L j + 2(1âˆ’Î± j+1)nÎ³(Î± j+1âˆ’Î± j)n(cid:17) =

L1 = Oâˆ—(1),

where gÎ³(x, y) := (1 âˆ’ y) + (y âˆ’ x) log2 Î³.

(cid:115)(cid:32)

(cid:33) (cid:16)

Î± j+1n
Î± jn

(12)

L j + 2gÎ³(Î± j,Î± j+1)(cid:17)

( j âˆˆ [k]),

(13)

(14)

Proof. Recall that algorithm FSâˆ— is used as a subroutine in OptOBDD(k, Î±) provided in Theorem 10.
Since the input and output of Î“ assumed in the statement are the same as those of algorithm FSâˆ—, one
can use Î“ instead of algorithm FSâˆ— in OptOBDD(k, Î±) (compromising on an exponentially small error
probability). Let OptOBDDÎ“(k, Î±) be the resulting algorithm. Then, one can see that the time complexity
T (n) of OptOBDDÎ“(k, Î±) satisï¬es the recurrence: Eqs. (12)-(14), which are obtained by just replacing
g(x, y) with gÎ³(x, y) in Eqs. (6)-(8). Suppose that T (n) = Oâˆ—(Î²n

k) follows from the recurrence.

Next, we generalize OptOBDDÎ“(k, Î±) so that it produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105))
for any disjoint subsets I1, . . . , Im, J âŠ† [n] with J (cid:44) âˆ…. The proof is very similar to that of Lemma 11. The
only difference is that the time complexity of Î“ is Oâˆ— (cid:16)
Namely, when m = 1 and n(cid:48) = |J|, the time complexity of OptOBDDâˆ—
rence: for each j âˆˆ [n],

Î“(k, Î±) satisï¬es the following recur-

2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· Î³|J|(cid:17)

, instead of Oâˆ— (cid:16)

2nâˆ’|I1(cid:116)Â·Â·Â·(cid:116)Im(cid:116)J| Â· 3|J|(cid:17)

.

T (cid:48)(n, n(cid:48)) = 2nâˆ’|I|âˆ’n(cid:48)

Î±1n(cid:48)
(cid:88)

2n(cid:48)âˆ’(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ L(cid:48)

k+1(n, n(cid:48)),

j+1(n, n(cid:48)) =
L(cid:48)

(cid:115)(cid:32)

(cid:96)=1
(cid:33) (cid:16)

Î± j+1n(cid:48)
Î± jn(cid:48)

j(n, n(cid:48)) + 2nâˆ’|I|âˆ’Î± j+1n(cid:48)
L(cid:48)

Î³(Î± j+1âˆ’Î± j)n(cid:48)(cid:17)

[ j âˆˆ [n]],

1(n, n(cid:48)) = Oâˆ—(1),
L(cid:48)

from which it follows that T (cid:48)(n, n(cid:48)) = 2nâˆ’|I|âˆ’n(cid:48)
ize to the case of m â‰¥ 2.

T (n(cid:48)) = Oâˆ— (cid:16)

2nâˆ’|I(cid:116)J| Â· Î²|J|
k

(cid:17)
. It is straightforward to general-
(cid:3)

4.2 The Final Algorithm

Lemmas 11 and 12 naturally lead to the following algorithm. We ï¬rst deï¬ne Î“1 as OptOBDDâˆ—
for some k(0) âˆˆ N and Î±(0) âˆˆ Rk(0)
Î±(1) âˆˆ Rk(1)

(k(i), Î±(i)) for some k(i) âˆˆ N and Î±(i) âˆˆ Rk(i)
.
1 , . . . , Î±(i)
6 ) âˆˆ
[0, 1]6 is set for each i so that it satisï¬es the system of equations, a natural generalization of Eqs. (9)(10),

Fix k(i) = 6 for every i. Note that, in the proof of Lemmas 11 and 12, parameter Î±(i) = (Î±(i)

FSâˆ—(k(0), Î±(0))
(k(1), Î±(1)) for some k(1) âˆˆ N and

. In this way, we can deï¬ne Î“i+1 as OptOBDDâˆ—
Î“i

. Then, we deï¬ne Î“2 as OptOBDDâˆ—
Î“1

+ H(Î±(i)
jâˆ’1, Î±(i)

1 âˆ’ Î±(i)
1
fÎ³(Î±(i)

1 ) = fÎ³(Î±(i)
j ) = gÎ³(Î±(i)

where fÎ³(x, y) := 1

6 , 1)
j , Î±(i)
( j = 2, . . . , 6),
(cid:17) + gÎ³(x, y) and gÎ³(x, y) := (1 âˆ’ y) + (y âˆ’ x) log2 Î³.
By numerically solving this system of equations for Î³ = 3, we have Î²6 < 2.83728 as shown in
Theorem 10. Then, numerically solving the system of equations with Î³ = 2.83728, we have Î²6 <
2.79364. In this way, we obtain a certain Î³ less than 2.77286 at the tenth composition (see Tab. 2 in
Appendix C). We therefore obtain the following theorem.

2 y Â· H

j+1)

(16)

(15)

(cid:16) x
y

17

Theorem 13 There exists a quantum algorithm that, for the truth table of f : {0, 1}n â†’ {0, 1} given as
input, produces F S([n]) in Oâˆ—(Î³n) time and space with probability 1 âˆ’ exp(âˆ’â„¦(n)), where the constant
Î³ is at most 2.77286.

Appendix

A Proofs of Lemmas

Proof of Lemma 4. To show the ï¬rst equality, assume k = Ï€I[|I|]. By the deï¬nition, we have

MINCOSTI =

(cid:88)

iâˆˆI

Costi( f, Ï€I) =

(cid:88)

iâˆˆI\{k}

Costi( f, Ï€I) + Costk( f, Ï€I).

The ï¬rst term is equal to MINCOSTI\k, since otherwise there exists Ï€(cid:48) âˆˆ Î (I) with Ï€(cid:48)[|I|] = k and
Ï€(cid:48)[ j] (cid:44) Ï€I[ j] for some j âˆˆ [|I| âˆ’ 1] such that
(cid:88)

(cid:88)

Costi( f, Ï€(cid:48)) =

Costi( f, Ï€(cid:48)) + Costk( f, Ï€(cid:48))

iâˆˆI

iâˆˆI\{k}
(cid:88)

iâˆˆI\{k}

<

Costi( f, Ï€I) + Costk( f, Ï€I) = MINCOSTI,

which contradicts the deï¬nition of MINCOSTI, where we use Costk( f, Ï€(cid:48)) = Costk( f, Ï€I) by Lemma 3.
The remaining term Costk( f, Ï€I) is equal to Costk( f, Ï€(cid:104)I\k,k(cid:105)) by Lemma 3. Thus, the ï¬rst equality
in the statement of the lemma holds. Since MINCOSTI\k = (cid:80)
iâˆˆI\k Costi( f, Ï€I\k) by the deï¬nition, and
Lemma 3 implies that Costi( f, Ï€I\k) = Costi( f, Ï€(cid:104)I\k,k(cid:105)) for every i âˆˆ I \ k, it holds that MINCOSTI\k =
(cid:80)
iâˆˆI Costi( f, Ï€(cid:104)I\k,k(cid:105)). This implies
(cid:3)

iâˆˆI\k Costi( f, Ï€(cid:104)I\k,k(cid:105)). Therefore, MINCOSTI\k + Costk( f, Ï€(cid:104)I\k,k(cid:105)) = (cid:80)

that the second equality in the lemma.

Proof of Lemma 7. We focus on the simplest case of m = 1 and write just I instead of I1, since it is
straightforward to generalize the proof to the case of m â‰¥ 2.

To show the ï¬rst equality, assume k = Ï€(cid:104)I,J(cid:105)[|I (cid:116) J|]. By the deï¬nition, we have

MINCOST(cid:104)I,J(cid:105) =

(cid:88)

iâˆˆI(cid:116)J

Costi( f, Ï€(cid:104)I,J(cid:105)) =

(cid:88)

iâˆˆI(cid:116)J\{k}

Costi( f, Ï€(cid:104)I,J(cid:105)) + Costk( f, Ï€(cid:104)I,J(cid:105)).

The ï¬rst term is equal to MINCOST(cid:104)I,J\{k}(cid:105), since otherwise there exists Ï€(cid:48) âˆˆ Î ((cid:104)I, J(cid:105)) with Ï€(cid:48)[|I (cid:116) J|] = k
and Ï€(cid:48)[ j] (cid:44) Ï€(cid:104)I,J(cid:105)[ j] for some j âˆˆ [|I (cid:116) J| âˆ’ 1] such that

(cid:88)

iâˆˆI(cid:116)J

Costi( f, Ï€(cid:48)) =

(cid:88)

Costi( f, Ï€(cid:48)) + Costk( f, Ï€(cid:48))

iâˆˆI(cid:116)J\{k}
(cid:88)

iâˆˆI(cid:116)J\{k}

<

Costi( f, Ï€(cid:104)I,J(cid:105)) + Costk( f, Ï€(cid:104)I,J(cid:105)) = MINCOST(cid:104)I,J(cid:105),

which contradicts the deï¬nition of MINCOST(cid:104)I,J(cid:105), where we use Costk( f, Ï€(cid:48)) = Costk( f, Ï€(cid:104)I,J(cid:105)) by Lemma 3.
The remaining term Costk( f, Ï€(cid:104)I,J(cid:105)) is equal to Costk( f, Ï€(cid:104)I,Jâˆ’k,k(cid:105)) by Lemma 3. Thus, the ï¬rst equality
in the statement of the lemma holds. Since MINCOST(cid:104)I,J\k(cid:105) = (cid:80)
iâˆˆI(cid:116)J\k Costi( f, Ï€(cid:104)I,J\k(cid:105)) by the deï¬ni-
tion, and Lemma 3 implies that Costi( f, Ï€(cid:104)I,J\k(cid:105)) = Costi( f, Ï€(cid:104)I,J\k,k(cid:105)) for every i âˆˆ I (cid:116) J \ k, it holds
that MINCOST(cid:104)I,J\k(cid:105) = (cid:80)
iâˆˆI(cid:116)J\k Costi( f, Ï€(cid:104)I,J\k,k(cid:105)). Therefore, MINCOST(cid:104)I,J\k(cid:105) + Costk( f, Ï€(cid:104)I,J\k,k(cid:105)) =
(cid:80)
(cid:3)

iâˆˆI(cid:116)J Costi( f, Ï€(cid:104)I,J\k,k(cid:105)). This implies that the second equality in the lemma.

18

min
K1âŠ‚K2 : |K1|=k1
(cid:16)k2
k1

B Two-Parameter Case

To improve the complexity bound further, we use Lemma 9 recursively. Let k1 and k2 be parameters
ï¬xed later such that 0 < k1 < k2 < n. By applying the lemma once, we have

MINCOST[n] =

(cid:0)

min
K2âŠ‚[n],|K(cid:48)|=k2

MINCOSTK2

+ MINCOST(cid:104)K2,[n]\K2(cid:105)[[n] \ K2](cid:1) .

Then, we apply the lemma again to MINCOSTK(cid:48) to obtain

MINCOSTK2

=

(cid:0)

MINCOSTK1

+ MINCOST(cid:104)K1,K2\K1(cid:105)[K2 \ K1](cid:1) .

To ï¬nd the optimal K1 from among

(cid:17)
, we use
the quantum minimum ï¬nding (Lemma 6). As in the previous case, we perform the classical preprocess
that computes F S(K1) for all K1 âŠ† [n] with |K1| = k1.

possibilities and the optimal K2 from among

(cid:16) n
k2

If we set k1 = Î±1n and k2 = Î±2n, assuming that Î±1 < 1/3, the total time complexity (up to a

(cid:17)

polynomial factor) is

where

L3(n) =

(cid:33) (cid:16)

(cid:115)(cid:32) n
Î±2n

T (n) =

2nâˆ’(cid:96) Â·

(cid:33)

(cid:32)n
(cid:96)

Î±1n(cid:88)

(cid:96)=1

+ L3(n) (cid:47) 2(1âˆ’Î±1)n+H(Î±1)n + L3(n),

L2(n) + 3(1âˆ’Î±2)n(cid:17) (cid:47) 2

1

2 H(Î±2)n(L2(n) + 2(1âˆ’Î±2)(log2 3)n),

(cid:115)(cid:32)

L2(n) =

Î±2n
Î±1n
L1(n) = Oâˆ—(1).

(cid:33) (cid:16)

L1(n) + 2(1âˆ’Î±2)n3(Î±2âˆ’Î±1)n(cid:17) (cid:47) 2

1

2 H(Î±1/Î±2)Î±2n(L1(n) + 2(1âˆ’Î±2)n+(Î±2âˆ’Î±1)(log2 3)n),

(17)

(18)

(19)

(20)

Intuitively, L2(n) represents the time required for producing F S((cid:104)K1, K2 \ K1(cid:105)) for ï¬xed K2 such that
MINCOST(cid:104)K1,K2(cid:105) is minimum over all K1 (âŠ‚ K2) for the given collection of F S(K1) for all K1. Note that,
by Lemma 8, the time required for computing F S((cid:104)K1, K2 \ K1, [n] \ K2(cid:105)) from F S((cid:104)K1, K2 \ K1(cid:105)) is
Oâˆ—(3(1âˆ’Î±2)n), and the time required for computing F S((cid:104)K1, K2\K1(cid:105)) from F S(K1) is Oâˆ—(2(1âˆ’Î±2)n3(Î±2âˆ’Î±1)n).
Since we are not interested in polynomial factors in T (n), we ignore them in the following analysis

of this system.

From Eqs. (19) and (20), L2(n) is at most 2

2 H(Î±1/Î±2)Î±2n Â· 2(1âˆ’Î±2)n+(Î±2âˆ’Î±1)(log2 3)n. Suppose that L2(n)
equals this upper bound, since our goal is to upper-bound the complexity. To balance the two terms on
the right-hand side of Eq. (18), we set L2(n) = 2(1âˆ’Î±2)(log2 3)n, which implies

1

1
2

Î±2H(Î±1/Î±2) + (1 âˆ’ Î±2) + (Î±2 âˆ’ Î±1) log2 3 = (1 âˆ’ Î±2) log2 3.

(21)

2 H(Î±2)n2(1âˆ’Î±2)(log2 3)n. Suppose again that L3(n) equals
On the condition that this holds, L3(n) is at most 2
this upper bound. To balance the two terms on the right-hand side in Eq. (17), we have L3(n) =
2(1âˆ’Î±1)n+H(Î±1)n, implying that

1

(1 âˆ’ Î±1) + H(Î±1) = 1
2

H(Î±2) + (1 âˆ’ Î±2) log2 3.

= 0.192755 and Î±âˆ—
By numerically solving Eqs. (21) and (22), we have Î±âˆ—
2
1
1 and Î±2 = Î±âˆ—
less than 1/3, as we assumed. For Î±1 = Î±âˆ—
2,
1)+H(Î±âˆ—
1)]n = Oâˆ—(Î³2),

T (n) (cid:47) 2[(1âˆ’Î±âˆ—

(22)

= 0.334571. Note that Î±âˆ—

1 is

where Î³2 = 2.8569 < Î³1.

19

C Numerical Optimization Data

Table 1: Values of Î³k and the corresponding Î±iâ€™s of algorithm OptOBDD(k, Î±): Each value is written
with 6 digits, but the actual calculation is done with 20-digit precision.

k
1
2
3
4
5
6

Î³k
2.97625
2.85690
2.83925
2.83744
2.83729
2.83728

Î±1
0.274862
0.192754
0.184664
0.183859
0.183795
0.183791

Î±2
â€”
0.334571
0.205128
0.186017
0.183967
0.183802

Î±3
â€”
â€”
0.342677
0.206375
0.186125
0.183974

Î±4
â€”
â€”
â€”
0.343503
0.206474
0.186131

Î±5
â€”
â€”
â€”
â€”
0.343569
0.206480

Î±6
â€”
â€”
â€”
â€”
â€”
0.343573

Î“(k, Î±): Each value is written

Table 2: Values of Î³ and the corresponding Î±iâ€™s of algorithm OptOBDDâˆ—
with 6 digits, but the actual calculation is done with 20-digit precision.
Î±4
0.186132
0.167339
0.16189
0.160124
0.159532
0.159332
0.159264
0.159241
0.159233
0.159230

Î±3
0.183974
0.165857
0.160574
0.158859
0.158284
0.158089
0.158023
0.158000
0.157992
0.157990

Î±1
0.183792
0.165753
0.160487
0.158777
0.158203
0.158009
0.157943
0.15792
0.157913
0.157910

Î±2
0.183802
0.165759
0.160491
0.15878
0.158207
0.158013
0.157947
0.157924
0.157916
0.157914

Î³
3
2.83728
2.79364
2.77981
2.77521
2.77366
2.77313
2.77295
2.77289
2.77287

Î²6
2.83728
2.79364
2.77981
2.77521
2.77366
2.77313
2.77295
2.77289
2.77287
2.77286

Î±5
0.206480
0.183883
0.177376
0.175273
0.174568
0.174330
0.174249
0.174221
0.174212
0.174208

Î±6
0.343573
0.312741
0.303603
0.300622
0.299621
0.299282
0.299166
0.299127
0.299114
0.299109

20

D Pseudo Codes of Algorithms

Algorithm FSâˆ—: Composable variant of algorithm FS.â€œA â† Bâ€ means that B is substituted for A.
Input: disjoint subsets I, J âˆˆ [n] and F S(I)
Output: F S((cid:104)I, J(cid:105))

1 Function Main()
2

for (cid:96) := 1 to |J| do

3

4

5

6

7

8

9

10

11

12

for each (cid:96)-element subset K âŠ† J do

MINCOST(cid:104)I,K(cid:105) â† +âˆ;
for each k âˆˆ K do

F S((cid:104)I, K âˆ’ k, k(cid:105)) â† FOLD(I, K, k, F S((cid:104)I, K âˆ’ k(cid:105)));
if MINCOST(cid:104)I,K(cid:105) > MINCOST(cid:104)I,Kâˆ’k,k(cid:105) then
F S((cid:104)I, K(cid:105)) â† F S((cid:104)I, K âˆ’ k, k(cid:105));

end

end

end

// initialization

end
return F S((cid:104)I, J(cid:105))

13
14 end
15 Function FOLD(I, K, k, F S((cid:104)I, K âˆ’ k(cid:105)))

// produce F S((cid:104)I, K âˆ’ k, k(cid:105)) from F S((cid:104)I, K âˆ’ k(cid:105))

Ï€(cid:104)I,Kâˆ’k,k(cid:105) â† (Ï€(cid:104)I,Kâˆ’k(cid:105)[1], . . . , Ï€(cid:104)I,Kâˆ’k(cid:105)[|I (cid:116) K| âˆ’ 1], k) ;
MINCOST(cid:104)I,Kâˆ’k,k(cid:105) â† MINCOST(cid:104)I,Kâˆ’k(cid:105);
NODE(cid:104)I, K âˆ’ k, k(cid:105) â† âˆ…;
for b âˆˆ {0, 1}nâˆ’|I|âˆ’|K| do

u0 â† TABLE(cid:104)I,Kâˆ’k(cid:105)[x[n]\(I(cid:116)K) = b, xk = 0];
u1 â† TABLE(cid:104)I,Kâˆ’k(cid:105)[x[n]\(I(cid:116)K) = b, xk = 1];
if u0 = u1 then

TABLE(cid:104)I,Kâˆ’k,k(cid:105)[x[n]\(I(cid:116)K) = b] â† u0
else if âˆƒu (u, u0, u1) âˆˆ NODE(cid:104)I, K âˆ’ k, k(cid:105) then

TABLE(cid:104)I,Kâˆ’k,k(cid:105)[x[n]\(I(cid:116)K) = b] â† u

else

end

TABLE(cid:104)I,Kâˆ’k,k(cid:105)[x[n]\(I(cid:116)K) = b] â† MINCOST(cid:104)I,Kâˆ’k,k(cid:105) + 2;
MINCOST(cid:104)I,Kâˆ’k,k(cid:105) â† MINCOST(cid:104)I,Kâˆ’k,k(cid:105) + 1;
Insert (u, u0, u1) into NODE(cid:104)I, K âˆ’ k, k(cid:105)

end
return F S((cid:104)I, K âˆ’ k, k(cid:105))

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32
33 end

// initialization
// initialization
// initialization

// create a new node

Adaptation to ZDD

To adapt FSâˆ— to ZDD, it sufï¬ces to modify lines 22 and 23 in algorithm FSâˆ— follows:

if u1 = 0 then

TABLE(cid:104)I,Kâˆ’k,k(cid:105)[x[n]\(I(cid:116)K) = b] â† u0

21

Algorithm OptOBDD(k, Î±): Quantum OBDD-minimization algorithm with parameters k âˆˆ N and Î± :=
(Î±1, . . . , Î±k) âˆˆ Rk satisfying 0 < Î±1 < Â· Â· Â· < Î±k < 1, where the quantum minimum ï¬nding algorithm is used
in line 8, and FSâˆ— is used in lines 2 and 15. â€œA â† Bâ€ means that B is substituted for A.
Input: F S(âˆ…) :={ TABLEâˆ…, Ï€âˆ…, MINCOSTâˆ…, NODEâˆ… } (accessible from all Functions)
Output: F S([n])

1 Function Main()
2

compute the set {F S(K) : K âŠ† [n], |K| = Î±1n} by algorithm FS (or FSâˆ—);
make the set of these F S(K) global (i.e., accessible from all Functions);
return DivideAndConquer([n], k + 1)

4
5 end
6 Function DivideAndConquer(L, t)
7

if t=1 then return F S(L);
Find K(âŠ‚ L) of cardinality Î±tâˆ’1n, with Lemma 6, that minimizes MINCOST(cid:104)K,L\K(cid:105),

which is computed as an component of F S((cid:104)K, L \ K(cid:105)) by calling ComputeFS(K, L \ K, t);

// Compute F S(L) with Î±1, . . . , Î±t(= |L|/n)
// F S(L) has been precomputed.

3

8

9

10

// Compute F S((cid:104)K, M(cid:105)) with Î±1, . . . , Î±t

let Kâˆ— be the set that achieves the minimum;
return F S((cid:104)Kâˆ—, L \ Kâˆ—(cid:105))

11
12 end
13 Function ComputeFS(K, M, t)
14

F S(K) â† DivideAndConquer(K, t âˆ’ 1);
F S((cid:104)K, M(cid:105)) â† FSâˆ—(K, M, F S(K));
return F S((cid:104)K, M(cid:105))

15

16
17 end

Î“(k, Î±): Composable Quantum OBDD-minimization algorithm with subroutine Î“ and pa-
Algorithm OptOBDDâˆ—
rameters k âˆˆ N and Î± = (Î±1, . . . , Î±k) âˆˆ Rk satisfying 0 < Î±1 < Â· Â· Â· < Î±k < 1, where the quantum minimum
ï¬nding algorithm is used in line 9, and subroutine Î“ is used in line 16. â€œA â† Bâ€ means that B is substituted for A.
Î“(I1, I2, J, F S(I1, I2)) produces F S(I1, I2, J) from F S(I1, I2).
Input: I âŠ† [n], J âŠ† [n], F S(I). (accessible from all Functions)
Output: F S((cid:104)I, J(cid:105))

3

4

1 Function Main()
n(cid:48) â† |J|;
2
compute the set {F S((cid:104)I, K(cid:105)) : K âŠ† J, |K| = Î±1n(cid:48)} by algorithm FSâˆ—;
make n(cid:48) and the above set of F S((cid:104)I, K(cid:105)) global (i.e., accessible from all Functions);
return DivideAndConquer(J, k + 1)

5
6 end
7 Function DivideAndConquer(L, t)
if t=1 then return F S(I, L);
8
Find K(âŠ‚ L) of cardinality Î±tâˆ’1n(cid:48), with Lemma 6, that minimizes MINCOST(cid:104)I,K,L\K(cid:105)

9

// Compute F S((cid:104)I, L(cid:105)) with Î±1, . . . , Î±t
// F S(I, L) has been precomputed.

// initialization

which is computed as an component of F S((cid:104)I, K, L \ K(cid:105)) by calling ComputeFS(I, K, L \ K, t);

10

11

let Kâˆ— be the set that achieves the minimum;
return F S((cid:104)I, Kâˆ—, L \ Kâˆ—(cid:105))

12
13 end
14 Function ComputeFS(I, K, M, t)
15

F S(I, K) â† DivideAndConquer(K, t âˆ’ 1);
F S((cid:104)I, K, M(cid:105)) â† Î“(I, K, M, F S(I, K));
return F S((cid:104)I, K, M(cid:105))

16

17
18 end

// Compute F S((cid:104)K, M(cid:105)) with Î±1, . . . , Î±t

22

References

[ABI+19]

Andris Ambainis, Kaspars Balodis, Janis Iraids, Martins Kokainis, Krisjanis Prusis, and
Jevgenijs Vihrovs. Quantum speedups for exponential-time dynamic programming al-
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
gorithms.
Algorithms, SODA 2019, pages 1783â€“1793, 2019.

[Ake78]

S. B. Akers. Binary decision diagrams. IEEE Trans. Comput., 27(6):509â€“516, June 1978.

[BC95]

Randal E. Bryant and Yirng-An Chen. Veriï¬cation of arithmetic circuits with binary mo-
ment diagrams. In Proceedings of the 32st Conference on Design Automation, San Fran-
cisco, California, USA, Moscone Center, June 12-16, 1995., pages 535â€“541, 1995.

[BCdWZ99] Harry M. Buhrman, Richard E. Cleve, Ronald de Wolf, and Christof Zalka. Bounds for
small-error and zero-error quantum algorithms. In Proceedings of 40th Annual Symposium
on Foundations of Computer Science, FOCS â€™99, pages 358â€“368, 1999.

[BFG+97]

R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, A. Pardo, and F. Somenzi.
Algebric decision diagrams and their applications. Formal Methods in System Design,
10(2â€“3):171â€“206, 1997.

[Bol16]

[Bry86]

[Bry91]

[Bry92]

[Bry18]

[BW96]

Beate Bollig. On the minimization of (complete) ordered binary decision diagrams. The-
ory of Computing Systems, 59(3):532â€“559, Oct 2016.

Randal E. Bryant. Graph-based algorithms for boolean function manipulation.
Trans. Comput., 35(8):677â€“691, August 1986.

IEEE

Randal E. Bryant. On the complexity of VLSI implementations and graph representations
IEEE Transactions on
of boolean functions with application to integer multiplication.
Computers, 40(2):205â€“213, Feb 1991.

Randal E. Bryant. Symbolic boolean manipulation with ordered binary-decision diagrams.
ACM Comput. Surv., 24(3):293â€“318, September 1992.

Randal E. Bryant. Binary decision diagrams. Handbook of Model Checking, pages 191â€“
217, 2018.

B. Bollig and I. Wegener. Improving the variable ordering of OBDDs is NP-complete.
IEEE Transactions on Computers, 45(9):993â€“1002, Sep. 1996.

[CMZ+97] E.M. Clarke, K. L. Mcmillan, X. Zhao, M. Fujita, and J. Yang. Spectral transforms for
large boolean functions with applications to technology mapping. Formal Methods in
System Design, 10(2â€“3):137â€“148, 1997.

[DB98]

[DH96]

Rolf Drechsler and Bernd Becker. Binary Decision Diagrams: Theory and Implementa-
tion. Springer, 1998.

Christoph DÂ¨urr and Peter HÃ¸yer. A quantum algorithm for ï¬nding the minimum. Technical
Report quant-ph/9607014, arXiv, 1996.

[DHHM06] Christoph DÂ¨urr, Mark Heiligman, Peter HÃ¸yer, and Mehdi Mhalla. Quantum query com-

plexity of some graph problems. SIAM Journal on Computing, 35(6):1310â€“1328, 2006.

[FS90]

S. J. Friedman and K. J. Supowit. Finding the optimal variable ordering for binary decision
diagrams. IEEE Transactions on Computers, 39(5):710â€“713, May 1990.

23

[GJ79]

Michael R. Garey and David S. Johnson. COMPUTERS AND INTRACTABILITY â€” A
Guide to the Theory of NP-Completeness. W. H. Freeman and Company, New York, 2
edition, 1979.

[GLM08]

Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access mem-
ory. Phys. Rev. Lett., 100:160501, Apr 2008.

[Gro96]

[HC92]

[Hea93]

[HI02]

[HM94]

[HM00]

Lov K. Grover. A fast quantum mechanical algorithm for database search. In Proceedings
of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pages 212â€“219,
1996.

Heh-Tyan Liaw and Chen-Shang Lin. On the obdd-representation of general boolean
functions. IEEE Transactions on Computers, 41(6):661â€“664, June 1992.

Mark Heap. On the exact ordered binary decision diagram size of totally symmetric func-
tions. Journal of Electronic Testing, 4(2):191â€“195, 1993.

Takashi Horiyama and Toshihide Ibaraki.
knowledge-bases. Artif. Intell., 136(2):189â€“213, 2002.

Ordered binary decision diagrams as

M. A. Heap and M. R. Mercer. Least upper bounds on obdd sizes. IEEE Transactions on
Computers, 43(6):764â€“767, June 1994.

L. Heinrich-Litan and P. Molitor. Least upper bounds for the size of obdds using symmetry
properties. IEEE Transactions on Computers, 49(4):360â€“368, April 2000.

[HTKY97] K. Hosaka, Y. Takenaga, T. Kaneda, and S. Yajima. Size of ordered binary decision di-
agrams representing threshold functions. Theoretical Computer Science, 180(1):47 â€“ 60,
1997.

[HY97]

[INY98]

Takashi Horiyama and Shuzo Yajima. Exponential lower bounds on the size of obdds
representing integer divistion. In Proceedings of the 8th International Symposium on Al-
gorithms and Computation, (ISAAC â€™97), Singapore, December 17-19, 1997, Proceedings,
Lecture Notes in Computer Science, pages 163â€“172. Springer, 1997.

Kazuo Iwama, Mitsushi Nouzoe, and Shuzo Yajima. Optimizing obdds is still intractable
for monotone functions. In Proceedings of the 23rd International Symposium on Mathe-
matical Foundations of Computer Science (MFCSâ€™98), volume 1450 of Lecture Notes in
Computer Science, pages 625â€“635. Springer, 1998.

[KLM07]

Phillip Kaye, Raymond Laï¬‚amme, and Michele Mosca. An Introduction to Quantum
Computing. Oxford University Press, 2007.

[Knu09]

Donald E. Knuth. The Art of Computer Programming, Volume 4, Fascicle 1: Bitwise
Tricks & Techniques; Binary Decision Diagrams. Addison-Wesley Professional, 1 edition,
March 2009.

[KSV02]

Alexei Yu. Kitaev, Alexander H. Shen, and Mikhail N. Vyalyi. Classical and Quantum
Computation, volume 47 of Graduate Studies in Mathematics. AMS, 2002.

[Lee59]

C. Y. Lee. Representation of switching circuits by binary-decision programs. The Bell
System Technical Journal, 38(4):985â€“999, July 1959.

[LGM18]

FrancÂ¸ois Le Gall and FrÂ´edÂ´eric Magniez. Sublinear-time quantum computation of the di-
ameter in congest networks. In Proceedings of the 2018 ACM Symposium on Principles
of Distributed Computing, PODC â€™18, pages 337â€“346, New York, NY, USA, 2018. ACM.

24

[Min93]

[Min11]

[MS94]

[MT98]

[NC00]

[Sie02a]

[Sie02b]

[SIT95]

[STY94]

[THY96]

[TI94]

[TY00]

[Weg00]

Shin-ichi Minato. Zero-suppressed BDDs for set manipulation in combinatorial problems.
In Proceedins of the 30th ACM/IEEE Design Automation Conference, pages 272â€“277,
June 1993.

Shin-ichi Minato. Ï€DD: A new decision diagram for efï¬cient problem solving in permu-
tation space. In Proceedings of the 14th International Conference on Theory and Appli-
cations of Satisï¬ability Testing (SAT 2011), volume 6695 of Lecture Notes in Computer
Science, pages 90â€“104, 2011.

Christoph Meinel and Anna SlobodovÂ´a. On the complexity of constructing optimal or-
In Proceedings of 19th Mathematical Foundations of
dered binary decision diagrams.
Computer Science 1994, pages 515â€“524, Berlin, Heidelberg, 1994. Springer Berlin Hei-
delberg.

Christoph Meinel and Thorsten Theobald. Algorithms and Data Structures in VLSI De-
sign: OBDD - Foundations and Applications. Springer, 1998.

Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Informa-
tion. Cambridge University Press, 2000.

Detlef Sieling. The complexity of minimizing and learning OBDDs and FBDDs. Discrete
Applied Mathematics, 122(1):263 â€“ 282, 2002.

Detlef Sieling. The nonapproximability of OBDD minimization. Information and Com-
putation, 172(2):103 â€“ 138, 2002.

Kyoko Sekine, Hiroshi Imai, and Seiichiro Tani. Computing the Tutte polynomial of
a graph of moderate size. In Proceedings of the Sixth International Symposium on Algo-
rithms and Computation (ISAAC â€™95), volume 1004 of Lecture Notes in Computer Science,
pages 224â€“233. Springer, 1995.

Hiroshi Sawada, Yasuhiko Takenaga, and Shuzo Yajima. On the computational power of
binary decision diagrams. IEICE Trans. Info. & Syst., D, 77(6):611â€“618, 1994.

Seiichiro Tani, Kiyoharu Hamaguchi, and Shuzo Yajima. The complexity of the optimal
variable ordering problems of a shared binary decision diagram. IEICE Transactions on
Information and Systems, E79-D(4):271â€“281, 1996. (Conference version is in Proceed-
ings of the Fourth International Symposium on Algorithms and Computation (ISAACâ€™93),
vol. 2906, pp.389â€“398, Lecture Notes in Computer Science, Springer, 1993).

Seiichiro Tani and Hiroshi Imai. A reordering operation for an ordered binary decision
diagram and an extended framework for combinatorics of graphs. In Proceedings of the
Fifth International Symposium on Algorithms and Computation (ISAACâ€™94), volume 834
of Lecture Notes in Computer Science, pages 575â€“583. Springer-Verlag, 1994.

Yasuhiko Takenaga and Shuzo Yajima. Hardness of identifying the minimum ordered
binary decision diagram. Discrete Applied Mathematics, 107(1-3):191â€“201, 2000.

Ingo Wegener. Branching Programs and Binary Decision Diagrams. SIAM Monographs
on Discrete Mathematics and Applications. SIAM, 2000.

25

