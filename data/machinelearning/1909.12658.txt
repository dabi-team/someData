Quantum Algorithm for Finding the Optimal Variable Ordering
for Binary Decision Diagrams

Seiichiro Tani
NTT Communication Science Laboratories, NTT Corporation.
seiichiro.tani.cs@hco.ntt.co.jp

Abstract

An ordered binary decision diagram (OBDD) is a directed acyclic graph that represents a Boolean
function. OBDDs are also known as special cases of oblivious read-once branching programs in the
ﬁeld of complexity theory. Since OBDDs have many nice properties as data structures, they have
been extensively studied for decades in both theoretical and practical ﬁelds, such as VLSI design,
formal veriﬁcation, machine learning, and combinatorial problems. Arguably, the most crucial prob-
lem in using OBDDs is that they may vary exponentially in size depending on their variable ordering
(i.e., the order in which the variables are to read) when they represent the same function. Indeed,
it is NP hard to ﬁnd an optimal variable ordering that minimizes an OBDD for a given function.
Hence, numerous studies have sought heuristics to ﬁnd an optimal variable ordering. From practi-
cal as well as theoretical points of view, it is also important to seek algorithms that output optimal
solutions with lower (exponential) time complexity than trivial brute-force algorithms do. Friedman
and Supowit provided a clever deterministic algorithm with time/space complexity O∗(3n), where n
is the number of variables of the function, which is much better than the trivial brute-force bound
O∗(n!2n). This paper shows that a further speedup is possible with quantum computers by demon-
strating the existence of a quantum algorithm that produces a minimum OBDD together with the
corresponding variable ordering in O∗(2.77286n) time and space with an exponentially small error.
Moreover, this algorithm can be adapted to constructing other minimum decision diagrams such as
zero-suppressed BDDs, which provide compact representations of sparse sets and are often used in
the ﬁeld of discrete optimization and enumeration.

0
2
0
2

b
e
F
7
1

]
h
p
-
t
n
a
u
q
[

2
v
8
5
6
2
1
.
9
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
1 Introduction

1.1 Background

Ordered binary decision diagrams. The ordered binary decision diagram (OBDD) [Lee59, Ake78]
is one of the data structures that have been most often used for decades to represent Boolean functions in
practical situations, such as VLSI design, formal veriﬁcation, optimization of combinatorial problems,
and machine learning, and it has been extensively studied from both theoretical and practical standpoints
(see standard textbooks and surveys, e.g., Refs. [Bry92, MT98, DB98, Weg00, Knu09, Bry18]). More-
over, many variants of OBDDs have been invented to more efﬁciently represent data with properties
observed frequently in speciﬁc applications (e.g., Refs. [Min93, BC95, BFG+97, CMZ+97, Min11]).
More technically speaking, OBDDs are directed acyclic graphs that represent Boolean functions and
also known as special cases of oblivious read-once branching programs in the ﬁeld of complexity the-
ory. The reason for OBDDs’ popularity lies in their nice properties — they can be uniquely determined
up to the isomorphism for each function once variable ordering (i.e., the order in which to read the vari-
ables) is ﬁxed and, thanks to this property, the equivalence of functions can be checked by just testing
the isomorphism between the OBDDs representing the functions. In addition, binary operations such
as AND and OR between two functions can be performed efﬁciently over the OBDDs representing
those functions [Bry86]. Since these properties are essential in many applications, OBDDs have gath-
ered much attention from various research ﬁelds. To enjoy these nice properties, however, we actually
need to address a crucial problem, which is that OBDDs may vary exponentially in size depending on
their variable ordering. For instance, a Boolean function f (x1, . . . , x2n) = x1x2 + x3x4 + · · · + x2n−1x2n
has the a (2n + 2)-sized OBDD for the ordering (x1, . . . , x2n) and a 2n+1-sized OBDD for the ordering
(x1, x3, . . . , x2n−1, x2, x4, . . . , x2n) [MT98, Sec. 8.1] (see Fig. 1 for the case where n = 6). This is not a
rare phenomenon; it could happen in many concrete functions that one encounters. Thus, since the early
stages of OBDD research, one of the most central problems has been how to ﬁnd an optimal variable
ordering, i.e., one that minimizes OBDDs. Since there are n! permutations over n variables, the brute-
force search requires at least n! = 2Ω(n log n) time to ﬁnd an optimal variable ordering. Indeed, ﬁnding an
optimal variable ordering for a given function is an NP hard problem.

To tackle this high complexity, many heuristics have been proposed to ﬁnd an optimal variable
ordering or a relatively good one. These heuristics work well for Boolean functions appearing in speciﬁc
applications since they are based on very insightful observations, but they do not guarantee a worst-case
time complexity lower than that achievable with the brute-force search. The only algorithm with a much
lower worst-case time complexity bound, O∗(3n) time,1 than the brute-force bound O∗(n!2n) for all
Boolean functions with n variables was provided by Friedman and Supowit [FS90], and that was almost
thirty years ago!

In practice, it is often too costly to construct a minimum OBDD and the optimal variable order-
ing may change as the function changes during a procedure, say, by imposing additional constraints.
Nevertheless, theoretically sound methods for ﬁnding an optimal variable ordering are worth studying
for several reasons, such as to judge the optimization quality of heuristics and to be able to apply such
methods at least to parts of the OBDDs within a heuristics procedure [MT98, Sec. 9.22].

Quantum Speedups of Dynamic Programming Grover’s quantum search algorithm [Gro96] and its
variants achieve quadratic speedups over any classical algorithm for the very fundamental problem of
exhaustive search. Thus, one of the merits of the quantum search is its wide applicability. However, it
does not immediately mean quantum speedups for all problems to which the quantum search is appli-
cable, since there may exist better classical algorithms than simple exhaustive search. Indeed, quantum
search for an optimal variable ordering of the OBDD from among n! candidates takes approximately
√
2 n log n time, while the best classical algorithm takes only O∗(3n) = O∗(2(log2 3)n). These clas-
sical algorithms often employ powerful algorithmic techniques such as dynamic programming, divide-

n! ≈ 2

1

1O∗(·) hides a polynomial factor. This hidden polynomial factor can be improved [BW96, Knu09].

1

and-conquer, and branch-and-bound. One typical strategy to gain quantum speedups would be to ﬁnd
exhaustive search (often implicitly) performed within such classical algorithms and apply the quantum
search to that part. For instance, D¨urr et al. [DHHM06] provided quantum algorithms for some graph
problems, among which the quantum algorithm for the single-source shortest path problem achieves a
quantum speedup by applying a variant of Grover’s search algorithm to select the cheapest border edge
in Dijkstra’s algorithm. However, applying the quantum search in this way does not work when the
number of states in a dynamic programming algorithm is much larger than the number of predecessors
of each state. For instance, the Traveling Salesman Problem (TSP) can be solved in O∗(2n) time by a
classical dynamic programming algorithm, but locally applying the quantum search can attain at most
a polynomial-factor improvement. Recently, Ambainis et al. [ABI+19] has introduced break-through
techniques to speed up dynamic programming approaches. They provide quantum algorithms that solve
a variety of vertex ordering problems on graphs in O∗(1.817n) time, graph bandwidth in O∗(2.946n) time,
and TSP and minimum set cover in O∗(1, 728n) time, where n is the number of vertices in the graphs.

1.2 Our Results

In this paper, we show that quantum speedup is possible for the problem of ﬁnding an optimal variable
ordering of the OBDD for a given function. This is the ﬁrst quantum speedup for the OBDD-related
problems. Our algorithms assume the quantum random access memory (QRAM) model [GLM08],
which is commonly used in the literature concerned with quantum algorithms. In the model, one can
read contents from or write them into quantum memory in a superposition.

We provide our main result in the following theorem.

Theorem 1 (Informal) There exists a quantum algorithm that, for a function f : {0, 1}n → {0, 1} given
as its truth table, produces a minimum OBDD representing f together with the corresponding variable
ordering in O∗(γn) time and space with an exponentially small error with respect to n, where the constant
γ is at most 2.77286. Moreover, the OBDD produced by our algorithm is always a valid one for f ,
although it is not minimum with an exponentially small probability.

This improves upon the classical best bound O∗(3n) [FS90] on time/space complexity. The classical
algorithm achieving this bound is a deterministic one. However, there are no randomized algorithms that
compute an optimal variable ordering in asymptotically less time complexity as far as we know.

It may seem somewhat restricted to assume that the function f is given as its truth table, since
there are other common representations of Boolean functions such as DNFs, CNFs, Boolean circuits
and OBDDs. However, this is not the case. Our algorithm actually works in more general settings
where the input function f is given as any representation such that the value of f on any speciﬁed
assignment can be computed over the representation in polynomial time in n, such as polynomial-size
DNFs/CNFs/circuits and OBDDs of any size. This is because, in such cases, the truth table of f can be
prepared in O∗(2n) time and the minimum OBDD is computable from that truth table with our algorithm.
We restate Theorem 1 in a more general form as follows.

Corollary 2 Let R( f ) be any representation of a Boolean function f with n variables such that the value
of f (x) on any given assignment x ∈ {0, 1}n can be computed on R( f ) in polynomial time with respect to
n. Then, there exists a quantum algorithm that, for a function f : {0, 1}n → {0, 1} given as R( f ), produces
a minimum OBDD representing f together with the corresponding variable ordering in O∗(γn) time and
space with an exponentially small error with respect to n, where the constant γ is at most 2.77286.
Possible representations as R( f ) are polynomial-size DNFs/CNFs/circuits and OBDDs of any size for
function f .

There are many variants of OBDDs, among which the zero-suppressed BDDs (ZDDs or ZBDDs)
introduced by Minato [Min93] have been shown to be very powerful in dealing with combinatorial
problems (see Knuth’s famous textbook [Knu09] for how to apply ZDDs to such problems). With
just two-line modiﬁcations, our algorithm can construct a minimum ZDD with the same time/space

2

Figure 1: The OBDDs represent the function f (x1, x2, x3, x4, x5, x6) = x1x2 + x3x4 + x5x6 under two
variable orderings: (x1, x2, x3, x4, x5, x6) (left) and (x1, x3, x5, x2, x4, x6) (right), where the solid and dot-
ted arcs express 1-edges and 0-edges, respectively, and the terminal nodes for true and false are labeled
In general, the function f (x1, . . . , x2n) = x1x2 + x3x4 + · · · + x2n−1x2n
with T and F, respectively.
has the a (2n + 2)-sized OBDD for the ordering (x1, . . . , x2n) and a 2n+1-sized OBDD for the ordering
(x1, x3, . . . , x2n−1, x2, x4, . . . , x2n). As deﬁned in Sec. 2.2 , for each OBDD, {V0, . . . , V6} is the partition
of the node set, the top node is identiﬁed with r, and the bottom two nodes are identiﬁed with t and f. A
more detailed explanation of these OBDDs is provided in Example 1 in Sec. 2.2 .

complexity. We believe that similar speedups are possible for many other variants of OBDDs (adapting
our algorithm to multiterminal BDDs (MTBDDs) [BFG+97, CMZ+97] is almost trivial).

1.3 Technical Outline

The ﬁrst step to take is to somehow adapt the dynamic programming approach of the classical algo-
rithm [FS90] (called, FS) to the framework provided by Ambainis et al. [ABI+19]. Consider a Boolean
function f over n variables: x1, . . . , xn. Intuitively, FS determines the variable ordering of the minimum
OBDD for f by performing dynamic programming from the variable to be read last toward that to be
read ﬁrst. More concretely, let (xπ[1], . . . , xπ[n]) be the variable ordering from the one read last (xπ[1])
to the one read ﬁrst (xπ[n]), where π = (π[1], . . . , π[n]) is a permutation over [n] := {1, . . . , n}. For
k = 1, . . . , n in this order, and for every subset K ⊆ [n] of cardinality k, the algorithm FS computes
a lower bound on OBDD size when {π[1], . . . , π[k]} = K from the lower bounds on OBDD size when
{π[1], . . . , π[k − 1]} = K \ {h} for all h ∈ K. Thus, by thinking of each node z ∈ {0, 1}n of weight k in
a Boolean hypercube as the characteristic vector of K, the algorithm FS can be seen as solving a kind
of shortest path problem on a Boolean hypercube. Hence, Ambainis et al.’s framework seems appli-
cable. Their results depend on the property that a large problem can be divided into the same kind of
subproblems or, in other words, symmetric subproblems in the sense that they can be solved with the
same algorithm. This property naturally holds in many graph problems. In our case, ﬁrstly, it is unclear
whether the problem can be divided into subproblems. Secondly, subproblems would be to optimize
the ordering of variable starting from the middle variable or even from the opposite end, i.e., from the
variable to be read ﬁrst, toward the one to be read last. Such subproblems cannot be solved with the
algorithm FS, and, in particular, optimizing in the latter case essentially requires the equivalence check
of subfunctions of f , which is very costly.

Our technical contribution is to ﬁnd, by carefully observing the unique properties of OBDDs, that
it is actually possible to even recursively divide the original problem into asymmetric subproblems, to
generalize the algorithm FS so that it can solve the subproblems, and to use the quantum minimum

3

𝑥"𝑥#𝑥$𝑥%𝑥&𝑥’TF𝑥"𝑥$𝑥$𝑥&𝑥&𝑥&𝑥&𝑥#𝑥#𝑥#𝑥#𝑥%𝑥%𝑥’TF𝑉)𝑉’𝑉&𝑉%𝑉$𝑉#𝑉"ﬁnding algorithm in order to efﬁciently select the subproblems that essentially contribute to the optimal
variable ordering.

More concretely, we show that, for any k ∈ [n], it is possible to divide the problem into two collec-

tions of subproblems as follows: for all K ⊆ [n] of cardinality k,

• problems of ﬁnding the ordering (π[1], . . . , π[k]) that minimizes the size of the bottom k-layers of

the corresponding OBDD, assuming that the set {π[1], . . . , π[k]} equals K,

• problems of ﬁnding the ordering (π[k + 1], . . . , π[n]) that minimizes the size of the upper (n − k)-
layers of the corresponding OBDD, assuming that the set {π[k + 1], . . . , π[n]} equals [n] \ K.

Then, taking the minimum of the OBDD size over all K and k with the quantum minimum ﬁnding pro-
vides a minimum OBDD and the corresponding variable ordering. To obtain a better bound, a straight-
forward strategy is to consider m division points (0 < k1 < · · · < km < n) and optimize each of the
(m + 1) suborderings (π[1], . . . , π[k1]), (π[k1 + 1], . . . , π[k2]), . . . , (π[km + 1], . . . , π[n]). However, this
makes subproblems even more asymmetric. To deal with this asymmetry, we generalize the algorithm
FS so that it can cover all the subproblems. Then, by applying it to each subproblem, we optimize
the suborderings with the quantum minimum ﬁnding so that the OBDD size is minimized. To improve
the complexity bound further, a simple idea would be to replace the generalized FS with the quantum
algorithm we have just obtained. However, the latter algorithm works only for the original problem.
Thus, we generalize the quantum algorithm so that it can be applied to the asymmetric subproblems. By
repeating this composition and generalization, we obtain the ﬁnal algorithm.

1.4 Related Work

The studies related to minimizing OBDDs are so numerous that we cannot cover all of them. We thus
pick up some of purely theoretical work.

Meinel and Slobodov´a [MS94] proved that it is NP hard to construct an optimal OBDD for a Boolean
function given by a logical circuit, a DNF, a CNF, or an OBDD, even if the optimal OBDD is of constant
size. Tani, Hamaguchi and Yajima [THY96] proved that it is NP hard to improve the variable ordering
(and thus, to ﬁnd an optimal variable ordering) for a given multi-rooted OBDD, where the NP hardness
is proved by a reduction from Optimal Linear Arrangement [GJ79]. Bollig and Wegener [BW96] ﬁnally
proved the NP hardness for a given single-rooted OBDD. This is still true if the input function is re-
stricted to monotone functions [INY98]. Minimizing the width of an OBDD is also NP hard [Bol16].
As for approximation hardness, Sieling [Sie02a, Sie02b] proved that if there exists a polynomial-time
approximation scheme for computing the size of the minimum OBDD for a given OBDD, it then holds
that NP = P.

It would be nice if, for every function, there exists at least one variable ordering under which the
OBDD for the function is of a size bounded by a polynomial. As one may expect, this is not the case.
It can be proved by a counting argument that there exists a function for which the OBDD size grows
exponentially in the number of variables under any variable ordering [Lee59, HC92, HM94]. Examples
of such functions include practical ones such as the multiplication function [Bry91], a threshold func-
tion [HTKY97], and the division function [HY97]. The sizes of OBDDs for several classes of Boolean
functions are investigated [STY94, Hea93, HM00]. The OBDD size is also studied from the viewpoint
of computational learning and knowledge-bases [TY00, HI02].

In applying OBDDs to graph problems, it is possible to ﬁnd variable orderings for which OBDD size
is nontrivially upper-bounded in terms of certain measures characterizing graph structures [TI94, SIT95].
A similar concept was discoved for ZDDs [Min93] by Knuth [Knu09]. This concept is now called the
frontier method, and lots of work is based on it.

4

2 Preliminaries

2.1 Basic Terminology

Let N, Z and R be the sets of natural numbers, integers, and real numbers, respectively. For each n ∈ N,
let [n] be the set {1, . . . , n}, and Sn be the permutation group over [n]. We may denote a singleton set {k}
by k for notational simplicity if it is clear from the context; for instance, I \ {k} may be denoted by I \ k,
if we know I is a set. For any subset I ⊆ [n], let Πn(I) be the set of π ∈ Sn such that the ﬁrst |I| members
{π[1], . . . , π[|I|]} constitutes I, i.e.,

Πn(I) := {π ∈ Sn : {π[1], . . . , π[|I|]} = I} ⊆ Sn.

For simplicity, we omit the subscript n and write Π(I). More generally, for any two disjoint subsets
I, J ⊆ [n], let

Πn((cid:104)I, J(cid:105)) := {π ∈ Sn : {π[1], . . . , π[|I|]} = I, {π[|I| + 1], . . . , π[|I| + |J|]} = J} ⊆ Sn.

For any disjoint subsets I1, . . . , Im ⊆ [n] for m ∈ [n], Πn((cid:104)I1, . . . , Im(cid:105)) is deﬁned similarly. For simplicity,
we may denote (cid:104)I(cid:105) by I, if it is clear from the context.

We denote the union operation over disjoint sets by (cid:116) (instead of ∪) when we emphasize the disjoint-
ness of the sets. For example, the union of the disjoint sets {1, 2} and {5, 6} is denoted by {1, 2} (cid:116) {5, 6}.
For n Boolean variables x1, . . . , xn, any set I ⊆ [n], and any vector b = (b1, . . . , b|I|) ∈ {0, 1}|I|, xI
denotes the ordered set (x j1, . . . , x j|I|), where { j1, . . . , j|I|} = I and j1 < · · · < j|I|, and xI = b denotes
= bi for each i = [|I|]. For any Boolean function f : {0, 1}n → {0, 1} with variables x1, . . . , xn, we
x ji
denote by f |xI =b the function obtained by restricting f with xI = b. If I is a singleton set, say, I = {i}, we
may write xi and f |xi=b to mean x{i} and f |x{i}=b, respectively, for notational simplicity. We say that g is a
subfunction of f if g is equivalent to the function f |xI =b for some I ⊆ [n] and b ∈ {0, 1}|I|.

For any function g(n) in n, we use the notation O∗(g(n)) to hide a polynomial factor in n. For

instance, n32n and 2n are both in O∗(2n). We further denote X = O∗(Y) by X (cid:47) Y.

We use the following upper bound many times in this paper. For n ∈ N and k ∈ [n] ∪ {0}, it holds that
≤ 2n H(k/n), where H(·) represents the binary entropy function H(δ) := −δ log2 δ − (1 − δ) log2(1 − δ).

(cid:17)
(cid:16)n
k

2.2 Ordered Binary Decision Diagrams

This subsection brieﬂy introduces ordered binary decision diagrams (OBDDs). For interested readers,
see standard textbooks and survey papers (e.g., Refs. [Bry92, MT98, DB98, Weg00, Bry18]).

OBDDs are a special case of read-once oblivious branching programs in complexity-theoretic terms
that is, branching programs that satisfy the following conditions: each variable is read at most once on
each directed path from the root to a terminal node, and the orderings of variables to be read on all such
paths are consistent with a certain ﬁxed ordering.

In the following, we formally deﬁne OBDDs in graph-theoretic terms and provide several notations.
For any Boolean function f : {0, 1}n → {0, 1} over variables x1, . . . , xn and any permutation π ∈ Sn
(called a variable ordering), an OBDD B( f, π) is a directed acyclic graph G(V, E) deﬁned as follows:

1. The node set V is the union of two disjoint sets N and T of non-terminal nodes with out-degree
two and terminal nodes with out-degree zero, respectively, where T contains exactly two nodes:
T = {f, t}. The set N contains a unique source node r, called the root.

2. B( f, π) is a leveled graph with n + 1 levels. Namely, the node set can be partitioned into n subsets:
V := V0 (cid:116) V1 (cid:116) · · · (cid:116) Vn, where Vn = {r} and V0 = T = {t, f}, such that each directed edge (u, v) ∈ E
is in Vi × V j for a pair (i, j) ∈ [n] × ({0} (cid:116) [n − 1]) with i > j. For each i ∈ [n], subset Vi (called the
level i) is associated with the variable xπ[i], or alternatively, each node in Vi is labeled with xπ[i].2
For convenience, we deﬁne a map var : N → [n] such that if v ∈ Vi then var = π[i].

2In the standard deﬁnition, Vi is associated with the variable xπ[n−i]. Our deﬁnition follows the one given in [FS90] to avoid

complicated subscripts of variables.

5

Figure 2: Examples of a redundant node (left) and equivalent nodes (right), and their removal rules,
where the solid and dotted arcs express 1-edges and 0-edges, respectively,

3. The two edges emanating from every non-terminal node v are called the 0-edge and the 1-edge,
which are labeled with 0 and 1, respectively. For every u ∈ N, let u0 and u1 be the destinations of
the 0-edge and 1-edge of u, respectively.

4. Let F ( f ) be the set of all subfunctions of f . Deﬁne a bijective map F : V → F ( f ) as follows:
(a) F(r) = f for r ∈ Vn; (b) F(t) = true and F(f) = false for t, f ∈ V0; (c) For every u ∈ N
and b ∈ {0, 1}, F(ub) is the subfunction obtained from F(u) by substituting xvar(u) with b, i.e.,
F(ub) = F(u)|xvar(u)=b.

5. B( f, π) must be minimal in the sense that the following reduction rules cannot be applied. In other

words, B( f, π) is obtained by maximally applying the following rules (Fig. 2):

(a) if there exists a redundant node u ∈ N, then remove u and its outgoing edges, and redirect
all the incoming edges of u to u0, where a node u is redundant if u0 is the same node as u1.
(b) if there exist equivalent nodes {u, v} ⊂ N, then remove v (i.e., any one of them) and its
outgoing edges, and redirect all incoming edges of v to u, where u and v are equivalent if (1)
var(u) is equal to var(v), and (2) u0 and u1 are the same nodes as v0 and v1, respectively.

Example 1 For ease of understanding the above notations, let us consider the OBDD on the right side
in Fig. 1. The root r is the uppermost node labeled with x6. The ordering π is (1, 3, 5, 2, 4, 6). Every node
in Vi (i = 1, . . . , 6) is represented by a circle labeled with xπ[i]. For instance, V3 consists of all the four
nodes labeled with x5. For each node v ∈ V3, it holds that var(v) = 5. Let u be the left node labeled with
x3. Since the path from r to u consists of three edges labeled with 0, 1, and 0 in this order from the root
side, F(u) is represented as F(r)|x6=0,x4=1,x2=0 = f |x6=0,x4=1,x2=0 = x3.

For each j ∈ [n], Cost j( f, π) denotes the width at the level associated with the variable x j, namely,
the number of nodes in the level π−1[ j] (Fig. 3). For I ⊆ [n], let πI be a permutation π in Π(I) that
minimizes the number of nodes in level 1 to level |I|:

πI := arg min





|I|(cid:88)

j=1

Costπ[ j]( f, π) : π ∈ Π(I)





.

(1)

j=1 Costπ[ j]( f, π) = (cid:80)

Note that (cid:80)|I|
i∈I Costi( f, π) for π ∈ Π(I). More generally, for disjoint subsets
I1, . . . , Im ⊆ [n], π(cid:104)I1,...,Im(cid:105) is a permutation in Π((cid:104)I1, . . . , Im(cid:105)) that minimizes the number of the nodes in
level 1 to level |I1| + · · · + |Im| over all π ∈ Π((cid:104)I1, . . . , Im(cid:105)):

π(cid:104)I1,...,Im(cid:105) := arg min





|I1|+···+|Im|(cid:88)

j=1

Costπ[ j]( f, π) : π ∈ Π((cid:104)I1, . . . , Im(cid:105))





.

(2)

6

𝑥"𝑥"#$remove𝑥"#$𝑥"𝑥"𝑥"#$𝑥"#$remove𝑥"𝑥"#$𝑥"#$Redundant nodeEquivalent nodesFigure 3: Schematic expression of Cost j( f, π)

j=1

Costπ[ j]( f, π) = (cid:80)

Note that min (cid:80)|I1|+···+|Im|
i∈I1(cid:116)···(cid:116)Im Costi( f, π) for any π ∈ Π((cid:104)I1, . . . , Im(cid:105)). The follow-
ing well-known lemma captures the essential property of OBDDs. It states that the number of nodes at
level i ∈ [n] is constant over all π, provided that the two sets {π[1], . . . , π[i − 1]} and {π[i + 1], . . . , π[n]}
are ﬁxed.

Lemma 3 ([FS90]) For any non-empty subset I ⊆ [n] and any i ∈ I, there exists a constant c f such that,
for each π ∈ Π((cid:104)I \ {i}, {i}(cid:105)), Costπ[|I|]( f, π) ≡ Costi( f, π) = c f .

For convenience, we deﬁne shorthand for the minimums of the sums in Eqs. (1) and (2). For I(cid:48) ⊆
I ⊆ [n], MINCOSTI[I(cid:48)] is deﬁned as the number of nodes in the levels associated with variables indexed
by elements in I(cid:48) under permutation πI, namely, MINCOSTI[I(cid:48)] := (cid:80)
i∈I(cid:48) Costi( f, πI). More generally, for
disjoint subsets I1, . . . , Im ⊆ [n] and I(cid:48) ⊆ I1 (cid:116) · · · (cid:116) Im,

MINCOST(cid:104)I1,...,Im(cid:105)[I(cid:48)] :=

(cid:88)

i∈I(cid:48)

Costi( f, π(cid:104)I1,...,Im(cid:105)).

As a special case, we denote MINCOST(cid:104)I1,...,Im(cid:105)[I1 (cid:116) · · · (cid:116) Im] by MINCOST(cid:104)I1,...,Im(cid:105). We deﬁne MINCOST∅
as 0.

2.3 The Algorithm by Friedman and Supowit

This subsection reviews the algorithm by Friedman and Supowit [FS90]. We will generalize their idea
later and heavily use the generalized form in our quantum algorithm. Hereafter, we call their algorithm
FS.

2.3.1 Key Lemma and Data Structures

The following lemma is the basis of the dynamic programming approach used in the algorithm.

Lemma 4 For any non-empty subset I ⊆ [n] and any Boolean function f : {0, 1}n → {0, 1}, the following
holds: MINCOSTI = mink∈I

MINCOSTI\k + Costk( f, π(cid:104)I\k,k(cid:105))(cid:1) = mink∈I

MINCOST(cid:104)I\k,k(cid:105)

(cid:1) .

(cid:0)

(cid:0)

The proof is given in Appendix A.

Before sketching algorithm FS, we provide several deﬁnitions. For any I ⊆ [n], TABLEI is an array
Intuitively, for b ∈ {0, 1}n−|I|, the cell
with 2n−|I| cells each of which stores a non-negative integer.
TABLEI[b] stores (the pointer to) the unique node of B( f, πI) associated via F with function f |x[n]\I =b.
Hence, we may write TABLEI[x[n]\I = b] instead of TABLEI[b] to clearly indicate the value assigned to
each variable x j for j ∈ [n]\I. The purpose of TABLEI is to relate all subfunctions f |x[n]\I =b (b ∈ {0, 1}n−|I|)

7

aTF𝑥"𝑥"𝑥"𝑥"𝑓𝑉%𝜋𝑘=𝑗=Cost.%𝑓,𝜋Cost"𝑓,𝜋Figure 4: Schematic expression of Lemma 3: For any two permutations π, π(cid:48) ∈ Sn such that
{π[1], . . . , π[|I| − 1]} = {π(cid:48)[1], . . . , π(cid:48)[|I| − 1]} and π[|I|] = π(cid:48)[|I|], it holds that the number of nodes
labeled with xi is equal to that of nodes labeled with xi, where it is assumed that π[|I|] = π(cid:48)[|I|].

to the corresponding nodes of B( f, πI). We assume without loss of generality that the pointers to nodes
of B( f, πI) are non-negative integers and, in particular, those to the two terminal nodes corresponding to
false and true are the integers 0 and 1, respectively. Thus, TABLE∅ is merely the truth table of f .

Algorithm FS computes TABLEI together with πI, MINCOSTI, and another data structure, NODEI
for all I ⊆ [n], starting from TABLE∅ via dynamic programming. NODEI is the set of all triples of (the
pointers to) nodes, (u, u0, u1) ∈ N × (N (cid:116) T ) × (N (cid:116) T ), in B( f, πI), where var(u) = πI[|I|], and (u, u0) and
(u, u1) are the 0-edge and 1-edge of u, respectively. Thus, NODEI contains the structure of the subgraph of
B( f, πI) induced by V|I|. The purpose of the NODEI is to prevent the algorithm from duplicating existing
nodes, i.e., creating nodes associated with the same subfunctions as those with which the existing nodes
are associated. By the deﬁnition, NODE∅ is the empty set. We assume that NODEI is implemented with an
appropriate data structure, such as a balanced tree, so that the time complexity required for membership
testing and insertion is the order of logarithm in the number of triples stored in NODEI. An example of
TABLEI and NODEI is shown in Fig. 5.

More generally, for disjoint subset I1, . . . , Im ⊆ [n], TABLE(cid:104)I1,...,Im(cid:105) is an array with 2n−|I1(cid:116)···(cid:116)Im| cells
such that, for b ∈ {0, 1}n−|I1(cid:116)···(cid:116)Im|, TABLE(cid:104)I1,...,Im(cid:105)[b] stores the nodes of B( f, π(cid:104)I1,...,Im(cid:105)) associated with the
function f |x[n]\I1(cid:116)···(cid:116)Im
=b. NODE(cid:104)I1,...,Im(cid:105) is deﬁned similarly for B( f, π(cid:104)I1,...,Im(cid:105)). For simplicity, we hereafter
denote by F S((cid:104)I1, . . . , Im(cid:105)) the quadruplet (π(cid:104)I1,...,Im(cid:105), MINCOST(cid:104)I1,...,Im(cid:105), TABLE(cid:104)I1,...,Im(cid:105), NODE(cid:104)I1,...,Im(cid:105)).

2.3.2 Sketch of Algorithm FS [FS90]

Algorithm FS performs the following operations for k = 1, . . . , n in this order. For each k-element
subset I ⊆ [n], compute F S((cid:104)I \ i, i(cid:105)) from F S((cid:104)I \ i(cid:105)) for each i ∈ I in the manner described later (note
that, since the cardinality of the set I \ i is k − 1, F S((cid:104)I \ i(cid:105)) has already been computed). Then set
F S(I) ←− F S((cid:104)I \ i∗, i∗(cid:105)), where i∗ is the index i ∈ I that minimizes MINCOST(cid:104)I\i,i(cid:105), implying that πI is
π(cid:104)I\i∗,i∗(cid:105). This is justiﬁed by Lemma 4. A schematic view of the algorithm is shown in Fig. 6.

To compute F S((cid:104)I \ i, i(cid:105)) from F S((cid:104)I \ i(cid:105)), do the following. First set NODE(cid:104)I\i,i(cid:105) ← ∅ and

MINCOST(cid:104)I\i,i(cid:105) ← MINCOSTI\i as their initial values. Then, for each b ∈ {0, 1}n−|I|, set

u0 ← TABLEI\i[x[n]\I = b, xi = 0],

u1 ← TABLEI\i[x[n]\I = b, xi = 1].

If u0 = u1, then store u0 in TABLE(cid:104)I\i,i(cid:105)[b]. Otherwise, test whether (u, u0, u1) for some u is a mem-
ber of NODEI\i. If it is, store u in the TABLE(cid:104)I\i,i(cid:105)[b]; otherwise create a new triple (u(cid:48), u0, u1), insert
it to NODE(cid:104)I\i,i(cid:105) and increment MINCOST(cid:104)I\i,i(cid:105). Since u(cid:48) is the pointer to the new node, u(cid:48) must be dif-
ferent from any pointer already included in NODE(cid:104)I\i,i(cid:105) and from any pointer to a node in V1 (cid:116) · · · (cid:116)

8

aTF𝑥"𝑥"𝑥"𝑥"𝑓Cost"𝑓,𝜋=Cost"𝑓,𝜋+𝜋1𝜋𝐼−1⋮𝜋𝐼𝜋𝐼+1⋮𝜋𝑛aTF𝑥"𝑥"𝑥"𝑥"𝑓𝜋+1𝜋+𝐼−1⋮𝜋+𝐼𝜋+𝐼+1⋮𝜋+𝑛𝐼𝐼𝑖==𝑖Figure 5: An example of data structure used in Algorithm FS. TABLEI and NODEI for the OBDD (rhs)
are shown, where the pointers (integers) to the nodes labeled with x1, x3, x5 are each shown at the top-left
positions of the nodes.

Vk−1 in B( f, π(cid:104)I\i(cid:105)), where k = |I|. Such u(cid:48) can be easily chosen by setting u(cid:48) to two plus the value
of MINCOST(cid:104)I\i,i(cid:105) before the increment, since the MINCOST(cid:104)I\i,i(cid:105) is exactly the number of triples in
NODE(cid:104)I\i,i(cid:105) plus |V1 (cid:116) · · · (cid:116) Vk−1|, and the numbers 0 and 1 are reserved for the terminal nodes. We
call the above procedure table folding with respect to xi, because it halves the size of TABLE(cid:104)I\i(cid:105). We
also mean it by “folding TABLE(cid:104)I\i(cid:105) with respect to xi”.

The complexity analysis is fairly simple. For each k, we need to compute F S(I) for

possible
I’s with |I| = k. For each I, it takes O∗(2n−k) time since the the size of TABLEI\i is 2n−k+1 and each
(cid:17) = 3n, up to a
operation to NODEI\i takes a polynomial time in n. Thus, the total time is (cid:80)n
polynomial factor. The point is that computing each F S(I) takes time linear to the size of TABLEI\i up
to a polynomial factor. The space required by Algorithm FS during the process for k is dominated by
that for TABLEI, TABLEI\i and NODEI for all I and i ∈ I, which is O∗ (cid:16)
. The space complexity is
(cid:17)(cid:17) = O∗(3n).
thus O∗ (cid:16)

maxk∈{0}∪[n] 2n−k(cid:16)n

k=0 2n−k(cid:16)n

(cid:17)(cid:17)
2n−k(cid:16)n

k

k

k

(cid:17)
(cid:16)n
k

Theorem 5 (Friedman and Supowit [FS90]) Suppose that the truth table of
given as input. Algorithm FS produces F S([n]) in O∗(3n) time and space.

f : {0, 1}n → {0, 1} is

2.4 Quantum Computation

We assume that readers have a basic knowledge of quantum computing (e.g., Refs. [NC00, KSV02,
KLM07]). We provide only a lemma used to obtain our results.

The quantum search algorithm discovered by Grover [Gro96] has been generalized in many ways.
For instance, Buhrman et al. [BCdWZ99] provided a small error version of quantum search, while D¨urr

9

[𝑛]−𝐼𝑥’𝑥(𝑥(𝑥)𝑥)𝑥)𝑥)𝑥*𝑥*𝑥*𝑥*𝑥+𝑥+𝑥,TF𝐼013425678𝑢𝑢7𝑢𝟏531601721841NODE=𝑥’𝑥(𝑥)pointer00000012010301141006101711051118Table=𝐼={1,3,5}𝜋==(1,3,5,2,4,6)Figure 6: Schematic view of Friedman-Supowit Algorithm. The algorithm goes from the left to the
(cid:17)
(cid:16)n
dots, each of which corresponds to a subset I ⊆ [n]
right. On the vertical line indicated by k, there are
k
of size k (or, more strictly speaking, F S(I)). FS (I) is computed from the collection of F S((cid:104)I \ i(cid:105)) over
all i ∈ I, which are arranged as dots on the line indicated by k − 1 and have already been computed.

and Høyer [DH96] devised a quantum algorithm that ﬁnds the minimum of a function. By combining
these two algorithms, we obtain the following lemma (an adaptation of Corollary 2.3 in Ref. [LGM18]).

Lemma 6 (Quantum Minimum Finding [DH96, BCdWZ99, LGM18]) For every ε > 0 there exists
a quantum algorithm that, for a function f : [N] → Z given as an oracle, ﬁnd an element x ∈ [N] at
which f (x) achieves the minimum, with error probability at most ε by making O(
N log(1/ε)) queries.

(cid:112)

In this paper, the search space N is exponentially large in n and we are interested in exponential com-
plexities, ignoring polynomial factors in them. We can thus safely assume ε = 1/2p(n) for a polynomial
p(n), so that the overhead is polynomially bounded. Since our algorithms use Lemma 6 constant times,
their overall error probabilities are exponentially small. In the following proofs, we thus assume that ε
is exponentially small whenever we use Lemma 6, and do not explicitly analyze the error probability for
simplicity.

Our algorithms assume that the quantum random access memory (QRAM) model [GLM08], which
is commonly used in the literature when considering quantum algorithms. In the model, one can read
contents from or write them into quantum memory in a superposition.

3 Quantum Algorithm with Divide-and-Conquer

We generalize Lemma 4 and Theorem 5 and use them in our quantum algorithm.
Lemma 7 For any disjoint subsets I1, . . . , Im, J ⊆ [n] with J (cid:44) ∅ and any Boolean function f : {0, 1}n →
{0, 1}, the following holds:

MINCOST(cid:104)I1,...,Im,J(cid:105) = min
k∈J
= min
k∈J

(cid:0)

(cid:0)

MINCOST(cid:104)I1,...,Im,J\{k}(cid:105) + Costk( f, π(cid:104)I1,...,Im,J\{k},{k}(cid:105))(cid:1)
(cid:1) .

MINCOST(cid:104)I1,...,Im,J\{k},{k}(cid:105)

The proof of this lemma is very similar to that of Lemma 4 and deferred to Appendix A. Based on
Lemma 7, we generalize Theorem 5 to obtain algorithm FS∗ (its pseudo code is given in Appendix D).
Recall that F S((cid:104)I1, . . . , Im(cid:105)) denotes the quadruplet (π(cid:104)I1,...,Im(cid:105), MINCOST(cid:104)I1,...,Im(cid:105), TABLE(cid:104)I1,...,Im(cid:105), NODE(cid:104)I1,...,Im(cid:105)).
A schematic view of FS∗ is shown in Fig. 7.

10

∅"Each dot corresponds to a subset #⊂"of size %%%−1(∗=argmin1mincost6⟨8∖:,⟩:0">?=>⟨?∖1∗,⟩1∗Figure 7: Schematic view of FS∗. This view corresponds to the case where m = 1 and J ⊂ [n] \ I in
Lemma 8. The shaded area is the one that FS∗ sweeps to produce F S((cid:104)I, J(cid:105)).

Lemma 8 (Classical Composition Lemma) For disjoint subsets I1, . . . , Im, J ⊆ [n] with J (cid:44) ∅, there
exists a deterministic algorithm FS∗ that produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)) for an un-
derlying function f : {0, 1}n → {0, 1} in O∗ (cid:16)
2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · 3|J|(cid:17)
time and space. More generally, for each
k ∈ [|J|], the algorithm produces the set {F S((cid:104)I1, . . . , Im, K(cid:105)) : K ⊆ J, |K| = k} from F S((cid:104)I1, . . . , Im(cid:105)) in
O∗ (cid:16)

2n−|I1(cid:116)···(cid:116)Im(cid:116)J| (cid:80)k

time and space.

(cid:17)(cid:17)
j=0 2|J|− j(cid:16)|J|

j

Note that if I1 (cid:116) · · · (cid:116) Im = ∅ and J = [n], then we obtain Theorem 5.
Proof. We focuses on the simplest case of m = 1, for which our goal is to show an algorithm that
produces F S((cid:104)I, J(cid:105)) from F S(I). It is straightforward to generalize the proof to the case of m ≥ 2.
Starting from F S(I), the algorithm ﬁrst folds TABLEI with respect to each variable in {x j : j ∈ J} to
obtain F S((cid:104)I, j(cid:105)) for every j ∈ J, then fold TABLE(cid:104)I, j1(cid:105) with respect to x j2 and TABLE(cid:104)I, j2(cid:105) with respect
to x j1 to obtain F S((cid:104)I, { j1, j2}(cid:105)) by taking the minimum of MINCOST(cid:104)I, j1, j2(cid:105) and MINCOST(cid:104)I, j2, j1(cid:105) for
every j1, j2 ∈ J, and repeat this to ﬁnally obtain F S((cid:104)I, J(cid:105)). This algorithms is justiﬁed by Lemma 7.
The details of algorithm FS∗ are shown in Appendix D. For each j ∈ [|J|], K ⊆ J with |K| = j, and h ∈ K,
the time complexity of computing F S((cid:104)I, K(cid:105)) from F S((cid:104)I, K − h(cid:105)) is linear to the size of TABLE(cid:104)I,K(cid:105),
i.e., 2n−|I|− j, up to a polynomial factor. The total time is thus, up to a polynomial factor,

2n−|I|− j

(cid:33)

(cid:32)|J|
j

|J|(cid:88)

j=1

< 2n−|I|−|J|

2|J|− j

(cid:33)

(cid:32)|J|
j

|J|(cid:88)

j=0

= 2n−|I(cid:116)J| · 3|J|.

If we stop the algorithm at j = k, then the algorithm produces the set {F S((cid:104)I, K(cid:105)) : K ⊆ J, |K| = k}.

The time complexity in this case is at most 2n−|I|−|J| (cid:80)k

(cid:17)
j=0 2|J|− j(cid:16)|J|

j

, up to a polynomial factor.

Since the space complexity is trivially upper-bounded by the time complexity, we complete the
(cid:3)

proof.

Remark 1 One may think that the actual space complexity could be much less than the time complexity.
However, this is not the case. The size of TABLE(cid:104)I,K(cid:105) is also the dominant factor determining the space
complexity. When computing TABLE(cid:104)I,K(cid:105) with |K| = j, it sufﬁces to keep TABLE(cid:104)I,K(cid:105) and TABLE(cid:104)I,K−h(cid:105)
for every h ∈ K in memory. The space complexity is thus, up to a polynomial factor, the maximum of
2n−|I|− j(cid:16)|J|

over all j ∈ [|J|], which is the same order as the time complexity.

(cid:17) + 2n−|I|−( j−1)(cid:16) |J|

(cid:17)
j−1

j

11

LemmaThere is a deterministic algorithm FS* that, given the partial OBDD for !"and #⊆%∖', produces the partial OBDD for !"⊔)in time *∗2-.".)3)time/space.∅%11⊔#1|1⊔#|Remark 2 It is not difﬁcult to see that the algorithm FS∗ works even when the function f has a multival-
ued function: f : {0, 1}n → Z. The only difference from the Boolean case is that the truth table maps each
Boolean assignment to a value in Z. In this case, the algorithm produces a variant of an OBDD (called
a multi-terminal BDD, MTBDD) of minimum size. In addition, our algorithm with two-line modiﬁca-
tions to the table folding rule in FS∗ can construct a minimum zero-suppressed BDD (ZDD) [Min93]
for a given Boolean function. The details are described in Appendix D. These modiﬁcations are also
possible for the quantum algorithms described later, since they perform table folding by running FS∗ as
a subroutine.

The following theorem is the basis of our quantum algorithms.

Lemma 9 (Divide-and-Conquer) For any disjoint subsets I1, . . . , Im, J ⊆ [n] with J (cid:44) ∅ and any k ∈
[|J|], it holds that

MINCOST(cid:104)I1,...,Im,J(cid:105)[J] =

min
K : K⊆J,|K|=k

(cid:0)

MINCOST(cid:104)I1,...,Im,K(cid:105)[K] + MINCOST(cid:104)I1,...,Im,K,J\K(cid:105)[J \ K](cid:1) .

(3)

In particular, when I1 (cid:116) · · · (cid:116) Im = ∅ and J = [n], it holds that

MINCOST[n] = min

K⊆[n],|K|=k

(cid:0)

MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K](cid:1) .

(4)

Proof. We ﬁrst prove the special case of I1 (cid:116) · · · (cid:116) Im = ∅ and J = [n]. By the deﬁnition, we have

MINCOST[n] =

n(cid:88)

j=1

Costπ[ j]( f, π) =

k(cid:88)

j=1

Costπ[ j]( f, π) +

n(cid:88)

j=k+1

Costπ[ j]( f, π),

for the optimal permutation π = π[n]. Let K = {π[1], . . . , π[k]}. By Lemma 3, the ﬁrst sum is independent
of how π maps {k + 1, . . . , n} to [n] \ K. Thus, it is equal to the minimum of (cid:80)k
j=1 Costπ1[ j]( f, π) over all
π1 ∈ Π(K), i.e., MINCOSTK. Similarly, the second sum is independent of how π maps [k] to K. Thus, it is
equal to the minimum of (cid:80)n
j=k+1 Costπ2[ j]( f, π) over all π2 ∈ Π((cid:104)K, [n] \ K(cid:105)), i.e., MINCOST(cid:104)K,[n]\K(cid:105)[[n] \
K]. This completes the proof of Eq. (4).

We can generalize this in a straightforward manner. Let π = π(cid:104)I1,...,Im,J(cid:105) and (cid:96) = |I1 (cid:116) · · · (cid:116) Im|. Then,

we have

MINCOST(cid:104)I1,...,Im,J(cid:105)[J] =

k(cid:88)

j=1

Costπ[(cid:96)+ j]( f, π) +

|J|(cid:88)

j=k+1

Costπ[(cid:96)+ j]( f, π).

By deﬁning K := {π[(cid:96) + 1], . . . , π[(cid:96) + k]}, the same argument as the special case of (cid:96) = 0 implies that
the ﬁrst and second sums are MINCOST(cid:104)I1,...,Im,K(cid:105)[K] and MINCOST(cid:104)I1,...,Im,K,J\K(cid:105)[J \ K], respectively. This
(cid:3)
completes the proof of Eq. (3).

A schematic view of the above lemma is shown in Fig. 8.

3.1 Simple Case

(cid:17)
(cid:16)n
k

We provide simple quantum algorithms on the basis of Lemma 9. The lemma states that, for any k ∈ [n],
MINCOST[n] is the minimum of MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K] over all K ⊆ [n] with |K| = k. To
ﬁnd K from among
possibilities that minimizes this amount, we use the quantum minimum ﬁnding
(Lemma 6). To compute MINCOSTK + MINCOST(cid:104)K,[n]\K(cid:105)[[n] \ K] = MINCOST(cid:104)K,[n]\K(cid:105), it sufﬁces to
ﬁrst compute F S(K) (including MINCOSTK), and then F S((cid:104)K, [n] \ K(cid:105)) (including MINCOST(cid:104)K,[n]\K(cid:105))
from F S(K). The time complexity for computing F S(K) from F S(∅) is O∗(2n−k3k) by Lemma 8 with
I1 (cid:116) · · · (cid:116) Im = ∅ and J = K, while that for computing F S((cid:104)K, [n] \ K(cid:105)) from F S(K) is O∗(3n−k) by
Lemma 8 with m = 1, I1 = K, and J = [n]\K. Thus, the time complexity for computing F S((cid:104)K, [n]\K(cid:105))

12

Figure 8: Schematic view of Eq. (4) in Lemma 9.
Intuitively, the lemma says that it is possible to
decompose FS∗ into the parts each of which goes through the dot corresponding to a subset I ⊆ [n] of
some ﬁxed size, and the optimal variable ordering is induced by one of the parts.

from F S(∅) is O∗(2n−k3k + 3n−k). Thus, for k = αn with α ∈ (0, 1) ﬁxed later, the total time complexity
up to a polynomial factor is

T (n) =

(cid:33) (cid:16)

(cid:115)(cid:32) n
αn

2(1−α)n3αn + 3(1−α)n(cid:17)

≤ 2

1

2 H(α)n (cid:110)

2[(1−α)+α log2 3]n + 2[(1−α) log2 3]n(cid:111)

.

To balance the both terms, we set (1 − α) + α log2 3 = (1 − α) log2 3 and obtain α = α∗, where α∗ =
log2 3−1
2 log2 3−1 ≈ 0.269577. We have minα∈[0,1] T (n) = O
0), where γ0 =
2.98581 . . . . This slightly improves the classical best bound O∗(3n) on the time complexity. To improve
the bound further, we introduce a preprocess that classically computes F S(K) for every K with |K| =
αn (α ∈ (0, 1)) by using algorithm FS∗. By Lemma 7, the preprocessing time is then

2 H(α∗)n+(1−α∗)n+α∗(log2 3)n(cid:17) = O(γn

2

(cid:16)

1

2n− j ·

(cid:32)n
(cid:33)
j

αn(cid:88)

j=1

≤ αn · max
j∈[αn]

2n− j

(cid:33)
(cid:32)n
j

(cid:47)

(cid:40) 2(1−α)n+H(α)n
3 n+H(1/3)n

2

2

(α < 1/3)
(α ≥ 1/3),

(5)

(cid:17)
since 2n− j(cid:16)n
increases when j < n/3 and decreases otherwise. Note that once this preprocess is com-
pleted, we can use F S(K) for free and assume that the cost for accessing F S(K) is polynomially
bounded for all K ⊆ [n] with |K| = αn.

j

Then, assuming that α < 1/3, the total time complexity up to a polynomial factor is

T (n) =

2n− j ·

(cid:33)
(cid:32)n
j

+

αn(cid:88)

j=1

(cid:33) (cid:16)

(cid:115)(cid:32) n
αn

nO(1) + 3(1−α)n(cid:17) (cid:47) 2[(1−α)+H(α)]n + 2[ 1

2 H(α)+(1−α) log2 3]n.

2 H(α) + (1 − α) log2 3 and obtain the solution
To balance the both terms, we set (1 − α) + H(α) = 1
α = α∗, where α∗ := 0.274863 . . . , which is less than 1/3 as we assumed. At α = α∗, we have
T (n) (cid:47) 2[(1−α∗)+H(α∗)]n = O∗(γn
1), where γ1 is at most 2.97625 (< γ0). Thus, introducing the preprocess
improves the complexity bound. A schematic view of the above algorithm is shown in Fig. 9.

Appendix B shows that, by using Lemma 9 twice, we have a better complexity bound O∗(γn

2), where

γ2 is at most 2.8569 (< γ1).

13

∅"This corresponds to #⊂".|#|0"Output '(,*∖('*='-,*∖-such thatK achieves the minimum of mincost56,7∖6Figure 9: Schematic view of our algorithm in the simplest case (one-parameter case). The dotted area
is computed in the classical preprocess, which is realized by truncating the process of FS∗ as stated in
Lemma 8. The shaded area is computed by using FS∗. The actual algorithm runs the quantum minimum
ﬁnding, which calls FS∗ to coherently compute the shaded area corresponding to every dot on the vertical
line indicated by k.

3.2 General Case

We can improve this bound further by applying Lemma 9 k times. We denote the resulting algorithm
with parameters k and α := (α1, . . . , αk) by OptOBDD(k, α) where 0 < α1 < · · · < αk < 1. Its pseudo
code is given in Appendix D. In addition, we assume α1 < 1/3 and αk+1 = 1 in the following complexity
analysis.

To simplify notations, deﬁne two function as follows: for x, y ∈ R such that 0 < x < y < 1,

f (x, y) := 1
2

y · H

(cid:33)

(cid:32) x
y

+ g(x, y),

g(x, y) := (1 − y) + (y − x) log2(3).

The time required for the preprocess is (cid:80)α1n
complexity can be described as the following recurrence:

(cid:96)=1 2n−(cid:96) ·

(cid:17)

(cid:16)n
(cid:96)

up to a polynomial factor. Thus, the total time

T (n) =

α1n(cid:88)

2n−(cid:96) ·

(cid:33)

+ Lk+1(n),

(cid:32)n
(cid:96)
(cid:33) (cid:16)

(cid:96)=1
(cid:115)(cid:32)

α j+1n
α jn

L j+1(n) =

L j(n) + 2(1−α j+1)n3(α j+1−α j)n(cid:17) =

L1(n) = O∗(1),

(cid:115)(cid:32)

(cid:33) (cid:16)

α j+1n
α jn

L j(n) + 2g(α j,α j+1)n(cid:17)

,

(6)

(7)

(8)

for each j ∈ [k]. Intuitively, L j(n) is the time required for producing F S((cid:104)K1, K2 \ K1, . . . , K j \ K j−1(cid:105))
such that MINCOST(cid:104)K1,K2\K1,...,K j\K j−1(cid:105) is minimum over all K1, . . . , K j−1 satisfying |K(cid:96)| = α(cid:96)n for every
(cid:96) ∈ [k + 1] and K(cid:96) ⊂ K(cid:96)+1 for every (cid:96) ∈ [k].
Since L1(n) = O∗(1), we have L2(n) (cid:47)

·2g(α1,α2)n (cid:47) 2 f (α1,α2)n. By setting f (α1, α2) = g(α2, α3),

(cid:113)(cid:16)α2n
(cid:17)
α1n

we have

L3(n) =

(cid:115)(cid:32)

(cid:33)
α3n
α2n

· (L2(n) + 2g(α2,α3)n) (cid:47)

(cid:115)(cid:32)

(cid:33)
α3n
α2n

· 2g(α2,α3)n (cid:47) 2 f (α2,α3)n.

In general, for j = 2, . . . , k, setting f (α j−1, α j) = g(α j, α j+1) yields L j+1(n) (cid:47) 2 f (α j,α j+1)n. Therefore, the

14

∅𝑛𝑘Quantumly find the minimumFS*Computed by classical preprocess (truncation of FS*)total complexity [Eq. (6)] is

T (n) (cid:47)

2n−(cid:96) ·

(cid:33)

(cid:32)n
(cid:96)

α1n(cid:88)

(cid:96)=1

+ 2 f (αk,αk+1)n (cid:47) 2(1−α1)n+H(α1)n + 2 f (αk,1)n,

where we use α1 < 1/3, αk+1 = 1, and Eq. (5). To optimize the right-hand side, we set parameters so
that 1 − α1 + H(α1) = f (αk, 1).

In summary, we need to ﬁnd the values of parameters α1, . . . , αk that satisfy the following system of

equations and α1 < 1/3:

1 − α1 + H(α1) = f (αk, 1),

f (α j−1, α j) = g(α j, α j+1)

( j = 2, . . . , k).

(9)

(10)

By numerically solving this system of equations, we obtain T (n) = O(γn
k), where γk and the correspond-
ing αi’s for k = 1, . . . , 6 are shown in Tab. 1 in Appendix C (α1 < 1/3 is satisﬁed as we assumed). The
value of γk becomes smaller as k increases. However, incrementing k beyond 6 provides only negligible
improvement of γk. The value of γ6 is at most 2.83728. Since the space complexity is trivially upper-
bounded by the time complexity, we have the following theorem. Note that the values of αi’s are not
symmetric with respect to 1/2. This reﬂects the fact that optimizing cost is not symmetric with respect
to 1/2, contrasting with many other combinatorial problems.

Theorem 10 There exists a quantum algorithm that, for the truth table of f : {0, 1}n → {0, 1} given as
input, produces F S([n]) with probability 1 − exp(−Ω(n)) in O∗(γn) time and space, where the constant
γ is at most 2.83728, which is achieved by OptOBDD(k, α) with k = 6 and

α = (0.183791, 0.183802, 0.183974, 0.186131, 0.206480, 0.343573).

4 Quantum Algorithm with Composition

4.1 Quantum Composition Lemma

Recall that the classical composition lemma (Lemma 8) states that algorithm FS can be generalized to
compute F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)). By generalizing the quantum algorithm given in
Theorem 10, we now provide a quantum version of Lemma 8, called the quantum composition lemma.

Lemma 11 (Quantum Composition: Base Part) For any disjoint subsets I1, . . . , Im, J ⊆ [n], there
exists a quantum algorithm that, with probability 1 − exp(−Ω(n)), produces F S((cid:104)I1, . . . , Im, J(cid:105)) from
F S((cid:104)I1, . . . , Im(cid:105)) for an underlying function f : {0, 1}n → {0, 1} in O∗ (cid:16)
time and
space, where γ is the constant deﬁned in Theorem 10.

2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · γ|J|(cid:17)

A pseudo code of the algorithm provided in Lemma 11 is shown as OptOBDD∗

Γ(k, α) in Appendix D,
where the subscript Γ, which represents the subroutine appearing in line 16, will be set to the determin-
istic algorithm FS∗, and k and α will be set to the values speciﬁed in Theorem 10.
Proof. Since the space complexity is trivially upper-bounded by the time complexity, we analyze the
time complexity in the following. For simplicity, we assume m = 1 and write just I instead of I1. It is
straightforward to generalize to the case of m ≥ 2.
We now provide the algorithm OptOBDD∗

Γ(k, α) that produces F S((cid:104)I, J(cid:105)) from F S((cid:104)I(cid:105)), where the
subroutine Γ used in line 16 is set to algorithm FS∗. As parameters, the algorithm has an integer k ∈ N
and a vector α := (α1, . . . , αk) ∈ Rk such that 0 < α1 < · · · < αk < 1. Set k and α to the same values
assumed in the algorithm in Theorem 10. In addition, we assume αk+1 = 1 in the following.

15

Let n(cid:48) = |J|. In the preprocess, the algorithm computes the collection {F S((cid:104)I, K(cid:105)) : K ⊆ J, |K| =
up to a poly-

α1n(cid:48)} based on Lemma 8 for given F S(I) with the time complexity 2n−|I|−n(cid:48) (cid:80)α1n(cid:48)
nomial factor. Thus, the total time complexity is expressed as

(cid:96)=1 2n(cid:48)−(cid:96)(cid:16)n(cid:48)

(cid:17)

(cid:96)

T (cid:48)(n, n(cid:48)) = 2n−|I|−n(cid:48)

α1n(cid:48)
(cid:88)

(cid:96)=1

2n(cid:48)−(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ L(cid:48)

k+1(n, n(cid:48)),

(11)

where L(cid:48)

k+1(n, n(cid:48)) is the time taken to perform all but the preprocess.

Based on Lemma 9, the algorithm proceeds in a way similar to the one given in Theorem 10, which
k+1(n, n(cid:48)) is then expressed by the

corresponds to the special case of I = ∅ and J = [n]. The complexity L(cid:48)
following recurrence:

j+1(n, n(cid:48)) =
L(cid:48)

(cid:115)(cid:32)

α j+1n(cid:48)
α jn(cid:48)

(cid:33) (cid:16)

1(n, n(cid:48)) = O∗(1).
L(cid:48)

j(n, n(cid:48)) + 2n−|I|−α j+1n(cid:48)
L(cid:48)

3(α j+1−α j)n(cid:48)(cid:17)

[ j ∈ [k]],

In the following, we prove by induction that T (cid:48)(n, n(cid:48)) (cid:47) 2n−|I|−n(cid:48)

T (n(cid:48)), where the function T (·) is

deﬁned in Eq. (6). Since n(cid:48) = |J| and T (n(cid:48)) = O∗(γn(cid:48)

), this completes the proof for m = 1.

Since L(cid:48)

1(n, n(cid:48)) = O∗(1), we have

2(n, n(cid:48)) (cid:47) 2n−|I|−n(cid:48)
L(cid:48)

(cid:33)

(cid:115)(cid:32)

α2n(cid:48)
α1n(cid:48)

2(1−α2)n(cid:48)

3(α2−α1)n(cid:48) = 2n−|I|−n(cid:48)

L2(n(cid:48)),

where the function L2(·) is deﬁned in Eq. (7) for j = 1. This is the base case of the induction. Then,
assuming that L(cid:48)

L j(n(cid:48)), we have

j(n, n(cid:48)) (cid:47) 2n−|I|−n(cid:48)
(cid:115)(cid:32)

α j+1n(cid:48)
α jn(cid:48)

j+1(n, n(cid:48)) (cid:47)
L(cid:48)

(cid:33) (cid:16)

2n−|I|−n(cid:48)

L j(n(cid:48)) + 2n−|I|−α j+1n(cid:48)

3(α j+1−α j)n(cid:48)(cid:17)

= 2n−|I|−n(cid:48)

(cid:115)(cid:32)

(cid:33) (cid:16)

α j+1n(cid:48)
α jn(cid:48)

L j(n(cid:48)) + 2n(cid:48)−α j+1n(cid:48)

3(α j+1−α j)n(cid:48)(cid:17) = 2n−|I|−n(cid:48)

L j+1(n(cid:48)).

Therefore, it holds that L(cid:48)

k+1(n, n(cid:48)) (cid:47) 2n−|I|−n(cid:48)

Lk+1(n(cid:48)) by induction. Then, it follows from Eq. (11) that

T (cid:48)(n, n(cid:48)) (cid:47) 2n−|I|−n(cid:48)

α1n(cid:48)
(cid:88)

(cid:96)=1

2n(cid:48)−(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ 2n−|I|−n(cid:48)

Lk+1(n(cid:48)) = 2n−|I|−n(cid:48)

T (n(cid:48)).

Since T (n(cid:48)) = T (|J|) = O∗(γ|J|) by Theorem 10, the lemma follows.

(cid:3)

Lemma 12 (Quantum Composition: Induction Part) Suppose that Γ is a quantum algorithm that, for
any disjoint subsets I1, . . . , Im, J ⊆ [n] with J (cid:44) ∅, produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105))
2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · γ|J|(cid:17)
with probability 1 − exp(−Ω(n)) in O∗ (cid:16)
time and space for an underlying function
f : {0, 1}n → {0, 1}. Then, for any possible k ∈ N and α ∈ Rk and for any disjoint subsets I1, . . . , Im, J ⊆
[n] with J (cid:44) ∅, OptOBDD∗
Γ(k, α) produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105)) with probability
1 − exp(−Ω(n)) in O∗ (cid:16)
time and space for the function f , where βn
k upper-bounds,
up to a polynomial factor, the time complexity required for OptOBDD∗
Γ(k, α) to compute F S([n]) from

2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · β|J|
k

(cid:17)

16

F S(∅), that is, T (n) = O∗(βn

k) for T (n) that satisﬁes the following recurrence:

T (n) =

α1n(cid:88)

2n−(cid:96)

(cid:32)n
(cid:33)
(cid:96)
(cid:33) (cid:16)

(cid:96)=1
(cid:115)(cid:32)

α j+1n
α jn

L j+1 =

+ Lk+1,

L j + 2(1−α j+1)nγ(α j+1−α j)n(cid:17) =

L1 = O∗(1),

where gγ(x, y) := (1 − y) + (y − x) log2 γ.

(cid:115)(cid:32)

(cid:33) (cid:16)

α j+1n
α jn

(12)

L j + 2gγ(α j,α j+1)(cid:17)

( j ∈ [k]),

(13)

(14)

Proof. Recall that algorithm FS∗ is used as a subroutine in OptOBDD(k, α) provided in Theorem 10.
Since the input and output of Γ assumed in the statement are the same as those of algorithm FS∗, one
can use Γ instead of algorithm FS∗ in OptOBDD(k, α) (compromising on an exponentially small error
probability). Let OptOBDDΓ(k, α) be the resulting algorithm. Then, one can see that the time complexity
T (n) of OptOBDDΓ(k, α) satisﬁes the recurrence: Eqs. (12)-(14), which are obtained by just replacing
g(x, y) with gγ(x, y) in Eqs. (6)-(8). Suppose that T (n) = O∗(βn

k) follows from the recurrence.

Next, we generalize OptOBDDΓ(k, α) so that it produces F S((cid:104)I1, . . . , Im, J(cid:105)) from F S((cid:104)I1, . . . , Im(cid:105))
for any disjoint subsets I1, . . . , Im, J ⊆ [n] with J (cid:44) ∅. The proof is very similar to that of Lemma 11. The
only difference is that the time complexity of Γ is O∗ (cid:16)
Namely, when m = 1 and n(cid:48) = |J|, the time complexity of OptOBDD∗
rence: for each j ∈ [n],

Γ(k, α) satisﬁes the following recur-

2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · γ|J|(cid:17)

, instead of O∗ (cid:16)

2n−|I1(cid:116)···(cid:116)Im(cid:116)J| · 3|J|(cid:17)

.

T (cid:48)(n, n(cid:48)) = 2n−|I|−n(cid:48)

α1n(cid:48)
(cid:88)

2n(cid:48)−(cid:96)

(cid:33)

(cid:32)n(cid:48)
(cid:96)

+ L(cid:48)

k+1(n, n(cid:48)),

j+1(n, n(cid:48)) =
L(cid:48)

(cid:115)(cid:32)

(cid:96)=1
(cid:33) (cid:16)

α j+1n(cid:48)
α jn(cid:48)

j(n, n(cid:48)) + 2n−|I|−α j+1n(cid:48)
L(cid:48)

γ(α j+1−α j)n(cid:48)(cid:17)

[ j ∈ [n]],

1(n, n(cid:48)) = O∗(1),
L(cid:48)

from which it follows that T (cid:48)(n, n(cid:48)) = 2n−|I|−n(cid:48)
ize to the case of m ≥ 2.

T (n(cid:48)) = O∗ (cid:16)

2n−|I(cid:116)J| · β|J|
k

(cid:17)
. It is straightforward to general-
(cid:3)

4.2 The Final Algorithm

Lemmas 11 and 12 naturally lead to the following algorithm. We ﬁrst deﬁne Γ1 as OptOBDD∗
for some k(0) ∈ N and α(0) ∈ Rk(0)
α(1) ∈ Rk(1)

(k(i), α(i)) for some k(i) ∈ N and α(i) ∈ Rk(i)
.
1 , . . . , α(i)
6 ) ∈
[0, 1]6 is set for each i so that it satisﬁes the system of equations, a natural generalization of Eqs. (9)(10),

Fix k(i) = 6 for every i. Note that, in the proof of Lemmas 11 and 12, parameter α(i) = (α(i)

FS∗(k(0), α(0))
(k(1), α(1)) for some k(1) ∈ N and

. In this way, we can deﬁne Γi+1 as OptOBDD∗
Γi

. Then, we deﬁne Γ2 as OptOBDD∗
Γ1

+ H(α(i)
j−1, α(i)

1 − α(i)
1
fγ(α(i)

1 ) = fγ(α(i)
j ) = gγ(α(i)

where fγ(x, y) := 1

6 , 1)
j , α(i)
( j = 2, . . . , 6),
(cid:17) + gγ(x, y) and gγ(x, y) := (1 − y) + (y − x) log2 γ.
By numerically solving this system of equations for γ = 3, we have β6 < 2.83728 as shown in
Theorem 10. Then, numerically solving the system of equations with γ = 2.83728, we have β6 <
2.79364. In this way, we obtain a certain γ less than 2.77286 at the tenth composition (see Tab. 2 in
Appendix C). We therefore obtain the following theorem.

2 y · H

j+1)

(16)

(15)

(cid:16) x
y

17

Theorem 13 There exists a quantum algorithm that, for the truth table of f : {0, 1}n → {0, 1} given as
input, produces F S([n]) in O∗(γn) time and space with probability 1 − exp(−Ω(n)), where the constant
γ is at most 2.77286.

Appendix

A Proofs of Lemmas

Proof of Lemma 4. To show the ﬁrst equality, assume k = πI[|I|]. By the deﬁnition, we have

MINCOSTI =

(cid:88)

i∈I

Costi( f, πI) =

(cid:88)

i∈I\{k}

Costi( f, πI) + Costk( f, πI).

The ﬁrst term is equal to MINCOSTI\k, since otherwise there exists π(cid:48) ∈ Π(I) with π(cid:48)[|I|] = k and
π(cid:48)[ j] (cid:44) πI[ j] for some j ∈ [|I| − 1] such that
(cid:88)

(cid:88)

Costi( f, π(cid:48)) =

Costi( f, π(cid:48)) + Costk( f, π(cid:48))

i∈I

i∈I\{k}
(cid:88)

i∈I\{k}

<

Costi( f, πI) + Costk( f, πI) = MINCOSTI,

which contradicts the deﬁnition of MINCOSTI, where we use Costk( f, π(cid:48)) = Costk( f, πI) by Lemma 3.
The remaining term Costk( f, πI) is equal to Costk( f, π(cid:104)I\k,k(cid:105)) by Lemma 3. Thus, the ﬁrst equality
in the statement of the lemma holds. Since MINCOSTI\k = (cid:80)
i∈I\k Costi( f, πI\k) by the deﬁnition, and
Lemma 3 implies that Costi( f, πI\k) = Costi( f, π(cid:104)I\k,k(cid:105)) for every i ∈ I \ k, it holds that MINCOSTI\k =
(cid:80)
i∈I Costi( f, π(cid:104)I\k,k(cid:105)). This implies
(cid:3)

i∈I\k Costi( f, π(cid:104)I\k,k(cid:105)). Therefore, MINCOSTI\k + Costk( f, π(cid:104)I\k,k(cid:105)) = (cid:80)

that the second equality in the lemma.

Proof of Lemma 7. We focus on the simplest case of m = 1 and write just I instead of I1, since it is
straightforward to generalize the proof to the case of m ≥ 2.

To show the ﬁrst equality, assume k = π(cid:104)I,J(cid:105)[|I (cid:116) J|]. By the deﬁnition, we have

MINCOST(cid:104)I,J(cid:105) =

(cid:88)

i∈I(cid:116)J

Costi( f, π(cid:104)I,J(cid:105)) =

(cid:88)

i∈I(cid:116)J\{k}

Costi( f, π(cid:104)I,J(cid:105)) + Costk( f, π(cid:104)I,J(cid:105)).

The ﬁrst term is equal to MINCOST(cid:104)I,J\{k}(cid:105), since otherwise there exists π(cid:48) ∈ Π((cid:104)I, J(cid:105)) with π(cid:48)[|I (cid:116) J|] = k
and π(cid:48)[ j] (cid:44) π(cid:104)I,J(cid:105)[ j] for some j ∈ [|I (cid:116) J| − 1] such that

(cid:88)

i∈I(cid:116)J

Costi( f, π(cid:48)) =

(cid:88)

Costi( f, π(cid:48)) + Costk( f, π(cid:48))

i∈I(cid:116)J\{k}
(cid:88)

i∈I(cid:116)J\{k}

<

Costi( f, π(cid:104)I,J(cid:105)) + Costk( f, π(cid:104)I,J(cid:105)) = MINCOST(cid:104)I,J(cid:105),

which contradicts the deﬁnition of MINCOST(cid:104)I,J(cid:105), where we use Costk( f, π(cid:48)) = Costk( f, π(cid:104)I,J(cid:105)) by Lemma 3.
The remaining term Costk( f, π(cid:104)I,J(cid:105)) is equal to Costk( f, π(cid:104)I,J−k,k(cid:105)) by Lemma 3. Thus, the ﬁrst equality
in the statement of the lemma holds. Since MINCOST(cid:104)I,J\k(cid:105) = (cid:80)
i∈I(cid:116)J\k Costi( f, π(cid:104)I,J\k(cid:105)) by the deﬁni-
tion, and Lemma 3 implies that Costi( f, π(cid:104)I,J\k(cid:105)) = Costi( f, π(cid:104)I,J\k,k(cid:105)) for every i ∈ I (cid:116) J \ k, it holds
that MINCOST(cid:104)I,J\k(cid:105) = (cid:80)
i∈I(cid:116)J\k Costi( f, π(cid:104)I,J\k,k(cid:105)). Therefore, MINCOST(cid:104)I,J\k(cid:105) + Costk( f, π(cid:104)I,J\k,k(cid:105)) =
(cid:80)
(cid:3)

i∈I(cid:116)J Costi( f, π(cid:104)I,J\k,k(cid:105)). This implies that the second equality in the lemma.

18

min
K1⊂K2 : |K1|=k1
(cid:16)k2
k1

B Two-Parameter Case

To improve the complexity bound further, we use Lemma 9 recursively. Let k1 and k2 be parameters
ﬁxed later such that 0 < k1 < k2 < n. By applying the lemma once, we have

MINCOST[n] =

(cid:0)

min
K2⊂[n],|K(cid:48)|=k2

MINCOSTK2

+ MINCOST(cid:104)K2,[n]\K2(cid:105)[[n] \ K2](cid:1) .

Then, we apply the lemma again to MINCOSTK(cid:48) to obtain

MINCOSTK2

=

(cid:0)

MINCOSTK1

+ MINCOST(cid:104)K1,K2\K1(cid:105)[K2 \ K1](cid:1) .

To ﬁnd the optimal K1 from among

(cid:17)
, we use
the quantum minimum ﬁnding (Lemma 6). As in the previous case, we perform the classical preprocess
that computes F S(K1) for all K1 ⊆ [n] with |K1| = k1.

possibilities and the optimal K2 from among

(cid:16) n
k2

If we set k1 = α1n and k2 = α2n, assuming that α1 < 1/3, the total time complexity (up to a

(cid:17)

polynomial factor) is

where

L3(n) =

(cid:33) (cid:16)

(cid:115)(cid:32) n
α2n

T (n) =

2n−(cid:96) ·

(cid:33)

(cid:32)n
(cid:96)

α1n(cid:88)

(cid:96)=1

+ L3(n) (cid:47) 2(1−α1)n+H(α1)n + L3(n),

L2(n) + 3(1−α2)n(cid:17) (cid:47) 2

1

2 H(α2)n(L2(n) + 2(1−α2)(log2 3)n),

(cid:115)(cid:32)

L2(n) =

α2n
α1n
L1(n) = O∗(1).

(cid:33) (cid:16)

L1(n) + 2(1−α2)n3(α2−α1)n(cid:17) (cid:47) 2

1

2 H(α1/α2)α2n(L1(n) + 2(1−α2)n+(α2−α1)(log2 3)n),

(17)

(18)

(19)

(20)

Intuitively, L2(n) represents the time required for producing F S((cid:104)K1, K2 \ K1(cid:105)) for ﬁxed K2 such that
MINCOST(cid:104)K1,K2(cid:105) is minimum over all K1 (⊂ K2) for the given collection of F S(K1) for all K1. Note that,
by Lemma 8, the time required for computing F S((cid:104)K1, K2 \ K1, [n] \ K2(cid:105)) from F S((cid:104)K1, K2 \ K1(cid:105)) is
O∗(3(1−α2)n), and the time required for computing F S((cid:104)K1, K2\K1(cid:105)) from F S(K1) is O∗(2(1−α2)n3(α2−α1)n).
Since we are not interested in polynomial factors in T (n), we ignore them in the following analysis

of this system.

From Eqs. (19) and (20), L2(n) is at most 2

2 H(α1/α2)α2n · 2(1−α2)n+(α2−α1)(log2 3)n. Suppose that L2(n)
equals this upper bound, since our goal is to upper-bound the complexity. To balance the two terms on
the right-hand side of Eq. (18), we set L2(n) = 2(1−α2)(log2 3)n, which implies

1

1
2

α2H(α1/α2) + (1 − α2) + (α2 − α1) log2 3 = (1 − α2) log2 3.

(21)

2 H(α2)n2(1−α2)(log2 3)n. Suppose again that L3(n) equals
On the condition that this holds, L3(n) is at most 2
this upper bound. To balance the two terms on the right-hand side in Eq. (17), we have L3(n) =
2(1−α1)n+H(α1)n, implying that

1

(1 − α1) + H(α1) = 1
2

H(α2) + (1 − α2) log2 3.

= 0.192755 and α∗
By numerically solving Eqs. (21) and (22), we have α∗
2
1
1 and α2 = α∗
less than 1/3, as we assumed. For α1 = α∗
2,
1)+H(α∗
1)]n = O∗(γ2),

T (n) (cid:47) 2[(1−α∗

(22)

= 0.334571. Note that α∗

1 is

where γ2 = 2.8569 < γ1.

19

C Numerical Optimization Data

Table 1: Values of γk and the corresponding αi’s of algorithm OptOBDD(k, α): Each value is written
with 6 digits, but the actual calculation is done with 20-digit precision.

k
1
2
3
4
5
6

γk
2.97625
2.85690
2.83925
2.83744
2.83729
2.83728

α1
0.274862
0.192754
0.184664
0.183859
0.183795
0.183791

α2
—
0.334571
0.205128
0.186017
0.183967
0.183802

α3
—
—
0.342677
0.206375
0.186125
0.183974

α4
—
—
—
0.343503
0.206474
0.186131

α5
—
—
—
—
0.343569
0.206480

α6
—
—
—
—
—
0.343573

Γ(k, α): Each value is written

Table 2: Values of γ and the corresponding αi’s of algorithm OptOBDD∗
with 6 digits, but the actual calculation is done with 20-digit precision.
α4
0.186132
0.167339
0.16189
0.160124
0.159532
0.159332
0.159264
0.159241
0.159233
0.159230

α3
0.183974
0.165857
0.160574
0.158859
0.158284
0.158089
0.158023
0.158000
0.157992
0.157990

α1
0.183792
0.165753
0.160487
0.158777
0.158203
0.158009
0.157943
0.15792
0.157913
0.157910

α2
0.183802
0.165759
0.160491
0.15878
0.158207
0.158013
0.157947
0.157924
0.157916
0.157914

γ
3
2.83728
2.79364
2.77981
2.77521
2.77366
2.77313
2.77295
2.77289
2.77287

β6
2.83728
2.79364
2.77981
2.77521
2.77366
2.77313
2.77295
2.77289
2.77287
2.77286

α5
0.206480
0.183883
0.177376
0.175273
0.174568
0.174330
0.174249
0.174221
0.174212
0.174208

α6
0.343573
0.312741
0.303603
0.300622
0.299621
0.299282
0.299166
0.299127
0.299114
0.299109

20

D Pseudo Codes of Algorithms

Algorithm FS∗: Composable variant of algorithm FS.“A ← B” means that B is substituted for A.
Input: disjoint subsets I, J ∈ [n] and F S(I)
Output: F S((cid:104)I, J(cid:105))

1 Function Main()
2

for (cid:96) := 1 to |J| do

3

4

5

6

7

8

9

10

11

12

for each (cid:96)-element subset K ⊆ J do

MINCOST(cid:104)I,K(cid:105) ← +∞;
for each k ∈ K do

F S((cid:104)I, K − k, k(cid:105)) ← FOLD(I, K, k, F S((cid:104)I, K − k(cid:105)));
if MINCOST(cid:104)I,K(cid:105) > MINCOST(cid:104)I,K−k,k(cid:105) then
F S((cid:104)I, K(cid:105)) ← F S((cid:104)I, K − k, k(cid:105));

end

end

end

// initialization

end
return F S((cid:104)I, J(cid:105))

13
14 end
15 Function FOLD(I, K, k, F S((cid:104)I, K − k(cid:105)))

// produce F S((cid:104)I, K − k, k(cid:105)) from F S((cid:104)I, K − k(cid:105))

π(cid:104)I,K−k,k(cid:105) ← (π(cid:104)I,K−k(cid:105)[1], . . . , π(cid:104)I,K−k(cid:105)[|I (cid:116) K| − 1], k) ;
MINCOST(cid:104)I,K−k,k(cid:105) ← MINCOST(cid:104)I,K−k(cid:105);
NODE(cid:104)I, K − k, k(cid:105) ← ∅;
for b ∈ {0, 1}n−|I|−|K| do

u0 ← TABLE(cid:104)I,K−k(cid:105)[x[n]\(I(cid:116)K) = b, xk = 0];
u1 ← TABLE(cid:104)I,K−k(cid:105)[x[n]\(I(cid:116)K) = b, xk = 1];
if u0 = u1 then

TABLE(cid:104)I,K−k,k(cid:105)[x[n]\(I(cid:116)K) = b] ← u0
else if ∃u (u, u0, u1) ∈ NODE(cid:104)I, K − k, k(cid:105) then

TABLE(cid:104)I,K−k,k(cid:105)[x[n]\(I(cid:116)K) = b] ← u

else

end

TABLE(cid:104)I,K−k,k(cid:105)[x[n]\(I(cid:116)K) = b] ← MINCOST(cid:104)I,K−k,k(cid:105) + 2;
MINCOST(cid:104)I,K−k,k(cid:105) ← MINCOST(cid:104)I,K−k,k(cid:105) + 1;
Insert (u, u0, u1) into NODE(cid:104)I, K − k, k(cid:105)

end
return F S((cid:104)I, K − k, k(cid:105))

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32
33 end

// initialization
// initialization
// initialization

// create a new node

Adaptation to ZDD

To adapt FS∗ to ZDD, it sufﬁces to modify lines 22 and 23 in algorithm FS∗ follows:

if u1 = 0 then

TABLE(cid:104)I,K−k,k(cid:105)[x[n]\(I(cid:116)K) = b] ← u0

21

Algorithm OptOBDD(k, α): Quantum OBDD-minimization algorithm with parameters k ∈ N and α :=
(α1, . . . , αk) ∈ Rk satisfying 0 < α1 < · · · < αk < 1, where the quantum minimum ﬁnding algorithm is used
in line 8, and FS∗ is used in lines 2 and 15. “A ← B” means that B is substituted for A.
Input: F S(∅) :={ TABLE∅, π∅, MINCOST∅, NODE∅ } (accessible from all Functions)
Output: F S([n])

1 Function Main()
2

compute the set {F S(K) : K ⊆ [n], |K| = α1n} by algorithm FS (or FS∗);
make the set of these F S(K) global (i.e., accessible from all Functions);
return DivideAndConquer([n], k + 1)

4
5 end
6 Function DivideAndConquer(L, t)
7

if t=1 then return F S(L);
Find K(⊂ L) of cardinality αt−1n, with Lemma 6, that minimizes MINCOST(cid:104)K,L\K(cid:105),

which is computed as an component of F S((cid:104)K, L \ K(cid:105)) by calling ComputeFS(K, L \ K, t);

// Compute F S(L) with α1, . . . , αt(= |L|/n)
// F S(L) has been precomputed.

3

8

9

10

// Compute F S((cid:104)K, M(cid:105)) with α1, . . . , αt

let K∗ be the set that achieves the minimum;
return F S((cid:104)K∗, L \ K∗(cid:105))

11
12 end
13 Function ComputeFS(K, M, t)
14

F S(K) ← DivideAndConquer(K, t − 1);
F S((cid:104)K, M(cid:105)) ← FS∗(K, M, F S(K));
return F S((cid:104)K, M(cid:105))

15

16
17 end

Γ(k, α): Composable Quantum OBDD-minimization algorithm with subroutine Γ and pa-
Algorithm OptOBDD∗
rameters k ∈ N and α = (α1, . . . , αk) ∈ Rk satisfying 0 < α1 < · · · < αk < 1, where the quantum minimum
ﬁnding algorithm is used in line 9, and subroutine Γ is used in line 16. “A ← B” means that B is substituted for A.
Γ(I1, I2, J, F S(I1, I2)) produces F S(I1, I2, J) from F S(I1, I2).
Input: I ⊆ [n], J ⊆ [n], F S(I). (accessible from all Functions)
Output: F S((cid:104)I, J(cid:105))

3

4

1 Function Main()
n(cid:48) ← |J|;
2
compute the set {F S((cid:104)I, K(cid:105)) : K ⊆ J, |K| = α1n(cid:48)} by algorithm FS∗;
make n(cid:48) and the above set of F S((cid:104)I, K(cid:105)) global (i.e., accessible from all Functions);
return DivideAndConquer(J, k + 1)

5
6 end
7 Function DivideAndConquer(L, t)
if t=1 then return F S(I, L);
8
Find K(⊂ L) of cardinality αt−1n(cid:48), with Lemma 6, that minimizes MINCOST(cid:104)I,K,L\K(cid:105)

9

// Compute F S((cid:104)I, L(cid:105)) with α1, . . . , αt
// F S(I, L) has been precomputed.

// initialization

which is computed as an component of F S((cid:104)I, K, L \ K(cid:105)) by calling ComputeFS(I, K, L \ K, t);

10

11

let K∗ be the set that achieves the minimum;
return F S((cid:104)I, K∗, L \ K∗(cid:105))

12
13 end
14 Function ComputeFS(I, K, M, t)
15

F S(I, K) ← DivideAndConquer(K, t − 1);
F S((cid:104)I, K, M(cid:105)) ← Γ(I, K, M, F S(I, K));
return F S((cid:104)I, K, M(cid:105))

16

17
18 end

// Compute F S((cid:104)K, M(cid:105)) with α1, . . . , αt

22

References

[ABI+19]

Andris Ambainis, Kaspars Balodis, Janis Iraids, Martins Kokainis, Krisjanis Prusis, and
Jevgenijs Vihrovs. Quantum speedups for exponential-time dynamic programming al-
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
gorithms.
Algorithms, SODA 2019, pages 1783–1793, 2019.

[Ake78]

S. B. Akers. Binary decision diagrams. IEEE Trans. Comput., 27(6):509–516, June 1978.

[BC95]

Randal E. Bryant and Yirng-An Chen. Veriﬁcation of arithmetic circuits with binary mo-
ment diagrams. In Proceedings of the 32st Conference on Design Automation, San Fran-
cisco, California, USA, Moscone Center, June 12-16, 1995., pages 535–541, 1995.

[BCdWZ99] Harry M. Buhrman, Richard E. Cleve, Ronald de Wolf, and Christof Zalka. Bounds for
small-error and zero-error quantum algorithms. In Proceedings of 40th Annual Symposium
on Foundations of Computer Science, FOCS ’99, pages 358–368, 1999.

[BFG+97]

R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, A. Pardo, and F. Somenzi.
Algebric decision diagrams and their applications. Formal Methods in System Design,
10(2–3):171–206, 1997.

[Bol16]

[Bry86]

[Bry91]

[Bry92]

[Bry18]

[BW96]

Beate Bollig. On the minimization of (complete) ordered binary decision diagrams. The-
ory of Computing Systems, 59(3):532–559, Oct 2016.

Randal E. Bryant. Graph-based algorithms for boolean function manipulation.
Trans. Comput., 35(8):677–691, August 1986.

IEEE

Randal E. Bryant. On the complexity of VLSI implementations and graph representations
IEEE Transactions on
of boolean functions with application to integer multiplication.
Computers, 40(2):205–213, Feb 1991.

Randal E. Bryant. Symbolic boolean manipulation with ordered binary-decision diagrams.
ACM Comput. Surv., 24(3):293–318, September 1992.

Randal E. Bryant. Binary decision diagrams. Handbook of Model Checking, pages 191–
217, 2018.

B. Bollig and I. Wegener. Improving the variable ordering of OBDDs is NP-complete.
IEEE Transactions on Computers, 45(9):993–1002, Sep. 1996.

[CMZ+97] E.M. Clarke, K. L. Mcmillan, X. Zhao, M. Fujita, and J. Yang. Spectral transforms for
large boolean functions with applications to technology mapping. Formal Methods in
System Design, 10(2–3):137–148, 1997.

[DB98]

[DH96]

Rolf Drechsler and Bernd Becker. Binary Decision Diagrams: Theory and Implementa-
tion. Springer, 1998.

Christoph D¨urr and Peter Høyer. A quantum algorithm for ﬁnding the minimum. Technical
Report quant-ph/9607014, arXiv, 1996.

[DHHM06] Christoph D¨urr, Mark Heiligman, Peter Høyer, and Mehdi Mhalla. Quantum query com-

plexity of some graph problems. SIAM Journal on Computing, 35(6):1310–1328, 2006.

[FS90]

S. J. Friedman and K. J. Supowit. Finding the optimal variable ordering for binary decision
diagrams. IEEE Transactions on Computers, 39(5):710–713, May 1990.

23

[GJ79]

Michael R. Garey and David S. Johnson. COMPUTERS AND INTRACTABILITY — A
Guide to the Theory of NP-Completeness. W. H. Freeman and Company, New York, 2
edition, 1979.

[GLM08]

Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access mem-
ory. Phys. Rev. Lett., 100:160501, Apr 2008.

[Gro96]

[HC92]

[Hea93]

[HI02]

[HM94]

[HM00]

Lov K. Grover. A fast quantum mechanical algorithm for database search. In Proceedings
of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pages 212–219,
1996.

Heh-Tyan Liaw and Chen-Shang Lin. On the obdd-representation of general boolean
functions. IEEE Transactions on Computers, 41(6):661–664, June 1992.

Mark Heap. On the exact ordered binary decision diagram size of totally symmetric func-
tions. Journal of Electronic Testing, 4(2):191–195, 1993.

Takashi Horiyama and Toshihide Ibaraki.
knowledge-bases. Artif. Intell., 136(2):189–213, 2002.

Ordered binary decision diagrams as

M. A. Heap and M. R. Mercer. Least upper bounds on obdd sizes. IEEE Transactions on
Computers, 43(6):764–767, June 1994.

L. Heinrich-Litan and P. Molitor. Least upper bounds for the size of obdds using symmetry
properties. IEEE Transactions on Computers, 49(4):360–368, April 2000.

[HTKY97] K. Hosaka, Y. Takenaga, T. Kaneda, and S. Yajima. Size of ordered binary decision di-
agrams representing threshold functions. Theoretical Computer Science, 180(1):47 – 60,
1997.

[HY97]

[INY98]

Takashi Horiyama and Shuzo Yajima. Exponential lower bounds on the size of obdds
representing integer divistion. In Proceedings of the 8th International Symposium on Al-
gorithms and Computation, (ISAAC ’97), Singapore, December 17-19, 1997, Proceedings,
Lecture Notes in Computer Science, pages 163–172. Springer, 1997.

Kazuo Iwama, Mitsushi Nouzoe, and Shuzo Yajima. Optimizing obdds is still intractable
for monotone functions. In Proceedings of the 23rd International Symposium on Mathe-
matical Foundations of Computer Science (MFCS’98), volume 1450 of Lecture Notes in
Computer Science, pages 625–635. Springer, 1998.

[KLM07]

Phillip Kaye, Raymond Laﬂamme, and Michele Mosca. An Introduction to Quantum
Computing. Oxford University Press, 2007.

[Knu09]

Donald E. Knuth. The Art of Computer Programming, Volume 4, Fascicle 1: Bitwise
Tricks & Techniques; Binary Decision Diagrams. Addison-Wesley Professional, 1 edition,
March 2009.

[KSV02]

Alexei Yu. Kitaev, Alexander H. Shen, and Mikhail N. Vyalyi. Classical and Quantum
Computation, volume 47 of Graduate Studies in Mathematics. AMS, 2002.

[Lee59]

C. Y. Lee. Representation of switching circuits by binary-decision programs. The Bell
System Technical Journal, 38(4):985–999, July 1959.

[LGM18]

Franc¸ois Le Gall and Fr´ed´eric Magniez. Sublinear-time quantum computation of the di-
ameter in congest networks. In Proceedings of the 2018 ACM Symposium on Principles
of Distributed Computing, PODC ’18, pages 337–346, New York, NY, USA, 2018. ACM.

24

[Min93]

[Min11]

[MS94]

[MT98]

[NC00]

[Sie02a]

[Sie02b]

[SIT95]

[STY94]

[THY96]

[TI94]

[TY00]

[Weg00]

Shin-ichi Minato. Zero-suppressed BDDs for set manipulation in combinatorial problems.
In Proceedins of the 30th ACM/IEEE Design Automation Conference, pages 272–277,
June 1993.

Shin-ichi Minato. πDD: A new decision diagram for efﬁcient problem solving in permu-
tation space. In Proceedings of the 14th International Conference on Theory and Appli-
cations of Satisﬁability Testing (SAT 2011), volume 6695 of Lecture Notes in Computer
Science, pages 90–104, 2011.

Christoph Meinel and Anna Slobodov´a. On the complexity of constructing optimal or-
In Proceedings of 19th Mathematical Foundations of
dered binary decision diagrams.
Computer Science 1994, pages 515–524, Berlin, Heidelberg, 1994. Springer Berlin Hei-
delberg.

Christoph Meinel and Thorsten Theobald. Algorithms and Data Structures in VLSI De-
sign: OBDD - Foundations and Applications. Springer, 1998.

Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Informa-
tion. Cambridge University Press, 2000.

Detlef Sieling. The complexity of minimizing and learning OBDDs and FBDDs. Discrete
Applied Mathematics, 122(1):263 – 282, 2002.

Detlef Sieling. The nonapproximability of OBDD minimization. Information and Com-
putation, 172(2):103 – 138, 2002.

Kyoko Sekine, Hiroshi Imai, and Seiichiro Tani. Computing the Tutte polynomial of
a graph of moderate size. In Proceedings of the Sixth International Symposium on Algo-
rithms and Computation (ISAAC ’95), volume 1004 of Lecture Notes in Computer Science,
pages 224–233. Springer, 1995.

Hiroshi Sawada, Yasuhiko Takenaga, and Shuzo Yajima. On the computational power of
binary decision diagrams. IEICE Trans. Info. & Syst., D, 77(6):611–618, 1994.

Seiichiro Tani, Kiyoharu Hamaguchi, and Shuzo Yajima. The complexity of the optimal
variable ordering problems of a shared binary decision diagram. IEICE Transactions on
Information and Systems, E79-D(4):271–281, 1996. (Conference version is in Proceed-
ings of the Fourth International Symposium on Algorithms and Computation (ISAAC’93),
vol. 2906, pp.389–398, Lecture Notes in Computer Science, Springer, 1993).

Seiichiro Tani and Hiroshi Imai. A reordering operation for an ordered binary decision
diagram and an extended framework for combinatorics of graphs. In Proceedings of the
Fifth International Symposium on Algorithms and Computation (ISAAC’94), volume 834
of Lecture Notes in Computer Science, pages 575–583. Springer-Verlag, 1994.

Yasuhiko Takenaga and Shuzo Yajima. Hardness of identifying the minimum ordered
binary decision diagram. Discrete Applied Mathematics, 107(1-3):191–201, 2000.

Ingo Wegener. Branching Programs and Binary Decision Diagrams. SIAM Monographs
on Discrete Mathematics and Applications. SIAM, 2000.

25

