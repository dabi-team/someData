1
2
0
2

t
c
O
5

]
L
P
.
s
c
[

1
v
7
0
3
2
0
.
0
1
1
2
:
v
i
X
r
a

Coarsening Optimization for Diï¬€erentiable Programming

Xipeng Shen+âˆ—, Guoqiang Zhang+, Irene Dea(cid:5), Samantha Andow(cid:5),
Emilio Arroyo-Fang(cid:5), Neal Gafter(cid:5), Johann George(cid:5), Melissa Grueter(cid:5),
Erik Meijer(cid:5), Steï¬ƒ Stumpos(cid:5), Alanna Tempest(cid:5), Christy Warden(cid:5), Shannon Yang(cid:5)

+ North Carolina State University

âˆ— CoCoPIE LLC

(cid:5) Facebook Inc.

Contact: xshen5@ncsu.edu

October 7, 2021

Abstract

This paper presents a novel optimization for diï¬€erentiable programming named coarsening opti-
mization. It oï¬€ers a systematic way to synergize symbolic diï¬€erentiation and algorithmic diï¬€erenti-
ation (AD). Through it, the granularity of the computations diï¬€erentiated by each step in AD can
become much larger than a single operation, and hence lead to much reduced runtime computations
and data allocations in AD. To circumvent the diï¬ƒculties that control ï¬‚ow creates to symbolic diï¬€er-
entiation in coarsening, this work introduces Ï†-calculus, a novel method to allow symbolic reasoning
and diï¬€erentiation of computations that involve branches and loops. It further avoids â€expression
swellâ€ in symbolic diï¬€erentiation and balance reuse and coarsening through the design of reuse-centric
segment of interest identiï¬cation. Experiments on a collection of real-world applications show that
coarsening optimization is eï¬€ective in speeding up AD, producing several times to two orders of
magnitude speedups.

1 Introduction

A program written with diï¬€erentiable programming can be diï¬€erentiated automatically. The diï¬€erenti-
ation results can then be used for gradient-based optimization (e.g., gradient descent) of the parameters
in the program.

Diï¬€erentiable programming have been used in scientiï¬c computing, physics simulations, and other
domains to help mitigate the burden of manual error-prone coding of derivative computations. Re-
cent several years have witnessed a growing interest of diï¬€erentiable programming in machine learning
(ML) [11, 34] and Probabilistic Programming [30], to accommodate the needs of various customized ML
operators, user-deï¬ned operations in the learning targets (e.g., the physical environment of reinforcement
learning) and statistical sampling.

The key technique in diï¬€erentiable programming is automatic diï¬€erentiation. For a program (P ) that
produces output (y) from some given values (X), automatic diï¬€erentiation automatically computes the
derivatives (âˆ‚y/âˆ‚x) (x âˆˆ X) without the need for users to write the diï¬€erentiation code. The given
program P is called the primal code, and x is called an active input variable.

Existing approaches of automatic diï¬€erentiation fall into two categories: (i) Symbolic diï¬€erentiation,
which uses expression manipulation in computer algebra systems, (ii) Algorithmic diï¬€erentiation, which
performs a non-standard interpretation of a given computer program by replacing the domain of the
variables to incorporate derivative values and redeï¬ning the semantics of the operators to propagate
derivatives per the chain rule of diï¬€erential calculus (elaborated in Section 2).

Symbolic diï¬€erentiation has been commonly regarded inappropriate for diï¬€erentiable programming,
for several reasons: (i) It results in complex and cryptic expressions plagued with the problem of â€œex-
pression swellâ€ [5]. (ii) It requires models to be deï¬ned as closed-form expressions, limiting the use of
control ï¬‚ow and other features that are common in computer programs.

1

 
 
 
 
 
 
Consequently, existing diï¬€erentiable programming systems are all based on algorithmic diï¬€erentia-
tion (AD). Algorithmic diï¬€erentiation computes derivatives through accumulation of values during code
execution to generate numerical derivative evaluations. In contrast with the eï¬€ort involved in arranging
code as closed-form expressions under the syntactic and semantic constraints of symbolic diï¬€erentiation,
algorithmic diï¬€erentiation can be applied to regular code, allowing branching, loops, and other language
features. Some examples are Autograd [22], PyTorch [25], JAX [12], and Zygote [19].

In this work, we advocate for a hybrid approach for diï¬€erentiable programming. This new ap-
proach seamlessly integrates symbolic diï¬€erentiation with algorithmic diï¬€erentiation through coarsening,
a compiler-based technique we introduce in this work.

The motivation of the new approach is to eliminate the large overhead in AD incurred by its ï¬ne-
grained diï¬€erentiation and operation overloading. Rather than diï¬€erentiation at each operation, this
new approach tries to enlarge the granularity to a sequence of operations, hence the name â€œcoarsening
optimizationâ€. It identiï¬es a part of the to-be-diï¬€erentiated computations that are amenable for symbolic
diï¬€erentiation, elevates it to a high-level symbolic representation, applies symbolic diï¬€erentiation on it,
generates the code, and then integrates it back into the computation ï¬‚ow of AD.

By doing that, the coarsening optimization gives four-fold beneï¬ts: (i) It avoids many calls to the
ï¬ne-grained diï¬€erentiation functions and the creations of many intermediate results; (ii) the symbolic rep-
resentation makes it easy to directly beneï¬t from expression simpliï¬cations by existing symbolic engines
and hence leads to more eï¬ƒcient code being generated; (iii) it can form a synergy with computation reuse
and hence amplify the beneï¬ts; (iv) it can sometimes remove the unnecessary primal computations. If
what users want is only the derivative of a function, current AD still needs to run the primal computation
because of the nature of its diï¬€erentiation process (Sec 2). But if coarsening can be applied to the entire
function, then only its generated diï¬€erentiation function needs to run, foregoing the executions of the
primal function.

In addition to those beneï¬ts, coarsening features several appealing properties: (i) As the coarsening
optimization typically happens at compile time, it trades a slight increase of compile time for signiï¬cant
runtime savings; (ii) functioning as a way to add â€shortcutsâ€ to AD, it can be seamlessly integrated into
both forward and backward diï¬€erentiation; (iii) it applies regardless whether the gradients are for ï¬rst
or higher order optimizations.

To materialize the optimization, there are several major challenges.
Challenge I: Complexities from control ï¬‚ow (e.g., branches, loops). Symbolic diï¬€erentiation requires
a closed form of the computation, which has been regarded as diï¬ƒcult for code involving complex control
ï¬‚ow. Limiting coarsening to the code segments between the appearances of such complexities in a program
would result in many short code segments, leaving many optimization opportunities submerged and much
power of coarsening untapped.

j(cid:54)=i fj.

Challenge II: â€Expression swellâ€. â€Expression swellâ€ is a criticism to symbolic diï¬€erentiation men-
tioned in some literature, which refers to the observation that the derivative often has a much larger
representation than the original function has [5]. For instance, in a straightforward implementation, the
derivative of the multiplication of n terms becomes an expression with n2 terms: d(f1f2 Â· Â· Â· fn)/dx =
(cid:80)

i d(fi)/dx (cid:81)
Challenge III: Tension with computation reuse. Some calculations in the primal computation may be
also required in the diï¬€erentiation (e.g., eXÂ·Î² is part of both (1 + eXÂ·Î²) and its diï¬€erentiation over Î²,
XeXÂ·Î²). Reuse opportunities can also exist between diï¬€erent parts of diï¬€erentiation. The ï¬ne-grained
operations in algorithmic diï¬€erentiation already build on such reuses. But in coarsened diï¬€erentiation,
without a careful design, such reuse opportunities can get lost as the symbolic transformation reorders
and reorganizes the involved calculations. On the other hand, naively maximizing computation reuse
would limit the granularity of coarsening. So there is a challenge in reconciling the tension between reuse
and coarsening.

We address the challenges through two major innovations. (i) For challenge I, we introduce Ï†-calculus,
a novel method that allows symbolic reasoning and diï¬€erentiation of computations with complex control
ï¬‚ow. Building on the Ï†-function in single static assignment (SSA), Ï†-calculus makes the derivation of
a closed form possible for computations involving complicated control ï¬‚ow.
It further oï¬€ers a set of
formulae for symbolically reasoning about and diï¬€erentiating the closed forms that involve Ï†-functions.

2

(ii) For challenges II and III, we propose reuse-aware SOI identiï¬cation as a way to identify the code
segments of interest (SOI) for coarsening. It can strike a good tradeoï¬€ between coarsening and reuse,
and at the same time keep the eï¬€ects of â€expression swellâ€ under control.

Based on an AD tool for Kotlin (Diï¬€Kt), we evaluated coarsening on 18 settings of six applications on
two machines. The results show that coarsening is eï¬€ective in signiï¬cantly expanding the applicable scope
of symbolic diï¬€erentiation, and hence dramatically reducing the runtime overhead of AD. The perfor-
mance improvement is substantial, 1.03Ã—-27Ã— speedups of the diï¬€erentiation and 1.08Ã—-11Ã— speedups of
the end-to-end application execution. We further examined the potential of coarsening on several other
AD tools (Zygote [19] for Julia, Jax [12] for Python, Adept [7] for C++) by experimenting with the
implementations of the symbolic diï¬€erentiation results in their corresponding languages. The speedups
from the coarsening results are even greater, 66Ã—-335Ã—, indicating the potential of coarsening in serving
as a general optimization technique for AD.

To the best of our knowledge, this is the ï¬rst work that proposes a systematic approach to integrating
symbolic diï¬€erentiation with algorithmic diï¬€erentiation for diï¬€erentiable programming. The developed
Ï†âˆ’calculus oï¬€ers the ï¬rst method to enable symbolic diï¬€erentiation of computations spanning over
complex control ï¬‚ow. The resulting hybrid diï¬€erentiation approach gets the best of both worlds, that is,
the eï¬ƒciency from the compile-time symbolic diï¬€erentiation and the generality of AD.

In summary, this work makes the following contributions:

â€¢ It introduces coarsening optimization, the ï¬rst approach to systematical integration of symbolic

diï¬€erentiation into algorithmic diï¬€erentiation for general programs.

â€¢ It develops Ï†-calculus that eliminates the barriers of control ï¬‚ow to symbolic diï¬€erentiation.

â€¢ It proposes reuse-aware SOI identiï¬cation to balance reuse and coarsening.

â€¢ It validates the beneï¬ts of coarsening, conï¬rming its potential for signiï¬cantly improving AD eï¬ƒ-

ciency.

2 Background and Terminology

At the foundation of AD is the chain rule. We explain it in a simple setting. Suppose y is the output of a
sequence computations on input x, and yi (i = 1, 2, Â· Â· Â· , k) are the intermediate results produced during
the sequence of computations from x to y, that is, y1 = f1(x), y2 = f2(y1), Â· Â· Â· , yk = fk(ykâˆ’1), y = f (yk).
The chain rule says that the derivative of y on x (or called xâ€™s gradient regarding y) can be computed as
follows:

dy/dx = dy/dyk âˆ— dyk/dykâˆ’1 âˆ— Â· Â· Â· âˆ— dy1/dx

In a program, besides the variables relevant to the deriatives of interest, there can be many other
variables. To distinguish them, we call the relevant output variables like y active output variables, relevant
input variables like x active input variables, and other relevant variables simply active variables. Further,
we call the computations in the original program from active input variables to active output variables
primal computations, and the computations to compute the derivatives gradient computations.

There are two ways to interpret the chain rule, which lead to the forward and backward AD respectively.
We explain them by assuming an implementation of AD via operator overloading, the most common way
of implementation of AD.

The ï¬rst is to regard the rhs of the chain rule a sequence of computations from the rightmost term
to the leftmost term. Corresponding to AD implementation, the derivatives of dy1/dx is computed as
a side step of operator overloading when y1 is computed from x in the primal computation, and the
result is then passed to the next step of primal computation, which computes y2 and dy2/dy1 and then
multiplies it with the received value of dy1/dx. The process continues and produces dy/dx eventually.
This implementation is called forward AD.

3

Figure 1: The overall workï¬‚ow of coarsening for AD. Solid boxes are the main components in coarsening.

The second way is to regard the rhs of the chain rule a sequence of computations from the leftmost
term to the rightmost term. In this case, at each step in the primal computation (which is still right-to-
left), some operations needed for diï¬€erentiation are recorded in a data structure, such as a stack [7], as
part of the operations of the overloaded operators. When the primal computations reach the last step
and the gradient computation actually starts, the operations on the stack are executed in a backward
order, starting from those of the leftmost term in the rhs of the chain rule. After dy/dyk is computed,
the result is passed to the next step, which computes dyk/dykâˆ’1 and then multiples it with the received
value of dy/dyk. The process continues until the gradient of x is computed. This implementation is called
backward AD.

Backward AD is a more popular choice in existing AD tools because it is overall more eï¬ƒcient in
general settings [23]. The two methods are sometimes used together. Note that in both of them, primal
computations are necessary to run so that the overloaded operators can take place, even if what the user
wants are just the gradients. Coarsening optimization can lift such a requirement as shown later in this
paper.

In cases that are not diï¬€erentiable (e.g., x=0 in relu(x)), AD tools approximate the gradients (e.g.,

using 0 at relu(0)); coarsening optimization preserves the same behavior.

3 Overview of Coarsening Optimization

The basic deï¬nition of coarsening optimization for AD is as follows:

Deï¬nition 1. Let S be a sequence of program statements that implement the computations from active
input I to active output P . Coarsening optimized AD is applied to S if a closed form F is produced that
captures the computations in S and F goes through a symbolic diï¬€erentiation with the results integrated
into the AD process.

Figure 1 shows the high-level workï¬‚ow of coarsening for AD. The input is a program written in a
certain AD-based diï¬€erentiable programming language. Coarsening works on the intermediate represen-
tation (IR) output from the front end of the default compiler. From it, the reuse-aware SOI identiï¬cation
component identiï¬es the code segments of interest (SOIs), which are sent to the symbolic elevation com-
ponent to produce a symbolic representation of the SOIs. The symbolic diï¬€erentiation component takes
them in and outputs the symbolic form of the diï¬€erentiated SOIs. The symbolic optimization compo-
nent optimizes both the SOIs and the diï¬€erentiated SOIs while drawing on their contexts captured in
the original IR. The optimizations include simpliï¬cations via algebra systems, as well as identifying the
places for proï¬table computation reuses between SOIs and the diï¬€erentiated SOIs.

Coarsening optimization can be applied for AD at both compilation and runtime. We take compile-

time optimization as the context of discussion.

4

Source program w/ alg. differen.Compiler front endReuse-aware SOI identiï¬cationSymbolic elevationSymbolic differentiationSymbolic optimizationCode gen. & integrationOpt. program w/ hybrid differen.Initial IRSOIsSymbolic repr. of SOIsDiï¬€erentiated SOIsOpt. SOIs & Opt. diï¬€eren. SOIs(c) Computation ï¬‚ow of the forward pass of CartPole with the main computations shown on the right.

Figure 2: A running example CartPole. (a) Problem illustration (artwork source: ï¬‚uxml.ai); (b) Pseudo-
code of the training harness; (c) The simpliï¬ed computation graph of the forward pass and the core
computations.

Example To help convey the intuition of coarsening optimization, we use CartPole as an example.
CartPole is an example that uses deep reinforcement learning (DRL). As illustrated in Figure 2 (a), a
pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The cart is
controlled by applying a force of +1 (to the right) or -1 (to the left) on the cart. The pendulum starts
upright, and the goal is to learn a cart control policy to prevent the pendulum from falling over. The
learning system consists of a Neural Network and a simulator of the cart and pole. At each time step, the
system goes through the computation outlined in the inner loop body in Figure 2 (b), that is, calculating
the output of the Neural Networks from the current cart and poleâ€™s states to decide the action for the cart
to take, based on which, it then updates the states of the cart and pole according to the physics model.
This process continues for another two time steps. The resulting total loss is then used in updating the
weights of the Neural Networks via gradient descent. For each weight w in the Neural Networks, (i) the
program obtains the value dw for the derivative of w w.r.t. the loss, (ii) dw is then used to create the
diï¬€erential Î»z.dw âˆ— z, and (iii) the value of weight w is updated by the result of evaluating the diï¬€erential
w.r.t. learning rate Î·:

w = w âˆ’ (Î»z.dw âˆ— z)(Î·).

The learning then continues until the Neural Networks converge. The right part of Figure 2 (c) shows
the core computations of states update in each iteration.

5

Input:  X0: initial states W0: initial Neural Networks parameters Output:     W: final parameters of Neural Networks t = 0 while (loss > threshold)      loss = 0  for (i=0; i< B; i++)       a = getAction(Xt+i, W)       Xt+i+1 = updateState(Xt+i, a)   loss += getLoss(Xt+i+1)   backpropagate(loss, W, X)  t += B(a) Illustration of the CartPole problem. The goal is to learn a cart control policy to prevent the pendulum from falling over (b) Pseudo-code of the training harness of CartPoleatWlt+1lt+2lt+3Lxt+1, 0xt+1, 1xt+1, 2xt+1, 3xt, 0xt, 1xt, 1xt, 2at = sign(tanh(relu(relu(Xtâ€™W1)W2)W3)-e)rt = 9at+0.045xt,32sinxt,2qt=(9.8sinxt,2-rt*cosxt,2)/(0.65-0.4cos2xt,2)pt=rt - 0.045qt*cosxt,2x(t+1),0 = xt,0+0.02xt,1
x(t+1),1 = xt,1+0.02pt
x(t+1),2 = xt,2+0.02xt,3
x(t+1),3 = xt,3+0.02qtlt+1 = (0.5-max(0, (2.4-|x(t+1),0|)*(0.21-|x(t+1),2|)))2rtqtptL = lt+1+lt+2+lt+3WNeural  Network  with parameters W1, W2, W3State of the cart pole systemIntermediate resultsLoss valuesTime tTime t+1at+1Wxt+2, 0xt+2, 1xt+2, 2xt+2, 3rt+1qt+1pt+1Time t+2at+2Wxt+3, 0xt+3, 1xt+3, 2xt+3, 3rt+2qt+2pt+2Time t+3at+3WIntuition of Beneï¬ts We can get some intuition about the potential beneï¬ts of coarsening by checking
the diï¬€erentiation of a state at time t + 2, xt+2,0, over the action at time t, at, in Figure 2. The result
is needed in the computation of the gradient of at regarding the loss Lt+2 and the total loss L. (The
computation from the weight W to a is through Neural Networks, which have a standard structure; the
gradient calculation of that part is through a highly polished vendor-provided library rather than the AD
tool. Therefore, the actual eï¬€ects of the AD on that benchmark is from the loss to a.)

If we put down the entire computation from at to xt+2,0, we get the following (with constants already

folded):

xt+2,0 =

xt,0 + 0.04xt,1 + 0.003636at + 0.000018xt,3

2sinxt,2
2sinxt,2cos2xt,2

âˆ’

0.00016sinxt,2 âˆ’ 0.00016atcos2xt,2 âˆ’ 0.000008xt,3

(0.65 âˆ’ 0.405cos2xt,2)

(3.1)

It consists of 29 operations.

If a (backward) AD library is used to get its diï¬€erentiation over at,
during the primal computation, at each of the 29 operations, a pullback function is generated for the
diï¬€erentiation of that operation, along with the closure and some intermediate objects allocated to hold
the intermediate results that the diï¬€erentiation would need to use.

In contrast, if symbolic diï¬€erentiation is applied to the expression in Equation 3.1, the result is much
simpler as shown as follows. The diï¬€erentiation would then need to just make an invocation to one
function that consists of only several straight-line calculations. Besides saving computations, it also saves
the allocations of many intermediate objects.

d(xt+2,0)
d(at)

= 0.003636 +

0.00016cos2xt,2
0.65 âˆ’ 0.405cos2xt,2

(3.2)

Besides the beneï¬ts demonstrated by the CartPole example, two other beneï¬ts are worth mentioning.
First, because AD libraries are typically implemented via operator overloading, they have to wrap data
objects in a special type (e.g., Tensor in PyTorch) so that customized operations can be invoked during
the primal computations to implement the needed AD operations. Accesses to the data objects are
therefore subject to the boxing overhead. Inside the code generated by the symbolic diï¬€erentiation, as no
operator overloading is needed, unboxed data objects can be directly used, reducing the boxing overhead.
This beneï¬t is especially prominent when the data involved are a collection of scalars or small vectors as
in many physical simulations or Probablistic Programming applications (examples in Sec 7).

The other beneï¬t of coarsening not captured by the CartPole example is the cancellation of terms or
other simpliï¬cations that symbolic transformation can often harness. We explain it with a simple expres-
sion involving two matrices (X1, X2) and one vector (v): (X2v)T (X1X2v). Its symbolic diï¬€erentiation
over v can easily combine terms with common multipliers, yielding a form X T
1 )X2v, signiï¬-
2 (X1X2v) + ((X2v)T X1X2)T . Simpliï¬cation
cantly simpler than what the default AD would compute, X T
of symbolic expressions is a common feature in symbolic engines; for more examples, please refer to the
simpliï¬cation module in Sympy [4].

2 (X1 + X T

As mentioned in Section 1, to make coarsening optimization eï¬€ective, there are three main challenges:
control ï¬‚ows, â€expression swellâ€, and tension between coarsening and reuse. We next explain our solutions
to these challenges.

4 Addressing Control Flow: Ï†-Calculus

Control ï¬‚ow complexities are commonly perceived obstacles for symbolic diï¬€erentiation. For straight-line
code, it is easy to derive a closed form for the computations by symbolically substituting later references
with their earlier deï¬nitions in the code. In the presence of control ï¬‚ow branches, the complexity increases
exponentially: If we build a closed form for each possible path, in the worst case, there would be O(2B)

6

closed forms for B conditional statements. That would not only increase the amount of work and
execution time of symbolic diï¬€erentiation, but also complicate compiler-based code generation from the
diï¬€erentiation results. The problem worsens when there are loops mixed with if-else. Without a closed
form, symbolic diï¬€erentiation cannot apply.

Without an eï¬€ective way to handle control ï¬‚ow, coarsening can apply only to small pieces in a
program, with each piece consisting of the code between two adjacent control ï¬‚ow branching or merging
points. For some programs, that could lead to only small optimization scopes, leaving the power of
coarsening optimization untapped.

We address the problem by proposing Ï†-calculus. It consists of a set of notations for symbolically
representing loops and conditional statements, and introduces a series of formulae to facilitate the rea-
soning and diï¬€erentiation on the extended symbolic form. As Ï†-calculus is inspired by the concept of Ï†
functions in SSA, we ï¬rst give a quick review of SSA.

4.1 Background on SSA and Ï† Functions

Static Single Assignment form (SSA) is a kind of code representation widely used in modern compilers [14,
9]. Code in SSA has two properties: (i) no two static assignments assign values to the same variable;
(ii) every reference refers to the value deï¬ned by a single static assignment. It uses a special Ï† function
to resolve name ambiguities. A Ï† function â€choosesâ€ the right name among its (two or more) arguments
based on the actual control ï¬‚ow.

Figure 3(c) shows the SSA form of the code in Figure 3(a). There are three assignments to z in
the original code. They are all replaced with diï¬€erent names z1, z2, z4. Meanwhile, two Ï† functions are
inserted in Figure 3(c), with the one on line 8 resolving the name ambiguity caused by the inner if-else,
and the one on line 11 for the outer if-else.

Figure 4(b) shows the SSA form of the loop in Figure 4(a). The loop structure involves two merging
points, with one at the entry (L1), the other at the exit (L2). There are two Ï† functions at the entry,
respectively for variables s and i; there is one Ï† function at the exit, for variable s. The former are called
entry Ï†-functions and the latter is called exit Ï†-function of the loop [24]. All loops, either regular or
irregular (e.g., while loops with breaks), have such pairs of Ï†-functions.

4.2 Notations in Ï†-Calculus

Inspired by SSA, Ï†-calculus uses Ï† functions and other notations to symbolically represent conditional
statements and loops:

â€¢ Ï†(a1, a2, Â· Â· Â· , ak): the standard Ï† function in SSA. Numerical subscripts are sometimes added to a

standard Ï† function to distinguish Ï† functions that have diï¬€erent conditions.

â€¢ Ï†Li(a1, a2, Â· Â· Â· , ak) & Ï†Li(cid:48)(a1, a2, Â· Â· Â· , ak): the entry and exit Ï† functions of loop i.

â€¢ Li

u < S > : the computations in a statement S are surrounded by loop i with u iterations.
Sometimes, the loop ID (i) is used in S to also denote the iteration number, which, by default, goes
from 0 to u âˆ’ 1. In the representation of a loop for symbolic diï¬€erentiation in coarsening, u can be
a constant, an expression, or a symbol. For irregular loops (e.g., the while loop in Figure 6), for
instance, u is a symbol in the expression that is symbolically diï¬€erentiated, and its value is recorded
in the execution of the primal code. In the following discussion, unless necessary, we omit u and/or
i in the loop notations for better readability.

â€¢ (cid:80), (cid:81): the standard math notations of summation and product.

â€¢ Ï†Li

(j): the instance of Ï†Li in the jth iteration of loop i

â€¢ a(j): if a is an expression in the argument list of Ï†Li, a(j) represents the value of a after j iterations

of loop i.

â€¢ aexit(L): the value(s) of a at the exit of loop L.

7

Figure 3: Illustration of SSA and Ï†-calculus on an example with conditional statements

Figure 4: Illustration of SSA and Ï†-calculus on a simple loop

8

// x: the active variable   1   d = p + qÂ·x   2   if ( d > 0)   3      a = d0.5   4      if (a > 50)   5         z = a   6      else   7         z = sin(a)   8   else   9      z = ln(1+ed) 10   L = z(1+ed)// x: the active variable   1   d = p + qÂ·x   2   if ( d > 0)    3      a = d0.5   4      if (a > 50)   5         z1 = a   6      else   7         z2 = sin(a)   8      z3 = É¸2(z1, z2)   9   else  10     z4 = ln(1+ed) 11   z5 = É¸1(z3, z4) 12   L = z5(1+ed) L = É¸1(É¸2((p+qÂ·x)0.5, (p+qÂ·x)sin((p+qÂ·x)0.5)), ln(1+e(p+qÂ·x)))(1+e(p+qÂ·x)) dL/dx = qÂ·e(p+qÂ·x)É¸1(É¸2( (p+qÂ·x)0.5, (p+qÂ·x)Â·sin((p+qÂ·x)0.5)), ln(1+e(p+qÂ·x)))             + É¸1(É¸2(0.5Â·q(p+qÂ·x)-0.5, qÂ·sin(p+qÂ·x)0.5+(p+qÂ·x)Â·0.5Â·qÂ·(p+qÂ·x)-0.5cos(p+qÂ·x)0.5), qÂ·e(p+qÂ·x)/(1+e(p+qÂ·x))) (1+e(p+qÂ·x))(a) original code(b) SOIs segmented by branches(d) SSA-based closed-form expression(e) symbolic diï¬€erentiation result on the closed form dL/dx = qÂ·e(p+qÂ·x)É¸1(É¸2( (p+qÂ·x)0.5, (p+qÂ·x)sin((p+qÂ·x)0.5)), ln(1+e(p+qÂ·x)))             + É¸1(É¸2(0.5(p+qÂ·x)-0.5, sin(p+qÂ·x)0.5+0.5(p+qÂ·x)0.5cos(p+qÂ·x)0.5) q(1+e(p+qÂ·x)), qÂ·e(p+qÂ·x))(f) simpliï¬ed symbolic diï¬€erentiation result on the closed form// x: the active variable SOI-1:   d = p+qÂ·x SOI-2:   a = d0.5 SOI-3:   z=a SOI-4:   z=sin(a) SOI-5:   z=ln(1+ed) SOI-6:   L=z(1+ed)(c) code in SSA// x: the active variable   1  s = a   2  for (i=0; i<k; i++)   3      s = s + x// x: the active variable   1          s1 = a   2          i1 = 0   3          if (i1 < k)   4  L1:     s2 = É¸L1 (s1, s3)   5             i2 = É¸L1 (i1, i3)   6             s3 = s2 + x   7             i3 = i2 + 1   8             if (i3<k) goto L1   9  L2:  s4 = É¸L1â€™ (s1, s3)(b) SSA form(a) Original codes3 = s2 + x s3 =  É¸L1 (s1, s3) + x s3 =  É¸L1 (a, s3) + x     = a + k*x s4 = É¸L1â€™ (a, s3exit)     = É¸L1â€™ (a, a + k*x)     = a + k*x(d) Get a closed forms3exitğ”ğ”ğ”(ğ”:  represents a loop)s4 = É¸L1â€™ (a, s3exit) s3 =  É¸L1 (s1, s3) + x ğ”(c) Symbolic representationFigure 5: Main formulae in Ï†-calculus.

â€¢ f [n]: recursively apply function f for n times.

This set of notations are simple extensions of the standard Ï† function. But with them, code with
complex control ï¬‚ow can now be symbolically expressed. Figure 3(d), for instance, shows the symbolic
form of the computation of L by the code in Figure 3(a). Figure 4(c) shows the computation of s by
the loop in Figure 4(a). The derivation of them involve just direct substitutions of names with their
corresponding expressions; the Ï†-notations oï¬€er ways to symbolically represent the eï¬€ects of loops and
conditional statements.

Note that unlike the form in Figure 3(d), in Figure 4(c) is not yet a closed form: There is still the
presence of L. Even for the closed form in Figure 3(d), it still contains Ï† functions. The other component
of Ï†-calculus, formulae in Ï†-calculus, oï¬€ers the facilities for (i) getting closed forms by removing L and
(ii) diï¬€erentiating expressions involving Ï† functions.

4.3 Formulae in Ï†-Calculus

Figure 5 provides the core set of formulae in Ï†-calculus; at the top of the ï¬gure are ï¬ve fundamental
formulae and at the bottom are nine useful corollaries derived from the fundamental formulae. Most of
the formulae are quite straightforward, but when being used together, they are powerful in getting rid
of Ï† and loop notations from the symbolic representation of code. We next explain each of the formulae
and provide brief proofs.

4.3.1 Fundamental Ï† Formulae

1) Identity Formula (F1 in Figure 5). The identity formula says that if all the arguments in a Ï†
function equal to one another, the Ï† function can be replaced with any of its argument. It immediately
follows the deï¬nition of the Ï† function.

2) Distributive Formula (F2 in Figure 5). This formula says that a function that applies to a Ï†
function can be distributed to each of the arguments of that Ï† function. It can be easily proved based
on the deï¬nition of Ï† function.

9

(F1) Identify formula:(F2) Distributive formula:(F3) Commutative formula:(F4) Loop entry formula:(F5) Loop exit formula:if  bi=bj (1â©½i,jâ©½m) & b1(0)=a      É¸L'(a,b1,b2,...,bm) = b1exitÉ¸(a, a, ..., a) = a f(É¸(a1, a2, ..., an)) = É¸(f(a1), f(a2), ..., f(an))É¸(a, b) = É¸(b, a) f(x1, ..., xi-1, É¸(a,b), xi, ..., xk)  = É¸(f(x1, ..., xi-1, a, xi, ..., xk), f(x1, ..., xi-1, b, xi, ..., xk))COROLLARIESğ”nL d = a É¸L(p, d) + b   =>  dexit(L) = anp + b âˆ‘n-1i=0 aiğ”nL d = a É¸L(p, d) + b[i]   => dexit(L) = anp + âˆ‘n-1i=0 aib[n-1-i]ğ”nL d = a[i] É¸L(p, d) + b[i] => dexit(L) = p(ğ€n-1i=0 a[i])        + âˆ‘n-1i=0  b[n-1-i]ğ€ij=0 a[n-1-j]FUNDAMENTAL FORMULAEğ”nL d = a(É¸L(p, d))b => dexit(L) = ab+n-1pbnğ”nL d = f(É¸L(p, d))    =>  dexit(L) = f[n](p)(C2)(C3)(C4)(C1)(C5)(C6)(C7)(C8)(C9)(i-1)3) Commutative Formula (F3 in Figure 5). This formula shows the relationship between a Ï†
function and its complement.
In the formula, Ï† is the complement of Ï†, that is, it chooses the ï¬rst
argument when Ï† chooses the second, and the second when Ï† chooses the ï¬rst. The correctness of this
formula immediately follows the deï¬nition.

4) Loop entry formula (F4 in Figure 5). This formula shows the inherent property of a loop-entry
Ï† function. For the deï¬nition of a loop-entry Ï† function, Ï†L is reached always through the back edge
of loop L except for its ï¬rst instance in that loop. The formula hence follows. This simple formula is
essential for Ï†-calculus to deal with loops as shown later.

5) Loop exit formula (F5 in Figure 5). This formula says that Ï†L(cid:48)(a, b1, b2, Â· Â· Â· , bm) equals the
value of b1 at the exit of loop L if (i) the value of all arguments, except the ï¬rst, of Ï†L(cid:48) are the same
at Ï†L(cid:48), and (ii) those arguments before the entry point of the loop have the value equaling the ï¬rst
argumentâ€™s value a. Its correctness can be easily proved with the identity formula. Notice that the only
time when Ï†L(cid:48) takes its ï¬rst argument is when the entire loop is skipped, in which condition, according
to (ii), b1exit equals a; in any other condition, Ï†L(cid:48) must take one of the other arguments, the value of
which at the exit of the loop, according to (i), must equal b1exit. This formula is useful for removing loop
exit Ï† functions in the application of Ï†-calculus as shown later.

4.3.2 Corollaries

At the bottom of Figure 5 are some of the corollaries attained from the fundamental formulae. They
provide facilities for transforming and simplifying Ï† expressions.

The corollaries are in two groups. The ï¬rst group consists of C1 to C4. These corollaries oï¬€er
conveniences for symbolic diï¬€erentiation, optimizations, and code generations, reducing computations
and code size. Corollary C1 can be easily derived from the distributed formula through currying. Corollary
C2 follows corollary C1 when we substitute f with partial derivative. Corollary C3 follows corollary C1
when we substitute f with the Ï† function. Corollary C4 is attained when we apply C1 and then the
identity formula (F1).

The other group consists of corollaries C5 to C9, which oï¬€er conveniences for transforming Ï† expres-

sions into closed forms for symbolic diï¬€erentiation.

Corollary C5 is proved as follows.

Proof. Because of F4, we have the following relations:

d(2) = f (Ï†(2)

d(3) = f (Ï†(3)

d(1) = f (Ï†(1)
L (p, d)) = f (p)
L (p, d)) = f (d(1)) = f (f (p)) = f [2](p)
L (p, d)) = f (d(2)) = f (f [2](p)) = f [3](p)
...

d(n) = f (Ï†(nâˆ’1)

L

(p, d)) = f (d(nâˆ’1)) = f (f [nâˆ’1](p)) = f [n](p)

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

Because of the deï¬nition of SSA, after the loop entry function Ï†L in the ï¬nal iteration of loop L,

there shall be no other assignment to d before the exit of the loop. Hence, dexit(L) = d(n) = f [n](p).

Corollaries C6 to C9 are variants of C5 with function f instantiated in several forms. They can be

proved in a way similar to C5.

4.4 Examples

We now use several examples to show how Ï†-calculus helps symbolic diï¬€erentiation. We start with two
simple ones and end with a more complicated case with nested loops, breaks, if-else, and arrays.

(I) If-Else Example. We ï¬rst look at the example in Figure 3. The Ï† functions resolve the diï¬ƒculty
for getting a closed form for the code. With the code in SSA, the derivation of the closed form for the code

10

can simply ignore the conditional statements. What it needs to do is only to apply simple substitution
of names with corresponding expressions based on the data ï¬‚ow. Figure 3(d) shows the closed form
obtained from the SSA form in Figure 3(c). We add subscripts to the Ï† functions to help tell diï¬€erent Ï†
functions apart.

According to corollary C2, we can apply derivation on x on the closed form and distribute the operation
to the arguments of the Ï† functions. The result is shown in Figure 3(e). The underlines indicate two
simpliï¬cation opportunities. (i) The ï¬rst underlined expression, (p+qx)âˆ—0.5âˆ—qâˆ—(p+qx)âˆ’0.5, can be easily
simpliï¬ed by symbolic engines into 0.5q(p + qx)0.5. (ii) The second simpliï¬cation opportunity appears
after the distributive formula (F2) is applied such that the ï¬nal term (1 + ep+qx) in the expression
in Figure 3 (e) is distributed into the Ï† functions. That term cancels the denominator of the second
underlined expression. Figure 3 (f) shows the result after symbolic simplication. It is worth noting that
such optimization opportunities appear because of Ï†-calculus: They are both about interactions of the
codelets across the boundaries of conditional branches, and hence would need the involved computations
to be treated together. If each straight-line section of the codelet is symbolically diï¬€erentiated individually
as shown by the segments of interest (SOIs) in Figure 3, those simpliï¬cations cannot get exposed. From
Figure 3(f), code can then be generated with the Ï† functions materialized with conditional statements
that check the corresponding branching decisions recorded during the primal computation.

(II) Simple Loop Example. Figure 4 (d) shows how Ï†-calculus helps produce a closed form for
the loop in Figure 4 (a). With Ï†-calculus notations, the loop is symbolically represented in Figure 4
(c), on which, corollary C6 removes the loop notation and the loop entry Ï†, and produces the closed
form of s3 at the exit of the loop: a + k Ã— x (k is the loop trip count). The application of loop exit
formula F5 to the expression of s4 removes the loop exit Ï† function, producing the simple expression
a + k Ã— x. Symbolic diï¬€erentiation can then be applied easily. For illustration purpose, this loop is made
simple and the derivation of the closed form may resemble the recognition of induction variables in loop
parallelizations [31]. The next example gives a more thorough demonstration of the power of Ï† calculus.
(III) Complex Example (BGDHyperOpt). Figure 6 shows a more complex example. The code
in Figure 6 (a) implements the use of batch gradient descent to determine the linear model on a dataset
(x for inputs, y for response). The diï¬€erentiation of interest is d(err)/dr, where r is the learning rate;
this gradient can be used in ï¬nding out the best learning rateâ€”a so-called meta learning problem that
optimizes hyperparameters of a machine learning process.

The code consists of a for loop nested within a while loop; the while loop has a break in an if-else
statement; there is another for loop following the while loop. To our best knowledge, no prior work can
compute d(err)/dr symbolically due to the control ï¬‚ow complexities.

Figure 6(b) shows the SSA form of the codelet. It includes nine Ï† functions; one of the loop exit Ï†

functions (Ï†k(cid:48)) has three arguments because of the break statement in the while loop.

Figure 6(c) shows the application of Ï†-calculus with the text boxes indicating the formulae or corol-

laries used at the important steps. We explain the process as follows.

inner for loop; Formula F5 then resolves the Ï†(cid:48)

Lines 1-5: Corollary C7 helps attain the closed-form expression of the value of d3 at the exit of the
i function and leads to the closed-form expression of d5.
Lines 7-10: This part tries to get the closed-form expression for the third argument (w3) of the Ï†k(cid:48)
function on Line L6 in Figure 6(b). The part starts with a series of substitutions based on the results
from Lines 1-5 in Figure 6(c), and then uses corollary C6 to get the closed-form expression of the value
that w3 has at the normal (rather than via break) exit of the while loop.

Lines 12-16: This part tries to get the closed-form expression for the second argument (w2) of Ï†k(cid:48).
It starts with substitutions with the results obtained already. It then uses the distributive formula F2 to
transform the Ï†k function on Line 14 in Figure 6(c) to a form matching the lhs form in corollary C6. The
transformation is to factor out the terms in the second argument of Ï†k such that the second arguement
turns into pure w2 as shown on Line 15. Then corollary C6 can be applied, resolving the loop notation
and also the Ï†k function and producing the closed-form expression of w2 at the exit of the while loop as
shown on Line 16 in Figure 6(c) (K stands for the trip count of the while loop).

Lines 17-18: simple substitutions of the three arguments in Ï†(cid:48)
k.
Lines 20-24: This part tries to get the closed-form expression for e4. It ï¬rst applies corollary C7 to
the expression on Line 20 in Figure 6(c) to resolve Ï†j and the loop notation, producing the closed-form

11

expression for e3exit on Line 21.
closed-form expression on Line 24 for e4.

It then applies formula F5 to resolve Ï†(cid:48)

j on Line 22, producing the

Line 26: A simple substitution with the results produced so far gives the closed-form expression
of the ï¬nal variable err. (For the sake of readability, we leave out the substitution of w4.) Symbolic
diï¬€erentiation can then be applied to err on r.

This part has demonstrated the applications of Ï†-calculus to several concrete examples. We will

present the general use of Ï†-calculus in the overall algorithm of coarsening in the next section.

5 SOI Identiï¬cation

With Ï†-calculus removing the barriers of control ï¬‚ow for coarsening, a segment of interest (SOI)â€”that is,
the segment of code for symbolic diï¬€erentiationâ€”of a program can be much larger than a basic block. A
larger SOI often oï¬€ers more opportunities for optimizations, but it is not always better due to a tradeoï¬€
caused by two factors.

The ï¬rst is â€expression swellâ€. As aforementioned, â€expression swellâ€ refers to the phenomenon that
the derivative often has a much larger (in the worst case, quadratically larger) representation than the
original function has [5]. As a result, a very long expression can cause large memory usage and long
running time of symbolic engines.

The second factor is the tension between coarsening and computation reuse. In coarsened diï¬€erentia-
tion, without a careful design, some computation reuse opportunities could get lost due to computation
reordering caused by coarsening transformations, a phenomenon we call reuse deprivation.

Deprivation Example. The impact of reuse deprivation can be seen on xt+2,0 and xt+2,1 in the
CartPole example in Figure 2. As Figure 2(c) shows, they are both computed from xt+1,1. So po-
tentially, if d(xt+2,0/d(at) has been computed, d(xt+1,1)/d(at) could be known and could be reused in
computing d(xt+2,1/d(at). Coarsening the computations from at and Xt (i.e., [xt,0, xt,1, xt,2, xt,3]) to
xt+2,0, however, deprives that reuse opportunity. The coarsening result has been shown in Equation 3.1,
in which the holders of intermediate results, such as xt+1,1, disappear. The diï¬€erentiation over at is
shown in Equation 3.2, which has no d(xt+1,1)/d(at) or the derivatives of any other intermediate vari-
ables over at. As a result, when we need to compute the derivative of xt+2,1 over at, we cannot reuse
those intermediate derivatives.

The example illustrates a tension between reuse and coarsening. The larger is the coarsening granu-
larity, the more opportunities there are for the enabled symbolic diï¬€erentiation and optimization to take
eï¬€ect, but at the same time, it could incur deprivation of computation reuse opportunities.

What adds subtly to the relation is that reuse deprivation does not always lead to fewer reuse opportu-
nities. Some reuse deprivations transform the reuse opportunities to another form. For instance, suppose
that we have a way to get a closed form for the entire computation from Xt to L. All reuses, including
xt+1,1 for xt+2,0 and xt+2,1, turn into explicit sub-expressions in the closed form of L. There can hence
be reuse opportunities exposed between them in the diï¬€erentiation of the closed-form expression. The
condition for such a transformation of reuse to occur is that the closed-form expression must subsume
both parties that contain the reusable computations.

A single solution, reuse-aware SOI identiï¬cation, addresses both factors. Reuse-aware SOI identiï¬ca-

tion refers to an algorithm that solves the following optimization problem:

Deï¬nition 2. Optimal SOI segmentation problem: Let G be a series of computations, l be the
upper limit of the allowed sizes of an SOI, P be the set of valid partitions of G, that is, for any partition
S in P , no element in S is larger than l. The problem is to ï¬nd the optimal partition Sâˆ— âˆˆ P such that
the total running time is minimized, that is,

âˆ€Q âˆˆ P, ad(Sâˆ—) +

(cid:88)

sâˆˆSâˆ—

compute(dif (s)) â‰¤ ad(Q) +

compute(dif (q)),

(cid:88)

qâˆˆQ

12

Figure 6: Application of Ï†-calculus on hyperparameter optimizations, showing the treatment of loops
and branches.

13

          w1 = 0;
k1 = 0;
          if (k1 < T) 
          w2 = É¸k(w1, w3);           k2 = É¸k(k1, k3);           k3 = k2 + 1;           d1 = 0;
          i1 = 0;
          if (i1 < M)
          d2 = É¸i(d1, d3);
          i2 = É¸i(i1, i3);
          d3 = d2 + 2x[i]*y[i]  - 2*x[i]*x[i]*w2;
          i3 = i2 + 1;
          if (i3 < M)     goto L4
          d4 = É¸iâ€™(d1, d3);
          d5 = d4/M
          if (d5 < 0.001)
              goto L6;
          else
              w3 = w2 - r * d5;
          if (k3 < T)   goto L3;
          w4 = É¸kâ€™(w1, w2, w3);           e1 = 0
          j1 = 0
          if (j1 < M) 
          e2 = É¸j(e1, e3);           j2 = É¸j(j1, j3);
          e3 = e2 + (y[j] -x[j] * w4)2
               j3 = j2 + 1
          if (j3 < M)      goto L7
          e4 = É¸jâ€™(e1, e3)
          err = e40.5/M 1  
2 
3
L3: 5
6
7
8
9
L4: 11
12
13
14
L5: 16
17
18
19
20
21
L6: 23
24
25
L7: 27
28
29
30
L8: 32for
loopwhile
loopfor
loop(a) Original code(b) SSA form//  r: the learning rate source (active variable) //  err: the error; sink (output variable) 1   w=0
2   k = 0
3   while (k<T)
4      k++
5      d=0
6      for (i=0; i< M; i++)
7          d += 2*x[i]*(y[i] - x[i] * w)
        d = d/M
8      if (d<0.001)
9          break
10    else
11        w = w - r * d
12  e = 0
13  for (j=0; j<M; j++)
14     e += (y[i] - x[i] * w)2
15  err = (e0.5)/Mğ”i d3 = É¸i(0, d3) + 2x[i]*y[i] - 2*x[i]*x[i]*w2
d3exit = âˆ‘i 2x[i]*y[i] - 2*x[i]*x[i]*w2
d4 = É¸iâ€™(d1, d3) = âˆ‘i 2x[i]*y[i] - 2*x[i]*x[i]*w2      =  2Sxy- 2SX2*É¸k(0,w3)
d5 = (2Sxy- 2SX2*É¸k(0,w3))/M
ğ”k w3 = w2 - r*d5
ğ”k w3 = É¸k(0, w3) - 2rSxy/M + 2rSX2*É¸k(0,w3)/M
ğ”k w3 =  (1+2rSX2/M)*É¸k(0, w3) - 2rSxy/M
w3exit = -2(r/M)Sxy âˆ‘K-1k=0(1+2(r/M)Sx2)k ğ”k w2 = É¸k(0, w3) 
ğ”k w2 = É¸k(0, w2-2(r/M)*(Sxy- SX2*w2))
ğ”k w2 = É¸k(0, (1+ 2(r/M)SX2)w2-2(r/M)*Sxy)
ğ”k w2 = (1+ 2(r/M)SX2)*              É¸k(2(r/M)*Sxy/(1+ 2(r/M)SX2),w2) - 2(r/M)*Sxy
w2exit = -2(r/M)Sxy âˆ‘K-2k=0(1+2(r/M)Sx2)k Distributive (F2) Corollary (C7) (c) Application of É¸-calculus ( and the formula used in boxes )Corollary (C7)w4 = É¸kâ€™(w1, w2exit, w3exit)
        = É¸kâ€™(0, -2(r/M)Sxy âˆ‘K-2k=0 (1+2(r/M)Sx2)k, 
                  -2(r/M)Sxy âˆ‘K-1k=0 (1+2(r/M)Sx2)k) ğ”j e3 = É¸j(0, e3) + (y[j] -x[j] * w4)2
e3exit = âˆ‘j (y[j] - x[j]*w4)2 
e4 = É¸iâ€™(e1, e3) = âˆ‘j (y[j] - x[j]*w4)2
     = âˆ‘j y[j]2 - 2x[j]y[j]*w4+x[j]2*w42
     = Sy2 - 2Sxy *w4+ SX2*w42
err = (Sy2 - 2Sxy *w4+ SX2*w42)0.5/M           1   2  3 4 5612 13 14 15 1617 18 19 20 2122 23 2425 26 7 8 9 10 Formula (F5)  Corollary (C6)  Corollary (C6)  Formula (F5) Sxy = âˆ‘i x[i]*y[i];                Sx = âˆ‘i x[i];               Sy = âˆ‘i y[i];              SX2 = âˆ‘i x[i]2;                 Sy2 = âˆ‘i y[i]211where compute(dif (x )) is the amount of computation involved in running the symbolically diï¬€erentiated
code segment for code x, and ad(X) is the cost of the remaining AD diï¬€erentiation of X after symbolic
diï¬€erentiation.

The upper bound of SOI size l in the problem description ensures that the symbolic engine works well
even in the presence of the â€expression swellâ€ eï¬€ects. The problem description indicates three factors
relevant to SOI deï¬nitions.

1) The cost ad(X). This cost is incurred at the boundaries of SOIs. Symbolic diï¬€erentiation of an
SOI computes only the derivatives of the active output variables of this SOI on the active input variables
of this SOI. These derivatives have to be connected into a chain by AD to compute the derivatives of the
ultimate active output variables on the ultimate active input variables. As a result, the more SOIs there
are, the more AD overhead is there, and the larger is ad(X). In the extreme case where each operation is
an SOI, ad(X) would equal to the cost taken by the default AD without coarsening. So this factor calls
for larger SOIs.

2) Computation simpliï¬cations. The cost (cid:80)

xâˆˆX compute(dif (x )) is smaller if more computations are
simpliï¬ed. As two cancellable computations falling into two separate SOIs are not going to get cancelled
by the symbolic engine, maximization of simpliï¬ed computations also calls for larger SOIs.

3) Computation reuse. Maximizing computation reuse helps reduce the cost (cid:80)

xâˆˆX compute(dif (x ))
as well. Unlike computation simpliï¬cation, computation reuse exists both within and across SOIs. Ex-
ploiting reuse within an SOI happens in the default symbolic optimization and code optimization (e.g.,
common subexpression elimination (CSE) [9]). Reuse across SOIs is the natural result of the chain rules
of diï¬€erentiation, as shown by the potential reuse of d(xt+1,1)/d(at) in computing d(xt+2,0)/d(at) and
d(xt+2,1)/d(at) in Figure 2. In general, if y is an active output variable of both SOIa and SOIb and it
is also an active input variable of SOIc, then the derivative of the ultimate active output z on y (dz/dy)
can be used in computing the derivatives of z on the active inputs of both SOIa and SOIb. Besides
saving computations, reuses are also helpful for mitigating the â€œexpression swellâ€ problem as they split a
long expression into shorter ones as noted in some previous work [35, 21]. Because expanding SOIs could
lose inter-SOI reuses as the deprivation example has shown, this factor suggests that simply maximizing
SOIs to the upper limit l cannot always give the best SOIs.

Finding the optimal SOI segmentation is diï¬ƒcult. For an SSA representation with N instructions,
assuming every instruction can be put into an SOI, the number of possible partitions is between lN/l
and lN , where, l is the upper limit of the allowed size of an SOI. The reason is that the number of
SOIs is between N/l and N , while the size of an SOI has l possibilities in the lower-bound case and
up to l possibilities in the upper-bound case. Besides the exponential space, it would require detailed
performance and overhead modeling, which is hard to be precise at static compile time.

It is however important for a viable solution to take all these factors into consideration. Figure 7(a)
outlines our designed algorithm. For a given function f , for each of its active variable s that outlives
f , the algorithm gets its def-use chain, which captures all the deï¬nitions in f that lead to the value of
s. It then builds a def-use region tree out of all the deï¬nitions on the def-use chain. Def-use region tree
is a data structure inspired by the classic code region hierarchy in compilers [9]. Traditionally, a code
region is deï¬ned as a collection of nodes N and edges E such that (i) a header node h in N dominates
all other nodes in the collection; (ii) if p is in N , then m must be in N if m reaches p without going
through h; (iii) E includes all edges between nodes in N , except for those that enter h. In the code region
hierarchy of a program, each code region is represented by a node in the hierarchy subsumed under the
nodes that represent its enclosing code regions. Def-use region tree has two major diï¬€erences from code
region hierarchy: (i) only relevant variable deï¬nitions are considered; (ii) every loop-exit Ï† function is
put as part of the region of the associated loop. The second property is for convenience in the derivation
of symbolic expressions for loops. As an example, Figure 7(b) shows the def-use region tree of all the
deï¬nitions on the def-use chain of err in Figure 6(b); each element in the boxes is a line number in
Figure 6(b). The three solid (green) boxes are the three loops. The three loop-exit Ï† functions (Lines
L5, L6, L8 in Figure 6(b)) are put together with the three loops respectively.

The SOI identiï¬cation algorithm then traverses the region tree in a bottom-up order, as outlined in
Figure 7(a). For each node, if it has no large child node (i.e., exceeding the SOI size limit), its symbolic
expression is derived through the Ï†-calculus. If the size of the derived expression exceeds the SOI size

14

Figure 7: (a) Algorithm of reuse-aware identiï¬cation of SOIs for coarsening. (b) The def-use chain of err
in the example shown in Figure 6(b) with r as the active input; inactive inputs (x, y, M ) are omitted.
(c) The def-use region tree for err in Figure 6(b); each element in a box is a line number in Figure 6(b);
solid (green) boxes stand for loops.

limit, that node is marked as a large node, and if it is a leaf node, it is split into two smaller nodes; the
splitting point is chosen to be the variable that is contained in that node and has the largest number
of references (and hence reuses) in f . The two new nodes created by the split are added to the front of
worklist. If the current node has large children, there is no need to go up further in the def-use region
tree as the upper nodes can only become even larger. In that case, the algorithm examines the immediate
children of this node, merge consecutive small children nodes (up to the SOI size limit); after that, it
puts each of the children nodes smaller than the limit as an SOI. The algorithm continues until worklist
becomes empty. The size limit L can be empirically selected based on the machine and the symbolic
engine.

The algorithm follows the principle of maximizing the size of SOIs within the SOI size limit while

respecting reuses when it is necessary to split a def-use chain into multiple SOIs.

6

Implementation

We implement the coarsening optimization on an in-house AD tool named Diï¬€Kt. The tool was developed
for Kotlin, a cross-platform, statically typed, general-purpose programming language with type inference.
The tool itself is also written in Kotlin. We choose it as the basis mainly because of its availability and the
statically typed nature of Kotlin which oï¬€ers conveniences for static code analysis and transformations.
But as a general optimization technique, coarsening can be potentially applied to many other AD tools;
for some (e.g., Python AD tools), it may need to be done dynamically.

Diï¬€Kt was developed and optimized by 10+ engineers in industry in over a year. It supports both
CPU and GPU, with high-performance native math/DNN libraries for Tensor computations. The tool is
planned to open source in the near future. A systematic benchmarking of the tool over other AD tools
is yet to be done, but preliminary measurements show that it outperforms PyTorch AD [25] by over 10Ã—
on scalar-intensive cases (e.g., HookeanSpring in Section 7), and achieves comparable speeds on common
deep learning models where pre-existing libraries are called for gradients calculations of the standard

15

erre4e1e3e2w4w1w2w3d5d4d3d1d2rroot1,2L3, 7, L4, 12, L5, 16, 20, L6 L3, 7L4, 12, L516, 20L6 L4, 12L523 L7, 28, L8 L7, 28L8 32 L: the upper limit of the size of an SOI
f: a function in SSA
S: the set of active sink variables in f
SOI: the placeholder of all SOIs of f
1.  SOI = { }
2.  for each s in S
3.     C = f.getDefUseChain(s)
4.     T = f.getRegionTree(C)
5.     W: a worklist with all nodes in T added in 
6.          bottom-up order
7.     while (W.notEmpty)
8.         n = W.removeANode( )
9.         if (n.hasLargeChildren( ))
10.           n.markLarge( )
11.           n.mergeSomeChildren( )
12.           SOI.add(all small children in n)
13.           next;
14.       e = n.getSymbExp( )
15.       if (e.size > L) 
16.          n.markLarge( )
17.          if (n.isLeaf( ))
18.             newNodes = n.splitOnReuses( )
19.             W.addToFront(elements in newNodes)(a)(b)(c)Figure 8: An example showing how the coarsened results are integrated into the original program. The
original primal code (left) is transformed to a form (right) such that the calculations of z in the SOI are
put into one expression, and an adjoint is appended to that expression, which, at runtime, makes the
AD take its content as the shortcuts for calculating the derivatives of z on the two active variables of the
SOI, w and v. The results are used by the default AD in diï¬€erentiating the rest of the code (i.e., from w
and v to x and y.

DNN layers under the hood of both of them. (As PyTorch AD and the Kotlin AD tool are in diï¬€erent
programming languages, the comparison is only to give readers a sense about the industrial quality of
the default tool.)

Similar to many other AD tools (e.g., PyTorch [25], JAX [12]), Diï¬€Kt is a library-based implementa-
tion, enabling AD through operator overloading via a generic class Tensor. Implemented in 250K lines
of code, it supports backward AD and includes a Tensor typing module as well.

As with other Automatic Diï¬€erentiation (AD) tools (e.g., Zygote [19]), Diï¬€Kt also allows the use of
adjoints for custom diï¬€erentiation. For a given expression e, if a custom diï¬€erentiation of e is provided,
the AD process will automatically invoke it rather than conduct the default operation-by-operation dif-
ferentiation. The diï¬€erentiations of the SOIs generated by coarsening are integrated into the original
program as custom adjoints. An example is shown in Figure 8. The coarsening results form the content
of the adjoint, which is associated with the coarsened primal expression through the call â€setIntermedi-
ateAdjointsâ€. If coarsening produces code for diï¬€erentiating the entire primal code, the compiler simply
replaces the calls of the corresponding backward function with the generated code; if the compiler in
addition determines that the primal results are used in the program only for getting the derivatives, the
compiler removes the invocations of the primal code.

Our implementation of coarsening on Diï¬€Kt is based on a Kotlin compiler. Our experiments focus on
ï¬rst-order backward AD, but it is worth noting that coarsening, as a way to oï¬€er shortcuts in AD, can in
principle help higher-order diï¬€erentiation and forward or mixed-direction AD as well. In addition to the
Ï†-calculus and the SOI identiï¬cation as presented earlier, our implementation also includes loop unrolling
and the use of the primal computation results in the generated adjoint functions when possible. As a side
beneï¬t of coarsening, our implementation of coarsening optimizes the primal in addition to the diï¬€eren-
tiation: After getting the closed form of the primal computation, it applies symbolic optimization to the
primal and regenerates the code; an example is shown in the BGDHyperOpt benchmark in Section 7.2. In
the case where only diï¬€erentiation is needed and the entire primal can be symbolically diï¬€erentiated, the
optimizer removes the primal from the program when possibleâ€”an optimization elusive to existing AD.
In the current implementation, for proof of concept, we use a symbolic engine extended from Sympy [4],
an open-source symbolic manipulation tool.

Like many other compiler-based optimizations that change the order of computations, coarsening could
aï¬€ect the numerical precision. A convenience oï¬€ered by coarsening is that measures to avoid numerical
unstableness can be seamlessly integrated into the code generation in coarsening. The code generator is
equipped with the patterns for dealing with common numerically unstable expressions. Before it generates

16

fun cubeTransformed() {
        val x = Tensor(5f).asVar()
        val y = Tensor(3f).asVar()
        val w = x - 2f * y
        val v = y * x - x
        ... 
        val z = (w * w * w - v * v * v).setIntermediateAdjoints(
                sequenceOf (
                        w to w * 2f ,   // dz/dw from coarsening
                        v to v * -2f     // dz/dv from coarsening
                )
        )   // using the coarsened results to compute 
            // derivatives of z w.r.t. w & v
        z.backward()
    }fun cube() {
        val x = Tensor(5f).asVar()  // an active variable
        val y = Tensor(3f).asVar()  // an active variable
        val w = x - 2f * y
        val v = y * x - x
        ...   // other code
        val z1 = w * w * w
        val z2 = v * v * v
        val z = z1 - z2
        z.backward()   // get derivatives of z w.r.t x & y
    }
SOIOriginal code with ADTransformed code after coarseningTransformationTable 1: Benchmarks and Conï¬gurations

Name

Domain

BGDHyperOpt Meta-

Brachist.

CartPole

HMC

HookeanSpring

Learning

Math.
Physics

Deep
Reinforcement
Learning
Statistic
Sampling
for Prob. Prog.
Physical
Simulation

QWOP

Gaming

Description
Optimizing the learning rate
of batch gradient descent
based linear regression
Brachistochrone curve
calculation

Training a CartPole system

Hamiltonian Monte Carlo
Sampling for logistic
regression
Simulating the dynamics of a
Hookean Springs system

An avatar learns walking
via motion optimization

Conï¬gs
200 data records
1
1000 data records
2
2000 data records
3
200 data points
1
400 data points
2
1000 data points
3
an update every 6 steps
1
an update every 8 steps
2
an update every 10 steps
3
100 one-dim records
1
1000 two-dim records
2
800 three-dim records
3
10 vertices
1
20 vertices
2
40 vertices
3
light-weight ï¬gure
1
2 medium-weight ï¬gure
3

heavy-weight ï¬gure

Machine Conï¬guration
devServer

Intel(R) Xeon(R) Gold 6138 40-core CPU 2.00GHz, 250GB, CentOS Stream 8, Kotlin
1.4.20-M1, Java HotSpot 64-Bit Server VM, Java 1.8.0 192

Table 2: Machines

Macbook MacBook Pro, 2.4GHz 8-core Intel Core i9, 32GB 2667MHz DDR4, MacOS Catalina (v.

10.15.7), Kotlin 1.4.20-M1, Graalvm 20.3.0, Java 11.0.9

the code for a symbolic expression, it examines it to identify the numerical unstable expressions through
pattern matching, and generates the code corresponding to their numerically stable forms. For expression
log(1+eâˆ’xÎ²), for example, as the code generator ï¬nds out that the expression matches one of the patterns
in its unstable list, log(1 + en), it generates the code to discern the value of the exponent, as illustrated
as follows (MAXEXP is set to 40 in our implementation):

temp0 = -xÎ²
temp1 = (temp0 Â¿ MAXEXP)? MAXEXP : log(1+etemp0)

When the expression operates on tensors, the generated code uses masking functions (like where in
PyTorch) for eï¬ƒciency. A concrete example of numerically stable code generation in coarsening is the
HMC benchmark detailed in Section 7.2.

7 Evaluation

To evaluate the eï¬ƒcacy of the proposed techniques, we test coarsening on six applications in 18 total
conï¬gurations on two diï¬€erent machines. Backward AD is used. The results show that the optimization
improves the diï¬€erentiation speed by 1.03-27Ã—, and the whole application execution speed by 1.08-11Ã—.

7.1 Methodology

Benchmarks There are no common benchmark suites designed for evaluating AD. We collected six
applications from several domains where AD is important, and implemented them in Kotlin with the
Kotlin AD library. Table 1 lists the set of benchmarks used in the experiments. These benchmarks come
from several domains, from physical simulation to statistical sampling, deep reinforcement learning,
gaming, and meta learning. They also show a range of code complexities, with BGDHyperOpt featuring
control ï¬‚ow complexities as Figure 6 has shown, Brachist.
featuring a case where primal computation
could be potentially removed, CartPole featuring a combination of matrix-based Deep Neural Network and

17

scalar-based environment simulations, HMC featuring potential value overï¬‚ow incurred by exponential
computations, HookeanSpring featuring a sequence of regular vector operations, and QWOP featuring
a long function with many small loops and if-else statements. For each benchmark, we include three
conï¬gurations as listed in the right column of Table 1, which will be explained later in the discussion of
the results of each application. We repeat the performance measurements multiple times and report both
the mean and standard deviation of the timing results. Kotlin runs on Java virtual machines. For both
the baseline and the optimized versions, the JRE went through a warm-up phase before timing starts to
get the stable performance.

Machines Because in practical scenarios, those AD-based applications may run on both servers and
personal computers/laptops, we have measured the performance of the applications on both kinds of
machines. Table 2 provides the machine details.

7.2 Results

In this part, we ï¬rst present an overview of the performance, and then provide detailed discussions on
each benchmark.

Table 3 reports the overall performance, where â€baselineâ€ represents executions of the default Kotlin
AD tool and â€optâ€ represents executions after coarsening is applied. The â€Diï¬€erentiation Timeâ€ column
reports the time taken by diï¬€erentiation in one iteration of each benchmark, while the â€Overall Timeâ€
column reports the overall time of an iteration. We make two observations.

(1) Coarsening brings 1.03â€“27Ã— speedups to the diï¬€erentiation of the benchmarks, and 1.08-11Ã—
speedups to the overall execution. In most cases, the overall speedups are smaller than the diï¬€erentiation
speedups as there are some parts of the computation in the programs outside the part of the code targeted
by the coarsening optimization (i.e., the part involved in diï¬€erentiation). Exceptions are Brachist. and
HookeanSpring; it is because in those two original programs, the only purpose of the primal computations
are to let the AD to compute the gradients. Because coarsening generates code that can directly computes
the gradients, the optimization removes the primal computations completely; the overall time is hence
shortened even more than the time savings on the diï¬€erentiation. (A side observation is that in all cases,
the laptop runs faster than the server, probably due to its faster CPUs and the use of a more recent
version of Java Runtime.)

(2) Coarsening is consistently beneï¬cial; it saves the execution time across benchmarks, conï¬gurations,
and machines. The main reasons for the time savings are three: (i) the savings of the operator overloading
overhead of AD, which comes from object boxing and memory allocations; (ii) the simpliï¬cations of the
computations thanks to the large-scoped symbolic diï¬€erentiation and optimizations; (iii) the removal of
unnecessary primal computations. We next elaborate these beneï¬ts through in-depth examinations of
each of the benchmarks.
BGDHyperOpt. BGDHyperOpt is a meta-learning program and has been introduced in Example III
in Section 4.4 and Figure 6. Meta-learning entails inspecting and optimizing a machine learning process,
which has recently drawing lots of interest. This program tries to optimize learning rates through gradient
descent. The three conï¬gurations correspond to three diï¬€erent sets of inputs.

The SOI in this program is the entire function that computes the learning error as shown in Figure 6(a).
There are nine Ï† functions in the SOI. Despite the control complexities, coarsening is able to get the
closed-form expression for the entire gradient computation and apply symbolic diï¬€erentiation on it, giving
more than 23Ã— diï¬€erentiation speedups in all cases. The primal cannot be removed because the generated
diï¬€erentiation code must use the trip-counts of the while loop in the primal. As a result, the overall
speedup is about 8Ã—. Meanwhile, coarsening saves over 70% of memory allocations because many of the
data allocations in the default AD are for holding intermediate data objects which are no longer needed
after coarsening.

We did an ablation study to examine the beneï¬ts from the Ï†-calculus.

In the study, we apply
coarsening without Ï†-calculus; symbolic diï¬€erentiation is hence applied to only the inner-most loop
(which is written as a single Tensor statement) and the code after the while loop. The diï¬€erentiation
speedups drop from 23-27Ã— to 8-9Ã—:

18

Table 3: Experimental Results: Time per iteration and Speedups
Diï¬€erentiation Time(ms)

Overall Time(ms)

Machine
devServer

Benchmark
BGDHyperOpt

Branchist.

CartPole

HMC

HookeanSpring

QWOP

macBook

BGDHyperOpt

Branchist.

CartPole

HMC

HookeanSpring

QWOP

Conï¬g
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3

opt
baseline
0.20Â± 1%
5.44Â± 6%
0.20Â± 2%
5.22Â± 8%
0.20Â± 2%
5.37Â± 6%
0.04Â± 7%
0.04Â± 3%
0.15Â± 1%
0.19Â± 36%
0.55Â± 2%
0.39Â± 4%
52.15Â± 4% 46.86Â± 1%
15.69Â± 1% 14.51Â± 1%
15.77Â± 2% 14.06Â± 2%
0.67Â± 2%
2.89Â± 5%
1.59Â± 4%
4.55Â± 1%
2.19Â± 5%
5.58Â± 7%
0.01Â± 2%
0.10Â± 4%
0.11Â± 8%
0.03Â± 2%
0.23Â± 12% 0.05Â± 13%
1.97Â± 6%
1.46Â± 4%
25.71Â± 5% 17.81Â± 6%
32.17Â± 5% 21.20Â± 6%
0.14Â± 2%
3.59Â± 4%
0.15Â± 6%
3.59Â± 3%
3.66Â± 4%
0.15Â± 3%
0.04Â± 18% 0.04Â± 12%
0.12Â± 2%
0.13Â± 4%
0.44Â± 2%
0.31Â± 5%
29.46Â± 3% 26.42Â± 2%
8.76Â± 0%
9.43Â± 2%
8.35Â± 1%
9.06Â± 0%
0.59Â± 2%
2.60Â± 1%
1.24Â± 1%
3.88Â± 1%
1.61Â± 1%
4.30Â± 1%
0.01Â± 7%
0.08Â± 8%
0.02Â± 2%
0.09Â± 11%
0.10Â± 2%
0.04Â± 4%
1.16Â± 2%
1.57Â± 3%
20.53Â± 2% 14.70Â± 2%
24.69Â± 2% 16.81Â± 1%

speedup
27.44X
26.18X
26.82X
1.10X
1.22X
1.41X
1.11X
1.08X
1.12X
4.29X
2.86X
2.54X
6.62X
4.09X
4.52X
1.35X
1.44X
1.52X
25.15X
23.43X
24.23X
1.03X
1.08X
1.42X
1.12X
1.08X
1.09X
4.42X
3.13X
2.67X
5.72X
4.17X
2.46X
1.35X
1.40X
1.47X

opt
baseline
1.22Â± 2%
10.44Â± 5%
1.21Â± 2%
10.12Â± 7%
1.21Â± 2%
10.36Â± 6%
0.04Â± 7%
0.09Â± 3%
0.15Â± 1%
0.32Â± 34%
0.69Â± 2%
0.39Â± 4%
55.06Â± 4% 49.38Â± 1%
16.99Â± 1% 15.65Â± 1%
17.07Â± 2% 16.24Â± 2%
0.94Â± 3%
3.26Â± 5%
2.11Â± 4%
5.17Â± 1%
2.79Â± 5%
6.35Â± 7%
0.01Â± 2%
0.16Â± 5%
0.18Â± 8%
0.03Â± 2%
0.38Â± 10% 0.05Â± 13%
3.89Â± 4%
2.76Â± 3%
44.50Â± 3% 29.61Â± 4%
56.86Â± 3% 36.37Â± 4%
0.86Â± 2%
7.23Â± 4%
0.91Â± 4%
7.31Â± 3%
7.41Â± 3%
0.91Â± 4%
0.10Â± 14% 0.04Â± 12%
0.12Â± 2%
0.23Â± 4%
0.55Â± 2%
0.31Â± 5%
31.16Â± 3% 27.92Â± 2%
11.89Â± 2% 11.03Â± 1%
12.14Â± 1% 11.25Â± 0%
0.82Â± 2%
2.90Â± 1%
1.64Â± 1%
4.37Â± 1%
2.06Â± 0%
4.85Â± 1%
0.01Â± 7%
0.14Â± 8%
0.02Â± 2%
0.15Â± 8%
0.17Â± 2%
0.04Â± 4%
2.45Â± 2%
3.60Â± 4%
40.43Â± 2% 26.46Â± 2%
50.01Â± 2% 31.89Â± 1%

speedup
8.52X
8.37X
8.56X
2.51X
2.08X
1.79X
1.12X
1.09X
1.05X
3.46X
2.45X
2.27X
11.02X
6.72X
7.53X
1.41X
1.50X
1.56X
8.41X
8.06X
8.18X
2.38X
1.91X
1.80X
1.12X
1.08X
1.08X
3.56X
2.67X
2.35X
9.84X
7.13X
4.09X
1.47X
1.53X
1.57X

19

Conï¬g
Speedup(X) w/o Ï†-calculus
Speedup(X) w/ Ï†-calculus

devServer
2
7.80
26.18

1
8.46
27.44

3
8.08
26.82

macBook
2
9.34
23.43

1
9.24
25.15

3
9.39
24.23

i x[i] âˆ— y[i] and (cid:80)

It can be seen that the larger scope of optimizations enabled by Ï†-calculus boosts the speedups by a
factor of three. Its eï¬€ects are multi-fold: (i) It allows a complete removal of the boxing overhead of the
Tensor data structure from the diï¬€erentiation process, whereas without Ï†-calculus, as only part of the
diï¬€erentiation is symbolically done, Tensors have to be used so that the operator overloading can still
work, which is what the remaining part of the AD depends on. (ii) It exposes large-scoped loop-invariant
calculations, for both the primal and the diï¬€erentiation. Symbolic transformation and analysis of Lines
6-7 in Figure 6(a) can show that the loop involves the calculations of (cid:80)
i x[i] âˆ— x[i]; when
the analysis scope spans across the entire while loop via Ï†-calculus, the optimization can easily recognize
that the two summations repeat in every iteration of the while loop and can be hoisted out of the while
loop. Similar phenomena are in the diï¬€erentiation. Neither the default optimizers in the Java Runtime
underlying Kotlin or the coarsening without Ï†-calculus can recognize and take advantage of that. (iii) It
saves the remaining AD overhead that the version by coarsening without Ï†-calculus has to suï¬€er.
Brachist. This program calculates the Brachistochrone curve (i.e., curve of fastest descent), which is
the one lying on the plane between a point A and a lower point B (called anchor points), where B is not
directly below A, on which a bead slides frictionlessly under the inï¬‚uence of a uniform gravitational ï¬eld
to a given end point in the shortest time. In each iteration, the program computes the time taken by
the bead to slide down the slope by summing the time it takes for each section of the current curve, and
then gets the gradient of every section over the total time. The three conï¬gurations correspond to the
number of sections that the target curve is regarded to be composed of. This is a relatively easy case,
but it demonstrates an important scenario where coarsening can remove the entire primal computation.
The entire primal code to compute the time taken to slide down the slope is identiï¬ed as the SOI. With
coarsening, the AD tool can symbolically diï¬€erentiate the entire computation. Because the program
only needs the gradients to update the curve in each iteration, it can now forego the computation of the
total time as the gradients can be directly computed. Therefore the coarsening optimization removes the
entire primal computation, making the programâ€™s overall speedups even more than the speedups on the
gradients calculations. As Table 3 shows, the speedups on the diï¬€erentiation part is modest (due to the
simplicity and regularity of the code), but the end-to-end executions get more signiï¬cant speedups (e.g.,
1.8-2.38Ã— versus 1.03-1.42Ã— on macBook). The overall speedups are more pronounced on the smaller
inputs because the primal computation weights more in those runs.
CartPole. CartPole is a deep reinforcement learning program as already introduced in Section 3. The
three conï¬gurations corresponding to the number of exploration steps observed before learner updates
the model parameters. It shows the least speedups among all the benchmarks, not because coarsening
is not eï¬€ective, but because the small portion of the optimized code weighs in the overall program.
Recall in Figure 2, the primal computation of CartPole contains two parts, the Neural Networks(NN)
and the environment update. As CartPole uses a simple simulation environment, the environment update
part weighs only about 15% of the primal time, with the rest dominated by the NN. As the NN has a
standard structure and the default gradient calculation is through a manually written highly polished
vendor library rather than the AD, the SOI is the second part, which updates the environment. There
are ï¬ve Ï† functions in the SOI. The speedups on the diï¬€erentiation of the environment update part are
actually signiï¬cant:

Conï¬g
Speedup(X) on diï¬€erentiating
the environment update part

devServer
2
2.63

1
2.68

3
3.19

macBook
2
2.73

1
3.01

3
3.21

In cases where reinforcement learning is applied to more complex environments, the speedups on the
end-to-end execution by coarsening are expected to be more substantial.
HMC. HMC stands for Hamiltonian Monte Carlo.
It is one of the main algorithms in Probabilistic
Programming or Statistics for ï¬nding out posterior distributions of random variables through a carefully
designed Monte Carlo sampling process [6], as illustrated in Figure 9(a).

20

(a) HMC sampling in a space ([2])

(b) HookeanSpring ([27])

(c) QWOP

Figure 9: Illustrations of several benchmarks.

We ï¬rst gives a conceptual view of HMC. The core sampling function in HMC takes in two functions
as part of the arguments: function â€Uâ€ and function â€grad Uâ€. The former does the primal computation
and the latter computes the derivative of â€Uâ€. HMC calls them many times on diï¬€erent values, more often
on â€grad Uâ€ than on â€Uâ€. In this benchmark, HMC is used for logistic regression. Logistic regression is
a classic method for classiï¬cation. HMC is used to estimate the posterior distribution of the parameters
in the logistic regression model. Its â€Uâ€ function is as follows:

U (Î²) = Î²T X T (y âˆ’ 1n) âˆ’ 1T

n [log(1 + eâˆ’XÎ²)] âˆ’

Î²T Î²
2Ïƒ2
Î²

where, Î² are the parameters in logistic regression, X and y are the input and response data (training
data), Ïƒ2
Î² is a hyperparameter (1000). All are vectors except that X is a matrix. The three conï¬gurations
correspond to three diï¬€erent training data sets.

A feature of HMC is that when it needs the gradient on some values, it often cares about the gradients
but not the actual primal value. It does call the primal function at some places, but on values diï¬€erent
from those needed for the calculation of the gradients.

But in the default implementation on AD, anytime gradient is needed, the primal is called because of
the inherent requirement for AD to work as we have described in Section 2. Coarsening takes the entire
primal function â€Uâ€ as the SOI, and there are one two-level nested loop and another two single-level
loops in the code with one of them containing an if-else statement. Coarsening is able to symbolically
diï¬€erentiate it and generate the entire â€grad Uâ€ function. It hence can save many primal computations.
Overall, the speedups are 2.3-3.6Ã—.

One special note on HMC is that the exponential term eâˆ’XÎ² can easily result in value overï¬‚ow;
special treatments must be given to large exponents (e.g., using âˆ’XÎ² to approximate log(1 + eâˆ’XÎ²
if âˆ’XÎ² exceeds 80). The default AD-based tool uses a masking function to deal with that (similar
to â€whereâ€ in PyTorch). Without the masking scheme, an alternative would be to wrap each element
in âˆ’XÎ² in a Tensor to discern their following uses; which creates huge runtime overhead, making the
program run about 25Ã— slower. Without the dependence on Tensor or operator overloading, coarsening
is not subject to the problem; when it generates the code, it directly generates the appropriate code for
the cases where the exponential value is large.
HookeanSpring. HookeanSpring is a physical simulation program. It simulates mass-spring systems
as illustrated in Figure 9 [27]. It demonstrates the transitions of physics-based states as energy mini-
mization procedures. The program keeps optimizing the vertex positions of a spring system to ï¬nd some
conï¬guration that minimizes the total elastic energy. Every spring has some preferred rest length and

21

they naturally tend to recover their rest shapes over time. Each optimization step uses the gradients of
the spring vertex locations regarding to the system energy. The three conï¬gurations correspond to three
sizes of the spring system in terms of the number of spring vertices. Coarsening is able to take the entire
energy calculation of the Spring system as the SOI and symbolically diï¬€erentiate it. As a result, the
primal computation which computes the system energy can be completely removed. The speedups are
4â€“11Ã—. The program even runs faster than the original primal computation alone; the following table
shows the times taken in one iteration of the simulation and the relative speedups (median values of
repeated measurements are used):

Conï¬g
Original primal only(Âµs)
Exec. after coarsening(Âµs)
Speedups(Ã—)

1
49.19
14.49
3.39

devServer
2
51.92
26.76
1.94

3
114.20
51.49
2.22

macBook
2
47.01
21.61
2.18

1
43.37
14.46
3.00

3
52.58
41.17
1.28

This result is signiï¬cant because there has been a common perception that a program would take a lot
more time to run if automatic diï¬€erentiation is added into it. For example, a previous work considers 2.4-
4Ã— slowdown after adding automatic diï¬€erentiation as already close to the optimal [7]. This coarsening
result shows that with coarsening, after adding automatic diï¬€erentiation, a program can even run several
times faster.
QWOP. QWOP is an avatar motion optimization program. It trains a virtual stick ï¬gure to run as
far as possible by providing a schedule for how much each muscle should be extended, as illustrated in
Figure 9(c). The three conï¬gurations correspond to three conï¬gurations of the mass of the body parts of
the stick ï¬gure. The special aspect about this program is that its core part is a 225-line function with 13
loops and many if-else statements. After loop unrolling, the function becomes 1117-line long. Coarsening
can successfully deal with the function, getting two SOIs, and achieving 1.17-1.51Ã— overall speedups.

7.3 Potential on Other AD Tools

Coarsening is a general optimization for AD. To check its potential beneï¬ts to AD tools beyond Diï¬€Kt,
we examined the performance of the benchmark BGDHyperOpt on three other AD tools: JAX [12] for
Python, Zygote [19] for Julia, and Adept [7] for C++.

For each of the three AD tools, we have two versions of the benchmark BGDHyperOpt: (i) the baseline
version which uses the default AD oï¬€ered by the tool; (ii) the coarsened version optimized by coarsening.
The latter was written based on the results from our symbolic engine. Table 4 reports the speedups of
the coarsened versions over the baseline counterparts. Please note that the JAX baseline version already
uses its JIT (the JIT gives 1.3-1.47X speedups over the default version that uses no JIT). The execution
times were measured on the Macbook after warm-ups. The speedups are 66Ã—-335Ã—, even greater than
on Diï¬€Kt, indicating the potential of coarsening as a general AD optimization technique.

Table 4: Speedups of the coarsened version over the baseline version on BGDHhyperOpt

AD Tool (Language)

Input Size

JAX (Python) [12]
Zygote (Julia) [19]
Adept (C++) [7]

2000

1000
87.4X 335.1X
90.8X
150X
96.2X
66X

8 Discussions

The study has demonstrated the signiï¬cant beneï¬ts of coarsening on the Kotlin AD tool on ï¬rst-order
backward AD, the most popular kind of AD. It is easy to see that the technique can help other types of
AD implementations (e.g., forward or mixed directions and higher-order diï¬€erentiation) as the outcome
of coarsening can always be used as a shortcut on the AD chains.

22

Our exploration of coarsening is at the static compilation time. The technique is potentially applicable
at runtime as well, which could be especially meaningful for languages (e.g., Python) that are diï¬ƒcult
for static time analysis and transformations. In that case, runtime proï¬ling could be useful, and extra
care (e.g., hot paths based selective optimizations) may be necessary to minimize the time overhead of
symbolic manipulations.

The current coarsening optimization applies to both regular and irregular loops as mentioned before.
But there is code with unstructured control ï¬‚ows where it is even unclear what the loop is (e.g., code
formed by go-to statements in certain languages). In those cases, the SOIs could be set to the sections
within the branches that form the unstructured control ï¬‚ows.

With coarsening, the compilation time does not have noticeable changes except for the time taken by
the symbolic engine in doing symbolic diï¬€erentiation and other symbolic manipulations. As mentioned,
as a proof of concept, the current implementation uses an extended Sympy for that. Written in Python,
Sympy is not the most eï¬ƒcient symbolic engine. For the benchmarks in the experiment, it takes up to a
minute to do symbolic diï¬€erentiation.

Coarsening is based on the SSA form of a program. When a program has assignments to arrays,
array SSA [20] would be necessary to discern the diï¬€erent ranges of data elements in an array when
they are treated diï¬€erently in the program. The representation and corresponding analysis are more
complicated than on the basic SSA form. We found that for AD programs written in Tensor-based
AD libraries, in most cases, array SSA is not necessary. It is because in those programs, if there are
large arrays, operations on them are typically written as Tensor operations (e.g., C = A + B for Loop:
c[i] = a[i]*b[i]) without explicit references to individual array elements; for such representations, the
standard SSA still applies. Even if sometimes a part of the array elements are treated diï¬€erently from
others, Tensor operations still suï¬ƒce via Tensor masking operations (e.g., the HMC case). In the cases
where individual array elements are used and updated diï¬€erently, those arrays are usually short and are
used in small loops; loop unrolling and scalar conversion can easily turn the code into a form amenable for
the standard SSA. Nonetheless, integration of array SSA with coarsening could still be useful especially
for AD tools without Tensor-like abstractions.

9 Related Work

There is a large body of work on eï¬ƒcient AD. Optimizations range from checkpointing [15] to edge/vertex
eliminations on computation graphs [17], combination of forward and backward diï¬€erentiation [11], loop
transformations [28], and so on. A recent work [29] proposes a diï¬€erentiable programming language to de-
liver a semantics for higher-order functions, higher-order derivatives, and Lipschitz but non-diï¬€erentiable
functions. Some of the relevant studies have been mentioned in earlier sections, and more on AD for ma-
chine learning can be seen in recent surveys [11, 34, 23]. Coarsening can be regarded as an optimization
complementary to those existing AD optimizations: They can be used together, with coarsening oï¬€ering
shortcuts and the other optimizations improving the remaining AD operations.

There are many tools capable of doing symbolic diï¬€erentiation. Examples include the Calculus module
in Julia [1], SageMath [3], KotlinGrad [13], and Acumen which maps from analytical models to simulation
codes via symbolic diï¬€erentiation [36]. None of them have addressed the complexities from control ï¬‚ow on
symbolic diï¬€erentiation, or the systematic integration of symbolic diï¬€erentiation with AD. The existing
symbolic engines can diï¬€erentiate only expressions not programs. For cases with simple control ï¬‚ows
where the problem of interest involves only several conditional cases, the user could enumerate those
cases and use the existing symbolic engines to diï¬€erentiate them each. That practice does not apply to
code with loops or many branches. The Ï†-calculus in this work oï¬€ers a solution to the complexity.

Several recent studies have challenged the common criticisms of â€œexpression swellâ€ of symbolic dif-
ferentiation [35, 21]. Even though the arguments may diï¬€er in form, the main points are similar: If
placeholders are used to store intermediate diï¬€erentiation results for reuses, the problem can be largely
alleviated. The design of our reuse-aware SOI identiï¬cation in coarsening is based on a similar insight,
but provides a systematic way to deal with the tradeoï¬€ between reuse and the granularity of symbolic
diï¬€erentiation.

23

Expression templates have been used in both forward and backward AD implementations to reduce
runtime space and time overhead[10, 26, 8]. In Adept, for instance, during the primal computations, the
algorithm records backward operations onto a stack. Its use of expression templates in C++ helps avoid
invocations of virtual function calls at runtime, and hence reduces the amount of objects needed to allocate
to hold intermediate results. Diï¬€erentiation happens however still at each individual operation. There
is no symbolic diï¬€erentiation or symbolic simpliï¬cations or optimizations in a large scope. Moreover,
as with all other operator overloading based AD, these solutions also require primal computations to be
executed before gradients can be computed. As a result, the highly optimized implementations are still
2.4-4X slower than the original algorithm (without gradients calculations) [7]. Coarsening optimization,
in comparison, harnesses large-scope optimization opportunities, and can sometimes forego the primal
computations completely, yielding even a higher speed than the original algorithm has as Section 7 has
shown.

Since it was ï¬rst proposed in late 1980s [14], SSA has been widely adopted in program representations
in compilers. Gated SSA (GSA) proposes to explicitly specify the conditions in the Ï† functions, and
introduces notations to distinguish loop entry, loop exit, and normal Ï†-functions [24], which share some
similarities with part of the Ï†-notations in this work. The loop notations in Ï†-calculus is inspired by the
notations in Glore [16], a work that detects large-scoped loop invariants. In 1990s and early 2000s, there
were a number of papers on recognizing and substituting inductive variables in loops based on SSA through
symbolic analysis [31]. An example is the symbolic analysis based on Chains of Recurrences [32, 33].
The purpose is to convert the array subscripts in a loop into a form ready for parallelization-oriented
dependence analysis. For symbolic diï¬€erentiation, what is needed is not only symbolic treatment to
inductive variables but derivations of the closed-form expressions for all the computations that are related
with the active variables, hence the need for the Ï†-calculus. As a type of standard IR, SSA is also the IR
leveraged in recent AD compilers, such as the Zygote for Julia [18, 19], which is a pure AD tool without
systematic integration of symbolic diï¬€erentiation.

Besides symbolic and algorithmic diï¬€erentiation, there is another approach called numerical diï¬€eren-
tiation, which uses ï¬nite diï¬€erence approximations. But because it is inaccurate and scales poorly for
gradients, it is rarely used for machine learning where gradients with respect to millions of parameters
are common.

10 Conclusion

This paper has presented coarsening, a novel optimization that expands the scope of symbolic diï¬€eren-
tiation and systematically integrates symbolic diï¬€erentiation with AD. It builds on two key innovations:
the Ï†-calculus and the reuse-aware SOI identiï¬cation. The Ï†-calculus oï¬€ers the ï¬rst mechanism that
allows symbolic diï¬€erentiation to apply on code with complicated control ï¬‚ow, while the reuse-aware
SOI identiï¬cation provides an algorithm to deal with the tension between computation reuse and coars-
ening. Experiments on several AD tools and various settings demonstrate that coarsening is an eï¬€ective
optimization for AD. It can remove the overloading overhead in AD and at the same time harness the
beneï¬ts of symbolic optimizations and diï¬€erentiation, yielding several times to two orders of magnitude
speedups.

References

[1] Calculus package for julia. Available at https://github.com/JuliaMath/Calculus.jl.

[2] Hmc explained. Available at https://arogozhnikov.github.io/2016/12/19/markov_chain_

monte_carlo.html.

[3] Sagemath. Available at https://www.sagemath.org/.

[4] Sympy software. https://www.sympy.org/en/index.html.

24

[5] Fast reverse-mode automatic diï¬€erentiation using expression templates in c++. Perspectives in

Computing, 19, 1988. Source of expression swell.

[6] Handbook of markov chain monte carlo. May 2011.

[7] Fast reverse-mode automatic diï¬€erentiation using expression templates in c++. Trans. Math. Soft-

ware, 40(26), 2014. ADEPT AD tool in C++.

[8] High-performance derivative computations using codipack. Trans. Math. Software, 45, 2017. CoDi-

Pack.

[9] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Principles, Techniques, and Tools.

Addison Wesley, 2nd edition, August 2006.

[10] P. Aubert, N. D. Cesare, and O. Pironneau. Automatic diï¬€erentiation in c++ using expression

templates Â´ and application to a ï¬‚ow control problem. Comput. Vis. Sci., 3:197â€“208, 2001.

[11] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic diï¬€erentiation in

machine learning: a survey. The Journal of Machine Learning Research, 18(1), 2018.

[12] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations
of Python+NumPy programs, 2018. https://jax.readthedocs.io/.

[13] B. Considine, M. Famelis, and L. Paull. Kotlinâˆ‡: A shape-safe eDSL for diï¬€erentiable programming,

2019. https://github.com/breandan/kotlingrad.

[14] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An eï¬ƒcient method
of computing static single assignment form. In Proceedings of the 16th ACM SIGPLAN-SIGACT
symposium on Principles of programming languages, pages 25â€“35, 1989.

[15] B. Dauvergne and L. Hascoet. The data-ï¬‚ow equations of checkpointing in reverse automatic diï¬€er-

entiation. Lecture Notes in Computer Science, 3994, 2006.

[16] Y. Ding and X. Shen. Glore: Generalized loop redundancy elimination upon ler-notation. In Pro-
ceedings of OOPSLA at The ACM SIGPLAN conference on Systems, Programming, Languages and
Applications: Software for Humanity (SPLASH), 2017.

[17] L. C. Dixon. Use of automatic diï¬€erentiation for calculating hessians and newton steps. Automatic
Diï¬€erentiation of Algorithms: Theory, Implementation, and Application, pages 114â€“125, 1991.

[18] M. Innes. Donâ€™t unroll adjoint: Diï¬€erentiating ssa-form programs. CoRR, abs/1810.07951, 2018.

[19] M. J. Innes. Sense & sensitivities: The path to general-purpose algorithmic diï¬€erentiation.

In

Proceedings of the 3rd MLSys Conference, 2020. https://ï¬‚uxml.ai/Zygote.jl/latest/.

[20] K. B. Knobe and V. Sarkar. Array ssa form and its use in parallelization. In Proceedings of the 25th

ACM SIGPLAN-SIGACT symposium on Principles of programming languages, 1998.

[21] S. Laue. On the equivalence of forward mode automatic diï¬€erentiation and symbolic diï¬€erentiation.

CoRR, abs/1904.02990, 2019.

[22] D. Maclaurin. Modeling, Inference and Optimization with Composable Diï¬€erentiable Procedures.

PhD thesis, Harvard University, 2016.

[23] C. C. Margossian. A review of automatic diï¬€erentiation and its eï¬ƒcient implementation. WIREs

Data Mining and Knowledge Discovery, 9(4), Mar 2019.

25

[24] K. J. Ottenstein, R. A. Ballance, and A. B. MacCabe. The program dependence web: a representation
supporting control-, data-, and demand-driven interpretation of imperative languages.
In ACM
SIGPLAN 1990 conference on Programming language design and implementation, pages 257â€“271,
1990.

[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic diï¬€erentiation in pytorch. In Proceedings of NIPS 2017 Workshop Autodiï¬€,
2017.

[26] E. Phipps and R. Pawlowski. Eï¬ƒcient expression templates for operator overloading-basedautomatic
diï¬€erentiation. In S. Forth, P. Hovland, E. Phipps, J. Utke, and A. Walther, editors, Recent Advances
in Algorithmic Diï¬€erentiation, pages 309â€“319, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.

[27] J. Rojas, S. Coros, and L. Kavan. Deep reinforcement learning for 2D soft body locomotion. In

NeurIPS Workshop on Machine Learning for Creativity and Design 3.0, 2019.

[28] A. Shaikhha, A. Fitzgibbon, D. Vytiniotis, and S. Peyton Jones. Eï¬ƒcient diï¬€erentiable programming

in a functional array-processing language. Proc. ACM Program. Lang., 3(ICFP), July 2019.

[29] B. Sherman, J. Michel, and M. Carbin. Computable semantics for diï¬€erentiable programming with
higher-order functions and datatypes. In Proceedings of the ACM SIGPLAN-SIGACT symposium
on Principles of programming languages, 2021.

[30] N. Tehrani, N. S. Arora, Y. L. Li, K. D. Shah, D. Noursi, M. Tingley, N. Torabi, S. Masouleh,
E. Lippert, and E. Meijer. Bean machine: A declarative probabilistic programming language for
eï¬ƒcient programmable inference. In Proceedings of the 10th International Conference on Probabilistic
Graphical Models, PMLR 138, 2020.

[31] P. Tu and D. Padua. Gated ssa-based demand-driven symbolic analysis for parallelizing compilers.

In Proceedings of the 9th International Conference on Supercomputing, pages 414â€“423, 1995.

[32] R. A. van Engelen. A method for recognizing and substitutions of generalized inductive variables
In Proceedings of the International Conference on Compiler

through chains of recurrences (crs).
Constructions, 2001.

[33] R. A. van Engelen, J. Birch, Y. Shou, B. Walsh, and K. A. Gallivan. A uniï¬ed framework for
nonlinear dependence testing and symbolic analysis. In Proceedings of the International Conference
on Supercomputing, 2004.

[34] B. van MerriÂ¨enboer, O. Breuleux, A. Bergeron, and P. Lamblin. Automatic diï¬€erentiation in ML:

where we are and where we should be going. CoRR, abs/1810.11530, 2018.

[35] F. Wang, X. Wu, G. M. Essertel, J. M. Decker, and T. Rompf. Demystifying diï¬€erentiable program-

ming: Shift/reset the penultimate backpropagator. CoRR, abs/1803.10228, 2018.

[36] Y. Zhu, E. Westbrook, J. Inoue, A. Chapoutot, C. Salama, M. Peralta, T. Martin, W. Taha,
R. Cartwright, A. Ames, and R. Bhattacharya. Mathematical equations as executable models of
mechanical systems. In Proceedings of International Conference on Cyber-Physical Systems, 2010.

26

