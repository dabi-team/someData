1
2
0
2

t
c
O
5

]
L
P
.
s
c
[

1
v
7
0
3
2
0
.
0
1
1
2
:
v
i
X
r
a

Coarsening Optimization for Diﬀerentiable Programming

Xipeng Shen+∗, Guoqiang Zhang+, Irene Dea(cid:5), Samantha Andow(cid:5),
Emilio Arroyo-Fang(cid:5), Neal Gafter(cid:5), Johann George(cid:5), Melissa Grueter(cid:5),
Erik Meijer(cid:5), Steﬃ Stumpos(cid:5), Alanna Tempest(cid:5), Christy Warden(cid:5), Shannon Yang(cid:5)

+ North Carolina State University

∗ CoCoPIE LLC

(cid:5) Facebook Inc.

Contact: xshen5@ncsu.edu

October 7, 2021

Abstract

This paper presents a novel optimization for diﬀerentiable programming named coarsening opti-
mization. It oﬀers a systematic way to synergize symbolic diﬀerentiation and algorithmic diﬀerenti-
ation (AD). Through it, the granularity of the computations diﬀerentiated by each step in AD can
become much larger than a single operation, and hence lead to much reduced runtime computations
and data allocations in AD. To circumvent the diﬃculties that control ﬂow creates to symbolic diﬀer-
entiation in coarsening, this work introduces φ-calculus, a novel method to allow symbolic reasoning
and diﬀerentiation of computations that involve branches and loops. It further avoids ”expression
swell” in symbolic diﬀerentiation and balance reuse and coarsening through the design of reuse-centric
segment of interest identiﬁcation. Experiments on a collection of real-world applications show that
coarsening optimization is eﬀective in speeding up AD, producing several times to two orders of
magnitude speedups.

1 Introduction

A program written with diﬀerentiable programming can be diﬀerentiated automatically. The diﬀerenti-
ation results can then be used for gradient-based optimization (e.g., gradient descent) of the parameters
in the program.

Diﬀerentiable programming have been used in scientiﬁc computing, physics simulations, and other
domains to help mitigate the burden of manual error-prone coding of derivative computations. Re-
cent several years have witnessed a growing interest of diﬀerentiable programming in machine learning
(ML) [11, 34] and Probabilistic Programming [30], to accommodate the needs of various customized ML
operators, user-deﬁned operations in the learning targets (e.g., the physical environment of reinforcement
learning) and statistical sampling.

The key technique in diﬀerentiable programming is automatic diﬀerentiation. For a program (P ) that
produces output (y) from some given values (X), automatic diﬀerentiation automatically computes the
derivatives (∂y/∂x) (x ∈ X) without the need for users to write the diﬀerentiation code. The given
program P is called the primal code, and x is called an active input variable.

Existing approaches of automatic diﬀerentiation fall into two categories: (i) Symbolic diﬀerentiation,
which uses expression manipulation in computer algebra systems, (ii) Algorithmic diﬀerentiation, which
performs a non-standard interpretation of a given computer program by replacing the domain of the
variables to incorporate derivative values and redeﬁning the semantics of the operators to propagate
derivatives per the chain rule of diﬀerential calculus (elaborated in Section 2).

Symbolic diﬀerentiation has been commonly regarded inappropriate for diﬀerentiable programming,
for several reasons: (i) It results in complex and cryptic expressions plagued with the problem of “ex-
pression swell” [5]. (ii) It requires models to be deﬁned as closed-form expressions, limiting the use of
control ﬂow and other features that are common in computer programs.

1

 
 
 
 
 
 
Consequently, existing diﬀerentiable programming systems are all based on algorithmic diﬀerentia-
tion (AD). Algorithmic diﬀerentiation computes derivatives through accumulation of values during code
execution to generate numerical derivative evaluations. In contrast with the eﬀort involved in arranging
code as closed-form expressions under the syntactic and semantic constraints of symbolic diﬀerentiation,
algorithmic diﬀerentiation can be applied to regular code, allowing branching, loops, and other language
features. Some examples are Autograd [22], PyTorch [25], JAX [12], and Zygote [19].

In this work, we advocate for a hybrid approach for diﬀerentiable programming. This new ap-
proach seamlessly integrates symbolic diﬀerentiation with algorithmic diﬀerentiation through coarsening,
a compiler-based technique we introduce in this work.

The motivation of the new approach is to eliminate the large overhead in AD incurred by its ﬁne-
grained diﬀerentiation and operation overloading. Rather than diﬀerentiation at each operation, this
new approach tries to enlarge the granularity to a sequence of operations, hence the name “coarsening
optimization”. It identiﬁes a part of the to-be-diﬀerentiated computations that are amenable for symbolic
diﬀerentiation, elevates it to a high-level symbolic representation, applies symbolic diﬀerentiation on it,
generates the code, and then integrates it back into the computation ﬂow of AD.

By doing that, the coarsening optimization gives four-fold beneﬁts: (i) It avoids many calls to the
ﬁne-grained diﬀerentiation functions and the creations of many intermediate results; (ii) the symbolic rep-
resentation makes it easy to directly beneﬁt from expression simpliﬁcations by existing symbolic engines
and hence leads to more eﬃcient code being generated; (iii) it can form a synergy with computation reuse
and hence amplify the beneﬁts; (iv) it can sometimes remove the unnecessary primal computations. If
what users want is only the derivative of a function, current AD still needs to run the primal computation
because of the nature of its diﬀerentiation process (Sec 2). But if coarsening can be applied to the entire
function, then only its generated diﬀerentiation function needs to run, foregoing the executions of the
primal function.

In addition to those beneﬁts, coarsening features several appealing properties: (i) As the coarsening
optimization typically happens at compile time, it trades a slight increase of compile time for signiﬁcant
runtime savings; (ii) functioning as a way to add ”shortcuts” to AD, it can be seamlessly integrated into
both forward and backward diﬀerentiation; (iii) it applies regardless whether the gradients are for ﬁrst
or higher order optimizations.

To materialize the optimization, there are several major challenges.
Challenge I: Complexities from control ﬂow (e.g., branches, loops). Symbolic diﬀerentiation requires
a closed form of the computation, which has been regarded as diﬃcult for code involving complex control
ﬂow. Limiting coarsening to the code segments between the appearances of such complexities in a program
would result in many short code segments, leaving many optimization opportunities submerged and much
power of coarsening untapped.

j(cid:54)=i fj.

Challenge II: ”Expression swell”. ”Expression swell” is a criticism to symbolic diﬀerentiation men-
tioned in some literature, which refers to the observation that the derivative often has a much larger
representation than the original function has [5]. For instance, in a straightforward implementation, the
derivative of the multiplication of n terms becomes an expression with n2 terms: d(f1f2 · · · fn)/dx =
(cid:80)

i d(fi)/dx (cid:81)
Challenge III: Tension with computation reuse. Some calculations in the primal computation may be
also required in the diﬀerentiation (e.g., eX·β is part of both (1 + eX·β) and its diﬀerentiation over β,
XeX·β). Reuse opportunities can also exist between diﬀerent parts of diﬀerentiation. The ﬁne-grained
operations in algorithmic diﬀerentiation already build on such reuses. But in coarsened diﬀerentiation,
without a careful design, such reuse opportunities can get lost as the symbolic transformation reorders
and reorganizes the involved calculations. On the other hand, naively maximizing computation reuse
would limit the granularity of coarsening. So there is a challenge in reconciling the tension between reuse
and coarsening.

We address the challenges through two major innovations. (i) For challenge I, we introduce φ-calculus,
a novel method that allows symbolic reasoning and diﬀerentiation of computations with complex control
ﬂow. Building on the φ-function in single static assignment (SSA), φ-calculus makes the derivation of
a closed form possible for computations involving complicated control ﬂow.
It further oﬀers a set of
formulae for symbolically reasoning about and diﬀerentiating the closed forms that involve φ-functions.

2

(ii) For challenges II and III, we propose reuse-aware SOI identiﬁcation as a way to identify the code
segments of interest (SOI) for coarsening. It can strike a good tradeoﬀ between coarsening and reuse,
and at the same time keep the eﬀects of ”expression swell” under control.

Based on an AD tool for Kotlin (DiﬀKt), we evaluated coarsening on 18 settings of six applications on
two machines. The results show that coarsening is eﬀective in signiﬁcantly expanding the applicable scope
of symbolic diﬀerentiation, and hence dramatically reducing the runtime overhead of AD. The perfor-
mance improvement is substantial, 1.03×-27× speedups of the diﬀerentiation and 1.08×-11× speedups of
the end-to-end application execution. We further examined the potential of coarsening on several other
AD tools (Zygote [19] for Julia, Jax [12] for Python, Adept [7] for C++) by experimenting with the
implementations of the symbolic diﬀerentiation results in their corresponding languages. The speedups
from the coarsening results are even greater, 66×-335×, indicating the potential of coarsening in serving
as a general optimization technique for AD.

To the best of our knowledge, this is the ﬁrst work that proposes a systematic approach to integrating
symbolic diﬀerentiation with algorithmic diﬀerentiation for diﬀerentiable programming. The developed
φ−calculus oﬀers the ﬁrst method to enable symbolic diﬀerentiation of computations spanning over
complex control ﬂow. The resulting hybrid diﬀerentiation approach gets the best of both worlds, that is,
the eﬃciency from the compile-time symbolic diﬀerentiation and the generality of AD.

In summary, this work makes the following contributions:

• It introduces coarsening optimization, the ﬁrst approach to systematical integration of symbolic

diﬀerentiation into algorithmic diﬀerentiation for general programs.

• It develops φ-calculus that eliminates the barriers of control ﬂow to symbolic diﬀerentiation.

• It proposes reuse-aware SOI identiﬁcation to balance reuse and coarsening.

• It validates the beneﬁts of coarsening, conﬁrming its potential for signiﬁcantly improving AD eﬃ-

ciency.

2 Background and Terminology

At the foundation of AD is the chain rule. We explain it in a simple setting. Suppose y is the output of a
sequence computations on input x, and yi (i = 1, 2, · · · , k) are the intermediate results produced during
the sequence of computations from x to y, that is, y1 = f1(x), y2 = f2(y1), · · · , yk = fk(yk−1), y = f (yk).
The chain rule says that the derivative of y on x (or called x’s gradient regarding y) can be computed as
follows:

dy/dx = dy/dyk ∗ dyk/dyk−1 ∗ · · · ∗ dy1/dx

In a program, besides the variables relevant to the deriatives of interest, there can be many other
variables. To distinguish them, we call the relevant output variables like y active output variables, relevant
input variables like x active input variables, and other relevant variables simply active variables. Further,
we call the computations in the original program from active input variables to active output variables
primal computations, and the computations to compute the derivatives gradient computations.

There are two ways to interpret the chain rule, which lead to the forward and backward AD respectively.
We explain them by assuming an implementation of AD via operator overloading, the most common way
of implementation of AD.

The ﬁrst is to regard the rhs of the chain rule a sequence of computations from the rightmost term
to the leftmost term. Corresponding to AD implementation, the derivatives of dy1/dx is computed as
a side step of operator overloading when y1 is computed from x in the primal computation, and the
result is then passed to the next step of primal computation, which computes y2 and dy2/dy1 and then
multiplies it with the received value of dy1/dx. The process continues and produces dy/dx eventually.
This implementation is called forward AD.

3

Figure 1: The overall workﬂow of coarsening for AD. Solid boxes are the main components in coarsening.

The second way is to regard the rhs of the chain rule a sequence of computations from the leftmost
term to the rightmost term. In this case, at each step in the primal computation (which is still right-to-
left), some operations needed for diﬀerentiation are recorded in a data structure, such as a stack [7], as
part of the operations of the overloaded operators. When the primal computations reach the last step
and the gradient computation actually starts, the operations on the stack are executed in a backward
order, starting from those of the leftmost term in the rhs of the chain rule. After dy/dyk is computed,
the result is passed to the next step, which computes dyk/dyk−1 and then multiples it with the received
value of dy/dyk. The process continues until the gradient of x is computed. This implementation is called
backward AD.

Backward AD is a more popular choice in existing AD tools because it is overall more eﬃcient in
general settings [23]. The two methods are sometimes used together. Note that in both of them, primal
computations are necessary to run so that the overloaded operators can take place, even if what the user
wants are just the gradients. Coarsening optimization can lift such a requirement as shown later in this
paper.

In cases that are not diﬀerentiable (e.g., x=0 in relu(x)), AD tools approximate the gradients (e.g.,

using 0 at relu(0)); coarsening optimization preserves the same behavior.

3 Overview of Coarsening Optimization

The basic deﬁnition of coarsening optimization for AD is as follows:

Deﬁnition 1. Let S be a sequence of program statements that implement the computations from active
input I to active output P . Coarsening optimized AD is applied to S if a closed form F is produced that
captures the computations in S and F goes through a symbolic diﬀerentiation with the results integrated
into the AD process.

Figure 1 shows the high-level workﬂow of coarsening for AD. The input is a program written in a
certain AD-based diﬀerentiable programming language. Coarsening works on the intermediate represen-
tation (IR) output from the front end of the default compiler. From it, the reuse-aware SOI identiﬁcation
component identiﬁes the code segments of interest (SOIs), which are sent to the symbolic elevation com-
ponent to produce a symbolic representation of the SOIs. The symbolic diﬀerentiation component takes
them in and outputs the symbolic form of the diﬀerentiated SOIs. The symbolic optimization compo-
nent optimizes both the SOIs and the diﬀerentiated SOIs while drawing on their contexts captured in
the original IR. The optimizations include simpliﬁcations via algebra systems, as well as identifying the
places for proﬁtable computation reuses between SOIs and the diﬀerentiated SOIs.

Coarsening optimization can be applied for AD at both compilation and runtime. We take compile-

time optimization as the context of discussion.

4

Source program w/ alg. differen.Compiler front endReuse-aware SOI identiﬁcationSymbolic elevationSymbolic differentiationSymbolic optimizationCode gen. & integrationOpt. program w/ hybrid differen.Initial IRSOIsSymbolic repr. of SOIsDiﬀerentiated SOIsOpt. SOIs & Opt. diﬀeren. SOIs(c) Computation ﬂow of the forward pass of CartPole with the main computations shown on the right.

Figure 2: A running example CartPole. (a) Problem illustration (artwork source: ﬂuxml.ai); (b) Pseudo-
code of the training harness; (c) The simpliﬁed computation graph of the forward pass and the core
computations.

Example To help convey the intuition of coarsening optimization, we use CartPole as an example.
CartPole is an example that uses deep reinforcement learning (DRL). As illustrated in Figure 2 (a), a
pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The cart is
controlled by applying a force of +1 (to the right) or -1 (to the left) on the cart. The pendulum starts
upright, and the goal is to learn a cart control policy to prevent the pendulum from falling over. The
learning system consists of a Neural Network and a simulator of the cart and pole. At each time step, the
system goes through the computation outlined in the inner loop body in Figure 2 (b), that is, calculating
the output of the Neural Networks from the current cart and pole’s states to decide the action for the cart
to take, based on which, it then updates the states of the cart and pole according to the physics model.
This process continues for another two time steps. The resulting total loss is then used in updating the
weights of the Neural Networks via gradient descent. For each weight w in the Neural Networks, (i) the
program obtains the value dw for the derivative of w w.r.t. the loss, (ii) dw is then used to create the
diﬀerential λz.dw ∗ z, and (iii) the value of weight w is updated by the result of evaluating the diﬀerential
w.r.t. learning rate η:

w = w − (λz.dw ∗ z)(η).

The learning then continues until the Neural Networks converge. The right part of Figure 2 (c) shows
the core computations of states update in each iteration.

5

Input:  X0: initial states W0: initial Neural Networks parameters Output:     W: final parameters of Neural Networks t = 0 while (loss > threshold)      loss = 0  for (i=0; i< B; i++)       a = getAction(Xt+i, W)       Xt+i+1 = updateState(Xt+i, a)   loss += getLoss(Xt+i+1)   backpropagate(loss, W, X)  t += B(a) Illustration of the CartPole problem. The goal is to learn a cart control policy to prevent the pendulum from falling over (b) Pseudo-code of the training harness of CartPoleatWlt+1lt+2lt+3Lxt+1, 0xt+1, 1xt+1, 2xt+1, 3xt, 0xt, 1xt, 1xt, 2at = sign(tanh(relu(relu(Xt’W1)W2)W3)-e)rt = 9at+0.045xt,32sinxt,2qt=(9.8sinxt,2-rt*cosxt,2)/(0.65-0.4cos2xt,2)pt=rt - 0.045qt*cosxt,2x(t+1),0 = xt,0+0.02xt,1
x(t+1),1 = xt,1+0.02pt
x(t+1),2 = xt,2+0.02xt,3
x(t+1),3 = xt,3+0.02qtlt+1 = (0.5-max(0, (2.4-|x(t+1),0|)*(0.21-|x(t+1),2|)))2rtqtptL = lt+1+lt+2+lt+3WNeural  Network  with parameters W1, W2, W3State of the cart pole systemIntermediate resultsLoss valuesTime tTime t+1at+1Wxt+2, 0xt+2, 1xt+2, 2xt+2, 3rt+1qt+1pt+1Time t+2at+2Wxt+3, 0xt+3, 1xt+3, 2xt+3, 3rt+2qt+2pt+2Time t+3at+3WIntuition of Beneﬁts We can get some intuition about the potential beneﬁts of coarsening by checking
the diﬀerentiation of a state at time t + 2, xt+2,0, over the action at time t, at, in Figure 2. The result
is needed in the computation of the gradient of at regarding the loss Lt+2 and the total loss L. (The
computation from the weight W to a is through Neural Networks, which have a standard structure; the
gradient calculation of that part is through a highly polished vendor-provided library rather than the AD
tool. Therefore, the actual eﬀects of the AD on that benchmark is from the loss to a.)

If we put down the entire computation from at to xt+2,0, we get the following (with constants already

folded):

xt+2,0 =

xt,0 + 0.04xt,1 + 0.003636at + 0.000018xt,3

2sinxt,2
2sinxt,2cos2xt,2

−

0.00016sinxt,2 − 0.00016atcos2xt,2 − 0.000008xt,3

(0.65 − 0.405cos2xt,2)

(3.1)

It consists of 29 operations.

If a (backward) AD library is used to get its diﬀerentiation over at,
during the primal computation, at each of the 29 operations, a pullback function is generated for the
diﬀerentiation of that operation, along with the closure and some intermediate objects allocated to hold
the intermediate results that the diﬀerentiation would need to use.

In contrast, if symbolic diﬀerentiation is applied to the expression in Equation 3.1, the result is much
simpler as shown as follows. The diﬀerentiation would then need to just make an invocation to one
function that consists of only several straight-line calculations. Besides saving computations, it also saves
the allocations of many intermediate objects.

d(xt+2,0)
d(at)

= 0.003636 +

0.00016cos2xt,2
0.65 − 0.405cos2xt,2

(3.2)

Besides the beneﬁts demonstrated by the CartPole example, two other beneﬁts are worth mentioning.
First, because AD libraries are typically implemented via operator overloading, they have to wrap data
objects in a special type (e.g., Tensor in PyTorch) so that customized operations can be invoked during
the primal computations to implement the needed AD operations. Accesses to the data objects are
therefore subject to the boxing overhead. Inside the code generated by the symbolic diﬀerentiation, as no
operator overloading is needed, unboxed data objects can be directly used, reducing the boxing overhead.
This beneﬁt is especially prominent when the data involved are a collection of scalars or small vectors as
in many physical simulations or Probablistic Programming applications (examples in Sec 7).

The other beneﬁt of coarsening not captured by the CartPole example is the cancellation of terms or
other simpliﬁcations that symbolic transformation can often harness. We explain it with a simple expres-
sion involving two matrices (X1, X2) and one vector (v): (X2v)T (X1X2v). Its symbolic diﬀerentiation
over v can easily combine terms with common multipliers, yielding a form X T
1 )X2v, signiﬁ-
2 (X1X2v) + ((X2v)T X1X2)T . Simpliﬁcation
cantly simpler than what the default AD would compute, X T
of symbolic expressions is a common feature in symbolic engines; for more examples, please refer to the
simpliﬁcation module in Sympy [4].

2 (X1 + X T

As mentioned in Section 1, to make coarsening optimization eﬀective, there are three main challenges:
control ﬂows, ”expression swell”, and tension between coarsening and reuse. We next explain our solutions
to these challenges.

4 Addressing Control Flow: φ-Calculus

Control ﬂow complexities are commonly perceived obstacles for symbolic diﬀerentiation. For straight-line
code, it is easy to derive a closed form for the computations by symbolically substituting later references
with their earlier deﬁnitions in the code. In the presence of control ﬂow branches, the complexity increases
exponentially: If we build a closed form for each possible path, in the worst case, there would be O(2B)

6

closed forms for B conditional statements. That would not only increase the amount of work and
execution time of symbolic diﬀerentiation, but also complicate compiler-based code generation from the
diﬀerentiation results. The problem worsens when there are loops mixed with if-else. Without a closed
form, symbolic diﬀerentiation cannot apply.

Without an eﬀective way to handle control ﬂow, coarsening can apply only to small pieces in a
program, with each piece consisting of the code between two adjacent control ﬂow branching or merging
points. For some programs, that could lead to only small optimization scopes, leaving the power of
coarsening optimization untapped.

We address the problem by proposing φ-calculus. It consists of a set of notations for symbolically
representing loops and conditional statements, and introduces a series of formulae to facilitate the rea-
soning and diﬀerentiation on the extended symbolic form. As φ-calculus is inspired by the concept of φ
functions in SSA, we ﬁrst give a quick review of SSA.

4.1 Background on SSA and φ Functions

Static Single Assignment form (SSA) is a kind of code representation widely used in modern compilers [14,
9]. Code in SSA has two properties: (i) no two static assignments assign values to the same variable;
(ii) every reference refers to the value deﬁned by a single static assignment. It uses a special φ function
to resolve name ambiguities. A φ function ”chooses” the right name among its (two or more) arguments
based on the actual control ﬂow.

Figure 3(c) shows the SSA form of the code in Figure 3(a). There are three assignments to z in
the original code. They are all replaced with diﬀerent names z1, z2, z4. Meanwhile, two φ functions are
inserted in Figure 3(c), with the one on line 8 resolving the name ambiguity caused by the inner if-else,
and the one on line 11 for the outer if-else.

Figure 4(b) shows the SSA form of the loop in Figure 4(a). The loop structure involves two merging
points, with one at the entry (L1), the other at the exit (L2). There are two φ functions at the entry,
respectively for variables s and i; there is one φ function at the exit, for variable s. The former are called
entry φ-functions and the latter is called exit φ-function of the loop [24]. All loops, either regular or
irregular (e.g., while loops with breaks), have such pairs of φ-functions.

4.2 Notations in φ-Calculus

Inspired by SSA, φ-calculus uses φ functions and other notations to symbolically represent conditional
statements and loops:

• φ(a1, a2, · · · , ak): the standard φ function in SSA. Numerical subscripts are sometimes added to a

standard φ function to distinguish φ functions that have diﬀerent conditions.

• φLi(a1, a2, · · · , ak) & φLi(cid:48)(a1, a2, · · · , ak): the entry and exit φ functions of loop i.

• Li

u < S > : the computations in a statement S are surrounded by loop i with u iterations.
Sometimes, the loop ID (i) is used in S to also denote the iteration number, which, by default, goes
from 0 to u − 1. In the representation of a loop for symbolic diﬀerentiation in coarsening, u can be
a constant, an expression, or a symbol. For irregular loops (e.g., the while loop in Figure 6), for
instance, u is a symbol in the expression that is symbolically diﬀerentiated, and its value is recorded
in the execution of the primal code. In the following discussion, unless necessary, we omit u and/or
i in the loop notations for better readability.

• (cid:80), (cid:81): the standard math notations of summation and product.

• φLi

(j): the instance of φLi in the jth iteration of loop i

• a(j): if a is an expression in the argument list of φLi, a(j) represents the value of a after j iterations

of loop i.

• aexit(L): the value(s) of a at the exit of loop L.

7

Figure 3: Illustration of SSA and φ-calculus on an example with conditional statements

Figure 4: Illustration of SSA and φ-calculus on a simple loop

8

// x: the active variable   1   d = p + q·x   2   if ( d > 0)   3      a = d0.5   4      if (a > 50)   5         z = a   6      else   7         z = sin(a)   8   else   9      z = ln(1+ed) 10   L = z(1+ed)// x: the active variable   1   d = p + q·x   2   if ( d > 0)    3      a = d0.5   4      if (a > 50)   5         z1 = a   6      else   7         z2 = sin(a)   8      z3 = ɸ2(z1, z2)   9   else  10     z4 = ln(1+ed) 11   z5 = ɸ1(z3, z4) 12   L = z5(1+ed) L = ɸ1(ɸ2((p+q·x)0.5, (p+q·x)sin((p+q·x)0.5)), ln(1+e(p+q·x)))(1+e(p+q·x)) dL/dx = q·e(p+q·x)ɸ1(ɸ2( (p+q·x)0.5, (p+q·x)·sin((p+q·x)0.5)), ln(1+e(p+q·x)))             + ɸ1(ɸ2(0.5·q(p+q·x)-0.5, q·sin(p+q·x)0.5+(p+q·x)·0.5·q·(p+q·x)-0.5cos(p+q·x)0.5), q·e(p+q·x)/(1+e(p+q·x))) (1+e(p+q·x))(a) original code(b) SOIs segmented by branches(d) SSA-based closed-form expression(e) symbolic diﬀerentiation result on the closed form dL/dx = q·e(p+q·x)ɸ1(ɸ2( (p+q·x)0.5, (p+q·x)sin((p+q·x)0.5)), ln(1+e(p+q·x)))             + ɸ1(ɸ2(0.5(p+q·x)-0.5, sin(p+q·x)0.5+0.5(p+q·x)0.5cos(p+q·x)0.5) q(1+e(p+q·x)), q·e(p+q·x))(f) simpliﬁed symbolic diﬀerentiation result on the closed form// x: the active variable SOI-1:   d = p+q·x SOI-2:   a = d0.5 SOI-3:   z=a SOI-4:   z=sin(a) SOI-5:   z=ln(1+ed) SOI-6:   L=z(1+ed)(c) code in SSA// x: the active variable   1  s = a   2  for (i=0; i<k; i++)   3      s = s + x// x: the active variable   1          s1 = a   2          i1 = 0   3          if (i1 < k)   4  L1:     s2 = ɸL1 (s1, s3)   5             i2 = ɸL1 (i1, i3)   6             s3 = s2 + x   7             i3 = i2 + 1   8             if (i3<k) goto L1   9  L2:  s4 = ɸL1’ (s1, s3)(b) SSA form(a) Original codes3 = s2 + x s3 =  ɸL1 (s1, s3) + x s3 =  ɸL1 (a, s3) + x     = a + k*x s4 = ɸL1’ (a, s3exit)     = ɸL1’ (a, a + k*x)     = a + k*x(d) Get a closed forms3exit𝔏𝔏𝔏(𝔏:  represents a loop)s4 = ɸL1’ (a, s3exit) s3 =  ɸL1 (s1, s3) + x 𝔏(c) Symbolic representationFigure 5: Main formulae in φ-calculus.

• f [n]: recursively apply function f for n times.

This set of notations are simple extensions of the standard φ function. But with them, code with
complex control ﬂow can now be symbolically expressed. Figure 3(d), for instance, shows the symbolic
form of the computation of L by the code in Figure 3(a). Figure 4(c) shows the computation of s by
the loop in Figure 4(a). The derivation of them involve just direct substitutions of names with their
corresponding expressions; the φ-notations oﬀer ways to symbolically represent the eﬀects of loops and
conditional statements.

Note that unlike the form in Figure 3(d), in Figure 4(c) is not yet a closed form: There is still the
presence of L. Even for the closed form in Figure 3(d), it still contains φ functions. The other component
of φ-calculus, formulae in φ-calculus, oﬀers the facilities for (i) getting closed forms by removing L and
(ii) diﬀerentiating expressions involving φ functions.

4.3 Formulae in φ-Calculus

Figure 5 provides the core set of formulae in φ-calculus; at the top of the ﬁgure are ﬁve fundamental
formulae and at the bottom are nine useful corollaries derived from the fundamental formulae. Most of
the formulae are quite straightforward, but when being used together, they are powerful in getting rid
of φ and loop notations from the symbolic representation of code. We next explain each of the formulae
and provide brief proofs.

4.3.1 Fundamental φ Formulae

1) Identity Formula (F1 in Figure 5). The identity formula says that if all the arguments in a φ
function equal to one another, the φ function can be replaced with any of its argument. It immediately
follows the deﬁnition of the φ function.

2) Distributive Formula (F2 in Figure 5). This formula says that a function that applies to a φ
function can be distributed to each of the arguments of that φ function. It can be easily proved based
on the deﬁnition of φ function.

9

(F1) Identify formula:(F2) Distributive formula:(F3) Commutative formula:(F4) Loop entry formula:(F5) Loop exit formula:if  bi=bj (1⩽i,j⩽m) & b1(0)=a      ɸL'(a,b1,b2,...,bm) = b1exitɸ(a, a, ..., a) = a f(ɸ(a1, a2, ..., an)) = ɸ(f(a1), f(a2), ..., f(an))ɸ(a, b) = ɸ(b, a) f(x1, ..., xi-1, ɸ(a,b), xi, ..., xk)  = ɸ(f(x1, ..., xi-1, a, xi, ..., xk), f(x1, ..., xi-1, b, xi, ..., xk))COROLLARIES𝔏nL d = a ɸL(p, d) + b   =>  dexit(L) = anp + b ∑n-1i=0 ai𝔏nL d = a ɸL(p, d) + b[i]   => dexit(L) = anp + ∑n-1i=0 aib[n-1-i]𝔏nL d = a[i] ɸL(p, d) + b[i] => dexit(L) = p(𐍀n-1i=0 a[i])        + ∑n-1i=0  b[n-1-i]𐍀ij=0 a[n-1-j]FUNDAMENTAL FORMULAE𝔏nL d = a(ɸL(p, d))b => dexit(L) = ab+n-1pbn𝔏nL d = f(ɸL(p, d))    =>  dexit(L) = f[n](p)(C2)(C3)(C4)(C1)(C5)(C6)(C7)(C8)(C9)(i-1)3) Commutative Formula (F3 in Figure 5). This formula shows the relationship between a φ
function and its complement.
In the formula, φ is the complement of φ, that is, it chooses the ﬁrst
argument when φ chooses the second, and the second when φ chooses the ﬁrst. The correctness of this
formula immediately follows the deﬁnition.

4) Loop entry formula (F4 in Figure 5). This formula shows the inherent property of a loop-entry
φ function. For the deﬁnition of a loop-entry φ function, φL is reached always through the back edge
of loop L except for its ﬁrst instance in that loop. The formula hence follows. This simple formula is
essential for φ-calculus to deal with loops as shown later.

5) Loop exit formula (F5 in Figure 5). This formula says that φL(cid:48)(a, b1, b2, · · · , bm) equals the
value of b1 at the exit of loop L if (i) the value of all arguments, except the ﬁrst, of φL(cid:48) are the same
at φL(cid:48), and (ii) those arguments before the entry point of the loop have the value equaling the ﬁrst
argument’s value a. Its correctness can be easily proved with the identity formula. Notice that the only
time when φL(cid:48) takes its ﬁrst argument is when the entire loop is skipped, in which condition, according
to (ii), b1exit equals a; in any other condition, φL(cid:48) must take one of the other arguments, the value of
which at the exit of the loop, according to (i), must equal b1exit. This formula is useful for removing loop
exit φ functions in the application of φ-calculus as shown later.

4.3.2 Corollaries

At the bottom of Figure 5 are some of the corollaries attained from the fundamental formulae. They
provide facilities for transforming and simplifying φ expressions.

The corollaries are in two groups. The ﬁrst group consists of C1 to C4. These corollaries oﬀer
conveniences for symbolic diﬀerentiation, optimizations, and code generations, reducing computations
and code size. Corollary C1 can be easily derived from the distributed formula through currying. Corollary
C2 follows corollary C1 when we substitute f with partial derivative. Corollary C3 follows corollary C1
when we substitute f with the φ function. Corollary C4 is attained when we apply C1 and then the
identity formula (F1).

The other group consists of corollaries C5 to C9, which oﬀer conveniences for transforming φ expres-

sions into closed forms for symbolic diﬀerentiation.

Corollary C5 is proved as follows.

Proof. Because of F4, we have the following relations:

d(2) = f (φ(2)

d(3) = f (φ(3)

d(1) = f (φ(1)
L (p, d)) = f (p)
L (p, d)) = f (d(1)) = f (f (p)) = f [2](p)
L (p, d)) = f (d(2)) = f (f [2](p)) = f [3](p)
...

d(n) = f (φ(n−1)

L

(p, d)) = f (d(n−1)) = f (f [n−1](p)) = f [n](p)

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

Because of the deﬁnition of SSA, after the loop entry function φL in the ﬁnal iteration of loop L,

there shall be no other assignment to d before the exit of the loop. Hence, dexit(L) = d(n) = f [n](p).

Corollaries C6 to C9 are variants of C5 with function f instantiated in several forms. They can be

proved in a way similar to C5.

4.4 Examples

We now use several examples to show how φ-calculus helps symbolic diﬀerentiation. We start with two
simple ones and end with a more complicated case with nested loops, breaks, if-else, and arrays.

(I) If-Else Example. We ﬁrst look at the example in Figure 3. The φ functions resolve the diﬃculty
for getting a closed form for the code. With the code in SSA, the derivation of the closed form for the code

10

can simply ignore the conditional statements. What it needs to do is only to apply simple substitution
of names with corresponding expressions based on the data ﬂow. Figure 3(d) shows the closed form
obtained from the SSA form in Figure 3(c). We add subscripts to the φ functions to help tell diﬀerent φ
functions apart.

According to corollary C2, we can apply derivation on x on the closed form and distribute the operation
to the arguments of the φ functions. The result is shown in Figure 3(e). The underlines indicate two
simpliﬁcation opportunities. (i) The ﬁrst underlined expression, (p+qx)∗0.5∗q∗(p+qx)−0.5, can be easily
simpliﬁed by symbolic engines into 0.5q(p + qx)0.5. (ii) The second simpliﬁcation opportunity appears
after the distributive formula (F2) is applied such that the ﬁnal term (1 + ep+qx) in the expression
in Figure 3 (e) is distributed into the φ functions. That term cancels the denominator of the second
underlined expression. Figure 3 (f) shows the result after symbolic simplication. It is worth noting that
such optimization opportunities appear because of φ-calculus: They are both about interactions of the
codelets across the boundaries of conditional branches, and hence would need the involved computations
to be treated together. If each straight-line section of the codelet is symbolically diﬀerentiated individually
as shown by the segments of interest (SOIs) in Figure 3, those simpliﬁcations cannot get exposed. From
Figure 3(f), code can then be generated with the φ functions materialized with conditional statements
that check the corresponding branching decisions recorded during the primal computation.

(II) Simple Loop Example. Figure 4 (d) shows how φ-calculus helps produce a closed form for
the loop in Figure 4 (a). With φ-calculus notations, the loop is symbolically represented in Figure 4
(c), on which, corollary C6 removes the loop notation and the loop entry φ, and produces the closed
form of s3 at the exit of the loop: a + k × x (k is the loop trip count). The application of loop exit
formula F5 to the expression of s4 removes the loop exit φ function, producing the simple expression
a + k × x. Symbolic diﬀerentiation can then be applied easily. For illustration purpose, this loop is made
simple and the derivation of the closed form may resemble the recognition of induction variables in loop
parallelizations [31]. The next example gives a more thorough demonstration of the power of φ calculus.
(III) Complex Example (BGDHyperOpt). Figure 6 shows a more complex example. The code
in Figure 6 (a) implements the use of batch gradient descent to determine the linear model on a dataset
(x for inputs, y for response). The diﬀerentiation of interest is d(err)/dr, where r is the learning rate;
this gradient can be used in ﬁnding out the best learning rate—a so-called meta learning problem that
optimizes hyperparameters of a machine learning process.

The code consists of a for loop nested within a while loop; the while loop has a break in an if-else
statement; there is another for loop following the while loop. To our best knowledge, no prior work can
compute d(err)/dr symbolically due to the control ﬂow complexities.

Figure 6(b) shows the SSA form of the codelet. It includes nine φ functions; one of the loop exit φ

functions (φk(cid:48)) has three arguments because of the break statement in the while loop.

Figure 6(c) shows the application of φ-calculus with the text boxes indicating the formulae or corol-

laries used at the important steps. We explain the process as follows.

inner for loop; Formula F5 then resolves the φ(cid:48)

Lines 1-5: Corollary C7 helps attain the closed-form expression of the value of d3 at the exit of the
i function and leads to the closed-form expression of d5.
Lines 7-10: This part tries to get the closed-form expression for the third argument (w3) of the φk(cid:48)
function on Line L6 in Figure 6(b). The part starts with a series of substitutions based on the results
from Lines 1-5 in Figure 6(c), and then uses corollary C6 to get the closed-form expression of the value
that w3 has at the normal (rather than via break) exit of the while loop.

Lines 12-16: This part tries to get the closed-form expression for the second argument (w2) of φk(cid:48).
It starts with substitutions with the results obtained already. It then uses the distributive formula F2 to
transform the φk function on Line 14 in Figure 6(c) to a form matching the lhs form in corollary C6. The
transformation is to factor out the terms in the second argument of φk such that the second arguement
turns into pure w2 as shown on Line 15. Then corollary C6 can be applied, resolving the loop notation
and also the φk function and producing the closed-form expression of w2 at the exit of the while loop as
shown on Line 16 in Figure 6(c) (K stands for the trip count of the while loop).

Lines 17-18: simple substitutions of the three arguments in φ(cid:48)
k.
Lines 20-24: This part tries to get the closed-form expression for e4. It ﬁrst applies corollary C7 to
the expression on Line 20 in Figure 6(c) to resolve φj and the loop notation, producing the closed-form

11

expression for e3exit on Line 21.
closed-form expression on Line 24 for e4.

It then applies formula F5 to resolve φ(cid:48)

j on Line 22, producing the

Line 26: A simple substitution with the results produced so far gives the closed-form expression
of the ﬁnal variable err. (For the sake of readability, we leave out the substitution of w4.) Symbolic
diﬀerentiation can then be applied to err on r.

This part has demonstrated the applications of φ-calculus to several concrete examples. We will

present the general use of φ-calculus in the overall algorithm of coarsening in the next section.

5 SOI Identiﬁcation

With φ-calculus removing the barriers of control ﬂow for coarsening, a segment of interest (SOI)—that is,
the segment of code for symbolic diﬀerentiation—of a program can be much larger than a basic block. A
larger SOI often oﬀers more opportunities for optimizations, but it is not always better due to a tradeoﬀ
caused by two factors.

The ﬁrst is ”expression swell”. As aforementioned, ”expression swell” refers to the phenomenon that
the derivative often has a much larger (in the worst case, quadratically larger) representation than the
original function has [5]. As a result, a very long expression can cause large memory usage and long
running time of symbolic engines.

The second factor is the tension between coarsening and computation reuse. In coarsened diﬀerentia-
tion, without a careful design, some computation reuse opportunities could get lost due to computation
reordering caused by coarsening transformations, a phenomenon we call reuse deprivation.

Deprivation Example. The impact of reuse deprivation can be seen on xt+2,0 and xt+2,1 in the
CartPole example in Figure 2. As Figure 2(c) shows, they are both computed from xt+1,1. So po-
tentially, if d(xt+2,0/d(at) has been computed, d(xt+1,1)/d(at) could be known and could be reused in
computing d(xt+2,1/d(at). Coarsening the computations from at and Xt (i.e., [xt,0, xt,1, xt,2, xt,3]) to
xt+2,0, however, deprives that reuse opportunity. The coarsening result has been shown in Equation 3.1,
in which the holders of intermediate results, such as xt+1,1, disappear. The diﬀerentiation over at is
shown in Equation 3.2, which has no d(xt+1,1)/d(at) or the derivatives of any other intermediate vari-
ables over at. As a result, when we need to compute the derivative of xt+2,1 over at, we cannot reuse
those intermediate derivatives.

The example illustrates a tension between reuse and coarsening. The larger is the coarsening granu-
larity, the more opportunities there are for the enabled symbolic diﬀerentiation and optimization to take
eﬀect, but at the same time, it could incur deprivation of computation reuse opportunities.

What adds subtly to the relation is that reuse deprivation does not always lead to fewer reuse opportu-
nities. Some reuse deprivations transform the reuse opportunities to another form. For instance, suppose
that we have a way to get a closed form for the entire computation from Xt to L. All reuses, including
xt+1,1 for xt+2,0 and xt+2,1, turn into explicit sub-expressions in the closed form of L. There can hence
be reuse opportunities exposed between them in the diﬀerentiation of the closed-form expression. The
condition for such a transformation of reuse to occur is that the closed-form expression must subsume
both parties that contain the reusable computations.

A single solution, reuse-aware SOI identiﬁcation, addresses both factors. Reuse-aware SOI identiﬁca-

tion refers to an algorithm that solves the following optimization problem:

Deﬁnition 2. Optimal SOI segmentation problem: Let G be a series of computations, l be the
upper limit of the allowed sizes of an SOI, P be the set of valid partitions of G, that is, for any partition
S in P , no element in S is larger than l. The problem is to ﬁnd the optimal partition S∗ ∈ P such that
the total running time is minimized, that is,

∀Q ∈ P, ad(S∗) +

(cid:88)

s∈S∗

compute(dif (s)) ≤ ad(Q) +

compute(dif (q)),

(cid:88)

q∈Q

12

Figure 6: Application of φ-calculus on hyperparameter optimizations, showing the treatment of loops
and branches.

13

          w1 = 0;
k1 = 0;
          if (k1 < T) 
          w2 = ɸk(w1, w3);           k2 = ɸk(k1, k3);           k3 = k2 + 1;           d1 = 0;
          i1 = 0;
          if (i1 < M)
          d2 = ɸi(d1, d3);
          i2 = ɸi(i1, i3);
          d3 = d2 + 2x[i]*y[i]  - 2*x[i]*x[i]*w2;
          i3 = i2 + 1;
          if (i3 < M)     goto L4
          d4 = ɸi’(d1, d3);
          d5 = d4/M
          if (d5 < 0.001)
              goto L6;
          else
              w3 = w2 - r * d5;
          if (k3 < T)   goto L3;
          w4 = ɸk’(w1, w2, w3);           e1 = 0
          j1 = 0
          if (j1 < M) 
          e2 = ɸj(e1, e3);           j2 = ɸj(j1, j3);
          e3 = e2 + (y[j] -x[j] * w4)2
               j3 = j2 + 1
          if (j3 < M)      goto L7
          e4 = ɸj’(e1, e3)
          err = e40.5/M 1  
2 
3
L3: 5
6
7
8
9
L4: 11
12
13
14
L5: 16
17
18
19
20
21
L6: 23
24
25
L7: 27
28
29
30
L8: 32for
loopwhile
loopfor
loop(a) Original code(b) SSA form//  r: the learning rate source (active variable) //  err: the error; sink (output variable) 1   w=0
2   k = 0
3   while (k<T)
4      k++
5      d=0
6      for (i=0; i< M; i++)
7          d += 2*x[i]*(y[i] - x[i] * w)
        d = d/M
8      if (d<0.001)
9          break
10    else
11        w = w - r * d
12  e = 0
13  for (j=0; j<M; j++)
14     e += (y[i] - x[i] * w)2
15  err = (e0.5)/M𝔏i d3 = ɸi(0, d3) + 2x[i]*y[i] - 2*x[i]*x[i]*w2
d3exit = ∑i 2x[i]*y[i] - 2*x[i]*x[i]*w2
d4 = ɸi’(d1, d3) = ∑i 2x[i]*y[i] - 2*x[i]*x[i]*w2      =  2Sxy- 2SX2*ɸk(0,w3)
d5 = (2Sxy- 2SX2*ɸk(0,w3))/M
𝔏k w3 = w2 - r*d5
𝔏k w3 = ɸk(0, w3) - 2rSxy/M + 2rSX2*ɸk(0,w3)/M
𝔏k w3 =  (1+2rSX2/M)*ɸk(0, w3) - 2rSxy/M
w3exit = -2(r/M)Sxy ∑K-1k=0(1+2(r/M)Sx2)k 𝔏k w2 = ɸk(0, w3) 
𝔏k w2 = ɸk(0, w2-2(r/M)*(Sxy- SX2*w2))
𝔏k w2 = ɸk(0, (1+ 2(r/M)SX2)w2-2(r/M)*Sxy)
𝔏k w2 = (1+ 2(r/M)SX2)*              ɸk(2(r/M)*Sxy/(1+ 2(r/M)SX2),w2) - 2(r/M)*Sxy
w2exit = -2(r/M)Sxy ∑K-2k=0(1+2(r/M)Sx2)k Distributive (F2) Corollary (C7) (c) Application of ɸ-calculus ( and the formula used in boxes )Corollary (C7)w4 = ɸk’(w1, w2exit, w3exit)
        = ɸk’(0, -2(r/M)Sxy ∑K-2k=0 (1+2(r/M)Sx2)k, 
                  -2(r/M)Sxy ∑K-1k=0 (1+2(r/M)Sx2)k) 𝔏j e3 = ɸj(0, e3) + (y[j] -x[j] * w4)2
e3exit = ∑j (y[j] - x[j]*w4)2 
e4 = ɸi’(e1, e3) = ∑j (y[j] - x[j]*w4)2
     = ∑j y[j]2 - 2x[j]y[j]*w4+x[j]2*w42
     = Sy2 - 2Sxy *w4+ SX2*w42
err = (Sy2 - 2Sxy *w4+ SX2*w42)0.5/M           1   2  3 4 5612 13 14 15 1617 18 19 20 2122 23 2425 26 7 8 9 10 Formula (F5)  Corollary (C6)  Corollary (C6)  Formula (F5) Sxy = ∑i x[i]*y[i];                Sx = ∑i x[i];               Sy = ∑i y[i];              SX2 = ∑i x[i]2;                 Sy2 = ∑i y[i]211where compute(dif (x )) is the amount of computation involved in running the symbolically diﬀerentiated
code segment for code x, and ad(X) is the cost of the remaining AD diﬀerentiation of X after symbolic
diﬀerentiation.

The upper bound of SOI size l in the problem description ensures that the symbolic engine works well
even in the presence of the ”expression swell” eﬀects. The problem description indicates three factors
relevant to SOI deﬁnitions.

1) The cost ad(X). This cost is incurred at the boundaries of SOIs. Symbolic diﬀerentiation of an
SOI computes only the derivatives of the active output variables of this SOI on the active input variables
of this SOI. These derivatives have to be connected into a chain by AD to compute the derivatives of the
ultimate active output variables on the ultimate active input variables. As a result, the more SOIs there
are, the more AD overhead is there, and the larger is ad(X). In the extreme case where each operation is
an SOI, ad(X) would equal to the cost taken by the default AD without coarsening. So this factor calls
for larger SOIs.

2) Computation simpliﬁcations. The cost (cid:80)

x∈X compute(dif (x )) is smaller if more computations are
simpliﬁed. As two cancellable computations falling into two separate SOIs are not going to get cancelled
by the symbolic engine, maximization of simpliﬁed computations also calls for larger SOIs.

3) Computation reuse. Maximizing computation reuse helps reduce the cost (cid:80)

x∈X compute(dif (x ))
as well. Unlike computation simpliﬁcation, computation reuse exists both within and across SOIs. Ex-
ploiting reuse within an SOI happens in the default symbolic optimization and code optimization (e.g.,
common subexpression elimination (CSE) [9]). Reuse across SOIs is the natural result of the chain rules
of diﬀerentiation, as shown by the potential reuse of d(xt+1,1)/d(at) in computing d(xt+2,0)/d(at) and
d(xt+2,1)/d(at) in Figure 2. In general, if y is an active output variable of both SOIa and SOIb and it
is also an active input variable of SOIc, then the derivative of the ultimate active output z on y (dz/dy)
can be used in computing the derivatives of z on the active inputs of both SOIa and SOIb. Besides
saving computations, reuses are also helpful for mitigating the “expression swell” problem as they split a
long expression into shorter ones as noted in some previous work [35, 21]. Because expanding SOIs could
lose inter-SOI reuses as the deprivation example has shown, this factor suggests that simply maximizing
SOIs to the upper limit l cannot always give the best SOIs.

Finding the optimal SOI segmentation is diﬃcult. For an SSA representation with N instructions,
assuming every instruction can be put into an SOI, the number of possible partitions is between lN/l
and lN , where, l is the upper limit of the allowed size of an SOI. The reason is that the number of
SOIs is between N/l and N , while the size of an SOI has l possibilities in the lower-bound case and
up to l possibilities in the upper-bound case. Besides the exponential space, it would require detailed
performance and overhead modeling, which is hard to be precise at static compile time.

It is however important for a viable solution to take all these factors into consideration. Figure 7(a)
outlines our designed algorithm. For a given function f , for each of its active variable s that outlives
f , the algorithm gets its def-use chain, which captures all the deﬁnitions in f that lead to the value of
s. It then builds a def-use region tree out of all the deﬁnitions on the def-use chain. Def-use region tree
is a data structure inspired by the classic code region hierarchy in compilers [9]. Traditionally, a code
region is deﬁned as a collection of nodes N and edges E such that (i) a header node h in N dominates
all other nodes in the collection; (ii) if p is in N , then m must be in N if m reaches p without going
through h; (iii) E includes all edges between nodes in N , except for those that enter h. In the code region
hierarchy of a program, each code region is represented by a node in the hierarchy subsumed under the
nodes that represent its enclosing code regions. Def-use region tree has two major diﬀerences from code
region hierarchy: (i) only relevant variable deﬁnitions are considered; (ii) every loop-exit φ function is
put as part of the region of the associated loop. The second property is for convenience in the derivation
of symbolic expressions for loops. As an example, Figure 7(b) shows the def-use region tree of all the
deﬁnitions on the def-use chain of err in Figure 6(b); each element in the boxes is a line number in
Figure 6(b). The three solid (green) boxes are the three loops. The three loop-exit φ functions (Lines
L5, L6, L8 in Figure 6(b)) are put together with the three loops respectively.

The SOI identiﬁcation algorithm then traverses the region tree in a bottom-up order, as outlined in
Figure 7(a). For each node, if it has no large child node (i.e., exceeding the SOI size limit), its symbolic
expression is derived through the φ-calculus. If the size of the derived expression exceeds the SOI size

14

Figure 7: (a) Algorithm of reuse-aware identiﬁcation of SOIs for coarsening. (b) The def-use chain of err
in the example shown in Figure 6(b) with r as the active input; inactive inputs (x, y, M ) are omitted.
(c) The def-use region tree for err in Figure 6(b); each element in a box is a line number in Figure 6(b);
solid (green) boxes stand for loops.

limit, that node is marked as a large node, and if it is a leaf node, it is split into two smaller nodes; the
splitting point is chosen to be the variable that is contained in that node and has the largest number
of references (and hence reuses) in f . The two new nodes created by the split are added to the front of
worklist. If the current node has large children, there is no need to go up further in the def-use region
tree as the upper nodes can only become even larger. In that case, the algorithm examines the immediate
children of this node, merge consecutive small children nodes (up to the SOI size limit); after that, it
puts each of the children nodes smaller than the limit as an SOI. The algorithm continues until worklist
becomes empty. The size limit L can be empirically selected based on the machine and the symbolic
engine.

The algorithm follows the principle of maximizing the size of SOIs within the SOI size limit while

respecting reuses when it is necessary to split a def-use chain into multiple SOIs.

6

Implementation

We implement the coarsening optimization on an in-house AD tool named DiﬀKt. The tool was developed
for Kotlin, a cross-platform, statically typed, general-purpose programming language with type inference.
The tool itself is also written in Kotlin. We choose it as the basis mainly because of its availability and the
statically typed nature of Kotlin which oﬀers conveniences for static code analysis and transformations.
But as a general optimization technique, coarsening can be potentially applied to many other AD tools;
for some (e.g., Python AD tools), it may need to be done dynamically.

DiﬀKt was developed and optimized by 10+ engineers in industry in over a year. It supports both
CPU and GPU, with high-performance native math/DNN libraries for Tensor computations. The tool is
planned to open source in the near future. A systematic benchmarking of the tool over other AD tools
is yet to be done, but preliminary measurements show that it outperforms PyTorch AD [25] by over 10×
on scalar-intensive cases (e.g., HookeanSpring in Section 7), and achieves comparable speeds on common
deep learning models where pre-existing libraries are called for gradients calculations of the standard

15

erre4e1e3e2w4w1w2w3d5d4d3d1d2rroot1,2L3, 7, L4, 12, L5, 16, 20, L6 L3, 7L4, 12, L516, 20L6 L4, 12L523 L7, 28, L8 L7, 28L8 32 L: the upper limit of the size of an SOI
f: a function in SSA
S: the set of active sink variables in f
SOI: the placeholder of all SOIs of f
1.  SOI = { }
2.  for each s in S
3.     C = f.getDefUseChain(s)
4.     T = f.getRegionTree(C)
5.     W: a worklist with all nodes in T added in 
6.          bottom-up order
7.     while (W.notEmpty)
8.         n = W.removeANode( )
9.         if (n.hasLargeChildren( ))
10.           n.markLarge( )
11.           n.mergeSomeChildren( )
12.           SOI.add(all small children in n)
13.           next;
14.       e = n.getSymbExp( )
15.       if (e.size > L) 
16.          n.markLarge( )
17.          if (n.isLeaf( ))
18.             newNodes = n.splitOnReuses( )
19.             W.addToFront(elements in newNodes)(a)(b)(c)Figure 8: An example showing how the coarsened results are integrated into the original program. The
original primal code (left) is transformed to a form (right) such that the calculations of z in the SOI are
put into one expression, and an adjoint is appended to that expression, which, at runtime, makes the
AD take its content as the shortcuts for calculating the derivatives of z on the two active variables of the
SOI, w and v. The results are used by the default AD in diﬀerentiating the rest of the code (i.e., from w
and v to x and y.

DNN layers under the hood of both of them. (As PyTorch AD and the Kotlin AD tool are in diﬀerent
programming languages, the comparison is only to give readers a sense about the industrial quality of
the default tool.)

Similar to many other AD tools (e.g., PyTorch [25], JAX [12]), DiﬀKt is a library-based implementa-
tion, enabling AD through operator overloading via a generic class Tensor. Implemented in 250K lines
of code, it supports backward AD and includes a Tensor typing module as well.

As with other Automatic Diﬀerentiation (AD) tools (e.g., Zygote [19]), DiﬀKt also allows the use of
adjoints for custom diﬀerentiation. For a given expression e, if a custom diﬀerentiation of e is provided,
the AD process will automatically invoke it rather than conduct the default operation-by-operation dif-
ferentiation. The diﬀerentiations of the SOIs generated by coarsening are integrated into the original
program as custom adjoints. An example is shown in Figure 8. The coarsening results form the content
of the adjoint, which is associated with the coarsened primal expression through the call ”setIntermedi-
ateAdjoints”. If coarsening produces code for diﬀerentiating the entire primal code, the compiler simply
replaces the calls of the corresponding backward function with the generated code; if the compiler in
addition determines that the primal results are used in the program only for getting the derivatives, the
compiler removes the invocations of the primal code.

Our implementation of coarsening on DiﬀKt is based on a Kotlin compiler. Our experiments focus on
ﬁrst-order backward AD, but it is worth noting that coarsening, as a way to oﬀer shortcuts in AD, can in
principle help higher-order diﬀerentiation and forward or mixed-direction AD as well. In addition to the
φ-calculus and the SOI identiﬁcation as presented earlier, our implementation also includes loop unrolling
and the use of the primal computation results in the generated adjoint functions when possible. As a side
beneﬁt of coarsening, our implementation of coarsening optimizes the primal in addition to the diﬀeren-
tiation: After getting the closed form of the primal computation, it applies symbolic optimization to the
primal and regenerates the code; an example is shown in the BGDHyperOpt benchmark in Section 7.2. In
the case where only diﬀerentiation is needed and the entire primal can be symbolically diﬀerentiated, the
optimizer removes the primal from the program when possible—an optimization elusive to existing AD.
In the current implementation, for proof of concept, we use a symbolic engine extended from Sympy [4],
an open-source symbolic manipulation tool.

Like many other compiler-based optimizations that change the order of computations, coarsening could
aﬀect the numerical precision. A convenience oﬀered by coarsening is that measures to avoid numerical
unstableness can be seamlessly integrated into the code generation in coarsening. The code generator is
equipped with the patterns for dealing with common numerically unstable expressions. Before it generates

16

fun cubeTransformed() {
        val x = Tensor(5f).asVar()
        val y = Tensor(3f).asVar()
        val w = x - 2f * y
        val v = y * x - x
        ... 
        val z = (w * w * w - v * v * v).setIntermediateAdjoints(
                sequenceOf (
                        w to w * 2f ,   // dz/dw from coarsening
                        v to v * -2f     // dz/dv from coarsening
                )
        )   // using the coarsened results to compute 
            // derivatives of z w.r.t. w & v
        z.backward()
    }fun cube() {
        val x = Tensor(5f).asVar()  // an active variable
        val y = Tensor(3f).asVar()  // an active variable
        val w = x - 2f * y
        val v = y * x - x
        ...   // other code
        val z1 = w * w * w
        val z2 = v * v * v
        val z = z1 - z2
        z.backward()   // get derivatives of z w.r.t x & y
    }
SOIOriginal code with ADTransformed code after coarseningTransformationTable 1: Benchmarks and Conﬁgurations

Name

Domain

BGDHyperOpt Meta-

Brachist.

CartPole

HMC

HookeanSpring

Learning

Math.
Physics

Deep
Reinforcement
Learning
Statistic
Sampling
for Prob. Prog.
Physical
Simulation

QWOP

Gaming

Description
Optimizing the learning rate
of batch gradient descent
based linear regression
Brachistochrone curve
calculation

Training a CartPole system

Hamiltonian Monte Carlo
Sampling for logistic
regression
Simulating the dynamics of a
Hookean Springs system

An avatar learns walking
via motion optimization

Conﬁgs
200 data records
1
1000 data records
2
2000 data records
3
200 data points
1
400 data points
2
1000 data points
3
an update every 6 steps
1
an update every 8 steps
2
an update every 10 steps
3
100 one-dim records
1
1000 two-dim records
2
800 three-dim records
3
10 vertices
1
20 vertices
2
40 vertices
3
light-weight ﬁgure
1
2 medium-weight ﬁgure
3

heavy-weight ﬁgure

Machine Conﬁguration
devServer

Intel(R) Xeon(R) Gold 6138 40-core CPU 2.00GHz, 250GB, CentOS Stream 8, Kotlin
1.4.20-M1, Java HotSpot 64-Bit Server VM, Java 1.8.0 192

Table 2: Machines

Macbook MacBook Pro, 2.4GHz 8-core Intel Core i9, 32GB 2667MHz DDR4, MacOS Catalina (v.

10.15.7), Kotlin 1.4.20-M1, Graalvm 20.3.0, Java 11.0.9

the code for a symbolic expression, it examines it to identify the numerical unstable expressions through
pattern matching, and generates the code corresponding to their numerically stable forms. For expression
log(1+e−xβ), for example, as the code generator ﬁnds out that the expression matches one of the patterns
in its unstable list, log(1 + en), it generates the code to discern the value of the exponent, as illustrated
as follows (MAXEXP is set to 40 in our implementation):

temp0 = -xβ
temp1 = (temp0 ¿ MAXEXP)? MAXEXP : log(1+etemp0)

When the expression operates on tensors, the generated code uses masking functions (like where in
PyTorch) for eﬃciency. A concrete example of numerically stable code generation in coarsening is the
HMC benchmark detailed in Section 7.2.

7 Evaluation

To evaluate the eﬃcacy of the proposed techniques, we test coarsening on six applications in 18 total
conﬁgurations on two diﬀerent machines. Backward AD is used. The results show that the optimization
improves the diﬀerentiation speed by 1.03-27×, and the whole application execution speed by 1.08-11×.

7.1 Methodology

Benchmarks There are no common benchmark suites designed for evaluating AD. We collected six
applications from several domains where AD is important, and implemented them in Kotlin with the
Kotlin AD library. Table 1 lists the set of benchmarks used in the experiments. These benchmarks come
from several domains, from physical simulation to statistical sampling, deep reinforcement learning,
gaming, and meta learning. They also show a range of code complexities, with BGDHyperOpt featuring
control ﬂow complexities as Figure 6 has shown, Brachist.
featuring a case where primal computation
could be potentially removed, CartPole featuring a combination of matrix-based Deep Neural Network and

17

scalar-based environment simulations, HMC featuring potential value overﬂow incurred by exponential
computations, HookeanSpring featuring a sequence of regular vector operations, and QWOP featuring
a long function with many small loops and if-else statements. For each benchmark, we include three
conﬁgurations as listed in the right column of Table 1, which will be explained later in the discussion of
the results of each application. We repeat the performance measurements multiple times and report both
the mean and standard deviation of the timing results. Kotlin runs on Java virtual machines. For both
the baseline and the optimized versions, the JRE went through a warm-up phase before timing starts to
get the stable performance.

Machines Because in practical scenarios, those AD-based applications may run on both servers and
personal computers/laptops, we have measured the performance of the applications on both kinds of
machines. Table 2 provides the machine details.

7.2 Results

In this part, we ﬁrst present an overview of the performance, and then provide detailed discussions on
each benchmark.

Table 3 reports the overall performance, where ”baseline” represents executions of the default Kotlin
AD tool and ”opt” represents executions after coarsening is applied. The ”Diﬀerentiation Time” column
reports the time taken by diﬀerentiation in one iteration of each benchmark, while the ”Overall Time”
column reports the overall time of an iteration. We make two observations.

(1) Coarsening brings 1.03–27× speedups to the diﬀerentiation of the benchmarks, and 1.08-11×
speedups to the overall execution. In most cases, the overall speedups are smaller than the diﬀerentiation
speedups as there are some parts of the computation in the programs outside the part of the code targeted
by the coarsening optimization (i.e., the part involved in diﬀerentiation). Exceptions are Brachist. and
HookeanSpring; it is because in those two original programs, the only purpose of the primal computations
are to let the AD to compute the gradients. Because coarsening generates code that can directly computes
the gradients, the optimization removes the primal computations completely; the overall time is hence
shortened even more than the time savings on the diﬀerentiation. (A side observation is that in all cases,
the laptop runs faster than the server, probably due to its faster CPUs and the use of a more recent
version of Java Runtime.)

(2) Coarsening is consistently beneﬁcial; it saves the execution time across benchmarks, conﬁgurations,
and machines. The main reasons for the time savings are three: (i) the savings of the operator overloading
overhead of AD, which comes from object boxing and memory allocations; (ii) the simpliﬁcations of the
computations thanks to the large-scoped symbolic diﬀerentiation and optimizations; (iii) the removal of
unnecessary primal computations. We next elaborate these beneﬁts through in-depth examinations of
each of the benchmarks.
BGDHyperOpt. BGDHyperOpt is a meta-learning program and has been introduced in Example III
in Section 4.4 and Figure 6. Meta-learning entails inspecting and optimizing a machine learning process,
which has recently drawing lots of interest. This program tries to optimize learning rates through gradient
descent. The three conﬁgurations correspond to three diﬀerent sets of inputs.

The SOI in this program is the entire function that computes the learning error as shown in Figure 6(a).
There are nine φ functions in the SOI. Despite the control complexities, coarsening is able to get the
closed-form expression for the entire gradient computation and apply symbolic diﬀerentiation on it, giving
more than 23× diﬀerentiation speedups in all cases. The primal cannot be removed because the generated
diﬀerentiation code must use the trip-counts of the while loop in the primal. As a result, the overall
speedup is about 8×. Meanwhile, coarsening saves over 70% of memory allocations because many of the
data allocations in the default AD are for holding intermediate data objects which are no longer needed
after coarsening.

We did an ablation study to examine the beneﬁts from the φ-calculus.

In the study, we apply
coarsening without φ-calculus; symbolic diﬀerentiation is hence applied to only the inner-most loop
(which is written as a single Tensor statement) and the code after the while loop. The diﬀerentiation
speedups drop from 23-27× to 8-9×:

18

Table 3: Experimental Results: Time per iteration and Speedups
Diﬀerentiation Time(ms)

Overall Time(ms)

Machine
devServer

Benchmark
BGDHyperOpt

Branchist.

CartPole

HMC

HookeanSpring

QWOP

macBook

BGDHyperOpt

Branchist.

CartPole

HMC

HookeanSpring

QWOP

Conﬁg
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3

opt
baseline
0.20± 1%
5.44± 6%
0.20± 2%
5.22± 8%
0.20± 2%
5.37± 6%
0.04± 7%
0.04± 3%
0.15± 1%
0.19± 36%
0.55± 2%
0.39± 4%
52.15± 4% 46.86± 1%
15.69± 1% 14.51± 1%
15.77± 2% 14.06± 2%
0.67± 2%
2.89± 5%
1.59± 4%
4.55± 1%
2.19± 5%
5.58± 7%
0.01± 2%
0.10± 4%
0.11± 8%
0.03± 2%
0.23± 12% 0.05± 13%
1.97± 6%
1.46± 4%
25.71± 5% 17.81± 6%
32.17± 5% 21.20± 6%
0.14± 2%
3.59± 4%
0.15± 6%
3.59± 3%
3.66± 4%
0.15± 3%
0.04± 18% 0.04± 12%
0.12± 2%
0.13± 4%
0.44± 2%
0.31± 5%
29.46± 3% 26.42± 2%
8.76± 0%
9.43± 2%
8.35± 1%
9.06± 0%
0.59± 2%
2.60± 1%
1.24± 1%
3.88± 1%
1.61± 1%
4.30± 1%
0.01± 7%
0.08± 8%
0.02± 2%
0.09± 11%
0.10± 2%
0.04± 4%
1.16± 2%
1.57± 3%
20.53± 2% 14.70± 2%
24.69± 2% 16.81± 1%

speedup
27.44X
26.18X
26.82X
1.10X
1.22X
1.41X
1.11X
1.08X
1.12X
4.29X
2.86X
2.54X
6.62X
4.09X
4.52X
1.35X
1.44X
1.52X
25.15X
23.43X
24.23X
1.03X
1.08X
1.42X
1.12X
1.08X
1.09X
4.42X
3.13X
2.67X
5.72X
4.17X
2.46X
1.35X
1.40X
1.47X

opt
baseline
1.22± 2%
10.44± 5%
1.21± 2%
10.12± 7%
1.21± 2%
10.36± 6%
0.04± 7%
0.09± 3%
0.15± 1%
0.32± 34%
0.69± 2%
0.39± 4%
55.06± 4% 49.38± 1%
16.99± 1% 15.65± 1%
17.07± 2% 16.24± 2%
0.94± 3%
3.26± 5%
2.11± 4%
5.17± 1%
2.79± 5%
6.35± 7%
0.01± 2%
0.16± 5%
0.18± 8%
0.03± 2%
0.38± 10% 0.05± 13%
3.89± 4%
2.76± 3%
44.50± 3% 29.61± 4%
56.86± 3% 36.37± 4%
0.86± 2%
7.23± 4%
0.91± 4%
7.31± 3%
7.41± 3%
0.91± 4%
0.10± 14% 0.04± 12%
0.12± 2%
0.23± 4%
0.55± 2%
0.31± 5%
31.16± 3% 27.92± 2%
11.89± 2% 11.03± 1%
12.14± 1% 11.25± 0%
0.82± 2%
2.90± 1%
1.64± 1%
4.37± 1%
2.06± 0%
4.85± 1%
0.01± 7%
0.14± 8%
0.02± 2%
0.15± 8%
0.17± 2%
0.04± 4%
2.45± 2%
3.60± 4%
40.43± 2% 26.46± 2%
50.01± 2% 31.89± 1%

speedup
8.52X
8.37X
8.56X
2.51X
2.08X
1.79X
1.12X
1.09X
1.05X
3.46X
2.45X
2.27X
11.02X
6.72X
7.53X
1.41X
1.50X
1.56X
8.41X
8.06X
8.18X
2.38X
1.91X
1.80X
1.12X
1.08X
1.08X
3.56X
2.67X
2.35X
9.84X
7.13X
4.09X
1.47X
1.53X
1.57X

19

Conﬁg
Speedup(X) w/o φ-calculus
Speedup(X) w/ φ-calculus

devServer
2
7.80
26.18

1
8.46
27.44

3
8.08
26.82

macBook
2
9.34
23.43

1
9.24
25.15

3
9.39
24.23

i x[i] ∗ y[i] and (cid:80)

It can be seen that the larger scope of optimizations enabled by φ-calculus boosts the speedups by a
factor of three. Its eﬀects are multi-fold: (i) It allows a complete removal of the boxing overhead of the
Tensor data structure from the diﬀerentiation process, whereas without φ-calculus, as only part of the
diﬀerentiation is symbolically done, Tensors have to be used so that the operator overloading can still
work, which is what the remaining part of the AD depends on. (ii) It exposes large-scoped loop-invariant
calculations, for both the primal and the diﬀerentiation. Symbolic transformation and analysis of Lines
6-7 in Figure 6(a) can show that the loop involves the calculations of (cid:80)
i x[i] ∗ x[i]; when
the analysis scope spans across the entire while loop via φ-calculus, the optimization can easily recognize
that the two summations repeat in every iteration of the while loop and can be hoisted out of the while
loop. Similar phenomena are in the diﬀerentiation. Neither the default optimizers in the Java Runtime
underlying Kotlin or the coarsening without φ-calculus can recognize and take advantage of that. (iii) It
saves the remaining AD overhead that the version by coarsening without φ-calculus has to suﬀer.
Brachist. This program calculates the Brachistochrone curve (i.e., curve of fastest descent), which is
the one lying on the plane between a point A and a lower point B (called anchor points), where B is not
directly below A, on which a bead slides frictionlessly under the inﬂuence of a uniform gravitational ﬁeld
to a given end point in the shortest time. In each iteration, the program computes the time taken by
the bead to slide down the slope by summing the time it takes for each section of the current curve, and
then gets the gradient of every section over the total time. The three conﬁgurations correspond to the
number of sections that the target curve is regarded to be composed of. This is a relatively easy case,
but it demonstrates an important scenario where coarsening can remove the entire primal computation.
The entire primal code to compute the time taken to slide down the slope is identiﬁed as the SOI. With
coarsening, the AD tool can symbolically diﬀerentiate the entire computation. Because the program
only needs the gradients to update the curve in each iteration, it can now forego the computation of the
total time as the gradients can be directly computed. Therefore the coarsening optimization removes the
entire primal computation, making the program’s overall speedups even more than the speedups on the
gradients calculations. As Table 3 shows, the speedups on the diﬀerentiation part is modest (due to the
simplicity and regularity of the code), but the end-to-end executions get more signiﬁcant speedups (e.g.,
1.8-2.38× versus 1.03-1.42× on macBook). The overall speedups are more pronounced on the smaller
inputs because the primal computation weights more in those runs.
CartPole. CartPole is a deep reinforcement learning program as already introduced in Section 3. The
three conﬁgurations corresponding to the number of exploration steps observed before learner updates
the model parameters. It shows the least speedups among all the benchmarks, not because coarsening
is not eﬀective, but because the small portion of the optimized code weighs in the overall program.
Recall in Figure 2, the primal computation of CartPole contains two parts, the Neural Networks(NN)
and the environment update. As CartPole uses a simple simulation environment, the environment update
part weighs only about 15% of the primal time, with the rest dominated by the NN. As the NN has a
standard structure and the default gradient calculation is through a manually written highly polished
vendor library rather than the AD, the SOI is the second part, which updates the environment. There
are ﬁve φ functions in the SOI. The speedups on the diﬀerentiation of the environment update part are
actually signiﬁcant:

Conﬁg
Speedup(X) on diﬀerentiating
the environment update part

devServer
2
2.63

1
2.68

3
3.19

macBook
2
2.73

1
3.01

3
3.21

In cases where reinforcement learning is applied to more complex environments, the speedups on the
end-to-end execution by coarsening are expected to be more substantial.
HMC. HMC stands for Hamiltonian Monte Carlo.
It is one of the main algorithms in Probabilistic
Programming or Statistics for ﬁnding out posterior distributions of random variables through a carefully
designed Monte Carlo sampling process [6], as illustrated in Figure 9(a).

20

(a) HMC sampling in a space ([2])

(b) HookeanSpring ([27])

(c) QWOP

Figure 9: Illustrations of several benchmarks.

We ﬁrst gives a conceptual view of HMC. The core sampling function in HMC takes in two functions
as part of the arguments: function ”U” and function ”grad U”. The former does the primal computation
and the latter computes the derivative of ”U”. HMC calls them many times on diﬀerent values, more often
on ”grad U” than on ”U”. In this benchmark, HMC is used for logistic regression. Logistic regression is
a classic method for classiﬁcation. HMC is used to estimate the posterior distribution of the parameters
in the logistic regression model. Its ”U” function is as follows:

U (β) = βT X T (y − 1n) − 1T

n [log(1 + e−Xβ)] −

βT β
2σ2
β

where, β are the parameters in logistic regression, X and y are the input and response data (training
data), σ2
β is a hyperparameter (1000). All are vectors except that X is a matrix. The three conﬁgurations
correspond to three diﬀerent training data sets.

A feature of HMC is that when it needs the gradient on some values, it often cares about the gradients
but not the actual primal value. It does call the primal function at some places, but on values diﬀerent
from those needed for the calculation of the gradients.

But in the default implementation on AD, anytime gradient is needed, the primal is called because of
the inherent requirement for AD to work as we have described in Section 2. Coarsening takes the entire
primal function ”U” as the SOI, and there are one two-level nested loop and another two single-level
loops in the code with one of them containing an if-else statement. Coarsening is able to symbolically
diﬀerentiate it and generate the entire ”grad U” function. It hence can save many primal computations.
Overall, the speedups are 2.3-3.6×.

One special note on HMC is that the exponential term e−Xβ can easily result in value overﬂow;
special treatments must be given to large exponents (e.g., using −Xβ to approximate log(1 + e−Xβ
if −Xβ exceeds 80). The default AD-based tool uses a masking function to deal with that (similar
to ”where” in PyTorch). Without the masking scheme, an alternative would be to wrap each element
in −Xβ in a Tensor to discern their following uses; which creates huge runtime overhead, making the
program run about 25× slower. Without the dependence on Tensor or operator overloading, coarsening
is not subject to the problem; when it generates the code, it directly generates the appropriate code for
the cases where the exponential value is large.
HookeanSpring. HookeanSpring is a physical simulation program. It simulates mass-spring systems
as illustrated in Figure 9 [27]. It demonstrates the transitions of physics-based states as energy mini-
mization procedures. The program keeps optimizing the vertex positions of a spring system to ﬁnd some
conﬁguration that minimizes the total elastic energy. Every spring has some preferred rest length and

21

they naturally tend to recover their rest shapes over time. Each optimization step uses the gradients of
the spring vertex locations regarding to the system energy. The three conﬁgurations correspond to three
sizes of the spring system in terms of the number of spring vertices. Coarsening is able to take the entire
energy calculation of the Spring system as the SOI and symbolically diﬀerentiate it. As a result, the
primal computation which computes the system energy can be completely removed. The speedups are
4–11×. The program even runs faster than the original primal computation alone; the following table
shows the times taken in one iteration of the simulation and the relative speedups (median values of
repeated measurements are used):

Conﬁg
Original primal only(µs)
Exec. after coarsening(µs)
Speedups(×)

1
49.19
14.49
3.39

devServer
2
51.92
26.76
1.94

3
114.20
51.49
2.22

macBook
2
47.01
21.61
2.18

1
43.37
14.46
3.00

3
52.58
41.17
1.28

This result is signiﬁcant because there has been a common perception that a program would take a lot
more time to run if automatic diﬀerentiation is added into it. For example, a previous work considers 2.4-
4× slowdown after adding automatic diﬀerentiation as already close to the optimal [7]. This coarsening
result shows that with coarsening, after adding automatic diﬀerentiation, a program can even run several
times faster.
QWOP. QWOP is an avatar motion optimization program. It trains a virtual stick ﬁgure to run as
far as possible by providing a schedule for how much each muscle should be extended, as illustrated in
Figure 9(c). The three conﬁgurations correspond to three conﬁgurations of the mass of the body parts of
the stick ﬁgure. The special aspect about this program is that its core part is a 225-line function with 13
loops and many if-else statements. After loop unrolling, the function becomes 1117-line long. Coarsening
can successfully deal with the function, getting two SOIs, and achieving 1.17-1.51× overall speedups.

7.3 Potential on Other AD Tools

Coarsening is a general optimization for AD. To check its potential beneﬁts to AD tools beyond DiﬀKt,
we examined the performance of the benchmark BGDHyperOpt on three other AD tools: JAX [12] for
Python, Zygote [19] for Julia, and Adept [7] for C++.

For each of the three AD tools, we have two versions of the benchmark BGDHyperOpt: (i) the baseline
version which uses the default AD oﬀered by the tool; (ii) the coarsened version optimized by coarsening.
The latter was written based on the results from our symbolic engine. Table 4 reports the speedups of
the coarsened versions over the baseline counterparts. Please note that the JAX baseline version already
uses its JIT (the JIT gives 1.3-1.47X speedups over the default version that uses no JIT). The execution
times were measured on the Macbook after warm-ups. The speedups are 66×-335×, even greater than
on DiﬀKt, indicating the potential of coarsening as a general AD optimization technique.

Table 4: Speedups of the coarsened version over the baseline version on BGDHhyperOpt

AD Tool (Language)

Input Size

JAX (Python) [12]
Zygote (Julia) [19]
Adept (C++) [7]

2000

1000
87.4X 335.1X
90.8X
150X
96.2X
66X

8 Discussions

The study has demonstrated the signiﬁcant beneﬁts of coarsening on the Kotlin AD tool on ﬁrst-order
backward AD, the most popular kind of AD. It is easy to see that the technique can help other types of
AD implementations (e.g., forward or mixed directions and higher-order diﬀerentiation) as the outcome
of coarsening can always be used as a shortcut on the AD chains.

22

Our exploration of coarsening is at the static compilation time. The technique is potentially applicable
at runtime as well, which could be especially meaningful for languages (e.g., Python) that are diﬃcult
for static time analysis and transformations. In that case, runtime proﬁling could be useful, and extra
care (e.g., hot paths based selective optimizations) may be necessary to minimize the time overhead of
symbolic manipulations.

The current coarsening optimization applies to both regular and irregular loops as mentioned before.
But there is code with unstructured control ﬂows where it is even unclear what the loop is (e.g., code
formed by go-to statements in certain languages). In those cases, the SOIs could be set to the sections
within the branches that form the unstructured control ﬂows.

With coarsening, the compilation time does not have noticeable changes except for the time taken by
the symbolic engine in doing symbolic diﬀerentiation and other symbolic manipulations. As mentioned,
as a proof of concept, the current implementation uses an extended Sympy for that. Written in Python,
Sympy is not the most eﬃcient symbolic engine. For the benchmarks in the experiment, it takes up to a
minute to do symbolic diﬀerentiation.

Coarsening is based on the SSA form of a program. When a program has assignments to arrays,
array SSA [20] would be necessary to discern the diﬀerent ranges of data elements in an array when
they are treated diﬀerently in the program. The representation and corresponding analysis are more
complicated than on the basic SSA form. We found that for AD programs written in Tensor-based
AD libraries, in most cases, array SSA is not necessary. It is because in those programs, if there are
large arrays, operations on them are typically written as Tensor operations (e.g., C = A + B for Loop:
c[i] = a[i]*b[i]) without explicit references to individual array elements; for such representations, the
standard SSA still applies. Even if sometimes a part of the array elements are treated diﬀerently from
others, Tensor operations still suﬃce via Tensor masking operations (e.g., the HMC case). In the cases
where individual array elements are used and updated diﬀerently, those arrays are usually short and are
used in small loops; loop unrolling and scalar conversion can easily turn the code into a form amenable for
the standard SSA. Nonetheless, integration of array SSA with coarsening could still be useful especially
for AD tools without Tensor-like abstractions.

9 Related Work

There is a large body of work on eﬃcient AD. Optimizations range from checkpointing [15] to edge/vertex
eliminations on computation graphs [17], combination of forward and backward diﬀerentiation [11], loop
transformations [28], and so on. A recent work [29] proposes a diﬀerentiable programming language to de-
liver a semantics for higher-order functions, higher-order derivatives, and Lipschitz but non-diﬀerentiable
functions. Some of the relevant studies have been mentioned in earlier sections, and more on AD for ma-
chine learning can be seen in recent surveys [11, 34, 23]. Coarsening can be regarded as an optimization
complementary to those existing AD optimizations: They can be used together, with coarsening oﬀering
shortcuts and the other optimizations improving the remaining AD operations.

There are many tools capable of doing symbolic diﬀerentiation. Examples include the Calculus module
in Julia [1], SageMath [3], KotlinGrad [13], and Acumen which maps from analytical models to simulation
codes via symbolic diﬀerentiation [36]. None of them have addressed the complexities from control ﬂow on
symbolic diﬀerentiation, or the systematic integration of symbolic diﬀerentiation with AD. The existing
symbolic engines can diﬀerentiate only expressions not programs. For cases with simple control ﬂows
where the problem of interest involves only several conditional cases, the user could enumerate those
cases and use the existing symbolic engines to diﬀerentiate them each. That practice does not apply to
code with loops or many branches. The φ-calculus in this work oﬀers a solution to the complexity.

Several recent studies have challenged the common criticisms of “expression swell” of symbolic dif-
ferentiation [35, 21]. Even though the arguments may diﬀer in form, the main points are similar: If
placeholders are used to store intermediate diﬀerentiation results for reuses, the problem can be largely
alleviated. The design of our reuse-aware SOI identiﬁcation in coarsening is based on a similar insight,
but provides a systematic way to deal with the tradeoﬀ between reuse and the granularity of symbolic
diﬀerentiation.

23

Expression templates have been used in both forward and backward AD implementations to reduce
runtime space and time overhead[10, 26, 8]. In Adept, for instance, during the primal computations, the
algorithm records backward operations onto a stack. Its use of expression templates in C++ helps avoid
invocations of virtual function calls at runtime, and hence reduces the amount of objects needed to allocate
to hold intermediate results. Diﬀerentiation happens however still at each individual operation. There
is no symbolic diﬀerentiation or symbolic simpliﬁcations or optimizations in a large scope. Moreover,
as with all other operator overloading based AD, these solutions also require primal computations to be
executed before gradients can be computed. As a result, the highly optimized implementations are still
2.4-4X slower than the original algorithm (without gradients calculations) [7]. Coarsening optimization,
in comparison, harnesses large-scope optimization opportunities, and can sometimes forego the primal
computations completely, yielding even a higher speed than the original algorithm has as Section 7 has
shown.

Since it was ﬁrst proposed in late 1980s [14], SSA has been widely adopted in program representations
in compilers. Gated SSA (GSA) proposes to explicitly specify the conditions in the φ functions, and
introduces notations to distinguish loop entry, loop exit, and normal φ-functions [24], which share some
similarities with part of the φ-notations in this work. The loop notations in φ-calculus is inspired by the
notations in Glore [16], a work that detects large-scoped loop invariants. In 1990s and early 2000s, there
were a number of papers on recognizing and substituting inductive variables in loops based on SSA through
symbolic analysis [31]. An example is the symbolic analysis based on Chains of Recurrences [32, 33].
The purpose is to convert the array subscripts in a loop into a form ready for parallelization-oriented
dependence analysis. For symbolic diﬀerentiation, what is needed is not only symbolic treatment to
inductive variables but derivations of the closed-form expressions for all the computations that are related
with the active variables, hence the need for the φ-calculus. As a type of standard IR, SSA is also the IR
leveraged in recent AD compilers, such as the Zygote for Julia [18, 19], which is a pure AD tool without
systematic integration of symbolic diﬀerentiation.

Besides symbolic and algorithmic diﬀerentiation, there is another approach called numerical diﬀeren-
tiation, which uses ﬁnite diﬀerence approximations. But because it is inaccurate and scales poorly for
gradients, it is rarely used for machine learning where gradients with respect to millions of parameters
are common.

10 Conclusion

This paper has presented coarsening, a novel optimization that expands the scope of symbolic diﬀeren-
tiation and systematically integrates symbolic diﬀerentiation with AD. It builds on two key innovations:
the φ-calculus and the reuse-aware SOI identiﬁcation. The φ-calculus oﬀers the ﬁrst mechanism that
allows symbolic diﬀerentiation to apply on code with complicated control ﬂow, while the reuse-aware
SOI identiﬁcation provides an algorithm to deal with the tension between computation reuse and coars-
ening. Experiments on several AD tools and various settings demonstrate that coarsening is an eﬀective
optimization for AD. It can remove the overloading overhead in AD and at the same time harness the
beneﬁts of symbolic optimizations and diﬀerentiation, yielding several times to two orders of magnitude
speedups.

References

[1] Calculus package for julia. Available at https://github.com/JuliaMath/Calculus.jl.

[2] Hmc explained. Available at https://arogozhnikov.github.io/2016/12/19/markov_chain_

monte_carlo.html.

[3] Sagemath. Available at https://www.sagemath.org/.

[4] Sympy software. https://www.sympy.org/en/index.html.

24

[5] Fast reverse-mode automatic diﬀerentiation using expression templates in c++. Perspectives in

Computing, 19, 1988. Source of expression swell.

[6] Handbook of markov chain monte carlo. May 2011.

[7] Fast reverse-mode automatic diﬀerentiation using expression templates in c++. Trans. Math. Soft-

ware, 40(26), 2014. ADEPT AD tool in C++.

[8] High-performance derivative computations using codipack. Trans. Math. Software, 45, 2017. CoDi-

Pack.

[9] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Principles, Techniques, and Tools.

Addison Wesley, 2nd edition, August 2006.

[10] P. Aubert, N. D. Cesare, and O. Pironneau. Automatic diﬀerentiation in c++ using expression

templates ´ and application to a ﬂow control problem. Comput. Vis. Sci., 3:197–208, 2001.

[11] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic diﬀerentiation in

machine learning: a survey. The Journal of Machine Learning Research, 18(1), 2018.

[12] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations
of Python+NumPy programs, 2018. https://jax.readthedocs.io/.

[13] B. Considine, M. Famelis, and L. Paull. Kotlin∇: A shape-safe eDSL for diﬀerentiable programming,

2019. https://github.com/breandan/kotlingrad.

[14] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An eﬃcient method
of computing static single assignment form. In Proceedings of the 16th ACM SIGPLAN-SIGACT
symposium on Principles of programming languages, pages 25–35, 1989.

[15] B. Dauvergne and L. Hascoet. The data-ﬂow equations of checkpointing in reverse automatic diﬀer-

entiation. Lecture Notes in Computer Science, 3994, 2006.

[16] Y. Ding and X. Shen. Glore: Generalized loop redundancy elimination upon ler-notation. In Pro-
ceedings of OOPSLA at The ACM SIGPLAN conference on Systems, Programming, Languages and
Applications: Software for Humanity (SPLASH), 2017.

[17] L. C. Dixon. Use of automatic diﬀerentiation for calculating hessians and newton steps. Automatic
Diﬀerentiation of Algorithms: Theory, Implementation, and Application, pages 114–125, 1991.

[18] M. Innes. Don’t unroll adjoint: Diﬀerentiating ssa-form programs. CoRR, abs/1810.07951, 2018.

[19] M. J. Innes. Sense & sensitivities: The path to general-purpose algorithmic diﬀerentiation.

In

Proceedings of the 3rd MLSys Conference, 2020. https://ﬂuxml.ai/Zygote.jl/latest/.

[20] K. B. Knobe and V. Sarkar. Array ssa form and its use in parallelization. In Proceedings of the 25th

ACM SIGPLAN-SIGACT symposium on Principles of programming languages, 1998.

[21] S. Laue. On the equivalence of forward mode automatic diﬀerentiation and symbolic diﬀerentiation.

CoRR, abs/1904.02990, 2019.

[22] D. Maclaurin. Modeling, Inference and Optimization with Composable Diﬀerentiable Procedures.

PhD thesis, Harvard University, 2016.

[23] C. C. Margossian. A review of automatic diﬀerentiation and its eﬃcient implementation. WIREs

Data Mining and Knowledge Discovery, 9(4), Mar 2019.

25

[24] K. J. Ottenstein, R. A. Ballance, and A. B. MacCabe. The program dependence web: a representation
supporting control-, data-, and demand-driven interpretation of imperative languages.
In ACM
SIGPLAN 1990 conference on Programming language design and implementation, pages 257–271,
1990.

[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic diﬀerentiation in pytorch. In Proceedings of NIPS 2017 Workshop Autodiﬀ,
2017.

[26] E. Phipps and R. Pawlowski. Eﬃcient expression templates for operator overloading-basedautomatic
diﬀerentiation. In S. Forth, P. Hovland, E. Phipps, J. Utke, and A. Walther, editors, Recent Advances
in Algorithmic Diﬀerentiation, pages 309–319, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.

[27] J. Rojas, S. Coros, and L. Kavan. Deep reinforcement learning for 2D soft body locomotion. In

NeurIPS Workshop on Machine Learning for Creativity and Design 3.0, 2019.

[28] A. Shaikhha, A. Fitzgibbon, D. Vytiniotis, and S. Peyton Jones. Eﬃcient diﬀerentiable programming

in a functional array-processing language. Proc. ACM Program. Lang., 3(ICFP), July 2019.

[29] B. Sherman, J. Michel, and M. Carbin. Computable semantics for diﬀerentiable programming with
higher-order functions and datatypes. In Proceedings of the ACM SIGPLAN-SIGACT symposium
on Principles of programming languages, 2021.

[30] N. Tehrani, N. S. Arora, Y. L. Li, K. D. Shah, D. Noursi, M. Tingley, N. Torabi, S. Masouleh,
E. Lippert, and E. Meijer. Bean machine: A declarative probabilistic programming language for
eﬃcient programmable inference. In Proceedings of the 10th International Conference on Probabilistic
Graphical Models, PMLR 138, 2020.

[31] P. Tu and D. Padua. Gated ssa-based demand-driven symbolic analysis for parallelizing compilers.

In Proceedings of the 9th International Conference on Supercomputing, pages 414–423, 1995.

[32] R. A. van Engelen. A method for recognizing and substitutions of generalized inductive variables
In Proceedings of the International Conference on Compiler

through chains of recurrences (crs).
Constructions, 2001.

[33] R. A. van Engelen, J. Birch, Y. Shou, B. Walsh, and K. A. Gallivan. A uniﬁed framework for
nonlinear dependence testing and symbolic analysis. In Proceedings of the International Conference
on Supercomputing, 2004.

[34] B. van Merri¨enboer, O. Breuleux, A. Bergeron, and P. Lamblin. Automatic diﬀerentiation in ML:

where we are and where we should be going. CoRR, abs/1810.11530, 2018.

[35] F. Wang, X. Wu, G. M. Essertel, J. M. Decker, and T. Rompf. Demystifying diﬀerentiable program-

ming: Shift/reset the penultimate backpropagator. CoRR, abs/1803.10228, 2018.

[36] Y. Zhu, E. Westbrook, J. Inoue, A. Chapoutot, C. Salama, M. Peralta, T. Martin, W. Taha,
R. Cartwright, A. Ames, and R. Bhattacharya. Mathematical equations as executable models of
mechanical systems. In Proceedings of International Conference on Cyber-Physical Systems, 2010.

26

