1
2
0
2

r
a

M
2
1

]
L
M

.
t
a
t
s
[

1
v
0
2
0
7
0
.
3
0
1
2
:
v
i
X
r
a

Max-Linear Regression by Scalable and Guaranteed Convex
Programming

Seonho Kim
Department of Electrical and Computer Engineering
The Ohio State University
Columbus, OH 43210

Sohail Bahmani
School of Electrical and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30332

Kiryung Lee
Department of Electrical and Computer Engineering
The Ohio State University
Columbus, OH 43210

kim.7604@osu.edu

sohail.bahmani@ece.gatech.edu

lee.8763@osu.edu

Abstract

We consider the multivariate max-linear regression problem where the model parameters
Rp need to be estimated from n independent samples of the (noisy) observa-
β1, . . . , βk
∈
tions y = max1≤j≤k βT
j x + noise. The max-linear model vastly generalizes the conventional
linear model, and it can approximate any convex function to an arbitrary accuracy when
the number of linear models k is large enough. However, the inherent nonlinearity of the
max-linear model renders the estimation of the regression parameters computationally chal-
lenging. Particularly, no estimator based on convex programming is known in the literature.
We formulate and analyze a scalable convex program as the estimator for the max-linear
regression problem. Under the standard Gaussian observation setting, we present a non-
asymptotic performance guarantee showing that the convex program recovers the param-
eters with high probability. When the k linear components are equally likely to achieve
the maximum, our result shows that a suﬃcient number of observations scales as k2p up
to a logarithmic factor. This signiﬁcantly improves on the analogous prior result based
on alternating minimization (Ghosh et al., 2019). Finally, through a set of Monte Carlo
simulations, we illustrate that our theoretical result is consistent with empirical behavior,
and the convex estimator for max-linear regression is as competitive as the alternating
minimization algorithm in practice.
Keywords: nonlinear regression, convex programming, max-linear model, empirical pro-
cesses, and sample complexity

1. Introduction

We consider the problem of estimating the parameters β⋆,1, . . . , β⋆,k ∈
the max-linear function

Rp that determine

Rp

x

∈

max
j∈[k] h

β⋆,j, x

,

i

7→

(1.1)

1

 
 
 
 
 
 
from independent and identically distributed (i.i.d.) observations, where [k] denotes the set
Rp, and denoting the value of a
. Speciﬁcally, given the data points x1, . . . , xn ∈
1, . . . , k
{
}
max-linear function, with parameter β, at these points by

fi(β)

we observe the nonlinear measurement

def
= max

j∈[k] h

xi, βji

,

(1.2)

yi = fi(β⋆) + wi ,

of the parameter vector β⋆ = [β⋆,1; . . . ; β⋆,k]

Rkp where wi denotes noise for i

[n].

∈

∈
The closest result in the literature to ours is that of Ghosh et al. (2019), which studies
the slightly more general problem of max-aﬃne regression and considers an alternating
minimization approach as the estimator. Each iteration of this alternating minimization
approach basically consists in a step for identifying the maximizing linear terms, and a
following step of solving least squares to estimate the corresponding parameters. However,
the theoretical and empirical performance of (Ghosh et al., 2019) critically depends on the
level of the observation noise, mainly due to the sensitivity of the “maximizer identiﬁcation”
step in each iteration of the algorithm. Leveraging recent theory for convexifying nonlinear
inverse problems in the original domain (Bahmani and Romberg, 2017; Bahmani, 2019;
Bahmani and Romberg, 2019), we propose an alternative approach by convex programming.
Due to the inherent geometry of the formulation, the convex estimator provides stable
performance in the presence of noise.
It is worth mentioning that (Ghosh et al., 2019)
considered a random noise model and analyzed the accuracy in terms of the noise variance,
whereas we consider the deterministic “gross error” model. Nevertheless, in the noiseless
case, both results essentially achieve the same sample complexity.

1.1 Convex estimator

The common estimators for β⋆ such as the least absolute deviation (LAD), i.e.,

minimize
β

fi(β)

|

yi|

−

n

1
n

,

(1.3)

Xi=1
are generally hard to compute as they involve a nonconvex optimization. Given an “anchor
vector” θ, we study the estimation of β⋆ through anchored regression that formulates the
estimation by the convex program

maximize
β

subject to

i

θ, β
h
1
n

n

Xi=1

(fi(β)

yi)+ ≤

−

η ,

(1.4)

)+ denotes the positive part function and η = 1
wi)+. The anchored regres-
where (
n
·
sion can be interpreted as a convexiﬁcation of the LAD estimator. Since the observation
functions (1.2) are convex, the LAD is nonconvex mainly due to the eﬀect of the absolute
value operator in (1.3). This source of nonconvexity is removed in anchored regression by

P

−

n
i=1(

2

relaxing the absolute deviation to the positive part of the error. The linear objective that
is determined by the anchor vector θ acts as a “regularizer” to prevent degenerate solutions
and guarantees exact recovery of the true parameter β⋆ under certain conditions on the
measurement model in the noiseless scenario.

Anchored regression has been originally developed as a scalable convex program to solve
the phase retrieval problem (Bahmani and Romberg, 2017; Goldstein and Studer, 2018)
with provable guarantees. Anchored regression is highly scalable compared to other convex
relaxations in this context (Candes et al., 2013; Waldspurger et al., 2015) that rely on
semideﬁnite programming. The idea of anchored regression is further studied in a broader
class of nonlinear parametric regression problems with convex observations (Bahmani and
Romberg, 2019) and diﬀerence of convex functions (Bahmani, 2019).

1.2 Background and motivation

A closely related problem is the max-aﬃne regression problem. A max-aﬃne model gen-
eralizes the max-linear model in (1.1) by introducing an extra oﬀset parameter to each
component. Alternatively, by ﬁxing any regressor to constant 1, each linear component in
(1.1) has its range away from the origin, which turns the model into a max-aﬃne model.
Thus methods developed for the two models are compatible. For the sake of simplicity, we
use the description in (1.1). If necessary, a coordinate of xi can be ﬁxed to 1 for all i
[n].
Since max-aﬃne model can approximate a convex function to an arbitrary accuracy, it
has been utilized in numerous applications, particularly in machine learning and optimiza-
tion. Recently it has been shown that an extension called max-aﬃne spline operators (MA-
SOs) can represent a large class of deep neural networks (DNNs) (Balestriero and Baraniuk,
2018; Balestriero et al., 2019, 2020). They leveraged the model to analyze the expressive
power of various DNNs. Max-aﬃne model has also been leveraged to approximate Brege-
man divergences in metric learning (Siahkamari et al., 2019) and utility functions in energy
storage and beer brewery optimization problems (Bal´azs, 2016).

∈

As mentioned above, the estimation for max-aﬃne or max-linear regression is challenging
due to the inherent nonlinearity in the model. In the literature, the max-aﬃne regression
problem has been studied mostly as a nonlinear least squares (Magnani and Boyd, 2009;
Toriello and Vielma, 2012; Hannah and Dunson, 2013; Bal´azs, 2016; Ghosh et al., 2019; Ho
et al., 2019):

β = argmin
[β1;...;βk]

n

Xi=1 (cid:18)

max
1≤j≤kh

xi, βji −

yi

2

.

(cid:19)

(1.5)

b

By utilizing a special structure in (1.5), a suite of alternating minimization algorithms have
been developed (Magnani and Boyd, 2009; Toriello and Vielma, 2012; Hannah and Dunson,
2013). The fact that (1.1) is a special case of piecewise linear function allows us to divide
x1, . . . , xn into k partitions based on their membership in the polyhedral cones
w, β⋆,j −
h
that are pairwise almost disjoint 1 and cover the entire space Rp.
Cj is
determined according to which component achieves the maximum in the max-linear model

∈
In other words,

β⋆,li ≥

Rp :

(1.6)

[k] ,

= j

def
=

Cj

w

0,

∈

∀

{

}

j

l

,

1

We say two sets are almost disjoint whenever their intersection has measure zero with respect to an

underlying measure.

3

6
in (1.1). If this oracle information is known a priori, then the estimation is divided into k
decoupled linear least squares given by

βj = argmin

βj

(

xi, βji −
h

yi)2 ,

[k] .

j

∈

Xi∈Cj

b

∈

∈

β⋆,j}
{

Magnani and Boyd (2009) proposed the least-square partition algorithm (LSPA), which
k
k
j=1, which is a reminiscent of the
j=1 and
progressively reﬁne estimates for both
{Cj}
k-means clustering algorithm. Hannah and Dunson (2013) proposed the convex adaptive
partitioning (CAP) method by applying the adaptive partitioning technique that splits and
reﬁnes the partitions. The CAP method exploits the adaptive partitioning technique which
[p] and selecting the only partition which
divides partitions with respect to coordinate u
minimizes the mean square error on the observations (xi, yi), i
[n]. By selecting this
adaptive partition method which is improved version of naive partitioning method used
in LSPA, they showed improved performance in empirical result. Later, Bal´azs (2016)
proposed the adaptive max-aﬃne partitioning algorithm (AMAP) that exploits both LSPA
and the CAP method with a cross-validation scheme. By applying the adaptive partition
method in CAP but replacing the partition criterion by partitioning at median, they reduced
computation time for a large data set. The performance of these alternating minimization
algorithms critically depends on the initialization. In fact, designing a good initialization
scheme is in turn an independent challenging problem. Bal´azs (2016) proposed a random
search scheme to compute an initial estimate of the parameters, but its search space grows
exponentially in p. Later, Ghosh et al. (2019) reduced its computational cost through
dimensionality reduction using a spectral method when p
k. Nevertheless the size of the
search space is still exponentially dependent on k. There also exist other approaches to solve
the nonlinear least squares in (1.5) along general strategies to tackle nonconvex optimization
problems. Toriello and Vielma (2012) formulated it into a mixed integer program via the
“big-M” method. Likewise other mixed integer programs, their solution suﬀers from high
computational cost and does scale well to a large instance. Ho et al. (2019) formulated (1.5)
into a diﬀerence of convex (DC) program and applied the DC algorithm (DCA) (Tao and
An, 1998). They showed its fast convergence to a local minimum.

≥

Aforementioned heuristic methods demonstrated satisfactory empirical performance on
a few benchmark sets at tractable computational cost. However, these methods lack any
statistical guarantees even under reasonable simplifying assumptions. The only exception
is the LSPA algorithm, for which a rigorous analysis is recently provided by Ghosh et al.
(2019). Under the standard Gaussian model for the data, Ghosh et al. (2019) provided
a sample complexity for exact recovery of the parameter recovery starting from a good
initialization. However, they observed a nontrivial gap between their sample complexity
result and empirical behavior of the LSPA algorithm. Speciﬁcally, their suﬃcient condition
for exact parameter recovery implies n & kpπ−3
∈ Cj}
Normal(0, Ip). Their numerical results, however, illustrate that the empirical phase
and g
transition occurs at n

min log(n/p), where πmin = minj∈[k]

min (see (Ghosh et al., 2019, Fig. 4)).

kpπ−1

g
{

∼

P

≈

1.3 Contributions

We provide a scalable convex estimator for the max-linear regression problem that is for-
mulated as a linear program and is backed by statistical guarantees. Under the standard

4

Gaussian covariate model, the convex estimator (1.4) is guaranteed to recover the regression
parameters exactly with high probability if the number of observations n scales as π−4
minp
up to some logarithmic factors. This sample complexity implicitly depends on k (i.e., the
number of components) through πmin. Particularly, when the k linear components form a
“well-balanced partition” in the sense that they are equally likely to achieve the maximum,
the smallest probability πmin is close to 1/k and the derived sample complexity reduces to
k4p up to the logarithmic factors.

2. Accuracy of the Convex Estimator

In this section, we provide our main results on the estimation error of the convex estimator
in (1.4). We consider an anchor vector θ constructed as

fi(

β)

∇

(2.1)

θ =

1
2n

n

Xi=1

e

Rkp.

from a given initial estimate

β = [

β1; . . . ;

βk]

∈
The following theorem illustrates the sample complexity and the corresponding estima-
tion error achieved by the estimator in (1.4). The estimation error is measured as the sum
of the ℓ2 norms of the diﬀerence between the corresponding components of the ground truth
β⋆ and the estimate

β.

e

e

e

k
j=1 be polyhedral cones deﬁned similar to
Theorem 1 Let
b
Cj}
{
replacing β⋆. With θ given by (2.1) and
xi}
{
e
xi}
is independent of
{

k
j=1 in (1.6), with
{Cj}
n
i=1 being i.i.d. Normal(0, Ip), suppose that

n
i=1. Then there exists a numerical constant c > 0 such that if

β
β
e
e

and

ζ

def
= min

j∈[k] r

π
32

P2

g
{

∈ Cj} −

2 max
j∈[k]

P

g
{

q

Cj△Cj}

∈

> 0

e

c ζ −2

4p log3 p log5 k + 4 log(δ−1) log k

,

n

≥

then the solution

β to the optimization problem in (1.4) obeys

(cid:0)

(cid:1)

b

with probability 1

δ.

−

3. Numerical results

k

Xj=1

β⋆,j −
k

βjk2 ≤
b

2
ζn

n

Xi=1

wi|
|

(2.2)

(2.3)

We provide numerical results through Monte Carlo simulations for max-linear regression
over generic data. The experiments were intended to illustrate two objectives: i) The lower
bound on suﬃcient number of observations by Theorem 1 is consistent with empirical be-
havior of the convex estimator, and ii) The convex estimator provides competitive empirical
performance to LSPA (Ghosh et al., 2019).

5

In the experiments, the convex estimator is implemented through the following equiva-

lent linear program

maximize
j=1,(ti)n

(βj )k

i=1 h

θ, [β1; . . . ; βk]
i

subject to ti ≥

0,

xi, βji −
h

yi ≤

ti,

1
n

n

Xi=1

η,

ti ≤

[n],

i

∀

∈

j

∀

∈

[k] ,

(3.1)

n
i=1(

−

P

where η = 1
wi)+ from (1.4). Since (3.1) is a standard linear program, it is highly
n
scalable and can be solved by oﬀ-the-shelf-software packages such as CPLEX and Gurobi
(Gurobi Optimization, 2021). For an extensive Monte Carlo simulation, we set the size
of the regression problem to a moderate number so that the optimization can be solved
eﬃciently using the Gurobi solver in MATLAB. The convex estimator is compared to the
version of LSPA by Ghosh et al. (2019), which terminates the iterations once the maximum
number of 200 is reached.

To be compatible with the assumptions of Theorem 1, the regressors x1, . . . , xn are gen-
erated as i.i.d Normal(0, Ip). The ground truth regression parameter vector is normalized
β⋆k2 = 1. The entries of β⋆ are chosen so that they correspond to a well-balanced
so that
k
case. Speciﬁcally, in the ﬁrst set of simulations (Figures 1 to 4), β⋆,1, . . . , β⋆,k are set to
In the second set (Figure 5), β⋆,1, . . . , β⋆,k are
be p-dimensional standard basis vectors.
generated i.i.d. Normal(0, Ikp). In all of the experiments, the given initial estimate
β is a
Normal(0, 1/(1000kp)Ikp).
slight perturbation of β⋆ in the sense that
β is a slightly perturbed version of the ground truth β⋆, we expect
Since the initial estimate
e
the assumption on ζ in Theorem 1 to hold. We measure the normalized estimation error as
β⋆,jk2 over 50 trials.
the median of normalized estimation errors

β = β⋆ + ǫ, where ǫ

β⋆,jk2/

k
j=1 k

k
j=1 k

∼

e

e

P

0

-5

-10

-15

1000

500

0

10 20 30 40 50

(a) CE

βj −
b

1000

P

500

0

10 20 30 40 50

0

-5

-10

-15

(b) LSPA
et al., 2019)

(Ghosh

Figure 1: Phase transition of estimation error in the noiseless case for k = 10. The log base
10 of the estimation error is plotted. Above the green line denotes the regime for
exact recovery with error below 10−5.

Figures 1 and 2 illustrate the normalized estimation error in the noiseless scenario, as a
function of the sample size, n, as well as the dimension and the number of segments of the
parameter, p and k. Green lines in these ﬁgures represent where the phase transition occurs

6

0

-5

-10

-15

2500

1600

800

100

2

10

20

(a) CE

2500

1600

800

100

2

10

20

0

-5

-10

-15

(b) LSPA (Ghosh et al.,
2019)

Figure 2: Phase transition of estimation error in the noiseless case for p = 50. The log base
10 of the estimation error is plotted. Above the green line denotes the regime for
exact recovery with error below 10−5.

0

-0.2

-0.4

-0.6

-0.8

-1

1000

500

0

10 20 30 40 50

(a) CE

1000

500

0

10 20 30 40 50

0

-0.2

-0.4

-0.6

-0.8

-1

(b) LSPA (Ghosh et al.,
2019)

Figure 3: Phase transition of estimation error in the noise case when σ = 0.1 and k = 10.

The log base 10 of the estimation error is plotted.

in the sense that the median of normalized estimation errors falls below 10−5. In Figure 1,
the number of segments, k, is ﬁxed to 10. Figure 1 suggests that the successful recovery
occurs if n grows linearly with p, for both CE and LSPA in a similar way. A complementary
view is provided by Figure 2, corresponding to a varying number of segment k, and a ﬁxed
dimension of p = 50. As Ghosh et al. (2019) pointed out, their suﬃcient condition for
exact recovery is conservative compared to the observed empirical performance of LSPA.
To be speciﬁc, the sample complexity of LSPA in their numerical results depends on k
instead of k4. We observed a similar gap between the theoretical prediction in Theorem 1
and the empirical performance. That is, the phase transition of CE in 2 occurs when n is
proportional to k instead of k2. In fact, as Figures 1 and 2 suggest, the sample complexity
of successful recovery using LSPA is only a constant factor better than CE in the noiseless
scenario.

7

0

-0.2

-0.4

-0.6

-0.8

-1

2500

1600

800

100

2

10

20

(a) CE

2500

1600

800

100

2

10

20

0

-0.2

-0.4

-0.6

-0.8

-1

(b) LSPA (Ghosh et al.,
2019)

Figure 4: Phase transition of estimation error in the noise case when σ = 0.1 and p = 50.

The log base 10 of the estimation error is plotted.

Figures 3 and 4 are the analogs of Figures 1 and 2 for noisy measurements. The noise
standard deviation σ is set to 0.1. Figures 3 and 4 suggest that in the noisy scenario, as the
number of observations required by the LSPA for accurate recovery, compared to CE, grows
more rapidly as k or p increase. This behavior is in contrast with comparable performance of
the two algorithms in the noiseless setting. Figures 3(b) and 4(b) contain peculiar triangular
regions adjacent to the horizontal axis where a nontrivial normalized error is achieved. We
believe that these regions are the artifact of the LSPA algorithm being stuck around the
initialization, which is close to the ground truth parameter, for small n.

Figure 5 shows log-scale plot of the normalized estimation error for both the CE and
LSPA with respect to the number of observations n at ﬁxed k = 5 and p = 10. Note
that while LSPA has an asymptotically vanishing error (i.e., it is consistent) (Ghosh et al.,
2019), CE appears to be a biased estimator whose bias depends on the noise strength in
(2.3). Despite this drawback, Figure 5 shows that CE outperforms LSPA for a moderate
number of observations. This implies that CE is more practical estimator in that we want
to estimate parameters with the limited number of samples in practice. Furthermore, we
can observe that the required oversampling factor of LSPA is more sensitive to σ, compared
to CE, in the sense that LSPA requires more observations than CE for accurate estimation
as σ increases.

4. Proof of Theorem 1

We prove Theorem 1 in two steps. First, in the following proposition, we present a suﬃcient
condition for robust estimation by convex program in (1.4). Then we derive an upper
bound on ̺ in the proposition, which provides the sample complexity condition along with
the corresponding error bound in Theorem 1.

8

100

10-1

100

10-1

σ = 0.05

σ = 0.1

σ = 0.15

100

10-1

100

10-1

0

500

1000

1500

2000

0

500

1000

1500

2000

0

500

1000

1500

2000

σ = 0.2

σ = 0.25

σ = 0.3

100

10-1

100

10-1

0

500

1000

1500

2000

0

500

1000

1500

2000

0

500

1000

1500

2000

Figure 5: Log-scale of

β⋆k1,2 versus the number of observations n for CE
(solid line) and LSPA (dotted line) according to σ when k = 5 and p = 10. The
estimation error is calculated as the median of 50 trials.

k1,2/
k
b

β⋆ −
k

β

Proposition 1 Under the hypothesis of Theorem 1, suppose that

β satisﬁes

̺

def
= inf
j∈[k]
w∈Sp−1

E 1Cj (g)

g, w

|h

i| −

sup
j∈[k]
w∈Sp−1

E1 e

Cj \Cj

g, w
(g)
h

i+ −

sup
j∈[k]
w∈Sp−1

Then there exists a numerical constant c > 0 such that if

e
E1

Cj\

e
Cj

c̺−2

4p log3 p log5 k + 4 log(δ−1) log k

,

n

≥

(cid:0)

(cid:1)

then the solution

β to the optimization problem in (1.4) obeys

b

k

Xj=1

β⋆,j −
k

βjk2 ≤
b

2
̺n

n

Xi=1

wi|
|

with probability 1

δ.

−

9

g, w
(g)
h

i+ > 0 .

(4.1)

(4.2)

(4.3)

Proof We ﬁrst show that, for a suﬃciently large ρ > 0, the following three conditions
cannot hold simultaneously:

yi)+ ≤

−

η ,

n

(fi(β⋆ + z)

1
n

Xi=1
z
k1,2 > ρ ,
k
θ, z
0 .
i ≥
h

Therefore, assuming (4.5) and (4.6) hold, it suﬃces to show

n

(z)

def
=

L

1
n

Xi=1
To this end, we derive a lower bound on

(z)

L

≥

1
n

(a)

≥

1
n

n

Xi=1
n

(fi(β⋆ + z)

−

fi(β⋆))+ −

1
n

fi(β⋆), z

(

h∇

)+ −

i

1
n

n

Xi=1
(wi)+

(fi(β⋆ + z)

yi)+ > η .

−

(z) as follows:

L
n

(wi)+

Xi=1
n

Xi=1
n

1
n

1
n

Xi=1
θ, z
h

i −

θ, z
h

i −

=

=

(b)
=

(c)
=

|h∇

fi(β⋆), z
2

i|

+

|h∇

fi(β⋆), z
2

i|

+

1
n

1
n

(wi)+ +

n

Xi=1
n

(wi)+ +

Xi=1

1
n

1
n

1
n

1
n

Xi=1
n
h∇

Xi=1
n

Xi=1
n

|h∇

Xi=1
n

k

Xi=1

Xj=1

fi(β⋆), z
2

i

−

1
n

n

(wi)+

h∇

fi(β⋆), z
2

i

− h

Xi=1
θ, z

fi(β⋆), z
2

i|

+

1
n

Xi=1

1Cj (xi)
xi, zj i|
|h
2

+

1
n

+

θ, z
h

i −

i

n

fi(β⋆)

h∇

n

k

Xi=1

Xj=1

(4.4)

(4.5)

(4.6)

(4.7)

i

n

(wi)+

1
n

fi(

Xi=1
− ∇
2
e
1Cj (xi)
{

β), z

−

1 e
Cj
2

(xi)

xi, zji

}h

,

(4.8)

where (a) holds by the convexity of fi, which implies

fi(β⋆ + z)

fi(β⋆) +

fi(β⋆), z

,

i

h∇

≥

β. We
(b) follows from (2.1), and (c) is obtained by calculating
further proceed by obtaining lower bounds on the last two terms in (4.8) by the following
lemmas, which are proved in Appendices A.1 and A.2.

fi(β) at β = β⋆ and β =

∇

e

Lemma 1 Let (Vz)z∈Rkp be a random process deﬁned by

Vz

def
=

1
n

n

k

Xi=1

Xj=1

1Cj (xi)

xi, zj i|
|h

,

10

where x1, . . . , xn are i.i.d. Normal(0, Ip). Then, for g
there exists a numerical constant c1 > 0 such that

∼

Normal(0, Ip) and any δ

(0, 1),

∈

V

def
= inf

kzk1,2=1

Vz

min
j∈[k],w∈Sp−1

≥

E 1Cj (g)

g, w

|h

i| −

c1

p log3 p log5 k + log(δ−1) log k
n

(cid:18)

1/2

(cid:19)

holds with probability at least 1

δ/2.

−

Lemma 2 Let (Qz)z∈B1,2 be a random process deﬁned by

Qz

def
=

1
n

n

k

1 e
Cj

(xi)

Xi=1

Xj=1 n

−

1Cj (xi)
o

xi, zj i
h

,

where x1, . . . , xn are i.i.d. Normal(0, Ip). Then, for g
there exists a numerical constant c2 > 0 such that

∼

Normal(0, Ip) and any δ

(0, 1),

∈

Q

def
= sup

kzk1,2=1

Qz

max
j∈[k],w∈Sp−1

≤

E1 e

Cj \Cj

g, w
(g)
h

i+ +

max
j∈[k],w∈Sp−1

p log3 p log5 k + log(δ−1) log k
n

+ c2

(cid:18)

1/2

(cid:19)

holds with probability at least 1

δ/2.

−

E1

Cj\

e
Cj

g, w
(g)
h

i+

Since Vz are Qz are homogeneous in z, we obtain that the third term in the right-hand

side of (4.8) is written as Vz and lower-bounded by

Vz
2 ≥

V

z
k

k1,2
2

.

(4.9)

Similarly, the last term in the right-hand side of (4.8) is written as
by

−

Qz and lower-bounded

Qz
2 ≥ −

Q
k

z

k1,2
2

.

−

Furthermore, by Lemmas 1 and 2, the condition in (4.2) implies that

holds with probability 1

−

δ. Then we choose ρ so that it satisﬁes

V

Q

−

≥

̺ > 0

(4.10)

(4.11)

ρ =

V

1
n

Q ·

2

−

n

Xi=1

.

wi|
|

11

Next, by plugging in the above estimates to (4.8), we obtain that, under the event in (4.11),
the conditions in (4.5) and (4.6) imply

(z)

L

θ, z

≥ h

i −

1
n

n

(wi)+ +

(V

−

z
Q)
k

2

k1,2

n

Xi=1
(wi)+ +

(V

Xi=1
n

(wi)+ +

Q)ρ

−
2

n

wi|
|

Xi=1

1
n

wi)+ = η .

>

1
n

−

1
n

=

=

−

1
n

Xi=1
n
(
−
Xi=1

Let

z =

β⋆. Recall that both

This lower bound implies (4.7). Therefore we have shown that the three conditions in (4.4),
(4.5), and (4.6) cannot hold simultaneously. It remains to apply the claim to a special case.
β and β⋆ are feasible for the optimization problem
β
β
, which implies
in (1.4). Moreover, since
z.
θ,
h
Since the three conditions cannot be satisﬁed simultaneously, the condition in (4.5) cannot
hold, i.e.
b
b

0. Therefore the conditions in (4.4) and (4.6) are satisﬁed with z substituted by
b

β is the maximizer, it follows that

z satisﬁes

θ, β⋆i

θ,
h

i ≥ h

i ≥

−

z

b

b

b

b

b

z
k

k1,2 ≤

ρ

≤

2
̺n

Thus we obtain the desired assertion.

b

n

.

wi|
|

Xi=1

Next we use the following lemma to obtain a lower bound on ̺ in (4.1).

Lemma 3 Let
have

A ⊂

Rp be of ﬁnite Gaussian measure and g

Normal(0, Ip). Then we

∼

inf
w∈Sp−1

E 1A(g)

g, w

|h

i| ≥

π
32

P2

g
{

∈ A}

r

and

sup
w∈Sp−1

E1A(g)
g, w
h

i+ ≤

P

g
{

.

∈ A}

Proof For an arbitrarily ﬁxed ǫ > 0, let Sǫ ⊂
Rp :

Sǫ :=

x
{

∈

p

Rp denote the set deﬁned by

x, w

|h

< ǫ

.

}

i|

Then we have

E1C(g)
|h

g, w

i| ≥

ǫ E1C(g)1S c

ǫ (g)

= ǫ E (1C(g)
ǫ E (1C(g)
g
{

≥
= ǫ (P

−

−

1C(g)1Sǫ(g))
1Sǫ(g))
P
g
{

∈ Sǫ}

∈ C} −

) .

12

(4.12)

Moreover, since

g, w
h

i ∼

Normal(0, 1), P

g
{

Sǫ}

∈

is upper-bounded by

P

g
{

Sǫ}

∈

= P

g, w

{|h

< ǫ

=

}

i|

ǫ

−ǫ

Z

1
√2π

e−u2/2du

2
π

.

δ

≤

r

By plugging in (4.13) to (4.12), we obtain

E1C(g)
|h

g, w

i| ≥

ǫ

P

g
{

∈ C} −

2
π !

.

ǫ

r

(4.13)

(4.14)

Since the parameter ǫ > 0 was arbitrary, one can we maximize the right-hand side of (4.14)
with respect to ǫ to obtain the tightest lower bound. Note that the objective is a concave
quadratic function and the maximum is attained at ǫ =
. This provides the
lower bound in the ﬁrst assertion. Next, by the Cauchy-Schwarz inequality, we obtain the
upper bound in the second assertion as follows:

π/8 P

∈ C}

p

g

{

E1A(g)
g, w
h

i+ ≤

E (1A(g))2

q

q

E

g, w
h

2
+ =
i

E1A(g)

p

E

g, w
h
2

2
i

=

P

g
{

r

r

∈ A}
2

.

Finally, by applying Lemma 3 to each of the expectation terms in ̺, we obtain a lower

bound on ̺ given by

̺

≥

min
j∈[k] r

π
32

P2

g

{

∈ Cj} −

max
j∈[k] r

≥

min
j∈[k] r

π
32

P2

g

{

∈ Cj} −

2 max

j∈[k] r

P

g

n
P

∈ Cj \

Cj

g

e
Cj
∈ Cj△

max
j∈[k]

−

P

g
{

q

Cj \ Cj}

∈

,

e

(4.15)

o

n
where the second inequality holds since
[k]. This
Cj△Cj = (
Cj) for all j
implies that (2.2) is a suﬃcient condition for (4.2). Moreover, substituting ̺ in (4.3) by the
e
e
lower bound in (4.15) provides (2.3). This completes the proof of Theorem 1.

o
e
Cj \ Cj)
e

(
Cj \

∈

∪

5. Discussion on Tightness of Theorem 1

The sample complexity in Theorem 1 is tight given an accurate initial estimate. This can
be deduced from the following example, where the lower bound on ̺ by Lemma 3 is tight
in terms of its dependence on P

for j

[k].

g

Example 1 Let p = 2. Then

denote the angular width of
that

∈

{

∈ Cj}
Cj \Cj and
Cj,
Cj \
e
P
min
j∈[k]

Cj \
Cj, and
e
∈ Cj} ≥

g

Cj are Lorentz cones. Let θCj , θCj\
Cj \ Cj respectively. Furthermore, we assume
e
e
(5.1)
max
j∈[k]

and θ e

Cj \Cj

e
Cj

P

g

.

Cj
∈ Cj△
In this case, the parameter ̺ in Proposition 1 is expressed as
e

n

{

o

̺ =

√2Γ(3/2)
Γ(1)

2
π

min
j∈[k]

"

sin2

θCj
4

(cid:18)

(cid:19)

max
j∈[k]

−

1
π

sin

θ e
Cj\Cj
2 ! −

max
j∈[k]

1
π

sin

θCj\

e
Cj
2 !#

.

(5.2)

13

 
 
 
When θC is small enough, sin(θC)
there exists numerical constants c1 > 0 and c2 > 0 such that

≈

θC holds by the Taylor series approximation. Hence,

̺ = c1 min
j∈[k]

P2

g
{

∈ Cj} −

c2 max
j∈[k]

P

g
{

Cj△Cj}

∈

.

This example shows that ζ in Theorem 1 is tight in the sense that the dominating term in
both ̺ and ζ is proportional to the squared probability measure of the smallest

e

Cj.

Let θCj denote the angular width of

Cj. Without loss of generality, we may assume that
π. Furthermore, the assumption in (5.1) implies that the angular width of
minj∈[k] θCj ≤
[k]. Therefore, the identity in (5.2) is obtained by applying
Cj△
the following lemma, proved in Appendix A.3, to the inﬁmum/supremum of expectation
terms in (4.1).

Cj is at most π for all j
e

∈

Lemma 4 Let
C
angular width of
C

and

6. Discussion

be a polyhedral cone in R2 and g
, denoted by θC satisﬁes 0
θC ≤
2√2Γ(3/2)
πΓ(2)

E 1C(g)

inf
w∈S1

g, w

≤

=

∼

|h

i|

Normal(0, I2). Suppose that the

π. Then we have

sin2

θC
4

(cid:18)

(cid:19)

.

sup
w∈S1

E1C(g)
g, w
h

i+ =

√2Γ(3/2)
πΓ(2)

sin

θC
2

(cid:18)

(cid:19)

As discussed in Section 3, the proposed convex estimator has weakness in the sense that it
does not provide a consistent estimator in the asymptotic in the number of observations.
To relax this weakness, one might consider an alternative optimization that has the data
ﬁdelity term as a penalty instead of a constraint. The analysis of this alternative convex
estimator will be investigated in our future work. Also, we will investigate the eﬀect of
nontrivial oﬀset terms in the max-aﬃne case. We conjecture that it might be necessary to
introduce a diﬀerent stochastic model on regressors than the standard Gaussian model to
avoid degenerate partition by the max-aﬃne model.

Acknowledgements

S.K. and K.L were supported in part by NSF CCF-1718771 and an NSF CAREER award
CCF-1943201. S.B. was supported in part by Semiconductor Research Corporation (SRC)
and DARPA.

References

Sohail Bahmani. Estimation from nonlinear observations via convex programming with
application to bilinear regression. Electronic Journal of Statistics, 13(1):1978–2011, 2019.

Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A
ﬂexible convex relaxation. In Artiﬁcial Intelligence and Statistics, pages 252–260, 2017.

14

Sohail Bahmani and Justin Romberg. Solving equations of random convex functions via
anchored regression. Foundations of Computational Mathematics, 19(4):813–841, 2019.

G´abor Bal´azs. Convex Regression: Theory, Practice, and Applications. PhD thesis, Univer-

sity of Alberta, 2016.

Randall Balestriero and Richard Baraniuk. Mad max: Aﬃne spline insights into deep

learning. arXiv preprint arXiv:1805.06576, 2018.

Randall Balestriero, Romain Cosentino, Behnaam Aazhang, and Richard Baraniuk. The ge-
ometry of deep networks: Power diagram subdivision. In Advances in Neural Information
Processing Systems, pages 15806–15815, 2019.

Randall Balestriero, Sebastien Paris, and Richard Baraniuk. Max-aﬃne spline insights into

deep generative networks. arXiv preprint arXiv:2002.11912, 2020.

Emmanuel J Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and
stable signal recovery from magnitude measurements via convex programming. Commu-
nications on Pure and Applied Mathematics, 66(8):1241–1274, 2013.

Bernd Carl. Inequalities of Bernstein-Jackson-type and the degree of compactness of op-
In Annales de l’institut Fourier, volume 35, pages 79–118,

erators in Banach spaces.
1985.

Richard M Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian

processes. Journal of Functional Analysis, 1(3):290–330, 1967.

Avishek Ghosh, Ashwin Pananjady, Adityanand Guntuboyina, and Kannan Ramchandran.
Max-aﬃne regression: Provable, tractable, and near-optimal statistical estimation. arXiv
preprint arXiv:1906.09255, 2019.

Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit.

IEEE Transactions on Information Theory, 64(4):2675–2689, 2018.

LLC Gurobi Optimization.
http://www.gurobi.com.

Gurobi optimizer

reference manual,

2021.

URL

Lauren A Hannah and David B Dunson. Multivariate convex regression with adaptive

partitioning. The Journal of Machine Learning Research, 14(1):3261–3294, 2013.

Vinh Thanh Ho, Hoai An Le Thi, and Tao Pham Dinh. DCA with successive DC decom-
In International Conference on Computer

position for convex piecewise-linear ﬁtting.
Science, Applied Mathematics and Applications, pages 39–51. Springer, 2019.

Marius Junge and Kiryung Lee. Generalized notions of sparsity and restricted isometry
property. Part I: A uniﬁed framework. Information and Inference: A Journal of the IMA,
9(1):157–193, 2020.

Alessandro Magnani and Stephen P Boyd. Convex piecewise-linear ﬁtting. Optimization

and Engineering, 10(1):1–17, 2009.

15

Ali Siahkamari, Venkatesh Saligrama, David Castanon, and Brian Kulis. Learning Bregman

divergences. arXiv preprint arXiv:1905.11545, 2019.

Pham Dinh Tao and Le Thi Hoai An. A DC optimization algorithm for solving the trust-

region subproblem. SIAM Journal on Optimization, 8(2):476–505, 1998.

Alejandro Toriello and Juan Pablo Vielma. Fitting piecewise linear continuous functions.

European Journal of Operational Research, 219(1):86–95, 2012.

Aad W van der Vaart and Jon A Wellner. Weak convergence and empirical processes.

Springer Series in Statistics. Springer, 1996.

Roman Vershynin. High-dimensional probability: An introduction with applications in data

science, volume 47. Cambridge university press, 2018.

Ir`ene Waldspurger, Alexandre d’Aspremont, and St´ephane Mallat. Phase recovery, max-
cut and complex semideﬁnite programming. Mathematical Programming, 149(1-2):47–81,
2015.

Appendix A. Proof of Supporting Lemmas

A.1 Proof of Lemma 1

For any z satisfying

z

k

k1,2 = 1, we have

Vz

min
kzk1,2=1

≥

EVz

sup
z∈B1,2 |

Vz

−

−

EVz

.

|

(A.1)

In what follows, we derive lower estimates of the summands in the right-hand side of (A.1).

First, we derive a lower bound on minkzk1,2=1

we have

EVz. Since x1, . . . , xn are i.i.d. Normal(0, Ip),

EVz = E 1
n

n

k

Xi=1

Xj=1

1Cj (xi)

xi, zji|
|h

= E

k

Xj=1

1Cj (g)

g, zji|
|h

=

where z = [z1; . . . ; zk]. Then EVz is lower-bounded by

k

Xj=1

zjk2
k

E 1Cj (g)

g,

(cid:28)

k

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

zj
zjk2 (cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

EVz

z

k1,2

inf
j∈[k],w∈Sp−1

≥ k

E 1Cj (g)

g, w

.

i|

|h

Next, we show that (Vz
using the following lemma.

−

EVz)z∈B1,2 is concentrated around 0 with high probability by

Lemma 5 Suppose that
process deﬁned by

A1, . . . ,

Ak be disjoint subsets in Rp. Let (Uz)z∈B1,2 be a random

Uz

def
=

1
n

n

k

Xi=1

Xj=1

1Aj (xi)
xi, zji+ ,
h

16

(A.2)

where x1, . . . , xn are i.i.d. Normal(0, Ip). Then, for any δ
constant c > 0 such that

∈

(0, 1), there exists a numerical

sup
z∈B1,2 |

Uz

−

EUz

c

| ≤

(cid:18)

p log3 p log5 k + log(δ−1) log k
n

1/2

(cid:19)

(A.3)

holds with probability at least 1

δ.

−

Proof We ﬁrst show that Uz has subgaussian increments with respect to the ℓk
i.e.

∞(ℓp

2)-norm,

Uz
k

Uz′

kψ2 .

−

√log k
√n

Since

A1, . . . ,

(cid:13)
(cid:13)
Ak are disjoint, it follows that
(cid:13)

(zj)k

j=1 −

z′
j

k
j=1

(cid:0)

(cid:1)

.

∞(ℓp
ℓk
2)

(cid:13)
(cid:13)
(cid:13)

Uz
|

−

Uz′

| ≤

1
n

n

k

Xi=1

Xj=1

1Aj (xi)

xi, zj −
h
(cid:12)
(cid:12)

z′
ji
(cid:12)
(cid:12)

1
n

≤

n

Xi=1

max
1≤j≤k

xi, zj −
h

(cid:12)
(cid:12)

z′
ji
(cid:12)
(cid:12)

holds almost surely, where the last step follows from H¨older’s inequality. We proceed with
the following lemma.

(A.4)

(A.5)

Lemma 6 (van der Vaart and Wellner (1996, Lemma 2.2.2)) Let g
and a1, . . . , ak ∈

Rp. Then

∼

Normal(0, Ip)

max
j∈[k] |h

g, aji|
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

log k max

j∈[k] k

ajk2 .

.

ψ2

p

It follows from (A.5) and Lemma 6 that

Uz
k

−

Uz′

n

1
n

.

Xi=1
n

kψ2 ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
nv
u
u
t
√log k
√n
√log k
√n

.

=

max
j∈[k]

xi, zj −
h
(cid:12)
(cid:12)

max
j∈[k]

xi, zj −
h

ψ2

z′
ji
(cid:13)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:13)
z′
ji
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)

2

ψ2

Xi=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
max
j∈[k]

(cid:12)
(cid:12)
(cid:12)
zj −
(cid:13)
(cid:13)
(zj)k
j=1 −

(cid:13)
(cid:13)
(cid:13)

z′
j

2

(cid:13)
(cid:13)
z′
j

k
j=1

(cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)

,

∞(ℓp
ℓk
2)

where the second inequality follows from (Vershynin, 2018, Proposition 2.6.1).

Since Uz has a subgaussian increment as in (A.4), by (Vershynin, 2018, Lemma 2.6.8),

which says that centering does not harm the sub-gaussianity, we also have

(Uz

k

−

EUz)

(Uz′

−

−

EUz′)

kψ2 .

√log k
√n

17

(zj)k

j=1 −

z′
j

k
j=1

(cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

.

∞(ℓp
ℓk
2)

(A.6)

Therefore Dudley’s inequality (Dudley, 1967) applies to provide a tail bound on the left-
hand side of (A.3). Speciﬁcally it follows from a version of Dudley’s inequality (Vershynin,
2018, Theorem 8.1.6) that

sup
z∈B1,2 |

Uz

−

EUz

.

|

√log k
√n

∞

0
(cid:18)Z

holds with probability at least 1
trivially upper-bounded by

−

q
2 exp(

−

log N (B1,2,

∞(ℓp

2), η)dη + u diam (B1,2)

(A.7)

k·kℓk

(cid:19)

u2). Note that the diameter term in (A.7) is

diam(B1,2) = sup

z,z′∈B1,2

z′

z

−

∞(ℓp
ℓk

2) ≤

2 .

√pB1, where B1 denotes the unit ball in ℓ1, we have

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Moreover, since B1,2 ⊆

∞

0
Z

q

log N (B1,2,

∞(ℓp

2), η)dη

k·kℓk

∞

log N (√pB1,

0

≤
. √p log3/2 p log2 k ,

q

Z

∞(ℓp

2), η)dη

k·kℓk

where the second inequality follows from Maurey’s empirical method (Carl, 1985) (also see
(Junge and Lee, 2020, Lemma 3.4)). By plugging in these estimates to (A.7), we obtain
that

sup
z∈B1,2 |

Uz

−

EUz

|

holds with probability at least 1

−

p log3 p log5 k + log(δ−1) log k
n

.

δ.

(cid:18)

(cid:19)

1/2

Note that

C1, . . . ,

Ck are disjoint except on a boundary, which corresponds to a set of
measure zero. Since the standard multivariate normal distribution is absolutely continuous
relative to the Lebesgue measure, these null sets can be ignored in getting a tail bound on
the inﬁmum of the random process (Vz)z∈B1,2 . Moreover, Vz is written as Vz = V +
z + V −
z ,
where

and

V −
z

def
=

1
n

n

k

V +
z

def
=

1
n

1Cj (xi)
xi, zji+
h

Xi=1

Xj=1

n

k

1Cj (xi)
xi,
h

zji+ .

−

Xi=1

Xj=1

Since (V +

z )z∈B1,2 and (V −

z )z∈B1,2 are in the form of (A.2), by Lemma 5, we obtain that

sup
z∈B1,2 |

Vz

−

EVz

sup
z∈B1,2

| ≤

V +
z −

EV +
z

+ sup
z∈B1,2

V −
z −

EV −
z

(cid:12)
(cid:12)

p log3 p log5 k + log(δ−1) log k
n

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1/2

(cid:12)
(cid:12)

(cid:19)

(A.8)

holds with probability at least 1

Finally, the assertion is obtained by plugging in the above estimates to (A.1).

.

−

(cid:18)
δ/2.

18

A.2 Proof of Lemma 2

Note that Qz is decomposed into

Qz =

1
n

n

k

Xi=1

Xj=1

1 e
Cj \Cj

xi, zji
(xi)
h

+

1
n

n

k

Xi=1

Xj=1

1

Cj \

e
Cj

xi,
(xi)
h

.

zji

−

(A.9)

Then the summands in the right-hand side of (A.9) are respectively upper-bounded by

and

Q′′
z

def
=

1
n

Q′
z

def
=

1
n

1 e
Cj \Cj

xi, zj i+
(xi)
h

n

k

Xi=1

Xj=1

n

k

1

Cj \

e
Cj

xi,
(xi)
h

zji+ .

−

Xi=1

Xj=1
z and supz∈B1,2 Q′′

We upper-bound supz∈B1,2 Q′
z to get an upper bound on supz∈B1,2 Qz
through (A.9) by the triangle inequality. Speciﬁcally, we show that there exists a numerical
constant c > 0 such that

sup
kzk1,2=1

Q′

z ≤

sup
j∈[k],w∈Sp−1

E1 e

Cj \Cj

g, w
(g)
h

i+ + c

and

sup
kzk1,2=1

Q′′

z ≤

sup
j∈[k],w∈Sp−1

E1

Cj \

e
Cj

g, w
(g)
h

i+ + c

p log3 p log5 k + log(δ−1) log k
n

1/2

(cid:19)

(A.10)

p log3 p log5 k + log(δ−1) log k
n

1/2

(cid:19)

(cid:18)

(cid:18)

hold simultaneously with probability at least 1

δ/2.

−

Due to the symmetry, it suﬃces to show that (A.10) holds with probability 1

the triangle inequality, it follows that

sup
kzk1,2=1

Q′

z ≤

sup
kzk1,2=1

EQ′

z + sup

z∈B1,2 |

Q′

z −

EQ′
z|

.

δ/4. By

−

Then, similar to Lemma 1, we derive (A.10) through the concentration of the maximum devi-
z. The supremum
ation, that is, supz∈B1,2 |
of the expectation is upper-bounded as

, and an upper bound on supz∈B1,2

EQ′
z|

EQ′

z −

Q′

k

EQ′

z = E

1 e
Cj \Cj

g, zj i+ ≤
(g)
h

max
j∈[k],w∈Sp−1

E1 e

Cj\Cj

g, w
(g)
h

i+

k

Xj=1

zjk2 .
k

Moreover, since
obtain that

Xj=1
C1, . . . ,
e

sup
z∈B1,2

holds with probability at least 1

(cid:12)
(cid:12)

(cid:12)
(cid:12)
−

Ck are disjoint (except on a set of measure zero), by Lemma 5, we
e
Q′

p log3 p log5 k + log(δ−1) log k
n

(A.11)

EQ′
z

z −

1/2

.

(cid:18)

(cid:19)

δ/4. This provides the assertion in (A.10).

19

A.3 Proof of Lemma 4

We ﬁrst prove the ﬁrst assertion. Since
g/
. Moreover, Bayes’ rule implies
k

k2 ∈ C

g

is a cone, it follows that g

if and only

∈ C

C

E 1C(g)

g, w

= P

g
{

i|

|h

E [

|h

g, w

i| |

g

] .

∈ C

∈ C}

(A.12)

g
k

k2 =

(A.13)

(A.14)

Therefore we have

inf
w∈S1

E 1C(g)

g, w

|h

= inf
w∈S1

i|

P

g

{

∈ C}

P

g
{

(a)
= inf
w∈S1
√2Γ(3/2)
Γ(2)

(b)
=

∈ C}

inf
w∈S1

k2

g
k2
g
h
(cid:12)
k
(cid:12)
(cid:12)
k2] E
(cid:12)
E

E

g
k
(cid:20)
E [
g
k
θC
2π

(cid:20) (cid:12)
(cid:28)
(cid:12)
g
(cid:12)
(cid:12)
g
k

k2

, w

i
(cid:12)
(cid:12)
g
(cid:12)
(cid:12)
g
k2
k
, w

(cid:21)

g
g
k2 ∈ C
(cid:12)
k
(cid:12)
g
(cid:12)
, w
(cid:12)
g
k2 ∈ C
k
,

(cid:12)
(cid:29)(cid:12)
(cid:12)
(cid:12)
g
(cid:12)
(cid:12)
(cid:12)
(cid:12)
g
k2 ∈ C
k

(cid:21)

(cid:21)

(cid:28)

(cid:20) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k2 are independent and (b) follows from E
θC
2π

= P

=

.

g
g
k2 ∈ C
k

(cid:27)

(cid:26)

where (a) holds since
√2Γ(3/2)/Γ(2) and

g
k

g
k2 and g/
k

P

g
{

∈ C}

Then it remains to compute the expectation in (A.12). Below we show that

and

E

inf
w∈S2

E

sup
w∈S2

g
g
k

k2

g
g
k

k2

, w

, w

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28)

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)(cid:12)
(cid:28)
(cid:12)
(cid:12)
(cid:12)

g
g
k2 ∈ C
k

(cid:21)

=

g
g
k2 ∈ C
k

(cid:21)

=

4
θC

2
θC

sin2

θC
2

(cid:18)

(cid:19)

sin

θC
2

.

(cid:19)

(cid:18)

S1 satisfy that

=

} ⊂

. Then let z be the unit vector
a, b
is the conic hull of
Let
T
{
obtained by normalizing (a + b)/2. Then we have ∠(a, z) = θC/2 and ∠(b, z) = θC/2. Let
R be deﬁned by φ(w) := ∠(z, w). Since the conditional expectation applies to
φ : S1
, which is invariant under the global sign change in w, it suﬃces to consider
g
g/
k
k2 is uniformly distributed on the unit sphere,

→
k2, w
|h
g
π. Since g/
w that satisﬁes 0
k
the expectation term in (A.13) is written as

φ(w)

≤

≤

i|

T

C

E

, w

g
g
k

k2

g
g
k2 ∈ C
k

=

1
θC Z

φ(w)+θC /2

φ(w)−θC/2 |

(cid:28)

(cid:29)(cid:12)
(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
It follows from the assumption on the range of θC and φ(w) that
(cid:12)
(cid:12)
φ(w) + θC/2
and 0
cases for (θC, φ(w)) given below.

π
3π/2. We proceed by separately considering the complementary

φ(w)

θC/2

π/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

≤

−

≤

≤

≤

(cid:21)

cos θ

dθ .
|

(A.15)

Case 1: Suppose that

π
2 ≤

−

φ(w)

θC
2

−

< φ(w) +

θC
2 ≤

π
2

.

(A.16)

20

Then φ(w) is constrained by

Furthermore, the integral in (A.15) is rewritten as

φ(w)

0

≤

π/2

−

≤

θC/2 .

(A.17)

φ(w)+θC /2

φ(w)+θC /2

φ(w)−θC/2 |

Z

cos θ

dθ =
|

φ(w)−θC/2

Z

cos θdθ

= sin

φ(w) +

(cid:18)

= 2 cos (φ(w)) sin

θC
2

θC
2

−

(cid:19)

sin

φ(w)

(cid:18)

.

(cid:19)

−
θC
2

(cid:19)

(cid:18)

(A.18)

0, the expression in (A.18) monotonically decreases in φ(w) for the in-
Since sin(θC/2)
terval given in (A.17). Thus the maximum (resp. minimum) is attained as 2 sin(θC/2) at
φ(w) = 0 (resp. 2 sin2(θC/2) at φ(w) = π/2

θC/2).

≥

−

Case 2: Suppose that

Then φ(w) satisﬁes

π
2 ≤

−

φ(w)

θC
2

−

<

π
2

< φ(w) +

θC
2 ≤

3π
2

.

π
2 −

θC
2 ≤

φ(w)

π
2

+

θC
2

≤

and the integral in (A.15) reduces to

φ(w)+θC/2

φ(w)−θC /2 |
Z

cos θ

dθ =
|

φ(w)−

Z

θC
2

cos θdθ

π
2

φ(w)+

θC
2

= 2

= 2

−

−

sin

φ(w)

(cid:18)

−

2 sin (φ(w)) cos

−

θC
2

cos θdθ

sin

φ(w) +

(cid:18)

.

(cid:19)

π
2

Z

−
θC
2

(cid:19)

(cid:18)

θC
2

(cid:19)

(A.19)

(A.20)

(A.21)

Since cos(θC/2)
2 sin2(θC/2) at φ(w) = π/2

0 for all θC ∈
−

≥

[0, π], the maximum (resp. minimum) is attained as

θC/2 (resp. 4 sin2(θC/4) at φ(w) = π/2).

Case 3: Suppose that

Then we have

π
2 ≤

φ(w)

θC
2

−

< φ(w) +

θC
2 ≤

3π
2

.

π
2

+

θC
2 ≤

φ(w)

π

≤

21

(A.22)

(A.23)

and

φ(w)+θC /2

φ(w)+θC /2

φ(w)−θC/2 |

Z

cos θ

dθ =
|

φ(w)−θC/2

Z

cos θ)dθ

(
−
θC
2

−

sin

φ(w) +

(cid:18)

θC
2

(cid:19)

−

= sin

φ(w)

(cid:18)

2 cos φ(w) sin

=

−

(cid:19)
θC
2

.

(A.24)

The maximum (resp. minimum) of (A.24) is attained as 2 sin(θC/2) at φ(w) = π (resp.
2 sin2(θC/2) at φ(w) = π/2 + θC/2).

By combining the results in the above three cases, we obtain (A.13) and (A.14). Then

substituting the expectation term in (A.12) by (A.13) provides the ﬁrst assertion.

Next we prove the second assertion. Similarly to (A.12), we have

sup
w∈S1

E1Cj (g)
g, w
h

i+ = sup
w∈S1

P

g
{

∈ C}

E

g
k
(cid:20)
E [
g
k

k2 ·

(cid:28)
k2] E

g
g
k

, w

k2
g
g
k

k2

g

{

P

(a)
= sup
w∈S1
√2Γ(3/2)
Γ(2)

(b)
=

∈ C}

E

θC
2π

sup
w∈S1

(cid:20)(cid:28)
g
g
(cid:20)(cid:28)
k
k2 are independent, (b) follows from E

(cid:29)+ (cid:12)
(cid:12)
(cid:12)
(cid:12)

, w

k2

, w

(cid:21)

g
g
k2 ∈ C
k
g
g
k2 ∈ C
k
,

(cid:29)+ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:29)+ (cid:12)
(cid:12)
g
(cid:12)
(cid:12)
g
k2 ∈ C
k

(cid:21)

(cid:21)

g
k

k2 = √2Γ(3/2)/Γ(2),

where (a) holds since
and

g
k

g
k2 and g/
k

P

g
{

∈ C}

= P

g
g
k2 ∈ C
k

(cid:27)

(cid:26)

=

θC
2π

.

If suﬃces to show that

E

max
w∈S1

g
g
k2 ∈ C
(cid:29)+ (cid:12)
k
(cid:12)
(cid:12)
k2 is uniformly distributed on the unit sphere S1 and u+ = (u +
(cid:12)

g
g
k

2
θC

θC
2

, w

(cid:20)(cid:28)

sin

k2

=

(cid:18)

(cid:19)

(cid:21)

.

Since g/
g
k
R, we have
u

∈

(A.25)

)/2 for all
u
|
|

g
g
k2 ∈ C
(cid:21)
k
cos θ
cos θ +
|
2θC

(cid:29)+ (cid:12)
(cid:12)
(cid:12)
(cid:12)

|

dθ

E

(cid:20)(cid:28)

, w

g
g
k2
k
φ(w)+θC/2

=

=

=

φ(w)−θC /2

Z
1
2  
1
2

(cid:18)

1
θC Z
E

φ(w)+θC /2

φ(w)−θC/2 |

cos θ

dθ +
|

g
g
k

k2

, w

|

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)(cid:12)
(cid:28)
(cid:12)
(cid:12)
(cid:12)

g
g
k2 ∈ C
k

(cid:21)

1
θC Z
+ E

φ(w)+θC/2

φ(w)−θC /2
g
g
k

(cid:20)(cid:28)

k2

22

cos θdθ

!
g
g
k2 ∈ C
k

.

(cid:21) (cid:19)

, w

(cid:29) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(A.26)

As shown above, the ﬁrst term in (A.26) is maximized at φ(w) = 0 and the maximum is
given in (A.14). Furthermore, the second term in (A.26) is rewritten as

φ(w)+θC /2

φ(w)−θC /2

Z

cos θdθ = sin

φ(w) +

(cid:18)

θC
2

= 2 cos φ(w) sin

(cid:18)

−

sin

φ(w)

(cid:18)

θC
2

−

(cid:19)

.

(cid:19)

(cid:19)
θC
2

Since sin (θC/2)
Hence, the maximum is attained at φ(w) = 0 as

≥

0, the expression in (A.27) is a decreasing function of φ(w)

max
w∈S1

2 cos φ(w) sin

θC
2

(cid:18)

(cid:19)

= 2 sin

θC
2

.

(cid:19)

(cid:18)

(A.27)

[0, π].

∈

(A.28)

Since the two terms in (A.26) are maximized simultaneously, by plugging in the above
results to (A.25), the second assertion is obtained.

23

