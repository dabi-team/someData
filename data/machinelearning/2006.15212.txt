HybridModelsforLearningtoBranchPrateekGupta∗UniversityofOxfordTheAlanTuringInstitutepgupta@robots.ox.ac.ukMaximeGasseMila,PolytechniqueMontréalmaxime.gasse@polymtl.caEliasB.KhalilUniversityofTorontokhalil@mie.utoronto.caM.PawanKumarUniversityofOxfordpawan@robots.ox.ac.ukAndreaLodiCERC,PolytechniqueMontréalandrea.lodi@polymtl.caYoshuaBengioMila,UniversitédeMontréalyoshua.bengio@mila.quebecAbstractArecentGraphNeuralNetwork(GNN)approachforlearningtobranchhasbeenshowntosuccessfullyreducetherunningtimeofbranch-and-bound(B&B)algorithmsforMixedIntegerLinearProgramming(MILP).WhiletheGNNreliesonaGPUforinference,MILPsolversarepurelyCPU-based.Thisseverelylimitsitsapplicationasmanypractitionersmaynothaveaccesstohigh-endGPUs.Inthiswork,weasktwokeyquestions.First,inamorerealisticsettingwhereonlyaCPUisavailable,istheGNNmodelstillcompetitive?Second,canwedeviseanalternatecomputationallyinexpensivemodelthatretainsthepredictivepoweroftheGNNarchitecture?Weanswertheﬁrstquestioninthenegative,andaddressthesecondquestionbyproposinganewhybridarchitectureforefﬁcientbranchingonCPUmachines.TheproposedarchitecturecombinestheexpressivepowerofGNNswithcomputationallyinexpensivemulti-layerperceptrons(MLP)forbranching.WeevaluateourmethodsonfourclassesofMILPproblems,andshowthattheyleadtoupto26%reductioninsolverrunningtimecomparedtostate-of-the-artmethodswithoutaGPU,whileextrapolatingtoharderproblemsthanitwastrainedon.Thecodeforthisprojectispubliclyavailableathttps://github.com/pg2455/Hybrid-learn2branch.1IntroductionMixed-IntegerLinearPrograms(MILPs)arisenaturallyinmanydecision-makingproblemssuchasauctiondesign[1],warehouseplanning[13],capitalbudgeting[14]orscheduling[15].Apartfromalinearobjectivefunctionandlinearconstraints,somedecisionvariablesofaMILParerequiredtotakeintegralvalues,whichmakestheproblemNP-hard[35].ModernmathematicalsolverstypicallyemploytheB&Balgorithm[29]tosolvegeneralMILPstoglobaloptimality.Whiletheworst-casetimecomplexityofB&Bisexponentialinthesizeoftheproblem[38],ithasprovenefﬁcientinpractice,leadingtowideadoptioninvariousindustries.Atahighlevel,B&Badoptsadivide-and-conquerapproachthatconsistsinrecursivelypartitioningtheoriginalproblemintoatreeofsmallersub-problems,andsolvinglinearrelaxationsofthesub-problemsuntilanintegralsolutionisfoundandprovenoptimal.∗TheworkwasdoneduringaninternshipatMilaandCERC.Correspondenceto:<pgupta@robots.ox.ac.uk>34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada.arXiv:2006.15212v3  [cs.LG]  23 Oct 2020Despiteitsapparentsimplicity,therearemanypracticalaspectsthatmustbeconsideredforB&Btoperformwell[2];suchdecisionswillaffectthesearchtree,andultimatelytheoverallrunningtime.Theseincludeseveraldecisionproblems[33]thatariseduringtheexecutionofthealgorithm,suchasnodeselection:whichsub-problemdoweanalyzenext?;andvariableselection(a.k.a.branching):whichdecisionvariablemustbeused(branchedon)topartitionthecurrentsub-problem?Whilesuchdecisionsaretypicallymadeusinghard-codedexpertheuristicswhichareimplementedinmodernsolvers,moreandmoreattentionisgiventostatisticallearningapproachesforreplacingandimprovinguponthoseheuristics[23;5;26;17;39].AnextensivereviewofdifferentapproachesattheintersectionofstatisticallearningandcombinatorialoptimizationisgiveninBengioetal.[7].50100150200250Number of branching decisions020406080100Cumulative Time (s)RPB-CPUGNN-CPUHybrid-CPU (Ours)GNN-GPUFigure1:Cumulativetimecostofdifferentbranchingpolicies:(i)thedefaultinternalruleRPBoftheSCIPsolver;(ii)aGNNmodel(usingaGPUoraCPU);and(iii)ourhybridmodel.ClearlytheGNNmodelrequiresaGPUforbeingcompetitive,whileourhybridmodeldoesnot.(Measuredonacapacitatedfacilitylocationproblem,mediumsize).Recently,Gasseetal.[17]proposedtotacklethevari-ableselectionprobleminB&BusingaGraphNeuralNetwork(GNN)model.TheGNNexploitsthebi-partitegraphformulationofMILPstogetherwithasharedparametricrepresentation,thusallowingittomodelproblemsofarbitrarysize.Usingimita-tionlearning,themodelistrainedtoapproximateaverygoodbutcomputationallyexpensive“expert"heuristicnamedstrongbranching[6].Theresultingbranchingstrategyisshowntoimproveuponprevi-ouslyproposedapproachesforbranchingonseveralMILPproblembenchmarks,andiscompetitivewithstate-of-the-artB&Bsolvers.Wenotethatonelim-itationofthisapproach,withrespecttogeneralB&Bheuristics,isthattheresultingstrategyisonlytailoredtotheclassofMILPproblemsitistrainedon.Thisisveryreasonableinourview,aspractitionersusuallyonlycareaboutsolvingveryspeciﬁcproblemtypesatanytime.WhiletheGNNmodelseemsparticularlysuitedforlearningbranchingstrategies,onedrawbackisahighcomputationalcostforinference,i.e.,choosingthebranchingvariableateachnodeoftheB&Btree.InGasseetal.[17],theauthorsuseahigh-endGPUcardtospeed-uptheGNNinferencetime,whichisacommonpracticeindeeplearningbutissomewhatunrealisticforMILPpractitioners.Indeed,commercialMILPsolversrelysolelyonCPUsforcomputation,andtheGNNmodelfromGasseetal.[17]isnotcompetitiveonCPU-onlymachines,asillustratedinFigure1.Thereisindeedatrade-offbetweenthequalityofthebranchingdecisionsmadeandthetimespentobtainingthosedecisions.Thistrade-offiswell-knowninMILPcommunity[2],andhasgivenrisetocarefullybalancedstrategiesdesignedbyMILPexperts,suchashybridbranching[3],whichderivefromthecomputationallyexpensivestrongbranchingheuristic[6].Inthispaper,westudythetime-accuracytrade-offinlearningtobranchwiththeaimofdevisingamodelthatisbothcomputationallyinexpensiveandaccurateforbranching.Tothisend,weproposeahybridarchitecturethatusesaGNNmodelonlyattherootnodeoftheB&Btreeandaweakbutfastpredictor,suchasasimpleMulti-LayerPerceptron(MLP),attheremainingnodes.Indoingso,theweakmodelisenhancedbyhigh-levelstructuralinformationextractedattherootnodebytheGNNmodel.Inadditiontothisnewhybridarchitecture,weexperimentandevaluatetheimpactofseveralvariantstoourtrainingprotocolforlearningtobranch,including:(i)end-to-endtraining[19;8],(ii)knowledgedistillation[25],(iii)auxiliarytasks[30],and(iv)depth-dependentweightingofthetraininglossforlearningtobranch,anideaoriginallyproposedbyHeetal.[23]inthecontextofnodeselection.Weevaluateourapproachonlarge-scaleMILPinstancesfromfourproblemfamilies:CapacitatedFacilityLocation,CombinatorialAuctions,SetCovering,andIndependentSet.Wedemonstrateempiricallythatourcombinationofhybridarchitectureandtrainingprotocolresultsinstate-of-the-artperformanceintherealisticsettingofaCPU-restrictedmachine.WhileweobserveaslightdecreaseinthepredictiveperformanceofourmodelwithrespecttotheoriginalGNNfrom[17],itsreducedcomputationalcoststillallowsforareductionofupto26%inoverallsolvingtimeonalltheevaluatedbenchmarkscomparedtothedefaultbranchingstrategyofthemodernopen-sourcesolverSCIP[20].Also,ourhybridmodelpreservestheabilitytoextrapolatetoharderproblemsthantrainedon,astheoriginalGNNmodel.22RelatedWorkFindingabranchingstrategythatresultsinthesmallestB&BtreeforaMILPisatleastashard–andpossiblymuchharder–thansolvingtheMILPwithanystrategy.Still,smalltreescanbeobtainedbyusingacomputationallyexpensiveheuristicnamedstrongbranching(SB)[6;31].ThemajorityoftheresearcheffortsinvariableselectionarethusaimedatmatchingtheperformanceofSBthroughfasterapproximations,viacleverlyhandcraftedheuristicssuchasreliabilitypseudocostbranching[4],andrecentlyviamachinelearning2[5;26;17].Wereferto[33]foranextensivesurveyofthetopic.Alvarezetal.[5]andKhaliletal.[26]showedthatafastdiscriminativeclassiﬁersuchasextremelyrandomizedtrees[18]orsupportvectormachines[24]onhand-designedfeaturescanbeusedtomimicSBdecisions.Subsequently,Gasseetal.[17]andZarpellonetal.[39]showedtheimportanceofrepresentationlearningforbranching.Ourapproach,insomesense,combinesthesuperiorrepresentationframeworkofGasseetal.[17]withthecomputationallycheaperframeworkofKhaliletal.[26].SuchhybridarchitectureshavebeensuccessfullyusedinMLproblemssuchasvisualreasoning[36],style-transfer[11],naturallanguageprocessing[37;10],andspeechrecognition[27].3PreliminariesThroughoutthispaper,weuseboldfaceforvectorsandmatrices.AMILPisamathematicaloptimizationproblemthatcombinesalinearobjectivefunction,asetoflinearconstraints,andamixofcontinuousandintegraldecisionvariables.Itcanbewrittenas:argminxc|x,s.t.Ax≤b,x∈Zp×Rn−p,wherec∈Rndenotesthecostvector,A∈Rm×nthematrixofconstraintcoefﬁcients,b∈Rmisthevectorofconstanttermsoftheconstraints,andtherearepintegervariables,1≤p≤n.TheB&Balgorithmcanbedescribedasfollows.Oneﬁrstsolvesthelinearprogram(LP)relaxationoftheMILP,obtainedbydisregardingtheintegralityconstraintsonthedecisionvariables.IftheLPsolutionx?satisﬁestheMILPintegralityconstraints,orisworsethanaknownintegralsolution,thenthereisnoneedtoproceedfurther.Ifnot,thenonedividestheMILPintotwosub-MILPs.Thisistypicallydonebypickinganintegraldecisionvariablethathasafractionalvalue,i∈C={i|x?i6∈Z,i≤p},andcreatetwosub-MILPswithadditionalconstraintsxi≤bx?icandxi≥dx?ie,respectively.Thedecisionvariableithatisusedtopartitionthefeasibleregioniscalledthebranchingvariable,whileCdenotesthebranchingcandidates.Thesecondstepistoselectoneoftheleavesofthetree,andrepeattheabovestepsuntilallleaveshavebeenprocessed3.Inthiswork,werefertotheﬁrstnodeprocessedbyB&Bastherootnode,whichcontainstheoriginalMILP,andallsubsequentnodescontainingalocalMILPastreenodes,wheneverthedistinctionisrequired.Otherwisewerefertothemsimplyasnodes.4MethodologyAsmentionedearlier,computationallyheavyGNNscanbeprohibitivelyslowwhenusedforbranchingonCPU-onlymachines.Inthissectionwedescribeourhybridalternative,whichcombinesthesuperiorinductivebiasofaGNNattherootnodewithacomputationallyinexpensivemodelatthetreenodes.Wealsodiscussvariousenhancementstothetrainingprotocol,inordertoenhancetheperformanceofthelearnedmodels.4.1HybridarchitectureAvariableselectionstrategyinB&Bcanbeseenasascoringfunctionfthatoutputsascoresi∈Rforeverybranchingcandidate.Assuch,fcanbemodeledasaparametricfunction,learnedbyML.Branchingthensimplyinvolvesselectingthehighest-scoringcandidateaccordingtof:i?f=argmaxi∈Csi2Interestingly,earlyworksonMLmethodsforbranchingcanbetracedbackto2000[2,Acknowledgements].3ForamoreinvolveddescriptionofB&B,thereaderisreferredtoAchterberg[2].3Table1:Variousfunctionalformsfconsideredforvariableselection((cid:12)denotesHadamardproduct).DataextractionComputationalCostDecisionFunctionGNN[17]expensiveExpensives=GNN(G)MLPcheapModerates=MLP(X)CONCAThybridModerateΨΨΨ=GNN(G0)s=MLP([ΨΨΨ,X])FiLM[36]hybridModerateγγγ,βββ=GNN(G0)s=FiLM(γγγ,βββ,MLP(X))HyperSVMhybridCheapestW=GNN(G0)s=(W(cid:12)X)1HyperSVM-FiLMhybridCheapestγγγ,β1β1β1,β2β2β2=GNN(G0)s=β2β2β2|max(0,β1β1β1(cid:12)X+γγγ)Weconsidertwoformsofnoderepresentationsformachinelearningmodels:(i)agraphrepresentationG∈G,suchasthevariable-constraintbipartitegraphofGasseetal.[17],whereG=(V,E,C),withV∈Rn×d1variablefeatures,E∈Rn×m×d2edgefeatures,andC∈Rm×d3constraintfeatures;and(ii)branchingcandidatefeaturesX∈R|C|×d4,suchasthosefromKhaliletal.[26],whichischeapertoextractthantheﬁrstrepresentation.Forconvenience,wedenotebyXthegenericspaceofthebranchingcandidatefeatures,andbyG0thegraphrepresentationoftherootnode.Thevariousfeaturesdiusedinthisworkaredetailedinthesupplementarymaterials.InB&B,structuralinformationinthetreenodes,G,sharesalotofsimilaritywiththatoftherootnode,G0.Extracting,butalsoprocessingthatinformationateverynodeisanexpensivetask,whichwewilltrytocircumvent.Themainideaofourhybridapproachisthentosuccinctlyextracttherelevantstructuralinformationonlyonce,attherootnode,withaparametricmodelGNN(G0;θθθ).WethencombineinthetreenodesthispreprocessedstructuralinformationwiththecheapcandidatefeaturesX,usingahybridmodelf:=MLPHYBRID(GNN(G0;θθθ),X;φφφ).Bydoingso,wehopethattheresultingmodelwillapproachtheperformanceofanexpensivebutpowerfulf:=GNN(G),atalmostthesamecostasaninexpensivebutlesspowerfulf:=MLP(X).Figure2illustratesthedifferencesbetweenthoseapproaches,intermsofdataextraction.Figure2:Dataextractionstrategies:bipartitegraphrepresentationGateverynode(expen-sive);candidatevariablefeaturesXateverynode(cheap);bipartitegraphattherootnodeandvariablefeaturesattreenode(hybrid).Foranexhaustivecoverageofthecomputationalspectrumofhybridmodels,weconsiderfourwaystoenrichthefeaturespaceofanMLPviaaGNN’soutput,sumarrizedinTable1.InCONCAT,weconcatenatethecandidate’srootrepresentationsΨΨΨwiththefeaturesXatanode.InFiLM[36],wegenerateﬁlmparametersγγγ,βββfromtheGNN,foreachcandidate,whicharefurtherusedtomod-ulatethehiddenlayersoftheMLP.Indetails,ifhhhistheintermediaterepresentationoftheMLP,itgetslinearlymodulatedashhh←βββ·hhh+γγγ.Whileboththeabovearchitectureshavesimilarcompu-tationalcomplexity,ithasbeenshownthatFiLMsubsumestheCONCATarchitecture[12].Ontheothertheendofthespectrumliethemostinexpen-sivehybridarchitectures,HyperSVMandHyperSVM-FiLM.HyperSVMisinspiredbyHyperNet-works[22],andsimplyconsistsinamulti-classSupportVectorMachine(SVM),whoseparametersarepredictedbytherootGNN.Wechoseasimplelineardisciminator,foraminimalcomputationalcost.Finally,inHyperSVM-FiLMweincreasetheexpressivityofHyperSVMwiththehelpofmodulations,similartothatofFiLM.4.2TrainingProtocolWeusestrongbranchingdecisionsasground-truthlabelsforimitationlearning,andcollectob-servationsoftheform(G0,G,X,i?SB).Thus,thedatausedfortrainingthemodelisD=4{(G0k,Gk,Xk,i?SB,k),k=1,2,...,N}.Wetreattheproblemofidentifyingi?SB,kasaclassiﬁ-cationproblem,suchthati∗=argmaxi∈Cf(G0,X)isthetargetoutcome.ConsideringDasourground-truth,ourobjective(1)istominimizethecross-entropylosslbetweenf(G0,X)∈R|C|andaone-hotvectorwithoneatthetarget:L(D;θθθ,φφφ)=1NNXk=1l(f(G0k,Xk;θθθ,φφφ),i?SB,k).(1)Performanceonunseeninstances,orgeneralization,isofutmostimportancewhenthetrainedmodelsareusedonbiggerinstances.Thechoicesintrainingtheaforementionedarchitecturesinﬂuencethisability.Inthissection,wediscussfoursuchimportantchoicesthatleadtobettergeneralization.4.2.1End-to-endTraining(e2e)AGNNˆθˆθˆθ,withpre-trainedparametersˆθˆθˆθ,isobtainedusingtheproceduredescribedinGasseetal.[17].Weusethispre-trainedGNNtoextractvariablerepresentationsattherootnodeanduseitasaninputtotheMLPsatatreenode(pre).Thisresultsinaconsiderableperformanceboostoverplain"salt-of-the-earth"MLPusedatalltreenodes.However,goingastepfurther,anend-to-end(e2e)approachinvolvestrainingtheGNNandtheMLPtogetherbybackpropagatingthegradientsfromatreenodetotherootnode.Indoingso,e2etrainingalignsthevariablerepresentationsattherootnodewiththepredictiontaskatatreenode.Atthesametime,itisnotobviousthatitshouldresultinastablelearningbehaviorbecausetheparametersforGNNneedtoadapttovarioustreenodes.Ourexperimentsexplorebothpre-training(pre)andend-to-endtraining(e2e),namely:(pre)φφφ∗=argminφφφL(D;ˆθˆθˆθ,φφφ),(e2e)φφφ∗,θθθ∗=argminθθθ,φφφL(D;θθθ,φφφ).(2)4.2.2KnowledgeDistillation(KD)Usingtheoutputsofapre-trainedexpertmodelasasoft-targetfortrainingasmallermodelhasbeensuccessfullyusedinmodelcompression[25].Inthisway,oneaimstolearnaninexpensivemodelthathasthesamegeneralizationpowerasanexpert.Thus,forbettergeneralization,insteadoftrainingourhybridarchitectureswithcross-entropy[21]onground-truthhard-labels,westudytheeffectoftrainingwithKLDivergence[28]betweentheoutputsofapre-trainedGNNandahybridmodel,namely:(KD)LKD(D;θθθ,φφφ)=1NNXk=1KL(f(G0k,Xk;θθθ,φφφ),GNNˆθˆθˆθ(Gk)).(3)4.2.3AuxiliaryTasks(AT)Aninductivebias,suchasGNN,encodesaprioronthewaytoprocessrawinput.Auxiliarytasks,ontheotherhand,injectpriorsinthemodelthroughadditionallearningobjectives,whicharenotdirectlylinkedtothemaintask.Thesetasksareneitherrelatedtotheﬁnaloutputnordotheyrequireadditionaltrainingdata.Onesuchauxiliarytaskistomaximizethediversityinvariablerepresentations.TheintuitionisthatverysimilarrepresentationsleadtoverycloseMLPscorepredictions,whichisnotusefulforbranching.Weminimizeapairwiselossfunctionthatensuresmaximumseparationbetweenthevariablerepre-sentationsprojectedonaunithypersphere.Weconsidertwotypesofobjectivesforthis:(i)EuclideanDistance(ED),and(ii)MinimumHypersphericalEnergy(MHE)[32],inspiredfromthewell-knownThomsonproblem[16]inPhysics.WhileEDseparatestherepresentationsintheEuclideanspaceonthehypersphere,MHEensuresuniformdistributionoverthehypersphere.Denotingˆψψψiasthevariablerepresentationforthevariableiprojectedonaunithypersphereandeij=||ˆψψψi−ˆψψψj||2astheEuclideandistancebetweentherepresentationsforthevariablesiandj,ournewobjectivefunctionisgivenasLAT(D;θθθ,φφφ)=L(D;θθθ,φφφ)+g(Ψ;θθθ),where(ED)g(Ψ;θθθ)=1N2NXi,j=1e2ij,(MHE)g(Ψ;θθθ)=1N2NXi,j=11eij.(4)54.2.4LossWeightingSchemeTheproblemofdistributionshiftisunavoidableinasequentialprocesslikeB&B.AsuboptimalbranchingdecisionatanodeclosertotherootnodecanhaveworseimpactonthesizeoftheB&Btreeascomparedtowhensuchadecisionismadefartherfromit.Insuchsituations,onecanusedepth(possiblynormalized)asafeature,butthegeneralizationonbiggerinstancesisabitunpredictableasthedistributionofthisfeaturemightbeverydifferentfromthatobservedinthetrainingset.Thus,weexperimentedwithdifferentdepth-dependentformulationsforweightingthelossatanynode.Denotingziasthedepthofatreenodeirelativetothedepthofthetree,weweightthelossatdifferenttreenodesbyw(zi),makingourobjectivefunctionasL(D;θθθ,φφφ)=1NNXk=1w(zk)·l(f(G0k,Xk;θθθ,φφφ),i?SB,k).(5)Speciﬁcally,weconsidered5differentweightingfunctionssuchthatallofthemhavethesameendpoints,i.e.,w(0)=1.0attherootnodeandw(1)=e−0.5atthedeepestnode.Differentfunctionswerechosendependingontheirintermediatebehaviourinbetweenthesetwopoints.Weexperimentedwithexponential,linear,quadraticandsigmoidaldecaybehaviorofthesefunctions.Table3listsvariousfunctionsandtheirmathematicalformsconsideredinourexperiments.5ExperimentsWefollowtheexperimentalsetupofGasseetal.[17],andevaluateeachbranchingstrategyacrossfourdifferentproblemclasses,namelyCapacitatedFacilityLocation,MinimumSetCovering,CombinatorialAuctions,andMaximumIndependentSet.RandomlygeneratedinstancesaresolvedofﬂineusingSCIP[20]tocollecttrainingsamplesoftheform(G0,G,X,i?SB).Weleavethedescriptionofthedatacollectionandtrainingdetailsofeachmodeltothesupplementarymaterials.Evaluation.AsinGasseetal.[17],ourevaluationinstancesarelabeledassmall,medium,andbigbasedonthesizeofunderlyingMILP.Smallinstanceshavethesamesizeasthoseusedtogeneratethetrainingdatasets,andthusmatchthetrainingdistribution,whileinstancesofincreasingsizeallowsustomeasurethegeneralizationabilityofthetrainedmodels.Eachscenariouses20instances,solvedusing3differentseedstoaccountforsolvervariability.WereportstandardmetricsusedintheMILPcommunityforbenchmarkingB&Bsolvers:(i)Time:1-shiftedgeometricmean4ofrunningtimesinseconds,includingtherunningtimesforunsolvedinstances,(ii)Nodes:hardware-independent1-shiftedgeometricmeanofB&Bnodecountoftheinstancessolvedbyallbranchingstrategies,and(iii)Wins:numberoftimeseachbranchingstrategyresultedinthefastestsolvingtime,overtotalnumberofsolvedinstances.Allbranchingstrategiesareevaluatedusingtheopen-sourcesolverSCIP[20]withatimelimitof45minutes,andcuttingplanesareallowedonlyattherootnode.cauctions363840424446Top-1 Accuracyfacilities6062646668indset4648505254565860setcover4042444648505254GNNCOMPMLPCONCATFiLMHyperSVMHyperSVM-FiLMFigure3:Testaccuracyofthedifferentmodels,withasimplee2etrainingprotocol.Baselines.Wecompareourhy-bridmodeltoseveralnon-MLbase-lines,includingSCIP’sdefaultbranch-ingheuristicReliabilityPseudocostBranching(RPB),the“goldstan-dard"heuristicFullStrongBranching(FSB),andaveryfastbutineffectiveheuristicPseudocostBranching(PB).WealsoincludetheGNNmodelfromGasseetal.[17]runonCPU(GNN),andalsoseveralfastbutlessexpres-sivemodelssuchasSVMRankfromKhaliletal.[26],LambdaMARTfromBurges[9],andExtraTreeClassiﬁerfromGeurtsetal.[18]asbenchmarks.Forconciseness,wechosetoreportthoselastthreecompetitormodelsusinganoptimisticaggregationscheme,bysystematicallychoosingonlythebestperformingmethodamongthethree(COMP).Forcompleteness,wealso4forcompletedeﬁnitionrefertoAppendixA.3inAchterberg[2]6reporttheperformanceofaGNNmodelrunonahigh-endGPU,althoughwedonotconsiderthatmethodasabaselineandthereforedonotincludeitintheWinsindicator.WeacknowledgethatourcomparisontogeneralbranchingstrategylikeRPBisnotcompletelyfair,however,developingspecializedversionsofsuchstrategiesisachallengeinitselffullofmodelingchoicesthatweforeseeasfuturework.Modelselection.Toinvestigatetheeffectivenessofdifferentarchitectures,weempiricallycomparetheperformanceoftheirend-to-endvariants.Figure3comparestheTop-1testaccuracyofmodelsacrossthefourproblemsets.TheperformanceofGNNs(blue),beingthemostexpressivemodel,servesasanupperboundtotheperformanceofhybridmodels.AlloftheconsideredhybridmodelsoutperformMLPs(red)acrossallproblemsets.Additionally,weobservethatFiLM(green)andCONCAT(purple)performsigniﬁcantlybetterthanotherarchitectures.However,thereisnoclearwinneramongthem.Wealsonotethatthecheapesthybridmodels,HyperSVMandHyperSVM-FiLM,thoughbetterthanMLPs,stilldonotperformaswellasFiLMorCONCATmodels.Trainingprotocols.InTable2,weshowtheeffectofdifferentprotocolsdiscussedinsection4onTop-1accuracyoftheFiLMmodels.Weobservethatthepresenceoftheseprotocolsimprovesthemodelaccuracyby0.5-0.9%,whichtranslatestoaminoryetpracticallyusefulimprovementinB&Bperformanceofthesolver.ExceptforCombinatorialAuctions,FiLM’sperformanceisimprovedbyknowledgedistillation,whichsuggeststhatthesofttargetsofpre-trainedGNNyieldabettergeneralizationperformance.Lastly,welaunchahyperparametersearchforauxiliaryobjective:EDandMHE,ontopofthebestperformingmodel(Top-1accuracy),amonge2eande2e&KDmodels.Auxiliarytasksfurtherhelptheaccuracyofthehybridmodels,butforsomeproblemclassesitisEDthatworkswellwhileforothersitisMHE.Weprovidethemodelperformancesontestdatasetinthesupplementforfurtherreference.Table2:TestaccuracyofFiLM,usingdifferenttrainingprotocols.cauctionsfacilitiesindsetsetcoverPretrainedGNN44.12±0.0965.78±0.0653.16±0.5150.00±0.09e2e44.31±0.0866.33±0.3353.23±0.5850.16±0.05e2e&KD44.10±0.0966.60±0.2153.08±0.350.31±0.19e2e&KD&AT44.56±0.1366.85±0.2853.68±0.2350.37±0.03Table3:Effectofdifferentsampleweightingschemesoncom-binatorialauctions(big)instances,withasimpleMLPmodel.z∈[0,1]istheratioofthedepthofthenodeandthemaximumdepthobservedinatree.TypeWeightingschemeNodesWinsConstant1967810/60Exponentialdecaye−0.5z979310/60Linear(e−0.5−1)∗z+1978912/60Quadraticdecay(e−0.5−1)∗z2+1956114/60Sigmoidal(1+e−0.5)/(1+ez−0.5)953414/60Effectoflossweighting.Weempiricallyinvestigatetheef-fectofthedifferentlossweight-ingschemesdiscussedinSec-tion4.2.4.WetrainasimpleMLPmodelonoursmallCombi-natorialAuctionsinstances,andmeasuretheresultingB&Btreesizeonbiginstances.WereportaggregatedresultsinTable3,andprovideinstance-levelresultsinthesupplement.Weobservethatthemostcommonlyusedexpo-nentialandlinearschemesac-tuallyseemtodegradetheper-formanceofthelearnedstrategy,aswebelievethosemaybetooaggressiveatdisregardingnodesearlyoninthetree.Ontheotherhand,boththequadraticandsigmoidalschemesresultinanimprovement,thusvalidatingtheideathatdepth-dependentweightingcanbebeneﬁcialforlearningtobranch.Wethereforeoptforasigmoidallossweightingschemeinourtrainingprotocol.Completebenchmark.Finally,toevaluatetheruntimeperformanceofourhybridapproach,wereplaceSCIP’sdefaultbranchingstrategywithourbestperformingmodel,FiLM.WeobserveinTable4thatFiLMperformssubstantiallybetterthanallotherCPU-basedbranchingstrategies.ThecomputationallyexpensiveFSB,our“goldstandard”,becomesimpracticalasthesizeofinstancesgrows,whereasRPBremainscompetitive.WhileGNNretainsitssmallnumberofnodes,itlosesinrunningtimeperformanceonCPU.NotethatwefoundthatFiLMmodelsforMaximumIndependentSetdidinitiallyoverﬁtonsmallinstances,suchthattheperformanceonlargerinstancesdegradedsubstantially.Toovercomethisissueweusedweightdecay[34]regularization,withavalidationsetof2000observationsgeneratedusingrandommediuminstances(notusedforevaluation).Wereporttheperformanceoftheregularizedmodelsinthesupplement,andusethebestperformingmodeltoreportevaluationperformanceonmediumandbiginstances.Wealsoshowinthesupplementthatthe7Table4:Performanceofbranchingstrategiesonevaluationinstances.Wereportgeometricmeanofsolvingtimes,numberoftimesamethodwon(insolvingtime)overtotalﬁnishedruns,andgeometricmeanofnumberofnodes.Refertosection5formoredetails.Thebestperformingresultsareinbold.∗Modelswereregularizedtopreventoverﬁttingonsmallinstances.SmallMediumBigModelTimeWinsNodesTimeWinsNodesTimeWinsNodesFSB42.531/6013313.330/5975997.230/5150PB31.354/60139177.694/60384712.453/56309RPB36.861/6023213.991/60152794.802/5499COMP30.373/60120172.514/60347633.426/57294GNN39.180/60112209.840/60314748.850/54286FILM(ours)24.6751/60109136.4251/60325531.7046/57295GNN-GPU28.91–/60112150.11–/60314628.12–/56286CapacitatedFacilityLocationFSB27.160/6017582.180/451162700.000/0n/aPB10.190/6028694.120/6024512208.570/2382624RPB14.050/605494.650/6011291887.707/2748395COMP9.833/6017889.240/6014742166.440/2152326GNN17.610/60136242.150/6010132700.170/0n/aFILM(ours)8.7357/6014763.7560/6011311843.2420/2637777GNN-GPU8.26–/6013653.56–/6010131535.80–/3631662SetCoveringFSB6.120/606132.380/60712127.350/28318PB2.761/6023425.830/602765393.600/5913719RPB4.010/601126.360/60714210.9529/604701COMP2.760/608229.760/60930494.590/545613GNN2.731/607122.260/60688257.996/603755FILM(ours)2.1358/607315.7160/60686217.0225/604315GNN-GPU1.96–/607111.70–/60688121.18–/603755CombinatorialAuctionsFSB673.430/53471689.750/20102700.000/0n/aPB172.032/575728753.950/4515702685.230/138215RPB59.875/60603173.1711/602051946.519/212461COMP82.221/58847383.971/522672393.750/65589GNN?44.0715/60331625.231/505992330.950/10687FiLM?(ours)52.9637/55376131.4547/542641823.2912/151201GNN-GPU?31.71–/6033163.96–/605991158.59–/27685MaximumIndependentSetcheapestcomputationmodelofHyperSVM/HyperSVM-FiLMdonotprovideanyruntimeadvantagesoverFiLM.Weachieveupto26%reductiononmediuminstancesandupto8%reductiononbiginstancesinoverallsolverrunningtimecomparedtothenext-bestbranchingstrategy,includingbothlearnedandclassicalstrategies.Finally,wenotethatthemajorityof“big"problemsinSetCoveringandMaximumIndependentSetarenotsolvedbyanyofthebranchingstrategies.Table5:Meanoptimalitygap(lowerthebetter)ofcommonlyunsolved“big"instances(numberofsuchinstancesinbrackets).setcover(33)indset(39)FSB0.17090.0755PB0.07130.0298RPB0.06280.0252COMP0.07400.0252GNN0.10390.0341FiLM0.05970.0187Therefore,weprovideacomparisonofoptimalitygapofun-solvedinstancesattimeoutinTable5.ItisevidentthatFiLMmodelsareabletoclosealargeroptimalitygapthantheotherbranchingstrategies.IntheabsenceofsigniﬁcantdifferenceinKDandKD&ATmodel’sTop-1accuracy(seeTable2),anaturalquestionthentoaskis:whatcanbeapracticallyusefulchoice?Toanswerthisquestion,wetestedtheeffectofeachtrainingprotocolontheﬁnalB&Bperformance.AlthoughthedetaileddiscussionisinthesupplementweconcludethatintheabsenceofsigniﬁcantdifferenceinthetestaccuracyofmodelsbetweenKDandAT&KD,apreferenceshouldbemadeforKDmodelstoattainbettergeneralization.Limitations.Wewouldliketopointoutsomelimitationsofourwork.First,giventheNP-HardnatureofMILPsolving,itisfairlytimeconsumingtoevaluateperformanceofthetrainedmodelsontheinstancesbiggerthanconsideredforthiswork.Onecanconsidertheprimal-dualboundgapafteratimelimitasanevaluationmetricforthebiggerinstances,butthisismisalignedwiththesolving8timeobjective.Second,wehaveusedTop-1accuracyonthetestsetasaproxyforthenumberofnodes,butthereisaninherentdistributionshiftbecauseofthesequentialnatureofB&Bthatleadstoout-of-distributionobservations.Third,generalizationtolargerinstancesisacentralprobleminthedesignofbranchingstrategies.Severaltechniquesdiscussedinthisworkformonlyapartofthesolutiontothisproblem.OntheMaximumIndependentSetproblem,weoriginallynoticedapoorgeneralizationcapability,whichweaddressedbycross-validationusingasmallvalidationset.Infuturework,weplantoperformanextensivestudyoftheeffectofarchitectureandtrainingprotocolsongeneralizationperformance.Anotherexperimentworthconductingwouldbetotrainonlargerinstancesthan“small"problemsusedinthework,inordertogetabetterviewofhowandtowhichpointthedifferentmodelsareabletogeneralize.However,notonlyisthisatime-consumingprocess,butthereisalsoanupperlimitonthesizeoftheproblemsonwhichitisreasonabletoconductexperiments,simplyduetohardwareconstraints.Finally,althoughweshowedtheefﬁcacyofourmodelsonabroadclassofMILPs,theremaybeotherproblemclassesforwhichourmodelsmightnotresultinasubstantialruntimeimprovements.6ConclusionAsmoreoperationsresearchandintegerprogrammingtoolsstarttoincludeMLanddeeplearningmodules,itisnecessarytobemindfulofthepracticalbottlenecksfacedinthatdomain.Tothisend,wecombinetheexpressivepowerofGNNswiththecomputationaladvantagesofMLPstoyieldnovelhybridmodelsforlearningtobranch,acentralcomponentinMILPsolving.WeintegratevarioustrainingprotocolsthataugmentthebasicMLPsandhelpbridgetheaccuracygapwithmoreexpensivemodels.ThiscompetitiveaccuracytranslatesintosavingsintimeandnodeswhenusedinMILPsolving,ascomparedtobothdefault,expert-designedbranchingstrategiesandexpensiveGNNmodels,thusobtainingthe“bestofbothworlds"intermsofthetime-accuracytrade-off.Morebroadly,ourphilosophyrevolvesaroundunderstandingtheintricaciesandpracticalconstraintsofMILPsolvingandcarefullyadaptingdeeplearningtechniquestherein.Webelievethatthisintegrativeapproachiscrucialtotheadoptionofstatisticallearninginexactoptimizationsolvers.BroaderImpactThispaperestablishesabridgebetweentheworkdoneinMLinthelastyearsonlearningtobranchandthetraditionalMILPsolversusedtoroutinelysolvethousandsofoptimizationapplicationsinenergy,telecommunications,logistics,biology,justtomentionafew.TheMILPsolversareexecutedonCPU-onlymachinesandthetechnicalchallengeofusingGPU-basedalgorithmictechniquestohybridizethemhadbeenneglectedthusfar.Admittedly,suchachallengewasnoturgentwhenthelearningtobranchliteraturewasinitsearlystages.ThatsituationhaschangeddrasticallywiththeGNNimplementationin[17],theﬁrstapproachtoshowsigniﬁcantbeneﬁtwithrespecttothedefaultversionofastate-of-the-artMILPsolverlikeSCIP.Forthisreason,thecurrentpapercomesattheduetimefortheliteratureintheﬁeldandaddressesthechallenge,fortheﬁrsttime,inasophisticated,yetrelativelysimpleway.Thus,ourworkprovidestheﬁrstviablewayforcommercialandnoncommercialMILPsolverdeveloperstoimplementandintegrateaML-based“learningtobranch"frameworkandforhundredsofthousandsofusersandpractitionerstouseit.Inanevenbroadersense,thefactthatwewereabletoapproximatetheperformanceofGPU-basedmodelswithasophisticatedintegrationofCPU-basedtechniquesisconsistentwith,forexample,Hintonetal.[25],andwidensthespaceofproblemstowhichMLtechniquescanbesuccessfullyapplied.AcknowledgmentsandDisclosureofFundingTheauthorsaregratefultoCIFARandIVADOforfundingandComputeCanadaforcomputingresources.WewouldfurtherliketoacknowledgetheimportantroleplayedbyourcolleaguesatMilaandCERCthroughbuildingafunlearningenvironment.WewouldalsoliketothankFelipeSeranoandBenjaminMüllerfortheirtechnicalhelpwithSCIPandinsightfuldiscussionsonbranchinginMILPs.PGwantstothankGiuliaZarpellon,DidierChételat,AntoineProuvost,KarstenRoth,David9Yu-TungHui,TristanDeleu,MaksymKorablyov,andAlexLambforenlighteningdiscussionsondeeplearningandintegerprogramming.References[1]JawadAbrache,TeodorGabrielCrainic,MichelGendreau,andMoniaRekik.Combinatorialauctions.AnnalsofOperationsResearch,153(1):131–164,2007.[2]TobiasAchterberg.ConstraintIntegerProgramming.Doctoralthesis,TechnischeUniversitätBerlin,FakultätII-MathematikundNaturwissenschaften,Berlin,2007.URLhttp://dx.doi.org/10.14279/depositonce-1634.[3]TobiasAchterbergandTimoBerthold.Hybridbranching.InIntegrationofAIandORTechniquesinConstraintProgrammingforCombinatorialOptimizationProblems,2009.[4]TobiasAchterberg,ThorstenKoch,andAlexanderMartin.Branchingrulesrevisited.OperationsResearchLetters,33(1):42–54,2005.ISSN0167-6377.doi:https://doi.org/10.1016/j.orl.2004.04.002.URLhttp://www.sciencedirect.com/science/article/pii/S0167637704000501.[5]AlejandroMarcosAlvarez,QuentinLouveaux,andLouisWehenkel.Amachinelearning-basedapproxi-mationofstrongbranching.INFORMSJournalonComputing,29(1):185–195,2017.[6]DavidApplegate,RobertBixby,VašekChvátal,andWilliamCook.FindingcutsintheTSP.Technicalreport,DIMACS,1995.[7]YoshuaBengio,AndreaLodi,andAntoineProuvost.Machinelearningforcombinatorialoptimization:amethodologicaltourd’horizon.arXiv:1811.06128,2018.[8]MariuszBojarski,DavideDelTesta,DanielDworakowski,BernhardFirner,BeatFlepp,PrasoonGoyal,LawrenceDJackel,MathewMonfort,UrsMuller,JiakaiZhang,etal.Endtoendlearningforself-drivingcars.arXivpreprintarXiv:1604.07316,2016.[9]ChristopherJCBurges.Fromranknettolambdaranktolambdamart:Anoverview.Learning,11(23-581):81,2010.[10]BhuwanDhingra,HanxiaoLiu,ZhilinYang,WilliamCohen,andRuslanSalakhutdinov.Gated-attentionreadersfortextcomprehension.InProceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages1832–1846,2017.[11]VincentDumoulin,JonathonShlens,andManjunathKudlur.Alearnedrepresentationforartisticstyle.arXivpreprintarXiv:1610.07629,2016.[12]VincentDumoulin,EthanPerez,NathanSchucher,FlorianStrub,HarmdeVries,AaronCourville,andYoshuaBengio.Feature-wisetransformations.Distill,3(7):e11,2018.[13]DGElson.Sitelocationviamixed-integerprogramming.JournaloftheOperationalResearchSociety,23(1):31–43,1972.[14]FrankFinn.Integerprogramming,linearprogrammingandcapitalbudgeting.Abacus,9:180–192,072005.doi:10.1111/j.1467-6281.1973.tb00186.x.[15]ChristodoulosAFloudasandXiaoxiaLin.Mixedintegerlinearprogramminginprocessscheduling:Modeling,algorithms,andapplications.AnnalsofOperationsResearch,139(1):131–162,2005.[16]J.J.ThomsonF.R.S.XXIV.onthestructureoftheatom:aninvestigationofthestabilityandperiodsofoscillationofanumberofcorpusclesarrangedatequalintervalsaroundthecircumferenceofacircle;withapplicationoftheresultstothetheoryofatomicstructure.TheLondon,Edinburgh,andDublinPhilosophicalMagazineandJournalofScience,7(39):237–265,1904.doi:10.1080/14786440409463107.URLhttps://doi.org/10.1080/14786440409463107.[17]MaximeGasse,DidierChételat,NicolaFerroni,LaurentCharlin,andAndreaLodi.Exactcombinatorialoptimizationwithgraphconvolutionalneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pages15554–15566,2019.[18]PierreGeurts,DamienErnst,andLouisWehenkel.Extremelyrandomizedtrees.Machinelearning,63(1):3–42,2006.[19]TobiasGlasmachers.Limitsofend-to-endlearning.arXivpreprintarXiv:1704.08305,2017.10[20]AmbrosGleixner,MichaelBastubbe,LeonEiﬂer,TristanGally,GeraldGamrath,RobertLionGottwald,GregorHendel,ChristopherHojny,ThorstenKoch,MarcoE.Lübbecke,StephenJ.Maher,MatthiasMiltenberger,BenjaminMüller,MarcE.Pfetsch,ChristianPuchert,DanielRehfeldt,FranziskaSchlösser,ChristophSchubert,FelipeSerrano,YujiShinano,JanMerlinViernickel,MatthiasWalter,FabianWegschei-der,JonasT.Witt,andJakobWitzig.TheSCIPOptimizationSuite6.0.Technicalreport,OptimizationOnline,July2018.URLhttp://www.optimization-online.org/DB_HTML/2018/07/6692.html.[21]IanGoodfellow,YoshuaBengio,andAaronCourville.DeepLearning.MITPress,2016.http://www.deeplearningbook.org.[22]DavidHa,AndrewDai,andQuocVLe.Hypernetworks.arXivpreprintarXiv:1609.09106,2016.[23]HeHe,HalIIIDaumé,andJasonEisner.Learningtosearchinbranch-and-boundalgorithms.InAdvancesinNeuralInformationProcessingSystems27,pages3293–3301,2014.[24]MartiA.Hearst.Supportvectormachines.IEEEIntelligentSystems,13(4):18–28,July1998.ISSN1541-1672.doi:10.1109/5254.708428.URLhttps://doi.org/10.1109/5254.708428.[25]GeoffreyHinton,OriolVinyals,andJeffDean.Distillingtheknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531,2015.[26]EliasB.Khalil,PierreLeBodic,LeSong,GeorgeNemhauser,andBistraDilkina.Learningtobranchinmixedintegerprogramming.InProceedingsoftheThirtiethAAAIConferenceonArtiﬁcialIntelligence,pages724–731,2016.[27]TaesupKim,InchulSong,andYoshuaBengio.Dynamiclayernormalizationforadaptiveneuralacousticmodelinginspeechrecognition.Proc.Interspeech2017,pages2411–2415,2017.[28]SolomonKullbackandRichardALeibler.Oninformationandsufﬁciency.Theannalsofmathematicalstatistics,22(1):79–86,1951.[29]A.H.LandandA.G.Doig.Anautomaticmethodofsolvingdiscreteprogrammingproblems.Economet-rica,28(3):pp.497–520,1960.[30]LukasLiebelandMarcoKörner.Auxiliarytasksinmulti-tasklearning.arXivpreprintarXiv:1805.06334,2018.[31]JeffLinderothandMartinSavelsbergh.Acomputationalstudyofsearchstrategiesformixedintegerprogramming.INFORMSJournalonComputing,11:173–187,051999.doi:10.1287/ijoc.11.2.173.[32]WeiyangLiu,RongmeiLin,ZhenLiu,LixinLiu,ZhidingYu,BoDai,andLeSong.Learningtowardsminimumhypersphericalenergy.InAdvancesinneuralinformationprocessingsystems,pages6222–6233,2018.[33]AndreaLodiandGiuliaZarpellon.Onlearningandbranching:asurvey.TOP,25:207–236,2017.[34]AndrewYNg.Featureselection,l1vs.l2regularization,androtationalinvariance.InProceedingsofthetwenty-ﬁrstinternationalconferenceonMachinelearning,page78,2004.[35]ChristosH.PapadimitriouandKennethSteiglitz.CombinatorialOptimization:AlgorithmsandComplexity.Prentice-Hall,Inc.,USA,1982.ISBN0131524623.[36]EthanPerez,FlorianStrub,HarmDeVries,VincentDumoulin,andAaronCourville.FiLM:Visualreasoningwithageneralconditioninglayer.InThirty-SecondAAAIConferenceonArtiﬁcialIntelligence,2018.[37]RupeshKumarSrivastava,KlausGreff,andJürgenSchmidhuber.Highwaynetworks.arXivpreprintarXiv:1505.00387,2015.[38]LaurenceA.Wolsey.IntegerProgramming.Wiley-Blackwell,1988.[39]GiuliaZarpellon,JasonJo,AndreaLodi,andYoshuaBengio.Parameterizingbranch-and-boundsearchtreestolearnbranchingpolicies.arXivpreprintarXiv:2002.05120,2020.11Supplement:HybridModelsforLearningtoBranchPrateekGupta∗UniversityofOxfordTheAlanTuringInstituteMaximeGasseMila,PolytechniqueMontréalEliasB.KhalilUniversityofTorontoM.PawanKumarUniversityofOxfordAndreaLodiCERC,PolytechniqueMontréalYoshuaBengioMila,UniversitédeMontréal1InefﬁciencyinusingGNNsforsolvingMILPsinparallelInthissection,wearguethattheGNNarchitectureloosesitsadvantagesinthefaceofsolvingmultipleMILPsatthesametime.Intheapplicationslikemulti-objectiveoptimization[4],wheremultipleMILPsaresolvedinparallel,aGNNforeachMILPneedstobeinitializedontheGPUbecauseofthesequentiallyasynchronousnatureofsolvingMILPs.NotonlyistherealimittothenumberofsuchGNNsthatcanﬁtonasingleGPUbecauseofmemoryconstraints,butalsoseveralGNNsonasingleGPUresultsinaninefﬁcientGPUutilization.Onecan,forinstance,trytotimemultipleMILPssuchthatthereisaneedforasingleforwardevaluationonaGPU,but,inourknowledge,ithasnotbeendoneanditresultsinfrequentinterruptionsinthesolvingprocedure.Analternative,muchsimpler,methodistopackmultipleGNNsonasingleGPUsuchthateachGNNisdedicatedtosolvingoneMILP.Forexample,wewereabletoput25GNNsonTeslaV10032GBGPU.Figure1showstheinefﬁcientutilizationofGPUswhenmultipleGNNsarepackedonasingleGPU.0.200.250.300.350.400.450.50pack-CPU12491625Size 0.000.010.020.030.040.05pack-GPUbatchTime (s)Figure1:PackingseveralGNNstogetheronaGPUkeepsitunderutilized.“Size”isthenumberofinputsbatchedtogether(unrealisticscenario)ornumberofinputssimultaneouslyputonaGPUseparately.∗TheworkwasdoneduringaninternshipatMilaandCERC.Correspondenceto:<pgupta@robots.ox.ac.uk>34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada.arXiv:2006.15212v3  [cs.LG]  23 Oct 20202InputFeaturesWeusethefeaturesthatwereusedbyGasseetal.[1]andKhaliletal.[3].ThelistoffeaturesinGaredescribedintheTable1.Thereareatotalof92featuresthatweuseforXasdescribedintheTable2.WefollowthepreprocessingprocedureasdescribedinGasseetal.[1].Table1:FeaturesofG.Itisconstructedasavariable-constraintbipartitegraph,wheretheverticesofthisgraphhavefeaturesasdescribedhere.ThesefeaturesaresameasusedinGasseetal.[1].TypeDescriptionVariableFeatures(Total-13)Variabletype(1)CategoricalAllowedvalues-Binary,Integer,Continuous,ImpliedIntegerNormalizedcoefﬁcient(1)RealObjectivecoefﬁcientofthevariablenormalizedbytheeuclideannormofitscoefﬁcientsintheconstraintsSpeciﬁedbounds(2)BinaryDoesthevariablehasalowerbound(upperbound)?Solutionbounds(2)BinaryIfthevariableiscurrentlyatitslowerbound(upperbound)?Solutionbractionality(1)Real∈[0,1)Fractionalpartofthevariablei.e.x−bxc,wherexisthevalueofthedecisioninvariableinthecurrentLPsolutionBasis(1)CategoricalOneof4classes-Lower(variableisatthelowerbound),Basic(variablehasavaluebetweenthebounds),Upper(variableisattheupperbound),Zero(rarecase)Reducedcost(1)RealAmountbywhichobjectivecoefﬁcientofthevariableshoulddecreasesothatthevariableassumesapositivevalueintheLPsolutionAge(1)RealNumberofLPiterationssincethelasttimethevariablewasbasicnormalizedbytotalnumberofLPiterationsSolutionvalue(1)RealValueofthevariableinthecurrentLPsolutionPrimalvalue(1)RealValueofthevariableinthecurrentbestprimalsolutionAverageprimalvalue(1)RealAveragevalueofthevariableinallofthepreviouslyobservedfeasibleprimalsolutionsConstraintFeatures(Total-5)Cossinesimilarity(1)RealCossineofanglebetweenthevectorrepresentedbyob-jectivecoefﬁcientsandthecoefﬁcientsofthisconstraintBias(1)RealRighthandsideoftheconstraintnormalizedbytheeu-clideannormoftherowcoefﬁcientsAge(1)RealNumberofiterationssincethelasttimetheconstraintwasactivenormalizedbytotalnumberofLPiterationsNormalizeddualvalue(1)RealValueofdualvariablecorrespondingtotheconstraintnormalizedbytheproductofnormsoftherowcoefﬁ-cientsandtheobjectivecoefﬁcientsBounds(1)BinaryIftheconstraintiscurrentlyatitsbounds?EdgeFeatures(Total-1)Normalizedcoefﬁcient(1)RealCoefﬁcientofthevariablenormalizedbythenormofthecoefﬁcientsofallthevariablesintheconstraintTable2:FeaturesinX,aninputtoMLP.countVariablefeaturesfromG13VariablefeaturesfromKhaliletal.[3]7223PreliminaryresultsonrunningtimesofvariousarchitecturesInordertohavearoughideaofrelativeimprovementinruntimesacrossvariousarchitectures,weconsidered20instancesineachdifﬁcultylevelforeachproblemclassandusethesolvertoobtainobservationsateachnode.Differentfunctionsarethenusedtoevaluatedecisionsateachnode,andthetotaltimetakenforallthefunctionevaluationsacrossthenodesinaninstanceisobserved.Notethatthequalityofdecisionisnotimportanthere;weareonlyinterestedintimetakenperdecision.Speciﬁcally,weconsidered5formsofarchitecturesaslistedinTable3.Tocovertheentirespectrumofstronginductivebiases,weconstructedanattentionmechanismasexplainedin4.Atthesametime,tocoverthespectrumofcheaperarchitecturesweconsidersimpledotproductastheformofpredictor.Finally,weconsiderascenariowhereweonlyconsiderMLPsaspredictorseverywhereintheB&Btree.Table3:Differentarchitecturesconsideredforpreliminrayruntimecomparison.TypeDescriptionGNNALLUseaGraphConvolutionNeuralNetwork(GNN)[1]atalltreenodesATTNALLAttentionmechanism(section4)atallnodesGNNDOTUseGNNattherootnodesandadotproductwithXtocomputescoresateverynodeATTNDOTUseattentionmechanismattherootnodeanddotproductwithXtocomputescoresateverynodeMLPALLUsea3-layermultilinearperceptronatallthenodeseasymediumhard05101520253035cauctionseasymediumhard05101520253035indseteasymediumhard020406080100setcovereasymediumhard0501001502002214facilitiesCPU PerformanceATTN ALLATTN-DOTGCNN-DOTMLP  ALL1.0Figure2:RelativetimeperformanceofvariousmethodswithrespecttoGNNALLonCPU(averagevalues).Avalueof10impliesthatthemethodis10timesfaster(onarithmeticaverage)thanGNNALLimplyingthatonecanaffordtoperform10timesworsethanGNNALLiniterativeperformancewhenusingCPUs.Notethatthisisjustaroughestimation.Figure2showstherelativeperformance(rel.toGNN)ofvariousdeeplearningarchitecturesacross4setsofproblems.ItisevidentthatMLPALLandGNNDOTarefavoredacrosstheproblemsets.Thisobservationinspiredtherangeofhybridarchitecturesthatweexploredinthepaper.Wealsoobservethatasuperiorinductivebiaslikethatofattentionmechanismandtransformers[7]hasabetterruntimeperformanceonGPUsasillustratedinFigure3,whichwesuspectismassiveparallelizationemployedinthecomputationsofattention.However,theirperformanceonCPUsisnotbetterthanGNNs.3easymediumhard0510152025303540cauctionseasymediumhard05101520253035indseteasymediumhard010203040setcovereasymediumhard010203040506070175facilitiesGPU PerformanceATTN ALLATTN-DOTGCNN-DOTMLP  ALL1.0Figure3:RelativetimeperformanceofvariousmethodswithrespecttoGNNALLonGPU(averagevalues).Avalueof10impliesthatthemethodis10timesfaster(onarithmeticaverage)thanGNNALLimplyingthatonecanaffordtoperform10timesworsethanGNNALLiniterativeperformancewhenusingCPUs.Notethatthisisjustaroughestimation.4AttentionMechanismforMILPsTocovertheentirespectrumofcomputationalcomplexityandexpressivityofinductivebias,weimplementedatransformer[7]asanarchitecturetoreplaceGNNs.Speciﬁcally,weletthevariables(constraints)attendtoallothervariables(constraints)viamulti-headedself-attentionmechanism.Finally,amodulatedattentionmechanismbetweenvariablerepresentationsasqueriesandconstraintrepresentationaskeysoutputsﬁnalvariablerepresentations,whichispassedthroughthesoftmaxlayerforclassiﬁcationobjective.Here,weusemodulationschemeasexplainedinShawetal.[6].Precisely,anedgeinvariable-constraintgraphisusedtoincrementtheattentionscorewithalearnablescalarvalue.Figure4:Multi-HeadAttentionmechanismwhereavariableorconstraintcanattendtoallothervari-ablesorconstraints.Finally,variablesareusedasaquerytoattendtoconstraints,wheretheattentionismodulatedthroughvariable-constraintfeatures.ModulationofattentionscoresfollowShawetal.[6]45DataGeneration&TrainingSpeciﬁcationsForourexperiments,weused10,000traininginstancesforthetrainingdatasetandcollected150,000observationsfromthetreenodesofthoseinstances.Inasimilarmanner,wegenerated20,000instanceseachforthevalidationandtestingset,resultingin30,000observationseachforvalidationandtestingrespectively.Weheldallthetrainingparametersﬁxedacrossthemodels.Speciﬁcally,weusedalearningrateof1e−3,trainingbatchsizeof32,alearningrateschedulertoreducelearningrateby0.2ifthereisnoimprovementinthevalidationlossfor15epochs,andanearlystoppingcriterionof30epochs.Ourepochconsistedof10Ktrainingexamplesand2Kvalidationsamples.Weuseda3layeredMLPwith256hiddenunitsineachlayer,whileweusedGNNmodelwithanembeddingsizeof64units.Weusedthisconﬁgurationacrossallthemodelsdiscussedinthemainpaper.Duetothelargesizeofinstancesincapacitatedfacilitylocation,weusedalearningrateof0.005,earlystoppingcriterionof20epochswithapatienceof10epochs.Further,forknowledgedistillationweusedT=2(temperature)andα=0.9(convexmixingofsoftandhardobjectives).ThesearetherecommendedsettingsinHintonetal.[2].Wedidahyperparametersearchforβ={0.01,0.001,0.0001}forEDandMHE.Followingarethevaluesforβthatresultedinthebestperformingmodels.Table4:BestATmodelsATCapacitatedFacilityLocationMHECombinatorialAuctionsMHESetCoveringEDMaximumIndependentSetMHEWeimplementedallthemodelsusingPyTorch[5],andranalltheCPUevaluationsonanIntel(R)Xeon(R)CPUE5-2650v4@2.20GHz.GPUevaluationsforGNNwereperformedonNVIDIA-TITANXpGPUcardwithCUDA10.1.6Depth-dependentlossweightingschemeInFigure5weplotthesortedratioofnumberofnodestotheminimumnumberofnodesobservedforaninstanceacrossalltheweightingschemes.Hereweattempttobreakdowntheperformanceofdifferentbranchingstrategieslearnedusingdifferentloss-weightingschemes.Weobservethatthesigmoidalschemeachievesthebestperformance.01020304050Instances1.001.251.501.752.002.252.502.75minimum nodes ratioconstantexponentiallinearquadraticsigmoidalFigure5:Performanceofdifferentweighingschemesacross"big"instances."Sigmoidal"evolvesslowestamongall.57ModelResultsThetablebelowisalistofallthearchitecturesalongwiththeirperformanceonthetestsets.PleasenotethattrainingwithauxiliarytaskisnotpossiblewithHyperSVMtypeofarchitecturesotheirresultsarenotreportedinthetable.Table5:Top-1accuracyofvariousmodels.CombinatorialAuctionsCapacitatedFa-cilityLocationSetCoverMaximumIndependentSetExpertGCNN46.53±0.1268.47±0.2254.82±0.1458.73±0.54ExistingmethodsExtratrees37.97±0.160.06±0.1842.66±0.2219.35±0.55LMART38.7±0.1663.28±0.0646.31±0.2850.98±0.37SVMRank39.14±0.1263.47±0.0646.23±0.1149.29±0.23SimpleNetworkMLP43.53±0.1365.26±0.1649.53±0.0453.06±0.03ConcatenateCONCAT(Pre)44.16±0.0365.5±0.149.98±0.1852.85+-0.34CONCAT(e2e)44.09±0.0866.59±0.0450.17±0.0453.23±0.41CONCAT(e2e&KD)44.19±0.0566.53±0.1350.11±0.0953.66+-0.32ModulateFiLM(Pre)44.12±0.0965.78±0.0650.0±0.0953.16±0.51FiLM(e2e)44.31±0.0866.33±0.3350.16±0.0553.23±0.58FiLM(e2e&KD)44.1±0.0966.6±0.2150.31±0.1953.08±0.3HyperSVMHyperSVM(e2e)43.19±0.0266.05±0.0649.78±0.2350.04±0.31HyperSVM(e2e&KD)42.55±0.0366.07±0.0549.53±0.1349.34+-0.43HyperSVM-FiLMHyperSVM-FiLM(e2e)43.64±0.1865.54±0.3249.81±0.2750.17±0.58HyperSVM-FiLM(e2e&KD)43.28±0.4865.52±0.3449.73±0.0549.73+-0.39FiLM(e2e&KD&AT)44.56±0.1366.85±0.2850.37±0.0353.68±0.238OverﬁttinginmaximumindependentsetTable6showsthattheFiLMmodelsoverﬁtonsmallinstancesofmaximumindependentset.Toaddressthisproblem,weusedaminidatasetof2000observationsobtainedbyrunningdatacollectiononmediuminstancesofmaximumindependentset.Further,weregularizedtheFiLMparametersoftheFiLMmodeltoyieldmuchsimplermodelsbasedontheperformanceonthismini-dataset,whichisnottooexpensivetoobtainowingtothesizeofobservations.TheresultsoftheregularizedmodelsareinTable7Forafaircomparison,weregularizedGNNandusedthebestperformingmodeltoreportevaluationresultsinthemainpaper.Table6:FiLMmodelsformaximumindependentsetoverﬁtsonsmallinstancessmallmediumbigTimeWinsNodesTimeWinsNodesTimeWinsNodesFiLM52.9639/554921515.199/1728042700.020/0nanGNN-CPU44.0721/60432371.8136/405581981.4310/106334GNN-GPU31.700/59432264.010/435581772.120/136313Table7:Top-1accuracyofregularizedmodelson2000observationsfrommediumrandominstancesofmaximumindependentset.weightdecayFiLMGNN1.055.15±0.0731.6±6.630.156.13±0.3237.23±0.920.0153.25±0.9526.8±16.710.019.18±4.2434.08±4.869PerformanceofHyperSVMarchitecturesHyperSVMarchitecturesarethecheapestincomputation.Inthissectionweaskifwearewillingtolosemachinelearningaccuracyby1-2%inHyperSVMarchitecture,canwestillgetfasterrunningtimeswithHyperSVMtypeofarchitectures?Table8comparessolverperformancebyusingHyperSVMarchitectureandFiLMarchitecture.WeobservethatHyperSVMtypesofarchitecturesloosetheirabilitytogeneralizeonlargerscaleinstances.Table8:FiLMarchitecturegeneralizestolargerinstancesbetterthantheHyperSVMtypesofarchitectures.WecomparetheperformanceofthebestFiLMarchitecturefromtheTable5withthebestHyperSVMarchitectureinthesametable.smallmediumbigTimeWinsNodesTimeWinsNodesTimeWinsNodesFiLM24.6753/60109136.4251/60336531.7046/57345HyperSVM27.267/60110158.979/60345614.3611/57346MLP27.614/60114156.3011/60347595.319/56334(a)CapacitatedFacilityLocationsmallmediumbigTimeWinsNodesTimeWinsNodesTimeWinsNodesFiLM8.7359/6014763.7560/6021691843.2424/2638530HyperSVM-FiLM9.731/6014872.530/6022172061.561/2247277MLP9.980/6015777.480/6022991984.261/2440188(b)SetCover10ScalingtotwicethesizeofBiginstancesInthissectionweinvestigatethegeneralizationpowerofFiLMmodelstrainedondatasetobtainedfromsmallinstances.Speciﬁcally,wegenerate20randominstancesofsizedoublethatofbiginstances,andusethetrainedmodelsofFiLMandGNNtocomparetheirperformanceagainstRPB.Weusethetimelimitof7200seconds,i.e.2hourstoaccountforlongersolvingrunningtimesasonescalesouttobiggerinstances.Weobservethatthepowertogeneralizeislargelydependentontheproblemfamily.Forexample,FiLMmodelscanstilloutperformotherstrategiesonscalingoutoncapacitatedfacilitylocationproblems.However,wefoundthatRPBremainscompetitiveonsetcoverproblems.WenotethatthisshortcomingoftheFiLMmodelscanbeovercomevialargersizeofhiddenlayersandtrainingonslightlylargerinstancesthanwhathasbeenusedinthemainpaper.Table9showstheseresults.Table9:Performanceofbranchingstrategiesontwicethesizeofbiggestinstances(2×Big)consideredinthemainpaper.20"Bigger"instancesweresolvedusing3seedseachresultinginatotalof60runs.WeseethatFiLMmodelsstillremaincompetitive,anditishighlydependentonthefamilyofproblem.facilitiessetcoverModelTimeWinsNodesTimeWinsNodesRPB7200.140/0n/a6346.179/12135132GNN7111.831/5n/a7200.250/0n/aFILM(ours)7052.274/4n/a6508.783/10107187GNN-GPU6625.55–/13n/a6008.14–/129390911EffectofdifferenttrainingprotocolsonB&BperformanceTable10showstheperformanceofdifferenttrainingprotocolsonB&Bperformance.Althoughthemodelstrainedwithauxiliarytasks(AT)andknowledgedistillation(KD)areaclearwinnerupuntilproblemsetsofsizemedium,thereisatiebetweenthemodelstrainedwithKDandthosetrainedwithKD&AT.However,itisworthnotingthatthedifferenceinB&Bperformanceacrossdifferenttrainingprotocolsisnothuge,andsuchperformanceevaluationcanbereadfromthetestaccuracyoftrainedmodels.Forexample,asevidentinTable5,differenceintestaccuracyofFiLM(e2e&KD)7Table10:Effectoftrainingprotocolsontheperformanceofbranchingstrategies.Wereportgeometricmeanofsolvingtimes,numberoftimesamethodwon(insolvingtime)overtotalﬁnishedruns,andgeometricmeanofnumberofnodes.Refertosection5(main)formoredetails.Thebestperformingresultsareinbold.∗Modelswereregularizedtopreventoverﬁttingonsmallinstances.SmallMediumBigModelTimeWinsNodesTimeWinsNodesTimeWinsNodese2e25.0522/60114154.124/60340550.0315/57339e2e+KD28.002/60111143.1218/60343507.5026/57325e2e+KD+AT24.6736/60109136.4238/60336531.7016/57345CapacitatedFacilityLocatione2e9.701/6015271.181/6021861869.574/2540341e2e+KD9.810/6014670.884/6021731842.2915/2540437e2e+KD+AT8.7359/6014763.7555/6021691843.248/2640881SetCoveringe2e2.451/607217.595/60702225.8817/608939e2e+KD2.350/607317.592/60720241.957/598846e2e+KD+AT2.1359/607315.7153/60686217.0236/608711CombinatorialAuctionse2e205.632/545591103.471/2911372457.941/42268e2e+KD333.521/52486926.121/416042503.650/71953e2e+KD+AT52.9654/55410131.4554/543311823.2914/153049MaximumIndependentSet∗andFiLM(e2e&KD&AT)isnotsigniﬁcantforproblemsets-CapacitatedFacilityLocationandSetCovering,butitssigniﬁcantenoughforproblemsets-CombinatorialAuctionsandMaximumIndependentSet.Giventhattheinferencecostisindependentoftrainingprotocols,toattainbettergeneralizationperformance,werecommendFiLM(e2e&KD&AT)onlywhenthesemodelshavesigniﬁcantlybetteraccuracythanFiLM(e2e&KD).References[1]MaximeGasse,DidierChételat,NicolaFerroni,LaurentCharlin,andAndreaLodi.Exactcombinatorialoptimizationwithgraphconvolutionalneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pages15554–15566,2019.[2]GeoffreyHinton,OriolVinyals,andJeffDean.Distillingtheknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531,2015.[3]EliasB.Khalil,PierreLeBodic,LeSong,GeorgeNemhauser,andBistraDilkina.Learningtobranchinmixedintegerprogramming.InProceedingsoftheThirtiethAAAIConferenceonArtiﬁcialIntelligence,pages724–731,2016.[4]GokhanKirlikandSerpilSayın.Anewalgorithmforgeneratingallnondominatedsolutionsofmultiobjectivediscreteoptimizationproblems.EuropeanJournalofOperationalResearch,232(3):479–488,2014.[5]AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.Automaticdifferentiationinpytorch.2017.[6]PeterShaw,JakobUszkoreit,andAshishVaswani.Self-attentionwithrelativepositionrepresentations.arXivpreprintarXiv:1803.02155,2018.[7]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinneuralinformationprocessingsystems,pages5998–6008,2017.8