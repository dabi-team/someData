From block-Toeplitz matrices to differential equations on graphs: towards a
general theory for scalable masked Transformers

2
2
0
2

l
u
J

2
2

]

G
L
.
s
c
[

7
v
9
9
9
7
0
.
7
0
1
2
:
v
i
X
r
a

Krzysztof Choromanski * 1 2 Han Lin * 2 Haoxian Chen * 2 Tianyi Zhang 2 Arijit Sehanobish 3
Valerii Likhosherstov 4 Jack Parker-Holder 5 Tamas Sarlos 6 Adrian Weller 4 7 Thomas Weingarten 8

Abstract

In this paper we provide, to the best of our knowl-
edge, the ﬁrst comprehensive approach for in-
corporating various masking mechanisms into
Transformers architectures in a scalable way. We
show that recent results on linear causal attention
(Choromanski et al., 2021) and log-linear RPE-
attention (Luo et al., 2021) are special cases of this
general mechanism. However by casting the prob-
lem as a topological (graph-based) modulation of
unmasked attention, we obtain several results un-
known before, including efﬁcient d-dimensional
RPE-masking and graph-kernel masking. We
leverage many mathematical techniques ranging
from spectral analysis through dynamic program-
ming and random walks to new algorithms for
solving Markov processes on graphs. We provide
a corresponding empirical evaluation.

1. Introduction & Related Work

Transformers (Vaswani et al., 2017; Brown et al., 2020; De-
vlin et al., 2019) have revolutionized machine learning by
reintroducing an attention mechanism explicitly modeling
complicated relationships between elementary ingredients
of the ML models’ inputs, e.g. words for text data, or
patches/pixels for the image data (Han et al., 2020; Doso-
vitskiy et al., 2021). Crucially, attention quantiﬁes these
relationships via dynamic weights that depend on the input
data. This architectural solution is the strength and at the
same time the weakness of Transformer models. An atten-
tion matrix scales quadratically in the length of the input
sequence, making corresponding computations prohibitively
expensive for longer inputs.

*Equal contribution

1Google Brain Robotics 2Columbia
University 3Independent Researcher 4University of Cambridge
5University of Oxford 6Google Research 7The Alan Turing In-
stitute 8Google. Correspondence to: Krzysztof Choromanski
<kchoro@google.com>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Several solutions were proposed to address this limitation.
Local attention (Vaswani et al., 2021; Parmar et al., 2019)
explicitly narrows down the attention context to a ﬁxed-size
window, effectively zeroing out most attention weights. In
applications where long-range attention is crucial (e.g. pro-
tein modeling), other techniques were introduced. These
include: (1) pooling mechanisms compressing sequences to
shorter-ones agglomerating multiple-tokens signal (Avsec
et al., 2021; Dai et al., 2020), (2) hashing/clustering methods
sparsifying attention by giving up attention modeling for to-
kens from different learnable hash-buckets/clusters (Kitaev
et al., 2020; Roy et al., 2021), (3) low-rank/kernel-based
methods decomposing the attention matrix (Choromanski
et al., 2020; 2021; Katharopoulos et al., 2020; Peng et al.,
2021; Xiong et al., 2021) and other (Qin et al., 2022).

Masking is a powerful mechanism altering the attention
matrix by incorporating structural inductive bias. Flagship
examples include (1) causal attention, applied in generative
Transformers (Yang et al., 2019), where the arrow of time
induces token-ordering with tokens not attending to their
successors in the sequences, (2) relative positional encoding
(RPE, Shaw et al., 2018) reducing interactions between dis-
tant tokens (but via a much more general mechanism than
local attention) and (3) graph attention incorporating topo-
logical signal from the graph (Ying et al., 2021b; Velickovic
et al., 2018). RPE-mechanisms were shown to signiﬁcantly
improve speech models (Pham et al., 2020; Zhou et al.,
2019) and masks obtained from shortest-path length matri-
ces were recently demonstrated to close the gap between the
best customized graph neural networks models and Trans-
formers (Ying et al., 2021a). Straightforward application
of the masking mechanism requires materialization of the
attention matrix and consequently - impractical quadratic
time complexity for long input sequences (or large graphs).

In this paper we aim to answer the following question: Un-
der which conditions can masking be incorporated into at-
tention mechanisms in a scalable way, i.e. in sub-quadratic
time complexity in the number of input tokens?

So far this question was answered only partially. Causality
was incorporated in linear time into linear low-rank atten-
tion via the so-called preﬁx sum mechanism by Choroman-

 
 
 
 
 
 
Towards a general theory for scalable masked Transformers

Figure 1. RPEs & Beyond: The (i, j)-entry of the regular RPE-mask is a (learnable) function f of the distance i − j between the ith and
jth token in the input sequence that can be interpreted as a 1d-grid, thus it has the so-called Toeplitz structure (ﬁrst graph and colored-matrix
in the ﬁgure). The proposed d-dimensional RPE acts on the d-dimensional grid input with the length of the shortest path d(i, j) between
node i and j in the gird replacing expression i − j in the corresponding mask (d = 2 includes image input and d = 3, video input, see:
second graph/matrix and third graph/matrix respectively). The corresponding mask is no longer Toeplitz, but is d-level block-Toeplitz.
Interestingly, all these matrix classes support fast matrix-vector multiplication (via Fast Fourier Transform) and thus, based on our ﬁrst
result, corresponding masked low-rank attention can be performed in sub-quadratic time (see Sec. 3.2).

ski et al. (2021). The same was proven recently for the
special class of stochastic RPEs (Liutkus et al., 2021). Even
more recently, a log-linear algorithm (applying Fast Fourier
Transform) for incorporating general RPEs into low-rank
attention was given by Luo et al. (2021). All these results
leverage low-rank attention since so far that was the only
known scalable mechanism which can approximate in par-
ticular regular dense softmax attention. Hence, the starting
point of our analysis is also a low-rank attention model.

Our contributions in this paper are as follows:

1. We answer the above question in Sec. 3 by providing
a surprisingly simple characterization of the efﬁcient
masking mechanisms: as long as the masking-matrix
(element-wise multiplied with the regular attention ma-
trix) supports sub-quadratic matrix-vector multiplica-
tion, the corresponding mechanism can be incorpo-
rated into low-rank attention in sub-quadratic time.
Interestingly, as we explain later, this result includes
all mentioned partial results as special cases.

2. We present multiple consequences, leading in particu-
lar to novel scalable d-dimensional RPE mechanisms
that can be applied in image and video processing (see
Fig. 1 and Sec. 3.2), efﬁcient implementations for the
low-rank attention with padding and packing mech-
anisms (in common practical use for regular Trans-
formers) (Sec. 3), and new scalable graph-based atten-
tion masking applying shortest-path signal and graph-
diffusion kernels (Sec. 3.3, Sec. 4).

3. Using our developed theory, we introduce a new
masked-attention ML-model called graph kernel at-
tention Transformer (GKAT, Sec. 4.2), and conduct
comprehensive comparisons against nine other SOTA
graph neural networks (GNNs) in Sec. 5.

We cast the masking problem as a topological (graph-based)
modulation of unmasked attention, and leverage many math-
ematical techniques ranging from spectral analysis through
dynamic programming on trees and random walks to new
algorithms for solving Markov processes on graphs. The
proofs of all theoretical results are given in the Appendix.

2. Preliminaries

We introduce notation used throughout the paper.

Denote by L the number of input tokens. The attention
used in a regular Transformer linearly projects their repre-
sentations into three learnable matrices Q, K ∈ RL×dQK ,
V ∈ RL×d called queries, keys and values respectively.
Deﬁnition 2.1 (general masked attention). General masked
softmax attention is of the following form, where N ∈
RL×L is the logits-mask, and A ∈ RL×L is the so-called
masked attention matrix (MAM):

AttSM(Q, K, V, N) = D−1AV,
A = exp(N + QK(cid:62)/(cid:112)dQK), D = diag(A1L).

(1)

Here exp(·) is applied element-wise, 1L is the all-ones vec-
tor of length L, and diag(·) is a diagonal matrix with the
input vector as the diagonal. The time complexity of com-
puting (1) is O(L2d). The above is a special instantiation
of the general masked kernel attention which is deﬁned as:

AttK(Q, K, V, M) = D−1AV,
A = M (cid:12) K(Q, K), D = diag(A1L),

(2)

where (cid:12) denotes the element-wise (Hadamard) matrix prod-
uct, K : Rd×Rd → R is some kernel function and K(Q, K)
is a kernel matrix deﬁned as: K(Q, K)i,j = K(q(cid:62)
j ) for
the ith row qi of Q and the jth row kj of K respectively. We

i , k(cid:62)

0123450123456789101101234567L1-dist:  f(0)f(5)……Towards a general theory for scalable masked Transformers

call A(cid:48) = K(Q, K) the unmasked attention matrix (UAM).
The softmax attention can be obtained from the kernel one
by taking: K(x, y) def= exp( x(cid:62)y√
) (the so-called softmax
dQK

kernel) and M def= exp(N) (element-wise exponentiation).

Low-rank attention methods provide (approximate) atten-
tion computation in time linear in the length L of the in-
put sequence if no masking is applied (i.e. M is all-ones)
and kernel K admits (at least in expectation) a dot-product
decomposition, i.e. K(x, y) = E[φ(x)(cid:62)φ(y)] for some
(usually randomized) mapping: φ : RdQK → Rm (and
some m > 0). Such a decomposition (in fact more than
one!) exists in particular for the softmax kernel used in most
applications of regular Transformers. We call φ(u) a (ran-
dom) feature map (RFM) for u ∈ Rd. For Q(cid:48), K(cid:48) ∈ RL×m
with rows given as φ(q(cid:62)
i )(cid:62) respectively, RFM-
i )(cid:62) and φ(k(cid:62)
based kernel linearization leads directly to the efﬁcient un-
masked attention mechanism of the form:

(cid:91)AttK(Q, K, V) = (cid:98)D−1(Q(cid:48)((K(cid:48))(cid:62)V)),
(cid:98)D = diag(Q(cid:48)((K(cid:48))(cid:62)1L)).

(3)

Here (cid:91)AttK stands for the approximate attention and brackets
indicate the order of computations. It is easy to see that such
a mechanism is characterized by time complexity O(Lmd)
as opposed to O(L2d) for regular attention. If m (cid:28) L,
computational gains are obtained.

3. Fast matrix-vector product is all you need

Our ﬁrst result, a natural extension of the theoretical analysis
by Luo et al. (2021), shows that as long as mask M ∈ RL×L
supports sub-quadratic matrix-vector multiplication, it can
be incorporated into low-rank attention in sub-quadratic
time. This is explained in Lemma 3.1.

Lemma 3.1 (Tractable Mask Lemma). Assume that mask
M ∈ RL×L from Deﬁnition 2.1 supports matrix-vector
multiplication in time TM(L). Then the general masked
kernel attention algorithm with mask M can be implemented
in time O((TM(L) + L)md).
The algorithm is given in the algorithmic box 1. We analyze
it in the Appendix, but the intuition is that, in the unmasked
low-rank setting, attention embeddings could be obtained
from the action of φ(qi) on the ﬁxed (token-independent)
matrix of shape Rm×d summarizing all the tokens, whereas
in the masked case the matrix depends on each token, but
can be obtained from mask-vector products.

Causal attention: Note that the preﬁx-sum algorithm
from (Choromanski et al., 2021) is a special instantiation
of Algorithm 1. Indeed, causality is encoded by the lower-
triangular mask M such that: Mi,j = 1 for j ≤ i and
Mi,j = 0 otherwise. Every product Mx is trivially a vector
of preﬁx sums: x1 + ... + xi for i = 1, ..., L and thus can
be computed in time O(L).

Packing & Padding: Both masking mechanisms are stan-
dard Transformers’ techniques used to optimize attention
computation on TPUs. The former packs multiple sequences
in one super-sequence. Mask is used here to prevent cross-
sequence attention. The latter adds fake tokens at the end of
the legitimate input sequence (used if input’s length varies).
Mask is used here to prevent attention to fake tokens. Both
masks M trivially support linear matrix-vector multiplica-
tion (see: Fig. 2) and thus both packing and padding can be
incorporated into low-rank attention in time linear in L.

Algorithm 1 General Efﬁcient Low-Rank Masked Attention
Input: Query/key matrices: Q, K ∈ RL×dQK , value
matrix V ∈ RL×d, mask M ∈ RL×L, procedure
FastMultM : RL → RL calculating Mx (or its ap-
proximation) for the input x ∈ RL, kernel feature map:
φ : RdQK → Rm. vec(·) denotes vectorization.
Output: Masked low-rank attention embeddings using φ.
1. Compute matrices V1 ∈ RL×(md), V2 ∈ RL×m with
rows deﬁned as: V1
i )(cid:62),
i: = vec(φ(k(cid:62)
where ki/vi stands for the ith row of K/V.
2. Take ˜D1 = [FastMultM(V1
RL×md, ˜D2 = [FastMultM(V2
RL×m for V1/2
3. Output
ri = φ(q(cid:62)
φ(q(cid:62)
devec(·) devectorizes its input back to Rm×d.

:i
the embedding ri of the ith tokens as:
i:)(cid:62) , where qi is the ith row of Q and

:1), ..., FastMultM(V1
:1), ..., FastMultM(V2

denoting ith column of V1/2.

i )(cid:62)devec( ˜D1
i:)
i )(cid:62)( ˜D2

i: = φ(k(cid:62)

i )vi), V2

:md)] ∈
:m)] ∈

Figure 2. Left: Padding with the super-sequence consisting of four
sequences and its corresponding mask M. Right: Packing with
three fake padded tokens and its corresponding mask M. For
both masks, colored entries are equal to one and non-colored are
equal to zero. Both masks trivially support linear matrix-vector
multiplication.

3.1. Mask M as graph topology encoder

From now on we will think about mask M ∈ RL×L as a
weighted adjacency matrix Adj(G) of some weighted graph
G = (V, E, W ) with nodes/vertex-set V of size L, edge
set E and edge-weight function W : E → R. Lemma
3.1 combined with this observation leads to several far-
reaching conclusions regarding efﬁcient incorporation of
various masking mechanisms to Transformers, to which we
devote the remaining part of our theoretical analysis.

paddedTowards a general theory for scalable masked Transformers

3.2. D-dimensional Relative Positional Encodings

R.

We need the following deﬁnition:

Deﬁnition 3.2 (block-Toeplitz matrices). We say that a ma-
trix M ∈ RL×L is Toeplitz (or 1-level block Toeplitz) if
there exists some ξ : Z → R such that Mi,j = ξ(i − j).
We say that M ∈ RL×L is d-level block-Toeplitz for d ≥ 2
if M = (Bi,j) consists of block-matrices Bi,j taken from
some set {A1, ..., Ar} of (d − 1)-level block-Toeplitz ma-
trices and if each block Bi,j is replaced with the index k of
its corresponding matrix Ak, a Toeplitz matrix is obtained.

Consider the unweighted (i.e. all-one edge-weights) 1d-grid
graph Gbase (see: left graph in Fig. 1) and a complete graph
(i.e. with all possible edges) G obtained from it by deﬁning
each weight as Wi,j = f (distGbase(i, j)) for some (learn-
able) function f : N → R and where distGbase(i, j) is the
length of the shortest path between i and j in Gbase. If we
deﬁne M = Adj(G) then we get the regular RPE mecha-
nism with the 1d-graph interpreted as the input sequence.

Note that M deﬁned in such a way is Toeplitz and thus
supports O(L log(L)) matrix-vector multiplication via Fast
Fourier Transform (FFT). Thus low-rank RPE-masked at-
tention can be conducted in O(Ldm log(L)) time. This was
the observation of Luo et al. (2021). What if we replace
the 1d-grid with the d-dimensional grid and deﬁne M in the
analogous way? The idea is to maintain the initial structure
of the topologically more complicated input, e.g. 2d-grid
for images (with nodes as patches or even individual pixels)
or 3d-grid for videos (with 2d-slices as different frames).

There is a particularly elegant answer to this question:

Lemma 3.3 (d-dimensional RPEs). Consider the gener-
alized RPE-mechanism for the d-dimensional grid input
deﬁned above. Then there exists an ordering of input nodes
such that M is a d-level block-Toeplitz matrix (see: Fig. 1).

Since d-level block-Toeplitz matrices support O(L log(L))
matrix-vector multiplication via FFT for any ﬁxed constant
d (see Lee, 1986), Lemma 3.3 immediately leads to efﬁcient
corresponding masked attention computation.

3.3. More general graph-masks using shortest-paths

So far Gbase was assumed to have a grid structure. What
if we replace it with an arbitrary weighted graph? The
following natural question arises: Which condition does
Gbase and mapping f : R → R need to satisfy for the
mask M def= [f (distGbase(i, j))]i,j=1,...,L to support sub-
quadratic matrix-vector multiplication ?

We call such a pair (Gbase, f ) tractable. From what we
have said so far, we conclude that:
Corollary 3.4. If Gbase is an unweighted grid (of any di-
mensionality) then (Gbase, f ) is tractable for any f : R →

In several bioinformatics applications, e.g. molecular as-
sembly trees (Artemova et al., 2011), the underlying input’s
topology is a forest (e.g. a tree). We prove the following:
Lemma 3.5. If Gbase is a forest and f (z) = exp(τ (z))
for afﬁne mapping τ , then (Gbase, f ) is tractable and the
related mask supports linear matrix-vector multiplication.

j∈Ti

Sketch of the proof: The efﬁcient algorithm for comput-
ing w = Mx in this case is an application of the dy-
namic programming method for rooted trees. The algorithm
ﬁrst computes for each node i the following expression:
si = (cid:80)
exp(τ (dist(i, j)))xj, where Ti stands for the
subtree rooted in i (in the bottom-up fashion from leaves to
the ﬁxed root). This is followed by the computation of the
following expression: wi = (cid:80)
j∈T exp(τ (dist(i, j)))xj
for every node in the order from the root to the leaves (lever-
aging already computed si). Details are given in the Ap-
pendix and computations are illustrated in Fig. 3.

We ﬁnd a comprehensive description of tractable (Gbase, f )
an exciting analytic and combinatorial open problem.

Figure 3. Illustration of sketch of the proof of Lemma 3.5. The
directions of arrows show computation-ﬂow. In phase I, si-terms
are calculated in bottom-up fashion (from leaves to the root). The
value of si involving paths in i-rooted subtrees (red path with
discarded directions) is updated based on svk -terms involving
paths in subtrees Tvk . To complete calculations, in phase II paths
to nodes outside of the i-rooted tree are considered (purple path
with directions discarded). Their contribution is calculated from
the already computed wp(i) for the parent p(i) of node i and si.

3.4. Low-rank masking

Note that in all previously considered cases, mask M is
in general full-rank. However in several applications M
can be assumed to have (at least in expectation) a low-rank
decomposition, i.e.: M = E[M1M2] for some (random)
M1 ∈ RL×r, M2 ∈ Rr×L and some 0 < r (cid:28) L. A ﬂag-
ship example is the stochastic RPE mechanism presented in
(Liutkus et al., 2021) corresponding to (logits-added) dot-
product mask N translating to the softmax-kernel values
mask M. The latter one can be low-rank decomposed using
any random feature map based softmax-kernel linearization
mechanism, e.g. from (Choromanski et al., 2021). In such
a case, matrix-vector product v = Mx can be computed
(approximately) as: ˜v = (M1(M2x)) in time O(Lr), lead-

……rootrootTowards a general theory for scalable masked Transformers

ing to overall time complexity O(Lmrd) of the attention
module using mask M.

4. Masking with graph kernels

Masks deﬁned by shortest paths were shown to provide ef-
fective inductive bias for graph data (see Ying et al., 2021b),
yet they cannot be interpreted as applying any valid kernel
function on graph nodes and are very sensitive to small
changes in the graph. A prominent class of kernel-functions
K : V × V → R deﬁned on pairs of graphs nodes is the
family of graph-diffusion or heat kernels (GDKs). Thus it is
natural to identify masks M for input graph data G with the
graph diffusion kernel matrices KK = [K(i, j)]i,j=1,...,L.
GDK is deﬁned, for a hyperparameter λ > 0 and Xi denot-
ing the ith power of matrix X, as:

KK = exp(−λT) def=

∞
(cid:88)

i=0

(−λ)iTi
i!

,

(4)

where either: T = L for the Laplacian matrix L =
D − Adj(G) and D = diag([deg(i)]L
i=1); or T = LD−1
(normalized Laplacian case) or T = −Adj(G).

GDK is related to the diffusion process (Kondor & Lafferty,
2002) which describes in particular heat propagation. In a
vacuum, the solution of the partial differential heat equation
is the Gaussian-kernel, and in graphs it leads to GDK. Nodes
better connected with each other (graph diffusion kernel
quantiﬁes it via the number of different-length walks with
longer walks exponentially-deprioritized) give rise to larger
kernel values. Finally, t = 1
λ can be interpreted as time
when the solution is taken. GDK imprints topological signal
of the propagation medium via left heat-signature. As t →
∞ the kernel “ﬂattens” and the topological signal is lost.

Direct computation of the GDK matrix KK is of O(L3) time
complexity, thus prohibitively expensive even for sparse
input graphs. However, a key observation is that Lemma 3.1
teaches us that for efﬁcient masked low-rank attention we
only need to compute efﬁciently the action exp(−λT)x of
KK on a given x ∈ RL. This leads to our next result.
Theorem 4.1 (scalable Laplacian-GDK masking). Let a
mask M be deﬁned as M = exp(−λA), for A = L
or A = LD−1, where L is the Laplacian of the input
graph, and D = diag([deg(i)]L
i=1). Then low-rank masked
attention can be computed in time ˜O((|E| + L) log(2 +
(cid:107)A(cid:107)F)md), where ˜O hides polylog(L) factors, |E| is the
number of graph edges and (cid:107) · (cid:107)F is the Frobenius norm.

The theorem is a consequence of Lemma 3.1 and Theorem
1.2 from (Orecchia et al., 2012). We see that if |E| = o(L2),
the masked attention mechanism is sub-quadratic in L.

Low-rank attention & Markov processes with random
initial distributions: As noted by Orecchia et al. (2012),

the heat kernel matrix for T = LD−1 can be interpreted as
the probability transition matrix of the discrete-time random
walk where ﬁrst the number of steps i is sampled from
a Poisson distribution with mean λ, and then i steps of
the natural random walk are performed on G. Looking at
Algorithm 1, we conclude that here the low-rank structure of
attention enables us to incorporate the GDK mask by solving
that process in md initial (randomized) distributions over
nodes (randomization coming from mapping φ) rather than
in all L one-hot initial distributions (that would correspond
to the reconstruction of the entire transition matrix).

Remark: The literature on efﬁciently computing the ac-
tions of matrix-exponentials (which is our main focus in this
section) is very rich (Al-Mohy & Higham, 2011), partially
because of straightforward applications in the theory of dif-
ferential equations (Li et al., 2021). In principle, each of
these methods can be used by our algorithm.

4.1. Grid with graph-diffusion kernels

If the underlying graph is a d-dimensional grid, then GDK
with T = L has a closed-form formula. The follow-
ing is true (Kondor & Lafferty, 2002): KGDK(i, j) ∝
(tanhλ)dist(i,j) for the hyperbolic tangent tanh. Thus, as
in Sec. 3.2, the corresponding mask M is d-level block-
Toeplitz and grid-induced GDK-masking can be incorpo-
rated into low-rank attention in O(Lmd log(L)) time.

4.2. Low-rank masking strikes back for GDKs

We now propose a proxy of the GDK with T = Adj(G)
such that the corresponding kernel matrix admits (in expec-
tation) low-rank decomposition as in Sec. 3.4. Thus, based
on the theory we developed, the corresponding mask M
can be efﬁciently incorporated into low-rank attention with
no need to call efﬁcient solvers for the actions of matrix
exponentials. We call our graph kernel the Random Walks
Graph-Nodes Kernel or RWGNK.

Intuitively, the value of the RWGNK for two nodes is given
as a dot-product of two frequency vectors that record vis-
its in graph nodes of random walks beginning in the two
nodes of interest. More formally, for the hyperparameters
λ, α ≥ 0, and two random walks ω(k), ω(l) with stopping
probability 0 ≤ p ≤ 1 (or of a ﬁxed length) starting at k and
l respectively, the RWGNK is given as:

Kλ,α
p

(k, l) =

Eω(k)[f ω(k),λ
(cid:107)Eω(k)[f ω(k),λ

k

k

]

](cid:107)α
2

(cid:32) Eω(l)[f ω(l),λ
(cid:107)Eω(l)[f ω(l),λ

l

l

](cid:107)α
2

(cid:33)(cid:62)

]

.

(5)
The (row) frequency vector f ω(h),λ
for h ∈ V is given as
f ω(h),λ
e∈Lω(h)(i) λe, where Lω(h)(i) is the set of
h
lengths of those preﬁx sub-walks of a given random walk
ω(h) that end at i (where the preﬁx sub-walk of the walk

(i) def= (cid:80)

h

Towards a general theory for scalable masked Transformers

Figure 4. From left to right: unweighted graph G, its adjacency matrix Adj(G), its GDK matrix exp(Adj(G)) and RWGNK-matrix with
walk length of 3 and α = 1. Colored cells measure the relationship among pairs of nodes (darker is stronger). The last two matrices can
be thought of as continuous smoothings of Adj(G).

(j1, .j2, ..., jt) is any walk of the form (j1, ..., jr) for some
r ≤ t or an empty walk). Note that Eq. 5 leads to the
(k, l) = Ψ(k)Ψ(l)(cid:62),
desired representation of Kλ,α
where Ψ(h) is the renormalized expected frequency vector.
In practice, expectations are replaced by Monte Carlo sam-
plings over a few random walks, and vectors Ψ(h) are not
stored explicitly but in the form of weighted lookup tables.

as Kλ,α
p

p

Figure 4 compares RWGNK-induced mask with the regular
GDK-mask and the adjacency matrix mask. We call a Trans-
former applying low-rank masked attention via RWGNK, a
Graph Kernel Attention Transformer (or GKAT).

Next we explore the connection of RWGNKs with GDKs.
We denote by dmax, dmin the maximum and minimum de-
gree of a vertex in G respectively.
Theorem 4.2 (RWGNKs count discounted numbers of
walks). The following is true for the kernel matrix
Kλ,α
(k, l)]k,l∈V(G) of the RWGNK kernel
p
with 0 ≤ λ ≤ 1, α = 0 and 0 < p < 1 for a graph G with
vertex set V(G) of size N (element-wise matrix inequality):

(G) = [Kλ,α

p

Γ

(cid:18) ρ

dmax

(cid:19)

Adj(G)

≤ Kλ,α
p

(G) ≤ Γ

(cid:18) ρ
dmin

(cid:19)

Adj(G)

,

(6)

i=0 ci

p (k, l) = (cid:80)∞
i√

where ρ = (1 − p)λ and Γ(A) = (cid:80)∞
i=0(i + 1)Ai. Us-
ing the fact that Adji(G) encodes the number of walks
of length i between pairs of vertices in G, we conclude
that Kλ,0
k,lrk,l(i), where: rk,l(i) is the
number of walks of length i between nodes: k and l
and
. Note that val-
ues of GDK with parameter λ satisfy: GDKλ(k, l) =
(cid:80)∞
. In practice, it
sufﬁces to have random walks of ﬁxed length (instead of tak-
ing p > 0) (see: Sec 5). Furthermore, by taking α > 0 (e.g.
α = 1) we can guarantee that kernel values are bounded.

i=0 ˜ci(k, l)rk,l(i), where: ˜c(k, l) = λ
i√
i!

i+1(1−p)λ
dmax

i+1(1−p)λ
dmin

≤ ck,l ≤

i√

5. Experiments

We focus on the GKAT architecture introduced in Sec. 4.2
as a prominent instantiation of the general mechanism pre-
sented in this paper and experiments with 2-level block-
Toeplitz masking mechanisms introduced in Sec. 3.2 for

vision Transformers.

Regarding GKAT, we conducted exhaustive evaluations on
tasks ranging from purely combinatorial to bioinformatics,
and benchmarked 10 different methods. All these experi-
ments were run on a single Tesla P100 GPU with 16GB
memory. Experiments with vision Transformers were con-
ducted on the ImageNet dataset.

5.1. Combinatorial Classiﬁcation
In this section we focus on the problem of detecting local
patterns in graphs. A model takes a graph G as an input
and decides whether it contains some graph from the given
family of graphs H as a subgraph (not necessarily induced)
or is H-free. This benchmark tests the abilities of different
methods to solve purely combinatorial tasks.

Figure 5. Five motifs (patterns) used in the ﬁrst class of combina-
torial classiﬁcation experiments. For each pattern H, an algorithm
is trained to distinguish between graphs G containing H and those
that are H-free. A naive brute-force algorithm for conducting this
has time complexity Ω(N h), where h is the number of nodes of the
motif, prohibitively expensive for all these motifs (since h ≥ 9).

5.1.1. ERD ˝OS-R ´ENYI RANDOM GRAPH WITH MOTIFS

Data Generation: Following the procedure from (Nikolent-
zos & Vazirgiannis, 2020), we used ﬁve binary classiﬁcation
datasets consisting of random Erd˝os-R´enyi (ER) graphs con-
nected with motifs (positive example) or other smaller ER
graphs with the same average degree as a motif (negative
example), see Fig. 5 (details in the Appendix, Sec. A.2).
For each dataset we constructed S = 2048 positive and S
negative examples.

Tested Algorithms & Parameter Setting: We tested our
GKAT, graph convolution networks (GCNs, Kipf & Welling,
2017), spectral graph convolution networks (SGCs, Def-
ferrard et al., 2016) and graph attention networks (GATs,
Velickovic et al., 2018). A feature vector in each vertex
was of length l = 5 and contained top ordered l degrees
of its neighbors (if there were fewer than l neighbors, we
padded zeroes). A dataset for each motif was randomly split

Towards a general theory for scalable masked Transformers

Figure 6. Model accuracy comparison of all four methods: GKAT, GAT, GCN and SGC on the motif-detection task. All architectures are
2-layer. GKAT outperforms other algorithms on all the tasks. See also Appendix:Sec. A.4 for the tabular version with 100K-size graphs.

into 75%/25% training/validation set. We chose: the num-
ber of epochs E = 500, batch size B = 128, used Adam
optimizer with learning rate η = 0.001 and early-stopped
training if neither the validation loss nor validation accuracy
improved for c = 80 continuous epochs.

tance from each other (a negative example was obtained by
connecting two random vertices, but not farthest from each
other). Note that a positive example constructed in such a
way has shortest induced cycle of length P + 1, where P is
the diameter of the tree.

We applied 2-layer architectures. For GCNs and SGCs,
we used h = 32 nodes in the hidden layer. For SGC, we
furthermore bound each hidden layer with 2 polynomial
localized ﬁlters. For GAT and GKAT, we used 2 attention
heads, with h = 9 nodes in the hidden layer to make all
models of comparable sizes. In GKAT we used random
walks of length τ = 3. The results are presented in Fig. 6.
GKAT outperforms all other methods for all the motifs.

5.1.2. GLOBAL GRAPH PROPERTIES & DEEP VS DENSE
Next we took as H an inﬁnite family of motifs rather than
just a single motif. The algorithm needs to decide whether
a graph contains an induced cycle of length > T for a given
constant T . Thus the motif itself became a global property
that cannot be detected by exploring just a close neighbor-
hood of a node. In this experiment we focused also on
the “depth versus density” trade-off. Shallow neural net-
works with dense attention are capable of modeling deeper
networks relying on sparse layers, yet the price is extra com-
putational cost per layer. We test here whether architectures
that apply RWGNK kernels leveraging efﬁcient decompos-
able long-range attention from Sec. 4.2 can also replace
deeper counterparts or if they lose their expressiveness.

Figure 7. Comparison of the two-layer GKAT with different vari-
ants of GCNs, GATs and SGCs, varying by the number of hidden
layers. Shallow GKAT architecture has the expressiveness of
deeper version of its counterparts and in fact outperforms many of
them (e.g. graph convolution networks.)

Dataset Generation: We created S = 2048 random bi-
nary trees, each having 50 nodes, with 75%/25% for train-
ing/validation. For each tree, we constructed a positive
example, by connecting two nodes with the farthest dis-

Tested Algorithms & Parameter Setting: We used the
same algorithms as before and run detailed ablation studies
on the depth of the GKAT competitors, by comparing two-
layer GKAT with GATs, GCNs and SGCs of up to six layers.

For a fair comparison, we used models with a comparable
number of parameters. For the two-layer GKAT, we applied
8 heads in the ﬁrst layer, and 1 head in the second layer.
The dimension of each head was d = 4. The last layer
was fully-connected with output dimensionality o = 2 for
binary classiﬁcation. We applied random walk length of
τ = 6. For GCN, GAT and SGC, we tested number of
layers ranging from 2 to 6. We controlled the number of
nodes in the hidden layer(s) for GCN, GAT and SGC, and
the number of attention heads in each head for GAT so that
their total number of trainable parameters was comparable
with that of our two-layer GKAT. All other parameters were
chosen as in Sec. 5.1.1. More details on parameter settings
and additional ablation tests over random walk length of
GKAT are given in Table 5 and Fig. 9 in the Appendix (Sec.
A.2). Our main results are presented in Fig. 7.

We see that a shallow two-layer GKAT beats all GCN-
variants (also deeper ones) as well as GATs and SGCs with
< 4 layers by a wide margin. A two-layer GKAT is asymp-
totically equivalent to the four-layer GAT and SGC, yet as
we show in Sec. 5.3, is faster to train and run inference on.

5.2. Bioinformatics & Social Networks experiments

Datasets: We tested GKAT for graph classiﬁcation tasks on
9 standard and publicly available bioinformatics and social
networks datasets (Kersting et al., 2016) using a carefully
designed model selection and assessment framework for a
fair comparison (Errica et al., 2020). The former include:
D&D (Dobson & Doig, 2003), PROTEINS (Borgwardt et al.,
2005), NCI1 (Wale et al., 2008) and ENZYMES (Schom-
burg et al., 2004), and the latter: IMDB-BINARY, IMDB-
MULTI, REDDIT-BINARY, REDDIT-5K and COLLAB
(Yanardag & Vishwanathan, 2015), see also Sec. A.3.1.

Towards a general theory for scalable masked Transformers

Tested Algorithms: We compared GKAT with top GNN
methods used previously for that data: DCGNN (Zhang
et al., 2018), DiffPool (Ying et al., 2018), ECC (Simonovsky
& Komodakis, 2017), GraphSAGE (Hamilton et al., 2017)
and RWNN (Nikolentzos & Vazirgiannis, 2020), which are
selected based on their popularity and architectural differ-
ences. For bioinformatics datasets, but ENZYMES, we used
the Molecular Fingerprinting (MF, Ralaivola et al., 2005;
Luzhnica et al., 2019) as a baseline. This ﬁrst applies global
sum pooling and then a single-layer MLP with ReLU ac-
tivations. For social datasets and ENZYMES, we applied
the DeepMultisets (DM) method (Zaheer et al., 2017) as a
baseline. We did not add the numbers for the GIN method
(Xu et al., 2018) since we could not reproduce its reported
results for the models of size similar to GKAT.

Table 1. Performance of different algorithms on the bioinformatics
data. For each dataset, we highlighted/underlined the best/second
best method. GKAT is the best on three out of four tasks.

D&D

NCI1

Proteins

Enzymes

Baseline

78.4 ±4.5% 69.8±2.2% 75.8±3.7% 65.2±6.4%

DGCNN
DiffPool
ECC

76.6±4.3% 76.4±1.7% 72.9±3.5% 38.9±5.7%
75.0±3.5% 76.9±1.9% 73.7±3.5% 59.5±5.6%
72.6±4.1% 76.2±1.4% 72.3±3.4% 29.5±8.2%
GraphSAGE 72.9±2.0% 76.0±1.8% 73.0±4.5% 58.2±6.0%
77.6±4.7% 71.4±1.8% 74.3±3.3% 56.7±5.2%
78.6±3.4% 75.2±2.4% 75.8 ±3.8% 69.7 ±6.0%

RWNN
GKAT

Table 2. Performance of different algorithms on the social network
data. GKAT is among two top methods for four out of ﬁve tasks.
IMDB-M REDDIT-B REDDIT-5K COLLAB

IMDB-B

Baseline

70.8±5.0% 49.1 ±3.5% 82.2±3.0%

52.2±1.5%

70.2±1.5%

DGCNN
DiffPool
ECC

69.2±5.0% 45.6±3.4% 87.8±2.5%
68.4±3.3% 45.6±3.4% 89.1±1.6%
67.7±2.8% 43.5±3.1%
GraphSAGE 68.8±4.5% 47.6±3.5% 84.3±1.9%

49.2±1.2%
53.8±1.4%
OOM
50.0±1.3%
70.8±4.8% 47.8±3.8% 90.4±1.9% 51.7±1.5%
71.4±2.6% 47.5±4.5% 89.3±2.3%

71.2±1.9%
68.9±2.0%
OOM
73.9±1.7%
71.7±2.1%
55.3±1.6% 73.1±2.0%

RWNN
GKAT

OOM

GKAT Setting: We used a two-layer GKAT followed by
the baseline layers: we ﬁrst applied an attention layer with
k heads (a hyperparameter to be tuned), and then another
one with one head to aggregate topological information
on graphs. Next, we applied either the MF method or the
DM method to further process the aggregated information.
The random walk length τ in each GKAT layer satisﬁed
τ ≤ 4 and depended on the evaluated datasets. The average
graph diameter shown in Table 6 in the Appendix helps to
calibrate walk length. We chose it to balance the pros of
using a shallow architecture and the cons of information loss
from dense layer compression. GKAT increased the number
of the baseline’s parameters by a negligible fraction.

Training Details: We used a 10-fold CV for model as-
sessment, and an inner holdout with 90%/10% train-
ing/validation split for model selection following the same
settings (Errica et al., 2020). We then trained the whole
training-fold three times, randomly holding out 10% of data
for early stopping after model selection in each fold. The

average score for these runs was reported in Table 1 & 2.

The results from Table 1 and Table 2 show that GKAT is
the best on three out of four bioinformatics datasets and is
among two best methods on four out of ﬁve social network
datasets. It is the only GNN method that consistently out-
performs baseline on all but one bioinformatics dataset (bio-
data beneﬁts more than others from efﬁcient longer-range
attention modeling as showed by Choromanski et al. (2021)).
In the Appendix (Sec. A.5) we provide additional com-
parisons of GKAT with GAT on citation networks, where
GKAT outperforms GAT on two out of three tested datasets.

5.3. Space & Time Complexity Gains of GKAT

We measured speed and memory improvements coming
from GKAT as compared to GAT as well as accuracy loss
in comparison to Transformer using GKAT masking, but
explicitly computed (GKAT-0), see Table 3. We decided to
report relative rather than absolute numbers since the former
are transferable across different computational setups. The
accuracy gaps of the corresponding GKAT-0 and GKAT
models (obtained after the same # of epochs) are marginal,
yet GKAT yields consistent speed and memory gains as
compared to GAT per attention layer, particularly substantial
for very large graphs as those from Citeseer and Pubmed.

Table 3. Speed & Space Complexity gains provided by GKAT.
First row: memory compression (lower better). Second & third
row: speedup in training and inference respectively per one at-
tention layer as compared to GAT. Last row: accuracy loss as
compared to GKAT-0 applying brute-force RWGNK masking. We
used four datasets from Sec. 5.1.1, a dataset from Sec. 5.1.2 (Tree)
and two citation network datasets (see: Sec. A.5): Citeseer and
Pubmed with graphs of much larger sizes and on which GKAT
also outperforms GAT. We applied r = 256 random features to
linearize softmax kernel for features in nodes for citation network
datasets, r = 16/8 for datasets from Sec. 5.1.1/ 5.1.2.

Cavem. Circle Grid

Ladder

Tree

Citeseer Pubmed

GKAT / GKAT-0 memory

0.54

0.53

0.55

train speedup vs GAT

inf speedup vs GAT

1.40x

1.46x

1.41x

1.42x

1.49x

1.49x

0.52

1.40x

1.47x

0.95

1.10x

1.12x

0.18

5.10x

5.21x

0.07

9.50x

9.54x

GKAT-0 - GKAT (accur.)

0.07% 0.09% 0.08% 0.07% 0.06% 0.05%

0.06%

In Table 4 we show that GKAT is also faster that its counter-
parts (GCN, GAT, SGC) in terms of wall clock time needed
to reach particular accuracy levels, by comparing accuracy
levels reached by different models in a given wall clock time
budget (time GKAT needs to complete ﬁrst 100 epochs).

5.4. 2-level block-Toeplitz masks for vision data

In this section (see: Fig. 8), we present additional results
in the vision domain, showing large, +2.5-3.4 percentage
point, gains in accuracy arising from applying the 2-level
block Toeplitz masking introduced in the paper (see: Sec.
3.2) on top of the regular vision Performer. As explained
before, the price we pay for these gains is only a log(L)
multiplicative factor in time complexity.

Towards a general theory for scalable masked Transformers

Table 4. Running time of training different networks on datasets
from Sec. 5.1.1 and Sec. 5.1.2. For GCN, GAT and SGC, we
reported the accuracy with 2 layers. For GKAT, we used a 2-layer
architecture and reported the accuracy with a ﬁxed walk length of
6 for Induced Cycle Detection, and of 3 for motifs from Sec. 5.1.1.

Induced Cycle Caveman Circle

Grid

Ladder Circle Ladder

GCN
GAT
SGC
GKAT

63.2%
77.0%
56.6%
83.6%

71.4% 59.3% 66.7%
62.1%
80.6% 73.8% 75.9%
69, 1%
55.4%
64.7% 58.2% 59.1%
85.1% 83.3% 77.1% 82.4%

87.4%
93.7%
66.5%
94.6%

Figure 8. Comparison of regular Performers using x2, ELU + 1,
and ReLU kernels, with their counterparts applying 2-level block
Toeplitz masking from our paper on the ImageNet classiﬁcation
task (hidden size = 768, 12 layers & heads, MLP dim = 3072).

6. Additional results & some open questions

In this section, we present additional theoretical results
regarding the theory of the scalable efﬁcient masked Trans-
formers that we have developed. We focus on masking
mechanisms for the tree-graph inputs. We also discuss some
open problems for future work.

In Section 3.3 we showed that for the speciﬁc classes of
functions f (exponentials of afﬁne mappings of the shortest-
distance paths) and arbitrary weighted trees, pairs (Gbase, f )
are tractable. Here we will provide additional related results,
but for arbitrary functions f . Our ﬁrst result is as follows:
Lemma 6.1. If T = Gbase is an unweighted tree and f is
an arbitrary function then the corresponding mask matrix
M = M(Gbase, f ) supports matrix-vector multiplication
in time O(L · log2(L)). Thus (Gbase, f ) is tractable.
We now show that if the diameter diam(T ) of the tree T is
of the order of magnitude o(log2(L)), where L = |V (T )|,
a more efﬁcient algorithm can be used.
Lemma 6.2. If T = Gbase is an unweighted tree and f is
an arbitrary function then the corresponding mask matrix
M = M(Gbase, f ) supports matrix-vector multiplication
in time O(L · diam(Gbase)).
Corollary 6.3. From the above, we obtain an O(L log(L))
algorithm for computing the action of M on x if Gbase is

a tree with: (a) a node of degree ≥ 2 that has same-length
paths to all the leaves and (b) all other nodes of degree ≥ 3
(e.g. complete binary tree).

Corollary 6.4. The algorithm from the proof of Lemma
6.2 (see:Appendix) can be used to improve algorithm from
Lemma 6.1 (that works for graphs with arbitrary diame-
ters) if the input T to that algorithm satisﬁes: diam(T ) =
o(log2(|V (T )|)). Note that computing the diameter of any
tree T can be done in time O(|V (T )|) by running two depth-
ﬁrst-search procedures: the ﬁrst one from an arbitrary nodes
v of T and the second one from the node farthest from v in
T (obtained via the ﬁrst depth-ﬁrst-search procedure).
We leave the Reader with an interesting open problem:

Can we improve Lemma 6.1 to obtain O(L log(L)) running
time, i.e. replace the log2(L) factor with a log(L) factor?

Note that if the unweighted tree is a single path, the an-
swer to the above question is: Yes. Indeed, this is precisely
the 1D-RPE setting that we have discussed before. Fur-
thermore, since in that setting the problem reduced to the
multiplication with Toeplitz matrices, inherently relying on
the FFT, the log(L) factor in all likelihood cannot be im-
proved (unless FFT can be replaced with a faster algorithm
or multiplication with Toeplitz matrices is conducted ap-
proximately). Still, we do not know whether for general
unweighted trees (or even nontrivial tree-extensions of the
path) we can reach the O(L log(L)) running time.

It might be also interesting to analyze how those of our
presented methods that work for tree input data can be ex-
tended to non-tree graphs, but with low treewidth (Cygan
et al., 2015), that can be thought of as relaxations of trees.

7. Conclusion

We presented a holistic approach to incorporating masking
into scalable low-rank Transformers. We provided general
theoretical results which include earlier results as special
cases. We conducted comprehensive empirical evaluations
of the new instantiations of the mechanism for graph data.

We focused in the paper not only on scalable variants, but
have introduced several new masking meethods that can
be used on their own, even in regular Transformers. These
include in particular d-level block-Toeplitz masking mecha-
nisms with applications in vision and video processing, that
we believe might lead to new vision-Transformers archi-
tectures. We show that topological masking is a powerful
inductive bias and that corresponding “topological Trans-
formers” turn out to be effective in various domains such as
bioinformatics and vision.

8. Acknowledgements
AW acknowledges support from a Turing AI Fellowship
under grant EP/V025279/1, The Alan Turing Institute, and
the Leverhulme Trust via CFI.

Towards a general theory for scalable masked Transformers

References

Al-Mohy, A. H. and Higham, N. J. Computing the action of
the matrix exponential, with an application to exponential
integrators. SIAM J. Sci. Comput., 33(2):488–511, 2011.
doi: 10.1137/100788860. URL https://doi.org/
10.1137/100788860.

Artemova, S., Grudinin, S., and Redon, S. Fast construction
of assembly trees for molecular graphs. J. Comput. Chem.,
32(8):1589–1598, 2011. doi: 10.1002/jcc.21738. URL
https://doi.org/10.1002/jcc.21738.

Avsec, ˇZ., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-
Barwinska, A., Taylor, K. R., Assael, Y., Jumper, J.,
Kohli, P., and Kelley, D. R. Effective gene expression
prediction from sequence by integrating long-range inter-
actions. bioRxiv, 2021. doi: 10.1101/2021.04.07.438649.
https://www.biorxiv.org/content/
URL
early/2021/04/08/2021.04.07.438649.

Borgwardt, K. M., Ong, C. S., Sch¨onauer, S., Vishwanathan,
S. V. N., Smola, A. J., and Kriegel, H.-P. Protein func-
tion prediction via graph kernels. Bioinformatics, 21
(suppl 1):i47–i56, 06 2005.
doi:
10.1093/bioinformatics/bti1007. URL https://doi.
org/10.1093/bioinformatics/bti1007.

ISSN 1367-4803.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances
in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Belanger, D.,
Colwell, L., and Weller, A. Masked language modeling
for proteins via linearly scalable long-context transform-
ers. arXiv preprint arXiv:2006.03555, 2020.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song,
X., Gane, A., Sarl´os, T., Hawkins, P., Davis, J. Q.,
Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell,
L. J., and Weller, A. Rethinking attention with per-
In 9th International Conference on Learn-
formers.
ing Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021. OpenReview.net, 2021. URL https:
//openreview.net/forum?id=Ua6zuk0WRH.

Cygan, M., Fomin, F. V., Kowalik, L., Lokshtanov,
D., Marx, D., Pilipczuk, M., Pilipczuk, M., and
Saurabh, S.
Springer,
2015.
doi: 10.1007/
978-3-319-21275-3. URL https://doi.org/10.
1007/978-3-319-21275-3.

Parameterized Algorithms.

ISBN 978-3-319-21274-6.

Dai, Z., Lai, G., Yang, Y., and Le, Q. Funnel-transformer:
Filtering out sequential redundancy for efﬁcient language
processing. In Larochelle, H., Ranzato, M., Hadsell, R.,
Balcan, M., and Lin, H. (eds.), Advances in Neural In-
formation Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020.

Daniely, A., Frostig, R., Gupta, V., and Singer, Y.
Random features for compositional kernels. CoRR,
abs/1703.07872, 2017. URL http://arxiv.org/
abs/1703.07872.

Defferrard, M., Bresson, X., and Vandergheynst, P. Con-
volutional neural networks on graphs with fast localized
spectral ﬁltering. CoRR, abs/1606.09375, 2016. URL
http://arxiv.org/abs/1606.09375.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
In Burstein, J., Doran, C., and
guage understanding.
Solorio, T. (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers), pp. 4171–4186.
Association for Computational Linguistics, 2019. doi:
10.18653/v1/n19-1423. URL https://doi.org/
10.18653/v1/n19-1423.

Dobson, P. D. and Doig, A. J. Distinguishing en-
zyme structures from non-enzymes without alignments.
Journal of molecular biology, 330(4):771—783, July
2003. ISSN 0022-2836. doi: 10.1016/s0022-2836(03)
URL https://doi.org/10.1016/
00628-4.
s0022-2836(03)00628-4.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
N. An image is worth 16x16 words: Transformers for
In 9th International Con-
image recognition at scale.
ference on Learning Representations, ICLR 2021, Vir-
tual Event, Austria, May 3-7, 2021. OpenReview.net,
2021. URL https://openreview.net/forum?
id=YicbFdNTTy.

Errica, F., Podda, M., , Bacciu, D., and Micheli, A. A fair
comparison of graph neural networks for graph classiﬁca-
tion. ICLR 2020, 2020.

Towards a general theory for scalable masked Transformers

Hamilton, W., Ying, Z., and Leskovec, J.

Inductive
representation learning on large graphs. In Guyon, I.,
Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish-
wanathan, S., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.
neurips.cc/paper/2017/file/
5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.
pdf.

Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z.,
Tang, Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang,
Y., and Tao, D. A survey on visual transformer. CoRR,
abs/2012.12556, 2020. URL https://arxiv.org/
abs/2012.12556.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-
18 July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pp. 5156–5165. PMLR,
2020. URL http://proceedings.mlr.press/
v119/katharopoulos20a.html.

Kersting, K., Kriege, N. M., Morris, C., Mutzel,
Benchmark data sets for
URL https://ls11-www.cs.tu-
795,

P., and Neumann, M.
graph kernels, 2016.
dortmund.de/staff/morris/graphkerneldatasets,
2016.

Kipf, T. N. and Welling, M. Semi-supervised classiﬁcation
with graph convolutional networks. In 5th International
Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017. URL https://
openreview.net/forum?id=SJU4ayYgl.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer:
The efﬁcient transformer. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020. URL https://openreview.net/forum?
id=rkgNKkHtvB.

Kondor, R. and Lafferty, J. D. Diffusion kernels on graphs
and other discrete input spaces. In Sammut, C. and Hoff-
mann, A. G. (eds.), Machine Learning, Proceedings of
the Nineteenth International Conference (ICML 2002),
University of New South Wales, Sydney, Australia, July
8-12, 2002, pp. 315–322. Morgan Kaufmann, 2002.

Lee, D. Fast multiplication of a recursive block toeplitz
J. Complex.,
doi: 10.1016/0885-064X(86)
URL https://doi.org/10.1016/

matrix by a vector and its application.
2(4):295–305, 1986.
90007-5.
0885-064X(86)90007-5.

Li, D., Zhang, X., and Liu, R. Exponential integrators for
large-scale stiff Riccati differential equations. J. Com-
put. Appl. Math., 389:113360, 2021. doi: 10.1016/j.cam.
2020.113360. URL https://doi.org/10.1016/
j.cam.2020.113360.

Liutkus, A., C´ıfka, O., Wu, S., Simsekli, U., Yang, Y., and
Richard, G. Relative positional encoding for transform-
ers with linear complexity.
In Meila, M. and Zhang,
T. (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Ma-
chine Learning Research, pp. 7067–7079. PMLR, 2021.
URL http://proceedings.mlr.press/v139/
liutkus21a.html.

Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S.,
Ke, G., Wang, L., and Liu, T. Stable, fast and accu-
rate: Kernelized attention with relative positional en-
coding. CoRR, abs/2106.12566, 2021. URL https:
//arxiv.org/abs/2106.12566.

Luzhnica, E., Day, B., and Li`o, P. On graph classiﬁca-
tion networks, datasets and baselines. arXiv preprint
arXiv:1905.04682, 2019.

Nikolentzos, G. and Vazirgiannis, M. Random walk
In Larochelle, H., Ranzato,
graph neural networks.
M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.),
Advances in Neural Information Processing Systems,
volume 33, pp. 16211–16222. Curran Associates,
URL https://proceedings.
Inc.,
neurips.cc/paper/2020/file/
ba95d78a7c942571185308775a97a3a0-Paper.
pdf.

2020.

Orecchia, L., Sachdeva, S., and Vishnoi, N. K. Approximat-
ing the exponential, the lanczos method and an ˜o(m)-time
spectral algorithm for balanced separator. In Karloff, H. J.
and Pitassi, T. (eds.), Proceedings of the 44th Symposium
on Theory of Computing Conference, STOC 2012, New
York, NY, USA, May 19 - 22, 2012, pp. 1141–1160. ACM,
2012. doi: 10.1145/2213977.2214080. URL https:
//doi.org/10.1145/2213977.2214080.

Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Lev-
skaya, A., and Shlens, J. Stand-alone self-attention in vi-
sion models. In Wallach, H. M., Larochelle, H., Beygelz-
imer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett, R.
(eds.), Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, pp. 68–80, 2019.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,
In
N. A., and Kong, L. Random feature attention.

Towards a general theory for scalable masked Transformers

9th International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net, 2021. URL https://openreview.
net/forum?id=QtTKTdVrFBB.

Pham, N., Ha, T., Nguyen, T., Nguyen, T., Salesky, E.,
St¨uker, S., Niehues, J., and Waibel, A. Relative positional
encoding for speech recognition and direct translation. In
Meng, H., Xu, B., and Zheng, T. F. (eds.), Interspeech
2020, 21st Annual Conference of the International Speech
Communication Association, Virtual Event, Shanghai,
China, 25-29 October 2020, pp. 31–35. ISCA, 2020. doi:
10.21437/Interspeech.2020-2526. URL https://doi.
org/10.21437/Interspeech.2020-2526.

Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J.,
Kong, L., and Zhong, Y. cosformer: Rethinking softmax
in attention. CoRR, abs/2202.08791, 2022. URL https:
//arxiv.org/abs/2202.08791.

Ralaivola, L., Swamidass, S. J., Saigo, H., and Baldi, P.
Graph kernels for chemical informatics. Neural networks,
18(8):1093–1110, 2005.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁ-
cient content-based sparse attention with routing trans-
Trans. Assoc. Comput. Linguistics, 9:53–
formers.
68, 2021. URL https://transacl.org/ojs/
index.php/tacl/article/view/2405.

Schomburg, I., Chang, A., Ebeling, C., Gremse, M., Heldt,
C., Huhn, G., and Schomburg, D. BRENDA, the en-
zyme database: updates and major new developments.
Nucleic Acids Research, 32(Database issue):D431–3, Jan-
uary 2004.

Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B.,
and Eliassi-Rad, T. Collective classiﬁcation in network
data, 2008.

Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with
relative position representations. In Walker, M. A., Ji,
H., and Stent, A. (eds.), Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT, New Orleans, Louisiana, USA,
June 1-6, 2018, Volume 2 (Short Papers), pp. 464–468.
Association for Computational Linguistics, 2018. doi:
10.18653/v1/n18-2074. URL https://doi.org/
10.18653/v1/n18-2074.

Simonovsky, M. and Komodakis, N. Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 3693–3702,
2017.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. In Guyon, I., von Luxburg, U., Bengio,
S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pp. 5998–6008, 2017.

Vaswani, A., Ramachandran, P., Srinivas, A., Parmar,
N., Hechtman, B. A., and Shlens, J. Scaling local
self-attention for parameter efﬁcient visual backbones.
In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021, virtual, June 19-25, 2021,
pp. 12894–12904. Computer Vision Foundation / IEEE,
URL https://openaccess.thecvf.
2021.
com/content/CVPR2021/html/Vaswani_
Scaling_Local_Self-Attention_for_
Parameter_Efficient_Visual_Backbones_
CVPR_2021_paper.html.

Velickovic, P., Cucurull, G., Casanova, A., Romero, A.,
Li`o, P., and Bengio, Y. Graph attention networks. In 6th
International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?
id=rJXMpikCZ.

Wale, N., Watson, I. A., and Karypis, G. Comparison
of descriptor spaces for chemical compound retrieval
and classiﬁcation. Knowl. Inf. Syst., 14(3):347–375,
2008. doi: 10.1007/s10115-007-0103-5. URL https:
//doi.org/10.1007/s10115-007-0103-5.

Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li,
Y., and Singh, V. Nystr¨omformer: A nystr¨om-based al-
gorithm for approximating self-attention. In Thirty-Fifth
AAAI Conference on Artiﬁcial Intelligence, AAAI 2021,
Thirty-Third Conference on Innovative Applications of
Artiﬁcial Intelligence, IAAI 2021, The Eleventh Sympo-
sium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2021, Virtual Event, February 2-9, 2021, pp. 14138–
14148. AAAI Press, 2021. URL https://ojs.aaai.
org/index.php/AAAI/article/view/17664.

Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How
arXiv preprint

powerful are graph neural networks?
arXiv:1810.00826, 2018.

Yanardag, P. and Vishwanathan, S. Deep graph kernels. In
Proceedings of the 21th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD
’15, pp. 1365–1374, New York, NY, USA, 2015. Associa-
tion for Computing Machinery. ISBN 9781450336642.
doi: 10.1145/2783258.2783417. URL https://doi.
org/10.1145/2783258.2783417.

Towards a general theory for scalable masked Transformers

Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov,
R., and Le, Q. V. Xlnet: Generalized autoregressive pre-
training for language understanding. In Wallach, H. M.,
Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox,
E. B., and Garnett, R. (eds.), Advances in Neural Infor-
mation Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
5754–5764, 2019.

Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen,
Y., and Liu, T. Do transformers really perform bad for
graph representation? CoRR, abs/2106.05234, 2021a.
URL https://arxiv.org/abs/2106.05234.

Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen,
Y., and Liu, T. Do transformers really perform bad for
graph representation? CoRR, abs/2106.05234, 2021b.
URL https://arxiv.org/abs/2106.05234.

Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W.,
and Leskovec, J. Hierarchical graph representation
In Bengio, S.,
learning with differentiable pooling.
Wallach, H., Larochelle, H., Grauman, K., Cesa-
Bianchi, N., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/
e77dbaf6759253c7c6d0efc5690369c7-Paper.
pdf.

(eds.), Advances

Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,
Salakhutdinov, R. R., and Smola, A. J. Deep sets.
In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R., Vishwanathan, S., and Garnett,
Information Pro-
R.
cessing Systems, volume 30. Curran Associates,
URL https://proceedings.
Inc.,
neurips.cc/paper/2017/file/
f22e4747da1aa27e363d86d40ff442fe-Paper.
pdf.

in Neural

2017.

Zhang, M., Cui, Z., Neumann, M., and Chen, Y. An end-
to-end deep learning architecture for graph classiﬁca-
tion, 2018. URL https://aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17146.

Zhou, P., Fan, R., Chen, W., and Jia, J. Improving gen-
eralization of transformer for speech recognition with
parallel schedule sampling and relative positional em-
bedding. CoRR, abs/1911.00203, 2019. URL http:
//arxiv.org/abs/1911.00203.

Towards a general theory for scalable masked Transformers

A. Appendix

A.1. Several Pointers

We include pointers to this part of the code that does not include sensitive/proprietary information. The core GKAT framework
(with the additional analysis of new graph sketches that GKAT leads to, called graphots, mixing regular feature vectors
in nodes with the topological features) is here: https://github.com/HL-hanlin/GKAT. We used (deterministic and random)
feature map mechanisms corresponding to the features deﬁned in graph nodes from this repository: https://github.com/google-
research/google-research/tree/master/performer.

A.2. Combinatorial Classiﬁcation Experiments: Additional Details

The data for the motif detection task from Section 5.1.1 was generated as follows:

• Firstly we created ﬁve simple motifs as shown in Fig. 5. Note that each motif has ≥ 9 vertices so a brute-force
combinatorial algorithm for motif-detection would take time Ω(N 9), prohibitively expensive even for small graphs G.

• We then generated for each motif S small Erd˝os-R´enyi graphs with the same number of nodes as that motif and the

same average degree.

• For each motif, we also generated S larger Erd˝os-R´enyi random graphs, each of 100 vertices, again of the same average

degree.

• We obtained positive/negative samples by connecting each larger Erd˝os-R´enyi random graph with the motif/previously

generated smaller Erd˝os-R´enyi random graph (with certain edge probability).

In Table 5 we present additional details regarding architectures used in the experiments from Section 5.1.2, in particular the
number of parameters and heads / polynomial ﬁlters used in different layers. Ablation tests over GKAT random walk length
for Section 5.1.2 are presented in Fig. 9.

Figure 9. Ablation tests over random walk length of GKAT in Sec. 5.1.2.

A.3. GNNs for Bioinformatics Tasks & Social Networks Data: Additional Details

A.3.1. DATASETS DESCRIPTIONS

Detailed proﬁles of the datasets used in the experiments from Sec. 5.2 are given in Table 6.

For each dataset, we chose graphs with the number of nodes close to the average number of nodes shown in Table 6.
Examples of bioinformatics-graphs from these datasets are given in Fig. 10. Examples of social network graphs from these
datasets are given in Fig. 11.

A.3.2. HYPERPARAMETER SELECTION

In this section, we present details regarding hyperparameter selection in Section 5.2 (see: Table 7).

Towards a general theory for scalable masked Transformers

Table 5. Additional details regarding architectures used in Section 5.1.2. For GKAT, we applied 8 heads in the ﬁrst layer, and 1 head in
the second layer, with 4 hidden units in each attention head. The total number of trainable parameters was 242. For GAT, we tested the
number of layers from 2 to 6, changing the number of attention heads in each layer, but with the same number of hidden units in each
attention head. For GCN, we modiﬁed the number of hidden units in each layer. For SGC, we modiﬁed the number of polynomial ﬁlters
and the number of hidden units in each layer. The number of attention heads in GAT, as well as the number of hidden units in each layer
in GCN and SGC were chosen to make their total number of trainable parameters comparable with the corresponding number of GKAT.

GKAT 2 layers

GAT

GCN

SGC

2 layers
3 layers
4 layers
5 layers
6 layers

2 layers
3 layers
4 layers
5 layers
6 layers

2 layers
3 layers
4 layers
5 layers
6 layers

#Heads

[8, 1]

[8, 1]
[4, 2, 1]
[4, 2, 1, 1]
[2, 2, 2, 1, 1]
[3, 2, 1, 1, 1, 1]

Dim. Head

#Parameters

4

4
4
4
4
4

242

242
242
266
258
270

Dim. Layer

#Parameters

[14, 14]
[10, 10, 10]
[8, 8, 8, 8]
[10, 8, 6, 6, 6]
[10, 6, 6, 6, 6, 6]

268
262
250
260
268

#Polynomial Filters

Dim. Layer

#Parameters

[4, 2]
[4, 2, 2]
[8, 2, 2, 2]
[8, 2, 2, 2, 2]
[8, 2, 2, 2, 2, 2]

[10, 8]
[8, 6, 6]
[8, 5, 4, 4]
[6, 5, 4, 4, 4]
[6, 4, 4, 4, 4, 3]

236
234
247
245
249

Figure 10. Representative plots for bioinformatics datasets. For each bioinformatics dataset, we chose the graph with number of nodes
most similar to the average number of nodes shown in Table 6.

The tunable parameters included: general parameters like batch size, learning rate, dropout ratio, global pooling methods,
regularization rate, data normalization methods, as well as parameters speciﬁc to our GKAT layers, which included: number
of GKAT layers, number of attention heads and dimension of each head in a GKAT layer. We also tuned other options:
whether to add a fully-connected layer after data normalization, but before GKAT layers, and dimension of fully-connected
layers (both preceding and coming after GKAT layers). Due to the large amount of tunable parameters, we decided to ﬁrst
conduct a rough search for each parameter using only one random CV fold, select one/several parameter combination(s)
with best performance, and then reused on all other folds.

For all other methods, we reported the best scores conducted via an extensive hyperparameters grid search (Errica et al.,
2020). For GKAT, we ﬁxed the number of epochs to E = 1000, early stopping patience as 500 epochs, the criterion
for early stopping as validation accuracy, global pooling method as summation, and used Adam optimizer. Then we
performed hyperparameter tuning for: batch size B ∈ {32, 128}, learning rate η ∈ {0.01, 0.001, 0.0001}, dropout
ratio ∈ {0.0, 0.1, 0.2, 0.4}, L2-regularization rate ∈ {0.001, 0.005}, dimension of attention head in the ﬁrst GKAT layer
h ∈ {4, 8, 16, 32}, number of attention heads in the ﬁrst GKAT layer ∈ {1, 4, 8, 12}, number of nodes in the MLP layer
∈ {32, 64, 128}, GKAT random walk length τ ∈ {1, 2, 3, 4}, whether to use a fully-connected layer before the ﬁrst GKAT
layer, and whether to apply batch normalization to pre-prosess data before feeding the model with it.

For some of the datasets (e.g. D&D), we selected the best hyperparameter set optimized over one random CV fold, and used
it across all cross-validation outer folds.

Towards a general theory for scalable masked Transformers

Table 6. Bioinformatics and Social Dataset descriptions. #NODES, #EDGES and Diameter columns contain values averaged over all
graphs in a given dataset.

#Graphs

#Classes

#Nodes

#Edges Diameter

#Features

.

F
N
I
O
I
B

L
A
I
C
O
S

D&D
ENZYMES
NCI1
PROTEINS

COLLAB
IMDB-BINARY
IMDB-MULTI
REDDIT-BINARY
REDDIT-5K

1178
600
4110
1113

5000
1000
1500
2000
4999

2
6
2
2

3
2
3
2
5

284.32
32.63
29.87
39.06

74.49
19.77
13.00
429.63
508.82

715.66
64.14
32.30
72.82

2457.78
96.53
65.94
497.75
594.87

19.90
10.86
13.26
11.48

1.86
1.86
1.47
9.72
11.96

89
3
37
3

1
1
1
1
1

Figure 11. Representative plots for social datasets. For each social dataset, we chose the graph with number of nodes most similar to the
average number of nodes shown in Table 6.

A.4. Space & Time Complexity Gains of GKAT: Additional Experiments

Additionally, we have conducted experiments on much larger Erd˝os-R´enyi graphs with motifs, see: Section 5.1.1. The
average number of nodes of each ER graph was 100K. Tested architectures had the same characteristics as in Section 5.1.1.
The results (ﬁnal model accuracy) are presented in Table 8 for datasets: Caveman, Circle, Grid, Ladder and Circular-Ladder
respectively.

A.5. Experiments with Citation Networks Datasets

A.5.1. DATASETS DESCRIPTIONS

Datasets: To directly compare GKAT with GAT, we also tested both algorithms on three publicly available citation networks datasets:
Cora, Citeseer and Pubmed ((Sen et al., 2008)) with the same data splits as in (Velickovic et al., 2018). Datasets descriptions are given in
Table 9.

A.5.2. COMPARISON WITH GAT

Experiment Settings: We used the same model architecture and parameters as in GAT for our GKAT to make the
comparison as accurate as possible. The only difference is that we replaced the adjacency matrix masking in GAT by the
normalized dot-product based similarity matrix generated from random walks, as described in Section 4.2. Both models
used two-layer attention, with 8 attention heads in the ﬁrst layer, and 1 head in the second layer. We used 8 hidden units
in the ﬁrst layer, and the number of output units in the second layer was the same as number of classes. Each layer was
followed by an exponential linear unit (ELU) activation. We applied L2-regularization with λ = 0.0005, dropout with
p = 0.6 for inputs and normalized attention coefﬁcients in both layers for all three datasets.

Results: The results are shown in Table 10. Our GKAT algorithm achieved lower accuracy on Cora dataset, but higher on
the remaining two.

Dynamic Generator of Random Walks: We also tried the so-called dynamic-GKAT. The dynamic variant generated
random walks from scratch in each training epoch, thus requiring additional compute. However one advantage of the
dynamic version is that we could assign different transition probabilities for adjacent nodes (rather than sampling next
point of the walk uniformly at random). The transition probability matrix can be a masked attention matrix and we only
need its actions on the L-dimensional vectors to compute probability vectors in the visited nodes. Since the GKAT-masked

Towards a general theory for scalable masked Transformers

Table 7. Hyperparameter settings for the bioinformatics and social network datasets from Section 5.2.

BS

#Heads

dHead

dFC Lenrw Drop

L2

add FC Norm

.

M
E
H
C

L
A
I
C
O
S

D&D
NCI1

PROTEINS

ENZYMES

COLLAB

IMDB-BINARY

IMDB-MULTI

REDDIT-BINARY
REDDIT-5K

32
32
32
128

32

32

32

32

32
32

8
8
4
8
4
8

12
8
12
8
12
4
8

16
32
8
32
16
32

4
4
8
4
8
4
8

128
64
32
128
32
64

128
64
128
64
128
128
64

2
4

3

3

2

1

1

2
2

−
0.1

−

0.005
0.001

0.001

0.1

0.001

−

−

−

−
−

0.005

0.005

0.005

0.005
0.005

No
Yes

Yes

Yes

No

No

No

No
No

BN
BN

BN

BN

No
No
BN
No
BN
BN
BN

Table 8. Running time of training different networks on datasets from Sec 5.1.1 but with much larger number of nodes (∼ 100K). For
GCN, GAT and SGC, we reported the accuracy with 2 layers. For GKAT, we used a 2-layer architecture and reported the accuracy with a
ﬁxed walk length of 3 for motifs from Sec. 5.1.1.

Caveman Circle

Grid

Ladder Circle Ladder

GCN
GAT
SGC
GKAT

82.7% 80.4% 80.6%
88.3%
81.0% 69.8% 77.0%
75.0%
80.4% 72.3% 76.1%
70.0%
89.3% 83.2% 80.7% 81.5%

91.4%
89.2%
82.4%
92.3%

Table 9. Citation Networks Datasets Descriptions.

Cora Citeseer Pubmed

#Nodes
#Edges
#Features
#Classes
#Training Nodes
#Validation Nodes
#Test Nodes

2708
5419
1433
7
140
500
1000

3327
4732
3703
6
120
500
1000

19717
44338
500
3
60
500
1000

Table 10. Comparison of GAT and GKAT on citation networks datasets. For Cora and Citeseer, we reported the results for GAT from
(Velickovic et al., 2018). For GAT and Pubmed dataset, we reported the results averaged over 15 runs with the same parameter settings as
in Cora and Citeseer. GKAT was run 15 times over multiple random walk lengths up to 7, and the best was reported.

Cora

Citeseer

Pubmed

GAT

83.0±0.7% 72.5 ± 0.7% 77.2 ± 0.6%

GKAT 82.1 ± 0.7% 73.0±0.7% 78.0 ±0.7%

attention can be interpreted as a kernelizable attention of a product-kernel (the product of the kernel between feature vectors
in nodes and the nodes in the graph) and each factor-kernel admits on expectation a linearization, the product-kernel also
does it via the mechanism of the Cartesian-product random features (see: (Daniely et al., 2017)). Thus this matrix admits on
expectation a lower-rank decomposition and thus based on our analysis from Sec. 3.4, the actions of that matrix on the input

Towards a general theory for scalable masked Transformers

vectors can be efﬁciently computed.

An intuition behind that particular variant is that we assign higher transition probabilities for neighbors with higher attention
coefﬁcients. The dynamic variant enabled us to improve accuracy of GKAT on Citeseer to 73.3% (with reduced 0.6%
standard deviation).

A.5.3. ABLATION TESTS ON RANDOM WALK LENGTH FOR GKAT

Figure 12 compares the effect of random walk path length of GKAT algorithms on training for Cora, Citeseer and Pubmed
datasets. We run GKAT with multiple random walk lengths up to 7. The results show that a small path length no longer than
4 is enough for GKAT and dynamic-GKAT, which supports our claim that short walks are sufﬁcient for GKAT.

Figure 12. Ablation tests over random walk path lengths of GKAT and dynamic-GKAT on Cora, Citeseer and Pubmed datasets. The
errorbar represents 1 standard deviation. Test-accuracies for the optimized GAT were shown as horizontal red dotted lines in each subplot.
Best test accuracies of GKAT algorithms were highlighted with red squares. The dynamic-GKAT was tested only on datasets, where
accuracy advantage of the regular optimized GKAT over optimized GAT was ≤ 0.5%.

A.6. Proof of Lemma 3.1 and Algorithm 1

Proof. Note that the ith token representation ri obtained from the general masked kernel attention is of the following form:

ri =

φ(q(cid:62)

i )(cid:62) (cid:80)L
i )(cid:62) (cid:80)L

j=1 Mi,jφ(k(cid:62)
j )vj
j=1 Mi,jφ(k(cid:62)
j )

φ(q(cid:62)

,

(7)

where qi, kj, vj stand for the ith query and jth key/value row vectors respectively. As in (Luo et al., 2021), we deﬁne the
following two sequences of matrices and vectors of shapes Rm×d and R1×m respectively:





L
(cid:88)



L

Mi,jφ(k(cid:62)

j )vj



, D2 =





L
(cid:88)



L

Mi,jφ(k(cid:62)

j )(cid:62)



.

(8)

D1 =

j=1

i=1

j=1

i=1

If we deﬁne ˜D1 and ˜D2 as the vectorized variants of D1 and D2, where each element of the sequence is (row) vectorized
(note that the elements of D2 are already row-vectorized) and the resulting vectors are stacked into matrices, then:

˜D1 = MV1, ˜D2 = MV2,

(9)

with the ith rows of V1 and V2 given as: V1

i = vec(φ(ki)(cid:62)vi), V2

i = φ(k(cid:62)

i )(cid:62).

We conclude that the computation of D1 and D2 takes time TN(L)md and consequently all representations ri can be
computed in time O((TN(L) + L)md). That completes the proof of Lemma 3.1.

Note that Algorithm 1 follows immediately from the above proof. The algorithm consists of two phases. In the ﬁrst
phase, sequences D1 and D2 are computed (in the form of ˜D1 and ˜D2). In the second phase, they are used to get
new token-representations (different tokens apply different elements of D1 and D2 for the computation of their new
representations).

Towards a general theory for scalable masked Transformers

A.7. Proof of Lemma 3.3

Proof. We deﬁne the so-called grid-ordering on the nodes of the d-dimensional grid recursively. For d = 1 we take the
natural ordering on the line. For the d-dimensional grid with d > 1, we take the (d − 1)-dimensional slices: S1, S2, ...
ordered according to their dth-dimension index. In each slice we recursively order all the tokens We then combine the
orderings of all the slices to get the ordering on the d-dimensional grid.

We claim that the mask corresponding to the d-dimensional grid with the grid-ordering of tokens is d-level block-Toeplitz.

We will proceed by an induction on d. For d = 1 the corresponding mask is of the form: M = [f (i − j)]i,j=1,...,L for some
learnable function f and a natural ordering on the line. Therefore M is constant on each diagonal thus it is Toeplitz (e.g.
1-level block-Toeplitz). Now let us assume that d > 1 and the result holds for d − 1. We take the grid-ordering for the
d-dimensional grid and the partitioning of the tokens given by the (d − 1)-dimensional slices S1, S2, ... ordered according
to their dth-dimension index. This partitioning induces the block-partitioning of the mask M (using grid-ordering) on
the d-dimensional grid. Now note that the shortest-path distance between two nodes v1 and v2 from slices Si1 and Si2
respectively can be computed as: dist(v1, v(cid:48)
2 is the projection of v2 into slice Si1. This observation
combined with the inductive assumption implies that deﬁned above block-partitioning of M produces matrices Bi,j from
Deﬁnition 3.2 and that completes the proof.

2) + |i1 − i2|, where v(cid:48)

A.8. Proof of Lemma 3.5

Proof. Without loss of generality we can assume that Gbase is a tree. Assume that τ is of the form: τ (z) = az + b for some
a, b ∈ R. Take some x ∈ RL. Let us root the tree T in one of its vertices that we denote as v0. Denote by v1, ..., vk for
some k ≥ 0 its neighbors. For every node i we deﬁne si as follows:

si =

(cid:88)

j∈Ti

exp(τ (dist(i, j)))xj,

(10)

where Ti denotes a subtree of T rooted in i. Our ﬁrst observation is that all si for i = 1, ..., L can be computed in O(L)
time. To see this, note that:

sv0 = exp(τ (dist(v0, v0)))x0 +

(cid:88)

(cid:88)

exp(τ (dist(v0, j)))xj =

ebx0 +

(cid:88)

(cid:88)

ea·dist(v0,j)+bxj = ebx0 +

l=1,...,k
(cid:88)

(cid:88)

j∈Tvl
ea·(dist(vl,j)+W (v0,vl))+bxj =

l=1,...,k

j∈Tvl
ebx0 +

(cid:88)

l=1,...,k

eW (v0,vl) (cid:88)
j∈Tvl

l=1,...,k

j∈Tvl
ea·dist(vl,j)+bxj = ebx0 +

(cid:88)

l=1,...,k

eW (v0,vl)svl

(11)

Thus we see that computing sv0 requires computing each svl , followed by additional addition/multiplication operations
that take time O(deg(v0)). We conclude that we can recursively compute all si in time O(L). Let us note that the entry of
w = Mx corresponding to node i is of the form:

wi =

(cid:88)

j∈T

exp(τ (dist(i, j)))xj,

Therefore ultimately we aim to compute all wi for i = 1, ..., L in time O(L). We observe that:

Now take node i (cid:54)= v0. Denote by p(i) the predecessor of i in T . We have:

wv0 = sv0

wi =

(cid:88)

j∈Ti

exp(τ (dist(i, j)))xj +

(cid:88)

j /∈Ti

exp(τ (dist(i, j)))xj =

si +

(cid:88)

j /∈Ti

ea·dist(i,j)+bxj = si +

(cid:88)

j /∈Ti

ea(W (i,p(i))+dist(p(i),j))+bxj =

si + eaW (i,p(i)) (cid:88)

eadist(p(i),j)+bxj = si + eaW (i,p(i))ti,

j /∈Ti

(12)

(13)

(14)

Towards a general theory for scalable masked Transformers

where ti = (cid:80)

j /∈Ti

eadist(p(i),j)+bxj. Now note that:

wp(i) = ti + eW (i,p(i))si

and thus: ti = wp(i) − eW (i,p(i))si. Plugging in the formula for ti into Equation 14, we get:

wi = eW (i,p(i))wp(i) + (1 − e2W (i,p(i)))si

(15)

(16)

We conclude that having computed all si for i = 1, ..., L in time O(L), we can compute all wi for i = 1, ..., L in time O(L)
by ordering vertices in their increasing distance from the root v0, setting up wv0 = sv0 and applying Equation 16.

A.9. Proof of Theorem 4.1

Proof. We need the following deﬁnition.

Deﬁnition A.1. A matrix A is Symmetric and Diagonally Dominant (SDD) if Ai,j = Aj,i for all i, j and Ai,i ≥
(cid:80)

j(cid:54)=i |Ai,j|.

The results is a straightforward consequence of Theorem 1.2 from (Orecchia et al., 2012) and Lemma 3.1. For Reader’s
convenience we restate that theorem here:

Theorem A.2 (SDD Matrix Exponential Computation). Given an L × L SDD matrix A, a vector x and a parameter δ ≤ 1,
there is an algorithm that computes a vector u such that (cid:107) exp(−A)x − u(cid:107) ≤ δ(cid:107)x(cid:107) in time ˜O((|E| + L) log(2 + (cid:107)A(cid:107))),
Here tilde hides poly(log(L)) and poly(log( 1

δ )) factors.

It sufﬁces to notice that both Laplacian matrix and its renormalized version are SDD. Furthermore, by Lemma 3.1, fast
(approximate) computation of exp(−λA)x for any x ∈ RL and A as in Theorem 4.1 leads to fast computation of the
low-ranked attention with mask M = exp(−λA), as explained in Algorithm 1.

A.10. Proof of Theorem 4.2

Proof. Note ﬁrst that since ω(k) and ω(l) are chosen independently, we have:

Kλ,0

p (k, l) = Eω(k)[f ω(k),λ

k

] · (Eω(l)[f ω(l),λ

l

])(cid:62) = Eω(k),ω(l)[f ω(k),λ

k

(f ω(l),λ
l

)(cid:62)]

Denote: X = f ω(k),λ

k

(f ω(l),λ
l

)(cid:62). The key observation is that X can be rewritten as:

(cid:88)

X =

(cid:88)

λaλb =

(cid:88)

λa+b

u∈V(G)

(j1=k,...,ja+1=u)=pref(ω(k)),
(j(cid:48)
b+1=u)=pref(ω(l))

1=l,...,j(cid:48)

(j1=k,...,ja+b+1=l)∈Ω(k,l)

(17)

(18)

where Ω(k, l) is the multi-set of walks from k to l that are built from some preﬁx of ω(k) concatenated with some preﬁx of
ω(l). Therefore we can write X as:

X =

(cid:88)

len(r)
(cid:88)

r∈R(k,l)

i=0

λlen(r)1[E(r, i)],

(19)

where R(k, l) is the set of walks from k to l, len(r) stands for the length (number of edges) of walk r and E(r, i) is an event
that ﬁrst i edges of the walk r (counting from k) form the preﬁx sub-walk of ω(k) and the remaining ones form the preﬁx
sub-walk of ω(l). Therefore we have:

Kλ,0

p (k, l) = Eω(k),ω(l)





(cid:88)

len(r)
(cid:88)

r∈R(k,l)

i=0



λlen(r)1[E(r, i)]

 =

(cid:88)

len(r)
(cid:88)

r∈R(k,l)

i=0

λlen(r)Pω(k),ω(l)[E(r, i)] =

(cid:88)

len(r)
(cid:88)

λlen(r)

r∈R(k,l)

i=0

i−1
(cid:89)

j=0

1 − p
deg(rj)

len(r)−i−1
(cid:89)

t=0

1 − p
deg(rlen(r)−1−t)

,

(20)

Towards a general theory for scalable masked Transformers

where ry stands for the yth vertex of the walk r starting from k and deg(v) denotes the degree of a vertex v.

Therefore we obtain:

(cid:88)

len(r)
(cid:88)

r∈R(k,l)

i=0

(cid:19)len(r)

(cid:18) (1 − p)λ
dmax

≤ Kλ,0

p (k, l) ≤

(cid:88)

len(r)
(cid:88)

r∈R(k,l)

i=0

(cid:19)len(r)

(cid:18) (1 − p)λ
dmin

(21)

We conclude that:

∞
(cid:88)

i=0

rk,l(i)

(cid:19)i

(cid:18) (1 − p)λ
dmax

(i + 1) ≤ Kλ,0

p (k, l) ≤

∞
(cid:88)

i=0

rk,l(i)

(cid:19)i

(cid:18) (1 − p)λ
dmin

(i + 1)

(22)

To complete the proof, it sufﬁces to notice that matrix Adji(G) encodes the number of walks of length i between pairs of
vertices in G.

A.11. Extensions of the results from Section 3.3

We will provide here the proofs of the results presented in Section 6. We ﬁrst introduce additional concepts wee will leverage
in the proofs.
Deﬁnition A.3 (balanced separators). Take some function w : V (G) → R≥0 and some α > 0. We say that a subset
S ⊆ V (G) is the α-balanced separator with respect to w, if the set of vertices C of every connected component of the
subgraph graph G|V (G)\S of G, induced by V (G)\S, satisﬁes: w(C) ≤ α · w(V (G)), where w(X ) def= (cid:80)
Lemma A.4. If G is a tree then for an arbitrary function w : V (G) → R≥0 the 1
adjacent vertices can be found in time O(L).

2 -balanced separator consisting of two

x∈X w(x).

Proof. The proof is given in the proof of Lemma 7.19 in (Cygan et al., 2015) and relies on the standard tree-search.

A.11.1. THE PROOF OF LEMMA 6.1

Proof. Take some vector x ∈ RL. The goal is to compute Mx in time O(L log2(L)). For the node i in a tree T , denote:

si =

(cid:88)

j∈T

f (dist(i, j))xj

(23)

Thus we want to compute all si in time O(L log2(L)). If |V (T )| ≤ 2 then all the calculations can be trivially done in O(1)
time, so we will assume now that |V (T )| > 2. Take the 1
2 -balanced separator {a, b} in T (with respect to the standard
measure that counts the number of vertices) that exists and can be found in time O(L) by Lemma A.4. Denote by Ta the
set of those trees in T|V (T )\{a,b} that are are incident to a in T and by Tb the set of those trees in T|V (T )\{a,b} that are are
incident to b in T . Note that one of these sets might be potentially empty. Let us assume, without loss of generality that Ta
is not empty. Denote by Va the union of the set of all the vertices of all the elements of Ta and by Vb the corresponding
set for Tb. If 1
10 |V (T )|, take: T1 to be the subtree of T induced by Va ∪ {a} and T2 to be the subtree
of T induced by Vb ∪ {a, b}. Otherwise take this c ∈ {a, b} such that |Vc| > 9
10 |V (T )|. Denote: Tc = {T 1, ..., T m}.
Note that m > 0 (Tc is not empty). By the deﬁnition of the balanced separator, we have: |V (T i)| ≤ 1
2 |V (T )| for
i = 1, ..., m. On the other hand: |V (T 1)| + ... + |V (T m)| ≥ 9
10 |V (T )|. Denote by i∗ the smallest i ∈ {1, ..., m} such that
|V (T 1)| + ... + |V (T i∗

10 ≤ |Va| ≤ 9

)| ≥ 9

10 |V (T )|. Note that i∗ > 1. We have:
1
2

|V (T )| −

9
10

|V (T )| =

2
5

|V (T )| ≤ |V (T 1)| + ... + |V (T i∗

)| − |V (T i∗

)|

= |V (T 1)| + ... + |V (T i∗−1)| ≤

9
10

|V (T )|

(24)

Denote by T1 a subtree of T induced by V (T 1) ∪ ... ∪ V (T i∗−1) ∪ {c} and by T2 a subtree of T induced by V (T )\(V (T 1) ∪
...∪V (T i∗−1)). Note that in both cases we obtain two trees: T1 and T2 sharing a single vertex and such that: V (T1)∪V (T2) =
V (T ). Furthermore, we have:

|V (T1)| = f |V (T )| + c1, |V (T2)| = (1 − f )|V (T )| + c2,

(25)

Towards a general theory for scalable masked Transformers

for c1, c2 ∈ {0, 1} and 2
i = (cid:80)
Denote: y1
in Tk with distance i from v. Note that all yk

5 ≤ f ≤ 9
xj and y2

i = (cid:80)

j∈Z1
i

j∈Z2
i

10 . Denote: {v} = V (T1) ∩ V (T2).

xj for i = 1, ..., |V (T )|, where Z k
i can be trivially computed in time O(|V (T )|).

i for k ∈ {1, 2} stands for the set of vertices

To compute all si for i = 1, ..., |V (T )|, we ﬁrst compute recursively the following expressions:

sk
i =

(cid:88)

j∈Tk

f (dist(i, j))xj

(26)

for i ∈ V (Tk) and k ∈ {1, 2}. In order to compute expressions si, in addition to expressions sk
i , we need to include the
cross-term contributions (for pairs of vertices where one is from T1 and the other from T2). Note that this can be trivially done
in time O(V (T )) as long as we have computed the following two vectors: Hy1 and Hy2, where yk = (yk
|V (T )|)(cid:62)
for k ∈ {1, 2} and H is the Hankel matrix with the ﬁrst row of the form: (f (2), f (3), ..., f (|V (T )| + 1)) and the last column
of the form: (f (|V (T )| + 1), ..., f (|V (T )| + |V (T )|))(cid:62). This can be done in time O(|V (T )| log(|V (T |))) with Fast
Fourier Transform. We conclude that our algorithm needs two recursive calls for subproblems of sizes which are constant
fractions of |V (T )| and given in Eq. 25, as well as additional computations conducted in time O(|V (T )| log(|V (T |))).
That leads to the total time complexity O(|V (T )| log2(|V (T |))) which completes the proof.

1 , ..., yk

A.11.2. THE PROOF OF LEMMA 6.2

Proof. Let us root T in a ﬁxed vertex v0. We denote by Ti the subtree of T rooted in i. For every node i, we maintain an
array gi of length diam(T ) + 1, where: gi[l] = (cid:80)
j∈Ti:dist(i,j)=l xj. Computing gi for vertices i which are the leaves of the
tree T rooted in v0 can be trivially done in time O(1) per vertex. Now assume that i is not a leaf and denote by: q1, ..., qk
(for some k > 0) its children. Note that: gi can be computed as follows:

gi[l] =

(cid:26)(cid:80)k

p=1 gqp [l − 1], if l ≥ 1
xi, if l = 0

We also deﬁne an array hi for every node i as follows:

hi[l] =

(cid:88)

xj

j∈T :dist(i,j)=l

(27)

For a given array z, denote by circ(z) its circulant-shift given as: circ(z)[l] = z[l − 1] for l > 0 and circ(z)[0] = 0. Note
that: hv0 = gv0 . Furthermore, for i (cid:54)= v0, we can compute hi from gi and hp(i), where p(i) stands for the parent of i in T
(rooted in v0), as follows:

hi = gi + circ(hp(i) − circ(gi)),

(28)

where addition and subtraction are dimension-wise. Thus having computed all gi, we can compute all hi by proceeding from
the root v0 in the order induced by the distance from the root. We conclude that calculating hi for all vertices i takes time
O(L · diam(T )). Therefore, as in the case of our proof for the unweighted tree with f given as the exponential mapping of
the afﬁne transform, effectively we perform in two stages - bottom-up to compute gi-arrays and from the root to the leaves
to compute arrays hi (with the use of already computed arrays gi).

Denote: w = Mx. Note that:

wi =

diam(T )
(cid:88)

l=0

f (l)hi(l)

(29)

Thus computing all wi can be also conducted in time O(L · diam(T )) and that completes the proof.

