Divide and Learn: A Divide and Conquer Approach for Predict+Optimize

Authors
Ali Ugur Guler,1 Emir Demirovic, 2 Jeffrey Chan, 3 James Bailey, 1 Christopher Leckie, 1 Peter J.
Stuckey, 4
1 University of Melbourne, 2 Delft University of Technology, 3 RMIT University, 4 Monash University
aguler@student.unimelb.edu.au, e.demirovic@tudelft.nl, jeffrey.chan@rmit.edu.au, baileyj@unimelb.edu.au,
caleckie@unimelb.edu.au, peter.stuckey@monash.edu.au

0
2
0
2

c
e
D
4

]

G
L
.
s
c
[

1
v
2
4
3
2
0
.
2
1
0
2
:
v
i
X
r
a

Abstract

The predict+optimize problem combines machine learning of
problem coefﬁcients with a combinatorial optimization prob-
lem that uses the predicted coefﬁcients. While this problem
can be solved in two separate stages, it is better to directly
minimize the optimization loss. However, this requires dif-
ferentiating through a discrete, non-differentiable combina-
torial function. Most existing approaches use some form of
surrogate gradient. Demirovic et al showed how to directly
express the loss of the optimization problem in terms of the
predicted coefﬁcients as a piece-wise linear function. How-
ever, their approach is restricted to optimization problems
with a dynamic programming formulation. In this work we
propose a novel divide and conquer algorithm to tackle op-
timization problems without this restriction and predict its
coefﬁcients using the optimization loss. We also introduce a
greedy version of this approach, which achieves similar re-
sults with less computation. We compare our approach with
other approaches to the predict+optimize problem and show
we can successfully tackle some hard combinatorial problems
better than other predict+optimize methods.

Introduction
Machine Learning (ML) has gained substantial attention
in the last decade, and has proven to be useful in a wide
range of industries. ML models usually focus on making
accurate predictions by minimizing errors, such as mean
squared error (MSE). These predictions can then be used
as coefﬁcients in other decision making processes, such as a
combinatorial optimization problem. The real performance
of these predictions is evaluated by their ability to lead to
the correct decisions. Such evidence based decision mak-
ing arises in many ﬁelds like transportation, healthcare, se-
curity and education (Horvitz and Mitchell 2010). Conse-
quently, there has been growing interest in ML models for
use in optimization problems. These models try to predict
coefﬁcients of the optimization problem in such a way that
even if the predictions are less accurate, they lead to bet-
ter decisions. This paradigm is called predict then opti-
mize (Elmachtoub and Grigas 2017) or predict+optimize
(Demirovic et al. 2020). In this paper we propose a new
framework for predict+optimize to learn coefﬁcients by di-
rectly reasoning over discrete combinatorial problems.

Motivation: Traditionally, ML models treat predictions
as the end goal. For example, a regression model will try

to minimize the MSE of its predictions. However, if these
predictions are the coefﬁcients of an optimization problem,
the prediction and optimization tasks are not independent
operations.
Example 1. There are two open research positions at a pres-
tigious institute, and three candidates for the two positions.
The principal investigator (PI) needs to decide which two
candidates to choose. In order to make this decision, the
PI needs to predict how productive each candidate will be,
given information about the candidates, like their past pa-
pers and institutions. Suppose the PI designs two models to
predict how many papers each candidate is likely to pub-
lish every year. The ﬁrst model is a traditional ML model.
It learns to minimize the MSE of the predicted number of
publications for each candidate. The second model is a pre-
dict+optimize model that learns how to pick the most pro-
ductive two candidates. Suppose these three candidates a,b,c
will publish va = 3, vb = 5, vc = 7 papers a year, re-
spectively. The ML model predicts that the candidates will
publish vpa = 5, vpb = 4, vpc = 6 papers a year. In con-
trast, the predict+optimize model seems to make inaccurate
predictions about the candidates, with vpa = 0, vpb = 50,
vpc = 70. The PI thinks predict+optimize predictions are
unlikely and they decide to use the ML model’ s predictions.
In this example the ML model makes noticeably more ac-
curate predictions compared to the predict+optimize model.
If we evaluate their performance with regards to MSE, then
the ML model outperforms the predict+optimize model by
an MSE of 2 to 2001. In contrast, if we evaluate the models’
performance using the result of the optimization problem,
predict+optimize outperforms ML by choosing the more
productive candidates {b, c} over the less productive can-
didates {a, c}. By choosing a standard ML model, the PI
fails to realize that predict+optimize reasoned over the opti-
mization problem, and learned the underlying ranking of the
candidates. The predict+optimize model penalized the least
productive candidate, and exaggerated the most productive
candidates. In fact these “inaccurate” predictions were help-
ful in making the correct decision.

If the ML model makes perfect predictions, it also leads to
the optimal decision. However all models are prone to errors.
When there are errors in predictions, MSE does not nec-
essarily represent the performance of the decisions (Ifrim,
O’Sullivan, and Simonis 2012). For this candidate selection

 
 
 
 
 
 
problem, penalizing errors that change the relative produc-
tivity is more important and the ML model failed to penalize
errors that disturbs the relative ordering of the candidates.
A predict+optimize framework trains parameters with re-
spect to the optimization objective, and it can understand
the underlying problem better. Although predict+optimize
can improve decision making, these models require learning
through hard, often non differentiable and discrete functions.
One way to differentiate through combinatorial problems
is to use surrogates, however surrogates induce an approx-
imation error to the optimization objective (Thapper and
ˇZivn`y 2018). Demirovic et al. (2020) propose a novel frame-
work to directly reason over the exact optimization loss for
problems with a linear objective with a dynamic program-
ming (DP) solution. They represent the optimization coefﬁ-
cients as parameterised linear functions and use the param-
eterised linear functions to solve the optimization problem
with DP. The DP solver performs piece-wise linear alge-
bra to construct a piece-wise linear function (PWLF). They
show that the transition points of the PWLF can capture the
underlying optimization loss. Their model uses the transi-
tion points to train model parameters, and they achieve im-
provements for knapsack problems. Although this approach
can understand the exact optimization problem, it relies on
DP, and therefore it’s application areas are limited only to
problems with a DP solution. Their frameworks exhaustively
ﬁnds all the transition points. However, large and complex
problems may have a large number of uninteresting transi-
tion points. For such problems the DP approach may fail to
scale well, and take too long to run.

In this paper we propose a novel framework to directly
reason over the exact optimization loss (with no restriction
to DP). Our framework builds upon the idea of representing
the optimization loss as a PWLF. However, unlike the DP
solution, we use a numerical approach to extract transition
points. We show that the predicted PWLF is a convex func-
tion. We use this knowledge to apply a divide and conquer
algorithm to compare different sample points and identify
transition points. We then evaluate the transition points on
the real optimization problem and train our model param-
eters with the exact optimization loss. We further propose
greedy methods and show that less accurate transition point
identiﬁcation can decrease the run time, and still achieve
similar performance to the full methods. First we experiment
on 0-1 knapsack problems with both unit weights and vary-
ing weights. We show that our divide and conquer approach
achieves identical results to the DP model and scales better
for larger problems. We also demonstrate that for hard knap-
sack problems the exact methods are more robust compared
to state of the art surrogate models (SPO-Relax, QPTL).
To demonstrate our framework’s ability to reason with ar-
bitrary optimization problems, we experiment on a complex
scheduling problem, which the DP method is not able to rea-
son about.

Our contributions are as follows:

• Show for optimization problems with linearly parame-
terised coefﬁcients and linear objectives, predicted objec-
tive value is convex.

• A novel framework to directly reason over the exact op-
timization problem based on PWLF mapping. Unlike the
previous state of the art DP method, our framework is not
limited to DP and can be used for any arbitrary optimiza-
tion problems with a linear objective.

• Greedy methods that show that a less than perfect PWLF
mapping still achieve similar performance to a full map-
ping, and reduce the run time considerably. We show that
our greedy methods scales better than the previous DP ap-
proach for larger problems.

• Evaluation on 0-1 knapsack problem and a non DP
scheduling problem, and comparison with the previous
state of the art exact method DP, and two state of the art
surrogate methods QPTL, SPO-Relax.

Related Work
The standard approach to predict+optimize problems is to
separately solve the prediction problem and then the op-
timization problem. Combined approaches are a relatively
new focus. Bengio (1997) showed that for hand crafted mod-
els to optimize a ﬁnancial portfolio, proﬁt performs better
than a standard loss function. Kao, Roy, and Yan (2009)
proposed using a combination of Empirical Optimization
and ordinary least squares loss to improve performance for
decision driven machine learning. Lim, Shanthikumar, and
Vahn (2012) deﬁne relative regret in the context of portfo-
lio optimization. Elmachtoub and Grigas (2017) deﬁne the
general Smart Predict and Optimize (SPO) loss, which we
call the regret in our paper. They propose a linear relaxation
SPO+ loss to train machine learning models. Their work
shows SPO+ loss can be used to achieve improved perfor-
mance for constrained linear programming problems. Amos
and Kolter (2017) propose to transform the optimization
loss into a quadratic problem using KKT equations. Donti,
Amos, and Kolter (2017) show that performance can be im-
proved by using sequential quadratic programming (QP) to
compute the new loss, and train the model with respect to it.
Wilder, Dilkina, and Tambe (2019) extend the QP approach
to linear programming problems. Ferber et al. (2020) extend
the approach of Wilder, Dilkina, and Tambe (2019) to di-
rectly apply to mixed integer programming by using pure
cutting plane methods to solve the MIP, resulting in an LP
sufﬁcient to deﬁne the MIP optimally. Mandi et al. (2020)
also show that SPO+ loss can be used as a surrogate loss
for relaxations of combinatorial problems and achieve per-
formance improvements. Luo et al. (2020) propose a spe-
cialised framework to optimize virtual machine provision-
ing. Black-box end to end frameworks are also used to dif-
ferentiate and learn combinatorial problems (Bello et al.
2016), (Li, Chen, and Koltun 2018),(Niculae et al. 2018).
Poganˇci´c et al. (2020) use a black-box approach to pre-
dict optimal solutions from coefﬁcient features. Demirovic
et al. (2019a) investigate the knapsack problem from a pre-
dict+optimize perspective and show how ranking methods
can be applied to it. Similarly Demirovic et al. (2019b) intro-
duce transition points for ranking problems. The direct inspi-
ration of our work is that of Demirovic et al. (2020), which
shows how to optimize parameters in a learning model di-

rectly using regret, as long as the optimization problem has
a dynamic programming formulation. They build a piece-
wise linear function using the dynamic programming for-
mulation that identiﬁes transition points, where the regret
changes. In this work we extend this approach to arbitrary
optimization problems by using numerical methods to ﬁnd
transition points in the regret loss function.

Divide and Learn

Preliminaries
Our framework divide and learn (DnL) predicts coefﬁcients
with a linear model. We show for linear models, predicted
objective value is a convex piecewise function. We now for-
mally deﬁne the predict+optimize problem. Given a set of
objective coefﬁcients vvv, we deﬁne an optimization problem
and its solution as:

max
xxx∈C

Obj(xxx, vvv)

s(vvv) ≡ arg max

Obj(xxx, vvv)

xxx∈C

(1)

(2)

where C is the set of feasible solutions (usually described
implicitly). The oracle s(vvv) ﬁnds a solution that maximizes
the objective value of the optimization problem given ob-
jective coefﬁcients vvv. In predict+optimize settings objective
coefﬁcients are not known beforehand and they are predicted
using features θθθ and parameters βββ, vpvpvp = vpvpvp(θθθ, βββ). We show
the new parameterized optimization problem as

s(θθθ, βββ) ≡ arg max

xxx∈C

Obj(xxx, vpvpvp(θθθ, βββ))

(3)

Regret: We measure the performance of predict+optimize
frameworks using regret. Regret is deﬁned as the cost of
making sub-optimal decisions due to incorrect coefﬁcient
predictions. If we deﬁne xxx∗ = s(vvv) as the optimal solution
of an optimization problem with true objective coefﬁcients
vvv, and xpxpxp = s(vpvpvp) as the optimal solution of an optimization
problem with predicted objective coefﬁcients vpvpvp, then regret
R is:

R(vpvpvp, vvv) = Obj(xxx∗, vvv) − Obj(xpxpxp, vvv)
(4)
The true optimal value Obj(xxx∗, vvv) represents a bound-
ary for the best decisions made with the predicted coefﬁ-
cients. The optimal objective value with predicted coefﬁ-
cients Obj(xpxpxp, vvv) can never exceed the true optimal. There-
fore the minimum value of regret is zero, and it is achieved
when the predicted optimal solution, xpxpxp, is equal to the true
optimal solution, xxx∗. The predict+optimize problem is to
ﬁnd βββ that minimizes R(vpvpvp(θθθ, βββ), vvv).

Transition points: Note that parameterised regret is a
piece wise function. The predicted coefﬁcients vpvpvp(θθθ, βββ) can
only affect regret by changing the solution of the optimiza-
tion problem. These changes are not continuous and only
happen at speciﬁc boundaries of the βββ values. Assume for
the moment a single (unﬁxed) parameter β. We call param-
eter values βt where the optimal solution changes as the
transition points of the piece wise regret function. Note
that for any two points between consecutive transition points
βti < β1 < β2 < βti+1 , s(θθθ, β1) = s(θθθ, β2), therefore

regret(β1) = regret(β2). This suggests mapping the op-
timization problem by identifying intervals deﬁned by the
transition points. Then we can choose any value in those in-
tervals to train model parameters.

i = β1 ∗ θi

1 + β2 ∗ θi

Example 2. Consider a knapsack problem with three items
valued at v1 = 2, v2 = 1, v3 = 3 . The capacity is enough
for only two items. The objective coefﬁcients (selling prices
of each item) of the items are not known but we know the fea-
tures θθθ1 = [−1, 3], θθθ2 = [0, 1], θθθ3 = [1, 1]. We have a linear
model to predict selling prices using the given features. Its
parameters are βββ = [β1, β2] and vp
2 + c
where i ∈ Z, 0 < i < 4. Assume β2 is ﬁxed and is equal to
1, and for simplicity the constant c is also equal to 0. Then
we can express the predicted objective coefﬁcients with lin-
ear functions (see Figure 1a), vp1 = −β1 + 3, vp2 = 1,
vp3 = β3+1. If we inspect Figures 1a and 1b we can see that
although vp continuously changes with β1, there are only
three different solutions (xxx ∈ {(1, 1, 0), (1, 0, 1), (0, 1, 1)})
provided by the solver. Each different solution represents a
sum of linear functions of the chosen items. By combining
all the separate linear functions, we represent the complete
solution space as a piece-wise-linear function (PWLF) seen
in Figure 1b.

In example 2 there are two transition points; β1 = 0, β1 =
2. We can express the solution space of this 0-1 knapsack
problem as






vp1, vp2 ≥ vp3, s(θθθ, β1) ≡ (1, 1, 0) β1 ≤ 0
vp1, vp3 ≥ vp2, s(θθθ, β1) ≡ (1, 0, 1)
vp2, vp3 ≥ vp1, s(θθθ, β1) ≡ (0, 1, 1)

0 ≤ β1 ≤ 2
2 ≤ β1

We deﬁne the predicted optimal value (POV) and true op-

timal value (TOV), for ﬁxed feature value θθθ, as follows:

P OV (βββ) = Obj(s(θθθ, βββ), vpvpvp(θθθ, βββ))
T OV (βββ) = Obj(s(θθθ, βββ), vvv)

Note that the predicted coefﬁcients do not directly affect
the true objective value shown in Figure 1c. However the
transition points of the predicted PWLF are exactly the same
as the transition points of the discrete, true objective. There-
fore, if we identify transition points of the predicted func-
tion, we can use them to dramatically reduce the effort to
map the real objective function.

Our framework works for any arbitrary optimization prob-
lem with a linear objective function. An optimization prob-
lem has a linear objective function when the relationship be-
tween the solution vector and the coefﬁcients are linear:

Obj(xxx, vvv) = xxxT · vvv

(5)

We also assume predicted coefﬁcients are parameterised lin-
ear functions

vp = βββT · θθθ + c
where c is a constant. This is the same restriction
as Demirovic et al. (2020). Equations 5 and 6 mean that the
predicted objective value of the optimization problem can be
expressed as a sum of linear functions.

(6)

Figure 1: Piece wise function construction

Figure 2: Divide and conquer algorithm

(a)
coefﬁcient(vpvpvp)

Predicted

(b) Predicted opti-
mal value(POV)

(c) True optimal
value(TOV)

We now discuss two properties of P OV assuming βββ is a
singleton β. Since we shall use coordinate descent to reason
about P OV while modifying only one parameter, this is the
only case we are interested in.
Lemma 1. P OV is a piecewise linear function.

=

Proof. P OV (β)
=
(arg maxxxx∈C vpvpvp(θθθ, β)xxx)T ·vpvpvp(θθθ, β) = maxxxx∈C xxxT ·vpvpvp(θθθ, β)
Since it is the max of a set of linear functions, it is piecewise
linear.

· vpvpvp(θθθ, β)

s(θθθ, β)T

Lemma 2. P OV is a convex function.

Proof. We need to show that P OV (tβ1 + (1 − t)β2) ≤
tP OV (β1)+(1−t)P OV (β2) for t ∈ [0, 1], β1, β2 arbitrary.
Deﬁne β3 = tβ1 + (1 − t)β2. For i ∈ {1, 2, 3}, let xxxi =
s(θθθ, βi), so xxxT
3 · vpvpvp(θθθ, β1) as xxx1 is the arg
max, and similarly xxxT

1 · vpvpvp(θθθ, β1) ≥ xxxT

3 · vpvpvp(θθθ, β2). Now

2 · vpvpvp(θθθ, β2) ≥ xxxT
P OV (tβ1 + (1 − t)β2)
= xxxT
3 · vpvpvp(θθθ, tβ1 + (1 − t)β2)
= xxxT
3 · ((tβ1 + (1 − t)β2)θθθ + c)
= xxxT
3 · (t(β1θθθ + c) + (1 − t)(β2θθθ + c)
= txxxT
3 · vpvpvp(θθθ, β1) + (1 − t)xxxT
≤ txxxT
1 · vpvpvp(θθθ, β1) + (1 − t)xxxT
= tP OV (β1) + (1 − t)P OV (β2)

3 · vpvpvp(θθθ, β2)
2 · vpvpvp(θθθ, β2)

Corollary 1. For any three values β1 < β2 < β3, the points
(β1, P OV (β1)), (β2, P OV (β2)), (β3, P OV (β3)) are not
collinear iff there is a transition point βt in the range β1 <
βt < β3.

Proof. Since P OV is piecewise linear (Lemma 1), if three
points are not collinear, there must be a transition point
between them. Given P OV is convex (Lemma 2) if three
points are collinear there can be no transition point between
them.

Transition point extraction
Demirovic et al. (2020) represent optimization coefﬁcients
as parameterised linear functions. They solve the optimiza-
tion problem with dynamic programming using piece-wise
linear algebra and parameterised coefﬁcients. The output of
the DP solution gives the piecewise linear function POV.

(a) Sample points
with
and
large
uniform step size.
The
algorithm
ﬁnds a transition
point
the
in
interval [-2,1]

(b) The algorithm
decreases the step
size and narrows
transition
the
the
to
point
interval [-1.5,0.5]

(c) With a pre-
cise step size, the
algorithm identi-
ﬁes two transition
points in the inter-
vals [-1.25,-0.75]
and [-0.25,0.25]

However this method is limited to problems with a DP so-
lution. In addition, the DP solution requires constructing the
full PWLF for every problem set. For larger problems this
may result in long run times. Instead we propose a numerical
approach to extract transition points. Our approach works
for any arbitrary optimization problem with a linear objec-
tive.

Divide and conquer: From corollary 1, we know that
if we compare arbitrary three points of POV and they are
not collinear, then there is at least one transition point be-
tween those values. When we decrease the distance between
these three values, we can accurately identify the location
of transition points. The simplest way to extract transition
points is to sample POV with a ﬁxed step size, and compare
collinearity of consecutive three points. Clearly this brute
force method can be infeasible for many problems except
the easiest ones. Especially for long intervals without transi-
tion points, using a small step size is redundant. With these
insights in mind we apply a divide and conquer algorithm
to sample POV. First we split the search region with ten
uniformly spread points, then we test collinearity of these
points. If we ﬁnd any points that are not collinear, we mark
the intervals deﬁned by these points as transition intervals.
Finding a transition interval means that there is at least one
transition point in the interval. Then we proceed to decrease
the step size as: stepn+1 = stepn
and sample the transi-
10
tion intervals. Finally we repeat ﬁnding transition intervals,
and reducing the step size until the step size reaches a de-
sired minimum. By starting with a large step size and itera-
tively reducing it, we identify long intervals without transi-
tion points with minimal processing (Fig 2).

Coordinate Descent: We described a method to construct
the piece-wise linear function POV and extract transition
points by comparing collinearity of sample points. However
we assumed single dimensional parameterised coefﬁcients.
In reality multi-variate models are widely used to predict
these coefﬁcients. We use coordinate descent to transform
a multivariate linear model into a one dimensional model
(Wright 2015). For a parameter vector βββ = [β1, ..., βm], co-
ordinate descent iterates over βββ. In each iteration one param-
eter, βk, is considered as a variable, and the rest are ﬁxed as
a constant, vp = βk · θk + (cid:80)
n(cid:54)=k βn · θn. Then for each pa-
rameter we consecutively perform transition point extraction

-101231234βvp1vβ+1=2=33-βvp=13pvp111-10123vp1p2+POV...54vp1p3+vp2p2+β1-10123v1+v2TOV...543v1+v3v2+v3β112345-10123β1-3-2POV12345-10123β1-3-2POV12345-10123β1-3-2POVand parameter updates.

After all transition points are identiﬁed we proceed to pick
the best overall transition intervals to update model parame-
ters. This process is explained in detail in the next section.

Parameter Update
In a predict+optimize setting a dataset is a collection of
multiple problem sets. Each problem set has the same con-
straints for the optimization problem, however their coef-
ﬁcients are different. We predict unknown coefﬁcients for
each problem set with the same model parameters, and the
goal of the framework is to train model parameters to mini-
mize the average regret across all problem sets. For a dataset
of size N , and coefﬁcient vectors vvv, the dataset is denoted
D = {vvv(1), ..., vvvN }. We choose the model parameters βββ to
minimize the average regret R.

βββ ≡ arg min

1
N

N
(cid:88)

n=0

R(vvv(n), vpvpvp

(n))

, β(i)

t1 , ...β(i)

Transition point comparison: We express the true objec-
tive value (TOV) of each problem set as a piece-wise func-
tion. The extraction method provides the transition points
of each problem set. Let T (i) = {β(i)
tL } be the set
of transition points of size L for a problem set i. We con-
struct the intervals of the piece wise function as I (i) =
{[β(i)
t(l+1)], 0 < l < L, l ∈ Z}. To ﬁnd the optimal
tl
model parameters for a problem set i, we can simply cal-
culate TOV for each interval and pick the best interval. As
a single sample point for each interval is enough to cal-
culate TOV, we choose the mid points of the intervals for
calculations. Let Imid represent the set of mid points then
β(i)
(i)). However, the optimal
opt = arg minβ∈I (i)
parameter for each problem set can be different. To ﬁnd the
optimal parameters over all problem sets, we compare over
every interval from each problem set.

R(vvv(i), vpvpvp

mid

βopt ≡ arg min
I (i)
mid

β∈

N
(cid:83)
i=1

1
N

N
(cid:88)

n=0

R(vvv(n), vpvpvp

(n))

With coordinate descent we perform these comparisons
for each parameter and update the parameters individually.
Mini-batch:
In our framework we use mini-batches to
train the model parameters. A mini-batch represents a sub-
set of problem sets. When using mini-batches we construct
a quasi-gradient in the direction of the global minimum of
that particular mini-batch. Then we update the model pa-
rameters with the quasi gradient. For a mini-batch i: βnew =
βold + learning rate · (β(i)

opt − βold)

Greedy Methods
To extract the transition points and compare them, we have
to repeatedly solve an optimization problem. Many opti-
mization problems are NP-hard, and obviously as the prob-
lem size increases it can become expensive to perform these
actions. Therefore we propose a greedy method to partially

extract transition points and a greedy method to compare
only the best transition points.

Divide and Learn Max (DnL-MAX) : Normally we
compare all intervals for all problem sets. If we assume
there are L intervals for each problem set i and a total of N
problem sets, then we solve the optimization problem N 2L
times. For a full method the complexity scales both with the
number of transition points and the size of the data. Instead
we propose to ﬁrst choose the optimal parameter for each
problem set i by β(i)
(i)), and
opt = arg minβ∈I (i)
then compare only the best parameters with the other prob-
lems sets:

R(vvv(i), vpvpvp

mid

βopt = arg min
β(i)
opt

β∈

N
(cid:83)
i=1

1
N

N
(cid:88)

n=0

R(vvv(n), vpvpvp

(n))

This reduces complexity by solving only (N − 1)N + LN
optimization problems, and the number of transition points
has minimal effect on the comparison complexity.

Divide and Learn Greedy (DnL-Greedy) : Our divide
and conquer algorithm repeatedly compares the collinearity
of POV samples. Depending on the complexity of the prob-
lem and the minimum step size for sample points, there can
be many redundant transition points. We propose a greedy
extraction method to stop the extraction at the ﬁrst transition
point βt that improves TOV over the old parameter βold.
We also prioritize regions around the old model parameter
for transition point search. We observed that although TOV
is not a convex function, the optimal model parameters are
clustered in similar regions. Our motivation with this greedy
method is to quickly iterate over parameters and bypass re-
dundant sampling. Note that as we use only one transition
point for each problem set, DnL-Greedy automatically in-
cludes the greedy comparison of DnL-MAX.

The greedy approaches do not guarantee ﬁnding the
global minimum. However we show empirically they
achieve the same performance as the full method, and re-
duce the run time dramatically.

Evaluation
In this section we detail our experiments. We experiment
on two optimization problems: 0-1 knapsack and schedul-
ing. We run experiments for four exact methods: DnL, DnL-
MAX, DnL-Greedy, dynamic programming (DP) (Demirovic
et al. 2020), two surrogate methods: SPO-Relax (Mandi
et al. 2020), QPTL (Ferber et al. 2020) and one indirect
method: ridge regression.

Dataset: We use the dataset from the ICON energy chal-
lenge (Simonis et al. 2014) for both knapsack and schedul-
ing problems. Data samples are collected from real electric-
ity prices every 30 minutes, from 2011 November to 2013
January. Wind forecast, wind speed, Co2 intensity, temper-
ature, load forecast and price forecast are used to predict
actual energy prices. Note that predicting energy prices is
challenging so even the best models have substantial predic-
tion error. The same dataset was used in previous work on
predict+optimize (Mandi et al. 2020; Demirovic et al. 2020).

Figure 3: Unit Knapsack: showing average and one standard
deviation. Note that the graph is truncated at 40 for readabil-
ity.

Figure 4: Weighted Knapsack: showing average and one
standard deviation. SPO-Relax and QPTL are truncated at
400 for some instances for readability.

In total there are 37877 data samples, each 48 data sam-
ples, representing a day, create a problem set. Therefore
we can only use 789 optimization problems to train pre-
dict+optimize models. We split the data set into 70% train
set, 10% validation set and 20% test set. Correspondingly
we have 552, 79 and 157 optimization problems for train-
ing, validation and testing, respectively. In order to under-
stand how models work for different distributions, we split
the dataset into 5 folds. For each fold we train every model
10 times and use the iteration with the least validation re-
gret. We report the performance of models over the mean
and standard deviation of all folds.

Knapsack problem: We consider a 0-1 knapsack of
n items, where we are given a capacity limit, W , item
weight w = [w1, w2..., wn], and have to predict item
values vvv = [v1, v2..., vn]. A 0-1 solution vector xxx =
arg maxxxxT ·w≤W xxxT · vvv decides the chosen items. We run
knapsack experiments for both unit weights and varying
weights. The original dataset does not have item weights,
and we generate the weights synthetically. Knapsack prob-
lems with high correlation between item value and weight
are considerably harder to solve than those with weak cor-
relation (Pisinger 2005). We create exactly correlated knap-
sack problems by choosing weight values in {3, 5, 7}, and
multiplying by the true energy price to generate their true
value. We experiment on varying capacity limits from 5% to
90%. For unit knapsack the capacity limits range from 5-45
(10%-90%) and for weighted knapsack the capacity limits
range from 12-220 (5%-90%).

Scheduling: We test on energy cost aware scheduling
problems. The scheduling problems are a simpliﬁed versions
of the ICON challenge (Mandi et al. 2020). There are M
machines and N jobs. Each machine has a resource capacity
Cm. Each job has a resource requirement Rn, power con-
sumption Pn and a duration Dn. Every job also has an ear-

liest starting time ten and a latest ﬁnishing time tln. A job
can only be run on one machine, and once a job is being pro-
cessed it cannot be split. All jobs have to be ﬁnished in the
24 hour period. The goal of the scheduling is to minimize
energy costs by taking energy prices into account. Energy
prices are not known beforehand and have to be predicted.
We consider three different loads with 3,5,3 machines and
15,20,10 machines correspondingly.

Experiments
The models are trained with Intel(R) Xeon(R) Gold 6254
CPU @ 3.10GHz processors using 8 cores with 3.10 Ghz
clock speed. We use Gurobi Optimization (2020) to solve
knapsack and scheduling problems. For the knapsack prob-
lems max training time is set to 4000 seconds (≈ 1 hour).
For scheduling problems max training time is set to 12000
seconds (≈ 3.3 hours). We tune hyper-parameters via grid
search for surrogate models and we use early stopping for
all models (Bishop 2006). DnL hyper-parameters are as fol-
lows: We set the mini-batch size to 32 and learning rate to
0.1. We warmstart the model parameters with ridge regres-
sion (Pratt and Jennings 1996). The search space for transi-
tion points are bounded relative to the current value of the
parameter as [β − 1.5β, β + 1.5β]. Similarly the minimum
step size of models are relative to the parameter and is equal
to stepmin = β/10.

Unit knapsack: Unit knapsack (Figure 3) is a relatively
simple optimization problem and there is not a large differ-
ence between regression and predict+optimize models. All
variations of DnL perform identically to DP, and this sug-
gests greedy approaches perform as well as their full coun-
terparts. Unlike regression and exact methods, the surro-
gates’ performances are sensitive to the changes. SPO-Relax
fails to capture the optimization problem for low capacities,
while QPTL fails to capture the high capacity problem.

Weighted knapsack: The exact methods outperform sur-

510152025303540Capacities0510152025303540Test RegretKnapsack-Unit WeightsDnLDnL-MAXDnL-GreedySPO-RelaxQPTLDPRidge Regression1224487296120144172196220Capacities050100150200250300350400Test RegretKnapsack-WeightedDnLDnL-MAXDnL-GreedySPO-RelaxQPTLDPRidge RegressionFigure 5: Low capacity knapsack, test regret vs epoch plots,

(a) weighted knapsack capac-
ity 12,fold 1. SPO-Relax con-
verges to low regret, QPTL
overﬁts

(b) weighted knapsack capac-
ity 12,fold 2. SPO-Relax con-
verges to high regret, QPTL
overﬁts

rogates for all capacities and regression for low and high ca-
pacities (Figure 4). Their advantage is more clear at the ex-
treme capacities {12,196,220}. For capacities {72,96,196}
DnL outperforms DP. At these capacities DP requires long
time to build the piece-wise function and fails to ﬁnish an
epoch within the time limit. In contrast for the same weights
DnL-Greedy can train an epoch under 10 seconds. Similar to
the unit knapsack problem SPO-Relax has higher regret for
low capacities. We have observed that SPO-Relax is very
sensitive to the data distribution for the weighted knapsack
problem. For some folds it successfully converges at a low
regret minimum. However for other folds, it converges at a
higher regret than regression (Fig 5). QPTL outperforms
regression for low capacities but for high capacities it has
the worst regret of all models. QPTL also tends to overﬁt
the problem, though early stopping helps to identify a min-
imum (Fig 5). These experiments show that exact methods
are able to accurately search the underlying knapsack prob-
lem for all capacities, and they are more robust to changes
in constraints.

Loads

DnL
DnL-Greedy
SPO-Relax
QPTL
Regression

1

2

3

8382 ± 3616
8625 ± 3829
8421 ± 4144
6584 ± 2157
6544 ± 4360
6544 ± 4360
6544 ± 4360

17168 ± 7292
15834 ± 6259
18454 ± 8302
27393 ± 12606
15672 ± 6542
15672 ± 6542
15672 ± 6542

10347 ± 5321
10347 ± 5321
10347 ± 5321
10895 ± 5626
11086 ± 4752
10544 ± 5138
10919 ± 6086

Table 1: ICON scheduling problems mean regret and stan-
dard deviations

works. For Load 1, regression has the best regret, QPTL
is the best predict+optimize framework, and the other three
have similar regrets. Interestingly for some folds of Load
2, QPTL cannot reason over regret. We believe the solution
space of these folds may resemble the high capacity knap-
sacks. We also observe that SPO-Relax has a higher regret
than regression for Load 2. Although DnL seems to have a
high regret as well, this is a result of slow training and not
completing within the 4 hour limit. The greedy variation is
able to complete the training and has a similar regret to the
regression. For Load 3, we do not see a signiﬁcant difference
between the frameworks.

Scalability/runtime Scalability is the biggest issue for ex-
act methods. Combinatorial optimization problems are ex-
pensive to solve, and exact methods require solving multi-
ple instances to map each problem set. Therefore we pro-
posed greedy methods, and observed improved efﬁciency
over the existing exact method DP. For example, DP requires
more than an hour to train an epoch for knapsack capacities
{72,96,196} and DnL-Greedy can complete the training of
an epoch under 10 seconds. For other capacities, DP requires
200–400 seconds for an epoch, while we require 2–10 sec-
onds. There are two reasons for this. First, the DP method
has to make use of a dynamic programming solution com-
patible with piece wise linear algebra, which may not be
the most efﬁcient solution approach. Second, the DP con-
structs the entire piecewise function every time. Since the
number of transition points can increase exponentially with
problem size, our greedy methods can avoid computing ir-
relevant transition points.

Although DnL-Greedy is more scalable than the previ-
ous exact methods, it requires more processing power than
surrogate methods. This difference is emphasized for larger
problems. For scheduling problems (load3) DnL-Greedy re-
quires 20 and DnL requires 30–60 minutes to train an epoch.
By comparison SPO-Relax, which uses the relaxation of
the optimization problem, can ﬁnish an epoch in under a
minute. For a fair comparison we ran SPO-Relax for the
same amount of time that we ran DnL-Greedy, but we did
not observe any change in the convergence behaviour or the
output regret.

Conclusion

Scheduling: Compared to knapsack problems scheduling
problems are more complex and do not have a dynamic pro-
gramming solution. As such DP method is not applicable for
these problem sets and to the best of our knowledge DnL is
the ﬁrst exact method applicable for an arbitrary MIP prob-
lem with a linear objective. Although all frameworks have
different loss models, they all converge at a similar point
and we do not identify one method that clearly outperforms
all the others. We believe the underlying problem for these
loads, like medium capacity knapsack problems, are similar
to MSE and understanding the underlying regression prob-
lem is also a successful result for predict+optimise frame-

Predict+optimize problems are challenging due to the com-
binatorial nature of the optimization problem. We propose a
new method to train parameters using regret, rather than a
surrogate or relaxation. The only previous existing method
requires a DP formulation of the optimization problem, in
contrast ours can be applied to any optimization problem
with a linear objective. By using greedy methods to ﬁnd
transition points we can substantially reduce the amount of
search required. Our framework outperforms regression and
surrogates for weighted knapsack problems and unlike the
previous DP method, is able to reason over more complex
problems.

0123456050100150200250Test RegretepochsDnLDnL-MaxDnL-GreedySPO-RelaxQPTLRidge RegressionKnapsack Capactity: 12,Fold:10123456050100150200250300DnLDnL-MaxDnL-GreedySPO-RelaxQPTLRidge RegressionKnapsack Capactity: 12,Fold:2Test RegretepochsReferences
Amos, B.; and Kolter, J. Z. 2017. OptNet: differentiable
optimization as a layer in neural networks. In Proceedings
of the 34th International Conference on Machine Learning-
Volume 70, 136–145.

Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio,
S. 2016. Neural combinatorial optimization with reinforce-
ment learning. arXiv preprint arXiv:1611.09940 .

Bengio, Y. 1997. Using a ﬁnancial training criterion rather
than a prediction criterion. International Journal of Neural
Systems 8(04): 433–443.

Bishop, C. M. 2006. Pattern recognition and machine learn-
ing. springer.

Demirovic, E.; Bailey, J.; Chan, J.; Guns, T.; Kotagiri, R.;
Leckie, C.; and Stuckey, P. J. 2019a. A Framework for Pre-
dict+Optimise with Ranking Objectives: Exhaustive Search
for Learning Linear Functions for Optimisation Parame-
ters. In Kraus, S., ed., Proceedings of the 28th International
Joint Conference on Artiﬁcial Intelligence, 1078–1085. IJ-
CAI Press. doi:https://doi.org/10.24963/ijcai.2019/151.

Demirovic, E.; Stuckey, P. J.; Bailey, J.; Chan, J.; Leckie,
C.; Ramamohanarao, K.; and Guns, T. 2019b. Predict+ Op-
timise with Ranking Objectives: Exhaustively Learning Lin-
ear Functions. In IJCAI, 1078–1085.

Demirovic, E.; Stuckey, P. J.; Guns, T.; Bailey, J.; Leckie,
C.; Kotagiri, R.; and Chan, J. 2020. Dynamic Program-
In Conitzer, V.; and Sha, F.,
ming for Predict+Optimise.
eds., Proceedings of the Thirty-Fourth AAAI Conference on
Artiﬁcial Intelligence (AAAI-20), 1441–1451. AAAI Press.
doi:https://doi.org/10.1609/aaai.v34i02.5502.

Donti, P.; Amos, B.; and Kolter, J. Z. 2017. Task-based
In
end-to-end model learning in stochastic optimization.
Advances in Neural Information Processing Systems, 5484–
5494.

Elmachtoub, A. N.; and Grigas, P. 2017. Smart” predict,
then optimize”. arXiv preprint arXiv:1710.08005 .

Ferber, A.; Wilder, B.; Dilkina, B.; and Tambe, M. 2020.
MIPaaL: Mixed Integer Program as a Layer. In AAAI, 1504–
1511.

Gurobi Optimization, L. 2020. Gurobi Optimizer Reference
Manual. URL http://www.gurobi.com.

Horvitz, E.; and Mitchell, T. 2010. From data to knowledge
to action: A global enabler for the 21st century. Computing
Community Consortium 1.

Ifrim, G.; O’Sullivan, B.; and Simonis, H. 2012. Properties
In International
of energy-price forecasts for scheduling.
Conference on Principles and Practice of Constraint Pro-
gramming, 957–972. Springer.

Kao, Y.-h.; Roy, B. V.; and Yan, X. 2009. Directed regres-
In Advances in Neural Information Processing Sys-
sion.
tems, 889–897.

Li, Z.; Chen, Q.; and Koltun, V. 2018. Combinatorial opti-
mization with graph convolutional networks and guided tree

search. In Advances in Neural Information Processing Sys-
tems, 539–548.
Lim, A. E.; Shanthikumar, J. G.; and Vahn, G.-Y. 2012. Ro-
bust portfolio choice with learning in the framework of re-
gret: Single-period case. Management Science 58(9): 1732–
1746.

Luo, C.; Qiao, B.; Chen, X.; Zhao, P.; Yao, R.; Zhang, H.;
Wu, W.; Zhou, A.; and Lin, Q. 2020. Intelligent Virtual Ma-
chine Provisioning in Cloud Computing. In Bessiere, C., ed.,
Proceedings of the Twenty-Ninth International Joint Confer-
ence on Artiﬁcial Intelligence, IJCAI-20, 1495–1502. Inter-
national Joint Conferences on Artiﬁcial Intelligence Orga-
nization. doi:10.24963/ijcai.2020/208. URL https://doi.org/
10.24963/ijcai.2020/208. Main track.

Mandi, J.; Guns, T.; Demirovic, E.; and Stuckey, P. J. 2020.
Smart Predict-and-Optimize for hard combinatorial opti-
mization problems. In Conitzer, V.; and Sha, F., eds., Pro-
ceedings of the Thirty-Fourth AAAI Conference on Artiﬁ-
cial Intelligence (AAAI-20), 1603–1610. AAAI Press. URL
https://doi.org/10.1609/aaai.v34i02.5521.

Niculae, V.; Martins, A.; Blondel, M.; and Cardie, C.
2018. SparseMAP: Differentiable Sparse Structured Infer-
In International Conference on Machine Learning,
ence.
3799–3808.

Pisinger, D. 2005. Where are the hard knapsack problems?
Computers & Operations Research 32(9): 2271–2284.
Poganˇci´c, M. V.; Paulus, A.; Musil, V.; Martius, G.; and
Rolinek, M. 2020. Differentiation of Blackbox Combi-
In International Conference on Learn-
natorial Solvers.
ing Representations. URL https://openreview.net/forum?id=
BkevoJSYPB.

Pratt, L.; and Jennings, B. 1996. A survey of connectionist
network reuse through transfer. In Learning to learn, 19–43.
Springer.

Simonis, H.; O’Sullivan, B.; Mehta, D.; Hurley, B.; and
Energy-Cost Aware Schedul-
De Cauwer, M. 2014.
ing/Forecasting Competition. URL http://www.csplib.org/
Problems/prob059/.
Thapper, J.; and ˇZivn`y, S. 2018. The limits of SDP relax-
ations for general-valued CSPs. ACM Transactions on Com-
putation Theory (TOCT) 10(3): 1–22.
Wilder, B.; Dilkina, B.; and Tambe, M. 2019. Melding the
data-decisions pipeline: Decision-focused learning for com-
binatorial optimization. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 33, 1658–1665.
Wright, S. J. 2015. Coordinate descent algorithms. Mathe-
matical Programming 151(1): 3–34.

