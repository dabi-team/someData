2
2
0
2

n
u
J

3
2

]

G
L
.
s
c
[

2
v
3
2
7
0
0
.
1
0
2
2
:
v
i
X
r
a

A Mixed-Integer Programming Approach to Training
Dense Neural Networks

Vrishabh Patil, Yonatan Mintz
Department of Industrial and Systems Engineering, University of Wisconsin-Madison, Madison, WI 53706, USA
vmpatil@wisc.edu, ymintz@wisc.edu

Artiﬁcial Neural Networks (ANNs) are prevalent machine learning models that are applied across various

real world classiﬁcation tasks. However, training ANNs is time-consuming and the resulting models take

a lot of memory to deploy. In order to train more parsimonious ANNs, we propose a novel mixed-integer

programming (MIP) formulation for training fully-connected ANNs. Our formulations can account for both

binary and rectiﬁed linear unit (ReLU) activations, and for the use of a log-likelihood loss. We present

numerical experiments comparing our MIP-based methods against existing approaches and show that we

are able to achieve competitive out-of-sample performance with more parsimonious models.

Key words : mixed-integer programming, neural networks, mathematical optimization, non-linear

optimization

1. Introduction

Artiﬁcial Neural Networks (ANNs) have become ubiquitous across numerous classiﬁcation tasks in

domain areas such as natural language processing and image recognition. Typically, these exist-

ing models are trained through the use of end-to-end back propagation and stochastic gradient

descent (SGD). These methods attempt to minimize a global loss function which is a function of the

network architecture and data by taking steps in the direction of a negative gradient that is calcu-

lated incrementally using the chain rule (Goodfellow et al. 2017). While these end-to-end methods

have been shown to have good empirical performance (Krizhevsky et al. 2012, Szegedy et al. 2015)

they suﬀer from several well known limitations. For example, there are several computational

challenges with end-to-end methods such as the vanishing gradient problem (where the gradient

computed through back propagation fails to update the parameters for large network architectures)

(Bengio et al. 1994, Hochreiter et al. 2001), and the numerical convergence of SGD is sensitive to

1

 
 
 
 
 
 
2

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

random initialization (Bottou et al. 1991, Sutskever et al. 2013). However, a major shortcoming

that underlies these computational challenges is that to obtain strong out of sample performance,

models using end-to-end methods often times need to have complex network architectures and

host numerous parameters while training. This means that training these models often costly with

regards to time and memory (Taylor et al. 2016). Given this challenge, recent research has explored

if more parsimonious and stable ANNs can be trained without using end-to-end training methods

(Elad et al. 2018, L¨owe et al. 2019, Belilovsky et al. 2019). In this paper, we continue to explore

this line of inquiry, by proposing none end-to-end ANN training methods based on principles from

mixed-integer programming (MIP) and constrained optimization. In particular we present MIP

formulations for network architectures that contain either binary or rectiﬁed linear unit (ReLU)

activation functions. In addition our contributions include a MIP formulation for the negative log

likelihood cost in multi-class classiﬁcation, making our formulations compatible with common soft-

max implementations. We also discuss how our formulations can be used in a layer by layer training

approach which can be used eﬀectively as a pre-training procedure for large network architectures.

Finally, we present computational results of how our methods compare to SGD-based training

methods in terms of resulting model size and performance.

2. Related Works

In this section, we present some related streams of literature that have explored the intersection

of ANNs and MIP, and discuss how they relate to our work. We discuss some work related to

training ANNs using MIPs, followed by papers that focus on the convexiﬁcation of the training

problem, along with studies involving the amalgamation of MIPs with ANNs. Finally, we review

methodological papers that relate to greedy layer-wise algorithms in machine learning.

In recent years, MIPs have gained popularity in training neural networks. Constraint Program-

ming, MIP, and a hybrid of the two have been used to train Binarized Neural Networks (BNNs),

a class of neural networks with the weights and activations restricted to the set {-1, +1}. In

Toro Icarte et al. (2019) the authors report on the availability of an optimal solution at an accept-

able optimality gap, given limitation on the size of the training set. To relax the binarized restriction

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

3

on weights, Thorbjarnarson and Yorke-Smith propose a formulation to train Integer Neural Net-

works with diﬀerent loss functions. However, all the experiments in the paper were restricted to 100

samples. Continuing along these lines, Kurtz and Bah (2021) has also develop a formulation for

integer weights, and oﬀer an iterative data splitting algorithm that uses the k-means classiﬁcation

method to train on subsets of data with similar activation patterns.

In a diﬀerent stream, Askari et al. (2018) propose using convex relaxations for training ANNs.

The authors also explore non gradient-based approaches and initialized weights to accelerate con-

vergence of gradient-based algorithms. MIP formulations have been used for already trained net-

works to provide adversarial samples that can improve network stability (Fischetti and Jo 2017,

Anderson et al. 2020, Tjeng et al. 2017). Finally, there has been research directed towards using

ANNs to solve MIPs (Nair et al. 2020). In contrast to the previous literature, we propose meth-

ods that directly use MIP techniques for training ANNs as opposed to only being used in their

evaluation.

With regards to pre-training neural networks using greedy layer-wise algorithms, models

introduced in Bengio et al. (2007) have been explored actively in the machine learning litera-

ture (Erhan et al. 2010, Belilovsky et al. 2019, Elad et al. 2018). The Greedy Infomax algorithm

(L¨owe et al. 2019) has also been shown to have good performance in training ANNs without the

need for end-to-end back-propagation. These results indicate that training deep networks through

decoupled training can still result in strong out of sample performance.

Our contributions to this ﬁeld of literature are threefold: we propose a tight MIP formulation

to train ANNs without integrality constraints on the weight matrix and develop a novel approach

that addresses nonlinear activations. We also formulate a negative log-likelihood loss that allows

for a soft-max output. Finally, we further contribute to literature pertaining the greedy layer-wise

algorithms by showing that the use of MIP in solving the layer by layer training problems can

result in more parsimonious ANN models that achieve competitive out of sample performance.

4

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

3. MIP Formulations for ANN Training

Suppose we have a data set composed of ordered pairs (xn, yn) for n = 1, ..., N where xn ∈ X ⊂ R

d

is the feature vector of data point n and yn ∈ J ⊂ Z is its label. We use the notation [N ]m =

{m, m + 1, ..., N } for any two integers N > m and assume that |J | < ∞. Our goal is to ﬁnd a

function f : R

d 7→ J that is parametrized by θ ∈ Θ such that for any x ∈ X , f (x; θ) is able to closely

predict the value of the corresponding y. This f is obtained by ﬁnding ˆθ ∈ Θ that minimizes the

empirical negative log likelihood loss L : J × J 7→ R that is ˆθ = argminθ∈Θ

N
n=1 L(f (x, θ), y). We

assume f is an ANN that can be expressed as a functional composition of L vector valued functions

P

that is f (x; θ) = hL ◦ hL−1 ◦ ... ◦ h1 ◦ h0(x). Here each function hℓ(·, θℓ) is referred to as a layer of

the neural network, we assume that h0 : X 7→ RK, hL : R

K 7→ J , and hℓ : R

K 7→ R

K , we denote each

component of hℓ as hk,ℓ for ℓ ∈ [L − 1]0 and k ∈ [K]1, and each component of hL as hj,L for j ∈ [J ]1.

We refer to these components as units or neurons. We refer to the index ℓ as the layer number of the

ANN, and the index k as the unit number. The layer corresponding to the index L is referred to as

the output layer, and all other indices are referred to as hidden layers. Each unit has the functional

form: hk,0 = σ(α⊤

k,0x + βk,0), hk,ℓ = σ(α⊤

k,ℓhℓ−1 + βk,ℓ) for ℓ ∈ [L − 1]1, hj,L = ϕ(α⊤

j,LhL−1 + βj,L),

where σ : R 7→ R is a non-linear function applied over the hidden layers ℓ = 0, ..., L − 1 called the

activation function, ϕ : R 7→ R is a diﬀerent non-linear function applied over the output layer L,

and α and β are the weights and biases matrices respectively where (α, β) = θ. The notation αa,b,ℓ

indicates the weight being applied to unit index a in layer ℓ − 1 for the evaluation of unit index b

in layer ℓ. Likewise, βk,ℓ indicates the bias associated with unit index k in layer ℓ of the network.

We consider two potential activation forms, either binary or ReLU activation. If σ is the binary

activation then we assume for any input z ∈ R, σ(z) = 1[z ≥ 0], where 1 is the indicator function.

If σ is a ReLU activation function then σ(z) = max{z, 0}.

Our goal is to show how the above training problem and model can be formulated as a MIP.

The key to our reformulation is the introduction of decision variables hn,k,ℓ which correspond to

the unit output of unit k in layer ℓ when the ANN is evaluated at data point with index n. Having

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

5

these decision variables be data point dependant, and ensuring that αk′ ,k,ℓ, βk,ℓ are the same across

all data points forms the back bone of our formulation. Thus, if xn,i denote the ith feature value

of data point n, the general form of the optimization problem is:

N

min

L(hn,j,L, yn,j)

n=1
X

j∈J
X

subject to

d

hn,k,0 = σ(

αi,k,0xn,i + βk,0), ∀ k, n ∈ [K]1 × [N ]1

i=1
X
K

hn,k,ℓ = σ(

αk′ ,k,ℓhn,k′,ℓ−1 + βk,ℓ), ∀ ℓ, k, n ∈ [L − 1]1 × [K]1, ×[N ]1

k′=1
X
K

hn,j,L = ϕ(

αk′,j,Lhn,k′,L−1 + βj,L), ∀j, n ∈ |J | × [N ]1

Xk′=1

αi,k,0, βk,0, αk′,k,ℓ, βk,ℓ, αk′ ,j,L, βj,L ∈ Θ, ∀ i, ℓ, k, k′, j ∈ [d]1 × [L]1 × [K]2

1 × |J |

(1)

(2)

(3)

(4)

(5)

3.1. MIP Formulation of Loss Function

First we focus on the reformulation of the objective function

N
n=1

j∈J L(hn,j,L, yn,j) in terms of

a set of linear constraints and linear objective function. Note that the the Lth layer outputs are

P

P

the only ones that interact directly with the loss function, so we focus on these decision variables

and leave the discussion of the model parameters when considering individual unit activations.

Objectives such as prediction inaccuracy and ℓ0 or ℓ1 losses can be trivially formulated using linear

and MIP techniques. However, in practice a common loss function used for training ANNs in clas-

siﬁcation tasks, is the negative log likelihood applied to a soft-max distribution (Goodfellow et al.

2017). This kind of loss can be considered analogous to minimizing the log likelihood of a multino-

mial distribution across the classes in J , thus outputting a predictive distribution over the classes.

This loss is particularly appealing as it harshly penalizes incorrect classiﬁcations and is numerically

stable (Goodfellow et al. 2017). The equation for the soft-max activation function for the output

of the ﬁnal layer hn,j,L is ϕ(hn,j,L) =

exp(hn,j,L)

Pj′∈J exp(hn,j′,L) , and the resulting negative log likelihood loss

is − log(

N

n=1(ϕ(hn,j,L)1[yn=j])).

Our main result is:

Q

6

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

Proposition 1. The negative

log

likelihood

loss

applied

to

the

soft-max

activation

− log(

N

n=1(ϕ(hn,j,L)1[yn=j])) is within a constant additive error of the optimal value of the

following linear optimization problem:

Q

min
hn,j,L,ωn

N

J

n=1
X

j=1
X

(cid:8)

1[yn = j](ωn − hn,j,L) : ωn ≥ hn,j′,L, j′ ∈ [J ]1, n ∈ [N ]1

(6)

(cid:9)

Here we present a brief sketch of the proof, for the detailed proof of this proposition please see the

appendix. The main insight used is the fact that the function log

n
i=1 exp(xi) = Θ(maxi∈[N ]0 xi)

(Calaﬁore and El Ghaoui 2014), where Θ is the Big Theta notion that characterizes the function

P

log

exp to be bounded asymptotically both above and below by the max function (Cormen et al.

P

2022), and the fact that the minimization of a maximum can be written using linear constraints

(Wolsey and Nemhauser 1999).

3.2. MIP Formulation for Binary Activated ANNs

In this section, we present a MIP reformulation for ANN units with binary activation that can

be solved using commercial solvers. We rewrite Constraints (2),(3),(4) for a single unit as hn,k,0 =

1[

d
i=1 αi,k,0xi + βk,0 ≥ 0], hn,k,ℓ = 1[

K
k′=1 αk,k′ ,ℓhk′,ℓ + βk,ℓ ≥ 0], hn,j,L =

K
k′ =1 αk′ ,j,Lhk′,L−1 + βj,ℓ

P

respectively. Note that for all layers that are not the input layer, the above constraints contain bi-

P

P

linear products which make this formulation challenging to solve. As such we propose the following

reformulation:

Proposition 2. In the binary activation case, the Constraints (2),(3),(4) can be reformulated as

a set of MIP constraints. Speciﬁcally for all k ∈ [K]1, n ∈ [N ]1 Constraint (2) can be reformulated

as:

(αi,k,0xn,i) + βk,0 ≤ M hn,k,0

(αi,k,0xn,i) + βk,0 ≥ ǫ + (−M − ǫ)(1 − hn,k,0)

(7)

(8)

d

i=1
X
d

i=1
X

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

For all ℓ ∈ [L − 1]1, k ∈ [K]1, n ∈ [N ]1 Constraints (3) can be reformulated as:

K

(zn,k′,k,ℓ) + βk,ℓ ≤ M hn,k,ℓ

k′ =1
X
K

(zn,k′,k,ℓ) + βk,ℓ ≥ ǫ + (−M − ǫ)(1 − hn,k,ℓ)

Xk′ =1
zn,k′,k,ℓ ≤ αk′ ,k,ℓ + M (1 − hn,k′,ℓ−1), ∀ k′ ∈ [K]1

zn,k′,k,ℓ ≥ αk′ ,k,ℓ − M (1 − hn,k′,ℓ−1), ∀ k′ ∈ [K]1

− M hn,k′,ℓ−1 ≤ zn,k′,k,ℓ ≤ M hn,k′,ℓ−1, ∀ k′ ∈ [K]1

And for all j ∈ J, n ∈ [N ]1, Constraints (4) can be reformulated as:

K

hn,j,L ≤

(zn,k′,j,L) + βj,L

k′=1
X
K

hn,j,L ≥

(zn,k′,j,L) + βj,L

Xk′=1

zn,k′ ,j,L ≤ αk′ ,j,L + M (1 − hn,k′,L−1), ∀ k′ ∈ [K]1

zn,k′ ,j,L ≥ αk′ ,j,L − M (1 − hn,k′,L−1), ∀ k′ ∈ [K]1

− M hn,k′,L−1 ≤ zn,k′,j,L ≤ M hn,k′,L−1, ∀ k′ ∈ [K]1

ωn ≥ hn,j,L

hn,j,L + hn,j′,L − 2hn,j,L ≤ −ǫ + M rn,j,j′, ∀ j, j′ ∈ [J ]2

1, j 6= j′

hn,j,L + hn,j′,L − 2hn,j,L ≥ ǫ − M (1 − rn,j,j′), ∀ j, j′ ∈ [J ]2

1, j 6= j′

7

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

Constraints (7) and (8) are big M constraints that, when combined with the integrality of

hn,k,0, impose the output of the ﬁrst hidden layer to be 1 if the linear combination of the units

of the input vector summed with the bias term is greater than or equal to some small constant

ǫ, and 0 otherwise. To reformulate Constraints (3), we introduce an auxiliary variable zn,k′,k,ℓ =

αk′,k,ℓhn,k′,ℓ−1. Constraints (9) and (10) are then similar to Constraints (7) and (8), and deﬁne the

output of the remaining hidden layers ℓ ∈ [L − 1]1. We leverage the fact that the bi-linear term is a

8

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

product of a continuous and binary variable for Constraints (11), (12), and (13); zn,k′,k,ℓ = 0 when

hn,k′,ℓ−1 = 0 or zn,k′ ,k,ℓ = αk′ ,k,ℓ when hn,k′,ℓ−1 = 1. A symmetric argument holds for the deﬁnition

of Constraints (16), (17), and (18), which ensures zn,k′,j,L = αk′,j,Lhn,k′,L−1, the bi-linear terms

associated with the output layer. Considering that the soft-max activation of the output layer is

captured by the objective function as deﬁned in Section 3.1, Constraints (14) and (15) guarantee

that the output of the ﬁnal layer is the linear combination of the units of the activated outputs

of the penultimate layer summed with the bias term. Constraints (19) forces ωn to be the max of

the components of the output vector. Finally, Constraints (20) and (21) ensure that no two units

in the output layer are equal. The idea behind these two constraints depends on the fact that the

average of two variables is equal to one of the variables if and only if the two variables are equal.

As such, we introduce the binary variables rn,j,j′ ∈ B to deﬁne Constraints (20) and (21) as big M

constraints.

A full proof of this proposition can be found in the appendix. However, we will present a sketch

here. The main techniques for this proof rely on ﬁrst using a big M formulation for disjunctive

constraints (Wolsey and Nemhauser 1999) to model the binary activation. Then bi-linear terms are

reformulated using the techniques applied to products of binary and continuous variables. We note

that for the output of the ﬁnal layer to give a unique prediction with the log likelihood soft-max

loss, we add Constraints (20) and (21). We call these constraints diversifying constraints, which

force the optimization problem to assign diverse values to hn,j,L, ∀ n ∈ [N ]1, j ∈ J . Without these

constraints, we could technically minimize our loss by setting the output of the ﬁnal layer, hn,j,L

to all be equal. However, owing to the fact that no data point can be labeled with multiple classes,

we require unique unit outputs. The complete mathematical model for our problem can be found

in the appendix.

3.3. MIP Formulations for ReLU Activated ANNs

ANNs with ReLU activated hidden layer units are another class of models commonly used in

practice. ReLU is commonly paired with soft-max loss functions as an activation function for

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

9

training deep supervised ANNs (Goodfellow et al. 2017). ReLU activations are especially beneﬁcial

in negating the vanishing gradient problem since the activation function has a gradient of 1 at

all activated units (Glorot et al. 2011). Although training ANNs with MIPs directly addresses the

vanishing gradient problem by eliminating the need for back-propagation entirely, this issue can

persist if the MIP model is used as pre-training initialization for an SGD approach to replace

randomized initialization. For this set of activations we can write Constraints (2),(3), for a single

unit as hReLU

n,k,0 = max{

d

i=1 αi,k,0xn,i + βk,0, 0}, hReLU

n,k,ℓ = max{

K
k′=1 αk′,k,ℓhk′,ℓ + βk,ℓ, 0} respectively.

Similar to the binary case, the two main reformulation challenges that arise from these constraints

P

P

are the piece-wise deﬁnition of the activation and the presence of bi-linear terms. Unlike the binary

case however, the bi-linear terms involve the multiplication of two real valued decision variables

and not a binary variable with a continuous variable. Since the resulting formulation would be

challenging for commercial solvers to solve eﬀectively we propose a relaxation formulation for the

the ReLU activation case. In particular, we utilize piece-wise McCormick relaxations to reformulate

the problem as a linear MIP.

Proposition 3. In the ReLU activation case, the Constraints (2),(3) can be approximated using

a MIP relaxation. Speciﬁcally, for a given partition number P , for all k ∈ [K]1, n ∈ [N ]1:

(αi,k,0xn,i) + βk,0 ≤ M hn,k,0

d

i=1
X
d

(αi,k,0xn,i) + βk,0 ≥ ǫ + (−M − ǫ)(1 − hn,k,0)

i=1
X
hReLU
n,k,0 ≤ (

D

(αi,k,0xn,d) + βk,0) + M (1 − hn,k,0)

i=1
X
D

hReLU
n,k,0 ≥ (

(αi,k,0xn,d) + βk,0) − M (1 − hn,k,0)

i=1
X

− M hn,k,0 ≤ hReLU

n,k,0 ≤ M hn,k,0

For all p ∈ [P ]1, ℓ ∈ [L − 1]1, k′, k ∈ [K]2

1, n ∈ [N ]1:

zn,k′,k,ℓ ≥ αL

p hReLU

n,k′,ℓ−1 − M (1 − λk′,k,ℓ,p)

(22)

(23)

(24)

(25)

(26)

(27)

10

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

zn,k′,k,ℓ ≥ αU

p hReLU

n,k′,ℓ−1 + αk′,k,ℓhU

ReLU − αU

p hU

ReLU − M (1 − λk′,k,ℓ,p)

zn,k′,k,ℓ ≤ αU

p hReLU

n,k′,ℓ−1 + M (1 − λk′,k,ℓ,p)

zn,k′,k,ℓ ≤ αL

p hReLU

n,k′,ℓ−1 + αk′,k,ℓhU

ReLU − αL

p hU

ReLU + M (1 − λk′,k,ℓ,p)

For all ℓ ∈ [L − 1]1, k′, k ∈ [K]2
1

P

p=1
X
P

p=1
X

λk′,k,ℓ,p = 1

αL

p λk′ ,k,ℓ,p ≤ αk′,k,ℓ ≤

αU

p λk′,k,ℓ,p

P

p=1
X

For all ℓ ∈ [L − 1]1, k ∈ [K]1, n ∈ [N ]1

K

(zn,k′,k,ℓ) + βk,ℓ ≤ M hn,k,ℓ

Xk′=1

K

(zn,k′,k,ℓ) + βk,ℓ ≥ ǫ + (−M − ǫ)(1 − hn,k,ℓ)

Xk′=1
hReLU
n,k,ℓ ≤ (

K

(zn,k′,k,ℓ) + βk,ℓ) + M (1 − hn,k,ℓ)

k′=1
X
K

hReLU
n,k,ℓ ≥ (

(zn,k′,k,ℓ) + βk,ℓ) − M (1 − hn,k,ℓ)

k′=1
X

− M hn,k,ℓ ≤ hReLU

n,k,ℓ ≤ M hn,k,ℓ

Where αL

p = αL + (αU − αL) · (p − 1)/P and αL

p = αL + (αU − αL) · p/P

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

A detailed proof of this proposition can be found in the appendix, however, we present a

brief sketch here. Similar to Proposition 2, we use big M formulations for disjunctive constraints

(Wolsey and Nemhauser 1999) to obtain hReLU
n,k,ℓ

from hn,k,ℓ, the binary indicator.

Constraints (22) and (23) are as deﬁned in Section 3.2. Constraints (22) through (26) then enforce

hReLU
n,k,0 = max{

d
i=1 αi,k,0xn,i + βk,0, 0}. Our formulation relies on piece-wise McCormick relaxations

to reformulate the training problem as a linear MIP. To do so, we introduce an auxiliary variable

P

zn,k′,k,ℓ as a placeholder for αk′,k,ℓhReLU

n,k′,ℓ−1, partition α into P pieces, and introduce a set of binary

variables λn,k′,k,ℓ ∈ B. Partitioning on α gives αU

p and αL

p , the respective upper- and lower-bounds

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

11

on the partitions. The values for αL

p and αU

p are adapted from the partition bounds ﬁrst deﬁned

by (Castro (2015)). We also let hU

ReLU and hL

ReLU to be the respective upper- and lower-bounds on

all hReLU

n,k′,ℓ . Constraints (27) and (28) act as under-estimators to zn,k′,k,l, while Constraints (29) and

(30) act as over-estimators. Constraints (31) chooses which partition best minimizes the objective.

Constraints (32) then enforces that αk′ ,k,ℓ is bounded by the upper- and lower-bounds deﬁned by the

chosen partition. Given a large enough P, (Raman and Grossmann 1994) show that the McCormick

relaxations are eﬀective estimators for the bi-linear terms. Finally, Constraints (33) through (37)

are analogous to Constraints (22) through (26) and are required for the ReLU activations of the

remaining hidden layers.

We note that although not explicitly stated, the same reformulation and relaxation techniques

can also be applied symmetrically to the output layer of the ANN. With the ReLU activated

neurons now deﬁned, the rest of model is identical to that deﬁned in Proposition 2. The complete

MIP formulation for our problem can be found in the appendix.

4. Greedy Layer-wise Pre-Training

In this section, we apply our MIP formulations to the greedy layer-wise pre-training framework

presented by (Bengio et al. 2007). Training full-scale ANNs with large data sets can be challeng-

ing even with state-of-the-art solvers despite having tight formulations (Toro Icarte et al. 2019,

Thorbjarnarson and Yorke-Smith, Kurtz and Bah 2021). It is therefore an area of interest to

explore the validity of our MIP formulation as pre-training parameters used to initialize SGD

solvers.

12

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

Algorithm 1 Greedy Layer-wise Pre-training with a Binary MIP
Require: L ≥ 1, Y be input labels Y

1: Initialize X0 with input matrix X

2: Initialize hℓ, αℓ, βℓ = 0

3: O, W, B ← BinaryM IP (X0, Y, L = 1)

4: α0 ← W0, β0 ← B0, h0 ← O0

5: for ℓ = 1, ..., L do

6:

7:

8:

Xℓ ← hℓ−1

O, W, B ← BinaryM IP (Xℓ, Y, L = 1)

αℓ ← W0, βℓ ← B0, hℓ ← O0

9: end for

10: αL ← W1, βL ← B1

In Algorithm 1, we require that we train neural networks with at least one layer, and that

the data labels are constant throughout the algorithm. The input data for training layer 0 are

initialized as the input matrix X ∈ R

n×d. We also deﬁne variables hℓ, αℓ, βℓ as 0 matrices such

that hℓ = 0n,k, ∀ n ∈ [N ]1, k ∈ [K]1, ℓ ∈ [L − 1]0, α0 = 0i,k, αℓ = 0k,k, αL = 0k,j, ∀ k ∈ [K]1, j ∈ [J ]1, ℓ ∈

[L − 1]1, βℓ = 0k,1, βL = 0j,1∀ k ∈ [K]1, j ∈ [J ]1, ℓ ∈ [L − 1]0. BinaryM IP (X, Y, L) represents a call to

a function that solves the Binary MIP formulation with given input values X, prediction labels Y ,

and the number of layers L as the arguments. For Algorithm 1, the number of hidden layers trained

in each function call is always 1 to maintain a layer-wise training format. In the ﬁrst function call,

it returns the activated outputs of the 0th hidden layer and output layer as matrix O0 ∈ R

n×k, O1 ∈

n×j, weight matrices W0 ∈ R

R

d×k, W1 ∈ R

k×j, and bias vectors B0 ∈ R

k×1, B1 ∈ R

j×1. The solved

parameters from the 0th layers are then saved in h, α, β. The remaining L − 1 layers are trained

in an iterative loop, where the input of the 1 layer neural network is initialized as the the hidden

layer output of the previous iteration. Finally, the last layer parameters are saved from the ﬁnal

iteration’s return values.

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

13

Algorithm 2, uses a similar framework to Algorithm 1, where ReLU M IP (X, Y, L) represents

a call to function that solves the ReLU MIP formulation. Note that Algorithm 2 diﬀers from

Algorithm 1 in steps 11-12, where we have another call to ReLU M IP , with 0 hidden layers, in

order to eliminate the need for relaxations when training the last layer.

Algorithm 2 Greedy Layer-wise Pre-training with a ReLU MIP
Require: L ≥ 1, Y be input labels Y

1: Initialize X0 with input matrix X

2: Initialize hℓ, αℓ, βℓ = 0

3: O, W, B ← ReLU M IP (X0, Y, L = 1)

4: α0 ← W0, β0 ← B0, h0 ← O0

5: for ℓ = 1, ..., L do

6:

7:

8:

Xℓ ← hℓ−1

O, W, B ← ReLU M IP (Xℓ, Y, L = 1)

αℓ ← W0, βℓ ← B0, hℓ ← O0

9: end for

10: XL ← hL−1

11: O, W, B ← ReLU M IP (XL, Y, L = 0)

12: αL ← W0, βL ← B0

5. Experiments and Results

In this section, we compare our MIP based training methods against SGD and end-to-end training

methods. We consider a full Binary MIP training, greedy layer-wise MIP training, greedy layer-wise

MIP training as pre-training for SGD, SGD applied on binary and ReLU ANNs, greedy layer-

wise SGD applied to ReLU and binary ANNs, and greedy SGD as pre-training for ReLU and

binary ANNs that are then trained fully with SGD. Due to computation resources constraints,

experimentation for our ReLU MIP formulation is beyond the scope of this work and will be

validated in future work. We also evaluate the eﬀectiveness of using MIP in a greedy layer-wise

14

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

Table 1

List of models and minimum number of layers and units needed to train to

Model
Binary MIP
Greedy Binary MIP
Greedy Binary MIP + SGD
Binary SGD
Greedy Binary SGD
Greedy Binary SGD + SGD
ReLU SGD
ReLU Greedy SGD
ReLU Greedy SGD + SGD

85% testing accuracy

Number of Layers Number of Units

1 layer
1 layer
> 5 layers
> 5 layers
> 5 layers
> 5 layers
> 5 layers
> 5 layers
> 5 layers

NaN
5 units
10 units
> 50 units
> 50 units
> 50 units
50 units
> 50 units
> 50 units

algorithm to obtain pre-training parameters for SGD training. The goal of these experiments is to

show which methods and models can result in the most parsimonious representations that achieve a

high out of sample prediction accuracy. All of the models in this section are trained using synthetic

data generated from an exclusive-or (XOR) logic gate. The input data X ∈ {x1, x2, x3, x4, x5}, xd ∈

{0, 1} produces one-hot encoded labels where an odd parity in x1, x3, x5 produces [0, 1], and an even

parity produces [1, 0]. The motivation for this structure is to test our method’s ability to only select

features that eﬀect the labels. The data is randomly sampled with replacement to produce 1000

training inputs and labels, to which we then introduce p = 0.1 Bernoulli noise. The testing data was

similarly obtained by randomly sampling 250 inputs and labels with replacement. Each experiment

was repeated across 5 diﬀerent seeds, and the average testing accuracy was used to obtain Table 1.

All experiments were conducted on a system conﬁgured at 1.6 GHz Dual-Core Intel Core i5 with

8 GB 1600 MHz DDR3 Memory. We used Gurobi Optimizer v9.1.2 (Gurobi Optimization, LLC

(2021)) on Python 3.7.2 to solve our MIP models.

First, we compare the various models and training methods across ANN architectures by varying

the number of layers. Speciﬁcally, we train ANNs ranging from one hidden layer to ﬁve hidden

layers with a ﬁxed ﬁve units in each hidden layer. We then recorded the minimum number of

hidden layers required by the combination of training methods and model to achieve 85% out of

sample accuracy to test which methods can provide the smallest eﬀective models. SGD models

were trained to 10,000 epochs.

As shown in Table 1, the Binary MIP and Greedy Binary MIP outperform other models by

requiring only one layer to achieve a test accuracy greater than our 85% threshold. On the other

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

15

hand, SGD reliant models fail to meet the threshold with the architectures chosen for this experi-

ment. Moreover, the SGD training model fails to meet the threshold even when using our Binary

MIP model solved with the greedy layer-wise algorithm as pre-training parameters.

For the next set of experiments we ﬁxed the number of hidden layers to 3 and varied the number

of units per hidden layer. We used thresholds of 5,10,20,30,40 and 50 units and recorded the lowest

threshold at which the models where able to achieve 85% accuracy.

Unfortunately, due to the large number of variables requires to train wide networks, our Binary

MIP failed to produce any results with the computation resources at hand. However, we see in

Table 1 that ANNs trained with the Binary MIP using the greedy layer-wise algorithm meet

the required testing accuracy threshold with the fewest units chosen in the experiment. We also

observed that the solver’s optimal objective did not change after training the ﬁrst layer, showing

that the threshold was met after training just one layer at 5 units. This result shows the potential

of MIP based methods in training parsimonious and accurate models. However, this does indicate

that full MIP based methods may be more appropriate for narrow deep network architectures,

though with a layer by layer approach MIP methods do provide an advantage over SGD based

methods and perform well for pre-trainning.

6. Conclusion

In this paper, we proposed several MIP models to solve ANN training problems. We presented

MIP formulations to train neural networks with binary activations as well as ReLU activations.

To exploit the structure of the model, we also adopted greedy algorithms for our programs to

train layer-by-layer. We showed that our greedy layer-wise, binary activated MIP outperforms

traditional training models in two experiments. In both experiments, our model meets the testing

accuracy threshold with the fewest number of layers and the fewer number of units in each layer.

For architectures constrained by the number of units, we also see that our binary activated MIP is

able to compete with its greedy counterpart. In essence, our models can achieve strong predictive

accuracy with more parsimonious architectures compared to traditional models that require deep

16

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

and neuron-dense architectures. We consider our paper to be another step in the exploration

of using mixed-integer programming as a tool for deep learning problems. There are interesting

challenges remaining that pertain to large-scale training problems for non-linear and non-convex

structures. We see potential in future research on the application of MIP models for training ANNs

that solve causal problems.

References

Anderson R, Huchette J, Ma W, Tjandraatmadja C, Vielma JP (2020) Strong mixed-integer programming

formulations for trained neural networks. Mathematical Programming 1–37.

Askari A, Negiar G, Sambharya R, Ghaoui LE (2018) Lifted neural networks. arXiv preprint

arXiv:1805.01532 .

Belilovsky E, Eickenberg M, Oyallon E (2019) Greedy layerwise learning can scale to imagenet. International

conference on machine learning, 583–593 (PMLR).

Bengio Y, Lamblin P, Popovici D, Larochelle H (2007) Greedy layer-wise training of deep networks. Advances

in neural information processing systems, 153–160.

Bengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is diﬃcult.

IEEE transactions on neural networks 5(2):157–166.

Bottou L, et al. (1991) Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes 91(8):12.

Calaﬁore GC, El Ghaoui L (2014) Optimization models (Cambridge university press).

Castro PM (2015) Tightening piecewise mccormick relaxations for bilinear problems. Computers & Chemical

Engineering 72:300–311.

Cormen TH, Leiserson CE, Rivest RL, Stein C (2022) Introduction to algorithms (MIT press).

Elad A, Haviv D, Blau Y, Michaeli T (2018) The eﬀectiveness of layer-by-layer training using the information

bottleneck principle .

Erhan D, Courville A, Bengio Y, Vincent P (2010) Why does unsupervised pre-training help deep learning?

Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, 201–208

(JMLR Workshop and Conference Proceedings).

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

17

Fischetti M, Jo J (2017) Deep neural networks as 0-1 mixed integer linear programs: A feasibility study.

arXiv preprint arXiv:1712.06174 .

Glorot X, Bordes A, Bengio Y (2011) Deep sparse rectiﬁer neural networks. Proceedings of the fourteenth

international conference on artiﬁcial intelligence and statistics, 315–323 (JMLR Workshop and Con-

ference Proceedings).

Goodfellow I, Bengio Y, Courville A (2017) Deep learning (adaptive computation and machine learning

series). Cambridge Massachusetts 321–359.

Gurobi Optimization, LLC (2021) Gurobi Optimizer Reference Manual. URL https://www.gurobi.com.

Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J, et al. (2001) Gradient ﬂow in recurrent nets: the diﬃculty

of learning long-term dependencies.

Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classiﬁcation with deep convolutional neural net-

works. Advances in neural information processing systems 25:1097–1105.

Kurtz J, Bah B (2021) Eﬃcient and robust mixed-integer optimization methods for training binarized deep

neural networks. arXiv preprint arXiv:2110.11382 .

L¨owe S, O’Connor P, Veeling BS (2019) Putting an end to end-to-end: Gradient-isolated learning of repre-

sentations. arXiv preprint arXiv:1905.11786 .

Nair V, Bartunov S, Gimeno F, von Glehn I, Lichocki P, Lobov I, O’Donoghue B, Sonnerat N, Tjandraat-

madja C, Wang P, et al. (2020) Solving mixed integer programs using neural networks. arXiv preprint

arXiv:2012.13349 .

Raman R, Grossmann IE (1994) Modelling and computational techniques for logic based integer program-

ming. Computers & Chemical Engineering 18(7):563–578.

Sutskever I, Martens J, Dahl G, Hinton G (2013) On the importance of initialization and momentum in deep

learning. International conference on machine learning, 1139–1147 (PMLR).

Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015)

Going deeper with convolutions. Proceedings of the IEEE conference on computer vision and pattern

recognition, 1–9.

18

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

Taylor G, Burmeister R, Xu Z, Singh B, Patel A, Goldstein T (2016) Training neural networks without gra-

dients: A scalable admm approach. International conference on machine learning, 2722–2731 (PMLR).

Thorbjarnarson T, Yorke-Smith N (????) Training integer-valued neural networks with mixed integer pro-

gramming .

Tjeng V, Xiao K, Tedrake R (2017) Evaluating robustness of neural networks with mixed integer program-

ming. arXiv preprint arXiv:1711.07356 .

Toro Icarte R, Illanes L, Castro MP, Cire AA, McIlraith SA, Beck JC (2019) Training binarized neural

networks using mip and cp. International Conference on Principles and Practice of Constraint Pro-

gramming, 401–417 (Springer).

Westerlund T, Lundell A, Westerlund J (2011) On convex relaxations in nonconvex optimization. Chem.

Eng. Trans 24:331–336.

Wolsey LA, Nemhauser GL (1999) Integer and combinatorial optimization, volume 55 (John Wiley & Sons).

Appendix

A. Proofs of Propositions

Proof of Proposition 1. First, we can explicitly write the negative log likelihood as:

N

N

− log(

(σ(hn,j,L)1[yn=j])) = −

1[yn = j] log

n=1
Y
N

n=1
X

= −

1[yn = j](hn,j.L − log

exp(hn,j′,L))

exp(hn,j,L)
j′∈J exp(hn,j′,L)

(cid:17)

(cid:16)

P

(38)

(39)

Here

we

note

that

the

i=n
X

Xj′∈J
expression

log

j′=J exp(hn,j′,L) = Θ(maxj′∈j hn,j′,L)

(Calaﬁore and El Ghaoui (2014)). Thus using this fact we obtain:

P

N

n=1
X

1[yn = j](hn,j,L − max
j′∈J

{hn,j′,L} − log(|J |))

1[yn = j](max
j′∈J

{hn,j′,L} − hn,j,L + log(|J |))

(39) ≤ −

N

=

n=1
X

(40)

(41)

Using the other side of the big Θ condition yields that (39) ≥

N
n=1 1[yn = j](maxj′∈J {hn,j′,L} −
hn,j,L). If we let ωn = maxj′∈J {hn,j′,L} we can use standard formulation techniques to reformu-

P

late it using linear constraints (Wolsey and Nemhauser (1999)). In conjunction with removing the
(cid:3)

constant terms that do not depend on ωn, {hn,j′,L} yields the desired result.

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

19

Proof of Proposition 2. There are two key challenges for the reformulation of Constraints

(2),(3),(4) ﬁrst is the binary activation function itself and second are the bi-linear terms present

in the hidden and output layers. Without loss of generality, let us consider a particular unit in

the neural network such that hn,k,ℓ = 1[

K
k′=0 αk′,k,ℓhn,k′,ℓ + βk,ℓ ≥ 0]. First let us consider the case
of the binary activation. This condition can be modeled as a disjunction (Wolsey and Nemhauser

P

(1999)), a form of constraint that can be reformulated as follows:

αk′ ,k,ℓ−1hn,k′,ℓ + βk,ℓ ≤ M hn,k,ℓ

K

Xk′=0

K

αk′ ,k,ℓ−1hn,k′,ℓ + βk,ℓ ≥ ǫ + (−M − ǫ)(1 − hn,k,ℓ)

(42)

(43)

Xk′=0

Where M is a suﬃciently large constant, and ǫ is a small constant. Next let us consider the bi-
linear terms αk′,k,ℓhn,k′,ℓ−1. Here we note that αk′ ,k,ℓ ∈ [αL, αU ] is a continuous real-valued variable,

while hn,k′,ℓ−1 ∈ B is binary valued. This type of products can be reformulated through a standard

technique by introducing an additional variable zn,k′,k,ℓ = αk′,k,ℓhn,k′,ℓ−1. Then these products can

be written as:

zn,k′,k,ℓ ≤ αk′ ,k,ℓ + M (1 − hn,k′,ℓ−1)

zn,k′,k,ℓ ≥ αk′ ,k,ℓ − M (1 − hn,k′,ℓ−1)

− M hn,k′,ℓ−1 ≤ zn,k′ ,k,ℓ ≤ M hn,k′,ℓ−1

(44)

(45)

(46)

Where again M is an appropriately picked suﬃciently large constant. Using these reformulations

where appropriate and making the necessary variable substitutions results in the formulations
(cid:3)

presented above.

Proof of Proposition 3. Much like the proof of Proposition 2 the two main challenges for refor-

mulation involve the piece-wise nature of the activation and reformulation of the bi-linear terms.

Without loss of generality consider a single unit evaluated at a particular data point with speciﬁc

indices ℓ, k, n. If we use the deﬁnition hn,k,ℓ = 1[

K
k′=0 αk′,k,ℓhn,k′,ℓ + βk,ℓ ≥ 0] then we can rewrite

the ReLU activation conditions as:

hReLU
n,k,ℓ =

(

P
αk′,k,ℓhReLU
0, if hn,k,ℓ = 0

n,k,ℓ−1 + βk,ℓ, if hn,k,ℓ = 1

(47)

Hence we can use the disjunction constraint reformulation from Proposition 2 to obtain the con-

ditions that ensure hn,k,ℓ when appropriate. Using disjunctions (Wolsey and Nemhauser (1999))
again on the conditions of when hReLU
n,k,ℓ

is equal to zero and using hn,k,ℓ as the binary indicator

we can model the conditions above resulting in the desired constraints. For the bilinear terms,

we introduce the proper auxiliary variables and bounds as proposed in (Westerlund et al. (2011))
(cid:3)

using general disjunctive programming (Raman and Grossmann (1994)).

20

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

B. Mixed-Integer Programming Formulation with Binary Activated Neural Networks

Table 2

Parameter Description
xn,i

A summary of parameters and decision varables used in the binary model
Range
[−∞, ∞]

1, ℓ ∈ [L − 1]1.

Vector inputs of size n×i, where n ∈ [N ]1 and
i ∈ [d]1. N is the number of data points, d is
the number of dimensions/features.
Binary vector outputs of size n×j indicating
category selection, where n ∈ [N ]1 and j ∈
[J]1. N is the number of data points and J is
the dimension of the label.
Weight for feature i in unit k in the 0th hidden
layer, ∀ i ∈ [d]1, k ∈ [K]1.
Weight from the k′th unit in layer ℓ − 1 to the
kth in layer ℓ, ∀ k′, k ∈ [K]2
Weight from the k′th unit in hidden layer L − 1
to the jth unit in the output layer L, ∀ k′ ∈
[K]1, j ∈ [J]1.
Bias for unit k in layer ℓ, ∀ k ∈ [K]1, ℓ ∈ [L −
1]0.
Bias for unit j in the ﬁnal layer, ∀ j ∈ [J]1.
Binary output of unit k in layer ℓ, ∀ n ∈
[N ]1, k ∈ [K]1, ℓ ∈ [L − 1]0.
Output of ﬁnal layer, ∀ n ∈ [N ]1, j ∈ [J]1.
Placeholder variable for maxj∈J{hn,j,L}, ∀ n ∈
[N ]1.
Binary helper variable to diversify output
layer of hn,j,L, ∀ n ∈ [N ]1, j, j′ ∈ [J]2
variable
Auxiliary
αk′,k,ℓhn,k′,ℓ−1, ∀ n ∈ [N ]1, k′, k ∈ [K]2
[L − 1]1.
Auxiliary
αk′,j,Lhn,k′,L−1, ∀ n ∈ [N ]1, k′ ∈ [K]1, j ∈ [J]1.

1, j 6= j′.
represents
1, ℓ ∈

represents

variable

that

that

{0, 1}

[αL, αU ]

[αL, αU ]

[αL, αU ]

[βL, βU ]

[βL, βU ]
{0, 1}

[−∞, ∞]
[−∞, ∞]

{0, 1}

[αL, αU ]

[αL, αU ]

yn,j

αi,k,0

αk′,k,ℓ

αk′,j,L

βk,ℓ

βj,L
hn,k,ℓ

hn,j,L
ωn

rn,j,j′ :

zn,k′,k,ℓ

zn,k′,j,L

Formulation:

min
α,β,h,z,ω,r

N

J

n=1
X

j=1
X

subject to

yn,j(ωn − hn,j,L)

For all k ∈ [K]1, n ∈ [N ]1:

(αi,k,0xn,i) + βk,0 ≤ M hn,k,0

(αi,k,0xn,i) + βk,0 ≥ ǫ + (−M − ǫ)(1 − hn,k,0)

d

i=1
X
d

i=1
X

(48)

(49)

(50)

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

For all ℓ ∈ [L − 1]1, k ∈ [K]1, n ∈ [N ]1:

K

(zn,k′,k,ℓ) + βk,ℓ ≤ M hn,k,ℓ

k′=1
X
K

(zn,k′,k,ℓ) + βk,ℓ ≥ ǫ + (−M − ǫ)(1 − hn,k,ℓ)

Xk′=1
zn,k′,k,ℓ ≤ αk′ ,k,ℓ + M (1 − hn,k′,ℓ−1)

zn,k′,k,ℓ ≥ αk′ ,k,ℓ − M (1 − hn,k′,ℓ−1)

− M hn,k′,ℓ−1 ≤ zn,k′ ,k,ℓ ≤ M hn,k′,ℓ−1

For all j ∈ [J ]1, n ∈ [N ]1:

K

hn,j,L ≤

(zn,k′,j,L) + βj,L

k′=0
X
K

hn,j,L ≥

(zn,k′,j,L) + βj,L

k′=0
X

zn,k′,j,L ≤ αk′ ,j,L + M (1 − hn,k′,L−1)

zn,k′,j,L ≥ αk′ ,j,L − M (1 − hn,k′,L−1)

− M hn,k′,L−1 ≤ zn,k′,j,L ≤ M hn,k′,L−1

ωn ≥ hn,j,L

hn,j,L + hn,j′,L − 2hn,j,L ≤ −ǫ + M rn,j,j′ ∀ j, j′ ∈ [J ]2

1, j 6= j′

hn,j,L + hn,j′,L − 2hn,j,L ≥ ǫ − M (1 − rn,j,j′) ∀ j, j′ ∈ [J ]2

1, j 6= j′

C. Mixed-Integer Programming Formulation with ReLU Activated Neural Networks

Table 3

A summary of parameters used in the ReLU model

21

(51)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)

Parameter Description
xn,i

yn,j

Vector inputs of size n×i, where n ∈ [N ]1 and
i ∈ [d]1. N is the number of data points, d is
the number of dimensions/features.
Binary vector outputs of size n×j indicating
category selection, where n ∈ [N ]1 and j ∈
[J]1. N is the number of data points and J is
the dimension of the label.

Range
[−∞, ∞]

{0, 1}

22

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

Table 4

Variable
αi,k,0

αk′,k,ℓ

αk′,j,L

βk,ℓ

βj,L
hn,k,ℓ

hReLU
n,k,ℓ

hn,j,L
ωn

rn,j,j′

zn,k′,k,ℓ

zn,k′,j,L

λk′,k,ℓ,p

λk′,j,L,p

A summary of decision variables used in the ReLU model
Description
Weight for feature i in unit k in the 0th hidden
layer, ∀i ∈ [d]1, k ∈ [K]1.
Weight from the k′th unit in layer ℓ − 1 to the
kth unit in layer ℓ, ∀ k′, k ∈ [K]2
1, ℓ ∈ [L − 1]1.
Weight from the k′th unit in the (L − 1)st layer
to the jth in the Lth, or output, layer, ∀ k′ ∈
[K]1, j ∈ [J]1.
Bias for unit k in layer ℓ, ∀ k ∈ [K]1, ℓ ∈ [L −
1]1.
Bias for unit j in the output layer, L, ∀j ∈ [J]1.
Binary output of unit k in the ℓth layer, ∀n ∈
[N ]1, k ∈ [K]1, ℓ ∈ [L − 1]0
ReLU activated output of unit k in the ℓth
layer, ∀ n ∈ [N ]1, k ∈ [K]1, ℓ ∈ [L − 1]0.
Output of ﬁnal layer, ∀ n ∈ [N ]1, j ∈ [J]1.
Placeholder variable for maxj∈J {hn,j,L} ∀ n ∈
[N ]1.
Binary helper variable to diversify output
layer of hn,j,L∀ n ∈ [N ]1, j, j′ ∈ [J]2
Auxiliary
variable
αk′,k,ℓhReLU
[L − 1]1.
Auxiliary
represents
n,k′,L−1 ∀ n ∈ [N ]1, k′ ∈ [K]1, j ∈ [J]1.
αk′,j,LhReLU
Binary variable that indicates which parti-
tions p of the McCormick relaxation needs to
be active, where we have P partitions for the
hidden layers ∀ k, k′ ∈ [K]2
1, ℓ ∈ [L]1, p ∈ [P ]1.
Binary variable that indicates which parti-
tion of the McCormick relaxation needs to be
active, where we have P partitions for the out-
put layer ∀ k′ ∈ [K]1, j ∈ [J]1, p ∈ [P ]1.

n,k′,ℓ−1 ∀ n ∈ [N ]1, k′, k ∈ [K]2

represents
1, ℓ ∈

1, j 6= j′.

variable

that

that

Range
[αL, αU ]

[αL, αU ]

[αL, αU ]

[βL, βU ]

[βL, βU ]
{0, 1}

[0, ∞]

[−∞, ∞]
[−∞, ∞]

{0, 1}

[0, ∞]

[0, αU ]

{0, 1}

{0, 1}

Formulation:

N

J

n=1
X

j=1
X

min
α,β,h,hReLU ,ω,r,λ

subject to

yn,j(ωn − hn,j,L)

For all k ∈ [K]1, n ∈ [N ]1

(αi,k,0xn,i) + βk,0 ≤ M hn,k,0

d

i=1
X
d

(αi,k,0xn,i) + βk,0 ≥ ǫ + (−M − ǫ)(1 − hn,k,0)

i=1
X
hReLU
n,k,0 ≤ (

D

(αi,k,0xn,i) + βk,0) + M (1 − hn,k,0)

i=0
X

(64)

(65)

(66)

(67)

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

D

hReLU
n,k,0 ≥ (

(αi,k,0xn,i) + βk,0) − M (1 − hn,k,0)

i=0
X

− M hn,k,0 ≤ hReLU

n,k,0 ≤ M hn,k,0

For all ℓ ∈ [L − 1]1, k ∈ [K]1, n ∈ [N ]1

K

(zn,k′,k,ℓ) + βk,ℓ ≤ M hn,k,ℓ

Xk′=1

K

(zn,k′,k,ℓ) + βk,ℓ ≥ ǫ + (−M − ǫ)(1 − hn,k,ℓ)

k′=1
X
hReLU
n,k,ℓ ≤ (

K

(zn,k′,k,ℓ) + βk,ℓ) + M (1 − hn,k,ℓ)

k′=1
X
K

hReLU
n,k,ℓ ≥ (

(zn,k′,k,ℓ) + βk,ℓ) − M (1 − hn,k,ℓ)

Xk′=1

− M hn,k,ℓ ≤ hReLU

n,k,ℓ ≤ M hn,k,ℓ

For all p ∈ [P ]1, ℓ ∈ [L − 1]1, k, k′ ∈ [K]2

1, n ∈ [N ]1

zn,k′ ,k,ℓ ≥ αL

p hReLU

n,k′,ℓ−1 + αk′ ,k,ℓhL

ReLU − αL

p hL

ReLU − M (1 − λk′,k,ℓ,p)

zn,k′ ,k,ℓ ≥ αU

p hReLU

n,k′,ℓ−1 + αk′ ,k,ℓhU

ReLU − αU

p hU

ReLU − M (1 − λk′,k,ℓ,p)

zn,k′ ,k,ℓ ≤ αU

p hReLU

n,k′,ℓ−1 + αk′ ,k,ℓhL

ReLU − αU

p hL

ReLU + M (1 − λk′,k,ℓ,p)

zn,k′ ,k,ℓ ≤ αL

p hReLU

n,k′,ℓ−1 + αk′ ,k,ℓhU

ReLU − αL

p hU

ReLU + M (1 − λk′,k,ℓ,p)

For all ℓ ∈ [L − 1]1, k, k′ ∈ [K]2
1

For all j ∈ [J ]1, n ∈ [N ]1

αL

p λk′ ,k,ℓ,p ≤ αk′,k,ℓ ≤

αU

p λk′,k,ℓ,p

P

p=1
X

λk′,k,ℓ,p = 1

P

p=1
X
P

p=0
X

K

hn,j,L ≤

(zn,k′,j,L) + βj,L

Xk′=0

K

hn,j,L ≥

(zn,k′,j,L) + βj,L

k′=0
X

For all p ∈ [P ]1, j ∈ [J ]1, k′ ∈ [K]1, n ∈ [N ]1

zn,k′,j,L ≥ αL

p hReLU

n,k′,L−1 − M (1 − λk′,j,L,p)

23

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

24

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

zn,k′,j,L ≥ αU

p hReLU

n,k′,L−1 + αk′ ,j,LhU

ReLU − αU

p hU

ReLU − M (1 − λk′,j,L,p)

zn,k′,j,L ≤ αU

p hReLU

n,k′,L−1 + M (1 − λk′,j,L,p)

zn,k′,j,L ≤ αL

p hReLU

n,k′,L−1 + αk′ ,j,LhU

ReLU − αL

p hU

ReLU + M (1 − λk′,j,L,p)

For all j ∈ [J ]1, k′ ∈ [K]1

For all j ∈ [J ]1, n ∈ [N ]1

For all j ∈ [J ]1, n ∈ [N ]1

αL

p λk′,j,L,p ≤ αk′ ,j,L ≤

αU

p λk′ ,j,L,p

P

p=1
X

λk′ ,j,L,p = 1

P

p=1
X
P

p=0
X

ωn ≥ hn,j,L ∀n ∈ N, j ∈ J

hn,i,L + hn,j,L − 2hn,i,L ≤ −ǫ + M rn,j,j′ ∀j, j′ ∈ [J ]2

1, j 6= j′

hn,i,L + hn,j,L − 2hn,i,L ≥ ǫ − M (1 − rn,j,j′) ∀j, j′ ∈ [J ]2

1, j 6= j′

D. Output Layer Formulation

Table 5

A summary of parameters and decision variables used in the output layer

model

Parameter Description
xn,i

Vector inputs of size n×i, where n ∈ [N ]1 and
i ∈ [d]1. N is the number of data points, d is
the number of dimensions/features.
Binary vector outputs of size n×j indicating
category selection, where n ∈ [N ]1 and j ∈
[J]1. N is the number of data points and J is
the dimension of the label.
Weight for feature i in unit j in the output
layer, ∀ i ∈ [d]1, j ∈ [J]1.
Output of unit j in the output layer, ∀ n ∈
[N ]1, j ∈ [J]1.
Placeholder variable for maxj∈J {hn,j,0} ∀ n ∈
[N ]1.
Binary helper variable to diversify output
layer of hn,j,0 ∀ n ∈ [N ]1, j, j′ ∈ [J]2

1, j 6= j′.

Range
[−∞, ∞]

{0, 1}

[αL, αU ]

[−∞, ∞]

[−∞, ∞]

{0, 1}

yn,j

αj,j′,0

hn,j,0

ωn

rn,j,j′ :

Formulation:

min
α,β,h,ω,r

N

J

n=1
X

j=1
X

yn,j(ωn − hn,j,0)

(84)

(85)

(86)

(87)

(88)

(89)

(90)

(91)

(92)

Patil and Mintz: A Mixed-Integer Programming Approach to Training Dense Neural Networks

subject to

For all j ∈ [J ]1, n ∈ [N ]1

(αd,j,0xn,d) + βj,0 ≤ hn,j,0

D

d=0
X
D

(αd,j,0xn,d) + βk,0 ≥ hn,j,0

d=0
X
ωn ≥ hn,j,0

For all n ∈ [N ]1

hn,i,0 + hn,j,0 − 2hn,i,0 ≤ −ǫ + M rn,j,j′, ∀j, j′ ∈ [J ]2

1, j 6= j′

hn,i,0 + hn,j,0 − 2hn,i,0 ≥ ǫ − M (1 − rn,j,j′), ∀j, j′ ∈ [J ]2

1, j 6= j′

25

(93)

(94)

(95)

(96)

(97)

