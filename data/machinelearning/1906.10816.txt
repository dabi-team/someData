9
1
0
2

v
o
N
5

]

G
L
.
s
c
[

4
v
6
1
8
0
1
.
6
0
9
1
:
v
i
X
r
a

Program Synthesis and Semantic Parsing
with Learned Code Idioms

Richard Shin∗
UC Berkeley
ricshin@berkeley.edu

Miltiadis Allamanis, Marc Brockschmidt & Oleksandr Polozov

Microsoft Research
{miallama,mabrocks,polozov}@microsoft.com

Abstract

Program synthesis of general-purpose source code from natural language speciﬁ-
cations is challenging due to the need to reason about high-level patterns in the
target program and low-level implementation details at the same time. In this
work, we present PATOIS, a system that allows a neural program synthesizer to
explicitly interleave high-level and low-level reasoning at every generation step.
It accomplishes this by automatically mining common code idioms from a given
corpus, incorporating them into the underlying language for neural synthesis, and
training a tree-based neural synthesizer to use these idioms during code generation.
We evaluate PATOIS on two complex semantic parsing datasets and show that using
learned code idioms improves the synthesizer’s accuracy.

1

Introduction

Program synthesis is a task of translating an incomplete speciﬁcation (e.g. natural language, input-
output examples, or a combination of the two) into the most likely program that satisﬁes this
speciﬁcation in a given language [15]. In the last decade, it has advanced dramatically thanks to
the novel neural and neuro-symbolic techniques [5, 10, 19], ﬁrst mass-market applications [28], and
massive datasets [9, 39, 41]. Table 1 shows a few examples of typical tasks of program synthesis from
natural language. Most of the successful applications apply program synthesis to manually crafted
domain-speciﬁc languages (DSLs) such as FlashFill and Karel, or to subsets of general-purpose
functional languages such as SQL and Lisp. However, scaling program synthesis to real-life programs
in a general-purpose language with complex control ﬂow remains an open challenge.

We conjecture that one of the main current challenges of synthesizing a program is insufﬁcient
separation between high-level and low-level reasoning. In a typical program generation process,
be it a neural model or a symbolic search, the program is generated in terms of its syntax tokens,
which represent low-level implementation details of the latent high-level patterns in the program. In
contrast, humans switch between high-level reasoning (“a binary search over an array”) and low-
level implementation (“while l < r: m = (l+r)/2 . . . ”) repeatedly when writing a single function.
Reasoning over multiple abstraction levels at once complicates the generation task for a model.

This conjecture is supported by two key observations. First, recent work [12, 25] has achieved great
results by splitting the synthesis process into sketch generation and sketch completion. The ﬁrst stage
generates a high-level sketch of the target program, and the second stage ﬁlls in missing details in the
sketch. Such separation improves the accuracy of synthesis as compared to an equivalent end-to-end
generation. However, it allows only one stage of high-level reasoning at the root level of the program,
whereas (a) real-life programs involve common patterns at all syntactic levels, and (b) programmers
often interleave high-level and low-level reasoning during implementation.

∗Work done partly during an internship at Microsoft Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
Table 1: Representative program synthesis tasks from real-world semantic parsing datasets.

Dataset

Natural Language Speciﬁcation

Program

Hearthstone
[24]

Mana Wyrn (1, 3, 1, Minion, Mage, Common)
Whenever you cast a spell, gain +1 Attack.

# . . .
def create_minion(self, player):

return Minion(1, 3, effects=[Effect(

SpellCast(), ActionTag(Give(
ChangeAttack(1)), SelfSelector()))])

Spider
[41]

For each stadium, how many concerts are there?

Schema:
stadium = {stadium_id, name, ...}, ...

SELECT T2.name, COUNT(*)
FROM concert AS T1 JOIN stadium AS T2

ON T1.stadium_id = T2.stadium_id

GROUP BY T1.stadium_id

Second, many successful applications of inductive program synthesis such as FlashFill [14] rely
on a manually designed DSL to make the underlying search process scalable. Such DSLs include
high-level operators that implement common subroutines in a given domain. Thus, they (i) compress
the search space, ensuring that every syntactically valid DSL program expresses some useful task,
and (ii) enable logical reasoning over the domain-speciﬁc operator semantics, making the search
efﬁcient. However, DSL design is laborious and requires domain expertise. Recently, Ellis et al. [13]
showed that such DSLs are learnable in the classic domains of inductive program synthesis; in this
work, we target general-purpose code generation, where DSL design is difﬁcult even for experts.

In this work, we present a system, called PATOIS, that equips a program synthesizer with automatically
learned high-level code idioms (i.e. common program fragments) and trains it to use these idioms
in program generation. While syntactic by deﬁnition, code idioms often represent useful semantic
concepts. Moreover, they compress and abstract the programs by explicitly representing common
patterns with unique tokens, thus simplifying generative process for the synthesis model.

PATOIS has three main components, illustrated in Figure 1. First, it employs nonparameteric Bayesian
inference to mine the code idioms that frequently occur in a given corpus. Second, it marks the
occurrences of these idioms in the training dataset as new named operators in an extended grammar.
Finally, it trains a neural generative model to optionally emit these named idioms instead of the
original code fragments, which allows it to learn idiom usage conditioned on a task speciﬁcation.
During generation, the model has the ability to emit entire idioms in a single step instead of multiple
steps of program tree nodes comprising the idioms’ deﬁnitions. As a result, PATOIS interleaves
high-level idioms with low-level tokens at all levels of program synthesis, generalizing beyond ﬁxed
top-level sketch generation.

We evaluate PATOIS on two challenging semantic parsing datasets: Hearthstone [24], a dataset of
small domain-speciﬁc Python programs, and Spider [41], a large dataset of SQL queries over various
databases. We ﬁnd that equipping the synthesizer with learned idioms improves its accuracy in
generating programs that satisfy the task description.

2 Background

Program Synthesis We consider the following formulation of the program synthesis problem.
Assume an underlying programming language L of programs. Each program P ∈ L can be
represented either as a sequence y1 · · · y|P | of its tokens, or, equivalently, as an abstract syntax tree
(AST) T parsed according to the context-free grammar (CFG) G of the language L. The goal of a
program synthesis model f : ϕ (cid:55)→ P is to generate a program P that maximizes the conditional
probability Pr (P | ϕ) i.e. the most likely program given the speciﬁcation. We also assume a training
set D = {(cid:104)ϕj, Pj(cid:105)}|D|
j=1, sampled from an unknown true distribution D, from which we wish to
estimate the conditional probability Pr (P | ϕ).

In this work, we consider general-purpose programming languages L with a known context-free
grammar G such as Python and SQL. Each speciﬁcation ϕ is represented as a natural language
task description, i.e. a sequence of words X = x1 · · · x|X| (although the PATOIS synthesizer can be
conditioned on any other type of incomplete spec). In principle, we do not impose any restrictions on
the generative model f apart from it being able to emit syntactically valid programs. However, as we
detail in Section 4, the PATOIS framework is most easily implemented on top of structural generative
models such as sequence-to-tree models [38] and graph neural networks [7, 21].

2

t
n
i

:
1
(cid:96)

t
s
i
l

:
0
(cid:96)

x
e
d
n

I

t
p
i
r
c
s
b
u
S

e
r
a
p
m
o
C

f
I

2

m (cid:96)
u
N

q
E

3
(cid:96)

i

n
g
s
s
A

*
t

m
t
s

3
(cid:96)

d
d
A

m 1
u
N

p
O
n
B

i

.

.

.

Figure 1: Top: An overview of PATOIS. A miner 1(cid:13) extracts common idioms from the programs in a
given dataset. All the idiom occurrences in the dataset programs are 2(cid:13) marked as optional alternative
grammar operators. The dataset with marked occurrences is used to 3(cid:13) train a neural generative
model. At inference time, the model 4(cid:13) generates programs with named idioms, which are inlined
before program execution. Note that idioms may have named subexpressions, may repeat, and may
occur at any program level. For clarity, we typeset idioms using function-like syntax Ij((cid:96)1, . . . , (cid:96)k)
in this paper, although they are actually represented as AST fragments with no syntax.
Bottom: AST fragment representation of the idiom I2 in Python. Here sans-serif nodes are ﬁxed
non-terminals, monospaced nodes are ﬁxed terminals, and boxed nodes are named arguments.

Code Idioms Following Allamanis and Sutton [2], we deﬁne code idioms as fragments I of valid
ASTs T in the CFG G, i.e. trees of nonterminals and terminals from G that may occur as subtrees
of valid parse trees from G. The grammar G extended with a set of idiom fragments forms a tree
substitution grammar (TSG). We also associate a non-unique label (cid:96) with each nonterminal leaf
in every idiom, and require that every instantiation of an idiom I must have its identically-labeled
nonterminals instantiated to identical subtrees. This enables the role of idioms as subroutines, where
labels act as “named arguments” in the “body” of an idiom. See Figure 1 for an example.

3 Mining Code Idioms

The ﬁrst step of PATOIS is obtaining a set of frequent and useful AST fragments as code idioms. The
trade-off between frequency and usefulness is crucial: it is trivial to mine commonly occurring short
patterns, but they are often meaningless [1]. Instead, we employ and extend the methodology of
Allamanis et al. [3] and frame idiom mining as a nonparameteric Bayesian problem.

We represent idiom mining as inference over probabilistic tree substitution grammars (pTSG). A
pTSG is a probabilistic context-free grammar extended with production rules that expand to a whole
AST fragment instead of a single level of symbols [8, 29]. The grammar G of our original language L
induces a pTSG G0 with no fragment rules and with choice probabilities estimated from the corpus D.
To construct a pTSG corresponding to the extension of L with common tree fragments representing
idioms, we deﬁne a distribution G over pTSGs as follows.

We ﬁrst choose a Pitman-Yor process [36] as a prior distribution G0 over pTSGs. It is a nonparameteric
process that has proven to be effective for mining code idioms in prior work thanks to its modeling
of production choices as a Zipﬁan distribution (in other words, it implements the desired “rich get
richer” effect, which encourages a smaller number of larger and more common idioms).

3

Formally, it is a “stick-breaking” process [31] that
deﬁnes G0 as a distribution for each set of idioms (cid:101)IN
rooted at a nonterminal symbol N as

Pr(I ∈ (cid:101)IN ) def=

∞
(cid:88)

k=0

πk δ (I = Ik) ,

Ik ∼ G0

πk

def= uk

k−1
(cid:89)

(1 − uj), uk ∼ Beta (1 − d, α + kd)

j=1

where δ(·) is the delta function, and α, d are hyper-
parameters. See Allamanis et al. [3] for details.

Figure 2: MCMC sampling for an AST (ﬁg-
ure from [2]). Dots show the inferred nodes
where the AST is split into fragments.

PATOIS uses G0 to compute a posterior distribu-
tion G1 = Pr (G1 | T1, . . . , TN ) using Bayes’ rule,
where T1, . . . , TN are concrete AST fragments in the training set D. As this calculation is compu-
tationally intractable, we approximate it using type-based MCMC [23]. At each iteration t of the
MCMC process, PATOIS generates a pTSG Gt whose distribution approaches G1 as t → ∞. It works
by sampling splitting points for each AST T in the corpus D, which by construction deﬁne a set of
fragments constituting Gt (see Figure 2). The split probabilities of this Gibbs sampling are set in a
way that incentivizes merging adjacent tree fragments that often cooccur in D. The ﬁnal idioms are
then extracted from the pTSG obtained at the last MCMC iteration.

While the Pitman-Yor process helps avoid overﬁtting the idioms to D, not all sampled idioms are
useful for synthesis. Thus we rank and ﬁlter the idioms before using them in the training. In this
work, we reuse two ranking functions deﬁned by Allamanis et al. [3]:

ScoreCov (I) def= coverage = count(T ∈ D | I ∈ T )

ScoreCXE (I) def= coverage · cross-entropy gain =

count(T ∈ D | I ∈ T )
|D|

·

1
|I|

log

PrG1 (I)
PrG0 (I)

and also ﬁlter out any terminal idioms (i.e. those that do not contain any named arguments (cid:96)).

We conclude with a brief analysis of computational complexity of idiom mining. Every iteration
of the MCMC sampling traverses the entire dataset D once to sample the random variables that
deﬁne the splitting points in each AST. When run for M iterations, the complexity of idiom mining
is O(M · (cid:80)
T ∈D |T |). Idiom ranking adds an additional step with complexity O(|(cid:101)I| log |(cid:101)I|) where
(cid:101)I is the set of idioms obtained at the last iteration. In our experiments (detailed in Section 5) we set
M = 10, and the entire idiom mining takes less than 10 minutes on a dataset of |D| ≈ 10,000 ASTs.

4 Using Idioms in Program Synthesis

Given a set of common idioms (cid:101)I = {I1, . . . , IN } mined by PATOIS, we now aim to learn a
synthesis model f that emits whole idioms Ij as atomic actions instead of individual AST nodes that
comprise Ij. Achieving this involves two key challenges.

First, since idioms are represented as AST fragments without concrete syntax, PATOIS works best
when the synthesis model f is structural, i.e. it generates the program AST instead of its syntax.
Prior work [7, 38, 40] also showed that tree- and graph-based code generation models outperform
sequence-to-sequence models, and thus we adopt a similar architecture in this work.

Second, exposing the model f to idiom usage patterns is not obvious. One approach could be to
extend the grammar with new named operators opI((cid:96)1, . . . , (cid:96)k) for each idiom I, replace every
occurrence of I with opI in the data, and train the synthesizer on the rewritten dataset. However, this
would not allow f to learn from the idiom deﬁnitions (bodies). In addition, idiom occurrences often
overlap, and any deterministic rewriting strategy would arbitrarily discard some occurrences from
the corpus, thus limiting the model’s exposure to idiom usage. In our experiments, we found that
greedy rewriting discarded as many as 75% potential idiom occurrences from the dataset. Therefore,
a successful training strategy must preserve all occurrences and instead let the model learn a rewriting
strategy that optimizes end-to-end synthesis accuracy.

4

To this end, we present a novel training setup for code generation that encourages the model to
choose the most useful subset of idioms and the best representation of each program in terms of the
idioms. It works by (a) marking occurrences of the idioms (cid:101)I in the training set D, (b) at training
time, encouraging the model to emit either the whole idiom or its body for every potential idiom
occurrence in the AST, and (c) at inference time, replacing the model’s state after emitting an idiom I
with the state the model would have if it had emitted I’s body step by step.

4.1 Model Architecture

The synthesis model f of PATOIS combines a spec encoder fenc and an AST decoder fdec, following
the formulation of Yin and Neubig [38]. The encoder fenc embeds the NL speciﬁcation X = x1 · · · xn
into word representations ˆX = ˆx1 · · · ˆxn. The decoder fdec uses an LSTM to model the sequential
generation of the AST in the depth-ﬁrst order, wherein each timestep t corresponds to an action at
— either (a) expanding a production from the grammar, (b) expanding an idiom, or (c) generating a
terminal token. Thus, the probability of generating an AST T given ˆX is

Pr(T | ˆX) =

(cid:89)
t

Pr(cid:0)at | Tt, ˆX(cid:1)

(1)

where at is the action taken at timestep t, and Tt is the partial AST generated before t. The probability
Pr(at | Tt, ˆX) is computed from the decoder’s hidden state ht−1 depending on at.

Production Actions For actions at = APPLYRULE[R] corresponding to expanding production
rules R ∈ G from the original CFG G, we compute the probability Pr(at | Tt, ˆX) by encoding
the current partial AST structure similarly to Yin and Neubig [38]. Speciﬁcally, we compute the
new hidden state as ht = fLSTM ([at−1 (cid:107) ct (cid:107) hpt (cid:107) apt (cid:107) nft], ht−1) where at−1 is the embedding
of the previous action, ct is the result of soft attention applied to the spec embeddings ˆX as per
Bahdanau et al. [4], pt is the timestep corresponding to expanding the parent AST node of the current
node, and nft is the embedding of the current node type. The hidden state ht is then used to compute
probabilities of the syntactically appropriate production rules R ∈ G:

Pr(at = APPLYRULE[R] | Tt, ˆX) = softmaxR (g(ht))

(2)

where g(·) is a 2-layer MLP with a tanh non-linearity.

Terminal Actions For actions at = GETTOKEN[y], we compute the probability Pr(at | Tt, ˆX) by
combining a small vocabulary V of tokens commonly observed in the training data with a copying
mechanism [24, 30] over the input X to handle UNK tokens. Speciﬁcally, we learn two functions
pgen(ht) and pcopy(ht, X) such that pgen produces a score for each vocabulary token y ∈ V and pcopy
computes a score for copying the token y from the input. The scores are then normalized across the
entries corresponding to the same constant, as in [7, 38].

4.2 Training to Emit Idioms

As discussed earlier, training the model to emit idioms presents computational and learning challenges.
Ideally, we would like to extend Eq. (1) to maximize

J =

(cid:88)

|τ |
(cid:89)

τ ∈T

i=1

Pr(aτi | Tτi, ˆX)

(3)

where T is a set of different action traces that may produce the output AST T . The traces τ ∈ T
differ only in their possible choices of idiom actions APPLYRULE[opI] that emit some tree fragments
of T in a single step. However, computing Eq. (3) is intractable because idiom occurrences overlap
and cause combinatorial explosion in the number of traces T . Instead, we apply Jensen’s inequality
and maximize a lower bound:

log J = log

(cid:88)

|τ |
(cid:89)

τ ∈T

i=1

Pr(aτi | Tτi, ˆX) ≥ log(|T |) +

1
|T |

(cid:88)

|τ |
(cid:88)

τ ∈T

i=1

log Pr(aτi | Tτi, ˆX)

(4)

5

Let A(Tt) = {a∗
t } ∪ I(Tt) be the set of all valid actions to expand the AST Tt at timestep t. Here
a∗
t is the action from the original action trace that generates T using the original CFG and I(Tt)
is the set of idiom actions APPLYRULE[opI] also applicable at the node to be expanded in Tt. Let
c(T , t) also denote the number of traces τ ∈ T that admit an action choice for the AST Tt from
the original action trace. Since each action a ∈ A(Tt) occurs in the sum in Eq. (4) with probability
c(T , t) (cid:14) |A(Tt)|, we can rearrange this sum over traces as a sum over timesteps of the original trace:

1
|T |

(cid:88)

|τ |
(cid:88)

τ ∈T

i=1

log Pr(aτi | Tτi , ˆX) =

1
|T |

(cid:88)

(cid:88)

t

a∈A(Tt)

c(T , t)
|A(Tt)|

log Pr(a | Tτi, ˆX)

(cid:88)

=

t

(cid:88)

≈

t

1
|A(Tt)|

1
|A(Tt)|

(cid:88)

a∈A(Tt)
(cid:104)

c(T , t)
|T |

log Pr(a | Tτi, ˆX) = E
Tt∼T

1
|A(Tt)|

(cid:88)

a∈A(Tt)

log Pr(a | Tτi, ˆX)

log Pr(a∗

t | Tt, ˆX) +

(cid:88)

(cid:105)
log Pr(at = APPLYRULE[opI] | Tt, ˆX)

(5)

I∈M (Tt)

In the last step of Equation (5), we approximate
the expectation over ASTs randomly drawn
from all traces T using only the original trace
(containing all possible Tt) as a Monte Carlo
estimate.

Intuitively, at each timestep during training we
encourage the model to emit either the original
AST action for this timestep or any applicable
idiom that matches the AST at this step, with no
penalty to either choice. However, to avoid the
combinatorial explosion, we only teacher-force
the original generation trace (not the idiom bod-
ies), thus optimizing the bound in Eq. (5). Fig-
ure 3 illustrates this optimization process on an
example.

At inference time, whenever the model emits
an APPLYRULE[opI] action, we teacher-force
the body of I by substituting the embedding
of the previous action at−1 with embedding of
the previous action in the idiom deﬁnition, thus
emulating the tree fragment expansion. Outside
the bounds of I (i.e. within the hole subtrees of
I) we use the actual at−1 as usual.

5 Evaluation

Figure 3: Decoding the AST sorted(my_list,
reverse=True), ﬁgure adapted from [38]. Sup-
pose an idiom I = sorted( (cid:96) , reverse=True) is
mined and added as an operator opI((cid:96)) to the gram-
mar. At training time, PATOIS adjusts the cross-
entropy objective at timestep t2 to additionally al-
low opI as a valid production, with no change to
further decoding. At inference time, if decoder
emits an action at2 = APPLYRULE[opI], PATOIS
unrolls I on the ﬂy by teacher-forcing the shaded
portion of the AST generation.

Datasets We evaluate PATOIS on two semantic parsing datasets: Hearthstone [24] and Spider [41].

Hearthstone is a dataset of 665 card descriptions from the trading card game of the same name, along
with the implementations of their effects in Python using the game APIs. The descriptions act as NL
specs X, and are on average 39.1 words long.

Spider is a dataset of 10,181 questions describing 5,693 unique SQL queries over 200 databases
with multiple tables each. Each question pertains to a particular database, whose schema is given to
the synthesizer. Database schemas do not overlap between the train and test splits, thus challenging
the model to generalize across different domains. The questions are on average 13 words long and
databases have on average 27.6 columns and 8.8 foreign keys.

Implementation We mine the idioms using the training split of each dataset. Thus PATOIS cannot
indirectly overﬁt to the test set by learning its idioms, but it also cannot generalize beyond the idioms
that occur in the training set. We run type-based MCMC (Section 3) for 10 iterations with α = 5

6

Table 2: Ablation tests on the Hearthstone dev set.

Table 3: Ablation tests on the Spider dev set.

Model

K Exact
match

Sentence
BLEU

Corpus
BLEU

Baseline decoder — 0.197

PATOIS, ScoreCov

PATOIS, ScoreCXE

10
20
40
80

10
20
40
80

0.151
0.091
0.167
0.197

0.151
0.167
0.182
0.151

0.767

0.781
0.745
0.765
0.780

0.780
0.787
0.773
0.771

0.763

0.785
0.745
0.764
0.774

0.783
0.782
0.770
0.768

Model

K Exact match

Baseline decoder —

PATOIS, ScoreCov

PATOIS, ScoreCXE

10
20
40
80

10
20
40
80

0.395

0.394
0.379
0.395
0.407

0.368
0.382
0.387
0.416

and d = 0.5. After ranking (with either ScoreCOV or ScoreCXE) and ﬁltering, we use K top-ranked
idioms to train the generative model. We ran ablation experiments with K ∈ {10, 20, 40, 80}.

As described in Section 4, for all our experiments we used a tree-based decoder with a pointer
mechanism as the synthesizer f , which we implemented in PyTorch [27]. For the Hearthstone dataset,
we use a bidirectional LSTM [16] to implement the description encoder ˆX = fenc(X), similarly to
Yin and Neubig [38]. The word embeddings ˆx and hidden LSTM states h have dimension 256. The
models are trained using the Adadelta optimizer [42] with learning rate 1.0, ρ = 0.95, ε = 10−6 for
up to 2,600 steps with a batch size of 10.

For the Spider dataset, word embeddings ˆx have dimension 300, and hidden LSTM states h have
dimension 256. The models are trained using the Adam optimizer [20] with β1 = 0.9, β2 = 0.999,
ε = 10−9 for up to 40,000 steps with a batch size of 10. The learning rate warms up linearly up to
2.5 × 10−4 during the ﬁrst 2,000 steps, and then decays polynomially by (1 − t/T )−0.5 where T is
the total number of steps. Each model conﬁguration is trained on one NVIDIA GTX 1080 Ti GPU.

The Spider tasks additionally include the database schema as an input in the description. We follow a
recent approach of embedding the schema using relation-aware self-attention within the encoder [34].
Speciﬁcally, we initialize a representation for each column, table, and word in the question, and then
update these representations using 4 layers of relation-aware self-attention [32] using a graph that
describes the relations between columns and tables in the schema. See Section A in the appendix for
more details about the Spider schema encoder.

5.1 Experimental Results

In each conﬁguration, we compare the performance of equivalent trained models on the same dataset
with and without idiom-based training of PATOIS. For fairness, we show the performance of the same
decoder implementation described in Section 4.1 as a baseline rather than the state-of-the-art results
achieved by different approaches from the literature. Thus, our baseline is the decoder described
in Section 4.1 trained with a regular cross-entropy objective rather than the PATOIS objective in
Equation (5). Following prior work, we evaluate program generation as a semantic parsing task, and
measure (i) exact match accuracy and BLEU scores for Hearthstone and (ii) exact match accuracy of
program sketches for Spider.

Tables 2 and 3 show our ablation analysis of different conﬁgurations of PATOIS on the
Hearthstone and Spider dev sets, respectively. Table 4 shows the test set results of the best
model conﬁguration for Hearthstone (the test instances for the Spider dataset are unreleased).
As the results show, small numbers of idioms do not
signiﬁcantly change the exact match accuracy but im-
prove BLEU score, and K = 80 gives a signiﬁcant
improvement in both the exact match accuracy and
BLEU scores. The improvement is even more pro-
nounced on the test set with 4.5% improvement in
exact match accuracy and more than 4 BLEU points,
which shows that mined training set idioms general-

Table 4: Test set results on Hearthstone (using
the best conﬁgurations on the dev set).

Sentence
BLEU

Baseline
PATOIS

Corpus
BLEU

Exact
match

0.723
0.766

0.152
0.197

0.743
0.780

Model

7

def __init__(self):

(cid:96)0 : id =

super().__init__( (cid:96)0 : str , (cid:96)1 : int ,

copy.copy( (cid:96)1 : expr )

CHARACTER_CLASS. (cid:96)3 : id ,
CARD_RARITY. (cid:96)4 : id , (cid:96)?

5 )

class (cid:96)0 : id ( (cid:96)1 : id ):
def __init__(self):

SELECT COUNT( (cid:96)0 : col ), (cid:96)∗

1 WHERE (cid:96)∗

2

INTERSECT (cid:96)?

4 : sql EXCEPT (cid:96)?

5 : sql

WHERE (cid:96)0 : col = $terminal

Figure 4: Five examples of commonly used idioms from the Hearthstone and Spider datasets.

Figure 5: The distribution of used idioms in the inferred ASTs on the Hearthstone test set. Left: in
the ASTs exactly matched with ground truth; Right: all ASTs.

ize well to the whole data distribution. As mentioned above, we compare only to the same baseline
architecture for fairness, but PATOIS could also be easily implemented on top of the structural CNN
decoder of Sun et al. [35], the current state of the art on the Hearthstone dataset.

Figure 4 shows some examples of idioms that were frequently used by the model. On Hearthstone,
the most popular idioms involve common syntactic elements (e.g. class and function deﬁnitions) and
domain-speciﬁc APIs commonly used in card implementations (e.g. CARD_RARITY enumerations or
copy.copy calls). On Spider, they capture the most common combinations of SQL syntax, such
as a SELECT query with a single COUNT column and optional INTERSECT or EXCEPT clauses. Notably,
popular idioms are also often big: for instance, the ﬁrst idiom in Figure 4 expands to a tree fragment
with more than 20 nodes. Emitting it in a single step vastly simpliﬁes the decoding process.

We further conducted qualitative experiments to analyze actual idiom usage by PATOIS on the
Hearthstone test set. Figure 5 shows the distribution of idioms used in the inferred (not ground truth)
ASTs. A typical program involves 7 idioms on average, or 6 for the programs that exactly match the
ground truth. Despite the widespread usage of idioms, not all of the mined idioms (cid:101)I were useful: only
51 out of K = 80 idioms appear in the inferred ASTs. This highlights the need for an end-to-end
version of PATOIS where idiom mining would be directly optimized to beneﬁt synthesis.

6 Related Work

Program synthesis & Semantic parsing Program synthesis from natural language and input-
output examples has a long history in Programming Languages (PL) and Machine Learning (ML)
communities (see Gulwani et al. [15] for a survey). When an input speciﬁcation is limited to natural
language, the resulting problem can be considered semantic parsing [22]. There has been a lot of
recent interest in applying recurrent sequence-based and tree-based neural networks to semantic
parsing [11, 18, 21, 38, 40]. These approaches commonly use insights from the PL literature,
such as grammar-based constraints to reduce the search space, non-deterministic training oracles
to enable multiple executable interpretations of intent, and supervision from program execution.
They typically either supervise the training on one or more golden programs, or use reinforcement
learning to supervise the training from a neural program execution result [26]. Our PATOIS approach

8

is applicable to any underlying neural semantic parsing model, as long as it is supervised by a corpus
of golden programs. It is, however, most easily applicable to tree-based and graph-based models,
which directly emit the AST of the target program. In this work we have evaluated PATOIS as applied
on top of the sequence-to-tree decoder of Yin and Neubig [38], and extended it with a novel training
regime that teaches the decoder to emit idiom operators in place of the idiomatic code fragments.

Sketch generation Two recent works [12, 25] learn abstractions of the target program to compress
and abstract the reasoning process of a neural synthesizer. Both of them split the generation process
into sketch generation and sketch completion, wherein the ﬁrst stage emits a partial tree/sequence (i.e.
a sketch of the program) and the second stage ﬁlls in the holes in this sketch. While sketch generation
is typically implemented with a neural model, sketch completion can be either a different neural
model or a combinatorial search. In contrast to PATOIS, both works deﬁne the grammar of sketches
manually by a deterministic program abstraction procedure and only allow a single top-level sketch
for each program. In addition, an earlier work of Bošnjak et al. [6] also formulates program synthesis
as sketch completion, but in their work program sketches are manually provided rather than learned.
In PATOIS, we learn the abstractions (code idioms) automatically from a corpus and allow them to
appear anywhere in the program, as is common in real-life programming.

Learning abstractions Recently, Ellis et al. [13] developed an Explore, Compress & Com-
pile (EC2) framework for automatically learning DSLs for program synthesis from I/O examples (such
as the DSLs used by FlashFill [14] and DeepCoder [5]). The workﬂow of EC2 is similar to PATOIS,
with three stages: (a) learn new DSL subroutines from a corpus of tasks, (b) train a recognition model
that maps a task speciﬁcation to a distribution over DSL operators as in DeepCoder [5], and (c) use
these operators in a program synthesizer. PATOIS differs from EC2 in three aspects: (i) we assume a
natural language speciﬁcation instead of examples, (ii) to handle NL speciﬁcations, our synthesizer is
a neural semantic parser instead of enumerative search, and (iii) most importantly, we discover idioms
that compress general-purpose languages instead of extending DSLs. Unlike for inductive synthesis
DSLs such as FlashFill, the existence of useful DSL abstractions for general-purpose languages is not
obvious, and our work is the ﬁrst to demonstrate them.

Concurrently with this work, Iyer et al. [17] developed a different approach of learning code idioms for
semantic parsing. They mine the idioms using a variation of byte-pair encoding (BPE) compression
extended to ASTs and greedily rewrite all the dataset ASTs in terms of the found idioms for training.
While the BPE-based idiom mining is more computationally efﬁcient than non-parametric Bayesian
inference of PATOIS, introducing ASTs greedily tends to lose information about overlapping idioms,
which we address in PATOIS using our novel training objective described in Section 4.2.

As described previously, our code idiom mining is an extension of the procedure developed by
Allamanis et al. [2, 3]. They are the ﬁrst to use the tree substitution grammar formalism and
Bayesian inference to ﬁnd non-trivial common idioms in a corpus of code. However, their problem
formalization does not involve any application for the learned idioms beyond their explanatory power.

7 Conclusion

Semantic parsing, or neural program synthesis from natural language, has made tremendous progress
over the past years, but state-of-the-art models still struggle with program generation at multiple
levels of abstraction. In this work, we present a framework that allows incorporating learned coding
patterns from a corpus into the vocabulary of a neural synthesizer, thus enabling it to emit high-level
or low-level program constructs interchangeably at each generation step. Our current instantiation,
PATOIS, uses Bayesian inference to mine common code idioms, and employs a novel nondeterministic
training regime to teach a tree-based generative model to optionally emit whole idiom fragments.
Such dataset abstraction using idioms improves the performance of neural program synthesis.

PATOIS is only the ﬁrst step toward learned abstractions in program synthesis. While code idioms
often correlate with latent semantic concepts and our training regime allows the model to learn which
idioms to use and in which context, our current method does not mine them with the intent to directly
optimize their usefulness for generation. In future work, we want to alleviate this by jointly learning
the mining and synthesis models, thus optimizing the idioms’ usefulness for synthesis by construction.
We also want to incorporate program semantics into the idiom deﬁnition, such as data ﬂow patterns
or natural language phrases from task specs.

9

References

[1] C. C. Aggarwal and J. Han. Frequent pattern mining. Springer, 2014.

[2] M. Allamanis and C. Sutton. Mining idioms from source code. In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering (FSE), pages 472–483. ACM, 2014.

[3] M. Allamanis, E. T. Barr, C. Bird, P. Devanbu, M. Marron, and C. Sutton. Mining semantic loop idioms.

IEEE Transactions on Software Engineering, 2018.

[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.

In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.

[5] M. Balog, A. L. Gaunt, M. Brockschmidt, S. Nowozin, and D. Tarlow. DeepCoder: Learning to write
programs. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.

[6] M. Bošnjak, T. Rocktäschel, J. Naradowsky, and S. Riedel. Programming with a differentiable Forth
interpreter. In Proceedings of the 34th International Conference on Machine Learning (ICML), volume 70,
pages 547–556, 2017.

[7] M. Brockschmidt, M. Allamanis, A. L. Gaunt, and O. Polozov. Generative code modeling with graphs. In

Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019.

[8] T. Cohn, P. Blunsom, and S. Goldwater. Inducing tree-substitution grammars. Journal of Machine Learning

Research, 11(Nov):3053–3096, 2010.

[9] J. Devlin, R. Bunel, R. Singh, M. Hausknecht, and P. Kohli. Neural program meta-induction. In Advances

in Neural Information Processing Systems (NIPS), pages 2080–2088, 2017.

[10] J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A.-r. Mohamed, and P. Kohli. RobustFill: Neural program
learning under noisy I/O. In Proceedings of the 34th International Conference on Machine Learning
(ICML), 2017.

[11] L. Dong and M. Lapata. Language to logical form with neural attention. In Proceedings of the 54th Annual

Meeting of the Association for Computational Linguistics (ACL), 2016.

[12] L. Dong and M. Lapata. Coarse-to-ﬁne decoding for neural semantic parsing. In Proceedings of the 56th

Annual Meeting of the Association for Computational Linguistics (ACL), 2018.

[13] K. Ellis, L. Morales, M. Sablé-Meyer, A. Solar-Lezama, and J. Tenenbaum. Learning libraries of
subroutines for neurally-guided Bayesian program induction. In Advances in Neural Information Processing
Systems, pages 7816–7826, 2018.

[14] S. Gulwani. Automating string processing in spreadsheets using input-output examples. In Proceedings of
the 38th ACM Symposium on Principles of Programming Languages (POPL), volume 46, pages 317–330,
2011.

[15] S. Gulwani, O. Polozov, and R. Singh. Program synthesis. Foundations and Trends® in Programming

Languages, 4(1-2):1–119, 2017.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[17] S. Iyer, A. Cheung, and L. Zettlemoyer. Learning programmatic idioms for scalable semantic parsing. In

EMNLP, 2019.

[18] R. Jia and P. Liang. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual

Meeting of the Association for Computational Linguistics (ACL), volume 1, pages 12–22, 2016.

[19] A. Kalyan, A. Mohta, O. Polozov, D. Batra, P. Jain, and S. Gulwani. Neural-guided deductive search
for real-time program synthesis from examples. In Proceedings of the 6th International Conference on
Learning Representations (ICLR), 2018.

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of 3rd International

Conference on Learning Representations (ICLR), 2015.

[21] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In Proceedings

of the 4th International Conference on Learning Representations (ICLR), 2016.

[22] P. Liang. Learning executable semantic parsers for natural language understanding. Communications of

the ACM, 59(9):68–76, 2016.

10

[23] P. Liang, M. I. Jordan, and D. Klein. Type-based MCMC. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
573–581. Association for Computational Linguistics, 2010.

[24] W. Ling, P. Blunsom, E. Grefenstette, K. M. Hermann, T. Koˇcisk`y, F. Wang, and A. Senior. Latent predictor
networks for code generation. In ACL, volume 1, pages 599–609, 2016. URL https://github.com/
deepmind/card2code.

[25] V. Murali, L. Qi, S. Chaudhuri, and C. Jermaine. Neural sketch learning for conditional program generation.

In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.

[26] A. Neelakantan, Q. V. Le, M. Abadi, A. McCallum, and D. Amodei. Learning a natural language interface
with neural programmer. In Proceedings of the 5th International Conference on Learning Representations
(ICLR), 2017.

[27] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and

A. Lerer. Automatic differentiation in PyTorch. 2017.

[28] O. Polozov and S. Gulwani. FlashMeta: A framework for inductive program synthesis. In Proceedings of
the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages,
and Applications (OOPSLA), pages 107–126, 2015.

[29] M. Post and D. Gildea. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-

IJCNLP 2009 Conference Short Papers, pages 45–48. Association for Computational Linguistics, 2009.

[30] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator networks. In
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), volume 1,
pages 1073–1083, 2017.

[31] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica sinica, pages 639–650, 1994.

[32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. In Proceedings
of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short Papers), 2018.

[33] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-Attention with Relative Position Representations. In Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468. Association for
Computational Linguistics, 2018. doi: 10.18653/v1/N18-2074.

[34] R. Shin. Encoding database schemas with relation-aware self-attention for text-to-SQL parsers. arXiv

preprint arXiv:1906.11790, 2019.

[35] Z. Sun, Q. Zhu, L. Mou, Y. Xiong, G. Li, and L. Zhang. A grammar-based structural CNN decoder for

code generation. In AAAI, 2019.

[36] Y. W. Teh and M. I. Jordan. Hierarchical Bayesian nonparametric models with applications. Bayesian

nonparametrics, 1:158–207, 2010.

[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008.
Curran Associates, Inc., 2017.

[38] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. In ACL, July 2017.

[39] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig. Learning to mine aligned code and natural language
pairs from StackOverﬂow. In International Conference on Mining Software Repositories (MSR), pages
476–486. ACM, 2018.

[40] P. Yin, C. Zhou, J. He, and G. Neubig. StructVAE: Tree-structured latent variable models for semi-
supervised semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2018.

[41] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and
D. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing
and text-to-SQL task. In EMNLP, 2018. URL https://yale-lily.github.io/spider.

[42] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

11

Table 5: Description of edge types present in the directed graph created to represent the schema. An
edge exists from node x to node y if the pair fulﬁlls one of the descriptions listed in the table, with
the corresponding label. Otherwise, no edge exists from x to y.
Edge label

Type of x Type of y

Description

Column

Column

Column

Table

Table

Column

Table

Table

x and y belong to the same table.
SAME-TABLE
x is a foreign key for y.
FOREIGN-KEY-COL-F
FOREIGN-KEY-COL-R y is a foreign key for x.

PRIMARY-KEY-F
BELONGS-TO-F

PRIMARY-KEY-R
BELONGS-TO-R

x is the primary key of y.
x is a column of y (but not the primary key).

y is the primary key of x.
y is a column of x (but not the primary key).

FOREIGN-KEY-TAB-F
FOREIGN-KEY-TAB-R Same as above, but x and y are reversed.
FOREIGN-KEY-TAB-B x and y have foreign keys in both directions.

Table x has a foreign key column in y.

A Encoder for Spider dataset

In the Spider dataset, each entry contains a question along with a database schema, containing tables and columns.
We will use the following notation:

• ci for each column in the schema. Each column contains words ci,1, · · · , ci,|ci|.
• ti for each table in the schema. Each table contains words ti,1, · · · , ti,|ti|.
• q for the input question. The question contains words q1, · · · , q|q|.

A.1 Encoding the Schema as a Graph

We begin by representing the database schema using a directed graph G, where each node and edge has a label.
We represent each table and column in the schema as a node in this graph, labeled with the words in the name;
for columns, we prepend the type of the column to the label. For each pair of nodes x and y in the graph, Table 5
describes when there exists an edge from x to y and the label it should have.

A.2

Initial Encoding of the Input

We now obtain an initial representation for each of the nodes in the graph, as well as for the words in the input
question. Formally, we perform the following:

i,0 , crev
(cfwd
i,1 , trev
(tfwd
(qfwd

i,0), · · · , (cfwd
i,1), · · · , (tfwd

i,|ci|, crev
i,|ti|, trev

1 , qrev

1 ), · · · , (qfwd

|q| , qrev

i,|ci|) = BiLSTMColumn(ctype
i,|ti|) = BiLSTMTable(ti,1, · · · , ti,|ti|);

i,|ci|, crev
i = Concat(cfwd
cinit
, ci,1, · · · , ci,|ci|);
i,0)
i,|ci|, trev
i = Concat(tfwd
tinit
i,1)
, qrev
i = Concat(qfwd
|q|) = BiLSTMQuestion(q1, · · · , q|q|); qinit
i )

i

i

where each of the BiLSTM functions ﬁrst lookup word embeddings for each of the input tokens. The LSTMs do
not share any parameters with each other.

A.3 Relation-Aware Self-Attention

At this point, we have representations cinit
. Now, we would like to imbue these representations
i
with the information in the schema graph. We use a form of self-attention [37] that is relation-aware [33] to
achieve this goal.
In one step of relation-aware self-attention, we begin with an input x of n elements (where xi ∈ Rdx ) and
transform each xi into yi ∈ Rdz . We follow the formulation described in Shaw et al. [33]:

, and qinit

, tinit
i

i

e(h)
ij =

z(h)
i =

xiW (h)

Q (xjW (h)
(cid:112)dz/H

K + rK

ij )T

n
(cid:88)

j=1

ij (xjW (h)
α(h)

V + rV

ij );

; α(h)

ij =

exp(e(h)
ij )
l=1 exp(e(h)
il )

(cid:80)n

zi = Concat(z(0)

i

, · · · , z(H)

i

)

12

˜yi = LayerNorm(xi + zi);

yi = LayerNorm(˜yi + FC(ReLU(FC(˜yi)))

The rij terms encode the relationship between the two elements xi and xj in the input. We explain how we
obtain rij in the next part.

For the application within the Spider encoder, we ﬁrst construct the input x of |c| + |t| + |q| elements using cinit
tinit
i

, and qinit

:

i

i

x = (cinit

1 , · · · , cinit

|c|, tinit

1 , · · · , tinit

|t| , qinit

1 , · · · , qinit

|q|).

,

We then apply a stack of 4 relation-aware self-attention layers. We set dz = dx to facilitate this stacking. The
weights of the encoder layers are not tied; each layer has its own set of weights.
We deﬁne a discrete set of possible relation types, and map each type to an embedding to obtain rV
ij . We
need a value of rij for every pair of elements in x. If xi and xj both correspond to nodes in G (i.e. each is either
a column or table) with an edge from xi to xj, then we use the label on that edge (possibilities listed in Table 5).

ij and rK

However, this is not sufﬁcient to obtain rij for every pair of i and j. In the graph we created for the schema, we
have no nodes corresponding to the question words; not every pair of nodes in the graph has an edge between
them (the graph is not complete); and we have no self-edges (for when i = j). As such, we add more types
beyond what is deﬁned in Table 5:

• xi ∈ question, xj ∈ question: QUESTION-DIST-d, where d = clip(j − i, D); clip(a, D) =

max(−D, min(D, a)). We use D = 2.

• If i = j, then COLUMN-IDENTITY or TABLE-IDENTITY.

• xi ∈ question, xj ∈ column ∪ table; or xi ∈ column ∪ table, xj ∈ question:

QUESTION-COLUMN, QUESTION-TABLE, COLUMN-QUESTION or TABLE-QUESTION depending
on the type of xi and xj.

• Otherwise, one of COLUMN-COLUMN, COLUMN-TABLE, TABLE-COLUMN, or TABLE-TABLE.

In the end, we add 2 + 5 + 4 + 4 types beyond the 10 in Table 5, for a total of 25 types.

After processing through the stack of N encoder layers, we obtain

(cﬁnal
1

, · · · , cﬁnal

|c| , tﬁnal

1

, · · · , tﬁnal

|t| , qﬁnal

1

, · · · , qﬁnal

|q| ) = y.

We use cﬁnal

i

, tﬁnal
i

, and qﬁnal

i

in the decoder.

13

