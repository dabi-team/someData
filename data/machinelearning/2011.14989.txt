The Calculus
A declarative model of reversible programming with relevance to Brownian
computers

Hannah Earley*

Department of Applied Mathematics and Theoretical Physics, University of Cambridge

1
2
0
2

v
o
N
0
3

]
L
P
.
s
c
[

3
v
9
8
9
4
1
.
1
1
0
2
:
v
i
X
r
a

Figure 1: An abstract molecular realisation of an ℵ deﬁnition of addition, here reversibly comput-

ing the sum of 4 and 3.

Lay Summary

In recent years, unconventional forms of computing ranging from molecular computers
made out of DNA to quantum computers have started to be realised. Not only that, but
they are becoming increasingly sophisticated and have a lot of potential to inﬂuence the
future of computing. Another interesting class of unconventional computers is that of
reversible computers, of which quantum computers are an example. Reversible computing—
wherein state transitions must be invertible, and therefore must conserve information—is
largely neglected outside of quantum computing, but is a promising avenue for realising
substantial gains in computational performance and energy efﬁciency. We are interested
in the intersection between molecular and reversible computing: although much success

*h.earley@damtp.cam.ac.uk,

orcid.org/0000-0002-6628-2130

1

 
 
 
 
 
 
has been achieved in developing irreversible molecular computers driven by strong entropic
forces, irreversibility can only be achieved in an approximate sense as the laws of physics
are fundamentally reversible. Moreover, as molecular systems operate far closer to the
underlying microscopic laws of physics, they are (arguably) inﬂuenced by this reversibility
to a far greater extent. As such, we believe that reversibility is a far more natural basis for
designing molecular computers.

In this paper, we introduce a novel model of computation—the ℵ calculus—that aims to
be a step towards this goal. At present, ℵ is a mathematical model of computation capable of
concurrency, and whose objects are self-contained computational entities that are well-suited
for a Brownian or molecular context. Reversibility of computation is achieved by the idea of
learning and un-learning the state of variables (whence the name of the calculus, inspired by
the Greek for not forgotten, ἀλήθεια). Whilst more work needs to be done in order to realise
something resembling ℵ experimentally in a molecular context (for which, DNA computing
is a very promising approach), we include in some of our examples what such molecular
computational entities may look like—a preview of which is shown in Figure 1. In the
meantime, computation with ℵ can be realised on conventional computers via an associated
programming language we also introduce, alethe.

Technical Abstract

Motivated by a need for a model of reversible computation appropriate for a Brownian mo-
lecular architecture, the ℵ calculus is introduced. This novel model is declarative, concurrent,
and term-based—encapsulating all information about the program data and state within a
single structure in order to obviate the need for a von Neumann-style discrete computational
‘machine’, a challenge in a molecular environment. The name is inspired by the Greek for
‘not forgotten’, due to the emphasis on (reversibly) learning and un-learning knowledge
of different variables. To demonstrate its utility for this purpose, as well as its elegance
as a programming language, a number of examples are presented; two of these examples,
addition/subtraction and squaring/square-rooting, are furnished with designs for abstract
molecular implementations. A natural by-product of these examples and accompanying
syntactic sugar is the design of a fully-ﬂedged programming language, alethe, which is
also presented along with an interpreter. Efﬁciently simulating ℵ on a deterministic computer
necessitates some static analysis of programs within the alethe interpreter in order to render
the declarative programs sequential. Finally, work towards a type system appropriate for
such a reversible, declarative model of computation is presented.

2

1. Introduction

Since the advent of thermodynamics and statistical mechanics, it has become increasingly clear
that the laws of physics are intrinsically reversible at a microscopic level. The connection between
statistical mechanics and information theory has also been resolved to most’s satisfaction, with
Szilard [1] and Landauer [2] providing a solution to Maxwell’s eponymous thought experiment,
the Maxwell Dæmon (Figure 2). In contrast, most models of computation are intrinsically
irreversible, readily discarding information at nearly every computational step. For example, a
Turing Machine [3] may overwrite or erase a square of its tape, whilst an implementation of the λ
calculus [4] may discard its redex. The consequence is that the heat generated by conventional
computers is an unavoidable byproduct of their operation (although the amount generated by
contemporary consumer processors is some 6–10 orders of magnitude greater1 still than the lower
bound found by Landauer [2]).

Fortunately computation is not inherently irreversible, as Bennett [6]—the father of reversible
computing—found. He introduced a reversible analogue of the Turing Machine and even
demonstrated an algorithm that could embed any irreversible computation x (cid:55)→ f (x) within a
reversible computer as x (cid:55)→ (x, f (x)), cleaning up any intermediate state through a reversible
process rather than discarding to the environment as is standard. He later [7] iterated upon this
algorithm to show this could be achieved efﬁciently in both time and space. Nevertheless we are
not satisﬁed with this embedding, and wish to better exploit reversible computational architectures
by programming directly with reversible primitives. The beneﬁts of doing so are that one often
ﬁnds that far less temporary information need be generated than Bennett’s algorithms might
suggest, and also the injective embedding, x (cid:55)→ (x, f (x)), retains the input which is excessive
except in the trivial case of f being constant; for any other function f having any correlation at all
between its outputs and inputs, only a partial image of x need be preserved. Moreover it is often
the case that one can imagine a suitable and more preferable injective or bijective embedding.
For example, for the operation of addition a useful embedding might take the form + : (x, y) ↔
(x, x + y), whereas Bennett’s algorithm would yield + : (x, y) ↔ (x, y, x + y). In deﬁning these
injections more carefully, one then often ﬁnds that the residual information of the input can in fact
be made use of; for example, a Peano arithmetic implementation of + : (x, y) ↔ (x, x + y) over
the naturals readily begets the (domain-restricted) injection ∀x > 0.× : (x, y) ↔ (x, xy) without
generating any temporary data whatsoever. Moreover, when redundancy is eliminated as in these
two cases one obtains the converse operation for free: that is, running + : (x, y) ↔ (x, x + y) in
reverse yields subtraction, and ∀x > 0.× : (x, y) ↔ (x, xy) yields division, whilst their Bennett

1It is somewhat challenging to precisely quantify the information processing capacity of a modern consumer
processor due to the extensive use of pipelining, vectorisation and multiple-instruction dispatch. Nevertheless,
we can obtain a reasonable estimate. arm’s processors are well known for emphasising power efﬁciency, and are
used extensively in mobile electronics as well as some laptops and desktops (notably, three of Apple’s new device
lineup). Considering arm’s A-78 processor micro-architecture, we ﬁnd [5] numbers of around 1 W/core, with each
core running at a clock speed of 3 GHz and executing up to 6 (macro)instructions per clock cycle with a width
of up to 128 bit. Taken together, these give an energy dissipation of roughly 1.5 × 10−13 J bit−1. In contrast, at
a temperature of around 300 K, Landauer’s bound gives just 2.9 × 10−21 J bit−1, a 5 × 107 overhead for the arm
processor. Intel’s chips have thermal design powers typically exceeding 100 W, but have compensatingly greater
vectorisation widths of some instructions, such that a similar Landauer overhead can be achieved when fully
exploiting the processor’s capabilities.

3

Figure 2: Maxwell’s Dæmon (1867) is a thought experiment which, if valid, would offer a method
to violate the second law of thermodynamics. Consider a box divided in two; to the left
of the divider is a red gas, and to the right a blue gas. If the divider is removed, then
mechanical work can be extracted as the gases mix and equilibrate. Now suppose the
divider is re-inserted, and endowed with a small window whose hinge is frictionless
and can therefore be opened and closed without energetic cost. Imagine that a dæmon
or other such entity, with particularly keen eyesight and nimble ﬁngers, is positioned at
the window. If she observes a red particle in the right compartment moving towards
the window, or a blue particle in the left compartment moving towards the window, she
brieﬂy opens it to allow the particle’s passage; otherwise she leaves the window closed.
Over time, this process will bring the box back to its initial state, thereby reducing its
entropy.

The problem is that this process requires the dæmon to learn information about
the state of the gas particles, and this information cannot be ‘unlearned’; therefore
her memory will eventually be ﬁlled by this stale information. Forgetting all this
information requires a commensuarate increase in entropy. If this entropy manifests as
heat, then Landauer showed that discarding a quantity of information ∆I requires the
production of heat ∆Q ≥ kT ∆I where k is Boltzmann’s constant, T the temperature,
and equality only in the limit of a thermodynamically reversible process (i.e. taking
inﬁnite time).

4

embeddings cannot do the same. A little thought shows these must fail in certain cases, and we
shall have more to say about this in the following section.

A number of designs of reversible computer architectures and reversible programming lan-
guages have thus arisen. One of the ﬁrst suggestive of a physical implementation was the ballistic
architecture of Fredkin and Toffoli [8], upon which Ressler [9] proposed a full processor design in
his Master’s thesis; the ballistic architecture makes use of perfectly elastic classical billiard balls
projected on a frictionless surface at prescribed velocities and initial positions, which then bounce
off of strategically placed walls and each other. Classical physics, being reversible, ensures
that the trajectories of the balls is reversible and it was shown by Fredkin and Toffoli that these
trajectories could encode arbitrary computation by devising a series of reversible logic gates.
Unfortunately Bennett’s analysis of this architecture showed that it could not be realised in our
universe:

“Even if classical balls could be shot with perfect accuracy into a perfect apparatus,
ﬂuctuating tidal forces from turbulence in the atmospheres of nearby stars would be
enough to randomise their motion within a few hundred collisions. Needless to say,
the trajectory would be spoiled much sooner if stronger nearby noise sources (e.g.,
— Bennett [10]
thermal radiation and conduction) were not eliminated.”

See Frank [11] and Earley [12, 13, 14] for further analysis of the constraints physics puts on the
performance of reversible computers. More practicable architectures have since arisen, such as
Pendulum [15], as well as a host of languages [16, 17, 18, 19, 20, 21]. Furthermore, it transpires
that Quantum Computers are necessarily reversible as irreversible logic can only be achieved by
disrupting the well-prepared quantum state of the system, and therefore all quantum computing
architectures and quantum programming languages are reversible.

In this paper we present a new model of reversible computing, the ℵ Calculus, and an associated
programming language alethe together with interpreter, whose properties we believe to be novel.
The name is inspired by the Greek meaning ‘not forgotten’, as the semantics of ℵ revolve around
the transformation of knowledge: ‘unlearning’ knowledge of one or more variables in order
to ‘learn’ knowledge about one or more other variables, but in a reversible fashion such that
nothing is ever truly forgotten. ℵ is declarative and concurrent and, whilst perhaps a little
too abstract and high level for this purpose, is motivated by a need for a reversible model for
molecular programming and DNA computing. Crucial to such applications is that almost all
information about not just the program’s data, but the program state too, should be encoded within
a computation ‘term’. This is in contrast to the imperative and functional languages referenced
above, wherein state information such as the instruction counter (indicating where in the program
the current execution context is) is implicitly stored in a special register or other hidden state of
the processor; in ℵ, the program itself is (reversibly) mutated during the course of its execution,
such that the distinction between ‘program’ and ‘processor’ vanishes. The reason for this design
is that a von Neumann-style architecture, in which there is a discrete processing unit that interacts
with a memory unit to execute a program, is unsuitable for a molecular context as it is—in some
sense—too ‘bulky’, and is difﬁcult to engineer. We assert that the proposed design is far more
suitable to molecular implementation, or at least serves as a step towards this goal, and it is hoped
the reader will be convinced of this by the examples and semantics illustrated herein.

5

2.

by Example

Before expositing the formal semantics of ℵ, it is illuminating to introduce a number of examples
of ℵ in order to gain an intutition for its syntax and features. We begin with the reversible
deﬁnition of natural addition from the introduction, but ﬁrst there is a point to be made regarding
injectivity and bijectivity. Recall the Bennett-style reversible embedding of addition, +Bennett :
(x, y) ↔ (x, y, x + y); it is easy to see that the forward direction of this program maps, e.g.,
(3, 4) to (3, 4, 7), and likewise the reverse direction maps (3, 4, 7) to (3, 4). Clearly the forward
direction is an injection, but what about the reverse? Suppose we attempt to feed in (3, 4, 5):
as 3 + 4 (cid:54)= 5, and as the forward direction is injective, there can be no pair (a, b) that maps
to (3, 4, 5). In fact, even our less redundant embedding + : (x, y) ↔ (x, x + y) suffers from
non-injectivity of its inverse, for example there is no value y ∈ N satisfying + : (5, y) ↔ (5, 2).
Yet another example occurs for the function +(cid:48) : (x, y) ↔ (x − y, x + y) deﬁned over the
integers, in which there is no pair (x, y) satisfying +(cid:48) : (x, y) ↔ (3, 6) (there is in Q, however;
namely, ( 9
2 )). Therefore, whilst we might expect reversible computers to compute bijective
operations, this appears to be violated in even the simple example of addition. In fact, no violation
of reversibility has occurred, and the ‘primitive’ steps of any reversible computer are bijective.
What we have encountered here is simply a (co)domain error, the same as if we were to ask a
conventional computer to evaluate 1/0. That is, whereas conventional computers compute partial
functions, reversible computers compute partial bijections (or partial isomorphisms). Exactly
what happens when such an error is encountered depends on both the algorithm and architecture
in question; for example, attempting to divide 1 by 0 may cause a computer to immediately
complain, or it may enter an inﬁnite loop if the algorithm is ‘repeatedly subtract the divisor until
the dividend vanishes’. We shall have more to say about how ℵ handles such errors in due course.

2 , 3

Addition Our examples will mostly concern natural numbers because they are particularly
amenable to inductive and recursive deﬁnitions of both their structure and operations over them,
such as addition and multiplication. The standard approach to this is the Peano axiomatic
formulation, in which a natural number is deﬁned to either be Z ≡ 0, or Sn ≡ n + 1 whenever n
is a natural number—i.e. the successor of n. For example, 4 is constructed as S(S(S(SZ))). In fact
there are seven more axioms in order to clarify such subtle points as uniqueness of representation,
conditions for equality, and non-negativity. Addition can then be deﬁned recursively by the base
case Z + b = b and the inductive step Sa + b = S(a + b), and multiplication by a · Z = Z
and a · Sb = a + a · b. To render addition reversible, we write + : (Z, b) ↔ (Z, b) and
+ : (Sa, b) ↔ (Sa, S(a + b)), realising the proposed embedding + : (x, y) ↔ (x, x + y).
Multiplication is embedded similarly, but there is a domain-restriction imposed in that a must not
be zero (or else b cannot be uniquely recovered); b may, however, be zero. In ℵ, this is written
thus

+ Z b () = () Z b +;
+ (Sa) b () = () (Sa) (Sb(cid:48)) +:
+ a b () = () a b(cid:48) +.

(ADD–BASE)

(ADD–STEP)

(ADD–STEP–SUB)

6

+ Z b () = () Z b +;
+ (Sa) b () = () (Sa) (Sb(cid:48)) +:
+ a b () = () a b(cid:48) +.

(ADD–BASE)

(ADD–STEP)

(ADD–STEP–SUB)

(a) The deﬁnition of reversible natural addition in ℵ.

+ 4 3 () (cid:33) {a : 3, b : 3}

{a : 3, b(cid:48) : 6} (cid:33) () 4 7 +

(ADD–STEP)

(ADD–STEP–SUB)

+ 3 3 () (cid:33) {a : 2, b : 3}

{a : 2, b(cid:48) : 5} (cid:33) () 3 6 +

(ADD–STEP)

(ADD–STEP–SUB)

+ 2 3 () (cid:33) {a : 1, b : 3}

{a : 1, b(cid:48) : 4} (cid:33) () 2 5 +

(ADD–STEP)

+ 1 3 () (cid:33) {a : 0, b : 3}

{a : 0, b(cid:48) : 3} (cid:33) () 1 4 +

(ADD–STEP)

+ Z 3 () (cid:33) {b : 3} (cid:33) () Z 3 +

(ADD–STEP–SUB)

(ADD–BASE)

(ADD–STEP–SUB)

(b) The ℵ execution path when reversibly adding 4 to 3 or, in reverse, subtracting 4 from 7. The (cid:33) arrows
refer to pattern matching/substitution, whilst the solid arrows refer to instantiation/consumption of
‘sub-terms’.

() 5 2 + (cid:33) {a : 4, b(cid:48) : 1}

() 4 1 + (cid:33) {a : 3, b(cid:48) : 0}

() 3 Z + (cid:32)/

(ADD–STEP)

(ADD–STEP–SUB)

(ADD–STEP)

(ADD–STEP–SUB)

no matching rule

(c) The ℵ execution path when attempting to (erroneously) subtract 5 from 2. The recursive algorithm
identiﬁes that 2 − 5 ≡ 0 − 3, but there is no matching deﬁnition for this and therefore computation
‘stalls’ on this sub-term. This is usually addressed in the natural numbers by employing the saturating
option of ‘monus’, i.e. 2 ·− 5 = 0, but it is not reversible.

Listing 1: The deﬁnition of, and example applications of, reversible addition in ℵ.

7

and is perhaps best understood by example. In Listing 1, we perform the example addition of 4
and 3 and, as promised, an example failure mode in which we attempt to subtract 5 from 2.

ℵ can thus be seen, in a loose sense, to be a term-rewriting system. It is ‘loose’ in the sense that
its ‘sub-terms’ exist ‘separate’ from their parent term. In the example of Listing 1, an addition
term is written + a b () and is mapped by the transition rules to () a c + where c ≡ a + b;
here + is an ‘atom’, a, b and c are terms representing natural numbers (as composite terms
formed from nested applications of the atoms S and Z), and (), or ‘unit’, is the empty term and
is used by convention in ℵ to avoid certain ambiguities. A program corresponds to a series of
deﬁnitions of transition rules which pattern match on terms, and then substitute the variables into
an output pattern. This matching process is subject to certain constraints that ensure reversibility.
In addition, a rule may specify sub-rules that indicate how to transform knowledge of some
variable, e.g. b, to knowledge of another, e.g. b(cid:48), and is the primary mechanism of composition in
ℵ. Inherent to the semantics of the calculus is a secondary composition mechanism, which was
the only mechanism available in an earlier iteration of this calculus (see Appendix D): composite
terms at any level are all subject to the same transition rules. This behaviour is more reminiscent
of functional programming languages, but is somewhat clumsy in practice; nevertheless it is not
without utility in ℵ—in particular, it is well suited for when a continuation-passing style approach
is favoured. The sub-rule mechanism, on the other hand, is more reminiscent of declarative
programming languages.

When there is no matched rule, such as in Listing 1c, this simply means that there is no
successor state and so computation cannot continue. It is in fact very similar in nature to the case
when computation succeeds, as then we obtain a term which has a predecessor state via some
rule, but lacks a rule to generate a successor state. To properly distinguish these two conditions,
we must explicitly mark ‘true’ halting states; for addition, this is written as

! +

();

! ()

+;

where ! (‘bang’) indicates that any term of the speciﬁed forms is a valid computational output.
The former corresponds to the output of a subtraction, and the latter to the output of an addition.
This subtlety is further contextualised by Figure 3.

We conclude this ﬁrst example by alluding to a possible molecular implementation, as depicted
in Figure 4. Here we see the importance of ℵ being interpretable as a term-rewriting system, as
the entirety of the state of the computation must be encoded within a single macromolecule (or
molecular complex). Strictly speaking, this is not a requirement as DNA Strand Displacement
systems [22] achieve computation without this requirement; in payment for this, though, the entire
reaction volume is dedicated to the same (typically analogue) program. The precise mechanism
of the reactions is omitted from the ﬁgure, instead expressing the model as an abstract chemical
reaction network. Finding possible reaction mechanisms to implement ℵ, or similar calculi, in
real chemical systems will be the subject of future work. Whilst the abstract molecular formalism
is attractive form an explanatory perspective, ℵ provides a more concise formalism that is also
easier to manipulate and to explicate its semantics.

Squaring Eliminating redundancy in the deﬁnition of addition yielded its inverse for free, but
one may still object that the additional output is ‘garbage’ and not of any utility. What will

8

Figure 3: An overview of the different classes of (deterministic) computations, both irreversible

and reversible.

(a) In an irreversible computation, each state (represented by a node) may have multiple predecessors.
For example, if two Boolean variables are consumed and replaced with their logical conjunction, and
if this is FALSE, then there are three possible predecessors: (FALSE, FALSE), (FALSE, TRUE), and
(TRUE, FALSE). This is loss of information, and it is impossible to determine which path was taken to
reach the current state.

(b) In contrast, a reversible computation can never lose information, and so it is always possible to retrace
one’s steps. As a result, every computational state has at most one successor and one predecessor.
Denoting a halting state by (cid:96), there are four kinds of reversible computation: (I) loops without any
halting state, (II) bi-inﬁnite computations without any halting state, (III) semi-inﬁnite computations
which have one halting state (i.e. the computation has an initial state but no ﬁnal state), and (IV) ﬁnite
computations bounded by two halting states. The fourth class corresponds to programs of interest,
as they have a well deﬁned initial and ﬁnal state (and can also be meaningfully reversed); the third
corresponds to classical non-halting programs in irreversible models of computation, as does the second
as the concept of an initial state is less well-deﬁned in irreversible computation.

9

happen if one uses this addition subroutine many times in a larger program? Naively we may
expect this garbage to accumulate, requiring either active dissipation of the additional entropy or
the application of Bennett’s algorithm to clean it up. In fact, by retraining one’s thought process
from the irreversible programming paradigm to the reversible paradigm, it is often possible to
make use of this garbage data. We demonstrate this with the example of ﬁnding the square of a
natural number. The candidate function, (cid:3) : n ↔ n2, is an injection and so clearly meets our
requirement of partial bijection. Therefore, we have good reason to believe that it is possible to
implement it. The obvious approach of using multiplication will not work because its reversible
embedding will take a form not dissimilar to × : (m, n) ↔ (m, m · n), and so would yield
(cid:3) : n ↔ (n, n2). Whilst this sufﬁces for realising the square of a number, it retains too much
redundancy in its outputs.

Often a helpful tactic is to consider an inductive approach. For the square numbers this
is encapsulated by (n + 1)2 − n2 = 2n + 1, from which can be obtained the identity n2 ≡
(cid:80)n−1
k=0 2k + 1. Again, we need to be clever: in order to achieve our desired (partial) bijection,
we need to completely consume our input value of n. This can be done by evaluating the sum in
reverse. Instantiate a new variable, m = 0; as m is set to a known value, this is reversible. Then,
perform the following loop until n reaches 0: decrement n, add 2n + 1 to m, repeat. At the end
of this loop, n will have reached a unique value (and can thus be reversibly destroyed) and m
will have been set to the square of the original value of n. In addition, this can be implemented
with our addition subroutine in two steps, by adding n twice to m (retaining the value of n) and
ﬁnally incrementing m. This is implemented in ℵ in Listing 2, and the example of squaring 3
(equivalently, taking the square root of 9) is presented in Listing 3.

3.

by Example 2: Parallelism & Concurrency

Having introduced the essence of ℵ in the previous section, we now dive deeper into some more
advanced features and examples of ℵ.

Sugar To reduce boilerplate and increase clarity in longer programs, it is helpful to introduce
some syntactic sugar (shorthands). More sugar will be introduced later for the deﬁnition of
the programming language alethe (which is really just sugared ℵ), but for now only a light
sprinkling is required.

Many rules take the rote form

! f x1 x2 · · · xm ();
f x1 x2 · · · xm () = () y1 y2 · · · yn f :

! () y1 y2 · · · yn f ;

· · ·

which we can abbreviate with an inﬁx form as

x1 x2 · · · xm
· · ·

(cid:56)f (cid:56) y1 y2 · · · yn:

10

Figure 4: An abstract molecular translation of the reversible addition deﬁnition given in Listing 1a,
as well as the example addition of 4 and 3 following Listing 1b. The abstract molecular
model espoused here is deﬁned by a ﬁxed set of atoms (e.g. {+, S, Z, •}) connected by
two kinds of bond. Atoms joined by single-headed bonds are analogous to ℵ terms,
whilst double-headed bonds corresponds to nesting of composite terms. The bonds
are represented by arrows because there is an intrinsic polarity/directionality to the
molecules; this is not necessary, and can be replaced by auxiliary atoms such as L and
R, but it does simplify our representation. Atoms are rendered by circles, whilst the
reaction deﬁnitions also use variables written as un-circled letters. The • ‘atom’ is
special in that it is a placeholder for a nested composite term. Reaction arrows for
‘elementary’ reactions are labelled by rule names, and starred reaction arrows indicate
an effective reaction composed of multiple elementary steps. The arrows are drawn
biased in anticipation of a biasing mechanism to be discussed later on within this
section.

11

where the halting patterns are implied. In the special case of f a single symbol, such as +, × or
(cid:3), we omit the backticks and write, e.g., 4 3 + 4 7. Note that, if f is a composite term, then we
additionally assert ! f .

We have already seen sugar for numeric data, e.g. 4 ≡ S(S(S(SZ))). Another common
data type is that of lists. As is standard in the functional world, we opt for singly linked lists
implemented as composite pairs. If CONS x y represents the pair (x, y) and NIL the empty list,
then the list [5 2 4 3] has corresponding representation CONS 5 (CONS 2 (CONS 4 (CONS 3 NIL))).

We also introduce sugar for matching on a partial preﬁx of a list, i.e. [x y · z/s ] corresponds to
CONS x (CONS y z/s ).

Parallelism With this sugar thus deﬁned, we can rewrite the somewhat clumsy deﬁnition of
recursively mapping a function f over a list,

! MAP ;

! (MAP )

();

! ()

(MAP );

(MAP f ) NIL () = () NIL (MAP f );

(MAP f ) (CONS x x/s ) () = () (CONS y y/s ) (MAP f ):

f x () = () y f.

(MAP f ) x/s () = () y/s (MAP f ).

more concisely and clearly as

[] (cid:56)
MAP f (cid:56) [];
[x · x/s ] (cid:56)
x (cid:56)f (cid:56) y.
x/s (cid:56)

MAP f (cid:56) [y · y/s ]:
MAP f (cid:56) y/s .

Notice that the order in which the sub-rules are executed does not make a difference to the
ﬁnal result, due to referential transparency; in fact ℵ, being declarative, does not ascribe any
importance to the ordering of the statements. Moreover, should a rule not be necessary for the
ﬁnal computation (or if there are multiple routes to the answer) then that rule will not necessarily
be executed. A rule may even be evaluated more than once; for example, Bennett’s algorithm
may be implemented as

x (cid:56)

BENNETT f (cid:56) x y:

x (cid:56)f (cid:56) garbage y.

wherein the function f will be evaluated once in the forward direction, its output y will be copied,
and then f will be evaluated again in the reverse direction to consume the garbage. This arbitrarity
in rule ordering and execution is important to its ability to operate in a stochastic system such as
a molecular context, although in practice a compilation pass that chooses and enforces an optimal
execution plan is important for efﬁciency.

12

! (cid:3) ();
! () (cid:3);
(cid:3) n () = (cid:3) n Z (cid:3);
(cid:3) Z m (cid:3) = () m (cid:3);
(cid:3) (Sn) m (cid:3) = (cid:3) n (Sm(cid:48)(cid:48)) (cid:3):
+ n m () = () n m(cid:48) +.
+ n m(cid:48) () = () n m(cid:48)(cid:48) +.

(SQUARE–INIT)

(SQUARE–TERM)

(SQUARE–STEP)

(SQUARE–STEP–SUB1)

(SQUARE–STEP–SUB2)

(a) The deﬁnition of (cid:3) in ℵ.

(b) The deﬁnition of (cid:3) in the abstract molecular formalism introduced in Figure 4. The dashed lines
indicate that there exists a term matching both patterns, and this is characteristic of conditionals and
looping in ℵ. Note that computation remains unambiguous and deterministic: by Figure 3, each
intermediate state has both a predecessor and a successor, and of the two matching patterns one will
correspond to the reverse direction and one to the forward direction. For example, in the middle of
a loop one may encounter the intermediate species (cid:3) 2 5 (cid:3) and this term matches both the patterns
(cid:3) (Sn) m (cid:3) and (cid:3) n (Sm(cid:48)(cid:48)) (cid:3), but if it matches the former the computation will proceed forward
whilst if the latter the computation will proceed backward. It will be shown later how the semantics
keep track of this, and how we ensure the program really is unambiguous.

Listing 2: A reversible deﬁnition of squaring of natural numbers, (cid:3) : n ↔ n2, both in ℵ and in

an abstract molecular formalism.

13

The implicit duplication of variables that may occur means that one possible interpretation of

MAP is

[] (cid:56)
MAP f (cid:56) [];
[x · x/s ] (cid:56)

(cid:56)

MAP f (cid:56) [y · y/s ]:
DUP f (cid:56) f (cid:48).
x (cid:56)f (cid:56) y.
MAP f (cid:48)(cid:56) y/s .
x/s (cid:56)

from which we can see that not only is the order of the sub-rules arbitrary, but that they can be
evaluated in parallel as the following example makes clear:

(MAP (cid:3)) [3 5 8] () (cid:33) {f : (cid:3), x : 3, f (cid:48) : (cid:3), x/s : [5 8]}

!

(cid:3) 3 () = () 9 (cid:3)

(MAP (cid:3)) [5 8] () = () [25 64] (MAP (cid:3))

{f : (cid:3), y : 9, f (cid:48) : (cid:3), y/s : [25 64]} (cid:33) () [9 25 64] (MAP (cid:3))

!

There remains a subtle point to be made: variables can be implicitly duplicated if they are
used by multiple rules, but there is then a contract made with these rules that they really do
return the variable unchanged. It is not possible to ensure this statically, however, and so it is
entirely possible that the copies of some variable may diverge. In this case, running DUP in
reverse will fail, and hence computation will stall. If duplication is not used, then computation
will occur linearly and the changed value may be fed into subsequent rules unnoticed. In this
case, computation may run to completion but yield an incorrect result due to the logic error.

Sorting A larger example program, which is capable of sorting a list using an arbitrary com-
parison function, is presented in Listing 4. Whilst of poor algorithmic complexity, insertion sort
is employed for simplicity. A more efﬁcient implementation using merge sort is available in the
accompanying standard library2. Clearly sorting is an irreversible process, as its purpose is to
discard information regarding the original ordering of the list. The presented sorting algorithm
puts a little effort towards increasing the utility of the garbage, in that the garbage data is a list
saying where the original list item was placed into the sorted list at the moment of insertion. The
insertion sort included in the standard library (Listing 12) applies some additional processing to
make this garbage data correspond to the permutation that maps the original list to the sorted list.

Concurrency In addition to automatic parallelisation of independent sub-rules, ℵ also supports
deﬁning transitions between separate terms. The motivation for this is for ℵ to be able to fully
exploit molecular architectures, but its utility is general and it is intended that a future version of
the alethe interpreter will support concurrency.

2The alethe standard library is available from https://github.com/hannah-earley/alethe-examples.

14

! (cid:3) 3 () (cid:33) {n : 3} (cid:33) (cid:3) 3 Z (cid:3) · · ·

(SQUARE–INIT)

· · · (cid:3) 3 Z (cid:3) (cid:33) {n : 2, m : Z}

+ 2 Z () ↔ () 2 2 +

{n : 2, m(cid:48) : 2}

+ 2 2 () ↔ () 2 4 +

(SQUARE–STEP)

(SQUARE–STEP–SUB1)

checkpoint

(SQUARE–STEP–SUB2)

{n : 2, m(cid:48)(cid:48) : 4} (cid:33) (cid:3) 2 5 (cid:3) · · ·

(SQUARE–STEP)

· · · (cid:3) 2 5 (cid:3) (cid:33) {n : 1, m : 5}

+ 1 5 () ↔ () 1 6 +

{n : 1, m(cid:48) : 6}

+ 1 6 () ↔ () 1 7 +

(SQUARE–STEP)

(SQUARE–STEP–SUB1)

checkpoint

(SQUARE–STEP–SUB2)

{n : 1, m(cid:48)(cid:48) : 7} (cid:33) (cid:3) 1 8 (cid:3) · · ·

(SQUARE–STEP)

· · · (cid:3) 1 8 (cid:3) (cid:33) {n : Z, m : 8}

+ Z 8 () ↔ () Z 8 +

{n : Z, m(cid:48) : 8}

+ Z 8 () ↔ () Z 8 +

(SQUARE–STEP)

(SQUARE–STEP–SUB1)

checkpoint

(SQUARE–STEP–SUB2)

{n : Z, m(cid:48)(cid:48) : 8} (cid:33) (cid:3) Z 9 (cid:3) · · ·

(SQUARE–STEP)

· · · (cid:3) Z 9 (cid:3) (cid:33) {m : 9} (cid:33) () 9 (cid:3) !

(SQUARE–TERM)

! (cid:3) 3 () ↔ (cid:3) 3 Z (cid:3) ↔ (cid:3) 2 5 (cid:3) ↔ (cid:3) 1 8 (cid:3) ↔ (cid:3) Z 9 (cid:3) ↔ () 9 (cid:3) !

Listing 3: An example application of the (cid:3) deﬁnition from Listing 2.

15

Biasing Computation In Figure 4 and Listing 2b, abstract molecular implementations
of addition and squaring were introduced respectively. Notably, the reaction arrows—whilst
reversible—were biased in the forward direction. From a thermodynamic perspective, the
direction in which a reaction occurs cannot be speciﬁed in an absolute sense, and depends on
the conditions of the reaction volume at a given point in time. Moreover, at equilibrium the net
direction of each reaction is null: they make no net progress. These concerns are discussed in
more detail in Earley [12], but sufﬁce it to say that trying to arrange the conditions of the system
such that the computational terms themselves are inherently biased is impracticable, and will also
limit the number of transition steps that can be executed. Moreover, it makes it difﬁcult to run
sub-rules in a reverse direction (such as is needed for Bennett’s algorithm).

The solution is to separate the concerns of computation and biasing said computation. In
biochemical systems, this is (mostly) achieved by using a common free energy carrier in the
form of ATP and ADP + Pi, which are held in disequilibrium such that the favourable reaction
direction is ATP + H2O (cid:41)−−−(cid:42) ADP + Pi. Other biochemical reactions are then coupled to one or
more copies of this hydrolysis reaction. For our purposes, we generalise this to assume that free
energy is available in the form of two terms ⊕ and (cid:9), where the concentration of ⊕ in the reaction
volume is greater than that of (cid:9). The ‘computational bias’ quantifying the average net proportion
of computational transitions that are successful is found to be b = ([⊕] − [(cid:9)])/([⊕] + [(cid:9)]).

With a bias system thus deﬁned, we can couple any computational transition to it by simply
having the input side consume a ⊕ term and the output side release a (cid:9) term. The more transitions
that couple to the bias source, the faster and more robustly the computation proceeds. For example,
the square deﬁnition can be amended like so:

! (cid:3) ();
(cid:26)⊕

(cid:27)

(cid:3) n ()

! () (cid:3);
(cid:26)(cid:9)

=

(cid:26)⊕

(cid:27)

(cid:3) Z m (cid:3)

=

(cid:26)⊕

(cid:27)

(cid:3) (Sn) m (cid:3)

=

(cid:27)

;

(cid:27)

(cid:3) n Z (cid:3)
(cid:26)(cid:9)

() m (cid:3)
(cid:26)(cid:9)

(SQUARE–INIT)

;

(SQUARE–TERM)

(cid:27)

:

(SQUARE–STEP)

(cid:3) n (Sm(cid:48)(cid:48)) (cid:3)

+ n m () = () n m(cid:48) +.
+ n m(cid:48) () = () n m(cid:48)(cid:48) +.

(SQUARE–STEP–SUB1)

(SQUARE–STEP–SUB2)

The braces indicate that these deﬁnitions are concurrent, in that the transition will draw (release)
two terms from (into) the reaction volume. As written above, the scheme is fairly basic in that
it only drives computation in one direction. An improved scheme may make use of a hidden
variable in each term to indicate its preferred direction of computation. If this direction is reversed,
then the transition will instead draw a (cid:9) term and release a ⊕ term. If a sub-rule then needs to be
run in the opposite direction to its parent, then the hidden variable need simply be inverted. This
scheme can be made even more sophisticated by allowing the hidden variable to refer to alternate
bias sources in case there are multiple to which the transition could couple (as is recommended
by Earley [13, 14]).

16

FALSE
(cid:56)

TRUE

(cid:56)

NOT
(cid:56)

NOT

(cid:56)

TRUE;

FALSE;

(cid:56)

FALSE;

(cid:56)< m Z
(cid:56)< Z (S n)(cid:56)
TRUE;
(cid:56)< (S m) (S n)(cid:56) b:
(cid:56)< m n(cid:56) b.

(cid:56)≤ m n(cid:56) b(cid:48):
(cid:56)> m n(cid:56) b:
(cid:56)≥ m n(cid:56) b(cid:48):

(cid:56)< n m(cid:56) b.
(cid:56)< n m(cid:56) b.
(cid:56)< m n(cid:56) b.

b (cid:56)

NOT

(cid:56) b(cid:48).

b (cid:56)

NOT

(cid:56) b(cid:48).

!

(cid:56)

INSERTIONSORT p(cid:56)

;

(INSERTIONSORT p) x/s () = IS p x/s [] [] IS;
IS p [x · x/s ] n/s y/s IS = IS p x/s [n · n/s ] z/s IS:
IS p [] n/s y/s IS = () n/s y/s (INSERTIONSORT p);

INSERT p(cid:56) n z/s .

x y/s (cid:56)

INSERT p(cid:56)

x [] (cid:56)
x [y · y/s ] (cid:56)

INSERT p(cid:56) n [z z(cid:48) · z/s ]:

Z [x];

(cid:56)p x y(cid:56) b.
x [y · y/s ] b (cid:56)
x [y · y/s ] TRUE
x [y · y/s ] FALSE
x y/s (cid:56)

INSERT’ p(cid:56) n [z z(cid:48) · z/s ].
Z [x y · y/s ];
INSERT’ p(cid:56)
INSERT’ p(cid:56) (S n) [y · z/s ]:
(cid:56)
INSERT p(cid:56) n z/s .

(cid:56)

[3 2 0 7 6 4 5 1] (cid:56)
[3 2 0 7 6 4 5 1] (cid:56)

INSERTIONSORT <(cid:56) [1 4 3 3 3 0 0 0] [0 1 2 3 4 5 6 7].
INSERTIONSORT ≥(cid:56)
[6 2 2 1 0 2 1 0] [7 6 5 4 3 2 1 0].
(cid:125)
(cid:124)

(cid:123)(cid:122)
garbage

Listing 4: An ℵ implementation of the comparison operators <, ≤, > and ≥, and of insertion
sort that can make use of these comparison operators. Lastly, the list [3 2 0 7 6 4 5 1]
is sorted into both ascending and descending order.

17

Communication A more canonical application of concurrency is that of communication
between different computers. There are many possible communication schemes, and an overview
and analysis of schemes appropriate for Brownian computers is presented by Earley [13], but we
shall consider the simple example of an open communication channel between two computers,
ALICE and BOB. ALICE has a list, the contents of which she wishes to send to BOB. A naive
approach to this may resemble the ℵ deﬁnition

ALICE [x · x/s ] =

(cid:40)

ALICE x/s

COURIER x

(cid:41)

;

(cid:40)

BOB y/s

COURIER y

(cid:41)

= BOB [y · y/s ];

where the COURIER terms are used to convey list items between the two computers. Unfortunately,
this does not quite realise the desired behaviour in a Brownian context: Suppose ALICE starts
with the list [1 2 3 4 5]. She will generate the courier terms (COURIER 1), (COURIER 2), . . . ,
(COURIER 5), as expected, but these terms will be delievered to BOB via a diffusive approach.
Except for the case of a one-dimensional system, BOB will almost certainly receive these couriers
in a random order, completely uncorrelated from the original order. This may be avoided if
delivery over the channel is substantially faster than the rates of term ﬁssion/fusion reactions, but
this is not likely in a chemical system. Moreover, this being a reversible Brownian system, ALICE
will from time to time re-absorb a courier that she previously dispatched, thereby increasing the
opportunity for the list order to be shufﬂed.

The take-home message, here, is that whilst the local dynamics of a concurrent reversible
computation system—such as ℵ—may well be reversible, the global dynamics are not necessarily
reversible. In fact, this is a common property of microscopically reversibly systems, and is the
basis of thermodynamics: the manifestation of macroscopically irreversibly dynamics from mi-
croscopically reversible physics. It is doubtful that a model of concurrent reversible programming
could preclude the possibility of such increases in entropy without severely restricting the system:
likely any attempt to do so would effectively prevent the use of concurrency. Nevertheless, the
programmer has a high degree of control here and so can, in principle, avoid entropy generation
except where desired. In the above case of sending an ordered list, one could for example
explicitly sequence the couriers. This suggests that a system of reversible molecular computers
could have exceptionally low entropy generation and could realise very strange behaviours by
getting as close to the implementation of a Maxwell Dæmon as physics allows. Conversely, one
is also free to exploit the thermodynamic properties of the system; for example, one could realise
(to the extent the laws of physics permit) a true random number generator.

Resources Our last example of concurrency concerns the distribution of conserved re-
sources amongst computers. If our computers are to respect the reversibility of the laws of
physics, they should also respect mass conservation and so will need to contend with their
ﬁnity. Amorphous computing presents one approach to achieving powerful computation from
limited computational subunits, but it does so by making extensive use of communication which
precludes the ability to perform meaningfully reversible computation. We instead suppose that
the computers can exchange resources, such as memory units and structural building blocks, with
the environment. Whilst this also has thermodynamic consequences [14], it at least separates the

18

concerns of computation from resource access and thus preserves the ability to perform reversible
computation.

A host of resource distribution schemes, and thermodynamic analyses thereof, are presented
by Earley [14], but we content ourselves here with demonstrating how computers may interact
with resources free in solution. In particular, we amend the deﬁnition of addition to be mass-
conserving:

+ Z b () = () Z b +;
(cid:40)

(cid:41)

S

+ (Sa) b ()

= () (Sa) (Sb(cid:48)) +:

(ADD–BASE)

(ADD–STEP)

+ a b () = () a b(cid:48) +.

(ADD–STEP–SUB)

Effects and Contexts As brieﬂy mentioned, composite terms are subject to the same transition
rules. Suppose LENA is learning ℵ and experiments with this, creating the following trivial wrapper
around (cid:3):

(cid:56)

(cid:56)

;

MYSQUARE

!
MYSQUARE n () = MYSQUARE ((cid:3) n ()) MYSQUARE;
MYSQUARE (() m (cid:3)) MYSQUARE = MYSQUARE m ();

This deﬁnition, if a little contrived, will function as intended. Suppose, now, that she wants to
inspect what happens within the loop of (cid:3), and so writes the following:

(cid:56)

(cid:56)

;

MYSQUARE’

!
MYSQUARE’ n m () = MYSQUARE’ ((cid:3) n m (cid:3)) MYSQUARE’;
MYSQUARE’ ((cid:3) n(cid:48) m(cid:48) (cid:3)) MYSQUARE’ = MYSQUARE’ n(cid:48) m(cid:48) ();

Now, what happens if we instantiate the term MYSQUARE’ 3 Z ()? Well, the ﬁnal term could be
any of () 3 Z MYSQUARE’, () 2 5 MYSQUARE’, () 1 8 MYSQUARE’ or () Z 9 MYSQUARE’. Whilst
this arguably achieves LENA’s aim of inspecting the execution of (cid:3), there is a problem in that ℵ
claims all of these terms are halting states yet Figure 3 asserts there should be a maximum of two
halting states. Despite being contrived, this example shows that we need to introduce another
constraint into ℵ to prevent unexpected entropy generation: Any composite term or sub-term can
only be created in a halting state, and can only be consumed in a halting state. Moreover, the
creation and consumption patterns must be unambiguous as to which of the term’s two halting
states they refer (if, indeed, there are two). Otherwise, each instance of this ambiguity would
multiply the state space and thus there would be an exponential increase in the size of the state
space over time.

With the constraint on composite terms clariﬁed, one can then ask whether all terms must
follow the same set of rules. It turns out there is an important case where distinguishing between
composite terms and top-level terms is useful; speciﬁcally, some cases of effectful computation.
Suppose that the reaction volume has been endowed with a spatial lattice along which terms can

19

travel, and imagine a term (CONS CHARLIE DAN) with such a wanderlust. If the composite terms
CHARLIE and DAN want to travel to different locations, then where will the term end up? It is
likely that a tug-of-war will occur, and so it makes sense to restrict the effecting of translocation
to top-level terms.

This distinction is achieved by introducing term contexts: a top-level term has a top-level
context, which is some label (itself a term) that may contain information, whilst composite terms
have ‘one-hole contexts’. For example, a term attached to a lattice may have context LATTICE
while a term free in solution might have context FREE. Concretely, the aforementioned tug-of-war
may be resolved by giving CHARLIE control by making him the top-level term, rendered as
LATTICE : CHARLIE (DAN). Meanwhile, DAN is rendered as (LATTICE : (CHARLIE •)) : DAN
where • indicates that this is a one-hole context. One-hole contexts can be deﬁned for many data
structures, but for a tree they correspond to removing some sub-tree of interest, replacing it with a
‘hole’; see McBride [23] for some interesting properties of one-hole contexts. Rules may pattern
match on top-level contexts and thus consume the information held within, or even create and
destroy top-level terms, but they cannot match on one-hole contexts as this would risk altering
their structure; instead, one-hole contexts can only be matched by ‘opaque variables’. An opaque
variable is a special variable found only in context patterns, which can match against a top-level
context or a one-hole context, whilst regular variables in a context pattern can only match against
top-level contexts. That is, the following are allowed

(cid:8)LATTICE : CHARLIE x(cid:9) =

(cid:8)γ : DAN [DAN’S STUFF](cid:9) =

(cid:40)

LATTICE : CHARLIE ()

(cid:41)

FREE : x

(cid:40)

γ : DAN

;

(cid:41)

;

LATTICE : [DAN’S STUFF]

whilst this is not

(cid:8)(LATTICE : (c •)) : DAN

(cid:9) = (cid:8)(LATTICE : (• DAN)) : c(cid:9) ;

This is not too onerous a restriction, as if one wishes to manipulate the structure of the one-hole
context one can simply match against a higher level context.

In the earlier ℵ deﬁnitions, contexts were missing from the rule patterns. This can be seen as
another example of syntactic sugar, with a missing context implying an opaque variable context
(i.e. γ :) such that the rule can match against any term at any level. For concurrent deﬁnitions,
however, all participating terms must be contextualised as otherwise it is unclear how to assign
the results to the bound one-hole contexts.

It is as yet unclear how translocation along a lattice, or other effects, is actually achieved.
Typically effects will be introduced as additional computational primitives, as by deﬁnition
their actions are not ‘computational’. As these computational primitives must be instantiated as
top-level terms, we require a way to interface between computational terms and effector terms
and this warrants a continuation-passing-style approach. For example, walking along a lattice

20

from coordinate α to coordinate β may be realised thus:

(cid:48)

(cid:48)(cid:48)
! CHARLIE
(cid:8)LATT : CHARLIE β x(cid:9) = (cid:8)LATT : WALK β (CHARLIE

! CHARLIE

;

;

(cid:48) x)(cid:9) ;

(primitive) (cid:8)LATT : WALK β c(cid:9) ↔ (cid:8)LATT : WALK

(cid:8)LATT : WALK

(cid:48) α (CHARLIE

(cid:48) α c(cid:9) ;
(cid:48)(cid:48) x)(cid:9) = (cid:8)LATT : CHARLIE

(cid:48)(cid:48)(cid:48) α x(cid:9) ;

That is, WALK is given a coordinate to travel to as well as a continuation (which is free to
perform additional computation during the walk, if desired). WALK then replaces the destination
coordinate with the origin coordinate to ensure reversibility. Finally, we deﬁne a transition from
(cid:48) in order to return control to the continuation. This reveals a subtle point, that the effectful
WALK
primitives are intentionally not marked as halting so that control can be transferred to and fro’
them.

4. The Calculus

The ℵ calculus thus introduced has a very simple deﬁnition. In BNF notation, it is

(PATTERN TERM)

τ ::= ATOM | VAR | ( τ ∗ )

(PARTY) π ::= τ : τ ∗ | VAR

(cid:48) : τ ∗

(DEFINITION)

δ ::= {π∗} = {π∗} : π.∗ | ! τ

where ATOM is an inﬁnite set of atomic symbols (conventionally starting with an uppercase letter
or a symbol), VAR an inﬁnite set of variables (conventionally starting with a lowercase letter),
(cid:48) is an orthogonal inﬁnite set of variables (conventionally rendered in Greek) used for
and VAR
opaquely matching one-hole-contexts. A program is a series of deﬁnitions, δ∗, and a physical
term is simply a pattern term without variables. Notice that the form of the sub-rules differs
from the examples: a sub-rule can be separated into the instantiation of a term according to an
input pattern, the evolution of that term, followed ﬁnally by the consumption of its ﬁnal halting
state according to the output pattern. In this view, these sub-terms are identiﬁed by one-hole-
contexts—speciﬁcally, opaque variables. This formulation not only allows the semantics to
represent automatic parallelisation and a non-deterministic sub-rule ordering, but also begets an
additional feature whereby sub-rules can instantiate top-level terms. The sub-rule forms in the
(cid:48) is
examples can then be seen as sugar, i.e. s = t. is equivalent to λ : s. λ : t. where λ ∈ VAR
a fresh opaque variable. To illustrate, recall the deﬁnition of natural addition from Listing 1a;
desugared, this takes the form

1. ! + a b ();

2. ! () a b +;

3. {α : + Z b ()} = {α : () Z b +};
4. {α : + (Sa) b ()} = {α : () (Sa) (Sb(cid:48)) +}:

(ADD–BASE)

(ADD–STEP)

4a.

4b.

β : + a b ().
β : () a b(cid:48) +.

(ADD–STEP–SUB)

21

That is, there are four deﬁnitions: two ‘halting’, and two ‘computational’. On the left side of
deﬁnition 4, we have a bag of one party. This party has as context pattern the opaque variable
α, and its pattern term is a composite pattern term of length 4, consisting of the atom +, the
composite pattern term of length 2 consisting of the atom S and the variable a, the variable b, and
the empty composite pattern term (‘unit’). There is also an opaque variable β, conﬁned to the
inner scope of deﬁnition 4. During execution of the sub-rule in the forward direction, the variables
a and b will ﬁrst be consumed in order to generate the sub-term + a b (), which will be bound to
the opaque variable β. This sub-term will then evolve to its other halting state, whereupon it will
match sub-rule 4b and thus β will be consumed and the variables a and b(cid:48) obtained.

Semantics To formalise the semantics embodied in the preceding examples, we deﬁne a
transition relation ↔ that maps a bag of terms to another bag of terms according to the rules
deﬁned for the current program. By a bag of terms, we aim to evoke the notion of a concoction of
computational molecules in solution; a bag is a multiset, which can contain multiple copies of
elements, but unlike a physical solution it lacks a concept of space. If desired, spatial locations
can be readily simulated by appropriately chosen top-level contexts. Recalling that the global
dynamics of the system are irreversible, we should expect that ↔ is a non-deterministic relation.
That is, application of the relation to a set of possible bags of terms may increase the size of this
set, and the logarithm of the set size is identiﬁed with the entropy of the system. Concretely, the
set of bags is the macrostate of the system, and each bag within the set is a microstate.

The calculus being reversible, the relation should share properties with equivalence relations.

Namely, if S, T, U are bags of terms, then we have

S ↔ S (REFL)

S ↔ T =⇒ T ↔ S (SYMM)

S ↔ T ∧ T ↔ U =⇒ S ↔ U (TRANS)

but we also make clear that these transitions can occur within an environment of other non-
participating terms, Γ,

S ↔ T =⇒ ∀Γ. Γ ∪ S ↔ Γ ∪ T

(EXT)
s ↔ t =⇒ ∀Γ. Γ ∪ {s} ↔ Γ ∪ {t} (EXT(cid:48))

where the second rule, in which s and t are single terms rather than bags of terms, is introduced
as a convenient abuse of notation for later deﬁnitions.

In order to physically effect rule transitions, we introduce ‘mediator terms’ delimited by angle
brackets, which interact with computational terms and can represent each of the intermediate
states. If the program is given by P, then the mechanism by which these mediator terms come
into and out of existence is as follows,

P (cid:96) (I = O : R) =⇒ {} ↔ {(cid:104)I∅∅R∅O(cid:105)} (INST–COMP)
P (cid:96) (! τ )

=⇒ {} ↔ {(cid:104)τ (cid:105)}

(INST–HALT)

22

from which we see that the environment can contain an arbitrary number of copies of each3. The
mediator terms for computational rules are sextuples (cid:104)II (cid:48)BRΓO(cid:105) consisting of, respectively, the
bag of unmatched input patterns I, the bag of matched input patterns I (cid:48), the bag of resultant
bindings B, the bag of sub-rules R, the local environment for internal sub-rules Γ, and the bag of
output patterns O. The mediator terms for halting rules are trivial singletons (cid:104)τ (cid:105) containing the
relevant pattern, τ .

To assist rules in binding composite terms, we permit the current focus of a term to vary over

time,

c : ((cid:126)l t (cid:126)r) ↔ (c : ((cid:126)l • (cid:126)r)) : t
c : [(cid:126)l t (cid:126)r] ↔ (c : [(cid:126)l • (cid:126)r]) : t

(OHC1)

(OHC2)

where (cid:126)l and (cid:126)r are (possibly empty) strings of terms and t is the term focus. The bracketed terms
in the second rule will be explained shortly. Note that these one-hole-context rules may not be
needed for all architectures, being implicitly true in a molecular architecture for example.

The action of halting mediators is simply to mark terms which are in a known halting state,

∃b(cid:48). t τ∼ b(cid:48) =⇒ {x : t, (cid:104)τ (cid:105)} ↔ {x : [t], (cid:104)τ (cid:105)} (TERM)

with [t] serving as an indicator of a halting state and where t τ∼ b(cid:48) means that t uniﬁes against τ
with bindings b(cid:48) (see Listing 6).

Computational mediators are somewhat more complicated. We ﬁrst render their temporal

symmetry manifest by two reversibility rules,

(cid:104)I∅∅R∅O(cid:105) ↔ (cid:104)O∅∅R∅I(cid:105)
(cid:104)∅IBRΓO(cid:105) ↔ (cid:104)∅OBRΓI(cid:105)

(REV1)

(REV2)

where these rules will be seen to be necessary even for computation in a single direction. Applying
a computational rule consists ﬁrst of binding against a matching term for each input pattern,
followed by substituting the bindings into sub-terms per the sub-rules. The sub-terms may either
be top-level or local. These transitions may all occur in parallel, i.e. a sub-term may be instantiated
if all of its requisite variables are bound, even if not all the input patterns have matched a term.

x : t π∼ b =⇒
{x : t, (cid:104)(I ∪ {π})I (cid:48)BRΓO(cid:105) ↔ {(cid:104)I(I (cid:48) ∪ {π})(B ∪ b)RΓO(cid:105)}

(INP)

π ∈ R ∧ x : t π∼ b =⇒

(SUB1)
(cid:104)II (cid:48)(B ∪ b)RΓO(cid:105) ↔ (cid:104)II (cid:48)BR(Γ ∪ {x : [t]})O(cid:105)
(cid:48) =⇒

x /∈ VAR
{(cid:104)II (cid:48)BR(Γ ∪ {x : [t]})O(cid:105)} ↔ {x : [t], (cid:104)II (cid:48)BSΓO(cid:105)}

(SUB2)

3This complicates the aforementioned entropic interpretation of sets of term-bags, as the entropy will tend to diverge
as the number of mediator terms tends to inﬁnity. A more realistic implementation would leave the mean number of
extant mediator terms ﬁnite and bounded, and perhaps even ﬁx the number. This would have a further consequence
on the kinetics and thermodynamics of the system, with the well-characterised Michælis-Menten kinetics a good
starting point to the analysis thereof.

23

where x is a context. The (SUB2) transition enables top-level terms to escape the locally scoped
environment. Completion of computation occurs by application of (REV2) followed by the (INP)
and (SUB1,2) in reverse; clearly if there is no route from the set of input variables to the set of
output variables then the computation will stall. It may be desirable to augment the transition
rules with implicit variable duplication,

(cid:104)II (cid:48)(B ∪ {b})RΓO(cid:105) ↔ (cid:104)II (cid:48)(B ∪ {b, b})RΓO(cid:105)

(DUP)

otherwise rules which wish to increase their parallelisability should explicitly duplicate variables
as needed. We shall also need to enable the sub-environment to evolve,

Γ ↔ Γ(cid:48) =⇒ (cid:104)II (cid:48)BRΓO(cid:105) ↔ (cid:104)II (cid:48)BRΓ(cid:48)O(cid:105)

(SUB3)

These semantics, encapsulated by the ↔ transition rule, are summarised in Listing 5. An
example of their application is provided in Listing 7. It remains to describe the operation of
binding/uniﬁcation. This operation is deﬁned as one would expect: a pattern matches if it is a
variable, if it is an atom and the term is the same atom, or if it is a composite pattern and the term
is a halting composite term of the same length and if the pattern and term match pair-wise. This
is summarised in Listing 6.

Reversibility The (SYMM) transition renders the semantics trivially reversible, but this is fairly
weak. We conclude the discussion of the semantics of ℵ by proving that the semantics are
microscopically reversible.

Deﬁnition 4.1 (Microscopic Reversibility). A transition is primitively microscopically reversible
if it is a uniquely invertible structural rearrangement. A transition is microscopically reversible if
it is decomposable into a series of primitively microscopically reversible transitions.

Theorem 4.2. The semantics of ℵare microscopically reversible.

Proof. The rules (REFL,SYMM,TRANS,EXT,EXT(cid:48),TERM,REV1,REV2,SUB3) are microscopically re-
versible either trivially or inductively.

The instantiation rules (INST–COMP,INST–HALT) are less obvious, but can be realised in a
microscopically reversible fashion in much the same way that cells translate an mRNA template
to a polypeptide product. We shall ﬁrst need a microscopically reversible way to duplicate a term,
for which we assume that the environment contains an excess of structural building blocks (i.e.
free atoms, variables, units (), etc). A term t to be duplicated is ﬁrst marked as such i.e. t (cid:55)→ t.
These modiﬁed terms deviate from the deﬁnitions introduced at the beginning of this section, and
are instead intermediate transitional structures used for the small-step microscopically reversible
semantics described here. This marking is then propagated throughout the structure to all atoms,
variables, and units by the following two microscopically reversible transitions:

γ : t ↔ γ : t

(x (cid:126)x/s ) ↔ (ˆx (cid:126)x/s )
((cid:126)x/s ˆx y (cid:126)y/s ) ↔ ((cid:126)x/s x ˆy (cid:126)y/s )

(DUP–PROP1)

(DUP–PROP2)

(DUP–PROP3)

24

S ↔ T =⇒

S ↔ T ∧ T ↔ U =⇒

S ↔ S

T ↔ S

S ↔ U

S ↔ T =⇒

∀Γ. Γ ∪ S ↔ Γ ∪ T

s ↔ t =⇒ ∀Γ. Γ ∪ {s} ↔ Γ ∪ {t}

(REFL)

(SYMM)

(TRANS)

(EXT)
(EXT(cid:48))

P (cid:96) (I = O : R) =⇒

P (cid:96) (! τ )

=⇒

{} ↔ {(cid:104)I∅∅R∅O(cid:105)} (INST–COMP)
{} ↔ {(cid:104)τ (cid:105)}
c : ((cid:126)l t (cid:126)r) ↔ (c : ((cid:126)l • (cid:126)r)) : t
c : [(cid:126)l t (cid:126)r] ↔ (c : [(cid:126)l • (cid:126)r]) : t
∃b(cid:48). t τ∼ b(cid:48) =⇒ {x : t, (cid:104)τ (cid:105)} ↔ {x : [t], (cid:104)τ (cid:105)}
(cid:104)I∅∅R∅O(cid:105) ↔ (cid:104)O∅∅R∅I(cid:105)
(cid:104)∅IBRΓO(cid:105) ↔ (cid:104)∅OBRΓI(cid:105)

(INST–HALT)

(OHC1)

(OHC2)

(TERM)

(REV1)

(REV2)

x : t π∼ b =⇒
{x : t, (cid:104)(I ∪ {π})I (cid:48)BRΓO(cid:105)} ↔ {(cid:104)I(I (cid:48) ∪ {π})(B ∪ b)RΓO(cid:105)}

(INP)

π ∈ R ∧ x : t π∼ b =⇒

(SUB1)
(cid:104)II (cid:48)(B ∪ b)RΓO(cid:105) ↔ (cid:104)II (cid:48)BR(Γ ∪ {x : [t]})O(cid:105)

x /∈ VAR

(cid:48) =⇒

(SUB2)

{(cid:104)II (cid:48)BR(Γ ∪ {x : [t]})O(cid:105) ↔ {x : [t], (cid:104)II (cid:48)BRΓO(cid:105)}

Γ ↔ Γ(cid:48) =⇒ (cid:104)II (cid:48)BRΓO(cid:105) ↔ (cid:104)II (cid:48)BRΓ(cid:48)O(cid:105)

(SUB3)

Listing 5: Summary of ℵ semantics.

α ∈ ATOM
α α∼ ∅
πi∼ bi
(cid:86)
i ti
[t1 · · · tn] π1···πn∼ (cid:83)
γ π1∼ b1
t π2∼ b2
γ : t π1:π2∼ b1 ∪ b2

i bi

(UNIF–ATOM)

(UNIF–SUB)

(UNIF–CTXT2)

v ∈ VAR
t v∼ {v (cid:55)→ t}
(cid:48)

λ ∈ VAR

t π∼ b

γ : t λ:π∼ {λ (cid:55)→ γ} ∪ b

(UNIF–VAR)

(UNIF–CTXT1)

Listing 6: Summary of uniﬁcation semantics for ℵ.

25

1. ! + a b ();

2. ! () a b +;

3. {α : + Z b ()} = {α : () Z b +};
4. {α : + (Sa) b ()} = {α : () (Sa) (Sb(cid:48)) +}:

(ADD–BASE)

(ADD–STEP)

4a.

4b.

β : + a b ().
β : () a b(cid:48) +.

(ADD–STEP–SUB)

(a) Desugared deﬁnition of reversible natural addition in ℵ. It will be convenient to make the deﬁnitions

I4 = {α : + (Sa) b ()}, O4 = {α : () (Sa) (Sb(cid:48)) +} and R4 = {β : + a b (), β : () a b(cid:48) +}.

{():[+34()]}

P (cid:96) !+ab() =⇒ ↔ {(cid:104)+ab()(cid:105), ():[+34()]}

↔ {(cid:104)+ab()(cid:105), ():(+34())}

P (cid:96) !+ab() =⇒ ↔ {():(+34())}

P (cid:96) (I4 = O4 : R4) =⇒ ↔ {(cid:104)I4∅∅R4∅O4(cid:105), ():(+34())}

α:+(Sa)b()
∼

():(+34())

{· · ·} =⇒ ↔ {(cid:104)∅I4{α(cid:55)→(), a(cid:55)→2, b(cid:55)→4}R4∅O4(cid:105)}
· · · =⇒ ↔ {∅I4{α(cid:55)→()}R4{β:[+24()]}O4}
{β:[+24()]} ↔ {β:[()26+]} =⇒ ↔ {∅I4{α(cid:55)→()}R4{β:[()26+]}O4}

(SUB3)
· · · =⇒ ↔ {(cid:104)∅I4{α(cid:55)→(), a(cid:55)→2, b(cid:48)(cid:55)→6}R4∅O4(cid:105)} (SUB1)
↔ {(cid:104)∅O4{α(cid:55)→(), a(cid:55)→2, b(cid:48)(cid:55)→6}R4∅I4(cid:105)} (REV2)

():(()37+)

α:()(Sa)(Sb(cid:48))+
∼

{· · ·} =⇒ ↔ {(cid:104)O4∅∅R4∅I4(cid:105), ():(()37+)}
↔ {(cid:104)I4∅∅R4∅O4(cid:105), ():(()37+)}

P (cid:96) (I4 = O4 : R4) =⇒ ↔ {():(()37+)}

P (cid:96) !()ab+ =⇒ ↔ {(cid:104)()ab+(cid:105), ():(()37+)}

↔ {(cid:104)()ab+(cid:105), ():[()37+]}

P (cid:96) !()ab+ =⇒ ↔ {():[()37+]}

(INST–HALT)

(TERM)

(INST–HALT)

(INST–COMP)

(INP)

(SUB1)

(INP)

(REV1)

(INST–COMP)

(INST–HALT)

(TERM)

(INST–HALT)

(b) One possible derivation of () : [+ 3 4 ()] ↔ () : [() 3 7 +] using the semantics for ℵ.

Listing 7

26

where rules (DUP–PROP2,3) are used to distribute the marking throughout composite terms. Note
the use of the auxiliary ‘hat’ marker ˆ· to sequence this propagation in a microscopically reversible
manner. Elementary terms thus marked recruit fresh copies of themselves from the free structural
building blocks,
a
a

(DUP–FRESH)

()
()

u
u

v
v

↔

↔

↔

↔

()

u

a

v

(cid:48), and where the drawing of a fresh copy from the
where a ∈ ATOM, u ∈ VAR and v ∈ VAR
environment is implicit. These are then ligated together in parallel, and the composite structure
disentangled,
(cid:18)
(cid:126)s

(DUP–TOPO1)

(DUP–LIG1)

(cid:18)
(cid:126)s

↔

↔

(cid:19)

(cid:19)

(cid:19)

(cid:126)v

(cid:126)v

(cid:126)u
(cid:126)u

(cid:126)t
(cid:126)t
(cid:126)t
(cid:126)t

(cid:126)t (cid:126)u
(cid:126)t (cid:126)u
(cid:126)t (cid:126)u
(cid:126)t (cid:126)u

(cid:26)
(cid:126)s

(cid:27)

(cid:126)v

↔

(cid:26)
(cid:126)s

(cid:126)u
(cid:126)u

(cid:27)

(cid:126)v

(DUP–LIG2)

(cid:18)(cid:126)t
(cid:126)t
(cid:26)(cid:126)π
(cid:126)π

(cid:27)

γ
γ

:

t
t

↔

↔

((cid:126)t)
((cid:126)t)
{(cid:126)π}
{(cid:126)π}
γ : t
γ : t

(DUP–TOPO2)

(DUP–TOPO3)

ﬁnally resulting in a fully duplicated and disentangled structure t
party-bag. Finally, the instantiation rules can be realised microscopically reversible thus,

t from t for t any term, party, or

(I = O : S) ↔ (I = O : S)

↔ {(I = O : S), (cid:104)I∅∅S∅O(cid:105)}

(cid:26)(cid:18) I
I

=

O
O

:

(cid:110)(cid:16)
!

(cid:19)(cid:27)

S
S
(! τ ) ↔ (! τ )
τ
τ

(cid:17)(cid:111)

↔ {(! τ ), (cid:104)τ (cid:105)}

(INST–COMP1)

(INST–COMP2)

(INST–HALT1)

(INST–HALT2)

The one-hole-context rules are nearly trivially microscopically reversible, except for the choice

of partition. This can be achieved by a movable marker like so,

c :

x
ˆ
(cid:76)

c : (x (cid:126)x/s ) ↔ c :
y (cid:126)y/s
(cid:126)x/s x
(cid:126)(cid:96) t
ˆ
(cid:76)

(cid:126)x/s
(cid:126)y/s
(cid:126)x/s x y
(cid:77)
↔ (c : ((cid:126)(cid:96) • (cid:126)r)) : t

↔ c :

(cid:126)r

(cid:77)

(cid:76)

(cid:77)

(cid:77)

ˆ

ˆ

(cid:76)
c :

(OHC11)

(OHC12)

c :

(OHC13)

x
ˆ
(cid:74)

c : [x (cid:126)x/s ] ↔ c :
y (cid:126)y/s
(cid:126)x/s x
(cid:126)(cid:96) t
ˆ
(cid:74)

(cid:126)x/s
(cid:126)y/s
(cid:126)x/s x y
(cid:75)
↔ (c : [(cid:126)(cid:96) • (cid:126)r]) : t

↔ c :

(cid:126)r

(cid:75)

(cid:74)

(cid:75)

(cid:75)

ˆ

ˆ

(cid:74)
c :

(OHC21)

(OHC22)

(OHC23)

The rules (INP,SUB1,SUB2) require a microscopically reversible realisation of a bag with random
draws. In order to be microscopically reversible, the draw needs to be deterministic at any given
time, even if draws at different times are uncorrelated. This is achieved by representing a bag as a
string that can be shufﬂed via swap operations, i.e.

{(cid:126)x/s x y (cid:126)y/s } ↔ {(cid:126)x/s y x (cid:126)y/s }

(BAG–SHUFF)

Random draws are then implemented by simply picking the ﬁrst element of the string.

The last rules to demonstrate microscopic reversibility for are the uniﬁcation semantics. We
leave this as an exercise for the reader, with the hint that its realisation is similar to that of the
instantiation rules.

27

! TAPE (cid:96) x r;

! SYM x;

! BLANK;

[BLANK x · x/s ] (cid:56)
[(SYM x) · x/s ] (cid:56)
[] (cid:56)

POP

(cid:56)

BLANK [];

POP
(cid:56)

POP

(cid:56)

BLANK [x · x/s ];
BLANK x/s ;

(cid:56) (TAPE (cid:96)(cid:48) x(cid:48) r(cid:48)):

LEFT
(cid:56) x(cid:48) (cid:96)(cid:48).
(cid:56) x r.

POP

(TAPE (cid:96) x r) (cid:56)
(cid:96) (cid:56)
r(cid:48) (cid:56)
RIGHT
t(cid:48) (cid:56)

POP
(cid:56) t(cid:48):
LEFT

t (cid:56)

(cid:56) t.

Listing 8: This ℵ program deﬁnes all the ingredients necessary to simulate any Reversible Turing
Machine. We represent a bi-inﬁnite tape as its one-hole-context; that is, a tape is given
by the square in the current position, x, as well as ‘all’ the squares to the left of it,
(cid:96), and ‘all’ the squares to the right of it, r. Obviously we can’t actually represent all
the squares to the left and the right. Instead we make use of the fact that, at any one
time, only a ﬁnite bounded region of the tape is non-blank. As such, (cid:96) contains all the
squares to the left up to the last symbol, and similarly for r. If we keep going left, past
the ﬁnal symbol, then (cid:96) will be the empty list, and BLANK squares will be created as
needed. The rules LEFT and RIGHT are convenience functions for changing the focus
of a tape.

Computability The earlier examples give reasonable assurance that ℵ is Turing complete and
that programming in it is ‘easy’ in the sense that programs can be readily composed. For avoidance
of doubt, however, we prove that ℵ is Turing complete in two ways: the ﬁrst proves a stronger
claim, that ℵ is Reversible-Turing complete, meaning that it can efﬁciently and faithfully simulate
a Reversible Turing Machine without generating garbage; the second proves Turing completeness
in a more conventional fashion in order to demonstrate its high level of composability.

Theorem 4.3. The ℵcalculus is Reversible-Turing Complete, i.e. it can simulate any Reversible
Turing Machine (RTM) as deﬁned by Bennett [6].

Proof. An RTM is a collection of one or more bi-inﬁnite tapes divided into squares, each of
which can be blank or can contain a symbol, as well as a machine head with an internal state.
Symbols are drawn from a ﬁnite alphabet, and internal states from a distinct ﬁnite alphabet. At
any time, the RTM head looks at a single square on each tape and executes one of a ﬁnite set of

28

data CONST n;

-- Cn((cid:126)x/s ) = n

data SUCC;
-- S(x) = x + 1

data PROJ i;

-- Pi((cid:126)x/s ) = xi
-- N.B: (cid:126)x/s is 1-indexed.
data SUB g fs ;
-- (g ◦ (cid:126)fs )((cid:126)x/s ) =

--

data REC f g;

g(f1((cid:126)x/s ) · · · fn((cid:126)x/s ))
-- ρ(f, g)(0, (cid:126)x/s ) = f ((cid:126)x/s )
-- ρ(f, g)(n + 1, (cid:126)x/s ) =
g(n, ρ(f, g)(n, (cid:126)x/s ), (cid:126)x/s )

--

data MINIM f ;

-- µ(f )((cid:126)x/s ) =
-- min{n : f (n, (cid:126)x/s ) = 0}

x/s (cid:56)

MU (CONST n)(cid:56) n(cid:48) (GARBAGE x/s ):
DUP n(cid:56) n(cid:48).

(cid:56)

MU SUCC

(cid:56) (Sx) (GARBAGE);

[x] (cid:56)
x/s (cid:56)

(cid:56)

x/s (cid:56)

MU (PROJ i)(cid:56) y (GARBAGE y/s z/s ):
DUP i(cid:56) i(cid:48).
! ∼GO i(cid:48) [] x/s = ∼GO Z [y · y/s ] z/s .
∼GO (Sn) y/s [x · x/s ] = ∼GO n [y · y/s ] x/s ;
MU (SUB g fs )(cid:56) z (GARBAGE x/s h h/s ):
y/s (cid:56)
MU g(cid:56) z h.
(cid:56)∼GO fs x/s (cid:56) y/s h/s .
(cid:56)∼GO [] x/s (cid:56) [] [];
(cid:56)∼GO [f · fs x/s ](cid:56) [y · y/s ] [h · h/s ]:
DUP x/s (cid:56) x/s (cid:48).
x/s (cid:48) (cid:56)
MU f (cid:56) y h.
(cid:56)∼∼GO fs x/s (cid:56) y/s h/s .
[n · x/s ] (cid:56)
MU (REC f g)(cid:56) y (GARBAGE n(cid:48) x/s h/s ):
DUP x/s (cid:56) x/s (cid:48).
x/s (cid:48) (cid:56)
! ∼GO n Z g x x/s [h] = ∼GO Z n(cid:48) g y x/s h/s .
∼GO (Sn) m g x x/s hs = ∼GO n (Sm) g y x/s [h · h/s ]:
DUP x/s (cid:56) x/s (cid:48).
MU g(cid:56) y h.

DUP m(cid:56) m(cid:48). (cid:56)
[m(cid:48) x · x/s (cid:48)] (cid:56)

MU f (cid:56) x h.

(cid:56)

(cid:56)

(cid:56)

x/s (cid:56)

MU (MINIM f )(cid:56) i (GARBAGE x/s h/s ):
! ∼GO f Z x/s (SZ) [] = ∼GO f (Si) x/s Z h/s .
∼GO f i x/s (Sn) h/s = ∼GO f (Si) x/s y [n h · h/s ]:
DUP x/s (cid:56) x/s (cid:48).
MU f (cid:56) y h.

DUP i(cid:56) i(cid:48). (cid:56)
[i(cid:48) · x/s (cid:48)] (cid:56)

(cid:56)

Listing 9: An ℵ program∗ implementing the µ-recursive functions. The µ-recursive functions
are generated by the three functions, Cn, S and Pi, and the three operators, ◦, ρ
and µ, whose deﬁnitions are given in the comments above. The ℵ values cor-
responding to these functions can be directly composed using the operators (e.g.
(REC (CONST 0) (PROJ 1))), and then evaluated with the MU atom.

∗

Some additional sugar has been used in this program, mainly in the form of nested deﬁnitions and the more concise expression of
looping constructs. See Appendix B for an in-depth deﬁnition of these sugared forms.

29

reversible rules. The rule chosen is uniquely determined by the current state of the system. Each
rule depends on the current internal state of the machine head, which it may alter, and for each
tape it can either ignore its value, possibly moving the tape one square to the left or right, or it can
depend on the square taking a certain value u, which it can replace with a certain other value v.
For example, a 6-tape RTM with symbol alphabet {A, B, C, D, E, F } and state alphabet
{START, S1, S2, STOP} might have a rule S1 C ∅ / D / / → B A − F 0 + S2. This rule
applies if and only if the current internal state is S1, the values of tapes 1 and 4 are the symbols
C and D respectively, and tape 2 is blank. If so, it will change the internal state to S2, replace
the values of tapes 1, 2 and 4 with the symbols B, A and F respectively, and move tapes 3 and
6 one square to the left and right respectively, leaving tape 5 alone. The reverse of the rule is
given by S2 B A / F / / → C ∅ + D 0 − S1. The necessary conditions for the ruleset to be
deterministic and reversible are that all the domains of the forward rules are mutually exclusive,
as are all the domains of the reversed rules. Of course, the domains of the forward and reverse
rules will intersect in any useful program.

It is easy to translate any description of an RTM into ℵ. Bi-inﬁnite tapes can be easily
represented and manipulated, per the program in Listing 8, and any rule (such as the above) can
be mechanically translated as so,

S1 (TAPE (cid:96)1 (SYM C) r1) (TAPE (cid:96)2 BLANK r2) t3 (TAPE (cid:96)4 (SYM D) r4) t5 t6
3 (TAPE (cid:96)4 (SYM F ) r4) t5 t(cid:48)
6:

= S2 (TAPE (cid:96)1 (SYM B) r1) (TAPE (cid:96)2 (SYM A) r2) t(cid:48)

t3
t6

(cid:56)

(cid:56)

LEFT

(cid:56) t(cid:48)
3.
(cid:56) t(cid:48)
6.
RIGHT

whilst the special START and STOP states are marked thus,

! START t1 t2 t3 t4 t5 t6;
! STOP t1 t2 t3 t4 t5 t6;

That this translation faithfully reproduces the operation of the RTM is obvious from the high level
semantics of ℵ.

Corollary 4.4. The ℵcalculus is Turing Complete.

Proof. Bennett [6] proved that a Reversible Turing Machine can simulate any Turing Machine
(and vice-versa), and so the corollary immediately follows immediately from ℵ’s Reversible-
Turing completeness. To drive the point home, however, we also implement the µ-Recursive
Functions (Listing 9)—three functions, and three composition operators, which together are
capable of representing any computable function over the naturals and hence are Turing complete.
For example, addition, multiplication and factorials can be deﬁned in terms of each other (in our
ℵ realisation) as

add (cid:55)→ (REC (PROJ 1) (SUB SUCC [(PROJ 2)]))

mul (cid:55)→ (REC (CONST 0) (SUB add [(PROJ 2) (PROJ 3)]))

fac (cid:55)→ (REC (CONST 1) (SUB mul [(PROJ 2) (SUB SUCC [(PROJ 1)])]))

and then one can evaluate, e.g., (MU fac) [7] (), obtaining () 5040 garbage (MU fac).

30

5. Implementation Concerns

Although the ℵ calculus is microscopically reversible by Theorem 4.2, ℵ has many degrees of
freedom wherein macroscopic reversibility can be violated. There are two sources of entropy
generation; the ﬁrst is ambiguity in rule application, and the second is intrinsic to any useful
implementation of concurrency. We already saw in Section 3 how concurrency leads to entropy
generation, but in this section we will expand on the other mechanism of entropy generation and
how it can be avoided at the compiler-level. This is important because the primary motivation for
reversible programming is to avoid unexpected entropy leaks, and so generally the appearance of
ambiguity is a bug rather than intentional. In addition to ambiguity, we will also discuss other
aspects of a realistic implementation to avoid or minimise the presence of random walks: optim-
ising the evaluation order of sub-rules (‘serialisation’) and inferring the direction of computation.
A reference implementation of these algorithms (as well as additional sugar) is made available as
a language and interpreter, alethe (see Appendix B).

Nevertheless, in some cases the appearance of ambiguity/non-determinism may be intentional
and so we should provide the programmer a mechanism to explicitly weaken the ambiguity
checker when desired. An example of this might be in implementing a fair coin toss for generating
randomness,

(cid:56)

(cid:56)

(cid:56)

(cid:56)

COIN

COIN

TAILS;

HEADS;

If the computational architecture is Brownian, such as a molecular computer, then this can be used
to exploit the thermal noise of the environment to get as close as classical physics allows to a true
random number generator (RNG). The way this RNG is used is by constructing a term COIN ().
This term then matches both rules, and so depending on which is chosen the term may evolve
to () TAILS COIN or to () HEADS COIN. Moreover, this process will depend on the probability
distribution realised by the implementation; if we assume this is uniform4, then the coin toss will
be fair with each term observed with probability 1
2 . If the probability distribution is non-uniform,
the situation is less clear and depends on the exact rates of the forward and reverse transitions for
each rule. The analysis is beyond the scope of this paper, but if the dynamics takes the form

() TAILS COIN

λ(cid:48)
t−−(cid:42)(cid:41)−−
λt

COIN ()

λh−−(cid:42)(cid:41)−−
λ(cid:48)
h

() HEADS COIN,

λh/λ(cid:48)
. By microscopic reversibility, we expect
then the steady-state ratio of heads to tails will be
h
λt/λ(cid:48)
t
λh = λ(cid:48)
h and similarly for λt (unless the system is biased away from equilibrium), and therefore
even a non-uniform distribution will have a uniformly distributed steady-state. Note the interesting
property that, in reverse, an ambiguous/non-deterministic reversible rule is indistinguishable from
irreversibility. Here, for example, the inverse of a fair coin toss is a process that consumes5 a bit.

4A more sophisticated implementation might reify and expose control over the distribution to the programmer,
allowing arbitrary probabilities to be assigned. Going further, an interesting research direction would be to what
extent ℵ can be augmented quantum mechanically.

5There is a subtle point to make here: if the bit being consumed has no computational content and is uniformly

31

Ambiguity Checker To exclude these sorts of process, we thus introduce an ambiguity checker.
Not only does the introduction of an ambiguity checker reassure the programmer that they are
writing information-preserving code, but the algorithm also serves as an invaluable debugging
tool. In a large codebase, it can be hard to keep track of all the different patterns employed (the
standard library has nearly two thousand), let alone ensure that there are no ambiguities. The
ambiguity checker in alethe performs this check for the programmer and, most importantly, is
able to tell the programmer where the ambiguities lie.

In an unambiguous program, there are only a few valid scenarios for each term. It can match
two computational rules (where we think of the forward and reverse directions of each rule as
distinct rules for the current purpose), in which case it is an intermediate state of a computation.
It can match one computational rule and one or more halting rules, in which case it is a terminal
state of the computation. It can match one computational rule, in which case the program has
entered an erroneous state and stalled. It can match one or more halting rules, in which case the
term has no computational capacity, and most likely represents a data value. Lastly, it can match
no rules, in which case it is an invalid term that cannot be constructed under normal conditions.
The remaining possibilities can be summarised two-fold: a term can match two computational
rules and one or more halting rules, in which case the computational path from the terminus is
ambiguous, or a term can match more than two computational rules (and zero or more halting
rules), which is the more obvious ambiguity scenario.

The above cases can be simpliﬁed: for a given term, consider all the rules it matches but
coalesce all halting rules, if any, into a single rule. Then, a term is unambiguous if it matches at
most two such rules, and ambiguous if it matches more than two.

Clearly it is impractical to test this for all possible terms, there being a countable inﬁnity of
them; instead, one can determine whether it is possible to construct an ambiguous term. To do
so, build a graph G whose nodes are all the patterns occurring on either side of a computational
rule—which we will colour white for reasons that will become apparent—and all the patterns
occurring in a halting rule—which we will colour black. An edge is drawn between two nodes
if it is possible to construct a term satisfying both patterns. This condition can be determined
inductively: if either pattern is variable, then draw an edge. If neither pattern is variable, then
draw an edge only if they are both the same atom, or they are both composite terms of the same
length and their respective sub-patterns matches pairwise. In order to proceed, we ﬁrst prove a
lemma about this graph.

Lemma 5.1. There exists a term that simultaneously satisﬁes each of a set of patterns Π if and
only if the subgraph of G formed from the nodes Π is complete.

Proof. The (⇐) case is obvious. We prove the converse by structural induction. As base cases,
we note that the statement is trivially true if |Π| = 0, 1, 2. Now consider the possible forms of
some π ∈ Π:

distributed, then this consumption does not generate any entropy. That is, drawing a random bit from the
environment and then replacing it is a completely isentropic process. For non-uniform distributions, the process
can also be isentropic provided that the producer/consumer has a matching distribution. The problem arises when
the distribution of the bit does not ﬁt that of the producer/consumer, with deterministic computation being an
extreme case wherein the bit can only take on a prescribed value (this is true even if the bit is pseudorandom).

32

(ATOM) If π is an atom, a, then the only term satisfying it is the same atom, a. As π has edges to

every other pattern in Π, a must satisfy each of these other patterns too.

(VAR) Suppose there is a term t that satisﬁes all the patterns Π \ {π}. As t satisﬁes the variable
pattern π vacuously, t must satisfy all of Π. Without loss of generality then, we can ignore
all variable patterns.

(τ1 · · · τn) If π is a composite term, then there can only be an edge to the other patterns if these
are composite terms of the same length or are the variable pattern. By the previous case,
we can ignore these variable patterns. Now, index the patterns by i = 1 . . . m and write
πi = (τ (i)
n ). Then construct graphs H1 · · · Hn such that the nodes of Hj are the
1
patterns {τ (i)
: i = 1 . . . m}. If we draw edges according to the same rules as for G,
then by the inductive deﬁnition of the edge condition for G, each of the graphs Hi will be
complete. By inductive assumption, a term ti exists satisfying all patterns in Hi for each i,
and therefore the term (t1 · · · tn) must satisfy all the patterns Π.

· · · τ (i)

j

The lemma follows.

Using Lemma 5.1, we see that the different cases of unambiguous and ambiguous terms reduce
to considering the structures of the complete subgraphs of G. Moreover, it sufﬁces to only
consider subgraphs formed from three nodes, i.e. triangles: the ﬁrst ambiguous case, of two
white nodes and one or more black nodes, implies the existence of a triangle with two white
nodes and one black node; the second case, of three or more white nodes and zero or more black
nodes, implies the existence of a triangle with three white nodes. Conversely, a triangle with
two or more white nodes implies one of these ambiguous cases. The only triangles present for
the unambiguous cases contain two or more black nodes. Therefore we can enumerate all the
triangles of G containing two or more white nodes; if there are any, then there is an ambiguity
and the patterns present in the triangles should be reported to the programmer. If there are none,
then the program is unambiguous and deterministic.

In fact, there is one more potential source of ambiguity. Suppose that the two patterns of a
sub-rule are not orthogonal (a common occurrence), then it is in principle possible that a halting
term might match either side, and therefore lead to a 2-way ambiguity. The evaluation rules of
our reference interpreter for alethe do not suffer from this ambiguity, as it determines ahead of
time the order and directionality of all sub-rules, but it is possible for a faithful implementation
of ℵ to suffer so. There are two ways to avoid this: the ﬁrst is to require sub-rule patterns be
orthogonal, though this also forbids many legal unambiguous programs—requiring additional
levels of tedious and unnecessary indirection to circumvent the restriction—and so is undesirable;
the second is to apply type inference (work towards which is presented in Appendix C) to gain
the extra knowledge needed to distinguish between legal and ambiguous programs. Namely,
type inference allows the compiler to infer what sequences of patterns and terms occur in a
program, and hence to determine which halting patterns are computationally connected. With
this knowledge, and knowledge of the types of variables in a rule, it can determine what forms
the halting terms of a sub-rule will take and therefore whether there is a unique mapping between
terms and patterns or not.

33

Serialisation Heuristics The Brownian semantics of ℵ, particularly in the absence of coupling
to a bias source, are not very performant. Speciﬁcally, sub-rule transitions execute a random
walk in a phase space whose size—in the worst case—scales exponentially with the number of
variables present in the current rule. Whilst explicit bias coupling circumvents this by essentially
serving to annotate the preferred order and directionality of the sub-rules, it undermines the
declarative nature of ℵ. Instead, it is possible to algorithmically infer an appropriate execution
path. Moreover, it can sometimes be the case (such as in the example of fractional addition) that
the execution path is non-trivial and so granting the compiler the power to perform this routing
automatically makes the job of the programmer easier: the programmer need only specify how
all the variables relate to one another, and may even specify this excessively, and the compiler
can ﬁgure out which relations are sensible to use.

To illustrate the problem, consider the following program excerpt which adds two simpliﬁed

fractions:

a/b (cid:56)+ (FRAC p q)(cid:56) c/d:

1.

2.

3.

4.

5.

6.

p − p(cid:48).
a/b (cid:56)∼ (FRAC p q)(cid:56) c/d g.
c/d (cid:56)∼ (FRAC p(cid:48) q)(cid:56) a/b g(cid:48).
(Sq) (cid:3) q2.
g(cid:48) (cid:56)× g(cid:56) q2.
g (cid:56)× g(cid:48)(cid:56) q2.

where a fraction is represented by p
q+1 ≡ (FRAC p q) with p ∈ Z and q ∈ N. Maintaining the
invariant that fractions are in their simplest form is non-trivial, but involves ﬁnding the greatest
common divisor of the numerators and denominators of two intermediate results, g and g(cid:48), and
then showing that their product gg(cid:48) = (q + 1)2. It turns out that this fact can then be used to
eliminate the intermediate garbage. In the unassisted semantics of ℵ, the graph of all intermediate
states generated by applying these rules transitively is given by Figure 5a. This graph has 117
nodes. It can therefore be seen that a computation is very likely to get stuck in one of the 115
intermediates, and will take a long time to ﬁnd its way from the input variables, {a/b, p, q}, to the
output variables, {p, q, c/d}. Many of these states are not interesting computational, and indeed
only a small subgraph of 11 (or even 8) nodes is needed to perform the desired computation as
shown in Figure 5b.

From this transition subgraph, we can see that there are two possible paths to get from the input
variables to the output variables, and the goal of the serialisation algorithm is to ﬁnd these paths
and to pick the most optimal. Here both of these paths are of equal complexity, and so either is
valid. Having made a choice, the compiler can then direct the computer to perform the prescribed
series of transitions, rather than executing a random walk. In a Brownian computational system,
for example, this would be achieved by adding ‘fake’ dependencies to the sub-rules to force a
linear ordering (or possibly partially parallel, where this makes sense) and automatically coupling
the sub-rules to the bias source in the correct direction. As we shall see, automatically determining
the cost of a path is non-trivial and sometimes even impossible, and so a perfect solution to the
serialisation problem is not generally possible. Nonetheless, for many programs it is possible to

34

(a) The full transition graph. Due to the size of the graph, labels have been suppressed. Halting states are

marked black.

c/d g
p q q2

5

c/d g
p q g(cid:48)

2

a/b g(cid:48)
p q

1

a/b g(cid:48)
p(cid:48) q

4

3

c/d
p(cid:48) q

1

c/d
p q

2

a/b
p q

4

c/d g
p q

1

c/d g
p(cid:48) q

3

a/b g(cid:48)
p(cid:48) q g

6

a/b g(cid:48)
p(cid:48) q q2

(b) A more practical excerpt of the full transition graph.

Figure 5: Two views of the transition graph for the fraction-addition routine. Nodes correspond to
bags of variables in a ‘known’ state. An edge with label (cid:96) is drawn from node α to node
β if sub-rule (cid:96) can map the knowledge state α to the knowledge state β (with possible
variable duplication/elision). It can be seen that there are two paths from {a/b, p, q}
to {c/d, p, q} worth considering: (cid:126)2−(cid:126)4−(cid:126)5−(cid:126)2−(cid:126)1−(cid:126)3−(cid:126)1, and (cid:126)2−(cid:126)1−(cid:126)3−(cid:126)6−(cid:126)4−(cid:126)3−(cid:126)1. These
are of roughly equal computational complexity, and so either is a viable choice.

35

ﬁnd the appropriate path, or to give the compiler enough information to make a better choice, and
so further discussion of serialisation is warranted.

The simplest and least efﬁcient serialisation strategy is as follows: suppose you start with the
input variables {x, y, z}. Enumerate all the sub-rules that can be evaluated given this current
(cid:56) vs would not
‘knowledge state’, e.g. x (cid:56)
because neither the variable ws nor the variable vs are currently available. Evaluate all of these
Here, we would end up with the knowledge state {ws , x, y, z}. Now ignore these sub-rules going

sub-rules in parallel, making sure to duplicate variables as necessary to avoid unlearning any.

(cid:56) y ws and z ∼ x would qualify but ws (cid:56)

MAP BAR

FOO

forward and repeat this process until all sub-rules have been used once (and only once), and all
variables are known. If some sub-rule has not been used or a variable remains unknown, then there
is a logic error and the programmer should be appropriately chastised. Now, perform this entire
process again, but starting from the output variables. If all is well, then we will have obtained
two routes: one from the input variables to all variables, and one from the output variables to
all variables. As these routes are reversible, we are immediately rewarded with a route from the
input to the output variables, using each sub-rule twice. Whilst clearly inefﬁcient, this strategy
demonstrates that each sub-rule need be used at most twice (once in each direction).

A better strategy is to construct a ‘transition graph’ as follows: take as nodes the powerset of
all variables, i.e. ∅, {x}, {y}, {x, y}, etc. Mark the input and output nodes specially for later.
Draw an edge between two nodes if a sub-rule (with possible variable duplication/elision) can
be used to map between the two knowledge states, and label the edge with this sub-rule and in
which direction it is to be applied. With the transition graph thus constructed, all possible routes
between the input and output nodes can be enumerated6. To proceed, a cost heuristic is needed in
order to rank routes by preference. For example, if the goal is simply to minimise the number
of sub-rules used then Dijkstra’s algorithm will sufﬁce. Note that, in the reference interpreter,
transition graphs are constructed more restrictively. Namely, variable duplication/elision is not
implemented as this always7 leads to an exponentially large transition graph and, in practice,
it is rarely needed. Where it is needed, the programmer must instead explicitly use DUP (and
create a new variable). This provides dramatically better performance at the expense of a slight
inconvenience.

We saw an example of this algorithm earlier in Figure 5. Another example, deserving of
further comment, is given in Listing 10. There are two routes of apparent equal cost, (cid:126)1−(cid:126)2−(cid:126)2−(cid:126)3
and (cid:126)1−(cid:126)2−(cid:126)3−(cid:126)3, but this is misleading. If the tree has n nodes across (cid:96) ∼ log n levels, a single
(non-recursive) step of POLISH has time complexity α, and a single (non-recursive) step of
POLISHREADS has time complexity β, then the ﬁrst route can be shown to have time complexity
O(2(cid:96)n(α + β)) whilst the second has complexity O(n(α + 2(cid:96)β)). That is, if the serialisation
algorithm picks a route which repeats a recursive step then consequently there will be an exponen-
tial overhead. Moreover, this becomes less obvious in the cases of corecursion, or higher order

6In fact there are inﬁnitely many routes. It is therefore appropriate to restrict the routes under consideration to a sane
(and ﬁnite) subset; namely, we avoid revisiting any node, and we also exclude routes that make use of a sub-rule
more than twice.

7The serialisation algorithm in alethe is still subject to an exponential worst case complexity. The size of the
transition graph constructed depends on the inter-dependency of the variables: the more dependency, the closer to
linear in the number of variables the graph size is. If there is minimal dependency, such as in the automatically
generated implemention of DUP for data , a b c d e, then the worst case will be realised.

36

data TREE α = TREE α [TREE α]

POLISH :: TREE α → [(INT, α)]

POLISH (TREE x x/s ) = (LENGTH x/s , x) : CONCATMAP POLISH x/s

(a) A snippet of Haskell code for converting an arbitrary tree into Polish notation. Polish notation is an
isomorphic linear representation of tree-like structures that is well known for its use for arithmetic
expressions. For example, the expression (3 + 4) × 7 − 25 can be represented as − × + 3 4 7 ∧ 2 5.

(cid:56)

POLISH

(TREE x t/s ) (cid:56)
LENGTH x/s (cid:56) n.
t/s (cid:56)
POLISHREADS n(cid:56) t/s (cid:96)/s .
p (cid:56)

(cid:56) [(, x n) · p]:
(cid:56) p (cid:96)/s ..

CONCATMAP POLISH

1.

2.

3.

data TREE x t/s ;

data , a b;

-- 2-tuples

(b) An excerpt of a translation of the Haskell implementation into alethe (see Listing S.1 for the full
program). Unfortunately, the implementation of CONCATMAP in alethe has additional garbage in

the form of a list of the lengths of the intermediate lists ((cid:96)/s ). To eliminate this garbage, we write a

function POLISHREADS which takes a string of trees in Polish notation and converts them back into
trees, also generating the same garbage value.

x

t/s

1

x n

t/s

2

x n

p (cid:96)/s

2

3

x n

t/s (cid:96)/s

3

x n
p

(c) The motif employed above—generating the garbage in two different ways—can be used to eliminate it
and achieve the desired (partial) bijection, as shown by its transition graph. There are two possible
execution paths, (cid:126)1−(cid:126)2−(cid:126)2−(cid:126)3 and (cid:126)1−(cid:126)2−(cid:126)3−(cid:126)3. As explained in the text, the second route is strongly
preferred— hence the cost annotation on (2).

POLISHREAD

(cid:56) [] t.

t (cid:56)

(cid:56) p:

POLISH
p (cid:56)
[(, x n) · x/s ] (cid:56)
x/s n (cid:56)

POLISHREAD

POLISHREADS

(cid:56) x/s (cid:48) (TREE x t/s ):
(cid:56) x/s (cid:48) t/s .

POLISHREADS

(cid:56)

x/s Z
x/s (Sn) (cid:56)
x/s (cid:56)
x/s (cid:48) n (cid:56)

(cid:56) x/s [];
(cid:56) x/s (cid:48)(cid:48) [t · t/s ]:
(cid:56) x/s (cid:48) t.
(cid:56) x/s (cid:48) t/s .

POLISHREADS

POLISHREAD

POLISHREADS

(d) In fact, programming in a ‘reversible-ﬁrst’ manner permits us to ﬁnd the far more efﬁcient and naturally
bijective program above. This program was written by considering how to read a tree from its Polish
representation: In our experience, picking the ‘harder’ direction of a bijective computation to implement
helps to suppress one’s learned intuition for irreversible programming, and hence makes it less likely
to fall into the traps of ‘shortcuts’ with their accompanying garbage.

Listing 10: An example of serialisation as applied to the interconversion between trees and Polish

notation.

37

functions where a function may be passed into itself (recursively or corecursively)—compare, for
example, the Y combinator. As such it is not generally possible to algorithmically discriminate
this scenario (although the extra knowledge afforded by the type inference algorithm developed
in Appendix C may help). In fact, the reference interpreter for alethe does not even try to do so.
Instead, it exposes a method by which the programmer can adjust the heuristic cost function used
in serialisation: each edge in the transition graph has a weight, given by the number of full-stops
following the sub-rule in the source; therefore, in this case rule (2) should be annotated with a
cost of 2 (via two full-stops) in order to penalise its repeated use. To inspect the route chosen, the
:p directive may be issued to the interpreter.

Directional Evaluation Consider an intermediate term along a reversible computation path
(Figure 3). In isolation, one cannot know in which direction computation should proceed. Bias
coupling helps with this, but alethe does not make use of this. Moreover, it would be nice to
avoid explicit bias-coupling where practical. As such, we need some concept of computational
‘momentum’ to maintain a consistent direction. This requires that the programs are deterministic
such that phase space is branchless, which fortunately we have guaranteed through the ambiguity
checker. Recall that, treating the forward and reverse directions of a rule as distinct, every term
matches at most two rules; if we maintain a consistent direction of computation, then one of these
rules will be the converse of the rule that was employed to reach the current state. More concretely,
let the terms of the computation be labelled as some contiguous subset of {|n(cid:105) : n ∈ Z}. By
determinism, there is a unique rule r(n (cid:55)→ n + 1) enacting each transition |n(cid:105) (cid:55)→ |n + 1(cid:105), and
by reversibility the unique rule enacting the transition |n + 1(cid:105) (cid:55)→ |n(cid:105) is ¯r, the converse of r.
Therefore, given a term |n + 1(cid:105), the rule ¯r(n (cid:55)→ n + 1) can only take us to |n(cid:105); it can never also
be the rule taking us to |n + 2(cid:105). It is, however, possible that the rule taking us to |n + 2(cid:105) is the
same as r(n (cid:55)→ n + 1). To summarise, all we need do is keep track of which rule we just applied,
r; then, when considering the new term, we ﬁnd any rules it matches and exclude ¯r from this
set. If the set is empty, we have entered an error state and should report it to the user. Otherwise,
our ambiguity check has ensured that it either contains a single computational rule, which we
duly apply, or contains one or more halting rules, whence we would halt the computation and
report the result. This evaluation logic applies just as well to sub-rules. The one edge case is
when evaluating a new term; as only halting terms can be constructed, it will have at most one
computational rule to choose from and thus there is no ambiguity.

6. Conclusion

The examples furnishing this paper demonstrate the utility of the ℵ calculus, as well as its
appropriateness as a candidate model targeting reversible molecular and Brownian computational
architectures. Though a molecular implementation is as yet unrealised, an interpreter for the
programming language alethe serves as a useful testbed for ℵ. Going forward, we hope to
extend the interpreter to support the full concurrent language. Additionally, development of a
type system appropriate for a language that is both reversible and declarative would be helpful
for improving code analysis and reducing programmer errors, and work towards this is underway
(see Appendix C). Lastly, it is hoped that ℵ can be realised experimentally in a molecular context.

38

(PATTERN TERM)

τ ::= α | VAR | ( τ ∗ ) | σ
(ATOM) α ::= ATOM | ∼ α | #(cid:56)(cid:56)
σ ::= N | (cid:56)(cid:56)
(VALUE)
CHAR
π ::= τ : τ ∗ | VAR

∗ (cid:48)(cid:48) | #α
CHAR
∗ (cid:48)(cid:48) | [ τ ∗ ] | [ τ + . τ ] |
(cid:48) : τ ∗

(PARTY)

(PARTIES) Π ::= π | Π ; π

(DEFINITION HEAD)

(DEFINITION RULE)

(DEFINITION HALT)

(RELATION)

ρ ::= τ ∗ = τ ∗ | τ ∗ (cid:56)τ ∗(cid:48) τ ∗
δh ::= ρ | { Π } = { Π }
δr ::= δh ; | δh : ∆+
δt ::= ! τ ∗; | ! ρ ;
δ ::= δr | δt
ξ ::= .∗
(DECLARATION) ∆ ::= δ | π .ξ | ρ .ξ | ! ρ .ξ

(DEFINITION)

(COST ANNOTATION)

(STATEMENT) Σ ::= δ | data τ ∗; | import (cid:56)(cid:56)

MODULE PATH

(cid:48)(cid:48);

(PROGRAM) P ::= Σ∗

Listing 11: Deﬁnition of alethe syntax.

A. Acknowledgements

The author would like to acknowledge the invaluable help and support of his supervisor, Gos
Micklem. This work was supported by the Engineering and Physical Sciences Research Council,
project reference 1781682.

B. A Spoonful of Sugar: alethe

For clarity and convenience, we have introduced a number of syntactic shorthands in Sections 2
and 3. We now summarise and extend this sugar to construct a programming language, alethe,
the deﬁnition of which is given in Listing 11. Implementing the additional measures discussed in
Section 5, a reference interpreter for alethe is also made available8 together with a ‘standard
library’ and select examples9. We proceed with the deﬁnition of alethe by desugaring each
form in turn.

(PATT. TERM) τ ::= α | VAR | ( τ ∗ ) | σ

The deﬁnition of a pattern term only differs from its deﬁnition in ℵ by the inclusion of
sugared values, σ.

8https://github.com/hannah-earley/alethe-repl
9https://github.com/hannah-earley/alethe-examples

39

x/s (cid:56)

x/s (cid:56)

REVERSE

(cid:56) n/s (cid:48).

INSERTIONSORT p(cid:56) n/s (cid:48) y/s :
n/s (cid:56)
! ∼GO p x/s [] [] = ∼GO p x/s [n · n/s ] y/s (cid:48).
∼GO p [x · x/s ] n/s y/s = ∼GO p x/s [n · n/s (cid:48)] y/s (cid:48):
INSERT p(cid:56) n y/s (cid:48).
MAP (∼GO n)(cid:56) n/s (cid:48).

x y/s (cid:56)
n/s (cid:56)
m (cid:56)∼GO n(cid:56) m(cid:48):
(cid:56)< m n(cid:56) b.
(cid:56)< m(cid:48) n(cid:56) b.
m (cid:56)∼∼ b(cid:56) m(cid:48).

(cid:56) m;
(cid:56) (Sm);

m (cid:56)∼ TRUE
m (cid:56)∼ FALSE
(cid:56) y/s :

REVERSE

! ∼GO x/s [] = ∼GO [] y/s .
∼GO [x · x/s ] y/s = ∼GO x/s [x · y/s ];

[77 2 42 68 41 36 8 36] (cid:56)

INSERTIONSORT <(cid:56) [7 0 5 6 4 2 1 3] [2 8 36 36 41 42 68 77].

Listing 12: The alethe implementation of insertion sort from the standard library, making use
of nested locally scoped deﬁnitions. In contrast to the implementation in Listing 4,

this deﬁnition yields more useful ‘garbage’ data in that n/s (cid:48) contains the permutation
which maps x/s to y/s , as shown in the boxed example. Speciﬁcally, this corresponds to

the following permutation (in standard notation):

(cid:18)0 1 2 3 4 5 6 7
7 0 5 6 4 2 1 3

(cid:19)

.

40

(ATOM) α ::= ATOM | ∼ α | #(cid:56)(cid:56)

CHAR

∗ (cid:48)(cid:48) | #α

Atoms are fundamentally the same as in ℵ, but there are four ways of inputting an atom.
If the name of an atom begins with an uppercase letter or a symbol, then it can be typed
directly. In fact, any string of non-reserved and non-whitespace characters that does not
begin with a lowercase letter (as deﬁned by Unicode) qualiﬁes as an atom; if it does begin
with a lowercase letter, it qualiﬁes as a variable. If one wishes to use an atom name that
does not follow this rule, one can use the form #(cid:56)(cid:56)
∗ is any string
(using Haskell-style character escapes if needed). Additionally you can preﬁx a symbol
with # to suppress its interpretation as a relation (e.g. #+). Finally, if an atom is preﬁxed
with some number of tildes then it is locally scoped: that is, you can nest rule deﬁnitions
and make these unavailable outside their scope, and you can refer to locally scoped atoms
in outer scopes by using more tildes (there is no scope inheritance). You can even use an
empty atom name if preceded by a tilde, which can be useful for introducing anonymous
deﬁnitions. Example code using this sugar is given in Listing 12.

∗ (cid:48)(cid:48), where CHAR

CHAR

(VALUE) σ ::= N | (cid:56)(cid:56)

CHAR

∗ (cid:48)(cid:48) | [ τ ∗ ] | [ τ + . τ ] |

The sugar here for natural numbers (N) and lists is familiar from earlier, but there are two
additional sugared value types: strings10, which are lists of character atoms where the
character atom for, e.g., c is ’c, and units, which can be inserted with instead of ().

(PARTY) π ::= τ : τ ∗ | VAR

(cid:48) : τ ∗

Parties are the same as in raw ℵ. Note, however, that the reference interpreter of alethe
has no separate representation for opaque variables: if the context is a variable, then it is
assumed to be opaque rather than the variable pattern. When concurrency and contextual
evaluation is supported in a future version, this deﬁciency will need to be addressed.

(PARTIES) Π ::= π | Π ; π

Bags of parties are delimited by semicolons.

(RELATION) ρ ::= τ ∗ = τ ∗ | τ ∗ (cid:56)τ ∗(cid:48) τ ∗

Relations are shorthand for where there is a single, variable context. These are familiar
from the previous examples.

(DEF. HEAD) δh ::= ρ | { Π } = { Π }

A rule is either a relation, or a mapping between party-bags which are enclosed in braces.

(DEF. RULE) δr ::= δh ; | δh : ∆+

If a rule has no sub-rules, then it is followed by a semicolon, otherwise it is followed by a
colon and then its sub-rules/any locally scoped rules. These declarations must either be in
the same line, or should be indented more than the rule head similar to Haskell’s off-side
rule or python’s block syntax.

(DEF. HALT) δt ::= ! τ ∗; | ! ρ ;

As well as the canonical halting pattern form, there is also sugar for a relation wherein both
sides of the relation each beget a halting pattern, as does the inﬁx term (if any).

10Once again, using Haskell-style character escapes if needed

41

(DEF.) δ ::= δr | δt

This is the same as for ℵ.

(COST ANN.) ξ ::= .∗

Sub-rules can be followed by more than one full-stop, with the number of full-stops used
quantifying the cost of the sub-rule for use in the serialisation algorithm heuristics as
explained towards the end of Section 5.

(DECL.) ∆ ::= δ | π .ξ | ρ .ξ | ! ρ .ξ

In addition to the party sub-rules present in ℵ, there are three sugared forms. If a declaration
is a deﬁnition then, after introducing and resolving fresh anonymous names for any locally
scoped atoms, the behaviour is the same as if the deﬁnition was written in the global scope.
If a declaration takes the form of a relation, then it is the same as if we introduced a fresh
context variable and bound each side of the relation to it. If the relation is preceded by !,
then it deﬁnes both a sub-rule and halting pattern, e.g.

! ∼GO p x/s [] [] = ∼GO p x/s [n · n/s ] y/s (cid:48).

becomes

! ∼GO p x/s [] [];
! ∼GO p x/s [n · n/s ] y/s (cid:48);
∼GO p x/s [] [] = ∼GO p x/s [n · n/s ] y/s (cid:48).
(cid:48)(cid:48);
A statement is a deﬁnition, a data deﬁnition, or an import statement. A data deﬁnition
data S n; is simply sugar for

MODULE PATH

(STATEMENT) Σ ::= δ | data τ ∗; | import (cid:56)(cid:56)

! S n;
DUP (S n)(cid:56) (S n(cid:48)):
(cid:56)
DUP n(cid:56) n(cid:48).

(cid:56)

i.e. it marks the pattern as halting and automatically writes a deﬁnition of DUP. Future
versions may automatically write other deﬁnitions, such as comparison functions, or include
a derivation syntax akin to Haskell’s. The import statement imports all the deﬁnitions
of the referenced ﬁle, as if they were one ﬁle, and supports mutual dependency. Future
versions may support partial, qualiﬁed and renaming import variants.

(PROGRAM) P ::= Σ∗

A program is a series of statements. Additionally, comments can be added in Haskell-
style:

-- this is a comment.

! S {- so is this... -} n; -- ...and this.

{- and this is

a multiline comment -}

42

We also note that the interpreter for alethe treats the atoms Z, S, CONS, NIL, GARBAGE, and
character atoms specially when printing to the terminal. Speciﬁcally, they are automatically
recognised and printed according to the sugared representations deﬁned above. That is, except for
the GARBAGE atom which, when in the ﬁrst position of a term, hides its contents—rendering as
{˜GARBAGE˜}. This is useful when working with reversible simulations of irreversible functions
that generate copious amounts of garbage data. If one assigns the garbage to a variable, then the
special :g directive can be used to inspect its contents. Other directives include :q to quit, :l
file1 ... to load the speciﬁed ﬁles as the current program, :r to reload the current program,
:v to list the currently assigned variables after the course of the interpreter session, and :p to
show all the loaded rules of the current program (and the derived serialisation strategy for each,
see later). Computations can be performed in one of two ways; the ﬁrst, | (+ 3) 4 -, takes as
input a halting term, attempts to evaluate it to completion, and returns the output if successful, i.e.
() 7 (+ 3). The second, > 4 ‘+ 3‘ y, takes a relation as input and attempts to get from one
side to the other. For example, here we evaluate (+ 3) 4 (), obtaining () 7 (+ 3) which we
then unify against () y (+ 3), ﬁnally resulting in the variable assignment y (cid:55)→ 7. To run in the
opposite direction, use < instead. Note that, whilst the interpreter does understand concurrent
rules, it is not yet able to evaluate them.

Uniﬁcation Efﬁciency Beyond the implementation concerns expressed in Section 5, another
issue of practical importance is the efﬁciency of identifying patterns matching a given term: to
date, the standard library alone contains just shy of two thousand patterns, and so searching the
known patterns linearly is impractical. Our interpreter constructs a trie-like structure for this
purpose such that the time complexity for uniﬁcation scales as O(m + log n) where m is the
number of patterns which match and n the total number of patterns. Though asymptotically
this is the best complexity possible, there are certainly improvements that could be made to the
uniﬁcation algorithm. In particular, the data structures used are primarily binary trees; hash
tables with ﬁxed lookup time for reasonable values of n would yield signiﬁcant improvements
in distinguishing atoms, and random access arrays would yield signiﬁcant improvements for
distinguishing between composite terms of differing length. Notwithstanding these possible
improvements, the reference interpreter has proven sufﬁciently fast for demonstrating non-trivial
computations in alethe. For example, computing 8! = 40 320 takes only a few seconds despite
the unary representation of natural numbers.

C. Towards a Type System

Type systems are very useful, in that they allow a compiler to statically analyse a program before
it is run and identify whether it is ever possible for a value to be passed to a function that does
not understand such an input. This is extremely powerful, and leads to the Haskell maxim ‘if it
compiles, it runs’: that is, although there may be logical errors, the program should not crash.

We may be tempted to try to start with a simple polymorphic type system, such as Hindley-
Milner, but unfortunately Hindley-Milner is unsuitable for ℵ being that it does not have objects
that can be uniquely assigned a computational role. Consider attempting to type the symbol (cid:3)
from Listing 2, for example, which implements squaring/square rooting for natural numbers.

43

One is tempted to say something like (cid:3) :: N () ↔ () N :: (cid:3), but this does not work. Inspecting
the deﬁnition more closely, we see that (cid:3) appears in a number of contexts: (cid:3) n (), (cid:3) n m (cid:3),
and () m (cid:3). It is the second context where our attempt to type (cid:3) breaks down: not only does
its apparent arity change, but it appears twice in the same expression! Similar problems are
encountered when trying to type other declarative languages, such as Prolog. To proceed, we
shall require that any candidate type system treats terms holistically, not ascribing any special
importance to any particular part of a term. Such a holistic type system will most likely also be
declarative in nature.

A fruitful way of thinking of types of terms is by considering their endpoints—their halting
states. A well-formed term will be able to eventually transition to one or more halting terms,
and the properties of these halting terms are what we are most interested in. For example, we
would like to know that if we construct a term from +, two natural numbers, and (), then we
will be able to extract two natural numbers—rather than, say, a boolean and a string—once
computation ﬁnishes. But, the ‘end’ state is free to wander back to the ‘beginning’, and so it
is more appropriate to consider the type of the term as an unordered pair, consisting of the two
possible halting states. Checking this type will then consist of enumerating which rules may
be encountered along a computational path, verifying that all these rules are consistent with the
initial type and that the other state is reachable in principle. If the ambiguity checker is turned
off, then we see that it is possible to have a term with more than two halting states and so its
type should be a set of types corresponding to each possible halting state. We then notice that
conventional data—such as a natural number—is of the same ilk, save that it only has one halting
state. We call these type-sets isotypes, and their inhabitants isovalues, as each isovalue has one or
more isomorphically equivalent representations.

How might these isotypes look? We begin with conventional data; let us call the isotype of
a natural number NAT, with inhabitants Z and Sn where n inhabits NAT. Let us now attempt to
type addition. As a ﬁrst approximation, we write :: + NAT NAT () = () NAT NAT +; where :: at
the beginning marks this as an isotype. This is in the right spirit, but doesn’t readily admit a
consistent type theory. In particular, how would we represent the isotype of a continuation in the
(cid:48) x), as introduced towards the end of Section 3? Being more careful, we
term WALK β (CHARLIE
can say that an addition isovalue is the unordered pair consisting of + a b () and () c d + where
a, b, c and d are all isovalues of isotype NAT. We need this explication, because the isotypes
of the arguments could be far more complicated. Where it is unambiguous, however, it will be
reasonable to write this isotype as :: + NAT NAT () = () NAT NAT +;. Notice that, as this isotype
is holistic, it is completely orthogonal to the isotype :: + RAT RAT () = () RAT RAT +; where
RAT is the isotype of rationals. This reveals that we get overloading for free in this type system.
It is nevertheless desirable to introduce a notion similar to Haskell’s typeclasses, however, or
else (partial) isomorphisms employing addition, but polymorphic in the isotype they are adding,
will have much too generic an isotype (in particular, the type inference algorithm would not be
able to conclude that an isotype :: + a b () = () c d +; does not exist, and so we would have four
isotype variables rather than the 1 expected). Typeclasses permit the programmer to specify that
an isotype of the form :: + a b () = () c d + is only legal if it is of the more restrictive form
:: + a a () = () a a + for some isotype a. Furthermore, the introduction of typeclasses permits
the abbreviation of complex (ad-hoc) polymorphic isotypes.

44

What, then, might a polymorphic isotype look like? A good example is given by the map

function, which would have as isotype the unordered pair of (MAP f ) x/s () and () y/s (MAP f )
where x/s is an inhabitant of the isotype [a], i.e. the isotype of lists of elements of isotype a, and
where y/s is an inhabitant of [b]. f is required to be some term which, when used to construct the

halting states f x () and () y f , yields the isotype :: f a () = () b f . More concisely, we write this
as

:: [a] (cid:56)
::

MAP f (cid:56) [b]:
a (cid:56)f (cid:56) b.
[] (cid:56)
MAP f (cid:56) [];
[x · x/s ] (cid:56)
x (cid:56)f (cid:56) y.
x/s (cid:56)

MAP f (cid:56) [y · y/s ]:
MAP f (cid:56) y/s .

and we note the similarity of the isotype to the implementation of MAP at the isovalue level. The
composite term MAP f deserves further attention; this is itself halting, and so can be considered
to be an implicitly declared anonymous isotype with one representation.

As brieﬂy mentioned earlier, a type checker (and a type inference algorithm, if it exists) will
perform a reachability analysis—starting from one halting pattern, it will enumerate all the
accessible rules, ﬁnding the most general type consistent with these. The information gleaned
this way is very valuable for all sorts of static analyses. We can determine what other halting
states are accessible, as well as whether there are any missing cases along the way that could
lead to a halting state. We can also resolve the issue of sub-rule ambiguity raised earlier, helping
strengthen the phase space restrictions and avoid unintentional generation of entropy. It may also
be possible to use this information to reﬁne the cost heuristic used for serialisation, improving
runtimes. Furthermore, knowledge of the execution path gives opportunities for identifying code
motifs and performing structural transformations, a necessary ingredient for performing the kind
of code optimisations a compiler is wont to do.

There is, however, a potential hiccup in the reachability analysis that occurs when a continuation-
passing-style is used. In this case, there are rules which are shared between different computations.
For example, in WALK β c ↔ WALK’ α c, c is a continuation. There will be a number of compu-
tations which each pass control to such a WALK term, and later accept control back from such
a WALK’ term. Consequently, no deﬁnitive isotype can be assigned to these intermediate terms.
Moreover, consider these two potential computations:

FOO = WALK ϕ FOO’;

BAR = WALK β BAR’;

It is conceivable that the reachability analyser would, starting from FOO, (correctly) determine
that the pattern WALK β c is reachable, and then (incorrectly) presume the term BAR is reachable
from FOO. If so, the type checker would then (rightly) complain of a type error, perhaps that the
isotypes of FOO’ and BAR’ do not unify. The remedy to the ﬁrst issue, of indeﬁnite isotyping
of intermediate terms, is to only assign deﬁnitive isotypes to sets of halting terms; intermediate

45

terms are assigned deﬁnite isotypes only in the course of inferring isotypes for particular halting
sets, and these intermediate isotypes are appropriately scoped such that the parallel process of
isotype inference may proceed in spite of conﬂicting intermediate term isotypes. The second issue
is remedied by specialising types as early as possible: in order to pass the ambiguity checker,
any computation making use of a shared rule as above must pass control to the shared rule in
a way orthogonal to any other computations passing control. In the above example, this holds
because FOO’ and BAR’ are orthogonal patterns such that no term can be constructed satisfying
them both simultaneously. Therefore, by immediately specialising the type of the continuation c
in WALK, there is no risk of reachability ‘leaking’ into inaccessible rules. One may be concerned
that perhaps the reachability graph is much more complicated, engendering other reachability
leaks, but each such conﬂuence point between distinct computational paths must have this same
explicit orthogonality property in order to satisfy the ambiguity checker, and so there is in fact no
risk. However, this remedy may be somewhat overzealous; suppose that the example above is
part of a larger program,

QUX FALSE = FOO;

QUX TRUE = BAR;

then the eager specialisation of the continuation isotype will yet result in a type-checking error,
similar to Haskell’s (optional) monomorphism restriction. To circumvent such monomorphism,
it would be advantageous to allow the type checker to introduce additional scopes for intermediate
types where such branches occur, providing that these alternate paths eventually converge to a
single type. If this is not possible, then the solution may simply be to expect the programmer
to annotate a ‘partial isotype’ of any shared polymorphic rules in order to give the type checker
sufﬁcient information.

We are optimistic that, in programs without shared rules, it is possible to perform automatic type
inference without any type annotations as in the Hindley-Milner type system. As for programs
with shared rules, we are less certain; nonetheless, the power and generality of type inference
algorithms in type systems as sophisticated as Haskell’s most recent iterations suggests that
it is attainable. We also note that the above is just a sketch of a possible type system, and
will require further reﬁnement and formalisation. Finally, we have neglected as yet to consider
concurrency. Concurrency severely undermines the power of the type system, as there is no longer
any continuity between terms: at any point, a term could be consumed, produced, or merged with
another. It may be possible to extend the notion of isotype to include all the terms that might
interact with one another, but it is unclear how useful this would be. Alternatively, one could
simply ascribe types to single halting patterns, instead of seeking isotypes and an enumeration of
all possible isomorphic representations of an isovalue. Again, this severely weakens the utility
of the type system. A desirable compromise would be to identify the non-concurrent parts of a
program, and apply type checking and inference to just these parts; a more rudimentary level of
type checking could then be applied to the concurrent parts to at least verify the consistency of
any non-concurrent sub-rules employed.

46

D. The Σ Calculus

An earlier prototype, Σ was more functional in nature. Its terms were nested applications of
‘general permutations’, which could be named for convenience (and for achieving recursion
without a ﬁxed point combinator). Its deﬁnition was

(TERM)

τ ::= (cid:104)πτ ∗ : τ ∗π(cid:105) | VAR | REF | τ ∗

where REF is a reference to a named pattern. A generalised permutation is given by (cid:104)πτ ∗ : τ ∗π(cid:105),
where π is a special variable that matches the permutation itself. That is, the term ((cid:104)π x y :
y x π(cid:105) 1 2) would transition to (2 1 (cid:104)π x y : y x π(cid:105)). Notice that this is the convention taken
in ℵ, that the atom or term in the ﬁrst position tends to take the ‘active’ role, and tends to place
itself at the end of the term after rule execution. In Σ, however, this convention is enshrined in
the language itself because all the information concerning the transition rules is held within the
permutation terms themselves, and so it is required that there be two special locations within a
term corresponding to the current and previous permutation. The ﬁrst position is used for the
current permutation in analogy to the λ calculus, and the last position is used for the previous
permutation for symmetry. Permutations are ‘general’ in the sense that they can alter tree structure,
and can copy/elide variables.

The Σ calculus can also be proven to be microscopically reversible and Reverse-Turing
complete, but the absence of sub-rules renders composition more unwieldy. Additionally, dis-
criminating different cases is tricky. As with the λ calculus, where data can be represented
with functions via Church encoding, we can represent data in Σ with permutations and these
permutations will perform case statements. Taking lists as an example, we have

CONS ≡ (cid:104)π{nc}zf : c{nf }zπ(cid:105)

NIL ≡ (cid:104)π{nc}zf : n{f c}zπ(cid:105)

where {xyz} is sugar for (⊥xyz(cid:62)) used to represent an inert expression ((cid:62) ≡ ⊥ ≡ ε ≡ (), not
being a permutation, is an inert object that does nothing and is used to represent halting states in
Σ). Using these deﬁnitions, list reversal can be implemented thus,

REV ≡ (cid:104)πl(cid:62) : NIL{λν}{{εε}l}π(cid:105)

λ ≡ (cid:104)π{REV ν}{{r(cid:48)r(cid:48)(cid:48)}{˜ll(cid:48)l(cid:48)(cid:48)}}˜r : ˜l{REV
ν ≡ (cid:104)π{REV
(cid:48) ≡ (cid:104)π{λν}{r{εε}}NIL : ⊥rπ(cid:105)

REV

(cid:48) λ}{r{l(cid:48)l(cid:48)(cid:48)}}CONS : CONS{REV λ}{{l(cid:48)r}l(cid:48)(cid:48)}π(cid:105)

(cid:48) ν}{{˜rr(cid:48)r(cid:48)(cid:48)}{l(cid:48)l(cid:48)(cid:48)}}π(cid:105)

which is certainly not the most edifying program in the world! It would be used as so:

(REV[1 4 6 2](cid:62))

∗←→ (⊥[2 6 4 1]REV

(cid:48))

µ-Recursive Functions To give a deeper ﬂavour of Σ, we implement the µ-recursive func-
tions (recall their deﬁnition and alethe implementation from Listing 9). In order to simulate a no-
tion of function composition, we ﬁrst adopt a ‘Σ-function’ motif: a function is represented by the

47

triple F = {f z f (cid:48)} where f and f (cid:48) are the initial and terminal permutations, and z is any data we
∗←→ (⊥ y g f z f (cid:48)),
wish to bind to F . The permutations f and f (cid:48) must be such that (f z f (cid:48) x (cid:62))
where x is the input, y the output and g any garbage data.

The base µ-recursive functions can then be implemented thus:

Z ≡ {zεz}

S ≡ {sεs}
i επk
i ≡ {πk
Πk
i }

z ≡ (cid:104)zεz n (cid:62) : ⊥ 0 n zεz(cid:105)

s ≡ (cid:104)sεs n (cid:62) : ⊥ {SUCCn} ε sεs(cid:105)
i ≡ (cid:104)πk
πk

i {n1, . . . , nk} (cid:62) :

i επk
⊥ ni {n1, . . . , ni−1, ni+1, . . . , nk} πk

i επk
i (cid:105)

Note that we have written (cid:104)zεz · · · instead of (cid:104)πεz · · · or (cid:104)πεπ · · · for clarity.

The composition operator for F of arity k over functions Gi is given by {ck{F (cid:126)G}c(cid:48)

k}, where

ck ≡ (cid:104)ck{F (cid:126)G}c(cid:48)
c(cid:48)(cid:48)
k ≡ (cid:104)c(cid:48)(cid:48)
k ≡ (cid:104)c(cid:48)
c(cid:48)
k

k (cid:126)n (cid:62) : c(cid:48)(cid:48)

k F (G1 ∧ (cid:126)n) · · · (Gk ∧ (cid:126)n) ck(cid:105)

k F (m1 h1 ∨ G1) · · · (mk hk ∨ Gk) ck : c(cid:48)
k
(cid:126)h} ck{F (cid:126)G}c(cid:48)
(cid:126)h (cid:126)G (y hf ∨ F ) c(cid:48)(cid:48)
k(cid:105)

k : ⊥ y {hf

(cid:126)h (cid:126)G (F ∧ (cid:126)m) c(cid:48)(cid:48)
k(cid:105)

where we have introduced sugared forms (F ∧ x) ≡ (f zf (cid:48) x (cid:62)) and (y h ∨ F ) ≡ (⊥ y h f zf (cid:48)).
The primitive recursion operator is a little more complicated. To understand the implementation
below, it is important to know that the number 0 is represented as {ZERO ε} and the number
m + 1 is given by {SUCC m}. When the form is unknown, we can write a uniﬁed representation
as { ˜mm} where ˜m is the constructor and m is either ε or the predecessor. Furthermore, the
constructors are deﬁned as follows:

ZERO ≡ (cid:104)π{pq}zr : p{rq}zπ(cid:105)

SUCC ≡ (cid:104)π{pq}zr : q{pr}zπ(cid:105)

We write the primitive recursion of F and G (arities k and k + 2) as RF G = {ρk{F G}ρ(cid:48)
deﬁned by:

k},

ρk = (cid:104)ρk{F G}ρ(cid:48)
k σ(cid:48)(cid:48)
k = (cid:104)ρ(cid:48)
ρ(cid:48)

k {ζ (cid:48)

k {(cid:126)n{ ˜mm}} (cid:62) : ˜m {ζk σk} {F G(cid:126)nm} ρk(cid:105)
k} {yhF G} ˜m : ⊥ y { ˜mh} ρk{F G}ρ(cid:48)
k(cid:105)

ζk = (cid:104)ζk {ρk σk} {F G(cid:126)nε} ZERO : ζ (cid:48)
k (y h ∨ F ) G ζk : ZERO {ρ(cid:48)
k = (cid:104)ζ (cid:48)
ζ (cid:48)

k (F ∧ (cid:126)n) G ζk(cid:105)
k} {yhF G} ζ (cid:48)
k σ(cid:48)(cid:48)
k(cid:105)

σk = (cid:104)σk {ζk ρk} {F G(cid:126)nm} SUCC : σ(cid:48)
k = (cid:104)σ(cid:48)
σ(cid:48)
k = (cid:104)σ(cid:48)(cid:48)
σ(cid:48)(cid:48)

k (y h ∨ RF G) (cid:126)nm σk : σ(cid:48)(cid:48)
k (z h(cid:48) ∨ G) F h σ(cid:48)

k : SUCC {ζ (cid:48)

k (RF G ∧ (cid:126)nm) (cid:126)nm σk(cid:105)

k (G ∧ (cid:126)nmy) F h σ(cid:48)
k(cid:105)

k ρ(cid:48)

k} {z{hh(cid:48)}F G} σ(cid:48)(cid:48)
k(cid:105)

In the above, the ρ permutation switches between the ζ and σ permutations depending on if
{ ˜mm} = 0 or > 0; the ζ route computes y = F ((cid:126)n), whilst the σ route recurses down to compute

48

y = RF G((cid:126)n, m) and then z = G((cid:126)n, m, y). Both these routes then put their results into a common
representation and merge the control ﬂow via ˜m.

Finally, we implement the minimalisation operator as M m

k}; M m
F is a gener-
alisation of the more canonical minimalisation operator (which is equivalent to M 0
F ),

F = {µk{F m}µ(cid:48)

M m

F ((cid:126)n) = min{(cid:96) ≥ m : f ((cid:126)n; (cid:96)) = 0}

Interestingly, this is much simpler to implement than primitive recursion—only requiring four
permutations—yet is the necessary ingredient for universality:

k m (F ∧ (cid:126)nm) (M {SUCC m}
kµ(cid:48)(cid:48)(cid:48)

∧ (cid:126)n) µk(cid:105)
k } m ({˜yy}h ∨ F ) q µ(cid:48)(cid:48)
k(cid:105)

F

µk = (cid:104)µk{F m}µ(cid:48)
µ(cid:48)(cid:48)
k = (cid:104)µ(cid:48)(cid:48)
k = (cid:104)µ(cid:48)(cid:48)(cid:48)
µ(cid:48)(cid:48)(cid:48)

k (cid:126)n (cid:62) : µ(cid:48)(cid:48)
k m ({˜yy}h ∨ F ) q µk : ˜y {µ(cid:48)
k } m p (m(cid:48)h ∨ M {SUCC m}
kµ(cid:48)(cid:48)(cid:48)
k {µ(cid:48)
k} m(cid:48) p (m(cid:48)h ∨ M {SUCC m}
kµ(cid:48)
SUCC {µ(cid:48)(cid:48)
k } m(cid:48) (F ∧ (cid:126)nm) (M {SUCC m}
kµ(cid:48)(cid:48)(cid:48)
k {µ(cid:48)(cid:48)

k = (cid:104)µ(cid:48)
µ(cid:48)

F

F

F

) SUCC :

) µ(cid:48)(cid:48)(cid:48)
k (cid:105)
∧ (cid:126)n) ˜µ : ⊥ m(cid:48) {˜µ(cid:126)n} µk{F m}µ(cid:48)
k(cid:105)

F

In the above, we compute F ((cid:126)n; m) and check if it’s 0, if so we return m as m(cid:48), otherwise we
compute M m+1
((cid:126)n) in a recursive manner and return its result as m(cid:48). Clearly, if there is no
m such that F ((cid:126)n; m) = 0 then M m
F ((cid:126)n) = ⊥ ∀m. For concision, we compute both functions
simultaneously, making use of Σ’s parallelism to approximate laziness. We then conveniently
reverse both computations in a Bennett-like manner to produce only a single bit of garbage (the
history data we return is ˜µ, representing whether or not m = m(cid:48), and the input vector (cid:126)n).

The Interpreter Whilst the implementation of the µ-recursive functions demonstrates that
realising meaningful programs in Σ is certainly possible, it is inelegant and clunky. Rather
than programming in Σ feeling like the λ calculus, as was intended, it feels much more like
programming in assembly. Nevertheless, we release the source code of the interpreter11, example
code12, and syntax highlighters13 for completeness.

11https://github.com/hannah-earley/sigma-repl
12https://github.com/hannah-earley/sigma-examples
13https://github.com/hannah-earley/sigma-syntax

49

References

[1]

Leo Szilard. ‘ ¨Uber die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter
Wesen’. In: Zeitschrift f¨ur Physik 53.11-12 (1929), pp. 840–856. English translation: Leo Szilard. ‘On the
decrease of entropy in a thermodynamic system by the intervention of intelligent beings’. In: Behavioral
Science 9.4 (1964), pp. 301–310.

[2] Rolf Landauer. ‘Irreversibility and heat generation in the computing process’. In: IBM J. Res. Dev. 5.3 (1961),

pp. 183–191.

[3] Alan Mathison Turing. ‘On computable numbers, with an application to the Entscheidungsproblem’. In: J. of

Math 58.345-363 (1936), p. 5.

[4] Alonzo Church. ‘A set of postulates for the foundation of logic’. In: Annals of mathematics (1932), pp. 346–

366.

[5] Cortex-A78 – Microarchitectures – ARM – WikiChip. https://en.wikichip.org/wiki/arm_holdings/

microarchitectures/cortex-a78. (Visited on 15/11/2020).

[6] Charles H Bennett. ‘Logical Reversibility of Computation’. In: IBM J. Res. Dev. 17.6 (Nov. 1973), pp. 525–

532.

[7] Charles H Bennett. ‘Time/space trade-offs for reversible computation’. In: SIAM Journal on Computing 18.4

(1989), pp. 766–776.

[8] Edward Fredkin and Tommaso Toffoli. ‘Conservative Logic’. en. In: Collision-Based Computing. Ed. by

Andrew Adamatzky. Springer London, 1981, pp. 47–81.

[9] Andrew Lewis Ressler. ‘The design of a conservative logic computer and a graphical editor simulator’.

PhD thesis. Massachusetts Institute of Technology, 1981.

[10] Charles H Bennett. ‘The thermodynamics of computation—a review’. en. In: Int. J. Theor. Phys. 21.12 (Dec.

1982), pp. 905–940.

[11] Michael Patrick Frank. ‘Reversibility for efﬁcient computing’. PhD thesis. Massachusetts Institute of Techno-

logy, Dept. of Electrical Engineering and Computer Science, 1999.

[12] Hannah Earley. ‘Engines of Parsimony: Part I. Limits on Computational Rates in Physical Systems’. In: (2020).

arXiv: 2007.03605 [cond-mat.stat-mech].

[13] Hannah Earley. ‘Engines of Parsimony: Part II. Performance Trade-offs for Communicating Reversible

Computers’. In: (2020). arXiv: 2011.04054 [cond-mat.stat-mech].

[14] Hannah Earley. ‘Engines of Parsimony: Part III. Performance Trade-offs for Reversible Computers Sharing

Resources’. 2020. arXiv: 2012.05655 [cond-mat.stat-mech].

[15] Carlin James Vieri. ‘Pendulum–a reversible computer architecture’. MA thesis. Massachusetts Institute of

Technology, 1995.

[16] Christopher Lutz and Howard Derby. ‘Janus: a time-reversible language’. In: Caltech class project (1982).

[17] Henry G Baker. ‘NREVERSAL of fortune—the thermodynamics of garbage collection’. In: Memory manage-

ment. Springer, 1992, pp. 507–524.

[18] Michael P Frank. The R programming language and compiler. 1997.

[19] Ben Rudiak-Gould. Kayak. http://esoteric.sange.fi/essie2/download/kayak/kayak.html.

2002. (Visited on 05/04/2017).

[20] R P James and A Sabry. Theseus: a high level language for reversible computing, work-in-progress report at

RC (2014). 2014.

[21]

Jacques Carette, Roshan P James and Amr Sabry. ‘Embracing the laws of physics: Three reversible models of
computation’. In: (2018). arXiv: 1811.03678 [cs.PL].

[22] Georg Seelig et al. ‘Enzyme-free nucleic acid logic circuits’. en. In: Science 314.5805 (Dec. 2006), pp. 1585–

1588. ISSN: 0036-8075, 1095-9203.

[23] Conor McBride. ‘The derivative of a regular type is its type of one-hole contexts’. In: Unpublished manuscript

(2001), pp. 74–88.

50

S Supplementary Material

(cid:56)

(cid:56)

(cid:56)

(cid:56)

(cid:56)

Z;

LENGTH [](cid:56)
LENGTH [x · x/s ](cid:56) (S(cid:96)):
LENGTH x/s (cid:56) (cid:96).

(cid:56)

(cid:56)

Z;

SUM [](cid:56)
SUM [Z · n/s ](cid:56) σ:
SUM n/s (cid:56) σ.
SUM [(Sn) · n/s ](cid:56) (Sσ):
SUM [n · n/s ](cid:56) σ.

(cid:56)

[] (cid:56)
MAP f (cid:56) [];
[x · x/s ] (cid:56)
x (cid:56)f (cid:56) y.
x/s (cid:56)

MAP f (cid:56) [y · y/s ];
MAP f (cid:56) y/s .

(cid:56)

(cid:56)

MAP f [](cid:56) [];
MAP f [x · x/s ](cid:56) [y · y/s ];
(cid:56)f x(cid:56) y.
MAP f x/s (cid:56) y/s .
(cid:56)

(cid:56) y/s [Z · (cid:96)/s ]:
(cid:56) y/s (cid:96)/s .

CONCAT

CONCAT

CONCAT

(cid:56) [] [];
CONCAT

[] (cid:56)
[[] · x/ss ] (cid:56)
x/ss (cid:56)
[[x · x/s ] · x/ss ] (cid:56)
[x/s · x/ss ] (cid:56)
CONCATMAP f (cid:56) y/s (cid:96)/s :
MAP f (cid:56) y/ss .
x/s (cid:56)
(cid:56) y/s (cid:96)/s .
y/ss (cid:56)

CONCAT

CONCAT

x/s (cid:56)

(cid:56) x · y/s [(S(cid:96)) · (cid:96)/s ]:
(cid:56) y/s [(cid:96) · (cid:96)/s ].

data TREE x t/s ;

data , a b;

n (cid:56)

TREESIZE’ (TREE x t/s )(cid:56) (Sn(cid:48)):
n (cid:56)∼GO t/s (cid:56) n(cid:48).
n (cid:56)∼GO [](cid:56) n;
n (cid:56)∼GO [t · t/s ](cid:56) n(cid:48)(cid:48):
n (cid:56)
TREESIZE’ t(cid:56) n(cid:48).
n(cid:48) (cid:56)∼∼GO t/s (cid:56) n(cid:48)(cid:48).

(cid:56)

TREESIZE t(cid:56) n:
(cid:56)

TREESIZE’ t(cid:56) n.

Z

(cid:56)

POLISH

(TREE x t/s ) (cid:56)
LENGTH x/s (cid:56) n.
t/s (cid:56)
POLISHREADS n(cid:56) t/s (cid:96)/s .
p (cid:56)

(cid:56) [(, x n) · p]:
(cid:56) p (cid:96)/s .

CONCATMAP POLISH

(cid:56) x/s (cid:48) (TREE x t/s ) (S(cid:96)):

(cid:56)

(cid:56)

(cid:56)

POLISHREAD

[(, x n) · x/s ] (cid:56)
x/s (cid:56)
POLISHREADS n(cid:56) x/s (cid:48) t/s (cid:96)/s .
LENGTH (cid:96)/s (cid:56) n.
SUM (cid:96)/s (cid:56) (cid:96).
MAP TREESIZE t/s (cid:56) (cid:96)/s .
(cid:56) x/s [] [];
POLISHREADS (Sn)(cid:56) x/s (cid:48)(cid:48) [t · t/s ] [(cid:96) · (cid:96)/s ]:
(cid:56) x/s (cid:48) t (cid:96).
x/s (cid:56)
POLISHREADS n(cid:56) x/s (cid:48) t/s (cid:96)/s .
x/s (cid:48) (cid:56)

POLISHREADS Z

POLISHREAD

x/s (cid:56)
x/s (cid:56)

Listing S.1: The full alethe program implementing interconversion between trees and Polish
notation, following the haskell snippet in Listing 10a. In fact, a little thought and
optimisation will reveal that the (cid:96) outputs of the read functions can be eliminated,
which then simpliﬁes the deﬁnition of POLISH to that given in Listing 10d and
obviates the need for the auxiliary functions LENGTH, SUM, MAP, etc. Nevertheless,
the point of this is to show that it is not unreasonable to arrive at the program listed
here, and it may not be entirely obvious that a simpler implementation with less
garbage is possible.

51

