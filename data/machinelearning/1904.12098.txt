9
1
0
2

r
p
A
7
2

]
E
S
.
s
c
[

1
v
8
9
0
2
1
.
4
0
9
1
:
v
i
X
r
a

Enabling Open-World Specification Mining via
Unsupervised Learning

JORDAN HENKEL, University of Wisconsin–Madison, USA
SHUVENDU K. LAHIRI, Microsoft Research, USA
BEN LIBLIT, University of Wisconsin–Madison, USA
THOMAS REPS, Univ. of Wisconsin–Madison and GrammaTech, Inc., USA

Many programming tasks require using both domain-specific code and well-established patterns (such as
routines concerned with file IO). Together, several small patterns combine to create complex interactions. This
compounding effect, mixed with domain-specific idiosyncrasies, creates a challenging environment for fully
automatic specification inference. Mining specifications in this environment, without the aid of rule templates,
user-directed feedback, or predefined API surfaces, is a major challenge. We call this challenge Open-World
Specification Mining.

In this paper, we present a framework for mining specifications and usage patterns in an Open-World
setting. We design this framework to be miner-agnostic and instead focus on disentangling complex and
noisy API interactions. To evaluate our framework, we introduce a benchmark of 71 clusters extracted from
five open-source projects. Using this dataset, we show that interesting clusters can be recovered, in a fully
automatic way, by leveraging unsupervised learning in the form of word embeddings. Once clusters have
been recovered, the challenge of Open-World Specification Mining is simplified and any trace-based mining
technique can be applied. In addition, we provide a comprehensive evaluation of three word-vector learners
to showcase the value of sub-word information for embeddings learned in the software-engineering domain.

1 INTRODUCTION

The continued growth of software in size, scale, scope, and complexity has created an increased
need for code reuse and encapsulation. To address this need, a growing number of frameworks
and libraries are being authored. These frameworks and libraries make functionality available
to downstream users through Application Programming Interfaces (APIs). Although some APIs
may be simple, many APIs offer a large range of operations over complex structures (such as the
orchestration and management of hardware interfaces).

Staying within correct usage patterns can require domain-specific knowledge about the API and
its idiosyncratic behaviors [Robillard and DeLine 2011]. This burden is often worsened by insuffi-
cient documentation and explanatory materials for a given API. In an effort to assist developers
utilizing these complex APIs, the research community has explored a wide variety of techniques to
automatically infer API properties [Lo et al. 2011; Robillard et al. 2013].

This paper contributes to the study of API-usage mining by identifying a new problem area
and exploring the combination of machine learning and traditional methodologies to address the
novel challenges that arise in this new domain. Specifically, we introduce the problem domain of
Open-World Specification Mining. The goal of Open-World Specification Mining can be stated as
follows:

Given noisy traces, from a mixed vocabulary, automatically identify and mine patterns or
specifications without the aid of (i) implicit or explicit groupings of terms, (ii) pre-defined
pattern templates, or (iii) user-directed feedback or intervention.

Authors’ addresses: Jordan Henkel, University of Wisconsin–Madison, USA, jjhenkel@cs.wisc.edu; Shuvendu K. Lahiri,
Microsoft Research, USA, Shuvendu.Lahiri@microsoft.com; Ben Liblit, University of Wisconsin–Madison, USA, liblit@cs.
wisc.edu; Thomas Reps, Univ. of Wisconsin–Madison and GrammaTech, Inc., USA, reps@cs.wisc.edu.

1

 
 
 
 
 
 
Open-World Specification Mining is motivated by the lack of adoption of specification-mining
tools outside of the research community. We believe that because Open-World Specification Mining
needs no user-supplied input, it will lead to tools that are easier to transition and apply in industry
settings. Although this setting reduces the burden imposed on users, it increases the challenges
associated with extracting patterns. We address these challenges with a toolchain, called ml4spec:

• We base our technique on a form of intraprocedural, parametric, lightweight symbolic
execution introduced by Henkel et al. [2018]. Using their tool gives us the ability to generate
abstracted symbolic traces and avoids any need for dynamically running the program.

• To address the lack of implicit or explicit groupings of terms (a challenged imposed by the
setting of Open-World Specification Mining) we introduce a technique, Domain-Adapted
Clustering (DAC), that is capable of recovering groupings of related terms.

• Finally, we remove the need for pre-defined pattern templates by mining specifications
using traditional, unrestricted, methods (such as k-Tails [Biermann and Feldman 1972] and
Hidden Markov Models [Seymore et al. 1999]). We are able to use these traditional methods
by leveraging Domain-Adapted Clustering to “focus” these traditional methods toward
interesting patterns.

The combination of both traditional techniques and machine-learning-assisted methods in the
pursuit of Open-World Specification Mining raises a number of natural research questions that we
consider.

First, we explore the ability of Domain-Adapted Clustering, our key technique, to successfully

extract informative and useful clusters of API methods in our Open-World setting:

Research Question 1: Can we effectively mine useful and clean clusters of API methods in an
Open-World setting?

Immediately, we run into the difficulty of judging the utility of clusters extracted from traces.
To provide the basis for a consistent and quantitative evaluation, we have manually extracted a
dataset of ground-truth clusters from five popular open-source projects written in C.

Next, we compare Domain-Adapted Clustering against several other baselines that do not utilize

the implicit structure of the extracted traces:

Research Question 2: How does Domain-Adapted Clustering (DAC) compare to off-the-shelf
clustering techniques?

We also explore how two key choices in our toolchain impact the overall utility of our results:

Research Question 3: How does the choice of word-vector learner and the choice of sampling
technique affect the resulting clusters?

Understanding how different pieces of our toolchain interact provides the ground work for
understanding how traditional metrics (co-occurrence statistics) interplay with our machine-
learning-assisted additions (word embeddings). To quantify the usefulness of unsupervised learning
in our approach, and to validate our central hypothesis, we ask:

Research Question 4: Is there a benefit from using a combination of co-occurrence statistics
and word embeddings?

2

Finally, we can explore how faithful we are to one of the key tenets of Open-World mining: the
lack of user intervention. To do so, we must understand what level of hyper-parameter tuning is
required to achieve reasonable results:

Research Question 5: Does our toolchain transfer to unseen projects with minimal reconfigu-
ration?

The contributions of our work can be summarized as follows:

• We define the new problem domain of Open-World Specification Mining. Our motivation
is to increase the adoption of specification-mining techniques by reducing the burden imposed
on users (at the cost of a more challenging mining task).

• We create a toolchain based on the key insight that unsupervised learning (specifically
word embeddings) can be combined with traditional metrics to enable automated mining in
an Open-World setting.

• We introduce a benchmark of 71 ground-truth clusters extracted from five open-source C

projects.

• We report on several experiments:

– In §7.2, we use our toolchain to recover, on average, two thirds of the ground-truth clusters

in our benchmark automatically.

– In §7.3, we compare our Domain-Adapted Clustering technique to three off-the-shelf
clustering algorithms; Domain-Adapted Clustering provides, on average, a 30% performance
increase relative to the best baseline.

– In §7.4, we confirm our intuition that sub-word information improves the quality of learned
vectors in the software-engineering domain; we also confirm that our Diversity Sampling
(§4) technique increases performance by solving the problem of prefix dominance.

– In §7.5, we quantify the impacts of our machine-learning-assisted approach.
– In §7.6, we find that just two configurations can be automatically tested to achieve perfor-

mance that is within 10% of the best configuration.

Organization. The remainder of the paper is organized as follows: §2 provides an overview
of the ml4spec toolchain. §3 reviews Parametric Lightweight Symbolic Execution. §4 describes
Diversity Sampling. §5 introduces Domain-Adapted Clustering. §6 describes trace projection and
mining. §7 provides an overview of our evaluation methodology. §7.2-§7.6 address our five re-
search questions. §8 considers threats to the validity of our approach. §9 discusses related work.
§10 concludes.

2 OVERVIEW
The ml4spec toolchain consists of five phases: Parametric Lightweight Symbolic Execution, thresh-
olding and sampling, unsupervised learning, Domain-Adapted Clustering, and mining. As input,
ml4spec expect a corpus of buildable C projects. As output, ml4spec produces clusters of related
terms and finite-state automata (or Hidden Markov Models) mined via traditional techniques.1 A
visualization of the way data flows through the ml4spec toolchain is given in Fig. 1. We illustrate
this process as applied to the example in Fig. 2.
Phase I: Parametric Lightweight Symbolic Execution. The first phase of the ml4spec toolchain
applies Parametric Lightweight Symbolic Execution (PLSE). PLSE takes, as input, a corpus of buildable

1Although we provide examples based on traditional miners that produce finite-state automata and Hidden Markov Models,
we do note that the ml4spec toolchain is miner-agnostic. By using ml4spec as a trace pre-processor, any trace-based miner
can be adapted to the Open-World mining setting.

3

Input Programs

Traces

Thresholded Traces

Sampled Traces

Phase I: Parametric Lightweight
Symbolic Execution

Phase II: Thresholding
and Sampling

Word Embeddings

Phase III: Unsupervised
Learning

Co-occurrence
Matrix (A)

Word Embedding
Matrix (B)

Combined Matrix:
αA + (1 − α)B

Phase IV: Domain-Adapted
Clustering

Clusters

Projected Traces

FSAs

HMMs

Phase V: Mining

Fig. 1. Overview of the ml4spec toolchain

C projects and a set of abstractions to apply. For our use case, we abstract calls, checks on the results
of calls, and return values. §3 describes these abstractions in more detail. Figure 2 presents both an
example procedure and a trace resulting from the application of PLSE. Already, examining Fig. 2b,
we can see one of the core challenges of Open-World mining: the mixed vocabulary present in
the trace from Fig. 2b involves many interesting behaviors but, without user input [Ammons
et al. 2002; Lo and Khoo 2006], pre-defined rule templates [Yun et al. 2016], or some pre-described

4

void example () {
void *A ,* B;
if (! strcasecmp ()) {
addReplyHelp ();

} else if (! strcasecmp ()) {
A = dictGetIterator ();
log ();
while (( B = dictNext (A )) != 0) {

dictGetKey (B );
if ( strmatchlen ( sdslen ())) {

addReplyBulk ();

}

}
dictReleaseIterator (A );
} else if (! strcasecmp ()) {

addReplyLongLong ( listLength ());

} else {

addReplySubcommandSyntaxError ();

}

}

(a) Sample procedure, showcasing an iter-
ator usage pattern from the Redis open-
source project

$START
strcasecmp
strcasecmp != 0
strcasecmp
strcasecmp == 0
dictGetIterator
log
dictGetIterator → dictNext
dictNext
dictNext != 0
dictNext → dictGetKey
dictGetKey
sdslen
sdslen → strmatchlen
strmatchlen
strmatchlen != 0
addReplyBulk
dictGetIterator → dictNext
dictNext
dictNext == 0
dictGetIterator

→ dictReleaseIterator

dictReleaseIterator
$END

(b) One example trace, taken from the
set of traces our Parametric Lightweight
Symbolic Executor generates for the ex-
ample procedure in Fig. 2a

Fig. 2. Example procedure and corresponding trace. The notation A → B signifies
that the result of call A is used as a parameter to call B.

notion of what methods are related [Le and Lo 2018], we have no straightforward route to sepa-
rating patterns from noise. We need to disentangle these disparate behaviors to facilitate better
specification mining.

Phase II: Thresholding and Sampling. Although our example procedure has a small number
of paths from entry to exit, many procedures have thousands of possible paths. Learning from
these traces can be challenging due to the number of times the same trace prefix is seen. This
problem, which Henkel et al. [2018] term prefix dominance, makes downstream learning tasks more
challenging. For instance, some terms that occur in multiple traces (e.g., in a common prefix) may
occur only a single time in the source program. Off-the-shelf word-vector learners cannot filter for
these kinds of rare words because they have no concept of the implicit hierarchy between traces and
the procedures they were extracted from. The ml4spec toolchain introduces two novel techniques
to address these challenges: Diversity Sampling and Hierarchical Thresholding. Diversity Sampling
attempts to recover a fixed number of highly representative traces via a metric-guided sampling

5

strcasecmp
strcasecmp != 0
strcasecmp == 0

(a) Cluster 1

dictGetIterator
dictGetIterator
→ dictNext

dictNext
dictNext == 0
dictNext != 0
dictGetIterator

sdslen

→ strmatchlen

strmatchlen
strmatchlen != 0

→ dictReleaseIterator

dictReleaseIterator

(c) Cluster 3

(b) Cluster 2

Fig. 3. Clusters generated via Domain-Adapted Clustering (DAC)

process. Hierarchical Sampling leverages the implicit hierarchy between procedures and traces to
remove rare words. Together, these techniques improve the quality of downstream results.

Phase III: Unsupervised Learning. Traditionally, specification and usage mining techniques
would define some method of measuring support or confidence in a candidate pattern. Often, these
measurements would be based on co-occurrences of terms (or sets of terms). The ml4spec toolchain
leverages a key insight: traditional co-occurrence statistics and machine-learning-assisted metrics
(extracted via unsupervised learning, specifically word embeddings) can be combined in fruitful
ways. Referencing our example in Fig. 2, we might hypothesize, based on co-occurrence, that
dictGetIterator and log are related. For the sake of argument, imagine that in each extracted
trace we find this same pattern. How can we refine our understanding of the relationship between
dictGetIterator and log?

It is in these situations that adding unsupervised learning improves the results. A word-vector
learner, such as Facebook’s fastText [Bojanowski et al. 2017], can provide us with a measurement
of the similarity between dictGetIterator and log. This measurement provides a contrast to the
co-occurrence based view of our data. Intuitively, word-vector learners utilize the Distributional
Hypothesis: similar words appear in similar contexts [Harris 1954]. The global context, captured by
co-occurrence statistics, can be supplemented and refined by the local-context information that
word-vector learners naturally encode. §7.5 explores the impact and relative importance of both
traditional co-occurrence statistics and machine-learning-assisted metrics.

Phase IV: Domain-Adapted Clustering. The trace given in Fig. 2b exhibits several different
patterns. The difficulty in mining from static traces like the one in Fig. 2b comes from the need
to learn a separation of the various, possibly interacting, patterns and behaviors. To address this
challenge, we introduce Domain-Adapted Clustering: a generalizable approach to clustering corpora
of sequential data. Domain-Adapted Clustering leverages the insight that it can be useful to combine
machine-learning-assisted metrics with co-occurrence statistics captured directly from the target
corpus. Using Domain-Adapted Clustering, we can extract the clusters shown in Fig. 3. These
clusters allow us to solve the problem of disentanglement by projecting the trace in Fig. 2b into the
vocabularies defined by each cluster. It is this “focusing” of the mining process that enables the
ml4spec toolchain to apply traditional specification-mining techniques in an Open-World setting.

Phase V: Mining. Finally, we can extract free-form specifications by applying traditional mining
techniques to the projected traces that ml4spec creates. One powerful aspect of the ml4spec

6

dictNext != 0

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

t
x
e
N
t
c
i
d

→

t
x
e
N
t
c
i
d

0

=
=

t
x
e
N
t
c
i
d

T
R
A
T
S
$

r
o
t
a
r
e
t
I
e
s
a
e
l
e
R
t
c
i
d

→

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

r
o
t
a
r
e
t
I
e
s
a
e
l
e
R
t
c
i
d

D
N
E
$

dictNext

dictNext == 0

Fig. 4. Example FSA that was mined by projecting all of the traces extracted
from Fig. 2a into the vocabulary defined by Fig. 3b. FSAs for the vocabularies
defined by the clusters in Figs. 3a and 3c are also generated, but not shown here.

1.0

T
R
A
T
S
$

1.0

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

t
x
e
N
t
c
i
d

→

t
x
e
N
t
c
i
d

0

=
!

1 . 0

1.0

0.4

t
x
e
N
t
c
i
d

0

.

6

1.0

t
x
e
N
t
c
i
d

0

=
=

r
o
t
a
r
e
t
I
t
e
G
t
c
i
d

r
o
t
a
r
e
t
I
e
s
a
e
l
e
R
t
c
i
d

→

1.0

r
o
t
a
r
e
t
I
e
s
a
e
l
e
R
t
c
i
d

1.0

D
N
E
$

Fig. 5. Example HMM that was mined by projecting all of the traces extracted
from Fig. 2a into the vocabulary defined by Fig. 3b

toolchain is its disassociation from any particular mining strategy. The real challenge of Open-
World Specification Mining is extracting, without user-directed feedback, reasonable clusters of
possibly related terms. With this information in hand, a myriad of trace-based miners can be applied.
Figures 4 and 5 highlight this ability by showing both a finite-state automaton (FSA) mined via the
classic k-Tails algorithm and a Hidden Markov Model (HMM) learned directly from the projected
traces [Biermann and Feldman 1972; Seymore et al. 1999].

7

3 PARAMETRIC LIGHTWEIGHT SYMBOLIC EXECUTION
The first phase of the ml4spec toolchain generates intraprocedural traces using a form of parametric
lightweight symbolic execution, introduced by Henkel et al. [2018]. Parametric Lightweight Sym-
bolic Execution (PLSE) takes, as input, a buildable C project and a set of abstractions. Abstractions
are used to parameterize the resulting traces. In our setting, the abstractions allow us to enrich the
output vocabulary. This enrichment enables the final phase of the ml4spec toolchain (mining) to
extract specifications that include each of the following types of information:

• Temporal properties: the ml4spec toolchain abstracts the sequence of calls encountered
on a given path of execution. The temporal ordering of these calls is preserved in the output
traces.

• Call-return constraints: often a sequence of API calls can only continue if previous calls
succeeded. In C APIs checking for success involves examining the return value of calls.
ml4spec abstracts simple checks over return values to capture specifications that involve
call-return checks.

• Dataflow properties: some specification miners are parametric—these miners can capture
relationships between parameters to calls and call-returns. To highlight the flexibility that
PLSE provides, we include an abstraction that tracks which call results are used, as parameters,
in future calls. This call-to-call dataflow occurs in many API usage patterns.

• Result propagation: the return value of a given procedure can encode valuable information.
Some procedures act as wrappers around lower-level APIs, while other procedures may
forward error results from failing calls. In either case, forwarding the result of a call, for any
purpose, is abstracted into our traces to aid in downstream specification mining. ml4spec
also abstracts constant return values: returning a constant may indicate success or failure,
and such information may aid in downstream specification mining.

With these various abstractions parameterizing our trace generation, simple downstream miners,
such a k-Tails, are capable of mining rich specifications. However, there is a cost to the variety of
abstractions we employ. Each abstraction introduces more words into the overall vocabulary, and,
as the size of the overall vocabulary grows, so does the challenge of disentangling traces.

Finally, it is worthwhile to address the limitations of Parametric Lightweight Symbolic Execution.
PLSE is intraprocedural and therefore risks extracting only partial specifications. PLSE also makes
no attempt to detect infeasible traces. Finally, PLSE enumerates a fixed number of paths. As part of
this enumeration, any loops are unrolled for a single iteration only.2 In practice these limitations
enable the PLSE technique to scale and, for the purposes of the ml4spec toolchain, losses in
precision are balanced by the utilization of machine-learning-assisted metrics (which can tolerate
noisy data).

4 THRESHOLDING AND SAMPLING
In this section, we outline the techniques used in the ml4spec toolchain to take a corpus of traces,
generated via Parametric Lightweight Symbolic Execution (PLSE), and prepare them for word-vector
learning and specification mining. In particular, we present two key contributions, Hierarchical
Thresholding and Diversity Sampling, which improve the overall quality of our results. In addition,
we discuss alternative approaches.

2This single iteration loop unrolling gives us traces in which the loop never occurred and traces in which we visit the loop
body exactly one time. Yun et al. [2016] follow a similar model and argue that most API usage patterns are captured in a
single loop unrolling.

8

4.1 Hierarchical Thresholding

When preparing data for a word-vector learner, it is common to select a vocabulary minimum
threshold, which limits the words for which vectors will be learned. Any word that appears fewer
times than the threshold is removed from the training corpus. Through this process extremely rare
words, which may be artifacts of data collection, typos, or domain-specific jargon, are removed. In
the domain of mining specifications, we have a similar need. We would like to pre-select terms, from
our overall vocabulary, that occur enough times to be used as part of a pattern or specification. We
could simply set an appropriate vocabulary minimum threshold using our word-vector learner of
choice; however, this approach ignores a unique aspect of our traces. The traces we have, which are
used as input to both the word-vector learner and specification miner, are intra-procedural traces
extracted from a variety of procedures. To select terms that occur frequently does not necessarily
select for terms that are used across a variety of procedures. Because our traces are paths through
a procedure, it is possible to have a frequently occurring term (with respect to our traces) that
only occurs in one procedure. To achieve our desire for terms that are used in a variety of diverse
contexts, we developed a modified thresholding approach: Hierarchical Thresholding. Hierarchical
Thresholding counts how often a term occurs across procedures instead of traces. This simple
technique, with its utilization of the extra level of hierarchical information that exists in the traces,
reduces the possibility of selecting terms that are rare at the source-code level but frequent in the
trace corpus.

4.2 Diversity Sampling

The corpus of symbolic traces that we obtain, via lightweight symbolic execution, can be a chal-
lenging artifact to learn from. The symbolic executor, at execution time, builds an execution tree
and it is from this tree that we enumerate traces. Any attempt to learn from such traces can be
thought of as an attempt to indirectly learn from the original execution trees. The gap between
the tree representation and trace representation introduces a challenge: terms that co-occur at the
start of a large procedure (with many branches) will be repeated hundreds of times in our trace
corpus. This prefix duplication, which Henkel et al. [2018] term prefix dominance, adversely affects
the quality of word embeddings learned from traces.

As part of the ml4spec toolchain, we introduce a novel trace-sampling methodology, which seeks
to resolve the impact of prefix dominance. We call this sampling methodology Diversity Sampling
because it samples a diverse and representative set of traces by using a similarity metric to drive
the sample-selection process.

Alg. 1 provides the details of our Diversity Sampling technique. Because we work with intra-
procedural traces, we can associate each trace with its source-code procedure. Consequently, the
sampling routine can sample maximally diverse traces for each procedure independently. (For
a simple reason, our algorithm treats the trace corpus as a collection of sets: each set holds the
intra-procedural traces for one source procedure.) To begin Diversity Sampling, we either return
all traces (if the number of traces for a given procedure is less than our sampling threshold), or we
begin to iterate over the available traces and make selections. At each step of the selection loop,
on lines 8–19, we identify a trace that has the maximum average Jaccard distance when measured
against our previous selections. Jaccard distance is a measure computed between sets and, in our
setting, we use the set of unique tokens in a given trace to compute Jaccard Distances. We take the
average Jaccard distance from the set of currently sampled traces to ensure that each new selection
differs from all of the previously selected traces. Finally, when we have selected an appropriate
number of samples, we return them and proceed to process traces from the next procedure.

9

Algorithm 1: Diversity Sampling
input : A trace corpus TR
output : A down-sampled trace corpus

1 outputs ← [];
2 for T ∈ TR do
3

if |T | ≤ SAMPLES then

outputs = outputs ∪ T ;
continue;

end

choices ← T [0];
while | choices | < SAMPLES do

D∗ = 0.0;
S = null;
for t ∈ T − choices do

D = AverageJaccardDistance(t, choices);
if D ≥ D∗ then
S = t;
D∗ = D;

end

end
choices = choices ∪ S

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

end
outputs = outputs ∪ choices;

20
21 end
22 return outputs;

4.3 Alternative Samplers

Although Diversity Sampling is rooted in the intuition of extracting the most representative set of
traces for each procedure, it may not make a difference in the quality of downstream results. It
is for this reason that we also consider, in our ml4spec toolchain, two alternative approaches to
trace sampling: no sampling and random sampling. We include the option of no sampling because
word-vector learners thrive on both the amount and quality of data available. It is reasonable to
ask whether the training data lost by downsampling our trace corpus has enough negative impact
to offset possible gains. We also include random sampling as a third alternative; our motivation for
this inclusion is to assess the impact of our metric-guided selection. §7.4 evaluates the sampling
strategies discussed here.

5 DOMAIN-ADAPTED CLUSTERING
§3 outlined how ml4spec makes use of Parametric Lightweight Symbolic Execution (PLSE) to
generate rich traces. In §4, we presented innovations that improved the traces generated by PLSE,
and addressed some of the challenges associated with learning from traces. In this section, we
introduce Domain-Adapted Clustering, our solution to the challenge of clustering related terms.
We seek to cluster related terms (words) to simplify the Open-World Specification Mining task.
Traditional specification miners often use either rule templates or some form of user-directed

10

input (the API surface of interest, or perhaps a specific class or selection of classes from which
specifications should be mined). In our Open-World setting, none of this information is available.
Therefore, we have developed a methodology for extracting clusters of related terms that harnesses
the power of unsupervised learning (in the form of word embeddings). With these clusters in hand,
the task of mining specifications is greatly simplified.

5.1 Motivation

To motivate Domain-Adapted Clustering, it is revealing to consider the relationships among the
following ideas:

• Co-occurrence: word–word co-occurrence can be a powerful indicator of some kind of
relationship between words. Co-occurrence is, by its nature, a global property that can,
optionally, be associated with a sense of direction (word A appears to the left/right of word
B).

• Analogy: analogies are another way in which words can be related. The words that form
an analogical relationship encode a kind of information that is subtly different from the
information that co-occurrence provides. Given the analogy A is to B as C is to D, one would
find that A and B often co-occur, as do C and D; however, there may be no strong relationship
(in terms of co-occurrence) between A/B and C/D.

• Synonymy: synonyms are, in some sense, encoding strictly local structure. Two synony-
mous words need not co-occur; instead, synonyms are understood through the concept of
replaceability: if one can replace A with B then they are likely synonyms.

We can now attempt to codify which of these concepts are of value for Open-World Specification
Mining. To do so, we will introduce a simple thought experiment: consider an extremely simple
specification that consists of a call to foo and a comparison of the result of this call to 0. In our
traces this pattern would manifest in one of two forms: (i) foo foo==0 or (ii) foo foo!=0. For
the sake of our thought experiment, also assume that, by chance, print follows foo in our traces
95% of the time. What kinds of relationships do we need to use to extract the cluster of terms: foo,
foo==0, and foo!=0? We could use co-occurrence, however using co-occurrence will likely pick up
on the uninformative fact that foo frequently co-occurs with print. Furthermore, co-occurrence
may struggle to pick up on the relationship between foo and the check on its result: because each
check is encoded as a distinct word, neither check will co-occur with extremely high frequency.
We could, instead, use synonymy, but it is trivial to imagine words, such as malloc and calloc,
that are synonyms but not related in the sense of a usage pattern or specification.

It is the insufficiency of both co-occurrence and synonymy that forms the basis of Domain-
Adapted Clustering. Because neither metric covers all cases, Domain-Adapted Clustering forms a
parameterized mix of two metrics: one based on left and right co-occurrence, and another based on
unsupervised learning. Because both of these metrics encode a distance (or similarity) of some sort,
Domain-Adapted Clustering can be thought of as computing the pair-wise distance matrices and
then mixing them via a parameter α ∈ [0, 1]. Figure 1 provides a visual overview of the mixing
process Domain-Adapted Clustering employs.

5.2 The Co-occurrence Distance Matrix

Domain-Adapted Clustering utilizes co-occurrence statistics extracted directly from the (sampled
and thresholded) trace corpus. To capture as much information as possible, Domain-Adapted
Clustering walks each trace and computes, for each word pair (A, B), the number of times that A
follows B and the number of times that B follows A. These counts are converted to percentages and
these percentages represent a kind of similarity between A and B. The higher the percentages, the

11

more often A and B co-occur. To turn the percentages into a distance, we subtract them from 1.0
and store the average of the left-distance and right-distance in our co-occurrence distance matrix.

5.3 The Word-Embedding Distance Matrix

To incorporate unsupervised learning, Domain-Adapted Clustering utilizes word-vector learners.
The use of word-vector learners in the software-engineering domain is not a new idea [DeFreez
et al. 2018; Henkel et al. 2018; Nguyen et al. 2017a; Pradel and Sen 2018; Ye et al. 2016b]. Many
recent works have explored the power of embeddings in the realm of understanding and improving
software. What we contribute is, to the best of our knowledge, the first thorough comparison
of three of the most widely used word-vector learners in the application domain of software
engineering. We do this comprehensive evaluation to test an intuition that sub-word information
improves the quality of embeddings learned from software artifacts. We base this intuition on the
observation that similarly named methods have similar meaning. §7.4 provides the details of this
evaluation.

Our choice of word-vector learners as an unsupervised learning methodology is a deliberate
one. Earlier, we saw how synonymy could be a useful (albeit incomplete) property to capture.
Furthermore, we already have a notion of distance between words (given to us via our co-occurrence
distance matrix). Word-vector learners mesh well with both of these pre-existing criteria: word
vectors encode local context and are able to capture synonymy. Additionally, word–word distance
is encoded in the learned vector space. These properties make word-vector learners a convenient
choice for Domain-Adapted Clustering. To extract a distance matrix from a learned word embedding,
Domain-Adapted Clustering computes, for each word pair (A, B), the cosine distance between the
embedding of A and the embedding of B (here, we use cosine distance because it is the distance of
choice for word vectors).

5.4 Cluster Generation

To generate clusters, Domain-Adapted Clustering applies the insight that the clusters we seek
should be expressed in concrete usages. This idea leads us to invert the problem of clustering—
instead of clustering all of the terms in the vocabulary, we take a more bottom-up approach. We
start with an individual trace from our corpus of sampled traces. Within the trace, we find topics
or collections of terms that are related under our machine-learning-assisted metric: we use the
combined distance matrix we created previously and apply a threshold to detect words that are
related. Within a trace, any two words whose distance is below the threshold are assigned to the
same intra-trace cluster. The next step uses the set of all intra-trace clusters to create a set of
reduced traces: each trace in the corpus of traces is projected onto each of the intra-trace clusters
to create a new corpus of reduced traces. To form final clusters, we apply a traditional clustering
method (DBSCAN [Ester et al. 1996]) to the collection of reduced traces. In this final step we use
Jaccard distance between the sets of tokens in the reduced traces as the distance metric.

One distinctive advantage of Domain-Adapted Clustering, for our use case, is its ability to
generate overlapping clusters. Most off-the-shelf clustering techniques produce disjoint sets but,
in the realm of Open-World Specification Mining, it is easy to conceive of multiple patterns that
share common terms (opening a file and reading versus opening a file and writing). Finally, it is
worthwhile to note that the clustering step we have outlined here introduces two hyper-parameters:
the threshold to use for intra-trace clustering (which we will call β) and DBSCAN’s ϵ, which controls
how close points must be to be considered neighbors. This leaves Domain-Adapted Clustering with
a total of three tunable hyper-parameters: α, β, ϵ.

12

$START
strcasecmp
strcasecmp != 0
strcasecmp
strcasecmp == 0
dictGetIterator
log
dictGetIterator
→ dictNext

dictNext
dictNext != 0
dictNext → dictGetKey
dictGetKey
sdslen
sdslen → strmatchlen
strmatchlen
strmatchlen != 0
addReplyBulk
dictGetIterator
→ dictNext

dictNext
dictNext == 0
dictGetIterator

→ dictReleaseIterator

dictReleaseIterator
$END

(a) Example trace

dictGetIterator
dictGetIterator
→ dictNext

dictNext
dictNext == 0
dictNext != 0
dictGetIterator

→ dictReleaseIterator

dictReleaseIterator

(b) Example cluster

dictGetIterator
dictGetIterator
→ dictNext

dictNext
dictNext != 0
dictGetIterator
→ dictNext

dictNext
dictNext == 0
dictGetIterator

→ dictReleaseIterator

dictReleaseIterator

(c) Result of projecting the trace in Fig. 6a
into the vocabulary defined by the cluster
in Fig. 6b

Fig. 6. Example of trace projection

6 MINING
The final phase of the ml4spec toolchain is mining. To mine specifications in an Open-World
setting, ml4spec applies several insights, described in the preceding sections, to create a corpus of
rich traces and a collection of clusters. These two artifacts are used, in the mining phase, to create a
new corpus of projected traces that can be fed to any pre-existing trace-based mining technique. To
create projected traces, ml4spec takes each cluster and each trace and generates a new projected
trace by removing, from the original trace, any word that is not in the currently selected cluster.
This projection process is shown in Fig. 6. After projection, traces can be de-duplicated (if desired)
and then passed to any trace-based miner. The ml4spec toolchain is unique in its non-reliance on
any particular trace-based miner. It is the dissociation from specific mining techniques that makes
ml4spec a toolchain for Open-World mining and not just another trace-based mining technique.

13

Table 1. Grid search parameters

Name

Values

Purpose

learner
sampler
alpha
beta
epsilon

{fastText, GloVe, word2vec} Word-vector learner to use
{Diversity, Random, None}
{0.00, 0.25, 0.50, 0.75, 1.00} Weight for combined distance matrix
{0.20, 0.25, . . . , 0.45, 0.50}
{0.10, 0.30, 0.50, 0.70, 0.90}

Threshold for intra-trace clustering
Parameter to DBSCAN

Sampling method to use

Phase

II (Learning)
III (Sampling)
IV (DAC)
IV (DAC)
IV (DAC)

7 EVALUATION

In this section we introduce our evaluation methodology and address each of our five research
questions. For the purposes of evaluation we ran the ml4spec toolchain on five different open
source projects:

• Curl: a popular command-line tool for transferring data.
• Hexchat: an IRC client.
• Ngnix: a web server implementation.
• Nmap: a network scanner.
• Redis: a key-value store.

These projects were selected because they exhibit a wide variety of usage patterns across diverse
domains. For each of these five projects, we performed a grid search to gain an understanding of
our various design decisions. §7.1 details this search.

7.1 Grid Search
To facilitate a comprehensive evaluation of ml4spec, we performed a grid search across thousands
of parameterizations of the ml4spec toolchain. The grid search serves two purposes. First, the
results of the grid search provide a firmer empirical footing for understanding the efficacy and
impacts of different aspects of our toolchain (in particular, the grid search aids in quantifying the
impacts of different word-vector learners and sampling methodologies). Second, the Open-World
Specification Mining task emphasizes a lack of user-directed feedback—to meet this standard we
must ensure, via the data gleaned from the grid search, that the hyper-parameters associated with
the ml4spec toolchain can be set, globally, to good default values. Table 1 outlines the parameters in
play and the ranges of values investigated for each parameter. Upper and lower limits for each search
range were carefully chosen, based on the results of smaller searches, to reduce the computational
costs of the larger search over the parameters presented in Tab. 1.

7.2 RQ1: Can we effectively mine useful and clean clusters in an Open-World setting?

Research Question 1 asked whether we can mine useful and clean clusters. The difficulty with
mining such clusters lies in the setting of our mining task. We seek to solve the problem of mining
specifications in an Open-World setting: one in which implicit and explicit sources of hierarchical
or taxonomic information are unavailable. It is this Open-World setting that creates a unique need
for disentangling the many different topics that may exist in an abstracted symbolic trace. The
purpose of Research Question 1 is to understand the efficacy of the techniques described earlier
(specifically, Domain-Adapted Clustering) against a key challenge of Open-World Specification
Mining: learning correct and clean clusters.

To measure the quality of our learned clusters we found the need for a benchmark. Unfortunately,
to the best of our knowledge, the problem of Open-World Specification Mining has not been

14

hashTypeInitIterator
hashTypeNext
hashTypeReleaseIterator

(a) Cluster in the vocabulary of simple
call names

hashTypeInitIterator
hashTypeInitIterator
→ hashTypeNext

hashTypeNext
hashTypeNext == -1
hashTypeNext != -1
hashTypeInitIterator

→ hashTypeReleaseIterator

hashTypeReleaseIterator

(b) Cluster in our enriched vocabulary

Fig. 7. Comparison between two clusters

previously addressed and, therefore, there is no ground truth to evaluate our learned clusters
against (due to the lack of need for gold standard clusters). One possible avenue of evaluation and
source of implicit clusters exists in source-code documentation. Many thoroughly documented and
heavily used APIs include information on the associations between functions (most commonly
in the form of a “See also. . . ” or “Related methods. . . ” listing). Another possible source of implicit
information comes from projects that have made the transition from a language like C to a language
like C++. In such a transition methods are often grouped into classes and this signal could be
used to induce a clustering. Finally, there is the implicit clustering induced by the locations where
various API methods are defined: even in C, functions defined in the same header are likely related.
Despite these various sources of implicit clusters, we have identified a need for manually defined
gold standard clusters. We use manually extracted ground truth clusters for two reasons. First, the
sources of information listed above are indications of relatedness but not necessarily indications of
a specification or usage pattern. For example, several different methods are commonly defined for
linked lists, such as length(), next(), prev(), and hasNext() but not all of these methods are
necessarily used together in a pattern. Second, the vocabulary we are working over includes more
than simple call names—we also have information related to the path condition and information
about dataflow between calls. For example, compare the two clusters given in Fig. 7. The cluster
in Fig. 7a consists only of call names, while the cluster in Fig. 7b includes call names, return value
checks, and dataflow information. In comparing these two clusters, it becomes clear that a cluster
over words in our enriched vocabulary (induced by the abstractions we choose) is strictly more
informative than a cluster over a vocabulary of simple call names.

Taken together, these two issues (the weak signal of the aforementioned sources and the lack of
labels for some words in our enriched vocabulary) make manually extracted clusters more desirable.
For the purpose of this evaluation we have extracted 71 gold standard clusters from five open
source projects. We have placed no explicit limit on the sizes of the clusters we included, thereby
increasing the challenge of recovering all the clusters in our benchmark correctly.

Using our set of 71 gold standard clusters we are able to perform a quantitative evaluation by
measuring the Jaccard similarity3 of our extracted clusters and our gold standard clusters. Because
our toolchain does not mine a fixed number of clusters, we need some way to “pair” an extracted
cluster with the gold standard cluster it most represents. To do this, we look for a pairing of
extracted clusters with gold standard clusters that maximizes the average Jaccard similarity. This

3Jaccard similarity between sets A and B is |A∩B |

|A∪B | . Jaccard distance is one minus the Jaccard similarity.

15

Table 2. Best scoring configurations for each of the five target projects

Benchmark

Measurement

curl

hexchat

nginx

nmap

redis

Top-1 (Jaccard)
Top-1 (Intersection)
Top-5 (Jaccard)
Top-5 (Intersection)

62.8%
79.7%
62.1%
77.9%

52.8% 44.9% 49.1% 71.9%
78.7% 67.6% 70.1% 83.5%
51.8% 43.7% 47.3% 69.3%
73.8% 70.1% 67.2% 81.0%

provides us with a way to have a consistent evaluation regardless of the number of total clusters
we extract. (One might argue that this allows for extracting an unreasonable amount of clusters
in an attempt to game this metric. However, this kind of “metric hacking” is unachievable in our
toolchain due to the use of DBSCAN to extract clusters from reduced traces. Clustering our reduced
traces, using the Jaccard distance between sets of tokens within a trace, removes the possibility
that our tool is simply enumerating all possible clusters to achieve a high score.)

In addition to Jaccard similarity, which penalizes both omissions and spurious inclusions, we
also measure the percent intersection between our extracted clusters and the clusters in our gold
standard dataset. Table 2 provides both of these measurements for each of the five open-source
projects we examined. To provide a robust understanding of performance, and reduce variance in
our measurements, we provide both the best (Top-1) results and an average of the five best results
(Top-5) for both similarity measures. (We use the data from our grid search, described in §7.1,
to compute these averages.) Examining Tab. 2, we observe that the ml4spec toolchain retrieves
clusters that have a strong agreement with the clusters in our gold standard dataset. Furthermore,
the intersection similarity results show that our extracted clusters contain, on average, over two
thirds of the desired terms from the clusters in our gold standard dataset. Together, these results
answer Research Question 1 in the affirmative: ml4spec is capable of extracting clean and useful
clusters in an Open-World setting.

7.3 RQ2: How does DAC compare to off-the-shelf clustering techniques?
In this section, we explore how our Domain-Adapted Clustering (DAC) technique (a key piece of
our Open-World specification miner) compares to traditional clustering approaches. To understand
the relationship between DAC and more traditional clustering methods, it is instructive to consider
the input data we have available to use in the clustering process. Prior to clustering, we have access
to a pairwise distance matrix (created via a combination of co-occurrence statistics and word–word
cosine distance), our learned word vectors, and our original traces.

Most clustering methods accept either vectors of data or pair-wise distance matrices. In principle,
this leaves our choices for clustering methods to compare to quite open. However, using our word
vectors as the input to clustering ignores our earlier insight about the advantage of combining word
embeddings and co-occurrence statistics. Therefore, we focus on clustering algorithms that accept
pre-computed pair-wise distances as input. From this class of clustering methods we have selected
the following techniques to compare against: DBSCAN [Ester et al. 1996], Affinity Propagation [Frey
and Dueck 2007], and Agglomerative Clustering.

To compare the selected traditional techniques to DAC we use the benchmark we introduced in
RQ1 as a means of consistent evaluation. Both DAC and our selection of traditional techniques
require some number of hyper-parameters to be set. To ensure a fair evaluation, we have searched
over a range of hyper-parameters for each of the selected techniques and compare between the

16

Table 3. DAC compared to off-the-shelf clustering techniques

Clustering

curl

hexchat

nginx

nmap

redis

Benchmark

49.9%
DBSCAN
38.9%
Agglomerative
12.1%
Affinity Prop.
DAC (Rel. Increase) +25.9%

34.9%
25.8%
11.9%

36.7%
15.8%
10.2%

59.6%
8.6%
11.5%
+41.2% +24.1% +34.5% +25.7%

36.5%
12.7%
10.3%

Table 4. DAC compared to off-the-shelf clustering techniques boosted by our
machine-learning-assisted metric

Clustering

curl

hexchat

nginx

nmap

redis

Benchmark

49.9%
DBSCAN
40.7%
Agglomerative
15.3%
Affinity Prop.
DAC (Rel. Increase) +25.9%

38.1%
39.2%
13.4%

38.0%
21.6%
12.5%

64.4%
26.7%
15.5%
+36.9% +10.6% +29.7% +16.5%

37.9%
19.7%
15.3%

best configurations for each technique. Table 3 provides performance measurements for each of the
three off-the-shelf clustering baselines across each of our five target projects. In addition, Tab. 3
provides the relative performance increase gained by using DAC in place of these baselines.4 For
this comparison we have made only the co-occurrence distance matrix available to our off-the-shelf
baselines as one of DAC’s key insights was the importance of a machine-learning-assisted metric.
Table 4 follows the same format but provides each off-the-shelf technique access to the combined
matrix DAC uses for clustering. In either case, we see that DAC outperforms each of the baselines
by a wide margin.

7.4 RQ3: How does the choice of word vector learner and the choice of sampling

techniques affect the resulting clusters?

Research Question 3 seeks to understand the impact of two choices made in the earlier portion of
our toolchain: the choice of word vector learner and the choice of trace sampling technique. For the
choice of word vector learner we argued that fastText with its utilization of sub-word information
(in the form of character level n-grams) would provide embeddings better suited to the task of
extracting clean clusters. We based this prediction on the observation, made by many, that similarly
named methods often have similar meaning. When it came to the choice of trace sampling we
sought to reduce the impact of a problem, identified by Henkel et al. [2018], called prefix dominance.
To address this issue of prefix dominance in our specification mining setting we introduced a trace
sampling methodology termed Diversity Sampling.

To understand the interplay and effects of these choices we have evaluated the ml4spec toolchain
in nine configurations. These nine configurations are defined by two choices: a choice of word vector
learner (either fastText [Bojanowski et al. 2017], GloVe [Pennington et al. 2014], or word2vec [Mikolov

4We compute the relative performance increase by comparing to the best overall off-the-shelf technique on a per-project
basis.

17

Table 5. Top-5 performance (geometric mean across our five target projects). The
shaded row and column represent the best sampler and learner, respectively.

Learner

Sampler

Diversity Sampling
Random Sampling
No Sampling

word2vec GloVe
50.3% 47.1%
42.0% 41.5%
44.9% 44.4%

fastText
52.2%
50.0%
48.6%

Table 6. Top-1 performance (geometric mean across our five target projects). The
shaded row and column represent the best sampler and learner, respectively.

Learner

Sampler

Diversity Sampling
Random Sampling
No Sampling

word2vec GloVe
52.7% 48.6%
44.5% 43.6%
47.7% 46.3%

fastText
54.9%
53.0%
50.7%

et al. 2013]) and a choice of trace sampling technique (either Diversity Sampling, random sampling,
or no sampling). By evaluating our full toolchain with varying choices of embedding and sam-
pling methodology we can either confirm or refute our intuitions. We leverage the gold standard
clusters introduced in RQ1 to provide a consistent benchmark for comparison between the nine
configurations we’ve outlined.

First, we examine top-5 performance (measured against our benchmark) across all of the con-
figurations we established in §7.1. We look at averages of the top-5 configurations (with sampler
and learner fixed to one of the nine choices outlined earlier) to understand effects of our choices of
interest (the sampler and learner) and to reduce any variance from other sources. Table 5 provides
top-5 performance measurements for each of our nine possible configurations. We can see that
fastText is superior (regardless of sampling choice) to any of the other word vector learners by a
wide margin. We also observe that fastText paired with Diversity Sampling is the most performant
combination. However, fastText with no sampling is not far behind—this is perhaps indicative of
both the impact of word embeddings and the need for larger corpora to learn suitable embeddings.
Although top-5 averages provide a robust picture of the performance of our selected configura-
tions, we also would like to understand which configurations have the best peak (top-1) performance.
To assess top-1 performance, we examine Tab. 6 which shows the geometric mean (across our
five target projects) of the best performing configuration identified via the data from our grid
search (§7.1). These results affirm the impact of Diversity Sampling and fastText as the superior
word-embedding learner for this use case. Finally, we observe that the combination of fastText and
Diversity Sampling again produces the best overall performance.

These results support two conclusions. First, fastText, with its use of sub-word information,
outperforms GloVe and word2vec in the cluster extraction task we have benchmarked. Second,
Diversity Sampling both improves the performance of our toolchain and word vector learner (by
reducing the amount of input data) and provides an increase in performance compared to the
other baseline choices of sampling routine. These results also support further examination of the
advantages of sub-word information in the software-engineering domain; specifically, we note that

18

e
r
o
c
S
k
r
a
m
h
c
n
e
B

100%

80%

60%

40%

20%

0%

redis
curl
hexchat
nmap
nginx

0.00

0.25

0.50
Alpha

0.75

1.00

Fig. 8. Average benchmark performance for varying values of α

fastText has no concept of the ideal boundaries between sub-tokens that naturally exist in program
identifiers. A word vector learner equipped with this knowledge may produce even more favorable
results.

7.5 RQ4: Is there a benefit to using a combination of co-occurrence statistics and word

embeddings?

One of the key insights from §5 was that word embeddings and co-occurrence statistics capture
subtly different information. Word embeddings excel at picking up on local context (a direct result
of being based on the distributional hypothesis which asserts that similar words appear in similar
contexts). This focus on local context makes word embeddings well-suited for tasks like word
similarity and analogy solving. For specification mining, co-occurrence information is often used, in
some form, to capture the “support” for a candidate rule or pattern. These co-occurrence statistics
encode a global relationship between words that is more far-reaching than the relationship captured
by word vectors.

Research Question 4 attempts to precisely quantify the impact of these two different sources of
information. This effort is made somewhat easier by the choice to include a tunable parameter in
our toolchain that represents the relative weight of word–word distance and co-occurrence distance
in our final pair-wise distance matrix. By evaluating our full toolchain with a gradation of weight
values we can pinpoint the mix of metrics that lead to optimal performance on the benchmark we
introduced earlier.

Figure 8 plots average benchmark scores for different values of the α parameter. Here we take an
average, with α fixed, over all configuration explored in §7.1. We observe a clear trend of increasing
performance as more weight is applied to the word embeddings (and less to the co-occurrence
statistics). In addition, we note that this performance increase reaches a peak at α = 0.75 for each

19

e
r
o
c
S
k
r
a
m
h
c
n
e
B

100%

80%

60%

40%

20%

0%

redis
curl
hexchat
nmap
nginx

0.00

0.25

0.50
Alpha

0.75

1.00

Fig. 9. Peak benchmark performance for varying values of α

of the five projects. As we did in Research Question 3, we also examine top-1 performance. The
results for top-1 performance, given in Fig. 9, paint a clearer picture of the relationship between
word embeddings and co-occurrence statistics. In Fig. 9 we observe that, for all five projects, adding
word embeddings to our distance matrix produces a pronounced increase in performance. Again
we are able to validate that, for each project, this increase in performance peaks at α = 0.75. These
results suggests an affirmative answer to Research Question 4: there is a quantifiable benefit to
using both co-occurrence statistics and word embeddings; furthermore, a combination that favors
the distances produced via word embeddings yields maximum performance across each of the
projects we examined.

7.6 RQ5: Does our toolchain transfer to unseen projects with minimal

reconfiguration?

Research Question 5 asked if our toolchain can be easily adapted to unseen projects. More precisely,
we would like to evaluate whether the few parameters in our tool can use reasonable defaults
without sacrificing too much performance. To do this we can re-examine some of the results from
the grid search in §7.1 to develop an understanding of the performance impact choosing defaults
would have when transferring to unseen data.

We can rank configurations by enumerating the top-20 configurations for each of our five target
projects and counting the number of times any given configuration occurs in the top-20 for any
project. This ranking reveals that two configurations each produce top-20 results in four out of the
five projects we considered. Examining these two configurations further, we find that exploring
just these two configurations allows ml4spec to find clusters that are within ten percent of the
best performing configuration for each of our five projects. Therefore, we can answer Research

20

Question 5 in the affirmative, with the knowledge that running in just two default configurations
allows for full automation with no more than a ten percent performance decrease.

8 THREATS TO VALIDITY

Our usage of Parametric Lightweight Symbolic Execution (PLSE) provides us with a way to quickly
extract rich traces from buildable C programs; however, PLSE is imprecise. It is possible that a more
precise symbolic-execution engine would provide our downstream mining techniques with more
accurate information and, in turn, reveal more correct specifications. In addition, it is likely that an
execution engine capable of generating interprocedural traces would improve the quality of our
results.

Our ground-truth clustering benchmark was manually extracted with the goal of providing a
quantitative benchmark in the rich vocabulary available to the ml4spec toolchain. This manual
process is susceptible to bias. To mitigate this risk, each cluster was validated against the source
program to confirm that the set of terms within the cluster appeared in a concrete usage. Further-
more, the gold-standard clusters were created with no bounds on their size: this variance in cluster
size greatly increases the difficulty of recovering correct clusters while matching the reality of
usage patterns which can range from simple checks to complex iterators or initialization routines.
It is our hope that the release of the ground-truth dataset will provide the groundwork for a larger,
more comprehensive, gold-standard dataset curated by the community.

We chose to focus on a collection of five open-source C projects. It is possible that our selection of
projects is not representative of the wider landscape of API usages in C. Furthermore, our technique,
while language agnostic in theory, may not easily transfer to other languages. Fortunately, the
PLSE implementation uses the GCC toolchain as a front end, which makes cross-language mining
a possibility for future work.

9 RELATED WORK

There exists a wide variety of related works from the specification mining, API misuse, program
understanding, and entity embedding communities. For comprehensive overviews of specification
mining and misuse we refer the reader to Lo et al. [2011] and Robillard et al. [2013]. For efforts in
machine learning and its application in the software-engineering domain Allamanis et al. [2017a]
provide an excellent survey. In addition, there exists a listing of machine learning on code resources
maintained by the community [source{d} 2019]. For details on embeddings and their use in the
software-engineering domain Martin Monperrus [2019] provide an up-to-date listing. In the fol-
lowing sections, we discuss related works in the realms of specification mining and embeddings of
software artifacts in greater detail.

Specification Mining

There is a rich history of work on mining specifications, or usage patterns, from programs. Earlier
approaches, such as Li and Zhou [2005], provided relatively simple specifications. Going forward in
time, a growing body of work attempted to produce richer FSA-based specifications [Acharya and
Xie 2009; Ammons et al. 2002; Dallmeier et al. 2006; Gabel and Su 2008; Lo and Khoo 2006; Lorenzoli
et al. 2008; Pradel and Gross 2009; Quante and Koschke 2007; Shoham et al. 2008; Walkinshaw and
Bogdanov 2008; Walkinshaw et al. 2007]. Some recent work such as Deep Specification Mining and
Doc2Spec, has incorporated NLP techniques [Le and Lo 2018; Zhong et al. 2009]. DeFreez et al. [2018]
use word-vector learners to bolster traditional support-based mining via the identification of func-
tion synonyms. In the broader field of non-FSA-based specification mining techniques, there exist
several novel techniques: Nguyen et al. [2009] mine graph-based specifications; Sankaranarayanan
et al. [2008] produce specifications as Datalog rules; Acharya et al. [2007] create a partial order

21

over function calls and Murali et al. [2017] develop a Bayesian framework for learning probabilistic
specifications. In addition to mining, several works focus on the related problem of detecting mis-
uses [Engler et al. 2001; Livshits and Zimmermann 2005; Monperrus and Mezini 2013; Wasylkowski
et al. 2007; Yun et al. 2016].

The ml4spec toolchain is agnostic to the choice of trace-based mining technique used to generate
specifications. This miner-agnostic perspective makes ml4spec a front end that enables prior trace-
based miners to work in the Open-World setting we have described. In addition, ml4spec’s use of
Parametric Lightweight Symbolic Execution makes it possible to mine, via traditional methods,
specifications that involve both control-flow and data-flow information.

Embeddings of Software Artifacts

Recently, several techniques have leveraged learned embeddings for artifacts generated from pro-
grams. Nguyen et al. [2016, 2017b] leverage word embeddings (learned from ASTs) in two domains
to facilitate translation from Java to C#. Le and Lo [2018] use embeddings to bootstrap anomaly
detection against a corpus of JavaScript programs. Gu et al. [2016] leverage an encoder/decoder
architecture to embed whole sequences in their DeepAPI tool for API recommendation.

Pradel and Sen [2017] use embeddings (learned from custom tree-based contexts built from
ASTs) to bootstrap anomaly detection against a corpus of JavaScript programs. Gu et al. [2016]
leverage an encoder/decoder architecture to embed whole sequences in their DeepAPI tool for API
recommendation. API2API by Ye et al. [2016a] also leverages word embeddings, but it learns the em-
beddings from API-related natural-language documents instead of an artifact derived directly from
source code. Alon et al. [2018b] learn from paths through ASTs to produce general representations
of programs; in [Alon et al. 2018a] they expand upon this general representation by leveraging
attention mechanisms. Ben-Nun et al. [2018] produce embeddings of programs that are learned from
both control-flow and data-flow information. Zhao et al. [2018] introduce type-directed encoders, a
framework for encoding compound data types via a recursive composition of more basic encoders.
Using input/output pairs as the input data for learning, Piech et al. [2015] and Parisotto et al. [2016]
learn to embed whole programs. Using sequences of live variable values, Wang et al. [2017] produce
embeddings to aid in program repair tasks. Allamanis et al. [2017b] learn to embed whole programs
via Gated Graph Recurrent Neural Networks (GG-RNNs) [Li et al. 2015]. Peng et al. [2015] provide
an AST-based encoding of programs with the goal of facilitating deep-learning methods.

In contrast to prior work on the embedding of software artifacts, we provide both a novel use
of embeddings in the software-engineering domain (in the form of Domain-Adapted Clustering
and its machine-learning-assisted metric) and a comprehensive comparison between three state-
of-the-art word embedding techniques (fastText [Bojanowski et al. 2017], GloVe [Pennington
et al. 2014], and word2vec [Mikolov et al. 2013]). Furthermore, we make an insight into a future
line of work involving the utilization of refined sub-token information to improve embeddings in
the software-engineering domain.

10 CONCLUSION

With a growing number of frameworks and libraries being authored each day, there is an increased
need for industrial-grade specification mining. In this paper, we introduced the problem of Open-
World Specification Mining with the hope of fostering new mining tools and techniques that
focus on reducing burdens on users. The challenge of mining is amplified in an Open-World
setting. To address this challenge, we introduced ml4spec: a toolchain that combines the power of
unsupervised learning (in the form of word embeddings) with traditional techniques to successfully
mine specifications and usage patterns in an Open-World setting.

22

Our work also provides a new dataset of ground-truth clusters which can be used to benchmark
attempts to extract related terms from programs. We provided a comprehensive evaluation across
three different word-vector learners to gain insight into the value of sub-word information in the
software-engineering domain. Lastly, we introduced three new techniques: Hierarchical Threshold-
ing, Diversity Sampling, and Domain Adapted Clustering each solving a different challenge in the
realm of Open-World Specification Mining.

REFERENCES
Mithun Acharya and Tao Xie. 2009. Mining API Error-Handling Specifications from Source Code. In Fundamental Approaches
to Software Engineering (Lecture Notes in Computer Science), Marsha Chechik and Martin Wirsing (Eds.). Springer Berlin
Heidelberg, 370–384.

Mithun Acharya, Tao Xie, Jian Pei, and Jun Xu. 2007. Mining API Patterns As Partial Orders from Source Code: From Usage
Scenarios to Specifications. In Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference
and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (ESEC-FSE ’07). ACM, New York, NY, USA,
25–34. https://doi.org/10.1145/1287624.1287630

Miltiadis Allamanis, Earl T Barr, Premkumar T Devanbu, and Charles A Sutton. 2017a. A Survey of Machine Learning for

Big Code and Naturalness. CoRR abs/1709.0 (2017). http://arxiv.org/abs/1709.06182

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017b. Learning to Represent Programs with Graphs.

CoRR abs/1711.0 (2017). arXiv:1711.00740 http://arxiv.org/abs/1711.00740

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018a. code2vec: Learning Distributed Representations of Code.

CoRR abs/1803.09473 (2018). arXiv:1803.09473 http://arxiv.org/abs/1803.09473

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018b. A General Path-based Representation for Predicting Program
Properties. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI 2018). ACM, New York, NY, USA, 404–419. https://doi.org/10.1145/3192366.3192412

Glenn Ammons, Rastislav BodÃŋk, and James R. Larus. 2002. Mining Specifications. In Proceedings of the 29th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL ’02). ACM, New York, NY, USA, 4–16.
https://doi.org/10.1145/503272.503275

Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural Code Comprehension: A Learnable Representation

of Code Semantics. CoRR abs/1806.07336 (2018). arXiv:1806.07336 http://arxiv.org/abs/1806.07336

A. W. Biermann and J. A. Feldman. 1972. On the Synthesis of Finite-State Machines from Samples of Their Behavior. IEEE

Trans. Comput. C-21, 6 (June 1972), 592–597. https://doi.org/10.1109/TC.1972.5009015

Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword
Information. Transactions of the Association for Computational Linguistics 5 (2017), 135–146. https://transacl.org/ojs/
index.php/tacl/article/view/999

Valentin Dallmeier, Christian Lindig, Andrzej Wasylkowski, and Andreas Zeller. 2006. Mining Object Behavior with ADABU.
In Proceedings of the 2006 International Workshop on Dynamic Systems Analysis (WODA ’06). ACM, New York, NY, USA,
17–24. https://doi.org/10.1145/1138912.1138918

Daniel DeFreez, Aditya V. Thakur, and Cindy Rubio-GonzÃąlez. 2018. Path-based Function Embedding and Its Application to
Error-handling Specification Mining. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018). ACM, New York, NY, USA,
423–433. https://doi.org/10.1145/3236024.3236059 event-place: Lake Buena Vista, FL, USA.

Dawson Engler, David Yu Chen, Seth Hallem, Andy Chou, and Benjamin Chelf. 2001. Bugs As Deviant Behavior: A General
Approach to Inferring Errors in Systems Code. In Proceedings of the Eighteenth ACM Symposium on Operating Systems
Principles (SOSP ’01). ACM, New York, NY, USA, 57–72. https://doi.org/10.1145/502034.502041

Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. A Density-based Algorithm for Discovering Clusters
a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In Proceedings of the
http:
Second International Conference on Knowledge Discovery and Data Mining (KDD’96). AAAI Press, 226–231.
//dl.acm.org/citation.cfm?id=3001460.3001507

Brendan J. Frey and Delbert Dueck. 2007. Clustering by Passing Messages Between Data Points. Science 315, 5814 (2007),
972–976. https://doi.org/10.1126/science.1136800 arXiv:http://science.sciencemag.org/content/315/5814/972.full.pdf
Mark Gabel and Zhendong Su. 2008. Javert: Fully Automatic Mining of General Temporal Properties from Dynamic Traces.
In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering (SIGSOFT
’08/FSE-16). ACM, New York, NY, USA, 339–349. https://doi.org/10.1145/1453101.1453150

Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API Learning. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2016). ACM, New York, NY, USA,
631–642. https://doi.org/10.1145/2950290.2950334

23

Zellig S. Harris. 1954. Distributional Structure. WORD 10, 2-3 (1954), 146–162. https://doi.org/10.1080/00437956.1954.

11659520

Jordan Henkel, Shuvendu K. Lahiri, Ben Liblit, and Thomas Reps. 2018. Code Vectors: Understanding Programs Through
Embedded Abstracted Symbolic Traces. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018). ACM, New York, NY, USA,
163–174. https://doi.org/10.1145/3236024.3236085

Tien-Duy B. Le and David Lo. 2018. Deep Specification Mining. In Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA 2018). ACM, New York, NY, USA, 106–117. https://doi.org/10.1145/
3213846.3213876

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S Zemel. 2015. Gated Graph Sequence Neural Networks. CoRR

abs/1511.0 (2015). arXiv:1511.05493 http://arxiv.org/abs/1511.05493

Zhenmin Li and Yuanyuan Zhou. 2005. PR-Miner: Automatically Extracting Implicit Programming Rules and Detecting
Violations in Large Software Code. In Proceedings of the 10th European Software Engineering Conference Held Jointly with
13th ACM SIGSOFT International Symposium on Foundations of Software Engineering (ESEC/FSE-13). ACM, New York, NY,
USA, 306–315. https://doi.org/10.1145/1081706.1081755

Benjamin Livshits and Thomas Zimmermann. 2005. DynaMine: Finding Common Error Patterns by Mining Software
Revision Histories. In Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM
SIGSOFT International Symposium on Foundations of Software Engineering (ESEC/FSE-13). ACM, New York, NY, USA,
296–305. https://doi.org/10.1145/1081706.1081754 event-place: Lisbon, Portugal.

David Lo and Siau-Cheng Khoo. 2006. SMArTIC: Towards Building an Accurate, Robust and Scalable Specification Miner.
In Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering (SIGSOFT
’06/FSE-14). ACM, New York, NY, USA, 265–275. https://doi.org/10.1145/1181775.1181808

David Lo, Siau-Cheng Khoo, Jiawei Han, and Chao Liu. 2011. Mining Software Specifications: Methodologies and Applications.

CRC Press. Google-Books-ID: VAzLBQAAQBAJ.

D. Lorenzoli, L. Mariani, and M. PezzÃĺ. 2008. Automatic generation of software behavioral models. In 2008 ACM/IEEE 30th

International Conference on Software Engineering. 501–510. https://doi.org/10.1145/1368088.1368157

Zimin Chen Martin Monperrus. 2019. Embeddings for Source Code. https://www.monperrus.net/martin/embeddings-for-

code

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and
Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26, C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.). Curran Associates, Inc., 3111–3119. http://papers.nips.cc/
paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf

Martin Monperrus and Mira Mezini. 2013. Detecting Missing Method Calls As Violations of the Majority Rule. ACM Trans.

Softw. Eng. Methodol. 22, 1 (March 2013), 7:1–7:25. https://doi.org/10.1145/2430536.2430541

Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. 2017. Bayesian Specification Learning for Finding API
Usage Errors. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017). ACM,
New York, NY, USA, 151–162. https://doi.org/10.1145/3106237.3106284

Trong Duc Nguyen, Anh H. T. Nguyen, Hung Dang Phan, and Tien N. Nguyen. 2017a. Exploring API Embedding for API
Usages and Applications. 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE) (2017), 438–449.
Trong Duc Nguyen, Anh Tuan Nguyen, and Tien N. Nguyen. 2016. Mapping API Elements for Code Migration with Vector
Representations. In Proceedings of the 38th International Conference on Software Engineering Companion (ICSE ’16). ACM,
New York, NY, USA, 756–758. https://doi.org/10.1145/2889160.2892661

Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N Nguyen. 2017b. Exploring API Embedding for API
Usages and Applications. In Proceedings of the 39th International Conference on Software Engineering (ICSE ’17). IEEE
Press, Piscataway, NJ, USA, 438–449. https://doi.org/10.1109/ICSE.2017.47

Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H. Pham, Jafar M. Al-Kofahi, and Tien N. Nguyen. 2009. Graph-based
Mining of Multiple Object Usage Patterns. In Proceedings of the the 7th Joint Meeting of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (ESEC/FSE ’09). ACM, New
York, NY, USA, 383–392. https://doi.org/10.1145/1595696.1595767

Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. 2016. Neuro-

Symbolic Program Synthesis. CoRR abs/1611.0 (2016). arXiv:1611.01855 http://arxiv.org/abs/1611.01855

Hao Peng, Lili Mou, Ge Li, Yuxuan Liu, Lu Zhang, and Zhi Jin. 2015. Building Program Vector Representations for Deep
Learning. In Proceedings of the 8th International Conference on Knowledge Science, Engineering and Management - Volume
9403 (KSEM 2015). Springer-Verlag New York, Inc., New York, NY, USA, 547–553. https://doi.org/10.1007/978-3-319-
25159-2_49

Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for

24

Computational Linguistics, Doha, Qatar, 1532–1543. http://www.aclweb.org/anthology/D14-1162

Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, and Leonidas Guibas. 2015. Learning
Program Embeddings to Propagate Feedback on Student Code. In Proceedings of the 32Nd International Conference on
International Conference on Machine Learning - Volume 37 (ICML’15). JMLR.org, 1093–1102. http://dl.acm.org/citation.
cfm?id=3045118.3045235

Michael Pradel and Thomas R. Gross. 2009. Automatic Generation of Object Usage Specifications from Large Method
Traces. In Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering (ASE ’09). IEEE
Computer Society, Washington, DC, USA, 371–382. https://doi.org/10.1109/ASE.2009.60

Michael Pradel and Koushik Sen. 2017. Deep Learning to Find Bugs. Technical Report TUD-CS-2017-0295. Technische

Universität Darmstadt, Department of Computer Science.

Michael Pradel and Koushik Sen. 2018. DeepBugs: a learning approach to name-based bug detection. PACMPL 2 (2018),

147:1–147:25.

J. Quante and R. Koschke. 2007. Dynamic Protocol Recovery. In 14th Working Conference on Reverse Engineering (WCRE

2007). 219–228. https://doi.org/10.1109/WCRE.2007.24

M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini, and T. Ratchford. 2013. Automated API Property Inference Techniques.

IEEE Transactions on Software Engineering 39, 5 (May 2013), 613–637. https://doi.org/10.1109/TSE.2012.63

Martin P. Robillard and Robert DeLine. 2011. A field study of API learning obstacles. Empirical Software Engineering 16, 6

(Dec. 2011), 703–732. https://doi.org/10.1007/s10664-010-9150-8

Sriram Sankaranarayanan, Franjo IvanÄŊiÄĞ, and Aarti Gupta. 2008. Mining Library Specifications Using Inductive Logic
Programming. In Proceedings of the 30th International Conference on Software Engineering (ICSE ’08). ACM, New York, NY,
USA, 131–140. https://doi.org/10.1145/1368088.1368107

Kristie Seymore, Andrew Mccallum, and Ronald Rosenfeld. 1999. Learning Hidden Markov Model Structure for Information

Extraction. (1999).

S. Shoham, E. Yahav, S. J. Fink, and M. Pistoia. 2008. Static Specification Mining Using Automata-Based Abstractions. IEEE

Transactions on Software Engineering 34, 5 (Sept. 2008), 651–666. https://doi.org/10.1109/TSE.2008.63

source{d}. 2019. Cool links & research papers related to Machine Learning applied to source code (MLonCode): src-
d/awesome-machine-learning-on-source-code. https://github.com/src-d/awesome-machine-learning-on-source-code
original-date: 2017-06-20T13:35:45Z.

N. Walkinshaw and K. Bogdanov. 2008. Inferring Finite-State Models with Temporal Constraints. In 2008 23rd IEEE/ACM

International Conference on Automated Software Engineering. 248–257. https://doi.org/10.1109/ASE.2008.35

N. Walkinshaw, K. Bogdanov, M. Holcombe, and S. Salahuddin. 2007. Reverse Engineering State Machines by Interactive
Grammar Inference. In 14th Working Conference on Reverse Engineering (WCRE 2007). 209–218. https://doi.org/10.1109/
WCRE.2007.45

Ke Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic Neural Program Embedding for Program Repair. CoRR

abs/1711.07163 (2017). arXiv:1711.07163 http://arxiv.org/abs/1711.07163

Andrzej Wasylkowski, Andreas Zeller, and Christian Lindig. 2007. Detecting Object Usage Anomalies. In Proceedings of the the
6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations
of Software Engineering (ESEC-FSE ’07). ACM, New York, NY, USA, 35–44. https://doi.org/10.1145/1287624.1287632
event-place: Dubrovnik, Croatia.

X Ye, H Shen, X Ma, R Bunescu, and C Liu. 2016a. From Word Embeddings to Document Similarities for Improved
Information Retrieval in Software Engineering. In 2016 IEEE/ACM 38th International Conference on Software Engineering
(ICSE). 404–415. https://doi.org/10.1145/2884781.2884862

Xin Ye, Hui Shen, Xiao Ma, Razvan C. Bunescu, and Chang Liu. 2016b. From Word Embeddings to Document Similarities
for Improved Information Retrieval in Software Engineering. 2016 IEEE/ACM 38th International Conference on Software
Engineering (ICSE) (2016), 404–415.

Insu Yun, Changwoo Min, Xujie Si, Yeongjin Jang, Taesoo Kim, and Mayur Naik. 2016. APISAN: Sanitizing API Usages
Through Semantic Cross-checking. In Proceedings of the 25th USENIX Conference on Security Symposium (SEC’16). USENIX
Association, Berkeley, CA, USA, 363–378. http://dl.acm.org/citation.cfm?id=3241094.3241123

Jinman Zhao, Aws Albarghouthi, Vaibhav Rastogi, Somesh Jha, and Damien Octeau. 2018. Neural-augmented Static
Analysis of Android Communication. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018). ACM, New York, NY, USA,
342–353. https://doi.org/10.1145/3236024.3236066

H. Zhong, L. Zhang, T. Xie, and H. Mei. 2009. Inferring Resource Specifications from Natural Language API Documentation. In
2009 IEEE/ACM International Conference on Automated Software Engineering. 307–318. https://doi.org/10.1109/ASE.2009.94

25

