Noname manuscript No.
(will be inserted by the editor)

Predicting sensitive information leakage in IoT
applications using ﬂows-aware machine learning
approach

Hajra Naeem · Manar H. Alalﬁ

2
2
0
2

n
a
J

7

]

R
C
.
s
c
[

1
v
7
7
6
2
0
.
1
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract This paper presents an approach for identiﬁcation of vulnerable
IoT applications. The approach focuses on a category of vulnerabilities that
leads to sensitive information leakage which can be identiﬁed by using taint
ﬂow analysis. Tainted ﬂows vulnerability is very much impacted by the struc-
ture of the program and the order of the statements in the code, designing
an approach to detect such vulnerability needs to take into consideration such
information in order to provide precise results. In this paper, we propose and
develop an approach, FlowsMiner, that mines features from the code related
to program structure such as control statements and methods, in addition
to program’s statement order. FlowsMiner, generates features in the form of
tainted ﬂows. We developed, Flows2Vec, a tool that transform the features re-
covered by FlowsMiner into vectors, which are then used to aid the process of
machine learning by providing a ﬂow’s aware model building process. The re-
sulting model is capable of accurately classify applications as vulnerable if the
vulnerability is exhibited by changes in the order of statements in source code.
When compared to a base Bag of Words (BoW) approach, the experiments
show that the proposed approach has improved the AUC of the prediction
models for all algorithms and the best case for Corpus1 dataset is improved
from 0.91 to 0.94 and for Corpus2 from 0.56 to 0.96.

Keywords Internet of Things · Machine learning · Sensitive information
leakage · Taint ﬂow analysis

F. Author
ﬁrst address
Tel.: +123-45-678910
Fax: +123-45-678910
E-mail: fauthor@example.com

S. Author
second address
Dataset Link: https://github.com/HajraNaeem/Dataset

 
 
 
 
 
 
2

1 Introduction

Hajra Naeem, Manar H. Alalﬁ

The Internet of Things (IoT) is the interconnection of uniquely identiﬁable,
ubiquitous, sensing/actuating capable, programmable and self conﬁgurable
things over the internet. These things share data and services without the need
for human involvement [55]. IoT applications are becoming popular in many
ﬁelds including: smart homes, automobiles, health care and agriculture. Along
the beneﬁts of these applications, there are also some serious risks associated
with their usage. The interconnection of multiple devices, sharing data using
the internet and the insecure development make these applications vulnerable
to serious attacks as explained by the OWASP IoT top Ten vulnerabilities
[53].

Software vulnerability is a weakness in a software that can be exploited
to harm a system or steal sensitive data. If an IoT application has access to
the private data then misuse of such data can create problems for the applica-
tion users. Techniques to identify vulnerabilities in such applications are much
needed. Repeated incidents of IoT security exploitation show an increasing
trend of IoT attacks. According to Avast Smart Home security report pub-
lished in 2019, 40.8% of the smart homes have at least one vulnerable device
[13] which makes the whole network exploitable to attackers.

Unlike traditional less connected software applications, the nature of IoT
applications makes addressing these vulnerabilities more crucial, since these
applications have access to private data like when one comes home, what
programs they watch on TV, who visits them, when are they alone. The state
of devices connected to the SmartApps is itself a sensitive information. Thus,
requiring to have privacy risks properly analysed for these applications. But,
such tools for SmartApps are not widely available. There are some solutions
that have used static analysis to identify vulnerable IoT applications [8,16,
38].

In this paper, we aim at identifying vulnerable IoT applications using ma-
chine learning algorithms. The approach focuses on a category of vulnerabil-
ities that leads to sensitive information leakage which can be identiﬁed using
taint analysis. Recovering features related to information leakage requires an
analysis that can identify tainted ﬂows in the applications under analysis. Taint
analysis required to identify tainted ﬂows by tracking information ﬂows from
sensitive sources to sensitive sinks. Previous work use machine learning or deep
learning algorithms for identiﬁcation of similar category of vulnerabilities in
platforms other than IoT applications, such as android and web applications.
[1,2,11,4,7,39,40,41,42,43,44].

This paper is an extension of the research paper published at SANER2020
in the early research achievement track where we proposed to use the tainted
ﬂows along source code token frequencies to detect vulnerable SmartApps [20].
For that research, only ﬂow sensitivity was considered to identify the tainted
ﬂows. We improve the proposed technique by the following research contri-
butions for more accurate and eﬃcient automatic identiﬁcation of vulnerable
and non-vulnerable SmartApps:

Title Suppressed Due to Excessive Length

3

– We have used an improved method for extraction of tainted ﬂows. In our
short paper, we extract tainted ﬂows ignoring control structures like if
statements, loops and functions [20]. This paper extends the approach to
consider path and context sensitivity analysis. Our approach to perform
taint analysis is based on source code mining which we demonstrate as more
eﬃcient than taint analysis done with static analysis. We have implemented
our approach to develop a tool called FlowsMiner.

– We have conducted comprehensive experiments by using a more complex
dataset and a larger set of machine learning algorithms. The mutated
dataset named as Corpus2 used for this research has larger number of
SmartApps and these SmartApps are prepared by injecting vulnerabilities.
Identiﬁcation of these vulnerabilities require deeper analysis as compared
to dataset used for our previously published research.

– The gain with performance measures for the comprehensive experiments
is better than previously published paper. This is mainly because of the
reﬁned technique for identiﬁcation of tainted ﬂows. For training machine
learning algorithms, we have combined two categories of features that are
source code token frequencies and tainted ﬂows recovered from SmartApps.
– We have added a new step in the taint ﬂows identiﬁcation process. This
step formats the source code. We call it preprocessing of source code of
SmartApps. This reduces the number of cases to be handled for taint anal-
ysis and optimises the taint ﬂows identiﬁcation process.

The remainder of this paper is organised as follows. Section 2 presents the
background work. Section 3 provides an overview of the proposed approach
and presents all of its steps in detail including features extraction, identiﬁca-
tion of tainted ﬂows by using the text mining and using them to build models
with machine learning algorithms. All conducted experiments are presented in
section 4. In section 5, we discuss our ﬁndings and related work is presented
in section 6. Section 7 concludes the paper.

2 Background

The research presented in this paper addresses a category of IoT applica-
tions, Smart-Home applications. There are many frameworks that can be used
to develop Smart-Home IoT applications including SmartThings, OpenHAB,
Google Home, Nest Mobile, Android Things, ThingSpeak and DeviceHive. In
this paper, we speciﬁcally focus on SmartThings, a popular, open source de-
velopment platform. This platform uses Groovy programming language [14].
The applications developed in this platform are called SmartApps. SmartApps
developed in this platform allow their users to connect with external services
and devices to perform actions which makes their homes intelligent by allowing
them to control the hardware like lights, fancy door locks and thermostats.
SmartApps can perform actions like: turn oﬀ lights when no motion is de-
tected, notify a user if the user is not at home and a door opens, turn oﬀ the
heating system when a user leaves home. SmartApps can communicate with

4

Hajra Naeem, Manar H. Alalﬁ

web services to perform even more sophisticated tasks. Intentional or unin-
tentional vulnerabilities are prevalent in the SmartApps. If a SmartApp has
access to the private data then misuse of such data can create problems to
application users.

The basic skeleton of a SmartApp contains a method call and four method

deﬁnitions: deﬁnition, preferences, installed, updated and initialize.

1 definition (
2

3
4 preferences {
5

name : " Template " ,
category : " Convenience " )

section ( " Change to this mode " ) {

6

input " newMode " , " mode " , title : " Mode ? "

subscribe ( switches , " switch . off " , sw itc h OffH a ndl e r )

}

7
8 }
9 def installed () {
10

11
12 }
13 def updated () {
14

log . debug " Current mode = $ { location . mode } "
c r e a t e S u b s c r i p ti o n s ()

15

log . debug " Current mode = $ { location . mode } "
unsubscribe ()
c r e a t e S u b s c r i p ti o n s ()

16
17 }
18 def c r e a t e S u b sc r i p t i o n s ()
19 {
20
21 }
22 def sw it chO ffHandler ( evt ) {
23
24 }
25 private takeActions () {
26

takeActions ()

27

28

29

def message = newMode
s e n d N o t i f i c a t i o n T o C o n t a c t s ( newMode )
send ( message )
setLo ca t ionMode ( newMode )
log . debug message

30
31 }
32 private send ( msg ) {
33

sendSms ( msg )
log . debug msg

34
35 }

Listing 1: Groovy Application Template

The deﬁnition method call provides the metadata of an application and
expects parameters in the form of a map from the developer. The information
that an application requires from a user is deﬁned in the preferences. A user has
to input this information at the installation of the application. The installed
method is called when an application is installed by a user and the updated
method is called when a user updates the installation e.g. if a user selects that
a diﬀerent switch should be turned on when the application is installed, the
updated method is called at such modiﬁcations. The initialize method is called
from both installed and updated methods.

Title Suppressed Due to Excessive Length

5

A smart application subscribes to various events to detect occurrence of
events, which is done with the usage of the subscribe method. The subscribe
method is called from the initialize method. All the event handlers and helper
methods are registered to these methods. An example of a Groovy application
is provided in the Listing 1. The example in the listing does not contain com-
plete code of the application, we have kept important statements from diﬀerent
methods required to demonstrate presented concepts. To detect vulnerabilities
in IoT applications, researchers have used static and dynamic analysis to ﬁnd
the ﬂows of paths that leak the sensitive information [10, 3].

Our research focuses on devising machine learning techniques that help
to detect vulnerable SmartApps using automatically extracted features that
include tainted ﬂows or in other words the SmartApps include information
leakage vulnerability. In the next subsection we present some concepts required
for the tainted ﬂows.

2.1 Information Leakage vulnerability

Information leakage caused by tainted ﬂows is always attributed to the exe-
cution order of the program statements. For instance, if a sensitive value is
passed to a sensitization function before it reaches sensitive sinks then the
ﬂow is not tainted and the SmartApp is not vulnerable. Tools developed to
identify tainted ﬂows can provide three levels of analysis depending on the
capability/sensitivity of the analysis to different language constructs.

Tools that are sensitive to the statement execution order in the program
as well as content change in variables are said to provide ﬂow sensitive anal-
ysis. For instance, the code in listing 2 deﬁnes a variable var1 and assigns to
it a non-vulnerable string value. The value for var1 was changed to include
sensitive information on line 3, then the value of that sensitive variable is sent
to a sensitive sink sendSms which would potentially leak the sensitive infor-
mation. This example needs a ﬂow sensitive analysis to ﬂag it as vulnerable.
If statments 2 and 3 are of opposite order, a sensitive ﬂow analysis tool would
declare the program as non-vulnerable.

1
2 def var1 = " safe input " ;
3 var1 = " This variable contains ’$ { sensitive source } ’ " ;
4 sendSms ( var1 ) ; // sendSms is a sensitive sink

Listing 2: Flow Sensitivity Example

A path sensitive analysis takes the execution path into account. This is ex-
empliﬁed in how it deals with conditional statements; a path sensitive analysis
would treat each conditional block as a separate path. Listing 3 presents an
example where a path sensitive analysis would only detect sendSms on line 7
as a tainted sink and that if the condition is evaluated to True, while an in-
sensitive analysis would mark the code vulnerable, regardless of the condition
result.

6

Hajra Naeem, Manar H. Alalﬁ

1 def var2 = " 111 "
2 def var3 = " Hello "
3 if ( condition )
4
5 else
var3 = " Hello "
6
7 sendSms ( var3 , var2 )

var3 = " $ { source } "

Listing 3: Path Sensitivity Example

An insensitive path analysis will ﬂag Listing 4 as vulnerable even though,
the value of msg was changed from including sensitive content into a regular
string in both blocks of the if statement.

1 def phone = " 11111 "
2 def msg = " $ { source } "
if ( condition )
3
msg = " Hello "

4

5

6

7

else

msg = " Hi "

sendSms ( phone , msg )

Listing 4: Non-vulnerable in Corpus2 Example

Context sensitivity mainly deals with function calls and callbacks within
the program. A context sensitive analysis identiﬁes each call as its own and can
track back to the context of the call. Listing 5 presents a case where a method
is called twice. In a context insensitive analysis, both calls on lines 4 and 5
might be conﬂated, so the tainted return in ﬁrstCall will also be considered
in secondCall , marking the ﬂow tainted. A context sensitive approach on the
other hand, doesn’t confuse method calls and distinguishes each call site, so it
won’t mark the ﬂow to sendSms tainted.

1 // $sensitiveData defined
2

def takeAction () {

3

4

5

6

7

8

def message = " This contains $sens itiveData " ;
def firstCall = returnMessage ( message ) ;
def secondCall = returnMessage ( " no sensitive data " ) ;
sendSms ( secondCall ) ; }

private returnMessage ( message ) {

return message ;}

Listing 5: Context Sensitivity Example

Looking at the above code listings, we can note that the difference be-
tween vulnerable and non-vulnerable version is mostly the execution order
of statements, the location of the sensitive information within a conditional
statement or the context of the method call which involves sensitive infor-
mation as parameter. In the following section, we present architecture of the
proposed approach along a detailed description of each step including the tool
FlowsMiner that can be used to recover features from the source code related
to tainted ﬂows.

Title Suppressed Due to Excessive Length

7

Fig. 1: Flows-aware Machine Learning Approach

3 Flows-aware Machine Learning

In this section, we present an approach that uses a combined bag of words of
source code and tainted ﬂows information recovered from source code and that
to build models capable of accurately classifying applications as vulnerable if
they contain information leakage vulnerability, such vulnerability is mostly
exhibited by changes in the order of statements in source code.

Figure 1 presents the architecture of the proposed approach, highlighted
labels in the ﬁgure mark the corresponding sections describing the details of
each step.

Machine learning algorithms require some inputs from data to build be-
havioral models. These inputs are called features of the data. Most of machine
learning algorithms are trained on numeric features to build the models. Our
research focuses on identifying vulnerable SmartApps using machine learning
algorithms. The data for us is the source code of these applications. The source
code is raw data in terms of features. To build a model for detection of vulner-
able SmartApps, we need numeric features from the raw data. We have used
text mining to extract and tune numeric features from the source code.

The features used for training a machine learning algorithm to build a
model have a signiﬁcant impact on the performance of the algorithm. The
relevant features have a positive impact and are important to build an exact
behavior of an application. The irrelevant or partially relevant features have a
negative impact on the performance of the machine learning algorithms. This
makes selection of relevant features important. It is hard to decide which of
the features are relevant to build appropriate models, especially considering
that for almost every dataset the set of relevant features changes [49,50].

To build accurate models, every dataset has a different set of relevant
features which depends on the information in the dataset and the research
questions. A different research question may require a different set of fea-

Labelled AppsSmartThings AppsInclude sink?BenignD= { Token1: f1,Token2: f2, … Tokenn: fn}V= { App1D--> V1UApp2D--> V2U … UAppnD--> Vn}taintFlows(app)-->{SrcToSnk: f1,eSrcToSnk: f2, …eSrcToeSnk: f4} YesNoFLs= { SrcToSnk,eSrcToSnk, …eSrcToeSnk}Features = VU FLsTraining DataTest DataMachine Learning AlgorithmsTokensVulnerableNon-VulnerableTokenizeSection 3.1.2Section 3.3.2Section 3.3.1Section 3.1.1Section 3.4, 3.58

Hajra Naeem, Manar H. Alalﬁ

tures. Considering all these aspects we have decided to prepare a feature set
according to the following two categories of features:

– Initially, we build a bag of words (BoW) representation of the source code
and then we analyse it to select relevant tokens as features. Details about
this step are presented in section 3.1. In the evaluation section, we at-
tempt to evaluate the effectiveness of the BoW approach alone in predict-
ing information leakage vulnerabilities in SmartApps and to investigate on
how does automatically extracted features from the BoW of source code
perform when the BoW representations of vulnerable and non-vulnerable
applications are almost similar? (RQ1, 4.1).

– Then, using a text mining approach, we extract tainted ﬂows from the
source code and add them to the features set we gathered from the BoW.
The proposed text mining technique is implemented in a tool called Flows-
Miner, presented in section 3.3.1. The tainted ﬂows are converted to feature
vectors using a proposed tool called Flow2Vec, presented in section 3.3.2.
Details of this step are presented in section 3.3. In the evaluation section,
we investigate whether a text mining approach is capable of extracting
tainted ﬂows features from SmartApps? and how such approach performs
in comparison to static analysis techniques? (RQ2, 4.2). Then we analyze
the impact of combining the tainted ﬂows features extracted from text
mining with the features extracted from the BoW (RQ3, 4.3).

3.1 Features Extraction and Preparation- Bag of Words

We build a BOW representation of the source code and analyse it to select
features from relevant tokens as presented in section 3.1.1. The Token2Vec
presented in section 3.1.2 prepares the features set from these BoW which is
then fed to machine learning algorithms to build the behavioral models. When
we apply the Algorithm 1 to the SmartApp of Listing 1, the output BOW
are presented in the Listing 6.

The BoW feature set for a SmartApp is generated by crawling each line of
its source code. The source code is read line by line and it’s tokens are gener-
ated. The frequencies of tokens are calculated to prepare BoW. The deﬁnition
method call from source code of a SmartApp is excluded from BoW because
the method call only contains metadata about the SmartApp. The BoW is
then converted to feature vectors and used to train the machine learning al-
gorithms to build the models.

3.1.1 Token Frequencies

Tokens in methods of an application like installed, updated and initialize are
crawled to form a vocabulary of tokens Vi for the application. The pref erences
method is excluded from this step. The vocabulary Vi contains distinct set of
tokens for the application. The frequencies of the tokens are recorded against
the tokens. The process of identifying vocabulary set Vi and calculating the

Title Suppressed Due to Excessive Length

9

frequencies is described in function calT okenF requencies presented in the
Algorithm 1.

Input: Tokens for all code lines of methods
Output: D containing tokens t and frequencies f

Function calTokenFrequencies(tknsCodeLines):

D ← empty dictionary
while a line of code in tknsCodeLines do

for every token t in a line do

if t ∈ D then

increment by 1 f of t in D

else

add t to D with f as 1

end

end

end
return (D)

Algorithm 1: Calculate Token Frequencies

The algorithm starts by deﬁning an empty dictionary D, the dictionaries
are [key, value] data structures. The tokens of vocabulary Vi are recorded as
keys t and their respective frequencies f are recorded as values in the dictionary
D. While crawling the lines of tokens, when a new element for Vi is identiﬁed,
it is added to the D along value 1, and for the existing elements of Vi, the
value of element in D is incremented by 1. All string tokens are counted as
aStr tokens and all numeric tokens are counted as aN um tokens. Once tokens
of all code lines are crawled, the algorithm returns D.

When the calT okenF requencies function is applied to the Groovy ﬁle of
the Listing 1, the tokens identiﬁed by the function and their frequencies are
shown in the Listing 6. We have implemented the function in Python 3. The
Listing 6 shows all tokens t and their frequencies f returned as a dictionary D
from the implementation. The function implementation takes name of D from
the name of the application which is T emplate in this case.

1 ’ Template ’: { ’ def ’: 5 , ’ installed ’: 1 , ’( ’: 15 , ’) ’: 15 , ’{ ’: 6 , ’

log . debug ’: 4 , ’ aStr ’: 3 , ’ c r e a t e S u b s c r i p t i o n s ’: 3 , ’} ’: 6 , ’
updated ’: 1 , ’ unsubscribe ’: 1 , ’ subscribe ’: 1 , ’ switches ’: 1 ,
’ , ’: 2 , ’ switchOffHandler ’: 2 , ’ evt ’: 1 , ’ takeActions ’: 2 , ’
private ’: 2 , ’ message ’: 3 , ’= ’: 1 , ’ newMode ’: 3 , ’
s e n d N o t i f i c a t i o n T o C o n t a c t s ’: 1 , ’ send ’: 2 , ’ set LocationMode ’:
1 , ’ msg ’: 3 , ’ sendSms ’: 1}

Listing 6: Tokens and Frequencies for Listing 1

3.1.2 Token2Vec

In the literature, diﬀerent techniques are used to prepare vectors to train
machine learning algorithms. One such technique is Word2Vec [32,33] which
converts words to vectors that can be used to build models. We don’t need a

10

Hajra Naeem, Manar H. Alalﬁ

vector for every source code token but a vector for source code of an appli-
cation. Therefore, we have developed a tool Token2Vec, which transforms the
source code BoW into numeric feature vectors. To prepare the numeric feature
vectors, we use token frequencies from SmartApps under analysis computed
by Algorithm 1. The process to identify distinct tokens Vi for an application
is described in the section 3.1.1. Algorithm 1 calculates the frequencies of dis-
tinct tokens Vi for an application. We have implemented the technique named
as Token2Vec using Python 3 which identiﬁes distinct tokens for a complete
dataset of SmartApps. It prepares a set of distinct tokens V for an entire
dataset by combining the set of distinct tokens identiﬁed for each SmartApp
i.e. V ← V1 ∪ V2 ∪ . . . ∪ Vn. The set of distinct tokens V is used to create a
dataFrame object of the pandas library [19] with its column names initialised to
V. A vector for each SmartApp is prepared against V and it is appended to the
dataFrame. Once this process is completed for all SmartApps, the dataFrame
serves as a training and testing set for the machine learning algorithms.

For Corpus1 dataset presented in section 3.4.1, the tool ﬁnds 3834 dis-
tinct tokens and then keeps only those tokens that are at least present in 5
applications and it ﬁnds 515 such tokens. For the Corpus2 dataset it ﬁnds
1603 distinct tokens and selects 1592 tokens that are at least present in 5 ap-
plications. In Corpus2 there are around 18 versions of every application (the
applications are prepared by injecting mutations in an application), which is
the reason to have fewer distinct but more frequent tokens in Corpus2 as
compared to Corpus1.

On ﬁnding that models built from features extracted from BOW represen-
tation of SmartApps does not work in all cases, we have proposed to extract
and use the tainted ﬂows as features for building the models with classiﬁers.
In the next section, we present extracting the ﬂows from the SmartApps and
using them as features.

3.2 Features Extraction- Tainted Flows traces

This section focuses on extracting ﬂows from the SmartApps which track the
sensitive information. Our approach uses text mining to extract the ﬂow of sen-
sitive information. The task is accomplished in four steps. First step includes
preparing BoW by splitting the source code into tokens and then calculating
their frequencies. The second step is to deﬁne a list of sensitive methods called
sinks. We have used the list of sinks identiﬁed by Celik et al. [8]. Third, a
list of sensitive information called sources is identiﬁed and the last step is to
use all of these to identify the ﬂows that can track a potential leakage of the
sensitive information.

To demonstrate the process, we have used the source code of a SmartApp
provided in the Listing 1 as an example. And the process needs to be applied
to all SmartApps of the dataset under test. All single-line and multi-line com-
ments are removed from the SmartApps to put the source code in a so called
standard format through a preprocessing stage which is described in detail in

Title Suppressed Due to Excessive Length

11

section 3.2.1. To identify ﬂows, each line of the source code is crawled. The
source code ﬁles are read line by line, and their tokens are generated. The deﬁ-
nition method call tokens are not included in this analysis because the method
call only contains metadata about a SmartApp. This step ends up in creating
a BoW of the source code as presented in the section 3.1.1.

3.2.1 Preprocessing the Source Code

The goal of this stage is to put the source code in a standard format. This
will reduce the number of cases to be handled by our implementation, and
thus reduces the complexity of our implementation for sensitive ﬂows identi-
ﬁcation. We name this process as preprocessing the source code. The process
of preprocessing consists of the following steps.

// sendPush (" Motion stopped ")

sendPush ( " Alarm motion detected " )

1 /* if evt . value is " active "
2 invoke sendPush method */
3 if ( evt . value == " active " ) { // the if satement
4
5 } // else {
6
7 // }
8
9 /* is modified to the following */
10
11 if ( evt . value == " active " ) {
12
13 }

sendPush ( " Alarm motion detected " )

Listing 7: Removing comments

1. All single line and multi line comments are removed.

This step is important because it allows to remove the text which is not
source code. Making comments part of the features will be incorrect be-
cause they are actually not part of behavior of a SmartApp. An example
for removing comments is presented in Listing 7.

2. Get multi line input method calls on one line.

The information required by a SmartApp from a user is deﬁned in the
pref erences method. The SmartApp needs to know the devices that it will
work with. The arguments of the input method call specify what a device
will be referred as in a SmartApp. The input method calls are grouped in
a section call which are statements in the pref erences method deﬁnition.

1 input (
2 name : " serverIp " ,
3 title : " In order for SmartThings to send commands back to the

SmartBlocks on your Minecraft server , you will have to enter

your Server Address " ,

4 type : " text " ,
5 required : false
6 )
7
8 /* is modified to the following */

12

Hajra Naeem, Manar H. Alalﬁ

9
10 input ( name : " serverIp " , title : " In order for SmartThings to send
commands back to the SmartBlocks on your Minecraft server ,
you will have to enter your Server Address " , type : " text " ,
required : false )

Listing 8: Preprocessing input method calls

The ﬁrst positional argument in the input method call is an identiﬁer for
a device in a SmartApp. This identiﬁer is used as a reference to the device
in all methods of the SmartApp. If it is a keyword argument, it can be at
any position in the input method call. Having all arguments of the input
method call on one line makes it possible to just analyse one line and
identify an identiﬁer which is used as a reference to a device. We call these
identiﬁers as sources of a SmartApp. Getting all parts of the input method
call on one line enables us to process the positional argument as a source by
extracting it from ﬁrst position of the arguments and in case of keywords
arguments, we prepare a Python dictionary for all the keyword arguments
and read the value of name key. An example of this step is presented in
Listing 8.

3. Get multi line map deﬁnitions on one line.

A map deﬁnition may contain an item which contains a source. We process
all items of a map to locate any sensitive information. Having all items
on one line makes it easier to analyse map items for sensitive information
by processing just one line of code. Otherwise, we will have to process
multiple lines to detect beginning and end of the source code of a map. If
some sensitive information is part of an item then we will have to go back
to the map identiﬁer which was processed on a previous line and mark it
as sensitive. An example of this step is presented in Listing 9.

1 takeParams =[
2 uri : " attacker . com " ,
3 path : " " ,
4 r eq u e s tC o nt en t Ty p e : " app /x - w ... " ,
5 body :[
6 " message " : " $ { switches } "
7 ]
8 ]
9
10 /* is modified to the following */
11
12 takeParams =[ uri : " attacker . com " , path : " " , r e q u e s t C o n t e nt T yp e : " app /

x - w ... " , body :[ " message " : " $ { switches } " ]]

Listing 9: Preprocessing map deﬁnitions

4. If there are multiple nested conditional statements on one line, split them

to have at most one conditional on one line.
To track the sensitive information ﬂows, considering that values of some
identiﬁers might be switching between sensitive to non sensitive informa-
tion. We need to identify the beginning and end of conditional blocks.
On identiﬁcation of beginning of a conditional, all source code after left

Title Suppressed Due to Excessive Length

13

curly brace is shifted to next line. An example for preprocessing multiple
conditionals on one line is presented in Listing 10.

1 if ( sendSms2 ( switches ,1) ) { if ( sendSms2 ( switches ,1) ) { sendPush (

switches ,1)

to the

modified

2
3 /* is
4
5 if ( sendSms2 ( switches ,1) ) {
6 if ( sendSms2 ( switches ,1) ) {
7 sendPush ( switches ,1)

following */

Listing 10: Preprocessing multiple conditionals on one line

5. If after the beginning of a conditional, a left curly brace on next line, get

it on the same line as presented in Listing 11.
We have used a stack to identify beginning and end of the conditionals. If
a conditional is found, we push a left curly brace into the stack. On ﬁnding
a right curly brace we pop from the stack. When the stack is empty that
means the body block of a conditional ends. If the ﬁrst line does not have a
left curly brace that means processing of next line will get an empty stack.
For simplicity, we ensure that every conditional line contains a left curly
brace.

1 if ( motion && minutes )
2 {
3
4 /* is
5
6 if ( motion && minutes ) {

modified

to the

following */

Listing 11: Preprocessing to get left curly brace on the same line

6. If there is a right curly brace outside of a function invocation, move all the
code after right curly brace to the next line as presented in Listing 12.

1 } else {
2
3 /* is modified to the following */
4
5 }
6 else {

Listing 12: Preprocessing to get code after right curly brace to next line

This step enables to easily identify the end of a block that appears before
a conditional by processing a right curly brace to pop an item from a stack
and beginning of the next block by pushing conditional along the right
curly into the stack. The stack enables to detect beginning and ending of
the conditional blocks.

7. If there are multiple statements on one line. Split on semi colon to have at

most one statement on one line, as presented in Listing 13.

14

Hajra Naeem, Manar H. Alalﬁ

The SmartApps are written in the groovy language. It is optional to end a
groovy statement on a semicolon. But, if we want to have multiple state-
ments on one line then we need to end all statements on semicolon with
the exception of the last statement on the line.

1 log . debug ( " sending text message " ) ; sendSms ( phoneNumber , msg )
2
3 /* is modified to the following */
4
5 log . debug ( " sending text message " ) ;
6 sendSms ( phoneNumber , msg )

Listing 13: Preprocessing to keep at most one statement on a line

We process the lines with multiple statements to have at most one state-
ment on a line. Without this step we will have to process all statements
on the line at once. Identify the start and end a statement which will
complicate processing of the statements.

We have done few similar processing for do, try, catch and f inally blocks in
the source code. The processing of these blocks is similar to above mentioned
points. Preprocessing of SmartApps source code makes feature and ﬂow mining
simpler. In the next section, we describe preparing the initial set of features
that we use for building the behavioral models of the SmartApps.

3.2.2 Sinks identiﬁcation

Our approach requires the identiﬁcations of the presence of tainted ﬂows. Some
tokens of the source code indicate presence of a potential tainted ﬂow, for ex-
ample, if an application contains sink or a token from sinks set, that application
may potentially contain a tainted ﬂow. If we ﬁnd a token which conﬁrms the
presence of a sink in an application, we investigate that application further to
identify tainted ﬂows. The set of sinks SN that we have used for this study is
provided in the Listing 14.

1 Sinks = { ’ s endNotification ’ , ’ httpHead ’ , ’ sendPush ’ , ’

sen dPushMessage ’ , ’ sendSms ’ , ’ sendSmsMessage ’ , ’ sendText ’ , ’
httpGet ’ , ’ httpPost ’ , ’ httpPut ’ , ’ httpPostJson ’ , ’
se nd Pu shAndFeeds ’ , ’ httpDelete ’ , ’ s e n d N o t i f i c a t i o n E v e n t ’ , ’
c on t a c tB o ok En a bl e d ’ , ’ httpPutJson ’ , ’
s e n d N o t i f i c a t i o n T o C o n t a c t s ’ , ’ inline_html ’}

Listing 14: Set of Sinks SN

If we check the tokens of D from the Listing 6. We can note that a sink
sendSms is present in the tokens. This means that there might be a tainted
ﬂow in the application and for that we need to investigate further. To identify
the tainted ﬂows, we prepare the set of sources as presented in section 3.2.3.

Title Suppressed Due to Excessive Length

15

3.2.3 Sources identiﬁcation

The information required from users by a SmartApp is deﬁned in the pref erences
method. On installation of a SmartApp, a user conﬁgures the devices a Smar-
tApp will be operating on. The SmartApp speciﬁes references of the devices
in arguments of the input method. These references are called sources by us.
The section call allows us to group the relevant inputs and give them a text
description. The section calls are located in the preferences method deﬁnition.
The preferences method tokens are crawled to identify the sources. If the
ﬁrst argument of the input method call is a positional argument; it is a refer-
ence that is used in a SmartApp to refer to an input device. If the arguments of
the input method call are keyword arguments then the identiﬁer of a device is
speciﬁed in the keyword argument named as name which can be at any index
in the method call. The value of name is an identiﬁer for the input device in a
SmartApp. The input device identiﬁers from the tokens of preferences method
are selected to form a set named as sources. The process of identifying set
of sources S from the preferences method deﬁnition is described in the func-
tion f indSources presented in Algorithm 2. The algorithm initializes the set
of sources S ← ∅ and crawls the tokens of the preferences method by going
through each line and adds a source found to S. Once all lines are crawled,
it returns a set of sources S for that application. The set of sources S for the
application in Listing 1 is {newM ode}.

Input: Tokenized code lines of a method
Output: Sources S in a method

Function findSources(tokensMethodCodeLines):

S ← ∅
for a line of code in tokensMethodCodeLines do

if ﬁrst token ft of line is ‘input’ then

if all arguments are keyword arguments then

ﬁnd value s of name argument
add s to S

end
else

ﬁnd ﬁrst positional argument s
add s to S

end

end

end
return (S)

Algorithm 2: Identify sources from preferences method

3.2.4 Method Invocations and Extended Sources Identiﬁcation

If a source is assigned to an identiﬁer that identiﬁer also becomes a source. We
name such identiﬁers as extended sources extS. For identiﬁcation of tainted
ﬂows, we also need to search for the extended sources. The set of extended

16

Hajra Naeem, Manar H. Alalﬁ

sources is maintained separately for every method a SmartApp because of the
scope of extended source identiﬁers. In the same method, an identiﬁer may
later get a value from a variable which is neither a source nor an extended
source, if this happens then the identiﬁer is removed from the extended sources
set.

Input: Tokenized code lines of a method, Sources S
Output: f uncSignature, extended sources set extS and method invocations

mthdInvocations

Function findExtS mtdInv(tokensMtdCodLines,S ):

extS ← ∅
mthdInvocations ← empty list
ﬁnd mthdSignature from ﬁrst line of code
for a line of code in tokensM tdCodLines do
if line is assignment statement then

ﬁnd identiﬁer eS and assigned expression val
if ∃s ∈ S ∪ extS : s = val ∨ s ∈ val then

add eS to extS

else

if eS ∈ extS remove eS from extS

end

end
if line contains a method invocation then

add line to mthdInvocations

end

end

return (mthdSignature,extS,mthdInvocations)

Algorithm 3: Identify extended sources and method invocations for every
method deﬁnition

All method invocations in the deﬁnition of a method of a SmartApp are
located despite of the locations of the invocations. A method can be invoked
directly from a method or a method invocation could be wrapped inside a
conditional. A method can be invoked from another method invocation or it
can also be a default parameter value in a method deﬁnition. The ternary
statements may also include a method invocation. All method invocations
mthdInvocations that are present in a method deﬁnition of an application
are saved against the name of that method. The function f indExtS mtdInv
ﬁnds elements of mthdInvocations, extS and returns along the name of the
method as described in Algorithm 3.

The set of extended sources extS identiﬁed by function f indExtS mtdInv
for the application in the Listing 1 is {takeActions : [(message, newM ode)]}.
Which means that if message is passed as a parameter to any method invo-
cation in the method takeActions, it is treated as if a source is passed to that
method. Lists of method invocations for all method deﬁnitions in the Listing
1 identiﬁed by f indExtS mtdInv are shown in Listing 15.

1 { def c r e a t e S ub s c r i p t i o ns () : [ subscribe ( switches , " switch . off " ,

sw it ch OffHandler ) ] ,

2

def installed () : [ c r ea t e S u b s c r ip t i o n s () ] ,

Title Suppressed Due to Excessive Length

17

3

4

5

6

private send ( msg ) : [ sendSms ( msg ) ] ,
def sw it chOffHandler ( evt ) : [ takeActions () ] ,
private takeActions () : [ s e n d N o t i f i c a t i o n T o C o n t a c t s ( newMode ) , send

( message ) , setLocationMode ( newMode ) ] ,

def updated () : [ unsubscribe () , c r e at e S u b s c r i p t i o n s () ]}

Listing 15: Method invocations list for every method deﬁnition in Listing 1

3.2.5 Extended Sinks Identiﬁcation

Our technique locates all method invocations which are a member of the Sink
SN and if an element from the set of sources S ∪extS is passed as a parameter
to that method invocation then it reports it as a tainted ﬂow. But, a parameter
p1 from signature of a method eSN deﬁned in an application could also be
passed to a method invocation which is a sink S. If the invocation of the
method eSN in deﬁnition of another method accepts an element from S ∪extS
as a parameter at the index of p1 then the method eSN is identiﬁed as an
extended sink method.

Input: List of parameters in method deﬁnition, method invocations from the

method

Output: data for extended sinks

Function findExtSink(params,mthdInvocations):

extendedSinkData ← empty dictionary
SN ← Sinks
for p in params do

for mtdIn in mthdInvocations do

if mtdIn ∈ SN ∧ p ∈ parameters of mtdIn then
add the following to extendedSinkData
p, index of p in params

end

end

end

return (extendedSinkData)

Algorithm 4: collect data for extended sinks

To identify elements of extended sink methods set, the signature tokens
of every method deﬁnition are crawled to locate parameters of that method.
Suppose M is name of a method. For every parameter p of M check all the
method invocations identiﬁed by the function f indExtS mtdInv described in
Algorithm 3. If any p of M is passed in the method deﬁnition to a method
invocation mtdIn and that invocation is for a sink SN then save M , p and
index of p in the extended Sinks set. Algorithm 4 returns this information
in extendedSinkData. When the invocation of method M in the deﬁnition
of another method accepts a source at the index of p then a tainted ﬂow is
reported. The set of extended sinks identiﬁed by f indExtSink for the appli-
cation in the Listing 1 is {send : [((0, msg), sendSms(msg))]}, the parameter

18

Hajra Naeem, Manar H. Alalﬁ

msg of the method send which is at the index 0 makes it extended sink be-
cause in the deﬁnition of the send method, msg is passed as a parameter to
the sink method sendSms. If method send is called from another method and
an element from S ∪ extS is passed as a parameter at index 0, It results in
passing a source to a sink.

3.2.6 Tainted Flows Identiﬁcation

Algorithm 5 focuses on identifying tainted ﬂows. It initializes S with empty
set, allM thdsData, extendedSinks and taintedF lows with empty dictionary.
All information extracted from each method deﬁnition like method invocations
and extended sources, is saved in allM thdsData. The identiﬁed extended sinks
are saved in extendedSinks and set of tainted ﬂows in taintedF lows.

The algorithm starts by creating tokens of the preprocessed source code.
The preprocessing is described in the section 3.2.1. The def inition method
call tokens are not included in the analysis because it is only metadata of
a SmartApp. The tokens are created for each line and the lines of tokens
are analyzed at a method level because of the scope of local variables of the
method. The tokens for the code lines of the pref erences method are saved in
tokensP CodeLines, whereas rest of the tokens are saved in tknsCodeLines.
The frequencies of tknsCodeLines tokens are calculated by using the function
calT okenF requencies and are saved in D. The algorithm checks that if a
token from set of sinks SN is present in the tokens of D then, the algorithm
proceeds to search for the tainted ﬂows, otherwise, the algorithm terminates.
If a sink is found in the tokens of D, the algorithm proceeds to the next
step and crawls the tokens of tokensP CodeLines line by line to identify set
of sources S by calling the f indSources method. Tokens for lines of codes of
every method tM tdCodLines are taken from tknsCodeLines and are passed
step by step to the function F indExtSM thdInv to identify extended sinks
and method invocations for every method, which are saved in allM thdsData.
Once this process is complete for all methods, the algorithm processes data
for each method from allM thdsData by using the function f indExtSink to
identify set of extended sinks extendedSinks and the parameter index of the
parameters that make a method extended sink. The algorithm again analyses
the method invocations from allM thdsData for each method and compares
those invocations with the set of sinks SN and extended sinks extendedSinks
to identify any matches. If an invocation inv belongs to a sink method SN ,
the algorithm checks if an element p from S ∪ es is passed as a parameter to
inv. If the algorithm is successful, it adds method name, inv and p to tainted
ﬂow taintedF lows. Here, es is the set of extended set of sources which is
diﬀerent for each method. In parallel, the algorithm also checks if a method
invocation inv matches to an extended sink eSN from extendedSinks, if the
algorithm is successful then it goes on to check if a parameter p from S ∪ es
is passed as a parameter to invocation inv and index of p in inv is same as
the index of a parameter which makes the method eSN an extended sink. If
all constraints are met, the algorithm adds the method name, inv and p to

Title Suppressed Due to Excessive Length

19

Input: Source code of an application, set of Sinks SN
Output: Tainted Flows

begin

S ← ∅, allM thdsData ← empty dictionary
extendedSinks ← empty dictionary
taintedF lows ← empty dictionary
create tokens for each line of preprocessed source code
create tknsCodeLines by ignoring the following

lines for def inition method call

save pref erences code lines in tokensP CodeLines
D ← calTokenFrequencies(tknsCodeLines)
setT okens ← get all tokens from D
if SN ∩ setT okens (cid:54)= ∅ then

S ← ﬁndSources(tokensP CodeLines)
for tMtdCodLines in tknsCodeLines do

x ← FindExtS MthdInv(tM tdCodLines)
add x to allM thdsData

end
for aMethodData in allMthdsData do
ms,es, mInvs ← aM ethodData
// ms is method Signature,
// es is set of extended sources,
// mInvs is method Invocations
get parameters params from ms
p, ip ← ﬁndExtSink(params,mInvs)
add ms, p, ip to extendedSinks

end
eSN ← all method names from extendedSinks for aMethodData in

allMthdsData do

ms, es, mInvs ← aM ethodData
∀inv : inv ∈ (mInvs ∩ SN )

∧∃p : p ∈ (parameters(inv) ∩ (S ∪ es))

add (ms → inv → p) to taintedF lows
∀inv : inv ∈ (mInvs ∩ eSN ) ∧∃p : p ∈ (parameters(inv) ∩ (S ∪ es))

∧index p(inv) ∈ extSinkIndices(inv)

add (ms → inv → p) to taintedF lows

end
return taintedF lows

end

end

Algorithm 5: Tainted Flows Identiﬁcation

tainted ﬂow taintedF lows. The tainted ﬂows identiﬁed by the Algorithm 5
from Listing 1 are presented in the Listing 16.

1 Tainted Sink Flow : Method : takeActions -->

s e n d N o t i f i c a t i o n T o C o n t a c t s ( newMode ) --> newMode

2 Tainted ExtSink Flow : Method : takeActions --> send ( message ) -->

message --> newMode

Listing 16: Tainted sink ﬂows identiﬁed for code in Listing 1

20

Hajra Naeem, Manar H. Alalﬁ

3.3 Features Preparation-Tainted Flows

3.3.1 FlowsMiner

The Python 3 implementation of Algorithm 5 is called FlowsMiner. The im-
plementation processes the source code of a SmartApp as described in the
section 3.2.1 before tokenizing the source code. This reduces the number of
cases to be handled at each step while identifying the ﬂows. The ﬁrst step
involves identifying sources of a SmartApp as described in section 3.2.3. If the
arguments of the input method call are positional arguments then the ﬁrst
argument is selected as a source, otherwise, the implementation prepares a
Python 3 dictionary object for the keyword arguments of the input method,
and then reads the value of name key. Dictionary is a suitable data structure
for keyword arguments because the keyword arguments are separated with
colons. Once the sources set is prepared, the sinks are read from a ﬁle named
as Sinks.txt. For our research, we have used a ﬁxed set of sinks. The Flows-
Miner provides us ﬂexibility to provide any set of sinks and the tool is capable
of handling the change. In case we want to use a diﬀerent set of sinks, all we
have to do is change the set of sinks in Sinks.txt. Changing them in the ﬁle
will have impact on the whole analysis and the updated sinks set will be used
for identiﬁcation of tainted ﬂows. After identifying the extended sources and
extended sinks the tainted ﬂows are identiﬁed as described in Algorithm 5.

The usage of FlowsMiner is fairly straightforward, one needs to provide
source code and the tool outputs the tainted ﬂows if there are any. A sample
output of FlowsMiner for the source code presented in Listing 1 is provided in
the Listing 16.

3.3.2 Flow2Vec

Our Python 3 implementation Flow2Vec maps the tainted ﬂows identiﬁed
by our text mining taint analysis approach described in Algorithm 5 to the
numeric feature vectors. To map the tainted ﬂows to vectors, Flow2Vec cal-
culates occurrences for each type of ﬂow and records them as frequencies for
the ﬂows. The Flow2Vec analyses the ﬂows identiﬁed by Algorithm 5 and cat-
egorises them into six types of tainted ﬂows named as F Ls. The six categories
of F Ls are: from a source to a sink, from a source to an extended sink, from
an extended source to a sink, from an extended source to an extended sink, a
sink in body of a tainted conditional, an extended Sink in body of a tainted
conditional. The frequencies for these ﬂows are denoted by f1, f2, f3, f4, f5
and f6, respectively.

The set F Ls is mapped to distinct numbers by mapping ﬁrst element to
0, second element to 1, third element to 2 and repeating the process for all
elements. These numbers are the indices of these elements in the columns of
the feature vectors table and the elements are the names of the columns. The
rows are labelled with the names of the SmartApps.

Title Suppressed Due to Excessive Length

21

For each SmartApp, we loop through the elements of the F Ls set, if an
element matches to a token in F Ls, it’s frequency is selected accordingly. We
populate all ﬂows frequencies of an application in the corresponding columns
of the table and if the corresponding tokens are not found from F Ls, zeros are
ﬁlled. This process is repeated for all SmartApps to prepare a numeric vector
of ﬂows for each application.

3.4 Models Building

To build models of SmartApps under test, we have used a set of machine
and deep learning algorithms including Logistic Regression, Naive Bayes, De-
cision Tree, Random Forest, K-Nearest Neighbors, Support Vector Machine
(SVM), Multilayer Perceptron (MLP), Convolutional Neural Network (CNN)
and Long short-term memory (LSTM). The training sets to build the behav-
ioral models of the SmartApps are prepared from the datasets presented in
the section 3.4.1. Machine learning algorithms are trained on these datasets
to build models. We use these models to identify vulnerable applications. The
built model is a function predict(x), where x is a source code of a SmartApp.
The function returns “vulnerable” if the SmartApp is vulnerable and “non-
vulnerable”, otherwise. The evaluation metrics Area under the ROC Curve
(AUC), F1 Score, Matthews correlation coefﬁcient (MCC), accuracy, preci-
sion and recall are used to evaluate the built models.

3.4.1 Dataset Selection

We have used two datasets to evaluate the proposed techniques. One dataset
is prepared by Celik et al. by collecting SmartApps from marketplace, commu-
nity, SmartThings forum and IoTBench [8]. This dataset contains 217 Smar-
tApps and we name it as Corpus1.

Fig. 2: Ratio of Vulnerable and Non-Vulnerable SmartApps in Corpus1

22

Hajra Naeem, Manar H. Alalﬁ

From these 217 SmartApps, 101 are labelled as Non-Vulnerable, whereas,
116 are labelled as Vulnerable. The ratio of vulnerable and non-vulnerable
SmartApps in the dataset is shown in Figure 2.

Corpus2 was created by Parveen and Alalﬁ [17] where they altered the
order of statements in benign SmartApps to introduce vulnerable apps that
leak sensitive information. Mutations created by them mainly focused on gen-
erating tainted ﬂows vulnerabilities by changing the order of statements in the
sequential code ﬂow. The ﬂow insensitive analyzers can not capture these vul-
nerabilities, this case is presented in the background section, Listing 2. Other
mutants generated generated by them introduced changes to the conditional
statements so it can produce vulnerabilities that path insensitive analyzers can
not detect, this case is presented in the background section, Listing 3. Finally
the third category of mutants introduced by them changes to method calls so
that vulnerabilities related to context are produced, this case is presented in
the background section, Listing 5. Corpus2 consists of 1186 SmartApps, from
these SmartApps 858 are Vulnerable and 328 are Non-Vulnerable. The ratio
of these SmartApps is presented in Figure 3.

Fig. 3: Ratio of Vulnerable and Non-Vulnerable SmartApps in Corpus2

To understand both of the datasets, lets perform an exploratory analysis
for the distribution of sinks in the datasets. Figure 4 shows the number of
SmartApps with any of the speciﬁc sinks for both datasets. Corpus2 has only
4 types of sinks and number of SmartApps with any of these are presented in
the ﬁgure. Whereas, Corpus1 has 14 types of sinks and in the ﬁgure only the
number of SmartApps with 4 most frequent sinks are presented. We can note
that 92 SmartApps have sendP ush and 86 SmartApps have sendSms sink.
Whereas, for Corpus2 sendSms and httpP ost are the most frequent sinks.
The number of SmartApps with sendSms sink is 483 and httpP ost is 482.

In the Figure 5, we have presented the frequency distribution of two most
frequent sinks for Corpus1 by showing a swarmplot. It can be noted that

Title Suppressed Due to Excessive Length

23

(a) SmartApps in Corpus1

(b) SmartApps in Corpus2

Fig. 4: Number of SmartApps with Speciﬁed Sinks in Corpus1 and Corpus2
Datasets

vulnerable SmartApps tend to have more instances of sinks. We have only
6 non-vulnerable SmartApps that have one or more instances of sendP ush
and 2 SmartApps with any instance of sendSM S sink. More ﬁgures about
distribution of sinks for this dataset are provided in the Appendix A. It can
be noted that the frequency distribution of sink tokens across vulnerable and
non-vulnerable SmartApps is different.

Fig. 5: Frequency distribution of SendPush w.r.t sendSMS in all SmartApps
of Corpus1

The swarmplot for the SmartApps with most frequent sinks for Corpus2

is shown in the Figure 6.

It can be noted for two most frequent sinks of Corpus2 that the distribu-
tion of sinks across the vulnerable and non-vulnerable SmartApps is similar.

24

Hajra Naeem, Manar H. Alalﬁ

Fig. 6: Frequency distribution of SendSMS w.r.t httpPost in all SmartApps of
Corpus2

Same is supported from the ﬁgures presented in Appendix A for the distri-
bution of sinks in Corpus2. The analysis of sinks is important and it can be
concluded that presence of sinks is an important indicator for a SmartApp to
be vulnerable. The vulnerable SmartApps may have more frequent instances of
sinks. Similar analysis can be done to identify more tokens whose distribution
across vulnerable and non-vulenrable SmartApps is different.

3.5 Models Testing

Learning a model with a machine learning algorithm is a complex task. Each
instance of data contains knowledge and is useful for improving the model.
Evaluation of a model helps to report its performance. A dataset that is previ-
ously not known to the model is required for the evaluation. The data should
have neither been used for building the model nor for tuning features of the
model. This can be done by dividing the historical data in two folds and use
one fold for training the models with machine learning algorithms and the
other for testing the models. This technique is called hold out cross validation
which is an approximation of k-fold cross validation. More data points are
kept in the training set for building models and relatively fewer data points
are kept in the testing set.

For training the machine learning algorithms to build models, the dataFrame
prepared from set of vectors prepared in section 3.1.2 and section 3.3.2 is split
in training and testing sets using stratiﬁed holdout cross validation. The algo-
rithms are trained on the training set and then the test set is used to evaluate
the built models. We have used the train test split method from the sklearn
library [21] which is an implementation of hold out cross validation.

Title Suppressed Due to Excessive Length

25

To compare the performance of machine learning algorithms we have en-
sured that the same training data is used to build the models for all machine
learning algorithms and we have tested the built models on the same strati-
ﬁed hold out test data for all the algorithms. In order to ﬁnd the best split
in training and test data, we have tried various combinations of splitting the
data starting from 51% for training and 49% for testing to 95% for training
and 5% for testing. We ran the same experiment for all algorithms and found
that all machine learning algorithms on the average perform well on the split
of 70% for training and 30% for the testing. These sets are stratiﬁed on the
labels of SmartApps which is achieved by setting the stratif y parameter to
T rue in the train test split method.

3.5.1 Evaluation Metrics

The built models are used to predict vulnerabilities in the SmartApps of the
test set and the ﬁndings about the predictions are saved in a confusion matrix.
All measures of a confusion matrix are shown in the Table 1.

Table 1: Confusion Matrix

Predicted Class
Vulnerable Non-Vulnerable

Actual Class

Vulnerable
Non-Vulnerable
Total

T P
F P
T P + F P

F N
T N
F N + T N

Total
T P + F N
F P + T N
N

A confusion matrix is a table which comprises of the following four mea-
sures: True Positives (TP), True Negatives (TN), False Positives (FP) and
False Negatives (FN).

TP: When the actual class is vulnerable and the predicted class is also vul-

nerable.

TN: When the actual class is non-vulnerable and predicted class is also non-

vulnerable.

FP: When actual class is non-vulnerable and predicted class is vulnerable.
FN: When actual class is vulnerable but predicted class in non-vulnerable.

To evaluate the learnt models, we have used the evaluation measures AUC,
F1 Score, MCC, and accuracy. Descriptions for all of these metrics are provided
in the following:

Area Under The Curve (AUC) The AUC metric shows how good a
model is at distinguishing between vulnerable and non-vulnerable applications.
A good model has an AUC closer to 1, whereas, a poor model has an AUC
near to 0.

26

Hajra Naeem, Manar H. Alalﬁ

Accuracy is a ratio of correct predictions to the total predictions on the

testing data.

Accuracy =

T P + T N
T P + T N + F P + F N

F1 Score The F1-Score is a weighted average of precision and recall and

can be calculated by using the following formula.

F1 =

T P

T P + 1

2 (F P + F N )

Matthews correlation coeﬃcient (MCC) The MCC is high only if pre-
diction is high in all confusion matrix categories. For this measure all classes
have same importance, if you change a negative class to be positive, the MCC
stays the same. It is calculated by using the following formula.

M CC =

T P × T N − F P × F N
(cid:112)(T P + F P )(T P + F N )(T N + F P )(T N + F N )

The maximum value of MCC is 1 when both FP and FN are zero which rep-
resents a perfect positive correlation and it is -1 when both TP and TN are
zero which represents a perfect negative correlation.

4 Evaluation

We have evaluated the proposed techniques by answering the following research
questions.

RQ1: How does automatically extracted features from the BoW of source code
perform when the BoW representations of vulnerable and non-vulnerable
applications are almost similar?

RQ2: Whether a text mining approach is capable of extracting tainted ﬂows
features from SmartApps? and how such approach compares to static anal-
ysis techniques?

RQ3: What would be the impact of adding the tainted ﬂows features auto-
matically extracted from text mining to automatically extracted features
from the BoW?

The questions are answered in the following sections.

4.1 Evaluation of “BOW based Machine Learning”

To address RQ1, this section presents an experiment using two datasets, the
ﬁrst dataset Corpus1 was captured from real world apps as described in sec-
tion 3.4.1. While the dataset in Corpus2 was produced from the benign subset
of Corpus1 by injecting cases of information leakage using a mutation analysis
framework. The goal of conducting the experiment in this section is to compare

Title Suppressed Due to Excessive Length

27

whether the BOW-based machine learning approach is capable of capturing
the vulnerabilities introduced in Corpus2. As explained in section 2.1, the
changes introduced to the dataset were mainly by altering the order of state-
ments in the code in order to produce the vulnerability that may leak sensitive
information. The process of preparing numeric feature vectors for Corpus1 is
initiated by preparing BoW from the source code of each SmartApp by using
the technique described in section 3.1.1. The BoW of SmartApps are converted
to feature vectors by using the tool Token2Vec presented in the section 3.1.2.
We have considered a token to be part of features if that is present in at least
4 SmartApps. The data prepared from feature vectors are split into training
and testing sets that are stratiﬁed on the class of SmartApps by using the
stratiﬁed hold out cross validation where the training data and the test data
are 70% and 30% of the total number of SmartApps.

Fig. 7: Confusion Matrices for Corpus1 Dataset

The set of machine learning algorithms listed in section 3.4 are tuned for
this experiment. We have used the Python implementations of these algo-
rithms available in the sklearn library [21]. We have used the solver (solver =
‘liblinear’) for Logistic Regression. For the Decision Tree, we have used en-
tropy (criterion = ‘entropy’). We have used 112 and 120 estimators for Ran-
dom Forest i.e. (n estimators = 112) and (n estimators = 120). For K-
Nearest Neighbors, we have used K as 4. For Support Vector Machine, we
have used a linear kernel i.e. (kernel = ‘linear’). We have used the Grid-
search for hyperparameters tuning to identify and use optimal parameters.
We have used 12 and 8 hidden layers and (solver = ‘lbf gs’) for the Multilayer
Perceptron. For CNN, we have used Conv1D and set epochs to 20 and 10, and
use relu as an activation with a 0.3 and 0.2 drop out rates. For LSTM, we

28

Hajra Naeem, Manar H. Alalﬁ

have used the sequential model with 20 epochs, and relu and softmax as the
activation function with 0.2 dropout rate.

The prediction results for the set of classiﬁers on the Corpus1, in the form
of confusion matrices, are shown in the Figure 7. From the confusion matrices,
it can be observed that the Decision Tree algorithm has identiﬁed most number
of true positive cases, whereas the random forest along Convolutional Neural
Network has identiﬁed most number of true negative cases. The LSTM algo-
rithm has most number of false negative cases and KNN has most number
of false positive cases. The Decision Tree algorithm has identiﬁed least num-
ber of false negative cases and Convolutional Neural Network algorithm has
identiﬁed least number of false positive cases. From the confusion matrices,
We can conclude that most of the machine learning algorithms performed well
considering the true positive and true negative cases.

Fig. 8: F1 Score, MCC and Accuracy for Corpus1 Dataset

The F1 Score, MCC and Accuracy are shown in the Figure 8. The Ran-
dom Forest algorithm performs the best considering the MCC and F1 Score,
whereas, Multilayer perceptron performs the best considering the accuracy
metric. The LSTM algorithm performs the worst considering all of the met-
rics. From Figure 9 and in terms of AUC, we can note that the 0.91 is AUC
for the CNN. We can also observe that most of the algorithms have a reason-
able performance with the Corpus1 dataset. The K-Nearest Neighbors and
LSTM algorithms were not able to perform like the others. We were able to
get these results with a little effort on hyperparameter tuning.

Title Suppressed Due to Excessive Length

29

Fig. 9: AUC for Corpus1 Dataset

We believe that performance of the algorithms that have not performed
well can be improved by feature engineering and hyperparameter tuning. The
K-Nearest Neighbors algorithm considers all features as equally important to
calculate the distances from K nearest neighbors to predict a target class. In-
creasing weights of important features and decreasing weights of less important
features may improve the AUC of this algorithm. Considering all the metrics
presented for this set of experiments, it can be concluded that BoW can be
successfully used to detect vulnerable SmartApps for the Corpus1.

Fig. 10: Confusion Matrices for Corpus2 Dataset

30

Hajra Naeem, Manar H. Alalﬁ

The second experiment is conducted on Corpus2. The BOW representa-
tion of vulnerable and non-vulnerable applications of this dataset end up with
almost similar tokens. We prepare the feature vectors data and divide the data
into training and testing data exactly like the ﬁrst experiment. We have used
the same threshold like the ﬁrst experiment to select a token to be a feature
for building the models. The models built with the classiﬁers are tested on
the testing data and the prediction results in the form of confusion matrices
are presented in the Figure 10. From the confusion matrices, it can be noted
that most of the classiﬁers are predicting the majority class. Support Vector
Machine is the algorithm which is able to predict the most number of correct
negative SmartApps.

Fig. 11: F1 Score, MCC and Accuracy for Corpus2 Dataset

For the Corpus2, F1 Score, MCC and Accuracy are shown in the Figure
11. It can be noted that the LSTM algorithm performs the best considering all
of these measures, but, this algorithm predicts only the majority class. It can
be noted from Figure 12 that the best performer is Support Vector Machine
with 0.56 AUC. The AUC of other algorithms is also very low, it is 0.31 in
one case. we can observe that the performance of the built models for all clas-
siﬁers deteriorates for the Corpus2. This answers the RQ1, the automatically
extracted features from the BoW of source code does not perform well when
BoW representations of vulnerable and non-vulnerable SmartApps are almost
similar. From the sink token analysis presented in section 3.4.1, we can note
that for Corpus1 frequency distribution of the sink tokens in vulnerable and
non-vulnerable SmartApps are different, whereas, it is almost the same for
Corpus2.

Title Suppressed Due to Excessive Length

31

Fig. 12: AUC for Corpus2 Dataset

We have learned from these experiments that we can use BOW of a Smar-
tApp to predict whether a SmartApp is vulnerable or non-vulnerable. But, it
can be problematic to identify the vulnerable SmartApps if the sequence of
some lines of source code change an application from non-vulnerable to vul-
nerable. If distribution of all tokens for both vulnerable and non-vulnerable
SmartApps are approximately the same then it becomes hard for a classiﬁ-
cation algorithm to distinguish between them only on the basis of features
extracted from BoW. This result demonstrates the need for ﬁnding more fea-
tures from the SmartApps that can help the machine learning algorithms to
build correct behavioural models. For that reason, we have proposed to extract
the tainted ﬂows using text mining and use them as features.

4.2 Evaluation of Tainted Flows Identiﬁcation Using Text Mining

In this section we answer research question RQ2. To report a SmartApp to
be vulnerable, we need to ﬁnd features which can help machine learning al-
gorithms to distinguish it from non-vulnerable SmartApps. In section 3.1, we
have presented a technique that uses BoW as features of a SmartApp. Af-
ter evaluating the technique on Corpus2, we noted that the approach may

32

Hajra Naeem, Manar H. Alalﬁ

fail when changing order of statements in a SmartApp makes the app non-
vulnerable, where initially it was vulnerable.

Tainted ﬂow analysis of SmartApps are required in such scenarios. Tools
like SAINT proposed by Celik et al. [8] perform static analysis to ﬁnd such
ﬂows. They build some dependency graphs and then used algorithms to reduce
the graphs to ﬂows. We have proposed a text mining alternative which is
less expensive as it requires less time to extract the ﬂows. The technique
focuses on tracking only the information that can end up in a tainted ﬂow. We
have evaluated the technique by implementing a tool, FlowsMiner, presented
in section 3.3.2. The experiments with FlowsMiner were conducted on the
datasets Corpus1 and Corpus2 presented in section 3.4.1. The experiments
were run on a laptop machine with Intel Core i5-7200 CPU @ 2.5GHz with a
8.00 GB RAM. The speciﬁcations of the machine are more or less similar to the
machine used by Celik et al. [8] for their experiments. The results calculated
by us are presented in the Table 2.

Table 2: Duration to identify Flows in both Corpus1 and Corpus2 Datasets

Dataset
Corpus1
Corpus2

Sc Sn
159
4

eSc Sn
35
858

Sc eSn
2
0

eSc eSn
61
0

Sn C eSn C Duration

145
7

50
0

∼17.5s
∼128s

It can be noted that the proposed technique is able to extract ﬂows from all
217 SmartApps of Corpus1 in around 17.5 secondes whereas Celik et al. report
that their static analysis tool requires 23±5 seconds for only one SmartApp of
the dataset Corpus1 [8]. The extraction of ﬂows with our technique for com-
plete data set takes less time than extracting ﬂows for one SmartApp using the
static analysis. The FlowsMiner is able to extract ﬂows from 1196 SmartApps
of dataset Corpus2 in around 128 seconds. The proposed text mining tech-
nique implemented as FlowsMiner to identify the ﬂows in SmartApps answers
the research question RQ2 by providing an alternative to the expensive static
analysis. The number of ﬂows identiﬁed by the FlowsMiner are presented in
the Table 2 by dividing them into 6 categories. Where source to sink, extended
source to sink, source to extended sink, extended source to extended sink, sink
in a body of tainted conditional and extended sink in a body of a tainted
conditional are denoted by Sc Sn, eSc Sn , Sc eSn , eSc eSn , Sn C and
eSn C, respectively.

4.3 Evaluation of Flows-aware Machine Learning

To answer RQ3 and observe the impact of combining tainted ﬂows features
with the BoW features, we run the experiments on Corpus1 and Corpus2

Title Suppressed Due to Excessive Length

33

datasets presented in section 3.4.1. The features set is prepared by converting
BoW to numeric vectors V as presented in section 3.1.2. The ﬂows are ex-
tracted using FlowsMiner tool presented in section 3.3.1 and are converted to
feature vectors F Ls using Flow2Vec tool presented in the section 3.3.2. The
vectors V and F Ls are combined together as V∪ F Ls. The output of this step,
numeric feature vectors for all SmartApps are used for training the machine
learning algorithms to build the behavioural models of the SmartApps. The
numeric feature vector datasets prepared for Corpus1 are split into stratiﬁed
training and testing samples on class variables of the SmartApps with 70%
and 30% samples, respectively.

Fig. 13: Confusion Matrices for Corpus1 Dataset

The set of machine learning algorithms are tuned to get best performance
results for this experiment. We have used the Python implementations of
these algorithms available in the sklearn library [21]. By hyperparameters
tuning, we found that for this experiment the Logistic Regression requires
(solver = ‘liblinear’) solver. For the Decision Tree, we have used entropy
(criterion = ‘entropy’). We have used 112 and 120 estimators for Random
Forest i.e. (n estimators = 112) and (n estimators = 120). For K-Nearest
Neighbors, we have used K as 5. For Support Vector Machine, we have used
a linear kernel i.e. (kernel = ‘linear’). We have used 4 and 9 hidden layers,
and (solver = ‘lbf gs’) for the Multilayer Perceptron. For CNN, we have used
Conv1D and set epochs to 20 and 13, and use relu as an activation with a 0.2
and 0.2 drop out rates. For LSTM, we have used the sequential model with 20
epochs and softmax as the activation function with 0.2 dropout rate.

The confusion matrices of this experiment are presented in the Figure 13.
From the confusion matrices, it can be noted that the Random Forest has the
most number of true positive and true negative cases. The LSTM is predicting
only the negative class.

34

Hajra Naeem, Manar H. Alalﬁ

Fig. 14: F1 Score, MCC and Accuracy for Corpus1 Dataset

From Figure 14, it can be noted that the Logistic Regression algorithm per-
forms the best considering the F1 Score and MCC, whereas, SVM performs
the best considering the Accuracy metric.

Fig. 15: AUC for Corpus1 Dataset with/without ﬂows

Title Suppressed Due to Excessive Length

35

The AUC calculated for all classiﬁers show that the Logistic Regression has
the highest AUC of 0.94 as shown in Figure 15. As such, we can observe that
the other algorithms also have high AUC with an exception of the LSTM. The
AUC comparison for the set of machine learning algorithms with the experi-
ments conducted without considering tainted ﬂows are presented in Figure 15.
It can be noted that if the tainted ﬂows are used for building and testing the
models, the AUC for all machine learning algorithms improves. This answers
research question RQ3. We conclude that: the identiﬁcation of tainted ﬂows in
the SmartApps and their usage for building models improves the performance
of machine learning algorithms to identify vulnerable SmartApps.

Fig. 16: Confusion Matrices for Corpus2 Dataset

To answer the same research question on a different dataset, we have con-
ducted our experiments on Corpus2, where authors of the dataset have in-
troduced some mutations to convert non-vulnerable SmartApps to vulnera-
ble. The changes were mainly focused on introducing structural changes to
the program such as statements reordering in the sequential program ﬂow,
the conditional ﬂow, and contextual ﬂow, i.e., method calls. Changes in some
cases would add few statements to the original program but those statements
belongs to the same vocabulary of the original code, as such, the BOW rep-
resentation for both original code and mutants of the SmartApps have al-
most similar tokens. The feature vectors are prepared exactly like we prepared
the feature vectors in our ﬁrst experiment presented at the beginning of this
section. Once the feature vectors are prepared they are split into training and
testing samples which are stratiﬁed on the class labels of the SmartApps. The
size of the training set is 70% and size of the test set is 30% of the size of
actual Corpus2 dataset. The results by training the set of machine learning
algorithms and then testing them on test sets are presented in the confusions
matrices presented in Figure 16. It can be noted that the most number of
true positive cases are identiﬁed by the K-Nearest Neighbors algorithm. We

36

Hajra Naeem, Manar H. Alalﬁ

can also note that the same algorithm identiﬁes most number of false positive
cases.

It can be noted that the algorithm is predicting the majority class. This is
because Corpus2 is not a balanced dataset and the algorithm is predicting by
considering neighbors of the test data. In Corpus2, the vulnerable application
are 3 times more than the non-vulnerable applications which can be noted
from the section 3.4.1.

We can also note that there are ﬁve algorithms that are predicting the
same number of true negative cases and the algorithm from these ﬁve with
high number of true positive cases is Support Vector Machine. Which makes
the Support Vector Machine, best predictor along CNN for this dataset. The
Support Vector Machine Algorithm has highest F1 Score and MCC and MLP
has highest Accuracy, as presented in Figure 17. The comparison for the AUC
of the machine learning algorithm’s by using the ﬂows features and without
using them is presented in Figure18. It can be noted that we have a significant
improvement on AUC for all machine learning algorithms. For the previous
experiment the best AUC was 0.56. For this experiment, AUC for all algo-
rithms is improved. There are two algorithms that have more than 0.9 AUC,
where CNN algorithm has the highest, 0.96. We can conclude that inclusion
of the tainted ﬂows features have significantly improved the performance of
the machine learning algorithms for this dataset. This answers our research

Fig. 17: F1, MCC and Accuracy for Corpus2 Dataset

Title Suppressed Due to Excessive Length

37

Fig. 18: AUC for Corpus2 Dataset with/without ﬂows

question RQ3 and we conclude that the identiﬁcation of tainted ﬂows is im-
portant and helps the algorithms to distinguish better between vulnerable and
non-vulnerable SmartApps. We can also note from the performance measures
that the gain with inclusion of ﬂows features for Corpus2 is more clear than
Corpus1.

5 Discussion

This research focuses on identifying vulnerable SmartApps using machine
learning algorithms. Feature selection is an important step for training the
machine learning algorithms. Manual features selection is a complex and time
consuming task. We opt to use an automated process of features selection. To
do so, a BOW is prepared from the source code of SmartApps and Token2Vec
is used to convert these BOW to feature vectors. This process automatically
prepares the set of features. To check the reliability of this technique, we use
the research question RQ1 to conduct experiments on Corpus1 and Corpus2.
We found that this technique works well for Corpus1 but fails for Corpus2.
This is due to the nature of Corpus2, where different versions of a SmartApp
are prepared by injecting tainted ﬂows in benign SmartApps using a muta-
tion analysis framework. The framework produces around 18 different versions
(mutants) for each SmartApp from the base benign Dataset. The BOW rep-

38

Hajra Naeem, Manar H. Alalﬁ

resentation of vulnerable and non-vulnerable versions of such SmartApps, in
many cases, is almost the same. For that reason, machine learning techniques
that rely on BOW for feature preparation fail to differentiate between vul-
nerable and non-vulnerable versions of SmartApps which is evident from the
experiments presented in section 4.1.

In such a situation, tainted ﬂows features can play an important role.
Therefore, we decided to use tainted ﬂows for distinguishing the vulnerable
SmartApps from the non-vulnerable ones like Avdiienko et al. [5]. We found
different tools from the literature that can identify ﬂows from source code of
SmartApps [5,8]. However, all of them require static analysis of the source
code. The static analysis of SmartApps is expensive and time consuming. To
investigate this problem, we have devised the research question RQ2. To an-
swer this question, we have proposed a text mining approach which was able
to identify the tainted ﬂows cheaply. Contrary to static code analysis, this
technique uses text mining approach to identify and remember only relevant
part of statements that may result in a tainted ﬂow. We have implemented
this technique to develop a tool called FlowsMiner. The experiments presented
in section 4.2 show that the proposed approach requires far less time than the
static analysis counterpart.

We then aimed to examine the use of ﬂows features to differentiate be-
tween vulnerbale and non-vulnerable SmartApps. This gives rise to our re-
search question RQ3. To answer this research question, the ﬂows identiﬁed
from FlowsMiner are converted to feature vectors by using the Flow2Vec. We
have combined both types of features and are able to improve the AUC for
all of the considered machine learning algorithms. The best case AUC for the
Corpus1 and Corpus2 data sets improve by 0.03 and 0.4, respectively.

As discussed above, we conducted our experiments for the evaluation of
the proposed techniques on two datasets. The dataset named Corpus1 has
an approximately balanced number of examples for each class label, whereas
Corpus2 examples are imbalanced. We want to split the dataset into train and
test sets in a way that preserves the same proportions of examples for each
class as observed in the actual dataset. We want to ensure that the training
and testing sets represent the actual dataset. However, splitting the data into
training and testing by randomly selecting data points is not a good choice
as it can end up in placing more representation of one class in a subset and
less representation in the other subset as compared to the actual data which
will end up with an estimate of the model error in high variance. As such,
we have opted for splitting the data with stratiﬁcation because stratiﬁcation
ensures that each subset of the data represents the whole data. All the splits
in training and testing were stratiﬁed on the class variable to have the training
and test sets as best representatives of the actual data.

In our experiments we want to achieve conﬁdence and stable estimates
about the performance of the built models. In a try to ﬁnd a method that is a
good compromise between complexity and accurate performance estimates, we
found that the stratiﬁed holdout cross validation is the best ﬁt. We tried vari-

0

Hajra Naeem, Manar H. Alalﬁ

ous validation methods including repeatedly running experiments with strati-
ﬁed 10-fold cross validation but we always ended up approximately with simi-
lar gain in performance estimates. We were not getting an advantage for using
computationally more expensive techniques since both validation methods are
consistent with each other and with our ﬁndings that the added ﬂows features
improved the prediction accuracy, therefore, we decided to stick with a rela-
tively less expensive validation technique, stratiﬁed holdout cross validation.

While the focus of this research is to devise a technique to identify vulnera-
ble SmartApps, the proposed technique is universal and can be used to identify
vulnerable applications developed in other programming languages. However,
it would require some tuning for adapting the proposed technique which in-
cludes sets of sensitive sources and sensitive sinks in addition to adaptation for
the extraction of the tainted ﬂows. An immediate future work would extend
our research to evaluate the proposed techniques on applications developed in
other programming languages and frameworks.

Our approach operates at App level, meaning that the approach predicts
whether the app as a whole is vulnerable or not. The granularity of the analy-
sis, however, is done at method level, since we extract vulnerable ﬂows features
at sequence ﬂow, conditional ﬂow, and method scope/context ﬂow. One could
wonder whether our approach can operate at another granularity level, such
as class level, (Motivation: If an app is vulnerable, it would be more helpful for
a developer if a tool/technique tells them which class is vulnerable rather then
saying that the App (whole) is vulnerable), however, this is not possible for
SmartApps, since developers can not create their own classes in SmartApps.
SmartApps are developed in a sandbox environment, and for performance
and security reasons, developers are limited to a speciﬁc subset of Groovy
language. while this seems to be a significant restriction, however, with the
various methods available to developers, the need to create their own classes
is rarely needed [57].

However, for our approach, and since our solution can be generalized to
applications in other languages that may allow/have multiple classes, we think
our approach can operate at class level. In order to extract features for appli-
cations that have classes, we need to extract features in the form of tainted
ﬂows and source code token frequencies for every class and then merge them
to serve as features of that application. A class can consist of one method or
multiple methods. We need to extract features in the form of tainted ﬂows and
source code token frequencies for every method and then merge them to serve
as features of the class. If the feature set of an application is prepared and is
provided to a model built with our approach and is predicted as vulnerable. A
developer might be interested in identifying a vulnerable class, if an applica-
tion has only one class, the answer is straightforward. In the case of multiple
classes, a feature set for each class can be provided to the model one by one to
ﬁgure out the vulnerable classes and the same can be done for the methods.
The process depends on the developer’s decision on which level of granularity
they want to operate. All they have to do is prepare features set for that level.

0

Hajra Naeem, Manar H. Alalﬁ

Table 3: Examples of vulnerabilities caused by tainted ﬂows,
(?) refers to the absence of speciﬁc sanitization function for those vulnerabilities, other
preventive techniques such as while list is required

The proposed technique works ﬁne for the SmartApps whose BOW to-
kens or their frequencies can be used to distinguish the vulnerable SmartApps
from the non-vulnerable ones. If the SmartApps have information leakage vul-
nerability then the tainted ﬂows can be used to distinguish the vulnerable
SmartApps. This technique should work ﬁne where the tainted ﬂows are iden-
tiﬁable by the FlowsMiner. In addition, it can be adapted to handle tainted
ﬂows results obtained from taint ﬂows static analysis tools.

-If we have vulnerabilities in the applications other than the information
leakage vulnerability, such as: Cross site scripting (XSS), SQL Injection, Com-
mand Execution, Remote File Inclusion, File System Access, or Malicious Eval-
uation. The FlowsMiner can be adapted to detect these vulnerabilities. All it
needs is the set of sources and sinks presented in Table 3 and adaptations to
track ﬂows for a different language. The FlowsMiner tool uses text mining to
identify the ﬂows, these adaptations should be straightforward for most of the
languages.

5.1 Threats to validity

The potential limitations along with mitigation strategies of this empirical
study are discussed in this section. Imbalanced datasets can have an impact
on internal and construct validity. Due to fewer data points for the minor-
ity class, the algorithms can generate suboptimal models. Balanced datasets
can be generated by randomly eliminating instances of the majority class from
the dataset or by synthesizing new examples from the minority class by us-
ing the synthetic minority oversampling technique (SMOTE) but it does not
always end up in performance gain [56].

We have not applied any data balancing technique and have used stratiﬁed
holdout cross validation to preserve the same class distribution in the train-
ing and test sets. Another possible threat for the construct validity is about

Vulnerability Type Sources(SO) Sinks(SI) Sanitizers(SA) Cross-site scripting (XSS) $_GET, $_POST, ... echo, print, printf, ... htmlentities, strip_tags, ... SQL injection $_GET, $_POST, ... mysql_query, pg_query, ... addslashes, pg_escape_string,... Command Execution $_GET, $_POST, ... exec, system, passthru, ... escapeshellcmd, escapeshellarg, ... Remote File Inclusion $_GET, $_POST, ... include, include_once, require, require_once, ... ? File System Access $_GET, $_POST, ... chdir, mkdir, rmdir, rename, copy, chgrp, chown, chmod, unlink, ... ? Malicious Evaluation $_GET, $_POST, ... eval, preg_replace, ... ?  Title Suppressed Due to Excessive Length

1

labeling the SmartApps. We have used the SAINT and Taint-Things tools to
label the SmartApps for Corpus1. Static analysis tools may be imprecise and
result in incorrect labeling in some cases, as such, we have manually reviewed
the apps to conﬁrm the assigned labels. We are conﬁdent about the labels of
Corpus2, as the vulnerable apps were created using a mutation framework
and the framework developers have provided the SmartApps along with the
labels.

-Threat to external validity can be related to the generalizability of our
proposed technique. In this paper, our dataset is based on Groovy language
and its usage in SmartApps. We have used datasets named as Corpus1 and
Corpus2 for estimating the performance of the proposed technique. Adapt-
ing the proposed technique for another dataset developed in a different pro-
gramming language may end up with different results. We have not tested
the proposed techniques on various projects developed in other programming
languages. To be more conﬁdent about our ﬁndings, it would be interesting
to test the proposed techniques on larger datasets and projects developed in
other programming languages. Although, we believe that the proposed ap-
proach is generalizable for the applications developed in different program-
ming languages. Theoretically, the proposed technique will be more reliable
once tested in other settings with applications developed in different program-
ming languages. Another threat to the validity could be about the choice of
selected machine learning algorithms. We have selected the algorithms that
have been frequently used in other studies [4].

For the conclusion validity, it can be noted that we have used well-
established metrics including AUC, F1-score and MCC for the performance
estimates. We have compared the proposed technique with the base Bag of
Words (BoW) approach to show the gain. There is no bias in the performance
estimates due to the opted stratiﬁed hold out cross validation because the gain
for the proposed technique for our experiments is similar to repeated stratiﬁed
k-fold cross validation. We have provided the datasets used for the experiments
on Github and the implementation details to address the validity threat about
the reliability and reproducibility of our ﬁndings. If the technique is applied
along with the provided settings, the same results will be obtained.

6 Related Work

This research presents an approach for identiﬁcation of vulnerable IoT ap-
plications using machine and deep learning techniques. We have focused on
the IoT applications developed in SmartThings. In the absence of literature
available on the topic, we explored the closest relevant ﬁeld that is identifying
vulnerable android and web applications using machine learning algorithms
[45, 46]. Most of the techniques extract a set of features from web, android or
other types of applications and then use the features to train machine learning
algorithms to identify vulnerable applications [1, 4, 7, 47, 48].

2

Hajra Naeem, Manar H. Alalﬁ

In this section, we have reviewed the literature to explore how researchers
have addressed similar problems. We start by reviewing the techniques to ex-
tract features using static analysis, then by extracting features using hybrid
analysis and text mining.

6.1 Identifying Features using Static Analysis

Zhu et al. build an android malware detection tool called DeepFlow [11]. They
have used FLOWDROID [12] to extract data ﬂows from sensitive sources to
sensitive sinks in android applications. They have applied the SUSI technique
for features transformation from method to categories level. They choose 3000
benign apps and 8000 malicious apps for running their experiments. Once
the dataset is ready they have applied deep learning techniques for building
models to identify malicious applications. Similar techniques are applied to the
web applications. Medeiros et al. propose a technique to automatically detect
web vulnerabilities using machine learning [10]. They have implemented their
approach in a tool called DEKANT. They have used Hidden Markov Models
(HMM) to learn models. A HMM is composed of set of symbols or tokens called
vocabulary, set of states, probabilities set that includes initial probabilities,
transition probabilities and emission probabilities. The model built by them
keeps a track of the order of code slices named as observation. The model learns
the state of observations with the intent to expose hidden state. Then the states
are checked to be vulnerable or non-vulnerable. Methods of a web application
can also be used to infer a model to report vulnerabilities in that application.
Shar et Tan took a set of web apps and extracted sanitization methods, divided
them into diﬀerent categories and used them to train machine learning models
to predict SQL injection and cross site scripting vulnerabilities [2].

Deciding about features to be extracted from software applications and
then selecting features for training a suitable model is a time consuming task.
Zhao et al. introduced a tool named Fest [6] to extract meaningful features from
android applications and evaluated which set of features are more eﬀective
in predicting malicious applications. Avdiienko et al. proposed to distinguish
between malicious and benign android applications on the basis of diﬀerent
ﬂows [5]. The name of their tool is MUDFLOW which uses FLOWDROID
[12], a static analysis tool to identify ﬂows in the android applications. They
compared the ﬂow of sources to sinks for malicious applications with benign
applications. They claim that their approach is good for prediction of any
application which is having a novel malware. They used FlowDroid for the
extraction of ﬂows from diﬀerent applications and then categorized those ﬂows
according to SUSI categories. Then they trained models on attributes of benign
applications and compared test applications to be malicious or benign. We can
ﬁnd other contributions that use ﬂows for identiﬁcation of vulnerable appli-
cations. SUI et al. have used static analysis to identify context-sensitive and
alias-aware data ﬂows [52]. They name their technique as Flow2Vec. They con-
sider all ﬂows regardless of if they are tainted or not. They compile a program

Title Suppressed Due to Excessive Length

3

to low-level intermediate representation (LLVM-IR) and build interprocedu-
ral value-ﬂow graph (IVFG) on top of that using Andersens pointer analysis
[54]. Then they prepare a matrix from IVFG. They formulate the ﬂow reach-
ability by using the matrix multiplication and the outcome is decomposed to
source and target vectors for every node of IVFG. Kim et al. have designed a
speciﬁcation language for describing ﬂow patterns of software vulnerabilities
[51]. They have implemented a vulnerability ﬂow detector that matches the
predeﬁned syntactic patterns with source codes. Their static analysis solution
constructs ﬂow graphs to identify ﬂows that are indicators of vulnerabilities.
The set of features used by each of the research papers discussed in this section
is presented in the Table 4.

6.2 Identifying Features using Hybrid Analysis

Shar et al. enhanced their previously presented approach [2] by using hybrid
attributes comprising of both static and dynamic, instead of just static at-
tributes [3]. After applying diﬀerent machine learning techniques they found
that the results were better with hybrid attributes. The features set used by
them is provided in the Table 4.

6.3 Identifying Features using Text Mining

Scandariato et al. identify vulnerabilities in android software components us-
ing text mining of source code [4]. They report a software component to be
vulnerable, if it has any warnings. They convert source code of a software
component to BoW and each word is considered as a feature that they use for
predicting the warnings and vulnerabilities. The set of features that is used for
building the predictor models is not ﬁxed because it depends on the vocabulary
of the source code. They have used Weka implementations of Decision Trees,
K-Nearest Neighbours, Na¨ıve Bayse, Random Forest and Support Vector Ma-
chine to conduct the experiments. Walden et al. provide 223 vulnerabilities
that they identiﬁed in three open source web applications written in PHP [1].
They have prepared a dataset comprising of software metrics including lines of
code, number of functions, maximum nesting complexity, cyclomatic complex-
ity, fan in and fan out to compare two vulnerabilities identiﬁcation techniques.
One based on dataset of software metrics and other based on the mined text
BoW representation. They report that the text mining technique performed
better than the software metrics.

We can save time spent on feature engineering by adapting techniques like
text mining which can automatically prepare set of features. A summary of
the features used for identifying vulnerable applications by diﬀerent researchers
discussed in the previous sections is presented in the Table 4. After exploring
the related work presented in this section, we were not able to ﬁnd a set
of features that can be used to detect vulnerable SmartApps. We opted to

4

Hajra Naeem, Manar H. Alalﬁ

extract all possible features and used all of them to train the machine learning
algorithms for building the behavioural models like we have done in [20].

When the performance of models is not satisfactory, we can select subset
of these features by using domain knowledge, adjusting weights of features,
feature selection or feature elimination techniques. We have used text mining
to prepare the features set for our research.

Features identified by Static Analysis Approach Platform Tools Features Zhu et al. [11] Android DeepFlow (Produced) Source categories: unique identier, sms mms, location information, image, email, synchronization data, nfc, contact information, system setting, calendar information, no category, network information, file information, account information, database information, browser information, Bluetooth information Sink categories: location information, phone connection, voip, phone state, email, bluetooth, account setting, audio, log, le, synchronization data, network, sms mms, contact information, calendar information, system setting, nfc, browser information, no category Medeiros et al. [10] Web DEKANT (Produced) Preparing a Vocabulary, a set of Symbols or Tokens, a set of States, Parameters, a set of Probabilities, Start-State or Initial probabilities, the transition probabilities and the emission probabilities. Shar et Tan. [2] Web PhpMinerI (Produced) WEKA  (Used) Sinks: $ GET, $ POST, $ COOKIE, fgets(), mysql result(), $ SESSION, $img size, mysql query(), print(), echo Sanitization methods: mysql real escape string(), htmlentities(), explode(), convert uuencode(), crypt(), sha1(), str replace(), preg replace(), intval(), substr(), html entity decode(), urldecode(), stripslashes(), localeconv(), mysql eld len() Avdiienko et al. [5] Android MUDFLOW (Produced) FLOWDROID (Used) Categories of the features are as follows:                                                                   Sources: hardware info, unique identier, location information, network information, account information, email information, file information, bluetooth information, voip information, database information, phone information, content resolver, no sensitive source Sinks: phone connection, voip, phone state, email, Bluetooth, account settings, synchronization data, network, email settings, file, log, intent, no sensitive sink Shared: Audio, sms mms, contact information, calendar information, system settings, image, browser information, nfc Zhao et al. [6] Android FEST (Produced) AppExtractor (Produced) Five most typical features in three categories are: Top5 Permissions: p.SEND SMS, p.READ SMS,p.RECEIVE SMS, p.WRITE SMS, p.INSTALL PACKAGES  Top5 APIs: SmsManager.getDefault, SmsManager. sendTextMessage, WifiManager. setWifiEnabled, SmsMessage. createFromPdu, Component- Name.getShortClassName Top5 Actions: a.SIG STR, a.BATTERY CHANGED ACTION, a.USER PRESENT, a.PHONE STATE, a.DATA SMS RECEIVED Zheng et al. [15] Source code (not specified) The CountVectorizer (Used) system functions, syntax keywords, user-defined variables and functions, system-related features SUI et al. [52] C/C++ source code Flow2Vec (Produced) Source code of applications Kim et al. [51] Apache tomcat 5.5.20  -- Predefined vulnerability patterns are matched with the source code to detect vulnerabilities Title Suppressed Due to Excessive Length

5

Table 4: Features used by researchers for vulnerability identiﬁcation

7 Conclusion and Future Work

We have proposed an automated technique to identify vulnerable SmartApps
using machine learning algorithms and that by training them on features au-
tomatically extracted by the tools developed by us, Toekn2Vec and Flow2Vec.
Our research contributions not only ﬂag a SmartApp to be vulnerable, but
they can also report the exact location of vulnerabilities. A high level sum-
mary of our research contributions is presented as follows:

– We have identiﬁed that the features extracted from the BoW are not always

appropriate.

– We have proposed a text mining approach to identify tainted ﬂows in Smar-
tApps. The approach is less expensive than static analysis because it only
considers relevant parts of the code to identify the tainted ﬂows. It only
keeps track of the ﬂows which are tainted. We have implemented this tech-
nique as a tool called FlowsMiner.

 Features identitied by Hybrid Analysis Shar et al. [3] Web PhpMinerI (Produced) Static attributes: Client, File, Database, Text-database, Otherdatabase, Session, Uninit, SQLI-sanitization, XSSsanitization, Numericcasting, Numeric-typecheck, Encoding, Un-taint, Boolean, Propagate. Dynamic attributes: LimitLength, URL, EventHandler, HTMLTag, Delimiter, AlternateEncode. Target attribute: Vulnerable? Features identified by Text Mining Scandariato et al. [4] Web Fortify Source Code Analyzer (used) Set of features are not fixed. It is a vocabulary used by the developers. Each software component has a different set of features. class HelloWorldApp public static void main(String[] args) System.out.println("Hello World!"); Above code (HelloWorldApp.java) would be tokenized and transformed into the feature vector of the following Listing, where each monogram is followed by a count. Feature vector for file HelloWorldApp.java: args: 1, class: 2, Hello: 2, HelloWorldApp: 2, main: 1, out: 1, println: 1, prints: 1, public: 1, static:1, String: 1, System: 1, The: 1, void: 1, World: 2 Walden et al. [1] Web Custom tool to compute a wider variety of metrics (Produced) The dataset used by them includes the following metrics: Lines of code, lines of code (non-HTML), number of functions, cyclomatic complexity, maximum nesting complexity, halstead's volume: A volume estimate ((N1 + N2) log n1 + n2) using the number of unique operators (n1) and operands (n2) and the number of total operators (N1) and operands (N2) in the file, total external calls, fan-in: The number of files (excluding the file being measured) which contain statements that invoke a function or method defined in the file being measured, fan-out: The number of files (excluding the file being measured) containing functions or methods invoked by statements in the le being measured, internal functions or methods called, external functions or methods called, external calls to functions or methods. 6

Hajra Naeem, Manar H. Alalﬁ

– We have developed two tools named as Token2Vec and Flow2Vec to prepare

feature vectors from BoW and Flows.

– We show that considering the features extracted from tainted ﬂows in ad-
dition to features extracted from BoW of source code improve the AUC of
machine learning algorithms.

As for future work, there are numerous possibilities that could be considered
for going forward to contribute more on the subject. Diﬀerent feature engineer-
ing techniques can be used to run more experiments. Balancing the datasets
for both vulnerable and non-vulnerable classes will reduce the chances for any
algorithm to predict a majority class. Normalisation of features will improve
the performance of the machine learning algorithms that are susceptible to dif-
ferent ranges of values for features. Dimensionality reduction techniques may
also help to improve performance of some machine learning algorithms.

Another possibility is to extend this study to other types of software like
android applications. We have a lot of labelled android applications available
in a variety of repositories. This will give us an opportunity to evaluate the
proposed techniques on more datasets and other languages.

References

1. James Walden, Jeﬀ Stuckman, Riccardo Scandariato, Predicting Vulnerable Components:
Software Metrics vs Text Mining, 25th IEEE International Symposium on Software Reli-
ability Engineering, ISSRE 2014, Naples, Italy, pp 23–33 (2014)

2. Lwin Khin Shar, Hee Beng Kuan Tan, Mining input sanitization patterns for predicting
SQL injection and cross site scripting vulnerabilities, 34th International Conference on
Software Engineering, ICSE 2012, Zurich, Switzerland, pp 1293–1296 (2012)

3. Lwin Khin Shar, Hee Beng Kuan Tan, Lionel C. Briand, Mining SQL injection and cross
site scripting vulnerabilities using hybrid program analysis, 35th International Conference
on Software Engineering, ICSE ’13, San Francisco, CA, USA, pp 642–651 (2013)

4. Riccardo Scandariato, James Walden, Aram Hovsepyan, Wouter Joosen, Predicting Vul-
nerable Software Components via Text Mining, IEEE Trans. Software Eng., vol 40, pp
993–1006 (2014)

5. Vitalii Avdiienko, Konstantin Kuznetsov, Alessandra Gorla, Andreas Zeller, Steven Arzt,
Siegfried Rasthofer, Eric Bodden, Mining Apps for Abnormal Usage of Sensitive Data, 37th
IEEE/ACM International Conference on Software Engineering, ICSE 2015, Florence, Italy,
vol 1, pp 426–436 (2015)

6. Kai Zhao, Dafang Zhang, Xin Su, Wenjia Li, Fest: A feature extraction and selection
tool for Android malware detection, 2015 IEEE Symposium on Computers and Commu-
nication, ISCC 2015, Larnaca, Cyprus, pp 714–720 (2015)

7. Muhammad-Naeem Irfan, Catherine Oriat, Roland Groz, Model Inference and Testing,

Advances in Computers, vol 89, pp 89–139 (2013)

8. Z. Berkay Celik, Leonardo Babun, Amit Kumar Sikder, Hidayet Aksu, Gang Tan, Patrick
D. McDaniel, A. Selcuk Uluagac, Sensitive Information Tracking in Commodity IoT, 27th
USENIX Security Symposium, USENIX Security 2018, Baltimore, MD, USA, pp 1687–
1704 (2018)

9. Stefan Lessmann, Bart Baesens, Christophe Mues, Swantje Pietsch, Benchmarking Clas-
siﬁcation Models for Software Defect Prediction: A Proposed Framework and Novel Find-
ings, IEEE Trans. Software Eng., vol 34, pp 485–496 (2008)

10. Ib´eria Medeiros, Nuno Ferreira Neves, Miguel Correia, DEKANT: a static analysis tool
that learns to detect web application vulnerabilitiess, Proceedings of the 25th International
Symposium on Software Testing and Analysis, ISSTA 2016, Saarbr¨ucken, Germany, pp 1–
11 (2016)

Title Suppressed Due to Excessive Length

7

11. Dali Zhu, Hao Jin, Ying Yang, Di Wu, Weiyi Chen, DeepFlow: Deep learning-based
malware detection by mining Android application for abnormal usage of sensitive data,
2017 IEEE Symposium on Computers and Communications (ISCC), pp 438-443 (2017)
12. Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel,
Jacques Klein, Le Traon, Yves, Damien Octeau, Patrick McDaniel, FLOWDROID: Precise
context, ﬂow, ﬁeld, object-sensitive and lifecycle-aware taint analysis for Android apps,
ACM SIGPLAN Notices, vol 49, pp 259–269 (2014)

13. url=https://press.avast.com/hubfs/media-materials/kits/smart-home-report-

2019/Report/Avast

14. url = https://docs.smartthings.com/en/latest/
15. W. Zheng, J. Gao, X. Wu, Y. Xun, G. Liu, X. Chen, An Empirical Study of High-Impact
Factors for Machine Learning-Based Vulnerability Detection, 2020 IEEE 2nd International
Workshop on Intelligent Bug Fixing (IBF), pp 26-34 (2020)

16. Florian Schmeidl, Bara’ Nazzal, Manar H. Alalﬁ, Security analysis for SmartThings
IoT applications, Proceedings of the 6th International Conference on Mobile Software
Engineering and Systems, MOBILESoft@ICSE, Montreal, QC, Canada, pp 25–29 (2019)
17. Sajeda Parveen, Manar H. Alalﬁ, A Mutation Framework for Evaluating Security Anal-
ysis Tools in IoT Applications, 27th IEEE International Conference on Software Analysis,
Evolution and Reengineering, SANER 2020, London, ON, Canada, pp 587–591 (2020)
18. Lessmann, Stefan, Baesens, Bart, Mues, Christophe, Pietsch, Swantje, Benchmarking
Classiﬁcation Models for Software Defect Prediction: A Proposed Framework and Novel
Findings, IEEE Trans. Software Eng., vol 34, pp 485-496 (2008)

19. The pandas development team, pandas-dev/pandas: Pandas, Zenodo, (2020)
20. Hajra Naeem and Manar H. Alalﬁ, Identifying Vulnerable IoT Applications using
Deep Learning, 27th IEEE International Conference on Software Analysis, Evolution and
Reengineering, SANER 2020, London, ON, Canada, pp 582–586 (2020)

21. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,
Brucher, M., Perrot, M., Duchesnay, E., Scikit-learn: Machine Learning in Python, Journal
of Machine Learning Research, vol 12, pp 2825–2830 (2011)

22. url

=

https://www.ibtimes.co.uk/hackers-leave-ﬁnnish-residents-cold-after-ddos-

attack-knocks-out-heating-systems-1590639

23. url = https://globalnews.ca/news/4785542/wiﬁ-baby-monitor-hacked-kidnap/
24. url = https://www.kaspersky.com/blog/blackhat-jeep-cherokee-hack-explained/9493/
25. url = https://money.cnn.com/2017/01/09/technology/fda-st-jude-cardiac-hack/
26. url = https://www.fda.gov/medical-devices/safety-communications/cybersecurity-
vulnerabilities-identiﬁed-st-jude-medicals-implantable-cardiac-devices-and-merlinhome

27. url

=

https://www.javatpoint.com/machine-learning-decision-tree-classiﬁcation-

algorithm

28. url = https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-

algorithm/

29. url = https://www.javatpoint.com/logistic-regression-in-machine-learning
30. url = https://medium.com/@sonish.sivarajkumar/k-nearest-neighbours-knn-algorithm-

9900c1427726

31. url=https://towardsdatascience.com/support-vector-machine-introduction-to-

machine-learning-algorithms-934a444fca47

32. Tomas Mikolov, Kai Chen, Greg Corrado, Jeﬀrey Dean, Eﬃcient Estimation of Word
Representations in Vector Space, 1st International Conference on Learning Representa-
tions, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings,
(2013)

33. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeﬀrey Dean, Distributed Rep-
resentations of Words and Phrases and their Compositionality, CoRR, vol abs/1310.4546
(2013)

34. Asaf Shabtai, Uri Kanonov, Yuval Elovici, Chanan Glezer, Yael Weiss, ”Andromaly”: a
behavioral malware detection framework for android devices, J. Intell. Inf. Syst., vol 38,
pp 161–190 (2012)

35. William Enck, Peter Gilbert, Byung-Gon Chun, Landon P. Cox, Jaeyeon Jung, Patrick
D. McDaniel, Anmol Sheth, TaintDroid: an information ﬂow tracking system for real-time
privacy monitoring on smartphones, Commun. ACM, vol 57, pp 99–106 (2014)

8

Hajra Naeem, Manar H. Alalﬁ

36. Fengguo Wei, Sankardas Roy, Xinming Ou, Robby, Amandroid: A Precise and General
Inter-component Data Flow Analysis Framework for Security Vetting of Android Apps,
Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications
Security, Scottsdale, AZ, USA, pp 1329–1341 (2014)

37. Z. Yuan and Y. Lu and Y. Xue, Droiddetector: android malware characterization and
detection using deep learning, Tsinghua Science and Technology, vol 21, pp 114-123 (2016)
38. A. Sadeghi and H. Bagheri and S. Malek, Analysis of Android Inter-App Security Vul-
nerabilities Using COVERT, 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering, vol 2, pp 725-728 (2015)

39. H. K. Dam, T. Tran, T. T. M. Pham, S. W. Ng, J. Grundy, A. Ghose, Automatic feature
learning for predicting vulnerable software components, IEEE Transactions on Software
Engineering, pp 1-1 (2018)

40. H. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose, T. Kim, C. Kim, Lessons
Learned from Using a Deep Tree-Based Model for Software Defect Prediction in Practice,
2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR),
pp 46-57 (2019)

41. H. K. Dam, T. Tran, T. T. M. Pham, S. W. Ng, J. Grundy, A. Ghose, Automatic feature
learning for predicting vulnerable software components, IEEE Transactions on Software
Engineering, pp 1-1 (2018)

42. Wang Song, Liu Taiyue, Tan Lin, Automatically learning semantic features for defect

prediction, pp 297-308 (2016)

43. Kai Sheng Tai, Richard Socher, Christopher D. Manning, Improved Semantic Rep-
resentations From Tree-Structured Long Short-Term Memory Networks, CoRR, vol
abs/1503.00075, (2015)

44. Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, code2vec: Learning Distributed

Representations of Code, CoRR, vol abs/1803.09473, (2018)

45. Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell, Onur Ozdemir, Leonard R.
Kosta, Akshay Rangamani, Lei H. Hamilton, Gabriel I. Centeno, Jonathan R. Key, Paul
M. Ellingwood, Marc W. McConley, Jeﬀrey M. Opper, Sang Peter Chin, Tomo La-
zovich, Automated software vulnerability detection with machine learning, CoRR, vol
abs/1803.04497, (2018)

46. Chernis Boris, Verma Rakesh, Machine Learning Methods for Software Vulnerability

Detection, pp 31-39 (2018)

47. Muhammad Naeem Irfan, Roland Groz, Catherine Oriat, Improving Model Inference
of Black Box Components having Large Input Test Set, Proceedings of the Eleventh
International Conference on Grammatical Inference, ICGI 2012, University of Maryland,
College Park, USA, vol 21, pp 133–138 (2012)

48. Irfan, Muhammad Naeem and Oriat, Catherine, Groz, Roland, Angluin Style Finite
State Machine Inference with Non-Optimal Counterexamples, Proceedings of the First
International Workshop on Model Inference In Testing, pp 11–19 (2010)

49. Umar Shoaib and Nadeem Ahmad and Paolo Prinetto and Gabriele Tiotto, Integrating
MultiWordNet with Italian Sign Language lexical resources, Expert Syst. Appl., vol 41,
pp 2300–2308 (2014)

50. Junaid Hassan and Umar Shoaib, Multi-class Review Rating Classiﬁcation using Deep

Recurrent Neural Network, Neural Process. Lett., vol 51, pp 1031–1048 (2020)

51. H. Kim, T. Choi, S. Jung, H. Kim, O. Lee and K. Doh, ”Applying Dataﬂow Analysis
to Detecting Software Vulnerability,” 2008 10th International Conference on Advanced
Communication Technology, 2008, pp. 255-258.

52. Yulei Sui, Xiao Cheng, Guanqin Zhang, Haoyu Wang: Flow2Vec: value-ﬂow-based pre-

cise code embedding. Proc. ACM Program. Lang. 4(OOPSLA): 233:1-233:27 (2020)

53. url=https://wiki.owasp.org/index.php/OWASP Internet of Things Project
54. Lars Ole Andersen. 1994. Program analysis and specialization for the C programming

language. Ph.D. Dissertation. University of Cophenhagen

55. Towards a deﬁnition of the Internet of Things (IoT), IEEE Internet Initiative and others,

2015

56. Victoria L´opez, Alberto Fern´andez, Salvador Garc´ıa, Vasile Palade, Francisco Herrera:
An insight into classiﬁcation with imbalanced data: Empirical results and current trends
on using data intrinsic characteristics. Inf. Sci. 250: 113-141 (2013)

Title Suppressed Due to Excessive Length

9

57. SmartThings

Classic

Developer

Documentation,

April,

2019.

https://buildmedia.readthedocs.org/media/pdf/smartthings/latest/smartthings.pdf

10

Hajra Naeem, Manar H. Alalﬁ

A Visual Analysis of sinks in Corpus1 and Coprpus2 datasets

A.A Figures for Multiple sinks in Corpus 1 and 2

Fig. 19: SendPush Frequency in all apps for Corpus 1

Fig. 20: Occurrence of SendPush w.r.t sendSMS in all apps for Corpus 1

Title Suppressed Due to Excessive Length

11

Fig. 21: Occurrence of SendPush w.r.t sendNotiﬁcationToContacts in all apps
for Corpus 1

Fig. 22: Occurrence of SendPush w.r.t httpGet in all apps for Corpus 1

12

Hajra Naeem, Manar H. Alalﬁ

Fig. 23: sendSMS Frequency in all apps for Corpus 1

Fig. 24: Occurrence of sendSMS w.r.t sendPushMessage in all apps for Corpus
1

Title Suppressed Due to Excessive Length

13

Fig. 25: sendNotiﬁcationToContacts Frequency in all apps for Corpus 1

Fig. 26: httpGet Frequency in all apps for Corpus 1

14

Hajra Naeem, Manar H. Alalﬁ

Fig. 27: httpPost Frequency in all apps for Corpus 1

Fig. 28: httpPostJson Frequency in all apps for Corpus 1

Title Suppressed Due to Excessive Length

15

Fig. 29: sendNotiﬁcation Frequency in all apps for Corpus 1

Fig. 30: httpPost Frequency in all apps for Corpus 2

16

Hajra Naeem, Manar H. Alalﬁ

Fig. 31: sendSMS Frequency in all apps for Corpus 2

Fig. 32: sendPush Frequency in all apps for Corpus 2

Title Suppressed Due to Excessive Length

17

Fig. 33: httpPostJson Frequency in all apps for Corpus 2

Fig. 34: Occurrence of httpPost w.r.t sendSMS in all apps for Corpus 2

18

Hajra Naeem, Manar H. Alalﬁ

Fig. 35: Occurrence of httpPost w.r.t sendPush in all apps for Corpus 2

