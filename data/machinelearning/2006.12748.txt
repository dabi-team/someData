Approximation Algorithms for Sparse Principal Component
Analysis

Agniva Chowdhury∗

Petros Drineas†

David P. Woodruﬀ‡

Samson Zhou§

June 7, 2021

Abstract

Principal component analysis (PCA) is a widely used dimension reduction technique in machine learn-
ing and multivariate statistics. To improve the interpretability of PCA, various approaches to obtain
sparse principal direction loadings have been proposed, which are termed Sparse Principal Component
Analysis (SPCA). In this paper, we present thresholding as a provably accurate, polynomial time, ap-
proximation algorithm for the SPCA problem, without imposing any restrictive assumptions on the input
covariance matrix. Our ﬁrst thresholding algorithm using the Singular Value Decomposition is conceptu-
ally simple; is faster than current state-of-the-art; and performs well in practice. On the negative side, our
(novel) theoretical bounds do not accurately predict the strong practical performance of this approach.
The second algorithm solves a well-known semideﬁnite programming relaxation and then uses a novel,
two step, deterministic thresholding scheme to compute a sparse principal vector. It works very well
in practice and, remarkably, this solid practical performance is accurately predicted by our theoretical
bounds, which bridge the theory-practice gap better than current state-of-the-art.

1 Introduction

Principal Component Analysis (PCA) and the related Singular Value Decomposition (SVD) are fundamen-
tal data analysis and dimensionality reduction tools in a wide range of areas including machine learning,
multivariate statistics and many others. These tools return a set of orthogonal vectors of decreasing impor-
tance that are often interpreted as fundamental latent factors that underlie the observed data. Even though
the vectors returned by PCA and SVD have strong optimality properties, they are notoriously diﬃcult to
interpret in terms of the underlying processes generating the data [Mahoney and Drineas, 2009], since they
are linear combinations of all available data points or all available features. The concept of Sparse Princi-
pal Components Analysis (SPCA) was introduced in the seminal work of [d’Aspremont et al., 2007], where
sparsity constraints were enforced on the singular vectors in order to improve interpretability. A prominent
example where sparsity improves interpretability is document analysis, where sparse principal components
can be mapped to speciﬁc topics by inspecting the (few) keywords in their support [d’Aspremont et al., 2007,
Mahoney and Drineas, 2009, Papailiopoulos et al., 2013].

Formally, given a positive semideﬁnite (PSD) matrix A ∈ Rn×n, SPCA can be deﬁned as follows:1

Z ∗ =

max
x∈Rn, (cid:107)x(cid:107)2≤1

x(cid:62)Ax,

subject to (cid:107)x(cid:107)0 ≤ k.

(1)

1
2
0
2

n
u
J

4

]
S
D
.
s
c
[

2
v
8
4
7
2
1
.
6
0
0
2
:
v
i
X
r
a

∗Department of Statistics, Purdue University, West Lafayette, IN, USA, 47906. chowdhu5@purdue.edu
†Department of Computer Science, Purdue University, West Lafayette, IN, USA, 47906. pdrineas@purdue.edu
‡School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, 15213. dwoodruf@andrew.cmu.edu
§School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, 15213. samsonzhou@gmail.com
1Recall that the p-th power of the (cid:96)p norm of a vector x ∈ Rn is deﬁned as (cid:107)x(cid:107)p

p = (cid:80)n

i=1 |xi|p for 0 < p < ∞. For p = 0,

(cid:107)x(cid:107)0 is a semi-norm denoting the number of non-zero entries of x.

1

 
 
 
 
 
 
In the above formulation, A is a covariance matrix representing, for example, all pairwise feature or object
similarities for an underlying data matrix. Therefore, SPCA can be applied to either the object or feature
space of the data matrix, while the parameter k controls the sparsity of the resulting vector and is part of
the input. Let x∗ denote a vector that achieves the optimal value Z ∗ in the above formulation. Intuitively,
the optimization problem of eqn. (1) seeks a sparse, unit norm vector x∗ that maximizes the data variance.
It is well-known that solving the above optimization problem is NP-hard [Moghaddam et al., 2006a] and
that its hardness is due to the sparsity constraint. Indeed, if the sparsity constraint were removed, then the
resulting optimization problem can be easily solved by computing the top left or right singular vector of A
and its maximal value Z ∗ is equal to the top singular value of A.
Notation. We use bold letters to denote matrices and vectors. For a matrix A ∈ Rn×n, we denote its (i, j)-th
entry by Ai,j; its i-th row by Ai∗, and its j-th column by A∗j; its 2-norm by (cid:107)A(cid:107)2 = maxx∈Rn, (cid:107)x(cid:107)2=1 (cid:107)Ax(cid:107)2;
and its (squared) Frobenius norm by (cid:107)A(cid:107)2
i,j A2
i,j. We use the notation A (cid:23) 0 to denote that the
matrix A is symmetric positive semideﬁnite (PSD) and Tr(A) = (cid:80)
i Ai,i to denote its trace, which is also
equal to the sum of its singular values. Given a PSD matrix A ∈ Rn×n, its Singular Value Decomposition
is given by A = UΣUT , where U is the matrix of left/right singular vectors and Σ is the diagonal matrix
of singular values.

F = (cid:80)

1.1 Our Contributions

Thresholding is a simple algorithmic concept, where each coordinate of, say, a vector is retained if its value
is suﬃciently high; otherwise, it is set to zero. Thresholding naturally preserves entries that have large
magnitude while creating sparsity by eliminating small entries. Thus, thresholding seems like a logical
strategy for SPCA: after computing a dense vector that approximately solves a PCA problem, perhaps with
additional constraints, thresholding can be used to sparsify it.

Our ﬁrst approach (spca-svd, Section 2) is a simple thresholding-based algorithm for SPCA that lever-
ages the fact that the top singular vector is an optimal solution for the SPCA problem without the sparsity
constraint. Our algorithm actually uses a thresholding scheme that leverages the top few singular vectors
of the underlying covariance matrix; it is simple and intuitive, yet oﬀers the ﬁrst of its kind runtime vs.
accuracy bounds. Our algorithm returns a vector that is provably sparse and, when applied to the input
covariance matrix A, provably captures the optimal solution Z ∗ up to a small additive error. Indeed, our
output vector has a sparsity that depends on k (the target sparsity of the original SPCA problem of eqn. (1))
and ε (an accuracy parameter between zero and one). Our analysis provides unconditional guarantees for
the accuracy of the solution of the proposed thresholding scheme. To the best of our knowledge, no such
analyses have appeared in prior work (see Section 1.2 for details). We emphasize that our approach only
requires an approximate singular value decomposition and, as a result, spca-svd runs very quickly. In prac-
tice, spca-svd is faster than current state-of-the-art and almost as accurate, at least in the datasets that
we used in our empirical evaluations. However, as shown in Section 4, there is a clear theory-practice gap,
since our theoretical bounds fail to predict the practical performance of spca-svd, which motivated us to
look for more elaborate thresholding schemes that come with improved theoretical accuracy guarantees.

Our second approach (spca-sdp, Section 3) uses a more elaborate semideﬁnite programming (SDP)
approach with (relaxed) sparsity constraints to compute a starting point on which thresholding strategies
are applied. Our algorithm provides novel bounds for the following standard convex relaxation of the problem
of eqn. (1):

max
Z∈Rn×n, Z(cid:23)0

Tr(AZ) s.t. Tr(Z) ≤ 1 and

(cid:88)

|Zi,j| ≤ k.

(2)

It is well-known that the optimal solution to eqn. (2) is greater than or equal to the optimal solution to
eqn. (1). We contribute a novel, two-step deterministic thresholding scheme that converts Z ∈ Rn×n to a
vector z ∈ Rn with sparsity O (cid:0)β2k2/ε2(cid:1) and satisﬁes2 z(cid:62)Az ≥ 1
α · Z ∗ − ε. Here α and β are parameters
of the optimal solution matrix Z that describe the extent to which the SDP relaxation of eqn. (2) is able

2For simplicity of presentation and following the lines of [Fountoulakis et al., 2017], we assume that the rows and columns

of the matrix A have unit norm; this assumption can be removed as in [Fountoulakis et al., 2017].

2

to capture the original problem. We empirically demonstrate that these quantities are close to one for the
(diverse) datasets used in our empirical evaluation. As a result (see Section 4), we demonstrate that the
empirical performance of spca-sdp is much better predicted by our theory, unlike spca-svd and current
state-of-the-art. To the best of our knowledge, this is the ﬁrst analysis of a rounding scheme for the convex
relaxation of eqn. (2) that does not assume a speciﬁc model for the covariance matrix A. However, this
approach introduces a major runtime bottleneck for our algorithm, namely solving an SDP.

An additional contribution of our work is that, unlike prior work, our algorithms have clear tradeoﬀs
between quality of approximation and output sparsity. Indeed, by increasing the density of the ﬁnal SPCA
vector, one can improve the amount of variance that is captured by our SPCA output. See Theorem 2.1
and Theorem 3.1 for details on this sparsity vs. accuracy tradeoﬀ for spca-svd and spca-sdp, respectively.
Applications to Sparse Kernel PCA. Our algorithms have immediate applications to sparse kernel
PCA (SKPCA), where the input matrix A ∈ Rn×n is instead implicitly given as a kernel matrix whose entry
(i, j) is the value k(i, j) := (cid:104)φ(Xi∗), φ(Xj∗)(cid:105) for some kernel function φ that implicitly maps an observation
vector into some high-dimensional feature space. Although A is not explicit, we can query all O (cid:0)n2(cid:1) entries
of A using O (cid:0)n2(cid:1) time assuming an oracle that computes the kernel function k. We can then subsequently
apply our SPCA algorithms and achieve polynomial runtime with the same approximation guarantees.

Experiments. Finally, we evaluate our algorithms on a variety of real and synthetic datasets in order
to practically assess their performance. As discussed earlier, from an accuracy perspective, our algorithms
perform comparably to current state-of-the-art. However, spca-svd is faster than current state-of-the-
art. Importantly, we evaluate the tightness of the theoretical bounds on the approximation accuracy: the
theoretical bounds for spca-svd and current state-of-the-art fail to predict the approximation accuracy
of the respective algorithms in practice. However, the theoretical bounds for spca-sdp are much tighter,
essentially bridging the theory-practice gap, at least in the datasets used in our evaluations.

1.2 Prior work

SPCA was formally introduced by [d’Aspremont et al., 2007]; however, previously studied PCA approaches
based on rotating [Jolliﬀe, 1995] or thresholding [Cadima and Jolliﬀe, 1995] the top singular vector of the
input matrix seemed to work well, at least in practice, given sparsity constraints. Following [d’Aspremont
et al., 2007], there has been an abundance of interest in SPCA. [Jolliﬀe et al., 2003] considered LASSO
(SCoTLASS) on an (cid:96)1 relaxation of the problem, while [Zou and Hastie, 2005] considered a non-convex
regression-type approximation, penalized similar to LASSO. Additional heuristics based on LASSO [Ando
et al., 2009] and non-convex (cid:96)1 regularizations [Zou and Hastie, 2005, Zou et al., 2006, Sriperumbudur et al.,
2007, Shen and Huang, 2008] have also been explored. Random sampling approaches based on non-convex (cid:96)1
relaxations [Fountoulakis et al., 2017] have also been studied; we highlight that unlike our approach, [Foun-
toulakis et al., 2017] solved a non-convex relaxation of the SPCA problem and thus perhaps relied on locally
optimal solutions. Beck and Vaisbourd [2016] presented a coordinate-wise optimality condition for SPCA
and designed algorithms to ﬁnd points satisfying that condition. While this method is guaranteed to con-
In another recent
verge to stationary points, it is also susceptible to getting trapped at local solutions.
work Yuan et al. [2019], the authors came up with a decomposition algorithm to solve the sparse generalized
eigenvalue problem using a random or a swapping strategy. However, the underlying method needs to solve a
subproblem globally using combinatorial search methods at each iteration, which may fail for a large sparsity
parameter k. Additionally, [Moghaddam et al., 2006b] considered a branch-and-bound heuristic motivated
by greedy spectral ideas. [Journ´ee et al., 2010, Papailiopoulos et al., 2013, Kuleshov, 2013, Yuan and Zhang,
2013] further explored other spectral approaches based on iterative methods similar to the power method.
[Yuan and Zhang, 2013] speciﬁcally designed a sparse PCA algorithm with early stopping for the power
method, based on the target sparsity.

Another line of work focused on using semideﬁnite programming (SDP) relaxations [d’Aspremont et al.,
2007, d’Aspremont et al., 2008, Amini and Wainwright, 2009, d’Orsi et al., 2020]. Notably, [Amini and
Wainwright, 2009] achieved provable theoretical guarantees regarding the SDP and thresholding approach
of [d’Aspremont et al., 2007] in a speciﬁc, high-dimensional spiked covariance model, in which a base matrix

3

is perturbed by adding a sparse maximal eigenvector. In other words, the input matrix is the identity matrix
plus a “spike”, i.e., a sparse rank-one matrix.

Despite the variety of heuristic-based sparse PCA approaches, very few theoretical guarantees have been
provided for SPCA; this is partially explained by a line of hardness-of-approximation results. The sparse
PCA problem is well-known to be NP-Hard [Moghaddam et al., 2006a]. [Magdon-Ismail, 2017] shows that if
the input matrix is not PSD, then even the sign of the optimal value cannot be determined in polynomial time
unless P = NP, ruling out any multiplicative approximation algorithm. In the case where the input matrix
is PSD, [Chan et al., 2016] shows that it is NP-hard to approximate the optimal value up to multiplicative
(1 + ε) error, ruling out any polynomial-time approximation scheme (PTAS). Moreover, they show Small-
Set Expansion hardness for any polynomial-time constant factor approximation algorithm and also that the
standard SDP relaxation might have an exponential gap.

We conclude by summarizing prior work that oﬀers provable guarantees (beyond the work of [Amini
and Wainwright, 2009]), typically given some assumptions about the input matrix. [d’Aspremont et al.,
2014] showed that the SDP relaxation can be used to ﬁnd provable bounds when the covariance input
matrix is formed by a number of data points sampled from Gaussian models with a single sparse singular
vector. Perhaps the most interesting, theoretically provable, prior work is [Papailiopoulos et al., 2013], which
presented a combinatorial algorithm that analyzed a speciﬁc set of vectors in a low-dimensional eigenspace
of the input matrix and presented relative error guarantees for the optimal objective, given the assumption
that the input covariance matrix has a decaying spectrum. From a theoretical perspective, this method is the
current state-of-the-art and we will present a detailed comparison with our approaches in Section 4. [Asteris
et al., 2011] gave a polynomial-time algorithm that solves sparse PCA exactly for input matrices of constant
[Chan et al., 2016] showed that sparse PCA can be approximated in polynomial time within a factor
rank.
of n−1/3 and also highlighted an additive PTAS of [Asteris et al., 2015] based on the idea of ﬁnding multiple
disjoint components and solving bipartite maximum weight matching problems. This PTAS needs time
npoly(1/ε), whereas all of our algorithms have running times that are a low-degree polynomial in n.

2 SPCA via SVD Thresholding

To achieve nearly input sparsity runtime, our ﬁrst thresholding algorithm is based upon using the top (cid:96) right
singular vectors of the PSD matrix A. Given A and an accuracy parameter ε, our approach ﬁrst computes
Σ(cid:96) ∈ R(cid:96)×(cid:96) (the diagonal matrix of the top (cid:96) singular values of A) and U(cid:96) ∈ Rn×(cid:96) (the matrix of the top
(cid:96) right singular vectors of A), for (cid:96) = 1/ε. Then, it deterministically selects a subset of O (cid:0)k/ε3(cid:1) columns
of Σ1/2
(cid:96) using a simple thresholding scheme based on the norms of the columns of Σ1/2
(cid:96) . (Recall
that k is the sparsity parameter of the SPCA problem.) In the last step, it returns the top right singular
vector of the matrix consisting of the chosen columns of Σ1/2
(cid:96) . Notice that this right singular vector is
an O (cid:0)k/ε3(cid:1)-dimensional vector, which is ﬁnally expanded to a vector in Rn by appropriate padding with
zeros. This sparse vector is our approximate solution to the SPCA problem of eqn. (1).

(cid:96) U(cid:62)

(cid:96) U(cid:62)

(cid:96) U(cid:62)

This simple algorithm is somewhat reminiscent of prior thresholding approaches for SPCA. However,
to the best of our knowledge, no provable a priori bounds were known for such algorithms without strong
assumptions on the input matrix. This might be due to the fact that prior approaches focused on thresholding
only the top right singular vector of A, whereas our approach thresholds the top (cid:96) = 1/ε right singular vectors
of A. This slight relaxation allows us to present provable bounds for the proposed algorithm.

In more detail, let the SVD of A be A = UΣUT . Let Σ(cid:96) ∈ R(cid:96)×(cid:96) be the diagonal matrix of the
top (cid:96) singular values and let U(cid:96) ∈ Rn×(cid:96) be the matrix of the top (cid:96) right (or left) singular vectors. Let
R = {i1, . . . , i|R|} be the set of indices of rows of U(cid:96) that have squared norm at least ε2/k and let ¯R be its
complement. Here |R| denotes the cardinality of the set R and R ∪ ¯R = {1, . . . , n}. Let R ∈ Rn×|R| be a
sampling matrix that selects3 the columns of U(cid:96) whose indices are in the set R. Given this notation, we are
now ready to state Algorithm 1.

3Each column of R has a single non-zero entry (set to one), corresponding to one of the |R| selected columns. Formally,

Rit,t = 1 for t = 1, . . . , |R|; all other entries of R are set to zero.

4

Algorithm 1 spca-svd: fast thresholding SPCA via SVD
Input: A ∈ Rn×n, sparsity k, error parameter ε > 0.
Output: y ∈ Rn such that (cid:107)y(cid:107)2 = 1 and (cid:107)y(cid:107)0 = k/ε2.
1: (cid:96) ← 1/ε;
2: Compute U(cid:96) ∈ Rn×(cid:96) (top (cid:96) left singular vectors of A) and Σ(cid:96) ∈ R(cid:96)×(cid:96) (the top (cid:96) singular values of A);
3: Let R = {i1, . . . , i|R|} be the set of rows of U(cid:96) with squared norm at least ε2/k and let R ∈ Rn×|R| be

the associated sampling matrix (see text for details);
(cid:13)
(cid:13)Σ1/2
(cid:13)

4: y ∈ R|R| ← argmax(cid:107)x(cid:107)2=1
5: return z = Ry ∈ Rn;

(cid:13)
2
(cid:13)
(cid:96) Rx
(cid:13)
2

(cid:96) U(cid:62)

;

Notice that Ry satisﬁes (cid:107)Ry(cid:107)2 = (cid:107)y(cid:107)2 = 1 (since R has orthogonal columns) and (cid:107)Ry(cid:107)0 = |R|. Since
F = (cid:96) = 1/ε, it follows that |R| ≤ k/ε3.

R is the set of rows of U(cid:96) with squared norm at least ε2/k and (cid:107)U(cid:96)(cid:107)2
Thus, the vector returned by Algorithm 1 has k/ε3 sparsity and unit norm.

Theorem 2.1 Let k be the sparsity parameter and ε ∈ (0, 1] be the accuracy parameter. Then, the vector
z ∈ Rn (the output of Algorithm 1) has sparsity k/ε3, unit norm, and satisﬁes

z(cid:62)Az ≥ Z ∗ − 3ε Tr(A).

The intuition behind Theorem 2.1 is that we can decompose the value of the optimal solution into the
value contributed by the coordinates in R, the value contributed by the coordinates outside of R, and a cross
term. The ﬁrst term we can upper bound by the output of the algorithm, which maximizes with respect
to the coordinates in R. For the latter two terms, we can upper bound the contribution due to the upper
bound on the squared row norms of indices outside of R and due to the largest singular value of U being at
most the trace of A. We defer the full proof of Theorem 2.1 to the supplementary material.

The running time of Algorithm 1 is dominated by the computation of the top (cid:96) singular vectors and
singular values of the matrix A. One could always use the SVD of the full matrix A (O (cid:0)n3(cid:1) time) to
compute the top (cid:96) singular vectors and singular values of A.
In practice, any iterative method, such as
subspace iteration using a random initial subspace or the Krylov subspace of the matrix, can be used towards
this end. We address the inevitable approximation error incurred by such approximate SVD methods below.
Finally, we highlight that, as an intermediate step in the proof of Theorem 2.1, we need to prove the
following Lemma 2.2, which is very much at the heart of our proof of Theorem 2.1 and, unlike prior work,
allows us to provide provably accurate bounds for the thresholding Algorithm 1.

Lemma 2.2 Let A ∈ Rn×n be a PSD matrix and Σ ∈ Rn×n (respectively, Σ(cid:96) ∈ R(cid:96)×(cid:96)) be the diagonal
matrix of all (respectively, top (cid:96)) singular values and let U ∈ Rn×n (respectively, U(cid:96) ∈ Rn×(cid:96)) be the matrix
of all (respectively, top (cid:96)) singular vectors. Then, for all unit vectors x ∈ Rn,

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)
(cid:96) x

(cid:13)
2
(cid:13)
(cid:13)
2

≥

(cid:13)
(cid:13)Σ1/2U(cid:62)x
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

− ε Tr(A).

At a high level, the proof of Lemma 2.2 ﬁrst decomposes a basis for the columns spanned by U into
those spanned by the top (cid:96) singular vectors and the remaining n − (cid:96) singular vectors. We then lower bound
the contribution of the top (cid:96) singular vectors by upper bounding the contribution of the remaining n − (cid:96)
singular vectors after noting that the largest remaining singular value is at most a 1/(cid:96)-fraction of the trace.
For additional details, we defer the full proof to the supplementary material.

Using an approximate SVD solution. The guarantees of Theorem 2.1 in Algorithm 1 use an exact
SVD computation, which could take time O (cid:0)n3(cid:1). We can further improve the running time by using an
approximate SVD algorithm such as the randomized block Krylov method of Musco and Musco [2015],

which runs in nearly input sparsity runtime. Our analysis uses the relationships

5

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96),⊥

(cid:13)
2
(cid:13)
(cid:13)
2

≤ Tr(A)/(cid:96) and

σ1(Σ(cid:96)) ≤ Tr(A). The randomized block Krylov method of Musco and Musco [2015] recovers these guarantees
up to a multiplicative (1 + ε) factor, in O (log n/ε1/2 · nnz(A)) time. Thus, by rescaling ε, we recover the same
guarantees of Theorem 2.1 by using an approximate SVD in nearly input sparsity time. This results in a
randomized algorithm; if one wants a deterministic algorithm then one should compute an exact SVD.

3 SPCA via SDP Relaxation and Thresholding

To achieve higher accuracy, our second thresholding algorithm uses an approach that is based on the SDP
relaxation of eqn. (2). Recall that solving eqn. (2) returns a PSD matrix Z∗ ∈ Rn×n that, by the deﬁnition
of the semideﬁnite programming relaxation, satisﬁes Tr(AZ∗) ≥ Z ∗, where Z ∗ is the true optimal solution
of SPCA in eqn. (2).

We would like to acquire a sparse vector z from Z∗. To that end, we ﬁrst take Z1 to be the best rank-1
approximation to Z∗. Note that Z1 can be quickly computed by taking the top eigenvector u of Z∗ and
setting Z1 = uu(cid:62). Consider a set S deﬁned to be the set of indices of the 9k2β2/ε2 coordinates of u with the
largest absolute value, where β is a parameter deﬁned below (and close to 1 in our experiments). Intuitively,
the indices in S should correlate with the “important” rows and columns of A. Hence, we deﬁne z ∈ Rn to
be the vector that matches the corresponding coordinates of u for indices of S and zero elsewhere, outside
of S. As a result, z will be a 9k2β2/ε2-sparse vector.

Algorithm 2 spca-sdp: accurate thresholding SPCA via SDP
Input: A ∈ Rn×n, sparsity k, error parameter ε > 0.
Output: z ∈ Rn such that (cid:107)z(cid:107)2 = 1 and (cid:107)z(cid:107)0 = 9k2β2/ε2.
1: Let Z∗ be the optimal solution to the relaxed SPCA problem of eqn. (2);
2: Let Z1 = uu(cid:62) be the best rank-1 approximation to Z∗;
3: Let z ∈ Rn be the sparse vector containing the top 9k2β2/ε2 coordinates in magnitude of u;
4: return z;

Our main quality-of-approximation result for Algorithm 2 is Theorem 3.1. For simplicity of presentation,
we make the standard assumption that all rows and columns of A have been normalized to have unit norm;
this assumption can be relaxed, e.g., see [Fountoulakis et al., 2017].

Theorem 3.1 Given a PSD matrix A ∈ Rn×n, a sparsity parameter k, and an error tolerance ε > 0, let Z
be an optimal solution to the relaxed SPCA problem of eqn. (2) and Z1 be the best rank-one approximation
to Z. Suppose α ≥ 1 is a constant such that Tr(AZ) ≤ α Tr(AZ1) and β is a constant such that β ≥ (cid:107)Z1(cid:107)1
.
(cid:107)Z(cid:107)1
Then, Algorithm 2 outputs a vector z ∈ Rn that satisﬁes (cid:107)z(cid:107)0 = 9k2β2

, (cid:107)z(cid:107)2 ≤ 1, and

ε2

z(cid:62)Az ≥ (1/α)Z ∗ − ε.

Proof :

If Z1 = uu(cid:62), then u(cid:62)Au = Tr(AZ1) ≥ (1/α) Tr(AZ) ≥ (1/α)Z ∗.

Since all eigenvalues of Z are at most one, we have that (cid:107)uu(cid:62)(cid:107)2
Also, letting β ≥ (cid:107)Z1(cid:107)1
(cid:107)Z(cid:107)1
Let z be the vector of top 9k2β2
ε2

, we have (cid:107)Z1(cid:107)1 ≤ β(cid:107)Z(cid:107)1 ≤ βk. So (cid:107)Z1(cid:107)1 = (cid:80)

Then by H¨older’s inequality,

F ≤ 1, which means (cid:107)u(cid:107)2 ≤ 1.
i,j |uiuj| = (cid:107)u(cid:107)2

1, so (cid:107)u(cid:107)1 ≤

√

βk.

coordinates in absolute value of u, and remaining entries equal to 0.

z(cid:62)Az = u(cid:62)Au − (u − z)(cid:62)Az − z(cid:62)A(u − z) − (u − z)(cid:62)A(u − z)

= u(cid:62)Au − 2z(cid:62)A(u − z) − (u − z)(cid:62)A(u − z)(cid:62)
≥ u(cid:62)Au − 2(cid:107)z(cid:107)1(cid:107)A(u − z)(cid:107)∞ − (cid:107)u − z(cid:107)1(cid:107)A(u − z)(cid:107)∞.

Each row of A has squared Euclidean norm at most 1, so that (cid:107)u − z(cid:107)2 ≥ (cid:107)A(u − z)(cid:107)∞. Since (cid:107)u(cid:107)1 ≤
then

z(cid:62)Az ≥ u(cid:62)Au − 2(cid:112)βk(cid:107)u − z(cid:107)2 − (cid:112)βk(cid:107)u − z(cid:107)2 = u(cid:62)Au − 3(cid:112)βk(cid:107)u − z(cid:107)2.

√

βk,

6

Because (cid:107)u(cid:107)1 ≤

√

(u − z) have magnitude at most

βk and z contains the top 9k2β2
ε2
ε2

9(kβ)3/2 . Hence, we have

coordinates in absolute value of u, then all entries of

(cid:107)u − z(cid:107)2

2 ≤

ε4
81(kβ)3 ·

9(kβ)2

ε2 =

ε2
9kβ

,

and therefore, (cid:107)u − z(cid:107)2 ≤ ε
√

kβ . Hence,

3

Note z has 9k2β2
ε2

non-zero entries, and (cid:107)z(cid:107)2 ≤ (cid:107)u(cid:107)2 ≤ 1.

(cid:50)

z(cid:62)Az ≥ u(cid:62)Au − ε ≥ (1/α)Z ∗ − ε.

Interpretation of our guarantee. Our assumptions in Theorem 3.1 simply say that much of the trace of
the matrix AZ should be captured by the trace of AZ1, as quantiﬁed by the constant α. For example, if Z
were a rank-one matrix, then the assumption would hold with α = 1. As the trace of AZ1 fails to approximate
the trace of AZ (which intuitively implies that the SDP relaxation of eqn. (2) did not suﬃciently capture
In our
the original problem), the constant α increases and the quality of the approximation decreases.
experiments, we indeed observed that α is close to one (see Table 1). Similarly, we empirically observe that
β, which is the ratio between the 1-norm of Z and its rank-one approximation is also close to one for our
datasets.

Using an approximate SDP solution. The guarantees of Theorem 3.1 in Algorithm 2 use an optimal
solution Z to the SDP relaxation in eqn. (2). In practice, we will only obtain an approximate solution ˜Z
to eqn. (2) using any standard SDP solver, e.g. [Alizadeh, 1995], such that Tr(A ˜Z) ≥ Tr(AZ) − ε after
O (log 1/ε) iterations. Since our analysis only uses the relationship u(cid:62)Au ≥ Tr(AZ), then the additive ε
guarantee can be absorbed into the other ε factors in the guarantees of Theorem 3.1. Thus, we recover the
same guarantees of Theorem 3.1 by using an approximate solution to the SDP relaxation in eqn. (2).

4 Experiments

We compare the output of our algorithms against state-of-the-art SPCA approaches, including the coordinate-
wise optimization algorithm of Beck and Vaisbourd [2016] (cwpca) the block decomposition algorithm of Yuan
et al. [2019] (dec), and the spannogram-based algorithm of Papailiopoulos et al. [2013] (spca-lowrank).
For the implementation of dec, we used the coordinate descent method and for cwpca we used the greedy
coordinate-wise (GCW) method. We implemented spca-lowrank with the low-rank parameter d set to three;
ﬁnally, for spca-svd, we ﬁxed the threshold parameter (cid:96) to one.

(a) Chr 1, n = 37, 493

(b) Chr 2, n = 40, 844

(c) Gene expression data

Fig. 1: Experimental results on real data: f (y) vs. sparsity.

7

Fig. 2: Tightness of our bounds: HGDP/HAPMAP chromosome 1 (left), HGDP/HAPMAP chromosome 2
(middle), and gene expression (right) data

In order to explore the sparsity patterns of the outputs, we ﬁrst applied our methods on the pit props
dataset, which was introduced in [Jeﬀers, 1967] and is a toy, benchmark example used to test sparse PCA.
It is a 13 × 13 correlation matrix, originally calculated from 180 observations with 13 explanatory variables.
We applied our algorithms to the Pit Props matrix in order to extract a sparse top principal component,
It is actually known that
having a sparsity pattern similar to that of cwpca, dec, and spca-lowrank.
the decomposition method of Yuan et al. [2019] can ﬁnd the global optimum for this dataset. We set the
sparsity parameter k to seven; Table 2 in Appendix B shows that both spca-svd and spca-sdp are able
to capture the right sparsity pattern. In terms of the optimal value Z ∗, spca-svd performs very similar to
spca-lowrank, while our SDP-based algorithm spca-sdp exactly recovers the optimal solution and matches
both dec and cwpca.

Next, we further demonstrate the empirical performance of our algorithms on larger real-world datasets,
as well as on synthetic datasets, similar to [Fountoulakis et al., 2017] (see Appendix B). We use genotypic data
from the Human Genome Diversity Panel (HGDP) [Consortium, 2007] and the International Haplotype Map
(HAPMAP) project [Li et al., 2008], forming 22 matrices, one for each chromosome, encoding all autosomal
genotypes. Each matrix contains 2,240 rows and a varying number of columns (typically in the tens of
thousands) that is equal to the number of single nucleotide polymorphisms (SNPs, well-known biallelic loci
of genetic variation across the human genome) in the respective chromosome. Finally, we also use a lung
cancer gene expression dataset (107 × 22, 215 matrix) from [Landi et al., 2008].

We compare our algorithms spca-svd and spca-sdp with the solutions returned by dec, cwpca, and
spca-lowrank. Let f (y) = y(cid:62)Ay/(cid:107)A(cid:107)2; then, f (y) measures the quality of an approximate solution y ∈ Rn
to the SPCA problem. Essentially, f (y) quantiﬁes the ratio of the explained variance coming from the ﬁrst
sparse PC to the explained variance of the ﬁrst sparse eigenvector. Note that 0 ≤ f (y) ≤ 1 for all y with
(cid:107)y(cid:107)2 ≤ 1. As f (y) gets closer to one, the vector y captures more of the variance of the matrix A that
corresponds to its top singular value and corresponding singular vector.

In our experiments, for spca-svd and spca-sdp, we ﬁx the sparsity s to be equal to k, so that all
algorithms return a sparse vector with the same number of non-zero elements. In Figures 1a-1c we evaluate
the performance of the diﬀerent SPCA algorithms by plotting f (y) against (cid:107)y(cid:107)0, i.e., the sparsity of the
output vector, on data from HGDP/HAPMAP chromosome 1, HGDP/HAPMAP chromosome 2, and the
gene expression data. Note that in terms of accuracy both spca-svd and spca-sdp, essentially match the
current state-of-the-art dec, cwpca, and spca-lowrank.

We now discuss the running time of the various approaches. All experiments were performed on a server
with two Rome 32 core, 2.0GHz CPUs and .8 TBs of RAM. Our spca-svd is the fastest approach, taking
about 2.5 to three hours for each sparsity value for the largest datasets (HGDP/HAPMAP chromosomes 1
and 2 data). The state-of-the-art methods spca-lowrank, cwpca, and dec take ﬁve to seven hours for the
same dataset and thus are at least two times slower. However, our second approach spca-spd is slower and
takes approximately 20 hours, as it needs to solve a large-scale SDP problem.

Comparing the theoretical guarantees of our approaches and current-state-of-the-art. A
straightforward theoretical comparison between the theoretical guarantees of our methods and current state-

8

of-the-art if quite tricky, since they depend on diﬀerent parameters that are not immediately comparable.
Therefore, we attempt to compare the tightness of the theoretical guarantees in the context of the datasets
used in our experiments, where these parameters can be directly evaluated. We chose to compare our
theoretical bounds with the method of [Papailiopoulos et al., 2013], namely spca-lowrank, which works well
in practice, has reasonable running times, and comes with state-of-the-art provable accuracy guarantees.
We used the following datasets in our comparison: the data from chromosome 1 and chromosome 2 of
HGDP/HapMap, and the gene expression data of Landi et al. [2008]. We set the accuracy parameter (cid:15)
in Theorem 2.1 and Theorem 3.1 to .9 (using diﬀerent values of (cid:15) between zero and one does not change
the ﬁndings of Figure 2). Figure 2 summarizes our ﬁndings and highlights an important observation on
the theory-practice gap: notice that for small values of the sparsity parameter, both spca-lowrank and
spca-svd predict negative values for the accuracy ratio, which are, of course, meaningless. The theoretical
bounds of spca-lowrank become meaningful (e.g., non-negative) when the sparsity parameter exceeds 3.5K
for the HAPMAP/HGDP chromosome 1 data, while the theoretical bounds of spca-svd remain consistently
meaningless, despite its solid performance in practice. However, the theoretical bounds of our spca-sdp
algorithm are consistently the best and very close to one, thus solidly predicting its high accuracy in practice.
Similar ﬁndings are shown in the other panels of Figure 2 for the other datasets (see Appendix B for additional
experiments), with the notable exception of the gene expression dataset, whose underlying eigenvalues nearly
follow a power-law decay, and, as a result, spca-lowrank exhibits a much tighter bound that almost matches
spca-sdp. We believe that the improved theoretical performance of spca-sdp is due to the novel dependency
of our approach (see Theorem 3.1) on the constants α and β, that are both close to one in real datasets (see
Table 1). On the other hand, we note that the approximation guarantee of spca-lowrank typically depends
on the the spectrum of A and the maximum diagonal entry of A, which are less well-behaved quantities.

Sparsity
624
1, 875
3, 125
4, 372
5, 628
6, 873
8, 122
9, 377

α
1.0000745
1.0000306
1.0000066
0.9999767
1.0000056
0.9999493
1.0000025
1.0000398

β
0.9999961
1.0000028
1.0000020
0.9999901
0.9999995
1.0000012
1.0000263
0.9999828

(cid:15)d
4.63
1.54
0.92
0.66
0.51
0.42
0.36
0.31

Sparsity
680
2, 043
3, 404
4, 765
6, 126
7, 486
8, 848
10, 209

α
1.0000611
1.0000374
1.0000435
1.0000704
1.0000432
0.9999997
1.0000726
0.9999913

β
0.9999994
0.9999985
0.9999939
1.0000090
0.9999896
1.0000215
1.0000685
1.0000321

(cid:15)d
5.18
1.72
1.03
0.74
0.57
0.47
0.40
0.34

Table 1: α, β (see Theorem 3.1) and (cid:15)d (the parameter of
HGDP/HAPMAP chromosome 1 (left) and chromosome 2 (right) data.

interest for Papailiopoulos et al.

[2013]) for

5 Conclusion, limitations, and future work

We present thresholding as a simple and intuitive approximation algorithm for SPCA, without imposing
restrictive assumptions on the input covariance matrix. Our ﬁrst algorithm provides runtime-vs-accuracy
trade-oﬀs and can be implemented in nearly input sparsity time; our second algorithm needs to solve an
SDP and provides highly accurate solutions with novel theoretical guarantees. Our algorithms immediately
extend to sparse kernel PCA. Our work does have limitations which are interesting topics for future work.
First, is it possible to improve the accuracy guarantees of our SVD-based thresholding scheme to match its
superior practical performance. Second, can we speed up our SDP-based thresholding scheme in practice by
using early termination of the SDP solvers or by using warm starts? Third, can we extend our approaches
to handle more than one sparse singular vectors, by deﬂation or other strategies? Finally, it would be
interesting to explore whether the proposed algorithms can approximately recover the support of the vector

9

x∗ (see eqn. (1)) instead of the optimal value Z ∗.

Broader Impacts and Limitations

Our work is focused on speeding up and improving the accuracy of algorithms for SPCA. As such, it
could have signiﬁcant broader impacts by allowing users to more accurately solve SPCA problems like the
ones discussed in our introduction. While applications of our work to real data could result in ethical
considerations, this is an indirect (and unpredictable) side-eﬀect of our work. Our experimental work uses
publicly available datasets to evaluate the performance of our algorithms; no ethical considerations are raised.

10

References

Farid Alizadeh. Interior Point Methods in Semideﬁnite Programming with Applications to Combinatorial

Optimization. SIAM Journal on Optimization, 5(1):13–51, 1995. 7

Arash A. Amini and Martin J. Wainwright. High-dimensional Analysis of Semideﬁnite Relaxations for Sparse

Principal Components. Annals of Statistics, 37:2877–2921, 2009. 3, 4

Ei Ando, Toshio Nakata, and Masafumi Yamashita. Approximating the Longest Path Length of a Stochastic
DAG by a Normal Distribution in Linear Time. Journal of Discrete Algorithms, 7(4):420–438, 2009. 3

Megasthenis Asteris, Dimitris Papailiopoulos, and George N Karystinos. Sparse Principal Component of a
Rank-deﬁcient Matrix. In 2011 IEEE International Symposium on Information Theory Proceedings, pages
673–677, 2011. 4

Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis. Sparse PCA
via Bipartite Matchings. In Advances in Neural Information Processing Systems, pages 766–774, 2015. 4

Amir Beck and Yakov Vaisbourd. The Sparse Principal Component Analysis Problem: Optimality Conditions

and Algorithms. Journal of Optimization Theory and Applications, 170(1):119–143, 2016. 3, 7, 15

Jorge Cadima and Ian T. Jolliﬀe. Loading and Correlations in the Interpretation of Principal Components.

Journal of Applied Statistics, 22(2):203–214, 1995. 3

Siu On Chan, Dimitris Papailliopoulos, and Aviad Rubinstein. On the Approximability of Sparse PCA. In

Proceedings of the 29th Conference on Learning Theory, pages 623–646, 2016. 4

International HapMap Consortium. A Second Generation Human Haplotype Map of over 3.1 Million SNPs.

Nature, 449(7164):851, 2007. 8, 15

Alexandre d’Aspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A Direct Formu-

lation for Sparse PCA using Semideﬁnite Programming. SIAM Review, 49(3):434–448, 2007. 1, 3

Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal Solutions for Sparse Principal

Component Analysis. Journal of Machine Learning Research, 9(Jul):1269–1294, 2008. 3

Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Approximation Bounds for Sparse Principal

Component Analysis. Mathematical Programming, 148(1-2):89–110, 2014. 4

Tommaso d’Orsi, Pravesh K. Kothari, Gleb Novikov, and David Steurer. Sparse PCA: algorithms, adversarial
perturbations and certiﬁcates. In 61st IEEE Annual Symposium on Foundations of Computer Science,
FOCS, pages 553–564, 2020. 3

Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou, and Petros Drineas. A Randomized
Rounding Algorithm for Sparse PCA. ACM Transactions on Knowledge Discovery from Data, 11(3):
38:1–38:26, 2017. 2, 3, 6, 8, 15

John NR Jeﬀers. Two case studies in the application of principal component analysis. Journal of the Royal

Statistical Society: Series C (Applied Statistics), 16(3):225–236, 1967. 8

Ian T. Jolliﬀe. Rotation of principal components: Choice of Normalization Constraints. Journal of Applied

Statistics, 22(1):29–35, 1995. 3

Ian T. Jolliﬀe, Nickolay T. Trendaﬁlov, and Mudassir Uddin. A Modiﬁed Principal Component Technique

Based on the LASSO. Journal of Computational and Graphical Statistics, 12(3):531–547, 2003. 3

Michel Journ´ee, Yurii Nesterov, Peter Richt´arik, and Rodolphe Sepulchre. Generalized Power Method for

Sparse Principal Component Analysis. Journal of Machine Learning Research, 11(2), 2010. 3

11

Volodymyr Kuleshov. Fast Algorithms for Sparse Principal Component Analysis Based on Rayleigh Quotient
Iteration. In Proceedings of the 30th International Conference on Machine Learning, pages 1418–1425,
2013. 3

Maria Teresa Landi, Tatiana Dracheva, Melissa Rotunno, Jonine D. Figueroa, Huaitian Liu, Abhijit Das-
gupta, Felecia E. Mann, Junya Fukuoka, Megan Hames, Andrew W. Bergen, et al. Gene Expression
Signature of Cigarette Smoking and Its Role in Lung Adenocarcinoma Development and Survival. PloS
one, 3(2), 2008. 8, 9, 15

Jun Z. Li, Devin M. Absher, Hua Tang, Audrey M. Southwick, Amanda M. Casto, Sohini Ramachandran,
Howard M. Cann, Gregory S. Barsh, Marcus Feldman, Luigi L. Cavalli-Sforza, et al. Worldwide Human
Relationships Inferred from Genome-Wide Patterns of Variation. Science, 319(5866):1100–1104, 2008. 8,
15

Shiqian Ma. Alternating Direction Method of Multipliers for Sparse Principal Component Analysis. Journal

of the Operations Research Society of China, 1(2):253–274, 2013. 15

Malik Magdon-Ismail. NP-Hardness and Inapproximability of Sparse PCA. Information Processing Letters,

126:35–38, 2017. 4

Michael W. Mahoney and P. Drineas. CUR Matrix Decompositions for Improved Data Analysis. In Pro-

ceedings of the National Academy of Sciences, pages 697–702, 106 (3), 2009. 1

Baback Moghaddam, Yair Weiss, and Shai Avidan. Generalized Spectral Bounds for Sparse LDA.
Proceedings of the 23rd International Conference on Machine learning, pages 641–648, 2006a. 2, 4

In

Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral Bounds for Sparse PCA: Exact and Greedy

Algorithms. In Advances in Neural Information Processing Systems, pages 915–922, 2006b. 3

Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster approx-
imate singular value decomposition. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems, pages 1396–1404, 2015. 5, 6

Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse PCA through Low-rank
Approximations. In Proceedings of the 30th International Conference on Machine Learning, pages 747–755,
2013. 1, 3, 4, 7, 9, 15

Haipeng Shen and Jianhua Z. Huang. Sparse Principal Component Analysis via Regularized Low Rank

Matrix Approximation. Journal of Multivariate Analysis, 99(6):1015–1034, 2008. 3

Bharath K. Sriperumbudur, David A. Torres, and Gert R.G. Lanckriet. Sparse Eigen Methods by D.C.
Programming. In Proceedings of the 24th International Conference on Machine Learning, pages 831–838,
2007. 3

Ganzhao Yuan, Li Shen, and Wei-Shi Zheng. A Decomposition Algorithm for the Sparse Generalized Eigen-
value Problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 3, 7, 8, 15

Xiao-Tong Yuan and Tong Zhang. Truncated Power Method for Sparse Eigenvalue Problems. Journal of

Machine Learning Research, 14(Apr):899–925, 2013. 3

Hui Zou and Trevor Hastie. Regularization and Variable Selection via the Elastic Net. Journal of the Royal

Statistical Society: Series B, 67(2):301–320, 2005. 3

Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse Principal Component Analysis. Journal of Compu-

tational and Graphical Statistics, 15(2):265–286, 2006. 3

12

A SPCA via thresholding: Proofs

We will use the notation of Section 2. For notational convenience, let σ1, . . . , σn be the diagonal entries of
the matrix Σ ∈ Rn×n, i.e., the singular values of A.

Lemma 2.2 Let A ∈ Rn×n be a PSD matrix and Σ ∈ Rn×n (respectively, Σ(cid:96) ∈ R(cid:96)×(cid:96)) be the diagonal
matrix of all (respectively, top (cid:96)) singular values and let U ∈ Rn×n (respectively, U(cid:96) ∈ Rn×(cid:96)) be the matrix
of all (respectively, top (cid:96)) singular vectors. Then, for all unit vectors x ∈ Rn,

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)
(cid:96) x

(cid:13)
2
(cid:13)
(cid:13)
2

≥

(cid:13)
(cid:13)Σ1/2U(cid:62)x
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

− ε Tr(A).

Let U(cid:96),⊥ ∈ Rn×(n−(cid:96)) be a matrix whose columns form a basis for the subspace perpendicular to
Proof :
the subspace spanned by the columns of U(cid:96). Similarly, let Σ(cid:96),⊥ ∈ R(n−(cid:96))×(n−(cid:96)) be the diagonal matrix of
the bottom n − (cid:96) singular values of A. Notice that U = [U(cid:96) U(cid:96),⊥] and Σ = [Σ(cid:96) 0; 0 Σ(cid:96),⊥]; thus,

UΣ1/2U(cid:62) = U(cid:96)Σ1/2

(cid:96) U(cid:62)

(cid:96) + U(cid:96),⊥Σ1/2

(cid:96),⊥U(cid:62)

(cid:96),⊥.

By the Pythagorean theorem,

(cid:13)
(cid:13)UΣ1/2U(cid:62)x
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

=

(cid:13)
(cid:13)U(cid:96)Σ1/2
(cid:13)

(cid:96) U(cid:62)
(cid:96) x

(cid:13)
2
(cid:13)
(cid:13)
2

+

(cid:13)
(cid:13)U(cid:96),⊥Σ1/2
(cid:13)

(cid:96),⊥U(cid:62)

(cid:96),⊥x

(cid:13)
2
(cid:13)
(cid:13)
2

.

Using invariance properties of the vector two-norm and sub-multiplicativity, we get

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)
(cid:96) x

(cid:13)
2
(cid:13)
(cid:13)
2

≥

(cid:13)
(cid:13)Σ1/2U(cid:62)x
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

−

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96),⊥

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)U(cid:62)

(cid:96),⊥x(cid:13)
2
(cid:13)
2

.

We conclude the proof by noting that (cid:13)

(cid:13)Σ1/2U(cid:62)x(cid:13)
2
2 = x(cid:62)UΣU(cid:62)x = x(cid:62)Ax and
(cid:13)

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96),⊥

(cid:13)
2
(cid:13)
(cid:13)
2

= σ(cid:96)+1 ≤

1
(cid:96)

n
(cid:88)

i=1

σi =

Tr(A)
(cid:96)

.

The inequality above follows since σ1 ≥ σ2 ≥ . . . σ(cid:96) ≥ σ(cid:96)+1 ≥ . . . ≥ σn. We conclude the proof by setting
(cid:50)
(cid:96) = 1/ε.

Theorem 2.1 Let k be the sparsity parameter and ε ∈ (0, 1] be the accuracy parameter. Then, the vector
z ∈ Rn (the output of Algorithm 1) has sparsity k/ε3, unit norm, and satisﬁes

z(cid:62)Az ≥ Z ∗ − 3ε Tr(A).

Proof : Let R = {i1, . . . , i|R|} be the set of indices of rows of U(cid:96) (columns of U(cid:62)
(cid:96) ) that have squared norm at
least ε2/k and let ¯R be its complement. Here |R| denotes the cardinality of the set R and R ∪ ¯R = {1, . . . , n}.
Let R ∈ Rn×|R| be the sampling matrix that selects the columns of U(cid:96) whose indices are in the set R and
let R⊥ ∈ Rn×(n−|R|) be the sampling matrix that selects the columns of U(cid:96) whose indices are in the set ¯R.
Thus, each column of R (respectively R⊥) has a single non-zero entry, equal to one, corresponding to one of
the |R| (respectively | ¯R|) selected columns. Formally, Rit,t = 1 for all t = 1, . . . , |R|, while all other entries
of R (respectively R⊥) are set to zero; R⊥ can be deﬁned analogously. The following properties are easy to
⊥R = 0. Recall that x∗ is the optimal solution to the
prove: RR(cid:62) + R⊥R(cid:62)
SPCA problem from eqn. (1). We proceed as follows:

⊥ = In; R(cid:62)R = I; R(cid:62)

⊥R⊥ = I; R(cid:62)

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) x∗(cid:13)
2
(cid:13)
(cid:13)
2

=

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) (RR(cid:62) + R⊥R(cid:62)

⊥)x∗(cid:13)
2
(cid:13)
(cid:13)
2

13

≤

+

(cid:96) RR(cid:62)x∗(cid:13)
2
(cid:13)
(cid:13)
2
(cid:96) RR(cid:62)x∗(cid:13)
(cid:13)
(cid:13)2
(cid:96) RR(cid:62)x∗(cid:13)
2
(cid:13)
(cid:13)
2
(cid:13)
(cid:96) RR(cid:62)x∗(cid:13)
(cid:13)U(cid:62)
(cid:13)2
The above inequalities follow from the Pythagorean theorem and sub-multiplicativity. We now bound the
second term in the right-hand side of the above inequality.

(cid:13)
(cid:13)Σ1/2
(cid:96) U(cid:62)
(cid:13)
(cid:13)
(cid:13)Σ1/2
(cid:13)
(cid:13)
(cid:13)Σ1/2
(cid:96) U(cid:62)
(cid:13)
(cid:13)
(cid:13)U(cid:62)

(cid:13)
(cid:13)Σ1/2
(cid:13)
(cid:13)
(cid:13)Σ1/2
(cid:13)
(cid:13)
(cid:13)U(cid:62)
(cid:96) R⊥R(cid:62)

⊥x∗(cid:13)
2
(cid:13)
(cid:13)
2
⊥x∗(cid:13)
(cid:13)
(cid:13)2
⊥x∗(cid:13)
2
(cid:13)
2

(cid:96) R⊥R(cid:62)
⊥x∗(cid:13)

(cid:96) R⊥R(cid:62)

(cid:96) R⊥R(cid:62)

(cid:96) U(cid:62)

(cid:96) U(cid:62)

(cid:96) U(cid:62)

+ 2σ1

+ σ1

(cid:13)2 .

+ 2

(3)

≤

(cid:13)
(cid:13)U(cid:62)

(cid:96) R⊥R(cid:62)

⊥x∗(cid:13)

(cid:13)2 = (cid:107)

n
(cid:88)

(U(cid:62)

(cid:96) R⊥)∗i(R(cid:62)

⊥x∗)i(cid:107)2

i=1
n
(cid:88)

(cid:107)(U(cid:62)

(cid:96) R⊥)∗i(cid:107)2 · |(R(cid:62)

⊥x∗)i| ≤

i=1
(cid:114)

ε2
k

(cid:107)R(cid:62)

⊥x∗(cid:107)1 ≤

√

(cid:114) ε
k

k = ε.

≤

≤

(cid:114)

ε2
k

n
(cid:88)

i=1

|(R(cid:62)

⊥x∗)i|

(4)

In the above derivations we use standard properties of norms and the fact that the columns of U(cid:62)
indices in the set ¯R have squared norm at most ε2/k. The last inequality follows from (cid:107)R(cid:62)
√

(cid:96) that have
⊥x∗(cid:107)1 ≤ (cid:107)x∗(cid:107)1 ≤

k, since x∗ has at most k non-zero entries and Euclidean norm at most one.

Recall that the vector y of Algorithm 1 maximizes (cid:107)Σ1/2

(cid:96) U(cid:62)

(cid:96) Rx(cid:107)2 over all vectors x of appropriate

dimensions (including Rx∗) and thus

(cid:107)Σ1/2

(cid:96) U(cid:62)

(cid:96) Ry(cid:107)2 ≥

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) RR(cid:62)x∗(cid:13)
(cid:13)
(cid:13)2

.

Combining eqns. (3), (4), and (5), we get that for suﬃciently small ε,

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) x∗(cid:13)
2
(cid:13)
(cid:13)
2

≤ (cid:107)Σ1/2

(cid:96) U(cid:62)

(cid:96) z(cid:107)2

2 + 2ε Tr(A).

In the above we used z = Ry (as in Algorithm 1) and σ1 ≤ Tr(A). Notice that

(cid:96) U(cid:62)
and using the Pythagorean theorem we get

U(cid:96)Σ1/2

(cid:96) z + U(cid:96),⊥Σ1/2

(cid:96),⊥U(cid:62)

(cid:96),⊥z = UΣ1/2U(cid:62)z,

2 + (cid:107)U(cid:96),⊥Σ1/2
Using the unitary invariance of the two norm and dropping a non-negative term, we get the bound

2 = (cid:107)UΣ1/2U(cid:62)z(cid:107)2
2.

(cid:107)U(cid:96)Σ1/2

(cid:96) U(cid:62)

(cid:96),⊥U(cid:62)

(cid:96),⊥z|2

(cid:96) z(cid:107)2

Combining eqns. (6) and (7), we conclude

(cid:107)Σ1/2

(cid:96) U(cid:62)

(cid:96) z(cid:107)2

2 ≤ (cid:107)Σ1/2U(cid:62)z(cid:107)2
2.

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) x∗(cid:13)
2
(cid:13)
(cid:13)
2

≤ (cid:107)Σ1/2U(cid:62)z(cid:107)2

2 + 2ε Tr(A).

We now apply Lemma 2.2 to the optimal vector x∗ to get

(cid:13)
(cid:13)

(cid:13)Σ1/2U(cid:62)x∗(cid:13)

2
(cid:13)
(cid:13)
2

− ε Tr(A) ≤

(cid:13)
(cid:13)Σ1/2
(cid:13)

(cid:96) U(cid:62)

(cid:96) x∗(cid:13)
2
(cid:13)
(cid:13)
2

.

Combining with eqn. (8) we get

z(cid:62)Az ≥ Z ∗ − 3ε Tr(A).

In the above we used (cid:107)Σ1/2U(cid:62)z(cid:107)2
from rescaling ε.

2 = z(cid:62)Az and (cid:13)

(cid:13)Σ1/2U(cid:62)x∗(cid:13)
2
2 = (x∗)(cid:62)Ax∗ = Z ∗. The result then follows
(cid:13)
(cid:50)

14

(5)

(6)

(7)

(8)

B Additional Notes on Experiments

For Algorithm 1, we ﬁx the threshold parameter (cid:96) to 1 for all datasets. For Algorithm 2, we rely on ADMM-
based ﬁrst-order methods to solve eqn. (2). More precisely, we use the admm.spca() function of the ADMM
package in R [Ma, 2013] as well as Python’s cvxpy package with SCS solver to solve eqn. (2). Next, we show
the performance of our algorithms on pitprops data:

topdiam length moist

(Algorithm 1)

0.422
spca-svd
0.430
spca-sdp (Algorithm 2)
−0.423 −0.430
dec [Yuan et al., 2019]
−0.423 −0.430
cwpca [Beck and Vaisbourd, 2016]
spca-lowrank [Papailiopoulos et al., 2013] −0.427 −0.432

0.420
0.424

0
0
0
0
0

testsg
0
0
0
0
0

ovensg
0
0
0
0
0

ringtop
0.296
0.268

ringbut
0.416
0.403
−0.268 −0.403
−0.268 −0.403
−0.249 −0.390

bowmax
0.305
0.313
−0.313
−0.313
−0.326

0.371
0.379

bowdist whorls
0.394
0.399
−0.379 −0.399
−0.379 −0.399
−0.383 −0.403

clear
0
0
0
0
0

knots
0
0
0
0
0

diaknot
0
0
0
0
0

Z ∗
PVE
30.71% 3.993
30.74% 3.996
30.74% 3.996
30.74% 3.996
30.72% 3.994

Table 2: Loadings, % of variance explained (PVE), and the objective function value for the ﬁrst principal component
of the Pit Props data.

B.1 Real Data

Population genetics data. We use population genetics data from the Human Genome Diversity Panel [Con-
sortium, 2007] and the HAPMAP [Li et al., 2008].
In particular, we use the 22 matrices (one for each
chromosome) that encode all autosomal genotypes. Each matrix contains 2,240 rows and a varying number
of columns that is equal to the number of single nucleotide polymorphisms (SNPs, well-known biallelic loci
of genetic variation across the human genome) in the respective chromosome. The columns of each matrix
were mean-centered as a preprocessing step. See Table 3 for summary statistics.

Gene expression data. We also use a lung cancer gene expression dataset (GSE10072) from the NCBI
Gene Expression Omnibus database [Landi et al., 2008]. This dataset contains 107 samples (58 cases and 49
controls) and 22,215 features. Both the population genetics and the gene expression datasets are interesting
in the context of sparse PCA beyond numerical evaluations, since the sparse components can be directly
interpreted to identify small sets of SNPs or genes that capture the data variance.

B.2 Synthetic Data

We also use a synthetic dataset generated using the same mechanism as in [Fountoulakis et al., 2017].
Speciﬁcally, we construct the m × n matrix X such that X = UΣV(cid:62) + Eσ. Here, Eσ is a noise matrix,
containing i.i.d. Gaussian elements with zero mean and we set σ = 10−3; U ∈ Rm×m is a Hadamard matrix
with normalized columns; Σ = ( ˜Σ 0) ∈ Rm×n such that ˜Σ ∈ Rm×m is a diagonal matrix with ˜Σ11 = 100
and ˜Σii = e−i for i = 2, . . . , m; V ∈ Rn×n such that V = Gn(θ) ˜V, where ˜V ∈ Rn×n is also a Hadamard
matrix with normalized columns and

Gn(θ) = G(i1, i1 + 1, θ) G(i2, i2 + 1, θ) . . . G(in/4, in/4 + 1, θ),

4 Givens rotation matrices with ik = n

4 . Here G(i, j, θ) ∈ Rn×n
is a composition of n
is a Givens rotation matrix, which rotates the plane i − j by an angle θ. For θ ≈ 0.27π and n = 212, the
2 components of the columns of ˜V, making half of them almost zero and
matrix Gn(θ) rotates the bottom n
the remaining half larger. Figure 3 shows the absolute values of the elements of the ﬁrst column of the
matrices V and ˜V.

2 +2k−1 for k = 1, 2, . . . , n

B.3 Additional Experiments

In our additional experiments on the large datasets, Figure 3b shows the performance of various SPCA
algorithms on synthetic data. We observe our algorithms perform optimally and closely match with dec,
4 elements of ˜V into large values
cwpca, and spca-lowrank. Moreover, notice that turning the bottom n

15

doesn’t aﬀect the performances of spca-svd and spca-sdp, which further highlights the robustness of our
methods. In Figure 4, we demonstrate how our algorithms perform on Chr 3 and Chr 4 of the population
genetics data. We see a similar behavior as observed for Chr 1 and Chr 2 in Figures 1a-1b.

(a)

(b)

Fig. 3: Experimental results on synthetic data with m = 27 and n = 212: (a) the red and the blue lines are
the sorted absolute values of the elements of the ﬁrst column of matrices V and ˜V respectively. (b) f (y) vs.
sparsity ratio.

(a) Chr 3, n = 34, 258

(b) Chr 4, n = 30, 328

(c) Pitprops data

Fig. 4: Experimental results on real data: f (y) vs. sparsity ratio.

Table 3: Statistics of the population genetics data.

Dataset # Rows # Columns Density
Chr 1
Chr 2
Chr 3
Chr 4
Gene expression

37,493
40,844
34,258
30,328
22,215

2,240
2,240
2,240
2,240
107

0.986
0.987
0.986
0.986
0.999

16

