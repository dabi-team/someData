Introduction to Quantum Reinforcement Learning:
Theory and PennyLane-based Implementation

◦Yunseok Kwak, ◦Won Joon Yun, †Soyi Jung, ◦Jong-Kook Kim, and ◦Joongheon Kim
◦School of Electrical Engineering, Korea University, Seoul, Republic of Korea
†School of Software, Hallym University, Chungcheon, Republic of Korea
E-mails: rhkrdbstjr0@korea.ac.kr, ywjoon95@korea.ac.kr, jungsoyi@korea.ac.kr,
jongkook@korea.ac.kr, joongheon@korea.ac.kr

1
2
0
2

g
u
A
6
1

]

G
L
.
s
c
[

1
v
9
4
8
6
0
.
8
0
1
2
:
v
i
X
r
a

Abstract—The emergence of quantum computing enables for
researchers to apply quantum circuit on many existing studies.
Utilizing quantum circuit and quantum differential program-
ming, many research are conducted such as Quantum Machine
Learning (QML). In particular, quantum reinforcement learning
is a good ﬁeld to test the possibility of quantum machine learning,
and a lot of research is being done. This work will introduce the
concept of quantum reinforcement learning using a variational
quantum circuit, and conﬁrm its possibility through implemen-
tation and experimentation. We will ﬁrst present the background
knowledge and working principle of quantum reinforcement
learning, and then guide the implementation method using the
PennyLane library. We will also discuss the power and possibility
of quantum reinforcement learning from the experimental results
obtained through this work.

I. INTRODUCTION

Since deep reinforcement learning has opened a new chapter
in reinforcement learning by leveraging the power of artiﬁ-
cial neural networks, there have been achievements such as
surpassing human limits in complex games such as chess and
Go, and it has already become an unavoidable ﬂow of artiﬁcial
intelligence (AI) research.

On the other hand, advanced quantum computing tech-
nology has already reached a level close to the imple-
mentation of quantum computational gain predicted through
many algorithm studies [1]–[3]. In addition, the advent of
a variational quantum circuit (VQC) that mimics the prin-
ciples and functions of artiﬁcial neural networks has made
it possible to apply these quantum calculations to existing
machine learning algorithms. This has established itself as a
major trend in quantum machine learning research, and many
studies using it are being actively conducted. In this context,
many studies are conducted using VQC as a Quantum Neural
Network (QNN) [4]–[7], including variational classiﬁer, image
preprocessor, federated learning, reinforcement learning, etc.
Among them, in this paper, we introduce and discuss quantum
reinforcement learning, a reinforcement learning model that
replaces the artiﬁcial neural network of a deep Q network
(DQN) with a VQC.

Since QPU has not been commercialized yet,

there is
nothing better in speed than the existing machine learn-
ing framework that utilizes NPU. However, as development
libraries such as Tensorﬂow-quantum [8], Qiskit [9], and
PennyLane [10] for future quantum computing environments

and quantum computing clouds such as IBMQ, IonQ, and
Amazon Braket are provided to developers, various stud-
ies on QML are in progress. Particularly, PennyLane [10],
is a suitable library for starting quantum machine learning
research because it provides a simulator that allows users
to easily implement quantum circuits by using a CPU to
perform QPU operations. Therefore, we aim to increase access
to quantum reinforcement learning and facilitate subsequent
research by brieﬂy introducing the implementation process
through PennyLane. In addition, we would like to discuss the
impacts and potentials of quantum computing in reinforcement
learning through experimentation and evaluation of quantum
reinforcement learning models in the CartPole environment
provided by OpenAI.

II. BACKGROUNDS

A. Reinforcement Learning

Reinforcement

learning is mathematically modeled with
Markov Decision Process (MDP) as a tuple (S, A, P, R, T ),
where S is a ﬁnite set of state information, and A is a ﬁnite
set of action information. The function P : S × A → P (S)
is a transition probability function, with P (s(cid:48) | s, a) being
the probability of transitioning into state s(cid:48) if an agent starts
executing action a in state s. The function R : S ×A×S → R
denotes the reward function, with Rt = R(st, at, st+1). The
MDP has a ﬁnite time horizon T , and solving an MDP means
ﬁnding a optimal policy π∗
θ ∈ Π : S × A → [0, 1], where πθ is
neural network-based policy with parameter θ; Observing s, πθ
determines agent’s action a ∈ A to maximize the cumulative
rewards received during the ﬁnite time T .

When the environment

transitions and the policy are
stochastic, the probability of a T -step trajectory is deﬁned as
P (τ | πθ) = ρ(s0) (cid:81)T −1
t=0 P (st+1 | st, at)πθ(at | st) where ρ
is the initial state distribution. Then, the expected return J (πθ)
is deﬁned as J (πθ) = (cid:82)
τ P (τ | πθ)R(τ ) = Eτ ∼πθ [R(τ )]
where the trajectory τ is a sequence of states and actions in
the environment. The objective of reinforcement learning is
to learn a policy that maximizes the expected return J (πθ)
when the agent acts according to the policy πθ. Therefore, the
optimization objective is expressed by

π∗
θ = arg max

with π∗

θ
θ being the optimal policy.

J (πθ)

(1)

 
 
 
 
 
 
Deep Q-Network (DQN)
[11]. One of the conventional
method for solving MDP is Q-Learning. Q-Learning utilizes
Q-table to ﬁnd optimal policy. However, Q-Learning has limi-
tation that it obtains optimal policy when the state dimension is
small. Inspired to Q-Learning, deep Q-network (DQN) which
is a model-free reinforcement learning, is proposed to learn the
optimal policy with a high-dimensional state space. Experience
replay D and target network are two key features used for
training deep neural network with stabilization. Experiences
et = (st, at, Rt+1, st+1) of the agent are stored in the
experience buffer D = (e1, e2, . . . , eT ), and are periodically
resampled to train the Q-networks. Sampled experience is used
to update the parameters θi of the policy with the loss function
at the i-th training iteration where the loss function is deﬁned
as

L(θi) = E [(Rt+1+

Q(st+1, a(cid:48); θ−

i ) − Q(st, at; θi))2(cid:105)

(2)

γ max
a(cid:48)

where θ−
are the target network parameters. The target
i
network parameters θ−
are updated using the Q-network
i
parameters θ in every predeﬁned step. The stochastic gradient
descent method is used to optimize the loss function.
Proximal Policy Optimization (PPO) [12]. PPO is one of the
breakthroughs of DRL algorithms for improving the training
stability by ensuring that πθ updates at every iteration are small
by clipping the probability ratio rπ(θ) = πθ(a | s)/πθold (a |
s), where θold is that of previous updated parameters of policy.
(Schulman et al., 2017) proposed a surrogate function that has
objective that prevents the new policy from straying away from
the old one is used to train the policy πθ. The clipped objective
function is as follows:

LCLIP
t

(θ) = min(rt(θ)At, clip(rt(θ), 1 − (cid:15), 1 + (cid:15))At),

(3)

where At is the estimated advantage function under hyperpa-
rameter (cid:15) < 1, which means how far away the new policy is
allowed to update from the old policy. PPO uses the stochastic
gradient descent to maximize the objective (3).

B. Quantum Computing

Quantum computers use a qubit as the basic unit of compu-
tation, which represent a quantum superposition state between
two basis state |0(cid:105) and |1(cid:105). It is controlled by unitary gates in
a quantum circuit to perform various quantum operations. It
can be represented as a normalized two-dimensional complex
vector as:

|ψ(cid:105) = α|0(cid:105) + β|1(cid:105), where (cid:107)α(cid:107)2

2 + (cid:107)β(cid:107)2

2 = 1,

(4)

and there also is a geometrical representation of a qubit space,
using polar coordinates θ and φ:

Fig. 1. A policy-VQC for deep-Q learning with parameter θ.

qubit state into another qubit state, which can be represented as
a 2×2 matrix with complex entries. There are some important
quantum gates, Pauli-X, Pauli-Y , and Pauli-Z, rotating by π
around their corresponding axes in Bloch sphere. The rotation
operator gates Rx(θ), Ry(θ), and Rz(θ) rotate by θ instead
of π in Pauli-X, Pauli-Y , and Pauli-Z gates, and it is known
that any single-qubit unitary gate in SU (2) can be written as
a product of three rotation operators of each axis. In addition,
there are quantum gates which operate on multiple qubits,
called controlled rotation gates. They act on a qubit accord-
ing to the signal of several control qubits, which generates
quantum entanglement between multiple qubits. Among them,
Controlled X(or CNOT) gate is one of the most used control
gates, changing the sign of the second qubit if the ﬁrst qubit is
|1(cid:105). These gates allow quantum algorithms to work using their
features on a quantum circuit that will be introduced later.

C. Variational Quantum Circuit

The variational quantum circuit (or parameterized quantum
circuit) is a quantum circuit using learnable parameters to
perform various numerical tasks, such as optimization, approx-
imation, and classiﬁcation. Operation of general VQC model
can be divided into 4 steps. First one is state preperation
step, the input information is encoded into corresponding qubit
states, which can be treated in the quantum circuit. Next step is
variational step, entangling qubit states by controlled gates and
rotating qubits by parameterized rotation gates. This process
can be repeated in a multi-layer manner with more parameters,
which possibly enhance the performance of the circuit. In the
third step, processed qubit states are measured and decoded
to the form of appropriate output information. Last step is
conducted outside the circuit. The quantum circuit parameters
are updated in the direction of optimizing the objective func-
tion of the algorithm by a classical CPU algorithm, like Adam
optimizer. Then the circuit updated with the new parameters
performs the calculation again from the beginning. This circuit
is known to be able to approximate any continuous function
like classical neural network [13], so VQC is often called
Quantum Neural Network (QNN) [14]. It has been widely
applied in quantum machine learning researches.

III. QUANTUM REINFORCEMENT LEARNING

|ψ(cid:105) = cos(θ/2)|0(cid:105) + eiφ sin(θ/2)|1(cid:105),

(5)

A. Variational Quantum Policy Circuit

where 0 ≤ θ ≤ π and 0 ≤ φ ≤ π. Qubit state is mapped into
the surface of 3-dimensional unit sphere, which is called Bloch
sphere. Quantum gate is a unitary operator transforming a

In recent studies of quantum reinforcement learning [15],
[16], VQC substitutes the policy training DNN of existing
DRL. At each episode, agent with given state information

𝑅y(s2)|ۧ0|ۧ0|ۧ0|ۧ0𝑅y(s1)𝑅y(s3)𝑅y(s4)𝑅x,𝑦,z(𝜃11,𝜃12,𝜃13)𝑅x,𝑦,z(𝜃21,𝜃22,𝜃23)𝑅x,𝑦,z(𝜃31,𝜃32,𝜃33)𝑅x,𝑦,z(𝜃41,𝜃42,𝜃43)State EncodingParameterized Rotation  & Entanglement LayersMeasure Decoding to Action ProbabilityAlgorithm 1 Variational Quantum Deep Q Learning with PPO

Initialize replay memory D to capacity N
Initialize action-value function quantum circuit Q with random parameters θ
Initialize state value function V (s; φ)
for episode = 1, 2, . . . , M do

Initialise state s1 and encode into the quantum state
# 1. Inference Process #
for t = 1, 2, . . . , T do

With probability (cid:15) select a random action at
otherwise select at = maxa Q∗(st, a; θ) from the output of the quantum circuit
Execute action at in emulator and observe reward rt and next state st+1
Store transition (st, at, Rt, st+1) in D

end for
# 2. Training Process #
for i = 1, ..., Kepoch do

Sample random mini-batch of transitions (sj, aj, Rj, sj+1) from D

Calculate temporal difference target, yj =

(cid:26) Rj

Rj + γ maxa(cid:48) Q(sj+1, a(cid:48); θ)

for terminal sj+1
for non-terminal sj+1

Calculate temporal difference, δj = yj − V (sj)
Calculate estimated advantage function, ˆAj = δj + (γλ)δj+1 + ... + (γλ)J−j+1δJ−1
Calculate ratio, rj = πθ(aj |sj )
Calculate surrogate actor loss function using (3)
Calculate critic loss function, |V (s) − yj|.
Calculate gradient and update actor and critic parameters

πθOLD (aj |sj )

end for

end for

B. Quantum Reinforcement Learning Systems

The quantum reinforcement learning system in this paper
is as described in Fig. 2. In the beginning of an episode,
quantum-classical hybrid agent receives a state information
from the environment, and determine its action by θπ made
from the VQC. Then the policy of the agent is evaluated and
updated by PPO algorithm, which is introduced before and de-
scribed in Algorithm 0. The PPO algorithm are effectively the
same as in the previous study [12]. The replay buffer functions
in the same way as in traditional approaches, keeping track of
the (cid:104)s, a, R, s(cid:48)(cid:105) tuples. One does not have to fundamentally or
drastically change an algorithm in order to apply the power of
VQCs to it. The algorithm presented in Algorithm 0.

IV. THE IMPLEMENTATION OF QRL

A. Implementation Guidelines

As quantum computing and quantum machine learning
research is actively progressing, many development libraries
for researchers have emerged, such as TensorFlow-quantum,
Qiskit, and PennyLane. Among them, PennyLane was cre-
ated to support quantum machine learning research, allowing
anyone to easily test the performance of quantum circuits
through quantum simulators. The quantum simulator supported
by PennyLane allows the CPU to imitate the operation of
QPU, and especially supports the use of parameters in the
form of PyTorch tensor and gradient operation [17]. Thanks
to these features, PennyLane makes it easy for anyone who

Fig. 2. Quantum Reinforcement Learning System

determines its action from policy-VQC and parameters are
updated with a classical CPU algorithm like Adam Optimizer.
This paper approaches similarly, by using a VQC depicted in
Fig. 1.
The quantum circuit
in Fig. 1 is a prototype of policy-
VQC, which consists of the basic structure of VQC. State
encoding part of the circuit includes Ry gates parameterized
by normalized state input s, having their values between −π
and π. Variational part in the center consists of entangling
CX Gates and Rx, Ry, Rz gates parameterized with free
parameter θ. This part is called a layer, and several can be
repeatedly stacked in a circuit. After that, measured output of
the circuit is decoded into the action space, yielding the action
probabilities. Then the obtained πθ is evaluated and updated
in a classical computer.

Environment(Cartpole)Hybrid (Quantum+Classical) RL AgentClassical OptimizerQuantum ActorClassical CriticReplay Buffer𝜋𝜃(𝑎|𝑠)Action𝑎Training Process⟨𝑆,𝐴,𝑃,𝑅,𝑆′⟩State𝑠Reward𝑠Next State𝑠′Evaluate Quantum actorAdam Optimizer, PPO(1) Train Classical Actor Parameters(2) Train Classical CriticVariational Quantum Circuit (see. Fig. 1)#Parameterized Rotation & Entanglement Layers
def layer(W):

for i in range(n_qubit):

qml.RX(W[i,0], wires=i)
qml.RY(W[i,1], wires=i)
qml.RZ(W[i,2], wires=i)

#Classical Critic
class V(nn.Module):

def __init__(self):

super(V, self).__init__()
self.fc1
self.fc_v

= nn.Linear(4,256)
= nn.Linear(256,1)

def forward(self,x):

x = F.relu(self.fc1(x))
v = self.fc_v(x)
return v

#Variational Quantum Policy Circuit (Actor)
@qml.qnode(dev, interface=’torch’)
def circuit(W,s):

# W: Layer Variable Parameters, s: State Variable

# Input Encoding
for i in range(n_qubit):

qml.RY(np.pi*s[i],wires=i)

#Variational Quantum Circuit
layer(W[0])
for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[1])

for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[2])
for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[3])
qml.CNOT(wires=[0,2])
qml.CNOT(wires=[1,3])
return [qml.expval(qml.PauliY(ind)) for ind in range(2,4)]

= Variable(torch.DoubleTensor(np.random.rand(4,4,3)),requires_grad=True)

#Declare Quantum Circuit and Parameters
W
v = V()
circuit_pi = circuit
optimizer1 = optim.Adam([W], lr=1e-3)
optimizer2 = optim.Adam(v.parameters(), lr=1e-5)

Fig. 3. Variational Quantum Policy Circuit with PennyLane

has previously done machine learning research using PyTorch
to start researching quantum machine learning. Based on this
background, in this paper, a quantum reinforcement learning
model was implemented using PennyLane and PyTorch as
shown in Fig. 3.

B. The CartPole Environment

Cartpole, the implementation environment in this paper, is
a test environment for reinforcement learning provided by
OpenAI [18]. This is a game where the agent moves the cart
back and forth to avoid dropping the stick on the cart and the
longer one holds the stick, the greater the reward. At every
moment, the player observes the cart’s position, velocity, and

the angle and angular velocity of the rod to determine which
direction to accelerate accordingly. The VQC in Fig. 1 using 4
qubits is suitable for policy making in this environment. Each
of the four pieces of information provided by the environment
is normalized and fed into the circuit as values between −π
and π, and the two measures are decoded into the probability
values of taking two actions via the softmax function. This
process continues until the agent can take the optimal action on
the given state information by optimizing the given parameters
in the reinforcement
learning algorithm. The experimental
result is showed later in this paper.

new to quantum reinforcement learning to start research with
interest. Although the performance of quantum reinforcement
learning cannot be said to be better than that of the existing
method, it is expected that many follow-up studies will yield
results that exceed the limitations of existing reinforcement
learning.

ACKNOWLEDGMENT

This work was supported by the National Research Foun-
dation of Korea (2019M3E4A1080391). Joongheon Kim is a
corresponding author of this paper.

REFERENCES

[1] J. Kim, Y. Kwak, S. Jung, and J.-H. Kim, “Quantum scheduling for
millimeter-wave observation satellite constellation,” in Proceedings of the
IEEE VTS Asia Paciﬁc Wireless Communications Symposium (APWCS),
2021, pp. 1–1.

[2] J. Choi and J. Kim, “A tutorial on quantum approximate optimization
algorithm (QAOA): Fundamentals and applications,” in Proceedings of
the IEEE International Conference on Information and Communication
Technology Convergence (ICTC), 2019, pp. 138–142.

[3] J. Choi, S. Oh, and J. Kim, “The useful quantum computing techniques for
artiﬁcial intelligence engineers,” in Proceedings of the IEEE International
Conference on Information Networking (ICOIN), 2020, pp. 1–3.

[4] Y. Kwak, W. J. Yun, S. Jung, and J. Kim, “Quantum neural networks:
Concepts, applications, and challenges,” CoRR, vol. abs/2108.01468,
2021.

[5] S. Oh, J. Choi, and J. Kim, “A tutorial on quantum convolutional neural
networks (QCNN),” in Proceedings of the IEEE International Conference
on Information and Communication Technology Convergence (ICTC),
2020, pp. 236–239.

[6] S. Oh, J. Choi, J.-K. Kim, and J. Kim, “Quantum convolutional neural
network for resource-efﬁcient image classiﬁcation: A quantum random
access memory (QRAM) approach,” in Proceedings of the IEEE Interna-
tional Conference on Information Networking (ICOIN), 2021, pp. 50–52.
[7] J. Choi, S. Oh, and J. Kim, “A tutorial on quantum graph recurrent neural
network (QGRNN),” in Proceedings of the IEEE International Conference
on Information Networking (ICOIN), 2021, pp. 46–49.

[8] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V.
Isakov, P. Massey, M. Y. Niu, R. Halavati, E. Peters, M. Leib, A. Skolik,
M. Streif, D. V. Dollen, J. R. McClean, S. Boixo, D. Bacon, A. K. Ho,
H. Neven, and M. Mohseni, “Tensorﬂow quantum: A software framework
for quantum machine learning,” arxiv, 2020.

[9] M. S. A. et al., “Qiskit: An open-source framework for quantum com-

puting,” arxiv, 2021.

[10] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S. Alam, S. Ahmed,
J. M. Arrazola, C. Blank, A. Delgado, S. Jahangiri, K. McKiernan,
J. J. Meyer, Z. Niu, A. Sz´ava, and N. Killoran, “Pennylane: Automatic
differentiation of hybrid quantum-classical computations,” arxiv, 2020.

[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing Atari with deep reinforcement learning,”
arXiv:1312.5602, 2013.

[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.
[13] J. Biamonte, “Universal variational quantum computation,” Physical

Review A, vol. 103, no. 3, p. L030401, 2021.

[14] N. Wiebe, A. Kapoor, and K. M. Svore, “Quantum deep learning,” arXiv

preprint arXiv:1412.3489, 2014.

[15] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.
Goan, “Variational quantum circuits for deep reinforcement learning,”
IEEE Access, vol. 8, pp. 141 007–141 024, 2020.

[16] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and V. Dunjko, “Vari-
learning,” arXiv preprint

ational quantum policies for reinforcement
arXiv:2103.05577, 2021.

[17] A. e. a. Paszke, “Pytorch: An imperative style, high-performance deep
learning library,” in Advances in Neural Information Processing Systems
(NIPS), 2019, pp. 8024–8035.

[18] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,

J. Tang, and W. Zaremba, “Openai gym,” 2016.

Fig. 4. Comparison of Total Reward on Average in Environment (CartPole-
v0)

C. Experimental Setup

Our experiment is conducted with the software packages,
PyTorch for speed and convenience of tensor operation and
and PennyLane for quantum circuit simulation. The quantum
simulator provided by Pennylane is very convenient to use,
but it is difﬁcult to use many qubits because of its slow
computational speed. Therefore, the CartPole environment was
used as a simple environment that can be operated with a
circuit of small qubits. We used classical parameter optimizer
as Adam optimizer with learning rate 0.001 for quantum policy
and 0.00001 for classical critic. Other hyperparameter settings
are γ = 0.98, λ = 0.95, (cid:15) = 0.01. The baseline model is a
random version of this model, using random parameters in
every time step without optimization.

D. Experimental Results

Fig. 4 shows the performance of the proposed quantum re-
inforcement learning model. Comparing with random actions,
one can see that the model is learning to ﬁnd the optimal
action. Also, it can be seen that the deviation of rewards
during the learning process is extremely high. This is due to
uncertainty within quantum systems, although the impact of re-
inforcement learning algorithms facilitating exploration cannot
be ignored either. This uncertainty simultaneously implies the
possibilities and limitations of quantum reinforcement learn-
ing. This allows effective policy exploration with relatively
few tens of parameters, but makes it difﬁcult to maintain the
good results once reached. Leveraging these characteristics is
an important challenge for quantum reinforcement learning.

V. CONCLUSIONS AND FUTURE WORK

Through this work, we implemented and tested a quantum
reinforcement learning model in the CartPole environment
based on PPO, one of the latest deep reinforcement learning
techniques, and demonstrated implementation guidelines in the
PennyLane library. We discussed the principle and potential of
the reinforcement learning using a variational quantum circuit.
Furthermore, this work aims to enable researchers who are

0200040006000Episode020406080100120Total Reward