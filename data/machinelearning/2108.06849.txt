Introduction to Quantum Reinforcement Learning:
Theory and PennyLane-based Implementation

â—¦Yunseok Kwak, â—¦Won Joon Yun, â€ Soyi Jung, â—¦Jong-Kook Kim, and â—¦Joongheon Kim
â—¦School of Electrical Engineering, Korea University, Seoul, Republic of Korea
â€ School of Software, Hallym University, Chungcheon, Republic of Korea
E-mails: rhkrdbstjr0@korea.ac.kr, ywjoon95@korea.ac.kr, jungsoyi@korea.ac.kr,
jongkook@korea.ac.kr, joongheon@korea.ac.kr

1
2
0
2

g
u
A
6
1

]

G
L
.
s
c
[

1
v
9
4
8
6
0
.
8
0
1
2
:
v
i
X
r
a

Abstractâ€”The emergence of quantum computing enables for
researchers to apply quantum circuit on many existing studies.
Utilizing quantum circuit and quantum differential program-
ming, many research are conducted such as Quantum Machine
Learning (QML). In particular, quantum reinforcement learning
is a good ï¬eld to test the possibility of quantum machine learning,
and a lot of research is being done. This work will introduce the
concept of quantum reinforcement learning using a variational
quantum circuit, and conï¬rm its possibility through implemen-
tation and experimentation. We will ï¬rst present the background
knowledge and working principle of quantum reinforcement
learning, and then guide the implementation method using the
PennyLane library. We will also discuss the power and possibility
of quantum reinforcement learning from the experimental results
obtained through this work.

I. INTRODUCTION

Since deep reinforcement learning has opened a new chapter
in reinforcement learning by leveraging the power of artiï¬-
cial neural networks, there have been achievements such as
surpassing human limits in complex games such as chess and
Go, and it has already become an unavoidable ï¬‚ow of artiï¬cial
intelligence (AI) research.

On the other hand, advanced quantum computing tech-
nology has already reached a level close to the imple-
mentation of quantum computational gain predicted through
many algorithm studies [1]â€“[3]. In addition, the advent of
a variational quantum circuit (VQC) that mimics the prin-
ciples and functions of artiï¬cial neural networks has made
it possible to apply these quantum calculations to existing
machine learning algorithms. This has established itself as a
major trend in quantum machine learning research, and many
studies using it are being actively conducted. In this context,
many studies are conducted using VQC as a Quantum Neural
Network (QNN) [4]â€“[7], including variational classiï¬er, image
preprocessor, federated learning, reinforcement learning, etc.
Among them, in this paper, we introduce and discuss quantum
reinforcement learning, a reinforcement learning model that
replaces the artiï¬cial neural network of a deep Q network
(DQN) with a VQC.

Since QPU has not been commercialized yet,

there is
nothing better in speed than the existing machine learn-
ing framework that utilizes NPU. However, as development
libraries such as Tensorï¬‚ow-quantum [8], Qiskit [9], and
PennyLane [10] for future quantum computing environments

and quantum computing clouds such as IBMQ, IonQ, and
Amazon Braket are provided to developers, various stud-
ies on QML are in progress. Particularly, PennyLane [10],
is a suitable library for starting quantum machine learning
research because it provides a simulator that allows users
to easily implement quantum circuits by using a CPU to
perform QPU operations. Therefore, we aim to increase access
to quantum reinforcement learning and facilitate subsequent
research by brieï¬‚y introducing the implementation process
through PennyLane. In addition, we would like to discuss the
impacts and potentials of quantum computing in reinforcement
learning through experimentation and evaluation of quantum
reinforcement learning models in the CartPole environment
provided by OpenAI.

II. BACKGROUNDS

A. Reinforcement Learning

Reinforcement

learning is mathematically modeled with
Markov Decision Process (MDP) as a tuple (S, A, P, R, T ),
where S is a ï¬nite set of state information, and A is a ï¬nite
set of action information. The function P : S Ã— A â†’ P (S)
is a transition probability function, with P (s(cid:48) | s, a) being
the probability of transitioning into state s(cid:48) if an agent starts
executing action a in state s. The function R : S Ã—AÃ—S â†’ R
denotes the reward function, with Rt = R(st, at, st+1). The
MDP has a ï¬nite time horizon T , and solving an MDP means
ï¬nding a optimal policy Ï€âˆ—
Î¸ âˆˆ Î  : S Ã— A â†’ [0, 1], where Ï€Î¸ is
neural network-based policy with parameter Î¸; Observing s, Ï€Î¸
determines agentâ€™s action a âˆˆ A to maximize the cumulative
rewards received during the ï¬nite time T .

When the environment

transitions and the policy are
stochastic, the probability of a T -step trajectory is deï¬ned as
P (Ï„ | Ï€Î¸) = Ï(s0) (cid:81)T âˆ’1
t=0 P (st+1 | st, at)Ï€Î¸(at | st) where Ï
is the initial state distribution. Then, the expected return J (Ï€Î¸)
is deï¬ned as J (Ï€Î¸) = (cid:82)
Ï„ P (Ï„ | Ï€Î¸)R(Ï„ ) = EÏ„ âˆ¼Ï€Î¸ [R(Ï„ )]
where the trajectory Ï„ is a sequence of states and actions in
the environment. The objective of reinforcement learning is
to learn a policy that maximizes the expected return J (Ï€Î¸)
when the agent acts according to the policy Ï€Î¸. Therefore, the
optimization objective is expressed by

Ï€âˆ—
Î¸ = arg max

with Ï€âˆ—

Î¸
Î¸ being the optimal policy.

J (Ï€Î¸)

(1)

 
 
 
 
 
 
Deep Q-Network (DQN)
[11]. One of the conventional
method for solving MDP is Q-Learning. Q-Learning utilizes
Q-table to ï¬nd optimal policy. However, Q-Learning has limi-
tation that it obtains optimal policy when the state dimension is
small. Inspired to Q-Learning, deep Q-network (DQN) which
is a model-free reinforcement learning, is proposed to learn the
optimal policy with a high-dimensional state space. Experience
replay D and target network are two key features used for
training deep neural network with stabilization. Experiences
et = (st, at, Rt+1, st+1) of the agent are stored in the
experience buffer D = (e1, e2, . . . , eT ), and are periodically
resampled to train the Q-networks. Sampled experience is used
to update the parameters Î¸i of the policy with the loss function
at the i-th training iteration where the loss function is deï¬ned
as

L(Î¸i) = E [(Rt+1+

Q(st+1, a(cid:48); Î¸âˆ’

i ) âˆ’ Q(st, at; Î¸i))2(cid:105)

(2)

Î³ max
a(cid:48)

where Î¸âˆ’
are the target network parameters. The target
i
network parameters Î¸âˆ’
are updated using the Q-network
i
parameters Î¸ in every predeï¬ned step. The stochastic gradient
descent method is used to optimize the loss function.
Proximal Policy Optimization (PPO) [12]. PPO is one of the
breakthroughs of DRL algorithms for improving the training
stability by ensuring that Ï€Î¸ updates at every iteration are small
by clipping the probability ratio rÏ€(Î¸) = Ï€Î¸(a | s)/Ï€Î¸old (a |
s), where Î¸old is that of previous updated parameters of policy.
(Schulman et al., 2017) proposed a surrogate function that has
objective that prevents the new policy from straying away from
the old one is used to train the policy Ï€Î¸. The clipped objective
function is as follows:

LCLIP
t

(Î¸) = min(rt(Î¸)At, clip(rt(Î¸), 1 âˆ’ (cid:15), 1 + (cid:15))At),

(3)

where At is the estimated advantage function under hyperpa-
rameter (cid:15) < 1, which means how far away the new policy is
allowed to update from the old policy. PPO uses the stochastic
gradient descent to maximize the objective (3).

B. Quantum Computing

Quantum computers use a qubit as the basic unit of compu-
tation, which represent a quantum superposition state between
two basis state |0(cid:105) and |1(cid:105). It is controlled by unitary gates in
a quantum circuit to perform various quantum operations. It
can be represented as a normalized two-dimensional complex
vector as:

|Ïˆ(cid:105) = Î±|0(cid:105) + Î²|1(cid:105), where (cid:107)Î±(cid:107)2

2 + (cid:107)Î²(cid:107)2

2 = 1,

(4)

and there also is a geometrical representation of a qubit space,
using polar coordinates Î¸ and Ï†:

Fig. 1. A policy-VQC for deep-Q learning with parameter Î¸.

qubit state into another qubit state, which can be represented as
a 2Ã—2 matrix with complex entries. There are some important
quantum gates, Pauli-X, Pauli-Y , and Pauli-Z, rotating by Ï€
around their corresponding axes in Bloch sphere. The rotation
operator gates Rx(Î¸), Ry(Î¸), and Rz(Î¸) rotate by Î¸ instead
of Ï€ in Pauli-X, Pauli-Y , and Pauli-Z gates, and it is known
that any single-qubit unitary gate in SU (2) can be written as
a product of three rotation operators of each axis. In addition,
there are quantum gates which operate on multiple qubits,
called controlled rotation gates. They act on a qubit accord-
ing to the signal of several control qubits, which generates
quantum entanglement between multiple qubits. Among them,
Controlled X(or CNOT) gate is one of the most used control
gates, changing the sign of the second qubit if the ï¬rst qubit is
|1(cid:105). These gates allow quantum algorithms to work using their
features on a quantum circuit that will be introduced later.

C. Variational Quantum Circuit

The variational quantum circuit (or parameterized quantum
circuit) is a quantum circuit using learnable parameters to
perform various numerical tasks, such as optimization, approx-
imation, and classiï¬cation. Operation of general VQC model
can be divided into 4 steps. First one is state preperation
step, the input information is encoded into corresponding qubit
states, which can be treated in the quantum circuit. Next step is
variational step, entangling qubit states by controlled gates and
rotating qubits by parameterized rotation gates. This process
can be repeated in a multi-layer manner with more parameters,
which possibly enhance the performance of the circuit. In the
third step, processed qubit states are measured and decoded
to the form of appropriate output information. Last step is
conducted outside the circuit. The quantum circuit parameters
are updated in the direction of optimizing the objective func-
tion of the algorithm by a classical CPU algorithm, like Adam
optimizer. Then the circuit updated with the new parameters
performs the calculation again from the beginning. This circuit
is known to be able to approximate any continuous function
like classical neural network [13], so VQC is often called
Quantum Neural Network (QNN) [14]. It has been widely
applied in quantum machine learning researches.

III. QUANTUM REINFORCEMENT LEARNING

|Ïˆ(cid:105) = cos(Î¸/2)|0(cid:105) + eiÏ† sin(Î¸/2)|1(cid:105),

(5)

A. Variational Quantum Policy Circuit

where 0 â‰¤ Î¸ â‰¤ Ï€ and 0 â‰¤ Ï† â‰¤ Ï€. Qubit state is mapped into
the surface of 3-dimensional unit sphere, which is called Bloch
sphere. Quantum gate is a unitary operator transforming a

In recent studies of quantum reinforcement learning [15],
[16], VQC substitutes the policy training DNN of existing
DRL. At each episode, agent with given state information

ğ‘…y(s2)|Û§0|Û§0|Û§0|Û§0ğ‘…y(s1)ğ‘…y(s3)ğ‘…y(s4)ğ‘…x,ğ‘¦,z(ğœƒ11,ğœƒ12,ğœƒ13)ğ‘…x,ğ‘¦,z(ğœƒ21,ğœƒ22,ğœƒ23)ğ‘…x,ğ‘¦,z(ğœƒ31,ğœƒ32,ğœƒ33)ğ‘…x,ğ‘¦,z(ğœƒ41,ğœƒ42,ğœƒ43)State EncodingParameterized Rotation  & Entanglement LayersMeasure Decoding to Action ProbabilityAlgorithm 1 Variational Quantum Deep Q Learning with PPO

Initialize replay memory D to capacity N
Initialize action-value function quantum circuit Q with random parameters Î¸
Initialize state value function V (s; Ï†)
for episode = 1, 2, . . . , M do

Initialise state s1 and encode into the quantum state
# 1. Inference Process #
for t = 1, 2, . . . , T do

With probability (cid:15) select a random action at
otherwise select at = maxa Qâˆ—(st, a; Î¸) from the output of the quantum circuit
Execute action at in emulator and observe reward rt and next state st+1
Store transition (st, at, Rt, st+1) in D

end for
# 2. Training Process #
for i = 1, ..., Kepoch do

Sample random mini-batch of transitions (sj, aj, Rj, sj+1) from D

Calculate temporal difference target, yj =

(cid:26) Rj

Rj + Î³ maxa(cid:48) Q(sj+1, a(cid:48); Î¸)

for terminal sj+1
for non-terminal sj+1

Calculate temporal difference, Î´j = yj âˆ’ V (sj)
Calculate estimated advantage function, Ë†Aj = Î´j + (Î³Î»)Î´j+1 + ... + (Î³Î»)Jâˆ’j+1Î´Jâˆ’1
Calculate ratio, rj = Ï€Î¸(aj |sj )
Calculate surrogate actor loss function using (3)
Calculate critic loss function, |V (s) âˆ’ yj|.
Calculate gradient and update actor and critic parameters

Ï€Î¸OLD (aj |sj )

end for

end for

B. Quantum Reinforcement Learning Systems

The quantum reinforcement learning system in this paper
is as described in Fig. 2. In the beginning of an episode,
quantum-classical hybrid agent receives a state information
from the environment, and determine its action by Î¸Ï€ made
from the VQC. Then the policy of the agent is evaluated and
updated by PPO algorithm, which is introduced before and de-
scribed in Algorithm 0. The PPO algorithm are effectively the
same as in the previous study [12]. The replay buffer functions
in the same way as in traditional approaches, keeping track of
the (cid:104)s, a, R, s(cid:48)(cid:105) tuples. One does not have to fundamentally or
drastically change an algorithm in order to apply the power of
VQCs to it. The algorithm presented in Algorithm 0.

IV. THE IMPLEMENTATION OF QRL

A. Implementation Guidelines

As quantum computing and quantum machine learning
research is actively progressing, many development libraries
for researchers have emerged, such as TensorFlow-quantum,
Qiskit, and PennyLane. Among them, PennyLane was cre-
ated to support quantum machine learning research, allowing
anyone to easily test the performance of quantum circuits
through quantum simulators. The quantum simulator supported
by PennyLane allows the CPU to imitate the operation of
QPU, and especially supports the use of parameters in the
form of PyTorch tensor and gradient operation [17]. Thanks
to these features, PennyLane makes it easy for anyone who

Fig. 2. Quantum Reinforcement Learning System

determines its action from policy-VQC and parameters are
updated with a classical CPU algorithm like Adam Optimizer.
This paper approaches similarly, by using a VQC depicted in
Fig. 1.
The quantum circuit
in Fig. 1 is a prototype of policy-
VQC, which consists of the basic structure of VQC. State
encoding part of the circuit includes Ry gates parameterized
by normalized state input s, having their values between âˆ’Ï€
and Ï€. Variational part in the center consists of entangling
CX Gates and Rx, Ry, Rz gates parameterized with free
parameter Î¸. This part is called a layer, and several can be
repeatedly stacked in a circuit. After that, measured output of
the circuit is decoded into the action space, yielding the action
probabilities. Then the obtained Ï€Î¸ is evaluated and updated
in a classical computer.

Environment(Cartpole)Hybrid (Quantum+Classical) RL AgentClassical OptimizerQuantum ActorClassical CriticReplay Bufferğœ‹ğœƒ(ğ‘|ğ‘ )Actionğ‘Training ProcessâŸ¨ğ‘†,ğ´,ğ‘ƒ,ğ‘…,ğ‘†â€²âŸ©Stateğ‘ Rewardğ‘ Next Stateğ‘ â€²Evaluate Quantum actorAdam Optimizer, PPO(1) Train Classical Actor Parameters(2) Train Classical CriticVariational Quantum Circuit (see. Fig. 1)#Parameterized Rotation & Entanglement Layers
def layer(W):

for i in range(n_qubit):

qml.RX(W[i,0], wires=i)
qml.RY(W[i,1], wires=i)
qml.RZ(W[i,2], wires=i)

#Classical Critic
class V(nn.Module):

def __init__(self):

super(V, self).__init__()
self.fc1
self.fc_v

= nn.Linear(4,256)
= nn.Linear(256,1)

def forward(self,x):

x = F.relu(self.fc1(x))
v = self.fc_v(x)
return v

#Variational Quantum Policy Circuit (Actor)
@qml.qnode(dev, interface=â€™torchâ€™)
def circuit(W,s):

# W: Layer Variable Parameters, s: State Variable

# Input Encoding
for i in range(n_qubit):

qml.RY(np.pi*s[i],wires=i)

#Variational Quantum Circuit
layer(W[0])
for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[1])

for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[2])
for i in range(n_qubit-1):

qml.CNOT(wires=[i,i+1])

layer(W[3])
qml.CNOT(wires=[0,2])
qml.CNOT(wires=[1,3])
return [qml.expval(qml.PauliY(ind)) for ind in range(2,4)]

= Variable(torch.DoubleTensor(np.random.rand(4,4,3)),requires_grad=True)

#Declare Quantum Circuit and Parameters
W
v = V()
circuit_pi = circuit
optimizer1 = optim.Adam([W], lr=1e-3)
optimizer2 = optim.Adam(v.parameters(), lr=1e-5)

Fig. 3. Variational Quantum Policy Circuit with PennyLane

has previously done machine learning research using PyTorch
to start researching quantum machine learning. Based on this
background, in this paper, a quantum reinforcement learning
model was implemented using PennyLane and PyTorch as
shown in Fig. 3.

B. The CartPole Environment

Cartpole, the implementation environment in this paper, is
a test environment for reinforcement learning provided by
OpenAI [18]. This is a game where the agent moves the cart
back and forth to avoid dropping the stick on the cart and the
longer one holds the stick, the greater the reward. At every
moment, the player observes the cartâ€™s position, velocity, and

the angle and angular velocity of the rod to determine which
direction to accelerate accordingly. The VQC in Fig. 1 using 4
qubits is suitable for policy making in this environment. Each
of the four pieces of information provided by the environment
is normalized and fed into the circuit as values between âˆ’Ï€
and Ï€, and the two measures are decoded into the probability
values of taking two actions via the softmax function. This
process continues until the agent can take the optimal action on
the given state information by optimizing the given parameters
in the reinforcement
learning algorithm. The experimental
result is showed later in this paper.

new to quantum reinforcement learning to start research with
interest. Although the performance of quantum reinforcement
learning cannot be said to be better than that of the existing
method, it is expected that many follow-up studies will yield
results that exceed the limitations of existing reinforcement
learning.

ACKNOWLEDGMENT

This work was supported by the National Research Foun-
dation of Korea (2019M3E4A1080391). Joongheon Kim is a
corresponding author of this paper.

REFERENCES

[1] J. Kim, Y. Kwak, S. Jung, and J.-H. Kim, â€œQuantum scheduling for
millimeter-wave observation satellite constellation,â€ in Proceedings of the
IEEE VTS Asia Paciï¬c Wireless Communications Symposium (APWCS),
2021, pp. 1â€“1.

[2] J. Choi and J. Kim, â€œA tutorial on quantum approximate optimization
algorithm (QAOA): Fundamentals and applications,â€ in Proceedings of
the IEEE International Conference on Information and Communication
Technology Convergence (ICTC), 2019, pp. 138â€“142.

[3] J. Choi, S. Oh, and J. Kim, â€œThe useful quantum computing techniques for
artiï¬cial intelligence engineers,â€ in Proceedings of the IEEE International
Conference on Information Networking (ICOIN), 2020, pp. 1â€“3.

[4] Y. Kwak, W. J. Yun, S. Jung, and J. Kim, â€œQuantum neural networks:
Concepts, applications, and challenges,â€ CoRR, vol. abs/2108.01468,
2021.

[5] S. Oh, J. Choi, and J. Kim, â€œA tutorial on quantum convolutional neural
networks (QCNN),â€ in Proceedings of the IEEE International Conference
on Information and Communication Technology Convergence (ICTC),
2020, pp. 236â€“239.

[6] S. Oh, J. Choi, J.-K. Kim, and J. Kim, â€œQuantum convolutional neural
network for resource-efï¬cient image classiï¬cation: A quantum random
access memory (QRAM) approach,â€ in Proceedings of the IEEE Interna-
tional Conference on Information Networking (ICOIN), 2021, pp. 50â€“52.
[7] J. Choi, S. Oh, and J. Kim, â€œA tutorial on quantum graph recurrent neural
network (QGRNN),â€ in Proceedings of the IEEE International Conference
on Information Networking (ICOIN), 2021, pp. 46â€“49.

[8] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V.
Isakov, P. Massey, M. Y. Niu, R. Halavati, E. Peters, M. Leib, A. Skolik,
M. Streif, D. V. Dollen, J. R. McClean, S. Boixo, D. Bacon, A. K. Ho,
H. Neven, and M. Mohseni, â€œTensorï¬‚ow quantum: A software framework
for quantum machine learning,â€ arxiv, 2020.

[9] M. S. A. et al., â€œQiskit: An open-source framework for quantum com-

puting,â€ arxiv, 2021.

[10] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S. Alam, S. Ahmed,
J. M. Arrazola, C. Blank, A. Delgado, S. Jahangiri, K. McKiernan,
J. J. Meyer, Z. Niu, A. SzÂ´ava, and N. Killoran, â€œPennylane: Automatic
differentiation of hybrid quantum-classical computations,â€ arxiv, 2020.

[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, â€œPlaying Atari with deep reinforcement learning,â€
arXiv:1312.5602, 2013.

[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
â€œProximal policy optimization algorithms,â€ arXiv:1707.06347, 2017.
[13] J. Biamonte, â€œUniversal variational quantum computation,â€ Physical

Review A, vol. 103, no. 3, p. L030401, 2021.

[14] N. Wiebe, A. Kapoor, and K. M. Svore, â€œQuantum deep learning,â€ arXiv

preprint arXiv:1412.3489, 2014.

[15] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.
Goan, â€œVariational quantum circuits for deep reinforcement learning,â€
IEEE Access, vol. 8, pp. 141 007â€“141 024, 2020.

[16] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and V. Dunjko, â€œVari-
learning,â€ arXiv preprint

ational quantum policies for reinforcement
arXiv:2103.05577, 2021.

[17] A. e. a. Paszke, â€œPytorch: An imperative style, high-performance deep
learning library,â€ in Advances in Neural Information Processing Systems
(NIPS), 2019, pp. 8024â€“8035.

[18] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,

J. Tang, and W. Zaremba, â€œOpenai gym,â€ 2016.

Fig. 4. Comparison of Total Reward on Average in Environment (CartPole-
v0)

C. Experimental Setup

Our experiment is conducted with the software packages,
PyTorch for speed and convenience of tensor operation and
and PennyLane for quantum circuit simulation. The quantum
simulator provided by Pennylane is very convenient to use,
but it is difï¬cult to use many qubits because of its slow
computational speed. Therefore, the CartPole environment was
used as a simple environment that can be operated with a
circuit of small qubits. We used classical parameter optimizer
as Adam optimizer with learning rate 0.001 for quantum policy
and 0.00001 for classical critic. Other hyperparameter settings
are Î³ = 0.98, Î» = 0.95, (cid:15) = 0.01. The baseline model is a
random version of this model, using random parameters in
every time step without optimization.

D. Experimental Results

Fig. 4 shows the performance of the proposed quantum re-
inforcement learning model. Comparing with random actions,
one can see that the model is learning to ï¬nd the optimal
action. Also, it can be seen that the deviation of rewards
during the learning process is extremely high. This is due to
uncertainty within quantum systems, although the impact of re-
inforcement learning algorithms facilitating exploration cannot
be ignored either. This uncertainty simultaneously implies the
possibilities and limitations of quantum reinforcement learn-
ing. This allows effective policy exploration with relatively
few tens of parameters, but makes it difï¬cult to maintain the
good results once reached. Leveraging these characteristics is
an important challenge for quantum reinforcement learning.

V. CONCLUSIONS AND FUTURE WORK

Through this work, we implemented and tested a quantum
reinforcement learning model in the CartPole environment
based on PPO, one of the latest deep reinforcement learning
techniques, and demonstrated implementation guidelines in the
PennyLane library. We discussed the principle and potential of
the reinforcement learning using a variational quantum circuit.
Furthermore, this work aims to enable researchers who are

0200040006000Episode020406080100120Total Reward