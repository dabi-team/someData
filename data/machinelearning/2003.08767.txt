A Review of Computational Approaches for Evaluation of Rehabilitation Exercises 

Yalin Liao1, Aleksandar Vakanski1*, Member, IEEE, Min Xian1, Member, IEEE, David Paul2, and Russell Baker2 

1 Department of Computer Science, University of Idaho, Idaho Falls, USA 

2 Department of Movement Sciences, University of Idaho, Moscow, USA 

*Corresponding Author: 1776 Science Center Drive, Idaho Falls, ID 83402; vakanski@uidaho.edu; (+1)208-757-5422 

Abstract—  Recent  advances  in  data  analytics  and  computer-aided  diagnostics  stimulate  the  vision  of  patient-centric  precision 

healthcare, where treatment plans are customized based on the health records and needs of every patient. In physical rehabilitation, 

the  progress in  machine learning and the advent of affordable and reliable motion capture sensors have been conducive to the 

development  of  approaches  for  automated  assessment  of  patient  performance  and  progress  toward  functional  recovery.  The 

presented study  reviews computational approaches  for evaluating patient performance  in rehabilitation  programs using  motion 

capture systems. Such approaches will play an important role in supplementing traditional rehabilitation assessment performed by 

trained clinicians, and in assisting patients participating in home-based rehabilitation. The reviewed computational methods for 

exercise evaluation are grouped into three main categories: discrete movement score, rule-based, and template-based approaches. 

The review places an emphasis on the application of machine learning methods for movement evaluation in rehabilitation. Related 

work in the literature on data representation, feature engineering, movement segmentation, and scoring functions is presented. The 

study  also  reviews  existing  sensors  for  capturing  rehabilitation  movements  and  provides  an  informative  listing  of  pertinent 

benchmark datasets. The significance of this paper is in being the first to provide a comprehensive review of computational methods 

for evaluation of patient performance in rehabilitation programs.  

Keywords: Physical rehabilitation; motion capture sensors; rehabilitation datasets; movement assessment methods  

1.  Introduction 

Physical rehabilitation is commonly prescribed to benefit patients who suffer from physical impairments or disabilities, or need to 

restore functional abilities after injury or surgery [1]–[3]. Numerous studies in the literature underline the essential role of physical 

rehabilitation for improved patient outcomes and  emphasize the  strong correlation between  exercise intensity and outcomes of 

rehabilitation  programs  [4]–[7].  However,  rehabilitation  treatment  imposes  a  substantial  economic  burden  on  patients  and 

healthcare systems [8]–[10]. For instance, the cost of physical rehabilitation programs in the US in 2007 was about 13.5 billion 

dollars based on the Medical Expenditure Panel Survey generated by the US federal government [9]. The expenditure was produced 

by nearly 9 million adults during approximately 88 million physical rehabilitation episodes.  

In rehabilitation programs,  a clinician instructs patients and monitors their performance of rehabilitation exercises in  a clinical 

setting. This type of rehabilitation treatment is restricted by the availability of trained clinicians and it places demands on patients’ 

schedules. To increase the flexibility of rehabilitation programs, home-based rehabilitation is often employed as a supplement to 

clinic-based programs. In home-based regimens, a clinician customizes a personal rehabilitation plan for a patient consisting of a 

set of recommended exercises. Patients then perform the exercises following the given instructions, record their daily progress in 

a logbook, and visit the clinic periodically for progress assessment. Reports in the literature indicate more than 90% of rehabilitation 

programs are executed in an at-home environment [11]. Nevertheless, a number of medical sources report low levels of patient 

motivation and adherence to the prescribed exercise regimens in home-based rehabilitation, leading to prolonged treatment duration 

and increased healthcare cost [12], [13]. Although many factors that reduce patient motivation and engagement in rehabilitation 

1 

 
 
 
 
training have been identified, the lack of timely feedback and real-time supervision by a healthcare professional  in an at-home 

setting is often cited as the most influential factor [14]. Poor motivation and supervision promote further risk because patients may 

perform exercises incorrectly as a result of those factors, which increases the risk of re-injury [3], [4], [12]. 

Accordingly,  there  is  a  demand  for  novel  tools  and  equipment  to  support  home-based  rehabilitation,  such  as  robotic  assistive 

devices [15], exoskeletons, haptic devices [16], and virtual gaming environments [17]. With the advent of low-cost motion capture 

sensors, like Microsoft Kinect [18] and Asus Xtion [19], there has been a surge in related biomedical applications [20], [21]. VERA 

(Virtual Exercise Rehabilitation Assistant) [11] and KiReS (Kinect Rehabilitation System) [22] are exemplars of such tools, used 

for support of rehabilitation exercises. These systems employ a Kinect sensor to track patient movements, where a user interface 

displays two avatars that perform the prescribed exercise by the clinician and the ongoing movements and postures performed by 

the patient in real-time. Such visual feedback assists patients in improving their exercise performance, as well as in taking self-

corrective action when needed [23]. Furthermore, the recordings of the daily exercise sessions can be sent via the internet to the 

respective clinician, who can assess the performance and provide feedback or corrective recommendations.  

Likewise, traditional clinical assessment of patient progress is often based on a clinician’s visual observation of patient movement 

or  exercise  performance  [24].  Commonly  used  tests  for  this  purpose  may  be  condition-specific  (e.g.,  FMA  [Fugl-Meyer 

assessment],  WMFT  [Wolf  motor  function  test],  the  ratio  of  optimal  motion  execution  [11],  [22]),  more  general  screening  of 

movement competency or performance (e.g., FMS [Functional Movement ScreenTM]), an evaluation of a specific muscle or joint 

(e.g.,  manual  muscle  test,  range  of  motion  testing),  or  a  generic  evaluation  of  specific  skill  of  sport  performance  [25].  Some 

commonly utilized clinical evaluations or screening tools may be more objective and quantitative in nature, while others may rely 

more on a clinician’s intuitive understanding and subjective rating of the patient's performance. Clinical tests/evaluations that are 

more subjective may have issues with the reliability and validity of the evaluation depending on the movement, exercise, or task 

being assessed [25]–[27]. Due to the subjectivity and challenge of evaluating many of the exercises and movements, sensors and 

data analytics may support clinical assessment by providing a complementary objective and quantitative measure of the quality of 

patient performance. For example, Oña Simbaña et al. [28] reviewed the systems for automated assessment of upper limb motor 

function using standard clinical tests. 

To address the challenges associated with home-based and in-clinic rehabilitation programs, the development of systems that can 

reliably capture human movements, automatically analyze the recorded data, and evaluate the quality of the movement performance 

is critical. The provision of low-cost sensors with integrated functionality for tracking human motions provides an opportunity for 

the development of such systems. Furthermore, devising efficient computational algorithms for modeling and analyzing human 

motions becomes central to solving the problem of rehabilitation evaluation.  

With  regard  to  the  sensory  perception  aspect  of  movement  evaluation,  obtaining  precise  movement  data  by  motion  sensors  is 

crucial. Although a standard vision camera can be used as a motion sensor [29], [30], these cameras provide only 2-dimensional 

information about the captured scene and the lack of the third dimension’s information imposes limits on the evaluation accuracy. 

To cope with this deficiency, one alternative is to use optical motion tracking systems, which employ a set of markers attached to 

strategic  locations  on  a  patient’s  body  that  are  tracked  by  multiple  high-resolution  cameras  [31].  These  systems  rely  on 

computational algorithms to reconstruct the 3-dimensional scene by comparing and aligning the images taken by the set of multiple 

cameras. Although optical trackers are highly accurate and reliable, the high cost and need for attaching a set of markers during 

every  session  render  them  unsuitable  for  most  cases  of  rehabilitation  evaluation.  Recent  technology  for  3-dimensional  scene 

reconstruction based on vison/depth  cameras has become popular due to  the low cost and ease of use.  Among the commercial 

2 

 
 
vision/depth sensors, Microsoft Kinect [18] has been the preferred choice in most related works. Inertial sensors and accelerometers 

have also been extensively used for motion tracking and evaluation [32], due to their low cost and simple principles of operation. 

Skeletal data extracted from color/depth cameras or inertial sensors are widely used in the domain of rehabilitation evaluation. 

Such data consist of time-ordered sequences of angular or position coordinates of the joints in the human body. Full-body skeletal 

data are highly redundant, thus, they are rarely applied directly for modeling and analysis of human motions. Consequently, feature 

engineering via selection of important skeletal dimensions or distances is often employed for extracting relevant information from 

skeletal data [33]–[35]. Another common avenue for feature engineering entails creating new local features for representing the 

motions, based on a set of kinematic parameters that are predefined for an exercise, or by employing a functional mapping [36], 

[37].  Similarly,  an  often  used processing step in  feature engineering is dimensionality reduction,  where unimportant or highly 

correlated dimensions are excluded from the data [38]–[41]. Principal component analysis (PCA) and its variants are widely used 

for dimensionality reduction  of  movement data [42]. Recently, a line of  work emerged  that  uses  machine learning  models for 

automated  extraction  of  features  from  collected  rehabilitation  data  [39],  [43],  [44].  The  efficient  feature  engineering  of  these 

algorithms produces an improved representation of the raw input data for movement evaluation, and it is an important constituent 

of the pertinent methods. 

In  this  review,  we  adopted  the  following  taxonomy  for  the  main  categories  of  computational  methods  for  evaluation  of 

rehabilitation exercises using motion capture data: discrete movement score, rule-based and template-based approaches. Discrete 

movement score approaches [45]–[48]  classify individual repetitions of rehabilitation exercises into discrete classes, e.g., correct 

or incorrect. Conventional machine learning classifiers are commonly employed for this task, where the outputs are discrete class 

values of 0 or 1 (i.e., incorrect or correct repetition). A shortcoming of these methods is the inability to detect subtle changes in 

patient performance and provide intermediate levels of movement quality (for instance, with scores between 0 and 1). Rule-based 

approaches [36], [49]–[51] utilize a set of rules for a considered rehabilitation exercise defined in advance by clinicians or human 

movement experts. The rules are used as a gold standard for evaluating the level of correctness. A disadvantage of these approaches 

is  that  the  rules  are  exercise-specific,  and  cannot  be  reused  for  other  exercises.  Template-based  approaches  are  based  on 

comparisons  of  measured  movements  with  a  template  of  the  movements.  The  template  is  typically  obtained  from  correct 

performance of the exercises by healthy subjects. One group of respective approaches employs distance functions for calculating 

a similarity score between patient-performed repetitions and reference template repetitions: Euclidean, Mahalanobis, and dynamic 

temporal warping distances are the most frequently used functions for this purpose [52]–[59]. The benefit of the distance functions 

is that they are not exercise-specific, and thus can be applied for  evaluation  of  new types of exercises.  Another line of  works 

exploits the ability by probabilistic models with latent variables to encode spatial variability and temporal dynamics of human 

movements [39]–[41], [54], [60]–[62]. For instance, Gaussian mixture models [39], [63] and hidden Markov models [61], [62] 

were used for motion modeling, and quality assessment is based on the likelihood that the individual sequences are being drawn 

from a derived model. Whereas the probabilistic models are advantageous in handling the variability due to the stochastic character 

of human movements, models with abilities for a hierarchical data representation (such as deep neural networks [64]) can produce 

more  reliable  outcomes  for  movement  quality  evaluation,  and  better  generalize  across  individual  patients  and  musculoskeletal 

conditions. 

In the context of objective evaluation of patient movements in rehabilitation programs, it is important to note that a large number 

of robotic and mechanical devices have been designed and are commonly used for quantitative movement evaluation. Accordingly, 

multiple review papers have provided overviews of related works in the literature [15], [65]–[70]. The review of rehabilitation 

3 

 
 
assessment  using  robotic  and  mechanical  devices  is  beyond  the  scope  of  this  work,  and  we  place  emphasis  on  quantitative 

evaluation using computational approaches for analysis of movement data collected with motion capture sensors.          

This review is organized as follows. Motion sensors for capturing rehabilitation exercises are reviewed in Section 2. Section 3 

presents  the  related  benchmark  datasets  for  rehabilitation  movements.  Sections  4  discusses  feature  engineering  for  movement 

analysis. Section 5 details the various approaches for evaluation of patient movements during the performance of rehabilitation 

exercises. Scoring functions used to scale and adjust movement performance quantities and movement segmentation are discussed 

in the ensuing two sections. Potential future research directions are presented in Section 8. The final section briefly summarizes 

and concludes the paper.  

2.  Motion Capture Sensors 

In general, a motion sensor is a device, module, or subsystem used to detect physical movement within an environment in real-

time. Over the last decades, various advanced sensors for capturing human movement were developed. Currently, two types of 

motion sensors are widely used for rehabilitation exercise evaluation: optical and inertial sensors. Further, the  respective optical 

sensors can be classified into two broad categories: vison/depth cameras and marker-based motion capture systems. 

2.1. Vision/depth Cameras 

There are two  main types of  depth cameras: structured light (SL) and time-of-flight (ToF)  cameras. The commercial products 

Kinect v1 and Asus Xtion belong to the former. Kinect v2 and Azure Kinect are based on the ToF principle. SL cameras have 

lower cost, as they are simpler to construct. On the other hand, ToF cameras are more expensive, but are less affected by light 

variations, and therefore can be used in an outdoor environment.  

Kinect v1—Microsoft launched Kinect for Xbox 360 as a gaming console in 2010, and the corresponding hardware version of the 

device  for Windows (called Kinect  v1)  was released in 2012. The sensor  includes a  vision (RGB) camera, an  SL depth camera, 

multiple  microphones,  and  a  motorized  tilt.  The  SL  camera  employs  an  infrared  laser  projector  combined  with  a  monochrome 

Complementary  Metal  Oxide  Semiconductor  (CMOS)   sensor  to  provide  depth  (i.e.,  range)  information.  Kinect  v1  outputs 

synchronized 640×480 RGB and 320×240 depth images at a frame rate of 30 Hz. The combination of RGB and depth streams is often 

referred to as RGB-D data. Regarding the motion capture capabilities, Kinect v1 can provide 3D coordinates for 15 or 20 joints of a 

moving subject by using either OpenNI SDK (Software Development Kit) or Microsoft SDK, respectively. Due to its versatility and 

low price, the device was widely utilized for measuring human movements across different applications [20]–[22], [49], [52], [55], 

[56], [71]–[75].  

Accordingly, many studies were conducted to verify the validity of Kinect v1  for posture measurement or motion capturing in 

biomedical applications [76]–[83]. The studies by Clark et al. [79], [83] were among the earliest works that evaluated the suitability 

of Kinect v1 for biomedical applications by comparing the measurements by Kinect v1 and a Vicon optical tracking system (used 

as  a  gold  standard).  Clark  et  al.  [79]  reported  that  the  agreement  between  the  Kinect  v1  and  Vicon  ranged  from  excellent  for 

parameters like gait speed, step length, and stride length (Pearson correlation > 0.9) to moderate for other parameters like the stride 

time (Pearson correlation = 0.69). In a similar study [83], the authors assessed the reliability of Kinect v1 for postural control in 

lateral reach, forward reach, and single-leg standing balance. The findings indicated that Kinect v1 exhibited excellent correlation 

for almost all measurements (Pearson correlation = 0.96; range, 0.84–0.99) and can validly be used for evaluating postural changes 

in  a  clinical  setting.  Galna  et  al.  [80]  studied  the  accuracy  of  Kinect  v1  for  measuring  movement  symptoms  in  people  with 

Parkinson’s  disease  and  found  that  Kinect  v1  measured  the  timing  of  the  movements  very  accurately  (Intraclass  Correlation 

Coefficient  (ICC)  >  0.9  and  Pearson  correlation  >  0.9),  whereas  for  measuring  the  spatial  characteristic  of  movements  the 

4 

 
 
 
agreement ranged from excellent (for gross movements such as sit-to-stand ICC = 0.989) to poor (for fine movements such as 

hand-clasping ICC = 0.012). Still, the authors reported high correlation between the spatial measurements for Kinect and Vicon 

for all movements (Pearson correlation > 0.8). Similarly, Tao et al. [78] estimated the root-mean-squared error (RMSE) between 

the measurements by Kinect v1 and an Optotrak optical tracker for hand reaching positions, trunk positions, and elbow angular 

orientations, and reported RMSE errors of 6.3 cm (2.5 inch), 9.8 cm (3.9 inch), and 26.7 degrees, respectively. Mishra et al. [81] 

designed a remote home-based rehabilitation program that uses Kinect v1 for motion tracking and streams the recorded videos in 

real-time  to a clinic. The  viability of the system  was compared to  a Vicon optical tracker. The authors  introduced  metrics for 

quantifying the angular trunk sway and reported a maximal measurement error of 17.2 degrees in anteroposterior (AP) direction 

and 7.3 degrees in mediolateral (ML) direction. In summary, almost all studies for validation of Kinect v1 reported an excellent 

temporal accuracy, whereas the spatial accuracy  was high  for larger body movements, and moderate to poor for more delicate 

movements. Other important considerations in practical applications of the sensor include: the accuracy of the measurements is 

dependent on the distance from the sensor and the  selected view point  for a particular  movement,  and  limb occlusions  during 

movements may impact the measurement accuracy of other body parts.     

Kinect v2—Microsoft Kinect for Xbox One, an upgraded version of Kinect v1, was released in 2013. The corresponding hardware for 

Windows with the supporting SDK was released in 2014 (known as Kinect v2). Kinect v2 has a similar construction to Kinect v1, 

except that the SL depth sensor is replaced with a ToF depth sensor. The ToF sensor obtains the depth information by measuring the 

time it takes for an infrared laser pulse to travel back and forth between the camera and the surrounding objects. Kinect v2 offers an 

improved resolution of 1920×1080 RGB and 512×424 depth images at a frame rate of 30 Hz. The open-source Microsoft SDK2 

allows extracting the skeletal coordinates of 25 joints. Kinect v2 has other advantages over its predecessor; for instance, it can detect 

objects up to 3 feet from the sensor, compared to 6 feet for Kinect v1.  

Similarly, the precision of Kinect v2 for motion tracking has been assessed and reported in numerous publications [77], [81], [84]–

[89]. For example, Napoli et al. [89] reported that the sensor can provide accurate joint displacements for a range of clinical tasks 

with  RMSE  of  3.4  cm  (1.3  inch),  while  lower  accuracy  was  noted  when  capturing  joint  angles  with  RMSE  of  24.6  degrees. 

Comparable validation results were reported by Capecci et al.  [34], where for a set of three rehabilitation exercises  an average 

positional  RMSE  of  3.3  cm  (1.3  inch)  and  an  average  orientational  RMSE  of  12.7  degrees  were  recorded.  In  [88],  Otte  et  al. 

assessed the validity of Kinect v2 for clinical motion analysis by comparing its accuracy against a Vicon system, and concluded 

that the accuracy of measurements is moderate to excellent, where for most clinical parameters there was excellent consistency 

between the two systems. Dolatabadi et al. [85] employed two-group mean differences, i.e.,  Bland-Altman Limit of Agreement 

(LoA), and Intraclass Correlation Coefficient (ICC) between Kinect v2 and a GAITRite mat to verify the capacity of Kinect v2 for 

recording gait. In three walking conditions, for all gait parameters the maximum values of the group mean differences were 8%, 

the 95% LoA was less than or equal to 11%, and the ICC ranged between 0.9 and 0.98. The comparison results implied that Kinect 

v2 is capable of measuring spatio-temporal gait parameters for objective evaluation. In [81], trunk sway measures calculated from 

3D joint positions  were employed for validation. The performance across all trials had the maximum trunk sway error  of 12.8 

degrees in AP direction and 5.6 degrees in ML direction. Conclusively, Kinect v2 provided greater accuracy than Kinect v1, and 

most studies reported low measurement errors and adequate motion tracking abilities for a number of biomedical applications.  

Azure Kinect—The latest generation of Kinect, called Azure Kinect, was released in July 2019. As the name suggests, the sensor’s 

functionality is based on the integration with Microsoft’s cloud computing service Azure. The target audience for this generation 

of the sensor is developers and businesses interested in artificial intelligence applications. The sensor offers an RGB camera with 

3,840×2,160 pixels, a ToF depth camera with 1,024×1,024 pixels, an inertial measurement unit, and seven microphones. The depth 

5 

 
 
camera supports five modes of operation, and the RGB camera offers six modes of operation, each with different resolutions and 

frame rates. Compared to Kinect v2, Azure Kinect is much smaller and lighter, the resolution of RGB/depth cameras is doubled 

and they support different modes and color formats, and it also provides inertial data apart from the RGB and depth data. More 

importantly, Azure Kinect is no longer a game peripheral (e.g., for Xbox One) but a smart device used primarily by developers 

with Azure cloud.  

Other Vision/Depth Sensors—Asus Xtion Pro and Asus Xtion Pro Live both provide an SL camera for capturing depth information. 

Asus  Xtion  Pro  Live  houses  also  a  color  RGB  camera  and  two  microphones.  The  sensors  are  supported by  OpenNI  SDK  for 

tracking 15 body joints. The accuracy of depth data recorded by Asus Xtion is studied in [90], where under seven lighting conditions, 

the reported median error was less than 1.3 cm (0.5 inch). Other commercial vision/depth sensors include Intel’s RealSense [91] 

and Structure Sensor [92]. However, they are rarely used exclusively for motion tracking, and almost no efforts have been made 

to verify their accuracy and reliability for this purpose.   

An overview of the main characteristics of the above vision/depth sensors used for motion capturing is provided in Table 1. 

2.2. Optical Motion Tracking Systems 

Vicon, OptiTrack [93], Optotrak, and PhaseSpace motion capture [94] are the most common optical motion capture systems. They 

employ a set of markers that are attached to predefined locations on the human body, while multiple cameras positioned at different 

viewing angles track the markers’ locations during the movements. A dedicated software program utilizes trigonometrical relations 

among the markers in captured images and the locations of the cameras to calculate the positions and orientations of  the body 

joints. Many studies confirmed the excellent positioning performance of these motion capture systems in both static and dynamic 

tests [95]–[97]. As a result, optical tracking systems are regarded as the gold standard for verifying tracking reliability of other 

motion sensors [76], [98]–[100]. On the other hand, their high cost limits their broad applicability for the assessment of patient 

rehabilitation progress. 

2.3. Inertial Sensors 

A general form of an inertial sensor includes an accelerometer and a gyroscope [101]. Integrated devices containing inertial sensors 

are collectively called inertial measurement units (IMUs). It should be noted that inertial sensors described in this review only refer 

to wearable sensors, and inertial data means measurements provided by accelerometers or inertial sensors. The accelerometer is a 

compact device designed to measure non-gravitational acceleration, which is the rate of change of the sensor’s positional velocity. 

The sensor’s position is typically obtained by first subtracting the earth’s gravity from accelerometer measurements and afterward 

applying double integration. A gyroscope records the angular velocity of the sensor, i.e., the rate of change of the sensor's orientation 

[102]. The measured positions and orientations of wearable inertial sensors are often used for analysis of human postures and motions. 

For the purpose of this study, joint positions and angles transformed from inertial sensors are still considered skeleton data, despite its 

sparse representation of the human motions. 

Analogously  to  the  previously  described  sensors,  prior  research  focused  on  verifying  the  validity  of  inertial  sensors  for 

rehabilitation analysis and evaluation. Chung and Ng [103] concluded that accelerometers measure motor reaction times with a 

large ICC value of 0.74 (p < 0.001). Yet, the reaction time measured by the accelerometer is on average 8 milliseconds slower than 

that detected by the Vicon system.  The work by Lugade et al. [104] reports that median sensitivities of an algorithm benchmarked 

on  data  provided  by  tri-axial  accelerometers  for  activity  identification  are  over  85%  accurate  in  comparison  to  human  raters. 

Fortune et al. [105] studied the ability of accelerometer data to be used for step counting. During walking or jogging tests, activity 

monitoring achieved a high median agreement of 92% with an interquartile range of 8%, which outperformed FitBits and a Nike 

Fuelband.  The  publications  [106]  and  [107]  both  focus  on  the  validity  and  reliability  of  inertial  sensors  for  recording  trunk 

6 

 
 
movements. The former used the Pearson correlation and RMSE as metrics to measure the agreement between inertial and Optotrak 

measurements.  The  median  values  of  the  Pearson  correlation  exceeded  0.95,  and  the  RMSE  had  a  median  value  of  1  and  1.2 

degrees in the AP and ML directions, respectively. In the latter, the coefficient of determination (𝑟2) and RMSE were adopted to 

validate the IMU system. In the primary movement direction, the RMSE ranged between 1.1 and 6.8 degrees, and the coefficients 

of determination were no less than 0.85. Washabaugh et al. [108] used Lin’s concordance correlation coefficient (i.e., LCC [101]) 

and the Pearson correlation coefficient to evaluate IMU’s validity for capturing spatiotemporal gait metrics. The authors concluded 

that inertial sensors provided accurate measurements for the respective study, however the degree of accuracy and reliability relied 

on several factors, such as the sensor position and movement speed. The validation studies in the literature generally agree that 

inertial sensors provide sufficiently accurate and fast movement data for rehabilitation analysis and evaluation. 

2.4. Other Sensors 

Other devices and sensors have been used in several works for rehabilitation evaluation. For instance, although standard vision 

cameras  do  not  provide  sufficient  accuracy  for  motion  capture,  they  can  be  employed  to  acquire  facial  expressions  for  pain 

detection  [109],  [110].  In  [44],  sequences  of  pressure  maps  were  generated  by  a  pressure-sensitive  bedsheet  to  identify  bed 

rehabilitation exercise. Teague et al. [111] performed joint health evaluation by sensing acoustical emissions from the knee using 

three types of microphones. And, in [112], thermal infrared images of patients were recorded by a medical infrared camera system 

and a quantitative evaluation of pain-related thermal dysfunction was obtained by analyzing the distribution of the skin temperature. 

(a)  

(c)  

(b) 

(d) 

Figure 1. (a) Kinect v1; (b) Asus Xtion PRO LIVE; (c) Kinect v2; (d) Azure Kinect DK. 

Table 1. RGB-D sensor capability comparison. 

  Feature 

Kinect v1 

Asus Xtion PRO LIVE 

Kinect v2 

Azure Kinect DK 

RGB camera 

1,280×960 px at 12 Hz 
640×480 px at 30 Hz 

1,280×1,024 px at 30 Hz 

1,920×1,080 px at 30 Hz  3,840×2,160 px at 30 Hz 

Depth camera 

320×240 px at 30 Hz 

640×480 px at 30 Hz 
320×240 px at 60 Hz 

Motion sensor 

SL 

None 

SL 

None 

512×424 px at 30 Hz 

ToF 

3-axis accelerometer 

Measuring range 

0.85~4m 

0.8m~3.5m 

0.5~4.5m 

640×576 px at 30 Hz 
512×512 px at 30 Hz 
1,024×1,024 px at 15 Hz 
ToF 
3-axis  accelerometer  +  3-
axis gyroscope 
0.5~3.86m; 0.5~5.46m;  
0.25~2.88m; 0.25~2.21m 
75×65 degrees;  
120×120 degrees 

Field of view 

Skeleton joints 

57×43 degrees 

58×45 degrees 

70×60 degrees 

15 or 20 joints  

15 joints 

25 joints 

32 joints 

7 

 
 
 
 
 
 
      
 
 
 
 
 
 
 
 
      
 
 
 
 
 
 
 
 
3.  Datasets 

A large number of publicly available datasets related to general human movements collected with healthy subjects are available 

for  analysis  [113]–[116].  The  datasets  are  extensively  used  for  benchmarking  algorithms  for  action  recognition,  gesture 

recognition, or pose estimation. On the other hand, collecting large data sets of rehabilitation exercise data from patients suffering 

from an impairment or injury is more challenging due to privacy and safety concerns. Consequently, only a few public datasets for 

rehabilitation  evaluation  currently  exist,  and  their  main  attributes  are  summarized  in  Table  2.  The  table  lists  the  referenced 

publications, used sensors, data modality, number of subjects performing the exercises, number of exercises, and data availability.   

Taylor et al.—For the dataset collected by Taylor et al. [45], experimental data is obtained from 9 subjects performing 3 exercises: 

standing hamstring curl, reverse hip abduction, and lying straight leg raise. Five inertial sensors (accelerometers) are positioned on 

the thigh and shin of both legs and the waist to measure the 3-axis acceleration of the corresponding body part. Each exercise is 

repeated by each subject 10 times, both on the left and right sides, respectively. The acquired data is manually segmented into the 

individual repetitions of each exercise. 

PAMAP2—Physical Activity Monitoring Dataset (PAMAP2) [117], [118] was designed for activity recognition and estimation of 

exercise intensity (which involves light, moderate, or vigorous intensity of effort). It consists of 3,850,505 instances, recorded by 

3 inertial sensors and a heart rate (HR) monitor. The sampling frequency of the inertial sensors and HR monitor are 100 Hz and 9 

Hz, respectively. The data collection involved 9 subjects (8 males and 1 female) performing 18 different physical activities. The 

duration of each activity is between 1 and 3 minutes. 

SPHERE-Staircase2014—This  dataset  [119]  consists  of  48  video  sequences  captured  by  an  Asus  Xtion  camera,  whereas  the 

associated skeleton data is obtained with OpenNI SDK. The activities include walking upstairs in normal or abnormal gaits. There 

is a total of 12 persons completing this process. The abnormal gaits include freezing of gait and using a leading leg (left or right 

leg). A qualified clinician manually labeled each frame as normal or abnormal performance. The data is provided in the form of 

skeleton time-series.  

HPTE dataset—Home-based Physical Therapy Exercises (HPTE) dataset by Ar and Akgul [71] contains 240 color/ depth video 

streams captured with Kinect v1. Although the RGB and depth videos collected by the Kinect sensor are 640×480 pixels, they are 

stored as 256 gray-level images in 320×240 pixels size. Five volunteers were tasked to perform eight exercises, where each subject 

performed an exercise 6 times consecutively, resulting in 30 repetitions per exercise. The duration of each repetition is between 15 

and 30 seconds. 

dataELEMENT— Created by Cuellar et al. [72] and collected with  Kinect v1, dataELEMENT includes movements by 10 healthy 

subjects performing 5 exercises, where each exercise was repeated 10 times. The recorded data comprises absolute angles of joints 

or bones with respect to an underlying 3D base coordinate system, and relative angles between the bones that share a joint.  

Kinect  3D  Active—The dataset  was  created  by  Leightley  et  al.  [120], [121]  and  it  contains  over  225,000  frames  of  depth  and 

skeleton data recorded with Kinect v2 in a lab-based indoor environment. The dataset includes the subjects’ personal information 

and the  tracking states of all  joints (i.e., ‘tracked’, ‘not tracked’, and ‘inferred’ states).  Fifty-four participants (32  men and 22 

women) participated in the data collection, and they were required to take standardized tests, including Short Physical Performance 

Battery (SPPB) [122], Timed-Up-and-Go (TUG) [123], vertical jump, and balance tests.  

UI-PRMD—University of Idaho – Physical Rehabilitation Movement Dataset (UI-PRMD) created by Vakanski et al. [124] consists 

of 10 exercises that are widely applied in physical rehabilitation programs: deep squat, hurdle step, inline lunge, side lunge, sit to 

stand,  standing  active  straight  leg  raise,  standing  shoulder  abduction,  standing  shoulder  extension,  standing  shoulder  internal-

external rotation, and standing shoulder scaption. Ten healthy subjects performed each exercise 10 times in a correct and incorrect 

8 

 
 
manner. Two sensors were employed for collecting the data: a Vicon optical tracking system and Kinect v2. The Vicon system 

provided positions and orientation angles for 39 joints, whereas Kinect measured the positions and orientation angles for 22 joints 

of the exercise movements.  

KIMORE—KInect-based MOvement Rehabilitation dataset (or KIMORE) by Capecci et al. [125] was collected with Kinect v2, 

and involves 78 subjects performing 5 exercises that are clinically recognized for low back pain physiotherapy. The exercises are: 

lifting the arms, lateral tilt of the trunk with arms in extension, trunk rotation, pelvis rotations on the transverse plane, and squatting. 

The  enrolled  population  consisted  of  44  healthy  subjects  and  34  patients  with  chronic  motor  disabilities.  For  each  exercise, 

clinicians  defined  rules  for  extracting  features  from  the  raw  data,  with  the  corresponding  features  provided  in  the  dataset. 

Additionally, the clinical scores derived from a clinical questionnaire [126] for evaluating the subjects’ movement performance 

are included in the dataset.  

Table 2. Datasets description. 

Dataset 

Ref. 

Year 

Sensor 

Modality 

Subjects  Exercises  Available 

Taylor et al. 

[45] 

2010 

PAMAP2 

[118] 

2012 

5 inertial sensors 
3  inertial  sensors,  
heart rate monitor 

Inertial data 

Inertial data; heart rate signal 

SPHERE-
Staircase2014 
HPTE 
dataELEMENT 
Kinect 3D Active 
UI-PRMD 
KIMORE 

[119] 

2014  Asus Xtion 

Depth video; skeleton data 

[71] 
[72] 
[121] 
[124] 
[125] 

2014  Kinect v1 
2014  Kinect v1 
2015  Kinect v2 
2018  Kinect v2 + Vicon  Skeleton data 
2019  Kinect v2 

Gray-level and depth video 
Skeleton data 
Depth and skeleton data 

RGB and depth videos; skeleton data 

9 

9 

12 

5 
10 
54 
10 
78 

3 

18 

18 

8 
5 
13 
10 
5 

No 

Yes 

Yes 

Yes 
Yes 
Yes 
Yes 
Yes 

4.  Feature Engineering 

Feature engineering is the process of creating  features from raw data  in order  to improve  the performance of  a computational 

method. In rehabilitation evaluation, the recorded data from motion capture sensors are often in the form of high-dimensional time-

series sequences of the  joints’ locations and/or orientations. Such data  is exceedingly redundant and correlated (e.g., the  wrist 

displacements are highly correlated to the elbow displacements), and they are rarely applied directly for modeling and analysis of 

rehabilitation movements. Accordingly, feature engineering via selection of important joints or limb distances to extract lower-

dimensional representations from the raw data is often applied as a data processing step in rehabilitation evaluation.     

In  general,  feature  engineering  involves  feature  extraction  and  feature  selection.  Feature  extraction  refers  to  generating  new 

features from raw input data by a functional mapping. The mapping can be determined manually by experts, or it can be learned 

from existing data representations. Feature selection entails selecting the most important features among existing and/or extracted 

features. The set of newly selected features is a subset of the original features. Efficient feature engineering produces an improved 

representation of the input data for the underlying task.  

In many related works, feature engineering is performed manually based on authors’ understanding of human movements [33]–

[37], [54], [97], [98], [126], [127]. For example, in [34], underarm angles and Euclidean distance between the elbows were used 

to describe the lifting of the arms. Similar, for squatting evaluation, knee angles and Euclidean distance between the ankles were 

extracted as clinical features. Jung et al. [37] proposed a set of distinctive features obtained from experimental data, consisting of 

mean speed, reaction time, duration, peak velocity, maximum velocity, distance error, direction error, and path length ratio. For 

9 

 
 
 
 
assessing upper body movement after stroke, the range of motion, movement speed, symmetry ratio among body sides, and vertical 

distance were adopted as movement performance indicators in [35]. Yu and Xiong [128] selected eight bone vectors as important 

features to an algorithm for producing quality scores in support of home-based rehabilitation.  

Although the approaches based on manual feature selection benefit from the authors’ intuitive understanding of the most important 

attributes for particular motions, creating features manually requires domain-knowledge and is time-consuming.  

Automated feature engineering to some degree can obviate the need for manual feature engineering. PCA is one of the most popular 

approaches for this task. It uses an orthogonal transformation to project measured correlated variables into linearly uncorrelated 

variables. Jun et al. [47] applied PCA on raw motion data consisting of joint positions for dimensionality reduction. However, PCA 

is not suited to identify the nonlinear structure of data [129], and therefore feature engineering based on nonlinear mapping has 

been studied extensively.  Huang et al. [44] employed two types of nonlinear manifold learning—Local Linear Embedding and 

Isomap— for reducing the dimensionality of images. Another manifold-based approach using diffusion maps was employed for 

dimensionality reduction of skeletal data by Paiement et al. [40]. This method can find meaningful geometric descriptions of the 

data and has robustness to noise (Coifman and Lafon [38]). The nonlinear transformation of inputs by autoencoder neural networks 

has been used for dimensionality reduction of skeletal data [39]. Similarly, Crabbe et al. [43] proposed a CNN-based algorithm to 

extract a low-dimensional pose representation from depth images. 

In many prior works, feature engineering has been realized in two or three consecutive steps. Researchers typically first define 

new features from the raw data and then apply automated feature engineering to simplify these features. For instance, several time-

domain and frequency-domain features were extracted from inertial data in [130], and afterward PCA was applied to reduce the 

overall number of features for training. Houmanfar et al. [53] first derived statistical features from motion sequences, and after 

that, Least Absolute Shrinkage and Selection Operator (LASSO) was utilized to select five most relevant features. In [41], Tao et 

al. first constructed four possible feature descriptors by the geometry of the human body, and afterward, extracted the final features 

for each descriptor by using the method described in [40]. 

Manual feature selection is very common in movement data analysis and evaluation, since human understanding of the importance 

of specific features of the rehabilitation exercises provides excellent leverage toward the design and initialization of computational 

methods. Nevertheless, human movements are very complex, resulting in high-dimensional data with intricate temporal and spatial 

dependencies between the joint positions and orientations, and involve numerous constraints between the joints which are difficult 

to  accurately  encode  or  define  manually.  We  hold  that  the  approaches  for  automated  feature  engineering  offer  more  powerful 

means for learning the underlying correlations and constraints in movement data than the approaches for manual feature selection. 

In this respect, deep learning models are the most compelling approaches for feature engineering at present, because of the ability 

to automatically learn spatio-temporal features at multiple levels of abstraction in high-dimensional data.          

The reader is referred to Table 3 for a complete list of publications based on the sensors, data representation modalities, selected 

features, and pertinent tasks.   

5.  Evaluation Methods 

Successful motion quality evaluation in rehabilitation programs depends on efficient quantification of the level of performance of 

rehabilitation  exercises  from  measured  motion  data.  The  approaches  for  evaluating  rehabilitation  exercises  can  generally  be 

categorized into: discrete movement score approaches, rule-based approaches, and template-based approaches. A summary of the 

approaches, advantages/disadvantages, and referenced works for each category is provided in Table 4. 

10 

 
 
 
5.1. Discrete Movement Score Approaches 

This category of approaches classifies individual exercise repetitions into several discrete classes. Most often, the discrete classes 

are:  correct  and  incorrect  movements.  Thus,  the  outputs  are  typically  binary  class  values  of  0  or  1  (i.e.,  incorrect  vs  correct 

repetition). Adaboost classifier [45], 𝑘-nearest neighbors [48], Bayesian classifier [71], and an ensemble of multi-layer perceptron 

neural networks [37] have been used to distinguish between the two classes. For instance, in [47] k-nearest neighbors classifier 

was  applied  for  exercise  classification  after  filtering  the  data  noise  and  applying  dimensionality  reduction  through  PCA.  The 

approach  achieved  95.6%  classification  accuracy.  Similarly,  machine  learning  classification  was  applied  in  prior  research  for 

movement classification into score categories for standards clinical tests [28]; e.g., Support Vector Machines (SVM) [131], random 

forest [132], and artificial neural networks [133] were used for automated FMA, naïve Bayes classifier [134] was implemented for 

WMFT, and random forest [135] was used for Functional Ability Scale (FAS) evaluation. Furthermore, Um et al. [46] utilized an 

ensemble of convolutional neural networks to detect Parkinson’s disease states in data collected with a wrist-worn wearable sensor, 

where the states were defined as: OFF state with Parkinson’s syndrome symptoms, DYS state with dyskinetic symptoms, and ON 

state with no salient Parkinson’s syndrome or dyskinetic symptoms observed. The studies employing discrete movement scores 

have reported high accuracy in distinguishing correct from incorrect movement sequences. In spite of that, a shared shortcoming 

of the approaches is the lack of ability to  monitor continuous changes in movement quality, or quantify the progress of patient 

performance over the duration of the rehabilitation program. Subsequently, discrete movement score category is less relevant to 

the development of systems for quantifying the rehabilitation performance.    

5.2. Rule-Based Approaches 

The class of  rule-based  approaches utilizes  a set of  rules for  a considered  rehabilitation exercise that  is defined in advance by 

clinicians  or  human  movement  experts.  The  rules  are  used  as  a  gold  standard  for  assessing  the  level  of  correctness  of  the 

movements. Whereas a smaller number of rules, such as relative angles or distances, may be sufficient for representation of simpler 

movements, a more comprehensive set of rules is needed to describe more complicated exercises. For instance, the quality of sit-

to-stand and squat exercises was measured by the knees and ankles angles in [50]. Similarly, in [49], three types of kinematic rules 

were defined to model rehabilitation exercises: rules for dynamic movement, rules for static postures, and rules for movement 

invariance.  Afterward,  fuzzy  logic  was  applied  to  generate  a  single  final  score  that  represents  the  quality  of  the  rehabilitation 

exercise.  Exploring  rule-based  approaches  for  rehabilitation  exercises  is  a  valuable  option  for  simpler  exercises,  however,  it 

becomes  increasingly  more  difficult  to  extract  reliable  features  and  obtain  an  objective  evaluation  for  more  complex  types  of 

rehabilitation exercises. In addition,  these approaches  lack  flexibility  and  capacity  for  generalization  to new  exercises, since a 

different set of rules is required to be selected for each individual exercise. 

5.3. Template-Based Approaches 

In  template-based  approaches,  patients’  exercise  performance  is  evaluated  based  on  the  difference  between  training  motion 

sequences executed by the patients and template motion sequences. For example, the training sequences may be captured during a 

patient’s practice, and the template sequences can be reference movements performed either by healthy subjects, clinicians, or by 

patients  under  a  clinician’s  supervision.  The  metrics  used  to  measure  motion  similarity  in  template-based  approaches  can  be 

classified into two categories: distance functions, and probability density functions.  

a) Distance Functions 

Several  distance  function-based  approaches  have  been  used  for  movement  evaluation.  In  [52],  Euclidean  distance  between 

reference  template  positions  and  velocities  and  the  user’s  positions  and  velocities  were  employed  for  calculating  the  motion 

similarity.  Such  an  approach  provided  immediate  feedback  to  the  users  performing  the  exercises  via  the  position  and  velocity 

11 

 
 
errors. Houmanfar et al. [53] measured the level of correctness of rehabilitation exercises performed by patients, by computing a 

Mahalanobis distance between the patient-performed repetitions and the mean value of a set of repetitions completed by a group 

of  healthy  subjects  (used  as  the  ground  truth).  Furthermore,  the  authors  used  the  derived  distance  measure  for  the  individual 

repetitions  to  develop  additional  metrics  for  quantifying  the  quality  of  a  set  of  repetitions,  a  set  of  exercises,  and  for  tracking 

patients’ progress over a period of time consisting of several sessions. The progress of a group of 18 patients was monitored during 

a  hospital  stay  ranging  from  4  to  12  days,  and  it  was  found  that  the  calculated  progress  scores  correlated  well  with  the 

physiotherapist’s evaluation of the patients’ performance.        

To formulate  distance  functions, let’s  assume  the notation 𝐗 = {𝒙(1), 𝒙(2), ⋯ , 𝒙(𝐿)} and 𝐘 = {𝒚(1), 𝒚(2), ⋯ , 𝒚(𝐿)} for two  motion 

sequences,  where 𝒙(𝑡)and 𝒚(𝑡) are  the  joint  measurement  vectors  at  time 𝑡. The  measurements  at  a  particular  time  moment  are 

multi-dimensional vectors denoted 𝒙(𝑡) = (𝑥1

data.  Euclidean  distance  between 

(𝑡), 𝑥2
two 

(𝑡)) and 𝒚(𝑡) = (𝑦1

(𝑡), ⋯ , 𝑥𝐷
sequences  𝐗  and  𝐘  is  defined  by  𝑑𝐸(𝑿, 𝒀) = ∑ ‖𝒙(𝑡) − 𝒚(𝑡)‖

(𝑡)), where 𝐷 is the dimensionality of the 

(𝑡), ⋯ , 𝑦𝐷

(𝑡), 𝑦2

𝐿
𝑡=1

=

∑ √∑ (𝑥𝑑

𝐷
𝑑=1

𝐿
𝑡=1

(𝑡) − 𝑦𝑑

2
(𝑡))

.  Similar,  Mahalanobis  distance  between 

two 

sequences  𝑿  and  𝒀  is  𝑑𝑀(𝑿, 𝒀) =

∑ √(𝒙(𝑡) − 𝒚(𝑡))𝑽−1(𝒙(𝑡) − 𝒚(𝑡))𝑇

𝐿
𝑡=1

 where 𝑽 is the covariance matrix of the data. In fact, the Euclidean distance is a special case 

of the Mahalanobis distance when the covariance matrix is an identity matrix.  

The main limitation of the Euclidean distance or its variants is the requirement for the compared motion sequences to have the 

same length. This drawback is overcome with the dynamic time warping (DTW) distance [136]. The DTW distance is the most 

commonly adopted for measuring motion dissimilarity between training sequences and reference template sequences [54]–[57], 

[128],  [137]–[141].  For  two  univariate  time  series  𝒙 = (𝑥1, 𝑥2, ⋯ , 𝑥𝑚)  and  𝒚 = (𝑦1, 𝑦2, ⋯ , 𝑦𝑛) ,  a  time  warping  path  is  a 

sequence 𝑊 = (𝑤1, 𝑤2, ⋯ , 𝑤𝐾), 𝑚𝑎𝑥(𝑚, 𝑛) ≤ 𝐾 < 𝑚 + 𝑛 − 1 where the element 𝑤𝑘 = (𝑖, 𝑗) indicates the matching relationship 

between 𝑥𝑖 and 𝑦𝑗.  The  DTW  distance  between  two  sequences 𝑿 and 𝒀   is 𝑑𝐷𝑇𝑊(𝒙, 𝒚) = 𝑚𝑎𝑥

𝑊

{∑

𝐾
𝑘=1

𝛿𝑘(𝑖, 𝑗)

}, where 𝛿𝑘(𝑖, 𝑗) =

2
|𝑥𝑖 − 𝑦𝑗| or (𝑥𝑖 − 𝑦𝑗)

.  

The DTW distance has been applied to a broad range of features extracted in movement data. For example, Saraee et al. [137] used 

the DTW distance and speed ratio of key body joints for an exercise to directly evaluate the quality of performance with respect to 

reference template sequences. In [138] and [123], movement quality scores were derived by scaling the DTW distance values in 

the [0, 1] range. In the former, the DTW distance errors were scaled by the lower bound and upper bound, based on a scoring 

function  introduced  in  [142]. The  latter  used  a  sigmoid  function  to  map  the  DTW  distance  error  into  the  required  range.  The 

publication [55] used a Euclidean norm of the DTW difference for evaluating exercise movements. In [57], the DTW distance was 

calculated for a set of selected features, and afterward, an adaptive neuro-fuzzy algorithm was used for calculating  the overall 

quality of the practice sequences. Likewise, the study by Yurtman and Barshan  [139] introduced a multi-template multi-match 

DTW algorithm to measure the similarity between training sequences and previously recorded template sequences.   

Other distance functions have also been reported in the literature. For instance, a distance function similar to the Hausdorff distance 

was proposed to measure the similarity between two exercise sequences [44]. Concretely, the distance between two sequences 𝐗 

and 𝐘 was defined as 𝑑(𝐗, 𝐘) = 𝑠(𝐗, 𝐘) + 𝑠(𝐘, 𝐗), where 𝑠(𝐗, 𝐘) =

1

𝐿

𝐿
𝑖=1

∑ min
1≤𝑗≤𝐿

‖𝒙(𝑖) − 𝒚(𝑖)‖

. Similarly, the coefficient of cross-

correlation  [143]  between  two  skeleton  sequences  was  used  to  measure  motion  similarity;  for  two  motion  sequences 𝐗 and 𝐘 

consisting  of 𝑁 data  points,  the  coefficient  of  cross  correlation  [144]  is  defined  by: 𝑟𝑥𝑦(𝑘) =

𝑐𝑥𝑦(𝑘)

√𝑐𝑥𝑥(0)𝑐𝑦𝑦(0)

 ,  where 𝑐𝑥𝑥(0) =

12 

 
 
∑ (𝑥𝑖 − 𝑥̅)2

𝑁
𝑖=1

𝑁
, 𝑐𝑦𝑦(0) = ∑ (𝑦𝑖 − 𝑦̅)2,
𝑖=1

 𝑐𝑥𝑦(𝑘) = {

∑

𝑁
𝑁−𝑘+𝑖

(𝑥𝑖 − 𝑥̅)

(𝑦𝑖+𝑘 − 𝑦̅) +
(𝑦𝑖−𝑁+𝑘 − 𝑦̅), 𝑘 = 1,2, ⋯ , 𝑁

𝑁−𝑘
∑
𝑖=1
(𝑥𝑖 − 𝑥̅)
∑ (𝑥𝑖 − 𝑥̅)(𝑦𝑖 − 𝑦̅), 𝑘 = 0

𝑁
𝑖=1

, and 𝑘 is an index indicating a 

time shift of one sequence with respect to the other. Another distance metric used for this purpose is the deep metric [145], which, 

differently  from  the  Euclidean  distance  and  DTW,  can  capture  contextual  information  and  semantic  relationship  between  two 

motion sequences. The deep metric is defined as the Euclidean distance in an embedding space 𝑓, i.e., the distance between two 

motion sequences 𝐗 and 𝐘 is 𝑑L(𝑿, 𝒀) = ‖𝑓(𝑿) − 𝑓(𝒀)‖. 

The studies in the literature based on evaluation using distance functions reported a high correlation between patient’s performance 

and clinicians’ evaluation. For instance, DTW-based evaluation achieved high posture monitoring accuracy (91.9%) and exercise 

monitoring accuracy (95.2%) in comparison to clinicians’ annotated rehabilitation data [55], high correlation with the Brunnstrom 

stages of recovery (86% at p<0.001) [138], and high predictive score accuracy (80%) [57] in comparison to clinical evaluation. 

The DTW distance is especially suitable for rehabilitation evaluation, since it can compensate for the variability and time-shift in 

movement sequences. Conclusively, the main advantage of the distance function approaches is that they are not exercise-specific, 

and hence can be applied for evaluation of new types of exercises. However, the distance functions also have shortcomings, because 

they do not attempt to derive a model of the rehabilitation data, and the distances are calculated at the level of the individual time-

steps in the raw measurements. 

b) Probability Density Functions 

A body of research work utilized probability density functions to model and evaluate rehabilitation exercises, due to the abilities 

of probabilistic models for handling the stochastic variability of human movement. For instance, the log-likelihood of individual 

sequences drawn from a trained Gaussian mixture model has been used for movement quality evaluation [146], [146]. Discrete hidden 

Markov models (HMM) were implemented for analysis and segmentation of human motion data for rehabilitation exercises [53], 

[147]. In [148], an approach based on hidden semi-Markov models (HSMM) was applied to evaluate five different rehabilitation 

exercises and provide an  evaluation score [61],  [62]. The requirement for segmenting the exercises into individual repetitions by 

discrete HMM or HSMM was overcome in [40], [41] by employing a continuous HMM.  

GMM Log-likelihood: A Gaussian mixture model (GMM) with 𝐾 Gaussian densities has the form 𝑃(𝒙) = ∑

𝐾
𝑘=1

𝜋𝑘𝜑(𝒙|𝜇𝑘, ∑𝑘)

,  

where 𝜑(𝒙|𝜇𝑘, ∑𝑘) is  the 𝑘th  Gaussian  function  with  mean   𝜇𝑘 and  covariance  matrix ∑𝑘,  and 𝜋𝑘 denotes  mixing  coefficients 

satisfying the constraint ∑

𝐾
𝑘=1

𝜋𝑘 = 1

. GMM is trained by maximizing the log-likelihood of the template sequences (e.g., which 

can  be  collected  from  a  group  of  healthy  subjects),  which  for  a  sequence  𝐘  is  defined  by  𝐿(𝐘) = log(∏ 𝑃(𝒚(𝑡))

𝐿
𝑡=1

) = 

∑ log{∑

𝑇
𝑡=1

𝐾
𝑘=1

𝜋𝑘𝜑(𝒚(𝑡)|𝜇𝑘, ∑𝑘)

. The deviation between the motion sequences 𝐗 and 𝐘 is 𝑑G(𝐗, 𝐘) =
}

1

𝐿

|𝐿(𝐗) − 𝐿(𝐘)| . 

HMM Log-likelihood: Hidden Markov model (HMM) has 𝑀 possible states denoted 𝑆 = {𝑠1, 𝑠2, ⋯ , 𝑠𝑀} where the state at time 𝑡 

is  𝑞𝑡 ∈ 𝑆. The likelihood of a sequence 𝐘 is calculated as 𝑃(𝐘) = ∑

𝑞1,𝑞2,⋯,𝑞𝐿∈𝑆

𝜋𝑞1𝑃(𝒚(1)|𝑞1) ∏ 𝑃(𝒚(𝑡)|𝑞𝑡)𝑃(𝑞𝑡|𝑞𝑡−1)

𝐿
𝑡=2

, where 

𝜋𝑞1 is the initial state distribution, 𝑃(𝒚(𝑡)|𝑞𝑡) is the probability that the observation 𝒚(𝑡) is seen if in state 𝑞𝑡, and 𝑃(𝑞𝑡|𝑞𝑡−1) is the 
transition probability from state 𝑞𝑡−1 to state 𝑞𝑡. The deviation between the motion sequences 𝐗 and 𝐘 is calculated by 𝑑H(𝐗, 𝐘) =

1

𝐿

|log

𝑃(𝐗)
𝑃(𝐘)

|. 

Employing  a  probability  density  function  approach,  Capecci  et  al.  [62]  produced  one  of  the  most  complete  recent  works  on 

rehabilitation evaluation, where the authors asked two clinicians to score a set of movements, and used the scores as a gold reference 

standard for validating an HSMM-based approach. The research reported a high correlation between the HSMM-generated and the 

clinicians’  movement  scores  (Pearson  correlation  coefficient  of  0.62,  p  <  0.01).  Furthermore,  in  comparison  to  a  DTW-based 

13 

 
 
evaluation (Pearson correlation coefficient of 0.56, p < 0.01), the probabilistic HSMM model demonstrated better correlation with the 

clinicians’ scores. In this work, a Kinect v2 sensor was used for motion capturing, and the authors indicated lower tracking errors for 

the upper body in comparison to the lower body.  Similar results have been reported by other researchers, noting high correlation 

between machine learning-based and physiotherapist-based evaluation [53].  

Utilizing probabilistic approaches for exercise evaluation is advantageous in comparison to all other evaluation methods, because they 

employ statistical probability distributions to handle the random variability of human movements. The ability to model the stochastic 

variations in performing the same exercise both by the same subject and across different subjects is essential for efficient movement 

modeling and  evaluation. One shortcoming of the probabilistic  models  is that the  movements are  represented at a single level of 

movement abstraction, and it is difficult to implement probabilistic modeling at multiple levels of movement abstraction.  

Table 3. Summary of sensors, modalities, features, and objectives per publication. G: gray-level image; D: depth image; S: skeleton 

data; I: inertial data; MC: motion classification; MA: motion assessment; MS: motion segmentation; PE: pose estimation; DA: 

Sensor 

Modality 

Feature engineering 

Objective 

data augmentation. 

Reference 

Ar and Akgul [20] 

Ar and Akgul [71] 

Kinect v1 

Kinect v1 

G + D 

G + D 

Kinect v1 
Benetazzo et al. [52] 
Kinect v1 
Cuellar et al. [72] 
Kinect v1 
Hagler et al. [73] 
Kinect v1 
Nomm and Buhhalko [74] 
Kinect v1 
Zhao et al. [49] 
Kinect v1 
Antón et al. [22] 
Kinect v1 
Antón et al. [55] 
Kinect v1 
Su [56] 
Kinect v1 
Su et al. [57] 
Kinect v1 
Crabbe et al. [43] (data from [119]) 
Kinect v1 
Uttarwar and Mishra [75] 
Saraee et al. [137] 
Kinect v2 
Vakanski et al. [39] (data from [113])  Kinect v2 
Paiement et al. [40] (data from [182])  Kinect v2 
Kinect v2 
Parisi et al. [182] 
Kinect v2 
Capecci et al. [33] 
Kinect v2 
Capecci et al. [34] 
Kinect v2 
Capecci et al. [126] 
Kinect v2 
Capecci et al. [61] 
Kinect v2 
Capecci et al. [62] 
Kinect v2 
Tao et al. [41] (data from [119]) 
Kinect v2 
Osgouei et al. [54] 
Kinect v2  
Spasojević et al. [35] 
Kinect v2 
Yu and Xiong [128] 
Kinect v2 
Saraee et al. [156] 
Inertial sensor 
Taylor et al. [45] 
Inertial sensor 
Zhang et al. [48] 
Inertial sensor 
Lin and Kulić [148] 

S 
S 
S 
S 
S 
S 
S 
S 
S 
D 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
S 
 I 
 I 
S 

Histogramming  3D  Haar-like 
features 
Histogramming  3D  Haar-like 
features 
None 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
None 
Hand-crafted 
Hand-crafted 
Autoencoder network 
Diffusion maps 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Diffusion maps 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Hand-crafted 
Cross-correlation function 
None 

MC 

MC 

MA 
MA 
MA 
MC 
MA 
MA 
MC 
MA 
MA 
PE 
MC 
MA 
MA 
MC + MA 
MA 
MA 
MA 
MA 
MA 
MA 
MC + MA 
MC 
MA 
MA 
MA 
MC 
MC 
MS 

14 

 
 
 
Chen et al. [183] 
Zhang et al. [138] 
Houmanfar et al. [53] 
Msayib et al. [184] 
Um et al. [185] 
Um et al. [186] 
Um et al. [46] 

Burns et al. [187] 

Yurtman and Barshan [139] 
Karime et al. [127] 

Coskun et al. [189] 

Vamsikrishna et al. [190] 

Inertial sensor 
Inertial sensor 
Inertial sensor 
Inertial sensor 
Inertial sensor 
Inertial sensor 
Inertial sensor 

Inertial sensor 

Inertial sensor 
Inertial sensor 
Vicon,  ToF  + 
camera 
Leap 
controller 

motion 

I 
I 
I 
S 
I 
I 
I 

I  

I 
I  

S 

S 

Hand-crafted 
None 
Hand-crafted + LASSO 
Hand-crafted 
None 
None 
None 
HAR  statistical  and  heuristic 
features [188] 
None 
Hand-crafted 

None 

Hand-crafted 

MC 
MA 
MA 
MA 
DA 
MC 
MC 

MC 

MC + MA 
MA 

MC 

MC 

Table 4. Summary of approaches for evaluation of rehabilitation movements. 

Approach 

Advantage 

Disadvantage 

Reference 

Discrete movement 
score approaches 
Rule-based 
approaches 

Template-based 
approaches 

Efficient and achieve high 
accuracy  
Provide multiple performance 
scores; less computation 
complexity 
Avoid the process of making 
rules; Can reflect the level of 
motor ability 

Cannot track diverse degrees 
of functional abilities 
New rehabilitation exercises 
require different rules to be 
designed 
Only give a overall score for 
exercise performance   

[37], [45]–[48], [55], [71], [74], [106], 
[183], [185]–[187] 
[33]–[36], [49], [50], [61], [62], [73], 
[126], [127], [156], [182], [191]–[195] 

[39]–[41], [52], [53], [55], [57], [96], 
[97], [103], [128], [138], [140], [143], 
[145], [148], [156], [190], [196] 

6.  Movement Segmentation 

The  objective  of  exercise  segmentation  is  to  extract  individual  repetitions  from  a  continuous  motion  sequence  of  an  exercise. 

Movement  segmentation  is  an  important  step  for  evaluation  of  physical  rehabilitation  exercises,  because  most  of  the  existing 

evaluation approaches are based on quantifying the quality of individual repetitions of an exercise. Consequently, after a patient’s 

movements are recorded with a motion capture sensor (and the patient performed multiple repetitions of the exercise), it is first 

required to segment the motion data into the instances of the individual repetitions, and only afterward is the evaluation technique 

applied  to  produce  a  quality  score  for  the  individual  repetitions.  The  overall  quality  of  the  exercise  is  usually  calculated  by 

averaging over the performance scores of the individual repetitions [148].  

Although in many studies the motion sequences are segmented manually, such an approach is not conducive to the realization of 

fully  automated  evaluation  of  rehabilitation  exercises.  Existing  approaches  for  automated  motion  segmentation  are  broadly 

classified into two categories: (1) approaches that model the common characteristics shared by segment points, and (2) approaches 

that learn a segment pattern from a template library. In the first class, kinematic zero crossing (KZC) methods are frequently used 

to  perform  exercise  segmentation.  These  methods  determine  segments  based  on  zero  crossings  for  the  velocity  [61],  [62]  or 

acceleration  [149] of  joint  trajectories.  Distance  functions,  such  as  Euclidean  distance  [150],  Mahalanobis  distance  [151],  and 

DTW distance  [152] have  also  been  used  for this purpose,  where segments are extracted at  the  points  having the  value of the 

distance function greater than a pre-selected threshold. Lee et al. [153] introduced a deep learning-based approach for segmentation 

15 

 
 
 
 
of time-series, in  which an autoencoder network  extracted representative  features  from  input data, and the  peaks in a distance 

function  calculated  from  the  features  were  selected  as  breakpoints  for  segmentation  purpose.  These  methods  rely  on  domain-

specific knowledge of the underlying data to select discriminative features for segmentation purpose, and do not offer a mechanism 

to reject false positives. Thus, further post-processing is often necessary. For instance, in [148] the segment candidates were first 

selected by velocity zero crossing, and then the final breakpoints were identified from the segment candidates using an HMM. 

The second class of approaches employs machine learning methodology to discover latent patterns from template libraries. HMM 

is often selected for segmentation of movement data, where each segment is treated as a hidden state, and the Viterbi algorithm is 

used to recover the state sequence [154]. Using regression-based techniques, a piecewise linear function was applied to fit the 

template data, and segmentation was performed when the difference between the data and the regression line was greater than a 

given threshold [155]. Traditional classifier methods (such as SVM) were also used for movement segmentation. In [148], all data 

points of motion sequences were assumed to be either segment points or non-segment points, and a trained SVM model was utilized 

to classify the points and segment the motion data.  

7.  Scoring Functions 

Scoring functions are often used to convert the output values of movement evaluation algorithms to a meaningful performance 

score limited within a certain range [33], [37], [61], [62], [72], [126], [128], [138], [156], [157]. Concretely, it is important that the 

approaches for movement evaluation generate quality scores that are understandable and interpretable both by patients and medical 

professionals. For instance, quality scores that range from 0 to 100, or from 0 to 1, are easy to understand, record, and compare. 

On the other hand, the outputs of the quantitative algorithms described in Section 5 may be spread within a small or a large range 

of values, or they may even be negative numbers. Scoring functions are mathematical functions that map the outputs of the various 

approaches for movement  evaluation  to a convenient range of values that is  meaningful  to the end-users, and are therefore an 

important component of these systems. Scoring functions are also central to the validation and comparison of different approaches. 

Let 𝑿 = (𝑥1, 𝑥2, ⋯ , 𝑥𝑚) and 𝒀 = (𝑦1, 𝑦2, ⋯ , 𝑦𝑛) denote the template and candidate training sequence, respectively. In [156], the 

dissimilarity between two motion sequences was converted to a performance score using the following formula: S(𝑿, 𝒀) = 1 −

𝑑𝐷𝑇𝑊(𝑿,𝒀)
𝑚𝑎𝑥𝑖𝑚𝑢𝑚 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒

,  where  maximum distance is the  DTW distance between the reference  movement and the practice  movement 

captured when the user was not moving at all. Zhang et al. [138] proposed a more complex score function based on constructed 

lower and upper bounds of the DTW distance. Concretely, the score function is 𝑆(𝑿, 𝒀) = 1 −

𝑑𝑑𝑡𝑤(𝑿,𝒀)−𝑑𝑙𝑏(𝑿,𝒀)
𝑑𝑢𝑏(𝑿,𝒀)−𝑑𝑙𝑏(𝒙𝑿,𝒀)

, with 𝑑𝑙𝑏(𝑿, 𝒀) =

max {

|First(𝑿) − First(𝒀)|
|Last(𝑿) − Last(𝒀)|
|max(𝑿) − max(𝒀)|
|min(𝑿) − min(𝒀)|

  and 𝑑𝑢𝑏(𝑿, 𝒀) = max(𝑚, 𝑛) ∙ max {

. In [128], the performance score function 

|max(𝑿) − min(𝒀)|
|min(𝑿) − max(𝒀)|

was defined as 𝑆(𝑿, 𝒀) = 1 −

from raw motion data.  

𝑑𝐷𝑇𝑊(𝑿,𝒀)
90×8×𝑠

, where 𝑠 is the length of the optimal path and 8 represents the eight bone vectors extracted 

A  score  function  on  the  basis  of  membership  functions  was  introduced  in  [72].  Specifically,  the  score  function  is  given  by 

𝑆(𝑿, 𝒀) =

1

𝑇

∑ 𝑃𝑠𝑐𝑜𝑟𝑒(𝑡)

𝑇
𝑡=1

𝑁
,  with 𝑃𝑠𝑐𝑜𝑟𝑒(𝑡) = ∑ 𝑟𝑖𝑓𝑖(𝑥𝑖(𝑡) − 𝑦𝑖(𝑡))
𝑖=1

𝑛
, ∑ 𝑟𝑖
𝑖=1 = 1,  where 𝑁 is  the  number  of  features,   𝑇 is  the 

length of motion sequences 𝑿 and 𝒀 which have been aligned using DTW, and 𝑓𝑖 is a Gaussian-shape function. In [61] and [62], a 
score  function  was  derived  using  the  log-likelihood  of  a  trained  HSMM.  The  total  score  for  the  𝑖𝑡ℎ  subject  is  𝑠𝑐𝑜𝑟𝑒𝑖 =

16 

 
 
 
(𝐶𝐸𝑚𝑎𝑥 − 𝐶𝐸𝑚𝑖𝑛) ×

log 𝐿𝑖−log 𝐿𝑚𝑖𝑛
log 𝐿𝑚𝑎𝑥−log 𝐿𝑚𝑖𝑛

+ 𝐶𝐸𝑚𝑖𝑛, where 𝐶𝐸𝑚𝑎𝑥, 𝐶𝐸𝑚𝑖𝑛 are the maximum and minimum scores of clinical evaluation 

and log 𝐿𝑚𝑎𝑥 , log 𝐿𝑚𝑖𝑛 are maximum and minimum values of the log-likelihood.  

In contrast to these template-based score functions, Capecci et al. introduced the following two target-based score functions in [33] 

and  [126],  respectively: score(𝑖𝑛𝑝𝑢𝑡) = {

0          𝑖𝑓 ∆ − |𝑡𝑎𝑟𝑔𝑒𝑡 − 𝑖𝑛𝑝𝑢𝑡| < 0
30

(∆ − |𝑡𝑎𝑟𝑔𝑒𝑡 − 𝑖𝑛𝑝𝑢𝑡|)  𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

∆

,  and score(𝑖𝑛𝑝𝑢𝑡) = (1 + |

𝑖𝑛𝑝𝑢𝑡−𝑡𝑎𝑟𝑔𝑒𝑡

∆

−1

, 

|)

where input denotes the extracted features, the target is the designed clinical goal to achieve and ∆ stands for the admitted tolerance.  

Different from the above scoring functions, Jung et al. [37] designed a function on the basis of classifiers. The reaching evaluation 

score of a session 𝑖 was defined as 𝑠𝑐𝑜𝑟𝑒 𝑖 = [∑

8
𝑓=1

𝑖
𝑂𝐿𝑓

, ∑

8
𝑓=1

𝑖
𝑂𝑅𝑓

𝑖
 ], where 𝑂𝐿𝑓

𝑖
 and 𝑂𝑅𝑓

 are the sub-classifier outputs for the left 

and right sides, respectively. The maximal score of [8, 8] corresponds to the subjects performed the experimental task perfectly 

with either the left or right side, whereas the minimal score of [1, 1] indicates the opposite.  

The choice of a scoring function depends on the method used for movement evaluation. Researchers have paid less attention to the 

scoring function component of the systems for rehabilitation evaluation, and there is a lack of studies that perform a comprehensive 

evaluation of the various scoring functions introduced in related works.    

8.  Future Directions 

This review summarized the motivations for automated rehabilitation evaluation, reviewed the main sensors for capturing human 

motion,  and  discussed  existing  evaluation  approaches  in  the  literature.  In  this  section,  several  future  research  directions  are 

discussed. 

Definition  of  movement  features  for  automated  progress  evaluation—In  clinical  practice,  the  quality  of  patient  movements  or 

exercise performance is often assessed subjectively based on visual observation by a clinician. Although some patient scenarios 

allow for the use of quantitative functional measures (e.g., postural sway analysis, reaction time, range of motion testing) or patient-

reported questionnaires for evaluation, clinicians often rely on more subjective tests (e.g., Functional Movement Screen™, specific 

skill  evaluation,  muscle  tests) or  their  individual  understanding of the  correct  performance  of an exercise  without the use of  a 

checklist, rubric, or strict rules to inform evaluation of patient function or performance [25], [28], [158]. The use of subjective and 

intuitive  evaluation  methods  creates  the  risk  of  measurement  errors  due  to  outside  factors  (e.g.,  clinician  bias,  measurement 

imprecision, etc.) which affect the reliability and validity of the evaluation, and clinicians may overestimate patient performance 

or function [25], [159]. Defining sets of recommended criteria for movement evaluation may reduce the variability and subjectivity 

in related tests [25], [158], as well it can benefit the approaches for automated evaluation of exercises.  

Although several research works have addressed this problem, the focus has been solely on quantifying rehabilitation movements 

for static body postures [49] and balance tests [81]. A recent study by Capecci et al. [62] proposed rules for assessing rehabilitation 

exercises based on attained target joint angles, target joint velocities, and postural constraints in accomplishing the goals of each 

exercise.  However,  the  study  proposed  guidelines  for  only  five  selected  rehabilitation  exercises.  The  development  of  similar 

standard features for quantifying the various exercises that are commonly used in rehabilitation applications, and subsequently, the 

availability of databases with clinician-scored exercises, would provide the needed ground truth and facilitate the development of 

systems  for  automated  performance  monitoring  and  evaluation.  Furthermore,  valuation  of  the  practical  relevance  of  such 

approaches should also include non-inferiority studies, to provide a better understanding of whether unsupervised rehabilitation 

with systems for automated performance evaluation produces similar therapeutic benefits as clinician-supervised rehabilitation. 

17 

 
 
 
Such studies will furnish the necessary insights underpinning the usability and benefits of the systems for clinical support of at-

home rehabilitation.  

Deep  neural  networks  for  feature  learning—The  majority  of  prior  related  studies  are  based  on  manual  selection  of  important 

movement features, or engineering new features from captured motion data. Such approaches have limitations since they require 

specialized expertise and motion kinematics knowledge to manually extract practical features from motion data, and the extracted 

features cannot be reused for new exercises. Although traditional feature engineering algorithms, such as PCA [47] and manifold 

learning [41], can be applied to encode local or global features for exercise evaluation, these methods generally assume certain 

preconditions. Deep learning-based approaches are widely used across various applications for encoding feature representations 

without the need for domain-specific knowledge. The ability of deep learning models to encapsulate highly nonlinear relations 

among  sets  of  observed  and  latent  variables,  as  well  as  the  capacity  to  encode  data  features  at  multiple  hierarchical  levels  of 

abstraction  make  them an attractive  means  for  motion  modeling and analysis.  For example, Vakanski et al.  [39] employed an 

autoencoder  neural  network  for  dimensionality  reduction  of  rehabilitation  data.  Other  related  research  proposed  different 

architectures  of  neural  networks  for  learning  spatial  and  temporal  features  from  movement  data  [160]–[164].  In [162],  a deep 

recurrent network was designed to learn co-occurrence features from skeletal data through a novel regularization scheme. Similarly, 

Song  et  al.  [163]  proposed  a  deep  network  that  introduced  spatial  and  temporal  attention  subnetworks.  The  spatial  attention 

mechanism facilitated the selection of dominant joints, whereas the temporal attention assigned greater  weights to salient time 

frames.  Despite  the  large  body  of  previous  literature  and  research  on  deep  learning  for  motion  modeling  (e.g.,  for  motion 

recognition, classification), little research has been conducted on movement evaluation in rehabilitation exercises.  

Large-scale rehabilitation datasets—One of the reasons for the limited research on deep learning for evaluation of human motions 

in physical rehabilitation applications is the lack of large-scale annotated datasets of rehabilitation exercises. The publicly available 

datasets for rehabilitation evaluation covered in this review are of relatively small size, and typically do not offer clinically relevant 

scores provided by experienced medical professionals. The emergence of larger and more comprehensive datasets, such as UI-

PRMD and KIMORE, provides a basis for research in this direction and  optimism that the community will put efforts into the 

collection of new datasets.  

Combining movement quality with pain level evaluation—Patients’ pain level during a rehabilitation exercise session can reflect 

their health status, and thus, it can be an important indicator of the treatment outcome. Accordingly, a great deal of study focused 

on assessing pain level [109], [110], [112], [165], [166]. E.g., in [110] facial images captured by a smartphone were used to estimate 

the pain level of cancer patients. Milton et al. [165] studied the relation between common symptoms and health aspects, and found 

that the pain intensity produces stronger relationships when compared to other symptoms. Aung et al. [166] reviewed the literature 

on non-verbal expression of chronic pain to select factors that contribute to the occurrence of pain-related behaviors, and further 

discussed  how  the  detection  of  pain-related  behaviors  could  support  rehabilitation.  Integrating  movement  evaluation  based  on 

captured motion data and pain evaluation based on visual facial expressions into a single comprehensive rehabilitation indicator 

can be an exciting research direction.    

Fusing data from depth sensors and  inertial sensors—As we stated earlier, the extraction of skeletal data is a trivial task with 

existing vision/depth sensors. However, these sensors are susceptible to the external environment (such as occlusion of body parts 

and lighting conditions) and have limitations in providing accurate information. For example, Destelle et al. [167] improved the 

accuracy of skeletal data extracted from depth cameras with the use of inertial data. Chen et al. [168] reported enhanced movement 

recognition  when  fusing  depth  and  inertial  data.  Moreover,  the  fusion  of  depth,  skeleton,  and  inertial  data  for  human  action 

18 

 
 
recognition has been explored in numerous other studies [169]–[173]. In contrast, data fusion from different types of sensors is 

rarely applied to assessing rehabilitation exercises, and it can be a promising research avenue. 

Motion capture using a smartphone—At the present time, smartphones are accessible to most people as they are becoming more 

affordable. Most smartphones possess high-resolution cameras and advanced inertial sensors. Therefore, the use of smartphones 

to record human motion for exercise evaluation is appealing. Some efforts have been made in this regard [174]–[181]; however, 

these works have been limited to the use of inertial data for rehabilitation evaluation. In addition, there is no systematic study on 

the accuracy and reliability of these sensors for motion capture. In 2018, Apple introduced the TrueDepth IR (infrared) camera to 

the  iPhone  X  line.  The  camera  allows  motion  capture  with  the  smartphone,  which  offers  great  potential  for  related  medical 

applications. It is very likely that most of the other smartphone manufacturers will introduce similar motion capture technology to 

their models in the very near future. The use of smartphones for movement evaluation is particularly attractive and suitable for 

home-based rehabilitation.  

Combining evaluation with voice assistants—Providing a qualitative or quantitative evaluation score of rehabilitation exercises to 

patients is far from sufficient  to support effective implementation of  at-home rehabilitation programs. The integration of voice 

assistants for conveying evaluation feedback to the patients can greatly improve the efficiency and user-friendliness of these types 

of systems. For instance, an integrated voice assistant (similar to Alexa or Google’s voice assistant) can instruct the patient on the 

sequence of movements to perform or the correctness of the posture during a practice session, as well as provide suggestions on 

how to improve the exercise quality or which aspects of the movements are not performed correctly.  

9.  Conclusion 

This paper presents a review of computational approaches for automatic evaluation of patient performance in rehabilitation exercise 

programs, with a focus on machine learning methods for quantification of the quality of patient movements performed in a home-

based setting.  

The review categorizes the pertinent approaches into three major groups: discrete movement score, rule-based, and template-based 

approaches. The main characteristics, advantages and disadvantages of these groups of approaches, and representative studies of 

related works in the literature are described in the paper. We also detail the sensory systems used for data collection of rehabilitation 

movements  and  provide  a  description  of  the  respective  datasets  for  rehabilitation  evaluation.  The  study  reviews  respective 

publications on the related topics on feature engineering, movement segmentation, and scoring functions. Lastly, we list several 

recommendations for future directions in rehabilitation evaluation.   

The advances in machine learning and the advent of inexpensive and reliable motion capture sensors have inspired an increased 

interest in automated evaluation of rehabilitation exercises. Related studies in the literature corroborated the feasibility and viability 

of  such  technology,  and  advocated  that  it  can  create  substantial  benefits  both  for  patients  and  healthcare  systems.  Numerous 

research works reported high accuracy in predicting the level of correctness of patient performance in comparison to reference 

movement data collected with healthy subjects. In addition, comparative studies that employed clinicians’ evaluation of movement 

quality as ground truth for validation of the computational approaches for exercise evaluation reported a high correlation in the 

assigned  quality  scores.  These  findings  have  been  encouraging  and  evinced  the  potential  of  computational  approaches  for 

automated evaluation of rehabilitation exercises.  

Despite the progress, there are still open questions and numerous challenges to overcome before we can witness a broad deployment 

of these systems in home-based and in-clinic settings. On one hand, modeling human movements remains a challenging problem 

and requires devising novel models that can successfully encapsulate the inherent variability in human movements. Furthermore, 

19 

 
 
 
little research has been conducted on evaluating the impact of the approaches for rehabilitation evaluation on the long-term patient 

outcomes and whether the provided evaluation produces similar therapeutic benefits as clinician-supervised exercise programs. 

Another  major  impediment  is  the  reliance  of  the  greatest  majority  of  the  related  systems  on  Kinect  v1  or  v2  sensors,  whose 

production  was  discontinued  by  Microsoft  in  2018.  Fortunately,  Microsoft  introduced  a  new  version  Azure  Kinect  in  2019; 

however, the new sensor uses a different programming platform, and cannot re-use the programs developed for the older Kinect 

sensors in numerous research studies.  

The recent introduction of cell-phones with motion tracking capabilities combined with the progress in cloud computing services 

offer enormous potential for widespread use of this technology. We believe that these systems will be ubiquitous in the near feature 

and they will play an important role in complementing the traditional approaches for evaluation of rehabilitation exercises.    

10. Acknowledgments 

This work was supported by the Institute for Modeling Collaboration and Innovation (IMCI) at the University of Idaho through 

NIH Award #P20GM104420.  

References 

[1]  R. P. Van Peppen, G. Kwakkel, S. Wood-Dauphinee, H. J. Hendriks, P. J. Van der Wees, and J. Dekker, “The impact of physical therapy 

on functional outcomes after stroke: what’s the evidence?,” Clin. Rehabil., vol. 18, no. 8, pp. 833–862, 2004. 

[2]  C. Burtin et al., “Early exercise in critically ill patients enhances short-term functional recovery,” Crit. Care Med., vol. 37, no. 9, pp. 2499–

2505, 2009. 

[3]  P. Langhorne, J. Bernhardt, and G. Kwakkel, “Stroke rehabilitation,” The Lancet, vol. 377, no. 9778, pp. 1693–1702, 2011. 

[4]  D.  U.  Jette,  R.  L.  Warren,  and  C.  Wirtalla,  “The  relation  between  therapy  intensity  and  outcomes  of  rehabilitation  in  skilled  nursing 

facilities,” Arch. Phys. Med. Rehabil., vol. 86, no. 3, pp. 373–379, 2005. 

[5]  M. J. Bade and J. E. Stevens-Lapsley, “Early high-intensity rehabilitation following total knee arthroplasty improves outcomes,” J. Orthop. 

Sports Phys. Ther., vol. 41, no. 12, pp. 932–941, 2011. 

[6]  T.  L.  Chmielewski  et  al.,  “Low-versus  high-intensity  plyometric  exercise  during  rehabilitation  after  anterior  cruciate  ligament 

reconstruction,” Am. J. Sports Med., vol. 44, no. 3, pp. 609–617, 2016. 

[7]  C. Sherrington, A. Tiedemann, N. Fairhall, J. C. Close, and S. R. Lord, “Exercise to prevent falls in older adults: an updated meta-analysis 

and best practice recommendations,” New South Wales Public Health Bull., vol. 22, no. 4, pp. 78–83, 2011. 

[8] 

I. K. Ho, K. R. Goldschneider, S. Kashikar-Zuck, U. Kotagal, C. Tessman, and B. Jones, “Healthcare utilization and indirect burden among 

families of pediatric patients with chronic pain,” J. Musculoskelet. Pain, vol. 16, no. 3, pp. 155–164, 2008. 

[9]  S. R. Machlin, J. Chevan, W. W. Yu, and M. W. Zodet, “Determinants of utilization and expenditures for episodes of ambulatory physical 

therapy among adults,” Phys. Ther., vol. 91, no. 7, pp. 1018–1029, 2011. 

[10]  S. K. Saxena, T. P. Ng, D. Yong, N. P. Fong, and K. Gerald, “Total direct cost, length of hospital stay, institutional discharges and their 

determinants from rehabilitation settings in stroke patients,” Acta Neurol. Scand., vol. 114, no. 5, pp. 307–314, 2006. 

[11]  R. Komatireddy, A. Chokshi, J. Basnett, M. Casale, D. Goble, and T. Shubert, “Quality and quantity of rehabilitation exercises delivered 

by a 3-D motion controlled camera: A pilot study,” Int. J. Phys. Med. Rehabil., vol. 2, no. 4, 2014. 

[12]  S.  F.  Bassett  and  H.  Prapavessis,  “Home-based  physical  therapy  intervention  with  adherence-enhancing  strategies  versus  clinic-based 

management for patients with ankle sprains,” Phys. Ther., vol. 87, no. 9, pp. 1132–1143, 2007. 

[13]  K. Jack, S. M. McLean, J. K. Moffett, and E. Gardiner, “Barriers to treatment adherence in physiotherapy outpatient clinics: a systematic 

review,” Man. Ther., vol. 15, no. 3, pp. 220–228, 2010. 

20 

 
 
 
 
[14]  K. K. Miller, R. E. Porter, E. DeBaun-Sprague, M. Van Puymbroeck, and A. A. Schmid, “Exercise after stroke: patient adherence and 

beliefs after discharge from rehabilitation,” Top. Stroke Rehabil., vol. 24, no. 2, pp. 142–148, 2017. 

[15]  P.  Maciejasz,  J.  Eschweiler,  K.  Gerlach-Hahn,  A.  Jansen-Troy,  and  S.  Leonhardt,  “A  survey  on  robotic  devices  for  upper  limb 

rehabilitation,” J. Neuroengineering Rehabil., vol. 11, no. 1, p. 3, 2014. 

[16]  J. Broeren, A. Björkdahl, R. Pascher, and M. Rydmark, “Virtual reality and haptics as an assessment device in the postacute phase after 

stroke,” Cyberpsychol. Behav., vol. 5, no. 3, pp. 207–211, 2002. 

[17]  L.  V.  Gauthier  et  al.,  “Video  Game  Rehabilitation  for  Outpatient  Stroke  (VIGoROUS):  protocol  for  a  multi-center  comparative 

effectiveness trial of in-home gamified constraint-induced movement therapy for rehabilitation of chronic upper extremity hemiparesis,” 

BMC Neurol., vol. 17, no. 1, p. 109, 2017. 

[18]  “Xbox  One :  Top  Issues,”  Xbox  Support.  [Online].  Available:  https://support.xbox.com/en-US/browse/xbox-one.  [Accessed:  04-Apr-

2019]. 

[19]  “Xtion PRO | 3D Sensor,” ASUS Global. [Online]. Available: https://www.asus.com/3D-Sensor/Xtion_PRO/. [Accessed: 04-Apr-2019]. 

[20]  I. Ar and Y. S. Akgul, “A monitoring system for home-based physiotherapy exercises,” in Computer and Information Sciences III, Springer, 

2013, pp. 487–494. 

[21]  H.  Mousavi  Hondori  and  M.  Khademi,  “A  review  on  technical  and  clinical  impact  of  microsoft  kinect  on  physical  therapy  and 

rehabilitation,” J. Med. Eng., vol. 2014, 2014. 

[22]  D.  Antón,  A.  Goñi,  A.  Illarramendi,  J.  J.  Torres-Unda,  and  J.  Seco,  “KiReS:  A  Kinect-based  telerehabilitation  system,”  in  e-Health 

Networking, Applications & Services (Healthcom), 2013 IEEE 15th International Conference on, 2013, pp. 444–448. 

[23]  W. Liao, S. M. Waller, and J. Whitall, “Kinect-based individualized upper extremity rehabilitation is effective and feasible for individuals 

with  stroke  using  a  transition  from  clinic  to  home  protocol,”  Cogent  Med.,  vol.  5,  no.  1,  p.  1428038,  Jan.  2018,  doi: 

10.1080/2331205X.2018.1428038. 

[24]  J. A. Howe, E. L. Inness, A. Venturini, J. I. Williams, and M. C. Verrier, “The Community Balance and Mobility Scale-a balance measure 

for individuals with traumatic brain injury,” Clin. Rehabil., vol. 20, no. 10, pp. 885–895, 2006. 

[25]  G. Cook, L. Burton, B. J. Hoogenboom, and M. Voight, “Functional Movement Screening: The Use of Fundamental Movements as an 

Assessment of Function ‐ Part 1,” Int. J. Sports Phys. Ther., vol. 9, no. 3, pp. 396–409, May 2014. 

[26]  J. W. Cuchna, M. C. Hoch, and J. M. Hoch, “The interrater and intrarater reliability of the functional movement screen: A systematic 

review  with  meta-analysis,”  Phys.  Ther.  Sport  Off.  J.  Assoc.  Chart.  Physiother.  Sports  Med.,  vol.  19,  pp.  57–65,  May  2016,  doi: 

10.1016/j.ptsp.2015.12.002. 

[27]  R. M. Hulteen, N. J. Lander, P. J. Morgan, L. M. Barnett, S. J. Robertson, and D. R. Lubans, “Validity and Reliability of Field-Based 

Measures for Assessing Movement Skill Competency in Lifelong Physical Activities: A Systematic Review,” Sports Med. Auckl. NZ, vol. 

45, no. 10, pp. 1443–1454, Oct. 2015, doi: 10.1007/s40279-015-0357-0. 

[28]  E.  D.  Oña  Simbaña,  P.  Sánchez-Herrera,  A.  J.  Huete,  and  C.  Balaguer,  “Review  of  Automated  Systems  for  Upper  Limbs  Functional 

Assessment in Neurorehabilitation.,” IEEE Access, vol. 7, pp. 32352–32367, 2019, doi: 10.1109/ACCESS.2019.2901814. 

[29]  Y.  Tao  and  H.  Hu,  “Colour  based  human  motion  tracking  for  home-based  rehabilitation,”  in  2004  IEEE  International  Conference  on 

Systems, Man and Cybernetics (IEEE Cat. No. 04CH37583), 2004, vol. 1, pp. 773–778. 

[30]  L.  E.  Sucar,  R.  Luis,  R.  Leder,  J.  Hernández,  and  I.  Sánchez,  “Gesture  therapy:  A  vision-based  system  for  upper  extremity  stroke 

rehabilitation,” in 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology, 2010, pp. 3690–3693. 

[31]  Y.  Tao  and  H.  Hu,  “Building  a  visual  tracking  system  for  home-based  rehabilitation,”  in  Proc.  of  the  9th  Chinese  Automation  and 

Computing Society Conf. In the UK, 2003, pp. 343–448. 

[32]  “MTi 600-series - Products,” Xsens 3D motion tracking. [Online]. Available: https://www.xsens.com/products/mti-600-series/. [Accessed: 

28-Jul-2019]. 

[33]  M. Capecci et al., “A tool for home-based rehabilitation allowing for clinical evaluation in a visual markerless scenario,” in 2015 37th 

Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2015, pp. 8034–8037. 

21 

 
 
[34]  M. Capecci et al., “Accuracy evaluation of the kinect v2 sensor during dynamic movements in a rehabilitation scenario,” in 2016 38th 

Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2016, pp. 5409–5412. 

[35]  S. Spasojević, A. Rodić, and J. Santos-Victor, “Kinect-Based Approach for Upper Body Movement Assessment in Stroke,” in New Trends 

in Medical and Service Robotics, Springer, 2019, pp. 153–160. 

[36]  T. Hachaj and M. R. Ogiela, “Rule-based approach to recognizing human body poses and gestures in real time,” Multimed. Syst., vol. 20, 

no. 1, pp. 81–99, 2014. 

[37]  J.-Y. Jung, J. I. Glasgow, and S. H. Scott, “Feature selection and classification for assessment of chronic stroke impairment,” in 2008 8th 

IEEE International Conference on BioInformatics and BioEngineering, 2008, pp. 1–5. 

[38]  R. R. Coifman and S. Lafon, “Diffusion maps,” Appl. Comput. Harmon. Anal., vol. 21, no. 1, pp. 5–30, 2006. 

[39]  A. Vakanski, J. M. Ferguson, and S. Lee, “Mathematical modeling and evaluation of human motions in physical therapy using mixture 

density neural networks,” J. Physiother. Phys. Rehabil., vol. 1, no. 4, 2016. 

[40]  A. Paiement, L. Tao, S. Hannuna, M. Camplani, D. Damen, and M. Mirmehdi, “Online quality assessment of human  movement from 

skeleton data,” in British Machine Vision Conference, 2014, pp. 153–166. 

[41]  L. Tao et al., “A comparative study of pose representation and dynamics modelling for online motion quality assessment,” Comput. Vis. 

Image Underst., vol. 148, pp. 136–152, 2016. 

[42]  F. Bashir, W. Qu, A. Khokhar, and D. Schonfeld, “HMM-based motion recognition system using segmented PCA,” in IEEE International 

Conference on Image Processing 2005, 2005, vol. 3, pp. III–1288. 

[43]  B. Crabbe, A. Paiement, S. Hannuna, and M. Mirmehdi, “Skeleton-free body pose estimation from depth images for movement analysis,” 

in Proceedings of the IEEE International Conference on Computer Vision Workshops, 2015, pp. 70–78. 

[44]  M.-C. Huang, J. J. Liu, W. Xu, N. Alshurafa, X. Zhang, and M. Sarrafzadeh, “Using pressure map sequences for recognition of on bed 

rehabilitation exercises,” IEEE J. Biomed. Health Inform., vol. 18, no. 2, pp. 411–418, 2014. 

[45]  P. E. Taylor, G. J. Almeida, T. Kanade, and J. K. Hodgins, “Classifying human motion quality for knee osteoarthritis using accelerometers,” 

in 2010 Annual international conference of the IEEE engineering in medicine and biology, 2010, pp. 339–343. 

[46]  T.  T.  Um  et  al.,  “Parkinson’s  Disease  Assessment  from  a  Wrist-Worn  Wearable  Sensor  in  Free-Living  Conditions:  Deep  Ensemble 

Learning and Visualization,” ArXiv Prepr. ArXiv180802870, 2018. 

[47]  S. Jun, S. Kumar, X. Zhou, D. K. Ramsey, and V. N. Krovi, “Automation for individualization of Kinect-based quantitative progressive 

exercise regimen,” in 2013 IEEE International Conference on Automation Science and Engineering (CASE), 2013, pp. 243–248. 

[48]  Z. Zhang, Q. Fang, L. Wang, and P. Barrett, “Template matching based motion classification for unsupervised post-stroke rehabilitation,” 

in International Symposium on Bioelectronics and Bioinformations 2011, 2011, pp. 199–202. 

[49]  W. Zhao, R. Lun, D. D. Espy, and M. A. Reinthal, “Realtime  motion assessment for rehabilitation exercises: Integration of kinematic 

modeling with fuzzy inference,” J. Artif. Intell. Soft Comput. Res., vol. 4, no. 4, pp. 267–285, 2014. 

[50]  A. P. L. Bo, M. Hayashibe, and P. Poignet, “Joint angle estimation in rehabilitation with inertial sensors and its integration with Kinect,” 

in 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2011, pp. 3479–3483. 

[51]  G.  Kurillo,  J.  J.  Han,  A.  Nicorici,  and  R.  Bajcsy,  “Tele-MFAsT:  Kinect-Based  Tele-Medicine  Tool  for  Remote  Motion  and  Function 

Assessment.,” in Medicine Meets Virtual Reality 21 - NextMed, MMVR 2014, Manhattan Beach, California, USA, February 19-22, 2014, 

2014, pp. 215–221, doi: 10.3233/978-1-61499-375-9-215. 

[52]  F. Benetazzo et al., “Low cost rgb-d vision based system for on-line performance evaluation of motor disabilities rehabilitation at home,” 

in Proceedings of the 5th Forum Italiano on Ambient Assisted Living ForItAAL. IEEE, 2014. 

[53]  R. Houmanfar, M. Karg, and D. Kulić, “Movement analysis of rehabilitation exercises: Distance metrics for measuring patient progress,” 

IEEE Syst. J., vol. 10, no. 3, pp. 1014–1025, 2016. 

[54]  R.  H.  Osgouei,  D.  Soulsbv,  and  F.  Bello,  “An  Objective  Evaluation  Method  for  Rehabilitation  Exergames,”  in  2018  IEEE  Games, 

Entertainment, Media Conference (GEM), 2018, pp. 28–34. 

22 

 
 
[55]  D. Antón, A. Goñi, and A. Illarramendi, “Exercise recognition for Kinect-based telerehabilitation,” Methods Inf. Med., vol. 54, no. 02, pp. 

145–155, 2015. 

[56]  C.-J. Su, “Personal rehabilitation exercise assistant with kinect and dynamic time warping,” Int. J. Inf. Educ. Technol., vol. 3, no. 4, pp. 

448–454, 2013. 

[57]  C.-J.  Su,  C.-Y.  Chiang,  and  J.-Y.  Huang,  “Kinect-enabled  home-based  rehabilitation  system  using  Dynamic  Time  Warping  and  fuzzy 

logic,” Appl. Soft Comput., vol. 22, pp. 652–666, 2014. 

[58]  A. Vakanski, J. M Ferguson, and S. Lee, “Metrics for Performance Evaluation of Patient Exercises during Physical Therapy,” Int. J. Phys. 

Med. Rehabil., vol. 05, no. 03, 2017, doi: 10.4172/2329-9096.1000403. 

[59]  J.  Dorado  et al.,  “A  computer-vision-based  system  for  at-home  rheumatoid  arthritis  rehabilitation.,”  IJDSN,  vol.  15,  no. 9, 2019, doi: 

10.1177/1550147719875649. 

[60]  M.  Devanne,  “Multi-level  motion  analysis  for  physical  exercises  assessment  in  kinaesthetic  rehabilitation,”  in  2017  IEEE-RAS  17th 

International Conference on Humanoid Robotics (Humanoids), 2017, pp. 529–534. 

[61]  M. Capecci et al., “Physical rehabilitation exercises assessment based on hidden semi-markov model by kinect v2,” in 2016 IEEE-EMBS 

International Conference on Biomedical and Health Informatics (BHI), 2016, pp. 256–259. 

[62]  M. Capecci et al., “A Hidden Semi-Markov Model based approach for rehabilitation exercise assessment,” J. Biomed. Inform., vol. 78, pp. 

1–11, 2018. 

[63]  C. Williams, A. Vakanski, S. Lee, and D. Paul, “Assessment of physical rehabilitation movements through dimensionality reduction and 

statistical modeling,” Med. Eng. Phys., vol. 74, pp. 13–22, Dec. 2019, doi: 10.1016/j.medengphy.2019.10.003. 

[64]  Y. Liao, A. Vakanski, and M. Xian, “A Deep Learning Framework for Assessing Physical Rehabilitation Exercises,” IEEE Trans. Neural 

Syst. Rehabil. Eng., vol. 28, no. 2, pp. 468–477, 2020. 

[65]  S. Maggioni et al., “Robot-aided assessment of lower extremity functions: a review,” J. NeuroEngineering Rehabil., vol. 13, no. 1, p. 72, 

Aug. 2016, doi: 10.1186/s12984-016-0180-3. 

[66]  C. Shirota et al., “Robot-supported assessment of balance in standing and walking,” J. NeuroEngineering Rehabil., vol. 14, no. 1, p. 80, 

Aug. 2017, doi: 10.1186/s12984-017-0273-7. 

[67]  A. Schwarz, C. M. Kanzler, O. Lambercy, A. R. Luft, and J. M. Veerbeek, “Systematic Review on Kinematic Assessments of Upper Limb 

Movements After Stroke,” Stroke, vol. 50, no. 3, pp. 718–727, 2019, doi: 10.1161/STROKEAHA.118.023531. 

[68]  O.  Lambercy,  S.  Maggioni,  L.  Lünenburger,  R.  Gassert,  and  M.  Bolliger,  “Robotic  and  Wearable  Sensor  Technologies  for 

Measurements/Clinical  Assessments,”  in  Neurorehabilitation  Technology,  D.  J.  Reinkensmeyer  and  V.  Dietz,  Eds.  Cham:  Springer 

International Publishing, 2016, pp. 183–207. 

[69]  I. Díaz, J. J. Gil, and E. Sánchez, “Lower-Limb Robotic Rehabilitation: Literature Review and Challenges,” Journal of Robotics, 2011. 

[Online]. Available: https://www.hindawi.com/journals/jr/2011/759764/. [Accessed: 14-Jan-2020]. 

[70]  J. Mehrholz, S. Thomas, C. Werner, J. Kugler, M. Pohl, and B. Elsner, “Electromechanical-assisted training for walking after stroke,” 

Cochrane Database Syst. Rev., vol. 5, p. CD006185, 10 2017, doi: 10.1002/14651858.CD006185.pub4. 

[71]  I. Ar and Y. S. Akgul, “A computerized recognition system for the home-based physiotherapy exercises using an RGBD camera,” IEEE 

Trans. Neural Syst. Rehabil. Eng., vol. 22, no. 6, pp. 1160–1171, 2014. 

[72]  M. P. Cuellar, M. Ros, M. J. Martin-Bautista, Y. Le Borgne, and G. Bontempi, “An approach for the evaluation of human activities in 

physical therapy scenarios,” in International Conference on Mobile Networks and Management, 2014, pp. 401–414. 

[73]  S. Hagler, H. B. Jimison, R. Bajcsy, and M. Pavel, “Quantification of human movement for assessment in automated exercise coaching,” 

in 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2014, pp. 5836–5839. 

[74]  S. Nomm and K. Buhhalko, “Monitoring of the human motor functions rehabilitation by neural networks based system with kinect sensor,” 

IFAC Proc. Vol., vol. 46, no. 15, pp. 249–253, 2013. 

[75]  P. Uttarwar and D. Mishra, “Development of a kinect-based physical rehabilitation system,” in 2015 Third International Conference on 

Image Information Processing (ICIIP), 2015, pp. 387–392. 

23 

 
 
[76]  T. Dutta, “Evaluation of the KinectTM sensor for 3-D kinematic measurement in the workplace,” Appl. Ergon., vol. 43, no. 4, pp. 645–649, 

2012. 

[77]  R. A. Clark et al., “Reliability and concurrent validity of the Microsoft Xbox One Kinect for assessment of standing balance and postural 

control,” Gait Posture, vol. 42, no. 2, pp. 210–213, 2015. 

[78]  G. Tao, P. S. Archambault, and M. F. Levin, “Evaluation of Kinect skeletal tracking in a virtual reality rehabilitation system for upper limb 

hemiparesis,” in 2013 international conference on virtual rehabilitation (ICVR), 2013, pp. 164–165. 

[79]  R. A. Clark, K. J. Bower, B. F. Mentiplay, K. Paterson, and Y.-H. Pua, “Concurrent validity of the Microsoft Kinect for assessment of 

spatiotemporal gait variables,” J. Biomech., vol. 46, no. 15, pp. 2722–2725, 2013. 

[80]  B.  Galna,  G.  Barry,  D.  Jackson,  D.  Mhiripiri,  P.  Olivier,  and  L.  Rochester,  “Accuracy  of  the  Microsoft  Kinect  sensor  for  measuring 

movement in people with Parkinson’s disease,” Gait Posture, vol. 39, no. 4, pp. 1062–1068, 2014. 

[81]  A. K. Mishra, M. Skubic, and C. Abbott, “Development and preliminary validation of an interactive remote physical therapy system,” in 

2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2015, pp. 190–193. 

[82]  X. Xu and R. W. McGorry, “The validity of the first and second generation Microsoft KinectTM for identifying joint center locations during 

static postures,” Appl. Ergon., vol. 49, pp. 47–54, 2015. 

[83]  R. A. Clark et al., “Validity of the Microsoft Kinect for assessment of postural control,” Gait Posture, vol. 36, no. 3, pp. 372–377, 2012. 

[84]  P. Fankhauser, M. Bloesch, D. Rodriguez, R. Kaestner, M. Hutter, and R. Siegwart, “Kinect v2 for mobile robot navigation: Evaluation 

and modeling,” in 2015 International Conference on Advanced Robotics (ICAR), 2015, pp. 388–394. 

[85]  E. Dolatabadi, B. Taati, and A. Mihailidis, “Concurrent validity of the Microsoft Kinect for Windows v2 for measuring spatiotemporal gait 

parameters,” Med. Eng. Phys., vol. 38, no. 9, pp. 952–958, 2016. 

[86]  M. Samir, E. Golkar, and A. A. A. Rahni, “Comparison between the KinectTM V1 and KinectTM V2 for respiratory motion tracking,” in 

2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), 2015, pp. 150–155. 

[87]  L. G. Wiedemann, R. Planinc, I. Nemec, and M. Kampel, “Performance evaluation of joint angles obtained by the Kinect v2,” 2015. 

[88]  K. Otte et al., “Accuracy and reliability of the kinect version 2 for clinical measurement of motor function,” PloS One, vol. 11, no. 11, p. 

e0166532, 2016. 

[89]  A. Napoli, S. Glass, C. Ward, C. Tucker, and I. Obeid, “Performance analysis of a generalized motion capture system using microsoft 

kinect 2.0,” Biomed. Signal Process. Control, vol. 38, pp. 265–280, 2017. 

[90]  D.  M.  Swoboda,  “A  comprehensive  characterization  of  the  asus  xtion  pro  depth  sensor,”  in  I:  European  Conference  on  Educational 

Robotics, 2014, p. 3. 

[91]  “Intel®  RealSenseTM  Technology,” 

Intel. 

[Online].  Available:  https://www.intel.com/content/www/us/en/architecture-and-

technology/realsense-overview.html. [Accessed: 09-Aug-2019]. 

[92]  “Official  Structure  Sensor  Store 

-  Give  Your 

iPad  3D  Vision.” 

[Online].  Available:  https://structure.io/structure-

sensor?gclid=Cj0KCQjws7TqBRDgARIsAAHLHP4GRtyEeq69V3WS2WG3DN6Xbye8tugb44pH_UDezhF6WPzOEocA7LMaAvs0E

ALw_wcB. [Accessed: 09-Aug-2019]. 

[93]  OptiTrack, “Motion Capture Systems,” OptiTrack. [Online]. Available: http://optitrack.com/index.html. [Accessed: 28-Jul-2019]. 

[94]  “PhaseSpace  Motion  Capture 

| 

Products 

| 

Impulse  X2  Motion  Capture 

Solution.” 

[Online].  Available: 

http://www.phasespace.com/impulse-motion-capture.html. [Accessed: 28-Jul-2019]. 

[95]  J. G. Richards, “The measurement of human motion: A comparison of commercially available systems,” Hum. Mov. Sci., vol. 18, no. 5, 

pp. 589–602, 1999. 

[96]  M. Windolf, N. Götzen, and M. Morlock, “Systematic accuracy and precision analysis of video motion capturing systems—exemplified 

on the Vicon-460 system,” J. Biomech., vol. 41, no. 12, pp. 2776–2780, 2008. 

[97]  P. Merriaux, Y. Dupuis, R. Boutteau, P. Vasseur, and X. Savatier, “A study of vicon system positioning performance,” Sensors, vol. 17, 

no. 7, p. 1591, 2017. 

24 

 
 
[98]  M. L’Hermette, X. Savatier, L. Baudry, C. Tourny-Chollet, and F. Dujardin, “A new portable device for assessing locomotor performance,” 

Int. J. Sports Med., vol. 29, no. 04, pp. 322–326, 2008. 

[99]  D. Thewlis, C. Bishop, N. Daniell, and G. Paul, A comparison of two commercially available motion capture systems for gait analysis: 

High end vs low-cost. 2011. 

[100] F. Schlagenhauf, P. P. Sahoo, and W. Singhose, “A comparison of dual-kinect and vicon tracking of human motion for use in robotic 

motion programming,” Robot Autom Eng J, vol. 1, no. 2, p. 555558, 2017. 

[101] I. Lawrence and K. Lin, “A concordance correlation coefficient to evaluate reproducibility,” Biometrics, pp. 255–268, 1989. 

[102] M. Kok, J. D. Hol, and T. B. Schön, “Using inertial sensors for position and orientation estimation,” ArXiv Prepr. ArXiv170406053, 2017. 

[103] P. Y. M. Chung and G. Y. F. Ng, “Comparison between an accelerometer and a three-dimensional motion analysis system for the detection 

of movement,” Physiotherapy, vol. 98, no. 3, pp. 256–259, 2012. 

[104] V. Lugade, E. Fortune, M. Morrow, and K. Kaufman, “Validity of using tri-axial accelerometers to measure human movement—Part I: 

Posture and movement detection,” Med. Eng. Phys., vol. 36, no. 2, pp. 169–176, 2014. 

[105] E. Fortune, V. Lugade, M. Morrow, and K. Kaufman, “Validity of using tri-axial accelerometers to measure human movement–Part II: 

Step counts at a wide range of gait velocities,” Med. Eng. Phys., vol. 36, no. 6, pp. 659–669, 2014. 

[106] C. Larivière, H. Mecheri, A. Shahvarpour, D. Gagnon, and A. Shirazi-Adl, “Criterion validity and between-day reliability of an inertial-

sensor-based trunk postural stability test during unstable sitting,” J. Electromyogr. Kinesiol., vol. 23, no. 4, pp. 899–907, 2013. 

[107] C. M. Bauer et al., “Concurrent validity and reliability of a novel wireless inertial measurement system to assess trunk movement,”  J. 

Electromyogr. Kinesiol., vol. 25, no. 5, pp. 782–790, 2015. 

[108] E. P. Washabaugh, T. Kalyanaraman, P. G. Adamczyk, E. S. Claflin, and C. Krishnan, “Validity and repeatability of inertial measurement 

units for measuring gait parameters,” Gait Posture, vol. 55, pp. 87–93, 2017. 

[109] T. Hadjistavropoulos, D. L. La Chapelle, H. D. Hadjistavropoulos, S. Green, and G. J. Asmundson, “Using facial expressions to assess 

musculoskeletal pain in older persons,” Eur. J. Pain, vol. 6, no. 3, pp. 179–187, 2002. 

[110] M. K. Hasan, G. M. T. Ahsan, S. I. Ahamed, R. Love, and R. Salim, “Pain level detection from facial image captured by smartphone,” J. 

Inf. Process., vol. 24, no. 4, pp. 598–608, 2016. 

[111] C. N. Teague et al., “Novel methods for sensing acoustical emissions from the knee for wearable joint health assessment,” IEEE Trans. 

Biomed. Eng., vol. 63, no. 8, pp. 1581–1590, 2016. 

[112] C. L. Herry and M. Frize, “Quantitative assessment of pain-related thermal dysfunction through clinical digital infrared thermal imaging,” 

Biomed. Eng. Online, vol. 3, no. 1, p. 19, 2004. 

[113] C. Chen, R. Jafari, and N. Kehtarnavaz, “UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a 

wearable inertial sensor,” in 2015 IEEE International conference on image processing (ICIP), 2015, pp. 168–172. 

[114] F. Han, B. Reily, W. Hoff, and H. Zhang, “Space-time representation of people based on 3D skeletal data: A review.,” Comput. Vis. Image 

Underst., vol. 158, pp. 85–105, 2017, doi: 10.1016/j.cviu.2017.01.011. 

[115] F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal, and R. Bajcsy, “Berkeley MHAD: A comprehensive Multimodal Human Action Database.,” in 

2013 IEEE Workshop on Applications of Computer Vision, WACV 2013, Clearwater Beach, FL, USA, January 15-17, 2013, 2013, pp. 53–

60, doi: 10.1109/WACV.2013.6474999. 

[116] J. Wang, Z. Liu, Y. Wu, and J. Yuan, “Mining actionlet ensemble for action recognition with depth cameras.,” in 2012 IEEE Conference 

on  Computer  Vision  and  Pattern  Recognition,  Providence,  RI,  USA,  June  16-21,  2012,  2012,  pp.  1290–1297,  doi: 

10.1109/CVPR.2012.6247813. 

[117] “UCI  Machine  Learning  Repository:  PAMAP2  Physical  Activity  Monitoring  Data  Set.” 

[Online].  Available: 

https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring. [Accessed: 10-Jul-2019]. 

[118] A.  Reiss  and  D.  Stricker,  “Creating  and  benchmarking  a  new  dataset  for  physical  activity  monitoring,”  in  Proceedings  of  the  5th 

International Conference on PErvasive Technologies Related to Assistive Environments, 2012, p. 40. 

25 

 
 
[119] S.  H.  Tilo  Burghardt,  “Depth  video  and  skeleton  of  people  walking  up  stairs,”  data.bris,  06-Nov-2014.  [Online].  Available: 

https://data.bris.ac.uk/data/dataset/bgresiy3olk41nilo7k6xpkqf. [Accessed: 16-Mar-2019]. 

[120] “K3Da Dataset | A Healthcare Dataset.” [Online]. Available: https://filestore.leightley.com/k3da/index.html. [Accessed: 11-Jul-2019]. 

[121] D. Leightley, M. H. Yap, J. Coulson, Y. Barnouin, and J. S. McPhee, “Benchmarking human motion analysis using kinect one: An open 

source dataset,” in 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015, pp. 

1–7. 

[122] J. M. Guralnik et al., “A short physical performance battery assessing lower extremity function: association with self-reported disability 

and prediction of mortality and nursing home admission,” J. Gerontol., vol. 49, no. 2, pp. M85–M94, 1994. 

[123] D. Podsiadlo and S. Richardson, “The timed ‘Up & Go’: a test of basic functional mobility for frail elderly persons,” J. Am. Geriatr. Soc., 

vol. 39, no. 2, pp. 142–148, 1991. 

[124] A. Vakanski, H. Jun, D. Paul, and R. Baker, “A Data Set of Human Body Movements for Physical Rehabilitation Exercises,” Data, vol. 3, 

no. 1, p. 2, 2018. 

[125] M. Capecci et al., “The KIMORE dataset: KInematic assessment of MOvement and clinical scores for remote  monitoring of physical 

REhabilitation,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 27, no. 7, pp. 1433–1448, 2019. 

[126] M. Capecci et al., “An instrumental approach for monitoring physical exercises in a visual markerless scenario: A proof of concept,” J. 

Biomech., vol. 69, pp. 70–80, 2018. 

[127] A. Karime, M. Eid, J. M. Alja’Am, A. El Saddik, and W. Gueaieb, “A fuzzy-based adaptive rehabilitation framework for home-based wrist 

training,” IEEE Trans. Instrum. Meas., vol. 63, no. 1, pp. 135–144, 2013. 

[128] X.  Yu  and  S.  Xiong,  “A  Dynamic  Time  Warping  Based  Algorithm  to  Evaluate  Kinect-Enabled  Home-Based  Physical  Rehabilitation 

Exercises for Older People,” Sensors, vol. 19, no. 13, p. 2882, 2019. 

[129] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by locally linear embedding,”  science, vol. 290, no. 5500, pp. 2323–

2326, 2000. 

[130] O. Giggins, K. T. Sweeney, and B. Caulfield, “The use of inertial sensors for the classification of rehabilitation exercises,” in 2014 36th 

Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2014, pp. 2965–2968. 

[131] P. Otten, J. Kim, and S. H. Son, “A Framework to Automate Assessment of Upper-Limb Motor Function Impairment: A Feasibility Study.,” 

Sensors, vol. 15, no. 8, pp. 20097–20114, 2015, doi: 10.3390/s150820097. 

[132] S. D. Din, S. Patel, C. Cobelli, and P. Bonato, “Estimating fugl-meyer clinical scores in stroke survivors using wearable sensors.,” in 33rd 

Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2011, Boston, MA, USA, August 30 - 

Sept. 3, 2011, 2011, pp. 5839–5842, doi: 10.1109/IEMBS.2011.6091444. 

[133] W.-S. Kim, S. Cho, D. Baek, H. Bang, and N.-J. Paik, “Upper Extremity Functional Evaluation by Fugl-Meyer Assessment Scoring Using 

Depth-Sensing Camera in Hemiplegic Stroke Patients,” PloS One, vol. 11, no. 7, p. e0158640, 2016, doi: 10.1371/journal.pone.0158640. 

[134] A. R. Parnandi, E. Wade, and M. J. Mataric, “Motor function assessment using wearable inertial sensors,”  2010 Annu. Int. Conf. IEEE 

Eng. Med. Biol., pp. 86–89, 2010, doi: 10.1109/IEMBS.2010.5626156. 

[135] S. Patel et al., “A Novel Approach to Monitor Rehabilitation Outcomes in Stroke Survivors Using Wearable Technology.,” Proc. IEEE, 

vol. 98, no. 3, pp. 450–461, 2010, doi: 10.1109/JPROC.2009.2038727. 

[136] H. Sakoe, S. Chiba, A. Waibel, and K. F. Lee, “Dynamic programming algorithm optimization for spoken word recognition,” Read. Speech 

Recognit., vol. 159, p. 224, 1990. 

[137] E. Saraee et al., “ExerciseCheck: remote monitoring and evaluation platform for home based physical therapy,” in Proceedings of the 10th 

International Conference on PErvasive Technologies Related to Assistive Environments, 2017, pp. 87–90. 

[138] Z. Zhang, Q. Fang, and X. Gu, “Objective assessment of upper-limb mobility for poststroke rehabilitation,” IEEE Trans. Biomed. Eng., 

vol. 63, no. 4, pp. 859–868, 2016. 

[139] A. Yurtman and B. Barshan, “Automated evaluation of physical therapy exercises using multi-template dynamic time warping on wearable 

sensor signals,” Comput. Methods Programs Biomed., vol. 117, no. 2, pp. 189–207, 2014. 

26 

 
 
[140] E. Saraee and M. Betke, “Dynamic adjustment of physical exercises based on performance using the proficio robotic arm,” in Proceedings 

of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments, 2016, p. 8. 

[141] W. Wei, Y. Lu, E. Rhoden, and S. Dey, “User performance evaluation and real-time guidance in cloud-based physical therapy monitoring 

and guidance system.,” Multimed. Tools Appl, vol. 78, no. 7, pp. 9051–9081, 2019, doi: 10.1007/s11042-017-5278-5. 

[142] S.-W. Kim, S. Park, and W. W. Chu, “An index-based approach for similarity search supporting time warping in large sequence databases,” 

in Proceedings 17th International Conference on Data Engineering, 2001, pp. 607–614. 

[143] L. Yao, H. Xu, and A. Li, “Kinect-based rehabilitation exercises system: therapist involved approach,” Biomed. Mater. Eng., vol. 24, no. 

6, pp. 2611–2618, 2014. 

[144] L. Li and G. E. Caldwell, “Coefficient of cross correlation and the time domain correspondence,” J. Electromyogr. Kinesiol., vol. 9, no. 6, 

pp. 385–389, 1999. 

[145] A. López-Méndez, J. Gall, J. R. Casas, and L. J. Van Gool, “Metric Learning from Poses for Temporal Clustering of Human Motion.,” in 

BMVC, 2012, pp. 1–12. 

[146] A.  Elkholy,  M.  E.  Hussein,  W.  Gomaa,  D.  Damen,  and  E.  Saba,  “Efficient  and  Robust  Skeleton-Based  Quality  Assessment  and 

Abnormality Detection in Human Action Performance,”  IEEE J. Biomed. Health Inform., vol. 24, no. 1, pp. 280–291, Jan. 2020, doi: 

10.1109/JBHI.2019.2904321. 

[147] W.  Wei,  C.  McElroy,  and  S.  Dey,  “Towards  On-Demand  Virtual  Physical  Therapist:  Machine  Learning-Based  Patient  Action 

Understanding,  Assessment 

and  Task  Recommendation,” 

IEEE  Trans.  Neural  Syst.  Rehabil.  Eng.,  2019,  doi: 

10.1109/TNSRE.2019.2934097. 

[148] J. F.-S. Lin and D. Kulić, “Online segmentation of human motion for automated rehabilitation exercise analysis,” IEEE Trans. Neural Syst. 

Rehabil. Eng., vol. 22, no. 1, pp. 168–180, 2014. 

[149] L. Ricci et al., “An experimental protocol for the definition of upper limb anatomical frames on children using magneto-inertial sensors,” 

in 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2013, pp. 4903–4906. 

[150] A. Valtazanos, D. K. Arvind, and S. Ramamoorthy, “Comparative study of segmentation of periodic motion data for mobile gait analysis,” 

in Wireless Health 2010, 2010, pp. 145–154. 

[151] J. Barbič, A. Safonova, J.-Y. Pan, C. Faloutsos, J. K. Hodgins, and N. S. Pollard, “Segmenting motion capture data into distinct behaviors,” 

in Proceedings of Graphics Interface 2004, 2004, pp. 185–194. 

[152] F. Zhou, F. De la Torre, and J. K. Hodgins, “Hierarchical aligned cluster analysis for temporal clustering of human motion,” IEEE Trans. 

Pattern Anal. Mach. Intell., vol. 35, no. 3, pp. 582–596, 2013. 

[153] W.-H. Lee, J. Ortiz, B. Ko, and R. Lee, “Time series segmentation through automatic feature learning,” ArXiv Prepr. ArXiv180105394, 

2018. 

[154] B.  Varadarajan,  C.  Reiley,  H.  Lin,  S.  Khudanpur,  and  G.  Hager,  “Data-derived  models  for  segmentation  with  application  to  surgical 

assessment and training,” in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2009, pp. 426–

434. 

[155] E.  Keogh,  S.  Chu,  D.  Hart,  and  M.  Pazzani,  “Segmenting  time  series:  A  survey  and  novel  approach,”  in  Data  mining  in  time  series 

databases, World Scientific, 2004, pp. 1–21. 

[156] E. Saraee et al., “ExerciseCheck: data analytics for a remote monitoring and evaluation platform for home-based physical therapy,” in 

Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, 2019, pp. 110–118. 

[157] J.  C.  P.  Pascual  et  al.,  “A  Socially  Assistive  Robotic  Platform  for  Upper-Limb  Rehabilitation:  A  Longitudinal  Study  With  Pediatric 

Patients,” IEEE Robot. Autom. Mag., vol. 26, no. 2, pp. 24–39, 2019. 

[158] Cook, Grey, Movement: Functional Movement Systems: Screening, Assessment, Corrective Strategies. Lotus Pub., 2011. 

[159] W. Hurley, C. Denegar, and J.  Hertel,  Research methods : a framework for evidence-based clinical practice. Lippincott Williams and 

Wilkins, 2011. 

27 

 
 
[160] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, “Sequential deep learning for human action recognition,” in International 

workshop on human behavior understanding, 2011, pp. 29–39. 

[161] G. Lefebvre, S. Berlemont, F. Mamalet, and C. Garcia, “BLSTM-RNN based 3D gesture classification,” in International conference on 

artificial neural networks, 2013, pp. 381–388. 

[162] W. Zhu et al., “Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks,” in Thirtieth 

AAAI Conference on Artificial Intelligence, 2016. 

[163] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “An end-to-end spatio-temporal attention model for human action recognition from skeleton 

data,” in Thirty-first AAAI conference on artificial intelligence, 2017. 

[164] Z.-A. Zhu, Y.-C. Lu, C.-H. You, and C.-K. Chiang, “Deep Learning for Sensor-Based Rehabilitation Exercise Recognition and Evaluation,” 

Sensors, vol. 19, no. 4, p. 887, Jan. 2019, doi: 10.3390/s19040887. 

[165] M.  B.  Milton,  B.  Börsbo,  G.  Rovner,  \AAsa  Lundgren-Nilsson, K.  Stibrant-Sunnerhagen,  and  B.  Gerdle,  “Is  pain  intensity  really  that 

important to assess in chronic pain patients? A study based on the Swedish Quality Registry for Pain Rehabilitation (SQRP),” PLoS One, 

vol. 8, no. 6, p. e65483, 2013. 

[166] M. S. Aung et al., “The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal EmoPain 

dataset,” IEEE Trans. Affect. Comput., vol. 7, no. 4, pp. 435–451, 2015. 

[167] F. Destelle et al., “Low-cost accurate skeleton tracking based on fusion of kinect and wearable inertial sensors,” in 2014 22nd European 

Signal Processing Conference (EUSIPCO), 2014, pp. 371–375. 

[168] C.  Chen,  R.  Jafari,  and  N.  Kehtarnavaz,  “Fusion  of  depth,  skeleton,  and  inertial  data  for  human  action  recognition,”  in  2016  IEEE 

international conference on acoustics, speech and signal processing (ICASSP), 2016, pp. 2712–2716. 

[169] B.  Delachaux,  J.  Rebetez,  A.  Perez-Uribe,  and  H.  F.  S.  Mejia,  “Indoor  activity  recognition  by  combining  one-vs.-all  neural  network 

classifiers exploiting wearable and depth sensors,” in International Work-Conference on Artificial Neural Networks, 2013, pp. 216–223. 

[170] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer vision for recognizing food preparation activities,” in 

Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, 2013, pp. 729–738. 

[171] J. Wu and J. Cheng, “Bayesian co-boosting for multi-modal gesture recognition,” J. Mach. Learn. Res., vol. 15, no. 1, pp. 3013–3036, 

2014. 

[172] C. Cao, Y. Zhang, and H. Lu, “Multi-modal learning for gesture recognition,” in 2015 IEEE International Conference on Multimedia and 

Expo (ICME), 2015, pp. 1–6. 

[173] C. Chen, R. Jafari, and N. Kehtarnavaz, “A real-time human action recognition system using depth and inertial sensor fusion,” IEEE Sens. 

J., vol. 16, no. 3, pp. 773–781, 2015. 

[174] A.  Goodney,  J.  Jung,  S.  Needham,  and  S.  Poduri,  “Dr.  droid:  Assisting  stroke  rehabilitation  using  mobile  phones,”  in  International 

Conference on Mobile Computing, Applications, and Services, 2010, pp. 231–242. 

[175] T.  Pascu,  M.  White,  and  Z.  Patoli,  “Motion  capture  and  activity  tracking  using  smartphone-driven  body  sensor  networks,”  in  Third 

International Conference on Innovative Computing Technology (INTECH 2013), 2013, pp. 456–462. 

[176] G. Spina, G. Huang, A. Vaes, M. Spruit, and O. Amft, “COPDTrainer: a smartphone-based motion rehabilitation training system with real-

time acoustic feedback,” in Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, 2013, 

pp. 597–606. 

[177] C. Ferreira, V. Guimarães, A. Santos, and I. Sousa, “Gamification of stroke rehabilitation exercises using a smartphone,” in Proceedings 

of the 8th International Conference on Pervasive Computing Technologies for Healthcare, 2014, pp. 282–285. 

[178] D. Irvine, A. Zemke, G. Pusateri, L. Gerlach, R. Chun, and W. M. Jay, “Tablet and smartphone accessibility features in the low vision 

rehabilitation,” Neuro-Ophthalmol., vol. 38, no. 2, pp. 53–59, 2014. 

[179] P.  Milani,  C.  A.  Coccetta,  A.  Rabini,  T.  Sciarra,  G.  Massazza,  and  G.  Ferriero,  “Mobile  smartphone  applications  for  body  position 

measurement in rehabilitation: a review of goniometric tools,” PM&R, vol. 6, no. 11, pp. 1038–1043, 2014. 

28 

 
 
[180] M. K. O’Brien et al., “Activity recognition for persons with stroke using mobile phone technology: toward improved performance in a 

home setting,” J. Med. Internet Res., vol. 19, no. 5, p. e184, 2017. 

[181] Y. Choi, J. Nam, D. Yang, W. Jung, H.-R. Lee, and S. H. Kim, “Effect of smartphone application-supported self-rehabilitation for frozen 

shoulder: a prospective randomized control study,” Clin. Rehabil., vol. 33, no. 4, pp. 653–660, 2019. 

[182] G. I. Parisi, S. Magg, and S. Wermter, “Human motion assessment in real time using recurrent self-organization,” in 2016 25th IEEE 

international symposium on robot and human interactive communication (RO-MAN), 2016, pp. 71–76. 

[183] K.-H. Chen, P.-C. Chen, K.-C. Liu, and C.-T. Chan, “Wearable sensor-based rehabilitation exercise assessment for knee osteoarthritis,” 

Sensors, vol. 15, no. 2, pp. 4193–4211, 2015. 

[184] Y.  Msayib,  P.  Gaydecki,  M.  Callaghan,  N.  Dale,  and  S.  Ismail,  “An  intelligent  remote  monitoring  system  for  total  knee  arthroplasty 

patients,” J. Med. Syst., vol. 41, no. 6, p. 90, 2017. 

[185] T. T. Um et al., “Data augmentation of wearable sensor data for Parkinson’s disease monitoring using convolutional neural networks,” 

ArXiv Prepr. ArXiv170600527, 2017. 

[186] T. T. Um, V. Babakeshizadeh, and D. Kulić, “Exercise motion classification from large-scale wearable sensor data using convolutional 

neural networks,” in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 2385–2390. 

[187] D. M. Burns, N. Leung, M. Hardisty, C. M. Whyne, P. Henry, and S. McLachlin, “Shoulder physiotherapy exercise recognition: machine 

learning the inertial signals from a smartwatch,” Physiol. Meas., vol. 39, no. 7, p. 075007, 2018. 

[188] S. González, J. Sedano, J. R. Villar, E. Corchado, Á. Herrero, and B. Baruque, “Features and models for human activity recognition,” 

Neurocomputing, vol. 167, pp. 52–60, 2015. 

[189] H. Coskun, D. Joseph Tan, S. Conjeti, N. Navab, and F. Tombari, “Human motion analysis with deep metric learning,” in Proceedings of 

the European Conference on Computer Vision (ECCV), 2018, pp. 667–683. 

[190] K. M. Vamsikrishna, D. P. Dogra, and M. S. Desarkar, “Computer-vision-assisted palm rehabilitation with supervised learning,”  IEEE 

Trans. Biomed. Eng., vol. 63, no. 5, pp. 991–1001, 2015. 

[191] E. Akdoğan, E. Taçgın, and M. A. Adli, “Knee rehabilitation using an intelligent robotic system,” J. Intell. Manuf., vol. 20, no. 2, p. 195, 

2009. 

[192] Q. Wang, P. Turaga, G. Coleman, and T. Ingalls, “Somatech: an exploratory interface for altering movement habits,” in Proceedings of 

the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems, 2014, pp. 1765–1770. 

[193] B. C. Bedregal, A. C. Costa, and G. P. Dimuro, “Fuzzy rule-based hand gesture recognition,” in IFIP International Conference on Artificial 

Intelligence in Theory and Practice, 2006, pp. 285–294. 

[194] I. Kramer, N. Schmidt, R. Memmesheimer, and D. Paulus, “Evaluation Of Physical Therapy Through Analysis Of Depth Images,” in 2019 

28th  IEEE  International  Conference  on  Robot  and  Human  Interactive  Communication  (RO-MAN),  2019,  pp.  1–6,  doi:  10.1109/RO-

MAN46459.2019.8956435. 

[195] M.  H.  Lee,  D.  P.  Siewiorek,  A.  Smailagic,  A.  Bernardino,  and  S.  B.  i  Badia,  “Learning  to  assess  the  quality  of  stroke  rehabilitation 

exercises,” in Proceedings of the 24th International Conference on Intelligent User Interfaces, Marina del Ray, California, 2019, pp. 218–

228, doi: 10.1145/3301275.3302273. 

[196] A. Lawrence, Modern inertial technology: navigation, guidance, and control. Springer Science & Business Media, 2012. 

29 

 
 
 
