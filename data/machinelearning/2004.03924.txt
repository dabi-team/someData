1
2
0
2

n
u
J

1
2

]

O
L
.
s
c
[

2
v
4
2
9
3
0
.
4
0
0
2
:
v
i
X
r
a

Densities of Almost Surely Terminating
Probabilistic Programs are Diﬀerentiable Almost
Everywhere

Carol Mak

, C.-H. Luke Ong

, Hugo Paquet

, and Dominik Wagner((cid:0))

Department of Computer Science, University of Oxford, Oxford, UK
{pui.mak,luke.ong,hugo.paquet,dominik.wagner}@cs.ox.ac.uk

Abstract. We study the diﬀerential properties of higher-order statisti-
cal probabilistic programs with recursion and conditioning. Our starting
point is an open problem posed by Hongseok Yang: what class of sta-
tistical probabilistic programs have densities that are diﬀerentiable al-
most everywhere? To formalise the problem, we consider Statistical PCF
(SPCF), an extension of call-by-value PCF with real numbers, and con-
structs for sampling and conditioning. We give SPCF a sampling-style
operational semantics `a la Borgstr¨om et al., and study the associated
weight (commonly referred to as the density) function and value func-
tion on the set of possible execution traces.
Our main result is that almost surely terminating SPCF programs, gen-
erated from a set of primitive functions (e.g. the set of analytic functions)
satisfying mild closure properties, have weight and value functions that
are almost everywhere diﬀerentiable. We use a stochastic form of sym-
bolic execution to reason about almost everywhere diﬀerentiability. A
by-product of this work is that almost surely terminating deterministic
(S)PCF programs with real parameters denote functions that are almost
everywhere diﬀerentiable.
Our result is of practical interest, as almost everywhere diﬀerentiability
of the density function is required to hold for the correctness of major
gradient-based inference algorithms.

1

Introduction

Probabilistic programming refers to a set of tools and techniques for the system-
atic use of programming languages in Bayesian statistical modelling. Users of
probabilistic programming — those wishing to make inferences or predictions
— (i) encode their domain knowledge in program form; (ii) condition certain
program variables based on observed data; and (iii) make a query. The resulting
code is then passed to an inference engine which performs the necessary com-
putation to answer the query, usually following a generic approximate Bayesian
inference algorithm. (In some recent systems [5,14], users may also write their
own inference code.) The Programming Language community has contributed to
the ﬁeld by developing formal methods for probabilistic programming languages
(PPLs), seen as usual languages enriched with primitives for (i) sampling and

 
 
 
 
 
 
2

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

(ii) conditioning. (The query (iii) can usually be encoded as the return value of
the program.)

It is crucial to have access to reasoning principles in this context. The com-
bination of these new primitives with the traditional constructs of programming
languages leads to a variety of new computational phenomena, and a major con-
cern is the correctness of inference: given a query, will the algorithm converge,
in some appropriate sense, to a correct answer? In a universal PPL (i.e. one
whose underlying language is Turing-complete), this is not obvious: the infer-
ence engine must account for a wide class of programs, going beyond the more
well-behaved models found in many of the current statistical applications. Thus
the design of inference algorithms, and the associated correctness proofs, are
quite delicate. It is well-known, for instance, that in its original version the pop-
ular lightweight Metropolis-Hastings algorithm [53] contained a bug aﬀecting the
result of inference [20,25].

Fortunately, research in this area beneﬁts from decades of work on the seman-
tics of programs with random features, starting with pioneering work by Kozen
[26] and Saheb-Djahromi [44]. Both operational and denotational models have
recently been applied to the validation of inference algorithms: see e.g. [20,8]
for the former and [45,10] for the latter. There are other approaches, e.g. using
reﬁned type systems [33].

Inference algorithms in probabilistic programming are often based on the
concept of program trace, because the operational behaviour of a program is
parametrised by the sequence of random numbers it draws along the way. Ac-
cordingly a probabilistic program has an associated value function which maps
traces to output values. But the inference procedure relies on another function on
traces, commonly called the density 1 of the program, which records a cumulative
likelihood for the samples in a given trace. Approximating a normalised version
of the density is the main challenge that inference algorithms aim to tackle. We
will formalise these notions: in Sec. 3 we demonstrate how the value function
and density of a program are deﬁned in terms of its operational semantics.

Contributions. The main result of this paper is that both the density and
value function are diﬀerentiable almost everywhere (that is, everywhere but on
a set of measure zero), provided the program is almost surely terminating in
a suitable sense. Our result holds for a universal language with recursion and
higher-order functions. We emphasise that it follows immediately that purely
deterministic programs with real parameters denote functions that are almost
everywhere diﬀerentiable. This class of programs is important, because they can
express machine learning models which rely on gradient descent [30].

This result is of practical interest, because many modern inference algo-
rithms are “gradient-based”: they exploit the derivative of the density function
in order to optimise the approximation process. This includes the well-known
methods of Hamiltonian Monte-Carlo [15,37] and stochastic variational infer-
ence [18,40,6,27]. But these techniques can only be applied when the derivative

1 For some readers this terminology may be ambiguous; see Remark 1 for clariﬁcation.

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

3

exists “often enough”, and thus, in the context of probabilistic programming, al-
most everywhere diﬀerentiability is often cited as a requirement for correctness
[55,31]. The question of which probabilistic programs satisfy this property was
selected by Hongseok Yang in his FSCD 2019 invited lecture [54] as one of three
open problems in the ﬁeld of semantics for probabilistic programs.

Points of non-diﬀerentiability exist largely because of branching, which typi-
cally arises in a program when the control ﬂow reaches a conditional statement.
Hence our work is a study of the connections between the traces of a probabilis-
tic program and its branching structure. To achieve this we introduce stochastic
symbolic execution, a form of operational semantics for probabilistic programs,
designed to identify sets of traces corresponding to the same control-ﬂow branch.
Roughly, a reduction sequence in this semantics corresponds to a control ﬂow
branch, and the rules additionally provide for every branch a symbolic expres-
sion of the trace density, parametrised by the outcome of the random draws that
the branch contains. We obtain our main result in conjunction with a careful
analysis of the branching structure of almost surely terminating programs.

Outline. We devote Sec. 2 to a more detailed introduction to the problem of
trace-based inference in probabilistic programming, and the issue of diﬀerentia-
bility in this context. In Sec. 3, we present a trace-based operational semantics
to Statistical PCF, a prototypical higher-order functional language previously
studied in the literature. This is followed by a discussion of diﬀerentiability and
almost sure termination of programs (Sec. 4). In Sec. 5 we deﬁne the “symbolic”
operational semantics required for the proof of our main result, which we present
in Sec. 6. We discuss related work and further directions in Sec. 7.

2 Probabilistic Programming and Trace-Based Inference

In this section we give a short introduction to probabilistic programs and the
densities they denote, and we motivate the need for gradient-based inference
methods. Our account relies on classical notions from measure theory, so we
start with a short recap.

2.1 Measures and Densities

A measurable space is a pair (X, ΣX ) consisting of a set together with a σ-
algebra of subsets, i.e. ΣX ⊆ P(X) contains ∅ and is closed under complements
and countable unions and intersections. Elements of ΣX are called measurable
sets. A measure on (X, ΣX ) is a function µ : ΣX → [0, ∞] satisfying µ(∅) = 0,
and µ(
i∈I µ(Ui) for every countable family {Ui}i∈I of pairwise
disjoint measurable subsets. A (possibly partial) function X ⇀ Y is measurable
P
if for every U ∈ ΣY we have f −1(U ) ∈ ΣX .

i∈I Ui) =

S

The space R of real numbers is an important example. The (Borel) σ-algebra
ΣR is the smallest one containing all intervals [a, b), and the Lebesgue measure

4

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

Leb is the unique measure on (R, ΣR) satisfying Leb([a, b)) = b − a. For measur-
able spaces (X, ΣX) and (Y, ΣY ), the product σ-algebra ΣX×Y is the smallest
one containing all U × V , where U ∈ ΣX and V ∈ ΣY . So in particular we get
for each n ∈ N a space (Rn, ΣRn ), and additionally there is a unique measure
Lebn on Rn satisfying Lebn(

i Leb(Ui).

i Ui) =

Q

When a function f : X → R is measurable and non-negative and µ is a
Q
measure on X, for each U ∈ ΣX we can deﬁne the integral
U (dµ)f ∈ [0, ∞].
Common families of probability distributions on the reals (Uniform, Normal,
etc.) are examples of measures on (R, ΣR). Most often these are deﬁned in terms
of probability density functions with respect to the Lebesgue measure, meaning
that for each µD there is a measurable function pdf D : R → R≥0 which deter-
U (d Leb) pdf D. As we will see, density functions such as
mines it: µD(U ) =
pdf D have a central place in Bayesian inference.

R

Formally, if µ is a measure on a measurable space X, a density for µ with
respect to another measure ν on X (most often ν is the Lebesgue measure) is a
measurable function f : X → R such that µ(U ) =
U (dν)f for every U ∈ ΣX . In
the context of the present work, an inference algorithm can be understood as a
method for approximating a distribution of which we only know the density up
to a normalising constant. In other words, if the algorithm is fed a (measurable)
function g : X → R, it should produce samples approximating the probability
measure U 7→ RU (dν)g

R

R

RX (dν)g on X.

We will make use of some basic notions from topology: given a topological
space X and an set A ⊆ X, the interior of A is the largest open set ˚A contained
in A. Dually the closure of A is the smallest closed set A containing A, and the
boundary of A is deﬁned as ∂A := A \ ˚A. Note that for all U ⊆ Rn, all of ˚U , U
and ∂U are measurable (in ΣRn).

2.2 Probabilistic Programming: a (Running) Example

Our running example is based on a random walk in R≥0.

The story is as follows: a pedestrian has gone on a walk on a certain semi-
inﬁnite street (i.e. extending inﬁnitely on one side), where she may periodically
change directions. Upon reaching the end of the street she has forgotten her
starting point, only remembering that she started no more than 3km away.
Thanks to an odometer, she knows the total distance she has walked is 1.1km,
although there is a small margin of error. Her starting point can be inferred
using probabilistic programming, via the program in Fig. 1a.

The function walk in Fig. 1a is a recursive simulation of the random walk:
note that in this model a new direction is sampled after at most 1km. Once the
pedestrian has travelled past 0 the function returns the total distance travelled.
The rest of the program ﬁrst speciﬁes a prior distribution for the starting point,
representing the pedestrian’s belief — uniform distribution on [0, 3] — before
observing the distance measured by the odometer. After drawing a value for
start the program simulates a random walk, and the execution is weighted (via
score) according to how close distance is to the observed value of 1.1. The return

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

5

(∗ r e t u r n s
l e t rec walk s t a r t =

t o t a l d i s t a n c e t r a v e l l e d ∗)

i f

( s t a r t <= 0 ) then

0
e l s e

(∗ each l e g < 1km∗)
l e t s t e p = Uniform ( 0 , 1 )
i f

( f l i p ( ) ) then

in

(∗ go t o w a r d s +i n f t y ∗)
s t e p + walk ( s t a r t+s t e p )

e l s e

(∗ go t o w a r d s 0 ∗)
s t e p + walk ( s t a r t −s t e p )

in
(∗ p r i o r ∗)
l e t s t a r t = Uniform ( 0 , 3 )
l e t d i s t a n c e = walk s t a r t
(∗ l i k e l i h o o d ∗)
s c o r e ( ( pdfN d i s t a n c e 0 . 1 ) 1 . 1 ) ;
(∗ q u e r y ∗)
s t a r t

in
in

·104

6

4

2

0

0

1

2

3

(cid:0)✁✂✄✁☎✆✝ ✞✟✠✂✁☎✟✆

(a) Running example in pseudo-code.

(b) Resulting histogram.

Fig. 1: Inferring the starting point of a random walk on R≥0, in a PPL.

value is our query: it indicates that we are interested in the posterior distribution
on the starting point.

The histogram in Fig. 1b is obtained by sampling repeatedly from the pos-
terior of a Python model of our running example. It shows the mode of the
pedestrian’s starting point to be around the 0.8km mark.

To approximate the posterior, inference engines for probabilistic programs
often proceed indirectly and operate on the space of program traces, rather than
on the space of possible return values. By trace, we mean the sequence of sam-
ples drawn in the course of a particular run, one for each random primitive
encountered. Because each random primitive (qua probability distribution) in
the language comes with a density, given a particular trace we can compute a
coeﬃcient as the appropriate product. We can then multiply this coeﬃcient by
all scores encountered in the execution, and this yields a (weight ) function, map-
ping traces to the non-negative reals, over which the chosen inference algorithm
may operate. This indirect approach is more practical, and enough to answer
the query, since every trace unambiguously induces a return value.

Remark 1. In much of the probabilistic programming literature (e.g. [31,55,54],
including this paper), the above-mentioned weight function on traces is called the
density of the probabilistic program. This may be confusing: as we have seen,
a probabilistic program induces a posterior probability distribution on return

✡
☛
☞
✌
✍
☞
✎
✏
✑
6

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

values, and it is natural to ask whether this distribution admits a probability
density function (Radon-Nikodym derivative) w.r.t. some base measure. This
problem is of current interest [2,3,21] but unrelated to the present work.

2.3 Gradient-Based Approximate Inference

Some of the most inﬂuential and practically important inference algorithms make
use of the gradient of the density functions they operate on, when these are
diﬀerentiable. Generally the use of gradient-based techniques allow for much
greater eﬃciency in inference.

A popular example is the Markov Chain Monte Carlo algorithm known as
Hamiltonian Monte Carlo (HMC) [15,37]. Given a density function g : X →
R, HMC samples are obtained as the states of a Markov chain by (approxi-
mately) simulating Hamilton’s equations via an integrator that uses the gra-
dient ∇x g(x). Another important example is (stochastic) variational inference
[18,40,6,27], which transforms the posterior inference problem to an optimisa-
tion problem. This method takes two inputs: the posterior density function of
interest g : X → R, and a function h : Θ × X → R; typically, the latter function
is a member of an expressive and mathematically well-behaved family of densi-
ties that are parameterised in Θ. The idea is to use stochastic gradient descent
to ﬁnd the parameter θ ∈ Θ that minimises the “distance” (typically the Kull-
back–Leibler divergence) between h(θ, −) and g, relying on a suitable estimate
of the gradient of the objective function. When g is the density of a probabilistic
program (the model ), h can be speciﬁed as the density of a second program (the
guide) whose traces have additional θ-parameters. The gradient of the objective
function is then estimated in one approach (score function [41]) by computing
the gradient ∇θ h(θ, x), and in another (reparameterised gradient [24,42,49]) by
computing the gradient ∇x g(x).

In probabilistic programming, the above inference methods must be adapted
to deal with the fact that in a universal PPL, the set of random primitives
encountered can vary between executions, and traces can have arbitrary and un-
bounded dimension; moreover, the density function of a probabilistic program is
generally not (everywhere) diﬀerentiable. Crucially these adapted algorithms
are only valid when the input densities are almost everywhere diﬀerentiable
[55,38,32]; this is the subject of this paper.

Our main result (Thm. 3) states that the weight function and value function
of almost surely terminating SPCF programs are almost everywhere diﬀeren-
tiable. This applies to our running example: the program in Fig. 1a (expressible
in SPCF using primitive functions that satisfy Assumption 1 – see Ex. 1) is
almost surely terminating.

3 Sampling Semantics for Statistical PCF

In this section, we present a simply-typed statistical probabilistic programming
language with recursion and its operational semantics.

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

7

σ, τ ::= R | σ ⇒ τ

M, N, L ::= y | r | f (M1, . . . , Mℓ) | λy. M | M N | YM | if

L ≤ 0, M, N
(cid:0)

(cid:1)

| sample | score(M )

Γ ⊢ sample : R

Γ ⊢ M : R
Γ ⊢ score(M ) : R

Γ ⊢ M : (σ ⇒ τ ) ⇒ (σ ⇒ τ )
Γ ⊢ YM : σ ⇒ τ

Fig. 2: Syntax of SPCF, where r ∈ R, x, y are variables, and f : Rn ⇀ R ranges
over a set F of partial, measurable primitive functions (see Sec. 4.2).

3.1 Statistical PCF

Statistical PCF (SPCF) is higher-order probabilistic programming with re-
cursion in puriﬁed form. The terms and part of the (standard) typing system of
SPCF are presented in Fig. 2 2. In the rest of the paper we write x to represent a
sequence of variables x1, . . . , xn, Λ for the set of SPCF terms, and Λ0 for the set
of closed SPCF terms. In the interest of readability, we sometimes use pseudo
code (e.g. Fig. 1a) in the style of Core ML to express SPCF terms.

SPCF is a statistical probabilistic version of call-by-value PCF [46,47] with re-
als as the ground type. The probabilistic constructs of SPCF are relatively stan-
dard (see for example [48]): the sampling construct sample draws from U(0, 1),
the standard uniform distribution with end points 0 and 1; the scoring construct
score(M ) enables conditioning on observed data by multiplying the weight of the
current execution with the (non-negative) real number denoted by M . Sampling
from other real-valued distributions can be obtained from U(0, 1) by applying
the inverse of the distribution’s cumulative distribution function.

Our SPCF is an (inconsequential) variant of CBV SPCF [51] and a (CBV)
extension of PPCF [16] with scoring; it may be viewed as a simply-typed version
of the untyped probabilistic languages of [8,13,52].

Example 1 (Running Example Ped). We express in SPCF the example in Fig. 1a.

Ped ≡

let x = sample · 3 in
let d = walk x in
let w = score(pdf N (1.1,0.1)(d)) in x


λf x. if x ≤ 0 then 0





where

walk ≡ Y



else

let s = sample in
(sample ≤ 0.5),
if

(cid:18)

s + f (x + s)

,

s + f (x − s)



(cid:0)

The let construct, let x = N in M , is syntactic sugar for the term (λx.M ) N ;
and pdf N (1.1,0.1), the density function of the normal distribution with mean 1.1
and variance 0.1, is a primitive function. To enhance readability we use inﬁx
notation and omit the underline for standard functions such as addition.
2 In Fig. 2 and in other ﬁgures, we highlight the elements that are new or otherwise

(cid:1)(cid:1)

(cid:1)

(cid:0)

(cid:0)





(cid:19)

noteworthy.

8

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

3.2 Operational Semantics

The execution of a probabilistic program generates a trace: a sequence contain-
ing the values sampled during a run. Our operational semantics captures this
dynamic perspective. This is closely related to the treatment in [8] which, follow-
ing [26], views a probabilistic program as a deterministic program parametrized
by the sequence of random draws made during the evaluation.

Traces. Recall that in our language, sample produces a random value in the
open unit interval; accordingly a trace is a ﬁnite sequence of elements of (0, 1).
n∈N(0, 1)n, equipped
We deﬁne a measure space S of traces to be the set
with the standard disjoint union σ-algebra, and the sum of the respective (higher-
dimensional) Lebesgue measures. Formally, writing Sn := (0, 1)n, we deﬁne:

S

S :=

Sn,

n∈N
[

(

n∈N
[

Un | Un ∈ ΣSn

, µS

!

)

and µS

:=

Un

!

n∈N
[

n∈N
X

Lebn(Un).

Henceforth we write traces as lists, such as [0.5, 0.999, 0.12]; the empty trace as
[]; and the concatenation of traces s, s′ ∈ S as s ++ s′.

More generally, to account for open terms, we deﬁne, for each m ∈ N, the

measure space

Rm × S :=

Rm × Sn,

n∈N
[

Vn | Vn ∈ ΣRm×Sn

, µRm×S

!

)

(

n∈N
[

where µRm×S
the subscript from µRm×S whenever it is clear from the context.

n∈N Lebm+n(Vn). To avoid clutter, we will elide

n∈N Vn

:=

(cid:16) S

(cid:17)

P

Small-Step Reduction. Next, we deﬁne the values (typically denoted V ),
redexes (typically R) and evaluation contexts (typically E):

V ::= r | λy.M

R ::= (λy. M ) V | f (r1, . . . , rℓ) | Y(λy. M ) | if

r ≤ 0, M, N

| sample | score(r)

E ::= [] | E M | (λy.M ) E | f (r1, . . . , ri−1, E, Mi+1, . . . , Mℓ) | YE

(cid:1)

(cid:0)

| if

E ≤ 0, M, N

| score(E)

(cid:0)

We write Λv for the set of SPCF values, and Λ0
values.

(cid:1)

v for the set of closed SPCF

It is easy to see that every closed SPCF term M is either a value, or there

exists a unique pair of context E and redex R such that M ≡ E[R].

We now present the operational semantics of SPCF as a rewrite system of
conﬁgurations, which are triples of the form hM, w, si where M is a closed
SPCF term, w ∈ R≥0 is a weight, and s ∈ S a trace. (We will sometimes refer to

 
 
 
Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

9

Redex Contractions:

h(λy. M ) V, w, si → hM [V /y], w, si

f (r1, . . . , rℓ), w, s
(cid:10)
f (r1, . . . , rℓ), w, s
(cid:10)

(cid:11)

(cid:11)

→ Df (r1, . . . , rℓ), w, s
→ fail

E

r ≤ 0, M, N

hY(λy.M ), w, si → hλz.M [Y(λy.M )/y] z, w, si
→ hM, w, si
→ hN, w, si

, w, s
, w, s

(cid:11)

(cid:1)

r ≤ 0, M, N

if
(cid:10)
if
(cid:10)

(cid:0)

(cid:0)

(cid:1)

(cid:11)

hsample, w, si → hr, w, s ++ [r]i
hscore(r), w, si → hr, r · w, si
hscore(r), w, si → fail

(if (r1, . . . , rℓ) ∈ dom(f ))

(if (r1, . . . , rℓ) 6∈ dom(f ))

(for fresh variable z)

(if r ≤ 0)

(if r > 0)

(for some r ∈ (0, 1))

(if r ≥ 0)

(if r < 0)

Evaluation Contexts:

hR, w, si → hR
, w
′
hE[R], w, si → hE[R

′

, s′
], w

i
, s′
′

′

hR, w, si → fail
hE[R], w, si → fail

i

Fig. 3: Operational small-step semantics of SPCF

these as the concrete conﬁgurations, in contrast with the abstract conﬁgurations
of our symbolic operational semantics, see Sec. 5.2.)

The small-step reduction relation → is deﬁned in Fig. 3. In the rule for sample,
a random value r ∈ (0, 1) is generated and recorded in the trace, while the weight
remains unchanged: in a uniform distribution on (0, 1) each value is drawn with
likelihood 1. In the rule for score(r), the current weight is multiplied by non-
negative r ∈ R: typically this reﬂects the likelihood of the current execution
given some observed data. Similarly to [8] we reduce terms which cannot be
reduced in a reasonable way (i.e. scoring with negative constants or evaluating
functions outside their domain) to fail.

Example 2. We present a possible reduction sequence for the program in Ex. 1:

hPed, 1, []i →∗

→∗

→∗

*


(cid:28)(cid:18)

let x = 0.2 · 3 in
let d = walk x in
let w = score(pdf N (1.1,0.1)(d)) in x

let d = walk 0.6 in
let w = score(pdf N (1.1,0.1)(d)) in 0.6

, 1, [0.2]

+

, 1, [0.2]

(cid:29)

(cid:19)

let w = score(pdf N (1.1,0.1)(0.9)) in 0.6, 1, [0.2, 0.9, 0.7]
E

→∗ hlet w = score(0.54) in 0.6, 1, [0.2, 0.9, 0.7]i
→∗ h0.6, 0.54, [0.2, 0.9, 0.7]i

D

(⋆)

In this execution, the initial sample yields 0.2, which is appended to the trace.
At step (⋆), we assume given a reduction sequence hwalk 0.6, 1, [0.2]i →∗

10

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

h0.9, 1, [0.2, 0.9, 0.7]i; this means that in the call to walk, 0.9 was sampled as
the the step size and 0.7 as the direction factor; this makes the new location
−0.3, which is negative, so the return value is 0.9. In the ﬁnal step, we perform
conditioning using the likelihood of observing 0.9 given the data 1.1: the score()
expression updates the current weight using the the density of 0.9 in the normal
distribution with parameters (1.1, 0.1).

Value and Weight Functions. Using the relation →, we now aim to reason
more globally about probabilistic programs in terms of the traces they produce.
Let M be an SPCF term with free variables amongst x1, . . . , xm of type R. Its
value function valueM : Rm × S → Λ0
v ∪ {⊥} returns, given values for each free
variable and a trace, the output value of the program, if the program terminates
in a value. The weight function weightM : Rm × S → R≥0 returns the ﬁnal
weight of the corresponding execution. Formally:

valueM (r, s) :=

(

if hM [r/x], 1, []i →∗ hV, w, si

V
⊥ otherwise

weightM (r, s) :=

w if hM [r/x], 1, []i →∗ hV, w, si
0

otherwise

(

For closed SPCF terms M we just write weightM (s) for weightM ([], s) (similarly
for valueM ), and it follows already from [8, Lemma 9] that the functions valueM
and weightM are measurable (see also Sec. 4.1).

Finally, every closed SPCF term M has an associated value measure

JM K : ΣΛ0

v → R≥0

deﬁned by JM K(U ) :=
−1(U) dµS weightM . This corresponds to the deno-
tational semantics of SPCF in the ω-quasi-Borel space model via computational
adequacy [51].

valueM

R

d JM K f =

v → [0, ∞],

Returning to Remark 1, what are the connections, if any, between the two
types of density of a program? To distinguish them, let’s refer to the weight func-
tion of the program, weightM , as its trace density, and the Radon-Nikodyn deriva-
tive of the program’s value-measure, dJMK
dν where ν is the reference measure of the
measurable space ΣΛ0
v , as the output density. Observe that, for any measurable
function f : Λ0
v ) dµS weightM · (f ◦ valueM ) =
−1
M (Λ0
S dµS weightM ·(f ◦valueM ) (because if s 6∈ value−1
v ) then weightM (s) = 0).
M (ΣΛ0
R
It follows that we can express any expectation w.r.t. the output density dJMK
dν as
R
an expectation w.r.t. the trace density weightM . If our aim is, instead, to gener-
ate samples from dJMK
then we can simply generate samples from weightM , and
dν
deterministically convert each sample to the space (Λ0
v ) via the value func-
tion valueM . In other words, if our intended output is just a sequence of samples,
then our inference engine does not need to concern itself with the consequences
of change of variables.

v, ΣΛ0

value

Λ0
v

R

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

11

4 Diﬀerentiability of the Weight and Value Functions

To reason about the diﬀerential properties of these functions we place ourselves in
a setting in which diﬀerentiation makes sense. We start with some preliminaries.

4.1 Background on Diﬀerentiable Functions

Basic real analysis gives a standard notion of diﬀerentiability at a point x ∈ Rn
for functions between Euclidean spaces Rn → Rm. In this context a function
f : Rn → Rm is smooth on an open U ⊆ Rn if it has derivatives of all orders
at every point of U . The theory of diﬀerential geometry (see e.g. the textbooks
[50,29,28]) abstracts away from Euclidean spaces to smooth manifolds. We recall
the formal deﬁnitions.

A topological space M is locally Euclidean at a point x ∈ M if x has
a neighbourhood U such that there is a homeomorphism φ from U onto an
open subset of Rn, for some n. The pair (U, φ : U → Rn) is called a chart (of
dimension n). We say M is locally Euclidean if it is locally Euclidean at every
point. A manifold M is a Hausdorﬀ, second countable, locally Euclidean space.
Two charts, (U, φ : U → Rn) and (V, ψ : V → Rm), are compatible if the
function ψ ◦ φ−1 : φ(U ∩ V ) → ψ(U ∩ V ) is smooth, with a smooth inverse. An
atlas on M is a family {(Uα, φα)} of pairwise compatible charts that cover M.
A smooth manifold is a manifold equipped with an atlas.

It follows from the topological invariance of dimension that charts that cover
a part of the same connected component have the same dimension. We empha-
sise that, although this might be considered slightly unusual, distinct connected
components need not have the same dimension. This is important for our pur-
poses: S is easily seen to be a smooth manifold since each connected compo-
nent Si is diﬀeomorphic to Ri. It is also straightforward to endow the set Λ
of SPCF terms with a (smooth) manifold structure. Following [8] we view Λ
, where SKm is the set of SPCF terms with exactly m
as
place-holders (a.k.a. skeleton terms) for numerals. Thus identiﬁed, we give Λ
the countable disjoint union topology of the product topology of the discrete
topology on SKm and the standard topology on Rm. Note that the connected
components of Λ have the form {M } × Rm, with M ranging over SKm, and m
over N. So in particular, the subspace Λv ⊆ Λ of values inherits the manifold
structure. We ﬁx the Borel algebra of this topology to be the σ-algebra on Λ.

SKm × Rm

m∈N

S

(cid:0)

(cid:1)

Given manifolds (M, {Uα, φα}) and (M′, {Vβ, ψβ}), a function f : M → M′
is diﬀerentiable at a point x ∈ M if there are charts (Uα, φα) about x and
(Vβ , ψβ) about f (x) such that the composite ψβ ◦ f ◦ φ−1
α restricted to the open
subset φα(f −1(Vβ ) ∩ Uα) is diﬀerentiable at φα(x).

The deﬁnitions above are useful because they allow for a uniform presen-
tation. But it is helpful to unpack the deﬁnition of diﬀerentiability in a few
instances, and we see that they boil down to the standard sense in real analysis.
Take an SPCF term M with free variables amongst x1, . . . , xm (all of type R),
and (r, s) ∈ Rm × Sn.

12

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

– The function weightM : Rm × S → R≥0 is diﬀerentiable at (r, s) ∈ Rm × Sn
just if its restriction weightM |Rm×Sn : Rm × Sn → R≥0 is diﬀerentiable at
(r, s).

– In case M is of type R, valueM : Rm × S → Λ0

v ∪ {⊥} is in essence a partial
function Rm × S ⇀ R. Precisely valueM is diﬀerentiable at (r, s) just if for
some open neighbourhood U ⊆ Rm × Sn of (r, s):
1. valueM (r′, s′) = ⊥ for all (r′, s′) ∈ U ; or
2. valueM (r′, s′) 6= ⊥ for all (r′, s′) ∈ U , and value′

M : U → R is dif-
M (r′, s′) := r′′ whenever

ferentiable at (r, s), where we deﬁne value′
valueM (r′, s′) = r′′.

4.2 Why Almost Everywhere Diﬀerentiability Can Fail

Conditional statements break diﬀerentiability. This is easy to see with an exam-
ple: the weight function of the term

if

sample ≤ sample, score(1), score(0)

(cid:1)

(cid:0)

is exactly the characteristic function of {[s1, s2] ∈ S | s1 ≤ s2}, which is not
diﬀerentiable on the diagonal {[s, s] ∈ S2 | s ∈ (0, 1)}.

This function is however diﬀerentiable almost everywhere: the diagonal is an
uncountable set but has Leb2 measure zero in the space S2. Unfortunately, this
is not true in general. Without suﬃcient restrictions, conditional statements also
break almost everywhere diﬀerentiability. This can happen for two reasons.

Problem 1: Pathological Primitive Functions. Recall that our deﬁnition
of SPCF is parametrised by a set F of primitive functions. It is tempting in this
context to take F to be the set of all diﬀerentiable functions, but this is too
general, as we show now. Consider that for every f : R → R the term

if

f (sample) ≤ 0, score(1), score(0)

(cid:0)

has weight function the characteristic function of {[s1] ∈ S | f (s1) ≤ 0}. This
function is non-diﬀerentiable at every s1 ∈ S1 ∩ ∂f −1(−∞, 0]: in every neigh-
bourhood of s1 there are s′
1 ) > 0. One can
construct a diﬀerentiable f for which this is not a measure zero set. (For exam-
ple, there exists a non-negative function f which is zero exactly on a fat Cantor
set, i.e., a Cantor-like set with strictly positive measure. See [43, Ex. 5.21].)

1 such that f (s′

1) ≤ 0 and f (s′′

1 and s′′

(cid:1)

Problem 2: Non-Terminating Runs. Our language has recursion, so we can
construct a term which samples a random number, halts if this number is in
Q ∩ [0, 1], and diverges otherwise. In pseudo-code:

l e t rec enumQ p q r =

i f

( r = p/q ) then ( s c o r e 1 ) e l s e

i f

( r < p/q ) then

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

13

enumQ p ( q+1) r

e l s e

enumQ ( p+1) q r

in enumQ 0 1 sample

The induced weight function is the characteristic function of {[s1] ∈ S | s1 ∈ Q};
the set of points at which this function is non-diﬀerentiable is S1, which has
measure 1.

We proceed to overcome Problem 1 by making appropriate assumptions on
the set of primitives. We will then address Problem 2 by focusing on almost
surely terminating programs.

4.3 Admissible Primitive Functions

One contribution of this work is to identify suﬃcient conditions for F . We will
show in Sec. 6 that our main result holds provided:

Assumption 1 (Admissible Primitive Functions). F is a set of partial, mea-
surable functions Rℓ ⇀ R including all constant and projection functions which
satisﬁes

1. if f : Rℓ ⇀ R and gi : Rm ⇀ R are elements of F for i = 1, . . . , ℓ, then

f ◦ hgiiℓ

i=1 : Rm ⇀ R is in F

2. if (f : Rℓ ⇀ R) ∈ F , then f is diﬀerentiable in the interior of dom(f )
3. if (f : Rℓ ⇀ R) ∈ F , then Lebℓ(∂f −1[0, ∞)) = 0.

Example 3. The following sets of primitive operations satisfy the above suﬃcient
conditions. (See Appendix A.1 for a proof.)

1. The set F1 of analytic functions with co-domain R. Recall that a function
f : Rℓ → Rn is analytic if it is inﬁnitely diﬀerentiable and its multivari-
ate Taylor expansion at every point x0 ∈ Rℓ converges pointwise to f in a
neighbourhood of x0.

2. The set F2 of (partial) functions f : Rℓ ⇀ R such that dom(f ) is open3,
and f is diﬀerentiable everywhere in dom(f ), and f −1(I) is a ﬁnite union of
(possibly unbounded) rectangles4 for (possibly unbounded) intervals I.

Note that all primitive functions mentioned in our examples (and in partic-

ular the density of the normal distribution) are included in both F1 and F2.

It is worth noting that both F1 and F2 satisfy the following stronger (than
Assumption 1.3) property: Lebn(∂f −1I) = 0 for every interval I, for every prim-
itive function f .

3 This requirement is crucial, and cannot be relaxed.
4 i.e. a ﬁnite union of I1 × · · · × Iℓ for (possibly unbounded) intervals Ii

14

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

4.4 Almost Sure Termination

To rule out the contrived counterexamples which diverge we restrict attention to
almost surely terminating SPCF terms. Intuitively, a program M (closed term
of ground type) is almost surely terminating if the probability that a run of M
terminates is 1.

Take an SPCF term M with variables amongst x1, . . . , xm (all of type R),

and set

TM,term :=

(r, s) ∈ Rm × S | ∃V, w . hM [r/x], 1, []i →∗ hV, w, si

.

(1)

(cid:8)

Let us ﬁrst consider the case of closed M ∈ Λ0 i.e. m = 0 (notice that the measure
µRm×S is not ﬁnite, for m ≥ 1). As TM,term now coincides with value−1
v),
TM,term is a measurable subset of S. Plainly if M is deterministic (i.e. sample-
free), then µS(TM,term) = 1 if M converges to a value, and 0 otherwise. Generally
for an arbitrary (stochastic) term M we can regard µS(TM,term) as the probability
that a run of M converges to a value, because of Lem. 1.

(cid:9)
M (Λ0

Lemma 1. If M ∈ Λ0 then µS(TM,term) ≤ 1.

More generally, if M has free variables amongst x1, . . . , xm (all of type R),
then we say that M is almost surely terminating if for almost every (instantiation
of the free variables by) r ∈ Rm, M [r/x] terminates with probability 1.
We formalise the notion of almost sure termination as follows.

Deﬁnition 1. Let M be an SPCF term. We say that M terminates almost
surely if
1. M is closed and µ(TM,term) = µ(value−1
2. M has free variables amongst x1, . . . , xm (all of which are of type R), and
there exists T ∈ ΣRm such that Lebm(Rm \ T ) = 0 and for each r ∈ T ,
M [r/x] terminates almost surely.

v)) = 1; or

M (Λ0

Suppose that M is a closed term and M ♭ is obtained from M by recursively
, where Nfail is a term
→∗

replacing subterms score(L) with the term if
that reduces to fail such as 1/0. It is easy to see that for all s ∈ S,
hV, 1, si iﬀ for some (unique) w ∈ R≥0, hM, 1, []i →∗ hV, w, si. Therefore,

L < 0, Nfail, L

M ♭, 1, []

(cid:0)

(cid:1)

(cid:10)

(cid:11)

JM ♭K(Λv) =

dµS weightM ♭

Zvalue

−1
M ♭ (Λv)
= µS({s ∈ S | ∃V .

M ♭, 1, []
E
D

→∗ hV, 1, si}) = µS(TM,term)

Consequently, the closed term M terminates almost surely iﬀ JM ♭K is a proba-
bility measure.

Remark 2. – Like many treatments of semantics of probabilistic programs in
the literature, we make no distinction between non-terminating runs and
aborted runs of a (closed) term M : both could result in the value semantics
JM ♭K being a sub-probabilty measure (cf. [4]).

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

15

– Even so, current probabilistic programming systems do not place any restric-
tions on the code that users can write: it is perfectly possible to construct
invalid models because catching programs that do not deﬁne valid proba-
bility distributions can be hard, or even impossible. This is not surprising,
because almost sure termination is hard to decide: it is Π 0
2 -complete in the
arithmetic hierarchy [22]. Nevertheless, because a.s. termination is an im-
portant correctness property of probabilistic programs (not least because of
the main result of this paper, Thm. 3), the development of methods to prove
a.s. termination is a hot research topic.

Accordingly the main theorem of this paper is stated as follows:

Theorem 3. Let M be an SPCF term (possibly with free variables of type R)
which terminates almost surely. Then its weight function weightM and value
function valueM are diﬀerentiable almost everywhere.

5 Stochastic Symbolic Execution

We have seen that a source of discontinuity is the use of if-statements. Our main
result therefore relies on an in-depth understanding of the branching behaviour
of programs. The operational semantics given in Sec. 3 is unsatisfactory in this
respect: any two execution paths are treated independently, whether they go
through diﬀerent branches of an if-statement or one is obtained from the other
by using slightly perturbed random samples not aﬀecting the control ﬂow.

More concretely, note that although we have derived weightPed[0.2, 0.9, 0.7] =
0.54 and valuePed[0.2, 0.9, 0.7] = 0.6 in Ex. 2, we cannot infer anything about
weightPed[0.21, 0.91, 0.71] and valuePed[0.21, 0.91, 0.71] unless we perform the cor-
responding reduction.

So we propose an alternative symbolic operational semantics (similar to the
“compilation scheme” in [55]), in which no sampling is performed: whenever a
sample command is encountered, we simply substitute a fresh variable αi for
it, and continue on with the execution. We can view this style of semantics
as a stochastic form of symbolic execution [12,23], i.e., a means of analysing a
program so as to determine what inputs, and random draws (from sample) cause
each part of a program to execute.

Consider the term M ≡ let x = sample · 3 in (walk x), deﬁned using the func-

tion walk of Ex. 1. We have a reduction path

M ⇒ let (x = α1 · 3) in (walk x) ⇒ walk (α1 · 3)

but at this point we are stuck: the CBV strategy requires a value for α1. We will
“delay” the evaluation of the multiplication α1 · 3; we signal this by drawing a
box around the delayed operation: α1 · 3. We continue the execution, inspecting
the deﬁnition of walk, and get:

M ⇒∗ walk (α1 · 3) ⇒∗ N ≡ if

α1 · 3 ≤ 0, 0, P

(cid:0)

(cid:1)

16

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

where

P ≡

let s = sample in
(sample ≤ 0.5),
if

(cid:18)

s + walk(α1 · 3 + s)

,

s + walk(α1 · 3 − s)

.

(cid:19)

(cid:0)

(cid:0)

We are stuck again: the value of α1 is needed in order to know which branch
to follow. Our approach consists in considering the space S1 = (0, 1) of possible
values for α1, and splitting it into {s1 ∈ (0, 1) | s1 · 3 ≤ 0} = ∅ and {s1 ∈ (0, 1) |
s1 · 3 > 0} = (0, 1). Each of the two branches will then yield a weight function
restricted to the appropriate subspace.

(cid:1)(cid:1)

(cid:0)

(cid:1)

Formally, our symbolic operational semantics is a rewrite system of conﬁgura-
tions of the form ⟪M , w , U ⟫, where M is a term with delayed (boxed) operations,
and free “sampling” variables5 α1, . . . , αn; U ⊆ Sn is the subspace of sampling
values compatible with the current branch; and w : U → R≥0 is a function
assigning to each s ∈ U a weight w (s). In particular, for our running example6

⟪M, λ[]. 1, S0⟫ ⇒∗ ⟪N, λ[s1]. 1, (0, 1)⟫.

As explained above, this leads to two branches:

⟪N, λ[s1]. 1, (0, 1)⟫

⇒ ∗

⇒∗

⟪0, λ[s1]. 1, ∅⟫

⟪P, λ[s1]. 1, (0, 1)⟫

The ﬁrst branch has reached a value, and the reader can check that the second
branch continues as

⟪P, λ[s1]. 1, (0, 1)⟫ ⇒

∗

⟪if

α3 ≤ 0.5, α2 + walk(α1 · 3 + α2), α2 + walk(α1 · 3 − α2)
(cid:0)

, λ[s1, s2, s3]. 1, (0, 1)3⟫
(cid:1)

where α2 and α3 stand for the two sample statements in P . From here we proceed
by splitting (0, 1)3 into (0, 1) × (0, 1) × (0, 0.5] and (0, 1) × (0, 1) × (0.5, 1) and
after having branched again (on whether we have passed 0) the evaluation of
walk can terminate in the conﬁguration

⟪α2 + 0, λ[s1, s2, s3]. 1, U ⟫

where U := {[s1, s2, s3] ∈ S3 | s3 > 0.5 ∧ s1 · 3 − s2 ≤ 0}.

Recall that M appears in the context of our running example Ped. Using our

calculations above we derive one of its branches:
⟪Ped, λ[]. 1, {[]}⟫ ⇒∗ ⟪let w = score(pdf N (1.1,0.1)(α2)) in α1 · 3, λ[s1, s2, s3]. 1, U⟫

⇒ ⟪let w = score( pdf N (1.1,0.1) (α2)) in α1 · 3, λ[s1, s2, s3]. 1, U ⟫
⇒∗ ⟪let w = pdf N (1.1,0.1) (α2) in α1 · 3, λ[s1, s2, s3]. pdf N (1.1,0.1)(s2), U ⟫
⇒∗ ⟪α1 · 3, λ[s1, s2, s3]. pdf N (1.1,0.1)(s2), U ⟫

5 Note that M may be open and contain other free “non-sampling” variables, usually

denoted x1, . . . , xm.

6 We use the meta-lambda-abstraction λx. f (x) to denote the set-theoretic function

x 7→ f (x).

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

17

In particular the trace [0.2, 0.9, 0.7] of Ex. 2 lies in the subspace U . We can imme-
diately read oﬀ the corresponding value and weight functions for all [s1, s2, s3] ∈
U simply by evaluating the computation α1 · 3, which we have delayed until now:

valuePed[s1, s2, s3] = s1 · 3

weightPed[s1, s2, s3] = pdf N (1.1,0.1)(s2)

5.1 Symbolic Terms and Values

We have just described informally our symbolic execution approach, which in-
volves delaying the evaluation of primitive operations. We make this formal by
introducing an extended notion of terms, which we call symbolic terms and
deﬁne in Fig. 4a along with a notion of symbolic values. For this we assume
ﬁxed denumerable sequences of distinguished variables: α1, α2, . . ., used to rep-
resent sampling, and x1, x2, . . . used for free variables of type R. Symbolic terms
are typically denoted M , N , or L. They contain terms of the form f (V1, . . . , Vℓ)
for f : Rℓ ⇀ R ∈ F a primitive function, representing delayed evaluations, and
they also contain the sampling variables αj. The type system is adapted in a
straightforward way, see Fig. 4b.

We use Λ(m,n) to refer to the set of well-typed symbolic terms with free
variables amongst x1, . . . , xm and α1, . . . , αn (and all are of type R). Note that
every term in the sense of Fig. 2 is also a symbolic term.

Each symbolic term M ∈ Λ(m,n) has a corresponding set of regular terms,
accounting for all possible values for its sampling variables α1, . . . , αn and its
(other) free variables x1, . . . , xm. For r ∈ Rm and s ∈ Sn, we call partially eval-
uated instantiation of M the term ⌊M ⌋ (r, s) obtained from M [r/x, s/α] by
recursively “evaluating” subterms of the form f (r1, . . . , rℓ) to f (r1, . . . , rℓ), pro-
vided (r1, . . . , rℓ) ∈ dom(f ). In this operation, subterms of the form f (r1, . . . , rℓ)
are left unchanged, and so are any other redexes. ⌊M ⌋ can be viewed as a
partial function ⌊M ⌋ : Rm × Sn ⇀ Λ and a formal deﬁnition is presented in
Fig. 5b. (To be completely rigorous, we deﬁne for ﬁxed m and n, partial func-
tions ⌊M ⌋m,n : Rm ×Sn ⇀ Λ for symbolic terms M whose distinguished variables
are amongst x1, . . . , xm and α1, . . . , αn. M may contain other variables y, z, . . .
of any type. Since m and n are usually clear from the context, we omit them.)
Observe that for M ∈ Λ(m,n) and (r, s) ∈ dom ⌊M ⌋, ⌊M ⌋ (r, s) is a closed term.
Example 4. Consider M ≡ (λz. α1 · 3) (score(pdf N (1.1,0.1)(α2))). Then, for r =
[] and s = [0.2, 0.9, 0.7], we have ⌊M ⌋ (r, s) = (λz. 0.6) (score(pdf N (1.1,0.1)(0.9))).

More generally, observe that if Γ ⊢ M : σ and (r, s) ∈ dom ⌊M ⌋ then Γ ⊢
⌊M ⌋ (r, s) : σ. In order to evaluate conditionals if
we need to
reduce L to a real constant, i.e., we need to have ⌊L⌋ (r, s) = r for some r ∈ R.
This is the case whenever L is a symbolic value of type R, since these are built
only out of delayed operations, real constants and distinguished variables xi or
αj. Indeed we can show the following:

L ≤ 0, M , N

(cid:1)

(cid:0)

Lemma 2. Let (r, s) ∈ dom ⌊M ⌋. Then M is a symbolic value iﬀ ⌊M ⌋ (r, s) is
a value.

18

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

V ::= r | xi | αj

| f (V1, . . . , Vℓ) | λy. M

M , N , L ::= V | y | f (M1, . . . , Mℓ) | M N | YM | if

L ≤ 0, M , N
(cid:0)

(cid:1)

| sample | score(M )

(a) Symbolic values (typically V ) and symbolic terms (typically M , N or L)

Γ ⊢ V1 : R · · · Γ ⊢ Vℓ : R
Γ ⊢ f (V1, . . . , Vℓ) : R

Γ ⊢ xi : R

Γ ⊢ αj : R

Γ, y : σ ⊢ y : σ

Γ ⊢ r : R

r ∈ R

Γ ⊢ M1 : R · · · Γ ⊢ Mℓ : R
Γ ⊢ f (M1, . . . , Mℓ) : R

Γ, y : σ ⊢ M : τ
Γ ⊢ λy. M : σ → τ

Γ ⊢ M : σ → τ Γ ⊢ N : σ
Γ ⊢ M N : τ

Γ ⊢ M : (σ ⇒ τ ) ⇒ σ ⇒ τ
Γ ⊢ YM : σ ⇒ τ

Γ ⊢ L : R Γ ⊢ M : σΓ ⊢ N : σ
L ≤ 0, M , N
(cid:0)

Γ ⊢ if

: σ

(cid:1)

Γ ⊢ sample : R

Γ ⊢ M : R
Γ ⊢ score(M ) : R

(b) Type system for symbolic terms

R ::= (λy. M ) V | f ( V1, . . . , Vℓ ) | Y(λy. M ) | if

V ≤ 0, M , N

(cid:1)

(cid:0)

| sample | score( V )

E ::= [ ] | E M | (λy. M ) E | f ( V1, . . . , Vi−1 , E, Mi+1, . . . , Mℓ) | YE |

if

E ≤ 0, M , N
(cid:0)

(cid:1)

| score(E)

(c) Symbolic values (typically V ), redexes (R ) and reduction contexts (E).

Fig. 4: Symbolic terms and values, type system, reduction contexts, and redexes.
As usual f ∈ F and r ∈ R.

For symbolic values V : R and (r, s) ∈ dom ⌊V ⌋ we employ the notation

kV k (r, s) := r′ provided that ⌊V ⌋ (r, s) = r′.

A simple induction on symbolic terms and values yields the following prop-

erty, which is crucial for the proof of our main result (Thm. 3):

Lemma 3. Suppose the set F of primitives satisﬁes Item 1 of Assumption 1.

1. For each symbolic value V of type R, by identifying dom kV k with a subset

of Rm+n, we have kV k ∈ F .

2. If F also satisﬁes item 2 of Assumption 1 then for each symbolic term M ,

⌊M ⌋ : Rm × Sn ⇀ Λ is diﬀerentiable in the interior of its domain.

5.2 Symbolic Operational Semantics

We aim to develop a symbolic operational semantics that provides a sound and
complete abstraction of the (concrete) operational trace semantics. The symbolic

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

19

dom

(cid:4)

f (V1, . . . , Vℓ)

(cid:5)

:= {(r, s) ∈ dom ⌊V1⌋ ∩ · · · ∩ dom ⌊Vℓ⌋ | (r

where r

′
′
ℓ) ∈ dom(f ),
1, . . . , r
′
′
ℓ = ⌊Vℓ⌋ (r, s)}
1 = ⌊V1⌋ (r, s), · · · , r
′
:= Rm × Sn

r
(cid:4)

(cid:5)

dom ⌊sample⌋ := dom ⌊xi⌋ := dom ⌊αj ⌋ := dom ⌊y⌋ := dom

domf (M1, . . . , Mℓ) := dom ⌊M1⌋ ∩ · · · ∩ dom ⌊Mℓ⌋

dom ⌊λy. M ⌋ := dom ⌊YM ⌋ := dom ⌊score(M )⌋ := dom ⌊M ⌋
dom ⌊M N ⌋ := dom ⌊M ⌋ ∩ dom ⌊N ⌋

dom

if

L ≤ 0, M , N
(cid:0)

(cid:4)

(cid:1)(cid:5)

:= dom ⌊L⌋ ∩ dom ⌊M ⌋ ∩ dom ⌊N ⌋

(a) Domain of ⌊·⌋

f (V1, . . . , Vℓ)

(cid:5)

(r, s) := f (r

′
1, . . . , r

′
ℓ) , where for 1 ≤ i ≤ ℓ, ⌊Vi⌋ (r, s) = r

′
i

⌊xi⌋ (r, s) := ri
⌊αj ⌋ (r, s) := sj
⌊y⌋ (r, s) := y
′
(r, s) := r
r
(cid:4)

(cid:5)

′

f (M1, . . . , Mℓ)

(cid:5)

(r, s) := f (⌊M1⌋ (r, s), . . . , ⌊Mℓ⌋ (r, s))

(cid:4)

(cid:4)

⌊λy. M ⌋ (r, s) := λy. ⌊M ⌋ (r, s)
⌊M N ⌋ (r, s) := (⌊M ⌋ (r, s)) (⌊N ⌋ (r, s))
⌊YM ⌋ (r, s) := Y(⌊M ⌋ (r, s))

if
(cid:4)

(cid:0)

L ≤ 0, M , N

(r, s) := if

⌊L⌋ (r, s) ≤ 0, ⌊M ⌋ (r, s), ⌊N ⌋ (r, s)

(cid:1)(cid:5)

(cid:0)

⌊sample⌋ (r, s) := sample

(cid:1)

⌊score(M )⌋ (r, s) := score(⌊M ⌋ (r, s))

(b) Deﬁnition of ⌊·⌋ on dom ⌊·⌋

Fig. 5: Formal deﬁnition of the instantiation and partial evaluation function ⌊·⌋

semantics is presented as a rewrite system of symbolic conﬁgurations, which
are deﬁned to be triples of the form ⟪M , w , U ⟫, where for some m and n, M ∈
Λ(m,n), U ⊆ dom ⌊M ⌋ ⊆ Rm × Sn is measurable, and w : Rm × S ⇀ R≥0
with dom(w ) = U . Thus we aim to prove the following result (writing 1 for the
constant function λ(r, s). 1):

Theorem 1. Let M be a term with free variables amongst x1, . . . , xm.

1. (Soundness). If ⟪M, 1, Rm⟫ ⇒∗ ⟪V , w , U ⟫ then for all (r, s) ∈ U it holds

weightM (r, s) = w (r, s) and valueM (r, s) = ⌊V ⌋ (r, s).

2. (Completeness). If r ∈ Rm and hM [r/x], 1, []i →∗ hV, w, si then there exists

⟪M, 1, Rm⟫ ⇒∗ ⟪V , w , U ⟫ such that (r, s) ∈ U .

20

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

As formalised by Thm. 1, the key intuition behind symbolic conﬁgurations
⟪M , w , U ⟫ (that are reachable from a given ⟪M, 1, Rm⟫) is that, whenever M is
a symbolic value:

– M gives a correct local view of valueM (restricted to U ), and
– w gives a correct local view of weightM (restricted to U );

moreover, the respective third components U (of the symbolic conﬁgurations
⟪M , w , U ⟫) cover TM,term.

To establish Thm. 1, we introduce symbolic reduction contexts and sym-
bolic redexes. These are presented in Fig. 4c and extend the usual notions
(replacing real constants with arbitrary symbolic values of type R).

Using Lem. 2 we obtain:

Lemma 4. If R is a symbolic redex and (r, s) ∈ dom ⌊R ⌋ then ⌊R ⌋ (r, s) is a
redex.

The following can be proven by a straightforward induction(see Appendix A.3):

Lemma 5 (Subject Construction). Let M be a symbolic term.

1. If M is a symbolic value then for all symbolic contexts E and symbolic redexes

R , M 6≡ E[R ].

2. If M ≡ E1[R1] ≡ E2[R2] then E1 ≡ E2 and R1 ≡ R2.
3. If M is not a symbolic value and dom ⌊M ⌋ 6= ∅ then there exist E and R

such that M ≡ E[R ].

The partial instantiation function also extends to symbolic contexts E in the

evident way – we give the full deﬁnition in Appendix A.3 (Def. 2).

Now, we introduce the following rules for symbolic redex contractions:

⟪(λy. M ) V , w , U ⟫ ⇒ ⟪M [V /y], w , U ⟫

⟪f (V1, . . . , Vℓ), w , U ⟫ ⇒ ⟪ f (V1, . . . , Vℓ), w , dom

f (V1, . . . , Vℓ)

∩ U ⟫

⟪Y(λy. M ), w , U ⟫ ⇒ ⟪λz. M [Y(λy. M )/y] z, w , U ⟫

(cid:13)
(cid:13)

(cid:13)
(cid:13)

⟪if

V ≤ 0, M , N

, w , U ⟫ ⇒ ⟪M , w , kV k−1 (−∞, 0] ∩ U ⟫

⟪if

(cid:0)
V ≤ 0, M , N

, w , U ⟫ ⇒ ⟪N , w , kV k−1 (0, ∞) ∩ U ⟫
(cid:1)

(cid:0)

⟪sample, w , U ⟫ ⇒ ⟪ αn+1 , w ′, U ′⟫

(cid:1)

(U ⊆ Rm × Sn)

⟪score(V ), w , U ⟫ ⇒ ⟪V , kV k · w , kV k−1 [0, ∞) ∩ U ⟫

In the rule for sample, U ′ := {(r, s ++ [s′]) | (r, s) ∈ U ∧ s′ ∈ (0, 1)} and w ′(r, s ++
[s′]) := w (r, s); in the rule for score(V ), (kV k · w )(r, s) := kV k (r, s) · w (r, s).

The rules are designed to closely mirror their concrete counterparts. Cru-
cially, the rule for sample introduces a “fresh” sampling variable, and the two
rules for conditionals split the last component U ⊆ Rm ×Sn according to whether

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

21

kV k (r, s) ≤ 0 or kV k (r, s) > 0. The “delay” contraction (second rule) is intro-
duced for a technical reason: ultimately, to enable item 1 (Soundness). Otherwise
it is, for example, unclear whether λy. α1 + 1 should correspond to λy. 0.5 + 1 or
λy. 1.5 for s1 = 0.5.

Finally we lift this to arbitrary symbolic terms using the obvious rule for

symbolic evaluation contexts:

⟪R , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫
⟪E[R ], w , U ⟫ ⇒ ⟪E[R ′], w ′, U ′⟫

Note that we do not need rules corresponding to reductions to fail because
the third component of the symbolic conﬁgurations “ﬁlters out” the pairs (r, s)
corresponding to undeﬁned behaviour. In particular, the following holds:

Lemma 6. Suppose ⟪M , w , U ⟫ is a symbolic conﬁguration and ⟪M , w , U ⟫ ⇒
⟪N , w ′, U ′⟫. Then ⟪N , w ′, U ′⟫ is a symbolic conﬁguration.

A key advantage of the symbolic execution is that the induced computation
tree is ﬁnitely branching, since branching only arises from conditionals, splitting
the trace space into disjoint subsets. This contrasts with the concrete situation
(from Sec. 3), in which sampling creates uncountably many branches.

Lemma 7 (Basic Properties). Let ⟪M , w , U ⟫ be a symbolic conﬁguration.
Then

1. There are at most countably distinct such U ′ that ⟪M , w , U ⟫ ⇒∗ ⟪N , w ′, U ′⟫.
2. If ⟪M , w , U ⟫ ⇒∗ ⟪Vi, wi, Ui⟫ for i ∈ {1, 2} then U1 = U2 or U1 ∩ U2 = ∅.
3. If ⟪M , w , U ⟫ ⇒∗ ⟪Ei[sample], wi, Ui⟫ for i ∈ {1, 2} then U1 = U2 or U1 ∩

U2 = ∅.

Crucially, there is a correspondence between the concrete and symbolic se-

mantics in that they can “simulate” each other:

Proposition 1 (Correspondence).
ﬁguration, and (r, s) ∈ U . Let M ≡ ⌊M ⌋ (r, s) and w := w (r, s). Then
1. If ⟪M , w , U ⟫ ⇒ ⟪N , w ′, U ′⟫ and (r, s ++ s′) ∈ U ′ then

Suppose ⟪M , w , U ⟫ is a symbolic con-

hM, w, si → h⌊N ⌋ (r, s ++ s′

), w (r, s′

), s ++ s′

i .

2. If hM, w, si → hN, w′, s′i then there exists ⟪M , w , U ⟫ ⇒ ⟪N , w ′, U ′⟫ such

that ⌊N ⌋ (r, s′) ≡ N , w ′(r, s′) = w′ and (r, s′) ∈ U ′.

As a consequence of Lem. 2, we obtain a proof of Thm. 1.

6 Densities of Almost Surely Terminating Programs are

Diﬀerentiable Almost Everywhere

So far we have seen that the symbolic execution semantics provides a sound and
complete way to reason about the weight and value functions. In this section we

22

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

impose further restrictions on the primitive operations and the terms to obtain
results about the diﬀerentiability of these functions.

Henceforth we assume Assumption 1 and we ﬁx a term M with free variables

amongst x1, . . . , xm.

From Lem. 3 we immediately obtain the following:

Lemma 8. Let ⟪M , w , U ⟫ be a symbolic conﬁguration such that w is diﬀeren-
tiable on ˚U and µ(∂U ) = 0. If ⟪M , w , U ⟫ ⇒ ⟪M ′, w ′, U ′⟫ then w ′ is diﬀeren-
tiable on ˚U ′ and µ(∂U ′) = 0.

6.1 Diﬀerentiability on Terminating Traces

As an immediate consequence of the preceding, Lem. 3 and the Soundness (item 1
of Thm. 1), whenever ⟪M, 1, Rm⟫ ⇒∗ ⟪V , w , U ⟫ then weightM and valueM are
diﬀerentiable everywhere in ˚U .

Recall the set TM,term of (r, s) ∈ Rm ×S from Eq. (1) for which M terminates.

We abbreviate TM,term to Tterm and deﬁne

Tterm := TM,term = {(r, s) ∈ Rm × S | ∃V, w . hM [r/x], 1, []i →∗ hV, w, si}
Tint

{˚U | ∃V , w . ⟪M, 1, Rm⟫ ⇒∗ ⟪V , w , U ⟫}

term :=

[

{U | ∃V , w . ⟪M, 1, Rm⟫ ⇒∗
By Completeness (item 2 of Thm. 1), Tterm =
⟪V , w , U ⟫}. Therefore, being countable unions of measurable sets (Lemmas 6
and 7), Tterm and Tint

term are measurable.

S

By what we have said above, weightM and valueM are diﬀerentiable every-

where on Tint

term. Observe that in general, Tint

term ( ˚Tterm. However,

µ

Tterm \ Tint
term

= µ

(cid:0)

(cid:18)

(cid:1)

[U:⟪M,1,Rm

⟪V ,w ,U⟫

⟫⇒∗

U \ ˚U

≤

(cid:0)

(cid:19)
(cid:1)

µ(∂U ) = 0 (2)

⟫⇒∗

XU:⟪M,1,Rm

⟪V ,w ,U⟫

The ﬁrst equation holds because the U -indexed union is of pairwise disjoint sets.
The inequality is due to (U \ ˚U ) ⊆ ∂U . The last equation above holds because
each µ(∂U ) = 0 (Assumption 1 and Lem. 8).

Thus we conclude:

Theorem 2. Let M be an SPCF term. Then its weight function weightM and
value function valueM are diﬀerentiable for almost all terminating traces.

6.2 Diﬀerentiability for Almost Surely Terminating Terms

Next, we would like to extend this insight for almost surely terminating terms to
suitable subsets of Rm × S, the union of which constitutes almost the entirety of
Rm×S. Therefore, it is worth examining consequences of almost sure termination
(see Def. 1).

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

23

Tpref

Tmax

Tint

term

T

Tint
pref

Tint

stuck

Tstuck

Fig. 6: Illustration of how Rm × S – visualised as the entire rectangle – is par-
titioned to prove Thm. 3. The value function returns ⊥ in the red dotted area
and a closed value elsewhere (i.e. in the blue shaded area).

We say that (r, s) ∈ Rm × S is maximal (for M ) if hM [r/x], 1, []i →∗
hN, w, si and for all s′ ∈ S \ {[]} and N ′, hN, w, si 6→∗ hN ′, w′, s ++ s′i. Intu-
itively, s contains a maximal number of samples to reduce M [r/x]. Let Tmax be
the set of maximal (r, s).

Note that Tterm ⊆ Tmax and there are terms for which the inclusion is strict
(e.g. for the diverging term M ≡ Y(λf. f ), [] ∈ Tmax but [] 6∈ Tterm). Besides,
Tmax is measurable because, thanks to Prop. 1, for every n ∈ N,

{(r, s) ∈ Rm × Sn | hM [r/x], 1, []i →∗ hN, w, si} =

U ∩ (Rm × Sn)

[U:⟪M,1,Rm

⟪N ,w ,U⟫

⟫⇒∗

and the RHS is a countable union of measurable sets (Lemmas 6 and 7).

The following is a consequnce of the deﬁnition of almost sure termination

and a corollary of Fubini’s theorem (see Appendix A.4 for details):

Lemma 9. If M terminates almost surely then µ(Tmax \ Tterm) = 0.

Now, observe that for all (r, s) ∈ Rm × S, exactly one of the following holds:

1. (r, s) is maximal
2. for a proper preﬁx s′ of s, (r, s′) is maximal
3. (r, s) is stuck, because s does not contain enough randomness.
Formally, we say (r, s) is stuck if hM [r/x], 1, []i →∗ hE[sample], w, si, and we
let Tstuck be the set of all (r, s) which get stuck. Thus,

Rm × S = Tmax ∪ Tpref ∪ Tstuck
where Tpref := {(r, s ++ s′) | (r, s) ∈ Tmax ∧ s′ 6= []}, and the union is disjoint.

Deﬁning Tint

stuck :=

{˚U | ⟪M, 1, Rm⟫ ⇒∗ ⟪E[sample], w , U ⟫} we can argue

analogously to Eq. (2) that µ(Tstuck \ Tint

stuck) = 0.

Moreover, for Tint

pref := {(r, s ++ s′) | (r, s) ∈ Tint

term and [] 6= s′ ∈ S} it holds

S

Tpref \ Tint

pref =

(r, s ++ s′

) | (r, s) ∈ Tmax \ Tint

term ∧ s′

∈ Sn

n∈N
[

(cid:8)

(cid:9)

24

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

and hence, µ(Tpref \ Tint
Finally, we deﬁne

pref) ≤

n∈N µ(Tmax \ Tint

term) ≤ 0.

P
T := Tint

term ∪ Tint

pref ∪ Tint

stuck

Clearly, this is an open set and the situation is illustrated in Fig. 6. By what we
have seen,

µ ((Rm × S) \ T) = µ(Tterm \ Tint

term) + µ(Tint

pref \ Tpref ) + µ(Tstuck \ Tint

stuck) = 0

Moreover, to conclude the proof of our main result Thm. 3 it suﬃces to note:
1. weightM and valueM are diﬀerentiable everywhere on Tint

term (as for Thm. 2),

and

2. weightM (r, s) = 0 and valueM (r, s) = ⊥ for (r, s) ∈ Tint

pref ∪ Tint

stuck.

Theorem 3. Let M be an SPCF term (possibly with free variables of type R)
which terminates almost surely. Then its weight function weightM and value
function valueM are diﬀerentiable almost everywhere.

We remark that almost sure termination was not used in our development
until the proof of Lem. 9. For Thm. 3 we could have instead directly assumed the
conclusion of Lem. 9; that is, almost all maximal traces are terminating. This
is a strictly weaker condition than almost sure termination. The exposition we
give is more appropriate: almost sure termination is a standard notion, and the
development of methods to prove almost sure termination is a subject of active
research.

We also note that the technique used in this paper to establish almost ev-
erywhere diﬀerentiability could be used to target another “almost everywhere”
property instead: one can simply remove the requirement that elements of F are
diﬀerentiable, and replace it with the desired property. A basic example of this
is smoothness.

7 Conclusion

We have solved an open problem in the theory of probabilistic programming.
This is mathematically interesting, and motivated the development of stochastic
symbolic execution, a more informative form of operational semantics in this
context. The result is also of major practical interest, since almost everywhere
diﬀerentiability is necessary for correct gradient-based inference.

Related Work. This problem was partially addressed in the work of Zhou et
al. [55] who prove a restricted form of our theorem for recursion-free ﬁrst-order
programs with analytic primitives. Our stochastic symbolic execution is related
to their compilation scheme, which we extend to a more general language.

The idea of considering the possible control paths through a probabilistic
programs is fairly natural and not new to this paper; it has been used towards

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

25

the design of specialised inference algorithms for probabilistic programming, see
[11,56]. To our knowledge, this is the ﬁrst semantic formalisation of the concept,
and the ﬁrst time it is used to reason about whole-program density.

The notions of weight function and value function in this paper are inspired
by the more standard trace-based operational semantics of Borgstr¨om et al. [8]
(see also [52,31]).

Mazza and Pagani [35] study the correctness of automatic diﬀerentiation
(AD) of purely deterministic programs. This problem is orthogonal to the work
reported here, but it is interesting to combine their result with ours. Speciﬁcally,
we show a.e. diﬀerentiability whilst [35] proves a.s. correctness of AD on the
diﬀerentiable domain. Combining both results one concludes that for a deter-
ministic program, AD returns a correct gradient a.s. on the entire domain. Going
deeper into the comparison, Mazza and Pagani propose a notion of admissible
primitive function strikingly similar to ours: given continuity, their condition 2
and our condition 3 are equivalent. On the other hand we require admissible
functions to be diﬀerentiable, when they are merely continuous in [35]. Finally,
we conjecture that “stable points”, a central notion in [35], have a clear counter-
part within our framework: for a symbolic evaluation path arriving at ⟪V , w, U ⟫,
for V a symbolic value, the points of ˚U are precisely the stable points.

Our work is also connected to recent developments in diﬀerentiable program-
ming. Lee et al. [30] study the family of piecewise functions under analytic parti-
tion, or just “PAP” functions. PAP functions are a well-behaved family of almost
everywhere diﬀerentiable functions, which can be used to reason about automatic
diﬀerentiation in recursion-free ﬁrst-order programs. An interesting question is
whether this can be extended to a more general language, and whether densities
of almost surely terminating SPCF programs are PAP functions. (See also [19,9]
for work on diﬀerentiable programs without conditionals.)

A similar class of functions is also introduced by Bolte and Pauwels [7] in very
recent work; this is used to prove a convergence result for stochastic gradient
descent in deep learning. Whether this class of functions can be used to reason
about probabilistic program densities remains to be explored.

Finally we note that open logical relations [1] are a convenient proof technique
for establishing properties of programs which hold at ﬁrst order, such as almost
everywhere diﬀerentiability. This approach remains to be investigated in this
context, as the connection with probabilistic densities is not immediate.

Further Directions. This investigation would beneﬁt from a denotational
treatment; this is not currently possible as existing models of probabilistic pro-
gramming do not account for diﬀerentiability.

In another direction, it is likely that we can generalise the main result by
extending SPCF with recursive types, as in [51], and, more speculatively, ﬁrst-
class diﬀerential operators as in [17]. It would also be useful to add to SPCF a
family of discrete distributions, and more generally continuous-discrete mixtures,
which have practical applications [36].

26

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

Our work will have interesting implications in the correctness of various
gradient-based inference algorithms, such as the recent discontinuous HMC [39]
and reparameterisation gradient for non-diﬀerentiable models [32]. But given the
lack of guarantees of correctness properties available until now, these algorithms
have not yet been developed in full generality, leaving many perspectives open.

Acknowledgements. We thank Wonyeol Lee for spotting an error in an example.
We gratefully acknowledge support from EPSRC and the Royal Society.

References

1. Gilles Barthe, Rapha¨elle Crubill´e, Ugo Dal Lago, and Francesco Gavazzo. On the
In European Symposium on Programming,

versatility of open logical relations.
pages 56–83. Springer, 2020.

2. Sooraj Bhat, Ashish Agarwal, Richard W. Vuduc, and Alexander G. Gray. A type
theory for probability density functions. In John Field and Michael Hicks, editors,
Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January
22-28, 2012, pages 545–556. ACM, 2012.

3. Sooraj Bhat, Johannes Borgstr¨om, Andrew D. Gordon, and Claudio V. Russo. De-
riving probability density functions from probabilistic functional programs. Logical
Methods in Computer Science, 13(2), 2017.

4. Benjamin Bichsel, Timon Gehr, and Martin T. Vechev. Fine-grained semantics
for probabilistic programs. In Amal Ahmed, editor, Programming Languages and
Systems - 27th European Symposium on Programming, ESOP 2018, Held as Part
of the European Joint Conferences on Theory and Practice of Software, ETAPS
2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings, volume 10801 of Lec-
ture Notes in Computer Science, pages 145–185. Springer, 2018.

5. Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Prad-
han, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D.
Goodman. Pyro: Deep universal probabilistic programming. J. Mach. Learn. Res.,
20:28:1–28:6, 2019.

6. David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational

inference:
Journal of the American statistical Association,

A review for statisticians.
112(518):859–877, 2017.

7. J´erˆome Bolte and Edouard Pauwels. A mathematical model for automatic diﬀer-
entiation in machine learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neu-
ral Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
8. Johannes Borgstr¨om, Ugo Dal Lago, Andrew D. Gordon, and Marcin Szymczak. A
lambda-calculus foundation for universal probabilistic programming. In Proceed-
ings of the 21st ACM SIGPLAN International Conference on Functional Program-
ming, ICFP 2016, Nara, Japan, September 18-22, 2016, pages 33–46, 2016.

9. Alo¨ıs Brunel, Damiano Mazza, and Michele Pagani. Backpropagation in the sim-
ply typed lambda-calculus with linear negation. Proc. ACM Program. Lang.,
4(POPL):64:1–64:27, 2020.

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

27

10. Simon Castellan and Hugo Paquet. Probabilistic programming inference via in-
In European Symposium on Programming, pages 322–349.

tensional semantics.
Springer, 2019.

11. Arun Chaganty, Aditya Nori, and Sriram Rajamani. Eﬃciently sampling prob-
abilistic programs via program analysis. In Artiﬁcial Intelligence and Statistics,
pages 153–160, 2013.

12. Lori A. Clarke. A system to generate test data and symbolically execute programs.

IEEE Trans. Software Eng., 2(3):215–222, 1976.

13. Ryan Culpepper and Andrew Cobb. Contextual equivalence for probabilistic pro-
grams with continuous random variables and scoring. In Hongseok Yang, editor,
Programming Languages and Systems - 26th European Symposium on Program-
ming, ESOP 2017, Held as Part of the European Joint Conferences on Theory and
Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-29, 2017, Proceed-
ings, volume 10201 of Lecture Notes in Computer Science, pages 368–392. Springer,
2017.

14. Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K.
Mansinghka. Gen: a general-purpose probabilistic programming system with pro-
grammable inference. In Kathryn S. McKinley and Kathleen Fisher, editors, Pro-
ceedings of the 40th ACM SIGPLAN Conference on Programming Language De-
sign and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pages
221–236. ACM, 2019.

15. S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid monte carlo.

Physics letters B, 1987.

16. Thomas Ehrhard, Michele Pagani, and Christine Tasson. Measurable cones and
stable, measurable functions: a model for probabilistic higher-order programming.
PACMPL, 2(POPL):59:1–59:28, 2018.

17. Thomas Ehrhard and Laurent Regnier. The diﬀerential lambda-calculus. Theor.

Comput. Sci., 309(1-3):1–41, 2003.

18. Matthew D. Hoﬀman, David M. Blei, Chong Wang, and John W. Paisley. Stochas-

tic variational inference. J. Mach. Learn. Res., 14(1):1303–1347, 2013.

19. Mathieu Huot, Sam Staton, and Matthijs V´ak´ar. Correctness of automatic diﬀeren-
tiation via diﬀeologies and categorical gluing. In Jean Goubault-Larrecq and Bar-
bara K¨onig, editors, Foundations of Software Science and Computation Structures
- 23rd International Conference, FOSSACS 2020, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ire-
land, April 25-30, 2020, Proceedings, volume 12077 of Lecture Notes in Computer
Science, pages 319–338. Springer, 2020.

20. Chung-Kil Hur, Aditya V Nori, Sriram K Rajamani, and Selva Samuel. A provably
correct sampler for probabilistic programs. In 35th IARCS Annual Conference on
Foundations of Software Technology and Theoretical Computer Science (FSTTCS
2015). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.

21. Wazim Mohammed Ismail and Chung-chieh Shan. Deriving a probability density
In Jacques Garrigue, Gabriele Keller, and Eijiro
calculator (functional pearl).
Sumii, editors, Proceedings of the 21st ACM SIGPLAN International Conference
on Functional Programming, ICFP 2016, Nara, Japan, September 18-22, 2016,
pages 47–59. ACM, 2016.

22. Benjamin Lucien Kaminski, Joost-Pieter Katoen, and Christoph Matheja. On the
hardness of analyzing probabilistic programs. Acta Inf., 56(3):255–285, 2019.

23. James C. King.

Symbolic execution and program testing. Commun. ACM,

19(7):385–394, 1976.

28

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

24. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua
Bengio and Yann LeCun, editors, 2nd International Conference on Learning Rep-
resentations, ICLR 2014, Banﬀ, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014.

25. Oleg Kiselyov. Problems of the Lightweight Implementation of Probabilistic Pro-

gramming. In PPS Workshop, 2016.

26. Dexter Kozen. Semantics of probabilistic programs. In 20th Annual Symposium
on Foundations of Computer Science, San Juan, Puerto Rico, 29-31 October 1979,
pages 101–114, 1979.

27. Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Auto-
matic variational inference in stan. In Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada, pages 568–576, 2015.

28. Jeﬀrey M. Lee. Manifolds and Diﬀerential Geometry, volume 107 of Graduate

Studies in Mathematics. AMS, 2009.

29. John M. Lee. An introduction to smooth manifolds, volume 218 of Graduate Texts

in Mathematics. Springer, second edition, 2013.

30. Wonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. On correctness
of automatic diﬀerentiation for non-diﬀerentiable functions. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020.

31. Wonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. Towards ver-
PACMPL,

inference for probabilistic programs.

iﬁed stochastic variational
4(POPL):16:1–16:33, 2020.

32. Wonyeol Lee, Hangyeol Yu, and Hongseok Yang. Reparameterization gradient for
non-diﬀerentiable models. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in
Neural Information Processing Systems 31: Annual Conference on Neural Infor-
mation Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr´eal,
Canada, pages 5558–5568, 2018.

33. Alexander K Lew, Marco F Cusumano-Towner, Benjamin Sherman, Michael
Carbin, and Vikash K Mansinghka. Trace types and denotational semantics for
sound programmable inference in probabilistic languages. Proceedings of the ACM
on Programming Languages, 4(POPL):1–32, 2019.

34. Carol Mak, C.-H. Luke Ong, Hugo Paquet, and Dominik Wagner. Densities of
almost-surely terminating probabilistic programs are diﬀerentiable almost every-
where. CoRR, abs/2004.03924, 2020.

35. Damiano Mazza and Michele Pagani. Automatic diﬀerentiation in PCF. Proc.

ACM Program. Lang., 5(POPL), January 2021.

36. Praveen Narayanan and Chung-chieh Shan. Symbolic disintegration with a variety
of base measures. ACM Transactions on Programming Languages and Systems
(TOPLAS), 42(2):1–60, 2020.

37. Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain

Monte Carlo, page 113, 2011.

38. Akihiko Nishimura, David B Dunson, and Jianfeng Lu. Discontinuous hamiltonian
monte carlo for discrete parameters and discontinuous likelihoods. Biometrika,
107(2):365–380, Mar 2020.

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

29

39. Akihiko Nishimura, David B Dunson, and Jianfeng Lu. Discontinuous Hamiltonian
Monte Carlo for discrete parameters and discontinuous likelihoods. Biometrika, 03
2020. asz083.

40. Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational in-
ference. In Proceedings of the Seventeenth International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25, 2014,
pages 814–822, 2014.

41. Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational in-
In Proceedings of the Seventeenth International Conference on Artiﬁ-
ference.
cial Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25,
2014, volume 33 of JMLR Workshop and Conference Proceedings, pages 814–822.
JMLR.org, 2014.

42. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back-
propagation and approximate inference in deep generative models. In Proceedings
of the 31th International Conference on Machine Learning, ICML 2014, Beijing,
China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceed-
ings, pages 1278–1286. JMLR.org, 2014.

43. Walter Rudin. Principles of Mathematical Analysis. International Series in Pure
and Applied Mathematics. McGraw-Hill Education, 3rd edition edition, 1976.
44. Nasser Saheb-Djahromi. Probabilistic lcf. In International Symposium on Mathe-

matical Foundations of Computer Science, pages 442–451. Springer, 1978.

45. Adam ´Scibior, Ohad Kammar, Matthijs V´ak´ar, Sam Staton, Hongseok Yang, Yufei
Cai, Klaus Ostermann, Sean K Moss, Chris Heunen, and Zoubin Ghahramani.
Denotational validation of higher-order bayesian inference. Proceedings of the ACM
on Programming Languages, 2(POPL):60, 2017.

46. Dana S. Scott. A type-theoretical alternative to ISWIM, CUCH, OWHY. Theor.

Comput. Sci., 121(1&2):411–440, 1993.

47. Kurt Sieber. Relating full abstraction results for diﬀerent programming languages.
In Foundations of Software Technology and Theoretical Computer Science, Tenth
Conference, Bangalore, India, December 17-19, 1990, Proceedings, pages 373–387,
1990.

48. Sam Staton. Commutative semantics for probabilistic programming. In Hongseok
Yang, editor, Programming Languages and Systems - 26th European Symposium
on Programming, ESOP 2017, Held as Part of the European Joint Conferences
on Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-
29, 2017, Proceedings, volume 10201 of Lecture Notes in Computer Science, pages
855–879. Springer, 2017.

49. Michalis K. Titsias and Miguel L´azaro-Gredilla. Doubly stochastic variational
bayes for non-conjugate inference. In Proceedings of the 31th International Con-
ference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014,
volume 32 of JMLR Workshop and Conference Proceedings, pages 1971–1979.
JMLR.org, 2014.

50. Loring W. Tu. An introduction to manifolds. Universitext. Springer-Verlag, 2011.
51. Matthijs V´ak´ar, Ohad Kammar, and Sam Staton. A domain theory for statistical

probabilistic programming. PACMPL, 3(POPL):36:1–36:29, 2019.

52. Mitchell Wand, Ryan Culpepper, Theophilos Giannakopoulos, and Andrew Cobb.
Contextual equivalence for a probabilistic language with continuous random vari-
ables and recursion. PACMPL, 2(ICFP):87:1–87:30, 2018.

53. David Wingate, Andreas Stuhlm¨uller, and Noah D. Goodman. Lightweight imple-
mentations of probabilistic programming languages via transformational compila-

30

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

tion. In Geoﬀrey J. Gordon, David B. Dunson, and Miroslav Dud´ık, editors, Pro-
ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, volume 15 of
JMLR Proceedings, pages 770–778. JMLR.org, 2011.

54. Hongseok Yang. Some semantic issues in probabilistic programming languages
(invited talk). In Herman Geuvers, editor, 4th International Conference on For-
mal Structures for Computation and Deduction, FSCD 2019, June 24-30, 2019,
Dortmund, Germany, volume 131 of LIPIcs, pages 4:1–4:6. Schloss Dagstuhl -
Leibniz-Zentrum f¨ur Informatik, 2019.

55. Yuan Zhou, Bradley J. Gram-Hansen, Tobias Kohn, Tom Rainforth, Hongseok
Yang, and Frank Wood. LF-PPL: A low-level ﬁrst order probabilistic program-
ming language for non-diﬀerentiable models. In Kamalika Chaudhuri and Masashi
Sugiyama, editors, The 22nd International Conference on Artiﬁcial Intelligence
and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, vol-
ume 89 of Proceedings of Machine Learning Research, pages 148–157. PMLR, 2019.
56. Yuan Zhou, Hongseok Yang, Yee Whye Teh, and Tom Rainforth. Divide, conquer,
and combine: a new inference strategy for probabilistic programs with stochastic
support. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine
Learning Research, pages 11534–11545. PMLR, 2020.

Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.

The images or other third party material in this chapter are included in the chapter’s
Creative Commons license, unless indicated otherwise in a credit line to the material. If
material is not included in the chapter’s Creative Commons license and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will need
to obtain permission directly from the copyright holder.

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

31

A Supplementary Materials

A.1 More Details on Ex. 3

We prove that F1 and F2 of Ex. 3 satisfy Assumption 1.

1. Clearly, all constant and projection functions are analytic. Since analytic
functions are total and diﬀerentiable (hence continuous) functions, they are
Borel-measurable. Therefore, due to the fact that analytic functions are
closed under pairs and composition [?, Prop. 2.4], it remains to check whether
the boundary of f −1([0, ∞)) has measure zero.
˚f −1(A) and f −1(A) ⊆ f −1(A) for any subset A ⊆ R, we
Since f −1( ˚A) ⊆
have ∂f −1(A) ⊆ f −1(A) \ f −1( ˚A) = f −1(∂A). Letting A = [0, ∞), we have
∂f −1([0, ∞)) ⊆ f −1({0}). Applying the well-known result [?] that the zero
set of all analytic functions, except the zero function, has measure zero we
conclude that ∂f −1([0, ∞)) has measure zero. It is easy to see that if f is
the zero function, ∂f −1([0, ∞)) = ∂Rn = ∅ has measure zero.

2. Clearly all constant functions and projections are in F2.

Note that the set of ﬁnite unions of (possibly unbounded) rectangles forms
an algebra A (i.e. a collection of subsets of Rn closed under complements
and ﬁnite unions, hence ﬁnite intersections). Then dom(f ) = f −1(−∞, 0] ∪
f −1(0, ∞) ∈ A. Besides, for every U ∈ A, Leb(∂U ) = 0 (because Leb(∂R) =
0 for every rectangle R).
It remains to prove that F2 is closed under composition. Suppose that
f : Rℓ ⇀ R ∈ F2 and g1, . . . , gℓ : Rm ⇀ R ∈ F2. It is straightforward to
see that dom(f ◦ hgiiℓ
i=1) remains open (note that ◦ is relational compo-
sition). Moreover, for every x in dom(f ◦ hgiiℓ
i=1(x) is an interior
point in dom(f ) which is open. It follows that the (standard) chain rule is
applicable, and we have f ◦ hgiiℓ
i=1 is diﬀerentiable at x. Besides, suppose
I is a (possibly unbounded) interval. By assumption there are m ∈ N and
(potentially unbounded) intervals Ii,j , where 1 ≤ i ≤ m and 1 ≤ j ≤ ℓ such
that f −1(I) =

m
i=1 Ii,1 × · · · × Ii,ℓ. Observe that

i=1), hgiiℓ

(cid:1)

r ∈ dom(g1) ∩ · · · ∩ dom(gℓ) | (g1(r), . . . , gℓ(r)) ∈ f −1(I)
m
(cid:8)

{r ∈ dom(g1) ∩ · · · ∩ dom(gℓ) | g1(r) ∈ Ii,1 ∧ · · · ∧ gℓ(r) ∈ Ii,ℓ}

(cid:9)

S

−1

(I)

f ◦ hgiiℓ
=
(cid:0)

i=1

=

=

i=1
[
m

i=1
[

g−1
1 (Ii,1) ∩ · · · ∩ g−1

ℓ (Ii,ℓ)

and this is in A because algebras are closed under ﬁnite unions and inter-
sections.

32

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

A.2 Supplementary Materials for Sec. 5.1

Lemma 2. Let (r, s) ∈ dom ⌊M ⌋. Then M is a symbolic value iﬀ ⌊M ⌋ (r, s) is
a value.

Proof sketch. First, suppose M is a symbolic value V . It is easy to prove in-
ductively that for a sybolic values V of type R, ⌊V ⌋ (r, s) is a real constant.
Otherwise both V and ⌊V ⌋ (r, s) are abstractions.

Conversely, suppose ⌊V ⌋ (r, s) is a value. If it is an abstraction then so is V .
Otherwise it is a real constant. By the deﬁnition of ⌊·⌋ and a case inspection of
M this is only possible if M is a symbolic value.

The following substitution property holds for symbolic terms M and N :

dom ⌊M [N /y]⌋ ⊆ dom ⌊M ⌋ ∩ dom ⌊N ⌋

⌊M ⌋ (r, s)[⌊N ⌋ (r, s)/y] ≡ ⌊M [N /y]⌋ (r, s)

(3)

(4)

Lemma 3. Suppose the set F of primitives satisﬁes Item 1 of Assumption 1.

1. For each symbolic value V of type R, by identifying dom kV k with a subset

of Rm+n, we have kV k ∈ F .

2. If F also satisﬁes item 2 of Assumption 1 then for each symbolic term M ,

⌊M ⌋ : Rm × Sn ⇀ Λ is diﬀerentiable in the interior of its domain.

Proof. 1. We prove the ﬁrst part by induction on symbolic values.

– For r′ ∈ R, kr′k is a constant function and kxik and kαj k are projections,

which are in F by assumption.

– Next, suppose V is a symbolic value f (V1, . . . , Vℓ). By the inductive hy-
(r, s) =
pothesis, each kVik ∈ F . It suﬃces to note that
f (kV1k (r, s), . . . , kVℓk (r, s)) for (r, s) ∈ dom f (V1, . . . , Vℓ). Therefore,
because F is assumed to be closed under composition.
– Finally, note that we do not need to consider abstractions because they

f (V1, . . . , Vℓ)

f (V1, . . . , Vℓ)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
do not have type R.
(cid:13)

(cid:13)
(cid:13)

2. – Note that ⌊xi⌋ and ⌊αj⌋ are projection functions and ⌊r⌋ are constant

functions, which are (everywhere) diﬀerentiable functions.
– Besides, f (V1, . . . , Vℓ) is a symbolic value and on its domain,

f (V1, . . . , Vℓ)

= λ(r, s).

f (V1, . . . , Vℓ)

(r, s)

(cid:4)

(cid:5)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

is in F by the ﬁrst part and by assumption this implies

f (V1, . . . , Vℓ)
diﬀerentiability.
(cid:13)
(cid:13)
– The function ⌊λy.M ⌋ is obtained by composing ⌊M ⌋ with the function
(cid:13)
(cid:13)
Λ → Λ : L 7→ λy.L. The latter is easily seen to be diﬀerentiable: recall
M {M } × Rn, where M ranges over skeleton terms
that Λ =
with n place-holders. On each component {M } × Rn the function acts as
(M, x) 7→ (λy.M, x); it is simply one of the coproduct injections, hence
diﬀerentiable.

n∈N

S

S

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

33

– The cases of ⌊YM ⌋ and ⌊score(M )⌋ are analogous.
– The function ⌊M N ⌋ is obtained by composing ⌊M ⌋ × ⌊N ⌋ with the

diagonal map (r, s) 7→ ((r, s), (r, s)); both are diﬀerentiable.

– The cases of

and
diagonal maps of diﬀerent arities.

f (M1, . . . , Mℓ)

if

L ≤ 0, M , N

are similar, using

– The function ⌊sample⌋ is a constant function, so it is diﬀerentiable. This

(cid:5)

(cid:4)

(cid:0)

(cid:1)(cid:5)

(cid:4)

covers all cases.

A.3 Supplementary Materials for Sec. 5.2

Lemma 5 (Subject Construction). Let M be a symbolic term.

1. If M is a symbolic value then for all symbolic contexts E and symbolic redexes

R , M 6≡ E[R ].

2. If M ≡ E1[R1] ≡ E2[R2] then E1 ≡ E2 and R1 ≡ R2.
3. If M is not a symbolic value and dom ⌊M ⌋ 6= ∅ then there exist E and R

such that M ≡ E[R ].

Proof. We prove all parts of the lemma simultaneously by structural induction
on M .

– First, note that for every E and R all of the following holds

xi 6≡ E[R ]

αj 6≡ E[R ]

y 6≡ E[R ]

r 6≡ E[R ]

λy. M 6≡ E[R ]

and the left hand sides are symbolic values.

– Note that dom

f (M1, . . . , Mℓ)

= ∅ unless it is a symbolic value. Besides,

for every E and R , f (M1, . . . , Mℓ) 6≡ E[R ].
– If M ≡ N1 N2 then M is not a symbolic value.

(cid:4)

(cid:5)

Suppose that N1 is an abstraction. If N2 is a symbolic value then M is a
symbolic redex and by the ﬁrst part of the inductive hypothesis, M ≡ E[R ]
implies E ≡ [] and R ≡ M .
If M2 is not a symbolic value then M is not a symbolic redex. Note that M ≡
E[R ] implies E ≡ N1 E ′. By the second part of the inductive hypothesis E ′
and R are unique if they exist. Besides, due to dom ⌊M ⌋ ⊆ dom ⌊N2⌋ and the
third part of the inductive hypothesis, such E ′ and R exist if dom ⌊M ⌋ 6= ∅.
If N1 is not an abstraction it cannot be a symbolic value and M ≡ E[R ]
implies E ≡ E ′ N2. By the second part of the inductive hypothesis, E ′ and R
are unique if they exist. Besides, because of dom ⌊M ⌋ ⊆ dom ⌊M1⌋ and the
third part of the inductive hypothesis, such E ′ and R exists if dom ⌊M ⌋ 6= ∅.

– Next, suppose M ≡ f (N1, . . . , Nℓ), which is clearly not a symbolic value.

If all Ni are symbolic values, M is a symbolic redex and by the ﬁrst part of
the inductive hypothesis, E ≡ [] and R ≡ M are unique such that M ≡ E[R ].
Otherwise, suppose i is minimal such that Ni is not a symbolic value. Clearly,
M ≡ E[R ] implies E ≡ f (N1, . . . , Ni−1, E ′, Ni+1, . . . , Nℓ) and Ni ≡ E ′[R ]. By
the second part of the inductive hypothesis E ′ and R are unique if they exist.
Besides due to dom ⌊M ⌋ ⊆ dom ⌊Ni⌋ and the third part of the inductive
hypothesis such E ′ and R exist if dom ⌊M ⌋ 6= ∅.

34

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

– If M ≡ YN , M ≡ sample or M ≡ score(N ), which are not symbolic values,

then this is obvious (using the inductive hypothesis).

– Finally, suppose M ≡ (if

L ≤ 0, N1, N2

) ≡ E1[R1] ≡ E2[R2]. If L is a
symbolic value then M is a symbolic redex and by the ﬁrst part of the
inductive hypothesis, M ≡ E[R ] implies E ≡ [] and R ≡ M .
E ′ ≤ 0, N1, N2
If L is not a symbolic value then M ≡ E[R ] implies E ≡ if
and L ≡ E ′[R ]. By the second part of the inductive E ′ and R are unique if
(cid:1)
(cid:0)
they exist. Due to dom ⌊M ⌋ ⊆ dom ⌊L⌋ and the third part of the inductive
hypothesis such E ′ and R exist provided that dom ⌊M ⌋ 6= ∅.

(cid:0)

(cid:1)

We obtain that for all E, M and (r, s) ∈ dom ⌊E[M ]⌋:

⌊E⌋ (r, s) [⌊M ⌋ (r, s)] ≡ ⌊E[M ]⌋ (r, s)

(5)

Lemma 6. Suppose ⟪M , w , U ⟫ is a symbolic conﬁguration and ⟪M , w , U ⟫ ⇒
⟪N , w ′, U ′⟫. Then ⟪N , w ′, U ′⟫ is a symbolic conﬁguration.

Proof. Suppose that M ≡ E[R ] and N ≡ E[R ′]. Because of Lem. 3 and the
assumption that the functions in F are measurable, U ′ is measurable again.
Furthermore, the rules ensure that U ′ ⊆ dom ⌊R ′⌋. (For the ﬁrst rule this
is because of the Substitution Eq. (3).) By the Substitution Eq. (5), U ′ ⊆
dom ⌊E[R ]⌋ ∩ dom ⌊R ′⌋ ⊆ dom ⌊E⌋ ∩ dom ⌊R ′⌋ = dom ⌊E[R ′]⌋.

Lemma 7 (Basic Properties). Let ⟪M , w , U ⟫ be a symbolic conﬁguration.
Then

1. There are at most countably distinct such U ′ that ⟪M , w , U ⟫ ⇒∗ ⟪N , w ′, U ′⟫.
2. If ⟪M , w , U ⟫ ⇒∗ ⟪Vi, wi, Ui⟫ for i ∈ {1, 2} then U1 = U2 or U1 ∩ U2 = ∅.
3. If ⟪M , w , U ⟫ ⇒∗ ⟪Ei[sample], wi, Ui⟫ for i ∈ {1, 2} then U1 = U2 or U1 ∩

U2 = ∅.

Proof sketch. By subject construction (Lem. 5), there is at most one E and
R such that M ≡ E[R ]. An inspection of the rules shows that U ′ such that
⟪R , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫ is unique unless R is a conditional, in which case
there are two distinct such U ′. Hence there are at most two distinct U ′ such that
⟪E[R ], w , U ⟫ ⇒ ⟪N , w ′, U ′⟫. The ﬁrst part follows by induction on the number
of reduction steps.

For the other two parts note that if ⟪M , w , U ⟫ ⇒ ⟪N , w ′, U ′⟫ either U ′ ⊆ U
or U ′ = {(r, s ++ [r′]) | (r, s) ∈ U ∧ r′ ∈ (0, 1)}. In particular, if ⟪M , w , U ⟫ ⇒∗
⟪N , w ′, U ′⟫, U ′ ⊆ {(r, s ++ s′) | (r, s) ∈ U ∧ s′ ∈ Sn} for some n ∈ N.

By the discussion for the ﬁrst part of the lemma, if hM , w , U i ⇒∗ ⟪Ni, wi, Ui⟫

for i ∈ {1, 2}, then w.l.o.g., either

1. ⟪N1, w1, U1⟫ ⇒∗ ⟪N2, w2, U2⟫ or

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

35

2. ⟪M , w , U ⟫ ⇒∗ ⟪E[if

L ≤ 0, M1, M2

], w ′, U ′⟫ and

(cid:0)

⟪E[if

L ≤ 0, M1, M2

], w ′, U ′⟫

(cid:0)

(cid:1)

(cid:1)

⇒ ∗

⟪E[M1], w ′, U ′ ∩ kLk−1 (−∞, 0]⟫
⇒∗ ⟪N1, w1, U1⟫

⇒∗

⟪E[M2], w ′, U ′ ∩ kLk−1 (0, ∞)⟫
⇒∗ ⟪N2, w2, U2⟫

for suitable N , E, L, M1, M2, w ′ and U ′.

In the latter case in particular U1 ∩ U2 = ∅ holds.

This implies the second and third part of the lemma.

Lemma 10. Suppose ⟪R , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫, (r, s) ∈ U and (r, s++s′) ∈ U ′.
Then h⌊R ⌋ (r, s), w (r, s), si → h⌊R ′⌋ (r, s ++ s′), w ′(r, s ++ s′), s ++ s′i.

Proof. We prove the lemma by case analysis on the symbolic redex contractions.
– First, suppose ⟪sample, w , U ⟫ ⇒ ⟪αn+1, w ′, U ′⟫. Note that s′ = [r′] for some
0 < r′ < 1. Then hsample, w (r, s), si → hr′, w (r, s), s ++ [r′]i and w (r, s) =
w ′(r, s ++ s′) and ⌊αn+1⌋ (r, s ++ s′) ≡ r′.

– Suppose ⟪score(V ), w , U ⟫ ⇒ ⟪V , w · kV k , U ∩ kV k−1 [0, ∞)⟫. Then (r, s) =
(r, s ++ s′) ∈ U ∩ kV k−1 [0, ∞). Hence, there must exist r′ ≥ 0 such that
⌊V ⌋ (r, s) ≡ ⌊V ⌋ (r, s ++ s′) ≡ r′. Besides, hscore(⌊V ⌋ (r, s)), w (r, s), si →
hr′, w (r, s) · r′, si and (w · kV k)(r, s ++ s′) = w (r, s) · r′.

– Suppose ⟪if

, w , U ⟫ ⇒ ⟪M , w , U ∩ kV k−1 (−∞, 0]⟫. Note that
(r, s) = (r, s ++ s′) ∈ U ∩ kV k−1 (−∞, 0]. Thus, kV k (r, s) ≤ 0. Therefore,

V ≤ 0, M , N

(cid:0)

(cid:1)

if

kV k (r, s) ≤ 0, ⌊M ⌋ (r, s), ⌊N ⌋ (r, s)

, w (r, s), s

D
→ h⌊M ⌋ (r, s), w (r, s), si

(cid:0)

(cid:1)

E

– Similar for the else-branch.
– Suppose ⟪(λy. M ) V , w , U ⟫ → ⟪M [V /y], w , U ⟫. Then (r, s) = (r, s ++ s′) ∈

U . By Lem. 2, ⌊V ⌋ (r, s) is a value. Hence,

h(λy. ⌊M ⌋ (r, s)) ⌊V ⌋ (r, s), w (r, s), si
→ h(⌊M ⌋ (r, s))[⌊V ⌋ (r, s)/y], w (r, s), si .

Besides, by Eq. (3), (⌊M ⌋ (r, s))[⌊V ⌋ (r, s)/y] ≡ ⌊M [V /y]⌋ (r, s ++ s′).

– Suppose ⟪f (V1, . . . , Vℓ), w , U ⟫ ⇒ ⟪ f (V1, . . . , Vℓ), w , U ∩dom

f (V1, . . . , Vℓ)
Then (r, s) = (r, s ++ s′) ∈ U ∩ dom
. In particular, (r, s) ∈
(cid:13)
dom kVik for each 1 ≤ i ≤ ℓ and (kV1k (r, s), . . . kVℓk (r, s)) ∈ dom(f ).
(cid:13)
(cid:13)
Therefore,
(cid:13)

f (V1, . . . , Vℓ)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

⟫.

f (kV1k (r, s), . . . , kVℓk (r, s)), w (r, s), s
D
E
→

f (kV1k (r, s), . . . , kVℓk (r, s)), w (r, s), s

D
because f (kV1k (r, s), . . . , kVℓk (r, s)) ≡

f (V1, . . . , Vℓ)

E

(r, s ++ s′).

(cid:4)

(cid:5)

36

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

– Finally, suppose ⟪Y(λx. M ), w , U ⟫ ⇒ ⟪λy. M [Y(λx. M )/x] y, w , U ⟫. Then

(r, s) = (r, s ++ s′) ∈ U . It holds

hY(λx. ⌊M ⌋ (r, s)), w (r, s), si
→ hλy. ⌊M ⌋ (r, s)[Y(λx. ⌊M ⌋ (r, s))/x] y, w (r, s), si

and by the Substitution Eq. (3),

λy. ⌊M ⌋ (r, s)[Y(λx. ⌊M ⌋ (r, s))/x] y ≡ ⌊λy. M [Y(λx.M )/x] y⌋ (r, s ++ s′

)

Lemma 11. Suppose ⌊M ⌋ (r, s) ≡ R, (r, s) ∈ U , w (r, s) = w and hR, w, si →
hR′, w′, s′i. Then there exists ⟪M , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫ such that ⌊R ′⌋ (r, s′) ≡
R′, w ′(r, s′) = w′ and (r, s′) ∈ U ′.

Proof. We prove the lemma by a case distinction on the redex contractions.

– First, suppose hsample, w, si → hr, w, s ++ [r]i, where 0 < r < 1 and (r, s) ∈
U ⊆ Rm × Sn. Then ⟪sample, w , U ⟫ ⇒ ⟪αn+1, w ′, U ′⟫, where U ′ = {(r, s ++
[r′]) | (r, s) ∈ U ∧ 0 < r′ < 1} ∋ (r, s ++ [r]) and w ′(r, s ++ [r]) = w (r, s) = w.
By deﬁnition, ⌊αn+1⌋ (r, s ++ [r]) ≡ r.

– Suppose hscore(r′), w, si → hr′, w′, si, where r′ ≥ 0, (r, s) ∈ U . Then M ≡
score(V ) for some V satisfying ⌊V ⌋ (r, s) ≡ r′. Hence, kV k (r, s) = r′ ≥ 0
and ⟪score(V ), w , U ⟫ ⇒ ⟪V , w · kV k , U ′⟫ where U ′ := U ∩ kV k−1 [0, ∞) ∋
(r, s) and (w · kV k)(r, s) = w × r′.
r′ ≤ 0, M, N

→ hM, w, si because r′ ≤ 0 and (r, s) ∈ U .
Suppose that V , M , N are such that ⌊V ⌋ (r, s) ≡ r′, ⌊M ⌋ (r, s) ≡ M and
(cid:1)
⌊N ⌋ (r, s) ≡ N . Then kV k (r, s) = r′ ≤ 0 and ⟪if
, w , U ⟫ ⇒
⟪M , w , U ′⟫, where U ′ := (U ∩ kV k−1 (−∞, 0]) ∋ (r, s) by assumption.

V ≤ 0, M , N

– Suppose

, w, s

if

(cid:10)

(cid:11)

(cid:0)

– Similar for the else-branch.
ℓ), w, s
f (r′
– Suppose

→

1, . . . , r′

f (r′
ℓ) ∈
dom(f ). Suppose further that V1, . . . , Vℓ are such that for each 1 ≤ i ≤
(cid:10)
ℓ, ⌊Vi⌋ (r, s) ≡ r′
f (V1, . . . , Vℓ)
, then
we have ⟪f (V1, . . . , Vℓ), w , U ⟫ ⇒ ⟪ f (V1, . . . , Vℓ), w , U ′⟫, where U ′ := (U ∩
dom

ℓ), w, s
E
= dom

f (V1, . . . , Vℓ)

i. Since dom

, where (r′

) ∋ (r, s).

1, . . . , r′

1, . . . , r′

f (V1, . . . , Vℓ)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

D

(cid:4)

(cid:11)

(cid:5)

(cid:13)
(cid:13)

– Suppose h(λy. M ) V, w, si → hM [V /y], w, si and (r, s) ∈ U . Let M , N be
such that ⌊M ⌋ (r, s) ≡ M and ⌊N ⌋ (r, s) ≡ V . By the Substitution Eq. (3),
⌊M [N /y]⌋ (r, s) ≡ M [N/y] and by Lem. 2, N must be a symbolic value.
Thus, ⟪(λy. M ) N , w , U ⟫ ⇒ ⟪M [N /y], w , U ⟫.

(cid:13)
(cid:13)

– Suppose hY(λy.M ), w, si → hλz.M [Y(λy.M )/y]z, w, si and (r, s) ∈ U . Let
M be such that ⌊M ⌋ (r, s) ≡ M , then by Substitution Eq. (3), we have
⌊λz.M [Y(λy.M )/y]z⌋ (r, s) ≡ λz.M [Y(λy.M )/y]z. Thus we conclude that
⟪Y(λy.M ), w , U ⟫ ⇒ ⟪λz.M [Y(λy.M )/y]z, w , U ⟫.

(cid:0)

(cid:1)

Densities of A.S. Terminating Programs are Diﬀerentiable A.E.

37

Deﬁnition 2. We extend ⌊·⌋ to symbolic contexts with domains

dom ⌊[]⌋ := Rm × Sn

dom ⌊E M ⌋ := dom ⌊(λy. M ) E⌋ := dom ⌊E⌋ ∩ dom ⌊M ⌋

f (V1, . . . , Vℓ−1, E, Mℓ+1, . . . , Mn)

dom
:= dom ⌊V1⌋ ∩ · · · ∩ dom ⌊Vℓ−1⌋ ∩ dom ⌊E⌋ ∩ dom ⌊Mℓ+1⌋ ∩ · · · ∩ dom ⌊Mn⌋

(cid:4)

(cid:5)

dom ⌊YE⌋ := dom ⌊score(E)⌋ := dom ⌊E⌋

dom

if

E ≤ 0, M , N

:= dom ⌊E⌋ ∩ dom ⌊M ⌋ ∩ dom ⌊N ⌋

by

(cid:4)

(cid:0)

(cid:1)(cid:5)
⌊[]⌋ (r, s) := []

⌊E M ⌋ (r, s) := (⌊E⌋ (r, s)) (⌊M ⌋ (r, s))
⌊(λy. M ) E⌋ (r, s) := (λy. ⌊M ⌋ (r, s)) (⌊E⌋ (r, s))

(r, s)
f (V1, . . . , Vℓ−1, E, Mℓ+1, . . . , Mn)
:= f (⌊V1⌋ (r, s), . . . , ⌊Vℓ−1⌋ (r, s), ⌊E⌋ (r, s), ⌊Mℓ+1⌋ (r, s), . . . , ⌊Mn⌋ (r, s))
(cid:4)

(cid:5)

⌊YE⌋ (r, s) := Y( ⌊E⌋ (r, s))

if

E ≤ 0, M , N

(r, s) := if

(⌊E⌋ (r, s)) ≤ 0, (⌊M ⌋ (r, s)), (⌊N ⌋ (r, s))

(cid:4)

(cid:0)

⌊score(E)⌋ (r, s) := score(() ⌊E⌋ (r, s))

(cid:1)(cid:5)

(cid:0)

(cid:1)

Proposition 1 (Correspondence). Suppose ⟪M , w , U ⟫ is a symbolic conﬁg-
uration, and (r, s) ∈ U . Let M ≡ ⌊M ⌋ (r, s) and w := w (r, s). Then
1. If ⟪M , w , U ⟫ ⇒ ⟪N , w ′, U ′⟫ and (r, s ++ s′) ∈ U ′ then

hM, w, si → h⌊N ⌋ (r, s ++ s′

), w (r, s′

), s ++ s′

i .

2. If hM, w, si → hN, w′, s′i then there exists ⟪M , w , U ⟫ ⇒ ⟪N , w ′, U ′⟫ such

that ⌊N ⌋ (r, s′) ≡ N , w ′(r, s′) = w′ and (r, s′) ∈ U ′.

Proof. Suppose that ⟪M , w , U ⟫ is a symbolic conﬁguration and (r, s) ∈ U .

If M is a symbolic value then ⌊M ⌋ (r, s) is a value Lem. 2 and there is nothing

to prove.

Otherwise, by Lem. 5, there exists unique E and R such that M ≡ E[R ].
Thus we can deﬁne the context E ≡ ⌊E⌋ (r, s) and redex R ≡ ⌊R ⌋ (r, s) (see
Lem. 4), and it holds by Eq. (5), ⌊M ⌋ (r, s) ≡ E[R].
1. If ⟪R , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫ and (r, s ++ s′) ∈ U ′ then by case inspection
(see Lem. 10), hR, w (r, s), si → hR′, w ′(r, s ++ s′), s ++ s′i such that R′ ≡
⌊R ′⌋ (r, s ++ s′). So, hE[R], w (r, s), si → hE[R′], w ′(r, s ++ s′), s ++ s′i and
by the substitution Eq. (5), E[R′] ≡ ⌊E[R ′]⌋ (r, s ++ s′).

2. Conversely, if hR, w (r, s), si → hR′, w′, s′i then a simple case analysis (see
Lem. 11) shows that for some R ′, w ′ and U ′, ⟪R , w , U ⟫ ⇒ ⟪R ′, w ′, U ′⟫
such that ⌊R ′⌋ (r, s′) ≡ R′, w (r, s′) = w′ and (r, s′) ∈ U ′. Thus also
⟪E[R ], w , U ⟫ ⇒∗ ⟪E[R ′], w ′, U ′⟫ and by the Substitution Eq. (5), we have
⌊E[R ′]⌋ (r, s) ≡ E[R].

38

C. Mak, C.-H. L. Ong, H. Paquet and D. Wagner

A.4 Supplementary Materials for Sec. 6

Lemma 8. Let ⟪M , w , U ⟫ be a symbolic conﬁguration such that w is diﬀeren-
tiable on ˚U and µ(∂U ) = 0. If ⟪M , w , U ⟫ ⇒ ⟪M ′, w ′, U ′⟫ then w ′ is diﬀeren-
tiable on ˚U ′ and µ(∂U ′) = 0.

Proof. For the Score-rule this is due to Lem. 3 and the fact that diﬀerentiable
functions are closed under multiplication. For the other rules diﬀerentiability of
w ′ is obvious.

Furthermore, note that µ(∂{(r, s ++ [s′]) | (r, s) ∈ U ∧ s′ ∈ (0, 1)}) = µ(∂U )

and for symbolic values V ,

kV k−1 [0, ∞)
(cid:16)

(cid:17)(cid:17)

= 0

µ

∂

kV k−1 (−∞, 0]

= µ

∂

kV k−1 (0, ∞)

= µ

∂

(cid:16)

(cid:16)

because of Lem. 3 and Assumption 1. Consequently, due to the general fact that
∂(U ∩ V ) ⊆ ∂U ∪ ∂V , in any case, µ(∂U ′) = 0.

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:17)(cid:17)

(cid:16)

Lemma 9. If M terminates almost surely then µ(Tmax \ Tterm) = 0.
Proof. Let T ∈ Bm be such that µ(Rm \ T ) = 0 and for every r ∈ T , M [r/x]
terminates almost surely. For r ∈ Rm we use the abbreviations

Sr,max := {s ∈ S | (r, s) ∈ Tmax}

Sr,term := {s ∈ S | (r, s) ∈ Tterm}

and we can argue analogously to Tmax and Tterm that they are measurable. Sim-
ilarly to Lem. 1, for all r ∈ Rm, µ(Sr,max) ≤ 1 because (s ++ s′) ∈ Sr,max and
s′ 6= [] implies s /∈ Sr,max.

Therefore, for every r ∈ T , µ(Sr,max \ Sr,term) = 0. Finally, due to a conse-
quence of Fubini’s theorem (Lem. 12) and the fact that the Lebesgue measure
is σ-ﬁnite,

µ(Tmax \ Tterm) = µ({(r, s) ∈ Rm × S | s ∈ Sr,max \ Sr,term) = 0

Lemma 12. Let (X, ΣX , µ) and (Y, ΣY , ν) be σ-ﬁnite measure spaces. Suppose
that U ∈ ΣX and that for every r ∈ X, Vr ∈ ΣY , and W := {(r, s) ∈ X × Y |
s ∈ Vr} is measurable.

If µ(X \ U ) = 0 and for every r ∈ U , µ(Vr) = 0 then µ(W ) = 0.

Proof. Let Xn ∈ ΣX and Yn ∈ ΣY (for n ∈ N) be such that X =
Y =
(Xn × Yn). Clearly (µ × ν)(Wn) is ﬁnite.

n∈N Xn = X,
n∈N Yn and µ(Xn) = ν(Yn) < ∞ for every n ∈ N. Deﬁne Wn := W ∩

S

By assumption the characteristic function 1W : X × Y → R≥0 is measurable.

S

By Fubini’s theorem [?, Thm. 1.27], for every n ∈ N,

µ(Wn) =

(d(µ × ν))1W =

(dµ)

(dν)1W =

(dµ)λr. ν(Vr) = 0

ZXn×Yn

ZXn

ZYn

ZU∩Xn

The third equation is due to µ(Xn \ U ) = 0. The claim is immediate by W =

n∈N Wn.

S

(cid:1009)(cid:1004)

(cid:1008)(cid:1009)

(cid:1008)(cid:1004)

(cid:1007)(cid:1009)

(cid:1007)(cid:1004)

(cid:1006)(cid:1009)

(cid:1006)(cid:1004)

(cid:1005)(cid:1009)

(cid:1005)(cid:1004)

(cid:1009)

(cid:1004)

6

4

2

0

(cid:1004)

·104

(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:4)

(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:17)

0

1

2

3

(cid:0)✁✂✄✁☎✆✝ ✞✟✠✂✁☎✟✆

(cid:1009)

(cid:1005)(cid:1004)

(cid:1005)(cid:1009)

(cid:1006)(cid:1004)

(cid:1006)(cid:1009)

(cid:1007)(cid:1004)

✡
☛
☞
✌
✍
☞
✎
✏
✑
This figure "plot.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2004.03924v2

