1
2
0
2

l
u
J

9
2

]

R
C
.
s
c
[

2
v
8
4
8
2
1
.
4
0
1
2
:
v
i
X
r
a

secml-malware: Pentesting Windows Malware Classiﬁers with
Adversarial EXEmples in Python

Luca Demetrio
Universit`a degli Studi di Cagliari

Battista Biggio
Universit`a degli Studi di Cagliari
Pluribus One

luca.demetrio93@unica.it

battista.biggio@unica.it

Abstract

Machine learning has been increasingly used as a ﬁrst line of defense for Windows malware
detection. Recent work has however shown that learning-based malware detectors can be
evaded by carefully-perturbed input malware samples, referred to as adversarial EXEmples,
thus demanding for tools that can ease and automate the adversarial robustness evalua-
tion of such detectors. To this end, we present secml-malware, the ﬁrst Python library
for computing adversarial attacks on Windows malware detectors. secml-malware imple-
ments state-of-the-art white-box and black-box attacks on Windows malware classiﬁers,
by leveraging a set of feasible manipulations that can be applied to Windows programs
while preserving their functionality. The library can be used to perform the penetration
testing and assessment of the adversarial robustness of Windows malware detectors, and
it can be easily extended to include novel attack strategies. Our library is available at
https://github.com/pralab/secml_malware.
Keywords: Python, Windows, malware, programs, adversarial, machine learning

1. Introduction

Machine learning is extensively used as a ﬁrst line of defence against the spread of Win-
dows malware, and both industry and academia are developing increasingly-sophisticated
algorithms for extracting malicious patterns from data, leveraging diﬀerent feature sets
and model architectures (Raﬀ et al., 2018; Coull and Gardner, 2019; Saxe and Berlin, 2015;
Anderson and Roth, 2018). Meanwhile, recent work has shown that these learning-based
malware detectors can be misled, enabling the attacker to infect the target device with mal-
ware (Demetrio et al., 2019, 2021, 2020; Kreuk et al., 2018; Suciu et al., 2019; Sharif et al.,
2019; Anderson et al., 2017; Castro et al., 2019). The latter can be achieved by applying
practical manipulations that do not alter the functionality of a malicious program, but
rather its ﬁle structure. In this way, the attacker crafts an adversarial EXEmple, i.e., an
adversarial example for Windows malware detectors, as deﬁned by Demetrio et al. (2021),
which can be run on the target machine even after the injection of well-crafted manipu-
lations. Hence, there is the need of open-source tools that allows analysts to test their
classiﬁers and defenses against these threats, with the possibility of understanding how to
mitigate such threats, and being one-step ahead of possible attackers.

For this reason, we propose secml-malware, the ﬁrst Python library for creating ad-
versarial EXEmples in input space, providing developer and analysts a tool for performing

©2021 Luca Demetrio, Battista Biggio.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Demetrio and Biggio

White-box attacks

Proposed by
Demetrio et al. (2019)
Demetrio et al. (2021)
Demetrio et al. (2021)
Demetrio et al. (2021)
Kolosnjaji et al. (2018)
Kreuk et al. (2018)
Suciu et al. (2019)

Practical manipulation
partial dos
full dos
extend
shift
padding
slack+padding
slack+padding

Black-box attacks

Proposed by
Demetrio et al. (2020)
Demetrio et al. (2020)
Demetrio et al. (2021)
Demetrio et al. (2021)
Demetrio et al. (2021)
Demetrio et al. (2021)
Demetrio et al. (2021)
Demetrio et al. (2021)

Practical manipulation
GAMMA padding
GAMMA section inj.
partial dos
full dos
extend
shift
padding
slack+padding

Table 1: Attacks implemented in secml-malware, along with the manipulations they apply.

security evaluations on their machine-learning Windows malware detectors. The library
contains most of the proposed practical manipulations to perturb Windows programs, and
it is written on top of the secml library (Melis et al., 2019). The structure is modular
enough for allowing users to code their own attacks and wrap the model to test easily,
and the library is shipped with both white-box and black-box strategies that can be used
as-is against Windows malware detectors. The codebase of this project can be found at
https://github.com/pralab/secml_malware. Also, we integrate secml-malware as part
of ToucanStrike,1 a command-line tool for creating adversarial EXEmples by typing com-
mands inside the shell.

2. secml-malware: Architecture and Implementation

The library is divided in three main modules: attack, models, and utils. Each of these
packages is provided with unit tests that asserts the correct behaviour of these techniques.

The attack module. This module contains all the attacking strategies, divided in whitebox
and blackbox modules, and we provide a complete list of them in Table 1.
The attacks inside the whitebox sub-module are implemented by leveraging practical ma-
nipulations that address the perturbing of single bytes inside the program (Demetrio et al.,
2019, 2021; Kreuk et al., 2018; Suciu et al., 2019). These attacks exploit the ambiguity of
the ﬁle format, by ﬁlling unused space inside the binary or injecting new content in partic-
ular positions, but always preserving the original functionality of the sampl. Since we use
secml, the optimizer uses pytorch (Paszke et al., 2019), and all the gradient computations
leverage this framework.
The attacks inside the blackbox sub-module are implemented using DEAP (Fortin et al.,
2012), a library for encoding genetic optimizers, and they span from byte-based to more
section and API injection (Demetrio et al. (2020, 2021)).
structural manipulations, e.g.
We also include GAMMA (Demetrio et al., 2020), that is a black-box attack leveraging the
injection of benign content to fool the target detector, by also keeping low the size of the

1. https://github.com/pralab/toucanstrike

2

secml-malware: Pentesting EXE Malware Classifiers

adversarial malware and the number of queries sent. Since we use secml, the optimizer
leverage the pytorch framework (Paszke et al., 2019) for computing gradients.
The models module. This sub-module hosts the deﬁnition of two state-of-the-art clas-
siﬁers: a deep neural network, called MalConv (Raﬀ et al., 2018), and a Gradient Boost
Decision Tree (GBDT) (Anderson and Roth, 2018). Both of them are encapsulated in
classes that can be passed to the underlying secml framework for computing the attacks.
The latter is modular enough for including most models from diﬀerent frameworks, and the
end user can leverage this feature to include their custom target.
The utils module. This sub-module contains support code for dealing with technicalities
of the practical manipulations contained in the library, such as code the for keeping the
constraints intact.
To boost reproducibility and testability of secml-malware, we also provide a detailed de-
scription on how to create a custom conda environment,2 and also a Docker container.3

3. Application Example: Evaluating Adversarial Robustness of MalConv

To show the potential of our library, we apply both white-box and black-box state-of-
the-art attacks already coded in secml-malware against a deep neural network called
MalConv (Raﬀ et al., 2018). The latter is just an example we chose for simplicity, but
secml-malware provides wrappings also for other classiﬁers as well (e.g. gradient boosting
decision trees and general Pytorch neural networks (Demetrio et al., 2021, 2020) For this
example, we choose to test the following strategies.
Partial DOS. Proposed by Demetrio et al. (2019), this attack leverage the editing of a
fraction of the unused DOS header, kept inside compiled binaries for retro-compatibility.
Extend. Proposed by Demetrio et al. (2021), this attack leverage the extension of the
unused DOS header, by shifting the real one by a custom amount, and injecting adversarial
content there.
Intuitively, the manipulation exploits the presence of an oﬀset inside the
DOS header, that instructs the loader where the real header of the program is placed inside
the binary. The injected content must be compliant with the constraints imposed by the
ﬁle format, e.g. the ﬁle alignment speciﬁed in the header.
Shift. Proposed by Demetrio et al. (2021), this attack inject new content before the ﬁrst
section, by shifting all the content by the custom amount. This manipulation exploits the
oﬀsets inside the header of the program that instructs the loader where the sections start
inside the binary. Since all sections must be aligned to a multiple of the ﬁle alignment, the
content must match this constraint in order to not break the structure.
Padding. Proposed by Kolosnjaji et al. (2018), this attack leverages the appending of new
bytes at the end of the executable. Such content is not controlled by the loader, since there
are no pointers to such addition inside neither the code and the header.
GAMMA-padding. Proposed by Demetrio et al. (2020), this attack leverage the padding
practical manipulation to inject content extracted from benign software (i.e. goodware
programs). It relies on a regularization parameter to control the size of the injected content,
so the adversarial EXEmples do not grow unbounded.

2. https://anaconda.org
3. https://www.docker.com

3

Demetrio and Biggio

White-box attacks
Partial DOS Extend Shift Padding

Black-box attacks
Partial DOS Extend Shift Padding GAMMA-padding

1 iter.
25 iter.
50 iter.

60%
28%
28%

5%
5%
5%

87.5%
80%
80%

85%
45%
45%

10 queries
250 queries
500 queries

69%
56%
42%

34%
25%
10%

80%
79%
65%

100%
100%
100%

14%
13%
12%

MalConv original DR: 100%

Table 2: Detection Rates (DRs) of MalConv against white-box/black-box attacks, opti-

mized with an increasing number of iterations/queries.

3.1 Experimental results

We test the Partial DOS, Extend, Shift and Padding attacks in both white-box and black-
box settings, along with GAMMA-padding. The results are shown in Table 2.
White-box attacks. We set the maximum number of iterations to 50, and we observe
how the detection rate decreases while optimizing the injected content. Our library is able
to identify a weakness to attacks that perturb the header of programs; in particular, the
Extend attack is able to decrease the detection rate close to 0 in only few iterations.
Black-box attacks. We bound the maximum number of queries to 500 for the black-box
attacks. For GAMMA-padding, we extracted 100 .data sections from legitimate programs,
and we set the regularization parameter to 10−5. Since the attack is estimating the best
direction to take, the attacks are less eﬀective that their white-box counterpart, except
for GAMMA-padding. The latter injects content extracted from the goodware class, hence
mimicking its distribution. This is enough for decreasing the detection rate close to 0.1.

4. Conclusions and Future Work

We show secml-malware, a tool for testing the robustness of machine learning Windows
malware classiﬁers, already shipped with many state-of-the-art attacks that can be evalu-
ated with minimal setup against a classiﬁer of choice. To give an intuition about its features,
we show example results against a state-of-the-art deep neural network, in both white-box
and black-box settings. We want to remark that this is the ﬁrst library that contains both
gradient and gradient-free techniques focused on this domain, and it might become a central
starting point for future applications. We have evidence that our library is being employed
in other scientiﬁc research as well (Quiring et al., 2020), and that also a community is grow-
ing around it. Currently, secml-malware has almost 70 stars on GitHub, with a monthly
quota of 100 downloads per month on average.4 We have already closed issues raised by the
community, by ﬁxing bugs and replying to curiosities from interested users.5 Moreover, we
are integrating our technology inside CounterFit, the latest Microsoft machine-learning pen-
testing tool.6 As future work, we plan to extend the attacks implemented in secml-malware
to also target classiﬁers that extract information from runtime behaviour of malware. This
line of work would be indeed beneﬁcial also for the defense side, as secml-malware would
become an ubiquitous tool for testing any kind of machine-learning malware classiﬁer.

4. https://pypistats.org/packages/secml-malware
5. https://github.com/pralab/secml_malware/issues?q=is%3Aissue+is%3Aclosed
6. https://github.com/azure/counterfit

4

secml-malware: Pentesting EXE Malware Classifiers

Acknowledgments

This work was partly supported by the PRIN 2017 project RexLearn (grant no. 2017TWNMH2),
funded by the Italian Ministry of Education, University and Research.

References

Hyrum S Anderson and Phil Roth. Ember: an open dataset for training static pe malware

machine learning models. arXiv preprint arXiv:1804.04637, 2018.

Hyrum S Anderson, Anant Kharkar, Bobby Filar, and Phil Roth. Evading machine learning

malware detection. black Hat, 2017.

Raphael Labaca Castro, Corinna Schmitt, and Gabi Dreo. Aimed: Evolving malware with
genetic programming to evade detection. In 2019 18th IEEE International Conference
On Trust, Security And Privacy In Computing And Communications/13th IEEE Interna-
tional Conference On Big Data Science And Engineering (TrustCom/BigDataSE), pages
240–247. IEEE, 2019.

Scott E Coull and Christopher Gardner. Activation analysis of a byte-based deep neu-
ral network for malware classiﬁcation. In 2019 IEEE Security and Privacy Workshops
(SPW), pages 21–27. IEEE, 2019.

Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando.
Explaining vulnerabilities of deep learning to adversarial malware binaries. Proceedings
of the Third Italian Conference on CyberSecurity (ITASEC), 2019.

Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando.
Functionality-preserving black-box optimization of adversarial windows malware, 2020.

Luca Demetrio, Scott E. Coull, Battista Biggio, Giovanni Lagorio, Alessandro Armando,
and Fabio Roli. Adversarial exemples: A survey and experimental evaluation of practical
attacks on machine learning for windows malware detection, 2021.

F´elix-Antoine Fortin, Fran¸cois-Michel De Rainville, Marc-Andr´e Gardner, Marc Parizeau,
and Christian Gagn´e. DEAP: Evolutionary algorithms made easy. Journal of Machine
Learning Research, 13:2171–2175, jul 2012.

Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto,
Claudia Eckert, and Fabio Roli. Adversarial malware binaries: Evading deep learning for
malware detection in executables. In 2018 26th European Signal Processing Conference
(EUSIPCO), pages 533–537. IEEE, 2018.

Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and Joseph
Keshet. Deceiving end-to-end deep learning malware detectors using adversarial examples.
arXiv preprint arXiv:1802.04528, 2018.

Marco Melis, Ambra Demontis, Maura Pintor, Angelo Sotgiu, and Battista Biggio. secml:

A python library for secure and explainable machine learning, 2019.

5

Demetrio and Biggio

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Advances in Neural Information
Processing Systems, 32:8026–8037, 2019.

Erwin Quiring, Lukas Pirch, Michael Reimsbach, Daniel Arp, and Konrad Rieck. Against
all odds: Winning the defense challenge in an evasion competition with diversiﬁcation.
arXiv preprint arXiv:2010.09569, 2020.

Edward Raﬀ, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and
In Workshops at the

Charles K Nicholas. Malware detection by eating a whole exe.
Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

Joshua Saxe and Konstantin Berlin. Deep neural network based malware detection using two
dimensional binary program features. In Malicious and Unwanted Software (MALWARE),
2015 10th International Conference on, pages 11–20. IEEE, 2015.

Mahmood Sharif, Keane Lucas, Lujo Bauer, Michael K Reiter, and Saurabh Shintre.
Optimization-guided binary diversiﬁcation to mislead neural networks for malware de-
tection. arXiv preprint arXiv:1912.09064, 2019.

Octavian Suciu, Scott E Coull, and Jeﬀrey Johns. Exploring adversarial examples in mal-
In 2019 IEEE Security and Privacy Workshops (SPW), pages 8–14.

ware detection.
IEEE, 2019.

6

