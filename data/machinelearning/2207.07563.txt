2
2
0
2

l
u
J

8
2

]
h
p
-
t
n
a
u
q
[

3
v
3
6
5
7
0
.
7
0
2
2
:
v
i
X
r
a

1

QSAN: A Near-term Achievable Quantum
Self-Attention Network

Ren-xin Zhao, Member, IEEE, Jinjing Shi, Member, IEEE, Shichao Zhang, Senior Member, IEEE,
and Xuelong Li, Fellow, IEEE

Abstract—A Quantum Self-Attention Network (QSAN) that can be achieved on near-term quantum devices is investigated. First, the
theoretical basis of QSAN, a linearized and reversible Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity
(QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM), is explored to solve the storage problem of Self-Attention Mechanism
(SAM) due to quadratic complexity. More importantly, QLS uses logical operations instead of inner product operations to enable QSAN
to be fully deployed on quantum computers and meanwhile saves quantum bits by avoiding numerical operations, and QBSASM is
a by-product generated with the evolution of QSAN, reﬂecting the output attention distribution in the form of a density matrix. Then,
the framework and the quantum circuit of QSAN are designed with 9 execution steps and 5 special functional sub-modules, which
can acquire QBSASM effectively in the intermediate process, as well as compressing the number of measurements. In addition, a
quantum coordinate prototype is proposed to describe the mathematical connection between the control and output bits in order to
realize programming and model optimization conveniently. Finally, a miniaturized experiment is implemented and it demonstrates that
QSAN can be trained faster in the presence of quantum natural gradient descent method, as well as produce quantum characteristic
attention distribution QBSASM. QSAN has great potential to be embedded in classical or quantum machine learning frameworks to lay
the foundation for quantum enhanced Natural Language Processing (NLP).

Index Terms—Quantum self-attention mechanism, Quantum machine learning, Quantum circuit, Quantum natural language processing,
Quantum network.

✦

1 INTRODUCTION

S AM is a powerful component embedded in machine

learning models that reduces the dependence on exter-
nal information and better captures the intrinsic relevance
of data or features, thus signiﬁcantly enhancing the per-
formance of the model. It was originally introduced by
a deep learning framework for machine translation called
Transformer [1], and now is extensively employed in NLP
[2–4], Speech [5], emotion analysis [6] and Computer Vision
[7–10].

Although SAM is very beneﬁcial, its complexity escalates
quadratically with the length of the input sequence, severely
hindering its exploration on longer sequences. Some pro-
posals, such as Nystr¨om matrix decomposition [11], kernel
methods [12, 13], hashing strategies [14], sparsiﬁcation [15–
17], random projections [18], etc., have good effect in di-
minishing the complexity. In further, Ref. [19] identiﬁed the
limitation of the SAM that it is unable to model periodic
ﬁnite state languages and hierarchical constructions, with a
ﬁxed number of layers or heads. Perhaps exploring struc-
tural adaptive SAM could bring new opportunities to this
challenge.

From another point of view, quantum computer is con-
sidered as a promising processor paradigm that surpasses

• Ren-xin Zhao, Jinjing Shi and Shichao Zhang are with the School of
Computer Science and Engineering, Central South Univerisity, China,
Changsha, 410083.

• Xuelong Li is with the School of Artiﬁcial Intelligence, OPtics and Elec-
troNics, Northwestern Polytechnical University, China, Xi’an, 710072.
Jinjing Shi is the corresponding author.

•
• E-mails: 13061508@alu.hdu.edu.cn, shijinjing@csu.edu.cn, zhangsc@ma-

ilbox.gxnu.edu.cn, li@nwpu.edu.cn.

Manuscript received XXXX; revised XXXX.

the limits of traditional computer computing and have
made signiﬁcant breakthroughs in recent years [20–22]. The
superiority offered by quantum computers, also known as
quantum supremacy, speciﬁcally refers to the exponential
storage and secondary computational acceleration arising
from the effects of quantum properties [23, 24]. Ref. [25]
exploited the idea of weak measurement in quantum me-
chanics to construct a parameter-free, more efﬁcient quan-
tum attention, which is used in the LSTM framework and
found to have better sentence modeling performance. Ref.
[26] understood the quantum attention mechanism as a
density matrix by which more powerful sentence represen-
tations can be constructed. Unfortunately, the above two
approaches only involve certain physical concepts in quan-
tum mechanics without providing speciﬁc quantum circuits.
A recent meaningful effort was contributed by the Baidu
group, where a Gaussian projection-based QSAN using
VQA [27] to build Parametric Quantum Circuits (PQC) [28]
on Noisy Intermediate-Scale Quantum (NISQ) [39] devices
was applied to text classiﬁcation [40]. While this work is
signiﬁcant, it is still worth discussing whether quantum
computers can take on more tasks since self-attention scores
need to be calculated on classical computers in order to
obtain the ultimate output. Furthermore, this model relies
on a large number of measurements to convert quantum
data into classical data for storage, and further optimization
is expected in terms of storage savings.

A novel QSAN based on a linearized, reversible QSAM
is formally presented as an attempt to address the fact that
the high complexity of SAM will consume more storage and
that QSAM lacks diverse research, which is able to fully
accomplish the computation of quantum attention scores

 
 
 
 
 
 
and word vectors with fewer measurements. Compared
to SAM, QSAM demands exponentially less storage with
the help of quantum representation for the same input
sequence. In contrast to Ref. [25, 26, 40], QSAN is potentially
fully deployed and realized on quantum devices with fewer
measurements and a beneﬁcial byproduct called QBSASM.
Whereas, the essential motivation for proposing this QSAN
is to explore whether young quantum computers can have
quantum characteristic attention and can depict the distribu-
tion of outputs in a quantum language, not to replace SAM
or to beat all the schemes in the Ref. [25, 26, 40]. Quantum
characteristic here could be understood as probability, lin-
earity and reversibility, while the quantum language refers
to specialized terms in quantum mechanics, such as density
matrix, Hamiltonian operators, etc. To this end, the major
innovations of this paper are summarized as follows.
⋆ A new QSAM theoretical framework with linearized,
reversible, and logical features is explored, in which
QLS and QBSASM are purposefully designed to serve
as theoretical guides for QSAN.

⋆ The overall framework of QSAN and quantum circuits
are designed, in which quantum coordinates are used
as an action criteria to simplify the design of QSAN.
⋆ The performance of QSAN on NISQ devices is tested
with a miniaturized experiment, and the results show
that QSAN is trained with quantum natural gradient
descent faster than the conventional gradient descent
approach.
The layout of this paper is as follows. Some basics
are reviewed in section 2. QLS, QBSASM and QSAM are
proposed in section 3. The potential of quantum coordinates
and the design of QSAN are explained in Section 4. The
experiments and discussions are conducted in Section 5.
Finally, the conclusion is elaborated in Section 6.

2 PRELIMINARIES

This section brieﬂy outlines SAM, VQA, and quantum
operators. First a protocol is made for subscripts: if not
speciﬁcally stated in this paper, subscripts always denote
the sequence number of the variable.

2.1 Self-Attention Mechanism
The input set In =
, wn−1}
w0,
and the output set
{
· · ·
Out =
, new wn−1}
are deﬁned, where any
· · ·
element wi as well as new wj with i, j
is
a vector of dimension l, n is regarded as the total number of
word vectors. Then SAM [1] can be stated as

new w0,

∈ {

· · ·

, n

0,

−

{

}

1

new wi =

softmax

Xj

QiKT
j
√d !

Vj.

(1)

In Eq. (1), √d is a scaling factor. Qi, Vj are row vectors,

where

Qi = wi ·
Vj = wj ·

UQ,

UV ,

(2)

(3)

wi and wj are inputs. KT
transpose of

j is a column vector, which is the

Kj = wj ·

UK.

UQ, UK and UV are three trainable parameter matrices
named as query conversion matrix, key conversion matrix
and value conversion matrix respectively. The weights

2

softmax

QiKT
j
√d !

,

(5)

also called attention scores, are obtained by normalizing the
inner product QiKT
j . new wi represents new word vector
after the weighting operation.

2.2 Variational Quantum Algorithm

In the NISQ era, it is very difﬁcult to fully deploy deep
networks for deep learning on quantum computers with
limited qubits. On the one hand, the dimensionality of
the model grows exponentially as the size of the quantum
circuit gets larger [29]. On the other hand, noise imposes
many unknowns on the training results [30]. Therefore,
quantum-classical hybrid model is deemed as an efﬁcient
path. VQA is one such class of algorithms. The framework
of VQA is exhibited in Fig. 1, which can be divided into two
parts.

Model

Prediction

Loss
Function

U1

Un

Update

Optimization
Process

search

Quantum Computer

Classical Computer

Fig. 1. Framework of VQA

1. The pink box designates the range of the classical
computer. This stage focuses on the calculation of the loss
function and the optimization of the parameters, as shown
in the two purple curves in Fig. 1. The general formulation
of the loss function is:

(θ)=

C

Xk

Fk(Tr[

OkU

(θ)ρk U

†(θ)])

(6)

{

ρk}

Fk is a set of certain functions determined by speciﬁc
where
⊗i Ui(θi) denotes the product of a series of
(θ) =
tasks.
U
unitary operators, where θ comprises a series of continuous
is the input state of the
or discrete hyperparameters.
training set, and
Ok is a set of observables. Some strategies
for training loss functions can be consulted in [31–33].
2. The tan box stands for the quantum computer domain.
In this box, a PQC model is drawn. The black dashed
box is the centerpiece of this model, the Ansatz, which is
a circuit with a speciﬁc structure and function. Common
examples of Ansatz contain hardware-efﬁcient Ansatz (A
quantum circuit model for decreasing the circuit depth of an
(θ) for a given hardware) [34, 35], quan-
implementation
tum alternating operator Ansatz (can searches for optimal
solutions to combinatorial optimization problems) [36–38],
etc.

U

The arrows in the ﬁgure illustrate the interaction of
information of quantum computers and classical computers.

(4)

 
 
The quantum computer provides the classical computer
with quantum circuit measurements and loss function forms
to be used for prediction. After the classical computer is
trained, a new round of hyperparameters is uploaded and
updated into the quantum circuit.

2.3 Qubit and Operators

In a quantum computer, the smallest element of informa-
ψ
tion is a qubit
which is usually expressed as a linear
|
superposition of two eigenstates

and

1

0

i

|

i

, namely
i

|

ψ

|

i

0

= α
|

i

+ β

1

,
i

|

where α and β as probability amplitudes, satisfy

2 +

α
|

|

β

|

|

2 = 1.

(7)

(8)

These qubits evolve through unitary operators U which
are also called quantum gates and refer to matrices that
satisfy

U −1 = U †,
U U † =
,

(9)

I
is the identity matrix, U † is the complex conjugate

where
of U . This article mainly uses Rotating Pauli Y Gate

I

Ry(θ) =

cos(θ/2)
sin(θ/2)

(cid:18)

sin(θ/2)
−
cos(θ/2)

(cid:19)

,

(10)

Hadamard Gate

H =

1
√2 (cid:18)

1
1

1
1
(cid:19)

−

,

SW AP = 





1 0
0 0
0 1
0 0

0 0
1 0
0 0
0 1



,





CN OT = 





1 0
0 1
0 0
0 0

0 0
0 0
0 1
1 0



,





SWAP gate

CNOT gate

Toffoli gate

(11)

(12)

(13)

T of f oli =

1 0
0 1
0 0
0 0
0 0
0 0
0 0
0 0















0 0
0 0
1 0
0 0
0 0
0 0
0 0
0 1

0 0
0 0
0 0
0 0
1 0
0 1
0 0
0 0

0 0
0 0
0 0
0 1
0 0
0 0
1 0
0 0















,

(14)

and multi-controlled Toffoli gate.

3 QUANTUM SELF-ATTENTION MECHANISM

3

This section presents a logicalized, linearized QSAM frame-
work in which QLS is used to measure logical similarity and
enables QSAM to be freed from numerical operations such
as addition, thus conserving more qubits. More importantly
QLS replaces the inner product similarity that needs to be
implemented by measurement, which ensures that the task
is always executed on the quantum computer without inter-
ruption. QBSASM derived from QLS expresses the weight
distribution of the quantum computer on word vectors in
the form of a density matrix.
Before starting, ﬁrst

the sets In and Out are re-
wn−1i}
w0i
expressed in quantum states as Qin =
,
|
· · ·
{|
new w0i
and Qout =
new wn−1i}
,
respectively,
|
· · ·
of Qin is
wai
1
, n
0,
, a
where each element
∈ {
}
−
· · ·
|
, and l is the feature
log2l
a vector of dimension m =
⌉
⌈
dimension of the classical word vector. The dimension of
new wbi
1
, b
is higher, mainly because
|
· · ·
QSAM is described as

∈ {

, n

{|

0,

−

}

,

,

new wii

|

:=

Qi|

⊙j h

Kji ⊗ |

Vji
.

In Eq. (15),

Qii

|

= Uq|

Wii
,

Kji

|

= Uk|

Wji
,

(15)

(16)

(17)

|

(18)

Vj i

Wji
,

= Uv|
where Uq, Uk and Uv are speciﬁed as three composite
unitary operators with the identical structure but distinct
parameters. The same composition means that all three
matrices above are composed of (m
1) Hadamard gates, m
rotating Pauli Y gates, and m CNOT gates, and are arranged
in order

−

UM∈{q,k,v} = CN OT ⊗(m−1)Ry(θM )⊗mH ⊗m.

(19)

Vji

|

are input word vectors. The symbol

The beneﬁt of this design is to maintain that the probability
wii
amplitudes are all real numbers [41]. Furthermore,
and
|
wji
signiﬁes a tensor
|
Qi|
Kji
is a QLS that will be introduced next.
operation.
h
The symbol
encompasses two operations. One is to apply
⊙
a multi-controlled Toffoli gate to several speciﬁc QLS ele-
ments. The method of selecting these speciﬁc QLS is called
slicing, which will be explained below. The other is to use
CNOT gates to

to perform dimensional compression.

⊗

Formally, Eq. (15) is very similar to Eq. (1), but there are
essential changes. Comparing Eq. (1) and Eq. (15), Eq. (1)
is an attention mechanism with nonlinear operations, while
Eq. (15) has a linearized, logical character, which makes it
easier to be implemented on quantum computers across the
board. Futhermore, in Eq. (1), a large number of numerical
operations are required, such as solving the inner product as
well as weighted summation, which is costly to implement
on existing quantum computers. In contrast, Eq. (15) reduces
the implementation cost with QLS and saves even more
qubits.

3.1 Quantum Logical Similarity

and

Qai

In quantum computing, a common way to characterize the
similarity between two quantum states
is
SWAP test [42] or Hadamard test [43]. However, these two
schemes are made by multiple measurements to obtain the
inner product of quantum states. Yet, the goal of QSAN is
not to obtain the similarity between quantum states, but
to construct new word vectors with the help of similarity.
Therefore, the classical method of using inner product as
similarity must be modiﬁed.

Kbi

|

|

|

Kbi

Qai

⊕h

0,

∈ {

(20)

and

}
:=

with a, b

and
Qai

Deﬁnition 1 (QLS): For any quantum state
, n
1
· · ·
−
Kbi
Qa|
h
|Kb,hi
and

|
, QLS is redeﬁned as
Qa,h ∧ Kb,h)
(
1
, m
0,
, respectively. The symbol

, h
where
|Qa,hi
∈ {
Kbi
h-th qubit of
⊕
indicates modulo-two addition and the symbol
is logical
AND operation. Eq. (20) may seem counter-intuitive, but
in fact, an AND operation can be performed between two
superposition states, and the consequence is also a superpo-
sition state. From the implementation point of view, AND
operation and modulo-two addition can be realized with
Toffoli gates and CNOT gates, respectively. Eq. (20) is then
explained in terms of quantum gates:

stand for the

· · ·

−

∧

}

|

|

T of f oli
=
|Qa,h,
CN OT
=

Kb,h, 0
i
Qa,h ∧ Kb,hi

|Qa,h,
Kb,h,
|Qa,j ∧ Kb,j,

|Qa,j ∧ Kb,j, (

Qa,i ∧ Kb,ii
.
Qa,j ∧ Kb,j)
(
i
⊕

Qa,i ∧ Kb,i)

(21)

Moreover, Eq. (20) is obviously consistent with the com-

mutation law, i.e.

4

is extracted from Eq. (23) to depict the distribution of the
output, where each element is computed by QLS. The slic-
ing operation mentioned previously comes into play here.
Speciﬁcally, slicing takes the element QLS in each row of
QBSASM as control bits and uses the result of the AND
operation on these elements as a new weight, thus reﬂecting
the weighting operation of QSAN. QBSASM is a valuable
by-product of QSAN, which can be acquired by way of
pennylane intercepting the density matrix of QLS.

In summary, the element QLS is a single qubit in the
superposition state. Therefore, the dimensionality of the
QBSASM is higher than the classical attention score matrix,
which reﬂects the quantum nature of the QBSASM. Par-
ticularly, as the dimensionality increases QBSASM is more
difﬁcult to simulate classically, which is manifesting the
storage advantage of quantum computers. Finally, it can be
known that the output and input of Eq. (15) do not have the
same dimensionality, but there is no need to worry about
this. The output dimensionality can be effectively controlled
using a layer of neural networks, but this is beyond the
scope of this paper.

4 QUANTUM SELF-ATTENTION NETWORK
In this section, the overall framework and quantum circuits
of QSAN are illustrated. Especially, a prototype of quantum
coordinates is presented, which is a design guideline for
quantum circuits with regular layout. With the guidance of
quantum coordinates, the functional link between control
bits and output bits can be established to facilitate pro-
gramming. It is also worth exploring in quantum circuit
optimization.

Qa|
h

Kbi

=

Kb|
h

Qai
,

(22)

4.1 Framework of Quantum Self-Attention Network

which contributes to computational efﬁciency and avoid-
ance of barren plateaus due to heavy entanglement [44, 45].
This property demonstrates that Eq. (20) involves the calcu-
n
i=1 i QLS rather than n2.
lation of only m

3.2 Quantum Bit Self-Attention Score Matrix

P

The procedure for solving the quantum circuit for a single
new word vector only is given by Eq. (15). The solution
process for all word vectors is redescribed by the matrix as
follows:

Q0|
(
h
Q1|
(
h

K0i ⊗ |
K0i ⊗ |

V0i
)
V0i
)



⊙ · · · ⊙
⊙ · · · ⊙
...
⊙ · · · ⊙

Q0|
(
h
Q1|
(
h

Kn−1i ⊗ |
Kn−1i ⊗ |

Vn−1i
)
Vn−1i
)

Qn−1|
(
h

Kn−1i ⊗ |

Vn−1i
)









Qn−1|
(


h

|
|

V0i
K0i ⊗ |
)
new word0i
new word1i
...

new wordn−1i






|

.

= 




The weight coefﬁcient matrix QBSASM
Q0|
Q0|
h
h
Q1|
Q1|
h
h

K0i
K0i

K1i
K1i

· · ·
· · ·

(23)

,

(24)

Kn−1i
Kn−1i
...

Kn−1i
Qn−1|


h






Q0|
h
Q1|
h
...

Qn−1|


h


K0i

· · ·

· · ·

|

|

i

Q

K

The main framework of QSAN, as shown in Fig. 2, con-
sists of one input register and three garbage registers for
computing the query quantum state
, the key quantum
i
and QLS. In terms of resource consumption, the
state
m qubits each,
ﬁrst, second and third registers take n
n
while the fourth register needs m
i=1 i qubits, for a total
n
i=1 i qubits. In addition, a trick that can
of 3m
be controlled by code, also called quantum encoding [47],
needs to be noted, i.e. keeping the ﬁrst three registers with
the same input. In Fig. 2, those with the same operation,
such as Step 1, 3, and 6, are marked with the same color.
, and the output through
The input here is denoted as
i
V
Uv is represented as the value quantum state

n + m

In

P

P

×

×

|

.
i

|

4.2 Quantum Coordinates

In order to discover the mathematical connection between
the control bits and the output bits, the prototype of quan-
tum coordinates is hereby proposed.

Deﬁnition 2 (Quantum Coordinates): For a regularly
arranged quantum circuit, the intersection of the number
of layers and the circuit line number is the quantum coordi-
nate.

Quantum coordinates are used to dig the mathematical
general term between control or output bits to quickly
model the network. And this mathematical formula can be
obtained by induction due to the regular distribution of the

5

Fig. 2. Circuit Model of QSAN. Step 1, 3 and 6 are dedicated to calculate the query quantum state, the key quantum state and the value quantum
state, respectively. Steps 2 and 4 are barbell operations and are designed to swap with the corresponding garbage registers. Step 5 is the QLS
module to compute the QLS elements, which will produce the by-product QBSASM. Step 7 is the entanglement compression operation, which will
reduce the measurements. Step 8 is the slicing operation for calculating the ﬁnal weights.

⊗n is shown in Fig. 3 in the order provided by Eq.

where Uq
(19).

network. Based on the above deﬁnition, it is even possible
to derive the coordinates of the entire network. Then the
whole quantum network will be displayed in the form of co-
ordinate points or can be generalized in a generalized term
formula, which enhances the interpretability of the network.
The induction by means of coordinate points or generalized
terms may provide a feasible solution for quantum circuit
optimization. Later on, the charm of quantum coordinates
will be exhibited.

Here, a CNOT gate coordinate law applicable to this
project is extracted, which will subsequently perform a
crucial role. In the same register, the quantum coordinate
of the CNOT gate is

CN OT [s(r), s(r) + 1]

(25)

where

s(r) = m

r

−

×

r mod (m

m

1

−

1)

−

+ r mod (m

1)

(26)

−

is a general term formula with respect to r. This expression
is more concise. The logical function it implies is to XOR
the s(r)-th and (s(r) + 1)-th in the same register. The value
range of r depends on the situation.

4.3 Quantum Circuit

Step 1: calculate the query quantum state
Eq. (16). The procedure is as follows.

Q

|

i

according to

Uq

⊗nIn, In, In, 0

|

=

|

i

Q, In, In, 0

,
i

(27)

Fig. 3. Circuit for Uq or Uk or Uv

Step 2: perform a barbell operation. The barbell opera-
tion, which gets its name from the module’s form factor, ac-
tually swaps the input value of the second garbage register
with the current value of the input register. This operation
causes the input register to be reset and the result
to be
saved in the second garbage register. The exact procedure is
explained by the following equation:

Q

i

|

SW AP ⊗(m×n)

Q, In, In, 0

(28)
where SW AP ⊗(m×n) as shown in Fig. 4 indicates that
SWAP gates must be used for each dimension of each word

In, Q, In, 0

,
i

=

i

|

|

vector.

6

Fig. 4. Circuit for barbell operation

Step 3: calculate the key quantum state

according to
Eq. (17). The details are shown in Fig. 3. The mathematical
equation is expressed as

i

|

K

Uk

⊗nIn, Q, In, 0

=

K, Q, In, 0

|

i
Step 4: perform a barbell operation. This time the present
data of the input register is exchanged with the content of
the third register:

|

(29)

.
i

Fig. 5. Circuit for QLS module

SW AP ⊗(m×n)

K, Q, In, 0

=

|

i

In, Q, K, 0

.
i

|

(30)

The above two steps complete the whole operation steps

Step 5: calculate the QLS according to Eq. (20). The

of QLS:

details are drawn in Fig. 5.

Firstly, the AND operation is conducted on the qubits in
K
, and the result is stored in
i

Q
the same position of
i
the last garbage register:

and

|

|

⊗(m

n
P
i=1

i)

T of f oli

In, Q, K, 0

|

Using the coordinates, Q

∧

=

In, Q, K, Q

|

i
K is deﬁned as

K

.
i

∧

(31)

K :=

Q

∧

⊗r,j

T of f oli[p1(r), p2(r), p3(r, j)]

(32)

where

p1(r) = r + m

n,

×

p2(r) = r + 2m

n,

×

n−1

n−1−⌊r/m⌋

(33)

(34)

p3(r, j) = r +m

×

j +m(

c

−

d)+3m

n (35)

×

Xc=1
1

Xd=1
0,

n

×

, m

· · ·
×

0,
∈ {
n, 2m

×
−
n and 3m

with r
}
m
×
register the current control or output bit is in, e.g. m
means it is in the ﬁrst garbage register and 2m
it is in the second garbage register.

and j
, n
.
}
n are biases for locating which
n
n represents

r/m

∈ {

−⌊

⌋−

· · ·

×

×

1

Secondly, the CNOT gates are applied to the fourth
garbage register to acquire the eventual result of LQS.
According to the law summarized in Eq. (25), the process
of applying a CNOT gate at this point is deﬁned as

CN OT [s(r) + 3m

⊗r

n, s(r) + 3m

n + 1],

(36)

×

0,

with r
· · ·
located by adding bias 3m
P

, (m

∈ {

1)

−

×
n
i=1 i
n.

×

. The fourth register can be

}

In, Q, K,

K

Q
h

|
But the fact is that the outputs (s(r) + 3m
do not all need to be concerned.

ii

|

(37)

n + 1) of QLS

×

g(o) = m

o

1 + 3m

n

(s(r) + 3m

∈

×

1,

· · ·

∈ {

−
,

n
i=1 i

with o

×
is picked as the true QLS output.
Once the effective outputs g(o) of QLS are available, the
distribution of the outputs can be accessed by programmati-
cally querying the density matrix of g(o), i.e., the by-product
QBSASM.

P

×

}

n + 1)

(38)

Step 6: calculate the key quantum state

Eq. (18) and Fig. (3):

V

|

i

according to

Uv

⊗nIn, Q, K,

|

K

Q
h

|

ii

=

|

V, Q, K,

K

Q
h

|

ii

(39)

Step 7: The entanglement compression operation, as
shown in Fig. 6, means that the output is compressed
to the last word vector output of the input register after
entanglement by CNOT gates to reduce the number of
measurements.

CNOT gates are added for

. The speciﬁc way of
i
adding CNOT is executed according to Eq. (15) and Eq. (23).
Speciﬁcally,

|

V

CN OT ⊗(m×n)

=

V

|

i

m−1

⊗i=0

n−1
⊕j=0 Vi,i+m×j

if

Vii

|

is written as

Vii

|

=

|Vi,0 · · · Vi,m−1i

(40)

(41)

where

indicates the j-th qubit of the i-th word vector.
Step 8: execute the slicing operation as shown in Fig. (7)

|Vi,ji

and select the control bits in accordance with Eq. (24).

word vector can be formed, which also conforms to the
reality that the output has 1 more dimension than the input.
If the dimensionality is to be guaranteed to be the same, a
layer of neural network can be used.

7

5 EXPERIMENT AND DISCUSSION

This section implements the simulation of QSAN using IBM
Qiskit and pennylane.

Preparation Currently qubits are severely limited, so an
attempt is made to verify the feasibility of this scheme with
a small homemade data sample. Firstly, 2 classical word
vectors follow Transformer’s practice and form a new set
of samples by positional encoding [1]. Each new sample is
re-characterized with 2 qubits as the input of QSAN.

Fig. 8. Training results of QSAN: the maximum number of iterations is
500, the convergence accuracy is 1e-06, and the step size is 0.115.

Training Once the simple dataset is available, QSAN
starts randomly assigning initial angles. It then carries out
training following the quantum natural gradient training
rule instead of the classical gradient descent law [33]. The
expectation function in this paper is deﬁned as

θ

=

H

A
i
h

θ
h
where the Hamiltonian H is equal to Z2Z3Z4Z5 (the sub-
script here indicates the line number of the quantum circuit),
that is, the expectation on the Pauli operator Z for the
observation of lines 2 to 5. The outcomes of quantum natural

(43)

i

|

|

Fig. 6. Circuit for entanglement compression operation

Fig. 7. Circuit for slicing operation

First of all, for Fig. (7), the Reset operation [46] must be
performed before applying the multi-controlled quantum
gates, as the original output is not allowed to have any
further effect on the present result. Secondly, the relation-
Qj1 |
of QBSASM and the
ship between the element
h
coordinate g(o) is explored, where j1 is the row number
and j2 is the column number.

Kj2 i

Observing Eq. (24) and Eq. (38), it is found that the
weight matrix is a symmetric matrix and has the following
relationship with the parameter o of Eq. (38):

1 +

n

i

n−j1

j + j2

−

j1 ≤

j2

i=1
P

j=1
P
i

j

−

n−1

(42)

n−1−j2

i=1
P

j1 > j2

o = 

j1 + 1 +

j1}
When j1 ≤
j2, j1 ∈ {
1,
otherwise j1 ∈ {
· · ·
In this way, the equivalence between the coordinates of
the quantum gate and the positions of the elements of the
weight matrix is established, then the coordinates of the
quantum gate can be conﬁrmed by retrieving the positions
of the corresponding elements.

j=1
P
, n
1
· · ·
−
}
, j2 ∈ {
1
−

, j2 ∈ {
0,
· · ·

0,
, j1}

0,
, n

· · ·
.

, n

−

}

;

Measurements: Combined measurements. This step is
measured with skill. Choosing the full output qubit of Eq.
(40) and one of the qubits in Eq. (38), the corresponding

8

n
o
i
t
a
r
e
t
i

t
s
r
i
F

n
o
i
t
a
r
e
t
i

t
s
a
L

Fig. 9. Quantum attention score matrix

gradient descent and gradient descent training are shown in
Fig. 8.

In Fig. 8, the ﬁrst ﬁgure explains the convergence of
QSAN during the training process, concluding that the
quantum natural gradient descent method converges faster
and helps avoid the optimization from falling into local
minima. The second ﬁgure shows the evolution of the 12
parameter angles during quantum natural gradient descent.
QBSASM Due to the extension of the classical attention
score to a quantum state, the QBSASM is thus formed,
giving the classical attention score a probabilistic character
while being higher in dimensionality. At ﬁrst, the self-
attentive fraction presents a random state due to the random
assignment of the initialization angle, as shown in the upper
part of Fig. 9. After the quantum natural gradient descent,
in the last round, a completely new attention distribution is
obtained by intercepting this matrix, as in the lower part of
Fig. 9. It is worth mentioning that the speciﬁc scores need
to be known by measurement due to the presence of prob-
abilistic properties, which means that by measurement, the
QBSASM also collapses to some speciﬁc classical attention
score matrix.

Discussion QSAN is difﬁcult for ordinary computers
to emulate because just one QBSASM consumes a large

amount of storage, which is the storage advantage of quan-
tum computers. In addition, QSAN uses logical operations
between qubits instead of taking quantum numerical opera-
tions, which helps to save the qubits needed to build QSAN.
However, whether more qubits can be saved for QSAN
is an open question. Quantum coordinates are utilized in the
paper, and since it can facilitate the construction of quantum
networks with similar structure repetition, whether it will be
a subject of further optimization of the structure is worthy
of deeper investigation. Establishing a complete theoretical
system of quantum coordinates, and a series of coordinate
operations, decomposition, and merging laws may give a
more concise form to QSAN.

Further, QSAN, as an important component of machine
learning, is merging with machine learning models, such as
forming the new Quantum Transformer. whether Quantum
Transformer will have a secondary acceleration to the clas-
sical model is the next topic of this paper.

6 CONCLUSION

In this paper, a novel
linearized and reversible QSAM
theoretical framework and its practical model QSAN are
proposed. QSAM consists of two major parts, QLS and

 
 
QBSASM, where QLS replaces the practice of inner product
similarity and avoids the construction of large quantum
numerical operation networks, thus saving more qubits and
making QSAN fully deployable on quantum computers,
and the by-product QBSASM can be obtained during the
evolution of QSAN to present quantum attention distribu-
tion in the form of density matrix. QSAN is a practical
model of QSAM, being divided into 9 speciﬁc steps and
5 special quantum subcircuits, which can obtain QBSASM
during the evolution, and the reduction of the number of
measurements. It is worth mentioning that QSAN belongs to
a network structure with regular layout, and quantum coor-
dinates are able to obtain mathematical connections between
quantum gate control bits and output bits by induction,
which enables to describe the network with mathematical
formulas, facilitating the programming implementation and
possibly laying the foundation for optimization. By con-
structing miniaturized samples as input and experiment-
ing with IBM qiskit and Pennylane, we prove that QSAN
is trained faster with quantum natural gradient descent
than classical gradient descent and observe the evolution
of quantum attention. In addition, QSAN, as an extensible
module, can be embedded into classical or quantum ma-
chine learning architectures, facilitating the construction of a
quantum version of Transformer and laying the foundation
for quantum-enhanced NLP.

ACKNOWLEDGMENT

We would like to thank all the reviewers who provided
valuable suggestions.

REFERENCES

[1] Ashish Vaswani, Noam Shazeer et al., “Attention is all
you need,” Advances in Neural Information Processing
Systems, vol. 30, 2017.

[2] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang, “Disan: Directional self-
attention network for RNN/CNN-free language under-
standing,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 2018, pp. 5446-5455.

[3] J. Shi, Z. Li, W. Lai, F. Li, R. Shi, Y. Feng, and S.
Zhang, “Two end-to-end quantum-inspired deep neural
networks for text classiﬁcation,” IEEE Transactions on
Knowledge and Data Engineering, pp. 1-1, 2021.

[4] Z. Zhang, Y. Wu, J. Zhou, S. Duan, H. Zhao, and R.
Wang, “SG-Net: Syntax guided Transformer for lan-
guage representation,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 44, no. 6, pp.
3285-3299, 2022.

[5] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and
Ming Liu, “Neural speech synthesis with Transformer
network,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 2019, pp. 6706-6713.

[6] F. Huang, X. Li, C. Yuan, S. Zhang, J. Zhang, and S. Qiao,
“Attention-emotion-enhanced convolutional LSTM for
sentiment analysis,” IEEE Transactions on Neural Net-
works and Learning Systems, pp. 1-14, 2021.

[7] B. Shi, M. Yang, X. Wang, P. Lyu, C. Yao, and X. Bai,
“Aster: An attentional scene text recognizer with ﬂexible

9

rectiﬁcation,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 41, no. 9, pp. 2035-2048, 2019.
[8] Z. Zhu, T. Huang, M. Xu, B. Shi, W. Cheng, and X.
Bai, “Progressive and aligned pose attention transfer for
person image generation,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 44, no. 8, pp.
4306-4320, 2022.

[9] M. Liao, Z. Zou, Z. Wan, C. Yao, and X. Bai, “Real-
time scene text detection with differentiable binarization
and adaptive scale fusion,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, pp. 1-1, 2022.

[11] Yunyang

[10] Y. Bin, Y. Yang, F. Shen, N. Xie, H. T. Shen, and X.
Li, “Describing video with attention-based bidirectional
LSTM,” IEEE Transactions on Cybernetics, vol. 49, no. 7,
pp. 2631-2641, 2019.
Xiong,

Rudrasis
Zhanpeng
Chakraborty, Mingxing Tan, Glenn Fung, Yin Li,
and Vikas
“Nystr¨omformer: A nystr¨om-
based algorithm for approximating self-attention,”
in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 2021, pp. 14138-14148.

Singh,

Zeng,

[12] Krzysztof Marcin Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mo-
hiuddin, and Lukasz Kaiser, “Rethinking attention with
performers,” in International Conference on Learning
Representations, 2020.

[13] Y. Kashiwagi, E. Tsunoo, and S. Watanabe, “Gaussian
kernelized self-attention for long sequence data and its
application to ctc-based speech recognition,” in ICASSP
2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 6214-
6218.

[14] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya,
“Reformer: The efﬁcient Transformer,” in International
Conference on Learning Representations, 2019.

[15] Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever, “Generating long sequences with sparse
Transformers,” arXiv preprint arXiv:1904.10509, 2019.
[16] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier, “Efﬁcient content-based sparse atten-
tion with routing Transformers,” Transactions of the
Association for Computational Linguistics, vol. 9, pp.
53-68, 2021.

[17] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang,
Xilin Chen, and Jingdong Wang, “Ocnet: Object con-
text for semantic segmentation,” International Journal
of Computer Vision, vol. 129, no. 8, pp. 2375-2398, 2021.
[18] S. Zhuoran, Z. Mingyuan, Z. Haiyu, Y. Shuai, and L.
Hongsheng, “Efﬁcient attention: Attention with linear
complexities,” in 2021 IEEE Winter Conference on Ap-
plications of Computer Vision (WACV), 2021, pp. 3530-
3538.

[19] Michael Hahn, “Theoretical limitations of self-attention
in neural sequence models,” Transactions of the Associ-
ation for Computational Linguistics, vol. 8, pp. 156-171,
2020.

[20] Frank Arute, Kunal Arya , et al., “Quantum supremacy
using a programmable superconducting processor,” Na-
ture, vol. 574, no. 7779, pp. 505-510, 2019.

[21] Han-Sen Zhong, Hui Wang et al., “Quantum computa-

tional advantage using photons,” Science, vol. 370, no.
6523, pp. 1460-1463, 2020.

approximate optimization algorithm,” arXiv: Quantum
Physics, 2014.

10

[37] Stuart Hadﬁeld, Zhihui Wang, Bryan O’gorman,
Eleanor G. Rieffel, Davide Venturelli, and Rupak Biswas,
“From the quantum approximate optimization algo-
rithm to a quantum alternating operator Ansatz,” Al-
gorithms, vol. 12, no. 2, 2019.

[38] M. E. S. Morales, J. D. Biamonte, and Z. Zimbor´as, “On
the universality of the quantum approximate optimiza-
tion algorithm,” Quantum Information Processing, vol.
19, no. 9, pp. 291, 2020.

[39] John Preskill, “Quantum computing in the NISQ era

and beyond,” Quantum, vol. 2, pp. 79, 2018.

[40] Guangxi Li, Xuanqiang Zhao, and Xin Wang, “Quan-
tum self-attention neural networks for text classiﬁca-
tion,” arXiv preprint arXiv:2205.05625, 2022.

[41] Qiskit Development Team. ”Realamplitudes documen-
tation,” https://qiskit.org/documentation/stubs/qiski-
t.circuit.library.RealAmplitudes.html.

[42] Harry Buhrman, Richard Cleve, John Watrous, and
Ronald De Wolf, “Quantum ﬁngerprinting,” Physical
Review Letters, vol. 87, no. 16, pp. 167902, 2001.

[43] Dorit Aharonov, Vaughan Jones, and Zeph Landau, “A
polynomial quantum algorithm for approximating the
jones polynomial,” Algorithmica, vol. 55, no. 3, pp. 395-
421, 2009.

[44] Carlos Ortiz Marrero, M´aria Kieferov´a, and Nathan
Wiebe, “Entanglement-induced barren plateaus,” PRX
Quantum, vol. 2, no. 4, pp. 040316, 2021.

[45] Jarrod R. Mcclean, Sergio Boixo, Vadim N. Smelyan-
skiy, Ryan Babbush, and Hartmut Neven, “Barren
plateaus in quantum neural network training land-
scapes,” Nature Communications, vol. 9, no. 1, pp. 4812,
2018.

[46] Qiskit Development Team. ”Summary of quantum op-
erations,” https://qiskit.org/documentation/tutorials/
circuits/3 summary of quantum operations.html.
[47] M. Weigold, J. Barzen et al., “Expanding data encoding
patterns for quantum algorithms,” in 2021 IEEE 18th
International Conference on Software Architecture Com-
panion (ICSA-C), 2021, pp. 95-101.

[22] Lars S. Madsen, Fabian Laudenbach et al., “Quantum
computational advantage with a programmable pho-
tonic processor,” Nature, vol. 606, no. 7912, pp. 75-81,
2022.

[23] Man-Hong Yung, “Quantum supremacy: Some funda-
mental concepts,” National Science Review, vol. 6, no. 1,
pp. 22-23, 2019.

[24] Aram W. Harrow, and Ashley Montanaro, “Quantum
computational supremacy,” Nature, vol. 549, no. 7671,
pp. 203-209, 2017.

[25] Xiaolei Niu, Yuexian Hou, and Panpan Wang, “Bi-
directional LSTM with quantum attention mechanism
for sentence modeling,” in Neural Information Process-
ing, Cham, 2017, pp. 178-188.

[26] Qin Zhao, Chenguang Hou, and Ruifeng Xu, “Quan-
tum attention based language model for answer selec-
tion,” in Artiﬁcial Intelligence and Mobile Services –
AIMS 2021, Cham, 2022, pp. 47-57.

[27] Alberto Peruzzo, Jarrod Mcclean, Peter Shadbolt, Man-
Hong Yung, Xiao-Qi Zhou, Peter J. Love, Al´an Aspuru-
Guzik, and Jeremy L. O’brien, “A variational eigenvalue
solver on a photonic quantum processor,” Nature Com-
munications, vol. 5, no. 1, pp. 4213, 2014.

[28] J. Shi, Y. Tang, Y. Lu, Y. Feng, R. Shi, and S. Zhang,
“Quantum circuit learning with parameterized boson
sampling,” IEEE Transactions on Knowledge and Data
Engineering, pp. 1-1, 2021.

[29] Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mat-
tia Fiorentini, “Parameterized quantum circuits as ma-
chine learning models,” Quantum Science and Technol-
ogy, vol. 4, no. 4, pp. 043001, 2019.

[30] Lukasz Cincio, Kenneth Rudinger, Mohan Sarovar, and
Patrick J. Coles, “Machine learning of noise-resilient
quantum circuits,” PRX Quantum, vol. 2, no. 1, pp.
010324, 2021.

[31] Jonas M K ¨ubler, Andrew Arrasmith, Lukasz Cin-
cio, and Patrick J Coles, “An adaptive optimizer for
measurement-frugal variational algorithms,” Quantum,
vol. 4, pp. 263, 2020.

[32] Ryan Sweke, Frederik Wilde, Johannes Meyer, Maria
Schuld, Paul K F¨ahrmann, Barth´el´emy Meynard-
Piganeau, and Jens Eisert, “Stochastic gradient descent
for hybrid quantum-classical optimization,” Quantum,
vol. 4, pp. 314, 2020.

[33] James Stokes,

Josh Izaac, Nathan Killoran, and
Giuseppe Carleo, “Quantum natural gradient,” Quan-
tum, vol. 4, pp. 269, 2020.

[34] Abhinav Kandala, Antonio Mezzacapo, Kristan
Temme, Maika Takita, Markus Brink, Jerry M. Chow,
and Jay M. Gambetta, “Hardware-efﬁcient variational
quantum eigensolver for small molecules and quantum
magnets,” Nature, vol. 549, no. 7671, pp. 242-246, 2017.
[35] Nikolay V. Tkachenko, James Sud, Yu Zhang, Sergei
Tretiak, Petr M. Anisimov, Andrew T. Arrasmith, Patrick
J. Coles, Lukasz Cincio, and Pavel A. Dub, “Correlation-
informed permutation of qubits for reducing Ansatz
depth in the variational quantum eigensolver,” PRX
Quantum, vol. 2, no. 2, pp. 020337, 2021.

[36] E. Farhi, J. Goldstone, and S. Gutmann, “A quantum

This figure "thumbnail.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/2207.07563v3

