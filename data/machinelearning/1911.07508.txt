Safe Squeezing for Antisparse Coding

Cl´ement Elvira and C´edric Herzet

1

Abstract—Spreading the information over all coefﬁcients of
a representation is a desirable property in many applications
such as digital communication or machine learning. This so-
called antisparse representation can be obtained by solving a
convex program involving an (cid:96)∞-norm penalty combined with a
quadratic discrepancy. In this paper, we propose a new method-
ology, dubbed safe squeezing, to accelerate the computation of
antisparse representation. We describe a test that allows to detect
saturated entries in the solution of the optimization problem. The
contribution of these entries is compacted into a single vector,
thus operating a form of dimensionality reduction. We propose
two algorithms to solve the resulting lower dimensional problem.
Numerical experiments show the effectiveness of the proposed
method to detect the saturated components of the solution and
illustrates the induced computational gains in the resolution of
the antisparse problem.

Index Terms—antisparse

coding,

safe

screening,

scaled

projected-gradient algorithm, Frank-Wolfe algorithm.

I. INTRODUCTION

central element

I N the last decades, convex optimization has become a

in the resolution of inverse problems.
This success revolves around two main ingredients. First, it
has been shown that proper “regularizing” functions enforce
desirable properties on the solutions of convex problems. One
of the ﬁrst (and maybe most striking) example of such a
behavior is the use of “(cid:96)1-norm” regularization, promoting
sparsity of the solutions. The second ingredient which sparked
the success of convex optimization is the advent of numerical
procedures able to solve efﬁciently (up to some accuracy)
problems involving thousands to billions of variables. To name
a few, let us mention the Augmented Lagrangian methods [1],
the forward-backward splitting [2], or the Alternating direction
method of multipliers [3].

Of particular interest in this paper is an acceleration method
ﬁrst proposed by El Ghaoui et al. in [4] in the context of
sparsity-promoting convex problems, namely “safe screening”.
This procedure leverages two main elements. First, the so-
lutions of convex problems involving (cid:96)1 regularization are
typically sparse, i.e., contains a large number of zeros. For
example, the solutions of the well-known “Lasso” problem

0
2
0
2

r
a

M
8
2

]

G
L
.
s
c
[

2
v
8
0
5
7
0
.
1
1
9
1
:
v
i
X
r
a

x(cid:63) ∈ arg min
x∈Rn

1

2 (cid:107)y − Ax(cid:107)2

2 + λ(cid:107)x(cid:107)1,

(1)

where y ∈ Rm is an observation vector, A ∈ Rm×n a
representation matrix and λ a penalization parameter, are

C. Elvira and C. Herzet are with Univ Rennes, Inria, CNRS, IRISA F-35000

Rennes, France.

e-mail: prenom.nom@inria.fr.
Part of this work has been funded thanks to the Becose ANR project no.

ANR-15-CE23-0021 and the Labex CominLabs.

The research presented in this paper is reproducible. Code and data are

available at https://gitlab.inria.fr/celvira/safe-squeezing.

known to contain at most m nonzero components, see [5,
Theorem 3.1] or [6, Theorem 3]. Second, if the position of
(some of) the zeros in x(cid:63)
λ are known, (1) can be transformed
into a problem of smaller dimension where the columns of A
associated to the position of known zeros are simply discarded.
Solving this reduced problem may then potentially result in
huge memory and computational savings. As far as sparsity-
promoting problems are concerned, the idea of safe screening
thus consists in designing simple tests allowing to detect zeros
of the solution.

Since the seminal work by El Ghaoui et al., safe screening
has been developed and improved in many contributions of the
literature, see e.g., [7–10]. Recently, this strategy has also been
extended to other families of regularizers [11–13]. However, to
the best of our knowledge, all these contributions have focused
so far on the resolution of “sparsity-promoting” problems,
where the solutions contain many zeros. In this paper, we
show that the principles ruling safe screening may be extended
to another family of convex optimization problems. More
speciﬁcally, we address the following optimization problem

x(cid:63) ∈ arg min
x∈Rn

1

2 (cid:107)y − Ax(cid:107)2

2 + λ(cid:107)x(cid:107)∞,

(2 − P ∞
λ )

where the admissible range of the coefﬁcients x is penalized
through an (cid:96)∞-norm. Here, the (cid:96)∞-norm is advocated for
spreading the information over all representation coefﬁcients
in the most uniform way. For this reason,
the solutions
of (2 − P ∞
λ ) are sometimes referred to as “antisparse” or
“spread” since, contrary to the Lasso problem, they are known
to be dense with many “saturated” entries satisfying

(3)

|x(cid:63)(i)| = (cid:107)x(cid:63)(cid:107)∞.
Although less popular than its “sparsity-promoting” (cid:96)1 coun-
terpart, the “spreading” property of the (cid:96)∞ norm has proved
to be of practical interest in several applicative domains, e.g.,
to design robust analog-to-digital conversion schemes [14,
15], reduce the peak-to-average power ratio in multi-carrier
transmissions [16–18], perform approximate nearest neighbor
search [19], outlier detection [20] or robust beamforming [21].
Besides, when combined with a set of linear equality con-
straints, minimizing an (cid:96)∞-norm is referred to as the minimum-
effort control problem in the optimal-control framework [22,
23].

From a numerical point of view, although many generic
convex optimization tools may be straightforwardly applied
to problem (2 − P ∞
λ ), the design of algorithms speciﬁcally
dedicated to this kind of problems has also focused less
attention than its (cid:96)1 counterpart. In this contribution, we
make one step in that direction by showing that the main
principles underlying “safe screening” may be extended to
problem (2 − P ∞
λ ). More speciﬁcally, we emphasize that

 
 
 
 
 
 
(2 − P ∞
λ ) can be reduced to a problem of smaller dimension
by identifying “saturated” components of the solution x(cid:63) and
propose simple tests to identify them. We refer to the proposed
methodology as “safe squeezing” since it allows to compact
several components of x(cid:63) into one single variable. We show
numerically that the proposed safe squeezing procedure can
signiﬁcantly reduce the computational complexity needed to
solve the antisparse problem (2 − P ∞

λ ) to some accuracy.

The rest of this paper is organized as follows. Section II
deﬁnes the main notations used in this work. In Section III,
we introduce the proposed safe squeezing test and discuss the
problem’s dimensionality reduction it allows. In Section IV
we describe two algorithms to solve the equivalent “reduced”
optimization problem. In Section V, we illustrate the perfor-
mance of the proposed methodology in numerical experiments.
Concluding remarks are ﬁnally made in Section VI.

II. NOTATIONS

Unless otherwise speciﬁed, we will use the following con-
vention throughout the paper. The vectors are denoted by
lowercase bold letters (e.g., x) and matrices by uppercase bold
letters (e.g., A); x(i) refers to the ith component of x and
ai to the ith column of A. Given q ∈ Rq and w ∈ R,
the shorthand notations “q ≤ w” should be understood as
“q(i) ≤ w ∀i”. The notations ker(A) and kruskal(A) will be
used to denote the null space and the Kruskal rank [5, p. 56]
of A, respectively.

Caligraphic letters are used to denote sets (e.g., I). ¯I stands
for the complementary set of I and card(I) refers to the
cardinality of I.

Given a vector x ∈ Rn and a set of indices I ⊆ {1, . . . , n},
we let xI be the vector of components of x with indices in
I. Similary, AI denotes the submatrix of A whose columns
have indices in I. Finally, σmin(A) and σmax(A) refer to the
lowest and highest eigenvalue of A, respectively.

III. DETECTING AND EXPLOITING SATURATION

A. Working hypotheses

In the rest of this paper, we will assume that the following

working hypothesis is veriﬁed:
0 < λ < (cid:13)

(cid:13)ATy(cid:13)
(cid:13)1.

(4)

The left inequality is natural since letting λ = 0 is tantamount
to removing the penalization of problem (2 − P ∞
λ ). On the
other hand,
the solution
the right
of problem (2 − P ∞
λ ) is not the all-zero vector 0n. More
speciﬁcally, it can be shown that

inequality ensures that

(cid:107)x(cid:63)(cid:107)∞ > 0 ⇐⇒ λ veriﬁes (4).
A proof of this result is available in Appendix A. Hypothe-
sis (4) implies in particular that y /∈ ker(AT) since the two
inequalities cannot be satisﬁed simultaneously in the opposite
case. In the sequel, we will always assume that (4) holds
although not explicitly mentioned in our statements.

(5)

We will also assume that (2 − P ∞

λ ) admits a unique mini-
mizer x(cid:63). Although our subsequent derivations may be quite
easily extended to the general case, this working hypothesis

2

greatly simpliﬁes the exposition of our procedure. We will
add different comments all along our presentation to point out
how the non-uniqueness of the solutions of (2 − P ∞
λ ) would
modify the quantities and results at stake.

B. Dimensionality reduction via saturation detection

In this section, we illustrate how the knowledge of the
positions of saturated entries in x(cid:63) can lead to memory and
complexity savings in the resolution of (2 − P ∞

λ ). Let1

I (cid:63)
+
I (cid:63)
−

(cid:44) {i : x(cid:63)(i) = +(cid:107)x(cid:63)(cid:107)∞},
(cid:44) {i : x(cid:63)(i) = −(cid:107)x(cid:63)(cid:107)∞},

(6a)

(6b)

be the sets of positive and negative saturated components of
x(cid:63), and I (cid:63) = I (cid:63)
+. Then, for any I = I+ ∪ I− with I+ ⊆
I (cid:63)
+ and I− ⊆ I (cid:63)
λ ) can be equivalently
rewritten as

−, problem (2 − P ∞

− ∪ I (cid:63)

arg min
(w,q)∈R×Rn−card(I)
subject to q ≤ w, −q ≤ w

1

2 (cid:107)y − A ¯Iq − s w(cid:107)2

2 + λw

(w(cid:63), q(cid:63)) ∈

where

(7 − P sq
λ )

(8)

s (cid:44) (cid:88)

ai −

(cid:88)

ai.

i∈I+

i∈I−

In particular, the following bijection holds between the solu-
λ ) and (7 − P sq
tions of (2 − P ∞
λ ):
¯I = q(cid:63)
x(cid:63)
(cid:40)

(9)

x(cid:63)(i) =

−w(cid:63)
+w(cid:63)

if i ∈ I−
if i ∈ I+.

(10)

In other words, any minimizer of (2 − P ∞
mizer of (7 − P sq

λ ), and vice-versa.

λ ) deﬁnes a mini-

The rationale behind problem (7 − P sq

λ ) is as follows. Since
any minimizer of (2 − P ∞
λ ) weighs (up to a sign) the elements
in I by the same value (namely (cid:107)x(cid:63)(cid:107)∞), the signed summation
of the columns of AI can be done once for all in advance.
This is the meaning of vector s in (8). Variable w plays the
role of the largest absolute value of the elements of x. In
particular, the constraints “q ≤ w” and “−q ≤ w” ensure that
the absolute value of the coefﬁcients weighting the elements
of A ¯I is no larger than w.

λ ) and (7 − P sq

We note that, although deﬁning the same set of solutions, the
dimensionality of problems (2 − P ∞
λ ) can be
quite different: whereas the former manipulates n-dimensional
variables, the latter only involves an optimization space of
dimension n − card(I) + 1. As a limit case, when I = I (cid:63),
the size of problem (7 − P sq
λ ) drops down to n−card(I (cid:63))+1.
The next lemma shows that the dimensionality reduction can
be drastic in this case:2

Lemma 1. If kruskal(A) = m then card(I (cid:63)) ≥ n − m + 1.

Lemma 1 is a direct consequence of Theorem 1 below. The
hypothesis “kruskal(A) = m” holds as soon as any group

1If x(cid:63) is not unique, I(cid:63) must be understood as the set of indices of the

components saturating for all the solutions of (2 − P∞

λ ).

2A related result is stated in [24, Sec. 4.1].

of m columns of A ∈ Rm×n is linearly independent [5,
p. 56]. This is, for example, a typical setup in machine learning
applications where the columns of matrix A contain randomly
generated features.3 In this case, we see from Lemma 1 that the
dimension of problem (7 − P sq
λ ) is no larger than m if I = I (cid:63).
In the overcomplete setting, when m (cid:28) n, solving (7 − P sq
λ )
instead of (2 − P ∞
λ ) then leads to dramatic dimensionality
reduction.

A decrease of the size of the problem leads de facto to
a reduction of the storage and computational costs necessary
to evaluate the solution. Tackling problem (7 − P sq
λ ) instead
of (2 − P ∞
λ ) may thus be obviously of interest to save com-
putational resources. The relevance of (7 − P sq
λ ) is however
conditioned to the identiﬁcation of a (large) subset of I (cid:63). In
the rest of this section, we propose a procedure, dubbed “safe
squeezing”, to perform efﬁciently this task. The term “safe”
refers to the fact that all the indices identiﬁed by our procedure
necessarily belong to I (cid:63). The term “squeezing” relates to
the dimensionality reduction allowed by the identiﬁcation of
(some of) the elements of I (cid:63).

C. Safe squeezing test

In this section, we provide the main ingredient of our safe
squeezing procedure. It takes the form of a simple test ensuring
that an index i belongs to I (cid:63), and is grounded on the following
result:

Theorem 1. Let I ⊆ I (cid:63) and
2 (cid:107)y(cid:107)2

u(cid:63) = arg max

1

u∈UI

2 − 1

2 (cid:107)y − u(cid:107)2

2

(11-D∞
λ )

(cid:13)1 + sTu ≤ λ(cid:9)

(12)

so that

where

(cid:13)AT

UI (cid:44) (cid:8)u : (cid:13)

I u(cid:13)
and s is deﬁned as in (8). Then,
∀i : (cid:12)

i u(cid:63)(cid:12)

(cid:12)aT

Moreover, if kruskal(A) = m, aT
ai’s.

(cid:12) > 0 ⇒ i ∈ I (cid:63)

sign(aT

i u(cid:63)).
i u(cid:63) (cid:54)= 0 for at least n−m+1

(13)

(cid:12) > 0.

A proof of this result is available in Appendix B. Theorem 1
provides an appealing way of identifying saturated components
in x(cid:63): we have from (13) that i ∈ I (cid:63) as soon as ai
is not orthogonal to some vector u(cid:63) deﬁned in (11-D∞
λ ).
Moreover, if kruskal(A) = m, then at least n − m + 1
saturated components of x(cid:63) can be identiﬁed by verifying that
(cid:12)
(cid:12)aT

i u(cid:63)(cid:12)
Unfortunately, in practice, ﬁnding a maximizer of (11-D∞
λ )
turns out to be as difﬁcult as solving our target optimiza-
tion problem (7 − P sq
λ ). In particular, if I = ∅, addressing
(11-D∞
λ ) requires the same order of complexity as solv-
ing (2 − P ∞
λ ). Hence, the direct use of (13) is of poor interest
to save computational resources. Nevertheless, we emphasize
below that relaxed versions of (13) can be devised to identify
subsets of I (cid:63) with a low computational burden.

3More particularly, this is the case with probability 1 when the columns of
A are drawn according to any distribution that admits a density with respect
to the Lebesgue measure.

3

Before giving a detailed description of the proposed method,
we make two important remarks about u(cid:63). First, we see from
λ ) that u(cid:63) is deﬁned as a maximizer of a feasible4 and
(11-D∞
strictly-concave problem with a continuous and coercive cost
function. Hence, u(cid:63) always exists [25, Propositions A.8] and is
unique [25, Propositions B.10]. This justiﬁes the equality sign
λ ). In fact, as shown in Appendix B-A, (11-D∞
used in (11-D∞
λ )
corresponds to the Lagrangian dual of problem (7 − P sq
λ ).

Second, the value of u(cid:63) does not depend on the particular
subset I ⊆ I (cid:63) considered in (11-D∞
λ ). This fact is only stated
here for the sake of keeping the discussion at a reasonable
level of complexity, but is proved in Appendix B. We note
that, although the value of u(cid:63) is independent of I, there may
be some computational advantages in considering I’s with
large numbers of elements. In particular, we emphasize in
Section III-D below that the computational cost of the “dual
scaling” operation, needed to implement the proposed safe
squeezing test, evolves linearly with card(¯I).

We are now ready to expose the main building block of our
safe squeezing procedure. Let S be some subset of Rm such
that

u(cid:63) ∈ S.

(14)

For now, we only assume that some S verifying (14) is
available. We will see in the next section how to construct such
a region. A region satisfying (14) is usually referred to as “safe
region” in the screening literature, and the same convention
will be used hereafter.

We note that since u(cid:63) ∈ S, we have
aT
i u < aT
aT
i u > aT

min
u∈S
max
u∈S

i u(cid:63)
i u(cid:63),

min
u∈S
max
u∈S

aT
i u > 0 ⇒ aT
aT
i u < 0 ⇒ aT

i u(cid:63) > 0
i u(cid:63) < 0.

Using (13), we thus obtain

min
u∈S
max
u∈S

aT
i u > 0 ⇒ i ∈ I (cid:63)
+
aT
i u < 0 ⇒ i ∈ I (cid:63)
−.

Hence, if the maximum and minimum values of aT
i u over
S are easy to evaluate, the left-hand side of (17) provides
a simple test to determine whether i belongs to I (cid:63)
+ or I (cid:63)
−.
In particular, considering a safe region S with a spherical
geometry, that is

S = B(c, r) (cid:44) {u ∈ Rm : (cid:107)u − c(cid:107)2 ≤ r},

(18)

for some c ∈ Rm and r > 0, leads to

min
u∈B(c,r)
max
u∈B(c,r)

i u = aT
aT
i u = aT
aT

i c − (cid:107)ai(cid:107)2r
i c + (cid:107)ai(cid:107)2r.

(19)

Plugging these expressions into (17) yields the following
result:

4We always have u = 0m ∈ UI since λ > 0 by hypothesis.

(15)

(16)

(17)

(20)

sign(aT

i c).

(cid:12)
(cid:12)aT

i c(cid:12)

Theorem 2 (Safe sphere squeezing test). If u(cid:63) ∈ B(c, r), then
(cid:12) > r (cid:107)ai(cid:107)2 ⇒ i ∈ I (cid:63)
Squeezing test (20) provides a very practical way of iden-
tifying some elements of I (cid:63). In particular, testing whether
i ∈ I (cid:63) only requires to evaluate one inner product between
ai and the center c of a safe sphere. We note however that
passing (20) is only a sufﬁcient condition for i ∈ I (cid:63). In
practice, depending on the choice of the center and the radius
of the safe region, some elements of I (cid:63) may not be identiﬁed
by the proposed safe squeezing test. As a general rule of
thumb, “small” safe regions will lead to squeezing tests able
to identify more saturated components. We will illustrate this
behavior in our numerical results in Section V. We elaborate
on the construction of “good” safe sphere regions in the next
subsection. For now, let us just notice that if c = u(cid:63) and
r = 0, S = B(c, r) is safe and its volume is equal to zero. In
this case, the safe squeezing test (20) boils down to (13). In
particular, provided that kruskal(A) = m, at least n − m + 1
saturated components of x(cid:63) can be identiﬁed (see last part of
Theorem 1).

As a ﬁnal remark, let us mention that Theorem 2 cor-
responds to one speciﬁc particularization of (17) (i.e.,
to
spherical regions S). As with procedures derived for Lasso
screening, it is also possible to devise squeezing tests based on
safe regions having more reﬁned geometries (e.g., dome [26]
or truncated dome [27]). The extension of our safe squeezing
procedure to these geometries is however left for future work
and will not be considered hereafter.

D. Construction of safe spheres

A cornerstone of the safe squeezing methodology presented
in Section III-C is the identiﬁcation of a “safe” region, that
is a region S verifying (14). We elaborate on this problem
hereafter: we emphasize that several safe spheres derived in
the context of safe screening for Lasso can be reused for
safe squeezing with minor modiﬁcations. We focus here on
the spheres “ST1” and ”GAP” respectively proposed in [4]
and [7].

Our reasoning is based on two key observations. First,
the Lasso dual problem shares the same cost function as
problem (11-D∞
λ ) but with a deﬁnition of the dual feasible
set different from (12), see e.g., [9, Eq. (2)]. Second, the safe
spheres proposed in [4, 7] are based on convex optimality
conditions and the knowledge of some dual feasible point
u, but never on the speciﬁc deﬁnition of the dual feasible
set. Hence, the center and radius of the “ST1” and “GAP”
spheres proposed in [4] and [7] can be reused here with the
provisio that the dual variable u appearing in these expressions
is feasible for (11-D∞
λ ) (that is u ∈ UI).
For example, the ST1 safe sphere derives from the fact that,

by deﬁnition of u(cid:63):

∀u ∈ UI : (cid:107)y − u(cid:107)2 ≥ (cid:107)y − u(cid:63)(cid:107)2.

Hence, the sphere B(c, r) with

c = y
r = (cid:107)y − u(cid:107)2

(21)

(22)

4

is safe for problem (11-D∞
λ ). We see that expression of the
center and the radius in (22) are the same as those proposed
in [4] with the difference that u must here be feasible for
(11-D∞

λ ) rather than for the Lasso dual problem.

Similarly, the expression of the GAP safe sphere can be
extended as follows in the context of safe squeezing: for any
I ⊆ I (cid:63) and (w, q, u) verifying

q ≤ w, −q ≤ w, u ∈ UI,

(23)

the sphere B(c, r) with

c = u
r = (cid:112)2 gap(w, q, u)

and

gap(w, q, u) = 1

2 (cid:107)y − A ¯Iq − s w(cid:107)2
2 (cid:107)y(cid:107)2
2 − 1

2 + λw
2 (cid:107)y − u(cid:107)2
2)

− ( 1

(24)

(25)

is safe for problem (11-D∞
λ ). The proof of this result is similar
to that provided in [7] and is omitted here. As for the sphere
ST1, the expressions of the center and radius remain the same
as for the Lasso problem but the dual (resp. primal) variable(s)
coming into play must now be feasible for problem (11-D∞
λ )
(resp. (7 − P sq

λ )).

The only computational difﬁculty in evaluating (22) and
(24) stands in the identiﬁcation of a dual feasible point. We
may nevertheless identify such a point by extending the “dual
scaling” procedure proposed in [4, Section 3.3] to the present
setup. More precisely, given any z ∈ Rm, we clearly have
that

u = dualscal(z)

(26)

where

dualscal(z) =

(cid:40) z

λ
z(cid:107)1

(cid:107)AT
¯I

+sTz

¯I z(cid:13)
if (cid:13)
(cid:13)AT
(cid:13)1
z otherwise.

+ sTz ≤ 0,

is dual feasible. We note that the implementation of (26)
requires the computation of card(¯I)+1 inner products in Rm.

E. Static versus dynamic squeezing

Similarly to screening methods, the proposed safe squeezing
procedure can be used either in a “static” or a “dynamic” way.
Static squeezing refers to the case where (20) is applied
once for all (with I = ∅) on the columns of matrix A before
the application of a numerical optimization procedure. In such
a case, the dual feasible point u used to build the safe sphere
is commonly deﬁned as follows:

u = dualscal(y).

(27)

Static screening may for example be of interest to reduce the
problem’s dimensionality so that its size ﬁts the computer’s
memory requirements.

Dynamic squeezing consists in interleaving test (20) with
the iterations of an optimization procedure. Here, the goal
is to reﬁne the quality of the safe region and increase the
cardinality of I all along the optimization process. The general
principle of dynamic squeezing (applied to the resolution

5

Algorithm 1: Principle of dynamic squeezing
// Initialization
1 x(0) = 0n, I (0) = ∅
2 u(0) = dualscal(y)
3 t = 1 // Iteration index
4 repeat

// Squeezing test
(c(t), r(t)) = sphere param(x(t−1), u(t−1), I (t−1))
I (t−½) = squeezing test(c(t), r(t))
I (t) = I (t−½) ∪ I (t−1)

// Iterations of the optimization procedure
(x(t), u(t)) = optim update(x(t−1), u(t−1), I (t))

// Update iteration index
t = t + 1

5

6

7

8

9

10 until convergence criterion is met

Output: x(t), I (t)

Algorithm 2: Frank-Wolfe algorithm for problem (29).
Input: y, A ¯I, s, w, w0, q0

1 w(0) = w0, q(0) = q0 // Initialization
2 t = 1 // Iteration index
3 repeat

// Search of a feasible descent direction
w(t−½) = w
q(t−½) = w sign(AT

¯I (y − A ¯Iq(t−1) − s w(t−1)))

// Convex update
w(t) = γw(t−1) + (1 − γ)w(t−½)
q(t) = γq(t−1) + (1 − γ)q(t−½)

// Update iteration index
t = t + 1

4

5

6

7

8

9 until convergence criterion is met

Output: w(t), q(t)

of (2 − P ∞
λ )) is described in Algorithm 1. At each iteration,
the parameters of a safe sphere are evaluated by using a couple
a primal-dual feasible points (x(t−1), u(t−1)), see line 5. The
safe sphere is then exploited in the squeezing test (20) to
identify some set I (t−½) ⊆ I (cid:63), see line 6. In line 7, the
indices identiﬁed at the current iteration are merged to those
previously identiﬁed. Finally, the primal-dual feasible couple
is updated in line 8. This operation is typically carried out by
running a few iterations of a numerical procedure addressing
problem (7 − P sq
λ ) with I = I (t) and initialized with the
previous iterate (x(t−1), u(t−1)). In the common case where
the optimization method only returns a primal iterate x(t),
a feasible dual point can for example be computed by dual
scaling of z = y − Ax(t). By construction, we have

I (0) ⊆ I (1) ⊆ . . . ⊆ I (t)

(28)

the dimension of the reduced problem (7 − P sq
so that
λ )
considered in line 8 does not increase through the iterations
if (x(t), u(t)) converges to
of Algorithm 1. In particular,
(x(cid:63), u(cid:63)), the parameters of the GAP safe sphere converge to
(c = u(cid:63), r = 0) and, from Theorem 1, card(I (t)) ≥ n−m+1
after a sufﬁcient number of iterations when kruskal(A) = m.

IV. ALGORITHMIC SOLUTIONS FOR (7 − P sq
λ )

A crucial difference between “screening” and “squeezing” is
the nature of the optimization problems obtained after reduc-
tion. Whereas the reduced problem has the same mathematical
the
form as the original one in the context of screening,
reduced problem (7 − P sq
λ ) obtained after squeezing is struc-
turally different from (2 − P ∞
λ ). As a consequence, numerical
procedures devised to solve (2 − P ∞
λ ) (e.g., [18]) cannot be
used straightforwardly to address (7 − P sq
λ ).

In this section, we describe two algorithmic solutions for
problem (7 − P sq
λ ): in Section IV-A a numerical procedure
based on the Frank-Wolfe algorithm [25, Section 2.2] is
proposed; in Section IV-B, we present a strategy based on
a rescaled projected gradient method [25, Section 2.3]. These

procedures will be used in the next section to assess computa-
tional gain allowed by the proposed safe squeezing procedure.

A. Frank-Wolfe algorithm

Let us consider
λ ):

of (7 − P sq

min
(w,q)∈R×Rn−card(I)

the following equivalent

formulation

1

2 (cid:107)y − A ¯Iq − s w(cid:107)2

2 + λw
(cid:26) q ≤ w, −q ≤ w,

subject to

w ≤ w,

(29)

where w is some constant such that w(cid:63) ≤ w. We will discuss
the identiﬁcation of w at the end of this section. For now,
let us notice that the additional constraint “w ≤ w” does not
change the minimizers of the problem but makes the feasible
set compact. As consequence, the conditional gradient method,
also known as Frank-Wolfe algorithm [28], can be applied to
ﬁnd a numerical solution of (29).

The recursions of the Frank-Wolfe algorithm particularized
to problem (29) are described in Algorithm 2. One iteration
of the procedure consists of two main steps: the identiﬁcation
of a feasible descent direction and the update of the current
iterate.

The feasible descent direction is identiﬁed as the solution

of the following optimization problem:

(w(t−½), q(t−½)) = arg min

(cid:0)w qT(cid:1) g(t−1)

(30)

(w,q)

subject to

(cid:26) q ≤ w, −q ≤ w

w ≤ w,

where g(t−1) is the gradient of the cost function evaluated at
the current iterate (w(t−1), q(t−1)). Problem (30) is linear and
admits the closed-form solution given in lines 4-5 of Algo-
rithm 2.

The update of the current iterate is performed via a con-
vex combination of (w(t−½), q(t−½)) and (w(t−1), q(t−1)),
see lines 6-7 in Algorithm 2. Parameter γ is chosen to
joining
minimize the cost function over the line segment

( (cid:101)w(t−½), q(t−½)) and (w(t−1), q(t−1)). In the particular setup
this amounts to ﬁnding the value of γ
considered here,
minimizing a quadratic function over [0, 1], which admits a
closed-form solution.

We conclude this section by showing that parameter w can

be chosen as follows:

w = 1

2λ (cid:107)y − A ¯Iq − s w(cid:107)2

2 + w

(31)

where (w, q) is any feasible point of problem (7 − P sq
follows from the following observation:

λ ). This

2 + λw,

2 + λw(cid:63)

λw(cid:63) ≤ 1
≤ 1

2 (cid:107)y − A ¯Iq(cid:63) − s w(cid:63)(cid:107)2
2 (cid:107)y − A ¯Iq − s w(cid:107)2
where the last inequality is valid for any couple (w, q) feasible
for (7 − P sq
λ ). Hence, the deﬁnition of w in (31) veriﬁes
“w(cid:63) ≤ w” as required at the beginning of this section. In
particular, (w, q) = (0, 0card( ¯I)) is feasible for (7 − P sq
λ ) and
leads to w = (cid:107)y(cid:107)2
2λ . We will consider the latter choice in our
simulations in Section V.

2

B. Rescaled projected gradient method

We consider a rescaled version of (7 − P sq
λ ):
α (cid:101)w(cid:13)
2
2 + λ
(cid:13)

(cid:13)
(cid:13)y − A ¯Iq − s

min
( (cid:101)w,q)∈R×Rn−card(I)

1
2

α (cid:101)w

(32)

subject to αq ≤ (cid:101)w, −αq ≤ (cid:101)w

where α > 0. The choice of α will be discussed later on
in this section and will be made based on convergence rate
arguments.

Particularizing the gradient projection algorithm described
in [25, Section 2.3] to problem (32) leads to Algorithm 3.
Each iteration is divided into three main stages: a “gradient
descent” step (lines 4-6), a “projection” operation (line 8) and
a “convex update” (line 9-10).

Starting from a current estimate (q(t−1), (cid:101)w(t−1)), the gradi-
ent step consists in updating the current iterate in the direction
of the negative gradient of the cost function (by an amount
η > 0). The step η is chosen to minimize the cost function
along the direction of the (negative) gradient. We note that
since the cost function is quadratic, the value of η admits a
simple closed-form expression.

The projection step consists in solving the optimization
problem speciﬁed in line 8 of Algorithm 3. Although con-
ceptually simpler than (32), the latter does not admit any
closed-form solution. We show nevertheless in Appendix C
that the procedure described in Algorithm 4 can compute
the unique minimizer of this problem in a ﬁnite number of
steps (upper-bounded by card(¯I)) with a complexity scaling
as O(card(¯I) log(card(¯I))). The interested reader is referred
to Appendix C-A for more details on the computational
complexity.

Finally, similarly to the Frank-Wolfe algorithm, the update
of the current iterate is performed via a convex combination,
see lines 9-10 in Algorithm 3. The choice of γ is made to
minimize the value of the cost function over the line segment
joining the two points.

6

Algorithm 3: Projected gradient method for problem (32)
Input: y, A ¯I, s, α, η, (cid:101)w0, q0

1 (cid:101)w(0) = (cid:101)w0, q(0) = q0 // Initialization
2 t = 1 // Iteration index
3 repeat

// Gradient step
q(t−2⁄3) = q(t−1) + η AT
(cid:101)w(t−2⁄3) = (cid:101)w(t−1) + η
where z(t−1) = y − A ¯Iq(t−1) − s

¯I z(t−1)
(cid:0)sTz(t−1) − λ(cid:1)

α

α (cid:101)w(t−1)

// Projection step (solved via Algorithm 4)
( (cid:101)w(t−1⁄3), q(t−1⁄3))
= arg min

(cid:107)q − q(t−2⁄3)(cid:107)2

2 + ( (cid:101)w − (cid:101)w(t−2⁄3))2

q, (cid:101)w

subject to αq ≤ (cid:101)w, −αq ≤ (cid:101)w

// Convex update
w(t) = γw(t−1) + (1 − γ)w(t−1⁄3)
q(t) = γq(t−1) + (1 − γ)q(t−1⁄3)

4

5

6

7

8

9

10

// Update iteration index
t = t + 1

11
12 until convergence criterion is met

Output: q(t), (cid:101)w(t)

Before concluding this section, we elaborate on the choice
of the parameters α. It can be seen (see [25, Section 2.3.1]),
that the speed of convergence of Algorithm 3 to the global
minimizer of (32) is a direct function of the conditioning of
the Hessian of the cost function5

H =

(cid:18)α−2(cid:107)s(cid:107)2
α−1AT

2 α−1sTA ¯I
¯I s AT
¯I A ¯I

(cid:19)

.

(33)

A proper choice of α may thus help improving the condi-
tioning of this matrix and, consequently, enhance the speed of
convergence of the overall procedure. In this paper, we suggest
the following rule of thumb:

α =

(cid:40)

(cid:107)s(cid:107)2
1

if s (cid:54)= 0m
otherwise.

(34)

This choice is motivated by the ideal case where the columns
of A ¯I and s are orthogonal. In this case, the Hessian matrix
in (33) is diagonal and (because the columns of A ¯I have unit-
norm) we have:

σmin(H)
σmax(H)

(cid:16)

= min

α−1(cid:107)s(cid:107)2, α(cid:107)s(cid:107)−1

2

(cid:17)

.

(35)

Setting α as in (34) ensures that the conditioning of H is equal
to 1 in this case. Although in practice A ¯I and s are commonly
not orthogonal, we noticed in our numerical experiments that
(34) leads to much better speeds of convergence than the trivial
choice α = 1.

V. NUMERICAL EXPERIMENTS

In this section, we report several simulation results illus-
trating the relevance of the proposed squeezing methodology.

5That is the ratio between σmax(H) and σmin(H).

Algorithm 4: Projection of ( (cid:101)w, q) onto the feasible set
of (32).
Input: q, (cid:101)w, α
// Initialization

7

1 (cid:101)w(0) = (cid:101)w
2 Q(0) (cid:44) {i : α|q(i)| ≥ (cid:101)w(0)}
3 t = 0 // Iteration index

// Main recursion

4 repeat
5

t = t + 1

6

α2
(cid:101)w(t) =
α2 + card(Q(t−1))
Q(t) (cid:44) (cid:8)i : α|q(i)| ≥ (cid:101)w(t)(cid:9)

7
8 until Q(t) = Q(t−1)

// Final assignment

9 if (cid:101)w(t) ≤ 0 then

(cid:101)wproj = 0 and qproj = 0


 (cid:101)w +





|q(i)|
α

(cid:88)

i∈Q(t−1)

(a)

(b)

Fig. 1: Percentage of saturated entries identiﬁed with the
safe sphere squeezing test as a function of the radius for (a)
the Gaussian dictionary, 3 values of λ/λmax using the GAP
sphere (thick line) and the ST1 sphere (dashed line) (b) the
four dictionaries and λ/λmax = 0.2.

10
11 else
12

13

14 end

(cid:40)

(cid:101)wproj = (cid:101)w(t)
q(i)
qproj(i) =
α−1sign(q(i)) (cid:101)wproj

if i /∈ Q(t)
otherwise

Output: (cid:101)wproj, qproj

In Section V-A, we study the effectiveness of the proposed
strategy as a function of the “quality” of the safe sphere used in
its construction. In Sections V-B and V-C, we demonstrate that
safe squeezing can be advocated to accelerate the resolution
of problem (2 − P ∞
λ ): in Section V-B, we investigate the
computational savings allowed by “dynamic squeezing”; in
Section V-C, we show that for a given computational budget,
safe squeezing enables to reach better convergence properties.
In our simulations, new realizations of the dictionary A and
observation vector y are drawn for each trial according to the
following distributions. The observation vector is generated
according to a standard normal distribution. The dictionary
obeys one of the following distribution: i) the entries are
i.i.d. realizations of a centered Gaussian; ii) the entries are
i.i.d. realizations of a uniform law on [0, 1]; iii) the rows are
randomly-sampled rows of a DCT matrix; iv) the rows are
shifted versions of a Gaussian curve. In the following, these
four options will be respectively referred to as “Gaussian”,
“Uniform”, “DCT” and “Toeplitz”.

A. Effectiveness of the safe sphere squeezing test

In this section, we assess the effectiveness of the proposed
squeezing test. More speciﬁcally, we evaluate the proportion of
saturated entries of x(cid:63) which can be identiﬁed by the proposed
procedure as a function of the “quality” of the safe sphere.

Fig. 1 presents our results. They have been obtained by
repeating the experiment described below 50 times. For each
simulation trial, we draw a realization of y ∈ R200 and
A ∈ R200×300 according to the distributions described at the

beginning of Section V. We then compute a “high-accuracy”
λ )-(11-D∞
primal-dual solution (xa, ua) of (2 − P ∞
λ ). xa is
evaluated numerically by solving (2 − P ∞
λ ) with Algorithm 3
and a dual gap of 10−14 as stopping criterion. ua is obtained
by dual scaling of y − Axa (see (26)). This high-accuracy
couple is used to construct the following safe spheres:

cST1 = y
rST1 = r0 + (cid:107)y − ua(cid:107)2

and

cGAP = ua

rGAP = r0 +

(cid:113)

2 gap((cid:107)xa(cid:107)∞, xa, ua)

(36)

(37)

(38)

(39)

where gap is deﬁned in (25) and r0 ≥ 0 is some parameter.
If r0 = 0, (36)-(37) and (38)-(39) respectively correspond to
the parameters6 of the ST1 and GAP safe spheres presented in
Section III-D. By construction, these expressions thus lead to
safe spheres for any value of r0 ≥ 0. In our experiments, we
consider (36)-(39) with different r0 ≥ 0 to simulate different
qualities of safe spheres. More speciﬁcally, if r0 = 0 and
(xa, ua) = (x(cid:63), u(cid:63)), rST1 (resp. rGAP) is the smallest possible
radius for an ST1 (resp. GAP) safe sphere. For each value
of r0 and each simulation trial, we apply the proposed safe
squeezing test (20) with the spheres deﬁned in (36)-(39). Fig. 1
represents the proportion of saturated entries of x(cid:63) detected by
our squeezing test as a function of parameter r0. The results
are averaged over 50 realizations for each value of r0.

Fig. 1a shows the percentage of saturated entries detected
with a Gaussian dictionary and three different values of
λ/λmax. Although we restrict our attention to the Gaussian
dictionary, the following comments are valid for the other
dictionaries as well. When r0 = 0 (i.e., the radius of the safe
sphere is equal to rGAP or rST1), one observes that all the
saturated entries are detected with the GAP sphere. However,
only a subset of these entries are detected with the ST1 sphere
(up to 60% in the best case). This result was expected since
the high accuracy estimate of u(cid:63) leads to ua (cid:39) u(cid:63) and rGAP

6Computed with the primal-dual feasible couple (xa, ua).

0.51.01.52.0r0020406080100% of detection/max=0.2/max=0.5/max=0.80.20.40.60.81.01.2r0020406080100% of detectionUniformGaussianDCTToeplitzis therefore close to zero when r0 = 0 (see Section III-E). On
the other side, the radius of the ST1 sphere is usually bounded
away from zero even when ua = u(cid:63) and r0 = 0.

For the two tests, the number of saturated entries detected
decreases as the volume of the safe sphere (parametrized by
r0 here) increases. The performance also seems to depend on
λ: higher values lead to better detection results.

Fig. 1b shows the percentage of saturated entries detected
for the four dictionaries when λ/λmax = 0.2. We observe
that some dictionaries seem to be more prone to squeezing.
For example, up to 30% additional saturated entries are
detected with the DCT dictionary as compared to the Toeplitz
dictionary when r0 (cid:39) 0.3.

B. Complexity savings

In this section, we evaluate the overall computational gain
induced by the proposed method in the resolution of prob-
lem (2 − P ∞
λ ). More precisely, the total number of operations7
carried out by several algorithms (with and without squeezing)
to solve (2 − P ∞
λ ) to some accuracy are compared. In the
case where a sequential implementation of the optimization
procedures is considered, this ﬁgure of merit can be directly
related to CPU time needed to solve (2 − P ∞
λ ); if parallel
implementation is envisaged, the number of operations can be
interpreted in terms of “energy consumption”.

Four numerical procedures are assessed: (i) FITRA, an
accelerated proximal gradient algorithm proposed in [18]; (ii)
the Frank-Wolfe procedure described in Algorithm 2 with
no squeezing (I = ∅); (iii) the projected gradient presented
in Algorithm 3 with dynamic squeezing (see Algorithm 1);
(iv) the Frank-Wolfe in Algorithm 2 with dynamic screening.
Procedures (ii), (iii) and (iv) are respectively denoted as
“FW”, “PGs”, “FWs” in the sequel. Finally, empirical evidence
suggests that the GAP sphere test is more effective than ST1,
and we concentrate therefore on the former in our experiments.
Since this experiment aims at demonstrating the advantage
of resorting to safe squeezing, we only compare procedures
belonging to the same family of algorithms and therefore with
similar rate of convergence. In such a setting, the induced
reduction in terms of number of operations is most likely
only due to safe squeezing. In particular, we compare FW
against FWs and FITRA against PGs since the two couples
of procedures belong to the families of Frank-Wolfe and
proximal/projection gradient methods, respectively.

The numbers of operations are computed for a decreasing
sequence of penalization parameters {λj}p
j=1. Moreover, we
consider a “warm-start” initialization: for each λj, the opti-
mization procedure is initialized with the solution obtained for
λj−1. For λ1, all algorithms are initialized with the zero vector
0 ∈ Rn. For each run, a stopping criterion in terms of dual gap
is used. To obtain results with the same order of magnitude8,
two values of the dual gap are used: 10−7 for FITRA and
PGs and 10−4 for the two Frank-Wolfe procedures (FW and

7We restrict our attention to multiplications since they entail a much higher

computational burden than additions in ﬂoating-point arithmetic.

8Frank-Wolfe procedures are indeed known to converge much more slowly

than proximal/projection gradient methods.

8

Fig. 2: Number of operations required to achieve convergence
for different values of λ/λmax and four dictionaries: Uniform
(ﬁrst row), Gaussian (second row), DCT (third row) and
Toeplitz (fourth row).

FWs). All results are averaged over 50 simulation trials. For
each simulation trial, we draw a realization of y ∈ R100 and
A ∈ R100×150 according to the distributions described at the
beginning of Section V.

Fig. 2 represents the averaged number of operations needed
by FITRA, PGs, FW and FWs to achieve the required con-
vergence accuracy. Each ﬁgure corresponds to a different
dictionary (Gaussian, Uniform, DCT and Toeplitz). The curves
are plotted as a function of − log10(λ/λmax). We note that,
to allow a fair comparison between the different algorithms,
we took particular care in the counting of the operations. In
particular, each operation which can be reused latter on in an
optimization procedure is not counted twice in our simulation.

106107108109number of operations105106107108number of operations105106107108109number of operations0.00.10.20.30.40.50.6log10(/max)105106107108number of operationsFITRAPGsFWFWs9

Fig. 3: Performance proﬁles obtained with the Uniform, Gaussian, DCT and Toeplitz dictionaries. First row: λ/λmax = 0.3;
second row: λ/λmax = 0.8.

In all cases, one observes that PGs (straight blue line)
achieves convergence with fewer operations than FITRA
(dashed blue line). As an example, up to 104 times more op-
erations are required by FITRA when − log10(λ/λmax) = 0.1
and A is a DCT dictionary (third row). However, such a
“computational” saving tends to decrease as − log10(λ/λmax)
increases. Such a behavior is in good accordance with typical
results of the screening literature where the performance
degrades for low values of the penalization parameter.

As far as our simulation setup is concerned, these results
show the advantage of considering safe squeezing to reduce the
computational burden of computing antisparse representations.
The observed reduction of complexity may be explained by
analyzing the cost of one iteration of the four algorithms.
Indeed, since the squeezing tests are done at few additional
costs, the implementations of the four considered algorithms
results in a cost per iteration of O(mn(t)) where n(t) =
n − card(I (t)) and I (t) is the number of saturated entries
detected at iteration t. While n(t) is decreasing along iterations
for PGs and FWs, it remains constant and equal to n for
FITRA and FW. The two mains consequences are (i) the
computational cost of one iteration is smaller and (ii) the
induced dimensionality reduction may help improving the
conditioning of the optimization problem, resulting potentially
in fewer iterations needed to reach convergence.

C. Benchmark proﬁles

As a ﬁnal assessment of the performance of the proposed
squeezing test, we make use of the Dolan-Mor´e performance
proﬁles [29]. For each family of dictionaries, we generate a test
set consisting of p = 50 random dictionaries A ∈ R100×150

and observations y ∈ R100 according to the distributions
described at the beginning of Section V. Each problem is
then solved by using the four algorithms mentioned in the
previous section, with a budget of 108 operations.9 For each
run j ∈ {1 . . . p} and solvers s ∈ {FITRA, PGs, FW, FWs},
we deﬁne

dj,s (cid:44) dual gap achieved by solver s at run j.

(40)

In this experiment, we compare the performance proﬁle

ρs(τ ) (cid:44) 100
p

card({j : dj,s ≤ τ }) ∀τ

(41)

of each algorithm s. In other words, for all possible values of
the dual gap τ , ρs(τ ) is the (empirical) probability that solver
s reaches a dual gap no greater than τ .

Figure 3 presents the performance proﬁles of the four
algorithms with different dictionaries and values of the ratio
λ/λmax. As far as our simulation setup is concerned, these
results show that for a given budget of operations and any
targeted dual gap smaller that 10−4, resorting to PGs (resp.
FWs) allows to solve10 more problems than FITRA (resp.
FW). Moreover, PGs outperforms the other procedures: in all
scenarios, at least 45% of the runs reach a dual gap with order
of magnitude equal to machine precision (τ = 10−16).

As detailed in the last paragraph of Section V-B, resorting
to safe squeezing may reduce the computational burden of
one iteration of an optimization procedure and help improving
the conditioning of the optimization problem through an easy

9The algorithm stops as soon as 108 multiplications have been carried out.
10that is, reach a solution with the desired dual gap.

0%20%40%60%80%100%s()GaussianUniformDCTToeplitz101610131010107104101 (Dual gap)0%20%40%60%80%100%s()FITRAPGsFWFWs101610131010107104101 (Dual gap)101610131010107104101 (Dual gap)101610131010107104101 (Dual gap)rescaling. With this conclusion in mind,
the performance
improvements observed in this experiment can be explained
by the combination of two following factors. First, improving
the conditioning of the problem results potentially in fewer
iterations needed to reach a given dual gap. Second, reducing
the computational cost of one iteration allows to perform more
iterations for the same budget of operations, reaching possibly
a solution with a lower dual gap.

VI. CONCLUSIONS

In this paper, we proposed a new methodology, dubbed safe
squeezing, to accelerate algorithms that compute a so-called
antisparse representation. The proposed procedure consists in
a set of tests that detect saturated entries in the solution of the
optimization problem. These tests rely on the notion of safe
region, that is a set that contains the solution of the dual prob-
lem. Beneﬁting from the recent developments in the screening
literature, we showed that two existing safe spheres can be
extended in the context of safe squeezing. The contribution of
the saturated entries is then compacted into one single vector,
allowing for a potentially dramatic dimensionality reduction.
Computing an antisparse representation becomes equivalent
to solving a lower dimensional quadratic problem. When
interleaved with the iterations of an optimization procedure,
the approach permits to dynamically detect new saturated en-
tries all along the optimization process. Numerical simulations
showed that the proposed safe squeezing methodology leads
to signiﬁcant computational gains in several numerical setups.

APPENDIX A
PROOF OF THE RESULT IN (5)
(cid:13)ATy(cid:13)

In this section, we prove that “λ < (cid:13)

(cid:13)1” is indeed a
necessary and sufﬁcient condition for the solution of (2 − P ∞
λ )
to be nonzero. A direct application of the Fermat’s rule [30,
Th. 16.2] ensures that x(cid:63) is a minimizer of (2 − P ∞
λ ) if and
only if

λ−1AT(Ax(cid:63) − y) ∈ ∂(cid:107)x(cid:63)(cid:107)∞
where ∂(cid:107)x(cid:63)(cid:107)∞ denotes the sub-differential of the (cid:96)∞-norm
evaluated at x(cid:63). Danskin’s theorem [25, Th. B.25] leads to

(42)

∂(cid:107)x(cid:107)∞

(cid:44) (cid:8)d ∈ Rn : (cid:107)d(cid:107)1 ≤ 1 and xTd = (cid:107)x(cid:107)∞
for all x ∈ Rn. Result (5) follows by particularizing (42)-(43)
to x(cid:63) = 0n.

(cid:9) (43)

APPENDIX B
PROOF OF THE RESULTS IN SECTION III

In this section, we provide a proof of Theorem 1. We also
λ ) is independent

show that the quantity u(cid:63) deﬁned in (11-D∞
of the set I.

In our derivations, we focus on the following generic

optimization problem:

min
z∈Rm,q∈Rp
w≥0

1

2 (cid:107)y − z(cid:107)2

2 + λw s.t.






q ≤ w
−q ≤ w
z = Bq + sw.

Conclusions about problems (2 − P ∞
drawn by considering the following substitutions:

λ ) and (7 − P sq

λ ) are then

10

Problem (2 − P ∞
Problem (7 − P sq

λ ): B = A, s = 0m
λ ): B = A ¯I, s =

(cid:88)

(45)

(cid:88)

ai −

ai.

(46)

i∈I+

i∈I−

We organize the presentation of the results as follows. In
Section B-A we derive the Lagrangian dual problem of (44). In
Section B-B, we emphasize some properties of the solutions of
the dual problem. The latter are then exploited in Section B-C
to derive (the ﬁrst part of) the result stated in Theorem 1. In
Section B-D, we elaborate on the uniqueness of the maximizer
of the dual problem and its independence with respect to
the set I. Finally, in Section B-E we prove the last part of
Theorem 1.

A. Dual problem of (44)

Let v+ ∈ Rm and v− ∈ Rm (resp. u ∈ Rn) denote
the dual variables associated to the inequality (resp. equality)
constraint(s) in (44). The Lagrangian function associated to
this problem can then be rewritten as

L (q, z, w, u, v+, v−) = 1

2 + uTz

2 (cid:107)y − z(cid:107)2
+ w(cid:0)λ − sTu − 1T
+ qT(cid:0)−BTu + v+ − v−

n v+ − 1T
n v−
(cid:1).

(cid:1)

Moreover, the corresponding dual function is deﬁned as

D(u, v+, v−) (cid:44) min

q,z,w≥0

L (q, z, w, u, v+, v−).

We note that since v+ ≥ 0 and v− ≥ 0, we have
D(u, v+, v−) = −∞ as soon as the following conditions are
not satisﬁed:

λ − sTu − 1T

n v+ − 1T

n v− ≥ 0,
−BTu + v+ − v− = 0p.

(47)

Hence, deﬁning

D (cid:44) {(u, v+, v−) : v+ ≥ 0, v− ≥ 0 and (47) holds},

the dual function D(u, v+, v−) can be written as
2 (cid:107)y(cid:107)2
−∞

D(u, v+, v−) =

2 (cid:107)y − u(cid:107)2

2 − 1

(cid:26) 1

2

if (u, v+, v−) ∈ D
otherwise.

The dual problem associated to (44) thus reads

max
(u,v+,v−)∈D

1

2 (cid:107)y(cid:107)2

2 − 1

2 (cid:107)y − u(cid:107)2
2.

(48)

From [25, Proposition 5.2.1],11 there is no duality gap be-
tween (44) and (48); moreover the dual problem (48) has
least) one maximizer. In fact, we will show in Sec-
(at
tion B-D below that (48) admits one unique maximizer,
denoted (u(cid:63), v(cid:63)

−) hereafter.

+, v(cid:63)

(44)

11More speciﬁcally, (44) is a feasible convex problem with linear constraints

and the minimum value of the cost function is upper bounded.

B. Connection between u(cid:63),v(cid:63)

+ and v(cid:63)
−
+ and v(cid:63)
In this section, we emphasize that v(cid:63)
connected to u(cid:63). More speciﬁcally, we have:

− are strongly

i u(cid:63))
i u(cid:63)).

v(cid:63)
+(i) = max(0, bT
−(i) = − min(0, bT
v(cid:63)
This connection can be shown as follows. First, since there
is no duality gap (see Section B-A), any couple of primal-
dual optimal variables, say (w(cid:63), q(cid:63), z(cid:63)) and (u(cid:63), v(cid:63)
−),
must verify the well-known Karush-Kuhn-Tucker optimality
conditions [25, Proposition 5.1.5]. In particular, we have

+, v(cid:63)

(49)

−BTu(cid:63) + v(cid:63)

y − u(cid:63) = z(cid:63)
+ − v(cid:63)
− = 0p
v(cid:63)
± ≥ 0
v(cid:63)
+(i)(q(cid:63)(i) − w(cid:63)) = 0
−(i)(q(cid:63)(i) + w(cid:63)) = 0.
v(cid:63)

∀i :

∀i :

(50a)

(50b)

(50c)

(50d)

(50e)

More precisely, (50a) corresponds to ﬁrst-order optimality
condition while (50b) and (50c) partially describe the set
of dual feasible variables (see (47)). One also recognizes
in (50d) and (50e) the complementary slackness conditions
of the problem.

Using these conditions, we have

since

and

+(i)v(cid:63)
v(cid:63)

−(i) = 0 ∀i

+(i) > 0 (50d)⇒ q(cid:63)(i) = w(cid:63) (4)−(5)
v(cid:63)
> 0

(50e)⇒ v(cid:63)

−(i) = 0,

−(i) > 0 (50e)⇒ q(cid:63)(i) = −w(cid:63) (4)−(5)
v(cid:63)
< 0

(50d)⇒ v(cid:63)

+(i) = 0.

(51)

(52)

(53)

(54)

(55)

Therefore, combining (50b), (50c) and (51), we obtain the
following implications:

bT
i u(cid:63) = 0 ⇒

i u(cid:63) > 0 ⇒
bT

i u(cid:63) < 0 ⇒
bT

(cid:26) v(cid:63)
−(i) = 0
v(cid:63)
+(i) = 0
(cid:26) v(cid:63)
−(i) = 0
v(cid:63)
i u(cid:63)
+(i) = bT
(cid:26) v(cid:63)
−(i) = −bT
v(cid:63)
+(i) = 0.

i u(cid:63)

(56)

(57)

(58)

Expression (49) corresponds to a compact
of (56), (57) and (58).

reformulation

C. Necessary optimality condition

We now exploit (49) together with conditions (50d)-(50e)
to prove the necessary optimality condition (13) stated in
Theorem 1. Let us ﬁrst show that u(cid:63) can be expressed as

where

u(cid:63) = arg max

u∈U

1

2 (cid:107)y(cid:107)2

2 − 1

2 (cid:107)y − u(cid:107)2

2

U (cid:44) (cid:8)u : (cid:13)

(cid:13)BTu(cid:13)

(cid:13)1 + sTu ≤ λ(cid:9).

(59)

(60)

To this end, let us deﬁne

(cid:26)

F (cid:44)

(u, v+, v−) :

max(0, bT
v+(i) =
v−(i) = − min(0, bT
Clearly, from (48) and (49) we have that (u(cid:63), v(cid:63)
F. Hence,
(u(cid:63), v(cid:63)

2 (cid:107)y(cid:107)2

2 − 1

+, v(cid:63)

−) ∈

1

arg max
(u,v+,v−)∈D∩F

(cid:27)

i u)
i u)
+, v(cid:63)

2 (cid:107)y − u(cid:107)2
2.

Moreover, combining (47) and (49) leads to

D ∩ F =






(u, v+, v−) :

λ ≥ (cid:13)

(cid:13)BTu(cid:13)

max(0, bT
v+(i) =
v−(i) = − min(0, bT

(cid:13)1 + sTu
i u)
i u)

11

.

(61)

−) ∈ D ∩

(62)






.

Plugging this deﬁnition into (62), we obtain that u(cid:63) is deﬁned
+ and v(cid:63)
by (59)-(60). Moreover, v(cid:63)
− are univocally speciﬁed
from u(cid:63) via (49).

We ﬁnally show that the following condition holds between

the optimal primal and dual variables:
i u(cid:63))(cid:0)q(cid:63)(i) − sign(bT

(bT

i u(cid:63))w(cid:63)(cid:1) = 0.

(63)

Condition (13) then corresponds to a particular instantiation
of (63) when B and s are deﬁned as in (46). Relation (63)
can be obtained as follows. Taking the difference between
conditions (50d)-(50e), we ﬁrst have
−(i)) − w(cid:63)(v(cid:63)

+(i) − v(cid:63)

+(i) + v(cid:63)

−(i)) = 0.

q(cid:63)(i)(v(cid:63)

(64)

Now, using (49) we obtain

+(i) + v(cid:63)
v(cid:63)
+(i) − v(cid:63)
v(cid:63)
and

i u(cid:63)(cid:12)
−(i) =(cid:12)
(cid:12)bT
(cid:12)
i u(cid:63).
−(i) =bT
=
fact
the
i u(cid:63)), we ﬁnally end up with the desired

i u(cid:63)(cid:12)
(cid:12)

(cid:12)
(cid:12)bT

(65b)

(65a)

that

(65a)-(65b)

i u(cid:63)(cid:1)sign(bT

Using
(cid:0)bT
result.

D. Uniqueness of u(cid:63) and independence with respect to I

Problem (59) admits a unique minimizer since the cost
function is continuous, coercive and strictly concave, see [25,
Propositions A.8 & B.10]. This shows the uniqueness of u(cid:63).
We note that the uniqueness of (u(cid:63), v(cid:63)
−) claimed in Sec-
tion B-A then straightforwardly follows from property (49).

+, v(cid:63)

Setting B and s as in (46), we now show the independence
of u(cid:63) with respect to the deﬁnition of I ⊆ I (cid:63). We note that
in this case, problem (44) is equivalent to (7 − P sq
λ ). From
optimality condition (50a), we have

u(cid:63) = y − z(cid:63).

Now, we have by deﬁnition that

z(cid:63) = A ¯Iq(cid:63) + sw(cid:63).
λ ) and (7 − P sq
Moreover, because problems (2 − P ∞
equivalent (see Section III), we have from (9)-(10) that

λ ) are

∀ I ⊆ I (cid:63) : A ¯Iq(cid:63) + sw(cid:63) = Ax(cid:63),
where x(cid:63) is the unique (see Section III-A) minimizer of
(2 − P ∞
λ ). Combining the last three equalities, we obtain that
u(cid:63) does not depend on the choice of I as long as I ⊆ I (cid:63).12

(68)

12This property holds true in the more generally setup where x(cid:63) is not

unique although the proof is slightly more involved.

(66)

(67)

E. Minimum number of saturated components

In this section we prove the last part of Theorem 1. Since
we showed in Section B-D that u(cid:63) is independent of the choice
of I ⊆ I (cid:63), we focus on the case where I = ∅ and B = A,
s = 0m.

We ﬁrst note that our working hypothesis (4) prevents us
from having u(cid:63) = 0m. Indeed, considering the optimality
conditions of problem (59), it can be seen that

y /∈ ker(AT) ⇒ u(cid:63) (cid:54)= 0m.

(69)

Now, as pointed out in Section III-A, hypothesis (4) implies
that y /∈ ker(AT) and thus necessarily u(cid:63) (cid:54)= 0m.

Since u(cid:63) (cid:54)= 0m, we observe that “aT

i u(cid:63) = 0” can only
occur for at most m − 1 ai’s when kruskal(A) = m. Indeed,
kruskal(A) = m implies that any subset of m columns of
A are linearly independent. On the other hand, the vector
space P (cid:44) {a ∈ Rm : aTu(cid:63) = 0} is a (m − 1)-dimensional
subspace of Rm. Hence, if more than m − 1 columns of A
belong to P, they must be linearly dependent, which is in
contradiction with kruskal(A) = m. This proves the last part
of Theorem 1.

APPENDIX C
CORRECTNESS OF ALGORITHM 4

12

Since card(Q(t−1)) must be nonnegative and card(Q(0)) ≤ q,
we thus have necessarily from (73) that t ≤ q. This proves the
convergence of Algorithm 4 in at most q steps.

In practice, Algorithm 4 is implemented as follows. Vector
q is ﬁrst sorted by absolute value and its cumulative sum is
precomputed. This requires a complexity of O(q log q). Then,
each of the (at most) q repetitions of lines 5-6-7 scales as
O(log(card(Q(t))) since the complexity is concentrated in the
identiﬁcation of the components of Q(t) verifying “α|q(i)| ≥
(cid:101)w(t+1)”. Inasmuch as card(Q(t)) ≤ q, the overall complexity
of the proposed procedure is therefore O(q log q).

B. Optimality conditions for (70)

In this section, we derive necessary and sufﬁcient optimality
conditions for problem (70). These conditions will be exploited
in the next section to show the correctness of Algorithm 4.

We ﬁrst note that

sign(q(cid:63)) = sign(q).

(74)

Indeed, q(cid:63) is also the minimizer of

min
q(cid:48)

(cid:107)q(cid:48) − q(cid:107)2
2

subject to ± αq(cid:48) ≤ (cid:101)w(cid:63),

(75)

In this appendix, we prove that the procedure described in

Algorithm 4 solves the following projection problem

which admits the following simple analytical expression:

min
q(cid:48),w(cid:48)

(w(cid:48) − (cid:101)w)2 + (cid:107)q(cid:48) − q(cid:107)2

2

∀i :

q(cid:63)(i) = sign(q(i)) min(|q(i)|, (cid:101)w(cid:63)).

(76)

subject to αq(cid:48) ≤ w(cid:48), −αq(cid:48) ≤ w(cid:48)

(70)

This shows (74).

in a ﬁnite number of steps. With a slight abuse of notations,13
we denote hereafter the unique minimizer of this problem as
( (cid:101)w(cid:63), q(cid:63)). The dimension of q will be denoted q hereafter.

The appendix is organized as follows. In Section C-A, we
prove the convergence of Algorithm 4 in a number of steps
no greater than q and discuss its computational complexity.
In Section C-B we elaborate on necessary and sufﬁcient
conditions for ( (cid:101)w(cid:63), q(cid:63)) to be a minimizer of (70). Finally, we
exploit these optimality conditions in Section C-C to show that
the output of Algorithm 4 corresponds to the unique minimizer
of (70).

A. Convergence of Algorithm 4 in a ﬁnite number of steps
We ﬁrst note that, by construction, the sequence (cid:8)

(cid:101)w(t)(cid:9) is
non-decreasing. As a matter of fact, by deﬁnition of Q(t) we
must have

In the rest of the appendix, we will thus focus, without loss

of generality, on the case where

q(i) ≥ 0 ∀i.

(77)

We note that this setup can always be obtained by some simple
changes of variables. In this case, we have from (74) that
q(cid:63) ≥ 0q and problem (70) can equivalently be expressed as

min
q(cid:48)≥0q, (cid:101)w(cid:48)≥0

( (cid:101)w(cid:48) − (cid:101)w)2 + (cid:107)q(cid:48) − q(cid:107)2

2 s.t. αq(cid:48) ≤ (cid:101)w(cid:48).

(78)

Since (78) is convex with linear inequality constraints, any
minimizer ( (cid:101)w(cid:63), q(cid:63)) must satisfy the following set of necessary
and sufﬁcient optimality conditions [25, Propositions 5.2.1 and
5.1.5]:

αq(cid:63) ≤ (cid:101)w(cid:63),

Q(t) ⊆ Q(t−1).

(71)

and ∃v(cid:63) ≥ 0q such that

Moreover, if equality occurs in (71), then the stopping criterion
of Algorithm 4 is satisﬁed and the procedure terminates.
Algorithm 4 carries out t iterations provided that

Q(t) = Q(t−1) ⊂ Q(t−2) ⊂ . . . ⊂ Q(0)

(72)

and, in particular,

0 = v(cid:63)(i)(αq(cid:63)(i) − (cid:101)w(cid:63)),

∀i :
( (cid:101)w(cid:63), q(cid:63)) = arg min
(cid:101)w(cid:48)≥0,q(cid:48)≥0q

(cid:110)
( (cid:101)w(cid:48) − (cid:101)w)2 + (cid:107)q(cid:48) − q(cid:107)2

2

+

q
(cid:88)

i=1

(cid:111)
v(cid:63)(i)(αq(cid:48)(i) − (cid:101)w(cid:48))
.

card(Q(t−1)) < card(Q(t−2)) < . . . < card(Q(0)).

(73)

13In particular, q(cid:63) should not be confused with the minimizer of prob-

lem (7 − P sq
λ ).

In the rest of this section, we derive other optimality con-
ditions, equivalent to (79)-(81), which are more amenable to
prove the correctness of Algorithm 4.

(79)

(80)

(81)

We ﬁrst note that (81) is a separable nonnegative least-
square problem and its minimizer admits the following closed-
form expression:

(cid:101)w(cid:63) = max

(cid:16)
0, (cid:101)w +

q
(cid:88)

(cid:17)

v(cid:63)(i)

q(cid:63)(i) = max (cid:0)0, q(i) − αv(cid:63)(i)(cid:1).

i=1

(82)

(83)

We then distinguish between the cases “ (cid:101)w(cid:63) = 0” and “ (cid:101)w(cid:63) >
0”.

From (79), we see that the case “ (cid:101)w(cid:63) = 0” may occur only

if q(cid:63) = 0q. Using (82)-(83), this leads to

( (cid:101)w(cid:63), q(cid:63)) = (0, 0q) ⇐⇒ ∃v(cid:63) ≥ 0q s.t.






(cid:101)w +

q
(cid:88)

i=1

v(cid:63)(i) ≤ 0,

α−1q(i) ≤ v(cid:63)(i),

or more compactly,

( (cid:101)w(cid:63), q(cid:63)) = (0, 0q) ⇐⇒ (cid:101)w + α−1

q
(cid:88)

i=1

q(i) ≤ 0.

(84)

We next consider the case “ (cid:101)w(cid:63) > 0”. First, this assumption

together with (82) leads to

(cid:101)w(cid:63) = (cid:101)w +

q
(cid:88)

i=1

v(cid:63)(i).

Moreover, from (80) we have that

where

v(cid:63)(i) = 0 ∀i /∈ Q(cid:63),

Q(cid:63) (cid:44) {i : αq(cid:63)(i) = (cid:101)w(cid:63)}.

(85) thus further simpliﬁes to

(cid:101)w(cid:63) = (cid:101)w +

v(cid:63)(i).

(cid:88)

i∈Q(cid:63)

On the other hand, we have (by deﬁnition of Q(cid:63)) that

q(cid:63)(i) = α−1

(cid:101)w(cid:63) ∀i ∈ Q(cid:63)

(85)

(86)

(87)

(88)

(89)

so that, invoking again our hypothesis “ (cid:101)w(cid:63) > 0”, we obtain
from (83)

q(cid:63)(i) = q(i) − αv(cid:63)(i) ∀i ∈ Q(cid:63).

(90)

Finally, combining (88), (89) and (90) leads to

(cid:101)w(cid:63) =

α2
α2 + card(Q(cid:63))

(cid:32)
(cid:101)w + α−1 (cid:88)

i∈Q(cid:63)

(cid:33)
.

q(i)

(91)

As a consequence ( (cid:101)w(cid:63), q(cid:63)) with (cid:101)w(cid:63) > 0 is a solution of (78)
if and only if ∃v(cid:63) ≥ 0q such that (88)-(91) are satisﬁed.

C. Optimality of ( (cid:101)wproj, qproj)

13

We ﬁnally prove the correctness of Algorithm 4. We assume
the algorithm terminates its main recursion (steps 4-
is Q(t) = Q(t−1)) and show
the assignments deﬁned in steps 9-14 are such that

that
8) after t iterations (that
that
( (cid:101)wproj, qproj) = ( (cid:101)w(cid:63), q(cid:63)).

First, if (cid:101)w(t) ≤ 0, we must have (by deﬁnition of Q(t)) that

Q(t) = {1, . . . , q}

(92)

since q ≥ 0q. Combining this equality with the deﬁnition of
(cid:101)w(t) in line 6 of Algorithm 4 leads to

(cid:32)

(cid:101)w(t) =

α2
α2 + q

(cid:101)w + α−1

(cid:33)
.

q(i)

q
(cid:88)

i=1

(93)

Since (cid:101)w(t) ≤ 0, the last equality implies

(cid:101)w + α−1

q
(cid:88)

i=1

q(i) ≤ 0.

(94)

In view of (84), we ﬁnally have

(cid:101)w(t) ≤ 0 ⇒ ( (cid:101)w(cid:63), q(cid:63)) = (0, 0q).
This corresponds to the assignment in line 9-10 of Algo-
rithm 4.

(95)

If (cid:101)w(t) > 0, it can easily be veriﬁed that the optimality

conditions (88)-(91) are veriﬁed for

q(cid:63) = qproj
(cid:101)w(cid:63) = (cid:101)wproj
(cid:40)
0
α−2(αq(i) − (cid:101)wproj)

v(cid:63)(i) =

if αqproj(i) < (cid:101)wproj
otherwise,

where (cid:101)wproj and qproj are deﬁned in lines 12-13 of Algo-
rithm 4. This concludes the proof.

REFERENCES
[1] R. T. Rockafellar, “A dual approach to solving nonlinear pro-
gramming problems by unconstrained optimization,” Mathemat-
ical Programming, vol. 5, no. 1, pp. 354–373, Dec. 1973.
[2] R. E. Bruck, “An iterative solution of a variational inequality
for certain monotone operators in hilbert space,” Bull. Amer.
Math. Soc., vol. 81, no. 5, pp. 890–892, Sep. 1975.

[3] D. Gabay and B. Mercier, “A dual algorithm for the solution
of nonlinear variational problems via ﬁnite element approxi-
mation,” Computers & Mathematics with Applications, vol. 2,
no. 1, pp. 17 – 40, 1976.

[4] L. El Ghaoui, V. Viallon, and T. Rabbani, “Safe feature elimi-
nation in sparse supervised learning,” EECS Dept., University
of California at Berkeley, Tech. Rep. UC/EECS-2010-126, Sep.
2010.

[5] S. Foucart and H. Rauhut, A Mathematical Introduction to

Compressive Sensing. Birkh¨auser Basel, 2013.

[6] S. Rosset, J. Zhu, and T. Hastie, “Boosting as a regularized path
to a maximum margin classiﬁer,” Journal of Machine Learning
Research, vol. 5, no. Aug, pp. 941–973, 2004.

[7] O. Fercoq, A. Gramfort, and J. Salmon, “Mind the duality gap:
safer rules for the Lasso,” in Proc. Int. Conf. Machine Learn-
ing (ICML), ser. Proceedings of Machine Learning Research,
vol. 37, Lille, France, Jul. 2015, pp. 333–342.

[8] A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval, “Dy-
namic Screening: Accelerating First-Order Algorithms for the

14

Lasso and Group-Lasso,” IEEE Trans. Signal Process., vol. 63,
no. 19, p. 20, 2015.

[9] A. Malti and C. Herzet, “Safe screening tests for Lasso based
on ﬁrmly non-expansiveness,” in Proc. IEEE Int. Conf. Acoust.,
Speech, and Signal Proces. (ICASSP), Mar. 2016, pp. 4732–
4736.

[10] C. Herzet, C. Dorffer, and A. Dr´emeau, “Gather and conquer:
Region-based strategies to accelerate safe screening tests,” IEEE
Trans. Signal Process., vol. 67, no. 12, pp. 3300–3315, Jun.
2019.

[11] E. Ndiaye, O. Fercoq, A. Gramfort, and J. Salmon, “Gap safe
screening rules for sparse multi-task and multi-class models,”
in Proc. Int. Conf. on Neural Information Processing Systems
(NIPS). MIT Press, 2015, pp. 811–819.

[12] ——, “Gap safe screening rules for sparsity enforcing penal-
ties,” J. Mach. Learn. Res., vol. 18, no. 1, pp. 4671–4703, Jan.
2017.

[13] G. G. A. Rakotomamonjy and J. Salmon, “Screening rules for
Lasso with non-convex sparse regularizers,” in Proc. Int. Conf.
Machine Learning (ICML), 2019.

[14] Z. Cvetkovi´c, “Resilience properties of redundant expansions
under additive noise and quantization,” IEEE Trans. Inf. Theory,
vol. 29, no. 3, pp. 644–656, Mar. 2003.

[15] A. R. Calderbank and I. Daubechies, “The pros and cons of
democracy,” IEEE Trans. Inf. Theory, vol. 48, no. 6, pp. 1721–
1725, Jun. 2002.

[16] B. Farrell and P. Jung, “A Kashin approach to the capacity of the
discrete amplitude constrained Gaussian channel,” in Proc. Int.
Conf. Sampling Theory and Applications (SAMPTA), Marseille,
France, May 2009.

[17] J. Ilic and T. Strohmer, “Papr reductioni in ofdm using kashin’s
representation,” in IEEE Workshop on Signal Processing Ad-
vances in Wireless Communications, Jun. 2009, pp. 444–448.

[18] C. Studer and E. G. Larsson, “PAR-aware large-scale multi-user
MIMO-OFDM downlink,” IEEE J. Sel. Areas Comm., vol. 31,
no. 2, pp. 303–313, Feb. 2013.

[19] H. Jegou, T. Furon, and J.-J. Fuchs, “Anti-sparse coding for
approximate nearest neighbor search,” in Proc. IEEE Int. Conf.
Acoust., Speech, and Signal Proces. (ICASSP), 2012, pp. 2029–
2032.

[20] M. Vural, P. Jung, and S. Sta´nczak, “A new outlier detection
method based on anti-sparse representations,” in Signal Process.
and Comm. Appl. Conf. (SIU), May 2017, pp. 1–4.

[21] X. Jiang, J. Chen, H. C. So, and X. Liu, “Large-scale robust
beamforming via (cid:96)∞ -minimization,” IEEE Trans. Signal Pro-
cess., vol. 66, no. 14, pp. 3824–3837, Jul. 2018.

[22] L. W. Neustadt, “Minimum effort control systems,” J. SIAM

Control, vol. 1, no. 1, pp. 16–31, 1962.

[23] J. A. Cadzow, “Algorithm for the minimum-effort problem,”
IEEE Trans. on Autom. Control, vol. 16, no. 1, pp. 60–63, 1971.

[24] J.-J. Fuchs, “Spread representations,” in asilomar, 2011.
[25] D. Bertsekas, Nonlinear Programming, 2nd ed. Athena Scien-

tiﬁc, 1999.

[26] Z. J. Xiang and P. J. Ramadge, “Fast Lasso screening tests based
on correlations,” in Proc. IEEE Int. Conf. Acoust., Speech, and
Signal Proces. (ICASSP), Mar. 2012, pp. 2137–2140.

[27] Z. J. Xiang, Y. Wang, and P. J. Ramadge, “Screening tests for
Lasso problems,” IEEE Trans. Patt. Anal. Mach. Intell., vol. 39,
no. 5, pp. 1008–1027, May 2017.

[28] M. Frank and P. Wolfe, “An algorithm for quadratic program-
ming,” Naval Research Logistics Quarterly, vol. 3, no. 1-2, pp.
95–110, 1956.

[29] E. D. Dolan and J. J. Mor´e, “Benchmarking optimization soft-
ware with performance proﬁles,” Mathematical Programming,
vol. 91, no. 2, pp. 201–213, Jan. 2002.

[30] H. H. Bauschke and P. L. Combettes, Subdifferentiability. New

York, NY: Springer New York, 2011, pp. 223–240.

