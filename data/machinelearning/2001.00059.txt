Learning and Evaluating Contextual Embedding of Source Code

Aditya Kanade * 1 2 Petros Maniatis * 2 Gogul Balakrishnan 2 Kensen Shi 2

Abstract

1. Introduction

0
2
0
2

g
u
A
7
1

]
E
S
.
s
c
[

3
v
9
5
0
0
0
.
1
0
0
2
:
v
i
X
r
a

Recent research has achieved impressive results
on understanding and improving source code by
building up on machine-learning techniques de-
veloped for natural languages. A signiﬁcant ad-
vancement in natural-language understanding has
come with the development of pre-trained con-
textual embeddings, such as BERT, which can
be ﬁne-tuned for downstream tasks with less la-
beled data and training budget, while achieving
better accuracies. However, there is no attempt
yet to obtain a high-quality contextual embed-
ding of source code, and to evaluate it on multiple
program-understanding tasks simultaneously; that
is the gap that this paper aims to mitigate. Speciﬁ-
cally, ﬁrst, we curate a massive, deduplicated cor-
pus of 7.4M Python ﬁles from GitHub, which we
use to pre-train CuBERT, an open-sourced code-
understanding BERT model; and, second, we cre-
ate an open-sourced benchmark that comprises
ﬁve classiﬁcation tasks and one program-repair
task, akin to code-understanding tasks proposed
in the literature before. We ﬁne-tune CuBERT on
our benchmark tasks, and compare the resulting
models to different variants of Word2Vec token
embeddings, BiLSTM and Transformer models,
as well as published state-of-the-art models, show-
ing that CuBERT outperforms them all, even with
shorter training, and with fewer labeled exam-
ples. Future work on source-code embedding can
beneﬁt from reusing our benchmark, and from
comparing against CuBERT models as a strong
baseline.

*Equal contribution 1Indian Institute of Science, Bangalore,
India 2Google Brain, Mountain View, USA. Correspondence to:
Aditya Kanade <kanade@iisc.ac.in>, Petros Maniatis <mani-
atis@google.com>.

Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).

Modern software engineering places a high value on writing
clean and readable code. This helps other developers under-
stand the author’s intent so that they can maintain and extend
the code. Developers use meaningful identiﬁer names and
natural-language documentation to make this happen (Mar-
tin, 2008). As a result, source code contains substantial
information that can be exploited by machine-learning algo-
rithms. Indeed, sequence modeling on source code has been
shown to be successful in a variety of software-engineering
tasks, such as code completion (Hindle et al., 2012; Raychev
et al., 2014), source code to pseudo-code mapping (Oda
et al., 2015), API-sequence prediction (Gu et al., 2016), pro-
gram repair (Pu et al., 2016; Gupta et al., 2017), and natural
language to code mapping (Iyer et al., 2018), among others.

The distributed vector representations of tokens, called to-
ken (or word) embeddings, are a crucial component of
neural methods for sequence modeling. Learning useful
embeddings in a supervised setting with limited data is
often difﬁcult. Therefore, many unsupervised learning ap-
proaches have been proposed to take advantage of large
amounts of unlabeled data that are more readily available.
This has resulted in ever more useful pre-trained token em-
beddings (Mikolov et al., 2013a; Pennington et al., 2014;
Bojanowski et al., 2017). However, the subtle differences
in the meaning of a token in varying contexts are lost when
each word is associated with a single representation. Recent
techniques for learning contextual embeddings (McCann
et al., 2017; Peters et al., 2018; Radford et al., 2018; 2019;
Devlin et al., 2019; Yang et al., 2019) provide ways to com-
pute representations of tokens based on their surrounding
context, and have shown signiﬁcant accuracy improvements
in downstream tasks, even with only a small number of
task-speciﬁc parameters.

Inspired by the success of pre-trained contextual embed-
dings for natural languages, we present the ﬁrst attempt to
apply the underlying techniques to source code. In partic-
ular, BERT (Devlin et al., 2019) produces a bidirectional
Transformer encoder (Vaswani et al., 2017) by training it to
predict values of masked tokens, and whether two sentences
follow each other in a natural discourse. The pre-trained
model can be ﬁne-tuned for downstream supervised tasks
and has been shown to produce state-of-the-art results on

 
 
 
 
 
 
Learning and Evaluating Contextual Embedding of Source Code

a number of natural-language understanding benchmarks.
In this work, we derive a contextual embedding of source
code by training a BERT model on source code. We call our
model CuBERT, short for Code Understanding BERT.

In order to achieve this, we curate a massive corpus of
Python programs collected from GitHub. GitHub projects
are known to contain a large amount of duplicate code. To
avoid biasing the model to such duplicated code, we perform
deduplication using the method of Allamanis (2018). The
resulting corpus has 7.4 million ﬁles with a total of 9.3
billion tokens (16 million unique). For comparison, we
also train Word2Vec embeddings (Mikolov et al., 2013a;b),
namely, continuous bag-of-words (CBOW) and Skipgram
embeddings, on the same corpus.

For evaluating CuBERT, we create a benchmark of ﬁve clas-
siﬁcation tasks, and a sixth localization and repair task. The
classiﬁcation tasks range from classiﬁcation of source code
according to presence or absence of certain classes of bugs,
to mismatch between a function’s natural language descrip-
tion and its body, to predicting the right kind of exception to
catch for a given code fragment. The localization and repair
task, deﬁned for variable-misuse bugs (Vasic et al., 2019),
is a pointer-prediction task. Although similar tasks have
appeared in prior work, the associated datasets come from
different languages and varied sources; instead we create a
cohesive multiple-task benchmark dataset in this work. To
produce a high-quality dataset, we ensure that there is no
overlap between pre-training and ﬁne-tuning examples, and
that all of the tasks are deﬁned on Python code.

We ﬁne-tune CuBERT on each of the classiﬁcation tasks
and compare the results to multi-layered bidirectional
LSTM (Hochreiter & Schmidhuber, 1997) models, as well
as Transformers (Vaswani et al., 2017). We train the LSTM
models from scratch and also using pre-trainined Word2Vec
embeddings. Our results show that CuBERT consistently
outperforms these baseline models by 3.2 % to 14.7 %
across the classiﬁcation tasks. We perform a number of
additional studies by varying the sampling strategies used
for training Word2Vec models, and by varying program
lengths. In addition, we also show that CuBERT can be
ﬁne-tuned effectively using only 33% of the task-speciﬁc
labeled data and with only 2 epochs, and that, even then,
it attains results competitive to the baseline models trained
with the full datasets and many more epochs. CuBERT,
when ﬁne-tuned on the variable-misuse localization and
repair task, produces high classiﬁcation, localization and
localization+repair accuracies and outperforms published
state-of-the-art models (Hellendoorn et al., 2020; Vasic et al.,
2019). Our contributions are as follows:

• We show the efﬁcacy of the pre-trained contextual em-
bedding on ﬁve classiﬁcation tasks. Our ﬁne-tuned
models outperform baseline LSTM models (with/with-
out Word2Vec embeddings), as well as Transformers
trained from scratch, even with reduced training data.

• We evaluate CuBERT on a pointer prediction task and
show that it outperforms state-of-the-art results signiﬁ-
cantly.

• We make the models and datasets publicly available.1
We hope that future work beneﬁts from our contribu-
tions, by reusing our benchmark tasks, and by compar-
ing against our strong baseline models.

2. Related Work

Given the abundance of natural-language text, and the rel-
ative difﬁculty of obtaining labeled data, much effort has
been devoted to using large corpora to learn about language
in an unsupervised fashion, before trying to focus on tasks
with small labeled training datasets. Word2Vec (Mikolov
et al., 2013a;b) computed word embeddings based on word
co-occurrence and proximity, but the same embedding is
used regardless of the context. The continued advances in
word (Pennington et al., 2014) and subword (Bojanowski
et al., 2017) embeddings led to publicly released pre-trained
embeddings, used in a variety of tasks.

To deal with varying word context, contextual word embed-
dings were developed (McCann et al., 2017; Peters et al.,
2018; Radford et al., 2018; 2019), in which an embedding
is learned for the context of a word in a particular sentence,
namely the sequence of words preceding it and possibly
following it. BERT (Devlin et al., 2019) improved natural-
language pre-training by using a de-noising autoencoder.
Instead of learning a language model, which is inherently
sequential, BERT optimizes for predicting a noised word
within a sentence. Such prediction instances are gener-
ated by choosing a word position and either keeping it un-
changed, removing the word, or replacing the word with a
random wrong word. It also pre-trains with the objective
of predicting whether two sentences can be next to each
other. These pre-training objectives, along with the use of
a Transformer-based architecture, gave BERT an accuracy
boost in a number of NLP tasks over the state-of-the-art.
BERT has been improved upon in various ways, including
modifying training objectives, utilizing ensembles, combin-
ing attention with autoregression (Yang et al., 2019), and
expanding pre-training corpora and time (Liu et al., 2019).
However, the main architecture of BERT seems to hold up
as the state-of-the-art, as of this writing.

• We present the ﬁrst attempt at pre-training a BERT

contextual embedding of source code.

1https://github.com/google-research/

google-research/tree/master/cubert

Learning and Evaluating Contextual Embedding of Source Code

In the space of programming languages, embeddings have
been learned for speciﬁc software-engineering tasks (Chen
& Monperrus, 2019). These include embeddings of variable
and method identiﬁers using local and global context (Al-
lamanis et al., 2015), abstract syntax trees (ASTs) (Mou
et al., 2016; Zhang et al., 2019), AST paths (Alon et al.,
2019), memory heap graphs (Li et al., 2016), and ASTs
enriched with data-ﬂow information (Allamanis et al., 2018;
Hellendoorn et al., 2020). These approaches require an-
alyzing source code beyond simple tokenization. In this
work, we derive a pre-trained contextual embedding of tok-
enized source code without explicitly modeling source-code-
speciﬁc information, and show that the resulting embedding
can be effectively ﬁne-tuned for downstream tasks.

CodeBERT (Feng et al., 2020) targets paired natural-
language (NL) and multi-lingual programming-language
(PL) tasks, such as code search and generation of code doc-
umentation. It pre-trains a Transformer encoder by treating
a natural-language description of a function and its body
as separate sentences in the sentence-pair representation
of BERT. We also handle natural language directly, but do
not require such a separation. Natural-language tokens can
be mixed with source-code tokens both within and across
sentences in our encoding. One of our benchmark tasks,
function-docstring mismatch, illustrates the ability of Cu-
BERT to handle NL-PL tasks.

3. Experimental Setup

We now outline our benchmarks and experimental study.
The supplementary material contains deeper detail aimed at
reproducing our results.

3.1. Code Corpus for Fine-Tuning Tasks

We use the ETH Py150 corpus (Raychev et al., 2016) to gen-
erate datasets for the ﬁne-tuning tasks. This corpus consists
of 150K Python ﬁles from GitHub, and is partitioned into
a training split (100K ﬁles) and a test split (50K ﬁles). We
held out 10K ﬁles from the training split as a validation split.
We deduplicated the dataset in the fashion of Allamanis
(2018). Finally, we drop from this corpus those projects
for which licensing information was not available or whose
licenses restrict use or redistribution. We call the resulting
corpus the ETH Py150 Open corpus.2 This is our Python
ﬁne-tuning code corpus, and it consists of 74,749 training
ﬁles, 8,302 validation ﬁles, and 41,457 test ﬁles.

3.2. The GitHub Python Pre-Training Code Corpus

We used the public GitHub repository hosted on Google’s
BigQuery platform (the github repos dataset under Big-

2https://github.com/

google-research-datasets/eth_py150_open

Query’s public-data project, bigquery-public-data).
We extracted all ﬁles ending in .py, under open-source, re-
distributable licenses, removed symbolic links, and retained
only ﬁles reported to be in the refs/heads/master
branch. This resulted in about 16.2 million ﬁles.

To avoid duplication between pre-training and ﬁne-tuning
data, we removed ﬁles that had high similarity to the ﬁles in
the ETH Py150 Open corpus, using the method of Allamanis
(2018). In particular, two ﬁles are considered similar to each
other if the Jaccard similarity between the sets of tokens
(identiﬁers and string literals) is above 0.8 and in addition,
it is above 0.7 for multi-sets of tokens. This brought the
dataset to 14.3 million ﬁles. We then further deduplicated
the remaining ﬁles, by clustering them into equivalence
classes holding similar ﬁles according to the same similarity
metric, and keeping only one exemplar per equivalence class.
This helps avoid biasing the pre-trained embedding. Finally,
we removed ﬁles that could not be parsed. In the end, we
were left with 7.4 million Python ﬁles containing over 9.3
billion tokens. This is our Python pre-training code corpus.

3.3. Source-Code Modeling

We ﬁrst tokenize a Python program using the standard
Python tokenizer (the tokenize package). We leave lan-
guage keywords intact and produce special tokens for syn-
tactic elements that have either no string representation (e.g.,
DEDENT tokens, which occur when a nested program scope
concludes), or ambiguous interpretation (e.g., new-line char-
acters inside string literals, at the logical end of a Python
statement, or in the middle of a Python statement result in
distinct special tokens). We split identiﬁers according to
common heuristic rules (e.g., snake or Camel case). Finally,
we split string literals using heuristic rules, on white-space
characters, and on special characters. We limit all thus pro-
duced tokens to a maximum length of 15 characters. We
call this the program vocabulary. Our Python pre-training
code corpus contained 16 million unique tokens.

We greedily compress the program vocabulary into a
subword vocabulary (Schuster & Nakajima, 2012) us-
ing the SubwordTextEncoder from the Tensor2Tensor
project (Vaswani et al., 2018)3, resulting in about 50K to-
kens. All words in the program vocabulary can be losslessly
encoded using one or more of the subword tokens.

We tokenize programs ﬁrst into program tokens, as de-
scribed above, and then encode those tokens one by one
in the subword vocabulary. The objective of this encod-
ing scheme is to preserve syntactically meaningful bound-
aries of tokens. For example, the identiﬁer “snake case”

3https://github.com/tensorflow/

tensor2tensor/blob/master/tensor2tensor/
data_generators/text_encoder.py

Learning and Evaluating Contextual Embedding of Source Code

could be encoded as “sna ke
ca se”, preserving the
snake case split of its characters, even if the subtoken “e c”
were very popular in the corpus; the latter encoding might
result in a smaller representation but would lose the intent of
the programmer in using a snake-case identiﬁer. Similarly,
“i=0” may be very frequent in the corpus, but we still force
it to be encoded as separate tokens i, =, and 0, ensuring that
we preserve the distinction between operators and operands.
Both the BERT model and the Word2Vec embeddings are
built on the subword vocabulary.

Swapped Operand Pradel & Sen (2018) propose the
wrong binary operand task where a variable or constant
is used incorrectly in an expression, but that task is quite
similar to the variable-misuse task we already use. We
therefore deﬁne another class of operand errors where the
operands of non-commutative binary operators are swapped.
The operands can be arbitrary subexpressions, and are not
restricted to be just variables or constants. To simplify ex-
ample generation, we restrict this task to examples in which
the operator and operands all ﬁt within a single line.

3.4. Fine-Tuning Tasks

To evaluate CuBERT, we design ﬁve classiﬁcation tasks and
a multi-headed pointer task. These are motivated by prior
work, but unfortunately, the associated datasets come from
different languages and varied sources. We want the tasks
to be on Python code, and for accurate results, we ensure
that there is no overlap between pre-training and ﬁne-tuning
datasets. We therefore create all the tasks on the ETH Py150
Open corpus (see Section 3.1). As discussed in Section 3.2,
we ensure that there is no duplication between this and the
pre-training corpus. We hope that our datasets for these
tasks will be useful to others as well. The ﬁne-tuning tasks
are described below. A more detailed discussion is presented
in the supplementary material.

Variable-Misuse Classiﬁcation Allamanis et al. (2018)
observed that developers may mistakenly use an incorrect
variable in the place of a correct one. These mistakes may
occur when developers copy-paste similar code but forget
to rename all occurrences of variables from the original
fragment, or when there are similar variable names that can
be confused with each other. These can be subtle errors
that remain undetected during compilation. The task by
Allamanis et al. (2018) is to choose the correct variable name
at a location within a C# function. We take the classiﬁcation
version restated by Vasic et al. (2019), wherein, given a
function, the task is to predict whether there is a variable
misuse at any location in the function, without specifying
a particular location to consider. Here, the classiﬁer has to
consider all variables and their usages to make the decision.
In order to create negative (buggy) examples, we replace a
variable use at some location with another variable that is
deﬁned within the function.

Wrong Binary Operator Pradel & Sen (2018) proposed
the task of detecting whether a binary operator in a given
expression is correct. They use features extracted from
limited surrounding context. We use the entire function
with the goal of detecting whether any binary operator in
the function is incorrect. The negative examples are created
by randomly replacing some binary operator with another
type-compatible operator.

Function-Docstring Mismatch Developers are encour-
aged to write descriptive docstrings to explain the function-
ality and usage of functions. This provides parallel corpora
between code and natural language sentences that have been
used for machine translation (Barone & Sennrich, 2017),
detecting uninformative docstrings (Louis et al., 2018) and
to evaluate their utility to provide supervision in neural code
search (Cambronero et al., 2019). We prepare a sentence-
pair classiﬁcation problem where the function and its doc-
string form two distinct sentences. The positive examples
come from the correct function-docstring pairs. We create
negative examples by replacing correct docstrings with doc-
strings of other functions, randomly chosen from the dataset.
For this task, the existing docstring is removed from the
function body.

Exception Type While it is possible to write generic
exception handlers (e.g., “except Exception” in
Python), it is considered a good coding practice to catch
and handle the precise exceptions that can be raised by a
code fragment.4 We identiﬁed the 20 most common excep-
tion types from the GitHub dataset, excluding the catch-all
Exception (full list in Table 1 in the supplementary ma-
terial). Given a function with an except clause for one of
these exception types, we replace the exception with a spe-
cial “hole” token. The task is the multi-class classiﬁcation
problem of predicting the original exception type.

Variable-Misuse Localization and Repair As an in-
stance of a non-classiﬁcation task, we consider the joint
classiﬁcation, localization, and repair version of the variable-
misuse task from Vasic et al. (2019). Given a function, the
task is to predict one pointer (called the localization pointer)
to identify a variable-misuse location, and another pointer
(called the repair pointer) to identify a variable from the
same function that is the right one to use at the faulty loca-
tion. The model is also trained to classify functions that do
not contain any variable misuse as bug-free by making the
localization pointer point to a special location in the func-
tion. We create negative examples using the same method

4https://google.github.io/styleguide/

pyguide.html#24-exceptions

Learning and Evaluating Contextual Embedding of Source Code

Train

Validation

Test

Variable-Misuse Classiﬁcation
Wrong Binary Operator
Swapped Operand
Function-Docstring
Exception Type
Variable-Misuse Localization and Repair

700,708
459,400
236,246
340,846
18,480
700,708

8,192 (75,478)
8,192 (49,804)
8,192 (26,118)
8,192 (37,592)
2,088
(2,088)
8,192 (75,478)

378,440
251,804
130,972
186,698
10,348
378,440

Table 1. Benchmark ﬁne-tuning datasets. Note that for validation, we have subsampled the original datasets (in parentheses) down to
8,192 examples, except for exception classiﬁcation, which only had 2,088 validation examples, all of which are included.

as used in the Variable-Misuse Classiﬁcation task.

Table 1 lists the sizes of the resulting benchmark datasets
extracted from the ﬁne-tuning corpus. The Exception Type
task contains signiﬁcantly fewer examples than the other
tasks, since examples for this task only come from functions
that catch one of the chosen 20 exception types.

3.5. BERT for Source Code

The BERT model (Devlin et al., 2019) consists of a multi-
layered Transformer encoder. It is trained with two tasks:
(1) to predict the correct tokens in a fraction of all positions,
some of which have been replaced with incorrect tokens or
the special [MASK] token (the Masked Language Model
task, or MLM) and (2) to predict whether the two sentences
separated by the special [SEP] token follow each other
in some natural discourse (the Next-Sentence Prediction
task, or NSP). Thus, each example consists of one or two
sentences, where a sentence is the concatenation of con-
tiguous lines from the source corpus, sized to ﬁt the target
example length. To ensure that every sentence is treated in
multiple instances of both MLM and NSP, BERT by default
duplicates the corpus 10 times, and generates independently
derived examples from each duplicate. With 50 % proba-
bility, the second example sentence comes from a random
document (for NSP). A token is chosen at random for an
MLM prediction (up to 20 per example), and from those
chosen, 80 % are masked, 10 % are left undisturbed, and
10 % are replaced with a random token.

CuBERT is similarly formulated, but a CuBERT line is a log-
ical code line, as deﬁned by the Python standard. Intuitively,
a logical code line is the shortest sequence of consecutive
lines that constitutes a legal statement, e.g., it has correctly
matching parentheses. We count example lengths by count-
ing the subword tokens of both sentences (see Section 3.3).

We train the BERT Large model having 24 layers with 16
attention heads and 1024 hidden units. Sentences are cre-
ated from our pre-training dataset. Task-speciﬁc classiﬁers
pass the embedding of a special start-of-example [CLS]
token through feed-forward and softmax layers. For the
pointer prediction task, the pointers are computed exactly as

by Vasic et al. (2019); whereas in that work, the pointers are
computed from the output of an LSTM layer, in our model,
they are computed from the last-layer hiddens of BERT.

3.6. Baselines

3.6.1. WORD2VEC

We train Word2Vec models using the same pre-training
corpus as the BERT model. To maintain parity, we gen-
erate the dataset for Word2Vec using the same pipeline as
BERT but by disabling masking and generation of negative
examples for NSP. The dataset is generated without any
duplication. We train both CBOW and Skipgram models
using GenSim ( ˇReh˚uˇrek & Sojka, 2010). To deal with the
large vocabulary, we use negative sampling and hierarchical
softmax (Mikolov et al., 2013a;b) to train the two versions.
In all, we obtain four types of Word2Vec embeddings.

3.6.2. BIDIRECTIONAL LSTM AND TRANSFORMER

In order to obtain context-sensitive encodings of input se-
quences for the ﬁne-tuning tasks, we use multi-layered bidi-
rectional LSTMs (Hochreiter & Schmidhuber, 1997) (BiL-
STMs). These are initialized with the pre-trained Word2Vec
embeddings. To further evaluate whether LSTMs alone
are sufﬁcient without pre-training, we also train BiLSTMs
with an embedding matrix that is initialized from scratch
with Xavier initialization (Glorot & Bengio, 2010). We
also trained Transformer models (Vaswani et al., 2017) for
our ﬁne-tuning tasks. We used BERT’s own Transformer
implementation, to ensure comparability of results. For com-
parison with prior work, we use the unidirectional LSTM
and pointer model from Vasic et al. (2019) for the Variable-
Misuse Localization and Repair task.

4. Experimental Results

4.1. Training Details

CuBERT’s dataset generation duplicates the corpus 10 times,
whereas Word2Vec is trained without duplication. To com-
pensate for this difference, we trained Word2Vec for 10

Learning and Evaluating Contextual Embedding of Source Code

epochs and CuBERT for 1 epoch. We chose models by
validation accuracy, both during hyperparameter searches,
and during model selection within an experiment.

We pre-train CuBERT with the default conﬁguration of the
BERT Large model, one model per example length (128,
256, 512, and 1,024 subword tokens) with batch sizes of
8,192, 4,096, 2,048, and 1,024 respectively, and the default
BERT learning rate of 1 × 10−4. Fine-tuned models also
used the same batch sizes as for pre-training, and BERT’s
default learning rate (5 × 10−5). For both, we gradually
warm up the learning rate for the ﬁrst 10 % of examples,
which is BERT’s default value.

For Word2Vec, when training with negative samples, we
choose 5 negative samples. The embedding size for all
the Word2Vec pre-trained models is set at 1,024. For the
baseline BiLSTM models, we performed a hyperparameter
search on each task and pre-training conﬁguration separately
(5 tasks, each trained with the four Word2Vec embeddings,
plus the randomly initialized embeddings), for the 512 ex-
ample length. For each of these 25 task conﬁgurations, we
varied the number of layers (1 to 3), the number of hid-
den units (128, 256 and 512), the LSTM output dropout
probability (0.1 and 0.5), and the learning rate (1 × 10−3,
1 × 10−4 and 1 × 10−5). We used the Adam (Kingma &
Ba, 2014) optimizer throughout, and batch size 8,192 for
all tasks except the Exception-Type task, for which we used
batch size 64. Invariably, the best hyperparameter selection
had 512 hidden units per layer and learning rate of 1 × 10−3,
but the number of layers (mostly 2 or 3) and dropout prob-
ability varied across best task conﬁgurations. Though no
single Word2Vec conﬁguration is the best, CBOW trained
with negative sampling gives the most consistent results
overall.

For the baseline Transformer models, we originally at-
tempted to train a model of the same conﬁguration as Cu-
BERT. However, the sizes of our ﬁne-tuning datasets seemed
too small to train that large a Transformer. Instead, we
performed a hyperparameter search for each task individ-
ually, for the 512 example length. We varied the num-
ber of transformer layers (1 to 6), hidden units (128, 256
and 512), learning rates (1 × 10−3, 5 × 10−4, 1 × 10−4,
5 × 10−5 and 1 × 10−5) and batch sizes (512, 1,024, 2,048
and 4,096). The best architecture varied across the tasks: for
example, 5 layers with 128 hiddens and the highest learning
rate worked best for the Function-Docstring task, whereas
for the Exception-Type task, 2 layers, 512 hiddens, and the
second lowest learning rate worked best.

Finally, for our baseline pointer model (referred to as
LSTM+pointer below) we searched over the following hy-
perparameter choices: hidden sizes of 512 and 1,024, token
embedding sizes of 512 and 1,024, and learning rates of
1 × 10−1, 1 × 10−2 and 1 × 10−3. We used the Adam op-

timizer, a batch size of 256, and example length 512. In
contrast to the original work (Vasic et al., 2019), we gen-
erated one pair of buggy/bug-free examples per function
(rather than one per variable use, per function, which would
bias towards longer functions), and use CuBERT’s subword-
tokenized vocabulary of 50K subtokens (rather than a lim-
ited full-token vocabulary, which leaves many tokens out of
vocabulary).

We used TPUs for training our models, except for pre-
training Word2Vec embeddings, and the pointer model by
Vasic et al. (2019). For the rest, and for all evaluations,
we used P100 or V100 GPUs. All experiments using pre-
trained word or contextual embeddings continued to ﬁne-
tune weights throughout training.

4.2. Research Questions

We set out to answer the following research questions. We
will address each with our results.

1. Do contextual embeddings help with source-code anal-
ysis tasks, when pre-trained on an unlabeled code cor-
pus? We compare CuBERT to BiLSTM models with
and without pre-trained Word2Vec embeddings on the
classiﬁcation tasks (Section 4.3).

2. Does ﬁne-tuning actually help, or is the Transformer
model by itself sufﬁcient? We compare ﬁne-tuned
CuBERT models to Transformer-based models trained
from scratch on the classiﬁcation tasks (Section 4.4).

3. How does the performance of CuBERT on the classiﬁ-
cation tasks scale with the amount of labeled training
data? We compare the performance of ﬁne-tuned Cu-
BERT models when ﬁne-tuning with 33 %, 66 % and
100 % of the task training data (Section 4.5).

4. How does context size affect CuBERT? We compare
ﬁne-tuning performance for different example lengths
on the classiﬁcation tasks (Section 4.6).

5. How does CuBERT perform on complex tasks, against
state-of-the-art methods? We implemented and ﬁne-
tuned a model for a multi-headed pointer prediction
task, namely, the Variable-Misuse Localization and
Repair task (Section 4.7). We compare it to the models
from (Vasic et al., 2019) and (Hellendoorn et al., 2020).

Except for Section 4.6, all the results are presented for se-
quences of length 512. We give examples of classiﬁcation
instances in the supplementary material and include visual-
izations of attention weights for them.

Learning and Evaluating Contextual Embedding of Source Code

BiLSTM
(100 epochs)

Setting

Misuse

Operator Operand Docstring Exception

From scratch

CBOW

Skipgram

ns
hs
ns
hs

76.29 %
88.07 %
83.65 %
80.33 % 86.82 % 89.80 %
85.85 % 90.14 % 87.69 %
78.00 %
83.81 %
89.31 %
85.14 %
77.06 %
88.80 %
89.75 %
80.53 % 86.34 %

76.01 %
52.79 %
89.08 % 67.01 %
60.31 %
60.07 %
65.06 %

CuBERT

2 epochs
10 epochs
20 epochs

94.04 %
95.14 %
95.21 %

89.90 %
92.15 %
92.46 %

92.20 %
93.62 %
93.36 %

97.21 %
98.08 %
98.09 %

61.04 %
77.97 %
79.12 %

Transformer

100 epochs

78.28 %

76.55 %

87.83 %

91.02 %

49.56 %

Table 2. Test accuracies of ﬁne-tuned CuBERT against BiLSTM (with and without Word2Vec embeddings) and Transformer trained from
scratch on the classiﬁcation tasks. “ns” and “hs” respectively refer to negative sampling and hierarchical softmax settings used for training
CBOW and Skipgram models. “From scratch” refers to training with freshly initialized token embeddings, without pre-training.

4.3. Contextual vs. Word Embeddings

The purpose of this analysis is to understand how much pre-
trained contextual embeddings help, compared to word em-
beddings. For each classiﬁcation task, we trained BiLSTM
models starting with each of the Word2Vec embeddings,
namely, continuous bag of words (CBOW) and Skipgram
trained with negative sampling or hierarchical softmax. We
trained the BiLSTM models for 100 epochs and the Cu-
BERT models for 20 epochs, and all models stopped im-
proving by the end.

The resulting test accuracies are shown in Table 2 (ﬁrst 5
rows and next-to-last row). CuBERT consistently outper-
forms BiLSTM (with the best task-wise Word2Vec conﬁgu-
ration) on all tasks, by a margin of 3.2 % to 14.7 %. Thus,
the pre-trained contextual embedding provides superior re-
sults even with a smaller budget of 20 epochs, compared
to the 100 epochs used for BiLSTMs. The Exception-Type
classiﬁcation task has an order of magnitude less training
data than the other tasks (see Table 1). The difference be-
tween the performance of BiLSTM and CuBERT is substan-
tially higher for this task. Thus, ﬁne-tuning is of much value
for tasks with limited labeled training data.

We analyzed the performance of CuBERT with the reduced
ﬁne-tuning budget of only 2 and 10 epochs (see the remain-
ing rows of the CuBERT section in Table 2). Except for
the Exception Type task, CuBERT outperforms the best
100-epoch BiLSTM within 2 ﬁne-tuning epochs. On the
Exception-Type task, CuBERT with 2 ﬁne-tuning epochs
outperforms all but two conﬁgurations of the BiLSTM base-
line. This shows that, even when restricted to just a few
ﬁne-tuning epochs, CuBERT can reach accuracies that are
comparable to or better than those of BiLSTMs trained with
Word2Vec embeddings.

To sanity-check our ﬁndings about BiLSTMs, we also
trained the BiLSTM models from scratch, without pre-

trained embeddings. The results are shown in the ﬁrst row
of Table 2. Compared to those, the use of Word2Vec embed-
dings performs better by a margin of 2.7 % to 14.2 %.

4.4. Is Transformer All You Need?

One may wonder if CuBERT’s promising results derive
more from using a Transformer-based model for its classi-
ﬁcation tasks, and less from the actual, unsupervised pre-
training. Here we compare our results on the classiﬁcation
tasks to a Transformer-based model trained from scratch,
i.e., without the beneﬁt of a pre-trained embedding. As
discussed in Section 4.1, the size of the training data limited
us to try out Transformers that were substantially smaller
than the CuBERT model (BERT Large architecture). All
the Transformer models were trained for 100 epochs during
which their performance stopped improving. We selected
the best model within the chosen hyperparameters for each
task based on best validation accuracy.

As seen from the last row of Table 2, the performance of Cu-
BERT is substantially higher than the Transformer models
trained from scratch. Thus, for the same choice of archi-
tecture (i.e., Transformer) pre-training seems to help by
enabling training of a larger and better model.

4.5. The Effects of Little Supervision

The big draw of unsupervised pre-training followed by
ﬁne-tuning is that some tasks have small labeled datasets.
We study here how CuBERT fares with reduced training
data. We sampled uniformly the ﬁne-tuning dataset to 33 %
and 66 % of its size, and produced corresponding training
datasets for each classiﬁcation task. We then ﬁne-tuned
the pre-trained CuBERT model with each of the 3 different
training splits. Validation and testing were done with the
same original datasets. Table 3 shows the results.

The Function Docstring task seems robust to the reduction

Learning and Evaluating Contextual Embedding of Source Code

Best of
# Epochs

Train
Fraction

Misuse Operator Operand Docstring Exception

2

10

20

100 %
66 %
33 %

100 %
66 %
33 %

100 %
66 %
33 %

94.04 % 89.90 %
93.11 % 88.76 %
91.40 % 86.42 %

95.14 % 92.15 %
94.78 % 91.51 %
94.28 % 90.66 %

95.21 % 92.46 %
94.90 % 91.79 %
94.45 % 91.09 %

92.20 %
91.61 %
90.52 %

93.62 %
93.37 %
92.58 %

93.36 %
93.39 %
92.82 %

97.21 %
97.04 %
96.38 %

98.08 %
97.93 %
97.36 %

98.09 %
97.99 %
97.63 %

61.04 %
19.49 %
20.09 %

77.97 %
75.24 %
67.34 %

79.12 %
77.31 %
74.98 %

Table 3. Effects of reducing training-split size on ﬁne-tuning performance on the classiﬁcation tasks.

Length Misuse Operator Operand Docstring Exception Misuse on BiLSTM

128
256
512
1024

83.97 % 79.29 %
92.02 % 88.19 %
95.21 % 92.46 %
95.83 % 93.38 %

78.02 %
88.03 %
93.36 %
95.62 %

98.19 %
98.14 %
98.09 %
97.90 %

62.03 %
72.80 %
79.12 %
81.27 %

74.32 %
78.47 %
80.33 %
81.92 %

Table 4. Best out of 20 epochs of ﬁne-tuning, for four example lengths, on the classiﬁcation tasks. For contrast, we also include results for
Variable Misuse using the BiLSTM Word2Vec (CBOW + ns) classiﬁer as length varies.

of the training dataset, both early and late in the ﬁne-tuning
process (that is, within 2 vs. 20 epochs), whereas the Excep-
tion Classiﬁcation task is heavily impacted by the dataset
reduction, given that it has relatively few training exam-
ples to begin with. Interestingly enough, for some tasks,
even ﬁne-tuning for only 2 epochs and only using a third of
the training data outperforms the baselines. For example,
for Variable Misuse and Function Docstring, CuBERT at 2
epochs and 33 % of training data substantially outperforms
the BiLSTM with Word2Vec and the Transformer baselines.

4.6. The Effects of Context

Context size is especially useful in code tasks, given that
some relevant information may lie many “sentences” away
from its locus of interest. Here we study how reducing
the context length (i.e., the length of the examples used to
pre-train and ﬁne-tune) affects performance. We produce
data with shorter example lengths, by ﬁrst pre-training a
model on a given example length, and then ﬁne-tuning that
model on the corresponding task with examples of that same
example length.5 Table 4 shows the results.

Although context seems to be important to most tasks, the
Function Docstring task paradoxically improves with less
context. This may be because the task primarily depends on

5Note that we did not attempt to, say, pre-train on length 1,024
and then ﬁne-tune that model on length 256-examples, which may
also be a practical scenario.

comparison between the docstring and the function signa-
ture, and including more context dilutes the model’s focus.

For comparison, we also evaluated the BiLSTM model on
varying example lengths for the Variable-Misuse task with
CBOW and negative sampling (last column of Table 4).
More context does seem to beneﬁt the BiLSTM Variable-
Misuse classiﬁer as well. However, the improvement offered
by CuBERT with increasing context is signiﬁcantly greater.

4.7. Evaluation on a Multi-Headed Pointer Task

We now discuss the results of ﬁne-tuning CuBERT to predict
the localization and repair pointers for the variable-misuse
task. For this task, we implement the multi-headed pointer
model from Vasic et al. (2019) on top of CuBERT. The
baseline consists of the same pointer model on a unidirec-
tional LSTM as used by Vasic et al. (2019). We refer to
these models as CuBERT+pointer and LSTM+pointer, re-
spectively. Due to limitations of space, we omit the details
of the pointer model and refer the reader to the above pa-
per. However, the two implementations are identical above
the sequence encoding layer; the difference is the BERT
encoder versus an LSTM encoder. As reported in Section 4
of that work, to enable comparison with an enumerative
approach, the evaluation was performed only on 12K test
examples. Instead, here we report the numbers on all 378K
of our test examples for both models.

We trained the baseline model for 100 epochs and ﬁne-tuned

Learning and Evaluating Contextual Embedding of Source Code

Model

Test Data

Setting

True
Positive

Classiﬁcation Localization Loc+Repair
Accuracy

Accuracy

Accuracy

LSTM

CuBERT

CuBERT

Hellendoorn et al. (2020)

C

C

H

H

100 epochs

82.41 %

2 epochs
10 epochs
20 epochs

2 epochs
10 epochs
20 epochs

96.90 %
97.23 %
97.27 %

95.63 %
96.07 %
96.14 %

79.30 %

94.87 %
95.49 %
95.40 %

90.71 %
91.71 %
91.49 %

81.90 %

64.39 %

91.14 %
92.33 %
92.12 %

83.50 %
85.37 %
84.85 %

56.89 %

89.41 %
90.84 %
90.61 %

80.77 %
82.91 %
82.30 %

73.80 %

Table 5. Variable-misuse localization and repair task. Comparison of the LSTM+pointer model (Vasic et al., 2019) to our ﬁne-tuned
CuBERT+pointer model. We also show results on the test data by Hellendoorn et al. (2020) computed by us and reported by the authors in
their Table 1. In the Test Data column, C means our CuBERT test dataset, and H means the test dataset used by Hellendoorn et al. (2020).

CuBERT for 2, 10, and 20 epochs. Table 5 gives the results
along the same metrics as Vasic et al. (2019). The metrics
are deﬁned as follows: 1) True Positive is the percentage of
bug-free functions classiﬁed as bug-free. 2) Classiﬁcation
Accuracy is the percentage of correctly classiﬁed examples
(between bug-free and buggy). 3) Localization Accuracy is
the percentage of buggy examples for which the localization
pointer correctly identiﬁes the bug location. 4) Localiza-
tion+Repair Accuracy is the percentage of buggy examples
for which both the localization and repair pointers make
correct predictions. As seen from Table 5 (top 4 rows),
CuBERT+pointer outperforms LSTM+pointer consistently
across all the metrics, and even within 2 and 10 epochs.

More recently, Hellendoorn et al. (2020) evaluated hybrid
models for the same task, combining graph neural networks,
Transformers, and RNNs, and greatly improving prior re-
sults. To compare, we obtained the same test dataset from
the authors, and evaluated our CuBERT ﬁne-tuned model
on it. The last four rows of Table 5 show our results and
the results reported in that work. Interestingly, the models
by Hellendoorn et al. (2020) make use of richer input rep-
resentations, including syntax, data ﬂow, and control ﬂow.
Nevertheless, CuBERT outperforms them while using only
a lexical representation of the input program.

5. Conclusions and Future Work

We present the ﬁrst attempt at pre-trained contextual em-
bedding of source code by training a BERT model, called
CuBERT, which we ﬁne-tuned on ﬁve classiﬁcation tasks,
and compared against BiLSTM with Word2Vec embeddings
and Transformer models. As a more challenging task, we
also evaluated CuBERT on a multi-headed pointer predic-
tion task. CuBERT outperformed the baseline models con-
sistently. We evaluated CuBERT with less data and fewer
epochs, highlighting the beneﬁts of pre-training on a mas-

sive code corpus.

We use only source-code tokens and leave it to the underly-
ing Transformer model to infer any structural interactions
between them through self-attention. Prior work (Allamanis
et al., 2018; Hellendoorn et al., 2020) has argued for ex-
plicitly using structural program information (e.g., control
ﬂow and data ﬂow). It is an interesting avenue of future
work to incorporate such information in pre-training using
relation-aware Transformers (Shaw et al., 2018). However,
our improved results in comparison to Hellendoorn et al.
(2020) show that CuBERT is a simple yet powerful tech-
nique and provides a strong baseline for future work on
source-code representations.

While surpassing the accuracies achieved by CuBERT with
newer models and pre-training/ﬁne-tuning methods would
be a natural extension to this work, we also envision other
follow-up work. There is increasing interest in develop-
ing pre-training methods that can produce smaller models
more efﬁciently and that trade-off accuracy for reduced
model size. Further, our benchmark could be valuable to
techniques that explore other program representations (e.g.,
trees and graphs), in multi-task learning, and to develop
related tasks such as program synthesis.

Acknowledgements

We are indebted to Daniel Tarlow for his guidance and gen-
erous advice throughout the development of this work. Our
work has also improved thanks to feedback, use cases, help-
ful libraries, and proofs of concept offered by David Bieber,
Vincent Hellendoorn, Ben Lerner, Hyeontaek Lim, Rishabh
Singh, Charles Sutton, and Manushree Vijayvergiya. Fi-
nally, we are grateful to the anonymous reviewers, who gave
useful, constructive comments and helped us improve our
presentation and results.

Learning and Evaluating Contextual Embedding of Source Code

References

Allamanis, M.

The adverse effects of code duplica-
CoRR,
tion in machine learning models of code.
abs/1812.06469, 2018. URL http://arxiv.org/
abs/1812.06469.

Allamanis, M., Barr, E. T., Bird, C., and Sutton, C. Sug-
gesting accurate method and class names. In Proceed-
ings of the 2015 10th Joint Meeting on Foundations
of Software Engineering, ESEC/FSE 2015, pp. 38–49,
New York, NY, USA, 2015. ACM. ISBN 978-1-4503-
3675-8. doi: 10.1145/2786805.2786849. URL http:
//doi.acm.org/10.1145/2786805.2786849.

Allamanis, M., Brockschmidt, M., and Khademi, M. Learn-
ing to represent programs with graphs. In International
Conference on Learning Representations, 2018.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. Code2vec:
Learning distributed representations of code. Proc. ACM
Program. Lang., 3(POPL):40:1–40:29, January 2019.
ISSN 2475-1421. doi: 10.1145/3290353. URL http:
//doi.acm.org/10.1145/3290353.

Barone, A. V. M. and Sennrich, R. A parallel corpus of
python functions and documentation strings for auto-
mated code documentation and code generation. arXiv
preprint arXiv:1707.02275, 2017.

Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. En-
riching word vectors with subword information. Transac-
tions of the Association for Computational Linguistics, 5:
135–146, 2017.

Cambronero, J., Li, H., Kim, S., Sen, K., and Chandra, S.
When deep learning met code search. arXiv preprint
arXiv:1905.03813, 2019.

Chen, Z. and Monperrus, M. A literature study of embed-
dings on source code. arXiv preprint arXiv:1904.03061,
2019.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp.
4171–4186, Minneapolis, Minnesota, June 2019. Asso-
ciation for Computational Linguistics. doi: 10.18653/
v1/N19-1423. URL https://www.aclweb.org/
anthology/N19-1423.

Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong,
M., Shou, L., Qin, B., Liu, T., Jiang, D., et al. Code-
bert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155, 2020.

Glorot, X. and Bengio, Y. Understanding the difﬁculty
of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on
artiﬁcial intelligence and statistics, pp. 249–256, 2010.

Gu, X., Zhang, H., Zhang, D., and Kim, S. Deep api
In Proceedings of the 2016 24th ACM SIG-
learning.
SOFT International Symposium on Foundations of Soft-
ware Engineering, FSE 2016, pp. 631–642, New York,
NY, USA, 2016. ACM. ISBN 978-1-4503-4218-6. doi:
10.1145/2950290.2950334. URL http://doi.acm.
org/10.1145/2950290.2950334.

Gupta, R., Pal, S., Kanade, A., and Shevade, S. Deep-
ﬁx: Fixing common c language errors by deep learn-
In Proceedings of the Thirty-First AAAI Confer-
ing.
ence on Artiﬁcial Intelligence, AAAI’17, pp. 1345–1351.
AAAI Press, 2017. URL http://dl.acm.org/
citation.cfm?id=3298239.3298436.

Hellendoorn, V. J., Sutton, C., Singh, R., Maniatis, P., and
Bieber, D. Global relational models of source code. In
International Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=B1lnbRNtwr.

Hindle, A., Barr, E. T., Su, Z., Gabel, M., and Devanbu, P.
On the naturalness of software. In 2012 34th International
Conference on Software Engineering (ICSE), pp. 837–
847, June 2012. doi: 10.1109/ICSE.2012.6227135.

Hochreiter, S. and Schmidhuber, J. Long short-term
memory. Neural Comput., 9(8):1735–1780, Novem-
ber 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.
9.8.1735. URL http://dx.doi.org/10.1162/
neco.1997.9.8.1735.

Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L.
Mapping language to code in programmatic context.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, Brussels,
Belgium, October 31 - November 4, 2018, pp. 1643–
1652, 2018. URL https://www.aclweb.org/
anthology/D18-1192/.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. S.
Gated graph sequence neural networks. In 4th Interna-
tional Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
Track Proceedings, 2016. URL http://arxiv.org/
abs/1511.05493.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:

Learning and Evaluating Contextual Embedding of Source Code

A robustly optimized BERT pretraining approach. CoRR,
abs/1907.11692, 2019. URL http://arxiv.org/
abs/1907.11692.

Louis, A., Dash, S. K., Barr, E. T., and Sutton, C. Deep
learning to detect redundant method comments. arXiv
preprint arXiv:1806.04616, 2018.

Martin, R. C. Clean Code: A Handbook of Agile Soft-
ware Craftsmanship. Prentice Hall PTR, Upper Saddle
ISBN 0132350882,
River, NJ, USA, 1 edition, 2008.
9780132350884.

McCann, B., Bradbury, J., Xiong, C., and Socher, R.
Learned in translation: Contextualized word vectors. In
Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fer-
gus, R., Vishwanathan, S., and Garnett, R. (eds.), Ad-
vances in Neural Information Processing Systems 30, pp.
6294–6305. Curran Associates, Inc., 2017.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient
estimation of word representations in vector space. In 1st
International Conference on Learning Representations,
ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013,
Workshop Track Proceedings, 2013a. URL http://
arxiv.org/abs/1301.3781.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
Dean, J. Distributed representations of words and phrases
and their compositionality. In Burges, C. J. C., Bottou,
L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.
(eds.), Advances in Neural Information Processing Sys-
tems 26, pp. 3111–3119. Curran Associates, Inc., 2013b.

Mou, L., Li, G., Zhang, L., Wang, T., and Jin, Z.
Convolutional neural networks over tree structures
In Proceed-
for programming language processing.
ings of the Thirtieth AAAI Conference on Artiﬁcial
Intelligence, AAAI’16, pp. 1287–1293. AAAI Press,
URL http://dl.acm.org/citation.
2016.
cfm?id=3015812.3016002.

Oda, Y., Fudaba, H., Neubig, G., Hata, H., Sakti, S., Toda,
T., and Nakamura, S. Learning to generate pseudo-code
from source code using statistical machine translation
(t). In 2015 30th IEEE/ACM International Conference
on Automated Software Engineering (ASE), pp. 574–584.
IEEE, 2015.

Pennington, J., Socher, R., and Manning, C. D. Glove:
Global vectors for word representation. In In EMNLP,
2014.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations. In Proceedings of NAACL-HLT, pp.
2227–2237, 2018.

Pradel, M. and Sen, K. Deepbugs: A learning approach to
name-based bug detection. Proc. ACM Program. Lang.,
2(OOPSLA):147:1–147:25, October 2018. ISSN 2475-
1421. doi: 10.1145/3276517. URL http://doi.acm.
org/10.1145/3276517.

Pu, Y., Narasimhan, K., Solar-Lezama, A., and Barzilay, R.
Sk p: A neural program corrector for moocs. In Com-
panion Proceedings of the 2016 ACM SIGPLAN Interna-
tional Conference on Systems, Programming, Languages
and Applications: Software for Humanity, SPLASH Com-
panion 2016, pp. 39–40, New York, NY, USA, 2016.
ACM. ISBN 978-1-4503-4437-1. doi: 10.1145/2984043.
2989222. URL http://doi.acm.org/10.1145/
2984043.2989222.

I.

Salimans,

Radford, A., Narasimhan, K.,

T.,
Improving language un-
and Sutskever,
URL
derstanding by generative pre-training.
https://s3-us-west-2.
com/openai-
assets/researchcovers/languageunsupervised/language
understanding paper. pdf, 2018.

amazonaws.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. OpenAI Blog, 1(8), 2019.

Raychev, V., Vechev, M., and Yahav, E. Code com-
In Proceed-
pletion with statistical language models.
ings of the 35th ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation, PLDI
’14, pp. 419–428, New York, NY, USA, 2014. ACM.
doi: 10.1145/2594291.
ISBN 978-1-4503-2784-8.
2594321. URL http://doi.acm.org/10.1145/
2594291.2594321.

Raychev, V., Bielik, P., and Vechev, M. T. Probabilistic
model for code with decision trees. In Proceedings of
the 2016 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and
Applications, OOPSLA 2016, part of SPLASH 2016, Ams-
terdam, The Netherlands, October 30 - November 4, 2016,
pp. 731–747, 2016.

ˇReh˚uˇrek, R. and Sojka, P. Software Framework for Topic
Modelling with Large Corpora. In Proceedings of the
LREC 2010 Workshop on New Challenges for NLP
Frameworks, pp. 45–50, Valletta, Malta, May 2010.
http://is.muni.cz/publication/
ELRA.
884893/en.

Schuster, M. and Nakajima, K. Japanese and korean voice
search. In International Conference on Acoustics, Speech
and Signal Processing, pp. 5149–5152, 2012.

Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention
with relative position representations. arXiv preprint
arXiv:1803.02155, 2018.

Learning and Evaluating Contextual Embedding of Source Code

Vasic, M., Kanade, A., Maniatis, P., Bieber, D., and Singh,
R. Neural program repair by jointly learning to localize
and repair. CoRR, abs/1904.01720, 2019. URL http:
//arxiv.org/abs/1904.01720.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-
tion is all you need. In Guyon, I., Luxburg, U. V., Bengio,
S., Wallach, H., Fergus, R., Vishwanathan, S., and Gar-
nett, R. (eds.), Advances in Neural Information Process-
ing Systems 30, pp. 5998–6008. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf.

Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,
A. N., Gouws, S., Jones, L., Kaiser, L., Kalchbrenner,
N., Parmar, N., Sepassi, R., Shazeer, N., and Uszko-
reit, J. Tensor2tensor for neural machine translation.
In Proceedings of the 13th Conference of the Associa-
tion for Machine Translation in the Americas, AMTA
2018, Boston, MA, USA, March 17-21, 2018 - Volume
1: Research Papers, pp. 193–199, 2018. URL https:
//www.aclweb.org/anthology/W18-1819/.

Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhut-
dinov, R., and Le, Q. V. Xlnet: Generalized autore-
gressive pretraining for language understanding. CoRR,
abs/1906.08237, 2019. URL http://arxiv.org/
abs/1906.08237.

Zhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., and
Liu, X. A novel neural source code representation based
on abstract syntax tree. In 2019 IEEE/ACM 41st Interna-
tional Conference on Software Engineering (ICSE), pp.
783–794. IEEE, 2019.

Learning and Evaluating Contextual Embedding of Source Code

A. Open-Sourced Artifacts

We release data and some source-code utilities at
https://github.com/google-research/
google-research/tree/master/cubert. The
repository contains the following:

GitHub Manifest A list of all the ﬁle versions we included
into our pre-training corpus, after removing ﬁles simi-
lar to the ﬁne-tuning corpus6, and after deduplication.
The manifest can be used to retrieve the ﬁle contents
from GitHub or Google’s BigQuery. This dataset was
retrieved from Google’s BigQuery on June 21, 2020.

Vocabulary Our subword vocabulary, computed from the

pre-training corpus.

Pre-trained Models Pre-trained models on the pre-
training corpus, after 1 and 2 epochs, for examples
of length 512, and the BERT Large architecture.

Task Datasets Datasets containing training, validation, and
testing examples for each of the 6 tasks. For the clas-
siﬁcation tasks, we provide original source code and
classiﬁcation labels. For the localization and repair
task, we provide subtokenized code, and masks speci-
fying the targets.

Fine-tuned Models Fine-tuned models for the 6 tasks.
Fine-tuning was done on the 1-epoch pre-trained model.
For each classiﬁcation task, we provide the checkpoint
with highest validation accuracy; for the localization
and repair task, we provide the checkpoint with highest
localization and repair accuracy. These are the check-
points we used to evaluate on our test datasets, and to
compute the numbers in the main paper.

Code-encoding Library We provide code for tokenizing
Python code, and for producing inputs to CuBERT’s
pre-training and ﬁne-tuning models.

Localization-and-repair Fine-tuning Model We provide
a library for constructing the localization-and-repair
model, on top of CuBERT’s encoder layers. For the
classiﬁcation tasks, the model is identical to that of
BERT’s classiﬁcation ﬁne-tuning model.

Please see the README for details, ﬁle encoding and
schema, and terms of use.

B. Data Preparation for Fine-Tuning Tasks

B.1. Label Frequencies

All four of our binary-classiﬁcation ﬁne-tuning tasks had
an equal number of buggy and bug-free examples. The

6https://github.com/

google-research-datasets/eth_py150_open

Exception task, which is a multi-class classiﬁcation task,
had a different number of examples per class (i.e., exception
types). For the Exception task, we show the breakdown of
example counts per label for our ﬁne-tuning dataset splits in
Table 6.

B.2. Fine-Tuning Task Datasets

In this section, we describe in detail how we produced our
ﬁne-tuning datasets (Section 3.4 of the main paper).

A common primitive in all our data generation is splitting
a Python module into functions. We do this by parsing
the Python ﬁle and identifying function deﬁnitions in the
Abstract Syntax Tree that have no other function deﬁnition
between themselves and the root of the tree. The resulting
functions include functions deﬁned at module scope, but
also methods of classes and subclasses. Not included are
functions deﬁned within other function and method bodies,
or methods of classes that are, themselves, deﬁned within
other function or method bodies.

We do not ﬁlter functions by length, although task-speciﬁc
data generation may ﬁlter out some functions (see below).
When generating examples for a ﬁxed-length pre-training or
ﬁne-tuning model, we prune all examples to the maximum
target sequence length (in this paper we consider 128, 256,
512, and 1,024 subtokenized sequence lengths). Note that
if a synthetically generated buggy/bug-free example pair
differs only at a location beyond the target length (say on
the 2,000-th subtoken), we still retain both examples. For
instance, for the Variable-Misuse Localization and Repair
task, we retain both buggy and bug-free examples, even if
the error and/or repair locations lie beyond the end of the
maximum target length. During evaluation, if the error or
repair locations fall beyond the length limit of the example,
we count the example as a model failure.

B.2.1. REPRODUCIBLE DATA GENERATION

We make pseudorandom choices at various stages in ﬁne-
tuning data generation. It was important to design a pseu-
dorandomness mechanism that gave (a) reproducible data
generation, (b) non-deterministic choices drawn from the
uniform distribution, and (c) order independence. Order
independence is important because our data generation is
done in a distributed fashion (using Apache Beam), so dif-
ferent pseudorandom number generator state machines are
used by each distributed worker.

Pseudorandomness is computed based on an experiment-
wide seed, but is independent of the order in which exam-
ples are generated. Speciﬁcally, to make a pseudorandom
choice about a function, we hash (using MD5) the seed and
the function data (its source code and metadata about its
provenance), and use the resulting hash as a uniform pseudo-

Learning and Evaluating Contextual Embedding of Source Code

Exception Type

Test

Validation

ASSERTION_ERROR
ATTRIBUTE_ERROR
DOES_NOT_EXIST
HTTP_ERROR
IMPORT_ERROR
INDEX_ERROR
IO_ERROR
KEY_ERROR
KEYBOARD_INTERRUPT
NAME_ERROR
NOT_IMPLEMENTED_ERROR
OBJECT_DOES_NOT_EXIST
OS_ERROR
RUNTIME_ERROR
STOP_ITERATION
SYSTEM_EXIT
TYPE_ERROR
UNICODE_DECODE_ERROR
VALIDATION_ERROR
VALUE_ERROR

155
1,372
7
55
690
586
721
1,926
232
78
119
95
779
107
270
105
809
134
92
2,016

29
274
2
9
170
139
136
362
58
19
24
16
131
34
61
16
156
21
16
415

Train
100% 66% 33%
86
834
2
38
363
346
427
1,112
166
60
72
71
459
80
131
52
531
63
39
1,117

189
1,599
3
78
750
684
881
2,272
336
117
127
142
901
159
284
120
1,038
135
96
2,232

323
2,444
3
104
1,180
1,035
1,318
3,384
509
166
206
197
1,396
247
432
200
1,564
196
159
3,417

Table 6. Example counts per class for the Exception Type task, broken down into the dataset splits. We show separately the 100% train
dataset, as well as its 33% and 66% subsamples used in the ablations.

random value from the function, for whatever needs the data
generator has (e.g., in choosing one of multiple choices). In
that way, the same function will always result in the same
choices given a seed, regardless of the order in which each
function is processed, thereby ensuring reproducible dataset
generation.

To select among multiple choices, we hash the function’s
pseudorandom value along with all choices (sorted in a
canonical order) and use the digest to compute an index
within the list of choices. Note that given two choices
over different candidates but for the same function, inde-
pendent decisions will be drawn. We also use such order-
independent pseudorandomness when subsampling datasets
(e.g., to generate the validation datasets). In those cases, we
hash a sample with the seed, as above, and turn the resulting
digest into a pseudorandom number in [0, 1], which can be
used to decide given a target sampling rate.

B.2.2. VARIABLE-MISUSE CLASSIFICATION

A variable use is any mention of a variable in a load scope.
This includes a variable that appears in the right-hand side of
an assignment, or a ﬁeld dereference. We regard as deﬁned
all variables mentioned either in the formal arguments of a
function deﬁnition, or on the left-hand side of an assignment.
We do not include in our deﬁned variables those declared in
module scope (i.e., globals).

To decide whether to generate examples from a function, we
parse it, and collect all variable-use locations, and all deﬁned
variables, as described above. We discard the function if it
has no variable uses, or if it deﬁnes fewer than two variables;
this is necessary, since if there is only one variable deﬁned,
the model has no choice to make but the default one. We also
discard the function if it has more than 50 deﬁned variables;
such functions are few, and tend to be auto-generated. For
any function that we do not discard, i.e., an eligible function,
we generate a buggy and a bug-free example, as described
next.

To generate a buggy example from an eligible function, we
choose one variable use pseudorandomly (see above how
multiple-choice decisions are done), and replace its current
occupant with a different pseudorandomly-chosen variable
deﬁned in the function (with a separate multiple-choice
decision).

Note that in the work by Vasic et al. (2019), a buggy and
bug-free example pair was generated for every variable use
in an eligible function. In the work by Hellendoorn et al.
(2020), a buggy and bug-free example pair was generated for
up to three variable uses in an eligible function, i.e., some
functions with one use would result in one example pair,
whereas functions with many variable uses would result in
three example pairs. In contrast, our work produces exactly
one example pair for every eligible function. Eligibility was
deﬁned identically in all three projects.

Learning and Evaluating Contextual Embedding of Source Code

Arithmetic
Comparison
Membership
Boolean

Commutative
+, *
==, !=, is, is not

and, or

Non-Commutative
-, /, %
<, <=, >, >=
in, not in

dataset, or functions that have an empty docstring. We
split the rest into the function deﬁnition without the doc-
string, and the docstring summary (i.e., the ﬁrst line of text
from its docstring), discarding the rest of the docstring.

Table 7. Binary operators.

B.2.3. WRONG BINARY OPERATOR

This task considers both commutative and non-commutative
binary operators (unlike the Swapped-Argument Classiﬁca-
tion task). See Table 7 for the full list, and note that we have
excluded relatively infrequent operators, e.g., the Python
integer division operator //.

If a function has no binary operators, it is discarded. Other-
wise, it is used to generate a bug-free example, and a single
buggy example as follows: one of the operators is chosen
pseudorandomly (as described above), and a different oper-
ator chosen to replace it from the same row of Table 7. So,
for instance, a buggy example would only swap == with
is, but not with not in, which would not type-check if
we performed static type inference on Python.

We take appropriate care to ensure the code parses after a
bug is introduced. For instance, if we swap the operator in
the expression 1==2 with is, we ensure that there is space
between the tokens (i.e., 1 is 2 rather than the incorrect
1is2), even though the space was not needed before.

B.2.4. SWAPPED OPERAND

Since this task targets swapping the arguments of binary
operators, we only consider non-commutative operators
from Table 7.

Functions without eligible operators are discarded, and the
choice of the operator to mutate in a function, as well as
the choice of buggy operator to use, are done as above, but
limiting choices only to non-commutative operators.

To avoid complications due to format changes, we only
consider expressions that ﬁt in a single line (in contrast to
the Wrong Binary Operator Classiﬁcation task). We also do
not consider expressions that look the same after swapping
(e.g., a - a).

B.2.5. FUNCTION-DOCSTRING MISMATCH

In Python, a function docstring is a string literal that di-
rectly follows the function signature and precedes the main
function body. Whereas in other common programming
languages, the function documentation is a comment, in
Python it is an actual, semantically meaningful string literal.

We discard functions that have no docstring from this

We create bug-free examples by pairing a function with its
own docstring summary.

To create buggy examples, we pair every function with an-
other function’s docstring summary, according to a global
pseudorandom permutation of all functions: for all i, we
combine the i-th function (without its docstring) with the
Pi-th function’s docstring summary, where P is a pseudoran-
dom permutation, under a given seed. We discard pairings
in which i == P [i], but for the seeds we chose, no such
pathological permuted pairings occurred.

B.2.6. EXCEPTION TYPE

Note that, unlike all other tasks, this task has no notion of
buggy or bug-free examples.

We discard functions that do not have any except clauses
in them.

For the rest, we collect all locations holding exception types
within except clauses, and choose one of those locations
to query the model for classiﬁcation. Note that a single
except clause may hold a comma-separated list of ex-
ception types, and the same type may appear in multiple
locations within a function. Once a location is chosen, we
replace it with a special HOLE token, and create a clas-
siﬁcation example that pairs the function (with the masked
exception location) with the true label (the removed excep-
tion type).

The count of examples per exception type can be found in
Table 6.

B.2.7. VARIABLE MISUSE LOCALIZATION AND REPAIR

The dataset for this task is identical to that for the Variable-
Misuse Classiﬁcation task (Section B.2.2). However, unlike
the classiﬁcation task, examples contain more features rele-
vant to localization and repair. Speciﬁcally, in addition to
the token sequence describing the program, we also extract
a number of boolean input masks:

• A candidates mask, which marks as True all tokens
holding a variable, which can therefore be either the
location of a bug, or the location of a repair. The ﬁrst
position is always a candidate, since it may be used to
indicate a bug-free program.

• A targets mask, which marks as True all tokens holding
the correct variable, for buggy examples. Note that the
correct variable may appear in multiple locations in a
function, therefore this mask may have multiple True

Learning and Evaluating Contextual Embedding of Source Code

positions. Bug-free examples have an all-False targets
mask.

• An error-location mask, which marks as True the loca-
tion where the bug occurs (for buggy examples) or the
ﬁrst location (for bug-free examples).

All the masks mark as True some of the locations that hold
variables. Because many variables are subtokenized into
multiple tokens, if a variable is to be marked as True in the
corresponding mask, we only mark as True its ﬁrst subtoken,
keeping trailing subtokens as False.

C. Attention Visualizations

In this section, we provide sample code snippets used to test
the different classiﬁcation tasks. Further, Figures 1–5 show
visualizations of the attention matrix of the last layer of the
ﬁne-tuned CuBERT model (?) for the code snippets. In the
visualization, the Y-axis shows the query tokens and X-axis
shows the tokens being attended to. The attention weight
between a pair of tokens is the maximum of the weights
assigned by the multi-head attention mechanism. The color
changes from dark to light as weight changes from 0 to 1.

Learning and Evaluating Contextual Embedding of Source Code

def on_resize(self, event):

event.apply_zoom()

Figure 1. Variable Misuse Example. In the code snippet, ‘event.apply zoom’ should actually be ‘self.apply zoom’. The
CuBERT variable-misuse model correctly predicts that the code has an error. As seen from the attention map, the query tokens are
attending to the second occurrence of the ‘event’ token in the snippet, which corresponds to the incorrect variable usage.

Learning and Evaluating Contextual Embedding of Source Code

def__gt__(self,other):

if isinstance(other,int)and other==0:

return self.get_value()>0

return other is not self

Figure 2. Wrong Operator Example. In this code snippet, ‘other is not self’ should actually be ‘other < self’. The
CuBERT wrong-binary-operator model correctly predicts that the code snippet has an error. As seen from the attention map, the query
tokens are all attending to the incorrect operator ‘is’.

Learning and Evaluating Contextual Embedding of Source Code

def__contains__(cls,model):

return cls._registry in model

Figure 3. Swapped Operand Example. In this code snippet, the return statement should be ‘model in cls. registry’. The
swapped-operand model correctly predicts that the code snippet has an error. The query tokens are paying substantial attention to ‘in’
and the second occurrence of ‘model’ in the snippet.

Learning and Evaluating Contextual Embedding of Source Code

Docstring: ’Get form initial data.’
Function:
def__add__(self,cov):

return SumOfKernel(self,cov)

Figure 4. Function Docstring Example. The CuBERT function-docstring model correctly predicts that the docstring is wrong for this code
snippet. Note that most of the query tokens are attending to the tokens in the docstring.

Learning and Evaluating Contextual Embedding of Source Code

try:

subprocess.call(hook_value)
return jsonify(success=True), 200

except __HOLE__ as e:

return jsonify(success=False,

error=str(e)), 400

Figure 5. Exception Classiﬁcation Example. For this code snippet, the CuBERT exception-classiﬁcation model correctly predicts
‘ HOLE ’ as ‘OSError’. The model’s attention matrix also shows that ‘ HOLE ’ is attending to ‘subprocess’, which is indicative
of an OS-related error.

