1
2
0
2

r
p
A
5

]

G
L
.
s
c
[

1
v
4
8
1
2
0
.
4
0
1
2
:
v
i
X
r
a

A ﬂexible and fast PyTorch toolkit for
simulating training and
inference on analog crossbar arrays∗

Malte J. Rasch1, Diego Moreda1, Tayfun Gokmen1, Manuel Le
Gallo1, Fabio Carta1, Cindy Goldberg1, Kaoutar El Maghraoui1,
Abu Sebastian1, and Vijay Narayanan1

1IBM Research

April 7, 2021

Abstract

We introduce the IBM Analog Hardware Acceleration Kit, a
new and ﬁrst of a kind open source toolkit to simulate analog crossbar
arrays in a convenient fashion from within PyTorch (freely available
at https://github.com/IBM/aihwkit). The toolkit is under active de-
velopment and is centered around the concept of an “analog tile” which
captures the computations performed on a crossbar array. Analog tiles
are building blocks that can be used to extend existing network modules
with analog components and compose arbitrary artiﬁcial neural networks
(ANNs) using the ﬂexibility of the PyTorch framework. Analog tiles
can be conveniently conﬁgured to emulate a plethora of diﬀerent analog
hardware characteristics and their non-idealities, such as device-to-device
and cycle-to-cycle variations, resistive device response curves, and weight
and output noise. Additionally, the toolkit makes it possible to design
custom unit cell conﬁgurations and to use advanced analog optimization
algorithms such as Tiki-Taka. Moreover, the backward and update be-
havior can be set to “ideal" to enable hardware-aware training features
for chips that target inference acceleration only. To evaluate the infer-
ence accuracy of such chips over time, we provide statistical programming
noise and drift models calibrated on phase-change memory hardware. Our
new toolkit is fully GPU accelerated and can be used to conveniently esti-
mate the impact of material properties and non-idealities of future analog
technology on the accuracy for arbitrary ANNs.

∗submitted to AICAS 2021

1

 
 
 
 
 
 
1 Introduction

To cope with the ever increasing demand for computing resources in artiﬁcial in-
telligence applications, dedicated hardware accelerators for artiﬁcial neural net-
work (ANN) training and inference have been proposed recently. One promising
future technology is the use of memristive crossbar arrays for accelerating the
ubiquitous matrix-vector multiply and rank-update operations in ANNs by em-
ploying in-memory computation of matrices stored as analog quantities in tun-
able resistive elements [1,7,8,13]. While considerable run-time and performance
gain over today’s technology is projected for analog hardware in principle [5],
noisy and non-linear device characteristics as well as design restrictions such
as stationary weight matrices, demand for a new breed of ANN topologies and
algorithmic advances adapted to analog technology to unleash its full potential.
An ideal outcome of such “analog AI” algorithmic endeavour would be reminis-
cent of the increase in popularity of convolutional networks once it became clear
that their compute is ideally suited to be performed on graphic processing units
(GPUs).

To be viable, these new analog algorithms and optimized network architec-
tures need to be tested in a simulator that takes analog hardware constraints
and realistic material properties into account. Moreover, these simulations need
to be applied on realistically sized deep neural networks (DNNs) and datasets
to make informed decisions about hardware design and the expected accuracy.
While there exists a number of current software packages that are dedi-
cated to simulating AI workloads on non-volatile memory elements, notably
MLP-NeuroSim [2] and RxNN [9], they are usually not tied to modern AI
frameworks, such as PyTorch [11]. Such frameworks have become indispens-
able tools for AI researchers to build new ANN architectures and implement
AI workloads. For instance, while MLP-NeuroSim [2] has the ability to han-
dle a great number of realistic hardware models, it is built around a small (by
today’s standard) MNIST network example, hence lacking the tools to ﬂexi-
bly implement new ANN topologies or simulate larger DNNs. RxNN [9] is
based on the rather outdated Caﬀe framework and only caters to analog chips
dedicated to inference, lacking more advanced algorithms and pulse update
schemes that are needed for training-enabled chip designs. A recent PyTorch
re-implementation of a subset of the MLP-NeuroSim package using PyTorch,
called DNN+NeuroSim [12], is closest to our framework. However, it does not
support fast custom CUDA kernels for in-situ parallel analog update, and also
lacks many of the aspects of modern code development, such as modularity, ob-
ject oriented programming, proper Python binding of fast C++ elements, unit
testing, easy Python package installation and so on. To address these limita-
tions, we decided to open source IBM’s internal analog AI simulation core with
a convenient PyTorch interface, hence providing adequate software tools for
the advancement of algorithmic analog AI.

2

high level API

Python

P
y
T
o
r
c
h

Analog 
tiles

RPU 
Configs

Device 
Configs

Python to C++ bindings

Forward

Backward

Update

RpuCuda tiles

RPU

RPU

RPU

RPU

RPU

RPU

RPU

RPU

RPU

l

o
w

-
l
e
v
e

l

t

e
n
s
o
r

A
P

I

C
U
D
A

b
a
c
k
e
n
d

C++

Figure 1: Repository and code structure of the IBM Analog Hardware Acceler-
ation Kit.

3

 
 
# Define crossbar (RPU) config
rpu_config = SingleRPUConfig(device=ReRamESPresetDevice())
# Define a single-layer network
model = AnalogLinear(4, 2, bias=True, rpu_config=config)
# Define analog-aware optimizer
opt = AnalogSGD(model.parameters(), lr=0.1)
opt.regroup_param_groups(model)
# Run the training
for epoch in range(100):

pred = model(x) # forward pass
loss = mse_loss(pred, y)
loss.backward() # backward pass
opt.step()

# (analog pulsed) update

Figure 2: Simple network deﬁnition and training with parallel pulsed update.
Note that imports and data preparation are omitted for brevity (see example
01 in our GitHub repository).

2 Toolkit overview

We provide a professionally maintained software kit that is easy to install (as
simple as pip install aihwkit), has an extensive documentation1 with a grow-
ing number of examples and tutorials, and has a well integrated PyTorch expe-
rience providing a number of new Analog layers, such as linear (fully-connected)
layer and convolutions (see Fig. 1). Under the hood, these layers perform the
analog hardware simulations using a highly optimized C++/CUDA core library
(RpuCuda) that is part of the package and made available under an Apache-2.0
Open Source license. The core library is bound to the Python layer so that no
re-compilation is needed for deﬁning models and parameters. Instead, all C++
functions and analog hardware deﬁning parameters are conveniently accessible
from Python. Figure 1 illustrates the architecture and code structure of the
toolkit.

Among others, we provide an AnalogLinear layer, which uses a single ana-
log tile to compute the forward pass of a fully-connected layer within a DNN.
Deﬁning a simple analog training simulations (with default device parameters)
is similar to how PyTorch to deﬁne DNN layers (see example in Fig. 2).

3 Analog tile model simulation

The toolkit is centered around an “analog tile” that corresponds to a 2D weight
matrix that is stored on a non-volatile resistive crossbar array and includes
most pre- and post-processing (such as dynamic input scaling and analog-digital
converters) that are based on (and extends) the resistive processing unit (RPU)
deﬁnitions in [5]. The basic operation of this tile is a matrix-vector product
yi = Pj wij xj, where the matrix elements wij are assumed to be stored in

1

https://aihwkit.readthedocs.io/en/latest/

4

the crossbar (optionally together with the biases). The analog matrix-vector
product is, however, corrupted by noise and non-idealities that are deﬁned by
the user through parameter conﬁgurations of the tile during creation (called
rpu_config).

For instance, input/output/weight noise processes, analog-to-digital conver-
sion resolutions, or other non-idealities can be added so that the eﬀective com-
putation simulated is e.g.2 given by

yi = fadc(cid:0) X
j

(wij + σwξij)(fdac(xj ) + σinpξj ) + σoutξi(cid:1)

(1)

where fadc and fdac are discretization functions (together with e.g. dynamic
scaling and clipping) of the digital-analog conversion process (as given by the
hardware design), and ξ are Gaussian noise processes. In our tool, we generally
opt for a more abstract functional representation of the underlying hardware
(i.e. largely working with normalized units), since in this manner the cause of
accuracy loss of diﬀerent hardware designs become more apparent and speciﬁ-
cations can be derived based on parameters relevant to the DNN.

The RPU C++ library is the core of the high-performing implementation of
Eq. (1), which extensively uses dedicated and fused GPU kernels. It provides
the above basic model, however, internal pre- and post processing (e.g. analog
to digital discretization) can be simply turned oﬀ or adapted by the user using
PyTorch’s functionality.

Within a DNN that is considered to be accelerated with analog crossbar
arrays, we generally assume that digital operations can be separated from ana-
log operations. For instance, by default, activation functions are performed
with digital computing using PyTorch’s library of ﬂoating point functions
and assuming that analog result values of a matrix vector product are digi-
tized. Moreover, we currently support only parallel read-out schemes and linear
analog-digital conversion. However, iterative read-out schemes and shift-and-
add for more complicated analog-digital converters could be straightforwardly
added in a similar manner using PyTorch.

While the forward pass includes cycle-to-cycle variations of the resistive ma-
terials and circuit noise sources (see Eq. 1), for chips dedicated to ANN training,
backward and (potentially) update passes need to be similarly simulated to be
performed on analog as well. Thus, in our framework, backward pass (the
transposed version of the forward pass) can be conﬁgured to contain noise and
non-linearity similar to the forward pass. In general, these parameters do not
need to be the same, to reﬂect hardware designs that diﬀer in the noisiness in
both directions.

A particular strength of our new software framework is the highly-optimized
pulse update with tune-able material pulse response properties for chips that
are dedicated to perform ANN training. For that, we consider many algorithmic
advances [3,5] and use stochastic (or deterministic) pulse trains to incrementally

2Consult

the
https://aihwkit.readthedocs.io.

online

documentation

for

a

complete

overview

at

5

perform the theoretical rank update

w(t+1)
ij

= w(t)

ij + λdixj

(2)

where di are the back-propagated error signals. One possible way to implement
the update pass is to use stochastic pulse sequences of variable or ﬁxed length,
where the probability of having a pulse is proportional to the size of xj and di.
In case of pulse coincidences, the weight element at crosspoint ij is updated by
a ﬁnite amount ∆wij (see [5]).

Importantly, due to physical constraints, the amount ∆wij can vary in a
systematic way from crosspoint to crosspoint (device-to-device variations) or
might have a systematic bias in one or the other direction (up versus down
pulses). Moreover, the step ∆wij might depend in a non-linear way on the
actual conductance of the crosspoint value, depending on the physical properties
of the material used as resistive element.

We thus provide a ﬂexible way to choose the correct update response and
ﬁt the update behavior to the material properties under investigation, such as
the amount of device-to-device variations in various parameters and additional
pulse-to-pulse variations. We also provide a number of presets calibrated on
hardware data (e.g.
for ReRAM [6] characteristics, see Fig. 3). Example re-
sponse curves are shown in Fig. 3.

Note that our framework diﬀers signiﬁcantly from the update approach im-
plemented by DNN+NeuroSim. For instance, DNN+NeuroSim uses native
PyTorch operators to compute the accumulated gradient, which in case of
batch size larger than one or generally for convolutions will accumulate p outer
products in digital, and then only discretize the accumulated result to calcu-
late the pulse number and add pulse variations. This means that the outer-
product is actually done in digital and not analog. This will result in a dramatic
under-estimation of the update noise properties for a system that performs the
rank-1 update in place in parallel in analog (as suggested by [5]). Thus, we re-
implemented convolution and linear operators in our C++ core to ensure that
gradient accumulation happens using parallel update in analog memory even
for batch sizes larger than 1 or in case of convolutions.

Moreover, our framework draws the stochastic pulse trains and applies the
update pulses (in case of a coincidence) sequentially on the devices, which are
then updated according to the device model pulse response curve. These pulsed
updates are highly optimized for GPU compute, where we provide hundreds of
custom CUDA kernel templates that are automatically tuned for each tile, thus
selecting the fastest template on the ﬂy. Consequently, the full analog training
simulation often takes only 2-5x longer than the conventional ﬂoating-point
training3.

3Depending on DNN and device setting, e.g. 60s per epoch for VGG-8/CIFAR10 with

parallel pulsed update versus 15s with ﬂoating point on a V100 GPU

6

A

)
e
c
n
a
t
c
u
d
n
o
c

.

m
r
o
n
(

B

t

i

h
g
e
W

C

i

t
h
g
e
W

)
e
c
n
a
t
c
u
d
n
o
c

.

m
r
o
n
(

1

0.8

0.6

0.4

0.2

0
1

0.8

0.6

0.4

0.2

0

ReRAM

Top electrode

Conductive 
filament

Bottom electrode

PCM

Top electrode

Crystalline

Amorphous 

Bottom 
electrode

500 pulses

ReRAM measurement
(Gong et al.)

AIHWKIT model

PCM measurement
(Joshi et al.)

AIHWKIT model

10 2

10 3

Time (s)

10 4

10 5

Figure 3: A) Schematic illustration of ReRAM and PCM devices. B) Pulse
response of the experimental [6] and simulated ReRAM device (with device-to-
device variations, write noise and, cycle-to-cycle variations). C) Experimentally
obtained temporal evolution of PCM conductance [10] compared to that simu-
lated by the statistical noise model of our inference tile. Note that we assume
currently that all weights are programmed at the same time in the simulation,
7
whereas in the experiment devices converge at diﬀerent iterations of program-
ming and there will be jitter in times at which drift starts in individual devices.
This eﬀect leads to the slight diﬀerence noticeable for small times (less than
1000 seconds) between model and experiment.

 
 
 
 
4 Advanced device models and optimizers

All the tile parameters are chosen by setting the resistive device conﬁguration.
The default is having a single device per crosspoint. However, in many hardware
designs, it might be advantageous to have multiple conductive elements within
a “unit cell”. Therefore, we provide a mechanism to deﬁne unit cells having an
arbitrary number of devices (each potentially having diﬀerent material proper-
ties), which can be updated one-by-one or all together. This allows deﬁning
two uni-directional devices per cell, a multi-device cell or a bi-directional single
device cell with reference devices, by conﬁguring the parameters of our analog
tile accordingly.

Moreover, some analog optimization schemes have been proposed, where
each analog tile of SGD is replaced with two analog tiles to create a coupled
system by exchanging information between the two [3] tiles. We provide such
constructs that can be conﬁgured to test diﬀerent and new analog inspired
optimization algorithms (see Fig. 4).

Lastly, a number of temporal noise processes are available, such as con-
ductance decay, reset, and diﬀusion, which are all implemented with systematic
device-to-device variability. These temporal processes are applied once per mini-
batch and are thus assumed to be negligibly small within the physical time a
one mini-batch computation takes.

5 Training for analog inference chips

It has become increasingly apparent that accuracy on chips that are designed
for inference beneﬁt greatly from “hardware-aware” training, where the DNN
workload is trained (or ﬁne-tuned) in software with some analog noise and non-
idealities in the forward pass, but with otherwise perfect backward and update
pass [4,8,10,14]. Our framework supports hardware-aware training for inference.
In particular, backward and update of a tile can simply be conﬁgured as being
“perfect”. Moreover, we provide weight noise functions for inference training,
that reversibly add noise onto weights during a mini-batch (in forward and
backward, but not during update pass) to train for noise resiliency.

Finally, we also provide a speciﬁc inference “tile” setting, which supports
adding carefully calibrated, conductance-dependent programming noise, weight
read noise and conductance drift onto a trained networks’ analog weights during
inference time and provides automatic global drift compensation (see eg. [10]).
The noise strength and temporal proﬁle is calibrated on a 1M phase-change
memory (PCM) device array [10] (see Fig. 3).

Thus, any network can be tested for realistic inference accuracy and how the

accuracy degrades over time.

8

# Define more complicated crossbar (RPU) config
rpu_config = UnitCellRPUConfig(
device=TransferCompound(

# Devices that compose the Tiki-taka compound.
unit_cell_devices=[

ReRamSBPresetDevice(dw_min_dtod=0.1),
ReramSBPresetDevice(dw_min_std=0.2)],
# Some adjustments to how to perform Tiki-Taka
units_in_mbatch=True,
transfer_every=2))

Figure 4: A more complex device conﬁguration. This example implements the
Tiki-taka modiﬁed SGD rule [3] for analog training. Once the rpu_config is
deﬁned, the DNN training is identical to Fig. 2.

6 Extendability and open source contributions

The reason of using PyTorch as an AI framework is to have a system to ﬂexi-
bility deﬁne arbitrary DNN architectures. Our simulator inherits this ﬂexibility
in deﬁning networks with both analog and digital components, where for the
latter all features of PyTorch can be used. Moreover, because of the modular
structure of our code base, it is easy to extend and incorporate new features.
For instance, the inference noise models use Python classes and could be easily
derived to implement other drift characteristics.

Device update behavior, on the other hand, is internally computed in C++.
This core C++ RpuCuda library is designed for speed but retains a modular
structure to make it also extendable. While a number of abstract device mod-
els are already provided, including a number of parameter presets, new device
models can be implemented relatively easily by inheriting most properties of
existing devices. In particular, if only the update response curve of a new ma-
terial needs to be changed, many optimized kernel routines can be re-used, as
only the update function itself needs to be written and added to the code (in
C++ for CPU and GPU).

In general, we follow an open source development model using GitHub stan-
dards, where we encourage contributions and discussions via an issue tracker,
while using modern industry best-practices for software development (continu-
ous integration, automated unit testing, code reviews) and packaging (semantic
versioning, documentation, PEP adherence).

7 Conclusion

We present a new ﬂexible open source software tool for simulating the functional
It is solely focused on
aspects of analog AI with a clear features roadmap.
the algorithmic development and functional veriﬁcation for ANN training and
inference on emerging analog chips. This toolkit is not intended to estimate
run time, latency, or power performance of a hardware chip. While this is

9

very important in its own right, performance aspects are (largely) irrelevant to
the functional characterization and need dedicated and complimentary tools.
We hope that this toolkit will make it easier for researchers from AI hardware
development and deep learning communities, to develop new algorithms and
design new ANN topologies that are optimized for analog AI, hence experiencing
the exciting new area of analog AI.

References

[1] G. W. Burr et al. Neuromorphic computing using non-volatile memory.

Advances in Physics: X, 2(1):89–124, 2017.

[2] P.-Y. Chen et al. NeuroSim+: An integrated device-to-algorithm frame-
work for benchmarking synaptic devices and array architectures. In 2017
IEEE International Electron Devices Meeting (IEDM), pages 6–1, 2017.

[3] T. Gokmen and W. Haensch. Algorithm for training neural networks on

resistive device arrays. Frontiers in Neuroscience, 14, 2020.

[4] T. Gokmen, M. J. Rasch, and W. Haensch. The marriage of training and
inference for scaled deep learning analog hardware. In 2019 IEEE Interna-
tional Electron Devices Meeting (IEDM), pages 22–3. IEEE, 2019.

[5] T. Gokmen and Y. Vlasov. Acceleration of deep neural network train-
ing with resistive cross-point devices: Design considerations. Frontiers in
neuroscience, 10:333, 2016.

[6] N. Gong and other. Signal and noise extraction from analog memory ele-
ments for neuromorphic computing. Nature communications, 9(2102), 2018.

[7] W. Haensch, T. Gokmen, and R. Puri. The next generation of deep learning
hardware: Analog computing. Proceedings of the IEEE, 107(1):108–122,
2018.

[8] S. Jain et al. Neural network accelerator design with resistive crossbars:
Opportunities and challenges. IBM Journal of Research and Development,
63(6):10–1, 2019.

[9] S. Jain, A. Sengupta, K. Roy, and A. Raghunathan. RxNN: A framework for
evaluating deep neural networks on resistive crossbars. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems, 2020.

[10] V. Joshi et al. Accurate deep neural network inference using computational

phase-change memory. Nature Communications, 11(2473), 2020.

[11] A. Paszke et al. Pytorch: An imperative style, high-performance deep

learning library.

10

[12] X. Peng, S. Huang, H. Jiang, A. Lu, and S. Yu. DNN+NeuroSim V2.0: An
end-to-end benchmarking framework for compute-in-memory accelerators
for on-chip training. arXiv:2003.06471, 2020.

[13] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou.
Memory devices and applications for in-memory computing. Nature Nan-
otechnology, 15:529–544, 2020.

[14] H. Tsai et al. Inference of long-short term memory networks at software-
equivalent accuracy using 2.5 M analog phase change memory devices. In
2019 Symposium on VLSI Technology, pages T82–T83. IEEE, 2019.

11

