0
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

2
v
3
1
6
9
0
.
7
0
9
1
:
v
i
X
r
a

Incremental and Decremental Fuzzy Bounded Twin Support Vector
Machine

Alexandre R. Melloa,b,c,, Marcelo R. Stemmerb, Alessandro L. Koericha

a ´Ecole de Technologie Sup´erieure - Universit´e du Qu´ebec, 1100 Notre-Dame West, Montr´eal, QC, H3C 1K3, Canada.
bUniversity of Santa Catarina, Campus Reitor Jo˜ao David Ferreira Lima, Trindade, Florian´opolis, SC, 88040-900,
Brazil.
cSENAI Innovation Institute of Embedded Systems, Avenida Luiz Boiteux Piazza, 574 - Cond. Sapiens Parque -
Canasvieiras - Florian´opolis, SC, 88054-700, Brazil.

Abstract

In this paper, we present an incremental variant of the Twin Support Vector Machine (TWSVM) called
Fuzzy Bounded Twin Support Vector Machine (FBTWSVM) to deal with large datasets and to learn
from data streams. We combine the TWSVM with a fuzzy membership function, so that each input
has a diﬀerent contribution to each hyperplane in a binary classiﬁer. To solve the pair of quadratic
programming problems (QPPs), we use a dual coordinate descent algorithm with a shrinking strategy,
and to obtain a robust classiﬁcation with a fast training we propose the use of a Fourier Gaussian ap-
proximation function with our linear FBTWSVM. Inspired by the shrinking technique, the incremental
algorithm re-utilizes part of the training method with some heuristics, while the decremental procedure
is based on a scoring window. The FBTWSVM is also extended for multi-class problems by combining
binary classiﬁers using a Directed Acyclic Graph (DAG) approach. Moreover, we analyzed the theoret-
ical foundation’s properties of the proposed approach and its extension, and the experimental results
on benchmark datasets indicate that the FBTWSVM has a fast training and retraining process while
maintaining a robust classiﬁcation performance.

Keywords: Twin-SVM, Incremental Learning, Multiclass Twin-SVM, Data Stream, On-line Learning

1. Introduction

Classical machine learning approaches, in which all data is simultaneously accessed, do not meet
the requirements to deal with the scenario in which training data is partially available at a time or
where the amount of data is so large that it does not ﬁt into the memory or the storage of a single
machine. Incremental or online learning is an approach to tackle problems in which only a subset of
the data is considered at each step of the learning process, or when the dataset is too large to be
processed at once [1]. From the computational point of view, incremental learning has three goals: (i)
transform previously learned knowledge to currently received data to facilitate learning from new data;
(ii) accumulate experience over time to support the decision-making process; (iii) and achieve global
Incremental learning often also refers to on-line
generalization through learning to accomplish goals.
learning strategies with limited memory resources, relying on creating a compact memory model that
represents the already observed data but providing accurate results for all relevant settings.

Losing et al. [2] evaluated the most common algorithms of incremental learning on diverse datasets,
and the conclusion is that the Support Vector Machines (SVMs) are usually the highest accurate models.
However, such accuracy is at the expense of the most complex model besides many other shortcomings.

Email addresses: alexandre.reeberg@posgrad.ufsc.br (Alexandre R. Mello), marcelo.stemmer@ufsc.br (Marcelo

R. Stemmer), alessandro.koerich@etsmtl.ca (Alessandro L. Koerich)

URL: https://orcid.org/0000-0003-3130-5328 (Alexandre R. Mello)

Preprint submitted to Information Sciences. Accepted for publication.

March 24, 2020

 
 
 
 
 
 
SVMs are appropriate to tackle two-class classiﬁcation problems by solving a complex Quadratic Pro-
gramming Problem (QPP) that determines a unique global hyperplane in the input space that maximizes
the separation between the classes [3]. However, it requires large memory and a high CPU power since
the computational complexity of the SVM for n data points is O(n3), which makes it impractical for
large datasets. To circumvent this problem, one may use the incremental version of SVM or its variants,
that learns from new data by discarding past data points excepting the support vectors (SVs), i.e., the
new data is used to retrain the model together with the current SVs [1, 4].

The Incremental SVM (ISVM) proposed by Cauwenberghs and Poggio [4] is an exact solution to
the problem of on-line SVM that updates the optimal solution of the SVM by adding or removing one
training data point. The bottleneck of the ISVM is that the computational complexity of a minor
iteration of the algorithm is quadratic in the number of training data points learned so far. Therefore,
the actual runtime depends on the balance between memory access and arithmetic operations in a minor
iteration The LASVM [5] is an on-line kernel classiﬁer that relies on the soft-margin SVM formulation
to handle noisy data. The iterations are similar to the sequential minimization optimization (SMO)
algorithm but with a diﬀerent search strategy. Furthermore, it introduces an SV removal step, where it
removes the vectors collected in the current kernel expansion during the on-line process. The iterations
run in epochs, where each epoch sequentially visits all the randomly shuﬄed training data points, and
the stopping criteria is a pre-deﬁned number of epochs. We can use a multiple number of epochs as a
stochastic optimization algorithm in the oﬀ-line training, and a single epoch in the on-line step. The
computational cost of the LASVM is O(p×nSV ×i), where (nSV ) is the number of SVs, i is the number of
on-line iterations, and p scales no more than linearly to the amount of training data points, which makes
the training process faster than the ISVM. Empirical results suggest that using a single epoch yields to
misclassiﬁcation rates comparable with the SVM. Despite the eﬀectiveness of the ISVM and the LASVM,
both methods still need to deal with one large QPP, requiring large memory storage and CPU processing
time on training and update steps. Mangasarian et al. [6] introduced the Generalized Eigenvalue Proximal
SVM (GEPSVM) that generates two non-parallel hyperplanes for a two-class problem. Thus, it solves
two smaller QPPs instead of a single complex QPP, laying each class data point in the proximity of a
hyperplane, which reduces the complexity compared to the SVM. Jayadeva et al. [7] proposed the Twin
Support Vector Machine (TWSVM), which also solves a pair of QPPs where the data points of one class
provide constraints to the other QPP and vice versa [8, 9]. The TWSVM classiﬁes the data points of
two classes using two non-parallel hyperplanes with a complexity of O(2×(n/2)3), which is four times
lower than an SVM. Twin-based models are mathematically smaller than the SVM, and they require low
memory storage and CPU processing time.

Based on the TWSVM, several variants and solvers have been proposed [8, 10]. Yuan-HaiShao et
al. [11] suggested the Twin Bounded SVM (TBSVM) that includes adherence to the structural risk min-
imization principle, so the dual formulation (whose inverse is guaranteed) can be solved by successive
over-relaxation (SOR) methodology. The Improved TWSVM (ITWSVM) [12] uses a diﬀerent representa-
tion from the TBSVM that leads to a diﬀerent Lagrangian function for the primal problem and diﬀerent
dual formulations. The ITWSVM does not need to compute the inverse of large matrices before training
and can be solved by the SOR or the SMO. However, the matrices in the dual form must involve all the
data points from both classes, which makes the dual QPPs larger than the TWSVM. Khemchandani et
al. [13] proposed a novel fuzzy TWSVM that assigns a fuzzy weight to each data point to mitigate the
eﬀect of outliers and improve accuracy. Gao et al. [14] proposed a coordinate descent fuzzy TWSVM,
assigning a membership function to mitigate the eﬀect of noisy data points, and solving the QPPs with
a coordinate descent with shrinking by active set. Other variants or extensions are the Least Square
TWSVM (LS-TWSVM) [15] that solves the primal problems of the TWSVM, and the ν-TWSVM [16]
where the ν parameter controls the bounds of the fractions of the SVs and the error margin.

Considering the TWSVM and its variations, Khemchandani et al. [1] introduced the incremental
TWSVM (I-TWSVM), which uses the concept of margin vectors and error vectors to select new data
points to update the classiﬁer. It learns from new data by retraining the model while discarding past
data points except for the previous SVs and erroneous classiﬁed data points from the training dataset.
However, for each new data point, both models need to be rebuilt entirely. Hao et al. [17] proposed a
fast incremental TWSVM that uses a distance-based strategy to determine if a new data point is above

2

a pre-deﬁned threshold. It selects the crucial data points that are near the proximal hyperplane from the
current training set and keep data points that are not near the proximal hyperplane from the new training
set. In each iteration, it retrains the model considering the previous SVs and the new data points (there
is no decremental step). The on-line Twin Independent SVM (OTWISVM) [18] uses a modiﬁed Newton
method to build a decision function via a subset of data points seen so far for each class separately (called
basis). The basis vectors are found (or added during the on-line procedure) during iterative minimization
by checking if a new data point is linearly independent in the feature space from the current basis. The
basis size is limited, so it does not grow linearly with the training set. The OTWISVM does not have
a decremental step, and as it utilizes a modiﬁed Newton solver, it needs to calculate the inverse of the
Hessian on every update, making the method unfeasible to deal with high-dimensional datasets. Besides
improving the model with new data, it is also essential to have a decremental procedure to prevent the
model from growing indeﬁnitely. Despite the update strategy be closely related to the model formulation,
there are many alternatives to choosing the SVs to be removed, such as the time-window proposed by
Fung et al. [19], the concept of informative margin vectors and error vectors [4], or decay coeﬃcients
[20].

Finally, to unleash the full potential of the incremental SVM, it is necessary to adapt it to deal with
non-linear problems using the kernel trick. However, conventional kernel approaches struggle to deal
with large datasets due to the storage and computational issues in handling large kernel matrices. A
feasible solution is using kernel approximations such as: exploiting low-rank approximation of the kernel
matrix, reducing the kernel space deﬁnition, or exploiting a randomized kernel space deﬁnition. Random
Fourier approximations (RF) provide an eﬃcient and elegant methodology [21]. The Fourier expansion
generates features based on a ﬁnite set of random basis projections with inner products that are the
kernel Monte Carlo approximations [22]. Fourier features are applicable to translation-invariant kernels,
so they can approximate the Gaussian kernel. Rahimi et al. [21] use RF to map the input data to a
randomized low-dimensional feature space providing convergence bounds to approximate various radial
basis kernel. Le et al. [23] proposed an RF-based approximation called Fastfood, which requires a smaller
computation and memory storage than Random Kitchen Sinks [24] to obtain an explicit function space
expansion.

Although many eﬀorts have been made, the incremental SVM approaches still have several short-
comings, such as the impossibility of endless learning, high model complexity, high training time, high
complexity of hyper-parameter optimization, adaptability to concept drifts, among others. In this pa-
per, we propose a novel incremental and decremental variant of the TWSVM called Fuzzy Bounded
Twin Support Vector Machine (FBTWSVM) that overcomes many of the shortcomings of the current
approaches. The FBTWSVM combines fast training and an incremental procedure (with the ability to
handle noisy data) without weakening the accuracy when updated. The proposed approach can continu-
ously integrate new information into already-built models, with the characteristics of being adherent to
the structural risk minimization principle [11], and using the dual coordinate descent (DCD) algorithm
with active shrinking [12, 13, 14, 25] to create the oﬀ-line classiﬁer. The incremental and decremental
strategies are based on the DCD with shrinking, exploiting the relevance of each support vector. More-
over, we propose the use of our linear formulation with a kernel approximation to speed up training
and classiﬁcation while maintaining the non-linearity. Finally, the FBTWSVM is extended to multiclass
problems using a strategy based on the Directed Acyclic Graph (DAG). The experimental results on
benchmarking datasets have shown that the proposed approach achieves accuracy comparable to the
exact solution besides being faster to integrate new information and to discard outdated information
into the already-built models.

This paper is organized as follows. Section 2 presents the deﬁnitions and notations, introduces
the Twin SVM, and presents the fuzzy SVM with the respective membership function and the kernel
approximation. Section 3 presents the proposed formulation for the FBTWSVM (both linear and non-
linear versions), and the solving method with implementation details. We also extend our formulation to
multiclass problems. In Section 4, we present the incremental and decremental procedures. In Section
5, we present our experimental procedure and the experimental results. We present the conclusions and
perspectives for future work in the last section.

3

2. Basic Concepts

We use the following deﬁnitions and notations throughout the paper. The problems are in a n−dimensional

space Rn. We denote the training data as D=(xi, yi)|i = 1, 2, . . . , l, where xi ∈ Rn is an input data
point, and l is the number of data points, with the corresponding label yi ∈ {1, 2, . . . , u} where u is
the number of classes. We adopt the deﬁnition of incremental learning proposed by Losing et al. [2]
as an algorithm that generates on a given stream of training data x1, x2, . . . , xt a sequence of models
h1, h2, . . . , ht, where (hi:Rn|i = 1, 2, . . . , l) is a model function solely depending on hi−1 and the recent
p data points xi, . . . , xi−p with p being strictly limited. The approach used to deal with multiclass
problems is the DAG, where it is necessary to create 2u−1 binary problems. For each binary problem
we assign either a positive or a negative label yi ∈ {+1, −1}. Therefore, the training set D is divided
into the l+ × n dimensional matrix X+ and l− × n dimensional matrix X− for positive and negative
labels respectively, where l+ and l− denote the number of data points from each label. We deﬁne the
aggregation per binary problem as X=[X (cid:62)
− ], and it denotes all input data points from both classes.

+ X (cid:62)

2.1. The Twin SVM (TWSVM)

The TWSVM [7] generates two non-parallel hyperplanes such that each hyperplane is closer to one
class and is as far as possible from the other class, [8, 26] as shown in Figure 1. The two non-parallel
decision planes are deﬁned as:

ω(cid:62)

+x + b+ = 0 and ω(cid:62)

−x + b− = 0

(1)

where ω+, ω−∈ Rn indicates normal vectors to the hyperplane, and b+, b− ∈ Rn are the bias terms.

Figure 1: Binary classiﬁcation using the TWSVM, inspired by [8].

The following pair of primal optimization problems is the setup to build the decision planes, as the

soft margin hyperplane can handle non-linearly separable data:

min
ω+,b+,ξ−

1
2

||X+ ω+ + e+ b+||2 + C1 e(cid:62)

− ξ−

s.t. y−(X− ω+ + e− b+) + ξ− ≤ e−, ξ− ≥ 0

(2)

4

and

min
ω−,b−,ξ+

1
2

||X− ω− + e− b−||2 + C2 e(cid:62)

+ ξ+

s.t. y+(X+ ω− + e+ b−) + ξ+ ≤ e+, ξ+ ≥ 0

(3)

where C1 > 0 and C2 > 0 are the penalty factors that trade-oﬀ the complexity and data misﬁt between
the minimization of the two terms in the objective function, ξ+ and ξ− denote the slack variable vectors
(the deviation from the margin that allows subsets of misclassiﬁcation error for positive and negative
classes respectively), e−, e− correspond to unit row vectors with their dimensions exact to data point
size in each class used for mathematical purpose only, y+ and y− are +1 and −1 respectively.

In each QPP (Equations 2 and 3) the objective function corresponds to a particular class and the
constraints are set by the data points of the opposite class. The ﬁrst term of both objective functions
aims to minimize the sum of squared distances between the hyperplane and the points of one class, which
tends to keep the hyperplane close to the points of such a class. On the other hand, the second term
of both objective functions aims to minimize the misclassiﬁcation due to points belonging to the other
class. The constraints require the distance between the hyperplane and the points of the other class is
at least 1, and a set of error variables is used to measure the error wherever the hyperplane is closer
than the minimum distance [7]. Assuming that the TWSVM is split into two QPPs of size n/2, and
that the complexity of the original SVM is less or equal to n3, the TWSVM is approximately four times
faster than the original SVM (2 × (n/2)3 = n3/4)[26]. After solving Equations 2 and 3 for (ω∗
+) and
(ω∗

−), respectively, we can classify a new data point x by:

+, b∗

−, b∗

f (x) = argmin

±

|ω∗(cid:62)

± x + b∗
±|
||ω∗
±||

(4)

and choose either +1 or −1 according to the lowest value of Equation 4, i.e., we classify a new data
point x depending on which of the two hyperplanes given by Equation 1 it lies closest. We can write
Equations 2 and 3 as an unconstrained problem using Lagrangian multipliers. The dual formulation of
the linear TWSVM for Equation 2 is:

e(cid:62)
−α −

1
max
2
α
s.t. 0 ≤ α ≤ C1

α(cid:62)H−(H (cid:62)

+ H+)−1H (cid:62)

− α

(5)

where H+=[X+, e+], H−=[X−, e−], ||·|| denotes the L2 norm, and α=(α1, . . . , αm)(cid:62) is the vector of
Lagrangian multipliers. In a similar manner we can write the dual formulation for Eq 3 as:

e(cid:62)
+ ν −

1
max
2
ν
s.t. 0 ≤ ν ≤ C2

ν(cid:62)H+(H (cid:62)

− H−)−1H (cid:62)
+ ν

(6)

where ν=(ν1, ν2, . . . , νm) is the vector of Lagrangian multipliers. For more detail on the dual formulation
one may refer to [7, 26]. Once we solve dual problems for α and ν, we can get the vectors [ω+, b+](cid:62) and
[ω−, b−](cid:62). Thus, the separating hyperplanes are given by:

During testing, a new data point is assigned to the closest hyperplane regarding the two classes by:

x(cid:62)ω+ + b+ = 0 and x(cid:62)ω− + b− = 0

(7)

where

class(x) = argmin
i={−1,+1}

di(x)

di(x) =

|x(cid:62)ωi + bi|
||ωi||

5

(8)

(9)

2.2. The Fuzzy SVM

The presence of outliers in the training dataset may aﬀect both the standard SVM and the TWSVM.
The Fuzzy SVM introduced by Lin et al. [27] uses the fuzzy theory to reduce the eﬀect of outliers by
applying a membership to each data point. Fuzzy numbers, denoted as si, are assigned to each input
data point to add information that reﬂects the noise contamination level, which is 0 ≤ si ≤ 1, i=1, 2, ..., l.
Therefore, the training dataset D becomes a triple D(cid:48)=(xi, yi, si) to accommodate the fuzzy number
and to reduce the inﬂuence of the contaminated data points in generating the decision functions. The
fuzzy SVM is formulated as:

1
2

||ω||2 + Cs(cid:62)ξ

min
ω,b,ξ
s.t. yi(ω(cid:62)xi + b) + ξi ≥ 1
ξi ≤ 0, i = 1, 2, ..., l

(10)

where C is the trade-oﬀ scalar and ξi is the slack variable that represents the error associated with the
i-th input data point. An important remark about this formulation is that a small si can reduce the
eﬀect of the slack variable ξi in Equation 10, so reducing the importance of the corresponding data point
xi. The classiﬁcation of an input x is given by the sign of ω∗(cid:62)x + b∗, where ω∗ and b∗ are the solution
of Equation 10.

The construction of the membership functions follows the strategy used by Gao et al. [14, 25]. The
method considers reducing the noise carried by outliers while keeping the importance of the SVs. We
integrate the fuzzy SVM into the TWSVM formulation by selecting two diﬀerent classes and assigning
a positive label to the ﬁrst class and a negative label to the second one. The class centers xc+ and xc−
are the mean points considering the input space of these two classes, deﬁned by:

xc+ =

1
l+

(cid:88)

yi=+1

xi, xc− =

1
l−

(cid:88)

xi

yi=−1

(11)

The hyperspheres radii r+ and r− are constructed by measuring the distance of the farthest scattering

data point of each class:

r+ = max

i

r− = max

i

||xi − xc+||

if yi = +1

||xi − xc−||

if yi = −1

(12)

The membership of si is assigned according to the distance relationship between ||xi − xc+|| and

||xi − xc−||, when xc+, xc−, r+, and r− are known. Formally, si of a positive data point is given as:

(cid:40)

si =

µ(cid:0)1 − ||xi − xc+||/(r+ + δ)(cid:1)
if
(1 − µ)(cid:0)1 − ||xi − xc+||/(r+ + δ)(cid:1) if

||xi − xc+|| ≥ ||xi − xc−|| ∧ yi = +1
||xi − xc+|| < ||xi − xc−|| ∧ yi = +1

(13)

where µ ∈ [0, 1] balances the eﬀect of normal and noisy data points, and δ > 0 avoids fuzzy numbers
equal 0. Figure 2 illustrates the fuzzy-related elements used to assign the fuzzy membership value.

A data point is usually assigned by a proportional decreasing value si when it drifts farther from its
native class center, which increases the uncertainty [25]. A small positive real number µ is assigned to
decrease the eﬀect of outliers towards the hyperplane. The fuzzy numbers for the negative data points
are calculated analogously.

2.3. Kernel Approximation

Kernel machines that operate on the data kernel matrix (Gram matrix) scale more than quadratically
in the data dimension [21, 22], which makes methods such as the ISVM or the LASVM impractical to
deal with large datasets or incremental data that require sequential learning. Approximating non-linear
kernels by linear kernels in the transformed space is a way to make possible the use of eﬃcient linear
methods that depend linearly on the size of the training set, allowing to solve large-scale and incremental
learning problems eﬃciently [21, 22]. Instead of relying on the kernel trick implicit lifting, the Random

6

Figure 2: Graphical representation of xc+, xc−, r+, r− and the distance between ||xi − xc+|| and ||xi − xc−|| given a
positive instance xi.

Fourier Features [21] explicitly map the data to a low-dimensional Euclidean inner product using a
randomized feature map z : Rn→RN , described as:

κ(x1, x2) = (cid:104)ϕ(x1), ϕ(x2)(cid:105) ≈ z(x1)(cid:62)z(x2)

(14)

where z is a low-dimensional space. The feature space approximates shift-invariant kernels κ(x1 − x2)
to within an error err with N =O(err−2n log 1
err2 ) dimensions. Rahimi and Recth [21] show empirically
that a similar classiﬁcation performance can be obtained for dimensions smaller than N .

The ﬁrst set of transformed features are the Random Fourier bases cos(τ (cid:62)x + b), where τ ∈Rn and
It maps projected data on a randomly chosen line, followed by
b∈R, which are random variables.
passing the resulting scalar through a sinusoidal function. The direction of these lines, in an appropriate
distribution, guarantees that the product of two transformed points approximates a desired shift-invariant
kernel [21]. The transformation follows Bochner’s theorem: A continuous kernel κ(x, y) = κ(x − y) on
Rn is positive deﬁnite if and only if κ(δ) is the Fourier transform of a non-negative measure. For a
properly scaled shift-invariant kernel κ(δ), Bochner’s theorem guarantees that its Fourier transform p(τ )
is a proper probability distribution:

κ(x − y) =

(cid:90)

Rn

p(τ )ejτ (cid:62)(x−y)dτ = Eτ [ζτ (x)ζτ (y)∗]

(15)

where ζτ (x) = ejτ (cid:62)x. ζτ (x)ζτ (y)∗ is an unbiased estimate of k(x, y) when τ is drawn for p, and ∗
denotes the complex conjugate. The integral of Equation 15 converges when the complex exponential are
2cos(τ (cid:62)x + b), obtaining a real-valued mapping that satisﬁes the condition
replaced by cosines, zτ (x)=
E[zτ (x)zτ (y)], where τ is drawn from p(τ ) and b is uniformly distributed from [0, 2π]. The estimate kernel
variance can be reduced by concatenating N randomly chosen zτ into one N -dimensional normalized
vector, i.e., the inner product z(x)(cid:62)z(y)= 1
j=1 zτ (x)zτ (y) is a low variance approximation to the
N

(cid:80)N

√

7

expectation of Equation 151. To summarize, the random Fourier feature algorithm starts by getting a
randomized feature map z(x):Rn→RN , so that z(x)(cid:62)z(y) ≈ k(x − y). The second step is to compute p
of the kernel, which in this case is the Fourier transform of k:

p(τ ) =

(cid:90)

1
2π

ejτ (cid:62)δk(δ)d∆

(16)

The third step is to draw N independent and identically distributed (iid) data points τ1, ..., τN ∈ Rn
from p and N iid data points b1, ..., bN ∈ R from the uniform distribution on [0, 2π]. Finally, z(x) is
computed as:

z(x) ≡

(cid:114) 2
N

[cos(τ (cid:62)

1 x + b1), . . . , cos(τ (cid:62)

N x + bN )](cid:62)

(17)

The scalar σ2

at the origin. For a Gaussian kernel denoted as k(x1, x2)=exp(−γ||x1 − x2||2), we have σ2
approximates the kernel to:

p is equal to the trace of the Hessian of k at 0, that quantiﬁes the curvature of the kernel
p=2nγ, that

p(τ ) = 2π− N

2 exp(−

||τ ||2
2
2

)

(18)

The important implications of using this kernel approximation in our incremental approach are: (i)
we approximate the non-linear model accuracy with a linear model; (ii) it is faster to calculate the
approximate kernel than the regular one; (iii) and mainly, we increase the model only in one dimension,
so we do not need to recalculate the kernel approximation of the previous data.

3. The Fuzzy Bounded Twin SVM (FBTWSVM)

We propose a formulation based on the TWSVM [7] and inspired by the FRTSVM [14, 25] and
TBSVM [11] properties. The fuzzy formulation incorporated by our method (Equation 10) is inspired
by the TWSVM (Equations 2 and 3), while the adherence to the structural risk minimization principle
is incorporated similarly to the TBSVM [11]. To maintain such an adherence, we need to guarantee
the existence of the dual formulation inverse matrix, which circumvents the drawback of the standard
TWSVM (i.e., the standard TWSVM only adheres to the empirical risk minimization problem in the
dual problem). Thus, we deﬁne the FBTWSVM primal formulation as:

min
ω+,b+,ξ−

1
2

C1(||ω+||2 + b2

+) +

1
2

||X+ ω+ + e+ b+||2 + C3s(cid:62)

− ξ−

s.t. y−(X− ω+ + e− b+) + ξ− ≥ e−, ξ− ≥ 0

min
ω−,b−,ξ+

1
2

C2(||ω−||2 + b2

−) +

1
2

||X− ω− + e− b−||2 + C4s(cid:62)

+ ξ+

s.t. y+(X+ ω− + e+ b−) + ξ+ ≥ e+, ξ+ ≥ 0

(19)

(20)

where C1, C2, C3, and C4 are the trade-oﬀ parameters between the margin and the complexity for
weighting the regularization, s+∈Rl+ and s−∈Rl− are the fuzzy number vectors sequentially associated
with the positive and negative input data points, which introduce the desired robustness in the weighted
regularized model [14, 25]. The additional b+ and b− in Equations 19 and 20 minimize the structural
risk.

The two hyperplanes in Rn are deﬁned as ω(cid:62)

± + b±=0, and since the TWSVM has two proximal
decision functions, two margin terms 1/||ω±|| are deﬁned for the proximal decision function [14]. The
margin between two classes can be measured by the distance between the proximal hyperplane x(cid:62)ω+ +
b+=0 and the bounding hyperplane x(cid:62)ω+ + b+=−1. The distance is 1/||ω+||2, and it is the one-sided

1The proof can be found in [21].

8

margin between the two classes with respect to the hyperplane x(cid:62)ω+ + b+=0 [11, 26]. The process
is analogous to the other hyperplane. We need to write the dual problems obtaining the solutions of
Equations 19 and 20. We start by taking the Lagrangian of Equation 19 to obtain the Wolfe dual:

L(ω+, b+, ξ−) =

C1(||ω+||2 + b2

+) +
− α(cid:62)(−(X− ω+ + e− b+) + ξ− − e−) + C3 s(cid:62)

||X+ω+ + e+b+||2

1
2

1
2

− ξ− − η(cid:62)ξ−

(21)

where α=(α1, . . . , αX+)(cid:62), and η=(η1, . . . , ηX+)(cid:62) are the Lagrange multiplier vectors. Considering that
Equation 19 represents a convex optimization problem, the Karush-Kuhn-Tucker (KKT) optimality
conditions are both necessary and suﬃcient, and they are written as:

− α = 0

+(X+ ω+ + e+ b+) + e(cid:62)

+ (X+ ω+ + e+ b+) + X (cid:62)

∇ω+ L = C1 ω+ + X (cid:62)
∇b+ L = C1 b+ + e(cid:62)
∇ξ− L = −α(cid:62) − η(cid:62) + C3 s− = 0
− (X− ω+ + e− b+) + ξ− ≥ e−ξ− ≥ 0
α(cid:62)(ω− X+ + e− b+ − ξ− + e−) = 0; η(cid:62)ξ− = 0
α ≥ 0, η ≥ 0, ξ− ≥ 0

− α = 0

(22a)

(22b)

(22c)

(22d)

(22e)

(22f)

Considering that η ≥ 0 and α ≥ 0 from Equation 22f, and using Equation 22c, we know that α
is bounded as 0 ≤ α ≤ C3s−. Summing Equations 22a and 22b, and using Equations 22c to 22f for
simpliﬁcation, we obtain:

([X+, e+](cid:62)[X+, e+] + C1I)[ω+, b+] + [X−, e−](cid:62)α = 0

(23)

Deﬁning H+=[X+, e+], H−=[X−, e−], u+=[ω+, b+] and u−=[ω−, b−] (one to each class), we can

rewrite Equation 23 as:

(H (cid:62)
u(cid:62)

+ H+ + C1I)u(cid:62)
+ = −(H (cid:62)

+ + H (cid:62)
+ H+ + C1I)−1H (cid:62)

− α = 0
− α

Using our notation, the Wolfe dual is deﬁned as:

max L(ω+, b+, ξ−, α, η)
s.t ∇ω+L(ω+, b+, ξ−, α, η)

= 0

∂L
∂b+
∂L
∂ξ−
α ≥ 0, η ≥ 0

= 0

or

(24)

(25)

Using the KKT conditions (from Equations 22a to 22f) and Equation 24, the Wolfe dual of Equa-

tions 19 and 20 can be written as:

e(cid:62)
− α −

max
α
s.t. 0 ≤ α ≤ C3s−

e(cid:62)
+ ν −

max
ν
s.t. 0 ≤ ν ≤ C4s+

1
2

1
2

α(cid:62)H−(H (cid:62)

+ H+ + C1I1)−1H (cid:62)

− α

ν(cid:62)H+(H (cid:62)

− H− + C2I2)−1H (cid:62)

+ ν

(26)

(27)

where I1 and I2 are identity matrices. The matrices (H (cid:62)

+ H+ + C1I1) and (H (cid:62)

− H− + C2I2) from Equa-

9

tions 26 and 27 are non-singular naturally, therefore their inverses are guaranteed to exist, which adds the
adherence to the structural risk minimization principle [11, 26]. Notice that the dual for Equation 20 can
be obtained is an analogous way. By solving the duals (Equations 26 and 27), we obtain the optimal solu-
tions for α∗ and ν∗, and furthermore, the corresponding classes u∗
± (as deﬁned in Equation 24) and the
non-parallel hyperplanes. The dual of Equation 26 and 27 relates to the primal problems (Equations 19
and 20) as:

u∗

+ = −(H (cid:62)
− = (H (cid:62)
u∗

+ H+ + C1I1)−1H (cid:62)
− H− + C2I2)−1H (cid:62)

− α∗
+ ν∗

(28)

Finally, for a test data point x ∈ Rn, the classiﬁcation decision function is given by Equation 4.

3.1. The Non-linear FBTWSVM

In the non-linear FBTWSVM, the input data points x ∈ Rn are mapped to a high-dimensional space
H through ϕ(x). The kernel function κ(·, ·) calculates implicitly the dot product of a pair of trans-
formations, which is applied as κ(x1, x2) = (cid:104)ϕ(x1), ϕ(x2)(cid:105). The non-linear dual proximal hyperplanes
are:

and the primal problems used to obtain the dual proximal hyperplanes are:

κ(x, x(cid:62))ω+ + b+ = 0
κ(x, x(cid:62))ω− + b− = 0

min
ω+,b+,ξ−

1
2

C1(||ω+||2 + b2

+) + C3 s(cid:62)

− ξ− +

1
2

||κ(X+, X (cid:62))ω+ + e+ b+||2

s.t. y−(κ(X−, X (cid:62))ω+ + e− b+) + ξ− ≥ e−, ξ− ≥ 0

min
ω−,b−,ξ+

1
2

C2(||ω−||2 + b2

−) + C4 s(cid:62)

+ ξ+ +

1
2

||κ(X+, X (cid:62))ω− + e− b−||2

s.t. y+(κ(X+, X (cid:62))ω− + e+ b−) + ξ+ ≥ e+, ξ+ ≥ 0

The dual forms of Equation 30 and 31 are:

e(cid:62)
− α −

max
α
s.t. 0 ≤ α ≤ C3s−

e(cid:62)
+ ν −

max
ν
s.t. 0 ≤ ν ≤ C4s+

1
2

1
2

α(cid:62)S−(S(cid:62)

+ S+ + C1I1)−1S(cid:62)

− α

ν(cid:62)S+(S(cid:62)

− S− + C2I2)−1S(cid:62)

+ ν

where S+ = [κ(X+, X (cid:62)), e+] and S− = [κ(X−, X (cid:62)), e−]. The solutions of the primal problems of
Equations 30 and 31 are υ∗
±](cid:62), which are the parametric relationships between the optimal
± and the optimal solutions α∗ and ν∗ of the dual forms of Equations 32 and 33:
υ∗

± = [ω∗(cid:62)

± , b∗

υ∗
+ = −(S(cid:62)
− = (S(cid:62)
υ∗

+ S+ + C1I1)−1S(cid:62)
+ ν∗

− α∗

− S− + C2I2)−1S(cid:62)
Once Equations 32 and 33 are solved to obtain the hyperplanes (Equation 29), a new data point

x ∈ Rn can be classiﬁed in a similar manner to the linear case by Equation 4.

3.2. Solving The FBTWSVM

We use the coordinate descent method (DCD) [28] to solve the dual problem of the FBTWSVM [29].
The DCD leads to fast training by updating one variable at a time through a single-variable sub-problem

10

(29)

(30)

(31)

(32)

(33)

(34)

minimization. Such fast training allows the processing of large and incremental datasets [29]. The dual
problems of Equations 26 and 27 and Equations 32 and 33 are solved in the same way. However, for conve-
nience, we only present the solution of Equation 26. We start by considering Q=H−(H (cid:62)
+ H++C1I1)−1H (cid:62)
−
and Q(cid:48)=(H (cid:62)
− . Consequently, Q=H−Q(cid:48), where qii and Q can be pre-computed and
stored if necessary. The matrix inversion is calculated with the Sherman-Morison-Woodbury formula.
Assuming αk,i=[αk+1,i
X−+1], where i=(1, . . . , X−+1) is the index for the data
points and k = (−1, +1) is the data label. We use the following problem updating from αk,i to αk,i+1:

+ H+ + C1I1)−1H (cid:62)

, . . . , αk+1,i

i−1 , αk,i

, . . . , αk,1

1

i

min
d
s.t. 0 ≤ αk

f (αk,i + d ei)

i + d ≤ C3s−

(35)

where ei=[0, . . . , 0, 1, 0, . . . , 0](cid:62) (the i−th position is 1), and di is an optimum solution to the problem
of minimizing f (αk,i + dei) subject to di ∈ Rn, i.e., f (αk,i + dei) achieves a minimum at di only
if ∇f (αk,i + dei)(cid:62)ei = ∇f (αk,i+1)(cid:62)ei = 02. The objective function of Equation 35 is a quadratic
function of d:

f (αk,i + dei) =

1
2

Qiid2 + ∇if (αk,i)d + constant

where ∇if is the i-th component of the gradient ∇f . Equation 35 has an optimum at d=0 iﬀ:

i f (αk,i) = 0
i f (α) is the projected gradient which is deﬁned as:

∇P

where ∇P

∇P

i f (α) =






min(0, ∇if (α)), αi = 0,
∇if (α),
max(0, ∇if (α)), αi = C3s−

0 ≤ αi ≤ C3s−

(36)

(37)

(38)

If Equation 37 is satisﬁed, we can move to the next iteration (i+1) without updating αk,i

in X−,
to temporally meet the optimal solution of Equation 35. The optimum of

i.e., we only update αk,i
Equation 36 is reached by introducing the Lipschitz continuity:

i

i

αk,i+1
i

= min(max(αk,i

i − ∇if (αk,i)/Q(cid:48)

ii, 0), C3si− )

(39)

In the update of Equation 39, Q(cid:48)

i,i can be pre-calculated by Q(cid:48)

ii = H−iQi, and ∇if (αk,i) can be

obtained by:

∇if (α) = (Q(cid:48)α)i − 1 =

X−
(cid:88)

j=1

Q(cid:48)

ijαj − 1

(40)

The computation of Equation 40 is approximated as O(X−l), where l is the average count of non-
zero elements in Q(cid:48) per data point. To reduce the number of operations, we can alternatively compute
Equation 40 as:

with a pre-deﬁned u+=−Qα and i is the row of the matrix H−, so the number of operations is O(n).
To maintain u+ throughout the coordinate descent procedure, we use:

∇if (α) = −H−iu+ − 1

(41)

u+i ← u+i − Qi(αi − αi)

(42)

2The proof can be found in [28]

11

The complexity to maintain u+ iteratively is O(l). Starting with α0 = 0, the optimal solution of u+
is obtained by iterative updating Equation 42, and furthermore, the optimal solution of Equation 26.
The cost per iteration for the whole process is O(X2n), and the memory requirement is the size of H−
and Q(cid:48).

3.3. Implementation

The dual problem of Equation 26 has the constraint 0 ≤ αi ≤ C3s−, and if αi is either 0 or C3s−,
it may achieve a steady state. Considering that our formulation produces many bounded Lagrange
multipliers, we apply the proposed shrinking technique to reduce the size of the optimization problem
without considering some bounded variables [30]. Considering Z as a subset of X after removing all data
points that have non-bounded Lagrange multipliers, and Z={1, . . . , X−}/Z its complement subset, the
dual of Equation 26 can be represented by a smaller problem that consumes less time and memory:

min
αZ

1
2

α(cid:62)

Z Q(cid:48)

ZZαZ + (Q(cid:48)

ZZαZ − eZ)(cid:62)αZ

(43)

where QZZ and QZZ are sub-matrices of Z and αZ is a vector of Lagrangian multipliers. To solve
Equation 43, we compute ∇if (α) as:

s.t. 0 ≤ αZ ≤ C3s−Z

∇if (α) = Qi,ZαZ + Qi,ZαZ − 1

(44)

If i ∈ Z, and deﬁning u1 as:

u1 = −(Q(cid:48)

i∈Zαi∈Z + Q(cid:48)
we have ∇if (α)=H−u1−1, which turns ∇if (α) easy to obtain. For a linear kernel we only need to update
(Qi∈Zαi∈Z), and we do not need to reconstruct all ∇f (α) to implement the shrink step3. Considering
the projected gradient ∇P f (α) deﬁned in Equation 38, and following the optimality condition of bound-
constrained problems, α is optimal iﬀ ∇P f (α) = 0. During the iteration procedure, the inequality
∇P f (α) (cid:54)= 0 means either maxj ∇P f (α) > 0 or minj ∇P f (α) < 0, and at the k−1 step, we obtain
mk−1
min ≡ minj ∇P f (α). In this way, at each inner step of the k−th iteration,
and before updating α, the element is shrunken if one of the two conditions holds:

max ≡ maxj ∇P f (α) and mk−1

i∈Zαi∈Z)

(45)

αk,i

i = 0

and ∇P f (αk,i) > m(cid:48)k−1
max

or

i = C3s− and ∇P f (αk,i) < m(cid:48)k−1
αk,i

min

(46)

where m(cid:48)k−1

max must be strictly positive and m(cid:48)k−1

min must be strictly negative, and they are deﬁned as:

(cid:40)

m(cid:48)k−1

max =

mk−1
max,
∞,

if mk−1

max > 0,
otherwise

m(cid:48)k−1

min =

(cid:40)

mk−1
max,
−∞,

if mk−1

min < 0,
otherwise

(47)

(48)

Next, we multiply both m(cid:48)k

min−1 by a shrinking rate smaller than one. A tolerance (cid:15)
indicates if the optimal value is satisﬁed after a ﬁnite number of iterations, thus it is used as a valid stop
criterion:

max−1 and m(cid:48)k

m(cid:48)k

max − m(cid:48)k

min < (cid:15)

(49)

If in the k−th iteration, the condition stated in Equation 49 is satisﬁed for Equation 43, we can
min = −∞, and continue with

enlarge the active set Z to {1, . . . , X−+1}, and set m(cid:48)k

max = +∞ and m(cid:48)k

3The proof can be found in [31].

12

the regular iterations. We store the previous values of m(cid:48)
min during the DCD process to
avoid recalculating them during the incremental step. Therefore, the shrinking technique is a key step
to avoid calculating and storing all training data during the training phase. Our method process one
class at each time, however, the inner processing can be done in parallel, where one input is assigned to
an available processor to calculate the membership followed by the Lagrangian multiplier. We present
the pseudo-code of the FBTWSVM training algorithm (Algorithm 1) for the positive class.

max and m(cid:48)

3.4. The Multiclass FBTWSVM

The FBTWSVM is based on the TWSVM foundations, which considers only binary problems. Yet,
we can extend the FBTWSVM to multiple classes by building and combining several binary classiﬁers
instead of considering all data in one optimization formula [32]. The multiclass FBTWSVM is based
on the Decision Directed Acyclic Graph (DDAG), which achieves better accuracy while requiring less
training time than other multiclass approaches [8, 33]. The DAG-based multiclass classiﬁer was originally
proposed by Platt et al. [33] for the multiclass SVM approach, and further introduced by Chen and Ji
[34] into the twin approach as the Optimal DAG to the Least Squares Twin SVM.

In the multiclass approach based on the DAG topology, for a u−class classiﬁcation problem, there
are u(u−1)/2 sub-classiﬁer nodes divided into u−1 layers. During the classiﬁcation process, there is no
need for combining all sub-classiﬁers, so to assign a class to a test data point, it makes u−1 decisions.
The classiﬁcation process starts at the root node, located in the ﬁrst layer, and includes all possible
classiﬁcation labels (node 1v4 in Figure 3). The decision-making step eliminates the most excluded
category at each sub-classiﬁer decision, i.e., considering a 4-class problem with a test data point with
label yi=4 and the topology presented in Figure 3. The root node sub-classiﬁer eliminates the possibility
of yi=1, following the Not 1 line. The next sub-classiﬁer eliminates the possibility of yi=2 following the
Not 2, and the last sub-classiﬁer eliminates the possibility of yi=3, assigning class 4 to the test data
point.

Figure 3: A 4-class classiﬁcation problem based one the DAG topology.

4. Incremental and Decremental FBTWSVM

The FBTWSVM can integrate new data points continuously into the existing model without fully
reconstructing it. Besides that, it can be trained fast due to the formulation and the solver choice, and it
generalizes well like a conventional SVM. These characteristics make it suitable for incremental learning
applications.

The incremental FBTWSVM is based on the shrinking heuristic that can increase the current model
considering the fuzzy information of new values. We update the model by selecting only new data points

13

1v41v31v22v42v33v41234Not 4 Not 31234Not 1 Not 4 Not 2 12312234 233 4 Layer 1Layer 2 Layer 3 (cid:46) this thread runs in parallel

ii = H−iQi

Let mmax = −∞, mmin = ∞
for ∀i ∈ Z, (a randomly and exclusively selected) do

min = −∞

− and Q(cid:48)

i f (α) = ∇if (α)

min then X = X/{i}

i f (α) > m(cid:48)

max then X = X/{i}

+ H+ + C1I)−1H (cid:62)

i f (α) > 0 then ∇P

i f (α) < 0 then ∇P

if ∇P
end if
if ∇P
end if

∇i f (α) = −H−iu+ − 1
∇P
i f (α) = 0
if αi = 0 then
if ∇P
end if
if ∇P
end if
if αi = C3s− then
i f (α) < m(cid:48)

Algorithm 1 FBTWSVM’s training procedure
Input: X, y, C1,C2,C3,C4
Output: u+
1: Compute Q = (H (cid:62)
2: Let Z = {1, . . . , X−}
3: Given (cid:15), α = 0 and u+ = 0
max = ∞ and m(cid:48)
4: m(cid:48)
5: while do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
end if
36:
if mmax ≤ 0 then m(cid:48)
37:
else m(cid:48)
max = mmax
38:
end if
39:
if mmax ≥ 0 then m(cid:48)
40:
else m(cid:48)
41:
42:
end if
43: end while
44: return u+

else∇P
end if
mmax = max(mmax, ∇P
mmin = min(mmin, ∇P
if ∇P

i f (α) (cid:54)= 0 then
α = αi
αi = min(max(αi − ∇if (α)/Q(cid:48)
u+i = u+i − Qi(αi − αi)

if X = {1, . . . , l−}, then break
end if

end for
if mmax − mmin < (cid:15) then

X = {1, . . . , l−}, m(cid:48)

i f (α))
i f (α))

i f (α) = ∇if (α)

max = ∞, m(cid:48)

min = mmin

min = −∞

min = −∞

max = ∞

end if

end if

else

ii, 0)C3si−)

i f (α) = ∇if (α)

min) and maximum (m(cid:48)

that extrapolate the minimum (m(cid:48)
max) values of the projected gradient from
the previous training step. Therefore, we do not need to process all the new incoming data points.
Considering a set of new data points as Xnew, and the subsets X (cid:48)
new- denoting the positive
and negative labeled data respectively. Not necessarily both subsets may exist, and here we consider
that Xnew = X (cid:48)
new+ to maintain the notation. We evaluate the projected gradient of the new set of data
points as:

new+ and X (cid:48)

∇if (αnew) = X (cid:48)

new+ ± u+ − 1

(50)

This operation keeps u+ in the coordinate descent procedure of Equation 41. We set a new heuristic rule
based on Equation 46 to select only new data points that are more likely to become SVs. We consider
the new data points as SVs if the projected gradient values are bounded by m(cid:48)
max.
As our method adheres structural and risk minimization principle, all Lagrangian multipliers can be
interpreted as SVs, and to let the evaluation of Equation 50 be more permissive, we can replace the

min<∇if (αnew)<m(cid:48)

14

Figure 4: Blue circles and red crosses represent classes respectively, and the green circles and crosses represent new data
points from each class.

max and min operators by the median, mean, or superior and inferior quartiles. Figure 4 depicts four
new data points (in green and numbered), two of each class. The new data points must have projected
gradient out of bounds from the respective model to be considered in the incremental procedure. For
instance, the circle 1 has a projected gradient lower than m(cid:48)
min of class +1 model, and the circle 2 has a
projected gradient greater than m(cid:48)
max of class +1 model. The cross 3 has a projected gradient greater
max of class -1, so it is not discarded, but the cross 4 is bounded by the m(cid:48)
than m(cid:48)
min of
class -1 model, so it is discarded. New data points that have projected gradient lower than m(cid:48)
min should
interfere in the model shape and placement regarding only its own class, while new data points that
have projected gradient greater than m(cid:48)
max interfere in the hyperplane placement regarding the opposite
class. We calculate the membership (as presented in Section 2.2) to each new data point from Xnew
that extrapolates the projected gradient bounds, where Xover is the data matrix that extrapolates the
bounds. Then, we start a new training iteration k→k+1 to update the model by enlarging the active
set with Xover. Algorithm 2 presents the pseudo-code for the incremental procedure.

min and m(cid:48)

(cid:46) We check previous mmax and mmin’s

for ∀i ∈ Znew, (a randomly and exclusively selected in the case of batch) do

(cid:46) this thread runs in parallel

Algorithm 2 Incremental procedure
Input: Xnew, ynew, C1,C2,C3,C4, and the previous model
Output: updated model
1: Let Znew- = {1, . . . , X−}
2: Given (cid:15), αnew = 0 and u+new = 0
3: ∇if (αnew) = −H−iu+ − 1
4: if ∇if (αnew) > max mmax or ∇if (αnew) < min mmin then
5:
6:
7: end if
8: while do
9:
10:
11:
end for
12: end while
13: return updated model ← u+, α, mmax, mmin

Compute the fuzzy membership s
Compute Q = (H (cid:62)

Run algorithm 1 from line 8 to 31

+ H+ + C1I)−1H (cid:62)

ii = H−iQi

− and Q(cid:48)

15

The incremental procedure adds Xover data to the model at each iteration, remembering that we need
to calculate beforehand the membership value to each data point in Xover, which increases the processing
time. In the worst case, we have Xover=Xnew, so the model dimension grows linearly with the number
of new data points, as well as the processing time increases at each new training iteration. To avoid
the continuous growth of the model dimension caused by the incremental procedure, we introduce a
decremental procedure to control the model dimension by removing data that has low or no interference
in the model accuracy. The proposed decremental procedure is also based on the shrinking technique,
where the SVs that have both Lagrangian multipliers smaller than a threshold (φ) after (d) occurrences
are removed. The decremental procedure is executed before each incremental training (except for the
ﬁrst training). We use the vector Zre=[01, . . . , 0q] (initially all points are assigned to zero) to keep track
of the number of occurrences per input data point.

Considering the current active set Z (without non-bounded Lagrangian multipliers), for each train-
ing data point there are two sets of Lagrangian Zα={α1, . . . , αq} and Zν={ν1, . . . , νq}, where q is the
number of Lagrangian multipliers. After each training iteration k → k + 1, we update the inputs (in its
corresponding position) that results in (αm ∧ βm)<φ in vector Zre. When the number of occurrences
reaches d, we remove all inputs and related data, so they will not be used in the next incremental training.
Algorithm 3 presents the pseudo-code for the decremental procedure.

Algorithm 3 Decremental procedure
Input: current model → X, y, α
Output: updated model ← X, y, α
1: Let Z = {1, . . . , X}
2: Given d, φ , Zre, Zα, and Zν
3: for do∀i ∈ Z
4:
5:
6:
7:
8: end for
9: return updated model ← X, y, α

if Zαi < φ ∧ Zνi < φ then Increase Zre i
end if
if Zre i = d then Remove Zre i, Zα i, Zν i
end if

5. Experimental Results

(cid:46) We assume that there is an existing classiﬁer
(cid:46) Zre is initially zero

In this section, we present the experimental protocol4 used to evaluate the FBTWSVM on bench-
marking datasets. For comparison purposes, we have used an experimental protocol similar to Losing et
al. [2], which compares a broad range of state-of-the-art on-line classiﬁcation algorithms, namely: ISVM
with RBF kernel, LASVM with RBF kernel, On-line Random Forest (ORF) [35], Incremental Learning
Vector Quantization (ILVQ) [36], Learn++ [37], Incremental Extreme Learning Machine (IELM) [38],
Naive Bayes [39], and Stochastic Gradient Descent (SGD). However, we have restricted to the evaluation
of the methods that led to the best accuracy in the on-line learning experiments for at least one of the
datasets, which are the ISVM, LASVM, ORF, and ILVQ [2]. The ORF [35] is an incremental Random
Forest algorithm that grows continuously from a pre-deﬁned number of trees by adding splits whenever
enough data points are gathered within one leaf. It uses Extreme Random Trees [40] to optimize the split,
using a pre-deﬁned number of random values. The ILVQ is a dynamic growth model derived from the
static Generalized Learning Vector Quantization [36], where the insertion rate is guided by the number
of misclassiﬁed data points. The ISVM and the LASVM were already described in Section 3.

The implementation used for comparison is from [41], which introduces a prototype placement strat-
egy to minimize the loss of a sliding window of recent data points. The experimental procedure of Losing
et al. [2] for on-line methods uses a window/chunk size from 500 to 2,000 and set all hyper-parameters
using the Hyperopt library [42] with the Tree-of-Parzen-Estimators [43] search algorithm, in which each

4All tests were performed in a machine running Ubuntu 16.04 LTS with an Intel Core i7-7700HQ CPU @ 2.80GHz and

16,144MB of RAM memory.

16

parameter is individually adjusted within 250 iterations of a 3-fold CV using only the training data. We
have carried out all our experiments with FBTWSVM using the approximated RBF kernel described in
Section 2.3, which enables the use of our linear formulation (Equations 19 and 20). We optimized the
model hyper-parameters using grid-search with a 3-fold cross validation on the training set, and we set
the Kernel approximation size following the strategy proposed by Rahimi and Recht [21].

Considering that we do not need to process all data points to obtain a model, the use of batches
accelerates the training phase. In this way, we use diﬀerent batch sizes but with the constraint that it
must encompass at least 5% of the data points of the fold, and the batch must contain at least one element
from each class in the ﬁrst training. We evaluated the FBTWSVM with six diﬀerent forgetting window
sizes empirically deﬁned as ϕ={1, 2, 4, 10}, and without the decremental procedure. We used publicly
available datasets without any preprocessing, although all attributes are numerical, either integer or real
values. The pre-deﬁned train-tests-splits were used when available. Otherwise, we adopted a stratiﬁed
train-test-split of 70-30%.Besides that, we have also created 15 synthetic datasets [44, 45] to evaluate
the scalability of the proposed method as well as a very large dataset of 23M samples [46]. However, for
such datasets, we have compared the FBTSVM just with other SVM-based methods. We have used the
following streaming generators with 10% of noise added: (i) the LED generator [47] yields instances with
24 Boolean features that correspond to the segments of a seven-segment LED display and another 17
irrelevant features; (ii) the SEA generator [48] generates streams from two relevant continuous attributes
f1, f2 and an irrelevant f3, with a range of values within 0 and 10; (iii) the Random Tree Generator (RTG)
[49] builds a decision tree by randomly selecting attributes as split nodes and assigning random classes
to each leaf. The number of values per nominal is set to 5, the max tree depth is 3, the ﬁrst leaf value is
3, and the leaf fraction is 0.15; (iv) the Radial Basis Function (RBF) generator creates 50 centroids at
random positions and associates them with a standard deviation value, a weight, and a class label. In
this way, new instances are set according to the random direction chosen to oﬀset the centroid, which
forms a Gaussian distribution according to the standard deviation associated with the given centroid; (v)
HYPER [49] generates instances that are separable by a hyperplane. We consider 10% sigma percentage,
and there is no magnitude change or drift attributes. We have created three datasets from each streaming
generator with 10,000, 100,000, and 1,000,000 training instances and 3,000, 30,000, and 300,000 testing
instances respectively. The focus of our evaluation is in incremental learning considering diﬀerent key
properties (as the number of classes, instances and dimensions), even though we can use the FBTWSVM
in oﬄine mode. The datasets5 encompass generated, artiﬁcial and real-world problems with diﬀerent
numbers of classes (from 2 to 100), data points (from 2,586 to 23M) and attributes (from 2 to 5,000),
as shown in Table 1, and although the largest dataset has roughly 21 million instances, the proposed
system does not speciﬁcally target learning from big data.

Table 1 also shows the parameter setting used for each dataset which was deﬁned in a 3-fold CV,
and ”Number of Points” stands for the initial training set size. For all datasets we used a ﬁxed fuzzy
parameter µ = 0.1. Using the 4-D case (C1, C2, C3, C4 are independent variables) for the hyperparameter
tuning may result in a model with a better generalization performance, i.e., the loss function may achieve
a lower value during the model selection compared to the 2-D case (we assume C1=C3 and C2=C4),
however, performing the hyperparameter tuning in a 2-D space may decrease substantially the number
of function evaluations needed, especially given that the grid search is essentially a brute force search
strategy that takes long. Other model selection strategies are able to speed-up the hyperparameter
tuning, however, this is out of the scope of this paper [50]. In many cases, using the 2-D space instead of
the 4-D is a valid heuristic estimation to decrease the number of function evaluations needed, and using
the Overlap dataset as an example, Figure 5 depicts that using the 2-D space it requires 34 function
evaluations to achieve the accuracy loss value of 0.1732, while the 4-D space requires 5,143 function
evaluations to achieve 0.1703.

Table 2 presents the accuracy of the incremental and decremental FBTWSVM against the best on-
line algorithms reported in [2]. The FBTWSVM achieved equal or better results in 8 out of 11 datasets
(from Border to Gisette, excluding the generated datasets) relative to the best on-line algorithms. The

5All datasets and algorithms are available at https://github.com/areeberg/FBTSVM

17

Number of Examples Number of Kernel

Attr Classes

Test

Train

2
1,000
2
990
16
4,000
18
500,000
21
1,400
21
5,400
180
1,186
256
2,007
617
1,559
10,000
784
1,000 5,000

4,000
3,960
16,000
4,500,000
2,600
1,800
1,400
7,291
6,238
60,000
6,000
21,668,504

Dataset
Border
Overlap
Letter
SUSY
Outdoor
COIL
DNA
USPS
Isolet
MNIST
Gisette
1,537,900
WESAD
10k|100k|1M 3k|30k|300k
LED
10k|100k|1M 3k|30k|300k
SEA
10k|100k|1M 3k|30k|300k
RTG
RBF
10k|100k|1M 3k|30k|300k
HYPER 10k|100k|1M 3k|30k|300k

8
24
3
10
10
10

3
4
26
2
40
100
3
10
26
10
2
3
10
2
2
5
2

Size
150
150
350
300
500
400
500
1,000
1,000
2,400
linear
linear
linear
linear
1,400
300
linear

γ
0.4
0.4
0.01
0.2
0.001
20
0.003
0.007
0.002
0.0002
linear
linear
linear
linear
0.6
0.45
linear

C1, C3 C2, C4

8
8
8
10
10
4
4
8
10
10
8
8
8
10
2.5
8
5

2
2
2
2
1
4
4
2
10
10
2
2
2
1
2
2
4

Number of
Points
100
100
1,000
100,000
300
500
50
1,000
500
10,000
500
1,537,900
5,000
5,000
5,000
5,000
5,000

Table 1: The datasets, their characteristics, and the experimental settings

SUSY dataset contains a signiﬁcant amount of data and to train the FBTWSVM we had limited the
size of the kernel approximation based on the memory available, and this also reduces the accuracy. For
instance, both the ISVM and the LASVM with an RBF kernel could not be trained with this dataset
due to the uncontrolled growth of the kernel matrix. We run an experiment using the SUSY full training
set considering a kernel approximation size of 600, resulting in 77.67%. Outdoor is a visual dataset that
consists of objects recorded outdoors under lighting conditions [41]. The dataset creation method has
caused a diﬀerence between training and test data [2], which reﬂects on the performance of the learning
algorithms. On-line algorithms with an adaptive learning mechanism presented the accuracy of about
20% better than oﬀ-line methods (the best result found was the oﬀ-line ISVM with 71.9% [2]).

Table 3 shows the relation between accuracy and the number of SVs resulting from diﬀerent forgetting
scores (d). The decremental procedure discards points that are less likely to be SVs. Smaller d leads
to classiﬁers with lower generalization performance, and for most of the datasets, the best performance
was achieved without forgetting or with large forgetting scores. On the other hand, the number of SVs
using the decremental procedure is considerably smaller, so the forgetting score must be chosen according
to the application. Table 3 also presents the comparison between the online and oﬄine approaches, in
which the online had better accuracy to all datasets with a smaller number of SVs compared to oﬄine.
A smaller d also implies in a faster training and classiﬁcation time, and Table 4 shows that the diﬀerence
in training time can be substantial (check the SUSY dataset values for example). The accuracy of
the COIL dataset with a forgetting score d=10 have similar accuracy (95.11%) when compared to the
oﬄine implementation of the FBTWSVM (95.00%), the ISVM (96.50%), the LASVM (93.20%) [2], and
the multiclass SVM implemented with the Error-Correcting Output Codes (ECOC) from MATLAB
(96.52%). In this manner, the forgetting strategy does not discard crucial support vectors, keeping the
accuracy score near the oﬄine approach. Table 4 presents the accuracy performance evolution when
increasing the forgetting score, which corroborates with the forgetting strategy, i.e., lower forgetting
scores tend to have smaller accuracy, however, by keeping the important SV the accuracy does not fall
substantially (the accuracy diﬀerence between d=1 and d=10 is 1.9%).

Table 5 compares the training time (in seconds), the real RAM consumed of the current process and
its children (in Gigabytes), and the accuracy of the FBTSVM with the other SVM based methods (ISVM
and LASVM). For these experiments we split the dataset into the largest batches that we can (that ﬁts
on the available memory, initially 15.4 Gb), to reduce the reloading procedure of the dataset during the
execution (more loading implies in a larger training time). Both the FBTWSVM and the ISVM (the

18

(a) Convergence plot for the 4-D case.

(b) Convergence plot for the 2-D case.

Figure 5: Overlap dataset convergence plot considering the 4-D and 2-D cases.

Table 2: On-line accuracy of the incremental and decremental FBTWSVM compared to other incremental algorithms on
several benchmark datasets. Statistically signiﬁcant diﬀerences are marked with (cid:63).

Accuracy (%)

ISVM LASVM ORF

ILVQ

Dataset

FBTWSVM
Best Mean±SD
97.60±1.10
98.70
84.14(cid:63)
82.58±1.49
96.75(cid:63)
96.68±0.07
76.00±1.20
77.67
73.72±0.42
74.44
95.11(cid:63)
94.99±0.14
93.59(cid:63)
92.90±0.30
94.91±0.30
95.47
96.28(cid:63)
95.88±0.37
97.00±0.12
97.80
96.40±0.01
96.50

Border
Overlap
Letter
SUSY
Outdoor
COIL
DNA
USPS
Isolet
MNIST
Gisette
” - ” denotes the non-available results due to limitations in memory size.

98.50
81.7
91.3
-
86.4
75.4
89.5
96.7
93.6
-
96.3

94.0
78.2
75.4
79.3
34.2
66.6
73.1
84.5
69.2
87.1
90.3

94.7
81.1
88.4
78.5
82.6
79.1
84.6
92.7
84.7
90.8
91.1

97.6
78.8
92.7
-
82.3
66.3
89.5
96.6
92.9
97.5
96.4

Table 3: On-line accuracy with diﬀerent forgetting scores (d) and the corresponding number of support vectors (nSV).

Accuracy(%)

93.30 |784
79.70 |2.3k
94.83 |122k

98.20 |1.3k
82.22 |3.5k
96.10 |204k

Dataset d=1 |nSV d=2 |nSV d=4 |nSV d=10 |nSV d=∞ |nSV OﬀL|nSV
Border
Overlap
Letter
SUSY
73.69 |68k
72.25 |39k
Outdoor
94.57 |153k
94.01 |100k
COIL
92.50 |1.2k
92.16 |848
DNA
94.87 |42k
94.32 |21k
USPS
95.32 |128k
95.51 |85k
Isolet
97.48 |169k
97.66 |342k
MNIST
97.00|1.9k 96.90 |2.9k
Gisette
” - ” denotes the non-available results due to limitations in memory size.

97.20|2.8k
91.90|538
82.83|7k
76.97|1.9k
93.38|83k
96.10|338k
45.84|754k 45.85 |1.5M 77.67|2.5M 73.78|3.4M
74.44|83k
72.00|24k
95.11|156k
93.21|98k
93.78|1.9k
91.82|770
95.36|60k
93.92|9k
95.89|143k
95.19|61k
97.91|455k
97.17|59k
96.20|5.3k
96.50|1.3k

98.70|7k
84.14|11.8k 83.30|11k
96.63|361k
76.45|3.8M -
73.88|92k
94.93|166k
93.59|2.6k
95.47|60k
96.28|143k
97.80|455k
96.50|6k

74.00|93k
95.00|178k
93.50|2.7k
95.30|65.5k
95.60|155k
97.80|540k
96.50|6k

96.90|384k

98.50|8k

19

0100020003000400050006000Number of function evaluations0.170.1750.180.1850.190.1950.20.2050.210.2150.22Best objective function value020406080100Number of function evaluations0.170.1750.180.1850.190.1950.20.2050.210.2150.22Best objective function valueTable 4: Training and testing processing time with diﬀerent forgetting scores (d) in seconds.

Training | Testing Time (sec)
d=10

d=∞

Dataset
Border
Overlap
Letter
SUSY
153.55 | 0.81
Outdoor
179.12 | 4.51
COIL
8.53 | 0.03
DNA
41.67 | 0.23
USPS
160.10 | 0.51
Isolet
435.48 | 1.26
MNIST
Gisette
49.41 | 0.01
” - ” denotes the non-available results due to limitations in memory size.

d=1
12.39 | 0.01
22.96 | 0.02
88.14 | 0.68
940.60 | 1.88
111.00 | 0.73
154.63 | 5.00
6.89 | 0.03
21.59 | 0.25
117.35 | 0.52
349.47 | 1.21
25.26 | 0.01

34.12 | 0.02
82.58 | 0.02
179.18 | 0.72
5264.41 | 2.62
153.09 | 0.80
169.10 | 5.29
8.63 | 0.03
41.59 | 0.24
181.14 | 0.49
419.49 | 1.19
50.52 | 0.01

43.22 | 0.02
94.37 | 0.02
192.36 | 0.70

-

-

|

ISVM multiclass adopt one-versus-one strategy) are implemented in MATLAB, thus it requires more
real RAM than the LASVM, that is for binary cases only and it is a C++ implementation. We do not
consider the LASVM in the multiclass cases (LED and RBF), and we discard the situations that the
training time took over 12 hours. All methods present competitive accuracy, however, the FBTSVM
is the only method (compared to ISVM and LASVM) able to train all dataset sizes in an acceptable
time, having the smallest training time for almost all situations (the only exception is the LASVM for
the RTG10K). The FBTSVM forgetting strategy is one of the factors (the kernel approximation also
plays an important role) that makes the training into large datasets possible, as Table 5 shows that the
real RAM consumed diﬀerence between the 100K and 1M datasets is not very expressive. The LED
dataset has a bigger memory diﬀerence between the datasets for the FBTWSVM, and this is caused
In this way, the scalability of
by the use of the multi-thread instead of the single processor version.
the FBTWSVM is superior to other online SVM-based methods, as it requires a smaller training time
to process large datasets and can handle the memory consumption in an eﬃcient manner. To further
explore the FBTWSVM potential for large datasets, we have also evaluated the accuracy, training time,
and memory consumption on the WESAD dataset [46] considering three classes (baseline, stress, and
amusement), eight attributes acquired from a sensor attached to the chest, and using the leave-one-
subject-out cross-validation (in total we have 17 subjects). The best result reported by Schmidt et
al. [46] is 76.50% using a Linear Discriminant Analysis, however, this is an oﬄine approach and the
authors do not present the training time or memory consumption. Our method achieved the accuracy
of 75.50% (Table 5), with a training time of 6,789 seconds and peak memory consumption of 9.8 GB.

Table 5: Comparison of the training time (TT) in seconds (s), real memory usage (RMU) in Gigabytes (GB) and percentual
accuracy (Acc) (%) for ﬁve synthetic datasets and three diﬀerent amount of data, and WESAD dataset.

Dataset
10k|100k|1M

RTG

ISVM

SEA
0.69|2.15|63

LED
5.81|19.1|380

Method

FBTWSVM

RBF
9.60| 119 |3.4k 11.4| 164 |5.3k 0.73|2.61|28.0
TT
RMU 3.26|3.81|7.34 0.84|0.91|1.31 1.84|10.1|10.6 1.04|2.06|7.03 0.84|0.96|1.47
74.1|74.1|74.2 89.0|87.1|89.1 95.8|95.5|96.0 88.3|89.4|89.0 89.1|94.7|94.0
Acc
63.9| - | -
TT
40.7| - | -
0.94| - | -
RMU 0.89| - | -
90.0| - | -
74.2| - | -
Acc
2.39| 546 | -
-
TT
0.03|0.41| -
-
RMU
88.0|95.7| -
-
Acc

8.80|4.5k| -
0.88|0.98| -
89.9|89.3| -
2.44| 328 | -
0.06|0.36| -
84.0|87.0| -
” - ” denotes the non-available results due to training times greater than 12 hours.

14.4|4.9k| -
0.82|0.95| -
94.1|93.0| -
2.21| 519 | -
0.05|0.37| -
91.7|93.8| -

31.8| - | -
0.94| - | -
87.6| - | -
-
-
-

LASVM

6.8k
9.80
75.5
-
-
-
-
-
-

HYPER WESAD

20

6. Conclusion

In this paper, we propose a novel SVM approach suitable for incremental and decremental on-line
learning. The incremental and decremental Fuzzy Bounded Twin SVM (FBTWSVM) integrates ideas
coming from diﬀerent SVM approaches such as the Twin SVM [7], the Fuzzy SVM [27], the Bounded
TWSVM [11], the Fast and Robust TWSVM [14, 25], the Optimal DAG TWSVM [34], and the dual
coordinate descent method [28]. The FBTWSVM calculates a pair of non-parallel hyperplanes using
two smaller QPPs, rather than one large QPP as in the original SVM, but with adherence to structural
risk minimization principle. The dual form of the FBTWSVM leads to a pair of convex quadratic
programming problems with a unique solution and singularity avoidance. The dual coordinate descent
method with shrinking requires less memory storage than the TWSVM, as it discards points that are
less likely to be SVs. The fuzzy concept enhances noise-resistance and generalization capability, while
the use of a kernel approximation shows a good generalization performance with our linear model.

The incremental solution follows the shrinking strategy and can run with diﬀerent batch sizes, from a
single individual to a number of data points that ﬁts the available memory. The decremental procedure
is fundamental to control the model complexity, keeping only the most critical SVs in the model. The
FBTWSVM is ﬂexible and both incremental and decremental procedures can be conﬁgured according
to the application, changing the threshold of adding new SVs in the incremental step and the num-
ber of occurrences in the decremental step. According to the experimental results, the DAG strategy
showed a good generalization capability and a fast training speed, but for further studies the use of train-
ing data structural and statistical information in the training process may increase the generalization
performance. A practical diﬃculty in the FBTWSVM is the optimization of the six hyper-parameters
C1, C2, C3, C4, µ, γ and the kernel approximation size, however, this problem will be addressed in the
future. The FBTWSVM can adapt current models using the window strategy, or even add new mod-
els (e.g.
in case of new classes) without retraining. Therefore, as a future work, we will evaluate the
FBTWSVM use in the context of concept drift, novelty detection, and big data.

References

[1] R. Khemchandani, Jayadeva, S. Chandra, Incremental Twin Support Vector Machines,

in: S. K.
Neogy, A. K. Das, R. B. Bapat (Eds.), Modeling, Computation and Optimization, World Sci Pub
Co, 2009, pp. 263–272.

[2] V. Losing, B. Hammer, H. Wersing, Incremental on-line learning: A review and comparison of state

of the art algorithms, Neurocomp 275 (2018) 1261–1274.

[3] C. Cortes, C. Cortes, V. Vapnik, Support-Vector Networks, Mach Learn 20 (1995) 273–297.

[4] G. Cauwenberghs, T. A. Poggio, Incremental and decremental support vector machine learning, in:

Advances in Neural Information Processing Systems 13, Denver, CO, USA, pp. 409–415.

[5] A. Bordes, S. Ertekin, J. Weston, L. Bottou, Fast Kernel Classiﬁers with Online and Active Learning,

J Mach Learn Res. 6 (2005) 1579–1619.

[6] O. Mangasarian, E. Wild, Multisurface proximal support vector machine classiﬁcation via general-

ized eigenvalues, IEEE Trans Patt Anal Mach Intell 28 (2006) 69–74.

[7] Jayadeva, R. Khemchandani, S. Chandra, Twin Support Vector Machines for Pattern Classiﬁcation,

IEEE Trans Patt Anal Mach Intell 29 (2007) 905–910.

[8] D. Tomar, S. Agarwal, Twin Support Vector Machine: A review from 2007 to 2014, Egyptian Inf

J 16 (2015) 55–69.

21

[9] S. Ding, J. Yu, B. Qi, H. Huang, An overview on twin support vector machines, Artif Intell Review

42 (2014) 245–252.

[10] Jayadeva, R. Khemchandani, S. Chandra, Twin Support Vector Machines, Studies in Comp Intell,

Springer, 2017.

[11] Y.-H. Shao, C.-H. Zhang, X.-B. Wang, Nai-Yang Deng,
Machines, IEEE Trans Neural Net 22 (2011) 962–968.

Improvements on Twin Support Vector

[12] Y. Tian, X. Ju, Z. Qi, Y. Shi, Improved twin support vector machine, Sci China Math 57 (2014)

417–432.

[13] R. Khemchandani, Jayadeva, S. Chandra, Fuzzy Twin Support Vector Machines for Pattern Clas-

siﬁcation, in: Math Program Game Theory for Decis Making, 2008, pp. 131–142.

[14] B.-B. Gao, J.-J. Wang, Y. Wang, C.-Y. Yang, Coordinate Descent Fuzzy Twin Support Vector

Machine for Classiﬁcation, in: IEEE 14th Intl Conf Mach Learn and Appl, pp. 7–12.

[15] M. Arun Kumar, M. Gopal, Least squares twin support vector machines for pattern classiﬁcation,

Expert Sys with Appl 36 (2009) 7535–7543.

[16] X. Peng, Xinjun, A ν-twin support vector machine (ν-TSVM) classiﬁer and its geometric algorithms,

Inf Sciences 180 (2010) 3863–3875.

[17] Y. Hao, H. Zhang, A Fast Incremental Learning Algorithm Based on Twin Support Vector Machine,

in: Intl Symp Comp Intel. and Design, pp. 92–95.

[18] F. Alamdar, S. Ghane, A. Amiri, On-line twin independent support vector machines, Neurocomp

186 (2016) 8–21.

[19] G. Fung, O. L. Mangasarian,

Incremental Support Vector Machine Classiﬁcation,

in: Intl Conf

Data Min, Society for Indus and Appl Math, Philadelphia, PA, 2002, pp. 247–260.

[20] A. Tveit, M. L. Hetland, H. Engum, Incremental and Decremental Proximal Support Vector Clas-
siﬁcation using Decay Coeﬃcients, in: Data Wareh Knowl Discov, Springer, 2003, pp. 422–429.

[21] A. Rahimi, B. Recht, Random Features for Large-Scale Kernel Machines, in: NIPS, pp. 1177–1184.

[22] F. Li, C. Ionescu, C. Sminchisescu, Random Fourier Approximations for Skewed Multiplicative

Histogram Kernels, in: Lec Notes in Comp Sci - 6376, 2010, pp. 262–271.

[23] Q. V. Le, T. Sarlos, A. J. Smola, Fastfood: Approximate Kernel Expansions in Loglinear Time,

2014.

[24] A. Rahimi, B. Recht, Weighted Sums of Random Kitchen Sinks: Replacing minimization with

randomization in learning, in: NIPS, pp. 1313–1320.

[25] B. Gao, J. Wang, A fast and robust TSVM for pattern classiﬁcation, CoRR abs/1711.05406 (2017).

[26] Jayadeva, R. Khemchandani, S. Chandra, Variants of Twin Support Vector Machines: Some More

Formulations, Springer, 2017.

[27] C.-F. Lin, S.-D. Wang, Fuzzy support vector machines, IEEE Trans Neural Net 13 (2002) 464–471.

[28] K.-W. Chang, C.-J. Hsieh, C.-J. Lin, Coordinate Descent Method for Large-scale L2-loss Linear

Support Vector Machines, J Mach Learn Res. 9 (2008) 1369–1398.

[29] Y.-H. Shao, N.-Y. Deng, A coordinate descent margin based-twin support vector machine for

classiﬁcation, Neural Net 25 (2012) 114–121.

22

[30] T. Joachims, Making large-scale SVM learning practical, in: Adv in kernel methods: support vector

learning, MIT Press, 1999, p. 376.

[31] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, S. Sundararajan, A dual coordinate descent

method for large-scale linear SVM, in: 25th Intl Conf Mach Learn, NY, USA, pp. 408–415.

[32] C.-W. Hsu, C.-J. Lin, A comparison of methods for multiclass support vector machines, IEEE Trans

Neural Net 13 (2002) 415–425.

[33] J. C. Platt, N. Cristianini, J. Shawe-Taylor, Large Margin DAGs for Multiclass Classiﬁcation, in:

NIPS, pp. 547–553.

[34] J. Chen, G. Ji, Multi-class LSTSVM classiﬁer based on optimal directed acyclic graph, in: 2nd Intl

Conf Comp and Autom Eng, IEEE, 2010, pp. 100–104.

[35] A. Saﬀari, C. Leistner, J. Santner, M. Godec, H. Bischof, On-line Random Forests, in: IEEE 12th

Intl Conf Comp Vis, pp. 1393–1400.

[36] A. Sato, K. Yamada, Generalized Learning Vector Quantization, in: NIPS, pp. 423–429.

[37] R. Polikar, L. Upda, S. Upda, V. Honavar, Learn++: an incremental learning algorithm for super-

vised neural networks, IEEE Trans Sys, Man, Cyber C, Appl Rev 31 (2001) 497–508.

[38] N.-Y. Liang, G.-B. Huang, P. Saratchandran, N. Sundararajan, A Fast and Accurate Online Sequen-

tial Learning Algorithm for Feedforward Networks, IEEE Trans Neural Net 17 (2006) 1411–1423.

[39] H. Zhang, The Optimality of Na¨ıve Bayes, in: 7th Intl Florida AI Res. Soc. Conf, pp. 562–567.

[40] P. Geurts, D. Ernst, L. Wehenkel, Extremely randomized trees, Mach Learn 63 (2006) 3–42.

[41] V. Losing, B. Hammer, H. Wersing, Interactive online learning for obstacle classiﬁcation on a mobile

robot, in: Intl Joint Conf Neural Networks, Killarney, Ireland, 2015, pp. 1–8.

[42] J. Bergstra, B. Komer, C. Eliasmith, D. Yamins, D. D. Cox, Hyperopt: a Python library for model

selection and hyperparameter optimization, Comp Sci & Discov 8 (2015) 014008.

[43] J. S. Bergstra, R. Bardenet, Y. Bengio, B. K´egl, Algorithms for Hyper-Parameter Optimization, in:

NIPS, pp. 2546–2554.

[44] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, I. H. Witten, The WEKA data mining

software, ACM SIGKDD Explor Newsl 11 (2009) 10.

[45] I. H. Witten, E. Frank, M. A. Hall, C. J. Pal, Data mining : pract mach learn tools techn, Morgan

Kaufmann; 3 ed, 2016.

[46] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, K. Van Laerhoven,

Multimodal Dataset for Wearable Stress and Aﬀect Detection,
NY, USA, pp. 400–408.

Introducing WESAD, a
in: Intl Conf Multimodal Interac,

[47] L. Breiman, J. H. J. H. Friedman, R. A. Olshen, C. J. Stone, Classiﬁcation and regression trees,

Wadsworth and Brooks, 1984.

[48] W. N. Street, Y. Kim, A streaming ensemble algorithm (SEA) for large-scale classiﬁcation, in: 7th

ACM SIGKDD Intl Conf Knowl Discov and Data Min, NY, USA, pp. 377–382.

[49] P. Domingos, G. Hulten, Mining high-speed data streams, in: 6th ACM SIGKDD Intl Conf Knowl

Discov and Data Min, NY, USA, pp. 71–80.

[50] A. R. de Mello, J. de Matos, M. R. Stemmer, A. de Souza Britto Jr., A. L. Koerich, A novel
orthogonal direction mesh adaptive direct search approach for SVM hyperparameter tuning, CoRR
abs/1904.11649 (2019).

23

