2
2
0
2

y
a
M
8
2

]

M
D
.
s
c
[

2
v
2
2
8
3
0
.
4
0
2
2
:
v
i
X
r
a

DIVERSITREE: A NEW METHOD TO EFFICIENTLY COMPUTE
DIVERSE SETS OF NEAR-OPTIMAL SOLUTIONS TO
MIXED-INTEGER OPTIMIZATION PROBLEMS

Izuwa Ahanor, Hugh Medal

Andrew C. Trapp

Department of Industrial and Systems Engineering,

Business School, Data Science Program

University of Tennessee-Knoxville

Worcester Polytechnic Institute

iahanor@vols.utk.edu, hmedal@utk.edu

atrapp@wpi.edu

ABSTRACT

While most methods for solving mixed-integer optimization problems compute a single optimal
solution, a diverse set of near-optimal solutions can often be more useful. We present a new method
for ﬁnding a set of diverse solutions by emphasizing diversity within the search for near-optimal
solutions. Speciﬁcally, within a branch-and-bound framework, we investigate parameterized node
selection rules that explicitly consider diversity. Our results indicate that our approach signiﬁcantly
increases diversity of the ﬁnal solution set. When compared with two existing methods, our method
runs with similar runtime as regular node selection methods and gives a diversity improvement of up
to 140%. In contrast, popular node selection rules such as best-ﬁrst search gives an improvement in
diversity of no more than 40%. Further, we ﬁnd that our method is most effective when diversity in
node selection is continuously emphasized after reaching a minimal depth in the tree and when the
solution set has grown sufﬁciently large. Our method can be easily incorporated into integer
programming solvers and has the potential to signiﬁcantly increase diversity of solution sets.

Keywords integer programming · near-optimal solutions · diversity · node-selection rules

1 Introduction

It is often useful to ﬁnd sets of near-optimal solutions to optimization problems rather than a single solution. In
particular, for these multiple near-optimal solutions to be usable, they should be diverse, to ensure that decision makers
are presented with a variety of options.

One class of methods for generating diverse sets of near-optimal solutions to mixed-integer optimization (MIO)
problems uses a two-phase approach. In the ﬁrst solution generation phase, an oracle ﬁnds a set of near-optimal
solutions without considering diversity. For example, Danna et al. (2007) developed the ONETREE oracle for this
purpose. In the second diverse subset selection phase the output set from the ﬁrst phase is processed by heuristics (e.g.,
Glover et al. (2000), Danna and Woodruff (2009)) or an optimization algorithm (e.g., Danna and Woodruff (2009)) to
select a small subset of the input set with maximum diversity. While this approach works well for problems with a
small number of near-optimal solutions, for MIOs with a very large set of near-optimal solutions it is not practical to
ﬁnd the complete set. As a result, the ﬁrst phase can only compute a subset of near-optimal solutions. Unfortunately,
because existing ﬁrst-phase methods do not consider diversity, the near-optimal sets they generate often lack diversity

 
 
 
 
 
 
Ahanor, Medal and Trapp

(see §4). If this subset is not diverse, then the smaller subset computed by the second phase necessarily will lack
diversity. To address this problem, we developed a method — DIVERSITREE — to emphasize diversity in the ﬁrst
phase (solution generation). The output from our solution generation oracle can be used by itself or as a more diverse
input set for the second phase in which a for diverse subset is computed.

1.1 Motivation

There has been increasing awareness of ﬁnding not only one but multiple optima due to increased computational
capabilities in the last two decades (Bertsimas et al. (2016)). Some speciﬁc applications in which it is desirable to ﬁnd
multiple near-optimal solutions to optimization problems include the correct identiﬁcation of metabolic activity of cells
and tissues in metabolic networks (Rodríguez-Mier et al. 2021), identifying alternative near-optimal structural designs
(He et al. 2020), providing policies that are more robust to data changes in reinforcement learning and machine learning
(Kumar et al. 2020, Eysenbach et al. 2018), enabling exploration and mapping searches to broader but speciﬁc solutions
in large search spaces by including near-optimal search results in search requests (Mouret and Clune 2015, Zahavy et al.
2021), adding more artistic alternatives to structural topology optimization (Cai et al. 2021, He et al. 2020), providing
competitive alternatives to facility location and location routing problems (Church and Baez 2020, Schittekat and
Sörensen 2009), diversifying software deployment to enable stronger computer software security (Tsoupidi et al. 2020),
aiding motif ﬁnding in computational molecular biology (Zaslavsky and Singh 2006), generating multiple near-optimal
group preferences in computational social choice analysis (Boehmer and Niedermeier 2021), and broadening
architectural testing in processor design (Van Hentenryck et al. 2009).

Many of these applications have a large number of near-optimal solutions (see Table 2 in Appendix A). For instance,
both the routing problem from Ceria et al. (1998) and the multi-period facility location problem studied in Eckstein
(1996) are known to have more than 10,000 solutions with objective value within 1% of optimality. In problems with
many near-optimal solutions, there is a need to identify a small subset of near-optimal solutions that is representative of
the whole. One measure of how well a subset represents the whole is the diversity of the subset. Unfortunately, methods
for ﬁnding near-optimal solutions such as ONETREE ﬁnd near-optimal sets consisting of solutions that are not very
diverse (see example provided in §5).

There are several important contexts in which it is useful to have a diverse set of near-optimal solutions. First, in some
design problems decision makers seek a set of designs from which to choose, for the purpose of considering other
difﬁcult-to-model factors when selecting a single design (Joseph et al. 2015). A similar context is in statistical model
selection for a speciﬁc domain. Presenting a domain expert with a diverse set of models with similar ﬁt allows the
expert to select the model that best matches intuition. Several new studies hypothesize that for many machine learning
tasks a set of models exist with near-minimal loss (Semenova et al. (2019); also see §9 of Rudin et al. (2022)). Second,
using optimization for decision problems can often be an iterative process in which a MIP solution is ﬁrst presented to a
decision maker, only to have the decision maker identify that the solution violates an important side constraint that was
not included in the model. If the decision maker is only given a single solution, the model must be re-solved after
adding the side constraint. However, if a diverse set of near-optimal solutions were readily available, the decision maker
may be able to ﬁnd a good solution in the set that does not violate the side constraint, avoiding the need to re-solve the
model. Third, in contexts in which solutions are implemented repeatedly, it can be useful to alternate between a diverse
set of near-optimal solutions. For example, one may use MIP to match workers to jobs to minimize total completion
time, but regularly implement different near-optimal matchings to increase cross-training. Lastly, a set of near-optimal
solutions can be used to measure and explain the importance of variables in applications such as statistical model
selection (see §9 of Rudin et al. (2022)). If a feature is present in a large number of diverse near-optimal solutions, this
provides additional evidence that the feature is an important predictor of the response variable.

In two-phase approaches the output of the solution generation phase is the input to the diverse subset selection phase
(see, e.g., Danna et al. 2007, Silva et al. 2007). Solution generation methods that do not include diversity in the solution
generation phase would use this approach with the intent of collecting enough solutions to cover the solution space in
the ﬁrst phase and hence a diverse set in the second phase. Apart from the additional time this takes, if the solution set

2

DIVERSITREE

computed in the solution generation phase lacks diversity, then the subset computed by the diverse subset selection
phase will lack diversity. To address this gap, this research explores methods that encourage diversity during the
solution generation phase.

1.2 Related Work

Although there exists some work on ﬁnding near-optimal solutions to continuous optimization problems (see Lavine
(2019)), most of the work has been in relation to mixed-integer optimization (MIO) problems and, in particular, with
respect to (binary) integer variables.

There are a number of different algorithms for generating a set of near-optimal solutions to a MIO problem. Achterberg
et al. (2008) developed an approach for generating all feasible solutions to an MIO problem called branch-and-count.
This approach is based on detecting “unrestricted subtrees" in the branching tree. This method can also be used to ﬁnd
all solutions within a certain threshold of the optimal objective value, if known. Other methods have been proposed for
ﬁnding all optimal solutions; an example is the work by Lee et al. (2000) which uses a recursive MIP algorithm to ﬁnd
all alternate optima. Serra (2020) used weighted decision diagrams to compactly represent all near-optimal solutions
generated for an integer programming problem. The compact representation eases resolves and information retrieval
from all generated solutions. Also, the ONETREE method (currently implemented in GAMS-CPLEX) given by Danna
et al. (2007) which extends the branching tree used to solve the MIP problem to generate near-optimal solutions. In
addition, approaches have been developed for speciﬁc classes of problems including using dynamic programming to
generate multiple solutions to graph-based problems (Baste et al. (2022)), methods to represent near-optimal solutions
compactly (Serra and Hooker (2020)) and algorithms speciﬁc to topology optimization problems (Wang et al. (2018)).
While these methods are effective at ﬁnding near-optimal solutions, they do not necessarily compute solution sets that
are diverse. One of the few papers that discusses balancing diversity and optimality is Zhou et al. (2016). They
developed a Dual Diverse Competitive Design (DDCD) method that formulates balancing optimality and diversity as an
optimization problem that maximizes diversity, subject to constraints on the performance penalty. While their method
speciﬁcally focuses on generating competitive designs that would give diverse solutions, the goal of this article is a
method for general MIO problems.

There are a number of different approaches for ﬁnding a diverse set of near-optimal solutions to an optimization
problem. In the sequential approach (Danna et al. (2007)), the optimization problem is solved multiple times, and after
each solve a constraint is added that requires the next optimal solution be different from the previous. The sequential
approach is also used in Trapp and Konrad (2015) to select diverse solutions to binary integer linear problems and in
Petit and Trapp (2015) in the context of constraint programming. Both methods consider maximizing a metric that is
the ratio of diversity of solutions to loss in objective function. Petit and Trapp (2019) extend these ideas further by
including the notion of solution quality when generating diverse near-optimal solutions. The variable copy approach
adds k copies of the variables, one copy for each near-optimal solution desired, in an optimization model and adds
constraints to enforce that the solutions differ (Cameron et al. (2021)).

Greistorfer et al. (2008) compared the sequential and variable copy approaches for the problem of ﬁnding two diverse
near-optimal solutions and found that the sequential approach usually required less computation time and yielded
solutions that were nearly as good as the simultaneous approach. Further, population-based metaheuristics are a natural
approach for ﬁnding diverse sets of solutions because these algorithms operate on a set of solutions. For instance,
Glover et al. (2000) used a scatter-search algorithm to ﬁnd a diverse set of solutions to MIPs. While metaheuristics are
often effective in practice, their main drawback is their inability to guarantee that the entire set of near-optimal solutions
has been found. Finally, two-phase approaches use an oracle such as the ONETREE algorithm (Danna et al. (2007)) to
ﬁnd a (not necessarily diverse) set of near-optimal solutions. Then the second phase inputs this set of near-optimal
solutions and chooses a subset that maximizes diversity. Regarding the second phase, Danna and Woodruff (2009)
developed exact and heuristics algorithms to ﬁnd a diverse subset. The work by Glover et al. (1998) which proposed
four different heuristics algorithms for generating the most diverse solutions from a larger set does not consider how
close they are to the optimal objective value. Kuo et al. (1993) proposed two different methods that use linear

3

Ahanor, Medal and Trapp

programming to select the most diverse solutions from a solution set. Schwind et al. (2020) developed methods for
computing a small subset of solutions that represent the larger set using bi-objective optimization. The authors of
Danna et al. (2007) found that the sequential and simultaneous methods do not scale as the number of solutions that are
been generated increases. The approach developed in this article combines both phases of the two-phase approach,
emphasizing diversity while searching for a near-optimal set.

1.3 Contributions and Findings

This article examines how using different node selection rules within a branch-and-bound framework affects the
diversity of the computed near-optimal solution set. In addition, we also present experimental results for new node
selection rules that prioritize solution diversity. We select random problems from MIPLIB and split them into a training
set (75% of the number of problems) and a test set (25% of the problems). We learn the optimal parameters for
DIVERSITREE on the training set and evaluate these parameters on the problems in the test set. Our speciﬁc
contributions are the following:

1. Diversity of solution sets computing by existing node selection rules. We investigated the diversity of

near-optimal sets generated using popular node selection rules within a branch-and-bound algorithm. We
found that, while the effect of node selection rules on solution set diversity is problem speciﬁc, the best-ﬁrst
search rule typically gave better diversity solutions overall.

2. A new node selection rule that emphasizes solution set diversity. We investigated a new node selection rule

used within a branch-and-bound framework that selects new nodes in the tree based on a criteria that infuses a
diversity metric into the traditional node relaxation value.

3. Best parameters for diversity-emphasizing node selection rules. We investigated several different

parameterized node selection rules for ﬁnding diverse solution sets, including the depth of a node within the
branch-and-bound tree and the number of solutions currently in the solution set. We found that the greatest
diversity is obtained when waiting to emphasize diversity until a number of solutions is gathered or when
selecting solutions deeper in the tree.

4. Beneﬁts of emphasizing diversity in node selection. Using 36 problem instances randomly selected from

MIPLIB (2003, 2010 and 2017), we found that using our new diversity-emphasizing node selection results in
solution sets that are up to 140% more diverse than the ONETREE algorithm—a leading method for ﬁnding
near-optimal sets currently implemented in GAMS. Regarding runtime, we found that our method of
emphasizing diversity within the node selection procedure runs within ±15% of the time used by regular node
selection methods that do not emphasize diversity and results in a better tradeoff between runtime and solution
diversity than competing approaches. Importantly, we found that the parameter settings that performed best on
a training set of problems also performed well even when applied to a “test set” of problems.

1.4 Outline of the paper

The remainder of this article ﬁrst gives mathematical and algorithmic preliminaries in §2, followed by a description of
the diversity-emphasizing node selection rules tested in this work in §3. Experimental results on the training and test set
data is discussed in §4, while §5 illustrates the use of DIVERSITREE on real data prior to concluding in §6.

2 Mathematical and Algorithmic Preliminaries

We consider ﬁnding a diverse set of near-optimal solutions to the following problem:

z∗ = min
x∈X

cT x where

X = {x ∈ Rd : Ax ≥ b, xi ∈ Z, ∀i ∈ I ⊆ {1, . . . , d}}.

(1)

4

DIVERSITREE

Let Sq = {x ∈ X : cT x ≤ (1 + q)z∗} denote a set of q%-optimal solutions to (1), q ≥ 0. While mixed-integer
programs are considered, diversity is only computed over binary integer variables. If |Sq| is small, it may be sufﬁcient
to use a near-optimal solution generation algorithm such as ONETREE (Danna et al. (2007)) to generate the entire set Sq
for presentation to a decision maker. In the case in which |Sq| is large but not very large, not more than 1,000 elements,
the following two-phase approach may be used. In the ﬁrst phase, a solution generation algorithm is used to obtain the
complete set of near-optimal solutions Sq. Then in the second phase, use a diverse subset selection algorithm (see, e.g.,
Danna and Woodruff 2009) to ﬁnd a small subset of Sq of cardinality p, solving the following problem:

max
S⊆Sq,|S|=p

D(S).

(2)

where D(S) is a measure of the diversity of solution set S.

In many cases, however, the set Sq has very many elements, perhaps 10,000 or more. In particular, for about half of the
instances tested in Danna and Woodruff (2009), |Sq| > 10, 000. For these instances, the authors limited the input to the
second phase to a subset of Sq consisting of the ﬁrst 10,000 solutions obtained by the ONETREE algorithm Danna et al.
(2007). That is, the ﬁrst phase ﬁnds a subset ¯S ⊆ Sq (| ¯S| = 10, 000) without explicit consideration of diversity, while
the second phase solves:

max
S⊆ ¯S,|S|=p

D(S).

(3)

These problem instances with very large Sq present two issues:

• Computational. These problems could not be solved to optimality in the second-phase of the two-phase

approach due to memory limitations or exceeding a 10-day time limit (Danna and Woodruff 2009). As a result,
heuristics were employed. For smaller problems these heuristics appear to produce solutions that nearly
maximize diversity. However, it is not known whether the heuristics produce good solutions to any of the
larger problems.

• Solution quality. Using the subset ¯S as an input to the second phase rather than the complete set Sq could

result in a loss of diversity if there is insufﬁcient diversity in the input subset to either of the heuristic or exact
methods.

To address these issues, we seek to solve (3) directly rather than via a two-phase approach. Speciﬁcally, we examine
how to modify the exploration strategy of the solution generation phase to increase the diversity of the subset ¯S.

2.1 Computing near-optimal sets

Given a mixed integer programming problem, we assume the existence of an oracle capable of enumerating a set of all
or a sufﬁciently high number of near-optimal solutions for the problem. We use the branch-and-count method as the
oracle due to its signiﬁcant speed at enumerating near-optimal solutions by detecting and pruning infeasible subtrees
(Achterberg et al. 2008). The branch-and-count method is implemented in SCIP (Gamrath et al. 2020), which provides
access to, as well as modiﬁcation privileges for, the branching tree and implementation methods of the oracle.

The branch-and-count algorithm is an extension of the branch-and-cut algorithm, a variant of branch-and-bound that
adds valid inequalities to the formulation during the traversal of the tree so as to strengthen the LP relaxation.
Branch-and-cut operates by going through the entire search tree generated during the regular branch-and-cut process,
collecting all feasible solutions at each node if that node is detected to represent an unrestricted subtree. The authors
prove that the subtree at a node in the search tree is unrestricted if all constraints at that subtree are satisﬁed by all
possible variable assignments of values in the domain of that subtree, that is, the constraints are locally redundant at
that node. This way, they can construct solution vectors for that subtree while avoiding traversing every node in that
subtree. This signiﬁcantly reduces the number of nodes this algorithm visits. The pseudocode is given in Algorithm 1.

5

To use this algorithm to generate the set ¯S, we ﬁrst solve equation (1) to global optimality to ﬁnd an optimal objective
value z∗. Next, we pass the number of near-optimal solutions requested (p1) as input to the algorithm and add the
constraint cT x ≤ (1 + q)z∗ to equation (1) and solve using the branch-and-count algorithm.

Ahanor, Medal and Trapp

Algorithm 1 Branch and Count Pseudocode
1: INPUT p1, MIP
2: ADD constraint cT x ≤ (1 + q)z∗ to initial MIP problem
3: Q ← ∅
4: ADD initial MIP problem to queue of active nodes Q
5: ¯S ← ∅
6: while Q is not empty and | ¯S| < p1 do
7:
8:
9:
10:
11:
12:
13:
14: end while
15: return ¯S

Use node selection rule to dequeue node i(cid:48) from list Q
if node i(cid:48) is unrestricted then

else if node i(cid:48) is infeasible then

Add solution to ¯S

Prune

end if
Use regular branching rule to discover more nodes and add to list Q

2.2 Node Selection Rules.

During line 6 of the branch-and-count search outlined in Algorithm 1, the node selection rule decides which node from
the queue of active nodes is selected as the next node. Popular node selection rules include:

• Best-First Search (BestFS). Selects the node with the best bound, that is, for minimization problems, select

i(cid:48) as the next node according to

i(cid:48) ∈ arg min
i∈Q

{LBi} ,

where LBi is the lower bound for node i.

• Depth First Search (DFS). Nodes encountered as the branch and bound search tree is traversed are added to

a queue and are selected in Last In First Out (LIFO) order.

• Breadth First Search (BrFS). Nodes are added to a queue and processed using the First In First Out (FIFO)

order.

• Upper Conﬁdence Bounds for Trees (UCT) (Gamrath et al. (2020)). Selects the next node i(cid:48) as a node

with the best UCT_score, that is:

U CT _score is calculated as:

i(cid:48) ∈ arg min
i∈Q

{U CT _Scorei} ,

U CT _Scorei = LBi + ρ

Vi
vi

6

DIVERSITREE

where vi and Vi are the number of times the algorithm has visited node i and its parent, respectively. ρ is a
weight parameter chosen by the user.

• Hybrid Estimate (HE) (Gamrath et al. (2020)). Selects the next node i(cid:48) as a node having the best

HE_Score, i.e.:

HE_Score is calculated as:

i(cid:48) ∈ arg min
i∈Q

{HE_Scorei} ,

HE_Scorei = (1 − ρ)LBi + ρ (cid:100)LBi

(cid:100)LBi is the estimated value of the best feasible solution in subtree of node i and ρ is a user-deﬁned weight
parameter.

2.3 Measuring the diversity of solutions

A number of metrics exist for measuring the diversity of a set of solutions. A good metric needs to be model agnostic

and ideally scaled such that diversity scores given by the metric are easy to interpret. Danna and Woodruff (2009)
outlined three problem agnostic measures for the diversity of solutions: DBin, deﬁned in more detail below and used
in our tests, is scaled by the number of variables and solutions generated and considers just the binary variables; DAll
which considers all variable types; and DCV which is the scaled version of DAll.
The DBin metric (deﬁned only on binary variables) is the average scaled Hamming distance between all pairs of
solutions in a set S, that is,

DBin(S) =

2
|S|(|S| − 1)

|S|
(cid:88)

|S|
(cid:88)

j=1

k=j+1

DBin(x(j), x(k)),

(4)

where x(j) is the jth solution generated by the oracle, and DBin() computes the Hamming distance between a pair of
solutions, that is,

DBin(x(j), x(k)) =

1
|B|

(cid:88)

i∈B

|x(j)

i − x(k)

i

|,

(5)

where B is the set of binary variables and |B| is the number of binary variables. An advantage of the DBin metric is
that it takes values between 0 and 1 inclusive and does not depend on the size of the solution set or the number of
variables.

3 Diversity-Emphasizing Node Selection rules

Within the branch-and-count algorithm (Achterberg et al. 2008), we investigated several variants of the best-ﬁrst search
(BestFS) node selection rule that consider solution set diversity when selecting the next node to evaluate. We focused
on BestFS because, as our results in §4 show, it generated the most diverse solution sets when compared with other
well-known node selection rules (such as DFS) when diversity was not considered in the node selection task. What
follows is a description of each of the custom node selection rules tested in this work.

7

Ahanor, Medal and Trapp

3.1 Diverse-BFS (D-BFS(α))

The Diverse-BFS (D-BFS α) node selection rule considers both the lower bound of a node as well as the diversity of the
node with respect to other solutions already in the near-optimal set. That is, this rule inputs a set of open nodes O and
selects the next node i(cid:48) according to:

i(cid:48) ∈ arg min
i∈O

{(1 − α)Li + αDi},

where α ∈ [0, 1] is a parameter that trades off the bound of node i against its diversity score Di and Li is a scaled lower
bound of node i, that is,

Li =

LBi − minj∈O LBj
maxj∈O LBj − minj∈O LBj

.

The lower bound is scaled to [0, 1], commensurate with the diversity score Di ∈ [0, 1]. The value of Di represents the
partial diversity of node i with respect to the current solution set ¯S. We use the term partial because it is computed
based only on the binary variables that have been ﬁxed at a particular node in the branching tree.

3.2 Diverse-BFS with tree depth (D-BFS(α, β))

The D-BFS(α, β) node selection rule considers the lower bound, the diversity and the depth of a node with respect to
other solutions in the near-optimal set. This rule inputs a set of open nodes O and selects the next node i(cid:48) according to:

i(cid:48) ∈ arg min
i∈O

{(1 − α − β)Li + αDi + βHi} ,

where (1 − α − β), α, β form a convex combination and are the parameters that control the weight of the scaled lower
bound of node i, Li, the diversity score Di, and the scaled depth of node i, Hi. Like Li, Hi is scaled as:

Hi =

Depthi − M inP lungeDepth
M axP lungeDepth − M inP lungeDepth

,

where MinPlungeDepth and MaxPlungeDepth are set at the beginning of the computation and Depthi is the depth of
node i in the tree.

3.3 Diverse-BFS with solution cutoff (D-BFS(α, s))

The D-BFS(α, s) rule considers only the lower bound of a node until it has generated a small set of up to s solutions
prior to incorporating the diversity of a node with respect to the solutions already in the near-optimal set. This rule
selects the next node i(cid:48) according to:

(cid:40)

i(cid:48) ∈

arg mini∈O {Li}
arg mini∈O {(1 − α)Li + αDi}

number of solutions found so far<s,

otherwise,

where s is the solution cutoff parameter, that is, the number of solutions that must be accumulated before diversity is
considered in node selection and α ∈ [0, 1] is a parameter that trades off the bound of node i against its diversity score
Di.

8

DIVERSITREE

3.4 Diverse-BFS with depth cutoff (D-BFS(α, d))

The D-BFS(α, d) rule considers only the lower bound of a node until the depth of the active node reaches a depth d
prior to also considering the diversity of the node with respect to the solutions already in the near-optimal set. This rule
selects the next node i(cid:48) according to:

i(cid:48) ∈

(cid:40)

arg mini∈O {Li}
arg mini∈O {(1 − α)Li + αDi}

depth of nodes in current iteration < d,

otherwise,

where d is the depth cutoff parameter, that is, the depth that must be reached before diversity is considered in the node
selection and α ∈ [0, 1] is a parameter that trades off the bound of node i against its diversity score Di. Diversity is
only triggered upon reaching a depth of d or greater.

3.5 DIVERSITREE - Diverse-BFS with solution cutoff and tree depth (D-BFS(α, β, s))

DIVERSITREE selects the next node i(cid:48) according to:

(cid:40)

i ∈

arg mini∈O {Li}
arg mini∈O {(1 − α − β)Li + αDi + βHi}

number of solutions found so far < s,

otherwise,

where α, β, s are parameters as deﬁned in previous sections.

3.6 Other Diverse-BFS methods tested

We also tested several other methods for emphasizing solution set diversity. However, these additional methods were
not as effective as the methods described in §3.1 - §3.5 above. They are:

1. Using the minimum of diversity and depth, that is, select next node i(cid:48) according to:

2. Using the maximum of diversity and depth, that is, select next node i(cid:48) according to:

i(cid:48) ∈ arg min
i∈O

{(1 − α)Li + α(min(Di, Hi))} .

3. Using the product of diversity and depth, that is, select next node i(cid:48) according to:

i(cid:48) ∈ arg min
i∈O

{(1 − α)Li + α(max(Di, Hi)} .

i(cid:48) ∈ arg min
i∈O

{(1 − α)Li + αDiHi} .

These node selection rules were also tested with a nonzero solution cutoff parameter s, but were still not effective.

4 Computational Experiments.

To measure the effect of using diversity-emphasizing node selection rules on solution set diversity, we ran several sets
of experiments on selected problems from MIPLIB (Koch et al. 2011, Bixby et al. 1998, Gleixner et al. 2021) using
several different node selection rules, including the customized ones described in §3. We compared our approach with
two state-the-art methods: ONETREE (Danna et al. 2007) and BRANCH-AND-COUNT (Achterberg et al. 2008).

Experiments were run to answer the following questions:

9

Ahanor, Medal and Trapp

Research Question 1: Among common node selection rules (such as BestFS and DFS), do some produce a more diverse set of

near-optimal solutions than others? §4.2

Research Question 2: What are the best parameters to use for the parameterized diversity-emphasizing node selection rules

presented in §3? When using the best parameters, do the diversity-emphasizing node selection rules compute
solution sets that are more diverse than those computed by competing approaches? §4.3

Research Question 3: What parameters should be used when using diversity-emphasizing node selection rules on a new problem?

§4.4

4.1 Experimental Setup

All code was implemented in C++ using SCIP Optimization suite 7.0 (Gamrath et al. 2020) and run on a server running
Intel Xeon processors with sixteen cores and thirty two GB of memory. Apart from SCIP currently being one of the
fastest non-commercial solvers for mixed integer programming (MIP) and mixed integer nonlinear programming, it
provides a convenient way to use custom node selection rules. We evaluated our methods against the state-of-the-art
ONETREE method, implemented in GAMS-CPLEX.

4.2 Diversity of common node-selection rules

We ﬁrst examined the diversity produced by common node selection rules. For this initial set of experiments, we
selected seven problems from MIPLIB that had greater than 10,000 solutions within 1% of optimality (see Table 2 in
Appendix A).

We used the common node selection rules listed in §2.2 to request the generation of sets with sizes ranging from 50 to
2,000 near-optimal solutions, and computed the diversity. Figure 1 below shows the DBin diversity scores achieved by
these common node selection rules for different solution set sizes (p1). We labeled the different node selection rules
with a “BC” preﬁx to indicate the use of branch-and-count to generate the solution sets. As shown in Figure 1, the
best-ﬁrst search (BCBFS) method ﬁnds sets with the greatest diversity in 5 out of 7 test problems. The upper
conﬁdence bounds for trees (BCUCT) node selection rule had the best performance in 2 out of the 7 test problems and
achieved a similar diversity value to hybrid estimate (BCHE) in all other cases. The depth ﬁrst search (BCDFS) was
outperformed in terms of diversity in all of the instances.

The plots also indicate that diversity (averaged for each problem instance) tends to start low and become incrementally
higher as the number of requested solutions increases. We hypothesize that this may be due to a situation where as
more solutions are generated, more variables are ﬁxed or modiﬁed; increasing the Hamming distance from the ﬁrst
solution found. However, this trend did not hold for all of the individual problem instances.

10

DIVERSITREE

Figure 1: The ﬁnal DBin diversity score achieved by common node selection rules (BestFS, DFS,
UCT and HE) available on SCIP. We preﬁx each node selection rule name with "BC" to indicate the
use of the "branch-and-count" method. The BestFS node selection rule resulted in greater diversity
for a majority of problems.

The superior performance of BestFS over other common rules suggested its continued use for comparison in the
remainder of our experiments.

4.3 Parameter optimization for diversity-emphasizing rules: training set

The most general diversity-emphasizing node selection rules have up to three parameters to be tuned: α controls the
emphasis on diversity for the selected node, β controls the emphasis on depth within the search tree for the selected
node, and S controls the emphasis on the number of solutions generated before employing the α and β values in the
node selection process. We used a grid search to tune the parameters and ﬁnd the best-performing values. Toward this
end, we used several problem instances from MIPLIB (Gleixner et al. 2021, Koch et al. 2011, Bixby et al. 1998) and
randomly split the set of problems into a training and testing set using a 75:25 split. We use the training set problems to
ﬁnd the best parameters and then tested the performance of these parameters on the testing set problems. Using a
grid-search on a 20×20×20 grid over α, β, and S on [0, 1] with increment 0.01 on [0, 0.09] and 0.1 on [0.1, 1], we

11

Ahanor, Medal and Trapp

tested different parameters values across all training set problems shown in Table 2 in Appendix A for different
numbers (10, 50, 100, 200 and 1000) of requested near optimal solutions p1 and values of q (% near optimal) in the
range [0.01, 0.1] with increment 0.01. As a result, we obtained for each instance the best performing values of α, β,
and S over every p1 and q.

It is computationally impractical to run a grid search for every new problem instance prior to applying our node
selection rule. We therefore attempted to identify patterns in the mapping between problems and optimal parameter
values, so as to allow the identiﬁcation of a smaller number of parameter settings with good performance. Thus, our
next step was to determine whether we could group problems together based on their optimal parameter settings.
Toward this end, we ran a standard hierarchical clustering algorithm available in Python’s Scikit-Learn package
(Pedregosa et al. 2011) to cluster the problems into groups based on their best parameter settings, that is, the settings
that yielded the highest DBin scores for that problem. When the number of requested solutions is small (that is, 10),
the clustering algorithm found the following four groups:

1. High α, High S, Low β (HHL): α ≥ 0.9, S ≥ 0.7, β ≤ 0.2.

2. High α, Low S, Low β (HLL): α ≥ 0.9, S ≤ 0.2, β ≤ 0.2.

3. Low α, Low S, High β (LLH): α ≤ 0.2, S ≤ 0.2, β ≥ 0.8.

4. Low α, High S, High β (LHH): α ≤ 0.2, S ≥ 0.7, β ≥ 0.8.

In the HHL group, diversity is emphasized heavily (α ≥ 0.90), but only after a large number of solutions have been
accumulated (S ≥ 0.70). In the HLL group, diversity is also heavily emphasized, but starting after a moderate number
of solutions have been found (S ≤ 0.20). The LLH group tends to select solutions at greater depths in the tree
(β ≥ 0.80) after a small number of solutions have been accumulated (S ≤ 0.20). Finally, the LHH group emphasizes
diversity lightly (α ≤ 0.20) and mostly selects solutions at greater depths in the tree (β ≥ 0.80) after a large number of
solutions have been accumulated (S ≥ 0.70). The structure of these four groups indicates that our method is most
effective when we generate a small number of seed solutions and then emphasize diversity at greater depths in the tree.

To assess the efﬁcacy of DIVERSITREE, we then selected a single parameter setting for each of the four groups, as
shown in Table 1. Using the data from the grid search, we took the weighted average of the settings in a group as G1
and the settings that occurred the most frequently in that group as G2. We computed the diversity of the training set
problems using G1 and G2 separately and selected either G1 or G2 as the best setting for the group, depending on
which of the two produced the best diversity. We found that these parameter settings worked well for different values of
p1 and q.

Table 1: The α, β and S parameter settings used in both training and testing.

Number of
solutions
requested

HHL

HLL

LLH

LHH

10, 50, 100, 200, 1000 α :0.94,
β :0.06,
s :0.80

α :0.95,
β :0.06,
s :0.20

α :0.01,
β :0.99,
s :0.05

α :0.18,
β :0.8,
s :0.70

For each problem instance, we then ran DIVERSITREE using the parameters from its assigned group from the clustering
algorithm (see Figure 6 in the Appendix). The parameter settings in Table 1 were used for all values of p1 and q.
Speciﬁcally, we use the following two-phase approach. In phase one, we generate a larger solution set p1 with 10, 50,
100, 200, and 1000 solutions. In phase two, we then use a subset selection method (similar to local search algorithm in
Danna and Woodruff (2009)) to select p = 10 diverse solutions—a number that seems reasonable to present to a
decision maker. We ran this process for values of q from 1% to 10% across all the problems in the training set. We

12

DIVERSITREE

compare the performance of our method with two state-of-the-art methods: with the BCBFS rule, and with the
ONETREE method results reported in Danna and Woodruff (2009). These competing methods were used in phase one
of the two-phase approach, with the subset selection method being employed in phase two.

In Figure 2 and 3 we plot the percent improvement of the different groups of DIVERSITREE over ONETREE for values
of q from 1% to 10%, and p = 10 for different p1 values. In Figure 2, we show plots for p1 ≤ 100 and in Figure 3, we
show plots for p1 ≥ 200. As the ﬁgure shows, DIVERSITREE achieves an improvement in diversity over ONETREE of
up to 160% and at least 60%, irrespective of the four parameter settings used. Beyond outperforming ONETREE, all the
parameter groups also give similar performance on the average with no more than 10% gap between any group. Among
the DIVERSITREE results, parameter groups HHL and HLL appear to perform best. Thus, we recommend that when
using our method on a new problem either the HHL (α = 0.94, β = 0.06, s = 0.80) of HLL
(α = 0.95, β = 0.06, s = 0.20) parameter settings should be used. In the next section we test the HHL parameter
setting on a set of new problems that were not used for parameter tuning.

(a) p1 = 10, p = 10

(b) p1 = 50, p = 10

(c) p1 = 100, p = 10

Figure 2: This ﬁgure shows plots for p1 ≤ 100. The plots on the left shows the average percent improvement on diversity (DBin)
achieved by different parameter group settings of DIVERSITREE over ONETREE aggregated across all problems in the training set.
The run times are shown in the plots on the right. The runtime for DIVERSITREE were similar for the four different parameter
groups, hence this ﬁgure shows the average runtime over the four groups.

13

Ahanor, Medal and Trapp

(a) p1 = 200, p = 10

(b) p1 = 1000, p = 10

Figure 3: This ﬁgure shows plots for p1 ≥ 200. The plots on the left shows the average percent improvement on diversity (DBin)
achieved by different parameter group settings of DIVERSITREE over ONETREE aggregated across all problems in the training set.
The run times are shown in the plots on the right. The runtime for DIVERSITREE were similar for the four different parameter
groups, hence this ﬁgure shows the average runtime over the four groups.

4.4 Performance of tuned parameter groups on test set of problems

The results of the previous section raise the question of how parameters should be tuned on a new problem. To answer
this question, we tested our method using the HHL parameter setting, which was recommended in the previous section.
We test our method with the HHL setting on a test set of problems not used in parameter tuning (see Table 3 in
Appendix B). We ran the same two-phase approach for diverse, near-optimal solution generation as we did in the
training phase. We ran this process for p1 ∈ {10, 50, 100, 200, 1000} and q from 1% to 10%. We kept the size of the
ﬁnal solution set set to p = 10. Although the our method performed better than BCBFS1 and ONETREE for all four of
the parameter settings, we only present results for the recommend HHL setting.

We use the HHL parameter setting on the test data and the results achieved are shown in Figure 4 (for p1 ≤ 100) and 5
(for p1 ≥ 200), which displays results similar to those in Figures 2 and 3 for the training set. DIVERSITREE generated
solution sets with improvement over ONETREE of up to 139% and no worse than 32% irrespective of the four
parameter groups used. DIVERSITREE also signiﬁcantly outperforms BCBFS in terms of diversity for all values of p1,
as BCBFS generated solution sets with improvement of up to 36% and no worse than 0%. In terms of runtime,
DIVERSITREE achieves similar runtime as BCBFS, although the ONETREE method runs signiﬁcantly faster than both
methods, which is unsurprising given that while DIVERSITREE explicitly searches for solution, ONETREE only focuses
on near-optimality. Unlike the training set data where parameter groups HHL and HLL dominated the DIVERSITREE
results, the parameter groups LHH and LLH dominated the diversity results generated in the test set.

1In using BCBFS, we found that

the default parameters on SCIP (i.e., M IN P LU N GEDEP T H = −1,
M AXP LU N GEDEP T H = −1, and M AXP LU N GEQU OT = 0.25) performed poorly. Thus, we manually tuned these
settings to improve the diversity obtained by BCBFS. These tuned parameter settings were used in the results shown in the remainder
of the paper.

14

DIVERSITREE

In practice, a decision-maker seeking a diverse set of near-optimal solutions would need to deﬁne p (the number of
near-optimal solutions), q% (a bound on how far these solutions may be from optimality), and select a setting in any of
the groups we speciﬁed in §4.3 (without need for parameter tuning); p near-optimal solutions can then be directly
generated within q% of the optimal, or a two-phase approach can be used to generate the solution set. The result of our
tests suggest that the solution set generated would be no worse than BCBFS and would be signiﬁcantly better than the
state-of-the-art ONETREE method.

(a) p1 = 10, p = 10

(b) p1 = 50, p = 10

(c) p1 = 100, p = 10

Figure 4: This ﬁgure shows plots for p1 ≤ 100. Plots on the left show the percent improvement on diversity (DBin) achieved by
DIVERSITREE and BCBFS on the problems in the test set using the parameter setting from group HHL shown in Table 1. The plots
on the right show the runtime. The runtime for DIVERSITREE were similar for the four different parameter groups, hence this ﬁgure
shows the average runtime over the four groups.

15

Ahanor, Medal and Trapp

(a) p1 = 200, p = 10

(b) p1 = 1000, p = 10

Figure 5: This ﬁgure shows plots for p1 ≥ 200. Plots on the left show the percent improvement on diversity (DBin) achieved by
DIVERSITREE and BCBFS on the problems in the test set using the parameter setting from group HHL shown in Table 1. The plots
on the right show the runtime. The runtime for DIVERSITREE were similar for the four different parameter groups, hence this ﬁgure
shows the average runtime over the four groups.

5 Example: Railway Timetabling Problem

To provide a concrete example, we tested DIVERSITREE on a public transport scheduling problem within the
MIPLIB2017 set (Gleixner et al. 2021). Details about the problem formulation and variables is available in Liebchen
and Möhring (2003). Liebchen and Peeters (2002) discuss the general problem formulation. The problem models the
cyclic railway timetabling problem where we have information about a railway network in a graph representing its
infrastructure and trafﬁc line. Each trafﬁc line is operated every T time units and the goal of the problem is to
determine periodic departure times within the interval [0, T ) at every stop of every line. The problem has 397 variables;
77 of them are binary variables, 94 integer variables and 226 continous variables. The ﬁnal objective is to obtain an
arrival/departure timetable with minimal passenger and vehicle waiting times. The problem is represented as a graph
G = (N, A, l, u) with a set of nodes N representing a set of events, and arcs A representing set of trains. The weights
on the arcs represent the time when event vi ∈ N occurs and [l, u] the allowable time interval for this event. The events
are represented as triplets (train,node,arrival) or (train,node,departure) and the binary variable bij is 1 if arc (i, j) is
selected. The model is mainly based on periodic constraints, which relate the arrival and departure time variables
through the time window [l, u] within which the event i must occur.

For problems such as these, the solutions provided by the MIP model will be reviewed by a decision maker prior to
implementation. Thus, in this context it can be useful to provide the decision maker with a set of near-optimal solutions
from which to choose. This will allow the decision maker to consider not only the cost of the solution but also other
factors such as variability in estimated demands, crew shortages, behavior of new equipment, or propagated delays,
without having to re-solve the problem. For q = 1%, the problem had 181 near-optimal solutions, and when q increased
to 3%, the number of near-optimal solutions increased to 6,264.

We ran the DIVERSITREE and ONETREE algorithms on this problem for p1 = 10, p = 10 and q = 3%. DIVERSITREE
computed a solution set with a DBin value of 0.299, while ONETREE’s set has a DBin value of 0.044. A further

16

DIVERSITREE

review of the generated solutions shows that of the 77 binary variables, ONETREE gave a solution set in which 68 of
the binary variables had the same value in all ten solutions. In addition, ONETREE generated a set of ten solutions of
which only six solutions were unique. Among the six unique solutions, ﬁve of them had at most two variables with
different values indicating very close similarity of the solutions. In constrast, DIVERSITREE’s gave a solution set in
which only 23 of the variables were the same in all ten solutions, all ten solutions were unique and at least nine
variables had different values among the solutions, indicating the diversity of the solutions. This experiment provides
further evidence supporting that the DIVERSITREE approach provides a much more diverse set of train schedules from
which a decision maker may select.

6 Conclusion

We introduce a novel approach to generate diverse, near-optimal solution sets by emphasizing solution diversity in the
node selection step of a branch-and-bound algorithm. Our results reveal that our modiﬁed node selection rule yields up
to 140% better diversity than known methods for generating diverse solutions. Our methods provide a fast way to
generate diverse sets of near-optimal solutions, useful where there is utility in having a diverse set of options for
decision making. We presented several methods for emphasizing diversity in node selection rules and optimal
parameters that yield the most diverse set of solutions for the problems tested. We identiﬁed four groups of optimal
parameters for different problems by clustering the parameter groups giving the best diversity (DBin) on the training
set. Each group sets an optimal solution cutoff value and also sets how much diversity and depth to consider in
determining the next solution added to the set of diverse solutions. When tested on new problems, our method (using
the identiﬁed optimal parameters) ran in similar time as regular node selection rules and gave solution pools that were
signiﬁcantly more diverse.

A positive result of our study was that the four parameter setting groups identiﬁed during the parameter tuning on the
training set performed well on a previously unseen set of problems in a test set. Even so, understanding the relationship
between parameter settings and different classes of problems such that we can select the best parameter for any problem
with a minimal amount of tuning would be beneﬁcial in practice. A second area of interest which is also an extension of
this work is diversifying solution pools on select variables (see Voll et al. (2015)). In cases where a cluster of variables
represents an attribute that is desirable in a machine learning algorithm (such as fairness or intelligence), it may be
useful and informative to generate solutions pools that are diverse on only that attribute or interpret and cluster the
generated solutions based the attribute. Finally, we only reported results using DBin as a diversity metric. It would be
useful to understand how the new node selection method performs when using other metrics for computing diversity.

Acknowledgments

Medal gratefully acknowledges funding from the Army Research Ofﬁce (grant W911NF-21-1-0079). However, the
views expressed in this article do no represent those of the US Government, the US Department of Defense, or the US
Army.

References

Tobias Achterberg, Stefan Heinz, and Thorsten Koch. Counting solutions of integer programs using unrestricted subtree detection.
In International Conference on Integration of Artiﬁcial Intelligence (AI) and Operations Research (OR) Techniques in
Constraint Programming, Lecture Notes in Computer Science, pages 278–282, 2008. ISBN 9783540681540. doi:
10.1007/978-3-540-68155-7\_22.

Julien Baste, Michael R Fellows, Lars Jaffke, Tomáš Masaˇrík, Mateus de Oliveira Oliveira, Geevarghese Philip, and Frances A
Rosamond. Diversity of solutions: An exploration through the lens of ﬁxed-parameter tractability theory. Artiﬁcial
Intelligence, 303:103644, 2022.

Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern optimization lens. The annals of

statistics, 44(2):813–852, 2016.

17

Ahanor, Medal and Trapp

R. E. Bixby, S. Ceria, C. M. McZeal, and M. W. P Savelsbergh. An updated mixed integer programming library: MIPLIB 3.0.

Optima, 58:12–15, 1998.

Niclas Boehmer and Rolf Niedermeier. Broadening the research agenda for computational social choice: Multiple preference

proﬁles and multiple solutions. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent
Systems, pages 1–5, 2021.

Qi Cai, Linwei He, Yimin Xie, Ruoqiang Feng, and Jiaming Ma. Simple and effective strategies to generate diverse designs for truss

structures. Structures, 32:268–278, 2021. ISSN 2352-0124. doi: 10.1016/j.istruc.2021.03.010.

Thomas R Cameron, Sebastian Charmot, and Jonad Pulaj. On the linear ordering problem and the rankability of data. arXiv preprint

arXiv:2104.05816, 2021.

Sebastian Ceria, Cecile Cordier, Hugues Marchand, and Laurence A Wolsey. Cutting planes for integer programs with general

integer variables. Mathematical programming, 81(2):201–214, 1998.

Richard L Church and Carlos A Baez. Generating optimal and near-optimal solutions to facility location problems. Environment and
Planning B: Urban Analytics and City Science, 47(6):1014–1030, 2020. ISSN 2399-8083. doi: 10.1177/2399808320930241.

Emilie Danna and David L Woodruff. How to select a small set of diverse solutions to mixed integer programming problems.

Operations Research Letters, 37(4):255–260, 2009.

Emilie Danna, Mary Fenelon, Zonghao Gu, and Roland Wunderling. Generating multiple solutions for mixed integer programming
problems. In International Conference on Integer Programming and Combinatorial Optimization, pages 280–294. Springer,
2007.

Jonathan Eckstein. Parallel branch-and-bound methods for mixed integer programming. In Applications on Advanced Architecture

Computers, pages 141–153. SIAM, 1996.

Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward

function. arXiv preprint arXiv:1802.06070, 2018.

Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eiﬂer, Maxime Gasse, Patrick Gemander, Ambros
Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J.
Maher, Frederic Matter, Matthias Miltenberger, Erik Mühmer, Benjamin Müller, Marc E. Pfetsch, Franziska Schlösser, Felipe
Serrano, Yuji Shinano, Christine Tawﬁk, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob Witzig. The SCIP
Optimization Suite 7.0. Technical report, Optimization Online, March 2020. URL
http://www.optimization-online.org/DB_HTML/2020/03/7705.html.

Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe, Timo Berthold, Philipp M. Christophel,
Kati Jarck, Thorsten Koch, Jeff Linderoth, Marco Lübbecke, Hans D. Mittelmann, Derya Ozyurt, Ted K. Ralphs, Domenico
Salvagnin, and Yuji Shinano. MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming Library.
Mathematical Programming Computation, 2021. doi: 10.1007/s12532-020-00194-3. URL
https://doi.org/10.1007/s12532-020-00194-3.

Fred Glover, Ching-Chung Kuo, and Krishna S. Dhir. Heuristic algorithms for the maximum diversity problem. Journal of

Information and Optimization Sciences, 19(1):109–132, 1998. ISSN 0252-2667. doi: 10.1080/02522667.1998.10699366.

Fred Glover, Arne Løkketangen, and David L Woodruff. Scatter search to generate diverse MIP solutions. In Computing Tools for

Modeling, Optimization and Simulation, pages 299–317. Springer, 2000.

Peter Greistorfer, Arne Løkketangen, Stefan Voß, and David L. Woodruff. Experiments concerning sequential versus simultaneous
maximization of objective function and distance. Journal of Heuristics, 14(6):613–625, 2008. ISSN 1381-1231. doi:
10.1007/s10732-007-9053-z.

Yunzhen He, Kun Cai, Zi-Long Zhao, and Yi Min Xie. Stochastic approaches to generating diverse and competitive structural

designs in topology optimization. Finite Elements in Analysis and Design, 173:103399, 2020. ISSN 0168-874X. doi:
10.1016/j.ﬁnel.2020.103399.

V Roshan Joseph, Tirthankar Dasgupta, Rui Tuo, and CF Jeff Wu. Sequential exploration of complex surfaces using minimum

energy designs. Technometrics, 57(1):64–74, 2015.

Thorsten Koch, Tobias Achterberg, Erling Andersen, Oliver Bastert, Timo Berthold, Robert E. Bixby, Emilie Danna, Gerald

Gamrath, Ambros M. Gleixner, Stefan Heinz, Andrea Lodi, Hans Mittelmann, Ted Ralphs, Domenico Salvagnin, Daniel E.
Steffy, and Kati Wolter. MIPLIB 2010. Mathematical Programming Computation, 3(2):103–163, 2011. doi:
10.1007/s12532-011-0025-9. URL http://mpc.zib.de/index.php/MPC/article/view/56/28.

18

DIVERSITREE

Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shot extrapolation via

structured MaxEnt RL. Advances in Neural Information Processing Systems, 33, 2020.

Ching-Chung Kuo, Fred Glover, and Krishna S Dhir. Analyzing and modeling the maximum diversity problem by zero-one

programming. Decision Sciences, 24(6):1171–1185, 1993.

Michael Lavine. Whim: function approximation where it matters. Communications in Statistics-Simulation and Computation, pages

1–31, 2019.

Sangbum Lee, Chan Phalakornkule, Michael M Domach, and Ignacio E Grossmann. Recursive milp model for ﬁnding all the

alternate optima in lp models for metabolic networks. Computers & Chemical Engineering, 24(2-7):711–716, 2000.

Christian Liebchen and Rolf H Möhring. Information on MIPLIB’s timetab-instances. 2003.

Christian Liebchen and Leon Peeters. On cyclic timetabling and cycles in graphs. 2002.

Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,

J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

Thierry Petit and Andrew C Trapp. Finding diverse solutions of high quality to constraint optimization problems. In Twenty-Fourth

International Joint Conference on Artiﬁcial Intelligence, 2015.

Thierry Petit and Andrew C Trapp. Enriching solutions to combinatorial problems via solution engineering. INFORMS Journal on

Computing, 31(3):429–444, 2019.

Pablo Rodríguez-Mier, Nathalie Poupin, Carlo de Blasio, Laurent Le Cam, and Fabien Jourdan. DEXOM: Diversity-based

enumeration of optimal context-speciﬁc metabolic networks. PLOS Computational Biology, 17(2):e1008730, 2021. ISSN
1553-734X. doi: 10.1371/journal.pcbi.1008730.

Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning:

Fundamental principles and 10 grand challenges. Statistics Surveys, 16:1–85, 2022.

Patrick Schittekat and Kenneth Sörensen. OR practice—supporting 3PL decisions in the automotive industry by generating diverse

solutions to a large-scale location-routing problem. Operations Research, 57(5):1058–1067, 2009.

Nicolas Schwind et al. Representative solutions for bi-objective optimisation. In Proceedings of the AAAI Conference on Artiﬁcial

Intelligence, volume 34, pages 1436–1443, 2020.

Lesia Semenova, Cynthia Rudin, and Ronald Parr. A study in Rashomon curves and volumes: A new perspective on generalization

and model simplicity in machine learning. arXiv preprint arXiv:1908.01755, 2019.

Thiago Serra. Enumerative branching with less repetition. In International Conference on Integration of Constraint Programming,

Artiﬁcial Intelligence, and Operations Research, Lecture Notes in Computer Science, pages 399–416, 2020. ISBN
9783030589417. doi: 10.1007/978-3-030-58942-4\_26.

Thiago Serra and J. N. Hooker. Compact representation of near-optimal integer programming solutions. Mathematical

Programming, 182(1-2):199–232, 2020. ISSN 0025-5610. doi: 10.1007/s10107-019-01390-3.

Geiza C Silva, Marcos RQ De Andrade, Luiz S Ochi, Simone L Martins, and Alexandre Plastino. New heuristics for the maximum

diversity problem. Journal of Heuristics, 13(4):315–336, 2007.

Andrew C Trapp and Renata A Konrad. Finding diverse optima and near-optima to binary integer programs. IIE Transactions, 47

(11):1300–1312, 2015.

Rodothea Myrsini Tsoupidi, Roberto Castañeda Lozano, and Benoit Baudry. Principles and Practice of Constraint Programming,
26th International Conference, CP 2020, Louvain-la-Neuve, Belgium, September 7–11, 2020, Proceedings. Lecture Notes in
Computer Science, pages 791–808, 2020. ISSN 0302-9743. doi: 10.1007/978-3-030-58475-7\_46.

Pascal Van Hentenryck, Carleton Coffrin, and Boris Gutkovich. Constraint-based local search for the automatic generation of
architectural tests. In International Conference on Principles and Practice of Constraint Programming, pages 787–801.
Springer, 2009.

Philip Voll, Mark Jennings, Maike Hennen, Nilay Shah, and André Bardow. The optimum is not enough: A near-optimal solution

paradigm for energy systems synthesis. Energy, 82:446–456, 2015.

Bo Wang, Yan Zhou, Yiming Zhou, Shengli Xu, and Bin Niu. Diverse competitive design for topology optimization. Structural and

Multidisciplinary Optimization, 57(2):891–902, 2018.

19

Ahanor, Medal and Trapp

Tom Zahavy, Brendan O’Donoghue, Andre Barreto, Volodymyr Mnih, Sebastian Flennerhag, and Satinder Singh. Discovering

diverse nearly optimal policies with successor features. arXiv preprint arXiv:2106.00669, 2021.

Elena Zaslavsky and Mona Singh. A combinatorial optimization approach for diverse motif ﬁnding applications. Algorithms for

Molecular Biology, 1(1):1–13, 2006. doi: 10.1186/1748-7188-1-13.

Yiming Zhou, Raphael T. Haftka, and Gengdong Cheng. Balancing diversity and performance in global optimization. Structural and

Multidisciplinary Optimization, 54(4):1093–1105, 2016. ISSN 1615-147X. doi: 10.1007/s00158-016-1434-1.

20

DIVERSITREE

Appendix A Problems in the training set

Table 2 captures the problems we solved in the training set. They are randomly selected problems from MIPLIB
(Gleixner et al. (2021), Koch et al. (2011), Bixby et al. (1998)). We did remove instances that did not complete
computation of the objective value within 30 minutes. In total, 27 problem instances are used in this training set. We
capture the characteristics of the problem below.

Table 2: The problem instances used for training in §4.2 and §4.3 and their characteristics are shown in this table.

Problem Instance

air03

bell5

dcmulti

ﬁber

ﬁxnet6

gen

gesa3

gt2

khb05250

l152lav

misc03

misc06

mod008

mod010

p0033

p0201

p0548

pp08a

pp08aCUTS

qnet1o

qnet1

rgn

set1ch

stein27

stein45

vpm1

vpm2

Total
variables

10757

104

548

1298

878

870

1152

188

1350

1989

160

1808

319

2655

33

201

548

240

240

1541

1541

180

712

27

45

378

378

Binary
variables

10757

30

75

1254

378

144

216

24

24

1989

159

112

319

2655

33

201

548

64

64

1288

1288

100

240

0

0

168

168

General
integer
variables

Continuous
ables

vari-

Solutions within 1%
of the optimum

0

46

473

44

500

720

768

0

1326

0

1

1696

0

0

0

0

0

176

176

124

124

90

472

0

0

210

210

938

>10,000

>10,000

>279

>10,000

>10,000

>10,000

>10,000

28

>10,000

24

>10,000

68

>10,000

15

44

>10,000

64

64

>10,000

>10,000

>720

>10,000

2106

70

>10,000

33

0

28

0

0

0

6

168

164

0

0

0

0

0

0

0

0

0

0

0

129

129

0

0

0

0

0

0

21

Ahanor, Medal and Trapp

Appendix B Problems in the testing set

Table 3 captures the problems we solved in the testing set. They are randomly selected problems from MIPLIB
(Gleixner et al. (2021)). We did remove instances that did not complete computation of the objective value within 30
minutes. In total, 9 problem instances are used in this training set. We capture the characteristics of the problem below.

Table 3: The problem instances used for testing in §4.2 and §4.3 and their characteristics are shown in this table.

Problem Instance

Total
variables

Binary
variables

General
integer
variables

Continuous
ables

vari-

Solutions within 1%
of the optimum

23588

bppc8-02

exp-1-500-5-5

368

232

990

mtest4ma

1950

neos-1425699

neos17

nexp-50-20-1-1

sp150x300d

105

535

490

600

231

229

250

975

5

300

245

300

0

1

0

0

80

0

0

0

137

2

740

975

20

235

245

300

82

>10,000

>1,338

>10,000

>10,000

>10,000

>10,000

>9,455

Appendix C Parameter groups for diversity-emphasizing rules

Figure 6 shows the clustering of the problem instances into the four groups. As the ﬁgure shows, these four groups
consolidate to three (HHL, HLL, and LLH) when the number of requested solutions reaches 50 and to two (HHL and
HLL) for 200 requested solutions or more. The consolidation to groups HHL and HLL indicate that emphasizing
diversity after generating a seed set of solutions results in a higher overall diversity of all generated solutions.

One might suspect that two instances of the same problem (e.g., stein27 and stein45) would typically have the same
parameter group. However, surprisingly we found that similar problems like stein27/stein45 and qnet1/qnet1_0 did not
always belong to the same group. At p1 ≥ 50, some problems took too long to complete all test cases in the grid search
and thus were excluded from the parameter groups shown in Figure 6.

Figure 6: The four parameter groups and the problem instances in each group as the number of
requested solutions increases from 10 to 1,000.

22

