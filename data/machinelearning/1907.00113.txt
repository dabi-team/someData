Learning Markov models via low-rank optimization

Ziwei Zhu
Department of Statistics, University of Michigan, Ann Arbor, MI 48109, ziweiz@umich.edu

Xudong Li
School of Data Science, Fudan University, Shanghai 200433, lixudong@fudan.edu.cn
Shanghai Center for Mathematical Sciences, Fudan University, Shanghai 200433

Mengdi Wang
Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544,
mengdiw@princeton.edu

Anru R. Zhang
Department of Statistics, University of Wisconsin-Madison, Madison, WI 53706, anruzhang@stat.wisc.edu
Department of Biostatistics and Bioinformatics, Duke University, 27710

Modeling unknown systems from data is a precursor of system optimization and sequential decision making.

In this paper, we focus on learning a Markov model from a single trajectory of states. Suppose that the

transition model has a small rank despite of having a large state space, meaning that the system admits a

low-dimensional latent structure. We show that one can estimate the full transition model accurately using a

trajectory of length that is proportional to the total number of states. We propose two maximum likelihood

estimation methods: a convex approach with nuclear-norm regularization and a nonconvex approach with

rank constraint. We explicitly derive the statistical rates of both estimators in terms of the Kullback-Leiber

divergence and the (cid:96)2 error and also establish a minimax lower bound to assess the tightness of these rates.

For computing the nonconvex estimator, we develop a novel DC (diﬀerence of convex function) programming

algorithm that starts with the convex M-estimator and then successively reﬁnes the solution till convergence.

Empirical experiments demonstrate consistent superiority of the nonconvex estimator over the convex one.

Key words : Markov Model, DC-programming, Non-convex Optimization, Rank Constrained Likelihood

0
2
0
2

v
o
N
6
2

]
E
M

.
t
a
t
s
[

2
v
3
1
1
0
0
.
7
0
9
1
:
v
i
X
r
a

1. Introduction

In engineering and management applications, one often has to collect data from unknown systems,

learn their transition functions, and learn to make predictions and decisions. A critical precursor

of decision making is to model the system from data. We study how to learn an unknown Markov

1

 
 
 
 
 
 
2

Zhu et al.: Estimation of Markov Models

model of the system from its state-transition trajectories. When the system admits a large number

of states, recovering the full model becomes sample expensive.

In this paper, we focus on Markov processes where the transition matrix has a small rank. The

small rank implies that the observed process is governed by a low-dimensional latent process which

we cannot see in a straightforward manner. It is a property that is (approximately) satisﬁed in

a wide range of practical systems. Despite the large state space, the low-rank property unlocks

potential of accurate learning of a full set of transition density functions based on short empirical

trajectories.

1.1. Motivating Examples

Practical state-transition processes with a large number of states often exhibit low-rank structures.

For example, the sequence of stops made by a taxi turns out to follow a Markov model with

approximately low rank structure (Liu et al. 2012, Benson et al. 2017). For another example,

random walk on a lumpable network has a low-rank transition matrix (Buchholz 1994, E et al.

2008). The transition kernel with fast decaying eigenvalues has been also observed in molecular

dynamics (Rohrdanz et al. 2011), which can be used to ﬁnd metastable states, coresets and manifold

structures of complicated dynamics (Chodera et al. 2007, Coifman et al. 2008).

Low-rank Markov models are also related to dimension reduction for control systems and rein-

forcement learning. For example, the state aggregation approach for modeling a high-dimensional

system can be viewed as a low-rank approximation approach (Bertsekas 1995, Bertsekas and Tsit-

siklis 1995, Singh et al. 1995). In state aggregation, one assumes that there exists a latent stochastic

process {zt} ⊂ [r] such that P(st+1 | st) = (cid:80)

z

P(zt = z | st)P(st+1 | zt = z), which is equivalent to a

factorization model of the transition kernel P. In the context of reinforcement learning, the non-

negative factorization model was referred to as the generalization to the rich-observation model

(Azizzadenesheli et al. 2016). The low-rank structure allows us to model and optimize the system

using signiﬁcantly fewer observations and less computation. Eﬀective methods for estimating the

low-rank Markov model would pave the way to better understanding of process data and more

eﬃcient decision making.

Zhu et al.: Estimation of Markov Models

1.2. Our approach

3

We propose to estimate the low-rank Markov model based on an empirical trajectory of states,

whose length is only proportional to the total number of states. We propose two approaches

based on the maximum likelihood principle and low-rank optimization. The ﬁrst approach uses

a convex nuclear-norm regularizer to enforce the low-rank structure and a polyhedral constraint

to ensure that optimization is over all probabilistic matrices. The second approach is to solve a

rank-constrained optimization problem using diﬀerence-of-convex (DC) programming. For both

approaches, we provide statistical upper bounds for the Kullback-Leibler (KL) divergence between

the estimator and the true transition matrix as well as the (cid:96)2 risk. We also provide an information-

theoretic lower bound to show that the proposed estimators are nearly rate-optimal. Note that

the low-rank estimation of the Markov model was considered in Zhang and Wang (2017) where a

spectral method with total variation bound is given. In comparison, the novelty of our methods lies

in the use of maximum likelihood principle and low-rank optimization, which allows us to obtain

the ﬁrst KL divergence bound for learning low-rank Markov models.

Our second approach involves solving a rank constraint optimization problem over probabilistic

matrices, which is a reﬁnement of the convex nuclear-norm approach. Due to the non-convex rank

constraint, the optimization problem is diﬃcult - to the best of our knowledge, there is no eﬃcient

approach that directly solves the rank-constraint problem. In this paper, we develop a penalty

approach to relax the rank constraint and transform the original problem into a DC (diﬀerence

of convex functions) programming one. Furthermore, we develop a particular DC algorithm to

solve the problem by initiating at the solution to the convex problem and successively reﬁning the

solution through solving a sequence of inner subproblems. Each subroutine is based on the multi-

block alternating direction method of multipliers (ADMM). Empirical experiments show that the

successive reﬁnements through DC programming do improve the learning quality. As a byproduct

of this research, we develop a new class of DC algorithms and a uniﬁed convergence analysis for

solving non-convex non-smooth problems, which were not available in the literature to our best

knowledge.

4

Zhu et al.: Estimation of Markov Models

1.3. Contributions and paper outline

The paper provides a full set of solutions for learning low-rank Markov models. The main contribu-

tions are: (1) We develop statistical methods for learning low-rank Markov model with rate-optimal

Kullback-Leiber divergence guarantee for the ﬁrst time; (2) We develop low-rank optimization

methods that are tailored to the computation problems for nuclear-norm regularized and rank-

constrained M-estimation; (3) A byproduct is a generalized DC algorithm that applies to nons-

mooth nonconvex optimization with convergence guarantee.

The rest of the paper is organized as follows. Section 2 surveys related literature. Section 3

proposes two maximum likelihood estimators based on low-rank optimization and establishes their

statistical properties. Section 4 develops computation methods and establishes convergence of the

methods. Section 5 presents the results of our numerical experiments.

2. Related literature

Model reduction for complicated systems has a long history. It traces back to variable-resolution

dynamic programming (Moore 1991) and state aggregation for decision process (Sutton and Barto

1998). In the case of Markov process, (Deng et al. 2011, Deng and Huang 2012) considered low-

rank reduction of Markov models with explicitly known transition probability matrix, but not the

estimation of the reduced models. Low-rank matrix approximation has been proved powerful in

analysis of large-scale panel data, with numerous applications including network analysis (E et al.

2008), community detection (Newman 2013), ranking (Negahban et al. 2016), product recommen-

dation (Keshavan et al. 2010) and many more. The main goal is to impute corrupted or missing

entries of a large data matrix. Statistical theory and computation methods are well understood in

the settings where a low-rank signal matrix is corrupted with independent Gaussian noise or its

entries are missed independently.

In contrast, our problem is to estimate the transition density functions from dependent state

trajectories, where statistical theory and eﬃcient methods are underdeveloped. When the Markov

model has rank 1, it becomes an independent process. In this case, our problem reduces to estima-

tion of a discrete distribution from independent samples (Steinhaus 1957, Lehmann and Casella

Zhu et al.: Estimation of Markov Models

5

2006, Han et al. 2015). For a rank-2 transition matrix, Huang et al. (2016) proposed an estimation

method using a small number of independent samples. Very recently there have been some works

on minimax learning of Markov chains. Hao et al. (2018) derived the minimax rates of estimat-

ing a Markov model in terms of a smooth class of f -divergences. They considered the family of

α-minorated Markov chains, i.e., all the transition probabilities are greater than α. Wolfer and

Kontorovich (2019b) computed the ﬁnite-sample PAC-type minimax sample complexity of recov-

ering the transition matrix from a state trajectory of a Markov chain, up to a tolerance in a

total-variation-based (TV-based) metric. This TV-based metric does not belong to the family of

the smooth f -divergences in Hao et al. (2018), and their class of Markov models strictly contains

the class of the α-minorated ones. Neither of these works considered low-rank Markov models

though.

The closest work to ours is Zhang and Wang (2017), in which a spectral method via truncated

singular value decomposition was introduced and the upper and lower error bounds in terms of

total variation were established. Yang et al. (2017) developed an online stochastic gradient method

for computing the leading singular space of a transition matrix from random walk data. To our

best knowledge, none of the existing works has analyzed eﬃcient recovery of a low-rank Markov

model with Kullback-Leiber divergence guarantee.

Hidden Markov Models (HMMs) are closely related with our low-rank Markov models. Note that

the observation trajectory of an HMM is not necessarily Markovian. Therefore, an HMM can be

regarded as a relaxed variant of low-rank Markov models. There have been many works on estimat-

ing HMM, in particular through spectral approaches, e.g., Hsu et al. (2012) and Anandkumar et al.

(2014). A critical diﬀerence is: States are not fully observable in HMM, but are fully observable

in low-rank Markov models. Although HMM is more general, the low-rank Markov model is more

suitable for dynamical processes where the state space is large but fully observable, for which we

will establish tighter error bounds.

On the optimization side, we adopt DC programming to handle the rank constraint and replace

it with the diﬀerence of two convex functions. DC programming was ﬁrst introduced by Pham Dinh

6

Zhu et al.: Estimation of Markov Models

and Le Thi (1997) and has become a prominent tool for handling a class of nonconvex optimization

problems (see also Pham Dinh and Le Thi (2005), Le Thi et al. (2012, 2017), Le Thi and Pham Dinh

(2018)). In particular, Van Dinh et al. (2015) and Wen et al. (2017) considered the majorized

DC algorithm, which motivated the optimization method developed in this paper. However, both

Van Dinh et al. (2015) and Wen et al. (2017) used the majorization technique with restricted choices

of majorants, and neither considered the introduction of the indeﬁnite proximal terms. In addition,

Wen et al. (2017) further assumes the smooth part in the objective to be convex. In comparison

with the existing methods, our DC programming method applies to nonsmooth problems and is

compatible with a more ﬂexible and possibly indeﬁnite proximal term.

Finally, we would like to mention the probabilistic tools we used to derive the statistical results.

Recent years have witnessed many works on measure concentration of dependent random variables,

e.g., Marton (1996), Kontorovich (2007), Kontorovich and Ramanan (2008), Paulin (2015), Jiang

et al. (2018), etc. Nevertheless, these results do not suﬃce to establish the desired statistical

guarantee, because exploiting low-rank structure requires studying the concentration of a matrix

martingale in terms of the spectral norm, as shown in Lemma 2. The matrix Freedman inequality

(Tropp 2011, Corollary 1.3) turns out to be the right tool for analyzing the concentration of the

matrix martingale. We also used an variant of Bernstein’s inequality for general Markov chains

(Jiang et al. 2018, Theorem 1.2) to derive an exponential tail bound for the status counts of the

Markov chain X .

3. Minimax rate-optimal estimation of low-rank Markov chains

Consider an ergodic Markov chain X = {X0, X1, . . . , Xn} on p states S = {sj}p

j=1 with the transi-

tion probability matrix P ∈ Rp×p and stationary distribution π, where Pij = P(X1 = sj|X0 = si) for

any i, j ∈ [p]. Let πmin := minj∈[p] πj and πmax := maxj∈[p] πj. We quantify the distance between two
transition matrices P and (cid:98)P in Frobenius norm (cid:107) (cid:98)P − P(cid:107)F = (cid:8)(cid:80)p
Leibler divergence DKL(P, (cid:98)P) = (cid:80)p

i,j=1 πiPij log(Pij/ (cid:98)Pij)1{Pij (cid:54)=0}. Suppose that the unknown tran-

i,j=1( (cid:98)Pij − Pij)2(cid:9)1/2

and Kullback–

sition matrix P has a small rank r (cid:28) p. Our goal is to estimate the transition matrix P via a state

trajectory of length n.

Zhu et al.: Estimation of Markov Models

7

3.1. Spectral gap of nonreversible Markov chains

We ﬁrst introduce the right L2-spectral gap of P (Fill 1991, Jiang et al. 2018), a quantity that

determines the convergence speed of the Markov chain X to its invariant distribution π. Let

L2(π) := {h ∈ (cid:60)p : (cid:80)

j∈[p] h2

j πj < ∞} be a Hilbert space endowed with the following inner product:

(cid:104)h1, h2(cid:105)π :=

(cid:88)

j∈[p]

h1jh2jπj.

The matrix P induces a linear operator on L2(π): h (cid:55)→ Ph, which we abuse P to denote. Let P∗

be the adjoint operator of P with respect to L2(π):

P∗ = Diag(π)−1P(cid:62) Diag(π).

Note that the following four statements are equivalent: (a) P is self-adjoint; (b) P∗ = P; (c)

the detailed balance condition holds: πiPij = πjPji; (d) the Markov chain is reversible. In our

analysis, we do not require the Markov chain to be reversible. We therefore introduce the additive

reversiblization of P: (P + P∗)/2, which is a self-adjoint operator on L2(π) and has the largest

eigenvalue as 1. The right spectral gap of P is deﬁned as follows:

Definition 1 (Right L2-spectral gap). We say the right L2-spectral gap of P is 1 − ρ+ if

ρ+ :=

sup
(cid:104)h,1(cid:105)π =0,(cid:104)h,h(cid:105)π =1

1
2

(cid:104)(P + P∗)h, h(cid:105)π < 1,

where 1 in (cid:104)h, 1(cid:105) refers to the all-one p-dimensional vector.

Deﬁne the (cid:15)-mixing time of the Markov chain X as

τ ((cid:15)) := min{t : max
j∈[p]

(cid:107)(Pt)j· − π(cid:107)TV ≤ (cid:15)},

where (cid:107)(Pt)j· − π(cid:107)TV := 2−1(cid:107)(Pt)j· − π(cid:107)1 is the total variation distance between Pt

j· and π. For

reversible and ergodic Markov chains, Levin and Peres (2017, Theorem 12.3) show that

τ ((cid:15)) ≤

1
1 − ρ+

log

(cid:18) 1

(cid:15)πmin

(cid:19)

,

(1)

which implies that the larger the spectral gap is, the faster the Markov chain converges to the

stationary distribution.

8

Zhu et al.: Estimation of Markov Models

3.2. Estimation methods and statistical results

Now we are in position to present our methods and statistical results. Given the trajectory

{X1, . . . , Xn}, we count the number of times that the state si transitions to sj:

nij := |{1 ≤ k ≤ n | Xk−1 = si, Xk = sj}| .

Let ni := (cid:80)p

j=1 nij for i = 1, . . . , p and n := (cid:80)p

i=1 ni. The averaged negative log-likelihood function

of P based on the state-transition trajectory {x0, . . . , xn} is

(cid:96)n(P) := −

1
n

n
(cid:88)

k=1

log((cid:104)P, Xk(cid:105)) = −

1
n

p
(cid:88)

p
(cid:88)

i=1

j=1

nij log(Pij),

(2)

where Xk := eie(cid:62)

j ∈ (cid:60)p×p if xk = si and xk+1 = sj. We ﬁrst impose the following assumptions on P

and π.

Assumption 1. (i) rank(P) = r; (ii) There exist some positive constants α, β > 0 such that for

any 1 ≤ j, k ≤ p, Pjk ∈ {0} ∪ [α/p, β/p].

Remark 1. The entrywise constraints on P are imposed by our theoretical analysis and may not

be necessary in practice. Speciﬁcally, the upper and lower bounds for the nonzero entries of P

ensure that (i) the gradient of the log-likelihood ∇(cid:96)n(P) is well controlled and exhibits exponential

concentration around its population mean (see (EC.9) for the reason we need α there); (ii) the

converter between the (cid:96)2-risk (cid:107) (cid:98)P − P(cid:107)F ((cid:107) (cid:98)Pr − P(cid:107)F resp.) and the KL-divergence DKL(P, (cid:98)P)

(DKL(P, (cid:98)Pr) resp.) depends on α and β, as per Lemma EC.1. The entry-wise upper and lower

bounds are common in statistical analysis of count data, e.g., Poisson matrix completion (Cao

and Xie 2016, Equation (10)), Poisson sparse regression (Jiang et al. 2015, Assumption 2.1), point

autoregressive model (Hall et al. 2016, Deﬁnition of As), etc.

Remark 2. If we remove 0 in the feasible range of Pjk, we obtain the (α/p)-minoration condition:

Pjk ≥ α/p for all j, k ∈ [p]. The (α/p)-minoration condition implies strong mixing since combining

Br´emaud (1999, pp. 237-238) and Kontorovich (2007, Lemma 2.2.2) yields 1 − ρ+ ≥ α and we can

deduce that τ ((cid:15)) ≤ α−1 log{((cid:15)πmin)−1} given (1).

Zhu et al.: Estimation of Markov Models

9

Next we propose and analyze a nuclear-norm regularized maximum likelihood estimator (MLE)

of P deﬁned as follows:

(cid:98)P := arg min (cid:96)n(Q) + λ(cid:107)Q(cid:107)∗

s.t. Q1p = 1p, α/p ≤ Qij ≤ β/p,

∀ 1 ≤ i, j ≤ p,

(3)

where λ > 0 is a tuning parameter. Note that we cannot allow Q to have zero entries as in Assump-

tion 1, because otherwise we may have that (cid:98)Pij = 0 and Pij > 0 for some (i, j), violating the

requirement of the deﬁnition of DKL(P, (cid:98)P). Our ﬁrst theorem shows that with an appropriate

choice of λ, (cid:98)P exhibits a sharp statistical rate. For simplicity, from now on we say a (cid:38) b (a (cid:46) b) if

there exists a universal constant c > 0 (C > 0) such that a ≥ cb (a ≤ Cb).

Theorem 1 (Statistical guarantee for the nuclear-norm regularized estimator).

Suppose the initial state X0 is drawn from the stationary distribution π and Assumption 1 holds.

There exists a universal constant C1 > 0, such that for any ξ > 1, if we choose

λ = C1

(cid:26)(cid:18) ξp2πmax log p

(cid:19)1/2

nα

+

ξp log p
nα

(cid:27)
,

then whenever nπmax(1 − ρ+) ≥ max{max(20, ξ2) log p, log n}, we have that

(cid:18)

P

DKL(P, (cid:98)P) (cid:38) ξrπmaxβ2p log p

πminα3n

+

ξπmin
rpπmax log p

and that

(cid:18)

P

(cid:107) (cid:98)P − P(cid:107)2
F

(cid:38) ξrπmaxβ4 log p
π2
minα4n

+

ξβ2
αrp2πmax log p

(cid:19)

(cid:19)

(cid:46) e−ξ + p−(ξ−1) + p−10,

(cid:46) e−ξ + p−(ξ−1) + p−10.

Remark 3. When n (cid:46) {rpπmax(log p)β/(πminα3/2)}2, the second terms of both the KL-Divergence

and Frobenius-norm error bounds are dominated by the ﬁrst terms respectively, so that

DKL(P, (cid:98)P) = OP

(cid:18) rπmaxβ2p log p
πminα3n

(cid:19)

and (cid:107) (cid:98)P − P(cid:107)2

F = OP

(cid:18) rπmaxβ4 log p

(cid:19)

π2
minα4n

.

(4)

When α (cid:16) β and πmax (cid:16) πmin, we have that α, β (cid:16) 1 and that πmax, πmin (cid:16) 1/p. Therefore,

DKL(P, (cid:98)P) = OP(rp log p/n) and (cid:107) (cid:98)P − P(cid:107)2

F = OP(rp log p/n). These rates are consistent with those

derived in the literature of low-rank matrix estimation (Negahban and Wainwright 2011, Koltchin-

skii et al. 2011). For a big n, the current error bounds are sub-optimal: the second terms of the

10

Zhu et al.: Estimation of Markov Models

bounds are independent of n and thus do not converge to zero as n goes to inﬁnity. These terms are

due to the requirement of the uniform concentration of (cid:101)DKL(P, (cid:98)P), the empirical counterpart of

DKL(P, (cid:98)P), to DKL(P, (cid:98)P) (see Lemma 3). We eliminate these trailing terms through an alternative

proof strategy in Section EC.9, though the resulting statistical rates have worse dependence on α

and β and are thus relegated to the appendix.

Remark 4. When r = 1, P can be written as 1v(cid:62) for some vector v ∈ (cid:60)p, and then estimating

P essentially reduces to estimating a discrete distribution from multinomial count data. The ﬁrst

term of the upper bounds in Theorem 1 nearly matches (up to a log factor) the classical results of

discrete distribution estimation (cid:96)2 risks (see, e.g., Lehmann and Casella (2006, Pg. 349)).

Next we move on to the second approach – using rank-constrained MLE to estimate P:

(cid:98)Pr := arg min (cid:96)n(Q)

s.t. Q1p = 1p, α/p ≤ Qij ≤ β/p,

∀ 1 ≤ i, j ≤ p,

rank(Q) ≤ r.

(5)

Similarly to (3), we cannot allow Q to have zero entries. In contrast to (cid:98)P, the rank-constrained

MLE (cid:98)Pr enforces the prior knowledge “P is low-rank” exactly without inducing any additional

bias. It requires solving a non-convex and non-smooth optimization problem, for which we will

provide an algorithm based on DC programming in Section 4.2. Here we ﬁrst present its statistical

guarantee.

Theorem 2 (Statistical guarantee for the rank-constrained estimator). Suppose

that

Assumption 1 holds and that nπmax(1 − ρ+) > max(20 log p, log n). There exist universal constants

C1, C2 > 0 such that for any ξ > 0,

(cid:26)

DKL(P, (cid:98)Pr) ≥ max

P

(cid:18) C1rπmaxβ2p log p
πminα3n

,

ξπmin
rpπmax log p

(cid:19)(cid:27)

≤ C2e−ξ,

and

P

(cid:26)

(cid:107) (cid:98)Pr − P(cid:107)2

F ≥ max

(cid:18) C1rπmaxβ4 log p

π2
minα4n

,

ξβ2
αrp2πmax log p

(cid:19)(cid:27)

≤ C2e−ξ.

Zhu et al.: Estimation of Markov Models

11

Remark 5. The proof of the rank constrained method requires fewer inequality steps and is more

straightforward than the that of the nuclear method. Although our upper bounds of the nuclear

norm regularized method and the rank constrained one have the same rate, the diﬀerence of their

proofs may implicitly suggest the advantage of the rank constrained method in the constant, as

futher illustrated by our numerical studies.

To assess the quality of the established statistical guarantee, we further provide a lower bound

result below. It shows that when α, β are constants, both estimators (cid:98)P and (cid:98)Pr are rate-optimal

up to a logarithmic factor. Informally speaking, they are not improvable for estimating the class

of rank-r Markov chains.

Theorem 3 (Minimax error lower bound for estimating low-rank Markov models).

Consider the following set of low-rank transition matrices

Θ := (cid:8)P : ∀j, k ∈ [p], Pjk ∈ {0} ∪ [α/p, +∞), P1p = 1p, rank(P) ≤ r(cid:9).

There exists a universal constant c > 0 such that when p(r − 1) ≥ 192 log 2, we have

inf
(cid:98)P

sup
P∈Θ

E(cid:107) (cid:98)P − P(cid:107)2

F ≥

cp(r − 1)
nα

.

Remark 6. Theorem 3 shows that a smaller α makes the estimation problem harder. It still

remains to be an open problem whether β in Assumption 1 should be in this minimax risk or not.

Besides the full transition matrix P, the leading left and right singular vectors of P, denoted by

U, V ∈ Op×r, also play important roles in Markov chain analysis. For example, performing k-means

on reliable estimate of U or V can give rise to state aggregation of the Markov chain (Zhang and

Wang 2017). In the following, we further establish the statistical rate of estimating the singular

subspace of the Markov transition matrix, based on the previous results.

Theorem 4. Under the setting of Theorem 1, let (cid:98)U, (cid:98)V ∈ Op×r be the left and right singular vectors

of (cid:98)P respectively. Then there exist universal constants C1, C2, such that for any ξ > 0, we have that

max

(cid:110)

(cid:107) sin Θ( (cid:98)U, U)(cid:107)2

F , (cid:107) sin Θ( (cid:98)V, V)(cid:107)2

F

(cid:111)

(cid:26)

≤ min

max

(cid:18) Crπmaxβ4 log p
π2
r (P)
minα4nσ2

,

ξβ2

αrp2πmax(log p)σ2

r (P)

(cid:19)

(cid:27)

, r

12

Zhu et al.: Estimation of Markov Models

with probability at least 1 − C2(e−ξ + p−(ξ−1) + p−10). Here, σr(P) is the r-th largest singular value

of P and (cid:107) sin Θ( (cid:98)U, U)(cid:107)F := (r − (cid:107) (cid:98)U(cid:62)U(cid:107)2

F )1/2 is the Frobenius norm sin Θ distance between (cid:98)U and

U.

3.3. Proof outline of Theorems 1, 2

In this section, we elucidate the roadmap to proving Theorems 1 and 2. Complete proofs are

deferred to the supplementary materials. We mainly focus on Theorem 1 for the nuclear-norm

penalized MLE (cid:98)P, as we use similar strategies to prove Theorem 2.

We ﬁrst show in the forthcoming Lemma 1 that when the regularization parameter λ is suﬃciently

large, the statistical error (cid:98)∆ := (cid:98)P − P falls in a restricted nuclear-norm cone. This cone structure

is crucial to establishing strong statistical guarantee for estimation of low-rank matrices with high-

dimensional scaling (Negahban and Wainwright 2011). Deﬁne a linear subspace N := {Q : Q1p =

1p} and denote the corresponding projection operator by ΠN . In other words, for any Q ∈ N and

any j = 1, . . . , p, the summation of all the entries in the jth row of Q equals one. One can verify

that for any Q ∈ (cid:60)p×p, ΠN (Q) = Q − Q11(cid:62)/p. Let P = UDV(cid:62) be an SVD of P, where U, V ∈ (cid:60)p×r

are orthonormal and the diagonals of D are in the non-increasing order. Deﬁne

M := {Q ∈ (cid:60)p×p | row(Q) ⊆ col(V), col(Q) ⊆ col(U)},

⊥

M

:= {Q ∈ (cid:60)p×p | row(Q) ⊥ col(V), col(Q) ⊥ col(U)},

where col(·) and row(·) denote the column space and row space respectively. We can write any

∆ ∈ (cid:60)p×p as

∆ = [U, U⊥]






Γ11 Γ12

Γ21 Γ22



 [V, V⊥](cid:62).

Deﬁne ∆W as the projection of ∆ onto any Hilbert space W ⊆ (cid:60)p×p. Then,

∆M = UΓ11V(cid:62), ∆

M

⊥ = U⊥Γ22(V⊥)(cid:62), ∆M = [U, U⊥]






Γ11 Γ12

Γ21 0



 [V, V⊥](cid:62).

(6)

The lemma below shows that (cid:98)∆ := (cid:98)P − P falls in a nuclear-norm cone if λ is suﬃciently large.

Zhu et al.: Estimation of Markov Models

13

Lemma 1. If λ ≥ 2(cid:107)ΠN (∇(cid:96)n(P))(cid:107)2 in (3), then we have that

(cid:107) (cid:98)∆

M

⊥(cid:107)∗ ≤ 3(cid:107) (cid:98)∆M(cid:107)∗ + 4(cid:107)PM⊥(cid:107)∗.

In particular, when P ∈ M, we have that (cid:107)PM⊥(cid:107)∗ = 0 and that

(cid:107) (cid:98)∆(cid:107)∗ ≤ (cid:107)∆

M

⊥(cid:107)∗ + (cid:107)∆M(cid:107)∗ ≤ 4(cid:107) (cid:98)∆M(cid:107)∗ ≤ 4(2r)1/2(cid:107) (cid:98)∆(cid:107)F.

(7)

Lemma 1 implies that the converting factor between the nuclear and Frobenius norms of (cid:98)∆

is merely 4(2r)1/2 when P ∈ M, which is much smaller than the worst-case factor p1/2 between

nuclear and Frobenius norms of general p-by-p matrices. This property of (cid:98)∆ is one cornerstone for

establishing Theorem 1.

Next, we derive the rate of (cid:107)ΠN (∇(cid:96)n(P))(cid:107)2 to determine the order of λ that ensures the condition

of Lemma 1 to hold.

Lemma 2. Under Assumption 1, whenever nπmax(1 − ρ+) ≥ 2 log p, for any ξ > 1,

(cid:26)

(cid:107)ΠN (∇(cid:96)n(P))(cid:107)2 (cid:38)

P

(cid:18) ξp2πmax log p
nα

(cid:19)1/2

(cid:27)

+

ξp log p
nα

≤ 4p−(ξ−1) + exp

(cid:18)

−

nπmax(1 − ρ+)
2

(cid:19)

.

Remark 7. Lemma 2 is essentially due to concentration of a matrix martingale. Many existing

results on measure concentration of dependent random variables (Marton 1996, Kontorovich 2007,

Kontorovich and Ramanan 2008, Paulin 2015) are not directly applicable because of the matrix

structure of ∇(cid:96)n(P). The main probabilistic tool we use here is the matrix Freedman inequality

(Tropp 2011, Corollary 1.3) that characterizes concentration behavior of a matrix martingale (See

(EC.18) for details). We notice two recent works, Wolfer and Kontorovich (2019b) and Wolfer

and Kontorovich (2019a), that use the same matrix Freedman inequality. Speciﬁcally, Wolfer and

Kontorovich (2019a) applied the matrix Freedman inequality to derive a conﬁdence interval for the

mixing time of a Markov chain based on its single trajectory, and Wolfer and Kontorovich (2019b)

used the same inequality to establish an upper bound for the sample complexity of learning a

Markov chain. Finally, we also use an variant of Bernstein’s inequality for general Markov chains

(Jiang et al. 2018, Theorem 1.2) to derive an exponential tail bound for the status counts of the

Markov chain X (See (EC.16) for details).

14

Zhu et al.: Estimation of Markov Models

Let C := {Q ∈ Rp×p : (cid:107)Q − P(cid:107)∗ ≤ 4 × 21/2(cid:107)Q − P(cid:107)F, Q1p = 1p, α/p ≤ Qjk ≤ β/p, ∀(j, k) ∈ [p] × [p]}.

For any Q ∈ C, deﬁne L(Q) := E{− log((cid:104)Q, Xi(cid:105))} and (cid:96)n(Q) := n−1 (cid:80)n

i=1 − log((cid:104)Q, Xi(cid:105)). Recall

that DKL(P, Q) = L(Q) − L(P) = (cid:80)p

i=1 πiDKL(Pi·, Qi·) = (cid:80)p

i=1

(cid:80)p

j=1 πiPij log(Pij/Qij). Deﬁne the

empirical KL divergence of Q from P as

(cid:101)DKL(P, Q) :=

1
n

n
(cid:88)

i=1

(cid:104)log(P) − log(Q), Xi(cid:105) = (cid:96)n(Q) − (cid:96)n(P).

The ﬁnal ingredient of the analysis is the uniform converegence of (cid:101)DKL(P, Q) to DKL(P, Q) when

DKL(P, Q) is large.

Lemma 3. Suppose that nπmax(1 − ρ+) ≥ max(20 log p, log n). For any η > πmin/(2πmaxrp log p),

deﬁne C(η) := {Q ∈ C : DKL(P, Q) ≥ η}. Then there exist universal constants C1, C2 > 0 such that

(cid:26)

∀Q ∈ C(η), | (cid:101)DKL(P, Q) − DKL(P, Q)| ≤

P

1
2

DKL(P, Q)+

C1πmaxβ2rp log p
πminα3n

(cid:27)

(cid:18)

≥ 1 − C2 exp

−

ηπmaxrp log p
πmin

(cid:19)

.

(8)

Theorem 1 follows immediatley after one combines Lemmas 1, 2 and 3. As for the rank-

constrained MLE (cid:98)Pr, let (cid:98)∆(r) := (cid:98)Pr − P. Note that the rank constraint in (5) implies that

rank( (cid:98)∆(r)) ≤ 2r. Thus, (cid:107) (cid:98)∆(r)(cid:107)∗ ≤ (2r)1/2(cid:107) (cid:98)∆(r)(cid:107)F and Lemma 3 remains applicable in the statis-

tical anlaysis of (cid:98)Pr.

4. Computing Markov models using low-rank optimization

In this section we develop eﬃcient optimization methods to compute the proposed estimators for

the low-rank Markov model. From now on, we drop the constraint that α/p ≤ Qij ≤ β/p, which is

used only to derive the statistical guarantees. In other words, α and β are motivated by statistical

theory, and do not need to be taken into account in the optimization.

4.1. Optimization methods for the nuclear-norm regularized likelihood problem

We ﬁrst consider the nuclear-norm regularized likelihood problem (3). It is a special case of the

following linearly constrained optimization problem:

min {g(X) + c(cid:107)X(cid:107)∗ | A(X) = b} ,

(9)

Zhu et al.: Estimation of Markov Models

15

where g : (cid:60)p×p → (−∞, +∞] is a closed, convex, but possibly non-smooth function, A : (cid:60)p×p → (cid:60)m

is a linear map, b ∈ (cid:60)m and c > 0 are given data. If we take α = 0, β = p in problem (3), it becomes

a special case of the general problem (9) with g(X) = −(cid:96)n(X) + δ(X ≥ 0), A(X) = X1p, b = 1p, and

δ(·) being the indicator function.

Despite of its convexity, problem (9) is highly nontrivial due to the nonsmoothness of g and the

presence of the nuclear norm regularizer. Here, we propose to solve it via the dual approach. The

dual of problem (9) is

min g∗(−Ξ) − (cid:104)b, y(cid:105)

s.t. Ξ + A∗(y) + S = 0,

(cid:107)S(cid:107)2 ≤ c,

(10)

where (cid:107) · (cid:107)2 denotes the spectral norm, and g∗ is the conjugate function of g given by

g∗(Ξ) =

(cid:88)

(i,j)∈Ω

nij
n

(log

nij
n

− 1 − log(−Ξij)) + δ(Ξ ≤ 0) ∀ Ξ ∈ (cid:60)p×p

with Ω = {(i, j) | nij (cid:54)= 0} and Ω = {(i, j) | nij = 0}. Given σ > 0, the augmented Lagrangian function

Lσ associated with (10) is

Lσ(Ξ, y, S; X) = g∗(−Ξ) − (cid:104)b, y(cid:105) +

σ
2

(cid:107)Ξ + A∗(y) + S + X/σ(cid:107)2 −

1
2σ

(cid:107)X(cid:107)2.

We consider popular ADMM type methods for solving problem (10) (A comprehensive numerical

study has been conducted in (Li et al. 2016b) and justiﬁes our procedure). Since there are three

separable blocks in (10) (namely Ξ, y, and S), the direct extended ADMM is not applicable. Indeed,

it has been shown in (Chen et al. 2016) that the direct extended ADMM for multi-block convex

minimization problem is not necessarily convergent. Fortunately, the functions corresponding to

block y in the objective of (10) is linear. Thus we can apply the multi-block symmetric Gauss-Sediel

based ADMM (sGS-ADMM) (Li et al. 2016b). In literature (Chen et al. 2017, Ferreira et al. 2017,

Lam et al. 2018, Li et al. 2016b, Wang and Zou 2018), extensive numerical experiments demonstrate

that sGS-ADMM is not only convergent but also faster than the directly extended multi-block

ADMM and its many other variants. Speciﬁcally, the algorithmic framework of sGS-ADMM for

solving (10) is presented in Algorithm 1.

Next, we discuss how the k-th iteration of Algorithm 1 is performed:

16

Zhu et al.: Estimation of Markov Models

Algorithm 1 An sGS-ADMM for solving (10)

Input: initial point (Ξ0, y0, S0, X0), penalty parameter σ > 0, maximum iteration number K,

and the step-length γ ∈ (0, (1 +

√

5)/2)

for k = 0 to K do

yk+ 1

2 = arg miny Lσ(Ξk, y, Sk; Xk)

Ξk+1 = arg minΞ Lσ(Ξ, yk+ 1

2 , Sk; Xk)

yk+1 = arg miny Lσ(Ξk+1, y, Sk; Xk)

Sk+1 = arg minS Lσ(Ξk+1, yk+1, S, ; Xk)

Xk+1 = Xk + γσ(Ξk+1 + A∗(yk+1) + Sk+1)

end for

Computation of yk+ 1

2 and yk+1. Simple calculations show that yk+ 1

2 and yk+1 can be obtained

by solving the following linear systems:

2 =



yk+ 1

yk+1 =

(AA∗)−1(cid:0)b − X k − σ(Ξk + Sk)(cid:1),

1
σ

(AA∗)−1(cid:0)b − X k − σ(Ξk+1 + Sk)(cid:1).

1
σ

In our estimation problem, it is not diﬃcult to verify that AA∗y = py for any y ∈ (cid:60)p. Thanks to

this special structure, the above formulas can be further reduced to

yk+ 1

2 =

1
σp

(cid:0)b − Xk − σ(Ξk + Sk)(cid:1) and yk+1 =

(cid:0)b − Xk − σ(Ξk+1 + Sk)(cid:1).

1
σp

Computation of Ξk+1. To compute Ξk+1, we need to solve the following optimization problem:

min
Ξ

(cid:110)

g∗(−Ξ) +

σ
2

(cid:107)Ξ + Rk(cid:107)2(cid:111)

,

where Rk ∈ (cid:60)p×p is given. Careful calculations, together with the Moreau identity (Rockafellar

2015, Theorem 31.5), show that

Ξk+1 =

1
σ

[Zk − σRk] and Zk = arg min

Z

(cid:26)

σg(Z) +

1
2

(cid:107)Z − σRk(cid:107)2

(cid:27)

.

For our estimation problem, i.e., g(X) = (cid:96)n(X) + δ(X ≥ 0), it is easy to see that Z k admits the

following form:

(cid:113)

σRk

ij + σ

Z k

ij =

(Rk

ij)2 + 4nij/(nσ)
2

if (i, j) ∈ Ω and Z k

ij = σ max(Rk

ij, 0)

if (i, j) ∈ Ω.

Zhu et al.: Estimation of Markov Models

17

Computation of Sk+1. The computation of Sk+1 can be simpliﬁed as:

Sk+1 = arg min

S

(cid:110) σ
2

(cid:107)S + Ξk+1 + A∗yk+1 + Xk/σ(cid:107)2 | (cid:107)S(cid:107)2 ≤ c

(cid:111)

.

Let Wk := −(Ξk+1 + A∗yk+1 + X k/σ) admit the following singular value decomposition (SVD)

Wk = UkΣkV(cid:62)

k , where Uk and Vk are orthogonal matrices, Σk = Diag(αk

1, . . . , αk

p) is the diagonal

matrix of singular values of Wk, with αk

1 ≥ . . . ≥ αk

p ≥ 0. Then, by Lemma 2.1 in (Jiang et al. 2014),

we know that

Sk+1 = Uk min(Σk, c)V(cid:62)
k ,

where min(Σk, c) = Diag(cid:0) min(αk

1, c), . . . , min(αk

p, c)(cid:1). We also note that in the implementation, only

partial SVD, which is much cheaper than full SVD, is needed as r (cid:28) p.

The nontrivial convergence results and the sublinear non-ergodic iteration complexity of Algorithm

1 can be obtained from Li et al. (2016b) and Chen et al. (2017). We put the convergence theorem

and a sketch of the proof in the supplementary material.

4.2. Optimization methods for the rank-constrained likelihood problem

Next we develop the optimization method for computing the rank-constrained likelihood maximizer

from (5). In Subsection 4.2.1, a penalty approach is applied to transform the original intractable

rank-constrained problem into a DC programming problem. Then we solve this problem by a

proximal DC (PDC) algorithm in Subsection 4.2.2. We also discuss the solver for the subproblems

involved in the proximal DC algorithm. Lastly, a uniﬁed convergence analysis of a class of majorized

indeﬁnite-proximal DC (Majorized iPDC) algorithms is provided in Subsection 4.2.3.

4.2.1. A penalty approach for problem (5). Recall (5) is intractable due to the non-

convex rank constraint, we introduce a penalty approach to relax. We particularly study the

following optimization problem:

min {f (X) | A(X) = b, rank(X) ≤ r} ,

(11)

18

Zhu et al.: Estimation of Markov Models

where f : (cid:60)p×p → (−∞, +∞] is a closed proper convex, but possibly non-smooth, function. The

original rank-constraint maximum likelihood problem (5) can be viewed as a special case of the

general model (11).

Given X ∈ (cid:60)p×p, let σ1(X) ≥ · · · ≥ σp(X) ≥ 0 be the singular values of X. Since rank(X) ≤ r if

and only σr+1(X) + . . . + σp(X) = (cid:107)X(cid:107)∗ − (cid:107)X(cid:107)(r) = 0 ((cid:107)X(cid:107)(r) = (cid:80)r

i=1 σi(X) is the Ky Fan r-norm

of X), (11) can be equivalently formulated as

min (cid:8)f (X) | (cid:107)X(cid:107)∗ − (cid:107)X(cid:107)(r) = 0, A(X) = b(cid:9) .

See also (Gao and Sun 2010, Equation (29)). The penalized formulation of problem (11) is

min (cid:8)f (X) + c((cid:107)X(cid:107)∗ − (cid:107)X(cid:107)(r)) | A(X) = b(cid:9) ,

(12)

where c > 0 is a penalty parameter. Since (cid:107) · (cid:107)(r) is convex, the objective in problem (12) is a

diﬀerence of two convex functions: f (X) + c(cid:107)X(cid:107)∗ and c(cid:107)X(cid:107)(r), i.e., (12) is a DC program.

Let X∗

c be an optimal solution to the penalized problem (12). The following proposition shows

that X∗

c is also the optimizer to (11) when it is low-rank.

Proposition 1. If rank(X∗

c) ≤ r, then X∗

c is also an optimal solution to the original problem (11).

In practice, one can gradually increase the penalty parameter c to obtain a suﬃcient low rank

solution X∗

c. In our numerical experiments, we can obtain solutions with the desired rank with a

properly chosen parameter c.

4.2.2. A PDC algorithm for the penalized problem (12). The central idea of the DC

algorithm (Pham Dinh and Le Thi 1997) is as follows: at each iteration, one approximates the

concave part of the objective function by its aﬃne majorant, then solves the resulting convex

optimization problem. In this subsection, we present a variant of the classic DC algorithm for

solving (12). For the execution of the algorithm, we recall that the sub-gradient of Ky Fan r-norm

at a point X ∈ (cid:60)p×p (Watson 1993) is

∂(cid:107)X(cid:107)(r) = (cid:8)U Diag(q∗)V(cid:62) | q∗ ∈ ∆(cid:9) ,

Zhu et al.: Estimation of Markov Models

19

where U and V are the singular vectors of X, and ∆ is the optimal solution set of the following

problem

(cid:40) p

(cid:88)

i=1

max
q∈(cid:60)p

σi(X)qi | (cid:104)1p, q(cid:105) ≤ r, 0 ≤ q ≤ 1

.

(cid:41)

Note that one can eﬃciently obtain a component of ∂(cid:107)X(cid:107)(r) by computing the SVD of X and

picking up the SVD vectors corresponding to the r largest singular values. After these preparations,

we are ready to state the PDC algorithm for problem (12) in Algorithm 2. Diﬀerent from the classic

DC algorithm, an additional proximal term is added to ensure that solutions of subproblems (13)

exist and the diﬀerence of two consecutive iterations converges. See Theorem 5 and Remark 8 for

more details.

Algorithm 2 A PDC algorithm for solving (12)
Given c > 0, α ≥ 0, and the stopping tolerance η, choose initial point X0 ∈ (cid:60)p×p. Iterate the

following steps for k = 0, 1, . . . :

1. Choose Wk ∈ ∂(cid:107)Xk(cid:107)(r). Compute

Xk+1 = arg min f (X) + c (cid:0)(cid:107)X(cid:107)∗ − (cid:104)Wk, X − Xk(cid:105) − (cid:107)Xk(cid:107)(r)

(cid:1) +

α
2

(cid:107)X − Xk(cid:107)2
F

(13)

subject to A(X) = b.

2. If (cid:107)Xk+1 − Xk(cid:107)F ≤ η, stop.

We say that X is a critical point of problem (12) if

∂(f (X) + c(cid:107)X(cid:107)∗ + δ(A(X) = b)) ∩ (c∂(cid:107)X(cid:107)(r)) (cid:54)= ∅.

We have the following convergence results for Algorithm 2.

Theorem 5 (Convergence of Algorithm 2). Let {Xk} be the sequence generated by Algorithm

2 and α ≥ 0. Then {f (Xk) + c((cid:107)Xk(cid:107)∗ − (cid:107)Xk(cid:107)(r))} is a non-increasing sequence. If Xk+1 = Xk for

some integer k ≥ 0, then Xk is a critical point of (12). Otherwise, it holds that

(cid:0)f (Xk+1) + c((cid:107)Xk+1(cid:107)∗ − (cid:107)Xk+1(cid:107)(r))(cid:1) − (cid:0)f (Xk) + c((cid:107)Xk(cid:107)∗ − (cid:107)Xk(cid:107)(r))(cid:1) ≤ −

α
2

(cid:107)Xk+1 − Xk(cid:107)2
F .

20

Zhu et al.: Estimation of Markov Models

Moreover, any accumulation point of the bounded sequence {Xk} is a critical point of problem (12).

In addition, if α > 0, it holds that limk→∞ (cid:107)Xk+1 − Xk(cid:107)F = 0.

Remark 8 (Adjusting Parameters). In practice, a small α > 0 is suggested to ensure strict

decrease of the objective value and convergence of {(cid:107)Xk+1 − Xk(cid:107)F }; if f is strongly convex, one

achieves these nice properties even if α = 0 based on the results of Theorem 6. The penalty param-

eter c can be adaptively adjusted according to the rank of the sequence generated by Algorithm

2.

Remark 9 (Number of iterations of Algorithm 2). Let η > 0 be the stopping tolerance

and F ∗ be the optimal value of problem (12). By using the inequality in Theorem 5, it can be

shown that if α > 0, then Algorithm 2 terminates in no more than K iterations, where

(cid:38)

K =

2(cid:0)f (X 0) + c((cid:107)X 0(cid:107)∗ − (cid:107)X 0(cid:107)(r)) − F ∗(cid:1)
αη2

(cid:39)

+ 1.

Remark 10 (Statistical properties). The statistical rate we derived in Theorem 2 does not

carry over to the iterates of the DC algorithm here. Though we show in Theorem 5 that the DC

algorithm can converge to a critical point, it remains unclear whethere this point is close to the

global optimum and provably enjoys the statistical guarantee. Recently there have been many

works conveying positive messages on the statistical properties of the non-convex optimization

algorithms. For example, Loh and Wainwright (2015) showed that any stationary point of the

composite objective function they considered lies within statistical precision of the true parameter.

We hope to establish similar theory for the proposed DC approach in future research.

Next, we discuss how to solve subproblems (13). (13) is still a nuclear norm penalized convex

optimization problem and is a special case of model (9) with g(X) = f (X) + (cid:104)W, X(cid:105) + α

2 (cid:107)X(cid:107)2
F .

Hence, Algorithm 1 can directly solve these subproblems eﬃciently. When Algorithm 1 is executed

on this new function g, all computations, except for the update of Ξ, have already been discussed

in Section 4.1. To update Ξ in the process of executing Algorithm 1 for solving (13) with g(X) =

Zhu et al.: Estimation of Markov Models

21

(cid:96)n(X) + δ(X ≥ 0) + (cid:104)W, X(cid:105) + α

2 (cid:107)X(cid:107)2

F , we need to solve the following minimization problem for

given R ∈ (cid:60)p×p and σ > 0,

Z∗ here can be calculated by

Z∗ = arg min

Z

(cid:26)

σg(Z) +

1
2

(cid:107)Z − σR(cid:107)2

(cid:27)

.

Z ∗

ij =

(σRij − Wij) + σ(cid:112)(Rij − Wij/σ)2 + 4(α + 1)nij/(nσ)
2(α + 1)




σ max(Rij − Wij/σ, 0)

if (i, j) ∈ Ω.

if (i, j) ∈ Ω;

4.2.3. A uniﬁed analysis for the majorized iPDC algorithm. Due to the presence of the

proximal term α

2 (cid:107)X − Xk(cid:107)2 in Algorithm 2, the classical DC analyses cannot be applied directly.

In this subsection, we provide a uniﬁed convergence analysis for the majorized indeﬁnite-proximal

DC (majorized iPDC) algorithm which includes Algorithm 2 as a special instance. Let X be a

ﬁnite-dimensional real Euclidean space endowed with inner product (cid:104)·, ·(cid:105) and induced norm (cid:107) · (cid:107).

Consider the following optimization problem

θ(x) (cid:44) g(x) + p(x) − q(x),

min
x∈X

(14)

where g : X → (cid:60) is a continuously diﬀerentiable function (not necessarily convex) with a Lipschitz

continuous gradient and Lipschitz modulus Lg > 0, i.e.,

(cid:107)∇f (x) − ∇f (x(cid:48))(cid:107) ≤ Lg(cid:107)x − x(cid:48)(cid:107) ∀ x, x(cid:48) ∈ X,

p : X → (−∞, +∞] and q : X → (−∞, +∞] are two proper closed convex functions. It is not diﬃcult

to observe that penalized problem (12) is a special instance of problem (14). For general model

(14), one can only expect the DC algorithm converges to a critical point ¯x ∈ X of (14) satisfying

(∇g(¯x) + ∂p(¯x)) ∩ ∂q(¯x) (cid:54)= ∅.

Since g is continuously diﬀerentiable with Lipschitz continuous gradient, there exists a self-adjoint

positive semideﬁnite linear operator G : X → X such that for any x, x(cid:48) ∈ X,

g(x) ≤ (cid:98)g(x; x(cid:48)) (cid:44) g(x(cid:48)) + (cid:104)∇g(x(cid:48)), x − x(cid:48)(cid:105) +

1
2

(cid:107)x − x(cid:48)(cid:107)2
G.

22

Zhu et al.: Estimation of Markov Models

Algorithm 3 A majorized indeﬁnite-proximal DC algorithm for solving problem (14)
Given initial point x0 ∈ X and stopping tolerance η, choose a self-adjoint, possibly indeﬁnite, linear

operator T : X → X. Iterate the following steps for k = 0, 1, . . . :

1. Choose ξk ∈ ∂q(xk). Compute

xk+1 ∈ arg min

x∈X

(cid:26)

(cid:98)θ(x; xk) +

(cid:107)x − xk(cid:107)2
T

(cid:27)

,

1
2

(15)

where (cid:98)θ(x; xk) (cid:44)

(cid:98)g(x; xk) + p(x) − (cid:0)q(xk) + (cid:104)x − xk, ξk(cid:105)(cid:1).

2. If (cid:107)xk+1 − xk(cid:107) ≤ η, stop.

We present the majorized iPDC algorithm for solving (14) in Algorithm 3 and provide the

following convergence results.

Theorem 6 (Convergence of iPDC). Assume that inf x∈X θ(x) > −∞. Let {xk} be the sequence

generated by Algorithm 3. If xk+1 = xk for some k ≥ 0, then xk is a critical point of (14). If

G + 2T (cid:23) 0, then any accumulation point of {xk}, if exists, is a critical point of (14). In addition,

if G + 2T (cid:31) 0, it holds that lim
k→∞

(cid:107)xk+1 − xk(cid:107) = 0.

The proof of Theorem 6 is provided in the supplementary material.

Remark 11. Here, we discuss the roles of linear operators G and T . First, G makes the subproblems

(15) in Algorithm 3 more amenable to eﬃcient computations. Theorem 6 shows the algorithm

is convergent if G + 2T (cid:23) 0. This indicates that instead of adding the commonly used positive

semideﬁnte or positive deﬁnite proximal terms, we allow T to be indeﬁnite for better practical

performance. The computational beneﬁt of using indeﬁnite proximal terms has also been observed

in (Gao and Sun 2010, Li et al. 2016a). As far as we know, Theorem 6 provides the ﬁrst rigorous

convergence proof of the DC algorithms with indeﬁnite proximal terms. Second, G and T also help

to guarantee that the solutions of the subproblems (15) exist. Since G + 2T (cid:23) 0 and G (cid:23) 0, we

have that 2G + 2T (cid:23) 0, i.e., G + T (cid:23) 0. Hence, G + 2T (cid:23) 0 (G + 2T (cid:31) 0) implies that subproblems

(15) are (strongly) convex. Third, the choices of G and T are very much problem dependent. The

general principle is that G + T should be as small as possible while ensuring xk+1 is relatively easy

to compute.

Zhu et al.: Estimation of Markov Models

5. Simulation results

23

In this section, we conduct numerical experiments to validate our theoretical results. We ﬁrst com-

pare the proposed nuclear-norm regularized estimator and the rank-constrained estimator with

previous methods in literature using synthetic data. We then use the rank-constrained method to

analyze a dataset of Manhattan taxi trips to reveal citywide traﬃc patterns. All of our computa-

tional results are obtained by running Matlab (version 9.5) on a windows workstation (8-core,

Intel Xeon W-2145 at 3.70GHz, 64 G RAM).

5.1. Experiments with simulated data

We randomly draw the transition matrix P as follows. Let U0, V0 ∈ (cid:60)p×r be random matrices with

i.i.d. standard normal entries and let

(cid:101)U[i,:] = (U0 ◦ U0)[i,:]/(cid:107)(U0)[i,:](cid:107)2

2 and (cid:101)V[:,j] = (V0 ◦ V0)[:,j]/(cid:107)(V0)[:,j](cid:107)2
2,

i = 1, . . . , p, j = 1, . . . , r,

where ◦ is the Hadamard product and (cid:101)U[i,:] denotes the i-th row of (cid:101)U. The transition matrix

P is obtained via P = (cid:101)U (cid:101)V(cid:62). Then we simulate a Markov chain trajectory of length n =

round(krp log(p)) on p states, {X0, . . . , Xn}, with varying values of k.

We compare the performance of four procedures: the nuclear norm penalized MLE, rank-

constrained MLE, empirical estimator and spectral estimator. Here, the empirical estimator is the

empirical count distribution matrix deﬁned as follows:

˜P =

(cid:17)

(cid:16) ˜Pij

,

˜Pij =

1≤i,j≤p





(cid:80)n

k=1 1{Xk−1=i,Xk =j}
(cid:80)n
k=1 1{Xk−1=i}

, when (cid:80)n

k=1 1{Xk−1=i} ≥ 1;

1

p ,

when (cid:80)n

k=1 1{Xk−1=i} = 0.

The empirical estimator is in fact the unconstrained maximum likelihood estimator without taking

into account the low-rank structure. The spectral estimator (Zhang and Wang 2017, Algorithm

1) is based on a truncated SVD. In the implementation of the nuclear norm penalized estimator,
the regularization parameter λ in (3) is set to be C(cid:112)p log p/n with constant C selected by cross-

validation. For each method, let (cid:98)U and (cid:98)V be the leading r left and right singular vectors of the

resulting estimator (cid:98)P. We measure the statistical performance of (cid:98)P through three quantities:

ηF := (cid:107)P − (cid:98)P(cid:107)2

F , ηKL := DKL(P, (cid:98)P), and ηU V := max(cid:8)(cid:107) sin Θ( (cid:98)U, U)(cid:107)2

F , (cid:107) sin Θ( (cid:98)V, V)(cid:107)2

F

(cid:9).

24

Zhu et al.: Estimation of Markov Models

We consider the following setting with p = 1000, r = 10, and k ∈ [10, 100]. The results are plotted

in Figure 1. One can observe from these results that for rank-constrained, nuclear norm penal-

ized and spectral methods, ηF , ηKL and ηU V converge to zero quickly as the number of the state

transitions n increases, while the statistical error of the empirical estimator decreases in a much

slower rate. Among the three estimators in the zoomed plots (second rows of Figure 1), the rank

constrained estimator slightly outperforms the nuclear norm penalized estimator and the spectral

estimator. This observation is consistent with our algorithmic design: the nuclear norm minimiza-

tion procedure is actually the initial step of Algorithm 2; thus the rank-constrained estimator can

be seen as a reﬁned version of the nuclear norm regularized estimator.

We also consider the case where the invariant distribution π is “imbalanced”, i.e., we construct P

such that mini=1,...,p πi is quite small and the appearance of some states is signiﬁcantly less than the

others. Speciﬁcally, given γ1, γ2 > 0, we generate a diagonal matrix D with i.i.d. beta-distributed

(Beta(γ1, γ2)) diagonal elements. After obtaining (cid:101)U and (cid:101)V in the same way as in the beginning of

this subsection, we compute (cid:101)P = (cid:101)U (cid:101)V(cid:62)D. The ground truth transition matrix P is obtained after a

normalization of (cid:101)P. Then, we simulate a Markov chain trajectory of length n = round(krp log(p))

on p states. In our experiment, we set p = 1000, r = 10, k ∈ [10, 100], and γ1 = γ2 = 0.5. The detailed

results are plotted in Figure 2. As can be seen from the ﬁgure, under the imbalanced setting,

the rank-constrained, nuclear norm penalized and spectral methods perform much better than the

empirical approach in terms of all the three statistical performance measures (ηF , ηKL and ηU V ).

In addition, the rank-constrained estimator exhibits a clear advantage over two other approaches.

5.2. Experiments with Manhattan Taxi data

In this experiment, we analyze a real dataset of 1.1 × 107 trip records of NYC Yellow cabs (Link:

https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv) in January

2016. Our goal is to partition the Manhattan island into several areas, in each of which the taxi

customers share similar destination preference. This can provide guidance for balancing the supply

and demand of taxi service and optimizing the allocation of traﬃc resources.

Zhu et al.: Estimation of Markov Models

25

Figure 1

The ﬁrst row compares the rank-constrained estimator, nuclear norm penalized estimator, spec-

tral method, and empirical estimator in terms of ηF = (cid:107)P − (cid:98)P(cid:107)2
max(cid:8)(cid:107) sin Θ( (cid:98)U, U, )(cid:107)2

F , (cid:107) sin Θ( (cid:98)V, V)(cid:107)2
F

(cid:9). The second row provides the zoomed plots of the ﬁrst row

F , ηKL = DKL(P, (cid:98)P), and ηU V =

without the empirical estimator. Here, n = round(krp log p) with p = 1, 000, r = 10 and k ranging from

10 to 100.

Figure 2

The ﬁrst row compares the rank-constrained estimator, nuclear norm penalized estimator, spec-

tral method, and empirical estimator in terms of ηF = (cid:107)P − (cid:98)P(cid:107)2
max(cid:8)(cid:107) sin Θ( (cid:98)U, U, )(cid:107)2

F , (cid:107) sin Θ( (cid:98)V, V)(cid:107)2
F

(cid:9) with imbalanced invariant distribution. The second row pro-

F , ηKL = DKL(P, (cid:98)P), and ηU V =

vides the zoomed plots of the ﬁrst row without the empirical estimator. Here, n = round(krp log p) with

p = 1, 000, r = 10 and k ranging from 10 to 100.

10203040506070809010000.050.10.150.20.250.3Frank-constrained MLEspectral methodempirical estimatornuclear MLE10203040506070809010000.20.40.60.81KLrank-constrained MLEspectral methodempirical estimatornuclear MLE10203040506070809010000.050.10.150.20.25UVrank-constrained MLEspectral methodempirical estimatornuclear MLE10203040506070809010000.0050.010.0150.020.0250.03Frank-constrained MLEspectral methodnuclear MLE10203040506070809010000.0020.0040.0060.0080.01KLrank-constrained MLEspectral methodnuclear MLE10203040506070809010000.050.10.150.20.25UVrank-constrained MLEspectral methodnuclear MLE102030405060708090100010203040Fspectral methodrank-constrained MLEempirical estimatornuclear MLE10203040506070809010000.050.10.150.20.250.30.35KLspectral methodrank-constrained MLEempirical estimatornuclear MLE102030405060708090100345678910UVspectral methodrank-constrained MLEempirical estimatornuclear MLE1020304050607080901000.10.20.30.40.50.60.70.8Frank-constrained MLEspectral methodnuclear MLE10203040506070809010000.0050.010.0150.020.0250.03KLrank-constrained MLEspectral methodnuclear MLE10203040506070809010033.544.55UVrank-constrained MLEspectral methodnuclear MLE26

Zhu et al.: Estimation of Markov Models

We discretize the Manhattan island into a ﬁne grid and model each cell of the grid as a state

of the Markov chain; each taxi trip can thus be viewed as a state transition of this Markov chain

(Yang et al. 2017, Benson et al. 2017, Liu et al. 2012). For stability concerns, our model ignores

the cells that have fewer than 1, 000 taxi visits. Given that the traﬃc dynamics typically vary over

time, we ﬁt the MC under three periods of a day, i.e., 06 : 00 ∼ 11 : 59 (morning), 12 : 00 ∼ 17 : 59

(afternoon) and 18 : 00 ∼ 23 : 59 (evening), where the number of the active states p = 803, 999 and

1, 079 respectively. We apply the rank-constrained likelihood approach to obtain the estimator (cid:98)Pr

of the transition matrix, and then apply k-means to the left singular subspaces of (cid:98)Pr to classify

all the states into several clusters. Figure 3 presents the clustering result with r = 4 and k = 4 for

the three periods of a day.

First of all, we notice that the locations within the same cluster are close with each other in geo-

graphical distance. This is non-trivial: we do not have exposure to GPS location in the clustering

analysis. This implies that taxi customers in neighboring locations have similar destination pref-

erence, which is consistent with common sense. Furthermore, to track the variation of the traﬃc

dynamics over time, Figure 4 visualizes the distribution of the destination choice that is corre-

spondent to the center of the green cluster in the morning, afternoon and evening respectively. We

identify the varying popular destinations in diﬀerent periods of the day and provide corresponding

explanations in the following table:

Time

Popular Destinations

Explanation

Morning

New York–Presbyterian Medical Center,

hospitals, workplaces,

42–59 St. Park Ave, Penn Station

the train station

Afternoon

66 St. Broadway

lunch, afternoon break,

Evening

Penn Station

short trips

go home

Finally, it might be tempting to model the taxi trips by an HMM, where regions of Manhattan

correspond to hidden states. However, such a region is always part of the current observation (i.e.,

Zhu et al.: Estimation of Markov Models

27

Figure 3

The meta-states compression of Manhattan traﬃc network via rank-constrained approach with r = 4:

mornings (left), afternoons (middle) and evenings (right). Each color or symbol represents a meta-state.

One can see the day-time state aggregation results diﬀer signiﬁcantly from that of the evening time.

location of taxi): It is observable and is not a hidden state that has to be inferred from all past

observations. As a result, although both HMM and the low-rank Markov model could apply to taxi

trips, the low-rank Markov model is simpler and more accurate.

6. Conclusion

This paper studies the recovery and state compression of low-rank Markov chains from empirical

trajectories via a rank-constrained likelihood approach. We provide statistical upper bounds for the

(cid:96)2 risk and Kullback-Leiber divergence between the estimator and the true probability transition

matrix for the proposed estimator. Then, a novel DC programming algorithm is developed to

solve the associated rank-constrained optimization problem. The proposed algorithm non-trivially

combines several recent optimization techniques, such as the penalty approach, the proximal DC

algorithm, and the multi-block sGS-ADMM. We further study a new class of majorized indeﬁnite-

proximal DC algorithms for solving general non-convex non-smooth DC programming problems

28

Zhu et al.: Estimation of Markov Models

Figure 4

Visualization of the destination distributions corresponding to the pick-up locations in the green clusters

in Figure 3: mornings (left), afternoons (middle) and evenings (right).

and provide a uniﬁed convergence analysis. Experiments on simulated data illustrate the merits of

our approach.

References

Adamczak R (2008) A tail inequality for suprema of unbounded empirical processes with applications to

Markov chains. Electronic Journal of Probability 13(34):1000–1034.

Anandkumar A, Ge R, Hsu D, Kakade SM, Telgarsky M (2014) Tensor decompositions for learning latent

variable models. The Journal of Machine Learning Research 15(1):2773–2832.

Azizzadenesheli K, Lazaric A, Anandkumar A (2016) Reinforcement learning in rich-observation MDPs using

spectral methods. arXiv preprint arXiv:1611.03907 .

Benson AR, Gleich DF, Lim LH (2017) The spacey random walk: A stochastic process for higher-order data.

SIAM Review 59(2):321–345.

Bertsekas DP (1995) Dynamic Programming and Optimal Control, volume 1 (Athena Scientiﬁc, Belmont,

MA).

12345678910-31234567810-32468101210-3Zhu et al.: Estimation of Markov Models

29

Bertsekas DP, Tsitsiklis JN (1995) Neuro-dynamic programming: an overview. Proceedings of the 34th IEEE

Conference on Decision and Control, volume 1, 560–564 (IEEE).

Boucheron S, Lugosi G, Massart P (2013) Concentration Inequalities: A Nonasymptotic Theory of Indepen-

dence (Oxford university press).

Br´emaud P (1999) Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues, volume 31 (Springer

Science & Business Media).

Buchholz P (1994) Exact and ordinary lumpability in ﬁnite Markov chains. Journal of Applied Probability

31(1):59–75.

Cao Y, Xie Y (2016) Poisson matrix recovery and completion. IEEE Transactions on Signal Processing

64(6):1609–1620.

Chen C, He B, Ye Y, Yuan X (2016) The direct extension of admm for multi-block convex minimization

problems is not necessarily convergent. Mathematical Programming 155(1-2):57–79.

Chen L, Sun D, Toh KC (2017) An eﬃcient inexact symmetric Gauss–Seidel based majorized ADMM for

high-dimensional convex composite conic programming. Mathematical Programming 161(1-2):237–270.

Chodera JD, Singhal N, Pande VS, Dill KA, Swope WC (2007) Automatic discovery of metastable states

for the construction of Markov models of macromolecular conformational dynamics. The Journal of

Chemical Physics 126(15).

Coifman RR, Kevrekidis IG, Lafon S, Maggioni M, Nadler B (2008) Diﬀusion maps, reduction coordinates,

and low dimensional representation of stochastic systems. Multiscale Modeling & Simulation 7(2):842–

864.

Deng K, Huang D (2012) Model reduction of Markov chains via low-rank approximation. American Control

Conference (ACC), 2012, 2651–2656 (IEEE).

Deng K, Mehta PG, Meyn SP (2011) Optimal Kullback-Leibler aggregation via spectral theory of Markov

chains. IEEE Transactions on Automatic Control 56(12):2793–2808.

E W, Li T, Vanden-Eijnden E (2008) Optimal partition and eﬀective dynamics of complex networks. Pro-

ceedings of the National Academy of Sciences 105(23):7907–7912.

30

Zhu et al.: Estimation of Markov Models

Fan J, Liu H, Sun Q, Zhang T (2018) I-LAMM for sparse learning: Simultaneous control of algorithmic

complexity and statistical error. The Annals of Statsitics 46(2):814–841.

Ferreira JFB, Khoo Y, Singer A (2017) Semideﬁnite programming approach for the quadratic assignment

problem with a sparse graph. Computational Optimization and Applications 1–36.

Fill JA (1991) Eigenvalue bounds on convergence to stationarity for nonreversible Markov chains, with an

application to the exclusion process. The Annals of Applied Probability 1(1):62–87.

Gao Y, Sun D (2010) A majorized penalty approach for calibrating rank constrained correlation matrix

problems. technical reprot .

Hall EC, Raskutti G, Willett R (2016) Inference of high-dimensional autoregressive generalized linear models.

arXiv preprint arXiv:1605.02693 .

Han Y, Jiao J, Weissman T (2015) Minimax estimation of discrete distributions under (cid:96)1 loss. IEEE Trans-

actions on Information Theory 61(11):6343–6354.

Hao Y, Orlitsky A, Pichapati V (2018) On learning Markov chains. Advances in Neural Information Pro-

cessing Systems, 648–657.

Hsu D, Kakade SM, Zhang T (2012) A spectral algorithm for learning hidden Markov models. Journal of

Computer and System Sciences 78(5):1460–1480.

Huang Q, Kakade SM, Kong W, Valiant G (2016) Recovering structured probability matrices. arXiv preprint

arXiv:1602.06586 .

Jiang B, Fan J, Sun Q (2018) Bernstein’s inequality for general markov chains. arXiv:1805.10721 .

Jiang K, Sun D, Toh KC (2014) A partial proximal point algorithm for nuclear norm regularized matrix

least squares problems. Mathematical Programming Computation 6(3):281–325.

Jiang X, Raskutti G, Willett R (2015) Minimax optimal rates for poisson inverse problems with physical

constraints. IEEE Transactions on Information Theory 61(8):4458–4474.

Keshavan RH, Montanari A, Oh S (2010) Matrix completion from a few entries. IEEE Transactions on

Information Theory 56(6):2980–2998.

Koltchinskii V, Lounici K, Tsybakov AB (2011) Nuclear-norm penalization and optimal rates for noisy

low-rank matrix completion. The Annals of Statistics 39(5):2302–2329.

Zhu et al.: Estimation of Markov Models

31

Kontorovich L (2007) Measure concentration of strongly mixing processes with applications. Ph.D. thesis,

Carnegie Mellon University, School of Computer Science, Machine Learning.

Kontorovich LA, Ramanan K (2008) Concentration inequalities for dependent random variables via the

martingale method. The Annals of Probability 36(6):2126–2158.

Lam XY, Marron JS, Sun D, Toh KC (2018) Fast algorithms for large-scale generalized distance weighted

discrimination. Journal of Computational and Graphical Statistics 27(2):368–379, URL http://dx.

doi.org/10.1080/10618600.2017.1366915.

Le Thi HA, Le HM, Phan DN, Tran B (2017) Stochastic DCA for the large-sum of non-convex functions

problem and its application to group variable selection in classiﬁcation. International Conference on

Machine Learning, 3394–3403.

Le Thi HA, Pham Dinh T (2018) DC programming and DCA: thirty years of developments. Mathematical

Programming 1–64, URL http://dx.doi.org/10.1007/s10107-018-1235-y.

Le Thi HA, Pham Dinh T, Van Ngai H (2012) Exact penalty and error bounds in DC programming. Journal

of Global Optimization 52(3):509–535.

Ledoux M, Talagrand M (2013) Probability in Banach Spaces: Isoperimetry and Processes (Springer Science

& Business Media).

Lehmann EL, Casella G (2006) Theory of Point Estimation (Springer Science & Business Media).

Levin DA, Peres Y (2017) Markov Chains and Mixing Times, volume 107 (American Mathematical Soc.).

Li M, Sun D, Toh KC (2016a) A majorized ADMM with indeﬁnite proximal terms for linearly constrained

convex composite optimization. SIAM Journal on Optimization 26(2):922–950.

Li X, Sun D, Toh KC (2016b) A Schur complement based semi-proximal ADMM for convex quadratic conic

programming and extensions. Mathematical Programming 155(1-2):333–373.

Liu Y, Kang C, Gao S, Xiao Y, Tian Y (2012) Understanding intra-urban trip patterns from taxi trajectory

data. Journal of Geographical Systems 14(4):463–483.

Loh PL, Wainwright MJ (2015) Regularized m-estimators with nonconvexity: Statistical and algorithmic

theory for local optima. The Journal of Machine Learning Research 16(1):559–616.

32

Zhu et al.: Estimation of Markov Models

Marton K (1996) Bounding ¯d-distance by informational divergence: A method to prove measure concentra-

tion. The Annals of Probability 24(2):857–866.

Moore AW (1991) Variable resolution dynamic programming: Eﬃciently learning action maps in multivariate

real-valued state-spaces. Machine Learning Proceedings 1991, 333–337 (Elsevier).

Negahban S, Oh S, Shah D (2016) Rank centrality: Ranking from pairwise comparisons. Operations Research

65(1):266–287.

Negahban S, Wainwright MJ (2011) Estimation of (near) low-rank matrices with noise and high-dimensional

scaling. The Annals of Statistics 39(2):1069–1097.

Negahban S, Wainwright MJ (2012) Restricted strong convexity and weighted matrix completion: Optimal

bounds with noise. Journal of Machine Learning Research 13(May):1665–1697.

Negahban SN, Ravikumar P, Wainwright MJ, Yu B (2012) A uniﬁed framework for high-dimensional analysis

of m-estimators with decomposable regularizers. Statistical Science 27(4):538–557.

Newman ME (2013) Spectral methods for community detection and graph partitioning. Physical Review E

88(4):042822.

Paulin D (2015) Concentration inequalities for markov chains by Marton couplings and spectral methods.

Electronic Journal of Probability 20.

Pham Dinh T, Le Thi Ha (1997) Convex analysis approach to DC programming: Theory, algorithms and

applications. Acta Mathematica Vietnamica 22(1):289–355.

Pham Dinh T, Le Thi Ha (2005) The DC (diﬀerence of convex functions) programming and DCA revisited

with DC models of real world nonconvex optimization problems. Annals of Operations Research 133(1-

4):23–46.

Rockafellar RT (2015) Convex Analysis (Princeton University Press).

Rohrdanz MA, Zheng W, Maggioni M, Clementi C (2011) Determination of reaction coordinates via locally

scaled diﬀusion map. The Journal of Chemical Physics 134(12):03B624.

Singh SP, Jaakkola T, Jordan MI (1995) Reinforcement learning with soft state aggregation. Advances in

Neural Information Processing Systems, 361–368.

Zhu et al.: Estimation of Markov Models

33

Steinhaus H (1957) The problem of estimation. The Annals of Mathematical Statistics 28(3):633–648.

Sutton RS, Barto AG (1998) Reinforcement Learning: An Introduction, volume 1 (MIT press Cambridge).

Tropp JA (2011) Freedman’s inequality for matrix martingales. Electronic Communications in Probability

16:262–270.

Van Dinh B, Kim DS, Jiao L (2015) Convergence analysis of algorithms for DC programming. arXiv preprint

arXiv:1508.03899 .

Wang B, Zou H (2018) Another look at distance-weighted discrimination. Journal of the Royal Statistical

Society: Series B (Statistical Methodology) 80(1):177–198.

Watson G (1993) On matrix approximation problems with Ky Fank norms. Numerical Algorithms 5(5):263–

272.

Wen B, Chen X, Pong TK (2017) A proximal diﬀerence-of-convex algorithm with extrapolation. Computa-

tional Optimization and Applications 1–28.

Wolfer G, Kontorovich A (2019a) Estimating the mixing time of ergodic Markov chains. COLT .

Wolfer G, Kontorovich A (2019b) Minimax learning of ergodic Markov chains. International Conference on

Algorithmic Learning Theory .

Yang LF, Braverman V, Zhao T, Wang M (2017) Dynamic partition of complex networks. arXiv preprint

arXiv:1705.07881 .

Yu B (1997) Assouad, fano, and le cam. Festschrift for Lucien Le Cam, 423–435 (Springer).

Zhang A, Wang M (2017) Spectral state compression of Markov processes. arXiv preprint arXiv:1802.02920

.

Zhang A, Wang M (2019) Spectral state compression of markov processes. IEEE Transactions on Information

Theory .

e-companion to Zhu et al.: Estimation of Markov Models

ec1

Technical lemmas and proofs

EC.1. Technical lemmas

Lemma EC.1. Given two discrete distributions u, v ∈ Rp, if there exist α, β > 0 such that uj ∈

{0} ∪ [α/p, β/p] and vj ∈ [α/p, β/p] for any j ∈ [p], then we have

DKL(u, v) ≥ {pα/(2β2)}(cid:107)u − v(cid:107)2
2.

This implies that under Assumption 1, for any Q ∈ C,

(cid:107)P − Q(cid:107)2

F ≤

2β2
απminp

DKL(P, Q).

Proof of Lemma EC.1 By the mean value theorem, for any j ∈ [p] such that uj (cid:54)= 0, there exists

ξj ∈ [α/p, β/p] such that

Therefore,

log(vj) − log(uj) =

vj − uj
uj

−

(vj − uj)2
2ξ2
j

.

DKL(u, v) =

(cid:88)

j:uj (cid:54)=0

uj log(uj/vj) =

(cid:88)

(uj − vj) +

j:uj (cid:54)=0

(cid:88)

j:uj (cid:54)=0

(uj − vj)2
2ξ2
j

≥ 1 −

(cid:88)

vj +

(cid:88)

j:uj (cid:54)=0

j:uj (cid:54)=0

pα(uj − vj)2
2β2

=

(cid:88)

j:uj =0

vj − uj +

(cid:88)

j:uj (cid:54)=0

pα(uj − vj)2
2β2

(cid:88)

≥

j:uj =0

p(vj − uj)2
β

+

(cid:88)

j:uj (cid:54)=0

pα(uj − vj)2
2β2

≥

pα
2β2

(cid:107)u − v(cid:107)2
2.

Then we have

(cid:107)P − Q(cid:107)2

F =

(cid:88)

i∈[p]

(cid:107)Pi· − Qi·(cid:107)2

2 ≤

(cid:88)

i∈[p]

2β2πi
pαπmin

DKL(Pi·, Qi·) =

2β2
pαπmin

DKL(P, Q).

EC.2. Proof of Theorem 1

Given the deﬁnition of (cid:98)P,

(cid:101)DKL(P, (cid:98)P) =

1
n

n
(cid:88)

(cid:104)log(P) − log( (cid:98)P), Xi(cid:105) = (cid:96)n( (cid:98)P) − (cid:96)n(P) ≤ λ((cid:107) (cid:98)P(cid:107)∗ − (cid:107)P(cid:107)∗) ≤ λ(cid:107)P − (cid:98)P(cid:107)∗. (EC.1)

i=1

ec2

Then we have

e-companion to Zhu et al.: Estimation of Markov Models

DKL(P, (cid:98)P) = L( (cid:98)P) − L(P) = L( (cid:98)P) − (cid:96)n( (cid:98)P) + (cid:96)n( (cid:98)P) − (cid:96)n(P) + (cid:96)n(P) − L(P)

≤ L( (cid:98)P) − (cid:96)n( (cid:98)P) + (cid:96)n(P) − L(P) + λ(cid:107)P − (cid:98)P(cid:107)∗

(EC.2)

= DKL(P, (cid:98)P) − (cid:101)DKL(P, (cid:98)P) + λ(cid:107)P − (cid:98)P(cid:107)∗.

Deﬁne E := {λ ≥ 2(cid:107)ΠN (∇(cid:96)n(P))(cid:107)2}. If E holds, then by Lemma 1 and then Lemma EC.1, we obtain

that

DKL(P, (cid:98)P) ≤ DKL(P, (cid:98)P) − (cid:101)DKL(P, (cid:98)P) + 4(2r)1/2λ(cid:107)P − (cid:98)P(cid:107)F
(cid:18) rDKL(P, (cid:98)P)
pπminα

≤ DKL(P, (cid:98)P) − (cid:101)DKL(P, (cid:98)P) + 8λβ

(cid:19)1/2

.

For any ξ > 1, an application of Lemma 3 with η = ξπmin/(rpπmax log p) yields

(cid:20)(cid:26)

DKL(P, (cid:98)P) ≤ 16λβ

P

(cid:18) rDKL(P, (cid:98)P)
pπminα

(cid:19)1/2

+

2C1rπmaxβ2p log p
πminα3n

(cid:27)

(cid:21)

+ η

∩ E

≥ 1 − C2e−ξ − P(E c),

where C1 and C2 are exactly the same constants as in Lemma 3. Some algebra yields that

(cid:20)(cid:26)

DKL(P, (cid:98)P) ≤

P

256λ2β2r
pπminα

+

2C1rπmaxβ2p log p
πminα3n

(cid:27)

(cid:21)

+ η

∩ E

≥ 1 − C2e−ξ − P(E c).

By Lemma 2, there exists a universal constant C3 > 0 such that if we choose

λ = C3

(cid:26)(cid:18) ξp2πmax log p

(cid:19)1/2

nα

+

ξp log p
nα

(cid:27)
,

then for any ξ > 1, whenever nπmax(1 − ρ+) ≥ max(20, ξ2) log p, we have that

(cid:18)

P

DKL(P, (cid:98)Pr) (cid:38) ξrπmaxβ2p log p

πminα3n

(cid:19)

+

ξπmin
rpπmax log p

(cid:46) e−ξ + p−(ξ−1) + p−10,

as desired. The Frobenius-norm error bound follows immediately by applying Lemma EC.1.

EC.3. Proof of Theorem 2

Given the deﬁnition of (cid:98)Pr,

(cid:101)DKL(P, (cid:98)Pr) =

1
n

n
(cid:88)

i=1

Then we have

(cid:104)log(P) − log( (cid:98)Pr), Xi(cid:105) = (cid:96)n( (cid:98)Pr) − (cid:96)n(P) ≤ 0.

(EC.3)

DKL(P, (cid:98)Pr) = L( (cid:98)Pr) − L(P) = L( (cid:98)Pr) − (cid:96)n( (cid:98)Pr) + (cid:96)n( (cid:98)Pr) − (cid:96)n(P) + (cid:96)n(P) − L(P)

(EC.4)

≤ L( (cid:98)Pr) − (cid:96)n( (cid:98)Pr) + (cid:96)n(P) − L(P) = DKL(P, (cid:98)Pr) − (cid:101)DKL(P, (cid:98)Pr).

e-companion to Zhu et al.: Estimation of Markov Models

ec3

For any ξ > 1, an application of Lemma 3 with η = πminξ/(rpπmax log p) yields

(cid:26)

DKL(P, (cid:98)Pr) ≥ max

P

(cid:18) 2C1rπmaxβ2p log p
πminα3n

,

ξα2
rp2πmax log p

(cid:19)(cid:27)

≤ C2e−ξ,

as desired. The Frobenius-norm error bound immediately follows by Lemma EC.1.

EC.4. Proof of Theorem 3

To simplify the notation, assume without loss of generality that p is a multiple of 4(r − 1). For

any 1 ≤ k ≤ m, consider

(cid:20)

P(k) =

2−α

p 1p×(p/2)


(cid:21)

α

p 1p×(p/2)

+

η(2 − α)
2p

R(k)








−R(k)

(cid:124)

0(p/2)×(p/4)

0(p/2)×(p/4)

0(p/2)×(p/2)

· · ·

R(k) −R(k)

· · · − R(k) 0(p/4)×(p/2)

· · · − R(k)
(cid:123)(cid:122)
(cid:125)
l0

(cid:124)

R(k)

· · ·
(cid:123)(cid:122)
l0

R(k)
(cid:125)

0(p/4)×(p/2)












,

(EC.5)

where l0 = p

4(r−1) , R(k) ∈ {0, 1}(p/4)×(r−1), and η is some positive value to be determined later. Let

µ :=

(cid:18) 2 − α
p

1(cid:62)
p/2

(cid:19)(cid:62)

.

1(cid:62)
p/2

α
p

(EC.6)

First of all, regardless of the value of R(k), one can see that for any k ∈ [m],

1. rank(P(k)) ≤ r;

2. µ(cid:62)P(k) = µ(cid:62), and hence µ is the invariant distribution of P(k);

3. P(k) ∈ Θ.

Let {R(k)}m

k=1 be i.i.d. matrices of

independent Rademacher entries,

i.e.,

for any k ∈ [m],

{R(k)

ij }i∈[n],j∈[d] are independent Rademacher variables, and {R(k)}k∈[m] are independent. For any

k (cid:54)= l, one can see that (cid:8)(cid:12)

(cid:12)R(k)

ij − R(l)

ij

(cid:9) are i.i.d. uniformly distributed on {0, 2}, and that

(cid:12)
(cid:12)

E(cid:12)
(cid:12)R(k)

ij − R(l)

ij

(cid:12) = 1, Var(cid:0)(cid:12)
(cid:12)

(cid:12)R(k)

ij − R(l)

ij

(cid:1) = 1,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)R(k)

ij − R(l)

ij

(cid:12) − 1(cid:12)
(cid:12)

(cid:12) = 1.

By Bernstein’s inequality (Boucheron et al. 2013, Theorem 2.10), for any t > 0,

P

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)R(k) − R(l)(cid:13)
(cid:13)
(cid:13)1

−

p(r − 1)
4

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) p(r − 1)t
2

≥

(cid:19)1/2

(cid:27)

+ t

≤ 2e−t.

ec4

e-companion to Zhu et al.: Estimation of Markov Models

Let t = p(r − 1)/64 and m = (cid:98)exp{p(r − 1)/128}/21/2(cid:99). Since p(r − 1) ≥ 192 log 2, we have that

m ≥ 2. Then a union bound yields that

(cid:18)

P

∀1 ≤ k < l ≤ m,

p(r − 1)
8

≤ (cid:13)

(cid:13)R(k) − R(l)(cid:13)
(cid:13)1

≤

(cid:19)

3p(r − 1)
8

≥ 1 − 2m2 exp

(cid:18) −p(r − 1)
64

(cid:19)

> 0.

Hence, there exist R(1), . . . , R(m) ⊆ {−1, 1}(p/4)×(r−1) such that

∀1 ≤ k < l ≤ m,

p(r − 1)
8

≤ (cid:13)

(cid:13)R(k) − R(l)(cid:13)
(cid:13)1

≤

3p(r − 1)
8

,

which, given that (cid:107)R(j) − R(k)(cid:107)2

F = 2(cid:107)R(j) − R(k)(cid:107)1, further implies that

∀1 ≤ k < l ≤ m,

p(r − 1)
4

≤ (cid:107)R(k) − R(l)(cid:107)2

F ≤

3p(r − 1)
4

.

Now we have that

(cid:13)P(k) − P(l)(cid:13)
(cid:13)
(cid:13)1

=

2l0η(2 − α)
p

(cid:107)R(k) − R(l)(cid:107)1 ≥

(cid:107)P(k) − P(l)(cid:107)2

F =

l0η2(2 − α)2
p2

(cid:107)R(k) − R(l)(cid:107)2

F ≥

Besides,

ηp(2 − α)
16
η2(2 − α)2
16

(cid:13)P(k) − P(l)(cid:13)
, (cid:13)
(cid:13)1

≤

(cid:13)P(k) − P(l)(cid:13)
, (cid:13)
(cid:13)1

≤

3ηp(2 − α)
16
3η2(2 − α)2
16

(EC.7)

(EC.8)

,

.

DKL(X (k)||X (l)) = n

(cid:88)

i∈[p]

πiDKL

(cid:0)P(k)

[i,:], P(l)

[i,:]

p
(cid:88)

p/2
(cid:88)

(cid:1) = n

i=(p/2)+1

j=1

α
p

P (k)
ij

log(cid:0)P (k)

ij /P (l)

ij

(cid:1)

=

2nα
p

p/4
(cid:88)

i=1

2 − α
2

DKL

(cid:0)u(k)

i

, u(l)
i

(cid:1),

(cid:104)

R(k)

[i,:] · · · R(k)

[i,:] − R(k)

[i,:] · · · − R(k)

[i,:]

(cid:105)

corresponds to a (p/2)-dimensional dis-

where u(k)

i = 2

p 1p/2 + η

p

tribution. By Zhang and Wang (2019, Lemma 4), we have that

DKL

(cid:0)u(k)

i

, u(l)
i

(cid:1) ≤

3l0η2
p

(cid:13)
(cid:13)R(k)

[i,:] − R(l)

[i,:]

(cid:13)
2
(cid:13)
2

=

6l0η2
p

(cid:13)
(cid:13)R(k)

[i,:] − R(l)

[i,:]

(cid:13)
(cid:13)1

.

Therefore,

DKL(X (k), X (l)) =

6nα(2 − α)l0η2
p2

p/4
(cid:88)

(cid:13)
(cid:13)R(k)

[i,:] − R(l)

[i,:]

(cid:13)
(cid:13)1

≤

12nαl0η2
p2

(cid:107)R(k) − R(l)(cid:107)1

≤

12nαl0η2
p2

i=1
3p(r − 1)
8

=

9nη2α
8

.

e-companion to Zhu et al.: Estimation of Markov Models

ec5

By Fano’s inequality (Yu 1997, Lemma 3), we have that

inf
(cid:98)P

sup
P∈Θ

(cid:107) (cid:98)P − P(cid:107)2

F ≥ inf
(cid:98)P

sup
P∈{P(1),...,P(m)}

(cid:107) ˆP − P(cid:107)2

F ≥

η2(2 − α)2
16

(cid:18)

1 −

9nη2α − log 2
log m

(cid:19)

.

There exist universal constants c1, c2 > 0 such that when p(r − 1) ≥ 192 log 2, choosing η = c1{p(r −

1)/(nα)}1/2 yields that

(cid:3)

EC.5. Proof of Theorem 4

inf
(cid:98)P

sup
P∈Θ

(cid:107) (cid:98)P − P(cid:107)2

F ≥ c2

p(r − 1)
nα

.

Let (cid:98)U⊥, (cid:98)V⊥ ∈ (cid:60)p×(p−r) be the orthogonal complement of (cid:98)U and (cid:98)V. Since U, V, (cid:98)U, and (cid:98)V are

the leading left and right singular vectors of P and (cid:98)P, we have

(cid:107) (cid:98)P − P(cid:107)F ≥(cid:107) (cid:98)U(cid:62)

⊥( (cid:98)P − UU(cid:62)P)(cid:107)F = (cid:107) (cid:98)U(cid:62)

⊥UU(cid:62)P(cid:107)F ≥ (cid:107) (cid:98)U(cid:62)

⊥U(cid:107)F σr(U(cid:62)P) = (cid:107) sin Θ( (cid:98)U, U)(cid:107)F σr(P).

Similar argument also applies to (cid:107) sin Θ( (cid:98)V, V)(cid:107). Thus,

max(cid:0)(cid:107) sin Θ( (cid:98)U, U)(cid:107)F , (cid:107) sin Θ( (cid:98)V, V)(cid:107)F

(cid:1) ≤ min

(cid:18) (cid:107) (cid:98)P − P(cid:107)F
σr(P)

(cid:19)

.

, r1/2

The rest of the proof immediately follows from Theorem 1.

EC.6. Proof of Lemma 1

By the inequality (52) in Lemma 3 in the Appendix of Negahban and Wainwright (2012), we

have for any ∆ ∈ (cid:60)p×p,

Besides,

(cid:107)P + ∆(cid:107)∗ − (cid:107)P(cid:107)∗ ≥ (cid:107)∆

⊥(cid:107)∗ − (cid:107)∆M(cid:107)∗ − 2(cid:107)PM⊥(cid:107)∗.

M

(cid:96)n(P + ∆) − (cid:96)n(P) ≥ (cid:104)∇(cid:96)n(P), ∆(cid:105) = (cid:104)ΠN (∇(cid:96)n(P)), ∆(cid:105) ≥ −|(cid:104)ΠN (∇(cid:96)n(P)), ∆(cid:105)|

≥ −(cid:107)ΠN (∇(cid:96)n(P))(cid:107)2(cid:107)∆(cid:107)∗ ≥ −

λ
2

(cid:0)(cid:107)∆M(cid:107)∗ + (cid:107)∆

⊥(cid:107)∗

(cid:1).

M

By the optimality of (cid:98)P, (cid:96)n( (cid:98)P) + λ(cid:107) (cid:98)P(cid:107)∗ ≤ (cid:96)n(P) + λ(cid:107)P(cid:107)∗. Therefore,

λ(cid:0)(cid:107)∆M(cid:107)∗ + 2(cid:107)PM⊥(cid:107)∗ − (cid:107)∆

M

⊥(cid:107)∗

(cid:1) ≥ λ((cid:107)P(cid:107)∗ − (cid:107) (cid:98)P(cid:107)∗) ≥ −

λ
2

(cid:0)(cid:107) (cid:98)∆M(cid:107)∗ + (cid:107) (cid:98)∆

⊥(cid:107)∗

M

(cid:1),

from which we deduce that

(cid:107) (cid:98)∆

M

⊥(cid:107)∗ ≤ 3(cid:107) (cid:98)∆M(cid:107)∗ + 4(cid:107)PM⊥(cid:107)∗.

ec6

e-companion to Zhu et al.: Estimation of Markov Models

EC.7. Proof of Lemma 2

Some algebra yields that

∇(cid:96)n(Q) =

1
n

n
(cid:88)

i=1

−

Xi
(cid:104)Q, Xi(cid:105)

.

(EC.9)

For ease of notation, write Zi := −Xi/(cid:104)P, Xi(cid:105). Note that Zi is well-deﬁned almost surely. Besides,

E(Zi|Zi−1) = E(Zi|Xi−1) =

p
(cid:88)

j=1

−

eXi−1e(cid:62)
j
PXi−1,j

PXi−1,j = −eXi−11(cid:62).

Thus (cid:107)Zi − E(Zi|Zi−1)(cid:107)2 ≤ p/α +

√

is a matrix martingale. In addition,

p =: R < ∞. Deﬁne Sk := (cid:80)k

i=1 Zi − E(Zi|Zi−1), then {Sk}n

k=1

E(cid:8)(Zi − E(Zi|Zi−1))(cid:62)(Zi − E(Zi|Zi−1))|{Sk}i−1

k=1

(cid:9) = E(cid:8)(Zi − E(Zi|Zi−1))(cid:62)(Zi − E(Zi|Zi−1))|Zi−1
(cid:18) p

(cid:19)

(cid:9)

= E(Z(cid:62)

i Zi|Zi−1) − E(Zi|Zi−1)(cid:62)E(Zi|Zi−1) =

− 11(cid:62) =: W(1)

,

i

(cid:88)

eje(cid:62)
j
PXi−1,j

and similarly,

j=1

E(cid:8)(Zi − E(Zi|Zi−1))(Zi − E(Zi|Zi−1))(cid:62)|{Sk}i−1

k=1

(cid:9) =

(cid:18) p

(cid:88)

j=1

(cid:19)

eXi−1e(cid:62)
PXi−1,j

Xi−1

− peXi−1e(cid:62)

Xi−1

=: W(2)

i

.

Write (cid:107) (cid:80)n

i=1 W(1)

i (cid:107)2 as W (1)

n , (cid:107) (cid:80)n

i=1 W(2)

i (cid:107)2 as W (2)

n

and max(W (1)

n , W (2)

n ) as Wn. By the matrix

Freedman inequality (Tropp 2011, Corollary 1.3), for any t ≥ 0 and σ2 > 0,

P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) ≤ 2p exp

(cid:18)

−

t2/2
σ2 + Rt/3

(cid:19)

.

(EC.10)

Now we need to choose an appropriate σ2 so that Wn ≤ σ2 holds with high probability. Note

that W (1)

n ≤ np(α−1 + 1) and W (2)

n ≤ (p2α−1 − p) supj∈[p]

(cid:80)n

i=1 1{Xi=sj }. In the following we derive

a bound for supj∈[p]

(cid:80)n

i=1 1{Xi=sj }. For any j ∈ [p], by Jiang et al. (2018, Theorem 1.2), which is a

variant of Bernstein’s inequality for Markov chains, we have that

P

(cid:26) 1
n

where

n
(cid:88)

i=1

(1{Xi=sj } − πj) > (cid:15)

(cid:27)

(cid:18)

≤ exp

−

n(cid:15)2
2(A1β/p + A2(cid:15))

(cid:19)

,

(EC.11)

A1 =

1 + max(ρ+, 0)
1 − max(ρ+, 0)

and A2 =

1
3

1{ρ+≤0} +

5
1 − ρ+

1{ρ+>0}.

e-companion to Zhu et al.: Estimation of Markov Models

ec7

Some algebra yields that for any ξ > 0,

P

(cid:26) 1
n

n
(cid:88)

i=1

1{Xi=sj } − πj >

(cid:19)1/2

(cid:18) 4A1ξ
np

(cid:27)

+

4A2ξ
n

≤ exp(−ξ).

A union bound over j ∈ [p] yields that

P

(cid:26)

sup
j∈[p]

1
n

n
(cid:88)

i=1

(1{Xi=sj } − πj) >

(cid:18) 4A1ξ log p
np

(cid:19)1/2

(cid:27)

+

4A2ξ log p
n

≤ p−(ξ−1),

which implies that

P

(cid:26)

sup
j∈[p]

1
n

n
(cid:88)

i=1

1{Xi=sj } > πmax +

(cid:18) 4A1ξ log p
np

(cid:19)1/2

(cid:27)

+

4A2ξ log p
n

≤ p−(ξ−1).

Therefore, whenever nπmax(1 − ρ+) ≥ 2 log p, we have that

(cid:18)

P

1
n

n
(cid:88)

i=1

sup
j∈[p]

1{Xi=sj } (cid:38) πmax

(cid:19)

(cid:18)

≤ exp

−

nπmax(1 − ρ+)
2

(cid:19)

.

Combining this with the bounds of W (1)

n and W (2)

n , we have that

(cid:18)

P

Wn ≥

C1np2πmax
α

(cid:19)

(cid:18)

≤ exp

−

nπmax(1 − ρ+)
2

(cid:19)

,

where C1 is a universal constant. Now choosing σ2 = C1np2πmax/α, we deduce that for any t ≥ 0,

P((cid:107)Sn(cid:107)2 ≥ t) = P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) + P((cid:107)Sn(cid:107)2 ≥ t, Wn > σ2)

≤ P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) + P(Wn > σ2)

(cid:18)

≤ 2p exp

−

t2/2
σ2 + Rt/3

(cid:19)

(cid:18)

+ exp

−

nπmax(1 − ρ+)
2

(cid:19)

.

Equivalently, for any ξ > 1,

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

Sn

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:38)

(cid:18) ξp2πmax log p
nα

(cid:19)1/2

(cid:27)

+

ξp log p
nα

≤ 4p−(ξ−1) + exp

(cid:18)

−

nπmax(1 − ρ+)
2

(cid:19)

.

Finally, observe that

for any i ∈ [n], ΠN (E(Zi|Zi−1)) = ΠN (−eXi−11(cid:62)) = 0. Therefore,

ΠN (∇(cid:96)n(P)) = n−1Sn and the ﬁnal conclusion then follows.

ec8

e-companion to Zhu et al.: Estimation of Markov Models

EC.8. Proof of Lemma 3

We ﬁrst split C(η) as the union of the sets

Cl := (cid:8)Q ∈ C(η) : 2l−1η ≤ DKL(P, Q) ≤ 2lη, rank(Q) ≤ r(cid:9) ,

l = 1, 2, 3, . . . .

Deﬁne

γl = sup
Q∈Cl

= sup
Q∈Cl

(cid:12)DKL(P, Q) − (cid:101)DKL(P, Q)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

1
n

i=1

(cid:104)log(P) − log(Q), Xi(cid:105) − E(cid:104)log(P) − log(Q), Xi(cid:105)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

First, we wish to apply Adamczak (2008, Theorem 7) to bound |γl − Eγl|. Adamczak’s bound

entails the following asymptotic weak variance

σ2 := sup
Q∈Cl

(cid:26) S2(cid:88)

Var

i=S1+1

(cid:104)log(P) − log(Q), Xi(cid:105) − E(cid:104)log(P) − log(Q), Xi(cid:105)

(cid:27)

/ET2.

We have that

σ2 ≤ sup
Q∈Cl

(cid:20)(cid:26) S2(cid:88)
E

(cid:104)log(P) − log(Q), Xi(cid:105) − E(cid:104)log(P) − log(Q), Xi(cid:105)

(cid:27)2(cid:21)

/ET2

i=S1+1

∞
(cid:88)

(cid:20)(cid:26) S2(cid:88)
E

(cid:104)log(P) − log(Q), Xi(cid:105) − E(cid:104)log(P) − log(Q), Xi(cid:105)

(cid:27)2

(cid:21)

1{T2=j}

j=1

i=S1+1

sup
Q∈Cl

∞
(cid:88)

j=1

4j2 log2(β/α)P(T2 = j) = 2 log2(β/α)E(T 2

2 ) = 8 log2(β/α).

=

≤

1
2

1
2

By Adamczak (2008, Theorem 7), there exists a universal constant K > 1 such that for any ξ > 0,

(cid:26)

P

|γl − Eγl| ≥ KEγl + 2 log(β/α)

(cid:19)1/2

(cid:18) 2Kξ
n

+

16K log(β/α)ξ log n
n

(cid:27)

≤ Ke−ξ.

Since n−1/2 ≥ 2n−1 log n for any positive integer n, we have that

(cid:26)

|γl − Eγl| ≥ KEγl + 11K log(β/α)

P

(cid:19)1/2(cid:27)

(cid:18) ξ
n

≤ Ke−ξ.

(EC.12)

Next, we bound Eγl. Let {εi}n

i=1 be n independent Rademacher random variables. By a symmetriza-

tion argument,

Eγl =E

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
Q∈Cl
(cid:18)

sup
Q∈Cl

i=1
n
1
(cid:88)
n

i=1

≤2E

n
(cid:88)

(cid:12)
(cid:12)
(cid:104)log(P) − log(Q), Xi(cid:105) − E(cid:104)log(P) − log(Q), Xi(cid:105)
(cid:12)
(cid:12)

(cid:19)

εk(cid:104)log(P) − log(Q), Xi(cid:105)

(cid:19)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

e-companion to Zhu et al.: Estimation of Markov Models

ec9

Let φi(t) = (α/p){log((cid:104)P, Xi(cid:105) + t) − log((cid:104)P, Xi(cid:105))}. Then φi(0) = 0 and |φ(cid:48)

i(t)| ≤ 1 for all t such that

t + (cid:104)P, Xi(cid:105) ≥ α/p. In other words, φi is a contraction map for t ≥ minj,k∈[p](Pjk − α/p). By the

contraction principle (Theorem 4.12 in Ledoux and Talagrand (2013)),

Eγl ≤

≤

(cid:18)

E

(cid:18)

E

2p
α

4p
α

sup
Q∈Cl

sup
Q∈Cl

(cid:12)
1
(cid:12)
(cid:12)
n
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

By Lemma EC.1,

i=1
n
(cid:88)

i=1

εiXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

n
(cid:88)

εiφi ((cid:104)Q − P, Xi(cid:105))

(cid:19)

(cid:18)

E

≤

4p
α

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

εi(cid:104)Q − P, Xi(cid:105)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(EC.13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)

sup
Q∈Cl
n
(cid:88)

i=1

4p
α

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

i=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:107)Q − P(cid:107)∗

≤

εiXi

(cid:107)Q − P(cid:107)∗.

sup
Q∈Cl

sup
Q∈Cl

(cid:107)Q − P(cid:107)∗ ≤ sup
Q∈Cl

(2r)1/2(cid:107)Q − P(cid:107)F ≤ 2β

(cid:18) 2lηr
pαπmin

(cid:19)1/2

.

(EC.14)

Hence, the remaining task is to bound E(cid:107)n−1 (cid:80)n

i=1 εiXi(cid:107). From now on, we denote εiXi by Zi. One

can see that (Zi)n

i=1 is a martingale diﬀerence sequence. We wish to apply the matrix Freedman

inequality (Tropp 2011, Corollary 1.3) to bound the average of (Zi)n

i=1. We have that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

E(cid:0)Z(cid:62)

i Zi|Xi−1

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

= max
j∈[p]

and that

PXi−1,j(eXi−1e(cid:62)

(cid:13)
(cid:13)
j )(cid:62)(eXi−1e(cid:62)
j )
(cid:13)
(cid:13)2

=

PXi−1,j =: W (1)

n

p
(cid:88)

j=1
n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

p
(cid:88)

n
(cid:88)

PXi−1,jeje(cid:62)

j

j=1

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

E(cid:0)ZiZ(cid:62)

i |Xi−1

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

= max
j∈[p]

PXi−1,jeXi−1e(cid:62)

Xi−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

=

1{Xi−1=j} =: W (2)
n .

p
(cid:88)

j=1
n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

eXi−1e(cid:62)

Xi−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

We ﬁrst bound W (1)

n . Note that for any j ∈ [p], E(PXi−1,j) = πj, and that

Varπ(PXi−1,j) =

p
(cid:88)

k=1

πk(Pkj − πj)2 =

p
(cid:88)

k=1

πkP 2

kj − π2

j ≤ πj(1 − πj).

By a variant of Bernstein’s inequality for Markov chains (Jiang et al. 2018, Theorem 1.2), we have

that for any j ∈ [p],

P

(cid:18) 1
n

n
(cid:88)

i=1

(cid:19)

(cid:26)

PXi−1,j − πj > (cid:15)

≤ exp

−

n(cid:15)2
2(A1πj + A2(cid:15))

(cid:27)

,

ec10

where

e-companion to Zhu et al.: Estimation of Markov Models

A1 :=

1 + max(ρ+, 0)
1 − max(ρ+, 0)

and A2 :=

1
3

1{ρ+≤0} +

5
1 − ρ+

1{ρ+>0}.

A union bound yields that

P(cid:8)W (1)

n ≥ nπmax + (4nA1πmaxξ log p)1/2 + 4A2ξ log p(cid:9) ≤ p−(ξ−1).

(EC.15)

Next we bound W (2)

n . Note that W (2)

n ≤ maxj∈[p]

(cid:80)n

i=1 1{Xi−1=sj }. Similarly, by Jiang et al. (2018,

Theorem 1.2), for any j ∈ [p],

P

(cid:26) 1
n

n
(cid:88)

i=1

1{Xi−1=sj } − πj > (cid:15)

≤ exp

−

(cid:27)

(cid:26)

n(cid:15)2
2(A1πj + A2(cid:15))

(cid:27)
,

(EC.16)

Some algebra yields that for any ξ > 0,

P

(cid:26) 1
n

n
(cid:88)

i=1

1{Xi−1=sj } − πj >

(cid:19)1/2

(cid:18) 4A1πjξ
n

(cid:27)

+

4A2ξ
n

≤ exp(−ξ).

By a union bound over j ∈ [p],

(cid:26)

P

max
j∈[p]

1
n

n
(cid:88)

i=1

1{Xi−1=sj } > πmax +

(cid:18) 4A1πmaxξ log p
n

(cid:19)1/2

(cid:27)

+

4A2ξ log p
n

≤ p−(ξ−1),

which further implies that

P(cid:8)W (2)

n ≥ nπmax + (4nA1πmaxξ log p)1/2 + 4A2ξ log p(cid:9) ≤ p−(ξ−1).

(EC.17)

Deﬁne Wn := max(W (1)

n , W (2)

n ). Let Sn := (cid:80)n

i=1 εiXi. By matrix Freedman’s inequality (Tropp 2011,

Corollary 1.3), for any t ≥ 0 and σ2 > 0,

P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) ≤ 2p exp

(cid:18)

−

t2/2
σ2 + t/3

(cid:19)

.

(EC.18)

Now we need to choose an appropriate σ2 so that Wn ≤ σ2 holds with high probability. Given that

ρ+ > 0 and nπmax ≥ 10ξ log p/(1 − ρ+), combining (EC.15) and (EC.17) yields that

P(cid:0)Wn ≥ 4nπmax

(cid:1) ≤ 2p−(ξ−1).

(EC.19)

e-companion to Zhu et al.: Estimation of Markov Models

ec11

Now choosing σ2 = 4nπmax in (EC.18), we deduce that

P((cid:107)Sn(cid:107)2 ≥ t) = P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) + P((cid:107)Sn(cid:107)2 ≥ t, Wn > σ2)

≤ P((cid:107)Sn(cid:107)2 ≥ t, Wn ≤ σ2) + P(Wn > σ2)

(cid:18)

≤ 2p exp

−

(cid:19)

t2/2
σ2 + t/3

+ 2p−(ξ−1).

Choose ξ = nπmax(1 − ρ+)/(10 log p). As long as nπmax(1 − ρ+) ≥ max(20 log p, log n), we have that

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

Sn

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:46)

(cid:18) πmax log p
n

(cid:19)1/2

.

(EC.20)

Combining (EC.13), (EC.14) and (EC.20) yields that

Eγl (cid:46) β
α3/2

(cid:18) 2lηπmaxrp log p
πminn

(cid:19)1/2

.

Then combining this with (EC.12) yields that

P

(cid:26)
γl (cid:38) β
α3/2

(cid:18) 2lηπmaxrp log p
πminn

(cid:19)1/2

+ log(β/α)

(cid:19)1/2(cid:27)

(cid:18) ξ
n

(cid:46) e−ξ.

Let ξ = 2lηπmaxrp log p/πmin. Then there exist universal constants C1, C2 > 0 such that

(cid:26)

γl ≥

P

C1β
α3/2

(cid:18) 2lηπmaxrp log p
πminn

(cid:19)1/2(cid:27)

(cid:26)

≤ C2 exp

−

(2l + 1)ηπmaxrp log p
πmin

(cid:27)
.

We can thus deduce that there exists a universal constant C3 > 0 such that

(cid:18)

P

| (cid:101)DKL(P, Q) − DKL(P, Q)| >

1
2

DKL(P, Q) +

C3πmaxβ2rp log p
πminα3n

(cid:19)

≤

≤

∞
(cid:88)

l=0
∞
(cid:88)

l=0

(cid:18)

P

∃Q ∈ Cl,

(cid:12)
(cid:12)
(cid:12) (cid:101)DKL(P, Q) − DKL(P, Q)

(cid:12)
(cid:12)
(cid:12) > 2l−2η +

C3πmaxβ2rp log p
πminα3n

(cid:19)

(cid:26)

P

γl ≥

C1β
α3/2

(cid:18) 2lηπmaxrp log p
πminn

(cid:19)1/2(cid:27)

≤C2

(cid:26)

exp

−

∞
(cid:88)

l=0

(2l + 1)ηπmaxrp log p
πmin

(cid:27)

(cid:18)

≤ 2C2 exp

−

ηπmaxrp log p
πmin

(cid:19)

.

where we use the Cauchy-Schwarz inequality in the second step.

ec12

e-companion to Zhu et al.: Estimation of Markov Models

EC.9. Alternative statistical error analysis

EC.9.1. Main results

In this section, we provide an alternative proof strategy that follows Negahban et al. (2012) to bound

the statistical error of (cid:98)P and (cid:98)Pr. This strategy resolves the inconsistency issue of Theorems 1 and

2 when n (cid:29) {rpπmax(log p)β/(πminα3/2)}2. For any R > 0, deﬁne a constraint set C(β, R, κ) := {∆ ∈

(cid:60)p×p : (cid:107)∆(cid:107)max ≤ β/p, (cid:107)∆(cid:107)F ≤ R, (cid:107)∆(cid:107)∗ ≤ κr1/2(cid:107)∆(cid:107)F}. An important ingredient of this statistical

analysis is the localized restricted strong convexity (Negahban and Wainwright 2011, Fan et al.

2018) of the loss function (cid:96)n(P) near P. This property allows us to bound the distance in the

parameter space by the diﬀerence in the objective function value. Deﬁne the ﬁrst-order Taylor

remainder term of the negative log-likelihood function (cid:96)n(Q) around P as

δ(cid:96)n(Q; P) := (cid:96)n(Q) − (cid:96)n(P) − ∇(cid:96)n(P)(cid:62)(Q − P).

The following lemma establishes the desired local restricted strong convexity.

Lemma EC.2. Under Assumption 1, there exists a universal constant K such that for any ξ > 1,

it holds with probability at least 1 − K exp(−ξ) that for any ∆ ∈ C(β, R, κ),

δ(cid:96)n(P + ∆; P) ≥

α2
8β2

(cid:107)∆(cid:107)2

F − 8R

(cid:19)1/2

(cid:18) 3Kξ
n

−

8Kξα2 log n
β2n

−

KpκR
β

(cid:18) rπmax log p
n

(cid:19)1/2

.

(EC.21)

Now we present the statistical rates of (cid:98)P and (cid:98)Pr.

Theorem EC.1 (Alternative statistical guarantee for (cid:98)P). Under the same assumptions of

Theorem 1, there exists a universal constant C1 > 0, such that for any ξ > 1, if we choose

λ = C1

(cid:26)(cid:18) ξp2πmax log p

(cid:19)1/2

nα

+

ξp log p
nα

(cid:27)
,

then whenever nπmax(1 − ρ+) ≥ max{max(20, ξ2) log p, log n}, we have with probability at least 1 −

K exp(−ξ) − 4p−(ξ−1) − p−1 that

(cid:107) (cid:98)P − P(cid:107)F (cid:46) β2
α2

(cid:18) ξrp2πmax log p
nα

(cid:19)1/2

and DKL(P, (cid:98)P) (cid:46) ξβ6πmaxrp2 log p

nα7

,

where K is the same constant as in Lemma EC.2.

e-companion to Zhu et al.: Estimation of Markov Models

ec13

Theorem EC.2 (Alternative statistical guarantee for (cid:98)Pr). Under the same assumptions of

Theorem 1, there exists a universal constant C1 > 0, for any ξ > 1, we have with probability at least

1 − K exp(−ξ) − 4p−(ξ−1) − p−1 that

(cid:107) (cid:98)Pr − P(cid:107)F (cid:46) β2
α2

(cid:18) ξrp2πmax log p
nα

(cid:19)1/2

and DKL(P, (cid:98)Pr) (cid:46) ξβ6πmaxrp2 log p

nα7

,

where K is the same constant as in Lemma EC.2.

One can see from the theorems above that the derived error bounds converge to zero as n goes

to inﬁnity. Nevertheless, their dependence on α and β is worse than those in Theorems 1 and 2

when n (cid:46) {rpπmax(log p)β/(πminα3/2)}2. This is why we do not present this result in the main text.

EC.9.2. Proof of Lemma EC.2

Given any ∆ ∈ C(β, R, κ), it holds that for some 0 ≤ v ≤ 1 that

δ(cid:96)n(P + ∆; P) =

1
2

vec(∆)(cid:62)Hn(P + v∆)vec(∆) =

1
2n

n
(cid:88)

i=1

(cid:104)Xi, ∆(cid:105)2
(cid:104)P + v∆, Xi(cid:105)2

≥

1
2n

n
(cid:88)

i=1

p2
4β2

(cid:104)∆, Xi(cid:105)2.

Deﬁne

Γn := sup

(cid:12)
(cid:12)
(cid:12)
(cid:12)
∆∈C(β,R,κ)

1
n

n
(cid:88)

(cid:104)∆, Xi(cid:105)2 − E((cid:104)∆, Xi(cid:105)2)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(EC.22)

We ﬁrst bound the deviation of Γr from its expectation EΓr. Note that {Xi}n

i=1 is a Markov chain

on M := {eje(cid:62)

k }p

j,k=1. Here we apply a tail inequality for suprema of unbounded empirical processes

due to Adamczak (2008, Theorem 7). To apply this result, we need to verify that {Xi}n

i=1 satisﬁes

the “minorization condition” as stated in Section 3.1 of Adamczak (2008). Below we characterize

a specialized version of this condition.

Condition 1 (minorized condition). We say that a Markov chain X on S satisﬁes the

minorized condition if there exist δ > 0, a set C ⊂ S and a probability measure ν on S for which

∀x∈C∀A⊂SP(x, A) ≥ δν(A) and ∀x∈S∃n∈NPn(x, C) > 0.

ec14

e-companion to Zhu et al.: Estimation of Markov Models

One can verify that the Markov chain {Xi}n

i=1 satisﬁes Condition 1 with δ = 1/2, C = {e1e(cid:62)

2 } and

ν(eje(cid:62)

k ) = Pjk1{j=2} for j, k ∈ [p].

Now consider a new Markov chain {( (cid:101)Xi, Ri)}n

i=1 constructed as follows. Let {Ri}n

i=1 be i.i.d.

Bernoulli random variables with ER1 = δ. For any i ∈ {0, . . . , n − 1}, at step i, if Xi /∈ C, we

sample (cid:101)Xi+1 according to P( (cid:101)Xi, ·); otherwise, the distribution of (cid:101)Xi depends on Ri: if Ri = 1,

the chain regenerates in the sense that we draw (cid:101)Xi from ν, and if Ri = 0, we draw (cid:101)Xi from

(P(Xi, ·)−δν(·))/(1−δ). One can verify that the sequence { (cid:101)Xi}n

i=1 has exactly the same distribution

as the original Markov chain {Xi}n

i=1. Deﬁne T1 := inf{n > 0 : Rn = 1} and Ti+1 := inf{n > 0 :

RT1+...+Ti+n = 1} for i ≥ 0. Note that {Ti}i≥0 are i.i.d. Geometric random variables with ET1 = 2
and (cid:107)T1(cid:107)ψ1 ≤ 4. Let S0 := −1, Sj := T1 + . . . + Tj and Yj := { (cid:101)Xi}Sj

i=Sj−1+1 for j ≥ 1. Based on our

construction, we deduce that {Yj}j≥1 are independent. Thus we chop the original Markov chain

{Xi}i∈[n] into independent sequences. Finally, Adamazak’s bound entails the following asymptotic

weak variance

We have

σ2 := sup

∆∈C(β,R,κ)

(cid:26) S2(cid:88)

Var

(cid:104)∆, Xi(cid:105)2 − E((cid:104)∆, Xi(cid:105)2)

(cid:27)

/ET2.

i=S1+1

σ2 ≤ sup

∆∈C(β,R,κ)

(cid:20)(cid:26) S2(cid:88)
E

(cid:104)∆, Xi(cid:105)2 − E((cid:104)∆, Xi(cid:105)2)

(cid:27)2(cid:21)

/ET2

i=S1+1

∞
(cid:88)

(cid:20)(cid:26) S2(cid:88)
E

(cid:104)∆, Xi(cid:105)2 − E((cid:104)∆, Xi(cid:105)2)

(cid:27)2

(cid:21)

1{T2=j}

j=1

i=S1+1

sup
∆∈C(β,R,κ)

∞
(cid:88)

j=1

j2R2β4
p4

P(T2 = j) =

R2β4E(T 2
2 )
2p4

=

3β4R2
p4

.

=

≤

1
2

1
2

By Adamczak (2008, Theorem 7), there exists a universal constant K such that for any ξ > 0,

(cid:26)

|Γn − EΓn| ≥ KEΓn +

P

(cid:19)1/2

Rβ2
p2

(cid:18) 3Kξ
n

+

64Kξα2 log n
np2

(cid:27)

≤ K exp(−ξ).

(EC.23)

Next, by the symmetrization argument and Ledoux-Talagrand contraction inequality (Ledoux

and Talagrand 2013), for n independent and identically distributed Rademacher variables {γi}n

i=1,

when nπmax(1 − ρ+) ≥ max(20 log p, log n), we have that

EΓn ≤ 2E

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
(cid:107)∆(cid:107)F≤R,
∆∈C(β,R,κ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

8β(cid:107)∆(cid:107)∗
p

1
n

≤

1
n

n
(cid:88)

i=1

γi(cid:104)∆, Xi(cid:105)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

8β
p

E

n
(cid:88)

i=1

γiXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤

8κβr1/2R
p

E

sup
(cid:107)∆(cid:107)F≤R,
∆∈C(β,R,κ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

1
n

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:104)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
γiXi, ∆(cid:105)
(cid:12)
(cid:12)

γiXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤

8κβR
p

(cid:18) rπmax log p
n

(cid:19)1/2

,

(EC.24)

e-companion to Zhu et al.: Estimation of Markov Models

ec15

where the penultimate inequality is due to the fact that ∆ ∈ C(β, R, κ), and where the last inequality

is due to (EC.20).

Finally,

E(cid:104)∆, Xi(cid:105)2 =

(cid:88)

1≤j,k≤d

πjPjk∆2

jk ≥

α2
p2

(cid:107)∆(cid:107)2
F.

(EC.25)

Combining all the bounds above, we have for any ξ > 1, with probability at least 1 − K exp(−ξ),

δ(cid:96)n(P + ∆; P) ≥

α2
8β2

(cid:107)∆(cid:107)2

F − 8R

(cid:19)1/2

(cid:18) 3Kξ
n

−

8Kξα2 log n
β2n

−

KpκR
β

(cid:18) rπmax log p
n

(cid:19)1/2

.

(EC.26)

EC.9.3. Proof of Theorem EC.1

For a speciﬁc R whose value will be determined later, we construct an intermediate estimator

(cid:98)Pη between (cid:98)P and P:

(cid:98)Pη = P + η( (cid:98)P − P),

where η = 1 if (cid:107) (cid:98)P − P(cid:107)F ≤ R and η = R/(cid:107) (cid:98)P − P(cid:107)F if (cid:107) (cid:98)P − P(cid:107)F > R. For any ξ > 1, there exists a

universal constant C > 0 such that when

λ = C

(cid:26)(cid:18) ξp2πmax log p

(cid:19)1/2

nα

+

ξp log p
nα

(cid:27)

,

we have by Lemmas EC.2 and 2 that with probability at least 1 − K exp(−ξ) − 4p−(ξ−1) − p−1,

α2
8β2

(cid:107)∆(cid:107)2

F − 8R

(cid:19)1/2

(cid:18) 3Kξ
n

−

8Kξα2 log n
β2n

−

KpκR
β

(cid:18) rπmax log p
n

(cid:19)1/2

≤ δ(cid:96)n( (cid:98)Pη; P) ≤ −(cid:104)ΠN (∇Ln(P)), (cid:98)∆η(cid:105) + λ((cid:107)P(cid:107)∗ − (cid:107) (cid:98)Pη(cid:107)∗)

(EC.27)

≤ −(cid:104)ΠN (∇Ln(P)), (cid:98)∆η(cid:105) + λ(cid:107) (cid:98)∆η(cid:107)∗ ≤ ((cid:107)ΠN (∇Ln(P))(cid:107)2 + λ)(cid:107) (cid:98)∆η(cid:107)∗

≤ 8λ(cid:107)[ (cid:98)∆η]M(cid:107)∗ ≤ 8λ

√

r(cid:107) (cid:98)∆η(cid:107)F,

where K is the same universal constant as in Theorem EC.2. Some algebra yields that

(cid:107) (cid:98)∆η(cid:107)2
F

(cid:46) β2
α2

max

(cid:26) λ2rβ2
α2

, R

(cid:18) ξ
n

(cid:19)1/2

,

ξα2 log n
β2n

,

pR
β

(cid:18) rπmax log p
n

(cid:19)1/2(cid:27)

.

(EC.28)

Letting R2 be greater than the RHS of the inequality above, we can ﬁnd a universal constant

C4 > 0 such that

R ≥

C4β2
α2

(cid:18) ξrp2πmax log p
nα

(cid:19)1/2

=: R0.

ec16

e-companion to Zhu et al.: Estimation of Markov Models

Choose R = R0. Therefore, (cid:107) (cid:98)∆η(cid:107)F ≤ R and (cid:98)∆η = (cid:98)∆. We can thus reach the conclusion. As to the

KL-Divergence, by Zhang and Wang (2017, Lemma 4), we deduce that

DKL( (cid:98)P, P) =

p
(cid:88)

j=1

πjDKL(Pj·, (cid:98)Pj·) ≤

p
(cid:88)

j=1

β2
2α2

(cid:107)Pj· − (cid:98)Pj·(cid:107)2

2 =

β2
2α2

(cid:107) (cid:98)P − P(cid:107)2
F,

(EC.29)

from which we attain the conclusion.

EC.9.4. Proof of Theorem EC.2

Deﬁne (cid:98)∆(r) := (cid:98)Pr − P. Since rank(P) ≤ r and rank( (cid:98)Pr) ≤ r, rank( (cid:98)∆(r)) ≤ 2r. Thus

(cid:107) (cid:98)∆(r)(cid:107)F ≤ (2r)1/2(cid:107) (cid:98)∆(r)(cid:107)∗. Now we follow the proof strategy of Theorem 1 to establish the statis-

tical error bound for (cid:98)Pr. Similarly, for a speciﬁc R > 0 whose value will be determined later, we

can construct an intermediate estimator (cid:98)Pr

η between (cid:98)Pr and P:

(cid:98)Pr

η = P + η( (cid:98)Pr − P),

where η = 1 if (cid:107) (cid:98)Pr − P(cid:107)F ≤ R and η = R/(cid:107) (cid:98)Pr − P(cid:107)F if (cid:107) (cid:98)Pr − P(cid:107)F > R. Let (cid:98)∆η(r) := (cid:98)Pr

η − P. Since

(cid:98)∆η(r) ∈ C(β, R,

√

2), applying Lemma EC.2 yields that

α2
8β2

(cid:107)∆(cid:107)2

F − 8R

(cid:19)1/2

(cid:18) 3Kξ
n

−

8Kξα2 log n
β2n

−

KpκR
β

(cid:18) rπmax log p
n

(cid:19)1/2

η; P) ≤ −(cid:104)ΠN (∇(cid:96)n(P)), (cid:98)∆η(r)(cid:105) ≤ (cid:107)ΠN (∇Ln(P))(cid:107)2(cid:107) (cid:98)∆η(r)(cid:107)∗

(EC.30)

≤ δ(cid:96)n( (cid:98)Pr
√

≤

2r(cid:107)ΠN (∇Ln(P))(cid:107)2(cid:107) (cid:98)∆η(r)(cid:107)F,

which futher implies that there exists C1 depending only on α and β such that

(cid:107) (cid:98)∆η(r)(cid:107)2

F ≤ C1 max

(cid:26)

r(cid:107)ΠN (∇Ln(P))(cid:107)2

2, R

(cid:18) ξ
n

(cid:19)1/2

,

ξα2 log n
β2n

,

pR
β

(cid:18) rπmax log p
n

(cid:19)1/2(cid:27)
.

By a contradiction argument as in the proof of Theorem 1, we can choose an appropriate R large

enough such that (cid:98)Pr

η = (cid:98)Pr and attain the conclusion.

EC.10. Proof of Proposition 1

Since rank(X∗

c) ≤ r, we know that X∗

c is in fact a feasible solution to the original problem (5)

and (cid:107)X∗

c(cid:107)∗ − (cid:107)X∗

c(cid:107)(r) = 0. Therefore, for any feasible solution X to (5), it holds that

f (X∗

c) = f (X∗

c) + c((cid:107)X∗

c(cid:107)∗ − (cid:107)X∗

c(cid:107)(r))

≤ f (X) + c((cid:107)X(cid:107)∗ − (cid:107)X(cid:107)(r)) = f (X).

e-companion to Zhu et al.: Estimation of Markov Models

ec17

This completes the proof of the proposition.

EC.11. Convergence and o(1/k) non-ergodic iteration complexity of Algorithm

1 (sGS-ADMM)

Before deriving the desired results of Algorithm 1 for solving problem (10), we present some

notation and deﬁnitions for the subsequent analysis. Assume that the solution sets of (9) and (10)

are nonempty. Then, the primal-dual solution pairs associated with problems (9) and (10) satisfy

the following Karush-Kuhn-Tucker (KKT) system:

0 ∈ R(X, Ξ, S), A(X) = b, Ξ + A∗(y) + S = 0,

(EC.31)

with

R(X, Ξ, S) :=









Ξ + ∂g(X)





X + ∂δ((cid:107)S(cid:107)2 ≤ c)

,

(X, Ξ, S) ∈ dom g × (cid:60)p×p × (cid:8)S ∈ (cid:60)p×p | (cid:107)S(cid:107)2 ≤ c(cid:9) .

Deﬁne the KKT residual function D : dom g × (cid:60)p×p × (cid:60)n × {S ∈ (cid:60)p×p | (cid:107)S(cid:107)2 ≤ c} → [0, +∞) as

D(X, Ξ, y, S) := dist2(0, R(X, Ξ, S)) + (cid:107)A(X) − b(cid:107)2 + (cid:107)Ξ + A∗(y) + S(cid:107)2.

We say (X, Ξ, y, S) ∈ dom g × (cid:60)p×p × (cid:60)n × {S ∈ (cid:60)p×p | (cid:107)S(cid:107)2 ≤ c} be an (cid:15)-approximate primal-dual

solution pair for problems (9) and (10) if D(X, Ξ, y, S) ≤ (cid:15). We show in the following theorem the

global convergence and the o(1/k) iteration complexity results of Algorithm sGS-ADMM.

Theorem EC.3. Suppose that

the solution sets of

(9) and (10) are nonempty. Let

{(Ξk, yk, Sk, Xk)} be the sequence generated by Algorithm 1. If τ ∈ (0, (1+

√

5 )/2), then the sequence

{(Ξk, yk, Sk)} converges to an optimal solution of (10) and {Xk} converges to an optimal solution

of (9). Moreover, there exist a constant ω > 0 such that

(cid:8)D(Xk, Ξk, yk, Sk)(cid:9) ≤

min
1≤i≤k

ω
k

, ∀ k ≥ 1,

and

(cid:26)

lim
k→∞

k min
1≤i≤k

(cid:8)D(Xk, Ξk, yk, Sk)(cid:9)

(cid:27)

= 0.

In order to use (Li et al. 2016b, Theorem 3), we need to write problem (10) as following

min g∗(−Ξ) − (cid:104)b, y(cid:105) + δ((cid:107)S(cid:107)2 ≤ c)

s.t. F(Ξ) + A∗

1(y) + G(S) = 0,

ec18

e-companion to Zhu et al.: Estimation of Markov Models

where F, A1 and G are linear operators such that for all (Ξ, y, S) ∈ (cid:60)p×p × (cid:60)n × (cid:60)p×p, F(Ξ) = Ξ,

A∗

1(y) = A∗(y) and G(S) = S. Clearly, F = G = I where I : (cid:60)p×p → (cid:60)p×p is the identity map.

Therefore, we have A1A∗

1 (cid:31) 0 and FF ∗ = GG∗ = I (cid:31) 0. Hence, the assumptions and conditions in

(Li et al. 2016b, Theorem 3) are satisﬁed. The convergence results thus follow directly. Meanwhile,

the non-ergodic iteration complexity results follows from (Chen et al. 2017, Theorem 6.1).

EC.12. Proof of Theorems 5 and 6

We only need to prove Theorem 6 as Theorem 5 is a special incidence. To prove Theorem 6, we

ﬁrst introduce the following lemma.

Lemma EC.3. Suppose that {xk} is the sequence generated by Algorithm 3. Then θ(xk+1) ≤ θ(xk)−

1

2 (cid:107)xk+1 − xk(cid:107)2

G+2T .

For any k ≥ 0, by the optimality condition of problem (10) at xk+1, we know that there exist

ηk+1 ∈ ∂p(xk+1) such that

0 = ∇g(xk) + (G + T )(xk+1 − xk) + ηk+1 − ξk = 0.

Then for any k ≥ 0, we deduce

θ(xk+1) − θ(xk) ≤ (cid:98)θ(xk+1; xk) − θ(xk)

= p(xk+1) − p(xk) + (cid:104)xk+1 − xk, ∇g(xk) − ξk(cid:105) + 1

2 (cid:107)xk+1 − xk(cid:107)2

G

≤ (cid:104)∇g(xk) + ηk+1 − ξk, xk+1 − xk(cid:105) + 1

2 (cid:107)xk+1 − xk(cid:107)2

G

= − 1

2 (cid:107)xk+1 − xk(cid:107)2

G+2T .

This completes the proof of this lemma.

Now we are ready to prove Theorem 6.

From the optimality condition at xk+1, we have that

0 ∈ ∇g(xk) + (G + T )(xk+1 − xk) + ∂p(xk+1) − ξk.

Since xk+1 = xk, this implies that

0 ∈ ∇g(xk) + ∂p(xk) − ∂q(xk),

e-companion to Zhu et al.: Estimation of Markov Models

ec19

i.e., xk is a critical point. Observe that the sequence {θ(xk)} is non-increasing since

θ(xk+1) ≤ (cid:98)θ(xk+1; xk) ≤ (cid:98)θ(xk; xk) = θ(xk),

k ≥ 0.

Suppose that there exists a subsequence {xkj } that converging to ¯x, i.e., one of the accumulation

points of {xk}. By Lemma EC.3 and the assumption that G + 2T (cid:23) 0, we know that for all x ∈ X

(cid:98)θ(xkj+1; xkj+1) = θ(xkj+1)

≤θ(xkj +1) ≤ (cid:98)θ(xkj +1; xkj ) ≤ (cid:98)θ(x; xkj ).

By letting j → ∞ in the above inequality, we obtain that

(cid:98)θ(¯x; ¯x) ≤ (cid:98)θ(x; ¯x).

By the optimality condition of (cid:98)θ(x; ¯x), we have that there exists ¯u ∈ ∂p(¯x) and ¯v ∈ ∂q(¯x) such that

0 ∈ ∇g(¯x) + ¯u − ¯v.

This implies that (∇g(¯x) + ∂p(¯x)) ∩ ∂q(¯x) (cid:54)= ∅. To establish the rest of this proposition, we obtain

from Lemma 1 that

lim
t→+∞

1
2

t
(cid:88)

i=0

(cid:107)xk+1 − xk(cid:107)2

G+2T

≤ lim inf
t→+∞

(cid:0)θ(x0) − θ(xk+1)(cid:1) ≤ θ(x0) < +∞ ,

which implies limi→+∞ (cid:107)xk+1 − xi(cid:107)G+2T = 0. The proof of this theorem is thus complete by the

positive deﬁniteness of the operator G + 2T .

