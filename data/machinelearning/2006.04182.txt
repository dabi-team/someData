PREDICTIVE CODING APPROXIMATES BACKPROP ALONG
ARBITRARY COMPUTATION GRAPHS

Beren Millidge
School of Informatics
University of Edinburgh
beren@millidge.name

Alexander Tschantz
Sackler Centre for Consciousness Science
School of Engineering and Informatics
University of Sussex
tschantz.alec@gmail.com

Christopher L Buckley
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
C.L.Buckley@sussex.ac.uk

October 7, 2020

ABSTRACT

Backpropagation of error (backprop) is a powerful algorithm for training machine learning architec-
tures through end-to-end differentiation. Recently it has been shown that backprop in multilayer-
perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process
theory of cortical computation which relies solely on local and Hebbian updates. The power of
backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differ-
entiation which allows for the optimisation of any differentiable program expressed as a computation
graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly)
to exact backprop gradients on arbitrary computation graphs using only local learning rules. We
apply this result to develop a straightforward strategy to translate core machine learning architectures
into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more
complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative
interactions. Our models perform equivalently to backprop on challenging machine learning bench-
marks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that
standard machine learning algorithms could in principle be directly implemented in neural circuitry,
and may also contribute to the development of completely distributed neuromorphic architectures.

0
2
0
2

t
c
O
5

]

G
L
.
s
c
[

5
v
2
8
1
4
0
.
6
0
0
2
:
v
i
X
r
a

1

Introduction

Deep learning has seen stunning successes in the last decade in computer vision (Krizhevsky, Sutskever, & Hinton, 2012;
Szegedy et al., 2015), natural language processing and translation (Kaplan et al., 2020; Radford et al., 2019; Vaswani et
al., 2017), and computer game playing (Mnih et al., 2015; Schrittwieser et al., 2019; Silver et al., 2017; Vinyals et al.,
2019). While there is a great variety of architectures and models, they are all trained by gradient descent using gradients

 
 
 
 
 
 
A PREPRINT - OCTOBER 7, 2020

computed by automatic differentiation (AD). The key insight of AD is that it sufﬁces to deﬁne a forward model which
maps inputs to predictions according to some parameters. Then, using the chain rule of calculus, it is possible, as
long as every operation of the forward model is differentiable, to differentiate back through the computation graph of
the model so as to compute the sensitivity of every parameter in the model to the error at the output, and thus adjust
every single parameter to best minimize the total loss. Early models were typically simple artiﬁcial neural networks
where the computation graph is simply a composition of matrix multiplications and elementwise nonlinearities, and
for which the implementation of automatic differentation has become known as ‘backpropagation’ (or ’backprop’).
However, automatic differentiation allows for substantially more complicated graphs to be differentiated through, up to,
and including, arbitrary programs (Baydin, Pearlmutter, Radul, & Siskind, 2017; Griewank et al., 1989; Innes et al.,
2019; Linnainmaa, 1970; Paszke et al., 2017; Revels, Lubin, & Papamarkou, 2016; Rumelhart & Zipser, 1985; Werbos,
1982). In recent years this has enabled the differentiation through differential equation solvers (T. Q. Chen, Rubanova,
Bettencourt, & Duvenaud, 2018; Rackauckas et al., 2019; Tzen & Raginsky, 2019), physics engines (Degrave, Hermans,
Dambre, & Wyffels, 2019; Heiden, Millard, & Sukhatme, 2019), raytracers (Pal, 2019), and planning algorithms (Amos
& Yarats, 2019; Okada, Rigazio, & Aoshima, 2017). These advances allow the straightforward training of models
which intrinsically embody complex processes and which can encode signiﬁcantly more prior knowledge and structure
about a given problem domain than previously possible.

Modern deep learning has also been closely intertwined with neuroscience (Hassabis, Kumaran, Summerﬁeld, &
Botvinick, 2017; Hawkins & Blakeslee, 2007; Richards et al., 2019). The backpropagation algorithm itself arose as a
technique for training multi-layer perceptrons – simple hierarchical models of neurons inspired by the brain (Werbos,
1982). Despite this origin, and its empirical successes, a consensus has emerged that the brain cannot directly implement
backprop, since to do so would require biologically implausible connection rules (Crick, 1989). There are two principal
problems. Firstly, backprop in the brain appears to require non-local information (since the activity of any speciﬁc
neuron affects all subsequent neurons down to the ﬁnal output neuron). It is difﬁcult to see how this information could
be transmitted ’backwards’ throughout the brain with the required ﬁdelity without precise connectivity constraints.
The second problem – the ‘weight transport problem’ is that backprop through MLP style networks requires identical
forward and backwards weights. In recent years, however, a succession of models have been introduced which claim
to implement backprop in MLP-style models using only biologically plausible connectivity schemes, and Hebbian
learning rules (Bengio & Fischer, 2015; Bengio, Mesnard, Fischer, Zhang, & Wu, 2017; Guerguiev, Lillicrap, &
Richards, 2017; Liao, Leibo, & Poggio, 2016; Ororbia, Mali, Giles, & Kifer, 2020; Sacramento, Costa, Bengio, &
Senn, 2018; Whittington & Bogacz, 2019). Of particular signiﬁcance is Whittington and Bogacz (2017) who show
that predictive coding networks – a type of biologically plausible network which learn through a hierarchical process
of prediction error minimization – are mathematically equivalent to backprop in MLP models. In this paper we
extend this work, showing that predictive coding can not only approximate backprop in MLPs, but can approximate
automatic differentiation along arbitrary computation graphs. This means that in theory there exist biologically plausible
algorithms for differentiating through arbitrary programs, utilizing only local connectivity. Moreover, in a class of
models which we call parameter-linear, which includes many current machine learning models, the required update
rules are Hebbian, raising the possibility that a wide range of current machine learning architectures may be faithfully
implemented in the brain, or in neuromorphic hardware.

In this paper we provide two main contributions. (i) We show that predictive coding converges to automatic differentia-
tion across arbitrary computation graphs. (ii) We showcase this result by implementing three core machine learning
architectures (CNNs, RNNs, and LSTMs) in a predictive coding framework which utilises only local learning rules and
mostly Hebbian plasticity.

2

A PREPRINT - OCTOBER 7, 2020

Figure 1: Top: Backpropagation on a chain. Backprop proceeds backwawrds sequentially and explicitly computes the
gradient at each step on the chain. Bottom: Predictive coding on a chain. Predictions, and prediction errors are updated
in parallel using only local information.

2 Predictive Coding on Arbitrary Computation Graphs

Predictive coding is an inﬂuential theory of cortical function in theoretical and computational neuroscience. Central
to the theory is the idea that the core function of the brain is to minimize prediction errors between what is expected
to happen and what actually happens. Predictive coding views the brain as composed of multiple hierarchical layers
which predict the activities of the layers below. Unpredicted activity is registered as prediction error which is then
transmitted upwards for a higher layer to process. Over time, synaptic connections are adjusted so that the system
improves at minimizing prediction error. Predictive coding possesses a wealth of empirical support (Bogacz, 2017;
Friston, 2003, 2005; Whittington & Bogacz, 2019) and offers a single mechanism that accounts for diverse perceptual
phenomena such as repetition-suppression (Auksztulewicz & Friston, 2016), end-stopping (Rao & Ballard, 1999),
bistable perception (Hohwy, Roepstorff, & Friston, 2008; Weilnhammer, Stuke, Hesselmann, Sterzer, & Schmack,
2017) and illusory motions (Lotter, Kreiman, & Cox, 2016; Watanabe, Kitaoka, Sakamoto, Yasugi, & Tanaka, 2018),
and even attentional modulation of neural activity (Feldman & Friston, 2010; Kanai, Komura, Shipp, & Friston, 2015).
Moreover, the central role of top-down predictions is consistent with the ubiquity, and importance of, top-down diffuse
connections between cortical areas. Predictive coding is consistent with many known aspects of neurophysiology, and
has been translated into biologically plausible process theories which deﬁne candidate cortical microcircuits which can
implement the algorithm. (Bastos et al., 2012; Kanai et al., 2015; Shipp, 2016; Spratling, 2008).

In previous work, predictive coding has always been conceptualised as operating on hierarchies of layers (Bogacz,
2017; Whittington & Bogacz, 2017). Here we present a generalized form of predictive coding applied to arbitrary
computation graphs. A computation graph G = {E, V} is a directed acyclic graph (DAG) which can represent the
computational ﬂow of essentially any program or computable function as a composition of elementary functions. Each
edge ei ∈ E of the graph corresponds to an intermediate step – the application of an elementary function – while
each vertex vi ∈ V is an intermediate variable computed by applying the functions of the edges to the values of their

3

zv0v1v2v3T∂L∂v2=∂L∂v3∂v3∂v2∂L∂v1=∂L∂v2∂v2∂v1∂L∂v0=∂L∂v1∂v1∂v0v0v1v2ϵ0ϵ1ϵ2ϵ3̂v1̂v3·v0=−ϵ0+ϵ1∂̂v1v0·v1=−ϵ1+ϵ2∂̂v2v1·v2=−ϵ2+ϵ3∂̂v3v2T̂v2A PREPRINT - OCTOBER 7, 2020

originating vertices. In this paper, vi denotes the vector of activations within a layer and we denote the set of all vertices
as {vi}. Effectively, computation ﬂows ’forward’ from parent nodes to all their children through the edge functions
until the leaf nodes give the ﬁnal output of the program as a whole (see Figure 1 and 2 for an example). Given a target T
and a loss function L = g(T, vout), the graph’s output can be evaluated and, and if every edge function is differentiable,
automatic differentiation can be performed on the computation graph.

Predictive coding can be derived elegantly as a variational inference algorithm under a hierarchical Gaussian generative
model (Buckley, Kim, McGregor, & Seth, 2017; Friston, 2005). We extend this approach to arbitrary computation
graphs in a supervised setting by deﬁning the inference problem to be solved as that of inferring the vertex value vi of
each node in the graph given ﬁxed start nodes v0 (the data), and end nodes vN (the targets). We deﬁne a generative model
which parametrises the value of each vertex given the feedforward prediction of its parents, p({vi}) = (cid:81)N
i p(vi|P(vi)),
and a factorised, variational posterior Q({vi}) = (cid:81)N
i Q(vi), where P(x) denotes the set of parents and C(x) denotes
the set of children of a given node x. From this, we can deﬁne a suitable objective functional, the variational free-energy
F (VFE), which acts as an upper bound on the divergence between the true and variational posteriors.

F = KL[(Q({vi})(cid:107)p({vi})] ≥ KL[(Q({vi})(cid:107)p({v1:N −1}|v0, vN )]

≈

N
(cid:88)

i=0

(cid:15)T
i (cid:15)i

(1)

Under Gaussian assumptions for the generative model p({vi}) = (cid:81)N
i N (vi; ˆvi, Σi), and the variational posterior
Q({vi}) = (cid:81)N
i N (vi), where the ‘predictions’ ˆvi = f (P(vi); θi) are deﬁned as as the feedforward value of the vertex
produced by running the graph forwards, and all the precisions, or inverse variances, Σ−1
are ﬁxed at the identity, we
can write F as simply a sum of prediction errors (see Appendix D or (Bogacz, 2017; Buckley et al., 2017; Friston,
2003) for full derivations), with the prediction errors deﬁned as (cid:15)i = vi − ˆvi. These prediction errors play a core role
in the framework and, in the biological process theories (Bastos et al., 2012; Friston, 2005), are generally considered
to be represented by a distinct population of ‘error units’. Since F is an upper bound on the divergence between true
and approximate posteriors, by minimizing F, we reduce this divergence, thus improving the quality of the variational
posterior and approximating exact Bayesian inference. Predictive coding minimizes F by employing the Cauchy
method of steepest descent to set the dynamics of the vertex variables vi as a gradient descent directly on F (Bogacz,
2017).

i

dvi
dt

= −

∂F
∂vi

= (cid:15)i −

(cid:88)

j∈C(vi)

(cid:15)j

∂ˆvj
∂vi

(2)

The dynamics of the parameters of the edge functions θ such that ˆvi = f (P(vi); θ), can also be derived as a gradient
descent on F. Importantly these dynamics require only information (the current vertex value, prediction error, and
prediction errors of child vertices) locally available at the vertex.

dθ
dt

= −

∂F
∂θ

= (cid:15)i

∂ˆvi
∂θi

(3)

To run generalized predictive coding in practice on a given computation graph G = {E, V}, we augment the graph with
error units (cid:15) ∈ E to obtain an augumented computation graph ˜G = {E, V, E}. The predictive coding algorithm then
operates in two phases – a feedforward sweep and a backwards iteration phase. In the feedforward sweep, the augmented
computation graph is run forwards to obtain the set of predictions { ˆvi}, and prediction errors {(cid:15)i} = {vi − ˆvi} for
every vertex. Following Whittington and Bogacz (2017), to achieve exact equivalence with the backprop gradients
computed on the original computation graph, we initialize vi = ˆvi in the initial feedforward sweep so that the output
error computed by the predictive coding network and the original graph are identical.

In the backwards iteration phase, the vertex activities {vi} and prediction errors {(cid:15)i} are updated with Equation 2 for all
vertices in parallel until the vertex values converge to a minimum of F. After convergence the parameters are updated
according to Equation 3. Note we also assume, following Whittington and Bogacz (2017),that the predictions at each

4

A PREPRINT - OCTOBER 7, 2020

layer are ﬁxed at the values assigned during the feedforward pass throughout the optimisation of the vs. We call this
the ﬁxed-prediction assumption. In effect, by removing the coupling between the vertex activities of the parents and
the prediction at the child, this assumption separates the global optimisation problem into a local one for each vertex.
We implement these dynamics with a simple forward Euler integration scheme so that the update rule for the vertices
became vt+1
where η is the step-size parameter. Importantly, if the edge function linearly combines the
activities and the parameters followed by an elementwise nonlinearity – a condition which we call ‘parameter-linear’ –
then both the update rule for the vertices (Equation 2) and the parameters (Equation 3) become Hebbian. Speciﬁcally,
the update rules for the vertices and weights become ∂ ˆvj
∂vi

i , respectively.

= f (cid:48)(θivi)vT

= f (cid:48)(θivi)θT

i and ∂ ˆvj
∂θi

i − η dF
dvt
i

i ← vt

Algorithm 1: Generalized Predictive Coding
Data: Dataset D = {X, L}, Augmented Computation Graph ˜G = {E, V, E}, inference learning rate ηv, weight

learning rate ηθ

begin

/* For each minibatch in the dataset
for (x, L) ∈ D do

/* Fix start of graph to inputs
ˆv0 ← x
/* Forward pass to compute predictions
for ˆvi ∈ V do

ˆvi ← f ({P(ˆvi); θ)

/* Compute output error
(cid:15)L ← L − ˆvL
/* Begin backwards iteration phase of the descent on the free energy
while not converged do
for (vi, (cid:15)i) ∈ ˜G do

/* Compute prediction errors
(cid:15)i ← vi − ˆvi
/* Update the vertex values
vt+1
dF
i ← vt
dvt
i

i + ηv
/* Update weights at equilibrium
for θi ∈ E do
θt+1
i ← θt

i + ηθ

dF
dθt
i

*/

*/

*/

*/

*/

*/

*/

*/

2.1 Approximation to Backprop

Here we show that at the equilibrium of the dynamics, the prediction errors (cid:15)∗
i converge to the correct backpropagated
gradients ∂L
, and consequently the parameter updates (Equation 3) become precisely those of a backprop trained
∂vi
network. Standard backprop works by computing the gradient of a vertex as the sum of the gradients of the child
vertices. Beginning with the gradient of the output vertex ∂L
, it recursively computes the gradients of vertices deeper
∂vL
in the graph by the chain rule:

∂L
∂vi

=

(cid:88)

j=C(vi)

∂L
∂vj

∂vj
∂vi

(4)

In comparison, in our predictive coding framework, at the equilibrium point ( dvi

dt = 0) the prediction errors (cid:15)∗

i become,

(cid:88)

(cid:15)∗
i =

j∈C(vi)

(cid:15)∗
j

∂ˆvi
∂vj

5

(5)

A PREPRINT - OCTOBER 7, 2020

Importantly, this means that the equilibrium value of the prediction error at a given vertex (Equation 5) satisﬁes the same
recursive structure as the chain rule of backprop (Equation 4). Since this relationship is recursive, all that is needed for
the prediction errors throughout the graph to converge to the backpropagated derivatives is for the prediction errors
L = ∂L
at the ﬁnal layer to be equal to the output gradient: (cid:15)∗
. To see this explicitly, consider a mean-squared-error
∂ ˆvL
loss function at the output layer L = 1
2 (T − ˆvL)2 with T as a vector of targets, and deﬁning (cid:15)L = T − ˆvL. We then
consider the equilibrium value of the prediction error unit at a penultimate vertex (cid:15)L−1. By Equation 5, we can see that
at equilibrium,

L−1 = (cid:15)∗
(cid:15)∗
L

∂ˆvL
∂vL−1

= (T − ˆv∗
L)

∂ˆvL
∂vL−1

since, (T − ˆvL) = ∂L
∂ ˆvL

, we can then write,

(cid:15)∗
L−1 =

∂L
∂ˆvL

∂ˆvL
∂vL−1

=

∂L
∂vL−1

(6)

Thus the prediction errors of the penultimate nodes converge to the correct backpropagated gradient. Furthermore,
recursing through the graph from children to parents allows the correct gradients to be computed1. Thus, by induction,
we have shown that the ﬁxed points of the prediction errors of the global optimization correspond exactly to the
backpropagated gradients. Intuitively, if we imagine the computation-graph as a chain and the error as ’tension’ in the
chain, backprop loads all the tension at the end (the output) and then systematically propagates it backwards. Predictive
coding, however, spreads the tension throughout the entire chain until it reaches an equilibrium where the amount of
tension at each link is precisely the backpropagated gradient.

By a similar argument, it is apparent that the dynamics of the parameters θi as a gradient descent on F also exactly
match the backpropagated parameter gradients.

dθ
dt

= −

dF
dθ

= (cid:15)∗
i

=

dL
dˆvi
dˆvi
dθi
dθ = dˆvi
dθi

i

.

and that d(cid:15)∗

=

d(cid:15)∗
i
dθi
dL
dθi

(7)

Which follows from the fact that (cid:15)∗

i = dL
dˆvi

3 Related Work

A number of recent works have tried to provide biologically plausible approximations to backprop. The requirement
of symmetry between the forwards and backwards weights has been questioned by Lillicrap, Cownden, Tweed, and
Akerman (2016) who show that random ﬁxed feedback weights sufﬁce for effective learning. Recent additional work
has shown that learning the backwards weights also helps (Akrout, Wilson, Humphreys, Lillicrap, & Tweed, 2019;
Amit, 2019). Several schemes have also been proposed to approximate backprop using only local learning rules
and/or Hebbian connectivity. These include target-prop (Lee, Zhang, Fischer, & Bengio, 2015) which approximate
the backward gradients with trained inverse functions, but which fails to asymptotically compute the exact backprop
gradients, and contrastive Hebbian (Scellier & Bengio, 2017; Scellier, Goyal, Binas, Mesnard, & Bengio, 2018;
Seung, 2003) approaches which do exactly approximate backprop, but which require two separate learning phases and
the storing of information across successive phases. There are also dendritic error theories (Guerguiev et al., 2017;
Sacramento et al., 2018) which are computationally similar to predictive coding (Lillicrap, Santoro, Marris, Akerman,
& Hinton, 2020; Whittington & Bogacz, 2019). Whittington and Bogacz (2017) showed that predictive coding can
approximate backprop in MLP models, and demonstrated comparable performance on MNIST. We advance upon this
work by extending the proof to arbitrary computation graphs, enabling the design of predictive coding variants of a

1Some subtlety is needed here since vL−1 may have many children which each contribute to the loss. However, these different

paths sum together at the node vL−1, thus propagating the correct gradient backwards.

6

A PREPRINT - OCTOBER 7, 2020

Figure 2: Top: The computation graph of the nonlinear test function vL = tan(
0). Bottom: graphs of
the log mean divergence from the true gradient and the divergence for different learning rates. Convergence to the exact
gradients is exponential and robust to high learning rates.

θv0) + sin(v2

√

range of standard machine learning architectures, which we show perform comparably to backprop on considerably
more difﬁcult tasks than MNIST. Our algorithm evinces asymptotic (and in practice rapid) convergence to the exact
backprop gradients, does not require separate learning phases, and utilises only local information and largely Hebbian
plasticity.

4 Results

4.1 Numerical Results

√

θv0) + sin(v2

To demonstrate the correctness of our derivation and empirical convergence to the true gradients, we present a numerical
test in the simple scalar case, where we use predictive coding to derive the gradients of an arbitrary, highly nonlinear
0) where θ is an arbitrary parameter. For our tests, we set v0 to 5 and θ to 2. The
test function vL = tan(
computation graph for this function is presented in Figure 2. Although simple, this is a good test of predictive coding
because the function is highly nonlinear, and its computation graph does not follow a simple layer structure but includes
some branching. An arbitrary target of T = 3 was set at the output and the gradient of the loss L = (vL − T )2 with
respect to the input v0 was computed by predictive coding. We show (Figure 2) that the predictive coding optimisation
rapidly converges to the exact numerical gradients computed by automatic differentiation, and that moreover this
optimization is very robust and can handle even exceptionally high learning rates (up to 0.5) without divergence.

In summary, we have shown and numerically veriﬁed that at the equilibrium point of the global free-energy F on
an arbitrary computation graph, the error units exactly equal the backpropagated gradients, and that this descent
requires only local connectivity, does not require a separate phases or a sequential backwards sweep, and in the case

7

θ*(⋅,⋅)⋅tan(⋅)+(⋅,⋅)(⋅)2sin(⋅)μ1=θ*v0μ2=(̂v1)μ3=tan(̂v2)μ4=v20μ5=sin(̂v4)μL=̂v3+̂v5̂v1̂v2̂v3̂v4̂v5ϵ1ϵ2ϵ3ϵ5ϵ4TϵLϵLdμLd̂v3ϵLdμLd̂v5ϵ5dμ5d̂v4ϵ3dμ3d̂v2ϵ2dμ2d̂v1ϵ1dμ1dθϵ1dμ1dv0ϵ4dμ4dv0v0A PREPRINT - OCTOBER 7, 2020

of parameter-linear functions, requires only Hebbian plasticity. Our results provide a straightforward recipe for the
direct implementation of predictive coding algorithms to approximate certain computation graphs, such as those found
in common machine learning algorithms, in a biologically plausible manner. Next, we showcase this capability by
developing predictive coding variants of core machine learning architectures - convolutional neural networks (CNNs)
recurrent neural networks (RNNs) and LSTMs (Hochreiter & Schmidhuber, 1997), and show performance comparable
with backprop on tasks substantially more challenging than MNIST2.

Figure 3: Top Row: Training and test accuracy plots for the predictive coding and backprop CNN on SVHN,CIFAR10,
and CIFAR10 dataest over 5 seeds. Bottom row: Training and test accuracy plots for the predictive coding and backprop
RNN and LSTM. Performance is largely indistinguishable. Due to the need to iterate the vs until convergence, the
predictive coding network had roughly a 100x greater computational cost than the backprop network.

4.2 Predictive Coding CNN, RNN, and LSTM

First, we constructed predictive coding CNN models (see Appendix B for full implementation details). In the predictive
coding CNN, each ﬁlter kernel was augmented with ’error maps’ which measured the difference between the forward
convolutional predictions and the backwards messages. Our CNN was composed of a convolutional layer, followed by
a max-pooling layer, then two further convolutional layers followed by 3 fully-connected layers. We compared our
predictive coding CNN to a backprop-trained CNN with the exact same architecture and hyperparameters. We tested
our models on three image classiﬁcation datasets signiﬁcantly more challenging than MNIST – SVHN, CIFAR10, and
CIFAR100. SVHN is a digit recognition task like MNIST, but has more naturalistic backgrounds, is in colour with
continuously varying inputs and contains distractor digits. CIFAR10 and CIFAR100 are large image datasets composed
of RGB 32x32 images. CIFAR10 has 10 classes of image, while CIFAR100 is substantially more challenging with
100 possible classes. In general, performance was identical between the predictive coding and backprop CNNs and
comparable to the standard performance of basic CNN models on these datasets, Moreover, the predictive coding
gradient remained close to the true numerical gradient throughout training.

We also constructed predictive coding RNN and LSTM models, thus demonstrating the ability of predictive coding
to scale to non-parameter-linear, branching, computation graphs. The RNN was trained on a character-level name
classiﬁcation task, while the LSTM was trained on a next-character prediction task on the full works of Shakespeare. Full
implementation details can be found in Appendices B and C. LSTMs and RNNs are recurrent networks which are trained

2Code

to

reproduce

all

experiments

and

ﬁgures

in

this

work

can

be

found

at

https://github.com/BerenMillidge/PredictiveCodingBackprop

8

A PREPRINT - OCTOBER 7, 2020

through backpropagation through time (BPTT). BPTT simply unrolls the network through time and backpropagates
through the unrolled graph. Analogously we trained the predictive coding RNN and LSTM by applying predictive
coding to the unrolled computation graph. The depth of the unrolled graph depends heavily on the sequence length,
and in our tasks using a sequence length of 100 we still found that predictive coding evinced rapid convergence to the
correct numerical gradient, thus showing that the algorithm is scalable even to very deep computation graphs.

5 Discussion

We have shown that predictive coding provides a local and often biologically plausible approximation to backprop on
arbitrary, deep, and branching computation graphs. Moreover, convergence to the exact backprop gradients is rapid
and robust, even in extremely deep graphs such as the unrolled LSTM. Our algorithm is fully parallelizable, does not
require separate phases, and can produce equivalent performance to backprop in core machine-learning architectures.
These results broaden the horizon of local approximations to backprop by demonstrating that they can be implemented
on arbitrary computation graphs, not only simple MLP architectures. Our work prescribes a straightforward recipe for
backpropagating through any computation graph with predictive coding using only local learning rules. In the future,
this process could potentially be made fully automatic and translated onto neuromorphic hardware. Our results also
raise the possibility that the brain may implement machine-learning type architectures much more directly than often
considered. Many lines of work suggest a close correspondence between the representations and activations of CNNs
and activity in higher visual areas (Eickenberg, Gramfort, Varoquaux, & Thirion, 2017; Khaligh-Razavi & Kriegeskorte,
2014; Lindsay, 2020; Tacchetti, Isik, & Poggio, 2017; Yamins et al., 2014), for instance, and this similarity may be
found to extend to other machine learning architectures.

Although we have implemented three core machine learning architectures as predictive coding networks, we have
nevertheless focused on relatively small and straightforward networks and thus both our backprop and predictive
coding networks perform below the state of the art on the presented tasks. This is primarily because our focus was
on demonstrating the theoretical convergence between the two algorithms. Nevertheless, we believe that due to the
generality of our theoretical results, ’scaling up’ the existing architectures to implement performance-matched predictive
coding versions of more advanced machine learning architectures such as resnets (He, Zhang, Ren, & Sun, 2016),
GANs (Goodfellow et al., 2014), and transformers (Vaswani et al., 2017) should be relatively straightforward.

In terms of computational cost, one inference iteration in the predictive coding network is about as costly as a backprop
backwards pass. Thus, due to using 100-200 iterations for full convergence, our algorithm is substantially more
expensive than backprop which limits the scalability of our method. However, this serial cost is misleading when talking
about highly parallel neural architectures. In the brain, neurons cannot wait for a sequential forward and backward
sweep. By phrasing our algorithm as a global descent, our algorithm is fully parallel across layers. There is no waiting
and no phases to be coordinated. Each neuron need only respond to its local driving inputs and downwards error signals.
We believe that this local and parallelizable property of our algorithm may engender the possibility of substantially
more efﬁcient implementations on neuromorphic hardware (Davies et al., 2018; Furber, Galluppi, Temple, & Plana,
2014; Merolla et al., 2014), which may ameliorate much of the computational overhead compared to backprop. Future
work could also examine whether our method is more capable than backprop of handling the continuously varying
inputs the brain is presented with in practice, rather than the artiﬁcial paradigm of being presented with a series of i.i.d.
datapoints.

Our work also reveals a close connection between backprop and inference. Namely, the recursive computation of
gradients is effectively a by-product of a variational-inference algorithm which infers the values of the vertices of the
computation graph under a hierarchical Gaussian generative model. While the deep connections between stochastic
gradient descent and inference in terms of Kalman ﬁltering (Ollivier, 2019; Ruck, Rogers, Kabrisky, Maybeck, & Oxley,
1992) or MCMC sampling methods (T. Chen, Fox, & Guestrin, 2014; Mandt, Hoffman, & Blei, 2017) is known, the
relation between recursive gradient computation itself and variational inference is underexplored except in the case of a

9

A PREPRINT - OCTOBER 7, 2020

single layer (Amari, 1995). Our method can provide a principled generalisation of backprop through the inverse-variance
Σ−1 parameters of the Gaussian generative model. These parameters weight the relative contribution of different factors
to the overall gradient by their uncertainty, thus naturally handling the case of backprop with differentially noisy inputs.
Moreover, the Σ−1 parameters can be learnt as a gradient descent on F: dΣi
. This
speciﬁc generalisation is afforded by the Gaussian form of the generative model, however, and other generative models
may yield novel optimisation algorithms able to quantify and handle uncertainties throughout the entire computational
graph.

dt = − dF
dΣi

i − Σ−1

= −Σ−1

i (cid:15)i(cid:15)T

i Σ−1

i

Acknowledgements

BM is supported by an EPSRC funded PhD Studentship. AT is funded by a PhD studentship from the Dr. Mortimer
and Theresa Sackler Foundation and the School of Engineering and Informatics at the University of Sussex. CLB
is supported by BBRSC grant number BB/P022197/1 and by Joint Research with the National Institutes of Natural
Sciences (NINS), Japan, program No. 01112005. AT is grateful to the Dr. Mortimer and Theresa Sackler Foundation,
which supports the Sackler Centre for Consciousness Science.

References

Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., & Tweed, D. B. (2019). Deep learning without weight transport.

In Advances in neural information processing systems (pp. 974–982).

Amari, S.-I. (1995). Information geometry of the em and em algorithms for neural networks. Neural networks, 8(9),

1379–1408.

Amit, Y. (2019). Deep learning with asymmetric connections and hebbian updates. Frontiers in computational

neuroscience, 13, 18.

Amos, B., & Yarats, D. (2019). The differentiable cross-entropy method. arXiv preprint arXiv:1909.12830.
Auksztulewicz, R., & Friston, K. (2016). Repetition suppression and its contextual determinants in predictive coding.

cortex, 80, 125–140.

Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical microcircuits

for predictive coding. Neuron, 76(4), 695–711.

Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic differentiation in machine learning:

a survey. The Journal of Machine Learning Research, 18(1), 5595–5637.

Beal, M. J., et al. (2003). Variational algorithms for approximate bayesian inference. university of London London.
Bengio, Y., & Fischer, A. (2015). Early inference in energy-based models approximates back-propagation. arXiv

preprint arXiv:1510.02777.

Bengio, Y., Mesnard, T., Fischer, A., Zhang, S., & Wu, Y. (2017). Stdp-compatible approximation of backpropagation

in an energy-based model. Neural computation, 29(3), 555–577.

Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the

American statistical Association, 112(518), 859–877.

Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning. Journal of

mathematical psychology, 76, 198–211.

Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and perception: A

mathematical review. Journal of Mathematical Psychology, 81, 55–79.

Chen, T., Fox, E., & Guestrin, C. (2014). Stochastic gradient hamiltonian monte carlo. In International conference on

machine learning (pp. 1683–1691).

Chen, T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. K. (2018). Neural ordinary differential equations. In

Advances in neural information processing systems (pp. 6571–6583).

Crick, F. (1989). The recent excitement about neural networks. Nature, 337(6203), 129–132.

10

A PREPRINT - OCTOBER 7, 2020

Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S. H., . . . others (2018). Loihi: A neuromorphic

manycore processor with on-chip learning. IEEE Micro, 38(1), 82–99.

Degrave, J., Hermans, M., Dambre, J., & Wyffels, F. (2019). A differentiable physics engine for deep learning in

robotics. Frontiers in neurorobotics, 13, 6.

Eickenberg, M., Gramfort, A., Varoquaux, G., & Thirion, B. (2017). Seeing it all: Convolutional network layers map

the function of the human visual system. NeuroImage, 152, 184–194.

Feldman, H., & Friston, K. (2010). Attention, uncertainty, and free-energy. Frontiers in human neuroscience, 4, 215.
Friston, K. (2003). Learning and inference in the brain. Neural Networks, 16(9), 1325–1352.
Friston, K. (2005). A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological

sciences, 360(1456), 815–836.

Friston, K. (2008). Hierarchical models in the brain. PLoS computational biology, 4(11).
Furber, S. B., Galluppi, F., Temple, S., & Plana, L. A. (2014). The spinnaker project. Proceedings of the IEEE, 102(5),

652–665.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., . . . Bengio, Y. (2014). Generative

adversarial nets. In Advances in neural information processing systems (pp. 2672–2680).

Griewank, A., et al. (1989). On automatic differentiation. Mathematical Programming: recent developments and

applications, 6(6), 83–107.

Guerguiev, J., Lillicrap, T. P., & Richards, B. A. (2017). Towards deep learning with segregated dendrites. Elife, 6,

e22901.

Hassabis, D., Kumaran, D., Summerﬁeld, C., & Botvinick, M. (2017). Neuroscience-inspired artiﬁcial intelligence.

Neuron, 95(2), 245–258.

Hawkins, J., & Blakeslee, S. (2007). On intelligence: How a new understanding of the brain will lead to the creation of

truly intelligent machines. Macmillan.

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the ieee

conference on computer vision and pattern recognition (pp. 770–778).

Heiden, E., Millard, D., & Sukhatme, G. (2019). Real2sim transfer using differentiable physics. In Workshop on

closing the reality gap in sim2real transfer for robotic manipulation.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735–1780.
Hohwy, J., Roepstorff, A., & Friston, K. (2008). Predictive coding explains binocular rivalry: An epistemological

review. Cognition, 108(3), 687–701.

Innes, M., Edelman, A., Fischer, K., Rackauckus, C., Saba, E., Shah, V. B., & Tebbutt, W.

(2019). Zygote:
A differentiable programming system to bridge machine learning and scientiﬁc computing. arXiv preprint
arXiv:1907.07587.

Kanai, R., Komura, Y., Shipp, S., & Friston, K. (2015). Cerebral hierarchies: predictive processing, precision and the
pulvinar. Philosophical Transactions of the Royal Society B: Biological Sciences, 370(1668), 20140169.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., . . . Amodei, D. (2020). Scaling laws for

neural language models. arXiv preprint arXiv:2001.08361.

Khaligh-Razavi, S.-M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain it

cortical representation. PLoS computational biology, 10(11).

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolutional neural networks.

In Advances in neural information processing systems (pp. 1097–1105).

Lee, D.-H., Zhang, S., Fischer, A., & Bengio, Y. (2015). Difference target propagation. In Joint european conference

on machine learning and knowledge discovery in databases (pp. 498–515).

Liao, Q., Leibo, J. Z., & Poggio, T. (2016). How important is weight symmetry in backpropagation? In Thirtieth aaai

conference on artiﬁcial intelligence.

Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2016). Random synaptic feedback weights support

error backpropagation for deep learning. Nature communications, 7(1), 1–10.

11

A PREPRINT - OCTOBER 7, 2020

Lillicrap, T. P., & Santoro, A. (2019). Backpropagation through time and the brain. Current opinion in neurobiology,

55, 82–89.

Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., & Hinton, G. (2020). Backpropagation and the brain. Nature

Reviews Neuroscience, 1–12.

Lindsay, G. (2020). Convolutional neural networks as a model of the visual system: past, present, and future. Journal

of Cognitive Neuroscience, 1–15.

Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a taylor expansion of

the local rounding errors. Master’s Thesis (in Finnish), Univ. Helsinki, 6–7.

Lotter, W., Kreiman, G., & Cox, D. (2016). Deep predictive coding networks for video prediction and unsupervised

learning. arXiv preprint arXiv:1605.08104.

Mandt, S., Hoffman, M. D., & Blei, D. M. (2017). Stochastic gradient descent as approximate bayesian inference. The

Journal of Machine Learning Research, 18(1), 4873–4907.

Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., . . . others (2014). A million
spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197),
668–673.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., . . . others (2015). Human-level

control through deep reinforcement learning. Nature, 518(7540), 529–533.

Okada, M., Rigazio, L., & Aoshima, T. (2017). Path integral networks: End-to-end differentiable optimal control.

arXiv preprint arXiv:1706.09597.

Ollivier, Y. (2019). The extended kalman ﬁlter is a natural gradient descent in trajectory space. arXiv preprint

arXiv:1901.00696.

Ollivier, Y., Tallec, C., & Charpiat, G. (2015). Training recurrent networks online without backtracking. arXiv preprint

arXiv:1507.07680.

Ororbia, A., Mali, A., Giles, C. L., & Kifer, D. (2020). Continual learning of recurrent neural networks by locally

aligning distributed representations. IEEE Transactions on Neural Networks and Learning Systems.

Pal, A. (2019). Raytracer. jl: A differentiable renderer that supports parameter optimization for scene reconstruction.

arXiv preprint arXiv:1907.07198.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., . . . Lerer, A. (2017). Automatic differentiation in

pytorch.

Rackauckas, C., Innes, M., Ma, Y., Bettencourt, J., White, L., & Dixit, V. (2019). Diffeqﬂux. jl-a julia library for neural

differential equations. arXiv preprint arXiv:1902.02376.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised

multitask learners. OpenAI Blog, 1(8), 9.

Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some

extra-classical receptive-ﬁeld effects. Nature neuroscience, 2(1), 79–87.

Revels, J., Lubin, M., & Papamarkou, T. (2016). Forward-mode automatic differentiation in julia. arXiv preprint

arXiv:1607.07892.

Richards, B. A., Lillicrap, T. P., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen, A., . . . others (2019). A deep

learning framework for neuroscience. Nature neuroscience, 22(11), 1761–1770.

Ruck, D. W., Rogers, S. K., Kabrisky, M., Maybeck, P. S., & Oxley, M. E.

(1992). Comparative analysis of
backpropagation and the extended kalman ﬁlter for training multilayer perceptrons. IEEE Transactions on Pattern
Analysis & Machine Intelligence(6), 686–691.

Rumelhart, D. E., & Zipser, D. (1985). Feature discovery by competitive learning. Cognitive science, 9(1), 75–112.
Sacramento, J., Costa, R. P., Bengio, Y., & Senn, W.

(2018). Dendritic cortical microcircuits approximate the

backpropagation algorithm. In Advances in neural information processing systems (pp. 8721–8732).

Scellier, B., & Bengio, Y. (2017). Equilibrium propagation: Bridging the gap between energy-based models and

backpropagation. Frontiers in computational neuroscience, 11, 24.

12

A PREPRINT - OCTOBER 7, 2020

Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y. (2018). Generalization of equilibrium propagation to vector

ﬁeld dynamics. arXiv preprint arXiv:1808.04873.

Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., . . . others (2019). Mastering atari, go,

chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265.

Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic transmission. Neuron,

40(6), 1063–1073.

Shipp, S. (2016). Neural elements for predictive coding. Frontiers in psychology, 7, 1792.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., . . . others (2017). Mastering the game

of go without human knowledge. Nature, 550(7676), 354–359.

Spratling, M. W. (2008). Reconciling predictive coding and biased competition models of cortical function. Frontiers

in computational neuroscience, 2, 4.

Steil, J. J. (2004). Backpropagation-decorrelation: online recurrent learning with o (n) complexity. In 2004 ieee

international joint conference on neural networks (ieee cat. no. 04ch37541) (Vol. 2, pp. 843–848).

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., . . . Rabinovich, A. (2015). Going deeper with
convolutions. In Proceedings of the ieee conference on computer vision and pattern recognition (pp. 1–9).
Tacchetti, A., Isik, L., & Poggio, T. (2017). Invariant recognition drives neural representations of action sequences.

PLoS computational biology, 13(12), e1005859.

Tallec, C., & Ollivier, Y. (2017). Unbiased online recurrent optimization. arXiv preprint arXiv:1702.05043.
Tzen, B., & Raginsky, M. (2019). Neural stochastic differential equations: Deep latent gaussian models in the diffusion

limit. arXiv preprint arXiv:1905.09883.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is all

you need. In Advances in neural information processing systems (pp. 5998–6008).

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., . . . others (2019). Grandmaster

level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782), 350–354.

Watanabe, E., Kitaoka, A., Sakamoto, K., Yasugi, M., & Tanaka, K. (2018). Illusory motion reproduced by deep neural

networks trained for prediction. Frontiers in psychology, 9, 345.

Weilnhammer, V., Stuke, H., Hesselmann, G., Sterzer, P., & Schmack, K. (2017). A predictive coding account of

bistable perception-a model-based fmri study. PLoS computational biology, 13(5), e1005536.

Werbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. In System modeling and optimization

(pp. 762–770). Springer.

Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation algorithm in a predictive

coding network with local hebbian synaptic plasticity. Neural computation, 29(5), 1229–1262.

Whittington, J. C., & Bogacz, R. (2019). Theories of error back-propagation in the brain. Trends in cognitive sciences.
Williams, R. J., & Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks.

Neural computation, 1(2), 270–280.

Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., & DiCarlo, J. J. (2014). Performance-optimized
hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of
Sciences, 111(23), 8619–8624.

Appendix A: Predictive Coding CNN Implementation Details

The key concept in a CNN is that of an image convolution, where a small weight matrix is ’slid’ (or convolved) across
an image to produce an output image. Each patch of the output image only depends on a relatively small patch of the
input image. Moreover, the weights of the ﬁlter stay the same during the convolution, so each pixel of the output image
is generated using the same weights. The weight sharing implicit in the convolution operation enforces translational
invariance, since different image patches are all processed with the same weights.

13

A PREPRINT - OCTOBER 7, 2020

The forward equations of a convolutional layer for a speciﬁc output pixel

vi,j =

k=i+f
(cid:88)

l=j+f
(cid:88)

k=i−f

l=j−f

θk,lxi+k,j+l

Where vi,j is the (i, j)th element of the output, xi,j is the element of the input image and θk,l is an weight element of a
feature map. To setup a predictive coding CNN, we augment each intermediate xi and vi with error units (cid:15)i of the same
dimension as the output of the convolutional layer.

Predictions ˆv are projected forward using the forward equations. Prediction errors also need to be transmitted backwards
for the architecture to work. To achieve this we must have that prediction errors are transmitted upwards by a ’backwards
convolution’. We thus deﬁne the backwards prediction errors ˆ(cid:15)j as follows:

ˆ(cid:15)i,j =

i+f
(cid:88)

j+f
(cid:88)

k=i−f

l=j−f

θj,i˜(cid:15)i,j

Where ˜(cid:15) is an error map zero-padded to ensure the correct convolutional output size. Inference in the predictive coding
network then proceeds by updating the intermediate values of each layer as follows:

dvl
dt

= (cid:15)l − ˆ(cid:15)l+1

Since the CNN is also parameter-linear, weights can be updated using the simple Hebbian rule of the multiplication of
the pre and post synaptic potentials.

dθl
dt

(cid:88)

=

i,j

(cid:15)li,j vl−1

T
i,j

There is an additional biological implausibility here due to the weight sharing of the CNN. Since the same weights
are copied for each position on the image, the weight updates have contributions from all aspects of the image
simultaneously which violates the locality condition. A simple ﬁx for this, which makes the network scheme plausible
is to simply give each position on the image a ﬁlter with separate weights, thus removing the weight sharing implicit in
the CNN. In effect this gives each patch of pixels a local receptive ﬁeld with its own set of weights. The performance
and scalability of such a locally connected predictive coding architecture would be an interesting avenue for future
work, as this architecture has substantial homologies with the structure of the visual cortex.

Figure 4: Training loss plots for the Predictive Coding and Backprop CNN on SVHN,CIFAR10, and CIFAR10 dataest
over 5 seeds.

In our experiments we used a relatively simple CNN architecture consisting of one convolutional layer of kernel size 5,
and a ﬁlter bank of 6 ﬁlters. This was followed by a max-pooling layer with a (2,2) kernel and a further convolutional
layer with a (5,5) kernel and ﬁlter bank of 16 ﬁlters. This was then followed by three fully connected layers of 200,
150, and 10 (or 100 for CIFAR100) output units. Although this architecture is far smaller than state of the art for

14

A PREPRINT - OCTOBER 7, 2020

convolutional networks, the primary point of our paper was to demonstrate the equivalence of predictive coding and
backprop. Further work could investigate scaling up predictive coding to more state-of-the-art architectures.

Our datasets consisted of 32x32 RGB images. We normalised the values of all pixels of each image to lie between 0 and
1, but otherwise performed no other image preprocessing. We did not use data augmentation of any kind. We set the
weight learning rate for the predictive coding and backprop networks 0.0001. A minibatch size of 64 was used. These
parameters were chosen without any detailed hyperparameter search and so are likely suboptimal. The magnitude of the
gradient updates was clamped to lie between -50 and 50 in all of our models. This was done to prevent divergences, as
occasionally occurred in the LSTM networks, likely due to exploding gradients.

The predictive coding scheme converged to the exact backprop gradients very precisely within 100 inference iterations
using an inference learning rate of 0.1. This gives the predictive coding CNN approximately a 100x computational
overhead compared to backprop. The divergence between the true and approximate gradients remained approximately
constant throughout training, as shown by Figure 5, which shows the mean divergence for each layer of the CNN over
the course of an example training run on the CIFAR10 dataset. The training loss of the predictive coding and backprop
networks for SVHN, CIFAR10 and CIFAR100 are presented in Figure 4.

Appendix B: Predictive Coding RNN

The computation graph on RNNs is relatively straightforward. We consider only a single layer RNN here although the
architecture can be straightforwardly extended to hierarchically stacked RNNs. An RNN is similar to a feedforward
network except that it possesses an additional hidden state h which is maintained and updated over time as a function of
both the current input x and the previous hidden state. The output of the network y is a function of h. By considering
the RNN at a single timestep we obtain the following equations.

ht = f (θhht−1 + θxxt)

yt = g(θyht)

(8)

Where f and g are elementwise nonlinear activation functions. And θh, θx, θy are weight matrices for each speciﬁc
input. To predict a sequence the RNN simply rolls forward the above equations to generate new predictions and hidden
states at each timestep.

RNNs are typically trained through an algorithm called backpropagation through time (BPTT) which essentially just
unrolls the RNN into a single feedforward computation graph and then performs backpropagation through this unrolled
graph. To train the RNN using predictive coding we take the same approach and simply apply predictive coding to the
unrolled graph.

It is important to note that this is an additional aspect of biological implausibility that we do not address in this paper.
BPTT requires updates to proceed backwards through time from the end of the sequence to the beginning. Ignoring
any biological implausibility with the rules themselves, this updating sequence is clearly not biologically plausible as
naively it requires maintaining the entire sequence of predictions and prediction errors perfectly in memory until the
end of the sequence, and waiting until the sequence ends before making any updates. There is a small literature on
trying to produce biologically plausible, or forward-looking approximations to BPTT which does not require updates to
be propagated back through time (Lillicrap & Santoro, 2019; Ollivier, Tallec, & Charpiat, 2015; Steil, 2004; Tallec
& Ollivier, 2017; Williams & Zipser, 1989). While this is a fascinating area, we do not address it in this paper. We
are solely concerned with the fact that predictive coding approximates backpropagation on feedforward computation
graphs for which the unrolled RNN graph is a sufﬁcient substrate.

To learn a predictive coding RNN, we ﬁrst augment each of the variables ht and yt of the original graph with additional
error units (cid:15)ht and (cid:15)yt. Predictions ˆyt, ˆht are generated according to the feedforward rules (16). A sequence of true
labels {T1...TT } is then presented to the network, and then inference proceeds by recursively applying the following

15

A PREPRINT - OCTOBER 7, 2020

(a) Conv Layer 1

(b) Conv Layer 2

(c) FC Layer 1

(d) FC Layer 2

Figure 5: Mean divergence between the true numerical and predictive coding backprops over the course of training. In general, the
divergence appeared to follow a largely random walk pattern, and was generally neglible. Importantly, the divergence did not grow
over time throughout training, implying that errors from slightly incorrect gradients did not appear to compound.

rules backwards through time until convergence.

(cid:15)yt = L − ˆyt
(cid:15)ht = ht − ˆht
dht
dt

= (cid:15)ht − (cid:15)ytθT

y − (cid:15)ht+1 θT
h

Upon convergence the weights are updated according to the following rules.

dθy
dt

dθx
dt

dθh
dt

=

=

=

T
(cid:88)

t=0

T
(cid:88)

t=0

T
(cid:88)

t=0

(cid:15)yt

∂g(θyht)
∂θy

hT
t

(cid:15)ht

∂f (θhht−1 + θxxt)
∂θx

xT
t

(cid:15)ht

∂f (θhht−1 + θxxt)
∂θh

hT
t+1

16

A PREPRINT - OCTOBER 7, 2020

Since the RNN feedforward updates are parameter-linear, these rules are Hebbian, only requiring the multiplication of
pre and post-synaptic potentials. This means that the predictive coding updates proposed here are biologically plausible
and could in theory be implemented in the brain. The only biological implausibility remains the BPTT learning scheme.

trained on a simple character-level name-origin dataset which can be found here:
Our RNN was
https://download.pytorch.org/tutorial/data.zip. The RNN was presented with sequences of characters representing
names and had to predict the national origin of the name – French, Spanish, Russian, etc. The characters were presented
to the network as one-hot-encoded vectors without any embedding. The output categories were also presented as a
one-hot vector. The RNN has a hidden size of 256 units. A tanh nonlinearity was used between hidden states and the
output layer was linear. The network was trained on randomly selected name-category pairs from the dataset. The
training loss for the predictive coding and backprop RNNs, averaged over 5 seeds is presented below (Figure 6).

Figure 6: Training losses for the predictive coding and backprop RNN. As expected, they are effectively identical.

Appendix C: Predictive Coding LSTM Implementation Details

Unlike the other two models, the LSTM possesses a complex and branching internal computation graph, and is thus a
good opportunity to make explicit the predictive coding ’recipe’ for approximating backprop on arbitrary computation
graphs. The computation graph for a single LSTM cell is shown (with backprop updates) in Figure 7. Prediction for
the LSTM occurs by simply rolling forward a copy of the LSTM cell for each timestep. The LSTM cell receives its
hidden state ht and cell state ct from the previous timestep. During training we compute derivatives on the unrolled
computation graph and receive backwards derivatives (or prediction errors) from the LSTM cell at time t + 1.

17

A PREPRINT - OCTOBER 7, 2020

Figure 7: Computation graph and backprop learning rules for a single LSTM cell.

The equations that specify the computation graph of the LSTM cell are as follows.

v1 = ht ⊕ xt

v2 = σ(θiv1)

v3 = ctv2

v4 = σ(θinpv1)

v5 = tanh(θcv1)

v6 = v4v5

v7 = v3 + v6

v8 = σ(θov1)

v9 = tanh(v7)

v10 = v8v9

y = σ(θyv10)

The recipe to convert this computation graph into a predictive coding algorithm is straightforward. We ﬁrst rewire
the connectivity so that the predictions are set to the forward functions of their parents. We then compute the errors

18

⊕θfxthtσ×ctθinpθcσtanh×+σθo×tanhct+1ht+1σθyyt−Tv1=ht⊕xtv2=σ(θfv1)v3=ctv2v4=σ(θinpv1)v5=tanh(θcv1)v6=v5v4v7=v3+v6v8=σ(θ0v1)v9=tanh(v7)v10=v8v9y=σ(θyv10)dLdydLdydydv10dLdht+1dht+1dv10dLdv10dv10dv9dLdv9dv9dv7dLdct+1dct+1dv7dLdv10dv10dv8dLdv7dv7dv3dLdv7dv7dv6dLdv6dv6dv4dLdv6dv6dv5dLdv5dv5dv1dLdv8dv8dv1dLdv4dv4dv1dLdv2dv2dv1dLdv3dv3dv2dLdv3dv3dctdLdv1dv1dxtdLdv1dv1dhtbetween the vertices and the predictions.

A PREPRINT - OCTOBER 7, 2020

ˆv1 = ht ⊕ xt

ˆv2 = σ(θiv1)

ˆv3 = ctv2

ˆv4 = σ(θinpv1)

ˆv5 = tanh(θcv1)

ˆv6 = v4v5

ˆv7 = v3 + v6

ˆv = σ(θov1)

ˆv9 = tanh(v7)

ˆv10 = v8v9

ˆvy = σ(θyv10)

(cid:15)1 = v1 − ˆv1

(cid:15)2 = v2 − ˆv2

(cid:15)3 = v3 − ˆv3

(cid:15)4 = v4 − ˆv4

(cid:15)5 = v5 − ˆv5

(cid:15)6 = v6 − ˆv6

(cid:15)7 = v7 − ˆv7

(cid:15)8 = v8 − ˆv8

(cid:15)9 = v9 − ˆv9

(cid:15)10 = v10 − ˆv10

During inference, the inputs ht,xt and the output yt are ﬁxed. The vertices and then the prediction errors are updated
according to Equation 2. This recipe is straightforward and can easily be extended to other more complex machine
learning architectures. The full augmented computation graph, including the vertex update rules, is presented in Figure
8.

Empirically, we observed rapid convergence to the exact backprop gradients even in the case of very deep computation
graphs (as is an unrolled LSTM with a sequence length of 100). Although convergence was slower than was the case for
CNNs or lesser sequence lengths, it was still straightforward to achieve convergence to the exact numerical gradients
with sufﬁcient iterations.

Below we plot the mean divergence between the predictive coding and true numerical gradients as a function of sequence
length (and hence depth of graph) for a ﬁxed computational budget of 200 iterations with an inference learning rate
of 0.05. As can be seen, the divergence increases roughly linearly with sequence length. Importantly, even with long
sequences, the divergence is not especially large, and can be decreased further by increasing the computational budget.
As the increase is linear, we believe that predictive coding approaches should be scalable even for backpropagating
through very deep and complex graphs.

Our architecture consisted of a single LSTM layer (more complex architectures would consist of multiple stacked
LSTM layers). The LSTM was trained on a next-character character-level prediction task. The dataset was the full
works of Shakespeare, downloadable from Tensorﬂow. The text was shufﬂed and split into sequences of 50 characters,
which were fed to the LSTM one character at a time. The LSTM was trained then to predict the next character, so as
to ultimately be able to generate text. The characters were presented as one-hot-encoded vectors. The LSTM had a

19

A PREPRINT - OCTOBER 7, 2020

Figure 8: The LSTM cell computation graph augmented with error units, evincing the connectivity scheme of the
predictive coding algorithm.

hidden size and a cell-size of 1056 units. A minibatch size of 64 was used and a weight learning rate of 0.0001 was
used for both predictive coding and backprop networks. To achieve sufﬁcient numerical convergence to the correct
gradient, we used 200 variational iterations with an inference learning rate of 0.1. This rendered the predictive LSTM
approximately 200x as costly as the backprop LSTM to run. A graph of the LSTM training loss for both predictive
coding and backprop LSTMs, averaged over 5 random seeds, can be found below (Figure 10).

Appendix D: Derivation of the Free Energy Functional

Here we derive in detail the form of the free-energy functional used in sections 2 and 4. We also expand upon the
assumptions required and the precise form of the generative model and variational density. Much of this material is
presented with considerably more detail in Buckley et al. (2017), and more approachably in Bogacz (2017).

Given an arbitrary computation graph with vertices {yi}, which we treat as random variables. Here we treat explicitly
an important fact that we glossed over for notational convenience in the introduction. The vis which are optimized
in the free-energy functional are technically the mean parameters of the variational density Q(yi; vi, σi) – i.e. they
represent the mean (variational) belief of the value of the vertex. The vertex values in the model, which we here denote
as {yi}, are technically separate. However, due to our Gaussian assumptions, and the expectation under the variational
density, in effect we end up replacing the yi with the vi and optimizing the vis, so in the interests of space and notational
simplicity we began as if the vis were variables in the generative model, but they are not. They are parameters of the
variational distribution.

Given an input y0 and a target yN (the multiple input and/or output case is a straightforward generalization). We wish to
infer the posterior p(y1:N −1|y0, yN ). We approximate this intractable posterior with variational inference. Variational
inference proceeds by deﬁning an approximate posterior Q(y1:N −1; φ) with some arbitrary parameters φ. We then wish

20

⊕θfxthtσ×ctθinpθcσtanh×+σθo×tanhct+1ht+1σθyv1=ht⊕xtv2=σ(θfv1)μ3=ctv2μ4=σ(θinpv1)μ6=v5v4μ7=v3+v6μ8=σ(θ0v1)μ9=tanh(v7)μ10=v8v9μy=σ(θyv10)ϵyTv10ϵ10v9ϵ9v7v3v6v5v8v4v4ϵ7ϵ6ϵ3ϵ4ϵ8ϵ5ϵ2ϵht+1ϵct+1ϵydμydv10ϵ10dμ10dv9ϵ9dμ9dv7ϵ7dμ7dv3ϵ7dμ7dv6ϵ6dμ6dv5ϵ10dμ10dv8ϵ8dμ8dv1ϵ5dμ5dv1ϵ4dμ4dv1ϵ2dμ2dv1ϵ3dμ3dv2ϵ3ϵ6dμ6dv4A PREPRINT - OCTOBER 7, 2020

Figure 9: Divergence between predictive coding and numerical gradients as a function of sequence length.

to minimize the KL divergence between the true and approximate posterior.

argmin
φ

KL[Q(y1:N −1; φ)(cid:107)p(y1:N −1|y0, yN )]

Although this KL is itself intractable, since it includes the intractable posterior, we can derive a tractable bound on this
KL called the variational free-energy.

KL[Q(y1:N −1; φ)(cid:107)p(y1:N |y0, yN )] = KL[Q(y1:N −1)(cid:107)

p(y1:N , y0, yN )
p(y0, yN )

]

= KL[Q(y1:N ; φ)(cid:107)p(y1:N , y0)] + ln p(y0, yN )
⇒ KL[Q(y1:N ; φ)(cid:107)p(y1:N −1, y0, yN )]
(cid:123)(cid:122)
(cid:125)
−F

(cid:124)

≤ KL[Q(y1:N −1; φ)(cid:107)p(y1:N −1|y0, yN )]

We deﬁne the negative free-energy −F = KL[Q(y1:N −1)(cid:107)p(y1:N −1, y0, yN )] which is a lower bound on the divergence
between the true and approximate posteriors. By thus maximizing the negative free-energy (which is identical to
the ELBO (Beal et al., 2003; Blei, Kucukelbir, & McAuliffe, 2017)), or equivalently minimizing the free-energy, we
decrease this divergence and make the variational distribution a better approximation to the true posterior.

To proceed further, it is necessary to deﬁne an explicit form of the generative model p(y0, y1:N −1, yN ) and the
approximate posterior Q(y1:N −1; φ). In predictive coding, we deﬁne a hierarchical Gaussian generative model which
mirrors the exact structure of the computation graph

(9)

p(y0:N ) = N (y0; ¯y0, Σ0)

N
(cid:89)

i=1

N (yi; f (P(yi); θyj ∈P(yi)), Σi);

21

A PREPRINT - OCTOBER 7, 2020

Figure 10: Training losses for the predictive coding and backprop LSTMs averaged over 5 seeds. The performance of
the two training methods is effectively equivalent.

Where essentially each vertex yi is a Gaussian with a mean which is a function of the prediction of all the parents of the
vertex, and the parameters of their edge-functions. ¯y0 is effectively an "input-prior" which is set to 0 throughout and
ignored. The output vertices yN = T are set to the target T .

We also deﬁne the variational density to be Gaussian with mean v1:N −1 and variance σ1:N −1, but under a mean ﬁeld
approximation, so that the approximation at each node is independent of all others (note the variational variance is
denoted σ while the variance of the generative model is denoted Σ. The lower-case σ is not used to denote a scalar
variable – both variances can be multivariate – but to distinguish between variational and generative variances)

Q(y1:N −1; v1:N −1, σ1:N −1) =

N −1
(cid:89)

i=1

N (yi; vi, σi)

We now can express the free-energy functional concretely. First we decompose it as the sum of an energy and an entropy

−F = KL[Q(y1:N −1; v1:N −1, σ1:N −1)(cid:107)p(y0, y1:N −1, yN )]
= −EQ(y1:N −1;v1:N −1,σ1:N −1)[ln p(y0, y1:N −1, yN )]
(cid:123)(cid:122)
(cid:125)
Energy

(cid:124)

+ EQ(y1:N −1;v1:N −1,σ1:N −1)[ln Q(y1:N −1; v1:N −1, σ1:N −1)]
(cid:123)(cid:122)
(cid:125)
Entropy

(cid:124)

22

A PREPRINT - OCTOBER 7, 2020

Then, taking the entropy term ﬁrst, we can express it concretely in terms of normal distributions.

EQ(y1:N −1;v1:N −1,σ1:N −1)[ln Q(y1:N −1; v1:N −1, σ1:N −1)] = EQ(y1:N −1;v1:N −1,σ1:N −1)[

N −1
(cid:88)

i=1

ln N (yi; vi, σi)]

=

=

=

=

N −1
(cid:88)

i=1

N −1
(cid:88)

i=1

N −1
(cid:88)

i=1

EQ(yi;vi,σi)[ln N (yi; vi, σi)]

EQ(yi;vi,σi)[−

1
2

ln det(2πσi]) + EQ(yi;vi,σi)[

(yi − vi)2
2σi

]

−

1
2

ln det(2πσi]) +

σi
2σi

N
2

+

N −1
(cid:88)

i=1

−

1
2

ln det(2πσi)

The entropy of a multivariate gaussian has a simple analytical form depending only on the variance. Next we turn
to the energy term, which is more complex. To derive a clean analytical result, we must make a further assumption,
the Laplace approximation, which requires the variational density to be tightly peaked around the mean so the only
non-negligible contribution to the expectation is from regions around the mean. This means that we can successfully
approximate the approximate posterior with a second-order Taylor expansion around the mean. From the ﬁrst line
onwards we ignore the ln p(y0) and ln p(yN |P(yN )) which lie outside the expectation.

EQ(y1:N −1;v1:N −1,σ1:N −1)[ln p(y0:N )] = ln p(y0) + ln p(yN |P(yN )) +

N −1
(cid:88)

i=1

EQ(yi;vi,σi)[ln p(yi|P(yi))]

=

N
(cid:88)

i=1

+ EQ[

=

N
(cid:88)

i=1

EQ[ln p(vi|P(yi))] + EQ[

∂ ln p(yi|P(yi))
∂yi

(vi − yi)]

d2 ln p(vi|P(yi))
dy2
i

(vi − yi)2]

ln p(vi|P(yi)) +

∂2 ln p(vi|P(yi))
∂y2
i

σi

Where the second term in the Taylor expansion evaluates to 0 since EQ[yi − vi] = (vi − vi) = 0 and the third term
contains the expression for the variance EQ[(yi − vi)2] = σi.

We can then write out the full Laplace-encoded free-energy as:

−F =

N
(cid:88)

i=1

ln p(vi|P(yi)) +

∂2 ln p(vi|P(yi))
∂y2
i

σi − −

1
2

ln det(2πσi])

We wish to minimize F with respect to the variational parameters vi and σi. There is in fact a closed-form expression
for the optimal variational variance which can be obtained simply by differentiating and setting the derivative to 0.

∂F
∂σi

=

− σ−1
i

∂2 ln p(vi|P(yi))
∂y2
i
∂2 ln p(vi|P(yi))
∂y2
i

i =

−1

∂F
∂σi

= 0 ⇒ σ∗

Because of this analytical result for the variational variance, we do not need to consider it further in the optimisation
problem, and only consider minimizing the variational means vi. This renders all the terms in the free-energy except

23

A PREPRINT - OCTOBER 7, 2020

the ln p(vi|P(yi)) terms constant with respect to the variational parameters. This allows us to write:

−F ≈ ln p(yN |P(yN )) +

N
(cid:88)

i=1

ln p(vi|P(yi))

(10)

as presented in section 2. The ﬁrst term ln p(yN |P(yN )) is effectively the loss at the output (yN = T ) so becomes an
additional prediction error ln p(yN |P(yN )) ∝ (T − ˆvN )T Σ−1
N (T − ˆvN ) which can be absorbed into the sum over other
prediction errors. Crucially, although the variational variances have an analytical form, the variances of the generative
model (the precisions Σi) do not and can be optimised directly to improve the log model-evidence. These precisions
allow for a kind of ’uncertainty-aware’ backprop.

Derivation of Variational Update Rules and Fixed points

Here, starting from Equation 10, we show how to obtain the variational update rule for the vi’s (Equation 2), and the
ﬁxed point equations (Equation 5) (Bogacz, 2017; Friston, 2005, 2008). We ﬁrst reduce the free-energy to a sum of
prediction errors.

−F ≈

≈

=

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

ln p(vi|P(vi))

(vi − f (P(v1))T Σ−1

i

(vi − f (P(v1))T + ln 2πΣ−1

i

i (cid:15)i + ln 2πΣ−1
(cid:15)T

i

Where (cid:15)i = vi − f (P(v1), and we have utilized the assumption made in section 2 that Σ−1 = I. By setting all
precisions to the identity, we are implicitly assuming that all datapoints and vertices of the computational graph have
equal variance. Next we assume that the dynamics of each vertex vi follow a gradient descent on the free-energy.

−

dvi
dt

=

∂F
∂vi

=

∂
∂vi

[

N
(cid:88)

j=1

= (cid:15)i

∂(cid:15)i
∂vi

+

(cid:15)T
j (cid:15)j]

(cid:88)

j∈C(vi)

(cid:15)j

∂(cid:15)j
∂vi

= (cid:15)i −

(cid:88)

j∈C(vi)

(cid:15)j

∂ˆvj
∂vi

Where we have used the fact that ∂(cid:15)i
∂vi
for dvi

dt = 0.

= 1 and ∂(cid:15)j
∂vj

= − ∂ ˆvj
∂vi

. To obtain the ﬁxed point of the dynamics, we simply solve

dvi
dt

=

∂F
∂vi

= 0

⇒ 0 = (cid:15)i −

(cid:88)

j∈C(vi)

(cid:15)j

∂ˆvj
∂vi

⇒ (cid:15)∗

i =

(cid:88)

j∈C(vi)

(cid:15)j

∂ˆv∗
j
∂v∗
i

Similarly, since (cid:15)∗

i = v∗

i − ˆv∗

i then v∗

i = (cid:15)∗

i + ˆv∗

i . So:

24

A PREPRINT - OCTOBER 7, 2020

i = (cid:15)∗
v∗

i + ˆv∗
i

= ˆv∗

i −

(cid:88)

j∈C(vi)

(cid:15)j

∂ˆv∗
j
∂v∗
i

25

