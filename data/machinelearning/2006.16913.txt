0
2
0
2

v
o
N
5

]

Y
C
.
s
c
[

3
v
3
1
9
6
1
.
6
0
0
2
:
v
i
X
r
a

Synthesizing Tasks for Block-based Programming∗

Umair Z. Ahmed1 Maria Christakis2 Aleksandr Efremov2 Nigel Fernandez2
Ahana Ghosh2 Abhik Roychoudhury1 Adish Singla2
1National University of Singapore, {umair, abhik}@comp.nus.edu.sg,
2MPI-SWS, {maria, aefremov, nfernand, gahana, adishs}@mpi-sws.org

Abstract

Block-based visual programming environments play a critical role in introducing
computing concepts to K-12 students. One of the key pedagogical challenges in
these environments is in designing new practice tasks for a student that match
a desired level of difﬁculty and exercise speciﬁc programming concepts. In this
paper, we formalize the problem of synthesizing visual programming tasks. In
particular, given a reference visual task Tin and its solution code Cin, we propose a
novel methodology to automatically generate a set {(Tout, Cout)} of new tasks along
with solution codes such that tasks Tin and Tout are conceptually similar but visually
dissimilar. Our methodology is based on the realization that the mapping from the
space of visual tasks to their solution codes is highly discontinuous; hence, directly
mutating reference task Tin to generate new tasks is futile. Our task synthesis
algorithm operates by ﬁrst mutating code Cin to obtain a set of codes {Cout}. Then,
the algorithm performs symbolic execution over a code Cout to obtain a visual task
Tout; this step uses the Monte Carlo Tree Search (MCTS) procedure to guide the
search in the symbolic tree. We demonstrate the effectiveness of our algorithm
through an extensive empirical evaluation and user study on reference tasks taken
from the Hour of Code: Classic Maze challenge by Code.org and the Intro to
Programming with Karel course by CodeHS.com.

1

Introduction

Block-based visual programming environments are increasingly used nowadays to introduce com-
puting concepts to novice programmers including children and K-12 students. Led by the success of
environments like Scratch [29], initiatives like Hour of Code by Code.org [24] (HOC) and online plat-
forms like CodeHS.com [21], block-based programming has become an integral part of introductory
computer science education. Considering HOC alone, over one billion hours of block-based program-
ming activity has been performed so far by over 50 million unique students worldwide [24, 35].

The societal need for enhancing K-12 computing education has led to a surge of interest in developing
AI-driven systems for pedagogy of block-based programming [33, 26, 27, 34, 16]. Existing works
have studied various aspects of intelligent support, including providing real-time next-step hints when
a student is stuck solving a task [20, 36, 18, 17, 9], giving data-driven feedback about a student’s
misconceptions [31, 19, 28, 30, 35], and demonstrating a worked-out solution for a task when a
student lacks the required programming concepts [37]. An underlying assumption when providing
such intelligent support is that afterwards the student can practice new similar tasks to ﬁnally learn the
missing concepts. However, this assumption is far from reality in existing systems—the programming
tasks are typically hand-curated by experts/tutors, and the available set of tasks is limited. Consider
HOC’s Classic Maze challenge [23], which provides a progression of 20 tasks: Millions of students
have attempted these tasks, yet when students fail to solve a task and receive assistance, they cannot
practice similar tasks, hindering their ability to master the desired concepts. We seek to tackle this
pedagogical challenge by developing techniques for synthesizing new programming tasks.

∗Authors listed alphabetically; Correspondence to: Ahana Ghosh <gahana@mpi-sws.org>.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
def Run(){
RepeatUntil(goal){
move
If(pathLeft){
turnLeft

}

}

}

def Run(){
move
turnLeft
RepeatUntil(goal){
move
If(pathRight){
turnRight

}

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 1: Illustration of our methodology for task Maze 16 from the Hour of Code: Classic Maze
challenge by Code.org [23]; the complete list of tasks with their speciﬁcations is in Fig. 6.

def Run(){
putMarker
While(pathAhead){

move
turnLeft
move
turnRight
putMarker

}

}

def Run(){
putMarker
While(pathAhead){

move
move
turnRight
move
turnLeft
putMarker

}

}

(a) Visual puzzle for Tin

(d) Solution code Cout
Figure 2: Illustration of our methodology for task Diagonal from the Intro to Programming with
Karel course by CodeHS.com [22]; the complete list of tasks with their speciﬁcations is in Fig. 6.

(c) Visual puzzle for Tout

(b) Solution code Cin

We formalize the problem of synthesizing visual programming tasks of the kind found in popular
learning platforms like Code.org (see Fig. 1) and CodeHS.com (see Fig. 2). As input, we are given a
reference task Tin, speciﬁed as a visual puzzle, and its solution code Cin. Our goal is to synthesize
a set {(Tout, Cout)} of new tasks along with their solution codes that are conceptually similar but
visually dissimilar to the input. This is motivated by the need for practice tasks that on one hand
exercise the same concepts, while looking fresh in order to maintain student engagement.

When tackling the problem of synthesizing new tasks with the above desirable properties, three key
challenges emerge. First, we are generating problems in a conceptual domain with no well-deﬁned
procedure that students follow to solve a task—consequently, existing work on educational problem
generation in procedural domains does not apply in our setting [3, 11]. Second, the mapping from
the space of visual tasks to their solution codes is highly discontinuous; hence, template-based
problem generation techniques [32, 25] that rely on directly mutating the input to generate new
tasks is ineffective (see Section 5 where we use this approach as a baseline). Furthermore, such a
direct task-mutation approach would require access to an automated solution synthesizer; however,
state-of-the-art program synthesis techniques are not yet on par with experts and their minimal
solutions [5, 8, 6]. Third, the space of possible tasks and their solutions is potentially unbounded, and
thus, any problem generation technique that relies on exhaustive enumeration is intractable [32, 1, 2].

To overcome these challenges, we propose a novel methodology that operates by ﬁrst mutating the
solution code Cin to obtain a set of codes {Cout}, and then performing symbolic execution over a code
Cout to obtain a visual puzzle Tout. Mutation is efﬁcient by creating an abstract representation of Cin
along with appropriate constraints and querying an SMT solver [4]; any solution to this query is a
mutated code Cout. During symbolic execution, we use Monte Carlo Tree Search (MCTS) to guide
the search over the (unbounded) symbolic execution tree. We demonstrate the effectiveness of our
methodology by performing an extensive empirical evaluation and user study on a set of reference
tasks from the Hour of code challenge by Code.org and the Intro to Programming with Karel course
by CodeHS.com. In summary, our main contributions are:

• We formalize the problem of synthesizing block-based visual programming tasks (Section 2).
• We present a novel approach for generating new visual tasks along with solution codes such that

they are conceptually similar but visually dissimilar to a given reference task (Section 3).

• We demonstrate the effectiveness of our approach through an extensive empirical evaluation and
user study on reference tasks from real-world programming platforms (Section 4 and Section 5).

2 Problem Formulation

The space of tasks. We deﬁne a task as a tuple T := (Tvis, Tstore, Tsize), where Tvis denotes the visual
puzzle, Tstore the available block types, and Tsize the maximum number of blocks allowed in the

2

solution code. For instance, considering the task T := Tin in Fig. 1a, Tvis is illustrated in Fig. 1a,
Tstore = {move, turnL, turnR, RepeatUntil, If}, and Tsize = 4.
The space of codes. The programming environment has a domain-speciﬁc language (DSL), which
deﬁnes the set of valid codes C and is shown in Fig. 4a. A code C ∈ C is characterized by several
properties, such as the set Cblocks of block types in C, the number of blocks Csize, the depth Cdepth
of the corresponding Abstract Syntax Tree (AST), and the nesting structure Cstruct representing pro-
gramming concepts exercised by C. For instance, considering the code C := Cin in Fig. 1b, Cblocks =
{move, turnL, RepeatUntil, If}, Csize = 4, Cdepth = 3, and Cstruct = {Run{RepeatUntil{If}}}.
Below, we introduce two useful deﬁnitions relating the task and code space.
Deﬁnition 1 (Solution code). C is a solution code for T if the following holds: C successfully solves
the visual puzzle Tvis, Cblocks ⊆ Tstore, and Csize ≤ Tsize. CT denotes the set of all solution codes for T.
Deﬁnition 2 (Minimality of a task). Given a solvable task T with |CT| ≥ 1 and a threshold δ ∈ N,
the task is minimal if (cid:64)C ∈ CT such that Csize < Tsize − δ.

Next, we introduce two deﬁnitions formalizing the notion of conceptual similarity. Deﬁnition 3 for-
malizes conceptual similarity of a task T along with one solution code C. Since a task can have multiple
solution codes, Deﬁnition 4 provides a stricter notion of conceptual similarity of a task T for all its solu-
tion codes. These deﬁnitions are used in our objective of task synthesis in conditions (I) and (V) below.
Deﬁnition 3 (Conceptual similarity of (T, C)). Given a reference (Tin, Cin) and a threshold δ ∈ N,
a task T along with a solution code C is conceptually similar to (Tin, Cin) if the following holds:
Tstore = Tin
struct.
Deﬁnition 4 (Conceptual similarity of (T, ·)). Given a reference (Tin, Cin) and a threshold δ ∈ N, a
task T is conceptually similar to (Tin, Cin) if the following holds: Tstore = Tin
size| ≤ δ,
and ∀C ∈ CT, Cstruct = Cin

size| ≤ δ, and Cstruct = Cin

store, |Tsize − Tin

store, |Tsize − Tin

struct.

Environment domain knowledge. We now formalize our domain knowledge about the block-based
environment to measure visual dissimilarity of two tasks, and capture some notion of interestingness
and quality of a task. Given tasks T and T(cid:48), we measure their visual dissimilarity by an environment-
speciﬁc function Fdiss(Tvis, T(cid:48)
vis) ∈ [0, 1]. Moreover, we measure generic quality of a task with
function Fqual(Tvis, C) ∈ [0, 1]. We provide speciﬁc instantiations of Fdiss and Fqual in our evaluation.
Objective of task synthesis. Given a reference task Tin and a solution code Cin ∈ CTin as input, we
seek to generate a set {(Tout, Cout)} of new tasks along with solution codes that are conceptually
similar but visually dissimilar to the input. Formally, given parameters (δsize, δdiss, δqual), our objective
is to synthesize new tasks meeting the following conditions:

(I) (Tout, Cout) is conceptually similar to (Tin, Cin) with threshold δsize in Deﬁnition 3.
(II) Tout is visually dissimilar to Tin with margin δdiss, i.e., Fdiss(Tin
(III) Tout has a quality score above threshold δqual, i.e., Fqual(Tout

vis, Tout
vis , Cout) ≥ δqual.

vis ) ≥ δdiss.

In addition, depending on the use case, it is desirable that the new tasks satisfy the following criteria:
(IV) Cout is different from the input solution code, i.e., Cout (cid:54)= Cin.
(V) Tout is conceptually similar to (Tin, Cin) with threshold δsize in Deﬁnition 4.
(VI) Tout is minimal as per Deﬁnition 2 for a desired value of δmini (e.g., δmini = 0 or δmini = 1).

3 Our Task Synthesis Algorithm

We now present the pipeline of our algorithm
(see Fig. 3), which takes as input a reference task
Tin and its solution code Cin, and generates a set
{(Tout, Cout)} of new tasks with their solution
codes. The goal is for this set to be conceptually
Figure 3: Stages in our task synthesis algorithm.
similar to (Tin, Cin), but for new tasks {Tout} to
be visually dissimilar to Tin. This is achieved by two main stages: (1) mutation of Cin to obtain a
set {Cout}, and (2) symbolic execution of each Cout to create a task Tout. The ﬁrst stage, presented in
Section 3.1, converts Cin into an abstract representation restricted by a set of constraints (Fig. 3(a)),
which must be satisﬁed by any generated Cout (Fig. 3(b)). The second stage, described in Section 3.2,
applies symbolic execution on each code Cout to create a corresponding visual task Tout (Fig. 3(c))
while using Monte Carlo Tree Search (MCTS) to guide the search in the symbolic execution tree.

3

task Tincode Cinsketch, constraintstask Toutcode Cout(a)(b)(c)code C
rule y
rule s

:= def Run () do y
:= s | g | s; g
:= a | s; s | If (b) do s | If (b) do s Else s

| While (b) do s | Repeat (x) do s

:= RepeatUntil (goal) do s
rule g
action a := move | turnL | turnR | putM | pickM
:= pathA | noPathA | pathL | noPathL
bool b

| pathR | noPathR | marker | noMarker

iter x

:= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10

(a) Code DSL

sketch Q := def Run () do Y
rule Y
rule S

:= S | G | S; G
:= A | S; S | If (B) do S | If (B) do S Else S

| While (B) do S | Repeat (X) do S

:= RepeatUntil (goal) do S
rule G
Comments: A may be φ or take values of action a

A denotes a sequence A1, . . . , An

(b) Sketch DSL

def Run(){

RepeatUntil(goal){

move
If(pathLeft){
turnLeft

}

}

}

def Run(){
A1
1, A2
1 (A1)
RepeatUntil(goal){
2 (A2)

2, A5

2, A4

2, A3
2, A2
A1
If( B1){
3, A2
A1

}

3, A3

3, A4

3, A5

3 (A3)

}

}

Input: code C, sketch Q ← Ω(C), map ω(·| C), δsize, δiter
(∆0) Size of generated code may be at most Csize + δsize
(∆1) Edit action sequences ACTIONEDITS({A ∈ Q}, ω(·| C))
(∆2) For each X ∈ Q : |X − ω(X| C)| ≤ δiter
(∆3) Constraints induced by structure {Abefore; Repeat {A}; Aafter}

i. A is not a sufﬁx of Abefore
ii. A is not a preﬁx of Aafter

(∆4) For each B ∈ Q :

i. ω(B | C) ∈ {pathA, noPathA }
⇒ B ∈ {pathA, noPathA }

ii. ω(B | C) ∈ {pathL, noPathL pathR, noPathR }
⇒ B ∈ {pathL, noPathL, pathR, noPathR }

iii. ω(B | C) ∈ {marker, noMarker }
⇒ B ∈ { marker,noMarker }

(∆5) Constraints induced on A nested inside conditional B
(∆6) For each A ∈ Q, constraints ensuring minimality of A

(c) Types of Sketch Constraints

Input: Cin, Qin, ω(·| Cin), δsize = 2
(∆0) Up to 2 new actions may be added in total to A1, A2, A3
(∆1) Edit action sequences ACTIONEDITS({A1, A2, A3}, ω(·| Cin))
(∆4) B1 = pathL ∨ B1 = pathR
(∆5) ∃i ∈ [5] s.t. (cid:0)Ai
(∆5) ∃i ∈ [5] s.t. (cid:0)Ai
(∆6) A1, A2, A3 are minimal

3 = turnL ∧ ( ∀j < i, Aj
3 = turnR ∧ ( ∀j < i, Aj

3 /∈ {move, turnR})(cid:1)
3 /∈ {move, turnL})(cid:1)

(d) Code Cin

(e) Sketch Qin

(f) Qin-Constraints

Figure 4: Illustration of key steps in Code Mutation. Fig. 4d shows code Cin from Fig. 1b. The code
mutation stage, when applied to Cin, generates many output codes, including Cout in Fig. 1d.

3.1 Code Mutation

This stage in our pipeline mutates code Cin of task Tin such that its conceptual elements are preserved.
Our mutation procedure consists of three main steps. First, we generate an abstract representation of
Cin, called sketch. Second, we restrict the sketch with constraints that describe the space of its concrete
instantiations. Although this formulation is inspired from work on generating algebra problems [32],
we use it in the entirely different context of generating conceptually similar mutations of Cin. This is
achieved in the last step, where we use the sketch and its constraints to query an SMT solver [4]; the
query solutions are mutated codes {Cout} such that Cout

struct (see Deﬁnition 3).

struct = Cin

Step 1: Sketch. The sketch of code C, denoted by Q, is an abstraction of C capturing its skeleton and
generalizing C to the space of conceptually similar codes. Q, expressed in the language of Fig. 4b, is
generated from C with mapping Ω. In particular, the map exploits the AST structure of the code: the
AST is traversed in a depth-ﬁrst manner, and all values are replaced with their corresponding sketch
variables, i.e., action a, bool b, and iter x are replaced with A, B, and X, respectively. In the following,
we also use mapping ω(·| C), which takes a sketch variable in Q and returns its value in C.

In addition to the above, we may extend a variable A to an action sequence A, since any A is allowed
to be empty (φ). We may also add an action sequence of length δsize at the beginning and end of
the obtained sketch. As an example, consider the code in Fig. 4d and the resulting sketch in Fig. 4e.
Notice that, while we add an action sequence at the beginning of the sketch (A1), no action sequence
is appended at the end because construct RepeatUntil renders any succeeding code unreachable.

Step 2: Sketch constraints. Sketch constraints restrict the possible concrete instantiations of a
sketch by encoding the required semantics of the mutated codes. All constraint types are in Fig. 4c.

In particular, ∆0 restricts the size of the mutated code within δsize. ∆1 speciﬁes the allowed mutations
to an action sequence based on its value in the code, given by ω(A | C). For instance, this constraint
could result in converting all turnLeft actions of a sequence to turnRight. ∆2 restricts the
possible values of the Repeat counter within threshold δiter. ∆3 ensures that the Repeat counter is
optimal, i.e., action subsequences before and after this construct are not nested in it. ∆4 speciﬁes
the possible values of the If condition based on its value in the code, given by ω(B | C). ∆5 refers
to constraints imposed on action sequences nested within conditionals. As an example, consider

4

Figure 5: Illustration of symbolic execution on Cout from Fig. 1d. (b) shows the initial conﬁguration of
the agent’s location and orientation as well as the status of the grid cells (unknown, free, blocked, goal).
(c)–(e) show the symbolic execution steps where conditions goal and pathRight are False. (f)
shows the step where goal is True. (g) shows the post-processing step where a puzzle Tout
vis is obtained.

∆5 in Fig. 4f, which states that if B1 = pathLeft, then the nested action sequence must have at
least one turnLeft action, and the ﬁrst occurrence of this action must not be preceded by a move or
turnRight, thus preventing invalid actions within the conditional. ∆6 ensures minimality of an
action sequence, i.e., optimality of the constituent actions to obtain the desired output. This constraint
would, for instance, eliminate redundant sequences such as turnLeft, turnRight, which does not
affect the output, or turnLeft, turnLeft, turnLeft, whose output could be achieved by a single
turnRight. All employed elimination sequences can be found in the supplementary material. The
entire list of constraints applied on the solution code in Fig. 4d is shown in Fig. 4f.

Step 3: SMT query. For a sketch Q generated from code C and its constraints, we pose the
following query to an SMT solver: (sketch Q, Q-constraints). As a result, the solver generates a set of
instantiations, which are conceptually similar to C. In our implementation, we used the Z3 solver [7].
For the code in Fig. 4d, Z3 generated 66 mutated codes in 0.8s from an exhaustive space of 2, 997
possible codes with δsize = 2. One such mutation is shown in Fig. 1d.

While this approach generates codes that are devoid of most semantic irregularities, it has its
limitations. Certain irregularities continue to exist in some generated codes: An example of such
a code included the action sequence move, turnLeft, move, turnLeft, move, turnLeft, move,
turnLeft, which results in the agent circling back to its initial location in the task space. This kind
of undesirable behaviour is eliminated in the symbolic execution stage of our pipeline.

3.2 Symbolic Execution

Symbolic execution [13] is an automated test-generation technique that symbolically explores execu-
tion paths in a program. During exploration of a path, it gathers symbolic constraints over program
inputs from statements along the path. These constraints are then mutated (according to a search
strategy), and an SMT solver is queried to generate new inputs that explore another path.

Obtaining visual tasks with symbolic execution. This stage in our pipeline applies symbolic exe-
cution on each generated code Cout to obtain a suitable visual task Tout. The program inputs of Cout are
the agent’s initial location/orientation and the status of the grid cells (unknown, free, blocked, marker,
goal), which is initially unknown. Symbolic execution collects constraints over these from code
statements. As in Fig. 5 for one path, symbolic execution generates a visual task for each path in Cout.

However, not all of these tasks are suitable. For instance, if the goal is reached after the ﬁrst move in
Fig. 1d, all other statements in Cout are not covered, rendering the task less suitable for this code.
Naïvely, symbolic execution could ﬁrst enumerate all paths in Cout and their corresponding tasks, and
then rank them in terms of suitability. However, solution codes may have an unbounded number of
paths, which leads to path explosion, that is, the inability to cover all paths with tractable resources.

Guiding symbolic execution using Monte Carlo Tree Search (MCTS). To address this issue,
we use MCTS [14] as a search strategy in symbolic execution with the goal of generating more
suitable tasks with fewer resources—we deﬁne task suitability next. Symbolic execution has been
previously combined with MCTS in order to direct the exploration towards costly paths [15]. In the
supplementary material, we provide an example demonstrating how MCTS could guide the symbolic
execution in generating more suitable tasks.

As previously observed [12], a critical component of effectively applying MCTS is to deﬁne an
evaluation function that describes the desired properties of the output, i.e., the visual tasks. Tailoring
the evaluation function to our unique setting is exactly what differentiates our approach from existing
work. In particular, our evaluation function, Fscore, distinguishes suitable tasks by assigning a score
(∈ [0, 1]) to them, which guides the MCTS search. A higher Fscore indicates a more suitable task.

5

(b)(c)(d)(e)(f)(g)(a)Task T

Tstore

Tsize (= Csize) Cdepth

Type: Source

move, turnL, turnR
move, turnL, turnR, Repeat
move, turnL, turnR, Repeat
move, turnL, turnR, RepeatUntil
move, turnL, turnR, RepeatUntil, If
move, turnL, turnR, RepeatUntil, IfElse
move, turnL, turnR, pickM, putM
move, turnL, turnR, pickM, putM, Repeat
move, turnL, turnR, pickM, putM, Repeat, IfElse

H1
H2
H3
H4
H5
H6
K7
K8
K9
K10 move, turnL, turnR, pickM, putM, While

5
3
5
5
4
4
5
4
5
7

1 HOC: Maze 4 [23]
2 HOC: Maze 7 [23]
2 HOC: Maze 8 [23]
2 HOC: Maze 12 [23]
3 HOC: Maze 16 [23]
3 HOC: Maze 18 [23]
1 Karel: Our ﬁrst [22]
2 Karel: Square [22]
3 Karel: One ball in each spot [22]
2 Karel: Diagonal [22]

Figure 6: Datasets for HOC and Karel tasks.

vis , Cout) ∈ {0, 1}, which evaluates to 1 in the event of
Its constituent components are: (i) Fcov(Tout
vis and 0 otherwise; (ii) Fdiss(Tout
complete coverage of code Cout by task Tout
vis) ∈ [0, 1], which
vis , Cout) ∈ [0, 1], which evaluates
evaluates the dissimilarity of Tout to Tin (see Section 2); (iii) Fqual(Tout
vis , Cout) ∈ {0, 1}, which evaluates to 0 in case the
the quality and validity of Tout; (iv) Fnocrash(Tout
vis , Cout) ∈ {0, 1}, which evaluates to 0 if
agent crashes into a wall and 1 otherwise; and (v) Fnocut(Tout
there is a shortcut sequence of actions (a in Fig. 4a) smaller than Cout
size that solves Tout and 1 otherwise.
Fqual and Fnocut also resolve the limitations of our mutation stage by eliminating codes and tasks
that lead to undesirable agent behavior. We instantiate Fscore in the next section.

vis , Tin

4 Experimental Evaluation

In this section, we evaluate our task synthesis algorithm on HOC and Karel tasks. Our implementation
is publicly available.2 While we give an overview of key results here, a detailed description of our
setup and additional experiments can be found in the supplementary material.

4.1 Reference Tasks and Speciﬁcations

Reference tasks. We use a set of ten reference tasks from HOC and Karel, shown in Fig. 6. The
HOC tasks were selected from the Hour of Code: Classic Maze challenge by Code.org [23] and the
Karel tasks from the Intro to Programming with Karel course by CodeHS.com [22]. The DSL of
Fig. 4a is generic in that it includes both HOC and Karel codes, with the following differences: (i) con-
struct While, marker-related actions putM, pickM, and conditions noPathA, noPathL, noPathR,
marker, noMarker are speciﬁc to Karel only; (ii) construct RepeatUntil and goal are speciﬁc to
HOC only. Furthermore, the puzzles for HOC and Karel are of different styles (see Fig. 1 and Fig. 2).
For all tasks, the grid size of the puzzles is ﬁxed to 10 × 10 cells (grid-size parameter n = 10).
Speciﬁcation of scoring functions. Fqual(Tout
vis , Cout) ∈ [0, 1] was approximated as the sum of the
normalized counts of ‘moves’, ‘turns’, ‘segments’, and ‘long-segments’ in the grid; segments and long-
segments are sequences of ≥ 3 and ≥ 5 move actions respectively. More precisely, for HOC tasks,
we used the following function where features are computed by executing Cout on Tout
vis :

F HOC

qual (Tout

vis , Cout) =

1
4

(cid:16) #moves
2n

+

#turns
n

+

#segments
n/2

+

#long-segments
n/3

(cid:17)

.

Furthermore, in our implementation, Fqual(·) value was set to 0 when Fnocrash(·) = 0. For Karel tasks,
Fqual additionally included the normalized counts of putM and pickM, and is provided in the supple-
mentary material. Fdiss(Tout
vis) ∈ [0, 1] was computed based on the dissimilarity of the agent’s
initial location/orientation w.r.t. Tin
vis, and the grid-cell level dissimilarity based on the Hamming
distance between Tout

vis. More precisely, we used the following function:

vis , Tin

Fdiss(Tout

vis , Tin

vis) =

diss(loc | Tout

vis , Tin

vis) + diss(dir | Tout

vis , Tin

vis) + diss(grid-cells | Tout

vis , Tin

vis)

(cid:17)

vis and Tin
(cid:16)

1
3

where diss(loc | Tout
[0, 1] (after the Hamming distance is normalized with a factor of 2

vis) ∈ {0, 1}, diss(dir | Tout

vis , Tin

vis , Tin

vis) ∈ {0, 1}, and diss(grid-cells | Tout

vis , Tin

vis) ∈

n2 ).

2https://github.com/adishs/neurips2020_synthesizing-tasks_code

6

Task
Tin

2:#Cout

∆=0 3:#Cout

∆=0,1 4:#Cout

Code Mutation

Symbolic Execution

Fraction of Tout with criteria

3, 159
H1
8, 991
H2
798, 255
H3
5, 913
H4
2, 997
H5
1, 728
H6
96, 875
K7
484, 875
K8
8.595 × 106
K9
K10 132.625 × 106

112
594
13, 122
152
294
294
150
4, 506
60, 768
19, 328

∆=all 5:Time 6:#Cout 7:#Tout 8:Time 9:(V) 10:(VI)δmini=1 11:(VI)δmini=0
1.00
68s
1.00
61s
0.90
60s
0.50
167s
0.27
348s
0.07
347s
1.00
61s
1.00
63s
0.88
185s
1.00
158s

272
428
1, 126
404
444
480
1, 196
4, 506
4, 258
5, 032

0.6s
1.7s
13.3s
1.0s
0.8s
0.6s
1.3s
11.6s
11.3s
17.1s

64
138
720
108
66
54
122
990
888
1, 404

1.00
1.00
0.98
1.00
0.59
0.45
1.00
1.00
0.92
1.00

1.00
1.00
0.90
1.00
0.98
0.80
1.00
1.00
0.92
1.00

28
48
196
44
46
48
122
469
432
532

Figure 7: Results on HOC and Karel tasks; details are provided in Section 4.

Next, we deﬁne the evaluation function Fscore(Tout, Cout, Tin, Cin) ∈ [0, 1] used by MCTS:

Fscore(Tout, Cout, Tin, Cin) = 1(cid:0)Fqual(Tout

vis , Cout) = 1, Fnocut(Tout

vis , Cout) ≥ δqual, Fnocrash(Tout
(cid:123)(cid:122)
(i)
vis , Cout) + α3Fdiss(Tout
vis , Cout) + α2Fqual(Tout

·

vis , Cout) = 1(cid:1)
(cid:125)
vis)(cid:3)
(cid:125)

vis , Tin

(cid:124)
(cid:2)α1Fcov(Tout
(cid:124)

(cid:123)(cid:122)
(ii)

where 1 is an indicator function and each constant α = 1/3. Component (ii) in the above function
supplies the gradients for guiding the search in MCTS; Component (i) is applied at the end of the
MCTS run to pick the output. More precisely, the best task (i.e, the one with the highest Fscore value)
is picked only from the pool of generated tasks which have Fscore(·) > 0 and satisfy Fcov(·) = 1.
Speciﬁcation of task synthesis and MCTS. As per Section 2, we set the following thresholds for our
algorithm: (i) δsize = 2, (ii) δdiss = 0.33, and (iii) δqual = 0.2 for codes with While or RepeatUntil,
and 0.05 otherwise. We run MCTS 10 times per code, with each run generating one task. We set
the maximum iterations of a run to 2 million (M) and the exploration constant to 2 [14]. Even when
considering a tree depth of 2n (= 20), there are millions of leaves for difﬁcult tasks H5 and H6,
reﬂecting the complexity of task generation. For each code Cout, we generated 10 different visual
tasks. To ensure sufﬁcient diversity among the tasks generated for the same code, we introduced a
measure Fdiversity. This measure, not only ensures visual task dissimilarity, but also ensures sufﬁcient
diversity in entire symbolic paths during generation (for details, see supplementary material).

4.2 Results

Performance of task synthesis algorithm. Fig. 7 shows the results of our algorithm. The second
column illustrates the enormity of the unconstrained space of mutated codes; we only impose size
constraint ∆0 from Fig. 4c. We then additionally impose constraint ∆1 resulting in a partially con-
strained space of mutated codes (column 3), and ﬁnally apply all constraints from Fig. 4c to obtain the
ﬁnal set of generated codes (column 4). This reﬂects the systematic reduction in the space of mutated
codes by our constraints. Column 5 shows the total running time for generating the ﬁnal codes, which
denotes the time taken by Z3 to compute solutions to our mutation query. As discussed in Section 3.1,
few codes with semantic irregularities still remain after the mutation stage. The symbolic execution
stage eliminates these to obtain the reduced set of valid codes (column 6). Column 7 shows the ﬁnal
number of generated tasks and column 8 is the average time per output task (i.e., one MCTS run).

Analyzing output tasks. We further analyze the generated tasks based on the objectives of Section 2.
All tasks satisfy properties (I)–(III) by design. Objective (IV) is easily achieved by excluding
generated tasks for which Cout = Cin. For a random sample of 100 of the generated tasks per reference
task, we performed manual validation to determine whether objectives (V) and (VI) are met. The
fraction of tasks that satisfy these objectives is listed in the last three columns of Fig. 7. We observe
that the vast majority of tasks meet the objectives, even if not by design. For H6, the fraction of tasks
satisfying (VI) is low because the corresponding codes are generic enough to solve several puzzles.

Deep dive into an MCTS run. To offer more insight into the task generation process, we take a
closer look at an MCTS run for task H5, shown in Fig. 8. Fig. 8a illustrates the improvement in
various components of Fscore as the number of MCTS iterations increases. Best tasks at different
iterations are shown in Fig. 8b, 8c, 8d. As expected, the more the iterations, the better the tasks are.

7

(a) Trends in Fscore features

(b) Best at 200
Figure 8: Illustration of a single MCTS run on Cout from Fig. 1d obtained from solution code of task
H5 by mutation. (a) shows the temporal trends of different feature values in Fscore averaged over a
time window of 100 steps. (b)–(d) show the best, i.e., highest scoring, tasks generated up to times
2 × 102, 2 × 104, and 2 × 106 respectively. Tout

vis shown in Fig. 1c is the puzzle produced in (d).

(c) Best at 20K

(d) Best at 2M

Remarks. We also ran the mutation stage by enumerating the programs within size constraints and
then post-checking other constraints without Z3. This implementation leads to a run-time increase
by a factor of 10 to 100 for different tasks. So, Z3 seems to be very effective by jointly considering
all the constraints. As a search method, although MCTS seems computationally expensive, the actual
run-time and memory footprint of an MCTS run depend on the unique traces explored (i.e., unique
symbolic executions done)—this number is typically much lower than the number of iterations,
also see discussion in the supplementary material. Considering the MCTS output in Figs. 8c, 8d, to
obtain a comparable evaluation score through a random search, the corresponding number of unique
symbolic executions required is at least 10 times more than executed by MCTS. We note that while
we considered one I/O pair for Karel tasks, our methodology can be easily extended to multiple
I/O pairs by adapting techniques designed for generating diverse tasks.

5 User Study and Comparison with Alternate Methods

In this section, we evaluate our task synthesis algorithm with a user study focusing on tasks H2,
H4, H5, and H6. We developed an online app3, which uses the publicly available toolkit of Blockly
Games [10] and provides an interface for a participant to practice block-based programming tasks
for HOC. Each “practice session” of the study involves three steps: (i) a reference task Tin ∈
{H2, H4, H5, H6} is shown to the participant along with its solution code Cin, (ii) a new task Tout
is generated for which the participant has to provide a solution code, and (iii) a post-survey asks
the participant to assess the visual dissimilarity of the two tasks on a 4-point Likert scale as used
in [25]. Details on the app interface and questionnaire are provided in the supplementary material.
Participants for the study were recruited through Amazon Mechanical Turk. We only selected four
tasks due to the high cost involved in conducting the study (about 1.8 USD per participant). The
number of participants and their performance are documented in Fig. 9.

Baselines and methods evaluated. We evaluated four different methods, including three baselines
(SAME, TUTOR, MUTTASK) and our algorithm (SYNTASK). SAME generates tasks such that
Tin = Tout. TUTOR produces tasks that are similar to Tin and designed by an expert. We picked similar
problems from the set of 20 Classic Maze challenge [23] tasks exercising the same programming
concepts: Maze 6, 9 for H2, Maze 11, 13 for H4, Maze 15, 17 for H5, and Maze 19 for H6.

MUTTASK generated tasks by directly mutating the grid-world of the original task, i.e., by moving
the agent or goal by up to two cells and potentially changing the agent’s orientation. A total of 18, 20,
15, and 17 tasks were generated for H2, H4, H5, and H6, respectively. Fig. 10 shows two output tasks
for H4 and illustrates the challenge in directly mutating the input task, given the high discontinuity
in mapping from the space of tasks to their codes. For H4, a total of 14 out of 20 new tasks were
structurally very different from the input.

SYNTASK uses our algorithm to generate tasks. We picked the generated tasks from three groups
based on the size of the code mutations from which they were produced, differing from the reference
solution code by +δsize for δsize ∈ {0, 1, 2}. For H2 and H4, we randomly selected 5 tasks from each
group, for a total of 15 new tasks per reference task. For H5 and H6, we selected 10 tasks from the
ﬁrst group (δsize = 0) only, due to their complexity stemming from nested constructs in their codes.
We observed that TUTOR tasks for H5, H6 were also of δsize = 0, i.e., Cout
size. All the generated
tasks picked for SYNTASK adhere to properties (I)–(VI) in Section 2.

size = Cin

3https://www.teaching-blocks.cc/

8

0,00,20,40,60,81,02002K5K10K15K20K40K60K80K100K120K140K160K180K200K400K600K800K1M1.2M1.4M1.6M1.8M2MNormalized features  coverageno crashlong segmentssegmentsmovesturnsMethod

Total participants

Fraction of tasks solved

Time spent in secs

Visual dissimilarity

H- H2 H4 H5 H6 H- H2 H4 H5 H6 H- H2 H4 H5 H6 H- H2 H4 H5 H6

96 24 24 24 24 .94 .92 1.00 .96 .88

SAME
TUTOR 170 48 48 49 25 .90 .90
MUTTASK 278 72 79 60 67 .68 .76
SYNTASK 197 59 57 40 41 .89 .92

89

60

59

93 145 1.07 1.12 1.04 1.00 1.12
.92 .88 .92 121 107 113 118 169 2.90 2.81 2.79 2.96 3.16
.71 .65 .60 219 135 299 219 215 2.17 2.36 2.33 1.95 1.99
85 183 130 189 2.63 2.41 2.42 2.68 3.20
.89 .92 .83 144

Figure 9: User study results for HOC tasks (H-represents all tasks in the study); see Section 5.
def Run(){
move
move
turnLeft
move
turnRight
move
turnLeft
15 more actions

def Run(){
move
move
RepeatUntil(goal){

def Run(){
move
turnLeft
move
turnRight

turnLeft
move
turnRight
move

RepeatUntil(goal){

}

}

}

}

}

vis for H4

(b) 1st Tout
vis

(a) Tin
(c) 2nd Tout
vis
Figure 10: MUTTASK applied to H4. Tin
Tout
vis obtained via small mutations of Tin
similar to Cin. (f) is the smallest solution code for (c) and is drastically different from Cin.

vis and Cin are shown in (a) and (d). (b)–(c) illustrate two tasks
vis. (e) is the smallest solution code for (b) and is structurally

(d) Cin for H4

(f) 2nd Cout

(e) 1st Cout

Results on task solving. In terms of successfully solving the generated tasks, SAME performed
best (mean success = 0.94) in comparison to TUTOR (mean = 0.90), SYNTASK (mean = 0.89), and
MUTTASK (mean = 0.68)—this is expected given the tasks generated by SAME. In comparison to
TUTOR, the performance of SYNTASK was not signiﬁcantly different (χ2 = 0.04, p = 0.83); in
comparison to MUTTASK, SYNTASK performed signiﬁcantly better (χ2 = 28.74, p < e−8). The
complexity of the generated tasks is also reﬂected in the average time that participants spent on
solving them. As shown in Fig. 9, they spent more time solving the tasks generated by MUTTASK.

Results on visual task dissimilarity. Visual dissimilarity was measured on a Likert scale ranging
from 1–4, 1 being highly similar and 4 highly dissimilar. Comparing the dissimilarity of the generated
tasks w.r.t. the reference task, we found that the performance of SAME was worst (mean dissimi-
larity = 1.07), while that of TUTOR was best (mean = 2.90). SYNTASK (mean = 2.63) performed
signiﬁcantly better than MUTTASK (mean = 2.17), yet slightly worse than TUTOR. This is because
TUTOR generates tasks with additional distracting paths and noise, which can also be done by our al-
gorithm (although not done for this study). Moreover, for H2, which had no conditionals, the resulting
codes were somewhat similar, and so were the generated puzzles. When excluding H2 from the anal-
ysis, the difference between SYNTASK (mean = 2.72) and TUTOR (mean =2.93) was not statistically
signiﬁcant. A detailed distribution of the responses can be found in the supplementary material.

Remarks. SAME’s performance in terms of tasks solved is below 1.00, possibly because participants
overlooked the solution of Step 1, unaware they will be receiving the same task in Step 2, and the app
did not allow them to go back to Step 1. This user study provides a proof-of-concept; more elaborate
studies are needed to fully reach the motivational goal of teaching K-12 students, and evaluate the
long term impact on students’ concept learning. As additional studies, it would be important to
understand the sensitivity of user study results w.r.t. the Likert scale deﬁnition; another possibility
is to use pairwise comparisons in eliciting user evaluations.

6 Conclusions and Outlook
We developed techniques for a critical aspect of pedagogy in block-based programming: Automat-
ically generating new tasks that exercise speciﬁc programming concepts, while looking visually
dissimilar to input. We demonstrated the effectiveness of our methodology through an extensive
empirical evaluation and user study on reference tasks from popular programming platforms.
We believe our techniques have the potential to drastically improve the success of pedagogy in
block-based visual programming environments by providing tutors and students with a substantial
pool of new tasks. Beyond the application domain of programming education, our methodology
can be used for generating large-scale datasets consisting of tasks and solution codes with desirable
characteristics—this can be potentially useful for training neural program synthesis methods.

There are several promising directions for future work, including but not limited to: Learning a
policy to guide the MCTS procedure (instead of running vanilla MCTS); automatically learning
the constraints and cost function from a human-generated pool of problems; and applying our
methodology to other programming environments (e.g., Python problems).

9

Broader Impact

This paper develops new techniques for improving pedagogy in block-based visual programming
environments. Such programming environments are increasingly used nowadays to introduce com-
puting concepts to novice programmers, and our work is motivated by the clear societal need of
enhancing K-12 computing education. In existing systems, the programming tasks are hand-curated
by tutors, and the available set of tasks is typically very limited. This severely limits the utility of
existing systems for long-term learning as students do not have access to practice tasks for mastering
the programming concepts.

We take a step towards tackling this challenge by developing a methodology to generate new practice
tasks for a student that match a desired level of difﬁculty and exercise speciﬁc programming concepts.
Our task synthesis algorithm is able to generate 1000’s of new similar tasks for reference tasks
taken from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming
with Karel course by CodeHS.com. Our extensive experiments and user study further validate the
quality of the generated tasks. Our task synthesis algorithm could be useful in many different ways
in practical systems. For instance, tutors can assign new practice tasks as homework or quizzes
to students to check their knowledge, students can automatically obtain new similar tasks after
they failed to solve a given task and received assistance, and intelligent tutoring systems could
automatically generate a personalized curriculum of problems for a student for long-term learning.

Acknowledgments and Disclosure of Funding

We would like to thank the anonymous reviewers for their helpful comments. Ahana Ghosh was
supported by Microsoft Research through its PhD Scholarship Programme. Umair Z. Ahmed and
Abhik Roychoudhury were supported by the National Research Foundation, Singapore and National
University of Singapore through its National Satellite of Excellence in Trustworthy Software Systems
(NSOE-TSS) project under the National Cybersecurity R&D (NCR) Grant award no. NRF2018NCR-
NSOE003-0001.

References

[1] Umair Z. Ahmed, Sumit Gulwani, and Amey Karkare. Automatically generating problems and

solutions for natural deduction. In IJCAI, pages 1968–1975, 2013.

[2] Chris Alvin, Sumit Gulwani, Rupak Majumdar, and Supratik Mukhopadhyay. Synthesis of

geometry proof problems. In AAAI, pages 245–252, 2014.

[3] Erik Andersen, Sumit Gulwani, and Zoran Popovic. A trace-based framework for analyzing

and synthesizing educational progressions. In CHI, pages 773–782, 2013.

[4] Clark W. Barrett and Cesare Tinelli. Satisﬁability modulo theories. In Handbook of Model

Checking, pages 305–343. Springer, 2018.

[5] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli.
Leveraging grammar and reinforcement learning for neural program synthesis. In ICLR, 2018.

[6] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In

ICLR, 2018.

[7] Leonardo de Moura and Nikolaj Bjørner. Z3: An efﬁcient SMT solver. In TACAS, pages

337–340, 2008.

[8] Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J. Hausknecht, and Pushmeet Kohli.
Neural program meta-induction. In Advances in Neural Information Processing Systems, pages
2080–2088, 2017.

[9] Aleksandr Efremov, Ahana Ghosh, and Adish Singla. Zero-shot learning of hint policy via

reinforcement learning and program synthesis. In EDM, 2020.

[10] Blockly Games. Games for tomorrow’s programmers. https://blockly.games/.

[11] Sumit Gulwani. Example-based learning in computer-aided STEM education. Communications

of the ACM, 57(8):70–80, 2014.

10

[12] Bilal Kartal, Nick Sohre, and Stephen J. Guy. Data driven Sokoban puzzle generation with

Monte Carlo tree search. In AIIDE, 2016.

[13] James C. King. Symbolic execution and program testing. Communications of the ACM,

19:385–394, 1976.

[14] Levente Kocsis and Csaba Szepesvári. Bandit based Monte-Carlo planning. In ECML, pages

282–293, 2006.

[15] Kasper Luckow, Corina S P˘as˘areanu, and Willem Visser. Monte Carlo tree search for ﬁnding

costly paths in programs. In SEFM, pages 123–138, 2018.

[16] John H. Maloney, Kylie Peppler, Yasmin Kafai, Mitchel Resnick, and Natalie Rusk. Program-
ming by choice: Urban youth learning programming with Scratch. In SIGCSE, pages 367–371,
2008.

[17] Samiha Marwan, Nicholas Lytle, Joseph Jay Williams, and Thomas W. Price. The impact of
adding textual explanations to next-step hints in a novice programming environment. In ITiCSE,
pages 520–526, 2019.

[18] Benjamin Paaßen, Barbara Hammer, Thomas W. Price, Tiffany Barnes, Sebastian Gross, and
Niels Pinkwart. The continuous hint factory – Providing hints in vast and sparsely populated
edit distance spaces. Journal of Educational Data Mining, 2018.

[19] Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, and
Leonidas J. Guibas. Learning program embeddings to propagate feedback on student code. In
ICML, pages 1093–1102, 2015.

[20] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas J. Guibas. Autonomously generat-

ing hints by inferring problem solving policies. In L@S, pages 195–204, 2015.

[21] CodeHS platform. CodeHS.com: Teaching Coding and Computer Science. https://

codehs.com/.

[22] CodeHS platform. Intro to Programming with Karel the Dog. https://codehs.com/

info/curriculum/introkarel.

[23] Code.org platform. Hour of Code: Classic Maze Challenge. https://studio.code.

org/s/hourofcode.

[24] Code.org platform. Hour of Code Initiative. https://hourofcode.com/.
[25] Oleksandr Polozov, Eleanor O’Rourke, Adam M. Smith, Luke Zettlemoyer, Sumit Gulwani,
and Zoran Popovic. Personalized mathematical word problem generation. In IJCAI, 2015.

[26] Thomas W. Price and Tiffany Barnes. Position paper: Block-based programming should offer

intelligent support for learners. In B&B, pages 65–68, 2017.

[27] Thomas W. Price, Yihuan Dong, and Dragan Lipovac. iSnap: Towards intelligent tutoring in

novice programming environments. In SIGCSE, pages 483–488, 2017.

[28] Thomas W. Price, Rui Zhi, and Tiffany Barnes. Evaluation of a data-driven feedback algorithm

for open-ended programming. EDM, 2017.

[29] Mitchel Resnick, John Maloney, Andrés Monroy-Hernández, Natalie Rusk, Evelyn Eastmond,
Karen Brennan, Amon Millner, Eric Rosenbaum, Jay Silver, Brian Silverman, et al. Scratch:
Programming for all. Communications of the ACM, 52(11):60–67, 2009.

[30] Reudismam Rolim, Gustavo Soares, Loris D’Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit
Gheyi, Ryo Suzuki, and Björn Hartmann. Learning syntactic program transformations from
examples. In ICSE, pages 404–415, 2017.

[31] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. Automated feedback generation

for introductory programming assignments. In PLDI, pages 15–26, 2013.

[32] Rohit Singh, Sumit Gulwani, and Sriram K. Rajamani. Automatically generating algebra

problems. In AAAI, 2012.

[33] Lisa Wang, Angela Sy, Larry Liu, and Chris Piech. Learning to represent student knowledge on

programming exercises using deep learning. EDM, 2017.

[34] David Weintrop and Uri Wilensky. Comparing block-based and text-based programming in

high school computer science classrooms. TOCE, 18(1):1–25, 2017.

11

[35] Mike Wu, Milan Mosse, Noah Goodman, and Chris Piech. Zero shot learning for code education:

Rubric sampling with deep learning inference. In AAAI, 2019.

[36] Jooyong Yi, Umair Z. Ahmed, Amey Karkare, Shin Hwei Tan, and Abhik Roychoudhury. A
feasibility study of using automated program repair for introductory programming assignments.
In ESEC/FSE, 2017.

[37] Rui Zhi, Thomas W. Price, Samiha Marwan, Alexandra Milliken, Tiffany Barnes, and Min Chi.
Exploring the impact of worked examples in a novice programming environment. In SIGCSE,
pages 98–104, 2019.

12

A List of Appendices

In this section, we provide a brief description of the content provided in the appendices of the paper.

• Appendix B shows all the 10 reference tasks from Fig. 6. For each task, we also illustrate our

methodology as was done in Fig. 1 and Fig. 2.

• Appendix C expands on the user study analysis. (Section 5)

• Appendix D provides additional details on the code mutation stage. (Section 3.1)

• Appendix E demonstrates how MCTS could guide the symbolic execution in generating more

suitable tasks. (Section 3.2)

• Appendix F provides additional details and results about experiments. (Section 4)

B Illustration of Our Methodology for the HOC and Karel Dataset

In this section, we illustrate our methodology for all the 10 reference tasks from Fig. 6. For each
reference task (Tin, Cin), we picked one output (Tout, Cout) from the entire set of generated outputs.

def Run(){
move
turnLeft
move
turnRight
move

}

def Run(){
move
move
move
turnRight
move
turnLeft
move

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 11: Task H1 – Illustration of our methodology.

def Run(){
turnRight
Repeat(5){
move

}

}

def Run(){
move
move
turnRight
Repeat(6){
move

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 12: Task H2 – Illustration of our methodology.

def Run(){

Repeat(4){
move

}
turnLeft
Repeat(5){
move

}

}

def Run(){
Repeat(5){
move
}
turnLeft
Repeat(5){
move
}
turnLeft
move

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 13: Task H3 – Illustration of our methodology.

def Run(){

RepeatUntil(goal){

move
turnLeft
move
turnRight

}

}

def Run(){

RepeatUntil(goal){

move
move
turnRight
move
turnLeft

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 14: Task H4 – Illustration of our methodology.

13

def Run(){

RepeatUntil(goal){
move
If(pathLeft){
turnLeft

}

}

}

def Run(){
move
turnLeft
RepeatUntil(goal){
move
If(pathRight){
turnRight

}

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 15: Task H5 – Illustration of our methodology (same as Fig. 1 and shown for completeness).

def Run(){

RepeatUntil(goal){
If(pathAhead){

move

}
Else{

turnLeft

}

}

}

def Run(){

RepeatUntil(goal){
If(pathAhead){

move

}
Else{

turnRight

}

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout

(d) Solution code Cout

Figure 16: Task H6 – Illustration of our methodology.

def Run(){
move
move
pickMarker
move
move

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout
Figure 17: Task K7 – Illustration of our methodology.

def Run(){
putMarker
turnLeft
move
move
pickMarker
move
move

}

(d) Solution code Cout

def Run(){

Repeat(4){
putMarker
move
turnLeft

}

}

def Run(){

Repeat(4){
move
pickMarker
move
turnLeft

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout
Figure 18: Task K8 – Illustration of our methodology.

(d) Solution code Cout

def Run(){

Repeat(8){

If(noMarker){
putMarker

}
Else{

}
move

pickMarker

}

}

def Run(){
turnRight
Repeat(7){

If(noMarker){
putMarker

}
Else{

}
move

pickMarker

}

}

(a) Visual puzzle for Tin

(b) Solution code Cin

(c) Visual puzzle for Tout
Figure 19: Task K9 – Illustration of our methodology.

(d) Solution code Cout

def Run(){
putMarker
While(pathAhead){

move
turnLeft
move
turnRight
putMarker

}

}

def Run(){
putMarker
While(pathAhead){

move
move
turnRight
move
turnLeft
putMarker

}

}

(a) Visual puzzle for Tin

(d) Solution code Cout
Figure 20: Task K10 – Illustration of our methodology (same as Fig. 2 and shown for completeness).

(c) Visual puzzle for Tout

(b) Solution code Cin

14

C User Study: Additional Details and Results

In this section, we discuss additional details of our user study and expand on the key results provided
in Section 5. For our user study, we used 4 reference tasks (H2, H4, H5, and H6). We developed an
web app where participants, recruited through Amazon Mechanical Turk, were asked to solve tasks
generated by four algorithms, SAME, TUTOR, MUTTASK, or SYNTASK (described in Section 5).
Next, we describe the interface of our app and details of the questionnaire. The web app is publicly
available (see Footnote 3).

(a) Login and welcome page

(b) Step 1: Coding task with solution

(c) Step 2: Solve new coding task

(d) Step 3: Survey

Figure 21: Interface of the app.

C.1 App Interface and Questionnaire

Our online app was developed using the publicly available toolkit of Blockly Games [10] and provides
an interface for a participant to practice Block-based programming tasks for HOC. Participants were
familiarized with the tasks by a small tutorial given before they logged-in to the app. Each participant
was randomly assigned a reference task, an algorithm (out of the four chosen for evaluation), and
a particular task generated based on the chosen algorithm. These elements constituted a “practice
session" for a participant. Each session consisted of three steps. In Step 1, the reference task
along with its solution code was shown to the participant (Fig. 21b). In Step 2, the participant was
asked to solve a new task (Fig. 21c). The new task was generated by one of the four algorithms:
SAME, TUTOR, MUTTASK, or SYNTASK. To solve this new task, the participant was given 10 tries.
If they successfully solved the task or ran out of tries, they were directed to Step 3 of the practice
session, the survey (Fig. 21d). Here, they were presented with a question on the visual dissimilarity of
the tasks from Step 1 and Step 2, which was to be answered on a 4-point Likert scale as used in [25]:

• Score 1: indicates that the tasks are visually exactly same

• Score 2: indicates that the tasks are visually similar

• Score 3: indicates that the tasks are visually slightly different

• Score 4: indicates that the tasks are visually very different

15

(a) H-

(b) H2

(c) H4

(d) H5

(e) H6

Figure 22: Distribution of participant responses on visual dissimilarity of tasks, rated on a 4-point
Likert scale (1: exactly same, 2: almost same, 3: slightly different, 4: very different)

C.2 Results on Visual Task Dissimilarity

The distribution of participant responses (on a 4-point scale) on the visual dissimilarity of the
presented tasks is shown in Fig. 22; the aggregate statistics are provided in Fig. 9.

Comparing the dissimilarity of the generated tasks w.r.t. all the reference tasks H-, we found that the
performance of SAME was worst (mean dissimilarity = 1.07) in comparison to MUTTASK (mean =
2.17), SYNTASK (mean = 2.63), and TUTOR (mean = 2.90). The poor performance of SAME, with
the majority of scores being 1, is expected. Furthermore, we also see in Fig. 22a that MUTTASK has a
greater percentage of lower scores of 1 and 2, and this is because the baseline directly mutates the task
puzzle. Comparing the performance of SYNTASK to the baselines, we ﬁnd that SYNTASK performed
signiﬁcantly better than MUTTASK (χ2 = 38.81, p < e−7). However, its performance was worse
than that of TUTOR and this difference was signiﬁcant (χ2 = 12.20, p = 0.0053).

When analyzing the differences in the performance of SYNTASK w.r.t. TUTOR, we ﬁnd that
TUTOR performs better primarily because of the following two reasons: (i) some of the tasks
generated by TUTOR have additional distracting paths / noise, and (ii) for simpler tasks without
conditionals like H2, more powerful code mutations were used. Next, we performed additional
analysis by limiting to two complex tasks with nested conditionals, H5 and H6. On these two
tasks, the mean scores of the methods MUTTASK, SYNTASK, and TUTOR were 1.97, 2.94, and
3.03 respectively. Furthermore, we ﬁnd that SYNTASK’s performance continued to be signiﬁ-
cantly better than MUTTASK (χ2 = 64.33, p < e−13). But, the difference in the performance of
SYNTASK and TUTOR was not statistically signiﬁcant (χ2 = 2.68, p = 0.44).

In general, the performance of our task synthesis algorithm can be further improved by allowing
for more powerful code mutations in tasks without conditionals (such as H2) and by adding more
variability in the output tasks by incorporating techniques discussed in Appendix F.4.

D Code Mutation: Additional Details

This section describes the mutation stage of the task synthesis algorithm. In particular, we describe
in detail the constraints applied on sketch Q = Ω(C). Note that we denote an empty action as φ.
Our implementation of the code mutation stage, using the Z3 solver [7], is publicly available (see
Footnote 2).

Constraint (∆1): ACTIONEDITS. ACTIONEDITS returns the values that all action sequences can
take, based on their values in the reference code, Cin. Consider the action sequence A in sketch
Q. The function ω(A| Cin) returns the value of A in Cin. The types of constraints returned by
ACTIONEDITS are:

1. Local A constraints. These constraints describe the values that one A can take w.r.t. ω(A| Cin). It

has the following rules:

• move action ∈ ω(A| Cin), would imply that the corresponding action in A must be move.
• Set of turnLeft, turnRight actions ∈ ω(A| Cin), would imply that the corresponding actions
in A will either have all the ‘turn’ actions changed to turnLeft, or to turnRight, or remain
the same, or be ﬂipped, i.e. turnLeft → turnRight and turnRight → turnLeft.

16

0%20%40%60%80%100%SAMETUTORMUTTSYNT12340%20%40%60%80%100%SAMETUTORMUTTSYNT12340%20%40%60%80%100%SAMETUTORMUTTSYNT12340%20%40%60%80%100%SAMETUTORMUTTSYNT12340%20%40%60%80%100%SAMETUTORMUTTSYNT1234• Set of pickMarker, putMarker actions ∈ ω(A| Cin), would imply that the corresponding
actions in A will either have all the ‘marker’ actions changed to pickMarker, or to putMarker,
or remain the same, or be ﬂipped i.e. pickMarker → putMarker and putMarker →
pickMarker.

• Additional actions (up to δsize) to A can either be appended before the existing actions (in

ω(A| Cin)) or after, but not both. In our experiments we set δsize = 2.
These constraints are listed under “Local A constraints” in Fig. 23h and Fig. 24h.

2. Global A constraints. These constraints apply on all the A’s in sketch Q. They allow only one of
all the A’s to have additional actions added to them. These constraints are listed under “Global
A constraints” in Fig. 23h and Fig. 24h.

Constraint (∆6): Action sequence is minimal. These constraints describe the sequences that
invalidate minimality of code. The constraints ensure that certain sequences of actions do not occur
in A. The detailed list of sequences for the two example codes described, which invalidate A if they
occur, are given in Fig. 23g and Fig. 24g.

D.1 Details of Code Mutation for HOC

Here, we build on the generic DSL presented in Fig. 4 for HOC codes. We describe the HOC-
DSL, Sketch DSL, and constraints for HOC codes (shown in Fig. 23a, Fig. 23b, and Fig. 23c,
respectively). It is to be noted that the HOC DSL does not contain ‘marker’ based actions/conditionals:
(action) pickMarker, (action) putMarker, (conditional) marker, (conditional) noMarker. It does
not allow few more conditionals: noPathA, noPathL and noPathR. It also does not contain the
While construct. We consider a concrete example (continued from the example presented in Fig. 4d),
in Fig. 23d, illustrate its sketch in Fig. 23e, and its constraints in Fig. 23f. We elaborate its A minimality
constraints and ACTIONEDITS in Fig. 23g and Fig. 23h, respectively.

A brief description of elimination sequences to ensure A-minimality. We identify four se-
quences in HOC codes, which must be removed to ensure code minimality. These are: turnLeft,
turnRight or turnRight, turnLeft, which do not lead to any change in the output; turnLeft,
turnLeft, turnLeft, which can be replaced by a single turnRight; and ﬁnally turnRight,
turnRight, turnRight, which can be replaced by a single turnLeft. It is to be noted that we
also eliminate variants of the above sequences, that contain φ, but effectively reduce to these four
sequences only, given that φ denotes an empty action.

D.2 Details of Code Mutation for Karel

Here, we present a modiﬁed/detailed form of Fig. 4 for Karel codes in particular. We describe the
Karel-DSL, Sketch DSL, and constraints for Karel codes (shown in Fig. 24a, Fig. 24b, and Fig. 24c,
respectively). It is to be noted that Karel-DSL does not contain the RepeatUntil construct. We
consider a concrete example (same as the Karel solution code presented in Fig. 2b), in Fig. 24d,
illustrate its sketch in Fig. 24e, and its constraints in Fig. 24f. We elaborate its A minimality constraints
and ACTIONEDITS in Fig. 24g and Fig. 24h, respectively.

A brief description of elimination sequences to ensure A-minimality. In addition to the four
sequences considered in HOC codes, we identify sixteen more sequences in Karel codes, which
must be removed to ensure code minimality. These are: pickMarker, putMarker or putMarker,
pickMarker or pickMarker, pickMarker or putMarker, putMarker, which do not lead to any
change in the output or leads to a crash in the Karel grid (only one marker is allowed per grid-
cell); turnLeft, pickMarker, turnRight or turnRight, pickMarker, turnLeft or turnLeft,
putMarker, turnRight or turnRight, putMarker, turnLeft, which bring about the same out-
put without the ‘turn’ actions; pickMarker, turnLeft, putMarker or putMarker, turnLeft,
pickMarker or pickMarker, turnRight, putMarker or putMarker, turnRight, pickMarker,
which bring about the same output without the ‘marker’ actions; and ﬁnally pickMarker, turnLeft,
pickMarker or pickMarker, turnRight, pickMarker or putMarker, turnLeft, putMarker or
putMarker, turnRight, putMarker, which leads to a crash in the Karel grid (only one marker is
allowed per grid-cell). It is to be noted that we also eliminate variants of the above sequences, that
contain φ, but effectively reduce to these basic sequences only, given that φ denotes an empty action.

17

code C
rule y
rule s

:= def Run () do y
:= s | g | s; g
:= a | s; s | If (b) do s | If (b) do s Else s

| Repeat (x) do s

:= RepeatUntil (goal) do s

rule g
action a := move | turnL | turnR
bool b
iter x

:= pathA | pathL | pathR
:= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10

(a) Code DSL – HOC

sketch Q := def Run () do Y
rule Y
rule S

:= S | G | S; G
:= A | S; S | If (B) do S | If (B) do S Else S

| Repeat (X) do S

:= RepeatUntil (goal) do S
rule G
Comments: A may be φ or take values of action a

A denotes a sequence A1, . . . , An

(b) Sketch DSL – HOC

def Run(){

RepeatUntil(goal){

move
If(pathLeft){
turnLeft

}

}

}

def Run(){
A1
1, A2
1 (A1)
RepeatUntil(goal){
2 (A2)

2, A4

2, A5

A1
2, A2
2, A3
If( B1){
3, A2
A1

}

3, A3

3, A4

3, A5

3 (A3)

}

}

Input: code C, sketch Q ← Ω(C), map ω(·| C), δsize, δiter
(∆0) Size of generated code may be at most Csize + δsize
(∆1) Edit action sequences ACTIONEDITS({A ∈ Q}, ω(·| C))
(∆2) For each X ∈ Q : |X − ω(X| C)| ≤ δiter
(∆3) Constraints induced by structure {Abefore; Repeat {A}; Aafter}

i. A is not a sufﬁx of Abefore
ii. A is not a preﬁx of Aafter

(∆4) For each B ∈ Q :

i. ω(B | C) ∈ {pathA }
⇒ B ∈ {pathA }
ii. ω(B | C) ∈ {pathL, pathR }
⇒ B ∈ {pathL, pathR }

(∆5) Constraints induced on A nested inside conditional B
(∆6) For each A ∈ Q, constraints ensuring minimality of A

(c) Types of Sketch Constraints – HOC

Input: Cin, Qin, ω(·| Cin), δsize = 2
(∆0) Up to 2 new actions may be added in total to A1, A2, A3
(∆1) Edit action sequences ACTIONEDITS({A1, A2, A3}, ω(·| Cin))
(∆4) B1 = pathL ∨ B1 = pathR
(∆5) (B1 = pathL) ⇒
∃i ∈ [5] s.t (cid:0)Ai
(∆5) (B1 = pathR) ⇒
∃i ∈ [5] s.t (cid:0)Ai

3 = turnL ∧ ( ∀j < i, Aj

3 /∈ {move, turnR})(cid:1)(cid:17)

3 /∈ {move, turnL})(cid:1)(cid:17)

3 = turnR ∧ ( ∀j < i, Aj

(cid:16)

(cid:16)

(d) Code Cin

(e) Sketch Qin

(f) Qin-Constraints for HOC

(∆6) A1, A2, A3 are minimal

Set of elimination sequences E := {(L, R), (R, L), (L, L, L), (R, R, R)}

(i) Apply each elimination sequence from E to A1 as a set of constraints.
(ii) Apply each elimination sequence from E to A2 as a set of constraints.
(iii) Apply each elimination sequence from E to A3 as a set of constraints.

(g) Qin-Constraints: (∆6) A1, A2, A3 are minimal

Shorthand notation for actions: move → M, turnL → L, turnR → R, φ → empty-action.

Local A constraints that deﬁne the values that each action sequence can take:

1 = φ)∧ (A2

1 = M ∨ A2

1 = L ∨ A2

1 = R ∨ A2

1 = φ)
2 = φ) (A3

2 = M∨A2

2 = L∨A2

2 = R∨A2

2 = M)∧ (A4

2 = M∨A4

2 = L∨A4

2 = R∨A4

2 = φ)∧

(i) (A1
(ii) (A1
(A5
(iii) (A1
(iv) (A4
(v) (A1
(A4
(vi) (A1
(vii) (A4

1 = M ∨ A1
2 = M∨A1
2 = M ∨ A5
2 (cid:54)= φ ∨ A2
2 (cid:54)= φ ∨ A5
3 = M ∨ A1
3 = M ∨ A4
3 (cid:54)= φ ∨ A2
3 (cid:54)= φ ∨ A5

1 = L ∨ A1
2 = L∨A1
2 = L ∨ A5
2 (cid:54)= φ) ⇒ (A4
2 (cid:54)= φ) ⇒ (A1
3 = L ∨ A1
3 = L ∨ A4
3 (cid:54)= φ) ⇒ (A4
3 (cid:54)= φ) ⇒ (A1

1 = R ∨ A1
2 = R∨A1
2 = R ∨ A5

2 = φ)∧ (A2
2 = φ)

2 = φ ∧ A5
2 = φ ∧ A2

2 = φ)
2 = φ)
3 = φ)∧ (A2
3 = φ)∧ (A5
3 = φ)
3 = φ)

3 = R ∨ A1
3 = R ∨ A4

3 = φ ∧ A4
3 = φ ∧ A2

3 = M ∨ A2
3 = M ∨ A5

3 = L ∨ A2
3 = L ∨ A5

3 = R ∨ A2
3 = R ∨ A5

3 = φ)∧ (A3
3 = φ)

3 = L ∨ A3

3 = R)∧

Global A constraints that allow actions to be added to either of A1, A2, A3:
2 = φ∧ A1
2 = φ ∧ A2
3 = φ ∧ A4
1 = φ ∧ A2
3 = φ ∧ A4
1 = φ ∧ A2
2 = φ∧ A4
1 = φ ∧ A2
2 = φ∧ A4
1 = φ ∧ A2

1 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ) ⇒ (A1
3 (cid:54)= φ) ⇒ (A1
3 (cid:54)= φ) ⇒ (A1

2 = φ ∧ A5
3 = φ ∧ A2
3 = φ ∧ A2
2 = φ ∧ A1
2 = φ ∧ A2

2 = φ ∧ A4
1 = φ∧ A1
1 = φ∧ A1
1 = φ ∧ A1
1 = φ ∧ A1

1 (cid:54)= φ ∨ A2
2 (cid:54)= φ ∨ A2
2 (cid:54)= φ ∨ A5
3 (cid:54)= φ ∨ A2
3 (cid:54)= φ ∨ A5

(i) (A1
(ii) (A1
(A4
(iii) (A1
(A4

3 = φ ∧ A2
3 = φ ∧ A5
3 = φ ∧ A5
2 = φ ∧ A5
2 = φ ∧ A5

3 = φ ∧ A4
3 = φ)
3 = φ)
2 = φ∧)
2 = φ)

3 = φ ∧ A5

3 = φ)

(h) Qin-Constraints: (∆1) ACTIONEDITS({A1, A2, A3}, ω(·| Cin))

Figure 23: Illustration of Code Mutation for HOC on the solution code for task Maze 16 from the
Hour of Code: Classic Maze challenge by Code.org [23]; We build on Fig. 4 here.

18

code C
rule s

:= def Run () do s
:= a | s; s | If (b) do s | If (b) do s Else s

| While (b) do s | Repeat (x) do s

action a := move | turnL | turnR | putM | pickM
:= pathA | noPathA | pathL | noPathL
bool b

| pathR | noPathR | marker | noMarker

iter x

:= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10

(a) Code DSL – Karel

Input: code C, sketch Q ← Ω(C), map ω(·| C), δsize, δiter
(∆0) Size of generated code may be at most Csize + δsize
(∆1) Edit action sequences ACTIONEDITS({A ∈ Q}, ω(·| C))
(∆2) For each X ∈ Q : |X − ω(X| C)| ≤ δiter
(∆3) Constraints induced by structure {Abefore; Repeat {A}; Aafter}

i. A is not a sufﬁx of Abefore
ii. A is not a preﬁx of Aafter

(∆4) For each B ∈ Q :

i. ω(B | C) ∈ {pathA, noPathA }
⇒ B ∈ {pathA, noPathA }

sketch Q := def Run () do Y
rule Y
rule S

:= S
:= A | S; S | If (B) do S | If (B) do S Else S

| While (B) do S | Repeat (X) do S
Comments: A may be φ or take values of action a

A denotes a sequence A1, . . . , An

(b) Sketch DSL – Karel

ii. ω(B | C) ∈ {pathL, noPathL pathR, noPathR }
⇒ B ∈ {pathL, noPathL, pathR, noPathR }

iii. ω(B | C) ∈ {marker, noMarker }
⇒ B ∈ { marker,noMarker }

(∆5) Constraints induced on A nested inside conditional B
(∆6) For each A ∈ Q, constraints ensuring minimality of A

(c) Types of Sketch Constraints – Karel

def Run(){
putMarker
While(pathAhead){

move
turnLeft
move
turnRight
putMarker

}

}

1, A5

1, (A1)

(A2)

def Run(){
A1
1, A4
1, A3
1, A2
While( B1){
A1
2, A2
2,
2, A5
A3
2, A4
2,
A6
2, A8
2, A7
2,
2 , A11
2, A10
A9
2 ,
A12
2 , A13
2

}
A1
3, A2

3 (A3)

}

Input: Cin, Qin, ω(·| Cin), δsize = 2
(∆0) Up to 2 new actions may be added in total to A1, A2, A3
(∆1) Edit action sequences ACTIONEDITS({A1, A2, A3}, ω(·| Cin))
(∆4) B1 = pathA ∨ B1 = noPathA
(∆5) (B1 = pathA) ⇒

2 = move ∧ ( ∀j < i, Aj

2 /∈ {turnL, turnR})(cid:1)(cid:17)

(cid:16)

∃i ∈ [13] s.t (cid:0)Ai
(∆5) (B1 = noPathA) ⇒
∃i ∈ [13] s.t (cid:0)Ai

(cid:16)

2 = move ⇒ ( ∃j < i, Aj

2 ∈ {turnL, turnR})(cid:1)(cid:17)

(d) Code Cin

(e) Sketch Qin

(f) Qin-Constraints for Karel

(∆6) A1, A2, A3 are minimal

Set of elimination sequences E := {(L, R), (R, L), (L, L, L), (R, R, R), (piM, puM), (puM, piM), (piM, piM), (puM, puM), (L, piM, R),
(R, piM, L), (L, puM, R), (R, puM, L), (piM, L, puM), (puM, L, piM), (piM, R, puM), (puM, R, piM),
(piM, L, piM), (piM, R, piM), (puM, L, puM), (puM, R, puM)}

(i) Apply each elimination sequence from E to A1 as a set of constraints.
(ii) Apply each elimination sequence from E to A2 as a set of constraints.
(iii) Apply each elimination sequence from E to A3 as a set of constraints.

(g) Qin-Constraints: (∆6) A1, A2, A3 are minimal

Figure 24: (a)–(g): Illustration of Code Mutation for Karel task Diagonal from the Intro to Program-
ming with Karel course by CodeHS.com [22]; We present the Karel variant of Fig. 4 here.

19

Shorthand notation for actions: move → M, turnL → L, turnR → R, pickM → piM, putM → puM, φ → empty-action.

Local A constraints that deﬁne the values that each action sequence can take:

1 = R ∨ A1
1 = R ∨ A2

1 = piM ∨ A1
1 = piM ∨ A2

1 = puM ∨ A1
1 = puM ∨ A2

1 = φ)
1 = φ)

1 = L ∨ A1
1 = L ∨ A2
1 = puM)
1 = L ∨ A4
1 = L ∨ A5
1 (cid:54)= φ) ⇒ (A4
1 (cid:54)= φ) ⇒ (A1
2 = L ∨ A1
2 = L ∨ A2

1 = R ∨ A4
1 = R ∨ A5

1 = φ ∧ A5
1 = φ ∧ A2

1 = piM ∨ A4
1 = piM ∨ A5
1 = φ)∧
1 = φ)
2 = piM ∨ A1
2 = piM ∨ A2

2 = R ∨ A1
2 = R ∨ A2

1 = puM ∨ A4
1 = puM ∨ A5

1 = φ)
1 = φ)

2 = puM ∨ A1
2 = puM ∨ A2

2 = φ)
2 = φ)

2 = L ∨ A4
2 = L) ∨ (A5
2 = L ∨ A6

2 = R ∨ A4

2 = piM ∨ A4

2 = R ∧ A9

2 = R) ∨ (A5

2 = R ∨ A6

2 = piM ∨ A6

2 = puM ∨ A4
2 = L ∧ A9
2 = puM ∨ A6

2 = φ)
2 = R) ∨ (A5
2 = φ)

2 = R ∧ A9

2 = L)(cid:1)

2 = L ∨ A8
2 = L ∨ A10
2 = puM)
2 = L ∨ A12
2 = L ∨ A13
2 (cid:54)= φ) ⇒ (A4
2 = φ ∧ A2
2 = φ ∧ A2
2 = φ ∧ A2
2 = φ ∧ A2
2 (cid:54)= φ) ⇒ (A1

2 = R ∨ A8

2 = piM ∨ A8

2 = R ∨ A10

2 = piM ∨ A10

2 = puM ∨ A8

2 = φ)
2 = puM ∨ A10

2 = φ)

2 = puM ∨ A12
2 = puM ∨ A13

2 = R ∨ A12
2 = R ∨ A13
2 = φ ∧ A6
2 = φ ∧ A6
2 = φ ∧ A4
2 = φ ∧ A4
2 = φ ∧ A4
2 = φ ∧ A2

2 = piM ∨ A12
2 = piM ∨ A13
2 = φ ∧ A8
2 = φ ∧ A8
2 = φ ∧ A8
2 = φ ∧ A6
2 = φ ∧ A6
2 = φ ∧ A4

2 = φ ∧ A10
2 = φ ∧ A10
2 = φ ∧ A10
2 = φ ∧ A10
2 = φ ∧ A8
2 = φ ∧ A6

2 = φ)
2 = φ)
2 = φ ∧ A12
2 = φ ∧ A12
2 = φ ∧ A12
2 = φ ∧ A12
2 = φ ∧ A12
2 = φ ∧ A8

2 = φ ∧ A13
2 = φ ∧ A13
2 = φ ∧ A13
2 = φ ∧ A13
2 = φ ∧ A13
2 = φ ∧ A10

2 = φ)(cid:1)
2 = φ)(cid:1)
2 = φ)(cid:1)
2 = φ)(cid:1)
2 = φ)(cid:1)
2 = φ)(cid:1)

(iii)

(i)

(A1
∧(A2
∧(A3
∧(A4
∧(A5
(ii) ∧(A1
∧(A4
(A1
∧(A2
∧(A3
∧(A4
∧(cid:0)(A5
∧(A6
∧(A7
∧(A8
∧(A10
∧(A11
∧(A12
∧(A13
(cid:0)(A1
∧(cid:0)(A4
∧(cid:0)(A6
∧(cid:0)(A8
∧(cid:0)(A10
∧(cid:0)(A12
(A1
∧(A2

1 = M ∨ A1
1 = M ∨ A2
1 = piM ∨ A3
1 = M ∨ A4
1 = M ∨ A5
1 (cid:54)= φ ∨ A2
1 (cid:54)= φ ∨ A5
2 = M ∨ A1
2 = M ∨ A2
2 = M)
2 = M ∨ A4
2 = L ∧ A9
2 = M ∨ A6
2 = M)
2 = M ∨ A8
2 = M ∨ A10
2 = piM ∨ A11
2 = M ∨ A12
2 = M ∨ A13
2 (cid:54)= φ ∨ A2
2 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ) ⇒ (A1
2 (cid:54)= φ ∨ A13
3 = M ∨ A1
3 = M ∨ A2

(v)

(iv)

(i) (A1

(ii) (A1

(iii) (A1

1 (cid:54)= φ ∨ A2
⇒ (A1
2 (cid:54)= φ ∨ A2
⇒ (A1
3 (cid:54)= φ ∨ A2
⇒ (A1

1 = φ ∧ A2
3 (cid:54)= φ)
1 = φ ∧ A2

3 = L ∨ A1
3 = L ∨ A2

3 = R ∨ A1
3 = R ∨ A2

3 = piM ∨ A1
3 = piM ∨ A2

3 = puM ∨ A1
3 = puM ∨ A2

3 = φ)
3 = φ)

Global A constraints that allow actions to be added to either of A1, A2, A3:

1 (cid:54)= φ ∨ A3

1 (cid:54)= φ ∨ A4

2 = φ ∧ A2

2 = φ ∧ A4

1 (cid:54)= φ)
2 = φ ∧ A6

2 (cid:54)= φ ∨ A4

2 (cid:54)= φ ∧ A6

2 (cid:54)= φ ∧ A8

2 (cid:54)= φ ∧ A10

2 (cid:54)= φ ∧ A12

2 (cid:54)= φ ∧ A13

1 = φ ∧ A4

1 = φ ∧ A5

1 = φ ∧ A1

3 = φ ∧ A2

3 = φ)

2 = φ ∧ A8

2 = φ ∧ A10

2 = φ ∧ A12

2 = φ ∧ A1

3 = φ ∧ A2

3 = φ)

2 = φ ∧ A13
2 (cid:54)= φ)

1 = φ ∧ A4

1 = φ ∧ A5

1 = φ ∧ A1

2 = φ ∧ A2

2 = φ ∧ A4

2 = φ ∧ A6

2 = φ ∧ A8

2 = φ ∧ A10

2 = φ∧ A12

2 = φ ∧ A13

2 = φ)

(h) Qin-Constraints: (∆1) ACTIONEDITS({A1, A2, A3}, ω(·| Cin))

Figure 24: (h): Illustration of Code Mutation for Karel task Diagonal from the Intro to Programming
with Karel course by CodeHS.com [22]; We present the Karel variant of Fig. 4 here.

20

E Symbolic Execution: Additional Details

In this section, we provide an example demonstrating how MCTS could guide the symbolic execution
in generating more suitable tasks, see Fig. 25.

(a) Initialization

(b) Search tree

(c) (1, 0, 1)

(d) (1, 1, 0)

(e) (1, 1, 1, 0, 0)

Figure 25: Search strategy for guiding the symbolic execution; see the text below for details.

In Fig. 25b, we consider the MCTS search tree for the code Cout from Fig. 1d obtained from the
solution code of task H5 by mutation. At the top of the tree, we have a “Root” node where the initial
location/direction of the agent is picked from among the available choices. Node “RU” corresponds
to the block RepeatUntil; the child “1” from a node “RU” corresponds to unrolling of the loop (i.e.,
goal is false). Node “If” corresponds to the block If; the child “1” from a node “If” corresponds to
executing the code inside (i.e., pathR is true). For the purpose of this demonstration, we limited the
unrolling of the loops to a maximum of 3; for our experiments, this depth is 2n(= 20) as discussed in
Appendix F. Furthermore, in this demonstration, the initial location/direction for the agent are ﬁxed
as shown in Fig. 25a; for our experiments, we consider 5 × 4(= 20) initial conﬁgurations which are
picked by MCTS resulting in a large branching factor at the root of the tree.

Figs. 25c, 25d, and 25e illustrate the symbolic execution output for three different paths in the search
tree as discussed below:

• Fig. 25c corresponds to the path (“RU”:1, “If”:0, “RU”: 1). This path results in agent crashing the
wall. As discussed in Section 4, Fqual(·) score is set to 0 when Fnocrash(·) = 0 and hence this tree
path evaluates to low Fscore(·) score.

• Fig. 25d corresponds to the path (“RU”:1, “If”:1, “RU”: 0). This path results in two moves and

two turns.

• Fig. 25e corresponds to the path (“RU”:1, “If”:1, “RU”: 1, “If”:0, “RU”: 0). This path results in

three moves and two turns, and is the higher scoring path compared to other two paths.

MCTS is extremely effective as a search strategy and quickly learns to pick paths with high scores.
For the same code Cout used in Fig. 25 above, Fig. 8a shows the temporal trends of different feature
values in Fscore averaged over a time window of 100 steps in our experiments. For further details, we
refer the reader to [14] for an overview of the MCTS procedure, and to [15] where MCTS is used
with symbolic execution to direct the exploration towards costly paths.

21

RUIfRUIfRURURUIfRURU1010101000001010Root(loc, dir)F Experimental Evaluation: Additional Details and Results

This section elaborates the experimental setup and results described in Section 4. We begin by
providing details on the MCTS procedure in the symbolic execution stage of our task synthesis
algorithm. Furthermore, we provide insights on generating multiple tasks for a single code. We also
illustrate some example output tasks which violate criteria (V) and (VI), and mutated codes which got
pruned in the symbolic execution stage (see Figure 7). Our implementation of the symbolic execution
stage with MCTS procedure is publicly available (see Footnote 2).

F.1 Speciﬁcation of MCTS for Single Run and Additional Results

In this section, we elaborate on the speciﬁcation of MCTS, brieﬂy discussed in Section 4. We begin
by discussing choices that effect the scalability and run-time of MCTS procedure.

Choice of initial location and direction. When doing symbolic execution using MCTS, the proce-
dure begins by picking an initial location and direction for the agent (see Fig. 5(b)). Given a grid-size
n, and four initial directions (north, east, south, west) of the agent to choose from, we get a total of
4n2 choices for our initial conﬁguration of the grid-puzzle. In the implementation used for generating
the results, we restricted the set of initial choices to 20 (by choosing only 5 initial locations of the
grid including four corners and centre, and 4 directions)—this aids in exploration by limiting the
branching factor at the root of the MCTS’s tree.

Tree depth for symbolic execution. The depth of the symbolic tree depends on the nature of the
corresponding solution code. For codes without the RepeatUntil or While constructs (H1, H2,
H3, K7, K8 and K9), the tree-depth is bounded. But, for more complex codes (H4, H5, H6, and
K10), the tree depth is unbounded. In our implementation, we limited the unrolling of the loops to
a maximum of 2n(= 20). To get an insight into the complexity of the problem, we note that with
20 initial choices and a depth of 2n, there are over 40 million leaves in the symbolic tree for codes
H5 and H6 which contain conditionals nested inside RepeatUntil or While constructs.4 Next, we
describe the details of our evaluation function for MCTS.
Details on the evaluation function Fscore. Our evaluation function Fscore(Tout, Cout, Tin, Cin) ∈ [0, 1]
measures the suitability of a generated task. A higher Fscore indicates a more suitable task. We
describe the elements of our evaluation function in greater detail here. We deﬁned it in Section 4 and
present it here again for completeness:

Fscore(Tout, Cout, Tin, Cin) = 1(cid:0)Fqual(Tout

vis , Cout) = 1, Fnocut(Tout

vis , Cout) ≥ δqual, Fnocrash(Tout
(cid:123)(cid:122)
2a
vis , Cout) + α3Fdiss(Tout
vis , Cout) + α2Fqual(Tout

vis , Cout) = 1(cid:1)
(cid:125)
vis)(cid:3)
(cid:125)

vis , Tin

·

(2)

(cid:124)
(cid:2)α1Fcov(Tout
(cid:124)

(cid:123)(cid:122)
2b

where 1 is an indicator function and each constant α = 1/3. It is to be noted that component 2b in
Eq.2 supplies the gradients for guiding the search in MCTS. At the end of the MCTS run (containing
2 million iterations), the best task (i.e, the one with the highest Fscore value) is picked only from
the pool of generated tasks which satisfy Fcov(·) = 1, Fscore(·) > 0. We discuss each constituent
function of Fscore next.
vis , Cout) ∈ [0, 1] evaluates the quality and
Task quality component of evaluation function. Fqual(Tout
validity of Tout. Its is deﬁned as a linear combination of the normalized counts of certain features
of Tout
vis when Cout is executed. As certain elements differ in the two task types, HOC and Karel, we
deﬁne the features differently for each. More precisely, for HOC tasks, we have:

F HOC

qual (Tout

vis , Cout) =

1
4

(cid:16) #moves
2n

+

#turns
n

+

#segments
n/2

+

#long-segments
n/3

(cid:17)

where the individual features are deﬁned as

4The actual memory footprint of the MCTS procedure is small given that the symbolic tree is dynamically
constructed during a run and the actual number of leaves explored is much lesser. In fact, the average runtime
per output task (i.e., one MCTS run) as reported in Column 8 of Fig. 7 is achieved on a laptop machine with 2.8
GHz Quad-Core Intel Core i7 processor and 16 GB RAM.

22

• #moves: This refers to the count of ‘moves’.
• #turns: This refers to the count of ‘turns’.
• #segments: This refers to the number of consecutive sequence (≥ 3) of ‘moves’.
• #long-segments: This refers to the number of longer consecutive sequence (≥ 5) of ‘moves’.

For Karel tasks, we additionally have two marker based features to deﬁne the quality of the task i.e,

F Karel

qual (Tout

vis , Cout) =

·

(cid:16) #moves
2n

+

#turns
n

+

#segments
n/2

+

#long-segments
n/3

(cid:17)

(cid:16) #pick-markers
n
where the additional features are deﬁned as

+

·

+

#put-markers
n

(cid:17)

3
4
1
4

1
4
1
2

• #pick-markers: This refers to the count of ‘pick-marker’ activity.
• #put-markers: This refers to the count of ‘put-marker’ activity.

While we have used high-level features of Tout
vis to deﬁne the quality of a task, one could also embed
more speciﬁc domain knowledge in deﬁning these features to obtain more interesting/complex tasks.
Task dissimilarity component of the evaluation function. Fdiss(Tout
visual dissimilarity of Tout
as follows:

vis) ∈ [0, 1] evaluates the
vis. We deﬁne it as a linear combination of the dissimilarity features

vis w.r.t. Tin

vis , Tin

Fdiss(Tout

1
3
where the individual features are deﬁned as

diss(loc | Tout

vis , Tin

vis , Tin

vis) =

(cid:16)

vis) + diss(dir | Tout

vis , Tin

vis) + diss(grid-cells | Tout

vis , Tin

vis)

(cid:17)

• diss(loc | Tout
puzzles Tout
• diss(dir | Tout

vis) ∈ {0, 1} measures the dissimilarity in the agent’s initial location in the task-

vis , Tin
vis and Tin
vis.
vis , Tin
vis) ∈ {0, 1} measures the dissimilarity in the agent’s initial direction in the
task-puzzles Tout
• diss(grid-cells | Tout

vis and Tin
vis.
vis) ∈ [0, 1] measures the grid-cell level dissimilarity in the task-puzzles Tout
vis , Tin
vis
vis. This is computed as the normalized Hamming distance w.r.t. the two grid-worlds (i.e.,

and Tin
number of cells which are different, multiplied with a normalization factor of 2

n2 ).

Deep dive into an MCTS run for Karel. Analogous to the example provided in Section 4, we take a
closer look at an MCTS run for the Karel task K10, shown in Fig. 26. Fig. 26a and Fig. 26b illustrate
the improvement in various components of Fscore as the number of MCTS iterations increases. Best
tasks at different iterations are shown in Fig. 26c and Fig. 26d. As expected, the more the iterations,
the better the tasks which are generated.

F.2 Speciﬁcation of MCTS for Multiple Runs with Diversity and Additional Results

Our task synthesis algorithm can also generate multiple tasks for a single code, with sufﬁcient
diversity. To achieve this, we modify the evaluation function Fscore guiding the MCTS search. We
introduce a diversity measure Fdiversity which measures the diversity between the generated tasks.
th
More concretely, when generating a new (k + 1)
task, we capture the diversity score w.r.t the tasks
generated in the previous k-runs of MCTS {Tout

vis,1, . . . , Tout

vis,k}.

Modiﬁed evaluation function Fscore. Our evaluation function with the new diversity component is
given below with each α = 1/4. We have Fscore(Tout, Cout, Tin, Cin, {Tout

vis,1, . . . , Tout

vis,k}) :=

1(cid:0)Fqual(Tout
(cid:124)
(cid:2)α1Fcov(Tout
(cid:124)

vis , Cout) = 1, Fnocut(Tout

vis , Cout) ≥ δqual, Fnocrash(Tout
(cid:123)(cid:122)
3a
vis , Tin
vis , Cout) + α3Fdiss(Tout
(cid:123)(cid:122)
3b

vis , Cout) + α2Fqual(Tout

vis , Cout) = 1(cid:1)
(cid:125)

·

vis) + α4Fdiversity(Tout

vis , Cout | {Tout

vis,1 . . . Tout

vis,k})(cid:3)
(cid:125)

(3)

23

(a) Trends in Fscore features

(b) Trends in Fscore features capturing dissimilarity

(c) Best at 20

(d) Best at 2M

Figure 26: Illustration of a single MCTS run on Cout from Fig. 2d obtained from solution code of task
K10 by mutation. (a, b) show the temporal trends of different feature values in Fscore averaged over
a time window of 100 steps. (c, d) show the best, i.e., highest scoring, tasks generated up to times
2 × 101 and 2 × 106 respectively.

vis,1, . . . , Tout

vis , Cout | {Tout

Diversity score of tasks Fdiversity. Here, we describe the diversity component of the evaluation
function. Fdiversity(Tout
vis,k}) ∈ [0, 1] operates on a pool of generated tasks, and
computes a diversity score for the new task w.r.t the tasks in the pool. Initially, the pool of tasks
generated is empty. In this case, we have Fdiversity(Tout
After one run of MCTS, we have one task in the task pool {Tout
subsequent task Tout
vis as follows. First, if Tout
Otherwise, the diversity score is given by

vis,1}. We deﬁne the diversity score for a
vis,1}) = 0.

vis,1 then we set Fdiversity(Tout

vis , Cout | {}) = 1.

vis , Cout | {Tout

vis = Tout

Fdiversity(Tout

vis , Cout | {Tout

vis,1}) =

(cid:16)

1
4

diss(loc | Tout

vis , Tout

vis,1) + diss(dir | Tout

vis , Tout

vis,1)

where the individual features are deﬁned as

+ diss(grid-cells | Tout

vis , Tout

vis,1) + diss(symbolic-paths | Tout

vis , Tout

vis,1)

(cid:17)

• diss(loc | Tout

vis , Tout

vis,1) ∈ {0, 1} measures the dissimilarity in the agent’s initial location in the

task-puzzles Tout

vis and Tout

vis,1.

• diss(dir | Tout

vis , Tout

vis,1) ∈ {0, 1} measures the dissimilarity in the agent’s initial direction in the

vis and Tout
vis , Tout

task-puzzles Tout
• diss(grid-cells | Tout

vis,1.
vis,1) ∈ [0, 1] measures the grid-cell level dissimilarity in the task-puzzles
vis,1. This is computed as the normalized Hamming distance w.r.t. the two grid-worlds

Tout
vis and Tout
(i.e., number of cells which are different, multiplied with a normalization factor of 2

n2 ).

• diss(symbolic-paths | Tout
in the generation of Tout
these paths.

vis , Tout
vis and Tout

vis,1) ∈ [0, 1] measures the dissimilarity in the symbolic-paths used
vis,1. This is computed as the normalized “edit distance" between

After we have run k MCTS runs, the pool of tasks generated is populated ({Tout
diversity score of the subsequent task is computed as follows:
vis , Cout | {Tout

vis,1, . . . , Tout

Fdiversity(Tout

Fdiversity(Tout

vis , Cout | {Tout

vis,i})

vis,k}) = min
i∈[k]

vis,1 . . . Tout

vis,k}) and the

As the pool of tasks grow, it becomes more constrained to generate a task that is diverse from
all the tasks in the pool. In general, there is a limit to the number of tasks per code that can be
generated—eventually, the new task will have Fdiversity to be 0 or Fscore to be 0. As stated in Section 4,
for each code Cout, we generated up to 10 different visual tasks using this process.

24

0,00,20,40,60,81,02002K5K10K15K20K40K60K80K100K120K140K160K180K200K400K600K800K1M1.2M1.4M1.6M1.8M2MNormalized features  coverageno crashmovesturnspick markers put markerssegments0,00,20,40,60,81,02002K5K10K15K20K40K60K80K100K120K140K160K180K200K400K600K800K1M1.2M1.4M1.6M1.8M2MNormalized features  diss (grid-cells)diss (loc,dir)Illustration of output tasks using diversity. We illustrate our diverse task generation process on
both HOC and Karel tasks. Fig. 27 shows the 10 diverse tasks generated for code shown in Fig. 1d,
while Fig. 28 shows the 6 diverse tasks generated for the code shown in Fig. 2d.

(a) Tout

vis,1

(b) Tout

vis,2

(c) Tout

vis,3

(d) Tout

vis,4

(e) Tout

vis,5

(f) Tout

vis,6

(g) Tout

vis,7

(h) Tout

vis,8

(i) Tout

vis,9

(j) Tout

vis,10

Figure 27: Illustration of task diversity on Cout from Fig. 1d which was obtained from solution code
of task H5 by mutation. All the 10 diverse tasks generated are shown here.

(a) Tout

vis,1

(b) Tout

vis,2

(c) Tout

vis,3

(d) Tout

vis,4

(e) Tout

vis,5

(f) Tout

vis,6

Figure 28: Illustration of task diversity on Cout from Fig. 2d which was obtained from solution code
of task K10 by mutation. First 6 diverse tasks are shown here.

F.3 Insights into Results of Fig. 7

In this section, we provide further insights into the ﬁnal results presented Fig. 7. In particular, we
illustrate few limitations of the codes and tasks we generate (w.r.t task-synthesis objectives deﬁned in
Section 2):

• Limitations of mutated codes generated: Column 4 of Fig. 7 lists the number of codes generated
by our mutation stage. However this set (#Cout
∆=all) continues to have few semantic irregularities (as
discussed in Section 3.1). Some of these irregularities are illustrated in Fig. 29. These codes are
pruned by the symbolic execution stage of our algorithm, and the revised set of codes(#Cout) is
listed in Column 6 of Fig. 7.

• Tasks which violate task-synthesis objective (V): Out of the ﬁnal set of tasks generated (shown
in Column 7 of Fig. 7), some of them violate objective (V). The fraction of the tasks which satisfy
this particular objective are listed in Column 9 of Fig. 7. In Fig. 30 we illustrate two examples
which violate the objective, in output tasks for H5 and H6.

• Tasks which violate task-synthesis objective (VI)δmini=1: Column 10 of Fig. 7 lists the fraction of
the tasks generated that satisfy this particular objective, on task-minimality. In Fig. 31, we illustrate
two examples of tasks which violate this minimality constraint with δmini = 1.

25

def Run(){
move
turnRight
move
turnRight
move

}

def Run(){
turnLeft
Repeat(5){
move
turnRight

}

}

def Run(){

Repeat(4){
move

}
turnLeft
move
turnLeft
Repeat(6){
move

}

}

def Run(){

RepeatUntil(goal){

move
turnLeft
move
turnLeft

}

}

(a) H1: Cout

(b) H2: Cout

(c) H3: Cout

(d) H4: Cout

def Run(){

RepeatUntil(goal){

turnRight
turnRight
move
If(pathRight){
turnRight

}

}

}

def Run(){

RepeatUntil(goal){
If(pathAhead){
move
turnLeft

}
Else{

turnLeft

}

}

}

def Run(){

Repeat(5){
pickMarker
move
turnRight
putMarker

}

}

def Run(){
putMarker
While(pathAhead){

move
turnLeft
move
turnLeft
putMarker

}

}

(e) H5: Cout

(f) H6: Cout

(g) K8: Cout

(h) K10: Cout

Figure 29: Illustration of codes with semantic irregularities in Column Cout
∆=all of Fig. 7. (a)–(f)
show the semantic irregularities in mutated codes generated for HOC tasks. All of these codes
lead to circular paths in output task, and are pruned out in the symbolic execution stage, by the
Fnocut component of the Fscore measure. In particular, consider the semantic irregularity presented
in (g). This code corresponding to the reference task K8, has redundant marker activity. With each
iteration of Repeat, the pickMarker and putMarker actions occur consecutively, leading to no
change in marker activity in the output. (h) illustrates a mutated code for reference task K10 where if
the While is executed more than once, putMarker activity occurs in the same location consecutively
leading to a crash (as only one marker is allowed per grid-cell); if While is executed only once, the
corresponding task generated has a short-cut to the goal.

def Run(){

RepeatUntil(goal){
move
If(pathRight){
turnRight
turnRight

}

}

}

def Run(){

RepeatUntil(goal){
If(pathAhead){

move
move
turnRight

}
Else{

turnRight

}

}

}

(a) H5: Tout
vis

(b) H5: Cout

(c) H6: Tout
vis

(d) H6: Cout

Figure 30: Illustration of violation of task-synthesis objective (V) by generated output tasks for
reference tasks H5 and H6. (a, b) show the irregularity in generated task (Tout
vis , Cout) for H5 which
leads to only a straight path in the visual-puzzle. The corresponding code renders the If construct
vis , Cout) for H6. Here,
redundant. (c, d) illustrate a similar irregularity in the generated task (Tout
IfElse construct is not needed for the optimal solution.

def Run(){
move
turnRight
RepeatUntil(goal){
move
If(pathRight){
turnRight

}

}

}

def Run(){
turnRight
move
RepeatUntil(goal){
If(pathAhead){

move

}
Else{

turnRight

}

}

}

(a) H5: Tout
vis

(b) H5: Cout

(c) H6: Tout
vis

(d) H6: Cout

Figure 31: Illustration of violation of task-synthesis objective (V1), with δmini = 1 by generated
output tasks for reference tasks H5 and H6. (a, b) show the violation of task minimality, for the task
vis , Cout) generated for H5. In particular, the ﬁrst two actions in the code shown in (b) are unnecessary.
(Tout
Similarly, (c, d) show the violation of task minimality, for the task (Tout

vis , Cout) generated for H6.

26

F.4 Adding More Variability to Output Tasks

Here, we propose a few simple extensions to our task generation algorithm allowing us to add more
variability in the visual puzzles. We show that high variability can be achieved through suitable
post-processing and pre-processing of the generated tasks (happening after and before the symbolic
execution process, respectively). We describe each of these strategies next.

Different grid-sizes. A simple strategy to enhance task variability is by altering the grid-size
parameter n. Fig. 32 illustrates the tasks generated with different values of the grid-size parameter n.
For instance, this strategy is employed in generating multiple input-output pairs for Karel tasks in
Intro to Programming with Karel course by CodeHS.com [22].

(a) Tout

vis ; n = 10

(b) Tout

vis ; n = 8

(c) Tout

vis ; n = 6

Figure 32: Tasks generated for Cout from Fig. 1d when varying grid-size (n).

Post-processing: Using distractor paths. One of the ways to add variability to the tasks is by
adding distractor-paths, to the generated tasks. These paths are added after the symbolic execution
stage is complete, and a basic task has been generated. Fig. 33a shows the task generated without any
post-processing of the output of symbolic execution, carried out on Cout from Fig. 1d. Fig. 33b, and
Fig. 33c illustrate two different post-processing patterns that yield tasks with greater variability, for
the same code.

Pre-processing: Using different initializations of grid cells. We could also add task variability by
initializing the grid-cells differently, before they are subjected to symbolic execution for task synthesis.
We have a set of ﬁxed grid-patterns, which when chosen as initializations of the grid-world yield
very different looking output tasks. Fig. 34a shows the task generated for Cout from Fig. 1d without
any pre-processing on the grid-cells. Fig. 34b and Fig. 34c show two different tasks obtained using
different grid-initializations, for the same code.

27

(a) No post processing

(b) Post-processing 1

(c) Post-processing 2

Figure 33: Illustration of the (post-processing) distractor path strategy to increase task variability on
tasks generated from Cout in Fig. 1d which was obtained from solution code of task H5 by mutation.
(a) illustrates the basic task obtained after symbolic execution. (b, c) show two different distractor
paths added after the symbolic execution stage, yielding visually very different tasks.

(a) No grid-cell initialization

(b) Grid-cell initialization 1

(c) Grid-cell initialization 2

Figure 34: Illustration of the (pre-processing) grid-cell initialization strategy to increase task vari-
ability on tasks generated from Cout in Fig. 1d which was obtained from solution code of task H5 by
mutation. (a) illustrates the basic task obtained after symbolic execution. (b, c) show two different
grid-cell initializations, yielding visually very different tasks.

28

(symbolic output)(visual output)(symbolic input)(symbolic output)(visual output)(symbolic input)(symbolic output)(visual output)(symbolic input)(symbolic output)(visual output)(symbolic input)(symbolic output)(visual output)(symbolic input)(symbolic output)(visual output)(symbolic input)