2
2
0
2

r
a

M
0
3

]

G
L
.
s
c
[

1
v
2
7
9
5
1
.
3
0
2
2
:
v
i
X
r
a

Higher-Order Generalization Bounds

Higher-Order Generalization Bounds: Learning Deep
Probabilistic Programs via PAC-Bayes Objectives

Jonathan Warrell
Program in Computational Biology and Bioinformatics
Department of Molecular Biophysics and Biochemistry
Yale University, New Haven, CT, 06520, USA

Mark Gerstein
Program in Computational Biology and Bioinformatics
Department of Molecular Biophysics and Biochemistry
Yale University, New Haven, CT, 06520, USA

Editor:

jonathan.warrell@yale.edu

pi@gersteinlab.org

Abstract

Deep Probabilistic Programming (DPP) allows powerful models based on recursive com-
putation to be learned using eﬃcient deep-learning optimization techniques. Additionally,
DPP oﬀers a uniﬁed perspective, where inference and learning algorithms are treated on
a par with models as stochastic programs. Here, we oﬀer a framework for representing
and learning ﬂexible PAC-Bayes bounds as stochastic programs using DPP-based methods.
In particular, we show that DPP techniques may be leveraged to derive generalization
bounds that draw on the compositionality of DPP representations. In turn, the bounds we
introduce oﬀer principled training objectives for higher-order probabilistic programs. We
oﬀer a deﬁnition of a higher-order generalization bound, which naturally encompasses single-
and multi-task generalization perspectives (including transfer- and meta-learning) and a
novel class of bound based on a learned measure of model complexity. Further, we show how
modiﬁed forms of all higher-order bounds can be eﬃciently optimized as objectives for DPP
training, using variational techniques. We test our framework using single- and multi-task
generalization settings on synthetic and biological data, showing improved performance and
generalization prediction using ﬂexible DPP model representations and learned complexity
measures.

Keywords: Statistical Learning Theory, Probabilistic Programming, Variational Inference,
Meta-Learning, Computational Biology

1. Introduction

Deep Probabilistic Programming (DPP) provides a framework which combines the beneﬁts
of models based on recursion with the strengths of deep-learning based optimization [21,22].
In addition, it provides a general model of probabilistic computation, for which recent work
has provided associated operational and denotational semantics [11,18,20]. As such, there is
no need to make a ﬁrm distinction between models and inference/training algorithms in a
DPP setting, since all are stochastic functions; this perspective naturally leads to treating
inference/training optimization algorithms as ‘ﬁrst-class’ citizens which may themselves
be optimized [21,22]. However, the objectives used to learn DPPs are typically standard

1

 
 
 
 
 
 
Warrell and Gerstein

functions such as the log-likelihood, or evidence lower bound (ELBO) [21,22]. Potentially,
more sophisticated training objectives can be derived by considering generalization bounds
linked to speciﬁc learning settings, for instance, transfer- or meta-learning [4,15,25], or data-
dependent priors [3,7,8,14,17]. We propose that, as for optimization algorithms, generalization
bounds should themselves be viewed as ‘ﬁrst-class’ stochastic functions to be learned in
a DPP setting. Doing so has the potential both to motivate new bounds based on DPP
compositionality and data-dependence, and to motivate new objectives for training programs
and collections of programs in a DPP setting.

In light of the above, we propose a general deﬁnition of a higher-order (h-o) generalization
bound. Brieﬂy, this is a higher-order function that returns a generalization bound, which holds
with high-probability given independence conditions regarding its inputs, and those of the
bound returned. We show that this deﬁnition provides a uniﬁed representation for existing
and novel bounds for single- and multi-task generalization based on a PAC-Bayes framework.
Particularly, we show that existing transfer-/meta-learning [4,15,25] and data-dependent
[3,7,8,14,17] PAC-Bayes bounds can be formulated directly as h-o bounds. An advantage of
the DPP perspective is that it places minimal restrictions on the functional forms used by
these bounds; existing approaches focus on restricted forms of the prior and/or posterior
[3,4,7,8,14,15,17,25,26], or hyper-prior and -posterior in the meta-learning setting [4,15,25],
which in our framework may be arbitrary in form. We show that modiﬁed forms of these
bounds can be derived in our framework using recent variational and multi-sample techniques
[16,19,21,22], providing objectives that can be eﬃciently optimized in the DPP setting.

Further, we introduce a novel form of h-o bound which uses a learned measure of model
complexity (which we term a second-order complexity bound). This bound takes as input
a generalization classiﬁer, which predicts a model’s task-speciﬁc generalization error from
its parameters, and may be trained jointly with a base-level task classiﬁer. The bound may
be naturally extended to a meta-learning setting using the framework we introduce. Unlike
previous meta-learning PAC-Bayes bounds [4,15,25], our learned model complexity bound
uses a feed-forward model to estimate the per-task model complexity, and hence avoids the
need to estimate separate KL-divergence terms for each task during training. As above, we
show that variational and multi-sample techniques can be used to derive a tractable training
objective from our novel bound. Finally, we introduce a convenient stochastic type system
for expressing DPP models and h-o bounds, which directly adapts aspects of the systems
introduced in [11,14,18,20] to provide a framework with a clear underlying semantics.

We begin by outlining our stochastic type-system, before introducing our general frame-
work for h-o generalization bounds. We then give several examples of how existing transfer-
and meta-learning bounds may be modiﬁed to produce DPP variational objectives in our
framework, before introducing our second-order complexity bound. We test our framework
using single- and multi-task generalization settings on synthetic and biological data, the
latter consisting of gene expression data from a psychiatric genomics dataset, containing
subjects with and without related psychiatric disorders [9,23]. We show improved task
performance and generalization prediction using the ﬂexible DPP objectives and learned
complexity measure we introduce.

2

Higher-Order Generalization Bounds

2. Preliminaries: Stochastic Type System

We ﬁrst introduce a notation for stochastic types and programs, which we will use throughout
the paper. The system we introduce is a variant of the higher-order language speciﬁed
in [20], with some small diﬀerences that we note. We assume we have types A, B, C..., Z
along with function and product types (e.g. A → B and A × B resp.), and write a : A
for a belongs to type A. The type I denotes the unit interval, and we write A(cid:48) for the
type of distributions over A, where we assume for convenience all types are discrete, and
A(cid:48) is the subset of (A → I) containing only maps which sum to 1. A(cid:48) is thus equivalent
to (Mass A) in [18] (the type A is mapped to by the Mass Function Monad). Further, we
include the constructions sample() and thunk() as used in the language of [20]. For p : A(cid:48),
sample(p) is a term of type A, which reduces to a base term in A through probabilistic
β-reduction with probability p(a) (we assume a probabilistic reduction semantics of the
kind in [20]). Further, for a : A, we let thunk(a) denote a term of type A(cid:48); speciﬁcally,
thunk(a) is a function which, for a term a(cid:48), returns the probability that a reduces to a(cid:48).
Hence (thunk(a))(a(cid:48)) = P (a →β a(cid:48)), where →β denotes probabilistic reduction. Particularly,
we have that thunk(sample(p)) = p, and sample(thunk(a)) = a. Here, ‘thunking’ can be
viewed as a means of suspending a probabilistic program sample(p) so that it is prevented
from executing, which may be reactivated by applying a sampling statement. We note that,
unlike [20], we do not distinguish between distributions and thunked programs over a given
type A, since we assign them both to A(cid:48), and hence ‘sampling’ and ‘forcing’ are synonyms
in our system. We outline further technical details of our system and its relation to [20] in
Appendix A.

For convenience, we now introduce a concise notation that will allow us to express DPPs
and associated bounds eﬃciently throughout the paper. We ﬁrst introduce the notation
informally. For a suspended probabilistic program, p : A(cid:48), we introduce a hierarchy of
sampling statements, where p∗ = sample(p), and p+, p++, p+++..., represent programs
suspended at varying levels of execution (referred to as sampling levels 1, 2, 3...). Speciﬁcally,
if f : A × A → B, then we may write f (p+, a(cid:48)(cid:48)) for a suspended program of type B(cid:48),
which when run (i.e. sampled), ﬁrst converts p+ to p∗, which reduces probabilistically to
a(cid:48), and then applies f to (a(cid:48), a(cid:48)(cid:48)) to generate b. Further, we may write f (p+, p++) for a
higher-order suspended program of type B(cid:48)(cid:48); here, the rule is that running a statement
(sampling and reducing) converts all sampling statements at level 1 (i.e. of the form p+)
to p∗ statements, and decrements the sampling levels of all others by one, before applying
probabilistic reduction. Hence, sample(f (p+, p++)) →β f (p∗, p+), and f (p+, p++) : B(cid:48)(cid:48).
Formally, this notation is shorthand for the following construction:

f [p+

1 , p+

2 , ...] = thunk(λ(a1, a2, ...).f (−)[a/p+

1 , a/p+

2 , ...] (p∗

1, p∗

2, ...))

(1)

where f [a1, a2, ...] denotes an expression f containing a1, a2, ..., f [a1/b1, a2/b2, ...] denotes
in f , and f (−) denotes the result of
the result of replacing b1 with a1, b2 with a2 etc.
decrementing the sampling levels of all statements by one in f (following any substitutions;
note that our notation can be further developed to incorporate memoization as in [10],
see Appendix A). The rule in Eq. 1 can be recursively applied to the above example:
f (p+, p++) = thunk(λa1.f (a1, p+) p∗) = thunk(λa2.thunk(λa1.f (a1, a2) p∗) p∗).

3

Warrell and Gerstein

3. Higher-order Generalization Bounds

We start by introducing a general class of deep h-o stochastic classiﬁers in the type system
above, before oﬀering our deﬁnition of a h-o generalization bound in this setting. For
convenience, we assume a binary classiﬁcation setting, although the generalization of our
framework to regression and multi-class classiﬁcation is straightforward.

Stochastic classiﬁer models. We assume we have input and output types X and
Y , where for classiﬁcation Y = {0, 1}. Further, let Z represent ﬁxed-precision positive
and negative reals. We use the ﬁxed notation N(.; µ, Σ) to represent a multivariate normal
(belonging to type Zn → I), and NNT1,T2(.; θ) to represent a neural network with parameters
θ (belonging to function type T1 → T2 for some types T1, T2). We then deﬁne a hierarchy of
types: F0 = (X → Y ), F1 = F (cid:48)
0 , and so on. Here, F0 is the type of
deterministic classiﬁers between X and Y ; F1 represents distributions over F0, corresponding
to stochastic classiﬁers; and F2 represents distributions over F1, which may be interpreted
as a h-o stochastic classiﬁer (which will be used to represent hyper-priors/posteriors in a
meta-learning setting). We can specify ﬂexible models at all these levels via the following
probabilistic programs, f0 : F0, f1 : F1, f2 : F2:

0 = (X → Y ) → I, F2 = F (cid:48)(cid:48)

f0 = NNX,Y (.; θ0)
f1 = NNX,Y (.; NNZd,Θ0(z+
f2 = NNX,Y (.; NNZd,Θ0(z++

1 ; θ1) + e+
1 )
; NNZd,Θ1(z+

1

2 , θ2) + e+

2 ) + e++

1

)

(2)

Here, Θ0, Θ1 are the parameter spaces (types) for θ0, θ1, z1, z2 = N(.; 0d, Id) are standard
normal latent variables (where d is the dimensionality of the latent space), and e1 =
N(.; 0|Θ0|, σI|Θ0|) is a noise term (similarly for e2, substituting Θ1 for Θ0).

Generalization bounds. In the setting above, a generalization bound may be deﬁned
as a function φ : F1 × X N → Z, which takes a stochastic classiﬁer f1 : F1 and a sample of
S size N (S : X N ) and returns a real value. Implicitly, the bound is also paired with an
associated distribution over input and output types, D : (X ×Y )(cid:48), and we additionally require
that, with probability (1 − δ), the Gibbs Risk R of f1 is less than the bound when applied to
a sample drawn from D; hence P (sample(f1)(x) (cid:54)= y) ≤(1−δ) φ(f1, S). For convenience, we
label the type of generalization bounds as Φ. A h-o generalization bound may then be deﬁned
as a h-o stochastic function, which returns a member of Φ subject to certain conditions:

Deﬁnition 1 (Higher-order Generalization Bound). Using the notation deﬁned above,
a higher-order generalization bound is (a) a stochastic function φho with type A1 × A2 ×
... × An → Φ, along with (b) a set of independence assumptions of the form a ⊥⊥ b, where
a ∈ {a1, ..., an} and b ∈ {f1, S}. Additionally, we require that, for a1 : A1, ..., an : An, f1 : F1
and S ∼ D, we have R(f1) = P (sample(f1)(x) (cid:54)= y) ≤(1−δ) (φho(a1, ..., an))(f1, S), assuming
the independence assumptions in (b) are met.

We provide below examples two classes of h-o generalization bound, transfer-/meta-
learning variational bounds and learned model complexity bounds, both within a PAC-Bayes
setting, and discuss how each can be used to provide general training objectives for DPP
optimization.

4

Higher-Order Generalization Bounds

3.1 Transfer-/Meta-learning Variational Bounds

Transfer-Learning. We begin by stating a basic form of the PAC-Bayes bound from [1], in
the general DPP setting:

(3)

1 , f π

φ1(f ρ

1 and f ρ

1 , S) + (1/λ)[KL(f ρ

1 ) + log(1/δ) + (λ2/N )]

1 , S) = R(f ρ
1 denote the PAC-Bayes prior and posterior respectively, R(f ρ

Here, f π
1 , S) =
PS(sample(f1)(x) (cid:54)= y) is the Gibbs risk on S, λ controls the tightness of the bound,
and the remaining notation is as deﬁned above. As proposed in [3,14], a data-dependent
prior may be used in Eq. 3, which is learned on hold-out data S(cid:48). The hold-out data may be
sampled from the same distribution D as the classiﬁer f1 is tested on, or a related distribution,
D(cid:48); we refer to the latter case as a transfer-learning setting. Assuming we have an algorithm
for training the prior, A : (X × Y )N → F1, we may express the transfer-learning bound as a
h-o bound: φTL(S(cid:48)) = λ(f ρ
1 = A(S(cid:48))). Here, we require the independence
assumption S(cid:48) ⊥⊥ S for part (b) of Def. 1. Note however that S(cid:48) ⊥⊥ f1 is not required; as
discussed in [14], f1 may depend on the combined dataset, [S(cid:48)S].

1 , S).φ1(f ρ

1 , S; f π

We would like to learn bounds of the form φTR(a, S(cid:48)) for ﬂexible DPPs of the kind in
Eq. 2 (unlike the restricted forms of distribution and classiﬁer used in [3,14]). However, the
KL term between two DPPs is typically intractable to evaluate. We thus derive a modiﬁed
variational bound (using techniques from [12]), which upper-bounds Eq. 3 (and thus bounds
the expected risk), while being tractable to optimize:

Theorem 1 (Variational Transfer-Learning Bound). Using the notation above, with
variational distributions represented by DPPs, r1 : (X, Y ) → (Zd)(cid:48) and r2 : Θ0 → (Zd)(cid:48), the
following forms a valid h-o generalization bound, under the condition S(cid:48) ⊥⊥ S:

a (S(cid:48)) = λ(f ρ
φTL

a(f ρ
φ1
1 , S; r1, r2) = −ES,r1(γ|x,y)[log(f ρ

1 , S). min
r1,r2

1 , S; r1, r2, f π
1 = A(S(cid:48)))
1 (y|x, γ)] + ES[KL(r1(γ|x, y), z1)] +

a(f ρ
φ1

(1/λ)[E
E

[log(f π

z1(γ)f ρ

1 (θ0|γ)[log z1(γ) + log f ρ
1 (θ0))] + log(1/δ) + (λ2/N )],

f ρ
1

1 (θ0|γ) − log r2(γ|θ0)] −

(4)

where we write f1(θ0) for f1(f0 = NNX,Y (.; θ0)); f1(.|γ) for f1[γ/z+
P (sample(f ρ
R(f1) under the same assumptions.

1 (.|γ))(x) = y). Further, we have that φTL

a (S(cid:48))(f ρ

1 ], and f ρ
1 , S) ≥ φTL(S(cid:48))(f ρ

a(f ρ
φ1

Proof. We ﬁrst note that the higher-order bound holds true if we substitute φ1(f ρ
1 , S; r1, r2), where φ1(f ρ
φ1(f ρ

1 , S) is the PAC-Bayes bound from [1]:

1 ) + log(1/δ) + (λ2/N )]

1 , S) + (1/λ)[KL(f ρ

1 , S) = R(f ρ

1 , f π

1 (y|x, γ) =
1 , S) ≥(1−δ)

1 , S) for

(5)

This follows, since the algorithm used to set the prior in Eq. 4 is applied to S(cid:48), and we have
by assumption that S(cid:48) ⊥⊥ S. We can re-express Eq. 5 by splitting the KL-term:

φ1(f ρ

1 , S) = R(f ρ

1 , S) + (1/λ)[−E

1 (f0)[log(f π
f ρ

1 (f0))] − H(f ρ

1 (f0)) + log(1/δ) + (λ2/N )],
(6)

where H(.) is the Shannon entropy. We then note that we can upper-bound the risk R(f ρ
1 , S)
by the negative log-likelihood, which in turn can be upper-bounded by the negative-ELBO,

5

Warrell and Gerstein

introducing the variational conditional distribution r1 : (X, Y ) → (Zd):

R(f ρ

1 ) = Ef1(f0),S[f0(x) (cid:54)= y] ≤ −ES[log(Ef1(f0)[f0(x) = y])]

≤ −ES,r1(γ|x,y)[log(f ρ

1 (y|x, γ)] + ES[KL(r1(γ|x, y), z1)].

(7)

Further, we have the following lower-bound on the entropy introduced in [16]: H(q(x)) ≥
−Eq(x,γ)[log q(γ) + log q(x|γ) − log r(γ|x)]. This can be used to upper-bound the negative
entropy term in Eq. 5, introducing the variational distribution r2 : Θ0 → (Zd)(cid:48):

−H(f ρ

1 (f0)) ≤ E

z1(γ)f ρ

1 (θ0|γ)[log z1(γ) + log f ρ

1 (θ0|γ) − log r2(γ|θ0)].

(8)

Substituting the upper-bounds in Eqs. 7 and 8 into Eq. 5 yields φ1
have φTL

1 , S) ≥ φTL(S(cid:48))(f ρ

1 , S) ≥(1−δ) R(f1).

a (S(cid:48))(f ρ

a in Eq. 4, and hence we

Meta-Learning. For the case of meta-learning, [4] introduce a bound, which can be

expressed in our notation as:

(cid:3)

φ2(f ρ

2 , f ρ,1

1 , f ρ,2

1 ...f ρ,M

1

) = Et[R(f ρ,t
+((KL(f ρ

1 ) + ((KL(f ρ
2 , f π

2 , f π
2 ) + c)/d)1/2

2 ) + KL(f ρ,t

1 , (f π

2 )∗) + a)/b)1/2]

(9)

2 and f ρ

where f π
2 denote a hyper-prior and hyper-posterior respectively (belonging to the type
F2, as in Eq. 2), M , and Nt are the tasks, and training examples for task t respectively, Et[.]
denotes the average as t ranges over tasks, a = log(2M Nm/δ), b = 2(Nm − 1), c = log(2M/δ)
and d = 2(M − 1). We may use Eq. 9 to deﬁne a meta-learning h-o generalization bound:

φML(S1, ..., SM ) = λ(f ρ

A(S1, ..., SM ) = argminf ρ

1 , SM +1).φ1
min
f ρ,1...M
1

2

b (f ρ,M +1
1
φ2(f ρ

, SM +1; f π
1 , f ρ,2

2 , f ρ,1

1 ...f ρ,M

1

1 = sample(A(S1, ..., SM )))

),

(10)

1

1

φ2(A(S1, ..., SM ), f ρ,1...M

where we require that the samples from the training tasks (S1, ..., SM ) are independent of the
test task SM +1, and φ1
b is deﬁned in Appendix B. Additionally, Eq. 10 provides a bound on the
transfer error, i.e. the expected error on a new task: minf1 φML(S1, ..., SM )(f1, SM +1) ≤(1−δ)
minf ρ,1...M
) (see [4], Theorem 2). Again, we would like to learn
bounds of the form φML for ﬂexible DPPs, without the restrictions on distributions used
in [4]; for this purpose, a modiﬁed variational bound can be derived φML
analogously to
Theorem 1 for tractable optimization, which we state in Appendix B. Finally, we note that
Eq. 10 can be simpliﬁed, following [25], by splitting the samples S1...M +1 into training and
testing partitions, and learning/ﬁxing a function V : F2 × (X × Y )N → F1 to generate
the task-speciﬁc classiﬁers using the training sets. The task priors and posteriors are then
) and V (f ρ
set to V (f π
2 )∗) terms to vanish in
Eq. 9 [25], and Strain
M +1 is treated as a further input to the h-o bound. Alternatively, V may
use summary features of the task-samples without creating a train/test partition, and a
diﬀerential privacy penalty added to the bound as used in the single-task setting in [7].

) resp., causing the KL(f ρ,t

2 , Strain
t

2 , Strain
t

1 , (f π

a

6

Higher-Order Generalization Bounds

3.2 Second-order Complexity Bounds

The bounds considered in Sec. 3.1 allow for a prior over classiﬁers to be trained in the case of
transfer-learning, or a hyper-prior in the case of meta-learning, assuming the prior is trained
on separate data S(cid:48) from that used to evaluate the bound S. This can be viewed as a form
of ‘learned complexity’, since the KL-divergence terms in the bounds outlined penalize the
divergence between the posterior and the trained prior, rather than one of generic form, such
as a Gaussian or Minimum Description Length (MDL) [26] prior. However, a more direct way
to introduce a learned complexity term is simply to train an additional model g to predict
the generalization error. In this section, we introduce a class of higher-order bounds based
on this principle (which we call ‘second-order complexity’ bounds). These bounds take as
input a ‘generalization predictor’ g1, and output a bound.

To simplify the analysis, we treat generalization prediction as a classiﬁcation task (the
regression case is discussed below). Hence, we introduce the type G0 = (F0 → {0, 1}) for a
deterministic generalization classiﬁer g0 : G0, and G1 = G(cid:48)
0 for a stochastic classiﬁer g1 : G1.
For a given threshold τ , along with a base classiﬁer of interest f0, g1 will be trained to predict
whether the generalization error of f0 exceeds τ , i.e. (R(f0) − R(f0, S)) > τ . The risk of g1
applied to a stochastic base classiﬁer f1 : F1 for threshold τ , can thus be expressed as the risk
that g1 incorrectly predicts the generalization error of classiﬁer f0 sampled according to f1:

Rτ

f1(g1) =

(cid:88)

f0,g0

f1(f0)g1(g0) [g0(f0) (cid:54)= [(R(f0) − R(f0, S)) > τ ]] .

(11)

f0,g0

f1(f0)g1(g0)· [g0(f0) = 1]. With these deﬁnitions, and letting I τ
f0

Further, we deﬁne P1(g1, f1) as the probability that g1 outputs 1 under f0: P1(g1, f1) =
(cid:80)
(S1, S2) = [(R(f0, S1)−
R(f0, S2)) > τ ] and S(cid:48) = {S(cid:48)
N (cid:48)} be a set of N (cid:48) auxiliary datasets (of arbitrary size)
sampled from D, we have the following h-o bound:

1, ..., S(cid:48)

Theorem 2 (Second-order Complexity Bound). Using the notation above, the following

forms a h-o generalization bound, under the condition S(cid:48) ⊥⊥ (f1, S):

φ2o-cplx(g, τ, S(cid:48)) = λ(f ρ

1 , S).(R(f1, S) + (cid:15)(f1, g1, τ ))

(cid:15)(f1, g1, τ ) = τ + (Rτ

f1(g1, SA) + η(g1) + P1(g1, f1))(1 − τ )
(cid:19)

(cid:19)(cid:19)

(cid:18)

η(g1) =

1
λ

KL(g1, π1) + log

,

(12)

(cid:18) 1
δ

+

(cid:18) λ2
N (cid:48)

where SA is an auxiliary sample, formed by sampling N (cid:48) values of f0 according to f1, i.e.
{f (m)
m, S)). Further, π1 is a ﬁxed prior on
0
g1.

|m = 1...N (cid:48)}, and creating the pairs (f (m)

, I τ
f0

(S(cid:48)

0

Proof. From the above, we have:

Rτ

f1(g0) =

Rτ

f1(g1) =

(cid:88)

f0
(cid:88)

f1(f0) [g0(f0) (cid:54)= [(R(f0) − R(f0, S)) > τ ]] ,

f1(f0)g1(g0) [g0(f0) (cid:54)= [(R(f0) − R(f0, S)) > τ ]] ,

R(f1, S) =

(cid:88)

f1(f0) [f0(x) (cid:54)= y] ,

(13)

f0,g0
1
|S|

f0,(x,y)∈S

7

Warrell and Gerstein

and

P1(g0, f1) =

(cid:88)

f0

f1(f0)[g0(f0) = 1],

P1(g1, f1) =

(cid:88)

f0,g0

f1(f0)g1(g0)[g0(f0) = 1],

(14)

while letting P0(g0, f1) = 1 − P1(g0, f1), P0(g1, f1) = 1 − P1(g1, f1). Next, we observe that
the following holds with probability 1:

R(f1) ≤ R(f1, S) + (cid:15)(cid:48)(f1, g1, τ )

(cid:15)(cid:48)(f1, g1, τ ) = τ + (Rτ

f1(g1) + P1(g1, f1))(1 − τ ).

(15)

We can demonstrate Eq. 15 by the following argument. Observe that, for a given g0, it will
classify f0 as having a generalization error less than τ with probability P0(g0, f1). However,
since its risk of misclassiﬁcation is Rτ
(g0), we can lower-bound the true 0 outputs (true
f1
negatives) by P0(g0, f1) − Rτ
(g0). By deﬁnition, the generalization error of these true
f1
negatives is less that τ , and the generalization error in all other cases cannot be more than 1.
Hence, taking a weighted average, a bound on the generalization error for a given g0 can be
written as:

R(f1) ≤ R(f1, S) + (cid:15)(cid:48)(f1, g0, τ )

(cid:15)(cid:48)(f1, g0, τ ) = (P0(g0, f1) − Rτ

f1(g0)) · τ + (1 − P0(g0, f1) + Rτ
f1(g0)) · τ + (P1(g0, f1) + Rτ

f1(g0)) · 1
f1(g0)) · 1

= (1 − P1(g0, f1) − Rτ
= τ + (Rτ

f1(g0) + P1(g0, f1))(1 − τ ).

(16)

Eq. 15 then follows by taking the expectation of both sides on Eq. 16 across g1 (i.e. g0 ∼ g1)
(note that if P0(g0, f1) − Rτ
(g0) is less than 0, the bound is greater than 1, and hence is
f1
valid vacuously).

Next, we wish to replace Rτ
f1

(g1, SA) as in the theorem,
where SA is the auxiliary sample discussed in Sec. 3.2. To do so, we ﬁrst consider the risk
of g1 not with respect to predicting the true generalization error (Eq. 13), but rather on a
further sample, S† of size M . We write this:

(g1) with an empirical estimate Rτ
f1

Rτ

f1(g1, S†) =

(cid:88)

f0,g0

f1(f0)g1(g0)

(cid:104)

(cid:105)
g0(f0) (cid:54)= [(R(f0, S†) − R(f0, S)) > τ ]

,

(17)

We can then apply the PAC Bayes bound from [1] to the empirical estimate of the risk of g1
on the auxiliary sample, where we assume each of the auxiliary datasets S(cid:48)
2, ... has size
M :

1, S(cid:48)

E[Rτ

f1(g1, S†)] ≤δ Rτ

f1(g1, SA) + (1/λ)[KL(g1, π1) + log(1/δ) + (λ2/N (cid:48))]

(18)

where π1 is an arbitrary prior. We note that Eq. 18 requires the assumption (from the
(g1, S†) is deﬁned over the product
theorem) that S(cid:48) ⊥⊥ f1: This is because the true risk Rτ
f1

8

Higher-Order Generalization Bounds

distribution of D (the base-level distribution over (X, Y )) and f1; hence SA will be a sample
from the same distribution iﬀ S(cid:48) ⊥⊥ f1. We then observe that, using Eq. 18:

Rτ

f1(g1, SA) + η(g1),
where η(g1) is as in Eq. 12. Finally, substituting the bound for Rτ
f1
15, results in the h-o bound given in Eq. 12.

f1(g1, S†)] ≤δ Rτ

f1(g1) = E[Rτ

(19)

(g1) in Eq. 19 into Eq.

(cid:3)

We note that the bound in Theorem 2 contains the term P1(g1, f1), which requires an
estimate of the probability the stochastic classiﬁer g1 will return 1 under inputs from f1 (Eq.
14). However, this quantity does not depend on external data (i.e. either S or S(cid:48)), and hence
it can be made arbitrarily precise by repeatedly drawing samples g0 and f0 from g1 and f1 resp.
and observing g0(f0), allowing the bound to be evaluated to arbitrary accuracy. Further, the
bound may be used during training in a number of distinct ways. Most directly, a stochastic
classiﬁer f1 may be trained on S and then ﬁxed; Eq. 12 may then be optimized, leading to the
bound R(f1) ≤(1−δ) φ2o-cplx(g∗
1, τ ∗) = argmin(g1,τ )φ2o-cplx(g, τ, S(cid:48))(f1, S).
Alternatively, a (small) set of stochastic classiﬁers may be considered, F (for instance,
those generated over an optimization path when training f1 on S); the bound may then be
optimized to pick the ﬁnal classiﬁer: f ∗
1 = argminf1∈F min(g1,τ ) φ2o-cplx(g, τ, S(cid:48))(f1, S), after
applying a union bound. Another possibility is to directly optimize the bound over (f1, g1, τ ),
while applying a diﬀerential privacy transformation to f1 when calculating (cid:15)(f1, g1, τ ), to
approximately enforce S(cid:48) ⊥⊥ f1. This results in the following optimization problem:

1, τ ∗, S(cid:48)), where (g∗

(f ∗

1 , g∗

1, τ ∗) = argmin(g1,τ )φ2o-cplx(g, τ, S(cid:48))(h(f1), S),

(20)

where h(.) is a privacy preserving transformation. A generic form for h(.) is given below:

(cid:32)

h(f1) = λf0.

exp(β log f1(f0))

exp(β log f1(f0))

(cid:80)

f0

(cid:33)

,

(21)

which increases the entropy of f1 according to the ‘temperature’ β. The bound in Th. 2
may then be modiﬁed by incorporating an additional diﬀerential privacy term, as in [7].
Additionally, we note that while Th. 2 uses a generalization error classiﬁer, g1, equally we
may consider the case of a 2-o complexity bound based on a generalization error regressor
greg
1 would be trained directly to predict (R(f0) − R(f0, S)).
1
Investigation of the analogous bound for optimizing (f1, greg

: (F0 → R)(cid:48). Here, greg

1 ) is left to future work.

Finally, we may form a meta-learning analogue of φ2o-cplx (see Appendix C):
Theorem 3 (Second-order Complexity ML-Bound). With notation as in Theorem 2,

and assuming (S1, ...SM , S(cid:48)
M +1) ⊥⊥ (f1, SM +1), we have the h-o bound:
φ2o-cplx-ML(S1:M , Af ) = λ(f1, SM +1).φ2o-cplx(g = sample(Ag(S1:M )), τ, S(cid:48)

1, ...S(cid:48)

M +1)(f1, SM +1)

Ag(S1:M ) = argming2

(cid:18)

η(g2) =

1
λ

Et[φ2o-cplx(g = sample(g2), τ, S(cid:48)
(cid:19)

(cid:19)(cid:19)

(cid:18) 1
δ

+

(cid:18) λ2
M

KL(g2, π2) + log

t)(Af (St), St)] + η(g2)

,

(22)

where g2 : G2 = G(cid:48)
samples, S(cid:48)
sample(g∗

t,1:N (cid:48)
t
2), τ, S(cid:48)

1, Af : (X × Y ) → F1, and each task t has its own auxiliary data
. Further, a bound on the transfer error is provided by Et[φ2o-cplx(g =
t)(Af (St), St)] + η(g2), where g∗

2 = Ag(S1, ..., SM ).

9

Warrell and Gerstein

Figure 1: Synthetic experiments. (A) and (B) show example data and results on single- and
multi-task synthetic generalization tasks respectively. The left panel shows example
data from a task in each case. The middle panel compares the test classiﬁcation
error and generalization bound predicted by a single-level variational PAC-Bayes
bound (φ1
a, Eq. 4). The third panel similarly compares test error and bound
for the second-order complexity bound (φ2o-cplx, Eq. 12) and DPP meta-learning
bound (φML, Eq. 10) for top and bottom respectively.

4. Results

4.1 Synthetic Experiments

We ﬁrst design a single-task classiﬁcation experiment, to compare the performance of the
single-level variational PAC-Bayes (φ1
a, Eq. 4) bound against the second-order complexity
bound (φ2o-cplx, Eq. 12). We create 10 synthetic datasets with 100 testing, validation and
training points each as illustrated in Fig. 1A. We use a generative process for the data
involving ﬁrst, sampling 20 ‘prototype’ points, each sampled from a standard 2-d Gaussian,
and labeled 0/1 such that (a) the ﬁrst two prototypes are 0 and 1 resp., (b) prototypes 3-20
are labeled by ﬁnding the nearest neighbor from the previous prototypes, and ﬂipping this
label with a 0.1 probability. The train/validation/test data points are then sampled likewise
from the 2-d standard Gaussian, and labeled according to their nearest prototype (while
balancing each set to contain 50 points from each class). This process is designed to generate
synthetic data with a complex decision boundary, which has structure on multiple scales. We
a and φ2o-cplx to predict generalization performance on
evaluate the ability of the bounds φ1
60 networks trained on these 10 datasets, where for each dataset we add a varying amount of
label noise, corresponding to 0, 20, 40, 60, 80 and 100% of the labels being ﬂipped. For φ1
a, we
learn a stochastic classiﬁer f ρ
1 using Eq. 4 on the training partition, after pre-training a prior
1 on the validation partition using the ELBO bound [12]. For φ2o-cplx, we train f1 directly
f π
on the training partition using the ELBO bound, and optimize φ2o-cplx for (g1, τ ) by using

10

Higher-Order Generalization Bounds

the validation partition to construct an auxiliary dataset with N (cid:48) = 20 bootstrapped samples.
For the second-order generalization classiﬁer g, we use 6 network features as predictors: the
(cid:96)1 and (cid:96)2-norms of the weights, the likelihood, log-likelihood and entropy of the stochastic
classiﬁer on the training set, and the path-norm [13] of the weights. In all cases, we use
networks with 2 hidden layers of 5 units each, a 2-d latent space, set σ = 0.1, λ = 10, δ = 0.05.
The results in Fig. 1A show that both bounds are able to predict generalization performance.
The traditional PAC-Bayes bound bound (φ1
a) achieves a moderate correlation with the test
error (r = 0.22, p = 0.086), while the second-order complexity bound (φ2o-cplx) achieves a
stronger correlation (r = 0.46, p = 1.9e − 4). Further, the second-order complexity bound is
shown to carry signiﬁcant additional information about the test error versus the training and
validation error alone (p = 0.03 and p = 0.02 resp., 1-tailed ANOVA), while the single-layer
bound is only weakly informative (p > 0.1), suggesting that the φ2o-cplx provides a more
eﬃcient representation for data-driven complexity than a PAC-Bayesian data-dependent
prior (we note that both methods had access to the same training/validation data during
optimization).

Next, we design a multi-task synthetic classiﬁcation experiment, to compare the single-level
and meta-learning variational PAC-Bayes bounds (φML, Eq. 10). Here, we are particularly
interested in the extra ﬂexibility aﬀorded by the modiﬁed DPP versions of these bounds, in
comparison the the restricted forms used in previous work [4,25]. For this purpose, we design
a synthetic dataset, having 33 tasks, each being a binary classiﬁcation problem with 2d input
features, where the inputs fall into 8 Gaussian clusters (σ = 0.1) arranged on the corners
and mid-points of a square around the origin, as shown in Fig. 1B, with 4 being randomly
assigned to classes 0 and 1 on each task. This allows for transfer of information across tasks,
since similar decision boundaries may occur in multiple tasks. For each task, we generate
6 datasets with varying levels of noise added (to permit diﬀerent levels of generalization),
ﬂipping 0, 20, 40, 60, 80 and 100% of the labels, and split the data into training, validation
and testing partitions of N = 15 data-points each. We ﬁrst learn a stochastic classiﬁer f ρ
1
using Eq. 4 on the validation partition, after pre-training a prior f π
1 on the training partition
using the ELBO bound [12]. Fig. 1B plots the test error against the bound, which are
signiﬁcantly correlated (r = 0.2, p = 0.008). Further, a regression of the test error on the
training error and bound show the bound to be moderately informative (p = 0.1, 1-tailed
ANOVA). We then use the DPP meta-learning bound (φML) to learn classiﬁers f ρ
1 for each
task, while simultaneously ﬁtting a hyper-posterior f ρ
2 to groups of 3 tasks at a time (using
the validation sets only). Fig. 1B shows this approach is able to achieve a better correlation
between the bound and test error (r = 0.7, p = 2e−30), and that the bound carries signiﬁcant
additional information about the test error versus the training error alone (p = 0.01, 1-tailed
ANOVA), showing that the meta-learning approach is able to share information between
tasks. We compare against the model of [4], in which the priors, and hyper-posterior/prior are
restricted to be Gaussian in form, which achieves signiﬁcantly lower test performance across
tasks (p = 0.015, 1-tailed t-test, 0.53 vs 0.56 mean accuracy), showing the ﬂexibility aﬀorded
by the DPP formulation to be beneﬁcial. Network hyper-parameters were set identically to
the single-task setting.

11

Warrell and Gerstein

Figure 2: Psychiatric genomics expression data. (A) shows results for transfer-learning on
genomics data, where prior and posterior are trained to identify diﬀerent psychiatric
conditions vs controls (Eq. 3), and (B) compares meta-learning performance on
genomics data using a restricted (Gaussian-based, [4]) and full DPP-based model.
(C) compares model-selection performance (10 models) using single-level PAC-
Bayes (φ1
a, Eq. 4) and second-order complexity bounds (φ2o-cplx). Panels 1-2 show
normalized bound values (scaled by mean) ordered by true model rank on test
data, while panel 3 compares ranks 1-5 with ranks 6-10 across disorders for each
bound. Error bars show quartiles.

4.2 Modeling psychiatric genomics expression data.

We further test our approach on psychiatric genomics data from the PsychENCODE project
[23], consisting of gene expression (RNA-Seq) levels from post-mortem prefrontal cortex
samples of control, schizophrenia (SCZ), bipolar (BDP) and autistic (ASD) subjects. We
create datasets balanced for cases and controls (and covariates, see [23]) for each disorder,
with 710, 188 and 62 subjects respectively, from which we create 10 training, validation and
testing partitions (approx. 0.45/0.45/0.1 split). For each data split, we select the 5 most
discriminative genes for each disorder using the training partitions to create a 15-d input
space; the network hyper-parameters and bound optimized are identical to the synthetic
experiments. We ﬁrst test the ability of our approach to perform transfer learning, by
training priors f π
1 on each of the training partitions (via an ELBO objective), before training
a posterior stochastic classiﬁer f ρ
1 using Eq. 4 on the validation data (via optimizing Eq. 4);
in doing so, we test all combinations of disorders when learning priors and posteriors. The
results in Fig. 2A show that both SCZ and ASD models are able to use the information in
the prior to improve generalization. The SCZ results are particularly interesting, in that the
priors trained on all 3 disorders are able to improve the baseline model; the improvements
for the SCZ and BPD priors here are signiﬁcant (p = 0.006 and p = 0.026 respectively,

12

Higher-Order Generalization Bounds

1-tailed t-test). In the ASD case, only the ASD prior improves performance, while for BPD,
no improvement is gained. We note that the SCZ dataset is substantially larger than the
other disorders’, which may aﬀect the results. We also compare against a model with a
Gaussian prior, observing lower performance across models (p = 9.9e − 3, 1-tailed t-test,
0.57 vs 0.59 mean accuracy). We then test the framework in the meta-learning setting,
by optimizing DPP meta-learning bound (φML, Eq. 10) for each of the 10 data-splits on all
tasks (SCZ, BPD, ASD classiﬁcation) simultaneously. As in the synthetic setting, we also
train a model in which the priors, and hyper-posterior/prior are restricted to be Gaussian
in form, replicating the setting of [4]. Fig. 2B shows that the DPP meta-learning model is
able to achieve better test performance overall (p = 0.13, 1-tailed t-test), particularly by
improving prediction on the BPD and ASD tasks. We note that, in general, the performance
of the models in Fig. 2B is slightly lower than 2A, since we used a limited subset of the
data in training the former (56 samples each) in order to balance the data across tasks. In
general, the results of the transfer and meta-learning tasks point to a shared etiology of
psychiatric conditions, as has been highlighted recently [2,9]. Finally, we also compare the
ability of φ1
a and φ2o-cplx to predict generalization performance across models trained on
each of the 10 data-splits for each disorder, hence performing model selection, using the
same (non-transfer) setting as for the synthetic data (Fig. 1A, i.e. training and validation
are from the same disorder). Fig. 2C shows how the bound varies with the actual test-set
ranking for each disorder. The traditional (data-dependent) PAC-Bayes bound achieves
a moderate correlation (r = 0.30, p = 0.11, panel 1), while the second-order complexity
bound is notably stronger (r = 0.42, p = 0.02, panel 2), suggesting, as in the synthetic
results, that the second-order complexity bound provides a more eﬃcient representation for
data-driven complexity. Further, comparing bound values for ranks 1-5 versus 6-10 across
disorders reveals a more signiﬁcant separation for φ2o-cplx than φ1
a (p = 0.023 versus p = 0.077
respectively, see Fig. 2C panel 3).

5. Discussion

We have introduced a framework for deriving higher-order generalization bounds in a DPP
context, and have shown that these lead to eﬃcient variational objectives for training DPPs,
as well as allowing novel generalization bounds to be derived. Particularly, we show that a
second-order complexity bound we introduce outperforms traditional PAC-Bayes bounds in
predicting generalization and model selection on synthetic and genomics tasks. Our results
suggest a number of future directions. First, as discussed, it is straightforward to include
task-based features in the DPP framework to conditionalize the higher-order bounds for
transfer- and meta-learning settings. Further, we note that second-order complexity bound
φ2o-cplx may naturally be modiﬁed by incorporating diﬀerential-privacy constraints (following
[7]) to mitigate the independence requirements of the bound, and allow joint training of f1 and
g1; additionally, the generalization classiﬁer g1 may itself be a more complex program, such
as a data-dependent compression algorithm, hence forming a second-order analogue of the
MDL bound in [26]. Potentially, exploring forms of φ2o-cplx incorporating diﬀerential-privacy
and MDL priors oﬀers the possibility of deriving tight absolute bounds on generalization as
in [26] (here, we have focused on looser bounds as training objectives, and their empirical
correlation with test-set generalization). Finally, we note that while we have assumed a

13

Warrell and Gerstein

discrete setting for formalizing DPPs, our framework may naturally be lifted to a continuous
setting, for instance with a denotational semantics based on Quasi-Borel spaces [11,18], while
incorporating the distinction between distributional and thunked types from [20]. More
generally, analogues of our bounds may be formulated in distinct probabilistic programming
paradigms (for instance, using a factor-graph semantics as in [5]), motivating novel training
algorithms based on principled objectives.

References

[1] Alquier, P., Ridgway, J., & Chopin, N. (2016). On the properties of variational approxi-
mations of Gibbs posteriors. Journal of Machine Learning Research, 17(1), 8374-8414.
[2] Anttila, V., Bulik-Sullivan, B., Finucane, H. K., Walters, R. K., Bras, J., Duncan, L.,
... & Neale, B. (2018). Analysis of shared heritability in common disorders of the brain.
Science, 360(6395), eaap8757.
[3] Ambroladze, A., Parrado-Hernández, E., & Shawe-taylor, J. S. (2007). Tighter PAC-Bayes
bounds. In Advances in neural information processing systems (pp. 9-16).
[4] Amit, R. and Meir, R., 2017. Meta-learning by adjusting priors based on extended
PAC-Bayes theory. arXiv preprint arXiv:1711.01244.
[5] Borgström, J., Gordon, A.D., Greenberg, M., Margetson, J. and Van Gael, J., 2011, March.
Measure transformer semantics for Bayesian machine learning. In European symposium on
programming (pp. 77-96). Springer, Berlin, Heidelberg.
[6] Dieng, A. B., Tran, D., Ranganath, R., Paisley, J., & Blei, D. (2017). Variational Inference
via χ Upper Bound Minimization. In Advances in Neural Information Processing Systems
(pp. 2732-2741).
[7] Dziugaite, G. K., & Roy, D. M. (2018). Data-dependent PAC-Bayes priors via diﬀerential
privacy. In Advances in Neural Information Processing Systems (pp. 8430-8441).
[8] Dziugaite, G.K. and Roy, D.M., 2017. Entropy-SGD optimizes the prior of a PAC-Bayes
bound: Generalization properties of Entropy-SGD and data-dependent priors. arXiv preprint
arXiv:1712.09376.
[9] Gandal, M. J., Haney, J. R., Parikshak, N. N., Leppa, V., Ramaswami, G., Hartl, C., ... &
Geschwind, D. (2018). Shared molecular neuropathology across major psychiatric disorders
parallels polygenic overlap. Science, 359(6376), 693-697.
[10] Goodman, N., Mansinghka, V., Roy, D. M., Bonawitz, K., & Tenenbaum, J. B. (2012).
Church: a language for generative models. arXiv preprint arXiv:1206.3255.
[11] Heunen, C., Kammar, O., Staton, S. and Yang, H., 2017, June. A convenient category
for higher-order probability theory. In 2017 32nd Annual ACM/IEEE Symposium on Logic
in Computer Science (LICS) (pp. 1-12). IEEE.
[12] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114.
[13] Neyshabur, B., Salakhutdinov, R.R. and Srebro, N., 2015. Path-sgd: Path-normalized
optimization in deep neural networks. In Advances in Neural Information Processing Systems
(pp. 2422-2430).
[14] Parrado-Hernández, E., Ambroladze, A., Shawe-Taylor, J., & Sun, S. (2012). PAC-
Bayes bounds with data dependent priors. Journal of Machine Learning Research, 13(Dec),
3507-3531.

14

Higher-Order Generalization Bounds

[15] Pentina, A. and Lampert, C., 2014. A PAC-Bayesian bound for lifelong learning. In
International Conference on Machine Learning (pp. 991-999).
[16] Ranganath, R., Tran, D., & Blei, D. (2016, June). Hierarchical variational models. In
International Conference on Machine Learning (pp. 324-333).
[17] Rivasplata, O., Szepesvari, C., Shawe-Taylor, J. S., Parrado-Hernandez, E., & Sun, S.
(2018). PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Advances
in Neural Information Processing Systems (pp. 9214-9224).
[18] Ścibior, A., Kammar, O., Vákár, M., Staton, S., Yang, H., Cai, Y., Ostermann, K.,
Moss, S.K., Heunen, C. and Ghahramani, Z., 2017. Denotational validation of higher-order
Bayesian inference. Proceedings of the ACM on Programming Languages, 2(POPL), p.60.
[19] Sobolev, A. and Vetrov, D., 2019. Importance Weighted Hierarchical Variational Inference.
arXiv preprint arXiv:1905.03290.
[20] Staton, S., Wood, F., Yang, H., Heunen, C. and Kammar, O., 2016, July. Semantics
for probabilistic programming: higher-order functions, continuous distributions, and soft
constraints. In 2016 31st Annual ACM/IEEE Symposium on Logic in Computer Science
(LICS) (pp. 1-10). IEEE.
[21] Tran, D., Hoﬀman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., & Blei, D. M. (2017).
Deep probabilistic programming. arXiv preprint arXiv:1701.03757.
[22] Tran, D., Hoﬀman, M. W., Moore, D., Suter, C., Vasudevan, S., & Radul, A. (2018).
Simple, distributed, and accelerated probabilistic programming. In Advances in Neural
Information Processing Systems (pp. 7598-7609).
[23] Wang, D., Liu, S., Warrell, J., Won, H., Shi, X., Navarro, F. C., ... & Gerstein, M.
(2018). Comprehensive functional genomic resource and integrative model for the human
brain. Science, 362(6420), eaat8464.
[24] Warrell J., & Gerstein M. (2018) Dependent Type Networks: A Probabilistic Logic
via the Curry-Howard Correspondence in a System of Probabilistic Dependent Types. In
Uncertainty in Artiﬁcial Intelligence, Workshop on Uncertainty in Deep Learning.
http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-19.pdf
[25] Yin, M., Tucker, G., Zhou, M., Levine, S. and Finn, C., 2019. Meta-Learning without
Memorization. ICLR, 2020.
[26] Zhou, W., Veitch, V., Austern, M., Adams, R. P., & Orbanz, P. (2018). Non-vacuous
generalization bounds at the imagenet scale: a PAC-bayesian compression approach. arXiv
preprint arXiv:1804.05862.

15

Warrell and Gerstein

Appendices

Appendix A. Stochastic Type System

In Sec. 2 of the main paper, we describe a stochastic type system based on the higher-order
formal language for probabilistic programs stated in [20], with several key diﬀerences which
we describe below in detail. Formally, we use the following syntax for types:

A, B ::= R | P(A) | 1 | A × B |

Ai | A → B.

(cid:88)

i

(23)

Unlike [20], where A, B are measurable spaces, we will not use the measure structure on
these spaces, and hence they can be arbitrary. R may be interpreted as the continuous
reals, or for convenience a discrete representation of the reals to a ﬁxed level of precision as
suggested in the main paper. Further, the constructor P(A) is taken to represent not the
type of probability measures over A as in [20], but instead the type of ﬁnite normalized mass
functions over A, where a mass function over a set X is deﬁned as in [18], as a function
µ : X → R+ for which there exists a ﬁnite set F ⊆ X such that µ is 0 outside F , and a
normalized mass function is one that sums to 1 over all (deterministic) values of a type.
Hence, for p : P(A), we may write p(a) for the mass assigned to a : A by p, unlike in [20]
where p requires a measurable set U ⊆ A as an argument. Further, we write A(cid:48) as a synonym
for P(A). The other constructions in Eq. 23 are standard (the unit type, product, sum and
function types).

We follow [20] in distinguishing between deterministic and probabilistic typing judgements,
written Γ (cid:96)d t : A and Γ (cid:96)p t : A respectively, where Γ = {a1 : A1, a2 : A2, ...} is a context of
paired term-type assignments. As in [20], we include the standard constructors/destructors
for sum, product and function types (see [20] Secs. 3 and 6), and the probabilistic constructor
for sampling, which from Γ (cid:96)d t : P(A) allows us to derive Γ (cid:96)p sample(t) : A. Like [20], we
include primitives in the language for standard functions and distributions, for instance, in
our case using N(.) and NN(.) to denote normal distributions and neural networks, as in the
main paper (noting that, since our system is based on normalized mass functions, N(.) must
be a discretized and bounded approximation to a normal distribution, such as one whose
support includes only values with a ﬁxed level of precision within a range determined by
the CDF). In addition, we allow probabilistically typed terms to be assigned to P(.) types
through ‘thunking’: Hence, from Γ (cid:96)p t : A we can derive Γ (cid:96)d thunk(t) : P(A), with the
proviso that t reduces probabilistically only to a ﬁnite number of terms in A (hence ensuring
the thunked expression represents a valid ﬁnite normalized mass function; this will be ensured
if all probabilistically typed terms are constructed from primitive sample(.) statements). As
stated in the paper, we then require that the following is true: (thunk(a))(a(cid:48)) = P (a →β a(cid:48)),
where →β is probabilistic beta-reduction (discussed below). Our approach to thunking here
diﬀers from [20], where a separate type constructor is introduced for thunked types (T(A)).
The approach in [20] allows thunking to interact with other features of the language (scoring
and normalization, based on the side eﬀects of running a) which we do not use; hence to
simplify the presentation, we use a compact language in which the constructor T(A) is not
used. We note that we do not require a special form of equality for P(A) types: only terms
of P(A) which reduce to the same normal form are regarded as equal (intensional equality),
and hence there may be many representations for the same ﬁnite normalized mass function

16

Higher-Order Generalization Bounds

(e.g. thunked and non-thunked expressions, or alternative (non-)thunked expressions) which
are semantically equivalent, but intensionally non-equal. Further, our use of ﬁnite normalized
mass functions for P(.) means that this constructor can be applied to function types A → B
and other probabilistic types; hence we may form P(A → B) and P(P(A)) = A(cid:48)(cid:48) (noting
that the latter has support over a ﬁnite number of normal forms in P(A), which may include
thunked and non-thunked values). This is unlike [20], where the absence of a measure on
A → B, P(A) and T(A) prevents the constructors P(A) and T(A) being applied recursively.
Finally, we note that we assume an operational semantics which is equivalent to that
outlined in [20] (Secs. 5 and 7) to deﬁne the stochastic reduction relation between terms
(notated above as probabilistic beta-reduction, →β). For our system, we restrict the semantics
outlined in [20] to ﬁnite discrete probability measures (as denoted by our P(A) type), replacing
measurable sets with deterministic values, and the T(.) and force(.) constructions with P(.)
and sample(.) constructions as detailed above. Following [20], the resulting operational
semantics requires that only deterministic values can be substituted into function bodies:
hence (λ(x : A).B)(sample(a)) must be ﬁrst reduced to (λx.B)(a1) (for a particular a1),
before being reduced to B[a1/x], and hence if x appears multiple times in B, the occurrences
will receive the same value rather than being independently sampled (enforcing memoization
as in [9]). If independent samples are required, B may be modiﬁed so that the occurrences
of x are labeled diﬀerently, e.g. x1, x2, or a probabilistic/thunked type is used as input, e.g.
(λ(x : A(cid:48)).B[sample(x)/x])(p). We note that, in Eq. 1 from the main paper, we implicitly
required that, for a term f containing multiple sampling statements p+
1 , these samples should
be subject to memoization during evaluation. If we require instead that certain sampling
statements are tied through memoization and others not, the notation in Eq. 1 from the
1 , p+
main paper may be adapted to reﬂect this; hence we may write f (p+
2 ) for a DPP
in which the two p1 arguments are subject to memoization, and f (p+(a)
2 ) where
they require independent sampling. To incorporate this notation, the construction may be
modiﬁed:

1 , p+
, p+(b)
1

, p+

1

f [p+

1

, p+(b)
1 , p+(a)
1
f (−)[a1/p+

, ..., p+
1 , a1a/p+(a)

1

2 , p+(a)

2
, a1b/p+(b)

, ...] = thunk(λ(a1, a1a, a1b, ..., a2, a2a).
1, p∗

2 , a2a/p+(a)

, ..., a2/p+

, ...] (p∗

2

1

1, p∗

1, ..., p∗

2, p∗

2, ...))

Appendix B. Transfer-/Meta-learning Variational Bounds

We provide here further results associated with Sec. 3.1 of the main paper. We ﬁrst restate
the meta-learning bound from [4] in our framework as a higher-order bound (Eq. 6 in the
main paper):

(24)

φML(S1, ..., SM ) = λ(f ρ

A(S1, ..., SM ) = argminf ρ

1 , SM +1).φ1
min
f ρ,1...M
1

2

b (f ρ,M +1
1
φ2(f ρ

, SM +1; f π
1 , f ρ,2

2 , f ρ,1

1 ...f ρ,M

1

2 = A(S1, ..., SM ))

)

(25)

where we deﬁne φ1

b as:

b (f ρ
φ1

1 , S; f π

2 ) = R(f ρ

1 , S) + Ef π

1

((KL(f ρ

2 , f π

2 ) + KL(f ρ,t

1 , f π

1 = sample(f π

2 )) + a)/b)1/2

(26)

17

Warrell and Gerstein

where a = log(2N/δ), b = 2(N − 1). We now derive a variational analogue of Eq. 25 based
on the techniques used in Theorem 1 above:

Theorem 4 (Variational Meta-Learning Bound). Using the notation above, with vari-
ational distributions represented by DPPs, r1 : (X, Y ) → (Zd)(cid:48), r2 : Θ1 → (Zd)(cid:48) and
r3 : Θ1 → (Zd)(cid:48), the following forms a valid h-o generalization bound, under the condition
(S1, ..., SM ) ⊥⊥ S:

φML(S1, ..., SM ) = λ(f ρ

1 , SM +1).φ1

A(S1, ..., SM ) = argminf ρ

b (f ρ,M +1
1
min

, SM +1; f π
b (f ρ
φ2

2 , f ρ,1...M

1

, r1...3)

2 = A(S1, ..., SM ))

b (f ρ
φ2

2 , f ρ,1...M

1

2

f ρ,1...M
1

,r1,r2,r3
, r1...3) = Et[−ESt,r1(γ|x,y)[log(f ρ,t
2 ) + E
[log(f π

((KL(cid:48)(f ρ

Ef π
log r2(γ|θ0)] − E
((KL(cid:48)(f ρ

2 , f π

1 ∼f π
2

f ρ,t
1
2 ) + c)/d)1/2
2 , f π
2 (θ1|γ)[log z2(γ) + log f ρ
2 (θ1))]
f ρ
2

[log(f π

log r3(γ|θ1)] − E

KL(cid:48)(f ρ

2 , f π

2 ) = E

z2(γ)f ρ

1 (y|x, γ)] + ESt[KL(r1(γ|x, y), z1)] +

1 (θ0|γ)[log z1(γ) + log f ρ,t

1 (θ0|γ) −

z1(γ)f ρ,t
1 (θ0))] + a)/b)1/2] +

2 (θ1|γ) −

(27)

where we use the same notational shorthands as in Theorem 1, a = log(2M Nm/(δ/(2M ))),
b = 2(Nm − 1), c = log(M/(δ/2)) and d = 2(M − 1).

Proof. Similarly to Theorem 1, we ﬁrst expand out the KL terms in Eq. 5 from the main
paper into cross-entropy and negative entropy terms. We then upper-bound the Gibbs risk
with the ELBO bound as in Theorem 1 (introducing r1), and upper-bound the negative
entropy terms using the bound H(q(x)) ≥ −Eq(x,γ)[log q(γ) + log q(x|γ) − log r(γ|x)] from
[16]. For this purpose, we introduce variational distribution r2 for the H(f ρ,t
1 ) terms, and r3
for the H(f ρ
2 ) terms. Eq. 27 then results from substituting the upper-bounds on these terms
into Eq. 5 from the main paper.

(cid:3)

We note that a Th. 4 shares the variational distributions r1 and r2 between tasks; a
tighter bound may be derived by introducing distributions r1,1...M and r2,1...M to allow these
distributions to vary by task (or by including the task variable as an additional input, hence
amortizing the family of distributions).

Appendix C. Second-order Complexity Bounds

Below, we provide a proof of Theorem 3 from Sec. 3.2 of the main paper:

Theorem 3 (Second-order Complexity ML-Bound). With notation as in Theorem 2,

and assuming (S1, ...SM , S(cid:48)

1, ...S(cid:48)

M +1) ⊥⊥ (f1, SM +1), we have the h-o bound:

φ2o-cplx-ML(S1:M , Af ) = λ(f1, SM +1).φ2o-cplx(g = sample(Ag(S1:M )), τ, S(cid:48)

M +1)(f1, SM +1)

Ag(S1:M ) = argming2

(cid:18)

η(g2) =

1
λ

Et[φ2o-cplx(g = sample(g2), τ, S(cid:48)
(cid:19)

(cid:19)(cid:19)

(cid:18) 1
δ

+

(cid:18) λ2
M

t)(Af (St), St)] + η(g2)

,

(28)

KL(g2, π2) + log

18

Higher-Order Generalization Bounds

where g2 : G2 = G(cid:48)
samples, S(cid:48)
sample(g∗

t,1:N (cid:48)
t
2), τ, S(cid:48)

1, Af : (X × Y ) → F1, and each task t has its own auxiliary data
. Further, a bound on the transfer error is provided by Et[φ2o-cplx(g =
t)(Af (St), St)] + η(g2), where g∗

2 = Ag(S1, ..., SM ).

Proof. We ﬁrst note that, since the conditions of the theorem ensure S(cid:48)

M +1 ⊥⊥ (f1, SM +1),
the bound returned by Eq. 28 for a new task satisﬁes the independence conditions of Theorem
2, and hence by the proof of Th. 2 it forms a valid h-o generalization bound. For the transfer
error bound in Th. 3, we note that each term φ2o-cplx(g = sample(g∗
t)(Af (St), St)] is a
valid bound on the task-speciﬁc risk for task t, since f1,t is chosen by a predeﬁned algorithm
Af (St), which ensures that S(cid:48)
t ⊥⊥ (f1,t). We can view these bounds themselves as a random
variable, with one observation for each of the M tasks. Hence, applying Eq. 5 results in the
bound on the transfer error noted in the theorem, i.e. the value φ2o-cplx takes on a new task,
when g1 is sampled according to g∗
2, and f1 is set using Af (SM +1) is with probability (1 − δ)
less than Et[φ2o-cplx(g = sample(g∗
2), τ, S(cid:48)
t)(Af (St), St)] + η(g2). By the union bound, this
holds with probability 1 − 2δ, since it requires both that φ2o-cplx returns a value bounded by
this quantity on the new task, and that the true risk on the new task does not exceed the
returned value.

2), τ, S(cid:48)

The bounds in Theorems 3 and 4 use stochastic generalization classiﬁers, with the types
0. For completeness, below we give the explicit

g0 : G0 = (F0 → {0, 1}), g1 : G(cid:48)
forms of these classiﬁers used in the experimentation:

0 and g2 : G(cid:48)(cid:48)

(cid:3)

g0 = NNF0,{0,1}(.; θ0)
g1 = NNF0,{0,1}(.; NNZd,Θ0(z+
g2 = NNF0,{0,1}(.; NNZd,Θ0(z++

1 ; θ1) + e+
1 )
; NNZd,Θ1(z+

1

2 , θ2) + e+

2 ) + e++

1

),

(29)

where the parameter spaces Θ0, Θ1 are as in Eq. 2 from the main paper. In practice, the
input to g0 is a parameter vector θ0 : Θ0; we only need consider f0’s which can be represented
in the form NNX,Y (.; θ0) in deﬁning g0, since f1, as deﬁned in Eq. 2 from the main paper,
returns classiﬁers only of this kind (the output on other members of F0 can be set arbitrarily).

19

