Sparse multiresolution representations with adaptive
kernels

Maria Peifer, Luiz F. O. Chamon, Santiago Paternain, and Alejandro Ribeiro

1

9
1
0
2

y
a
M
7

]
P
S
.
s
s
e
e
[

1
v
7
9
7
2
0
.
5
0
9
1
:
v
i
X
r
a

Abstract—Reproducing kernel Hilbert spaces (RKHSs) are
key elements of many non-parametric tools successfully used
in signal processing, statistics, and machine learning. In this
work, we aim to address three issues of the classical RKHS-
based techniques. First, they require the RKHS to be known a
priori, which is unrealistic in many applications. Furthermore,
the choice of RKHS affects the shape and smoothness of the
solution, thus impacting its performance. Second, RKHSs are ill-
equipped to deal with heterogeneous degrees of smoothness, i.e.,
with functions that are smooth in some parts of their domain but
vary rapidly in others. Finally, the computational complexity of
evaluating the solution of these methods grows with the number
of data points, rendering these techniques infeasible for many
applications. Though kernel learning, local kernel adaptation,
and sparsity have been used to address these issues, many of these
approaches are computationally intensive or forgo optimality
guarantees. We tackle these problems by leveraging a novel
integral representation of functions in RKHSs that allows for
arbitrary centers and different kernels at each center. To address
the complexity issues, we then write the function estimation
problem as a sparse functional program that explicitly minimizes
the support of the representation leading to low complexity
solutions. Despite their non-convexity and inﬁnite dimensionality,
we show these problems can be solved exactly and efﬁciently
by leveraging duality, and we illustrate this new approach in
simulated and real data.

Index Terms—multikernel learning, RKHS

I. INTRODUCTION

Reproducing kernel Hilbert spaces (RKHSs) are at the core
of non-parametric techniques in signal processing, statistics,
and machine learning [3]–[9]. Their success stems from the
fact that they combine the versatility of functional spaces
with the tractability of parametric methods. Indeed, despite the
richness of functions found in RKHSs, they can be represented
as a (possibly inﬁnite) linear combination of simple basis
functions called reproducing kernels (or Mercer kernels) [4],
[7]. For smooth functions, however, a celebrated variational
result known as the representer theorem states that the number
of basis is ﬁnite and that
they are given by the kernels
evaluated at the data points [10], [11]. In other words, learning
smooth functions in RKHSs is effectively a parametric, ﬁnite
dimensional problem. Despite their success, RKHS learning
methods have two major practical drawbacks: (i) the RKHS
must be known a priori and (ii) evaluating the learned function
can be computationally prohibitive.

Department of Electrical

e-mail: mariaop@seas.upenn.edu (contact

and Systems Engineering, University of
Pennsylvania.
author),
{luizf,spater,aribeiro}@seas.upenn.edu. Their work is par-
tially supported by the National Science Foundation CCF 1717120 and ARO
W911NF1710438.

Part of the results in this paper previously appeared in [1], [2].

Indeed, the appropriate functional space of the solution
is seldom known in practice. Moreover, since the RKHS
dictates the shape and smoothness properties of its functions,
its choice is application-speciﬁc and ultimately affects the
learning performance [5], [12]–[15]. Although there exist
classes of kernels (and thus RKHSs) that can approximate
continuous function arbitrarily well [16], they may need a large
number of data points to do so. In fact, it is well-known that
RKHS methods are not sample efﬁcient for learning functions
with varying degrees of smoothness, i.e., functions that are
smooth in some parts of their domain but vary rapidly in
others [17].

is known,

Kernel learning approaches have been proposed to address
this issue by ﬁtting a combination of kernels from a predeﬁned
set [12], [16], [18] or by using spectral representations of
positive-deﬁnite functions [19]–[21]. Even when the general
form of the kernel
the choice of smoothness
parameter remains (e.g., selecting the bandwidth of Gaus-
sian kernels). In this case, common approaches include grid
search with cross-validation [13], [22] or application-speciﬁc
heuristic such as maximizing the margin of the support-vectors
machine (SVM) [14], [15]. These methods, however, quickly
become impractical as they often must search over ﬁne grids,
use a large set of kernels, or require additional data. They
also fail to address the issue of estimating functions with
heterogeneous degrees of smoothness. Although this can be
done by choosing different RKHSs for different regions of
the domain by means of plug-in rules [23], binary optimiza-
tion [24], hypothesis testing [25], or gradient descent and
alternating minimization [26], [27], these solutions come with
no optimality guarantee due to the non-convexity of these
locally adapted smoothness formulations.

The issue of computational complexity stems directly from
the use of the representer theorem [10], [11], [28]. Despite
the reduction from the inﬁnite dimensional problem of es-
timating smooth functions in RKHSs to that of estimating
a ﬁnite number of parameters, solving of the problem can
become computationally expensive because the number of
parameters is proportional to the number of data points. Thus,
evaluating the function at any point requires as many kernel
evaluations as training samples, which in many applications is
prohibitively high. This issue is often addressed by imposing
a sparsity penalty on the coefﬁcients to reduce the number of
kernel evaluations. Greedy heuristics [29], [30] and ℓ1-norm
relaxations [31]–[34] are then often used to cope with the
combinatorial nature of this problem, which is known to be
NP-hard in general [35], [36]. These methods, however, often
implicitly rely on the classical representer theorems [10], [11],

 
 
 
 
 
 
2

despite the fact that they no longer hold in the presence of
sparsity penalties (see Remark 1). Hence, even if the sparse
optimization program could be solved exactly, the solution
would remain suboptimal with respect to the original function
estimation problem.

In this work, we propose to tackle these issues by simultane-
ously (i) adapting the kernels (RKHSs) locally and (ii) allow-
ing arbitrary centers instead of constraining the kernels to be
evaluated at the training samples. To do so, we put forward an
integral representation of functions that lie in the sum space of
an uncountable number of RKHSs taken from a parametrized
family. This representation does not assume that the kernels
are evaluated at any particular points of the domain. Enforcing
sparsity on the coefﬁcients of this representation allows us
to determine the optimal parameter and center of each kernel
locally. Moreover, sparsity has the added beneﬁt of minimizing
the number of kernels used. Despite the non-convex (sparsity)
and inﬁnite dimensional (functional) nature of the resulting
optimization problem, we can leverage the results from [37]
to solve it exactly and efﬁciently using duality. These results
also give rise to a new sparse representer theorem.

The paper is structured as follows: Section II gives an
overview of reproducing kernel Hilbert spaces and deﬁnes the
problems associated with learning in these spaces. In section
III we formulate the problem using an integral representation
of a function in an RKHS. In Section IV we show the solution
to our problem in the dual domain, prove that the duality
gap between the primal and the dual problem is zero, and
present our algorithm. Numerical examples are then used to
illustrate the effectiveness of this method at locally identifying
the correct kernel parameters as well as the kernel centers in
different applications (Section V).

X ⊂

∈
X →

R in the RKHS

II. LEARNING IN RKHSS: THE CLASSICAL PROBLEM
Given a training set of data pairs (xi, yi), i = 1, . . . , N ,
where xi
are the observations or independent variables,
∈ X
R is the label or dependent
Rp compact, and yi
with
variable, we seek a function f :
H0 that
ﬁts these data, i.e., such that c(f (xi), yi) is small for some
convex ﬁgure of merit c, e.g., quadratic loss, hinge loss, or lo-
gistic log-likelihood. Formally, an RKHS is a complete, linear
function space endowed with a unique reproducing kernel. A
R is a positive-semideﬁnite
reproducing kernel k : Rp
Rp
→
×
iH0 = f (x) for any
, x)
function with the property
), k(
f (
h
·
·
Rp, where
∈ H0 and point x
·iH0 denotes the
function f
h·
inner product of the Hilbert space
H0 [7]. Reproducing kernels
are often parametrized by a constant that characterizes the
smoothness/richness of the RKHS, such as the bandwidth of
sincs, the order of polynomial kernels, or the scale/variance of
Gaussian (radial basis function, RBF) kernels [3], [4]. Just as
each RKHS has a unique kernel, each kernel deﬁnes a unique
RKHS. Explicitly, every function f
∈ H0 can be written as
the pointwise limit of a linear combination of kernels, i.e.,
n

∈

,

f (x) = lim
n→∞

ajk(x, zj ; w0),

(1)

where w0 denotes the kernel parameter and the zj
called the kernel centers. In other words, the RKHS

∈ X

are
H0 is

j=1
X

i=1
X

the completion of the space span
[38,
{
Sec. 2]. Note that different parameters w0 result in different
RKHSs.

, z ; w0)

∈ X }

k(
·

z

|

There are inﬁnitely many representations of the form (1)
that can interpolate a ﬁnite set of points. To avoid overﬁtting
the data and obtain a unique solution, a smoothness prior is
often used by minimizing the RKHS norm of the solution [3],
[4] as in

minimize
f ∈H0
subject to

f

kH0
k
c(f (xi), yi)

0,

i = 1, . . . , N .

≤

(PI)

Despite its inﬁnite dimensional nature, this optimization prob-
lem admits a solution written as a ﬁnite linear combination
of kernels centered at the sample points, i.e., there exists a
solution f ⋆ of (PI) of the form [10], [11]

N

f ⋆(
·

) =

a⋆
i k(
·

, xi ; w0).

(2)

This celebrated variational result is known as the representer
theorem and lies at the core of the success of RKHS methods
by reducing the functional (PI) to the ﬁnite dimensional

N

N

aiajk(xi, xj ; w0)

minimize
{ai}∈R

j=1
X
subject to c(ˆyi, yi)

i=1
X

0,

≤

i = 1, . . . , N ,
N

(PI′)

ˆyi = f (xi) =

ajk(xi, xj ; w0).

j=1
X

Their success notwithstanding, RKHS-based methods have
limitations that can hinder their use in practice. Firstly, the
H0) must be chosen a priori
kernel function k (i.e., the RKHS
and given that it determines the shape of the solution [see (2)],
its choice directly affects the method performance. Even
when the form of k is known, selecting the smoothness
parameter w0 in (1) can be challenging without application-
speciﬁc prior knowledge. Secondly, functions RKHSs have
limited capability to ﬁt functions with heterogeneous degrees
of smoothness, i.e., functions that vary slowly in some re-
gions of their domains and rapidly in others [17]. Although
adapting k or the parameter w0 in (2) has been proposed to
address this issue, (PI′) then becomes a non-convex program,
foregoing efﬁcient solutions and/or optimality guarantees. Fi-
nally, although the representer theorem allows us to use the
ﬁnite dimensional (PI′) to solve the functional (PI), note that
evaluating the solution (2) requires as many kernel evaluations
as observations. Hence, the complexity of the representation
grows with the sample size, which is infeasible for large data
sets. Moreover, even if ﬁnding a sparse set of coefﬁcients aj
in (PI′) were tractable, which it is not [35], [36], the repre-
sentation in (2) does not hold in the presence of a sparsity
regularization. In fact, there often exist more parsimonious
representations that ﬁt the samples as well as any solution
of (PI′) (see Remark 1).

To overcome these issues, the next section puts forward an
integral parametrization of functions in RKHSs. This repre-
sentation can be used to pose problems that can locally adapt

1

0.8

0.6

0.4

0.2

0

0

3

true function
sample points
true kernel center

of kernels, i.e., they need not belong to the same family.
Explicitly, we write

f (x) = lim
n→∞

n

j=1
X

ajk(x, zj ; wj ).

(4)

1

2

3

4

5

Figure 1. Illustration of Remark 1 on the importance of kernel centers
for model complexity.

not only the kernel centers, but also the kernel itself (i.e.,
the RKHS). Moreover, it allows for regularizations beyond
smoothness, most notably sparsity. Although the resulting op-
timization programs are non-convex and inﬁnite dimensional,
we show they have zero duality gap and can therefore be
solved efﬁciently and exactly using conventional methods,
such as (stochastic) (sub)gradient descent. This also allows
us to derive a new sparse representer theorem.

Remark 1. When seeking parsimonious representations of
functions in RKHSs, i.e., representations as in (1) for which
the aj are sparse, the classical representer theorems leading
to (2) do not apply. To see this is the case, take the counter-
example illustrated in Figure 1. Let the sample points be taken
from an underlying function composed of a single Gaussian
kernel, namely

yi = exp

(xi

2.5)2
−
2

(cid:21)

−

(cid:20)

,

i = 1, . . . , N ,

(3)

= 2.5 for all i. What is more, assume that we
where xi
H0, i.e., that the kernel function
know the correct RKHS
z)2/2
. Then, it is
in (1) and (2) is k(x, z) = exp
H0 that ﬁts the
clear that the most parsimonious function in
(cid:2)
(cid:3)
data is f ′(
. However, f ′ is not in the
·
feasible set of (PI′). Hence, though it can ﬁnd functions with
the same approximation error, they will not be the simplest
representation, as illustrated in Figure 1.

(·−2.5)2
2

) = exp

−
h

(x

−

−

i

The function f in (4) now lies in the sum space of a countable
∞
j are deﬁned
number of RKHSs
j=1 H
by the parameters wj. Note that taking wj = w0 for all j
L
in (4) recovers the representation (1) of a function in

j, where the

H

H

=

Similar formulations have been proposed to deal with the
aforementioned issues, although optimally selecting wj [is-
sue (i)–(ii)] and zj [issue (iii)] remains an open problem [29]–
[34]. Our second step is therefore to tackle this hurdle by
introducing an integral counterpart of (4). Explicitly, we deﬁne
the function

H0.

) =

h(
·

ZX ×W

α(z, w)k(
·

, z ; w)dzdw,

(5)

X × W

is a compact subset of R and α :

R
where
W
is in L2(
). Notice that, in contrast to (1) and (4), the
expression in (5) does not depend on a choice of centers or
kernel parameters, thus addressing issues (i) and (ii). Before
proceeding, we show that (4) and (5) are essentially equivalent,
i.e., that (5) can essentially represent the functions in

X × W →

.

H

Proposition 1. Let k be a continuous reproducing kernel,
, z;
i.e., k(
) is continuous over the compact set
·
for each z
sequence
pointwise.

. Then, for each f
of functions as in (5) such that hm

X × W
p there exists a
f

∈ X
}

∈ H

hm

→

{

·

|

x
|

< 1/m] and note that rm(x)

Proof. Consider the approximation of the identity rm(x) =
m I [
δ(x) weakly in
ϕ(0) for all ϕ
the vague topology, i.e.,
p be
continuous and
D
written as f (
) =
) =
·
X ×W αm(z, w)k(
·
n
R

D rm(x)ϕ(x)
compact [39]. Now let f
R
n
j=1 ajk(
·
, z ; w)dzdw with
P

∈ H
, zj ; wj) and take hm(
·

→

→

p

αm(z, w) =

ajrm(w

wj)

−

j=1
X

k=1
Y

rm([z]k

[zj]k),

(6)

−

where [z]k indicates the k-th element of the vector z. Note
that αm
L2, so that hm is indeed of the form (5). Since
the reproducing kernel is continuous, it readily holds from (6)
(cid:4)
that gm

f pointwise for all x

∈

.

→

∈ X

III. LEARNING AN INTEGRAL REPRESENTATION INSTEAD

In the previous section, we have argued that classical
RKHS-based methods suffer from three main drawbacks:
(i) the RKHS must be ﬁxed a priori, (ii) they are suboptimal
for functions with heterogeneous degrees of smoothness, and
(iii) the computational complexity of evaluating solutions
is proportional to the sample size. Our ﬁrst step towards
addressing these issues is to extend (1) by allowing different
RKHSs at different centers. Throughout this work, we assume
that the reproducing kernel family k is ﬁxed (e.g., Gaussian
kernels) and adapt its smoothness parameter (e.g., bandwidth).
Nevertheless, all results hold for any parametrized dictionary

H

H

Proposition 1 shows that there is no loss in using (5) since
it can essentially represent all functions in the sum space
of
interest. Though this result is straightforward when (5) deﬁnes
or if α allowed distributions (Dirac
the inner product in
deltas), the former is typically not the case (except for sinc
kernels) and the latter does not hold since α
L2. Also,
it would appear that this integral representation did nothing
but aggravate the computational complexity problems [(iii)],
now that
is more,
the coefﬁcients α are functions. What
estimating α is once again a functional problem. In the
sequel, we address the computational complexity issue by
explicitly minimizing the support of α using sparse functional
optimization programs (SFPs). In Section IV, we then show

∈

6
4

that these optimization problems can be solved efﬁciently
and exactly using duality, providing an explicit algorithm to
compute α.

A. A Sparse Functional Formulation

Proposition 1 suggests that the centers and kernel param-
eters can be obtained by leveraging sparsity. Indeed, notice
from (6) that functions in
admit an integral representa-
tion (5) in which α is a superposition of bump functions
centered around (zj , wj), i.e., a function that vanishes over
most of its domain. In other words, functions in
admit
a sparse integral parametrization. This observation motivates
estimating f , equivalently α, using the following SFP:

H

H

minimize
α∈L2(X ×W)
subject to

2
L2 + γ

1
α
2 k
k
c(ˆyi, yi)

≤

L0

α
k
i = 1, . . . , N ,

k
0,

ˆyi = f (xi) =

α(z, w)k(xi, z ; w)dzdw,

ZX ×W

(PII)
where γ
0 is a regularization factor that trades-off smooth-
≥
L2 denotes the L2-norm, which induces
ness and sparsity;
smoothness, enhances robustness to noise, and improves the
numerical properties of the optimization problem; and
L0
denotes the “L0-norm,” deﬁned as the measure of the support
of a function, i.e.,

k · k

k · k

L0 =

I [α(z, w)

= 0] dzdw,

(7)

α
k

k

ZX ×W
= 0] = 1 if x

Problem (PII) seeks the function f

where I[x
= 0 and zero otherwise. Notice
that (7) is the functional counterpart of the discrete “ℓ0-norm”.
with the sparsest
integral representation that ﬁts the data according to the convex
loss c, which is both non-convex and inﬁnite dimensional. As it
is, it therefore appears to be intractable. Before addressing this
matter, however, we argue that its solutions would indeed cope
with the statistical and computational issues of classical RKHS
methods. In Section IV, we then derive efﬁcient algorithms to
obtain these solutions.

∈ H

To be sure,

(PII) precludes the choice of a speciﬁc
RKHS [issue (i)] or kernel centers by leveraging the integral
representation (5). Simultaneously, it enables the solution to
locally adapt the RKHS over the domain to account for func-
tions with heterogeneous degrees of smoothness [issue (ii)].
Finally, the sparse solutions of (PII) can be used to obtain low
,
complexity representations of functions in the sum space
H
i.e., representations with a small number of kernels [issue (iii)].
Intuitively, we can obtain a ﬁnite series from the integral
representation (5) by using the bumps in α to determine the
pair (zj, wj ) that deﬁne (4), as suggested by (6). The aj can
then be obtained by directly minimizing c.

Although the remainder of this work studies the gen-
eral (PII), two particular cases are of marked interest. First, the
functional space of the solution is sometimes dictated by the
application. For instance, one may seek bandlimited functions

of a speciﬁc bandwidth. In this case, the reproducing kernel k
and its parameter w0 are ﬁxed and (PII) reduces to

minimize
α∈L2(X )
subject to

2
L2 + γ

1
α
2 k
k
c(ˆyi, yi)

≤

L0

α
k
i = 1, . . . , N ,

(PII′)

k
0,

ˆyi = f (xi) =

α(z)k(xi, z ; w0)dz.

ZX
Problem (PII′) sets to ﬁnd solutions that use as few kernels
as possible and its often tackled using greedy methods such
as KOMP [30]. Despite its success, its applications either
implicitly relies on the representer theorem [9], which does
not hold for sparse problems (see Remark 1), or use a grid
search over the space, which can quickly become prohibitive.
In contrast, the solution of (PII′) is guaranteed to provide
the sparsest integral representation. These can then be ap-
proximated using the aforementioned peak ﬁnding method
to yield low complexity, discrete solutions. Problem (PII′)
can therefore obtain solutions with the same cost and lower
complexity, as illustrated in Section V.

Second, a set of candidate kernel centers might be available
a priori, given by domain experts or unsupervised learning
techniques such as clustering. Problem (PII′′) can then be
used to optimally select a subset of these centers as well as
determine the appropriate RKHS. Explicitly,

minimize
α∈L2(W)

subject to c(ˆyi, yi)

M

j=1 (cid:20)
X

1
2 k

αj

k

2
L2 + γ

αj

k

L0

k

(cid:21)
i = 1, . . . , N ,
M

0,

≤

ˆyi = f (xi) =

αj(w)k(xi, zj ; w)dw,

j=1 ZW
X

(PII′′)
where zj , for j = 1, . . . , M , are the predeﬁned candidate
centers. Observe that (PII′′) promotes the sparsity of each αj,
so that the coefﬁcient of those centers that do not contribute
to satisfy the ﬁt constraint will vanish. Hence, the solution
of (PII′′) effectively selects the smallest subset of candidate
centers. Furthermore, by locally adapting the RKHS, it can
further reduce the number of kernels (centers) in the ﬁnal
solution by using less kernels to ﬁt smoother portions of
the data. The formulation (PII′′) is particularly attractive for
high dimensional problems for which evaluating the integrals
in (PII) may be challenging.

In the next section, we derive a method for solving (PII)
exactly and efﬁciently. Solutions to problems (PII′) and (PII′′)
were presented in [2] and [1] respectively. We do so by formu-
lating the dual problem of (PII) and showing that its solution
can be used to obtain an optimal α⋆ (strong duality). This then
allows us to propose efﬁcient solutions for (PII) by means of
its dual problem. This strong duality result is also exploited
to derive a new integral representer theorem (Corollary 2) that
accounts for sparsity.

IV. LEARNING IN THE DUAL DOMAIN
Having argued that (PII) [or (PII′)–(PII′′)] is worth solving,
we now return to the issue of how. To understand the challenge

6
6
6
of directly tackling (PII), observe that it is a non-convex, inﬁ-
nite dimensional optimization program. Moreover, its discrete
version is in general NP-hard to solve [35]. In the sequel,
we address these hurdles using duality. It is worth noting that
duality is an established approach to solve semi-inﬁnite convex
programs [40]–[42]. Indeed, dual problems are convex and
their dimension is equal to the number of constraints. Thus,
they can be solved efﬁciently. Moreover, it is well-known
that when the original problem is convex, its solutions can
be obtained from solutions of the dual problem under mild
conditions [43]. Nevertheless, this is not the case of (PII).

In the sequel, we derive a method to solve (PII) by ﬁrst
obtaining its dual problem in closed-form (Section IV-A).
Then, we show that strong duality holds (Section IV-B).
Consequently, solutions of (PII) can be obtained from solutions
of its dual problem. We then conclude by showing how to
efﬁciently solve the latter (Section IV-C).

A. The dual problem of (PII)

To derive the dual problem of (PII), start by introducing the
RN , associated with its equality
∈
RN
+ , associated with its inequality

Lagrange multipliers λ
constraints and µ
∈
constraints. Its Lagrangian is then deﬁned as

(α, ˆy, λ, µ) =

L

1
2 k
N

α
k

2
L2 + γ

α
k

k

L0

5

is the value a convex optimization problem, since c is convex
and µi

0, and

≥

α(α, λ) =

L

1
2

Z (cid:20)

−

α2(z, w) + γ I [α(z, w)

= 0]

N

λiα(z, w)k(xi, z ; w)

#

dzdw,

(12)

i=1
X

where we used the integral deﬁnitions of the “L0-norm” in (7)
and the L2-norm. Hence, evaluating g involves solving a
non-convex functional optimization problem similar to the
original (PII). Here, however, we can exploit separability to
obtain the closed-form thresholding solution presented in the
following proposition.

Proposition 2. A minimizer αd of (12) is given by

αd(z, w; λ) =

¯αd(z, w; λ),
0,

(

¯αd(z, w; λ)
|
|
otherwise

> √2γ

(13)

where ¯αd(z, w; λ) =

N
i=1 λik(xi, z; w).

Proof. To obtain (13), we start by separating the objective
of (12) across z and w. To do so, we leverage the following
lemma:

P

Lemma 1. Let F (α, x) be a normal integrand, i.e., continuous
in α for ﬁxed x and measurable in x for ﬁxed α. Then,

λi

α(z, w)k(xi, z ; w)dzdw

i=1
X
N

Z

N

(8)

inf
α∈L2

Z

F (α(x), x)dx =

Proof. See [44, Thm. 3A].

λi ˆyi +

µic(ˆyi, yi).

Taking

−

+

inf
¯α∈R

Z

F (¯α, x)dx.

(14)

(cid:4)

i=1
X

i=1
X
For conciseness, we omit the set
over which the
integrals are computed. To proceed, obtain the dual function
by minimizing the Lagrangian as in

X × W

g(λ, µ) = min
α∈L2
ˆyi∈R

(α, ˆy, λ, µ).

L

(9)

The dual function is the objective of the dual problem, deﬁned
explicitly as

maximize
λ∈RN , µ∈RN
+

g(λ, µ).

(DII)

Notice that the dual function (9) is concave regardless of
the convexity of the original problem since it is the minimum
over a set of afﬁne functions in (λ, µ). What
is more,
despite the inﬁnite dimensionality of the original problem, it is
deﬁned over 2N variables. Hence, (DII) is a ﬁnite dimensional
convex program, which can be solved efﬁciently as long as its
objective can be evaluated efﬁciently.

Yet, computing g involves a functional, non-convex prob-

lem. Indeed, notice that (9) can be separated as

g(λ, µ) = min

ˆyi∈R Lˆy(ˆy, λ, µ) + min

α∈L2 L

α(α, λ),

(10)

where

F (¯α, z, w) =

¯α2
2

+ γ I [¯α

= 0]

−

N

λik(xi, z ; w)¯α,

(15)

i=1
X

in Lemma 1, yields that minimizing (12) is equivalent to
minimizing F individually for each (z, w). Although still
non-convex, this problem is now scalar and admits a simple
solution.

Indeed, notice that the indicator function in (15) can only
take two values depending on ¯α. Hence, its optimal value is
the minimum of two cases: (i) if ¯α = 0, then F (0, z, w) = 0
for all (z, w); alternatively, (ii) if ¯α
= 0, then (15) becomes

F ′(¯α, z, w) =

¯α2
2 −

i=1
X

N

λik(xi, z ; w)¯α + γ,

(16)

whose minimization is a quadratic problem with closed-form
solution

¯α⋆(z, w) = argmin

¯α∈R

N

F ′(¯α, z, w) =

λik(xi, z ; w),

(17)

so that min ¯α6=0 F (¯α⋆, z, w) = γ
αd(z, w) = ¯α⋆(z, w) if γ
vanishes, which yields (13).

−

−

¯α⋆(z, w)2/2. Immediately,
¯αd(z, w)2/2 < 0 or α(z, w)
(cid:4)

i=1
X

Lˆy(ˆy, λ, µ) =

N

i=1
X

N

µic(ˆyi, yi) +

λi ˆyi

(11)

i=1
X

Hence, despite the non-convexity and inﬁnite dimensionality
nature of (12), it admits an explicit solution in the form of the
thresholded function (13). We can thus evaluate the convex

6
6
6
6

objective of (DII), that can then be solved using classical
convex optimization tools such as (stochastic) (sub)gradient
the question remains of
ascent (see Section IV-C). Still,
whether this is a worthwhile endeavor. Indeed, (9) can be
interpreted as a relaxation of the hard constraints in (PII), so
that (DII) provides a lower bound on its optimal value. Though
we can evaluate (13) on the solution (λ⋆, µ⋆) of (DII), there
are no guarantees that it is a solution of (PII). We tackle this
challenge in the sequel.

B. Strong duality and the integral representer theorem

Due to the non-convexity of the original problem, the only
immediate guarantee we can give about the optimal value
of (DII) is that it is a lower bound on the optimal value of (PII).
The central technical result of this section shows that strong
duality holds for (PII), i.e., it has null duality gap (Theorem 1).
This result has deep implications for the problems we posed
in Section II. First, it implies that we can obtain a solution
of (PII) by solving its (DII) (Corollary 1). Second, it allows
us to write a representer theorem similar to the original ones
stating that the solution α⋆ of (PII) is a linear combination of
kernels evaluated at the data points.

Let us start with the main theorem:

Theorem 1. Strong duality holds for (PII)
the ker-
, z ; ω) has no point masses and Slater’s condition is
nel k(
·
met. In other words, if P is the optimal solution of (PII) and D
is the optimal solution of (DII), then the duality gap P
D
vanishes.

−

if

Proof. This result can be found in [37]. For ease of reference,
(cid:4)
a proof is provided in Appendix A.

Theorem (1) states that, though it is not a convex pro-
gram, (PII) has null duality gap. Immediately, we obtain the
following corollary:

Corollary 1. Let (λ⋆, µ⋆) be a solution of (DII) and as-
; λ⋆) is a
sume k
solution of (PII) for αd as in (13).

L2 and analytic. Then, α⋆
d(
·

) = αd(
·

∈

·

·

,

,

Proof. See Appendix B.

(cid:4)

Thus, despite the apparent challenges, (PII) is tractable and can
be solved efﬁciently and exactly using duality (as detailed in
Section IV-C). It is worth noting that the technical hypotheses
of Corollary 1 are mild and hold for all commonly used
reproducing kernels, such as Gaussian, polynomial, and sinc
kernels. A fundamental feature of this approach is that it solves
the functional problem without relying on discretizations. This
is of utmost importance since discretizing (PII) can lead to
NP-hard, large dimensional, and potentially ill-conditioned
problems. Moreover, it tackles the sparse problem directly
instead of using convex relaxations.

Another fundamental implication of Theorem 1 is the fol-

lowing integral representer theorem.

Corollary 2 (Integral representer theorem). A solution α⋆
of (PII) can be obtained by thresholding a parametrized family
of functions ¯α⋆
w is the RKHS induced by
w, where
w ∈ H
H
; w). In fact, ¯α⋆
w lives in a ﬁnite dimensional
the kernel k(
,
·
·

∈

N

i=1
X

subspace of
points. Explicitly, there exist ai

H

w spanned by the kernels evaluated at the data

R such that

α⋆(
·

, w) = ¯α⋆

w(
·

) =

aik(xi,

; w).

·

(18)

P

w
(cid:4)

that

∈ H

Recall

d almost everywhere
the corollary stems

; w), so that αw
αw(z)
> √2γ).
|

Proof. From Corollary 1, α⋆ = α⋆
d(z, w) = αd(z, w; λ⋆
with α⋆
i ). Thus,
N
i=1 λ⋆
i k(xi,
from (13) for αw(
) =
·
·
by deﬁnition and α⋆(z, w) = αw(z) I(
|
the classical representer theorem [10], [11]
reduces the functional problem (PI) to the ﬁnite dimensional
problem (PI′) by showing that it has a solution of the form (2)
that lies in the span of the kernels evaluated at the data points.
Likewise, Theorem 2 states that a solution of (PII) can be
obtained from a family of functions with the similar ﬁnite
representation (18). Note, however, that although classical
representer theorems do not account for sparse solution (as
argued in Remark 1), Theorem 2 holds in the presence of the
“L0-norm” regularizer. The cost of doing so is adding a layer
of indirection with respect to the original functional problem:
whereas the classical representer theorem yields a function
in the RKHS directly from the parameter ai
through (2),
Corollary 2 states that the functional parameters of the integral
representation (5) of this function can be obtained from the ai.
Still, the problem it solves is considerably more complex and
comprehensive than (PI).

∈

It is worth noting that although (PII) searches for α

L2,
Corollary 2 shows its solution depends only on a family of
functions belonging to the RKHSs considered. Explicitly, the
solution α⋆(
, w) of (PII) is a thresholded version of a function
·
; w). In partic-
w with reproducing kernel k(
in the RKHS
·
H
ular, when considering the problem without sparsity (γ = 0),
w. In the case of (PII′), this further
we have that α⋆(
, w)
·
simpliﬁes to α⋆
∈ H0.

This observation allows us to think of (5) as building the
function h⋆ point-by-point by integrating the value of partial
L2-inner products between the reproducing kernel of
w and
a function ¯αw
w. To be more speciﬁc, notice that (5) can
∈ H
be written as the iterated integrals

∈ H

H

·

,

h⋆(
·

) =

¯αw(z)k(
·

, z; w)dz
(cid:21)

dw,

ZW (cid:20)ZX

(19)

,

> √2γ

, and
(z, w)

α(z, w)
|

W ⊆ W
{

X × W
∈ X × W | |

where
is the set induced by
X ⊆ X
the support of α⋆, i.e.,
.
}
The innermost integral in (19) can be interpreted as an inner
, z; w) computed only
product in L2 between ¯αw and k(
·
where the magnitude of ¯αw is large enough, deﬁned by the
regularization parameter γ. This sort of trimmed inner product
is linked to robust projections found in different statistical
methods [45], [46]. The outer integral then accumulates the
projections of ¯αw over the relevant subset
of RKHSs
considered to form the functional solution.

W

it

Before proceeding,

is worth noting that Theorem 1
holds under very mild conditions. Indeed, the reproducing
kernels typically used in applications, such as polynomial or
Gaussian kernels, do not have point masses. In fact, if the
L2, then we need not
function of interest is in L2, i.e.,

H ⊆

αd(zk, wk; λi(t))k(xi, zk; wk)

dµi (λ, µ) = c (ˆyd,i(λ, µ), yi) .

dλi (λ, µ) = ˆyd,i(λ, µ)

−

Z

αd(z, w; λ)k(xi, z; w)dzdw,

(21a)
(21b)

Algorithm 1 Stochastic optimization for (PII)

1: Initialize λi(0) and µi(0) > 0
2: for t = 0, 1, . . . , T do
3:

Evaluate the supergradient dµi (t) = c (ˆyd,i(t), yi) for

ˆyd,i(t) = argmin

ˆyi

N

µi(t)c(ˆyi, yi)

N

λi(t)ˆyi

−

i=1
X
, k = 1, . . . , B, uniformly at random

i=1
X

Draw

(zk, wk)
}
and compute the stochastic supergradient

{

4:

5:

ˆdλi (t) = ˆyd,i(t)

1
B

−

B

k=1
X

Update the dual variables:

λi(t + 1) = λi(t) + ηλ ˆdλi (t)
µi(t + 1) = [µi(t) + ηµdµi (t)]+

6: end for
7: Evaluate the primal solution as

α⋆(z, w) =

¯α⋆(z, w),
0,

(

¯α⋆(z, w)
|
|
otherwise

> √2γ

for ¯α⋆(z, w) =

N
i=1 λi(T )k(xi, z; w)

P

consider kernels containing Dirac deltas since they are not
square integrable. As for Slater’s condition [43], the inﬁnite
dimensionality of α makes it so we can always ﬁnd one that
perfectly interpolates the data, though it may neither be smooth
nor have a sparse representation. Hence, ﬁnding a strictly
feasible solution of (PII) is straightforward for most c.

In the next section, we leverage the closed-form of the dual
function from Proposition 2 and the strong duality result from
Theorem 1 to obtain an explicit algorithm to solve (PII).

C. Dual gradient ascent

Theorem 1 shows that we can obtain a solution of (PII)
through (13). Still, although (13) can be evaluated using the
closed form expression from Proposition 2, it requires the
optimal dual variables (λ⋆, µ⋆). In this section, we propose
a projected supergradient ascent method to solve (DII) (Al-
gorithm 1). Alternatively, other standard convex optimization
algorithms can be used to exploit structure in the solution
of (DII). For instance, efﬁcient solvers based on coordinate
ascent can be leveraged to solve large-scale instances [47].

Start by recalling that a supergradient of a function f :

x) for all y

∈ D ⊆
−

Rn is any vector d such that f (y)

R at x
≤
D →
f (x) + dT (y
. Though supergradients
may not be an ascent direction at x, taking small steps in
its direction decreases the distance to any maximizer of a
convex function f [43]. Thus, we can solve (DII) by repeating,
for t = 0, 1, . . . ,

∈ D

λi(t + 1) = λi(t) + ηλdλi (λi, µi),
µi(t + 1) = [µi(t) + ηµdµi (λi, µi)]+ ,

(20a)

(20b)

7

λ , d(t)

where d(t)
µ are the supergradients of λ and µ, respec-
tively, ηλ, ηµ > 0 are step sizes, and [x]+ = max(0, x). The
projection of µi on the non-negative numbers guarantees that
the constraints of (DII) are satisﬁed. The supergradient in (20)
are readily obtained from the constraint violation of the dual
minimizers [43]. Explicitly, let ˆyd,i(λ, µ) and αd(z, w; λ) be
minimizers of (11) and (12) respectively. Then,

Since ˆyd,i is the solution of the convex optimization prob-
lem (11), the update for the dual variables µi in (20b) can
be efﬁciently evaluated using (21b). The update expression
for λi in (20a), however, requires that the integral in (21a) be
evaluated. To do so, we can either use numerical integration
methods, since αd is available in closed-form from (13), or
rely on Monte Carlo methods. The latter approach is especially
interesting because it can be integrated with the optimization
iterations in (20) to obtain a stochastic supergradient ascent al-
gorithm summarized in Algorithm 1. Since Monte Carlo gives
an unbiased estimate of dλi , typical convergence guarantees
for stochastic optimization apply [48]–[50].

V. NUMERICAL EXPERIMENTS

In the previous sections we have claimed that our algorithm
can estimate (i) kernel widths (PII′′), (ii) kernel centers (PII′),
and (iii) kernels of varying centers and widths (PII). In this
section we show through a sample signal, how we can achieve
claim (i). Then we show how moving from (i) to (iii) reduces
complexity. In our discussion about the complexity of the
representation in section V-A, we show how we can achieve
(ii) on random signals of ﬁxed width. In section V-B, we solve
(PII) for a signal of varying degrees of smoothness and show
how we can reduce complexity regardless of sample size.
Lastly, in sections V-C and V-D we apply our algorithm to
solve (PII) and (PII′′) on two examples of real applications: a
user localization problem and a digit classiﬁcation problem.

For the estimation, we search over functions in the family

of RKHSs, which have Gaussian functions as kernels

k(x, x′) = exp

−k

x′

2

k

,

(22)

x
−
2w2

(cid:26)
where width of the kernel is directly proportional to the hyper-
parameter w.

(cid:27)

To start, the effect of the choice of RKHS on the perfor-
mance of a learning algorithm is examined. To this end, a
signal, which lies in the RKHS with a Gaussian kernel of
width w0 = 0.453 is constructed. The classical problem in
(PI′) is compared to the problem presented in (PII′′). A grid
search is used to examine the performance of (PI′) for different
values of w. The value of w0 was chosen such that it would
not be directly on the grid, since in practice it is unlikely to

8

include the value of the width of the originating signal. We
generate S signals of the form

fj(x) =

m

i=1
X

exp

ai

×

k

x
2

2

˜xi
k
w2
0 (cid:21)

−
∗

−

(cid:20)

+ ξj

(23)

U
N

with j = 1 . . . S. For each fj a training set of N = 50 samples
was generated with m = 10. The amplitude ai of each function
is selected at random from a uniform distribution
(1, 2).
The ˜xi are i.i.d random variables drawn from the uniform
(1, 2) and the ξj are i.i.d. random variables
distribution
(0, 10−3), which represent the noise.
drawn from

U

It should be noted that, given sufﬁcient iterations, well
chosen step sizes, and a large γ, our method can approxi-
mate point masses. However, smoother approximations of the
point masses can be obtained by using only few iterations.
Additionally, these smooth approximations are more robust to
the choice of the tuning parameters. Kernel centers and widths
can subsequently be obtained by selecting the extreme points
of the function α(z, w), since the optimal α(z, w) is a function
of w, the kernel width, and z the kernel centers. Kernels using
the widths and centers approximated from the extreme points
of the α(z, w) are used to train a least squared estimator.

A grid search is performed for problem (PI′) by uniformly
sampling w over the interval [0,1] at 0.1 increments. The
problem (PII′′) is solved using γ = 4000, ηλ = 0.001,
ηµ = 0.1 and T = 5000. The performance of the two
algorithms is compared over 1000 repetitions of the sampled
signal, each with a training set of size N = 100 and a test
set of size Ntest = 1000. The MSE of (PI′) decreases as the
value of w increases—see Figure 2. Due to the non-uniform
sampling of the signal, smoother kernels on average have a
better performance. In areas, in which the sampling is sparse,
the thinner kernels cannot represent the signal between the
samples. Additionally, the thinner kernels are more likely to
overﬁt to the noise than the smoother kernels. However, the
smoother kernels cannot model the faster variation in the signal
well. In contrast (PII′′) ﬁnds a sparse solution, which uses 14
kernels on average, of varying smoothness, with an average
MSE of 0.0457, which can both take advantage of the ability
of smoother kernels to avoid overﬁtting as well as thinner
kernels to model fast variation. Indeed, we observe in Figure
3 that our algorithm chooses a mixture of kernels of width
around 0.453 and kernels of width 1.

Smoother kernels perform better because of the random
sampling combined with the restriction of only using kernels
centered at the sample points. Therefore, we investigate the
effect of solving problem (PII) which ﬁnds both kernel centers
and kernel widths. Problem (PII) is solved using γ = 1000,
ηλ = 0.01, ηµ = 1 and T = 1000 over 1000 randomly
sampled training sets, and results in an MSE of 0.0588.
Although the MSE of (PII) is similar to that of (PII′′), it is
important to note that by placing kernels arbitrarily we are
able to better estimate the width of the kernel: by comparing
Figure 4 to Figure 3 it can be seen that (PII) uses only 1 to 2
kernels per representation of width 1 whereas (PII′′) uses on
average 6 kernels of width 1. Moreover, we consistently obtain

Figure 2. MSE obtained by (PII′′) and (PI′) over 1000 repetitions of
random sampling of the signal in (23). (PI′) is solved over different
values of w over a grid on the interval [0.1, 1]. (PII′′) ﬁnds the
width as part of the algorithm and is presented for comparison with
(PI′). The standard deviation around each mean is plotted in gray
for both (PII′′) and (PI′). The ﬁgure shows that the selection of
the width within the algorithm gives the advantage of a lower mean
generalization error.

Figure 3. Histogram of the widths found using (PII′′) over 1000
repetitions of random sampling of the signal in (23). On average,
14 kernels were selected for the representation of the function out of
which an average of 6 kernels have a width of 1.

Figure 4. Histogram of the widths found using (PII) over 1000
repetitions of random sampling of the signal in (23). On average
a representation had 6 kernels out of which between 1 and 2 kernels
had a width of w = 1 and 4 kernels had a width in the interval
[0.384, 0.648].

representations of lower complexity when solving (PII)—see
Figure 5.

500

400

300

200

100

y
c
n
e
u
q
e
r
F

0

0

(PII'')
(PII)

5

10

15

20

25

Number of kernels

Figure 5. Histogram of the number of kernels in the representation
of the estimated functions by solving problems (PII′′) and (PII). (PII)
achieves a lower complexity representation by moving the centers in
addition to the widths.

A. Examining the Complexity of the Solution

So far we have shown that the complexity of the formulation
can be reduced by moving centers in addition to moving the
width. To further explore the effect of kernel centers on the
complexity of the solution, we compare the performance of
(PII′) to that of kernel orthogonal matching pursuit (KOMP)
with pre-ﬁtting (see [9], [30]), for a simulated signal as in (23).
KOMP takes an initial function and a set of sample points
and tries to estimate it by a parsimonious function of a lower
complexity. As a backwards feature selection metheod, the
algorithm starts by including all samples and then reduces the
complexity of the function by reducing one feature at a time.
The KOMP algorithm in [9], [30] was modiﬁed by changing
the stopping criteria to be the estimation error, rather than the
distance to the original function. This stopping criteria allows
us to compare the sparsity needed to obtain similar estimation
error.

The signal was sampled from the function in (23) us-
ing w0 = 0.5 and m = [5, 10, 20] by generating N =
[2m, 4m, 6m] samples for each function,
thus creating 9
different sample size and signal pairs. The problem in (PII′)
was solved using γ = 30, ηλ = 0.05, ηµ = 0.1 and
T = 1000. Subsequently, a least squares algorithm was trained
using kernels at the location found by our algorithm. Both
our method and KOMP used w = 0.5 as the kernel hyper-
parameter.

The number of kernels needed to obtain the same MSE
is compared over 1000 realizations of each signal between
(PII′) and KOMP. When the number of samples is at least
30, our method is able to ﬁnd a sparser representation 100%
of the times. In the cases with fewer samples the problem is
likely undersampled, such that the estimation of the function
is more difﬁcult. Figure 6 shows two cases in which 20
samples are simulated, where m = 5 and m = 10. In both
cases, our method ﬁnds sparser representations in 99% of the
realizations. When 10 kernels and 5 kernels are superimposed,
our method ﬁnds a representation which is less sparse in only
0.4% and 0.3% of realization respectively. Lastly, when the
signal is a weighted sum of 5 functions and only 10 samples
are generated, our method cannot ﬁnd a sparser solution for

9

3.7% of the realizations.

The generalization MSE was compared between the two
methods for different levels of sparsity. Figure 7 shows the
changes in generalization MSE as the number of kernels used
in the representation increases. 1000 realizations of a signal
with m = 10 and a training set of size N = 100 were used. The
ability of our method to place kernels at any location, beyond
the training set, allows it to achieve signiﬁcantly lower errors
compared to KOMP at any sparsity level. As the number of
kernels used increases, the difference in performance between
the two methods decreases. At approximately 25 kernels the
performance of our method plateaus. Comparatively, KOMP
achieves a plateau when the representation holds 50 kernels.

B. Varying Degrees of Smoothness

In the previous sections we have only considered signals
from functions belonging to an RKHS in the family of RKHSs
with Gaussian kernels. In this section we explore the effect of
sample size on the complexity of the representation and the
MSE on a signal of varying degrees of smoothness. To this
end, a signal of varying smoothness is simulated using the
following equation:

yi = sin(0.5πx2

i ) + ξi

(24)

(0, 10−3) represents the noise.

where ξi

∈ N

−

−

The solution of problem (PII) was compared to destructive
KOMP, with the stopping criteria set to be the desired number
of kernels rather than the distance from the original function.
This stopping criteria allows us to have a fair comparison
between our method and KOMP by using equally sparse
functions. The problem in (PII) was solved using γ = 2,
ηλ = 0.001, ηµ = 30 and T = 1000. Sample sizes of 51, 101,
201, 301, 401, and 501 were created by uniformly sampling
5, 5]. Test sets of 1000 samples randomly
in the interval [
selected on the interval [
5, 5] were created. Using the method
of selecting kernel centers and widths by selecting the peaks of
the function α(z, w), our method ﬁnds a representation with 26
kernels regardless of the sample size. It can be seen in Figure
8 that in addition to the number of kernels being consistent
across all sample sizes the MSE is also consistent for our
method. The MSE of the estimation using KOMP, however,
increases as the sample size grows. The problem of reducing
features is a combinatorial problem which grows exponentially
with the sample size. The backwards approach used by KOMP
is a greedy approach which removes only one kernel at a time.
As the sample size increases, there are more misleading paths
of removal it can take. Additionally, it is only using kernels
placed at the sample points, which means it will need more
kernels when the true kernel is centered between two sample
points.

C. User Localization Problem

In the remainder of this section we will apply our method
to real world application for which the class of functions
the signal belongs to is unknown. We consider the problem
of using RF signals to identify the location of a receiver.
Speciﬁcally, given the Wi-Fi signal strength from seven routers

10

(a)

(b)

Figure 6. Comparison of the complexity of the representation of (PII′) and KOMP for a similar MSE over 1000 realizations. In Figure (a)
5 Gaussian functions were used to simulate the signal. In Figure (b) 10 Gaussian functions were used to simulate the signal. In both cases
(PII′) achieves a lower complexity for 99% of the realizations.

1010

E
S
M

105

100

10-5

0

KOMP
(PII')

20

40

60

80

100

Number of Kernels

Figure 7. Generalization MSE as a function of number of kernels for
KOMP and (PII′) over 1000 realizations of the signal in (23).

KOMP
(PII)

E
S
M

100

0

100

200

300
number of sample points

400

500

600

Figure 8. MSE for varying sample sizes using (PII) and KOMP with
26 kernels over 100 realizations of the signal in (24).

we wish to identify the room in which our receiver is located
[51]. The signal strength varies depending on the location of
the router. The signal was received from 7 routers spread
throughout an ofﬁce building. The data was collected using
an Android device. At each location the signal strength from
each router was observed at 1s intervals. The data was then
categorized into 4 groups, each representing the room in which
the signal strength was observed. All the rooms are on the
same ﬂoor, with the rooms representing the conference room,
the kitchen, the indoor sports room, and work areas [51]. The
goal is to be able to accurately detect the location of the
android device given the measured signal strength.

We use 10-fold cross-validation in order to estimate the
generalization error of our algorithm as was used in [51].
The dataset was split into 10 sets of equal size with equal
distribution of each label. At each turn one of the sets was
used for testing while the others were concatenated and used
to train the algorithm. This multiclass classiﬁcation problem
was solved using the one-vs-one strategy, which required
6 comparisons. The ﬁnal class assignment is made through
voting. Each comparison makes a prediction on the class of a
sample and thus casts a vote for a particular class. The class
with the majority of votes is assigned to the sample. The cost
function was for this classiﬁcation problem is

c(z, y) =

i
X

max

0, 1

{

−

yi ˆyi

} −

ǫ.

(25)

Solving problem (PII) we obtain an average accuracy of 98%,
similar to the performance observed in [51], in which a fuzzy
decision tree algorithm with 50 rules was used to obtain an
accuracy of 96.65%. This result has been observed to be
consistent over increasing values of the sparsity parameter γ.

D. Mnist Digits Classiﬁcation

We use data of handwritten digits from the MNIST data
set [52], which consists of a training set of 60, 000 sample-
label pairs and a testing set of 10, 000 images and labels. Each
sample is a 28-pixel by 28-pixel grayscale image, which was
vectorized to form 784 dimensional features. The labels are
between 0 and 9 and correspond to the digit written. There
are a total of 10 classes.

′

∈

X

X

R784. In order to ﬁnd a set

The number of features is too large to estimate the value
of α(z, w) at every z
over
which α(z, w) is deﬁned, we use k-means with 400 clusters
in (PII′′) is deﬁned as the set of
for each digit. Then
all cluster centers and the cost function in (25) is used. We
then run our algorithm using a one-to-one strategy for multi-
class classiﬁcation and achieve an accuracy of 98.12% for an
average of 788 features per classiﬁcation which is comparable
to the accuracy found using (PI′) using the training set as
kernel centers and (PI′) using the centers found through k-
means. The complexity of the representation can be further

Table I: Classiﬁcation results for (PI′) using the training
samples as kernel centers and using centers selected from k-
means and (PII′′) using the centers selected from k-means

Method
(PI′)
(PI′) with k-means
(PII′′)
(PII′′)
(PII′′)

Number of Kernels per Classiﬁer
12000
800
788
731
53

Accuracy
98.83 %
98.16 %
98.12 %
96.71 %
85.66 %

11

The theoretical results were validated though simulated
signals. We showed that our algorithm ﬁnds kernel centers
and widths which can represent the function. Furthermore, we
showed that our algorithm is able to ﬁnd sparser representa-
tions than KOMP, with the same error. The sparseness of the
representation was shown to be independent of sample size,
which is not true for greedy kernel reduction methods. We also
validated our method on a localization dataset, for which we
were able to reduce the complexity by 86% while maintaining
the high accuracy. Similarly, a sparse kernel representation was
obtained for classifying the digits from the MNIST dataset.

APPENDIX A
PROOF OF THEOREM 1

Proof. In order to show strong duality, it is sufﬁcient to show
that the perturbed function P (ξ) in (26) is convex [53], [54].
Consider the perturbed version of the optimization function

(a)

(b)

Figure 9. Kernel centers obtained by solving (PII) with the highest
value for α(z, w) for each digit. These centers are representative
of the digits, however, are distinct from any of the samples in the
training set.

f0(α)

P (ξ) = min
α,θ
s.t. c(zi, yi)

be reduced, however it comes at the cost of the classiﬁcation
accuracy.

Although the dimensionality of the features in the original
data makes the use of (PII) impractical, we can solve that
problem, by projecting the data into a lower dimensional space
by using principal component analysis (PCA). The formulation
the found kernel centers
in (PII) has the advantage that
can give some intuition about the distribution of the signal.
Particularly, in the case of digits they can describe digits
which are representative of written digits. To illustrate that
we have performed the classiﬁcation of the digits ’0’ and ’1’
using the ﬁrst 3 principal components. The low dimensional
feature set allows us to ﬁnd the x which result in the highest
value for α(w, x). From these points we can reconstruct the
corresponding digits. Figure 9 shows the resulting images.
These are not part of the initial written digit data set but
rather represent an image that is closest to all written digit.
The accuracy of the classiﬁcation is 99.62%.

VI. CONCLUSIONS

In this paper, we have introduced a method for function esti-
mation in RKHS which can model signals of varying degrees
of smoothness. The algorithm ﬁnds a sparse representation
of a function in the sum space of a family of RKHSs, and
determines the kernel parameter for each kernel. Additionally,
due to the sparsity, traditional representer theorems no longer
hold, so our algorithm placed kernels arbitrarily. While the
problem was not convex, a change in the representation of
the function to the integral over the product of the coefﬁcient
function and the kernel function, allowed us to solve the
problem in the dual domain. By leveraging the results on
strong duality, we were able to solve the problem in the dual
domain.

ξi

≤
α(z, w)

·

k(xi, z; w) dw dz.

(26)

zi =

Z

In equation (26) f0(α) represents the objective function
of our original problem: f0(α) = γ
= 0) +
0.5α2(w, x) dw dz. The optimal solution for P (0) is the
solution to the primal problem.

I(α(z, w)

R

Convexity of the perturbed problem can be shown, by
proving that given an arbitrary pair of perturbations ξ1 and ξ2
and the corresponding optimal values P (ξ1) and P (ξ2), for
any β
[0, 1] the solution P (ξβ) has the following property,
where ξβ is deﬁned by ξβ = βξ1 + (1

β)ξ2:

∈

−

P (ξβ)

≤

βP (ξ1) + (1

β)P (ξ2).

−

(27)

In order to prove the convexity of the perturbed problem

we need to introduce the following lemma.

Lemma 2. The set of constraints given by

=

{

B

b, b = f0(α), c(zi, yi) < ξi,

zi =

α(z, w)

Z

k(xi, z; w) dw dz,

i = 1

·

· · ·

N.

}
(28)

is convex.

Proof. Given an arbitrary pair b1, b2 ∈ B
there exists a
,
L2 such that b1 = f0(α1) and
corresponding α1, α2 ∈
b2 = f0(α2). In order to prove the convexity of the set, we
will show that there exists a feasible αβ
L2 such that for
any β

[0, 1]

∈

∈

f0(αβ) = β b1 + (1

β)b2

−

(29)

Let B be the Borel ﬁeld of all possible subsets of
=

, where
is the set of all possible kernel centers and

U

{X × W}

U

6
12

kernel widths and. Let us construct a measure over B, where

B.

V ⊂



(30)

m(

V

) = 

V α1(v)k(v) dv
V α2(v)k(v) dv
= 0) + α2
= 0) + α2

R
V γI(α1(v)
R
V γI(α2(v)


R

R

1(v) dv
2(v) dv





The ﬁrst 2N elements of the measure represent the esti-
mated function of the signal y using α1 and α2 and a subset of
the kernels, where k(v)i = k(X, zv; wv) and v = [zT
v , wv]T .
two elements of m(
) measure the sparsity of
The last
V
functions α1 and α2 over the set
respectively. Two sets are
V
. The measure of the former
of interest, the empty set and
is m(
can be inferred from our
∅
optimization problem.

U
) = 0 and the measure for

U

U



m(

= 

) = 

ˆy1
ˆy2
b1
b2

U α1(v)k(v) dv
U α2(v)k(v) dv
R
U γI(α1(v)
= 0) + 1
2 α2
R
U γI(α2(v)
= 0) + 1
2 α2


R

Lyapunov’s convexity theorem [55] states that a non-atomic
R
measure vector on a Borel ﬁeld is convex. Note that the
representation in (5) allows us to construct the measure with
non-atomic masses and is essential to the proof of strong
duality. Since α does not contain any point masses our measure
m is convex . Therefore, for any β
[0, 1], there exists a set

1(v) dv
2(v) dv

(31)















∈

B such that:

β

V

⊂

m(

V

β) = β m(

U

) + (1

−

β)m(
∅

The measure of the complement of the set
c
β ∩ V

by
∅
the additivity property of measures:

c
β ∪ V

β =

β =

and

U

V

V

) = β m(

)

(32)

U
β, as deﬁned
V
, can be computed, due to

m(

V

c
β) = m(

U

)

−

m(

V

β) = (1

β)m(

U

−

) = m(

V(1−β)).
(33)

We can deﬁne the function αβ from (32) and (33):

αβ(v) =

α1(v) v
α2(v) v

(

β
c
β

∈ V
∈ V

(34)

From this construction of αβ(v) it can be easily seen that
β)f0(α2). Next we will show that

f0(αβ) = βf0(α1) + (1
αβ is feasible. Deﬁne ˆyβ as:

−

ˆyβ =

αβ(v)k(v) dv =

ZU

=

α1(v)k(v) dv +

ZVβ

= β ˆy1 + (1

β)ˆy2

−

α2(v)k(v) dv =

(35)

ZV c

β

Since c(zi, yi) is convex it follows that for any i

c(βzi,1 + (1
−
We can use this property to show that c(zi,β, yi)

βc(zi,1, yi) + (1

β)zi,2, yi)

−

≤

[1, N ],
β)c(zi,2, yi).

∈

ξβ.

≤

c(zi,β, yi)

≤

βc(zi,1, yi) + (1

β)c(zi,2, yi)

−

βξ1 + (1

≤
β)ξ2 = ξβ

≤

−

(36)

Thus it was proven that αβ is also feasible and therefore
(cid:4)

the set of constraints is convex.

Let (α1, z1, ξ1) and (α2, z2, ξ2) be the pair of optimal
solutions to the two perturbed problems P (ξ1) and P (ξ2).
We have shown that there exists a feasible point αβ for the
β)ξ′, which satisﬁes
problem perturbed by ξβ = βξ + (1
β)P (ξ2). Given that it is a feasible
f0(αβ) = P (ξ1) + (1
point the objective function is greater or equal to the solution
of the problem

−

−

βP (ξ1)+(1

−

β)P (ξ2) = f0(αβ)

P (βξ1+(1

−

≥

β)ξ2). (37)

Since the perturbed problem is convex, the original problem

has zero duality gap.

(cid:4)

APPENDIX B
PROOF OF COROLLARY 1
Proof. Theorem 1 implies that any solution (α⋆, ˆy⋆) of (PII)
(α, ˆy, λ⋆, µ⋆) [43].
is such that (α⋆, ˆy⋆)
argminα, ˆyi L
in (8) separates across α and ˆy, we can consider
Since
the minimizations individually to obtain

L

∈

α⋆

∈

argmin
α∈L2 L

α(α, λ⋆),

(38)

L

α from (12). We also know from Proposition 2 that α⋆
for
d
is in the argmin set of (38). In the sequel, we show that it
is (essentially) its only element.

L
d. On
> =
, we know that α⋆

To do so, we construct α⋆ piece by piece by partitioning
α into three disjoint sets depending on the
the integral in
value of α⋆
>
∈ X × W | |
d takes values from (17), the unique
√2γ
minimizer of
α since it stems from the minimization of the
strongly convex function (16). Moreover, our assumptions on
the reproducing kernel together with (17) imply that α⋆
L2
when restricted to

d(z, w)
|

(z, w)

>. Hence,

d ∈

α⋆

A

L

}

{

A

α⋆(z, w) = α⋆

d(z, w),

for (z, w)

>.

(39)

∈ A
α⋆
d(z, w)
,
}
|
α is always non-

{

A

(z, w)

< √2γ

Over the set

∈ X × W | |
L

< =
notice from (12) that the integrand of
negative. What is more, it is always strictly positive unless α
≡
0. This is ready by applying Lemma (1) and the fact that the
minimum of (16) is positive. Thus, from the monotonicity of
the integral operator, α⋆ is again unique and equal to zero
on

<. From (13), so is α⋆

d and we obtain

A

α⋆(z, w) = α⋆

d(z, w) = 0,

for (z, w)

<.

(40)

>

∈

A

X × W

L2 over

Immediately, we have that α⋆

To conclude the proof, observe that m [

∈ A
<.
∪ A
<] =
>
A
m [
], where m denotes the Lebesgue measure. Indeed,
the complement of
< is the set
A= =
>
A
∪ A
∈
d(z, w)
. From our assumption on the
= √2γ
|
X × W | |
}
reproducing kernel,
A= is the set of zeros of a real analytic
function, which are isolated and therefore countable [56]. In
other words, α⋆ and α⋆
d are in the same equivalence class
in L2 since they are equal except perhaps on a set of measure
(cid:4)
zero.

(z, w)

∪ A

α⋆

{

6
6
6
6
13

[28] A. Argyriou, C. A. Micchelli, and M. Pontil, “When is there a representer
theorem? vector versus matrix regularizers,” J. of Mach. Learning
Research, vol. 10, no. Nov, pp. 2507–2529, 2009.

[29] A. J. Smola and B. Sch¨olkopf, “Sparse greedy matrix approximation for

machine learning,” 2000.

[30] P. Vincent and Y. Bengio, “Kernel matching pursuit,” Mach. Learning,

vol. 48, no. 1-3, pp. 165–187, 2002.

[31] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. of
the Royal Stat. Soc.: Series B (Methodological), vol. 58, no. 1, pp. 267–
288, 1996.

[32] G. M. Fung, O. L. Mangasarian, and A. J. Smola, “Minimal kernel
classiﬁers,” J. of Mach. Learning Research, vol. 3, no. Nov, pp. 303–
321, 2002.

[33] C. Jud, N. Mori, and P. C. Cattin, “Sparse kernel machines for discontin-
uous registration and nonstationary regularization,” in Proc. of the IEEE
Conf. on Comput. Vision and Pattern Recognition Workshops, 2016, pp.
9–16.

[34] S. Gao, I. W.-H. Tsang, and L.-T. Chia, “Sparse representation with
kernels,” IEEE Trans. Image Process., vol. 22, no. 2, pp. 423–434, 2013.
[35] B. K. Natarajan, “Sparse approximate solutions to linear systems,” SIAM

J. Computing, vol. 24[2], pp. 227–234, 1995.

[36] E. Amaldi and V. Kann, “On the approximability of minimizing nonzero
variables or unsatisﬁed relations in linear systems,” Theoretical Comput.
Sci., vol. 209, no. 1-2, pp. 237–260, 1998.

[37] L. Chamon, Y. Eldar, and A. Ribeiro, “Strong duality of sparse functional
optimization,” in Int. Conf. on Acoust., Speech and Signal Process.,
2018, http://bit.ly/2zVHJLy.

[38] N. Aronszajn, “Theory of reproducing kernels,” Trans. of the Amer.

Math. Soc., vol. 68, no. 3, pp. 337–404, 1950.
[39] W. Rudin, Functional Analysis. McGraw-Hill, 1991.
[40] A. Shapiro, “On duality theory of convex semi-inﬁnite programming,”

Optimization, vol. 54[6], pp. 535–543, 2006.

[41] G. Tang, B. N. Bhaskar, P. Shah, and B. Recht, “Compressed sensing off
the grid,” IEEE Trans. Inf. Theory, vol. 59[11], pp. 7465–7490, 2013.
[42] E. J. Cand`es and C. Fernandez-Granda, “Towards a mathematical theory
of super-resolution,” Commun. on Pure and Appl. Math., vol. 67[6], pp.
906–956, 2014.

[43] S. Boyd and L. Vandenberghe, Convex optimization.

Cambridge

University Press, 2004.

[44] R. T. Rockafellar, “Integral functionals, normal integrands and measur-
able selections,” in Nonlinear Operators and the Calculus of Variations.
Springer, 1976, pp. 157–207.

[45] Y. Chen, C. Caramanis, and S. Mannor, “Robust sparse regression under

adversarial corruption,” in ICML, 2013, pp. 774–782.

[46] J. Feng, H. Xu, S. Mannor, and S. Yan, “Robust logistic regression and

classiﬁcation,” in NIPS, 2014, pp. 253–261.

[47] D. Bertsekas, Convex optimization algorithms. Athena Scientiﬁc, 2015.
[48] A. Ruszczy´nski and W. Syski, “On convergence of the stochastic
subgradient method with on-line stepsize rules,” J. of Math. Anal. and
Applicat., vol. 114[2], pp. 512–527, 1986.

[49] A. Ribeiro, “Ergodic stochastic optimization algorithms for wireless
communication and networking,” IEEE Trans. Signal Process., vol.
58[12], pp. 6369–6386, 2010.

[50] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization methods for large-

scale machine learning,” 2016, arXiv:1606.04838.

[51] S. J. Narayanan, R. B. Bhatt, and I. Paramasivam, “User localisation
using wireless signal strength-an application for pattern classiﬁcation
using fuzzy decision tree,” Int. J. of Internet Protocol Technology, vol. 9,
no. 2-3, pp. 138–150, 2016.

[52] Y. LeCun, “The mnist database of handwritten digits,” http://yann. lecun.

com/exdb/mnist/, 1998.

[53] R. T. Rockafellar, Convex analysis. Princeton university press, 2015.
[54] A. Shapiro and K. Scheinberg, “Duality and optimality conditions,” in
Handbook of semideﬁnite programming. Springer, 2000, pp. 67–110.
[55] A. Liapounoff, “Sur les fonctions-vecteurs completement additives,”
Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematicheskaya, vol. 4,
no. 6, pp. 465–478, 1940.

[56] S. Krantz and H. Parks, A Primer of Real Analytic Functions.

Birkh¨auser, 2002.

REFERENCES

[1] M. Peifer, C. L. FO, S. Paternain, and A. Ribeiro, “Locally adaptive
kernel estimation using sparse functional programming,” in Asilomar
Conf. Signals, Systems and Comput.

IEEE, 2018, pp. 2022–2026.

[2] ——, “Sparse learning of parsimonious reproducing kernel Hilbert space
IEEE,

models,” in IEEE Int. Conf. Acoust., Speech and Signal Process.
2019.

[3] B. Sch¨olkopf and A. J. Smola, Learning with kernels: support vector
machines, regularization, optimization, and beyond. MIT press, 2001.
Springer,

[4] C. M. Bishop, Pattern recognition and machine learning.

2006.

[5] T. Hofmann, B. Sch¨olkopf, and A. J. Smola, “Kernel methods in machine

learning,” The Ann. of Stat., pp. 1171–1220, 2008.

[6] M. Yuan, T. T. Cai et al., “A reproducing kernel Hilbert space approach
to functional linear regression,” The Ann. of Stat., vol. 38, no. 6, pp.
3412–3444, 2010.

[7] A. Berlinet and C. Thomas-Agnan, Reproducing kernel Hilbert spaces
in probability and statistics. Springer Science & Business Media, 2011.
[8] J. Arenas-Garcia, K. Petersen, G. Camps-Valls, and L. Hansen, “Kernel
multivariate analysis framework for supervised subspace learning: A
tutorial on linear and kernel multivariate methods,” IEEE Signal Process.
Mag., vol. 30[4], pp. 16–29, 2013.

[9] A. Koppel, G. Warnell, E. Stump, and A. Ribeiro, “Parsimonious online
learning with kernels via sparse projections in function space,” in J. of
Mach. Learning Research, January 2019.

[10] G. Kimeldorf and G. Wahba, “Some results on tchebychefﬁan spline
functions,” J. of Math. Anal. and Applicat., vol. 33, no. 1, pp. 82–95,
1971.

[11] B. Sch¨olkopf, R. Herbrich, and A. J. Smola, “A generalized representer
theorem,” in Int. Conf. on Computational Learning Theory. Springer,
2001, pp. 416–426.

[12] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I.
Jordan, “Learning the kernel matrix with semideﬁnite programming,” J.
of Mach. Learning Research, vol. 5, no. Jan, pp. 27–72, 2004.

[13] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-
mization,” J. of Mach. Learning Research, vol. 13, no. Feb, pp. 281–305,
2012.

[14] C.-H. Li, H.-H. Ho, Y.-L. Liu, C.-T. Lin, B.-C. Kuo, and J.-S. Taur, “An
automatic method for selecting the parameter of the normalized kernel
function to support vector machines,” J. Inform. Sci. Eng., vol. 28, pp.
1–15, 2012.

[15] B. Kuo, H. Ho, C. Li, C. Hung, and J. Taur, “A kernel-based feature
selection method for SVM with RBF kernel for hyperspectral image
classiﬁcation,” IEEE J. of Selected Topics in Appl. Earth Observations
and Remote Sensing, vol. 7, no. 1, pp. 317–326, 2014.

[16] C. A. Micchelli and M. Pontil, “Learning the kernel function via
regularization,” J. of Mach. Learning Research, vol. 6, no. Jul, pp. 1099–
1125, 2005.

[17] D. Donoho and I. Johnstone, “Minimax estimation via wavelet shrink-

age,” The Ann. of Stat., vol. 26[3], pp. 879–921, 1998.

[18] M. G¨onen and E. Alpaydın, “Multiple kernel learning algorithms,” J. of
Mach. Learning Research, vol. 12, no. Jul, pp. 2211–2268, 2011.
[19] C. Ong, A. J. Smola, and R. C. Williamson, “Learning the kernel with
hyperkernels,” J. of Mach. Learning Research, vol. 6, pp. 1043–1071,
2005.

[20] A. G. Wilson and R. P. Adams, “Gaussian process kernels for pattern
discovery and extrapolation,” in Int. Conf. on Mach. Learning, 2013, pp.
III–1067–III–1075.

[21] Z. Yang, A. Wilson, A. Smola, and L. Song, “A la carte – Learning fast
kernels,” in Int. Conf. on Artiﬁcial Intell. and Stat., 2015, pp. 1098–1106.
[22] M. Kuhn and K. Johnson, Applied Predictive Modeling. Springer, 2016.
[23] M. Brockmann, T. Gasser, and E. Herrmann, “Locally adaptive band-
width choice for kernel regression estimators,” J. of the Amer. Statistical
Assoc., vol. 88[24], pp. 1302–1309, 1993.

[24] X. Liu, L. Wang, J. Zhang, and J. Yin, “Sample-adaptive multiple kernel
learning,” in AAAI Conf. on Artiﬁcial Intell., 1993, pp. 1975–1981.
[25] A. K. Ghosh, “Kernel discriminant analysis using case-speciﬁc smooth-
ing parameters,” IEEE Trans. Syst., Man, Cybern. B, vol. 38, no. 5, pp.
1413–1418, 2008.

[26] J. Yuan, L. Bo, K. Wang, and T. Yu, “Adaptive spherical Gaussian kernel
in sparse Bayesian learning framework for nonlinear regression,” Expert
Syst. with Applicat., vol. 36, no. 2, Part 2, pp. 3982–3989, 2009.
[27] B. Chen, J. Liang, N. Zheng, and J. C. Pr´ıncipe, “Kernel least mean
square with adaptive kernel size,” Neurocomputing, vol. 191, no. 5, pp.
95–106, 2016.

