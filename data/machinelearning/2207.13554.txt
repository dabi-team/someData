2
2
0
2

l
u
J

7
2

]

C
O
.
h
t
a
m

[

1
v
4
5
5
3
1
.
7
0
2
2
:
v
i
X
r
a

Data-Driven Sample Average Approximation
with Covariate Information∗

Rohit Kannan†1, G¨uzin Bayraksan2, and James R. Luedtke3

1Center for Nonlinear Studies (T-CNLS) and Applied Mathematics & Plasma Physics (T-5),
Los Alamos National Laboratory, Los Alamos, NM, USA. E-mail: rohitk@alum.mit.edu
2Department of Integrated Systems Engineering, The Ohio State University, Columbus, OH, USA.
E-mail: bayraksan.1@osu.edu
3Department of Industrial & Systems Engineering and Wisconsin Institute for Discovery,
University of Wisconsin-Madison, Madison, WI, USA. E-mail: jim.luedtke@wisc.edu

Version 2 (this document): July 25, 2022

Version 1: July 24, 2020

Abstract

We study optimization for data-driven decision-making when we have observations of the uncertain
parameters within the optimization model together with concurrent observations of covariates. Given a
new covariate observation, the goal is to choose a decision that minimizes the expected cost conditioned
on this observation. We investigate three data-driven frameworks that integrate a machine learning pre-
diction model within a stochastic programming sample average approximation (SAA) for approximating
the solution to this problem. Two of the SAA frameworks are new and use out-of-sample residuals of
leave-one-out prediction models for scenario generation. The frameworks we investigate are ﬂexible and
accommodate parametric, nonparametric, and semiparametric regression techniques. We derive condi-
tions on the data generation process, the prediction model, and the stochastic program under which
solutions of these data-driven SAAs are consistent and asymptotically optimal, and also derive conver-
gence rates and ﬁnite sample guarantees. Computational experiments validate our theoretical results,
demonstrate the potential advantages of our data-driven formulations over existing approaches (even
when the prediction model is misspeciﬁed), and illustrate the beneﬁts of our new data-driven formula-
tions in the limited data regime.

Key words: Data-driven stochastic programming, covariates, regression, sample average approximation,
jackknife, large deviations

1

Introduction

We study data-driven decision-making under uncertainty, where the decision-maker (DM) has access to a
ﬁnite number of observations of uncertain parameters of an optimization model together with concurrent
observations of auxiliary features/covariates. Stochastic programming [17, 73] is a popular modeling frame-
work for decision-making under uncertainty in such applications. A standard formulation of a stochastic
program is

∗An earlier version of this article was available on Optimization Online on July 24, 2020. This version also incorporates

analysis from our unpublished technical report arXiv preprint arXiv:2101.03139.

†I dedicate this work to the memory of my grandparents (Thathi and Thatha).

E [c(z, Y )] ,

min
z∈Z

1

 
 
 
 
 
 
where z denotes the decision variables, Y denotes the uncertain model parameters,
denotes the feasible
region, c is a cost function, and the expectation is computed with respect to the distribution of Y . Data-
driven solution methods such as sample average approximation (SAA) traditionally assume access to only
samples of the random vector Y [43, 73]. However, in many real-world applications, values of Y (e.g., demand
for water and energy) are predicted using available covariate information (e.g., weather).

Z

Motivated by the developments in Ban and Rudin [4], Bertsimas and Kallus [12], and Sen and Deng [72],
we study the case in which covariate information is available and can be used to inform the distribution of
Y . Speciﬁcally, given a new random observation X = x of covariates, the goal of the DM is to solve the
conditional stochastic program

E [c(z, Y )

min
z∈Z

|

X = x] .

(SP)

The aim of this paper is to analyze the SAA framework when a prediction model—obtained by statistical or
machine learning—is explicitly integrated into the SAA for (SP) to leverage the covariate observation X = x.
Here, residuals of the prediction model are scaled and added on to a point prediction of Y at X = x to
construct scenarios of Y for use within the SAA. We formally deﬁne our data-driven approximations to (SP)
in Section 2.

Applications of this framework include (i) the data-driven newsvendor problem [4], where the product’s
demand can be predicted using seasonality and location data before making order decisions, (ii) dynamic
procurement of a new product [5] whose demand can be predicted using historical data for similar past prod-
ucts, (iii) shipment planning under uncertainty [12], where historical demands, weather forecasts, and web
search results can be used to predict products’ demands before making production and inventory decisions,
and (iv) grid scheduling under uncertainty [30], where seasonality, weather, and historical demand data can
be used to predict the load before creating generator schedules.

Formulation (SP) requires knowledge of the conditional distribution of the random variables given a new
realization of the covariates. Since this distribution is typically unknown, we are interested in using an
estimate of it to approximately solve (SP) given access to a ﬁnite set of joint observations of (X, Y ). In this
setting, we would like to construct approximations to (SP) that not only have good statistical properties,
but are also practically eﬀective in the limited data regime. At a minimum, we would like a data-driven
approach that is asymptotically optimal in the sense that the objective value of its solutions approaches the
optimal value of (SP) as the number of samples increases. We would also like to determine the rate at which
this convergence occurs.

Our ﬁrst contribution is to generalize and analyze the approach proposed in Ban et al. [5] and Sen
and Deng [72], in which data-driven approximations to (SP) are constructed using explicit models that
predict the random vector Y using the covariates X. In this approach, a prediction model is ﬁrst used to
generate a point prediction of Y at the new observation X = x. The residuals obtained during the training
of the prediction model are then scaled and added on to this point prediction to construct scenarios for
use within an SAA framework to approximate the solution to (SP). We refer to this approximation as
the empirical residuals-based SAA (ER-SAA). We demonstrate asymptotic optimality, rates of convergence,
and ﬁnite sample guarantees of solutions obtained from the ER-SAA under mild assumptions. Inspired by
jackknife-based methods for constructing prediction intervals [6], we also propose two new data-driven SAA
frameworks that use leave-one-out residuals instead of empirical residuals, and demonstrate how our analysis
can be extended to these frameworks. The motivation for these new data-driven SAA formulations is that
using leave-one-out residuals might result in a better approximation of the true conditional distribution of
Y given X = x, particularly when the sample size is small.

The prediction frameworks we analyze are ﬂexible and accommodate parametric, nonparametric, and
semiparametric regression techniques [41, 78, 83]. While our results imply that using nonparametric re-
gression techniques within our SAA frameworks results in convergent approximations to (SP) under mild
assumptions [cf. 12], the rate at which such approximations converge typically exhibits poor dependence on
the dimension of the covariate vector X. Parametric (and semiparametric) regression approaches, on the
other hand, presume some knowledge of the functional dependence of Y on X. If the assumed functional
dependence is a good approximation of the true dependence, they may yield signiﬁcantly better solutions

2

when the number of samples is limited. The tradeoﬀ between employing parametric and nonparametric
regression techniques within our framework is evident upon looking at the assumptions under which these
approaches are guaranteed to yield convergent approximations to (SP), the rates at which their optimal
solutions converge, and numerical experience in Section 4. The generality of our framework enables DMs to
choose the modeling approach that works best for their application.

1.1 Relation to existing literature

The papers of Ban et al. [5] and Sen and Deng [72] are most closely related to this work. Motivated by the
application of dynamic procurement of a short-life-cycle product in the presence of demand uncertainty, Ban
et al. [5] propose a residual tree method for the data-driven solution of multistage stochastic programs
(also see references to the operations management literature therein for other data-driven approaches).
They propose to use ordinary least squares (OLS) or Lasso regression to generate demand forecasts for
a new product using historical demand and covariate data for similar products, and establish asymptotic
optimality of their data-driven procurement decisions for their particular application. Sen and Deng [72]
also use predictive models to generate scenarios of random variables in stochastic programs with exogenous
and endogenous uncertainty when covariate information is available. They propose an empirical additive
error method that is similar to the residual tree method of Ban et al. [5]. They also consider estimating
distributions of the coeﬃcients and residuals of a linear regression model and propose to subsequently sample
from these distributions to generate scenarios of the random variables. They present model validation and
model selection strategies for when the DM has access to several candidate prediction models. Kim and
Mehrotra [52] use empirical residuals to construct scenarios in a computational study, but conduct no analysis
of the approach.

Our work diﬀers from the above in the following respects: we introduce a general framework that applies
to a wide range of prediction and optimization models and allows for the covariance matrix of the errors
to depend on the covariates; we establish asymptotic optimality of the solutions from the ER-SAA under
general conditions; we derive results establishing rates of convergence and ﬁnite sample guarantees of the
solutions from the ER-SAA; we propose two new frameworks that use leave-one-out residuals and extend the
asymptotic optimality and rate of convergence analysis to these frameworks; and we present an empirical
study demonstrating the potential advantage of using these frameworks.

Bertsimas and Kallus [12] consider approximating the solution to (SP) by solving a reweighted SAA
problem, where the weights are chosen using nonparametric regression methods based on k-nearest neigh-
bors (kNN), kernels, classiﬁcation and regression trees (CART), or random forests (RF). They pay particular
attention to the setting where the joint observations of (X, Y ) may not be i.i.d., but arise from a mixing
process. They also consider the setting where decisions aﬀect the realization of the uncertainty, and estab-
lish asymptotic optimality and consistency of their data-driven solutions. They also consider policy-based
empirical risk minimization (ERM) approaches for (SP), and develop out-of-sample guarantees for costs
of decisions constructed using such policies. Diao and Sen [29] develop stochastic quasigradient methods
for eﬃciently solving the kNN and kernel-based reweighted SAA formulations of Bertsimas and Kallus [12]
without sacriﬁcing theoretical guarantees. Ban and Rudin [4] also propose a policy-based ERM approach
and a kernel regression-based nonparametric approach for solving (SP) in the context of the data-driven
newsvendor problem. They derive ﬁnite sample guarantees on the out-of-sample costs of order decisions,
and quantify the gains from using feature information under diﬀerent demand models. Bazier-Matte and
Delage [9] derive out-of-sample performance guarantees for regularized portfolio selection with side infor-
mation. Bertsimas and McCord [13] extend the analysis of Bertsimas and Kallus [12] to the multistage
setting when the covariates evolve according to a Markov process. They establish asymptotic optimality and
consistency of their data-driven decisions along with ﬁnite sample guarantees for the solutions to the kNN-
based approach. Kallus and Mao [49] propose RF-based decision policies for problem (SP) and demonstrate
asymptotic optimality of these policies. Finally, Hu et al. [46] show that “estimate and then optimize” meth-
ods can have faster regret convergence rates than ERM-based approaches for contextual linear optimization
when the estimated parameters appear in the objective coeﬃcients.

3

Our work diﬀers from the above in the following respects: we propose data-driven approaches to approx-
imate the solution to (SP) that rely on the construction of explicit models to predict the random variables
from covariates, allow for both parametric and nonparametric regression models, and derive convergence
rates and ﬁnite sample guarantees for solutions to our approximations that complement the above analyses.
Another stream of research has been investigating methods that change the training of the prediction
model in order to obtain better solutions to (SP) [e.g., see 26, 30, 34, 35]. The philosophy behind these
approaches is that, instead of constructing the prediction model purely for high predictive accuracy, the DM
should construct a model to predict Y using X such that the resulting optimization decisions provide the
lowest cost solution to the true conditional stochastic program (SP). These methods result in harder joint
estimation and optimization problems that can only be solved to optimality in special settings. In contrast,
we focus on the setting where the prediction framework is independent of the stochastic programming model.
This is common in many real-world applications and facilitates easily changing or improving the prediction
model.

Several recent works [e.g., see 14, 31, 36, 61] use distributionally robust optimization (DRO) in a bid to
construct better approximations to (SP) than SAA in the limited data regime. In follow-up work [50], we
study residuals-based DRO formulations that are built around our data-driven SAA formulations, analyze
their theoretical guarantees, and illustrate their advantages in the limited data regime through a case study.
Our data-driven SAA formulations in this work are ﬂexible and remain tractable under milder assumptions
on (SP) compared to such DRO approaches.

A ‘traditional data-driven SAA approach’ for the conditional stochastic program (SP) would involve
constructing a model to predict the random variables Y given X, ﬁtting a distribution to the residuals of
the prediction model, and using samples from this distribution along with the prediction model to construct
scenarios for Y given X = x. While it is diﬃcult to pin down a reference that is the ﬁrst to adopt this
approach, we point to the works of Sch¨utz et al. [69], Royset and Wets [67], and the references therein
for applications-motivated versions. Recent work by Grigas et al. [40] contributes to this approach by
attempting to directly estimate the conditional distribution of Y given X while considering the structure of
the optimization problem (assuming Y has ﬁnite support). Instead of ﬁtting a distribution to the residuals
of the prediction model, we propose and analyze methods that directly use empirical residuals within the
SAA framework. These methods avoid the need to ﬁt a distribution of the residuals, and hence we expect
them to be advantageous when the available data is insuﬃcient to provide a good estimate of the residuals
distribution.

1.2 Summary of main contributions

The key contributions of this paper are as follows:

1. We demonstrate asymptotic optimality, rates of convergence, and ﬁnite sample guarantees of solutions
to the ER-SAA formulation under mild assumptions on the data, the prediction framework, and the
stochastic programming formulation.

2. We introduce and analyze two new variants of ER-SAA that use leave-one-out residuals instead of

empirical residuals, which may lead to better solutions when data is limited.

3. We verify that the assumptions on the underlying stochastic programming formulation hold for a
broad class of two-stage stochastic programs, including two-stage stochastic mixed-integer program-
ming (MIP) with continuous recourse. Additionally, we verify that the assumptions on the prediction
step hold for a broad class of M-estimation procedures and nonparametric regression methods, including
OLS, Lasso, kNN, and RF regression.

4. Finally, we empirically validate our theoretical results, demonstrate the advantages of our data-driven
SAA formulations over existing approaches in the limited data regime, and demonstrate the potential
beneﬁt of using a structured prediction model even if it is misspeciﬁed.

4

2 Data-driven SAA frameworks

Recall that our goal is to approximate the solution to the conditional stochastic program (SP):

E [c(z, Y )

min
z∈Z

|

X = x] ,

where X = x is a new random observation of the covariates and the expectation is taken with respect to
the conditional distribution of Y given X = x. Let PX and PY denote the marginal distributions of the
Rdy denote their supports. We
covariates X and the random vector Y , respectively, and
assume that the support

is nonempty and convex and c :

Rdx and
R.

X ⊆

Y ⊆

We assume that the ‘true relationship’ between the random vector Y and the random covariates X can

Z × Y →

Y

be described as

Y = f ∗(X) + Q∗(X)ε,

|

where f ∗(x) := E [Y
X = x] is the regression function, Q∗(X) is the square root of the conditional covariance
matrix of the error term, and the zero-mean random errors ε are independent of the covariates X. Because
the the error term Q∗(X)ε is inﬂuenced by the covariate X, our model is heteroscedastic. When Q∗
I, as
assumed in Ban et al. [5] and Sen and Deng [72], the error distribution is homoscedastic. Heteroscedasticity
arises, for instance, when variability of the random vector Y such as the variability of product demands or
wind power availability depends on the covariates X like location and seasonality. It can also arise when the
DM cannot fully identify all the covariates and the remaining covariates appear in the error term.

≡

We suppose that f ∗ and Q∗ belong to known classes of functions

F

Q

and

may comprise parametric or nonparametric models. Let Ξ

classes
and Pε denote its distribution. We assume Q∗(¯x)
≻
deﬁnite. We may also assume without loss of generality that the covariance matrix E
general covariance matrix for ε can be handled by suitably redeﬁning Q∗.

, where A

∈ X

0,

≻

¯x

∀

Under these structural assumptions, the conditional stochastic program (SP) is equivalent to

(cid:2)

(cid:3)

F

and

, respectively. The model
Q
Rdy denote the support of ε
0 denotes the matrix A is positive
εεT
= I since a

⊆

v∗(x) := min

g(z; x) := E [c(z, f ∗(x) + Q∗(x)ε)]

z∈Z {

,

}

(1)

Rdz is nonempty and compact, E [

where the expectation is computed with respect to the distribution Pε of ε. We refer to problem (1) as
the true problem, and denote its optimal solution set by S∗(x). Throughout, we assume that the feasible
c(z, f ∗(x) + Q∗(x)ε)
and almost every
set
|
Z ⊂
(a.e.) x
(see Theorem 7.42
; x) is lsc). These assumptions ensure problem (1) is
of Shapiro et al. [73] for conditions that guarantee g(
·
well deﬁned and the solution set S∗(x)
for a.e. x
∈ X
(yi, xi)
}

i=1 denote joint observations of (Y, X). If the functions f ∗ and Q∗ are known, then
n

] < +
; x) is lower semicontinuous (lsc) on

, and the function g(
·

the full-information SAA (FI-SAA) counterpart to the true problem (1) using data

for each z
for a.e. x

∈ Z
∈ X

Dn :=

∞
Z

∈ X

Let

=

∅

{

|

.

Dn is

g∗
n(z; x) :=

1
n

min
z∈Z

(cid:26)

i=1
X
1, . . . , n

n

c(z, f ∗(x) + Q∗(x)εi)

,

(2)

(cid:27)

where εi := [Q∗(xi)]−1(yi
denote the realizations of the errors ε at the given
}
−
observations. We cannot solve problems (1) or (2) directly because the functions f ∗ and Q∗ are unknown. A
practical alternative is to ﬁrst estimate f ∗ from the data
Dn, for instance by using an M-estimator [76, 78]
of the form

f ∗(xi)),

∈ {

i
∀

ˆfn(
·

)

arg min
f (·)∈F

∈

1
n

n

ℓ

yi, f (xi)

i=1
X

(cid:0)

(cid:1)

(3)

with some loss function ℓ : Rdy
F
is parameterized by θ (e.g., the parameters of a linear regression model) and let θ∗ denote the true value

R+. We sometimes assume that the regression model class

Rdy

→

×

5

6
of θ corresponding to the regression function f ∗. In this setting, the aim of the regression step (3) is to
estimate θ∗, and we denote the estimate corresponding to ˆfn by ˆθn.

Given an estimate ˆfn of f ∗, we use the fact that E

= Q∗(x)Q∗(x)T
and plug in ˆfn instead of f ∗ to determine the best regression estimate ˆQn of Q∗ in the model class
[8]. We
(cid:2)
may then update our estimate ˆfn using an estimate ˆQn of Q∗, e.g., using weighted least squares regression [66],
which could yield an improved estimate of f ∗ with lower variance. Alternatively, we could estimate f ∗ and
Q∗ jointly using M-estimation [27]. In the homoscedastic setting, we simply set ˆQn := Q∗
I. Throughout,
we reference equation (3) for both regression steps (i.e., for estimating f ∗ and Q∗) with the understanding
that our regression setup is not restricted to M-estimation.

f ∗(X))T

f ∗(X))(Y

X = x

(Y

Q

≡

−

−

(cid:3)

|

Given an estimate ˆfn of f ∗ and a nonsingular estimate ˆQn of Q∗, residuals ˆεi

ˆfn(xi)),
, of this estimate can be used as proxy for samples of ε from Pε. Let projY (v) denote the
. Then, the empirical residuals-based SAA (ER-SAA) corresponding

n := [ ˆQn(xi)]−1(yi

Rdy onto

1, . . . , n

−

∈ {

i
}
orthogonal projection of v
∈
to problem (1) is deﬁned as

Y

ˆvER
n (x) := min
z∈Z

ˆgER
n (z; x) :=

1
n

n

c

z, projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

.
(cid:27)
(cid:1)

(4)

(cid:26)

(cid:0)

{

n}

ˆfn(x) + ˆQn(x)ˆεi

are not projected onto the support

i=1
X
While the ER-SAA problem (4) is equally tractable and does not lose any theoretical guarantees if the
, this projection step may be helpful in
scenarios
situations where (some of) the ER-SAA scenarios
even though
n (x) denote an optimal solution
the “true” FI-SAA scenarios
to problem (4) and ˆSER
n (x) is
nonempty for a.e. x
for
each y
. We stress that problem (4) is diﬀerent from the following naive SAA (N-SAA) problem that
n
i=1 of the random vector Y without using the new covariate observation
directly uses the observations
X = x:

n (x) denote its optimal solution set. We assume throughout that the set ˆSER
, y) is lower semicontinuous on

, which holds, for example, if the function c(
·

Y
ˆfn(x) + ˆQn(x)ˆεi
n}

lie outside the support

f ∗(x)+Q∗(x)εi

are contained in

. We let ˆzER

∈ X

∈ Y

yi

Z

Y

Y

{

{

}

{

}

ˆvNSAA
n

:= min
z∈Z

1
n

n

c

z, yi

.

i=1
X

(cid:0)

(cid:1)

(5)

The computational complexity of the ER-SAA problem (4) is similar to that of the N-SAA problem (5) with
the only additional computation cost being the cost of estimating f ∗ and Q∗. Problem (4) also diﬀers from
the following point prediction-based deterministic approximation to (1):

ˆvPP
n (x):= min
z∈Z

c

z, projY ( ˆfn(x))

.

(6)

Problem (4) is a modiﬁcation of problem (6) that accounts for the uncertainty in the point estimate.

(cid:0)

(cid:1)

n

{

Dn =

We also propose two alternatives to the ER-SAA problem (4) that construct scenarios diﬀerently. Note
i=1 that are used to estimate the regression functions ˆfn and ˆQn are
that the n observations
also used to estimate the errors ˆεi
1, . . . , n
. This can cause a bias in the estimation of the residuals,
especially when the sample size n is small, yielding suboptimal solutions for some problems. To alleviate this
, let ˆf−i and
issue, we propose to use jackknife-based variants of the ER-SAA problem. For each i
ˆQ−i denote the estimates of f ∗ and Q∗ obtained by omitting the data point (yi, xi) from the training set
Dn
ˆf−i(xi)),
while carrying out the regression step (3), and deﬁne the residual term ˆεi
calculated at the omitted point (yi, xi). The alternatives we propose are

∈ {
}
n,J := [ ˆQ−i(xi)]−1(yi

(yi, xi)
}
n, i
∈ {

1, . . . , n

−

}

ˆvJ
n (x) := min

z∈Z(

ˆgJ
n(z; x) :=

1
n

ˆvJ+
n (x) := min

z∈Z(

ˆgJ+
n (z; x) :=

n

c

i=1
X
n
1
n

z, projY ( ˆfn(x) + ˆQn(x)ˆεi

n,J )

,

)
(cid:1)
z, projY ( ˆf−i(x) + ˆQ−i(x)ˆεi
n,J )

(cid:0)
c

(7)

(8)

.

)
(cid:1)

i=1
X

(cid:0)

6

We call problems (7) and (8) jackknife-based SAA (J-SAA) and jackknife+-based SAA (J+-SAA), respec-
tively [cf. 6]. These data-driven SAAs are well-motivated when the data
Dn is independent, in which case
the leave-one-out residual ˆεi
n,J may be a signiﬁcantly more accurate estimate of the scaled prediction error at
the covariate observation xi than the empirical residual ˆεi
n, particularly when n is small relative to the com-
plexity of the regression step (3) due to overﬁtting [6]. When
Dn is not independently generated, omitting
blocks of data (instead of individual observations as in the jackknife-based methods) during the regression
steps (3) can yield better-motivated variants of the J-SAA and J+-SAA formulations [54].

Problems (7) and (8) roughly require the construction of n regression models, which may be computation-
ally unattractive in some settings. This extra computational burden can be alleviated in some special settings
such as OLS regression by re-using information from one regression model to the next (see page 13 of Bar-
ber et al. [6] for other regression setups that can re-use information). We make use of this computational
speed-up in our experiments in Section 4.

We use the following two-stage stochastic linear program (LP) as our running example for problem (1).
Section E in the Appendix includes a discussion of more general forms of problem (1) that satisfy the
assumptions of our framework. The more general classes of problems discussed in the Appendix subsume
the problem class presented in Example 1.

Example 1. [Two-stage stochastic LP] The set
cT
cT
v v : W v = Y
z z + V (z, Y ), with V (z, Y ) := minv∈Rdv
commensurate dimensions. We assume that V (z, y) < +
full row rank, and the dual feasible set

(cid:8)
λ : λTW

Z

+

−
for each z
(cid:9)
∞
is nonempty.

is a nonempty convex polytope and the function c(z, Y ) :=
. The quantities cz, cv, W , and T have
Rdy , the matrix W has

and y

T z

∈ Z

∈

cT
v

≤

(cid:9)
We also use OLS regression with a structured parametric model for heteroscedasticity as our running
example for the regression step (3). Section F in the Appendix includes a detailed discussion of how other
prediction models ﬁt within our framework.

(cid:8)

y

k

∈

F

−

:=

f (
·

) : f (X) = θX for some θ

ˆy
k
Rdy×dx, and estimate θ∗ by ˆθn ∈
Q : Rdx

Example 2. [OLS regression] The model class
, with X1 ≡
1
2. We assume the regression function is f ∗(X) = θ∗X for some
and loss function is ℓ(y, ˆy) :=
(cid:9)
θ∗
θxi
arg minθ∈Rdy ×dx
In the homoscedastic set-
Rdy×dy : Q
In
ting, the model class is
the heteroscedastic setting, following Romano and Wolf [66], the model class is a set of diagonal matrices
Xi|
i log
)
|
= 0. For

n
i=1k
I
P
}
≡
Rdy×dy : Q(X) = diag(q1(X), q2(X), . . . , qdy (X))
}
Rdx. We assume that Q∗
with parameters
= q2
(Yj −

yi
and we simply set ˆQn := Q∗

∈
, we use E
1, . . . , dy}

Q
{
for some πj
j

j (x) to estimate πj∗ by

j=1 and PX {

≡
i=1 πj

j (X) := exp(

∈ Q
X = x

, where q2

X = 0
P

j (X))2
f ∗

Q : Rdx

πj∗

∈ {

:=

:=

→

→

2.

I.

Q

−

1
n

∈

dx

(cid:8)

dy

{

k

}

}

{

|

Rdy×dx

(cid:2)
arg min
πj ∈Rdx

1
n

ˆπj
n ∈

log(max

(cid:3)

{

n

i=1(cid:16)
X

δn, yi

j −

(ˆθnxi)j}

2)

−

dx

Xk=1

πj
k log

Xk|

|

2

.

(cid:17)

The tolerance sequence

δn} ↓

{

0 is chosen to avoid ill-conditioning [66].

There is an inherent tradeoﬀ between using parametric and nonparametric regression techniques for
estimating the functions f ∗ and Q∗. If the function classes
are correctly speciﬁed, then parametric
regression approaches may yield much faster rates of convergence of the data-driven SAA estimators relative
to nonparametric approaches (see Section 3.3). On the other hand, misspeciﬁcation of the prediction model
can result in our data-driven solutions being asymptotically inconsistent and suboptimal. Empirical evidence
in Section 4 indicates that it may still be beneﬁcial to use a misspeciﬁed prediction model when we do not
have access to an abundance of data.

and

Q

F

Remark 1. Although we assume that the functions f ∗
and Q∗
guarantees, our ER-SAA formulation (4) is well deﬁned even when f ∗
regression models are misspeciﬁed. In this misspeciﬁed setting, the regression estimates satisfy ˆfn
and ˆQn

to establish our theoretical
and Q∗
, i.e., when the
p
−→

∈ F
denotes convergence in probability, and ¯f and ¯Q are the best (in terms of

∈ Q
6∈ F

, where

6∈ Q

∈ F

¯Q

¯f

p
−→

∈ Q

p
−→

7

prediction error) approximations to f ∗ and Q∗ in
, respectively, under mild assumptions. The
F
optimal value and solutions of the ER-SAA formulation then converge in probability to the optimal value
and solutions of

and

Q

n

min
z∈Z

1
n

i=1
X

(cid:0)

c

z, projY ( ¯f (x) + ¯Q(x)¯εi)

,

(cid:1)

¯f (xi)+ Q∗(xi)εi). Although the ER-SAA estimators
under mild assumptions, where ¯εi := [ ¯Q(xi)]−1(f ∗(xi)
−
may no longer be consistent, their asymptotic and ﬁnite sample properties in this setting may be characterized
by replacing f ∗, Q∗,

in our assumptions and results.

by ¯f , ¯Q, and

¯εi

εi

{

}

{

}

|

(cid:2)

{

k

S

≤

≻

⊆
−

exp(0.5σ2s2),

denote the cardinality of a ﬁnite set S,

Notation. Let [n] :=
1, . . . , n
denote the Euclidean
,
k·k
}
|
Bδ(v) denote a Euclidean ball of radius δ > 0
k·k0 denote the ℓ0 “norm”,
norm or its induced matrix norm,
around a point v, M[j] denote the jth row of a matrix M and M
0 denote that it is positive deﬁnite. For
Rdz , let D (A, B) := supv∈A dist(v, B) denote the deviation of A from B, where dist(v, B) :=
sets A, B
. A random vector V is said to be sub-Gaussian with variance proxy σ2 if E [V ] = 0 and
w
v
inf w∈Bk
E
exp(suTV )
= 1. The abbreviations ‘a.e.’, ‘LLN’, and ‘r.h.s.’ are
shorthand for ‘almost everywhere’, ‘law of large numbers’, and ‘right-hand side’. By ‘a.e. X’ and ‘a.e. Y ’,
and PY -a.e. Y . Throughout, ‘a.s.’ is written to mean almost surely with respect to
we mean PX -a.e. x
∈ X
the probability measure by which the data
are used to
is generated. The symbols
denote convergence in probability, almost surely, and in distribution with respect to this probability measure.
, Vn = op(Wn) and Vn = Op(Wn) convey that Vn = RnWn
For sequences of random variables
p
0), or being bounded in probability, respectively (see
with
−→
Chapter 2 of van der Vaart [78] for basic theory). We write O(1) to denote generic constants. We assume
throughout this work that all functions, sets and selections are measurable (see van der Vaart and Wellner
[79] and Shapiro et al. [73] for detailed consideration of these issues).

Wn}
converging in probability to zero (Rn

, and d
−→

(xi, εi)
}

, a.s.
−−→

R and

Rn}

Vn}

p
−→

s
∀

and

∈

u

{

{

{

{

k

k

(cid:3)

3 Analysis of the empirical residuals-based SAA

In particular, after
We ﬁrst analyze the theoretical properties of solutions to the ER-SAA problem (4).
establishing some preliminary results that are useful for the remaining analysis (Section 3.1) we investigate
conditions under which solutions to problem (4) are asymptotically optimal and consistent (Section 3.2) and
develop ﬁnite sample guarantees for solutions to problem (4) using large deviations theory (Section 3.3).
We outline the modiﬁcations required to analyze the J-SAA and J+-SAA methods in Section 3.4. Omitted
proofs are provided in Appendix A. We also present a number of complementary results in the Appendix. In
Section B of the Appendix, we brieﬂy discuss alternative assumptions under which our theoretical guarantees
hold. We analyze the rate of convergence of the optimal value of problem (4) to that of problem (1) in
Section C. In Section D, we provide more details about the adaptation of our analysis to the J-SAA and
J+-SAA methods. Finally, in Sections E and F, we verify that a variety of stochastic optimization and
regression setups satisfy the assumptions made in our analysis.

3.1 Preliminary results

The diﬀerence between the objective functions of the ER-SAA problem (4) and the true problem (1) can be
bounded uniformly over the decision variables z

as follows:

ˆgER
n (z; x)

sup
z∈Z

g(z; x)

−

sup
z∈Z

≤

g∗
n(z; x)

−

+ sup
z∈Z |

g∗
n(z; x)

g(z; x)
|

.

−

(9)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

The second term on the r.h.s. of inequality (9) corresponds to the maximum deviation between the FI-SAA
objective function (2) and the objective function of the true problem on
. This term converges to zero in
probability whenever a uniform weak LLN result holds [73]. The ﬁrst term on the r.h.s. of inequality (9)
. Let
corresponds to the maximum deviation between the ER-SAA and FI-SAA objective functions on

Z

Z

8

∈ Z
ˆgER
n (z; x)

n(x) denote the diﬀerence between the ith ER-SAA scenario ( ˆfn(x)+ ˆQn(x)ˆεi
˜εi
(f ∗(x) + Q∗(x)εi), i.e.,

n) and the ith FI-SAA scenario

˜εi
n(x) :=

ˆfn(x) + ˆQn(x)ˆεi
n

f ∗(x) + Q∗(x)εi

,

−

[n].

i
∀

∈

(cid:1)
Then, the ﬁrst term on the r.h.s. of inequality (9) can be bounded using the deviation sequence
whenever the following Lipschitz assumption holds.

(cid:0)

(cid:1)

(cid:0)

˜εi
n(x)
}

{

n
i=1

Assumption 1. For each z

∈ Z

, the function c in problem (1) satisﬁes the Lipschitz condition

−
with Lipschitz constant L satisfying sup
z∈Z

|

c(z, ¯y)

c(z, y)

| ≤
L(z) < +

.
∞

L(z)
k

¯y

−

y

,

k

y, ¯y

∀

,

∈ Y

Assumption 1 requires the function c(z,

. We show in
Section E in the Appendix that it is satisﬁed by Example 1. When Assumption 1 holds, the ﬁrst term on
the r.h.s. of inequality (9) can be bounded as follows.

) to be Lipschitz continuous on

for each z

∈ Z

Y

·

Lemma 1. Suppose Assumption 1 holds. Then

sup
z∈Z

ˆgER
n (z; x)

g∗
n(z; x)

−

(cid:12)
(cid:12)

sup
z∈Z

≤

(cid:16)

L(z)

(cid:17)(cid:18)

1
n

n

i=1
X

˜εi
n(x)
k

k

.
(cid:19)

(cid:12)
(cid:12)

Therefore, when Assumption 1 holds, convergence of the mean deviation term 1
n

to zero
in probability readily translates to convergence to zero in probability of the ﬁrst term on the r.h.s. of
inequality (9). Next, we focus on bounding the mean deviation term 1
using the arguments in
n
Section 3.1 of Kannan et al. [51].

˜εi
n(x)
k

˜εi
n(x)
k

n
i=1k

P

n
i=1k

P

Lemma 2. Given regression estimates ˆfn of f ∗ and ˆQn of Q∗ with [ ˆQn(¯x)]−1

0 for each ¯x

:

∈ X

≻

1. In the homoscedastic setting (i.e., ˆQn := Q∗

I), we have

≡

1
n

n

k

i=1
X

˜εi
n(x)

k ≤ k

ˆfn(x)

f ∗(x)
k

−

+

1
n

n

k

i=1
X

ˆfn(xi)

f ∗(xi)
k

.

−

2. In the heteroscedastic setting, we have

1
n

n

k
i=1
X

˜εi
n(x)

k≤k

ˆfn(x)

f ∗(x)
k

−

+

k

ˆQn(x)

Q∗(x)
k

−

1
n

(cid:18)

n

k

2

i=1
X
−1

εi

+

k

(cid:19)
1/2

ˆQn(x)
k

k

ˆQn(x)
k

k

1
n

1
n

(cid:18)

(cid:18)

n

i=1
X
n

(cid:13)
(cid:13)

i=1
X

(cid:13)
(cid:13)

ˆQn(xi)

−1

Q∗(xi)

−

(cid:3)
(cid:2)
ˆQn(xi)

(cid:2)

1/2

−1

2

(cid:2)

(cid:3)

(cid:13)
(cid:13)

(cid:19)

(cid:18)

1
n

(cid:3)

n

(cid:13)
(cid:13)

k

i=1
X

n

1
n

Q∗(xi)
k

k

1/4

4

(cid:19)

(cid:18)

(cid:19)

(cid:18)

f ∗(xi)

−

i=1
X
ˆfn(xi)
k

2

1/2

.

(cid:19)

1
n

n

i=1
X

εi

4

k

k

1/4

+

(cid:19)

(10)

Proof. We begin by noting that

n

1
n

n

i=1
X

˜εi
n(x)
k

k

:=

1
n

i=1
X
ˆfn(x)

≤ k

( ˆfn(x) + ˆQn(x)ˆεi
n)

(f ∗(x) + Q∗(x)εi)
k

−

k

f ∗(x)
k

−

+

1
n

n

k

i=1
X

ˆQn(x)ˆεi

n −

Q∗(x)εi

.

k

(11)

9

In the homoscedastic case, we can bound the r.h.s. of inequality (11) further as

1
n

n

k

i=1
X

˜εi
n(x)

k ≤ k

ˆfn(x)

f ∗(x)
k

−

+

1
n

n

i=1
X

ˆεi
n −

k

εi

k

=

k

ˆfn(x)

f ∗(x)
k

−

+

1
n

n

k

i=1
X

ˆfn(xi)

f ∗(xi)
k

.

−

We now focus on the heteroscedastic setting by bounding the second term on the r.h.s. of (11).

ˆQn(x)ˆεi

n −

Q∗(x)εi

k

ˆQn(x)

ˆQn(xi)

−1

(yi

ˆfn(xi))

−

−

Q∗(x)εi

(cid:13)
(cid:2)
(cid:13)
ˆQn(x)

(cid:3)
ˆQn(xi)

−1

f ∗(xi)

(cid:13)
(cid:2)
(cid:13)
ˆQn(x)

(cid:3)
ˆQn(xi)

(cid:0)
(f ∗(xi)

−1

−

−

(cid:13)
(cid:13)

ˆfn(xi) + Q∗(xi)εi

Q∗(x)εi

k

−

ˆfn(xi))

+

1
n

(cid:13)
(cid:13)

(cid:1)

n

ˆQn(x)

ˆQn(xi)

−1Q∗(xi)

i=1
X

(cid:13)
(cid:0)
(cid:13)

(cid:2)

(cid:3)

Q∗(x)

εi

.

(12)

−

(cid:1)

(cid:13)
(cid:13)

Q∗(x)=

ˆQn(x)
= ˆQn(x)
(cid:0)

(cid:2)

ˆQn(xi)
ˆQn(xi)
(cid:3)

−1

−1

−

−

Q∗(x)

Q∗(xi)
(cid:2)

−1

Q∗(xi)
−1

Q∗(xi)
Q∗(xi) + [ ˆQn(x)
(cid:3)

(cid:1)

Q∗(x)].

−

Plugging the above equality into inequality (12), we get

(cid:0)(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:1)

1
n

1
n

1
n

1
n

=

=

≤

n

k

i=1
X
n

i=1
X
n

i=1
X
n

i=1
X

(cid:13)
(cid:13)

ˆQn(x)

1
n

1
n

≤

1
n

≤

We have for each i

(cid:2)

(cid:3)

[n]

−1Q∗(xi)

−

∈
ˆQn(xi)

(cid:2)

(cid:3)

n

k

i=1
X
n

ˆQn(x)ˆεi

n −

Q∗(x)εi

k

ˆQn(x)

ˆQn(xi)

−1

Q∗(xi)

−1

Q∗(xi) +

ˆQn(x)

−

−

Q∗(x)

εi

+

(cid:1)(cid:17)

(cid:13)
(cid:13)
(cid:13)

(cid:16)
n

i=1(cid:13)
X
(cid:13)
1
(cid:13)
n

n

i=1
X
(cid:13)
(cid:13)
ˆQn(x)

(cid:0)(cid:2)
ˆQn(x)

(cid:3)
ˆQn(xi)

(cid:2)
(f ∗(xi)

−1

(cid:1)
ˆfn(xi))

(cid:3)

−

(cid:2)
ˆQn(xi)

(cid:3)

−1

−

Q∗(xi)

−1

(cid:13)
(cid:13)

Q∗(xi)εi

(cid:0)

+

1
n

i=1
X
(cid:13)
(cid:13)
1
n

n

(cid:0)(cid:2)
ˆQn(x)

(cid:3)
ˆQn(xi)

(cid:2)
(cid:3)
−1
(f ∗(xi)

−

(cid:1)
ˆfn(xi))

(cid:13)
(cid:13)

n

ˆQn(x)

i=1
X

(cid:13)
(cid:0)
(cid:13)

Q∗(x)

εi

+

−

(cid:1)

(cid:13)
(cid:13)

i=1
X

(cid:13)
(cid:13)
1
n

ˆQn(x)
k

≤k

n

(cid:2)
ˆQn(xi)

(cid:3)
−1

(cid:13)
1/2
(cid:13)

Q∗(xi)

−1

2

(cid:18)

i=1
X
(cid:13)
(cid:2)
(cid:13)
Q∗(x)
k

1
n

ˆQn(x)

k

−

n

(cid:3)
εi

k

k

(cid:2)

+

(cid:3)
ˆQn(x)
k

k

(cid:18)

i=1
X

(cid:18)

n

i=1
X
ˆQn(xi)

(cid:13)
(cid:13)

(cid:19)
1
n

(cid:18)

i=1
X

−

(cid:19)

1
n

n

1/4

Q∗(xi)
k

k

4

(cid:19)
1/2

−1

2

n

i=1
X

1
n

n

εi

4

k

k

1/4

+

(cid:19)

f ∗(xi)

k

2

ˆfn(xi)
k

−

(cid:19)

(cid:19)

(cid:18)

i=1
X

(cid:13)
(cid:2)
(cid:13)
where the last step above follows by the Cauchy-Schwarz inequality. Finally, using inequality (13) in in-
equality (11), we get the stated result.

(cid:13)
(cid:13)

(cid:3)

(13)

1/2

,

(cid:18)
1
n

In the homoscedastic setting, the bound on the mean deviation term 1
n
ˆfn(x)
k
ˆfn(xi)
k

as the sum of the prediction error
estimation error 1
n

−
at the training data points

f ∗(x)
k

n
i=1k

f ∗(xi)

xi

−

}

{

.

at the new covariate realization x

n
i=1k

˜εi
n(x)
k
∈ X

P

can be interpreted
and the average

P

10

We remark that Section B of the Appendix presents alternative assumptions under which the theoretical
guarantees studied in the paper continue to hold. These alternative assumptions relax the uniform Lipschitz
continuity of Assumption 1 to a weaker local Lipschitz continuity condition but require more stringent
conditions on the regression step. Conditions and examples under which these alternative assumptions hold
are discussed in the Appendix.

3.2 Consistency and asymptotic optimality

In this section, we investigate conditions under which the optimal value and optimal solutions to the ER-SAA
problem (4) asymptotically converge to those of the true problem (1). We begin by making the following
assumption on the uniform convergence of the sequence of objective functions of the FI-SAA problem (2) to
the objective function of the true problem (1) on

.

Z

∈ X

deﬁned in (2) converges

, the sequence of sample average functions

Assumption 2. For a.e. x
in probability to the true function g(
·

g∗
; x)
n(
}
·
{
; x) deﬁned in (1) uniformly on the set
.
Z
Assumption 2 is a uniform weak LLN result that is guaranteed to hold if c(
, y) is continuous for a.e. y
·
εi
, y) is dominated by an integrable function for a.e. y
{

,
∈ Y
c(
are i.i.d. [see Theorem 7.48
·
of 73]. Using pointwise LLN results in Walk [84] and White [85], we can show that Assumption 2 also holds
for some mixing/stationary processes by noting the proof of Theorem 7.48 of Shapiro et al. [73] also extends
to these settings. Proposition 20 in the Appendix shows that Assumption 2 holds for our running example
(Example 1) of two-stage stochastic LP whenever E [
Y
k
Next, we need the following weak LLN assumptions on the function Q∗ and the errors ε. These
4],
(xi, εi)
Q∗(X)
assumptions hold, for instance, when the samples
k
}
k
2], and E[
E[
4] are ﬁnite. In particular, Assumption 3 holds for Example 2 if
ε
is i.i.d. and
k
k
2
k=1 πj∗
E
Xk|
1, . . . , dy}
))
k log(
.
|
Assumptions 3 and 4 also hold for non-i.i.d. data arising from mixing/stationary processes that satisfy
P
suitable assumptions (see the discussion above).

{
∞
are i.i.d. and the quantities E[

[Q∗(X)]−1
exp(

{
for each j

, and the errors

and the errors

k
k=1 πj∗

k log(
|

}
∈ {

are i.i.d.

and E

Xk|

] < +

< +

< +

exp(

∈ Y

P

∞

∞

xi

(cid:2)(cid:0)

εi

−

))

dx

dx

k

}

{

k

}

(cid:1)

(cid:2)

(cid:3)

(cid:3)

Assumption 3. The function Q∗ and the covariate samples

xi

{

}

n
i=1 satisfy the weak LLNs

1
n

n

i=1
X

Q∗(xi)
k

k

4 p
−→

E[

Q∗(X)
k

k

4]

and

1
n

n

i=1
X

(cid:13)
(cid:2)
(cid:13)

Assumption 4. The error samples

εi

{

}

n
i=1 satisfy the weak LLN

Q∗(xi)

−1

E

2 p
−→

Q∗(X)

−1

2

.

(cid:2)(cid:13)
(cid:2)
(cid:13)
4 p
−→

E[

εi

k

k

(cid:3)
1
n

(cid:13)
(cid:13)
n

i=1
X

(cid:3)

(cid:3)

(cid:13)
(cid:13)

4].

ε

k

k

Finally, we need the assumption below on the consistency of the regression estimates ˆfn and ˆQn.

Assumption 5. The regression estimates ˆfn and ˆQn possess the following consistency properties:
(5a) ˆfn(x)
n

Q∗(x) for a.e. x

f ∗(x) for a.e. x

(5c) ˆQn(x)

∈ X

n

,

,

ˆfn(xi)
k

−

2 p
−→

0,

(5d)

p
−→
ˆQn(xi)

1
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

−1

−

(cid:2)

(cid:3)

∈ X
Q∗(xi)

−1

2 p
−→

0.

(cid:3)

(cid:13)
(cid:13)

p
−→
f ∗(xi)

(5b)

1
n

k

i=1
X

E

xi

n
i=1k

2 p
−→

Assumption (5a) holds for our running example of OLS regression (Example 2) if the parameter esti-
θ∗), and Assumption (5b) holds if, in addition, the weak LLN

mate ˆθn is weakly consistent (i.e., ˆθn
1
Dn
n
ˆfn(xi)
and the distributions PX and Pε under which these conditions hold). The quantity 1
2
n
k
is called the empirical L2 semi-norm in the empirical process theory literature [76]. Assumption (5c) holds
for our running example of OLS regression with structured heteroscedasticity if the parameter estimate ˆπn
π∗, see Appendix B.2 of Romano and Wolf [66] for assumptions under
is weakly consistent (i.e., ˆπn

is satisﬁed (see Chapter 3 of White [85] for various assumptions on the data

n
i=1k

f ∗(xi)

p
−→

P

P

X

−

k

k

k

(cid:3)

(cid:2)

2

p
−→

11

X

to be compact and bounded away from the origin (i.e., for each x

which this holds). Assumption (5d) holds for our running example if, e.g., we additionally have the sup-
B for constants
port
∈ X
b, B > 0) and assume that the estimates
lie in a compact set a.s. for n large enough. Assumption 5
ˆπn}
is implied by the stronger assumption of uniform convergence of the estimates ˆfn and ˆQn to the func-
tions f ∗ and Q∗, respectively, on the support
f ∗(x)
0,
supx∈X k
0. Section F in the Appendix expands
0, and supx∈X k
−
on the above arguments and shows that Assumption 5 also holds when f ∗ is estimated using Lasso, kNN,
and RF regression under certain conditions.

of the covariates, i.e., when supx∈X k
p
−→

X
[Q∗(x)]−1

ˆQn(x)
k

ˆfn(x)
k

[ ˆQn(x)]−1

Q∗(x)

p
−→

p
−→

k ≤

≤ k

, b

−

−

x

k

{

The following result will prove useful in our analysis of ER-SAA in the heteroscedastic setting.

Lemma 3. We have

1
n

(cid:18)

n

ˆQn(xi)

−1

2

i=1
X

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:13)
(cid:13)

n

1/2

(cid:19)

≤

(cid:18)

1
n

i=1
X

(cid:2)

(cid:13)
(cid:13)

ˆQn(xi)

−1

−

(cid:3)

(cid:2)

Proof. The triangle inequality for the operator norm implies

Q∗(xi)

−1

2

1/2

(cid:19)

(cid:3)

(cid:13)
(cid:13)

+

1
n

(cid:18)

n

Q∗(xi)

−1

2

1/2

.

i=1
X

(cid:13)
(cid:2)
(cid:13)

(cid:19)

(cid:3)

(cid:13)
(cid:13)

ˆQn(xi)

−1

ˆQn(xi)

−1

−

≤

Q∗(xi)

−1

+

Q∗(xi)

−1

,

[n].

i
∀

∈

Therefore, the following component-wise inequality holds:
(cid:3)

(cid:2)

(cid:3)

(cid:3)

(cid:2)

0

≤



(cid:13)
(cid:13)

ˆQn(x1)
...
(cid:3)
(cid:2)
ˆQn(xn)

(cid:13)
(cid:2)
(cid:13)





(cid:13)
(cid:13)

(cid:13)
(cid:13)
−1





≤

(cid:13)
(cid:13)

−1









ˆQn(x1)

−1

(cid:13)
(cid:3)
(cid:2)
(cid:13)
−1
ˆQn(xn)

−
...

−

(cid:13)
(cid:13)
(cid:2)
(cid:13)
(cid:13)
−1
Q∗(x1)

(cid:2)

(cid:3)
Q∗(xn)

(cid:13)
(cid:13)

−1

(cid:3)

(cid:13)
(cid:13)

Q∗(x1)
...
(cid:13)
(cid:3)
(cid:2)
(cid:13)
Q∗(xn)

−1

(cid:13)
(cid:13)

−1



.







+ 









The stated result then follows as a consequence of the triangle inequality for the ℓ2-norm.
(cid:13)
(cid:13)

(cid:13)
(cid:2)
(cid:13)

(cid:13)
(cid:2)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:3)

(cid:2)

(cid:3)

(cid:3)

(cid:2)

(cid:3)

Our next result uses Lemmas 1 and 2 to establish conditions under which the sequence of objective
functions of the ER-SAA problem (4) converges uniformly to the objective function of the true problem (1)
on the feasible region

.

Proposition 4. Suppose Assumptions 1 through 5 hold. Then, for a.e. x
, the sequence of objective
functions of the ER-SAA problem (4) converges in probability to the objective function of the true problem (1)
uniformly on the feasible region

∈ X

.

Z

Z
Proof. We wish to show that sup
z∈Z

ˆgER
n (z; x)

ˆgER
n (z; x)

−

(cid:12)
(cid:12)

g(z; x)

sup
z∈Z

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

g(z; x)

−

(cid:12)
ˆgER
(cid:12)
n (z; x)

p
−→

0 for a.e. x

∈ X

. Equation (9) yields

g∗
n(z; x)

−

+ sup

z∈Z |

g∗
n(z; x)

g(z; x)
|

.

−

(cid:12)
(cid:12)

The second term on the r.h.s. of the above inequality vanishes in the limit in probability under Assumption 2.
If the ﬁrst term also vanishes in the limit in probability, by op(1) + op(1) = op(1), we obtain the desired
result. We now show that the ﬁrst term vanishes in the limit in probability.

n
i=1k

Since Assumption 1 holds, Lemma 1 implies the ﬁrst term converges to zero in probability whenever the
mean deviation term 1
0. Therefore, the desired result holds if each term on the r.h.s. of
n
inequality (10) converges to zero in probability. The ﬁrst term on the r.h.s. of (10) converges to zero in
probability by Assumption (5a), and the second term converges to zero in probability by Assumptions 4
and (5c). The third term on the r.h.s. of (10) converges to zero in probability by Assumptions 3, 4, (5c),
and (5d). Finally, the last term on the r.h.s. of inequality (10) converges to zero in probability by Assump-
tions 3, (5b), (5c), and (5d) and Lemma 3.

˜εi
n(x)
k

p
−→

P

Fewer assumptions are needed to establish Proposition 4 in the homoscedastic case.

In that setting,
Assumptions 3, 4, (5c), and (5d) are not required and Assumption (5b) may be weakened to the assumption
1
0 on account of Lemma 2. Proposition 4 provides the foundation for the following
n
result, which demonstrates that the optimal value and solutions of the ER-SAA problem (4) converge to
those of the true problem (1).

ˆfn(xi)
k

n
i=1k

f ∗(xi)

p
−→

P

−

12

Theorem 5. Suppose Assumptions 1 to 5 hold. Then, we have ˆvER
v∗(x) for a.e. x
and supz∈ ˆSER

n (x) g(z; x)

.

n (x)

p
−→

∈ X

p
−→

v∗(x), D

ˆSER
n (x), S∗(x)

p
−→

0,

(cid:0)

(cid:1)

The proof of Theorem 5 follows a similar outline as the proof of Theorem 5.3 of Shapiro et al. [73],
except that we consider convergence in probability rather than almost sure convergence. Under an inf-
compactness condition on the ER-SAA problem (4), the conclusions of Theorem 5 hold even if the set
is
unbounded [see the discussion following Theorem 5.3 of 73]. While we consider convergence in probability
instead of almost sure convergence (because the statistics literature is typically concerned with conditions
under which Assumption 5 holds rather than its almost sure counterpart), note that our results until this
point can be naturally extended to the latter setting by suitably strengthening Assumptions 2 to 5.

Z

Next, we identify conditions under which the optimal value of the ER-SAA problem (4) converges to the
optimal value of the true problem (1) on average over the space of the covariates. This can be important
in situations where the new covariate observations are random and the DM needs to make decisions facing
kLq to denote the Lq-norm of a
diﬀerent observations of the covariates. Given q
measurable function F :
. We require the following adaptation of
kLq :=
Assumptions 2 and 5.

], we write
1/q

∞
qdPX
k

[1, +
F

RdF , i.e.,

X →

Sk

∈

F

F

k

k

Assumption 6. The sequence of sample average functions

{

(cid:0)R

(cid:1)
g∗
n(
·

g(z; X)
|
(cid:13)
(cid:13)
(cid:13)

Lq

g∗
n(z; X)

sup
z∈Z|

−

(cid:13)
(cid:13)
(cid:13)

; x)
}

deﬁned in (2) satisﬁes

p
−→

0.

Assumption 7. The regression estimates ˆfn and ˆQn possess the following consistency properties:
(7a)

Q∗(X)

f ∗(X)

(7c)

0,

0,

−
f ∗(xi)

p
kLq
−→
ˆfn(xi)
k

−

ˆfn(X)
k
n
1
n

k

i=1
X

(7b)

2 p
−→

0,

(7d)

ˆQn(X)
k
n
1
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

−
ˆQn(xi)

kLq

−1

−

p
−→
Q∗(xi)

−1

2 p
−→

0.

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

Z × X

g∗
n(z; x)

Assumption 6 is implied by the uniform convergence in probability of the FI-SAA objective function g∗
n
p
to g on
g(z; x)
0. We show in Section E that this assumption holds
, i.e., sup(z,x)∈Z×X |
−→
|
for our running Examples 1 and 2 whenever E [
] <
is compact and bounded away
k
from the origin. Assumptions (7a) and (7c) hold for our running Example 2 if the estimates ˆθn and ˆπn
kLq < +
to be compact and
are weakly consistent,
X
lie in a compact set a.s. for n large enough
bounded away from the origin and assume that the estimates
(see Section F in the Appendix for details). Unlike Assumptions (5a) and (5c) that require ˆfn and ˆQn to
be pointwise consistent, Assumptions (7a) and (7c) only require ˆfn and ˆQn to be consistent on average over
the covariates X. We have the following result.

, and if, e.g., we additionally have the support

and the support

ˆπn}

∞

∞

X

−

X

k

{

k

ε

Theorem 6. Suppose Assumptions 1, 3, 4, 6, and 7 hold and
q

]. Then,

v∗(X)

g(ˆzER

0 and

[1, +

ˆvER
n (X)

−

p
−→

Lq

∈

∞

k
n (X); X)
(cid:13)
−
(cid:13)

(cid:13)
(cid:13)
3.3 Finite sample guarantees

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Q∗(X)
k
v∗(X)

Lq < +
p
0.
(cid:13)
Lq
−→
(cid:13)
(cid:13)
(cid:13)

for some constant

∞

We now establish a lower bound on the probability that solutions to the ER-SAA problem (4) are nearly
optimal to the true problem (1). Section C in the Appendix investigates the rate of convergence of ER-
SAA estimators under weaker assumptions. Our next assumption is motivated by the analysis in Section 2
of Homem-de-Mello [42] and Section 7.2.9 of Shapiro et al. [73].

Assumption 8. The full-information SAA problem (2) possesses the following uniform exponential bound
property: for any constant κ > 0 and a.e. x
, there exist positive constants K(κ, x) and β(κ, x) such
that P

K(κ, x) exp (

nβ(κ, x)),

∈ X

> κ

N.

n

g∗
n(z; x)

sup
z∈Z |

g(z; x)
|

−

n

≤

o

∀

∈

−

13

Z

for a.e. y

, y) is Lipschitz continuous on

Lemma 2.4 of Homem-de-Mello [42] provides conditions under which Assumption 8 holds [also see Sec-
tion 7.2.9 of 73]. In particular, Homem-de-Mello [42] shows that Assumption 8 holds whenever the func-
tion c(
with an integrable Lipschitz constant and some
·
εi
pointwise exponential bounds hold. When the errors
are i.i.d., Section 7.2.9 of Shapiro et al. [73]
presents conditions under which these pointwise exponential bound conditions are satisﬁed via Cram´er’s
large deviation theorem. Bryc and Dembo [18] present mixing conditions on the errors
under which
these assumptions are also satisﬁed (also see the references therein). The G¨artner-Ellis Theorem [see Sec-
tion 2.3 of 28] provides an alternative avenue for verifying Assumption 8 for non-i.i.d. errors
[24]. If
E [c(z, f ∗(x) + Q∗(x)ε)] is sub-Gaussian for
we also assume that the random variable c(z, f ∗(x) + Q∗(x)ε)
each z
, then we can characterize the dependence of β(κ, x) on κ, see Assumption (C4)
on page 396 and Theorem 7.67 of Shapiro et al. [73].

and a.e. x

∈ Y
{

∈ Z

∈ X

εi

εi

−

{

}

}

{

}

Proposition 20 in the Appendix shows that Assumption 8 holds for Example 1 whenever the errors

are
i.i.d. and sub-Gaussian (which includes zero-mean Gaussian). Unlike Assumption 2, Assumption 8 may not
hold when the distribution of the errors ε is heavy-tailed (heavy-tailed error distributions such as the Pareto
and Weibull distributions occur in ﬁnance, weather forecasting, and reliability engineering applications).

}

{

εi

Next, we require the following strengthening of Assumptions 3 and 4.

Assumption 9. For any constant κ > 0 and n
and ¯γQ(n, κ), with limn→∞ γQ(n, κ) =

∈

and limn→∞ ¯γQ(n, κ) =

∞

for each κ > 0, such that

∞

N, there exist positive constants JQ(κ), γQ(n, κ), ¯JQ(κ),

1
n

P

(cid:26)(cid:18)

n

i=1
X
(cid:13)
(cid:2)
(cid:13)
P

Q∗(xi)

−1

2

1/2

>

E

Q∗(X)

−1

2

1/2

+ κ

(cid:3)
n

(cid:19)

(cid:13)
(cid:13)
Q∗(xi)
k

4

k

1
n

(cid:19)

(cid:26)(cid:18)

i=1
X

(cid:16)
1/4

h(cid:13)
(cid:13)
>

(cid:2)
E

(cid:13)
(cid:3)
(cid:13)
Q∗(X)
k

i(cid:17)

4

1/4

+ κ

(cid:3)(cid:1)

k

(cid:2)

JQ(κ) exp(

−

γQ(n, κ)),

¯JQ(κ) exp(

−

¯γQ(n, κ)).

≤

(cid:27)

≤

(cid:27)

(cid:0)
Assumption 10. For any constant κ > 0 and n
and ¯γε(n, κ), with limn→∞ γε(n, κ) =

N, there exist positive constants Jε(κ), γε(n, κ), ¯Jε(κ),
for each κ > 0, such that

∈

and limn→∞ ¯γε(n, κ) =

∞

n

1
n

P

(cid:26)

i=1
X

1/4

> E [

εi

k

k

ε

] + κ
k

k

n

1
n

P

(cid:26)(cid:18)

εi

4

k

(cid:19)

k
i=1
X

> (E

4

ε

k

k

(cid:2)

(cid:3)

)1/4 + κ

∞

Jε(κ) exp(

−

γε(n, κ)),

¯Jε(κ) exp(

−

¯γε(n, κ)).

≤

(cid:27)

≤

(cid:27)

The ﬁrst part of Assumption 9 holds, e.g., if for each κ > 0 and n

γQ(n, κ) > 0 exist such that

N, constants JQ(κ) > 0 and

∈

P

1
n

(cid:26)

i=1
X

n

Q∗(xi)

−1

2 > E

Q∗(X)

−1

2

+ κ2

JQ(κ) exp(

−

≤

γQ(n, κ)).

i

(cid:27)

h(cid:13)
(cid:2)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:3)
The function γQ in the inequality above is related to the so-called rate function in large deviations theory
(see Dembo and Zeitouni [28] and Section 7.2.8 of [73]). Similar conclusions hold for the probability inequali-
n
ties involving the terms 1
4 in Assumptions 9 and 10. Using large deviations
i=1k
n
theory, we can show that the constants γQ(n, κ) and ¯γQ(n, κ) in Assumption 9 increase linearly with the
sample size n (i.e., γQ(n, κ) = nγQ,1(κ) and ¯γQ(n, κ) = n¯γQ,1(κ)) for our running Example 2 with i.i.d.
of the covariates is compact and bounded away from the origin. The
data
constant γε(n, κ) in Assumption 10 also increases linearly with n whenever the errors ε are sub-Gaussian
(see Chapter 3 of Vershynin [81]). The discussion following Assumption 8 provides avenues for verifying
Assumptions 9 and 10 for non-i.i.d. data

whenever the support

Q∗(xi)
k

(xi, εi)
}

4 and 1
n

n
i=1k

P

P

εi

X

{

k

.

Next, we make the following large deviation assumption on the regression procedure (3) that is similar

(xi, εi)
}

{

in spirit to Assumption 8.

14

Assumption 11. The regression estimates ˆfn and ˆQn possess the following ﬁnite sample properties: for
N, there exist positive constants Kf (κ, x), ¯Kf (κ), βf (n, κ, x), ¯βf (n, κ), KQ(κ, x),
any constant κ > 0 and n
¯KQ(κ), βQ(n, κ, x), and ¯βQ(n, κ), with limn→∞ βf (n, κ, x) =
, limn→∞ βQ(n, κ, x) =

, limn→∞ ¯βf (n, κ) =

∈
, and limn→∞ ¯βQ(n, κ) =

for each κ > 0 and a.e. x

∞

∈ X

∞
, such that

∞

∞
(11a) P

(11b) P

(11c) P

(11d) P

k
(cid:8)
1
n

(cid:26)

k
(cid:8)
1
n

(cid:26)

i=1
X
Q∗(x)

n

i=1
X

(cid:13)
(cid:2)
(cid:13)

f ∗(x)

ˆfn(x)
k

−

> κ

≤

Kf (κ, x) exp

−

βf (n, κ, x)

for a.e. x

,

∈ X

n

f ∗(xi)

(cid:9)
ˆfn(xi)
k

−

2 > κ2

≤

(cid:27)

k

(cid:0)

¯Kf (κ) exp

(cid:1)
¯βf (n, κ)

,

ˆQn(x)
k

−

> κ

≤

KQ(κ, x) exp

βQ(n, κ, x)

−
(cid:0)

(cid:1)

for a.e. x

,

∈ X

(cid:1)

¯KQ(κ) exp

¯βQ(n, κ)

.

−
(cid:0)

(cid:1)

−
(cid:0)
2 > κ2

≤

(cid:27)

ˆQn(xi)

−1

−

(cid:9)
Q∗(xi)

−1

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

We verify in Section F of the Appendix that Assumptions (11a) and (11b) hold for OLS regression and
the Lasso with constants βf (n, κ, x) and ¯βf (n, κ) scaling as O(nκ2) under sub-Gaussian assumptions on the
errors ε. The ﬁnite sample guarantees on the estimate ˆQn in Assumptions (11c) and (11d) are typically
harder to verify.

The next result provides conditions under which the maximum deviation of the ER-SAA objective from
satisﬁes a qualitatively similar large deviations

the full-information SAA objective on the feasible set
bound as that in Assumption 8.

Z

Lemma 7. Suppose Assumptions 1, 8 to 11 hold. Then for any constant κ > 0, n
there exist positive constants ¯K(κ, x) and ¯β(n, κ, x), with limn→∞ ¯β(n, κ, x) =
¯β(n, κ, x)
g∗
n(z; x)
x
.

, such that P

¯K(κ, x) exp

ˆgER
n (z; x)

> κ

∞

∈ X

−

≤

o

(cid:12)
(cid:12)

−

(cid:0)

(cid:1)

sup
z∈Z

n

(cid:12)
(cid:12)

We are now ready to present the main result of this section. It extends ﬁnite sample results that are known
for traditional SAA estimators, see, e.g., Theorem 2.3 of Homem-de-Mello [42] and Section 5.3 of Shapiro
et al. [73], to the ER-SAA setting.

N, and a.e. x
,
∈ X
for each κ > 0 and a.e.

∈

Theorem 8. Suppose Assumptions 1, 8 to 11 hold. Then, for each n
exist constants Q(η, x) > 0 and γ(n, η, x) > 0, with limn→∞ γ(n, η, x) =
such that P
η

Q(η, x) exp(

γ(n, η, x)).

dist(ˆzER

n (x), S∗(x))

∈

∞

≥

≤

−

N and a.e. x

for each η > 0 and a.e. x

, given η > 0, there
,

∈ X

∈ X

(cid:8)

(cid:9)

(cid:8)

−

≡

≤

P

n
i=1k

¯βf (n, κ)

¯Kf (κ) exp

ˆfn(xi)
k

homoscedastic case ( ˆQn := Q∗
P
f ∗(xi)
> κ

Assumptions 9, 10, (11c), and (11d) are not required to establish Lemma 7 and Theorem 8 in the
I). Additionally, Assumption (11b) can be weakened in this setting to
on account of Lemma 2.

1
n
To give an example of how Theorem 8 can be used to provide a qualitative estimate of sample size
required to obtain a desired accuracy with high probability, we now specialize the results in this section to
the homoscedastic setting when the ER-SAA formulation is applied to two-stage stochastic LP with OLS,
Lasso, or kNN regression for estimating f ∗. Given κ > 0, let Sκ(x) :=
denote
≤
the set of κ-optimal solutions to the true problem (1). Given an unreliability level δ
(0, 1), our goal is to
estimate the sample size n required for every solution to the ER-SAA problem (4) to be κ-optimal to the
δ.
1
true problem (1) with probability at least 1
In the following, we make stronger than necessary assumptions on the regression setups for readability.

δ, i.e., to estimate n such that P

v∗(x) + κ

: g(z; x)

n (x)

Sκ(x)

−
(cid:0)

ˆSER

z
{

∈ Z

−

⊆

−

≥

∈

(cid:9)

}

(cid:1)

Our sample size estimate for ER-SAA proceeds by estimating the sample size required for the full-
information SAA problem (2) to be ‘close to’ the true problem (1) and for the ER-SAA problem (4) to be
‘close to’ the FI-SAA problem (2); see also (9). From Section 5.3 of Shapiro et al. [73], we have the following
sample size estimate for every solution to the FI-SAA problem (2) to be κ-optimal to the true problem (1)
with probability at least 1

δ:

(cid:8)

(cid:9)

−

n∗ :=

n

≥

c (x)

O(1)σ2
κ2

dz log
(cid:20)

(cid:18)

O(1)D
κ

+ log

(cid:19)

(cid:18)

O(1)
δ

.

(cid:19)(cid:21)

15

Learning of the regression function f ∗ introduces additional terms in the estimate for ER-SAA that depend
on the dimensions dy and dx of the random vector Y and the random covariates X.

Proposition 9. Consider Example 1 and assume ˆQn = Q∗
and for each z
∈ X
εi
with variance proxy σ2
{
be the target optimality gap, and δ

is compact with diameter D
E [c(z, f ∗(x) + ε)] is sub-Gaussian
, the random variable c(z, f ∗(x) + ε)
−
}i∈[n] be i.i.d. sub-Gaussian random vectors with variance proxy σ2, κ > 0
∈

(0, 1) be the desired unreliability level.

and a.e. x
c (x). Let

I. Suppose

∈ Z

≡

Z

1. Suppose the regression function f ∗ is linear, the regression step (3) is OLS regression, the covariance
− 1
X X is sub-Gaussian. Then,
2

matrix ΣX of the covariates is positive deﬁnite, and the random vector Σ
we have P

δ for sample size n satisfying

Sκ(x)

ˆSER

1

n (x)

⊆

≥

−

(cid:8)

(cid:9)

n∗ +

n

≥

O(1)σ2dy
κ2

O(1)
δ

log

(cid:20)

(cid:18)

+ dx

.

(cid:19)

(cid:21)

2. Suppose the regression function f ∗ is linear with

Lasso regression, the support
matrix E
have P

−
Sκ(x)

τ diag(E

XX T
ˆSER
n (x)
(cid:2)

X
XX T
1
(cid:2)

≥

−

(cid:3)
⊆

(cid:3)

∈
k
of the covariates X is compact, E

> 0,
) is positive semideﬁnite for some constant τ

[dy], the regression step (3) is
Xj|
[dx], and the
∈
(0, 1]. Then, we

θ∗
[j]k0 ≤

j
∀

s,

2

|

(cid:2)

(cid:3)

j
∀
∈

δ for sample size n satisfying

(cid:8)

(cid:9)

n∗ +

n

≥

O(1)σ2sdy
κ2

O(1)
δ

log

(cid:20)

(cid:18)

+ log(dx)
(cid:21)

.

(cid:19)

3. Suppose the regression function f ∗ is Lipschitz continuous, the regression step (3) is kNN regression
of the covariates X is
and κ > 0.

with parameter k =
compact, and there exists a constant τ > 0 such that P

for some constant γ

O(1)nγ

X
τ κdx,

∈

x

⌈

⌉

(0, 1), the support
∈ Bκ(x)

X
{

} ≥

Then, we have P

ˆSER

n (x)

nγ

log(n) ≥

O(1)dxdyσ2
κ2

(cid:8)

, and

Sκ(x)

⊆

1

−

≥

(cid:9)

δ for sample size n satisfying n

≥

n∗

+

n

≥

(cid:18)

O(1)σ2dy
κ2

1
γ

(cid:19)

(cid:20)

dx log

O(1)
dx (cid:19)

(cid:18)

+ log

O(1)
δ

(cid:18)

1
γ

+

(cid:19)(cid:21)

(cid:18)

O(1)dy
κ2

dx

(cid:19)

(cid:20)

dx
2

log

(cid:18)

O(1)dxdy
κ2

+ log

(cid:19)

(cid:18)

O(1)
δ

(cid:19)(cid:21)

∀

∈ X
O(1)

dx
1−γ

O(1)
κ

(cid:19)

(cid:18)

,

.

Proposition 9 illustrates the tradeoﬀ between using parametric and nonparametric regression approaches
within the ER-SAA framework. The sample size estimates in Proposition 9 involve the sum of two contribu-
tions, the FI-SAA contribution n∗ and additional regression-related terms introduced by the use of estimates
of f ∗ within the ER-SAA. Assuming that the regression function f ∗ satisﬁes the necessary structural prop-
erties, using OLS regression or the Lasso for the regression step (3) can yield sample size estimates that
depend modestly on the accuracy κ and the dimensions dx and dy compared to kNN regression. On the
other hand, unlike OLS and Lasso regression, the sample size estimates for kNN regression are valid under
mild assumptions on the regression function f ∗. Nevertheless, we empirically demonstrate in Section 4 that
it may be beneﬁcial to use a structured but misspeciﬁed prediction model when we do not have an abun-
dance of data. Note that the OLS estimate includes a term that depends linearly on the dimension dx of
the covariates, whereas the corresponding term in the Lasso estimate only depends logarithmically on dx.

3.4 Outline of analysis for the jackknife-based estimators

The results thus far carry over to the J-SAA and J+-SAA estimators if the assumptions that ensure that the
ER-SAA mean deviation term 1
0 at a certain rate are adapted to ensure that the J-SAA
n

n
i=1k

˜εi
n(x)
k

p
−→

P

16

and J+-SAA mean deviation terms 1
n
at a certain rate, where

n
i=1k

˜εi,J
n (x)
k

and 1
n

n
i=1k

˜εi,J+
n

(x)
k

P

P

converge to zero in probability

˜εi,J
n (x) :=

˜εi,J+
n

(x) :=

n,J

ˆfn(x) + ˆQn(x)ˆεi
(cid:16)
ˆf−i(x) + ˆQ−i(x)ˆεi
(cid:16)

(cid:17)
n,J

f ∗(x) + Q∗(x)εi

,

−

i
∀

(cid:1)
f ∗(x) + Q∗(x)εi

,

(cid:0)
−

[n],

∈
i
∀

∈

[n].

(cid:17)
Lemma 14 in Section D of the Appendix presents the analogue of Lemma 2 for the jackknife-based mean
deviation terms. It also provides guidance for how the assumptions on the quantities appearing in inequal-
ity (10) could be replaced with assumptions on the quantities appearing in the bounds of the jackknife-based
mean deviation terms to derive similar results for the J-SAA and J+-SAA estimators as the ER-SAA esti-
mator. Since the formal statements of the assumptions and results for the J-SAA and J+-SAA estimators
closely mirror those for the ER-SAA given in Sections 3.2 and 3.3, we present these details in Section D of
the Appendix.

(cid:1)

(cid:0)

4 Computational experiments

We consider instances of the following resource allocation model adapted from Luedtke [56]:

where the second-stage function is deﬁned as

z z + E [Q(z, Y )] ,
cT

min
z∈R|I|
+

Q(z, y) :=

min
v∈R|I|×|J |
+

,w∈R|J |

qT
ww :

vij ≤

ρizi,

i
∀

,

∈ I

µij vij + wj ≥

yj,

j
∀

∈ J

.

Xj∈J

+ n

Xi∈I
The ﬁrst-stage variables zi denote the order quantities of resources i
, and the second-stage variables
vij and wj denote the amount of resource i
and the unmet demand
∈ J
of customer type j, respectively. We consider instances with
= 30. The yield and service
rate parameters ρ and µ and the cost coeﬃcients cz and qw are assumed to be deterministic. Parameters
cz, ρ, and µ are set using the procedure described in Luedtke [56], and the coeﬃcients qw are determined by
qw := τ
(0.5, 0.05)
distribution.

czk∞, where each component of the vector τ is drawn independently from a lognormal
= R|J |
that some of the variability in the demands can be explained with knowledge of covariates Xl, l

, of the customer types are considered to be stochastic with

+ . We assume
, where

allocated to customer type j

The demands yj, j

= 20 and

∈ J

∈ I

∈ I

LN

|J |

|I|

Y

o

k

∈ L

= dx. The demands Y are assumed to be related to the covariates through

|L|

Yj = ϕ∗

j +

jl(Xl)p + q∗
ζ∗

j (X)εj,

Xl∈L∗

j
∀

,

∈ J

|

{

1/2

(cid:1)(cid:1)

∈ {

(cid:0)P

X →

where p

}
jl log

R+ is deﬁned as q∗

for parameters

0.5, 1, 2
l∈L∗ π∗

is a ﬁxed parameter that determines the model class, q∗
j :
π∗
jl}l∈L∗ [66], errors εj ∼ N
1 + Xl|
sj exp
∗
ϕ∗, ζ∗ and σj are additional model parameters, and
(cid:0)
L
∗ does not depend on j
with predictive power (note that
sj is a numerical approximation of the median of exp
The form of the heteroscedasticity functions q∗
(cid:0)P
magnitude of the covariates [66]. Throughout, we assume
three covariates. We simulate i.i.d. data
unless otherwise speciﬁed, and draw covariate samples
(see Section G in the Appendix for details). We vary the heteroscedasticity level ω
2, and 3 correspond to zero, moderate, and severe heteroscedasiticity, respectively, and sample

j (X) =
are independent of X,
contains the indices of a subset of covariates
(cid:1)
(cid:0)
). In the deﬁnition of q∗
j (X), the scaling factor
so that PX (q∗
0.5.
j (X) > 1)
1 + Xl|
j is chosen to simulate increasing error variance with increasing
(cid:1)
= 3, i.e., the demands truly depend only on
,
∈ J
n
i=1 from a multivariate folded normal distribution
, where ω = 1,
π∗
jl}l∈L∗

with ϕ∗ and ζ∗ randomly generated, σj = σ = 5,

⊆ L
∈ J
l∈L∗ π∗

(xi, εi)
}

0, σ2
j

jl log

∗
|L

1, 2, 3

j
∀

∈ {

xi

≈

L

{

}

{

}

{

|

|

17

j

*
q

10

5

2

1

0.5

0.2

0.1

1

ω = 2
ω = 3

10

20

30

j

Figure 1: Median (dotted line) and the 10th and 90th quantiles (dashed lines for ω = 2 [moderate het-
eroscedasticity] and solid lines for ω = 3 [severe heteroscedasticity]) of the heteroscedasticity function q∗
j
over 1100 independent realizations of the covariates. The y-axis is in logarithmic scale and the indices j are
reordered to generate approximately monotonic plots.

independently from the uniform distribution U (0, 2(ω
moderate and severe heteroscedasticity.

−

1)2). Figure 1 plots statistics of the functions q∗

j for

Given data

Dn on the demands and covariates, we estimate the coeﬃcients of the linear model
ζjlXl + ηj,

Yj = ϕj +

,

j
∀

∈ J

Xl∈L

,

⌈

⌋

⌉

n0.1

n0.9

where ηj are zero-mean errors, using OLS or Lasso regression and use this prediction model within the ER-
SAA, J-SAA, and J+-SAA frameworks. We use this linear prediction model even when the degree p
= 1, in
which case the prediction model is misspeciﬁed. We also evaluate the performance of the ER-SAA approach
when kNN regression is used to predict Y from X (the parameter k in kNN regression is chosen from
to minimize the 5-fold CV test error). We compare these ER-SAA and J-SAA estimators
⌊
with the point prediction-based (PP) deterministic approximation (6) that uses OLS or Lasso regression
(cid:2)
by default in our
(assuming a linear model). While we do not estimate the heteroscedasticity functions
experiments (equivalent to setting ˆQn ≡
I), we also compare the performance of ER-SAA formulations that
ignore heteroscedasticity with heteroscedasticity-aware ER-SAA formulations where each q∗
j is estimated
using Lasso regression (cf. Example 2). When the function f ∗ is estimated using OLS regression, we use the
estimate of Q∗ to update the estimate ˆfn of f ∗ using weighted least squares [66]. While estimating Q∗, we
assume that its parametric form is known; model misspeciﬁcation for f ∗ also simulates model misspeciﬁcation
for heteroscedasticity in our two-step approach for estimating Q∗.

q∗
j }

{

(cid:3)

We compare our data-driven SAA estimators with the kNN-based reweighted SAA (kNN-SAA) approach
of Bertsimas and Kallus [12] on a few test instances by varying the dimensions of the covariates dx, the
sample size n, the degree p, the standard deviation σ of the errors ε, and the degree of heteroscedasticity ω.
While our case studies illustrate the potential advantages of employing parametric regression models (such
as OLS and the Lasso) within our data-driven formulations, we do not claim that this advantage holds for
arbitrary model instances. We choose the kNN-SAA approach to compare against because it is easy to
implement and tune this approach, and the empirical results of Bertsimas and Kallus [12] and Bertsimas
and McCord [13] show that this approach is one of the better performing reweighted SAA approaches.The
parameter k in kNN-SAA is once again chosen from

n0.1
⌊
Solutions from the diﬀerent approaches are compared by estimating a normalized version of the upper
(cid:2)
bound of a 99% conﬁdence interval (UCB) on their optimality gaps using the multiple replication procedure

to minimize the 5-fold CV test error.

n0.9

⌉

⌋

⌈

(cid:3)

,

18

6
of Mak et al. [57] (see Section G for details). We choose to compare 99% UCBs of the diﬀerent estimators
as it provides a conservative estimate of their suboptimality and mitigates the variability of a single random
evaluation sample. Because the data-driven solutions depend on the realization of samples
Dn, we perform
100 replications per test instance and report our results in the form of box plots of these UCBs (the boxes
denote the 25th, 50th, and 75th percentiles of the 99% UCBs, and the whiskers denote their 5th and 95th
percentiles over the 100 replicates).

Source code and data for the test instances are available at https://github.com/rohitkannan/DD-SAA.
Our codes are written in Julia 0.6.4 [15], use Gurobi 8.1.0 to solve LPs through the JuMP 0.18.5 interface [33],
and use glmnet 0.3.0 [39] for Lasso regression. All computational tests were conducted through the UW-
Madison high throughput computing software HTCondor (http://chtc.cs.wisc.edu/).

≡

3, 10, 100

, and the sample size among n

. Note that OLS regression estimates dx + 1 parameters for each j

}
I, but directly assume ˆQn ≡

Eﬀect of varying covariate dimension. Figure 2 compares the performance of the kNN-SAA, ER-
SAA+kNN, ER-SAA+OLS, and PP+OLS approaches for the homoscedastic case (ω = 1) by varying
the model degree p, the covariate dimension among dx ∈ {
∈
5(dx + 1), 20(dx + 1), 100(dx + 1)
.
∈ J
{
}
The ER-SAA approaches in this case study do not estimate Q∗
I. The
kNN-SAA and ER-SAA+kNN approaches exhibit similar performance overall. When the prediction model
is correctly speciﬁed (i.e., p = 1), the ER-SAA+OLS approach unsurprisingly dominates the ER-SAA+kNN
and kNN-SAA approaches. When p
= 1, as anticipated, the ER-SAA+OLS approach does not yield a con-
sistent estimator, whereas the ER-SAA+kNN and kNN-SAA approaches yields consistent estimators, albeit
with a slow rate of convergence (cf. Proposition 9). However, the ER-SAA+OLS approach consistently
outperforms the ER-SAA+kNN and kNN-SAA approaches when p = 0.5 even for the largest sample size
of n = 100(dx + 1). When the degree p = 2, the ER-SAA+kNN and kNN-SAA approaches fare better
than the ER-SAA+OLS approach only for a sample size of n
80 when the covariate dimension is small
(dx = 3), and lose this advantage in the larger covariate dimensions. Although the PP+OLS does not
yield a consistent estimator even when p = 1, it performs better than the ER-SAA+kNN and kNN-SAA
approaches in many cases even for large sample sizes n. However, the ER-SAA+OLS approach outperforms
the PP+OLS approach across all cases. While we do not show results, we mention that the N-SAA estimator
is not asymptotically optimal for all three model instances with the median values of the 99% UCBs of its
percentage optimality gaps being about 11%, 5%, and 26% for the p = 1, p = 0.5, and p = 2 instances,
respectively, for n = 10100. This indicates that using covariate information can be advantageous in these
instances.

≥

}

, and the sample size among n

Impact of the jackknife-based formulations. Figure 3 compares the performance of the ER-SAA
and J-SAA approaches with OLS regression by varying the model degree p, the covariate dimension among
dx ∈ {
for ω = 1. Once again, we
1.3(dx + 1), 1.5(dx +1), 2(dx + 1)
10, 100
}
directly assume ˆQn ≡
I. We employ smaller sample sizes in these experiments to see if the jackknife-based
SAAs perform better in the limited data regime. We observe that the solutions obtained from the J-SAA
formulation typically have smaller 75th and 95th percentiles of the 99% UCBs than those from the ER-SAA
formulation, particularly when the sample size n is small. Performance gains are more pronounced for larger
sample sizes when the covariate dimension is larger (dx = 100), possibly because the OLS estimators overﬁt
more. Note that, as expected, the J-SAA results converge to the ER-SAA results when the sample size
increases. We do not plot the results for the J+-SAA formulation because they are similar to those of the
J-SAA formulation.

∈ {

Impact of the prediction setup. Figure 3 also compares the performance of the ER-SAA+OLS, ER-
SAA+Lasso, and PP+Lasso approaches by varying the model degree p, the covariate dimension among
dx ∈ {
for ω = 1. We again
, and the sample size among n
10, 100
}
assume ˆQn ≡
I in the ER-SAA approaches. We observe that the ER-SAA+Lasso formulation yields better
estimators than the ER-SAA+OLS formulation when the sample size n is small relative to the covariate
dimension dx. This eﬀect is accentuated when the covariate dimension is larger (dx = 100), in which case the

1.3(dx + 1), 1.5(dx + 1), 2(dx + 1)
}

∈ {

19

6
dx = 3

12

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

8

6

4

2

0

R k

O P

R k

O P

R k

O P

12

10

8

6

4

2

0

dx = 10

R k

O P

R k

O P

R k

O P

12

10

8

6

4

2

0

dx = 100

R k

O P

R k

O P

n =

20

80

400

n =

55

220

1100

n =

505

2020

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

8

6

4

2

0

8

6

4

2

0

R k

O P

R k

O P

R k

O P

8

6

4

2

0

R k

O P

R k

O P

R k

O P

R k

O P

R k

O P

n =

20

80

400

n =

55

220

1100

n =

505

2020

30

25

20

15

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

5

0

R k

O P

R k

O P

R k

O P

30

25

20

15

10

5

0

R k

O P

R k

O P

R k

O P

30

25

20

15

10

5

0

R k

O P

R k

O P

n =

20

80

400

n =

55

220

1100

n =

505

2020

R k

O P

10100

R k

O P

10100

R k

O P

10100

Figure 2: Comparison of kNN-SAA (R), ER-SAA+kNN (k), ER-SAA+OLS (O), and PP+OLS (P) approaches
for ω = 1. Top row: p = 1. Middle row: p = 0.5. Bottom row: p = 2. Left column: dx = 3. Middle column:
dx = 10. Right column: dx = 100.

OLS-based estimators overﬁt more and there is increased beneﬁt in using the Lasso to ﬁt a sparser model.
The advantage of the Lasso-based estimators shrinks as the sample size increases. The ER-SAA+Lasso
approach outperforms the PP+Lasso approach with increased gains for larger sample sizes and covariate
dimensions.

Impact of the error variance. Figure 4 and Figure 7 in Section G of the Appendix compare the perfor-
mance of the kNN-SAA, ER-SAA+kNN, ER-SAA+OLS, and PP+OLS approaches by varying the standard
(the case studies thus far used σ = 5), the model degree p,
deviation of the errors ε among σ
and the sample size among n
for dx = 10 and ω = 1. We observe that
the ER-SAA+OLS formulation needs a larger sample size to yield a similar certiﬁcate of optimality as the
standard deviation σ increases. On the other hand, the performance of the ER-SAA+kNN and kNN-SAA
approaches appear to be unaﬀected (and even slightly improve!) with increasing error variance. The perfor-
mance of the PP+OLS approach deteriorates signiﬁcantly with increasing σ, especially for p
= 2, and it no
longer dominates the ER-SAA+kNN and kNN-SAA approaches for σ = 20.

5(dx + 1), 20(dx + 1), 100(dx + 1)
}

5, 10, 20

∈ {

∈ {

}

20

 
 
 
 
 
 
 
 
 
 
 
 
6
p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

12

10

10

dx = 10

8

6

4

2

0

J O

L P'

J O

L P'

J O

L P'

10

8

6

4

2

0

dx = 100

J O

L P'

J O

L P'

J O

L P'

n =

14

16

22

n =

131

152

202

8

6

4

2

0

J O

L P'

J O

L P'

J O

L P'

12

10

8

6

4

2

0

J O

L P'

J O

L P'

J O

L P'

n =

14

16

22

n =

131

152

202

50

40

30

20

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

0

J O

L P'

J O

L P'

J O

L P'

50

40

30

20

10

0

J O

L P'

J O

L P'

J O

L P'

n =

14

16

22

n =

131

152

202

Figure 3: Comparison of J-SAA+OLS (J), ER-SAA+OLS (O), ER-SAA+Lasso (L), and PP+Lasso (P′)
approaches for ω = 1. Top row: p = 1. Middle row: p = 0.5. Bottom row: p = 2. Left column: dx = 10.
Right column: dx = 100.

σ = 5

12

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

8

6

4

2

0

R k

O P

R k

O P

R k

O P

12

10

8

6

4

2

0

σ = 10

R k

O P

R k

O P

R k

O P

12

10

8

6

4

2

0

σ = 20

R k

O P

R k

O P

R k

O P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

Figure 4: Eﬀect of increasing σ on kNN-SAA (R), ER-SAA+kNN (k), ER-SAA+OLS (O), and PP+OLS (P)
approaches when dx = 10, p = 1, and ω = 1.

21

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
}

∈ {

∈ {

1, 2, 3

Impact of heteroscedasticity. Figures 5 and 6 compare the performance of the kNN-SAA approach,
ER-SAA+kNN approaches with and without estimation of Q∗, ER-SAA+OLS approaches with and without
estimation of Q∗, and the PP+OLS approach by varying the heteroscedasticity level ω
, the model
degree p, and the sample size among n
for dx = 10 and σ = 5. We assume
5(dx+1), 20(dx+1), 100(dx+1)
}
ˆQn ≡
I in the ER-SAA approaches that do not estimate Q∗. The kNN-SAA approach and the ER-SAA+kNN
approach without heteroscedasticity estimation again exhibit similar performance across the diﬀerent test
instances, with the former performing slightly better on instances with severe heteroscedasticity (ω = 3) and
the latter performing slightly better on instances with zero and moderate heteroscedasticity (ω
).
The ER-SAA+kNN approach with heteroscedasticity estimation outperforms the kNN-SAA approach and
the ER-SAA+kNN approach without heteroscedasticity estimation, particularly for model degree p = 2 and
for severe heteroscedasticity (ω = 3). The ER-SAA+OLS approach without heteroscedasticity estimation
outperforms the kNN-SAA approach and the ER-SAA+kNN approaches for ω = 1 and ω = 2 irrespective
of the model degree p. However, the kNN-SAA approach outperforms the ER-SAA+OLS approach without
heteroscedasticity estimation for ω = 3, p
, and small sample sizes, with the reverse holding true
for large sample sizes. Similar to the observation for kNN regression, the ER-SAA+OLS approach with
estimation of Q∗ outperforms the ER-SAA+OLS approach without estimation of Q∗ in several instances,
particularly for large sample sizes and for severe heteroscedasticity (ω = 3). Finally, the ER-SAA+OLS
approaches demonstrate signiﬁcant gains over the PP+OLS approach, especially when the degree of model
misspeciﬁcation is low (p = 1 or p = 0.5). The results for the PP+OLS and ER-SAA+OLS approaches
for ω = 3 may be partly explained by an increase in the variance of the error terms (cf. Figure 1). Note
that the median values of the 99% UCBs of the percentage optimality gaps of the N-SAA estimator (for
n = 10100) for both ω = 2 and ω = 3 are roughly 11%, 5%, and 25% for p = 1, 0.5, and 2, respectively,
which underscores the beneﬁt of using covariate information.

0.5, 1

∈ {

∈ {

1, 2

}

}

5 Conclusion and future work

We propose three data-driven SAA frameworks for approximating the solution to two-stage stochastic pro-
grams when the DM has access to a ﬁnite number of samples of random variables and concurrently observed
covariates. These formulations ﬁt a model to predict the random variables given covariate values, and use the
prediction model and its (out-of-sample) residuals on the given data to construct scenarios for the original
stochastic program at a new covariate realization. We provide conditions on the prediction and optimization
frameworks and the data generation process under which these data-driven estimators are asymptotically
optimal, possess a certain rate of convergence, and possess ﬁnite sample guarantees. In particular, we show
that our assumptions hold for two-stage stochastic LP in conjunction with popular regression setups such as
OLS, Lasso, kNN, and RF regression under various assumptions on the data generation process. Numerical
experiments demonstrate the beneﬁts of our data-driven SAA frameworks, in particular, those of our new
data-driven formulations in the limited data regime.

Verifying the assumptions on the prediction setup for other frameworks of interest is an important task
to be undertaken by the DM. Ongoing work includes analysis of an extension of the ER-SAA approach to
multistage stochastic programming [cf. 5, 13]. Designing asymptotically optimal estimators for problems with
stochastic constraints [44] and problems with decision-dependent uncertainty [12] are interesting avenues
for future work. Modiﬁcations of the ER-SAA that achieve better performance guarantees for a given
covariate x

would also be interesting to explore.

∈ X

Acknowledgments

This research is supported by the Department of Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc
Computing Research, Applied Mathematics program under Contract Number DE-AC02-06CH11357, and
was performed using the computing resources of the UW-Madison Center For High Throughput Computing
(CHTC) in the Dept. of Computer Sciences. The CHTC is supported by UW-Madison, the Advanced

22

ω = 1

5

0

n =

R

k k'
55

R

k k'

220

R

k k'

1100

8

6

4

2

0

n =

R

k k'
55

R

k k'

220

R

k k'

1100

20

15

10

12

10

30

20

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

ω = 2

R

k k'
55

R

k k'

220

R

k k'

1100

R

k k'
55

R

k k'

220

R

k k'

1100

20

15

10

5

0

n =

12

10

8

6

4

2

0

n =

30

20

10

ω = 3

R

k k'
55

R

k k'

220

R

k k'

1100

R

k k'
55

R

k k'

220

R

k k'

1100

20

15

10

5

0

n =

12

10

8

6

4

2

0

n =

30

20

10

0

n =

R

k k'
55

R

k k'

220

R

k k'

1100

0

n =

R

k k'
55

R

k k'

220

R

k k'

1100

0

n =

R

k k'
55

R

k k'

220

R

k k'

1100

Figure 5: Comparison of kNN-SAA (R), ER-SAA+kNN without heteroscedasticity estimation (k) and ER-
SAA+kNN with heteroscedasticity estimation (k′) approaches for dx = 10. Top row: p = 1. Middle row:
p = 0.5. Bottom row: p = 2. Left column: ω = 1. Middle column: ω = 2. Right column: ω = 3.

Computing Initiative, the Wisconsin Alumni Research Foundation, the Wisconsin Institutes for Discovery,
and the National Science Foundation. R.K. also gratefully acknowledges the support of the U.S. Department
of Energy through the LANL/LDRD Program and the Center for Nonlinear Studies. The authors thank the
three anonymous reviewers, the associate editor, and Prof. Erick Delage for suggestions that helped improve
this paper. R.K. also thanks Dr. Rui Chen and Prof. Garvesh Raskutti for helpful discussions.

References

[1] D. W. Andrews and D. Pollard. An introduction to functional central limit theorems for dependent stochastic processes.

International Statistical Review, 62(1):119–132, 1994.

[2] M. A. Arcones. Limit theorems for nonlinear functionals of a stationary Gaussian sequence of vectors. The Annals of

Probability, 22(4):2242–2274, 1994.

[3] M. A. Arcones and B. Yu. Central limit theorems for empirical and U-processes of stationary mixing sequences. Journal

of Theoretical Probability, 7(1):47–71, 1994.

[4] G.-Y. Ban and C. Rudin. The big data newsvendor: Practical insights from machine learning. Operations Research, 67

(1):90–108, 2018.

23

 
 
 
 
 
 
 
 
 
 
 
 
p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

3.0

2.5

2.0

1.5

1.0

0.5

0.0

ω = 1

R O

O' P

R O

O' P

R O

O' P

3.0

2.5

2.0

1.5

1.0

0.5

0.0

ω = 2

R O

O' P

R O

O' P

R O

O' P

20

15

10

5

0

ω = 3

R O

O' P

R O

O' P

R O

O' P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

5

4

3

2

1

0

5

4

3

2

1

0

R O

O' P

R O

O' P

R O

O' P

20

15

10

5

0

R O

O' P

R O

O' P

R O

O' P

R O

O' P

R O

O' P

R O

O' P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

25

20

15

10

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

5

0

R O

O' P

R O

O' P

R O

O' P

25

20

15

10

5

0

R O

O' P

R O

O' P

R O

O' P

25

20

15

10

5

0

R O

O' P

R O

O' P

R O

O' P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

Figure 6: Comparison of kNN-SAA (R), ER-SAA+OLS without heteroscedasticity estimation (O), ER-
SAA+OLS with heteroscedasticity estimation (O′) and PP+OLS (P) approaches for dx = 10. Top row:
p = 1. Middle row: p = 0.5. Bottom row: p = 2. Left column: ω = 1. Middle column: ω = 2. Right
column: ω = 3.

[5] G.-Y. Ban, J. Gallien, and A. J. Mersereau. Dynamic procurement of new products with covariate information: The

residual tree method. Manufacturing & Service Operations Management, 21:798–815, 2019.

[6] R. F. Barber, E. J. Candes, A. Ramdas, and R. J. Tibshirani. Predictive inference with the jackknife+. The Annals of

Statistics, 49:486–507, 2021.

[7] S. Basu and G. Michailidis. Regularized estimation in sparse high-dimensional time series models. The Annals of Statistics,

43(4):1535–1567, 2015.

[8] L. Bauwens, S. Laurent, and J. V. Rombouts. Multivariate GARCH models: a survey. Journal of applied econometrics,

21(1):79–109, 2006.

[9] T. Bazier-Matte and E. Delage. Generalization bounds for regularized portfolio selection with market side information.

INFOR: Information Systems and Operational Research, 58(2):374–401, 2020.

[10] A. Belloni, D. Chen, V. Chernozhukov, and C. Hansen. Sparse models and methods for optimal instruments with an

application to eminent domain. Econometrica, 80(6):2369–2429, 2012.

[11] A. Belloni, V. Chernozhukov, and L. Wang. Pivotal estimation via square-root lasso in nonparametric regression. The

Annals of Statistics, 42(2):757–788, 2014.

[12] D. Bertsimas and N. Kallus. From predictive to prescriptive analytics. Management Science, 66(3):1025–1044, 2020.

24

 
 
 
 
 
 
 
 
 
 
 
 
[13] D. Bertsimas and C. McCord. From predictions to prescriptions in multistage optimization problems. arXiv preprint

arXiv:1904.11637, pages 1–38, 2019.

[14] D. Bertsimas, C. McCord, and B. Sturt. Dynamic optimization with side information. European Journal of Operational

Research, 2022.

[15] J. Bezanson, A. Edelman, S. Karpinski, and V. Shah. Julia: a fresh approach to numerical computing. SIAM Review, 59

(1):65–98, 2017.

[16] G. Biau and L. Devroye. Lectures on the nearest neighbor method, volume 246. Springer, 2015.

[17] J. R. Birge and F. Louveaux. Introduction to stochastic programming. Springer Science & Business Media, 2011.

[18] W. Bryc and A. Dembo. Large deviations and strong mixing. In Annales de l’IHP Probabilit´es et statistiques, volume 32,

pages 549–569, 1996.

[19] F. Bunea, A. Tsybakov, M. Wegkamp, et al. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 1:

169–194, 2007.

[20] R. J. Carroll and D. Ruppert. Robust estimation in heteroscedastic linear models. The Annals of Statistics, pages 429–441,

1982.

[21] S. Chatterjee. Assumptionless consistency of the Lasso. arXiv preprint arXiv:1303.5817, 2013.

[22] G. H. Chen and D. Shah. Explaining the success of nearest neighbor methods in prediction. Foundations and Trends in

Machine Learning, 10(5-6):337–588, 2018.

[23] C. Chesneau, S. El Kolei, J. Kou, and F. Navarro. Nonparametric estimation in a regression model with additive and

multiplicative noise. Journal of Computational and Applied Mathematics, page 112971, 2020.

[24] L. Dai, C. Chen, and J. Birge. Convergence properties of two-stage stochastic programming. Journal of Optimization

Theory and Applications, 106(3):489–509, 2000.

[25] A. Dalalyan, M. Hebiri, K. Meziani, and J. Salmon. Learning heteroscedastic models by convex programming under group

sparsity. In Proceedings of the 30th International Conference on Machine Learning, pages 379–387, 2013.

[26] D. Davarnia, B. Kocuk, and G. Cornu´ejols. Computational aspects of Bayesian solution estimators in stochastic optimiza-

tion. INFORMS Journal on Optimization, 2:256–272, 2020.

[27] M. Davidian and R. J. Carroll. Variance function estimation. Journal of the American statistical association, 82(400):

1079–1091, 1987.

[28] A. Dembo and O. Zeitouni. Large deviations techniques and applications, volume 38 of Stochastic Modelling and Applied

Probability. Springer, 2nd edition, 2010.

[29] S. Diao and S. Sen. Distribution-free algorithms for learning enabled optimization with non-parametric estimation. Opti-

mization Online. URL: http://www.optimization-online.org/DB_HTML/2020/03/7661.html , 2020.

[30] P. Donti, B. Amos, and J. Z. Kolter. Task-based end-to-end model learning in stochastic optimization. In Advances in

Neural Information Processing Systems, pages 5484–5494, 2017.

[31] X. Dou and M. Anitescu. Distributionally robust optimization with correlated data from vector autoregressive processes.

Operations Research Letters, 47(4):294–299, 2019.

[32] P. Doukhan, P. Massart, and E. Rio. Invariance principles for absolutely regular empirical processes. In Annales de l’IHP

Probabilit´es et statistiques, volume 31, pages 393–427, 1995.

[33] I. Dunning, J. Huchette, and M. Lubin. JuMP: A modeling language for mathematical optimization. SIAM Review, 59

(2):295–320, 2017.

[34] O. El Balghiti, A. N. Elmachtoub, P. Grigas, and A. Tewari. Generalization bounds in the predict-then-optimize framework.

Advances in neural information processing systems, 32, 2019.

[35] A. N. Elmachtoub and P. Grigas. Smart “predict, then optimize”. Management Science, 68(1):9–26, 2022.

[36] A. Esteban-P´erez and J. M. Morales. Distributionally robust stochastic programs with side information based on trimmings.

Mathematical Programming, pages 1–37, 2021.

[37] J. Fan and Q. Yao. Nonlinear time series: nonparametric and parametric methods. Springer Science & Business Media,

2008.

[38] J. Fan, L. Qi, and D. Xiu. Quasi-maximum likelihood estimation of GARCH models with heavy-tailed likelihoods. Journal

of Business & Economic Statistics, 32(2):178–191, 2014.

[39] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent.

Journal of Statistical Software, 33(1):1–22, 2010.

[40] P. Grigas, M. Qi, and Z.-J. Shen. Integrated conditional estimation-optimization, 2021.

[41] L. Gy¨orﬁ, M. Kohler, A. Krzyzak, and H. Walk. A distribution-free theory of nonparametric regression. Springer Science

& Business Media, 2006.

[42] T. Homem-de-Mello. On rates of convergence for stochastic optimization problems under non–independent and identically

distributed sampling. SIAM Journal on Optimization, 19(2):524–551, 2008.

25

[43] T. Homem-de Mello and G. Bayraksan. Monte Carlo sampling-based methods for stochastic optimization. Surveys in

Operations Research and Management Science, 19(1):56–85, 2014.

[44] T. Homem-de-Mello and G. Bayraksan. Stochastic constraints and variance reduction techniques. In M. C. Fu, editor,

Handbook of Simulation Optimization, pages 245–276. Springer New York, 2015.

[45] D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. In Conference on learning theory, pages

9–1, 2012.

[46] Y. Hu, N. Kallus, and X. Mao. Fast rates for contextual linear optimization. Management Science, 68:4236–4245, 2022.

[47] H. Jiang. Non-asymptotic uniform rates of consistency for k-NN regression. In Proceedings of the AAAI Conference on

Artiﬁcial Intelligence, volume 33, pages 3999–4006, 2019.

[48] S. Jin, L. Su, and Z. Xiao. Adaptive nonparametric regression with conditional heteroskedasticity. Econometric Theory,

31(6):1153, 2015.

[49] N. Kallus and X. Mao. Stochastic optimization forests. Management Science, 2022. Articles in Advance.

[50] R. Kannan, G. Bayraksan, and J. R. Luedtke. Residuals-based distributionally robust optimization with covariate infor-

mation. arXiv preprint arXiv:2012.01088, 2020.

[51] R. Kannan, G. Bayraksan, and J. Luedtke. Heteroscedasticity-aware residuals-based contextual stochastic optimization.

arXiv preprint arXiv:2101.03139, 2021.

[52] K. Kim and S. Mehrotra. A two-stage stochastic integer programming approach to integrated staﬃng and scheduling with

application to nurse management. Operations Research, 63(6):1431–1451, 2015.

[53] V. Koltchinskii. The Dantzig selector and sparsity oracle inequalities. Bernoulli, 15(3):799–828, 2009.

[54] S. N. Lahiri. Resampling methods for dependent data. Springer Science & Business Media, 2013.

[55] D. Lewandowski, D. Kurowicka, and H. Joe. Generating random correlation matrices based on vines and extended onion

method. Journal of multivariate analysis, 100(9):1989–2001, 2009.

[56] J. Luedtke. A branch-and-cut decomposition algorithm for solving chance-constrained mathematical programs with ﬁnite

support. Mathematical Programming, 146(1-2):219–244, 2014.

[57] W.-K. Mak, D. P. Morton, and R. K. Wood. Monte Carlo bounding techniques for determining solution quality in stochastic

programs. Operations Research Letters, 24(1-2):47–56, 1999.

[58] R. Mazumder, A. Choudhury, G. Iyengar, and B. Sen. A computational framework for multivariate convex regression and

its variants. Journal of the American Statistical Association, 114(525):318–331, 2019.

[59] M. C. Medeiros and E. F. Mendes.

ℓ1-regularization of high-dimensional time-series models with non-Gaussian and

heteroskedastic errors. Journal of Econometrics, 191(1):255–271, 2016.

[60] S. N. Negahban, P. Ravikumar, M. J. Wainwright, B. Yu, et al. A uniﬁed framework for high-dimensional analysis of

M-estimators with decomposable regularizers. Statistical Science, 27(4):538–557, 2012.

[61] V. A. Nguyen, F. Zhang, J. Blanchet, E. Delage, and Y. Ye. Distributionally robust local non-parametric conditional

estimation. Advances in Neural Information Processing Systems, 33:15232–15242, 2020.

[62] J. L. Powell. Models, testing, and correction of heteroskedasticity. Lecture notes, Department of Economics, University of

California, Berkeley. URL: https://eml.berkeley.edu/~ powell/e240b_sp10/hetnotes.pdf, 2010.

[63] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex

programming. Journal of Machine Learning Research, 13(Feb):389–427, 2012.

[64] P. Rigollet and J.-C. H¨utter. High dimensional statistics. Lecture notes for MIT’s 18.657 course, 2017. URL

http://www-math.mit.edu/~ rigollet/PDFs/RigNotes17.pdf .

[65] P. M. Robinson. Asymptotically eﬃcient estimation in the presence of heteroskedasticity of unknown form. Econometrica:

Journal of the Econometric Society, pages 875–891, 1987.

[66] J. P. Romano and M. Wolf. Resurrecting weighted least squares. Journal of Econometrics, 197(1):1–19, 2017.

[67] J. O. Royset and R. J. Wets. From data to assessments and decisions: Epi-spline technology.

In Bridging data and

decisions, pages 27–53. INFORMS, 2014.

[68] D. Ruppert, M. P. Wand, U. Holst, and O. H¨ossjer. Local polynomial variance-function estimation. Technometrics, 39(3):

262–273, 1997.

[69] P. Sch¨utz, A. Tomasgard, and S. Ahmed. Supply chain design under uncertainty using sample average approximation and

dual decomposition. European Journal of Operational Research, 199(2):409–419, 2009.

[70] G. A. Seber and A. J. Lee. Linear regression analysis. John Wiley & Sons, 2003.

[71] E. Seijo and B. Sen. Nonparametric least squares estimation of a multivariate convex regression function. The Annals of

Statistics, 39(3):1633–1657, 2011.

[72] S. Sen and Y. Deng. Predictive stochastic programming. Computational Management Science, 19:1–45, 2022.

[73] A. Shapiro, D. Dentcheva, and A. Ruszczy´nski. Lectures on stochastic programming: modeling and theory. SIAM, 2009.

[74] Q. Sun, W.-X. Zhou, and J. Fan. Adaptive Huber regression. Journal of the American Statistical Association, 115(529):

26

254–265, 2020.

[75] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B

(Methodological), 58(1):267–288, 1996.

[76] S. A. van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.

[77] S. A. van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of Statistics, 36(2):614–645,

2008.

[78] A. W. van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 1998.

[79] A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes: with applications to statistics. Springer,

1996.

[80] I. Van Keilegom and L. Wang. Semiparametric modeling and estimation of heteroscedasticity in regression analysis of

cross-sectional data. Electronic Journal of Statistics, 4:133–160, 2010.

[81] R. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge

university press, 2018.

[82] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment eﬀects using random forests. Journal of the

American Statistical Association, 113(523):1228–1242, 2018.

[83] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press,

2019.

[84] H. Walk. Strong laws of large numbers and nonparametric estimation. In Recent Developments in Applied Probability and

Statistics, pages 183–214. Springer, 2010.

[85] H. White. Asymptotic theory for econometricians. Academic press, 2014.

[86] W.-X. Zhou, K. Bose, J. Fan, and H. Liu. A new perspective on robust M-estimation: Finite sample theory and applications

to dependence-adjusted multiple testing. Annals of Statistics, 46(5):1904, 2018.

[87] F. Ziel. Iteratively reweighted adaptive Lasso for conditional heteroscedastic time series with applications to AR-ARCH

type processes. Computational Statistics & Data Analysis, 100:773–793, 2016.

Appendix

Section A provides omitted proofs for results in Section 3. We continue in Section B with a discussion of
alternative assumptions under which the results of Section 3 hold and derive rates of convergence for the
ER-SAA in Section C. We then outline the analysis for the jackknife-based SAA variants in Section D. Then,
in Section E, we present a class of two-stage stochastic programs that satisfy our assumptions. Section F
lists several prediction setups (including M-estimators, OLS, Lasso, kNN, CART, and RF regression) that
satisfy the assumptions in our analysis. Finally, we end with Section G by providing omitted details for the
computational experiments.

A Omitted proofs

A.1 Proof of Lemma 1

We have

sup
z∈Z

ˆgER
n (z; x)

g∗
n(z; x)

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

n

i=1
X
n

c

z, projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

(cid:1)
(cid:0)
z, projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

c

n

c

z, f ∗(x) + Q∗(x)εi

(cid:0)

i=1
X
z, f ∗(x) + Q∗(x)εi

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

−

c

−

(cid:12)
(cid:0)
(cid:12)
L(z)
k

(cid:1)
projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

−

(cid:0)
f ∗(x) + Q∗(x)εi

(cid:1) (cid:12)
(cid:12)

k
(cid:1)

1
n

= sup
z∈Z (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

sup
z∈Z

≤

sup
z∈Z

≤

1
n

i=1
X
n

i=1
X

≤

sup
z∈Z

(cid:16)

L(z)

(cid:17)(cid:18)

(cid:0)

1
n

n

i=1
X

˜εi
n(x)
k

k

,

(cid:19)

27

where the second inequality above follows by Assumption 1 and the ﬁnal inequality follows by the Lipschitz
continuity of orthogonal projections.

A.2 Proof of Theorem 5

Before we prove Theorem 5, we present the following lemma that is needed in its proof.

Rdw be a nonempty and compact set and h : W
Lemma 10. Let W
function. Deﬁne W ∗ := arg minw∈W h(w). Suppose there exists δ > 0 and ¯w
Then, there exists κ > 0 such that h( ¯w)

minw∈W h(w) + κ.

⊂

→
∈

R be a lower semicontinuous
W such that dist( ¯w, W ∗)
δ.

≥

Proof. Let Wδ :=
Wδ is nonempty and compact,

w
{

∈

Setting κ = min
w∈Wδ

h(w)

min
w∈W

−

δ

W : dist(w, W ∗)
inf
w∈Wδ
h(w) yields the desired result.

, and note that ¯w
}

≥

h(w) is attained. Furthermore, we readily have min
w∈Wδ

h(w) > min
w∈W

Wδ. Since h is lower semicontinuous and
h(w).

∈

≥

Proof of Theorem 5. Let z∗(x)
Proposition 4, we have for a.e. x

S∗(x) and ˆzER
:

n (x)

∈
∈ X

ˆSER

n (x). Consider any constant δ > 0. From

∈

P

P

−
g(ˆzER

−
ˆvER
n (x)

n (z∗(x); x)
ˆgER
ˆvER
n (x)
(cid:8)(cid:12)
(cid:12)
∴ P
(cid:8)(cid:12)
(cid:12)
−
Suppose for contradiction that D
implies for any ¯x
∈
ˆSER
P
nq (¯x), S∗(¯x)
(cid:17)

such that

¯
X

(cid:8)(cid:12)
(cid:12)

β,

≥

≥

D

o

n

(cid:16)

δ

v∗(x)

> δ

> δ

(cid:9)

> δ

(cid:9)

n (x); x)
(cid:12)
(cid:12)
v∗(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

→
ˆSER
n (x), S∗(x)

(cid:9)

→

→

0 =

0 =

⇒

⇒

0 =

⇒

P

P

→

n (x) > v∗(x) + δ
ˆvER
v∗(x) > ˆvER
n (x) + δ
(cid:8)
→
p
ˆvER
v∗(x) for a.e. x
n (x)
(cid:8)
−→
, where ¯
¯
X

X ⊆ X

∈

(cid:9)

(cid:9)

x

∀

0 and

0.

.

∈ X

p
−→
, there exist constants δ > 0 and β > 0 and a subsequence
¯
X

N. Lemma 10 then implies that for a.e. ¯x

(cid:16)
q
∀

0,

∈

∈

(cid:17)

with PX ( ¯
) > 0. This
X
of N such that
nq}
, there exists κ(¯x) > 0

{

P

sup
z∈ ˆSER
nq (¯x)

n

g(z; ¯x) > v∗(¯x) + κ(¯x)

o

β,

≥

N.

q
∀

∈

From Proposition 4, we have for a.e. ¯x

P

n
P

sup
z∈ ˆSER
n (¯x)

sup
z∈S∗(¯x)

ˆgER
n (z; ¯x)

(cid:12)
(cid:12)

ˆgER
n (z, ¯x)

g(z; ¯x)

(cid:12)
(cid:12)
g(z, ¯x)

−

−

≤

≤

(cid:12)
(cid:12)

n
n (¯x)

Since ˆvER

(cid:12)
(cid:12)
≤

:

¯
∈
X
0.5κ(¯x)

o

0.5κ(¯x)

o

1 =

⇒

→

1 =

⇒

→

P

P

sup
z∈ ˆSER
n (¯x)

n

g(z; ¯x)

≤

sup
z∈S∗(¯x)

n

ˆgER
n (z; ¯x)

≤

ˆvER
n (¯x) + 0.5κ(¯x)

o

v∗(¯x) + 0.5κ(¯x)

o

supz∈S∗(¯x) ˆgER

n (z; ¯x) by deﬁnition, the above inequalities in turn imply

(14)

1,

→

1.

→

P

sup
z∈ ˆSER
n (¯x)

n

g(z; ¯x)

≤

v∗(¯x) + κ(¯x)

o

1,

→

which contradicts the inequality (14). The above arguments also readily imply that the ER-SAA estimators
p
are asymptotically optimal, i.e., supz∈ ˆSER
−→

v∗(x) for a.e. x

n (x) g(z; x)

∈ X

.

A.3 Proof of Theorem 6

Since

v∗(X)

ˆvER
n (X)

−
n (X); X)

(cid:13)
g(ˆzER
(cid:13)

Lq =
(cid:13)
v∗(X)
(cid:13)

−

(cid:13)
(cid:13)

min
z∈Z

ˆgER
n (z; X)

min
z∈Z

−

(cid:13)
(cid:13)
Lq ≤
(cid:13)
(cid:13)

g(ˆzER

n (X); X)

−

(cid:13)
(cid:13)

28

g(z; X)

Lq ≤
(cid:13)
(cid:13)
ˆvER
(cid:13)
n (X)
Lq +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ˆgER
n (z; X)

g(z; X)

sup
z∈Z
(cid:12)
ˆvER
n (X)
(cid:12)

−
v∗(X)

−

,

Lq

(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)

Lq ,
(cid:13)
(cid:13)

6
we focus on establishing convergence of ˆgER
n (
·
inequality (9) and the triangle inequality for the Lq-norm, we have

; X) to g(
·

; X) with respect to the Lq-norm on

. From

X

sup
z∈Z

ˆgER
n (z; X)

−

g(z; X)

Lq ≤

sup
z∈Z

ˆgER
n (z; X)

−

g∗
n(z; X)

+

Lq

sup
z∈Z |

g∗
n(z; X)

−

Assumption 6 implies that the second term on the r.h.s. of the above inequality converges to zero in proba-
bility. We now show that the ﬁrst term also converges to zero in probability. We have

(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)

(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

.

Lq

g(z; X)
|
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)

sup
z∈Z

sup
z∈Z

(cid:13)
(cid:13)
(cid:13)

(cid:16)

≤

≤

ˆgER
n (z; X)

g∗
n(z; X)

(cid:12)
(cid:12)
L(z)

1
n

˜εi
n(X)
k

k

−
n

Lq

(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
Lq

(cid:13)
(cid:13)
(cid:13)

(cid:17)(cid:13)
(cid:13)
(cid:13)
ˆfn(X)

i=1
X
f ∗(X)
k

−

O(1)

k

(cid:13)
(cid:13)

O(1)

(cid:13)
(cid:13)

O(1)

ˆQn(X)
k

k

Lq

(cid:13)
(cid:13)

Lq

ˆQn(X)
k

k

1
n

1
n

(cid:18)

(cid:18)

Lq + O(1)
(cid:13)
(cid:13)
n
(cid:13)
(cid:13)
ˆQn(xi)

i=1
X
n

(cid:13)
(cid:3)
(cid:2)
(cid:13)
ˆQn(xi)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

i=1
X

(cid:13)
(cid:2)
(cid:13)

−1

−

−1

2

(cid:3)

(cid:13)
(cid:13)

ˆQn(X)

k

Q∗(X)
k

−

(cid:13)
(cid:13)

Q∗(xi)

−1

2

(cid:2)

1/2

(cid:19)

(cid:18)

1
n

(cid:3)

n

(cid:13)
(cid:13)

i=1
X

(cid:19)

(cid:18)

f ∗(xi)

−

k

εi

k

+

(cid:19)

k

Lq

(cid:18)
1/2

n

1
n

i=1
X
n
1
n

Q∗(xi)
k

k

i=1
X
ˆfn(xi)
k

2

1/2

,

(cid:19)

1/4

4

(cid:19)

(cid:18)

1
n

n

i=1
X

εi

4

k

k

1/4

+

(cid:19)

where the ﬁrst inequality follows from Lemma 1 and the second inequality follows from Lemma 2 and the
triangle inequality. The rest of the arguments for
supz∈Z
0 follow by similar
arguments as in the proof of Proposition 4 upon noting that
(cid:12)
(cid:12)
ˆQn(X)
Q∗(X)
k
k

(cid:13)
(cid:13)
ˆQn(X)

Q∗(X)
k

ˆgER
n (z; X)

g∗
n(z; X)

(cid:12)
(cid:13)
(cid:12)
(cid:13)
Lq

p
−→

−

−

Lq

k

and by replacing Assumptions 2 and 5 with Assumptions 6 and 7.

Lq +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k
(cid:13)
(cid:13)

Lq ≤
(cid:13)
(cid:13)

k
(cid:13)
(cid:13)

A.4 Proofs of Lemma 7, Theorem 8, and Proposition 9

Proof of Lemma 7. Assumption 1 and Lemma 1 imply

P

sup
z∈Z

n

ˆgER
n (z; x)

−

g∗
n(z; x)

> κ

(cid:12)
(cid:12)

(cid:12)
(cid:12)

P

P

≤

o

≤

L(z)

sup
z∈Z

(cid:26)(cid:16)

1
n

((cid:18)

n

k

i=1
X

(cid:17)(cid:18)
˜εi
n(x)
k

1
n

n

i=1
X

>

(cid:19)

Therefore, it suﬃces to bound P

1
n

From Lemma 2 and the inequality P
variables V , W and constants c1, c2, we have:

P

(cid:8)

n
i=1k
{

˜εi
n(x)
> κ
k
V + W > c1 + c2} ≤
(cid:9)

for any κ > 0.

P

V > c1}

{

˜εi
n(x)
k

k

(cid:19)

> κ

κ

(cid:27)

.

sup
z∈Z

(cid:16)

L(z)

)

(cid:17)

+ P

W > c2}

{

(15)

for any random

n

k

˜εi
n(x)
k

> κ

o

f ∗(x)
k

−

>

κ
4

i=1
X
ˆfn(x)

1
n

P

n

P

≤

k
n

P

P

k
(cid:26)

k
(cid:26)

ˆQn(x)
k

ˆQn(x)
k

1
n

1
n

(cid:18)

(cid:18)

n

i=1
X
n

(cid:13)
(cid:2)
(cid:13)

i=1
X

(cid:13)
(cid:2)
(cid:13)

+ P

o
ˆQn(xi)

1
n

(cid:26)(cid:18)

−1

−

n

εi

k

k

k
(cid:19)

ˆQn(x)

Q∗(x)
k

−

>

κ
4

+

(cid:27)

i=1
X
Q∗(xi)

1/2

−1

2

(cid:3)
ˆQn(xi)

(cid:2)

1/2

−1

2

(cid:19)

(cid:18)

(cid:3)

(cid:13)
(cid:13)

1
n

(cid:3)

n

(cid:13)
(cid:13)

k
i=1
X

(cid:19)

(cid:18)

f ∗(xi)

−

29

n

1
n

Q∗(xi)
k

k

1/4

4

i=1
X
ˆfn(xi)
k

(cid:19)

>

(cid:18)

.
(cid:27)

κ
4

1/2

2

(cid:19)

1
n

n

1/4

εi

4

k

k

(cid:19)

i=1
X

>

κ
4

+

(cid:27)

(16)

For a.e. x

∈ X

, the ﬁrst term on the r.h.s. of (16) can be bounded using Assumption (11a) as

ˆfn(x)

f ∗(x)
k

−

>

κ
4

P

k
n

o

Kf ( κ

4 , x) exp(

−

≤

βf (n, κ

4 , x)).

Next, consider the second term on the r.h.s. of inequality (16). We have for a.e. x

P

(cid:26)(cid:18)
1
n

(cid:26)

P

≤

n

1
n

i=1
X
n
εi

k

i=1
X

Jε(κ) exp(

Jε(κ) exp(

≤

≤

εi

k

ˆQn(x)

Q∗(x)
k

−

>

k

k
(cid:19)

κ
4

(cid:27)

> E[

] + κ

ε

k

k

k

+ P

(E[

ε

] + κ)
k

k

k

ˆQn(x)

Q∗(x)
k

−

>

γε(n, κ)) + P

k
(cid:26)
γε(n, κ)) + KQ

−

−

(cid:27)
n
ˆQn(x)

>

Q∗(x)
k
exp

−
κ
4(E[kεk]+κ) , x
(cid:1)

−
(cid:0)

4(E[
βQ

κ
ε

k
k
n,

] + κ)
(cid:27)
κ
4(E[kεk]+κ) , x

,

∈ X

κ
4

o

(cid:0)
where the second inequality follows by Assumption 10 and the last step by Assumption (11c).

(cid:1)(cid:1)

(cid:0)

The third term on the r.h.s. of inequality (16) can be bounded for a.e. x

∈ X

Q∗(xi)

−1

2

−

(cid:19)

(cid:13)
(cid:3)
(cid:13)
Q∗(xi)
k

k

4

(cid:19)

1
n

n

i=1
X

1/2

1
n

(cid:18)
1/4

n

i=1
X

>

Q∗(xi)
k

k

4

1/4

(cid:19)

(cid:18)

1
n

E[

Q∗(X)
k

k

4]

(cid:0)

(cid:1)

1/4

+ κ

+

as

n

1/4

εi

4

k

k

(cid:19)

i=1
X

>

κ
4

(cid:27)

P

k
(cid:26)

ˆQn(x)
k

(cid:18)

1
n

ˆQn(x)
k

>

k

P

≤

k
(cid:8)

n

1/4

εi

k

4

k

(cid:19)

+ κ

i=1
X
Q∗(x)
k

n

ˆQn(xi)

−1

i=1
X
(cid:13)
(cid:2)
(cid:13)
Q∗(x)
k

(cid:2)

(cid:3)
+ P

+ κ

(cid:26)(cid:18)

1/4

(cid:9)
ε

k

4]

k

>

E[

(cid:0)
Q∗(X)
k

(cid:1)
4]

k

E[

+ κ

+

(cid:27)

1/4

+ κ

KQ(κ, x) exp (

(cid:1)(cid:0)(cid:0)
βQ(n, κ, x)) + ¯JQ(κ) exp(

(cid:1)

n

−
ˆQn(xi)

−1

−

Q∗(xi)

−1

2

i=1
X

(cid:13)
(cid:2)
(cid:13)

KQ(κ, x) exp (
¯KQ(h1(κ, x)) exp(

−

(cid:3)

(cid:2)

(cid:19)
βQ(n, κ, x)) + ¯JQ(κ) exp(
¯βQ(n, h1(κ, x))),

(cid:13)
(cid:13)

(cid:3)

−

−

1
n

P

P

(cid:26)(cid:18)

k
(cid:26)
(cid:0)

1
n

P

(cid:26)(cid:18)

≤

≤

(cid:27)

−

(cid:2)

Q∗(xi)

−1

2

1/2

(cid:19)

(cid:3)

(cid:13)
(cid:13)

>

κ
4

(cid:27)

ε

k

1/4

4]

E[

+ κ

1
n
¯γQ(n, κ)) + ¯Jε(κ) exp(

(cid:18)
(cid:1)

k

(cid:1)

n

ˆQn(xi)

−1

i=1
X
−

(cid:13)
(cid:3)
(cid:2)
(cid:13)
¯γε(n, κ))+

(cid:1)(cid:0)(cid:0)
−
1/2

> h1(κ, x)

(cid:27)

¯γQ(n, κ)) + ¯Jε(κ) exp(

¯γε(n, κ))+

−

where the second inequality follows from Assumptions 9, 10, and (11c), the ﬁnal inequality follows from
Assumption (11d), and

h1(κ, x) :=

κ

4

k
(cid:0)

Q∗(x)
k

+ κ

E[

Q∗(X)
k

k

4]

1/4

+ κ

E[

4]

ε

k

k

1/4

+ κ

(cid:1)(cid:0)(cid:0)

(cid:1)

(cid:1)(cid:0)(cid:0)

(cid:1)

.

(cid:1)

30

Finally, the fourth term on the r.h.s. of inequality (16) can be bounded for a.e. x

as

∈ X

P

k
(cid:26)

ˆQn(x)
k

(cid:18)

1
n

ˆQn(x)
k

>

k

P

≤

k
(cid:8)

Q∗(x)
k

P

k
(cid:26)
(cid:0)

KQ(κ, x) exp (

−

n

ˆQn(xi)

−1

2

i=1
X
(cid:13)
(cid:2)
(cid:13)
Q∗(x)
k

(cid:13)
(cid:3)
(cid:13)
+ P

+ κ

1/2

(cid:19)

1
n

(cid:18)
n

1
n

n

f ∗(xi)

k
i=1
X
ˆQn(xi)

1/2

2

ˆfn(xi)
k

−

(cid:19)

>

κ
4

(cid:27)

1/2

−1

2

>

E

Q∗(X)

−1

2

1/2

(cid:9)
Q∗(X)

(cid:26)(cid:18)

−1

2

i=1
X

(cid:13)
(cid:2)
(cid:13)
1/2
+ 2κ

(cid:3)

(cid:19)

(cid:13)
n
(cid:13)

f ∗(xi)

+ κ

E

(cid:17)(cid:18)

k

i=1
X
ˆfn(xi)
k

f ∗(xi)

k

−

1
n

n

(cid:16)

−

(cid:2)(cid:13)
(cid:2)
(cid:13)
ˆfn(xi)
k

2

(cid:3)
1/2

(cid:19)

(cid:13)
(cid:13)
>

(cid:3)(cid:17)
κ
4

(cid:27)

1/2

2

(cid:19)

> h2(κ, x)

+

(cid:27)

(cid:1)(cid:16)(cid:16)
(cid:2)(cid:13)
(cid:2)
(cid:13)
βQ(n, κ, x)) + P

(cid:3)

(cid:26)(cid:18)

(cid:13)
(cid:13)
1
n

(cid:3)(cid:17)
n

i=1
X
2

+ 2κ

+

(cid:27)

n

ˆQn(xi)

−1

1
n

Q∗(xi)

−1

−

1/2

+

(cid:18)
βQ(n, κ, x)) + ¯Kf (h2(κ, x)) exp(

(cid:13)
(cid:13)

(cid:19)

(cid:3)

(cid:2)

(cid:3)

1/2

2

−1

Q∗(xi)

1
n
i=1
(cid:16)
X
(cid:3)
¯βf (n, h2(κ, x))) + ¯KQ(κ) exp(

(cid:13)
(cid:2)
(cid:13)

(cid:13)
(cid:13)

>

(cid:19)

E

−

−1

2

1/2

Q∗(X)

(cid:2)(cid:13)
(cid:2)
¯βQ(n, κ))+
(cid:13)
−

(cid:3)

(cid:3)(cid:17)

(cid:13)
(cid:13)

+ 2κ

(cid:27)

P

≤

≤

(cid:26)(cid:18)

i=1
X

(cid:13)
(cid:2)
(cid:13)
KQ(κ, x) exp (
JQ(κ) exp(

−

γQ(n, κ)),

−

where the second inequality follows by Assumption 11 and Lemma 3, the ﬁnal inequality follows by Assump-
tions 11 and 9 and the probability inequality stated at the beginning of this proof, and

h2(κ, x) :=

κ

k
(cid:3)(cid:17)
(cid:0)
Putting the above bounds together in inequality (16), we have for a.e. x

(cid:1)(cid:16)(cid:16)

(cid:2)(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:2)

(cid:3)

4

Q∗(x)
k

+ κ

E

Q∗(X)

−1

2

1/2

+ 2κ

.

(cid:17)

:

∈ X

1
n

P

(cid:26)

n

i=1
X

˜εi
n(x)
k

k

> κ

≤

(cid:27)

Jε(κ) exp(

−

γε(n, κ)) + ¯Jε(κ) exp(

−
γQ(n, κ)) + ¯JQ(κ) exp(

¯γε(n, κ))+

(17)

JQ(κ) exp(
Kf ( κ
KQ
¯KQ(κ) exp(

−
4 , x) exp(
κ
4(E[kεk]+κ) , x
(cid:1)

(cid:0)

βf (n, κ
exp

¯γQ(n, κ))+
−
4 , x)) + ¯Kf (h2(κ, x)) exp(

¯βf (n, h2(κ, x)))+

−

−
+ 2KQ(κ, x) exp (
4(E[kεk]+κ) , x)
βQ(n,
¯βQ(n, h1(κ, x))),
¯βQ(n, κ)) + ¯KQ(h1(κ, x)) exp(
(cid:1)
−

−
(cid:0)

κ

−

βQ(n, κ, x)) +

−

which along with inequality (15) implies the desired result.

Proof of Theorem 8. Note that for any κ > 0:

ˆgER
n (z; x)

−

g(z; x)

> κ

ˆgER
n (z; x)

−

g∗
n(z; x)

>

P

≤

sup
z∈Z

n

o

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

κ
2

o

+ P

sup
z∈Z |

n

g∗
n(z; x)

g(z; x)
|

−

>

κ
2

.

o

Bounding the two terms on the r.h.s. of the above inequality using Assumption 8 and Lemma 7 yields for
a.e. x

:

P

sup
z∈Z

n

(cid:12)
(cid:12)

∈ X

P

where ˜K(κ, x) := 2 max

sup
z∈Z

ˆgER
n (z; x)

g(z; x)

> κ

−

≤

˜K(κ, x) exp

˜β(n, κ, x)

,

(18)

n
K(0.5κ, x), ¯K(0.5κ, x)

(cid:12)
(cid:12)

o

(cid:12)
(cid:12)
and ˜β(n, κ, x) := min

(cid:1)
nβ(0.5κ, x), ¯β(n, 0.5κ, x)

. Inequal-

−
(cid:0)

(cid:8)

(cid:9)

(cid:8)

(cid:9)

31

ity (18) implies for n

N, a.e. x

∈

∈ X

, and any κ := κ(x) > 0:

P

P

sup
z∈ ˆSER
n (x)

n

sup
z∈ ˆSER
n (x)

n

=

⇒

ˆgER
n (z; x)

g(z; x)

−

0.5κ

≤

1

−

≥

(cid:12)
(cid:12)
g(z; x)

≤

ˆvER
n (x) + 0.5κ

(cid:12)
(cid:12)

o
1

≥

−

˜K(0.5κ, x) exp

˜β(n, 0.5κ, x)

−
(cid:0)
˜β(n, 0.5κ, x)

(cid:1)

,

˜K(0.5κ, x) exp

−
(cid:0)

and P

P

=

⇒

sup
z∈S∗(x)

n

sup
z∈S∗(x)

n

ˆgER
n (z; x)

g(z; x)

−

≤

(cid:12)
(cid:12)
ˆgER
n (z; x)

≤

v∗(x) + 0.5κ

(cid:12)
(cid:12)

o

o
1

≥

o
0.5κ

1

−

≥

˜K(0.5κ, x) exp

˜β(n, 0.5κ, x)

−
(cid:0)
˜β(n, 0.5κ, x)

(cid:1)

.

−

˜K(0.5κ, x) exp

−
(cid:0)

(cid:1)

(cid:1)

Since ˆvER

n (x)

≤

supz∈S∗(x) ˆgER

n (z; x) by the deﬁnition of ˆvER

n (x), the above two inequalities imply

P

sup
z∈ ˆSER
n (x)
n
g(ˆzER
n (x); x)

g(z; x)

v∗(x) + κ

≤

o
v∗(x) + κ(x)

=

P

1

1

−

−

≥

≥

2 ˜K(0.5κ, x) exp

−
(cid:0)
2 ˜K(0.5κ, x) exp
−
(cid:0)

˜β(n, 0.5κ, x)

,

(cid:1)
˜β(n, 0.5κ, x)
.

(19)

⇒
(cid:8)
n (x), S∗(x))
Suppose dist(ˆzER

≤

; x) is lsc on the compact
n (x); x) > v∗(x) + κ(η, x)
set
on that path (except for some paths of measure zero). We now provide a bound on the probability of this
event. By the above arguments, we have for a.e. x

(cid:9)
and some sample path. Since g(
·
, Lemma 10 implies that there exists κ(η, x) > 0 such that g(ˆzER

η for some x

for a.e. x

∈ X

∈ X

≥

Z

(cid:1)

:

P

dist(ˆzER

n (x), S∗(x))

(cid:8)

η

≥

≤

≤

(cid:9)

P

∈ X
g(ˆzER

n (x); x) > v∗(x) + κ(η, x)

2 ˜K(0.5κ(η, x), x) exp

(cid:8)

−
(cid:0)
N and a.e. x

˜β(n, 0.5κ(η, x), x)

(cid:9)

.

(cid:1)

Proof of Proposition 9. We show that for each n
, there exist positive constants
∈
˜K(κ, x) and ˜β(n, κ, x) such that inequality (18) holds. Following the arguments in the proof of Theorem 8,
inequality (19) then implies

∈ X

P

n

sup
z∈ ˆSER
n (x)

g(z; x)

v∗(x) + κ

≤

1

−

≥

o

2 ˜K(0.5κ, x) exp

−
(cid:0)

˜β(n, 0.5κ, x)

,

(cid:1)

which along with the deﬁnition of Sκ(x) in turn implies that

P

ˆSER

n (x)

Sκ(x)

⊆

1

−

≥

n

o

2 ˜K(0.5κ, x) exp

˜β(n, 0.5κ, x)

.

−
(cid:0)

(cid:1)

We now state results that can be used to bound the constants ˜K(κ, x) and ˜β(κ, x) in inequality (18); we
ignore their dependence on x to keep the exposition simple. Theorems 7.66 and 7.67 of Shapiro et al. [73]
imply for our setting of two-stage stochastic LP the bound

P

sup
z∈Z |

n

g∗
n(z; x)

g(z; x)
|

−

> κ

O(1)

O(1)D
κ

(cid:18)

dz

exp

(cid:19)

nκ2
O(1)σ2

c (x)

(cid:19)

−

(cid:18)

≤

o

(20)

for a.e. x
Section F) can be used to specialize the bound aﬀorded by Lemma 7:

. The following large deviation inequalities for our three diﬀerent regression setups (see

∈ X

1. OLS regression: P

n

1
n

n

i=1
X

˜εi
n(x)
k

k

2 > κ2

exp(dx) exp

≤

o

nκ2
O(1)σ2dy (cid:19)

−

(cid:18)

of Hsu et al. [45], Theorem 2.2 and Remark 2.3 of Rigollet and H¨utter [64].

, which follows from Remark 12

32

2. Lasso regression: P

n

1
n

n

i=1
X

˜εi
n(x)
k

k

2 > κ2

2dx exp

−

(cid:18)

≤

o

nκ2
O(1)σ2sdy (cid:19)

, which follows from Theorem 2.1

and Corollary 1 of Bunea et al. [19].

3. kNN regression: Whenever n

O(1)

O(1)
κ

dx

(cid:19)

exp

≥

(cid:18)
O(1)√dx
κ

n

P

1
n

(

2 > κ2dy
˜εi
n(x)
k
k

) ≤

(cid:18)

Xi=1

(cid:16)
from Lemma 10 of Bertsimas and McCord [13].

(cid:19)

dx
1−γ

and

nγ

log(n) ≥

O(1)dxdyσ2
κ2

, we have

O(1)n(O(1)κ)2dx

+ O(1)n2dx

(cid:17)

dx

O(1)
dx (cid:19)

(cid:18)

exp

−

(cid:18)

nγ κ2
O(1)σ2

(cid:19)

−

Suppose the regression step (3) is Lasso regression. We have from Lemma 7 that

P

sup
z∈Z

(cid:26)

(cid:12)
(cid:12)

ˆgER
n (z; x)

−

g∗
n(z; x)

> κ

O(1)dx exp

≤

−

(cid:18)

(cid:27)

(cid:12)
(cid:12)

Along with the uniform exponential bound inequality (20), this yields for a.e. x

.

nκ2
O(1)σ2sdy (cid:19)
:

∈ X

P

sup
z∈Z

(cid:26)

ˆgER
n (z; x)

−

g(z; x)

> κ

O(1)

O(1)D
κ

(cid:18)

≤

(cid:27)

dz

exp

(cid:19)

nκ2
O(1)σ2

c (x)

(cid:19)

−

(cid:18)

+ O(1)dx exp

nκ2
O(1)σ2sdy (cid:19)

.

−

(cid:18)

0.5δ and using the union bound yields the
Requiring each term in the r.h.s. of the above inequality to be
stated conservative sample size estimates. Sample complexities for OLS and kNN regression can be similarly
derived.

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

B Alternative assumptions for analysis of the ER-SAA

In this section, we present alternative sets of assumptions under which theoretical guarantees similar to
those in Section 3 hold for the ER-SAA problem. The following assumption provides an alternative to the
Lipschitz continuity Assumption 1, and can be used to derive asymptotic guarantees for the ER-SAA.

Assumption 12. Problem (1), the regression step (3), and the data

enough,

(12b) for a.e. x

(12a) there exists a function δ :

R+ such that for a.e. x

,

k

∈ X

X →

i
∀

∈

[n], a.s. for n large

and for each (z, ε)

Ξ, the function c in problem (1) satisﬁes the following local

Lipschitz inequality for each ¯y

∈ X

∈ Z ×

∈ Bδ(x)(f ∗(x) + Q∗(x)ε)

:

∩ Y

c(z, ¯y)

Lδ(x)(z, f ∗(x) + Q∗(x)ε)
k
with the ‘local Lipschitz constant’ Lδ(x)(z, f ∗(x) + Q∗(x)ε) satisfying

c(z, f ∗(x) + Q∗(x)ε)

| ≤

−

|

¯y

(f ∗(x) + Q∗(x)ε)
k

,

−

1
n

sup
z∈Z

n

i=1
X

δ(x)(z, f ∗(x) + Q∗(x)εi) = Op(1).
L2

∈ Z

Unlike Assumption 1, Assumption (12b) only requires the function c(z,

) to be locally Lipschitz contin-
uous for each z
with the local Lipschitz constant satisfying a uniform stochastic boundedness con-
dition, but using this weaker assumption necessitates the stronger Assumption (12a) on the regression
step (3). Consider the homoscedastic setting (i.e., ˆQn := Q∗
I). Because the deviation terms satisfy
˜εi
[n], Assumption (12a) is satisﬁed in this setting for our
n(x)
+
k
is compact, the population regression problem
running example of OLS regression, e.g., if the support
has a unique solution θ∗, the strong pointwise LLN holds for the objective function (i.e., the empirical loss)

ˆfn(xi)
k

ˆfn(x)
k

f ∗(xi)

f ∗(x)

k ≤ k

i
∀

−

−

≡

X

∈

k

,

·

33

Dn satisfy:
˜εi
n(x)

δ(x),

k ≤

2

ε

k

k

X

< +

of the regression problem (3), and E
(see Theorem 5.4 of Shapiro et al. [73] for details). The
support
of the covariates being compact may not be an overly restrictive assumption because in many ap-
plications the covariates (e.g., temperature, precipitation, wind, or location) can be assumed to be bounded
for all practical purposes. For the heteroscedastic setting in Example 2, following the proof of Lemma 2,
and Ξ of the covariates X and the
we can show that Assumption (12a) holds if in addition the supports
errors ε are compact and a.s. for n large enough the parameter estimates ˆπn of π∗ lie in a compact set. We
present conditions under which Assumption (12b) holds in Section E of the Appendix (note that it readily
holds for our running example of two-stage stochastic LP).

∞

X

(cid:2)

(cid:3)

Replacing Assumption 1 with Assumption 12 yields the following analogue of Lemma 1.

Lemma 11. Let Assumption 12 hold. Then, for a.e. x

, we a.s. have for n large enough:

∈ X

sup
z∈Z

ˆgER
n (z; x)

−

Proof. We have

(cid:12)
(cid:12)

g∗
n(z; x)

≤
(cid:12)
(cid:12)

1
n

sup
z∈Z(cid:18)

n

i=1
X

δ(x)(z, f ∗(x) + Q∗(x)εi)
L2

1/2

(cid:19)

(cid:18)

1
n

n

i=1
X

2

˜εi
n(x)
k

k

1/2

.

(cid:19)

sup
z∈Z

ˆgER
n (z; x)

g∗
n(z; x)

−

(cid:12)
(cid:12)

1
n

= sup
z∈Z (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

sup
z∈Z

≤

sup
z∈Z

≤

1
n

≤

sup
z∈Z(cid:18)

i=1
X
n

i=1
X
n
1
n

i=1
X

n

(cid:12)
(cid:12)
z, projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

c

i=1
X
n

(cid:0)
(cid:1)
z, projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

c

n

c

z, f ∗(x) + Q∗(x)εi

(cid:0)

i=1
X
z, f ∗(x) + Q∗(x)εi

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

−

c

−

(cid:0)

(cid:12)
(cid:12)
Lδ(x)(z, f ∗(x) + Q∗(x)εi)
k

(cid:1)

(cid:0)

projY ( ˆfn(x) + ˆQn(x)ˆεi
n)

−

(cid:1) (cid:12)
(cid:12)
f ∗(x) + Q∗(x)εi

δ(x)(z, f ∗(x) + Q∗(x)εi)
L2

1/2

(cid:19)

(cid:18)

1
n

n

i=1
X

2

˜εi
n(x)
k

k

(cid:0)

1/2

,

(cid:19)

k

(cid:1)

where the second inequality follows by Assumption 12 and the ﬁnal inequality follows by the Cauchy-Schwarz
inequality and the Lipschitz continuity of orthogonal projections.

Comparing with Lemma 1, Proposition 4, and Theorem 5, we see that convergence in probability of the
root mean square deviation term
0 directly translates to the asymptotic guarantees
for ER-SAA in Section 3.2 when Assumption 1 is replaced with Assumption 12. The following analogue of
1/2
.
Lemma 2, which we state without proof, bounds the root mean square deviation term

1/2 p
−→

˜εi
n(x)
k

n
i=1k

P

1
n

(cid:1)

(cid:0)

2

2

1
n

n
i=1k

˜εi
n(x)
k

Lemma 12. Given estimates ˆfn of f ∗ and ˆQn of Q∗ with [ ˆQn(¯x)]−1

0 for each ¯x

≻

P
:

(cid:0)
∈ X

(cid:1)

1. In the homoscedastic setting (i.e., ˆQn = Q∗

I), we have

≡

1
n

(cid:18)

n

i=1
X

1/2

2

˜εi
n(x)
k

k

(cid:19)

√2

k

≤

ˆfn(x)

f ∗(x)
k

+

−

2
n

(cid:18)

n

k

i=1
X

ˆfn(xi)

2

f ∗(xi)
k

−

1/2

.

(cid:19)

34

2. In the heteroscedastic setting, we have

1
n

(cid:18)

n

i=1
X

1/2

2

˜εi
n(x)
k

k

(cid:19)

O(1)
k

≤

ˆfn(x)

f ∗(x)
k

+ O(1)
k

−

ˆQn(x)

Q∗(x)
k

−

1
n

(cid:18)

O(1)
k

ˆQn(x)
k

(cid:18)

O(1)
k

ˆQn(x)
k

n

i=1
X
n

(cid:13)
(cid:2)
(cid:13)

1
n

1
n

(cid:18)

i=1
X

ˆQn(xi)

−1

−

Q∗(xi)

−1

4

(cid:3)
ˆQn(xi)

−1

4

(cid:2)

1/4

(cid:19)

(cid:18)

1
n

(cid:3)

n

(cid:13)
(cid:13)

k
i=1
X

εi

n

k

i=1
X
1/4

(cid:19)

(cid:18)

k

1
n

f ∗(xi)

−

1/2

2

+

(cid:19)
n

Q∗(xi)
k

k

i=1
X
ˆfn(xi)
k

4

1/4

.

(cid:19)

(cid:13)
(cid:13)
Proof. Use the AM-QM inequality and follow the arguments in the proof of Lemma 2.

(cid:13)
(cid:2)
(cid:13)

(cid:3)

1/8

8

(cid:19)

(cid:18)

1
n

n

i=1
X

εi

8

k

k

1/8

+

(cid:19)

Lemma 12 informs how Assumptions 3, 4, and 5 may to be adapted to derive similar theoretical guarantees
for ER-SAA as in Section 3.2. Next, we consider the following strengthening of Assumption 12 under which
similar ﬁnite guarantees as in Section 3.3 hold for the ER-SAA.
Assumption 13. There exists a function δ :
˜εi
n(x)
data
∈
Lipschitz constant satisfying for a.e. x

R+ such that for a.e. x
, the regression step and the
N. Furthermore, Assumption (12b) holds with the local

X →
∈
, κ > 0, and n

Dn satisfy

[n] and n

δ(x),

∈ X

k ≤

i
∀

N:

k

1
n

P

(cid:26)(cid:18)

n

Xi=1

sup
z∈Z

δ(x)(z, f ∗(x) + Q∗(x)εi)
L2
(cid:19)

E

(cid:18)

(cid:20)

sup
z∈Z

δ(x)(z, f ∗(x) + Q∗(x)ε)
L2

(cid:21)(cid:19)

1/2

+ κ

≤

(cid:27)

JL(κ) exp(

−

γL(n, κ; x)),

where JL(κ), γL(n, κ; x) > 0 with limn→∞ γL(n, κ; x) =

for each κ > 0 and a.e. x

∈ X
1/2
>

∈

∞

The discussion following Assumption 12 provides conditions under which

N,
if we further assume the estimates ˆθn and ˆπn lie in compact sets. Section E identiﬁes conditions under which
the uniform Lipschitz condition in Assumption 13 holds (again, it readily holds for our running example of
two-stage stochastic LP).

˜εi
n(x)

[n], n

δ(x),

i
∀

k ≤

∈

∈

k

When Assumption 13 is used in place of Assumption 1, we can derive the following analogue of inequal-

ity (15) in the proof of Lemma 7:

.

∈ X

2

˜εi
n(x)
k

k

1/2

> κ

(cid:19)

(cid:27)

sup
z∈Z

ˆgER
n (z; x)

−

g∗
n(z; x)

> κ

(cid:12)
(cid:12)
L2
δ(x)(z, f ∗(x) + Q∗(x)εi)

o

1/2

P

P

P

≤

≤

n

(cid:26)(cid:18)

(cid:26)(cid:18)

1
n

1
n

n

(cid:12)
(cid:12)

i=1
X
n

i=1
X

sup
z∈Z

sup
z∈Z

(cid:19)

(cid:18)

1/2

1
n

n

i=1
X
E

(cid:18)

(cid:20)

+ κ

δ(x)(z, f ∗(x) + Q∗(x)εi)
L2

(cid:19)

>

1/2

P

E

(cid:26)(cid:20)(cid:18)

(cid:20)

sup
z∈Z

δ(x)(z, f ∗(x) + Q∗(x)ε)
L2

(cid:21)(cid:19)

sup
z∈Z

δ(x)(z, f ∗(x) + Q∗(x)ε)
L2

+ κ

+

1/2

(cid:21)(cid:19)

(cid:27)

1
n

n

1/2

˜εi
n(x)
k

k

2

(cid:19)

> κ

(cid:27)

JL(κ) exp(

γL(n, κ; x)) + P

≤

−

n

1
n

˜εi
n(x)
k

k

1/2

2

>

(cid:21)(cid:18)

i=1
X
κ
BL(κ; x)

,
(cid:27)

(cid:26)(cid:18)

E

where BL(κ; x) :=

i=1
X
δ(x)(z, f ∗(x) + Q∗(x)ε)
tion 13. Therefore, ﬁnite sample guarantees for the root mean square deviation term
directly translate to ﬁnite sample guarantees for ER-SAA of the form in Lemma 7 and Theorem 8. Once
again, Lemma 12 provides guidance for how Assumptions 9, 10, and 11 may to be adapted to derive similar
theoretical guarantees for ER-SAA as in Section 3.3.

+ κ and the last inequality follows by Assump-
1/2

supz∈Z L2

˜εi
n(x)
k

n
i=1k

(cid:3)(cid:17)

P

1/2

1
n

(cid:16)

(cid:0)

(cid:1)

(cid:2)

2

(cid:19)

35

C Rate of convergence of the ER-SAA estimator

We investigate the rate of convergence of the optimal objective value of the sequence of ER-SAA problems (4)
to that of the true problem (1). This analysis requires the following additional assumptions on the true
problem (1) and the regression step (3).

Assumption 14. The function c in problem (1) and the errors
limit theorem (CLT) for the full-information SAA objective (2):

εi

{

}

satisfy the following functional central

where g∗
n(
·
functions on

; x), g(
·

; x), and V (
·

equipped with the supremum norm.

Z

; x)

√n (g∗
n(
·

V (
·
; x) are (random) elements of

; x)) d
−→

g(
·

−

; x),

for a.e. x

,

∈ X

), the Banach space of real-valued continuous

(
Z

C

Z

with an L2(

Assumption 14 holds, for instance, when the errors

for a.e. y
∈ Y
(c(˜z, f ∗(x) + Q∗(x)ε))2

εi
{
) Lipschitz constant, and, for a.e. x

, y) is Lipschitz contin-
such that
uous on
E
(see page 164 of Shapiro et al. [73] for details). Theorem 1 of Doukhan
et al. [32], Theorem 2.1 of Arcones and Yu [3], Theorem 9 of Arcones [2], and Corollary 2.3 of Andrews
and Pollard [1] provide conditions under which the functional CLT holds under mixing assumptions on
.
}
Theorems 1.5.4 and 1.5.6 of van der Vaart and Wellner [79] present a general set of conditions under which
the functional CLT holds.

are i.i.d., the function c(
·

, there exists ˜z

< +

∈ Z

∈ X

∞

εi

Y

}

{

(cid:2)

(cid:3)

The next assumption, which strengthens Assumption 5, ensures that the deviation of the ER-SAA prob-

lem (4) from the full-information SAA problem (2) converges at a certain rate.

Assumption 15. There is a constant 0 < α
1 (that is independent of the number of samples n, but could
depend on the dimension dx of the covariates X) such that the regression estimates ˆfn and ˆQn satisfy the
following convergence rate criteria:

≤

−
ˆfn(xi)

(15a)

(15b)

(15c)

(15d)

ˆfn(x)
n

k

i=1
X
ˆQn(x)
n

k
1
n

k
1
n

f ∗(x)
k

= Op(n−α/2) for a.e. x

,

∈ X

f ∗(xi)
k

−

2 = Op(n−α),

Q∗(x)
k

−

= Op(n−α/2) for a.e. x

,

∈ X

ˆQn(xi)

−1

−

Q∗(xi)

−1

2

= Op(n−α).

i=1
X

(cid:13)
(cid:3)
(cid:2)
(cid:13)
Note that the Op(
·

(cid:2)

(cid:3)

(cid:13)
(cid:13)

) terms in Assumption 15 hide factors proportional to the dimension dy of the random
vector Y . For our running example of OLS regression, Assumptions (15a) and (15b) hold with α = 1 under
Dn and the distribution PX of the covariates [see Chapter 5 of 85]. A similar
mild assumptions on the data
rate holds for Lasso, best subset selection, and many other parametric regression procedures under mild
assumptions. Nonparametric regression procedures such as kNN and RF regression, on the other hand,
typically only satisfy these assumptions with constant α = O(1)
. This rate cannot be improved upon in
dx
general, and is commonly referred to as the curse of dimensionality. Structured nonparametric regression
methods such as sparse additive models [63] can hope to break the curse of dimensionality and achieve rates
with α = 1. Assumptions (15c) and (15d) also hold for our running Example 2 if the parameter estimates
= Op(n−α/2). Section F of the Appendix veriﬁes that Assumption 15 holds for these
ˆπn satisfy
prediction setups with the stated constants α.

ˆπn −

π∗

k

k

Our main result of this section extends Theorem 5.7 of Shapiro et al. [73] to establish a rate at which
the optimal objective value of the ER-SAA problem (4) converges to that of the true problem (1). We
hide the dependence of the convergence rate on the dimensions dx and dy of the covariates X and random
vector Y . We discuss how these dimensions aﬀect the rate of convergence via a non-asymptotic/ﬁnite sample
analysis in Section 3.3. Note that the convergence rate analysis in Theorem 5.7 of Shapiro et al. [73] for

36

the full-information SAA problem (2) is sharper than Theorem 13 in the sense that it also characterizes the
asymptotic distribution of the optimal objective value, see equations (5.25) and (5.26) therein. Deriving the
asymptotic distribution of the optimal value of the ER-SAA is an interesting direction for future work.

Theorem 13. Suppose Assumptions 1, 3, 4, 14 and 15 hold. Then, we have
and

2 ) for a.e. x

= Op(n− α

g(ˆzER

n (x); x)

.

ˆvER
n (x)
|

−

|

ˆvER
n (x)

|

v∗(x)
|

−

= Op(n− α
2 )

ˆgER
n (z; x)
Proof. We begin by showing that supz∈Z
g∗
n(z; x)
Assumption 14 implies √n supz∈Z|
(cid:12)
−
= Op(n−1/2) for a.e. x
g(z; x)
(cid:12)
|
ˆgER
g(z; x)
n (z; x)
We now bound supz∈Z

g∗
n(z; x)

−

supz∈Z |
Lemma 1 imply

−

∈ X

−
g(z; x)
|
.

(cid:12)
(cid:12)

g(z; x)

= Op(n−α/2) for a.e. x

= Op(1) for a.e. x

, which in turn implies

.

∈ X

∈ X

∈ X
from above using Lemmas 1 and 2. Assumption 1 and

(cid:12)
(cid:12)

−

g∗
n(z; x)

(cid:12)
(cid:12)

L(z)

(cid:17)(cid:18)

1
n

n

i=1
X

sup
z∈Z

≤

(cid:16)

˜εi
n(x)
k

k

(cid:19)

= O(1)

(cid:18)

1
n

n

i=1
X

˜εi
n(x)
k

k

.
(cid:19)

ˆgER
n (z; x)

sup
z∈Z

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Lemma 2 along with Assumptions 3, 4, and 15 and the continuous mapping theorem imply 1
n
Op(n−α/2) for a.e. x
α

ˆgER
n (z; x)
(0, 1], the above two probability inequalities yield

. Consequently, supz∈Z

= Op(n−α/2) for a.e. x
P

g∗
n(z; x)

∈ X

−

n
i=1k

˜εi
n(x)
=
k
. Since

∈ X

∈

sup
z∈Z

ˆgER
n (z; x)

g(z; x)

−

(cid:12)
(cid:12)
= Op(n−α/2),

(cid:12)
(cid:12)
for a.e. x

.

∈ X

and any β > 0, there exists Mβ > 0 such that

(cid:12)
(cid:12)

sup
z∈Z

(cid:26)

(cid:12)
(cid:12)
:

ˆgER
n (z; x)

−

g(z; x)

> Mβn−α/2

< β.

(cid:12)
(cid:12)

(cid:27)

This implies that for a.e. x

(cid:12)
(cid:12)
∈ X

P

Consequently, we have for a.e. x

∈ X

P

n (x) > v∗(x) + Mβn−α/2
ˆvER

n

P

v∗(x) > ˆvER

n (x) + Mβn−α/2

n

≤

≤

≤

o

o

P

n (z∗(x); x) > v∗(x) + Mβn−α/2
ˆgER

n

P

ˆgER
n (z∗(x); x)

v∗(x)

o
> Mβn−α/2

−

n (x); x) > ˆvER

n (x) + Mβn−α/2

(cid:12)
(cid:12)

P

P

n(cid:12)
g(ˆzER
(cid:12)
n

ˆvER
n (x)

g(ˆzER

n (x); x)

o
> Mβn−α/2

β,

≤

o

β.

≤

o

≤

g(ˆzER

n(cid:12)
(cid:12)
n (x); x)
−

v∗(x)

(cid:12)
(cid:12)
are Op(n−α/2).

−

(cid:12)
(cid:12)

Therefore, both

ˆvER
n (x)

|

v∗(x)
|

−

and

(cid:12)
(cid:12)

f ∗(xi)

n
i=1k

Note that Assumptions 3, 4, (15c), and (15d) are not required to establish Theorem 13 in the ho-
moscedastic case ( ˆQn := Q∗
I). Additionally, Assumption (15b) may be weakened in this setting to
1
= Op(n−α/2) on account of Lemma 2. When Assumption 1 is replaced with
n
0
Assumption 12, rates of convergence for the root mean square deviation term
directly translate to rates of convergence of the ER-SAA estimators similar to Theorem 13 (cf. Section B of
the Appendix).

ˆfn(xi)
k

1/2 p
−→

˜εi
n(x)
k

n
i=1k

P

P

−

≡

1
n

(cid:1)

(cid:0)

2

D Analysis for the jackknife and jackknife+ estimators

In this section, we analyze the consistency, rate of convergence, and ﬁnite sample guarantees of the J-SAA
and J+-SAA estimators obtained by solving problems (7) and (8), respectively, under certain assumptions
on the true problem (1) and the prediction step (3). We omit proofs because they are similar to the proofs of

37

considered in Section 3, we consider

results in Section 3. In place of the sequence of deviation terms
the following deviation sequences

and

˜εi,J+
n

˜εi,J
n (x)
{
{
}
ˆfn(x) + ˆQn(x)ˆεi
(cid:16)
ˆf−i(x) + ˆQ−i(x)ˆεi
(cid:16)

n,J

(cid:17)
n,J

−

(cid:17)

˜εi
n(x)
}

{

:

(x)
}
f ∗(x) + Q∗(x)εi

(cid:1)
f ∗(x) + Q∗(x)εi

,

(cid:0)
−

(cid:0)

i
∀

∈
i
∀

[n],

[n].

∈

,

(cid:1)

˜εi,J
n (x) :=

˜εi,J+
n

(x) :=

We let ˆzJ
and ˆSJ+
and ˆSJ+

n(x) and ˆzJ+

n (x) denote an optimal solution to problem (7) and (8), respectively, and ˆSJ
n (x) denote the corresponding sets of optimal solutions. We assume throughout that the sets ˆSJ
n (x) are nonempty for a.e. x

.

n (x)
n (x)

We have the following analogue of Lemma 2 for the jackknife-based mean deviation terms.
ˆf−i}

Lemma 14. Given regression estimates ˆfn,
[ ˆQ−i(¯x)]−1

of f ∗ and ˆQn,

of Q∗ with [ ˆQn(¯x)]−1

0 for each i

ˆQ−i}

[n] and ¯x

{

{

:

0 and

≻

∈ X

≻

∈

∈ X

1. In the homoscedastic setting (i.e., ˆQn = ˆQ−i = Q∗

I), we have

≡

1
n

n

k

i=1
X

1
n

n

k

i=1
X
n

˜εi,J
n (x)

k≤ k

ˆfn(x)

f ∗(x)
k

−

+

1
n

˜εi,J+
n

(x)

k≤

k

i=1
X

1
n

n

k

i=1
X

ˆf−i(x)

f ∗(x)
k

−

+

ˆf−i(xi)

f ∗(xi)
,
k

−

1
n

n

k

i=1
X

ˆf−i(xi)

f ∗(xi)
.
k

−

2. In the heteroscedastic setting, we have

n

˜εi,J
n (x)
k

k ≤

i=1
X
ˆfn(x)
k

f ∗

(x)

+

ˆQn(x)
k

−

k

−

Q∗

(x)

1
n

k

(cid:18)

ˆQn(x)
k

ˆQn(x)
k

1
n

1
n

k

(cid:18)

k

(cid:18)

n

i=1
X
n

(cid:2)

(cid:13)
(cid:13)

i=1
X

(cid:2)

(cid:13)
(cid:13)

ˆQ−i(xi)

−1

Q∗

(xi)

−

ˆQ−i(xi)

(cid:3)

(cid:3)

(cid:2)

1/2

−1

2

(cid:19)

(cid:18)

(cid:13)
(cid:13)

1
n

1
n

n

n

εi
k

2

i=1
X
−1

+

k

(cid:19)
1/2

n

1
n

Q∗
k

(xi)

4
k

1/4

(cid:19)

(cid:18)

1
n

n

i=1
X

εi
k

4
k

(cid:19)

1/4

+

(cid:19)

(cid:18)

f ∗
k

(xi)

−

i=1
X
ˆf−i(xi)

(cid:3)

n

(cid:13)
(cid:13)

i=1
X

1/2

2
k

(cid:19)

and

1
n

˜εi,J +
n
k

(x)

k ≤

n

ˆf−i(x)
k

i=1
X
1
n

i=1
X
n
1
n

(cid:18)

(cid:18)

1
n

i=1
X
n

i=1
X

ˆQ−i(x)
k

2
k

ˆQ−i(x)
k

2
k

(cid:19)

(cid:18)

1/2

(cid:19)

(cid:18)

(cid:18)
1
n

1
n

+

f ∗(x)

k

−

1/2

n

i=1
X

ˆQ−i(x)
k

−

Q∗(x)

2
k

1/2

(cid:19)

(cid:18)

1
n

1
n

n

1/2

+

n

i=1
X
1/4

εi
k

2
k

(cid:19)
n

ˆQ−i(xi)

−1

−

Q∗

(xi)

−1

4

i=1
X
n

(cid:2)

(cid:13)
(cid:13)

ˆQ−i(xi)

(cid:2)

1/4

−1

4

(cid:3)

n

(cid:13)
(cid:13)

(cid:19)

(cid:18)

f ∗
k

(xi)

−

i=1
X
ˆf−i(xi)

1
n

i=1
X

(cid:2)

(cid:13)
(cid:13)

(cid:19)

(cid:18)

i=1
X

(cid:13)
(cid:13)

(cid:3)

(cid:3)

1/4

.

4
k

(cid:19)

1
n

Q∗
k

(xi)

8
k

1/8

(cid:19)

(cid:18)

1
n

n

i=1
X

εi
k

8
k

(cid:19)

1/8

+

Therefore, assumptions on the quantities appearing in Lemma 2 for the mean deviation term of the
ER-SAA may be replaced with assumptions on the quantities appearing in the above inequalities to derive
similar results for the jackknife-based estimators as the ER-SAA estimator. While we focus on analyzing
the jackknife-based SAAs when Assumption 1 holds, note that a similar analysis can be carried out when
Assumptions 12 and 13 are adapted for the jackknife-based SAAs by following the arguments in Section B.
We omit these details for brevity.

38

D.1 Consistency and asymptotic optimality

We present conditions under which the optimal value and optimal solutions to the J-SAA and J+-SAA
problems (7) and (8) asymptotically converge to those of the true problem (1). We make the following
assumptions on the consistency of the (leave-one-out version of the) regression procedure (3) that adapts
Assumption 5 for the J-SAA and J+-SAA approaches.
Assumption 5J. The regression estimates ˆfn and
of Q∗ satisfy the following consistency properties:
(5Ja) ˆfn(x)
n

ˆf−i}
(5Jc) ˆQn(x)

of f ∗ and the regression estimates ˆQn and

Q∗(x) for a.e. x

f ∗(x) for a.e. x

ˆQ−i}

∈ X

{

{

n

,

,

ˆf−i(xi)
k

−

2 p
−→

0,

(5Jd)

1
n

p
−→
ˆQ−i(xi)

−1

−

(cid:2)

(cid:3)

∈ X
Q∗(xi)

−1

2 p
−→

0.

(cid:3)

(cid:13)
(cid:13)

of f ∗ and the regression estimates ˆQn and

ˆQ−i}

{

p
−→
f ∗(xi)

(5Jb)

1
n

k

i=1
X

Assumption 5J+. The regression estimates ˆfn and
of Q∗ satisfy the following consistency properties:

ˆf−i}

{

(5J+a)

(5J+b)

1
n

1
n

n

k

k

i=1
X
n

i=1
X

f ∗(x)

ˆf−i(x)
k

p
−→

−

0 for a.e. x

,

(5J+c)

∈ X

f ∗(xi)

ˆf−i(xi)
k

4 p
−→

−

0,

(5J+d)

1
n

1
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

n

k

i=1
X
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

Q∗(x)

ˆQ−i(x)
k

2 p
−→

−

0 for a.e. x

,

∈ X

ˆQ−i(xi)

−1

Q∗(xi)

−1

4 p
−→

0.

−

(cid:2)

(cid:3)

(cid:3)

(cid:13)
(cid:13)

Section F identiﬁes conditions under which Assumptions 5J and 5J+ hold for OLS, Lasso, kNN, and RF
. We also require the following strengthening of Assumptions 3 and 4 for

regression with i.i.d. data
the J+-SAA problem.

(xi, εi)
}

{

Assumption 3J+. The function Q∗ and the data

xi

{

satisfy the weak LLNs

1
n

n

i=1
X

Q∗(xi)
k

k

8 p
−→

E[

Q∗(X)
k

k

8]

and

1
n

}
n

Assumption 4J+. The error samples

εi

{

}

n
i=1 satisfy the weak LLN

Q∗(xi)

−1

E

4 p
−→

Q∗(X)

−1

4

.

(cid:3)

i=1
X

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:2)(cid:13)
(cid:2)
(cid:13)
8 p
−→

k

εi

(cid:3)

E[

ε

k

k

(cid:13)
(cid:13)
8].

(cid:13)
(cid:13)
1
n

n

k
i=1
X

We now state conditions under which the sequence of objective functions of problems (7) and (8) converge
. We group the results for the J-SAA

uniformly to the objective function of the true problem (1) on the set
and J+-SAA problems for brevity (the individual results are apparent).

Z

Proposition 15. Suppose Assumptions 1 through 4 and Assumptions 3J+, 4J+, 5J, and 5J+ hold. Then,
for a.e. x
, the sequences of objective functions of J-SAA and J+-SAA problems (7) and (8) converge
uniformly in probability to the objective function of the true problem (1) on the feasible region

∈ X

.

Z

Proposition 15 helps us establish conditions under which the optimal objective values and solutions of

the J-SAA and J+-SAA problems (7) and (8) converge to those of the true problem (1).

Theorem 16. Suppose Assumptions 1 through 4 and Assumptions 3J+, 4J+, 5J, and 5J+ hold. Then,
p
ˆSJ
we have ˆvJ
n (x), S∗(x)
−→

ˆSJ+
n (x), S∗(x)

v∗(x), ˆvJ+

v∗(x), D

n (x)

g(z; x)

n (x)

0, D

p
−→

p
−→

p
−→

p
−→

0,

sup
z∈ ˆSJ

n (x)

(cid:17)

(cid:16)

(cid:17)

v∗(x), and

sup
z∈ ˆSJ+
n (x)

g(z; x)

p
−→

(cid:16)
v∗(x) for a.e. x

.

∈ X

Assumptions 3, 4, 3J+, 4J+, (5Jc), (5J+c), (5Jd), and (5J+d) are not required to establish Proposi-
tion 15 and Theorem 16 in the homoscedastic case (assumptions that only involve ˆQn and
may be
omitted in this setting). Additionally, Assumptions (5Jb) and (5J+b) may be weakened in this setting to
1
n

0 on account of Lemma 14.

ˆQ−i}

f ∗(xi)

{

n
i=1k

ˆf−i(xi)
k

p
−→

−

P

39

D.2 Rates of convergence

We derive rates of convergence of the optimal objective value of the sequence of J-SAA and J+-SAA prob-
lems (7) and (8) to the optimal objective value of the true problem (1). To enable this, we make the follow-
ing assumptions on the regression procedure (3) that adapt Assumption 15 to strengthen Assumptions 5J
and 5J+. Assumptions 15J and 15J+ ensure that the deviations of the J-SAA and J+-SAA problems from
the FI-SAA problem (2) converge at a certain rate.

Assumption 15J. There is a constant 0 < α
1 (that is independent of the number of samples n, but
could depend on the dimension dx of the covariates X) such that the regression procedure (3) satisﬁes the
following asymptotic convergence rate criterion:

≤

(15Ja)

(15Jb)

(15Jc)

(15Jd)

ˆfn(x)
n

k

i=1
X
ˆQn(x)
n

k
1
n

k
1
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

−
f ∗(xi)

f ∗(x)
k
ˆf−i(xi)
k

−

= Op(n−α/2) for a.e. x

,

∈ X

2 = Op(n−α),

Q∗(x)
k
−1

−
ˆQ−i(xi)

= Op(n−α/2) for a.e. x

,

∈ X

Q∗(xi)

−1

2

= Op(n−α).

−

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

Assumption 15J+. There is a constant 0 < α
1 (that is independent of the number of samples n, but
could depend on the dimension dx of the covariates X) such that the regression procedure (3) satisﬁes the
following asymptotic convergence rate criterion:

≤

(15J+a)

(15J+b)

(15J+c)

(15J+d)

1
n

1
n

1
n

1
n

n

i=1
X
n

i=1
X
n

i=1
X
n

i=1
X

k

k

k

f ∗(x)

ˆf−i(x)
k

−

= Op(n−α/2) for a.e. x

,

∈ X

f ∗(xi)

ˆf−i(xi)
k

−

4 = Op(n−2α),

Q∗(x)

ˆQ−i(x)
k

−

2 = Op(n−α) for a.e. x

,

∈ X

ˆQ−i(xi)

−1

−

Q∗(xi)

−1

4

= Op(n−2α).

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

Section F demonstrates that Assumptions 15J and 15J+ hold with rates similar to those in Assumption 15
is i.i.d. Along with Lemma 14, Assumptions 15J and 15J+ imply that the mean
= Op(n−α)

when the data
deviation terms for the J-SAA and J+-SAA approaches can be bounded as 1
n
and 1
n

(xi, εi)
}
= Op(n−α) for a.e. x

˜εi,J
n (x)
k

{
˜εi,J+
n

n
i=1k

We now establish rates at which the optimal objective value of the J-SAA and J+-SAA problems converge
to the optimal objective value of the true problem (1). We hide the dependence of the convergence rate on
the dimensions dx and dy of the covariates X and random vector Y . The analysis in the next section can
account for how these dimensions aﬀect the rate of convergence.

n
i=1k

(x)
k

∈ X

P

P

.

Theorem 17. Suppose Assumptions 1, 3, 4, 3J+, 4J+, 15J, and 15J+ hold. Then, for a.e. x
v∗(x)
have
|
|
g(ˆzJ+

= Op(n− α
= Op(n− α

v∗(x)
|
v∗(x)

= Op(n− α

ˆvJ
n (x)
n (x); x)

n(x); x)

ˆvJ+
n (x)

2 ),
2 ).

v∗(x)

g(ˆzJ

= Op(n− α

2 ),

−

−

|

, we
∈ X
2 ), and

−
−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
rem 17 in the homoscedastic case ( ˆQn = ˆQ−i = Q∗
may be weakened in this setting to 1
f ∗(xi)
n

Assumptions 3, 4, 3J+, 4J+, (15Jc), (15J+c), (15Jd), and (15J+d) are not required to establish Theo-
I). Additionally, Assumptions (15Jb) and (15J+b)
= Op(n−α/2) on account of Lemma 14.

(cid:12)
(cid:12)

n
i=1k

≡
ˆf−i(xi)
k

−

P

40

D.3 Finite sample guarantees

We now establish exponential convergence of solutions to the J-SAA and J+-SAA problems to solutions to
the true problem (1) under additional assumptions. We begin by adapting Assumption 11 to assume that
the regression procedure (3) satisﬁes the following large deviation properties.

Assumption 11J. The regression estimates ˆfn,
properties: for any constant κ > 0 and n
¯βJ
f (n, κ), K J
limn→∞ βJ

Q(κ, x), ¯K J
Q(n, κ, x) =

Q(n, κ, x), and ¯βJ

, and limn→∞ ¯βJ

Q(κ), βJ

Q(n, κ) =

∈

{

ˆf−i}

, ˆQn, and

ˆQ−i}
N, there exist positive constants K J
Q(n, κ), with limn→∞ βJ
f (n, κ, x) =

possess the following large deviation
f (n, κ, x),
,
∞

f (κ, x), ¯K J

f (n, κ) =

f (κ), βJ

{

, limn→∞ ¯βJ
, satisfying

for each κ > 0 and a.e. x

∞
∈ X

(11Ja) P

(11Jb) P

(11Jc) P

(11Jd) P

k
(cid:8)
1
n

(cid:26)

k
1
(cid:8)
n

i=1
X
Q∗(x)
n

∞
ˆfn(x)
k

f ∗(x)
n

−

f ∗(xi)

−

k

> κ

≤
(cid:9)
ˆf−i(xi)
k

∞
βJ
f (n, κ, x)

−
(cid:0)
¯K J
f (κ) exp

K J

f (κ, x) exp

2 > κ2

≤

(cid:27)

K J

Q(κ, x) exp

−
(cid:0)
βJ
Q(n, κ, x)

> κ

ˆQn(x)
−
k
−1
ˆQ−i(xi)

(cid:9)
−

≤

Q∗(xi)

−1

−
(cid:0)
2 > κ2

≤

for a.e. x

,

∈ X

(cid:1)
¯βJ
f (n, κ)

,

(cid:1)

for a.e. x

,

∈ X

¯K J

(cid:1)
Q(κ) exp

¯βJ
Q(n, κ)

.

−
(cid:0)

(cid:2)

(cid:3)

(cid:26)

(cid:13)
(cid:2)
(cid:13)

i=1
X

(cid:27)
(cid:3)
Assumption 11J+. The regression estimates ˆfn,
viation properties: for any constant κ > 0 and n
βJ+
Q (κ), βJ+
f
limn→∞ ¯βJ+
Q (n, κ, x) =
x

Q (κ, x), ¯K J+

, limn→∞ βJ+

(n, κ, x), ¯βJ+

(n, κ), K J+

, satisfying

(n, κ) =

∞

(cid:13)
(cid:13)

∞

f

f

, ˆQn, and

ˆf−i}
ˆQ−i}
{
N, there exist positive constants K J+
Q (n, κ, x), and ¯βJ+
Q (n, κ), with limn→∞ βJ+

(cid:1)
possess the following large de-
(κ),
,
∞
for each κ > 0 and a.e.

(κ, x), ¯K J+
(n, κ, x) =

, and limn→∞ ¯βJ+

Q (n, κ) =

∈

{

f

f

f

∞

∈ X

(11J+a) P

(11J+b) P

(11J+c) P

(11J+d) P

1
n

1
n

1
n

1
n

(cid:26)

(cid:26)

(cid:26)

(cid:26)

n

k

k

k

i=1
X
n

i=1
X
n

i=1
X
n

i=1
X

(cid:13)
(cid:2)
(cid:13)

f ∗(x)

ˆf−i(x)
k

−

> κ

K J+
f

≤

(cid:27)

f ∗(xi)

ˆf−i(xi)
k

−

4 > κ4

≤

(cid:27)

Q∗(x)

ˆQ−i(x)
k

−

2 > κ2

≤

(cid:27)

(κ, x) exp

−
(cid:0)
(κ) exp

¯K J+
f

−
(cid:0)
Q (κ, x) exp

K J+

ˆQ−i(xi)

−1

−

Q∗(xi)

−1

4 > κ4

≤

(cid:27)

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

βJ+
f

(n, κ, x)

for a.e. x

,

∈ X

¯βJ+
f

(n, κ)

(cid:1)

,

(cid:1)

βJ+
Q (n, κ, x)

for a.e. x

,

∈ X

−
(cid:0)
¯K J+

Q (κ) exp

(cid:1)
¯βJ+
Q (n, κ)

.

(cid:1)

−
(cid:0)

Assumptions 11J and 11J+ strengthen Assumptions 15J and 15J+ by imposing restrictions on the tails
of the regression estimators. Please see the discussion in Section F for when these strengthened assumptions
. We also require the following strengthening of Assumptions 9 and 10
are satisﬁed with i.i.d. data
for the J+-SAA problem.

(xi, εi)
}

{

Assumption 9J+. For any κ > 0 and n
and ¯γJ+

Q (n, κ), with lim
n→∞

γJ+
Q (n, κ) =

∈
and lim
n→∞

∞

N, there exist positive constants LJ+

Q (κ), γJ+
for each κ > 0, such that

¯γJ+
Q (n, κ) =

Q (n, κ), ¯LJ+

Q (κ),

∞

1/4

+ κ

1
n

P

(cid:26)(cid:18)

n

Q∗(xi)

−1

4

1/4

>

E

Q∗(X)

−1

4

i=1
X
(cid:13)
(cid:2)
(cid:13)
P

(cid:26)(cid:18)

(cid:19)

(cid:13)
(cid:13)
Q∗(xi)
k

8

k

1
n

(cid:3)
n

i=1
X

(cid:19)

(cid:16)
1/8

h(cid:13)
(cid:2)
(cid:13)
E
>

(cid:13)
(cid:3)
(cid:13)
Q∗(X)
k

k

i(cid:17)

8

1/8

+ κ

(cid:0)

(cid:2)

(cid:3)(cid:1)

41

LJ+

Q (κ) exp(

−

¯LJ+

Q (κ) exp(

−

γJ+
Q (n, κ)),

¯γJ+
Q (n, κ)).

≤

(cid:27)

≤

(cid:27)

Assumption 10J+. For any κ > 0 and n
and ¯γJ+

(n, κ) =

(n, κ), with lim
n→∞

γJ+
ε

∈
and lim
n→∞

ε

∞

¯γJ+
ε

(n, κ) =

∞

N, there exist positive constants LJ+

(κ), γJ+

(n, κ), ¯LJ+

ε

(κ),

ε
for each κ > 0, such that

ε

1
n

P

(cid:26)(cid:18)

P

(cid:26)(cid:18)

i=1
X
n
1
n

i=1
X

n

1/2

εi

2

k

k

(cid:19)

1/8

εi

8

k

k

(cid:19)

>

E

ε

k

k

(cid:16)
(cid:2)
> (E

1/2

2

+ κ

(cid:3)(cid:17)
)1/8 + κ
8

ε

k

k

(cid:2)

(cid:3)

LJ+
ε

(κ) exp(

−

γJ+
ε

(n, κ)),

¯LJ+
ε

(κ) exp(

−

¯γJ+
ε

(n, κ)).

≤

(cid:27)

≤

(cid:27)

The next result presents conditions under which the maximum deviations of the J-SAA and J+-SAA
objectives from the FI-SAA objective satisfy qualitatively similar large deviations bounds as that in As-
sumption 8.

Lemma 18. Suppose Assumptions, 1, 8, 9, 10, 9J+, 10J+, 11J and 11J+ hold. Then for any constant κ > 0,
, there exist positive constants ¯K J (κ, x), ¯K J+(κ, x), ¯βJ (n, κ, x), and ¯βJ+(n, κ, x),
n
with limn→∞ ¯βJ (n, κ, x) =

and limn→∞ ¯βJ+(n, κ, x) =

for each κ > 0 and a.e. x

N, and a.e. x

, satisfying

∈ X

∈

∞

∞

∈ X

P

sup
z∈Z

(cid:26)

P

sup
z∈Z

(cid:26)

(cid:12)
(cid:12)

ˆgJ
n(z; x)

g∗
n(z; x)

> κ

(cid:12)
(cid:12)
ˆgJ+
n (z; x)

(cid:12)
(cid:12)
g∗
n(z; x)

−

−

≤

≤

(cid:27)

> κ

(cid:27)

(cid:12)
(cid:12)

¯K J (κ, x) exp

−
(cid:0)
¯K J+(κ, x) exp

¯βJ (n, κ, x)

, and

(cid:1)

¯βJ+(n, κ, x)

.

−
(cid:0)

(cid:1)

We now ﬁnite sample guarantees for the distances between solutions to the J-SAA and J+-SAA prob-

lems (7) and (8) and the set of optimal solutions to the true problem (1).

Theorem 19. Suppose Assumptions, 1, 8, 9, 10, 9J+, 10J+, 11J and 11J+ hold. Then, for each n
and a.e. x
∈ X
with limn→∞ γJ (n, η, x) =

N
, given η > 0, there exist positive constants QJ (η, x), QJ+(η, x), γJ (n, η, x), and γJ+(n, η, x),

and limn→∞ γJ+(n, η, x) =

for each η > 0 and a.e. x

, such that

∈

∞
dist(ˆzJ
dist(ˆzJ+
(cid:8)

n (x), S∗(x))
n (x), S∗(x))

P

P

η

η

(cid:9)

≥

≥

≤

≤

(cid:9)

∞
QJ (η, x) exp(
−
QJ+(η, x) exp(

γJ (n, η, x)),

and

γJ+(n, η, x)).

−

∈ X

(cid:8)

Assumptions 9, 10, 9J+, 10J+, (11Jc), (11J+c), (11Jd), and (11J+d) are not required to establish
Theorem 19 in the homoscedastic case. Additionally, Assumptions (11Jb) and (11J+b) may be weakened
in this setting to P
ˆf−i(xi)
> κ
k

f (κ) exp
on account of Lemma 14.

1
n
(κ) exp
P

f ∗(xi)
¯βJ+
f

ˆf−i(xi)
k

¯βJ
f (n, κ)

−
(n, κ)

n
i=1k

n
i=1k

¯K J+
(cid:8)
f

and P

f ∗(xi)

−
(cid:0)

> κ

¯K J

P

−

≤

≤

1
n

(cid:9)

(cid:8)

(cid:1)

(cid:9)

−
(cid:0)

(cid:1)

E Application to two-stage stochastic programming problems

We present a class of stochastic programs that satisfy Assumptions 1, 2, 6, 8, 12, 13, and 14. We ﬁrst consider
a class of two-stage stochastic programs with continuous recourse decisions that subsumes Example 1, our
running example of two-stage stochastic LP. We then brieﬂy outline the veriﬁcation of these assumptions for
a broader class of stochastic programs.

Consider ﬁrst the two-stage stochastic program

E [c(z, Y )] := p(z) + E [V (z, Y )] ,

min
z∈Z

(21)

where the second-stage function V is deﬁned by the optimal value of the following LP:

We make the following assumptions on problem (21).

V (z, y) := min
v∈Rdv
+

cT
v v : W v = h(y)

T (y, z)

.

−

(cid:8)

(cid:9)

42

Assumption 16. The set
cT
v

λ : λTW

Z

≤

is nonempty, and the value function V (z, y) < +

for each (z, y)

.

∈ Z × Y

∞

is nonempty and compact, the matrix W has full row rank, the set Λ :=

(cid:9)

(cid:8)
Assumption 17. The functions h and T (
·
constants Lh and LT,y(z), and the functions p and T (y,
∈ Y
with Lipschitz constants Lp and LT,z(y). Additionally, the Lipschitz constants for the function T satisfy
sup
z∈Z

for each z
) are Lipschitz continuous on

with Lipschitz
for each y

, z) are Lipschitz continuous on

and sup
y∈Y

LT,y(z) < +

LT,z(y) < +

∈ Z
Z

.
∞

∞

Y

·

Assumption 18. The support
on

. Additionally, E
h

X
sup
(z,x)∈Z×X k

X

of the covariates X is compact and the functions f ∗ and Q∗ are continuous
h(f ∗(x) + Q∗(x)ε)

< +

T (f ∗(x) + Q∗(x)ε, z)
k

−

.
∞

i

Let vert(Λ) denote the ﬁnite set of extreme points of the dual feasible region Λ, and deﬁne
vert(Λ), and z

T (Y, z)) for each Y

, λ

λT (h(Y )

.

(Y, λ, z) :=

V

−

∈ Y

∈

∈ Z

Assumption 19. We have E
sup
z∈Z k
h

Q∗(x)ε, λ, z)
z

and a.e. x

−

E¯ε∼Pε [

V
.

∈ Z

∈ X

h(Y )

T (Y, z)
k

−

2

< +

∞

. Additionally, the random variable

i
(f ∗(x) + Q∗(x)¯ε, λ, z)] is sub-Gaussian with variance proxy σ2

c (x) for each λ

(f ∗(x) +

V

vert(Λ),

∈

Note that the ﬁrst-stage feasible set

can include integrality constraints. Our running example of two-
stage stochastic LP with OLS regression ﬁts within the above setup and readily satisﬁes Assumptions 16
and 17. It also satisﬁes Assumption 18 when E [
and Assumption 19 when the error ε is sub-
Gaussian. Additionally, under Assumption 16, we have by LP duality that for each y

] < +
k

∞

Z

k

ε

:

∈ Y

V (z, y) = max
λ∈Λ

λT (h(y)

−

T (y, z)) = max

λ∈vert(Λ)

λT (h(y)

−

T (y, z)) .

(22)

Proposition 20. Suppose Assumptions 16, 17, 18, and 19 hold and the data
satisﬁes Assumptions 1, 2, 6, 8, and 14.

εi

{

}

is i.i.d. Then, problem (21)

Proof. We have by Assumptions 16 and 17 that for any y, ¯y

and z

:

∈ Z

∈ Y

c(z, y)

|

c(z, ¯y)
|

−

=

≤

≤

≤

max
λ∈vert(Λ)

(cid:12)
(cid:12)
max
(cid:12)
λ∈vert(Λ)

λT (h(y)

λT (h(y)

−

−

T (y, z))

max
−
λ∈vert(Λ)
h(¯y)) + λT (T (¯y, z)

λT (h(¯y)

−
T (y, z))

−

T (¯y, z))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
max
(cid:12)
λ∈vert(Λ) k

λ

kk

h(y)

h(¯y)
k

−

+ max

λ∈vert(Λ) k

λ

kk

T (¯y, z)

(cid:12)
(cid:12)
−

T (y, z)
k

[Lh + LT,y(z)]

max
λ∈vert(Λ) k

λ
k

y

.

¯y
k

−

k
(cid:17)

(cid:16)

Therefore, the Lipschitz continuity Assumption 1 holds since supz∈Z LT,y(z) < +
+

and maxλ∈vert(Λ)k

λ
k

<

∞

.
∞
The function c(
·
Lipschitz continuous on
for each y

, y) is continuous on

for each y

∈ Y
Z
and equation (22) implies that V (
·
:

Z

. Furthermore, for any ¯z

∈ Y

∈ Z

by virtue of Assumptions 16 and 17 since p is
, y) is a ﬁnite maximum of continuous functions

c(¯z, Y )
|

|

=

≤

(cid:12)
(cid:12)
max
(cid:12)
z∈Z |

p(¯z) + max

λ∈vert(Λ)

λT (h(Y )

−

T (Y, ¯z))

p(z)
|

+

(cid:16)

max
λ∈vert(Λ) k

λ
k

(cid:17)(cid:16)

(cid:12)
(cid:12)
h(Y )
(cid:12)

max
z∈Z k

T (Y, z)
k

−

.

(cid:17)

The uniform weak LLN Assumption 2 then holds by virtue of Assumption 16, the ﬁrst part of Assumption 19,
and Theorem 7.48 of Shapiro et al. [73], which also implies that the objective function of the true problem (1)
is continuous on

.

Z

43

The function c(
·

, f ∗(
·
and 18 since p is Lipschitz continuous on
independent of y
continuous functions for each ε

) + Q∗(
·

, c(
·
(see below), and equation (22) implies that V (
·
∈ Z × X

Ξ. Additionally, for any (¯z, ¯x)

)ε) is continuous on

for each ε

Z × X

∈ Y

∈
Z
) + Q∗(
, f ∗(
·
·
:

, y) is Lipschitz continuous on

Z

∈

Ξ by virtue of Assumptions 16, 17,
with a Lipschitz constant
)ε) is a ﬁnite maximum of

c(¯z, f ∗(¯x) + Q∗(¯x)ε)
|

|
=

(cid:12)
(cid:12)
max
(cid:12)
z∈Z |

≤

p(¯z) + max

λ∈vert(Λ)

λT (h(f ∗(¯x) + Q∗(¯x)ε)

−

T (f ∗(¯x) + Q∗(¯x)ε, ¯z))

p(z)
|

+

(cid:16)

max
λ∈vert(Λ) k

λ
k

(cid:17)(cid:16)

max
(z,x)∈Z×Xk

h(f ∗(x) + Q∗(x)ε)

(cid:12)
(cid:12)
(cid:12)

T (f ∗(x) + Q∗(x)ε, z)
k

−

.

(cid:17)

Therefore, Assumptions 16, 17, and 18 along with Theorem 7.48 of Shapiro et al. [73] together implies that
0, which in turn implies Assumption 6.
sup(z,x)∈Z×X |
:

g∗
g(z; x)
n(z; x)
|
Next, note that for any z, ¯z

−

p
−→
∈ Z

c(z, Y )

|

c(¯z, Y )
|

−

=

p(z) + max

λT (h(Y )

λ∈vert(Λ)

(cid:12)
(cid:12)
p(z)
(cid:12)
≤ |

p(¯z)
|

−

+ max

λ∈vert(Λ)

T (Y, z))

−
λT (T (Y, ¯z)

max
λ∈vert(Λ)

p(¯z)

−
T (Y, z))

λT (h(Y )

T (Y, ¯z))

−

(cid:12)
(cid:12)
(cid:12)

−

−

Additionally, for any ¯z

≤

Lp + LT,z(Y )
h
:

∈ Z

(cid:12)
(cid:12)

max
λ∈vert(Λ) k

z

.

¯z
k

−

λ
k

k
(cid:17)i

(cid:12)
(cid:12)

(cid:16)

E

(c(¯z, Y ))2

= E

p(¯z) + max

λ∈vert(Λ)

λT (h(Y )

−

(cid:2)

(cid:3)

≤

≤

h(cid:16)

2(p(¯z))2 + 2

2(p(¯z))2 + 2

max
λ∈vert(Λ) k

max
λ∈vert(Λ) k

(cid:16)

(cid:16)

2

E

λ
k

λ
k

(cid:17)

2

(cid:17)

2

T (Y, ¯z))

h(Y )

k

(cid:17)
i
T (Y, ¯z)
k

2

−

(cid:2)
E

sup
z∈Zk
h

h(Y )

(cid:3)
T (Y, z)
k

2

−

.

i

Consequently, the functional CLT Assumption 14 holds by virtue of Assumptions 16 and 17, and the ﬁrst
part of Assumption 19, see page 164 of Shapiro et al. [73] for details.

Finally, note that for any z

and a.e. x

:

∈ X

∈ Z

c(z, f ∗(x) + Q∗(x)ε)
=V (z, f ∗(x) + Q∗(x)ε)

g(z; x)
E¯ε∼Pε [V (z, f ∗(x) + Q∗(x)¯ε)]

−
−

= max

λ∈vert(Λ)V

max
λ∈vert(Λ)

≤

V

(f ∗(x) + Q∗(x)ε, λ, z)

−
(f ∗(x) + Q∗(x)ε, λ, z)

E¯ε∼Pε

(cid:20)
E¯ε∼Pε [

V

−

max
λ∈vert(Λ)V

(f ∗(x) + Q∗(x)¯ε, λ, z)
(cid:21)

(f ∗(x) + Q∗(x)¯ε, λ, z)]

Consequently, for any z

E

exp

(cid:8)
∈ Z
t

and a.e. x

:

c(z, f ∗

(x) + Q∗

∈ X
(x)ε)

−
(x) + Q∗

(cid:9)

g(z; x)

(cid:1)(cid:1)(cid:3)
(x)ε, λ, z)

E¯ε∼Pε [
V

−

(f ∗

(x) + Q∗

(x)¯ε, λ, z)]

(f ∗

(x) + Q∗

(x)ε, λ, z)

E

exp

(f ∗

(x) + Q∗

(x)ε, λ, z)

E

≤

E

≤

≤

≤|

(cid:2)

(cid:0)

exp

(cid:20)

(cid:18)

(cid:0)
t max
λ∈vert(Λ)

(f ∗

V

max
λ∈vert(Λ)

(cid:20)

exp

Xλ∈vert(Λ)
vert(Λ)

h

exp

(cid:18)

|

(cid:8)
t

V
(cid:0)
t

(cid:16)

V
(cid:16)
(cid:0)
σ2
c (x)t2
2

(cid:19)

44

(cid:19)(cid:21)

(cid:9)

E¯ε∼Pε [

V
E¯ε∼Pε [
V

−

−

(f ∗

(x) + Q∗

(x)¯ε, λ, z)]

(f ∗

(x) + Q∗

(x)¯ε, λ, z)]

(cid:1)(cid:17)(cid:21)

(cid:1)(cid:17)i

where the last inequality follows by Assumption 19. Therefore, the large deviation Assumption 8 follows by
Assumptions 16, 17, and 19 and Theorem 7.65 of Shapiro et al. [73].

The assumption supy∈Y LT,z(y) < +

can be relaxed to assume that the moment generating function
(mgf) of LT,z(Y ) is ﬁnite valued in a neighborhood of zero, see Assumption (C3) and Theorem 7.65 in
Section 7.2.9 of Shapiro et al. [73]. The discussion in Section 3 following Assumptions 2 and 8 and the
discussion following Assumption 14 provide avenues for relaxing the i.i.d. assumption on the errors
.
}
The conclusions of Proposition 20 can also be established for the case of objective uncertainty (i.e., only the
objective coeﬃcients q depend on Y ) if Assumptions 16, 17, 18, and 19 are suitably modiﬁed. Note that the
second parts of Assumptions 12 and 13 also readily hold for problem (21) whenever Assumption 1 holds.

∞

εi

{

Generalization to a broader class of stochastic programs. Suppose the feasible region
support
problem (1) is of the form:

are nonempty and compact, functions f ∗ and Q∗ are continuous on

and the
, and the function c in

Z

X

X

|I|

c(z, Y ) :=

ci(z)φi(Y )

i=1
X

}
with Lipschitz constant Lc,i and the function φi is continuously diﬀerentiable on

∈ I

|I|

{

R

{

ci :

and

for functions
Z →
Lipschitz continuous on
Additionally, assume for each i
E

Y →
that E
φi(f ∗(x) + Q∗(x)ε + v)
k

∈ I

φi :

Z

R

, where

}
(φi(Y ))2
< +

< +

∞

∞
, E [maxx∈X |
∈ X

maxv∈Bδ(x)(0)k∇
h

, where δ is deﬁned in Assumption 12.
for a.e. x
(cid:3)
∞
The above conditions hold, e.g., if each function φi is polynomial and the errors ε are sub-exponential.
We argue that the above class of stochastic programs satisﬁes Assumptions 2, 6, 12, and 14 whenever the
errors

are i.i.d. Additionally, we argue that if ﬁnite sample guarantees of the form

εi

2
(cid:2)
i

the function ci is
.
Y
, and

] < +

∞

φi(f ∗(x) + Q∗(x)ε)
|

< +

. Suppose for each i

max

v∈Bδ(x)(0)k∇

φi(f ∗

(x) + Q∗

(x)εj + v)

1/2

2
k

(cid:19)

>

E

(cid:18)

max

v∈Bδ(x)(0)k∇

"

φi(f ∗

(x) + Q∗

(x)ε + v)

1/2

2
k

#(cid:19)

+ κ

(cid:27)

}
n

{

1
n

P

≤

(cid:26)(cid:18)
j=1
X
Jφ,i(κ; x) exp(

γφ,i(n, κ; x))

−

N, and a.e. x

, n

∈ I

hold for each κ > 0, i
E [φi(Y )]
we argue that if the moment generating functions of
| −
E [
are ﬁnite-valued for all z
in a neighborhood of the origin [73, cf. Section 7.2.9], then
P
Assumption 8 also holds. The above two conditions hold, e.g., if each function φi is polynomial and the
distribution of the errors ε is suﬃciently light-tail.

, then the second part of Assumption 13 holds. Finally,

|I|
i=1 ci(z)

φi(Y )
|

|I|
i=1 Lc,i

φi(Y )

φi(Y )

∈ Z

∈ X

and

P

−

∈

(cid:2)

(cid:3)

(cid:2)

(cid:3)

|

|

]

for each y

∈ Y

since each ci is assumed to be Lipschitz continuous

The function c(
·

on

. Furthermore, for any ¯z

, y) is continuous on

:

∈ Z

Z

Z

|I|

c(¯z, Y )
|

|

=

ci(¯z)φi(Y )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
< +

Therefore, the fact that E
that the uniform weak LLN Assumption 2 holds.

(φi(Y ))2

,
∞

i
∀

∈ I

|I|

≤

i=1(cid:16)
X

sup
z∈Z|

ci(z)
|

φi(Y )
|

.

|
(cid:17)

, along with Theorem 7.48 of Shapiro et al. [73] implies

The function c(
·

, f ∗(
·

(cid:2)
)+ Q∗(
·

(cid:3)

)ε) is continuous on

for each ε

Z × X

be Lipschitz continuous and f ∗ and Q∗ are assumed to be continuous. Furthermore, for any (¯z, ¯x)

∈

Ξ since each ci and φi are assumed to
:

∈ Z × X

c(¯z, f ∗(¯x) + Q∗(¯x)ε)
|

|

=

Therefore, the fact that E [supx∈X |
et al. [73] implies that sup(z,x)∈Z×X |

|I|

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci(¯z)φi(f ∗(¯x) + Q∗(¯x)ε)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
] < +
p
−→

φi(f ∗(x) + Q∗(x)ε)
|
g(z; x)
|

g∗
n(z; x)

−

≤

|I|

i=1(cid:16)
X
i
,
∀
∞

∈ I

sup
z∈Z|

ci(z)
|

(cid:17)(cid:16)

sup
x∈X|

φi(f ∗(x) + Q∗(x)ε)
|

.

(cid:17)

, along with Theorem 7.48 of Shapiro

0, which in turn implies Assumption 6.

45

Next, note that for any z, ¯z

:

∈ Z

c(z, Y )

|

−

c(¯z, Y )
|

ci(z)

ci(¯z)

−

|I|

=

i=1
X
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
φi(Y )
|

(cid:2)

≤

O(1)

≤

φi(Y )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(φi(Y ))2

(cid:3)
E

i=1
X

|I|
i=1

|I|

ci(z)

|

−

ci(¯z)

||

φi(Y )

| ≤

|I|

(cid:18)

i=1
X

Lc,i|

φi(Y )
|

z

k

.

¯z
k

−

(cid:19)

< +

. Additionally, for any ¯z

∞

:

∈ Z

with E

|I|

i=1 Lc,i|

(cid:2)(cid:0)P
(c(¯z, Y ))2

E

= E

|I|

(cid:3)

(cid:1)
ci(¯z)φi(Y )

P
2

(cid:2)

(cid:3)

(cid:20)(cid:18)

i=1
X

(cid:19)

(cid:21)

(cid:2)
E

(cid:20)

≤

|I|

(cid:3)

(ci(¯z))2(φi(Y ))2

|I|

i=1
X

|I|

≤ |I|

(cid:21)

i=1
X

(ci(¯z))2E

(φi(Y ))2

< +

.
∞

(cid:2)

(cid:3)

|I|

Consequently, the CLT Assumption 14 holds by arguments in page 164 of Shapiro et al. [73]. Since
, Assumption 8 holds if the stated conditions on
φi(Y )
hold.

is a Lipschitz constant for c(
·
and

φi(Y )
|
|I|
i=1 ci(z)

, Y ) on
Z
|I|
i=1 Lc,i

E [φi(Y )]

φi(Y )

E [

]

i=1 Lc,i|
the mgfs of
P

We focus on verifying the second parts of Assumptions 12 and 13 next. We have for any z
P

(cid:2)

(cid:2)

(cid:3)

(cid:3)

, y

,

∈ Y

∈ Z

φi(Y )
|

| −

−

|

|

and ¯y

P
∈ Bδ(x)(y)

:

∩ Y

c(z, ¯y)

|

c(z, y)
|

−

=

|I|

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci(z)

φi(¯y)

−

(cid:2)

φi(y)

(cid:3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|I|

≤

ci(z)

||

|

φi(¯y)

φi(y)
|

−

i=1
X

|I|

ci(z)
|

≤

(cid:18)

|
i=1
X

max

v∈Bδ(x)(y)k∇

φi(v)
k

¯y

y

,

k

−

k
(cid:19)

where the last inequality follows by the mean-value theorem. Therefore, we can choose

Lδ(x)(z, y) :=

|I|

(cid:18)

i=1
X

ci(z)
|

|

max

v∈Bδ(x)(y)k∇

φi(v)
k

(cid:19)

in Assumptions 12 and 13. Noting that

1
n

sup
z∈Z

n

j=1
X

L2
δ(x)(z, f ∗(x) + Q∗(x)εj )

≤

1
n

n

|I|

sup
z∈Z(cid:18)
|I|

i=1
X

j=1
X

ci(z)
|

|

O(1)

≤

i=1(cid:16)
X

sup
z∈Z |

ci(z)
|

2

max

v∈Bδ(x)(0)k∇

φi(f ∗(x) + Q∗(x)εj + v)
k

2

(cid:19)

1
n

(cid:17)

n

j=1
X

max

v∈Bδ(x)(0)k∇

φi(f ∗(x) + Q∗(x)εj + v)
k

2,

we see that supz∈Z
tion (12b) holds. Additionally, we have for each ε

n
j=1 L2

1
n

P

Ξ and a.e. x

:

∈ X

∈

δ(x)(z, f ∗(x) + Q∗(x)εj ) = Op(1) by the weak LLN, which implies that Assump-

δ(x)(z, f ∗(x) + Q∗(x)ε)
L2

sup
z∈Z

|I|

≤ |I|

sup
z∈Z|

i=1(cid:16)
X

2

ci(z)
|

(cid:17)

max

v∈Bδ(x)(0)k∇

φi(f ∗(x) + Q∗(x)ε + v)
k

2.

Therefore, ﬁnite sample guarantees for maxv∈Bδ(x)(0)k∇
to ﬁnite sample guarantees of the form in Assumption 13.

φi(f ∗(x) + Q∗(x)ε + v)
k

2, i

∈ I

, directly translate

F Some prediction setups that satisfy our assumptions

We verify that Assumptions 5, 11, and 15 and the corresponding assumptions for the J-SAA and J+-SAA
problems hold for speciﬁc regression procedures, and point to resources within the literature for verifying

46

these assumptions more broadly. We do not attempt to be exhaustive and, for the most part, restrict our
attention to M-estimators [76, 78], which encapsulate a rich class of prediction techniques. We often also
consider the special case where the true model can be written as Y = f (X; θ∗) + Q(X; π∗)ε and the goal of
the regression procedure (3) is to estimate the ﬁnite-dimensional parameters θ∗
Π using the
data

Dn. To summarize, we largely consider the regression setup (possibly with a regularization term)

Θ and π∗

∈

∈

ˆθn ∈

arg min
θ∈Θ

1
n

n

ℓ

yi, f (xi; θ)

i=1
X

(cid:0)

(cid:1)

for estimating θ∗ with a particular emphasis on the squared loss ℓ(y, ˆy) :=
problem

E(Y,X)[ℓ (Y, f (X; θ))]

min
θ∈Θ

y

k

ˆy
k

−

2. We call the optimization

the population regression problem, where the above expectation is taken with respect to the joint distribution
of (Y, X). We mostly assume that the solution set of the population regression problem is the singleton
,
and assume throughout this section that X1 ≡
f ∗(X))T
uses the fact that E
(Y
determine the best regression estimate ˆQn of Q∗ in the model class
obtain an estimate ˆθn of θ∗, we can estimate π∗ using

}
We consider two distinct approaches for estimating the heteroscedasticity function Q∗. The ﬁrst approach
= Q∗(x)Q∗(x)T and plugs in ˆfn instead of f ∗ to
. For the parametric case, once we

f ∗(X))(Y

X = x

θ∗

Q

1.

−

−

{

(cid:3)

(cid:2)

|

ˆπn ∈

arg min
π∈Π

1
n

n

¯ℓ

(yi

i=1
X

(cid:0)

−

f (xi; ˆθn))(yi

−

T
f (xi; ˆθn))

, Q(xi; π)

,

(cid:1)

where ¯ℓ(V, ˆV ) :=
2
F is the squared Frobenius norm. In special settings (cf. our running Example 2),
k
the above problem for estimating π∗ can be transformed into a linear regression problem. An alternative is
to estimate f ∗ and Q∗ jointly using M-estimation:

ˆV

−

V

k

(ˆθn, ˆπn)

arg min
(θ,π)∈Θ×Π

∈

1
n

n

¯ℓ

(yi

i=1
X

(cid:0)

−

f (xi; θ))(yi

−

T
f (xi; θ))

, Q(xi; π)

(cid:1)

for some loss function ¯ℓ [27].

Finally, we emphasize that we only deal with the random design case (where the covariates X are
considered to be random) in this work. Much of the statistics literature presents results for the ﬁxed design
n
i=1 are deterministic and designed by the DM. These results
setting in which the covariate observations
readily carry over to the random design setting if Q∗
I, the errors ε are independent of X, and no
restriction is made on the design points

{
xi

xi

≡

}

n
i=1.

{

}

F.1 Parametric regression techniques for estimating f ∗
We verify that the parts of Assumptions 5, 11, and 15 relating to the estimate ˆfn of f ∗ hold for OLS
regression, the Lasso and generalized linear regression models under suitable assumptions. We also verify
their counterparts for the J-SAA and J+-SAA problems in Section D. Note that Assumption (7a) holds for
the regression techniques considered in this section whenever ˆθn
. Theorem 2.6
and Corollary 2.8 of Rigollet and H¨utter [64] present conditions under which these assumptions hold for
best subset selection regression in the homoscedastic setting, and Theorem 2.14 therein presents similar
guarantees for the Bayes Information Criterion estimator. Koltchinskii [53] veriﬁes these assumptions for
the Dantzig selector under certain conditions (including homoscedasticity of the error distribution). Hsu
et al. [45] veriﬁes these conditions for ridge regression. Negahban et al. [60] provides results for regularized
M-estimators in the high-dimensional setting.

kLq < +

θ∗ and

p
−→

∞

X

k

47

F.1.1 Ordinary least squares regression

We present suﬃcient conditions from White [85], Hsu et al. [45], and Rigollet and H¨utter [64] under which
the parts of Assumptions 5, 11, and 15 relating to the estimate ˆfn of f ∗ hold. Note that Theorems 2.31
and 4.25 of White [85] present a general set of suﬃcient conditions for ˆθn
θ∗) to be
asymptotically normally distributed. Chapters 3 to 5 of White [85] also present analyses that can handle
instrumental variables, which can be used to verify Assumptions 5, 15, and 11 when the errors ε are correlated
with the features X. We have the following result:

θ∗ and for √n(ˆθn −

p
−→

Proposition 21. Suppose f ∗(X) = θ∗X and we use OLS regression to estimate θ∗. Deﬁne ¯ε := Q∗(X)ε.

1. Suppose for some γ > 0, E

XX T

|
is positive deﬁnite. If

E
θ∗. Consequently, Assumptions (5a) and (5b) hold.

(xi, εi)
(cid:3)
}

(cid:2)

1+γ

Xi ¯εj|
{

,
∞

< +
∈
∞
i=1 is i.i.d., then ˆθn a.s. exists for n large enough and ˆθn
n

[dx] and j

< +

i
∀

X

∈

k

[dy], E

2(1+γ)

k
(cid:2)

(cid:3)

, and
a.s.
−−→

(cid:2)

(cid:3)

2. Suppose for some γ > 0, E

Xi ¯εj|
is positive deﬁnite, the covariance matrix of the random variable
(xi, εi)
}
3. Suppose

(cid:2)
n
i=1 is i.i.d. Then Assumptions (15a) and (15b) hold with α = 1.

[dx] and j

,
∞

< +

i
∀

P

2+γ

∈

∈

{

(cid:3)

(cid:2)

|

i=1 is i.i.d., ¯ε is uniformly sub-Gaussian with variance proxy σ2, i.e.,
n

X

2(1+γ)

[dy], E
XX T
< +
dy
j=1 X ¯εj is positive deﬁnite, and
(cid:3)

, E

∞

k

k

(cid:3)

(cid:2)

exp(suT ¯ε)

X = x

|

≤

exp(0.5s2σ2),

R,

s
∀

∈

u

k

k

= 1, a.e. x

,

∈ X

(cid:2)

(cid:3)

− 1
X X is sub-
the covariance matrix ΣX of the covariates is positive deﬁnite, and the random vector Σ
2
Gaussian. Then Assumptions (11a) and (11b) hold with constants Kf (κ, x) = O(exp(dx)), βf (κ, x) =
O

, ¯Kf (κ) = O(exp(dx)), and ¯βf (κ) = O

.

{

(xi, εi)
}
E

κ2
σ2dykxk2

(cid:16)

(cid:17)

Proof. Part 1 follows from Theorems 3.15 of White [85]. Part 2 follows from Theorem 5.13 of White [85].
Part 3 follows from Remark 12 of Hsu et al. [45]. For the homoscedastic case, part 3 also follows from
Theorem 2.2 and Remark 2.3 of Rigollet and H¨utter [64]. Although Rigollet and H¨utter [64] consider the
ﬁxed design case, their proof readily extends to the random design setting since no restrictions were placed
on the design.

{

(xi, εi)
}

For the homoscedastic setting, part 1 of Proposition 21 follows from Theorems 3.5 and 3.37 of White
[85] and part 2 follows from Theorem 5.3 of White [85]. Theorems 3.49 and 3.78 of White [85] present
suﬃcient conditions under which Assumptions (5a) and (5b) hold under mixing and martingale conditions
. Theorem 5.17 and Exercise 5.21 of White [85] present suﬃcient conditions under
on the data
which Assumptions (15a) and (15b) hold with α = 1 for ergodic and mixing data
, respectively.
Note that results in Bryc and Dembo [18] and [28] can be used to establish Assumptions (11a) and (11b)
for the non-i.i.d. setting. Note that once we have an estimate ˆQn of Q∗, we can re-estimate θ∗ using feasible
weighted least squares regression [66]. This yields an estimate ˆθn that is asymptotically more eﬃcient than
the OLS estimator of θ∗ whenever ˆQn is a consistent estimate of Q∗ [65, 66]. Even if the estimate ˆQn of Q∗ is
inconsistent, the weighted least squares estimator ˆθn remains consistent but may no longer be asymptotically
eﬃcient (see, e.g., Section 3.3 of [66]).

(xi, εi)
}

{

The above results can be used in conjunction with the techniques in Section F.2 to verify Assump-
tions 5J, 5J+, 15J, 15J+, 11J, and 11J+ for i.i.d. data
In the remainder of this section, we
specialize the veriﬁcation of these assumptions for OLS regression in the homoscedastic setting when prob-
lem (1) is a two-stage stochastic LP (see Example 1). We assume that dy = 1 for ease of exposition.

(xi, εi)
}

{

.

Following Lemma 14 in Section D, it suﬃces to establish rates and ﬁnite sample guarantees for the terms
n
when the assumptions for the ER-SAA problem hold.
i=1k
, hi := ( ¯X( ¯X T ¯X)−1 ¯X T)ii denote the ith leverage

and 1
1
n
n
T
Let ¯X denote the Rn×dx design matrix with ¯X[i] = (xi)

ˆf−i(xi)
k

ˆf−i(x)
k

n
i=1k

ˆfn(xi)

ˆfn(x)

−

−

P

P

48

κ2
σ2dy

(cid:16)

(cid:17)

score, and ei := yi
of Seber and Lee [70], we have

−

n xi denote the residual of the model ˆfn at the ith data point. From Section 10.6.3
ˆθT

1
n

n

i=1
X

ˆfn(xi)
k

−

ˆf−i(xi)

1
n

=

k

1
n

n

i=1
X

ˆfn(x)
k

−

ˆf−i(x)

k ≤

n

i=1
X

x
k
k
n

hi
1

ei
|
|
hi ≤
−

1
n

(cid:18)

n

1/2

(hi)2

i=1
X

(cid:19)

(cid:18)

1
n

n

ei

2

1/2

hi

1

−

(cid:19)

(cid:19)

i=1 (cid:18)
X

dx
√n

≤

1
n

(cid:18)

n

ei

2

1/2

hi

1

−

(cid:19)

(cid:19)

i=1 (cid:18)
X

,

n

( ¯X T ¯X)−1xiei

hi

1

−

i=1(cid:13)
X
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

1
n

x

≤ k

k

(cid:18)

x

≤ k

k

(cid:18)

n

( ¯X T ¯X)

−1xi

1/2

2

i=1(cid:13)
X
(cid:13)
(cid:13)
Tr(( ¯X T ¯X)

−1)

(cid:13)
(cid:13)
1/2
(cid:13)

(cid:19)

(cid:19)
1
n

(cid:18)

1
n

(cid:18)
n

n

i=1 (cid:18)
X
ei

1

ei

−
2

2

1/2

hi

(cid:19)
1/2

(cid:19)

,

hi

1

−

(cid:19)

(cid:19)

i=1 (cid:18)
X

is called the prediction residual
where Tr denotes the trace operator. The quantity
sum of squares statistic and is bounded in probability under mild assumptions. The above inequalities can
(cid:16)
be used to verify the assumptions for the jackknife-based estimators for Example 1.

P

(cid:16)

(cid:17)

(cid:17)

1
n

n
i=1

ei
1−hi

2

1/2

F.1.2 The Lasso and high-dimensional generalized linear models

Following van de Geer [77] and Bunea et al. [19], we consider generalized linear models with an ℓ1-penalty. We
m
k=1 θkψk(
assume dy = 1 for ease of exposition. The setup is: the model class
,
·
}
∈
(xi, εi)
is assumed
where
}
to be i.i.d., the number of basis functions m grows subexponentially with the number of data samples n, the
set Θ is convex, the loss function ℓ satisﬁes some Lipschitz assumptions [see Assumption L and Example 4
of 77], and the estimate ˆθn of θ∗ is obtained as

{
m
k=1 is a sequence of real-valued basis functions with domain

; θ) :=
, the data

f : f (
·

ψk(
·

)
}

), θ

{
P

:=

Θ

F

X

{

ˆθn ∈

arg min
θ∈Θ

1
n

for some penalty parameter λn = O

i=1
X

(cid:0)

log m
n

n

ℓ

yi, f (xi; θ)

+ λn

m

Xk=1  

1
n

n

i=1
X

1
2

k(xi)
ψ2

!

θk|

|

(cid:1)

that is chosen large enough. The above setup captures both

parametric and nonparametric regression models. Theorem 2.2 of van de Geer [77] and Theorems 2.1, 2.2,
and 2.3 of Bunea et al. [19] present conditions under which Assumptions 5, 11, and 15 hold for the above
setting.

(cid:18)q

(cid:19)

In the remainder of this section, we specialize the results of Bunea et al. [19] to the traditional Lasso
2. Once again, we deﬁne

setup [75]. In this setup, m = dx, ψk(x) = xk, Θ = Rdx, and ℓ(y, ˆy) =
¯ε := Q∗(X)ε.

ˆy
k

−

k

y

Proposition 22. Suppose f ∗(X) = θ∗X with
i.i.d., and the error ¯ε is uniformly sub-Gaussian with variance proxy σ2, i.e.,

θ∗
[j]k0 ≤

j
∀

[dy], the sequences

s,

∈

k

xi

{

}

n
i=1 and

¯εi

{

}

n
i=1 are

E

exp(suT ¯ε)

X = x

|

≤

exp(0.5s2σ2),

R,

s
∀

∈

u

k

k

= 1, a.e. x

.

∈ X

> 0,
[dx], and the
Additionally, suppose the support
matrix E
XX T
(0, 1]. If we use the Lasso
to estimate θ∗, then Assumptions (5a) and (5b) hold, Assumptions (15a) and (15b) hold with α = 1, and
Assumption (11a) and (11b) hold with ¯Kf (κ) = Kf (κ, x) = O(dx), ¯βf (κ) = O
, and βf (κ, x) =

(cid:3)
of the covariates X is compact, E
Xj|
X
) is positive semideﬁnite for some constant τ

(cid:2)
τ diag(E

XX T

j
∀

|
(cid:2)

−

(cid:3)
∈

∈

(cid:3)

(cid:2)

(cid:3)

(cid:2)

2

κ2
σ2sdy

O

κ2
σ2sdykxk2

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Proof. Follows from Theorem 2.1 and Corollary 1 of Bunea et al. [19].

49

{

(xi, εi)
}

Chatterjee [21] establishes consistency of the Lasso in the homoscedastic setting under the following
is i.i.d., the error ε is sub-Gaussian with variance proxy σ2 and is
weaker assumptions: the data
independent of the covariates X, the support
of the covariates is compact, and the covariance matrix of
the covariates is positive deﬁnite. Theorems 1 and 2 therein present conditions under which Assumption 15
holds at a slower rate with α = 0.5. Theorem 2.15 of Rigollet and H¨utter [64] can then be used to show
that Assumptions (11a) and (11b) hold in the homoscedastic setting with ¯Kf (κ) = Kf (κ, x) = O(dx),
¯βf (κ) = O
. Basu and Michailidis [7] present conditions under
which Assumptions 5, 11, and 15 can be veriﬁed for time series data
in the homoscedastic case.
(cid:17)
The above results can be used in conjunction with the discussion in Section F.2 to derive rates of convergence
and ﬁnite sample guarantees for the jackknife-based estimators for i.i.d. data

, and βf (κ, x) = O

κ4
σ2s2d2
ykxk2

κ4
σ2s2d2
y

X

(cid:16)

(cid:16)

(cid:17)

{

.

(xi, εi)
}
(xi, εi)
}

{

Theorem 1 of Belloni et al. [10] outlines conditions under which Assumptions 5 and 15 hold for the
heteroscedasticity-adapted Lasso with α = 1. Medeiros and Mendes [59] and Ziel [87] present asymptotic
analyses of the adaptive Lasso for time series data, including GARCH-type processes. Theorems 2 and 3
of [59] and Theorem 1 of [87] present conditions under which Assumptions 5 and 15 hold with α = 1. Belloni
et al. [11] study asymptotic and ﬁnite sample guarantees for the heteroscedasticity-adapted square-root
Lasso. Finally, Theorem 5.2 of Dalalyan et al. [25] introduces a scaled heteroscedastic Dantzig selector and
presents large deviation bounds for both ˆfn and ˆQn under certain sparsity assumptions.

F.2 General M-estimation procedures for estimating f ∗

We use results from Chapter 5 of van der Vaart [78], Chapter 3 of van der Vaart and Wellner [79], and Shapiro
et al. [73] to verify the parts of Assumptions 5, 11, and 15 relating to the estimate ˆfn of f ∗ for general M-
) is Lipschitz continuous at θ∗ for
estimators. To begin, we suppose that the regression function f (x;
·
ˆθnk
θ∗
f (x; θ∗)
a.e. x
Lf (x)
. To
−
k
establish Assumptions (5a), (5b), (15a), and (15b), it suﬃces to assume that the function f (x,
) is locally
·
Lipschitz continuous at θ∗ and a.s. for n large enough, the estimates ˆθn of θ∗ lie in some compact subset of
Θ. Note that

with Lipschitz constant Lf (x), i.e., we a.s. have

f (x; ˆθn)

∈ X

k ≤

−

k

1
n

n

i=1
X

f (xi; θ∗)

k

f (xi; ˆθn)
k

2

−

1
n

≤

(cid:18)

L2

f (xi)

k
(cid:19)

θ∗

2,

ˆθnk

−

n

i=1
X

with the ﬁrst term in the r.h.s. of the above inequality bounded in probability under a suitable weak LLN
assumption. Therefore, our main focus is presenting rates at which
0. Note that Assumption (7a)
also holds whenever ˆθn

θ∗ and

Lf (X)

ˆθnk

p
−→

θ∗

−

k

p
−→

k

kLq < +

.
∞

Verifying Assumption 5. Theorem 5.7 of van der Vaart [78] presents conditions under which ˆθn
(xi, εi)
for i.i.d. data
}
this result also holds when
[78] also presents alternative conditions for ˆθn

θ∗
[cf. Theorems 5.3 and 5.4 of 73]. Similar to the discussion following Assumption 2,
Dn satisﬁes certain mixing/stationarity assumptions. Section 5.2 of van der Vaart

θ∗.

{

p
−→

p
−→

ˆθn −

θ∗

p
−→

Verifying Assumption 15. We discuss conditions under which
0 at certain rates. The-
orem 5.23 of van der Vaart [78] presents regularity conditions under which this convergence holds at the
conventional n−1/2 rate, in which case Assumption 15 holds with α = 1 [cf. Theorem 5.8 of 73]. Once
again, the above conclusion holds when the observations
Dn satisfy certain mixing/stationarity assumptions.
Chapter 5 of van der Vaart [78] and Chapter 3.2 of van der Vaart and Wellner [79] provide some examples
of M-estimators that possess this rate of convergence. Theorem 5.52 and Chapter 25 of van der Vaart [78]
present conditions under which Assumption 15 holds with constant α < 1 (including for semiparametric
regression). Similar to the special case of OLS regression, vanilla M-estimators that do not account for het-
eroscedasticity may no longer be eﬃcient. Feasible weighted M-estimation may provide an asymptotically
eﬃcient alternative in the heteroscedastic setting.

k

k

50

Verifying Assumption 11. We verify this assumption by establishing ﬁnite sample guarantees for ˆθn
when the M-estimation problem satisﬁes uniform exponential bounds similar to Assumption 8. Speciﬁcally,
suppose for any constant κ > 0, there exist positive constants ˆK(κ) and ˆβ(κ) such that

1
n

n

ℓ

yi, f (xi; θ)

i=1
X

(cid:0)

(cid:1)

−

P

(cid:26)

sup
θ∈Θ (cid:12)
(cid:12)
(cid:12)
(cid:12)

E(Y,X)[ℓ (Y, f (X; θ))]

> κ

≤

(cid:27)

ˆK(κ) exp

−
(cid:0)

n ˆβ(κ)

,

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

see the discussion surrounding Assumption 8 for conditions under which such a uniform exponential bound
holds [the main restriction there is that Θ is compact, but this can be relaxed by assuming that the estimates
ˆθn lie in a compact subset of Θ, see the discussion following Theorem 5.3 of 73]. Theorem 2.3 of Homem-de-
Mello [42] then implies that Assumption 11 holds whenever the sample average term 1
f (xi) satisﬁes
n
a large deviation property (i.e., it is concentrated around E
). We note that results in Bryc and
P
Dembo [18], Dembo and Zeitouni [28] can be used to establish such uniform exponential bounds for mixing
(cid:2)
by adapting Lemma 2.4 of Homem-de-Mello [42]. Theorems 1, 3, and 5 of Sun et al. [74] and
data
Theorem 2.1 of Zhou et al. [86] present large deviation results in the form of Assumptions (11a) and (11b)
for adaptive Huber regression when f ∗ is linear.

(xi, εi)
}

n
i=1 L2

f (X)

L2

{

(cid:3)

(xi, εi)
}

{
n

Verifying the assumptions for the jackknife-based methods. We now present techniques for veri-
fying the parts of Assumptions 5J, 5J+, 15J, 15J+, 11J, and 11J+ relating to the estimate ˆfn of f ∗ when
the data

is i.i.d. Noting from Markov’s inequality that

ˆfn−1(X)

−

4
k

,

i

(X)

f ∗
k
h
ˆfn−1(x)

,

1
n

1
n

P

P

(cid:26)

(cid:26)

i=1
X
n

i=1
X

(xi)

f ∗
k

−

ˆf−i(xi)

4 > κ4
k

≤

(cid:27)

f ∗
k

(x)

ˆf−i(x)

> κ

1
nκ

1
nκ4

n

n

i=1
X
EDn

EDn

f ∗
k
h

(xi)

−

ˆf−i(xi)

4
k

(x)

ˆf−i(x)

=

f ∗
k
h

=

1
κ4

EDn−1,X

i
EDn−1

1
κ

(x)

f ∗
k
h

k

(cid:27)

≤

−

−

−

i=1
X

∈ X

f ∗(X)

k
i
Dn is i.i.d., we have that Assumptions 15J and 15J+ on the jackknife-based methods hold if,
ˆfn−1(X)
, the expectations EDn−1,X
converge to
k
k
) at θ∗
h
, it suﬃces to establish rates of convergence for the expectation

when the data
for a.e. x
−
zero at suitable rates. Under the aforementioned Lipshitz continuity assumption on the function f (x;
and the assumption that E
h
term EDn−1
. These results can be readily obtained under assumptions on the curvature of the
loss function of the M-estimation problem (e.g., restricted strong convexity) around the true parameter θ∗,
see Negahban et al. [60] for instance. Chapter 14 of Biau and Devroye [16] provides similar rate results for
kNN regression. Alternatively, we can also bound the terms appearing in the assumptions for the jackknife-
based formulations as

k
h
< +

ˆfn−1(x)
k

f (X)
i

and EDn−1

ˆθn−1k

f ∗(x)

k
i

L4

∞

θ∗

−

−

k

h

i

i

i

4

4

·

1
n

1
n

n

k

k

i=1
X
n

i=1
X

f (x; θ∗)

−

f (x; ˆθ−i)

k ≤

1
n

f (xi; θ∗)

f (xi; ˆθ−i)
k

4

−

≤

n

i=1
X
1
n

i=1
X

Lf (x)
k

θ∗

ˆθ−ik

,

−

n

L4

f (xi)
k

θ∗

4

ˆθ−ik

−

1
n

n

i=1
X

≤

(cid:18)

1/2

L8

f (xi)

(cid:19)

(cid:18)

1
n

n

i=1
X

θ∗

k

8

ˆθ−ik

−

1/2

,

(cid:19)

with the ﬁrst term in the r.h.s. of the last inequality bounded under appropriate LLN assumptions. Therefore,
an alternative is to establish rates and ﬁnite sample guarantees for the two terms 1
8 and
n
1
n

f (xi). A third direct approach is to use the weaker bounds

n
i=1 L8

ˆθ−ik

n
i=1k

θ∗

−

P

P
P

(cid:26)

(cid:26)

P

1
n

1
n

n

i=1
X
n

i=1
X

(xi)

f ∗
k

−

ˆf−i(xi)

4 > κ4
k

f ∗
k

(x)

−

ˆf−i(x)

k

(cid:27)

≤

n

> κ

(cid:27)

≤

i=1
X

n

P

i=1
X
P

n
f ∗
k

n

f ∗
k

(x)

(xi)

ˆf−i(xi)

4 > κ4
k

−

= nP

f ∗
k

n

o

(X)

−

ˆfn−1(X)

4 > κ4
k

,

o

ˆf−i(x)

k

−

> κ

= nP

o

(x)

−

ˆfn−1(x)

k

f ∗
k

n

> κ

.

o

51

Finally, note that it is suﬃcient to establish rates and ﬁnite sample guarantees for 1
n
and 1
n

when Assumptions 5, 15, and 11 hold.

ˆfn(x)

n
i=1k

ˆf−i(x)
k

−

ˆfn(xi)

n
i=1k

ˆf−i(xi)
k

4

−

P

P

F.3 Nonparametric regression techniques for estimating f ∗

We verify that Assumptions (5a), (5b), (15a), and (15b) hold for kNN regression, CART, and RF regression,
and state a large deviation result similar to Assumptions (11a) and (11b) for kNN regression. The discussion
in Section F.2 then provides an avenue for verifying the corresponding assumptions for the J-SAA and J+-
(xi, εi)
. Theorem 14.5 in Biau and Devroye [16] presents conditions under
SAA problems for i.i.d. data
}
which Assumption (7a) holds for q
2. Note that results in Walk [84], Gy¨orﬁ et al. [41], and Chen and Shah
[22] can be used to verify some of these assumptions for kernel regression and semi-recursive Devroye-Wagner
estimates for mixing data
Dn, results in Raskutti et al. [63] can be used to verify these assumptions for sparse
additive nonparametric regression, Chapter 13 of Wainwright [83] can be used to verify these assumptions
for (regularized) nonparametric least squares regression, and results in Seijo and Sen [71] and Mazumder
et al. [58] can be used to verify these assumptions for convex regression. In what follows, we only consider
the setting where the data

is i.i.d.

≤

{

We assume that the kNN regression estimate is computed as follows: given parameter k
n
i=1 is a reordering of the data
x(k)(x)

deﬁne ˆfn(x) := 1
k
x(j)(x)
x(j)(x)
x
that
−
assume that (y(j)(x), x(j)(x)) appears ﬁrst in the reordering).

k
i=1 y(i)(x), where
x(k)(x)

(y(i)(x), x(i)(x))
}

{
whenever j

P
k ≤ k

k (if

x
k

x
k

x
k

[n] and x

,
∈
∈ X
(yi, xi)
n
i=1 such
}
{
for some j < k, then we

−

−

−

≤

=

k

k

k

(xi, εi)
}

{

of the covariates is compact. Deﬁne
for
[dy] and some λ > 0. Consider the setting where we use kNN regression to estimate the regression

(xi, εi)
Proposition 23. Suppose the data
}
¯ε := Q∗(X)ε, and suppose the distribution of the errors ¯ε satisﬁes supx∈Rdx E [exp(λ
|
each j
function f ∗ with the parameter ‘k’ satisfying lim
n→∞

is i.i.d. and the support

and k = o(n).

X = x] < +

k
log(n) =

¯εj |

∞

X

∈

{

)

|

∞

1. Suppose the function f ∗ is continuous on

. Then Assumptions (5a) and (5b) hold.

X

2. Suppose the function f ∗ is twice continuously diﬀerentiable on

and the random vector X has a
density that is twice continuously diﬀerentiable. Then, there exists a choice of the parameter ‘k’ such
that Assumptions (15a) and (15b) hold with α = O(1)
dx

X

.

3. Suppose the function f ∗ is Lipschitz continuous on

distribution PX of the covariates satisﬁes P
size n satisfying n

X
{
and nγ

O(1)k

dx

O(1)
κ

X
∈ Bκ(x)

log(n) ≥

≥

} ≥

O(1)dxdyσ2
κ2

∀
, we have

∈ X

and there exists a constant τ > 0 such that the
and κ > 0. Then, for sample

τ κdx,

x

P

(cid:26)

f ∗

sup
x∈X k

(x)

−

ˆfn(x)

k

> κ

dy

p

(cid:16)

≤

(cid:18)

(cid:27)

(cid:17)
O(1)√dx
κ

dx

exp

(cid:19)

−

(cid:16)

O(1)n(O(1)κ)2dx

+ O(1)n2dx

(cid:17)

dx

exp

O(1)
dx (cid:19)

(cid:18)

kκ2
O(1)σ2

.

(cid:19)

−

(cid:18)

Proof. The ﬁrst part follows from Theorem 12.1 of Biau and Devroye [16]. The second part follows from
Theorems 14.3 and 14.5 of Biau and Devroye [16] and Markov’s inequality. The last part follows from
Lemma 10 of Bertsimas and McCord [13].

Jiang [47] presents improved rates of convergence and ﬁnite sample guarantees for kNN regression in
the homoscedastic setting when the data
Dn lies on a low-dimensional manifold. Lemma 7 of Bertsimas
and McCord [13] presents conditions under which CART regression satisﬁes Assumption 5. Along with
Theorem 8 of Wager and Athey [82], the above result can be used to show that Assumption 15 holds for
CART regression with α = O(1)
. Lemma 9 of Bertsimas and McCord [13] presents conditions under which
dx
RF regression satisﬁes Assumption 5. Once again, we can use this result along with Theorem 8 of Wager
and Athey [82] to show that Assumption 15 holds for RF regression with α = O(1)
dx

.

52

F.4 Verifying assumptions on the estimation of Q∗
We verify that the parts of Assumptions 5, 11, and 15 relating to the estimate ˆQn of Q∗ hold for some setups
with structured heteroscedasticity. The techniques in Section F.2 may then be used to verify the assumptions
is i.i.d. These assumptions for ˆQn—in
for the jackknife-based estimators in Section D when the data
particular, Assumption 11—are not as well-studied in the literature as those for ˆfn and are typically harder
to verify. Because deriving theoretical properties of estimators in the heteroscedastic setting and deriving
ﬁnite sample properties of estimators in general are areas of topical interest, we envision that future research
will enable easier veriﬁcation of these assumptions. For simplicity, we only consider function classes
that
comprise diagonal covariance matrices [86], i.e.,

(xi, εi)
}

Q

{

:=

{

Q

Q : Rdx

Rdy×dy : Q(X) = diag(q1(X), q2(X), . . . , qdy (X))
}

,

→

although the ER-SAA approach is more generally applicable. Bauwens et al. [8] review some model classes
with non-diagonal covariance matrices that are popular in time series modeling.

Q

F.4.1 Parametric models for heteroscedasticity

We assume that Q∗(X)
estimate π∗. Forms of the functions qj of interest include [62, 66]:

Q(X; π∗) for some ﬁnite-dimensional parameter π∗

≡

Π and the goal is to

∈

i. (qj(X; π))2 = ((πj )

TX)2,

ii. (qj(X; π))2 = exp((πj )

TX),

iii. (qj(X; π))2 = exp

T
(πj )

X

log(
|

|

)

, where X1 ≡
(cid:1)

2 (instead of X1 ≡

1).

(cid:0)

The above setup can also accommodate cases where the parameters of the function Q∗ include some of the
parameters of the function f ∗.

, the function Q(x;

Let ˆπn denote the estimate of π∗ corresponding to the regression estimate ˆQn. Suppose for a.e. re-
) is Lipschitz continuous with Lipschitz constant LQ(x) and its in-
alization x
∈ X
)]−1 is also Lipschitz continuous with Lipschitz constant ¯LQ(x). These assumptions hold for
verse [Q(x;
·
the above parametric models if the parameters π therein are restricted to lie in a compact sets (similar
to Section F.2, it suﬃces to assume that the above Lipschitz continuity assumptions hold locally for the
asymptotic results). Because

·

ˆQn(x)

k

−

Q∗(x)

k ≤

LQ(x)
k

ˆπn −

π∗

,

k

1
n

n

i=1
X

ˆQn(xi)

−1

−

Q∗(xi)

−1

2

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

1
n

n

i=1
X

≤

(cid:16)

¯L2

Q(xi)
(cid:17)

ˆπn −

k

π∗

2,

k

asymptotic and ﬁnite sample guarantees on the estimator ˆπn of π∗ directly translate to the asymptotic and
ﬁnite sample guarantees on the estimate ˆQn in Assumptions (5c), (5d), (15c), (15d), (11c), and (11d). When
the functions f ∗ and Q∗ are jointly estimated using an M-estimation procedure, the results in Section F.2
provide conditions under which the estimator ˆπn of π∗ is consistent and Assumptions (5c), (5d), (15c),
and (15d) hold with α = 1. Section F.2 also presents a hard-to-verify uniform exponential bound condition
under which ˆπn possesses a ﬁnite sample guarantee. Carroll and Ruppert [20] consider robust M-estimators
for π∗ that possess a similar rate of convergence when f ∗ is linear. Dalalyan et al. [25] present asymptotic and
ﬁnite sample guarantees for a scaled Dantzig estimator of π∗ under some sparsity assumptions. Finally, Fan
et al. [38] present a quasi-maximum likelihood approach for estimating the parameters of GARCH models
and investigate their asymptotic properties.

In the remainder of this section, we specialize the veriﬁcation of Assumptions (5c), (5d), (15c), and (15d),
to Example 2. We are unable to verify Assumptions (11c) and (11d) because the literature lacks suitable
ﬁnite-sample guarantees for heteroscedasticity estimation.

53

Verifying Assumptions 5 and 7. We verify Assumptions (5c), (5d), and (7c) for Example 2. Since

ˆQn(x)

k

−

Q∗(x)

k≤

max
j∈[dy]|

ˆqjn(x)

q∗
j (x)
|

−

dx

1/2

dx

= max

exp

j∈[dy] (cid:12)
(cid:18)
(cid:12)
(cid:12)
(cid:12)

ˆπj
kn log(
|

xk|

)
(cid:19)(cid:19)

−

exp

(cid:18)

(cid:18)

Xk=1

πj∗
k log(
|

xk|

)
(cid:19)(cid:19)

(cid:18)

Xk=1

1/2

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

we have that Assumption (5c) holds whenever ˆπn
mapping theorem. Appendix B.2 of Romano and Wolf [66] identiﬁes conditions under which ˆπn
Example 2. To verify Assumption (5d), assume for simplicity that the support
away from the origin and the estimates

π∗ on account of Slutsky’s lemma and the continuous
π∗ for
is compact and bounded

lie in a compact set a.s. for n large enough. Since

p
−→

X

p
−→

1
n

n

i=1
X

(cid:2)

(cid:13)
(cid:13)

ˆQn(xi)

−1

(cid:3)

−

(cid:2)

Q∗

(xi)

−1

2

(cid:3)

(cid:13)
(cid:13)

≤

≤

=

≤

≤

ˆπn}
{
1
n

n

i=1
X
n

1
n

1
n

1
n

i=1
X
n

i=1
X
n

i=1
X

(ˆqjn(xi))

max
j∈[dy]|

(ˆqjn(xi))

max
j∈[dy]|

−

−

−1

−2

dx

(q∗

j (xi))

−1

(q∗

j (xi))

−2

2
|

|

−

max
j∈[dy]

exp

dx

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
O(1) max
j∈[dy] (cid:12)
(cid:12)
(cid:12)
dx
(cid:12)
πj∗
k −
|
Xk=1

Xk=1

Xk=1
(πj∗

k −

ˆπj
,
kn|

O(1) max
j∈[dy ]

ˆπj
kn) log(

xi
)
k|
|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆπj
kn log(

xi
)
k|
|

−

(cid:19)

exp

(cid:18)

dx

−

Xk=1

πj∗
k log(

xi
)
k|
|

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last two steps follow by the mean-value theorem, the assumption that
away from the origin, and the assumption that the sequence
enough (note that the compactness assumption on
nonnegative orthant and
∈ X
The above arguments also imply supx∈X k
Assumption (7c) holds since

is compact and bounded
lies in a compact set a.s. for n large
lies in the
p
π∗.
−→
0. These conditions also guarantee that

). Consequently, Assumption (5d) also holds whenever ˆπn
[Q∗(x)]−1

can be relaxed, e.g., if the sequence

[ ˆQn(x)]−1

ˆπn}

ˆπn}

p
−→

| ≥

1,

−

X

X

¯x

¯x

∀

{

{

k

|

ˆQn(X)
k

−

Q∗

(X)

Lq

k

≤

dy

E

(cid:18)

(cid:20)

j=1
X
dy

ˆqjn(X)
|

−

q∗
j (X)

q
|

1/q

(cid:21)(cid:19)

E

(cid:18)

(cid:20)
dy

exp

(cid:18)

j=1 (cid:12)
X
(cid:12)
(cid:12)
(cid:12)
O(1)

E

dx

(cid:18)
Xk=1
dx

ˆπj
kn log(

Xk
|

)
|

1/2

dx

(cid:19)(cid:19)

−

(cid:18)

exp

(cid:18)

Xk=1

πj∗
k log(

Xk
|

)
|

1/2

q

1/q

(cid:19)(cid:19)

(cid:21)(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ˆπj

kn −

πj∗
k ) log(

Xk
|

)
|

q

1/q

(cid:21)(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

≤

(cid:20)

(cid:18)

j=1
X
dy

O(1)

Xk=1

(cid:12)
(cid:12)
(cid:12)
dx
(cid:12)

ˆπj
kn −
|
(cid:12)
Xk=1
(cid:12)
(cid:12)
dx
(cid:12)

ˆπj
kn −
|
Xk=1

πj∗
k |
(cid:12)
(cid:12)
(cid:12)
(cid:12)

πj∗
,
k |

(cid:18)
j=1
X
dy

O(1)

j=1
X

q

1/q

(cid:19)

where the third and fourth steps again follow by the mean-value theorem, the compactness of
that it is bounded away from the origin, and the a.s. compactness of the sequence
The above arguments also imply supx∈X k

ˆQn(x)
k

Q∗(x)

ˆπn}

p
−→

0.

−

{

and the fact
for n large enough.

X

54

Verifying Assumption 15. We show that Assumptions (15c) and (15d) hold whenever
Op(n−α/2), the support
is compact and bounded away from the origin, and the estimates
compact set a.s. for n large enough. Note that

X

ˆπn −
k
ˆπn}
{

π∗
=
k
lie in a

Q∗(x)

sup
x∈X k

−

ˆQn(x)

k≤

max
j∈[dy]

sup
x∈X |

ˆqjn(x)

q∗
j (x)
|

−

dx

1/2

dx

1/2

exp

ˆπj
kn log(
|

xk|

)
(cid:19)(cid:19)

−

exp

(cid:18)

(cid:18)

Xk=1

πj∗
k log(
|

xk|

)
(cid:19)(cid:19)

(cid:18)

Xk=1

= max
j∈[dy]

max
j∈[dy]

≤

sup
x∈X (cid:12)
(cid:18)
(cid:12)
(cid:12)
dx
(cid:12)
O(1)

Xk=1

πj∗
k −

|

ˆπj
kn|

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last step follows by arguments similar to the derivation of Assumption (7c) using the mean-value
and the fact that it is bounded away from the origin, and the a.s. compactness
theorem, the compactness of
= Op(n−α/2).
of the sequence
Similarly, we have
[Q∗

for n large enough. Therefore, Assumption (15c) holds whenever

ˆπn−

ˆπn}

[ ˆQn(x)]

(ˆqjn(x))

(q∗

(x)]

π∗

X

−1

−1

−1

−1

{

k

k

j (x))

sup
x∈Xk

−

max
j∈[dy]

sup
x∈X|

k≤

−

|

max
j∈[dy]

≤

max
j∈[dy]

≤

sup
x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
O(1)

dx

ˆπj
kn log(

exp

−

(cid:18)

(cid:18)
dx

πj∗
k −
|
Xk=1

Xk=1
ˆπj
,
kn|

1/2

dx

xk
|

)
|

(cid:19)(cid:19)

−

exp

(cid:18)

−

(cid:18)

Xk=1

πj∗
k log(

xk
|

)
|

1/2

(cid:19)(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

by arguments similar to the derivation of Assumption (5d). Therefore, Assumption (15d) holds whenever
π∗
ˆπn −
k
for Example 2 at a certain rate.

= Op(n−α/2). Appendix B.2 of Romano and Wolf [66] identiﬁes conditions under which ˆπn

p
−→

π∗

k

F.4.2 Nonparametric models for heteroscedasticity

We assume that each function qj : Rdx
some popular models for the functions qj in a time series context.

→

R+ is ‘suﬃciently smooth’. Chapter 8 of Fan and Yao [37] presents

Suppose the function Q∗ and its regression estimate ˆQn are (asymptotically) a.s. uniformly invertible,
−1

−1

i.e., sup¯x∈X

Q∗(¯x)

< +

∞

and sup¯x∈X

ˆQn(¯x)

< +

. We have

∞

1
n

n

(cid:13)
(cid:2)
(cid:13)
ˆQn(xi)

(cid:3)
−1

(cid:13)
Q∗
(cid:13)

(xi)

−1

2

1
n

≤

−

n

(cid:13)
(cid:2)
Q∗
(cid:13)

(xi)

(cid:3)
−1

(cid:13)
2
(cid:13)

ˆQn(xi)

−1

2

ˆQn(xi)
k

−

Q∗

(xi)

2
k

i=1
X

(cid:2)

(cid:13)
(cid:13)

(cid:3)

(cid:2)

(cid:3)

(cid:13)
(cid:13)

i=1
X

≤

sup
¯x∈X

(cid:18)

(cid:13)
(cid:2)
(cid:13)
Q∗(¯x)

(cid:3)
−1

(cid:13)
(cid:13)
2

(cid:2)

(cid:13)
(cid:13)

(cid:13)
(cid:3)
(cid:13)
ˆQn(¯x)

−1

2

sup
¯x∈X

(cid:19)(cid:18)

(cid:13)
(cid:2)
(cid:13)
ˆQn(x)

(cid:13)
(cid:3)
(cid:13)
Q∗(x)
k

(cid:13)
(cid:2)
(cid:13)

(cid:3)

(cid:13)
(cid:13)

1
n

n

i=1
X

(cid:19)(cid:18)

ˆQn(xi)
k

−

Q∗(xi)

2
k

(cid:19)

n
i=1k

ˆQn(xi)

k

−

and 1
n

Q∗(xi)
Therefore, asymptotic and ﬁnite sample guarantees for
k
are suﬃcient for verifying Assumptions (5c), (5d), (15c), (15d), (11c), and (11d). Theorem 8.5 of Fan and
Yao [37] can be used to identify conditions under which these asymptotic guarantees hold for local linear
estimators on time series data when the dimension of the covariates dx = 1. They also mention approaches
for estimating Q∗ when dx > 1. Theorem 2 of Ruppert et al. [68] can be used to verify Assumptions 5
and 15 for local polynomial smoothers. Proposition 2.1 and Theorem 3.1 of Jin et al. [48] identify conditions
under which Assumptions 5 and 15 hold for a local likelihood estimator. Van Keilegom and Wang [80]
consider semiparametric models for both f ∗ and Q∗. Theorems 3.1 and 3.2 therein can be used to verify
Assumptions 5 and 15 for the estimates ˆQn. Section 3 of Zhou et al. [86] presents robust estimators of Q∗
when f ∗ is linear and notes that these estimators ˆQn possess asymptotic and ﬁnite sample guarantees in
the form of Assumptions 5, 15, and 11. Finally, Theorem 3.1 of Chesneau et al. [23] can be used to derive
asymptotic guarantees for wavelet estimators of Q∗.

P

−

2

55

G Omitted details for the computational experiments

The parameters ϕ∗ and ζ∗ in the true demand model are speciﬁed as:

ϕ∗

j = 50 + 5δj0,

ζ∗
j1 = 10 + δj1,

ζ∗
j2 = 5 + δj2,

and ζ∗

j3 = 2 + δj3,

j
∀

,

∈ J
δj1}j∈J ,

{

δj0}j∈J are i.i.d. samples from the standard normal distribution

{
δj3}j∈J are i.i.d. samples from the uniform distribution U (
−

δj2}j∈J ,
where
{
4, 4). We generate i.i.d. samples of the
and
covariates X from a multivariate folded/half-normal distribution. We specify the underlying normal distri-
bution to have mean µX = 0 and set its covariance matrix ΣX to be a random correlation matrix that is
generated using the ‘vine method’ of Lewandowski et al. [55] (each partial correlation is sampled from the
Beta(2, 2) distribution and rescaled to [
1, 1]). Finally, Algorithm 1 describes our procedure for estimating
the normalized 99% UCB on the optimality gap of our data-driven solutions using the multiple replication
procedure [57].

(0, 1), and

N

−

{

Algorithm 1 Estimating the normalized 99% UCB on the optimality gap of a given solution.
1: Input: Covariate realization X = x and data-driven solution ˆzn(x) for a particular realization of the

data

Dn.
2: Output: ˆB99(x), which is a normalized estimate of the 99% UCB on the optimality gap of ˆzn(x).
3: for k = 1, . . . , 30 do
4:

Draw 1000 i.i.d. samples ¯
D
Estimate the optimal value v∗(x) by solving the full-information SAA problem (2) using
the data ¯
D

1000
i=1 of ε according to the distribution Pε.

k :=

¯εk,i

1000

k:

{

}

¯vk(x) := min
z∈Z

1
1000

i=1
X

c(z, f ∗(x) + Q∗(x)¯εk,i).

Estimate the out-of-sample cost of the solution ˆzn(x) using the data ¯
D

k:

5:

6:

ˆvk(x) :=

1
1000

1000

i=1
X

c(ˆzn(x), f ∗(x) + Q∗(x)¯εk,i).

Estimate the optimality gap of the solution ˆzn(x) as ˆGk(x) = ˆvk(x)

7:
8: end for
9: Construct the normalized estimate of the 99% UCB on the optimality gap of ˆzn(x) as

¯vk(x).

−

ˆB99(x) :=

100
¯v(x)
|

|

1
30

30

Xk=1





ˆGk(x) + 2.462

var(
{

)

ˆGk(x)
}
30

s

,





¯vk(x) and var(
{

ˆGk(x)
}

) denotes the variance of the gaps

ˆGk(x)
}

{

30
k=1.

where ¯v(x) :=

1
30

30

Xk=1

56

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

p
a
g
y
t
i
l
a
m

i
t
p
o
%
n
o
B
C
U

30

25

20

15

10

10

σ = 5

8

6

4

2

0

R k

O P

R k

O P

R k

O P

10

8

6

4

2

0

σ = 10

R k

O P

R k

O P

R k

O P

10

8

6

4

2

0

σ = 20

R k

O P

R k

O P

R k

O P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

30

25

20

15

10

5

0

R k

O P

R k

O P

R k

O P

30

25

20

15

10

5

0

R k

O P

R k

O P

R k

O P

5

0

R k

O P

R k

O P

R k

O P

n =

55

220

1100

n =

55

220

1100

n =

55

220

1100

Figure 7: Eﬀect of increasing σ on re-weighted kNN-SAA (R), ER-SAA+kNN (k), ER-SAA+OLS (O), and
PP+OLS (P) approaches when ω = 1. Top row: p = 0.5. Bottom row: p = 2. Left column: σ = 5. Middle
column: σ = 10. Right column: σ = 20.

57

 
 
 
 
 
 
 
 
