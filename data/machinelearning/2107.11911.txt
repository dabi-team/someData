Submitted to Operations Research
manuscript (Please, provide the manuscript number!)

Authors are encouraged to submit new papers to INFORMS journals by means of
a style ﬁle template, which includes the journal title. However, use of a template
does not certify that the paper has been accepted for publication in the named jour-
nal. INFORMS journal templates are for the exclusive purpose of submitting to an
INFORMS journal and should not be used to distribute the papers in print or online
or to submit the papers to another publication.

Restless Bandits with Many Arms:
Beating the Central Limit Theorem

Xiangyu Zhang
Department of Operations Research and Information Engineering, Cornell University, Ithaca, NY 14850, xz556@cornell.edu

Peter I. Frazier
Department of Operations Research and Information Engineering, Cornell University, Ithaca, NY 14850, pf98@cornell.edu

We consider ﬁnite-horizon restless bandits with multiple pulls per period, which play an important role in

recommender systems, active learning, revenue management, and many other areas. While an optimal policy

can be computed, in principle, using dynamic programming, the computation required scales exponentially

in the number of arms N . Thus, there is substantial value in understanding the performance of index policies

and other policies that can be computed eﬃciently for large N . We study the growth of the optimality

gap, i.e., the loss in expected performance compared to an optimal policy, for such policies in a classical

asymptotic regime proposed by Whittle in which N grows while holding constant the fraction of arms that

can be pulled per period. Intuition from the Central Limit Theorem and the tightest previous theoretical

bounds suggest that this optimality gap should grow like O(

√

N ). Surprisingly, we show that it is possible

to outperform this bound. We characterize a non-degeneracy condition and a wide class of novel practically-

computable policies, called ﬂuid-priority policies, in which the optimality gap is O(1). These include most

widely-used index policies. When this non-degeneracy condition does not hold, we show that ﬂuid-priority

policies nevertheless have an optimality gap that is O(

√

N ), signiﬁcantly generalizing the class of policies

for which convergence rates are known. We demonstrate that ﬂuid-priority policies oﬀer state-of-the-art

performance on a collection of restless bandit problems in numerical experiments.

Key words : restless bandit, Markov decision process, index policies

1
2
0
2

l
u
J

5
2

]

C
O
.
h
t
a
m

[

1
v
1
1
9
1
1
.
7
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
2

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

1. Introduction

We study a stochastic control problem called the ﬁnite-horizon restless bandit. In this problem,

a decision maker controls N Markov processes (colloquially called “arms”) with known transition

kernels and state-dependent rewards. The arms produce rewards and evolve independently but

are coupled through a constraint (the “budget” constraint) on the number that can be activated

(colloquially, “pulled”) in each period. Subject to this constraint, the decision-maker seeks to

maximize the expected total reward.

This problem arises in various ﬁelds. For example, when pursing an active learning approach to

classifying images with crowd workers (Chen et al. 2013), each image is an arm, asking a worker

to label that image corresponds to pulling that arm, and the arm’s state is the resulting Bayesian

posterior distribution on the corresponding image’s class given past noisy labels. A limited supply

of crowd workers imposes constraints on the number of arms that can be pulled per period. In

dynamic assortment optimization (Brown and Smith 2020), a sales manager selects products to

display for sale subject to limited display space. Each product generates revenue at an unknown

rate, which can be learned from the revenue it generates when it is displayed. Each arm is a product,

pulling an arm corresponds to displaying that product, and the arm’s state is the Bayesian posterior

distribution on the product’s revenue-generation rate. Problems in target search by unmanned

aerial vehicles (Le Ny et al. 2006, Ni˜no-Mora and Villar 2011), online advertising (Gupta et al.

2011, Scott 2010, Chakrabarti et al. 2009), network communication (Liu and Zhao 2009, Al Islam

et al. 2012), and sensor management (Hero and Cochran 2011, Ni˜no-Mora and Villar 2011, Evans

et al. 2005, Ni˜no-Mora and Villar 2011) also ﬁt into our framework.

We study a regime in which the number of arms grows large and the per-period budgets grow pro-

portionally. This regime was ﬁrst studied in Whittle (1980) and has been of longstanding theoretical

interest. Moreover, it is practically important in many settings. In examples above, crowdsourced

labeling is most challenging when there are many images to label, and selecting products for display

is most challenging when many products are available.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

3

Despite its importance, this regime presents substantial algorithmic diﬃculties. While, in prin-

ciple, one can compute the optimal policy for restless bandit problems via stochastic dynamic

programming, the state of this dynamic program includes the state of each arm and so its dimension

grows linearly with N . Because of the curse of dimensionality (Powell 2007), solving this dynamic

program requires computation exponential in N .

As a result, there has been substantial interest (e.g., Whittle 1980, Weber and Weiss 1990, Zayas-

Caban et al. 2019, Hu and Frazier 2017, Brown and Smith 2020) in developing approximate policies

whose performance is provably close to optimal but require computation that does not grow with

N . Despite, however, substantial interest and eﬀort focusing on this regime, current understanding

is limited in several important ways.

First, simulation studies show much better performance for large N in some problems than

the best existing theoretical results. Indeed, the tightest existing upper bound on the optimality

gap (the diﬀerence in performance between the optimal policy and an approximate policy) for

such policies is O(

√

N ), shown by Brown and Smith (2020) (Zayas-Caban et al. 2019 provides a

policy with a slightly weaker bound of O(

√

N log N )). Surprisingly, however, simulation studies by

Brown and Smith (2020) suggest that the true optimality gap in some problems actually does not

grow at all with the number of arms and remains constant at O(1). The proof techniques used

by Brown and Smith (2020) and Zayas-Caban et al. (2019), however, rely heavily on the Central

Limit Theorem (CLT), and do not oﬀer a path toward showing a bound tighter than O(

√

N ).

Second, existing theoretical results showing bounds on the optimality gap are restricted to speciﬁc

policies (o(N ), O(

√

N log N ) and O(

√

N ), respectively in Hu and Frazier 2017, Zayas-Caban et al.

2019, Brown and Smith 2020). At the same time, one would expect a very wide class of policies

would achieve o(N ) and O(

√

N ) optimality gaps.

Our work ﬁlls these two gaps: we propose a broad class of policies, called ﬂuid-priority policies,

which generalize the essential characteristics of policies proposed by Brown and Smith (2020)

and Hu and Frazier (2017). Addressing the inconsistency between simulation studies and past

4

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

theoretical results, we characterize a suﬃcient condition, which we call “non-degeneracy”, under

which any ﬂuid-priority policy achieves an O(1) optimality gap, strictly better than all previous

results. The simulation study consistent with an O(1) optimality gap in Brown and Smith (2020)

satisﬁes this non-degeneracy condition. We also address the current literature’s lack of generality by

providing general easy-to-verify suﬃcient conditions ensuring o(N ) and O(

√

N ) optimality gaps.

All ﬂuid-priority policies satisfy these conditions and thus always achieve an O(

√

N ) optimality

gap. The policies proposed by Hu and Frazier (2017) and Brown and Smith (2020) also satisfy the

suﬃcient conditions for an O(

√

N ) optimality gap and thus our results generalize those in this

previous work.

To achieve such strong performance, ﬂuid-priority policies generalize well-known index policies

by classifying an arm’s state into active, neutral and inactive categories. This classiﬁcation is based

on a solution to a linear programming (LP) relaxation that has been important in past analyses of

the restless multi-armed bandit problem (Whittle 1980, Bertsimas and Ni˜no-Mora 2000, Hawkins

2003). To be called a ﬂuid-priority policy, it should ﬁrst pull as many arms as possible in active

states, up to the budget constraint on the number of arms that can be pulled in this period. Then,

if budget remains, it should should pull arms in neutral states in proportions determined by the

solution of the relaxed problem. Finally, only if budget remains, it should pull arms in inactive

states. There exist many ﬂuid-priority policies because they may prioritize arms in diﬀerent orders

within active, neutral and inactive categories.

Understanding that ﬂuid-priority policies all have good asymptotic performance brings several

beneﬁts. First, it provides a uniﬁed understanding of the convergence properties of existing meth-

ods, like those proposed in Hu and Frazier (2017) and Brown and Smith (2020). Second, it can

serve as a guideline when developing new policies: it is reasonable to restrict policy development

to those within the ﬂuid-priority class. Third, it creates an opportunity for focused simulation-

based search to create policies with excellent empirical performance and provably state-of-the-art

asymptotic performance. Fluid-priority policies are parameterized by the priority order over active

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

5

categories, neutral categories, and inactive categories. (Also, if there are multiple optimal solutions

to the relaxed problem, they are additionally parameterized by the choice of solution.) While still

large in problems whose arms have many states, one can perform a focused simulation-based search

over this class to ﬁnd policies with good performance in a speciﬁc problem of interest. In problems

where the number of single-arm states is small enough, it is even possible to search exhaustively

over all ﬂuid-priority policies. In one numerical experiment, we use this strategy to develop a new

ﬂuid-priority policy that signiﬁcantly outperforms the existing state of the art.

We demonstrate and illustrate these contributions via numerical experiments. Our ﬁrst experi-

ment is a Bayesian multi-armed bandit problem with Bernoulli rewards. We ﬁrst verify numerically

that this problem is non-degenerate. We then use simulation to calculate expected performance

under a ﬂuid priority policy similar to the policies in Hu and Frazier (2017) and Brown and Smith

(2020) and observe that the optimality gap stays constant. In contrast, we show that the widely

used UCB (Agrawal 1995) and Thompson Sampling (Agrawal and Goyal 2012) policies have Ω(N )

optimality gaps and signiﬁcantly underperform by our ﬂuid-priority policy. Our second experiment

is an active learning problem in which one seeks to allocate crowdsourcing eﬀort to accurately

classify items, previously studied by Chen et al. (2013). We iterate over all possible ﬂuid-priority

policies and choose the one with best performance. We ﬁnd that this ﬂuid priority policy signiﬁ-

cantly outperforms two previously proposed policies: the Knowledge Gradient (Frazier et al. 2008)

and Optimistic Knowledge Gradient (Chen et al. 2013) policies. Finally, we verify numerically that

our non-degeneracy condition holds for dynamic assortment problem studied in Brown and Smith

(2020), thus explaining why their simulation study shows an O(1) optimality gap.

Below, we summarize our contribution after ﬁrst reviewing the literature.

1.1. Literature Review

Here we review in more detail the three streams of literature most related to our work.

6

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Frequentist Bandits: The most well-known stream of related work uses frequentist analysis and

focuses on problems in which we have uncertainty about an arm’s underlying state. In such prob-

lems, arms are characterized by some underlying but unknown distribution over rewards. This is

typically assumed ﬁxed (Lai and Robbins 1985, Auer et al. 2002), but can change in some recent

analysis (Besbes et al. 2014, Zhou et al. 2020). This literature designs strategies that minimize

worst-case expected regret, i.e., the expected diﬀerence in total reward compared with a policy

that knows arms’ underlying characteristics.

This work is quite diﬀerent from ours for two reasons. First, it studies a diﬀerent model using a

diﬀerent performance measure. The model we study assumes that all arms have a fully observed

state that evolves stochastically according to Markov processes with known transition kernels and

known state-dependent rewards. To apply our model to systems whose arms have unknown reward

distributions (which can either be static or vary with a stochastically varying and observable

state), one ﬁrst proposes a Bayesian prior probability distribution over the parameters of these

distributions. Then, the Bayesian posterior (which is fully observable) is computed and included as

part of the arm’s state. Rather than worst-case expected regret, we maximize average case expected

reward where initial arm characteristics are drawn at random from the prior. This follows the

practice used in partially observable Markov Decision Processes of studying average case reward

under a prior belief state. The model we study can also be proﬁtably applied to dynamic systems

without uncertainty about arms, such as the allocation of airplanes to maintenance bays (Cho et al.

2015).

Second, this diﬀerence in model and performance measure creates signiﬁcant diﬀerences in achiev-

able performance. As we show, policies exist whose average case optimality gap is O(1) in the

number of arms N . In frequentist bandits, however, the (worst-case) regret grows linearly with N

in the simplest case in which arms’ characteristics do not change over time (Lai and Robbins 1985).

There is some work that imposes constraints on the relationships between arms, which allows regret

to be o(N ), such as work on linear bandits (Goldenshluger and Zeevi 2013), but these models are

quite diﬀerent from the one we consider.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

7

Rather than focusing on N , most of this literature focuses on the regime where the horizon T

increases to inﬁnity with the number of arms N ﬁxed. Lai and Robbins (1985) bounds the regret

below by a factor proportional to log(T ). Celebrated algorithms such as upper conﬁdence bound

(UCB) (Auer et al. 2002) and Thompson Sampling (Agrawal and Goyal 2012) are proved to achieve

this lower bound asymptotically. This stream of work relies on the fact that a long horizon permits a

large number of pulls per arm, which distinguish the “best” arm from others with high probability.

In our setting, however, where the number of arms is large enough to permit only a small number

of pulls per arm and the horizon remains ﬁxed, asymptotic guarantees focusing on large T may not

be relevant. Thus, although there is a large literature demonstrating that variants of UCB (Auer

et al. 2002), Thompson sampling (Agrawal and Goyal 2012), epsilon greedy (Sutton and Barto

1999), and other related algorithms have provably small regret in the large T setting, these results

do not imply good performance in the large N setting that we study. Indeed, in our simulation

study, we show that the optimality gap grows linearly with N under UCB and Thompson sampling

in the Bayesian ﬁnite-horizon multi-armed bandit with Bernoulli reward.

Fixed N and T : A second and more relevant stream of work (Guha and Munagala 2007, 2008,

2013, Guha et al. 2010, Farias and Madan 2011, Bertsimas and Ni˜no-Mora 2000) considers the same

model that we consider here and focuses on average-case performance, but considers a regime with

a ﬁxed horizon and a ﬁxed number of arms. This work often solves a linear relaxation of the original

problem, constructs a policy based on the solution, and then proves that this policy provides a

constant-factor approximation to the optimal one. For example, Farias and Madan (2011) shows

that the heuristic they propose achieves an 8-approximation of the optimal policy. Nevertheless,

the optimality gap of such policies may scale linearly with N , as a constant factor approximation

does not preclude this possibility.

Large N , ﬁxed T : The third and most closely related stream considers the same model as the

one we consider here and the same asymptotic regime, where the number of arms and the budget

per period increase proportionally to inﬁnity while holding the horizon ﬁxed. The regime was

8

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

ﬁrst studied by Whittle (1980) in the inﬁnite-horizon discounted reward setting. Whittle (1980)

introduced a time-homogeneous Lagrangian relaxation of the budget constraints and proposed the

so-called “Whittle index” policy when arms are “indexable”, and conjectured the Whittle index

achieves an o(N ) optimality gap when this indexability condition holds. However, Weber and Weiss

(1990) later showed that even under indexability, the optimality gap under the Whittle index policy

grows linearly in N for some problems. Though intuitively promising, the Whittle index policy

suﬀers from the diﬃculty of verifying the indexibility condition, the inability to use the policy if

indexability does not hold, and, in some problems, from weak empirical performance. Nevertheless,

as a pioneering work in restless bandits, the Whittle index inspired a stream of follow-up work, in

both the inﬁnite-horizon (Bertsimas and Ni˜no-Mora 2000, Glazebrook et al. 2006, Dayanik et al.

2008) and ﬁnite-horizon cases. As it is the focus of our work, we now discuss the ﬁnite-horizon case

in detail.

Following Whittle’s earlier work, later literature (e.g., Hu and Frazier 2017, Zayas-Caban et al.

2019, Brown and Smith 2020) studies the ﬁnite-horizon restless bandit using Lagrangian relax-

ations. Unlike Whittle’s work, these use time-dependent Lagrange multipliers because of the non-

stationary nature of ﬁnite-horizon problems. This technique yields promising performance guaran-

tees and empirical results without the need for an indexability condition. Hu and Frazier (2017)

studies the binary-action bandit problem and proposes an index policy achieving an o(N ) optimal-

ity gap. Zayas-Caban et al. (2019) studies the multi-action bandit problem and proposes a policy

achieving an O(

√

N log N ) optimality gap. Brown and Smith (2020) studies the same setting as
√

Hu and Frazier (2017) and proposes policies with an O(

N ) optimality gap. However, simulation

experiments (Brown and Smith 2020) suggest, surprisingly, that the optimality gap might not grow

with N . Our work proposes a novel policy class, the class of ﬂuid-priority policies, and shows that

policies proposed by Hu and Frazier (2017), Brown and Smith (2020) are special cases in this

class. Furthermore, we show that any policy in this class achieves an O(

√

N ) optimality gap in

all circumstances and achieves an O(1) optimality gap when a non-degeneracy condition holds.

Specially, for the setting discussed above in which simulation experiments from Brown and Smith

(2020) suggest the optimality gap is O(1), we show the non-degeneracy condition holds.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

9

1.2. Summary of Contributions and Outline

There are three main contributions in our work.

Main Contribution: Our main contribution is to propose a novel and general class of policies,

ﬂuid-priority policies, and show that they have strong theoretical performance guarantees. We show

theoretically that any ﬂuid-priority policy achieves an O(

√

N ) optimality gap in all circumstances

and achieves an O(1) optimality gap under a non-degeneracy condition.

Secondary Contributions: Building on our main result, we provide three secondary contributions.

1. We establish o(N ) and O(

√

N ) optimality gaps for classes of policies broader than ﬂuid

priority policies. The suﬃcient conditions used by our proof are easy to verify and general

enough to apply to policies proposed in Hu and Frazier (2017), Brown and Smith (2020).

2. We propose an algorithm for verifying whether non-degeneracy condition holds. If so, this

algorithm searches over optimal occupation measures to ﬁnd one that is non-degenerate.

3. We demonstrate the value of ﬂuid policies through numerical experiments and additional

theory.

• By searching numerically over ﬂuid priority policies, we identify a novel ﬂuid priority

policy that outperforms a previously proposed state-of-the-art policy designed speciﬁcally

for crowdsourced labeling.

• We show that the dynamic assortment problem previously studied by Brown and Smith

(2020) satisﬁes the non-degeneracy condition, and thus our theoretical results explain the

hitherto poorly understood performance of Lagrangian index policies in this setting.

• We show theoretically that the widely-used UCB and Thompson sampling algorithms for

ﬁnite-horizon Bernoulli bandits have strictly worse asymptotic expected performance (in

the worst case over problem instances) than ﬂuid priority policies for problems with many

arms. We then demonstrate numerically that ﬂuid priority policies have substantially

better empirical performance in a collection of such problems.

Organization of This Paper : The rest of the paper is organized as follows. §2 deﬁnes the restless

bandit problem as a MDP. §3 introduces notation and provides background on an existing linear

10

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

programming relaxation used in our later novel theoretical results. This relaxation provides an

upper bound on the problem’s optimal performance. Based on the upper bound, §4 describes

suﬃcient conditions to achieve an O(N ) optimality gap and §5 describes suﬃcient conditions to

achieve an O(

√

N ) optimality gap. §6 proposes the class of ﬂuid-priority policies and proves that

they achieve an O(

√

N ) optimality gap. §7 proves that ﬂuid-priority policies achieve an O(1)

optimality gap when the non-degeneracy condition is met. §8 provides numerical studies and §9

concludes our work.

2. System Model

This section formulates our decision-making problem as a Markov Decision Process (MDP).

Model: There are N arms, each of which shares the same ﬁnite state space S. We use si,t to

indicate the state of arm i at time t. At each period t for each arm i, the decision-maker chooses

whether to pull the arm (ai,t = 1) or leave it idle (ai,t = 0). We deﬁne A = {0, 1} to be the space

of available actions in which ai,t takes values. These actions must respect a so-called “budget

constraint” in which the number of arms pulled at period t is Bt = ⌊αtN ⌋, where 0 ≤ αt ≤ 1 is a

pre-speciﬁed budget ratio.

Based on the action applied, each arm’s state transitions stochastically to time t + 1 according

to a known transition kernel Pt = {pt(s, a, s′)}s,s′∈S,a∈A where pt(s, a, s′) = P(st+1,i = s′∣st,i = s, at,i = a).

All arms share the same transition kernel, and any arm’s transition is conditionally independent

from others given its own state and action. (Arm-speciﬁc transition kernels can be modeled by

deﬁning static arm “types” and extending the state space to specify the arm’s type.) At time

period t, each state-action pair is associated with a reward, given by a known reward function

rt ∶ S × A → R. The decision-maker aims to maximize the total reward collected from all N arms

over a ﬁnite horizon subject to the budget constraint.

To complete the formal deﬁnition of our problem involving N arms, we introduce some additional

notation. We use S = SN to denote the N -fold Cartesian product of the state space S and deﬁne

A = AN similarly. All N arms together form an MDP with state space S and action space A. We

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

11

call this the “joint MDP” to distinguish it from MDPs that we reference later involving a single

arm. The state in this joint MDP at time t is st = (st,1, st,2, ..., st,N ) ∈ S, which indicates that arm i

has state st,i. The action is at = (at,1, at,2, ..., at,N ) ∈ A, which indicates that action at,i is applied to

arm i.

The reward function of the joint MDP, Rt ∶ S × A → R, is the sum of the single-arm rewards

deﬁned above,

Rt(st, at) =

N
∑
i=1

rt(st,i, at,i).

For element a = (a1, a2, ..., aN ) in A, we use ∣a∣ = ∑N

i=1 ai to indicate the L1-norm of a, i.e, the

number of pulled arms. We write our budget constraint at time t as ∣at∣ = Bt.

The transition kernel for the joint MDP is the product of each arm’s transition kernel,

P[st+1∣st, at] =

N
∏
i=1

pt(st,i, at,i, st+1,i).

We assume all arms start from the same initial state s∗. Our analysis can be easily generalized

to the case where arms start from diﬀerent states.

A policy π is a function that maps the current state st ∈ S and time t to an action at ∈ A. The

objective of the policy is to maximize the expected total reward, subject to the budget constraint

speciﬁed above.

This objective can be written as,

max
π

Eπ

T
∑
t=1

Rt(st, at)

subject to: ∣at∣ = ⌊αtN ⌋, ∀t ∈ [T ],

(1)

where Eπ indicates the expectation taken under policy π.

We deﬁne the value function of a policy π as VN (π) = Eπ ∑T

t=1 Rt(st, at). We measure a policy’s

performance by comparing its value with that of an optimal policy solving (1). Let V ∗
N

= supπ VN (π)

be the value of an optimal policy. Then the optimality gap of the policy π is deﬁned as

V ∗
N

− VN (π).

12

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Maximizing the value function across policies is equivalent to minimizing the optimality gap. We

are interested in ﬁnding policies with small optimality gaps when N is large.

Applications: The above model has many applications. In the most direct application, each arm

corresponds to a physical process that evolves stochastically and independently of the other physical

processes according to a known transition kernel. Examples include network communication (Liu

and Zhao 2009, Al Islam et al. 2012) and machine maintenance (Glazebrook et al. 2006, Abbou

and Makis 2019, Cho et al. 2015). For example, in maintenance of military aircraft with low radar

visibility (so-called “stealth” aircraft) (Cho et al. 2015), each aircraft is treated as an arm. Radar

visibility (the state of the arm) increases stochastically according to a known transition kernel each

time the aircraft ﬂies as small particles in the air damage the aircraft’s paint and underlying metal

surface. This damage can be repaired (the arm can be pulled) by pausing an aircraft’s ﬂights and

performing maintenance. Our objective is allocate limited maintenance resources to maximize an

objective combining ﬂights ﬂown and number of aircraft with low radar visibility.

In addition, there are many applications in which information evolves over time. In such settings,

we often have several independent unknown quantities, each arm corresponds to one of these quan-

tities, and an arm’s state represents the information that we have about this quantity. Examples

include autonomous target tracking (Le Ny et al. 2006, Hero and Cochran 2011), where each target

is treated as an arm, and its state is whether it is tracked by a sensor and some physical feature

aﬀecting the motion of the target. Based on its state, the target moves to a new location, and our

objective is to track as many targets for as long as possible.

In perhaps the most famous restless bandit, each arm corresponds to a slot machine. Each slot

machine generates payoﬀs according to a distribution from a parametric family (e.g., Bernoulli).

The parameter governing an arm’s rewards (for Bernoulli arms, the payoﬀ probability) is drawn at

random from a Bayesian prior distribution and is unobserved. The state of the arm is the Bayesian

posterior distribution on its parameter, given all observed payoﬀs from the arm. When we pull an

arm, we earn a reward (whose distribution is given by marginalizing over the posterior on the arm’s

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

13

uncertain parameter) and the new state is determined by Bayes’ rule and the observed reward. If

an arm’s underlying parameter changes over time, then this causes the posterior to change even if

the arm is not pulled, making the problem restless. A common point of confusion arises from the

fact that this problem has a similar motivation to the more widely studied non-Bayesian stochastic

bandit (Lai and Robbins 1985, Auer et al. 2002), but uses a diﬀerent formulation. For a tutorial

on Bayesian formulations of multi-armed bandits, some of which are restless, see Mahajan and

Teneketzis (2008).

3. Background: Preliminary Results and Notation

In this section, we deﬁne a linear programming relaxation that bounds ˆV ∗

N for V ∗

N . Although this

bound is standard in the literature and is not part of our contribution, we include it to provide a

self-contained presentation and to establish notation used later.

Linear Programming Relaxation: Similar to Wu et al. (2015), Farias and Madan (2011),

Guha and Munagala (2008), we introduce this relaxation of Problem (1):

ˆV ∗
N

∶= max
π

Eπ

T
∑
t=1

Rt(st, at)

subject to Eπ∣at∣ = αtN, ∀t ∈ [T ].

(2)

This relaxes problem (1)’s almost sure cardinality constraints (on both the initial occupation

measure and the number of pulls) to constraints on the expected cardinality. As we will see soon,

solving relaxation (2) is equivalent to solving a linear program whose number of decision variables

does not depend on N (see Lemma 1 and the linear program (4)). For simplicity of presentation,

we assume that αt are rational and we restrict attention and limits taken below over N causing

αtN to be integral for all t ∈ [T ]. Our results essentially generalize to irrational αt and non-integral

αtN as discussed brieﬂy in Appendix 10.2.

The value of this relaxed problem, ˆV ∗

N , is an upper bound on V ∗

N . We use this upper bound

extensively later to bound the optimality gap of the policies we study. Moreover, the policies we

study in §6 heavily leverage this relaxation in their deﬁnition. They beneﬁt from the fact that the

14

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

relaxation yields a low-dimensional problem whose number of decision variables and constraints do

not scale with N . This allows the relaxation’s solution to be computed and used to deﬁne practical

policies, even when N is large.

The following lemma formally states this bound and also observes (via Fenchel’s duality theorem,

and the separability of a dualized version of Problem (2)) that ˆV ∗

N is determined by the solution

to a single-armed problem ˆV ∗

1 . Its proof can be found in the appendix.

Lemma 1. V ∗
N

≤ ˆV ∗
N

= N ˆV ∗
1 .

The quantity ˆV ∗

1 is given by,

max
π

Eπ

T
∑
t=1

rt(st, at)

subject to Eπ∣at∣ = αt, ∀t ∈ [T ].

(3)

Later analysis and computation is supported by the following equivalent version of Problem (3).

Deﬁning the occupation measure, xt(s, a) ∶= P[st = s, at = a], Problem (3) is equivalent to

max ∑

s∈S,a∈A

T
∑
t=1

rt(s, a)xt(s, a)

subject to

∑
a∈A

xt(s, a) = ∑
a∈A

∑
s′∈S

xt−1(s′, a)pt−1(s′, a, s), ∀s ∈ S, 2 ≤ t ≤ T ,

xt(s, 1) = αt, t ∈ [T ],

x1(s∗, a) = 1,

∑
s∈S

∑
a∈A

∑
a∈A

∑
s∈S

x1(s, a) = 1, ∀s ∈ S,

xt(s, a) ≥ 0, ∀s ∈ S, a ∈ A, t ∈ [T ].

(4)

The ﬁrst constraint of Problem (4) ensures that ﬂows are balanced; the second ensures that the

budget constraint is met; and the third follows from the initial occupation measure. We let xt(s, a)

denote the entries in an optimal occupation measure, i.e., one that solves Problem (4). Then, we

can compute,

ˆV ∗
1

= ∑

s∈S,a∈A

T
∑
t=1

rt(s, a)xt(s, a).

(5)

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

15

The class of policies we analyze depend on solving Problem (4) computationally using a linear

programming solver. As noted above, this is possible, even when N is large, because the dimen-

sionality of Problem (4) does not depend on the number of arms N .

Additional Notation: Here we introduce some additional notation used in the following sec-

tions. Given the optimal occupation measure, we use zt(s) ∶= ∑a∈A xt(s, a) to denote the probability

that an arm is in state s at time t under this measure. We use zt and xt to refer to the corresponding

vector (or matrix), i.e., zt ∶= (zt(s), s ∈ S) or xt ∶= (xt(s, a) ∶ s ∈ S, a ∈ A).

In the joint MDP with N arms, we let X N
t

(s, a) be the number of arms in state s for which we

take action a at time t. We let Z N
t

(s) be the number of arms in state s at time t. We use Z N

t , X N
t

to refer to the vectors (Z N
t

(s) ∶ s ∈ S) and matrix (X N
t

(s, a) ∶ s ∈ S, a ∈ A). Using this notation, a

policy π of the joint MDP is a map from Z N
t

to X N
t .

§5 will study deviations between the realization of (Z N

t , X N
t

) and (N zt, N xt), and how these

deviations impact the joint MDP’s reward. To support this analysis, we deﬁne diﬀusion statistics

t and ˜X N
˜Z N

t as

˜Z N
t

t

− N zt
= Z N
√
N

,

˜X N
t

t

− N xt
= X N
√
N

.

Using this notation, a policy π of the joint MDP naturally induces a class of maps ˜πt,N indexed by

t and N , from diﬀusion ˜Z N
t

to diﬀusion ˜X N

t , such that

π(t, Z N
t

) = X N
t

⇐⇒ ˜πt,N ( ˜Z N

t

) = ˜X N
t .

(6)

4. Suﬃcient Conditions for Achieving an o(N ) Optimality Gap

This section establishes the ﬁrst of our contributions: general suﬃcient conditions for an o(N )

optimality gap. This result allows us to directly verify that the policy in Zayas-Caban et al. (2019)

has an o(N ) optimality gap. We build on the results here in the next section, where we give stronger

conditions suﬃcient for an O(

√

N ) gap and apply it to the policies in Hu and Frazier (2017) and

Brown and Smith (2020). This is in preparation for our main contribution in §6, a class of policies

with an O(1) gap.

16

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

The main idea in this section is, essentially, that as long as the number of arms we pull in each

state, X N

t , is approximately proportional to the optimal occupation measure xt (a property we

formalize and give the name “ﬂuid consistency”), the number of arms in the next period Z N

t+1 in

each state will be approximately proportional to zt+1. This will cause the reward of the joint MDP

to scale proportionally with ˆV ∗

1 . While random ﬂuctuations cause proportionality to hold only

approximately, their resulting loss in reward is o(N ).

We begin by formally deﬁning the notion of ﬂuid consistency.

Definition 1. Under a policy π, if π(t, Z N
t

)/N → xt for all t ∈ [T ] and sequences (Z N

t

∶ N ) satis-

fying Z N
t

/N → zt, then we say the policy π is ﬂuid consistent.

Based on this deﬁnition, we have the following lemma, whose proof can be found in the appendix.

Lemma 2. If a policy π is ﬂuid consistent, then

Z N
t
N

→ zt,

X N
t
N

→ xt,

almost surely for any t ∈ [T ] as N → ∞.

Using Lemma 2, we now show the main result of this section: that ﬂuid consistency implies the

optimality gap is o(N ).

Theorem 1. If a policy π is ﬂuid consistent, then V ∗
N

− VN (π) = o(N ).

Proof of Theorem 1 Because the policy π is ﬂuid consistent, Lemma 2 shows

Z N
t
N

→ zt,

X N
t
N

→ xt.

The total reward of the joint MDP, divided by N , is

1
N

Eπ

T
∑
t=1

Rt(st, at) = 1
N

Eπ

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a)

= Eπ

T
∑
t=1

∑
s∈S,a∈A

rt(s, a) X N

t

(s, a)
N

→ Eπ

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)xt(s, a)

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

17

as N → ∞, where we leverage the dominated convergence theorem, the fact that rewards are

bounded, and 0 ≤ X N
t

(s, a) ≤ N . Thus, we have shown that V ∗
N

− VN (π) = o(N ).

◻

One can show that the the policies in Hu and Frazier (2017), Zayas-Caban et al. (2019), Brown

and Smith (2020) are all ﬂuid consistent and thus have o(N ) optimality gaps. We show this for

Zayas-Caban et al. (2019) in Appendix 10.13. Below, we show that Hu and Frazier (2017), Brown

and Smith (2020) meet a stronger condition and thus have O(

√

N ) optimality gaps.

5. Suﬃcient Conditions for Achieving an O(

N ) Optimality Gap

√

This section establishes our second contribution: a substantially more general result than in the
√

literature showing suﬃcient conditions for an O(

N ) optimality gap. Using this result, we directly

verify that policies in Hu and Frazier (2017) and Brown and Smith (2020) have O(

√

N ) optimality

gaps. This section also provides stepping stones towards our main contribution, described in §6.

The main idea in this section is that, as long as the diﬀusion statistic ˜X N
t

is bounded by O(1),

then ˜Z N

t+1 will also be bounded by O(1). Thus, the deviation between the reward of the joint MDP

and the relaxation’s upper bound ˆV ∗

N will be bounded by

√

N ⋅ O(1) = O(

√

N ).

Recall Equation (6), that a policy π naturally induces a class of maps ˜πt,N . Using this idea, we

say a policy π is “diﬀusion regular” if all induced maps ˜πt,N keep the diﬀusion ˜X N

t bounded by

O(1). We deﬁne this formally here.

Definition 2. A policy π is called diﬀusion regular if its induced maps ˜πt,N satisfy the following

conditions, where ∣ ⋅ ∣ indicates the L1-norm in Euclidean space.

1. There exists C1 > 0 s.t. ∣˜πt,N (θ1) − ˜πt,N (θ2)∣ ≤ C1∣θ1 − θ2∣ for all t, N , θ1 and θ2.

2. There exists C2 > 0 s.t. ∣˜πt,N (0)∣ ≤ C2 for all t and N .

3. There exists a map ˜πt,∞ s.t. ˜πt,N (θ) → ˜πt,∞(θ) as N → ∞ for all θ.

We brieﬂy note the following fact, useful when proving subsequent results. Its proof is found in

the appendix.

Lemma 3. If a policy is diﬀusion regular then it is also ﬂuid consistent.

18

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

We now show that if a policy π is diﬀusion regular, the diﬀusion statistics ˜X ∞

t and ˜Z ∞

t converge

in distribution (Lemma 4) and their second moments are uniformly bounded (Lemma 5). Proofs

of Lemma 4 and Lemma 5 are in the Appendix.

Lemma 4. If a policy π is diﬀusion regular, then for any t ∈ [T ], there exists sub-Gaussian random

vectors ( ˜Z ∞

t , ˜X ∞

t

) such that ( ˜Z N

t , ˜X N

t

) → ( ˜Z ∞

t , ˜X ∞

t

) in distribution as N → ∞.

Lemma 5. If a policy π is diﬀusion regular, then there exists a constant C such that Eπ[∣∣ ˜Z N

t

∣∣2
2

] ≤ C

and Eπ[∣∣ ˜X N

t

∣∣2
2

] ≤ C for all t ∈ [T ] and N , where ∣∣ ⋅ ∣∣2 indicates the L2 norm.

Based on Lemma 4 and 5, we can prove the following theorem.

Theorem 2. If a policy π is diﬀusion regular, then V ∗
N

√

− VN (π) = O(

N ).

Proof of Theorem 2 Since the policy π is diﬀusion regular, there exists sub-Gaussian random

vectors ˜Z ∞

t , ˜X ∞

t , such that ˜Z N

t

→ ˜Z ∞

t and ˜X N

t

→ ˜X ∞
t

in distribution as N → ∞ by Lemma 4.

Also, the optimality gap is bounded above by

V ∗
N

− VN (π) ≤ N ˆV ∗

1

− VN (π)

= N

T
∑
t=1
√

= −

N Eπ

∑
s∈S,a∈A

rt(s, a)xt(s, a) − Eπ

T
∑
t=1

Rt(st, at)

T
∑
t=1

∑
s∈S,a∈A

rt(s, a) ˜X N

t

(s, a).

Divide both sides of this bound by

integrable (Lemma 5),

√

N and take N → ∞. Then, since ˜X N

t and ˆY N

t are uniformly

lim sup
N

1√
N

(V ∗
N

− VN (π)) ≤ lim sup

−Eπ

N

T
∑
t=1

∑
s∈S,a∈A

rt(s, a) ˜X N

t

(s, a) = −Eπ

T
∑
t=1

∑
s∈S,a∈A

rt(s, a) ˜X ∞

t

(s, a).

To summarize, we have shown V ∗
N

√

− VN (π) = O(

N ).

◻

We verify in the Appendix 10.13 that the policies proposed by Hu and Frazier (2017) and Brown

and Smith (2020) are diﬀusion regular and thus (by Theorem 2) have O(

√

N ) optimality gaps.

Thus, Theorem 2 generalizes the performance guarantees shown in that previous work.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

19

6. Fluid-priority policies

This section deﬁnes ﬂuid-priority policies and show that they are always diﬀusion regular and

thus achieve an O(

√

N ) optimality gap. Later, in Section 7, we show that they achieve an O(1)

optimality gap if an additional condition is satisﬁed.

Roughly speaking, a ﬂuid-priority policy is deﬁned by ﬁrst fetching an optimal solution of the LP

relaxation, then classifying states into three disjoint categories based on the solution: ﬂuid-active,

ﬂuid-neutral and ﬂuid-inactive. A ﬂuid priority policy is one that pulls arms while respecting a

prioritization derived from these categories: arms in ﬂuid-active states are prioritized for pulling

over those in ﬂuid-neutral states; and arms in ﬂuid-neutral states are prioritized in turn over arms in

ﬂuid-inactive states. Additionally, when pulling arms in ﬂuid-neutral states, a ﬂuid-priority policy

must do so according to proportions derived from LP relaxation.

Mathematically speaking, a ﬂuid-priority policy is parameterized by an occupation measure

{xt(s, a)}t,s,a solving Problem (4) and a sequence of “priority-score” functions {Pt(⋅)}t assigning

each state a real number. Based on the occupation measure {xt(s, a)}t,s,a, a ﬂuid-priority policy

classiﬁes states into these three disjoint categories:

The ﬂuid-active category: C +
t

∶= {s ∈ S∣xt(s, 1) > 0, xt(s, 0) = 0},

The ﬂuid-neutral category: C 0
t

∶= {s ∈ S∣xt(s, 1) > 0, xt(s, 0) > 0},

(7)

The ﬂuid-inactive category: C −
t

∶= {s ∈ S∣xt(s, 1) = 0, xt(s, 0) = 0}.

We refer to an arm whose state is in the ﬂuid-active category as a ﬂuid-active arm. We deﬁne

the terminology ﬂuid-neutral arm and ﬂuid-inactive arm similarly. With these deﬁnitions in place,

the ﬂuid-priority policy corresponding to a given occupation measure and priority-score function

is deﬁned by Algorithm 1.

Algorithm 1 allocates its budget by ﬁrst pulling as many ﬂuid-active arms as possible, subject

to the budget constraint (Lines 5-7). If budget remains, then it pulls as many ﬂuid-neutral arms

as possible, again subject to the constraint on the remaining budget (Lines 9-17).

20

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

When there is enough budget to pull some ﬂuid-neutral arms, but not all of them, the budget is

allocated carefully across them to ensure ﬂuid-consistency. This is closely related to “tie-breaking”

as discussed in Algorithm 2 of Hu and Frazier (2017). In particular, lines 9-13 ensure that the

number of arms pulled in each ﬂuid-neutral state is at least equal to ⌊N xt(s, 1)⌋, the number of

arms from this state pulled in the ﬂuid relaxation, as long as the budget constraint Bt and number

of available arms Zt(s) allows. If budget remains after this is achieved, additional ﬂuid-neutral

arms are pulled.

Finally, if budget remains after all ﬂuid-neutral arms are pulled, additional ﬂuid-inactive arms

are pulled until the budget is exhausted. Within each category (ﬂuid-active, ﬂuid-neutral, ﬂuid-

inactive), states are prioritized based on the priority score.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

21

Algorithm 1 Fluid-priority policy

Input: optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A found by solving the linear program

(4) and priority-score functions {Pt}t∈[T ].

1: for t = 1, 2, ..., T do

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

Classify states into ﬂuid-active (C +

t ), ﬂuid-neutral (C 0

t ) and ﬂuid-inactive (C −

t ) categories

based on the occupation measure, according to equation (7).

Observe there are Zt(s) arms in state s and remaining budget Bt = ⌊αtN ⌋.

Each of the for loops below iterates over states in decreasing order of Pt(s)

for state s in C +

t do

Plan to pull X N
t

(s, 1) ← min{Bt, Zt(s)} arms out of the Zt(s) arms in state s.

Update remaining budget Bt ← Bt − min{Bt, Zt(s)}.

end for

for state s in C 0

t do

Plan to pull (at least) X N
t

(s, 1) ← min{Bt, Zt(s), ⌊N xt(s, 1)⌋} arms in state s.

Store the number of undecided arms U N
t

(s) ← Z N
t

(s) − X N
t

(s, 1).

Update the remaining budget Bt ← Bt − min{Bt, Zt(s), ⌊N xt(s, 1)⌋}.

end for

for state s in C 0

t do

Plan to pull min{Bt, Ut(s)} additional undecided arms in state s.

Update X N
t

(s, 1) ← X N
t

(s, 1) + min{Bt, Ut(s)}.

Update Bt ← Bt − min{Bt, Ut(s)}.

end for

for state s in C −

t do

Plan to pull X N
t

(s, 1) ← min{Bt, Zt(s)} arms out of Zt(s) arms in state s.

Update remaining budget Bt ← Bt − min{Bt, Zt(s)}.

end for

For each s, pull X N
t

(s, 1) arms in state s (as planned above)

24: end for

22

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

With this deﬁnition in place, we now state the main result of this section: that ﬂuid-priority

policies are diﬀusion regular, implying they have an O(

√

N ) optimality gap by Theorem 2.

Theorem 3. Any ﬂuid-priority policy π is diﬀusion regular and its optimality gap is O(

√

N ).

The proof of Theorem 3 is in the Appendix.

7. Non-degeneracy Condition: Achieving an O(1) Optimality Gap

This section presents our main contribution: that ﬂuid-priority policies achieve an O(1) optimality

gap under a non-degeneracy condition. We ﬁrst deﬁne and discuss this condition and then show

this result.

To motivate this non-degeneracy condition, consider a ﬂuid-priority policy and another policy

motivated by the relaxed problem (2) in which the the almost-sure budget constraint (∣at∣ = αtN )

has been relaxed. This so-called “budget-relaxed” policy ﬁrst categorizes states into ﬂuid-active,

ﬂuid-neutral, and ﬂuid-inactive categories in the same way as its corresponding ﬂuid-priority policy.

It pulls all ﬂuid-active arms (even if this would exceed the budget). If budget remains, it then pulls

ﬂuid-neutral arms in the same way as its corresponding ﬂuid-priority policy. It does not pull any

ﬂuid-inactive arms, even if budget remains after ﬂuid-active and ﬂuid-neutral arms are pulled.

Pulling all ﬂuid-active arms and idling ﬂuid-inactive arms is exactly the property required for

any feasible policy to be optimal in the LP relaxation (2). Thus, this budget-relaxed policy’s reward

is close to the relaxed problem’s optimal reward (Lemma 8). Moreover, it behaves identically to its

corresponding ﬂuid-priority policy (Lemma 6) except on a speciﬁc “budget violation” event: that

the number of ﬂuid-active arms exceeds the budget, or the number of ﬂuid-active and ﬂuid-neutral

arms together fail to exceed the budget. The probability of budget-violation allows us to bound the

optimality gap for ﬂuid-priority policies by comparing them with their budget-relaxed versions.

The non-degeneracy condition (Deﬁnition 3) characterizes the probability of budget violation:

when it is met, the expected number of ﬂuid-active arms is strictly below the budget and the

expected number of ﬂuid-active and ﬂuid-neutral arms is strictly above the budget. Thus, using

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

23

concentration bounds, problems meeting the non-degeneracy condition are ones in which the prob-

ability of budget violation vanishes exponentially fast as N grows (Lemma 7). As a result, in

such problems, the ﬂuid-priority policy behaves the same as its budget-relaxed version with high

probability for large N . We use this fact to show an O(1) optimality gap in Theorem 4.

The rest of this section is organized as follow: we ﬁrst formally introduce budget-relaxed policies,

then deﬁne the non-degeneracy condition, and ﬁnally prove ﬂuid-priority policies achieve an O(1)

optimality gap when this condition holds. In addition, we show that no index policy can strictly

outperform all ﬂuid-priority policies. At the end of the section, we propose an algorithm to verify

whether non-degeneracy condition holds, and search for a non-degenerate occupation measure when

exists.

7.1. Budget-relaxed ﬂuid-priority policies

Given a ﬂuid-priority policy πF , its budget-relaxed version πR is deﬁned formally by Algorithm 2.

Similar to πF , πR ﬁrst classiﬁes states into three categories: ﬂuid-active, ﬂuid-neutral and ﬂuid-

inactive, using the same occupation measure as πF . Then, πR sorts states in each category in order

of decreasing priority-score (line 4), using the same priority score as πF . Afterwards, πR pulls all

arms in the ﬂuid-active category (lines 5 - 8), exceeding the budget if necessary. If there is still

budget remaining, πR iterates over each state s in the ﬂuid-neutral category C 0

t . It pulls arms

in this state until the number pulled reaches the quantity ⌊N xt(s, 1)⌋ derived from the optimal

occupation measure, no arms remain in this state, or we reach the budget. Unpulled arms in each

such state are called “undecided”. (lines 12 - 16). Finally, πR iterates over each state in the ﬂuid-

neutral category C 0

t again and pulls undecided arms until either the budget is met or all undecided

arms are pulled (line 17 - 21). Notice πR idles all arms in the ﬂuid-inactive category, even if budget

remains.

24

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Algorithm 2 Budget-relaxed ﬂuid-priority policies

Input: optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A, priority-score function {Pt}t∈[T ].

1: for t = 1, 2, ..., T do

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

Classify states into ﬂuid-active (C +

t ), ﬂuid-neutral (C 0

t ) and ﬂuid-inactive (C −

t ) categories

based on the occupation measure, according to equation (7).

Observe there are Zt(s) arms in state s, and remaining budget Bt = ⌊αtN ⌋.

Each of the for loops below iterates over states in decreasing order of Pt(s).

for state s in C +

t do

Plan to pull X N
t

(s, 1) ← Zt(s) arms out of Zt(s) arms in state s.

Update the remaining budget Bt ← Bt − Zt(s).

end for

if Bt ≤ 0 then

continue

end if

for state s in C 0

t do

Plan to pull (at least) X N
t

(s, 1) ← min{Bt, Zt(s), ⌊N xt(s, 1)⌋} in state s.

Store the number of undecided arms U N
t

(s) ← Z N
t

(s) − X N
t

(s, 1).

Update the remaining budget Bt ← Bt − min{Bt, Z N

t

(s), N xt(s, 1)}.

end for

for state s in C 0

t do

Plan to pull min{Bt, Ut(s)} additional undecided arms in state s.

Update X N
t

(s, 1) ← X N
t

(s, 1) + min{Bt, Ut(s)},

Update Bt ← Bt − min{Bt, Ut(s)}.

end for

For each s ∈ C +
t

∪ C 0

t , pull X N
t

(s, 1) arms in state s (as planned above). Idle all arms in C −
t .

23: end for

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

25

Policy πR behaves the same as the corresponding ﬂuid-priority policy πF with the same occu-

pation measure and priority-score function, except πR pulls all arms in the ﬂuid-active category

and idles all arms in the ﬂuid-inactive category regardless of the budget constraint. We state this

formally in the following lemma, whose proof is in the Appendix.

Lemma 6. Deﬁne event ∆t as

∆t ∶= { ∑
s∈C+
t

Z N
t

(s) ≤ αtN ≤ ∑
s∈C+
t

Z N
t

(s) + ∑
s∈C0
t

Z N
t

(s)}.

Then πR(t, Z N

t

) = πF (t, Z N

t

) on the event ∆t.

We write the complement of ∆t as ∆c

t and refer to this as a “budget violation” event.

7.2. Non-degeneracy

The non-degeneracy condition states that the ﬂuid-neutral category is not empty, which is suﬃcient

to prove the budget-violation events, ∆c

t, are probabilistically negligible. Roughly speaking, non-

emptiness of C 0

t guarantees that the occupation measure satisﬁes,

∑
s∈C+
t

zt(s) < αt < ∑
s∈C+
t

zt(s) + ∑
s∈C0
t

zt(s).

Along with both the budget-relaxed ﬂuid-priority policy and the ﬂuid-priority policy being ﬂuid

consistent, the number of arms in state s, Z N
t

(s), is roughly proportional to zt(s), with excursions

described by a central limit theorem. Thus the probability of event ∆t approaches 1 exponentially

fast with N by concentration inequalities.

We formally state this in the following deﬁnition, ﬁrst deﬁning non-degeneracy, and then stating

in Lemma 7 that non-degeneracy implies that budget violations are probabilistically negligible for

large N . The proof of Lemma 7 is in the Appendix.

Definition 3. We say an optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A is non-degenerate if

∣C 0
t

∣ ≥ 1, ∀t ∈ [T ].

Otherwise, we call it degenerate. We also call a ﬂuid-priority policy non-degenerate (degenerate)

when its associated occupation measure is non-degenerate (degenerate).

26

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Lemma 7. If an optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A is non-degenerate, then for any

priority-score functions {Pt}t∈[T ] and the corresponding ﬂuid-priority policy πF and budget-relaxed

policy πR, there exists a constant δ > 0 and a constant L such that

max{PπR

(∆c
t

), PπF

(∆c
t

)} ≤ L exp(−δN ),

for all t ∈ [T ] and all N .

Empirically, one can check the non-degeneracy condition for a given optimal occupation mea-

sure x∗ returned by solving the linear programming relaxation (4) with a commercial LP solver.

Recalling from (7) that states s in the ﬂuid-neutral category are those with both xt(s, 1) > 0 and

xt(s, 0) > 0, we check whether x∗ is non-degenerate by assessing whether there is at least one such

state for each t.

7.3. Main result

We now state and prove this section’s main result: a ﬂuid-priority policy achieves an O(1) optimality

gap when it is non-degenerate. Before that, we need one last building block: the budget-relaxed

policies’ reward deviates from the relaxed problem’s optimal reward by at most O(1) under non-

degeneracy.

We show this in the following lemma, whose proof is in the appendix. There are two main ideas

in the proof. First, recall that the budget-relaxed ﬂuid-priority policy πR pulls all arms in C +

t and

no arms in C −

t . Thus, its decisions are optimal under a Lagrangian relaxation of Problem 2 in

which the budget constraint on the expected number of arms pulled is replaced by a well-chosen

linear penalty (in this Lagrangian relaxation, ﬂuid-neutral arms does not aﬀect optimality as the

incremental reward is oﬀset by the linear penalty). Second, the fact that πR pulls a number of arms

equal to the almost-sure budget constraint with high probability ensures that it nearly satisﬁes the

constraint in Problem 2 on the expected budget. This fact causes the linear penalty to be nearly 0.

Finally, the fact that the Lagrangian relaxation is the sum of the unpenalized reward, which we call

VN (πR), and this penalty imply that the VN (πR) is within a constant of the value of Problem 2,

ˆV ∗
N .

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

27

Lemma 8. Let VN (πR) = maxπ Eπ ∑T

t=1 Rt(st, at) for a budget-relaxed ﬂuid priority policy πR. If

an optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A is non-degenerate, then for any priority-score

functions {Pt}t∈[T ], the corresponding budget-relaxed ﬂuid-priority policy πR satisﬁes

∣ ˆV ∗
N

− VN (πR)∣ ≤ m,

where m is a constant not depending on N .

Now we are ready to state and prove our main result: that a ﬂuid-priority policy achieves an

O(1) optimality gap when it is non-degenerate. A ﬂuid-priority policy πF ’s optimality gap can be

bounded by ﬁrst comparing the reward VN (πF ) with the reward of its corresponding budget-relaxed

policy πR. Combining the fact that πF deviates from πR with negligible probability (Lemma 7)

and that πR’s reward deviates O(1) from ˆV ∗

N (Lemma 8), VN (πF ) is at most O(1) away from ˆV ∗
N .

Theorem 4. If an optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A is non-degenerate, then for

any priority-score functions {Pt}t∈[T ], the corresponding ﬂuid-priority policy πF satisﬁes

ˆV ∗
N

− VN (πF ) ≤ m,

where m is a constant not depending on N .

Proof of Theorem 4 Under πF , the reward is

Under πR, the reward is

VN (πF ) = EπF

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a).

VN (πR) = EπR

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a).

Denote ΩT ∶= ∆1 ∩∆2 ∩...∩∆T . On this event, πR and πF produce identical decisions by Lemma 6.

Using this in the second line below, we have:

VN (πR) − VN (πF ) = EπR

[1ΩT

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a)] + EπR

[1Ωc

T

T
∑
t=1

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a)]

28

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

− EπF

[1ΩT

= EπR

[1Ωc

T

≤ EπR

[1Ωc

T

T
∑
t=1

T
∑
t=1

T
∑
t=1

∑
s∈S,a∈A

∑
s∈S,a∈A

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a)] − EπF

[1Ωc

T

rt(s, a)X N

t

(s, a)] − EπF

[1Ωc

T

T
∑
t=1

T
∑
t=1

∑
s∈S,a∈A

∑
s∈S,a∈A

rt(s, a)X N

t

(s, a)]

rt(s, a)X N

t

(s, a)]

∣rt(s, a)∣∣X N

t

(s, a)∣] + EπF

[1Ωc

T

T
∑
t=1

∑
s∈S,a∈A

∣rt(s, a)∣∣X N

t

(s, a)∣].

Inequalities 0 ≤ X N
t

(s) ≤ N and 0 ≤ Y N
t

(s) ≤ N then imply

VN (πR) − VN (πF ) ≤ EπR

[1Ωc

T

T
∑
t=1

∑
s∈S,a∈A

∣rt(s, a)∣N ] + EπF

[1Ωc

T

T
∑
t=1

∑
s∈S,a∈A

∣rt(s, a)∣N ]

≤ 2 (EπR

[1Ωc

T

] + EπF

[1Ωc

T

]) T ∣S∣r∗N,

where r∗ ∶= maxt,s,a ∣rt(s, a)∣.

Then, applying Lemma 7 and Pπ[Ωc

T

] ≤ ∑T

t=1

Pπ[∆c

t

], we have:

VN (πR) − VN (πF ) ≤ 2T ∣S∣r∗N

T
∑
t=1

PπR

[∆c
t

] + PπF

[∆c
t

]

≤ 4T 2∣S∣r∗N L exp(−δN ).

Finally, applying Lemma 8 concludes the proof.

◻

7.4. The best ﬂuid-priority policy is at least as good as the best index policy

Here we compare ﬂuid-priority policies against index policies (Whittle 1980, Gittins et al. 2011).

An index policy assigns each state an “index” and prioritizes each arm based on the index of its

current state from high to low, pulling arms until we exhaust the current period’s budget.

A policy can be both a ﬂuid-priority policy and index policy. This occurs if there is at most

one ﬂuid-neutral state in any period and indices of all ﬂuid-active states are higher than those of

all ﬂuid-neutral states, which are higher in turn than the indices of all ﬂuid-inactive states. There

are, however, index policies that are not ﬂuid priority policies, and vice versa. If the indices do not

respect the ordering implied by the ﬂuid-active, ﬂuid-neutral, and ﬂuid-inactive categories then

this index policy is not a ﬂuid priority policy. Also, if multiple ﬂuid-neutral states can be occupied

in one period, a ﬂuid-priority policy will allocate pulls across these arms in accordance with an

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

29

occupation measure and in a way that is diﬀerent from the strict prioritization used by an index

policy.

Since index policies are widely known and used, it is instructive to compare them with ﬂuid-

priority policies. The discussion above shows that the diﬀerence between ˆV ∗

N (the optimal objective

of the relaxation) and the value of a ﬂuid-priority policy VN (πF ) is bounded above by a constant

when πF is non-degenerate, i.e., that supN

ˆV ∗
N

− VN (πF ) is ﬁnite. The following proposition shows

that the best ﬂuid priority is at least as good as the best index policy, when measured by supN

ˆV ∗
N

−

VN (π) ∈ R ∪ {∞}, regardless of whether non-degeneracy holds.

Proposition 1. Consider an index policy πI such that supN

ˆV ∗
N

− VN (πI) < ∞. Then, there exists

a ﬂuid priority policy πF such that

ˆV ∗
N

sup
N

− VN (πF ) ≤ sup

N

ˆV ∗
N

− VN (πI).

The proof of Proposition 1 is in the Appendix.

7.5. Choice of Occupation Measure

The above discussion of ﬂuid-priority policies and degeneracy applies to any optimal occupation

measure. Multiple optimal occupation measures may exist, some degenerate and others not. In this

situation, a ﬂuid-priority policy constructed using a non-degenerate optimal occupation measure

is guaranteed to have an O(1) optimality gap while another constructed using a degenerate one

is not. A natural question then arises: how can we determine whether a non-degenerate optimal

occupation measure exists and how can we select one if it does? Here we describe a computational

procedure for answering this question.

First, observe from (4) that any convex combination of optimal occupation measures is also

optimal. Thus, suppose we can ﬁnd a collection of optimal occupation measures, x∗,k, k ∈ [K], such

that, for each t, there is either (1) a state s that is ﬂuid-neutral under some k, or (2) there is

a state s that is ﬂuid-active under some k and ﬂuid-inactive under another k. Then any convex

30

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

combination with strictly positive weight on each k is non-degenerate. We describe an algorithm

for ﬁnding such a collection, if it exists, or establishing that it does not.

To accomplish this, ﬁrst solve the LP (4), call the solution x∗,1, and record its optimal value for

later use. Assess for each t whether there is a state s satisfying x∗
t

(s, 0) > 0 and x∗
t

(s, 1) > 0. If all t

satisfy this condition, then we have found a non-degenerate optimal occupation measure.

Otherwise, we will continue iteratively in our search. In each stage k, we will maintain a collection

of solutions {x∗,k′ ∶ k′ = 1, . . . , k} and a set of times Ak ⊆ [T ]. Ak contains those times for which we

have not yet been able to construct a ﬂuid-neutral state. Formally, a time t is in Ak if and only

one of the following holds: (1) all states are ﬂuid-active at t in all x∗,k′

, k′ ≤ k; or (2) all states

are ﬂuid-inactive at t in all x∗,k′

, k′ ≤ k. If Ak is empty, then a non-degenerate optimal occupation

measure can be constructed as a convex combination of {x∗,k′ ∶ k′ = 1, . . . , k} using strictly positive

weights on every solution in this collection. If Ak is not empty, we will then attempt to construct

an optimal occupation measure that, when added to our collection of solutions, causes Ak+1 to be

a strict subset of Ak.

Toward this goal, in stage k, choose tk ∈ Ak. This will be the time that we seek to remove from Ak

in constructing Ak+1. Let C +,k contain all of the states for which x∗,k′
tk

(s, 1) > 0 and x∗,k′
tk

(s, 0) = 0 for

all k′ ≤ k. These are the states that are ﬂuid-active at time tk for all previously computed optimal

occupation measures. Then solve a linear program minimizing ∑s∈C+,k xtk

(s, 1) subject to all of the

constraints in (4) and the linear constraint that the objective in (4) is equal to its optimal value

recorded above. Call the solution x∗,k+1.

This linear program assesses whether there is an optimal occupation measure x∗,k+1 satisfying

∑s∈C+,k xtk

(s, 1) < αk. If no such x∗,k+1 exists, then this establishes that all optimal occupation

measures are degenerate. Otherwise, if we ﬁnd such a x∗,k+1, then we add it to our collection of

solutions. We also construct Ak+1 by removing the time tk from Ak. We additionally remove any

other times t for which the new solution x∗,k+1 provides a state whose category at that time t is

diﬀerent from those in the previous solutions x∗,k′

, k′ ≤ k.

If Ak+1 is the empty set, then this implies that there is a non-degenerate optimal occupation

measure. We set K = k + 1 and construct it as described above from the collection {x∗,k′ ∶ k ≤ K}.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

31

8. Numerical Experiments

This section evaluates the performance of ﬂuid-priority and other policies on three problems, lever-

aging both simulation experiments, computational investigations and our earlier theoretical results.

§8.1 studies a classical problem: the ﬁnite-horizon Bayesian bandit with Bernoulli rewards. It

compares ﬂuid-priority policies against the widely used UCB and Thompson Sampling policies.

Fluid-priority policies are ﬁrst shown to substantially outperform both methods in numerical exper-

iments. We then show that the optimality gap for UCB and Thompson sampling is Ω(N ), while

it is O(1) for the ﬂuid-priority policy evaluated as it is non-degenerate in this problem.

§8.2 considers an active learning problem based on Chen et al. (2013) in which an algorithm

allocates crowd workers (e.g., participants on Amazon’s Mechanical Turk) to image labeling tasks

to support learning an accurate classiﬁer. We show via numerical experiments that ﬂuid-priority

policies outperform a previously proposed state-of-the-art policy (Optimistic Knowledge Gradient,

Chen et al. 2013) speciﬁcally designed for this problem.

§8.3 shows via direct computation that the dynamic assortment problem previously studied in

Brown and Smith (2020) satisﬁes the non-degeneracy condition. This and our main theoretical

result shows that ﬂuid-priority policies have an O(1) optimality gap, explaining the hitherto poorly

understood performance of Lagrangian index policies ﬁrst noticed in numerical experiments in

Brown and Smith (2020).

8.1. Bayesian bandit with Bernoulli rewards

This section evaluates ﬂuid-priority policies performance on the Bayesian bandit problems, which

is a standard benchmark in the bandit literature. While the problem is not restless, and so does not

demand the full capabilities of our proposed ﬂuid-priority policies, it allows us to study benchmarks

designed for non-restless settings. Problems with restless arms are studied later.

We compare the performance of ﬂuid-priorities against Upper Conﬁdence Bound (UCB, Agrawal

1995) and Thompson Sampling (TS, Agrawal and Goyal 2012) policies and show that the ﬂuid-

priority policy achieves an O(1) optimality gap while the optimality gaps of both UCB and TS

32

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

grow linearly with the number of arms. While UCB and TS are well-known for having a logarithmic

asymptotic performance guarantee of O(N log(T )), this is linear in N . (It also applies to a slightly

diﬀerent problem setting than the one we study here: a stochastic frequentist setting with one pull

period and where regret is measured with respect to the policy that pulls the best arm.) Thus, the

classical regret guarantee for these policies is not inconsistent with our ﬁnding that these policies

have a Ω(N ) optimality gap in a Bayesian analysis with multiple pulls per period.

This suggests that when T is small and N is large, and where prior information supports the

use of a Bayesian analysis, there is signiﬁcant value in using ﬂuid priority policies over UCB or TS,

and in using a Bayesian ﬁnite-horizon analysis rather than a stochastic frequentist analysis.

Problem Setup: There are N arms in total, of which we may pull at most ⌊N /3⌋ in each of T

periods. Before any arms are pulled, each arm i has a parameter θi sampled independently from the

Bayesian prior distribution on the arm’s reward. This prior distribution is uniform with support

[0, 1]. Then, conditioning on θi, each arm i’s rewards are generated when pulled as conditionally

independent Bernoulli random variables with a common parameter θi. Our objective is to maximize

the expected total reward collected across all periods.

This problem is similar to the more widely-studied stochastic bandit, except that the arm’s

reward is drawn at random from the prior. The expected reward calculated can be understood

as the average-case reward over stochastic bandit problem instances, i.e., over (θi ∶ i), where the

weight on a particular instance (θi ∶ i) is proportional to its density under the prior.

The non-degeneracy condition holds in this problem for both horizons T = 15 and T = 20. We

veriﬁed this numerically by solving the linear program (4) and conﬁrming that there is at least one

state with a strictly positive occupation measure in the ﬂuid-neutral category in each period.

Policy Implementation: We brieﬂy discuss how we implement UCB, TS and ﬂuid-priority

policies in these experiments.

UCB tracks the posterior belief on θi for each arm i based on the arm’s past reward realization,

and calculates an upper conﬁdence bound for θi as µi + δσi, where δ is a ﬁxed parameter, µi is the

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

33

mean of the posterior belief and σi is the standard deviation. The top ⌊N /3⌋ arms ranked by their

upper conﬁdence bound are selected to be pulled. We run UCB with δ varying from 0.1 to 1 and

report results for the one with the best expected reward (δ∗ = 0.5) in both experiments.

TS also tracks the posterior belief on θi for each arm i. At each period, TS samples a value from

each arm’s posterior belief on θi, then pulls the ⌊N /3⌋ arms with the highest sampled values.

The ﬂuid-priority policy is constructed as follows. First, to construct the optimal occupation

measure, we solve the relaxed problem (4) and fetch its solution. Second, to construct the priority-

score function, we use a Lagrangian-relaxation approach similar to Hu and Frazier (2017) and

Brown and Smith (2020): we solve the min-max problem

(λ∗

1, ..., λ∗

T

) ← min
λ1,...,λT

max
π

Eπ

T
∑
t=1

rt(st, at) + λt(αt − at),

(8)

where the inner max can be solved via dynamic programming and the outer min can be solved

via the subgradient method. Then we compute the Q−function based on the optimal Lagrangian

multiplier (λ∗

1, λ∗

2, ..., λ∗

T

) iteratively:

Qt(s, a) = rt(s, a) − λta + ∑
s′

pt(s, a, s′) max

a′

Qt+1(s′, a′) for 0 ≤ t ≤ T − 1,

with QT (s, a) = rT (s, a)−λT a, and construct the priority-score function as Pt(s) = Qt(s, 1)−Qt(s, 0).

Finally, we plug the optimal occupation measure and the score-function into Algorithm 1 to con-

struct the ﬂuid-priority policy.

Numerical Experiments: We compare the just-described ﬂuid-priority policy against UCB

and TS using two diﬀerent time horizons T of 15 and 20. Figure 1a displays results for T = 15

while Figure 1b shows results for T = 20. In both experiments, we iteratively double the number

of arms (from N = 300 to 38400) and plot an upper bound on the optimality gap. This bound on

the optimality gap is computed by ﬁrst computing the value of the relaxed problem ˆV ∗

N (which is

an upper bound on the value of the optimal policy) and then subtracting the value of the UCB,

TS or ﬂuid-priority policy in question estimated via simulation. We compare this upper bound

across policies instead of the exact optimality gap because computing the exact optimality gap

34

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

would require knowing the value of the optimal policy, which would take time exponential in N ,

as discussed in §3. We use 50N replications to estimate a policy’s value when there are N arms.

We use more samples when there are more arms because having more arms increases the variance

of a policy’s reward. We also compute a conﬁdence interval on this upper bound, computed as

the diﬀerence between ˆV ∗

N and the upper and lower limits of a conﬁdence interval on the policy’s

expected reward.

Figure 1

Bayesian bandit with Bernoulli rewards. An upper bound on the optimality gap (relaxed problem’s

(a) T = 15

(b) T = 20

expected total reward minus a simulation-based estimate of reward) vs number of arms N , for the ﬁnite-

horizon Bayesian multi-armed bandit with horizons T = 15 (left) and T = 20 (right). The ﬂuid-priority

policy has its optimality gap bounded above by a constant while UCB and Thompson sampling have

optimality gaps that grow linearly with the number of arms.

Figure 1 compares the performance of ﬂuid-priority, UCB and TS policies. For both time horizons

T of 15 and 20, the ﬂuid-priority policy performs signiﬁcantly better than UCB and TS, especially

for large N . The ﬂuid-priority policy’s reward diﬀers from the optimal policy’s reward by at most

1 for T = 15 and at most 2 for T = 20 even when there are 38400 arms available. UCB outperforms

TS in both time horizons, perhaps due to the tuning of UCB’s hyperparameter.

These results are consistent with Theorem 4 and our numerical validation that the non-

degeneracy condition is satisﬁed, which implies that the ﬂuid-priority policy’s optimality gap is

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

35

bounded across all values of N . In contrast, the optimality gap for both UCB and Thompson

sampling grows with N as shown in Figure 1.

Proposition 2 provides an additional analysis to conﬁrm theoretically that UCB and Thompson

sampling have optimality gaps that grow linearly in N . The proof of this proposition, which may

be found in the appendix, deﬁnes an iterative algorithm over t to calculate the occupation measure

for UCB and TS in the large N limit. We then use this algorithm to compute this occupation

measure for speciﬁc values for T and compare it to the optimal occupation measure. We ﬁnd that

the occupation measures are suboptimal for the values of T used in these experiments, implying

that UCB and TS are not ﬂuid-consistent and their optimality gaps are Ω(N ). These values of T

are representative, and UCB and TS have Ω(N ) optimality gaps for other T as well.

Proposition 2. The optimality gap for both UCB and TS is Ω(N ) for T = 15 and T = 20.

8.2. Crowdsourced Labeling

This section evaluates a ﬂuid-priority policy’s performance on an active learning problem intro-

duced by Chen et al. (2013) focused on the allocation of crowd workers for accurate image classiﬁca-

tion. We compare its performance against the Optimistic Knowledge-Gradient (Chen et al. 2013),

a method speciﬁcally designed for this problem, and the Online Knowledge-Gradient (Ryzhov et al.

2012). The ﬂuid-priority policy outperforms both methods signiﬁcantly.

We formulate the crowdsourced labeling problem as follows. Suppose there are N images needing

binary labels (e.g., whether this is a picture of a pedestrain or not) to support training of an

automatic image classiﬁer that will be built later. We ask crowd workers to label these images.

This approach to “jump starting” machine learning classiﬁers with labels from crowd workers is

common in indutry (Chen et al. 2013). Each image i has a true underlying binary class, along

with an associated probability pi that a crowd worker will label the image with the correct class.

A crowd worker may provide an incorrect label because, e.g., the image is blurry or the worker

is distracted. We assume pi > 1/2 (following Chen et al. 2013), i.e. the majority of crowd workers

36

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

give the correct label. We use an independent prior belief for each image’s pi ∼ U [1/2, 1]. We are

allowed to request T = 7 batches of labels from crowd workers, with up to ⌊N /4⌋ images per batch.

After the last batch, we estimate each image’s class via majority vote, which is also the class with

maximum probability under posterior.

Figure 2 compares the ﬂuid-priority policy against the Online Knowledge-Gradient and Opti-

mistic Knowledge-Gradient methods as we vary the number of arms N , reporting an upper bound

on the optimality gap for each policy computed in the same way as Section 8.1. The ﬂuid-priority

policy seems to perform extremely well, and incorrectly classiﬁes at most 1 more image on average

than the optimal policy even when there are 1000 images’ labels to be learned. Online Knowledge-

Gradient and Optimistic Knowledge-Gradient perform similarly in our experiment, and they both

underperform the ﬂuid-priority policy by wrongly classifying at least 8 more images on average with

1000 images’ labels to be learned. Even though the optimistic knowledge-gradient was designed

speciﬁcally for this problem, the ﬂuid-priority policy nevertheless has a signiﬁcantly smaller opti-

mality gap.

Figure 2 is consistent with our theoretical results. We can verify the non-degeneracy condition

does not hold in this example by implementing the algorithm in §7.5. The lack of non-degeneracy

implies that the optimality gap of the ﬂuid-priority policy is O(

√

N ). Its performance in the plot

is consistent with this scaling. The Online Knowledge Gradient and the Optimistic Knowledge

Gradient, however, seem to have suboptimality that scales linearly with N .

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

37

Figure 2

Crowdsourced labeling. An upper bound on the optimality gap (relaxed problem’s expected total reward

(a)

(b)

minus a simulation-based estimate of reward) vs. number of arms N . The left and right panel show the

same data but using diﬀerent scales for the y-axis. Both Knowledge Gradient and Optimistic Knowledge

Gradient have optimality gaps that seem to grow linearly. The ﬂuid-priority policy has an optimality

gap that is O(

√

N ) because the non-degeneracy condition does not hold in this problem.

8.3. Dynamic Assortment Optimization

This section discusses a dynamic assortment optimization problem proposed in Section 6.2.1 in

Brown and Smith (2020). In this problem, Brown and Smith (2020) observes empirically that the

optimality gap of the policy proposed in that paper (a so-called Lagrangian policy), shown there

√

to be O(

N ), seems to stay constant with N , suggesting that the O(

√

N ) bound is loose. We

ﬁrst describe the problem setting and then conﬁrm that our theoretical results provide the tighter

bound suggested by these empirical results.

A retailer repeatedly chooses products to display in a selling season. The retailer has N products

but a shelf-space constraint allows only showing ⌊N /4⌋ of them in each time period. Each product,

if sold, generates proﬁt of $1. The demand rate for each product i is unknown to the retailer but

follows a Poisson process with intensity γi. The retailer holds a Bayesian prior belief on γi, which is

Gamma-distributed with shape parameter mi and inverse scale parameter ai, γi ∼ Gamma(mi, ai).

38

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

All products share the same prior belief (mi, ai) = (1, 0.1). The retailer updates these prior beliefs

after observing demand realizations for displayed products using Bayes rule.

The Gamma distribution is a conjugate prior distribution when we have Poisson-distributed

observations, which causes the posterior to remain Gamma-distributed. More speciﬁcally, the pos-

terior on γi in time period t is Gamma(mt,i, at,i) where mt,i and at,i can be computed recursively.

For a product i that was displayed in time period t, letting xt,i be the demand for the product

in the period, mt+1,i = mt,i + xt,i and at+1,i = at,i + 1. For a product i that was not displayed in t,

mt+1,i = mt,i and at+1,i = at,i. At t = 0, m0,i = 1 and a0,i = 0.1.

The retailer’s objective is to adaptively choose which products to display in each period subject

to the shelf-space constraint to maximize the expected total proﬁt over a selling season lasting

T periods. This is formulated as a restless bandit with time horizon T where each product i is

an arm whose state at time t is (mt,i, at,i). The optimal policy and good approximate policies

must balance exploration and exploitation by showing products that observed sales and the prior

suggest have large γi (exploitation) and also showing those for which we have little observed sales

data to support learning γi (exploration). This must be done cognizant of the time horizon T :

as the remaining time in the selling season shrinks, exploration becomes unimportant. This must

also be done while leveraging the prior, especially when T is small and observed sales alone leave

substantial uncertainty.

Brown and Smith (2020) study performance of their proposed Lagrangian policy when T = 8.

They ﬁnd their policy “perform(s) very well for large N ”, and produces proﬁt “within $6 of

the optimal value!” when N = 16, 384. They do not, however, oﬀer an explanation for why the

performance would be so good for a policy with an O(

√

N ) optimality gap, the tightest bound

known at the time.

Our results explain this phenomenon. First, by solving the linear programming relaxation (4)

for this problem, we conﬁrm that the set of ﬂuid-neutral states is non-empty in each time period,

thus conﬁrming that the problem is non-degenerate. Moreover, there is exactly one state in each

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

39

period’s ﬂuid-priority category. This is also observed by Brown and Smith (2020), as they mention

that “there are no scenarios where products in diﬀerent states have the same priority indices”.

Thus, for this optimal occupation measure, ﬂuid-priority policies are also index policies and the

Lagrangian policy is one speciﬁc example. This explains why the Lagrangian policy achieves an

O(1) optimality gap.

9. Conclusion

In this work, we have developed ﬂuid-priority policies, a class of new policies with strong theoretical

guarantees and numerical performance for Bayesian restless bandits with many arms. In the limit

as the number of arms N grows large, ﬂuid-priority policies always achieve an O(

√

N ) optimality

gap, matching the best existing scaling in the past literature. When the non-degeneracy condition

holds, surprisingly ﬂuid-priority policies achieve an O(1) optimality gap.

Although our analysis is speciﬁc to Bayesian restless bandits , our general approach may support

analysis of policies based on ﬂuid approximations in other areas. Fluid approximations and policies

based on them are a widely used tool in revenue management (Dai et al. 2019), inventory control

(Kunnumkal and Topaloglu 2011) and other areas. They are used, in particular, in Weakly Coupled

Markov Decision Processes, which generalize restless bandits by allowing multiple actions for each

arm and multiple resource constraints. As our analysis of Bayesian restless bandits only leverages

a ﬂuid-approximation method and a concentration property in a many-arm regime, our results

are likely generalizable to problem formulations from these other areas under a similar asymptotic

regime. We also believe it is possible to extend our work to inﬁnite-horizon problems, and to

incorporate correlated randomness across resource constraints, e.g., through resource constraint

bounds given not by deterministic values but by a Markov process that is common across arms.

In summary, we feel that the technique demonstrated here of concentration-inequality analysis of

deviations from ﬂuid policies is a powerful technique that can be broadly applied.

40

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

10. Appendix

This section provides all technical proofs not included in the main paper.

10.1. Proof for Lemma 1

In the original formulation of the restless bandit, problem (1), the budget constraint ∣at∣ = ⌊αtN ⌋

applies on each sample path. The relaxed problem (2) is identical except that this constraint is

replaced by the weaker one, E∣at∣ = αtN . Recalling our assumption here that αtN is an integer, the

right-hand sides of these two constraints are the same. (Generalizations to non-integer αtN are

discussed in Appendix 10.2). Thus, the set of feasible policies in (1) is a subset of those in (2),

implying that the value of (1) is bounded above by that of (2), i.e.,

V ∗
N

≤ ˆV ∗
N .

(9)

To prove ˆV ∗(N ) = N ˆV ∗(1), we use a Lagrangian Relaxation similar to Farias and Madan (2011),

Guha and Munagala (2008) as the key idea in the following argument.

Through straightforward imitation of the proof of the Fenchel Duality Theorem (Rockafellar

1970),

max
π

min
λ1∶T

Eπ

T
∑
t=1

Rt(st, at) + λt(αtN − ∣at∣) = min
λ1∶T

max
π

Eπ

T
∑
t=1

Rt(st, at) + λt(αtN − ∣at∣).

(10)

In this use of the Fenchel Duality Theorem, we note that maximization over policies π on the right-

hand side of (10) with ﬁxed λ1∶T can be viewed as as a linear program. More detailed discussion

of this standard result can be found in Brown and Smith (2020).

The left-hand side of Equation (10) equals ˆV ∗(N ). On the right hand side, for ﬁxed λ1∶T ,

Eπ

T
∑
t=1

Rt(st, at) + λt(αtN − ∣at∣) = Eπ

T
∑
t=1

N
∑
i=1

rt(st,i, at,i) + λt(αt − at,i).

Since all arms share the same transition kernel, reward function, and distribution over initial state,

Eπ

T
∑
t=1

N
∑
i=1

rt(st,i, at,i) + λt(αt − at,i) = N Eπ

T
∑
t=1

rt(st,1, at,1) + λt(αt − at,1).

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

41

So we conclude

min
λ1∶T

max
π

Eπ

T
∑
t=1

Rt(st, at) + λt(αtN − ∣at∣) = N min
λ1∶T

max
π

Eπ

T
∑
t=1

rt(st,1, at,1) + λt(αt − at,1).

(11)

By using Fenchel Duality again on the one-arm problem,

min
λ1∶T

max
π

Eπ

T
∑
t=1

rt(st,1, at,1) + λt(αt − at,1) = max

π

Eπ

min
λ1∶T

T
∑
t=1

rt(st,1, at,1) + λt(αt − at,1)

= ˆV ∗(1).

(12)

Summarizing, equations (10), (11) and (12) together imply,

ˆV ∗(N ) = N ˆV ∗(1).

10.2. Discussion of the rounding error in budget constraints and initial states

The original problem (1) constrains the number of pulls to ⌊αtN ⌋ (almost surely), while the relaxed

problem (2) constrains this number to αtN (in expectation). We think of these diﬀerences as

“rounding errors” in the relaxed problem. Here we discuss their eﬀect and show that they result

in at most a constant diﬀerence in the optimal objective value.

Mathematically speaking, denote

ˆV ∗
N

ˆV ∗
N,R

T
∑
t=1

T
∑
t=1

= max
π

Eπ

⎧⎪⎪
⎨
⎪⎪⎩
⎧⎪⎪
Eπ
⎨
⎪⎪⎩
− ˆV ∗

N,R

= max
π

Rt(st, at)

E∣at∣ = αtN, E

Rt(st, at)

E∣at∣ = ⌊αtN ⌋, E

N
∑
i=1

1(s1,i = s∗) = N, ∀t ∈ [T ]

⎫⎪⎪
⎬
⎪⎪⎭

,

N
∑
i=1

1(s1,i = s∗) = N, ∀t ∈ [T ]

⎫⎪⎪
⎬
⎪⎪⎭

.

RRRRRRRRRRR
RRRRRRRRRRR

We claim that ∣ ˆV ∗
N

∣ ≤ c, where c does not depend on N . The theoretical analysis through

the rest of the paper after Lemma 1 compares policy performance against ˆV ∗

N and shows that this

diﬀerence is o(N ), O(

√

N ), or O(1) depending on conditions. The fact that ˆV ∗

N is separated from

ˆVN,R by at most a constant then implies that the diﬀerence in policy performance compared to ˆV ∗

N,R

has the same asymptotic dependence on N . This and the fact that ˆVN,R is an upper bound on (1)

even when αtN are not integers provides optimality gaps of o(N ), O(

√

N ) or O(1) respectively.

The proof of the claim that ∣ ˆV ∗
N

− ˆV ∗

N,R

∣ ≤ c is straightforward. As seen from Lemma 1, there exists

a single-arm strategy that pulls αt arms per period in expectation and achieves objective value

42

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

⌉ arms according to this strategy and pull each remaining arm

with probability

ˆV ∗
1 . Thus, we can pull N − maxt ⌈ 1
αt
⌊αtN ⌋−αt(N −maxt ⌈ 1
αt
maxt ⌈ 1
αt
N − maxt ⌈ 1
αt
N

⌉)

⌉

∈ [0, 1] at period t. Thus, we show

⌉

ˆV ∗
N

− ˆV ∗

N,R

≤ T max
s,a,t

rt(s, a).

Similarly, we can show

N − maxt ⌈ 1
αt
N

⌉

ˆV ∗
N,R

− ˆV ∗
N

≤ T max
s,a,t

rt(s, a).

Combining the above two inequalities with the fact that ˆV ∗

N,R

/N and ˆV ∗
N

/N are both uniformly

bounded by T maxs,a,t ∣rt(s, a)∣ concludes the statement with c = T (1 + maxt ⌈ 1
αt

⌉) maxs,a,t ∣rt(s, a)∣.

10.3. Proof of Lemma 2

We prove Lemma 2 by induction on t. When t = 1, that all arms are in state s∗ implies ZN

1
N

→ z1.

Then, by the deﬁnition of ﬂuid consistency, XN

1
N

→ x1. Thus Lemma 2 holds for t = 1.

Now assume Lemma 2 holds for t, and we will show it holds for t + 1. By the deﬁnition of ﬂuid

consistency, we only need to prove

Recalling our system dynamics,

Z N
t+1
N

→ zt+1.

Z N
t+1

(s) = ∑

s′∈S,a∈A

N
∑
i=1

1(st,i = s′, at,i = a, st+1,i = s),

(13)

(14)

where 1(st,i = s′, at,i = a, st+1,i = s) is the indicator function of the event {st,i = s′, at,i = a, st+1,i = s},

we only need to show that

1
N

N
∑
i=1

1(st,i = s′, at,i = a, st+1,i = s) → xt(s′, a)p(s′, a, s).

(15)

since the sum over s′ of the right-hand side is ∑s′∈S xt(s′, a)p(s′, a, s) = zt+1(s).

If xt(s′, a) > 0, then X N

t

(s′, a) → ∞ as N → ∞ by the induction hypothesis. Thus as N → ∞,

1(st,i = s′, at,i = a, st+1,i = s) = X N

t

1
N

N
∑
i=1

(s′, a)
N

1
(s′, a)

N
∑
i=1

X N
t

1(st,i = s′, at,i = a, st+1,i = s)

→ xt(s′, a)p(s′, a, s),

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

43

by the deﬁnition of ﬂuid consistency and the strong law of large numbers.

If xt(s′, a) = 0,

1
N

N
∑
i=1

1(st,i = s′, at,i = a, st+1,i = s) ≤ X N

t

(s′, a)
N

→ 0.

Combining the cases xt(s′, a) > 0 and xt(s′, a) = 0, equation (15) is shown.

To summarize,

(s)

Z N
t+1
N

= ∑
s′∈S

1
N

N
∑
i=1

1(st,i = s′, at,i = a, st+1,i = s) → ∑
s′∈S

∑
a∈A

xt(s′, a)p(s′, a, s) = zt+1(s).

10.4. Proof of Lemma 3

This section proves Lemma 3. When Z N
t

/N → zt, we have

˜Z N
t√
N

= Z N

t

− N zt
N

→ 0.

To show Xt
N

→ xt, it is equivalent to show

)

πt,N ( ˜Z N
√
N

t

→ 0.

Notice by Conditions 1 and 2 in Deﬁnition 2,

∣πt,N ( ˜Z N

t

)∣ ≤ ∣πt,N (0)∣ + C1∣ ˜Z N

t

∣ ≤ C2 + C1∣ ˜Z N

t

∣.

Thus,

10.5. Proof of Lemma 4

)

πt,N ( ˜Z N
√
N

t

≤ C2√
N

+ C1

∣

∣ ˜Z N
√
t
N

→ 0.

This section proves Lemma 4. To begin with, we ﬁrst state and prove Lemma 9.

Lemma 9. If a policy π is diﬀusion regular and ˜Z N
t

→ ˜Z ∞
t

in distribution, then ˜X N
t

→ ˜X ∞
t

in

distribution for some random variable ˜X ∞
t .

44

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Proof of Lemma 9 By the Skorokhod representation Theorem, there exists a probability space

(Ω, P) and a sequence of random variables { ̃Z N

t

}N and ̃Z ∞

t

such that

̃Z N

t

= ˜Z N

t and ̃Z ∞

t

= ˜Z ∞
t

in distribution,

̃Z N

t

→ ̃Z ∞

t as N → ∞ a.s.

We will prove convergence in distribution of ˜X N
t

= ˜πt,N ( ̃Z N

t

) to ˜X ∞
t

∶= ˜πt,∞( ̃Z ∞

t

). Notice

∣˜πt,N ( ̃Z N

t

) − ˜πt,∞( ̃Z ∞

t

)∣ ≤ ∣˜πt,N ( ̃Z N

t

) − ˜πt,N ( ̃Z ∞

t

)∣ + ∣˜πt,N ( ̃Z ∞

t

) − ˜πt,∞( ̃Z ∞

t

)∣

≤ C1∣ ̃Z N

t

− ̃Z ∞

t

∣ + ∣˜πt,N ( ̃Z ∞

t

) − ˜πt,∞( ̃Z ∞

t

)∣,

which converges to 0 as N → ∞ by almost sure convergence of ̃Z N

t

to ̃Z ∞

t and the convergence of

˜πt,N to ˜πt,∞ required by the fact that π is diﬀusion regular.

◻

Proof of Lemma 4 We prove Lemma 4 by induction on t. When t = 1, ˜Z N
1

= 0 implying ˜Z ∞
1

= 0.

Then, according to Lemma 9, we know there exists a constant vector ˜X ∞
1

s.t. ˜X N
1

→ ˜X ∞

1 . Thus,

Lemma 4 holds true for t = 1.

Now assume Lemma 4 holds for t and we will prove it holds for t + 1. It is suﬃcient to prove

there exists a sub-Gaussian random vector ˜Z ∞

t+1 s.t. ˜Z N

t+1

→ ˜Z ∞

t+1 in distribution. This is because (1)

existence of the limit ˜X ∞

t+1 follows from Lemma 9 and (2) showing ˜Z ∞

t+1 is sub-Gaussian implies

˜X ∞

t+1 is sub-Gaussian because

∣ ˜X ∞
t+1

∣ = ∣˜πt,∞( ˜Z ∞

t+1

)∣ ≤ ∣˜πt,∞( ˜Z ∞

t+1

) − ˜πt,∞(0)∣ + ∣˜πt,∞(0)∣ ≤ C1∣ ˜Z ∞

t+1

∣ + C2.

We prove the existence of ˜Z ∞

t+1 by constructing an explicit formula for this limit,

˜Z N
t+1

→ ˜Z ∞
t+1

∶= Mt + ∑
s′∈S

∑
a∈A

p(s′, a, ⋅) ˜X ∞
t

(s′, a)

(16)

where Mt ∼ N (0, Σt) is independent of ˜X ∞

t

(s′, a). The covariance matrix Σt is deﬁned as

Σt(s′′, s′′′) = ∑
s′

∑
a∈A

xt(s′, a)Cov[1(st+1,1 = s′′), 1(st+1,1 = s′′′)∣st,1 = s′, at,1 = a]

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

45

where Cov[1(st+1,1 = s′′), 1(st+1,1 = s′′′)∣st = s′, at = a] is the conditional covariance of the indicators

of events {st+1,1 = s′′} and {st+1,1 = s′′′} given st,1 = s′, at,1 = a.

Once (16) is shown, then the fact that ˜Z ∞

t+1 is sub-Gaussian follows because Mt and ˜X ∞

t are both

sub-Gaussian.

To prove (16), by our system dynamics (14) with a vector form,

Z N
t+1

= ∑

s′∈S,a∈A

BN
t

(s′, a),

(17)

where the BN
t

(s′, a) are conditionally independent (across s′ and a) multinomial distributions with

parameters X N
t

(s′, a) and p(s′, a) ∶= (p(s′, a, s))s∈S, i.e.,

BN
t

(s′, a) ∣ X N
t

∼ Multinomial(X N
t

(s′, a), p(s′, a)).

BN
t

(s′, a) is a vector counting the number of arms in each state, among those arms that were

previously in state s′ and for which we used action a.

Recall that X N
t

(s′, a) can be decomposed as N xt(s′, a) +

√

N ˜X N
t

(s′, a). According to Lemma 10,

there exists two random variables C N
t

(s′, a) and ∆N
t

(s′, a), such that

BN
t

(s′, a) = C N
t

(s′, a) + ∆N
t

(s′, a),

(18)

and that, conditionally on X N
t

(s′, a), have marginal distributions:

C N
t

(s′, a) ∣ X N
t

∼ Multinomial(N xt(s′, a), p(s′, a)),

∆N
t

(s′, a) ∣ X N
t

∼ sgn( ˜X N
t

(s′, a))Multinomial(

√

N ∣ ˜X N
t

(s′, a)∣ , p(s′, a)).

By (17), (18) and the deﬁnition of our diﬀusion statistic ˜Z N

t+1 in terms of Z N

t+1,

˜Z N
t+1

= 1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N + ∆N

t

(s′, a).

By Lemma 11,

1√
N

∆N
t

(s′, a) − p(s′, a) ˜X N
t

(s′, a) → 0.

46

Thus,

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

˜Z N
t+1

=

=

⎡
⎢
⎢
⎢
⎣
⎡
⎢
⎢
⎢
⎣

1√
N
1√
N

∑
s′∈S,a∈A

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N

C N
t

(s′, a) − xt(s′, a)p(s′, a)N

⎤
⎥
⎥
⎥
⎦
⎤
⎥
⎥
⎥
⎦

+

+

⎡
⎢
⎢
∑
⎢
⎣
s′∈S,a∈A
⎡
⎢
⎢
∑
⎢
⎣
s′∈S,a∈A

p(s′, a) ˜X N
t

p(s′, a) ˜X ∞
t

⎤
⎥
⎥
(s′, a)
⎥
⎦
⎤
⎥
⎥
(s′, a)
⎥
⎦

+ (cid:15)N

+ (cid:15)N + (cid:15)′

N ,

where (cid:15)N , (cid:15)′
N

→ 0.

The ﬁrst term satisﬁes

1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N → N (0, Σt),

where Σt is deﬁned above. We deﬁne Mt to be equal to this limit. This shows (16) as claimed.

Although it is not needed for the proof, we observe that because ˜X ∞

t was constructed to be equal

only in distribution to limN

˜X N

t , we are free to construct it so that it is independent of Mt.

To summarize, we have shown ˜Z N
t+1

→ ˜Z ∞

t+1 in distribution.

◻

Here we give the statement and proof of Lemma 10 and 11.

Lemma 10. Let Y ∼ Multinomial(n, p). Then for a given non-negative integer m, there exist ran-

dom vectors Y1 and Y2 such that Y = Y1 + Y2 and

Y1 ∼ Multinomial(m, p), Y2 ∼ sgn(n − m)Multinomial(∣n − m∣, p),

where sgn(⋅) is the sign function.

Proof of Lemma 10 There exists a sequence of i.i.d random vectors Xi ∼ Multinomial(1, p) s.t.

Y =

n
∑
i=1

Xi.

If n > m, taking Y1 = ∑m

i=1 Xi, Y2 = ∑n

j=m+1 Xj concludes the proof. If n ≤ m, taking Y1 = ∑m

i=1 Xi, Y2 =

− ∑m

j=n+1 Xj concludes the proof.

◻

Lemma 11. Consider a sequence of random variables X1, X2, ..., XN , ... converging to X∞ in

distribution and a sequence of i.i.d Bernoulli random variable B1, B2, ..., Bn, .... with E[B1] = p

that are also independent of sequence X1, X2, .... Then deﬁne

Then YN → 0.

YN = 1√
N

√

N

XN

∑
n=1

(Bn − p).

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

47

Proof of Lemma 11 We calculate the characteristic function of YN as follows:

E[exp(iλYN )] = E[E[exp(iλYN )∣XN ]]

1√
N

(Bn − p))∣XN ]]

)∣XN ]XN

√

N ]

= E[E[exp(iλ

= E[E[exp(iλ

= E[(p exp(iλ

√

N

XN

∑
n=1

B1 − p√
N
1 − p√
N

) + (1 − p) exp(−iλ

))XN

√

N ].

p√
N

We have

(p exp(iλ

) + (1 − p) exp(−iλ

√

N

))XN

p√
N

1 − p√
N
1 − p√
N

= (p(1 + iλ

+ O( 1
N

)) + (1 − p)(1 − iλ

p√
N

+ O( 1
N

)))XN

√

N

= (1 + O( 1
N

√

))XN

N → 1, as N → ∞.

We would like to then argue that this almost sure convergence implies convergence of the expecta-

tions as well, i.e., that E[exp(iλYN )] converges to 1. To show this we use the dominated convergence

theorem and the following bound:

∣p exp(iλ

1 − p√
N

) + (1 − p) exp(−iλ

p√
N

)∣ ≤ p∣ exp(iλ

1 − p√
N

)∣ + (1 − p)∣ exp(−iλ

p√
N

)∣ = p + (1 − p) = 1.

Thus E[exp(iλYN )] → 1, which implies YN → 0.

◻

10.6. Proof of Lemma 5

We only need to prove there exists a constant C s.t. E[∣∣ ˜Z N
t

∣∣2
2

] ≤ C for all t ∈ [T ] and N . The claim

in the lemma for ˜X N
t

follows directly from diﬀusion regularity. Because

∣ ˜X N
t

∣ = ∣˜πt,N ( ˜Z N

t

)∣ ≤ ∣˜πt,N ( ˜Z N

t

) − ˜πt,N (0)∣ + ∣˜πt,N (0)∣ ≤ C1∣ ˜Z N

t

∣ + C2,

we have

∣∣ ˜X N
t

∣∣2
2

≤ ∣S∣∣ ˜X N
t

∣2 ≤ ∣S∣∣C1∣ ˜Z N

t

∣ + C2∣2 ≤ 2∣S∣C 2

1

∣ ˜Z N
t

∣2 + 2∣S∣C 2
2

≤ 2∣S∣2C 2
1

∣∣ ˜Z N
t

∣∣2
2

+ 2∣S∣C 2
2 .

48

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

By taking the expectation,

E∣∣ ˜X N
t

∣∣2
2

≤ 2∣S∣2C 2
1

E∣∣ ˜Z N
t

∣∣2
2

+ 2∣S∣C 2
2 .

(19)

Similar to the analysis in the proof of Lemma 4,

˜Z N
t+1

= 1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N + ∆N

t

(s′, a),

where

C N
t

(s′, a) ∣ X N
t

∼ Multinomial(N xt(s′, a), p(s′, a)),

∆N
t

(s′, a) ∣ X N
t

∼ sgn( ˜X N
t

(s′, a))Multinomial(

√

N ∣ ˜X N
t

(s′, a)∣ , p(s′, a)).

Thus,

∣∣ ˜Z N
t+1

∣∣2
2

=

1√
N

RRRRRRRRRRR
RRRRRRRRRRR
RRRRRRRRRRR
RRRRRRRRRRR

≤ 2

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N + ∆N

t

(s′, a)

1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N

RRRRRRRRRRR

RRRRRRRRRRR

2

2

+ 2

RRRRRRRRRRR
RRRRRRRRRRR

1√
N

∑
s′∈S,a∈S

∆N
t

(s′, a)

RRRRRRRRRRR
RRRRRRRRRRR

2

2

.

RRRRRRRRRRR
RRRRRRRRRRR

2

2

Notice from its deﬁnition as a multinomial random variable that the absolute value of each

component of

1√
N

∆N
t

(s′, a) is bounded above by ∣ ˜X N
t

(s′, a)∣. Thus

RRRRRRRRRRR
RRRRRRRRRRR

1√
N

∑
s′∈S,a∈S

∆N
t

(s′, a)

RRRRRRRRRRR
RRRRRRRRRRR

2

2

≤ ∣S∣

⎡
⎢
⎢
∑
⎢
⎣
s′∈S,a∈S

∣ ˜X N
t

⎤
2
⎥
(s′, a)∣
⎥
⎥
⎦

≤ 2∣S∣2 ∑

s′∈S,a∈S

∣ ˜X N
t

(s′, a)∣2 = 2∣S∣2∣∣ ˜X N

t

∣∣2
2.

(20)

On the other hand, noting that C N
t

(s′, a) − xt(s′, a)p(s′, a)N has mean 0 and is independent

across diﬀerent s, a to get the ﬁrst equality, we have

E

RRRRRRRRRRR
RRRRRRRRRRR

1√
N

∑
s′∈S,a∈A

C N
t

(s′, a) − xt(s′, a)p(s′, a)N

RRRRRRRRRRR
RRRRRRRRRRR

2

2

= 1
N

∑
s′∈S,a∈A

E ∣∣C N
t

(s′, a) − xt(s′, a)p(s′, a)N ∣∣2

2

(21)

= ∑

s′∈S,a∈A

xt(s′, a)(1 − ∑
s∈S

p(s′, a, s)2).

Combining inequality (20) and (21) together,

E∣∣ ˜Z N
t+1

∣∣2
2

≤ 2C4 + 4∣S∣2E∣∣ ˜X N

t

∣∣2
2.

where C4 ∶= ∑s′∈S,a∈A xt(s′, a)(1 − ∑s∈S p(s′, a, s)2).

Recall inequality (19) and combine it with (23) to obtain,

(22)

(23)

E∣∣ ˜Z N
t+1

∣∣2
2

≤ 2C4 + 8∣S∣3C 2

2

+ 8∣S∣4C 2

2 C 2

1

E∣∣ ˜Z N
t

∣∣2
2.

By ˜Z N
1

= 0 and induction, there exists a constant C s.t. E[∣∣ ˜Z N
t

∣∣2
2

] ≤ C for all t ∈ [T ] and N .

◻

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

49

10.7. Proof of Theorem 3

Given a ﬂuid-priority policy π, we directly check whether the induced map ˜πt,N satisﬁes all three

conditions in Deﬁnition 2.

Veriﬁcation of Condition 1 Write the induced map ˜πt,N as a collection of maps, (˜π1

t,N , ..., ˜π∣S∣

t,N

),

one giving each component. That is, ˜πt,N (θ) is the vector comprised of (˜πi

t,N

(θ) ∶ 1 ≤ i ≤ ∣S∣).

A direct calculation shows each component function, ˜πi

t,N

(1 ≤ i ≤ ∣S∣), is continuous, piecewise

linear, and has bounded gradients when they exist. Mathematically speaking, there exists a constant

˜C1, s.t., for any θ, any t and any N ,

∣∇θ ˜πi

t,N

(θ)∣ ≤ ˜C1, when ∇θ ˜πi

t,N

(θ) exists.

For any θ1 and θ2, there exists a sequence (ν0, ν 1, ..., νm) lying on the line segment between θ1

and θ2, s.t.

1. ˜πi

t,N restricted on the line segment between ν j and νj+1 is linear for j = 0, 1, ..., m − 1

2. ν0 = θ1 and νm = θ2.

Thus

∣˜πi

t,N

(θ1) − ˜πi

t,N

(θ2)∣ ≤

m−1
∑
j=0

∣˜πi

t,N

(νj) − ˜πi

t,N

(νj+1)∣ ≤

m−1
∑
j=0

˜C1∣νj − νj+1∣ = ˜C1∣θ1 − θ2∣.

So by taking C1 = ∣S∣ ˜C1,

∣˜πt,N (θ1) − ˜πt,N (θ2)∣ ≤

∣S∣
∑
i=1

∣˜πi

t,N

(θ1) − ˜πi

t,N

(θ2)∣ ≤

∣S∣
∑
i=1

˜C1∣θ1 − θ2∣ = C1∣θ1 − θ2∣.

Veriﬁcation of Condition 2 Direct calculation shows ˜πt,N (0) = 0.

Veriﬁcation of Condition 3 Direct calculation shows ˜πt,∞( ˜Zt,∞) is a linear mapping. The form

of this linear mapping diﬀers across the following three cases. We state the results of detailed cal-

culations here providing these linear forms without including the (tedious) calculations themselves.

Case 1. C 0
t

∪ C −
t

= ∅:

˜πt,∞( ˜Zt,∞)(s, 1) = ˜Zt,∞(s), for each s ∈ S.

50

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Case 2. C 0
t

≠ ∅:

˜πt,∞( ˜Zt,∞)(s, 1) = ˜Zt,∞(s), for each s ∈ C +
t ;

˜πt,∞( ˜Zt,∞)(s, 1) = − ∑
s′∈C+
t

˜Zt,∞(s′), for the state s ∈ C 0

t with highest priority-score in C 0
t ;

˜πt,∞( ˜Zt,∞)(s, 1) = 0, otherwise.

Case 3. C 0
t

= ∅, C −
t

≠ ∅:

˜πt,∞( ˜Zt,∞)(s, 1) = ˜Zt,∞(s), for each s ∈ C +
t ;

˜πt,∞( ˜Zt,∞)(s, 1) = − ∑
s′∈C+
t

˜Zt,∞(s′), for the state s ∈ C −

t with highest priority-score in C −
t ;

˜πt,∞( ˜Zt,∞)(s, 1) = 0, otherwise.

To summarize, we prove the induced map of any ﬂuid-priority policy satisﬁes all three conditions

in Deﬁnition 2 and thus any ﬂuid-priority policy is diﬀusion regular.

10.8. Proof of Lemma 6

Direct comparison of Algorithm 1 and Algorithm 2 justiﬁes Lemma 6.

10.9. Proof of Lemma 7

Before we prove Lemma 7, we prove the following preliminary lemma.

Lemma 12. Suppose the non-degeneracy condition holds. Then there exists constants δ > 0 and

C > 0 s.t., ∀(cid:15) > 0, t ∈ [T ], we have

PπR

[∣ ˜Z N
t

√

∣ ≥ (cid:15)

N ] ≤ C exp(−N δ(cid:15)2).

Proof of Lemma 12 We will show that, for 0 ≤ t ≤ T − 1,

PπR

[∣ ˜Z N
t+1

√

∣ ≥ (cid:15)

N ] ≤ 4∣S∣2 exp(− (cid:15)2N
8∣S∣4

) + 2∣S∣2PπR

[∣ ˜Z N
t

√

∣ ≥ (cid:15)

N
4∣S∣2

] .

(24)

The above inequality and the observation ˜Z N
1

= 0 would complete Lemma 12. So in the remainder

of this proof, we show inequality (24) holds true.

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

51

By a union bound and the fact that ∣ ˜Z N
t+1

∣ ≥ (cid:15)

N implies ∣ ˜Z N
t+1

√

√

(s)∣ ≥ (cid:15)
√

N /∣S∣ for at least one s,

PπR

[∣ ˜Z N
t+1

√

∣ ≥ (cid:15)

N ] ≤ ∑
s∈S

PπR

[∣ ˜Z N
t+1

(s)∣ ≥ (cid:15)

N
∣S∣

] .

Thus, we only need to show, for any s ∈ S,

√

PπR

[∣ ˜Z N
t+1

(s)∣ ≥ (cid:15)

N
∣S∣

] ≤ 4∣S∣ exp(− (cid:15)2N
8∣S∣4

) + 2∣S∣PπR

(∣ ˜Z N
t

√

∣ ≥ (cid:15)

N
4∣S∣2

).

(25)

Following a similar approach to the proof of Lemma 4, we ﬁrst write our system dynamics in a

vector form:

where the BN
t

(s′, a) are conditionally independent (across s′ and a) multinomial distributions with

Z N
t+1

= ∑

s′∈S,a∈A

BN
t

(s′, a),

(26)

parameters X N
t

(s′, a) and p(s′, a) ∶= (p(s′, a, s))s∈S, i.e.,

BN
t

(s′, a) ∣ X N
t

∼ Multinomial(X N
t

(s′, a), p(s′, a)).

BN
t

(s′, a) is a vector counting the number of arms in each state, among those arms that were

previously in state s′ and for which we used action a. We use BN
t

(s′, a, s) to denote component s

of BN
t

(s′, a), i.e. the number of arms that were previously in state s′, for which we used action a,

and which transitioned to state s.

Recall the deﬁnition of our diﬀusion statistic ˜Z N

t+1 and combine it with equation (26),

˜Z N
t+1

(s) =

⎡
⎢
⎢
∑
⎢
⎣
s′∈S,a∈A

1√
N

(BN
t

(s′, a, s) − p(s′, a, s)X N
t

(s′, a))

⎤
⎥
⎥
⎥
⎦

+

⎡
⎢
⎢
∑
⎢
⎣
s′∈S,a∈A

p(s′, a, s) ˜X N
t

⎤
⎥
⎥
(s′, a)
⎥
⎦

.

So we have

√

PπR

[∣ ˜Z N
t+1

(s)∣ ≥ (cid:15)

N
∣S∣

] ≤ PπR

+ PπR

Notice

⎡
⎢
⎢
⎢
⎣
⎡
⎢
⎢
⎢
⎣

RRRRRRRRRRR
RRRRRRRRRRR

∑
s′∈S,a∈A

1√
N

(BN
t

(s′, a, s) − p(s′, a, s)X N
t

(s′, a))

∑
s′∈S,a∈A

p(s′, a, s) ˜X N
t

(s′, a)

√

≥ (cid:15)

N
2∣S∣

⎤
⎥
⎥
⎥
⎦

.

RRRRRRRRRRR

√

≥ (cid:15)

N
2∣S∣

⎤
⎥
⎥
⎥
⎦

RRRRRRRRRRR

PπR

⎡
⎢
⎢
⎢
⎣

RRRRRRRRRRR

∑
s′∈S,a∈A

≤ ∑

s′∈S,a∈A

PπR

1√
N
[ 1√
N

(BN
t

(s′, a, s) − p(s′, a, s)X N
t

(s′, a))

RRRRRRRRRRR

√

≥ (cid:15)

N
2∣S∣
√

∣BN
t

(s′, a, s) − p(s′, a, s)X N
t

(s′, a)∣ ≥ (cid:15)

N
4∣S∣2

⎤
⎥
⎥
⎥
⎦

] .

52

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

By Hoeﬀding’s inequality, we have

PπR

[ 1√
N

∣BN
t

(s′, a, s) − p(s′, a, s)X N
t

√

(s′, a)∣ ≥ (cid:15)

N
4∣S∣2

Combining the above inequalities together,

] ≤ 2 exp (−

(cid:15)2N 2

8∣S∣4∣X N
t

(s′, a)∣

) ≤ 2 exp (− (cid:15)2N
8∣S∣4

) .

⎡
⎢
⎢
⎢
⎣

RRRRRRRRRRR

∑
s′∈S,a∈A

1√
N

(BN
t

(s′, a, s) − p(s′, a, s)X N
t

(s′, a))

√

≥ (cid:15)

N
2∣S∣

⎤
⎥
⎥
⎥
⎦

RRRRRRRRRRR

≤ 4∣S∣ exp(− (cid:15)2N
8∣S∣4

).

(27)

PπR

On the other hand, combining the bound

RRRRRRRRRRR

∑
s′∈S,a∈A

p(s′, a, s) ˜X N
t

(s′, a)

RRRRRRRRRRR

≤ ∑

s′∈S,a∈A

∣ ˜X N
t

(s′, a)∣

with a union bound, we have

⎡
⎢
⎢
⎢
⎣

RRRRRRRRRRR

∑
s′∈S,a∈A

p(s′, a, s) ˜X N
t

(s′, a)

√

≥ (cid:15)

N
2∣S∣

⎤
⎥
⎥
⎥
⎦

RRRRRRRRRRR

≤ ∑

s′∈S,a∈A

PπR

[∣ ˜X N
t

(s′, a)∣ ≥ (cid:15)

N
4∣S∣2

] .

√

PπR

Analysis similar to Condition 3 in §10.7 shows that ∣ ˜X N
t

(s′, a)∣ ≤ ∣ ˜Z N
t

∣ for any s ∈ S. Thus,

⎡
⎢
⎢
⎢
⎣

RRRRRRRRRRR

∑
s′∈S,a∈A

p(s′, a, s) ˜X N
t

(s′, a)

√

≥ (cid:15)

N
2∣S∣

⎤
⎥
⎥
⎥
⎦

RRRRRRRRRRR

≤ 2∣S∣PπR

[∣ ˜Z N
t

√

∣ ≥ (cid:15)

N
4∣S∣2

] .

PπR

Combining inequality (27) and (28) together implies inequality (25), concluding the proof.

(28)

◻

Now we can prove Lemma 7.

Proof of Lemma 7 Let Ωt ∶= ∆1 ∩ ∆2 ∩ . . . ∩ ∆t and let Ωc

t denote its complement. First we notice,

PπF

(∆c

t+1

) = PπF

(Ωt ∩ ∆c

t+1

) + PπF

(Ωc
t

∩ ∆c

t+1

)

= PπR

(Ωt ∩ ∆c

t+1

) + PπF

(Ωc
t

∩ ∆c

t+1

)

≤ PπR

(∆c

t+1

) + PπF

(Ωc
t

)

≤ PπR

(∆c

t+1

) +

t
∑
k=1

PπF

(∆c
k

).

We will use this recursive expression show that PπF

(∆c
t

) ≤ L exp(−δN ) by induction on t. The

base case, t = 1, follows immediately from PπR

(∆c
1

) = 0. Thus, it is suﬃcient to prove there exists

constants δ > 0 and L, s.t. for all t,

PπR

(∆c
t

) ≤ L exp(−δN ).

(29)

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

53

We rewrite ∆c

t in terms of ˜Z N

t , by ﬁrst noting that there are two ways to have a budget violation

event ∆c

t. The ﬁrst arises when the number of arms available to pull in ﬂuid-active and ﬂuid-neutral

states, ∑s∈C0

t ∪C+
t

Z N
t

(s), falls below the number of arms that the optimal occupation measure plans

to pull N ∑s∈C0

t ∪C+
t

xt(s, 1), where we note that the optimal occupation measure never pulls arms

in C −

t . We deﬁne our diﬀusion statistics ˜Z N

t by subtracting (xt(s, 0) + xt(s, 1))N from Z N

t and

dividing the diﬀerence by

N , and so the following conditions are all equivalent:

√

N ∑
s∈C0

t ∪C+
t

xt(s, 1) > ∑
s∈C0

t ∪C+
t

Z N
t

(s)

−N ∑
s∈C0

t ∪C+
t

xt(s, 0) > ∑
s∈C0

t ∪C+
t

Z N
t

(s) − N (xt(s, 0) + xt(s, 1)),

√

−

N ∑
s∈C0

t ∪C+
t

xt(s, 0) > ∑
s∈C0

t ∪C+
t

˜Z N
t

(s).

Moreover, optimal occupation measures set xt(s, 0) = 0 for s ∈ C +

t . Thus, the conditions above are

equivalent to

√

−

N ∑
s∈C0
t

xt(s, 0) > ∑
s∈C0

t ∪C+
t

˜Z N
t

(s).

The other way in which we can have a budget violation is to have the number of arms available

to idle in ﬂuid-inactive and ﬂuid-neutral states fall below the number of arms that the optimal

occupation measure plans to idle, N ∑s∈C0

t ∪C−
t

xt(s, 0). By a similar sequence of computations, this

occurs if and only if

Thus,

˜Z N
t

(s) >

∑
s∈C+
t

√

N ∑
s∈C0
t

xt(s, 1)

∆c
t

= { −

√

N ∑
s∈C0
t

xt(s, 0) > ∑
s∈C0

t ∪C+
t

˜Z N
t

(s)} ⋃ { ∑
s∈C+
t

˜Z N
t

(s) >

√

N ∑
s∈C0
t

xt(s, 1)}.

Thus we have,

PπR

(∆c
t

) ≤ PπR

√

⎡
⎢
⎢
−
⎢
⎢
⎣

N ∑
s∈C0
t

xt(s, 0) > ∑
s∈C0

PπR

[∣ ˜Z N
t

(s)∣ >

√

N

≤ ∑
s∈C0

t ⋃ C+
t

˜Z N
t

⎤
⎥
⎥
(st)
⎥
⎥
⎦
xt(s, 0)
∣S∣

t

+ PπR

⎡
⎢
⎢
∑
⎢
⎢
⎣
s∈C+
t

˜Z N
t

(s) >

√

N ∑
s∈C0
t

PπR

[∣ ˜Z N
t

(s)∣ >

√

N

] + ∑
s∈C0
t

⎤
⎥
⎥
xt(s, 1)
⎥
⎥
⎦
xt(s, 1)
∣S∣

t

∑s∈C0

]

t ∪C+
t
∑s∈C0

Using Lemma 12, it is easy to see inequality (29) holds.

54

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

10.10. Proof of Lemma 8

By the Fenchel Duality Theorem (Rockafellar 1970), there exists λ∗
1∶T

ˆV ∗
1

= max
π

Eπ

T
∑
t=1

rt(st,1, at,1) + λ∗

t

(αt − at,1)

= (λ∗

1, λ2∗, ..., λ∗

T

) s.t.

(30)

where here the maximum is taken over all policies, not just those satisfying the budget constraint

Eπ∣at,1∣ = αt.

Following a dynamic programming argument, we deﬁne the value function Vt on states and the

Q-factor Qt on state-action pairs recursively as

Qt(s, a) = rt(s, a) − λ∗

t a + ∑
s′∈S

pt(s, 1, s′)Vt+1(s′),

Vt(s) = max{Qt(s, 0), Qt(s, 1)}.

for 0 ≤ t ≤ T with VT +1(s) = 0 for all s ∈ S. Thus, we can classify states into three disjoint sets

Must-Pullt = {s ∈ S∣Qt(s, 0) < Q},

Indiﬀerentt = {s ∈ S∣Qt(s, 0) = Qt(s, 1)},

Never-Pullt = {s ∈ S∣Qt(s, 0) > Qt(s, 1)}.

A policy is optimal for (30) if and only if it satisﬁes these two conditions for each t:

• It pulls all arms whose states are in Must-Pullt.

• It never pulls any arms whose states are in Never-Pullt.

It can behave arbitrarily for arms whose states are in Indiﬀerentt.

Any optimal occupation measure (xt(s, a))t∈[T ],s∈S,a∈A achieves ˆV ∗

1 and so must correspond to an

optimal policy. Thus

Must-Pullt ⊆ C +

t , Never-Pullt ⊆ C −
t .

Any budget-relaxed ﬂuid-priority policy πR pulls an arm whenever its state is in C +

t and lets an

arm idle whenever its state is in C −

t . Thus, it is optimal for (30) and

VN (πR) +

T
∑
t=1

λ∗
t

(αtN − EπR

[∣at∣]) = ˆV ∗
N .

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

55

Using the fact that ∣αtN − EπR

[at]∣ is bounded above by the probability of a budget violation event

times a bound N on the maximum size of a budget violation, as well as Lemma 7,

∣αtN − EπR

[∣at∣]∣ ≤ N PπR

(∆c
t

) ≤ N L exp(−δN ) ≤ m, ∀t,

where m is a constant not depending on N .

Thus,

∣VN (πR) − ˆV ∗

N

∣ = ∣

T
∑
t=1

λ∗
t

(αtN − EπR

[∣at∣])∣ ≤

T
∑
t=1

∣λ∗
t

∣∣αtN − EπR

[∣at∣])∣ ≤ m

T
∑
t=1

∣λ∗
t

∣.

10.11. Proof for Proposition 1

For any index policy πI, by the strong law of large numbers, X N
t

(s, a)/N converges as N → ∞ to

a quantity that we denote xI,t(s, a) and refer to as the occupation measure of the index policy.

We argue that xI,t(s, a) has at most one state s for each t satisfying both xI,t(s, 0) > 0 and

xI,t(s, 1) > 0. To see this, ﬁrst recall that index policies use a strict priority order over states, pulling

all arms in states higher in the priority order before pulling any arms in lower states. Then deﬁne

for each state s and time t the following quantities:

• Let P (s) denote the set of states that have equal or higher priority to s according to the index

policy.

• Let LN
t

(s) denote the number of arms whose states have equal or higher priority than s and

that are not pulled.

• Let M N
t

(s) denote the number of arms pulled whose states have priority strictly lower than

s.

By the mechanics of an index policy’s decisions, we either have LN
t

(s) = 0, i.e., we pull all of the

arms whose states have equal or higher priority to s, or M N
t

(s) = 0, i.e., we pull no arms whose

states have priority strictly lower than s

Then, taking the limit as N → ∞ and using the strong law of large numbers, we have

0 = lim
N →∞

LN
t

(s)M N
t
N 2

(s)

=

⎛
⎝

∑
s′∈P (s)

xI,t(s′, 0)

⎞
⎠

⎛
⎝

∑
s′∉P (s)

xI,t(s′, 1)

⎞
⎠

,

56

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

This then implies that there is a unique s such that xI,t(s′, 0) = 1 for s′ ∈ P (s)∖{s} and xI,t(s′, 1) =

0 for all s′ ∉ P (s). That is, states that have strictly higher priority than s are always pulled in the

ﬂuid limit, while states that have strictly lower priority than s are never pulled in this limit.

Now, since any index policy meeting the condition of the proposition has ˆV ∗
N

− VN (πI) bounded

above by a constant, this index policy’s occupation measure xI,t solves Problem 4. We then con-

struct a ﬂuid-priority policy to match this index policy.

First, we note that the set of ﬂuid-active states for the optimal occupation measure xI,t are those

with xI,t(s, 0) = 0 and that the index policy ranks these above all other states. We take the priority

score used by our ﬂuid priority policy to rank these ﬂuid-active states among themselves in the

same way as the index policy.

Second, the set of ﬂuid-inactive states for xI,t are those with xI,t(s, 1) = 0. The index policy ranks

these below all other states. Again, we take the priority score used by our ﬂuid priority policy to

rank these ﬂuid-inactive states in the same way as the index policy.

Third, the at most one state with xI,t(s, 0) > 0 and xI,t(s, 1) > 0 is a ﬂuid-neutral state, and it is

ranked by the index policy below the ﬂuid-active states and above the ﬂuid-inactive states.

Because our ﬂuid-priority policy’s priority score matches the index policy’s prioritizations on

ﬂuid-active and ﬂuid-inactive states, and its prioritizations also across categories (ﬂuid-active, ﬂuid-

neutral, ﬂuid-inactive) match those of the index policy, our ﬂuid-priority policy is the same as the

index policy.

10.12. Proof for Proposition 2

The proof is similar for both UCB and Thompson Sampling policy. We only show the proof for

UCB here.

Under the UCB policy, there exists zU CB

t

(s) and xU CB

t

(s, a) which are feasible for the LP (4)

and satisfy

(s)
Z N
t
N

→ zU CB
t

(s),

X N
t

(s, a)
N

→ xU CB
t

(s, a).

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

57

So we have

RπU CB (N )
N

→

T
∑
t=1

∑
s∈S

∑
a∈A

rt(s, a)xU CB

t

(s, a).

UCB policy is an index policy. Thus, the occupation measure xU CB

t

(s, a) can be calculated via

forward propagation. Numerically, we can verify xU CB

t

(s, a) is not an optimal solution for LP (4)

under T = 15 and T = 20.

10.13. Discussion of policies in previous literature

In this section, we show the power of the techniques developed in §4 and 5 by applying them to

policies proposed by previous literature to demonstrate theoretical guarantees from that literature

can be seen as consequences of our results. Speciﬁcally, we observe that the Randomized Assignment

Control (RAC) policy proposed by Zayas-Caban et al. (2019) is ﬂuid-consistent, thus achieving an

o(N ) optimality gap. The policy proposed by Hu and Frazier (2017) and the “optimal Lagrangian

index policy” proposed by Brown and Smith (2020) are diﬀusion-regular, thus achieving O(

√

N )

optimality gaps.

10.13.1. Zayas-Caban et al. (2019) achieves o(N ) optimality gap This section shows

the RAC policy proposed by Zayas-Caban et al. (2019) achieves an o(N ) optimality gap. To start

with, let us ﬁrst describe the RAC policy. Although Zayas-Caban et al. (2019) deﬁnes RAC policy

in settings more general than the binary-action bandit (referring to their more general problem

setting as a “multi-action bandit”), we only focus on the binary bandit here.

Similar to our approach, Zayas-Caban et al. (2019) ﬁrst solves the linear programming relax-

ation (3) and then fetch an optimal occupation measure {xt(s, a)}t∈[T ],s∈S,a∈A. Then, based on the

occupation measure, an activation probability is deﬁned for each state s at period t:

qt(s) =

⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩

⎨

xt(s,1)
zt(s) ,

if zt(s) > 0;

0,

if zt(s) = 0.

58

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Then when deciding which arm to pull at period t under the RAC policy, we ﬁrst randomly

choose an arm that has not been chosen in this period. If the arm’s state is s, then we randomly

generate a Bernoulli variable with mean qt(s). If this random realization is 0 or there is no remaining

budget, idle the arm; otherwise, activate the arm. Repeat this process until no budget remains in

the period.

Direct computation and the strong law of large numbers show that the RAC policy is ﬂuid

consistent. Thus, it achieves an o(N ) optimality gap.

10.13.2. Hu and Frazier (2017) and Brown and Smith (2020) achieve O(

√

N ) opti-

mality gaps The policies proposed by Hu and Frazier (2017) and Brown and Smith (2020) are

very similar. Thus we only discuss Brown and Smith (2020)’s policy here. The analysis for Hu and

Frazier (2017)’s policy can be generalized without any essential diﬃculty.

To start with, we ﬁrst describe the “optimal Lagrangian index policy” proposed by Brown and

Smith (2020). Similar to our approach, Brown and Smith (2020) ﬁrst solves the linear programming

relaxation (3) and fetches an optimal occupation measure {xt(s, a)}t∈[T ],s∈S,a∈A, which is used to do

“tie-breaking” discussed later. While solving the relaxed problem using the Simplex method (Nash

2000), as a byproduct, its dual problem

min
λ1∶T

max
π

Eπ

T
∑
t=1

rt(st, at) + λt(αtN − at).

is also solved, which yields optimal Lagrange multipliers {λ∗
t

}T
t=1.

Then, following a dynamic programming argument, the value function Vt on states and the

Q-factor Qt on state-action pairs are deﬁned as

Qt(s, a) = rt(s, a) − λ∗

t a + ∑
s′∈S

pt(s, 1, s′)Vt+1(s′),

Vt(s) = max{Qt(s, 0), Qt(s, 1)}

for 0 ≤ t ≤ T with VT +1(s) = 0 for all s ∈ S. Finally the index of a state s at period t is deﬁned as

Index(s) = Qt(s, 1) − Qt(s, 0).

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

59

When deciding which arm to pull, arms are activated from high index to low index until no

budget remains. When there is a tie, i.e., some states share the same index value, the number of

arms activated from a state is proportional to its occupation measure. More details can be found

in Brown and Smith (2020) Section 4.

Now we show the optimal Lagrangian index policy is diﬀusion regular. First of all, we can show

its associated map ˆπt,N is a piece-wise linear map, thus satisfying Condition 1 in Deﬁnition 2.

Second, we can show ˆπt,N (0) = 0, thus satisfying Condition 2. As a piece-wise linear map, we can

also show that ˆπt,N converges as N → ∞. Thus, Condition 3 is satisﬁed.

References

Abbou A, Makis V (2019) Group maintenance: A restless bandits approach. INFORMS Journal on Com-

puting 31(4):719–731.

Agrawal R (1995) Sample mean based index policies with o (log n) regret for the multi-armed bandit problem.

Advances in Applied Probability 1054–1078.

Agrawal S, Goyal N (2012) Analysis of thompson sampling for the multi-armed bandit problem. Conference

on Learning Theory, 39–1.

Al Islam AA, Alam SI, Raghunathan V, Bagchi S (2012) Multi-armed bandit congestion control in multi-hop

infrastructure wireless mesh networks. 2012 IEEE 20th International Symposium on Modeling, Analysis

and Simulation of Computer and Telecommunication Systems, 31–40 (IEEE).

Auer P, Cesa-Bianchi N, Fischer P (2002) Finite-time analysis of the multiarmed bandit problem. Machine

learning 47(2-3):235–256.

Bertsimas D, Ni˜no-Mora J (2000) Restless bandits, linear programming relaxations, and a primal-dual index

heuristic. Operations Research 48(1):80–90.

Besbes O, Gur Y, Zeevi A (2014) Stochastic multi-armed-bandit problem with non-stationary rewards.

Advances in neural information processing systems 27:199–207.

Brown DB, Smith JE (2020) Index policies and performance bounds for dynamic selection problems. Man-

agement Science .

60

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Chakrabarti D, Kumar R, Radlinski F, Upfal E (2009) Mortal multi-armed bandits. Advances in neural

information processing systems, 273–280.

Chen X, Lin Q, Zhou D (2013) Optimistic knowledge gradient policy for optimal budget allocation in

crowdsourcing. International conference on machine learning, 64–72.

Cho P, Farias V, Kessler J, Levi R, Magnanti T, Zarybnisky E (2015) Maintenance and ﬂight scheduling of

low observable aircraft. Naval Research Logistics (NRL) 62(1):60–80.

Dai J, Kleywegt AJ, Xiao Y (2019) Network revenue management with cancellations and no-shows. Produc-

tion and Operations Management 28(2):292–318.

Dayanik S, Powell W, Yamazaki K (2008) Index policies for discounted bandit problems with availability

constraints. Advances in Applied Probability 40(2):377–400.

Evans R, Krishnamurthy V, Nair G, Sciacca L (2005) Networked sensor management and data rate control

for tracking maneuvering targets. IEEE Transactions on Signal Processing 53(6):1979–1991.

Farias VF, Madan R (2011) The irrevocable multiarmed bandit problem. Operations Research 59(2):383–399.

Frazier PI, Powell WB, Dayanik S (2008) A knowledge-gradient policy for sequential information collection.

SIAM Journal on Control and Optimization 47(5):2410–2439.

Gittins J, Glazebrook K, Weber R (2011) Multi-armed bandit allocation indices (John Wiley & Sons).

Glazebrook KD, Ruiz-Hernandez D, Kirkbride C (2006) Some indexable families of restless bandit problems.

Advances in Applied Probability 38(3):643–672.

Goldenshluger A, Zeevi A (2013) A linear response bandit problem. Stochastic Systems 3(1):230–261.

Guha S, Munagala K (2007) Approximation algorithms for budgeted learning problems. Proceedings of the

thirty-ninth annual ACM symposium on Theory of computing, 104–113 (ACM).

Guha S, Munagala K (2008) Sequential design of experiments via linear programming. arXiv preprint

arXiv:0805.2630 .

Guha S, Munagala K (2013) Approximate indexability and bandit problems with concave rewards and

delayed feedback. Approximation, Randomization, and Combinatorial Optimization. Algorithms and

Techniques, 189–204 (Springer).

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

61

Guha S, Munagala K, Shi P (2010) Approximation algorithms for restless bandit problems. Journal of the

ACM (JACM) 58(1):3.

Gupta N, Granmo OC, Agrawala A (2011) Thompson sampling for dynamic multi-armed bandits. 2011 10th

International Conference on Machine Learning and Applications and Workshops, volume 1, 484–489

(IEEE).

Hawkins JT (2003) A Langrangian decomposition approach to weakly coupled dynamic optimization problems

and its applications. Ph.D. thesis, Massachusetts Institute of Technology.

Hero AO, Cochran D (2011) Sensor management: Past, present, and future. IEEE Sensors Journal

11(12):3064–3075.

Hu W, Frazier P (2017) An asymptotically optimal index policy for ﬁnite-horizon restless bandits. arXiv

preprint arXiv:1707.00205 .

Kunnumkal S, Topaloglu H (2011) Linear programming based decomposition methods for inventory distri-

bution systems. European Journal of Operational Research 211(2):282–297.

Lai TL, Robbins H (1985) Asymptotically eﬃcient adaptive allocation rules. Advances in applied mathematics

6(1):4–22.

Le Ny J, Dahleh M, Feron E (2006) Multi-agent task assignment in the bandit framework. Proceedings of

the 45th IEEE Conference on Decision and Control, 5281–5286 (IEEE).

Liu K, Zhao Q (2009) On the myopic policy for a class of restless bandit problems with applications in

dynamic multichannel access. Proceedings of the 48h IEEE Conference on Decision and Control (CDC)

held jointly with 2009 28th Chinese Control Conference, 3592–3597 (IEEE).

Mahajan A, Teneketzis D (2008) Multi-armed bandit problems. Foundations and applications of sensor

management, 121–151 (Springer).

Nash JC (2000) The (dantzig) simplex method for linear programming. Computing in Science & Engineering

2(1):29–31.

Ni˜no-Mora J, Villar SS (2011) Sensor scheduling for hunting elusive hiding targets via whittle’s restless bandit

index policy. International Conference on NETwork Games, Control and Optimization (NetGCooP

2011), 1–8 (IEEE).

62

Xiangyu Zhang and Peter I. Frazier: Restless Bandits with Many Arms
Article submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)

Powell WB (2007) Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703

(John Wiley & Sons).

Rockafellar RT (1970) Convex analysis princeton university press. Princeton, NJ .

Ryzhov IO, Powell WB, Frazier PI (2012) The knowledge gradient algorithm for a general class of online

learning problems. Operations Research 60(1):180–195.

Scott SL (2010) A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in Business

and Industry 26(6):639–658.

Sutton RS, Barto AG (1999) Reinforcement learning: An introduction. Robotica 17(2):229–235.

Weber RR, Weiss G (1990) On an index policy for restless bandits. Journal of Applied Probability 27(3):637–

648.

Whittle P (1980) Multi-armed bandits and the gittins index. Journal of the Royal Statistical Society: Series

B (Methodological) 42(2):143–149.

Wu H, Srikant R, Liu X, Jiang C (2015) Algorithms with logarithmic or sublinear regret for constrained

contextual bandits. Advances in Neural Information Processing Systems, 433–441.

Zayas-Caban G, Jasin S, Wang G (2019) An asymptotically optimal heuristic for general nonstationary ﬁnite-

horizon restless multi-armed, multi-action bandits. Advances in Applied Probability 51(3):745–772.

Zhou X, Chen N, Gao X, Xiong Y (2020) Regime switching bandits. arXiv preprint arXiv:2001.09390 .

