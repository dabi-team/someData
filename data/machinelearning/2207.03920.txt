Towards Semantic Communication Protocols:

1

A Probabilistic Logic Perspective

Sejin Seo, †Jihong Park, ‡Seung-Woo Ko, †Jinho Choi,

∗Mehdi Bennis, and Seong-Lyun Kim

2
2
0
2

l
u
J

8

]
T
I
.
s
c
[

1
v
0
2
9
3
0
.
7
0
2
2
:
v
i
X
r
a

Abstract

Classical medium access control (MAC) protocols are interpretable, yet their task-agnostic control

signaling messages (CMs) are ill-suited for emerging mission-critical applications. By contrast, neural

network (NN) based protocol models (NPMs) learn to generate task-speciﬁc CMs, but their rationale

and impact lack interpretability. To ﬁll this void, in this article we propose, for the ﬁrst time, a

semantic protocol model (SPM) constructed by transforming an NPM into an interpretable symbolic

graph written in the probabilistic logic programming language (ProbLog). This transformation is viable

by extracting and merging common CMs and their connections while treating the NPM as a CM

generator. By extensive simulations, we corroborate that the SPM tightly approximates its original

NPM while occupying only 0.02% memory. By leveraging its interpretability and memory-efﬁciency,

we demonstrate several SPM-enabled applications such as SPM reconﬁguration for collision-avoidance,

as well as comparing different SPMs via semantic entropy calculation and storing multiple SPMs to

cope with non-stationary environments.

Index Terms

Semantic protocol, protocol learning, medium access control (MAC), probabilistic logic program-

ming language (ProbLog), semantic information theory, multi-agent deep reinforcement learning.

I. INTRODUCTION

Traditionally, cellular medium access control (MAC) protocols have been designed primarily

for general purposes. To this end, classical MAC protocols have been pre-determined by taking

S. Seo and S.-L. Kim are with Yonsei University, Seoul, Korea (e-mail: {sjseo, slkim}@ramo.yonsei.ac.kr). †J. Park and
†J. Choi are with Deakin University, Geelong, VIC 3220, Australia (e-mail: {jihong.park, jinho.choi}@deakin.edu.au). ‡S.-W.
Ko is with Inha University, Incheon, Korea (e-mail: swko@inha.ac.kr). ∗M. Bennis is with Oulu University, Finland (e-mail:
mehdi.bennis@oulu.ﬁ). This work has been submitted to the IEEE for possible publication. Copyright may be transferred without
notice, after which this version may no longer be accessible.

 
 
 
 
 
 
2

into account all possible cases via extensive experiments and standardization activities [1]. While

handshaking rules and scheduling policies can partly be manipulated (e.g., grant-free access

prioritization [2]), their control signaling messages (CMs) remain unchanged even when tasks

and other environmental characteristics vary over time. By its nature, the effectiveness of classical

MAC protocols in 6G has recently been questioned [3]. In fact, emerging 6G applications are

often mission-critical under non-stationary environments, such as drones and satellites in a non-

terrestrial network (NTN) [4], autonomous cars in a vehicle-to-everything (V2X) network [5], and

visuo-haptic immersive applications in the metaverse [6]. These applications cannot be supported

without tightly ﬁne-tuning MAC protocols.

Alternatively, goal-oriented and task-speciﬁc MAC protocols have recently been proposed,

wherein signaling messages emerge naturally from a given environment while conducting a

downstream task [7]–[9]. For instance, with two user equipments (UEs) and a base station

(BS) as visualized in Fig. 1, M. Mota et al. [7] have shown that uplink CMs (UCMs) and

downlink CMs (DCMs) in the control plane can emerge for the task of establishing collision-

free data communication in the user plane. The key idea comes from interpreting MAC protocol

operations through the lens of multi-agent deep reinforcement learning (MADRL) with cheap talk

communication among UE agents through the BS [10]. In this MAC protocol model, hereafter

referred to as a neural protocol model (NPM), a single neural network (NN) represents a single

cycle of uplink and downlink operations, as illustrated in Fig. 2.

Precisely, the NN consists of multiple layers, wherein the input and output correspond to

the UEs’ current states (i.e., buffer states) and actions (i.e., access or silence), respectively.

Meanwhile, the activations of two speciﬁc layers imply the UCMs and DCMs, respectively.

In this architecture, the forward propagation (FP) of the NN describes the sequence of state-

UCM-DCM-action operations, and training the NN is tantamount to iteratively giving rewards

or Q-values to such sequences under a given environment. Before training, the NN is randomly

initialized, and so are the actions, during which these CMs are meaningless activations. However,

as the NN training converges, actions become task-optimal, while the UCMs and DCMs gradually

emerge as effective CMs in the given task and environment.

Despite their effectiveness, NPMs have major disadvantages due to their NN architecture. First,

NPMs often overﬁt to their training samples. By experiment, we observed that even multiple

NPMs emerging from the same environmental statistics (e.g., mean trafﬁc arrival rate) share

neither common CMs nor their connections (see Sec. V). Second, since the NN architecture

3

Fig. 1. A MAC scenario where two UEs are contending for the same channel by sending UCMs and receiving DCMs. A
collision occurs when they send their SDUs simultaneously.

TABLE I
COMPARISON OF KPIS FOR NPM AND SPM.

Protocol

NPM

SPM

Goodput
(Packets/Cycle)
0.729
0.729
(100%)

UCM
Vocabularies
10
4
(40%)

DCM
Vocabularies
50
3
(6%)

UCM/DCM Length
(Bytes)
32
0.125
(0.19%)

Model Storage
(Bytes)
4.55M
1K
(0.02%)

Inference
(FLOPs)
14K
8
(5.7 × 10−4%)

Collision
(Collisions/Cycle)
0.275
0.0
(0.00%)

is over-parameterized, NPM operations entail large communication payloads, computing time,

and memory usage (see Fig. 2). Lastly, the NPM is a black-box function where the knowledge

regarding the protocol operations is hidden in the NN’s model parameters [11], limiting its

interpretability or explainability. Besides, we cannot immediately reconﬁgure the NN’s model

parameters, because they are updated through iterative gradient descent iterations, degrading its

ﬂexibility in manipulating protocol operations.

The overarching goal of this paper is to ﬁll the void between goal-oriented NPMs and general-

purpose MAC protocols. As its ﬁrst step, we revisit the NPM learning scenario considered

in [7], and propose a novel framework to transform the trained NPM into a semantic protocol

model (SPM). This NPM-to-SPM transformation is inspired by human language [12] and logic

programming language for symbolic artiﬁcial intelligence (AI) [13], as summarized next. At ﬁrst,

treating an NPM (i.e., a trained NN) as the generator of protocol operations, we feed the UE states

experienced during training into the NPM, and thereby extract UCMs and DCMs as well as their

causal connections. This extraction process is motivated by humans transforming the episodic

memory of experiences into the semantic knowledge of general concepts and their relations [12].

The resultant NPM extract shown in Fig. 3 is akin to a language for machine agents, and we

hereafter refer the UCMs and DCMs as vocabularies and their operational connections as rules.

DCM1UCM1UE1UE2BSUCM2DCM2SDU1ACKAction1State1Control PlaneUser PlaneAction2State24

Fig. 2. Schematic illustrations and KPI performance of the protocol models: (a) NPM; (b) graphical representation of SPM,
and the number on the edges denotes the clause truth probability; and (c) ProbLog representation of SPM.

Next, to cope with the limited number of CMs in practice, we reduce the redundant vocabular-

ies and connections in the NPM extract. This is viable by merging vocabularies and connections

based on common NN activation patterns and input-output relationships, respectively. The merg-

ing may incur the problem of polysemy (a word with multiple meanings), e.g., A → B1 → C1

and A → B2 → C1 can be merged into either A → B → C1 or A → B → C2, incurring the

polysemous vocabulary B leading to C1 or C2. Inspired by how humans distinguish the different

meanings of a polysemous vocabulary based on communication context [14], [15], we make

each connection associated with its generation frequency at the NPM as the context.

Finally, we represent each connection of two neighboring vocabularies as a clause written in

the probabilistic logic programming language (ProbLog) [16], e.g., A

p
→ B with probability p is

written as (cid:104)p :: A :- B(cid:105). Then, the collection of these probabilistic clauses forms an SPM. With

this, an SPM is much more than an approximation to its original NPM, providing multi-fold

beneﬁts. First, it is symbolic and interpretable, enabling manipulation without re-training, e.g.,

for immediately controlling collision-avoidance. Second, semantic information stored in each

SPM is measurable by evaluating the average entropy of all clauses, enabling the numerical

comparison of different SPMs, e.g., for selecting the best SPM for a given environment. Last

but not least, as Table I shows, the SPM occupies only 0.02% of the memory compared to its

original NPM. This not only accelerates the protocol operations, but also allows one to store a

5

portfolio of multiple SPMs to cope with a non-stationary environment.

A. Related Works

The related works aim to design emergent protocols based on NNs, which we call System 1

MAC, however these works overlook the attributes of classical protocols such as reconﬁgurability

and measurability. Also, recent interests in semantic communication and traditional endeavor on

symbolic and knowledge-based systems are related to our work, but they are different from

SPMs because they either limit their scope to training NN models or focus on point-to-point

communication overlooking realistic network-wide communication systems. We acknowledge

the works below, which we build upon while articulating our key novelty.

1) Classic MAC: MAC plays a key role in facilitating multiple UEs’ error-free wireless com-

munications via the efﬁciency-reliability tradeoff [17]. Due to the nature of wireless propagation

characteristics, every UE can access the medium whenever they have data to transmit, calling

for an efﬁcient utilization of the medium. On the other hand, a UE’s excessive access without

coordination causes frequent collisions, hampering wireless communication reliability. Based

on this tradeoff, various MAC protocols have been proposed for different networks’ speciﬁc

constraints, e.g., differentiation between primary and secondary users’ priorities for cognitive

radios [18], limited energy and computation capacities of the Internet-of-Things [19], ultra-

reliable and ultra-low latency for vehicle-to-everything [20] and so on. However, as scenarios

and requirements become more complex and diverse, designing a MAC protocol that fulﬁlls those

constraints is a daunting task. Moreover, even if this were possible, signiﬁcant signaling overhead

is entailed when implementing the protocol, making it impractical in an actual communication

system.

2) Emergent Protocol Design: As explained above, the classical MAC protocol design process

is limited because it is not grounded in an actual environment. To cope with such limitation, in

system 1 MAC [7]–[9], the advantage for designing protocols using NN is presented. However,

contrary to our endeavor to develop novel semantic communication protocols, most works on

emergent communication protocol design are based on learning an NN for a given environment

such as wireless MAC using MADRL [7], sensor networks, and low-earth orbit satellite net-

works [21], [22]. In addition, in [23], the authors apply NN for optimizing the MAC pipeline

by selecting appropriate protocols. However, the performance is bounded by classical protocols

that lack grounding in the actual environment.

6

3) Semantic Communication: Aligned with our interest in learning semantic representations

that contain meaning for the environment, recent works on semantic communication aim at

achieving service-speciﬁc goals via NNs or semantic information theory [24]–[27]. However,

contrary to our interest in developing interpretable and reconﬁgurable semantic representations,

these works focus on using NNs to achieve optimal encoding and decoding parameters [26], or

quantify semantic information within the message or the communication agent [27], [28].

4) Symbolic Artiﬁcial Intelligence: Related to our focus on logical relationships, learning

logical relationships to represent expert knowledge has been traditionally embodied by the ﬁeld of

expert systems [29], multi-agent systems [30], and knowledge query manipulation language [31].

However, while interesting, these works are not grounded in wireless communication systems.

Furthermore, due to the known limitations of subsymbolic (NN-based) AI, the interests in

symbolic AI, which attempts to discover the causal structure behind reasoning [13] and data [32],

is currently surging. However, to date, no tangible approach is available for discovering symbolic

knowledge in the context of communication protocols.

B. Contributions and Organization

This work is the ﬁrst of its kind to design a novel MAC protocol based on logic programming

and symbolic AI, i.e., SPM. Like an NPM, the SPM is task-speciﬁc, and at the same time

symbolized and interpretable as in classical MAC protocols, thereby achieving communication

and memory efﬁciencies as well as adaptability to non-stationary environments. Our main con-

tributions are summarized as follows.

1) SPM Construction: We propose a novel method to construct a ProbLog-based SPM from

an NN-based NPM, which occupies only 0.02% of the NPM memory usage by extracting

and merging semantically common vocabularies.

2) SPM Reconﬁguration for Collision Avoidance: Without re-training, we demonstrate that

an SPM is reconﬁgurable for collision avoidance by identifying colliding rules and instantly

manipulating their connections written in ProbLog.

3) Best SPM Selection via Semantic Entropy: We empirically show that minimizing the

average semantic entropy of an SPM (i.e., mean uncertainty of the SPM operations) achieves

the highest goodput or equivalently the highest reward, allowing one to select the best SPM

in a stationary environment.

7

4) SPM Portfolio for Non-Stationary Environments: By exploiting the memory efﬁciency

of SPMs, we propose an SPM portfolio storing a set of SPMs, each of which is the best

SPM for a different environment.

The rest of the paper is organized as follows. In Section II, we explain NPM from the per-

spective of a MAC problem and summarize its limitations. Section III details SPM construction

based on ProbLog and shows its advantages over NPM. Section IV presents the signiﬁcant

attributes of SPM and potential applications, including collision avoidance, SPM selection, and

SPM portfolio. In Section V, we numerically demonstrate the effectiveness of SPM by comparing

it with several benchmarks. Last, we conclude the work in Section IV.

II. REVISITING NEURAL PROTOCOL LEARNING

Recognizing the limitations of hand-crafted protocols, we revisit neural protocol learning for

a MAC scenario as in [7]. In Sec. II-A, we explain the scenario for MAC and the KPIs for

evaluating the protocols. In Sec. II-B, we train an NPM by using a multi-agent NN in an MARL

environment. In Sec. II-C, we explain the potentials and limitations of an NPM.

A. A Two-User MAC Scenario

We consider a cellular BS serving two UEs that are contending for the same frequency band

at a communication cycle t = 0, 1, 2, ..., T . A single unit of data that UE i can send within one

communication cycle is termed its service data unit (SDU), and it arrives at a rate of λi (SDU

per communication cycle) right before the beginning of a communication cycle, until a total of

Dmax SDUs arrive. To temporarily store the SDUs, UE i has a buffer with capacity bmax. The

buffer starts empty, and when an SDU arrives, it occupies one buffer slot; and when the SDU

is sent, the corresponding buffer slot is freed. CMs are exchanged to coordinate the UEs to

send their SDUs without collision, which occurs when they try to send their SDUs at the same

cycle. We assume that the CMs are collision-free and error-free, by using low modulation level

and advanced linear block error correcting schemes, e.g. [33], but SDUs can experience a block

error, which occurs at the rate of (cid:15). As illustrated in Fig. 1, during a communication cycle, both

the control plane and user plane communication take place between each UE and the BS. A

communication cycle consists of the following 4 phases:

(i) UCM from UE to BS: UEs communicate via UCMs before sending an SDU to inform the

BS about their current buffer state.

8

(ii) DCM from BS to UE: the BS sends DCMs to the UEs to coordinate them in their decisions

of sending the SDU or not.

(iii) Actions: UEs either stay silent, access the channel, or discard an SDU. First, nothing

happens when a UE stays silent. Second, an SDU in the buffer is sent to the BS in FIFO

manner, when the UE decides to access. Third, the newly incoming SDU is discarded when

the buffer is already full.

(iv) ACK/NACK signals from BS to UE: An ACK signal is sent to notify the UE that its SDU

has been received without collision and decoded without error. A NACK signal is sent when

the SDUs are lost due to collision, or due to a block error.

Meanwhile, because a cycle is shared between the control and user plane data, the communication

efﬁciency of the CMs is directly related to the amount of data that can be packed into an SDU.

Consequently, the main KPI is the goodput nR/T , where nR is the number of successfully

received SDUs. The system objective is thus to maximize the goodput under the efﬁcient usage

of communication, memory, and computation resources.

B. NPM Construction and Operation via Multi-Agent Reinforcement Learning

We ground the MAC system model into an MARL environment as in [7], referred to as neural

protocol learning. RL is instrumental in modeling the behaviors of agents in an environment by

rewarding desired behaviors and penalizing undesired ones. A multi-agent DQN is chosen in

particular to construct an NPM, because it leverages NNs and experience replay to approximate

the Q-function in an environment grounded manner; and the problem is formulated as a de-

centralized partially observable Markov decision process (Dec-POMDP) with a communication

channel between the UEs and the BS [34]. Consequently, NPM follows centralized training with

decentralized execution (CTDE) to address non-stationarity [7].

1) States, Actions, Observations, and Rewards: UEs and BS are decentralized agents indexed

by i ∈ {1, 2, 3}, i = 3 being the BS. Agents make partial observations on their current state by

observing the input buffer states:

b = [b1, b2]

(1)

9

where bi = 0, 1, 2, ..., bmax, ∀i ∈ {1, 2}. The actions determine what the UE does to its SDU.

Depending on b and the current protocol, the UE can choose its actions as follows:

ai =






S,

silence,

A,

access the channel and send the oldest SDU from the buffer,

(2)

D,

discard the oldest SDU from the buffer.

For notational clarity, we denote the action where UE i chooses S, A, and D, as aS

i , and aD
i ,
i , the buffer state bi is decremented by 1, unless a new SDU
arrives before the next cycle1. According to the contention and decoding result, the following

respectively. When UE i selects aA

i , aA

observation is available at the BS:

o =






idle, no SDU is received,

ACKi, received and decoded SDU from UE 1,

(3)

NACK, failed to receive or decode an SDU.

According to the BS’ observation and the actions taken by the UEs, a reward is given to the

agents by a central critic. The rewards are deﬁned as follows:

r =






+ρ1, if one SDU packet is successfully received,

−ρ2, if a UE discards an SDU packet from its buffer,

(4)

−1, otherwise.

2) NPM Learning: NPM aims to maximize the average system reward by approximating

the optimal Q-function for a given environment. The Q-function is the expectation of the sum

of discounted future reward [36]. To construct an NPM for such a scenario, we optimize the

DQN weight parameters w illustrated in Fig. 2(a) in a centralized manner, with a central critic

calculating the reward. The NPM architecture consists of two NN segments for each UE, and one

NN segment for the BS. Each NN segment consists of two hidden layers, and the output layer

pertaining to a CM or the Q-values. The layers are activated with ReLU, and the hyperparameters

are tuned to maximize the average goodput. For the NN segments, 16 nodes are used for each

1In this work, techniques are targeted toward resource-constrained Internet-of-Things devices, and it is implicitly assumed
that no retransmission is allowed to avoid excessive resource consumption [35]. It is interesting to extend the current design
towards incorporating retransmission schemes, e.g., random backoff and automatic repeat request, which is outside the scope of
the current work.

hidden layer and 8 nodes are used for the outputs. Detailed hyperparameter settings are explained

in Sec. V.

3) NPM Operation: The CMs and actions for UE i ∈ {1, 2} are chosen according to the

following functions, which correspond to the last activation function of each NN segment:

10

(NPM UCM) ui = gU

i (bi|wU ),

(NPM DCM) di = gD

i (u1, u2|wD),

(NPM Action) ai = gA

i (di|wA),

(5)

(6)

(7)

where wU , wD, wA are the NN segment parameters within the NPM. As shown in Fig. 2(a),

the following NPM operation completes a communication cycle:

(i) UCMs: the upper NN segment of each UE gU
i

transforms its buffer state bi into a vector of

activation values ui, where each activation value is a positive decimal number represented

by a ﬂoating point 32 (FP32) data type. The BS concatenates the activations from the UEs

into u = [u1, u2].

(ii) DCMs: the BS uses its NNs gD

1 and gD
2

to transform u into another activation vector

d = [d1, d2]. UE 1 gets d1 from the BS, and UE 2 gets d2.

(iii) Actions: each UE uses its bottom NN segment gA
i

to transform the vector into three

activations that correspond to the Q-value of each action that the UE could take, and decide

the action ai with the highest Q-value, as its action for this cycle.

4) Vocabulary and Connection Extraction: As shown in Fig. 2, an NPM could be used for

extracting vocabularies and their connections by treating it as a simulator. Then, as illustrated

in Fig. 3, we can obtain an NPM extract by logically connecting all activations that correspond

to input buffer states, UCMs, DCMs, and actions that are inferred simultaneously during the

episodic memory, which is implemented to store the historical data on the states, intermediate
activations, and actions. For a buffer state denoted by b(k)
K buffer states of UE i, a distinct UCM denoted by u(k)
UCM pair u(k), a distinct DCM d(k)

is inferred by (6). Lastly, an action ai is inferred from the

is inferred by (5). Next, for each distinct

, which is the k-th buffer state among

i

i

i

DCM. To cast these directed relationships as a graph, we connect the vocabularies that occur

simultaneously and consequentially with a single-headed arrow. For example, if the buffer states
are b(1)
1

2 are
inferred for UE 2. Graphing these sequential inference relationships for every input buffer state

1 are consequentially inferred for UE 1, and u(2)

2 , then u(1)

1 , d(2)

2 , d(4)

and b(2)

1 , aD

2 , aA

11

Fig. 3. Vocabularies and connections extracted from an NPM when N = 2 and |B1| = |B2| = 3. The bold arrows and boxes
indicate the vocabularies being activated with the current input b(1)
2 . The inference result makes UE 2 to remain silent
whereas UE 1 accesses the channel.

and b(2)

1

Fig. 4. NPM needs CM length of at least 8 output activation nodes (left), and 16 intermediate activation nodes (right) to ensure
good training results on average.

gives the NPM extract expressed as a symbolic graph in Fig. 3.

C. Potentials and Limitations of NPMs

Via overparametrization, NPMs can be trained to achieve high performance for the environment

they are grounded in, e.g. [7], [9], [21], [22]. However, overparametrization results in the

construction of inefﬁcient NPMs, which exerts excessive communication, computation, and

memory overhead during each communication cycle. The overparametrized layers and nodes

directly translate into increased storage and computation load, which makes the system less

Average RewardEpisodes (Training)Maximum Avg. RewardAverage RewardEpisodes (Training)12

energy efﬁcient and hinders low latency communication. Additionally, as shown in Fig. 1,

increased communication overhead due to longer CM is critical because it reduces the time

for an SDU to be sent, resulting in a degraded throughput. Fig. 4(a) shows the average utility

when training with different numbers of activation nodes. In particular, the number of activation

nodes at the intermediate output layers directly translates into the length of CMs. Empirically,

we observe that DQNs need at least 8 activations to achieve the average reward of 2. This means

that an NPM has the minimum control length requirement of 32 Bytes, which becomes a huge

overhead to send in every communication cycle.

Even if we use the symbolic graph version instead of a full NPM, an NPM encounters

communication and memory inefﬁciencies for larger buffer capacity and more users. In fact,

the UCM vocabulary size increases combinatorially and DCM vocabulary size increases expo-

nentially because a DCM is required for every combination of UCM from the UEs. Consequently,

the control plane complexity is increased due to the number of connections needed to explain

the relationship between all UCMs and DCMs. The communication inefﬁciency due to larger

vocabulary size, and increased control plane complexity due to the complex protocol model are

incompatible for 6G, which is projected to have a uniﬁed control plane architecture [37] with

distributed core units capable of computing at sub-1ms control plane latency [38], [39]. Thus,

it is necessary to devise methods where we can emulate the performance of an NPM while

ensuring communication and memory efﬁciency.

III. SPM: SEMANTIC PROTOCOL MODEL VIA

PROBABILISTIC LOGIC PROGRAMMING LANGUAGE

To obviate the shortcomings of an NPM, we propose the construction of an SPM. We

make SPM communication and memory efﬁcient by merging UCM and DCM vocabularies that

exhibit redundant semantics. To greatly reduce the vocabulary sizes, we combine two merging

techniques, which let us beneﬁt from the reduction capability of both merging techniques.

However, combining the two leads to problematic situations where signaling vocabularies become

polysemous, which we refer to as the polysemy problem. We resolve such problem by formulating

SPM rules based on probabilistic logic clauses and utilizing logical inference based on contextual

information stored in the empirically derived conditional probabilities.

13

A. An Overview of ProbLog and SPM Construction Procedure

We deconstruct an NPM into an SPM by extracting the logical relationships from the episodic

memory of MARL. To express the logical aspects of an SPM such as vocabularies, clauses,

predicates, and rules, and to describe an SPM’s construction procedure succinctly and effectively,

we adhere to the syntax of ProbLog [16] as follows:

(i) Vocabularies for CMs: We use four types of vocabularies denoted by bi, ui, di, ai, for input

buffer states, UCMs, DCMs, and actions of UE i, respectively. Given two vocabularies that

are connected as a causal relationship, i.e. “→”, a vocabulary is referred to as the Tail

if it is the cause of a causal relationship, and referred to as Head if it is the effect. The

vocabularies are extracted in Sec. III-B1 and merged in Sec. III-B2-III-B3.

(ii) Clauses for CM Relations A probabilistic logic clause c that connects a Head and a Tail,

i.e. “with probability p, Head is true, if Tail is true” is expressed as c = (cid:104)p :: Head :- Tail(cid:105)

in ProbLog. Each vocabulary in c could be accessed by cH = Head and cT = Tail, and the

probability is accessed by cP = p, where P denotes the clause’s probability of being true,

i.e. the truth probability. The semantic clauses are deﬁned in Sec. III-B4.

(iii) Predicates for Semantic Clause Clustering: We use four predicates isInput(·), isUCM(·),

isDCM(·), and isAction(·) that are used on the vocabularies to describe its type. An uplink

clause α is described by isInput(αT) = isUCM(αH) = true, a downlink clause β is described

by isUCM(βT) = isDCM(βH) = true, and an action clause γ is described by isDCM(βT) =

isAction(βH) = true. Subsequently, we can cluster the clauses according to the clause types

α, β, and γ. The predicates are used for clustering clauses in Sec. III-B4.

(iv) Rules and Entailment for SPM Construction : We deﬁne a rule R(·) as a sequence

of simultaneously occurring clauses that are logically consequential from an input. For

example, given a simple protocol that consists of the two clauses (cid:104)A :- B(cid:105) and (cid:104)B :- C(cid:105),

we could consequentially derive that the two clauses are true if we know that A is true.
We describe this relationship as entailment, denoted by R(A) (cid:15) (cid:104)A :- B(cid:105), (cid:104)B :- C(cid:105), where
(cid:15) means to logically entail. SPM rules are formulated in Sec. III-B5.

The SPM construction requires the following procedure, which is expressed with ProbLog for

clarity. First, we extract the vocabularies from the episodic memory of MARL. Second, we merge

the vocabularies that exhibit redundant semantics according to two schemes. Third, we deﬁne the

three logic clause types α, β, and γ that we can use for clustering the logical clauses according

14

to the operation it entails such as uplink, downlink, and action. Fourth, we formulate SPM rules

R(·) that act as the smallest element of SPM operation. Lastly, an SPM S is constructed as the

set of all rules that are formulated from each input buffer state.

B. SPM Construction

Fig. 5 illustrates the SPM merging process needed for constructing a communication and

memory efﬁcient SPM. The main idea is to merge the UCM and DCM vocabularies if they exhibit

semantic redundancy in terms of activations and connections. This merging process results in

fewer UCM and DCM bits to be transmitted for each communication cycle, and a reduction

of clauses required for expressing a model. Despite its efﬁciency, merging CMs, which contain

inter-UE semantics, introduces the cases where the DCMs become polysemous. We resolve this

issue by exploiting contextual information embedded in the empirical distributions of UCM and

DCM realizations.

1) Vocabulary Extraction: We deﬁne the input state, UCM, DCM, and action vocabularies as

possible outputs from the NPM generators (5)-(7), i.e. bi ∈ Bi, ui = gU
Di, and i ∈ {1, 2} and ai = gA

i (u) ∈
i (di) ∈ Ai for i ∈ {1, 2}. First of all, the vocabularies we need to
consider can be restricted to those extracted from the episodic memories within the experience

i (bi) ∈ Ui, di = gD

replay buffer. To elaborate, assuming that the memory of the experience replay is available, we

can consider the input states experienced in the memory, i.e. B1 and B2, to be the feasible domain

that is plugged into (5). This is because the other states are very unlikely to be traversed. Even

when the replay memory is unavailable, we can obtain one by running a few test trials. The rest

of the procedure merges these vocabularies according to their semantics, and embeds them into

SPM clauses that contain the semantic information regarding their relationships.

2) Activation-Aware Vocabulary Merging: As Fig. 5(c) illustrates, we merge the CM vo-

cabularies by using their activation pattern, i.e. the location of non-zero elements in the ac-

tivation vector extracted by vi = θ(vi), where we assume ReLU activations, and θ(vi) is the
Heaviside step function θ(x) = 1{x>0} applied to each decimal number within the vocabu-

lary vi. For example, suppose that v1 is the vocabulary expressed with 8 decimal numbers:

[0.382, 4.292, 0, 0, 1.249, 0, 0, 0]. The activation pattern of v1 is the vocabulary [1, 1, 0, 0, 1, 0, 0, 0].

Without activation and parameter quantization techniques [40], [41], the performance of NPMs

depends heavily on the precision of the activation signals. Nevertheless, from empirical ob-

servations, we hypothesize that there is the possibility of reducing the vocabularies greatly by

15

Fig. 5. Transformation process: blurred vocabulary are merged; (a) the original SPM is (b) merged according to their activation
patterns, and (c) merged according to their vocabulary connections.

exploiting the activation patterns. Consequently, we merge the UCM and DCM vocabularies
denoted by u(k1)

, respectively, for all UCMs index by k1 and k2, and DCMs

and d((cid:96)1)

, u(k2)
i

, d((cid:96)2)
i

i

i

indexed by (cid:96)1 and (cid:96)2, if they have the same activation pattern as follows:

(Activation-aware UCM merging) u(k(cid:48))

i ←− u(k1)

i

, u(k2)
i

,

if θ(u(k1)

i

) = θ(u(k2)

i

),

(Activation-aware DCM merging) d((cid:96)(cid:48))

i ←− d((cid:96)1)

i

, d((cid:96)2)
i

,

if θ(d((cid:96)1)

i

) = θ(d((cid:96)2)

i

),

(8)

(9)

By using fewer vocabulary for UCM and DCM messages, the logical rules become overlapped

with each other, as shown in Fig. 5(b). Because of the other UE’s impact on the decision of a

DCM, the merging process makes the choice of some actions arbitrary due to polysemous DCMs,

in contrast to the deterministic choices made by the NPM. To disambiguate the polysemous

vocabularies, we devise a method to determine which CMs and actions have more contextual

meaning for the current operation, by exploiting their likelihood represented by the clauses’

conditional probabilities.

3) Connection-Aware Merging: To further increase communication efﬁciency, we merge the

DCMs, UCMs, and clauses altogether, identifying cases when the connected vocabularies are
identical, as illustrated in step (b) of Fig. 5. Firstly, the DCM vocabularies denoted by d((cid:96)1)
and d((cid:96)2)

are merged when the set of action vocabularies they are connected to are identical.

i

i

The following update rule is applied for all DCMs indexed by (cid:96)1 and (cid:96)2 to merge the DCM

vocabulary and its connections:

(Connection-aware DCM merging) d((cid:96)(cid:48))

i ←− d((cid:96)1)

i

, d((cid:96)2)
i

,

if A((cid:96)1)

i = A((cid:96)2)

i

,

(10)

where A((cid:96))
and u(k2)

i = ai}. Secondly, the UCM vocabularies denoted by u(k1)
i = {ai ∈ Ai|γT
are merged when the set of DCM vocabularies they are connected to are identical.

i = d((cid:96))

, γH

i

i

i

The following update rule is applied for all UCMs indexed by k1 and k2 to merge the UCM

vocabulary and its connections:

16

(Connection-aware UCM merging) u(k(cid:48))

i ←− u(k1)

i

, u(k2)
i

,

if D(k1)

i = D(k2)

i

,

(11)

where D(k)

i = {di ∈ Di|βT

i = di}. Applying connection-aware merging to UCMs
reduces the UCM vocabulary further, but it makes the UCMs polysemous. Again, to resolve

, βH

i

i = u(k)

the polysemy problem, we embed additional semantics within the downlink clauses via the

conditional probability and utilize them with the SPM rule and operation.

4) Clause Deﬁnition and Semantic Clustering: To construct an SPM clause, we need to

decide the type of clauses we will construct, dictating the logical relationships of interest as

follows. Technically, we could construct a clause that explains the relationship between any

activation layer to any other activation layer, but we only focus on the layers corresponding to

the vocabularies extracted above. We deﬁne the clause sets Ai, Bi,j, Γi ∀i, j ∈ {1, 2} to describe

the logical relationships of interest. The uplink clause set Ai contains clause αi, named uplink

clause, which connects UE i’s input buffer state bi to its UCM ui. The downlink clause set Bi

contains clause βi, named downlink clause, which connects the UCM uj from UE j ∈ {1, 2}

to the DCM di that will be sent to UE i ∈ {1, 2}. The action clause set Γi contains clause γi,

named action clause, which connects UE i’s DCM di to its action ai.

To resolve the polysemy problem, we construct SPM clauses that express the level of truth

in a logical connection. We use ProbLog to express the uplink, downlink, and action clauses as

follows:

(Uplink clause)

αi = (cid:104)1 :: ui :- bi(cid:105) ,

(Downlink clause)

βi,j = (cid:104)Pr(di|uj) :: di :- uj(cid:105) ,

(Action clause)

γi = (cid:104)Pr(ai|di) :: ai :- di(cid:105) ,

(12)

(13)

(14)

where αi ∈ Ai, βi,j ∈ Bi,j, γi ∈ Γi, ui ∈ Ui, di ∈ Di, ai ∈ Ai, and i, j ∈ {1, 2}, and

the conditional probabilities are calculated empirically by simulating over the domain given

by the episodic memory. The probability is 1 for (12) because bi and ui are always selected

simultaneously; the conditional probability of (13) is empirically calculated as the following

17

Fig. 6. An example of rules that entail the sequential logical connections of α, β, and γ. The clauses in the example are
deterministic for simplicity.

Fig. 7. An example of SPM when N = 2 and B = 3, which is the transformed version of Fig. 3. The bold arrows and boxes
indicate the decisions made using the maximum truth probability for input b(1)
2 . The operation result makes UE 1
remain silent whereas UE 2 accesses the channel.

and b(2)

1

ratio: the number of times uj and di are simultaneously selected divided by the total number

of times uj is selected; lastly, the conditional probability of (14) is calculated as the following

ratio: the number of times di and ai are simultaneously selected divided by the total number of

times di is selected.

5) SPM Rule Formulation and SPM Construction: We construct an SPM rule, which is the

smallest building block of an SPM, that entails a logical sequence of clauses that connects the

UCM, DCM, and action vocabularies that are causal from an input buffer state, as illustrated

in Fig. 6. Speciﬁcally, plugging in a particular bi to the NPM, we can obtain the selected

vocabularies that correspond to the heads and tails of α, β, and γ that occur simultaneously.

To express this consequential relationship, we deﬁne an SPM rule for the action of UE i that

originates from UE j’s state as the following set-valued function that entails the clauses as

follows:

18

(15)

, ..., γ(L,M )
i

Ri,j(b) (cid:15) αj, β(1)

i,j , ..., β(L)

i,j , γ(1,1)

i

where the conditions for αj, β((cid:96))
and m = 1, ..., M as follows:

i,j , γ((cid:96),m)

i

entailed by Ri,j(b) must be met for all (cid:96) = 1, 2, ..., L

αT

j = bj,

j = β((cid:96))
αH

i,j

T

= uj, ∀(cid:96) = 1, 2, ..., L

H

β((cid:96))
i,j

T

= γ((cid:96),m)
i

= di, ∀m = 1, ..., M,

H

γ((cid:96),m)
i

= a(m)
i

,

(16)

(17)

(18)

(19)

where β((cid:96))
i,j

P

> 0, γ((cid:96),m)

i

P

> 0, and the vocabulary belonging to the same clause is highlighted

with the same color. The conditions (16)-(19) describe the consequential relationship between

the clauses. In (16), the state b ﬁrst entails a clause αj that has b as its tail; in (17), the UCM
that has UCM uj as its tail; in (18), the DCM of each β((cid:96))
uj of the clause αj, entails all β((cid:96))
i,j
i,j
that has the same DCM as β((cid:96))
entails all γ((cid:96),m)
entails an action am
i

i,j ; lastly, in (19), each γ((cid:96),m)

i

i

that is implicated with a non-zero probability. An SPM is constructed as the set of all rules that

originate from all input buffer state experienced in the episodic memory. Let the SPM rules be

deﬁned with (15), the SPM is constructed as follows:

S =

(cid:91)

i,j,b

Ri,j(b),

(20)

where b ∈ B1 × B2 and i, j ∈ {1, 2}.

C. SPM Design Motivation and Empirical Justiﬁcation

For communication and memory efﬁciency, we have constructed the SPM after merging

the vocabularies and connections that exhibit redundant semantics. The total reduction of the

vocabulary size can be seen by comparing the graph of an SPM and the blurred graph of the

original NPM extracted in Fig. 7. To visualize both positive and negative effect of the merging

schemes, we use the t-distributed stochastic neighbor embedding (t-SNE) [42] graphs of the

UCMs and DCMs pairs, i.e. u and d, respectively, as illustrated in Fig. 8. Fig. 8(a) shows the

t-SNE graphs of vocabularies before their merger, which shows that there are multiple UCM

19

Fig. 8. The t-distributed stochastic neighbor embedding (t-SNE) of concatenated UCM and DCM vocabularies (a) before merging,
(b) after activation-aware merging (A-merging), (c) after connection-aware merging (C-merging), and (d) after A-merging and
C-merging. The colors indicate the semantics dictated as the action pair [a1, a2] of the two UEs taken at that state; and the
perplexity parameter is set to 15.

and DCM clusters exhibiting identical semantics, expressed by identical actions pair [a1, a2] and

indicated by the same color. After activation-aware merging, we observe that UCM clusters with

the same semantics are merged, but some of the DCM clusters with different semantics exhibits

semantic polysemy. On the other hand, connection-aware merging shows promising clustering of

DCMs, but introduces severe polysemy for UCMs. Lastly, combining both schemes signiﬁcantly

merges the vocabularies further, but the polysemy occurs for both UCMs and DCMs. The results

suggest that there is a minimum number of UCM vocabularies and DCM vocabularies required

to express the protocol without any polysemy. However, further reduction gain could be achieved

if polysemous vocabularies can be disambiguated.

D. SPM Operation

To disambiguate the polysemous vocabularies, we utilize the conditional probabilities that

contain contextual information regarding the environment. To express the level of conﬁdence in

the selection of a vocabulary, the truth probability of the CMs and actions for UE i are calculated

DCMs with differentsemantics get merged= polysemousvocabulariesUCMs with redundantsemantics get mergedUCMsDCMs(a) Before merging.(b) A-merging.(c) C-merging.(d) A&C-merging. More polysemousvocabulariesUCMs with differentsemantics get mergedDCMs with redundantsemantics get mergedMore polysemousvocabulariesas follows when the entailment at state b is given by Ri,i(b) (cid:15) αi, βi,i, γi and Ri,j(b) (cid:15) αj, βi,j, γi:

20

Pr(ui|S, b) = αP
i ,

Pr(di|S, b) = βP

i,iβP
i,j,

Pr(ai|S, b) = γP
i ,

(21)

(22)

(23)

where i (cid:54)= j ∈ {1, 2}. A higher truth probability indicates higher conﬁdence in the selection of

a particular vocabulary for the current context S and b. Note that (22) is the joint probability of

the event that the same DCM is implicated by the UCMs from UE i and j; also, the equality for

the second equation holds, because we assume that the probabilities βP

i,j are independent.
Inspired by RL principles, which transform the soft Q-values to hard decisions, we select the

i,i and βP

CMs and actions according to the maximum truth probability as follows:

(SPM UCM) ui = αH
i ,

(SPM DCM) di = arg max

Pr(d|S, b),

d∈Di

(SPM Action) ai = arg max

a∈Ai

Pr(a|S, b),

(24)

(25)

(26)

where i, j ∈ {1, 2}. The advantages of using the vocabulary with the maximum truth probability

are twofold: the computation cost from random sampling is saved, and the operation becomes

certain, removing any uncertainty, which is an important trait for a protocol.

Notwithstanding the amount of communication efﬁciency brought by the schemes explained

above, it is best if actions could be taken without a grant given by the DCMs. We enable grant-

free communication by comparing the SPM rules, which is referred to as rule-aware grant-free
communication. When the SPM rule Ri,i(bi, bj) is unaffected by bj, j (cid:54)= i, i.e. Ri,i(bi, b(1)
· · · = Ri,i(bi, b(|Bj |)
the fourth clause type deﬁned as follows:

j ) =
), we skip the DCM and take an action in a grant-free manner according to

j

(Grant-free clause)

δi = (cid:104)1 :: ai :- bi(cid:105) ,

(27)

where ai = γH

i . Compared with our approach that enables grant-free communication with
semantic vocabularies using the logical relationship, a tabular RL-based approach in [43] uses

predeﬁned signaling messages to learn to skip the ACK signal when (cid:15) is low, and the deep

RL-based approach in [7] learns emergent signaling messages but the DCM cannot be skipped

21

because of the intrinsic NN structure.

Inspired by natural language based on pragmatics such as contextual meaning to disambiguate

polysemous vocabularies, we have disambiguated the UCMs and DCMs via probabilistic logic-

based inference in (21)-(26). The results from Sec. V will corroborate the probabilisitic logic in-

ference’s disambiguation capability. As a summary of the efforts in this section, Table I compares

the measured KPIs of an SPM compared with an NPM, such as the goodput, vocabulary sizes,

memory requirement, and so forth. The reduced CM length and model storage is consequential

to the reduction gain achieved by the merging process, and the measurements in Fig. 2 are

empirically derived from an actual NPM and its SPM transformation.

IV. ATTRIBUTES AND POTENTIALS OF SPM

Classical protocols are designed to meet the requirements for a general purpose, whereas NPM

is trained to perform well in a speciﬁc environment. Inspired by both classical protocols and

emergent NPM, an SPM exhibits three major potentials that enable its wider usage: reconﬁg-

urability, measurability, and compactness. In this section, we explain each potential and provide

an example application that exploits each potential.

A. Reconﬁgurability

The classical protocols can be reconﬁgured to address changes in the environment or system

requirement, but an NPM should be retrained with the hyperparameters set accordingly to the

new constraint. This process could be taxing because hyperparameters need to be reoptimized,

and NPMs need many trials to converge to an optimal solution. Inspired by classical protocols,

an SPM can be reconﬁgured to perform well for a new constraint, by manipulating the logical

relationships based on vocabularies, clauses, and rules. As a general principle for manipulation,

we need to make the least manipulation steps as possible, to maintain the performance of the

original SPM. For example, consider that we want to reduce collisions. We can manipulate the
action clause γi ∈ Γi to reconﬁgure UE i’s action to another action. Let γ((cid:96),A)
clause that is entailed by b, which leads to a collision. We can achieve the desired reconﬁguration

be the action

i

while making the least manipulation to the SPM by updating the SPM as follows:

(Si \ γi) ∪ (cid:104)γP
i

:: aS
i

:- di(cid:105),

(28)

22

where Ri,i(b) (cid:15) αi, β((cid:96))
stay silent instead of its original decision to access at b, hence avoiding collisions.

i = di, and the reconﬁguration makes UE i to

i,i , γ((cid:96),A)

i and γT

i = aA

; γH

i

B. Measurability

Multiple protocols can be constructed from a stationary channel environment, due to the

characteristic of RL, which causes non-static channel ﬂuctuations and random realization of

SDU arrivals. Thus, a method is needed to attain a consensus on which protocol should be

used for the current environment. Inspired by the information theoretical metrics that evaluate

classical protocols, we evaluate multiple SPMs based on semantic information theory. This is

possible because SPM gains structural variability from the merging process of its vocabularies.

On this note, we measure the level of randomness within a clause with the following deﬁnition

of semantic entropy [27]:

H(c) = −{cP log(cP) + (1 − cP) log(1 − cP)},

(29)

where cP = 1 for an extracted NPM, and H(c) is deﬁned as 0 for cP = 0. Due to NPM’s

deterministic structure, it has a zero net semantic entropy. However, its structural complexity and

excessive vocabularies due to the overparametrization makes it impractical to use. Consequently,

extensive simulation runs are required to obtain an understanding of its operation or the task’s

complexity [44]. In contrary, an SPM has a compact structure; and thanks to theoretical metrics,

it can be evaluated without running costly and time-consuming simulation runs. For example,

we can evaluate the net randomness within the SPM with the net model entropy:

Hnet(S) =

H(c).

(cid:88)

c∈S

(30)

Hnet is insightful due to its strong correlation to the UCM and DCM vocabulary sizes |D| and

|U|, respectively, and the downlink and action clause sets’ cardinality |Bi| and |Γi|, respectively.

We can also calculate the partial net entropies for downlink clauses β and action clauses γ by the
following: Hβ

net(S) = HSPM(S ∩Γ). These metrics measure the net

net(S) = HSPM(S ∩B), and Hγ

entropy pertaining to a speciﬁc clause type, which let us differentiate which logical connection
type has more polysemous behavior within the protocol. For a consensual purpose, Hβ
used by the BS to achieve a simpler control plane function, and Hγ

net can be used by the UEs

net can be

to lower the variance of its actions. Lastly, by utilizing the SPM entropy, we can measure the

23

task complexity by ﬁnding the minimum Hnet(S ψ) value among the protocols {S 1, ..., S ψ}:

min
ψ

Hnet(S ψ).

(31)

The rationale behind this metric is that complex tasks require more vocabularies and connections

to express the divergent situations present in the task.

C. Compactness

So far, the communication environment parameters remained stationary, which we now relax

and consider a non-stationary environment to address more realistic situations. Classical protocols

are designed for a general purpose, so it may suffer from a non-stationary environment that

deviates from the general purpose. On the other hand, NPM can be trained for a speciﬁc

environment, but keeping and loading multiple models cause heavy memory overhead due to

the overparametrization of NNs. In contrary, SPM’s compactness makes it easy to store and

compute. For example, consider an environment where UEs have periods of bursty SDUs, UEs

leave and enter, or the wireless channel deteriorates over time. To cope with these non-stationary

environment, keeping multiple compact SPMs as a portfolio grant model diversity and ensemble

gain [45], [46].

V. SIMULATION RESULTS AND DISCUSSION

In this section, communication related hyperparameters are tuned to reﬂect a realistic com-

munication scenario. We set λ1 = λ2 = 0.5, bmax = 5, Dmax = 12, ρ1 = ρ2 = 5, T = 24, and

(cid:15) = 0.02 to model a situation where two UEs are contending for a noisy channel with equal

SDU payload, and a BS capable of serving both UE’s data stream successfully if the protocol is

effective. We use the Pytorch library to implement NPMs, and we extend the Python ProbLog

library to implement SPMs. The Huber loss function [47] is chosen thanks to its robustness

against outliers, reducing the ﬂuctuation magnitude of DQN training; Adam optimizer is used

with the initial learning rate of 0.0001, the ﬁrst and second exponential decay rate of 0.9 and

0.999, and epsilon as 10−7 for numerical stability. As additional KPIs to measure the contention

performance, we denote nC and nD as the number of collisions and discarded SDUs, respectively.

A. Comparison with NPM and Slotted ALOHA

Fig. 9 compares the emulation performance of an SPM. Fig. 9(a) and (b) plot the second

Q-value element that corresponds to the level of inclination of the UE to access the channel.

24

Fig. 9. NPM vs. SPM: the operation results of UEs and the policy maps of (a) NPM, (b) SPM. Given a buffer state, NPM
returns the Q-values for each action; then, the action with the highest Q-value is selected. An SPM compares the truth probability
of the actions given the input state, then chooses the action with the highest value. Plotting the decision regions for all input
states gives the policy maps.

Similarly, for an SPM, the truth probability of action ai displays the level of inclination to access,

because it is the value that is compared with the other actions to decide whether to access or not.

The Q-values are well imitated by the SPMs’ truth probabilities for the access action in Fig. 9(a)

and (b). For bmax = 5, the policy emulation is correct for 97.22% of the states. Minor deviation

occurred when b1 = b2 = 0, which is acceptable because the decision when both buffers are

empty is negligible, which is corroborated by the identical average goodput performance 0.729

at λ = 0.5 and (cid:15) = 0.02.

In Fig. 10, we check the contention performance of the SPMs compared to the conventional

protocols. SPM’s performance is compared with the performance of random access schemes

such as slotted ALOHA (S-ALOHA) with access probability p = 0.5 [48], and S-ALOHA with

binary exponential backoff (BEB) with base 2 and adverse collision event. At λ = 0.5, SPM’s

nR is greater than S-ALOHA and S-ALOHA (BEB) by 182.2% and 206.7%; and at (cid:15) = 0.01

Value forSilenceValue forAccessValue forDiscardUE1UE2UE1UE2(a) NPM.(b) SPM.Q-ValueQ-ValueQ-ValueTruth ProbabilityTruth ProbabilityTruth Probability25

Fig. 10. SPM vs. S-ALOHA: comparison of the contention performance of S-ALOHA, S-ALOHA with binary exponential
backoff (BEB), and SPM: nC , nR, and nD at varied λ and (cid:15).

by 161.5% and 204.7%. This is due to the NPM counterpart’s performance grounded on the

environment, the SPM’s intrinsic ability to emulate such behavior, and logical operations that

obviate the polysemy problem. To verify the performance across diverse environments, we vary

the packet arrival rate λ and block error rate (cid:15). At each environment, the test is repeated 10

times. The performance gap between SPM and random access schemes is evident except for

when (cid:15) approaches 1, where all SDUs are inevitably lost due to bad channel conditions. SPM

shows improved performance regarding the KPIs nC, nR, and nD throughout other environments.

Averaging over all environments, SPM’s nR is greater than that of S-ALOHA and S-ALOHA

(BEB) by 142.8% and 184.4%; nD is reduced to 52.7%, 47.0%; and nC is reduced to 26.4%

and 50.9%.

B. Collision-Free SPM

With simple manipulation steps, an SPM can be adjusted to perform well under a non-

stationary environment where a new constraint such as collision avoidance should be con-

sidered. Let n∗

lision occurs when the actions aA
1 , aA

C be the number of collisions that the system should not exceed. For b, col-
2 are chosen simultaneously, which occurs with the
2 |S, b). We decide to manipulate γ for b when

2 |S, b) = Pr(aA

probability Pr(aA

1 |S, b)Pr(aA

1 and aA

# of Collisions (𝑛𝑐)# of Received Packets (𝑛𝑅)# of Discarded Packets (𝑛𝐷)Packet Arrival Rate (𝜆)Block Error Rate (𝜖)Packet Arrival Rate (𝜆)Packet Arrival Rate (𝜆)Block Error Rate (𝜖)Block Error Rate (𝜖)# of Collisions (𝑛𝑐)# of Received Packets (𝑛𝑅)# of Discarded Packets (𝑛𝐷)S-ALOHA p=0.5S-ALOHA (BEB)SPMS-ALOHA p=0.5S-ALOHA (BEB)SPMS-ALOHA p=0.5S-ALOHA (BEB)SPMS-ALOHA p=0.5S-ALOHA (BEB)SPMS-ALOHA p=0.5S-ALOHA (BEB)SPMS-ALOHA p=0.5S-ALOHA (BEB)SPM26

Fig. 11. The results of manipulation for collision avoidance compared with the original SPM’s and classical protocols’
performance.

Pr(aA

1 , aA

2 |S, b) > pth, where pth is the collision avoidance threshold set to satisfy n∗
C. To avoid
collision for the input while making the least change to the protocol, we manipulate the SPM

of the UE with lower access probability to remain silent instead, and choose an arbitrary UE

for a tie. Suppose that UE i has the lower access probability, we manipulate the γH
i

from aA
i
i as explained in Section IV. Repeating this for all input states that exceeds pth ultimately
manipulates an SPM to become collision-free. Fig. 11 shows the performance after manipulation

to aS

for collision avoidance. With manipulations on actions that caused collisions, nC approaches 0

for SPM after the manipulation. Only two steps of manipulation were required, which is much

more efﬁcient and scalable than manipulating an SPM, or retraining an NPM. The manipulation

also resulted in a marginal improvement for nR, i.e. 107.21% improvement on average for all

environments, which shows that the manipulation’s net impact on the SPM was positive.

C. Consensual SPM in a Stationary Environment

Based on the theoretical evaluation tools from Sec. IV, we can select one model among

multiple models with simple evaluation on the protocol structure, without running performance

tests. Fig. 12(a) compares the average rewards of three different selection schemes. Given a set

of NS SPM samples, the random selection scheme chooses one SPM randomly from the set;

min |U| + |D| scheme chooses the SPM that uses the least UCM and DCM vocabularies; and

the min Hnet scheme chooses the SPM with the smallest net entropy. In the experiment, we

experiment with 200 SPMs, and NS SPMs are sampled from them. 3000 trials are repeated at

each NS. The results show that the net SPM entropy is the best metric for getting a consensual

SPM among the three, because it selects the well-performing SPM from smaller sample sizes.

# of Collisions (𝑛𝑐)# of Received Packets (𝑛𝑅)Packet Arrival Rate (𝜆)Packet Arrival Rate (𝜆)SPM Manipulated SPMS-ALOHA p = 0.5S-ALOHASPM Manipulated SPMS-ALOHA p = 0.5S-ALOHA27

(a) Protocol selection based on KPI.

(b) Adaptation to non-stationary environment.

Fig. 12.
(a) comparison of the average reward and its standard deviation for three selection schemes: choosing one SPM
randomly, choosing the SPM with minimum net entropy, and choosing the SPM with the least number of vocabularies. (b)
comparison of the average reward of deep RL in a stationary environment, and two schemes in a non-stationary environment,
which are continual learning and SPM portfolio.

Selecting according to the minimum net entropy gives an average reward that converges to 3.84

around NS = 20 with the standard deviation of 0.22, and choosing for the minimum vocabulary

size gives an average reward that converges to 3.84 around NS = 60, whereas random selection

scheme’s average reward stays between 1.5 and 1.8 for all sample sizes, with the average standard

deviation of 1.86.

D. Portfolio SPM for a Non-Stationary Environment

Thanks to the compactness of SPMs, we can maintain a portfolio of SPMs for a non-stationary

environment where communication parameters change frequently. To verify the robustness of an

SPM portfolio, we compare it with a continual learning based NPM over a changing environment.

Fig. 12(b) compares two schemes that try to adapt to a frequently changing environment, and

an NPM that is trained for a stationary environment, to provide a baseline. In the simulation,

we try to model a non-stationary environment with UEs that have incoming bursts of SDUs by

a two-state Markov chain, where in one state UE 1 has λ = 0.9 and UE 2 has λ = 0.1, and

in the other state, the packet arrival rates are reversed; the state transition probability is 0.8 for

both states. In such a dynamic environment, a protocol that prioritizes only one UE to transmit

regardless of the other UE suffers when the environment state is changed. The red triangle graph

shows how the training would have progressed if the environment was stationary, with each UE

having λ = 0.5; the blue circled graph shows the test performance of a protocol being adjusted

by continual learning, which retrains at the current environment, starting from the NPM of the

28

previous state. The purple diamond graph shows the test performance of an SPM portfolio, with

SPMs constructed for the two alternating environments. From the ﬁgure, it can be observed

that continual learning leads to detrimental results, i.e. average reward being less than 0 for

74.3% of the communication cycles, which is due to catastrophic forgetting [49], and learning

being stuck at bad local minima when the environment changes frequently. In contrast, an SPM

portfolio never falls below the average reward of 2.63 because the portfolio contains a model

that is constructed for each environment.

VI. CONCLUSION

In this work, we proposed a novel MAC protocol model extracted and symbolized from an

NN-based NPM, coined an SPM. The SPM turns the black-box NN into a directed graph or a

collection of semantic clauses written in ProbLog, a logic programming language for symbolic

AI. The SPM is therefore interpretable by both humans and machines, and instantly reconﬁg-

urable. Furthermore, the SPM is measurable via semantic entropy, and compact occupying only

0.02% of the memory compared to its original SPM, thereby enabling best SPM selection and

SPM portfolio applications. There are several intriguing future problems such as extensions to

dynamic spectrum access, viewing this problem from a privacy standpoint, and exploiting SPMs

for CM error correction. Furthermore, while we focus only on adjusting SPMs, it could be an

interesting topic to jointly design NPM learning and SPM construction.

REFERENCES

[1] K. Chen, M. Ma, E. Cheng, F. Yuan, and W. Su, “A survey on MAC protocols for underwater wireless sensor networks,”

IEEE Commun. Surveys Tuts., vol. 16, no. 3, pp. 1433–1447, 2014.

[2] M. B. Shahab, R. Abbas, M. Shirvanimoghaddam, and S. J. Johnson, “Grant-free non-orthogonal multiple access for IoT:

A survey,” IEEE Commun. Surveys Tuts., vol. 22, no. 3, pp. 1805–1838, 2020.

[3] M. Polese, J. M. Jornet, T. Melodia, and M. Zorzi, “Toward end-to-end, full-stack 6g terahertz networks,” IEEE Commun.

Mag., vol. 58, no. 11, pp. 48–54, 2020.

[4] M. Giordani and M. Zorzi, “Non-terrestrial networks in the 6g era: Challenges and opportunities,” IEEE Netw., vol. 35,

no. 2, pp. 244–251, 2021.

[5] M. Noor-A-Rahim, Z. Liu, H. Lee, M. O. Khyam, J. He, D. Pesch, K. Moessner, W. Saad, and H. V. Poor, “6g for

vehicle-to-everything (v2x) communications: Enabling technologies, challenges, and opportunities,” Proc. IEEE, vol. 110,

no. 6, pp. 712–734, 2022.

[6] M. Maier, A. Ebrahimzadeh, S. Rostami, and A. Beniiche, “The internet of no things: Making the internet disappear and

”see the invisible”,” IEEE Commun. Mag., vol. 58, no. 11, pp. 76–82, 2020.

[7] M. P. Mota, A. Valcarce, J.-M. Gorce, and J. Hoydis, “The emergence of wireless MAC protocols with multi-agent

reinforcement learning,” 2021. arXiv:2108.07144. [Online]. Available: https://arxiv.org/abs/2108.07144.

29

[8] J. Hoydis, F. A. Aoudia, A. Valcarce, and H. Viswanathan, “Toward a 6g ai-native air interface,” IEEE Commun. Mag.,

vol. 59, no. 5, pp. 76–81, 2021.

[9] S. Han, T. Xie, C.-L. I, L. Chai, Z. Liu, Y. Yuan, and C. Cui, “Artiﬁcial-intelligence-enabled air interface for 6g: Solutions,

challenges, and standardization impacts,” IEEE Commun. Mag., vol. 58, no. 10, pp. 73–79, 2020.

[10] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to communicate with deep multi-agent reinforcement

learning,” Advances in neural inf. process. syst., vol. 29, 2016.

[11] A. Achille, G. Paolini, and S. Soatto, “Where is the information in a deep neural network?,” 2019. arXiv:1905.12213.

[Online]. Available: https://arxiv.org/abs/1905.12213.

[12] E. Tulving, Organization of memory. Academic Press, 1972.

[13] J. Pearl, “The seven tools of causal inference, with reﬂections on machine learning,” Commun. ACM, vol. 62, pp. 54–60,

Feb. 2019.

[14] D. Klein and G. Murphy, “The representation of polysemous words,” J. Memory Language, vol. 45, no. 2, pp. 259–282,

2001.

[15] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng, “Improving word representations via global context and multiple

word prototypes,” in Proc. Annu. Meeting ACL (Volume 1: Long Papers), pp. 873–882, 2012.

[16] H. T. Luc De Raedt, Angelika Kimmig, “Problog: a probabilistic prolog and its application in link discovery,” in Proc.

IJCAI, pp. 2468–2473, January 2007.

[17] N. Zenia, M. Aseeri, M. Ahmed, Z. Chowdhury, and M. Shamim Kaiser, “Energy-efﬁciency and reliability in mac and

routing protocols for underwater wireless sensor network: A survey,” J. Netw. Comput. Appl., vol. 71, pp. 72–85, 2016.

[18] S. Kim, H. Cha, J. Kim, S.-W. Ko, and S.-L. Kim, “Sense-and-predict: Harnessing spatial interference correlation for

cognitive radio networks,” IEEE Trans. Wireless Commun., vol. 18, no. 5, pp. 2777–2793, 2019.

[19] K. Shaﬁque, B. A. Khawaja, F. Sabir, S. Qazi, and M. Mustaqim, “Internet of things (iot) for next-generation smart

systems: A review of current challenges, future trends and prospects for emerging 5g-iot scenarios,” IEEE Access, vol. 8,

pp. 23022–23040, 2020.

[20] Z. Amjad, A. Sikora, B. Hilt, and J.-P. Lauffenburger, “Low latency v2x applications and network requirements:

Performance evaluation,” in Proc. IEEE Intell. Veh. Symp, pp. 220–225, 2018.

[21] Z. Liu and I. Elhanany, “RL-MAC: A qos-aware reinforcement learning based MAC protocol for wireless sensor networks,”

in Proc. IEEE Int. Conf. Netw. Sens. and Control, pp. 768–773, 2006.

[22] J.-H. Lee, H. Seo, J. Park, M. Bennis, and Y.-C. Ko, “Learning emergent random access protocol for leo satellite networks,”

2021. arXiv:2112.01765. [Online]. Available: https://arxiv.org/abs/2112.01765.

[23] H. B. Pasandi and T. Nadeem, “MAC protocol design optimization using deep learning,” in Proc. Int. Conf. Artif. Intell.

Inf. Commun. (ICAIIC), pp. 709–715, 2020.

[24] C. K. Thomas and W. Saad, “Neuro-symbolic artiﬁcial intelligence (ai) for intent based semantic communication,” 2022.

arXiv:2205.10768. [Online]. Available: https://arxiv.org/abs/2205.10768.

[25] E. Calvanese Strinati and S. Barbarossa, “6g networks: Beyond shannon towards semantic and goal-oriented communica-

tions,” Comput. Netw., vol. 190, p. 107930, 2021.

[26] Z. Weng, Z. Qin, and G. Y. Li, “Semantic communications for speech signals,” in Proc. IEEE Int. Conf. Commun., pp. 1–6,

2021.

[27] J. Choi, S. W. Loke, and J. Park, “A uniﬁed approach to semantic information and communication based on probablistic

logic,” 2022. arXiv:2205.00621. [Online]. Available: https://arxiv.org/abs/2205.00621.

[28] H. Seo, J. Park, M. Bennis, and M. Debbah, “Semantics-native communication with contextual reasoning,” 2021.

arXiv:2108.05681. [Online]. Available: https://arxiv.org/abs/2108.05681.

30

[29] P. Jackson, Introduction to expert systems. US, Jan. 1986.

[30] R. Calegari, G. Ciatto, V. Mascardi, and A. Omicini, “Logic-based technologies for multi-agent systems: a systematic

literature review,” Autonomous Agents and Multi-Agent Syste., vol. 35, Oct. 2020.

[31] T. Finin, R. Fritzson, D. McKay, and R. McEntire, “KQML as an agent communication language,” in Proc. Int. Conf.

Inform. Knowl. Manag., ACM Press, 1994.

[32] R. Manhaeve, S. Dumanˇci´c, A. Kimmig, T. Demeester, and L. De Raedt, “Neural probabilistic logic programming in

deepproblog,” 2019. arXiv:1907.08194. [Online]. Available: https://arxiv.org/abs/1907.08194.

[33] V. Bioglio, C. Condo, and I. Land, “Design of polar codes in 5g new radio,” IEEE Commun. Surveys Tuts, vol. 23, no. 1,

pp. 29–40, 2021.

[34] F. A. Oliehoek, M. T. J. Spaan, and N. Vlassis, “Optimal and approximate q-value functions for decentralized POMDPs,”

J. Artif. Intell. Research, vol. 32, pp. 289–353, May 2008.

[35] W. Wang, Q. Wang, and K. Sohraby, “Multimedia sensing as a service (msaas): Exploring resource saving potentials of

at cloud-edge iot and fogs,” IEEE IoT J., vol. 4, no. 2, pp. 487–495, 2017.

[36] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.

[37] S. Wang, T. Sun, H. Yang, X. Duan, and L. Lu, “6g network: Towards a distributed and autonomous system,” in Proc.

6G Wireless Summit, pp. 1–5, 2020.

[38] G. Liu, Y. Huang, N. Li, J. Dong, J. Jin, Q. Wang, and N. Li, “Vision, requirements and network architecture of 6g mobile

network beyond 2030,” China Commun., vol. 17, no. 9, pp. 92–104, 2020.

[39] N. Kato, B. Mao, F. Tang, Y. Kawamoto, and J. Liu, “Ten challenges in advancing machine learning technologies toward

6g,” IEEE Wireless Commun., vol. 27, no. 3, pp. 96–103, 2020.

[40] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural network with pruning, trained quantization

and huffman coding,” in Proc. Int. Conf. Learning Representations (ICLR 2016), May 2016.

[41] Z. Cai, X. He, J. Sun, and N. Vasconcelos, “Deep learning with low precision by half-wave gaussian quantization,” in

Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), July 2017.

[42] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.,” J. Mach. Learning Research, vol. 9, no. 11, 2008.

[43] A. Valcarce and J. Hoydis, “Toward joint learning of optimal MAC signaling and wireless channel access,” IEEE Trans.

Cognitive Commun. Netw., vol. 7, no. 4, pp. 1233–1243, 2021.

[44] C. Li, H. Farkhoor, R. Liu, and J. Yosinski, “Measuring the intrinsic dimension of objective landscapes,” in Proc. Int.

Conf. Learning Representations (ICLR 2018), May 2018.

[45] S. Seo, S. W. Choi, S. Kook, S.-L. Kim, and S.-W. Ko, “Understanding uncertainty of edge computing: New principle and

design approach,” 2020. arXiv:2006.01032. [Online]. Available: https://arxiv.org/abs/2006.01032.

[46] Z. Gong, P. Zhong, and W. Hu, “Diversity in machine learning,” IEEE Access, vol. 7, pp. 64323–64350, 2019.

[47] P. J. Huber, “Robust estimation of a location parameter,” Ann. Math. Stat., vol. 35, no. 1, pp. 73 – 101, 1964.

[48] S. Adireddy and L. Tong, “Optimal transmission probabilities for slotted ALOHA in fading channels,” in Proc. Conf. Inf.

Sci. Syst., March 2002.

[49] R. M. French, “Catastrophic forgetting in connectionist networks,” Trends in Cognitive Sci., vol. 3, no. 4, pp. 128–135,

1999.

