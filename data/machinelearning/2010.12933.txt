0
2
0
2

t
c
O
4
2

]

C
D
.
s
c
[

1
v
3
3
9
2
1
.
0
1
0
2
:
v
i
X
r
a

Triclustering in Big Data Setting

Dmitry Egurnov, Dmitry I. Ignatov, and Dmitry Tochilkin

Abstract In this paper1, we describe versions of triclustering algorithms adapted
for eï¬ƒcient calculations in distributed environments with MapReduce model or par-
allelisation mechanism provided by modern programming languages. OAC-family
of triclustering algorithms shows good parallelisation capabilities due to the in-
dependent processing of triples of a triadic formal context. We provide time and
space complexity of the algorithms and justify their relevance. We also compare
performance gain from using a distributed system and scalability.

Key words: Formal Concept Analysis, n-ary relations, Boolean tensors, Data Min-
ing, Big Data, MapReduce

1 Introduction

Mining of multimodal patterns in ğ‘›-ary relations or Boolean tensors is among popular
topics in Data Mining and Machine Learning [8, 2, 36, 14, 26, 35, 13]. Thus, cluster
analysis of multimodal data and speciï¬cally of dyadic and triadic relations is a
natural extension of the idea of original clustering. In dyadic case, biclustering
methods (the term bicluster was coined in [27]) are used to simultaneously ï¬nd
subsets of objects and attributes that form homogeneous patterns of the input object-
attribute data. Triclustering methods operate in triadic case, where for each object-
attribute pair one assigns a set of some conditions [28, 16, 9]. Both biclustering and
triclustering algorithms are widely used in such areas as gene expression analysis

Dmitry Egurnov Â· Dmitry I. Ignatov Â· Dmitry Tochilkin
National Research University Higher School of Economics, Russian Federation e-mail: degurnov@
hse.ru

1 The paper contains an extended version of the prior work presented at the workshop on FCA in
the Big Data Era held on June 25, 2019 at Frankfurt University of Applied Sciences, Frankfurt,
Germany [18].

1

 
 
 
 
 
 
2

Egurnov et al.

[25, 5, 42, 24, 21], recommender systems [30, 19, 17], social networks analysis [11],
natural language processing [37], etc. The processing of numeric multimodal data
is also possible by modiï¬cations of existing approaches for mining binary relations
[20]. Another interesting venue closely related to attribute dependencies in object-
attribute data also take place in triadic case, namely, mining of triadic association
rules and implications [6, 29].

Though there are methods that can enumerate all triclusters satisfying certain
constraints [2] (in most cases they ensure that triclusters are dense), their time
complexity is rather high, as in the worst case the maximal number of triclusters is
usually exponential (e.g. in case of formal triconcepts), showing that these methods
are hardly scalable. Algorithms that process big data should have at most linear time
complexity (e.g., ğ‘‚ (|ğ¼ |) in case of ğ‘›-ary relation ğ¼) and be easily parallelisable. In
addition, especially in the case of data streams [34], the output patterns should be
the results of one pass over data.

Earlier, in order to create an algorithm satisfying these requirements, we adapted
a triclustering method based on prime operators (prime OAC-triclustering method)
[9] and proposed its online version, which has linear time complexity; it is also one-
pass and easily parallelisable [10]. However, its parallelisation is possible in diï¬€erent
ways. For example, one can use a popular framework for commodity hardware, Map-
Reduce (M/R) [33]. In the past, there were several successful M/R implementations
in the FCA community and other lattice-oriented domains. Thus, in [22], the authors
adapted Close-by-One algorithm to M/R framework and showed its eï¬ƒciency. In
the same year, in [23], an eï¬ƒcient M/R algorithm for computation of closed cube
lattices was proposed. The authors of [40] demonstrated that iterative algorithms
like Ganterâ€™s NextClosure can beneï¬t from the usage of iterative M/R schemes.

Our previous M/R implementation of the triclustering method based on prime
operators was proposed in [43] showing computational beneï¬ts on rather large
datasets. M/R triclustering algorithm [43] is a successful distributed adaptation of
the online version of prime OAC-triclustering [10]. This method uses the MapReduce
approach as means for task allocation on computational clusters, launching the online
version of prime OAC-triclustering on each reducer of the ï¬rst phase. However, due
to the simplicity of this adaptation, the algorithm does not use the advantages of
MapReduce to the full extent. In the ï¬rst stage, all the input triples are split into the
number of groups equals to the number of reducers by means of hash-function for
entities of one of the types, object, attribute, or condition, which values are used as
keys. It is clear that this way of data allocation cannot guarantee uniformness in terms
of group sizes. The JobTracker used in Apache Hadoop is able to evenly allocate
tasks by nodes2. To do so, the number of tasks should be larger than the number
of working nodes, which is not fulï¬lled in this implementation. For example, let us
assume we have 10 reduce SlaveNodes; respectively, ğ‘Ÿ = 10, and the hash function
is applied to the objects (the ï¬rst element in each input triple). However, due to
the non-uniformity of hash-function values by modulo 10, it may happen that the
set of objects will result in less than 10 diï¬€erent residuals during division by 10.

2 https://wiki.apache.org/hadoop/JobTracker

Triclustering in Big Data Setting

3

Table 1 An example with triadic data

ğ‘–1 ğ‘–2

ğ‘¢1 Ã—
ğ‘¢2 Ã— Ã—
ğ‘¢3
Ã—
ğ‘™1

ğ‘–1 ğ‘–2

ğ‘¢1 Ã—
ğ‘¢2 Ã— Ã—
ğ‘¢3 Ã—
ğ‘™2

In this case, the input triples will be distributed between parts of diï¬€erent sizes
and processed by only a part of cluster nodes. Such cases are rather rare; it could
be possible only for slicing by entities (objects, attributes, or conditions) with a
small number of diï¬€erent elements. However, they may slow down the cluster work
drastically.

The weakest link is the second stage of the algorithm. First of all, during the ï¬rst
stage, it ï¬nds triclusters computed for each data slice separately. Hence, they are not
the ï¬nal triclusters; we need to merge the obtained results.

Let us consider an example in Table 1 with the ternary relation on users-items-
labels. Let us assume that the ï¬rst mapper splits data according to their labelsâ€™
component, ğ‘Ÿ = 2, then triples containing label ğ‘™1 and those related to label ğ‘™2 are
processed on diï¬€erent nodes. After the ï¬rst stage completion on the ï¬rst node, we
have tricluster ({ğ‘¢2}, {ğ‘–1, ğ‘–2}, {ğ‘™1}) among the others, while the second node results
in tricluster ({ğ‘¢2}, {ğ‘–1, ğ‘–2}, {ğ‘™2}). It is clear that both triclusters are not complete
for the whole input dataset and should be merged into ({ğ‘¢2}, {ğ‘–1, ğ‘–2}, {ğ‘™1, ğ‘™2}). The
second stage of the algorithm is responsible for this type of merging. However, as
one can see, this merging assumes that all intermediate data should be located on
the same node. In a big data setting, this allocation of all the intermediate results on
a single node is a critical point for application performance.

To calculate tricluster components (or cumuli, see Section 3.1) and assemble the
ï¬nal triclusters from them, we need to have large data slices on the computational
nodes. Moreover, during parallelisation of the algorithm, those data slices can be
required simultaneously on diï¬€erent nodes. Thus, to solve these problems one needs
to fulï¬l data centralisation (all the required slices should be present at the same node
simultaneously). However, it leads to the accumulation of too large parts of the data
as described. Another approach is to perform data replication. In this case, the total
amount of data processed on computational nodes is increased, while the data are
evenly distributed in the system. The latter approach has been chosen for our updated
study on multimodal clustering.

Note that experts warn the potential M/R users: â€œthe entire distributed-ï¬le-system
milieu makes sense only when ï¬les are very large and are rarely updated in placeâ€
[33]. In this work, as in our previous study, we assume that there is a large bulk of data
to process that is not coming online. As for experimental comparison, since we would
like to follow an authentic M/R approach, without using the online algorithm [10],
we implement and enhance only MapReduce schemes from the appendix in [43]
supposed for future studies in that time.

4

Egurnov et al.

The rest of the paper is organised as follows: in Section 2, we recall the original
method and the online version of the algorithm of prime OAC-triclustering. Section 3
is dedicated to extensions of OAC triclustering. Speciï¬cally, Subsection 3.1 gener-
alise prime OAC-triclustering for the case of multimodal data, and Subsection 3.2
touches the case of multi-valued context. In Section 4 we describe implementation
details. Subsection 4.1, gives the M/R setting of the problem and the corresponding
M/R version of the original algorithm with important implementation aspects. In
Subsection 4.3 we speak about parallel implementation of OAC triclustering. Fi-
nally, in Section 5 we show the results of several experiments that demonstrate the
eï¬ƒciency of the M/R version of the algorithm.

2 Prime object-attribute-condition triclustering

Prime object-attribute-condition triclustering method (OAC-prime) based on Formal
Concept Analysis [39, 7] is an extension for the triadic case of object-attribute bi-
clustering method [15]. Triclusters generated by this method have a similar structure
as the corresponding biclusters, namely the cross-like structure of triples inside the
input data cuboid (i.e. formal tricontext).

Let K = (ğº, ğ‘€, ğµ, ğ¼) be a triadic context, where ğº, ğ‘€, ğµ are respectively the
sets of objects, attributes, and conditions, and ğ¼ âŠ† ğº Ã— ğ‘€ Ã— ğµ is a triadic incidence
relation. Each prime OAC-tricluster is generated by applying the following prime
operators to each pair of components of some triple:

(ğ‘‹, ğ‘Œ ) (cid:48) = {ğ‘ âˆˆ ğµ | (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ for all ğ‘” âˆˆ ğ‘‹, ğ‘š âˆˆ ğ‘Œ },
(ğ‘‹, ğ‘) (cid:48) = {ğ‘š âˆˆ ğ‘€ | (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ for all ğ‘” âˆˆ ğ‘‹, ğ‘ âˆˆ ğ‘ },
(ğ‘Œ , ğ‘) (cid:48) = {ğ‘” âˆˆ ğº | (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ for all ğ‘š âˆˆ ğ‘Œ , ğ‘ âˆˆ ğ‘ },

(1)

where ğ‘‹ âŠ† ğº, ğ‘Œ âŠ† ğ‘€, and ğ‘ âŠ† ğµ.

Then the triple ğ‘‡ = ((ğ‘š, ğ‘) (cid:48), (ğ‘”, ğ‘) (cid:48), (ğ‘”, ğ‘š) (cid:48)) is called prime OAC-tricluster
based on triple (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼. The components of tricluster are called, respectively,
tricluster extent, tricluster intent, and tricluster modus. The triple (ğ‘”, ğ‘š, ğ‘) is called
a generating triple of the tricluster ğ‘‡. Density of a tricluster ğ‘‡ = (ğºğ‘‡ , ğ‘€ğ‘‡ , ğµğ‘‡ ) is
the relation of actual number of triples in the tricluster to its size:

ğœŒ(ğ‘‡) =

|ğºğ‘‡ Ã— ğ‘€ğ‘‡ Ã— ğµğ‘‡ âˆ© ğ¼ |
|ğºğ‘‡ ||ğ‘€ğ‘‡ ||ğµğ‘‡ |

In these terms a triconcept is a tricluster with density ğœŒ = 1.

Figure 1 shows the structure of an OAC-tricluster (ğ‘‹, ğ‘Œ , ğ‘) based on triple
ğ‘š, (cid:101)ğ‘), triples corresponding to the gray cells are contained in the tricluster,
(cid:101)

ğ‘”,
((cid:101)
other triples may be contained in the tricluster (cuboid) as well.

The basic algorithm for the prime OAC-triclustering method is rather straightfor-
ward (see [9]). First of all, for each combination of elements from each of the two
sets of K we apply the corresponding prime operator (we call the resulting sets prime

Triclustering in Big Data Setting

5

Fig. 1 Structure of prime OAC-triclusters: the dense cross-like central layer containing Ëœğ‘” (left) and
the layer for an object ğ‘” (right) in ğ‘€ Ã— ğµ dimensions.

sets). After that, we enumerate all triples from ğ¼ and on each step, we must generate
a tricluster based on the corresponding triple, check whether this tricluster is already
contained in the tricluster set (by using hashing) and also check extra conditions.

The total time complexity of the algorithm depends on whether there is a non-zero
minimal density threshold or not and on the complexity of the hashing algorithm
used. In case we use some basic hashing algorithm processing the triclusterâ€™s extent,
intent and modus without a minimal density threshold, the total time complexity is
ğ‘‚ (|ğº ||ğ‘€ ||ğµ| + |ğ¼ |(|ğº | + |ğ‘€ | + |ğµ|)) (assuming the hashing algorithm takes ğ‘‚ (1) to
operate, it takes 3 Ã— ğ‘‚ (|ğº ||ğ‘€ ||ğµ|) to precompute the prime sets and then ğ‘‚ (|ğº | +
|ğ‘€ | + |ğµ|) for each triple (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ to compute the hash); in case of a non-zero
minimal density threshold, it is ğ‘‚ (|ğ¼ ||ğº ||ğ‘€ ||ğµ|) (since computing density takes
ğ‘‚ (|ğº ||ğ‘€ ||ğµ|) for each tricluster generated from a triple (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼). The memory
complexity is ğ‘‚ (|ğ¼ |(|ğº | + |ğ‘€ | + |ğµ|)), as we need to keep the dictionaries with the
prime sets in memory.

In online setting, for triples coming from triadic context K = (ğº, ğ‘€, ğµ, ğ¼), the
user has no a priori knowledge of the elements and even cardinalities of ğº, ğ‘€, ğµ, and
ğ¼. At each iteration, we receive some set of triples from ğ¼: ğ½ âŠ† ğ¼. After that, we must
process ğ½ and get the current version of the set of all triclusters. It is important in this
setting to consider every pair of triclusters as being diï¬€erent as they have diï¬€erent
generating triples, even if their respective extents, intents, and modi are equal. Thus,
any other triple can change only one of these two triclusters, making them diï¬€erent.
To eï¬ƒciently access prime sets for their processing, the dictionaries containing

the prime sets are implemented as hash-tables.

The algorithm is straightforward as well (Alg. 1). It takes some set of triples (ğ½),
the current tricluster set (T ), and the dictionaries containing prime sets (ğ‘ƒğ‘Ÿğ‘–ğ‘šğ‘’ğ‘ )
as input and outputs the modiï¬ed versions of the tricluster set and dictionaries. The
algorithm processes each triple (ğ‘”, ğ‘š, ğ‘) of ğ½ sequentially (line 1). At each iteration,
the algorithm modiï¬es the corresponding prime sets (lines 2-4).

Finally, it adds a new tricluster to the tricluster set. Note that this tricluster contains
pointers to the corresponding prime sets (in the corresponding dictionaries) instead
of the copies of the prime sets (line 5) which allows lowering the memory and access
costs.

6

Egurnov et al.

Algorithm 1 Add function for the online algorithm for prime OAC-triclustering.
Require: ğ½ is a set of triples;

T = {ğ‘‡ = (âˆ—ğ‘‹ , âˆ—ğ‘Œ , âˆ—ğ‘ ) } is a current set of triclusters;
ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ ğ´, ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ğ¶, ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘  ğ´ğ¶.

Ensure: T = {ğ‘‡ = (âˆ—ğ‘‹ , âˆ—ğ‘Œ , âˆ—ğ‘ ) };

ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ ğ´, ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ğ¶, ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘  ğ´ğ¶.

1: for all (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ½ do
2:
3:
4:
5:
6: end for

ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ ğ´[ğ‘”, ğ‘š] := ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ ğ´[ğ‘”, ğ‘š] âˆª {ğ‘ }
ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ğ¶ [ğ‘”, ğ‘] := ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ğ¶ [ğ‘”, ğ‘] âˆª {ğ‘š}
ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘  ğ´ğ¶ [ğ‘š, ğ‘] := ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘  ğ´ğ¶ [ğ‘š, ğ‘] âˆª {ğ‘” }
T := T âˆª { (&ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘  ğ´ğ¶ [ğ‘š, ğ‘], &ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ğ¶ [ğ‘”, ğ‘], &ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘ ğ‘‚ ğ´[ğ‘”, ğ‘š]) }

The algorithm is one-pass and its time and memory complexities are ğ‘‚ (|ğ¼ |).
Duplicate elimination and selection patterns by user-speciï¬c constraints are done
as post-processing to avoid patternsâ€™ loss. The time complexity of the basic post-
processing is ğ‘‚ (|ğ¼ |) and it does not require any additional memory.

The algorithm can be easily parallelised by splitting the subset of triples ğ½ into
several subsets, processing each of them independently, and merging the resulting
sets afterwards. This fact results in our previous MapReduce implementation [43].

3 Triclustering extensions

3.1 Multimodal clustering

The direct extension of the prime object-attribute-condition triclustering is multi-
modal clustering for higher input relation arities. For the input polyadic context
Kğ‘ = ( ğ´1, ğ´2, . . . , ğ´ğ‘ , ğ¼ âŠ† ğ´1 Ã— ğ´2 Ã— Â· Â· Â· Ã— ğ´ğ‘ ) [38], we introduce the notion of
cumulus for each input tuple ğ‘– = (ğ‘’1, ğ‘’2, . . . , ğ‘’ ğ‘ ) âˆˆ ğ¼ and the corresponding entity
ğ‘’ğ‘˜ , where ğ‘˜ âˆˆ {1, . . . , ğ‘ } as follows:

ğ‘ğ‘¢ğ‘š(ğ‘–, ğ‘˜) = {ğ‘’ | (ğ‘’1, ğ‘’2, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’, ğ‘’ğ‘˜+1, . . . , ğ‘’ ğ‘ ) âˆˆ ğ¼}.

The multimodal cluster generated by the tuple ğ‘– âˆˆ ğ¼ is deï¬ned as follows:

(cid:0)(ğ‘ğ‘¢ğ‘š(ğ‘–, 1), . . . , ğ‘ğ‘¢ğ‘š(ğ‘–, ğ‘)(cid:1).

Those cumuli operators are similar to primes for pairs (or tuples) of sets Eq. 1:

ğ‘ğ‘¢ğ‘š(ğ‘–, ğ‘˜) = ({ğ‘’1}, {ğ‘’2}, . . . , {ğ‘’ğ‘˜âˆ’1}, {ğ‘’ğ‘˜+1}, . . . , {ğ‘’ ğ‘ }) (cid:48).

However, here, they are applied to the tuples of input relation rather than to pairs

(tuples) of sets.

In a certain sense, cumuli accumulate all the elements of a ï¬xed type that are

related by ğ¼.

Triclustering in Big Data Setting

7

As its triadic version, multimodal clustering is not greater than the number of
tuples in the input relation, whereas the complete set of polyadic concepts may be
exponential w.r.t. the input size [38].

3.2 Many-valued Triclustering

Another extension of prime OAC triclustering is many-valued triclustering where
each triple of the incidence relation of a triadic context is associated with a certain
value from an arbitrary set ğ‘Š (by means of a valuation function ğ‘‰ : ğ¼ â†’ ğ‘Š).
Therefore an input triadic context is changed to a many-valued triadic context KV =
(ğº, ğ‘€, ğµ, ğ‘Š, ğ¼, ğ‘‰), where for each triple (ğ‘”, ğ‘š, ğ‘) in ternary relation ğ¼ between ğº,
ğ‘€, ğµ, there is a unique ğ‘‰ (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ‘Š.

This deï¬nition contains the explicitly given valuation function ğ‘‰ similar to the
one in the deï¬nition of attribute-based information systems [32]. However, we could
also extend the original deï¬nition of a many-valued context in line with [7]. In this
case, a many-valued triadic context K = (ğº, ğ‘€, ğµ, ğ‘Š, ğ½) consists of sets ğº, ğ‘€, ğµ,
ğ‘Š and a quaternary relation between them ğ½ âŠ† ğº Ã— ğ‘€ Ã— ğµ Ã— ğ‘Š, where the following
holds: (ğ‘”, ğ‘š, ğ‘, ğ‘¤) âˆˆ ğ½ and (ğ‘”, ğ‘š, ğ‘, ğ‘£) âˆˆ ğ½ imply ğ‘¤ = ğ‘£.

In what follows, we prefer the ï¬rst variant since it simpliï¬es further exposition.
For numeric values, the most common case is for ğ‘Š = R. To mine triclusters in
such context prime operators are modiï¬ed into so-called ğ›¿-operators. For a generating
triple ( Ëœğ‘”, Ëœğ‘š, Ëœğ‘) âˆˆ ğ¼ and some parameter ğ›¿:

( Ëœğ‘š, Ëœğ‘) ğ›¿ = (cid:8)ğ‘” | (ğ‘”, Ëœğ‘š, Ëœğ‘) âˆˆ ğ¼ âˆ§ |ğ‘‰ (ğ‘”, Ëœğ‘š, Ëœğ‘) âˆ’ ğ‘‰ ( Ëœğ‘”, Ëœğ‘š, Ëœğ‘)| â‰¤ ğ›¿(cid:9)

( Ëœğ‘”, Ëœğ‘) ğ›¿ = (cid:8)ğ‘š | ( Ëœğ‘”, ğ‘š, Ëœğ‘) âˆˆ ğ¼ âˆ§ |ğ‘‰ ( Ëœğ‘”, ğ‘š, Ëœğ‘) âˆ’ ğ‘‰ ( Ëœğ‘”, Ëœğ‘š, Ëœğ‘)| â‰¤ ğ›¿(cid:9)
( Ëœğ‘”, Ëœğ‘š) ğ›¿ = (cid:8)ğ‘ | ( Ëœğ‘”, Ëœğ‘š, ğ‘) âˆˆ ğ¼ âˆ§ |ğ‘‰ ( Ëœğ‘”, Ëœğ‘š, ğ‘) âˆ’ ğ‘‰ ( Ëœğ‘”, Ëœğ‘š, Ëœğ‘)| â‰¤ ğ›¿(cid:9)
This deï¬nition can still be used to mine regular triclusters, if we set ğ‘Š = {0, 1}

and ğ›¿ = 0.

4 Implementations

4.1 Map-reduce-based multimodal clustering

We follow a three-stage approach here. On each stage, we sequentially run the map
and reduce procedures: First map â†’ First reduce â†’ Second map â†’ Second reduce â†’
Third map â†’ Third reduce. Each map/reduce procedure of a certain stage is executed
in parallel on all the available nodes/clusters. How tasks are distributed among the
nodes/clusters depends on the concrete MapReduce technology implementation (in

8

Egurnov et al.

our case, Apache Hadoop). Below, we describe the data ï¬‚ow between computational
nodes and their processing.

1) The ï¬rst map (Algorithm 2)

tuples. Each tu-
ple (ğ‘’1, ğ‘’2, . . . , ğ‘’ ğ‘ ) is transformed into ğ‘ key-value pairs: (cid:104)(ğ‘’2, . . . , ğ‘’ ğ‘ ), ğ‘’1(cid:105),
(cid:104)ğ‘’1, ğ‘’3, . . . , ğ‘’ ğ‘ ), ğ‘’2(cid:105), . . . , (cid:104)(ğ‘’1, ğ‘’2, . . . , ğ‘’ ğ‘ âˆ’1), ğ‘’ ğ‘ (cid:105). The resulting pairs are passed
to the further step.

takes a set of

input

Algorithm 2 Distributed Multimodal clustering: First Map
Require: ğ¼ is a set of tuples of length ğ‘ each
Ensure: (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ ğ‘¦ (cid:105) pairs.
1: for all (ğ‘’1, ğ‘’2, . . . , ğ‘’ğ‘ ) âˆˆ ğ¼ do
2:
3:
4:
5:
6: end for

ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› := (ğ‘’1, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’ğ‘˜+1, . . . , ğ‘’ğ‘ )
emit (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘’ğ‘˜ (cid:105)

for all ğ‘˜ âˆˆ {1, . . . , ğ‘ } do

end for

2) The ï¬rst reduce (Algorithm 3) receives all the accumulated values of each key.
Thus, for each (ğ‘’1, . . . , ğ‘’ ğ‘ ) âˆˆ ğ¼ and the context entity type ğ‘˜ âˆˆ {1, 2, . . . , ğ‘ }, we
compute the cumulus (ğ‘’1, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’ğ‘˜+1, . . . , ğ‘’ ğ‘ ) (cid:48). The values are passed to the
next MapReduce stage with the key (ğ‘’1, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’ğ‘˜+1, . . . , ğ‘’ ğ‘ ).

Algorithm 3 Distributed Multimodal clustering: First Reduce
Require: key-value pairs (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘’ğ‘  {ğ‘’1
Ensure: (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘ (cid:105)
1: cumulus:={}
2: for all ğ‘’ğ‘˜ âˆˆ {ğ‘’1
3:
4:
5: end for

ğ‘˜ , . . . , ğ‘’ğ¿
ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘  := ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘  âˆª {ğ‘’ğ‘˜ }
emit (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘ (cid:105)

ğ‘˜ , . . . , ğ‘’ğ¿

ğ‘˜ } do

ğ‘˜ }(cid:105)

3) Second map (Algorithm 4). All the received keys are transformed into the
original relations and passed to the second reduce procedure with unchanged values.

Algorithm 4 Distributed Multimodal clustering: Second Map
Require: (cid:104)ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘ (cid:105), where ğ‘ ğ‘¢ğ‘ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = (ğ‘’1, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’ğ‘˜+1, . . . , ğ‘’ğ‘ )
Ensure: (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘ (cid:105) pairs.
1: for all ğ‘’ğ‘˜ âˆˆ ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘  do
2:
3:
4: end for

ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› := (ğ‘’1, . . . , ğ‘’ğ‘˜âˆ’1, ğ‘’ğ‘˜ , ğ‘’ğ‘˜+1, . . . , ğ‘’ğ‘ )
emit (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘¢ğ‘ (cid:105)

4) Second reduce (Algorithm 5). All the cumuli obtained for each input tu-
ple of the original relation ğ¼ are reduced to a single set. At this stage, we ob-

Triclustering in Big Data Setting

9

tain all the original tuples and generated multimodal clusters. These clusters are
presented as tuples of cumuli for respective entity types. All the obtained pairs
(cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ(cid:105) are passed to the next stage.

Algorithm 5 Distributed Multimodal clustering: Second Reduce
Require: (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘– { ğ´1, ğ´2, Â· Â· Â· , ğ´ğ‘ }(cid:105)
Ensure: (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ (cid:105) pairs
1: ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ := ( ğ´1, ğ´2, Â· Â· Â· , ğ´ğ‘ )
2: emit (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ (cid:105)

5) Third map (Algorithm 6). The task of the third MapReduce stage is duplicate
elimination and ï¬ltration by density threshold. It is beneï¬cial to implement within
the reduce step, but to do so each obtained key-value pair (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›,
ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ(cid:105) should be passed further as follows (cid:104)ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ,
ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(cid:105).

Algorithm 6 Distributed Multimodal clustering: Third Map
Require: (cid:104)ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ (cid:105)
1: emit (cid:104)ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ , ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(cid:105)

6) Third reduce (Algorithm 7). For each input multimodal cluster and its generat-
ing tuples, it is possible to directly compute density. All the unique clusters will be
stored.

Algorithm 7 Distributed Multimodal clustering: Third Reduce
Require: (cid:104)ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ , ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”_ğ‘Ÿ ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  {ğ‘Ÿ1, ğ‘Ÿ2 . . . , ğ‘Ÿğ‘€ }(cid:105)
1: if

â‰¥ ğœƒ then

| {ğ‘Ÿ1, ğ‘Ÿ2 . . . , ğ‘Ÿğ‘€ } |
ğ‘£ğ‘œğ‘™ (ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ )
store (cid:104)ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–ğ‘šğ‘œğ‘‘ğ‘ğ‘™_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ (cid:105)

2:
3: end if

The time and memory complexities are provided assuming that the worst-case
scenario corresponds to the absolutely dense cuboid, i.e. polyadic context Kğ‘ =
( ğ´1, ğ´2, . . . , ğ´ğ‘ , ğ´1 Ã— ğ´2 Ã— Â· Â· Â· Ã— ğ´ğ‘ ). Thus, after careful analysis, the worst-case
time complexity of the proposed three-stage algorithm is ğ‘‚ (|ğ¼ | (cid:205)ğ‘
ğ‘—=1 | ğ´ ğ‘— |). Not
surprisingly it has the same worst-case memory complexity since the stored and
passed the maximal number of multimodal clusters is |ğ¼ |, and the size of each of
them is not greater (cid:205)ğ‘
ğ‘—=1 | ğ´ ğ‘— |. However, from an implementation point of view, since
HDFS has default replication factor 3, those data elements are copied thrice to fulï¬l
fault-tolerance.

10

Egurnov et al.

4.2 Implementation aspects and used technologies

The application3 has been implemented in Java and as distributed computation
framework we use Apache Hadoop4.

We have used many other technologies: Apache Maven (framework for automatic
project assembling), Apache Commons (for work with extended Java collections),
Jackson JSON (open-source library for the transformation of object-oriented repre-
sentation of an object like tricluster to string), TypeTools (for real-time type resolution
of inbound and outbound key-value pairs), etc.

To provide the reader with basic information on the most important classes for

M/R implementation, let us shortly describe them below.
Entity. This is a basic abstraction for an element of a certain type. Each entity is
deï¬ned by its type index from 0 to ğ‘›-1, where ğ‘› is the arity of the input formal context.
An entity value is a string that needs to be kept during the program execution. This
class inherits Writable interface for storing its objects in temporary and permanent
Hadoop ï¬les. This is a mandatory requirement for all classes that pass or take their
objects as keys and values of the map and reduce methods.
Tuple. This class implements a representation of relation. Each object of the class
Tuple contains the list of objects of Entity class and the arity of its data given by
its numeric value. This class implements interface WritableComparable<Tuple> to
make it possible to use an object class Tuple as a key. The interface is similar to
Writable, however, one needs to deï¬ne comparison function to use in the key sorting
phase.
Cumulus. This is an abstraction of cumulus, introduced earlier. It contains the list
of string values and the index of a respective entity. It also implements the fol-
lowing interfaces: WritableComparable<Cumulus> for using cumulus as a key and
Iterable<String> for iteration by its values.
FormalConcept. This is an abstraction of both formal concepts and multimodal
clusters, it contains the list of cumuli and implements interface Writable.

The process-like M/R classes are summarised below.

FirstMapper, SecondMapper, ThirdMapper. These are the classes that extend class
Mapper<> of the Hadoop MapReduce library by respective mapping function from
Subsection 4.1.
FirstReducer, SecondReducer, ThirdReducer. These classes extend class Reducer<>
of the Hadoop MapReduce library for fulï¬lling Algorithms 2,4,6.
TextCumulusInputFormat, TextCumulusOutputFormat. These classes implement
reading and writing for objects of Cumulus class; they also inherit RecordReader and
RecordWriter interfaces, respectively. They are required to exchange results between
diï¬€erent MapReduce phases within one MapReduce program.
JobConï¬gurator. This is the class for setting conï¬guration of a single MapReduce
stage. It deï¬nes the classes of input/output/intermediate keys and values of the
mapper and reducer as well as formats of input and output data.

3 https://github.com/kostarion/multimodal-clustering-hadoop
4 https://hadoop.apache.org/

Triclustering in Big Data Setting

11

App. This class is responsible for launching the application and chaining of M/R
stages.

4.3 Parallel many-valued triclustering

A generic algorithm for OAC triclustering is described below

Algorithm 8 General algorithm for OAC triclustering
Require: Context K = (ğº, ğ‘€ , ğµ, ğ¼ )
Ensure: Set of triclusters T
1: T := âˆ…
2: for all (ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ do
3:
4:
5:
6:
7:
8:
end if
9:
10: end for
11: return T

ğ‘œğ‘†ğ‘’ğ‘¡ := ğ‘ ğ‘ ğ‘ğ‘™ğ‘¦ ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘‚ ğ‘ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘œğ‘Ÿ (ğ‘š, ğ‘)
ğ‘ğ‘†ğ‘’ğ‘¡ := ğ‘ ğ‘ ğ‘ğ‘™ğ‘¦ ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘‚ ğ‘ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘œğ‘Ÿ (ğ‘”, ğ‘)
ğ‘ğ‘†ğ‘’ğ‘¡ := ğ‘ ğ‘ ğ‘ğ‘™ğ‘¦ ğ‘ƒğ‘Ÿ ğ‘–ğ‘šğ‘’ğ‘‚ ğ‘ğ‘’ğ‘Ÿ ğ‘ğ‘¡ğ‘œğ‘Ÿ (ğ‘”, ğ‘š)
ğ‘¡ğ‘Ÿ ğ‘–ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ := (ğ‘œğ‘†ğ‘’ğ‘¡ , ğ‘ğ‘†ğ‘’ğ‘¡ , ğ‘ğ‘†ğ‘’ğ‘¡)
if ğ‘¡ğ‘Ÿ ğ‘–ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ is valid then
ğ´ğ‘‘ğ‘‘ ( T, ğ‘¡ğ‘Ÿ ğ‘–ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ )

To get a speciï¬c version of the algorithm one only needs to add an appropriate
implementation of the prime operator and optional validity check. A tricluster mined
from one triple does not depend on triclusters mined from other triples, so, in case
of parallel implementation, each triple is processed in an individual thread.

We used ğ›¿-operators deï¬ned in 3.2, minimal density, and minimal cardinality

(w.r.t. to every dimension) constraints [3].

5 Experiments

Two series of experiments have been conducted in order to test the application on
the synthetic contexts and real-world datasets with a moderate and large number of
triples in each. In each experiment, both versions of the OAC-triclustering algorithm
have been used to extract triclusters from a given context. Only online and M/R
versions of OAC-triclustering algorithm have managed to result in patterns for large
contexts since the computation time of the compared algorithms was too high (>3000
s). To evaluate the runtime more carefully, for each context the average result of 5
runs of the algorithms has been recorded.

12

5.1 Datasets

Egurnov et al.

Synthetic datasets. The following synthetic datasets were generated.
The dense context K1 = (ğº, ğ‘€, ğµ, ğ¼), where ğº = ğ‘€ = ğµ = {1, . . . , 60} and
ğ¼ = ğº Ã— ğ‘€ Ã— ğµ \ {(ğ‘”, ğ‘š, ğ‘) âˆˆ ğ¼ | ğ‘” = ğ‘š = ğ‘}. In total, 603 - 60 =215,940 triples.
The context of three non-overlapped cuboids K2 = (ğº1 (cid:116) ğº2 (cid:116) ğº3, ğ‘€1 (cid:116) ğ‘€2 (cid:116)
ğ‘€3, ğµ1 (cid:116) ğµ2 (cid:116) ğµ3, ğ¼), where ğ¼ = (ğº1 Ã— ğ‘€1 Ã— ğµ1) âˆª (ğº2 Ã— ğ‘€2 Ã— ğµ2) âˆª (ğº3 Ã— ğ‘€3 Ã— ğµ3).
In total, 3 Â· 503 = 375, 000 triples.
The context K3 = ( ğ´1, ğ´2, ğ´3, ğ´4, ğ´1 Ã— ğ´2 Ã— ğ´3 Ã— ğ´4) is a dense fourth dimensional
cuboid with | ğ´1| = | ğ´2| = | ğ´3| = | ğ´4| = 30 containing 304 = 810, 000 triples.

These tests have sense since in M/R setting due to the tuples can be (partially)
repeated, e.g., because of M/R task failures on some nodes (i.e. restarting pro-
cessing of some key-value pairs). Even though the third dataset does not result in
3ğ‘šğ‘–ğ‘›( | ğ´1 |, | ğ´2 |, | ğ´3 |, | ğ´4 |) formal triconcepts, the worst-case for formal triconcepts gen-
eration in terms of the number of patterns, this is an example of the worst-case
scenario for the reducers since the input has its maximal size w.r.t. to the size of ğ´ğ‘–-s
and the number of duplicates. In fact, our algorithm correctly assembles the only
one tricluster ( ğ´1, ğ´2, ğ´3, ğ´4).
IMDB. This dataset consists of the 250 best movies from the Internet Movie Database
based on user reviews.

The following triadic context is composed: the set of objects consists of movie
names, the set of attributes (tags), the set of conditions (genres), and each triple of
the ternary relation means that the given movie has the given genre and is assigned
the given tag. In total, there are 3,818 triples.
Movielens. The dataset contains 1,000,000 tuples that relate 6,040 users, 3,952
movies, ratings, and timestamps, where ratings are made on a 5-star scale [12].
Bibsonomy. Finally, a sample of the data of bibsonomy.org from ECML PKDD
discovery challenge 2008 has been used.

This website allows users to share bookmarks and lists of literature and tag them.
For the tests the following triadic context has been prepared: the set of objects
consists of users, the set of attributes (tags), the set of conditions (bookmarks), and
a triple of the ternary relation means that the given user has assigned the given tag
to the given bookmark.

Table 2 contains the summary of IMDB and Bibsonomy contexts.

Table 2 Tricontexts based of real data systems for the experiments

Context
IMDB

|ğº |
250

| ğ‘€ |
795

|ğµ | # triples Density
3,818 0.00087
22

BibSonomy 2,337 67,464 28,920 816,197 1.8 Â· 10âˆ’7

Due to a number of requests, we would like to show small excerpts of the input
data we used (in this section) and a few examples of the outputted patterns (in the
next section).

Triclustering in Big Data Setting

13

Input data example. The input ï¬le for the Top-250 IMDB dataset comprises
triples in lines with tab characters as separators.

One Flew Over the Cuckooâ€™s Nest (1975) Nurse Drama
One Flew Over the Cuckooâ€™s Nest (1975)
Patient Drama
One Flew Over the Cuckooâ€™s Nest (1975) Asylum Drama
One Flew Over the Cuckooâ€™s Nest (1975) Rebel Drama
One Flew Over the Cuckooâ€™s Nest (1975) Basketball Drama
Star Wars V: The Empire Strikes Back (1980)
Star Wars V: The Empire Strikes Back (1980)
Star Wars V: The Empire Strikes Back (1980)
. . .

Princess Action
Princess Adventure
Princess

Sci-Fi

5.2 Results

The experiments have been conducted on the computer IntelÂ®Core(TM) i5-2450M
CPU @ 2.50GHz, 4Gb RAM (typical commodity hardware) in the emulation mode,
when Hadoop cluster contains only one node and operates locally and sequentially.
By time execution results one can estimate the performance in a real distributed
environment assuming that each node workload is (roughly) the same.

Table 3 Three-stage MapReduce multimodal clustering time, ms

IMDB MovieLens100k K1
Method
16,298
368
Online OAC prime clustering
14,582
MapReduce multimodal clustering 7,124

K2
96,990 185,072 643,978
37,572 61,367 102,699

K3

Table 4 Three-stage MapReduce multimodal clustering time, ms

Dataset

Online M/R total

MapReduce stages

# clusters

OAC Prime
89,931
MovieLens100k
225,242
MovieLens250k
MovieLens500k
461,198
MovieLens1M 958,345
Bibsonomy
(â‰ˆ800k triples)

2nd
1st
5,292
8,724
16,348
42,708 10,075 20,338
94,701 15,016 46,300
217,694 28,027 114,221

89,932
225,251
461,238
942,757
> 6 hours 3,651,072 19,117 1,972,135 1,659,820 486,221

3rd
2,332
12,295
33,384
74,446

(â‰ˆ1 hour)

In Tables 3 and 4 we summarise the results of performed tests. It is clear that
on average our application has a smaller execution time than its competitor, the on-

Egurnov et al.

Online OAC prime clustering
MapReduce multimodal clustering

14

s

,
e
m
T

i

1,200

1,000

800

600

400

200

0

I M100K K1

K2

Dataset

K3

M

Â·106

Fig. 2 Performance curves for six datasets: I stands for IMDB dataset with 3,818 triples, M100K
â€“ MovieLens dataset with 100K tuples, M â€“ MovieLens dataset with 1M tuples

line version of OAC-triclustering. If we compare the implemented program with its
original online version, the results are worse for the not that big and sparse dataset
as IMDB. It is the consequence of the fact that the application architecture aimed
at processing large amounts of data; in particular, it is implemented in three stages
with time-consuming communication. Launching and stopping Apache Hadoop, data
writing, and passing between Map and Reduce steps in both stages requires substan-
tial time, that is why for not that big datasets when execution time is comparable
with time for infrastructure management, time performance is not perfect. However,
with data size increase the relative performance is growing up to ï¬ve-six times (see
Fig. 2). Thus, the last test for BibSonomy data has been successfully passed, but the
competitor was not able to ï¬nish it within one hour. As for the M/R stages, the most
time-consuming phases are the 2nd and 3rd stages.

Output triclusters example. The output triclusters are stored in a format,
similar to JSON: the sets of entities (modalities) are given in curly brackets
separated with commas. Both sets of entities and triclusters start with a new
line.

Thus, for the modality of movies, in case of IMDB data, we can see two

remaining modalities as keywords and genres.

{
{Apocalypse Now (1979), Forrest Gump (1994),
Full Metal Jacket (1987), Platoon (1986)}

Triclustering in Big Data Setting

15

{Vietnam}
{Drama, Action}
}
{
{Toy Story (1995), Toy Story 2 (1999)}
{Toy, Friend}
{Animation, Adventure, Comedy, Family, Fantasy}
}
{
{Star Wars: Episode V (cid:21) The Empire Strikes Back (1980),
WALL-E (2008), Toy Story 2 (1999)}
{Rescue}
{Animation, Adventure}
}
{
{Into the Wild (2007), The Gold Rush (1925)}
{Love, Alaska}
{Adventure}
}
...

6 Experiments with parallelisation

In the additional set of experiments, we investigated parallelisation as another way of
improving the performance of our triclustering algorithms. Modern computers sup-
port multi-threading and have several processing cores. However one needs special
instructions and thread-safe data structures to produce an eï¬ƒcient parallel algorithm.
We used a dataset on semantic tri-frames featured in [37]. These triples represent
semantic frames extracted from FrameNet1.7 [1] and each triple is accompanied
by a frequency from DepCC dataset [31]. The total count of triples reached 100
thousand. The data was processed with the NOAC algorithm [3] implemented in C#
.NetFramework 4.5 [4] and modiï¬ed for parallel computation with Parallel library,
namely each triple from the context is processed in a separate thread. We ran two
series of experiments with diï¬€erent algorithm parameters and measured execution
time against the number of processed triples and also provided the number of ex-
tracted triclusters. In Table 5, for example, the record NOAC(100, 0.8, 2) 1k means
that ğ›¿ = 100, ğœŒğ‘šğ‘–ğ‘› = 0.8, ğ‘šğ‘–ğ‘›ğ‘ ğ‘¢ ğ‘ = 2 (for each dimension) and 1,000 triples are
being processed. All additional experiments were conducted on an IntelÂ®Core(TM)
i7-8750H CPU @ 2.20GHz, 16Gb RAM.

These experiments show that performance noticeably beneï¬ts from parallelisa-
tion. The execution time of the parallel algorithm is on average 35% lower. Another

16

Egurnov et al.

Table 5 NOAC. Regular ans parallel version

Time, ms (regular) Time, ms (parallel) # Triclusters

109
5,025
16,825
33,067
56,878
80,095
102,092
133,974
175,597
223,932
268,021
110
5,121
82,130
268,128

117
3,642
11,759
22,519
36,994
52,322
67,748
87,956
110,044
135,268
157,073
169
3,681
52,558
159,333

0
3
20
52
92
116
145
160
201
223
254
803
4942
14214
23134

Experiment
NOAC(100, 0.8, 2) 1k
NOAC(100, 0.8, 2) 10k
NOAC(100, 0.8, 2) 20k
NOAC(100, 0.8, 2) 30k
NOAC(100, 0.8, 2) 40k
NOAC(100, 0.8, 2) 50k
NOAC(100, 0.8, 2) 60k
NOAC(100, 0.8, 2) 70k
NOAC(100, 0.8, 2) 80k
NOAC(100, 0.8, 2) 90k
NOAC(100, 0.8, 2) 100k
NOAC(100, 0.5, 0) 1k
NOAC(100, 0.5, 0) 10k
NOAC(100, 0.5, 0) 50k
NOAC(100, 0.5, 0) 100k

Â·105

3

NOAC(100, 0.8, 2)
NOAC(100, 0.8, 2) (parallel)
NOAC(100, 0.5, 0)
NOAC(100, 0.5, 0) (parallel)

s
m

,
e
m
T

i

2.5

2

1.5

1

0.5

0

1k

10k

20k

30k

40k

50k

60k

70k

80k

90k

# Triples

100k
Â·105

Fig. 3 Performance curves for parallelisation experiments

interesting outcome is that execution time does not depend on the algorithm param-
eters, which only change the number of extracted triclusters.

Triclustering in Big Data Setting

7 Conclusion

17

In this paper, we have presented a map-reduce version of the multimodal clustering
algorithm, which extends triclustering approaches and copes with bottlenecks of the
earlier M/R triclustering version [43]. We have shown that the proposed algorithm
is eï¬ƒcient from both theoretical and practical points of view. This is a variant of
map-reduce based algorithm where the reducer exploits composite keys directly (see
also Appendix section [43]). However, despite the step towards Big Data technolo-
gies, a proper comparison of the proposed multimodal clustering and noise-tolerant
patterns in ğ‘›-ary relations by DataPeeler and its descendants [2] is not yet conducted
(including MapReduce setting).

Two of the most challenging problems in OAC-triclustering are approximate
triclustering density estimation (e.g., employing the Monte Carlo approach) and
duplicate elimination (the same triclusters can be generated by diï¬€erent generat-
ing triples). In MapReduce setting, both procedures are implemented as separate
MapReduce stages, while for online triclustering generation it is better to avoid du-
plicate generation, and the tricluster density should be computed both approximately
to maintain appropriate time-complexity and iteratively with minimal updates for
each incoming triple.

Further development of the proposed triclustering methods for large datasets is

possible with Apache Spark5 [41].

We also examined parallelisation possibilities (namely, multi-thread and milti-
core advantages in modern C#) of the numeric triclustering algorithm NOAC and
showed that it may signiï¬cantly improve performance.

Acknowledgements.

The study was implemented in the framework of the Basic Research Program at
the National Research University Higher School of Economics (Sections 2 and 5),
and funded by the Russian Academic Excellence Project â€™5-100â€™. The second author
was also supported by the Russian Science Foundation (Section 1, 3, and 4) under
grant 17-11-01294. The authors would like to thank Dmitry Gnatyshak and Sergey
Zudin for their earlier work on incremental and MapReduce-based triclustering
implementations, respectively, anonymous reviewers as well as Yuri Kudriavtsev
from PM-Square and Dominik Slezak from Infobright and Warsaw University for
their encouragement given to our studies of M/R technologies.

5 https://spark.apache.org/

18

References

Egurnov et al.

1. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In
Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and
17th International Conference on Computational Linguistics - Volume 1, ACL â€™98/COLING
â€™98, pages 86â€“90, Stroudsburg, PA, USA, 1998. Association for Computational Linguistics.
2. LoÃ¯c Cerf, JÃ©rÃ©my Besson, Kim-Ngan Nguyen, and Jean-FranÃ§ois Boulicaut. Closed and
noise-tolerant patterns in n-ary relations. Data Min. Knowl. Discov., 26(3):574â€“619, 2013.
3. Dmitrii Egurnov, Dmitry Ignatov, and Engelbert Mephu Nguifo. Mining triclusters of similar
values in triadic real-valued contexts. In 14th International Conference on Formal Concept
Analysis-Supplementary Proceedings, pages 31â€“47, 2017.

4. Dmitrii Egurnov and Dmitry I. Ignatov. Triclustring toolbox. In Supplementary Proceedings
of ICFCA 2019 Conference and Workshops, Frankfurt, Germany, June 25-28, 2019, pages
65â€“69, 2019.

5. Kemal Eren, Mehmet Deveci, Onur Kucuktunc, and Catalyurek, Umit V. A comparative
analysis of biclustering algorithms for gene expression data. Brieï¬ngs in Bioinform., 2012.
6. Bernhard Ganter, Peter A. Grigoriev, Sergei O. Kuznetsov, and Mikhail V. Samokhin. Concept-

based data mining with scaled labeled graphs. In ICCS, pages 94â€“108, 2004.

7. Bernhard Ganter and Rudolf Wille. Formal Concept Analysis: Mathematical Foundations.

Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1st edition, 1999.

8. Elisabeth Georgii, Koji Tsuda, and Bernhard SchÃ¶lkopf. Multi-way set enumeration in weight

tensors. Machine Learning, 82(2):123â€“155, 2011.

9. Dmitry V. Gnatyshak, Dmitry I. Ignatov, and Sergei O. Kuznetsov. From triadic FCA to
triclustering: Experimental comparison of some triclustering algorithms. In CLA, pages 249â€“
260, 2013.

10. Dmitry V. Gnatyshak, Dmitry I. Ignatov, Sergei O. Kuznetsov, and Lhouari Nourine. A

one-pass triclustering approach: Is there any room for big data? In CLA 2014, 2014.

11. Dmitry V. Gnatyshak, Dmitry I. Ignatov, Alexander V. Semenov, and Jonas Poelmans. Gaining
insight in social networks with biclustering and triclustering. In BIR, volume 128 of Lecture
Notes in Business Information Processing, pages 162â€“171. Springer, 2012.

12. F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. TiiS,

5(4):19:1â€“19:19, 2016.

13. Rui Henriques and Sara C. Madeira. Triclustering algorithms for three-dimensional data
analysis: A comprehensive survey. ACM Comput. Surv., 51(5):95:1â€“95:43, September 2018.
14. Dmitry I. Ignatov, Dmitry V. Gnatyshak, Sergei O. Kuznetsov, and Boris Mirkin. Triadic
formal concept analysis and triclustering: searching for optimal patterns. Machine Learning,
pages 1â€“32, 2015.

15. Dmitry I. Ignatov, Sergei O. Kuznetsov, and Jonas Poelmans. Concept-based biclustering for
internet advertisement. In ICDM Workshops, pages 123â€“130. IEEE Computer Society, 2012.
16. Dmitry I. Ignatov, Sergei O. Kuznetsov, Jonas Poelmans, and Leonid E. Zhukov. Can triconcepts

become triclusters? International Journal of General Systems, 42(6):572â€“593, 2013.

17. Dmitry I. Ignatov, Elena Nenova, Natalia Konstantinova, and Andrey V. Konstantinov. Boolean
Matrix Factorisation for Collaborative Filtering: An FCA-Based Approach. In AIMSA 2014,
Varna, Bulgaria, Proceedings, volume LNCS 8722, pages 47â€“58, 2014.

18. Dmitry I. Ignatov, Dmitry Tochilkin, and Dmitry Egurnov. Multimodal clustering of boolean
tensors on mapreduce: Experiments revisited. In Supplementary Proceedings of ICFCA 2019
Conference and Workshops, Frankfurt, Germany, June 25-28, 2019, pages 137â€“151, 2019.
19. Mohamed Nader Jelassi, Sadok Ben Yahia, and Engelbert Mephu Nguifo. A personalized
recommender system based on usersâ€™ information in folksonomies. In Leslie Carr and et al.,
editors, WWW (Companion Volume), pages 1215â€“1224. ACM, 2013.

20. Mehdi Kaytoue, Sergei O. Kuznetsov, Juraj Macko, and Amedeo Napoli. Biclustering meets

triadic concept analysis. Ann. Math. Artif. Intell., 70(1-2):55â€“79, 2014.

21. Mehdi Kaytoue, Sergei O. Kuznetsov, Amedeo Napoli, and SÃ©bastien Duplessis. Mining gene
expression data with pattern structures in formal concept analysis. Inf. Sci., 181(10):1989â€“
2001, 2011.

Triclustering in Big Data Setting

19

22. Petr Krajca and Vilem Vychodil. Distributed algorithm for computing formal concepts using
In N. Adams et al. (Eds.): IDA 2009, volume LNCS 5772, pages

map-reduce framework.
333â€“344, 2009.

23. Sergey Kuznecov and Yury Kudryavcev. Applying map-reduce paradigm for parallel closed
cube computation. In 1st Int. Conf. on Advances in Databases, Knowledge, and Data Appli-
cations, DBKDS 2009, pages 62â€“67, 2009.

24. Ao Li and David Tuck. An eï¬€ective tri-clustering algorithm combining expression data with

gene regulation information. Gene regul. and syst. biol., 3:49â€“64, 2009.

25. Sara C. Madeira and Arlindo L. Oliveira. Biclustering algorithms for biological data analysis:

A survey. IEEE/ACM Trans. Comput. Biology Bioinform., 1(1):24â€“45, 2004.

26. Saskia Metzler and Pauli Miettinen. Clustering boolean tensors. Data Min. Knowl. Discov.,

29(5):1343â€“1373, 2015.

27. Boris Mirkin. Mathematical Classiï¬cation and Clustering. Kluwer, Dordrecht, 1996.
28. Boris G. Mirkin and Andrey V. Kramarenko. Approximate bicluster and tricluster boxes in the
analysis of binary data. In Sergei O. Kuznetsov and et al., editors, RSFDGrC 2011, volume
6743 of Lecture Notes in Computer Science, pages 248â€“256. Springer, 2011.

29. Rokia Missaoui and LÃ©onard Kwuida. Mining triadic association rules from ternary relations.
In Formal Concept Analysis - 9th International Conference, ICFCA 2011, Nicosia, Cyprus,
May 2-6, 2011. Proceedings, pages 204â€“218, 2011.

30. Alexandros Nanopoulos, Dimitrios Rafailidis, Panagiotis Symeonidis, and Yannis Manolopou-
los. Musicbox: Personalized music recommendation based on cubic analysis of social tags.
IEEE Transactions on Audio, Speech & Language Processing, 18(2):407â€“412, 2010.

31. Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone Paolo Ponzetto, and Chris
Biemann. Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl. In Nico-
letta Calzolari (Conference chair), Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara
Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HÃ©lÃ¨ne Mazo, Asun-
cion Moreno, Jan OdÄ³k, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of
the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),
Miyazaki, Japan, May 7-12, 2018 2018. European Language Resources Association (ELRA).
32. Z. Pawlak. Information systems â€“ theoretical foundations. Information Systems, 6(3):205 â€“

218, 1981.

33. Anand Rajaraman, Jure Leskovec, and Jeï¬€rey D. Ullman. Mining of Massive Datasets, chapter
MapReduce and the New Software Stack, pages 19â€“70. Cambridge University Press, England,
Cambridge, 2013.

34. Nicole Schweikardt. One-pass algorithm.

In Encyclopedia of Database Systems, Second

Edition. 2018.

35. KÄ³ung Shin, Bryan Hooi, and Christos Faloutsos. Fast, accurate, and ï¬‚exible algorithms for

dense subtensor mining. TKDD, 12(3):28:1â€“28:30, 2018.

36. Eirini Spyropoulou, TÄ³l De Bie, and Mario Boley. Interesting pattern mining in multi-relational

data. Data Mining and Knowledge Discovery, 28(3):808â€“849, 2014.

37. Dmitry Ustalov, Alexander Panchenko, Andrey Kutuzov, Chris Biemann, and Simone Paolo
Ponzetto. Unsupervised semantic frame induction using triclustering.
In Iryna Gurevych
and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2:
Short Papers, pages 55â€“62. Association for Computational Linguistics, 2018.
38. George Voutsadakis. Polyadic concept analysis. Order, 19(3):295â€“304, 2002.
39. Rudolf Wille. Restructuring lattice theory: An approach based on hierarchies of concepts. In
Ivan Rival, editor, Ordered Sets, volume 83 of NATO Advanced Study Institutes Series, pages
445â€“470. Springer Netherlands, 1982.

40. Biao Xu, RuairÄ± de Frein, Eric Robson, and MÄ±cheal O Foghlu. Distributed formal concept
analysis algorithms based on an iterative mapreduce framework. In F. Domenach, D.I. Ignatov,
and J. Poelmans, editors, ICFCA 2012, volume LNAI 7278, pages 292â€“308, 2012.

41. Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur
Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi,

20

Egurnov et al.

Joseph Gonzalez, Scott Shenker, and Ion Stoica. Apache spark: A uniï¬ed engine for big data
processing. Commun. ACM, 59(11):56â€“65, October 2016.

42. Lizhuang Zhao and Mohammed Javeed Zaki. Tricluster: An eï¬€ective algorithm for mining
coherent clusters in 3d microarray data. In SIGMOD 2005 Conference, pages 694â€“705, 2005.
43. Sergey Zudin, Dmitry V. Gnatyshak, and Dmitry I. Ignatov. Putting oac-triclustering on mapre-
duce. In Sadok Ben Yahia and Jan Konecny, editors, Proceedings of the Twelfth International
Conference on Concept Lattices and Their Applications, Clermont-Ferrand, France, October
13-16, 2015., volume 1466 of CEUR Workshop Proceedings, pages 47â€“58. CEUR-WS.org,
2015.

