0
2
0
2

t
c
O
4
2

]

C
D
.
s
c
[

1
v
3
3
9
2
1
.
0
1
0
2
:
v
i
X
r
a

Triclustering in Big Data Setting

Dmitry Egurnov, Dmitry I. Ignatov, and Dmitry Tochilkin

Abstract In this paper1, we describe versions of triclustering algorithms adapted
for eﬃcient calculations in distributed environments with MapReduce model or par-
allelisation mechanism provided by modern programming languages. OAC-family
of triclustering algorithms shows good parallelisation capabilities due to the in-
dependent processing of triples of a triadic formal context. We provide time and
space complexity of the algorithms and justify their relevance. We also compare
performance gain from using a distributed system and scalability.

Key words: Formal Concept Analysis, n-ary relations, Boolean tensors, Data Min-
ing, Big Data, MapReduce

1 Introduction

Mining of multimodal patterns in 𝑛-ary relations or Boolean tensors is among popular
topics in Data Mining and Machine Learning [8, 2, 36, 14, 26, 35, 13]. Thus, cluster
analysis of multimodal data and speciﬁcally of dyadic and triadic relations is a
natural extension of the idea of original clustering. In dyadic case, biclustering
methods (the term bicluster was coined in [27]) are used to simultaneously ﬁnd
subsets of objects and attributes that form homogeneous patterns of the input object-
attribute data. Triclustering methods operate in triadic case, where for each object-
attribute pair one assigns a set of some conditions [28, 16, 9]. Both biclustering and
triclustering algorithms are widely used in such areas as gene expression analysis

Dmitry Egurnov · Dmitry I. Ignatov · Dmitry Tochilkin
National Research University Higher School of Economics, Russian Federation e-mail: degurnov@
hse.ru

1 The paper contains an extended version of the prior work presented at the workshop on FCA in
the Big Data Era held on June 25, 2019 at Frankfurt University of Applied Sciences, Frankfurt,
Germany [18].

1

 
 
 
 
 
 
2

Egurnov et al.

[25, 5, 42, 24, 21], recommender systems [30, 19, 17], social networks analysis [11],
natural language processing [37], etc. The processing of numeric multimodal data
is also possible by modiﬁcations of existing approaches for mining binary relations
[20]. Another interesting venue closely related to attribute dependencies in object-
attribute data also take place in triadic case, namely, mining of triadic association
rules and implications [6, 29].

Though there are methods that can enumerate all triclusters satisfying certain
constraints [2] (in most cases they ensure that triclusters are dense), their time
complexity is rather high, as in the worst case the maximal number of triclusters is
usually exponential (e.g. in case of formal triconcepts), showing that these methods
are hardly scalable. Algorithms that process big data should have at most linear time
complexity (e.g., 𝑂 (|𝐼 |) in case of 𝑛-ary relation 𝐼) and be easily parallelisable. In
addition, especially in the case of data streams [34], the output patterns should be
the results of one pass over data.

Earlier, in order to create an algorithm satisfying these requirements, we adapted
a triclustering method based on prime operators (prime OAC-triclustering method)
[9] and proposed its online version, which has linear time complexity; it is also one-
pass and easily parallelisable [10]. However, its parallelisation is possible in diﬀerent
ways. For example, one can use a popular framework for commodity hardware, Map-
Reduce (M/R) [33]. In the past, there were several successful M/R implementations
in the FCA community and other lattice-oriented domains. Thus, in [22], the authors
adapted Close-by-One algorithm to M/R framework and showed its eﬃciency. In
the same year, in [23], an eﬃcient M/R algorithm for computation of closed cube
lattices was proposed. The authors of [40] demonstrated that iterative algorithms
like Ganter’s NextClosure can beneﬁt from the usage of iterative M/R schemes.

Our previous M/R implementation of the triclustering method based on prime
operators was proposed in [43] showing computational beneﬁts on rather large
datasets. M/R triclustering algorithm [43] is a successful distributed adaptation of
the online version of prime OAC-triclustering [10]. This method uses the MapReduce
approach as means for task allocation on computational clusters, launching the online
version of prime OAC-triclustering on each reducer of the ﬁrst phase. However, due
to the simplicity of this adaptation, the algorithm does not use the advantages of
MapReduce to the full extent. In the ﬁrst stage, all the input triples are split into the
number of groups equals to the number of reducers by means of hash-function for
entities of one of the types, object, attribute, or condition, which values are used as
keys. It is clear that this way of data allocation cannot guarantee uniformness in terms
of group sizes. The JobTracker used in Apache Hadoop is able to evenly allocate
tasks by nodes2. To do so, the number of tasks should be larger than the number
of working nodes, which is not fulﬁlled in this implementation. For example, let us
assume we have 10 reduce SlaveNodes; respectively, 𝑟 = 10, and the hash function
is applied to the objects (the ﬁrst element in each input triple). However, due to
the non-uniformity of hash-function values by modulo 10, it may happen that the
set of objects will result in less than 10 diﬀerent residuals during division by 10.

2 https://wiki.apache.org/hadoop/JobTracker

Triclustering in Big Data Setting

3

Table 1 An example with triadic data

𝑖1 𝑖2

𝑢1 ×
𝑢2 × ×
𝑢3
×
𝑙1

𝑖1 𝑖2

𝑢1 ×
𝑢2 × ×
𝑢3 ×
𝑙2

In this case, the input triples will be distributed between parts of diﬀerent sizes
and processed by only a part of cluster nodes. Such cases are rather rare; it could
be possible only for slicing by entities (objects, attributes, or conditions) with a
small number of diﬀerent elements. However, they may slow down the cluster work
drastically.

The weakest link is the second stage of the algorithm. First of all, during the ﬁrst
stage, it ﬁnds triclusters computed for each data slice separately. Hence, they are not
the ﬁnal triclusters; we need to merge the obtained results.

Let us consider an example in Table 1 with the ternary relation on users-items-
labels. Let us assume that the ﬁrst mapper splits data according to their labels’
component, 𝑟 = 2, then triples containing label 𝑙1 and those related to label 𝑙2 are
processed on diﬀerent nodes. After the ﬁrst stage completion on the ﬁrst node, we
have tricluster ({𝑢2}, {𝑖1, 𝑖2}, {𝑙1}) among the others, while the second node results
in tricluster ({𝑢2}, {𝑖1, 𝑖2}, {𝑙2}). It is clear that both triclusters are not complete
for the whole input dataset and should be merged into ({𝑢2}, {𝑖1, 𝑖2}, {𝑙1, 𝑙2}). The
second stage of the algorithm is responsible for this type of merging. However, as
one can see, this merging assumes that all intermediate data should be located on
the same node. In a big data setting, this allocation of all the intermediate results on
a single node is a critical point for application performance.

To calculate tricluster components (or cumuli, see Section 3.1) and assemble the
ﬁnal triclusters from them, we need to have large data slices on the computational
nodes. Moreover, during parallelisation of the algorithm, those data slices can be
required simultaneously on diﬀerent nodes. Thus, to solve these problems one needs
to fulﬁl data centralisation (all the required slices should be present at the same node
simultaneously). However, it leads to the accumulation of too large parts of the data
as described. Another approach is to perform data replication. In this case, the total
amount of data processed on computational nodes is increased, while the data are
evenly distributed in the system. The latter approach has been chosen for our updated
study on multimodal clustering.

Note that experts warn the potential M/R users: “the entire distributed-ﬁle-system
milieu makes sense only when ﬁles are very large and are rarely updated in place”
[33]. In this work, as in our previous study, we assume that there is a large bulk of data
to process that is not coming online. As for experimental comparison, since we would
like to follow an authentic M/R approach, without using the online algorithm [10],
we implement and enhance only MapReduce schemes from the appendix in [43]
supposed for future studies in that time.

4

Egurnov et al.

The rest of the paper is organised as follows: in Section 2, we recall the original
method and the online version of the algorithm of prime OAC-triclustering. Section 3
is dedicated to extensions of OAC triclustering. Speciﬁcally, Subsection 3.1 gener-
alise prime OAC-triclustering for the case of multimodal data, and Subsection 3.2
touches the case of multi-valued context. In Section 4 we describe implementation
details. Subsection 4.1, gives the M/R setting of the problem and the corresponding
M/R version of the original algorithm with important implementation aspects. In
Subsection 4.3 we speak about parallel implementation of OAC triclustering. Fi-
nally, in Section 5 we show the results of several experiments that demonstrate the
eﬃciency of the M/R version of the algorithm.

2 Prime object-attribute-condition triclustering

Prime object-attribute-condition triclustering method (OAC-prime) based on Formal
Concept Analysis [39, 7] is an extension for the triadic case of object-attribute bi-
clustering method [15]. Triclusters generated by this method have a similar structure
as the corresponding biclusters, namely the cross-like structure of triples inside the
input data cuboid (i.e. formal tricontext).

Let K = (𝐺, 𝑀, 𝐵, 𝐼) be a triadic context, where 𝐺, 𝑀, 𝐵 are respectively the
sets of objects, attributes, and conditions, and 𝐼 ⊆ 𝐺 × 𝑀 × 𝐵 is a triadic incidence
relation. Each prime OAC-tricluster is generated by applying the following prime
operators to each pair of components of some triple:

(𝑋, 𝑌 ) (cid:48) = {𝑏 ∈ 𝐵 | (𝑔, 𝑚, 𝑏) ∈ 𝐼 for all 𝑔 ∈ 𝑋, 𝑚 ∈ 𝑌 },
(𝑋, 𝑍) (cid:48) = {𝑚 ∈ 𝑀 | (𝑔, 𝑚, 𝑏) ∈ 𝐼 for all 𝑔 ∈ 𝑋, 𝑏 ∈ 𝑍 },
(𝑌 , 𝑍) (cid:48) = {𝑔 ∈ 𝐺 | (𝑔, 𝑚, 𝑏) ∈ 𝐼 for all 𝑚 ∈ 𝑌 , 𝑏 ∈ 𝑍 },

(1)

where 𝑋 ⊆ 𝐺, 𝑌 ⊆ 𝑀, and 𝑍 ⊆ 𝐵.

Then the triple 𝑇 = ((𝑚, 𝑏) (cid:48), (𝑔, 𝑏) (cid:48), (𝑔, 𝑚) (cid:48)) is called prime OAC-tricluster
based on triple (𝑔, 𝑚, 𝑏) ∈ 𝐼. The components of tricluster are called, respectively,
tricluster extent, tricluster intent, and tricluster modus. The triple (𝑔, 𝑚, 𝑏) is called
a generating triple of the tricluster 𝑇. Density of a tricluster 𝑇 = (𝐺𝑇 , 𝑀𝑇 , 𝐵𝑇 ) is
the relation of actual number of triples in the tricluster to its size:

𝜌(𝑇) =

|𝐺𝑇 × 𝑀𝑇 × 𝐵𝑇 ∩ 𝐼 |
|𝐺𝑇 ||𝑀𝑇 ||𝐵𝑇 |

In these terms a triconcept is a tricluster with density 𝜌 = 1.

Figure 1 shows the structure of an OAC-tricluster (𝑋, 𝑌 , 𝑍) based on triple
𝑚, (cid:101)𝑏), triples corresponding to the gray cells are contained in the tricluster,
(cid:101)

𝑔,
((cid:101)
other triples may be contained in the tricluster (cuboid) as well.

The basic algorithm for the prime OAC-triclustering method is rather straightfor-
ward (see [9]). First of all, for each combination of elements from each of the two
sets of K we apply the corresponding prime operator (we call the resulting sets prime

Triclustering in Big Data Setting

5

Fig. 1 Structure of prime OAC-triclusters: the dense cross-like central layer containing ˜𝑔 (left) and
the layer for an object 𝑔 (right) in 𝑀 × 𝐵 dimensions.

sets). After that, we enumerate all triples from 𝐼 and on each step, we must generate
a tricluster based on the corresponding triple, check whether this tricluster is already
contained in the tricluster set (by using hashing) and also check extra conditions.

The total time complexity of the algorithm depends on whether there is a non-zero
minimal density threshold or not and on the complexity of the hashing algorithm
used. In case we use some basic hashing algorithm processing the tricluster’s extent,
intent and modus without a minimal density threshold, the total time complexity is
𝑂 (|𝐺 ||𝑀 ||𝐵| + |𝐼 |(|𝐺 | + |𝑀 | + |𝐵|)) (assuming the hashing algorithm takes 𝑂 (1) to
operate, it takes 3 × 𝑂 (|𝐺 ||𝑀 ||𝐵|) to precompute the prime sets and then 𝑂 (|𝐺 | +
|𝑀 | + |𝐵|) for each triple (𝑔, 𝑚, 𝑏) ∈ 𝐼 to compute the hash); in case of a non-zero
minimal density threshold, it is 𝑂 (|𝐼 ||𝐺 ||𝑀 ||𝐵|) (since computing density takes
𝑂 (|𝐺 ||𝑀 ||𝐵|) for each tricluster generated from a triple (𝑔, 𝑚, 𝑏) ∈ 𝐼). The memory
complexity is 𝑂 (|𝐼 |(|𝐺 | + |𝑀 | + |𝐵|)), as we need to keep the dictionaries with the
prime sets in memory.

In online setting, for triples coming from triadic context K = (𝐺, 𝑀, 𝐵, 𝐼), the
user has no a priori knowledge of the elements and even cardinalities of 𝐺, 𝑀, 𝐵, and
𝐼. At each iteration, we receive some set of triples from 𝐼: 𝐽 ⊆ 𝐼. After that, we must
process 𝐽 and get the current version of the set of all triclusters. It is important in this
setting to consider every pair of triclusters as being diﬀerent as they have diﬀerent
generating triples, even if their respective extents, intents, and modi are equal. Thus,
any other triple can change only one of these two triclusters, making them diﬀerent.
To eﬃciently access prime sets for their processing, the dictionaries containing

the prime sets are implemented as hash-tables.

The algorithm is straightforward as well (Alg. 1). It takes some set of triples (𝐽),
the current tricluster set (T ), and the dictionaries containing prime sets (𝑃𝑟𝑖𝑚𝑒𝑠)
as input and outputs the modiﬁed versions of the tricluster set and dictionaries. The
algorithm processes each triple (𝑔, 𝑚, 𝑏) of 𝐽 sequentially (line 1). At each iteration,
the algorithm modiﬁes the corresponding prime sets (lines 2-4).

Finally, it adds a new tricluster to the tricluster set. Note that this tricluster contains
pointers to the corresponding prime sets (in the corresponding dictionaries) instead
of the copies of the prime sets (line 5) which allows lowering the memory and access
costs.

6

Egurnov et al.

Algorithm 1 Add function for the online algorithm for prime OAC-triclustering.
Require: 𝐽 is a set of triples;

T = {𝑇 = (∗𝑋 , ∗𝑌 , ∗𝑍 ) } is a current set of triclusters;
𝑃𝑟 𝑖𝑚𝑒𝑠𝑂 𝐴, 𝑃𝑟 𝑖𝑚𝑒𝑠𝑂𝐶, 𝑃𝑟 𝑖𝑚𝑒𝑠 𝐴𝐶.

Ensure: T = {𝑇 = (∗𝑋 , ∗𝑌 , ∗𝑍 ) };

𝑃𝑟 𝑖𝑚𝑒𝑠𝑂 𝐴, 𝑃𝑟 𝑖𝑚𝑒𝑠𝑂𝐶, 𝑃𝑟 𝑖𝑚𝑒𝑠 𝐴𝐶.

1: for all (𝑔, 𝑚, 𝑏) ∈ 𝐽 do
2:
3:
4:
5:
6: end for

𝑃𝑟 𝑖𝑚𝑒𝑠𝑂 𝐴[𝑔, 𝑚] := 𝑃𝑟 𝑖𝑚𝑒𝑠𝑂 𝐴[𝑔, 𝑚] ∪ {𝑏 }
𝑃𝑟 𝑖𝑚𝑒𝑠𝑂𝐶 [𝑔, 𝑏] := 𝑃𝑟 𝑖𝑚𝑒𝑠𝑂𝐶 [𝑔, 𝑏] ∪ {𝑚}
𝑃𝑟 𝑖𝑚𝑒𝑠 𝐴𝐶 [𝑚, 𝑏] := 𝑃𝑟 𝑖𝑚𝑒𝑠 𝐴𝐶 [𝑚, 𝑏] ∪ {𝑔 }
T := T ∪ { (&𝑃𝑟 𝑖𝑚𝑒𝑠 𝐴𝐶 [𝑚, 𝑏], &𝑃𝑟 𝑖𝑚𝑒𝑠𝑂𝐶 [𝑔, 𝑏], &𝑃𝑟 𝑖𝑚𝑒𝑠𝑂 𝐴[𝑔, 𝑚]) }

The algorithm is one-pass and its time and memory complexities are 𝑂 (|𝐼 |).
Duplicate elimination and selection patterns by user-speciﬁc constraints are done
as post-processing to avoid patterns’ loss. The time complexity of the basic post-
processing is 𝑂 (|𝐼 |) and it does not require any additional memory.

The algorithm can be easily parallelised by splitting the subset of triples 𝐽 into
several subsets, processing each of them independently, and merging the resulting
sets afterwards. This fact results in our previous MapReduce implementation [43].

3 Triclustering extensions

3.1 Multimodal clustering

The direct extension of the prime object-attribute-condition triclustering is multi-
modal clustering for higher input relation arities. For the input polyadic context
K𝑁 = ( 𝐴1, 𝐴2, . . . , 𝐴𝑁 , 𝐼 ⊆ 𝐴1 × 𝐴2 × · · · × 𝐴𝑁 ) [38], we introduce the notion of
cumulus for each input tuple 𝑖 = (𝑒1, 𝑒2, . . . , 𝑒 𝑁 ) ∈ 𝐼 and the corresponding entity
𝑒𝑘 , where 𝑘 ∈ {1, . . . , 𝑁 } as follows:

𝑐𝑢𝑚(𝑖, 𝑘) = {𝑒 | (𝑒1, 𝑒2, . . . , 𝑒𝑘−1, 𝑒, 𝑒𝑘+1, . . . , 𝑒 𝑁 ) ∈ 𝐼}.

The multimodal cluster generated by the tuple 𝑖 ∈ 𝐼 is deﬁned as follows:

(cid:0)(𝑐𝑢𝑚(𝑖, 1), . . . , 𝑐𝑢𝑚(𝑖, 𝑁)(cid:1).

Those cumuli operators are similar to primes for pairs (or tuples) of sets Eq. 1:

𝑐𝑢𝑚(𝑖, 𝑘) = ({𝑒1}, {𝑒2}, . . . , {𝑒𝑘−1}, {𝑒𝑘+1}, . . . , {𝑒 𝑁 }) (cid:48).

However, here, they are applied to the tuples of input relation rather than to pairs

(tuples) of sets.

In a certain sense, cumuli accumulate all the elements of a ﬁxed type that are

related by 𝐼.

Triclustering in Big Data Setting

7

As its triadic version, multimodal clustering is not greater than the number of
tuples in the input relation, whereas the complete set of polyadic concepts may be
exponential w.r.t. the input size [38].

3.2 Many-valued Triclustering

Another extension of prime OAC triclustering is many-valued triclustering where
each triple of the incidence relation of a triadic context is associated with a certain
value from an arbitrary set 𝑊 (by means of a valuation function 𝑉 : 𝐼 → 𝑊).
Therefore an input triadic context is changed to a many-valued triadic context KV =
(𝐺, 𝑀, 𝐵, 𝑊, 𝐼, 𝑉), where for each triple (𝑔, 𝑚, 𝑏) in ternary relation 𝐼 between 𝐺,
𝑀, 𝐵, there is a unique 𝑉 (𝑔, 𝑚, 𝑏) ∈ 𝑊.

This deﬁnition contains the explicitly given valuation function 𝑉 similar to the
one in the deﬁnition of attribute-based information systems [32]. However, we could
also extend the original deﬁnition of a many-valued context in line with [7]. In this
case, a many-valued triadic context K = (𝐺, 𝑀, 𝐵, 𝑊, 𝐽) consists of sets 𝐺, 𝑀, 𝐵,
𝑊 and a quaternary relation between them 𝐽 ⊆ 𝐺 × 𝑀 × 𝐵 × 𝑊, where the following
holds: (𝑔, 𝑚, 𝑏, 𝑤) ∈ 𝐽 and (𝑔, 𝑚, 𝑏, 𝑣) ∈ 𝐽 imply 𝑤 = 𝑣.

In what follows, we prefer the ﬁrst variant since it simpliﬁes further exposition.
For numeric values, the most common case is for 𝑊 = R. To mine triclusters in
such context prime operators are modiﬁed into so-called 𝛿-operators. For a generating
triple ( ˜𝑔, ˜𝑚, ˜𝑏) ∈ 𝐼 and some parameter 𝛿:

( ˜𝑚, ˜𝑏) 𝛿 = (cid:8)𝑔 | (𝑔, ˜𝑚, ˜𝑏) ∈ 𝐼 ∧ |𝑉 (𝑔, ˜𝑚, ˜𝑏) − 𝑉 ( ˜𝑔, ˜𝑚, ˜𝑏)| ≤ 𝛿(cid:9)

( ˜𝑔, ˜𝑏) 𝛿 = (cid:8)𝑚 | ( ˜𝑔, 𝑚, ˜𝑏) ∈ 𝐼 ∧ |𝑉 ( ˜𝑔, 𝑚, ˜𝑏) − 𝑉 ( ˜𝑔, ˜𝑚, ˜𝑏)| ≤ 𝛿(cid:9)
( ˜𝑔, ˜𝑚) 𝛿 = (cid:8)𝑏 | ( ˜𝑔, ˜𝑚, 𝑏) ∈ 𝐼 ∧ |𝑉 ( ˜𝑔, ˜𝑚, 𝑏) − 𝑉 ( ˜𝑔, ˜𝑚, ˜𝑏)| ≤ 𝛿(cid:9)
This deﬁnition can still be used to mine regular triclusters, if we set 𝑊 = {0, 1}

and 𝛿 = 0.

4 Implementations

4.1 Map-reduce-based multimodal clustering

We follow a three-stage approach here. On each stage, we sequentially run the map
and reduce procedures: First map → First reduce → Second map → Second reduce →
Third map → Third reduce. Each map/reduce procedure of a certain stage is executed
in parallel on all the available nodes/clusters. How tasks are distributed among the
nodes/clusters depends on the concrete MapReduce technology implementation (in

8

Egurnov et al.

our case, Apache Hadoop). Below, we describe the data ﬂow between computational
nodes and their processing.

1) The ﬁrst map (Algorithm 2)

tuples. Each tu-
ple (𝑒1, 𝑒2, . . . , 𝑒 𝑁 ) is transformed into 𝑁 key-value pairs: (cid:104)(𝑒2, . . . , 𝑒 𝑁 ), 𝑒1(cid:105),
(cid:104)𝑒1, 𝑒3, . . . , 𝑒 𝑁 ), 𝑒2(cid:105), . . . , (cid:104)(𝑒1, 𝑒2, . . . , 𝑒 𝑁 −1), 𝑒 𝑁 (cid:105). The resulting pairs are passed
to the further step.

takes a set of

input

Algorithm 2 Distributed Multimodal clustering: First Map
Require: 𝐼 is a set of tuples of length 𝑁 each
Ensure: (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑒𝑛𝑡𝑖𝑡 𝑦 (cid:105) pairs.
1: for all (𝑒1, 𝑒2, . . . , 𝑒𝑁 ) ∈ 𝐼 do
2:
3:
4:
5:
6: end for

𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛 := (𝑒1, . . . , 𝑒𝑘−1, 𝑒𝑘+1, . . . , 𝑒𝑁 )
emit (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑒𝑘 (cid:105)

for all 𝑘 ∈ {1, . . . , 𝑁 } do

end for

2) The ﬁrst reduce (Algorithm 3) receives all the accumulated values of each key.
Thus, for each (𝑒1, . . . , 𝑒 𝑁 ) ∈ 𝐼 and the context entity type 𝑘 ∈ {1, 2, . . . , 𝑁 }, we
compute the cumulus (𝑒1, . . . , 𝑒𝑘−1, 𝑒𝑘+1, . . . , 𝑒 𝑁 ) (cid:48). The values are passed to the
next MapReduce stage with the key (𝑒1, . . . , 𝑒𝑘−1, 𝑒𝑘+1, . . . , 𝑒 𝑁 ).

Algorithm 3 Distributed Multimodal clustering: First Reduce
Require: key-value pairs (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑒𝑛𝑡𝑖𝑡𝑖𝑒𝑠 {𝑒1
Ensure: (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑢𝑠(cid:105)
1: cumulus:={}
2: for all 𝑒𝑘 ∈ {𝑒1
3:
4:
5: end for

𝑘 , . . . , 𝑒𝐿
𝑐𝑢𝑚𝑢𝑙𝑢𝑠 := 𝑐𝑢𝑚𝑢𝑙𝑢𝑠 ∪ {𝑒𝑘 }
emit (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑢𝑠(cid:105)

𝑘 , . . . , 𝑒𝐿

𝑘 } do

𝑘 }(cid:105)

3) Second map (Algorithm 4). All the received keys are transformed into the
original relations and passed to the second reduce procedure with unchanged values.

Algorithm 4 Distributed Multimodal clustering: Second Map
Require: (cid:104)𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑢𝑠(cid:105), where 𝑠𝑢𝑏𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛 = (𝑒1, . . . , 𝑒𝑘−1, 𝑒𝑘+1, . . . , 𝑒𝑁 )
Ensure: (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑢𝑠(cid:105) pairs.
1: for all 𝑒𝑘 ∈ 𝑐𝑢𝑚𝑢𝑙𝑢𝑠 do
2:
3:
4: end for

𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛 := (𝑒1, . . . , 𝑒𝑘−1, 𝑒𝑘 , 𝑒𝑘+1, . . . , 𝑒𝑁 )
emit (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑢𝑠(cid:105)

4) Second reduce (Algorithm 5). All the cumuli obtained for each input tu-
ple of the original relation 𝐼 are reduced to a single set. At this stage, we ob-

Triclustering in Big Data Setting

9

tain all the original tuples and generated multimodal clusters. These clusters are
presented as tuples of cumuli for respective entity types. All the obtained pairs
(cid:104)𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑛𝑔_𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟(cid:105) are passed to the next stage.

Algorithm 5 Distributed Multimodal clustering: Second Reduce
Require: (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑐𝑢𝑚𝑢𝑙𝑖 { 𝐴1, 𝐴2, · · · , 𝐴𝑁 }(cid:105)
Ensure: (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 (cid:105) pairs
1: 𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 := ( 𝐴1, 𝐴2, · · · , 𝐴𝑁 )
2: emit (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 (cid:105)

5) Third map (Algorithm 6). The task of the third MapReduce stage is duplicate
elimination and ﬁltration by density threshold. It is beneﬁcial to implement within
the reduce step, but to do so each obtained key-value pair (cid:104)𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑛𝑔_𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛,
𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟(cid:105) should be passed further as follows (cid:104)𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟,
𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑛𝑔_𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛(cid:105).

Algorithm 6 Distributed Multimodal clustering: Third Map
Require: (cid:104)𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 (cid:105)
1: emit (cid:104)𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 , 𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛(cid:105)

6) Third reduce (Algorithm 7). For each input multimodal cluster and its generat-
ing tuples, it is possible to directly compute density. All the unique clusters will be
stored.

Algorithm 7 Distributed Multimodal clustering: Third Reduce
Require: (cid:104)𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 , 𝑔𝑒𝑛𝑒𝑟 𝑎𝑡𝑖𝑛𝑔_𝑟 𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑠 {𝑟1, 𝑟2 . . . , 𝑟𝑀 }(cid:105)
1: if

≥ 𝜃 then

| {𝑟1, 𝑟2 . . . , 𝑟𝑀 } |
𝑣𝑜𝑙 (𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 )
store (cid:104)𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙_𝑐𝑙𝑢𝑠𝑡𝑒𝑟 (cid:105)

2:
3: end if

The time and memory complexities are provided assuming that the worst-case
scenario corresponds to the absolutely dense cuboid, i.e. polyadic context K𝑁 =
( 𝐴1, 𝐴2, . . . , 𝐴𝑁 , 𝐴1 × 𝐴2 × · · · × 𝐴𝑁 ). Thus, after careful analysis, the worst-case
time complexity of the proposed three-stage algorithm is 𝑂 (|𝐼 | (cid:205)𝑁
𝑗=1 | 𝐴 𝑗 |). Not
surprisingly it has the same worst-case memory complexity since the stored and
passed the maximal number of multimodal clusters is |𝐼 |, and the size of each of
them is not greater (cid:205)𝑁
𝑗=1 | 𝐴 𝑗 |. However, from an implementation point of view, since
HDFS has default replication factor 3, those data elements are copied thrice to fulﬁl
fault-tolerance.

10

Egurnov et al.

4.2 Implementation aspects and used technologies

The application3 has been implemented in Java and as distributed computation
framework we use Apache Hadoop4.

We have used many other technologies: Apache Maven (framework for automatic
project assembling), Apache Commons (for work with extended Java collections),
Jackson JSON (open-source library for the transformation of object-oriented repre-
sentation of an object like tricluster to string), TypeTools (for real-time type resolution
of inbound and outbound key-value pairs), etc.

To provide the reader with basic information on the most important classes for

M/R implementation, let us shortly describe them below.
Entity. This is a basic abstraction for an element of a certain type. Each entity is
deﬁned by its type index from 0 to 𝑛-1, where 𝑛 is the arity of the input formal context.
An entity value is a string that needs to be kept during the program execution. This
class inherits Writable interface for storing its objects in temporary and permanent
Hadoop ﬁles. This is a mandatory requirement for all classes that pass or take their
objects as keys and values of the map and reduce methods.
Tuple. This class implements a representation of relation. Each object of the class
Tuple contains the list of objects of Entity class and the arity of its data given by
its numeric value. This class implements interface WritableComparable<Tuple> to
make it possible to use an object class Tuple as a key. The interface is similar to
Writable, however, one needs to deﬁne comparison function to use in the key sorting
phase.
Cumulus. This is an abstraction of cumulus, introduced earlier. It contains the list
of string values and the index of a respective entity. It also implements the fol-
lowing interfaces: WritableComparable<Cumulus> for using cumulus as a key and
Iterable<String> for iteration by its values.
FormalConcept. This is an abstraction of both formal concepts and multimodal
clusters, it contains the list of cumuli and implements interface Writable.

The process-like M/R classes are summarised below.

FirstMapper, SecondMapper, ThirdMapper. These are the classes that extend class
Mapper<> of the Hadoop MapReduce library by respective mapping function from
Subsection 4.1.
FirstReducer, SecondReducer, ThirdReducer. These classes extend class Reducer<>
of the Hadoop MapReduce library for fulﬁlling Algorithms 2,4,6.
TextCumulusInputFormat, TextCumulusOutputFormat. These classes implement
reading and writing for objects of Cumulus class; they also inherit RecordReader and
RecordWriter interfaces, respectively. They are required to exchange results between
diﬀerent MapReduce phases within one MapReduce program.
JobConﬁgurator. This is the class for setting conﬁguration of a single MapReduce
stage. It deﬁnes the classes of input/output/intermediate keys and values of the
mapper and reducer as well as formats of input and output data.

3 https://github.com/kostarion/multimodal-clustering-hadoop
4 https://hadoop.apache.org/

Triclustering in Big Data Setting

11

App. This class is responsible for launching the application and chaining of M/R
stages.

4.3 Parallel many-valued triclustering

A generic algorithm for OAC triclustering is described below

Algorithm 8 General algorithm for OAC triclustering
Require: Context K = (𝐺, 𝑀 , 𝐵, 𝐼 )
Ensure: Set of triclusters T
1: T := ∅
2: for all (𝑔, 𝑚, 𝑏) ∈ 𝐼 do
3:
4:
5:
6:
7:
8:
end if
9:
10: end for
11: return T

𝑜𝑆𝑒𝑡 := 𝑎 𝑝 𝑝𝑙𝑦 𝑃𝑟 𝑖𝑚𝑒𝑂 𝑝𝑒𝑟 𝑎𝑡𝑜𝑟 (𝑚, 𝑏)
𝑎𝑆𝑒𝑡 := 𝑎 𝑝 𝑝𝑙𝑦 𝑃𝑟 𝑖𝑚𝑒𝑂 𝑝𝑒𝑟 𝑎𝑡𝑜𝑟 (𝑔, 𝑏)
𝑐𝑆𝑒𝑡 := 𝑎 𝑝 𝑝𝑙𝑦 𝑃𝑟 𝑖𝑚𝑒𝑂 𝑝𝑒𝑟 𝑎𝑡𝑜𝑟 (𝑔, 𝑚)
𝑡𝑟 𝑖𝑐𝑙𝑢𝑠𝑡𝑒𝑟 := (𝑜𝑆𝑒𝑡 , 𝑎𝑆𝑒𝑡 , 𝑐𝑆𝑒𝑡)
if 𝑡𝑟 𝑖𝑐𝑙𝑢𝑠𝑡𝑒𝑟 is valid then
𝐴𝑑𝑑 ( T, 𝑡𝑟 𝑖𝑐𝑙𝑢𝑠𝑡𝑒𝑟 )

To get a speciﬁc version of the algorithm one only needs to add an appropriate
implementation of the prime operator and optional validity check. A tricluster mined
from one triple does not depend on triclusters mined from other triples, so, in case
of parallel implementation, each triple is processed in an individual thread.

We used 𝛿-operators deﬁned in 3.2, minimal density, and minimal cardinality

(w.r.t. to every dimension) constraints [3].

5 Experiments

Two series of experiments have been conducted in order to test the application on
the synthetic contexts and real-world datasets with a moderate and large number of
triples in each. In each experiment, both versions of the OAC-triclustering algorithm
have been used to extract triclusters from a given context. Only online and M/R
versions of OAC-triclustering algorithm have managed to result in patterns for large
contexts since the computation time of the compared algorithms was too high (>3000
s). To evaluate the runtime more carefully, for each context the average result of 5
runs of the algorithms has been recorded.

12

5.1 Datasets

Egurnov et al.

Synthetic datasets. The following synthetic datasets were generated.
The dense context K1 = (𝐺, 𝑀, 𝐵, 𝐼), where 𝐺 = 𝑀 = 𝐵 = {1, . . . , 60} and
𝐼 = 𝐺 × 𝑀 × 𝐵 \ {(𝑔, 𝑚, 𝑏) ∈ 𝐼 | 𝑔 = 𝑚 = 𝑏}. In total, 603 - 60 =215,940 triples.
The context of three non-overlapped cuboids K2 = (𝐺1 (cid:116) 𝐺2 (cid:116) 𝐺3, 𝑀1 (cid:116) 𝑀2 (cid:116)
𝑀3, 𝐵1 (cid:116) 𝐵2 (cid:116) 𝐵3, 𝐼), where 𝐼 = (𝐺1 × 𝑀1 × 𝐵1) ∪ (𝐺2 × 𝑀2 × 𝐵2) ∪ (𝐺3 × 𝑀3 × 𝐵3).
In total, 3 · 503 = 375, 000 triples.
The context K3 = ( 𝐴1, 𝐴2, 𝐴3, 𝐴4, 𝐴1 × 𝐴2 × 𝐴3 × 𝐴4) is a dense fourth dimensional
cuboid with | 𝐴1| = | 𝐴2| = | 𝐴3| = | 𝐴4| = 30 containing 304 = 810, 000 triples.

These tests have sense since in M/R setting due to the tuples can be (partially)
repeated, e.g., because of M/R task failures on some nodes (i.e. restarting pro-
cessing of some key-value pairs). Even though the third dataset does not result in
3𝑚𝑖𝑛( | 𝐴1 |, | 𝐴2 |, | 𝐴3 |, | 𝐴4 |) formal triconcepts, the worst-case for formal triconcepts gen-
eration in terms of the number of patterns, this is an example of the worst-case
scenario for the reducers since the input has its maximal size w.r.t. to the size of 𝐴𝑖-s
and the number of duplicates. In fact, our algorithm correctly assembles the only
one tricluster ( 𝐴1, 𝐴2, 𝐴3, 𝐴4).
IMDB. This dataset consists of the 250 best movies from the Internet Movie Database
based on user reviews.

The following triadic context is composed: the set of objects consists of movie
names, the set of attributes (tags), the set of conditions (genres), and each triple of
the ternary relation means that the given movie has the given genre and is assigned
the given tag. In total, there are 3,818 triples.
Movielens. The dataset contains 1,000,000 tuples that relate 6,040 users, 3,952
movies, ratings, and timestamps, where ratings are made on a 5-star scale [12].
Bibsonomy. Finally, a sample of the data of bibsonomy.org from ECML PKDD
discovery challenge 2008 has been used.

This website allows users to share bookmarks and lists of literature and tag them.
For the tests the following triadic context has been prepared: the set of objects
consists of users, the set of attributes (tags), the set of conditions (bookmarks), and
a triple of the ternary relation means that the given user has assigned the given tag
to the given bookmark.

Table 2 contains the summary of IMDB and Bibsonomy contexts.

Table 2 Tricontexts based of real data systems for the experiments

Context
IMDB

|𝐺 |
250

| 𝑀 |
795

|𝐵 | # triples Density
3,818 0.00087
22

BibSonomy 2,337 67,464 28,920 816,197 1.8 · 10−7

Due to a number of requests, we would like to show small excerpts of the input
data we used (in this section) and a few examples of the outputted patterns (in the
next section).

Triclustering in Big Data Setting

13

Input data example. The input ﬁle for the Top-250 IMDB dataset comprises
triples in lines with tab characters as separators.

One Flew Over the Cuckoo’s Nest (1975) Nurse Drama
One Flew Over the Cuckoo’s Nest (1975)
Patient Drama
One Flew Over the Cuckoo’s Nest (1975) Asylum Drama
One Flew Over the Cuckoo’s Nest (1975) Rebel Drama
One Flew Over the Cuckoo’s Nest (1975) Basketball Drama
Star Wars V: The Empire Strikes Back (1980)
Star Wars V: The Empire Strikes Back (1980)
Star Wars V: The Empire Strikes Back (1980)
. . .

Princess Action
Princess Adventure
Princess

Sci-Fi

5.2 Results

The experiments have been conducted on the computer Intel®Core(TM) i5-2450M
CPU @ 2.50GHz, 4Gb RAM (typical commodity hardware) in the emulation mode,
when Hadoop cluster contains only one node and operates locally and sequentially.
By time execution results one can estimate the performance in a real distributed
environment assuming that each node workload is (roughly) the same.

Table 3 Three-stage MapReduce multimodal clustering time, ms

IMDB MovieLens100k K1
Method
16,298
368
Online OAC prime clustering
14,582
MapReduce multimodal clustering 7,124

K2
96,990 185,072 643,978
37,572 61,367 102,699

K3

Table 4 Three-stage MapReduce multimodal clustering time, ms

Dataset

Online M/R total

MapReduce stages

# clusters

OAC Prime
89,931
MovieLens100k
225,242
MovieLens250k
MovieLens500k
461,198
MovieLens1M 958,345
Bibsonomy
(≈800k triples)

2nd
1st
5,292
8,724
16,348
42,708 10,075 20,338
94,701 15,016 46,300
217,694 28,027 114,221

89,932
225,251
461,238
942,757
> 6 hours 3,651,072 19,117 1,972,135 1,659,820 486,221

3rd
2,332
12,295
33,384
74,446

(≈1 hour)

In Tables 3 and 4 we summarise the results of performed tests. It is clear that
on average our application has a smaller execution time than its competitor, the on-

Egurnov et al.

Online OAC prime clustering
MapReduce multimodal clustering

14

s

,
e
m
T

i

1,200

1,000

800

600

400

200

0

I M100K K1

K2

Dataset

K3

M

·106

Fig. 2 Performance curves for six datasets: I stands for IMDB dataset with 3,818 triples, M100K
– MovieLens dataset with 100K tuples, M – MovieLens dataset with 1M tuples

line version of OAC-triclustering. If we compare the implemented program with its
original online version, the results are worse for the not that big and sparse dataset
as IMDB. It is the consequence of the fact that the application architecture aimed
at processing large amounts of data; in particular, it is implemented in three stages
with time-consuming communication. Launching and stopping Apache Hadoop, data
writing, and passing between Map and Reduce steps in both stages requires substan-
tial time, that is why for not that big datasets when execution time is comparable
with time for infrastructure management, time performance is not perfect. However,
with data size increase the relative performance is growing up to ﬁve-six times (see
Fig. 2). Thus, the last test for BibSonomy data has been successfully passed, but the
competitor was not able to ﬁnish it within one hour. As for the M/R stages, the most
time-consuming phases are the 2nd and 3rd stages.

Output triclusters example. The output triclusters are stored in a format,
similar to JSON: the sets of entities (modalities) are given in curly brackets
separated with commas. Both sets of entities and triclusters start with a new
line.

Thus, for the modality of movies, in case of IMDB data, we can see two

remaining modalities as keywords and genres.

{
{Apocalypse Now (1979), Forrest Gump (1994),
Full Metal Jacket (1987), Platoon (1986)}

Triclustering in Big Data Setting

15

{Vietnam}
{Drama, Action}
}
{
{Toy Story (1995), Toy Story 2 (1999)}
{Toy, Friend}
{Animation, Adventure, Comedy, Family, Fantasy}
}
{
{Star Wars: Episode V (cid:21) The Empire Strikes Back (1980),
WALL-E (2008), Toy Story 2 (1999)}
{Rescue}
{Animation, Adventure}
}
{
{Into the Wild (2007), The Gold Rush (1925)}
{Love, Alaska}
{Adventure}
}
...

6 Experiments with parallelisation

In the additional set of experiments, we investigated parallelisation as another way of
improving the performance of our triclustering algorithms. Modern computers sup-
port multi-threading and have several processing cores. However one needs special
instructions and thread-safe data structures to produce an eﬃcient parallel algorithm.
We used a dataset on semantic tri-frames featured in [37]. These triples represent
semantic frames extracted from FrameNet1.7 [1] and each triple is accompanied
by a frequency from DepCC dataset [31]. The total count of triples reached 100
thousand. The data was processed with the NOAC algorithm [3] implemented in C#
.NetFramework 4.5 [4] and modiﬁed for parallel computation with Parallel library,
namely each triple from the context is processed in a separate thread. We ran two
series of experiments with diﬀerent algorithm parameters and measured execution
time against the number of processed triples and also provided the number of ex-
tracted triclusters. In Table 5, for example, the record NOAC(100, 0.8, 2) 1k means
that 𝛿 = 100, 𝜌𝑚𝑖𝑛 = 0.8, 𝑚𝑖𝑛𝑠𝑢 𝑝 = 2 (for each dimension) and 1,000 triples are
being processed. All additional experiments were conducted on an Intel®Core(TM)
i7-8750H CPU @ 2.20GHz, 16Gb RAM.

These experiments show that performance noticeably beneﬁts from parallelisa-
tion. The execution time of the parallel algorithm is on average 35% lower. Another

16

Egurnov et al.

Table 5 NOAC. Regular ans parallel version

Time, ms (regular) Time, ms (parallel) # Triclusters

109
5,025
16,825
33,067
56,878
80,095
102,092
133,974
175,597
223,932
268,021
110
5,121
82,130
268,128

117
3,642
11,759
22,519
36,994
52,322
67,748
87,956
110,044
135,268
157,073
169
3,681
52,558
159,333

0
3
20
52
92
116
145
160
201
223
254
803
4942
14214
23134

Experiment
NOAC(100, 0.8, 2) 1k
NOAC(100, 0.8, 2) 10k
NOAC(100, 0.8, 2) 20k
NOAC(100, 0.8, 2) 30k
NOAC(100, 0.8, 2) 40k
NOAC(100, 0.8, 2) 50k
NOAC(100, 0.8, 2) 60k
NOAC(100, 0.8, 2) 70k
NOAC(100, 0.8, 2) 80k
NOAC(100, 0.8, 2) 90k
NOAC(100, 0.8, 2) 100k
NOAC(100, 0.5, 0) 1k
NOAC(100, 0.5, 0) 10k
NOAC(100, 0.5, 0) 50k
NOAC(100, 0.5, 0) 100k

·105

3

NOAC(100, 0.8, 2)
NOAC(100, 0.8, 2) (parallel)
NOAC(100, 0.5, 0)
NOAC(100, 0.5, 0) (parallel)

s
m

,
e
m
T

i

2.5

2

1.5

1

0.5

0

1k

10k

20k

30k

40k

50k

60k

70k

80k

90k

# Triples

100k
·105

Fig. 3 Performance curves for parallelisation experiments

interesting outcome is that execution time does not depend on the algorithm param-
eters, which only change the number of extracted triclusters.

Triclustering in Big Data Setting

7 Conclusion

17

In this paper, we have presented a map-reduce version of the multimodal clustering
algorithm, which extends triclustering approaches and copes with bottlenecks of the
earlier M/R triclustering version [43]. We have shown that the proposed algorithm
is eﬃcient from both theoretical and practical points of view. This is a variant of
map-reduce based algorithm where the reducer exploits composite keys directly (see
also Appendix section [43]). However, despite the step towards Big Data technolo-
gies, a proper comparison of the proposed multimodal clustering and noise-tolerant
patterns in 𝑛-ary relations by DataPeeler and its descendants [2] is not yet conducted
(including MapReduce setting).

Two of the most challenging problems in OAC-triclustering are approximate
triclustering density estimation (e.g., employing the Monte Carlo approach) and
duplicate elimination (the same triclusters can be generated by diﬀerent generat-
ing triples). In MapReduce setting, both procedures are implemented as separate
MapReduce stages, while for online triclustering generation it is better to avoid du-
plicate generation, and the tricluster density should be computed both approximately
to maintain appropriate time-complexity and iteratively with minimal updates for
each incoming triple.

Further development of the proposed triclustering methods for large datasets is

possible with Apache Spark5 [41].

We also examined parallelisation possibilities (namely, multi-thread and milti-
core advantages in modern C#) of the numeric triclustering algorithm NOAC and
showed that it may signiﬁcantly improve performance.

Acknowledgements.

The study was implemented in the framework of the Basic Research Program at
the National Research University Higher School of Economics (Sections 2 and 5),
and funded by the Russian Academic Excellence Project ’5-100’. The second author
was also supported by the Russian Science Foundation (Section 1, 3, and 4) under
grant 17-11-01294. The authors would like to thank Dmitry Gnatyshak and Sergey
Zudin for their earlier work on incremental and MapReduce-based triclustering
implementations, respectively, anonymous reviewers as well as Yuri Kudriavtsev
from PM-Square and Dominik Slezak from Infobright and Warsaw University for
their encouragement given to our studies of M/R technologies.

5 https://spark.apache.org/

18

References

Egurnov et al.

1. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In
Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and
17th International Conference on Computational Linguistics - Volume 1, ACL ’98/COLING
’98, pages 86–90, Stroudsburg, PA, USA, 1998. Association for Computational Linguistics.
2. Loïc Cerf, Jérémy Besson, Kim-Ngan Nguyen, and Jean-François Boulicaut. Closed and
noise-tolerant patterns in n-ary relations. Data Min. Knowl. Discov., 26(3):574–619, 2013.
3. Dmitrii Egurnov, Dmitry Ignatov, and Engelbert Mephu Nguifo. Mining triclusters of similar
values in triadic real-valued contexts. In 14th International Conference on Formal Concept
Analysis-Supplementary Proceedings, pages 31–47, 2017.

4. Dmitrii Egurnov and Dmitry I. Ignatov. Triclustring toolbox. In Supplementary Proceedings
of ICFCA 2019 Conference and Workshops, Frankfurt, Germany, June 25-28, 2019, pages
65–69, 2019.

5. Kemal Eren, Mehmet Deveci, Onur Kucuktunc, and Catalyurek, Umit V. A comparative
analysis of biclustering algorithms for gene expression data. Brieﬁngs in Bioinform., 2012.
6. Bernhard Ganter, Peter A. Grigoriev, Sergei O. Kuznetsov, and Mikhail V. Samokhin. Concept-

based data mining with scaled labeled graphs. In ICCS, pages 94–108, 2004.

7. Bernhard Ganter and Rudolf Wille. Formal Concept Analysis: Mathematical Foundations.

Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1st edition, 1999.

8. Elisabeth Georgii, Koji Tsuda, and Bernhard Schölkopf. Multi-way set enumeration in weight

tensors. Machine Learning, 82(2):123–155, 2011.

9. Dmitry V. Gnatyshak, Dmitry I. Ignatov, and Sergei O. Kuznetsov. From triadic FCA to
triclustering: Experimental comparison of some triclustering algorithms. In CLA, pages 249–
260, 2013.

10. Dmitry V. Gnatyshak, Dmitry I. Ignatov, Sergei O. Kuznetsov, and Lhouari Nourine. A

one-pass triclustering approach: Is there any room for big data? In CLA 2014, 2014.

11. Dmitry V. Gnatyshak, Dmitry I. Ignatov, Alexander V. Semenov, and Jonas Poelmans. Gaining
insight in social networks with biclustering and triclustering. In BIR, volume 128 of Lecture
Notes in Business Information Processing, pages 162–171. Springer, 2012.

12. F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. TiiS,

5(4):19:1–19:19, 2016.

13. Rui Henriques and Sara C. Madeira. Triclustering algorithms for three-dimensional data
analysis: A comprehensive survey. ACM Comput. Surv., 51(5):95:1–95:43, September 2018.
14. Dmitry I. Ignatov, Dmitry V. Gnatyshak, Sergei O. Kuznetsov, and Boris Mirkin. Triadic
formal concept analysis and triclustering: searching for optimal patterns. Machine Learning,
pages 1–32, 2015.

15. Dmitry I. Ignatov, Sergei O. Kuznetsov, and Jonas Poelmans. Concept-based biclustering for
internet advertisement. In ICDM Workshops, pages 123–130. IEEE Computer Society, 2012.
16. Dmitry I. Ignatov, Sergei O. Kuznetsov, Jonas Poelmans, and Leonid E. Zhukov. Can triconcepts

become triclusters? International Journal of General Systems, 42(6):572–593, 2013.

17. Dmitry I. Ignatov, Elena Nenova, Natalia Konstantinova, and Andrey V. Konstantinov. Boolean
Matrix Factorisation for Collaborative Filtering: An FCA-Based Approach. In AIMSA 2014,
Varna, Bulgaria, Proceedings, volume LNCS 8722, pages 47–58, 2014.

18. Dmitry I. Ignatov, Dmitry Tochilkin, and Dmitry Egurnov. Multimodal clustering of boolean
tensors on mapreduce: Experiments revisited. In Supplementary Proceedings of ICFCA 2019
Conference and Workshops, Frankfurt, Germany, June 25-28, 2019, pages 137–151, 2019.
19. Mohamed Nader Jelassi, Sadok Ben Yahia, and Engelbert Mephu Nguifo. A personalized
recommender system based on users’ information in folksonomies. In Leslie Carr and et al.,
editors, WWW (Companion Volume), pages 1215–1224. ACM, 2013.

20. Mehdi Kaytoue, Sergei O. Kuznetsov, Juraj Macko, and Amedeo Napoli. Biclustering meets

triadic concept analysis. Ann. Math. Artif. Intell., 70(1-2):55–79, 2014.

21. Mehdi Kaytoue, Sergei O. Kuznetsov, Amedeo Napoli, and Sébastien Duplessis. Mining gene
expression data with pattern structures in formal concept analysis. Inf. Sci., 181(10):1989–
2001, 2011.

Triclustering in Big Data Setting

19

22. Petr Krajca and Vilem Vychodil. Distributed algorithm for computing formal concepts using
In N. Adams et al. (Eds.): IDA 2009, volume LNCS 5772, pages

map-reduce framework.
333–344, 2009.

23. Sergey Kuznecov and Yury Kudryavcev. Applying map-reduce paradigm for parallel closed
cube computation. In 1st Int. Conf. on Advances in Databases, Knowledge, and Data Appli-
cations, DBKDS 2009, pages 62–67, 2009.

24. Ao Li and David Tuck. An eﬀective tri-clustering algorithm combining expression data with

gene regulation information. Gene regul. and syst. biol., 3:49–64, 2009.

25. Sara C. Madeira and Arlindo L. Oliveira. Biclustering algorithms for biological data analysis:

A survey. IEEE/ACM Trans. Comput. Biology Bioinform., 1(1):24–45, 2004.

26. Saskia Metzler and Pauli Miettinen. Clustering boolean tensors. Data Min. Knowl. Discov.,

29(5):1343–1373, 2015.

27. Boris Mirkin. Mathematical Classiﬁcation and Clustering. Kluwer, Dordrecht, 1996.
28. Boris G. Mirkin and Andrey V. Kramarenko. Approximate bicluster and tricluster boxes in the
analysis of binary data. In Sergei O. Kuznetsov and et al., editors, RSFDGrC 2011, volume
6743 of Lecture Notes in Computer Science, pages 248–256. Springer, 2011.

29. Rokia Missaoui and Léonard Kwuida. Mining triadic association rules from ternary relations.
In Formal Concept Analysis - 9th International Conference, ICFCA 2011, Nicosia, Cyprus,
May 2-6, 2011. Proceedings, pages 204–218, 2011.

30. Alexandros Nanopoulos, Dimitrios Rafailidis, Panagiotis Symeonidis, and Yannis Manolopou-
los. Musicbox: Personalized music recommendation based on cubic analysis of social tags.
IEEE Transactions on Audio, Speech & Language Processing, 18(2):407–412, 2010.

31. Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone Paolo Ponzetto, and Chris
Biemann. Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl. In Nico-
letta Calzolari (Conference chair), Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara
Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asun-
cion Moreno, Jan Odĳk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of
the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),
Miyazaki, Japan, May 7-12, 2018 2018. European Language Resources Association (ELRA).
32. Z. Pawlak. Information systems – theoretical foundations. Information Systems, 6(3):205 –

218, 1981.

33. Anand Rajaraman, Jure Leskovec, and Jeﬀrey D. Ullman. Mining of Massive Datasets, chapter
MapReduce and the New Software Stack, pages 19–70. Cambridge University Press, England,
Cambridge, 2013.

34. Nicole Schweikardt. One-pass algorithm.

In Encyclopedia of Database Systems, Second

Edition. 2018.

35. Kĳung Shin, Bryan Hooi, and Christos Faloutsos. Fast, accurate, and ﬂexible algorithms for

dense subtensor mining. TKDD, 12(3):28:1–28:30, 2018.

36. Eirini Spyropoulou, Tĳl De Bie, and Mario Boley. Interesting pattern mining in multi-relational

data. Data Mining and Knowledge Discovery, 28(3):808–849, 2014.

37. Dmitry Ustalov, Alexander Panchenko, Andrey Kutuzov, Chris Biemann, and Simone Paolo
Ponzetto. Unsupervised semantic frame induction using triclustering.
In Iryna Gurevych
and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2:
Short Papers, pages 55–62. Association for Computational Linguistics, 2018.
38. George Voutsadakis. Polyadic concept analysis. Order, 19(3):295–304, 2002.
39. Rudolf Wille. Restructuring lattice theory: An approach based on hierarchies of concepts. In
Ivan Rival, editor, Ordered Sets, volume 83 of NATO Advanced Study Institutes Series, pages
445–470. Springer Netherlands, 1982.

40. Biao Xu, Ruairı de Frein, Eric Robson, and Mıcheal O Foghlu. Distributed formal concept
analysis algorithms based on an iterative mapreduce framework. In F. Domenach, D.I. Ignatov,
and J. Poelmans, editors, ICFCA 2012, volume LNAI 7278, pages 292–308, 2012.

41. Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur
Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi,

20

Egurnov et al.

Joseph Gonzalez, Scott Shenker, and Ion Stoica. Apache spark: A uniﬁed engine for big data
processing. Commun. ACM, 59(11):56–65, October 2016.

42. Lizhuang Zhao and Mohammed Javeed Zaki. Tricluster: An eﬀective algorithm for mining
coherent clusters in 3d microarray data. In SIGMOD 2005 Conference, pages 694–705, 2005.
43. Sergey Zudin, Dmitry V. Gnatyshak, and Dmitry I. Ignatov. Putting oac-triclustering on mapre-
duce. In Sadok Ben Yahia and Jan Konecny, editors, Proceedings of the Twelfth International
Conference on Concept Lattices and Their Applications, Clermont-Ferrand, France, October
13-16, 2015., volume 1466 of CEUR Workshop Proceedings, pages 47–58. CEUR-WS.org,
2015.

