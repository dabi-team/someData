0
2
0
2

v
o
N
0
2

]

G
L
.
s
c
[

2
v
6
8
2
3
0
.
1
0
0
2
:
v
i
X
r
a

1

Probabilistic K-means Clustering via
Nonlinear Programming

Yujian Li*, Bowen Liu, Zhaoying Liu, and Ting Zhang

Abstract—K-means is a classical clustering algorithm with wide applications. However, soft K-means, or fuzzy c-means at m = 1,
remains unsolved since 1981. To address this challenging open problem, we propose a novel clustering model, i.e. Probabilistic
K-Means (PKM), which is also a nonlinear programming model constrained on linear equalities and linear inequalities. In theory, we
can solve the model by active gradient projection, while inefﬁciently. Thus, we further propose maximum-step active gradient projection
and fast maximum-step active gradient projection to solve it more efﬁciently. By experiments, we evaluate the performance of PKM and
how well the proposed methods solve it in ﬁve aspects: initialization robustness, clustering performance, descending stability, iteration
number, and convergence speed.

Index Terms—K-means, soft K-means, probabilistic K-means, fuzzy c-means, nonlinear programming, active gradient projection,
maximum-step active gradient projection, fast maximum-step active gradient projection.

(cid:70)

1 INTRODUCTION

C LUSTERING analysis is an unsupervised machine learn-

ing method. It has been widely used in image and
video processing [1] - [4], speech processing [5], biology [6],
medicine [7], sociology [8], and so on. The task of clustering
is to ﬁnd some similarities from datasets [9], [10], and then
to classify samples with the similarities into clusters (i.e.
classes or categories). Clustering methods mainly include:
partition-based clustering [11] - [13], hierarchical-based clus-
tering [14] - [16], density-based clustering [10], [17], [18],
graph-based clustering [19] -
[21], grid-based clustering
[22] - [24], model-based clustering [25] - [27] and subspace
clustering [21], [24], [28] - [31], etc. Note that these methods
may have intersections.

Partition-based clustering algorithms, such as K-means
[11], K-medoids [12], and EM-K-means [32], divide a dataset
into several disjoint subsets with a similarity criterion. In
general, they ﬁrst choose initial cluster centers randomly or
manually, then adjust categories of samples and update the
cluster centers according to the nearest neighbor principle
until convergence. Hierarchical-based clustering algorithms
fall into two types: bottom-up and top-down. The bottom-
up clustering algorithms, such as CURE [14] and BIRCH
[15], start with each sample treated as a separate cluster,
and repeatedly merge two or more clusters until satisfy-
ing certain conditions, e.g. only one cluster left. The top-
down clustering algorithms, like MMDCA [16], start with
a single cluster of all samples, and repeatedly split one
cluster into two or more dissimilar clusters until meeting

• Y. Li

is with Faculty of Information Technology, Beijing University
of Technology, Beijing 100124, China. As corresponding author, he is
with School of Artiﬁcial Intelligence, Guilin University of Electronic
Technology, Guilin 541004, China.
E-mails: liyujian@guet.edu.cn

• B. Liu, Z. Liu and T. Zhang are with Faculty of Information Technology,

Beijing University of Technology, Beijing 100124, China.
E-mail:
zhangting@bjut.edu.cn

liubw2017@emails.bjut.edu.cn,

zhaoying.liu@bjut.edu.cn,

some criteria, e.g. only one sample in each cluster. Density-
based clustering algorithms, such as DBSCAN [17], ”afﬁnity
propagation” [10] and ”density peak” [18], cluster samples
according to their density distributions with local centers,
where the deﬁnition of density may change in different
algorithms.

Graph-based clustering algorithms include variational
clustering [19], minimum spanning tree [20] and spectral
clustering [29], etc. These algorithms construct a similarity
graph for a dataset and then ﬁnd an optimal partition
such that the graph has high weights for edges within
each group and low weights for edges between different
groups. Grid-based clustering algorithms, such as STING
[22], WaveCluster [23] and CLIQUE [24], classify datasets
with spatial grid cells, which may contain different numbers
of data points. Model-based clustering algorithms mainly
include self-organizing map (SOM) [25] and statistical clus-
tering [26]. SOM is an unsupervised neural network for
projecting all data points onto a two-dimensional plane with
similarities between them. Statistical clustering classiﬁes
datasets with their probability distributions in optimization
of expectation maximization [27].

Subspace clustering algorithms ﬁnd several subspaces
from a data set, where each subspace contains only one
cluster. These clustering algorithms may also contain grid-
based CLIQUE [24], model-based RAS [26], and graph-
based spectral clustering [28] - [31], etc.

K-means clustering algorithms minimize the sum of
squared (Euclidean) distances between each sample point
and its nearest cluster center, mainly including Lloyd’s algo-
rithm [33], Hartigan-Wong’s algorithm [34] and MacQueen’s
algorithm [11]. These heuristic algorithms take advantages
of easy implementation, high efﬁciency and low complexity.
However, they are sensitive to initial centers and outliers,
and may converge to partitions that are signiﬁcantly inferior
to the global optimum. Meanwhile, they are not suitable
for ﬁnding non-spherical clusters in a dataset. To improve
them, many new algorithms have been proposed, such as

 
 
 
 
 
 
GA-K-means, K-means++, K-medoids, Lp-norm K-means,
symmetric distance K-means, and kernel K-means. GA-K-
means [35] uses a genetic algorithm to select initial centers,
and can signiﬁcantly improve clustering performance on
low-dimensional datasets. K-means++ [36] sets initial cen-
ters to be far apart data points, and can improve the stability
of clustering results. K-medoids [12] replaces mean centers
with some special samples called medoids, and can reduce
the sensitivity to outliers. Lp-norm K-means [37] exploits
Minkowski distance to measure similarities between sam-
ples, and can be thought of as an extension of K-means.
Symmetric distance K-means [38] employs a non-metric
”point symmetric” distance for clustering, and can ﬁnd
different-shape clusters. Kernel K-means [39] selects a kernel
function to cluster in a feature space, and can ﬁnd non-
linear separable structures. Ref. [40] extends K-means type
algorithms by integrating both intracluster compactness and
intercluster separation.

Another well-known variant of K-means is fuzzy c-
means (FCM) [13] proposed by Bezdek in 1981. FCM deter-
mines partitions by computing the membership degree to
each cluster for each data point. The higher the membership
degree of a data point to a cluster, the greater possibility the
data point belongs to the cluster. Compared with K-means,
FCM is more ﬂexible in applications, but with one more
parameter m to be selected manually. In theory, m ranges
from 1 to inﬁnity. In practice, m ≈ 1.3 is a relatively good
value for clustering [41]. As m approaches 1, FCM degener-
ates to K-means, with all membership degrees converging
to 0 or 1. Additionally, FCM has many variants, such as
possibilistic c-means [42], sparse possibilistic c-means [43],
and size insensitive integrity-based FCM [44]. However, a
remaining problem is how to solve FCM at m = 1. The
problem will be solved by a novel clustering model, called
probabilistic K-means (PKM).

In fact, PKM is an equivalent model to FCM at m = 1.
Theoretically, PKM is also a nonlinear programming prob-
lem with many constraints of linear equalities and linear
inequalities. It can be solved by an active gradient projection
(AGP) method, i.e. a combination of active set [45] and
gradient projection [46], [47]. To speed up the AGP method,
we further present a maximum-step AGP (MSAGP) method
by iteratively computing maximum feasible step length and
a fast MSAGP (FMSAGP) method by efﬁciently computing
projection matrices. Also, we evaluate the performance of
PKM and how well the proposed methods solve it on
artiﬁcial and real datasets.

The remainder of this paper is organized as follows:
Section 2 introduces K-means, fuzzy c-means and proba-
bilistic K-means. Section 3 presents solutions to PKM in
detail. Experimental results are reported in Section 4. Finally,
conclusions are drawn in Section 5.

2 PKM CLUSTERING MODEL
Given a set of data points X = (cid:8)xi|xi ∈ RD, 1 ≤ i ≤ L(cid:9)
, let
us divide it into K clusters. Suppose ωj is the j-th cluster.
(cid:84) ωj = ∅. If Lj
denotes the number of elements in ωj with the center of

ωj, ∀1 ≤ i (cid:54)= j ≤ K, ωi

Then, X =

K
(cid:83)
j=1

cj, and L =

K
(cid:80)
i=1

Lj, then the standard K-means clustering

2

model is described as minimizing a hard objective function:

where

J =

K
(cid:88)

(cid:88)

i=1

xi∈ωj

(cid:107)xi − cj(cid:107)2

cj =

1
Lj

(cid:88)

xi

xi∈ωj

(1)

(2)

With membership degree wij assigned to the i-th data
point in the j-th cluster for 1 ≤ i ≤ L and 1 ≤ j ≤ K, the
hard K-means model is extended to a soft model, namely the
FCM model. FCM minimizes the objective function below:

J =

K
(cid:88)

L
(cid:88)

j=1

i=1

wm

ij

(cid:107)xi − cj(cid:107)2

s.t.

K
(cid:88)

j=1

wij = 1, wij ≥ 0

(3)

where m ranges from 1 to inﬁnity.

When m > 1, wij and cj can be computed alternately as

follows [13]






wij =

cj =

m−1

m−1

(cid:107)xi − ck(cid:107)− 2

(cid:107)xi − cj(cid:107)− 2
K
(cid:80)
k=1
L
(cid:80)
i=1
L
(cid:80)
i=1

ij xi

wm

wm

ij

(4)

However, FCM at m = 1 is an challenging open problem
that remains unsolved since 1981. It is also called ”soft K-
means”. Replacing membership degree wij by probability
pij, we get the soft K-means clustering model as follows,

J =

K
(cid:88)

L
(cid:88)

j=1

i=1

pij(cid:107)xi − cj(cid:107)2

s.t.

K
(cid:88)

j=1

pij = 1, pij ≥ 0

(5)

Note that probability pij takes a value in [0, 1]. This
means that data point xi may belong to class j with prob-
ability pij, rather than to only one class in hard K-means.
The higher the probability pij, the more likely that xi is in
class j. The view of probability is more in line with Bayesian
decision theory [48].

Additionally, we have to emphasize that the soft K-
means, namely (5), is not equivalent to the optimization
problem of hard K-means, namely, (1) and (2). On the one
hand, cj may have something to do with pij. On the other
hand, if setting pij = 1 with the minimal value of (cid:107)xi − cj(cid:107)
for 1 ≤ j ≤ K and pik = 0 for k (cid:54)= j, we cannot compute cj
as (2) in hard K-means because of no deﬁnition for cj in (5).
For the soft K-means, we can construct a Lagrangian

function,

J =

K
(cid:88)

L
(cid:88)

j=1

i=1

pij(cid:107)xi − cj(cid:107)2 +

L
(cid:88)

K
(cid:88)

λi(

i=1

j=1

pij − 1)

(6)

Letting ∂J
∂cj

=

K
(cid:80)
j=1

L
(cid:80)
i=1

2pij(cj − xi) = 0, we obtain

cj =

pijxi

L
(cid:80)
i=1
L
(cid:80)
i=1

pij

(7)

Substituting (7) into (5), we get an equivalent model,

namely, PKM

J =

K
(cid:88)

L
(cid:88)

j=1

i=1

pij

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xi −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(8)

s.t.

K
(cid:88)

j=1

pij = 1, pij ≥ 0

If setting K = 2 and L = 2 with x1 = (1, 1)T, x2 = (2, 2)T,

we can rewrite the objective function as follows,

pijxi

2
(cid:80)
i=1
2
(cid:80)
i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xi −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
p11x2 − p11x1
p11 + p21

pij

p12x2 − p12x1
p12 + p22

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

J(p11, p12, p21, p22) =

2
(cid:88)

2
(cid:88)

j=1

i=1

pij

(cid:34)

=

p11

(cid:13)
(cid:13)
(cid:13)
(cid:13)

p21x1 − p21x2
p11 + p21

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+ p21

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

p12

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+ p22

(cid:13)
p22x1 − p22x2
(cid:13)
(cid:13)
p12 + p22
(cid:13)
p11p12 + p21p22
(p11 + p21) (p12 + p22)
p11 (1 − p11) + p21 (1 − p21)
(p11 + p21) (2 − p11 − p21)

p11 + p21 − p2

11 − p2
21

(p11 + p21) (2 − p11 − p21)

+

=

=

=

(cid:107)x1 − x2(cid:107)2

(cid:107)x1 − x2(cid:107)2

(cid:107)x1 − x2(cid:107)2 = J(p11, p21),

(9)
where p12 = 1 − p11 (0 ≤ p11 ≤ 1) and p22 = 1 − p21
(0 ≤ p21 ≤ 1).

(0.5, 0.5). But it has only two probability pairs that take the
same minimum value of 0, namely (0, 1) or (1, 0). Because
of symmetricity, the two pairs produce the same cluster
assignation: x1 in one class, and x2 in the other.

3

3 SOLUTIONS TO PKM
According to (8), PKM is a nonlinear programming problem
constrained on linear equalities and linear inequalities. In
theory, the problem can be solved by AGP, but requiring
optimization of step length and fast computation of projec-
tion matrices.

In this section, we ﬁrst calculate the gradient of PKM’s
objective function, and then propose the AGP method and
its two improvements: the MSAGP method and the FM-
SAGP method, to solve the PKM model.

3.1 Gradient Calculation
For convenience, we deﬁne Fk as

Fk =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xk −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Using the chain rule on (8), we obtain

∂J
∂pij

= Fi +

L
(cid:88)

k=1

pkj

∂Fk
∂pij

(10)

(11)

According to (10), we can further derive (12)-(15) as follows,



∂Fk
∂pij

=

∂
∂pij






xk

Txk − xk

T















−

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij


T






xk +








L
(cid:80)
i=1
L
(cid:80)
i=1

L
(cid:80)
i=1
L
(cid:80)
i=1


pijxi

pij

T 


















(12)















L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

∂Fk
∂pij

= −2

xT
kxi

L
(cid:80)
i=1

pij − xT
k

pijxi

(cid:19)2

(cid:18) L
(cid:80)
i=1

pij

xT
i

(cid:18) L
(cid:80)
i=1

pijxi

(cid:19) L
(cid:80)
i=1

pij −

+2

pijxT
i

(cid:19) L
(cid:80)
i=1

pijxi

(13)

(cid:21)

pijxi

pij

L
(cid:80)
i=1

(cid:20)(cid:18) L
(cid:80)
i=1
(cid:19)3

pij

(cid:18) L
(cid:80)
i=1

xT
kxi −

xT
k

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

(cid:20)(cid:18) L
(cid:80)
i=1

pijxT
i

+

(cid:18) L
(cid:80)
i=1

(14)

pijxi

(cid:21)








(cid:19) L
(cid:80)
i=1
(cid:19)2

pij

(a)

(b)

Fig. 1. A surface of J(p11, p21) (a), and its vertical view (b).

Therefore, J(p11, p12, p21, p22) = J(p11, p21) is indeed
a function of p11 and p21, as shown in Fig. 1. Note that
Fig. 1a is a surface of J(p11, p21), with its vertical view in
Fig. 1b. In Fig. 1, p11 is the probability of x1 in cluster 1,
and p21 is the probability of x2 in cluster 1. Obviously, the
function J(p11, p21) has countless probability pairs that take
the same maximum value of 1, satisfying p11 = p21, e.g.








(cid:19)

∂Fk
∂pij

=

−2
L
(cid:80)
i=1

pij

xT
i

−

(cid:18) L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij








xk −

∂Fk
∂pij

= −

2

L
(cid:80)
i=1

pij

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij



T 











xi −

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

Substituting (15) into (11), we ﬁnally get,

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂J
∂pij

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xi −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

pijxi

pij

L
(cid:80)
i=1
L
(cid:80)
i=1


−

2

L
(cid:88)

L
(cid:80)
i=1

pij

k=1

pkj






xk −















xi −

∂J
∂pij

=

L
(cid:80)
i=1
L
(cid:80)
i=1







pijxi

pij

L
(cid:80)
i=1
L
(cid:80)
i=1


−

2

L
(cid:88)

L
(cid:80)
i=1

pij

k=1

pkj






xk −

and the gradient

pijxi

pij



T 











xi −

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij





T 
















xi −

L
(cid:80)
i=1
L
(cid:80)
i=1

pijxi

pij








(15)

(16)















(17)

(18)

(cid:104)

∇J =

∂J
∂pij

∂J
∂p11

...

...

∂J
∂pLj

∂J
∂p1K

...

...

(cid:105)

∂J
∂pLK

3.2 The AGP Method

In the constraints of PKM, there are L equalities and KL
inequalities,

∀1 ≤ i ≤ L,

K
(cid:88)

j=1

pij = 1

∀1 ≤ i ≤ L, 1 ≤ j ≤ K, pij ≥ 0

where probability matrix (pij)L×K can be vectorized as
P = (cid:2) p11

... p1K ...

... pLK

pLj

pij

...

(19)

(20)

(cid:3)T

(21)
Let ILK×LK be the identity matrix of size LK × LK. De-
ﬁne two matrices, inequality matrix A and equality matrix
E,

A = ILK×LK

(22)














1

0

(cid:124)

(cid:124)

... 1
(cid:123)(cid:122)
K
... 0
(cid:123)(cid:122)
K

(cid:125)

(cid:125)

0 0

...

(cid:124)

E =

0 0

...

0 0

0

1

... 1
(cid:123)(cid:122)
K
...
0 0

0

0

... 0

(cid:125)

1 ... 1
(cid:123)(cid:122)
K

(cid:125)

(cid:124)














(23)
Obviously, each row of A corresponds to one and only
one inequality in (20). Moreover, each row of E corresponds

L×LK

to one and only one equality in (19). Accordingly, the PKM’s
constraints, namely, (19)-(20), can be simply expressed as

4

AP ≥ 0, EP = 1

(24)
Let P(0) stand for an initial probability vector, and P(k)
for the probability vector at iteration k. Note that P(0) is
randomly initialized with the constraints of (24). At iteration
k, the rows of inequality matrix A are broken into two
groups: one is active, the other is inactive. The active group
is composed of all inequalities that must work exactly as an
equality at P(k), whereas the inactive group is composed of
the left inequalities. If A
and A
respectively denote the
active group and the inactive group, we have

(k)
1

(k)
2

Combining A

namely,

(25)

A

(k)
1 P(k) = 0
(k)
2 P(k) > 0

A

(26)
(k)
1 with E, we deﬁne an active matrix N(k),

N(k) =

(cid:35)

(cid:34)

A

(k)
1

E

(27)

which determines the active constraints at iteration k.

When N = N(k) is not a square matrix, we construct two

matrices G(k) and Q(k) as follows,

G(k) = NT(NNT)−1N
Q(k) = ILK×LK − G(k)
(29)
where G(k) is called projection matrix, and Q(k) is its or-
thogonal projection matrix [46].

(28)

At iteration k, projecting the gradient of PKM’s objective
function on the subspace of the active constraints, we obtain
the projected gradient,

(30)
If d(k) = 0, we stop at a local minimum. Otherwise, we

d(k) = −Q(k)∇J(P(k))

compute the probability vector at iteration k + 1,

P(k+1) = P(k) + td(k)

(31)

where t is a step length. Usually, t takes a small value, e.g.
t = 0.01 or t = 0.1.

When N = N(k) is a square matrix, N must be invertible.
Meanwhile, the projection matrix G(k) = NT(NNT)−1N =
ILK×LK , and Q(k) = 0. Thus, the projected gradient d(k) =
−Q(k)∇J(P(k)) = 0, which cannot be chosen as a descent
direction. Based on [47], we ﬁnd the descent direction as
follows.

1) Compute a new vector,

q(k) = (NNT)−1N∇J = (cid:0)NT(cid:1)−1

∇J

2) Break q(k) into two parts q
(cid:34)

(cid:35)

(k)
1

and q

q(k) =

(k)
q
1
(k)
q
2

(cid:20) (cid:16)

=

(k)
q
1

(cid:17)T (cid:16)

(k)
q
2

(k)
2 , namely,
(cid:17)T (cid:21)T

(32)

(33)

(k)
where the size of q
1
(k)
that of q
2

is the number of rows of E.

is the number of rows of A

(k)
1 , and

(k)
1

3) If all elements of q

are greater than or equal to 0,
(k)
1 ≥ 0, then stop. Otherwise, choose any one that is
(k)
1 , and use

i.e. q
less than 0, delete the corresponding row of A
(27)-(30) to compute d(k).

Using the above lemma, we can devise the FMSAGP

5

method as follows,

initialize P(0), go to 4);

1) Let k = 0, N = N(0) = E, G(0) = NT(NNT)−1N,

(k−1)
2) Find an active row vector n from A
2

, such that

nTP(k−1)> 0, nTP(k) = 0;
3) Compute G(k) as
G(k) = G(k - 1) + Q(k - 1)nT(cid:68)

Q(k - 1)nT, Q(k - 1)nT(cid:69)−1

nQ(k - 1)

Fig. 2. A comparison of step length t(k) for AGP and t(k)

max for MSAGP.

3.3 The MSAGP Method

In the AGP method, the step length is manually chosen as
a small number, leading to slow convergence. To address
this issue, we present the MSAGP method by iteratively
estimating the maximum step length in the feasible region.
(k)
2 P(k) > 0, we
At iteration k, because A
can estimate the maximum step length

(k)
1 P(k) = 0 and A

t(k) = t(k)

max

(34)

as follows.

1) Let p(k+1)

ij

= p(k)

ij + tijd(k)
p(k)
ij
d(k)
ij

ij = 0 for p(k)
for p(k)

ij > 0,
ij > 0 and d(k)

ij < 0,

max = min {tij}.

2) Compute tij = −
3) t(k)
In Fig. 2, we can see that step length t(k) for AGP is
shorter than t(k)
max for MSAGP at iteration k. Thus, MSAGP
is expected to converge faster than AGP because it always
ﬁnds a boundary point P(k + 1) to update probability vector
P(k). The faster convergence will be conﬁrmed in Section 4.

3.4 The FMSAGP Method

Although MSAGP converges faster than AGP, it has to cal-
culate a large number of matrix inversions. In fact, according
to (28), the calculation of G(k) = NT(NNT)−1N iteratively
requires (NNT)−1 with N = N(k). To calculate G(k) more
efﬁciently, it needs the following lemma.

(cid:2)nT, NT(cid:3)T

Lemma. Suppose that n is a row vector and

is
full row rank. If projection matrix GN = NT(NNT)−1N and
its orthogonal projection matrix QN = I − GN, then
G(nT,NT)T = GN + QNnT(cid:10)QNnT, QNnT(cid:11)−1
(cid:2)nT, NT(cid:3)T

is full row rank, by matrix

Proof. Because

nQN.

(35)

theory [49] - [51], there must exist a matrix G such that

(cid:40)

G(nT,NT)T = G + GN,
GGN = GNG = 0

,

Meanwhile, there must exist a matrix Z, such that

G = ZTnT(cid:10)ZTnT, ZTnT(cid:11)−1

nZ

Combining (36) with (37), we obtain
ZTnT(cid:10)ZTnT, ZTnT(cid:11)−1

nZGN = 0

(36)

(37)

(38)

Thus, nZGN = 0 must hold for all n. This leads to ZGN = 0.
That is to say, Z is the orthogonal projection matrix of GN,
Z = I−GN = QN. Moreover, both GN and QN are symmetric
matrices. Hence, (35) holds.

(39)
4) Compute Q(k) = ILK×LK − G(k), and d(k) =

5) Compute step length t(k) by (34) and probability

−Q(k)∇J(P(k));

vector P(k+1) by (31);

6) Let k = k + 1, if G(k) (cid:54)= ILK×LK , go to 2);
7) Construct N = N(k) by (27), and compute a new

vector q(k) = (cid:0)NT(cid:1)−1

∇J by (32);
8) Break q(k) into two parts q
9) If all elements of q

(k)
1

stop;

(k)
1

and q

(k)
2 by (33);

are greater than or equal to 0,

10) Choose any element less than 0 from q

(k)
1 , and delete

the corresponding row of A

(k)
1 ;

11) Reconstruct N = N(k) by (27);
12) Compute d(k) by (28)-(30), t(k) by (34), and P(k+1) by

(31);

13) Let k = k + 1, and go to 7).

4 EXPERIMENTS
In order to evaluate the performance of PKM and how well
the methods of AGP, MSAGP, and FMSAGP solve it, we
conduct a large number of experiments on ﬁfteen datasets:
one artiﬁcial dataset, one human face dataset, and thirteen
UCI datasets. These datasets are detailed in Table 1.

TABLE 1
Details of Artiﬁcial and Benchmark Datasets.

Instance

Class Dimension

Artiﬁcial dataset

Yale-faces-B

UCI

Iris
Parkinson
Seeds
Segmentation
Glass
Ionosphere
Dermatology
Breast-cancer
Natural
Yeast
Waveform
Satellite
Epileptic

310

5850

150
195
210
210
214
351
358
683
2000
2426
5000
6435
11500

4

10

2

1200

3
2
3
7
6
2
6
2
9
3
3
6
5

4
22
7
19
9
33
34
9
294
24
21
36
178

The artiﬁcial dataset is generated by ourselves. As shown
in Fig. 3, it is composed of 310 data points, with 150, 150, 5
and 5 respectively from 4 classes. The human face dataset is
Yale-faces-B1, which consists of 5850 images scaled down to

1. https://cervisia.org/machine learning data.php

TABLE 2
Initialization robustness of PKM, KM++ and FCM on the artiﬁcial dataset.

PKM KM++

Correct times

954

645

1.05

—

FCM (m = 1.05, 1.09, 1.1, 1.3, 1.4, 1.5, 1.9, 2.0, 2.1)
1.09

1.9

1.4

1.5

1.3

1.1

2.0

687

693

780

691

0

0

0

0

0

0

Correct percentage

95.4%

64.5%

— 68.7% 69.3% 78.0% 69.1%

6

2.1

0

0

4.1 Initialization Robustness

In this subsection, we evaluate initialization robustness of
PKM, KM++ and FCM on the artiﬁcial dataset, with the re-
sult given in Table 2. The initialization robustness is deﬁned
as the number of correct times out of running an algorithm
1000 times initialized randomly. The correctness means the
cluster assignation of a dataset is completely consistent with
its original distribution. With the artiﬁcial dataset in Fig. 3,
three inconsistent distributions are displayed as examples in
Fig. 4. From Table 2, we can see that PKM have 954 correct
times, more than 645 by KM++. However, FCM may have
number of correct times that varies with parameter m. For
example, it has 687, 693, 780 and 691 correct times at m =
1.09, 1.1, 1.3 and 1.4, respectively. However, it has no correct
times at m = 1.5, 1.09, 2.0 and 2.1. At m = 1.3, FCM achieves
the maximum number of correct times, namely 780, still
lower than PKM.

Overall, PKM has a correct percentage of 95.4%, better
than KM++ (64.5%) and FCM (0 to 78.0%) in terms of
initialization robustness.

4.2 Clustering Performance

In this subsection, we evaluate clustering performance of
PKM, KM++ and FCM in terms of ﬁve measures: Stan-
dard Squared Error (SSE), Davies Bouldin Index (DBI) [52],
Normalized Mutual Information (NMI) [53], Adjusted Rand
Index (ARI) [53], and V-measure (VM) [54]. The SSE is
deﬁned as

K
(cid:88)

SSE =

(cid:88)

(cid:107)xi − cj(cid:107)2

(40)

i=1

xi∈ωj

where K is the number of clusters, and cj is the center
(mean) of the j-th cluster. DBI is the ratio of within-class
distances to between-class distances. NMI is a normalization
of the mutual information score to scale the results between
0 (no mutual information) and 1 (perfect correlation). ARI
is an adjusted similarity measure between two clusters. V-
measure is a harmonic mean between homogeneity and
completeness, where homogeneity means that each cluster
is a subset of a single class, and completeness means that
each class is a subset of a single cluster.

The above ﬁve measures fall into two types: internal
evaluation and external evaluation. Internal evaluation, in-
cluding SSE and DBI, can work without ground truth labels.
But external evaluation, such as NMI, ARI and VM, must
work with ground truth labels. The lower the internal eval-
uation, the better the clustering result. However, the higher
the external evaluation, the better the clustering result.

We test PKM (solved by FMSAGP), KM++ and FCM
(m = 1.3) on Yale-faces-B and 13 UCI datasets, with the 5-
time average results reported in Table 3. The best in each

Fig. 3. Original distribution of the artiﬁcial dataset with 4 classes.

(a)

(b)

(c)

Fig. 4. Three inconsistent distributions with the artiﬁcial dataset.

30×40 pixels, with each saved as a 1200-dimensional vector.
The thirteen UCI datasets are selected from UCI Machine
Learning Repository2, including Iris, Parkinson, Seeds, Seg-
mentation, Glass, Ionosphere, Dermatology, Breast-cancer,
Natural, Yeast, Waveform, Satellite, and Epileptic.

All experiments are carried out on the same computer
(i7-4790 CPU, 3.60GHz, 8.00G RAM), running Windows
7 and Matlab 2015a. The results of FCM are obtained
by Matlab’s build-in function ”fcm”. The results of K-
means++ (KM++) are obtained by Matlab’s build-in func-
tion ”kmeans”. The results of PKM are obtained by Matlab
implementation of AGP, MSAGP and FMSAGP, using two
build-in functions ”sparse” and ”full” for matrix optimiza-
tion.

The experiments are organized and analyzed in ﬁve
aspects: initialization robustness, clustering performance,
descent stability, iteration number, and convergence speed.
The ﬁrst two respects are used to evaluate the performance
of PKM, and the last three respects are used to evaluate
how well the methods of AGP, MSAGP, and FMSAGP solve
PKM.

2. http://archive.ics.uci.edu/ml

7

TABLE 3
Internal Evaluation of PKM, KM++ and FCM ( m = 1.3).

Datasets

Yale-faces-B

Iris

Seeds

Breast-cancer

Dermatology

Ionosphere

Parkinson

Glass

Natural

Satellite

Yeast

Segmentation

Waveform

Epileptic

SSE
PKM KM++

8.51e9
78.942

587.05

2419.3

11702

2419.4

1.34e6

372.77

1888.3
1.67e7

713.89

1.01e6
1.33e5

5.1e10

8.52e9

110.89

588.05
2419.3

11702

2419.4

1.34e6

379.41

1883.5

1.69e7
713.88

9.85e5

1.33e5

5.1e10

FCM
8.42e9

110.9

587.32
2419.3

11702

2419.4

1.34e6

366.35

1879.2

1.69e7

714.07

1.99e6
1.33e5

5.3e10

DBI

PKM KM++

1.6184
0.3928

0.3980
0.3785

0.7180

0.7567

0.4932

0.8505

1.7035
0.8524

1.1601
0.7461

0.8663

2.6051

1.6128

0.4801

0.4421

0.3786

0.7939
0.7567

0.4974
0.7074

1.6587

0.8731
1.1582

0.7530

0.9301

2.6956

FCM
1.5986

0.4561
0.3962

0.3786

0.7609

0.7579

0.4974

0.7909
1.6089

0.8957

1.1753

1.0899

0.8665

3.6407

case is highlighted in bold. Some big values are in scientiﬁc
notation. For instance, 8.42e9 means 8.42 × 109.

Obviously, we can see that MSAGP descends stably without
oscillation.

From Table 3, we have the following observations.
1) By SSE, PKM performs better than KM++ on 5
datasets, and as well as KM++ on 6 datasets. Meanwhile,
it performs better than FCM on 6 datasets, and as well as
FCM on 5 datasets.

2) By DBI, PKM performs better than KM++ on 9
datasets, and as well as KM++ on 1 datasets. Meanwhile,
it performs better than FCM on 10 datasets.

From Table 4, we have the following observations.
1) By NMI, PKM performs better than KM++ on 5
datasets, and as well as KM++ on 5 datasets. Meanwhile,
it outperforms FCM on 12 datasets.

2) By ARI, PKM perform better than KM++ on 5 datasets,
and as well as KM++ on 4 datasets. Meanwhile, it outper-
forms FCM on 11 datasets.

3) By VM, PKM perform better than KM++ on 9 datasets,
and as well as KM++ on 2 datasets. Meanwhile, it outper-
forms FCM on 11 datasets.

Overall, PKM outperforms KM++ and FCM in terms of

SSE, DBI, NMI, ARI and VM.

4.3 Descent Stability of MSAGP

In this subsection, we evaluate the descent stability of
MSAGP. Theoretically, AGP needs a small step length to
make the value of its objective function stably descend at
each iteration and gradually converge to a local minimum.
If the step length is too large, the value may descend with
oscillation, and even does not converge at all. MSAGP
iteratively estimates a maximum step length for AGP in the
feasible region of PKM so as to speed up its convergence
with fewer iterations. However, will this estimate have any
serious inﬂuence of oscillation on the convergence of AGP?
To demonstrate the inﬂuence, we select 9 UCI datasets
from Table 1, including Satellite, Yale, Epileptic, Natural,
Yeast, Waveform, Glass, Dermatology, and Breast-cancer. On
these datasets, we use MSAGP to solve PKM, and illustrate
the value of its objective function vs. iteration in Fig. 5.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

Fig. 5. Descent stability of MSAGP on 9 UCI datasets.(a) Glass. (b)
Dermatology. (c) Breast-cancer. (d) Natural. (e) Waveform. f) Yeast. (g)
Satellite. (h) Yale-faces. (i) Epileptic.

4.4 Iteration Number of MSAGP and AGP

In this subsection, we compare iteration number for MSAGP
and AGP (t = 0.01 and 0.1) to converge on 8 UCI datasets,
including Segmentation, Parkinson, Dermatology, Breast-
cancer, Seeds, Ionosphere, Glass and Iris. The results are
illustrated in Figs. 6-7 and reported in Table 5, where we
have the following observations.

8

TABLE 4
External Evaluation of PKM, KM++ and FCM (m = 1.3).

Datasets

Yale-faces-B

Iris

Seeds

Breast-cancer

Dermatology

Ionosphere

Parkinson

Glass

Natural

Satellite

Yeast

Segmentation

Waveform

Epileptic

NMI
PKM KM++
0.7959

0.7952

0.7501

0.7025

0.7546

0.1141

0.1349

0.1124

0.3294

0.0523
0.6117

0.0050

0.5338

0.3622

0.1228

0.6733
0.7025

0.7478

0.1032
0.1349

0.1217

0.4178

0.0536

0.6117

0.0050

0.5132
0.3622

0.1675

FCM

0.7722

0.6723

0.6949

0.7478

0.1046

0.1299
0.1217

0.3489

0.0521

0.5471

0.0043

0.4678

0.3606

0.0033

ARI

PKM KM++
0.6631

0.6123

0.7233

0.7135
0.8521

0.0391

0.1777

0.1876

0.2201

0.0247

0.5292
0.0117

0.3823

0.2536

0.0245

0.5779

0.7135

0.8464

0.0266
0.1777

0.2046

0.2551

0.0261
0.5293

0.0117

0.3313
0.2536

0.0263

FCM

0.6386

0.5763
0.7166

0.8464

0.0261

0.1727
0.2046

0.2126
0.0273

0.4289

0.0109

0.3172

0.2529

0.0025

VM

PKM KM++
0.8264

0.1744

0.7419

0.7050

0.7545

0.1140

0.1348

0.1153
0.3871

0.0518

0.5790
0.0048

0.4996
0.3622

0.1226

0.7149

0.6999

0.7478

0.1056
0.1348

0.0607

0.3857
0.0529

0.5544

0.0045

0.5252
0.3622

0.1291

FCM

0.0059

0.7081

0.6949

0.7478

0.1095

0.1298
0.1262

0.3807

0.0495
0.6128

0.0045
0.5729

0.3559

0.0058

TABLE 5
Iteration number for MSAGP and AGP to converge with objective function value on 8 UCI datasets.

Iteration number
MSAGP AGP (t = 0.01) AGP (t = 0.1)

Segmentation

Parkinson

Dermatology

Breast-cancer

Seeds

Glass

Ionosphere

Iris

1290

195

1801

452

421

1079

368

308

1338

196

6329

577

634

190683

2070

53022

1291

196

2247

462

534

115145

524

5573

Objective function value

MSAGP

AGP (t = 0.01) AGP (t = 0.1)

986167.4343
1343350.02

11715.1043
19323.2

986167.4343

1343350.02

11715.1041

19323.2

986167.4343
1343350.02

11715.1043
19323.2

587.32

378.9
2419.3

78.95

587.32

377.19

2419.3

78.95

587.32

381.74
2419.3

78.95

1) MSAGP requires fewer iterations to converge than
AGP on the eight datasets, especially much fewer on Der-
matology (Fig. 6c), Ionosphere (Fig. 6f), Glass (Figs. 7a-c)
and Iris (Figs. 7d-f).

2) When converging, MSAGP and AGP have the same
objective function value on Parkinson, Breast-cancer, Seeds,
Ionosphere and Iris. However, MSAGP has an objective
function value slightly greater than AGP (t = 0.01) on Seg-
mentation, Dermatology and Glass, while slightly smaller
than AGP (t = 0.1) on Glass.

Overall, MSAGP can reach a competitive convergence

with AGP in fewer iterations.

4.5 Convergence Speed of FMSAGP and MSAGP

In this subsection, we compare convergence speed of FM-
SAGP and MSAGP in running time on 9 UCI datasets:
Parkinson, Iris, Seeds, Glass, Segmentation, Breast-cancer,
Dermatology, Yeast and Waveform. The results are reported
in Table 6, where we have the following observations.

1) FMSAGP runs faster than MSAGP on all the nine

datasets.

2) The larger the dataset, the lower the speed-up (η) of
FMSAGP to MSAGP. For example, the speed-up is 47.3%
on Iris (150 samples), 22.3% on Glass (214 samples), 9.7% on
Yeast (2426 samples) and 1.5% on Waveform (5000 samples).

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 6. Iteration number (in x-axis) for MSAGP and AGP (t = 0.01 and
0.1) to converge on 6 UCI datasets: Segmentation (a), Parkinson (b),
Dermatology (c), Breast-cancer (d), Seeds (e) and Ionosphere (f). Y-axis
means objective function value of PKM.

5 CONCLUSION

In this paper, the most important contribution is our so-
lution to the clustering problem of FCM at m = 1 by a

9

[2] Otto C, Wang D, Jain A. Clustering Millions of Faces by Identity[J].
IEEE Transactions on Pattern Analysis and Machine Intelligence,
2018, 40(2):289-303.

[3] Kim S, Yoo C D, Nowozin S, et al. Image Segmentation Using
Higher-Order Correlation Clustering[J]. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 2014, 36(9):1761-1774.
[4] Marin D, Tang M, Ayed I B, et al. Kernel Clustering: Density
Biases and Solutions[J]. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2019, 41(1):136-147.

[5] Heymann J, Drude L, Haeb-Umbach R. A Generic Neural Acous-
tic Beamforming Architecture for Robust Multi-Channel Speech
Processing[J]. Computer Speech and Language, 2017, 46: 374-385.
[6] Ronan T, Qi Z, Naegle K M. Avoiding Common Pitfalls When

Clustering Biological Data[J]. Science Signaling, 2016, 9(432):re6.

[7] Taylor M J, Husain K, Gartner Z J, et al. A DNA-Based T Cell
Receptor Reveals a Role for Receptor Clustering in Ligand Dis-
crimination[J]. Cell, 2017, 169(1):108-119.

[8] Liu A A, Su Y T, Nie W Z, et al. Hierarchical Clustering Multi-Task
Learning for Joint Human Action Grouping and Recognition[J].
IEEE Transactions on Pattern Analysis and Machine Intelligence,
2016, 39(1):102-114.

[9] LI Y J. A Clustering Algorithm Based on Maximal θ-Distant

Subtrees. Pattern Recognition. 2007, 40(5):1425-1431.

[10] Frey B J, Dueck D. Clustering by Passing Messages Between Data

Points [J]. Science, 2007, 315(5814):972-976.

[11] Macqueen J. Some Methods for Classiﬁcation and Analysis of
Multivariate Observations[C]. Proc. of 5th Berkeley Symposium
on Mathematical Statistics and Probability, 1967:281-297.

[12] Kaufmann L, Rousseeuw P J. Clustering by Means of Medoids[C].
Statistical Data Analysis Based on the L1-norm and Related Meth-
ods. North-Holland, 1987:405-416.

[13] Bezdek J C. Pattern Recognition with Fuzzy Objective Function

Algorithms[M]. Plenum Press, 1981.

[14] Guha S, Rastogi R, Shim K, et al. CURE: An Efﬁcient Cluster-
ing Algorithm for Large Databases[J]. Information Systems, 1998,
26(1):35-58.

[15] Madan S, Dana K J. Modiﬁed Balanced Iterative Reducing and
Clustering Using Hierarchies (m-BIRCH) for Visual Clustering[J].
Pattern Analysis and Applications, 2016, 19(4):1023-1040.

[16] Johnson T, Singh S K. Divisive Hierarchical Bisecting MinCMax
Clustering Algorithm[M]. Proceedings of the International Confer-
ence on Data Engineering and Communication Technology. 2017.
[17] Ester M, Kriegel H P, Xu X. A Density-Based Algorithm for
Discovering Clusters in Large Spatial Databases with Noise[C].
International Conference on Knowledge Discovery and Data Min-
ing, 1996:226-231.

[18] Rodriguez A, Laio A. Clustering by Fast Search and Find of

Density Peaks[J]. Science, 2014, 344(6191):1492-1496.

[19] Szlam A, Bresson X. Total Variation and Cheeger Cuts[C]. Interna-

tional Conference on Machine Learning. 2010: 1039-1046.

[20] Elankavi R, Kalaiprasath R, Udayakumar R. A Fast Clustering
Algorithm for High-Dimensional Data[J]. International Journal of
Civil Engineering and Technology, 2017, 8(5):1220-1227.

[21] Luxburg U V. A Tutorial on Spectral Clustering[J]. Statistics and

Computing, 2007, 17(4):395-416.

[22] Wu B, Wilamowski B M. A Fast Density and Grid Based Clus-
tering Method for Data with Arbitrary Shapes and Noise[J]. IEEE
Transactions on Industrial Informatics, 2016, 13(4):1620-1628.
[23] Sheikholeslami G, Chatterjee S, Zhang A. WaveCluster: A Wavelet-
Based Clustering Approach for Spatial Data in Very Large
Databases[J]. Vldb Journal, 2000, 8(3-4):289-304.

[24] Schikuta E. Grid-Clustering: An Efﬁcient Hierarchical Clustering
Method for Very Large Data Sets[C]. IEEE International Confer-
ence on Pattern Recognition, 1996, 2:101-105.

[25] Lin X, Soergel D, Marchionini G. A Self-Organizing Semantic Map
for Information Retrieval[C]. ACM SIGIR conference on Research
and development in information retrieval, 1991: 262-269.

[26] Rao S R, Yang A Y, Sastry S S, et al. Robust Algebraic Segmentation
of Mixed Rigid-Body and Planar Motions from Two Views[J].
International Journal of Computer Vision, 2010, 88(3):425-446.
[27] Kadir S N, Goodman D F, Harris K D. High-Dimensional Cluster
Analysis with The Masked Em Algorithm[J]. Neural Computation,
2014, 26(11):2379-2394.

[28] Elhamifar E, Vidal R. Sparse Subspace Clustering[C]. IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2009:2790-2797.

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 7. Iteration number (in x-axis) for MSAGP (a) and AGP (t = 0.01)
(b) AGP (t = 0.1) (c) to converge on Glass, and for MSAGP (d) and AGP
(t = 0.01) (e) AGP (t = 0.1) (f) on Iris. Y-axis means objective function
value of PKM.

TABLE 6
Running Time (s) of FMSAGP and MSAGP with speed-up on 9 UCI
datasets.

FMSAGP MSAGP

Parkinson

Iris

Seeds

Glass

Segmentation

Breast-cancer

Dermatology

Yeast

Waveform

0.1395

0.2691

0.4868

2.4432

2.7152

0.8911

6.1486

137.98

173.49

0.2895

0.5692

1.3117

10.9369

14.6201

5.4898

40.7612

1418.47

11957.21

η

48.2%

47.3%

37.1%

22.3%

18.6%

16.2%

15.1%

9.7%

1.5%

new model, i.e. PKM. Moreover, we have addressed the
PKM model by three methods: AGP, MSAGP and FM-
SAGP. Additionally, we have conducted a large number
of experiments to evaluate how well these methods work
in initialization robustness, clustering performance, descent
stability, iteration number, and convergence speed. As fu-
ture work, we will further improve our solution to PKM,
and apply the basic idea to other relevant problems, like Lp-
norm K-means, kernel PKM, and even possibilistic c-means.
Particularly, we will combine deep neural networks [55],
[56] with PKM to build deep PKM models, and to develop
more nonlinear programming models for machine learning.

ACKNOWLEDGMENTS

This work was supported by the National Natural Science
Foundation of China under grant numbers: 61876010 and
61806013.

REFERENCES

[1] Purkait P, Chin T J, Sadri A, et al. Clustering with Hypergraphs:
The Case for Large Hyperedges[J]. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2017, 39(9): 1697-1711.

[55] Lecun Y, Bengio Y, Hinton G. Deep learning[J]. Nature, 2015,

521(7553):436.

[56] Li Y, Zhang T. Deep neural mapping support vector machines[J].

Neural Networks, 2017, 93:185-194.

10

[29] Lu C, Feng J, Lin Z, et al. Subspace Clustering by Block Diago-
nal Representation[J]. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2018, 41(2):487-501.

[30] Chen J, Li Z, Huang B. Linear Spectral Clustering Superpixel[J].
IEEE Transactions on Image Processing, 2017, 26(7):3317-3330.
[31] Nie F, Wang X, Huang H. Clustering and Projected Clustering
with Adaptive Neighbors[C]. ACM International Conference on
Knowledge Discovery and Data Mining, 2014:977-986.

[32] Kearns M, Mansour Y, Ng A. An Information-Theoretic Analysis
of Hard and Soft Assignment Methods for Clustering[C]. Uncer-
tainty in Artiﬁcial Intelligence, 1997.

[33] Lloyd S. Least Squares Quantization In PCM[J]. IEEE transactions

on information theory, 1982, 28(2):129-137.

[34] Hartigan J A, Wong M A. Algorithm AS 136: A K-Means Clus-
tering Algorithm[J]. Journal of the Royal Statistical Society, 1979,
28(1):100-108.

[35] Laszlo M, Mukherjee S. A Genetic Algorithm Using Hyper-
Quadtrees for Low-Dimensional K-Means Clustering[J]. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2006,
28(4):533-543.

[36] Arthur D, Vassilvitskii S. K-means++: The Advantages of Careful
Seeding[C]. Society for Industrial and Applied Mathematics, 2007,
11(6):1027-1035.

[37] Selim S and Ismail M. K-Means-Type Algorithms: A Generalized
Convergence Theorem and Characterization of Local Optimal-
ity[J]. IEEE Transactions on Pattern Analysis and Machine Intel-
ligence, 1984, 6(1):81-87.

[38] Su M C, Chou C H. A Modiﬁed Version of the K-Means Algorithm
with a Distance Based on Cluster Symmetry[J]. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2001, 23(6):674-680.
[39] Yu S, Tranchevent L C, Liu X, et al. Optimized Data Fusion
for Kernel K-Means Clustering[J]. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2012, 34(5):1031-1039.

[40] Huang X and Ye Y and Zhang H. Extensions of Kmeans-Type Al-
gorithms: A New Clustering Framework by Integrating Intraclus-
ter Compactness and Intercluster Separation[J].IEEE Transactions
on Neural Networks and Learning Systems. 2014, 25(8):1433-1446.
[41] Pal N R, Bezdek J C. On Cluster Validity for The Fuzzy C-Means
Model[J]. IEEE Transactions on Fuzzy Systems, 1995, 3(3):370-379.
[42] Krishnapuram R, Keller J M. The Possibilistic C-Means Algorithm:
Insights and Recommendations[J]. IEEE Transactions on Fuzzy
Systems, 1996, 4(3):385-393.

[43] Koutroumbas K D, Xenaki S D, Rontogiannis A A. On the Con-
vergence of the Sparse Possibilistic C-Means Algorithm[J]. IEEE
Transactions on Fuzzy Systems, 2018, 26(1):324-337.

[44] Lin P L, Huang P W, Kuo C H, et al. A Size-Insensitive Integrity-
Based Fuzzy C-Means Method for Data Clustering[J]. Pattern
Recognition, 2014, 47(5):2042-2056.

[45] Nocedal J, Wright S. Numerical Optimization[M]. Springer Science

and Business Media, 2006.

[46] Rosen J B. The Gradient Projection Method for Nonlinear Pro-
gramming. Part I. Linear Constraints[J]. Journal of the Society for
Industrial and Applied Mathematics, 1961, 9(4):514-532.

[47] Goldfarb D, Lapidus L. Conjugate Gradient Method for Nonlinear
Programming Problems with Linear Constraints[J]. Industrial and
Engineering Chemistry Fundamentals, 1968, 7(1):142-151.

[48] Duda R O, Hart P E, Stork D G. Pattern Classiﬁcation: 2nd

Edition[M]. Wiley and Sons, 2001.

[49] Zhang X D. Matrix Analysis and Applications[M]. Tsinghua Uni-

versity Press, 2004:681-682.

[50] Honig M, Madhow U, Verdu S. Blind Adaptive Multiuser Detec-
tion[J]. IEEE Transactions on Information Theory, 1995, 41(4):944-
960.

[51] Shores T S. Applied Linear Algebra and Matrix Analysis: 2nd

ed[M]. Undergraduate Texts in Mathematics, 2007.

[52] Davies D L, Bouldin D W. A Cluster Separation Measure[J]. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 1979,
1(2):224-227.

[53] Vinh N X, Epps J, Bailey J. Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance[J]. Journal of Machine Learning Research,
2010, 11(1):2837-2854.

[54] Rosenberg A, Hirschberg J. V-measure: A Conditional Entropy-
Based External Cluster Evaluation Measure[C]. Empirical Meth-
ods in Natural Language Processing and Computational Natural
Language Learning, 2007, 410-420.

