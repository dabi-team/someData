IAN: Iterated Adaptive Neighborhoods for manifold
learning and dimensionality estimation

Luciano Dyballa, Steven W. Zucker

Abstract

Invoking the manifold assumption in machine learning requires knowledge of the mani-
fold’s geometry and dimension, and theory dictates how many samples are required. However,
in applications data are limited, sampling may not be uniform, and manifold properties are
unknown and (possibly) non-pure; this implies that neighborhoods must adapt to the local
structure. We introduce an algorithm for inferring adaptive neighborhoods for data given by a
similarity kernel. Starting with a locally-conservative neighborhood (Gabriel) graph, we spar-
sify it iteratively according to a weighted counterpart. In each step, a linear program yields
minimal neighborhoods globally and a volumetric statistic reveals neighbor outliers likely to
violate manifold geometry. We apply our adaptive neighborhoods to non-linear dimensional-
ity reduction, geodesic computation and dimension estimation. A comparison against standard
algorithms using, e.g., k-nearest neighbors, demonstrates their usefulness.

Research supported by NIH Grant EY031059, NSF CRCNS Grant 1822598.

2
2
0
2

g
u
A
9
2

]

G
L
.
s
c
[

2
v
3
2
1
9
0
.
8
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1 Introduction

A starting point for many algorithms in data science, from clustering to manifold inference, is
knowing the neighbor relationships among the data points. Clustering, for example, often begins
with the “k-nearest neighbor graph”, while manifold inference involves a kernel, i.e., a measure
of similarity between data points. In the ﬁrst case the neighborhoods are local and discrete; in
the second they are global and continuous, with concentration of inﬂuence controlled by kernel
bandwidth or scale. Knowing neighbor relationships is fundamental to deﬁning a topology. Di-
mensionality estimation involves the rate of change in the density of points within a ball, that is,
within a neighborhood, with respect to the radius of such ball. It is helpful when the number of
data points is large, a requirement that grows with dimensionality. Asymptotic analysis is often
favored by theoreticians.

In practice, however, we are rarely given enough data points to satisfy asymptotic bounds, nor
are we given the global number k, the precise number of neighbors each point should have. We
often make the manifold assumption—that the data points are drawn randomly from on (or near)
a manifold, but rarely proceed to questions about the manifold: what is its dimension, what is
the probability distribution of points over it, and what is the sampling density. All of these could
inﬂuence k. In practice we rarely try to assess the basic properties of the manifold assumed by
theorists, e.g., its curvature, medial axis or reach (deﬁned in the next section). Instead, we rely on
different visualization algorithms, such as spectral methods, Isomap, t-SNE, or UMAP (references
in the next section), to ﬁnd a pleasing organization of the data. This is dangerous, of course,
because these algorithms have free parameters. Central to this paper, most require specifying
the number of neighbors, k (or its equivalent): changing k or the parameters changes the result.
Unless one knows the answer, one is caught in a chicken-and-egg conundrum: imposing a prior
belief amounts to “ﬁxing” the solution (examples of changing k are shown later in the paper).

This gap between theory and practice shows up right from the start. If the manifold is not pure,
i.e., if it consists of a union of manifolds of possibly different dimensionality, then there may be
no global k that sufﬁces; furthermore, the manifold may have a boundary. Even if it is pure and
without boundary, the temptation to choose k large is common. But this can ﬁll in the open space
around curved manifolds incorrectly (“folding”), linking distant points that are not neighbors. On
the other hand, choosing k small can induce holes and break connectivity; see Fig. 1. As we
shall demonstrate, sampling issues and manifold geometry interact in causing such phenomena.
Moreover, for real datasets the appropriate number of neighbors may differ from data point to data
point. This ﬁnal point is a principal motivation for this paper.

We present an algorithm to estimate the functional neighborhood—the immediate neighbors,
or scale of a similarity kernel—around each point. We seek to identify those nearest neighbors that
are “correct” in the sense that they support dimensionality and volume estimates, and manifold in-
ference, without covering holes or ﬁlling in concavities. It is inspired by the philosophical position
that views discrete and continuous mathematics as ‘two sides of the same,’ as argued by Lov´asz
([75]), and iterates between them.

Our algorithm builds from a conservative initial estimate of neighbors (based a discrete con-
struct, the Gabriel graph) to a more reﬁned one, based on continuous estimates from a Gaussian
kernel. The discrete and continuous volume estimates must be consistent, however, and this pro-
vides the glue for our iteration. Since not all of the initial “closest” neighbors are actually closest

2

neighbors, those putative neighbors that violate the volume relationship are pruned, and the pro-
cess repeats until the two perspectives agree. Our algorithm, then, can be considered an iterative
graph sparsiﬁcation. Technically, it involves two different graphs: a discrete one, that links only
putative nearest neighbors (pairs of points deﬁning the diameter of an otherwise empty ball), and
a weighted one, structured by a multi-scale Gaussian kernel, whose individual scales must cover
the neighborhood given in the discrete graph. Keeping the two graphs consistent is another way
to think about our iteration. Each resulting graph can be applied to many different algorithms for
data visualization, dimensionality reduction, and manifold inference.

Our approach to the problem is in the spirit of exploratory data analysis; it works with the
available data. This provides another view regarding the interaction between sampling and geom-
etry: one can only do as well as the available data allow; see Fig. 2. The situation is analogous
to that in learning theory, where there is a trade-off between the accuracy of the learner and the
coarseness of the hypothesis class over which she is learning [2]. Here, the space of manifolds
over which inferences are made is dictated by the available samples.

An overview of the paper is as follows. In the next section we review the background in some
detail, covering both the zoo of similarity kernels that exist in the literature plus several relevant
tutorial notions, such as manifold reach, that are well studied the in the theory literature. The
discussion is organized to emphasize the centrality of scale, or neighborhood, in all of these papers.
In Sec. 3 we provide an overview of our algorithm. It includes a brief sketch of both the two graphs
with which we work, plus the connection back to manifolds. Pseudocode for the algorithm is given
in 1, which also includes pointers to where those functions are developed.

We then expand on the algorithm. In Sec 3.3 we study the Gabriel graph and putative neigh-
bors. Two features are emphasized: scale-free neighborhoods and the relationship between node
degree and local dimensionality. A structural criterion is revealed, showing how putative edges
between neighbors ”ﬁll volumes” that block others from being neighbors. This graph serves as
an initialization. It is reﬁned iteratively in several steps. First, continuous scales are computed
from the discrete, putative ones; these continuous scales are then related to kernel scales. A linear
program relaxation bridges local scales to a global cover, in which each node’s weighted degree
is comparable to the number of its neighbors. In other words, each neighborhood radius should
not cover too many outside points. If it does, then it indicates that the neighborhood itself should
be reﬁned. That is, some putative scales are likely wrong, in the sense that their neighborhood
contains an extreme outlier. This leads directly to a volumetric statistic (Sec. 3.5.1, and to a prun-
ing technique for sparsifying the discrete graph. The process iterates until there are no outliers to
sparsify. In the end both the discrete neighborhood graph, the continuous weighted graph and the
scale associated with each node are returned.

Since the paper is largely algorithmic, we evaluate the results for estimating manifold dimen-
sion, geodesics, and low-dimensional embeddings. Comparisons against standard algorithms, such
as UMAP and t-SNE, illustrate the power of the approach. In the end we demonstrate that it is
possible to infer local scales from data given by similarity kernels that remain consistent with
geometric and topological properties of manifolds.

3

M

M

. Their pairwise distances are the only available data; properties of

Figure 1: Inferring the geometry of manifolds requires neighborhoods around each given data
point. Getting the scale for these neighborhoods, shown as balls, is fundamental. (A) Example of
. (B) Collection of points sampled from an unknown distribution
a one-dimensional manifold,
over
are not given
a priori. (C–D) Using a global kernel scale: if it is too small (left), the manifold will appear
disconnected, artiﬁcially producing clusters (poor sampling, good reach); notice how some balls do
not touch. If too big (right), the manifold may collapse giving rise to incorrect geometry/topology
(good sampling, poor reach). Notice how the balls overlap (covering dimension). (E) The use
of local scales based on a global number of nearest neighbors (in this example, k = 2) is still
susceptible to the problems above. (F) Our approach computes locally adaptive neighborhood
sizes, resulting in scales that conform to the local geometry and sampling.

M

4

Figure 2: The available data constrain manifold complexity: sampling Swiss cheese. Viewed from
left to right, as the number of sample points increases, the apparent manifold goes from a plane to
a plane with holes. The central panel indicates roughly where the holes in the manifold become
distinguishable from holes due to sampling. The result of our algorithm on these examples is
shown in Fig. 23.

5

2 Background

Manifold learning is a vast area of machine learning in which high-dimensional data are analyzed
[44],
based on the assumption that the data were sampled from a low-dimensional manifold
i.e., when geodesic distances over
provide a better description of the relationships between
data points than Euclidean distances in ambient space [11]. The manifold assumption ﬁnds ap-
plications in non-linear dimensionality reduction [102], de-noising[57], interpolation[23], dimen-
sionality estimation[25], computational geometry [33], and many others.

M

M

M

Since

locally resembles Euclidean space, it is standard to deﬁne a similarity kernel to deﬁne
(possibly weighted) neighborhoods around each point xi in terms of other points. This naturally
leads to a graph having data points as nodes and similarity values as edge weights. Then, com-
puting the graph Laplacian, one can apply a variety of methods from spectral graph theory[94].
Formal analysis involves the limit as the number of data points grows large; the practical success
of such methods on real samples depends on how well graph neighborhoods capture the topology
and geometry of

.

We here review the many approaches to specifying a kernel or a local neighborhood. We work
in ambient space Rn. When only pairwise distances are known,
with a d-dimensional manifold
an intuitive approach is to deﬁne the neighbors of a point xi as those within a certain distance
threshold, or, equivalently, inside an n-dimensional ball around xi. A kernel function assumes the
role of this “ball”, by assigning values to neighboring points as a function (discrete or continuous)
of how close they are to xi. The question becomes: what kernel sizes should be used for each
point?

M

M

2.1 Similarity kernel

Rn. Typically, a symmetric, positive semi-deﬁnite similarity kernel
Consider a set of points
is chosen to determine weighted connections between data points based on the ambient Euclidean
Rn, it returns a number between
distances between them. For each pair of data points xi, xj ∈
0 and 1 which determines how close, or strongly connected, they are. This effectively deﬁnes a
neighborhood around each point.

X ∈

2.1.1 Discrete kernels

Possibly the simplest choice for a kernel is the ε-neighborhood[10]:

Kij(ε) =

(

if

xi −
1,
k
0, otherwise.

xjk

< ε

(1)

This results in discrete-like neighborhoods whose sizes may be quite sensitive to the choice of ε,
so implicit is the assumption that sampling is approximately uniform.

Instead of deﬁning a neighborhood radius, a more common approach is to specify the number
Nk(xi) be the set containing the k points closest to xi in Rn (not

of neighboring points, k. Letting

6

including xi

1), a k-nearest-neighbor kernel can be deﬁned as:

Kij(k) =

if xj ∈ Nk(xi)

1,
0, otherwise.

(

(2)

which is commonly symmetrized by making Kij(k) = 1 if xj ∈ Nk(xi)
2.1.2 Continuous kernels – global scale

xi ∈ Nk(xj).

∨

To have the kernel values decrease with increasing distance between data points, a Gaussian kernel
is frequently used:

2

Kij(σ) = exp

xi −
σ2
is the Euclidean norm in Rn. This gives a continuous similarity scale from 1 (when xi
where
and xj are identical) down to some predetermined cutoff below which the kernel is considered to
be zero (meaning no connection in the data graph). Such a threshold is typically chosen due to be
a very small value, often at the limit of numerical precision, and is required to ensure compactness
of the kernel.

xjk

k · k

−k

(3)

!

,

One would like σ to be just large enough to be able to capture local manifold patches. There
are several heuristics for ﬁnding such a scale: the median of all pairwise distances in
(or another
percentile), the mean (or median) of the distances to each point’s kth nearest neighbor [67], or
a multiple of the maximal distance from a point to its nearest neighbor in the data [60]. Also
common is to choose a scale so that each data point is sufﬁciently connected to at least one other
point[68].

X

A different approach is based on inspection of the curve given by the sum of pairwise kernel

values [31]. When the double-sum

i,j Kij(σ) is plotted against σ on a log-log scale, the slope

P

d log

i,j Kij(σ)

(4)

d log σ
P
is proportional to the intrinsic dimensionality of the data. A global scale is then chosen from within
a linear region of such curve.

In [53], a similar procedure is proposed that considers, instead, the curve given by the weighted
average of the degrees Zi(σ) (i.e. sums of kernel values Kij) of each data point, after taking the
logarithm:

log Zi(σ)

=

i

h

P

i log Zi(σ)

(1/Zi(σ))

·
i(1/Zi(σ))

,

(5)

where Zi(σ) is the degree of xi. This weighted average (using the inverse of each point’s degree
as weights) is intended to compensate for density heterogeneities. The choice of σ is then made
plotted against log σ, which in many
precise by choosing the argmax of the slope of
h
cases should occur near the center of the linear region of eq. 4. One complication occurring in both
approaches, however, is that more than one linear section may exist (and, equivalently, more than
one local maximum of the slope), requiring that additional criteria be deﬁned to make the choice
of σ truly automated.

log Zi(σ)

P

i

1Throughout, when referring to a point’s set of k-nearest neighbors we will not include the point itself, unless

otherwise stated, and further assume that no two points are identical.

7

 
2.1.3 Continuous kernels – multi-scale

A more localized strategy is a multi-scale kernel, where each point has an individual scale or
bandwidth. Instead of a single, global scale, there are now N parameters, which has the advantage
that, if the scale selection were adequate, the kernel could capture the characteristics of more
complex datasets and manifolds (i.e., non-uniform sampling and geometry).

In the self-tuning method [111], the local scales are used in a Gaussian kernel by replacing the
global scale σ in eq. 3 by √σiσj, where σi and σj are the scales assigned to xi and xj, respectively.
This results in the symmetric kernel

Kij(σ) = exp

−k

2

xi −
σiσj

xjk

,

!

(6)

Each scale σi is commonly set to be the distance to the kth nearest neighbor of xi, using k = 7
[111, 80].

In [16, 17], a multi-scale kernel was proposed that combines the use of local bandwidths with

a global scale parameter ǫ. The kernel then takes the form:

Kε (xi, xj) = exp

2

xi −
4ǫ(qǫ(xi)qǫ(xj))β

xjk

−k

,

!

(7)

where qǫ is a local density function and β an additional (non-positive) parameter. An initial estimate
for the local bandwidth around each point xi is set as the square-root of the mean squared distance
to the k-nearest neighbors of xi, with k = 8. Finally, ǫ is automatically tuned as the argmax of
eq. 4 from [31]; however, the authors do not consider cases in which than one local maximum may
exist.

Other methods also adopt individual bandwidth parameters, but use asymmetric kernels that

are symmetrized a posteriori. In t-SNE [101], the single-scale Gaussian kernel

Kij(σi) = exp

−k

2

xjk

xi −
2σ2
i

!

gives a measure of afﬁnity, or similarity, between pairs of points. It is then normalized as

i(σi) =

pj

|

Kij(σi)
=i Kik(σi)

k

to yield transition probabilities, and ﬁnally symmetrized as

P

pij(σi, σj) =

1
2N

pj

|

i(σi) + pi

j(σj)

,

|

(8)

(9)

(10)

(cid:0)
Each σi is ﬁt to xi so that the distribution of pj
j attains entropy Hi such that its perplexity,
i,
|
2Hi (a real-valued number representing the ‘effective number of neighbors’), approximates some
prespeciﬁed value k. The authors recommend a value for k between 5 and 50.

∀

(cid:1)

8

 
 
 
6
In UMAP[78], an exponential kernel is used instead of the typical Gaussian. Using a prespeci-
Nk(i) be the set of k-nearest neighbors of xi. With ρi the distance to

ﬁed neighborhood size k, let
the nearest neighbor of xi, the kernel has the form

Kij(σi) = exp

max

−

0,

{

xi −
k
σi

xjk −

ρi}

(cid:18)

, j

∈ Nk(i),

(cid:19)

and is symmetrized as

Uij(σi, σj) = Kij(σi) + Kji(σj)

Kij(σi)Kji(σj),

−

(11)

(12)

so it can be seen as a hybrid between continuous and discrete, since Uil is set to zero for any point l
Nk(i). Each σi is ﬁt to xi so that
not in
j Kij(σi) approximates log2 k (in some sense analogous
to the perplexity approach from t-SNE).

P

2.1.4 Adaptive neighborhood size methods

A different class of methods attempt to automatically ﬁnd optimal neighborhoods. Most of these
are based on determining an optimal k for a k-nearest neighbors (k-NN) graph; this can be done
either globally or by selecting a local neighborhood size ki around each point xi (adaptive neigh-
borhood selection [102]).

Some approaches optimize a global k based on its performance in a speciﬁc embedding al-
gorithm. E.g., [92] is tailored to Isomap[96] and [66, 3] to LLE[89]. In [3], a local method is
additionally proposed that produces a nearest-neighbor graph with variable ki, under the assump-
tion that the manifold is connected.

Others are based on estimating the local tangent space around each point, then setting ki to
include as neighbors those points that are close to it. Such methods include [79] and [106], and
require positional information for the tangent space computation (usually via SVD).

Also available are methods that are not based on the nearest-neighbors concept. In compu-
tational geometry, the idea of reﬁning an initial estimate of connectivity from a simplicial mesh
has been used before, usually speciﬁc to the case when d = 2 and n = 3 (surfaces in 3-D space)
[4, 5, 14, 12]. Other approaches extend this idea to arbitrary dimensionality (e.g., [19, 13], but still
require knowledge of d. Most of the algorithms in this class use point clouds as input, so they can
exploit positional information to decide on the appropriate neighborhood/connectivity.

Among these myriad ways of estimating neighborhoods, there is little agreement on which is
most successful (see [72] for a review). Before proceeding to our algorithm, then, it is helpful
to ﬁrst understand what makes this such a hard problem. How can it fail, and what requirements
must it fulﬁll in order to properly capture the topology and geometry of
? This brings us to the
geometry of manifolds.

M

2.2 Reach and the geometry of manifolds

The neighborhoods implied by a kernel should agree with
, or at least approximate a tubular
neighborhood of it. As we showed (Fig. 1), if neighborhoods are too small, the implied mani-
fold may be disconnected (i.e., falsely divided into disjoint sub-manifolds, or clusters[92]); if too
to self-intersect, to collapse “bottlenecks” or curved regions, or cause
large, they may cause

M

M

9

“smoothing”, or “folding”. Such shortcomings are well-known in the manifold inference literature—
while the former case typically occurs due to non-uniform sampling, the latter is mainly caused by
an incompatibility between the sampling rate and the reach, or condition number, of
[42, 97].
We now expand these points.

M

Letting the medial axis of
M
, the reach τ can be deﬁned as the minimum distance from

be the set of points in Rn with at least two closest points in
to its medial axis. Locally, it
M
is constrained by the minimal radius of curvature (i.e., maximal curvature of a geodesic though
); globally, it is constrained by presence of bottlenecks (Fig. 3). The reach encodes essential
M
, and has been widely used in the manifold learning community [83, 84,
geometric properties of
1, 74, 4, 50, 43, 20, 19, 12]. It approximates the size of the largest ball in ambient Rn such that
can be seen as lying in Euclidean space Rd[18]. A related concept, the local feature
points in
M
size of a point xi ∈ M
, so τ can be
seen as the inﬁmum of the local feature size anywhere on

, is the smallest distance between xi and the medial axis of

.[13]

M

M

M

M

Figure 3: The reach, τ is a measure of the shape of a manifold. (A) A 1-dimensional manifold
M
with a bottleneck; the reach (double arrow) is the smallest distance between
and its medial axis
(dashed curve). (B) A highly curved manifold; now the reach indicates the high curvature region.

M

When τ is positive, it provides a measure of the “local distortion”[18]; the larger it is, the
easier inference becomes. Some authors, e.g. [82, 44], assume large reach in order to test the
manifold hypothesis and to ﬁnd bounds on the required sample size. In [18], the reach is used
when establishing bounds on the quality of an intrinsic dimensionality estimation based on k-
nearest neighbors.

Obtaining a good representation of

, therefore, requires consideration of its reach. In terms
of our problem of ﬁnding an appropriate neighborhood kernel, this effectively means that no edge
segment ℓij between two points xi and xj that are considered neighbors should cross the medial
axis of

M

.

Sampling is a further complication, and essentially what makes this a hard problem—when it
is nonuniform and sparse (common in real-life datasets), it is not always clear whether the space
between points constitutes an undersampled piece of
, a hole, or a gap between disjoint sub-
manifolds (cf. Fig. 2). The latter two conditions, of course, relate to reach. Narayanan et a. [82]
prove that the number of required samples depends polynomially on the curvature, exponentially
on intrinsic dimension, and linearly on intrinsic volume. Aspects of our algorithm address each of
these during the iteration process.

M

M

10

M

In all such cases, choosing a globally-ﬁxed radius is likely to be problematic. While deﬁning
neighborhood size based on a ﬁxed number k of neighbors can be helpful to deal with nonuniform
density (since the neighborhood radius adapts to the local pairwise distances), it is bound to violate
the reach if k is too large. It will also be a problem when the intrinsic dimensionality is not constant
throughout

, as higher dimensions require exponentially more neighbors.

The authors in [79] point out the lack of a principled way for setting this parameter, which
in practice is often tuned empirically based on prior knowledge of the desired output. As put by
[106], the effectiveness of manifold learning algorithms depends on how nearby neighborhoods
overlap and on the interplay between the curvature of the manifold and sampling density.

In terms relevant to this paper, the kernel’s neighborhood radius should be smaller than the
local feature size, but large enough to account for sampling variability and local dimensionality.
We propose an iterative approach to developing the kernel, so that it can adapt appropriately to the
neighborhood characteristics around each point.

11

3 The algorithm

We here overview our algorithm for ﬁnding the neighborhood scale around each point in a manner
that makes it globally consistent as a covering of the data points. As is common in ML, we start
with the distance matrix between data points, not the points themselves. This distance matrix
normally derives from a kernel. The ﬁrst step is to build a graph in which each datum is connected
to an appropriate neighborhood of other data points. This data graph deﬁnes a topology; in the end
we refer to it as the neighborhood graph. As we reviewed above, in the discrete case one might
choose k-nearest neighbors, while in the continuous kernel case there is a bandwidth parameter
that effectively deﬁnes a “ball of inﬂuence” around each point. Scale is the radius of that ball; a
level set of the kernel function that essentially contains those neighbors whose weights are non-
trivial. Our goal, then, is to ﬁnd those scales – or that neighborhood graph – that support non-linear
dimensionality reduction, geodesic estimation and, in general, manifold inference algorithms from
the given distances. We do not have sampling guarantees, so will develop a statistic to check
whether reach and curvature constraints might be violated.

3.1 Subtleties of Scale

Since scale may not be constant across the data set, we argue that it should be the ﬁrst property to
infer from the data. We start by imposing the manifold assumption, but from an empirical perspec-
tive. Unlike most theoretical studies, we do not assume the manifold is pure—i.e., has constant
dimension. In a simple case, the data may be drawn from a union of different manifolds, whose
dimensions are not known a priori (such datasets have been considered infrequently, although
exceptions exist, e.g., [54, 74]). Second, we do not know the sampling rate, or density. Rather,
we build it up, conservatively, with putative nearest neighbors to each data point, by imposing a
necessary (but not sufﬁcient) condition. These putative neighbors will be reﬁned as the algorithm
iterates, to achieve sufﬁciency. While the manifold assumption does imply the existence of local
neighborhoods, their size may vary over the dataset; we require that the sampling be nearly con-
stant over each of them. In effect the density of points must be determined locally while respecting
the global manifold geometry.

We illustrate the complexity of this situation in Fig. 4. Shown is a data sphere with an apparent
spike emerging from it. On one hand, such complex datasets could derive from two unrelated
systems, which only appear to connect through their embeddings. On the other hand, the data
could derive from a non-linear system that includes two regimes, one responsible for the spherical
data and the other for the spike. To handle the ﬁrst situation, we must allow datasets to consist of
unions of manifolds. This suggests the interpretation in Fig. 4(b), where the separation is obscured
by sampling. Since manifolds with boundary and high curvature are also possible, the situation
in Fig. 4(c) arises. Here there is an apparent change in intrinsic dimension due to the small reach
in the spike and the large boundary curvature. Because the (2-D) spike is so narrow, sampling
suggests it is 1-dimensional while the bulk of the points derive from a 2-D manifold.

We submit that such situations occur in real datasets and, since the data are ﬁxed, we cannot
appeal to knowing the sampling density or the manifold dimensions and reach. Instead we address
the interplay between manifold reach and sampling density pragmatically. “Standing” on the spike,
the data appear to be 1-D; on the ball, 2-D. We seek a neighborhood graph that supports these
inferences, so “most” points enjoy a neighborhood that agrees with their apparent dimension. At

12

the join, or the high-curvature neck, it is unclear. Moving from the spike to the ball suggests
that dimension should be increasing; from the ball to the spike it should be decreasing. For the
neighborhood graph, most points along the spike should see
2 neighbors, and most points in the
4 neighbors; the problematic points should see something intermediate. Such
ball should see
results will be shown to follow from our algorithm.

∼

∼

We claim that either of the alternatives is worse; one should not impose an apparent dimension-
ality (or connectivity in the neighborhood graph) globally. To wit, if small numbers of neighbors
(appropriate for the spike) are enforced on the ball, then holes are likely to be introduced. Or if too
many neighbors are enforced on the spike, it will collapse on itself. (These situations are illustrated
later, in Fig. 25.) Both change the topology drastically.

Figure 4: The ‘manifold’ subtleties of complex datasets. (A) A sampled data from a non-linear
system that includes two regimes. (B) It may be the case that the data in each regime deﬁne a
manifold, and they are actually separate (indicated by the plane). After sampling, however, the
evidence for the plane is absent. (C) Or the data may be drawn from a single, connected manifold
whose geometric properties change rapidly.

3.2 Overview of algorithm

be a sampling of a (possibly non-pure) manifold

consists of N points xi in the ambient space Rn, n > dα,

Notation: Let the dataset
dim
manifold may have a boundary. The number of components, α, is not known a priori.

X
Mα = dα. The dataset
We work with two graphs, the ﬁrst unweighted and the second with edge weights given by a
= N and adjacency matrix A with
V . We begin with a conservative

kernel. Denote the (unweighted) graph G = (V, E) with
|
entries aij, where to each point xi is associated a node i
estimate of G(0); successive reﬁnements are indicated G(t) until convergence (G∗).

∪αMα, with
α. The
∀

|
∈

M

=

X

V

Our strategy: after obtaining a coarse initial estimate of the discrete neighbor graph, we extend
it to a global weighted graph that suggests an estimated manifold covering. The validity of this
extension is evaluated by a measure of volume in both graphs; an iterative algorithm to estimate
local scales for each point xi. Before presenting the algorithm, we introduce the two graphs.

Since we seek a scale for each data point, we work with a multi-scale Gaussian similarity kernel

deﬁned as in Sec. 2.1:

2

xi −
σiσj

xjk

.

(cid:19)

Kij = exp

−k

(cid:18)

13

(13)

M

The kernel value Kij is therefore symmetric and equivalent to that of a traditional Gaussian kernel
using the geometric mean of σi and σj as its scale or bandwidth. Notice, in particular, how the
scales and the kernel values are coupled: setting the scale incorrectly could make distant points xi
and xj appear close in similarity.

Given a set of individual point scales σi (sometimes collected into the vector σ

RN ), we
, W ) as the complete graph on all pairs of data points
deﬁne a second, weighted graph
in
. Its weighted adjacency matrix, W , has entries wij = Kij. While the unweighted graph will
be related to nearest neighbors and computational geometry, the weighted graph will be related to
spectral methods on manifold inference. In particular, we expect the Laplacian of
to approximate
the Laplace-Beltrami operator on

, subject to the number of data points and their sampling.

= (V,

X

∈

G

G

E

The algorithm is initialized by computing a coarse estimate of G. As described in section 3.3,
this is achieved by exploiting the geometry of medial balls between pairs of points to produce
a Gabriel graph [49, 77]. The main advantages of the Gabriel graph as a starting point are: (i)
it is scale invariant, so a prespeciﬁed ε-neighborhood (eq. 1) is not required; and (ii) neither are
neighborhoods determined by a number ki of nearest neighbors, which allows for connections to
“jump across” sampling gaps while keeping the data graph sparse.
However, as described above, obtaining a good inference of

really means ﬁnding reasonable
estimates of its reach. For that to occur, no edge segment ℓij between any two points xi and xj
should cross a medial axis of
. As the examples that follow will show, there are several cases in
which the Gabriel graph will violate this. Therefore, additional steps are necessary to reﬁne it. The
Gabriel graph provides a necessary condition (all the correct connections are present, but possibly
others as well); the reﬁnement moves toward sufﬁciency.

M

M

G

G

In order to estimate

—the weighted counterpart of G—we will use the weights that are ob-
tained by applying a continuous kernel over the points in
. Such a kernel requires scales, or
bandwidths, σ that must be estimated from G. These will be obtained from an optimization proce-
dure that ﬁnds the smallest such scales that ensure all discrete edges have a minimum kernel value
as weight. At this point, a weighted graph

can be obtained from σ.

X

M

Rn -dense in

pi −
,

pjk
i or at least that

At this point it is helpful to articulate the geometry more carefully; see Fig. 5, which illustrates
how the discrete connectivity relates to the manifold geometry. In particular, for a real dataset, the
few closest points surrounding xi are the best candidates for ‘nearest’ neighbors—this is all that
can be asserted locally. Now, let pi and pj be the projections of xi and xj onto
, respectively. If
between xi and xj—again, all that we can assume from
the sampling is
M
k
our limited initial information—then there should be a geodesic between pi and pj. By further
xi −
assuming xi ∈ M
xi − Mk
Rn approximates the
∀
k
geodesic when the curvature between pi and pj is small. Equivalently, the line segment ℓij between
, where p is the midpoint between xi and xj; see Fig. 5.
xi and xj lies on the tangent space TpM
Such an “edge-centric” approach connects differential geometry to the underlying graph. In
effect, we are assuming that each node i and its neighbors in G all lie near the tangent space
around xi. This is shown in Fig. 5 where the kernel values are shown shading in the tangent plane.
Notice how the point xi and neighbor xj both fall under the bright kernel values; i.e., they are very
similar (in this measure) to each other. Stated in geometric terms, we assume that the neighbors all
lie within the injectivity radius around xi. We will show (Fig. 14) that the value of a multi-scale
kernel between two data points is equivalent to that of a rescaled kernel centered at the midpoint
between those two points.

Rn < δ,

i, then

xjk

∀

k

Now, the optimized scales can in turn be used to evaluate the current approximation and iden-

14

Figure 5: Relating the discrete neighborhood graph to the manifold geometry. Nearby sampled
points (xi and xj) on a patch of manifold
to the midpoint
(p). Line segments (edges) between neighboring points lift, via the exponential map, to geodesics
in
. The continuous kernel extends this discrete relationship to the full tangent plane. The values
of the kernel centered at p are shown as shading, which extends in every direction in the tangent
plane. Our algorithm shall enforce this relationship; i.e., the consistency between large values of
the kernel and the discrete edges.

lie in (or near) the tangent plane TpM

M

M

tify the edges in G that are “too expensive;” i.e., are likely violating reach. We proceed by com-
puting successive reﬁnements of both G and σ in an iterative manner, until no further change is
observed. We then return the ﬁnal version of the discrete and weighted graphs (denoted G⋆ and

⋆, respectively).

G

One can view the computation of

as a relaxation of the discrete connectivity in G. In fact,
as we shall see in section 3.5, a relaxation statistic, δ′i, will be used to prune discrete edges that
produce a poor approximation. More speciﬁcally, when a node i with degree deg(i) in G has δ′i
close to 1, it means i has retained the same degree in
, only continuously spread as a Gaussian
around it.

G

G

Each of the steps above are listed in Algorithm 1 and will be described in detail. We begin with
the discrete connectivity rule (Gabriel graph); then the scale optimization is developed, followed

15

by the edge-pruning step. Fig. 6 illustrates the action of our algorithm on datasets for which the
Gabriel graph alone cannot infer a good approximation of the manifold connectivity.

A

G (0)

G (cid:1)

¾(cid:0)

G(cid:2)

B

G (0)

G (cid:3)

G(cid:4)

Figure 6: Steps of Algorithm 1 on toy datasets with various challenges. (A) Dataset with several
challenges: non-uniform density, non-uniform dimensionality, and high curvature. After pruning
6 edges (dashed red lines) from the original Gabriel graph (G(0)) the algorithm converges, infer-
ring reasonable discrete neighborhoods (G⋆); the optimal scales σ⋆ produce a weighted graph
⋆
G
whose connectivity closely approximates that of G⋆. (B) Dataset with three Gaussian clusters of
non-uniform density. The Gabriel graph approximation (G(0)) naively connects all clusters using
multiple edges. After convergence, the clusters become disconnected in G⋆, and its weighted ver-
sion follows this by assigning negligible weights (due to σ⋆ between points from different clusters.

16

Algorithm 1 Iterated Adaptive Neighborhoods kernel

⊲ Input: distance matrix, D
⊲ Compute initial G (sec. 3.3)

←

σ(t),
(t)

GABRIELGRAPH(D)

G(0)
repeat Iteration

1: procedure IANKERNEL(D)
2:
3:
4:
5:
6:
7:
until no further change in G
8:
⋆, σ⋆
return G⋆,
9:
10: end procedure

←
←
G
δ, C
←
G(t+1)

←

G

SPARSIFY(G(t) , δ)

OPTIMIZESCALES(G(t) , D)
MULTISCALEKERNEL(D, σ(t) )
COMPUTEVOLUMERATIOS(G(t) , σ(t))

⊲ Update scales σ based on G (sec. 3.4)
⊲ Compute weighted graph (eq. 13)
⊲ Relaxation statistic, δ (sec. 3.5.1)
⊲ Update G (sec. 3.5.2)

⊲ Output: ﬁnal discrete and weighted graphs and optimal scales

3.3 Nearest neighbors in a Gabriel graph

We begin by deﬁning a set of putative neighboring points of xi (denoted as
(i)), which uses the
connectivity rule found in a Gabriel graph [49, 77]. It incorporates directly the observation that
closest neighbors should have no points ‘between’ them.

N

Deﬁnition: two points i and j are Gabriel-nearest neighbors to each other if and only
if they both touch the same closed ball

Bij that is empty except for i and j.

Note that

Bij is a medial ball, i.e. a ball whose center point is a medial axis, so it has at least
two closest points. Thus, this connectivity criterion can be restated as creating an edge for all those
medial balls (and only those) touching exclusively two points (to be clear, if a third point touches
Bij no edge shall be formed).
and xj with radius
xjk
deﬁnitions:

Bij centered and the mid-point between xi
/2 (Fig. 7). This is furthermore equivalent to the following alternative

Hence, to each edge eij is associated a medial ball

xi −

k

Remark 1: xi and xj are Gabriel-nearest neighbors if and only if any point along the
line segment ℓij = xixj in Rn has either xi or xj as closest point (or both) and no
other point.

Remark 2: In terms of the Voronoi diagram[45] of
(with the cell around xi denoted
by Vi), xi and xj are neighbors when ℓij crosses a single Voronoi hyperplane Hij,
namely that between the cells Vi and Vj, and the mid-point between xi and xj is in
Hij.

X

As a concrete example, consider two points i and j at a distance rij from each other, with
midpoint c. Assume the region in the manifold between them is uniformly sampled. Now consider
the ball centered at c with radius rij/2, therefore touching i and j. If it contains no points in its
interior, we say i and j are nearest neighbors. Conversely, if there are other points in its interior,
under our assumption of uniform density this means there is at least one other point k “between”
i and j. So we say i and j are not nearest neighbors, in the sense that connecting i and j directly
would be “crossing over” the point k; this implies that a direct edge eij in the resulting graph would
is ‘locally uniformly sampled,’ moving
be a poor approximation to a geodesic over

(i.e., if

M

M

17

). Fig. 7 illustrates this procedure. Note that,
directly from i to j would be passing outside of
even when the input to the algorithm is solely a distance matrix (i.e., with no position information),
this connectivity criterion may still be evaluated by considering the triangle xi-xj-xk and using
Apollonius’s theorem to compute the length of the median from xk to c (Fig. 7-A).

M

Figure 7: Connecting ‘nearest neighbors’. (A) A set of data points in space. (B) An edge can be
formed between xi and xj because there is no other point in the interior of the ball
Bij centered
halfway between xi and xj. (C) In this case, because of the presence of a third point, xj, inside
Bij, xi and xj cannot be neighbors. (D) Even in the absence of the original data point coordinates,
i.e., given only the distances between all pairs of points, Apollonius’s formula can be used to
determine the length of the segment p–xk, where p is the center of
Bij. Namely, p–xk is a median
of the triangle depicted in orange. Here, because the length of the median is less than the radius of
Bij, xi and xj cannot be neighbors. (E) Edges are drawn connecting points xi to xk and xk to xj
because both

Bkj are empty except for those pairs of points.

Bik and

The Gabriel graph is a subgraph of the Delaunay graph [38] and enjoy a number of key prop-
erties [77]. We emphasize: (1) they are scale invariant, i.e., there is no pre-speciﬁed threshold on
the diameter of medial balls to form connections; (2) the guarantee that Gabriel graphs connect
is uniformly sampled as grid (as in Fig. 9); and
points to their true nearest neighbors when
(3), Gabriel graphs provide a locally-adapted neighborhood size ki for each point xi based on the
local geometry. They do not require an initial guess of the number of neighbors, of the intrinsic
dimensionality, or of a maximum neighborhood radius.

M

But the neighborhoods given by the Gabriel graph as not sufﬁcent. We now expand several
properties that illustrate their limitations (these will be useful in motivating the rest of the algo-
rithm).

3.3.1 Closing triangles

Here we show that the edges created using the above connectivity rule can only form acute triangles
in Rn. Let three points i,j,k be such that i and k are connected, as well as j and k. The rule says
i and j will only be connected if k is outside the closed ball
Bij of radius R = rij/2 between i
and j (where rij stands for the Euclidean distance between i and j). Apollonius’s formula for the
squared distance m2 between k and the mid-point between i and j is:

m2 =

1
4

(2r2

ik + 2r2

jk −

r2
ij).

(14)

18

Then, k is inside

Bij if and only if m2 < R2, so
1
4

(2r2

r2
ik + 2r2
ij)
jk −
ik + r2
r2

≤

jk ≤

R2 = (

rij
2

)2

r2
ij.

(15)

Notice that equality will hold when i

j

−

−

k is a right triangle. Therefore,

Remark 3: A triangle will be closed by edges only when the triangle i
(Fig. 8-A).

j

−

−

k is acute

M

Figure 8: Implications of the connectivity rule in a Gabriel graph on closing triangles from edges
and on the maximum curvature of a geodesic on
that can be inferred from the resulting con-
nectivity. (A) Left: three points will be mutual neighbors if and only if they form an acute triangle
(left). If the angle between i and j at k is at least π/2, all three points will touch the ball
Bij,
so no edge is created (right). (B) Geodesics over a manifold (in blue). The maximum principal
curvature in
that can be reasonably approximated by the resulting graph geodesic is a function
of the sampling interval. The limiting case occurs when three points form a right triangle (top)
(see eq. 16). Closing a triangle (bottom left) prevents the graph from adequately capturing the
manifold’s geometry. As sampling frequency increases (bottom right), higher curvatures can be
better approximated by the Gabriel graph. (Distances denoted by r

are taken in ambient space.)

M

··

3.3.2 Maximum curvature

The above result leads to a bound on the maximum principal curvature that is allowed locally on
such that G correctly approximates it (i.e., without closing a triangle). Assume i, j, and k are
M
as in Fig. 8-B, up to the level that the sampling deﬁnes; then, if
points on a smooth manifold
we assume constant curvature κ locally, the geodesic from i to j passing through k is an arc of a
circle
, but not ℓij (which in
fact would even increase the local intrinsic dimensionality). Hence, values of curvature that can

. Therefore, the segments ℓik and ℓkj approximate geodesics on

M

M

C

19

be correctly inferred are those that do not create an edge between i and j in the Gabriel graph
(i.e., those for which the ball
Bij is non-empty), as shown in Fig. 8-B. In this case, from Eq. 15,
the maximum such curvature, κmax, occurs when i, k, and j form a right triangle in space (as any
larger value would cause this triangle to be acute and i would be connected to j). Then, from
Thales’s theorem, the diameter D of

would equal that of the hypotenuse ℓij, so

C

κmax =

1
D/2

=

2
D

=

2
ik + r2
r2
jk

.

(16)

An interesting case to consider is that when
is uniformly sampled with constant interval T over
arc length. Then, the arc length s between i and j is 2T ; but, since rij = D, s covers half the circle
and we have 2T = πD/2. Eq. 16 then becomes

M

q

Remark 4:

κmax(T ) =

π
2T

.

(17)

These deﬁne the maximum geodesic curvature in
graph. As a consequence, the reach is lower bounded by 1/κmax.

M

that can be correctly inferred from a Gabriel

3.3.3 Degree distribution in Gabriel graphs

We now study the above connectivity rule starting with ﬂat, uniformly sampled manifolds (“grids”)
to illustrate how Gabriel graphs naturally adapt to both the geometry and dimensionality of the grid.
As shown in Fig. 9, in such ideal cases the degree of an interior node in the Gabriel graph agrees
with the true number of (literal) nearest neighbors, i.e.: 2 for collinear points, 4 for a square grid,
and 6 for a triangular grid (Fig. 9-A).

The degree corresponds to the dimension (degree

2d) except for the (curious) triangular
grid. Adding noise (Gaussian noise with standard deviation equal to half the spacing between
neighboring points) supports this conjecture, as the degree then approaches 4 regardless of the
original grid structure. This holds in higher dimensions as well (Fig. 9(B,C) for both normal and
uniform sampling at random (Fig. 11).

∼

Remark 5: The expected number of neighbors in a Gabriel graph approximately fol-
lows a distribution centered at 2d (where d is the intrinsic dimensionality of the dataset)
for a variety of sampling strategies. (Fig. 9-C).

Because Gabriel graphs are inherently scale invariant, this degree distribution is largely indepen-
dent of sampling density.

How to explain such remarkable regularity despite the randomness of sampling? There is
a geometric view of the Gabriel graphs: each edge between data points implies an “occluding
hyperplane” that blocks other points from becoming neighbors (Fig. 7). For example, when d =
1, two points necessarily occlude any additional connections, and every non-boundary point must
have 2 neighbors. Now, using the diagrams in Figs. 9-D–F as reference, we ﬁnd that, when d
4 points are sufﬁcient to occlude a point xi from all sides. For d = 3, this
= 2, on average
8, revealing the trend.
number is doubled again and the expected number of neighbors becomes

∼

∼

20

Every additional dimension adds a new coordinate axis along which the previous constraints are
duplicated, roughly doubling the average number of directions available for neighbors to connect
from. Once 2d balls are “attached” to xi, the space available becomes too small, and so does
enclosed by the hyperplanes.
the probability of drawing a sample point from inside the region

H

Figure 9: Gabriel graphs computed on different uniformly sampled datasets (regular grids) with
various amounts of Gaussian noise and the resulting distributions of node degree, di (number of
neighbors) for non-boundary points. (A) Top: A sequence of collinear points (left) produces a
one-dimensional grid (center). Addition of noise (right) does not change the mean degree ¯di (a
constant 2 for interior points). Middle: A square grid (left) result in a quadrilateral mesh with with
constant degree 4 in its interior. Although the addition of noise considerably scrambles the points,
the mean degree is approximately unchanged. Bottom: Points arranged as a triangular grid (left)
result in a triangular mesh where every interior node has degree 6. Its noisy version looks similar
to a noisy square grid, with its mean degree decreasing to about 4 as well. (Cont. next page.)

21

(Cont. from previous page.) (B) Change in degree distribution for interior points of d-
Figure 9:
dimensional triangular and square grids upon addition of Gaussian noise with increasing standard
deviation (as a scalar multiple of the original sampling interval, T ). Moderate amounts of noise are
sufﬁcient to make the mean degrees be approximately 2d. Error bars indicate standard deviation;
dotted lines show constant 2n values for reference. Noise applied to grids in (A) has 0.5T std. dev.
(C) Mean node degree of n-dimensional manifolds sampled using different strategies: uniformly
at random, normally at random, and as jittered versions of regular triangular and square grids
(added Gaussian noise with std. dev. 0.5T ). Remarkably, mean degree grows approximately as 2n
regardless of the sampling strategy.

When the neighbors are regularly spread around xi, by construction this region
is equivalent to a
d-dimensional orthoplex2 (or cross-polytope). A d-orthoplex has 2d facets (or (d - 1)-faces), and it
is one of the three regular, convex polytopes that exist in dimension higher than 4 (hypercubes and
simplices). Naturally, when sampling is not uniform, we should ﬁnd irregular orthoplexes instead.
While this geometric construction supports our empirical results, and implies they should hold in
higher dimensions, it also suggests:

H

2An orthoplex is a line segment in 1-D, a square in 2-D, a regular octahedron in 3-D, and a 16-cell in 4-D.

Figure 10: A central point xi and the occlusion planes due to neighbors (Fig. 7) in dimensions 1,
2, and 3 (panels A–C, respectively). Every additional dimension adds a new coordinate axis along
which the previous constraints are duplicated, roughly doubling the average amount of directions
available for neighbors to connect from. Once 2d balls are “attached” to xi, the remaining space is
greatly reduced, and so is the probability of drawing a sample point from inside the region enclosed
by the hyperplanes.

22

Figure 11: Distribution of node degree deg(i) in the Gabriel graph of datasets with different
sampling strategies and dimensionalities. Top: Points sampled normally (orange) or uniformly
(blue) at random from a two-dimensional ball result in similar degree distributions centered at
22. Bottom: In higher dimensions, interior points continue to follow this pattern. On the left, a
4-dimensional unit ball sampled uniformly at random is shown projected onto ﬁrst 3 coordinates,
with boundary points labeled as those with vector norm > 0.9 (edges omitted for clarity).
It
24, and the
produces a Gabriel graph where interior points have degree distribution centered at
mean degree of boundary points is closer to 23.

∼

Remark 6: The organization of randomly scattered neighbors around a point is more
similar to facets of an orthoplex than to those of a hypercube or of a simplex (these
latter two correspond to square grids and triangular grids, respectively, and are too
sparse and non-generic to correctly describe random points).

The Gabriel graph enjoys many attractive properties, and provides the starting point for our
algorithm. The above arguments show how the space is largely ﬁlled by Gabriel balls within the
manifold, but Gabriel balls may also ﬁll space across holes and reaches. Curvature must be dealt
with. This was illustrated in Fig. 6, where we showed that Gabriel connections can arise incorrectly
and must be removed. To do so one must ‘look’ in every direction (of the tangent plane), and one
must look past immediate neighbors. For this we now develop the weighted graph counterpart
to the Gabriel graph, exploiting the kernel to extend local information globally. This begins to
connect the graph construction more directly to manifold properties.

23

Figure 12: (A) A central point xi (in blue) and its neighbors (in black). Every neighbor xj that xi
Bij at xj (dashed lines).That is, no
has “occludes” the entire area behind a hyperplane tangent to
point inside the occluded areas (shaded region) can form a connection with xi. Here, the dashed
Bik does not form a connection because xj lies exactly on its boundary; despite this, xk still
ball
contributes with an occluding hyperplane, preventing farther points from forming a connection
with xi. (B) In principle there is no limit to the number of neighbors a point in ambient space Rn
2), since any point on the hypersphere of radius rij around xi (dotted curve
may have (when n
in orange) will not be occluded. Sets of nodes with connectivity such as this are termed “wheels”
in graph theory, and the more points they contain the more unlikely to occur they become in real
datasets. In this example, any appreciable variability in the distances from xi to its neighbors would
occlude one (or several) of the others. (C) Even points inside occluded areas can contribute with
additional occluding hyperplanes. Here, although xk lies inside the region occluded by xj (and
therefore cannot form a connection with xi), it produces further occlusion behind a hyperplane of
its own (region shaded in red). So xl cannot connect to xi either, due to the presence of xk (even
though it is not occluded by xj).

≥

3.4 Multi-scale optimization

We now begin to develop the iteration in Algorithm 1, given the initial Gabriel neighborhood
graph G(0). Assuming (temporarily) that this gives correct local neighborhoods, what should the
corresponding scales be for a Gaussian kernel? In effect this is an extension of G into a weighted
counterpart,
. From Fig. 5, this weighted graph is also a type of approximation of (aspects of)
the continuous manifold. Because density is not necessarily uniform, different points might have
different neighborhood radii, so a multi-scale Gaussian similarity kernel (eq. 13) is used. Each
point xi has its own associated scale, σi. To develop this scale, we now move into the continuous
domain and exploit the geometric notion of a cover.

G

3.4.1 Covering criterion

A criterion for separability between two Gaussians has been developed in the mixture-of-Gaussians
literature. [34, 7, 103]: two spherical Gaussians i and j can be distinguished (in the sense of solving
a classiﬁcation problem) with reasonable probability when they have a separation of at least

at which the overlap in their probability mass is a constant fraction [103]. We ﬂip this around, by
using Gaussians centered at midpoints to indicate when neighboring points should be connected,

µi −

k

µjk

> C max

σi, σj}

,

{

(18)

24

not separated (Fig. 5 illustrates this construction directly). Furthermore, because we use a multi-
scale kernel, the (non-normalized) density is a function of √σiσj. Hence we obtain a criterion for
what we term C-connectivity:

Deﬁnition: Two neighbors i and j in the discrete graph G = (E, V ) are C-connected
by the multi-scale kernel when the geometric mean of their individual scales is at least
the distance between xi and xj scaled by a positive constant C:

C

xi −

k

xjk ≤

√σiσj.

(19)

The constant, C, plays a role in normalizing for unknown density. It will be developed in Sec. 3.5.2.
For now, we illustrate its role in the connection from graphs to manifolds. Fig. 13 shows the graph
over a set of data points, and the local scales obtained (by the algorithm below) for different values
of C. Choosing C too large yields scales (and hence Gaussians) that are too large; that is, their
overlap has peaks. Choosing it too small yields scales that introduce holes. Choosing it correctly
(in this case, C = 0.9) shows how the Gaussians form a covering of the manifold that approximates
a partition of unity. Such partitions of unity are used in differential geometry to extend local
information (in our case, the scales) to global information (a covering of the manifold).

By choosing appropriate scales, i.e. scales that meet our criterion for all edges in E, we also
ensure a covering of the edges, in the following sense. The value of the multi-scale kernel Kij
between xi and xj is identical to that of a kernel re-centered at the midpoint p
(xi + xj)/2 and
re-scaled using half the geometric mean of σi and σj as its scale σp:

≡

Kij = exp −k

2

xi −
σiσj

xjk

= exp −k

2

xj)/2

(xi −
σiσj/22

k

= exp −k

xi)

2

k

(p

−
σ2
p

(20)

with σp

≡

√σiσj/2.

Remark 7: We say a C-covering is attained when every pair (i, j)
E is C-connected
(eq. 19). Additionally, when the spacing between neighboring points is approximately
uniform locally, the pointwise summation over all Gaussian kernel bumps given by the
individual scales provides an (un-normalized) partition of unity of

∈

.

M

G

close to that of G. Thus, one idea is to ﬁnd scales such that the sum of edge weights in

We now use the covering constraints to solve for the set of scales σ. It is desirable to have
the scales be small (respecting the reach), while at the same time maintaining the connectivity
in
G
incident to a node i from its neighbors in G is approximately equal to the degree of i in G, for all
i, while at the same time ensuring a C-covering. This, however, amounts to a non-convex problem
in which the cost function involves a summation of multi-scale kernel values. We are unable to
solve this efﬁciently. Instead, we ﬁnd the smallest individual scales such that our covering criterion
is satisﬁed for all edges (a ‘minimal covering’), and later address the quality of the relaxation by
using a statistical pruning (edge sparsiﬁcation). This can be transformed into a convex, linear
program with linear constraints by which all scales can be solved for simultaneously, as we now
show.

25

1
=
C

9
:
0
=
C

8
:
0
=
C

2.0

1.5

1.0

0.5

0.0

s
n
a
i
s
s
u
a
G

f
o

n
o
i
t
a
m
m
u
S

Figure 13: Effect of hyperparameter C from eq. 19 on the resulting weighted graph (left), optimal
scales (middle), and manifold approximation (right, shown as the resulting summation over the
Gaussian kernels around each point using their individual scales). For C = 1 (top), the scales
overlap too much and as a result the Gaussian summation (right) is highly non-uniform. For
C = 0.8 (bottom), the scales are not sufﬁciently large to properly cover the underlying manifold,
resulting in holes (right). When C = 0.9, there is a good compromise between covering and keeping
1
a uniform density, so the Gaussian summation approximates a partition of unity (summing to
everywhere) when the scales correctly conform to the local sampling characteristics. Our approach
will allow us to tune C based on a relaxation statistic, δ.

∼

3.4.2 Linear program relaxation

To achieve a minimal covering one might minimize
i σi (or, equivalently, the 1-norm of the
vector σ, since scales are positive) subject to the covering constraint3. This suggests the following:

P

Optimization Problem:

min
σ
s.t.

1⊺σ

(i, j) is C-connected
σi is bounded,

i

∀
V

∀

∈

(i, j)

E

∈

(21)

3Another possibility is to use a weighted sum

i νiσi while keeping the same constraints, thus still guaranteeing a
covering. The weights νi add a bias to how the length of an edge is split between its two incident nodes (by balancing
P
their individual scales). One interesting option is to set νi = rnon
the ratio between the distance to the
i
, and the farthest neighbor, rFN
nearest non-neighboring point, rnon
.
i

/rFN
i

, i.e.

i

26

 
 
2rij

A

j
¾

rij

2
3 rij

B

1
2 rij

rij

¾i

e
u
a
v

l

l

e
n
r
e
k

1

0

C

2rij

i p j

scale:

p ¾i¾j

p ¾i¾j =2

Figure 14: Covering constraint for the multi-scale kernel of eq. 13. Left: A graph G with two
nodes i and j at a distance rij from each other in Rn. Since they are connected, their assigned
r2
ij to satisfy the covering constraint (here we
individual scales σi and σj must satisfy σiσj ≥
assume C=1). Center: All feasible pairs (σi, σj) lie inside the region above a positive hyperbola,
three of which are indicated as colored points; the pairs A and B satisfy exactly, while C satisﬁes
in excess. Each one is also depicted as a pair of circles on the left plot using the same color code,
each one centered at its corresponding node (radii are set to half the scale, for clarity). Although
pairs A and B differ in their ratio σi/σj, both result in the same multi-scale kernel value for the
edge (i, j) since the product σiσj is the same (pair C will yield a slightly higher value). This
illustrates the freedom that might exist in choosing an optimal combination of scales for all nodes
(i.e., a covering). Right: multi-scale kernel (Kij) values centered at either i or j (shown in green)
are symmetric (with scale √σiσj). Horizontal axis represents position over the line in Rn passing
through i and j. A kernel centered at the midpoint p between i and j using half the scale (black
curve) attains the same values as Kij. Dashed red line indicates the common value between all
three kernels.

where σ is the vector of individual scales σi and 1 the all-ones vector. Now it remains to represent
the C-covering requirement with a set of constraints.

Looking in detail at C-connectedness (eq. 19) as a function of σi and σj, observe that it repre-
sents a region delimited by a single-branched hyperbola (since the distance and scales are positive):

σiσj ≥

(Crij)2,

σi > 0, σj > 0,

(22)

). Each σi is naturally bounded above by the distance to i’s farthest neighbor,

where rij ≡ k
rFN
i

:

xi −

xjk

rFN
i

,

(23)

σi ≤

beyond which all neighbors are satisﬁed4, so further increasing either scale would make the weights
to non-neighbors larger than strictly necessary (thereby hurting the kernel graph relaxation). These
bounds, combined, specify a bounding box for each edge that must necessarily be crossed (or at
least touched) by the hyperbola since rij > 0.

Due to the hyperbolae, this amounts to a non-linear, non-convex set of constraints. However,
we can convexify the feasible set by considering, for each edge (i, j), the line(s) passing through

4That is assuming C

1 (a natural choice, as explained above). If for some reason one needs C > 1, then the

upper bounds must be scaled by C in order to ensure feasibility.

≤

27

 
A

uj

¾j

B

uj

¾j

v

v

C

¾j
uj

v

D

¾j
uj

0

0

ui

0

0

¾i

ui ¾i

0

0

ui

0

0

¾i

v

ui ¾i

k

xjk

xi −

Figure 15: Examples of constraints introduced by an edge eij in G. The C-connectivity rule, i.e.,
)2 (dashed curve), when convexiﬁed may give rise
the hyperbola given by σiσj = (C
to 1 or 2 linear constraints depending on whether the hyperbola’s vertex v (point where σi = σj)
intersects the bounding box given by the lines σi = 0, σj = 0, σi = ui, and σj = uj. Hatched
area (in orange) shows feasible region using convexiﬁed constraints; tangent line at v is shown in
gray. When v is interior to the bounding box (A), two secants (in blue) deﬁne the feasible region
(namely the lines passing through v and the points where the hyperbola intersects the lines σi = ui
and σj = uj); when either v = ui (B) or v = uj (C), only one secant is necessary; when v coincides
2) = uiul (which may occur if C is set to 1), again only
with both ui and uj (D), i.e., (C
one inequality is necessary, namely tangent line at v.

xi −

xjk

k

the hyperbola’s vertex (the point at which σi = σj = Crij) and the points where the hyperbola
intersects the bounding box (the four possibilities are shown in Fig. 15). The feasible region for
each edge, therefore, is bounded by a convex envelope given by such line(s) and those deﬁned by
the upper bounds to σi and σj. Such envelopes for all edges, combined, deﬁne the boundaries of a
convex polytope. Note that this convexiﬁcation is conservative in the sense that only the objective
is relaxed—the feasible scales are always at least as large as required by the original non-convex
problem, therefore our covering requirement is not relaxed. (Because of the presence of a later
pruning stage in the algorithm, it is better to over-connect here than to inadvertently disconnect
nodes that should otherwise be connected.)

Letting m

2

E

≤
of nodes in G, we deﬁne the m
its two possible constraints be expressed as

×

|

|

be the total number of linear constraints obtained as above and N the number
1 vector b. Now, for each edge eij, let

N matrix Λ and the m

×

ij σi + β(1)
α(1)
ij σi + β(2)
α(2)

ij

ij

σj ≥
σj ≥

(24)

(25)

with αij and βij denoting, respectively, the slope and intercept of the corresponding line(s) forming
βij for each line, which is encoded as
its convex envelope. Rearranging, we obtain αijσi −
a row in Λ with values αij and
1 at columns i and j, respectively (with zeros everywhere else)
and an entry in b with value

σj ≤ −

−
βij:

−

28

...
e(1)
ij
e(2)
ij
...

. . .

0

0









i

...
. . . α(1)
ij
. . . α(2)
ij
...

...

· · ·

· · ·
...

Λ

. . .

0

0

m

N

×

j

. . .

b

...
. . .

. . .
...

1
−
1
−

...
. . .

. . .
...

0

0









σ

N

1

×

≤









...
β(1)
ij
β(2)
ij
...

−

−

m

1

×


.







Remark 8: The convex envelope deﬁning the constraints can be expressed by the linear
inqualities:

Λσ
0 < σ

b
rFN

≤

≤

(26)

(27)

where rFN is the vector of distances to each node’s farthest neighbor.

Hence the problem now amounts to a convex, linear program with linear constraints:

LP Relaxation:

min
σ

1⊺σ

s.t. Λσ
0 < σ

b
rFN

≤

≤

which can be readily solved by a variety of methods (see, e.g., [21]). Figs. 19 , 21 , 20 show the
results of running this optimization on different examples.

3.5 Sparsiﬁcation

To recap: the Gabriel graph provides an initial estimate of neighbors; the LP optimization provides
minimal scales for a continuous kernel to cover these connections. However, since the initial
estimate of the discrete graph might contain incorrect connections, its resulting optimal scales
might also be inadequate. An example of this can be seen in Fig. 19: initially, two pairs of nodes
are connected across the central gap since a medial ball exists between them. This will cause very
large scales to be necessary to “cover” these edges. Furthermore, the Gabriel graph is based on a
local connectivity rule. However, as illustrated in Fig. 16, decisions about connecting nodes across
a gap should not be local. We here address both of these issues, by introducing a global statistics
based on how frequently such a gap occurs in the dataset. In terms of Algorithm 1 1, we are now
at steps 6 and 7.

29

Figure 16: Local vs. global assessment of neighborhoods. Left: The points inside the cropped win-
dow appear to form two well-deﬁned clusters when looked up close (local estimation). However,
when considered in the context of the full dataset (global estimation), the apparent gap between the
top and bottom groups “disappears”, i.e., it is well within the range of gaps observed throughout the
data. More precisely, it does not signiﬁcantly deviate from the average sampling interval. Right:
The converged graph G indeed connects the two groups by edges, and the distribution of degree
ratios, δ′i (lower inset), conﬁrms that all edges are reasonable up to 4 standard deviations from the
median.

3.5.1 Volume ratio

Because incorrect connections can be given by Gabriel balls lying in the free space between parts
of a manifold, i.e. across the reach, it is tempting to simply prune the longest connections. Note,
however, that the size of a scale by itself is not necessarily telling: in both examples shown in
Fig. 6, the non-uniform density causes scale sizes to vary, and some of the large ones are appro-
priate. That is, the largest scale is consistent with the distances to its neighbors. Conversely, and
importantly, large scales that are excessive will likely cover “too many” points. That is, the scale
will suggest neighbors in excess of the number of discrete neighbors to its corresponding node in
G. We quantify this notion by observing that the individual scale σi should produce kernel values
whose sum is comparable to the discrete degree di of node i in G. As we shall show, after proper
normalization this also means σi relates to a local volume element around xi, or the inverse of the
local density. Since each connection in G can be seen as having unit weight, a Gaussian kernel
around xi with scale σi should distribute that same amount, di, only continuously over ambient
space.

We start our derivation with a deﬁnition, for non-isolated nodes xi:

Deﬁnition: let w(σi)
iteration t. A (non-isolated) node’s volume ratio at iteration t, δ(t)
i

be the Gaussian kernel value between xi and xj using scale σt

, is

ij

i at

i.e., the ratio between node i’s weighted degree due to σ(t)
i

and its discrete degree in G(t). (Hence-

,

(28)

i

V w(σt
ij )
V aij
∈

j

δ(t)
i ≡ P
P

∈

j

30

forth we suppress the iteration dependency (t) to simplify notation.) Here an individual-scale
Gaussian kernel is needed in order to correctly assess the impact of σi on the relaxation from
the perspective of i alone—a multi-scale kernel may artiﬁcially increase the weighted degree of i
when other nodes (even non-neighbors of i) have incorrect scales. (However, as discussed below,
a corresponding ratio using the actual weights in

may be used for convergence purposes.)

G
Now, using a mean-value integral (as in [31]), the numerator approximates the volume under

the continuous Gaussian kernel over

, and can be further approximated by

M

w(σi)

ij =

j
X

j
X

w(σi)

ij ≈

N
vol(

M

)

ZM

exp

−k

(cid:18)

2

xjk

xi −
σ2
i

dxj

(cid:19)

(29)

M

has uniform density and low curvature. In practice, the kernel will have compact support
when
due to numerical precision (i.e., its values become effectively zero for sufﬁciently large distances),
of a continuous neighborhood
so by deﬁning the volume element dVi ≡
N

vol(
around xi, we may rewrite eq. 29 as

(xi))/

∈ M

(xi)

(xi)

|N

N

|

w(σi)
ij dVi ≈

exp

−k

ZM

(cid:18)

2

xjk

xi −
σ2
i

dxj

(cid:19)

j
X

(30)

when the sampling is approximately locally uniform. By further assuming that σi is small, and that

can be well-approximated locally by its tangent space Rd, then

exp

−k

ZM

(cid:18)

2

xjk

xi −
σ2
i

dxj ≈

(cid:19)

Rd

Z

exp

−k

(cid:18)

2

xjk

xi −
σ2
i

(cid:19)

dxj = (√πσi)d,

(31)

M

so

w(σi)
ij dVi ≈

(√πσi)d,

j
X

(32)

as shown in Fig. 17.

An analogous derivation for the discrete degree summation is as follows. First, note that the
edge weight now is a constant = 1; it remains to determine its support over
. From section 3.3.3,
we know that, for simple manifolds with random sampling, the node degree deg(i) in a Gabriel
graph is approximately 2di within a region of constant intrinsic dimensionality (where di denotes
)5. In more
the local intrinsic dimensionality around xi (possibly different around other points in
general manifolds, we expect the converged graph G⋆ instead to approach such a property. This
2di will approximate the volume of a hyperrectangle (or box) of height = 1 and
means
having a di-dimensional hypercube of side 2 as its base6. So, deﬁning ρi as the radius of the local
volume element dVi (such that ρi = √dVi

d), then we may write:

j aij ≈

M

P

X

aijdVi ≈

ρi

ρi

ρi · · ·

Z

−

ρi

Z

−

j
X

1dxj1 . . . dxjd = (2ρi)di,

(33)

Rd”.

5We abuse notation, therefore, when referring to a “d-dimensional manifold”, or when using the expression “

M ∈
6This agrees with our observation (section 3.3.3) that the unoccluded region around xi is similar to a d-orthoplex:
by placing a vertex (i.e., a neighbor) in each of its 2d facets we obtain a d-hypercube, which is the dual polytope of an
orthoplex.

31

as illustrated in Fig. 17. Hence ρi is a kind of “neighborhood radius” of node i.

From eqs. 32 and 33, eq. 28 becomes

j w(σi)
ij
j aij

P

=

j w(σi)
ij dVi
j aij dVi ≈

P

di

,

√πσi
2ρi (cid:19)

(cid:18)

(34)

representing the ratio between the volume of a Gaussian with scale σi and that of a box of side 2ρi
P
ρi (scales are
and height 1 (Fig. 17). As the algorithm approaches convergence, we expect σi ≈
compatible with neighborhood radius) and deg(i) should approach the empirically-observed value
2di (number of neighbors in G is compatible with dimensionality of
), resulting in

P

M

j w(σi)
ij
j aij ≈

P

di

.

√π
2

(cid:18)

(cid:19)

Finally, we can estimate d as

P
˜di ≡

log2

aij,

j
X

(35)

(36)

based on the empirical distribution of node degrees in G(t). From this, we can compute a normal-
(t)
ized volume ratio, δ′
i

i by the value from eq. 35:

, by dividing δ(t)

j w(σi)
j aij (cid:18)
Nodes whose degree deviate from exactly 2di will, likewise, under- or overestimate the local di-
mensionality, so reasonable volume estimates are still obtained regardless. However, in order to
avoid dimensionality less than 1 for connected nodes, in practice when deg(i) = 1 we replace

δ′
i ≡ P
P

2
√π

(37)

(cid:19)

(t)

ij

.

˜di

2,

j aij with max
Thus, we expect δ′i ≈

P

{

j aij}

.
1 for points obeying σi ≈

di. Crucially, points for which
P
these conditions are not met (i.e., those having “wrong” neighbors in the original Gabriel graph
G(0)) will depart from this by having δ′i > 1. In the next section, we shall use this quantity to guide
a sparsiﬁcation of edges in G(0).

ρi and ˜di ≈

Interestingly, δ′i can also be interpreted as measuring how well the scale σi ﬁts the local volume
element dVi (or, equivalently, how it counteracts the local sampling density, 1/dVi). Since dVi = ρd
i
(from the deﬁnition of ρi), we may rewrite eq. 34 as:

Finally, when ˜di ≈

P
P
di and relaxing the assumption σi ≈
Remark 9: A node’s normalized volume ratio at iteration t, is

j w(σi)
ij
j aij ≈

(√πσi)di
2didVi

.

(38)

ρi, and summarizing the above we have:

ij

j w(σi)
j aij (cid:18)

2
√π

˜di

≈

(cid:19)

(√πσi)di
2didVi (cid:18)

2
√π

˜di

(cid:19)

σdi
i
dVi

≈

(39)

(t)

δ′
i ≡ P
P

Therefore, δ′i can be thought of as the product between kernel scale and local density. When σi is
optimal, it should be approximately equal to the inverse of the local density, so δ′i ≈

1.

32

Figure 17: Computing the volume ratio between continuous and discrete degrees of a node i with
. Top row: Using a Gaus-
neighboring points sampled uniformly over a d-dimensional manifold
sian kernel, the weighted degree of i (sum of kernel values
approximates the volume
) in
of a Gaussian with scale σi when sampling is locally uniform (eq. 32. Bottom row: The number of
edges adjacent to i in G (sum of unit weights) approximates the volume of a box with unit height
and a hypercube of side 2ρi as its base, where ρi is the radius of a local volume element of
M
around xi (eq. 33. Right: When the scale σi is compatible with ρi, the volume ratio, δi, is expected
to be approximately √π/2, therefore it is a dimensionless and scale-invariant quantity.

j w(σi)

M
G

P

ij

3.5.2 Uniformity of sampling and edge pruning

(t)
is evaluated for every node xi, we can collect it across nodes and view it as a statistic.
Since δ′
i
This has two consequences: (i) it can be used to enforce consistency in sampling and (ii) outliers
in this statistic are likely candidates for edge pruning. We address consistency of sampling ﬁrst.

We have several times stated that sampling is required to be locally uniform, although the rate
may change over the manifold. Examples of this were shown in e.g. Fig. 11, where the sampling
was denser in the center of the Gaussian than in the periphery. This example differs from the
tesselations, in which all nearest neighbors had exactly the same distance. Putting this together,
we have

Remark 10: Locally Uniform Sampling Let a node i have ki neighbors in G(t). Among
denote the distance from i to its furthest neighbor, and rNN
these, let rFN
to its nearest
i
neighbor. When rFN
for all points xi in the dataset, we say the sampling is
locally uniform.

i ≈

rNN
i

i

This is useful because a departure from the assumption that sampling is locally uniform will
cause δ′i to be on average greater than 1 throughout the dataset. To see this, when sampling is not
uniform, we have rFN
i > rNN
. Now, since σi is optimized to cover all of i’s neighbors, it will have
i
in most cases the same order of magnitude as rFN
(minus some possible slack due to the multi-
i
scale interaction). Therefore, the higher the variability in the neighbors’ distances, the larger the
difference between rFN
i making, in turn, σi larger than the distance to most neighbors of i.

and rNN

i

33

Ultimately this will increase
(in which rFN

).

rNN
i

i ≈

P

j w(σi)

ij beyond what we would have in a uniform-sampling scenario

When data are acquired using a global sampling strategy, this variability in the neighbors’
distances should be roughly constant throughout the dataset (rather than the distances). So we use
the scalar parameter C from eq. 19 to correct for this “bias” and bring the median of the distribution
(t)
of δ′
i

) close to 17.

(denoted as

(t)
δ′
i

h

i

Remark 11: Let the tuned C ⋆(t) be that which causes

(t)
δ′
i

h

i

to be closest to 1.

Typically, C (t) < 1, which, in the scale optimization procedure, means the covering constraints
(eq. 22) are being relaxed using the distribution of δ′i as a guide (Fig. 18). (Although the tuning of
C is not necessary for ﬁnding candidates for sparsiﬁcation, it attributes a quantitative meaning to
the value of δ′i, so any δ′i ≫
Such tuning should be performed at t = 0, and repeated as needed over the iterations whenever
(t)
deviates too much from unity (which may happen after several edges have been pruned).
δ′
i
h
Most commonly, we ﬁnd 0.5 < C ⋆(t) < 1.

1 is guaranteed to indicate the need for edge pruning.)

i

G⋆

C ⋆ = 0:915

G ⋆

®
0i
±

1.2

1.1

1.0

0.9

0.8

0.7

0.70

0.75

0.80

0.85
C

0.90

0.95

1.00

C = 0:800

C = 0:915

C = 1:000

t
n
u
o
c

e
d
o
N

20

10

0

20

10

0

20

10

0

0.5

1.0
± 0

i

1.5

0.5

1.5

1.0
± 0

i

0.5

1.0

1.5

± 0

i

h

δ′ii

Figure 18: Tuning the hyperparameter C based on the median of the distribution of normalized
. Left: Converged unweighted graph G⋆ obtained for the dataset from Fig. 19.
volume ratios,
1, the optimal C ⋆ is that resulting in a
Center: After computing
closest to 1. Histograms below show distribution of δ′i for different values of C, including C ⋆
δ′ii
h
⋆ after C-tuning typically exhibits a more uniform
= 0.915. Right: The resulting weighted graph
connectivity throughout (see Fig. 13).

for a range of values of C

δ′ii

≤

G

h

Thus we have a data-driven way of ﬁnding an appropriate value for C. Because it is a global
constant applied to all connection constraints, it shifts the distribution of δ′i to a median around 1

7Although the mean typically gives smoother tuning curves (see Fig. 18), the median is more robust. This matters,

becasue of the possible outlying δ′

i values.

34

 
without changing its general shape. This leads us to the second use of the δ′ statistic: a node whose
variability of neighbor distances is much greater than the median (i.e., has a farthest neighbor
considerably more distant than their median neighbor) will still have
1, but will have
signiﬁcant outliers. It is these outliers that are candidates for having edges that need to be pruned
in the sparsiﬁcation step.

(t)
δ′
i

i ≫

h

Remark 12: Nodes that are robust outliers in the δ statistic have an overly distant
neighbor, relative to the other neighbors for that node, and hence are likely to be in
violation of reach or other geometric constraints. These relatively distant neighbors
are candidates for pruning.

Given the distribution of normalized volume ratios, robust statistics suggest models for spread
and outliers. We take this to deﬁne a threshold, and nodes with δ′i above such threshold should
have their connection to farthest neighbors deleted. Ideally, only one such connection should be
pruned after each iteration; however, should that become impractical with datasets, a compromise
is to limit the pruning, at each iteration, to a single edge from each node that is above the threshold.
It is possible that datasets with a large number of problematic connections will exhibit a heavy
tail or may look like a mixture of two distributions (cf. example in Fig. 22), so using the distribu-
tion’s quartiles may give a more robust result. One option that seems to work particularly well is
to use estimates of the mean and standard deviation from the quartiles, as in [105] – throughout,
we make use of the C3 method derived therein. Finally, we note that our algorithm can be run
interactively, so the user can analyze the histogram of the distribution after each iteration to judge
whether the choice of threshold is reasonable and thus be conﬁdent in the results.

3.5.3 Convergence

(t)
The algorithm converges at iteration t when no point xi has an outlying δ′
(i.e., greater than a
i
statistical threshold). This implies that no edges will be pruned, so G(t+1) = G(t) and therefore
no further changes can occur to either σ(t) or
(t). Note that convergence is guaranteed—since at
every iteration t an edge is removed, the algorithm must necessarily reach a certain t at which all
outliers (if there were any to begin with) have been pruned.

G

If one is solely interested in obtaining

⋆ (i.e., not interested in G⋆), an alternative convergence
condition may be adopted that looks at the distribution of the (normalized) multi-scale volume
ratio, δ′

G

(t)
iMS:

˜di

j wij
j aij (cid:18)

2
√π

(t)

δ′
iMS ≡ P
(t) directly. Since the multi-scale kernel values
analogous to eq. 37 but using the weights from
P
G
take into account the interaction between individual scales, the distribution of δ′iMS will be typically
tighter than that of δ′i (i.e., some of the excessively large scales might be compensated by small
neighboring scales). Therefore, one may wish to allow for an earlier convergence when there are
no remaining outliers in the distribution of δ′iMS.

(cid:19)

(40)

,

In closing this section, we return to the introductory examples and show the graphs for the sam-
pling Swiss cheese patterns (Fig. 2) in Fig. 23. Finally, should it be required that G⋆ be connected,

35

Volume ratios; t = 0

Individual scales; t = 0

Edge sparsification; t = 0 ¡ 1

¾(0)

G (1)

0.0

2.5

5.0

7.5

10.0

12.5

15.0

17.5

(0)

± 0
i

Volume ratios; t = 2

Individual scales; t = 2
¾⋆

Converged

G ⋆

t
n
u
o
c

e
d
o
N

t
n
u
o
c

e
d
o
N

25

20

15

10

5

0

25

20

15

10

5

0

0.50

0.75

1.00

1.25
(2)

± 0
i

1.50

1.75

2.00

Figure 19: Optimal scales and associated normalized volume ratios at iterations 0 and 2 of Algo-
rithm 1 on the horseshoe dataset (see Fig. 13). Top row: the δi statistic has median around 1 and
several outliers. These are caused by the long edge (and huge scales) (middle). Right column: G(t)
after iteration 1, with edges deleted so far shown in red (top), and after iteration 2 (bottom).

pruning can be stopped at that point. Naturally,
or some numerical tolerance.

G

⋆ is always connected up to machine precision,

3.6 Comparison with other kernel methods

Here we compare the data graphs obtained using our method (IAN) with those from other popular
manifold learning methods. To start we introduce a synthetic “stingray” dataset that exhibits a
transition of apparent dimensionality from 2 (body) to 1 (tail), cf. Fig. 4. Points were uniformly
sampled with 20% deleted at random.

Our unweighted graph G⋆ can be compared with the k-NN graph (bottom row in Fig. 24), used
in a variety of methods including Isomap[96], which will be further examined in section 4. When
k = 2, the tail exhibits perfect connectivity, but the body is too sparse. If k = 4, the body is more
properly connected but the tail is overly connected, and “short-circuits” start to appear. Finally, for
k

8 the connectivity is inappropriate as the tip of the tail connects directly to the body.
In contrast, G⋆ manages to retain a minimally-connected tail while covering the body almost
everywhere, creating edges across many of the sampling gaps (compare with the holes that remain
in the k-NN graph with k = 4, some of which are present even when k = 8).

≥

Our weighted graph

⋆ can be compared against methods that use a Gaussian-like kernel, and
where each point has an individual scale. These methods where described in Sec. 2.1: t-SNE [101],

G

36

Volume ratios; t = 0

Individual scales; t = 0

Edge sparsification; t = 0 ¡ 2

¾(0)

G (2)

0

5

15

20

10

(0)

± 0
i

Volume ratios; t = 3

Individual scales; t = 3

Edge sparsification; t = 3 ¡ 5

¾ (cid:7)(cid:8)(cid:9)

G (5)

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

± 0
i

((cid:5)(cid:6)

Volume ratios; t = 6

Individual scales; t = 6
¾⋆

Converged

G ⋆

t
n
u
o
c

e
d
o
N

t
n
u
o
c

e
d
o
N

t
n
u
o
c

e
d
o
N

40

35

30

25

20

15

10

5

0

40

30

20

10

0

35

30

25

20

15

10

5

0

0.6

0.8

1.0

1.2

1.4

1.6

(6)

± 0
i

Figure 20: Optimal scales and associated normalized volume ratios and at different iterations of
Algorithm 1 on the “limbs” dataset from Fig. 6. The distribution of δ′i (left) indicates those connec-
tions that are least likely to represent (reasonable) geodesics over the underlying manifold. Right
column shows G(t) after iterations 2, 5 and 6 (deleted edges in red).

UMAP [78], self-tuning[111], and variable-bandwidth[16, 17]; their resulting connectivity can be
visualized in Fig. 24, where edges have transparency proportional to their weight.

We also plot the respective individual scales resultant from each method (Fig. 25). They are
represented, around each point i, as level set corresponding to a kernel value of 0.75. For the
asymmetric kernels of t-SNE and UMAP this is straightforward; for the multi-scale kernels (self-
tuning, variable-bandwidth, and our own), we replace the product σiσj by σ2
i .

At the top of both Fig. 25, we see that the scales found by our method seem to nicely conform
to the space between each point and its neighbors. Especially illuminating is what happens along

37

Volume ratios; t = 0

Individual scales; t = 0

Edge sparsification; t = 0 ¡ 2

(0)

G (2)

0.0

2.5

5.0

7.5

10.0 12.5 15.0 17.5
(0)

± 0
i

Volume ratios; t = 3

Individual scales; t = 3

Edge sparsification; t = 3 ¡ 6

(3)

G (6)

0

2

4

6

10

12

14

8
(3)

± 0
i

Volume ratios; t = 7

Individual scales; t = 7

Converged

G ⋆

t
n
u
o
c

e
d
o
N

t
n
u
o
c

e
d
o
N

t
n
u
o
c

e
d
o
N

40

30

20

10

0

40

30

20

10

0

40

30

20

10

0

0.5

1.0

1.5

2.5

3.0

3.5

2.0

± 0
i

(7)

Figure 21: Optimal scales and associated volume ratio statistics at different iterations of Algo-
rithm 1 on the clustered dataset from Fig. 6. Pruned edges (in red) are precisely those connecting
the three clusters.

the tail, where scales either “expand” or “shrink” so as to minimally cover the spaces between
neighboring points, thus illustrating what our scale optimization achieves.

Among the other methods, with few exceptions the scales seem to cover either too much (col-
lapsing the tail on itself) or too little (leaving holes in the body). The weighted graphs in Fig. 24
reveal the result of the interaction between these individual scales (namely, the edge weights). Our
⋆ (top right) manages to cover almost the entire body with edges, while keeping the tail mini-
G
mally connected—in fact, resembling the unweighted version in G⋆, and therefore respecting the
original curvature and reach. Other methods, in contrast, have a hard time achieving both things
with a single value for k. In t-SNE, the scales over the body are much too small when k
4; so its
8 the scales over the tail become too large, and therefore
weighted graph look too sparse; for k

≤

≥

38

Figure 22: Optimal scales and associated normalized volume ratios δ′i after each iteration of the
algorithm on the bent plane dataset used in Fig. 28 (here, seen from a lateral view). The number
of initial connections in G(0) (Gabriel graph) is very large, so initial distribution of δ′i shows two
modes. However, ratios in right-side peak are very high, and are therefore easily identiﬁed as
outliers when compared to the mean. The algorithm terminates soon after all edges crossing the
gap are eliminated.

strong edges appear connecting it to the body. In UMAP, the scales do not grow as much with
increasing k, but at k = 4 the body in the weighted graph is still too sparse, while for k
8 the
tail is strongly connected to the body. With self-tuning, scales seem to grow faster with k, while
with variable-bandwidth this growth is somewhat counteracted by the action of their global scale ǫ
⋆ is the one using the variable-bandwidth
(eq. 7). In fact, the graph that most resembles our own
kernel with k = 2, the main difference being that the big sampling gap near the tip of the body
is poorly connected, while in our case it is slightly overly connected (due to connections in G⋆
crossing that gap).

≥

G

39

Figure 23: Sampled Swiss cheese results (cf. Fig. 2). The original sampled points (holes outlined)
are shown, together with the converged graphs. In the sparse case the sampling is locally uniform,
and no holes are visible, while in the dense case no holes are violated.

40

⋆; pruned edges are shown
Figure 24: Top: The stingray dataset and its converged graphs G⋆ and
in red. Bottom: Other algorithms produce qualitatively different graphs depending on the neigh-
borhood size parameter, k. All graphs shown are weighted (using a continuous kernel) except for
the k-nearest-neighbors graph (bottom row). Edge weights are visualized as the intensity of the line
segments (each wij is divided by the kernel value when rij equals the scale, for a fair comparison
across algorithms).

G

41

Figure 25: Individual scales obtained using our algorithm (top) compared to other methods (bottom
table), as represented by their level sets for a (single-scale) kernel value of 0.75.

42

4 Applications

We now provide examples of application of our kernel to three different manifold learning tasks:
dimensionality reduction, by means of non-linear embedding algorithms; geodesic estimation,
which typically ﬁnds application in computational geometry, vision, and graphics; and local in-
trinsic dimensionality estimation.

4.1 Low-dimensional embeddings

Dimensionality reduction is now ubiquitous in visualization of high-dimensional data. Several
methods exist[93, 102, 70, 51], and most of the non-linear methods are manifold-based[88, 96,
78, 58, 101, 89, 112, 10, 28, 37, 109, 81, 95]: given a collection of points in high-dimensional
, the goal is to ﬁnd a good parametrization
space sampled from a low-dimensional manifold
for the data in terms of intrinsic coordinates over
, which in turn can be used to produce a
low-dimensional embedding.

M

M

In surveying the literature, it is common to ﬁnd a heuristic or range of values suggested for
choosing the neighborhood radius or size, but rarely do we see examples of the sensitivity of the
results to that choice. In this section, we ran a few of the most popular methods using a wide range
of values for the kernel scale parameter k, and furthermore compared their results when using our
own kernel.

We have limited our comparison to embedding methods that use a neighborhood kernel and
for which pairwise information is sufﬁcient as input (i.e., do not require positional information):
diffusion maps[28, 29, 30], Isomap[96], t-SNE[101], and UMAP[78]. As shown in Figs. 26–28,
results can vary qualitatively depending on the choice of the scale parameter k. Five values of k
were tested on three synthetic datasets, spanning a wide range of scales and different geometries.

4.1.1 Diffusion maps + self-tuning kernel

Diffusion maps are based on the spectral properties of the random walk matrix (normalized graph
Laplacian) over the weighted data graph; integration over all paths in the graph makes diffusion
distances in principle more robust to “short-circuiting” than graph geodesics. For better compari-
son with our own kernel, instead of the standard single-scale Gaussian kernel we use the self-tuning
⋆ as the
approach of [111] (eq. 6). Our kernel was applied to diffusion maps by directly using
similarity matrix (weighted adjacency matrix), and by setting the additional parameters α = 1 and
t = 1.

G

With the stingray dataset (Fig. 26, we see that the fully-extended tail at k = 2 becomes pro-
gressively more folded and compressed as k increases. The body is overly contracted at k = 2 but
⋆, although we obtain excellent embeddings of both body
expands with larger k. Using our own
and tail (right-most column), they are represented by separate sets of coordinates (two for the body,
and a third for the tail), which happens due to the change in dimensionality. The plot shows a 2-D
projection of the 3-D embedding.

G

When applied to the spiral dataset (Fig. 26), only k = 2 and 4 were able to prevent folding.
The bent plane (Fig. 28) was more tolerant, with good results for all k except 64, for which the
plane remained folded. When using our own kernel, a good parametrization was obtained for both
datasets.

43

Figure 26: Running different embedding algorithms on the “stingray” dataset (see Fig. 24). Dif-
ferent choices of the neighborhood size, k, may produce qualitatively different results, depending
on the algorithm. Running those same algorithms using our IAN kernel (right) typically gives a
reasonable result. Refer to main text for details.

4.1.2 Variable bandwidth diffusion embedding

We also tested a variant of diffusion maps using the variable bandwidth kernel of [16, 17], in which
a distinct type of multi-scale kernel is proposed along with a speciﬁc normalization of the weighted
graph Laplacian. Because it computes an additional global scale ǫ based on the individual scales,
in order to apply our algorithm to this method we replaced the density estimates qǫ (eq. 7) with the
inverse of our optimal scales. We used α = 0 and β = -1/2, cf. recommended in [17]; eigenvectors
were scaled by the square-root of the inverse of their respective eigenvalues [91, 85], following the
implementation in [8].

This method produced good embeddings for the stingray, especially for k = 8 (Fig. 26). For the
spiral (Fig. 27), using k
8 caused some points to drift apart, and although it returned basically
the original curve when k = 16 or 32, a spectral algorithm is expected to “unroll” the spiral, ﬁnding
a good (1-D) parametrization of it.

≤

The same happened with the bent plane (Fig. 28), which could not be embedded into 2 coor-
dinates for any choice of k. Using our scales, however, the algorithm managed to ﬁnd appropriate
parametrizations for all three datasets.

44

4.1.3 Isomap

Isomap applies classical multidimensional scaling (MDS) to geodesic distances computed as short-
est paths over a k-nearest neighbors graph (eq. 2). Because the graph is unweighted, this method
is particularly sensitive to the choice of k. Our kernel applied to Isomap by directly replacing the
k-nearest neighbors graph with G⋆.

With the stingray (Fig. 26), Isomap produced a good embedding with k = 4. The result with
k = 2 was completely wrong (an additional tail appears), and with k = 8 the tip of the tail is dis-
connected. With k = 16 and 32, it essentially returns the original data, without any dimensionality
reduction. Using our G⋆, the result of k = 4 was improved by making the points in the body more
uniformly spread.

The spiral (Fig. 27) is properly embedded (1-dimensional) only when k is 2 or 4, while with the
bent plane (Fig. 28) a good result is obtained for k between 4 and 16 (k = 2 produces 1-dimensional
curves and k = 64 distorts the unfolding into 2-D). Using our G⋆ produces the correct mapping in
either case.

4.1.4 t-SNE and UMAP

t-SNE and UMAP are related methods that have gained popularity in recent years[9]. Both com-
pute similarities between data points by using individual scales based on log2 k (section 2.1), and
adopt a second kernel to compute similarities between embedded points—t-SNE uses a Student
t-distribution (Cauchy kernel); UMAP uses an non-normalized variant requiring a hyperparame-
ter, min dist. In t-SNE, embedding coordinates at initialized at random, while UMAP adopts
the strategy of reﬁning an initial spectral embedding. Both then optimize their embeddings by
running gradient descent on an information-theoretic cost function between similarities in input
space and embedded space: t-SNE minimizes the KL-divergence, while UMAP uses a variant of
cross-entropy. Alternative initializations are typically used with t-SNE (e.g., PCA) to improve
results[64, 65, 73]—in our experiments, for better comparison with UMAP, we used a spectral
embedding initialization computed from its own symmetrized similarity matrix (eq. 10).

Our kernel was applied to t-SNE by replacing the individual scales (eq. 8) with those in σ⋆;
with UMAP, because a different kernel function is used, we directly replaced the weighted graph
⋆.
(with adjacency given by U in eq. 12) with our
t-SNE was executed for 3000 iterations assigning the various k values to the perplexity pa-
rameter, leaving the remaining parameters to their defaults in the scikit-learn implementation[86]
(Barnes-Hut method[100] for cylinder dataset; ‘exact’ method for all others); in UMAP, the
n neighbors parameter was set to k, with remaining parameters using default values (in partic-
ular, min dist = 0.1). Because of the stochastic nature of both algorithms (even when using a ﬁxed
initialization), different runs will produce slightly different results—in order to avoid “cherry-
picking” (unrealistic in a truly unsupervised scenario), both algorithms were executed a single
time, using the same random seed.

G

Results for the stingray (Fig. 26) were quite analogous between the two algorithms: both pro-
duced artiﬁcial clustering for k
16 the tail began to fuse with the body. The
gaps in sampling within the body were accentuated by both algorithm, even at k = 32, where we
see a big hole in the UMAP embedding, and in t-SNE it almost breaks into two pieces (despite the
large neighborhood size). This example is illustrative of how much an embedding algorithm based

8, while for k

≥

≤

45

on attractive vs. repulsive forces can end up exaggerating nonuniform sampling.

The spiral (Fig. 27) was broken into pieces by t-SNE for all values of k except 8. UMAP
produced reasonable results when k = 4 or 8; however, when k = 2 a multitude of clusters was
16 the curve twisted over itself. Using our kernel (right-most column)
obtained, and when k
produced a connected, non-self-intersecting curve. Neither algorithm was capable of returning a
good arc-length parametrization of the spiral, however.

≥

With the bent plane (Fig. 28), although both algorithms succeeded in unfolding it, t-SNE was
only able to produce a fully two-dimensional plane (with no gaps) when setting k = 32 (not shown)
or 64, while UMAP required k

16. Both gave reasonable results using our kernel.

≥

4.1.5 A higher dimensional example

2, we also tested our kernel when applied to a higher
Because all of the examples above have d
dimensional manifold, namely a 5-dimensional cylinder (R1
S4) with radius = 1 and length =
3 sampled uniformly at random (N = 8403, ambient space R6). On the other hand, to simplify
interpretation we used a pure, connected manifold with no bottlenecks and low curvature.

≤

×

Fig. 29 shows two-dimensional embeddings obtained using our kernel with different algo-
rithms. Although all correctly produced an oblong, various degrees of mixing of the original
color labels were observed, and can be used to qualitatively indicate the quality of the embedding
schemes. A quantitative assessment was computed as the rank correlation coefﬁcient (Kendall’s
τ )[61, 63] between the ranking (positional order) of each point along the main axis in the original
vs. embedded space.

Use of our IAN kernel gave similar or better results with both t-SNE and UMAP (k = 27
was set as the mean degree in G⋆, compatible with d = 5). Both, despite their current popularity
(e.g., [9, 65, 108, 64, 27, 47, 6, 36, 107]) produced considerably jittered outputs, implying that the
original neighborhoods were not preserved. This appears to be caused by an attempt to reproduce
the spherical shape of the cylinder’s base along the main axis, to different “slices” are projected on
top of one another. However, UMAP produced nearly identical results even when the set to return
the 6 components (as in the original space).

Diffusion maps using IAN resulted in very little mixing except near the boundaries, so neigh-
borhoods were better preserved. Running it with either of the kernels discussed above using k = 27
gave comparable results (not shown). Isomap also produced excellent results, with τ = 0.98 (not
shown); although, because there is no ambient noise added to the data nor bottlenecks, choosing k
large would trivially produce perfect results.

46

Figure 27: Running different embedding algorithms on the spiral dataset (top), in which points
are sampled from a unit-speed parametrized Archimedean spiral. Different choices of the neigh-
borhood size, k, may produce qualitatively different results, depending on the algorithm. Running
those same algorithms using our IAN kernel (right) typically gives a reasonable result. Refer to
main text for details.

47

Figure 28: Running different embedding algorithms on the “bent plane” dataset (top), generated
by extending a unit-speed parametrized catenary curve into two dimensions. Different choices of
the neighborhood size, k, may produce qualitatively different results, depending on the algorithm.
Running those same algorithms using our IAN kernel (right) typically gives a reasonable result.
Refer to main text for details.

48

×

Figure 29: Performance of different embedding algorithms applied to a 5-dimensional cylinder
S4) sampled uniformly at random (N = 8403, ambient space R6). Top left: original data,
(R1
,
X
projected onto ﬁrst 2 coordinates (points colored according to their position along the cylinder’s
long axis). Other plots show embeddings using different kernels and/or algorithms. The resulting
degree of mixing of the original color labels indicates the quality of the embedding scheme. A
quantitative assessment (plots to the left of each embedding) was computed as the rank correlation
coefﬁcient, τ (see main text), between the ranking (positional order) of each point along the hor-
izontal axis in the original vs. embedded space (a value closer to 1 indicates fewer exchanges in
the original order). Use of our IAN kernel produced similar or better results with both t-SNE and
UMAP (k = 27 was set as the mean degree in G⋆, compatible with d = 5). Diffusion maps resulted
in very little mixing except near the boundaries.

49

4.2 Geodesic computation

From G⋆, we can immediately compute graph geodesics (shortest paths using distances in ambient
space as edge lengths) as an estimate of the true geodesics over
[96]. The latter are likely to
be underestimated when sampling is sparse[15], even when the graph connectivity is correct, e.g.,
due to curvature (see section 3.3.2), or when the neighborhood sizes as small (as in G⋆, due to
the minimality of neighborhood sizes obtained from the Gabriel connectivity rule, or in a k-NN
graph with a tightly ﬁt k). It seems a good idea then to incorporate the continuous kernel values
⋆, as means to improve geodesic estimation. Computing graph
present in its weighted counterpart,
⋆, of course, cannot work, since it is a complete graph up to a numerical
geodesics directly on
precision (or an arbitrary tolerance set for the weight values).

M

G

G

Instead, we propose to use the heat method for geodesic computation of [33]. It consists in
solving the Poisson equation to ﬁnd a function φ whose gradient follows a unit vector ﬁeld X
pointing along geodesics; X can be obtained by normalizing the temperature gradient
u due to
a diffusion process in which heat u is allowed to diffuse for a short time. Although this method is
tailored for applications where positional information and dimensionality are known (in particular,
surfaces in R3), here we apply it to
⋆, since discrete versions of the operators used (Laplacian,
G
gradient, and divergence) can be readily deﬁned on a weighted graph[35].

∇

Despite being restricted to pairwise information, our method produces reasonable estimates,
as shown in Figs. 30,31. To understand why, notice that the IAN algorithm indirectly solves for a
weighted graph for which a random walk starting at node i has a higher probability of reaching a
(i) than any other non-neighboring node. Given that random walks are closely related to
node in
⋆ to be able to provide reasonable information about how
diffusion over a graph, we should expect
⋆ should
a diffusion process propagates over
be a good approximation of a continuous operator over
; this is empirically conﬁrmed by our
results.

G
. In other words, the Laplacian obtained from

M

M

N

G

In Fig. 30, heat geodesics computed from

⋆ for the bent plane dataset approximate well the
, and graph geodesics obtained from G⋆ follow closely. Comparison with
true geodesics over
those from a naive k-NN graph illustrates that the choice of k is critical (compare with bottom row
of Fig. 28).

M

G

In Fig. 31, we compare the results using weighted graphs from various kernels on the stingray
⋆ hold reasonably well even when facing a

dataset; interestingly, heat geodesics computed from
continuous change in dimensionality.

G

50

Figure 30: Geodesic estimation for the bent plane from Fig. 28; yellow points are closer to
the source (marked with an arrow in the ground truth plot). Top: different views of the bent
⋆. Middle:
plane in 3-D, with points colored according to the heat geodesics computed from
Geodesics displayed on an unbent version of the dataset: heat geodesics approximate well the
, and graph geodesics computed from G⋆ follow closely. Bottom: graph
true geodesics over
geodesics computed from k-NN graphs using different choices of k; choosing k = 16 gives near-
perfect results, but k = 4 shows distortions and k = 64 misses completely.

M

G

51

⋆ yields results close to the
Figure 31: Geodesic estimation using the heat method applied to
true geodesics (top). Other kernels yield suboptimal results for most choices of k (bottom); in
particular, notice how the tip of the tail is usually inferred to be closer than it should (due to its
being directly connected to the body in the underlying graph, cf. Fig. 24). Yellow points are closer
to the source (marked with an arrow in the ground truth plot).

G

52

4.3 Local dimensionality estimation

Intrinsic dimensionality (ID) estimation is tightly associated with dimensionality reduction tasks,
especially in manifold learning, where knowledge of d can help, among others, to determine the
appropriate number of embedding dimensions.

There are many different ways to estimate it[24, 25]. Global approaches are typically divided
into two. The ﬁrst group are based on some variant of PCA (e.g., [48, 74]) and use the number of
signiﬁcant eigenvalues to infer dimensionality, and can be applied globally or by combining local
estimates. The second group of methods, termed geometric (or fractal, when a non-integer ID is
computed), exploit the geometric relationships in the data such as neighboring distances. Some
are based on estimating packing numbers[59] or on distances to nearest-neighbors [104, 87, 99,
40, 18, 32]. Among the most popular are the correlation dimension methods [52, 26, 56], a variant
of which has been speciﬁcally applied in the context of determining an appropriate kernel width
for manifold learning[31, 16, 53]. The dimension is computed as the slope of a log-log plot of the
number of neighboring points vs. neighborhood radius (see section 2.1). A recent variation is [62].
Others cover the difﬁcult case of high ID [26, 90].

In our scenario, since we do not assume a pure manifold (section 3.1), we focus on local
(i.e., pointwise) ID estimation approaches, namely those in which dimension is estimated within a
neighborhood around each data point (e.g., [41, 55]). This notion can be formalized as the local
Hausdorff dimension[110, 25], and a global estimate is typically found by averaging over local
values.

A popular approach is Levina and Bickel’s maximum likelihood estimator (MLE)[71], which

computes the local dimensionality based on k-nearest neighbors[71]:

ˆmk(xi) =

1

−

1

k

k

j=1
X

log

Tk(xi)
Tj(xi) !

(41)

where Tj(x) denotes the distance between x and its jth nearest neighbor. We shall use this method
in our experiments, in which we compute a ﬁnal mk(xi) by averaging ˆmk(xi) over i’s neighbors in
order to reduce the variance of the local estimates (in the original, this is done over all data points).
Notice that our kernel can be readily used with this method by simply replacing the k-NN graph
with G⋆, therefore summing over nodes in the neighborhood
(i) instead of over the k nearest.
Additionally, we propose a correlation dimension-based method that allows for local estimates.
We describe it next, then compare its results with those from the MLE method.

N

4.3.1 Algorithm: Neighborhood Correlation Dimension

Our proposed method is adapted from the approach used in [56] (also in [31, 53, 16]), where an
estimate of correlation dimension is computed by using a general kernel. It consists in computing
a curve Z(σ) over all pairwise kernel values (e.g., a Gaussian) at different values of the scale
parameter σ:

N

N

Z =

exp − k

i=1
X

j=1
X

2

xjk

.

xi −
2σ2

(42)

53

 
As in [31] (and analogously to eqs. 31–29), by assuming that for small values of σ the manifold

looks locally like its tangent space Rd, we have

M

which, after taking the logarithm, yields

Z

N 2(√2πσ)d

≈

vol2(

)

M

,

(43)

(44)

,

log Z

d log σ + log

≈

N 2(2π)d/2
)
vol(

M
so the slope of log Z
log σ can be used to estimate of the global dimensionality of the manifold,
d. To do so, one typically looks for a region where this slope is most stable, i.e., the curve is ap-
proximately linear. Automated ways of ﬁnding the slope of such a region are: by linear regression
of the middle portion of the curve [56] or by taking a point of maximum of Z ′(σ) [16, 53].

×

M

Because we assume intrinsic dimensionality may vary across

, global averages cannot work
in general. Moreover, nonuniform density, curvature, or multiple connected components may all
create multiple peaks, so inspection of the log-log plot cannot be automated. Therefore, we modify
this approach to use individual Zi(σ) curves for each data point xi. To keep the summation local,
points are restricted to those in the neighborhood of i in G⋆. Here, it is advantageous to work with
an extended neighborhood (e.g., by also including neighbors-of-neighbors) due to the theoretical
limit to the value of the dimension d that can be accurately estimated given a set of N points[39],
namely d < 2 log10 N. In fact, if N is large compared to d, even additional hops away from i may
be considered. Because such extension is done by following edges in G⋆ (as opposed to naively
expanding a ball in Rn), we may thus obtain a larger (approximately tubular) neighborhood around
xi without ever leaving the manifold. We denote such a neighborhood
′(i), as opposed to the
immediate neighborhood

(i), both including i itself.

N

Our algorithm involves the following steps:

N

1. For each data point xi and its extended neighborhood

′i , deﬁne Zi as

Zi(σ) =

exp − k

N
xi −
2σ2

2

xjk

.

′(i)
Xj
|
∈|N
2. Analogously to 44, by taking the logarithm we have that the slope of the log Zi ×
Z ′i(σ)

def
=

,

d log Zi
d log σ

(45)

log σ curve,

(46)

is an estimate of di, the dimension around xi as a function of σ. Computationally, it is
desirable to use the closed-form expression, for accuracy:

Z ′i(σ) =

′(i)
|

|N
j=1

k

P

σ2

xi −

′(i)
|

|N
j=1

xjk
exp −k

2 exp −k
xi
xj
−
2σ2

xi
−
2σ2
2
k

xj

2
k

.

(47)

3. A region of stability of Z ′i, i.e. a local maximum, is then an estimate of the apparent dimen-

P

sionality around xi.

54

j

N

∈ N
′(i):

A local maximum (“peak”) in Z ′i(σ) means that, as a ball around xi is expanded, the rate
at which neighbors are seen has stopping increasing and must decrease with larger σ, since no
more neighbors can be found after the ball encompasses all points in
′(i) is
sufﬁciently representative of the manifold around xi, i.e., if neighbors are approximately uniformly
distributed and dimensionality is constant within it, then Z ′i should remain constant over some
appreciable range of σ, whence the notion of ‘stability’.

′(i). Intuitively,

N

N

X

1 as σ

Even though we work with a subset of

, there may still be multiple maxima in Z ′i, e.g.,
when the neighbors of xi are for from uniformly distributed around it. So, operationally, we use
the global maximum of Z ′i, since this takes into account the information given by the majority of
neighboring points. Now, because Zi →
, the slope of log Zi
must 0 at both extremes, thus the global maximum of Z ′i must also be a relative one (a “peak”).
We now proceed to avoid boundary effects by re-centering neighborhoods. The boundary ∂
M
of a d-dimensional manifold (when present) has dimensionality d - 1 [69]. The correlation integral
approach often fails for these points—it typically returns d/2 for points in ∂
—since they tend
to have half the number of neighbors as interior points. For the same reason, it also tends to under-
estimate d for points near the boundary. Since we work locally over a graph, we can regularize the
computation by moving the focus to a nearby point (therefore regularizing over sampling artifacts
as well):

0, and Zi →

N as σ

→ ∞

M

→

4. Letting

(i) be the set of adjacent nodes to i in G⋆ and including i itself, deﬁne ¯ι as the node
N
(i) with smallest median squared distance to all points in the extended neighborhood

¯ι = argminj

(i)median

∈N

xj −

xlk

2),

l
∀

∈ N

′(i)

.

(48)

¯ι is, in effect, the most central node in i’s immediate neighborhood8.

(cid:9)

k
(cid:8)

5. Use ¯ι as the point from which kernel values are computed for Zi(σ) (replacing xi with x¯ι in
eq. 45, thereby shifting the center of estimation of di. This assumes dimensionality does not
change abruptly across neighboring points. Denote the resulting estimate ˆdi.

Finally, recall from section 3.5 that we also obtain a degree-based estimate, ˜di, when computing
volume ratios (eq. 36); we can use this information to further improve our results. A ﬁnal estimate,
d⋆
i is obtained as follows:

6. As with the MLE method (section 4.3), we can obtain a smoother estimate ˆd′i as the average

over the estimates among immediate neighbors in

(i):

N

ˆdj.

ˆd′i =

1
(i)

|N

| Xj

∈N

(i)

7. To avoid overestimating the true dimension, we compute an average ˜d′i over

(i) as

N

˜d′i =

1
(i)

|N

˜dj

=

1
(i)

|N

| Xj

∈N

(i) j

k

| Xj

∈N

(i)

log2 deg(j)
⌊

⌋

.

8Since we know G⋆, graph-theoretical quantities such as, e.g., shortest-path betweenness centrality [46, 22] can

(49)

(50)

also be readily employed.

55

8. Compute the optimal estimate d⋆

i as

Application of this technique and comparison with other methods are given next.

d⋆
i = max

ˆd′i, ˜d′i
n

o

(51)

4.3.2 Experimental results

Results of applying our neighborhood correlation dimension (NCD) algorithm compared to Levina
and Bickel’s MLE estimator (eq. 41) are shown in Figs. 32–34. For NCD, we compared results us-
ing IAN vs. those from k-NN graphs with various values of k (a range was chosen that included the
best results for each algorithm). The IAN kernel was applied by using the discrete neighborhoods
of G⋆, re-centered using neighbors-of-neighbors at most 3 hops away from i (eq. 48).

Using IAN, we obtained near-optimal results for the stingray and bent plane. For the 5-
dimensional cylinder, dimensionality was underestimated (mean 4.6). Methods based on corre-
lation dimension are known to underestimate the true d when the sample size is not sufﬁciently
large[25]. In these cases, the method of [26] can be applied a posteriori to improve results.

For the MLE method, using large values of k tends to improve results, but only when dimen-
sionality was constant (as in the bent plane and cylinder). For the stingray dataset, however, no
value of k gave correct results; small values of k increase the estimates due to a bias, and large
values tend to produce a uniform value throughout (thus giving better estimates only when d is
constant). We found that computing the neighborhood averages using MacKay and Ghahramani’s
correction (averaging the inverse of the estimators to reduce bias when k is small)[76] gave slightly
better results. (We did not use the ﬁnal smoothing procedure which involves choosing two addi-
tional neighborhood size parameters, k1 and k2.)

Finally, we conﬁrm these observations by testing two additional datasets with non-uniform
dimensionality (Fig. 35). Again, while our algorithm achieves good results locally, there is no
single value of k that allows MLE to ﬁnd appropriate local estimates everywhere.

56

k = 2

k = 4

k = 8

I(cid:12)(cid:13)

k = (cid:16)

k = (cid:17)

k = 1(cid:18)

k = (cid:19)(cid:20)

1.0

1.5

2.0
dimensionality

2.5

3.0

Figure 32: Estimation of local intrinsic dimensionality on the stingray dataset. Top row shows re-
sults for our neighborhood correlation dimension (NCD) algorithm using k-NN graphs for various
k and using adaptive neighborhoods from G⋆ (IAN). Bottom row shows results using Levina and
Bickel’s MLE estimator, which are quite sensitive to the choice of k: using a small value grossly
overestimates the dimensionality over the body, and a large k ignores the geometry of the tail. With
NCD using IAN gave the best results, estimated a dimension 2 for the body and 1 for the tail, with
intermediate values for the transition tail/body and the boundary.

57

N
(cid:10)
(cid:11)
M
(cid:14)
(cid:15)
Figure 33: Estimation of local intrinsic dimensionality on the bent plane dataset (cf. Fig. 32). As
with the stingray, results are sensitive to the choice of k, but here a wider range of values work
due to the constant dimension. For NCD, results with IAN are comparable to those using the best
k-NN graph (k = 16). With MLE, larger k improved results (comparable to those from NCD).

NCD

MLE

s
t
n
u
o
c

2

3
local dimensionality

4

5

2

3

5

4
7
local dimensionality

6

8

Figure 34: Estimation of local intrinsic dimensionality on the 5-D cylinder dataset (cf. Fig. 32).
With NCD, results using IAN underestimated the true dimensionality (mean 4.63), but are still
better than using a k-NN graph with arbitrary k. With MLE, larger values of k gave tighter distri-
butions around the correct value (mean 4.86 for k = 32).

58

Figure 35: Dimensionality estimation for two datasets with non-uniform local di: a “tiara” (top
row), where dimension varies smoothly from 2 to 1, and a “spinning top” (bottom row, middle
cross-section shown), where dimension goes from dimensionality reduces from 3 to 1 as one moves
from the bulky part toward the tip. Using the optimal k for MLE cannot give good results for the
entire data. Our NCD method, on the other hand, is able to correctly adapt to the local geometry
by using the IAN kernel.

59

5 Summary and Conclusions

In theory, applying the manifold assumption requires knowing about the manifold: its geometry,
In practice, however, these manifold properties are
topology, as well as how it was sampled.
rarely known. Instead, one typically applies a reasonable assumption, such as the dimension of the
manifold is about d and therefore about k = 2d nearest neighbors sufﬁce. Thus many of the data
graphs underlying manifold inference and non-linear dimensionality reduction are built. Since it is
essentially impossible to remain completely agnostic, it is alright to choose a few values of k and
choose among the results.

But this cannot work in general. Manifolds may not have a ﬁxed dimension, they may be
curved or with boundary, and sampling may vary. Finding a compromise k is, well, a compromise.
We suggest a different tack:
that one should build the nearest neighbor graph, and hence the
graph-Laplacian approximation, in as data-driven a manner as possible. The number of neighbors,
and hence the intrinsic dimensionality, may then very across the data. A multi-scale approach is
required.

Our algorithm of iterated adaptive neighborhoods (IAN) starts with an extremely conservative
assumption: that nearest neighbors should have no ‘nearer’ neighbors between them. We alternate
between a discrete and a continuous view of neighborhood graphs, and use a volumetric statistic to
check for outliers. A linear program keeps the scales minimal while providing a global cover. This
optimization is convex, so results are deterministic; other approaches, such as t-SNE, are stochastic
so depend critically on the initialization.

Our multi-scale kernel has been applied successfully to a variety of datasets, and compared
against many of the most popular algorithms available. In all cases our performance dominates.
Furthermore, our multi-scale kernel can be incorporated directly into many of them, including
diffusion maps, Isomap, UMAP, and t-SNE. In all of these cases performance is improved. Most
of these algorithms involve many free parameters; we have none other than the robust requirement
for an outlier.

Other algorithms (e.g. LLE) approximate the tangent space over a local neighborhood around
each point. Although not explored here, using G⋆ to automatically provide such neighborhoods is
straightforward (analogous to what was done in section 4.3 to estimate the local dimensionality).
Applications to clustering need to be explored.

Our weighted graph has also been applied to geodesic estimation, achieving comparable results
to those obtained from graph geodesics. In contrast, the graphs obtained from other similarity
kernels produce less than optimal results.

Our unweighted graph has found application in local dimensionality estimation. Our proposed
algorithm, neighborhood correlation dimension (NCD) takes advantage of the adaptive connectiv-
ity of our graphs in order to improve correlation dimension-based results, namely by restricting
the correlation integral to an approximately tubular neighborhood around xi in
. As a result, we
obtained accurate estimates of the local dimensionality in datasets where it is not uniform.

M

Much remains to be done. Several theoretical bounds are implied; these need to be proved.
Multi-scale kernels, such as those from eqs. 6,7 have been shown to approximate Laplacian oper-
ators asymptotically[98, 17]. Using our application examples as evidence, we conjecture that our
version also results in good approximations.

60

References

[1] Eddie Aamari, Jisu Kim, Fr´ed´eric Chazal, Bertrand Michel, Alessandro Rinaldo, and Larry
Wasserman. Estimating the reach of a manifold. Electronic journal of statistics, 13(1):1359–
1399, 2019.

[2] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive
dimensions, uniform convergence, and learnability. Journal of the ACM (JACM), 44(4):615–
631, 1997.

[3] Andr´es ´Alvarez-Meza, Juliana Valencia-Aguirre, Genaro Daza-Santacoloma, and Germ´an
Castellanos-Dom´ınguez. Global and local choice of the number of nearest neighbors in
locally linear embedding. Pattern Recognition Letters, 32(16):2171–2177, 2011.

[4] Nina Amenta and Marshall Bern. Surface reconstruction by voronoi ﬁltering. Discrete &

Computational Geometry, 22(4):481–504, 1999.

[5] Nina Amenta, Marshall Bern, and Manolis Kamvysselis. A new voronoi-based surface re-
construction algorithm. In Proceedings of the 25th annual conference on Computer graphics
and interactive techniques, pages 415–421, 1998.

[6] Sanjeev Arora, Wei Hu, and Pravesh K Kothari. An analysis of the t-sne algorithm for data

visualization. In Conference On Learning Theory, pages 1455–1462. PMLR, 2018.

[7] Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians.

The Annals of Applied Probability, 15(1A):69–92, 2005.

[8] R.

Banisch,

E.

H.

Thiede,

and

Z.

Trstanova.

pydiffmap.

https://github.com/DiffusionMapsAcademics/pyDiffMap, 2017.

[9] Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH
Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction
for visualizing single-cell data using UMAP. Nature biotechnology, 37(1):38–44, 2019.

[10] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and

data representation. Neural computation, 15(6):1373–1396, 2003.

[11] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on riemannian manifolds.

Machine learning, 56(1):209–239, 2004.

[12] Mikhail Belkin, Jian Sun, and Yusu Wang. Discrete laplace operator on meshed surfaces.
In Proceedings of the twenty-fourth annual symposium on Computational geometry, pages
278–287, 2008.

[13] Mikhail Belkin, Jian Sun, and Yusu Wang. Constructing laplace operator from point clouds
in r d. In Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algo-
rithms, pages 1031–1040. SIAM, 2009.

61

[14] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cl´audio Silva, and Gabriel Taubin.
The ball-pivoting algorithm for surface reconstruction. IEEE transactions on visualization
and computer graphics, 5(4):349–359, 1999.

[15] M Bernstein, V De Silva, JC Langford, and JB Tenenbaum. Graph approximations to

geodesics on embedded manifolds (technical report), 2000.

[16] Tyrus Berry, Dimitrios Giannakis, and John Harlim. Nonparametric forecasting of low-

dimensional dynamical systems. Physical Review E, 91(3):032915, 2015.

[17] Tyrus Berry and John Harlim. Variable bandwidth diffusion kernels. Applied and Compu-

tational Harmonic Analysis, 40(1):68–96, 2016.

[18] Adam Block, Zeyu Jia, Yury Polyanskiy, and Alexander Rakhlin. Intrinsic dimension esti-

mation. Journal of Machine Learning Research, 22:1–30, 2021.

[19] Jean-Daniel Boissonnat, Leonidas J Guibas, and Steve Y Oudot. Manifold reconstruction
in arbitrary dimensions using witness complexes. Discrete & Computational Geometry,
42(1):37–70, 2009.

[20] Jean-Daniel Boissonnat, Andr´e Lieutier, and Mathijs Wintraecken. The reach, metric dis-
tortion, geodesic convexity and the variation of tangent spaces. Journal of applied and
computational topology, 3(1):29–58, 2019.

[21] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cam-

bridge university press, 2004.

[22] Ulrik Brandes. A faster algorithm for betweenness centrality. Journal of mathematical

sociology, 25(2):163–177, 2001.

[23] Christoph Bregler and Stephen Omohundro. Nonlinear image interpolation using manifold

learning. Advances in neural information processing systems, 7, 1994.

[24] Francesco Camastra. Data dimensionality estimation methods: a survey. Pattern recogni-

tion, 36(12):2945–2954, 2003.

[25] Francesco Camastra and Antonino Staiano. Intrinsic dimension estimation: Advances and

open problems. Information Sciences, 328:26–41, 2016.

[26] Francesco Camastra and Alessandro Vinciarelli. Estimating the intrinsic dimension of data
with a fractal-based method. IEEE Transactions on pattern analysis and machine intelli-
gence, 24(10):1404–1407, 2002.

[27] David M Chan, Roshan Rao, Forrest Huang, and John F Canny. t-sne-cuda: Gpu-accelerated
t-sne and its applications to modern data. In 2018 30th International Symposium on Com-
puter Architecture and High Performance Computing (SBAC-PAD), pages 330–338. IEEE,
2018.

[28] Ronald R Coifman and St´ephane Lafon. Diffusion maps. Applied and computational har-

monic analysis, 21(1):5–30, 2006.

62

[29] Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Freder-
ick Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis
and structure deﬁnition of data: Diffusion maps. Proceedings of the national academy of
sciences, 102(21):7426–7431, 2005.

[30] Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick
Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and
structure deﬁnition of data: Multiscale methods. Proceedings of the National Academy of
Sciences, 102(21):7432–7437, 2005.

[31] Ronald R Coifman, Yoel Shkolnisky, Fred J Sigworth, and Amit Singer. Graph laplacian
tomography from unknown random projections. IEEE Transactions on Image Processing,
17(10):1891–1899, 2008.

[32] Jose A Costa, Abhishek Girotra, and AO Hero. Estimating local intrinsic dimension with k-
nearest neighbor graphs. In IEEE/SP 13th Workshop on Statistical Signal Processing, 2005,
pages 417–422. IEEE, 2005.

[33] Keenan Crane, Clarisse Weischedel, and Max Wardetzky. Geodesics in heat: A new ap-
proach to computing distance based on heat ﬂow. ACM Transactions on Graphics (TOG),
32(5):1–11, 2013.

[34] Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Founda-

tions of Computer Science (Cat. No. 99CB37039), pages 634–644. IEEE, 1999.

[35] Xavier Desquesnes, Abderrahim Elmoataz, and Olivier L´ezoray. Eikonal equation adapta-
tion on weighted graphs: fast geometric diffusion process for local and non-local image and
data processing. Journal of Mathematical Imaging and Vision, 46(2):238–257, 2013.

[36] George Dimitriadis, Joana P Neto, and Adam R Kampff. t-sne visualization of large-scale

neural recordings. Neural computation, 30(7):1750–1774, 2018.

[37] David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding tech-
niques for high-dimensional data. Proceedings of the National Academy of Sciences,
100(10):5591–5596, 2003.

[38] Ramsay Dyer, Hao Zhang, and Torsten M¨oller. Gabriel meshes and delaunay edge ﬂips. In
2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling, pages 295–300,
2009.

[39] J-P Eckmann and David Ruelle. Fundamental limitations for estimating dimensions and
lyapunov exponents in dynamical systems. Physica D: Nonlinear Phenomena, 56(2-3):185–
187, 1992.

[40] Elena Facco, Maria d’Errico, Alex Rodriguez, and Alessandro Laio. Estimating the intrinsic
dimension of datasets by a minimal neighborhood information. Scientiﬁc reports, 7(1):1–8,
2017.

63

[41] Amir Massoud Farahmand, Csaba Szepesv´ari, and Jean-Yves Audibert. Manifold-adaptive
In Proceedings of the 24th international conference on Machine

dimension estimation.
learning, pages 265–272, 2007.

[42] Herbert Federer. Curvature measures. Transactions of the American Mathematical Society,

93(3):418–491, 1959.

[43] Charles Fefferman, Sergei Ivanov, Yaroslav Kurylev, Matti Lassas, and Hariharan
Narayanan. Fitting a putative manifold to noisy data. In Proceedings of the 31st Conference
On Learning Theory, volume 75, pages 688–720. PMLR, 2018.

[44] Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypoth-

esis. Journal of the American Mathematical Society, 29(4):983–1049, 2016.

[45] Steven Fortune. Voronoi diagrams and delaunay triangulations. Computing in Euclidean

geometry, pages 225–265, 1995.

[46] Linton C Freeman. A set of measures of centrality based on betweenness. Sociometry, pages

35–41, 1977.

[47] Yasuhiro Fujiwara, Yasutoshi Ida, Sekitoshi Kanai, Atsutoshi Kumagai, and Naonori Ueda.
Fast similarity computation for t-sne. In 2021 IEEE 37th International Conference on Data
Engineering (ICDE), pages 1691–1702. IEEE, 2021.

[48] Keinosuke Fukunaga and David R Olsen. An algorithm for ﬁnding intrinsic dimensionality

of data. IEEE Transactions on Computers, 100(2):176–183, 1971.

[49] K Ruben Gabriel and Robert R Sokal. A new statistical approach to geographic variation

analysis. Systematic zoology, 18(3):259–278, 1969.

[50] Christopher Genovese, Marco Perone-Paciﬁco, Isabella Verdinelli, and Larry Wasserman.
Minimax manifold estimation. Journal of Machine Learning Research, 13(43):1263–1291,
2012.

[51] Yair Goldberg and Ya’acov Ritov. Local procrustes for manifold embedding: a measure of
embedding quality and embedding algorithms. Machine learning, 77(1):1–25, 2009.

[52] Peter Grassberger and Itamar Procaccia. Measuring the strangeness of strange attractors. In

The theory of chaotic attractors, pages 170–189. Springer, 2004.

[53] Laleh Haghverdi, Florian Buettner, and Fabian J Theis. Diffusion maps for high-
dimensional single-cell analysis of differentiation data. Bioinformatics, 31(18):2989–2998,
2015.

[54] Gloria Haro, Gregory Randall, and Guillermo Sapiro. Translated poisson mixture model for
stratiﬁcation learning. International Journal of Computer Vision, 80(3):358–374, 2008.

[55] Jinrong He, Lixin Ding, Lei Jiang, Zhaokui Li, and Qinghui Hu. Intrinsic dimensionality
estimation based on manifold assumption. Journal of Visual Communication and Image
Representation, 25(5):740–747, 2014.

64

[56] Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds
In Proceedings of the 22nd international conference on Machine learning, pages

in Rd.
289–296, 2005.

[57] Matthias Hein and Markus Maier. Manifold denoising. Advances in neural information

processing systems, 19, 2006.

[58] Geoffrey E Hinton and Sam Roweis. Stochastic neighbor embedding. Advances in neural

information processing systems, 15, 2002.

[59] Bal´azs K´egl. Intrinsic dimension estimation using packing numbers. Advances in neural

information processing systems, 15, 2002.

[60] Yosi Keller, Ronald R Coifman, St´ephane Lafon, and Steven W Zucker. Audio-visual group
recognition using diffusion maps. IEEE Transactions on Signal Processing, 58(1):403–413,
2009.

[61] Maurice George Kendall. Rank correlation methods. Grifﬁn, 1948.

[62] Matth¨aus Kleindessner and Ulrike Luxburg. Dimensionality estimation without distances.

In Artiﬁcial Intelligence and Statistics, pages 471–479. PMLR, 2015.

[63] William R Knight. A computer method for calculating kendall’s tau with ungrouped data.

Journal of the American Statistical Association, 61(314):436–439, 1966.

[64] Dmitry Kobak and Philipp Berens. The art of using t-SNE for single-cell transcriptomics.

Nature communications, 10(1):1–14, 2019.

[65] Dmitry Kobak and George C Linderman. Initialization is critical for preserving global data

structure in both t-sne and umap. Nature biotechnology, 39(2):156–157, 2021.

[66] Olga Kouropteva, Oleg Okun, and Matti Pietik¨ainen. Selection of the optimal parameter

value for the locally linear embedding algorithm. FSKD, 2:359–363, 2002.

[67] St´ephane Lafon. Diffusion maps and geometric harmonics. PhD thesis, Yale University,

2004.

[68] Stephane Lafon, Yosi Keller, and Ronald R Coifman. Data fusion and multicue data match-
ing by diffusion maps. IEEE Transactions on pattern analysis and machine intelligence,
28(11):1784–1797, 2006.

[69] John Lee. Introduction to topological manifolds, volume 202. Springer Science & Business

Media, 2010.

[70] John A Lee and Michel Verleysen. Nonlinear dimensionality reduction, volume 1. Springer,

2007.

[71] Elizaveta Levina and Peter Bickel. Maximum likelihood estimation of intrinsic dimension.

Advances in neural information processing systems, 17, 2004.

65

[72] Oﬁr Lindenbaum, Moshe Salhov, Arie Yeredor, and Amir Averbuch. Gaussian bandwidth
selection for manifold learning and classiﬁcation. Data mining and knowledge discovery,
34(6):1676–1712, 2020.

[73] George C Linderman, Manas Rachh, Jeremy G Hoskins, Stefan Steinerberger, and Yuval
Kluger. Fast interpolation-based t-sne for improved visualization of single-cell rna-seq data.
Nature methods, 16(3):243–245, 2019.

[74] Anna V Little, Mauro Maggioni, and Lorenzo Rosasco. Multiscale geometric methods for
data sets i: Multiscale svd, noise and curvature. Applied and Computational Harmonic
Analysis, 43(3):504–567, 2017.

[75] L´aszl´o Lov´asz. Discrete and continuous: two sides of the same? In Visions in mathematics,

pages 359–382. Springer, 2010.

[76] David JC MacKay and Zoubin Ghahramani. Comments on’maximum likelihood estimation
of intrinsic dimension’by e. levina and p. bickel (2005). The Inference Group Website,
Cavendish Laboratory, Cambridge University, 2005.

[77] David W Matula and Robert R Sokal. Properties of gabriel graphs relevant to geo-
graphic variation research and the clustering of points in the plane. Geographical analysis,
12(3):205–222, 1980.

[78] Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approxima-

tion and projection for dimension reduction, 2018.

[79] Nathan Mekuz and John K Tsotsos. Parameterless isomap with adaptive neighborhood
selection. In Joint Pattern Recognition Symposium, pages 364–373. Springer, 2006.

[80] Gal Mishne and Israel Cohen. Multiscale anomaly detection using diffusion maps. IEEE

Journal of selected topics in signal processing, 7(1):111–123, 2012.

[81] Kevin R Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B Burkhardt, William S
Chen, Kristina Yim, Antonia van den Elzen, Matthew J Hirn, Ronald R Coifman, et al. Visu-
alizing structure and transitions in high-dimensional biological data. Nature biotechnology,
37(12):1482–1492, 2019.

[82] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hy-

pothesis. Advances in neural information processing systems, 23, 2010.

[83] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of subman-
ifolds with high conﬁdence from random samples. Discrete & Computational Geometry,
39(1):419–441, 2008.

[84] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. A topological view of unsupervised

learning from noisy data. SIAM Journal on Computing, 40(3):646–663, 2011.

[85] F. No´e, R. Banisch, and C. Clementi. Commute maps: separating slowly mixing molec-
ular conﬁgurations for kinetic modeling. Journal of chemical theory and computation,
12(11):5620–5630, 2016.

66

[86] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[87] Karl W Pettis, Thomas A Bailey, Anil K Jain, and Richard C Dubes. An intrinsic dimen-
sionality estimator from near-neighbor information. IEEE Transactions on pattern analysis
and machine intelligence, 1(1):25–37, 1979.

[88] Sam Roweis, Lawrence Saul, and Geoffrey E Hinton. Global coordination of local linear

models. Advances in neural information processing systems, 14, 2001.

[89] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear

embedding. science, 290(5500):2323–2326, 2000.

[90] Alessandro Rozza, Gabriele Lombardi, Claudio Ceruti, Elena Casiraghi, and Paola Cam-
padelli. Novel high intrinsic dimensionality estimators. Machine learning, 89(1):37–65,
2012.

[91] Marco Saerens, Francois Fouss, Luh Yen, and Pierre Dupont. The principal components
analysis of a graph, and its relationships to spectral clustering. In European conference on
machine learning, pages 371–383. Springer, 2004.

[92] Oksana Samko, A David Marshall, and Paul L Rosin. Selection of the optimal parameter

value for the Isomap algorithm. Pattern Recognition Letters, 27(9):968–979, 2006.

[93] Lawrence K Saul, Kilian Q Weinberger, Fei Sha, Jihun Ham, and Daniel D Lee. Spectral

methods for dimensionality reduction. Semi-supervised learning, 3, 2006.

[94] Daniel Spielman. Spectral graph theory. Combinatorial scientiﬁc computing, 18, 2012.

[95] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and high-
dimensional data. In Proceedings of the 25th international conference on world wide web,
pages 287–297, 2016.

[96] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework

for nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.

[97] Christoph Th¨ale. 50 years sets with positive reach–a survey. Surveys in Mathematics and

its Applications, 3:123–165, 2008.

[98] Daniel Ting, Ling Huang, and Michael I. Jordan. An analysis of the convergence of graph
laplacians. In Proceedings of the 27th International Conference on International Conference
on Machine Learning, ICML’10, page 1079–1086, Madison, WI, USA, 2010. Omnipress.

[99] Gerard V Trunk. Statistical estimation of the intrinsic dimensionality of a noisy signal

collection. IEEE Transactions on Computers, 100(2):165–171, 1976.

[100] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of

Machine Learning Research, 15(1):3221–3245, 2014.

67

[101] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of

machine learning research, 9(11), 2008.

[102] Laurens van der Maaten, Eric Postma, and Jaap van den Herik. Dimensionality reduction:

a comparative review. J Mach Learn Res, 10(66-71):13, 2009.

[103] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Jour-

nal of Computer and System Sciences, 68(4):841–860, 2004.

[104] Peter J. Verveer and Robert P. W. Duin. An evaluation of intrinsic dimensionality estimators.
IEEE Transactions on pattern analysis and machine intelligence, 17(1):81–86, 1995.

[105] Xiang Wan, Wenqian Wang, Jiming Liu, and Tiejun Tong. Estimating the sample mean and
standard deviation from the sample size, median, range and/or interquartile range. BMC
medical research methodology, 14(1):1–13, 2014.

[106] Jing Wang, Zhenyue Zhang, and Hongyuan Zha. Adaptive manifold learning. Advances in

neural information processing systems, 17, 2004.

[107] Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding how
dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap,
and pacmap for data visualization. J. Mach. Learn. Res., 22(201):1–73, 2021.

[108] Martin Wattenberg, Fernanda Vi´egas, and Ian Johnson. How to use t-SNE effectively. Dis-

till, 1(10):e2, 2016.

[109] Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by

semideﬁnite programming. International journal of computer vision, 70(1):77–90, 2006.

[110] Lai-Sang Young. Dimension, entropy and lyapunov exponents. Ergodic theory and dynam-

ical systems, 2(1):109–124, 1982.

[111] Lihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Proceedings of the
17th International Conference on Neural Information Processing Systems, NIPS’04, page
1601–1608, Cambridge, MA, USA, 2004. MIT Press.

[112] Zhenyue Zhang and Hongyuan Zha. Principal manifolds and nonlinear dimensionality re-
duction via tangent space alignment. SIAM journal on scientiﬁc computing, 26(1):313–338,
2004.

68

A Greedy splitting

As an alternative to the optimization from section 3.4 (which can be expensive when the number
of edges in G is very large, mainly due to large d), we developed a greedy approach in which we
assign scales that “C-cover” each edge eij in decreasing order of length rij (the Euclidean distance
between xi and xj in Rn).

Starting with the edge eij with largest rij, set σi = σj = Crij, thereby satisfying σiσj = (Crij)2
, we know

max

(we say Crij is evenly “split” between σi and σj). Moreover, since rij ≤
rFN
the constraints σi ≤
j
cases in which a (re)assignment of scales is needed:

are also satisﬁed.

and σj ≤

rFN
i

Continue with the edge eij with the next largest distance, rij. Here we are met with 3 possible

rFN
i

, rFN
j }

{

1. if neither of the nodes have been assigned a scale yet, evenly split scales as above;

2. if one of the nodes does not have a scale yet (wlog, let that node be j), set σ′j to the minimum

scale that ensures σiσ′j ≥

(Crij)2, i.e., σ′j = (Crij)2/σi;

3. if both nodes have previously been assigned a scale but eij is not covered by the current
and update the scales: σ′i = aσi and

values of σi and σj, then set the quotient a = Crij
√σiσj
σ′j = aσj thereby evenly splitting the quotient between the two nodes.

rFN
i

rFN
j

and σ′j ≤

After cases (2) and (3), the updated scales might need to be “re-balanced” in order to meet the
σ′
.
constraints σ′i ≤
i
σ′′
i
Only one of the two scales may exceed its upper bounds: in (2) this is trivially true since only the
newly-assigned scale may be greater than Crij; in (3), since both σi and σj have been previously
rFN
rFN
, so therefore it
and σj ≤
assigned, we have σi ≤
i
i
must be the case that rFN
r2
i rFN
ij = σ′iσ′j. Note that, as a corollary, both scales must meet their
j ≥
respective constraints after being re-balanced as above.

. Then, we set σ′′i > rFN

, as well as rij ≤

. Wlog, let σ′i > rFN

and rij ≤

and σ′′j = σ′j

rFN
j

rFN
j

i

i

This is repeated until all edges have been visited. The idea is that, by covering the largest edges
ﬁrst, we assign the largest, most constrained scales ﬁrst, allowing for the later, less constrained
scales, to be as small as possible. Because this tends to equally “split” the scaled edge lengths Crij
two scales σi and σj, it produces reasonable but usually sub-optimal results.

69

