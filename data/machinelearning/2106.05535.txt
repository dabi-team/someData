Differentiable Robust LQR Layers

Ngo Anh Vien1

Gerhard Neumann2

1Bosch Center for Artiﬁcial Intelligence, Germany
2Karlsruhe Institute of Technology, Germany

1
2
0
2

n
u
J

0
1

]

O
R
.
s
c
[

1
v
5
3
5
5
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

This paper proposes a differentiable robust LQR
layer for reinforcement learning and imitation
learning under model uncertainty and stochastic
dynamics. The robust LQR layer can exploit the
advantages of robust optimal control and model-
free learning. It provides a new type of inductive
bias for stochasticity and uncertainty modeling in
control systems. In particular, we propose an ef-
ﬁcient way to differentiate through a robust LQR
optimization program by rewriting it as a convex
program (i.e. semi-deﬁnite program) of the worst-
case cost. Based on recent work on using convex
optimization inside neural network layers, we de-
velop a fully differentiable layer for optimizing
this worst-case cost, i.e. we compute the derivative
of a performance measure w.r.t the model’s un-
known parameters, model uncertainty and stochas-
ticity parameters. We demonstrate the proposed
method on imitation learning and approximate dy-
namic programming on stochastic and uncertain
domains. The experiment results show that the pro-
posed method can optimize robust policies under
uncertain situations, and are able to achieve a sig-
niﬁcantly better performance than existing meth-
ods that do not model uncertainty directly.

1 INTRODUCTION

Combining model-free and model-based reinforcement
learning (RL) has recently received much attentions by re-
searchers in the community. While model-free approaches
can be easy for training, they suffer from poor sample efﬁ-
ciency and weak generalization and being too task-speciﬁc.
Therefore they might have limited applicability to real-world
physical systems, where long-time running would lead to
being unsafe or to break the system. In contrast, model-
based approaches are known to be sample-efﬁcient, however
when trained on a physical system they often suffer from a
similar safety and robustness issues. These issues can pos-

sibly mislead the system to breaking states [Amodei et al.,
2016]. Model learning and policy optimization are some-
times treated as two separate stages, which is a common ap-
proach in dual control, i.e. learning and control [Feldbaum,
1960]. However, they can not fully exploit the advantages of
domain knowledge in an end-to-end fashion, i.e. inductive
bias of optimal control. Injecting inductive bias of optimal
control in end-to-end RL algorithms has recently shown
many great successes in terms of sample-efﬁciency and ro-
bustness [Tamar et al., 2016, Silver et al., 2017, Karkus
et al., 2017, Farquhar et al., 2018, Hafner et al., 2020].

We follow work using inductive bias of optimal control in
end-to-end RL [Amos et al., 2018]. While existing work has
not yet included model uncertainty and dynamics stochas-
ticity, modeling uncertainty and perturbation is considered
very important for robot learning [Mankowitz et al., 2019].
In this paper, we will propose a differentiable robust linear
quadratic regulator (LQR) layer that introduces an inductive
bias of robust optimal control. We combine the advantages
of model-based robust optimal control and model-free policy
learning. Based on recent work on using convex optimiza-
tion inside neural network layers [Agrawal et al., 2019a],
our algorithm differentiates through a robust optimization
algorithm, and hence becomes end-to-end trainable without
the necessity of unrolling the planning procedure. Speciﬁ-
cally, similar to a standard neural network (NN) layer the
developed robust optimization layer computes an optimal
solution of a robust optimization program as output. This
layer can be differentiated through, and integrated into a
NN as a differentiable module to achieve an end-to-end task
network. The parameters of this layer are parameters used in
the parameterization of the dynamics model, uncertainty set,
and the distribution of the dynamics stochasticity. Different
from existing approaches, our robust LQR layer is able to
model stochasticity (stochastic dynamics models), and un-
certainty (uncertainty set over models). We will show that
we can differentiate through this robust layer by resorting
to the Linear Matrix Inequality (LMI) formulation [Boyd
et al., 1994]. In particular, we rewrite robust constraints

 
 
 
 
 
 
as the worst-case cost, and derive an approximation to the
objective of the robust control program. The resulting con-
vex semi-deﬁnite program can then be differentiated using
the differentiable conic program algorithm as introduced
recently by Agrawal et. al. [Agrawal et al., 2019b].

We will show that on imitation learning under the presence
of task uncertainty we can estimate both i) the model pa-
rameters, i.e. the cost and dynamics, and also ii) the model
uncertainty, i.e. the stochasticity of the dynamics and the
model parameter’s uncertainty. The uncertainty might come
from environment disturbances or from the expert’s non-
stationary model, i.e. where the environment dynamics or
the expert’s mental state change.

2 RELATED WORK

Model-free and model-based methods are two common
approaches in RL. Model-free approaches are learning direct
mappings from raw inputs to actions [Mnih et al., 2013,
Lillicrap et al., 2016, Schulman et al., 2015]. Although
these approaches might be easy to implement and can make
training fast and easy, they have weak generalization ability,
are not sample-efﬁcient, and tend to be task-speciﬁc [Sun,
2019]. On the other hand, model-based approaches learn an
approximate model of the environment dynamics from real
interactions, and optimize an optimal policy using ﬁctitious
data generated from the learnt dynamics model. There have
been efforts to combine the advantages of model-based RL
with control theory, where optimal control is used both as
domain knowledge embedded in model learning and as a
solver for policy optimization, i.e. model predictive control
(MPC), linear quadratic regulator (LQR), or differential
dynamic programming (DDP) [Levine and Koltun, 2013,
Zhang et al., 2016, Nagabandi et al., 2018, Hafner et al.,
2019].

Differentiable optimization layer: There has been recent
effort in using convex optimization inside neural network
layers. Most existing model-based methods propose to in-
tegrate unrolling out planning steps into policy networks,
i.e. a connection between two layers is deﬁned as coming
from a planning step. [Tamar et al., 2016, Karkus et al.,
2017, Silver et al., 2017, Oh et al., 2017, Farquhar et al.,
2018]. However this rolling procedure makes forward and
backward calculations become computationally expensive.
There are recent approaches that can handle analytic differ-
entiation of optimization, hence they have more efﬁcient
computation. In particular, Mensch et. al. [Mensch and Blon-
del, 2018] propose a differentiable dynamics programming
layer. Otto et al. [2021] propose a differentiable trust region
layer that can be integrated directly into TRPO and PPO
[Schulman et al., 2015]. A concurrent work by Donti et al.
[2020] also proposes to integrate robust control as a cus-
tom convex-optimization-based projection layer for generic
nonlinear control policy networks. Other work [Amos and

Kolter, 2017, Agrawal et al., 2019a] incorporate general
convex programs inside deep networks. However it is non-
trivial to extend those approaches to incorporate a robust
optimal control program within deep policy networks.

Safe reinforcement learning [Garcıa and Fernández, 2015]
is concerned with how safety should be addressed in learn-
ing to control systems. Safety can be deﬁned through risk-
averse reward functions [Coraluppi and Marcus, 1999]
where high costs are at risk or at undesirable states. Simi-
larly, safe exploration [Schreiter et al., 2015, Achiam et al.,
2017] and robust policy search [Mankowitz et al., 2019]
guide RL agents to explore the state space while adher-
ing to certain safety constraints while the policy is being
optimized. In addition, robust Markov decisions processes
(MDPs) [Iyengar, 2005] and robust optimal control [Van-
denberghe et al., 2002, Bemporad and Morari, 1999] con-
sider policy optimization under the presence of uncertainty
over the dynamics models and state knowledge. These ap-
proaches require an explicit deﬁnition of safety, e.g. a risk-
averse reward function, or auxiliary cost constraint func-
tions. Alternatively, robust dual control is concerned with
simultaneous learning and control subject to uncertainty
and stochastic dynamics [Abbasi-Yadkori and Szepesvári,
2011, Weiss and Di Cairano, 2014, Cohen et al., 2018, Dean
et al., 2018, 2019]. These algorithms are able to offer a
differentiable objective for policy optimization and model
estimation, however they cannot perform full end-to-end
learning, i.e. the possibility of being integrated into a fully
differentiable task network.

3 BACKGROUND

In this paper, we are concerned with the robust inﬁnite-
horizon linear-quadratic-Gaussian (LQG) control problem
under ellipsoidal uncertainty,

s.t.

(cid:17)

,

t Rut

(cid:16)
x(cid:62)
t Qxt + u(cid:62)

π ∗ = arg min
u=π(x)

∞
∑
t=0
xt+1 = Axt + But + wt
wt ∈ N (0, σ 2In),
(cid:16)
X (cid:62) − µ

[A, B] ∈ {A, B :

x0 ∼ P0,
(cid:17)(cid:62)

D

(1)

(cid:16)
X (cid:62) − µ

(cid:17)

≤ I},

where xt ∈ Rn, ut ∈ Rm, A ∈ Rn×n, B ∈ Rn×m. In, I are n × n
and (m + n) × (m + n) identity matrices, respectively. We de-
note X = [A, B] a concatenated vector of A and B; P0 denotes
an initial state distribution. The random variable wt denotes
the noise of the stochastic dynamics, which is assumed to
follow a Gaussian distribution N (0, σ 2In), where σ ∈ R.
Q ∈ Rn×n, R ∈ Rm×m are positive deﬁnite matrices which
model the quadratic costs. In addition, we assume the model
parameter uncertainty is described by an ellipsoidal uncer-
tainty that is parameterized by a nominal model µ = [ ¯A, ¯B]
and a symmetric positive deﬁnite matrix D.

3.1 DIFFERENTIABLE LQR

from a poor convergence and high computation when the
horizon T increases.

Recently there has been signiﬁcant effort in using differen-
tiable programs as a neural network layer, e.g. optimization
layers [Agrawal et al., 2019a], differentiable ODE solvers
[Chen et al., 2018], MPC-based policy layers [Amos et al.,
2018]. Amos et.al. [Amos et al., 2018] propose an algorithm
that can differentiate through a policy that is represented by
a LQR and MPC. In particular, Amos et.al. [Amos et al.,
2018] use a discrete-time ﬁnite-horizon LQR (with a formu-
lation similar to (1) with a ﬁnite time horizon and without
uncertainty and stochasticity, i.e. without the last two in-
equality constraints) as a learnable module with trainable
parameters θ = {A, B, Q, R}. The differentiation is made
through the ﬁxed-point solution τ ∗ = {x∗
t }t=1:T of the
LQR problem. The ﬁxed-point solution can be found using
the iterative method (involving a forward and a backward
recursion). The main challenge is to compute the derivative
∂ l/∂ θ of a generic loss function l(τ ∗) of τ ∗ w.r.t the pa-
rameters θ of the LQR-based policy. This derivative can be
∂ τ ∗
written as ∂ l/∂ θ = ∂ l
∂ θ . Then the authors suggested to
∂ τ ∗
compute the derivatives and differentiation through the con-
strained convex quadratic LQR problem at the ﬁxed point
by applying the implicit function theorem [Dontchev and
Rockafellar, 2009]. The implicit mapping between τ ∗ and
θ is expressed as the zero of the partial derivative of the
Lagrange function ∇τ L (τ ∗, λ ∗) = 0, where L is the La-
grange function of the constrained optimization problem
Eq. 1, and can be written as follows

t , u∗

L (τ, λ ) =

T
∑
t=1

(cid:16)
t Qxt + u(cid:62)
x(cid:62)

t Rut

(cid:17)

+

T −1
∑
t=0

λ (cid:62)
t (Axt + But − xt+1),

where T is the length of the time horizon, and λ = {λt }T −1
t=0
with λt ∈ Rn being a Langrange multiplier at time t.

This LQR control layer can also be extended to become a
differentiable MPC-based control layer by differentiating
the convex approximation at a ﬁxed point of the iterative
approximation procedure, i.e. an iterative linearization of
the dynamics and second-order Taylor approximation of
the cost. Finite-horizon LQR-based policies are open-loop,
therefore stability is not guaranteed [Bitmead and Gevers,
1991]. This problem could be mitigated through the online
open-loop MPC-based extension [Amos et al., 2018].

However, it is non-trivial to extend this approach to obtain
a differentiable robust control layer that would require to
model stochastic dynamics and/or model uncertainty. A dif-
ferentiable robust control layer with a robust optimal control
inductive bias is expected to better model the underlying
problem, hence will improve the robustness of learnt poli-
cies. For example, demonstration data can be generated by
a stochastic process with an uncertain model, as shown in
Eq. 1. In addition, both the forward and backward recursion
used by Amos et. al. [Amos et al., 2018] based on the con-
strained convex quadratic program formulation will suffer

3.2 LQR AND LINEAR MATRIX INEQUALITIES

In the case of deterministic dynamics and without uncer-
tainty, the problem in Eq. 1 becomes an inﬁnite-horizon
LQR control problem with a standard formulation as fol-
lows

π ∗ = arg min

∞
∑
t=0

(cid:16)
t Qxt + u(cid:62)
x(cid:62)

t Rut

(cid:17)

(2)

π
xt+1 = Axt + But ,

s.t.

x0 = xinit,

Its optimal controller is state-feedback, ut = Kxt , with
K = −(B(cid:62)PB + R)−1B(cid:62)PA, where P is a positive deﬁnite
matrix and can be computed by solving the Algebraic Ric-
cati Equation (ARE) [Camacho and Alba, 2013],

P = A(cid:62)PA + Q − A(cid:62)PB(B(cid:62)PB + R)B(cid:62)PA.

(3)

A solution for P can be found using iterative methods
[Hewer, 1971] (backward recursion) or by a convex SDP
formulation through the use of Linear Matrix Inequalities
(LMI) [Boyd et al., 1994, Balakrishnan and Vandenberghe,
2003]. The SDP formulation can be done via by assuming
the ARE to be a Lyapunov-inequality. The objective is to
minimize the trace of P,

max
P

trace(P)



s.t.



R + B(cid:62)PB
A(cid:62)PB
0

B(cid:62)PA

0
A(cid:62)PA + Q − P 0
P

0



 (cid:23) 0,

(4)

where P is symmetric positive deﬁnite: P (cid:23) 0, P = P(cid:62).

Stability of inﬁnite-horizon LQR has been extensively stud-
ied in optimal control [Kalman et al., 1960]. It has been
shown via the Lyapunov analysis that LQR is robust to un-
certainty in the model parameters (e.g. the A and B matrices)
and to perturbations (e.g. the noise w) under certain condi-
tions, i.e. the bounds on the uncertainty. In other words, as
long as the uncertainty and perturbations are small enough
the solution to the LMI constraint problem could exist and
could stabilize the controller. We will show that such a
limited guarantee is not sufﬁcient if the uncertainty level in-
creases. A higher uncertainty would increase the divergence
possibility and result in a higher task cost.

4 DIFFERENTIABLE ROBUST LQG

We now propose a family of robust LQR-based policies that
is based on the robust discrete time-invariant LQG problem
in Eq. 1. We consider a differentiable robust LQR program
in the inﬁnite-horizon setting. One of the main technical
challenges is how to differentiate through such a program

with an unbounded dimensionality and uncertain constraints.
To tackle this challenge, we will exploit the LMI techniques
which are used commonly in optimal control. In particular,
rewriting the objective and constraints using LMI techniques
would take advantages of robust optimal control and convex
optimization. The robust LQR problem’s solution can be
found by solving a (convex) SDP. This SDP can be solved
and differentiated through using the technique developed by
Agrawal et. al. [Agrawal et al., 2019a], called differentiable
convex optimization layers. We will discuss how to make
this SDP disciplined parameterized programming (DPP)
compliant so that it can be differentiated through.

where µ = [ ¯A, ¯B] are the nominal parameters. Thus, the
robust LQR-based policy module has parameters θ =
{ ¯A, ¯B, Q, R, D, σ }. We now discuss how to rewrite our ro-
bust problem so that its derivatives w.r.t θ can be computed
efﬁciently. We follow similar derivations like the system
level synthesis framework for H∞ optimal control studied
by Anderson et. al. [Anderson et al., 2019], which is re-
cently used in the robust RL framework by Umenberger et.
al. [Umenberger et al., 2019]. The robust LQR objective can
be rewritten as a worst-case problem,

(cid:34)

J(θ , K) = min

K

E

sup
θ ∈Θ

lim
T →∞

(cid:35)

c(xt , ut )

,

T
∑
t=0

4.1 DIFFERENTIABLE INFINITE-HORIZON LQR

s.t. the linear time-invariant dynamics

This section starts with the LMI formulation in Eq. 4, for the
robust LQR-based policy whose output is the solution to the
LQR problem as written in Eq. 2. We term this approach as
our ﬁrst contribution, LMI-LQR layer, which is considered
as an alternative solution for the dynamic Riccati recursion
solver used in the differentiable LQR approach described
in 3.1. The LMI-based LQR layer provides the solution of
the SDP program in Eq. 4 as output. This LQR-based policy
module is parameterized by parameters θ = {A, B, Q, R}.
In order for learning with this policy to be end-to-end dif-
ferentiable, we need an efﬁcient method to compute the
derivatives, ∂ π ∗/∂ θ , of the output policy π ∗ w.r.t the pa-
rameters θ . The SDP program in Eq. 4 can be solved using
standard convex optimization tools, however it can only be
differentiated through efﬁciently if its constraints and ob-
jectives are afﬁne mappings of the problem data (e.g. θ )
[Agrawal et al., 2019b], i.e. to be DPP-compliant for gen-
eral differentiable convex optimization layers [Agrawal et al.,
2019a]. In this paper, we resort to the differentiable convex
optimization layers as a differentiation technique, with the
introduction of two auxiliary variables S1, S2, subject to the
additional constraints S1 = PB, S2 = PA in order to make
the SDP program in Eq. 4 to be DPP-compliant.

The above LMI-based LQR layer cannot directly model
uncertainty. Thus this approach will only work well if the
uncertainty is small enough. In next section, we will propose
a new approach that directly models uncertainty by means
of the parameters {D, σ }.

4.2 DIFFERENTIABLE ROBUST

INFINITE-HORIZON LQR

We now want to explicitly represent the uncertainty and
perturbations and learn these terms. As a ﬁrst step we focus
on the dynamics alone by assuming that there is uncertainty
about X = [A, B]. We denote the uncertainty set as

xt+1 = Axt + But + wt , wt ∼ N (0, σ 2I), x0 = xinit.

Assuming the policy parameterization ut = Kxt . We can
rewrite the inﬁnite horizon cost as

J(θ , K) = lim
T →∞

(cid:34) T
∑
t=0
(cid:21)

E

1
T
(cid:32)(cid:20) Q 0
R
0
(cid:32)(cid:20) Q 0
R
0
(cid:18)(cid:20) Q 0
R
0

= trace

= trace

= trace

(x(cid:62)

t Qxt + u(cid:62)

(cid:35)
t Rut )

lim
T→∞

1
T

(cid:21)

1
lim
T
T→∞
(cid:21) (cid:20) W

T
∑
t=1

T
∑
t=1

E

E

(cid:34)(cid:20) xt
ut
(cid:34)(cid:20) xt
Kxt
(cid:21)(cid:19)

W K(cid:62)
KW KW K(cid:62)

(cid:21)(cid:35)(cid:33)

(cid:21)(cid:62) (cid:20) xt
ut

(cid:21)(cid:35)(cid:33)

(cid:21)(cid:62) (cid:20) xt
Kxt

,

where we denote W = E (cid:2)xt x(cid:62)
(cid:3), the stationary state covari-
t
ance. Given certain A and B, W can be found by solving the
following optimization problem,

arg min
W

trace(W),

s.t. W (cid:23) (A + BK)W (A + BK)(cid:62) + σ 2I.

(5)

If we assume that the policy has only parameters K, we
receive a semideﬁnite program (SDP), which is convex.
However, in the case of uncertain {A, B}, solving the above
program is very challenging [El Ghaoui et al., 1998].

We will now reformulate it by resorting to a worst-case
scenario. Using similar notations used as by Umenberger
et. al. [Umenberger et al., 2019]. We denote Z = W K(cid:62),Y =
KW K(cid:62), and

Ξ =

(cid:20) W Z
Z(cid:62) Y

(cid:21)

.

(6)

The non-convex semideﬁnite constraint in Eq. 5 can be
rewritten as a convex one as

Θ = {X : (X (cid:62) − µ)(cid:62)D(X (cid:62) − µ) ≤ I},

W (cid:23) XΞX (cid:62) + σ 2I,

(7)

where according to the the Schur complement, it can be
further rewritten as
(cid:20) I

(cid:21)

σ I

σ I W − XΞX (cid:62)

(cid:23) 0.

The second challenge of the uncertain parameters can be
handled through the use of the worst-case formulation as

trace

min
W

s.t.

(cid:18)(cid:20) Q 0
0 R

(cid:21)

(cid:19)

Ξ

(cid:20) I

σ I

σ I W − XΞX (cid:62)

(cid:21)

(cid:23) 0,

(8)

(X (cid:62) − µ)(cid:62)D(X (cid:62) − µ) ≤ I,

where Ξ is deﬁned in Eq. 6. In order to further rewrite the
uncertain constraints in Eq. 8, we follow similar derivations
from [Umenberger et al., 2019, Luo et al., 2004]. We use
the following Theorem from [Luo et al., 2004] (Theorem
3.7).

Theorem 1 The data matrices (A , B, C , D, F , G , H )
satisfy the robust fractional quadratic matrix inequalities

(cid:20)

H

F + G X

(F + G X )(cid:62) C + X (cid:62)B + B(cid:62)X + X (cid:62)A X
I − X (cid:62)DX (cid:23) 0,

∀X such that

(cid:21)

(cid:23) 0,

if and only if there is λ ≥ 0 such that





F

H
F (cid:62) C − λ I
G (cid:62)

B

G
B(cid:62)
A + λ D



 (cid:23) 0.

We use substitutions similar to the one in [Umenberger et al.,
2019]: A = −Ξ, B = Ξ[ ¯A, ¯B](cid:62), C = W − [ ¯A, ¯B]Ξ[ ¯A, ¯B](cid:62),
D = D, F = σ I, G = 0, H = I. In addition, we substitute
X = X − µ with the deﬁnition of X = [A, B] = [ ¯A, ¯B] − X .

As a ﬁnal result of the substitutions, we receive the program
in Eq. 8 as a SDP,

trace

min
Ξ,λ

(cid:18)(cid:20) Q 0
R
0

(cid:21)

(cid:19)

Ξ

I
σ I W − [ ¯A
0

Ξ[ ¯A

σ I
¯B]Ξ[ ¯A
¯B](cid:62)

¯B](cid:62) [ ¯A

0
¯B]Ξ(cid:62)
λ D − Ξ



 (cid:23) 0,

(9)





s.t.

λ ≥ 0.

Solving the above cone program, we receive the optimal W ,
that helps to reconstruct the policy as K = Z(cid:62)W −1. We are
interested in computing the derivative:

∂ l
∂ θ

∂ l
∂ K
where θ = [ ¯A, ¯B, Q, R, D, σ ] are the robust LQR’s parame-
ters. The scalar function l is the task objective depending

∂ K
∂W

∂W
∂ θ

(10)

=

on the policy (parameterized by K), i.e. imitation learning
cost. The middle part in R.H.S can be computed easily, ac-
cording to the deﬁnition of the resulting policy where W is
the optimal solution of the SDP problem. The last derivative
can be computed by differentiating through a cone program,
for which we can utilize the general approach proposed in
a recent work by Agrawal et. al. [Agrawal et al., 2019b],
differentiating through a cone program. Our practical imple-
mentation needs the introduction of an auxiliary variable S,
with an additional constraint S = Ξ[ ¯A, ¯B]. An extension to
modeling also the uncertainty Q and R is simple, which has
little intuition so we do not consider in this work.

5 EXPERIMENTS

In this section, we evaluate the differentiable robust LQR
layers, i.e. the LMI-based LQR (LMI-LQR) and LMI-based
robust LQR (LMI-Robust-LQR) layers proposed in 4.1 and
4.2, in terms of their performance and uncertainty han-
dling capabilities in comparisons to other baseline methods.
Our main contender is the differentiable LQR framework
(mpc.pytorch) [Amos et al., 2018]. Both LMI-LQR and
mpc.pytorch can be considered as a solver for the nominal
system. Our implementation is based on PyTorch and uses
cvxpylayers [Agrawal et al., 2019a] as the main differen-
tiable solver for both LMI-LQR and LMI-Robust-LQR lay-
ers (where a SCS cone programming solver [O’Donoghue
et al., 2016] is chosen by default).

5.1

IMITATION LEARNING ON ROBUST LQR

In this section we design a robust LQR task with dynam-
ics as deﬁned in Eq. 1. The expert is parameterized with
a robust LQR controller. This assumption also reﬂects the
reality since under many uncertain scenarios in nature hu-
man behave following to some worst-case strategies [Lip-
shitz and Strauss, 1997]. Three learners are mpc.pytorch,
LMI-LQR, and LMI-Robust-LQR with different parame-
terization. We assume that the quadratic costs Q = R = I,
state and control dimensions n = 3, m = 3 and the variance
of the noise σ = 0.1 are known to the learners. The learn-
ers are supposed to learn the the parameters ¯A and ¯B of
the linear system dynamics and the model uncertainty D.
Note that mpc.pytorch and LMI-LQR can not learn D. The
experts are generated from random ¯A, ¯B, D (where we ad-
ditionally control their stability), depending on a random
seed. All algorithms use the same initialization for ¯A, ¯B,
D. All algorithms are implemented using PyTorch with the
following settings: RMSprop optimizer (momentum = 0.5,
learning rate = 0.01), a minibatch of 16 trajectories. Given
demonstration data D = {τ ∗
i }N
i=1, all algorithms use a sim-
ilar imitation objective for training, l = 1
i − τi(cid:107)2.
For validation, we generate separate N = 32 trajectories
τ ∗
i using the expert’s robust LQR policy. Each generated

(cid:107)D(cid:107) ∑i (cid:107)τ ∗

trajectory τi is conditioned on only an initial state x∗
0 that
initiated the expert trajectory τ ∗
i . The model loss is deﬁned
as a l2 norm of the difference between the estimate vs. the
ground-truth: e.g. ¯A, ¯B vs. ¯A∗, ¯B∗ (scenario 1), or D vs. D∗
(scenario 2). This loss is used only for reporting, which
measures how the model-based module works based purely
on a model-free loss (the imitation loss). The validation
cost is obtained by running optimized controllers on the true
stochastic dynamics and a known uncertainty set.

All results are averaged over ten random seeds. Experiments
are run on an Intel i7 CPU (2.6Ghz, 12 core).

Scenario 1: Known ¯A, ¯B, unknown model uncertainty D
In this scenario only D has to be learned. Since the differen-
tiable LQR framework [Amos et al., 2018] and LMI-based
LQR layer do not have uncertainty modeling, they do not re-
quire training. On the other hand, we train LMI-based robust
LQR for 200 iterations to optimize the imitation loss w.r.t
parameter D, where we assume both the true D∗ and D are
diagonal. We ﬁrst evaluate the solving time of different algo-
rithms. Table 1 shows the total computation time (forward
passes) of three algorithms on different horizon lengths. The
results show that our differentiable inﬁnite-horizon (robust)
LQR layer is more computationally efﬁcient by a factor of
the horizon length.

Table 2 (1st row: S1) shows the performance comparison of
optimal controllers found by the three algorithms in terms of
the validation cost. Only the LMI-based robust LQR layer
algorithm requires training, so the performance of the ﬁ-
nal controller at convergence is reported. This result shows
that mpc.pytorch performs poorly because it is not optimiz-
ing a controller that can be robust under uncertainty. On
contrary, the differentiable LMI-based LQR layer method
which incorporates robust control constraints via a LMI for-
mulation performs much better. This shows the beneﬁt of
using a robust control constraint to stabilize the optimized
controller as output. As this simple approach does not model
uncertainty directly, therefore it can only stabilize the output
controller within small bounded perturbations. In this ex-
periment, the environment uncertainty is set to a high value,
therefore we can see in Table 2 (1st row: S1) the perfor-
mance of LMI-LQR is not optimal. This drawback is ad-
dressed by the LMI-based robust LQR layer method where
its validation cost is signiﬁcantly better than mpc.pytorch
and LMI-LQR. This performance level is equal to the opti-
mal cost received by running an optimal worse-case policy
found on the true model. The plots of the model and imita-
tion losses are reported in Figure 1. Similar to ﬁndings in
mpc.pytorch [Amos et al., 2018], the imitation loss might
converge to a local optima, while there are possible diver-
gences of the the model loss. This shows the challenges of
optimizing a highly non-linear layer in which its weights
are from parameters of an optimal control program.

Table 1: Running time (in seconds) for solving a batch of 128
sampled LQR problems, with a varying length of horizon.

Time horizon
mpc.pytorch
LMI-LQR

10
15
0.99
LMI-Robust-LQR 1.67

50
71.9
0.99
1.67

100
139.9
0.99
1.67

Figure 1: Model and imitation losses for 10 different runs
(each plot corresponds to one run).

Scenario 2: Known uncertainty D, unknown model ¯A, ¯B
In this scenario,we evaluate the performance of the different
algorithms on imitation learning, where D is known and
¯A, ¯B are supposed to be learnt. All algorithms are initial-
ized randomly with the same stable ¯A, ¯B, with a horizon
length of 20. Figure 2 shows the imitation and model losses
of mpc.pytorch (right), LMI-LQR (middle), LMI-Robust-
LQR (left). While the model losses may not be suitable
to assess the performance of three differentiable layers (a
similar observation was made in [Amos et al., 2018]), the
imitation losses can reﬂect the actual quality of the controls,
which are generated by these layers. The results show that
mpc.pytorch converges only to a local optima that still has
a large imitation loss. While LMI-LQR can achieve an op-
timal imitation loss, its optimized controller is not robust
enough as reﬂected by its validation cost in Table 1 (2nd
row: S1). LMI-Robust-LQR leads to controller that is more
robust because it has an ability to learn model uncertainties.

5.2 CONVEX APPROXIMATE DYNAMIC

PROGRAMMING

In this section we evaluate our methods with full func-
tionalities, i.e. we differentiate and learn all parameters:

Table 2: Validation cost evaluated on the true dynamics,
with a horizon set to 50. Both the mean cost and its standard
deviation over 10 different runs are reported.

mpc.pytorch
44.1 ± 1.6
124.5 ± 10.2

LMI-LQR
41.2 ± 1.4
67.4 ± 18.7

LMI-Robust-LQR
10.9 ± 0.5
11.8 ± 2.3

S1
S2

0255075100125150175200iteration0.000000.000250.000500.000750.001000.001250.001500.001750.00200model loss0255075100125150175200iteration0.000000.000050.000100.000150.000200.000250.00030imitation lossFigure 2: Model and imitation losses (5 runs) for: mpc.pytorch (left), LMI-LQR (middle), LMI-Robust-LQR (right).

{A, B, Q, R} in LMI-LQR layer and { ¯A, ¯B, Q, R, D, σ } in
LMI-Robust-LQR. For this purpose, we evaluate the pro-
posed algorithms on an uncertain stochastic optimal control
(SOC) problem,

minimizeu=π(x)

limT →∞E

(cid:34)

1
T

T −1
∑
t=0

(cid:35)

(cid:107)xt (cid:107)2

2 + (cid:107)ut (cid:107)2
2

s.t.

xt+1 = Axt + But + wt
wt ∈ N (0, σ 2In),
(cid:16)
X (cid:62) − µ

[A, B] ∈ {A, B :

x0 ∼ P0,
(cid:17)(cid:62)

D

(11)

(cid:16)
X (cid:62) − µ

(cid:17)

≤ I}

where X denotes [A, B], and µ = [ ¯A, ¯B]. This problem in-
troduces an uncertainty set D over model parameters A, B,
which is different from a similar problem for a nominal dy-
namic system considered by Agrawal et. al. [Agrawal et al.,
2019a]. We evaluate three different policy parameterization
that are all based on the Lyapunov stability theory [Boyd
et al., 1994], and three simple baselines that do not have
optimal control base.

Simple baselines We use three baseline methods that do
not use differentiable optimal control layers, denoted as
u = fθ (x): (1) a linear controller u = Kx, where θ = K
is a parameterized feedback gain matrix; 2) fθ is a multi-
layer perceptron (MLP) with two hidden layers of shape
(32, 32) using ReLU activations; and 3) fθ is a long-short
term memory network (LSTM) (a recurrent controller) with
one 64-unit hidden layer. All three baselines use the Adam
optimizer with a step-size α = 1e − 4.

Optimal control-based policies Besides our two propos-
als LMI-LQR, LMI-Robust-LQR, the next baseline is an
approximate dynamic programming policy (ADP) from

[Agrawal et al., 2019a]. They proposed to use a quadratic
control-Lyapunov policy as output of a differentiable convex
layer. This convex layer is designed to solve the following
second-order cone program (SOCP) subject to a constraint
of bounded controls,

minimizeu
s.t.

t Qu + q(cid:62)u

u(cid:62)Pu + x(cid:62)
(cid:107)u(cid:107) ≤ 1

(12)

This SOCP policy receives u as variable, and {P, Q, q, xt } as
parameters. Using a differentiable convex layer [Agrawal
et al., 2019a], we can differentiate through the program
in Eq. 12. As an alternative approach, our two proposed
differentiable LQR layers are based on the Lyapunov in-
equality where we parameterize the policy directly, π is the
optimal solution of the program deﬁned in Eq. 1. Our differ-
entiable layers receive K (as deﬁned in Section 4.1) as vari-
able (where T is the horizon length), and {A, B, Q, R} (for
LMI-LQR layer) and { ¯A, ¯B, Q, R, D, σ } (for LMI-Robust-
LQR) as parameters. Note that both the LMI-LQR layer and
the SOCP policy cannot model the uncertainty set directly,
hence they are just solvers for a nominal system (without
uncertainty).

We use state and action dimensions n = m = 3, a time hori-
zon of T = 20, a batch size of 64 randomly initial states,
and run for 200 update iterations. We randomize different
SOC problems in Eq. 11: the dynamics model ¯A, ¯B are ini-
tialized with Lyapunov-stable matrices, a random diagonal
matrix D with diagonal entries, and a ﬁxed noise variance
σ = 0.1. All algorithms use the same RMSprop optimizer
with their optimized learning rate and decay rate, i.e. using
grid-search. The parameters of the SOCP policy are initial-
ized as in the original paper of Agrawal et. al. [Agrawal
et al., 2019a] where {P, Q, q} are initialized with the exact

0255075100125150175200iteration1.002.003.004.005.006.007.008.00model loss0255075100125150175200iteration0.500.751.001.251.501.752.002.252.50model loss0255075100125150175200iteration0.400.600.801.001.201.40model loss0255075100125150175200iteration1.301.401.501.601.70imitation loss0255075100125150175200iteration0.000.501.001.502.00imitation loss0255075100125150175200iteration0.000.030.050.080.100.130.150.180.20imitation lossFigure 3: Average cost for robust SOC with a horizon of 20 (left and center), and 100 (right). The plot of the SOCP policy is
separated due to its large domain. The shading is the standard deviation of the mean estimator.

LQR solution of the program in Eq. 12 without constraints
on actions u. We initialize the parameters { ¯A, ¯B, Q, R, D, σ }
randomly (using Gaussian distributions) for the LMI-LQR
and LMI-Robust-LQR layers. The evaluations are averaged
over 10 different random seeds. We report the mean cost
and its standard deviation.

Results Figure 3 reports the average cost optimized by
each algorithm. While the ﬁrst simple baseline diverges on
both tasks with a horizon of 20 and 100, hence they are
not reported in the plots. Its evaluated cost (the objective in
Eq. 12) always returns nan or ∞. The second baseline using
MLP may solve the task with a horizon of 20, but shows a
very unstable behavior due to ﬂuctuated cost evaluations, i.e.
spikes as seen on the plots. As a result, it is not surprised that
it becomes diverging when the horizon increases to 100 (we
also design this long horizon task with a higher dynamics
stochasticity σ = 0.5), hence not reported in the (right) plot.
The third baseline using LSTM is known to learn better
for tasks under uncertainty, e.g. POMDP. However on this
challenging robust control task, it performs worse than our
proposed methods in terms of sample-efﬁciency and the
ﬁnal task cost. The main reason is that LSTM is a general
neural network that can not exploit the inherent structure
of the task, i.e. uncertainty constraints like in our method.
This is further demonstrated on the right plot for the task
with a horizon of 100 in which the LSTM controller is very
unstable (in this task we set a longer horizon and a higher
variance for the stochastic dynamics).

The results also show that directly incorporating robust con-
straints in LMI-LQR and LMI-Robust-LQR in the layer,
i.e. via the Lyapunov inequality, helps achieve much lower
cost. The SOCP policy can only converge to an inferior
local policy that incurs a very high cost. A direct principled
uncertainty modeling like in LMI-Robust-LQR achieves
the best overall performance. In addition, we observe that
the total training time of one trial for each algorithm on
a personal workstation with a modern CPU is 1-2 hours
for SCOP policy, 20-30 minutes for LMI-LQR, and 30-45
minutes for LMI-Robust-LQR. This running time increases
approximately linearly with respect to the time horizon.

As the time horizon increases, the effect of uncertain model
and stochastic dynamics can lead the system to instability.
We rerun all three algorithms for 1000 training iterations.
All trials using the SOCP policy return numerical errors
within few iterations (so no plots are reported for the SOCP
policy). The validation costs for LMI-LQR and LMI-Robust-
LQR are reported in Fig. 3 (right). The results show that
there is an extreme effect of uncertainty and stochasticity to
training. While LMI-LQR seems to gradually attenuate the
effect of perturbations, it might take the training very long
to achieve a stable result. In contrary, LMI-Robust-LQR
reduces the effect of uncertainty well. Its ﬁnal controller is
less ﬂuctuating and starts becoming stable.

6 CONCLUSION

This paper proposes a new differentiable optimal control-
based layer. Our approach is motivated by recent work on
using convex optimization inside neural network layers. We
directly use a robust optimal control optimization program
as a differentiable layer that can be incorporated in stan-
dard end-to-end neural networks. Our main contribution,
which differentiates our work from the current state-of-the-
art, is to model uncertainty and stochasticity directly inside
a differentiable optimal control layer. As a result, our pro-
posed optimal control layers can optimize a controller as
output that is stable and robust to perturbations and noises
residing in demonstration data. Our layer can be integrated
as a differentiable controller in reinforcement learning and
learning from demonstration on tasks where uncertainty and
perturbations are present.

Using an inﬁnite-horizon LQR setting has helped accelerate
the computation time of both forward and backward passes
signiﬁcantly. This important achievement would open a va-
riety of more applications for the differentiable LQR/MPC
layers, for example on practical robotics tasks or hybrid
model-based and model-free policy optimization. One main
drawback of this setting is that the output is a linear con-
troller that will have limitations on highly non-linear control
tasks. One potential research direction can look at extend-

0255075100125150175200iterations100.00200.00300.00400.00500.00average costSOCP Policy0255075100125150175200iterations2.004.006.008.0010.0012.0014.0016.0018.0020.00average costLMI-Robust-LQRLMI-LQRLSTMMLP02004006008001000iterations10.0020.0030.0040.0050.00average costLMI-Robust-LQRLMI-LQRLSTMing to robust time-dependent and ﬁnite-horizon settings for
either LQR or MPC control theory.

Acknowledgements

Brieﬂy acknowledge people and organizations here.

All acknowledgements go in this section.

References

Yasin Abbasi-Yadkori and Csaba Szepesvári. Regret bounds
for the adaptive control of linear quadratic systems. In
Proceedings of the 24th Annual Conference on Learning
Theory, pages 1–26, 2011.

Joshua Achiam, David Held, Aviv Tamar, and Pieter
Abbeel. Constrained policy optimization. arXiv preprint
arXiv:1705.10528, 2017.

Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen
Boyd, Steven Diamond, and J Zico Kolter. Differentiable
convex optimization layers. In Advances in Neural Infor-
mation Processing Systems, pages 9558–9570, 2019a.

Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Bus-
seti, and Walaa M Moursi. Differentiating through a conic
program. arXiv preprint arXiv:1904.09043, 2019b.

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Chris-
tiano, John Schulman, and Dan Mané. Concrete problems
in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Brandon Amos and J Zico Kolter. Optnet: Differentiable
optimization as a layer in neural networks. In Proceed-
ings of the 34th International Conference on Machine
Learning-Volume 70, pages 136–145. JMLR. org, 2017.

Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks,
Byron Boots, and J. Zico Kolter. Differentiable MPC for
end-to-end planning and control. In Advances in Neural
Information Processing Systems 31, pages 8299–8310,
2018.

Robert R Bitmead and Michel Gevers. Riccati difference
and differential equations: Convergence, monotonicity
and stability. In The Riccati Equation, pages 263–291.
Springer, 1991.

Stephen Boyd, Laurent El Ghaoui, Eric Feron, and
Venkataramanan Balakrishnan. Linear matrix inequalities
in system and control theory. SIAM, 1994.

Eduardo F Camacho and Carlos Bordons Alba. Model
predictive control. Springer Science & Business Media,
2013.

Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and
David K Duvenaud. Neural ordinary differential equa-
tions. In Advances in neural information processing sys-
tems, pages 6571–6583, 2018.

Alon Cohen, Avinatan Hassidim, Tomer Koren, Nevena
Lazic, Yishay Mansour, and Kunal Talwar. Online lin-
ear quadratic control. arXiv preprint arXiv:1806.07104,
2018.

Stefano P Coraluppi and Steven I Marcus. Risk-sensitive
and minimax control of discrete-time, ﬁnite-state markov
decision processes. Automatica, 35(2):301–309, 1999.

Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht,
and Stephen Tu. Regret bounds for robust adaptive control
of the linear quadratic regulator. In Advances in Neural
Information Processing Systems 31, pages 4192–4201,
2018.

Sarah Dean, Stephen Tu, Nikolai Matni, and Benjamin
Recht. Safely learning to control the constrained linear
quadratic regulator. In 2019 American Control Confer-
ence (ACC), pages 5582–5588. IEEE, 2019.

Asen L Dontchev and R Tyrrell Rockafellar. Implicit func-
tions and solution mappings, volume 543. Springer, 2009.

Priya L. Donti, Melrose Roderick, Mahyar Fazlyab, and
J. Zico Kolter. Enforcing robust control guarantees within
neural network policies. CoRR, abs/2011.08105, 2020.
URL https://arxiv.org/abs/2011.08105.

James Anderson, John C Doyle, Steven H Low, and Niko-
lai Matni. System level synthesis. Annual Reviews in
Control, 47:364–393, 2019.

Laurent El Ghaoui, Francois Oustry, and Hervé Lebret. Ro-
bust solutions to uncertain semideﬁnite programs. SIAM
Journal on Optimization, 9(1):33–52, 1998.

Venkataramanan Balakrishnan and Lieven Vandenberghe.
Semideﬁnite programming duality and linear time-
invariant systems. IEEE Transactions on Automatic Con-
trol, 48(1):30–41, 2003.

Alberto Bemporad and Manfred Morari. Robust model pre-
dictive control: A survey. In Robustness in identiﬁcation
and control, pages 207–226. Springer, 1999.

Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and
Shimon Whiteson. Treeqn and atreec: Differentiable tree-
structured models for deep reinforcement learning. In 6th
International Conference on Learning Representations,
ICLR. OpenReview.net, 2018.

AA Feldbaum. Dual control theory. i. Avtomatika i Tele-

mekhanika, 21(9):1240–1249, 1960.

Javier Garcıa and Fernando Fernández. A comprehensive
survey on safe reinforcement learning. Journal of Ma-
chine Learning Research, 16(1):1437–1480, 2015.

Arthur Mensch and Mathieu Blondel. Differentiable dy-
namic programming for structured prediction and atten-
tion. arXiv preprint arXiv:1802.03676, 2018.

Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Ville-
gas, David Ha, Honglak Lee, and James Davidson. Learn-
ing latent dynamics for planning from pixels. In Inter-
national Conference on Machine Learning, pages 2555–
2565, 2019.

Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mo-
hammad Norouzi. Dream to control: Learning behaviors
by latent imagination. In 8th International Conference on
Learning Representations, ICLR. OpenReview.net, 2020.

G Hewer. An iterative technique for the computation of
the steady state gains for the discrete optimal regulator.
IEEE Transactions on Automatic Control, 16(4):382–384,
1971.

Garud N Iyengar. Robust dynamic programming. Mathe-
matics of Operations Research, 30(2):257–280, 2005.

Rudolf Emil Kalman et al. Contributions to the theory of
optimal control. Bol. soc. mat. mexicana, 5(2):102–119,
1960.

Peter Karkus, David Hsu, and Wee Sun Lee. Qmdp-net:
Deep learning for planning under partial observability.
In Advances in Neural Information Processing Systems,
pages 4694–4704, 2017.

Sergey Levine and Vladlen Koltun. Guided policy search.
In Proceedings of the 30th International Conference on
Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21
June 2013, volume 28 of JMLR Workshop and Conference
Proceedings, pages 1–9. JMLR.org, 2013.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforce-
ment learning. In 4th International Conference on Learn-
ing Representations, ICLR, 2016.

Raanan Lipshitz and Orna Strauss. Coping with uncertainty:
A naturalistic decision-making analysis. Organizational
behavior and human decision processes, 69(2):149–163,
1997.

Zhi-Quan Luo, Jos F Sturm, and Shuzhong Zhang. Multi-
variate nonnegative quadratic mappings. SIAM Journal
on Optimization, 14(4):1140–1162, 2004.

Daniel J Mankowitz, Nir Levine, Rae Jeong, Abbas Abdol-
maleki, Jost Tobias Springenberg, Timothy Mann, Todd
Hester, and Martin Riedmiller. Robust reinforcement
learning for continuous control with model misspeciﬁca-
tion. arXiv preprint arXiv:1906.07516, 2019.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Mar-
tin Riedmiller. Playing atari with deep reinforcement
learning. In NIPS Deep Learning Workshop. 2013.

Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and
Sergey Levine. Neural network dynamics for model-
based deep reinforcement learning with model-free ﬁne-
In 2018 IEEE International Conference on
tuning.
Robotics and Automation (ICRA), pages 7559–7566.
IEEE, 2018.

B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Conic
optimization via operator splitting and homogeneous self-
dual embedding. Journal of Optimization Theory and
Applications, 169(3):1042–1068, June 2016. URL http:
//stanford.edu/~boyd/papers/scs.html.

Junhyuk Oh, Satinder Singh, and Honglak Lee. Value pre-
In Advances in Neural Information

diction network.
Processing Systems, pages 6118–6128, 2017.

Fabian Otto, Philipp Becker, Ngo Anh Vien, Hanna Car-
olin Ziesche, and Gerhard Neumann. Differentiable trust
region layers for deep reinforcement learning. CoRR,
abs/2101.09207, 2021. URL https://arxiv.org/
abs/2101.09207.

Jens Schreiter, Duy Nguyen-Tuong, Mona Eberts, Bastian
Bischoff, Heiner Markert, and Marc Toussaint. Safe
exploration for active learning with gaussian processes.
In Machine Learning and Knowledge Discovery in
Databases - European Conference, ECML PKDD, vol-
ume 9286 of Lecture Notes in Computer Science, pages
133–149. Springer, 2015.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jor-
dan, and Philipp Moritz. Trust region policy optimization.
In International conference on machine learning, pages
1889–1897, 2015.

David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul,
Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David
Reichert, Neil Rabinowitz, Andre Barreto, et al. The
predictron: End-to-end learning and planning. In Inter-
national Conference on Machine Learning, pages 3191–
3199. PMLR, 2017.

Wen Sun. Towards Generalization and Efﬁciency in Rein-
forcement Learning. PhD thesis, Carnegie Mellon Uni-
versity, 2019.

Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and
Pieter Abbeel. Value iteration networks. In Advances
in Neural Information Processing Systems, pages 2154–
2162, 2016.

Jack Umenberger, Mina Ferizbegovic, Thomas B Schön,
and Håkan Hjalmarsson. Robust exploration in linear
quadratic reinforcement learning. In Advances in Neural
Information Processing Systems, pages 15310–15320,
2019.

Lieven Vandenberghe, Stephen Boyd, and Mehrdad Noural-
ishahi. Robust linear programming and optimal control.
IFAC Proceedings Volumes, 35(1):271–276, 2002.

Avishai Weiss and Stefano Di Cairano. Robust dual control
mpc with guaranteed constraint satisfaction. In 53rd IEEE
Conference on Decision and Control, pages 6713–6718.
IEEE, 2014.

Tianhao Zhang, Gregory Kahn, Sergey Levine, and Pieter
Abbeel. Learning deep control policies for autonomous
aerial vehicles with mpc-guided policy search. In 2016
IEEE international conference on robotics and automa-
tion (ICRA), pages 528–535. IEEE, 2016.

