TensorFlow Quantum:
A Software Framework for Quantum Machine Learning

Michael Broughton,1, 5, ‚àó Guillaume Verdon,1, 2, 4, 6, ‚Ä† Trevor McCourt,1, 7 Antonio J. Martinez,1, 2, 4, 8
Jae Hyeon Yoo,2, 3 Sergei V. Isakov,1 Philip Massey,3 Ramin Halavati,3 Murphy Yuezhen Niu,1
Alexander Zlokapa,9, 1 Evan Peters,4, 6, 10 Owen Lockwood,11 Andrea Skolik,12, 13, 14, 15 SoÔ¨Åene Jerbi,16
Vedran Dunjko,13 Martin Leib,12 Michael Streif,12, 14, 15, 17 David Von Dollen,18 Hongxiang Chen,19, 20
Shuxiang Cao,19, 21 Roeland Wiersema,22, 23 Hsin-Yuan Huang,1, 24, 25 Jarrod R. McClean,1 Ryan
Babbush,1 Sergio Boixo,1 Dave Bacon,1 Alan K. Ho,1 Hartmut Neven,1 and Masoud Mohseni1, ‚Ä°

1Google Quantum AI, Mountain View, CA
2Sandbox@Alphabet, Mountain View, CA
3Google, Mountain View, CA
4Institute for Quantum Computing, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
5School of Computer Science, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
6Department of Applied Mathematics, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
7Department of Mechanical & Mechatronics Engineering,
University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
8Department of Physics & Astronomy, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
9Division of Physics, Mathematics and Astronomy, Caltech, Pasadena, CA 91125
10Fermi National Accelerator Laboratory, P.O. Box 500, Batavia, IL, 605010
11Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
12Data:Lab, Volkswagen Group, Ungererstr. 69, 80805 M√ºnchen, Germany
13Leiden University, Niels Bohrweg 1, 2333 CA Leiden, Netherlands
14Quantum ArtiÔ¨Åcial Intelligence Laboratory, NASA Ames Research Center (QuAIL)
15USRA Research Institute for Advanced Computer Science (RIACS)
16Institute for Theoretical Physics, University of Innsbruck, Technikerstr. 21a, A-6020 Innsbruck, Austria
17University Erlangen-N√ºrnberg (FAU), Institute of Theoretical Physics, Staudtstr. 7, 91058 Erlangen, Germany
18Volkswagen Group Advanced Technologies, San Francisco, CA 94108
19Rahko Ltd., Finsbury Park, London, N4 3JP, United Kingdom
20Department of Computer Science, University College London, WC1E 6BT, United Kingdom
21Clarendon Laboratory, Department of Physics, University of Oxford. Oxford, OX1 3PU, United Kingdom
22Vector Institute, MaRS Centre, Toronto, Ontario, M5G 1M1, Canada
23Department of Physics and Astronomy, University of Waterloo, Ontario, N2L 3G1, Canada
24Institute for Quantum Information and Matter, Caltech, Pasadena, CA, USA
25Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA, USA
(Dated: August 30, 2021)

We introduce TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of
hybrid quantum-classical models for classical or quantum data. This framework oÔ¨Äers high-level ab-
stractions for the design and training of both discriminative and generative quantum models under
TensorFlow and supports high-performance quantum circuit simulators. We provide an overview
of the software architecture and building blocks through several examples and review the theory of
hybrid quantum-classical neural networks. We illustrate TFQ functionalities via several basic appli-
cations including supervised learning for quantum classiÔ¨Åcation, quantum control, simulating noisy
quantum circuits, and quantum approximate optimization. Moreover, we demonstrate how one can
apply TFQ to tackle advanced quantum learning tasks including meta-learning, layerwise learning,
Hamiltonian learning, sampling thermal states, variational quantum eigensolvers, classiÔ¨Åcation of
quantum phase transitions, generative adversarial networks, and reinforcement learning. We hope
this framework provides the necessary tools for the quantum computing and machine learning re-
search communities to explore models of both natural and artiÔ¨Åcial quantum systems, and ultimately
discover new quantum algorithms which could potentially yield a quantum advantage.

1
2
0
2

g
u
A
6
2

]
h
p
-
t
n
a
u
q
[

2
v
9
8
9
2
0
.
3
0
0
2
:
v
i
X
r
a

CONTENTS

I. Introduction

3

‚àó mbbrough@google.com
‚Ä† gverdon@google.com
‚Ä° mohseni@google.com

A. Quantum Machine Learning
B. Hybrid Quantum-Classical Models
C. Quantum Data
D. TensorFlow Quantum

II. Software Architecture & Building Blocks

A. Cirq
B. TensorFlow
C. Technical Hurdles in Combining Cirq with

3
3
4
5

5
5
6

 
 
 
 
 
 
TensorFlow

D. TFQ architecture

1. Design Principles and Overview
2. The Abstract TFQ Pipeline for a speciÔ¨Åc

hybrid discriminator model

3. Hello Many-Worlds: Binary ClassiÔ¨Åer for

Quantum Data

E. TFQ Building Blocks

1. Quantum Computations as Tensors
2. Composing Quantum Models
3. Sampling and Expectation Values
4. DiÔ¨Äerentiating Quantum Circuits
5. SimpliÔ¨Åed Layers
6. Basic Quantum Datasets

F. High Performance Quantum Circuit

Simulation with qsim
1. Comment on the Simulation of Quantum

Circuits

2. Gate Fusion with qsim
3. Hardware-Acceleration
4. Benchmarks
5. Large-scale simulation for quantum

machine learning

6. Noise in qsim

III. Theory of Hybrid Quantum-Classical Machine

Learning
A. Quantum Neural Networks
B. Sampling and Expectations
C. Gradients of Quantum Neural Networks

1. Finite diÔ¨Äerence methods
2. Parameter shift methods
3. Stochastic Parameter Shift Gradient

Estimation

4. Adjoint Gradient Backpropagation in

Simulations

D. Hybrid Quantum-Classical Computational

Graphs
1. Hybrid Quantum-Classical Neural

Networks

E. AutodiÔ¨Äerentiation through hybrid
quantum-classical backpropagation

IV. Basic Quantum Applications

A. Hybrid Quantum-Classical Convolutional

Neural Network ClassiÔ¨Åer
1. Background
2. Implementations

B. Hybrid Machine Learning for Quantum

Control
1. Time-Constant Hamiltonian Control
2. Time-dependent Hamiltonian Control

C. Simulating Noisy Circuits

1. Background
2. Noise in Cirq and TFQ
3. Training Models with Noise

D. Quantum Approximate Optimization

1. Background

6
7
7

8

9
10
11
11
11
12
12
13

13

13
13
14
14

15
15

15
15
16
17
17
17

18

18

20

20

21

22

22
22
23

25
26
27
28
28
28
29
30
30

2. Implementation

V. Advanced Quantum Applications

A. Meta-learning for Variational Quantum

Optimization

B. Vanishing Gradients and Adaptive Layerwise

Training Strategies
1. Random Quantum Circuits and Barren

Plateaus

2. Layerwise quantum circuit learning

C. Hamiltonian Learning with Quantum Graph

Recurrent Neural Networks
1. Motivation: Learning Quantum Dynamics

with a Quantum Representation

2. Implementation

D. Generative Modelling of Quantum Mixed

States with Hybrid Quantum-Probabilistic
Models
1. Background
2. Variational Quantum Thermalizer
3. Quantum Generative Learning from

Quantum Data

E. Subspace-Search Variational Quantum

Eigensolver for Excited States: Integration
with OpenFermion
1. Background
2. Implementation

F. ClassiÔ¨Åcation of Quantum Many-Body Phase

2

31

31

32

33

33
34

35

35
36

38
38
39

41

41
41
42

Transitions
1. Background
2. Implementation

44
44
45
G. Quantum Generative Adversarial Networks 46
46

1. Background
2. Noise Suppression with Adversarial

Learning

3. Approximate Quantum Random Access

Memory

H. Reinforcement Learning with Parameterized

Quantum Circuits
1. Background
2. Policy-Gradient Reinforcement Learning

with PQCs
3. Implementation
4. Value-Based Reinforcement Learning with

PQCs

5. Quantum Environments

VI. Closing Remarks

VII. Acknowledgements

References

46

47

48
48

49
49

51
51

52

52

52

I.

INTRODUCTION

A. Quantum Machine Learning

Machine learning (ML) is the construction of algo-
rithms and statistical models which can extract infor-
mation hidden within a dataset. By learning a model
from a dataset, one then has the ability to make predic-
tions on unseen data from the same underlying probabil-
ity distribution. For several decades, research in machine
learning was focused on models that can provide theoret-
ical guarantees for their performance [1‚Äì4]. However, in
recent years, methods based on heuristics have become
dominant, partly due to an abundance of data and com-
putational resources [5].

Deep learning is one such heuristic method which has
seen great success [6, 7]. Deep learning methods are
based on learning a representation of the dataset in the
form of networks of parameterized layers. These parame-
ters are then tuned by minimizing a function of the model
outputs, called the loss function. This function quantiÔ¨Åes
the Ô¨Åt of the model to the dataset.

In parallel to the recent advances in deep learning,
there has been a signiÔ¨Åcant growth of interest in quantum
computing in both academia and industry [8]. Quan-
tum computing is the use of engineered quantum sys-
tems to perform computations. Quantum systems are
described by a generalization of probability theory al-
lowing novel behavior such as superposition and entan-
glement, which are generally diÔ¨Écult to simulate with
a classical computer [9]. The main motivation to build
a quantum computer is to access eÔ¨Écient simulation of
these uniquely quantum mechanical behaviors. Quan-
tum computers could one day accelerate computations
for chemical and materials development [10], decryption
[11], optimization [12], and many other tasks. Google‚Äôs
recent achievement of quantum supremacy [13] marked
the Ô¨Årst glimpse of this promised power.

How may one apply quantum computing to practical
tasks? One area of research that has attracted consider-
able interest is the design of machine learning algorithms
that inherently rely on quantum properties to accelerate
their performance. One key observation that has led to
the application of quantum computers to machine learn-
ing is their ability to perform fast linear algebra on a
state space that grows exponentially with the number of
qubits. These quantum accelerated linear-algebra based
techniques for machine learning can be considered the
Ô¨Årst generation of quantum machine learning (QML) al-
gorithms tackling a wide range of applications in both su-
pervised and unsupervised learning, including principal
component analysis [14], support vector machines [15], k-
means clustering [16], and recommendation systems [17].
These algorithms often admit exponentially faster solu-
tions compared to their classical counterparts on certain
types of quantum data. This has led to a signiÔ¨Åcant
surge of interest in the subject [18]. However, to apply
these algorithms to classical data, the data must Ô¨Årst

3

be embedded into quantum states [19], a process whose
scalability is under debate [20]. Additionally, many com-
mon approaches for applying these algorithms to classical
data rely on speciÔ¨Åc structure in the data that can also be
exploited by classical algorithms, sometimes precluding
the possibility of a quantum speedup [21]. Tests based
on the structure of a classical dataset have recently been
developed that can sometimes determine if a quantum
speedup is possible on that data [22]. Continuing de-
bates around speedups and assumptions make it prudent
to look beyond classical data for applications of quantum
computation to machine learning.

With the availability of Noisy Intermediate-Scale
Quantum (NISQ) processors [23], the second generation
of QML has emerged [8, 12, 18, 24‚Äì44]. In contrast to
the Ô¨Årst generation, this new trend in QML is based on
heuristic methods which can be studied empirically due
to the increased computational capability of quantum
hardware. This is reminiscent of how machine learning
evolved towards deep learning with the advent of new
computational capabilities [45]. These new algorithms
use parameterized quantum transformations called pa-
rameterized quantum circuits (PQCs) or Quantum Neu-
ral Networks (QNNs) [24, 43]. In analogy with classical
deep learning, the parameters of a QNN are then opti-
mized with respect to a cost function via either black-box
optimization heuristics [46] or gradient-based methods
[47], in order to learn a representation of the training
data. In this paradigm, quantum machine learning is the
development of models, training strategies, and inference
schemes built on parameterized quantum circuits.

B. Hybrid Quantum-Classical Models

Near-term quantum processors are still fairly small and
noisy, thus quantum models cannot disentangle and gen-
eralize quantum data using quantum processors alone.
NISQ processors will need to work in concert with clas-
sical co-processors to become eÔ¨Äective. We anticipate
that investigations into various possible hybrid quantum-
classical machine learning algorithms will be a produc-
tive area of research and that quantum computers will
be most useful as hardware accelerators, working in sym-
biosis with traditional computers. In order to understand
the power and limitations of classical deep learning meth-
ods, and how they could be possibly improved by in-
corporating parameterized quantum circuits, it is worth
deÔ¨Åning key indicators of learning performance:

Representation capacity: the model architecture has
the capacity to accurately replicate, or extract useful in-
formation from, the underlying correlations in the train-
ing data for some value of the model‚Äôs parameters.

Training eÔ¨Éciency: minimizing the cost function via
stochastic optimization heuristics should converge to an
approximate minimum of the loss function in a reasonable
number of iterations.

Inference tractability: the ability to run inference on

a given model in a scalable fashion is needed in order to
make predictions in the training or test phase.

Generalization power : the cost function for a given
model should yield a landscape where typically initialized
and trained networks Ô¨Ånd approximate solutions which
generalize well to unseen data.

In principle, any or all combinations of these at-
tributes could be susceptible to possible improvements
by quantum computation. There are many ways to com-
bine classical and quantum computations. One well-
known method is to use classical computers as outer-
loop optimizers for QNNs. When training a QNN with
a classical optimizer in a quantum-classical loop, the
overall algorithm is sometimes referred to as a Varia-
tional Quantum-Classical Algorithm. Some recently pro-
posed architectures of QNN-based variational quantum-
classical algorithms include Variational Quantum Eigen-
solvers (VQEs) [29, 48], Quantum Approximate Opti-
mization Algorithms (QAOAs) [12, 28, 49, 50], Quan-
tum Neural Networks (QNNs) for classiÔ¨Åcation [51, 52],
Quantum Convolutional Neural Networks (QCNN) [53],
and Quantum Generative Models [54]. Generally, the
goal is to optimize over a parameterized class of compu-
tations to either generate a certain low energy wavefunc-
tion (VQE/QAOA), learn to extract non-local informa-
tion (QNN classiÔ¨Åers), or learn how to generate a quan-
tum distribution from data (generative models).

It is important to note that in the standard model ar-
chitecture for these applications, the representation typ-
ically resides entirely on the quantum processor, with
classical heuristics participating only as optimizers for
the tunable parameters of the quantum model. One of
major obstacles in training of such quantum models is
known as barren plateaus
[52], which generally arises
when a network architecture is lacking any structure and
it is randomly initialized. This unusual Ô¨Çat energy land-
scape of quantum models could seriously hinder the per-
formance of both gradient-based and gradient-free opti-
mization techniques [55]. We discuss various strategies
for overcoming this issue in section V B.

While the use of classical processors as outer-loop op-
timizers for quantum neural networks is promising, the
reality is that near-term quantum devices are still fairly
noisy, thus limiting the depth of quantum circuit achiev-
able with acceptable Ô¨Ådelity. This motivates allowing as
much of the model as possible to reside on classical hard-
ware. Several applications of quantum computation have
ventured beyond the scope of typical variational quantum
algorithms to explore this combination. Instead of train-
ing a purely quantum model via a classical optimizer,
one then considers scenarios where the model itself is a
hybrid between quantum computational building blocks
and classical computational building blocks [56‚Äì59] and
is trained typically via gradient-based methods. Such
scenarios leverage a new form of automatic diÔ¨Äerentia-
tion that allows the backwards propagation of gradients
in between parameterized quantum and classical compu-
tations. The theory of such hybrid backpropagation will

4

be covered in section III C.

In summary, a hybrid quantum-classical model is a
learning heuristic in which both the classical and quan-
tum processors contribute to the indicators of learning
performance deÔ¨Åned above.

C. Quantum Data

Although it is not yet proven that heuristic QML can
provide a speedup on practical classical ML applications,
there is growing evidence that hybrid quantum-classical
machine learning applications on ‚Äúquantum data‚Äù can
provide a quantum advantage over classical-only machine
learning for reasons described below. These results mo-
tivate the development of software frameworks that can
combine coherent access to quantum data with the power
of machine learning.

Abstractly, any data emerging from an underlying
quantum mechanical process can be considered quan-
tum data. This can be the classical data resulting from
quantum mechanical experiments [22], or data which is
directly generated by a quantum device and then fed
into an algorithm as input [60]. A quantum or hybrid
quantum-classical model will be at least partially repre-
sented by a quantum device, and therefore have the inher-
ent capacity to capture the characteristics of a quantum
mechanical process. Concretely, we list practical exam-
ples of classes of quantum data, which can be routinely
generated or simulated on existing quantum devices or
processors:

Quantum simulations: These can include output states
of quantum chemistry simulations used to extract in-
formation about chemical structures and chemical reac-
tions [61]. Potential applications include material sci-
ence, computational chemistry, computational biology,
and drug discovery. Another example is data from quan-
tum many-body systems and quantum critical systems in
condensed matter physics, which could be used to model
and design exotic states of matter which exhibit many-
body quantum eÔ¨Äects.

Quantum communication networks: Machine learn-
ing in this class of systems will be related to distilling
small-scale quantum data; e.g., to discriminate among
non-orthogonal quantum states [43, 62], with applica-
tion to design and construction of quantum error cor-
recting codes for quantum repeaters, quantum receivers,
and puriÔ¨Åcation units, devices which are critical for the
construction of a quantum internet [63].

Quantum metrology: Quantum-enhanced high preci-
sion measurements such as quantum sensing and quan-
tum imaging are inherently done on probes that are
small-scale quantum devices and could be designed or
improved by variational quantum models [64].

Quantum control :

Variationally learning hybrid
quantum-classical algorithms can lead to new optimal
open or closed-loop control [65], calibration, and error
mitigation, correction, and veriÔ¨Åcation strategies [66] for

near-term quantum devices and quantum processors.

Of course, this is not a comprehensive list of quantum
data. We hope that, with proper software tooling, re-
searchers will be able to Ô¨Ånd applications of QML in all
of the above areas and other categories of applications
beyond what we can currently envision.

D. TensorFlow Quantum

Today, exploring new hybrid quantum-classical mod-
els is a diÔ¨Écult and error-prone task. The engineering
eÔ¨Äort required to manually construct such models, de-
velop quantum datasets, and set up training and valida-
tion stages decreases a researcher‚Äôs ability to iterate and
discover. TensorFlow has accelerated the research and
understanding of deep learning in part by automating
common model building tasks. Development of software
tooling for hybrid quantum-classical models should simi-
larly accelerate research and understanding for quantum
machine learning.

To develop such tooling, the requirement of accommo-
dating a heterogeneous computational environment in-
volving both classical and quantum processors is key.
This computational heterogeneity suggested the need to
expand TensorFlow, which is designed to distribute com-
putations across CPUs, GPUs, and TPUs [67], to also en-
compass quantum processing units (QPUs). This project
has evolved into TensorFlow Quantum. TFQ is an inte-
gration of Cirq with TensorFlow that allows researchers
and students to simulate QPUs while designing, training,
and testing hybrid quantum-classical models, and eventu-
ally run the quantum portions of these models on actual
quantum processors as they come online. A core con-
tribution of TFQ is seamless backpropagation through
combinations of classical and quantum layers in hybrid
quantum-classical models. This allows QML researchers
to directly harness the rich set of tools already available
in TF and Keras.

The remainder of this document describes TFQ and a
selection of applications demonstrating some of the chal-
lenges TFQ can help tackle. In section II, we introduce
the software architecture of TFQ. We highlight its main
features including batched circuit execution, automated
expectation estimation, estimation of quantum gradients,
hybrid quantum-classical automatic diÔ¨Äerentiation, and
rapid model construction, all from within TensorFlow.
We also present a simple ‚ÄúHello, World" example for bi-
nary quantum data classiÔ¨Åcation on a single qubit. By
the end of section II, we expect most readers to have suf-
Ô¨Åcient knowledge to begin development with TFQ. For
readers who are interested in a more theoretical under-
standing of QNNs, we provide in section III an overview
of QNN models and hybrid quantum-classical backprop-
agation. For researchers interested in applying TFQ to
their own projects, we provide various applications in
sections IV and V. In section IV, we describe hybrid
quantum-classical CNNs for binary classiÔ¨Åcation of quan-

5

tum phases, hybrid quantum-classical ML for quantum
control, and MaxCut QAOA. In the advanced applica-
tions section V, we describe meta-learning for quantum
approximate optimization, discuss issues with vanishing
gradients and how we can overcome them by adaptive
layer-wise learning schemes, Hamiltonian learning with
quantum graph networks, quantum mixed state genera-
tion via classical energy-based models, subspace-Search
variational quantum eigensolver for excited states to il-
lustrate an integration with OpenFermion, quantum clas-
siÔ¨Åcation of quantum phase transitions, entangling quan-
tum generative adversarial networks, and quantum rein-
forcement learning.

We hope that TFQ enables the machine learning and
quantum computing communities to work together more
closely on important challenges and opportunities in the
near-term and beyond.

II. SOFTWARE ARCHITECTURE & BUILDING
BLOCKS

As stated in the introduction, the goal of TFQ is
to bridge the quantum computing and machine learn-
ing communities. Google already has well-established
products for these communities: Cirq, an open source
library for invoking quantum circuits [68], and Tensor-
Flow, an end-to-end open source machine learning plat-
form [67]. However, the emerging community of quantum
machine learning researchers requires the capabilities of
both products. The prospect of combining Cirq and Ten-
sorFlow then naturally arises.

First, we review the capabilities of Cirq and Tensor-
Flow. We confront the challenges that arise when one
attempts to combine both products. These challenges
inform the design goals relevant when building software
speciÔ¨Åc to quantum machine learning. We provide an
overview of the architecture of TFQ and describe a par-
ticular abstract pipeline for building a hybrid model for
classiÔ¨Åcation of quantum data. Then we illustrate this
pipeline via the exposition of a minimal hybrid model
which makes use of the core features of TFQ. We con-
clude with a description of our performant C++ sim-
ulator for quantum circuits and provide benchmarks of
performance on two complementary classes of dense and
sparse quantum circuits.

A. Cirq

Cirq is an open-source framework for invoking quan-
tum circuits on near term devices [68].
It contains
the basic structures, such as qubits, gates, circuits, and
measurement operators, that are required for specifying
quantum computations. User-speciÔ¨Åed quantum compu-
tations can then be executed in simulation or on real
hardware. Cirq also contains substantial machinery that
helps users design eÔ¨Écient algorithms for NISQ machines,

such as compilers and schedulers. Below we show ex-
ample Cirq code for calculating the expectation value of
ÀÜZ1 ÀÜZ2 for a Bell state:

( q1 , q2 ) = cirq . GridQubit . rect (1 ,2)
c = cirq . Circuit ( cirq . H ( q1 ) , cirq . CNOT ( q1 , q2 ) )
ZZ = cirq . Z ( q1 ) * cirq . Z ( q2 )
bell = cirq . Simulator () . simulate ( c ) . final_state
expectation = ZZ . e x p e c t a t i o n _ f r o m _ w a v e f u n c t i o n (

bell , dict ( zip ([ q1 , q2 ] ,[0 ,1]) ) )

Cirq uses SymPy [69] symbols to represent free param-
eters in gates and circuits. You replace free parame-
ters in a circuit with speciÔ¨Åc numbers by passing a Cirq
ParamResolver object with your circuit to the simulator.
Below we construct a parameterized circuit and simulate
the output state for Œ∏ = 1:

theta = sympy . Symbol ( ‚Äô theta ‚Äô)
c = cirq . Circuit ( cirq . Rx ( theta ) . on ( q1 ) )
resolver = cirq . ParamResolver ({ theta :1})
results = cirq . Simulator () . simulate (c , resolver )

B. TensorFlow

TensorFlow is a language for describing computations
as stateful dataÔ¨Çow graphs [67]. Describing machine
learning models as dataÔ¨Çow graphs is advantageous for
performance during training. First,
it is easy to ob-
tain gradients of dataÔ¨Çow graphs using backpropagation
[70], allowing eÔ¨Écient parameter updates. Second, inde-
pendent nodes of the computational graph may be dis-
tributed across independent machines, including GPUs
and TPUs, and run in parallel. These computational ad-
vantages established TensorFlow as a powerful tool for
machine learning and deep learning.

TensorFlow constructs this dataÔ¨Çow graph using ten-
sors for the directed edges and operations (ops) for the
nodes. For our purposes, a rank n tensor is simply an n-
dimensional array. In TensorFlow, tensors are addition-
ally associated with a data type, such as integer or string.
Tensors are a convenient way of thinking about data; in
machine learning, the Ô¨Årst index is often reserved for iter-
ation over the members of a dataset. Additional indices
can indicate the application of several Ô¨Ålters, e.g., in con-
volutional neural networks with several feature maps.

In general, an op is a function mapping input tensors
to output tensors. Ops may act on zero or more input
tensors, always producing at least one tensor as output.
For example, the addition op ingests two tensors and out-
puts one tensor representing the elementwise sum of the
inputs, while a constant op ingests no tensors, taking the
role of a root node in the dataÔ¨Çow graph. The combina-
tion of ops and tensors gives the backend of TensorFlow
the structure of a directed acyclic graph. A visualiza-
tion of the backend structure corresponding to a simple
computation in TensorFlow is given in Fig. 1.

6

Figure 1. A simple example of the TensorFlow computational
model. Two tensor inputs A and B are added and then mul-
tiplied against a third tensor input C, before Ô¨Çowing on to
further nodes in the graph. Blue nodes are tensor injections
(ops), arrows are tensors Ô¨Çowing through the computational
graph, and orange nodes are tensor transformations (ops).
Tensor injections are ops in the sense that they are functions
which take in zero tensors and output one tensor.

It is worth noting that this tensorial data format is
not to be confused with Tensor Networks [71], which are
a mathematical tool used in condensed matter physics
and quantum information science to eÔ¨Éciently represent
many-body quantum states and operations. Recently, li-
braries for building such Tensor Networks in TensorFlow
have become available [72], we refer the reader to the
corresponding blog post for better understanding of the
diÔ¨Äerence between TensorFlow tensors and the tensor ob-
jects in Tensor Networks [73].

The recently announced TensorFlow 2 [74] takes the
dataÔ¨Çow graph structure as a foundation and adds high-
level abstractions. One new feature is the Python func-
tion decorator @tf.function , which automatically con-
verts the decorated function into a graph computation.
Also relevant is the native support for Keras [75], which
provides the Layer and Model constructs. These ab-
stractions allow the concise deÔ¨Ånition of machine learn-
ing models which ingest and process data, all backed by
dataÔ¨Çow graph computation. The increasing levels of ab-
straction and heterogenous hardware backing which to-
gether constitute the TensorFlow stack can be visualized
with the orange and gray boxes in our stack diagram in
Fig. 4. The combination of these high-level abstractions
and eÔ¨Écient dataÔ¨Çow graph backend makes TensorFlow
2 an ideal platform for data-driven machine learning re-
search.

C. Technical Hurdles in Combining Cirq with
TensorFlow

There are many ways one could imagine combining the
capabilities of Cirq and TensorFlow. One possible ap-
proach is to let graph edges represent quantum states and
let ops represent transformations of the state, such as ap-
plying circuits and taking measurements. This approach
can be called the ‚Äústates-as-edges" architecture. We show
in Fig. 2 how to reformulate the Bell state preparation
and measurement discussed in section II A within this
proposed architecture.

7

Figure 2. The states-as-edges approach to embedding quan-
tum computation in TensorFlow. Blue nodes are input ten-
sors, arrows are tensors Ô¨Çowing through the graph, and orange
nodes are TF Ops transforming the simulated quantum state.
Note that the above is not the architecture used in TFQ but
rather an alternative which was considered, see Fig. 3 for the
equivalent diagram for the true TFQ architecture.

This architecture may at Ô¨Årst glance seem like an at-
tractive option as it is a direct formulation of quantum
computation as a dataÔ¨Çow graph. However, this ap-
proach is suboptimal for several reasons. First, in this
architecture, the structure of the circuit being run is
static in the computational graph, thus running a diÔ¨Äer-
ent circuit would require the graph to be rebuilt. This is
far from ideal for variational quantum algorithms which
learn over many iterations with a slightly modiÔ¨Åed quan-
tum circuit at each iteration. A second problem is the
lack of a clear way to embed such a quantum dataÔ¨Çow
graph on a real quantum processor: the states would
have to remain held in quantum memory on the quan-
tum device itself, and the high latency between classi-
cal and quantum processors makes sending transforma-
tions one-by-one prohibitive. Lastly, one needs a way
to specify gates and measurements within TF. One may
be tempted to deÔ¨Åne these directly; however, Cirq al-
ready has the necessary tools and objects deÔ¨Åned which
are most relevant for the near-term quantum computing
era. Duplicating Cirq functionality in TF would lead to
several issues, requiring users to re-learn how to inter-
face with quantum computers in TFQ versus Cirq, and
adding to the maintenance overhead by needing to keep
two separate quantum circuit construction frameworks
up-to-date as new compilation techniques arise. These
considerations motivate our core design principles.

D. TFQ architecture

1. Design Principles and Overview

To avoid the aforementioned technical hurdles and in
order to satisfy the diverse needs of the research commu-
nity, we have arrived at the following four design princi-
ples:

1. DiÔ¨Äerentiability. As described in the introduc-
tion, gradient-based methods leveraging autodiÔ¨Äer-
entiation have become the leading heuristic for op-
timization of machine learning models. A software
framework for QML must support diÔ¨Äerentiation of
quantum circuits so that hybrid quantum-classical
models can participate in backpropagation.

2. Circuit Batching. Learning on quantum data re-
quires re-running parameterized model circuits on

Figure 3. The TensorFlow graph generated to calculate the
expectation value of a parameterized circuit. The symbol
values can come from other TensorFlow ops, such as from
the outputs of a classical neural network. The output can be
passed on to other ops in the graph; here, for illustration, the
output is passed to the absolute value op.

each quantum data point. A QML software frame-
work must be optimized for running large num-
bers of such circuits. Ideally, the semantics should
match established TensorFlow norms for batching
over data.

3. Execution Backend Agnostic.

Experimen-
tal quantum computing often involves reconciling
perfectly simulated algorithms with the outputs of
real, noisy devices. Thus, QML software must al-
low users to easily switch between running models
in simulation and running models on real hardware,
such that simulated results and experimental re-
sults can be directly compared.

4. Minimalism. Cirq provides an extensive set of
tools for preparing quantum circuits. TensorFlow
provides a very complete machine learning toolkit
through its hundreds of ops and Keras high-level
API, with a massive community of active users. Ex-
isting functionality in Cirq and TensorFlow should
be used as much as possible. TFQ should serve as a
bridge between the two that does not require users
to re-learn how to interface with quantum comput-
ers or re-learn how to solve problems using machine
learning.

First, we provide a bottom-up overview of TFQ to
provide intuition on how the framework functions at a
fundamental level. In TFQ, circuits and other quantum
computing constructs are tensors, and converting these
tensors into classical information via simulation or exe-
cution on a quantum device is done by ops. These ten-
sors are created by converting Cirq objects to TensorFlow
string tensors, using the tfq.convert_to_tensor function.
This takes in a cirq.Circuit or cirq.PauliSum object and
creates a string tensor representation. The cirq.Circuit
objects may be parameterized by SymPy symbols.

These tensors are then converted to classical informa-
tion via state simulation, expectation value calculation,
or sampling. TFQ provides ops for each of these compu-
tations. The following code snippet shows how a simple
parameterized circuit may be created using Cirq, and
its ÀÜZ expectation evaluated at diÔ¨Äerent parameter values

using the tfq expectation value calculation op. We feed
the output into the tf.math.abs op to show that tfq ops
integrate naively with tf ops.

qubit = cirq . GridQubit (0 , 0)
theta = sympy . Symbol ( ‚Äô theta ‚Äô)
c = cirq . Circuit ( cirq . X ( qubit ) ** theta )
c_tensor = tfq . con ve rt_ to_ ten sor ([ c ] * 3)
theta_values = tf . constant ([[0] ,[1] ,[2]])
m = cirq . Z ( qubit )
paulis = tfq . con ver t_t o_t en sor ([ m ] * 3)
exp ectati on_op = tfq . g et _ ex p ec t at i on _o p ()
output = e xpectation_op (

c_tensor , [ ‚Äô theta ‚Äô] , theta_values , paulis )

abs_output = tf . math . abs ( output )

We supply the expectation op with a tensor of parame-
terized circuits, a list of symbols contained in the circuits,
a tensor of values to use for those symbols, and tensor
operators to measure with respect to. Given this, it out-
puts a tensor of expectation values. The graph this code
generates is given by Fig. 3.

The expectation op is capable of running circuits on
a simulated backend, which can be a Cirq simulator or
our native TFQ simulator qsim (described in detail in
section II F), or on a real device. This is conÔ¨Ågured on
instantiation.

The expectation op is fully diÔ¨Äerentiable. Given
that there are many ways to calculate the gradient of
a quantum circuit with respect to its input parame-
ters, TFQ allows expectation ops to be conÔ¨Ågured with
one of many built-in diÔ¨Äerentiation methods using the
tfq.Differentiator interface, such as Ô¨Ånite diÔ¨Äerencing,
parameter shift rules, and various stochastic methods.
The tfq.Differentiator interface also allows users to de-
Ô¨Åne their own gradient calculation methods for their spe-
ciÔ¨Åc problem if they desire.

The tensor representation of circuits and Paulis along
with the execution ops are all that are required to solve
any problem in QML. However, as a convenience, TFQ
provides an additional op for in-graph circuit construc-
tion. This was found to be convenient when solving prob-
lems where most of the circuit being run is static and
only a small part of it is being changed during train-
ing or inference. This functionality is provided by the
tfq.tfq_append_circuit op.
It is expected that all but
the most dedicated users will never touch these low-
level ops, and instead will interface with TFQ using our
tf.keras.layers that provide a simpliÔ¨Åed interface.

The tools provided by TFQ can interact with both core
TensorFlow and, via Cirq, real quantum hardware. The
functionality of all three software products and the inter-
faces between them can be visualized with the help of a
‚Äúsoftware-stack" diagram, shown in Fig. 4. Importantly,
these interfaces allow users to write a single TensorFlow
Quantum program which could then easily be run locally
on a workstation, in a highly parallel and distributed set-
ting at the PetaFLOP/s or higher throughput scale [22],
or on real QPU device [76].

In the next section, we describe an example of an
abstract quantum machine learning pipeline for hybrid

8

Figure 4. The software stack of TFQ, showing its interactions
with TensorFlow, Cirq, and computational hardware. At the
top of the stack is the data to be processed. Classical data
is natively processed by TensorFlow; TFQ adds the ability to
process quantum data, consisting of both quantum circuits
and quantum operators. The next level down the stack is the
Keras API in TensorFlow. Since a core principle of TFQ is
native integration with core TensorFlow, in particular with
Keras models and optimizers, this level spans the full width
of the stack. Underneath the Keras model abstractions are
our quantum layers and diÔ¨Äerentiators, which enable hybrid
quantum-classical automatic diÔ¨Äerentiation when connected
with classical TensorFlow layers. Underneath the layers and
diÔ¨Äerentiators, we have TensorFlow ops, which instantiate the
dataÔ¨Çow graph. Our custom ops control quantum circuit ex-
ecution. The circuits can be run in simulation mode, by in-
voking qsim or Cirq, or eventually will be executed on QPU
hardware.

discriminator model that TFQ was designed to support.
Then we illustrate the TFQ pipeline via a Hello Many-
Worlds example, which involves building the simplest
possible hybrid quantum-classical model for a binary
classiÔ¨Åcation task on a single qubit. More detailed in-
formation on the building blocks of TFQ features will be
given in section II E.

2. The Abstract TFQ Pipeline for a speciÔ¨Åc hybrid
discriminator model

Here, we provide a high-level abstract overview of the
computational steps involved in the end-to-end pipeline
for inference and training of a hybrid quantum-classical
discriminative model for quantum data in TFQ.

(1) Prepare Quantum Dataset: In general, this
might come from a given black-box source. However,
as current quantum computers cannot import quantum
data from external sources, the user has to specify quan-
tum circuits which generate the data. Quantum datasets
are prepared using unparameterized cirq.Circuit ob-
jects and are injected into the computational graph using
tfq.convert_to_tensor .

(2) Evaluate Quantum Model: Parameterized
quantum models can be selected from several categories

TF Keras ModelsTF Layers TF Execution Engine       TPUCirq TFQ OpsTFQ LayersTFQ DifferentiatorsTFQTensorFlowClassical hardwareQuantum hardwareTFQ qsimGPUCPUQPUTF OpsClassical Data: integers/Ô¨Çoats/stringsQuantum Data: Circuits/Operators9

is unsupervised. Wrapping the model built in stages (1)
through (4) inside a tf.keras.Model gives the user access
to all the losses in the tf.keras.losses module.

(6) Evaluate Gradients & Update Parameters:
After evaluating the cost function, the free parame-
ters in the pipeline is updated in a direction expected
to decrease the cost. This is most commonly per-
formed via gradient descent. To support gradient de-
scent, TFQ exposes derivatives of quantum operations
to the TensorFlow backpropagation machinery via the
tfq.differentiators.Differentiator interface. This allows
both the quantum and classical models‚Äô parameters to
be optimized against quantum data via hybrid quantum-
classical backpropagation. See section III for details on
the theory.

In the next section, we illustrate this abstract pipeline
by applying it to a speciÔ¨Åc example. While simple, the
example is the minimum instance of a hybrid quantum-
classical model operating on quantum data.

3. Hello Many-Worlds: Binary ClassiÔ¨Åer for Quantum
Data

Binary classiÔ¨Åcation is a basic task in machine learn-
ing that can be applied to quantum data as well. As a
minimal example of a hybrid quantum-classical model,
we present here a binary classiÔ¨Åer for regions on a sin-
gle qubit. In this task, two random vectors in the X-Z
plane of the Bloch sphere are chosen. Around these two
vectors, we randomly sample two sets of quantum data
points; the task is to learn to distinguish the two sets. An
example quantum dataset of this type is shown in Fig. 6.
The following can all be run in-browser by navigating to
the Colab example notebook at

research/binary_classifier/binary_classifier.ipynb

Additionally, the code in this example can be copy-pasted
into a python script after installing TFQ.

To solve this problem, we use the pipeline shown in
Fig. 5, specialized to one-qubit binary classiÔ¨Åcation. This
specialization is shown in Fig. 7.

The Ô¨Årst step is to generate the quantum data. We can
use Cirq for this task. The common imports required for
working with TFQ are shown below:

import cirq , random , sympy
import numpy as np
import tensorflow as tf
import t e ns o rf l ow _ qu a nt u m as tfq

The function below generates the quantum dataset; la-
bels use a one-hot encoding:

def generate_dataset (

qubit , theta_a , theta_b , num_samples ) :

q_data = []
labels = []
blob_size = abs ( theta_a - theta_b ) / 5
for _ in range ( num_samples ) :

coin = random . random ()
spread_x , spread_y = np . random . uniform (

Figure 5. Abstract pipeline for inference and training of
a hybrid discriminative model in TFQ. Here, Œ¶ represents
the quantum model parameters and Œ∏ represents the classical
model parameters.

based on knowledge of the quantum data‚Äôs structure.
The goal of the model is to perform a quantum compu-
tation in order to extract information hidden in a quan-
tum subspace or subsystem. In the case of discrimina-
tive learning, this information is the hidden label pa-
rameters. To extract a quantum non-local subsystem,
the quantum model disentangles the input data, leaving
the hidden information encoded in classical correlations,
thus making it accessible to local measurements and clas-
sical post-processing. Quantum models are constructed
using cirq.Circuit objects containing SymPy symbols,
and can be attached to quantum data sources using the
tfq.AddCircuit layer.

(3) Sample or Average: Measurement of quantum
states extracts classical information, in the form of sam-
ples from a classical random variable. The distribution
of values from this random variable generally depends on
both the quantum state itself and the measured observ-
able. As many variational algorithms depend on mean
values of measurements, TFQ provides methods for av-
eraging over several runs involving steps (1) and (2).
Sampling or averaging are performed by feeding quan-
tum data and quantum models to the tfq.Sample or
tfq.Expectation layers.

(4) Evaluate Classical Model: Once classical
information has been extracted,
it is in a format
amenable to further classical post-processing. As the
extracted information may still be encoded in classi-
cal correlations between measured expectations, clas-
sical deep neural networks can be applied to distill
such correlations. Since TFQ is fully compatible with
core TensorFlow, quantum models can be attached di-
rectly to classical tf.keras.layers.Layer objects such as
tf.keras.layers.Dense .

(5) Evaluate Cost Function: Given the results of
classical post-processing, a cost function is calculated.
This may be based on the accuracy of classiÔ¨Åcation if the
quantum data was labeled, or other criteria if the task

Prepare  Quantum DatasetEvaluate Quantum ModelSample or AverageEvaluate Classical ModelEvaluate Cost FunctionùõâEvaluate Gradients & Update Parameters10

theta = sympy . Symbol ( ‚Äô theta ‚Äô)
q_model = cirq . Circuit ( cirq . Ry ( theta ) ( qubit ) )
q_data_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

expectation = tfq . layers . PQC (

q_model , cirq . Z ( qubit ) )

e xp e ct a ti o n_ o ut p ut = expectation ( q_data_input )

The purpose of the rotation gate is to minimize the su-
perposition from the input quantum data such that we
can get maximum useful information from the measure-
ment. This quantum model is then attached to a small
classiÔ¨Åer NN to complete our hybrid model. Notice in
the code below that quantum layers can appear among
classical layers inside a standard Keras model:

classifier = tf . keras . layers . Dense (

2 , activation = tf . keras . activations . softmax )

cl ass ifi er _ou tpu t = classifier (

e xp e ct a ti o n_ o ut p ut )

model = tf . keras . Model ( inputs = q_data_input ,

outputs = c las sif ier _ou tp ut )

We can train this hybrid model on the quantum data
deÔ¨Åned earlier. Below we use as our loss function the
cross entropy between the labels and the predictions of
the classical NN; the ADAM optimizer is chosen for pa-
rameter updates.

optimizer = tf . keras . optimizers . Adam (

learning_rate =0.1)

loss = tf . keras . losses . C a t e g o r i c a l C r o s s e n t r o p y ()
model . compile ( optimizer = optimizer , loss = loss )
history = model . fit (

x = q_data , y = labels , epochs =50)

Finally, we can use our trained hybrid model to classify
new quantum datapoints:

test_data , _ = generate_dataset (

qubit , theta_a , theta_b , 1)
p = model . predict ( test_data ) [0]
print ( f " prob ( a ) ={ p [0]:.4 f } , prob ( b ) ={ p [1]:.4 f } " )

This section provided a rapid introduction to just that
code needed to complete the task at hand. The follow-
ing section reviews the features of TFQ in a more API
reference inspired style.

E. TFQ Building Blocks

Having provided a minimum working example in the
previous section, we now seek to provide more details
about the components of the TFQ framework. First, we
describe how quantum computations speciÔ¨Åed in Cirq are
converted to tensors for use inside the TensorFlow graph.
Then, we describe how these tensors can be combined in-
graph to yield larger models. Next, we show how circuits
are simulated and measured in TFQ. The core functional-
ity of the framework, diÔ¨Äerentiation of quantum circuits,
is then explored. Finally, we describe our more abstract
layers, which can be used to simplify many QML work-
Ô¨Çows.

Figure 6. Quantum data represented on the Bloch sphere.
States in category a are blue, while states in category b are
orange. The vectors are the states around which the samples
were taken. The parameters used to generate this data are:
Œ∏a = 1, Œ∏b = 4, and N = 200. The QuTiP package was used
to visualize the Bloch sphere [77].

(1) Quantum data to be classiÔ¨Åed. (2) Parame-
Figure 7.
terized rotation gate, whose job is to remove superpositions
in the quantum data. (3) Measurement along the Z axis of
the Bloch sphere converts the quantum data into classical
data. (4) Classical post-processing is a two-output SoftMax
layer, which outputs probabilities for the data to come from
category a or category b.
(5) Categorical cross entropy is
computed between the predictions and the labels. The Adam
optimizer [78] is used to update both the quantum and clas-
sical portions of the hybrid model.

- blob_size , blob_size , 2)

if coin < 0.5:

label = [1 , 0]
angle = theta_a + spread_y

else :

label = [0 , 1]
angle = theta_b + spread_y

labels . append ( label )
q_data . append ( cirq . Circuit (
cirq . Ry ( - angle ) ( qubit ) ,
cirq . Rx ( - spread_x ) ( qubit ) ) )

return ( tfq . c onv ert _t o_t ens or ( q_data ) ,

np . array ( labels ) )

We can generate a dataset and the associated labels after
picking some parameter values:

qubit = cirq . GridQubit (0 , 0)
theta_a = 1
theta_b = 4
num_samples = 200
q_data , labels = generate_dataset (

qubit , theta_a , theta_b , num_samples )

As our quantum parametric model, we use the simplest
case of a universal quantum discriminator [43, 66], a
single parameterized rotation (linear) and measurement
along the Z axis (non-linear):

11

1. Quantum Computations as Tensors

3. Sampling and Expectation Values

As pointed out in section II A, Cirq already contains
the language necessary to express quantum computa-
tions, parameterized circuits, and measurements. Guided
by principle 4, TFQ should allow direct injection of Cirq
expressions into the computational graph of TensorFlow.
This is enabled by the tfq.convert_to_tensor function.
We saw the use of this function in the quantum binary
classiÔ¨Åer, where a list of data generation circuits spec-
iÔ¨Åed in Cirq was wrapped in this function to promote
them to tensors. Below we show how a quantum data
point, a quantum model, and a quantum measurement
can be converted into tensors:

q0 = cirq . GridQubit (0 , 0)
q_data_raw = cirq . Circuit ( cirq . H ( q0 ) )
q_data = tfq . con ver t_t o_t en sor ([ q_data_raw ])

theta = sympy . Symbol ( ‚Äô theta ‚Äô)
q_model_raw = cirq . Circuit (

cirq . Ry ( theta ) . on ( q0 ) )

q_model = tfq . co nve rt_ to _te nso r ([ q_model_raw ])

q_measure_raw = 0.5 * cirq . Z ( q0 )
q_measure = tfq . co nve rt_ to_ ten sor (

[ q_measure_raw ])

This conversion is backed by our custom serializers. Once
a Circuit or PauliSum is serialized, it becomes a ten-
sor of type tf.string . This is the reason for the use
of tf.keras.Input(shape=(), dtype=tf.dtypes.string) when
creating inputs to Keras models, as seen in the quantum
binary classiÔ¨Åer example.

2. Composing Quantum Models

After injecting quantum data and quantum mod-
els into the computational graph, a custom Tensor-
Flow operation is required to combine them.
In
support of guiding principle 2, TFQ implements the
tfq.layers.AddCircuit layer for combining tensors of cir-
cuits.
In the following code, we use this functionality
to combine the quantum data point and quantum model
deÔ¨Åned in subsection II E 1:

add_op = tfq . layers . AddCircuit ()
dat a_and_ model = add_op ( q_data , append = q_model )

To quantify the performance of a quantum model on a
quantum dataset, we need the ability to deÔ¨Åne loss func-
tions. This requires converting quantum information into
classical information. This conversion process is accom-
plished by either sampling the quantum model, which
stochastically produces bitstrings according to the prob-
ability amplitudes of the model, or by specifying a mea-
surement and taking expectation values.

Sampling from quantum circuits is an important use
case in quantum computing. The recently achieved mile-
stone of quantum supremacy [13] is one such application,
in which the diÔ¨Éculty of sampling from a quantum model
was used to gain a computational edge over classical ma-
chines.

TFQ implements

tfq.layers.Sample , a Keras layer
which enables sampling from batches of circuits in sup-
port of design objective 2. The user supplies a ten-
sor of parameterized circuits, a list of symbols con-
tained in the circuits, and a tensor of values to sub-
stitute for the symbols in the circuit. Given these,
the Sample layer produces a tf.RaggedTensor of shape
[batch_size, num_samples, n_qubits] , where the n_qubits
dimension is ragged to account for the possibly varying
circuit size over the input batch of quantum data. For
example, the following code takes the combined data and
model from section II E 2 and produces a tensor of size
[1, 4, 1] containing four single-bit samples:

sample_layer = tfq . layers . Sample ()
samples = sample_layer (

data_and_model , symbol_names =[ ‚Äô theta ‚Äô] ,
symbol_values =[[0.5]] , repetitions =4)

Though sampling is the fundamental interface between
quantum and classical information, diÔ¨Äerentiability of
quantum circuits is much more convenient when using
expectation values, as gradient information can then
be backpropagated (see section III for more details).

In the simplest case, expectation values are simply av-
erages over samples. In quantum computing, expectation
values are typically taken with respect to a measurement
operator M . This involves sampling bitstrings from the
quantum circuit as described above, applying M to the
list of bitstring samples to produce a list of numbers,
then taking the average of the result. TFQ provides two
related layers with this capability:

In contrast to sampling (which is by default in the stan-
dard computational basis, the ÀÜZ eigenbasis of all qubits),
taking expectation values requires deÔ¨Åning a measure-
ment. As discussed in section II A, these are Ô¨Årst deÔ¨Åned
as cirq.PauliSum objects and converted to tensors. TFQ
implements tfq.layers.Expectation , a Keras layer which
enables the extraction of measurement expectation val-
ues from quantum models. The user supplies a tensor of
parameterized circuits, a list of symbols contained in the
circuits, a tensor of values to substitute for the symbols
in the circuit, and a tensor of operators to measure with
respect to them. Given these inputs, the layer outputs
a tensor of expectation values. Below, we show how to
take an expectation value of the measurement deÔ¨Åned in
section II E 1:

ex pec tat io n_l aye r = tfq . layers . Expectation ()
expectations = e xpe cta tio n_l aye r (
circuit = data_and_model ,
symbol_names = [ ‚Äô theta ‚Äô] ,

symbol_values = [[0.5]] ,
operators = q_measure )

In Fig. 3, we illustrate the dataÔ¨Çow graph which backs
the expectation layer, when the parameter values are sup-
plied by a classical neural network. The expectation layer
is capable of using either a simulator or a real device for
execution, and this choice is simply speciÔ¨Åed at run time.
While Cirq simulators may be used for the backend, TFQ
provides its own native TensorFlow simulator written in
performant C++. A description of our quantum circuit
simulation code is given in section II F.

Having converted the output of a quantum model into
classical information, the results can be fed into subse-
quent computations. In particular, they can be fed into
functions that produce a single number, allowing us to
deÔ¨Åne loss functions over quantum models in the same
way we do for classical models.

4. DiÔ¨Äerentiating Quantum Circuits

We have taken the Ô¨Årst steps towards implementation
of quantum machine learning, having deÔ¨Åned quantum
models over quantum data and loss functions over those
models. As described in both the introduction and our
Ô¨Årst guiding principle, diÔ¨Äerentiability is the critical ma-
chinery needed to allow training of these models. As de-
scribed in section II B, the architecture of TensorFlow is
optimized around backpropagation of errors for eÔ¨Écient
updates of model parameters; one of the core contribu-
tions of TFQ is integration with TensorFlow‚Äôs backprop-
agation mechanism. TFQ implements this functionality
with our diÔ¨Äerentiators module. The theory of quan-
tum circuit diÔ¨Äerentiation will be covered in section III C;
here, we overview the software that implements the the-
ory.

of

and

Since

dients

interface.
layers

there are many ways

tfq.differentiators.Differentiator

quantum circuits, TFQ provides

to calculate gra-
the
Our
rely on
Expectation
SampledExpectation
classes inheriting from this interface to specify how
TensorFlow should compute their gradients. While
advanced users can implement their own custom diÔ¨Äer-
entiators by inheriting from the interface, TFQ comes
with several built-in options, two of which we highlight
here. These two methods are instances of the two main
categories of quantum circuit diÔ¨Äerentiators: the Ô¨Ånite
diÔ¨Äerence methods and the parameter shift methods.

12

For the 2-local circuits implementable on near-term
hardware, methods more sophisticated than Ô¨Ånite diÔ¨Äer-
ences are possible. These methods involve running an
ancillary quantum circuit, from which the gradient of the
primary circuit with respect to some parameter can be
directly measured. One speciÔ¨Åc method is gate decompo-
sition and parameter shifting [79], implemented in TFQ
as the ParameterShift diÔ¨Äerentiator. For in-depth discus-
sion of the theory, see section III C 2.

The diÔ¨Äerentiation rule used by our layers is speciÔ¨Åed
through an optional keyword argument. Below, we show
the expectation layer being called with our parameter
shift rule:

diff = tfq . differentiators . ParameterShift ()
expectation = tfq . layers . Expectation (

differentiator = diff )

For further discussion of the trade-oÔ¨Äs when choosing
between diÔ¨Äerentiators, see the gradients tutorial on our
GitHub website:

docs/tutorials/gradients.ipynb

5. SimpliÔ¨Åed Layers

Some workÔ¨Çows do not require control as sophisticated
as our Expectation , Sample , and SampledExpectation lay-
ers allow. For these workÔ¨Çows we provide the PQC and
ControlledPQC layers. Both of these layers allow parame-
terized circuits to be updated by hybrid backpropagation
without the user needing to provide the list of symbols
associated with the circuit. The PQC layer provides au-
tomated Keras management of the variables in a param-
eterized circuit:

q = cirq . GridQubit (0 , 0)
(a , b ) = sympy . symbols ( " a b " )
circuit = cirq . Circuit (

cirq . Rz ( a ) ( q ) , cirq . Rx ( b ) ( q ) )

outputs = tfq . layers . PQC ( circuit , cirq . Z ( q ) )
quantum_data = tfq . c onv ert _to _te nso r ([

cirq . Circuit () , cirq . Circuit ( cirq . X ( q ) ) ])

res = outputs ( quantum_data )

When the variables in a parameterized circuit will be
controlled completely by other user-speciÔ¨Åed machinery,
for example by a classical neural network, then the user
can call our ControlledPQC layer:

The Ô¨Årst class of quantum circuit diÔ¨Äerentiators is the
Ô¨Ånite diÔ¨Äerence methods. This class samples the primary
quantum circuit for at least two diÔ¨Äerent parameter set-
tings, then combines them to estimate the derivative.
The ForwardDifference diÔ¨Äerentiator provides most ba-
sic version of this. For each parameter in the circuit, the
circuit is sampled at the current setting of the parame-
ter. Then, each parameter is perturbed separately and
the circuit resampled.

outputs = tfq . layers . ControlledPQC (

circuit , cirq . Z ( bit ) )

model_params = tf . co nve rt_ to_ ten so r (

[[0.5 , 0.5] , [0.25 , 0.75]])

res = outputs ([ quantum_data , model_params ])

Notice that the call is similar to that for PQC, except
that we provide parameter values for the symbols in the
circuit. These two layers are used extensively in the ap-
plications highlighted in the following sections.

13

6. Basic Quantum Datasets

F. High Performance Quantum Circuit Simulation
with qsim

A major goal of TensorFlow Quantum is to expand
the application of machine learning to quantum data.
Towards that goal, here we provide some basic labelled
datasets with the tfq.datasets module.

The Ô¨Årst dataset is tfq.datasets.excited_cluster_states .
Given a list of qubits, this function builds a dataset of
ground state and excited cluster states. The ground
state is labelled with ‚àí1, while excited states are labelled
with +1. With this data, a QML model can be trained
to distinguish between the ground and excited states
on a ring. This is the same dataset used in the QCNN
tutorial IV A.

The second dataset oÔ¨Äered is tfq.datasets.tfi_chain .
This is a 1D Transverse Ô¨Åeld Ising-model, which can be
written as

ÀÜH = ‚àí

(cid:88)

( ÀÜZj ÀÜZj+1 ‚àí g ÀÜXj).

j

This dataset contains 81 datapoints, corresponding to
the ground states of the 1D TFI chain for g in [0.2, 1.8]
in increments of 0.2. Each datapoint contains a circuit,
a label, a Hamiltonian, and some additional metadata.
The circuit prepares the approximate ground state of the
Hamiltonian in the datapoint.

This dataset can be used for many purposes. For one,
the labels can be used to train a QML model to distin-
guish diÔ¨Äerent phases of a chain. The labels are 0 for the
ferromagnetic phase (occurs for g < 1), 1 for the crit-
ical point (g == 1) and 2 for the paramagnetic phase
(g > 1). Further, the additional metadata contains the
exact ground state energy from expensive exact diago-
nalization; this can be used as a benchmark for VQE-like
models. If a QML model is successfully trained to prepare
the ground state of a Hamiltonian given in the dataset,
it should achieve the corresponding ground state energy
given in the datapoint.

The third dataset oÔ¨Äered is tfq.datasets.xxz_chain .

(cid:88)

ÀÜH =

j

( ÀÜXj ÀÜXj+1 + ÀÜYj ÀÜYj+1 + ‚àÜ ÀÜZj ÀÜZj+1)

This dataset contains 76 datapoints, corresponding to the
ground states of the 1D XXZ chain for ‚àÜ in [0.3, 1.8] in
increments of 0.2.

Similar to the TFI dataset, each datapoint contains
a circuit, a label, a Hamiltonian, and some additional
metadata. The circuit prepares the approximate ground
state of the Hamiltonian in the datapoint. The labels are
0 for the critical metallic phase (‚àÜ <= 1) and 1 for the
insulating phase (‚àÜ > 1). As such, this dataset can also
be used for classiÔ¨Åcation and optimization benchmarking.
We expect to add more datasets as consensus is reached
in the QML community around good benchmark tasks.
As such, we welcome contributions! Those looking to
contribute new quantum datasets should reach out via
our GitHub page.

Concurrently with TFQ, we are open sourcing qsim,
a software package for simulating quantum circuits on
classical computers. We have adapted its C++ imple-
mentation to work inside TFQ‚Äôs TensorFlow ops. The
performance of qsim derives from two key ideas that can
be seen in the literature on classical simulators for quan-
tum circuits [80, 81]. The Ô¨Årst idea is the fusion of gates
in a quantum circuit with their neighbors to reduce the
number of matrix-vector multiplications required when
applying the circuit to a wavefunction. The second idea
is to create a set of matrix-vector multiplication functions
speciÔ¨Åcally optimized for the application of two-qubit (or
more) gates to state vectors, to take maximal advantage
of gate fusion. We discuss these points in detail below.
To quantify the performance of qsim, we also provide an
initial benchmark comparing qsim to Cirq. We note that
qsim is signiÔ¨Åcantly faster. We further note that the qsim
benchmark times include the full TFQ software stack of
serializing a circuit to ProtoBuÔ¨Äs in Python, conversion
of ProtoBuÔ¨Äs to C++ objects inside the dataÔ¨Çow graph
for our custom TensorFlow ops, and the relaying of re-
sults back to Python.

1. Comment on the Simulation of Quantum Circuits

To motivate the qsim fusing algorithm, consider how
circuits are applied to states in simulation. Suppose
we wish to apply two gates ÀÜG1 and ÀÜG2 to our initial
state |œà(cid:105), and suppose these gates act on the same two
qubits. Since gate application is associative, we have
( ÀÜG2 ÀÜG1) |œà(cid:105) = ÀÜG2( ÀÜG1 |œà(cid:105)). However, as the number of
qubits n supporting |œà(cid:105) grows, the left side of the equal-
ity becomes much faster to compute. This is because
applying a gate to a state requires broadcasting the pa-
rameters of the gate to all 2n elements of the state vec-
tor, so that each gate application incurs a cost scaling as
2n. In contrast, multiplying small gate matrices incurs a
small cost. This means a simulation of a circuit will be
fastest if we pre-multiply as many gates as possible, while
keeping the matrix size small, before applying them to a
state. This pre-multiplication is called gate fusion and is
accomplished with the qsim fusion algorithm.

2. Gate Fusion with qsim

The core idea of the fusion algorithm is to construct
multi-qubit gates out of smaller gates. The circuit can
be interpreted as a (d + 1)-dimensional lattice, where d
denotes the spatial direction and 1 denotes the time di-
rection. Suppose we choose a fusion size F between 2 and
6 inclusive. The fusion algorithm combines gates that are
close in space and time to form larger gates that act on up
to F qubits. There are two steps in the algorithm. First,

14

Figure 9. Performance of TFQ and Cirq on simulation tasks.
The plots show the base 10 logarithm of the total time to
solution (in seconds) versus the number of qubits simulated.
Simulation of 500 random circuits, taken in batches of 50.
Circuits were of depth 40.
In the dense case, we see that
TFQ + qsim on CPU is around 10x faster than Cirq. With
the GPU this gap grows to approximately 50x. In the case of
sparse circuits, qsim gate fusion makes this speed diÔ¨Äerence
even more pronounced reaching well over 100x performance
diÔ¨Äerence on both CPU and GPU.

the Cirq function
circuits were
generated using
cirq.generate_boixo_2018_supremacy_circuits_v2 .
These
circuits are only tractable for benchmarking due to the
small numbers of qubits involved here. These circuits
involve dense (subject to a constraint [84]) interleaved
two-qubit gates to generate entanglement as quickly as
possible. In summary, at the largest benchmarked prob-
lem size of 30 qubits, qsim achieves an approximately
10-fold improvement in simulation time over Cirq. The
performance curves are shown in Fig. 9.

When the simulated circuits have more sparse struc-
ture, the Fusion algorithm allows us to achieve a larger
performance boost by reducing the number of gates that
ultimately need to be simulated. The circuits for this
task are a factorized version of the supremacy-style cir-
cuits which generate entanglement only on small subsets
of the qubits.
In summary, for these circuits, we Ô¨Ånd
a roughly 100 times improvement in simulation time in
TFQ versus Cirq. The performance curves are shown in
Fig. 9.

Thus we see that in addition to our core functionality
of implementing native TensorFlow gradients for quan-
tum circuits, TFQ also provides a signiÔ¨Åcant boost in
performance over Cirq when running in simulation mode.
Additionally, as noted before, this performance boost is
despite the additional overhead of serialization between
the TensorFlow frontend and qsim proper.

Figure 8. Visualization of the qsim fusing algorithm. In this
example, a set of single and two-qubit gates are fused into
three or four-qubit gates, increasing the speed of subsequent
circuit simulation. qsim supports gate fusions for larger num-
bers of qubits (up to 6 qubits at a time).

we fuse each q-qubit gate (q ‚â• 2) with smaller or same
size neighbors in time direction that act on the same set
of qubits. Second, we greedily (in increasing time order)
combine small gates that are neighbors in space and time
to construct the largest gates possible (up to F qubits).
Typically F = 4 is optimal for many threads and F = 2
or F = 3 is optimal for one or two threads.

3. Hardware-Acceleration

With the given quantum circuit fused into the mini-
mal number of up to F -qubit gates, we need simulators
optimized for applying up to 2F √ó 2F matrices to state
vectors. TFQ will adapt to the user‚Äôs available hard-
ware. For CPU based simulations, SSE2 instruction set
[82] and AVX2 + AVX512 instruction sets [83] will be
detected and used to increase performance. If a compat-
ible CUDA GPU is detected TFQ will be able to switch
to GPU based simulation as well. The next section illus-
trates this power with benchmarks comparing the per-
formance of TFQ to the performance of parallelized Cirq
running in simulation mode. In the future, we hope to ex-
pand the range of custom simulation hardware supported
to include TPU integration.

4. Benchmarks

Here, we demonstrate the performance of TFQ, backed
by qsim, relative to Cirq on two benchmark simula-
tion tasks. As detailed above, the performance diÔ¨Äer-
ence is due to circuit pre-processing via gate fusion com-
bined with performant C++ simulators. The bench-
marks were performed on a desktop equipped with an In-
tel(R) Xeon(R) Gold 6154 CPU (18 cores and 36 threads)
and an NVidia V100 GPU.

The Ô¨Årst benchmark task is

simulation of 500
random (early variant of beyond-classical/supremacy-
These
style)

circuits batched 50

time.

at

a

15202530Number of qubits12345log10(Simulation time) (s)SimulatorTFQ qsim - GPUTFQ qsim - CPUCirq + MultiprocessingCircuit TypeDenseSparse5. Large-scale simulation for quantum machine learning

Recently, we have provided a series of

learning-
theoretic tests for evaluating whether a quantum machine
learning model can predict more accurately than classical
machine learning models [22]. One of the key Ô¨Åndings is
that various existing quantum machine learning models
perform slightly better than standard classical machine
learning models in small system sizes. However, these
quantum machine learning models perform substantially
worse than classical machine learning models in larger
system sizes (more than 10 to 15 qubits). Note that per-
forming better in small system sizes is not useful since a
classical algorithm can eÔ¨Éciently simulate the quantum
machine learning model. The above observation shows
that understanding the performance of quantum machine
learning models in large system sizes is very crucial for
assessing whether the quantum machine learning model
will eventually provide an advantage over classical mod-
els. Ref. [22] also provide improvements to existing quan-
tum machine learning models to improve the prediction
performance in large system sizes.

In the numerical experiments conducted in [22], we
consider the simulation of these quantum machine learn-
ing models up to 30 qubits. The large-scale simulation
allows us to gauge the potential and limitations of dif-
ferent quantum machine learning models better. We uti-
lize the qsim software package in TFQ to perform large-
scale quantum simulations using Google Cloud Plat-
form. The simulation reaches a peak throughput of up
to 1.1 quadrillion Ô¨Çoating-point operations per second
(petaÔ¨Çop/s). Trends of approximately 300 teraÔ¨Çop/s for
quantum simulation and 800 teraÔ¨Çop/s for classical anal-
ysis were observed up to the maximum experiment size
with the overall Ô¨Çoating-point operations across all exper-
iments totaling approximately two quintillions (exaÔ¨Çop).

6. Noise in qsim

The study of noise in quantum circuits is an important
use-case for simulators to support. To address this, qsim
supports simulation of all the common channels provided
by Cirq. Further, TFQ supports the serialization of cir-
cuits containing such channels, allowing users to apply
the many features of TFQ to the study of noisy circuits.
The two main choices for simulation of noisy circuits
are density matrix simulation and trajectory simulation.
There are trade-oÔ¨Äs between these choices. Density ma-
trix simulation keeps a full density matrix in memory,
and at each timestep applies all the Kraus operators as-
sociated with a channel; at the end of simulation, prop-
erties of the state can be computed against this density
matrix. For a circuit on N qubits, a full density matrix
simulation requires memory of size 22N . In contrast, tra-
jectory simulations keep only a pure state in memory, and
at each time step, a single Kraus operator in the chan-
nel is selected probabilistically and applied to the state;

15

properties of interest are measured against the resulting
pure state. The process must be run many times, and
the results averaged; but, the memory required at any
one time is only 2N . When a circuit is not too noisy, it
is often more eÔ¨Écient to average over trajectories than
it is to simulate the full density matrix, so we choose
this method for TFQ. For information on how to use this
feature, see IV C.

III. THEORY OF HYBRID
QUANTUM-CLASSICAL MACHINE LEARNING

In the previous section, we reviewed the building blocks
required for use of TFQ. In this section, we consider the
theory behind the software. We deÔ¨Åne quantum neu-
ral networks as products of parameterized unitary ma-
trices. Samples and expectation values are deÔ¨Åned by
expressing the loss function as an inner product. With
quantum neural networks and expectation values deÔ¨Åned,
we can then deÔ¨Åne gradients. We Ô¨Ånally combine quan-
tum and classical neural networks and formalize hybrid
quantum-classical backpropagation, one of core compo-
nents of TFQ.

A. Quantum Neural Networks

A Quantum Neural Network ansatz can generally be

written as a product of layers of unitaries in the form

ÀÜU (Œ∏) =

L
(cid:89)

(cid:96)=1

ÀÜV (cid:96) ÀÜU (cid:96)(Œ∏(cid:96)),

(1)

where the (cid:96)th layer of the QNN consists of the prod-
uct of ÀÜV (cid:96), a non-parametric unitary, and ÀÜU (cid:96)(Œ∏(cid:96)) a uni-
tary with variational parameters (note the superscripts
here represent indices rather than exponents). The multi-
parameter unitary of a given layer can itself be generally
comprised of multiple unitaries { ÀÜU (cid:96)
applied in
parallel:

j)}M(cid:96)

j (Œ∏(cid:96)

j=1

ÀÜU (cid:96)(Œ∏(cid:96)) ‚â°

M(cid:96)(cid:79)

j=1

ÀÜU (cid:96)

j (Œ∏(cid:96)

j).

(2)

Finally, each of these unitaries ÀÜU (cid:96)
can be expressed as
j
the exponential of some generator ÀÜgj(cid:96), which itself can
be any Hermitian operator on n qubits (thus expressible
as a linear combination of n-qubit Pauli‚Äôs),

ÀÜU (cid:96)

j (Œ∏(cid:96)

j) = e‚àíiŒ∏(cid:96)

j ÀÜg(cid:96)
j ,

ÀÜg(cid:96)
j =

Kj(cid:96)
(cid:88)

k=1

Œ≤j(cid:96)
k

ÀÜPk,

(3)

where ÀÜPk ‚àà Pn, here Pn denotes the Paulis on n-qubits
[85], and Œ≤j(cid:96)
k ‚àà R for all k, j, (cid:96). For a given j and (cid:96), in the
case where all the Pauli terms commute, i.e. [ ÀÜPk, ÀÜPm] =

16

where we deÔ¨Åned a vector of coeÔ¨Écients Œ± ‚àà RN and
a vector of N operators ÀÜh. Often this decomposition is
chosen such that each of these sub-Hamiltonians is in the
n-qubit Pauli group ÀÜhk ‚àà Pn. The expectation value of
this Hamiltonian is then generally evaluated via quantum
expectation estimation, i.e. by taking the linear combi-
nation of expectation values of each term

f (Œ∏) = (cid:104) ÀÜH(cid:105)Œ∏ =

N
(cid:88)

k=1

Œ±k (cid:104)ÀÜhk(cid:105)Œ∏ ‚â° Œ± ¬∑ hŒ∏,

(8)

.

where we introduced the vector of expectations hŒ∏ ‚â°
(cid:104)ÀÜh(cid:105)Œ∏
In the case of non-commuting terms, the vari-
ous expectation values (cid:104)ÀÜhk(cid:105)Œ∏
are estimated over separate
runs.

Note that, in practice, each of these quantum expecta-
tions is estimated via sampling of the output of the quan-
tum computer [29]. Even assuming a perfect Ô¨Ådelity of
quantum computation, sampling measurement outcomes
of eigenvalues of observables from the output of the quan-
tum computer to estimate an expectation will have some
non-negligible variance for any Ô¨Ånite number of samples.
Assuming each of the Hamiltonian terms of equation (7)
admit a Pauli operator decomposition as

ÀÜhj =

Jj
(cid:88)

k=1

Œ≥j
k

ÀÜPk,

(9)

‚àó/(cid:15)2), where (cid:107)ÀÜhk(cid:107)‚àó = (cid:80)Jj

‚Äôs are real-valued coeÔ¨Écients and the ÀÜPj‚Äôs
where the Œ≥j
k
are Paulis that are Pauli operators [85], then to get an
estimate of the expectation value (cid:104)ÀÜhj(cid:105) within an accu-
racy (cid:15), one needs to take a number of measurement sam-
ples scaling as ‚àº O((cid:107)ÀÜhk(cid:107)2
k=1 |Œ≥j
k|
is the Pauli coeÔ¨Écient norm of each Hamiltonian term.
Thus, to estimate the expectation value of the full Hamil-
tonian (7) accurately within a precision Œµ = (cid:15) (cid:80)
k |Œ±k|2,
k |Œ±k|2(cid:107)ÀÜhk(cid:107)2
we would need on the order of ‚àº O( 1
‚àó)
(cid:15)2
measurement samples in total [27, 88], as we would need
to measure each expectation independently if we are fol-
lowing the quantum expectation estimation trick of (8).
This is in sharp contrast to classical methods for gradi-
ents involving backpropagation, where gradients can be
estimated to numerical precision; i.e. within a precision
(cid:15) with ‚àº O(PolyLog((cid:15))) overhead. Although there have
been attempts to formulate a backpropagation principle
for quantum computations [56], these methods also rely
on the measurement of a quantum observable, thus also
requiring ‚àº O( 1

(cid:80)

(cid:15)2 ) samples.

As we will see in the following section III C, estimat-
ing gradients of quantum neural networks on quantum
computers involves the estimation of several expectation
values of the cost function for various values of the pa-
rameters. One trick that was recently pointed out [47, 89]
and has been proven to be successful both theoretically
and empirically to estimate such gradients is the stochas-
tic selection of various terms in the quantum expectation

Figure 10.
High-level depiction of a multilayer quantum
neural network (also known as a parameterized quantum cir-
cuit), at various levels of abstraction. (a) At the most detailed
level we have both Ô¨Åxed and parameterized quantum gates,
any parameterized operation is compiled into a combination
of parameterized single-qubit operations.
(b) Many singly-
parameterized gates Wj(Œ∏j) form a multi-parameterized uni-
tary Vl((cid:126)Œ∏l) which performs a speciÔ¨Åc function. (c) The prod-
uct of multiple unitaries Vl generates the quantum model U (Œ∏)
shown in (d).

0 for all m, k such that Œ≤j(cid:96)
(cid:54)= 0, one can simply
decompose the unitary into a product of exponentials of
each term,

m, Œ≤j(cid:96)

k

ÀÜU (cid:96)

j (Œ∏(cid:96)

j) =

(cid:89)

e‚àíiŒ∏(cid:96)

j Œ≤j(cid:96)

k

ÀÜPk .

(4)

k
Otherwise, in instances where the various terms do not
commute, one may apply a Trotter-Suzuki decomposi-
tion of this exponential [86], or other quantum simula-
tion methods [87]. Note that in the above case where
the unitary of a given parameter is decomposable as the
product of exponentials of Pauli terms, one can explicitly
express the layer as

ÀÜU (cid:96)

j (Œ∏(cid:96)

j) =

(cid:104)
cos(Œ∏(cid:96)

jŒ≤j(cid:96)

k ) ÀÜI ‚àí i sin(Œ∏(cid:96)

jŒ≤j(cid:96)

k ) ÀÜPk

(cid:105)

.

(5)

(cid:89)

k

The above form will be useful for our discussion of gra-
dients of expectation values below.

B. Sampling and Expectations

To optimize the parameters of an ansatz from equation
(1), we need a cost function to optimize. In the case of
standard variational quantum algorithms, this cost func-
tion is most often chosen to be the expectation value of
a cost Hamiltonian,

f (Œ∏) = (cid:104) ÀÜH(cid:105)Œ∏ ‚â° (cid:104)Œ®0| ÀÜU ‚Ä†(Œ∏) ÀÜH ÀÜU (Œ∏) |Œ®0(cid:105)
where |Œ®0(cid:105) is the input state to the parameterized circuit.
In general, the cost Hamiltonian can be expressed as a
linear combination of operators, e.g. in the form

(6)

ÀÜH =

N
(cid:88)

k=1

Œ±k

ÀÜhk ‚â° Œ± ¬∑ ÀÜh,

(7)

estimation. This can greatly reduce the number of mea-
surements needed per gradient update, we will cover this
in subsection III C 3.

C. Gradients of Quantum Neural Networks

Now that we have established how to evaluate the
loss function, let us describe how to obtain gradients of
the cost function with respect to the parameters. Why
should we care about gradients of quantum neural net-
works? In classical deep learning, the most common fam-
ily of optimization heuristics for the minimization of cost
functions are gradient-based techniques [90‚Äì92], which
include stochastic gradient descent and its variants. To
leverage gradient-based techniques for the learning of
multilayered models, the ability to rapidly diÔ¨Äerentiate
error functionals is key. For this, the backwards propa-
gation of errors [93] (colloquially known as backprop), is
a now canonical method to progressively calculate gradi-
ents of parameters in deep networks. In its most general
form, this technique is known as automatic diÔ¨Äerentiation
[94], and has become so central to deep learning that this
feature of diÔ¨Äerentiability is at the core of several frame-
works for deep learning, including of course TensorFlow
(TF) [95], JAX [96], and several others.

To be able to train hybrid quantum-classical models
(section III D), the ability to take gradients of quantum
neural networks is key. Now that we understand the
greater context, let us describe a few techniques below
for the estimation of these gradients.

17

value of the circuit for Hamiltonians which have a single-
term in their Pauli decomposition (3) (or, alternatively, if
the Hamiltonian has a spectrum {¬±Œª} for some positive
Œª). For multi-term Hamiltonians, in [98] a method to
obtain the analytic gradients is proposed which uses a
linear combination of unitaries. Here, instead, we will
simply use a change of variables and the chain rule to
obtain analytic gradients of parametric unitaries of the
form (5) without the need for ancilla qubits or additional
unitaries.

For a parameter of interest Œ∏(cid:96)
j

appearing in a layer (cid:96) in a
parametric circuit in the form (5), consider the change of
variables Œ∑j(cid:96)
, then from the chain rule of calculus
[99], we have

k ‚â° Œ∏(cid:96)

jŒ≤j(cid:96)

k

‚àÇf
‚àÇŒ∏(cid:96)
j

(cid:88)

=

k

‚àÇf
‚àÇŒ∑j(cid:96)
k

‚àÇŒ∑j(cid:96)
k
‚àÇŒ∏(cid:96)
j

(cid:88)

=

Œ≤j(cid:96)
k

k

‚àÇf
‚àÇŒ∑k

.

(11)

Thus, all we need to compute are the derivatives of the
cost function with respect to Œ∑j(cid:96)
. Due to this change of
k
variables, we need to reparameterize our unitary ÀÜU (Œ∏)
from (1) as

ÀÜU (cid:96)(Œ∏(cid:96)) (cid:55)‚Üí ÀÜU (cid:96)(Œ∑(cid:96)) ‚â°

(cid:79)

(cid:16) (cid:89)

e‚àíiŒ∑j(cid:96)

k

ÀÜPk

(cid:17)

,

(12)

j‚ààI(cid:96)

k

where I(cid:96) ‚â° {1, . . . , M(cid:96)} is an index set for the QNN
layers. One can then expand each exponential in the
above in a similar fashion to (5):

e‚àíiŒ∑j(cid:96)

k

ÀÜPk = cos(Œ∑j(cid:96)

k ) ÀÜI ‚àí i sin(Œ∑j(cid:96)

k ) ÀÜPk.

(13)

1. Finite diÔ¨Äerence methods

A simple approach is to use simple Ô¨Ånite-diÔ¨Äerence

methods, for example, the central diÔ¨Äerence method,

As can be shown from this form, the analytic derivative
of the expectation value f (Œ∑) ‚â° (cid:104)Œ®0| ÀÜU ‚Ä†(Œ∑) ÀÜH ÀÜU (Œ∑) |Œ®0(cid:105)
with respect to a component Œ∑j(cid:96)
can be reduced to fol-
k
lowing parameter shift rule [47, 89, 100]:

(10)

+ O(Œµ2)

‚àÇkf (Œ∏) =

f (Œ∏ + Œµ‚àÜk) ‚àí f (Œ∏ ‚àí Œµ‚àÜk)
2Œµ
which, in the case where there are M continuous param-
eters, involves 2M evaluations of the objective function,
each evaluation varying the parameters by Œµ in some di-
rection, thereby giving us an estimate of the gradient
of the function with a precision O(Œµ2). Here the ‚àÜk
is a unit-norm perturbation vector in the kth direction
of parameter space, (‚àÜk)j = Œ¥jk. In general, one may
use lower-order methods, such as forward diÔ¨Äerence with
O(Œµ) error from M + 1 objective queries [24], or higher
order methods, such as a Ô¨Åve-point stencil method, with
O(Œµ4) error from 4M queries [97].

2. Parameter shift methods

As recently pointed out in various works [89, 98], given
knowledge of the form of the ansatz (e.g. as in (3)), one
can measure the analytic gradients of the expectation

‚àÇ
‚àÇŒ∑j(cid:96)
k

f (Œ∑) = f (Œ∑ + œÄ

4 ‚àÜj(cid:96)

k ) ‚àí f (Œ∑ ‚àí œÄ

4 ‚àÜj(cid:96)
k )

(14)

where ‚àÜj(cid:96)
is a vector representing unit-norm perturba-
k
tion of the variable Œ∑j(cid:96)
in the positive direction. We thus
k
see that this shift in parameters can generally be much
larger than that of the numerical diÔ¨Äerentiation parame-
ter shifts as in equation (10). In some cases this is useful
as one does not have to resolve as Ô¨Åne-grained a diÔ¨Äer-
ence in the cost function as an inÔ¨Ånitesimal shift, hence
requiring less runs to achieve a suÔ¨Éciently precise esti-
mate of the value of the gradient.

Note that in order to compute through the chain rule
in (11) for a parametric unitary as in (3), we need to
evaluate the expectation function 2K(cid:96) times to obtain
the gradient of the parameter Œ∏(cid:96)
. Thus, in some cases
j
where each parameter generates an exponential of many
terms, although the gradient estimate is of higher pre-
cision, obtaining an analytic gradient can be too costly
in terms of required queries to the objective function.
[89]
To remedy this additional overhead, Harrow et al.

proposed to stochastically select terms according to a dis-
tribution weighted by the coeÔ¨Écients of each term in the
generator, and to perform gradient descent from these
stochastic estimates of the gradient. Let us review this
stochastic gradient estimation technique as it is imple-
mented in TFQ.

3. Stochastic Parameter Shift Gradient Estimation

Consider the full analytic gradient from (14) and
(11), if we have M(cid:96) parameters and L layers, there are
(cid:80)L

(cid:96)=1 M(cid:96) terms of the following form to estimate:

‚àÇf
‚àÇŒ∏(cid:96)
j

=

Kj(cid:96)
(cid:88)

k=1

Œ≤j(cid:96)
k

‚àÇf
‚àÇŒ∑k

=

Kj(cid:96)
(cid:88)

(cid:104) (cid:88)

k=1

¬±

¬±Œ≤j(cid:96)

k f (Œ∑ ¬± œÄ

4 ‚àÜj(cid:96)
k )

(cid:105)
. (15)

These terms come from the components of the gradient
vector itself which has the dimension equal to that of the
total number of free parameters in the QNN, dim(Œ∏).
For each of these components, for the jth component of
the (cid:96)th layer, there 2Kj(cid:96) parameter-shifted expectation
values to evaluate, thus in total there are (cid:80)L
(cid:96)=1 2Kj(cid:96)M(cid:96)
parameterized expectation values of the cost Hamiltonian
to evaluate.

For practical implementation of this estimation proce-
dure, we must expand this sum further. Recall that, as
the cost Hamiltonian generally will have many terms, for
each quantum expectation estimation of the cost function
for some value of the parameters, we have

f (Œ∏) = (cid:104) ÀÜH(cid:105)Œ∏ =

N
(cid:88)

Œ±m (cid:104)ÀÜhm(cid:105)Œ∏ =

N
(cid:88)

Jm(cid:88)

Œ±mŒ≥m

q (cid:104) ÀÜPqm(cid:105)Œ∏ ,

m=1

m=1

q=1

(16)
which has (cid:80)N
j=1 Jj terms. Thus, if we consider that all
the terms in (15) are of the form of (16), we see that we
have a total number of (cid:80)N
(cid:96)=1 2JmKj(cid:96)M(cid:96) expecta-
m=1
tion values to estimate the gradient. Note that one of
these sums comes from the total number of appearances
of parameters in front of Paulis in the generators of the
parameterized quantum circuit, the second sum comes
from the various terms in the cost Hamiltonian in the
Pauli expansion.

(cid:80)L

As the cost of accurately estimating all these terms
one by one and subsequently linearly combining the val-
ues such as to yield an estimate of the total gradient
may be prohibitively expensive in terms of numbers of
runs, instead, one can stochastically estimate this sum,
by randomly picking terms according to their weighting
[47, 89].
One
ap-
pearances
in the QNN, k ‚àº
Pr(k|j, (cid:96)) = |Œ≤j(cid:96)
o |), one then estimates the
two parameter-shifted terms corresponding to this in-
dex in (15) and averages over samples. We consider
this case to be simply stochastic gradient estimation for

a parameter
k |/((cid:80)Kj(cid:96)
o=1 |Œ≤j(cid:96)

a distribution over

can sample

the

of

18

d=1

(cid:80)Jd

r=1 |Œ±dŒ≥d

the gradient component corresponding to the parame-
ter Œ∏(cid:96)
. One can go even further in this spirit, for each
j
of these sampled expectation values, by also sampling
terms from (16) according to a similar distribution de-
termined by the magnitude of the Pauli expansion co-
eÔ¨Écients. Sampling the indices {q, m} ‚àº Pr(q, m) =
q |/((cid:80)N
r |) and estimating the expec-
|Œ±mŒ≥m
tation (cid:104) ÀÜPqm(cid:105)Œ∏
for the appropriate parameter-shifted val-
ues sampled from the terms of (15) according to the pro-
cedure outlined above. This is considered doubly stochas-
In principle, one could go one
tic gradient estimation.
step further, and per iteration of gradient descent, ran-
domly sample indices representing subsets of parame-
ters for which we will estimate the gradient component,
and set the non-sampled indices corresponding gradient
components to 0 for the given iteration. The distribu-
j ‚àº Pr(j, (cid:96)) =
tion we sample in this case is given by Œ∏(cid:96)
(cid:80)Kj(cid:96)
(cid:80)Mu
o |). This is, in a sense,
i=1
akin to the SPSA algorithm [101],
in the sense that
it is a gradient-based method with a stochastic mask.
The above component sampling, combined with doubly
stochastic gradient descent, yields what we consider to be
triply stochastic gradient descent. This is equivalent to si-
multaneously sampling {j, (cid:96), k, q, m} ‚àº Pr(j, (cid:96), k, q, m) =
Pr(k|j, (cid:96))Pr(j, (cid:96))Pr(q, m) using the probabilities outlined
in the paragraph above, where j and (cid:96) index the param-
eter and layer, k is the index from the sum in equation
(15), and q and m are the indices of the sum in equation
(16).

k |/((cid:80)L

k=1 |Œ≤j(cid:96)

o=1 |Œ≤iu

(cid:80)Kiu

u=1

In TFQ, all three of the stochastic averaging meth-
ods above can be turned on or oÔ¨Ä independently for
stochastic parameter-shift gradients. See the details in
the Differentiator module of TFQ on GitHub.

4. Adjoint Gradient Backpropagation in Simulations

For experiments with tractably classically simulatable
system sizes, the derivatives can be computed entirely
in simulation, using an analogue of backpropagation
called Adjoint DiÔ¨Äerentiation [102, 103]. This is a high-
performance way to obtain gradients of deep circuits with
many parameters. Although it is not possible to perform
this technique in quantum hardware, let us outline here
how it is implemented in TFQ for numerical simulator
backends such as qsim.

Suppose we are given a parameterized quantum circuit

in the form of (1)

ÀÜU (Œ∏) =

L
(cid:89)

(cid:96)=1

ÀÜV (cid:96) ÀÜU (cid:96)(Œ∏(cid:96)) =

L
(cid:89)

(cid:96)=1

ÀÜW (cid:96)

Œ∏(cid:96) = ÀÜW n

Œ∏n ÀÜW n‚àí1

Œ∏n‚àí1 ¬∑ ¬∑ ¬∑ ÀÜW 1
Œ∏1,

where for compactness of notation, we denoted the pa-
rameterized layers in a more succinct form,

ÀÜW (cid:96)

Œ∏(cid:96) = ÀÜV (cid:96) ÀÜU (cid:96)(Œ∏(cid:96)).

19

Figure 11. Tensor network contraction diagram [71, 72] depicting the adjoint backpropagation process for the computation of
the gradient of an expectation value at the output of multilayer QNN with respect to a parameter in the (cid:96)th layer. Yellow
channel depicts the forward pass, which is then duplicated into two copies of the Ô¨Ånal state. The blue and orange channels depict
the backwards pass, performed via layer-wise recursive uncomputation. Both the orange and blue backwards pass outputs are
then contracted with the gradient of the (cid:96)th layer‚Äôs unitary. The arrows with dotted lines indicate how to modify the contraction
to compute a gradient with respect to a parameter in the ((cid:96) ‚àí 1)th layer.

Let us label the initial state |œà0(cid:105) Ô¨Ånal quantum state
Œ∏n (cid:105), with

|œàn

value of this observable with respect to our Ô¨Ånal layer‚Äôs
parameterized state is

|œà(cid:96)

Œ∏(cid:96)(cid:105) = ÀÜW (cid:96)

Œ∏(cid:96) |œà(cid:96)‚àí1

Œ∏(cid:96)‚àí1(cid:105)

being the recursive deÔ¨Ånition of the Schr√∂dinger-evolved
state vector at layer (cid:96). The derivatives of the state with
respect to Œ∏(cid:96), the jth parameter of the (cid:96)th layer, is given
by

‚àÇŒ∏(cid:96)

j

|œàn

Œ∏n (cid:105) = ÀÜW n

Œ∏n ¬∑ ¬∑ ¬∑ ÀÜW (cid:96)+1

Œ∏(cid:96)+1 ‚àÇŒ∏(cid:96)

j

ÀÜW (cid:96)
Œ∏(cid:96)

ÀÜW (cid:96)‚àí1

Œ∏(cid:96)‚àí1 ¬∑ ¬∑ ¬∑ ÀÜW 1

Œ∏1 |œà0(cid:105) .

This gradient of the (cid:96)th layer parameters, assuming the
QNN is structured as in equations (1), (2), and (3) is
given analytically by

‚àÇŒ∏(cid:96)

j

ÀÜW (cid:96)

Œ∏(cid:96) = ÀÜV (cid:96)(‚àíiÀÜg(cid:96)

j) ÀÜU (cid:96)(Œ∏(cid:96))

which is an operator that can be computed numerically
and inserted in the circuit when dealing with a classical
simulation of the quantum circuit.

A key trick for adjoint backpropagation is leveraging
the fact that we can reverse unitary layers, so that we do
not have to store the history of states; we can access the
state at any inner layer by uncomputing the later layers:

ÀÜW (cid:96)‚àí1

Œ∏(cid:96)‚àí1 ¬∑ ¬∑ ¬∑ ÀÜW 1

Œ∏1 |œà0(cid:105) = ÀÜW ‚Ä† (cid:96)
Œ∏(cid:96)

ÀÜW ‚Ä† (cid:96)+1

Œ∏(cid:96)+1 ¬∑ ¬∑ ¬∑ ÀÜW ‚Ä† n

Œ∏n |œàn

Œ∏n (cid:105) .

We can leverage this trick in order to evaluate gradients
of the quantum state with respect to parameters of in-
termediate layers of the QNN,

‚àÇŒ∏(cid:96)

j

|œàn

Œ∏n (cid:105) = ÀÜW n

Œ∏n ¬∑ ¬∑ ¬∑ ÀÜW (cid:96)+1

Œ∏(cid:96)+1‚àÇŒ∏(cid:96)

j

ÀÜW (cid:96)
Œ∏(cid:96)

ÀÜW ‚Ä† (cid:96)

Œ∏(cid:96) ¬∑ ¬∑ ¬∑ ÀÜW ‚Ä† n

Œ∏n |œàn

Œ∏n (cid:105) ,

this allows us to compute gradients layer-wise starting
from the Ô¨Ånal layer via a backwards pass, following of
course a forward pass to compute the Ô¨Ånal state at the
output layer.

In most contexts, we typically want to take gradients
of expectation values at the output of several QNN lay-
ers. Given a Hermitian observable ÀÜH, the expectation

(cid:104) ÀÜH(cid:105)Œ∏n = (cid:104)œàn

Œ∏n| ÀÜH |œàn

Œ∏n (cid:105) ,

and the derivative of this expectation value is given by

‚àÇŒ∏(cid:96)

j

(cid:104) ÀÜH(cid:105)Œ∏n = (‚àÇŒ∏(cid:96)
j
(cid:104)

= 2(cid:60)

(cid:104)œàn

(cid:104)œàn

Œ∏n|) ÀÜH |œàn
Œ∏n | ÀÜH(‚àÇŒ∏(cid:96)

Œ∏n | ÀÜH(‚àÇŒ∏(cid:96)
Œ∏n (cid:105) + (cid:104)œàn
(cid:105)
Œ∏n(cid:105))

|œàn

,

j

j

|œàn

Œ∏n (cid:105))

|œàn

Œ∏n (cid:105)) for each parameter.

where we employed the fact that the observable is Her-
mitian. Thus the computational task of gradient evalua-
tion reduces to the evaluation of the modiÔ¨Åed expectation
(cid:104)œàn

Œ∏n | ÀÜH(‚àÇŒ∏(cid:96)
In order to compute these gradients, we can proceed in
a recursive fashion during the backwards pass, starting
from the Ô¨Ånal layer‚Äôs output. Denoting this recursion
using nested parentheses, the gradient evaluation is given
by

j

‚àÇŒ∏(cid:96)

j

(cid:104) ÀÜH(cid:105)Œ∏n =

(cid:16)

(cid:17)

¬∑ ¬∑ ¬∑
(cid:16)

¬∑

(cid:16)(cid:16)

‚àÇŒ∏(cid:96)
j
(cid:16)

(cid:104)œàn

Œ∏n | ÀÜH
(cid:17)

ÀÜW (cid:96)
Œ∏(cid:96)

Œ∏n

(cid:17) ÀÜW n
(cid:16) ÀÜW ‚Ä† (cid:96)
¬∑
(cid:17)

Œ∏(cid:96) ¬∑ ¬∑ ¬∑

= (cid:104) Àúœà(cid:96)

Œ∏(cid:96)|

‚àÇŒ∏(cid:96)

j

ÀÜW (cid:96)
Œ∏(cid:96)

|œà(cid:96)‚àí1

Œ∏(cid:96)‚àí1 (cid:105)

(cid:17)

¬∑ ¬∑ ¬∑ ÀÜW (cid:96)+1
Œ∏(cid:96)+1
(cid:16) ÀÜW ‚Ä† n

Œ∏n (|œàn

Œ∏n(cid:105))

(cid:17)

(cid:17)

. . .

This consists of recursively computing the backwards

propagated state,

|œà(cid:96)‚àí1

Œ∏(cid:96)‚àí1(cid:105) = ÀÜW (cid:96)‚Ä†

Œ∏(cid:96) |œà(cid:96)

Œ∏(cid:96)(cid:105)

and contracting it with both the gradient of the (cid:96)th
Œ∏(cid:96) and the backwards propagated

layer‚Äôs unitary ‚àÇŒ∏(cid:96)
contraction of the Ô¨Ånal state with the observable:

ÀÜW (cid:96)

j

| Àúœà(cid:96)‚àí1

Œ∏(cid:96)‚àí1(cid:105) ‚â° ÀÜW (cid:96)‚Ä†

| Àúœàn

Œ∏n (cid:105) ‚â° ÀÜH |œàn

Œ∏n (cid:105) .

Œ∏(cid:96)(cid:105) ,

Œ∏(cid:96) | Àúœà(cid:96)
Œ∏(cid:96)‚àí1(cid:105) and | Àúœà(cid:96)

By only storing |œà(cid:96)‚àí1
Œ∏(cid:96)(cid:105) in memory for gradient
evaluations of the (cid:96)th layer during the backwards pass,
we saved the need for far more forward passes while only

20

Let us Ô¨Årst describe how to create these functions from
expectation values of QNN‚Äôs.

As we saw in equations (6) and (7), we get a diÔ¨Äer-
entiable cost function f : RM (cid:55)‚Üí R from taking the ex-
pectation value of a single Hamiltonian at the end of the
parameterized circuit, f (Œ∏) = (cid:104) ÀÜH(cid:105)Œ∏
. As we saw in equa-
tions (7) and (8), to compute this expectation value, as
the readout Hamiltonian is often decomposed into a lin-
ear combination of operators ÀÜH = Œ± ¬∑ ÀÜh (see (7)), then
the function is itself a linear combination of expectation
values of multiple terms (see (8)), (cid:104) ÀÜH(cid:105)Œ∏ = Œ± ¬∑ hŒ∏ where
hŒ∏ ‚â° (cid:104)ÀÜh(cid:105)Œ∏ ‚àà RN is a vector of expectation values. Thus,
before the scalar value of the cost function is evaluated,
QNN‚Äôs naturally are evaluated as a vector of expectation
values, hŒ∏.

Hence, if we would like the QNN to become more like
a classical neural network block, i.e. mapping vectors to
vectors f : RM ‚Üí RN , we can obtain a vector-valued
diÔ¨Äerentiable function from the QNN by considering it
as a function of the parameters which outputs a vector
of expectation values of diÔ¨Äerent operators,

f : Œ∏ (cid:55)‚Üí hŒ∏

(17)

where

(hŒ∏)k = (cid:104)ÀÜhk(cid:105)Œ∏ ‚â° (cid:104)Œ®0| ÀÜU ‚Ä†(Œ∏)ÀÜhk ÀÜU (Œ∏) |Œ®0(cid:105) .

(18)

We represent such a QNN-based function in Fig. 13. Note
that, in general, each of these ÀÜhk‚Äôs could be comprised of
multiple terms themselves,

ÀÜhk =

Nk(cid:88)

t=1

Œ≥t ÀÜmt

(19)

k=1

hence, one can perform Quantum Expectation Estima-
tion to estimate the expectation of each term as (cid:104)ÀÜhk(cid:105) =
(cid:80)

t Œ≥t (cid:104) ÀÜmt(cid:105).
Note that, in some cases, instead of the expectation
values of the set of operators {ÀÜhk}M
, one may instead
want to relay the histogram of measurement results ob-
tained from multiple measurements of the eigenvalues
of each of these observables. This case can also be
phrased as a vector of expectation values, as we will
now show. First, note that the histogram of the mea-
surement results of some ÀÜhk with eigendecomposition
ÀÜhk = (cid:80)rk
j=1 Œªjk |Œªjk(cid:105) (cid:104)Œªjk| can be considered as a vec-
tor of expectation values where the observables are the
eigenstate projectors |Œªjk(cid:105)(cid:104)Œªjk|. Instead of obtaining a
single real number from the expectation value of ÀÜhk, we
can obtain a vector hk ‚àà Rrk , where rk = rank(ÀÜhk) and
the components are given by (hk)j ‚â° (cid:104)|Œªjk(cid:105)(cid:104)Œªjk|(cid:105)Œ∏
. We
are then eÔ¨Äectively considering the categorical (empiri-
cal) distribution as our vector.

Now, if we consider measuring the eigenvalues of mul-
tiple observables {ÀÜhk}M
and collecting the measure-
ment result histograms, we get a 2-dimensional tensor
. Without loss of generality, we
(hŒ∏)jk = (cid:104)|Œªjk(cid:105)(cid:104)Œªjk|(cid:105)Œ∏

k=1

Figure 12. High-level depiction of a quantum-classical neural
network. Blue blocks represent Deep Neural Network (DNN)
function blocks and orange boxes represent Quantum Neural
Network (QNN) function blocks. Arrows represent the Ô¨Çow
of information during the feedforward (inference) phase. For
an example of the interface between quantum and classical
neural network blocks, see Fig. 13.

using twice the memory. See Figure 11 for a depiction
of the adjoint-backpropagation-based gradient computa-
tion described above. This is to be contrasted with, for
example, Ô¨Ånite-diÔ¨Äerence gradients, which would require
2M forward passes for M parameters. Thus, adjoint dif-
ferentiation is a useful method for rapid prototyping of
QNN‚Äôs using classical simulators of quantum circuits such
as qsim in TFQ.

D. Hybrid Quantum-Classical Computational
Graphs

Now that we have reviewed various ways of obtaining
gradients of expectation values, let us consider how to go
beyond basic variational quantum algorithms and con-
sider fully hybrid quantum-classical neural networks. As
we will see, our general framework of gradients of cost
Hamiltonians will carry over.

1. Hybrid Quantum-Classical Neural Networks

Now, we are ready to formally introduce the no-
tion of Hybrid Quantum-Classical Neural Networks
(HQCNN‚Äôs). HQCNN‚Äôs are meta-networks comprised
of quantum and classical neural network-based function
blocks composed with one another in the topology of a
directed graph. We can consider this a rendition of a hy-
brid quantum-classical computational graph where the
inner workings (variables, component functions) of vari-
ous functions are abstracted into boxes (see Fig. 12 for a
depiction of such a graph). The edges then simply repre-
sent the Ô¨Çow of classical information through the meta-
network of quantum and classical functions. The key will
be to construct parameterized (diÔ¨Äerentiable) functions
fŒ∏ : RM ‚Üí RN from expectation values of parameterized
quantum circuits, then creating a meta-graph of quan-
tum and classical computational nodes from these blocks.

can Ô¨Çatten this array into a vector of dimension RR where
R = (cid:80)M
k=1 rk. Thus, considering vectors of expectation
values is a relatively general way of representing the out-
put of a quantum neural network. In the limit where the
set of observables considered forms an informationally-
complete set of observables [104], then the array of mea-
surement outcomes would fully characterize the wave-
function, albeit at an overhead exponential in the number
of qubits.

We should mention that in some cases, instead of ex-
pectation values or histograms, single samples from the
output of a measurement can be used for direct feedback-
control on the quantum system, e.g.
in quantum error
correction [85]. At least in the current implementation of
TFQuantum, since quantum circuits are built in Cirq and
this feature is not supported in the latter, such scenar-
ios are currently out-of-scope. Mathematically, in such
a scenario, one could then consider the QNN and mea-
surement as map from quantum circuit parameters Œ∏ to
the conditional random variable ŒõŒ∏ valued over RNk cor-
responding to the measured eigenvalues Œªjk with a prob-
ability density Pr[(ŒõŒ∏)k ‚â° Œªjk] = p(Œªjk|Œ∏) which cor-
responds to the measurement statistics distribution in-
duced by the Born rule, p(Œªjk|Œ∏) = (cid:104)|Œªjk(cid:105)(cid:104)Œªjk|(cid:105)Œ∏
. This
QNN and single measurement map from the parameters
to the conditional random variable f : Œ∏ (cid:55)‚Üí ŒõŒ∏ can be
considered as a classical stochastic map (classical condi-
tional probability distribution over output variables given
the parameters). In the case where only expectation val-
ues are used, this stochastic map reduces to a determinis-
tic node through which we may backpropagate gradients,
as we will see in the next subsection. In the case where
this map is used dynamically per-sample, this remains a
stochastic map, and though there exists some algorithms
for backpropagation through stochastic nodes [105], these
are not currently supported natively in TFQ.

E. AutodiÔ¨Äerentiation through hybrid
quantum-classical backpropagation

As described above, hybrid quantum-classical neural
network blocks take as input a set of real parameters Œ∏ ‚àà
RM , apply a circuit ÀÜU (Œ∏) and take a set of expectation
values of various observables

(hŒ∏)k = (cid:104)ÀÜhk(cid:105)Œ∏ .
The result of this parameter-to-expected value map is
a function f : RM ‚Üí RN which maps parameters to a
real-valued vector,

f : Œ∏ (cid:55)‚Üí hŒ∏.

This function can then be composed with other param-
eterized function blocks comprised of either quantum or
classical neural network blocks, as depicted in Fig. 12.

To be able to backpropagate gradients through gen-
eral meta-networks of quantum and classical neural net-

21

work blocks, we simply have to Ô¨Ågure out how to back-
propagate gradients through a quantum parameterized
block function when it is composed with other parame-
terized block functions. Due to the partial ordering of
the quantum-classical computational graph, we can fo-
cus on how to backpropagate gradients through a QNN
in the scenario where we consider a simpliÔ¨Åed quantum-
classical network where we combine all function blocks
that precede and postcede the QNN block into mono-
lithic function blocks. Namely, we can consider a sce-
nario where we have fpre : xin (cid:55)‚Üí Œ∏ (fpre : Rin ‚Üí RM )
as the block preceding the QNN, the QNN block as
fqnn : Œ∏ (cid:55)‚Üí hŒ∏, (fqnn : RM ‚Üí RN ), the post-QNN
block as fpost : hŒ∏ (cid:55)‚Üí yout (fpost : RM ‚Üí RN
) and
out
Ô¨Ånally the loss function for training the entire network
out ‚Üí R. The
being computed from this output L : RN
composition of functions from the input data to the out-
put loss is then the sequentially composited function
(L‚ó¶fpost‚ó¶fqnn‚ó¶fpre). This scenario is depicted in Fig. 13.
Now, let us describe the process of backpropagation
through this composition of functions. As is standard in
backpropagation of gradients through feedforward net-
works, we begin with the loss function evaluated at the
output units and work our way back through the sev-
eral layers of functional composition of the network to
get the gradients. The Ô¨Årst step is to obtain the gra-
dient of the loss function ‚àÇL/‚àÇyout and to use classi-
cal (regular) backpropagation of gradients to obtain the
gradient of the loss with respect to the output of the
QNN, i.e. we get ‚àÇ(L ‚ó¶ fpost)/‚àÇh via the usual use of
the chain rule for backpropagation, i.e., the contraction
of the Jacobian with the gradient of the subsequent layer,
.
‚àÇ(L ‚ó¶ fpost)/‚àÇh = ‚àÇL
Now, let us label the evaluated gradient of the loss
function with respect to the QNN‚Äôs expectation values
as

‚àÇy ¬∑ ‚àÇfpost

‚àÇh

g ‚â° ‚àÇ(L‚ó¶fpost)

‚àÇh

(cid:12)
(cid:12)
(cid:12)h=hŒ∏

.

(20)

We can now consider this value as eÔ¨Äectively a constant.
Let us then deÔ¨Åne an eÔ¨Äective backpropagated Hamilto-
nian with respect to this gradient as

ÀÜHg ‚â°

(cid:88)

gk

ÀÜhk,

k
where gk are the components of (20). Notice that expec-
tations of this Hamiltonian are given by

(cid:104) ÀÜHg(cid:105)Œ∏ = g ¬∑ hŒ∏,
and so, the gradients of the expectation value of this
Hamiltonian are given by

‚àÇ
‚àÇŒ∏j

(cid:104) ÀÜHg(cid:105)Œ∏ = ‚àÇ

‚àÇŒ∏j

(g ¬∑ hŒ∏) =

(cid:88)

k

gk

‚àÇhŒ∏,k
‚àÇŒ∏j

which is exactly the Jacobian of the QNN function fqnn
contracted with the backpropagated gradient of previous
layers. Explicitly,

‚àÇ(L ‚ó¶ fpost ‚ó¶ fqnn)/‚àÇŒ∏ = ‚àÇ

‚àÇŒ∏ (cid:104) ÀÜHg(cid:105)Œ∏ .

22

IV. BASIC QUANTUM APPLICATIONS

The following examples show how one may use the var-
ious features of TFQ to reproduce and extend existing
results in second generation quantum machine learning.
Each application has an associated Colab notebook which
can be run in-browser to reproduce any results shown.
Here, we use snippets of code from those example note-
books for illustration; please see the example notebooks
for full code details.

A. Hybrid Quantum-Classical Convolutional
Neural Network ClassiÔ¨Åer

To run this example in the browser through Colab,

follow the link:

docs/tutorials/qcnn.ipynb

1. Background

Supervised classiÔ¨Åcation is a canonical task in classical
machine learning. Similarly, it is also one of the most
well-studied applications for QNNs [15, 31, 43, 106, 107].
As such, it is a natural starting point for our exploration
of applications for quantum machine learning. Discrimi-
native machine learning with hierarchical models can be
understood as a form of compression to isolate the infor-
mation containing the label [108]. In the case of quantum
data, the hidden classical parameter (real scalar in the
case of regression, discrete label in the case of classiÔ¨Åca-
tion) can be embedded in a non-local subsystem or sub-
space of the quantum system. One then has to perform
some disentangling quantum transformation to extract
information from this non-local subspace.

To choose an architecture for a neural network model,
one can draw inspiration from the symmetries in the
training data. For example, in computer vision, one of-
ten needs to detect corners and edges regardless of their
position in an image; we thus postulate that a neural net-
work to detect these features should be invariant under
translations.
In classical deep learning, an example of
such translationally-invariant neural networks are convo-
lutional neural networks. These networks tie parameters
across space, learning a shared set of Ô¨Ålters which are
applied equally to all portions of the data.

To the best of the authors‚Äô knowledge, there is no
strong indication that we should expect a quantum ad-
vantage for the classiÔ¨Åcation of classical data using QNNs
in the near term. For this reason, we focus on classifying
quantum data as deÔ¨Åned in section I C. There are many
kinds of quantum data with translational symmetry. One
example of such quantum data are cluster states. These
states are important because they are the initial states
for measurement-based quantum computation [109, 110].
In this example we will tackle the problem of detect-
ing errors in the preparation of a simple cluster state.

Figure 13. Example of inference and hybrid backpropaga-
tion at the interface of a quantum and classical part of a hy-
brid computational graph. Here we have classical deep neural
networks (DNN) both preceding and postceding the quan-
tum neural network (QNN). In this example, the preceding
DNN outputs a set of parameters Œ∏ which are used as then
used by the QNN as parameters for inference. The QNN
outputs a vector of expectation values (estimated through
several runs) whose components are (hŒ∏)k = (cid:104)ÀÜhk(cid:105)Œ∏
. This
vector is then fed as input to another (post-ceding) DNN,
and the loss function L is computed from this output. For
backpropagation through this hybrid graph, one Ô¨Årst back-
propagates the gradient of the loss through the post-ceding
DNN to obtain gk = ‚àÇL/‚àÇhk. Then, one takes the gradi-
ent of the following functional of the output of the QNN:
fŒ∏ = g ¬∑ hŒ∏ = (cid:80)
with respect to the
QNN parameters Œ∏ (which can be achieved with any of the
methods for taking gradients of QNN‚Äôs described in previous
subsections of this section). This completes the backprop-
agation of gradients of the loss function through the QNN,
the preceding DNN can use the now computed ‚àÇL/‚àÇŒ∏ to fur-
ther backpropagate gradients to preceding nodes of the hybrid
computational graph.

k gkhŒ∏,k = (cid:80)

k gk (cid:104)ÀÜhk(cid:105)Œ∏

Thus, by taking gradients of the expectation value of
the backpropagated eÔ¨Äective Hamiltonian, we can get the
gradients of the loss function with respect to QNN pa-
rameters, thereby successfully backpropagating gradients
through the QNN. Further backpropagation through the
preceding function block fpre can be done using standard
classical backpropagation by using this evaluated QNN
gradient.

Note that for a given value of g, the eÔ¨Äective back-
propagated Hamiltonian is simply a Ô¨Åxed Hamiltonian
operator, as such, taking gradients of the expectation
of a single multi-term operator can be achieved by any
choice in a multitude of the methods for taking gradients
of QNN‚Äôs described earlier in this section.

Backpropagation through parameterized quantum cir-

cuits is enabled by our Differentiator interface.

We oÔ¨Äer both Ô¨Ånite diÔ¨Äerence, regular parameter shift,
and stochastic parameter shift gradients, while the gen-
eral
interface allows users to deÔ¨Åne custom gradient
methods.

We can think of this as a supervised classiÔ¨Åcation task:
our training data will consist of a variety of correctly
and incorrectly prepared cluster states, each paired with
their label. This classiÔ¨Åcation task can be generalized
to condensed matter physics and beyond, for example to
the classiÔ¨Åcation of phases near quantum critical points,
where the degree of entanglement is high.

Since our simple cluster states are translationally in-
variant, we can extend the spatial parameter-tying of
convolutional neural networks to quantum neural net-
works, using recent work by Cong, et al.
[53], which
introduced a Quantum Convolutional Neural Network
(QCNN) architecture. QCNNs are essentially a quan-
tum circuit version of a MERA (Multiscale Entanglement
Renormalization Ansatz) network [111]. MERA has been
extensively studied in condensed matter physics. It is a
hierarchical representation of highly entangled wavefunc-
tions. The intuition is that as we go higher in the net-
work, the wavefunction‚Äôs entanglement gets renormalized
(coarse grained) and simultaneously a compressed repre-
sentation of the wavefunction is formed. This is akin to
the compression eÔ¨Äects encountered in deep neural net-
works [112].

Here we extend the QCNN architecture to include clas-
sical neural network postprocessing, yielding a Hybrid
Quantum Convolutional Neural Network (HQCNN). We
perform several low-depth quantum operations in order
to begin to extract hidden parameter information from a
wavefunction, then pass the resulting statistical informa-
tion to a classical neural network for further processing.
SpeciÔ¨Åcally, we will apply one layer of the hierarchy of the
QCNN. This allows us to partially disentangle the input
state and obtain statistics about values of multi-local ob-
servables. In this strategy, we deviate from the original
construction of Cong et al. [53]. Indeed, we are more in
the spirit of classical convolutional networks, where there
are several families of Ô¨Ålters, or feature maps, applied in
a translationally-invariant fashion. Here, we apply a sin-
gle QCNN layer followed by several feature maps. The
outputs of these feature maps can then be fed into clas-
sical convolutional network layers, or in this particular
simpliÔ¨Åed example directly to fully-connected layers.

Target problems:

1. Learn to extract classical information hidden in cor-

relations of a quantum system

2. Utilize shallow quantum circuits via hybridization
with classical neural networks to extract informa-
tion

Required TFQ functionalities:

1. Hybrid quantum-classical network models
2. Batch quantum circuit simulator
3. Quantum expectation based backpropagation
4. Fast classical gradient-based optimizer

23

Figure 14. The quantum portion of our classiÔ¨Åers, shown on
4 qubits. The combination of quantum convolution (blue)
and quantum pooling (orange) reduce the system size from 4
qubits to 2 qubits.

2.

Implementations

As discussed in section II D 2, the Ô¨Årst step in the QML
pipeline is the preparation of quantum data. In this ex-
ample, our quantum dataset is a collection of correctly
and incorrectly prepared cluster states on 8 qubits, and
the task is to classify theses states. The dataset prepara-
tion proceeds in two stages; in the Ô¨Årst stage, we generate
a correctly prepared cluster state:

def c l u s t e r _ s t a t e _ c i r c u i t ( bits ) :
circuit = cirq . Circuit ()
circuit . append ( cirq . H . on_each ( bits ) )
for this_bit , next_bit in zip (

bits , bits [1:] + [ bits [0]]) :
circuit . append (

cirq . CZ ( this_bit , next_bit ) )

return circuit

Errors in cluster state preparation will be simulated by
applying Rx(Œ∏) gates that rotate a qubit about the X-axis
of the Bloch sphere by some amount 0 ‚â§ Œ∏ ‚â§ 2œÄ. These
excitations will be labeled 1 if the rotation is larger than
some threshold, and -1 otherwise. Since the correctly
prepared cluster state is always the same, we can think
of it as the initial state in the pipeline and append the
excitation circuits corresponding to various error states:

c lu s te r _s t at e _b i ts = cirq . GridQubit . rect (1 , 8)
excitation_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )
cluster_state = tfq . layers . AddCircuit () (

excitation_input , prepend =
c l u s t e r _ s t a t e _ c i r c u i t ( cl u st e r_ s ta t e_ b it s ) )

Note how excitation_input is a standard Keras data in-
gester. The datatype of the input is string to account
for our circuit serialization mechanics described in sec-
tion II E 1.

Having prepared our dataset, we begin construction of
our model. The quantum portion of all the models we
consider in this section will be made of the same oper-
ations: quantum convolution and quantum pooling. A
visualization of these operations on 4 qubits is shown in
Fig. 14. Quantum convolution layers are enacted by ap-
plying a 2 qubit unitary U ((cid:126)Œ∏) with a stride of 1. In anal-

24

of the quantum model by measuring the expectation of
Pauli-Z on this Ô¨Ånal qubit. Measurement and parameter
control are enacted via our tfq.layers.PQC object. The
code for this model is shown below:

re ado ut_ op era tor s = cirq . Z (
c lu s te r _s t at e _b i ts [ -1])
quantum_model = tfq . layers . PQC (

c r e a t e _ m o d e l _ c i r c u i t ( cl u st e r_ s ta t e_ b it s ) ,
re ado ut_ op era tor s ) ( cluster_state )

qcnn_model = tf . keras . Model (

inputs =[ excitation_input ] ,
outputs =[ quantum_model ])

In the code, create_model_circuit is a function which ap-
plies the successive layers of quantum convolution and
quantum pooling. A simpliÔ¨Åed version of the resulting
model on 4 qubits is shown in Fig. 15.

With the model constructed, we turn to training and
validation. These steps can be accomplished using stan-
dard Keras tools. During training, the model output on
each quantum datapoint is compared against the label;
the cost function used is the mean squared error between
the model output and the label, where the mean is taken
over each batch from the dataset. The training and vali-
dation code is shown below:

qcnn_model . compile ( optimizer = tf . keras . Adam ,

loss = tf . losses . mse )

( train_excitations , train_labels ,
test_excitations , test_labels
) = generate_data ( c l us te r _s t at e _b i ts )

history = qcnn_model . fit (

x = train_excitations ,
y = train_labels ,
batch_size =16 ,
epochs =25 ,
validation_data =(

test_excitations , test_labels ) )
In the code, the generate_data function builds the exci-
tation circuits that are applied to the initial cluster state
input, along with the associated labels. The loss plots
for both the training and validation datasets can be gen-
erated by running the associated example notebook.

We now consider a hybrid classiÔ¨Åer.

Instead of us-
ing quantum layers to pool all the way down to 1 qubit,
we can truncate the QCNN and measure a vector of op-
erators on the remaining qubits. The resulting vector
of expectation values is then fed into a classical neural
network. This hybrid model is shown schematically in
Fig. 16.

This can be achieved in TFQ with a few simple mod-
iÔ¨Åcations to the previous model, implemented with the
code below:

# Build multi - readout quantum layer
readouts = [ cirq . Z ( bit ) for bit in

c lu s te r _s t at e _b i ts [4:]]

q ua n tu m _m o de l _d u al = tfq . layers . PQC (
m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

# Build classical neural network layers

Figure 15. Architecture of the purely quantum CNN for de-
tecting excited cluster states.

ogy with classical convolutional layers, the parameters of
the unitaries are tied, so that the same operation is ap-
plied to every nearest-neighbor pair of qubits. Pooling is
achieved using a diÔ¨Äerent 2 qubit unitary G((cid:126)œÜ) designed
to disentangle, allowing information to be projected from
2 qubits down to 1. The code below deÔ¨Ånes the quantum
convolution and quantum pooling operations:

def q u a n t u m _ c o n v _ c i r c u i t ( bits , syms ) :

circuit = cirq . Circuit ()
for a , b in zip ( bits [0::2] , bits [1::2]) :

circuit += two_q_unitary ([ a , b ] , syms )
for a , b in zip ( bits [1::2] , bits [2::2] + [
bits [0]]) :

circuit += two_q_unitary ([ a , b ] , syms )

return circuit

def q u a n t u m _ p o o l _ c i r c u i t ( srcs , snks , syms ) :

circuit = cirq . Circuit ()
for src , snk in zip ( srcs , snks ) :

circuit += two_q_pool ( src , snk , syms )

return circuit

In the code, two_q_unitary constructs a general param-
eterized two qubit unitary [113], while two_q_pool rep-
resents a CNOT with general one qubit unitaries on the
control and target qubits, allowing for variational selec-
tion of control and target basis.

With the quantum portion of our model deÔ¨Åned, we
move on to the third and fourth stages of the QML
pipeline, measurement and classical post-processing. We
consider three classiÔ¨Åer variants, each containing a dif-
ferent degree of hybridization with classical networks:

1. Purely quantum CNN
2. Hybrid CNN in which the outputs of a truncated
QCNN are fed into a standard densely connected
neural net

3. Hybrid CNN in which the outputs of multiple trun-
cated QCNNs are fed into a standard densely con-
nected neural net

The Ô¨Årst model we construct uses only quantum op-
erations to decorrelate the inputs. After preparing the
cluster state dataset on N = 8 qubits, we repeatedly ap-
ply the quantum convolution and pooling layers until the
system size is reduced to 1 qubit. We average the output

1-111Prepare Cluster StateQconv  + QPoolQconv  + QPoolMSELoss-1125

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

QCNN_3 = tfq . layers . PQC (

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

# Feed all QCNNs into a classical NN
concat_out = tf . keras . layers . concatenate (

[ QCNN_1 , QCNN_2 , QCNN_3 ])

dense_1 = tf . keras . layers . Dense (8) ( concat_out )
dense_2 = tf . keras . layers . Dense (1) ( dense_1 )
mu lti _qc on v_m ode l = tf . keras . Model (
inputs =[ excitation_input ] ,
outputs =[ dense_2 ])

We Ô¨Ånd that that for the same optimization settings, the
purely quantum model trains the slowest, while the three-
quantum-Ô¨Ålter hybrid model trains the fastest. This data
is shown in Fig. 18. This demonstrates the advantage
of exploring hybrid quantum-classical architectures for
classifying quantum data.

Figure 16. A simple hybrid architecture in which the outputs
of a truncated QCNN are fed into a classical neural network.

Figure 17. A hybrid architecture in which the outputs of
3 separate truncated QCNNs are fed into a classical neural
network.

Figure 18. Mean squared error loss as a function of training
epoch for three diÔ¨Äerent hybrid classiÔ¨Åers. We Ô¨Ånd that the
purely quantum classiÔ¨Åer trains the slowest, while the hybrid
architecture with multiple quantum Ô¨Ålters trains the fastest.

d1_dual = tf . keras . layers . Dense (8) (

q ua n tu m _ m o de l _d u al )

d2_dual = tf . keras . layers . Dense (1) ( d1_dual )
hybrid_model = tf . keras . Model ( inputs =[

ex cit a tio n _input ] , outputs =[ d2_dual ])

In the code, multi_readout_model_circuit applies just one
round of convolution and pooling, reducing the system
size from 8 to 4 qubits. This hybrid model can be trained
using the same Keras tools as the purely quantum model.
Accuracy plots can be seen in the example notebook.

The third architecture we will explore creates three
independent quantum Ô¨Ålters, and combines the outputs
from all three with a single classical neural network. This
architecture is shown in Fig. 17. This multi-Ô¨Ålter archi-
tecture can be implemented in TFQ as below:

# Build 3 quantum filters
QCNN_1 = tfq . layers . PQC (

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _ s t at e _b i ts ) ,
readouts ) ( cluster_state )

QCNN_2 = tfq . layers . PQC (

B. Hybrid Machine Learning for Quantum Control

To run this example in the browser through Colab,

follow the link:

research/control/control.ipynb

Recently, neural networks have been successfully de-
ployed for solving quantum control problems ranging
from optimizing gate decomposition, error correction
subroutines, to continuous Hamiltonian controls. To fully
leverage the power of neural networks without being hob-
bled by possible computational overhead, it is essential
to obtain a deeper understanding of the connection be-
tween various neural network representations and diÔ¨Äer-
ent types of quantum control dynamics. We demonstrate
tailoring machine learning architectures to underlying
quantum dynamics using TFQ in [114]. As a summary
of how the unique functionalities of TFQ ease quantum
control optimization, we list the problem deÔ¨Ånition and
required TFQ toolboxes as follows.

Prepare Cluster TestQconv  + QPoolMSELoss1-111-11Prepare Cluster StateQconv  + QPoolMSELossQconv  + QPoolQconv  + QPool1-111-1126

This problem can be solved exactly if H and the re-
lationships between g‚àó
and xj are well understood and
j
invertible. Alternatively, one can Ô¨Ånd an approximate
solution using a parameterized controller F in the form
of a feed-forward neural network. We can calculate a set
of control pairs of size N : {xi, yi} with i ‚àà [N ]. We can
input xi into F which is parameterized by its weight ma-
trices {Wi} and biases {bi} of each ith layer. A successful
training will Ô¨Ånd network parameters given by {Wi} and
{bi} such that for any given input xi the network out-
puts gi which leads to a system output H(gi) (cid:117) yi. This
architecture is shown schematically in Fig. 19

There are two important reasons behind the use of
supervised learning for practical control optimization.
Firstly, not all time-invariant Hamiltonian control prob-
lems permit analytical solutions, so inverting the con-
trol error function map can be costly in computation.
Secondly, realistic deployment of even a time-constant
quantum controller faces stochastic Ô¨Çuctuations due to
noise in the classical electronics and systematic errors
which cause the behavior of the system H to deviate
from ideal. Deploying supervised learning with experi-
mentally measured control pairs will therefore enable the
Ô¨Ånding of robust quantum control solutions facing sys-
tematic control oÔ¨Äset and electronic noise. However, this
necessitates seamlessly connecting the output of a clas-
sical neural network with the execution of a quantum
circuit in the laboratory. This functionality is oÔ¨Äered by
TFQ through ControlledPQC .

We showcase the use of supervised learning with hy-
brid quantum-classical feed-forward neural networks in
TFQ for the single-qubit gate decomposition problem. A
general unitary transform on one qubit can be speciÔ¨Åed
by the exponential

U (œÜ, Œ∏1, Œ∏2) = e‚àíiœÜ(cos Œ∏1 ÀÜZ+sin Œ∏1(cos Œ∏2 ÀÜX+sin Œ∏2 ÀÜY )).

(21)

However, it is often preferable to enact single qubit uni-
taries using rotations about a single axis at a time.
Therefore, given a single qubit gate speciÔ¨Åed by the vec-
tor of three rotations {œÜ, Œ∏1, Œ∏2}, we want Ô¨Ånd the control
sequence that implements this gate in the form

U (Œ≤, Œ≥, Œ¥) = eiŒ±e‚àíi Œ≤

2

ÀÜZe‚àíi Œ≥

2

ÀÜY e‚àíi Œ¥

2

ÀÜZ.

(22)

This is the optimal decomposition, namely the Bloch the-
orem, given any unknown single-qubit unitary into hard-
ware friendly gates which only involve the rotation along
a Ô¨Åxed axis at a time.

The Ô¨Årst step in the training involves preparing the
training data. Since quantum control optimization only
focuses on the performance in hardware deployment, the
control inputs and output have to be chosen such that
they are experimentally observable. We deÔ¨Åne the vec-
tor of expectation values yi = [(cid:104) ÀÜX(cid:105)xi , (cid:104) ÀÜY (cid:105)xi , (cid:104) ÀÜZ(cid:105)xi ] of all
single-qubit Pauli operators given by the quantum state

Figure 19. Architecture for hybrid quantum-classical neural
network model for learning a quantum control decomposition.

Target problems:

1. Learning quantum dynamics.
2. Optimizing quantum control signal with regard to

a cost objective

3. Error mitigation in realistic quantum device

Required TFQ functionalities:

1. Hybrid quantum-classical network model
2. Batch quantum circuit simulator
3. Quantum expectation-based backpropagation
4. Fast classical optimizers, both gradient based and

non-gradient based

We exemplify the importance of appropriately choos-
ing the right neural network architecture for the corre-
sponding quantum control problems with two simple but
realistic control optimizations. The two types of con-
trols we have considered cover the full range of quan-
tum dynamics: constant Hamiltonian evolution vs time
dependent Hamiltonian evolution. In the Ô¨Årst problem,
we design a DNN to machine-learn (noise-free) control
of a single qubit. In the second problem, we design an
RNN with long-term memory to learn a stochastic non-
Markovian control noise model.

1. Time-Constant Hamiltonian Control

If the underlying system Hamiltonian is time-invariant
the task of quantum control can be simpliÔ¨Åed with open-
loop optimization. Since the optimal solution is indepen-
dent of instance by instance control actualization, control
optimization can be done oÔ¨Ñine. Let x be the input to a
controller, which produces some control vector g = F (x).
This control vector actuates a system which then pro-
duces a vector output H(g). For a set of control inputs
xj and desired outputs yj, we can then deÔ¨Åne controller
error as ej(F ) = |yj ‚àí H(F (xj))|. The optimal control
problem can then be deÔ¨Åned as minimizing (cid:80)
j ej(F ) for
F . This optimal controller will produce the optimal con-
trol vector g‚àó
j

given xj

MSEprepared by the associated input xi:

27

ÀÜY e‚àíi Œ¥

2

2

0(cid:105) = ÀÜU i
|œài
o |0(cid:105)
|œà(cid:105)x =e‚àíi Œ≤
(cid:104) ÀÜX(cid:105)x = (cid:104)œà|x
(cid:104) ÀÜY (cid:105)x = (cid:104)œà|x
(cid:104) ÀÜZ(cid:105)x = (cid:104)œà|x

2

ÀÜZe‚àíi Œ≥
ÀÜX |œà(cid:105)x ,
ÀÜY |œà(cid:105)x ,
ÀÜZ |œà(cid:105)x .

ÀÜZ |œài

0(cid:105) ,

(23)

(24)

(25)

(26)

(27)

Assuming we have prepared the training dataset, each
set consists of input vectors xi = [œÜ, Œ∏1, Œ∏2] which derives
from the randomly drawn g, unitaries that prepare each
initial state ÀÜU i
, and the associated expectation values
0
yi = [(cid:104) ÀÜX(cid:105)xi, (cid:104) ÀÜY (cid:105)xi, (cid:104) ÀÜZ(cid:105)}xi ].

Now we are ready to deÔ¨Åne the hybrid quantum-
classical neural network model in Keras with Tensfor-
Flow API. To start, we Ô¨Årst deÔ¨Åne the quantum part of
the hybrid neural network, which is a simple quantum
circuit of three single-qubit gates as follows.

con trol_p arams = sympy . symbols ( ‚Äô theta_ {1:3} ‚Äô)
qubit = cirq . GridQubit (0 , 0)
control_circ = cirq . Circuit (

cirq . Rz ( control_params [2]) ( qubit ) ,
cirq . Ry ( control_params [1]) ( qubit ) ,
cirq . Rz ( control_params [0]) ( qubit ) )

We are now ready to Ô¨Ånish oÔ¨Ä the hybrid network by
deÔ¨Åning the classical part, which maps the target params
to the control vector g = {Œ≤, Œ≥, Œ¥}. Assuming we have
deÔ¨Åned the vector of observables ops , the code to build
the model is:

circ_in = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

x_in = tf . keras . Input ((3 ,) )
d1 = tf . keras . layers . Dense (128) ( x_in )
d2 = tf . keras . layers . Dense (128) ( d1 )
d3 = tf . keras . layers . Dense (64) ( d2 )
g = tf . keras . layers . Dense (3) ( d3 )
exp_out = tfq . layers . ControlledPQC (

control_circ , ops ) ([ circ_in , x_in ])

Now, we are ready to put everything together to deÔ¨Åne
and train a model in Keras. The two axis control model
is deÔ¨Åned as follows:

model = tf . keras . Model (

inputs =[ circ_in , x_in ] , outputs = exp_out )

To train this hybrid supervised model, we deÔ¨Åne an opti-
mizer, which in our case is the Adam optimizer, with an
appropriately chosen loss function:

model . compile ( tf . keras . Adam , loss = ‚Äô mse ‚Äô)

We Ô¨Ånish oÔ¨Ä by training on the prepared supervised data
in the standard way:

hist ory _ two _ axis = model . fit (...

The training converges after around 100 epochs as seen
in Fig. 20, which also shows excellent generalization to
validation data.

Figure 20. Mean square error on training dataset, and valida-
tion dataset, each of size 5000, as a function training epoch.

2. Time-dependent Hamiltonian Control

Now we consider a second kind of quantum control
problem, where the actuated system H is allowed to
change in time. If the system is changing with time, the
optimal control g‚àó is also generally time varying. Gener-
alizing the discussion of section IV B 1, we can write the
time varying control error given the time-varying con-
troller F (t) as ej(F (t), t) = |yj ‚àí H(F (xj, t), t)|. The
optimal control can then be written as g‚àó(t) = ¬Øg‚àó + Œ¥(t).
This task is signiÔ¨Åcantly harder than the problem dis-
cussed in section IV B 1, since we need to learn the hidden
variable Œ¥(t) which can result in potentially highly com-
plex real-time system dynamics. We showcase how TFQ
provides the perfect toolbox for such diÔ¨Écult control op-
timization with an important and realistic problem of
learning and thus compensating the low frequency noise.
One of the main contributions to time-drifting errors
in realistic quantum control is 1/f Œ±- like errors, which
encapsulate errors in the Hamiltonian amplitudes whose
frequency spectrum has a large component at the low
frequency regime. The origin of such low frequency noise
remains largely controversial. Mathematically, we can
parameterize the low frequency noise in the time domain
with the amplitude of the Pauli Z Hamiltonian on each
qubit as:

ÀÜHlow(t) = Œ±te ÀÜZ.

(28)

A simple phase control signal is given by œâ(t) = œâ0t with
the Hamiltonian ÀÜH0(t) = œâ0t ÀÜZ. The time-dependant
wavefunction is then given by

|œà(ti)(cid:105) = T[e

(cid:82) ti
0 ( ÀÜHlow(t)+ ÀÜH0(t))dt] |+(cid:105) .

(29)

We can attempt to learn the noise parameters Œ± and e
by training a recurrent neural network to perform time-
series prediction on the noise.
In other words, given a
record of expectation values {(cid:104)œà(ti)| ÀÜX |œà(ti)(cid:105)} for ti ‚àà
{0, Œ¥t, 2Œ¥t, . . . , T } obtained on state |œà(t)(cid:105), we want the
RNN to predict the future observables {(cid:104)œà(ti)| ÀÜX |œà(ti)(cid:105)}
for ti ‚àà {T, T + Œ¥t, T + 2Œ¥t, . . . , 2T }.

28

C. Simulating Noisy Circuits

To run this example in the browser through Colab,

follow the link:

docs/tutorials/qcnn.ipynb

1. Background

Noise is present in modern day quantum computers.
Qubits are susceptible to interference from the surround-
ing environment, imperfect fabrication, TLS and some-
times even cosmic rays [115, 116]. Until large scale error
correction is reached, the algorithms of today must be
able to remain functional in the presence of noise. This
makes testing algorithms under noise an important step
for validating quantum algorithms / models will function
on the quantum computers of today.

Target problems:

1. Simulating noisy circuits
2. Compare hybrid quantum-classical model perfor-
mance under diÔ¨Äerent types and strengths of noise

Required TFQ functionalities:

1. Serialize Cirq circuits which contain channels
2. Quantum trajectory simulation with qsim

2. Noise in Cirq and TFQ

Noise on a quantum computer impacts the bitstring
samples you are able to measure from it. One intuitive
way to think about noise on a quantum computer is that
it will "insert", "delete" or "replace" gates in random
places. An example is shown in Fig. 22, where the actual
circuit run has one gate corrupted and two gates deleted
compared to the desired circuit.

Figure 22.
Illustration of the eÔ¨Äect of noise on a quantum
circuit. On the left is an example circuit on three qubits; on
the right, the same circuit is shown after corruption by noise.
An initial Hadamard gate has been changed to an X gate, and
the Ô¨Ånal Hadamard and CNot have been deleted.

Building oÔ¨Ä of this intuition, when dealing with noise,
you are no longer using a single pure state |œà(cid:105) but instead
dealing with an ensemble of all possible noisy realizations
of your desired circuit, œÅ = (cid:80)
j pj |œàj(cid:105) (cid:104)œàj|, where pj gives

Figure 21. Mean square error on LSTM predictions on 500
randomly generated inputs.

There are several possible ways to build such an RNN.
One option is recording several timeseries on a device a-
priori, later training and testing an RNN oÔ¨Ñine. Another
option is an online method, which would allow for real-
time controller tuning. The oÔ¨Ñine method will be brieÔ¨Çy
explained here, leaving the details of both methods to
the notebook associated with this example.

First, we can use TFQ or Cirq to prepare several time-
series for testing and validation. The function below per-
forms this task using TFQ:

def generate_data ( end_time , timesteps , omega_0 ,

exponent , alpha ) :
t_steps = linspace (0 , end_time , timesteps )
q = cirq . GridQubit (0 , 0)
phase = sympy . symbols ( " phaseshift " )
c = cirq . Circuit ( cirq . H ( q ) ,

cirq . Rz ( phase_s ) ( q ) )

ops = [ cirq . X ( q ) ]
phases = t_steps * omega_0 +

t_steps **( exponent + 1) /( exponent + 1)

return tfq . layers . Expectation () (

c ,
symbol_names = [ phase ] ,
symbol_values = transpose ( phases ) ,
operators = ops )

We can use this function to prepare many realizations of
the noise process, which can be fed to an LSTM deÔ¨Åned
using tf.keras as below:

model = tf . keras . Sequential ([

tf . keras . layers . LSTM (

rnn_units ,
r e c u r r e n t _ i n i t i a l i z e r = ‚Äô glorot_uniform ‚Äô ,
ba t ch _i n put _s hap e =[ batch_size , None , 1]) ,

tf . keras . layers . Dense (1) ])

We can then train this LSTM on the realizations and
evaluate the success of our training using validation data,
on which we calculate prediction accuracy. A typical ex-
ample of this is shown in Fig. 21. The LSTM converges
quickly to within the accuracy from the expectation value
measurements within 30 epochs.

the probability that the actual state of the quantum com-
puter after your noisy circuit is |œàj(cid:105) .

Revisiting Fig. 22, if we knew beforehand that 90% of
the time our system executed perfectly, or errored 10%
of the time with just this one mode of failure, then our
ensemble would be:

œÅ = 0.9 |œàdesired(cid:105) (cid:104)œàdesired| + 0.1 |œànoisy(cid:105) (cid:104)œànoisy| .
If there was more than just one way that our circuit could
error, then the ensemble œÅ would contain more than just
two terms (one for each new noisy realization that could
happen). œÅ is referred to as the density operator describ-
ing your noisy system.

in practice,

Unfortunately,

it‚Äôs nearly impossible to
know all the ways your circuit might error and their exact
probabilities. A simplifying assumption you can make
is that after each operation in your circuit a quantum
channel is applied. Just like a quantum circuit produces
pure states, a quantum channel produces density opera-
tors. Cirq has a selection of built-in quantum channels
to model the most common ways a quantum circuit can
go wrong in the real world. For example, here is how you
would write down a quantum circuit with depolarizing
noise:

29

3. Training Models with Noise

Now that you have implemented some noisy circuit
simulations in TFQ, you can experiment with how noise
impacts hybrid quantum-classical models. Since quan-
tum devices are currently in the NISQ era, it is important
to compare model performance in the noisy and noiseless
cases, to ensure performance does not degrade excessively
under noise.

A good Ô¨Årst check to see if a model or algorithm is
robust to noise is to test it under a circuit-wide depolar-
izing model. An example circuit with depolarizing noise
applied is shown in Fig. 23. Applying a channel to a
circuit using the circuit.with_noise(channel) method cre-
ates a new circuit with the speciÔ¨Åed channel applied after
each operation in the circuit. In the case of a depolariz-
ing channel, one of {X, Y, Z} is applied with probability
p and an identity (keeping the original operation) with
probability 1 ‚àí p.

def x_circuit ( qubits ) :

""" Produces an X wall circuit on ‚Äò qubits ‚Äò. """
return cirq . Circuit ( cirq . X . on_each (* qubits ) )

def make_noisy ( circuit , p ) :

""" Adds a depolarization channel to all qubits

in ‚Äò circuit ‚Äò before measurement . """

Figure 23. Example circuit before and after the application
of circuit-wide depolarizing noise.

return circuit + cirq . Circuit (

cirq . depolarize ( p ) . on_each (
* circuit . all_qubits () ) )

my_qubits = cirq . GridQubit . rect (1 , 2)
my_circuit = x_circuit ( my_qubits )
my_n ois y _ci r cuit = make_noisy ( my_circuit , 0.5)

Both my_circuit and my_noisy_circuit can be mea-
sured, and their outputs compared. In the noiseless case
you would always expect to sample the |11(cid:105) state. But
in the noisy case there is a nonzero probability of sam-
pling |00(cid:105) or |01(cid:105) or |10(cid:105) as well. See the notebook for a
demonstration of this eÔ¨Äect.

trajectory methods

With this understanding of how noise can im-
pact circuit execution, we are ready to move on
in TFQ. TensorFlow Quan-
to how noise works
simulate noise,
tum uses
To enable noisy simu-
as described in II F 6.
lations,
the
option is available on
tfq.layers.Sample ,
and
tfq.layers.Expectation . For example, here is how to in-
stantiate and call the tfq.layers.Sample on the noisy cir-
cuit created above:

tfq.layers.SampledExpectation

backend=‚Äônoisy‚Äô

to

""" Draw samples from ‚Äò my_noisy_circuit ‚Äò """
bitstrings = tfq . layers . Sample ( backend = ‚Äô noisy ‚Äô) (

my_noisy_circuit , repetitions =1000)

See the notebook for more examples of noisy usage of
TFQ layers.

In the notebook, we show how to create a noisy dataset
based on the XXZ chain dataset described in section
II E 6. This noisy dataset will be called with the func-
tion modelling_circuit . Then, we build a tf.keras.Model
which uses a noisy PQC layer to process the quan-
tum data, along with Dense layers for classical post-
processing. The code to build the model is shown below:

def bui ld_ ker as _mo del ( qubits , depolarize_p =0.) :
""" Prepare a noisy hybrid quantum classical

Keras model . """

spin_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

c i r c u i t _a n d _ r e a d o u t = mo del lin g_c ir cui t (

qubits , 4 , depolarize_p )

if depolarize_p >= 1e -5:

quantum_model = tfq . layers . NoisyPQC (

* circuit_and_readout ,
sample_based = False ,
repetitions =10

) ( spin_input )

else :

quantum_model = tfq . layers . PQC (

* c i r c u i t _ an d _ r e a d o u t ) ( spin_input )

intermediate = tf . keras . layers . Dense (

4 , activation = ‚Äô sigmoid ‚Äô) ( quantum_model )

post_process = tf . keras . layers . Dense (1) (

intermediate )

return tf . keras . Model (

inputs =[ spin_input ] ,
outputs =[ post_process ])

The goal of this model is to correctly classify the quantum
datapoints according to their phase. For the XXZ chain,
the two possible phases are the critical metallic phase
and the insulating phase. Fig. 24 shows the accuracy of
the model as a function of number of training epochs, for
both noisy and noiseless training data.

Figure 24. Accuracy versus training epoch for a quantum clas-
siÔ¨Åer. The blue line shows the performance of the classiÔ¨Åer
when trained on clean quantum data, while the orange line
shows the performance of the classiÔ¨Åer given noisy quantum
data. Note that the Ô¨Ånal values for accuracy are approxi-
mately equal between the noisy and noiseless cases.

Notice that when the model is trained on noisy data,
it achieves a similar accuracy as when it is trained on
noiseless data. This shows that QML models can still
succeed at learning despite the presence of mild noise.
You can adapt this workÔ¨Çow to examine the performance
of your own QML models in the presence of noise.

D. Quantum Approximate Optimization

To run this example in the browser through Colab,

follow the link:

research/qaoa/qaoa.ipynb

including Hamiltonians of higher-order and continuous-
variable Hamiltonians [49].

30

j

j=1 zpj

In general, the goal of the QAOA for binary variables
is to Ô¨Ånd approximate minima of a pseudo Boolean func-
tion f on n bits, f (z), z ‚àà {‚àí1, 1}√ón. This function is of-
ten an mth-order polynomial of binary variables for some
positive integer m, e.g., f (z) = (cid:80)
p‚àà{0,1}m Œ±pzp, where
zp = (cid:81)n
. QAOA has been applied to NP-hard
problems such as Max-Cut [12] or Max-3-Lin-2 [118].
The case where this polynomial is quadratic (m = 2)
has been extensively explored in the literature. It should
be noted that there have also been recent advances us-
ing quantum-inspired machine learning techniques, such
as deep generative models, to produce approximate solu-
tions to such problems [119, 120]. These 2-local problems
will be the main focus in this example. In this tutorial,
we Ô¨Årst show how to utilize TFQ to solve a MaxCut in-
stance with QAOA with p = 1.

The QAOA approach to optimization Ô¨Årst starts in
an initial product state |œà0(cid:105)‚äón and then a tunable gate
sequence produces a wavefunction with a high probability
of being measured in a low-energy state (with respect to
a cost Hamiltonian).

Let us deÔ¨Åne our parameterized quantum circuit
ansatz. The canonical choice is to start with a uniform
superposition |œà0(cid:105)‚äón = |+(cid:105)‚äón = 1‚àö
x‚àà{0,1}n |x(cid:105)),
hence a Ô¨Åxed state. The QAOA unitary itself then con-
sists of applying

2n ((cid:80)

ÀÜU (Œ∑, Œ≥) =

P
(cid:89)

j=1

e‚àíiŒ∑j ÀÜHM e‚àíiŒ≥j ÀÜHC ,

(30)

j‚ààV

onto the starter state, where ÀÜHM = (cid:80)
ÀÜXj is known
as the mixer Hamiltonian, and ÀÜHC ‚â° f ( ÀÜZ) is our
cost Hamiltonian, which is a function of Pauli opera-
tors ÀÜZ = { ÀÜZj}n
. The resulting state is given by
|Œ®Œ∑Œ≥(cid:105) = ÀÜU (Œ∑, Œ≥) |+(cid:105)‚äón, which is our parameterized out-
put. We deÔ¨Åne the energy to be minimized as the expec-
tation value of the cost Hamiltonian ÀÜHC ‚â° f ( ÀÜZ), where
ÀÜZ = { ÀÜZj}n
with respect to the output parameterized
state.

j=1

j=1

1. Background

Target problems:

In this section, we introduce the basics of Quantum
Approximate Optimization Algorithms and show how to
implement a basic case of this class of algorithms in TFQ.
In the advanced applications section V, we explore how to
apply meta-learning techniques [117] to the optimization
of the parameters of the algorithm.

The Quantum Approximate Optimization Algorithm
was originally proposed to solve instances of the Max-
Cut problem [12]. The QAOA framework has since been
extended to encompass multiple problem classes related
to Ô¨Ånding the low-energy states of Ising Hamiltonians,

1. Train a parameterized quantum circuit for a dis-

crete optimization problem (MaxCut)

2. Minimize a cost function of a parameterized quan-

tum circuit

Required TFQ functionalities:

1. Conversion of simple circuits to TFQ tensors
2. Evaluation of gradients for quantum circuits
3. Use of gradient-based optimizers from TF

2.

Implementation

For the MaxCut QAOA, the cost Hamiltonian function

f is a second order polynomial of the form,

ÀÜHC = f ( ÀÜZ) =

(cid:88)

{j,k}‚ààE

1

2 ( ÀÜI ‚àí ÀÜZj ÀÜZk),

(31)

where G = {V, E} is a graph for which we would like to
Ô¨Ånd the MaxCut; the largest size subset of edges (cut
set) such that vertices at the end of these edges belong
to a diÔ¨Äerent partition of the vertices into two disjoint
subsets [12].

To train the QAOA, we simply optimize the expecta-
tion value of our cost Hamiltonian with respect to our
parameterized output to Ô¨Ånd (approximately) optimal
parameters; Œ∑‚àó, Œ≥‚àó = argmin
Œ∑,Œ≥L(Œ∑, Œ≥) where L(Œ∑, Œ≥) =
(cid:104)Œ®Œ∑Œ≥| ÀÜHC |Œ®Œ∑Œ≥(cid:105) is our loss. Once trained, we use the
QPU to sample the probability distribution of measure-
ments of the parameterized output state at optimal an-
gles in the standard basis, x ‚àº p(x) = | (cid:104)x|Œ®Œ∑‚àóŒ≥‚àó (cid:105) |2,
and pick the lowest energy bitstring from those samples
as our approximate optimum found by the QAOA.

Let us walkthrough how to implement such a basic
QAOA in TFQ. The Ô¨Årst step is to generate an instance
of the MaxCut problem. For this tutorial we generate
a random 3-regular graph with 10 nodes with NetworkX
[121].

# generate a 3 - regular graph with 10 nodes
maxcut_graph = nx . r a n d o m _ r e g u l a r _ g r a p h ( n =10 , d =3)
The next step is to allocate 10 qubits, to deÔ¨Åne
the Hadamard layer generating the initial superposition
state, the mixing Hamiltonian HM and the cost Hamil-
tonian HP.
# define 10 qubits
cirq_qubits = cirq . GridQubit . rect (1 , 10)

# create layer of hadamards to initialize the

superposition state of all computational
states

hada mar d _ci r cuit = cirq . Circuit ()
for node in maxcut_graph . nodes () :
qubit = cirq_qubits [ node ]
ha dam a rd_ c ircuit . append ( cirq . H . on ( qubit ) )

# define the two parameters for one block of

QAOA

qao a_param eters = sympy . symbols ( ‚Äôa b ‚Äô)

# define the the mixing and the cost

Hamiltonians

mixing_ham = 0
for node in maxcut_graph . nodes () :
qubit = cirq_qubits [ node ]
mixing_ham += cirq . PauliString ( cirq . X ( qubit )
)

cost_ham = maxcut_graph . number_of_edges () /2
for edge in maxcut_graph . edges () :
qubit1 = cirq_qubits [ edge [0]]
qubit2 = cirq_qubits [ edge [1]]
cost_ham += cirq . PauliString (1/2*( cirq . Z (
qubit1 ) * cirq . Z ( qubit2 ) ) )

31

With this, we generate the unitaries representing the
quantum circuit

# generate the qaoa circuit
qaoa_circuit = tfq . util . exponential ( operators =
[ cost_ham , mixing_ham ] , coefficients =
qaoa_parameters )

Subsequently, we use these ingredients to build our
model. We note here in this case that QAOA has no
input data and labels, as we have mapped our graph to
the QAOA circuit. To use the TFQ framework we specify
the Hadamard circuit as input and convert it to a TFQ
tensor. We may then construct a tf.keras model using
our QAOA circuit and cost in a TFQ PQC layer, and
use a single instance sample for training the variational
parameters of the QAOA with the Hadamard gates as an
input layer and a target value of 0 for our loss function,
as this is the theoretical minimum of this optimization
problem.

This translates into the following code:

# define the model and training data
model_circuit , model_readout = qaoa_circuit ,

cost_ham

input_ = [ hadamard_circuit ]
input_ = tfq . con ver t_ to_ ten sor ( input_ )
optimum = [0]

# Build the Keras model .
optimum = np . array ( optimum )
model = tf . keras . Sequential ()
model . add ( tf . keras . layers . Input ( shape =() , dtype =

tf . dtypes . string ) )

model . add ( tfq . layers . PQC ( model_circuit ,

model_readout ) )

To optimize the parameters of the ansatz state, we use
a classical optimization routine. In general, it would be
possible to use pre-calculated parameters [122] or to im-
plement for QAOA tailored optimization routines [123].
For this tutorial, we choose the Adam optimizer imple-
mented in TensorFlow. We also choose the mean absolute
error as our loss function.

model . compile ( loss = tf . keras . losses .

mean_absolute_error , optimizer = tf . keras .
optimizers . Adam () )

history = model . fit ( input_ , optimum , epochs =1000 ,

verbose =1)

V. ADVANCED QUANTUM APPLICATIONS

The following applications represent how we have ap-
plied TFQ to accelerate their discovery of new quantum
algorithms. The examples presented in this section are
newer research as compared to the previous section, as
such they have not had as much time for feedback from
the community. We include these here as they are demon-
stration of the sort of advanced QML research that can be
accomplished by combining several building blocks pro-
vided in TFQ. As many of these examples involve the
building and training of hybrid quantum-classical models

32

the choice of parameters after each iteration of quantum-
classical optimization can be seen as the task of generat-
ing a sequence of parameters which converges rapidly to
an approximate optimum of the landscape, we can use a
type of classical neural network that is naturally suited
to generate sequential data, namely, recurrent neural net-
works. This technique was derived from work by Deep-
Mind [117] for optimization of classical neural networks
and was extended to be applied to quantum neural net-
works [46].

The application of such classical learning to learn tech-
niques to quantum neural networks was Ô¨Årst proposed in
[46].
In this work, an RNN (long short term memory;
LSTM) gets fed the parameters of the current iteration
and the value of the expectation of the cost Hamiltonian
of the QAOA, as depicted in Fig. 25. More precisely, the
RNN receives as input the previous QNN query‚Äôs esti-
mated cost function expectation yt ‚àº p(y|Œ∏t), where yt
is the estimate of (cid:104) ÀÜH(cid:105)t
, as well as the parameters for
which the QNN was evaluated Œ∏t. The RNN at this time
step also receives information stored in its internal hid-
den state from the previous time step ht. The RNN itself
has trainable parameters œï, and hence it applies the pa-
rameterized mapping

ht+1, Œ∏t+1 = RNNœï(ht, Œ∏t, yt)

(32)

which generates a new suggestion for the QNN parame-
ters as well as a new hidden state. Once this new set of
QNN parameters is suggested, the RNN sends it to the
QPU for evaluation and the loop continues.

The RNN is trained over random instances of QAOA
problems selected from an ensemble of possible QAOA
MaxCut problems. See the notebook for full details on
the meta-training dataset of sampled problems.

The loss function we chose for our experiments is the
observed improvement at each time step, summed over
the history of the optimization:

L(œï) = Ef,y

(cid:20) T
(cid:80)
t=1

(cid:21)
min{f (Œ∏t) ‚àí minj<t[f (Œ∏j)], 0}

, (33)

The observed improvement at time step t is given by the
diÔ¨Äerence between the proposed value, f (Œ∏t), and the
best value obtained over the history of the optimization
until that point, minj<t[f (Œ∏j)].

In our particular example in this section, we will con-
sider a time horizon of 5 time steps, hence the RNN will
have to learn to very rapidly approximately optimize the
parameters of the QAOA. Results are featured in Fig. 26.
The details of the implementation are available in the Co-
lab. Here is an overview of the problem that was tackled
and the TFQ features that facilitated this implementa-
tion:

Target problems:

1. Learning to learn with quantum neural networks

via classical neural networks

Figure 25. Quantum-classical computational graph for the
meta-learning optimization of the recurrent neural network
(RNN) optimizer and a quantum neural network (QNN) over
several optimization iterations. The hidden state of the RNN
is represented by h, we represent the Ô¨Çow of data used to eval-
uate the meta-learning loss function. This meta loss function
L is a functional of the history of expectation value estimate
samples y = {yt}T
t=1, it is not directly dependent on the RNN
parameters œï. TFQ‚Äôs hybrid quantum-classical backpropaga-
tion then becomes key to train the RNN to learn to optimize
the QNN, which in our particular example was the QAOA.
Figure taken from [46], originally inspired from [117].

and advanced optimizers, such research would be much
more diÔ¨Écult to implement without TFQ. In our re-
searchers‚Äô experience, the performance gains and the ease
of use of TFQ decreased the time-to-working-prototype
from weeks to days or even hours when it is compared to
using alternative tools.

Finally, as we would like to provide users with ad-
vanced examples to see TFQ in action for research use-
cases beyond basic implementations, along with the ex-
amples presented in this section are several notebooks
accessible on Github:

github.com/tensorflow/quantum/tree/research

We encourage readers to read the section below for an
overview of the theory and use of TFQ functions and
would encourage avid readers who want to experiment
with the code to visit the full notebooks.

A. Meta-learning for Variational Quantum
Optimization

To run this example in the browser through Colab,

follow the link:

research/metalearning_qaoa/metalearning_qaoa.ipynb

In section IV D, we have shown how to implement basic
QAOA in TFQ and optimize it with a gradient-based
optimizer, we can now explore how to leverage classical
neural networks to optimize QAOA parameters. To run
this example in the browser via Colab, follow the link:

In recent works, the use of classical recurrent neural
networks to learn to optimize the parameters [46] (or gra-
dient descent hyperparameters [124]) was proposed. As

33

overwhelming volume of quantum space with an expo-
nentially small gradient, making straightforward training
impossible if on enters one of these dead regions. The rate
of this vanishing increases exponentially with the num-
ber of qubits and depends on whether the cost function
is global or local [128]. While strategies have been devel-
oped to deal with the challenges of vanishing gradients
classically [129], the combination of diÔ¨Äerences in read-
out complexity and other constraints of unitarity make
direct implementation of these Ô¨Åxes challenging. In par-
ticular, the readout of information from a quantum sys-
tem has a complexity of O(1/(cid:15)Œ±) where (cid:15) is the desired
precision, and Œ± is some small integer, while the complex-
ity of the same task classically often scales as O(log 1/(cid:15))
[130]. This means that for a vanishingly small gradi-
ent (e.g. 10‚àí7), a classical algorithm can easily obtain
at least some signal, while a quantum-classical one may
diÔ¨Äuse essentially randomly until ‚àº 1014 samples have
been taken. This has fundamental consequences for the
methods one uses to train, as we detail below. The re-
quirement on depth to reach these plateaus is only that
a portion of the circuit approximates a unitary 2‚àídesign
which can occur at a depth occurring at O(n1/d) where
n is the number of qubits and d is the dimension of the
connectivity of the quantum circuit, possibly requiring as
little depth as O(log(n)) in the all-to-all case [84]. One
may imagine that a solution to this problem could be
to simply initialize a circuit to the identity to avoid this
problem, but this incurs some subtle challenges. First,
such a Ô¨Åxed initialization will tend to bias results on gen-
eral datasets. This challenge has been studied in the
context of more general block initialization of identity
schemes [131].

Perhaps the more insidious way this problem arises, is
that training with a method like stochastic gradient de-
scent (or sophisticated variants like Adam) on the entire
network, can accidentally lead one onto a barren plateau
if the learning rate is too high. This is due to the fact
that the barren plateaus argument is one of volume of
space and quantum-classical information exchange, and
random diÔ¨Äusion in parameter space will tend to lead
one onto a plateau. This means that even the most
clever initialization can be thwarted by the impact of
this phenomenon on the training process. In practice this
severely limits learning rate and hence training eÔ¨Éciency
of QNNs.

For this reason, one can consider training on subsets of
the network which do not have the ability to completely
randomize during a random walk. This layerwise learning
strategy allows one to use larger learning rates and im-
proves training eÔ¨Éciency in quantum circuits [132]. We
advocate the use of these strategies in combination with
appropriately designed local cost functions in order to cir-
cumvent the dramatically worse problems with objectives
like Ô¨Ådelity [126, 128]. TFQ has been designed to make
experimenting with both of these strategies straightfor-
ward for the user, as we now document. For an example

Figure 26. The path chosen by the RNN optimizer on a 12-
qubit MaxCut problem after being trained on a set of random
10-qubit MaxCut problems. We see that the neural network
learned to generalize its heuristic to larger system sizes, as
originally pointed out in [46].

2. Building a neural-network-based optimizer

for

QAOA

3. Lowering the number of iterations needed to opti-

mize QAOA

Required TFQ functionalities:

1. Hybrid quantum-classical networks and hybrid

backpropagation

2. Batching training over quantum data (QAOA prob-

lem instances)

3. Integration with TF for the classical RNN

B. Vanishing Gradients and Adaptive Layerwise
Training Strategies

1. Random Quantum Circuits and Barren Plateaus

When using parameterized quantum circuits for a
learning task, inevitably one must choose an initial con-
Ô¨Åguration and training strategy that is compatible with
that initialization. In contrast to problems more known
structure, such as speciÔ¨Åc quantum simulation tasks [25]
or optimizations [125], the structure of circuits used for
learning may need to be more adaptive or general to en-
compass unknown data distributions.
In classical ma-
chine learning, this problem can be partially addressed
by using a network with suÔ¨Écient expressive power and
random initialization of the network parameters.

Unfortunately, it has been proven that due to funda-
mental limits on quantum readout complexity in com-
bination with the geometry of the quantum space, an
exact duplication of this strategy is doomed to fail [126].
In particular, in analog to the vanishing gradients prob-
lem that has plagued deep classical networks and histor-
ically slowed their progress [127], an exacerbated version
of this problem appears in quantum circuits of suÔ¨Écient
depth that are randomly initialized. This problem, also
known as the problem of barren plateaus, refers to the

of barren plateaus, see the notebook at the following link:

docs/tutorials/barren_plateaus.ipynb

2. Layerwise quantum circuit learning

So far, the network training methods demonstrated in
section IV have focused on simultaneous optimization of
all network parameters. As alluded to in the section on
the barren plateau eÔ¨Äect (V B), this type of strategy can
lead to vanishing gradients as the number of qubits and
layers in a random circuit grows. A parameter initializa-
tion strategy to avoid this eÔ¨Äect at the initial training
steps has been proposed in [131]. This strategy initial-
izes layers in a blockwise fashion such that pairs of layers
are randomly initialized but yield an identity operation
when acting on the circuit and thereby prevent initializa-
tion on a plateau. However, as described in section V B,
this does not avert the possibility for the circuit to drift
onto a plateau during training. In this section, we will
implement a layerwise training strategy that avoids ini-
tialization on a plateau as well as drifting onto a plateau
during training by only training subsets of parameters in
the circuit at a given update step [132].

The strategy consists of two training phases. In phase
one, the algorithm constructs the circuit by subsequently
adding and training layers of a predeÔ¨Åned structure. We
start by picking a number of initial layers s, that con-
tains parameters which are always active during training
to avoid entering a regime where the number of active
parameters is too small to decrease the loss [133]. These
layers are then trained for a Ô¨Åxed number of iterations el,
after which another set of layers is added to the circuit.
How many layers this set contains is controlled by a hy-
perparameter p. Another hyperparameter q determines
after how many layers the parameters in previous layers
are frozen. I.e., for p = 2 and q = 4, we add two layers at
intervals of el iterations, and only train the parameters
in the last four layers, while all other parameters in the
circuit are kept Ô¨Åxed. This procedure is repeated until a
Ô¨Åxed, predeÔ¨Åned circuit depth is reached.

In phase two, we take the Ô¨Ånal circuit from phase one
and now split the parameters into larger partitions. A
hyperparameter r speciÔ¨Åes the percentage of parameters
which are trained in the circuit simultaneously, i.e., for
r = 0.5 we split the circuit in two halves and alternate
between training each of these two halves where the other
half‚Äôs parameters are kept Ô¨Åxed. By this approach, any
randomization eÔ¨Äect that occurs during training is con-
tained to a subset of the circuit parameters and eÔ¨Äec-
tively prevents drifting onto a plateau, as can be seen in
the original paper for the case of randomization induced
by sampling noise [132]. The size of these partitions has
to be chosen with care, as overly large partitions will
increase the probability of randomization, while overly
small partitions may be insuÔ¨Écient to decrease the loss
[133]. This alternate training of circuit partitions is then

34

performed until the loss converges or we reach a prede-
Ô¨Åned number of iterations.

In the following, we will

implement phase one
of the algorithm, and a complete implementation of
both phases can be found in the accompanying notebook.

Target problems:

1. Dynamically building circuits for arbitrary learning

tasks

2. Manipulating circuit structure and parameters dur-

ing training

3. Reducing the number of trained parameters
4. Avoiding initialization on or drifting to a barren

plateau

Required TFQ functionalities:

1. Parameterized circuit layers
2. Keras weight manipulation interface
3. Parameter shift diÔ¨Äerentiator for exact gradient

computation

To run this example in the browser through Colab,

follow the link:

research/layerwise_learning/layerwise_learning.ipynb

As an example to show how this functionality may be
explored in TFQ, we will look at randomly generated
layers as shown in section V B, where one layer consists
of a randomly chosen rotation gate around the X, Y , or
Z axis on each qubit, followed by a ladder of CZ gates
over all qubits.

def create_layer ( qubits , layer_id ) :

# create symbols for trainable parameters
symbols = [

sympy . Symbol (

f ‚Äô{ layer_id } -{ str ( i ) } ‚Äô)

for i in range ( len ( qubits ) ) ]

# build layer from random gates
gates = [

random . choice ([

cirq . Rx , cirq . Ry , cirq . Rz ]) (
symbols [ i ]) ( q )

for i , q in enumerate ( qubits ) ]

# add connections between qubits
for control , target in zip (
qubits , qubits [1:]) :
gates . append ( cirq . CZ ( control , target ) )

return gates , symbols

We assume that we don‚Äôt know the ideal circuit struc-
ture to solve our learning problem, so we start with the
shallowest circuit possible and let our model grow from
there. In this case we start with one initial layer s = 1,
and add a new layer after it has trained for el = 10
epochs. First, we need to specify some variables:

# number of qubits and layers in our circuit
n_qubits = 6
n_layers = 8

# define data and readout qubits
data_qubits = cirq . GridQubit . rect (1 , n_qubits )
readout = cirq . GridQubit (0 , n_qubits -1)
readout_op = cirq . Z ( readout )

# symbols to parametrize circuit
symbols = []
layers = []
weights = []

We use the same training data as speciÔ¨Åed in the TFQ
MNIST classiÔ¨Åer example notebook available in the TFQ
Github repository, which encodes a downsampled version
of the digits into binary vectors. Ones in these vectors
are encoded as local X gates on the corresponding qubit
in the register, as shown in [24]. For this reason, we also
use the readout procedure speciÔ¨Åed in that work where
a sequence of XHX gates is added to the readout qubit
at the end of the circuit. Now we train the circuit, layer
by layer:

for layer_id in range ( n_layers ) :
circuit = cirq . Circuit ()
layer , layer_symbols = create_layer (

data_qubits , f ‚Äô layer_ { layer_id } ‚Äô)

layers . append ( layer )

circuit += layers
symbols += layer_symbols

# set up the readout qubit
circuit . append ( cirq . X ( readout ) )
circuit . append ( cirq . H ( readout ) )
circuit . append ( cirq . X ( readout ) )
readout_op = cirq . Z ( readout )

# create the Keras model
model = tf . keras . Sequential ()
model . add ( tf . keras . layers . Input (

shape =() , dtype = tf . dtypes . string ) )

model . add ( tfq . layers . PQC (

model_circuit = circuit ,
operators = readout_op ,
differentiator = ParameterShift () ,
initializer = tf . keras . initializers . Zeros )

)
model . compile (

loss = tf . keras . losses . squared_hinge ,
optimizer = tf . keras . optimizers . Adam (

learning_rate =0.01) )

# Update model parameters and add
# new 0 parameters for new layers .
model . set_weights (

[ np . pad ( weights , ( n_qubits , 0) ) ])

model . fit ( x_train ,
y_train ,
batch_size =128 ,
epochs =10 ,
verbose =1 ,
validation_data =( x_test , y_test ) )

qnn_results = model . evaluate ( x_test , y_test )

# store weights after training a layer
weights = model . get_weights () [0]

In general, one can choose many diÔ¨Äerent conÔ¨Ågura-

35

tions of how many layers should be trained in each step.
One can also control which layers are trained by manip-
ulating the symbols we feed into the circuit and keeping
track of the weights of previous layers. The number of
layers, layers trained at a time, epochs spent on a layer,
and learning rate are all hyperparameters whose optimal
values depend on both the data and structure of the cir-
cuits being used for learning. This example is meant to
exemplify how TFQ can be used to easily explore these
choices to maximize the eÔ¨Éciency and eÔ¨Écacy of training.
See our notebook linked above for the complete imple-
mentation of these features. Using TFQ to explore this
type of learning strategy relieves us of manually imple-
menting training procedures and optimizers, and autod-
iÔ¨Äerentiation with the parameter shift rule. It also lets us
readily use the rich functionality provided by TensorFlow
and Keras. Implementing and testing all of the function-
ality needed in this project by hand could take up to a
week, whereas all this eÔ¨Äort reduces to a couple of lines of
code with TFQ as shown in the notebook. Additionally,
it lets us speed up training by using the integrated qsim
simulator as shown in section II F 4. Last but not least,
TFQ provides a thoroughly tested and maintained QML
framework which greatly enhances the reproducibility of
our research.

C. Hamiltonian Learning with Quantum Graph
Recurrent Neural Networks

1. Motivation: Learning Quantum Dynamics with a
Quantum Representation

Quantum simulation of time evolution was one of the
original envisioned applications of quantum computers
when they were Ô¨Årst proposed by Feynman [9]. Since
then, quantum time evolution simulation methods have
seen several waves of great progress, from the early days
of Trotter-Suzuki methods, to methods of qubitization
and randomized compiling [87], and Ô¨Ånally recently with
some methods for quantum variational methods for ap-
proximate time evolution [134].

The reason that quantum simulation has been such
a focus of the quantum computing community is be-
cause we have some indications to believe that quan-
tum computers can demonstrate a quantum advantage
when evolving quantum states through unitary time evo-
lution; the classical simulation overhead scales exponen-
tially with the depth of the time evolution.

As such, it is natural to consider if such a potential
quantum simulation advantage can be extended to the
realm of quantum machine learning as an inverse prob-
lem, that is, given access to some black-box dynamics,
can we learn a Hamiltonian such that time evolution un-
der this Hamiltonian replicates the unknown dynamics.
This is known as the problem of Hamiltonian learning,
or quantum dynamics learning, which has been studied
in the literature [66, 135]. Here, we use a Quantum Neu-

ral Network-based approach to learn the Hamiltonian of
a quantum dynamical process, given access to quantum
states at various time steps.

As was pointed out in the barren plateaus section V B,
attempting to do QML with no prior on the physics of
the system or no imposed structure of the ansatz hits the
quantum version of the no free lunch theorem; the net-
work has too high a capacity for the problem at hand and
is thus hard to train, as evidenced by its vanishing gra-
dients. Here, instead, we use a highly structured ansatz,
from work featured in [136]. First of all, given that we
know we are trying to replicate quantum dynamics, we
can structure our ansatz to be based on Trotter-Suzuki
evolution [86] of a learnable parameterized Hamiltonian.
This eÔ¨Äectively performs a form of parameter-tying in
our ansatz between several layers representing time evo-
lution. In a previous example on quantum convolutional
networks IV A 1, we performed parameter tying for spa-
tial translation invariance, whereas here, we will assume
the dynamics remain constant through time, and per-
form parameter tying across time, hence it is akin to a
quantum form of recurrent neural networks (RNN). More
precisely, as it is a parameterization of a Hamiltonian evo-
lution, it is akin to a quantum form of recently proposed
models in classical machine learning called Hamiltonian
neural networks [137].

Beyond the quantum RNN form, we can impose further
structure. We can consider a scenario where we know we
have a one-dimensional quantum many-body system. As
Hamiltonians of physical have local couplings, we can
use our prior assumptions of locality in the Hamiltonian
and encode this as a graph-based parameterization of the
Hamiltonian. As we will see below, by using a Quantum
Graph Recurrent Neural network [136] implemented in
TFQ, we will be able to learn the eÔ¨Äective Hamiltonian
topology and coupling strengths quite accurately, simply
from having access to quantum states at diÔ¨Äerent times
and employing mini-batched gradient-based training.

Before we proceed, it is worth mentioning that the ap-
proach featured in this section is quite diÔ¨Äerent from the
learning of quantum dynamics using a classical RNN fea-
ture in previous example section IV B. As sampling the
output of a quantum simulation at diÔ¨Äerent times can be-
come exponentially hard, we can imagine that for large
systems, the Quantum RNN dynamics learning approach
could have primacy over the classical RNN approach,
thus potentially demonstrating a quantum advantage of
QML over classical ML for this problem.

Target problems:

1. Preparing low-energy states of a quantum system
2. Learning Quantum Dynamics using a Quantum

Neural Network Model

Required TFQ functionalities:

36

2. Training multi-layered quantum neural networks

with shared parameters

3. Batching QNN training data (input-output pairs
and time steps) for supervised learning of quantum
unitary map

2.

Implementation

Please see the tutorial notebook for full code details:

research/qgrnn_ising/qgrnn_ising.ipynb

Here we provide an overview of our implementa-
tion. We can deÔ¨Åne a general Quantum Graph Neu-
ral Network as a repeating sequence of exponentials
of a Hamiltonian deÔ¨Åned on a graph, ÀÜUqgnn(Œ∑, Œ∏) =
q=1 e‚àíiŒ∑pq ÀÜHq(Œ∏)(cid:3) where the ÀÜHq(Œ∏) are generally
(cid:81)P
2-local Hamiltonians whose coupling topology is that of
an assumed graph structure.

(cid:2) (cid:81)Q

p=1

v

In our Hamiltonian learning problem, we aim to learn
a target ÀÜHtarget which will be an Ising model Hamiltonian
with Jjk as couplings and Bv for site bias term of each
spin, i.e., ÀÜHtarget = (cid:80)
ÀÜXv,
given access to pairs of states at diÔ¨Äerent times that were
subjected to the target time evolution operator ÀÜU (T ) =
e‚àíi ÀÜHtargetT .

j,k Jjk ÀÜZj ÀÜZk + (cid:80)

v Bv ÀÜZv + (cid:80)

{j,k}‚ààE Œ∏jk ÀÜZj ÀÜZk + (cid:80)

We will use a recurrent form of QGNN, using Hamil-
tonian generators ÀÜH1(Œ∏) = (cid:80)
v‚ààV Œ±v ÀÜXv and ÀÜH2(Œ∏) =
(cid:80)
v‚ààV œÜv ÀÜZv, with trainable param-
eters1 {Œ∏jk, œÜv, Œ±v}, for our choice of graph structure
prior G = {V, E}. The QGRNN is then resembles ap-
plying a Trotterized time evolution of a parameterized
Ising Hamiltonian ÀÜH(Œ∏) = ÀÜH1(Œ∏) + ÀÜH2(Œ∏) where P is the
number of Trotter steps. This is a good parameteriza-
tion to learn the eÔ¨Äective Hamiltonian of the black-box
dynamics as we know from quantum simulation theory
that Trotterized time evolution operators can closely ap-
proximate true dynamics in the limit of |Œ∑jk| ‚Üí 0 while
P ‚Üí ‚àû.

For our TFQ software implementation, we can initial-
ize Ising model & QGRNN model parameters as random
values on a graph. It is very easy to construct this kind of
graph structure Hamiltonian by using Python NetworkX
library.

N = 6
dt = 0.01
# Target Ising model parameters
G_ising = nx . cycle_graph ( N )
ising_w = [ dt * np . random . random () for _ in G .

edges ]

ising_b = [ dt * np . random . random () for _ in G .

nodes ]

Because the target Hamiltonian and its nearest-neighbor
graph structure is unknown to the QGRNN, we need to

1. Quantum compilation of exponentials of Hamilto-

nians

1 For simplicity we set Œ±v to constant 1‚Äôs in this example.

initialize a new random graph prior for our QGRNN. In
this example we will use a random 4-regular graph with
a cycle as our prior. Here, params is a list of trainable
parameters of the QGRNN.

# QGRNN model parameters
G_qgrnn = nx . r a n d o m _ r e g u l a r _ g r a p h ( n =N , d =4)
qgrnn_w = [ dt ] * len ( G_qgrnn . edges )
qgrnn_b = [ dt ] * len ( G_qgrnn . nodes )
theta = [ ‚Äô theta {} ‚Äô. format ( e ) for e in G . edges ]
phi = [ ‚Äô phi {} ‚Äô. format ( v ) for v in G . nodes ]
params = theta + phi

Now that we have the graph structure, weights of edges
& nodes, we can construct Cirq-based Hamiltonian oper-
ator which can be directly calculated in Cirq and TFQ.
To create a Hamiltonian by using cirq.PauliSum ‚Äôs or
cirq.PauliString ‚Äôs, we need to assign appropriate qubits
on them. Let‚Äôs assume Hamiltonian() is the Hamilto-
nian preparation function to generate cost Hamiltonian
from interaction weights and mixer Hamiltonian from
bias terms. We can bring qubits of Ising & QGRNN
models by using cirq.GridQubit .

qubits_ising = cirq . GridQubit . rect (1 , N )
qubits_qgrnn = cirq . GridQubit . rect (1 , N , 0 , N )
ising_cost , ising_mixer = Hamiltonian (

G_ising , ising_w , ising_b , qubits_ising )

qgrnn_cost , qgrnn_mixer = Hamiltonian (

G_qgrnn , qgrnn_w , qgrnn_b , qubits_qgrnn )

To train the QGRNN, we need to create an ensemble
of states which are to be subjected to the unknown dy-
namics. We chose to prepare a low-energy states by Ô¨Årst
performing a Variational Quantum Eigensolver (VQE)
[29] optimization to obtain an approximate ground state.
Following this, we can apply diÔ¨Äerent amounts of simu-
lated time evolutions onto to this state to obtain a varied
dataset. This emulates having a physical system in a low-
energy state and randomly picking the state at diÔ¨Äerent
times. First things Ô¨Årst, let us build a VQE model

def VQE ( H_target , q )

# Parameters
x = [ ‚Äôx {} ‚Äô. format ( i ) for i , _ in enumerate ( q ) ]
z = [ ‚Äôz {} ‚Äô. format ( i ) for i , _ in enumerate ( q ) ]
symbols = x + z
circuit = cirq . Circuit ()
circuit . append ( cirq . X ( q_ ) ** sympy . Symbol ( x_ )

for q_ , x_ in zip (q , x ) )

circuit . append ( cirq . Z ( q_ ) ** sympy . Symbol ( z_ )

for q_ , z_ in zip (q , z ) )

Now that we have a parameterized quantum circuit,
we can minimize the expectation value of given Hamil-
tonian. Again, we can construct a Keras model with
Expectation . Because the output expectation values are
calculated respectively, we need to sum them up at the
last.

circuit_input = tf . keras . Input (

shape =() , dtype = tf . string )
output = tfq . layers . Expectation () (

circuit_input ,
symbol_names = symbols ,
operators = tfq . co nv ert _to _te nso r (

37

[ H_target ]) )
output = tf . math . reduce_sum (

output , axis = -1 , keepdims = True )

Finally, we can get approximated lowest energy states
of the VQE model by compiling and training the above
Keras model.2

model = tf . keras . Model (

inputs = circuit_input , outputs = output )

adam = tf . keras . optimizers . Adam (

learning_rate =0.05)

low_bound = - np . sum ( np . abs ( ising_w + ising_b ) )

- N

inputs = tfq . con ver t_t o_ ten sor ([ circuit ])
outputs = tf . co nve rt_ to_ ten sor ([[ low_bound ]])
model . compile ( optimizer = adam , loss = ‚Äô mse ‚Äô)
model . fit ( x = inputs , y = outputs ,

batch_size =1 , epochs =100)

params = model . get_weights () [0]
res = { k : v for k , v in zip ( symbols , params ) }
return cirq . re s ol v e_ p ar a me t er s ( circuit , res )

Now that the VQE function is built, we can generate
the initial quantum data input with the low energy states
near to the ground state of the target Hamiltonian for
both our data and our input state to our QGRNN.

H_target = ising_cost + ising_mixer
low_energy_ising = VQE ( H_target , qubits_ising )
low_energy_qgrnn = VQE ( H_target , qubits_qgrnn )

The QGRNN is fed the same input data as the
true process. We will use gradient-based training over
minibatches of randomized timesteps chosen for our
QGRNN and the target quantum evolution. We will
thus need to aggregate the results among the diÔ¨Äerent
timestep evolutions to train the QGRNN model. To cre-
ate these time evolution exponentials, we can use the
tfq.util.exponential function to exponentiate our target
and QGRNN Hamiltonians3

exp_ising_cost = tfq . util . exponential (

operators = ising_cost )

exp_ising_mix = tfq . util . exponential (

operators = ising_mixer )

exp_qgrnn_cost = tfq . util . exponential (

operators = qgrnn_cost , coefficients = params )

exp_qgrnn_mix = tfq . util . exponential (

operators = qgrnn_mixer )

Here we randomly pick the 15 timesteps and apply the
Trotterized time evolution operators using our above con-
structed exponentials. We can have a quantum dataset
{(|œàTj (cid:105), |œÜTj (cid:105))|j = 1..M } where M is the number of

2 Here is some tip for training. Setting the output true value to
theoretical lower bound, we can minimize our expectation value
in the Keras model Ô¨Åt framework. That is, we can use the in-
equality (cid:104) ÀÜHtarget(cid:105) = (cid:80)
jk Jjk(cid:104)Zj Zk(cid:105) + (cid:80)
v(cid:104)Xv(cid:105) ‚â•
jk(‚àí)|Jjk| ‚àí (cid:80)
(cid:80)

v Bv(cid:104)Zv(cid:105) + (cid:80)

v |Bv| ‚àí N .

3 Here, we use the terminology cost and mixer Hamiltonians as the
Trotterization of an Ising model time evolution is very similar to
a QAOA, and thus we borrow nomenclature from this analogous
QNN.

38

data, or batch size (in our case we chose M = 15),
|œàTj (cid:105) = ÀÜU j

target|œà0(cid:105) and |œÜTj (cid:105) = ÀÜU j

qgrnn|œà0(cid:105).

L(Œ∏, œÜ) = 1 ‚àí 1
B
1 ‚àí 1
=
B

j=1 |(cid:104)œàTj |œÜTj (cid:105)|2
(cid:80)B
j=1(cid:104) ÀÜZtest(cid:105)j

(cid:80)B

def trotterize ( inp , depth , cost , mix ) :
add = tfq . layers . AddCircuit ()
outp = add ( cirq . Circuit () , append = inp )
for _ in range ( depth ) :

outp = add ( outp , append = cost )
outp = add ( outp , append = mix )

return outp

batch_size = 15
T = np . random . uniform (0 , T_max , batch_size )
depth = [ int ( t / dt ) +1 for t in T ]
true_states = []
pred_states = []
for P in depth :

true_states . append (

trotterize ( low_energy_ising , P ,

exp_ising_cost , exp_ising_mix ) )

pred_states . append (

trotterize ( low_energy_qgrnn , P ,

exp_qgrnn_cost , exp_qgrnn_mix ) )

Now we have both quantum data from (1) the true
time evolution of the target Ising model and (2) the pre-
dicted data state from the QGRNN. In order to maximize
overlap between these two wavefunctions, we can aim to
maximize the Ô¨Ådelity between the true state and the state
output by the QGRNN, averaged over random choices
of time evolution. To evaluate the Ô¨Ådelity between two
quantum states (say |A(cid:105) and |B(cid:105)) on a quantum com-
puter, a well-known approach is to perform the swap test
[138]. In the swap test, an additional observer qubit is
used, by putting this qubit in a superposition and using
it as control for a Fredkin gate (controlled-SWAP), fol-
lowed by a Hadamard on the observer qubit, the observer
qubit‚Äôs expectation value in the encodes the Ô¨Ådelity of the
two states, | (cid:104)A|B(cid:105) |2. Thus, right after Fidelity Swap
Test, we can measure the swap test qubit with Pauli ÀÜZ
operator with Expectation , (cid:104) ÀÜZtest(cid:105), and then we can cal-
culate the average of Ô¨Ådelity (inner product) between a
batch of two sets of quantum data states, which can be
used as our classical loss function in TensorFlow.

# Check class SwapTestFidelity in the notebook .
fidelity = S wapTestFidelity (

qubits_ising , qubits_qgrnn , batch_size )

state_true = tf . keras . Input ( shape =() ,

state_pred = tf . keras . Input ( shape =() ,

fid_output = fidelity ( state_true , state_pred )
fid_output = tfq . layers . Expectation () (

dtype = tf . string )

dtype = tf . string )

fid_output ,
symbol_names = symbols ,
operators = fidelity . op )

model = tf . keras . Model (

inputs =[ state_true , state_pred ] ,
outputs = fid_output )

Here, we introduce the average Ô¨Ådelity and implement

this with custom Keras loss function.

def average_fidelity ( y_true , y_pred ) :

return 1 - K . mean ( y_pred )

Again, we can use Keras model Ô¨Åt. To feed a batch of
quantum data, we can use tf.concat because the quan-
tum circuits are already in tf.Tensor . In this case, we
know that the lower bound of Ô¨Ådelity is 0, but the y_true
is not used in our custom loss function average_fidelity .
We set learning rate of Adam optimizer to 0.05.

y_true = tf . concat ( true_states , axis =0)
y_pred = tf . concat ( pred_states , axis =0)

model . compile (

loss = average_fidelity ,
optimizer = tf . keras . optimizers . Adam (

learning_rate =0.05) )

model . fit ( x =[ y_true , y_pred ] ,

y = tf . zeros ([ batch_size , 1]) ,
batch_size = batch_size ,
epochs =500)

The full results are displayed in the notebook, we
see for this example that our time-randomized gradient-
based optimization of our parameterized class of quan-
tum Hamiltonian evolution ends up learning the target
Hamiltonian and its couplings to a high degree of accu-
racy.

Figure 27. Left: True (target) Ising Hamiltonian with edges
representing couplings and nodes representing biases. Middle:
randomly chosen initial graph structure and parameter values
for the QGRNN. Right: learned Hamiltonian from the trained
QGRNN.

D. Generative Modelling of Quantum Mixed
States with Hybrid Quantum-Probabilistic Models

1. Background

Often in quantum mechanical systems, one encounters
so-called mixed states, which can be understood as proba-
bilistic mixtures over pure quantum states [139]. Typical
cases where such mixed states arise are when looking at
Ô¨Ånite-temperature quantum systems, open quantum sys-
tems, and subsystems of pure quantum mechanical sys-
tems. As the ability to model mixed states are thus key
to understanding quantum mechanical systems, in this
section, we focus on models to learn to represent and
mimic the statistics of quantum mixed states.

As mixed states are a combination of a classical prob-
ability distribution and quantum wavefunctions, their

statistics can exhibit both classical and quantum forms
of correlations (e.g., entanglement). As such, if we wish
to learn a representation of such mixed state which can
generatively model its statistics, one can expect that a
hybrid representation combining classical probabilistic
models and quantum neural networks can be an ideal.
Such a decomposition is ideal for near-term noisy de-
vices, as it reduces the overhead of representation on the
quantum device, leading to lower-depth quantum neu-
ral networks. Furthermore, the quantum layers provide
a valuable addition in representation power to the clas-
sical probabilistic model, as they allow the addition of
quantum correlations to the model.

Thus, in this section, we cover some examples where
one learns to generatively model mixed states using a
hybrid quantum-probabilistic model [49]. Such models
use a parameterized ansatz of the form

ÀÜœÅŒ∏œÜ = ÀÜU (œÜ)ÀÜœÅŒ∏ ÀÜU ‚Ä†(œÜ),

ÀÜœÅŒ∏ =

pŒ∏(x) |x(cid:105)(cid:104)x|

(34)

(cid:88)

x

where ÀÜU (œÜ) is a unitary quantum neural network with
parameters œÜ and pŒ∏(x) is a classical probabilistic model
with parameters Œ∏. We call ÀÜœÅŒ∏œÜ the visible state and
ÀÜœÅŒ∏ the latent state. Note the latent state is eÔ¨Äectively a
classical distribution over the standard basis states, and
its only parameters are those of the classical probabilistic
model.

As we shall see below, there are methods to train both
networks simultaneously. In terms of software implemen-
tation, as we have to combine probabilistic models and
quantum neural networks, we will use a combination of
TensorFlow Probability [140] along with TFQ. A Ô¨Årst
class of application we will consider is the task of gen-
erating a thermal state of a quantum system given its
Hamiltonian. A second set of applications is given sev-
eral copies of a mixed state, learn a generative model
which replicates the statistics of the state.

Target problems:

1. Incorporating probabilistic and quantum models
2. Variational Quantum Simulation of Quantum

Thermal States

3. Learning to generatively model mixed states from

data

Required TFQ functionalities:

1. Integration with TF Probability [140]
2. Sample-based simulation of quantum circuits
3. Parameter shift diÔ¨Äerentiator for gradient compu-

tation

2. Variational Quantum Thermalizer

39

Consider the task of preparing a thermal state: given
a Hamiltonian ÀÜH and a target inverse temperature Œ≤ =
1/T , we want to variationally approximate the state

ÀÜœÉŒ≤ = 1
ZŒ≤

e‚àíŒ≤ ÀÜH , ZŒ≤ = tr(e‚àíŒ≤ ÀÜH ),

(35)

using a state of the form presented in equation (34).
That is, we aim to Ô¨Ånd a value of the hybrid model
parameters {Œ∏‚àó, œÜ‚àó} such that ÀÜœÅŒ∏‚àóœÜ‚àó ‚âà ÀÜœÉŒ≤.
In order
to converge to this approximation via optimization of
the parameters, we need a loss function to optimize
which quantiÔ¨Åes statistical distance between these quan-
tum mixed states.
If we aim to minimize the discrep-
ancy between states in terms of quantum relative en-
tropy D(ÀÜœÅŒ∏œÜ(cid:107)ÀÜœÉŒ≤) = ‚àíS(ÀÜœÅŒ∏œÜ) ‚àí tr(ÀÜœÅŒ∏œÜ log ÀÜœÉŒ≤), (where
S(ÀÜœÅŒ∏œÜ) = ‚àítr(ÀÜœÅŒ∏œÜ log ÀÜœÅŒ∏œÜ) is the entropy), then, as de-
scribed in the full paper [59] we can equivalently minimize
the free energy4, and hence use it as our loss function:

(36)

Lfe(Œ∏, œÜ) = Œ≤tr(ÀÜœÅŒ∏œÜ ÀÜH) ‚àí S(ÀÜœÅŒ∏œÜ).
The Ô¨Årst term is simply the expectation value of the
energy of our model, while the second term is the en-
tropy. Due to the structure of our quantum-probabilistic
model, the entropy of the visible state is equal to the
entropy of the latent state, which is simply the clas-
sical entropy of the distribution, S(ÀÜœÅŒ∏œÜ) = S(ÀÜœÅŒ∏) =
‚àí (cid:80)
x pŒ∏(x) log pŒ∏(x). This comes in quite useful dur-
ing the optimization of our model.

Let us implement a simple example of the VQT model
which minimizes free energy to achieve an approximation
of the thermal state of a physical system. Let us consider
a two-dimensional Heisenberg spin chain

ÀÜHheis =

(cid:88)

(cid:104)ij(cid:105)h

Jh ÀÜSi ¬∑ ÀÜSj +

(cid:88)

(cid:104)ij(cid:105)v

Jv ÀÜSi ¬∑ ÀÜSj

(37)

where h (v) denote horizontal (vertical) bonds, while (cid:104)¬∑(cid:105)
represent nearest-neighbor pairings. First, we deÔ¨Åne this
Hamiltonian on a grid of qubits:

def get_bond ( q0 , q1 ) :

return cirq . PauliSum . f ro m _p au l i_ s tr i ng s ([

cirq . PauliString ( cirq . X ( q0 ) , cirq . X ( q1 ) ) ,
cirq . PauliString ( cirq . Y ( q0 ) , cirq . Y ( q1 ) ) ,
cirq . PauliString ( cirq . Z ( q0 ) , cirq . Z ( q1 ) ) ])

def g e t _ h e i s e n b e r g _ h a m i l t o n i a n ( qubits , jh , jv ) :

heisenberg = cirq . PauliSum ()
# Apply horizontal bonds
for r in qubits :

for q0 , q1 in zip (r , r [1::]) :

heisenberg += jh * get_bond ( q0 , q1 )

# Apply vertical bonds
for r0 , r1 in zip ( qubits , qubits [1::]) :

for q0 , q1 in zip ( r0 , r1 ) :

heisenberg += jv * get_bond ( q0 , q1 )

return heisenberg

Full notebook of the implementations below are avail-

able at:

research/vqt_qmhl/vqt_qmhl.ipynb

4 More precisely, the loss function here is in fact the inverse tem-
perature multiplied by the free energy, but this detail is of little
import to our optimization.

For our QNN, we consider a unitary consisting of general
single qubit rotations and powers of controlled-not gates.
Our code returns the associated symbols so that these
can be fed into the Expectation op:

def get_r ota tion_1q (q , a , b , c ) :

return cirq . Circuit (

cirq . X ( q ) ** a , cirq . Y ( q ) ** b , cirq . Z ( q ) ** c )

def get_r ota tion_2q ( q0 , q1 , a ) :

return cirq . Circuit (

cirq . CNotPowGate ( exponent = a ) ( q0 , q1 ) )

def get_layer_1q ( qubits , layer_num , L_name ) :

layer_symbols = []
circuit = cirq . Circuit ()
for n , q in enumerate ( qubits ) :

a , b , c = sympy . symbols (

" a {2} _ {0} _ {1} b {2} _ {0} _ {1} c {2} _ {0} _ {1} "

. format ( layer_num , n , L_name ) )
layer_symbols += [a , b , c ]
circuit += get_rotation_1q (q , a , b , c )

return circuit , layer_symbols

def get_layer_2q ( qubits , layer_num , L_name ) :

layer_symbols = []
circuit = cirq . Circuit ()
for n , ( q0 , q1 ) in enumerate ( zip ( qubits [::2] ,

qubits [1::2]) ) :
a = sympy . symbols ( " a {2} _ {0} _ {1} " . format (
layer_num , n , L_name ) )
layer_symbols += [ a ]
circuit += get_rotation_2q ( q0 , q1 , a )

return circuit , layer_symbols

It will be convenient to consider a particular class of
probabilistic models where the estimation of the gradi-
ent of the model parameters is straightforward to per-
form. This class of models are called exponential families
or energy-based models (EBMs).
If our parameterized
probabilistic model is an EBM, then it is of the form:

pŒ∏(x) = 1
ZŒ∏

e‚àíEŒ∏ (x), ZŒ∏ ‚â° (cid:80)

x‚àà‚Ñ¶ e‚àíEŒ∏ (x).

(38)

For gradients of the VQT free energy loss function
with respect to the QNN parameters, ‚àÇœÜLfe(Œ∏, œÜ) =
Œ≤‚àÇœÜtr(ÀÜœÅŒ∏œÜ ÀÜH), this is simply the gradient of an expec-
tation value, hence we can use TFQ parameter shift gra-
dients or any other method for estimating the gradients
of QNN‚Äôs outlined in previous sections.

As for gradients of the classical probabilistic model,
one can readily derive that they are given by the following
covariance:

‚àÇŒ∏Lfe = Ex‚àºpŒ∏ (x)

(cid:104)

(EŒ∏(x) ‚àí Œ≤HœÜ(x))‚àáŒ∏EŒ∏(x)

(cid:105)

‚àí(Ex‚àºpŒ∏ (x)

(cid:2)EŒ∏(x)‚àíŒ≤HœÜ(x)(cid:3))(Ey‚àºpŒ∏ (y)

(cid:2)‚àáŒ∏EŒ∏(y)(cid:3)),
(39)

where HœÜ(x) ‚â° (cid:104)x| ÀÜU ‚Ä†(œÜ) ÀÜH ÀÜU (œÜ) |x(cid:105) is the expectation
value of the Hamiltonian at the output of the QNN with
the standard basis element |x(cid:105) as input. Since the energy
function and its gradients can easily be evaluated as it is
a neural network, the above gradient is straightforward to

40

estimate via sampling of the classical probabilistic model
and the output of the QPU.

j Œ∏xj

For our classical latent probability distribution pŒ∏(x),
as a Ô¨Årst simple case, we can use the product of inde-
pendent Bernoulli distributions pŒ∏(x) = (cid:81)
j pŒ∏j (xj) =
(cid:81)
j (1 ‚àí Œ∏j)1‚àíxj , where xj ‚àà {0, 1} are binary values.
We can re-phrase this distribution as an energy based
model to take advantage of equation (V D 2). We move
the parameters into an exponential, so that the probabil-
ity of a bitstring becomes pŒ∏(x) = (cid:81)
j eŒ∏j xj /(eŒ∏j + e‚àíŒ∏j ).
Since this distribution is a product of independent vari-
ables, it is easy to sample from. We can use the Tensor-
Flow Probability library [140] to produce samples from
this distribution, using the tfp.distributions.Bernoulli
object:

def b e r n o u l l i _ b i t _ p r o b a b i l i t y ( b ) :

return np . exp ( b ) /( np . exp ( b ) + np . exp ( - b ) )

def sample_bernoulli ( num_samples , biases ) :

prob_list = []
for bias in biases . numpy () :

prob_list . append (

b e r n o u l l i _ b i t _ p r o b a b i l i t y ( bias ) )

latent_dist = tfp . distributions . Bernoulli (

probs = prob_list , dtype = tf . float32 )
return latent_dist . sample ( num_samples )

After getting samples from our classical probabilistic
model, we take gradients of our QNN parameters. Be-
cause TFQ implements gradients for its expectation ops,
we can use tf.GradientTape to obtain these derivatives.
Note that below we used tf.tile to give our Hamiltonian
operator and visible state circuit the correct dimensions:

bitstring_tensor = sample_bernoulli (

num_samples , vqt_biases )
with tf . GradientTape () as tape :

t i l e d _ v q t _ m o d e l _ p a r a m s = tf . tile (

[ vqt_model_params ] , [ num_samples , 1])

s a m p l e d _ e x p e c t a t i o n s = expectation (

tiled_visible_state ,
vqt_symbol_names ,
tf . concat ([ bitstring_tensor ,

t i l e d _ v q t _ m o d e l _ p a r a m s ] , 1) ,

tiled_H )

energy_losses = beta * s a m p l e d _ e x p e c t a t i o n s
en erg y_l oss es_ av g = tf . reduce_mean (

energy_losses )

v q t _ m o d e l_ g r a d i e n t s = tape . gradient (

energy_losses_avg , [ vqt_model_params ])

Putting these pieces together, we train our model to out-
put thermal states of the 2D Heisenberg model on a 2x2
grid. The result after 100 epochs is shown in Fig. 28.

A great advantage of this approach to optimization of
the probabilistic model is that the partition function ZŒ∏
does not need to be estimated. As such, more general
more expressive models beyond factorized distributions
can be used for the probabilistic modelling of the la-
tent classical distribution. In the advanced section of the
notebook, we show how to use a Boltzmann machine as
our energy based model. Boltzmann machines are EBM‚Äôs
where for bitstring x ‚àà {0, 1}n, the energy is deÔ¨Åned as
E(x) = ‚àí (cid:80)

i,j wijxixj ‚àí (cid:80)

i bixi.

It is worthy to note that our factorized Bernoulli distri-
bution is in fact a special case of the Boltzmann machine,
one where only the so-called bias terms in the energy
function are present: E(x) = ‚àí (cid:80)
i bixi. In the notebook,
we start with this simpler Bernoulli example of the VQT,
the resulting density matrix converges to the known ex-
act result for this system, as shown in Fig. 28. We also
provide a more advanced example with a general Boltz-
mann machine. In the latter example, we picked a fully
visible, fully-connected classical Ising model energy func-
tion, and used MCMC with Metropolis-Hastings [141] to
sample from the energy function.

Figure 28. Final density matrix output by the VQT algo-
rithm run with a factorized Bernoulli distribution as classical
latent distribution, trained via a gradient-based optimizer.
See notebook for details.

3. Quantum Generative Learning from Quantum Data

Now that we have seen how to prepare thermal states
from a given Hamiltonian, we can consider how we can
learn to generatively model mixed quantum states using
quantum-probabilistic models in the case where we are
given several copies of a mixed state rather than a Hamil-
tonian. That is, we are given access to a data mixed
state ÀÜœÉD, and we would like to Ô¨Ånd optimal parameters
{Œ∏‚àó, œÜ‚àó} such that ÀÜœÅŒ∏‚àóœÜ‚àó ‚âà ÀÜœÉD, where the model state is
of the form described in (34). Furthermore, for reasons
of convenience which will be apparent below, it is useful
to posit that our classical probabilistic model is of the
form of an energy-based model as in equation (38).

If we aim to minimize the quantum relative entropy
between the data and our model (in reverse compared to
the VQT) i.e., D(ÀÜœÉD(cid:107)ÀÜœÅŒ∏œÜ) then it suÔ¨Éces to minimize
the quantum cross entropy as our loss function

Lxe(Œ∏, œÜ) ‚â° ‚àítr(ÀÜœÉD log ÀÜœÅŒ∏œÜ).
By using the energy-based form of our latent classical
probability distribution, as can be readily derived (see
[59]), the cross entropy is given by

Lxe(Œ∏, œÜ) = Ex‚àºœÉœÜ(x)[EŒ∏(x)] + log ZŒ∏,

41

where œÉœÜ(x) ‚â° (cid:104)x| ÀÜU ‚Ä†(œÜ)ÀÜœÉD ÀÜU (œÜ) |x(cid:105) is the distribution
obtained by feeding the data state ÀÜœÉD through the inverse
QNN circuit ÀÜU ‚Ä†(œÜ) and measuring in the standard basis.
As this is simply an expectation value of a state prop-
agated through a QNN, for gradients of the loss with re-
spect to QNN parameters we can use standard TFQ dif-
ferentiators, such as the parameter shift rule presented in
section III. As for the gradient with respect to the EBM
parameters, it is given by

‚àÇŒ∏Lxe(Œ∏, œÜ) = Ex‚àºœÉœÜ(x)[‚àáŒ∏EŒ∏(x)]‚àíEy‚àºpŒ∏(y)[‚àáŒ∏EŒ∏(y)].

Let us implement a scenario where we were given the
output density matrix from our last VQT example as
data, let us see if we can learn to replicate its statistics
from data rather than from the Hamiltonian. For sim-
plicity we focus on the Bernoulli EBM deÔ¨Åned above. We
can eÔ¨Éciently sample bitstrings from our learned classical
distribution and feed them through the learned VQT uni-
tary to produce our data state. These VQT parameters
are assumed Ô¨Åxed; they represent a quantum datasource
for QMHL.

We use the same ansatz for our QMHL unitary as we
did for VQT, layers of single qubit rotations and expo-
nentiated CNOTs. We apply our QMHL model unitary
to the output of our VQT to produce the pulled-back
data distribution. Then, we take expectation values of
our current best estimate of the modular Hamiltonian:

def g e t _ q m h l _ w e i g h t s _ g r a d _ a n d _ b i a s e s _ g r a d (

ebm_deriv_expectations , bitstring_list ,
biases ) :

b a r e _ q m h l _ b i a s e s _ g r a d = tf . reduce_mean (

ebm_deriv_expectations , 0)

c _q m hl _ bi a se s _g r ad = e b m _ b i a s e s _ d e r i v a t i v e _ a v g

( bitstring_list )

return tf . subtract ( bare_qmhl_biases_grad ,

c _q m hl _ bi a se s _g r ad )

Note that we use the tf.GradientTape functionality to
obtain the gradients of the QMHL model unitary. This
functionality is enabled by our TFQ diÔ¨Äerentiators mod-
ule.

The classical model parameters can be updated
according to the gradient formula above. See the VQT
notebook for the results of this training.

E. Subspace-Search Variational Quantum
Eigensolver for Excited States: Integration with
OpenFermion

1. Background

The Variational Quantum Eigensolver (VQE) [25] is a
heuristic algorithm for preparing the ground state of a
many-body quantum system. It is often used for mod-
elling strongly correlated systems that appear challeng-
ing to simulate classically but contain enough structure

that ground state preparation is believed to be tractable
quantum mechanically (at least for some interesting in-
stances); e.g., it is often studied in the context of the
molecular electronic structure Hamiltonian of interest
in quantum chemistry. The basic idea of VQE is that
one can use a quantum computer with limited resources
to prepare a highly entangled parameterized state that
can be optimized via classical feedback to provide a
classically inaccessible description of the ground states
of these systems. SpeciÔ¨Åcally, given the time indepen-
dent Schr√∂dinger equation ÀÜH|Œ®(cid:105) = E|Œ®(cid:105), with Hamil-
tonian ÀÜH = (cid:80)N
i=0 Œªi|œài(cid:105)(cid:104)œài|, where Œªi are the eigen-
values of increasing size and œài the eigenvectors in the
spectral decomposition, VQE provides an approxima-
tion of Œª0. Measurements of the Hamiltonian yield real
eigenvalues (due to the Hermitian nature of quantum
operators), therefore Œª0 ‚â§ (cid:104)Œ®| ÀÜH|Œ®(cid:105). Given optimiza-
tion parameters Œ∏ and Œ® = U (Œ∏)|0‚äóN (cid:105), the classical op-
timization problem associated with VQE is deÔ¨Åned as
minŒ∏ (cid:104)0|U ‚Ä†(Œ∏) ÀÜHU (Œ∏)|0(cid:105). This formulation is limited to
Ô¨Ånding the ground state energy, but higher energy states
are often chemically and physically important [142]. In
order to expand the VQE algorithm to higher energy
states, exploitation of the orthogonality of diÔ¨Äerent quan-
tum energy levels has been proposed [143‚Äì145].

Here, we focus on describing how TFQ can be used
to investigate an example of a state-of-the-art varia-
tional algorithm known as the Subspace-Search Varia-
tional Quantum Eigensolver (SSVQE) for excited states
[145]. The SSVQE modiÔ¨Åes the VQE optimization prob-
lem to be:

min
Œ∏

k
(cid:88)

i=0

wi(cid:104)Œ®i|U ‚Ä†(Œ∏) ÀÜHU (Œ∏)|Œ®i(cid:105)

s.t. wi+1 ‚â§ wi, (cid:104)Œ®i|Œ®j(cid:105) = Œ¥ij

The goal is to minimize the expectation value of the
Hamiltonian, with the initial state of each energy level‚Äôs
evaluation being orthogonal.
In SSVQE, as in VQE,
there are a number of choices for anstaz, i.e. the circuit
structure of U (Œ∏), and optimization method [146‚Äì150].

TensorÔ¨Çow-Quantum has several features that make it
appealing for work with VQE algorithms. The adjoint
diÔ¨Äerentiator enables rapid code execution, which is es-
pecially important given the number of Hamiltonians en-
countered in quantum chemistry experiments. The abil-
ity to work with any optimization target via custom lay-
ers and custom objectives enables straightforward imple-
mentation of and experimentation with any VQE based
algorithm. Additionally, OpenFermion [151] has native
methods for working with Cirq, enabling the quantum
chemistry methods OpenFermion provides to be easily
combined with the quantum machine learning methods
TensorÔ¨Çow-Quantum provides.
Target problems:

1. Implement SSVQE [145] in TensorÔ¨Çow-Quantum
2. Find the ground state and Ô¨Årst excited state ener-

gies for H2 at increasing bond lengths

42

Required TFQ functionalities:

1. Integration with OpenFermion [151]
2. Custom TensorÔ¨Çow-Quantum Layers

2.

Implementation

The full notebook and implementation is available at:

research/ssvqe
First, we need to create the VQE anstaz. Here, we con-
struct one similar to the Hardware EÔ¨Écient Ansatz [146].
We create layers of parameterized Ry and Rz gates, en-
tangled with CNOTs.

def layer ( circuit , qubits , parameters ) :

for i in range ( len ( qubits ) ) :

circuit += cirq . ry ( parameters [3* i ]) . on (

qubits [ i ])

circuit += cirq . rz ( parameters [3* i +1]) . on

( qubits [ i ])

circuit += cirq . ry ( parameters [3* i +2]) . on

( qubits [ i ])
for i in range ( len ( qubits ) -1) :

circuit += cirq . CNOT ( qubits [ i ] , qubits [ i

+1])
circuit += cirq . CNOT ( qubits [ -1] , qubits [0])
return circuit

def ansatz ( circuit , qubits , layers , parameters ) :

for i in range ( layers ) :

params = parameters [3* i * len ( qubits ) :3*( i

+1) * len ( qubits ) ]

circuit = layer ( circuit , qubits , params )

return circuit

Now we create the readout operators for each qubit,
which are equivalent to the associated terms in the
Hamiltonian. This will yield the expectation value of
the Hamiltonian.

def exp_val ( qubits , hamiltonian ) :

return prod ([ op ( qubits [ i ]) for i , op in
enumerate ( hamiltonian ) if hamiltonian [ i ] !=
0])

Since the Hamiltonian can be expressed as a sum of

simple operators, i.e.,

ÀÜH =

L
(cid:88)

(cid:96)=1

a(cid:96)P(cid:96)

for real scalars a(cid:96) and Pauli strings P(cid:96). The Hamiltonian
is sparse in this representation; typically L = O(N 4)
in the number of spin-orbitals (or equivalently, qubits)
N . We create custom VQE layers with diÔ¨Äerent readout
operators but shared parameters. The SSVQE is then
implemented as a collection of these VQE layers, with
each input being orthogonal. This is implemented by
applying a Pauli X gate prior to creating U (Œ∏).

class VQE ( tf . keras . layers . Layer ) :

def __init__ ( self , circuits , ops ) :
super ( VQE , self ) . __init__ ()

43

already been generated. These Ô¨Åles were generated using
OpenFermion [151] and PySCF [152]. Using the Open-
FermionPySCF function generate_molecular_hamiltonian
to generate the Hamiltonians for each bond length,
we then converted this to a form compatible with
quantum circuits using the Jordan-Wigner Transform,
which maps fermionic annihilation operators to qubits
via:
(cid:55)‚Üí
2 (Xp ‚àí iYp) Z1 ¬∑ ¬∑ ¬∑ Zp‚àí1. This yields a Hamiltonian of
1
ÀÜH = g01 + g1X0X1Y2Y3 + g2X0Y1Y2X3 +
the form:
g3Y0X1X2Y3 + g4Y0Y1X2X3 + g5Z0 + g6Z0Z1 + g7Z0Z2 +
g8Z0Z3 + g9Z1 + g10Z1Z2 + g11Z1Z3 + g12Z2 + g13Z2Z3 +
g13Z3, where gn is determined by the bond length.
It
is this PauliSum object which is then saved and loaded.
With the Hamiltonians created, we iterate over the bond
lengths and compute the predicted energies of the states.

2 (Xp + iYp) Z1 ¬∑ ¬∑ ¬∑ Zp‚àí1 and a‚Ä†

(cid:55)‚Üí 1

ap

p

d i a t o m i c _ b o n d _ l e n g t h = 0.2
interval = 0.1
max_bond_length = 2.0
k = 2

# VQE Hyperparameters
layers = 4
n_qubits = 4
optimizer = tf . keras . optimizers . Adam ( lr =0.1)

step = 0
while d i a t o m i c _ b o n d _ l e n g t h <= max_bond_le ngth :

eigs = real [ step ]
# Load the Data
ham_name = " mo l_ ham ilt oni ans _ " + str ( step )
coef_name = " c o ef _ ha m il t on i an s _ " + str ( step )
with open ( ham_name , " rb " ) as ham_file :
hamiltonians = load ( ham_file )

with open ( coef_name , " rb " ) as coeff_file :

coefficients = load ( coeff_file )
# Create the SSVQE and Approximate the
Energies
ssvqe = make_ssvqe ( n_qubits , layers ,
coefficients , hamiltonians , k )
ground , excited = train_ssvqe ( ssvqe ,
optimizer )
d i a t o m i c _ b o n d _ l e n g t h += interval
step += 1

self . layers = [ tfq . layers . ControlledPQC (

circuits [ i ] , ops [ i ] , differentiator = tfq .
dif fe r en tiators . Adjoint () ) for i in range (
len ( circuits ) ) ]

def call ( self , inputs ) :

return sum ([ self . layers [ i ]([ inputs [0] ,

inputs [1]]) for i in range ( len ( self . layers ) )
])

class SSVQE ( tf . keras . layers . Layer ) :

def __init__ ( self , num_weights , circuits ,
ops , k , const ) :

super ( SSVQE , self ) . __init__ ()
self . theta = tf . Variable ( np . random .

uniform (0 , 2 * np . pi , (1 , num_weights ) ) ,
dtype = tf . float32 )

self . hamiltonians = []
self . k = k
self . const = const
for i in range ( k ) :

self . hamiltonians . append ( VQE (

circuits [ i ] , ops [ i ]) )

def call ( self , inputs ) :

total = 0
energies = []
for i in range ( self . k ) :

c = self . hamiltonians [ i ]([ inputs ,

self . theta ]) + self . const

energies . append ( c )
if i == 0:

total += c

else :

total += ((0.9 - i * 0.1) * c )

return total , energies

Next we need to set up the training function for the
SSVQE. The optimization loop directly minimizes the
output, as we have already encoded the constraints.

def train_ssvqe ( ssvqe , opt , tol =5 e -6 , patience

=10) :
ssvqe_model = ssvqe [0]
energies = ssvqe [1]
prev_loss = 100
counter = 0
inputs = tfq . con ver t_t o_t en sor ([ cirq . Circuit
() ])
while True :

with tf . GradientTape () as tape :

loss = ssvqe_model ( inputs )

grads = tape . gradient ( loss , ssvqe_model .

t r a i n a b l e _ v a r i a b l e s )

opt . apply_gradients ( zip ( grads ,

ssvqe_model . t ra i n a b l e _ v a r i a b l es ) )
loss = loss . numpy () [0][0]
if abs ( loss - prev_loss ) < tol :

counter += 1

if counter > patience :

break

prev_loss = loss

energies . theta = ssvqe_model .
t r a i n a b l e _ v a r i a b l e s [0]
energies = [ i . numpy () [0][0] for i in
energies ( inputs ) [1]]
return energies [0] , energies [1]

With the SSVQE set up, we now need to generate the
In the code below, we load the data that has

inputs.

This implementation takes several minutes to run, even
with the Adjoint diÔ¨Äerentiator. This is because there are
4 layers * 3 parameters per qubit * 4 qubits = 48 pa-
rameters per layer and because we must optimize these
parameters for 20 diÔ¨Äerent Hamiltonians. We can then
plot and compare the actual energies with the VQE pre-
dicted energies, as done in Figure 29.

44

The quantum classiÔ¨Åer we consider here is a made up
of three parts: a circuit that prepares a set of states of
interest, a variational quantum circuit consisting of mul-
tiple trainable layers and a collection of measurements
that can be combined to create a predictor for the clas-
siÔ¨Åcation. In Ô¨Åg. 30 we give a schematic depiction of the
full model.

TensorÔ¨Çow Quantum has a data set module that con-
tains quantum circuits for diÔ¨Äerent quantum many-body
ground states. This makes it easy to set up a workÔ¨Çow
where we want to load a set of quantum states and per-
form a quantum machine learning task on these data.

As an example, we study the two-dimensional trans-

verse Ô¨Åeld Ising-model (TFIM) on a torus,

HTFIM = ‚àí

i œÉz
œÉz

j ‚àí g

(cid:88)

(cid:104)i,j(cid:105)

N
(cid:88)

i

œÉx
i = Hzz + gHx,

where (cid:104)i, j(cid:105) indicates the set of nearest neighbor indices
for each point in the lattice. On the torus, this system
has a phase transition at g ‚âà 3.04 from an ordered to a
disordered phase [162, 163].

After the state preparation circuit, we use the hard-
ware eÔ¨Écient ansatz to train the quantum classiÔ¨Åer. This
ansatz consists of two layers of parameterized single qubit
gates and a single layer of two-qubit parameterized gates
[146]. Note that the parameters in the state preparation
circuit stay Ô¨Åxed during training. Finally, we measure
the observables (cid:104)Zi(cid:105) on each qubit and apply a rescaled
sigmoid to the linear predictor of the outcomes,

¬Øy = tanh

(cid:88)

(cid:104)Zi(cid:105)Wi,

i

where Wi ‚àà Rn is a weight vector. Hence, given an input
ground state |œà0(Œ∏‚àó
k)(cid:105), our classiÔ¨Åer will output a single
scalar ¬Øy ‚àà (‚àí1, 1).

We use the Hinge loss function to train the model to
output the correct labels. This loss function is given by

(cid:96) = max(0, t ‚àí y ¬∑ ¬Øy)

(40)

where, y ‚àà {‚àí1, 1} are the ground truth labels and
¬Øy ‚àà (‚àí1, 1) the model predictions. The entire end-to-
end model contains several non-trivial components, such
as loading quantum states as data or backpropagating
classical gradients through quantum gates.

Target problems:

1. Training a quantum circuit classiÔ¨Åer to detect a
phase transition in a condensed matter physics sys-
tem

Required TFQ functionalities:

1. Hybrid quantum-classical optimization of a varia-

tional quantum circuit and logistic function.

2. Batching training over quantum data (multiple

ground states and labels)

Figure 29. Comparison of SSVQE predicted energy for the
ground state and Ô¨Årst excited state compared with the true
values

F. ClassiÔ¨Åcation of Quantum Many-Body Phase
Transitions

To run this example see the Colab notebook at:

research/phase_classifier/phase_classification.ipynb

1. Background

In quantum many-body systems we are interested in
studying diÔ¨Äerent phases of matter and the transitions
between these diÔ¨Äerent phases. This requires a detailed
characterization of the observables that change as a func-
tion of the order values of the system. To estimate
these observables, we have to rely on quantum Monte
Carlo techniques or tensor network approaches, that each
come with their respective advantages and drawbacks
[153, 154].

Recently, work on classifying phases with supervised
machine learning has been proposed as a method for un-
derstanding phase transitions [155, 156]. For such a su-
pervised approach, we label the diÔ¨Äerent phases of the
system and use classical data obtained from the system
of interest and train a machine learning model to predict
the labels. A natural extension of these ideas is a quan-
tum machine learning model for phase classiÔ¨Åcation [157],
where the information that characterizes the state is en-
coded in a variational quantum circuit. This would re-
quire preparing high Ô¨Ådelity quantum many-body states
on NISQ devices, an area of active research in the Ô¨Åeld
In this section, we
of quantum computing [158‚Äì161].
show how a quantum classiÔ¨Åer can be trained end-to-end
in TensorÔ¨Çow quantum by using a data set of quantum
many-body states.

45

def c r e a t e _ q u a n t u m _ m o d e l (N , num_layers ) :

qubits = cirq . GridQubit . rect (N , 1)
circuit = cirq . Circuit ()
for l in range ( num_layers ) :

add_layer_single ( circuit , qubits , cirq .X

, f " x_ { l } " )

a d d _ l a y e r _ n e a r e s t _ n e i g h b o u r s ( circuit ,

qubits , cirq . XX , f " xx_ { l } " )

add_layer_single ( circuit , qubits , cirq .Z

, f " z_ { l } " )

readout = [ cirq . Z ( q ) for q in qubits ]
return circuit , readout

We can seamlessly integrate our quantum classiÔ¨Åer into
a Keras model.

model = tf . keras . Sequential ([

tf . keras . layers . Input ( shape =() , dtype = tf .
string ) ,
tfq . layers . PQC ( circuit , output ) ,
tf . keras . layers . Dense (1 , activation = tf . keras
. activations . tanh )

])

Additionally, we can compile the model with other met-
rics to track during training.
In our case, we use the
hinge accuracy to count the number of missclasiÔ¨Åed data
points.

def hinge_accuracy ( y_true , y_pred ) :

y_true = tf . squeeze ( y_true ) > 0.0
y_pred = tf . squeeze ( y_pred ) > 0.0
result = tf . cast ( y_true == y_pred , tf .
float32 )

return tf . reduce_mean ( result )

model . compile (

loss = tf . keras . losses . Hinge () ,
optimizer = tf . keras . optimizers . Adam (
learning_rate =0.01) ,
metrics =[ hinge_accuracy ])

If we set the maximum number of epochs and batch size,
we are ready to Ô¨Åt the model. We add an early stopping
callback to make sure that the training can terminate
when the loss no longer decreases consistently.

epochs = 25
batch_size = 32

qnn_history = model . fit (

x_train_tfcirc , labels ,
batch_size = BATCH_SIZE ,
epochs = EPOCHS ,
verbose =1 ,
callbacks =[ tf . keras . callbacks . EarlyStopping (
‚Äô loss ‚Äô , patience =5) ])

such that |œà0(Œ∏‚àó

Figure 30. The Ô¨Årst part of the circuit prepares the ground
state of a Hamiltonian H(Œª) with a Ô¨Åxed ansatz |œà0(Œ∏)(cid:105). For
each order value Œªk there is a corresponding set of Ô¨Åxed pa-
rameters Œ∏‚àó
k)(cid:105) is the ground state of H(Œªk).
k
The second part of the circuit is the quantum classiÔ¨Åer, a
hardware eÔ¨Écient ansatz with trainable parameters for each
gate (Œ±i, Œ≤i, Œ≥i), where i = 1, . . . , d indicates the layer. Fi-
nally, we obtain the expectation value (cid:104)Z(cid:105)i on all qubits and
combine these into a the output label ¬Øy after a rescaled sig-
moid activation function.

2.

Implementation

For the two-dimensional TFIM on a rectangular lat-
tice, the available data sets are a 3 √ó 3, 4 √ó 3, and
4 √ó 4 lattice for g ‚àà [2.5, 3.5]. To load the data, we use
the tfi_rectangular function to obtain both the circuits
and labels indicating the ordered (y = 1) and disordered
(y = ‚àí1) phase. Here, we consider the 4 √ó 4 lattice.

nspins = 16
qubits = cirq . GridQubit . rect ( nspins , 1)

circuits , labels , _ , _ = tfq . datasets .

tfi _r e ct angular ( qubits )

labels = np . array ( labels )
labels [ labels >= 1] = 1.0
labels = labels * 2 - 1
x_t rain_t fcirc = tfq . con ver t_t o_t ens or ( circuits )

circuits now contains 51 quantum circuits that prepare
> 0.999 Ô¨Ådelity ground states of the two-dimensional
TFIM at the respective order values g. Next, we add
the layers of our hardware eÔ¨Écient ansatz. We parame-
terize each gate in the classiÔ¨Åer individually.

def a d d _ l a y e r _ n e a r e s t _ n e i g h b o u r s ( circuit , qubits

, gate , prefix ) :
for i , q in enumerate ( zip ( qubits , qubits
[1:]) ) :

symbol = sympy . Symbol ( prefix + ‚Äô - ‚Äô + str

( i ) )

circuit . append ( gate (* q ) ** symbol )

predictions = model . predict ( x_train_tfcirc )

def ad d _la y er_ single ( circuit , qubits , gate ,

prefix ) :
for i , q in enumerate ( qubits ) :

symbol = sympy . Symbol ( prefix + ‚Äô - ‚Äô + str

( i ) )

circuit . append ( gate ( q ) ** symbol )

After the model is trained, we can predict the labels of
the phases from the model output ¬Øy and visualize how
well the model has captured the phase transition.
In
Ô¨Åg. 31, we see that the inÔ¨Çection point at ¬Øy ‚âà 0 coin-
cides with the phase transition at g ‚âà 3.04, as expected.
Although we have not explored this here, we could train
the classiÔ¨Åer on a subset of the data, for instance around

Ground state preparationClassifier...Predictionthe critical point, and then see how well the classiÔ¨Åer
generalizes to states outside of the seen data.

Figure 31. Quantum classiÔ¨Åer output versus order value g
of the two-dimensional TFIM. The dashed line indicates the
critical point g ‚âà 3.04.

G. Quantum Generative Adversarial Networks

1. Background

Generative adversarial networks (GANs) [164] have
met widespread success in classical machine learning.
While classical data can be seen as a special case of data
corresponding to a probabilistic mixture, we consider the
task of adversarially learning most general form of data:
quantum states. A generator seeks to create a circuit
that reproduces a given quantum state, while a discrim-
inator circuit is presented either with the true data or
In this section, we
with fake data from the generator.
present viable applications of a new quantum GAN archi-
tecture for both purely quantum datasets and quantum
states built from classical datasets.

Recent work on a quantum GAN (QuGAN) [54, 165]
has proposed a direct analogy of the classical GAN ar-
chitecture in designing the generator and discriminator
circuits. However, the QuGAN does not always converge
but rather in certain cases oscillates between a Ô¨Ånite set of
states due to mode collapse, and in general suÔ¨Äers from a
non-unique Nash equilibrium [76]. This motivates a new
entangling quantum GAN (EQ-GAN) with a uniquely
quantum twist: rather than providing the discriminator
with either true or fake data, we allow the discriminator
to entangle both true and fake data. The convergence of
the EQ-GAN to the global optimal Nash equilibrium is
theoretically veriÔ¨Åed and numerical experiments conÔ¨Årm

46

that the EQ-GAN converges on problem instances that
the QuGAN failed on [76].

A pervasive issue in near-term quantum computing is
noise: when performing an entangling operation, the re-
quired two-qubit gate introduces phase errors that are
diÔ¨Écult to fully calibrate against due to a time-dependent
noise model. However, such operations are required to
measure the overlap between two quantum states, in-
cluding the assessment of how close the fake data is to
the true data. This issue provides further motivation for
the use of adversarial generative learning. Without ad-
versarial learning, one may freeze the discriminator to
perform an exact Ô¨Ådelity measurement between the true
and fake data. While this would replicate the original
state in the absence of noise, gate errors in the imple-
mentation of the discriminator will cause convergence to
the incorrect optimum. As seen in the Ô¨Årst example be-
low, the adversarial approach of the EQ-GAN is more
robust to such errors than the simpler supervised learn-
ing approach. Since training quantum machine learning
models can require extensive time to compute gradients
on current quantum hardware, resilience to gate errors
drifting during the training process is especially valuable
in the noisy intermediate-scale quantum era of quantum
computing.

Most proposals for quantum machine learning algo-
rithms on classical data require a quantum random
access memory (QRAM) [166]. A QRAM typically
stores the classical dataset in a superposition of quan-
tum states, allowing a quantum device to eÔ¨Éciently ac-
cess the dataset. However, a QRAM is diÔ¨Écult to achieve
experimentally, posing a further roadblock for near-term
quantum machine learning applications. We provide an
example application of the EQ-GAN to create an approx-
imate QRAM by learning a shallow quantum circuit that
generates a superposition of classical data. In particular,
the QRAM is applied to quantum neural networks [24],
improving the performance of a quantum neural network
for a classiÔ¨Åcation task.

Target problems:

1. Suppress noise in a generative learning task for

quantum states

2. Prepare an approximate quantum memory for

faster training of a quantum neural network

Required TFQ functionalities:

1. Use of a quantum hardware backend
2. Shared variables between quantum neural network

layers

3. Training on a noisy quantum circuit

2. Noise Suppression with Adversarial Learning

To run this example, see the Colab notebook at:

research/eq_gan/noise_suppression.ipynb

2.62.83.03.23.4g0.750.500.250.000.250.500.75Output LayerThe entangling quantum generative adversarial net-

work (EQ-GAN) uses a minimax cost function

min
Œ∏g

max
Œ∏d

V (Œ∏g, Œ∏d) = min
Œ∏g

max
Œ∏d

[1 ‚àí DœÉ(Œ∏d, œÅ(Œ∏g))]

(41)

to learn the true data density matrix œÉ. The generator
produces a fake quantum state œÅ(Œ∏g), while the discrimi-
nator performs a parameterized swap test (Ô¨Ådelity mea-
surement) DœÉ(Œ∏d, œÅ(Œ∏g)) between the true data œÉ and the
fake data œÅ. The swap test is parameterized such that
there exist parameters Œ∏opt
that realize a perfect swap
test, i.e. DœÉ(Œ∏opt
œÉ (œÅ(Œ∏g)) where
d , œÅ(Œ∏g)) = 1
2 + 1

2 DÔ¨Åd

d

47

As an example, we consider the task of learning the su-
perposition 1‚àö
(|0(cid:105) + |1(cid:105)) on a quantum device with noise
2
(Fig. 32). The discriminator is deÔ¨Åned by a swap test
with CZ gate providing the necessary two-qubit opera-
tion. To learn to correct gate errors, however, the dis-
criminator adversarially learns the angles of single-qubit
Z rotations insert directly after the CZ gate. Hence, the
EQ-GAN obtains a state overlap signiÔ¨Åcantly better than
that of the perfect swap test.

DÔ¨Åd

œÉ (œÅ(Œ∏g)) =

(cid:18)

Tr

(cid:113)

œÉ1/2 œÅ(Œ∏g) œÉ1/2

(cid:19)2

.

(42)

d

In the presence of noise, it is generally diÔ¨Écult to de-
termine a circuit ansatz for DœÉ(Œ∏d, œÅ) such that param-
eters Œ∏opt
exist. When implementing a CZ gate, gate
parameters such as the conditional Z phase, single qubit
Z phase and swap angles in two-qubit entangling gate
can drift and oscillate over the time scale of O(10) min-
utes [167, 168]. Such unknown systematic and time-
dependent coherent errors provides signiÔ¨Åcant challenges
for applications in quantum machine learning where gra-
dient computation and update requires many measure-
ments, especially because of the use of entangling gates
in a swap test circuit. However, the large deviations in
single-qubit and two-qubit Z rotation angles can largely
be mitigated by including additional single-qubit Z phase
compensations. In learning the discriminator circuit that
is closest to a true swap test, the adversarial learning
of EQ-GAN provides a useful paradigm that may be
broadly applicable to improving the Ô¨Ådelity of other near-
term quantum algorithms.

To operate under this noise model, we can deÔ¨Åne a
cirq.NoiseModel that is compatible with TFQ, imple-
menting the noisy_operation method to add CZ and Z
phase errors on all CZ gates.

def noisy _op eration ( self , op ) :

if isinstance ( op . gate , cirq . ops . CZPowGate ) :

error_2q = cirq . ops . CZPowGate ( exponent = np .
random . normal ( self . mean [0] , self . stdev [0]) )
(* op . qubits )
error_1q_0 = cirq . ops . ZPowGate ( exponent = self
. single_errors [ op . qubits [0]]) ( op . qubits [0])
error_1q_1 = cirq . ops . ZPowGate ( exponent = self
. single_errors [ op . qubits [1]]) ( op . qubits [1])
return [ op , error_2q , error_1q_0 , error_1q_1
]

return op

To add the noise to TFQ, we can simply convert the
circuit with noise into a tensor. For instance, to generate
swap tests over n_data datapoints:

sw a p_ te s t _c ir c ui t = sw ap _te st_ cir cui t . with_noise

( noise_model )

sw a p_ te s t _c ir c ui t = tf . tile ( tfq .

co nv e r t_ to _ te nso r ([ swa p_t est _c irc uit ]) , tf .
constant ([ n_data ]) )

Figure 32. EQ-GAN experiment for learning a single-qubit
state. The discriminator (U (Œ∏d) is constructed with free Z
rotation angles to suppress CZ gate errors, allowing the gen-
erator œÅ(Œ∏g) to converge closer to the true data state œÉ by
varying X and Z rotation angles.

While the noise suppression experiment can be evalu-
ated with a simulated noise model as described above, the
full model can also be run by straightforwardly changing
the backend via the Google Quantum Engine API.

engine = cirq . google . Engine ( project_id =

project_id )

backend = engine . sampler ( processor_id =[ ‚Äô rainbow ‚Äô

] , gate_set = cirq . google . XMON )

The hardware backend can then be directly applied in

a model.

e xp e ct a ti o n_ o ut p ut = tfq . layers . Expectation (

backend = backend ) (

full_swaptest ,
symbol_names = generator_symbols ,
operators = circuits . swap_readout_op (
generator_qubits , data_qubits ) ,
initializer = tf . c o n s t a n t _ i n i t i a l i z e r (

g e n e r a t o r _ i n i t i a l i a t i o n ) )

When evaluating the adversarial swap test on a cali-
brated quantum device, we found that that the error was
reduced by a factor of around four compared to a direct
implementation of the standard swap test [76].

3. Approximate Quantum Random Access Memory

To run this example, see the Colab notebook at:

research/eq_gan/variational_qram.ipynb

When applying quantum machine learning to classi-
cal data, most algorithms require access to a quantum

|0‚ü©|0‚ü©HHH{{{ùúéùúåg)ùúÉ(Ud)ùúÉ(X‚àöZ‚àöXùúÉ2ZùúÉ3ZùúÉ1ZùúÉ4ZùúÉ5‚àö

random access memory (QRAM) that stores the classi-
cal dataset in superposition. More particularly, a set of
classical data can be described by the empirical distribu-
tion {Pi} over all possible input data i. Most quantum
machine learning algorithms require the conversion from
{Pi} into a quantum state (cid:80)
Pi |œài(cid:105), i.e. a superposi-
tion of orthogonal basis states |œài(cid:105) representing each sin-
gle classical data entry with an amplitude proportional
to the square root of the classical probability Pi. Prepar-
ing such a superposition of an arbitrary set of n states
takes O(n) operations at best, which ruins the exponen-
tial speedup. Given a suitable ansatz, we may use an
EQ-GAN to learn a state approximately equivalent to
the superposition of data.

i

To demonstrate a variational QRAM, we consider a
dataset of two peaks sampled from diÔ¨Äerent Gaussian
distributions. Exactly encoding the empirical probability
density function requires a very deep circuit and multiple-
control rotations; similarly, preparing a Gaussian dis-
tribution on a device with planar connectivity requires
deep circuits. Hence, we select shallow circuit ansatzes
that generate concatenated exponential functions to ap-
proximate a symmetric peak [169]. Once trained to ap-
proximate the empirical data distribution, the variational
QRAM closely reproduces the original dataset (Fig. 33).

Figure 33. Two-peak total dataset (sampled from normal dis-
tributions, N = 120) and variational QRAM of the training
dataset (N = 60). The variational QRAM is obtained by
training an EQ-GAN to generate a state œÅ with the shallow
peak ansatz to approximate an exact superposition of states
œÉ. The training and test datasets (each N = 60) are both
balanced between the two classes.

As a proof of principle for using such QRAM in a quan-
tum machine learning context, we demonstrate in the ex-
ample code an application of the QRAM to train a quan-
tum neural network [24]. The loss function is computed
either by considering each data entry individually (en-
coded as a quantum circuit) or by considering each class
individually (encoded as a superposition in variational
QRAM). Given the same number of circuit evaluations
to compute gradients, the superposition converges to a
better accuracy at the end of training despite using an
approximate distribution.

48

H. Reinforcement Learning with Parameterized
Quantum Circuits

1. Background

Reinforcement learning (RL) [170] is one of the three
It pertains to a
main paradigms of machine learning.
learning setting where an agent interacts with an en-
vironment as to solve a sequential decision task set by
this environment, e.g., playing Atari games [171] or nav-
igating cities without a map [172]. This interaction is
commonly divided into episodes (s0, a0, r0, s1, a1, r1, . . .)
of states, actions and rewards exchanged between agent
and environment. For each state s the agent perceives, it
performs an action a sampled from its policy œÄ(a|s), i.e.,
a probability distribution over actions given states. The
environment then rewards the agent‚Äôs action with a real-
valued reward r and updates the state of the agent. This
interaction repeats cyclically until episode termination.
The goal of the agent here is to optimize its policy œÄ(a|s)
as to maximize its expected rewards in an episode, and
hence solve the task at hand. More precisely, the agent‚Äôs
Ô¨Ågure of merit is deÔ¨Åned by a so-called value function

VœÄ(s0) = EœÄ

(cid:35)

Œ≥trt

(cid:34) H
(cid:88)

t=0

(43)

where H is the horizon (or length) of an episode, and
Œ≥ is a discount factor in [0, 1] that adjusts the relative
value of immediate versus long-term rewards.

Traditionally, RL algorithms belong to two families:

‚Ä¢ Policy-based algorithms, where an agent‚Äôs policy is de-
Ô¨Åned by a parametrized model œÄŒ∏(a|s) and is updated
via steepest ascent on its resulting value function.

‚Ä¢ Value-based algorithms, where a parametrized model
VŒ∏(s) is used to approximate the optimal value func-
tion, i.e., associated to an optimal policy. The agent
interacts with the environment with a policy that gen-
erally depends on VŒ∏(s) and its experienced rewards
are used to improve the quality of the approximation.

When facing environments with large (or continuous)
state/action spaces, DNNs recently became the standard
models used in both policy-based and value-based ap-
proaches. Deep RL has achieved a number of unprece-
dented achievements, such as superhuman performance
in Go [173], StarCraft II [174] and Dota 2 [175]. More re-
cently, several proposals have been made to enhance deep
RL agents with quantum analogues of DNNs (QNNs,
or PQCs), both in policy-based [176] and value-based
RL [177‚Äì180]. These works essentially described how to
adapt deep RL algorithms to work with PQCs as either
policies or value-function approximators, and tested their
performance on benchmarking environments [181]. As
pointed out in [176, 179], certain design choices of PQCs
can lead to signiÔ¨Åcant gaps in learning performance. The

(a) Empirical PDF(b) Variational QRAMmost crucial of these choices is arguably the data encod-
ing strategy. As supported by theoretical investigations
[182, 183], data re-uploading (see Fig. 34) where encod-
ing layers and variational layers of parametrized gates are
sequentially alternated, stands out as a way to get highly
expressive models.

Figure 34. A parametrized quantum circuit with data re-
uploading as used in quantum reinforcement learning. Each
white gate represents a single-qubit rotation, parametrized
either by a variational angle Œ∏ in the variational layers (or-
ange), or by a data component s (re-scaled by a parameter Œª)
in encoding layers (blue).

2. Policy-Gradient Reinforcement Learning with PQCs

In the following, we focus on the implementation of a
policy-based approach to RL using PQCs [176]. For that,
let us deÔ¨Åne the parametrized policy of our agent as:

49

2. Train a reinforcement learning agent based on this

quantum circuit

Required TFQ functionalities:

1. Parametrized circuit layers
2. Custom Keras layers
3. Automatic diÔ¨Äerentiation w.r.t. a custom loss using

tf.GradientTape

3.

Implementation

To run this example in the browser through Colab,

follow the link:

docs/tutorials/quantum_reinforcement_learning.ipynb

We demonstrate the implementation of the quantum
policy-gradient algorithm on CartPole-v1, a benchmark-
ing task from OpenAI Gym [181]. This environment has
a 4-dimensional continuous state space and a discrete ac-
tion space of size 2. We hence use a PQC acting on 4
qubits, on which we evaluate 2 expectation values.

We start by implementing the quantum circuit used as
the agent‚Äôs PQC. It is composed of alternating layers of
variational single-qubit rotations, entangling CZ opera-
tors, and data-encoding single-qubit rotations.

def o ne _ qu b it _ ro t at i on (q , symbols ) :

return [ cirq . X ( q ) ** symbols [0] , cirq . Y ( q ) **

symbols [1] , cirq . Z ( q ) ** symbols [2]]

œÄŒ∏(a|s) =

ewa(cid:104)Oa(cid:105)s,Œ∏,Œª
ewa(cid:48) (cid:104)Oa(cid:48) (cid:105)s,Œ∏,Œª

(44)

def entangling_layer ( qubits ) :

cz_ops = [ cirq . CZ ( q0 , q1 ) for q0 , q1 in zip (

where (cid:104)Oa(cid:105)s,Œ∏,Œª
is the expectation value of an observable
Oa (e.g., Pauli Z on Ô¨Årst qubit) associated to action a,
as measured at the output of the PQC. This expectation
value is also augmented by a trainable weight wa, which
is also action speciÔ¨Åc (see Fig. 34).

As stated above, the parameters of this policy are up-
dated as to perform gradient ascent on their correspond-
ing value function (see (43)). Thanks to the so-called
policy gradient theorem [184], we know that another for-
mulation of this objective function is given by the follow-
ing loss function:

L(Œ∏) = ‚àí

1
|B|

(cid:88)

(cid:34)H‚àí1
(cid:88)

s0,a0,r0,...‚ààB

t=0

log(œÄŒ∏(at|st))

(cid:35)

Œ≥t(cid:48)

rt+t(cid:48)

H‚àít
(cid:88)

t(cid:48)=1

for a batch B of episodes (s0, a0, r0, ...) sampled by
following œÄŒ∏ in the environment.
This expression
has the advantage that it avoids numerical diÔ¨Äerentia-
tion to compute the gradient of the loss with respect to Œ∏.

Target problems:

qubits , qubits [1:]) ]

cz_ops += ([ cirq . CZ ( qubits [0] , qubits [ -1]) ] if

len ( qubits ) != 2 else [])

return cz_ops

def generate_circuit ( qubits , n_layers ) :

n_qubits = len ( qubits )

params = sympy . symbols ( f ‚Äô theta (0:{3*( n_layers

+1) * n_qubits }) ‚Äô)

params = np . asarray ( params ) . reshape (( n_layers

+1 , n_qubits , 3) )

inputs = sympy . symbols ( f ‚Äôx (0:{ n_qubits }) ‚Äô+ f ‚Äô

(0:{ n_layers }) ‚Äô)

inputs = np . asarray ( inputs ) . reshape (( n_layers ,

n_qubits ) )

circuit = cirq . Circuit ()
for l in range ( n_layers ) :

circuit += cirq . Circuit ( on e _q u bi t _r o ta t io n (q
, params [l , i ]) for i , q in enumerate ( qubits ) )
circuit += entangling_layer ( qubits )
circuit += cirq . Circuit ( cirq . rx ( inputs [l , i ])
( q ) for i , q in enumerate ( qubits ) )

circuit += cirq . Circuit ( o ne _ qu bi t _r o ta t io n (q ,
params [ n_layers , i ]) for i , q in enumerate (
qubits ) )

1. Implement a parametrized quantum circuit with

data re-uploading

return circuit , list ( params . flat ) , list ( inputs

. flat )

ControlledPQC

We use this quantum circuit to deÔ¨Åne a ControlledPQC
layer.
To sort between variational and encoding
angles in the data re-uploading scheme, we include
the
This
ReUploadingPQC layer will manage the trainable param-
eters (variational angles Œ∏ and input-scaling parameters
Œª) and resolve the input values (input state s) into the
appropriate symbols in the circuit.

in a custom Keras layer.

class ReUploadingPQC ( tf . keras . layers . Layer ) :

def __init__ ( self , qubits , n_layers ,

observables , activation = ‚Äô linear ‚Äô , name = " re -
uploading_PQC " ) :
super ( ReUploading , self ) . __init__ ( name = name )
self . n_layers = n_layers

circuit , theta_symbols , input_symbols =
ge ner a te_ c ircuit ( qubits , n_layers )
self . c o m put ati on_ lay er = tfq . layers .
ControlledPQC ( circuit , observables )

theta_init = tf . r a n d o m _ u n i f o r m _ i n i t i a l i z e r (
minval =0. , maxval = np . pi )
self . theta = tf . Variable ( initial_value =
theta_init ( shape =(1 , len ( theta_symbols ) ) ,
dtype = " float32 " ) , trainable = True , name = "
thetas " )
lmbd_init = tf . ones ( shape =( len ( qubits ) *
n_layers ,) )
self . lmbd = tf . Variable ( initial_value =
lmbd_init , dtype = " float32 " , trainable = True ,
name = " lambdas " )
symbols = [ str ( x ) for x in theta_symbols +
input_symbols ]
self . indices = tf . constant ([ sorted ( symbols ) .
index ( a ) for a in symbols ])
self . activation = activation
self . empty_circuit = tfq . co nve rt_ to_ ten sor ([
cirq . Circuit () ])

def call ( self , inputs ) :

batch_dim = tf . gather ( tf . shape ( inputs [0]) ,
0)
ti le d _ up _c i rcu it s = tf . repeat ( self .
empty_circuit , repeats = batch_dim )
til ed _ up _thetas = tf . tile ( self . theta ,
multiples =[ batch_dim , 1])
til ed _ up _inputs = tf . tile ( inputs [0] ,
multiples =[1 , self . n_layers ])
scaled_inputs = tf . einsum ( "i , ji - > ji " , self .
lmbd , t i le d_up_inputs )
squ as h ed _inputs = tf . keras . layers . Activation
( self . activation ) ( scaled_inputs )
joined_vars = tf . concat ([ tiled_up_thetas ,
squ as h ed _inputs ] , axis =1)
joined_vars = tf . gather ( joined_vars , self .
indices , axis =1)

return self . c omp uta tio n_l ay er ([
tiled_up_circuits , joined_vars ])

We also implement a custom Keras layer to post-
process the expectation values (cid:104)Oa(cid:105)s,Œ∏,Œª
at the output of
the PQC. Here, the Alternating layer multiplies a single
expectation value by weights (w0, w1)
(cid:104)Z0Z1Z2Z3(cid:105)s,Œ∏,Œª
initialized to (1, ‚àí1).

50

def __init__ ( self , output_dim ) :

super ( Alternating , self ) . __init__ ()
self . w = tf . Variable ( initial_value = tf .
constant ([[( -1.) ** i for i in range (
output_dim ) ]]) , dtype = " float32 " , trainable =
True , name = " obs - weights " )

def call ( self , inputs ) :

return tf . matmul ( inputs , self . w )

ops = [ cirq . Z ( q ) for q in qubits ]
observables = [ reduce (( lambda x , y : x * y ) , ops ) ]

We now put together all these layers to deÔ¨Åne a Keras

model of the policy in equation (44).

def g e n e r a t e _ m o d e l _ p o l i c y ( qubits , n_layers ,

n_actions , beta , observables ) :

input_tensor = tf . keras . Input ( shape =( len (

qubits ) , ) , dtype = tf . dtypes . float32 , name = ‚Äô
input ‚Äô)

re_uploading = ReUploading ( qubits , n_layers ,

observables ) ([ input_tensor ])
process = tf . keras . Sequential ([

Alternating ( n_actions ) ,
tf . keras . layers . Lambda ( lambda x : x * beta ) ,
tf . keras . layers . Softmax ()
] , name = " observables - policy " )
policy = process ( re_uploading )
model = tf . keras . Model ( inputs =[ input_tensor ] ,

outputs = policy )

return model
We now move on to the implementation of the learn-
ing algorithm. We start by deÔ¨Åning two helper func-
tions: a gather_episodes function that gathers a batch
of episodes of interaction with the environment, and a
compute_returns function that computes the discounted
sums of rewards appearing in the agent‚Äôs loss function.

def gather_episodes ( state_bounds , n_actions ,

model , n_episodes , env_name ) :

trajectories = [ defaultdict ( list ) for _ in

range ( n_episodes ) ]

envs = [ gym . make ( env_name ) for _ in range (

n_episodes ) ]

done = [ False for _ in range ( n_episodes ) ]
states = [ e . reset () for e in envs ]

while not all ( done ) :

unfinished_ids = [ i for i in range (
n_episodes ) if not done [ i ]]
no rma liz ed _st ate s = [ s / state_bounds for i , s

in enumerate ( states ) if not done [ i ]]

for i , state in zip ( unfinished_ids ,
no rma liz ed_ sta te s ) :

trajectories [ i ][ ‚Äô states ‚Äô ]. append ( state )

# Compute policy for all unfinished envs in
parallel
states = tf . con ver t_t o_t ens or (
no rma liz ed_ sta te s )
action_probs = model ([ states ])

# Store action and transition all
environments to the next state
states = [ None for i in range ( n_episodes ) ]
for i , policy in zip ( unfinished_ids ,
action_probs . numpy () ) :

action = np . random . choice ( n_actions , p =

class Alternating ( tf . keras . layers . Layer ) :

policy )

states [ i ] , reward , done [ i ] , _ = envs [ i ].

step ( action )

trajectories [ i ][ ‚Äô actions ‚Äô ]. append ( action )
trajectories [ i ][ ‚Äô rewards ‚Äô ]. append ( reward )

return trajectories

def compu te_ returns ( rewards_history , gamma ) :

returns = []
disc ounted _sum = 0
for r in re wards_history [:: -1]:

discounted_sum = r + gamma * discounted_sum
returns . insert (0 , discounted_sum )

return returns

To train the policy, we need a function that updates
the model parameters. This is done via gradient descent
on the loss of the agent. Since the loss function in policy-
gradient approaches is more involved than, for instance,
a supervised learning loss, we use a tf.GradientTape to
store the contributions of our model evaluation to the
loss. When all contributions have been added, this tape
can then be used to backpropagate the loss on all the
model evaluations and hence compute the required gra-
dients.

@tf . function
def re i nfo r ce_ update ( states , actions , returns ,

model ) :

states = tf . c on ver t_t o_t ens or ( states )
actions = tf . co nve rt_ to_ ten sor ( actions )
returns = tf . co nve rt_ to_ ten sor ( returns )

with tf . GradientTape () as tape :

tape . watch ( model . t r a i n a b l e _ v ar i a b l e s )
logits = model ( states )
p_actions = tf . gather_nd ( logits , actions )
log_probs = tf . math . log ( p_actions )
loss = tf . math . reduce_sum ( - log_probs *
returns ) / batch_size

grads = tape . gradient ( loss , model .

t r a i n a b l e _ v a r i a b l e s )

for optimizer , w in zip ([ optimizer_in ,

optimizer_var , optimizer_out ] , [ w_in , w_var ,

w_out ]) :

optimizer . apply_gradients ([( grads [ w ] , model .
t r a i n a b l e _ v a r i a b l e s [ w ]) ])

With this, we can implement the main training loop of

the agent.

env_name = " CartPole - v1 "

for batch in range ( n_episodes // batch_size ) :

# Gather episodes
episodes = gather_episodes ( state_bounds ,

n_actions , model , batch_size , env_name )

# Group states , actions and returns in arrays
states = np . concatenate ([ ep [ ‚Äô states ‚Äô] for ep

in episodes ])

actions = np . concatenate ([ ep [ ‚Äô actions ‚Äô] for ep

in episodes ])

rewards = [ ep [ ‚Äô rewards ‚Äô] for ep in episodes ]
returns = np . concatenate ([ compute_returns (
ep_rwds , gamma ) for ep_rwds in rewards ])
returns = np . array ( returns , dtype = np . float32 )
id_ ac t io n _p airs = np . array ([[ i , a ] for i , a in

enumerate ( actions ) ])

51

# Update model parameters
reinforce_update ( states , id_action_pairs ,

returns , model )

Thanks

to the parallelization of model

evalua-
tions and fast automatic diÔ¨Äerentiation using the
tfq.differentiators.Adjoint() diÔ¨Äerentiator, the execu-
tion of this training loop for 500 episodes takes about
15 minutes on a regular laptop (for a PQC acting on 4
qubits and with 5 re-uploading layers).

4. Value-Based Reinforcement Learning with PQCs

The example above provides the code for a policy-
based approach to RL with PQCs. The tutorial under
this link:

docs/tutorials/quantum_reinforcement_learning.ipynb

also implements the value-based algorithm introduced in
[179]. Aside from the learning mechanisms speciÔ¨Åc to
deep value-based RL (e.g., a replay memory to re-use
past experience and a target model to stabilize learning),
these two methods essentially diÔ¨Äer in the role played
of the PQC. In our
by the expectation values (cid:104)Oa(cid:105)s,Œ∏,Œª
quantum approach to value-based RL, these correspond
to approximations of the values Q(s, a) (i.e., the value
function V (s) taken as a function of a state and an ac-
tion), trained using a loss function of the form:

L(Œ∏) =

1
|B|

(cid:88)

(cid:16)

s,a,r,s(cid:48)‚ààB

QŒ∏(s, a) ‚àí [r + max

a(cid:48)

QŒ∏(cid:48) (s(cid:48), a(cid:48))]

(cid:17)2

derived from Q-learning [170]. We refer to [178, 179] and
the tutorial for more details.

5. Quantum Environments

In the initial works,

it was proven that there exist
learning environments that can only be eÔ¨Éciently tackled
by quantum agents (barring well-established computa-
tional assumptions) [176]. However, these problems are
artiÔ¨Åcial and contrived, and it is a key question in the
Ô¨Åeld of Quantum RL whether there exist natural environ-
ments for which quantum agents can have a large learn-
ing advantage over their classical counterparts.
Intu-
itive candidates are RL environments that are themselves
quantum in nature.
In this setting, early results have
already demonstrated that variational quantum methods
can be advantageous when the data perceived by a learn-
ing agent stems from measurements on a quantum sys-
tem [176]. Going a step further, one could also consider a
setting where the states perceived by the agent are gen-
uinely quantum and can therefore be processed directly
by a PQC, as was recently explored in a quantum control
environment [185].

VI. CLOSING REMARKS

The rapid development of quantum hardware repre-
sents an impetus for the equally rapid development of
quantum applications. In October 2017, the Google AI
Quantum team and collaborators released its Ô¨Årst soft-
ware library, OpenFermion, to accelerate the develop-
ment of quantum simulation algorithms for chemistry
and materials sciences. Likewise, TensorFlow Quantum
is intended to accelerate the development of quantum
machine learning algorithms for a wide array of applica-
tions. Quantum machine learning is a very new and ex-
citing Ô¨Åeld, so we expect the framework to change with
the needs of the research community, and the availabil-
ity of new quantum hardware. We have open-sourced
the framework under the commercially friendly Apache2
license, allowing future commercial products to embed
TFQ royalty-free. If you would like to participate in our
community, visit us at:

github.com/tensorflow/quantum/

VII. ACKNOWLEDGEMENTS

The authors would like to thank Google Research for
supporting this project. In particular, M.B., G.V., T.M.,
and A.J.M. would like to thank the Google Quantum AI
team for their support during their respective internships,
and several useful discussions with Matt Harrigan, John

52

Platt, and Nicholas Rubin. The authors would like to
also thank Achim Kempf from the University of Water-
loo for sponsoring this project. M.B. and J.Y. would like
to thank the Google Brain and Core team for support-
ing this project, in particular Christian Howard, Billy
Lamberta, Tom O‚ÄôMalley, Francois Chollet, Yifei Feng,
David Rim, Justin Hong, and Megan Kacholia. G.V.,
A.J.M. and J.Y. would like to thank Stefan Leichenauer,
Jack Hidary and the rest of the Sandbox@Alphabet team
for their support during their respective Quantum Resi-
dencies. G.V. acknowledges support from NSERC. D.B.
is an Associate Fellow in the CIFAR program on Quan-
tum Information Science. A.S. and M.S. were supported
by the USRA Feynman Quantum Academy funded by
the NAMS R&D Student Program at NASA Ames Re-
search Center and by the Air Force Research Labora-
tory (AFRL), NYSTEC-USRA Contract (FA8750-19-3-
6101). A.Z. acknowledges support from Caltech‚Äôs Intel-
ligent Quantum Networks and Technologies (INQNET)
research program and by the DOE/HEP QuantISED pro-
gram grant, Quantum Machine Learning and Quantum
Computation Frameworks (QMLQCF) for HEP, award
number DE-SC0019227. S.J. acknowledges support from
the Austrian Science Fund (FWF) through the projects
DK-ALM:W1259-N27 and SFB BeyondC F7102, and
from the Austrian Academy of Sciences as a recipient of
the DOC Fellowship. V.D. acknowledges support from
the Dutch Research Council (NWO/OCW), as part of
the Quantum Software Consortium programme (project
number 024.003.037).

[1] K. P. Murphy, Machine learning: a probabilistic perspec-

tive (MIT press, 2012).

[2] J. A. Suykens and J. Vandewalle, Neural processing let-

ters 9, 293 (1999).

[3] S. Wold, K. Esbensen, and P. Geladi, Chemometrics

and intelligent laboratory systems 2, 37 (1987).

[4] A. K. Jain, Pattern recognition letters 31, 651 (2010).
[5] Y. LeCun, Y. Bengio, and G. Hinton, nature 521, 436

(2015).

[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, in Ad-
vances in Neural Information Processing Systems 25 ,
edited by F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger (Curran Associates, Inc., 2012) pp.
1097‚Äì1105.

[7] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio,
Deep learning, Vol. 1 (MIT press Cambridge, 2016).
[8] J. Preskill, arXiv preprint arXiv:1801.00862 (2018).
[9] R. P. Feynman, International Journal of Theoretical

Physics 21, 467 (1982).

[10] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D.
Johnson, M. Kieferov√°, I. D. Kivlichan, T. Menke,
B. Peropadre, N. P. Sawaya, et al., Chemical reviews
119, 10856 (2019).

[11] P. W. Shor, in Proceedings 35th annual symposium on
foundations of computer science (Ieee, 1994) pp. 124‚Äì

134.

[12] E. Farhi, J. Goldstone,

tum approximate optimization algorithm,‚Äù
arXiv:1411.4028 [quant-ph].

and S. Gutmann, ‚ÄúA quan-
(2014),

[13] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A.
Buell, et al., Nature 574, 505 (2019).

[14] S. Lloyd, M. Mohseni,

and P. Rebentrost, Nature

Physics 10, 631‚Äì633 (2014).

[15] P. Rebentrost, M. Mohseni, and S. Lloyd, Phys. Rev.

Lett. 113, 130503 (2014).

[16] S. Lloyd, M. Mohseni, and P. Rebentrost, ‚ÄúQuantum
algorithms for supervised and unsupervised machine
learning,‚Äù (2013), arXiv:1307.0411 [quant-ph].

[17] I. Kerenidis and A. Prakash, ‚ÄúQuantum recommenda-

tion systems,‚Äù (2016), arXiv:1603.08675.

[18] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, Nature 549, 195 (2017).
[19] V. Giovannetti, S. Lloyd, and L. Maccone, Physical

review letters 100, 160501 (2008).

[20] S. Arunachalam, V. Gheorghiu, T. Jochym-O‚ÄôConnor,
M. Mosca, and P. V. Srinivasan, New Journal of Physics
17, 123010 (2015).

[21] E. Tang,

the 51st Annual ACM
SIGACT Symposium on Theory of Computing (2019)

in Proceedings of

pp. 217‚Äì228.

[22] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush,
S. Boixo, H. Neven, and J. R. McClean, Nature Com-
munications 12, 2631 (2021).
[23] J. Preskill, Quantum 2, 79 (2018).
[24] E. Farhi and H. Neven, arXiv preprint arXiv:1802.06002

(2018).

[25] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-
and J. L.

Q. Zhou, P. J. Love, A. Aspuru-Guzik,
O‚Äôbrien, Nature communications 5, 4213 (2014).

[26] N. Killoran, T. R. Bromley,

J. M. Arrazola,
M. Schuld, N. Quesada, and S. Lloyd, arXiv preprint
arXiv:1806.06871 (2018).

[27] D. Wecker, M. B. Hastings, and M. Troyer, Phys. Rev.

A 92, 042303 (2015).

[28] L. Zhou, S.-T. Wang, S. Choi, H. Pichler, and M. D.

53

[50] Z. Wang, N. C. Rubin, J. M. Dominy, and E. G. RieÔ¨Äel,

arXiv preprint arXiv:1904.09314 (2019).

[51] E. Farhi and H. Neven, ‚ÄúClassiÔ¨Åcation with quantum
(2018),

neural networks on near term processors,‚Äù
arXiv:1802.06002 [quant-ph].

[52] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
and H. Neven, Nature communications 9, 1

bush,
(2018).

[53] I. Cong, S. Choi, and M. D. Lukin, Nature Physics 15,

1273‚Äì1278 (2019).

[54] S. Lloyd and C. Weedbrook, Physical review letters 121,

040502 (2018).

[55] A. Arrasmith, M. Cerezo, P. Czarnik, L. Cincio, and
P. J. Coles, ‚ÄúEÔ¨Äect of barren plateaus on gradient-free
optimization,‚Äù (2020), arXiv:2011.12245.

[56] G. Verdon, J. Pye, and M. Broughton, arXiv preprint

Lukin, arXiv preprint arXiv:1812.01041 (2018).

arXiv:1806.09729 (2018).

[29] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, New Journal of Physics 18, 023023 (2016).
[30] S. HadÔ¨Åeld, Z. Wang, B. O‚ÄôGorman, E. G. Rief-
and R. Biswas, arXiv preprint

fel, D. Venturelli,
arXiv:1709.03489 (2017).

[31] E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart,
V. Stojevic, A. G. Green, and S. Severini, npj Quantum
Information 4, 1 (2018).

[32] S. Khatri, R. LaRose, A. Poremba, L. Cincio, A. T.

Sornborger, and P. J. Coles, Quantum 3, 140 (2019).

[33] M. Schuld and N. Killoran, Physical review letters 122,

040504 (2019).

[34] S. McArdle, T. Jones, S. Endo, Y. Li, S. Benjamin, and
X. Yuan, arXiv preprint arXiv:1804.03023 (2018).
[35] M. Benedetti, E. Grant, L. Wossnig, and S. Severini,

New Journal of Physics 21, 043023 (2019).

[36] B. Nash, V. Gheorghiu, and M. Mosca, arXiv preprint

arXiv:1904.01972 (2019).

[37] Z. Jiang, J. McClean, R. Babbush, and H. Neven, arXiv

preprint arXiv:1812.08190 (2018).

[57] J. Romero and A. Aspuru-Guzik, arXiv preprint

arXiv:1901.00848 (2019).

[58] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, C. Blank,
and N. Killoran, arXiv preprint

K. McKiernan,
arXiv:1811.04968 (2018).
[59] G. Verdon, A. J. Martinez,

J. Marks, S. Pa-
tel, G. Roeder, F. Sbahi, J. H. Yoo, S. Nanda,
S. Leichenauer,
and J. Hidary, arXiv preprint
arXiv:1910.02071 (2019).

[60] H.-Y. Huang, R. Kueng, and J. Preskill, Phys. Rev.

Lett. 126, 190505 (2021).

[61] Google AI Quantum and Collaborators, Science 369,

1084 (2020).

[62] M. Mohseni, A. M. Steinberg, and J. A. Bergou, Phys.

Rev. Lett. 93, 200403 (2004).

[63] K. K. van Dam, From Long-distance Entanglement to
Building a Nationwide Quantum Internet: Report of
the DOE Quantum Internet Blueprint Workshop, Tech.
Rep. BNL-216179-2020-FORE (Brookhaven National
Laboratory, 2020).

[38] G. R. Steinbrecher, J. P. Olson, D. Englund, and J. Car-

[64] J. J. Meyer, J. Borregaard, and J. Eisert, npj Quantum

olan, arXiv preprint arXiv:1808.10047 (2018).

[39] M. Fingerhuth, T. Babej,
arXiv:1810.13411 (2018).

et al., arXiv preprint

[40] R. LaRose, A. Tikku, √â. O‚ÄôNeel-Judy, L. Cincio, and

P. J. Coles, arXiv preprint arXiv:1810.10506 (2018).

[41] L. Cincio, Y. Suba≈üƒ±, A. T. Sornborger, and P. J. Coles,

New Journal of Physics 20, 113022 (2018).

[42] H. Situ, Z. Huang, X. Zou, and S. Zheng, Quantum

Information Processing 18, 230 (2019).

[43] H. Chen, L. Wossnig, S. Severini, H. Neven,

and
M. Mohseni, arXiv preprint arXiv:1805.08654 (2018).
and J. Biamonte, arXiv

[44] G. Verdon, M. Broughton,

preprint arXiv:1712.05304 (2017).

[45] M. Mohseni et al., Nature 543, 171 (2017).
[46] G. Verdon, M. Broughton, J. R. McClean, K. J. Sung,
R. Babbush, Z. Jiang, H. Neven,
and M. Mohseni,
‚ÄúLearning to learn with quantum neural networks via
classical neural networks,‚Äù (2019), arXiv:1907.05415.

[47] R. Sweke, F. Wilde, J. Meyer, M. Schuld, P. K.
F√§hrmann, B. Meynard-Piganeau, and J. Eisert, arXiv
preprint arXiv:1910.01155 (2019).

[48] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, New Journal of Physics 18, 023023 (2016).
[49] G. Verdon, J. M. Arrazola, K. Br√°dler, and N. Killoran,

arXiv preprint arXiv:1902.00409 (2019).

Information 7, 1 (2021).

[65] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven,

npj Quantum Information 5, 1 (2019).

[66] J. Carolan, M. Mohseni, J. P. Olson, M. Prabhu,
C. Chen, D. Bunandar, M. Y. Niu, N. C. Harris, F. N. C.
Wong, M. Hochberg, S. Lloyd, and D. Englund, Nature
Physics (2020).

[67] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-
ard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Lev-
enberg, D. Mane, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever,
K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan,
F. Viegas, O. Vinyals, P. Warden, M. Wattenberg,
M. Wicke, Y. Yu, and X. Zheng, ‚ÄúTensorÔ¨Çow: Large-
scale machine learning on heterogeneous distributed sys-
tems,‚Äù (2016), arXiv:1603.04467 [cs.DC].

[68] G. T. C. Developers, ‚ÄúCirq: A python framework for
creating, editing, and invoking noisy intermediate scale
quantum circuits,‚Äù (2018).

[69] A. Meurer, C. P. Smith, M. Paprocki, O. ƒåert√≠k, S. B.
Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K.
Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger,
R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johans-
son, F. Pedregosa, M. J. Curry, A. R. Terrel, v. Rouƒçka,

A. Saboo, I. Fernando, S. Kulal, R. Cimrman,
A. Scopatz, PeerJ Computer Science 3, e103 (2017).
[70] D. E. Rumelhart, G. E. Hinton, and R. J. Williams,

and

nature 323, 533 (1986).

[71] J. Biamonte and V. Bergholm, arXiv

(2017),

arXiv:1708.00006 [quant-ph].

[72] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman,
B. Fontaine, Y. Zou, J. Hidary, G. Vidal, and S. Le-
‚ÄúTensorNetwork: A Library for Physics
ichenauer,
and Machine Learning,‚Äù
(2019), arXiv:1905.01330
[physics.comp-ph].

[73] C. Roberts and S. Leichenauer, ‚ÄúIntroducing Tensor-
Network, an Open Source Library for EÔ¨Écient Tensor
Calculations,‚Äù (2019).

[74] The TensorFlow Authors, ‚ÄúEÔ¨Äective TensorFlow 2,‚Äù

(2019).

[75] F. Chollet et al., ‚ÄúKeras,‚Äù https://keras.io (2015).
[76] M. Y. Niu, A. Zlokapa, M. Broughton, S. Boixo,
M. Mohseni, V. Smelyanskyi,
and H. Neven, ‚ÄúEn-
tangling quantum generative adversarial networks,‚Äù
(2021), arXiv:2105.00080 [quant-ph].

[77] J. R. Johansson, P. D. Nation, and F. Nori, Comput.

Phys. Commun. 183, 1760 (2012).

[78] D. P. Kingma and J. Ba, arXiv preprint arXiv:1412.6980

(2014).

[79] G. E. Crooks, arXiv preprint arXiv:1905.13311 (2019).
and A. Aspuru-
[80] M. Smelyanskiy, N. P. D. Sawaya,
Guzik, ArXiv e-prints (2016), arXiv:1601.07195 [quant-
ph].

[81] T. H√§ner and D. S. Steiger, ArXiv e-prints

(2017),

arXiv:1704.01127 [quant-ph].

[82] Intel¬Æ, ‚ÄúIntel¬Æ Instruction Set Extensions Technol-

ogy,‚Äù (2020).

[83] Mark Buxton, ‚ÄúHaswell New Instruction Descriptions

Now Available!‚Äù

(2011).

[84] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush,
N. Ding, Z. Jiang, M. J. Bremner, J. M. Martinis, and
H. Neven, Nature Physics 14, 595 (2018).

[85] D. Gottesman, Stabilizer codes and quantum error cor-
rection, Ph.D. thesis, California Institute of Technology
(1997).

[86] M. Suzuki, Physics Letters A 146, 319 (1990).
[87] E. Campbell, Physical Review Letters 123 (2019),

10.1103/physrevlett.123.070503.

[88] N. C. Rubin, R. Babbush, and J. McClean, New Jour-

nal of Physics 20, 053020 (2018).

[89] A. Harrow

and

J. Napp,

arXiv

preprint

arXiv:1901.05374 (2019).

54

pp. 265‚Äì283.

[96] R. Frostig, M. J. Johnson, and C. Leary, Systems for

Machine Learning (2018).

[97] M. Abramowitz and I. Stegun, Appl. Math. Ser 55

(2006).

[98] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac,

and

N. Killoran, arXiv preprint arXiv:1811.11184 (2018).

[99] I. Newton, The Principia: mathematical principles of
natural philosophy (Univ of California Press, 1999).

[100] K. Mitarai, M. Negoro, M. Kitagawa,
Physical Review A 98, 032309 (2018).

and K. Fujii,

[101] S. Bhatnagar, H. Prasad, and L. Prashanth, Stochastic
Recursive Algorithms for Optimization: Simultaneous
Perturbation Methods (Springer, 2013).

[102] L. S. Pontryagin, Mathematical theory of optimal pro-

cesses (CRC press, 1987).

[103] X.-Z. Luo, J.-G. Liu, P. Zhang, and L. Wang, Quantum

4, 341 (2020).

[104] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker,
and J. Eisert, Physical Review Letters 105 (2010),
10.1103/physrevlett.105.150401.
[105] J. Schulman, N. Heess, T. Weber,

and P. Abbeel,
in Advances in Neural Information Processing Systems
(2015) pp. 3528‚Äì3536.

[106] V. Havl√≠ƒçek, A. D. C√≥rcoles, K. Temme, A. W. Harrow,
A. Kandala, J. M. Chow, and J. M. Gambetta, Nature
567, 209 (2019).

[107] W. Huggins, P. Patil, B. Mitchell, K. B. Whaley, and
E. M. Stoudenmire, Quantum Science and Technology
4, 024001 (2019).

[108] N. Tishby and N. Zaslavsky,

‚ÄúDeep learning and
(2015),

information bottleneck principle,‚Äù

the
arXiv:1503.02406 [cs.LG].

[109] P. Walther, K. J. Resch, T. Rudolph, E. Schenck, H. We-
infurter, V. Vedral, M. Aspelmeyer, and A. Zeilinger,
Nature 434, 169 (2005).

[110] M. A. Nielsen, Reports on Mathematical Physics 57,

147 (2006).

[111] G. Vidal, Physical Review Letters 101 (2008),

10.1103/physrevlett.101.110501.

[112] R. Shwartz-Ziv and N. Tishby, ‚ÄúOpening the black
(2017),

box of deep neural networks via information,‚Äù
arXiv:1703.00810 [cs.LG].

[113] P. B. M. Sousa and R. V. Ramos, ‚ÄúUniversal quan-
for n-qubit quantum gate: A pro-
(2006), arXiv:quant-

tum circuit
grammable quantum gate,‚Äù
ph/0602174 [quant-ph].

[114] M. Y. Niu, L. Li, M. Mohseni, and I. Chuang, to be

[90] Y. LeCun, L. Bottou, Y. Bengio, and P. HaÔ¨Äner, Pro-

published (2020).

ceedings of the IEEE 86, 2278 (1998).

[91] L. Bottou,

in Proceedings
(Springer, 2010) pp. 177‚Äì186.

of COMPSTAT‚Äô2010

[92] S. Ruder, arXiv preprint arXiv:1609.04747 (2016).
[93] Y. LeCun, D. Touresky, G. Hinton, and T. Sejnowski,
in Proceedings of the 1988 connectionist models summer
school, Vol. 1 (CMU, Pittsburgh, Pa: Morgan Kauf-
mann, 1988) pp. 21‚Äì28.

[94] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and
J. M. Siskind, The Journal of Machine Learning Re-
search 18, 5595 (2017).

[95] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
et al., in 12th {USENIX} symposium on operating sys-
tems design and implementation ({OSDI} 16) (2016)

[115] J. M. Martinis, npj Quantum Inf 7 (2021).
[116] M. McEwen, L. Faoro, K. Arya, A. Dunsworth,
T. Huang, S. Kim, B. Burkett, A. Fowler, F. Arute,
J. C. Bardin, A. Bengtsson, A. Bilmes, B. B. Buck-
ley, N. Bushnell, Z. Chen, R. Collins, S. Demura,
A. R. Derk, C. Erickson, M. Giustina, S. D. Har-
rington, S. Hong, E. JeÔ¨Ärey, J. Kelly, P. V. Klimov,
F. Kostritsa, P. Laptev, A. Locharla, X. Mi, K. C.
Miao, S. Montazeri, J. Mutus, O. Naaman, M. Nee-
ley, C. Neill, A. Opremcak, C. Quintana, N. Redd,
P. Roushan, D. Sank, K. J. Satzinger, V. Shvarts,
T. White, Z. J. Yao, P. Yeh, J. Yoo, Y. Chen,
V. Smelyanskiy, J. M. Martinis, H. Neven, A. Megrant,
L. IoÔ¨Äe, and R. Barends, ‚ÄúResolving catastrophic error
bursts from cosmic rays in large arrays of superconduct-

ing qubits,‚Äù (2021), arXiv:2104.05219 [quant-ph].
[117] Y. Chen, M. W. HoÔ¨Äman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas, arXiv
preprint arXiv:1611.03824 (2016).

[140] J. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Va-
sudevan, D. Moore, B. Patton, A. Alemi, M. HoÔ¨Äman,
and R. A. Saurous, ‚ÄúTensorÔ¨Çow distributions,‚Äù (2017),
arXiv:1711.10604 [cs.LG].

[118] E. Farhi, J. Goldstone, and S. Gutmann, arXiv preprint

[141] C. P. Robert,

‚ÄúThe metropolis-hastings algorithm,‚Äù

55

arXiv:1412.6062 (2014).

[119] G. S. Hartnett and M. Mohseni,

ity density theory for spin-glass systems,‚Äù
arXiv:2001.00927.

‚ÄúA probabil-
(2020),

[120] G. S. Hartnett and M. Mohseni, ‚ÄúSelf-supervised learn-
ing of generative spin-glasses with normalizing Ô¨Çows,‚Äù
(2020), arXiv:2001.00585.

[121] A. Hagberg, P. Swart,

and D. S Chult, Exploring
network structure, dynamics, and function using Net-
workX, Tech. Rep. (Los Alamos National Lab.(LANL),
Los Alamos, NM (United States), 2008).

[122] M. Streif and M. Leib, Quantum Sci. Technol. 5, 034008

(2020).

[123] H. Pichler, S.-T. Wang, L. Zhou, S. Choi,

and
M. D. Lukin, ‚ÄúQuantum optimization for maximum
independent set using rydberg atom arrays,‚Äù
(2018),
arXiv:1808.10816.

[124] M. Wilson, S. Stromswold, F. Wudarski, S. Had-
‚ÄúOptimiz-
(2019),

Ô¨Åeld, N. M. Tubman,
ing quantum heuristics with meta-learning,‚Äù
arXiv:1908.03185 [quant-ph].

and E. RieÔ¨Äel,

[125] E. Farhi, J. Goldstone, and S. Gutmann, ArXiv e-prints

(2014), arXiv:1411.4028 [quant-ph].

[126] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
bush, and H. Neven, Nature Communications 9 (2018),
10.1038/s41467-018-07090-4.

[127] X. Glorot and Y. Bengio, in AISTATS (2010) pp. 249‚Äì

256.

[128] M. Cerezo, A. Sone, T. VolkoÔ¨Ä, L. Cincio, and P. J.
Coles, arXiv preprint arXiv:2001.00550 (2020).

[129] Y. Bengio,

in Neural networks: Tricks of the trade

(Springer, 2012) pp. 437‚Äì478.

[130] E. Knill, G. Ortiz, and R. D. Somma, Physical Review

A 75, 012328 (2007).

[131] E. Grant, L. Wossnig, M. Ostaszewski,
M. Benedetti, Quantum 3, 214 (2019).

and

[132] A. Skolik, J. R. McClean, M. Mohseni, P. van der
Smagt, and M. Leib, Quantum Machine Intelligence
3, 1 (2021).

[133] E. Campos, A. Nasrallah, and J. Biamonte, Physical

Review A 103, 032607 (2021).

[134] C. Cirstoiu, Z. Holmes, J. Iosue, L. Cincio, P. J.
Coles, and A. Sornborger, ‚ÄúVariational fast forward-
ing for quantum simulation beyond the coherence time,‚Äù
(2019), arXiv:1910.04292 [quant-ph].
[135] N. Wiebe, C. Granade, C. Ferrie,

and D. Cory,
Physical Review Letters 112 (2014), 10.1103/phys-
revlett.112.190501.

[136] G. Verdon, T. McCourt, E. Luzhnica, V. Singh, S. Le-
ichenauer, and J. Hidary, ‚ÄúQuantum graph neural net-
works,‚Äù (2019), arXiv:1909.12264 [quant-ph].

[137] S. Greydanus, M. Dzamba, and J. Yosinski, ‚ÄúHamil-
(2019), arXiv:1906.01563

tonian neural networks,‚Äù
[cs.NE].

[138] L. Cincio, Y. Suba≈üƒ±, A. T. Sornborger, and P. J. Coles,

New Journal of Physics 20, 113022 (2018).

[139] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information: 10th Anniversary Edition,
10th ed. (Cambridge University Press, USA, 2011).

(2015), arXiv:1504.01896 [stat.CO].

[142] S. McArdle, S. Endo, A. Aspuru-Guzik, S. C. Benjamin,
and X. Yuan, Reviews of Modern Physics 92, 015003
(2020).

[143] J. R. McClean, M. E. Kimchi-Schwartz, J. Carter, and
W. A. De Jong, Physical Review A 95, 042308 (2017).
[144] O. Higgott, D. Wang, and S. Brierley, Quantum 3, 156

(2019).

[145] K. M. Nakanishi, K. Mitarai, and K. Fujii, Physical

Review Research 1, 033062 (2019).
[146] K. Abhinav et al., Nature 549, 242 (2017).
[147] H. R. Grimsley, S. E. Economou, E. Barnes, and N. J.
Mayhall, Nature communications 10, 1 (2019).
[148] M. Ostaszewski, E. Grant, and M. Benedetti, Quantum

5, 391 (2021).

[149] P. K. Barkoutsos, G. Nannicini, A. Robert, I. Tavernelli,

and S. Woerner, Quantum 4, 256 (2020).

[150] N. Yamamoto, arXiv preprint arXiv:1909.05074 (2019).
[151] J. R. McClean, N. C. Rubin, K. J. Sung, I. D. Kivlichan,
X. Bonet-Monroig, Y. Cao, C. Dai, E. S. Fried, C. Gid-
ney, B. Gimby, et al., Quantum Science and Technology
5, 034014 (2020).

[152] Q. Sun, T. C. Berkelbach, N. S. Blunt, G. H. Booth,
S. Guo, Z. Li, J. Liu, J. D. McClain, E. R. Say-
futyarova, S. Sharma, et al., Wiley Interdisciplinary
Reviews: Computational Molecular Science 8, e1340
(2018).

[153] A.

W.
Proceedings

Sandvik,

ence
https://aip.scitation.org/doi/pdf/10.1063/1.3518900.

1297,

135

Confer-
(2010),

AIP

[154] F. Verstraete,
in
Advances
https://doi.org/10.1080/14789940801912366.

V. Murg,
Physics

57,

and

143

J. Cirac,
(2008),

[155] J. Carrasquilla and R. G. Melko, Nature Physics 13,

431 (2017).

[156] E. P. L. van Nieuwenburg, Y.-H. Liu, and S. D. Huber,

Nature Physics 13, 435 (2017).

[157] A. V. Uvarov, A. S. Kardashin, and J. D. Biamonte,

Phys. Rev. A 102, 012415 (2020).

[158] A. Smith, M. S. Kim, F. Pollmann, and J. Knolle, npj

Quantum Information 5, 106 (2019).

[159] W. W. Ho and T. H. Hsieh, SciPost Phys. 6, 29 (2019).
[160] D. Wierichs, C. Gogolin, and M. Kastoryano, Phys.

Rev. Research 2, 043246 (2020).

[161] R. Wiersema, C. Zhou, Y. de Sereville, J. F. Car-
and H. Yuen, PRX Quantum

rasquilla, Y. B. Kim,
1, 020319 (2020).

[162] M. S. L. du Croo de Jongh and J. M. J. van Leeuwen,

Phys. Rev. B 57, 8494 (1998).

[163] H. W. J. Bl√∂te and Y. Deng, Phys. Rev. E 66, 066110

(2002).

[164] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
in Advances in neural information processing systems
(2014) pp. 2672‚Äì2680.

[165] P.-L. Dallaire-Demers and N. Killoran, Phys. Rev. A 98,

012324 (2018).

[166] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, Nature (London) 549, 195

56

(2017), arXiv:1611.09347 [quant-ph].

[167] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A.
Buell, et al., arXiv:1910.11333 (2019).

[168] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, A. Bengtsson, S. Boixo, M. Broughton,
B. B. Buckley, et al., arXiv preprint arXiv:2010.07965
(2020).

[169] N. Klco and M. J. Savage, Phys. Rev. A 102, 012612

(2020).

[170] R. S. Sutton, A. G. Barto, et al., Introduction to re-
inforcement learning, Vol. 135 (MIT press Cambridge,
1998).

[171] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-
ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, et al., nature 518, 529 (2015).
[172] P. Mirowski, M. K. Grimes, M. Malinowski, K. M.
Hermann, K. Anderson, D. Teplyashin, K. Simonyan,
K. Kavukcuoglu, A. Zisserman, and R. Hadsell, arXiv
preprint arXiv:1804.00168 (2018).

[173] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,
A. Bolton, et al., nature 550, 354 (2017).

[174] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Math-
ieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, et al., Nature 575, 350 (2019).
[175] C. Berner, G. Brockman, B. Chan, V. Cheung,
P. Dƒôbiak, C. Dennison, D. Farhi, Q. Fischer,

S. Hashme, C. Hesse, et al., ‚ÄúDota 2 with large scale
deep reinforcement learning,‚Äù (2019), arXiv:1912.06680
[cs.LG].

[176] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and
V. Dunjko, ‚ÄúVariational quantum policies for reinforce-
ment learning,‚Äù (2021), arXiv:2103.05577 [quant-ph].

[177] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma,
and H.-S. Goan, IEEE Access 8, 141007 (2020).
[178] O. Lockwood and M. Si, in Proceedings of the AAAI
Conference on ArtiÔ¨Åcial Intelligence and Interactive
Digital Entertainment, Vol. 16 (2020) pp. 245‚Äì251.
[179] A. Skolik, S. Jerbi, and V. Dunjko, ‚ÄúQuantum agents
in the gym: a variational quantum algorithm for deep
q-learning,‚Äù (2021), arXiv:2103.15084 [quant-ph].
[180] O. Lockwood and M. Si, in NeurIPS 2020 Workshop
on Pre-registration in Machine Learning (PMLR, 2021)
pp. 285‚Äì301.

[181] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,
J. Schulman, J. Tang, and W. Zaremba, ‚ÄúOpenai gym,‚Äù
(2016), arXiv:1606.01540 [cs.LG].

[182] M. Schuld, R. Sweke, and J. J. Meyer, Physical Review

A 103, 032430 (2021).

[183] A. P√©rez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and

J. I. Latorre, Quantum 4, 226 (2020).

[184] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,

et al., in NIPS, Vol. 99 (1999) pp. 1057‚Äì1063.

[185] S. Wu, S. Jin, D. Wen, and X. Wang, arXiv preprint

arXiv:2012.10711 (2020).

