TensorFlow Quantum:
A Software Framework for Quantum Machine Learning

Michael Broughton,1, 5, ∗ Guillaume Verdon,1, 2, 4, 6, † Trevor McCourt,1, 7 Antonio J. Martinez,1, 2, 4, 8
Jae Hyeon Yoo,2, 3 Sergei V. Isakov,1 Philip Massey,3 Ramin Halavati,3 Murphy Yuezhen Niu,1
Alexander Zlokapa,9, 1 Evan Peters,4, 6, 10 Owen Lockwood,11 Andrea Skolik,12, 13, 14, 15 Soﬁene Jerbi,16
Vedran Dunjko,13 Martin Leib,12 Michael Streif,12, 14, 15, 17 David Von Dollen,18 Hongxiang Chen,19, 20
Shuxiang Cao,19, 21 Roeland Wiersema,22, 23 Hsin-Yuan Huang,1, 24, 25 Jarrod R. McClean,1 Ryan
Babbush,1 Sergio Boixo,1 Dave Bacon,1 Alan K. Ho,1 Hartmut Neven,1 and Masoud Mohseni1, ‡

1Google Quantum AI, Mountain View, CA
2Sandbox@Alphabet, Mountain View, CA
3Google, Mountain View, CA
4Institute for Quantum Computing, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
5School of Computer Science, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
6Department of Applied Mathematics, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
7Department of Mechanical & Mechatronics Engineering,
University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
8Department of Physics & Astronomy, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada
9Division of Physics, Mathematics and Astronomy, Caltech, Pasadena, CA 91125
10Fermi National Accelerator Laboratory, P.O. Box 500, Batavia, IL, 605010
11Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
12Data:Lab, Volkswagen Group, Ungererstr. 69, 80805 München, Germany
13Leiden University, Niels Bohrweg 1, 2333 CA Leiden, Netherlands
14Quantum Artiﬁcial Intelligence Laboratory, NASA Ames Research Center (QuAIL)
15USRA Research Institute for Advanced Computer Science (RIACS)
16Institute for Theoretical Physics, University of Innsbruck, Technikerstr. 21a, A-6020 Innsbruck, Austria
17University Erlangen-Nürnberg (FAU), Institute of Theoretical Physics, Staudtstr. 7, 91058 Erlangen, Germany
18Volkswagen Group Advanced Technologies, San Francisco, CA 94108
19Rahko Ltd., Finsbury Park, London, N4 3JP, United Kingdom
20Department of Computer Science, University College London, WC1E 6BT, United Kingdom
21Clarendon Laboratory, Department of Physics, University of Oxford. Oxford, OX1 3PU, United Kingdom
22Vector Institute, MaRS Centre, Toronto, Ontario, M5G 1M1, Canada
23Department of Physics and Astronomy, University of Waterloo, Ontario, N2L 3G1, Canada
24Institute for Quantum Information and Matter, Caltech, Pasadena, CA, USA
25Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA, USA
(Dated: August 30, 2021)

We introduce TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of
hybrid quantum-classical models for classical or quantum data. This framework oﬀers high-level ab-
stractions for the design and training of both discriminative and generative quantum models under
TensorFlow and supports high-performance quantum circuit simulators. We provide an overview
of the software architecture and building blocks through several examples and review the theory of
hybrid quantum-classical neural networks. We illustrate TFQ functionalities via several basic appli-
cations including supervised learning for quantum classiﬁcation, quantum control, simulating noisy
quantum circuits, and quantum approximate optimization. Moreover, we demonstrate how one can
apply TFQ to tackle advanced quantum learning tasks including meta-learning, layerwise learning,
Hamiltonian learning, sampling thermal states, variational quantum eigensolvers, classiﬁcation of
quantum phase transitions, generative adversarial networks, and reinforcement learning. We hope
this framework provides the necessary tools for the quantum computing and machine learning re-
search communities to explore models of both natural and artiﬁcial quantum systems, and ultimately
discover new quantum algorithms which could potentially yield a quantum advantage.

1
2
0
2

g
u
A
6
2

]
h
p
-
t
n
a
u
q
[

2
v
9
8
9
2
0
.
3
0
0
2
:
v
i
X
r
a

CONTENTS

I. Introduction

3

∗ mbbrough@google.com
† gverdon@google.com
‡ mohseni@google.com

A. Quantum Machine Learning
B. Hybrid Quantum-Classical Models
C. Quantum Data
D. TensorFlow Quantum

II. Software Architecture & Building Blocks

A. Cirq
B. TensorFlow
C. Technical Hurdles in Combining Cirq with

3
3
4
5

5
5
6

 
 
 
 
 
 
TensorFlow

D. TFQ architecture

1. Design Principles and Overview
2. The Abstract TFQ Pipeline for a speciﬁc

hybrid discriminator model

3. Hello Many-Worlds: Binary Classiﬁer for

Quantum Data

E. TFQ Building Blocks

1. Quantum Computations as Tensors
2. Composing Quantum Models
3. Sampling and Expectation Values
4. Diﬀerentiating Quantum Circuits
5. Simpliﬁed Layers
6. Basic Quantum Datasets

F. High Performance Quantum Circuit

Simulation with qsim
1. Comment on the Simulation of Quantum

Circuits

2. Gate Fusion with qsim
3. Hardware-Acceleration
4. Benchmarks
5. Large-scale simulation for quantum

machine learning

6. Noise in qsim

III. Theory of Hybrid Quantum-Classical Machine

Learning
A. Quantum Neural Networks
B. Sampling and Expectations
C. Gradients of Quantum Neural Networks

1. Finite diﬀerence methods
2. Parameter shift methods
3. Stochastic Parameter Shift Gradient

Estimation

4. Adjoint Gradient Backpropagation in

Simulations

D. Hybrid Quantum-Classical Computational

Graphs
1. Hybrid Quantum-Classical Neural

Networks

E. Autodiﬀerentiation through hybrid
quantum-classical backpropagation

IV. Basic Quantum Applications

A. Hybrid Quantum-Classical Convolutional

Neural Network Classiﬁer
1. Background
2. Implementations

B. Hybrid Machine Learning for Quantum

Control
1. Time-Constant Hamiltonian Control
2. Time-dependent Hamiltonian Control

C. Simulating Noisy Circuits

1. Background
2. Noise in Cirq and TFQ
3. Training Models with Noise

D. Quantum Approximate Optimization

1. Background

6
7
7

8

9
10
11
11
11
12
12
13

13

13
13
14
14

15
15

15
15
16
17
17
17

18

18

20

20

21

22

22
22
23

25
26
27
28
28
28
29
30
30

2. Implementation

V. Advanced Quantum Applications

A. Meta-learning for Variational Quantum

Optimization

B. Vanishing Gradients and Adaptive Layerwise

Training Strategies
1. Random Quantum Circuits and Barren

Plateaus

2. Layerwise quantum circuit learning

C. Hamiltonian Learning with Quantum Graph

Recurrent Neural Networks
1. Motivation: Learning Quantum Dynamics

with a Quantum Representation

2. Implementation

D. Generative Modelling of Quantum Mixed

States with Hybrid Quantum-Probabilistic
Models
1. Background
2. Variational Quantum Thermalizer
3. Quantum Generative Learning from

Quantum Data

E. Subspace-Search Variational Quantum

Eigensolver for Excited States: Integration
with OpenFermion
1. Background
2. Implementation

F. Classiﬁcation of Quantum Many-Body Phase

2

31

31

32

33

33
34

35

35
36

38
38
39

41

41
41
42

Transitions
1. Background
2. Implementation

44
44
45
G. Quantum Generative Adversarial Networks 46
46

1. Background
2. Noise Suppression with Adversarial

Learning

3. Approximate Quantum Random Access

Memory

H. Reinforcement Learning with Parameterized

Quantum Circuits
1. Background
2. Policy-Gradient Reinforcement Learning

with PQCs
3. Implementation
4. Value-Based Reinforcement Learning with

PQCs

5. Quantum Environments

VI. Closing Remarks

VII. Acknowledgements

References

46

47

48
48

49
49

51
51

52

52

52

I.

INTRODUCTION

A. Quantum Machine Learning

Machine learning (ML) is the construction of algo-
rithms and statistical models which can extract infor-
mation hidden within a dataset. By learning a model
from a dataset, one then has the ability to make predic-
tions on unseen data from the same underlying probabil-
ity distribution. For several decades, research in machine
learning was focused on models that can provide theoret-
ical guarantees for their performance [1–4]. However, in
recent years, methods based on heuristics have become
dominant, partly due to an abundance of data and com-
putational resources [5].

Deep learning is one such heuristic method which has
seen great success [6, 7]. Deep learning methods are
based on learning a representation of the dataset in the
form of networks of parameterized layers. These parame-
ters are then tuned by minimizing a function of the model
outputs, called the loss function. This function quantiﬁes
the ﬁt of the model to the dataset.

In parallel to the recent advances in deep learning,
there has been a signiﬁcant growth of interest in quantum
computing in both academia and industry [8]. Quan-
tum computing is the use of engineered quantum sys-
tems to perform computations. Quantum systems are
described by a generalization of probability theory al-
lowing novel behavior such as superposition and entan-
glement, which are generally diﬃcult to simulate with
a classical computer [9]. The main motivation to build
a quantum computer is to access eﬃcient simulation of
these uniquely quantum mechanical behaviors. Quan-
tum computers could one day accelerate computations
for chemical and materials development [10], decryption
[11], optimization [12], and many other tasks. Google’s
recent achievement of quantum supremacy [13] marked
the ﬁrst glimpse of this promised power.

How may one apply quantum computing to practical
tasks? One area of research that has attracted consider-
able interest is the design of machine learning algorithms
that inherently rely on quantum properties to accelerate
their performance. One key observation that has led to
the application of quantum computers to machine learn-
ing is their ability to perform fast linear algebra on a
state space that grows exponentially with the number of
qubits. These quantum accelerated linear-algebra based
techniques for machine learning can be considered the
ﬁrst generation of quantum machine learning (QML) al-
gorithms tackling a wide range of applications in both su-
pervised and unsupervised learning, including principal
component analysis [14], support vector machines [15], k-
means clustering [16], and recommendation systems [17].
These algorithms often admit exponentially faster solu-
tions compared to their classical counterparts on certain
types of quantum data. This has led to a signiﬁcant
surge of interest in the subject [18]. However, to apply
these algorithms to classical data, the data must ﬁrst

3

be embedded into quantum states [19], a process whose
scalability is under debate [20]. Additionally, many com-
mon approaches for applying these algorithms to classical
data rely on speciﬁc structure in the data that can also be
exploited by classical algorithms, sometimes precluding
the possibility of a quantum speedup [21]. Tests based
on the structure of a classical dataset have recently been
developed that can sometimes determine if a quantum
speedup is possible on that data [22]. Continuing de-
bates around speedups and assumptions make it prudent
to look beyond classical data for applications of quantum
computation to machine learning.

With the availability of Noisy Intermediate-Scale
Quantum (NISQ) processors [23], the second generation
of QML has emerged [8, 12, 18, 24–44]. In contrast to
the ﬁrst generation, this new trend in QML is based on
heuristic methods which can be studied empirically due
to the increased computational capability of quantum
hardware. This is reminiscent of how machine learning
evolved towards deep learning with the advent of new
computational capabilities [45]. These new algorithms
use parameterized quantum transformations called pa-
rameterized quantum circuits (PQCs) or Quantum Neu-
ral Networks (QNNs) [24, 43]. In analogy with classical
deep learning, the parameters of a QNN are then opti-
mized with respect to a cost function via either black-box
optimization heuristics [46] or gradient-based methods
[47], in order to learn a representation of the training
data. In this paradigm, quantum machine learning is the
development of models, training strategies, and inference
schemes built on parameterized quantum circuits.

B. Hybrid Quantum-Classical Models

Near-term quantum processors are still fairly small and
noisy, thus quantum models cannot disentangle and gen-
eralize quantum data using quantum processors alone.
NISQ processors will need to work in concert with clas-
sical co-processors to become eﬀective. We anticipate
that investigations into various possible hybrid quantum-
classical machine learning algorithms will be a produc-
tive area of research and that quantum computers will
be most useful as hardware accelerators, working in sym-
biosis with traditional computers. In order to understand
the power and limitations of classical deep learning meth-
ods, and how they could be possibly improved by in-
corporating parameterized quantum circuits, it is worth
deﬁning key indicators of learning performance:

Representation capacity: the model architecture has
the capacity to accurately replicate, or extract useful in-
formation from, the underlying correlations in the train-
ing data for some value of the model’s parameters.

Training eﬃciency: minimizing the cost function via
stochastic optimization heuristics should converge to an
approximate minimum of the loss function in a reasonable
number of iterations.

Inference tractability: the ability to run inference on

a given model in a scalable fashion is needed in order to
make predictions in the training or test phase.

Generalization power : the cost function for a given
model should yield a landscape where typically initialized
and trained networks ﬁnd approximate solutions which
generalize well to unseen data.

In principle, any or all combinations of these at-
tributes could be susceptible to possible improvements
by quantum computation. There are many ways to com-
bine classical and quantum computations. One well-
known method is to use classical computers as outer-
loop optimizers for QNNs. When training a QNN with
a classical optimizer in a quantum-classical loop, the
overall algorithm is sometimes referred to as a Varia-
tional Quantum-Classical Algorithm. Some recently pro-
posed architectures of QNN-based variational quantum-
classical algorithms include Variational Quantum Eigen-
solvers (VQEs) [29, 48], Quantum Approximate Opti-
mization Algorithms (QAOAs) [12, 28, 49, 50], Quan-
tum Neural Networks (QNNs) for classiﬁcation [51, 52],
Quantum Convolutional Neural Networks (QCNN) [53],
and Quantum Generative Models [54]. Generally, the
goal is to optimize over a parameterized class of compu-
tations to either generate a certain low energy wavefunc-
tion (VQE/QAOA), learn to extract non-local informa-
tion (QNN classiﬁers), or learn how to generate a quan-
tum distribution from data (generative models).

It is important to note that in the standard model ar-
chitecture for these applications, the representation typ-
ically resides entirely on the quantum processor, with
classical heuristics participating only as optimizers for
the tunable parameters of the quantum model. One of
major obstacles in training of such quantum models is
known as barren plateaus
[52], which generally arises
when a network architecture is lacking any structure and
it is randomly initialized. This unusual ﬂat energy land-
scape of quantum models could seriously hinder the per-
formance of both gradient-based and gradient-free opti-
mization techniques [55]. We discuss various strategies
for overcoming this issue in section V B.

While the use of classical processors as outer-loop op-
timizers for quantum neural networks is promising, the
reality is that near-term quantum devices are still fairly
noisy, thus limiting the depth of quantum circuit achiev-
able with acceptable ﬁdelity. This motivates allowing as
much of the model as possible to reside on classical hard-
ware. Several applications of quantum computation have
ventured beyond the scope of typical variational quantum
algorithms to explore this combination. Instead of train-
ing a purely quantum model via a classical optimizer,
one then considers scenarios where the model itself is a
hybrid between quantum computational building blocks
and classical computational building blocks [56–59] and
is trained typically via gradient-based methods. Such
scenarios leverage a new form of automatic diﬀerentia-
tion that allows the backwards propagation of gradients
in between parameterized quantum and classical compu-
tations. The theory of such hybrid backpropagation will

4

be covered in section III C.

In summary, a hybrid quantum-classical model is a
learning heuristic in which both the classical and quan-
tum processors contribute to the indicators of learning
performance deﬁned above.

C. Quantum Data

Although it is not yet proven that heuristic QML can
provide a speedup on practical classical ML applications,
there is growing evidence that hybrid quantum-classical
machine learning applications on “quantum data” can
provide a quantum advantage over classical-only machine
learning for reasons described below. These results mo-
tivate the development of software frameworks that can
combine coherent access to quantum data with the power
of machine learning.

Abstractly, any data emerging from an underlying
quantum mechanical process can be considered quan-
tum data. This can be the classical data resulting from
quantum mechanical experiments [22], or data which is
directly generated by a quantum device and then fed
into an algorithm as input [60]. A quantum or hybrid
quantum-classical model will be at least partially repre-
sented by a quantum device, and therefore have the inher-
ent capacity to capture the characteristics of a quantum
mechanical process. Concretely, we list practical exam-
ples of classes of quantum data, which can be routinely
generated or simulated on existing quantum devices or
processors:

Quantum simulations: These can include output states
of quantum chemistry simulations used to extract in-
formation about chemical structures and chemical reac-
tions [61]. Potential applications include material sci-
ence, computational chemistry, computational biology,
and drug discovery. Another example is data from quan-
tum many-body systems and quantum critical systems in
condensed matter physics, which could be used to model
and design exotic states of matter which exhibit many-
body quantum eﬀects.

Quantum communication networks: Machine learn-
ing in this class of systems will be related to distilling
small-scale quantum data; e.g., to discriminate among
non-orthogonal quantum states [43, 62], with applica-
tion to design and construction of quantum error cor-
recting codes for quantum repeaters, quantum receivers,
and puriﬁcation units, devices which are critical for the
construction of a quantum internet [63].

Quantum metrology: Quantum-enhanced high preci-
sion measurements such as quantum sensing and quan-
tum imaging are inherently done on probes that are
small-scale quantum devices and could be designed or
improved by variational quantum models [64].

Quantum control :

Variationally learning hybrid
quantum-classical algorithms can lead to new optimal
open or closed-loop control [65], calibration, and error
mitigation, correction, and veriﬁcation strategies [66] for

near-term quantum devices and quantum processors.

Of course, this is not a comprehensive list of quantum
data. We hope that, with proper software tooling, re-
searchers will be able to ﬁnd applications of QML in all
of the above areas and other categories of applications
beyond what we can currently envision.

D. TensorFlow Quantum

Today, exploring new hybrid quantum-classical mod-
els is a diﬃcult and error-prone task. The engineering
eﬀort required to manually construct such models, de-
velop quantum datasets, and set up training and valida-
tion stages decreases a researcher’s ability to iterate and
discover. TensorFlow has accelerated the research and
understanding of deep learning in part by automating
common model building tasks. Development of software
tooling for hybrid quantum-classical models should simi-
larly accelerate research and understanding for quantum
machine learning.

To develop such tooling, the requirement of accommo-
dating a heterogeneous computational environment in-
volving both classical and quantum processors is key.
This computational heterogeneity suggested the need to
expand TensorFlow, which is designed to distribute com-
putations across CPUs, GPUs, and TPUs [67], to also en-
compass quantum processing units (QPUs). This project
has evolved into TensorFlow Quantum. TFQ is an inte-
gration of Cirq with TensorFlow that allows researchers
and students to simulate QPUs while designing, training,
and testing hybrid quantum-classical models, and eventu-
ally run the quantum portions of these models on actual
quantum processors as they come online. A core con-
tribution of TFQ is seamless backpropagation through
combinations of classical and quantum layers in hybrid
quantum-classical models. This allows QML researchers
to directly harness the rich set of tools already available
in TF and Keras.

The remainder of this document describes TFQ and a
selection of applications demonstrating some of the chal-
lenges TFQ can help tackle. In section II, we introduce
the software architecture of TFQ. We highlight its main
features including batched circuit execution, automated
expectation estimation, estimation of quantum gradients,
hybrid quantum-classical automatic diﬀerentiation, and
rapid model construction, all from within TensorFlow.
We also present a simple “Hello, World" example for bi-
nary quantum data classiﬁcation on a single qubit. By
the end of section II, we expect most readers to have suf-
ﬁcient knowledge to begin development with TFQ. For
readers who are interested in a more theoretical under-
standing of QNNs, we provide in section III an overview
of QNN models and hybrid quantum-classical backprop-
agation. For researchers interested in applying TFQ to
their own projects, we provide various applications in
sections IV and V. In section IV, we describe hybrid
quantum-classical CNNs for binary classiﬁcation of quan-

5

tum phases, hybrid quantum-classical ML for quantum
control, and MaxCut QAOA. In the advanced applica-
tions section V, we describe meta-learning for quantum
approximate optimization, discuss issues with vanishing
gradients and how we can overcome them by adaptive
layer-wise learning schemes, Hamiltonian learning with
quantum graph networks, quantum mixed state genera-
tion via classical energy-based models, subspace-Search
variational quantum eigensolver for excited states to il-
lustrate an integration with OpenFermion, quantum clas-
siﬁcation of quantum phase transitions, entangling quan-
tum generative adversarial networks, and quantum rein-
forcement learning.

We hope that TFQ enables the machine learning and
quantum computing communities to work together more
closely on important challenges and opportunities in the
near-term and beyond.

II. SOFTWARE ARCHITECTURE & BUILDING
BLOCKS

As stated in the introduction, the goal of TFQ is
to bridge the quantum computing and machine learn-
ing communities. Google already has well-established
products for these communities: Cirq, an open source
library for invoking quantum circuits [68], and Tensor-
Flow, an end-to-end open source machine learning plat-
form [67]. However, the emerging community of quantum
machine learning researchers requires the capabilities of
both products. The prospect of combining Cirq and Ten-
sorFlow then naturally arises.

First, we review the capabilities of Cirq and Tensor-
Flow. We confront the challenges that arise when one
attempts to combine both products. These challenges
inform the design goals relevant when building software
speciﬁc to quantum machine learning. We provide an
overview of the architecture of TFQ and describe a par-
ticular abstract pipeline for building a hybrid model for
classiﬁcation of quantum data. Then we illustrate this
pipeline via the exposition of a minimal hybrid model
which makes use of the core features of TFQ. We con-
clude with a description of our performant C++ sim-
ulator for quantum circuits and provide benchmarks of
performance on two complementary classes of dense and
sparse quantum circuits.

A. Cirq

Cirq is an open-source framework for invoking quan-
tum circuits on near term devices [68].
It contains
the basic structures, such as qubits, gates, circuits, and
measurement operators, that are required for specifying
quantum computations. User-speciﬁed quantum compu-
tations can then be executed in simulation or on real
hardware. Cirq also contains substantial machinery that
helps users design eﬃcient algorithms for NISQ machines,

such as compilers and schedulers. Below we show ex-
ample Cirq code for calculating the expectation value of
ˆZ1 ˆZ2 for a Bell state:

( q1 , q2 ) = cirq . GridQubit . rect (1 ,2)
c = cirq . Circuit ( cirq . H ( q1 ) , cirq . CNOT ( q1 , q2 ) )
ZZ = cirq . Z ( q1 ) * cirq . Z ( q2 )
bell = cirq . Simulator () . simulate ( c ) . final_state
expectation = ZZ . e x p e c t a t i o n _ f r o m _ w a v e f u n c t i o n (

bell , dict ( zip ([ q1 , q2 ] ,[0 ,1]) ) )

Cirq uses SymPy [69] symbols to represent free param-
eters in gates and circuits. You replace free parame-
ters in a circuit with speciﬁc numbers by passing a Cirq
ParamResolver object with your circuit to the simulator.
Below we construct a parameterized circuit and simulate
the output state for θ = 1:

theta = sympy . Symbol ( ’ theta ’)
c = cirq . Circuit ( cirq . Rx ( theta ) . on ( q1 ) )
resolver = cirq . ParamResolver ({ theta :1})
results = cirq . Simulator () . simulate (c , resolver )

B. TensorFlow

TensorFlow is a language for describing computations
as stateful dataﬂow graphs [67]. Describing machine
learning models as dataﬂow graphs is advantageous for
performance during training. First,
it is easy to ob-
tain gradients of dataﬂow graphs using backpropagation
[70], allowing eﬃcient parameter updates. Second, inde-
pendent nodes of the computational graph may be dis-
tributed across independent machines, including GPUs
and TPUs, and run in parallel. These computational ad-
vantages established TensorFlow as a powerful tool for
machine learning and deep learning.

TensorFlow constructs this dataﬂow graph using ten-
sors for the directed edges and operations (ops) for the
nodes. For our purposes, a rank n tensor is simply an n-
dimensional array. In TensorFlow, tensors are addition-
ally associated with a data type, such as integer or string.
Tensors are a convenient way of thinking about data; in
machine learning, the ﬁrst index is often reserved for iter-
ation over the members of a dataset. Additional indices
can indicate the application of several ﬁlters, e.g., in con-
volutional neural networks with several feature maps.

In general, an op is a function mapping input tensors
to output tensors. Ops may act on zero or more input
tensors, always producing at least one tensor as output.
For example, the addition op ingests two tensors and out-
puts one tensor representing the elementwise sum of the
inputs, while a constant op ingests no tensors, taking the
role of a root node in the dataﬂow graph. The combina-
tion of ops and tensors gives the backend of TensorFlow
the structure of a directed acyclic graph. A visualiza-
tion of the backend structure corresponding to a simple
computation in TensorFlow is given in Fig. 1.

6

Figure 1. A simple example of the TensorFlow computational
model. Two tensor inputs A and B are added and then mul-
tiplied against a third tensor input C, before ﬂowing on to
further nodes in the graph. Blue nodes are tensor injections
(ops), arrows are tensors ﬂowing through the computational
graph, and orange nodes are tensor transformations (ops).
Tensor injections are ops in the sense that they are functions
which take in zero tensors and output one tensor.

It is worth noting that this tensorial data format is
not to be confused with Tensor Networks [71], which are
a mathematical tool used in condensed matter physics
and quantum information science to eﬃciently represent
many-body quantum states and operations. Recently, li-
braries for building such Tensor Networks in TensorFlow
have become available [72], we refer the reader to the
corresponding blog post for better understanding of the
diﬀerence between TensorFlow tensors and the tensor ob-
jects in Tensor Networks [73].

The recently announced TensorFlow 2 [74] takes the
dataﬂow graph structure as a foundation and adds high-
level abstractions. One new feature is the Python func-
tion decorator @tf.function , which automatically con-
verts the decorated function into a graph computation.
Also relevant is the native support for Keras [75], which
provides the Layer and Model constructs. These ab-
stractions allow the concise deﬁnition of machine learn-
ing models which ingest and process data, all backed by
dataﬂow graph computation. The increasing levels of ab-
straction and heterogenous hardware backing which to-
gether constitute the TensorFlow stack can be visualized
with the orange and gray boxes in our stack diagram in
Fig. 4. The combination of these high-level abstractions
and eﬃcient dataﬂow graph backend makes TensorFlow
2 an ideal platform for data-driven machine learning re-
search.

C. Technical Hurdles in Combining Cirq with
TensorFlow

There are many ways one could imagine combining the
capabilities of Cirq and TensorFlow. One possible ap-
proach is to let graph edges represent quantum states and
let ops represent transformations of the state, such as ap-
plying circuits and taking measurements. This approach
can be called the “states-as-edges" architecture. We show
in Fig. 2 how to reformulate the Bell state preparation
and measurement discussed in section II A within this
proposed architecture.

7

Figure 2. The states-as-edges approach to embedding quan-
tum computation in TensorFlow. Blue nodes are input ten-
sors, arrows are tensors ﬂowing through the graph, and orange
nodes are TF Ops transforming the simulated quantum state.
Note that the above is not the architecture used in TFQ but
rather an alternative which was considered, see Fig. 3 for the
equivalent diagram for the true TFQ architecture.

This architecture may at ﬁrst glance seem like an at-
tractive option as it is a direct formulation of quantum
computation as a dataﬂow graph. However, this ap-
proach is suboptimal for several reasons. First, in this
architecture, the structure of the circuit being run is
static in the computational graph, thus running a diﬀer-
ent circuit would require the graph to be rebuilt. This is
far from ideal for variational quantum algorithms which
learn over many iterations with a slightly modiﬁed quan-
tum circuit at each iteration. A second problem is the
lack of a clear way to embed such a quantum dataﬂow
graph on a real quantum processor: the states would
have to remain held in quantum memory on the quan-
tum device itself, and the high latency between classi-
cal and quantum processors makes sending transforma-
tions one-by-one prohibitive. Lastly, one needs a way
to specify gates and measurements within TF. One may
be tempted to deﬁne these directly; however, Cirq al-
ready has the necessary tools and objects deﬁned which
are most relevant for the near-term quantum computing
era. Duplicating Cirq functionality in TF would lead to
several issues, requiring users to re-learn how to inter-
face with quantum computers in TFQ versus Cirq, and
adding to the maintenance overhead by needing to keep
two separate quantum circuit construction frameworks
up-to-date as new compilation techniques arise. These
considerations motivate our core design principles.

D. TFQ architecture

1. Design Principles and Overview

To avoid the aforementioned technical hurdles and in
order to satisfy the diverse needs of the research commu-
nity, we have arrived at the following four design princi-
ples:

1. Diﬀerentiability. As described in the introduc-
tion, gradient-based methods leveraging autodiﬀer-
entiation have become the leading heuristic for op-
timization of machine learning models. A software
framework for QML must support diﬀerentiation of
quantum circuits so that hybrid quantum-classical
models can participate in backpropagation.

2. Circuit Batching. Learning on quantum data re-
quires re-running parameterized model circuits on

Figure 3. The TensorFlow graph generated to calculate the
expectation value of a parameterized circuit. The symbol
values can come from other TensorFlow ops, such as from
the outputs of a classical neural network. The output can be
passed on to other ops in the graph; here, for illustration, the
output is passed to the absolute value op.

each quantum data point. A QML software frame-
work must be optimized for running large num-
bers of such circuits. Ideally, the semantics should
match established TensorFlow norms for batching
over data.

3. Execution Backend Agnostic.

Experimen-
tal quantum computing often involves reconciling
perfectly simulated algorithms with the outputs of
real, noisy devices. Thus, QML software must al-
low users to easily switch between running models
in simulation and running models on real hardware,
such that simulated results and experimental re-
sults can be directly compared.

4. Minimalism. Cirq provides an extensive set of
tools for preparing quantum circuits. TensorFlow
provides a very complete machine learning toolkit
through its hundreds of ops and Keras high-level
API, with a massive community of active users. Ex-
isting functionality in Cirq and TensorFlow should
be used as much as possible. TFQ should serve as a
bridge between the two that does not require users
to re-learn how to interface with quantum comput-
ers or re-learn how to solve problems using machine
learning.

First, we provide a bottom-up overview of TFQ to
provide intuition on how the framework functions at a
fundamental level. In TFQ, circuits and other quantum
computing constructs are tensors, and converting these
tensors into classical information via simulation or exe-
cution on a quantum device is done by ops. These ten-
sors are created by converting Cirq objects to TensorFlow
string tensors, using the tfq.convert_to_tensor function.
This takes in a cirq.Circuit or cirq.PauliSum object and
creates a string tensor representation. The cirq.Circuit
objects may be parameterized by SymPy symbols.

These tensors are then converted to classical informa-
tion via state simulation, expectation value calculation,
or sampling. TFQ provides ops for each of these compu-
tations. The following code snippet shows how a simple
parameterized circuit may be created using Cirq, and
its ˆZ expectation evaluated at diﬀerent parameter values

using the tfq expectation value calculation op. We feed
the output into the tf.math.abs op to show that tfq ops
integrate naively with tf ops.

qubit = cirq . GridQubit (0 , 0)
theta = sympy . Symbol ( ’ theta ’)
c = cirq . Circuit ( cirq . X ( qubit ) ** theta )
c_tensor = tfq . con ve rt_ to_ ten sor ([ c ] * 3)
theta_values = tf . constant ([[0] ,[1] ,[2]])
m = cirq . Z ( qubit )
paulis = tfq . con ver t_t o_t en sor ([ m ] * 3)
exp ectati on_op = tfq . g et _ ex p ec t at i on _o p ()
output = e xpectation_op (

c_tensor , [ ’ theta ’] , theta_values , paulis )

abs_output = tf . math . abs ( output )

We supply the expectation op with a tensor of parame-
terized circuits, a list of symbols contained in the circuits,
a tensor of values to use for those symbols, and tensor
operators to measure with respect to. Given this, it out-
puts a tensor of expectation values. The graph this code
generates is given by Fig. 3.

The expectation op is capable of running circuits on
a simulated backend, which can be a Cirq simulator or
our native TFQ simulator qsim (described in detail in
section II F), or on a real device. This is conﬁgured on
instantiation.

The expectation op is fully diﬀerentiable. Given
that there are many ways to calculate the gradient of
a quantum circuit with respect to its input parame-
ters, TFQ allows expectation ops to be conﬁgured with
one of many built-in diﬀerentiation methods using the
tfq.Differentiator interface, such as ﬁnite diﬀerencing,
parameter shift rules, and various stochastic methods.
The tfq.Differentiator interface also allows users to de-
ﬁne their own gradient calculation methods for their spe-
ciﬁc problem if they desire.

The tensor representation of circuits and Paulis along
with the execution ops are all that are required to solve
any problem in QML. However, as a convenience, TFQ
provides an additional op for in-graph circuit construc-
tion. This was found to be convenient when solving prob-
lems where most of the circuit being run is static and
only a small part of it is being changed during train-
ing or inference. This functionality is provided by the
tfq.tfq_append_circuit op.
It is expected that all but
the most dedicated users will never touch these low-
level ops, and instead will interface with TFQ using our
tf.keras.layers that provide a simpliﬁed interface.

The tools provided by TFQ can interact with both core
TensorFlow and, via Cirq, real quantum hardware. The
functionality of all three software products and the inter-
faces between them can be visualized with the help of a
“software-stack" diagram, shown in Fig. 4. Importantly,
these interfaces allow users to write a single TensorFlow
Quantum program which could then easily be run locally
on a workstation, in a highly parallel and distributed set-
ting at the PetaFLOP/s or higher throughput scale [22],
or on real QPU device [76].

In the next section, we describe an example of an
abstract quantum machine learning pipeline for hybrid

8

Figure 4. The software stack of TFQ, showing its interactions
with TensorFlow, Cirq, and computational hardware. At the
top of the stack is the data to be processed. Classical data
is natively processed by TensorFlow; TFQ adds the ability to
process quantum data, consisting of both quantum circuits
and quantum operators. The next level down the stack is the
Keras API in TensorFlow. Since a core principle of TFQ is
native integration with core TensorFlow, in particular with
Keras models and optimizers, this level spans the full width
of the stack. Underneath the Keras model abstractions are
our quantum layers and diﬀerentiators, which enable hybrid
quantum-classical automatic diﬀerentiation when connected
with classical TensorFlow layers. Underneath the layers and
diﬀerentiators, we have TensorFlow ops, which instantiate the
dataﬂow graph. Our custom ops control quantum circuit ex-
ecution. The circuits can be run in simulation mode, by in-
voking qsim or Cirq, or eventually will be executed on QPU
hardware.

discriminator model that TFQ was designed to support.
Then we illustrate the TFQ pipeline via a Hello Many-
Worlds example, which involves building the simplest
possible hybrid quantum-classical model for a binary
classiﬁcation task on a single qubit. More detailed in-
formation on the building blocks of TFQ features will be
given in section II E.

2. The Abstract TFQ Pipeline for a speciﬁc hybrid
discriminator model

Here, we provide a high-level abstract overview of the
computational steps involved in the end-to-end pipeline
for inference and training of a hybrid quantum-classical
discriminative model for quantum data in TFQ.

(1) Prepare Quantum Dataset: In general, this
might come from a given black-box source. However,
as current quantum computers cannot import quantum
data from external sources, the user has to specify quan-
tum circuits which generate the data. Quantum datasets
are prepared using unparameterized cirq.Circuit ob-
jects and are injected into the computational graph using
tfq.convert_to_tensor .

(2) Evaluate Quantum Model: Parameterized
quantum models can be selected from several categories

TF Keras ModelsTF Layers TF Execution Engine       TPUCirq TFQ OpsTFQ LayersTFQ DifferentiatorsTFQTensorFlowClassical hardwareQuantum hardwareTFQ qsimGPUCPUQPUTF OpsClassical Data: integers/ﬂoats/stringsQuantum Data: Circuits/Operators9

is unsupervised. Wrapping the model built in stages (1)
through (4) inside a tf.keras.Model gives the user access
to all the losses in the tf.keras.losses module.

(6) Evaluate Gradients & Update Parameters:
After evaluating the cost function, the free parame-
ters in the pipeline is updated in a direction expected
to decrease the cost. This is most commonly per-
formed via gradient descent. To support gradient de-
scent, TFQ exposes derivatives of quantum operations
to the TensorFlow backpropagation machinery via the
tfq.differentiators.Differentiator interface. This allows
both the quantum and classical models’ parameters to
be optimized against quantum data via hybrid quantum-
classical backpropagation. See section III for details on
the theory.

In the next section, we illustrate this abstract pipeline
by applying it to a speciﬁc example. While simple, the
example is the minimum instance of a hybrid quantum-
classical model operating on quantum data.

3. Hello Many-Worlds: Binary Classiﬁer for Quantum
Data

Binary classiﬁcation is a basic task in machine learn-
ing that can be applied to quantum data as well. As a
minimal example of a hybrid quantum-classical model,
we present here a binary classiﬁer for regions on a sin-
gle qubit. In this task, two random vectors in the X-Z
plane of the Bloch sphere are chosen. Around these two
vectors, we randomly sample two sets of quantum data
points; the task is to learn to distinguish the two sets. An
example quantum dataset of this type is shown in Fig. 6.
The following can all be run in-browser by navigating to
the Colab example notebook at

research/binary_classifier/binary_classifier.ipynb

Additionally, the code in this example can be copy-pasted
into a python script after installing TFQ.

To solve this problem, we use the pipeline shown in
Fig. 5, specialized to one-qubit binary classiﬁcation. This
specialization is shown in Fig. 7.

The ﬁrst step is to generate the quantum data. We can
use Cirq for this task. The common imports required for
working with TFQ are shown below:

import cirq , random , sympy
import numpy as np
import tensorflow as tf
import t e ns o rf l ow _ qu a nt u m as tfq

The function below generates the quantum dataset; la-
bels use a one-hot encoding:

def generate_dataset (

qubit , theta_a , theta_b , num_samples ) :

q_data = []
labels = []
blob_size = abs ( theta_a - theta_b ) / 5
for _ in range ( num_samples ) :

coin = random . random ()
spread_x , spread_y = np . random . uniform (

Figure 5. Abstract pipeline for inference and training of
a hybrid discriminative model in TFQ. Here, Φ represents
the quantum model parameters and θ represents the classical
model parameters.

based on knowledge of the quantum data’s structure.
The goal of the model is to perform a quantum compu-
tation in order to extract information hidden in a quan-
tum subspace or subsystem. In the case of discrimina-
tive learning, this information is the hidden label pa-
rameters. To extract a quantum non-local subsystem,
the quantum model disentangles the input data, leaving
the hidden information encoded in classical correlations,
thus making it accessible to local measurements and clas-
sical post-processing. Quantum models are constructed
using cirq.Circuit objects containing SymPy symbols,
and can be attached to quantum data sources using the
tfq.AddCircuit layer.

(3) Sample or Average: Measurement of quantum
states extracts classical information, in the form of sam-
ples from a classical random variable. The distribution
of values from this random variable generally depends on
both the quantum state itself and the measured observ-
able. As many variational algorithms depend on mean
values of measurements, TFQ provides methods for av-
eraging over several runs involving steps (1) and (2).
Sampling or averaging are performed by feeding quan-
tum data and quantum models to the tfq.Sample or
tfq.Expectation layers.

(4) Evaluate Classical Model: Once classical
information has been extracted,
it is in a format
amenable to further classical post-processing. As the
extracted information may still be encoded in classi-
cal correlations between measured expectations, clas-
sical deep neural networks can be applied to distill
such correlations. Since TFQ is fully compatible with
core TensorFlow, quantum models can be attached di-
rectly to classical tf.keras.layers.Layer objects such as
tf.keras.layers.Dense .

(5) Evaluate Cost Function: Given the results of
classical post-processing, a cost function is calculated.
This may be based on the accuracy of classiﬁcation if the
quantum data was labeled, or other criteria if the task

Prepare  Quantum DatasetEvaluate Quantum ModelSample or AverageEvaluate Classical ModelEvaluate Cost Function𝛉Evaluate Gradients & Update Parameters10

theta = sympy . Symbol ( ’ theta ’)
q_model = cirq . Circuit ( cirq . Ry ( theta ) ( qubit ) )
q_data_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

expectation = tfq . layers . PQC (

q_model , cirq . Z ( qubit ) )

e xp e ct a ti o n_ o ut p ut = expectation ( q_data_input )

The purpose of the rotation gate is to minimize the su-
perposition from the input quantum data such that we
can get maximum useful information from the measure-
ment. This quantum model is then attached to a small
classiﬁer NN to complete our hybrid model. Notice in
the code below that quantum layers can appear among
classical layers inside a standard Keras model:

classifier = tf . keras . layers . Dense (

2 , activation = tf . keras . activations . softmax )

cl ass ifi er _ou tpu t = classifier (

e xp e ct a ti o n_ o ut p ut )

model = tf . keras . Model ( inputs = q_data_input ,

outputs = c las sif ier _ou tp ut )

We can train this hybrid model on the quantum data
deﬁned earlier. Below we use as our loss function the
cross entropy between the labels and the predictions of
the classical NN; the ADAM optimizer is chosen for pa-
rameter updates.

optimizer = tf . keras . optimizers . Adam (

learning_rate =0.1)

loss = tf . keras . losses . C a t e g o r i c a l C r o s s e n t r o p y ()
model . compile ( optimizer = optimizer , loss = loss )
history = model . fit (

x = q_data , y = labels , epochs =50)

Finally, we can use our trained hybrid model to classify
new quantum datapoints:

test_data , _ = generate_dataset (

qubit , theta_a , theta_b , 1)
p = model . predict ( test_data ) [0]
print ( f " prob ( a ) ={ p [0]:.4 f } , prob ( b ) ={ p [1]:.4 f } " )

This section provided a rapid introduction to just that
code needed to complete the task at hand. The follow-
ing section reviews the features of TFQ in a more API
reference inspired style.

E. TFQ Building Blocks

Having provided a minimum working example in the
previous section, we now seek to provide more details
about the components of the TFQ framework. First, we
describe how quantum computations speciﬁed in Cirq are
converted to tensors for use inside the TensorFlow graph.
Then, we describe how these tensors can be combined in-
graph to yield larger models. Next, we show how circuits
are simulated and measured in TFQ. The core functional-
ity of the framework, diﬀerentiation of quantum circuits,
is then explored. Finally, we describe our more abstract
layers, which can be used to simplify many QML work-
ﬂows.

Figure 6. Quantum data represented on the Bloch sphere.
States in category a are blue, while states in category b are
orange. The vectors are the states around which the samples
were taken. The parameters used to generate this data are:
θa = 1, θb = 4, and N = 200. The QuTiP package was used
to visualize the Bloch sphere [77].

(1) Quantum data to be classiﬁed. (2) Parame-
Figure 7.
terized rotation gate, whose job is to remove superpositions
in the quantum data. (3) Measurement along the Z axis of
the Bloch sphere converts the quantum data into classical
data. (4) Classical post-processing is a two-output SoftMax
layer, which outputs probabilities for the data to come from
category a or category b.
(5) Categorical cross entropy is
computed between the predictions and the labels. The Adam
optimizer [78] is used to update both the quantum and clas-
sical portions of the hybrid model.

- blob_size , blob_size , 2)

if coin < 0.5:

label = [1 , 0]
angle = theta_a + spread_y

else :

label = [0 , 1]
angle = theta_b + spread_y

labels . append ( label )
q_data . append ( cirq . Circuit (
cirq . Ry ( - angle ) ( qubit ) ,
cirq . Rx ( - spread_x ) ( qubit ) ) )

return ( tfq . c onv ert _t o_t ens or ( q_data ) ,

np . array ( labels ) )

We can generate a dataset and the associated labels after
picking some parameter values:

qubit = cirq . GridQubit (0 , 0)
theta_a = 1
theta_b = 4
num_samples = 200
q_data , labels = generate_dataset (

qubit , theta_a , theta_b , num_samples )

As our quantum parametric model, we use the simplest
case of a universal quantum discriminator [43, 66], a
single parameterized rotation (linear) and measurement
along the Z axis (non-linear):

11

1. Quantum Computations as Tensors

3. Sampling and Expectation Values

As pointed out in section II A, Cirq already contains
the language necessary to express quantum computa-
tions, parameterized circuits, and measurements. Guided
by principle 4, TFQ should allow direct injection of Cirq
expressions into the computational graph of TensorFlow.
This is enabled by the tfq.convert_to_tensor function.
We saw the use of this function in the quantum binary
classiﬁer, where a list of data generation circuits spec-
iﬁed in Cirq was wrapped in this function to promote
them to tensors. Below we show how a quantum data
point, a quantum model, and a quantum measurement
can be converted into tensors:

q0 = cirq . GridQubit (0 , 0)
q_data_raw = cirq . Circuit ( cirq . H ( q0 ) )
q_data = tfq . con ver t_t o_t en sor ([ q_data_raw ])

theta = sympy . Symbol ( ’ theta ’)
q_model_raw = cirq . Circuit (

cirq . Ry ( theta ) . on ( q0 ) )

q_model = tfq . co nve rt_ to _te nso r ([ q_model_raw ])

q_measure_raw = 0.5 * cirq . Z ( q0 )
q_measure = tfq . co nve rt_ to_ ten sor (

[ q_measure_raw ])

This conversion is backed by our custom serializers. Once
a Circuit or PauliSum is serialized, it becomes a ten-
sor of type tf.string . This is the reason for the use
of tf.keras.Input(shape=(), dtype=tf.dtypes.string) when
creating inputs to Keras models, as seen in the quantum
binary classiﬁer example.

2. Composing Quantum Models

After injecting quantum data and quantum mod-
els into the computational graph, a custom Tensor-
Flow operation is required to combine them.
In
support of guiding principle 2, TFQ implements the
tfq.layers.AddCircuit layer for combining tensors of cir-
cuits.
In the following code, we use this functionality
to combine the quantum data point and quantum model
deﬁned in subsection II E 1:

add_op = tfq . layers . AddCircuit ()
dat a_and_ model = add_op ( q_data , append = q_model )

To quantify the performance of a quantum model on a
quantum dataset, we need the ability to deﬁne loss func-
tions. This requires converting quantum information into
classical information. This conversion process is accom-
plished by either sampling the quantum model, which
stochastically produces bitstrings according to the prob-
ability amplitudes of the model, or by specifying a mea-
surement and taking expectation values.

Sampling from quantum circuits is an important use
case in quantum computing. The recently achieved mile-
stone of quantum supremacy [13] is one such application,
in which the diﬃculty of sampling from a quantum model
was used to gain a computational edge over classical ma-
chines.

TFQ implements

tfq.layers.Sample , a Keras layer
which enables sampling from batches of circuits in sup-
port of design objective 2. The user supplies a ten-
sor of parameterized circuits, a list of symbols con-
tained in the circuits, and a tensor of values to sub-
stitute for the symbols in the circuit. Given these,
the Sample layer produces a tf.RaggedTensor of shape
[batch_size, num_samples, n_qubits] , where the n_qubits
dimension is ragged to account for the possibly varying
circuit size over the input batch of quantum data. For
example, the following code takes the combined data and
model from section II E 2 and produces a tensor of size
[1, 4, 1] containing four single-bit samples:

sample_layer = tfq . layers . Sample ()
samples = sample_layer (

data_and_model , symbol_names =[ ’ theta ’] ,
symbol_values =[[0.5]] , repetitions =4)

Though sampling is the fundamental interface between
quantum and classical information, diﬀerentiability of
quantum circuits is much more convenient when using
expectation values, as gradient information can then
be backpropagated (see section III for more details).

In the simplest case, expectation values are simply av-
erages over samples. In quantum computing, expectation
values are typically taken with respect to a measurement
operator M . This involves sampling bitstrings from the
quantum circuit as described above, applying M to the
list of bitstring samples to produce a list of numbers,
then taking the average of the result. TFQ provides two
related layers with this capability:

In contrast to sampling (which is by default in the stan-
dard computational basis, the ˆZ eigenbasis of all qubits),
taking expectation values requires deﬁning a measure-
ment. As discussed in section II A, these are ﬁrst deﬁned
as cirq.PauliSum objects and converted to tensors. TFQ
implements tfq.layers.Expectation , a Keras layer which
enables the extraction of measurement expectation val-
ues from quantum models. The user supplies a tensor of
parameterized circuits, a list of symbols contained in the
circuits, a tensor of values to substitute for the symbols
in the circuit, and a tensor of operators to measure with
respect to them. Given these inputs, the layer outputs
a tensor of expectation values. Below, we show how to
take an expectation value of the measurement deﬁned in
section II E 1:

ex pec tat io n_l aye r = tfq . layers . Expectation ()
expectations = e xpe cta tio n_l aye r (
circuit = data_and_model ,
symbol_names = [ ’ theta ’] ,

symbol_values = [[0.5]] ,
operators = q_measure )

In Fig. 3, we illustrate the dataﬂow graph which backs
the expectation layer, when the parameter values are sup-
plied by a classical neural network. The expectation layer
is capable of using either a simulator or a real device for
execution, and this choice is simply speciﬁed at run time.
While Cirq simulators may be used for the backend, TFQ
provides its own native TensorFlow simulator written in
performant C++. A description of our quantum circuit
simulation code is given in section II F.

Having converted the output of a quantum model into
classical information, the results can be fed into subse-
quent computations. In particular, they can be fed into
functions that produce a single number, allowing us to
deﬁne loss functions over quantum models in the same
way we do for classical models.

4. Diﬀerentiating Quantum Circuits

We have taken the ﬁrst steps towards implementation
of quantum machine learning, having deﬁned quantum
models over quantum data and loss functions over those
models. As described in both the introduction and our
ﬁrst guiding principle, diﬀerentiability is the critical ma-
chinery needed to allow training of these models. As de-
scribed in section II B, the architecture of TensorFlow is
optimized around backpropagation of errors for eﬃcient
updates of model parameters; one of the core contribu-
tions of TFQ is integration with TensorFlow’s backprop-
agation mechanism. TFQ implements this functionality
with our diﬀerentiators module. The theory of quan-
tum circuit diﬀerentiation will be covered in section III C;
here, we overview the software that implements the the-
ory.

of

and

Since

dients

interface.
layers

there are many ways

tfq.differentiators.Differentiator

quantum circuits, TFQ provides

to calculate gra-
the
Our
rely on
Expectation
SampledExpectation
classes inheriting from this interface to specify how
TensorFlow should compute their gradients. While
advanced users can implement their own custom diﬀer-
entiators by inheriting from the interface, TFQ comes
with several built-in options, two of which we highlight
here. These two methods are instances of the two main
categories of quantum circuit diﬀerentiators: the ﬁnite
diﬀerence methods and the parameter shift methods.

12

For the 2-local circuits implementable on near-term
hardware, methods more sophisticated than ﬁnite diﬀer-
ences are possible. These methods involve running an
ancillary quantum circuit, from which the gradient of the
primary circuit with respect to some parameter can be
directly measured. One speciﬁc method is gate decompo-
sition and parameter shifting [79], implemented in TFQ
as the ParameterShift diﬀerentiator. For in-depth discus-
sion of the theory, see section III C 2.

The diﬀerentiation rule used by our layers is speciﬁed
through an optional keyword argument. Below, we show
the expectation layer being called with our parameter
shift rule:

diff = tfq . differentiators . ParameterShift ()
expectation = tfq . layers . Expectation (

differentiator = diff )

For further discussion of the trade-oﬀs when choosing
between diﬀerentiators, see the gradients tutorial on our
GitHub website:

docs/tutorials/gradients.ipynb

5. Simpliﬁed Layers

Some workﬂows do not require control as sophisticated
as our Expectation , Sample , and SampledExpectation lay-
ers allow. For these workﬂows we provide the PQC and
ControlledPQC layers. Both of these layers allow parame-
terized circuits to be updated by hybrid backpropagation
without the user needing to provide the list of symbols
associated with the circuit. The PQC layer provides au-
tomated Keras management of the variables in a param-
eterized circuit:

q = cirq . GridQubit (0 , 0)
(a , b ) = sympy . symbols ( " a b " )
circuit = cirq . Circuit (

cirq . Rz ( a ) ( q ) , cirq . Rx ( b ) ( q ) )

outputs = tfq . layers . PQC ( circuit , cirq . Z ( q ) )
quantum_data = tfq . c onv ert _to _te nso r ([

cirq . Circuit () , cirq . Circuit ( cirq . X ( q ) ) ])

res = outputs ( quantum_data )

When the variables in a parameterized circuit will be
controlled completely by other user-speciﬁed machinery,
for example by a classical neural network, then the user
can call our ControlledPQC layer:

The ﬁrst class of quantum circuit diﬀerentiators is the
ﬁnite diﬀerence methods. This class samples the primary
quantum circuit for at least two diﬀerent parameter set-
tings, then combines them to estimate the derivative.
The ForwardDifference diﬀerentiator provides most ba-
sic version of this. For each parameter in the circuit, the
circuit is sampled at the current setting of the parame-
ter. Then, each parameter is perturbed separately and
the circuit resampled.

outputs = tfq . layers . ControlledPQC (

circuit , cirq . Z ( bit ) )

model_params = tf . co nve rt_ to_ ten so r (

[[0.5 , 0.5] , [0.25 , 0.75]])

res = outputs ([ quantum_data , model_params ])

Notice that the call is similar to that for PQC, except
that we provide parameter values for the symbols in the
circuit. These two layers are used extensively in the ap-
plications highlighted in the following sections.

13

6. Basic Quantum Datasets

F. High Performance Quantum Circuit Simulation
with qsim

A major goal of TensorFlow Quantum is to expand
the application of machine learning to quantum data.
Towards that goal, here we provide some basic labelled
datasets with the tfq.datasets module.

The ﬁrst dataset is tfq.datasets.excited_cluster_states .
Given a list of qubits, this function builds a dataset of
ground state and excited cluster states. The ground
state is labelled with −1, while excited states are labelled
with +1. With this data, a QML model can be trained
to distinguish between the ground and excited states
on a ring. This is the same dataset used in the QCNN
tutorial IV A.

The second dataset oﬀered is tfq.datasets.tfi_chain .
This is a 1D Transverse ﬁeld Ising-model, which can be
written as

ˆH = −

(cid:88)

( ˆZj ˆZj+1 − g ˆXj).

j

This dataset contains 81 datapoints, corresponding to
the ground states of the 1D TFI chain for g in [0.2, 1.8]
in increments of 0.2. Each datapoint contains a circuit,
a label, a Hamiltonian, and some additional metadata.
The circuit prepares the approximate ground state of the
Hamiltonian in the datapoint.

This dataset can be used for many purposes. For one,
the labels can be used to train a QML model to distin-
guish diﬀerent phases of a chain. The labels are 0 for the
ferromagnetic phase (occurs for g < 1), 1 for the crit-
ical point (g == 1) and 2 for the paramagnetic phase
(g > 1). Further, the additional metadata contains the
exact ground state energy from expensive exact diago-
nalization; this can be used as a benchmark for VQE-like
models. If a QML model is successfully trained to prepare
the ground state of a Hamiltonian given in the dataset,
it should achieve the corresponding ground state energy
given in the datapoint.

The third dataset oﬀered is tfq.datasets.xxz_chain .

(cid:88)

ˆH =

j

( ˆXj ˆXj+1 + ˆYj ˆYj+1 + ∆ ˆZj ˆZj+1)

This dataset contains 76 datapoints, corresponding to the
ground states of the 1D XXZ chain for ∆ in [0.3, 1.8] in
increments of 0.2.

Similar to the TFI dataset, each datapoint contains
a circuit, a label, a Hamiltonian, and some additional
metadata. The circuit prepares the approximate ground
state of the Hamiltonian in the datapoint. The labels are
0 for the critical metallic phase (∆ <= 1) and 1 for the
insulating phase (∆ > 1). As such, this dataset can also
be used for classiﬁcation and optimization benchmarking.
We expect to add more datasets as consensus is reached
in the QML community around good benchmark tasks.
As such, we welcome contributions! Those looking to
contribute new quantum datasets should reach out via
our GitHub page.

Concurrently with TFQ, we are open sourcing qsim,
a software package for simulating quantum circuits on
classical computers. We have adapted its C++ imple-
mentation to work inside TFQ’s TensorFlow ops. The
performance of qsim derives from two key ideas that can
be seen in the literature on classical simulators for quan-
tum circuits [80, 81]. The ﬁrst idea is the fusion of gates
in a quantum circuit with their neighbors to reduce the
number of matrix-vector multiplications required when
applying the circuit to a wavefunction. The second idea
is to create a set of matrix-vector multiplication functions
speciﬁcally optimized for the application of two-qubit (or
more) gates to state vectors, to take maximal advantage
of gate fusion. We discuss these points in detail below.
To quantify the performance of qsim, we also provide an
initial benchmark comparing qsim to Cirq. We note that
qsim is signiﬁcantly faster. We further note that the qsim
benchmark times include the full TFQ software stack of
serializing a circuit to ProtoBuﬀs in Python, conversion
of ProtoBuﬀs to C++ objects inside the dataﬂow graph
for our custom TensorFlow ops, and the relaying of re-
sults back to Python.

1. Comment on the Simulation of Quantum Circuits

To motivate the qsim fusing algorithm, consider how
circuits are applied to states in simulation. Suppose
we wish to apply two gates ˆG1 and ˆG2 to our initial
state |ψ(cid:105), and suppose these gates act on the same two
qubits. Since gate application is associative, we have
( ˆG2 ˆG1) |ψ(cid:105) = ˆG2( ˆG1 |ψ(cid:105)). However, as the number of
qubits n supporting |ψ(cid:105) grows, the left side of the equal-
ity becomes much faster to compute. This is because
applying a gate to a state requires broadcasting the pa-
rameters of the gate to all 2n elements of the state vec-
tor, so that each gate application incurs a cost scaling as
2n. In contrast, multiplying small gate matrices incurs a
small cost. This means a simulation of a circuit will be
fastest if we pre-multiply as many gates as possible, while
keeping the matrix size small, before applying them to a
state. This pre-multiplication is called gate fusion and is
accomplished with the qsim fusion algorithm.

2. Gate Fusion with qsim

The core idea of the fusion algorithm is to construct
multi-qubit gates out of smaller gates. The circuit can
be interpreted as a (d + 1)-dimensional lattice, where d
denotes the spatial direction and 1 denotes the time di-
rection. Suppose we choose a fusion size F between 2 and
6 inclusive. The fusion algorithm combines gates that are
close in space and time to form larger gates that act on up
to F qubits. There are two steps in the algorithm. First,

14

Figure 9. Performance of TFQ and Cirq on simulation tasks.
The plots show the base 10 logarithm of the total time to
solution (in seconds) versus the number of qubits simulated.
Simulation of 500 random circuits, taken in batches of 50.
Circuits were of depth 40.
In the dense case, we see that
TFQ + qsim on CPU is around 10x faster than Cirq. With
the GPU this gap grows to approximately 50x. In the case of
sparse circuits, qsim gate fusion makes this speed diﬀerence
even more pronounced reaching well over 100x performance
diﬀerence on both CPU and GPU.

the Cirq function
circuits were
generated using
cirq.generate_boixo_2018_supremacy_circuits_v2 .
These
circuits are only tractable for benchmarking due to the
small numbers of qubits involved here. These circuits
involve dense (subject to a constraint [84]) interleaved
two-qubit gates to generate entanglement as quickly as
possible. In summary, at the largest benchmarked prob-
lem size of 30 qubits, qsim achieves an approximately
10-fold improvement in simulation time over Cirq. The
performance curves are shown in Fig. 9.

When the simulated circuits have more sparse struc-
ture, the Fusion algorithm allows us to achieve a larger
performance boost by reducing the number of gates that
ultimately need to be simulated. The circuits for this
task are a factorized version of the supremacy-style cir-
cuits which generate entanglement only on small subsets
of the qubits.
In summary, for these circuits, we ﬁnd
a roughly 100 times improvement in simulation time in
TFQ versus Cirq. The performance curves are shown in
Fig. 9.

Thus we see that in addition to our core functionality
of implementing native TensorFlow gradients for quan-
tum circuits, TFQ also provides a signiﬁcant boost in
performance over Cirq when running in simulation mode.
Additionally, as noted before, this performance boost is
despite the additional overhead of serialization between
the TensorFlow frontend and qsim proper.

Figure 8. Visualization of the qsim fusing algorithm. In this
example, a set of single and two-qubit gates are fused into
three or four-qubit gates, increasing the speed of subsequent
circuit simulation. qsim supports gate fusions for larger num-
bers of qubits (up to 6 qubits at a time).

we fuse each q-qubit gate (q ≥ 2) with smaller or same
size neighbors in time direction that act on the same set
of qubits. Second, we greedily (in increasing time order)
combine small gates that are neighbors in space and time
to construct the largest gates possible (up to F qubits).
Typically F = 4 is optimal for many threads and F = 2
or F = 3 is optimal for one or two threads.

3. Hardware-Acceleration

With the given quantum circuit fused into the mini-
mal number of up to F -qubit gates, we need simulators
optimized for applying up to 2F × 2F matrices to state
vectors. TFQ will adapt to the user’s available hard-
ware. For CPU based simulations, SSE2 instruction set
[82] and AVX2 + AVX512 instruction sets [83] will be
detected and used to increase performance. If a compat-
ible CUDA GPU is detected TFQ will be able to switch
to GPU based simulation as well. The next section illus-
trates this power with benchmarks comparing the per-
formance of TFQ to the performance of parallelized Cirq
running in simulation mode. In the future, we hope to ex-
pand the range of custom simulation hardware supported
to include TPU integration.

4. Benchmarks

Here, we demonstrate the performance of TFQ, backed
by qsim, relative to Cirq on two benchmark simula-
tion tasks. As detailed above, the performance diﬀer-
ence is due to circuit pre-processing via gate fusion com-
bined with performant C++ simulators. The bench-
marks were performed on a desktop equipped with an In-
tel(R) Xeon(R) Gold 6154 CPU (18 cores and 36 threads)
and an NVidia V100 GPU.

The ﬁrst benchmark task is

simulation of 500
random (early variant of beyond-classical/supremacy-
These
style)

circuits batched 50

time.

at

a

15202530Number of qubits12345log10(Simulation time) (s)SimulatorTFQ qsim - GPUTFQ qsim - CPUCirq + MultiprocessingCircuit TypeDenseSparse5. Large-scale simulation for quantum machine learning

Recently, we have provided a series of

learning-
theoretic tests for evaluating whether a quantum machine
learning model can predict more accurately than classical
machine learning models [22]. One of the key ﬁndings is
that various existing quantum machine learning models
perform slightly better than standard classical machine
learning models in small system sizes. However, these
quantum machine learning models perform substantially
worse than classical machine learning models in larger
system sizes (more than 10 to 15 qubits). Note that per-
forming better in small system sizes is not useful since a
classical algorithm can eﬃciently simulate the quantum
machine learning model. The above observation shows
that understanding the performance of quantum machine
learning models in large system sizes is very crucial for
assessing whether the quantum machine learning model
will eventually provide an advantage over classical mod-
els. Ref. [22] also provide improvements to existing quan-
tum machine learning models to improve the prediction
performance in large system sizes.

In the numerical experiments conducted in [22], we
consider the simulation of these quantum machine learn-
ing models up to 30 qubits. The large-scale simulation
allows us to gauge the potential and limitations of dif-
ferent quantum machine learning models better. We uti-
lize the qsim software package in TFQ to perform large-
scale quantum simulations using Google Cloud Plat-
form. The simulation reaches a peak throughput of up
to 1.1 quadrillion ﬂoating-point operations per second
(petaﬂop/s). Trends of approximately 300 teraﬂop/s for
quantum simulation and 800 teraﬂop/s for classical anal-
ysis were observed up to the maximum experiment size
with the overall ﬂoating-point operations across all exper-
iments totaling approximately two quintillions (exaﬂop).

6. Noise in qsim

The study of noise in quantum circuits is an important
use-case for simulators to support. To address this, qsim
supports simulation of all the common channels provided
by Cirq. Further, TFQ supports the serialization of cir-
cuits containing such channels, allowing users to apply
the many features of TFQ to the study of noisy circuits.
The two main choices for simulation of noisy circuits
are density matrix simulation and trajectory simulation.
There are trade-oﬀs between these choices. Density ma-
trix simulation keeps a full density matrix in memory,
and at each timestep applies all the Kraus operators as-
sociated with a channel; at the end of simulation, prop-
erties of the state can be computed against this density
matrix. For a circuit on N qubits, a full density matrix
simulation requires memory of size 22N . In contrast, tra-
jectory simulations keep only a pure state in memory, and
at each time step, a single Kraus operator in the chan-
nel is selected probabilistically and applied to the state;

15

properties of interest are measured against the resulting
pure state. The process must be run many times, and
the results averaged; but, the memory required at any
one time is only 2N . When a circuit is not too noisy, it
is often more eﬃcient to average over trajectories than
it is to simulate the full density matrix, so we choose
this method for TFQ. For information on how to use this
feature, see IV C.

III. THEORY OF HYBRID
QUANTUM-CLASSICAL MACHINE LEARNING

In the previous section, we reviewed the building blocks
required for use of TFQ. In this section, we consider the
theory behind the software. We deﬁne quantum neu-
ral networks as products of parameterized unitary ma-
trices. Samples and expectation values are deﬁned by
expressing the loss function as an inner product. With
quantum neural networks and expectation values deﬁned,
we can then deﬁne gradients. We ﬁnally combine quan-
tum and classical neural networks and formalize hybrid
quantum-classical backpropagation, one of core compo-
nents of TFQ.

A. Quantum Neural Networks

A Quantum Neural Network ansatz can generally be

written as a product of layers of unitaries in the form

ˆU (θ) =

L
(cid:89)

(cid:96)=1

ˆV (cid:96) ˆU (cid:96)(θ(cid:96)),

(1)

where the (cid:96)th layer of the QNN consists of the prod-
uct of ˆV (cid:96), a non-parametric unitary, and ˆU (cid:96)(θ(cid:96)) a uni-
tary with variational parameters (note the superscripts
here represent indices rather than exponents). The multi-
parameter unitary of a given layer can itself be generally
comprised of multiple unitaries { ˆU (cid:96)
applied in
parallel:

j)}M(cid:96)

j (θ(cid:96)

j=1

ˆU (cid:96)(θ(cid:96)) ≡

M(cid:96)(cid:79)

j=1

ˆU (cid:96)

j (θ(cid:96)

j).

(2)

Finally, each of these unitaries ˆU (cid:96)
can be expressed as
j
the exponential of some generator ˆgj(cid:96), which itself can
be any Hermitian operator on n qubits (thus expressible
as a linear combination of n-qubit Pauli’s),

ˆU (cid:96)

j (θ(cid:96)

j) = e−iθ(cid:96)

j ˆg(cid:96)
j ,

ˆg(cid:96)
j =

Kj(cid:96)
(cid:88)

k=1

βj(cid:96)
k

ˆPk,

(3)

where ˆPk ∈ Pn, here Pn denotes the Paulis on n-qubits
[85], and βj(cid:96)
k ∈ R for all k, j, (cid:96). For a given j and (cid:96), in the
case where all the Pauli terms commute, i.e. [ ˆPk, ˆPm] =

16

where we deﬁned a vector of coeﬃcients α ∈ RN and
a vector of N operators ˆh. Often this decomposition is
chosen such that each of these sub-Hamiltonians is in the
n-qubit Pauli group ˆhk ∈ Pn. The expectation value of
this Hamiltonian is then generally evaluated via quantum
expectation estimation, i.e. by taking the linear combi-
nation of expectation values of each term

f (θ) = (cid:104) ˆH(cid:105)θ =

N
(cid:88)

k=1

αk (cid:104)ˆhk(cid:105)θ ≡ α · hθ,

(8)

.

where we introduced the vector of expectations hθ ≡
(cid:104)ˆh(cid:105)θ
In the case of non-commuting terms, the vari-
ous expectation values (cid:104)ˆhk(cid:105)θ
are estimated over separate
runs.

Note that, in practice, each of these quantum expecta-
tions is estimated via sampling of the output of the quan-
tum computer [29]. Even assuming a perfect ﬁdelity of
quantum computation, sampling measurement outcomes
of eigenvalues of observables from the output of the quan-
tum computer to estimate an expectation will have some
non-negligible variance for any ﬁnite number of samples.
Assuming each of the Hamiltonian terms of equation (7)
admit a Pauli operator decomposition as

ˆhj =

Jj
(cid:88)

k=1

γj
k

ˆPk,

(9)

∗/(cid:15)2), where (cid:107)ˆhk(cid:107)∗ = (cid:80)Jj

’s are real-valued coeﬃcients and the ˆPj’s
where the γj
k
are Paulis that are Pauli operators [85], then to get an
estimate of the expectation value (cid:104)ˆhj(cid:105) within an accu-
racy (cid:15), one needs to take a number of measurement sam-
ples scaling as ∼ O((cid:107)ˆhk(cid:107)2
k=1 |γj
k|
is the Pauli coeﬃcient norm of each Hamiltonian term.
Thus, to estimate the expectation value of the full Hamil-
tonian (7) accurately within a precision ε = (cid:15) (cid:80)
k |αk|2,
k |αk|2(cid:107)ˆhk(cid:107)2
we would need on the order of ∼ O( 1
∗)
(cid:15)2
measurement samples in total [27, 88], as we would need
to measure each expectation independently if we are fol-
lowing the quantum expectation estimation trick of (8).
This is in sharp contrast to classical methods for gradi-
ents involving backpropagation, where gradients can be
estimated to numerical precision; i.e. within a precision
(cid:15) with ∼ O(PolyLog((cid:15))) overhead. Although there have
been attempts to formulate a backpropagation principle
for quantum computations [56], these methods also rely
on the measurement of a quantum observable, thus also
requiring ∼ O( 1

(cid:80)

(cid:15)2 ) samples.

As we will see in the following section III C, estimat-
ing gradients of quantum neural networks on quantum
computers involves the estimation of several expectation
values of the cost function for various values of the pa-
rameters. One trick that was recently pointed out [47, 89]
and has been proven to be successful both theoretically
and empirically to estimate such gradients is the stochas-
tic selection of various terms in the quantum expectation

Figure 10.
High-level depiction of a multilayer quantum
neural network (also known as a parameterized quantum cir-
cuit), at various levels of abstraction. (a) At the most detailed
level we have both ﬁxed and parameterized quantum gates,
any parameterized operation is compiled into a combination
of parameterized single-qubit operations.
(b) Many singly-
parameterized gates Wj(θj) form a multi-parameterized uni-
tary Vl((cid:126)θl) which performs a speciﬁc function. (c) The prod-
uct of multiple unitaries Vl generates the quantum model U (θ)
shown in (d).

0 for all m, k such that βj(cid:96)
(cid:54)= 0, one can simply
decompose the unitary into a product of exponentials of
each term,

m, βj(cid:96)

k

ˆU (cid:96)

j (θ(cid:96)

j) =

(cid:89)

e−iθ(cid:96)

j βj(cid:96)

k

ˆPk .

(4)

k
Otherwise, in instances where the various terms do not
commute, one may apply a Trotter-Suzuki decomposi-
tion of this exponential [86], or other quantum simula-
tion methods [87]. Note that in the above case where
the unitary of a given parameter is decomposable as the
product of exponentials of Pauli terms, one can explicitly
express the layer as

ˆU (cid:96)

j (θ(cid:96)

j) =

(cid:104)
cos(θ(cid:96)

jβj(cid:96)

k ) ˆI − i sin(θ(cid:96)

jβj(cid:96)

k ) ˆPk

(cid:105)

.

(5)

(cid:89)

k

The above form will be useful for our discussion of gra-
dients of expectation values below.

B. Sampling and Expectations

To optimize the parameters of an ansatz from equation
(1), we need a cost function to optimize. In the case of
standard variational quantum algorithms, this cost func-
tion is most often chosen to be the expectation value of
a cost Hamiltonian,

f (θ) = (cid:104) ˆH(cid:105)θ ≡ (cid:104)Ψ0| ˆU †(θ) ˆH ˆU (θ) |Ψ0(cid:105)
where |Ψ0(cid:105) is the input state to the parameterized circuit.
In general, the cost Hamiltonian can be expressed as a
linear combination of operators, e.g. in the form

(6)

ˆH =

N
(cid:88)

k=1

αk

ˆhk ≡ α · ˆh,

(7)

estimation. This can greatly reduce the number of mea-
surements needed per gradient update, we will cover this
in subsection III C 3.

C. Gradients of Quantum Neural Networks

Now that we have established how to evaluate the
loss function, let us describe how to obtain gradients of
the cost function with respect to the parameters. Why
should we care about gradients of quantum neural net-
works? In classical deep learning, the most common fam-
ily of optimization heuristics for the minimization of cost
functions are gradient-based techniques [90–92], which
include stochastic gradient descent and its variants. To
leverage gradient-based techniques for the learning of
multilayered models, the ability to rapidly diﬀerentiate
error functionals is key. For this, the backwards propa-
gation of errors [93] (colloquially known as backprop), is
a now canonical method to progressively calculate gradi-
ents of parameters in deep networks. In its most general
form, this technique is known as automatic diﬀerentiation
[94], and has become so central to deep learning that this
feature of diﬀerentiability is at the core of several frame-
works for deep learning, including of course TensorFlow
(TF) [95], JAX [96], and several others.

To be able to train hybrid quantum-classical models
(section III D), the ability to take gradients of quantum
neural networks is key. Now that we understand the
greater context, let us describe a few techniques below
for the estimation of these gradients.

17

value of the circuit for Hamiltonians which have a single-
term in their Pauli decomposition (3) (or, alternatively, if
the Hamiltonian has a spectrum {±λ} for some positive
λ). For multi-term Hamiltonians, in [98] a method to
obtain the analytic gradients is proposed which uses a
linear combination of unitaries. Here, instead, we will
simply use a change of variables and the chain rule to
obtain analytic gradients of parametric unitaries of the
form (5) without the need for ancilla qubits or additional
unitaries.

For a parameter of interest θ(cid:96)
j

appearing in a layer (cid:96) in a
parametric circuit in the form (5), consider the change of
variables ηj(cid:96)
, then from the chain rule of calculus
[99], we have

k ≡ θ(cid:96)

jβj(cid:96)

k

∂f
∂θ(cid:96)
j

(cid:88)

=

k

∂f
∂ηj(cid:96)
k

∂ηj(cid:96)
k
∂θ(cid:96)
j

(cid:88)

=

βj(cid:96)
k

k

∂f
∂ηk

.

(11)

Thus, all we need to compute are the derivatives of the
cost function with respect to ηj(cid:96)
. Due to this change of
k
variables, we need to reparameterize our unitary ˆU (θ)
from (1) as

ˆU (cid:96)(θ(cid:96)) (cid:55)→ ˆU (cid:96)(η(cid:96)) ≡

(cid:79)

(cid:16) (cid:89)

e−iηj(cid:96)

k

ˆPk

(cid:17)

,

(12)

j∈I(cid:96)

k

where I(cid:96) ≡ {1, . . . , M(cid:96)} is an index set for the QNN
layers. One can then expand each exponential in the
above in a similar fashion to (5):

e−iηj(cid:96)

k

ˆPk = cos(ηj(cid:96)

k ) ˆI − i sin(ηj(cid:96)

k ) ˆPk.

(13)

1. Finite diﬀerence methods

A simple approach is to use simple ﬁnite-diﬀerence

methods, for example, the central diﬀerence method,

As can be shown from this form, the analytic derivative
of the expectation value f (η) ≡ (cid:104)Ψ0| ˆU †(η) ˆH ˆU (η) |Ψ0(cid:105)
with respect to a component ηj(cid:96)
can be reduced to fol-
k
lowing parameter shift rule [47, 89, 100]:

(10)

+ O(ε2)

∂kf (θ) =

f (θ + ε∆k) − f (θ − ε∆k)
2ε
which, in the case where there are M continuous param-
eters, involves 2M evaluations of the objective function,
each evaluation varying the parameters by ε in some di-
rection, thereby giving us an estimate of the gradient
of the function with a precision O(ε2). Here the ∆k
is a unit-norm perturbation vector in the kth direction
of parameter space, (∆k)j = δjk. In general, one may
use lower-order methods, such as forward diﬀerence with
O(ε) error from M + 1 objective queries [24], or higher
order methods, such as a ﬁve-point stencil method, with
O(ε4) error from 4M queries [97].

2. Parameter shift methods

As recently pointed out in various works [89, 98], given
knowledge of the form of the ansatz (e.g. as in (3)), one
can measure the analytic gradients of the expectation

∂
∂ηj(cid:96)
k

f (η) = f (η + π

4 ∆j(cid:96)

k ) − f (η − π

4 ∆j(cid:96)
k )

(14)

where ∆j(cid:96)
is a vector representing unit-norm perturba-
k
tion of the variable ηj(cid:96)
in the positive direction. We thus
k
see that this shift in parameters can generally be much
larger than that of the numerical diﬀerentiation parame-
ter shifts as in equation (10). In some cases this is useful
as one does not have to resolve as ﬁne-grained a diﬀer-
ence in the cost function as an inﬁnitesimal shift, hence
requiring less runs to achieve a suﬃciently precise esti-
mate of the value of the gradient.

Note that in order to compute through the chain rule
in (11) for a parametric unitary as in (3), we need to
evaluate the expectation function 2K(cid:96) times to obtain
the gradient of the parameter θ(cid:96)
. Thus, in some cases
j
where each parameter generates an exponential of many
terms, although the gradient estimate is of higher pre-
cision, obtaining an analytic gradient can be too costly
in terms of required queries to the objective function.
[89]
To remedy this additional overhead, Harrow et al.

proposed to stochastically select terms according to a dis-
tribution weighted by the coeﬃcients of each term in the
generator, and to perform gradient descent from these
stochastic estimates of the gradient. Let us review this
stochastic gradient estimation technique as it is imple-
mented in TFQ.

3. Stochastic Parameter Shift Gradient Estimation

Consider the full analytic gradient from (14) and
(11), if we have M(cid:96) parameters and L layers, there are
(cid:80)L

(cid:96)=1 M(cid:96) terms of the following form to estimate:

∂f
∂θ(cid:96)
j

=

Kj(cid:96)
(cid:88)

k=1

βj(cid:96)
k

∂f
∂ηk

=

Kj(cid:96)
(cid:88)

(cid:104) (cid:88)

k=1

±

±βj(cid:96)

k f (η ± π

4 ∆j(cid:96)
k )

(cid:105)
. (15)

These terms come from the components of the gradient
vector itself which has the dimension equal to that of the
total number of free parameters in the QNN, dim(θ).
For each of these components, for the jth component of
the (cid:96)th layer, there 2Kj(cid:96) parameter-shifted expectation
values to evaluate, thus in total there are (cid:80)L
(cid:96)=1 2Kj(cid:96)M(cid:96)
parameterized expectation values of the cost Hamiltonian
to evaluate.

For practical implementation of this estimation proce-
dure, we must expand this sum further. Recall that, as
the cost Hamiltonian generally will have many terms, for
each quantum expectation estimation of the cost function
for some value of the parameters, we have

f (θ) = (cid:104) ˆH(cid:105)θ =

N
(cid:88)

αm (cid:104)ˆhm(cid:105)θ =

N
(cid:88)

Jm(cid:88)

αmγm

q (cid:104) ˆPqm(cid:105)θ ,

m=1

m=1

q=1

(16)
which has (cid:80)N
j=1 Jj terms. Thus, if we consider that all
the terms in (15) are of the form of (16), we see that we
have a total number of (cid:80)N
(cid:96)=1 2JmKj(cid:96)M(cid:96) expecta-
m=1
tion values to estimate the gradient. Note that one of
these sums comes from the total number of appearances
of parameters in front of Paulis in the generators of the
parameterized quantum circuit, the second sum comes
from the various terms in the cost Hamiltonian in the
Pauli expansion.

(cid:80)L

As the cost of accurately estimating all these terms
one by one and subsequently linearly combining the val-
ues such as to yield an estimate of the total gradient
may be prohibitively expensive in terms of numbers of
runs, instead, one can stochastically estimate this sum,
by randomly picking terms according to their weighting
[47, 89].
One
ap-
pearances
in the QNN, k ∼
Pr(k|j, (cid:96)) = |βj(cid:96)
o |), one then estimates the
two parameter-shifted terms corresponding to this in-
dex in (15) and averages over samples. We consider
this case to be simply stochastic gradient estimation for

a parameter
k |/((cid:80)Kj(cid:96)
o=1 |βj(cid:96)

a distribution over

can sample

the

of

18

d=1

(cid:80)Jd

r=1 |αdγd

the gradient component corresponding to the parame-
ter θ(cid:96)
. One can go even further in this spirit, for each
j
of these sampled expectation values, by also sampling
terms from (16) according to a similar distribution de-
termined by the magnitude of the Pauli expansion co-
eﬃcients. Sampling the indices {q, m} ∼ Pr(q, m) =
q |/((cid:80)N
r |) and estimating the expec-
|αmγm
tation (cid:104) ˆPqm(cid:105)θ
for the appropriate parameter-shifted val-
ues sampled from the terms of (15) according to the pro-
cedure outlined above. This is considered doubly stochas-
In principle, one could go one
tic gradient estimation.
step further, and per iteration of gradient descent, ran-
domly sample indices representing subsets of parame-
ters for which we will estimate the gradient component,
and set the non-sampled indices corresponding gradient
components to 0 for the given iteration. The distribu-
j ∼ Pr(j, (cid:96)) =
tion we sample in this case is given by θ(cid:96)
(cid:80)Kj(cid:96)
(cid:80)Mu
o |). This is, in a sense,
i=1
akin to the SPSA algorithm [101],
in the sense that
it is a gradient-based method with a stochastic mask.
The above component sampling, combined with doubly
stochastic gradient descent, yields what we consider to be
triply stochastic gradient descent. This is equivalent to si-
multaneously sampling {j, (cid:96), k, q, m} ∼ Pr(j, (cid:96), k, q, m) =
Pr(k|j, (cid:96))Pr(j, (cid:96))Pr(q, m) using the probabilities outlined
in the paragraph above, where j and (cid:96) index the param-
eter and layer, k is the index from the sum in equation
(15), and q and m are the indices of the sum in equation
(16).

k |/((cid:80)L

k=1 |βj(cid:96)

o=1 |βiu

(cid:80)Kiu

u=1

In TFQ, all three of the stochastic averaging meth-
ods above can be turned on or oﬀ independently for
stochastic parameter-shift gradients. See the details in
the Differentiator module of TFQ on GitHub.

4. Adjoint Gradient Backpropagation in Simulations

For experiments with tractably classically simulatable
system sizes, the derivatives can be computed entirely
in simulation, using an analogue of backpropagation
called Adjoint Diﬀerentiation [102, 103]. This is a high-
performance way to obtain gradients of deep circuits with
many parameters. Although it is not possible to perform
this technique in quantum hardware, let us outline here
how it is implemented in TFQ for numerical simulator
backends such as qsim.

Suppose we are given a parameterized quantum circuit

in the form of (1)

ˆU (θ) =

L
(cid:89)

(cid:96)=1

ˆV (cid:96) ˆU (cid:96)(θ(cid:96)) =

L
(cid:89)

(cid:96)=1

ˆW (cid:96)

θ(cid:96) = ˆW n

θn ˆW n−1

θn−1 · · · ˆW 1
θ1,

where for compactness of notation, we denoted the pa-
rameterized layers in a more succinct form,

ˆW (cid:96)

θ(cid:96) = ˆV (cid:96) ˆU (cid:96)(θ(cid:96)).

19

Figure 11. Tensor network contraction diagram [71, 72] depicting the adjoint backpropagation process for the computation of
the gradient of an expectation value at the output of multilayer QNN with respect to a parameter in the (cid:96)th layer. Yellow
channel depicts the forward pass, which is then duplicated into two copies of the ﬁnal state. The blue and orange channels depict
the backwards pass, performed via layer-wise recursive uncomputation. Both the orange and blue backwards pass outputs are
then contracted with the gradient of the (cid:96)th layer’s unitary. The arrows with dotted lines indicate how to modify the contraction
to compute a gradient with respect to a parameter in the ((cid:96) − 1)th layer.

Let us label the initial state |ψ0(cid:105) ﬁnal quantum state
θn (cid:105), with

|ψn

value of this observable with respect to our ﬁnal layer’s
parameterized state is

|ψ(cid:96)

θ(cid:96)(cid:105) = ˆW (cid:96)

θ(cid:96) |ψ(cid:96)−1

θ(cid:96)−1(cid:105)

being the recursive deﬁnition of the Schrödinger-evolved
state vector at layer (cid:96). The derivatives of the state with
respect to θ(cid:96), the jth parameter of the (cid:96)th layer, is given
by

∂θ(cid:96)

j

|ψn

θn (cid:105) = ˆW n

θn · · · ˆW (cid:96)+1

θ(cid:96)+1 ∂θ(cid:96)

j

ˆW (cid:96)
θ(cid:96)

ˆW (cid:96)−1

θ(cid:96)−1 · · · ˆW 1

θ1 |ψ0(cid:105) .

This gradient of the (cid:96)th layer parameters, assuming the
QNN is structured as in equations (1), (2), and (3) is
given analytically by

∂θ(cid:96)

j

ˆW (cid:96)

θ(cid:96) = ˆV (cid:96)(−iˆg(cid:96)

j) ˆU (cid:96)(θ(cid:96))

which is an operator that can be computed numerically
and inserted in the circuit when dealing with a classical
simulation of the quantum circuit.

A key trick for adjoint backpropagation is leveraging
the fact that we can reverse unitary layers, so that we do
not have to store the history of states; we can access the
state at any inner layer by uncomputing the later layers:

ˆW (cid:96)−1

θ(cid:96)−1 · · · ˆW 1

θ1 |ψ0(cid:105) = ˆW † (cid:96)
θ(cid:96)

ˆW † (cid:96)+1

θ(cid:96)+1 · · · ˆW † n

θn |ψn

θn (cid:105) .

We can leverage this trick in order to evaluate gradients
of the quantum state with respect to parameters of in-
termediate layers of the QNN,

∂θ(cid:96)

j

|ψn

θn (cid:105) = ˆW n

θn · · · ˆW (cid:96)+1

θ(cid:96)+1∂θ(cid:96)

j

ˆW (cid:96)
θ(cid:96)

ˆW † (cid:96)

θ(cid:96) · · · ˆW † n

θn |ψn

θn (cid:105) ,

this allows us to compute gradients layer-wise starting
from the ﬁnal layer via a backwards pass, following of
course a forward pass to compute the ﬁnal state at the
output layer.

In most contexts, we typically want to take gradients
of expectation values at the output of several QNN lay-
ers. Given a Hermitian observable ˆH, the expectation

(cid:104) ˆH(cid:105)θn = (cid:104)ψn

θn| ˆH |ψn

θn (cid:105) ,

and the derivative of this expectation value is given by

∂θ(cid:96)

j

(cid:104) ˆH(cid:105)θn = (∂θ(cid:96)
j
(cid:104)

= 2(cid:60)

(cid:104)ψn

(cid:104)ψn

θn|) ˆH |ψn
θn | ˆH(∂θ(cid:96)

θn | ˆH(∂θ(cid:96)
θn (cid:105) + (cid:104)ψn
(cid:105)
θn(cid:105))

|ψn

,

j

j

|ψn

θn (cid:105))

|ψn

θn (cid:105)) for each parameter.

where we employed the fact that the observable is Her-
mitian. Thus the computational task of gradient evalua-
tion reduces to the evaluation of the modiﬁed expectation
(cid:104)ψn

θn | ˆH(∂θ(cid:96)
In order to compute these gradients, we can proceed in
a recursive fashion during the backwards pass, starting
from the ﬁnal layer’s output. Denoting this recursion
using nested parentheses, the gradient evaluation is given
by

j

∂θ(cid:96)

j

(cid:104) ˆH(cid:105)θn =

(cid:16)

(cid:17)

· · ·
(cid:16)

·

(cid:16)(cid:16)

∂θ(cid:96)
j
(cid:16)

(cid:104)ψn

θn | ˆH
(cid:17)

ˆW (cid:96)
θ(cid:96)

θn

(cid:17) ˆW n
(cid:16) ˆW † (cid:96)
·
(cid:17)

θ(cid:96) · · ·

= (cid:104) ˜ψ(cid:96)

θ(cid:96)|

∂θ(cid:96)

j

ˆW (cid:96)
θ(cid:96)

|ψ(cid:96)−1

θ(cid:96)−1 (cid:105)

(cid:17)

· · · ˆW (cid:96)+1
θ(cid:96)+1
(cid:16) ˆW † n

θn (|ψn

θn(cid:105))

(cid:17)

(cid:17)

. . .

This consists of recursively computing the backwards

propagated state,

|ψ(cid:96)−1

θ(cid:96)−1(cid:105) = ˆW (cid:96)†

θ(cid:96) |ψ(cid:96)

θ(cid:96)(cid:105)

and contracting it with both the gradient of the (cid:96)th
θ(cid:96) and the backwards propagated

layer’s unitary ∂θ(cid:96)
contraction of the ﬁnal state with the observable:

ˆW (cid:96)

j

| ˜ψ(cid:96)−1

θ(cid:96)−1(cid:105) ≡ ˆW (cid:96)†

| ˜ψn

θn (cid:105) ≡ ˆH |ψn

θn (cid:105) .

θ(cid:96)(cid:105) ,

θ(cid:96) | ˜ψ(cid:96)
θ(cid:96)−1(cid:105) and | ˜ψ(cid:96)

By only storing |ψ(cid:96)−1
θ(cid:96)(cid:105) in memory for gradient
evaluations of the (cid:96)th layer during the backwards pass,
we saved the need for far more forward passes while only

20

Let us ﬁrst describe how to create these functions from
expectation values of QNN’s.

As we saw in equations (6) and (7), we get a diﬀer-
entiable cost function f : RM (cid:55)→ R from taking the ex-
pectation value of a single Hamiltonian at the end of the
parameterized circuit, f (θ) = (cid:104) ˆH(cid:105)θ
. As we saw in equa-
tions (7) and (8), to compute this expectation value, as
the readout Hamiltonian is often decomposed into a lin-
ear combination of operators ˆH = α · ˆh (see (7)), then
the function is itself a linear combination of expectation
values of multiple terms (see (8)), (cid:104) ˆH(cid:105)θ = α · hθ where
hθ ≡ (cid:104)ˆh(cid:105)θ ∈ RN is a vector of expectation values. Thus,
before the scalar value of the cost function is evaluated,
QNN’s naturally are evaluated as a vector of expectation
values, hθ.

Hence, if we would like the QNN to become more like
a classical neural network block, i.e. mapping vectors to
vectors f : RM → RN , we can obtain a vector-valued
diﬀerentiable function from the QNN by considering it
as a function of the parameters which outputs a vector
of expectation values of diﬀerent operators,

f : θ (cid:55)→ hθ

(17)

where

(hθ)k = (cid:104)ˆhk(cid:105)θ ≡ (cid:104)Ψ0| ˆU †(θ)ˆhk ˆU (θ) |Ψ0(cid:105) .

(18)

We represent such a QNN-based function in Fig. 13. Note
that, in general, each of these ˆhk’s could be comprised of
multiple terms themselves,

ˆhk =

Nk(cid:88)

t=1

γt ˆmt

(19)

k=1

hence, one can perform Quantum Expectation Estima-
tion to estimate the expectation of each term as (cid:104)ˆhk(cid:105) =
(cid:80)

t γt (cid:104) ˆmt(cid:105).
Note that, in some cases, instead of the expectation
values of the set of operators {ˆhk}M
, one may instead
want to relay the histogram of measurement results ob-
tained from multiple measurements of the eigenvalues
of each of these observables. This case can also be
phrased as a vector of expectation values, as we will
now show. First, note that the histogram of the mea-
surement results of some ˆhk with eigendecomposition
ˆhk = (cid:80)rk
j=1 λjk |λjk(cid:105) (cid:104)λjk| can be considered as a vec-
tor of expectation values where the observables are the
eigenstate projectors |λjk(cid:105)(cid:104)λjk|. Instead of obtaining a
single real number from the expectation value of ˆhk, we
can obtain a vector hk ∈ Rrk , where rk = rank(ˆhk) and
the components are given by (hk)j ≡ (cid:104)|λjk(cid:105)(cid:104)λjk|(cid:105)θ
. We
are then eﬀectively considering the categorical (empiri-
cal) distribution as our vector.

Now, if we consider measuring the eigenvalues of mul-
tiple observables {ˆhk}M
and collecting the measure-
ment result histograms, we get a 2-dimensional tensor
. Without loss of generality, we
(hθ)jk = (cid:104)|λjk(cid:105)(cid:104)λjk|(cid:105)θ

k=1

Figure 12. High-level depiction of a quantum-classical neural
network. Blue blocks represent Deep Neural Network (DNN)
function blocks and orange boxes represent Quantum Neural
Network (QNN) function blocks. Arrows represent the ﬂow
of information during the feedforward (inference) phase. For
an example of the interface between quantum and classical
neural network blocks, see Fig. 13.

using twice the memory. See Figure 11 for a depiction
of the adjoint-backpropagation-based gradient computa-
tion described above. This is to be contrasted with, for
example, ﬁnite-diﬀerence gradients, which would require
2M forward passes for M parameters. Thus, adjoint dif-
ferentiation is a useful method for rapid prototyping of
QNN’s using classical simulators of quantum circuits such
as qsim in TFQ.

D. Hybrid Quantum-Classical Computational
Graphs

Now that we have reviewed various ways of obtaining
gradients of expectation values, let us consider how to go
beyond basic variational quantum algorithms and con-
sider fully hybrid quantum-classical neural networks. As
we will see, our general framework of gradients of cost
Hamiltonians will carry over.

1. Hybrid Quantum-Classical Neural Networks

Now, we are ready to formally introduce the no-
tion of Hybrid Quantum-Classical Neural Networks
(HQCNN’s). HQCNN’s are meta-networks comprised
of quantum and classical neural network-based function
blocks composed with one another in the topology of a
directed graph. We can consider this a rendition of a hy-
brid quantum-classical computational graph where the
inner workings (variables, component functions) of vari-
ous functions are abstracted into boxes (see Fig. 12 for a
depiction of such a graph). The edges then simply repre-
sent the ﬂow of classical information through the meta-
network of quantum and classical functions. The key will
be to construct parameterized (diﬀerentiable) functions
fθ : RM → RN from expectation values of parameterized
quantum circuits, then creating a meta-graph of quan-
tum and classical computational nodes from these blocks.

can ﬂatten this array into a vector of dimension RR where
R = (cid:80)M
k=1 rk. Thus, considering vectors of expectation
values is a relatively general way of representing the out-
put of a quantum neural network. In the limit where the
set of observables considered forms an informationally-
complete set of observables [104], then the array of mea-
surement outcomes would fully characterize the wave-
function, albeit at an overhead exponential in the number
of qubits.

We should mention that in some cases, instead of ex-
pectation values or histograms, single samples from the
output of a measurement can be used for direct feedback-
control on the quantum system, e.g.
in quantum error
correction [85]. At least in the current implementation of
TFQuantum, since quantum circuits are built in Cirq and
this feature is not supported in the latter, such scenar-
ios are currently out-of-scope. Mathematically, in such
a scenario, one could then consider the QNN and mea-
surement as map from quantum circuit parameters θ to
the conditional random variable Λθ valued over RNk cor-
responding to the measured eigenvalues λjk with a prob-
ability density Pr[(Λθ)k ≡ λjk] = p(λjk|θ) which cor-
responds to the measurement statistics distribution in-
duced by the Born rule, p(λjk|θ) = (cid:104)|λjk(cid:105)(cid:104)λjk|(cid:105)θ
. This
QNN and single measurement map from the parameters
to the conditional random variable f : θ (cid:55)→ Λθ can be
considered as a classical stochastic map (classical condi-
tional probability distribution over output variables given
the parameters). In the case where only expectation val-
ues are used, this stochastic map reduces to a determinis-
tic node through which we may backpropagate gradients,
as we will see in the next subsection. In the case where
this map is used dynamically per-sample, this remains a
stochastic map, and though there exists some algorithms
for backpropagation through stochastic nodes [105], these
are not currently supported natively in TFQ.

E. Autodiﬀerentiation through hybrid
quantum-classical backpropagation

As described above, hybrid quantum-classical neural
network blocks take as input a set of real parameters θ ∈
RM , apply a circuit ˆU (θ) and take a set of expectation
values of various observables

(hθ)k = (cid:104)ˆhk(cid:105)θ .
The result of this parameter-to-expected value map is
a function f : RM → RN which maps parameters to a
real-valued vector,

f : θ (cid:55)→ hθ.

This function can then be composed with other param-
eterized function blocks comprised of either quantum or
classical neural network blocks, as depicted in Fig. 12.

To be able to backpropagate gradients through gen-
eral meta-networks of quantum and classical neural net-

21

work blocks, we simply have to ﬁgure out how to back-
propagate gradients through a quantum parameterized
block function when it is composed with other parame-
terized block functions. Due to the partial ordering of
the quantum-classical computational graph, we can fo-
cus on how to backpropagate gradients through a QNN
in the scenario where we consider a simpliﬁed quantum-
classical network where we combine all function blocks
that precede and postcede the QNN block into mono-
lithic function blocks. Namely, we can consider a sce-
nario where we have fpre : xin (cid:55)→ θ (fpre : Rin → RM )
as the block preceding the QNN, the QNN block as
fqnn : θ (cid:55)→ hθ, (fqnn : RM → RN ), the post-QNN
block as fpost : hθ (cid:55)→ yout (fpost : RM → RN
) and
out
ﬁnally the loss function for training the entire network
out → R. The
being computed from this output L : RN
composition of functions from the input data to the out-
put loss is then the sequentially composited function
(L◦fpost◦fqnn◦fpre). This scenario is depicted in Fig. 13.
Now, let us describe the process of backpropagation
through this composition of functions. As is standard in
backpropagation of gradients through feedforward net-
works, we begin with the loss function evaluated at the
output units and work our way back through the sev-
eral layers of functional composition of the network to
get the gradients. The ﬁrst step is to obtain the gra-
dient of the loss function ∂L/∂yout and to use classi-
cal (regular) backpropagation of gradients to obtain the
gradient of the loss with respect to the output of the
QNN, i.e. we get ∂(L ◦ fpost)/∂h via the usual use of
the chain rule for backpropagation, i.e., the contraction
of the Jacobian with the gradient of the subsequent layer,
.
∂(L ◦ fpost)/∂h = ∂L
Now, let us label the evaluated gradient of the loss
function with respect to the QNN’s expectation values
as

∂y · ∂fpost

∂h

g ≡ ∂(L◦fpost)

∂h

(cid:12)
(cid:12)
(cid:12)h=hθ

.

(20)

We can now consider this value as eﬀectively a constant.
Let us then deﬁne an eﬀective backpropagated Hamilto-
nian with respect to this gradient as

ˆHg ≡

(cid:88)

gk

ˆhk,

k
where gk are the components of (20). Notice that expec-
tations of this Hamiltonian are given by

(cid:104) ˆHg(cid:105)θ = g · hθ,
and so, the gradients of the expectation value of this
Hamiltonian are given by

∂
∂θj

(cid:104) ˆHg(cid:105)θ = ∂

∂θj

(g · hθ) =

(cid:88)

k

gk

∂hθ,k
∂θj

which is exactly the Jacobian of the QNN function fqnn
contracted with the backpropagated gradient of previous
layers. Explicitly,

∂(L ◦ fpost ◦ fqnn)/∂θ = ∂

∂θ (cid:104) ˆHg(cid:105)θ .

22

IV. BASIC QUANTUM APPLICATIONS

The following examples show how one may use the var-
ious features of TFQ to reproduce and extend existing
results in second generation quantum machine learning.
Each application has an associated Colab notebook which
can be run in-browser to reproduce any results shown.
Here, we use snippets of code from those example note-
books for illustration; please see the example notebooks
for full code details.

A. Hybrid Quantum-Classical Convolutional
Neural Network Classiﬁer

To run this example in the browser through Colab,

follow the link:

docs/tutorials/qcnn.ipynb

1. Background

Supervised classiﬁcation is a canonical task in classical
machine learning. Similarly, it is also one of the most
well-studied applications for QNNs [15, 31, 43, 106, 107].
As such, it is a natural starting point for our exploration
of applications for quantum machine learning. Discrimi-
native machine learning with hierarchical models can be
understood as a form of compression to isolate the infor-
mation containing the label [108]. In the case of quantum
data, the hidden classical parameter (real scalar in the
case of regression, discrete label in the case of classiﬁca-
tion) can be embedded in a non-local subsystem or sub-
space of the quantum system. One then has to perform
some disentangling quantum transformation to extract
information from this non-local subspace.

To choose an architecture for a neural network model,
one can draw inspiration from the symmetries in the
training data. For example, in computer vision, one of-
ten needs to detect corners and edges regardless of their
position in an image; we thus postulate that a neural net-
work to detect these features should be invariant under
translations.
In classical deep learning, an example of
such translationally-invariant neural networks are convo-
lutional neural networks. These networks tie parameters
across space, learning a shared set of ﬁlters which are
applied equally to all portions of the data.

To the best of the authors’ knowledge, there is no
strong indication that we should expect a quantum ad-
vantage for the classiﬁcation of classical data using QNNs
in the near term. For this reason, we focus on classifying
quantum data as deﬁned in section I C. There are many
kinds of quantum data with translational symmetry. One
example of such quantum data are cluster states. These
states are important because they are the initial states
for measurement-based quantum computation [109, 110].
In this example we will tackle the problem of detect-
ing errors in the preparation of a simple cluster state.

Figure 13. Example of inference and hybrid backpropaga-
tion at the interface of a quantum and classical part of a hy-
brid computational graph. Here we have classical deep neural
networks (DNN) both preceding and postceding the quan-
tum neural network (QNN). In this example, the preceding
DNN outputs a set of parameters θ which are used as then
used by the QNN as parameters for inference. The QNN
outputs a vector of expectation values (estimated through
several runs) whose components are (hθ)k = (cid:104)ˆhk(cid:105)θ
. This
vector is then fed as input to another (post-ceding) DNN,
and the loss function L is computed from this output. For
backpropagation through this hybrid graph, one ﬁrst back-
propagates the gradient of the loss through the post-ceding
DNN to obtain gk = ∂L/∂hk. Then, one takes the gradi-
ent of the following functional of the output of the QNN:
fθ = g · hθ = (cid:80)
with respect to the
QNN parameters θ (which can be achieved with any of the
methods for taking gradients of QNN’s described in previous
subsections of this section). This completes the backprop-
agation of gradients of the loss function through the QNN,
the preceding DNN can use the now computed ∂L/∂θ to fur-
ther backpropagate gradients to preceding nodes of the hybrid
computational graph.

k gkhθ,k = (cid:80)

k gk (cid:104)ˆhk(cid:105)θ

Thus, by taking gradients of the expectation value of
the backpropagated eﬀective Hamiltonian, we can get the
gradients of the loss function with respect to QNN pa-
rameters, thereby successfully backpropagating gradients
through the QNN. Further backpropagation through the
preceding function block fpre can be done using standard
classical backpropagation by using this evaluated QNN
gradient.

Note that for a given value of g, the eﬀective back-
propagated Hamiltonian is simply a ﬁxed Hamiltonian
operator, as such, taking gradients of the expectation
of a single multi-term operator can be achieved by any
choice in a multitude of the methods for taking gradients
of QNN’s described earlier in this section.

Backpropagation through parameterized quantum cir-

cuits is enabled by our Differentiator interface.

We oﬀer both ﬁnite diﬀerence, regular parameter shift,
and stochastic parameter shift gradients, while the gen-
eral
interface allows users to deﬁne custom gradient
methods.

We can think of this as a supervised classiﬁcation task:
our training data will consist of a variety of correctly
and incorrectly prepared cluster states, each paired with
their label. This classiﬁcation task can be generalized
to condensed matter physics and beyond, for example to
the classiﬁcation of phases near quantum critical points,
where the degree of entanglement is high.

Since our simple cluster states are translationally in-
variant, we can extend the spatial parameter-tying of
convolutional neural networks to quantum neural net-
works, using recent work by Cong, et al.
[53], which
introduced a Quantum Convolutional Neural Network
(QCNN) architecture. QCNNs are essentially a quan-
tum circuit version of a MERA (Multiscale Entanglement
Renormalization Ansatz) network [111]. MERA has been
extensively studied in condensed matter physics. It is a
hierarchical representation of highly entangled wavefunc-
tions. The intuition is that as we go higher in the net-
work, the wavefunction’s entanglement gets renormalized
(coarse grained) and simultaneously a compressed repre-
sentation of the wavefunction is formed. This is akin to
the compression eﬀects encountered in deep neural net-
works [112].

Here we extend the QCNN architecture to include clas-
sical neural network postprocessing, yielding a Hybrid
Quantum Convolutional Neural Network (HQCNN). We
perform several low-depth quantum operations in order
to begin to extract hidden parameter information from a
wavefunction, then pass the resulting statistical informa-
tion to a classical neural network for further processing.
Speciﬁcally, we will apply one layer of the hierarchy of the
QCNN. This allows us to partially disentangle the input
state and obtain statistics about values of multi-local ob-
servables. In this strategy, we deviate from the original
construction of Cong et al. [53]. Indeed, we are more in
the spirit of classical convolutional networks, where there
are several families of ﬁlters, or feature maps, applied in
a translationally-invariant fashion. Here, we apply a sin-
gle QCNN layer followed by several feature maps. The
outputs of these feature maps can then be fed into clas-
sical convolutional network layers, or in this particular
simpliﬁed example directly to fully-connected layers.

Target problems:

1. Learn to extract classical information hidden in cor-

relations of a quantum system

2. Utilize shallow quantum circuits via hybridization
with classical neural networks to extract informa-
tion

Required TFQ functionalities:

1. Hybrid quantum-classical network models
2. Batch quantum circuit simulator
3. Quantum expectation based backpropagation
4. Fast classical gradient-based optimizer

23

Figure 14. The quantum portion of our classiﬁers, shown on
4 qubits. The combination of quantum convolution (blue)
and quantum pooling (orange) reduce the system size from 4
qubits to 2 qubits.

2.

Implementations

As discussed in section II D 2, the ﬁrst step in the QML
pipeline is the preparation of quantum data. In this ex-
ample, our quantum dataset is a collection of correctly
and incorrectly prepared cluster states on 8 qubits, and
the task is to classify theses states. The dataset prepara-
tion proceeds in two stages; in the ﬁrst stage, we generate
a correctly prepared cluster state:

def c l u s t e r _ s t a t e _ c i r c u i t ( bits ) :
circuit = cirq . Circuit ()
circuit . append ( cirq . H . on_each ( bits ) )
for this_bit , next_bit in zip (

bits , bits [1:] + [ bits [0]]) :
circuit . append (

cirq . CZ ( this_bit , next_bit ) )

return circuit

Errors in cluster state preparation will be simulated by
applying Rx(θ) gates that rotate a qubit about the X-axis
of the Bloch sphere by some amount 0 ≤ θ ≤ 2π. These
excitations will be labeled 1 if the rotation is larger than
some threshold, and -1 otherwise. Since the correctly
prepared cluster state is always the same, we can think
of it as the initial state in the pipeline and append the
excitation circuits corresponding to various error states:

c lu s te r _s t at e _b i ts = cirq . GridQubit . rect (1 , 8)
excitation_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )
cluster_state = tfq . layers . AddCircuit () (

excitation_input , prepend =
c l u s t e r _ s t a t e _ c i r c u i t ( cl u st e r_ s ta t e_ b it s ) )

Note how excitation_input is a standard Keras data in-
gester. The datatype of the input is string to account
for our circuit serialization mechanics described in sec-
tion II E 1.

Having prepared our dataset, we begin construction of
our model. The quantum portion of all the models we
consider in this section will be made of the same oper-
ations: quantum convolution and quantum pooling. A
visualization of these operations on 4 qubits is shown in
Fig. 14. Quantum convolution layers are enacted by ap-
plying a 2 qubit unitary U ((cid:126)θ) with a stride of 1. In anal-

24

of the quantum model by measuring the expectation of
Pauli-Z on this ﬁnal qubit. Measurement and parameter
control are enacted via our tfq.layers.PQC object. The
code for this model is shown below:

re ado ut_ op era tor s = cirq . Z (
c lu s te r _s t at e _b i ts [ -1])
quantum_model = tfq . layers . PQC (

c r e a t e _ m o d e l _ c i r c u i t ( cl u st e r_ s ta t e_ b it s ) ,
re ado ut_ op era tor s ) ( cluster_state )

qcnn_model = tf . keras . Model (

inputs =[ excitation_input ] ,
outputs =[ quantum_model ])

In the code, create_model_circuit is a function which ap-
plies the successive layers of quantum convolution and
quantum pooling. A simpliﬁed version of the resulting
model on 4 qubits is shown in Fig. 15.

With the model constructed, we turn to training and
validation. These steps can be accomplished using stan-
dard Keras tools. During training, the model output on
each quantum datapoint is compared against the label;
the cost function used is the mean squared error between
the model output and the label, where the mean is taken
over each batch from the dataset. The training and vali-
dation code is shown below:

qcnn_model . compile ( optimizer = tf . keras . Adam ,

loss = tf . losses . mse )

( train_excitations , train_labels ,
test_excitations , test_labels
) = generate_data ( c l us te r _s t at e _b i ts )

history = qcnn_model . fit (

x = train_excitations ,
y = train_labels ,
batch_size =16 ,
epochs =25 ,
validation_data =(

test_excitations , test_labels ) )
In the code, the generate_data function builds the exci-
tation circuits that are applied to the initial cluster state
input, along with the associated labels. The loss plots
for both the training and validation datasets can be gen-
erated by running the associated example notebook.

We now consider a hybrid classiﬁer.

Instead of us-
ing quantum layers to pool all the way down to 1 qubit,
we can truncate the QCNN and measure a vector of op-
erators on the remaining qubits. The resulting vector
of expectation values is then fed into a classical neural
network. This hybrid model is shown schematically in
Fig. 16.

This can be achieved in TFQ with a few simple mod-
iﬁcations to the previous model, implemented with the
code below:

# Build multi - readout quantum layer
readouts = [ cirq . Z ( bit ) for bit in

c lu s te r _s t at e _b i ts [4:]]

q ua n tu m _m o de l _d u al = tfq . layers . PQC (
m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

# Build classical neural network layers

Figure 15. Architecture of the purely quantum CNN for de-
tecting excited cluster states.

ogy with classical convolutional layers, the parameters of
the unitaries are tied, so that the same operation is ap-
plied to every nearest-neighbor pair of qubits. Pooling is
achieved using a diﬀerent 2 qubit unitary G((cid:126)φ) designed
to disentangle, allowing information to be projected from
2 qubits down to 1. The code below deﬁnes the quantum
convolution and quantum pooling operations:

def q u a n t u m _ c o n v _ c i r c u i t ( bits , syms ) :

circuit = cirq . Circuit ()
for a , b in zip ( bits [0::2] , bits [1::2]) :

circuit += two_q_unitary ([ a , b ] , syms )
for a , b in zip ( bits [1::2] , bits [2::2] + [
bits [0]]) :

circuit += two_q_unitary ([ a , b ] , syms )

return circuit

def q u a n t u m _ p o o l _ c i r c u i t ( srcs , snks , syms ) :

circuit = cirq . Circuit ()
for src , snk in zip ( srcs , snks ) :

circuit += two_q_pool ( src , snk , syms )

return circuit

In the code, two_q_unitary constructs a general param-
eterized two qubit unitary [113], while two_q_pool rep-
resents a CNOT with general one qubit unitaries on the
control and target qubits, allowing for variational selec-
tion of control and target basis.

With the quantum portion of our model deﬁned, we
move on to the third and fourth stages of the QML
pipeline, measurement and classical post-processing. We
consider three classiﬁer variants, each containing a dif-
ferent degree of hybridization with classical networks:

1. Purely quantum CNN
2. Hybrid CNN in which the outputs of a truncated
QCNN are fed into a standard densely connected
neural net

3. Hybrid CNN in which the outputs of multiple trun-
cated QCNNs are fed into a standard densely con-
nected neural net

The ﬁrst model we construct uses only quantum op-
erations to decorrelate the inputs. After preparing the
cluster state dataset on N = 8 qubits, we repeatedly ap-
ply the quantum convolution and pooling layers until the
system size is reduced to 1 qubit. We average the output

1-111Prepare Cluster StateQconv  + QPoolQconv  + QPoolMSELoss-1125

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

QCNN_3 = tfq . layers . PQC (

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _s t at e _b i ts ) ,
readouts ) ( cluster_state )

# Feed all QCNNs into a classical NN
concat_out = tf . keras . layers . concatenate (

[ QCNN_1 , QCNN_2 , QCNN_3 ])

dense_1 = tf . keras . layers . Dense (8) ( concat_out )
dense_2 = tf . keras . layers . Dense (1) ( dense_1 )
mu lti _qc on v_m ode l = tf . keras . Model (
inputs =[ excitation_input ] ,
outputs =[ dense_2 ])

We ﬁnd that that for the same optimization settings, the
purely quantum model trains the slowest, while the three-
quantum-ﬁlter hybrid model trains the fastest. This data
is shown in Fig. 18. This demonstrates the advantage
of exploring hybrid quantum-classical architectures for
classifying quantum data.

Figure 16. A simple hybrid architecture in which the outputs
of a truncated QCNN are fed into a classical neural network.

Figure 17. A hybrid architecture in which the outputs of
3 separate truncated QCNNs are fed into a classical neural
network.

Figure 18. Mean squared error loss as a function of training
epoch for three diﬀerent hybrid classiﬁers. We ﬁnd that the
purely quantum classiﬁer trains the slowest, while the hybrid
architecture with multiple quantum ﬁlters trains the fastest.

d1_dual = tf . keras . layers . Dense (8) (

q ua n tu m _ m o de l _d u al )

d2_dual = tf . keras . layers . Dense (1) ( d1_dual )
hybrid_model = tf . keras . Model ( inputs =[

ex cit a tio n _input ] , outputs =[ d2_dual ])

In the code, multi_readout_model_circuit applies just one
round of convolution and pooling, reducing the system
size from 8 to 4 qubits. This hybrid model can be trained
using the same Keras tools as the purely quantum model.
Accuracy plots can be seen in the example notebook.

The third architecture we will explore creates three
independent quantum ﬁlters, and combines the outputs
from all three with a single classical neural network. This
architecture is shown in Fig. 17. This multi-ﬁlter archi-
tecture can be implemented in TFQ as below:

# Build 3 quantum filters
QCNN_1 = tfq . layers . PQC (

m u l t i _ r e a d o u t _ m o d e l _ c i r c u i t (
c lu s te r _ s t at e _b i ts ) ,
readouts ) ( cluster_state )

QCNN_2 = tfq . layers . PQC (

B. Hybrid Machine Learning for Quantum Control

To run this example in the browser through Colab,

follow the link:

research/control/control.ipynb

Recently, neural networks have been successfully de-
ployed for solving quantum control problems ranging
from optimizing gate decomposition, error correction
subroutines, to continuous Hamiltonian controls. To fully
leverage the power of neural networks without being hob-
bled by possible computational overhead, it is essential
to obtain a deeper understanding of the connection be-
tween various neural network representations and diﬀer-
ent types of quantum control dynamics. We demonstrate
tailoring machine learning architectures to underlying
quantum dynamics using TFQ in [114]. As a summary
of how the unique functionalities of TFQ ease quantum
control optimization, we list the problem deﬁnition and
required TFQ toolboxes as follows.

Prepare Cluster TestQconv  + QPoolMSELoss1-111-11Prepare Cluster StateQconv  + QPoolMSELossQconv  + QPoolQconv  + QPool1-111-1126

This problem can be solved exactly if H and the re-
lationships between g∗
and xj are well understood and
j
invertible. Alternatively, one can ﬁnd an approximate
solution using a parameterized controller F in the form
of a feed-forward neural network. We can calculate a set
of control pairs of size N : {xi, yi} with i ∈ [N ]. We can
input xi into F which is parameterized by its weight ma-
trices {Wi} and biases {bi} of each ith layer. A successful
training will ﬁnd network parameters given by {Wi} and
{bi} such that for any given input xi the network out-
puts gi which leads to a system output H(gi) (cid:117) yi. This
architecture is shown schematically in Fig. 19

There are two important reasons behind the use of
supervised learning for practical control optimization.
Firstly, not all time-invariant Hamiltonian control prob-
lems permit analytical solutions, so inverting the con-
trol error function map can be costly in computation.
Secondly, realistic deployment of even a time-constant
quantum controller faces stochastic ﬂuctuations due to
noise in the classical electronics and systematic errors
which cause the behavior of the system H to deviate
from ideal. Deploying supervised learning with experi-
mentally measured control pairs will therefore enable the
ﬁnding of robust quantum control solutions facing sys-
tematic control oﬀset and electronic noise. However, this
necessitates seamlessly connecting the output of a clas-
sical neural network with the execution of a quantum
circuit in the laboratory. This functionality is oﬀered by
TFQ through ControlledPQC .

We showcase the use of supervised learning with hy-
brid quantum-classical feed-forward neural networks in
TFQ for the single-qubit gate decomposition problem. A
general unitary transform on one qubit can be speciﬁed
by the exponential

U (φ, θ1, θ2) = e−iφ(cos θ1 ˆZ+sin θ1(cos θ2 ˆX+sin θ2 ˆY )).

(21)

However, it is often preferable to enact single qubit uni-
taries using rotations about a single axis at a time.
Therefore, given a single qubit gate speciﬁed by the vec-
tor of three rotations {φ, θ1, θ2}, we want ﬁnd the control
sequence that implements this gate in the form

U (β, γ, δ) = eiαe−i β

2

ˆZe−i γ

2

ˆY e−i δ

2

ˆZ.

(22)

This is the optimal decomposition, namely the Bloch the-
orem, given any unknown single-qubit unitary into hard-
ware friendly gates which only involve the rotation along
a ﬁxed axis at a time.

The ﬁrst step in the training involves preparing the
training data. Since quantum control optimization only
focuses on the performance in hardware deployment, the
control inputs and output have to be chosen such that
they are experimentally observable. We deﬁne the vec-
tor of expectation values yi = [(cid:104) ˆX(cid:105)xi , (cid:104) ˆY (cid:105)xi , (cid:104) ˆZ(cid:105)xi ] of all
single-qubit Pauli operators given by the quantum state

Figure 19. Architecture for hybrid quantum-classical neural
network model for learning a quantum control decomposition.

Target problems:

1. Learning quantum dynamics.
2. Optimizing quantum control signal with regard to

a cost objective

3. Error mitigation in realistic quantum device

Required TFQ functionalities:

1. Hybrid quantum-classical network model
2. Batch quantum circuit simulator
3. Quantum expectation-based backpropagation
4. Fast classical optimizers, both gradient based and

non-gradient based

We exemplify the importance of appropriately choos-
ing the right neural network architecture for the corre-
sponding quantum control problems with two simple but
realistic control optimizations. The two types of con-
trols we have considered cover the full range of quan-
tum dynamics: constant Hamiltonian evolution vs time
dependent Hamiltonian evolution. In the ﬁrst problem,
we design a DNN to machine-learn (noise-free) control
of a single qubit. In the second problem, we design an
RNN with long-term memory to learn a stochastic non-
Markovian control noise model.

1. Time-Constant Hamiltonian Control

If the underlying system Hamiltonian is time-invariant
the task of quantum control can be simpliﬁed with open-
loop optimization. Since the optimal solution is indepen-
dent of instance by instance control actualization, control
optimization can be done oﬄine. Let x be the input to a
controller, which produces some control vector g = F (x).
This control vector actuates a system which then pro-
duces a vector output H(g). For a set of control inputs
xj and desired outputs yj, we can then deﬁne controller
error as ej(F ) = |yj − H(F (xj))|. The optimal control
problem can then be deﬁned as minimizing (cid:80)
j ej(F ) for
F . This optimal controller will produce the optimal con-
trol vector g∗
j

given xj

MSEprepared by the associated input xi:

27

ˆY e−i δ

2

2

0(cid:105) = ˆU i
|ψi
o |0(cid:105)
|ψ(cid:105)x =e−i β
(cid:104) ˆX(cid:105)x = (cid:104)ψ|x
(cid:104) ˆY (cid:105)x = (cid:104)ψ|x
(cid:104) ˆZ(cid:105)x = (cid:104)ψ|x

2

ˆZe−i γ
ˆX |ψ(cid:105)x ,
ˆY |ψ(cid:105)x ,
ˆZ |ψ(cid:105)x .

ˆZ |ψi

0(cid:105) ,

(23)

(24)

(25)

(26)

(27)

Assuming we have prepared the training dataset, each
set consists of input vectors xi = [φ, θ1, θ2] which derives
from the randomly drawn g, unitaries that prepare each
initial state ˆU i
, and the associated expectation values
0
yi = [(cid:104) ˆX(cid:105)xi, (cid:104) ˆY (cid:105)xi, (cid:104) ˆZ(cid:105)}xi ].

Now we are ready to deﬁne the hybrid quantum-
classical neural network model in Keras with Tensfor-
Flow API. To start, we ﬁrst deﬁne the quantum part of
the hybrid neural network, which is a simple quantum
circuit of three single-qubit gates as follows.

con trol_p arams = sympy . symbols ( ’ theta_ {1:3} ’)
qubit = cirq . GridQubit (0 , 0)
control_circ = cirq . Circuit (

cirq . Rz ( control_params [2]) ( qubit ) ,
cirq . Ry ( control_params [1]) ( qubit ) ,
cirq . Rz ( control_params [0]) ( qubit ) )

We are now ready to ﬁnish oﬀ the hybrid network by
deﬁning the classical part, which maps the target params
to the control vector g = {β, γ, δ}. Assuming we have
deﬁned the vector of observables ops , the code to build
the model is:

circ_in = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

x_in = tf . keras . Input ((3 ,) )
d1 = tf . keras . layers . Dense (128) ( x_in )
d2 = tf . keras . layers . Dense (128) ( d1 )
d3 = tf . keras . layers . Dense (64) ( d2 )
g = tf . keras . layers . Dense (3) ( d3 )
exp_out = tfq . layers . ControlledPQC (

control_circ , ops ) ([ circ_in , x_in ])

Now, we are ready to put everything together to deﬁne
and train a model in Keras. The two axis control model
is deﬁned as follows:

model = tf . keras . Model (

inputs =[ circ_in , x_in ] , outputs = exp_out )

To train this hybrid supervised model, we deﬁne an opti-
mizer, which in our case is the Adam optimizer, with an
appropriately chosen loss function:

model . compile ( tf . keras . Adam , loss = ’ mse ’)

We ﬁnish oﬀ by training on the prepared supervised data
in the standard way:

hist ory _ two _ axis = model . fit (...

The training converges after around 100 epochs as seen
in Fig. 20, which also shows excellent generalization to
validation data.

Figure 20. Mean square error on training dataset, and valida-
tion dataset, each of size 5000, as a function training epoch.

2. Time-dependent Hamiltonian Control

Now we consider a second kind of quantum control
problem, where the actuated system H is allowed to
change in time. If the system is changing with time, the
optimal control g∗ is also generally time varying. Gener-
alizing the discussion of section IV B 1, we can write the
time varying control error given the time-varying con-
troller F (t) as ej(F (t), t) = |yj − H(F (xj, t), t)|. The
optimal control can then be written as g∗(t) = ¯g∗ + δ(t).
This task is signiﬁcantly harder than the problem dis-
cussed in section IV B 1, since we need to learn the hidden
variable δ(t) which can result in potentially highly com-
plex real-time system dynamics. We showcase how TFQ
provides the perfect toolbox for such diﬃcult control op-
timization with an important and realistic problem of
learning and thus compensating the low frequency noise.
One of the main contributions to time-drifting errors
in realistic quantum control is 1/f α- like errors, which
encapsulate errors in the Hamiltonian amplitudes whose
frequency spectrum has a large component at the low
frequency regime. The origin of such low frequency noise
remains largely controversial. Mathematically, we can
parameterize the low frequency noise in the time domain
with the amplitude of the Pauli Z Hamiltonian on each
qubit as:

ˆHlow(t) = αte ˆZ.

(28)

A simple phase control signal is given by ω(t) = ω0t with
the Hamiltonian ˆH0(t) = ω0t ˆZ. The time-dependant
wavefunction is then given by

|ψ(ti)(cid:105) = T[e

(cid:82) ti
0 ( ˆHlow(t)+ ˆH0(t))dt] |+(cid:105) .

(29)

We can attempt to learn the noise parameters α and e
by training a recurrent neural network to perform time-
series prediction on the noise.
In other words, given a
record of expectation values {(cid:104)ψ(ti)| ˆX |ψ(ti)(cid:105)} for ti ∈
{0, δt, 2δt, . . . , T } obtained on state |ψ(t)(cid:105), we want the
RNN to predict the future observables {(cid:104)ψ(ti)| ˆX |ψ(ti)(cid:105)}
for ti ∈ {T, T + δt, T + 2δt, . . . , 2T }.

28

C. Simulating Noisy Circuits

To run this example in the browser through Colab,

follow the link:

docs/tutorials/qcnn.ipynb

1. Background

Noise is present in modern day quantum computers.
Qubits are susceptible to interference from the surround-
ing environment, imperfect fabrication, TLS and some-
times even cosmic rays [115, 116]. Until large scale error
correction is reached, the algorithms of today must be
able to remain functional in the presence of noise. This
makes testing algorithms under noise an important step
for validating quantum algorithms / models will function
on the quantum computers of today.

Target problems:

1. Simulating noisy circuits
2. Compare hybrid quantum-classical model perfor-
mance under diﬀerent types and strengths of noise

Required TFQ functionalities:

1. Serialize Cirq circuits which contain channels
2. Quantum trajectory simulation with qsim

2. Noise in Cirq and TFQ

Noise on a quantum computer impacts the bitstring
samples you are able to measure from it. One intuitive
way to think about noise on a quantum computer is that
it will "insert", "delete" or "replace" gates in random
places. An example is shown in Fig. 22, where the actual
circuit run has one gate corrupted and two gates deleted
compared to the desired circuit.

Figure 22.
Illustration of the eﬀect of noise on a quantum
circuit. On the left is an example circuit on three qubits; on
the right, the same circuit is shown after corruption by noise.
An initial Hadamard gate has been changed to an X gate, and
the ﬁnal Hadamard and CNot have been deleted.

Building oﬀ of this intuition, when dealing with noise,
you are no longer using a single pure state |ψ(cid:105) but instead
dealing with an ensemble of all possible noisy realizations
of your desired circuit, ρ = (cid:80)
j pj |ψj(cid:105) (cid:104)ψj|, where pj gives

Figure 21. Mean square error on LSTM predictions on 500
randomly generated inputs.

There are several possible ways to build such an RNN.
One option is recording several timeseries on a device a-
priori, later training and testing an RNN oﬄine. Another
option is an online method, which would allow for real-
time controller tuning. The oﬄine method will be brieﬂy
explained here, leaving the details of both methods to
the notebook associated with this example.

First, we can use TFQ or Cirq to prepare several time-
series for testing and validation. The function below per-
forms this task using TFQ:

def generate_data ( end_time , timesteps , omega_0 ,

exponent , alpha ) :
t_steps = linspace (0 , end_time , timesteps )
q = cirq . GridQubit (0 , 0)
phase = sympy . symbols ( " phaseshift " )
c = cirq . Circuit ( cirq . H ( q ) ,

cirq . Rz ( phase_s ) ( q ) )

ops = [ cirq . X ( q ) ]
phases = t_steps * omega_0 +

t_steps **( exponent + 1) /( exponent + 1)

return tfq . layers . Expectation () (

c ,
symbol_names = [ phase ] ,
symbol_values = transpose ( phases ) ,
operators = ops )

We can use this function to prepare many realizations of
the noise process, which can be fed to an LSTM deﬁned
using tf.keras as below:

model = tf . keras . Sequential ([

tf . keras . layers . LSTM (

rnn_units ,
r e c u r r e n t _ i n i t i a l i z e r = ’ glorot_uniform ’ ,
ba t ch _i n put _s hap e =[ batch_size , None , 1]) ,

tf . keras . layers . Dense (1) ])

We can then train this LSTM on the realizations and
evaluate the success of our training using validation data,
on which we calculate prediction accuracy. A typical ex-
ample of this is shown in Fig. 21. The LSTM converges
quickly to within the accuracy from the expectation value
measurements within 30 epochs.

the probability that the actual state of the quantum com-
puter after your noisy circuit is |ψj(cid:105) .

Revisiting Fig. 22, if we knew beforehand that 90% of
the time our system executed perfectly, or errored 10%
of the time with just this one mode of failure, then our
ensemble would be:

ρ = 0.9 |ψdesired(cid:105) (cid:104)ψdesired| + 0.1 |ψnoisy(cid:105) (cid:104)ψnoisy| .
If there was more than just one way that our circuit could
error, then the ensemble ρ would contain more than just
two terms (one for each new noisy realization that could
happen). ρ is referred to as the density operator describ-
ing your noisy system.

in practice,

Unfortunately,

it’s nearly impossible to
know all the ways your circuit might error and their exact
probabilities. A simplifying assumption you can make
is that after each operation in your circuit a quantum
channel is applied. Just like a quantum circuit produces
pure states, a quantum channel produces density opera-
tors. Cirq has a selection of built-in quantum channels
to model the most common ways a quantum circuit can
go wrong in the real world. For example, here is how you
would write down a quantum circuit with depolarizing
noise:

29

3. Training Models with Noise

Now that you have implemented some noisy circuit
simulations in TFQ, you can experiment with how noise
impacts hybrid quantum-classical models. Since quan-
tum devices are currently in the NISQ era, it is important
to compare model performance in the noisy and noiseless
cases, to ensure performance does not degrade excessively
under noise.

A good ﬁrst check to see if a model or algorithm is
robust to noise is to test it under a circuit-wide depolar-
izing model. An example circuit with depolarizing noise
applied is shown in Fig. 23. Applying a channel to a
circuit using the circuit.with_noise(channel) method cre-
ates a new circuit with the speciﬁed channel applied after
each operation in the circuit. In the case of a depolariz-
ing channel, one of {X, Y, Z} is applied with probability
p and an identity (keeping the original operation) with
probability 1 − p.

def x_circuit ( qubits ) :

""" Produces an X wall circuit on ‘ qubits ‘. """
return cirq . Circuit ( cirq . X . on_each (* qubits ) )

def make_noisy ( circuit , p ) :

""" Adds a depolarization channel to all qubits

in ‘ circuit ‘ before measurement . """

Figure 23. Example circuit before and after the application
of circuit-wide depolarizing noise.

return circuit + cirq . Circuit (

cirq . depolarize ( p ) . on_each (
* circuit . all_qubits () ) )

my_qubits = cirq . GridQubit . rect (1 , 2)
my_circuit = x_circuit ( my_qubits )
my_n ois y _ci r cuit = make_noisy ( my_circuit , 0.5)

Both my_circuit and my_noisy_circuit can be mea-
sured, and their outputs compared. In the noiseless case
you would always expect to sample the |11(cid:105) state. But
in the noisy case there is a nonzero probability of sam-
pling |00(cid:105) or |01(cid:105) or |10(cid:105) as well. See the notebook for a
demonstration of this eﬀect.

trajectory methods

With this understanding of how noise can im-
pact circuit execution, we are ready to move on
in TFQ. TensorFlow Quan-
to how noise works
simulate noise,
tum uses
To enable noisy simu-
as described in II F 6.
lations,
the
option is available on
tfq.layers.Sample ,
and
tfq.layers.Expectation . For example, here is how to in-
stantiate and call the tfq.layers.Sample on the noisy cir-
cuit created above:

tfq.layers.SampledExpectation

backend=’noisy’

to

""" Draw samples from ‘ my_noisy_circuit ‘ """
bitstrings = tfq . layers . Sample ( backend = ’ noisy ’) (

my_noisy_circuit , repetitions =1000)

See the notebook for more examples of noisy usage of
TFQ layers.

In the notebook, we show how to create a noisy dataset
based on the XXZ chain dataset described in section
II E 6. This noisy dataset will be called with the func-
tion modelling_circuit . Then, we build a tf.keras.Model
which uses a noisy PQC layer to process the quan-
tum data, along with Dense layers for classical post-
processing. The code to build the model is shown below:

def bui ld_ ker as _mo del ( qubits , depolarize_p =0.) :
""" Prepare a noisy hybrid quantum classical

Keras model . """

spin_input = tf . keras . Input (

shape =() , dtype = tf . dtypes . string )

c i r c u i t _a n d _ r e a d o u t = mo del lin g_c ir cui t (

qubits , 4 , depolarize_p )

if depolarize_p >= 1e -5:

quantum_model = tfq . layers . NoisyPQC (

* circuit_and_readout ,
sample_based = False ,
repetitions =10

) ( spin_input )

else :

quantum_model = tfq . layers . PQC (

* c i r c u i t _ an d _ r e a d o u t ) ( spin_input )

intermediate = tf . keras . layers . Dense (

4 , activation = ’ sigmoid ’) ( quantum_model )

post_process = tf . keras . layers . Dense (1) (

intermediate )

return tf . keras . Model (

inputs =[ spin_input ] ,
outputs =[ post_process ])

The goal of this model is to correctly classify the quantum
datapoints according to their phase. For the XXZ chain,
the two possible phases are the critical metallic phase
and the insulating phase. Fig. 24 shows the accuracy of
the model as a function of number of training epochs, for
both noisy and noiseless training data.

Figure 24. Accuracy versus training epoch for a quantum clas-
siﬁer. The blue line shows the performance of the classiﬁer
when trained on clean quantum data, while the orange line
shows the performance of the classiﬁer given noisy quantum
data. Note that the ﬁnal values for accuracy are approxi-
mately equal between the noisy and noiseless cases.

Notice that when the model is trained on noisy data,
it achieves a similar accuracy as when it is trained on
noiseless data. This shows that QML models can still
succeed at learning despite the presence of mild noise.
You can adapt this workﬂow to examine the performance
of your own QML models in the presence of noise.

D. Quantum Approximate Optimization

To run this example in the browser through Colab,

follow the link:

research/qaoa/qaoa.ipynb

including Hamiltonians of higher-order and continuous-
variable Hamiltonians [49].

30

j

j=1 zpj

In general, the goal of the QAOA for binary variables
is to ﬁnd approximate minima of a pseudo Boolean func-
tion f on n bits, f (z), z ∈ {−1, 1}×n. This function is of-
ten an mth-order polynomial of binary variables for some
positive integer m, e.g., f (z) = (cid:80)
p∈{0,1}m αpzp, where
zp = (cid:81)n
. QAOA has been applied to NP-hard
problems such as Max-Cut [12] or Max-3-Lin-2 [118].
The case where this polynomial is quadratic (m = 2)
has been extensively explored in the literature. It should
be noted that there have also been recent advances us-
ing quantum-inspired machine learning techniques, such
as deep generative models, to produce approximate solu-
tions to such problems [119, 120]. These 2-local problems
will be the main focus in this example. In this tutorial,
we ﬁrst show how to utilize TFQ to solve a MaxCut in-
stance with QAOA with p = 1.

The QAOA approach to optimization ﬁrst starts in
an initial product state |ψ0(cid:105)⊗n and then a tunable gate
sequence produces a wavefunction with a high probability
of being measured in a low-energy state (with respect to
a cost Hamiltonian).

Let us deﬁne our parameterized quantum circuit
ansatz. The canonical choice is to start with a uniform
superposition |ψ0(cid:105)⊗n = |+(cid:105)⊗n = 1√
x∈{0,1}n |x(cid:105)),
hence a ﬁxed state. The QAOA unitary itself then con-
sists of applying

2n ((cid:80)

ˆU (η, γ) =

P
(cid:89)

j=1

e−iηj ˆHM e−iγj ˆHC ,

(30)

j∈V

onto the starter state, where ˆHM = (cid:80)
ˆXj is known
as the mixer Hamiltonian, and ˆHC ≡ f ( ˆZ) is our
cost Hamiltonian, which is a function of Pauli opera-
tors ˆZ = { ˆZj}n
. The resulting state is given by
|Ψηγ(cid:105) = ˆU (η, γ) |+(cid:105)⊗n, which is our parameterized out-
put. We deﬁne the energy to be minimized as the expec-
tation value of the cost Hamiltonian ˆHC ≡ f ( ˆZ), where
ˆZ = { ˆZj}n
with respect to the output parameterized
state.

j=1

j=1

1. Background

Target problems:

In this section, we introduce the basics of Quantum
Approximate Optimization Algorithms and show how to
implement a basic case of this class of algorithms in TFQ.
In the advanced applications section V, we explore how to
apply meta-learning techniques [117] to the optimization
of the parameters of the algorithm.

The Quantum Approximate Optimization Algorithm
was originally proposed to solve instances of the Max-
Cut problem [12]. The QAOA framework has since been
extended to encompass multiple problem classes related
to ﬁnding the low-energy states of Ising Hamiltonians,

1. Train a parameterized quantum circuit for a dis-

crete optimization problem (MaxCut)

2. Minimize a cost function of a parameterized quan-

tum circuit

Required TFQ functionalities:

1. Conversion of simple circuits to TFQ tensors
2. Evaluation of gradients for quantum circuits
3. Use of gradient-based optimizers from TF

2.

Implementation

For the MaxCut QAOA, the cost Hamiltonian function

f is a second order polynomial of the form,

ˆHC = f ( ˆZ) =

(cid:88)

{j,k}∈E

1

2 ( ˆI − ˆZj ˆZk),

(31)

where G = {V, E} is a graph for which we would like to
ﬁnd the MaxCut; the largest size subset of edges (cut
set) such that vertices at the end of these edges belong
to a diﬀerent partition of the vertices into two disjoint
subsets [12].

To train the QAOA, we simply optimize the expecta-
tion value of our cost Hamiltonian with respect to our
parameterized output to ﬁnd (approximately) optimal
parameters; η∗, γ∗ = argmin
η,γL(η, γ) where L(η, γ) =
(cid:104)Ψηγ| ˆHC |Ψηγ(cid:105) is our loss. Once trained, we use the
QPU to sample the probability distribution of measure-
ments of the parameterized output state at optimal an-
gles in the standard basis, x ∼ p(x) = | (cid:104)x|Ψη∗γ∗ (cid:105) |2,
and pick the lowest energy bitstring from those samples
as our approximate optimum found by the QAOA.

Let us walkthrough how to implement such a basic
QAOA in TFQ. The ﬁrst step is to generate an instance
of the MaxCut problem. For this tutorial we generate
a random 3-regular graph with 10 nodes with NetworkX
[121].

# generate a 3 - regular graph with 10 nodes
maxcut_graph = nx . r a n d o m _ r e g u l a r _ g r a p h ( n =10 , d =3)
The next step is to allocate 10 qubits, to deﬁne
the Hadamard layer generating the initial superposition
state, the mixing Hamiltonian HM and the cost Hamil-
tonian HP.
# define 10 qubits
cirq_qubits = cirq . GridQubit . rect (1 , 10)

# create layer of hadamards to initialize the

superposition state of all computational
states

hada mar d _ci r cuit = cirq . Circuit ()
for node in maxcut_graph . nodes () :
qubit = cirq_qubits [ node ]
ha dam a rd_ c ircuit . append ( cirq . H . on ( qubit ) )

# define the two parameters for one block of

QAOA

qao a_param eters = sympy . symbols ( ’a b ’)

# define the the mixing and the cost

Hamiltonians

mixing_ham = 0
for node in maxcut_graph . nodes () :
qubit = cirq_qubits [ node ]
mixing_ham += cirq . PauliString ( cirq . X ( qubit )
)

cost_ham = maxcut_graph . number_of_edges () /2
for edge in maxcut_graph . edges () :
qubit1 = cirq_qubits [ edge [0]]
qubit2 = cirq_qubits [ edge [1]]
cost_ham += cirq . PauliString (1/2*( cirq . Z (
qubit1 ) * cirq . Z ( qubit2 ) ) )

31

With this, we generate the unitaries representing the
quantum circuit

# generate the qaoa circuit
qaoa_circuit = tfq . util . exponential ( operators =
[ cost_ham , mixing_ham ] , coefficients =
qaoa_parameters )

Subsequently, we use these ingredients to build our
model. We note here in this case that QAOA has no
input data and labels, as we have mapped our graph to
the QAOA circuit. To use the TFQ framework we specify
the Hadamard circuit as input and convert it to a TFQ
tensor. We may then construct a tf.keras model using
our QAOA circuit and cost in a TFQ PQC layer, and
use a single instance sample for training the variational
parameters of the QAOA with the Hadamard gates as an
input layer and a target value of 0 for our loss function,
as this is the theoretical minimum of this optimization
problem.

This translates into the following code:

# define the model and training data
model_circuit , model_readout = qaoa_circuit ,

cost_ham

input_ = [ hadamard_circuit ]
input_ = tfq . con ver t_ to_ ten sor ( input_ )
optimum = [0]

# Build the Keras model .
optimum = np . array ( optimum )
model = tf . keras . Sequential ()
model . add ( tf . keras . layers . Input ( shape =() , dtype =

tf . dtypes . string ) )

model . add ( tfq . layers . PQC ( model_circuit ,

model_readout ) )

To optimize the parameters of the ansatz state, we use
a classical optimization routine. In general, it would be
possible to use pre-calculated parameters [122] or to im-
plement for QAOA tailored optimization routines [123].
For this tutorial, we choose the Adam optimizer imple-
mented in TensorFlow. We also choose the mean absolute
error as our loss function.

model . compile ( loss = tf . keras . losses .

mean_absolute_error , optimizer = tf . keras .
optimizers . Adam () )

history = model . fit ( input_ , optimum , epochs =1000 ,

verbose =1)

V. ADVANCED QUANTUM APPLICATIONS

The following applications represent how we have ap-
plied TFQ to accelerate their discovery of new quantum
algorithms. The examples presented in this section are
newer research as compared to the previous section, as
such they have not had as much time for feedback from
the community. We include these here as they are demon-
stration of the sort of advanced QML research that can be
accomplished by combining several building blocks pro-
vided in TFQ. As many of these examples involve the
building and training of hybrid quantum-classical models

32

the choice of parameters after each iteration of quantum-
classical optimization can be seen as the task of generat-
ing a sequence of parameters which converges rapidly to
an approximate optimum of the landscape, we can use a
type of classical neural network that is naturally suited
to generate sequential data, namely, recurrent neural net-
works. This technique was derived from work by Deep-
Mind [117] for optimization of classical neural networks
and was extended to be applied to quantum neural net-
works [46].

The application of such classical learning to learn tech-
niques to quantum neural networks was ﬁrst proposed in
[46].
In this work, an RNN (long short term memory;
LSTM) gets fed the parameters of the current iteration
and the value of the expectation of the cost Hamiltonian
of the QAOA, as depicted in Fig. 25. More precisely, the
RNN receives as input the previous QNN query’s esti-
mated cost function expectation yt ∼ p(y|θt), where yt
is the estimate of (cid:104) ˆH(cid:105)t
, as well as the parameters for
which the QNN was evaluated θt. The RNN at this time
step also receives information stored in its internal hid-
den state from the previous time step ht. The RNN itself
has trainable parameters ϕ, and hence it applies the pa-
rameterized mapping

ht+1, θt+1 = RNNϕ(ht, θt, yt)

(32)

which generates a new suggestion for the QNN parame-
ters as well as a new hidden state. Once this new set of
QNN parameters is suggested, the RNN sends it to the
QPU for evaluation and the loop continues.

The RNN is trained over random instances of QAOA
problems selected from an ensemble of possible QAOA
MaxCut problems. See the notebook for full details on
the meta-training dataset of sampled problems.

The loss function we chose for our experiments is the
observed improvement at each time step, summed over
the history of the optimization:

L(ϕ) = Ef,y

(cid:20) T
(cid:80)
t=1

(cid:21)
min{f (θt) − minj<t[f (θj)], 0}

, (33)

The observed improvement at time step t is given by the
diﬀerence between the proposed value, f (θt), and the
best value obtained over the history of the optimization
until that point, minj<t[f (θj)].

In our particular example in this section, we will con-
sider a time horizon of 5 time steps, hence the RNN will
have to learn to very rapidly approximately optimize the
parameters of the QAOA. Results are featured in Fig. 26.
The details of the implementation are available in the Co-
lab. Here is an overview of the problem that was tackled
and the TFQ features that facilitated this implementa-
tion:

Target problems:

1. Learning to learn with quantum neural networks

via classical neural networks

Figure 25. Quantum-classical computational graph for the
meta-learning optimization of the recurrent neural network
(RNN) optimizer and a quantum neural network (QNN) over
several optimization iterations. The hidden state of the RNN
is represented by h, we represent the ﬂow of data used to eval-
uate the meta-learning loss function. This meta loss function
L is a functional of the history of expectation value estimate
samples y = {yt}T
t=1, it is not directly dependent on the RNN
parameters ϕ. TFQ’s hybrid quantum-classical backpropaga-
tion then becomes key to train the RNN to learn to optimize
the QNN, which in our particular example was the QAOA.
Figure taken from [46], originally inspired from [117].

and advanced optimizers, such research would be much
more diﬃcult to implement without TFQ. In our re-
searchers’ experience, the performance gains and the ease
of use of TFQ decreased the time-to-working-prototype
from weeks to days or even hours when it is compared to
using alternative tools.

Finally, as we would like to provide users with ad-
vanced examples to see TFQ in action for research use-
cases beyond basic implementations, along with the ex-
amples presented in this section are several notebooks
accessible on Github:

github.com/tensorflow/quantum/tree/research

We encourage readers to read the section below for an
overview of the theory and use of TFQ functions and
would encourage avid readers who want to experiment
with the code to visit the full notebooks.

A. Meta-learning for Variational Quantum
Optimization

To run this example in the browser through Colab,

follow the link:

research/metalearning_qaoa/metalearning_qaoa.ipynb

In section IV D, we have shown how to implement basic
QAOA in TFQ and optimize it with a gradient-based
optimizer, we can now explore how to leverage classical
neural networks to optimize QAOA parameters. To run
this example in the browser via Colab, follow the link:

In recent works, the use of classical recurrent neural
networks to learn to optimize the parameters [46] (or gra-
dient descent hyperparameters [124]) was proposed. As

33

overwhelming volume of quantum space with an expo-
nentially small gradient, making straightforward training
impossible if on enters one of these dead regions. The rate
of this vanishing increases exponentially with the num-
ber of qubits and depends on whether the cost function
is global or local [128]. While strategies have been devel-
oped to deal with the challenges of vanishing gradients
classically [129], the combination of diﬀerences in read-
out complexity and other constraints of unitarity make
direct implementation of these ﬁxes challenging. In par-
ticular, the readout of information from a quantum sys-
tem has a complexity of O(1/(cid:15)α) where (cid:15) is the desired
precision, and α is some small integer, while the complex-
ity of the same task classically often scales as O(log 1/(cid:15))
[130]. This means that for a vanishingly small gradi-
ent (e.g. 10−7), a classical algorithm can easily obtain
at least some signal, while a quantum-classical one may
diﬀuse essentially randomly until ∼ 1014 samples have
been taken. This has fundamental consequences for the
methods one uses to train, as we detail below. The re-
quirement on depth to reach these plateaus is only that
a portion of the circuit approximates a unitary 2−design
which can occur at a depth occurring at O(n1/d) where
n is the number of qubits and d is the dimension of the
connectivity of the quantum circuit, possibly requiring as
little depth as O(log(n)) in the all-to-all case [84]. One
may imagine that a solution to this problem could be
to simply initialize a circuit to the identity to avoid this
problem, but this incurs some subtle challenges. First,
such a ﬁxed initialization will tend to bias results on gen-
eral datasets. This challenge has been studied in the
context of more general block initialization of identity
schemes [131].

Perhaps the more insidious way this problem arises, is
that training with a method like stochastic gradient de-
scent (or sophisticated variants like Adam) on the entire
network, can accidentally lead one onto a barren plateau
if the learning rate is too high. This is due to the fact
that the barren plateaus argument is one of volume of
space and quantum-classical information exchange, and
random diﬀusion in parameter space will tend to lead
one onto a plateau. This means that even the most
clever initialization can be thwarted by the impact of
this phenomenon on the training process. In practice this
severely limits learning rate and hence training eﬃciency
of QNNs.

For this reason, one can consider training on subsets of
the network which do not have the ability to completely
randomize during a random walk. This layerwise learning
strategy allows one to use larger learning rates and im-
proves training eﬃciency in quantum circuits [132]. We
advocate the use of these strategies in combination with
appropriately designed local cost functions in order to cir-
cumvent the dramatically worse problems with objectives
like ﬁdelity [126, 128]. TFQ has been designed to make
experimenting with both of these strategies straightfor-
ward for the user, as we now document. For an example

Figure 26. The path chosen by the RNN optimizer on a 12-
qubit MaxCut problem after being trained on a set of random
10-qubit MaxCut problems. We see that the neural network
learned to generalize its heuristic to larger system sizes, as
originally pointed out in [46].

2. Building a neural-network-based optimizer

for

QAOA

3. Lowering the number of iterations needed to opti-

mize QAOA

Required TFQ functionalities:

1. Hybrid quantum-classical networks and hybrid

backpropagation

2. Batching training over quantum data (QAOA prob-

lem instances)

3. Integration with TF for the classical RNN

B. Vanishing Gradients and Adaptive Layerwise
Training Strategies

1. Random Quantum Circuits and Barren Plateaus

When using parameterized quantum circuits for a
learning task, inevitably one must choose an initial con-
ﬁguration and training strategy that is compatible with
that initialization. In contrast to problems more known
structure, such as speciﬁc quantum simulation tasks [25]
or optimizations [125], the structure of circuits used for
learning may need to be more adaptive or general to en-
compass unknown data distributions.
In classical ma-
chine learning, this problem can be partially addressed
by using a network with suﬃcient expressive power and
random initialization of the network parameters.

Unfortunately, it has been proven that due to funda-
mental limits on quantum readout complexity in com-
bination with the geometry of the quantum space, an
exact duplication of this strategy is doomed to fail [126].
In particular, in analog to the vanishing gradients prob-
lem that has plagued deep classical networks and histor-
ically slowed their progress [127], an exacerbated version
of this problem appears in quantum circuits of suﬃcient
depth that are randomly initialized. This problem, also
known as the problem of barren plateaus, refers to the

of barren plateaus, see the notebook at the following link:

docs/tutorials/barren_plateaus.ipynb

2. Layerwise quantum circuit learning

So far, the network training methods demonstrated in
section IV have focused on simultaneous optimization of
all network parameters. As alluded to in the section on
the barren plateau eﬀect (V B), this type of strategy can
lead to vanishing gradients as the number of qubits and
layers in a random circuit grows. A parameter initializa-
tion strategy to avoid this eﬀect at the initial training
steps has been proposed in [131]. This strategy initial-
izes layers in a blockwise fashion such that pairs of layers
are randomly initialized but yield an identity operation
when acting on the circuit and thereby prevent initializa-
tion on a plateau. However, as described in section V B,
this does not avert the possibility for the circuit to drift
onto a plateau during training. In this section, we will
implement a layerwise training strategy that avoids ini-
tialization on a plateau as well as drifting onto a plateau
during training by only training subsets of parameters in
the circuit at a given update step [132].

The strategy consists of two training phases. In phase
one, the algorithm constructs the circuit by subsequently
adding and training layers of a predeﬁned structure. We
start by picking a number of initial layers s, that con-
tains parameters which are always active during training
to avoid entering a regime where the number of active
parameters is too small to decrease the loss [133]. These
layers are then trained for a ﬁxed number of iterations el,
after which another set of layers is added to the circuit.
How many layers this set contains is controlled by a hy-
perparameter p. Another hyperparameter q determines
after how many layers the parameters in previous layers
are frozen. I.e., for p = 2 and q = 4, we add two layers at
intervals of el iterations, and only train the parameters
in the last four layers, while all other parameters in the
circuit are kept ﬁxed. This procedure is repeated until a
ﬁxed, predeﬁned circuit depth is reached.

In phase two, we take the ﬁnal circuit from phase one
and now split the parameters into larger partitions. A
hyperparameter r speciﬁes the percentage of parameters
which are trained in the circuit simultaneously, i.e., for
r = 0.5 we split the circuit in two halves and alternate
between training each of these two halves where the other
half’s parameters are kept ﬁxed. By this approach, any
randomization eﬀect that occurs during training is con-
tained to a subset of the circuit parameters and eﬀec-
tively prevents drifting onto a plateau, as can be seen in
the original paper for the case of randomization induced
by sampling noise [132]. The size of these partitions has
to be chosen with care, as overly large partitions will
increase the probability of randomization, while overly
small partitions may be insuﬃcient to decrease the loss
[133]. This alternate training of circuit partitions is then

34

performed until the loss converges or we reach a prede-
ﬁned number of iterations.

In the following, we will

implement phase one
of the algorithm, and a complete implementation of
both phases can be found in the accompanying notebook.

Target problems:

1. Dynamically building circuits for arbitrary learning

tasks

2. Manipulating circuit structure and parameters dur-

ing training

3. Reducing the number of trained parameters
4. Avoiding initialization on or drifting to a barren

plateau

Required TFQ functionalities:

1. Parameterized circuit layers
2. Keras weight manipulation interface
3. Parameter shift diﬀerentiator for exact gradient

computation

To run this example in the browser through Colab,

follow the link:

research/layerwise_learning/layerwise_learning.ipynb

As an example to show how this functionality may be
explored in TFQ, we will look at randomly generated
layers as shown in section V B, where one layer consists
of a randomly chosen rotation gate around the X, Y , or
Z axis on each qubit, followed by a ladder of CZ gates
over all qubits.

def create_layer ( qubits , layer_id ) :

# create symbols for trainable parameters
symbols = [

sympy . Symbol (

f ’{ layer_id } -{ str ( i ) } ’)

for i in range ( len ( qubits ) ) ]

# build layer from random gates
gates = [

random . choice ([

cirq . Rx , cirq . Ry , cirq . Rz ]) (
symbols [ i ]) ( q )

for i , q in enumerate ( qubits ) ]

# add connections between qubits
for control , target in zip (
qubits , qubits [1:]) :
gates . append ( cirq . CZ ( control , target ) )

return gates , symbols

We assume that we don’t know the ideal circuit struc-
ture to solve our learning problem, so we start with the
shallowest circuit possible and let our model grow from
there. In this case we start with one initial layer s = 1,
and add a new layer after it has trained for el = 10
epochs. First, we need to specify some variables:

# number of qubits and layers in our circuit
n_qubits = 6
n_layers = 8

# define data and readout qubits
data_qubits = cirq . GridQubit . rect (1 , n_qubits )
readout = cirq . GridQubit (0 , n_qubits -1)
readout_op = cirq . Z ( readout )

# symbols to parametrize circuit
symbols = []
layers = []
weights = []

We use the same training data as speciﬁed in the TFQ
MNIST classiﬁer example notebook available in the TFQ
Github repository, which encodes a downsampled version
of the digits into binary vectors. Ones in these vectors
are encoded as local X gates on the corresponding qubit
in the register, as shown in [24]. For this reason, we also
use the readout procedure speciﬁed in that work where
a sequence of XHX gates is added to the readout qubit
at the end of the circuit. Now we train the circuit, layer
by layer:

for layer_id in range ( n_layers ) :
circuit = cirq . Circuit ()
layer , layer_symbols = create_layer (

data_qubits , f ’ layer_ { layer_id } ’)

layers . append ( layer )

circuit += layers
symbols += layer_symbols

# set up the readout qubit
circuit . append ( cirq . X ( readout ) )
circuit . append ( cirq . H ( readout ) )
circuit . append ( cirq . X ( readout ) )
readout_op = cirq . Z ( readout )

# create the Keras model
model = tf . keras . Sequential ()
model . add ( tf . keras . layers . Input (

shape =() , dtype = tf . dtypes . string ) )

model . add ( tfq . layers . PQC (

model_circuit = circuit ,
operators = readout_op ,
differentiator = ParameterShift () ,
initializer = tf . keras . initializers . Zeros )

)
model . compile (

loss = tf . keras . losses . squared_hinge ,
optimizer = tf . keras . optimizers . Adam (

learning_rate =0.01) )

# Update model parameters and add
# new 0 parameters for new layers .
model . set_weights (

[ np . pad ( weights , ( n_qubits , 0) ) ])

model . fit ( x_train ,
y_train ,
batch_size =128 ,
epochs =10 ,
verbose =1 ,
validation_data =( x_test , y_test ) )

qnn_results = model . evaluate ( x_test , y_test )

# store weights after training a layer
weights = model . get_weights () [0]

In general, one can choose many diﬀerent conﬁgura-

35

tions of how many layers should be trained in each step.
One can also control which layers are trained by manip-
ulating the symbols we feed into the circuit and keeping
track of the weights of previous layers. The number of
layers, layers trained at a time, epochs spent on a layer,
and learning rate are all hyperparameters whose optimal
values depend on both the data and structure of the cir-
cuits being used for learning. This example is meant to
exemplify how TFQ can be used to easily explore these
choices to maximize the eﬃciency and eﬃcacy of training.
See our notebook linked above for the complete imple-
mentation of these features. Using TFQ to explore this
type of learning strategy relieves us of manually imple-
menting training procedures and optimizers, and autod-
iﬀerentiation with the parameter shift rule. It also lets us
readily use the rich functionality provided by TensorFlow
and Keras. Implementing and testing all of the function-
ality needed in this project by hand could take up to a
week, whereas all this eﬀort reduces to a couple of lines of
code with TFQ as shown in the notebook. Additionally,
it lets us speed up training by using the integrated qsim
simulator as shown in section II F 4. Last but not least,
TFQ provides a thoroughly tested and maintained QML
framework which greatly enhances the reproducibility of
our research.

C. Hamiltonian Learning with Quantum Graph
Recurrent Neural Networks

1. Motivation: Learning Quantum Dynamics with a
Quantum Representation

Quantum simulation of time evolution was one of the
original envisioned applications of quantum computers
when they were ﬁrst proposed by Feynman [9]. Since
then, quantum time evolution simulation methods have
seen several waves of great progress, from the early days
of Trotter-Suzuki methods, to methods of qubitization
and randomized compiling [87], and ﬁnally recently with
some methods for quantum variational methods for ap-
proximate time evolution [134].

The reason that quantum simulation has been such
a focus of the quantum computing community is be-
cause we have some indications to believe that quan-
tum computers can demonstrate a quantum advantage
when evolving quantum states through unitary time evo-
lution; the classical simulation overhead scales exponen-
tially with the depth of the time evolution.

As such, it is natural to consider if such a potential
quantum simulation advantage can be extended to the
realm of quantum machine learning as an inverse prob-
lem, that is, given access to some black-box dynamics,
can we learn a Hamiltonian such that time evolution un-
der this Hamiltonian replicates the unknown dynamics.
This is known as the problem of Hamiltonian learning,
or quantum dynamics learning, which has been studied
in the literature [66, 135]. Here, we use a Quantum Neu-

ral Network-based approach to learn the Hamiltonian of
a quantum dynamical process, given access to quantum
states at various time steps.

As was pointed out in the barren plateaus section V B,
attempting to do QML with no prior on the physics of
the system or no imposed structure of the ansatz hits the
quantum version of the no free lunch theorem; the net-
work has too high a capacity for the problem at hand and
is thus hard to train, as evidenced by its vanishing gra-
dients. Here, instead, we use a highly structured ansatz,
from work featured in [136]. First of all, given that we
know we are trying to replicate quantum dynamics, we
can structure our ansatz to be based on Trotter-Suzuki
evolution [86] of a learnable parameterized Hamiltonian.
This eﬀectively performs a form of parameter-tying in
our ansatz between several layers representing time evo-
lution. In a previous example on quantum convolutional
networks IV A 1, we performed parameter tying for spa-
tial translation invariance, whereas here, we will assume
the dynamics remain constant through time, and per-
form parameter tying across time, hence it is akin to a
quantum form of recurrent neural networks (RNN). More
precisely, as it is a parameterization of a Hamiltonian evo-
lution, it is akin to a quantum form of recently proposed
models in classical machine learning called Hamiltonian
neural networks [137].

Beyond the quantum RNN form, we can impose further
structure. We can consider a scenario where we know we
have a one-dimensional quantum many-body system. As
Hamiltonians of physical have local couplings, we can
use our prior assumptions of locality in the Hamiltonian
and encode this as a graph-based parameterization of the
Hamiltonian. As we will see below, by using a Quantum
Graph Recurrent Neural network [136] implemented in
TFQ, we will be able to learn the eﬀective Hamiltonian
topology and coupling strengths quite accurately, simply
from having access to quantum states at diﬀerent times
and employing mini-batched gradient-based training.

Before we proceed, it is worth mentioning that the ap-
proach featured in this section is quite diﬀerent from the
learning of quantum dynamics using a classical RNN fea-
ture in previous example section IV B. As sampling the
output of a quantum simulation at diﬀerent times can be-
come exponentially hard, we can imagine that for large
systems, the Quantum RNN dynamics learning approach
could have primacy over the classical RNN approach,
thus potentially demonstrating a quantum advantage of
QML over classical ML for this problem.

Target problems:

1. Preparing low-energy states of a quantum system
2. Learning Quantum Dynamics using a Quantum

Neural Network Model

Required TFQ functionalities:

36

2. Training multi-layered quantum neural networks

with shared parameters

3. Batching QNN training data (input-output pairs
and time steps) for supervised learning of quantum
unitary map

2.

Implementation

Please see the tutorial notebook for full code details:

research/qgrnn_ising/qgrnn_ising.ipynb

Here we provide an overview of our implementa-
tion. We can deﬁne a general Quantum Graph Neu-
ral Network as a repeating sequence of exponentials
of a Hamiltonian deﬁned on a graph, ˆUqgnn(η, θ) =
q=1 e−iηpq ˆHq(θ)(cid:3) where the ˆHq(θ) are generally
(cid:81)P
2-local Hamiltonians whose coupling topology is that of
an assumed graph structure.

(cid:2) (cid:81)Q

p=1

v

In our Hamiltonian learning problem, we aim to learn
a target ˆHtarget which will be an Ising model Hamiltonian
with Jjk as couplings and Bv for site bias term of each
spin, i.e., ˆHtarget = (cid:80)
ˆXv,
given access to pairs of states at diﬀerent times that were
subjected to the target time evolution operator ˆU (T ) =
e−i ˆHtargetT .

j,k Jjk ˆZj ˆZk + (cid:80)

v Bv ˆZv + (cid:80)

{j,k}∈E θjk ˆZj ˆZk + (cid:80)

We will use a recurrent form of QGNN, using Hamil-
tonian generators ˆH1(θ) = (cid:80)
v∈V αv ˆXv and ˆH2(θ) =
(cid:80)
v∈V φv ˆZv, with trainable param-
eters1 {θjk, φv, αv}, for our choice of graph structure
prior G = {V, E}. The QGRNN is then resembles ap-
plying a Trotterized time evolution of a parameterized
Ising Hamiltonian ˆH(θ) = ˆH1(θ) + ˆH2(θ) where P is the
number of Trotter steps. This is a good parameteriza-
tion to learn the eﬀective Hamiltonian of the black-box
dynamics as we know from quantum simulation theory
that Trotterized time evolution operators can closely ap-
proximate true dynamics in the limit of |ηjk| → 0 while
P → ∞.

For our TFQ software implementation, we can initial-
ize Ising model & QGRNN model parameters as random
values on a graph. It is very easy to construct this kind of
graph structure Hamiltonian by using Python NetworkX
library.

N = 6
dt = 0.01
# Target Ising model parameters
G_ising = nx . cycle_graph ( N )
ising_w = [ dt * np . random . random () for _ in G .

edges ]

ising_b = [ dt * np . random . random () for _ in G .

nodes ]

Because the target Hamiltonian and its nearest-neighbor
graph structure is unknown to the QGRNN, we need to

1. Quantum compilation of exponentials of Hamilto-

nians

1 For simplicity we set αv to constant 1’s in this example.

initialize a new random graph prior for our QGRNN. In
this example we will use a random 4-regular graph with
a cycle as our prior. Here, params is a list of trainable
parameters of the QGRNN.

# QGRNN model parameters
G_qgrnn = nx . r a n d o m _ r e g u l a r _ g r a p h ( n =N , d =4)
qgrnn_w = [ dt ] * len ( G_qgrnn . edges )
qgrnn_b = [ dt ] * len ( G_qgrnn . nodes )
theta = [ ’ theta {} ’. format ( e ) for e in G . edges ]
phi = [ ’ phi {} ’. format ( v ) for v in G . nodes ]
params = theta + phi

Now that we have the graph structure, weights of edges
& nodes, we can construct Cirq-based Hamiltonian oper-
ator which can be directly calculated in Cirq and TFQ.
To create a Hamiltonian by using cirq.PauliSum ’s or
cirq.PauliString ’s, we need to assign appropriate qubits
on them. Let’s assume Hamiltonian() is the Hamilto-
nian preparation function to generate cost Hamiltonian
from interaction weights and mixer Hamiltonian from
bias terms. We can bring qubits of Ising & QGRNN
models by using cirq.GridQubit .

qubits_ising = cirq . GridQubit . rect (1 , N )
qubits_qgrnn = cirq . GridQubit . rect (1 , N , 0 , N )
ising_cost , ising_mixer = Hamiltonian (

G_ising , ising_w , ising_b , qubits_ising )

qgrnn_cost , qgrnn_mixer = Hamiltonian (

G_qgrnn , qgrnn_w , qgrnn_b , qubits_qgrnn )

To train the QGRNN, we need to create an ensemble
of states which are to be subjected to the unknown dy-
namics. We chose to prepare a low-energy states by ﬁrst
performing a Variational Quantum Eigensolver (VQE)
[29] optimization to obtain an approximate ground state.
Following this, we can apply diﬀerent amounts of simu-
lated time evolutions onto to this state to obtain a varied
dataset. This emulates having a physical system in a low-
energy state and randomly picking the state at diﬀerent
times. First things ﬁrst, let us build a VQE model

def VQE ( H_target , q )

# Parameters
x = [ ’x {} ’. format ( i ) for i , _ in enumerate ( q ) ]
z = [ ’z {} ’. format ( i ) for i , _ in enumerate ( q ) ]
symbols = x + z
circuit = cirq . Circuit ()
circuit . append ( cirq . X ( q_ ) ** sympy . Symbol ( x_ )

for q_ , x_ in zip (q , x ) )

circuit . append ( cirq . Z ( q_ ) ** sympy . Symbol ( z_ )

for q_ , z_ in zip (q , z ) )

Now that we have a parameterized quantum circuit,
we can minimize the expectation value of given Hamil-
tonian. Again, we can construct a Keras model with
Expectation . Because the output expectation values are
calculated respectively, we need to sum them up at the
last.

circuit_input = tf . keras . Input (

shape =() , dtype = tf . string )
output = tfq . layers . Expectation () (

circuit_input ,
symbol_names = symbols ,
operators = tfq . co nv ert _to _te nso r (

37

[ H_target ]) )
output = tf . math . reduce_sum (

output , axis = -1 , keepdims = True )

Finally, we can get approximated lowest energy states
of the VQE model by compiling and training the above
Keras model.2

model = tf . keras . Model (

inputs = circuit_input , outputs = output )

adam = tf . keras . optimizers . Adam (

learning_rate =0.05)

low_bound = - np . sum ( np . abs ( ising_w + ising_b ) )

- N

inputs = tfq . con ver t_t o_ ten sor ([ circuit ])
outputs = tf . co nve rt_ to_ ten sor ([[ low_bound ]])
model . compile ( optimizer = adam , loss = ’ mse ’)
model . fit ( x = inputs , y = outputs ,

batch_size =1 , epochs =100)

params = model . get_weights () [0]
res = { k : v for k , v in zip ( symbols , params ) }
return cirq . re s ol v e_ p ar a me t er s ( circuit , res )

Now that the VQE function is built, we can generate
the initial quantum data input with the low energy states
near to the ground state of the target Hamiltonian for
both our data and our input state to our QGRNN.

H_target = ising_cost + ising_mixer
low_energy_ising = VQE ( H_target , qubits_ising )
low_energy_qgrnn = VQE ( H_target , qubits_qgrnn )

The QGRNN is fed the same input data as the
true process. We will use gradient-based training over
minibatches of randomized timesteps chosen for our
QGRNN and the target quantum evolution. We will
thus need to aggregate the results among the diﬀerent
timestep evolutions to train the QGRNN model. To cre-
ate these time evolution exponentials, we can use the
tfq.util.exponential function to exponentiate our target
and QGRNN Hamiltonians3

exp_ising_cost = tfq . util . exponential (

operators = ising_cost )

exp_ising_mix = tfq . util . exponential (

operators = ising_mixer )

exp_qgrnn_cost = tfq . util . exponential (

operators = qgrnn_cost , coefficients = params )

exp_qgrnn_mix = tfq . util . exponential (

operators = qgrnn_mixer )

Here we randomly pick the 15 timesteps and apply the
Trotterized time evolution operators using our above con-
structed exponentials. We can have a quantum dataset
{(|ψTj (cid:105), |φTj (cid:105))|j = 1..M } where M is the number of

2 Here is some tip for training. Setting the output true value to
theoretical lower bound, we can minimize our expectation value
in the Keras model ﬁt framework. That is, we can use the in-
equality (cid:104) ˆHtarget(cid:105) = (cid:80)
jk Jjk(cid:104)Zj Zk(cid:105) + (cid:80)
v(cid:104)Xv(cid:105) ≥
jk(−)|Jjk| − (cid:80)
(cid:80)

v Bv(cid:104)Zv(cid:105) + (cid:80)

v |Bv| − N .

3 Here, we use the terminology cost and mixer Hamiltonians as the
Trotterization of an Ising model time evolution is very similar to
a QAOA, and thus we borrow nomenclature from this analogous
QNN.

38

data, or batch size (in our case we chose M = 15),
|ψTj (cid:105) = ˆU j

target|ψ0(cid:105) and |φTj (cid:105) = ˆU j

qgrnn|ψ0(cid:105).

L(θ, φ) = 1 − 1
B
1 − 1
=
B

j=1 |(cid:104)ψTj |φTj (cid:105)|2
(cid:80)B
j=1(cid:104) ˆZtest(cid:105)j

(cid:80)B

def trotterize ( inp , depth , cost , mix ) :
add = tfq . layers . AddCircuit ()
outp = add ( cirq . Circuit () , append = inp )
for _ in range ( depth ) :

outp = add ( outp , append = cost )
outp = add ( outp , append = mix )

return outp

batch_size = 15
T = np . random . uniform (0 , T_max , batch_size )
depth = [ int ( t / dt ) +1 for t in T ]
true_states = []
pred_states = []
for P in depth :

true_states . append (

trotterize ( low_energy_ising , P ,

exp_ising_cost , exp_ising_mix ) )

pred_states . append (

trotterize ( low_energy_qgrnn , P ,

exp_qgrnn_cost , exp_qgrnn_mix ) )

Now we have both quantum data from (1) the true
time evolution of the target Ising model and (2) the pre-
dicted data state from the QGRNN. In order to maximize
overlap between these two wavefunctions, we can aim to
maximize the ﬁdelity between the true state and the state
output by the QGRNN, averaged over random choices
of time evolution. To evaluate the ﬁdelity between two
quantum states (say |A(cid:105) and |B(cid:105)) on a quantum com-
puter, a well-known approach is to perform the swap test
[138]. In the swap test, an additional observer qubit is
used, by putting this qubit in a superposition and using
it as control for a Fredkin gate (controlled-SWAP), fol-
lowed by a Hadamard on the observer qubit, the observer
qubit’s expectation value in the encodes the ﬁdelity of the
two states, | (cid:104)A|B(cid:105) |2. Thus, right after Fidelity Swap
Test, we can measure the swap test qubit with Pauli ˆZ
operator with Expectation , (cid:104) ˆZtest(cid:105), and then we can cal-
culate the average of ﬁdelity (inner product) between a
batch of two sets of quantum data states, which can be
used as our classical loss function in TensorFlow.

# Check class SwapTestFidelity in the notebook .
fidelity = S wapTestFidelity (

qubits_ising , qubits_qgrnn , batch_size )

state_true = tf . keras . Input ( shape =() ,

state_pred = tf . keras . Input ( shape =() ,

fid_output = fidelity ( state_true , state_pred )
fid_output = tfq . layers . Expectation () (

dtype = tf . string )

dtype = tf . string )

fid_output ,
symbol_names = symbols ,
operators = fidelity . op )

model = tf . keras . Model (

inputs =[ state_true , state_pred ] ,
outputs = fid_output )

Here, we introduce the average ﬁdelity and implement

this with custom Keras loss function.

def average_fidelity ( y_true , y_pred ) :

return 1 - K . mean ( y_pred )

Again, we can use Keras model ﬁt. To feed a batch of
quantum data, we can use tf.concat because the quan-
tum circuits are already in tf.Tensor . In this case, we
know that the lower bound of ﬁdelity is 0, but the y_true
is not used in our custom loss function average_fidelity .
We set learning rate of Adam optimizer to 0.05.

y_true = tf . concat ( true_states , axis =0)
y_pred = tf . concat ( pred_states , axis =0)

model . compile (

loss = average_fidelity ,
optimizer = tf . keras . optimizers . Adam (

learning_rate =0.05) )

model . fit ( x =[ y_true , y_pred ] ,

y = tf . zeros ([ batch_size , 1]) ,
batch_size = batch_size ,
epochs =500)

The full results are displayed in the notebook, we
see for this example that our time-randomized gradient-
based optimization of our parameterized class of quan-
tum Hamiltonian evolution ends up learning the target
Hamiltonian and its couplings to a high degree of accu-
racy.

Figure 27. Left: True (target) Ising Hamiltonian with edges
representing couplings and nodes representing biases. Middle:
randomly chosen initial graph structure and parameter values
for the QGRNN. Right: learned Hamiltonian from the trained
QGRNN.

D. Generative Modelling of Quantum Mixed
States with Hybrid Quantum-Probabilistic Models

1. Background

Often in quantum mechanical systems, one encounters
so-called mixed states, which can be understood as proba-
bilistic mixtures over pure quantum states [139]. Typical
cases where such mixed states arise are when looking at
ﬁnite-temperature quantum systems, open quantum sys-
tems, and subsystems of pure quantum mechanical sys-
tems. As the ability to model mixed states are thus key
to understanding quantum mechanical systems, in this
section, we focus on models to learn to represent and
mimic the statistics of quantum mixed states.

As mixed states are a combination of a classical prob-
ability distribution and quantum wavefunctions, their

statistics can exhibit both classical and quantum forms
of correlations (e.g., entanglement). As such, if we wish
to learn a representation of such mixed state which can
generatively model its statistics, one can expect that a
hybrid representation combining classical probabilistic
models and quantum neural networks can be an ideal.
Such a decomposition is ideal for near-term noisy de-
vices, as it reduces the overhead of representation on the
quantum device, leading to lower-depth quantum neu-
ral networks. Furthermore, the quantum layers provide
a valuable addition in representation power to the clas-
sical probabilistic model, as they allow the addition of
quantum correlations to the model.

Thus, in this section, we cover some examples where
one learns to generatively model mixed states using a
hybrid quantum-probabilistic model [49]. Such models
use a parameterized ansatz of the form

ˆρθφ = ˆU (φ)ˆρθ ˆU †(φ),

ˆρθ =

pθ(x) |x(cid:105)(cid:104)x|

(34)

(cid:88)

x

where ˆU (φ) is a unitary quantum neural network with
parameters φ and pθ(x) is a classical probabilistic model
with parameters θ. We call ˆρθφ the visible state and
ˆρθ the latent state. Note the latent state is eﬀectively a
classical distribution over the standard basis states, and
its only parameters are those of the classical probabilistic
model.

As we shall see below, there are methods to train both
networks simultaneously. In terms of software implemen-
tation, as we have to combine probabilistic models and
quantum neural networks, we will use a combination of
TensorFlow Probability [140] along with TFQ. A ﬁrst
class of application we will consider is the task of gen-
erating a thermal state of a quantum system given its
Hamiltonian. A second set of applications is given sev-
eral copies of a mixed state, learn a generative model
which replicates the statistics of the state.

Target problems:

1. Incorporating probabilistic and quantum models
2. Variational Quantum Simulation of Quantum

Thermal States

3. Learning to generatively model mixed states from

data

Required TFQ functionalities:

1. Integration with TF Probability [140]
2. Sample-based simulation of quantum circuits
3. Parameter shift diﬀerentiator for gradient compu-

tation

2. Variational Quantum Thermalizer

39

Consider the task of preparing a thermal state: given
a Hamiltonian ˆH and a target inverse temperature β =
1/T , we want to variationally approximate the state

ˆσβ = 1
Zβ

e−β ˆH , Zβ = tr(e−β ˆH ),

(35)

using a state of the form presented in equation (34).
That is, we aim to ﬁnd a value of the hybrid model
parameters {θ∗, φ∗} such that ˆρθ∗φ∗ ≈ ˆσβ.
In order
to converge to this approximation via optimization of
the parameters, we need a loss function to optimize
which quantiﬁes statistical distance between these quan-
tum mixed states.
If we aim to minimize the discrep-
ancy between states in terms of quantum relative en-
tropy D(ˆρθφ(cid:107)ˆσβ) = −S(ˆρθφ) − tr(ˆρθφ log ˆσβ), (where
S(ˆρθφ) = −tr(ˆρθφ log ˆρθφ) is the entropy), then, as de-
scribed in the full paper [59] we can equivalently minimize
the free energy4, and hence use it as our loss function:

(36)

Lfe(θ, φ) = βtr(ˆρθφ ˆH) − S(ˆρθφ).
The ﬁrst term is simply the expectation value of the
energy of our model, while the second term is the en-
tropy. Due to the structure of our quantum-probabilistic
model, the entropy of the visible state is equal to the
entropy of the latent state, which is simply the clas-
sical entropy of the distribution, S(ˆρθφ) = S(ˆρθ) =
− (cid:80)
x pθ(x) log pθ(x). This comes in quite useful dur-
ing the optimization of our model.

Let us implement a simple example of the VQT model
which minimizes free energy to achieve an approximation
of the thermal state of a physical system. Let us consider
a two-dimensional Heisenberg spin chain

ˆHheis =

(cid:88)

(cid:104)ij(cid:105)h

Jh ˆSi · ˆSj +

(cid:88)

(cid:104)ij(cid:105)v

Jv ˆSi · ˆSj

(37)

where h (v) denote horizontal (vertical) bonds, while (cid:104)·(cid:105)
represent nearest-neighbor pairings. First, we deﬁne this
Hamiltonian on a grid of qubits:

def get_bond ( q0 , q1 ) :

return cirq . PauliSum . f ro m _p au l i_ s tr i ng s ([

cirq . PauliString ( cirq . X ( q0 ) , cirq . X ( q1 ) ) ,
cirq . PauliString ( cirq . Y ( q0 ) , cirq . Y ( q1 ) ) ,
cirq . PauliString ( cirq . Z ( q0 ) , cirq . Z ( q1 ) ) ])

def g e t _ h e i s e n b e r g _ h a m i l t o n i a n ( qubits , jh , jv ) :

heisenberg = cirq . PauliSum ()
# Apply horizontal bonds
for r in qubits :

for q0 , q1 in zip (r , r [1::]) :

heisenberg += jh * get_bond ( q0 , q1 )

# Apply vertical bonds
for r0 , r1 in zip ( qubits , qubits [1::]) :

for q0 , q1 in zip ( r0 , r1 ) :

heisenberg += jv * get_bond ( q0 , q1 )

return heisenberg

Full notebook of the implementations below are avail-

able at:

research/vqt_qmhl/vqt_qmhl.ipynb

4 More precisely, the loss function here is in fact the inverse tem-
perature multiplied by the free energy, but this detail is of little
import to our optimization.

For our QNN, we consider a unitary consisting of general
single qubit rotations and powers of controlled-not gates.
Our code returns the associated symbols so that these
can be fed into the Expectation op:

def get_r ota tion_1q (q , a , b , c ) :

return cirq . Circuit (

cirq . X ( q ) ** a , cirq . Y ( q ) ** b , cirq . Z ( q ) ** c )

def get_r ota tion_2q ( q0 , q1 , a ) :

return cirq . Circuit (

cirq . CNotPowGate ( exponent = a ) ( q0 , q1 ) )

def get_layer_1q ( qubits , layer_num , L_name ) :

layer_symbols = []
circuit = cirq . Circuit ()
for n , q in enumerate ( qubits ) :

a , b , c = sympy . symbols (

" a {2} _ {0} _ {1} b {2} _ {0} _ {1} c {2} _ {0} _ {1} "

. format ( layer_num , n , L_name ) )
layer_symbols += [a , b , c ]
circuit += get_rotation_1q (q , a , b , c )

return circuit , layer_symbols

def get_layer_2q ( qubits , layer_num , L_name ) :

layer_symbols = []
circuit = cirq . Circuit ()
for n , ( q0 , q1 ) in enumerate ( zip ( qubits [::2] ,

qubits [1::2]) ) :
a = sympy . symbols ( " a {2} _ {0} _ {1} " . format (
layer_num , n , L_name ) )
layer_symbols += [ a ]
circuit += get_rotation_2q ( q0 , q1 , a )

return circuit , layer_symbols

It will be convenient to consider a particular class of
probabilistic models where the estimation of the gradi-
ent of the model parameters is straightforward to per-
form. This class of models are called exponential families
or energy-based models (EBMs).
If our parameterized
probabilistic model is an EBM, then it is of the form:

pθ(x) = 1
Zθ

e−Eθ (x), Zθ ≡ (cid:80)

x∈Ω e−Eθ (x).

(38)

For gradients of the VQT free energy loss function
with respect to the QNN parameters, ∂φLfe(θ, φ) =
β∂φtr(ˆρθφ ˆH), this is simply the gradient of an expec-
tation value, hence we can use TFQ parameter shift gra-
dients or any other method for estimating the gradients
of QNN’s outlined in previous sections.

As for gradients of the classical probabilistic model,
one can readily derive that they are given by the following
covariance:

∂θLfe = Ex∼pθ (x)

(cid:104)

(Eθ(x) − βHφ(x))∇θEθ(x)

(cid:105)

−(Ex∼pθ (x)

(cid:2)Eθ(x)−βHφ(x)(cid:3))(Ey∼pθ (y)

(cid:2)∇θEθ(y)(cid:3)),
(39)

where Hφ(x) ≡ (cid:104)x| ˆU †(φ) ˆH ˆU (φ) |x(cid:105) is the expectation
value of the Hamiltonian at the output of the QNN with
the standard basis element |x(cid:105) as input. Since the energy
function and its gradients can easily be evaluated as it is
a neural network, the above gradient is straightforward to

40

estimate via sampling of the classical probabilistic model
and the output of the QPU.

j θxj

For our classical latent probability distribution pθ(x),
as a ﬁrst simple case, we can use the product of inde-
pendent Bernoulli distributions pθ(x) = (cid:81)
j pθj (xj) =
(cid:81)
j (1 − θj)1−xj , where xj ∈ {0, 1} are binary values.
We can re-phrase this distribution as an energy based
model to take advantage of equation (V D 2). We move
the parameters into an exponential, so that the probabil-
ity of a bitstring becomes pθ(x) = (cid:81)
j eθj xj /(eθj + e−θj ).
Since this distribution is a product of independent vari-
ables, it is easy to sample from. We can use the Tensor-
Flow Probability library [140] to produce samples from
this distribution, using the tfp.distributions.Bernoulli
object:

def b e r n o u l l i _ b i t _ p r o b a b i l i t y ( b ) :

return np . exp ( b ) /( np . exp ( b ) + np . exp ( - b ) )

def sample_bernoulli ( num_samples , biases ) :

prob_list = []
for bias in biases . numpy () :

prob_list . append (

b e r n o u l l i _ b i t _ p r o b a b i l i t y ( bias ) )

latent_dist = tfp . distributions . Bernoulli (

probs = prob_list , dtype = tf . float32 )
return latent_dist . sample ( num_samples )

After getting samples from our classical probabilistic
model, we take gradients of our QNN parameters. Be-
cause TFQ implements gradients for its expectation ops,
we can use tf.GradientTape to obtain these derivatives.
Note that below we used tf.tile to give our Hamiltonian
operator and visible state circuit the correct dimensions:

bitstring_tensor = sample_bernoulli (

num_samples , vqt_biases )
with tf . GradientTape () as tape :

t i l e d _ v q t _ m o d e l _ p a r a m s = tf . tile (

[ vqt_model_params ] , [ num_samples , 1])

s a m p l e d _ e x p e c t a t i o n s = expectation (

tiled_visible_state ,
vqt_symbol_names ,
tf . concat ([ bitstring_tensor ,

t i l e d _ v q t _ m o d e l _ p a r a m s ] , 1) ,

tiled_H )

energy_losses = beta * s a m p l e d _ e x p e c t a t i o n s
en erg y_l oss es_ av g = tf . reduce_mean (

energy_losses )

v q t _ m o d e l_ g r a d i e n t s = tape . gradient (

energy_losses_avg , [ vqt_model_params ])

Putting these pieces together, we train our model to out-
put thermal states of the 2D Heisenberg model on a 2x2
grid. The result after 100 epochs is shown in Fig. 28.

A great advantage of this approach to optimization of
the probabilistic model is that the partition function Zθ
does not need to be estimated. As such, more general
more expressive models beyond factorized distributions
can be used for the probabilistic modelling of the la-
tent classical distribution. In the advanced section of the
notebook, we show how to use a Boltzmann machine as
our energy based model. Boltzmann machines are EBM’s
where for bitstring x ∈ {0, 1}n, the energy is deﬁned as
E(x) = − (cid:80)

i,j wijxixj − (cid:80)

i bixi.

It is worthy to note that our factorized Bernoulli distri-
bution is in fact a special case of the Boltzmann machine,
one where only the so-called bias terms in the energy
function are present: E(x) = − (cid:80)
i bixi. In the notebook,
we start with this simpler Bernoulli example of the VQT,
the resulting density matrix converges to the known ex-
act result for this system, as shown in Fig. 28. We also
provide a more advanced example with a general Boltz-
mann machine. In the latter example, we picked a fully
visible, fully-connected classical Ising model energy func-
tion, and used MCMC with Metropolis-Hastings [141] to
sample from the energy function.

Figure 28. Final density matrix output by the VQT algo-
rithm run with a factorized Bernoulli distribution as classical
latent distribution, trained via a gradient-based optimizer.
See notebook for details.

3. Quantum Generative Learning from Quantum Data

Now that we have seen how to prepare thermal states
from a given Hamiltonian, we can consider how we can
learn to generatively model mixed quantum states using
quantum-probabilistic models in the case where we are
given several copies of a mixed state rather than a Hamil-
tonian. That is, we are given access to a data mixed
state ˆσD, and we would like to ﬁnd optimal parameters
{θ∗, φ∗} such that ˆρθ∗φ∗ ≈ ˆσD, where the model state is
of the form described in (34). Furthermore, for reasons
of convenience which will be apparent below, it is useful
to posit that our classical probabilistic model is of the
form of an energy-based model as in equation (38).

If we aim to minimize the quantum relative entropy
between the data and our model (in reverse compared to
the VQT) i.e., D(ˆσD(cid:107)ˆρθφ) then it suﬃces to minimize
the quantum cross entropy as our loss function

Lxe(θ, φ) ≡ −tr(ˆσD log ˆρθφ).
By using the energy-based form of our latent classical
probability distribution, as can be readily derived (see
[59]), the cross entropy is given by

Lxe(θ, φ) = Ex∼σφ(x)[Eθ(x)] + log Zθ,

41

where σφ(x) ≡ (cid:104)x| ˆU †(φ)ˆσD ˆU (φ) |x(cid:105) is the distribution
obtained by feeding the data state ˆσD through the inverse
QNN circuit ˆU †(φ) and measuring in the standard basis.
As this is simply an expectation value of a state prop-
agated through a QNN, for gradients of the loss with re-
spect to QNN parameters we can use standard TFQ dif-
ferentiators, such as the parameter shift rule presented in
section III. As for the gradient with respect to the EBM
parameters, it is given by

∂θLxe(θ, φ) = Ex∼σφ(x)[∇θEθ(x)]−Ey∼pθ(y)[∇θEθ(y)].

Let us implement a scenario where we were given the
output density matrix from our last VQT example as
data, let us see if we can learn to replicate its statistics
from data rather than from the Hamiltonian. For sim-
plicity we focus on the Bernoulli EBM deﬁned above. We
can eﬃciently sample bitstrings from our learned classical
distribution and feed them through the learned VQT uni-
tary to produce our data state. These VQT parameters
are assumed ﬁxed; they represent a quantum datasource
for QMHL.

We use the same ansatz for our QMHL unitary as we
did for VQT, layers of single qubit rotations and expo-
nentiated CNOTs. We apply our QMHL model unitary
to the output of our VQT to produce the pulled-back
data distribution. Then, we take expectation values of
our current best estimate of the modular Hamiltonian:

def g e t _ q m h l _ w e i g h t s _ g r a d _ a n d _ b i a s e s _ g r a d (

ebm_deriv_expectations , bitstring_list ,
biases ) :

b a r e _ q m h l _ b i a s e s _ g r a d = tf . reduce_mean (

ebm_deriv_expectations , 0)

c _q m hl _ bi a se s _g r ad = e b m _ b i a s e s _ d e r i v a t i v e _ a v g

( bitstring_list )

return tf . subtract ( bare_qmhl_biases_grad ,

c _q m hl _ bi a se s _g r ad )

Note that we use the tf.GradientTape functionality to
obtain the gradients of the QMHL model unitary. This
functionality is enabled by our TFQ diﬀerentiators mod-
ule.

The classical model parameters can be updated
according to the gradient formula above. See the VQT
notebook for the results of this training.

E. Subspace-Search Variational Quantum
Eigensolver for Excited States: Integration with
OpenFermion

1. Background

The Variational Quantum Eigensolver (VQE) [25] is a
heuristic algorithm for preparing the ground state of a
many-body quantum system. It is often used for mod-
elling strongly correlated systems that appear challeng-
ing to simulate classically but contain enough structure

that ground state preparation is believed to be tractable
quantum mechanically (at least for some interesting in-
stances); e.g., it is often studied in the context of the
molecular electronic structure Hamiltonian of interest
in quantum chemistry. The basic idea of VQE is that
one can use a quantum computer with limited resources
to prepare a highly entangled parameterized state that
can be optimized via classical feedback to provide a
classically inaccessible description of the ground states
of these systems. Speciﬁcally, given the time indepen-
dent Schrödinger equation ˆH|Ψ(cid:105) = E|Ψ(cid:105), with Hamil-
tonian ˆH = (cid:80)N
i=0 λi|ψi(cid:105)(cid:104)ψi|, where λi are the eigen-
values of increasing size and ψi the eigenvectors in the
spectral decomposition, VQE provides an approxima-
tion of λ0. Measurements of the Hamiltonian yield real
eigenvalues (due to the Hermitian nature of quantum
operators), therefore λ0 ≤ (cid:104)Ψ| ˆH|Ψ(cid:105). Given optimiza-
tion parameters θ and Ψ = U (θ)|0⊗N (cid:105), the classical op-
timization problem associated with VQE is deﬁned as
minθ (cid:104)0|U †(θ) ˆHU (θ)|0(cid:105). This formulation is limited to
ﬁnding the ground state energy, but higher energy states
are often chemically and physically important [142]. In
order to expand the VQE algorithm to higher energy
states, exploitation of the orthogonality of diﬀerent quan-
tum energy levels has been proposed [143–145].

Here, we focus on describing how TFQ can be used
to investigate an example of a state-of-the-art varia-
tional algorithm known as the Subspace-Search Varia-
tional Quantum Eigensolver (SSVQE) for excited states
[145]. The SSVQE modiﬁes the VQE optimization prob-
lem to be:

min
θ

k
(cid:88)

i=0

wi(cid:104)Ψi|U †(θ) ˆHU (θ)|Ψi(cid:105)

s.t. wi+1 ≤ wi, (cid:104)Ψi|Ψj(cid:105) = δij

The goal is to minimize the expectation value of the
Hamiltonian, with the initial state of each energy level’s
evaluation being orthogonal.
In SSVQE, as in VQE,
there are a number of choices for anstaz, i.e. the circuit
structure of U (θ), and optimization method [146–150].

Tensorﬂow-Quantum has several features that make it
appealing for work with VQE algorithms. The adjoint
diﬀerentiator enables rapid code execution, which is es-
pecially important given the number of Hamiltonians en-
countered in quantum chemistry experiments. The abil-
ity to work with any optimization target via custom lay-
ers and custom objectives enables straightforward imple-
mentation of and experimentation with any VQE based
algorithm. Additionally, OpenFermion [151] has native
methods for working with Cirq, enabling the quantum
chemistry methods OpenFermion provides to be easily
combined with the quantum machine learning methods
Tensorﬂow-Quantum provides.
Target problems:

1. Implement SSVQE [145] in Tensorﬂow-Quantum
2. Find the ground state and ﬁrst excited state ener-

gies for H2 at increasing bond lengths

42

Required TFQ functionalities:

1. Integration with OpenFermion [151]
2. Custom Tensorﬂow-Quantum Layers

2.

Implementation

The full notebook and implementation is available at:

research/ssvqe
First, we need to create the VQE anstaz. Here, we con-
struct one similar to the Hardware Eﬃcient Ansatz [146].
We create layers of parameterized Ry and Rz gates, en-
tangled with CNOTs.

def layer ( circuit , qubits , parameters ) :

for i in range ( len ( qubits ) ) :

circuit += cirq . ry ( parameters [3* i ]) . on (

qubits [ i ])

circuit += cirq . rz ( parameters [3* i +1]) . on

( qubits [ i ])

circuit += cirq . ry ( parameters [3* i +2]) . on

( qubits [ i ])
for i in range ( len ( qubits ) -1) :

circuit += cirq . CNOT ( qubits [ i ] , qubits [ i

+1])
circuit += cirq . CNOT ( qubits [ -1] , qubits [0])
return circuit

def ansatz ( circuit , qubits , layers , parameters ) :

for i in range ( layers ) :

params = parameters [3* i * len ( qubits ) :3*( i

+1) * len ( qubits ) ]

circuit = layer ( circuit , qubits , params )

return circuit

Now we create the readout operators for each qubit,
which are equivalent to the associated terms in the
Hamiltonian. This will yield the expectation value of
the Hamiltonian.

def exp_val ( qubits , hamiltonian ) :

return prod ([ op ( qubits [ i ]) for i , op in
enumerate ( hamiltonian ) if hamiltonian [ i ] !=
0])

Since the Hamiltonian can be expressed as a sum of

simple operators, i.e.,

ˆH =

L
(cid:88)

(cid:96)=1

a(cid:96)P(cid:96)

for real scalars a(cid:96) and Pauli strings P(cid:96). The Hamiltonian
is sparse in this representation; typically L = O(N 4)
in the number of spin-orbitals (or equivalently, qubits)
N . We create custom VQE layers with diﬀerent readout
operators but shared parameters. The SSVQE is then
implemented as a collection of these VQE layers, with
each input being orthogonal. This is implemented by
applying a Pauli X gate prior to creating U (θ).

class VQE ( tf . keras . layers . Layer ) :

def __init__ ( self , circuits , ops ) :
super ( VQE , self ) . __init__ ()

43

already been generated. These ﬁles were generated using
OpenFermion [151] and PySCF [152]. Using the Open-
FermionPySCF function generate_molecular_hamiltonian
to generate the Hamiltonians for each bond length,
we then converted this to a form compatible with
quantum circuits using the Jordan-Wigner Transform,
which maps fermionic annihilation operators to qubits
via:
(cid:55)→
2 (Xp − iYp) Z1 · · · Zp−1. This yields a Hamiltonian of
1
ˆH = g01 + g1X0X1Y2Y3 + g2X0Y1Y2X3 +
the form:
g3Y0X1X2Y3 + g4Y0Y1X2X3 + g5Z0 + g6Z0Z1 + g7Z0Z2 +
g8Z0Z3 + g9Z1 + g10Z1Z2 + g11Z1Z3 + g12Z2 + g13Z2Z3 +
g13Z3, where gn is determined by the bond length.
It
is this PauliSum object which is then saved and loaded.
With the Hamiltonians created, we iterate over the bond
lengths and compute the predicted energies of the states.

2 (Xp + iYp) Z1 · · · Zp−1 and a†

(cid:55)→ 1

ap

p

d i a t o m i c _ b o n d _ l e n g t h = 0.2
interval = 0.1
max_bond_length = 2.0
k = 2

# VQE Hyperparameters
layers = 4
n_qubits = 4
optimizer = tf . keras . optimizers . Adam ( lr =0.1)

step = 0
while d i a t o m i c _ b o n d _ l e n g t h <= max_bond_le ngth :

eigs = real [ step ]
# Load the Data
ham_name = " mo l_ ham ilt oni ans _ " + str ( step )
coef_name = " c o ef _ ha m il t on i an s _ " + str ( step )
with open ( ham_name , " rb " ) as ham_file :
hamiltonians = load ( ham_file )

with open ( coef_name , " rb " ) as coeff_file :

coefficients = load ( coeff_file )
# Create the SSVQE and Approximate the
Energies
ssvqe = make_ssvqe ( n_qubits , layers ,
coefficients , hamiltonians , k )
ground , excited = train_ssvqe ( ssvqe ,
optimizer )
d i a t o m i c _ b o n d _ l e n g t h += interval
step += 1

self . layers = [ tfq . layers . ControlledPQC (

circuits [ i ] , ops [ i ] , differentiator = tfq .
dif fe r en tiators . Adjoint () ) for i in range (
len ( circuits ) ) ]

def call ( self , inputs ) :

return sum ([ self . layers [ i ]([ inputs [0] ,

inputs [1]]) for i in range ( len ( self . layers ) )
])

class SSVQE ( tf . keras . layers . Layer ) :

def __init__ ( self , num_weights , circuits ,
ops , k , const ) :

super ( SSVQE , self ) . __init__ ()
self . theta = tf . Variable ( np . random .

uniform (0 , 2 * np . pi , (1 , num_weights ) ) ,
dtype = tf . float32 )

self . hamiltonians = []
self . k = k
self . const = const
for i in range ( k ) :

self . hamiltonians . append ( VQE (

circuits [ i ] , ops [ i ]) )

def call ( self , inputs ) :

total = 0
energies = []
for i in range ( self . k ) :

c = self . hamiltonians [ i ]([ inputs ,

self . theta ]) + self . const

energies . append ( c )
if i == 0:

total += c

else :

total += ((0.9 - i * 0.1) * c )

return total , energies

Next we need to set up the training function for the
SSVQE. The optimization loop directly minimizes the
output, as we have already encoded the constraints.

def train_ssvqe ( ssvqe , opt , tol =5 e -6 , patience

=10) :
ssvqe_model = ssvqe [0]
energies = ssvqe [1]
prev_loss = 100
counter = 0
inputs = tfq . con ver t_t o_t en sor ([ cirq . Circuit
() ])
while True :

with tf . GradientTape () as tape :

loss = ssvqe_model ( inputs )

grads = tape . gradient ( loss , ssvqe_model .

t r a i n a b l e _ v a r i a b l e s )

opt . apply_gradients ( zip ( grads ,

ssvqe_model . t ra i n a b l e _ v a r i a b l es ) )
loss = loss . numpy () [0][0]
if abs ( loss - prev_loss ) < tol :

counter += 1

if counter > patience :

break

prev_loss = loss

energies . theta = ssvqe_model .
t r a i n a b l e _ v a r i a b l e s [0]
energies = [ i . numpy () [0][0] for i in
energies ( inputs ) [1]]
return energies [0] , energies [1]

With the SSVQE set up, we now need to generate the
In the code below, we load the data that has

inputs.

This implementation takes several minutes to run, even
with the Adjoint diﬀerentiator. This is because there are
4 layers * 3 parameters per qubit * 4 qubits = 48 pa-
rameters per layer and because we must optimize these
parameters for 20 diﬀerent Hamiltonians. We can then
plot and compare the actual energies with the VQE pre-
dicted energies, as done in Figure 29.

44

The quantum classiﬁer we consider here is a made up
of three parts: a circuit that prepares a set of states of
interest, a variational quantum circuit consisting of mul-
tiple trainable layers and a collection of measurements
that can be combined to create a predictor for the clas-
siﬁcation. In ﬁg. 30 we give a schematic depiction of the
full model.

Tensorﬂow Quantum has a data set module that con-
tains quantum circuits for diﬀerent quantum many-body
ground states. This makes it easy to set up a workﬂow
where we want to load a set of quantum states and per-
form a quantum machine learning task on these data.

As an example, we study the two-dimensional trans-

verse ﬁeld Ising-model (TFIM) on a torus,

HTFIM = −

i σz
σz

j − g

(cid:88)

(cid:104)i,j(cid:105)

N
(cid:88)

i

σx
i = Hzz + gHx,

where (cid:104)i, j(cid:105) indicates the set of nearest neighbor indices
for each point in the lattice. On the torus, this system
has a phase transition at g ≈ 3.04 from an ordered to a
disordered phase [162, 163].

After the state preparation circuit, we use the hard-
ware eﬃcient ansatz to train the quantum classiﬁer. This
ansatz consists of two layers of parameterized single qubit
gates and a single layer of two-qubit parameterized gates
[146]. Note that the parameters in the state preparation
circuit stay ﬁxed during training. Finally, we measure
the observables (cid:104)Zi(cid:105) on each qubit and apply a rescaled
sigmoid to the linear predictor of the outcomes,

¯y = tanh

(cid:88)

(cid:104)Zi(cid:105)Wi,

i

where Wi ∈ Rn is a weight vector. Hence, given an input
ground state |ψ0(θ∗
k)(cid:105), our classiﬁer will output a single
scalar ¯y ∈ (−1, 1).

We use the Hinge loss function to train the model to
output the correct labels. This loss function is given by

(cid:96) = max(0, t − y · ¯y)

(40)

where, y ∈ {−1, 1} are the ground truth labels and
¯y ∈ (−1, 1) the model predictions. The entire end-to-
end model contains several non-trivial components, such
as loading quantum states as data or backpropagating
classical gradients through quantum gates.

Target problems:

1. Training a quantum circuit classiﬁer to detect a
phase transition in a condensed matter physics sys-
tem

Required TFQ functionalities:

1. Hybrid quantum-classical optimization of a varia-

tional quantum circuit and logistic function.

2. Batching training over quantum data (multiple

ground states and labels)

Figure 29. Comparison of SSVQE predicted energy for the
ground state and ﬁrst excited state compared with the true
values

F. Classiﬁcation of Quantum Many-Body Phase
Transitions

To run this example see the Colab notebook at:

research/phase_classifier/phase_classification.ipynb

1. Background

In quantum many-body systems we are interested in
studying diﬀerent phases of matter and the transitions
between these diﬀerent phases. This requires a detailed
characterization of the observables that change as a func-
tion of the order values of the system. To estimate
these observables, we have to rely on quantum Monte
Carlo techniques or tensor network approaches, that each
come with their respective advantages and drawbacks
[153, 154].

Recently, work on classifying phases with supervised
machine learning has been proposed as a method for un-
derstanding phase transitions [155, 156]. For such a su-
pervised approach, we label the diﬀerent phases of the
system and use classical data obtained from the system
of interest and train a machine learning model to predict
the labels. A natural extension of these ideas is a quan-
tum machine learning model for phase classiﬁcation [157],
where the information that characterizes the state is en-
coded in a variational quantum circuit. This would re-
quire preparing high ﬁdelity quantum many-body states
on NISQ devices, an area of active research in the ﬁeld
In this section, we
of quantum computing [158–161].
show how a quantum classiﬁer can be trained end-to-end
in Tensorﬂow quantum by using a data set of quantum
many-body states.

45

def c r e a t e _ q u a n t u m _ m o d e l (N , num_layers ) :

qubits = cirq . GridQubit . rect (N , 1)
circuit = cirq . Circuit ()
for l in range ( num_layers ) :

add_layer_single ( circuit , qubits , cirq .X

, f " x_ { l } " )

a d d _ l a y e r _ n e a r e s t _ n e i g h b o u r s ( circuit ,

qubits , cirq . XX , f " xx_ { l } " )

add_layer_single ( circuit , qubits , cirq .Z

, f " z_ { l } " )

readout = [ cirq . Z ( q ) for q in qubits ]
return circuit , readout

We can seamlessly integrate our quantum classiﬁer into
a Keras model.

model = tf . keras . Sequential ([

tf . keras . layers . Input ( shape =() , dtype = tf .
string ) ,
tfq . layers . PQC ( circuit , output ) ,
tf . keras . layers . Dense (1 , activation = tf . keras
. activations . tanh )

])

Additionally, we can compile the model with other met-
rics to track during training.
In our case, we use the
hinge accuracy to count the number of missclasiﬁed data
points.

def hinge_accuracy ( y_true , y_pred ) :

y_true = tf . squeeze ( y_true ) > 0.0
y_pred = tf . squeeze ( y_pred ) > 0.0
result = tf . cast ( y_true == y_pred , tf .
float32 )

return tf . reduce_mean ( result )

model . compile (

loss = tf . keras . losses . Hinge () ,
optimizer = tf . keras . optimizers . Adam (
learning_rate =0.01) ,
metrics =[ hinge_accuracy ])

If we set the maximum number of epochs and batch size,
we are ready to ﬁt the model. We add an early stopping
callback to make sure that the training can terminate
when the loss no longer decreases consistently.

epochs = 25
batch_size = 32

qnn_history = model . fit (

x_train_tfcirc , labels ,
batch_size = BATCH_SIZE ,
epochs = EPOCHS ,
verbose =1 ,
callbacks =[ tf . keras . callbacks . EarlyStopping (
’ loss ’ , patience =5) ])

such that |ψ0(θ∗

Figure 30. The ﬁrst part of the circuit prepares the ground
state of a Hamiltonian H(λ) with a ﬁxed ansatz |ψ0(θ)(cid:105). For
each order value λk there is a corresponding set of ﬁxed pa-
rameters θ∗
k)(cid:105) is the ground state of H(λk).
k
The second part of the circuit is the quantum classiﬁer, a
hardware eﬃcient ansatz with trainable parameters for each
gate (αi, βi, γi), where i = 1, . . . , d indicates the layer. Fi-
nally, we obtain the expectation value (cid:104)Z(cid:105)i on all qubits and
combine these into a the output label ¯y after a rescaled sig-
moid activation function.

2.

Implementation

For the two-dimensional TFIM on a rectangular lat-
tice, the available data sets are a 3 × 3, 4 × 3, and
4 × 4 lattice for g ∈ [2.5, 3.5]. To load the data, we use
the tfi_rectangular function to obtain both the circuits
and labels indicating the ordered (y = 1) and disordered
(y = −1) phase. Here, we consider the 4 × 4 lattice.

nspins = 16
qubits = cirq . GridQubit . rect ( nspins , 1)

circuits , labels , _ , _ = tfq . datasets .

tfi _r e ct angular ( qubits )

labels = np . array ( labels )
labels [ labels >= 1] = 1.0
labels = labels * 2 - 1
x_t rain_t fcirc = tfq . con ver t_t o_t ens or ( circuits )

circuits now contains 51 quantum circuits that prepare
> 0.999 ﬁdelity ground states of the two-dimensional
TFIM at the respective order values g. Next, we add
the layers of our hardware eﬃcient ansatz. We parame-
terize each gate in the classiﬁer individually.

def a d d _ l a y e r _ n e a r e s t _ n e i g h b o u r s ( circuit , qubits

, gate , prefix ) :
for i , q in enumerate ( zip ( qubits , qubits
[1:]) ) :

symbol = sympy . Symbol ( prefix + ’ - ’ + str

( i ) )

circuit . append ( gate (* q ) ** symbol )

predictions = model . predict ( x_train_tfcirc )

def ad d _la y er_ single ( circuit , qubits , gate ,

prefix ) :
for i , q in enumerate ( qubits ) :

symbol = sympy . Symbol ( prefix + ’ - ’ + str

( i ) )

circuit . append ( gate ( q ) ** symbol )

After the model is trained, we can predict the labels of
the phases from the model output ¯y and visualize how
well the model has captured the phase transition.
In
ﬁg. 31, we see that the inﬂection point at ¯y ≈ 0 coin-
cides with the phase transition at g ≈ 3.04, as expected.
Although we have not explored this here, we could train
the classiﬁer on a subset of the data, for instance around

Ground state preparationClassifier...Predictionthe critical point, and then see how well the classiﬁer
generalizes to states outside of the seen data.

Figure 31. Quantum classiﬁer output versus order value g
of the two-dimensional TFIM. The dashed line indicates the
critical point g ≈ 3.04.

G. Quantum Generative Adversarial Networks

1. Background

Generative adversarial networks (GANs) [164] have
met widespread success in classical machine learning.
While classical data can be seen as a special case of data
corresponding to a probabilistic mixture, we consider the
task of adversarially learning most general form of data:
quantum states. A generator seeks to create a circuit
that reproduces a given quantum state, while a discrim-
inator circuit is presented either with the true data or
In this section, we
with fake data from the generator.
present viable applications of a new quantum GAN archi-
tecture for both purely quantum datasets and quantum
states built from classical datasets.

Recent work on a quantum GAN (QuGAN) [54, 165]
has proposed a direct analogy of the classical GAN ar-
chitecture in designing the generator and discriminator
circuits. However, the QuGAN does not always converge
but rather in certain cases oscillates between a ﬁnite set of
states due to mode collapse, and in general suﬀers from a
non-unique Nash equilibrium [76]. This motivates a new
entangling quantum GAN (EQ-GAN) with a uniquely
quantum twist: rather than providing the discriminator
with either true or fake data, we allow the discriminator
to entangle both true and fake data. The convergence of
the EQ-GAN to the global optimal Nash equilibrium is
theoretically veriﬁed and numerical experiments conﬁrm

46

that the EQ-GAN converges on problem instances that
the QuGAN failed on [76].

A pervasive issue in near-term quantum computing is
noise: when performing an entangling operation, the re-
quired two-qubit gate introduces phase errors that are
diﬃcult to fully calibrate against due to a time-dependent
noise model. However, such operations are required to
measure the overlap between two quantum states, in-
cluding the assessment of how close the fake data is to
the true data. This issue provides further motivation for
the use of adversarial generative learning. Without ad-
versarial learning, one may freeze the discriminator to
perform an exact ﬁdelity measurement between the true
and fake data. While this would replicate the original
state in the absence of noise, gate errors in the imple-
mentation of the discriminator will cause convergence to
the incorrect optimum. As seen in the ﬁrst example be-
low, the adversarial approach of the EQ-GAN is more
robust to such errors than the simpler supervised learn-
ing approach. Since training quantum machine learning
models can require extensive time to compute gradients
on current quantum hardware, resilience to gate errors
drifting during the training process is especially valuable
in the noisy intermediate-scale quantum era of quantum
computing.

Most proposals for quantum machine learning algo-
rithms on classical data require a quantum random
access memory (QRAM) [166]. A QRAM typically
stores the classical dataset in a superposition of quan-
tum states, allowing a quantum device to eﬃciently ac-
cess the dataset. However, a QRAM is diﬃcult to achieve
experimentally, posing a further roadblock for near-term
quantum machine learning applications. We provide an
example application of the EQ-GAN to create an approx-
imate QRAM by learning a shallow quantum circuit that
generates a superposition of classical data. In particular,
the QRAM is applied to quantum neural networks [24],
improving the performance of a quantum neural network
for a classiﬁcation task.

Target problems:

1. Suppress noise in a generative learning task for

quantum states

2. Prepare an approximate quantum memory for

faster training of a quantum neural network

Required TFQ functionalities:

1. Use of a quantum hardware backend
2. Shared variables between quantum neural network

layers

3. Training on a noisy quantum circuit

2. Noise Suppression with Adversarial Learning

To run this example, see the Colab notebook at:

research/eq_gan/noise_suppression.ipynb

2.62.83.03.23.4g0.750.500.250.000.250.500.75Output LayerThe entangling quantum generative adversarial net-

work (EQ-GAN) uses a minimax cost function

min
θg

max
θd

V (θg, θd) = min
θg

max
θd

[1 − Dσ(θd, ρ(θg))]

(41)

to learn the true data density matrix σ. The generator
produces a fake quantum state ρ(θg), while the discrimi-
nator performs a parameterized swap test (ﬁdelity mea-
surement) Dσ(θd, ρ(θg)) between the true data σ and the
fake data ρ. The swap test is parameterized such that
there exist parameters θopt
that realize a perfect swap
test, i.e. Dσ(θopt
σ (ρ(θg)) where
d , ρ(θg)) = 1
2 + 1

2 Dﬁd

d

47

As an example, we consider the task of learning the su-
perposition 1√
(|0(cid:105) + |1(cid:105)) on a quantum device with noise
2
(Fig. 32). The discriminator is deﬁned by a swap test
with CZ gate providing the necessary two-qubit opera-
tion. To learn to correct gate errors, however, the dis-
criminator adversarially learns the angles of single-qubit
Z rotations insert directly after the CZ gate. Hence, the
EQ-GAN obtains a state overlap signiﬁcantly better than
that of the perfect swap test.

Dﬁd

σ (ρ(θg)) =

(cid:18)

Tr

(cid:113)

σ1/2 ρ(θg) σ1/2

(cid:19)2

.

(42)

d

In the presence of noise, it is generally diﬃcult to de-
termine a circuit ansatz for Dσ(θd, ρ) such that param-
eters θopt
exist. When implementing a CZ gate, gate
parameters such as the conditional Z phase, single qubit
Z phase and swap angles in two-qubit entangling gate
can drift and oscillate over the time scale of O(10) min-
utes [167, 168]. Such unknown systematic and time-
dependent coherent errors provides signiﬁcant challenges
for applications in quantum machine learning where gra-
dient computation and update requires many measure-
ments, especially because of the use of entangling gates
in a swap test circuit. However, the large deviations in
single-qubit and two-qubit Z rotation angles can largely
be mitigated by including additional single-qubit Z phase
compensations. In learning the discriminator circuit that
is closest to a true swap test, the adversarial learning
of EQ-GAN provides a useful paradigm that may be
broadly applicable to improving the ﬁdelity of other near-
term quantum algorithms.

To operate under this noise model, we can deﬁne a
cirq.NoiseModel that is compatible with TFQ, imple-
menting the noisy_operation method to add CZ and Z
phase errors on all CZ gates.

def noisy _op eration ( self , op ) :

if isinstance ( op . gate , cirq . ops . CZPowGate ) :

error_2q = cirq . ops . CZPowGate ( exponent = np .
random . normal ( self . mean [0] , self . stdev [0]) )
(* op . qubits )
error_1q_0 = cirq . ops . ZPowGate ( exponent = self
. single_errors [ op . qubits [0]]) ( op . qubits [0])
error_1q_1 = cirq . ops . ZPowGate ( exponent = self
. single_errors [ op . qubits [1]]) ( op . qubits [1])
return [ op , error_2q , error_1q_0 , error_1q_1
]

return op

To add the noise to TFQ, we can simply convert the
circuit with noise into a tensor. For instance, to generate
swap tests over n_data datapoints:

sw a p_ te s t _c ir c ui t = sw ap _te st_ cir cui t . with_noise

( noise_model )

sw a p_ te s t _c ir c ui t = tf . tile ( tfq .

co nv e r t_ to _ te nso r ([ swa p_t est _c irc uit ]) , tf .
constant ([ n_data ]) )

Figure 32. EQ-GAN experiment for learning a single-qubit
state. The discriminator (U (θd) is constructed with free Z
rotation angles to suppress CZ gate errors, allowing the gen-
erator ρ(θg) to converge closer to the true data state σ by
varying X and Z rotation angles.

While the noise suppression experiment can be evalu-
ated with a simulated noise model as described above, the
full model can also be run by straightforwardly changing
the backend via the Google Quantum Engine API.

engine = cirq . google . Engine ( project_id =

project_id )

backend = engine . sampler ( processor_id =[ ’ rainbow ’

] , gate_set = cirq . google . XMON )

The hardware backend can then be directly applied in

a model.

e xp e ct a ti o n_ o ut p ut = tfq . layers . Expectation (

backend = backend ) (

full_swaptest ,
symbol_names = generator_symbols ,
operators = circuits . swap_readout_op (
generator_qubits , data_qubits ) ,
initializer = tf . c o n s t a n t _ i n i t i a l i z e r (

g e n e r a t o r _ i n i t i a l i a t i o n ) )

When evaluating the adversarial swap test on a cali-
brated quantum device, we found that that the error was
reduced by a factor of around four compared to a direct
implementation of the standard swap test [76].

3. Approximate Quantum Random Access Memory

To run this example, see the Colab notebook at:

research/eq_gan/variational_qram.ipynb

When applying quantum machine learning to classi-
cal data, most algorithms require access to a quantum

|0⟩|0⟩HHH{{{𝜎𝜌g)𝜃(Ud)𝜃(X√Z√X𝜃2Z𝜃3Z𝜃1Z𝜃4Z𝜃5√

random access memory (QRAM) that stores the classi-
cal dataset in superposition. More particularly, a set of
classical data can be described by the empirical distribu-
tion {Pi} over all possible input data i. Most quantum
machine learning algorithms require the conversion from
{Pi} into a quantum state (cid:80)
Pi |ψi(cid:105), i.e. a superposi-
tion of orthogonal basis states |ψi(cid:105) representing each sin-
gle classical data entry with an amplitude proportional
to the square root of the classical probability Pi. Prepar-
ing such a superposition of an arbitrary set of n states
takes O(n) operations at best, which ruins the exponen-
tial speedup. Given a suitable ansatz, we may use an
EQ-GAN to learn a state approximately equivalent to
the superposition of data.

i

To demonstrate a variational QRAM, we consider a
dataset of two peaks sampled from diﬀerent Gaussian
distributions. Exactly encoding the empirical probability
density function requires a very deep circuit and multiple-
control rotations; similarly, preparing a Gaussian dis-
tribution on a device with planar connectivity requires
deep circuits. Hence, we select shallow circuit ansatzes
that generate concatenated exponential functions to ap-
proximate a symmetric peak [169]. Once trained to ap-
proximate the empirical data distribution, the variational
QRAM closely reproduces the original dataset (Fig. 33).

Figure 33. Two-peak total dataset (sampled from normal dis-
tributions, N = 120) and variational QRAM of the training
dataset (N = 60). The variational QRAM is obtained by
training an EQ-GAN to generate a state ρ with the shallow
peak ansatz to approximate an exact superposition of states
σ. The training and test datasets (each N = 60) are both
balanced between the two classes.

As a proof of principle for using such QRAM in a quan-
tum machine learning context, we demonstrate in the ex-
ample code an application of the QRAM to train a quan-
tum neural network [24]. The loss function is computed
either by considering each data entry individually (en-
coded as a quantum circuit) or by considering each class
individually (encoded as a superposition in variational
QRAM). Given the same number of circuit evaluations
to compute gradients, the superposition converges to a
better accuracy at the end of training despite using an
approximate distribution.

48

H. Reinforcement Learning with Parameterized
Quantum Circuits

1. Background

Reinforcement learning (RL) [170] is one of the three
It pertains to a
main paradigms of machine learning.
learning setting where an agent interacts with an en-
vironment as to solve a sequential decision task set by
this environment, e.g., playing Atari games [171] or nav-
igating cities without a map [172]. This interaction is
commonly divided into episodes (s0, a0, r0, s1, a1, r1, . . .)
of states, actions and rewards exchanged between agent
and environment. For each state s the agent perceives, it
performs an action a sampled from its policy π(a|s), i.e.,
a probability distribution over actions given states. The
environment then rewards the agent’s action with a real-
valued reward r and updates the state of the agent. This
interaction repeats cyclically until episode termination.
The goal of the agent here is to optimize its policy π(a|s)
as to maximize its expected rewards in an episode, and
hence solve the task at hand. More precisely, the agent’s
ﬁgure of merit is deﬁned by a so-called value function

Vπ(s0) = Eπ

(cid:35)

γtrt

(cid:34) H
(cid:88)

t=0

(43)

where H is the horizon (or length) of an episode, and
γ is a discount factor in [0, 1] that adjusts the relative
value of immediate versus long-term rewards.

Traditionally, RL algorithms belong to two families:

• Policy-based algorithms, where an agent’s policy is de-
ﬁned by a parametrized model πθ(a|s) and is updated
via steepest ascent on its resulting value function.

• Value-based algorithms, where a parametrized model
Vθ(s) is used to approximate the optimal value func-
tion, i.e., associated to an optimal policy. The agent
interacts with the environment with a policy that gen-
erally depends on Vθ(s) and its experienced rewards
are used to improve the quality of the approximation.

When facing environments with large (or continuous)
state/action spaces, DNNs recently became the standard
models used in both policy-based and value-based ap-
proaches. Deep RL has achieved a number of unprece-
dented achievements, such as superhuman performance
in Go [173], StarCraft II [174] and Dota 2 [175]. More re-
cently, several proposals have been made to enhance deep
RL agents with quantum analogues of DNNs (QNNs,
or PQCs), both in policy-based [176] and value-based
RL [177–180]. These works essentially described how to
adapt deep RL algorithms to work with PQCs as either
policies or value-function approximators, and tested their
performance on benchmarking environments [181]. As
pointed out in [176, 179], certain design choices of PQCs
can lead to signiﬁcant gaps in learning performance. The

(a) Empirical PDF(b) Variational QRAMmost crucial of these choices is arguably the data encod-
ing strategy. As supported by theoretical investigations
[182, 183], data re-uploading (see Fig. 34) where encod-
ing layers and variational layers of parametrized gates are
sequentially alternated, stands out as a way to get highly
expressive models.

Figure 34. A parametrized quantum circuit with data re-
uploading as used in quantum reinforcement learning. Each
white gate represents a single-qubit rotation, parametrized
either by a variational angle θ in the variational layers (or-
ange), or by a data component s (re-scaled by a parameter λ)
in encoding layers (blue).

2. Policy-Gradient Reinforcement Learning with PQCs

In the following, we focus on the implementation of a
policy-based approach to RL using PQCs [176]. For that,
let us deﬁne the parametrized policy of our agent as:

49

2. Train a reinforcement learning agent based on this

quantum circuit

Required TFQ functionalities:

1. Parametrized circuit layers
2. Custom Keras layers
3. Automatic diﬀerentiation w.r.t. a custom loss using

tf.GradientTape

3.

Implementation

To run this example in the browser through Colab,

follow the link:

docs/tutorials/quantum_reinforcement_learning.ipynb

We demonstrate the implementation of the quantum
policy-gradient algorithm on CartPole-v1, a benchmark-
ing task from OpenAI Gym [181]. This environment has
a 4-dimensional continuous state space and a discrete ac-
tion space of size 2. We hence use a PQC acting on 4
qubits, on which we evaluate 2 expectation values.

We start by implementing the quantum circuit used as
the agent’s PQC. It is composed of alternating layers of
variational single-qubit rotations, entangling CZ opera-
tors, and data-encoding single-qubit rotations.

def o ne _ qu b it _ ro t at i on (q , symbols ) :

return [ cirq . X ( q ) ** symbols [0] , cirq . Y ( q ) **

symbols [1] , cirq . Z ( q ) ** symbols [2]]

πθ(a|s) =

ewa(cid:104)Oa(cid:105)s,θ,λ
ewa(cid:48) (cid:104)Oa(cid:48) (cid:105)s,θ,λ

(44)

def entangling_layer ( qubits ) :

cz_ops = [ cirq . CZ ( q0 , q1 ) for q0 , q1 in zip (

where (cid:104)Oa(cid:105)s,θ,λ
is the expectation value of an observable
Oa (e.g., Pauli Z on ﬁrst qubit) associated to action a,
as measured at the output of the PQC. This expectation
value is also augmented by a trainable weight wa, which
is also action speciﬁc (see Fig. 34).

As stated above, the parameters of this policy are up-
dated as to perform gradient ascent on their correspond-
ing value function (see (43)). Thanks to the so-called
policy gradient theorem [184], we know that another for-
mulation of this objective function is given by the follow-
ing loss function:

L(θ) = −

1
|B|

(cid:88)

(cid:34)H−1
(cid:88)

s0,a0,r0,...∈B

t=0

log(πθ(at|st))

(cid:35)

γt(cid:48)

rt+t(cid:48)

H−t
(cid:88)

t(cid:48)=1

for a batch B of episodes (s0, a0, r0, ...) sampled by
following πθ in the environment.
This expression
has the advantage that it avoids numerical diﬀerentia-
tion to compute the gradient of the loss with respect to θ.

Target problems:

qubits , qubits [1:]) ]

cz_ops += ([ cirq . CZ ( qubits [0] , qubits [ -1]) ] if

len ( qubits ) != 2 else [])

return cz_ops

def generate_circuit ( qubits , n_layers ) :

n_qubits = len ( qubits )

params = sympy . symbols ( f ’ theta (0:{3*( n_layers

+1) * n_qubits }) ’)

params = np . asarray ( params ) . reshape (( n_layers

+1 , n_qubits , 3) )

inputs = sympy . symbols ( f ’x (0:{ n_qubits }) ’+ f ’

(0:{ n_layers }) ’)

inputs = np . asarray ( inputs ) . reshape (( n_layers ,

n_qubits ) )

circuit = cirq . Circuit ()
for l in range ( n_layers ) :

circuit += cirq . Circuit ( on e _q u bi t _r o ta t io n (q
, params [l , i ]) for i , q in enumerate ( qubits ) )
circuit += entangling_layer ( qubits )
circuit += cirq . Circuit ( cirq . rx ( inputs [l , i ])
( q ) for i , q in enumerate ( qubits ) )

circuit += cirq . Circuit ( o ne _ qu bi t _r o ta t io n (q ,
params [ n_layers , i ]) for i , q in enumerate (
qubits ) )

1. Implement a parametrized quantum circuit with

data re-uploading

return circuit , list ( params . flat ) , list ( inputs

. flat )

ControlledPQC

We use this quantum circuit to deﬁne a ControlledPQC
layer.
To sort between variational and encoding
angles in the data re-uploading scheme, we include
the
This
ReUploadingPQC layer will manage the trainable param-
eters (variational angles θ and input-scaling parameters
λ) and resolve the input values (input state s) into the
appropriate symbols in the circuit.

in a custom Keras layer.

class ReUploadingPQC ( tf . keras . layers . Layer ) :

def __init__ ( self , qubits , n_layers ,

observables , activation = ’ linear ’ , name = " re -
uploading_PQC " ) :
super ( ReUploading , self ) . __init__ ( name = name )
self . n_layers = n_layers

circuit , theta_symbols , input_symbols =
ge ner a te_ c ircuit ( qubits , n_layers )
self . c o m put ati on_ lay er = tfq . layers .
ControlledPQC ( circuit , observables )

theta_init = tf . r a n d o m _ u n i f o r m _ i n i t i a l i z e r (
minval =0. , maxval = np . pi )
self . theta = tf . Variable ( initial_value =
theta_init ( shape =(1 , len ( theta_symbols ) ) ,
dtype = " float32 " ) , trainable = True , name = "
thetas " )
lmbd_init = tf . ones ( shape =( len ( qubits ) *
n_layers ,) )
self . lmbd = tf . Variable ( initial_value =
lmbd_init , dtype = " float32 " , trainable = True ,
name = " lambdas " )
symbols = [ str ( x ) for x in theta_symbols +
input_symbols ]
self . indices = tf . constant ([ sorted ( symbols ) .
index ( a ) for a in symbols ])
self . activation = activation
self . empty_circuit = tfq . co nve rt_ to_ ten sor ([
cirq . Circuit () ])

def call ( self , inputs ) :

batch_dim = tf . gather ( tf . shape ( inputs [0]) ,
0)
ti le d _ up _c i rcu it s = tf . repeat ( self .
empty_circuit , repeats = batch_dim )
til ed _ up _thetas = tf . tile ( self . theta ,
multiples =[ batch_dim , 1])
til ed _ up _inputs = tf . tile ( inputs [0] ,
multiples =[1 , self . n_layers ])
scaled_inputs = tf . einsum ( "i , ji - > ji " , self .
lmbd , t i le d_up_inputs )
squ as h ed _inputs = tf . keras . layers . Activation
( self . activation ) ( scaled_inputs )
joined_vars = tf . concat ([ tiled_up_thetas ,
squ as h ed _inputs ] , axis =1)
joined_vars = tf . gather ( joined_vars , self .
indices , axis =1)

return self . c omp uta tio n_l ay er ([
tiled_up_circuits , joined_vars ])

We also implement a custom Keras layer to post-
process the expectation values (cid:104)Oa(cid:105)s,θ,λ
at the output of
the PQC. Here, the Alternating layer multiplies a single
expectation value by weights (w0, w1)
(cid:104)Z0Z1Z2Z3(cid:105)s,θ,λ
initialized to (1, −1).

50

def __init__ ( self , output_dim ) :

super ( Alternating , self ) . __init__ ()
self . w = tf . Variable ( initial_value = tf .
constant ([[( -1.) ** i for i in range (
output_dim ) ]]) , dtype = " float32 " , trainable =
True , name = " obs - weights " )

def call ( self , inputs ) :

return tf . matmul ( inputs , self . w )

ops = [ cirq . Z ( q ) for q in qubits ]
observables = [ reduce (( lambda x , y : x * y ) , ops ) ]

We now put together all these layers to deﬁne a Keras

model of the policy in equation (44).

def g e n e r a t e _ m o d e l _ p o l i c y ( qubits , n_layers ,

n_actions , beta , observables ) :

input_tensor = tf . keras . Input ( shape =( len (

qubits ) , ) , dtype = tf . dtypes . float32 , name = ’
input ’)

re_uploading = ReUploading ( qubits , n_layers ,

observables ) ([ input_tensor ])
process = tf . keras . Sequential ([

Alternating ( n_actions ) ,
tf . keras . layers . Lambda ( lambda x : x * beta ) ,
tf . keras . layers . Softmax ()
] , name = " observables - policy " )
policy = process ( re_uploading )
model = tf . keras . Model ( inputs =[ input_tensor ] ,

outputs = policy )

return model
We now move on to the implementation of the learn-
ing algorithm. We start by deﬁning two helper func-
tions: a gather_episodes function that gathers a batch
of episodes of interaction with the environment, and a
compute_returns function that computes the discounted
sums of rewards appearing in the agent’s loss function.

def gather_episodes ( state_bounds , n_actions ,

model , n_episodes , env_name ) :

trajectories = [ defaultdict ( list ) for _ in

range ( n_episodes ) ]

envs = [ gym . make ( env_name ) for _ in range (

n_episodes ) ]

done = [ False for _ in range ( n_episodes ) ]
states = [ e . reset () for e in envs ]

while not all ( done ) :

unfinished_ids = [ i for i in range (
n_episodes ) if not done [ i ]]
no rma liz ed _st ate s = [ s / state_bounds for i , s

in enumerate ( states ) if not done [ i ]]

for i , state in zip ( unfinished_ids ,
no rma liz ed_ sta te s ) :

trajectories [ i ][ ’ states ’ ]. append ( state )

# Compute policy for all unfinished envs in
parallel
states = tf . con ver t_t o_t ens or (
no rma liz ed_ sta te s )
action_probs = model ([ states ])

# Store action and transition all
environments to the next state
states = [ None for i in range ( n_episodes ) ]
for i , policy in zip ( unfinished_ids ,
action_probs . numpy () ) :

action = np . random . choice ( n_actions , p =

class Alternating ( tf . keras . layers . Layer ) :

policy )

states [ i ] , reward , done [ i ] , _ = envs [ i ].

step ( action )

trajectories [ i ][ ’ actions ’ ]. append ( action )
trajectories [ i ][ ’ rewards ’ ]. append ( reward )

return trajectories

def compu te_ returns ( rewards_history , gamma ) :

returns = []
disc ounted _sum = 0
for r in re wards_history [:: -1]:

discounted_sum = r + gamma * discounted_sum
returns . insert (0 , discounted_sum )

return returns

To train the policy, we need a function that updates
the model parameters. This is done via gradient descent
on the loss of the agent. Since the loss function in policy-
gradient approaches is more involved than, for instance,
a supervised learning loss, we use a tf.GradientTape to
store the contributions of our model evaluation to the
loss. When all contributions have been added, this tape
can then be used to backpropagate the loss on all the
model evaluations and hence compute the required gra-
dients.

@tf . function
def re i nfo r ce_ update ( states , actions , returns ,

model ) :

states = tf . c on ver t_t o_t ens or ( states )
actions = tf . co nve rt_ to_ ten sor ( actions )
returns = tf . co nve rt_ to_ ten sor ( returns )

with tf . GradientTape () as tape :

tape . watch ( model . t r a i n a b l e _ v ar i a b l e s )
logits = model ( states )
p_actions = tf . gather_nd ( logits , actions )
log_probs = tf . math . log ( p_actions )
loss = tf . math . reduce_sum ( - log_probs *
returns ) / batch_size

grads = tape . gradient ( loss , model .

t r a i n a b l e _ v a r i a b l e s )

for optimizer , w in zip ([ optimizer_in ,

optimizer_var , optimizer_out ] , [ w_in , w_var ,

w_out ]) :

optimizer . apply_gradients ([( grads [ w ] , model .
t r a i n a b l e _ v a r i a b l e s [ w ]) ])

With this, we can implement the main training loop of

the agent.

env_name = " CartPole - v1 "

for batch in range ( n_episodes // batch_size ) :

# Gather episodes
episodes = gather_episodes ( state_bounds ,

n_actions , model , batch_size , env_name )

# Group states , actions and returns in arrays
states = np . concatenate ([ ep [ ’ states ’] for ep

in episodes ])

actions = np . concatenate ([ ep [ ’ actions ’] for ep

in episodes ])

rewards = [ ep [ ’ rewards ’] for ep in episodes ]
returns = np . concatenate ([ compute_returns (
ep_rwds , gamma ) for ep_rwds in rewards ])
returns = np . array ( returns , dtype = np . float32 )
id_ ac t io n _p airs = np . array ([[ i , a ] for i , a in

enumerate ( actions ) ])

51

# Update model parameters
reinforce_update ( states , id_action_pairs ,

returns , model )

Thanks

to the parallelization of model

evalua-
tions and fast automatic diﬀerentiation using the
tfq.differentiators.Adjoint() diﬀerentiator, the execu-
tion of this training loop for 500 episodes takes about
15 minutes on a regular laptop (for a PQC acting on 4
qubits and with 5 re-uploading layers).

4. Value-Based Reinforcement Learning with PQCs

The example above provides the code for a policy-
based approach to RL with PQCs. The tutorial under
this link:

docs/tutorials/quantum_reinforcement_learning.ipynb

also implements the value-based algorithm introduced in
[179]. Aside from the learning mechanisms speciﬁc to
deep value-based RL (e.g., a replay memory to re-use
past experience and a target model to stabilize learning),
these two methods essentially diﬀer in the role played
of the PQC. In our
by the expectation values (cid:104)Oa(cid:105)s,θ,λ
quantum approach to value-based RL, these correspond
to approximations of the values Q(s, a) (i.e., the value
function V (s) taken as a function of a state and an ac-
tion), trained using a loss function of the form:

L(θ) =

1
|B|

(cid:88)

(cid:16)

s,a,r,s(cid:48)∈B

Qθ(s, a) − [r + max

a(cid:48)

Qθ(cid:48) (s(cid:48), a(cid:48))]

(cid:17)2

derived from Q-learning [170]. We refer to [178, 179] and
the tutorial for more details.

5. Quantum Environments

In the initial works,

it was proven that there exist
learning environments that can only be eﬃciently tackled
by quantum agents (barring well-established computa-
tional assumptions) [176]. However, these problems are
artiﬁcial and contrived, and it is a key question in the
ﬁeld of Quantum RL whether there exist natural environ-
ments for which quantum agents can have a large learn-
ing advantage over their classical counterparts.
Intu-
itive candidates are RL environments that are themselves
quantum in nature.
In this setting, early results have
already demonstrated that variational quantum methods
can be advantageous when the data perceived by a learn-
ing agent stems from measurements on a quantum sys-
tem [176]. Going a step further, one could also consider a
setting where the states perceived by the agent are gen-
uinely quantum and can therefore be processed directly
by a PQC, as was recently explored in a quantum control
environment [185].

VI. CLOSING REMARKS

The rapid development of quantum hardware repre-
sents an impetus for the equally rapid development of
quantum applications. In October 2017, the Google AI
Quantum team and collaborators released its ﬁrst soft-
ware library, OpenFermion, to accelerate the develop-
ment of quantum simulation algorithms for chemistry
and materials sciences. Likewise, TensorFlow Quantum
is intended to accelerate the development of quantum
machine learning algorithms for a wide array of applica-
tions. Quantum machine learning is a very new and ex-
citing ﬁeld, so we expect the framework to change with
the needs of the research community, and the availabil-
ity of new quantum hardware. We have open-sourced
the framework under the commercially friendly Apache2
license, allowing future commercial products to embed
TFQ royalty-free. If you would like to participate in our
community, visit us at:

github.com/tensorflow/quantum/

VII. ACKNOWLEDGEMENTS

The authors would like to thank Google Research for
supporting this project. In particular, M.B., G.V., T.M.,
and A.J.M. would like to thank the Google Quantum AI
team for their support during their respective internships,
and several useful discussions with Matt Harrigan, John

52

Platt, and Nicholas Rubin. The authors would like to
also thank Achim Kempf from the University of Water-
loo for sponsoring this project. M.B. and J.Y. would like
to thank the Google Brain and Core team for support-
ing this project, in particular Christian Howard, Billy
Lamberta, Tom O’Malley, Francois Chollet, Yifei Feng,
David Rim, Justin Hong, and Megan Kacholia. G.V.,
A.J.M. and J.Y. would like to thank Stefan Leichenauer,
Jack Hidary and the rest of the Sandbox@Alphabet team
for their support during their respective Quantum Resi-
dencies. G.V. acknowledges support from NSERC. D.B.
is an Associate Fellow in the CIFAR program on Quan-
tum Information Science. A.S. and M.S. were supported
by the USRA Feynman Quantum Academy funded by
the NAMS R&D Student Program at NASA Ames Re-
search Center and by the Air Force Research Labora-
tory (AFRL), NYSTEC-USRA Contract (FA8750-19-3-
6101). A.Z. acknowledges support from Caltech’s Intel-
ligent Quantum Networks and Technologies (INQNET)
research program and by the DOE/HEP QuantISED pro-
gram grant, Quantum Machine Learning and Quantum
Computation Frameworks (QMLQCF) for HEP, award
number DE-SC0019227. S.J. acknowledges support from
the Austrian Science Fund (FWF) through the projects
DK-ALM:W1259-N27 and SFB BeyondC F7102, and
from the Austrian Academy of Sciences as a recipient of
the DOC Fellowship. V.D. acknowledges support from
the Dutch Research Council (NWO/OCW), as part of
the Quantum Software Consortium programme (project
number 024.003.037).

[1] K. P. Murphy, Machine learning: a probabilistic perspec-

tive (MIT press, 2012).

[2] J. A. Suykens and J. Vandewalle, Neural processing let-

ters 9, 293 (1999).

[3] S. Wold, K. Esbensen, and P. Geladi, Chemometrics

and intelligent laboratory systems 2, 37 (1987).

[4] A. K. Jain, Pattern recognition letters 31, 651 (2010).
[5] Y. LeCun, Y. Bengio, and G. Hinton, nature 521, 436

(2015).

[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, in Ad-
vances in Neural Information Processing Systems 25 ,
edited by F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger (Curran Associates, Inc., 2012) pp.
1097–1105.

[7] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio,
Deep learning, Vol. 1 (MIT press Cambridge, 2016).
[8] J. Preskill, arXiv preprint arXiv:1801.00862 (2018).
[9] R. P. Feynman, International Journal of Theoretical

Physics 21, 467 (1982).

[10] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D.
Johnson, M. Kieferová, I. D. Kivlichan, T. Menke,
B. Peropadre, N. P. Sawaya, et al., Chemical reviews
119, 10856 (2019).

[11] P. W. Shor, in Proceedings 35th annual symposium on
foundations of computer science (Ieee, 1994) pp. 124–

134.

[12] E. Farhi, J. Goldstone,

tum approximate optimization algorithm,”
arXiv:1411.4028 [quant-ph].

and S. Gutmann, “A quan-
(2014),

[13] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A.
Buell, et al., Nature 574, 505 (2019).

[14] S. Lloyd, M. Mohseni,

and P. Rebentrost, Nature

Physics 10, 631–633 (2014).

[15] P. Rebentrost, M. Mohseni, and S. Lloyd, Phys. Rev.

Lett. 113, 130503 (2014).

[16] S. Lloyd, M. Mohseni, and P. Rebentrost, “Quantum
algorithms for supervised and unsupervised machine
learning,” (2013), arXiv:1307.0411 [quant-ph].

[17] I. Kerenidis and A. Prakash, “Quantum recommenda-

tion systems,” (2016), arXiv:1603.08675.

[18] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, Nature 549, 195 (2017).
[19] V. Giovannetti, S. Lloyd, and L. Maccone, Physical

review letters 100, 160501 (2008).

[20] S. Arunachalam, V. Gheorghiu, T. Jochym-O’Connor,
M. Mosca, and P. V. Srinivasan, New Journal of Physics
17, 123010 (2015).

[21] E. Tang,

the 51st Annual ACM
SIGACT Symposium on Theory of Computing (2019)

in Proceedings of

pp. 217–228.

[22] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush,
S. Boixo, H. Neven, and J. R. McClean, Nature Com-
munications 12, 2631 (2021).
[23] J. Preskill, Quantum 2, 79 (2018).
[24] E. Farhi and H. Neven, arXiv preprint arXiv:1802.06002

(2018).

[25] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-
and J. L.

Q. Zhou, P. J. Love, A. Aspuru-Guzik,
O’brien, Nature communications 5, 4213 (2014).

[26] N. Killoran, T. R. Bromley,

J. M. Arrazola,
M. Schuld, N. Quesada, and S. Lloyd, arXiv preprint
arXiv:1806.06871 (2018).

[27] D. Wecker, M. B. Hastings, and M. Troyer, Phys. Rev.

A 92, 042303 (2015).

[28] L. Zhou, S.-T. Wang, S. Choi, H. Pichler, and M. D.

53

[50] Z. Wang, N. C. Rubin, J. M. Dominy, and E. G. Rieﬀel,

arXiv preprint arXiv:1904.09314 (2019).

[51] E. Farhi and H. Neven, “Classiﬁcation with quantum
(2018),

neural networks on near term processors,”
arXiv:1802.06002 [quant-ph].

[52] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
and H. Neven, Nature communications 9, 1

bush,
(2018).

[53] I. Cong, S. Choi, and M. D. Lukin, Nature Physics 15,

1273–1278 (2019).

[54] S. Lloyd and C. Weedbrook, Physical review letters 121,

040502 (2018).

[55] A. Arrasmith, M. Cerezo, P. Czarnik, L. Cincio, and
P. J. Coles, “Eﬀect of barren plateaus on gradient-free
optimization,” (2020), arXiv:2011.12245.

[56] G. Verdon, J. Pye, and M. Broughton, arXiv preprint

Lukin, arXiv preprint arXiv:1812.01041 (2018).

arXiv:1806.09729 (2018).

[29] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, New Journal of Physics 18, 023023 (2016).
[30] S. Hadﬁeld, Z. Wang, B. O’Gorman, E. G. Rief-
and R. Biswas, arXiv preprint

fel, D. Venturelli,
arXiv:1709.03489 (2017).

[31] E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart,
V. Stojevic, A. G. Green, and S. Severini, npj Quantum
Information 4, 1 (2018).

[32] S. Khatri, R. LaRose, A. Poremba, L. Cincio, A. T.

Sornborger, and P. J. Coles, Quantum 3, 140 (2019).

[33] M. Schuld and N. Killoran, Physical review letters 122,

040504 (2019).

[34] S. McArdle, T. Jones, S. Endo, Y. Li, S. Benjamin, and
X. Yuan, arXiv preprint arXiv:1804.03023 (2018).
[35] M. Benedetti, E. Grant, L. Wossnig, and S. Severini,

New Journal of Physics 21, 043023 (2019).

[36] B. Nash, V. Gheorghiu, and M. Mosca, arXiv preprint

arXiv:1904.01972 (2019).

[37] Z. Jiang, J. McClean, R. Babbush, and H. Neven, arXiv

preprint arXiv:1812.08190 (2018).

[57] J. Romero and A. Aspuru-Guzik, arXiv preprint

arXiv:1901.00848 (2019).

[58] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, C. Blank,
and N. Killoran, arXiv preprint

K. McKiernan,
arXiv:1811.04968 (2018).
[59] G. Verdon, A. J. Martinez,

J. Marks, S. Pa-
tel, G. Roeder, F. Sbahi, J. H. Yoo, S. Nanda,
S. Leichenauer,
and J. Hidary, arXiv preprint
arXiv:1910.02071 (2019).

[60] H.-Y. Huang, R. Kueng, and J. Preskill, Phys. Rev.

Lett. 126, 190505 (2021).

[61] Google AI Quantum and Collaborators, Science 369,

1084 (2020).

[62] M. Mohseni, A. M. Steinberg, and J. A. Bergou, Phys.

Rev. Lett. 93, 200403 (2004).

[63] K. K. van Dam, From Long-distance Entanglement to
Building a Nationwide Quantum Internet: Report of
the DOE Quantum Internet Blueprint Workshop, Tech.
Rep. BNL-216179-2020-FORE (Brookhaven National
Laboratory, 2020).

[38] G. R. Steinbrecher, J. P. Olson, D. Englund, and J. Car-

[64] J. J. Meyer, J. Borregaard, and J. Eisert, npj Quantum

olan, arXiv preprint arXiv:1808.10047 (2018).

[39] M. Fingerhuth, T. Babej,
arXiv:1810.13411 (2018).

et al., arXiv preprint

[40] R. LaRose, A. Tikku, É. O’Neel-Judy, L. Cincio, and

P. J. Coles, arXiv preprint arXiv:1810.10506 (2018).

[41] L. Cincio, Y. Subaşı, A. T. Sornborger, and P. J. Coles,

New Journal of Physics 20, 113022 (2018).

[42] H. Situ, Z. Huang, X. Zou, and S. Zheng, Quantum

Information Processing 18, 230 (2019).

[43] H. Chen, L. Wossnig, S. Severini, H. Neven,

and
M. Mohseni, arXiv preprint arXiv:1805.08654 (2018).
and J. Biamonte, arXiv

[44] G. Verdon, M. Broughton,

preprint arXiv:1712.05304 (2017).

[45] M. Mohseni et al., Nature 543, 171 (2017).
[46] G. Verdon, M. Broughton, J. R. McClean, K. J. Sung,
R. Babbush, Z. Jiang, H. Neven,
and M. Mohseni,
“Learning to learn with quantum neural networks via
classical neural networks,” (2019), arXiv:1907.05415.

[47] R. Sweke, F. Wilde, J. Meyer, M. Schuld, P. K.
Fährmann, B. Meynard-Piganeau, and J. Eisert, arXiv
preprint arXiv:1910.01155 (2019).

[48] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, New Journal of Physics 18, 023023 (2016).
[49] G. Verdon, J. M. Arrazola, K. Brádler, and N. Killoran,

arXiv preprint arXiv:1902.00409 (2019).

Information 7, 1 (2021).

[65] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven,

npj Quantum Information 5, 1 (2019).

[66] J. Carolan, M. Mohseni, J. P. Olson, M. Prabhu,
C. Chen, D. Bunandar, M. Y. Niu, N. C. Harris, F. N. C.
Wong, M. Hochberg, S. Lloyd, and D. Englund, Nature
Physics (2020).

[67] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-
ard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Lev-
enberg, D. Mane, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever,
K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan,
F. Viegas, O. Vinyals, P. Warden, M. Wattenberg,
M. Wicke, Y. Yu, and X. Zheng, “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed sys-
tems,” (2016), arXiv:1603.04467 [cs.DC].

[68] G. T. C. Developers, “Cirq: A python framework for
creating, editing, and invoking noisy intermediate scale
quantum circuits,” (2018).

[69] A. Meurer, C. P. Smith, M. Paprocki, O. Čertík, S. B.
Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K.
Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger,
R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johans-
son, F. Pedregosa, M. J. Curry, A. R. Terrel, v. Roučka,

A. Saboo, I. Fernando, S. Kulal, R. Cimrman,
A. Scopatz, PeerJ Computer Science 3, e103 (2017).
[70] D. E. Rumelhart, G. E. Hinton, and R. J. Williams,

and

nature 323, 533 (1986).

[71] J. Biamonte and V. Bergholm, arXiv

(2017),

arXiv:1708.00006 [quant-ph].

[72] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman,
B. Fontaine, Y. Zou, J. Hidary, G. Vidal, and S. Le-
“TensorNetwork: A Library for Physics
ichenauer,
and Machine Learning,”
(2019), arXiv:1905.01330
[physics.comp-ph].

[73] C. Roberts and S. Leichenauer, “Introducing Tensor-
Network, an Open Source Library for Eﬃcient Tensor
Calculations,” (2019).

[74] The TensorFlow Authors, “Eﬀective TensorFlow 2,”

(2019).

[75] F. Chollet et al., “Keras,” https://keras.io (2015).
[76] M. Y. Niu, A. Zlokapa, M. Broughton, S. Boixo,
M. Mohseni, V. Smelyanskyi,
and H. Neven, “En-
tangling quantum generative adversarial networks,”
(2021), arXiv:2105.00080 [quant-ph].

[77] J. R. Johansson, P. D. Nation, and F. Nori, Comput.

Phys. Commun. 183, 1760 (2012).

[78] D. P. Kingma and J. Ba, arXiv preprint arXiv:1412.6980

(2014).

[79] G. E. Crooks, arXiv preprint arXiv:1905.13311 (2019).
and A. Aspuru-
[80] M. Smelyanskiy, N. P. D. Sawaya,
Guzik, ArXiv e-prints (2016), arXiv:1601.07195 [quant-
ph].

[81] T. Häner and D. S. Steiger, ArXiv e-prints

(2017),

arXiv:1704.01127 [quant-ph].

[82] Intel®, “Intel® Instruction Set Extensions Technol-

ogy,” (2020).

[83] Mark Buxton, “Haswell New Instruction Descriptions

Now Available!”

(2011).

[84] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush,
N. Ding, Z. Jiang, M. J. Bremner, J. M. Martinis, and
H. Neven, Nature Physics 14, 595 (2018).

[85] D. Gottesman, Stabilizer codes and quantum error cor-
rection, Ph.D. thesis, California Institute of Technology
(1997).

[86] M. Suzuki, Physics Letters A 146, 319 (1990).
[87] E. Campbell, Physical Review Letters 123 (2019),

10.1103/physrevlett.123.070503.

[88] N. C. Rubin, R. Babbush, and J. McClean, New Jour-

nal of Physics 20, 053020 (2018).

[89] A. Harrow

and

J. Napp,

arXiv

preprint

arXiv:1901.05374 (2019).

54

pp. 265–283.

[96] R. Frostig, M. J. Johnson, and C. Leary, Systems for

Machine Learning (2018).

[97] M. Abramowitz and I. Stegun, Appl. Math. Ser 55

(2006).

[98] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac,

and

N. Killoran, arXiv preprint arXiv:1811.11184 (2018).

[99] I. Newton, The Principia: mathematical principles of
natural philosophy (Univ of California Press, 1999).

[100] K. Mitarai, M. Negoro, M. Kitagawa,
Physical Review A 98, 032309 (2018).

and K. Fujii,

[101] S. Bhatnagar, H. Prasad, and L. Prashanth, Stochastic
Recursive Algorithms for Optimization: Simultaneous
Perturbation Methods (Springer, 2013).

[102] L. S. Pontryagin, Mathematical theory of optimal pro-

cesses (CRC press, 1987).

[103] X.-Z. Luo, J.-G. Liu, P. Zhang, and L. Wang, Quantum

4, 341 (2020).

[104] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker,
and J. Eisert, Physical Review Letters 105 (2010),
10.1103/physrevlett.105.150401.
[105] J. Schulman, N. Heess, T. Weber,

and P. Abbeel,
in Advances in Neural Information Processing Systems
(2015) pp. 3528–3536.

[106] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow,
A. Kandala, J. M. Chow, and J. M. Gambetta, Nature
567, 209 (2019).

[107] W. Huggins, P. Patil, B. Mitchell, K. B. Whaley, and
E. M. Stoudenmire, Quantum Science and Technology
4, 024001 (2019).

[108] N. Tishby and N. Zaslavsky,

“Deep learning and
(2015),

information bottleneck principle,”

the
arXiv:1503.02406 [cs.LG].

[109] P. Walther, K. J. Resch, T. Rudolph, E. Schenck, H. We-
infurter, V. Vedral, M. Aspelmeyer, and A. Zeilinger,
Nature 434, 169 (2005).

[110] M. A. Nielsen, Reports on Mathematical Physics 57,

147 (2006).

[111] G. Vidal, Physical Review Letters 101 (2008),

10.1103/physrevlett.101.110501.

[112] R. Shwartz-Ziv and N. Tishby, “Opening the black
(2017),

box of deep neural networks via information,”
arXiv:1703.00810 [cs.LG].

[113] P. B. M. Sousa and R. V. Ramos, “Universal quan-
for n-qubit quantum gate: A pro-
(2006), arXiv:quant-

tum circuit
grammable quantum gate,”
ph/0602174 [quant-ph].

[114] M. Y. Niu, L. Li, M. Mohseni, and I. Chuang, to be

[90] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner, Pro-

published (2020).

ceedings of the IEEE 86, 2278 (1998).

[91] L. Bottou,

in Proceedings
(Springer, 2010) pp. 177–186.

of COMPSTAT’2010

[92] S. Ruder, arXiv preprint arXiv:1609.04747 (2016).
[93] Y. LeCun, D. Touresky, G. Hinton, and T. Sejnowski,
in Proceedings of the 1988 connectionist models summer
school, Vol. 1 (CMU, Pittsburgh, Pa: Morgan Kauf-
mann, 1988) pp. 21–28.

[94] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and
J. M. Siskind, The Journal of Machine Learning Re-
search 18, 5595 (2017).

[95] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
et al., in 12th {USENIX} symposium on operating sys-
tems design and implementation ({OSDI} 16) (2016)

[115] J. M. Martinis, npj Quantum Inf 7 (2021).
[116] M. McEwen, L. Faoro, K. Arya, A. Dunsworth,
T. Huang, S. Kim, B. Burkett, A. Fowler, F. Arute,
J. C. Bardin, A. Bengtsson, A. Bilmes, B. B. Buck-
ley, N. Bushnell, Z. Chen, R. Collins, S. Demura,
A. R. Derk, C. Erickson, M. Giustina, S. D. Har-
rington, S. Hong, E. Jeﬀrey, J. Kelly, P. V. Klimov,
F. Kostritsa, P. Laptev, A. Locharla, X. Mi, K. C.
Miao, S. Montazeri, J. Mutus, O. Naaman, M. Nee-
ley, C. Neill, A. Opremcak, C. Quintana, N. Redd,
P. Roushan, D. Sank, K. J. Satzinger, V. Shvarts,
T. White, Z. J. Yao, P. Yeh, J. Yoo, Y. Chen,
V. Smelyanskiy, J. M. Martinis, H. Neven, A. Megrant,
L. Ioﬀe, and R. Barends, “Resolving catastrophic error
bursts from cosmic rays in large arrays of superconduct-

ing qubits,” (2021), arXiv:2104.05219 [quant-ph].
[117] Y. Chen, M. W. Hoﬀman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas, arXiv
preprint arXiv:1611.03824 (2016).

[140] J. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Va-
sudevan, D. Moore, B. Patton, A. Alemi, M. Hoﬀman,
and R. A. Saurous, “Tensorﬂow distributions,” (2017),
arXiv:1711.10604 [cs.LG].

[118] E. Farhi, J. Goldstone, and S. Gutmann, arXiv preprint

[141] C. P. Robert,

“The metropolis-hastings algorithm,”

55

arXiv:1412.6062 (2014).

[119] G. S. Hartnett and M. Mohseni,

ity density theory for spin-glass systems,”
arXiv:2001.00927.

“A probabil-
(2020),

[120] G. S. Hartnett and M. Mohseni, “Self-supervised learn-
ing of generative spin-glasses with normalizing ﬂows,”
(2020), arXiv:2001.00585.

[121] A. Hagberg, P. Swart,

and D. S Chult, Exploring
network structure, dynamics, and function using Net-
workX, Tech. Rep. (Los Alamos National Lab.(LANL),
Los Alamos, NM (United States), 2008).

[122] M. Streif and M. Leib, Quantum Sci. Technol. 5, 034008

(2020).

[123] H. Pichler, S.-T. Wang, L. Zhou, S. Choi,

and
M. D. Lukin, “Quantum optimization for maximum
independent set using rydberg atom arrays,”
(2018),
arXiv:1808.10816.

[124] M. Wilson, S. Stromswold, F. Wudarski, S. Had-
“Optimiz-
(2019),

ﬁeld, N. M. Tubman,
ing quantum heuristics with meta-learning,”
arXiv:1908.03185 [quant-ph].

and E. Rieﬀel,

[125] E. Farhi, J. Goldstone, and S. Gutmann, ArXiv e-prints

(2014), arXiv:1411.4028 [quant-ph].

[126] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
bush, and H. Neven, Nature Communications 9 (2018),
10.1038/s41467-018-07090-4.

[127] X. Glorot and Y. Bengio, in AISTATS (2010) pp. 249–

256.

[128] M. Cerezo, A. Sone, T. Volkoﬀ, L. Cincio, and P. J.
Coles, arXiv preprint arXiv:2001.00550 (2020).

[129] Y. Bengio,

in Neural networks: Tricks of the trade

(Springer, 2012) pp. 437–478.

[130] E. Knill, G. Ortiz, and R. D. Somma, Physical Review

A 75, 012328 (2007).

[131] E. Grant, L. Wossnig, M. Ostaszewski,
M. Benedetti, Quantum 3, 214 (2019).

and

[132] A. Skolik, J. R. McClean, M. Mohseni, P. van der
Smagt, and M. Leib, Quantum Machine Intelligence
3, 1 (2021).

[133] E. Campos, A. Nasrallah, and J. Biamonte, Physical

Review A 103, 032607 (2021).

[134] C. Cirstoiu, Z. Holmes, J. Iosue, L. Cincio, P. J.
Coles, and A. Sornborger, “Variational fast forward-
ing for quantum simulation beyond the coherence time,”
(2019), arXiv:1910.04292 [quant-ph].
[135] N. Wiebe, C. Granade, C. Ferrie,

and D. Cory,
Physical Review Letters 112 (2014), 10.1103/phys-
revlett.112.190501.

[136] G. Verdon, T. McCourt, E. Luzhnica, V. Singh, S. Le-
ichenauer, and J. Hidary, “Quantum graph neural net-
works,” (2019), arXiv:1909.12264 [quant-ph].

[137] S. Greydanus, M. Dzamba, and J. Yosinski, “Hamil-
(2019), arXiv:1906.01563

tonian neural networks,”
[cs.NE].

[138] L. Cincio, Y. Subaşı, A. T. Sornborger, and P. J. Coles,

New Journal of Physics 20, 113022 (2018).

[139] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information: 10th Anniversary Edition,
10th ed. (Cambridge University Press, USA, 2011).

(2015), arXiv:1504.01896 [stat.CO].

[142] S. McArdle, S. Endo, A. Aspuru-Guzik, S. C. Benjamin,
and X. Yuan, Reviews of Modern Physics 92, 015003
(2020).

[143] J. R. McClean, M. E. Kimchi-Schwartz, J. Carter, and
W. A. De Jong, Physical Review A 95, 042308 (2017).
[144] O. Higgott, D. Wang, and S. Brierley, Quantum 3, 156

(2019).

[145] K. M. Nakanishi, K. Mitarai, and K. Fujii, Physical

Review Research 1, 033062 (2019).
[146] K. Abhinav et al., Nature 549, 242 (2017).
[147] H. R. Grimsley, S. E. Economou, E. Barnes, and N. J.
Mayhall, Nature communications 10, 1 (2019).
[148] M. Ostaszewski, E. Grant, and M. Benedetti, Quantum

5, 391 (2021).

[149] P. K. Barkoutsos, G. Nannicini, A. Robert, I. Tavernelli,

and S. Woerner, Quantum 4, 256 (2020).

[150] N. Yamamoto, arXiv preprint arXiv:1909.05074 (2019).
[151] J. R. McClean, N. C. Rubin, K. J. Sung, I. D. Kivlichan,
X. Bonet-Monroig, Y. Cao, C. Dai, E. S. Fried, C. Gid-
ney, B. Gimby, et al., Quantum Science and Technology
5, 034014 (2020).

[152] Q. Sun, T. C. Berkelbach, N. S. Blunt, G. H. Booth,
S. Guo, Z. Li, J. Liu, J. D. McClain, E. R. Say-
futyarova, S. Sharma, et al., Wiley Interdisciplinary
Reviews: Computational Molecular Science 8, e1340
(2018).

[153] A.

W.
Proceedings

Sandvik,

ence
https://aip.scitation.org/doi/pdf/10.1063/1.3518900.

1297,

135

Confer-
(2010),

AIP

[154] F. Verstraete,
in
Advances
https://doi.org/10.1080/14789940801912366.

V. Murg,
Physics

57,

and

143

J. Cirac,
(2008),

[155] J. Carrasquilla and R. G. Melko, Nature Physics 13,

431 (2017).

[156] E. P. L. van Nieuwenburg, Y.-H. Liu, and S. D. Huber,

Nature Physics 13, 435 (2017).

[157] A. V. Uvarov, A. S. Kardashin, and J. D. Biamonte,

Phys. Rev. A 102, 012415 (2020).

[158] A. Smith, M. S. Kim, F. Pollmann, and J. Knolle, npj

Quantum Information 5, 106 (2019).

[159] W. W. Ho and T. H. Hsieh, SciPost Phys. 6, 29 (2019).
[160] D. Wierichs, C. Gogolin, and M. Kastoryano, Phys.

Rev. Research 2, 043246 (2020).

[161] R. Wiersema, C. Zhou, Y. de Sereville, J. F. Car-
and H. Yuen, PRX Quantum

rasquilla, Y. B. Kim,
1, 020319 (2020).

[162] M. S. L. du Croo de Jongh and J. M. J. van Leeuwen,

Phys. Rev. B 57, 8494 (1998).

[163] H. W. J. Blöte and Y. Deng, Phys. Rev. E 66, 066110

(2002).

[164] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
in Advances in neural information processing systems
(2014) pp. 2672–2680.

[165] P.-L. Dallaire-Demers and N. Killoran, Phys. Rev. A 98,

012324 (2018).

[166] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, Nature (London) 549, 195

56

(2017), arXiv:1611.09347 [quant-ph].

[167] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A.
Buell, et al., arXiv:1910.11333 (2019).

[168] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, A. Bengtsson, S. Boixo, M. Broughton,
B. B. Buckley, et al., arXiv preprint arXiv:2010.07965
(2020).

[169] N. Klco and M. J. Savage, Phys. Rev. A 102, 012612

(2020).

[170] R. S. Sutton, A. G. Barto, et al., Introduction to re-
inforcement learning, Vol. 135 (MIT press Cambridge,
1998).

[171] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-
ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, et al., nature 518, 529 (2015).
[172] P. Mirowski, M. K. Grimes, M. Malinowski, K. M.
Hermann, K. Anderson, D. Teplyashin, K. Simonyan,
K. Kavukcuoglu, A. Zisserman, and R. Hadsell, arXiv
preprint arXiv:1804.00168 (2018).

[173] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,
A. Bolton, et al., nature 550, 354 (2017).

[174] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Math-
ieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, et al., Nature 575, 350 (2019).
[175] C. Berner, G. Brockman, B. Chan, V. Cheung,
P. Dębiak, C. Dennison, D. Farhi, Q. Fischer,

S. Hashme, C. Hesse, et al., “Dota 2 with large scale
deep reinforcement learning,” (2019), arXiv:1912.06680
[cs.LG].

[176] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and
V. Dunjko, “Variational quantum policies for reinforce-
ment learning,” (2021), arXiv:2103.05577 [quant-ph].

[177] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma,
and H.-S. Goan, IEEE Access 8, 141007 (2020).
[178] O. Lockwood and M. Si, in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence and Interactive
Digital Entertainment, Vol. 16 (2020) pp. 245–251.
[179] A. Skolik, S. Jerbi, and V. Dunjko, “Quantum agents
in the gym: a variational quantum algorithm for deep
q-learning,” (2021), arXiv:2103.15084 [quant-ph].
[180] O. Lockwood and M. Si, in NeurIPS 2020 Workshop
on Pre-registration in Machine Learning (PMLR, 2021)
pp. 285–301.

[181] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,
J. Schulman, J. Tang, and W. Zaremba, “Openai gym,”
(2016), arXiv:1606.01540 [cs.LG].

[182] M. Schuld, R. Sweke, and J. J. Meyer, Physical Review

A 103, 032430 (2021).

[183] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and

J. I. Latorre, Quantum 4, 226 (2020).

[184] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,

et al., in NIPS, Vol. 99 (1999) pp. 1057–1063.

[185] S. Wu, S. Jin, D. Wen, and X. Wang, arXiv preprint

arXiv:2012.10711 (2020).

