1
2
0
2

l
u
J

8

]

G
L
.
s
c
[

2
v
1
8
6
3
1
.
6
0
1
2
:
v
i
X
r
a

Robust Matrix Factorization with Grouping Effect

Haiyan Jiang1,

Shuyu Li2†, Luwei Zhang2†, Haoyi Xiong1, Dejing Dou1

1 Baidu Research, Baidu Inc., China
2 Columbia University, New York, NY, USA

jianghaiyan01@baidu.com,
shuryli@outlook.com, helenzhang0322@gmail.com,
xionghaoyi@baidu.com, doudejing@baidu.com

Abstract

Although many techniques have been applied to matrix factorization (MF), they
may not fully exploit the feature structure. In this paper, we incorporate the group-
ing effect into MF and propose a novel method called Robust Matrix Factorization
with Grouping effect (GRMF). The grouping effect is a generalization of the spar-
sity effect, which conducts denoising by clustering similar values around multiple
centers instead of just around 0. Compared with existing algorithms, the proposed
GRMF can automatically learn the grouping structure and sparsity in MF without
prior knowledge, by introducing a naturally adjustable non-convex regulariza-
tion to achieve simultaneous sparsity and grouping effect. Speciﬁcally, GRMF
uses an efﬁcient alternating minimization framework to perform MF, in which
the original non-convex problem is ﬁrst converted into a convex problem through
Difference-of-Convex (DC) programming, and then solved by Alternating Direc-
tion Method of Multipliers (ADMM). In addition, GRMF can be easily extended
to the Non-negative Matrix Factorization (NMF) settings. Extensive experiments
have been conducted using real-world data sets with outliers and contaminated
noise, where the experimental results show that GRMF has promoted performance
and robustness, compared to ﬁve benchmark algorithms.
Keywords: Robust Matrix Factorization, Feature Grouping, Sparsity, Non-convex
Regularization

1

Introduction

Matrix factorization (MF) is a fundamental yet widely-used technique in machine learning (Chi et al.,
2019, Choi, 2008, Dai et al., 2020, Li et al., 2019, Mnih and Salakhutdinov, 2007, Parvin et al.,
2019, Srebro et al., 2005, Wang et al., 2020), with numerous applications in computer vision (Gao
et al., 2022, Haeffele and Vidal, 2019, Wang and Yeung, 2013, Wang et al., 2012, Xu et al., 2020),
recommender systems (Jakomin et al., 2020, Koren et al., 2009, Xue et al., 2017), bioinformatics (Cui
et al., 2019, Gaujoux and Seoighe, 2010, Jamali et al., 2020, Pascual-Montano et al., 2006), social
networks (Gurini et al., 2018, Ma et al., 2008, Zhang et al., 2020), and many others.
In MF,
with a given data matrix Y ∈ Rd×n, one is interested in approximating Y by UVT such that
the reconstruction error between the given matrix and its factorization is minimized, where U ∈
Rd×r, V ∈ Rn×r, and r (cid:28) min(d, n) is the rank. Various regularizers have been introduced to
generalize the low-rank MF under different conditions and constraints in the following form

min
U∈Rd×r,V∈Rn×r

1
2

(cid:107)Y − UVT (cid:107)α

α + R1(U) + R2(V),

(1.1)

†The work was done when the author was an intern at Baidu Research.

Preprint. Under review.

 
 
 
 
 
 
where R1 and R2 refer to the regularization terms and α corresponds to the loss types (e.g., α = 2
for the quadratic loss) for reconstruction errors. Existing studies (Abdolali and Gillis, 2021, Kim
et al., 2012, Lin et al., 2017) show that the use of regularization terms could prevent overﬁtting or
impose sparsity, while the choice of loss types (e.g., α = 1 for (cid:96)1-loss) helps to establish robustness
of MF with outliers and noise.

Background The goal of matrix factorization is to obtain the low-dimensional structures of the data
matrix while preserving the major information, where singular value decomposition (SVD) (Klema
and Laub, 1980) and principal component analysis (PCA) (Wold et al., 1987) are two conventional
solutions. To better deal with sparsity in high-dimensional data, the truncated-SVD (Hansen, 1987)
is proposed to achieve result with a determined rank. Like traditional SVD, it uses the (cid:96)2 loss for
reconstruction errors, but has additional requirement on the norm of the solution.

Loss terms in MF While the above models could partially meet the goal of MF, they might be
vulnerable to outliers and noise and lack robustness. Thus they often give unsatisfactory results
on real world data due to the use of inappropriate loss terms for reconstruction errors. To solve
the problem, Wang et al. (2012) proposed a probabilistic method for robust matrix factorization
(PRMF) based on the (cid:96)1 loss which is formulated using the Laplace error assumption and optimized
with a parallelizable Expectation-Maximization (EM) algorithm. In addition to traditional (cid:96)1 or
(cid:96)2-loss, researchers have also made attempts with non-convex loss functions. Yao and Kwok (2018)
constructed a scalable robust matrix factorization with non-convex loss, using general non-convex
loss functions to make the MF model applicable in more situations. On the contrary, Haeffele et al.
(2014) employed the non-convex projective tensor norm. Yet another algorithm with identiﬁability
guarantee has been proposed in Abdolali and Gillis (2021), where the simplex-structured matrix
factorization (SSMF) is invented to constrain the matrix V to belong to the unit simplex.

Sparsity control in MF In addition to the loss terms, sparsity regularizers or constraints have
been proposed to control the number of nonzero elements in MF (either the recovered matrix or the
noise matrix). For example, Hoyer (2004) employed a sparseness measure based on the relationship
between the (cid:96)1-norm and the (cid:96)2-norm. Candès et al. (2011) introduced a Robust PCA (RPCA)
which decomposes the matrix M by M = L0 + S0, where L0 is assumed to have low-rank and
S0 is constrained to be sparse by an (cid:96)1 regularizer. The Robust Non-negative Matrix Factorization
(RNMF) (Zhang et al., 2011) decomposes the non-negative data matrix as the summation of one
sparse error matrix and the product of two non-negative matrices, where the sparsity of the error
matrix is constrained with (cid:96)1-norm penalty. Compared with the (cid:96)2-norm penalty and the (cid:96)1-norm
penalty used in the mentioned methods, the (cid:96)0-norm penalty directly addresses the sparsity measure.
However, the direct use of the (cid:96)0-norm penalty gives us a non-convex NP-hard optimization problem.
To address that difﬁculty, researchers have proposed different approximations, such as the truncated
(cid:96)1 penalty proposed in Shen et al. (2012).

Our work Previous studies on MF pay great attention to sparsity, yet few of them (Li et al., 2013,
Ortega et al., 2016, Rahimpour et al., 2017, Yuen et al., 2012) take care of the grouping effect. While
sparse control can only identify factors whose weights are clustered around 0 and eliminate the noise
caused by them, the grouping effect can identify factor groups whose weights are clustered around
any similar value and eliminate the noise caused by all these groups. In fact, the grouping effect can
bring intepretability and recovery accuracy to MFs to a larger extent than sparsity alone does. With
the desire to conduct matrix factorization with robustness, sparsity, and grouping effect, we construct
a new matrix factorization model named Robust Matrix Factorization with Grouping effect (GRMF)
and evaluate its performance through experiments.

To the best of our knowledge, this work has made unique contributions in introducing sparsity
and grouping effect into MF. The most relevant works are Kim et al. (2012), Yang et al. (2012).
Speciﬁcally, Shen et al. (2012), Yang et al. (2012) proposed to use truncated (cid:96)1 penalty to pursue
both sparsity and grouping effect in estimation of linear regression models, while the way of using
such techniques to improve MF is not known. On the other hand, Kim et al. (2012) also modeled MF
with groups but assumes features in the same group sharing the same sparsity pattern with predeﬁned
group partition. Thus a mixed norm regularization is proposed to promote group sparsity in the
factor matrices of non-negative MF. Compared to Kim et al. (2012), our proposed GRMF is more
general-purpose, where the sparsity (e.g., elements in the factor matrix centered around zero) is only
one special group in all possible groups for elements in factor matrices.

2

Speciﬁcally, to achieve robustness, the focus of GRMF is on matrix factorization under the (cid:96)1-loss.
GRMF further adopts an (cid:96)0 surrogate—truncated (cid:96)1, for the penalty term to control the sparsity and
the grouping effect. As the resulting problem is non-convex, we solve it with difference-of-convex
algorithm (DC) and Alternating Direction Method of Multipliers (ADMM). The algorithms are
implemented to conduct experiments on COIL-20, ORL, extended Yale B, and JAFFE datasets. In
addition, we compare the result with 5 benchmark algorithms: Truncated SVD (Hansen, 1987),
RPCA (Candès et al., 2011), RNMF (Wen et al., 2018), RPMF (Wang et al., 2012) and GoDec+ (Guo
et al., 2017). The results show that the proposed GRMF signiﬁcantly outperforms existing benchmark
algorithms.

The remainder of the paper is organized as follows. In Section 2, we brieﬂy introduce the robust
MF and the non-negative MF. We propose the GRMF in Section 3 and give the algorithm for GRMF
which uses DC and ADMM for solving the resulting non-convex optimization problem in Section
4. Experiment results to show the performances of GRMF and comparison with other benchmark
algorithms are presented in Section 5. We conclude the paper in Section 6.

Notations For a scalar x, sign(x) = 1 if x > 0, 0 if x = 0, and −1 otherwise. For a matrix
A, (cid:107)A(cid:107)F = ((cid:80)
2 is its Frobenius norm, and (cid:107)A(cid:107)(cid:96)1 = (cid:80)
ij) 1
i,j A2
i,j |Aij| is its (cid:96)1-norm. For a
vector x, (cid:107)x(cid:107)(cid:96)1 = (cid:80)
i |xi| is its (cid:96)1-norm. Denote data matrix Yd×n = [y·1, · · · , y·n] the stack
of n column vectors with each y·j ∈ Rd, or YT = [yT
1·, · · · , yT
d·] the stack of d row vectors with
each yi· ∈ Rn. Denote UT = [u1, · · · , ud] ∈ Rr×d, and VT = [v1, · · · , vn] ∈ Rr×n, with each
u1, · · · , ud, v1, · · · , vn ∈ Rr. Denote vjl the l-th element of the vector vj ∈ Rr and uil the l-th
element of the vector ui ∈ Rr.

2 Preliminaries

Robust matrix factorization (RMF) Given a data matrix Y ∈ Rd×n, the matrix factoriza-
tion (Yao and Kwok, 2018) is formulated as Y = UVT + ε. Here U ∈ Rd×r, V ∈ Rn×r,
r (cid:28) min(d, n) is the rank, and ε is the noise/error term. The RMF can be formulated under
the Laplacian error assumption, minU,V (cid:107)Y − UVT (cid:107)(cid:96)1 = (cid:80)d
i vj|, where
UT = [u1, · · · , ud] ∈ Rr×d, and VT = [v1, · · · , vn] ∈ Rr×n. Adding regularizers gives the opti-
mization problem, minU∈Rd×r,V∈Rn×r (cid:107)Y − UVT (cid:107)(cid:96)1 + λu

j=1 |yij − uT

2 + λv

(cid:80)n

i=1

2 (cid:107)V(cid:107)2
2.

2 (cid:107)U(cid:107)2

Robust non-negative matrix factorization (RNMF) As the traditional NMF is optimized under
the Gaussian noise or Poisson noise assumption, RNMF (Zhang et al., 2011) is introduced to deal with
data that are grossly corrupted. RNMF decomposes the non-negative data matrix as the summation
of one sparse error matrix and the product of two non-negative matrices. The formulation states
minW,H,S (cid:107)Y − WH − S(cid:107)2
s.t. W ≥ 0, H ≥ 0, where (cid:107) · (cid:107)F is the Frobenius norm,
(cid:107)S(cid:107)(cid:96)1 = (cid:80)d
j=1 |Sij|, and λ > 0 is the regularization parameter, controlling the sparsity of S.

F + λ(cid:107)S(cid:107)(cid:96)1

(cid:80)n

i=1

3 Proposed GRMF formulation

In this section, we introduce our Robust Matrix Factorization with Grouping effect (GRMF) by
incorporating the grouping effect into MF. The problem of GRMF is formulated as follows

min
U∈Rd×r,V∈Rn×r

f (U, V) = (cid:107)Y − UVT (cid:107)(cid:96)1 + R(U) + R(V) ,

(3.1)

where R(U) and R(V) are two regularizers corresponding to U and V, given by

R(U) =

R(V) =

d
(cid:88)

i=1
n
(cid:88)

j=1

λ1P1(ui) +

λ1P1(vj) +

d
(cid:88)

i=1
n
(cid:88)

j=1

λ2P2(ui) +

λ2P2(vj) +

d
(cid:88)

i=1
n
(cid:88)

j=1

λ3P3(ui), and

λ3P3(vj) .

3

Here P1(·), P2(·) and P3(·) are different penalty functions, which take the following form with
respect to a vector x ∈ Rr,

P1(x) =

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

, P2(x) =

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

, 1

, P3(x) =

r
(cid:88)

l=1

x2
l , (3.2)

where P1(·) and P2(·) are two regularization terms controlling the sparsity (feature selection) and
the grouping effect (feature grouping), τ1 and τ2 are thresholding parameters asserting when a small
weight or a small difference between two weights will be penalized, λ1 and λ2 are the corresponding
tuning parameters. Here P3(·) is an inherited penalty term from MF with its tuning parameter λ3.
The truncated (cid:96)1-norm penalty P1(·) can be viewed as a surrogate of the (cid:96)0-norm penalty for feature
selection (Shen et al., 2012), where min(|xl|/τ, 1) is an approximation of I(xl (cid:54)= 0) when τ → 0. In
addition, GRMF can be extended to non-negative MF (see details in Appendix B). For other notations,
denote E = {(l, l(cid:48)) : l (cid:54)= l(cid:48), l, l(cid:48) = 1, · · · , r} a set of edges between two distinct nodes l (cid:54)= l(cid:48) of an
undirected complete graph.

In the proposed GRMF, we adopt the (cid:96)1-loss to attain the robustness and introduce a naturally
adjustable non-convex regularization to achieve simultaneous sparsity and grouping effect. Due to
the non-convex regularization and the low-rank constraint in MF, the GRMF is a non-convex problem.
By ﬁxing U or V and updating the other one, GRMF becomes solvable.

4 Algorithms for GRMF

The GRMF problem includes the optimization of two matrices U and V, which are treated as two
independent variables in the alternative optimization process, while the other is ﬁxed.

Note that

(cid:107)Y − UVT (cid:107)(cid:96)1 =

n
(cid:88)

j=1

(cid:107)y·j − Uvj(cid:107)(cid:96)1 =

d
(cid:88)

i=1

(cid:107)yi· − Vui(cid:107)(cid:96)1 .

During the alternative optimization procedure, U and V should be updated alternatively according to
L(U|V) and L(V|U), which are given by

min
U∈Rd×r

L(U|V) =

min
V∈Rn×r

L(V|U) =

d
(cid:88)

i=1
n
(cid:88)

j=1

(cid:107)yi· − Vui(cid:107)(cid:96)1 +

d
(cid:88)

λ1P1(ui) +

d
(cid:88)

λ2P2(ui) +

d
(cid:88)

λ3P3(ui),

(4.1)

i=1
n
(cid:88)

(cid:107)y·j − Uvj(cid:107)(cid:96)1 +

i=1
n
(cid:88)

λ1P1(vj) +

i=1
n
(cid:88)

λ2P2(vj) +

λ3P3(vj). (4.2)

j=1

j=1

j=1

Note that the optimization problem of L(U|V) can be decomposed into d independent subproblems.
The same procedure applies to the minimization of L(V|U). Thus, the problem of GRMF is a
combination of n + d optimization subproblems, each with respect to a vector in Rr. For each
subproblem, we use the difference-of-convex algorithm (DC) to approximate a non-convex cost
function. At each iteration, a quadratic problem with equality constraints is solved by the Alternating
Direction Method of Multipliers (ADMM).

The algorithm for GRMF consists of three steps. First, we apply an alternative minimization
framework to decompose the problem of GRMF into d+n independent subproblems, each optimizing
vj (a column in VT ) or ui (a column in UT ). Second, the non-convex regularization function in
each subproblem is decomposed into a difference of two convex functions, and, through linearizing

4

the trailing convex function, a sequence of approximations is constructed by its afﬁne minorization.
Third, the constructed quadratic problem with equality constraints is solved by ADMM.

Algorithm 1: The alternating minimization algorithm for GRMF
Input: Initialization U(0), V(0); tuning parameters λ1, λ2, λ3, τ1, τ2; small tolerance δU, δV,

t = 1.
Result: Optimal U, V.
while (cid:107)U(t) − U(t−1)(cid:107)2
for j = 1, · · · , n do
Update v(t)

F < δU or (cid:107)V(t) − V(t−1)(cid:107)2

F < δV do

j = arg minvj ∈Rr L(vj|U(t−1)) by Algorithm 2;

end
Then update [V(t)]T = [v(t)
for i = 1, · · · , d do
Update u(t)

end
Then update [U(t)]T = [u(t)
t ← t + 1;

1 , · · · , v(t)
n ];

1 , · · · , u(t)
d ];

i = arg minui∈Rr L(ui|V(t)) by Algorithm 2;

end

4.1 Alternative minimization framework for GRMF

To solve the GRMF problem, we alternatively update VT = [v1, · · · , vn] ∈ Rr×n (while ﬁxing U)
and UT = [u1, · · · , ud] ∈ Rr×d (while ﬁxing V). Thus the alternative minimization framework at
the t-th step consists of updating U(t) = arg minU L(U|V(t)) and V(t) = arg minV L(V|U(t−1))
iteratively until convergence. Problem (4.1) is therefore decomposed into d independent small
problems, each one optimizing ui, ui = arg minx∈Rr L(x|V(t)),
(cid:40)

ui = arg min

x∈Rr

(cid:107)yi· − V(t)x(cid:107)(cid:96)1 + λ1

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

+ λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈EVj

min

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

, 1

+ λ3

(cid:41)

x2
l

,

r
(cid:88)

l=1

(4.3)

where λ1 is a parameter controlling the sparsity and λ2 controls the grouping effect. Problem (4.2)
can be decomposed into n independent small subproblems, each optimizing with respect to vj, with
similar structures. We propose an alternating minimization framework to solve the GRMF problem
(Algorithm 1), by updating U and V alternatively until convergence.

Theorem 1 (Global Convergence of GRMF). Let Y be a matrix in Rd×n. Let (cid:8)(U(t), V(t))(cid:9)
t∈N be
a bounded sequence generated by Algorithm 1 to solve GRMF, with λ1, λ2, λ3, τ1, τ2 appropriately
selected. Then the sequence has ﬁnite length and converges to a critical point.

As for convergence analysis, Theorem 1 gives a convergence guarantee of the alternative minimization
framework to solve the GRMF problem. The main theorem of Csiszár and Tusnády (1984) (Theorem
3) proves a general convergence behavior of alternating minimization, and Hardt (2014) (Theorem
3.8) proves the convergence of a speciﬁc application of alternating minimization algorithm in MF. As
a special case of alternating minimization applied in MF, the convergence behavior of GRMF is thus
summarized in Theorem 1. The proof just mimics the proof of Theorem 3.8 in Hardt (2014), and we
omit the details, as Algorithm 1 is a special case of alternating minimization algorithm applied in MF.

Limitation of the model One limitation of GRMF is that it uses non-convex regularizers which
are neither smooth nor differentiable. Na et al. (2019) has pointed out that, when using non-convex
penalties, iterative methods such as gradient or coordinate descent may terminate undesirably at a
local optimum, which can be different from the global optimum we pursue. As is pointed out in Wen
et al. (2018), the performance of non-convex optimization problems is usually closely related to the
initialization. These are the inherent drawbacks of non-convex optimization problems.

5

4.2 General formulation for each GRMF subproblem

The minimization problem (4.3) can be viewed as a special form of the following constrained
regression-type optimization problem with simultaneous supervised grouping and feature selection,

min
x∈Rr

(cid:107)Ax−b(cid:107)(cid:96)1 +λ1

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

+λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

, 1

+λ3

r
(cid:88)

l=1

x2
l . (4.4)

The notation b ∈ Rn in (4.4) corresponds to yi· ∈ Rn in (4.3), and A ∈ Rn×r corresponds to
V(t) ∈ Rn×r. Thus (cid:107)Ax − b(cid:107)(cid:96)1 = (cid:80)n
i x|, where ai ∈ Rr. Here λ1, λ2 are positive
tuning parameters controlling feature sparsity and grouping effect, λ3 > 0 is to prevent overﬁtting,
τ1 > 0 is a thresholding parameter determining when a large regression coefﬁcient will be penalized
in the model, and τ2 > 0 determines when a large difference between two coefﬁcients will be
penalized. E ⊂ {1, · · · , r}2 denotes a set of edges between distinct nodes l (cid:54)= l(cid:48) for a complete
undirected graph, with l ∼ l(cid:48) representing an edge directly connecting two nodes. Note that the edge
information on the complete undirected graph E is unknown, and need be learned from the model.

i=1 |bi − aT

The DC algorithm To solve problem (4.4), considering the non-smooth property of the (cid:96)1 loss,
we use a smooth approximation |bi − aT
i x)2 + (cid:15))1/2, where (cid:15) is chosen to be a very
small positive value, e.g. 10−6. For each alternative minimization, we need to solve the following
optimization problem, Then optimization problem (4.4) becomes minimizing

i x| ≈ ((bi − aT

S(x) =

n
(cid:88)

i=1

((bi−aT

i x)2+(cid:15))1/2+λ1

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

+λ2

r
(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |xl − xl(cid:48)|
τ2

, 1

(cid:19)

+λ3(cid:107)x(cid:107)2
2 .

By applying min(a, b) = a − (a − b)+, the two regularizers can be rewritten and S(x) can be
decomposed into a difference of two convex functions, S1(x) − S2(x), which are deﬁned as follows
respectively,

S1(x) =

n
(cid:88)

i=1

((bi − aT

i x)2 + (cid:15))1/2 + λ1

r
(cid:88)

l=1

|xl|
τ1

+ λ2

(cid:88)

|xl − xl(cid:48)|
τ2

+ λ3

r
(cid:88)

l=1

x2
l

S2(x) = λ1

r
(cid:88)

l=1

(cid:18) |xl|
τ1

(cid:19)

− 1

+ λ2

(cid:88)

+

l<l(cid:48):(l,l(cid:48))∈E

(cid:18) |xl − xl(cid:48)|
τ2

− 1

.

+

l<l(cid:48):(l,l(cid:48))∈E
(cid:19)

At the m-th iteration, S2(x) should be linearized as a linear approximation of S2(x) at (m-1)-th
iteration. Thus, for the m-th minimization, we need to solve the following subproblem (see Appendix
A.1 for details)

S(m)(x) =

where

n
(cid:88)

((bi − aT

i x)2 + (cid:15))1/2 +

i=1

λ1
τ1

(cid:88)

l∈F (m−1)

|xl| +

λ2
τ2

(cid:88)

|xl − xl(cid:48)| + λ3(cid:107)x(cid:107)2
2,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

F (m−1) =

(cid:110)

l : |ˆx(m−1)
l

| < τ1

(cid:111)

,

E (m−1) =

(l, l(cid:48)) ∈ E, |ˆx(m−1)

l

(cid:110)

− ˆx(m−1)
l(cid:48)

| < τ2

(cid:111)

.

(4.5)

(4.6)

Here ˆx(m) = arg minx S(m)(x), and ˆx(m−1) is the result at the (m-1)-th iteration. Denote xll(cid:48) =
xl − xl(cid:48), and deﬁne ξ = (x1, · · · , xr, x12, · · · , x1r, · · · , x(r−1)r). The m-th subproblem (4.5) can
be reformulated as an equality-constrained convex optimization problem,

S(m)(ξ) =

n
(cid:88)

i=1

((bi−aT

i x)2+(cid:15))1/2+

λ1
τ1

(cid:88)

l∈F (m−1)

|xl|+

λ2
τ2

(cid:88)

|xll(cid:48)|+λ3(cid:107)x(cid:107)2

2 , (4.7)

l<l(cid:48):(l,l(cid:48))∈E (m−1)

subject to xll(cid:48) = xl − xl(cid:48),

∀l < l(cid:48) : (l, l(cid:48)) ∈ E (m−1).

6

Alternating direction method of multipliers
In this step, at each iteration, a quadratic problem
with linear equality constraints is solved by the augmented Lagrange method. The augmented
Lagrangian form of (4.7) is as follows (see details in Appendix A.2)

L(m)
ν

(ξ, τ ) = S(m)(ξ) +

(cid:88)

τll(cid:48)(xl − xl(cid:48) − xll(cid:48)) +

l<l(cid:48):(l,l(cid:48))∈E (m−1)

ν
2

(cid:88)

(xl − xl(cid:48) − xll(cid:48))2 .

l<l(cid:48):(l,l(cid:48))∈E (m−1)

Here τll(cid:48) and ν are the Lagrangian multipliers for the linear constraints and for the computational
acceleration. Update τll(cid:48) and ν, with a constant value α > 1 to accelerate the convergence,

τ k+1
ll(cid:48) = τ k

ll(cid:48) + νk(ˆx(m,k)

l

− ˆx(m,k)
l(cid:48)

− ˆx(m,k)
ll(cid:48)

),

νk+1 = ανk .

(4.8)

(m,k)

To compute (cid:98)ξ
xll(cid:48), while ﬁxing τ k
the details in Appendix A.3).

= arg minξ L(m)
(ξ, τ k), we use ADMM and alternatively update between x and
ll(cid:48) and νk (for k = 1, 2, · · · ). The following two items give the ﬁnal results (check

ν

• Given ˆx(m,k−1)
l

, update ˆx(m,k)

l

by

ˆx(m,k)
l

= α−1γ

(l = 1, · · · , r) ,

(4.9)

where

Let

γ(cid:63) =

n
(cid:88)

i=1

α =

n
(cid:88)

i=1

D− 1

i A2

2

il + 2λ3 + νk (cid:12)
(cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)
(cid:12)
(cid:12) .

D− 1

2

i

cil −

(cid:88)

ll(cid:48) + νk (cid:88)
τ k

(ˆx(m,k−1)

l(cid:48)

+ ˆx(m,k−1)
ll(cid:48)

) .

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l

| ≥ τ1; otherwise γ = ST(γ∗, λ1
τ1

Then γ = γ∗ if |ˆx(m−1)
threshold function ST(b, a) = sign(b)(|b| − a)+. Here Di = (bi − aT
cil = Ail(bi − aT
and (cid:98)x(m,k−1)
positive constant 10−6.

). ST(·, ·) is the soft
i (cid:98)x(m,k−1))2 + (cid:15), and
). ai,−l is the i-th row vector of A without the l-th column,
is the same as (cid:98)x(m,k−1) without the l-th element. (cid:15) is often set to be a small

i,−l(cid:98)x(m,k−1)

−l

−l

• Given ˆx(m,k−1)

ll(cid:48)

, update ˆx(m,k)

ll(cid:48)

(with ˆx(m,k)
l

already updated and ﬁxed).

(cid:16)

ll(cid:48) + νk(ˆx(m,k)
τ k

l

− ˆx(m,k)
l(cid:48)

), λ2
τ2

(cid:17)

,

(cid:40) 1

νk ST
ˆx(m−1)
ll(cid:48)

,

if (l, l(cid:48)) ∈ E (m−1)
if (l, l(cid:48)) (cid:54)∈ E (m−1)

.

(4.10)

ˆx(m,k)
ll(cid:48)

=

5 Experiments

Now that we have presented our GRMF model in detail, we turn to its experimental validation by
conducting experiments using real data sets and evaluate the performance of GRMF. The biggest
concern of our algorithm is to achieve the grouping effect and sparsity simultaneously, while keeping
the reconstruction error at a low level, and to demonstrate the robustness to outliers and corruption
noise. We compare GRMF with several state-of-the-art matrix factorization methods, which include
Truncated SVD (Hansen, 1987), RPCA (Candès et al., 2011), RNMF (Wen et al., 2018), RPMF (Wang
et al., 2012) and GoDec+ (Guo et al., 2017). GRMF is our main method with (cid:96)1-loss, GMF-L2
is its variant with (cid:96)2-loss. We ﬁrst study the behaviors of the 7 algorithms against different ranks
and corruption ratios. We then apply all the algorithms to the face recovery problem. The desktop
computer used to run our experiments has 2.7 GHz 64-bit Intel Core i5 processor (with two cores)
and 8GB RAM.

7

, ∀l < l(cid:48)(l, l(cid:48)) ∈ E (0), m=1;

Algorithm 2: The DC-ADMM algorithm for ui updating
Input: Initialization x(0), tuning parameters λ1, λ2, λ3, τ1, τ2.
. The resulting x is assigned to u(t)
Result: u(t)
.
i
i
l − ˆx(0)
ll(cid:48) = ˆx(0)
Assign F (0), E (0) given (cid:98)x(0). Assign ˆx(0)
= ˆx(m−1)
ˆx(m,0) = ˆx(m−1), ˆx(m,0)
∀ l < l(cid:48) and (l, l(cid:48)) ∈ E (0);
ll(cid:48)
while (cid:107)ˆx(m)
(cid:107)2
2 > (cid:15)outer do

l − ˆx(m−1)

ll(cid:48)

l(cid:48)

l(cid:48)

l − ˆx(m)

l
m ← m + 1;
Update F (m), E (m) by (4.6) given (cid:98)x(m);
ˆx(m)
ll(cid:48) = ˆx(m)
while (cid:107)ˆx(m,k)
l
k ← k + 1;
Update τ k
Update ˆx(m,k)
Update ˆx(m,k)

ll(cid:48) and νk by (4.8);
by (4.9);
by (4.10);

− ˆx(m,k−1)
l

l

ll(cid:48)

, ∀l < l(cid:48) and (l, l(cid:48)) ∈ E (m);
(cid:107)2
2 > (cid:15)inner do

end

end

5.1 Performance under different corruption ratios and reduced ranks

The reduced rank and the corruption ratio can have a strong impact on the performance and the
robustness of matrix factorization methods. Thus the performance caused by these two factors should
be thoroughly investigated in comparison of different matrix factorization methods. In this section,
we show the behaviors of the 7 algorithms against different ranks and corruption ratios with a pilot
experiment of one randomly chosen image from the Yale B dataset.

Robustness vs. corruption ratio
In order to explore the robustness of the algorithms with the
corruption ratio, we track the trajectory of the reconstruction errors with varying corruption ratios,
while ﬁxing the reduced rank. The reduced rank of 4 is used to demonstrate the performance of
different MF methods, as too large a value might cause overﬁtting. The relative mean absolute error
(Relative MAE) is used as a measure of reconstruction error, which is calculated by comparing the
original image with the approximated image using the formula (cid:107)Y−UVT (cid:107)(cid:96)1
. Since the Relative MAE
uses the (cid:96)1-norm loss, it can be viewed as the criterion to measure the robustness of different MF
methods. The type of corruption is salt and pepper noise, which is commonly used in computer vision
applications. We add it to the original image by randomly masking part of the pixels in the image
with the value of 0 or 255 under given corruption ratio. For example, a 50% corruption ratio means
half of the pixels in a image is replaced by either 0 or 255 with equal probability.

(cid:107)Y(cid:107)(cid:96)1

Figure 1a shows that both GRMF and RPMF can maintain the robustness with the lowest recon-
struction errors, even when the corruption ratio gets very large. Their errors increase slowly with
the increase of the corruption ratio, and even when half of the pixels are corrupted with a corruption
ratio of 50% their errors will only increase slightly. In particular, when the corruption ratio between
the corrupted image and the original image is above 50%, GRMF and RPMF can still reduce the
reconstruction error to around 0.2. It is also worth mentioning that RPCA can always recover the raw
input images perfectly, but it does not have the ability to deal with the corrupted images with noise.
This behavior results from the formulation of RPCA that it is not modeling the latent factors, and that
it does not take rank as a parameter, which also explains that RPCA keeps almost all the information
of the input image and its error increases linearly with the corruption ratio.

Robustness vs. reduced rank In order to explore the robustness of the algorithms with the reduced
rank, we track the trajectory of the reconstruction errors with varying reduced ranks, while ﬁxing
the corruption ratio. The corruption ratio of 50% salt and pepper noise is used to demonstrate the
performance of different MF methods, as most of the other MF methods fail to reconstruct the image
except for GRMF and RPMF.

8

(a) reconstruction error vs. corruption ratio

(b) reconstruction error vs. reduced rank

(c) Number of groups vs. reduced rank

(d) Sparsity vs. reduced rank

Figure 1: Performance under different corruption ratios and reduced ranks.

Table 1: Datasets information. We randomly choose some images of the same person/item, as the
images for the same objective are very similar except in different angles/expressions/light conditions.
The optimal rank for each dataset is reported on the fourth row. For the corrupted images, the rank is
chosen to be one less than the optimal rank to avoid ﬁtting the noise.

Datasets
Number of images
Size of images
Optimal rank

Yale B
210
192×168
5

COIL20
206
128×128
5

ORL
200
112×92
4

JAFFE
213
180×150
5

We run GRMF and RPMF with different ranks, since they are the only two algorithms that could
well recover the original image under 50% corruption ratio. The results from Figure 1b show that the
reconstruction error remains at a low level when the reduced ranks are relatively small, and it ﬁrst
decreases then starts to increase as the reduced rank gets larger and larger, which shows a potential of
overﬁtting. Speciﬁcally, the reconstruction error of RPMF starts to increase when the reduced rank is
bigger than 5, while the error of GRMF starts to increase when the reduced rank is bigger than 7,
which shows that RPMF starts to overﬁt much earlier than GRMF. Obviously, the grouping effect
of GRMF helps denoising the factorization and makes it less sensitive to the choice of the reduced
rank. However, both GRMF and RPMF learn the noise and overﬁt the image as the rank gets too
large. In addition, GRMF and RPMF have similar performance regarding the error, but GRMF has
stronger grouping effect and sparsity, as illustrated in Figure 1c and Figure 1d. GRMF tends to have
less number of groups and higher sparsity than RPMF.

9

051015202530354045505560Corruption Ratio (%)0.00.20.40.60.81.01.2Relative MAEGRMFRPMFGMF-L2GoDec+Truncated SVDRPCArNMF2468101214Rank0.120.140.160.180.200.220.240.260.28Relative MAEGRMFRPMF2468101214Rank1.752.002.252.502.753.003.253.50Number of GroupsGRMFRPMF2468101214Rank0123456Sparsity (%)GRMFRPMFGMF-L25.2

Image recovery

Now we consider the application to image recovery in computer vision. We apply all the 7 algorithms
on 4 datasets, Extended Yale B (Georghiades et al., 2001), COIL20 (Nene et al., 1996), ORL (Samaria
et al., 1994), and JAFFE (Lyons et al., 1998). To avoid overﬁtting, we try several ranks and ﬁnd an
optimal rank in each situation. Table 1 reports the basic information about the datasets. We conduct
the experiment on the original image and its 50% corrupted version. The results on Table 2 show that
our GRMF algorithm has a better recovery ability than the other algorithms, especially under severe
corruption. After running experiment over all the images in each dataset, we report the mean and
the standard deviation of the relative MAE of the seven algorithms. All the algorithms perform well
when there is no corruption, but only GRMF and PRMF remain at a low level with respect to the
reconstruction error under 50% corruption. GRMF has the lowest reconstruction error under all cases
while exhibiting a low standard deviation. On average, each image costs less than 300s. And the
parallelizable nature of our algorithm allows a multiprocessing or multithreding settings to accelerate
the computation. More results are reported on Appendix C.

Table 2: Comparison of the reconstruction errors on the 4 datasets. The average of the relative mean
absolute error (RMAE) is reported with the standard deviation in the parentheses. For the corrupted
images, we randomly masked 50% of the pixels in each image with salt and pepper noise. Note that
we report RPCA only on the corrupted case. T-SVD stands for Truncated SVD.

Methods

Yale B

COIL

ORL

JAFFE

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

GRMF 0.093±(0.022) 0.143±(0.041) 0.123±(0.066) 0.245±(0.135) 0.105±(0.020) 0.204±(0.036) 0.121±(0.013) 0.165±(0.018)
PRMF 0.095±(0.023) 0.154±(0.047) 0.127±(0.070) 0.273±(0.142) 0.107±(0.020) 0.210±(0.037) 0.125±(0.014) 0.182±(0.025)
GMF-L2 0.103±(0.026) 0.555±(0.367) 0.143±(0.075) 0.821±(0.467) 0.114±(0.021) 0.320±(0.052) 0.133±(0.013) 0.398±(0.055)
GoDec+ 0.101±(0.025) 0.565±(0.370) 0.139±(0.074) 0.830±(0.470) 0.113±(0.022) 0.325±(0.053) 0.132±(0.013) 0.412±(0.058)
T-SVD 0.103±(0.026) 0.560±(0.368) 0.143±(0.075) 0.824±(0.469) 0.114±(0.021) 0.324±(0.052) 0.133±(0.013) 0.401±(0.056)
0.608±(0.078)
RPCA
RNMF 0.149±(0.053) 0.626±(0.393) 0.285±(0.280) 0.722±(0.214) 0.130±(0.025) 0.360±(0.055) 0.207±(0.022) 0.456±(0.064)

0.835±(0.380)

0.963±(0.446)

0.577±(0.073)

-

-

-

-

6 Conclusion

In this work, we studied the problem of matrix factorization incorporating sparsity and grouping effect.
We proposed a novel method, namely Robust Matrix Factorization with Grouping effect (GRMF),
which intends to lower the reconstruction errors while promoting intepretability of the solution through
automatically determining the number of latent groups in the solution. To the best of our knowledge, it
is the ﬁrst paper to introduce the automatic learning of grouping effect without prior information into
MF. Speciﬁcally, GRMF incorporates two novel non-convex regularizers that control both sparsity
and grouping effect in the objective function, and a novel optimization framework is proposed to
obtain the solution. Moreover, GRMF employs an alternative minimization procedure to decompose
the problem into a number of coupled non-convex subproblems, where each subproblem optimizes
a row or a column in the solution of MF through difference-of-convex (DC) programming and the
alternating direction method of multipliers (ADMM). We have conducted extensive experiments
to evaluate GRMF using (1) Extended Yale B dataset under different rank choices and corruption
ratios and (2) image reconstruction tasks on 4 image datasets (COIL-20, ORL, Extended Yale B, and
JAFFE) under 50% corruption ratio. Compared with 6 baseline algorithms, GRMF has achieved the
best reconstruction accuracy in both tasks, while demonstrating the performance advances from the
use of grouping effects and sparsity, especially under severe data corruption.

References

Maryam Abdolali and Nicolas Gillis. Simplex-structured matrix factorization: Sparsity-based
identiﬁability and provably correct algorithms. SIAM Journal on Mathematics of Data Science, 3
(2):593–623, 2021.

Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?

Journal of the ACM (JACM), 58(3):1–37, 2011.

Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization:

An overview. IEEE Transactions on Signal Processing, 67(20):5239–5269, 2019.

10

Seungjin Choi. Algorithms for orthogonal nonnegative matrix factorization. In 2008 ieee international
joint conference on neural networks (ieee world congress on computational intelligence), pages
1828–1832. IEEE, 2008.

I. Csiszár and G. Tusnády. Information geometry and alternating minimization procedures. Statistics

& Decisions, (1):205–237, 1984.

Zhen Cui, Jin-Xing Liu, Ying-Lian Gao, Chun-Hou Zheng, and Juan Wang. Rcmf: a robust collabo-
rative matrix factorization method to predict mirna-disease associations. BMC bioinformatics, 20
(25):1–10, 2019.

Xiangguang Dai, Xiaojie Su, Wei Zhang, Fangzheng Xue, and Huaqing Li. Robust manhattan
non-negative matrix factorization for image recovery and representation. Information Sciences,
527:70–87, 2020.

Hongbo Gao, Fang Guo, Juping Zhu, Zhen Kan, and Xinyu Zhang. Human motion segmentation

based on structure constraint matrix factorization. Inform. Sci, 2022(65):119103, 2022.

Renaud Gaujoux and Cathal Seoighe. A ﬂexible r package for nonnegative matrix factorization. BMC

bioinformatics, 11(1):1–9, 2010.

A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone models
for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intelligence,
23(6):643–660, 2001.

Kailing Guo, Liu Liu, Xiangmin Xu, Dong Xu, and Dacheng Tao. Godec+: Fast and robust low-rank
matrix decomposition based on maximum correntropy. IEEE transactions on neural networks and
learning systems, 29(6):2323–2336, 2017.

Davide Feltoni Gurini, Fabio Gasparetti, Alessandro Micarelli, and Giuseppe Sansonetti. Temporal
people-to-people recommendation on social networks with sentiment-based matrix factorization.
Future Generation Computer Systems, 78:430–439, 2018.

Benjamin Haeffele, Eric Young, and Rene Vidal. Structured low-rank matrix factorization: Optimality,
algorithm, and applications to image processing. In International conference on machine learning,
pages 2007–2015. PMLR, 2014.

Benjamin D Haeffele and René Vidal. Structured low-rank matrix factorization: Global optimality,
algorithms, and applications. IEEE transactions on pattern analysis and machine intelligence, 42
(6):1468–1482, 2019.

Per Christian Hansen. The truncatedsvd as a method for regularization. BIT Numerical Mathematics,

27(4):534–553, 1987.

Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th

Annual Symposium on Foundations of Computer Science, pages 651–660. IEEE, 2014.

Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine

learning research, 5(9), 2004.

Martin Jakomin, Zoran Bosni´c, and Tomaž Curk. Simultaneous incremental matrix factorization for

streaming recommender systems. Expert Systems with Applications, 160:113685, 2020.

Ali Akbar Jamali, Anthony Kusalik, and Fang-Xiang Wu. Mdipa: a microrna–drug interaction
prediction approach based on non-negative matrix factorization. Bioinformatics, 36(20):5061–
5067, 2020.

Jingu Kim, Renato DC Monteiro, and Haesun Park. Group sparsity in nonnegative matrix factorization.
In Proceedings of the 2012 SIAM International Conference on Data Mining, pages 851–862. SIAM,
2012.

Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some

applications. IEEE Transactions on automatic control, 25(2):164–176, 1980.

11

Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender

systems. Computer, 42(8):30–37, 2009.

Fangfang Li, Guandong Xu, Longbing Cao, Xiaozhong Fan, and Zhendong Niu. Cgmf: Coupled
group-based matrix factorization for recommender system. In International Conference on Web
Information Systems Engineering, pages 189–198. Springer, 2013.

Ruyue Li, Lefei Zhang, and Bo Du. A robust dimensionality reduction and matrix factorization

framework for data clustering. Pattern Recognition Letters, 128:440–446, 2019.

Zhouchen Lin, Chen Xu, and Hongbin Zha. Robust matrix factorization by majorization minimization.

IEEE transactions on pattern analysis and machine intelligence, 40(1):208–220, 2017.

Michael Lyons, Miyuki Kamachi, and Jiro Gyoba. The Japanese Female Facial Expression (JAFFE)
Dataset, April 1998. URL https://doi.org/10.5281/zenodo.3451524. The images are
provided at no cost for non- commercial scientiﬁc research only. If you agree to the conditions
listed below, you may request access to download.

Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using
probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and
knowledge management, pages 931–940, 2008.

Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in neural

information processing systems, 20:1257–1264, 2007.

Hanwool Na, Myeongmin Kang, Miyoun Jung, and Myungjoo Kang. Nonconvex tgv regularization
model for multiplicative noise removal with spatially varying parameters. Inverse Problems &
Imaging, 13(1):117, 2019.

Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20.

Technical report, 1996.

Fernando Ortega, Antonio Hernando, Jesus Bobadilla, and Jeon Hyung Kang. Recommending items
to group of users using matrix factorization based collaborative ﬁltering. Information Sciences,
345:313–324, 2016.

Hashem Parvin, Parham Moradi, Shahrokh Esmaeili, and Nooruldeen Nasih Qader. A scalable and
robust trust-based nonnegative matrix factorization recommender using the alternating direction
method. Knowledge-Based Systems, 166:92–107, 2019.

Alberto Pascual-Montano, Pedro Carmona-Saez, Monica Chagoyen, Francisco Tirado, Jose M Carazo,
and Roberto D Pascual-Marqui. bionmf: a versatile tool for non-negative matrix factorization in
biology. BMC bioinformatics, 7(1):1–9, 2006.

Alireza Rahimpour, Hairong Qi, David Fugate, and Teja Kuruganti. Non-intrusive energy disaggre-
gation using non-negative matrix factorization with sum-to-k constraint. IEEE Transactions on
Power Systems, 32(6):4430–4441, 2017.

F. S. Samaria, F. S. Samaria *t, A.C. Harter, and Old Addenbrooke’s Site. Parameterisation of a

stochastic model for human face identiﬁcation, 1994.

Xiaotong Shen, Wei Pan, and Yunzhang Zhu. Likelihood-based selection and sharp parameter

estimation. Journal of the American Statistical Association, 107(497):223–232, 2012.

Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In

Advances in neural information processing systems, pages 1329–1336, 2005.

Naiyan Wang and Dit-Yan Yeung. Bayesian robust matrix factorization for image and video process-
ing. In Proceedings of the IEEE International Conference on Computer Vision, pages 1785–1792,
2013.

Naiyan Wang, Tiansheng Yao, Jingdong Wang, and Dit-Yan Yeung. A probabilistic approach to
robust matrix factorization. In European Conference on Computer Vision, pages 126–139. Springer,
2012.

12

Qi Wang, Xiang He, Xu Jiang, and Xuelong Li. Robust bi-stochastic graph regularized matrix
factorization for data clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2020.

Fei Wen, Lei Chu, Peilin Liu, and Robert C Qiu. A survey on nonconvex regularization-based sparse
and low-rank recovery in signal processing, statistics, and machine learning. IEEE Access, 6:
69883–69906, 2018.

Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and

intelligent laboratory systems, 2(1-3):37–52, 1987.

Shuang Xu, Chunxia Zhang, and Jiangshe Zhang. Bayesian deep matrix factorization network for

multiple images denoising. Neural Networks, 123:420–428, 2020.

Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Deep matrix factor-
ization models for recommender systems. In IJCAI, volume 17, pages 3203–3209. Melbourne,
Australia, 2017.

Sen Yang, Lei Yuan, Ying-Cheng Lai, Xiaotong Shen, Peter Wonka, and Jieping Ye. Feature grouping
and selection over an undirected graph. In Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 922–930, 2012.

Quanming Yao and James Kwok. Scalable robust matrix factorization with nonconvex loss. arXiv
preprint arXiv:1710.07205, 31, 2018. URL https://proceedings.neurips.cc/paper/
2018/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf.

Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. Taskrec: probabilistic matrix factorization
In International Conference on Neural

in task recommendation in crowdsourcing systems.
Information Processing, pages 516–525. Springer, 2012.

Lijun Zhang, Zhengguang Chen, Miao Zheng, and Xiaofei He. Robust non-negative matrix factoriza-

tion. Frontiers of Electrical and Electronic Engineering in China, 6(2):192–200, 2011.

Yupei Zhang, Yue Yun, Huan Dai, Jiaqi Cui, and Xuequn Shang. Graphs regularized robust matrix
factorization and its application on student grade prediction. Applied Sciences, 10(5):1755, 2020.

13

Appendices

Illustration of the grouping effect of GRMF










100 110 90 . . . 200 190 210
100 110 90 . . . 200 190 210
...
...
0.01 100 120 . . . 2.5 0.21 90

...

...

...

...


















Grouping
=====⇒

101 99.5 100 . . . 200 199.5 201
100 100.5 99.5 . . . 201 200 199
...
...
0.01 100 105 . . . 0.91 0.01 97.5

...

...

...

...









Figure 2: Illustration of grouping effect

We take the grouping effect in a matrix as an example. As shown in Figure 2, the grouping effect
along each row of the matrix can introduce clustering between similar items, where elements close to
each other are clustered in the same group which is highlighted with the same color. In addition, the
elements clustered in the same group do not need to be adjacent, In the decomposed matrix obtained
from GRMF, this grouping effect along the hidden factors is expected in both U and V.

A Details of the algorithm of GRMF

A Python implementation of GRMF can be found on github2.

A.1 Difference-of-convex algorithm (DC)

Note that the optimization problem of L(U|V) in (4.1) can be decomposed into d independent
subproblems. The same procedure applies to the minimization of L(V|U) in (4.2). Thus, the
problem of GRMF is a combination of n + d optimization subproblems, each with respect to a vector
in Rr. For each alternative minimization, we need to solve the following optimization problem:

(cid:40)

arg min
x

S(x) =

n
(cid:88)

[(bi − aT

i x)2 + (cid:15)]

1

2 + λ1

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

i=1

+ λ2

r
(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

, 1

+ λ3

(cid:41)

x2
l

.

r
(cid:88)

l=1

By applying that min(a, b) = a − (a − b)+, S(x) can be decomposed into a difference of two convex
functions as follows,

S1(x) =

n
(cid:88)

i=1

[(bi − aT

i x)2 + (cid:15)]

1

2 + λ1

S2(x) = λ1

r
(cid:88)

l=1

(cid:18) |xl|
τ1

(cid:19)

− 1

+ λ2

+

l<l(cid:48):(l,l(cid:48))∈E

r
(cid:88)

l=1

|xl|
τ1

r
(cid:88)

r
(cid:88)

+ λ2

l<l(cid:48):(l,l(cid:48))∈E

|xl − xl(cid:48)|
τ2

+ λ3

r
(cid:88)

l=1

x2
l ,

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

− 1

.

+

Approximation of the trailing convex function We then construct a sequence of approximations
of S2(x) iteratively. At the m-th iteration, we approximate S2(x) by its afﬁne minorizaion, by
linearizing the trailing convex function S2(x).

By linearizing the trailing convex function S2(x) in the difference convex decomposition, we replace
S2(x) at the m-th iteration with its afﬁne minorization at (m − 1)-th iteration. We linearize the

2https://github.com/GroupingEffects/GRMF

14

trailing convex function S2(η) = S2(η∗) + (cid:104)η − η∗, ∂S2(η∗)(cid:105) at a neighborhood of η∗ ∈ R(r2+r)/2,
where ∂S2(η) is the ﬁrst derivative of S2(η) with respect to η, and (cid:104)·, ·(cid:105) is the inner product, where
η = (cid:0)|x1|, |x2|, · · · , |xr|, |x12|, · · · , |x1r|, · · · , |x(r−1)r|(cid:1)T

.

Then we get

S(η) = S(η∗) + (cid:104)η − η∗, ∇S(η)|η=η∗ (cid:105).

Thus, at the m-th iteration, we replace S2(x) with the m-th approximation by

S(m)
2

(x) = ˜S(m)

2

(η) = ˜S2((cid:98)η(m−1)) + (cid:104)η − (cid:98)η(m−1), ∂ ˜S2((cid:98)η(m−1))(cid:105).

Speciﬁcally,

S(m)
2

(x) =S2((cid:98)x(m−1)) + (cid:104)x − (cid:98)x(m−1), ∂S2(x)|x=(cid:98)x(m−1)(cid:105)
r
(cid:88)

(cid:16)

=S2((cid:98)x(m−1)) +

I(cid:110)

|ˆx(m−1)

l

|≥τ1

(cid:111) ·

|xl| − |ˆx(m−1)

l

λ1
τ1

l=1

(cid:17)
|

(A.1)

+

λ2
τ2

(cid:88)

I(cid:110)

l<l(cid:48):(l,l(cid:48))∈E

|ˆx(m−1)

l

−ˆx(m−1)
l(cid:48)

|≥τ2

(cid:16)

(cid:111) ·

|xl − xl(cid:48)| − |ˆx(m−1)

l

− ˆx(m−1)
l(cid:48)

(cid:17)
|

.

Finally, a sequence of approximations of S(x) is constructed iteratively. For the m-th approximation,
an upper convex approximating function to S(x) can be obtained by S(m)(x) = S1(x) − S(m)
(x),
which formulates the following subproblem.

2

[(bi − aT

i x)2 + (cid:15)]

1
2 +

λ1
τ1

(cid:88)

l∈F (m−1)

|xl| +

λ2
τ2

(cid:88)

|xl − xl(cid:48)| + λ3

l<l(cid:48):(l,l(cid:48))∈E (m−1)

r
(cid:88)

l=1

x2
l . (A.2)

min
x

n
(cid:88)

i=1

where

F (m−1) = {l : |ˆx(m−1)
| < τ1},
E (m−1) = {(l, l(cid:48)) ∈ E, |ˆx(m−1)

l

l

− ˆx(m−1)
l(cid:48)

| < τ2} .

(A.3)

Denote xll(cid:48) = xl − xl(cid:48) and deﬁne ξ = (x1, · · · , xr, x12, · · · , x1r, · · · , x(r−1)r). The m-th sub-
problem Eq.(A.2) can be reformulated as an equality-constrained convex optimization problem,

min
ξ

n
(cid:88)

[(bi − aT

i x)2 + (cid:15)]

i=1

1
2 +

λ1
τ1

(cid:88)

l∈F (m−1)

|xl| +

λ2
τ2

(cid:88)

|xll(cid:48)| + λ3

l<l(cid:48):(l,l(cid:48))∈E (m−1)

r
(cid:88)

l=1

x2
l ,

(A.4)

subject to

xll(cid:48) = xl − xl(cid:48), ∀l < l(cid:48) : (l, l(cid:48)) ∈ E (m−1).
For the equality-constrained problem Eq.(A.4), we employ the augmented Lagrange method to solve
its equivalent unconstrained version iteratively with respect to k for the m-th approximation.

A.2 Alternating direction method of multipliers (ADMM)

To apply ADMM in our constrained optimization problem Eq.(A.4), we separate Eq.(A.4) in two
parts:

n
(cid:88)

f (x) =

[(bi − aT

i x)2 + (cid:15)]

1
2 +

g(xll(cid:48)) =

i=1
λ2
τ2

(cid:88)

|xll(cid:48)|,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

λ1
τ1

(cid:88)

|xl| + λ3

l∈F (m−1)

r
(cid:88)

l=1

x2
l ,

(A.5)

subject to

xl − xl(cid:48) = xll(cid:48),

∀l < l(cid:48), (l, l(cid:48)) ∈ E (m−1).

15

With deﬁnition ξ = (x, xll(cid:48)), the augmented Lagrangian for Eq.(A.5) is

L(m)
ν

(ξ, τ ) = L(m)

ν

(x, xll(cid:48), τ ) =f (x) + g(xll(cid:48)) +

(cid:88)

τll(cid:48)(xl − xl(cid:48) − xll(cid:48))

+

ν
2

l<l(cid:48):(l,l(cid:48))∈E (m−1)

(cid:88)

(xl − xl(cid:48) − xll(cid:48))2,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

(A.6)

where τll(cid:48) and ν are the Lagrangian multipliers for the linear constraints and for the computational
acceleration.

) for given values of
ll(cid:48), νk). Note that as the iteration proceeds, xl − xl(cid:48) and xll(cid:48) becomes closer and closer, so we need

Minimizing Eq.(A.6) over ξ = (x, xll(cid:48)) yields (cid:98)ξ
(τ k
ν to increase through the process. In particular, the ADMM updating rules are as follows

= ((cid:98)x(m,k), (cid:98)x(m,k)

ll(cid:48)

(m,k)

x–updating

xll(cid:48)–updating

τ ll(cid:48)–updating

τ k+1
ll(cid:48) = τ k

ll(cid:48) + νk (cid:16)

ˆx(m,k+1)
l

(cid:98)x(m,k+1)

ll(cid:48)

(cid:98)x(m,k+1) = arg min
x
L(m)
ν

= arg min
xll(cid:48)
− ˆx(m,k+1)
ll(cid:48)

− ˆx(m,k+1)
l(cid:48)

ll(cid:48)

L(m)
ν

, τ k),

(x, (cid:98)x(m,k)
((cid:98)x(m,k+1), xll(cid:48), τ k),
(cid:17)

νk+1 = ρνk.

,

(A.7)

(A.8)

(A.9)

Here ρ is some constant chosen larger than 1, which controls the acceleration of the algorithm. The
details of the (cid:98)ξ

minimization step can be found in the following two subsections.

(m,k)

A.3 The x–updating

Updating ˆx(m,k)

l

in ADMM We ﬁrst write the function of x as H(x).

H(x) ∆= L(m)

, τ k)

ν
n
(cid:88)

ll(cid:48)

(x, (cid:98)x(m,k)
[(bi − aT

i x)2 + (cid:15)]

=

+

i=1
(cid:124)

(cid:123)(cid:122)
(cid:104)1(cid:105)

(cid:88)

1
2

+

(cid:125)

λ1
τ1
(cid:124)

(cid:88)

|xl|

+ λ3

l∈F (m−1)
(cid:123)(cid:122)
(cid:104)2(cid:105)

(cid:125)

(cid:124)

r
(cid:88)

l=1
(cid:123)(cid:122)
(cid:104)3(cid:105)

x2
l

(cid:125)

ll(cid:48)(xl − ˆx(m,k−1)
τ k
l(cid:48)

− ˆx(m,k−1)
ll(cid:48)

)

+

(cid:88)

ll(cid:48)(xl − ˆx(m,k)
τ k

l(cid:48)

− ˆx(m,k−1)
ll(cid:48)

)

l<l(cid:48):(l,l(cid:48))∈E (m−1)
(cid:124)

(cid:123)(cid:122)
(cid:104)4−1(cid:105)

l>l(cid:48):(l,l(cid:48))∈E (m−1)
(cid:124)

(cid:125)

(cid:123)(cid:122)
(cid:104)4−2(cid:105)

(cid:125)

+

νk
2

(cid:124)

(cid:88)

(xl − ˆx(m,k−1)
l(cid:48)

− ˆx(m,k−1)
ll(cid:48)

)2

+

l<l(cid:48):(l,l(cid:48))∈E (m−1)

νk
2

(cid:88)

(xl − ˆx(m,k)

l(cid:48)

− ˆx(m,k−1)
ll(cid:48)

)2

l>l(cid:48):(l,l(cid:48))∈E (m−1)

(cid:123)(cid:122)
(cid:104)5−1(cid:105)

(cid:125)

(cid:124)

(cid:123)(cid:122)
(cid:104)5−2(cid:105)

(cid:125)

We calculate the derivative of H(x) part by part. Set the derivatives of (cid:104)1(cid:105) + (cid:104)2(cid:105) + (cid:104)3(cid:105) + (cid:104)4 − 1(cid:105) +
(cid:104)4 − 2(cid:105) + (cid:104)5 − 1(cid:105) + (cid:104)5 − 2(cid:105) = 0, we get an equation about xl.

The xl terms:

n
(cid:88)

i=1

where

D− 1

i A2

2

ilxl + 2λ3 · xl + νk (cid:88)

1 · xl + νk (cid:88)

1 · xl

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l>l(cid:48):(l,l(cid:48))∈E (m−1)

νk (cid:88)

1 · xl + νk (cid:88)

1 · xl = νk ·

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l>l(cid:48):(l,l(cid:48))∈E (m−1)

(cid:12)
(cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)

(cid:12)
(cid:12) · xl

and (cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)

(cid:12) is the number of the elements in the set.

16

The constant terms:

n
(cid:88)

i=1

−

+

D− 1

2

i

(biAil − (ai,−l(cid:98)x(m,k−1)

−l

) · Ail) +

(cid:88)

ll(cid:48) − νk (cid:88)
τ k

(ˆx(m,k−1)

l(cid:48)

(cid:40) λ1
τ1
− λ1
τ1
0
+ ˆx(m,k−1)
ll(cid:48)

(l,l(cid:48))∈E (m−1)

(l,l(cid:48))∈E (m−1)

Thus the updating of ˆx(m,k)

l

,

ˆx(m,k)
l

= α−1γ,

if |ˆx(m−1)
l
if |ˆx(m−1)
l

| < τ1 and xl > 0
| < τ1 and xl < 0

otherwise

)

where

and

α =

n
(cid:88)

i=1

D− 1

i A2

2

il + 2λ3 + νk (cid:12)
(cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)
(cid:12)
(cid:12) ,

γ =

(cid:40)

γ∗,
ST(γ∗, λ1
τ1

),

if |ˆx(m−1)
l
if |ˆx(m−1)
l

| ≥ τ1
| < τ1

.

ST is the soft threshold function,

ST(x, δ) = sign(x)(|x| − δ)+ =






x − δ,
x + δ,
0,

if b > δ
if b < −δ
if |b| ≤ δ

,

and

γ∗ =

n
(cid:88)

i=1

D− 1

2

i

cil −

(cid:88)

ll(cid:48) + νk (cid:88)
τ k

(ˆx(m,k−1)

l(cid:48)

+ ˆx(m,k−1)
ll(cid:48)

) ,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l<l(cid:48):(l,l(cid:48))∈E (m−1)

where Di = (bi − aT
of A without the l-th column, and (cid:98)x(m,k−1)

i (cid:98)x(m,k−1))2 + (cid:15), cil = Ail · (bi − aT

(−l)

i,−l(cid:98)x(m,k−1)

−l

), ai,−l is the i-th row vector

is the vector (cid:98)x(m,k−1) without the l-th component.

A.4 The xll(cid:48) –updating

Updating ˆx(m,k)
updated and ﬁxed.

ll(cid:48)

in ADMM Given ˆx(m,k−1)

ll(cid:48)

, update ˆx(m,k)

ll(cid:48)

(1 ≤ l < l(cid:48) ≤ r) with ˆx(m,k)

l

already

Q(xll(cid:48)) ∆= L(m)

((cid:98)x(m,k), xll(cid:48), τ k)

(cid:88)

|xll(cid:48)|

+ τ k
ll(cid:48)

(cid:88)

(ˆx(m,k)
l

− ˆx(m,k)
l(cid:48)

− xll(cid:48))

l<l(cid:48):(l,l(cid:48))∈E (m−1)
(cid:123)(cid:122)
(I)

(cid:125)

(cid:124)

l<l(cid:48):(l,l(cid:48))∈E (m−1)

(cid:123)(cid:122)
(II)

(cid:125)

ν
λ2
τ2

(cid:124)

=

+

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E (m−1)
(cid:124)

νk
2

(ˆx(m,k)
l

− ˆx(m,k)
l(cid:48)

− xll(cid:48))2

.

(cid:123)(cid:122)
(III)

(cid:125)

We calculate

∂Q(xll(cid:48))
∂xll(cid:48)

by part. Setting the derivative equals to zero, we have that

0 = νkxll(cid:48) − τ k

ll(cid:48) − νk(ˆx(m,k)

l

− ˆx(m,k)
l(cid:48)

) +

(cid:40) λ2
,
τ2
− λ2
τ2

,

if xll(cid:48) > 0
if xll(cid:48) < 0

.

Then

ˆx(m,k)
ll(cid:48)

=

(cid:40) 1

ll(cid:48) + νk(ˆx(m,k)
νk ST(τ k
ˆx(m−1)
,
ll(cid:48)

l

− ˆx(m,k)
l(cid:48)

), λ2
τ2

),

if (l, l(cid:48)) ∈ E (m−1)
if (l, l(cid:48)) (cid:54)∈ E (m−1)

.

(A.10)

17

B Extension to Non-negative GRMF (N-GRMF)

B.1 Formulation for N-GRMF

In this section we show that our GRMF model can be easy extended to the robust non-negative MF
with grouping effect (N-GRMF). The problem is formulated as follows

min
U∈Rd×r,V∈Rn×r

f (U, V) = (cid:107)Y − UVT (cid:107)(cid:96)1 + R(U) + R(V) ,

(B.1)

where R(U) and R(V) are two regularizers corresponding to U and V, given by

R(U) =

R(V) =

d
(cid:88)

i=1
n
(cid:88)

j=1

λ1P1(ui) +

λ1P1(vj) +

d
(cid:88)

i=1
n
(cid:88)

j=1

λ2P2(ui) +

λ2P2(vj) +

d
(cid:88)

i=1
n
(cid:88)

j=1

λ3 ˜P3(ui), and

λ3 ˜P3(vj) .

Here P1(·) and P2(·) are the same as in GRMF, while ˜P3(·) is slightly different from P3(·), which
takes the following form,

P1(x) =

r
(cid:88)

l=1

min

(cid:18) |xl|
τ1

(cid:19)

, 1

, P2(x) =

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

, 1

, ˜P3(x) =

r
(cid:88)

(min(xl, 0))2

l=1

(B.2)

We still adopt the (cid:96)1-loss to attain the robustness and solve the problem by ﬁxing U or V and updating
the other one.

B.2 Algorithms for N-GRMF

The solution for N-GRMF is very similar to that of GRMF. Thus, we will not walk through the whole
solution here again. Instead, we only detail what differs in this model.

B.2.1 The DC algorithm

Note that in this model ˜P3(x) needs to be decomposed by DC just as P1(x) and P2(x) do. In
particular, S(x) can be decomposed as follows,

S1(x) =

n
(cid:88)

[(bi − aT

i x)2 + (cid:15)]

1

2 + λ1

i=1

S2(x) = λ1

r
(cid:88)

l=1

(cid:18) |xl|
τ1

(cid:19)

− 1

+ λ2

r
(cid:88)

l=1

|xl|
τ1

(cid:88)

+ λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

|xl − xl(cid:48)|
τ2

+ λ3

r
(cid:88)

l=1

x2
l

(cid:18) |xl − xl(cid:48)|
τ2

(cid:19)

− 1

+ λ3

r
(cid:88)

[(xl)+]2 .

+

l=1

+

l<l(cid:48):(l,l(cid:48))∈E

For the m-th minimization, S2(x) is linearized and the subproblem becomes as follows,

n
(cid:88)

[(bi−aT

i x)2+(cid:15)]

i=1

1
2 +

λ1
τ1

(cid:88)

l∈F (m−1)

|xl|+

λ2
τ2

(cid:88)

|xl−xl(cid:48)|+λ3

(cid:88)

x2
l ,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l∈N (m−1)

(B.3)

S(m)(x) =

where

F (m−1) =

(cid:110)

l : |ˆx(m−1)
l

| < τ1

(cid:111)

,

E (m−1) =

(cid:110)

N (m−1) =

(l, l(cid:48)) ∈ E, |ˆx(m−1)
(cid:110)

(cid:111)

l

l : ˆx(m−1)
l

< 0

.

− ˆx(m−1)
l(cid:48)

| < τ2

(cid:111)

,

18

Denote xll(cid:48) = xl − xl(cid:48), and deﬁne ξ = (x1, · · · , xr, x12, · · · , x1r, · · · , x(r−1)r). The m-th subprob-
lem (B.3) can be reformulated as an equality-constrained convex optimization problem,

S(m)(ξ) =

n
(cid:88)

i=1

[(bi − aT

i x)2 + (cid:15)]

1
2 +

λ1
τ1

(cid:88)

l∈F (m−1)

|xl| +

λ2
τ2

(cid:88)

|xll(cid:48)| + λ3

(cid:88)

x2
l ,

l<l(cid:48):(l,l(cid:48))∈E (m−1)

l∈N (m−1)

subject to xll(cid:48) = xl − xl(cid:48),

∀l < l(cid:48) : (l, l(cid:48)) ∈ E (m−1).

(B.4)

B.2.2 ADMM
The only thing different for N-GRMF in this step is the updating rule of ˆx(m,k)
involve other variables but xl. In particular, when updating by ˆx(m,k)
compared to the solution of GRMF and γ stays the same: it is formulated as follows,

l

, since P3 does not
= α−1γ, α is a little different

l

α =

n
(cid:88)

i=1

D− 1

i A2

2

il + 2λ3I{ˆx(m−1)

l

<0} + νk (cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)
(cid:12)
(cid:12) ,

(cid:12)

(B.5)

where I{ˆx(m−1)

l

<0} = 1 when ˆx(m−1)

l

< 0 and I{ˆx(m−1)

l

<0} = 0 otherwise.

C More experiments

C.1 Additions to the main results

Figure 3 is a demonstration of the four datasets we used in the experiments. Each dataset consists
of a large number of people or objects in different angles/expressions/light conditions. In this
demonstration, the ﬁrst row of images is the original version, and the second row of images is the
corrupted version with 50% salt and pepper noise. In the experiment, we use each image as the input
data, and then decompose it into two smaller matrices whose product constitutes the reconstructed
image. We compare the reconstruction error between the original image and the reconstructed one.

Figure 3: The original images and the corrupted versions. the ﬁrst row of images is the original
version, and the second row of images is the corrupted version with 50% salt and pepper noise.

Figure 4 shows one example of image recovery. The ﬁrst column consists of input images with
different corruption ratios. Each row represents the output images recovered by different methods.
Among all methods, GRMF and PRMF demonstrate great recovery ability, especially when the cor-
ruption ratio is as high as 50%, where the output images look very similar to the original undamaged
ones. Unlike these two, RPCA recovers the input image to the greatest extent, but it does not show
robustness when the input image is corrupted with noise.

19

Yale BCOIL20ORLJAFFETable 3: Comparison of running time on the four datasets. The average time cost (seconds) per image
is reported with the standard deviation in the parentheses. T-SVD stands for the Truncated SVD.

Datasets

Yale B

COIL

ORL

JAFFE

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

GRMF 155.7±(10.3) 334.6±(75.4) 188.4±(66.2) 480.7±(409.2) 85.1±(11.7) 159.2±(16.0) 177.4±(10.5) 353.4±(40.9)
3.3±(0.1)
PRMF
GMF-L2 27.2±(6.6)
0.1±(0.0)
GoDec+
0.0±(0.0)
T-SVD
5.4±(0.1)
RPCA
0.5±(0.5)
rNMF

8.8±(20.3)
1.4±(0.1)
2.3±(0.6)
42.0±(43.6) 22.5±(104.0) 21.3±(5.3)
0.0±(0.0)
0.0±(0.0)
0.1±(0.3)
0.0±(0.0)
0.0±(0.0)
0.0±(0.0)
1.6±(0.1)
3.5±(0.4)
6.5±(12.9
1.7±(0.1)
3.0±(0.5)
6.5±(10.9)

3.1±(0.2)
18.1±(1.8)
0.0±(0.0)
0.0±(0.0)
0.6±(0.1)
4.5±(0.1)

3.2±(0.1)
22.5±(4.6)
0.0±(0.0)
0.0±(0.0)
4.7±(0.1)
4.5±(0.2)

3.1±(0.1)
20.6±(0.1)
0.0±(0.0)
0.0±(0.0)
0.6±(0.1)
5.1±(0.2)

1.3±(0.0)
9.0±(1.1)
0.0±(0.0)
0.0±(0.0)
1.8±(0.1)
1.8±(0.0)

As for the time cost, Table 3 demonstrates the average running time of different algorithms on the
four datasets, including both the original version and the 50% corrupted version. Our algorithm is
more time consuming compared to the benchmarks because we apply an algorithm which consists of
multi-layer nested loops. To be speciﬁc, we update each row (column) of U(V) independently, and
each subproblem is solved with two nested loops combining DC and coordinate-wise ADMM. This
structure is necessary in our approach because of the (cid:96)0-surrogate regularization and the grouping
effect we are pursuing. The (cid:96)0-surrogate penalties are non-convex and thus can only be solved
after decomposition into a difference of two convex functions. In addition, to pursue the grouping
effect, we need to compare every pair of values in the output vector. Thus the resulting optimization
problem could be solved by ADMM. In spite of that, the promoted performance of GRMF makes it
worth being studied. The grouping effect of GRMF helps denoising the factorization and makes it
less sensitive to outliers and corruption, which is demonstrated by the great recovery ability in the
experiments of 50% corrupted images. Therefore, to enjoy the robustness and accuracy of GRMF,
one signiﬁcant future development of GRMF is to accelerate the training process.

Figure 4: An example of image recovery with different MF methods under different corruption ratios.

20

OriginGRMFPRMFGMF-L2GoDec+TSVDRPCArNMFNo Corruption10% Corruption20% Corruption30% Corruption40% Corruption50% CorruptionTable 4: Comparison of Relative MAE among N-GRMF, regular GRMF, and PRMF on four datasets
with 50% corruption ratio. The mean RMAE is reported with the standard deviation in the parentheses.
N-GRMF stands for non-negative GRMF.

Datasets

Yale B

COIL

ORL

JAFFE

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

Origin

Corrupted

N-GRMF 0.093±(0.022) 0.143±(0.042) 0.123±(0.066) 0.234±(0.126) 0.105±(0.020) 0.196±(0.035) 0.121±(0.013) 0.164±(0.018)
GRMF 0.093±(0.022) 0.143±(0.041) 0.123±(0.066) 0.245±(0.135) 0.105±(0.020) 0.204±(0.036) 0.121±(0.013) 0.165±(0.018)
PRMF 0.095±(0.023) 0.154±(0.047) 0.127±(0.070) 0.273±(0.142) 0.107±(0.020) 0.107±(0.037) 0.125±(0.014) 0.182±(0.025)

C.2 Non-negative extension results

We extend our GRMF model to a non-negative variant (N-GRMF) and conduct matrix factorization
using the same four datasets in this section. The algorithm of N-GRMF is covered in Appendix
B. Then Table 4 demonstrates a comparison of the relative mean absolute error (RMAE) between
non-negative GRMF, regular GRMF, and PRMF on four datasets with both its original version and the
50% corrupted version. The RMAE of GRMF and N-GRMF remains at the same level when doing
factorization with respect to the origin image. Both GRMF and N-GRMF have a minor improvement
on the corrupted image recovery. Figure 5 illustrates the comparison of the reconstruction error
between N-GRMF, regular GRMF, and all the benchmarks on four datasets with 50% corruption
ratio. The distribution of the reconstruction error for each image is represented by the box plot and
scatter plot, which shows the maximum, upper quantile, mean, standard deviation, lower quantile,
minimum. Besides, every error point is plotted over it. As illustrated in Table 4 and Figure 5,
the error of N-GRMF has a minor decrease compared with that of the regular GRMF without the
non-negative constraint, when the input data is corrupted, implying an enhanced robustness from
GRMF to N-GRMF.

21

(a) RMAE of Yale B

(b) RMAE of COIL

(c) RMAE of ORL

(d) RMAE of JAFFE

Figure 5: Comparison of the distribution of relative mean absolute error(RMAE) among non-negative
GRMF, regular GRMF, and all the benchmarks on four datasets with 50% corruption ratio. N-GRMF
stands for non-negative GRMF, and T-SVD stands for the truncated SVD.

22

N-GRMFGRMFPRMFGMF-L2GoDec+TSVDRPCArNMFmethod0.000.250.500.751.001.251.501.75RMAEN-GRMFGRMFPRMFGMF-L2GoDec+TSVDRPCArNMFmethod0.000.250.500.751.001.251.501.752.00RMAEN-GRMFGRMFPRMFGMF-L2GoDec+TSVDRPCArNMFmethod0.10.20.30.40.50.60.70.8RMAEN-GRMFGRMFPRMFGMF-L2GoDec+TSVDRPCArNMFmethod0.10.20.30.40.50.60.70.8RMAE