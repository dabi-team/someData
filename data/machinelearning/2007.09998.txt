0
2
0
2

l
u
J

5
2

]

G
L
.
s
c
[

3
v
8
9
9
9
0
.
7
0
0
2
:
v
i
X
r
a

Lagrangian Duality in Reinforcement Learning

Pranay Pasula
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
pasula@berkeley.edu

Abstract

Although duality is used extensively in certain ﬁelds, such as supervised learning
in machine learning, it has been much less explored in others, such as reinforce-
ment learning (RL). In this paper, we show how duality is involved in a variety of
RL work, from that which spearheaded the ﬁeld, such as Richard Bellman’s value
iteration, to that which was done within just the past few years yet has already had
signiﬁcant impact, such as TRPO, A3C, and GAIL. We show that duality is not
uncommon in reinforcement learning, especially when value iteration, or dynamic
programming, is used or when ﬁrst or second order approximations are made to
transform initially intractable problems into tractable convex programs.

1 Introduction

The study of optimization problems that are dual to certain initial problems has led to key insights,
including efﬁcient ways to bound or exactly solve these original problems [5]. Though duality is
used extensively in certain ﬁelds, such as supervised learning in machine learning, it has been less
explored in others, such as reinforcement learning (RL). In this paper, we show how duality is
involved in a variety of RL work, from that which spearheaded the ﬁeld [3, 11, 4] to that which was
done within the past few years but has already had signiﬁcant impact [19, 10, 14].

We show that duality is not uncommon in reinforcement learning, especially when value iteration, or
dynamic programming, is used or when ﬁrst or second order approximations are made to transform
initially intractable problems into tractable convex programs [19, 12, 1]. We touch on a number
of works that span a wide range of RL paradigms but focus on some works that have been particu-
larly inﬂuential, such as Trust Region Policy Optimization (TRPO) [19], Asynchronous Advantage
Actor-Critic (A3C) [14], and Generative Adversarial Imitation Learning (GAIL) [10]. In some cases
duality is used as a theoretical tool to prove certain results or to gain insight into the meaning of the
problem involved. In other cases duality is leveraged to employ gradient-based methods over some
dual space, as is done in alternating direction method of multipliers (ADMM) [6], mirror descent
[2], and dual averaging [22, 8].

We hope this work will encourage others to more strongly consider duality in reinforcement learning,
which we believe to be an often underexplored yet promising line of reasoning.

2 Preliminaries

We consider a standard reinforcement learning framework in which we have a Markov decision
process (MDP) M = (S, A, P, R, T, γ), where S is the state space, A is the action space, P :
S × A × S → [0, 1] is the distribution representing the transition probabilities of the agent arriving
in state s′ after taking action a while in state s, R : S × A → R is the reward function representing
the reward the agent obtains by taking action a while in state s, T > 0 is the time horizon and can be
either ﬁnite or inﬁnite, and γ ∈ [0, 1] is the multiplicative rate at which future rewards are discounted
per time step. For simplicity, we will usually set γ = 1 and drop this term. Throughout this paper

 
 
 
 
 
 
we will use the terms trajectory, behavior, and demonstration interchangeably, all of which refer to
a ﬁnite or inﬁnite sequence of states and actions {s1, a1, s2, a2, . . . , sT , aT }.
We assume the reader has at least an introductory understanding of duality theory and defer its
exposition to [5].

3 Duality in Dynamic Programming

3.1 Unregularized Markov Decision Processes

Duality has well-established history in the theory underlying fundamental classes of RL algorithms,
such as value iteration and policy optimization.
In particular, the Bellman backup operator Tπ
[3] induced by a policy π and applied to Q functions Qπ induced by π represents the dynamic
programming update

Qπ(st, at) = R(st, at) +

st+1∈S
X

P (st+1 | st, at)





at+1∈A
X

Qπ(st+1, at+1)



,



which under mild conditions [18] converges to the ﬁxed point Qπ.
Since the goal in reinforcement learning is to ﬁnd the optimal policy π∗ that maximizes the expected
sum of rewards obtained by the agent, an equivalent problem is to ﬁnd π∗ that maximizes the average
reward ¯R obtained by the agent over timesteps t = 1, 2, 3, . . .
MDP theory allows us to write the average reward ¯R(π), obtained by following policy π, in terms
of R, π, and the stationary state distribution νπ(x) under π. Namely, we have

¯R(π) =

νπ(s)π(a | s)R(s, a).

X(s,a)∈S×A

The stationary state distribution νπ is linear in the stationary state-action distribution µπ = νππ,
which suggests that the problem of ﬁnding the optimal policy can be formulated as a linear program
(LP) with decision variable µ over the probability simplex C:

µ∗ = arg max

¯R(µ).

µ∈C

(1)

[18] shows that by assuming µ∗ ∈ C, which holds when the MDP has only one recurrent class, (1)
is the dual of the LP

max
¯R∈R

¯R

subj. to

¯R − R(s, a) + V (s) −

P (s′ | s, a)V (s′),

for all s, a ∈ S × A,

s′
X

(2)

a P (a | ·)Q( · , a) is known as the value function. Since this is
where the dual variable V (·) =
a linear program, if the optimal average reward ¯R∗ is attained, strong duality holds, and it can be
shown that the optimal value function V ∗ is the solution to the average-reward Bellman equations

P

V ∗(s) = max

a  

R(s, a) − ¯R∗ +

P (s′ | s, a)V ∗(s)

!

s′
X

,

for all s ∈ S,

(3)

and that V ∗ is a ﬁxed point of the Bellman operator Tπ∗ under π∗.

2

3.2 Regularized Markov Decision Processes

[16] deﬁnes a regularized objective ¯Rη(µ) inspired by the linear program (2) and proposes the
concave optimization problem

max
µ∈C

¯Rη(µ) = max

µ∈C (

s,a
X

µ(s, a)R(s, a)

−

(cid:2)

(cid:3)

1
η

W (µ)

)

,

(4)

where η > 0 is the scale parameter that trades-off between the original objective and regularization
and W : RS×A → R is a convex regularization function.
[16] shows that by using Bregman divergences DS(µ k µ′), induced by the negative Shannon
entropy, and DC (µ k µ′), induced by the conditional entropy, for some reference distribution µ′ as
choices of W (µ) in (4), the dual to (4) is similar to the average-reward Bellman equations (3). To
note, these choices of W allow us to perform mirror descent to ﬁnd both the primal and dual optimal
values.

Furthermore, [16] shows that the popular reinforcement learning algorithms Trust Region Policy
Optimization (TRPO) [19] and Asynchronous Advantage Actor-Critic (A3C) [14] can be formulated
as optimization problems with the form of (4) for particular choices of W . We will revisit each of
these when we discuss TRPO in Section 5.1 and A3C in Section 5.2.

3.2.1 Relative entropy Bregman divergence

Namely, [17, 25, 16] show that for W := DS(· k µ′), we have

µη(s, a)∗ ∝ µ′(s, a)eη

R(s,a)+Ps′ P (s′

|s,a)V ∗

(s′

)−V ∗

(s)

and the dual function

(cid:0)

,

(cid:1)

g(V ) =

1
η

log

µ′(s, a)eη

R(s,a)+Ps′ P (s′

|s,a)V ∗

(s′

)−V ∗

(s)

s,a
X

(cid:0)

.

(cid:1)

Assuming that strong duality holds, we have

g(V ∗

η ) = ¯R∗

η = max
µ∈C

¯Rη(µ)

3.2.2 Conditional entropy Bregman divergence

[16] shows that for W := DC(· k µ′), we have

η(a | s) ∝ πµ′ (a | s)eη
π∗

R(s,a)+Ps′ P (s′

|s,a)V ∗

(s′

)−V ∗

(s)

and the dual problem of (4) is

(cid:0)

max
λ∈R

λ

subj. to

V (s) =

1
η

log

a
X

πµ′ (a | s)eη

R(s,a)−λ+Ps′ P (s′

|s,a)V (s′

)

(cid:0)

which has dual optimal solution

(cid:1)

,

(cid:1)

for all s ∈ S,

(5)

(6)

(7)

(8)

(9)

V ∗
η (s) =

1
η

log

πµ′ (a | s)eη

R(s,a)− ¯R∗

η+Ps′ P (s′

|s,a)V ∗

η (s′

)

a
X

(cid:0)

3

for all s′ ∈ S.

(10)

,

(cid:1)

4 Duality in Learning from Demonstration

Learning from demonstration (LfD) is a paradigm in which an agent is given demonstrations from
some presumably expert agent and aims to imitate those demonstrations as closely as possible or
learn the reward function that the expert was trying to maximize. LfD can be broken into three
subﬁelds, behavioral cloning, adversarial imitation learning, and inverse reinforcement learning.

In behavioral cloning (BC), the agent aims to mimic the expert demonstrations without reward signal
nor interaction with the environment. Though attractively simple, BC is susceptible to compounding
errors due to covariate shift and lack of feedback. Since an agent that performs BC learns only from
the demonstrations provided to it, BC usually requires a large amount of data that captures both the
behaviors under optimal conditions as well as corrective actions that rectify suboptimal behaviors.

In adversarial imitation learning, a discriminator is given unlabeled demonstrations from the agent
and expert. For each demonstration the discriminator is trained to classify whether the demonstration
was generated by the agent or by the expert. The agent’s goal is to fool the discriminator as often as
possible, and by training to maximize this objective, the agent learns to generate demonstrations that
are similar to the expert demonstrations. This is akin to the generative adversarial network (GAN)
framework [9], and indeed, we will soon see that adversarial imitation learning can be see as GAN
training [10]. Unlike in behavioral cloning, in adversarial imitation learning the agent generates
trajectories by interacting with the environment.

In inverse reinforcement learning (IRL), the primary objective is to recover the reward function
that the expert agent was optimizing for. Often simultaneously, an agent is trained to maximize an
iteration of this reward function and generates demonstrations comparable to expert demonstrations
at convergence. A major downside to IRL is that it is typically posed as a two-loop problem, in
which the inner loop requires performing an expensive reinforcement learning procedure.

4.1 Generative Adversarial Imitation Learning (GAIL)

Generative Adversarial Imitation Learning [10] is a framework for learning an expert policy from
expert demonstrations while avoiding the need to recover the optimal reward function. However, it
assumes that the expert agent has optimized for some optimal reward function R∗ : S × A −→ R
from which a unique optimal policy π∗ : S −→ A can be learned. In fact, since we assume that
R∗ ∈ IRL(π∗) and π∗ ∈ RL(R∗), we have

R∗ ∈ arg max

R∈R

Eπ∗

∞

"

t=0
X

γtR(st, at)
#

.

π∗ ∈ arg max

π∈Π

Eπ

∞

"

t=0
X

γtR∗(st, at)
#

.

IRL optimizes for R∗ and π∗ simultaneously by solving the saddle-point optimization problem

max
R∈R

min
π∈Π

(cid:18)

E

(s,a)∼π [R(s, a)]

− E

(s,a)∼π∗ [R(s, a)] .

(cid:19)

Maximum causal entropy IRL [24, 23] aims to recover a policy that not only performs well but also
captures diverse behaviors. It does so by modifying the objective directly above to also maximize
the causal entropy H of the recovered policy.

[10] shows that under mild conditions, the dual to this modiﬁed problem is actually a problem
that aims to match the stationary state-action distributions of the expert policy and the policy being
trained. Through this result, they propose an adversarial imitation learning framework with guaran-
teed convergence results as well as a robust, practical imitation learning algorithm.

4

5 Duality in Policy Search

5.1 Trust Region Policy Optimization (TRPO)

Policy gradient (PG) algorithms [21, 20] are a family of algorithms that aim to recover the optimal
policy π∗ with respect to some reward function R, usually by interacting with the environment,
obtaining Monte Carlo estimates of the discounted rewards, and computing the gradient of the policy
with respect to these rewards so that gradient-based optimization over the policy space converges to
π∗.

In vanilla PG, the policy being trained undergoes gradient updates that are based on a loss function
with respect to the policy parameters. Though the policy is kept close in parameter space, in prac-
tice vanilla PG can be highly unstable, and policy performance can vary signiﬁcantly from iteration
to iteration. [19] proposes Trust Region Policy Optimization (TRPO), which overcomes this issue
by constraining the expected Kullback-Leibler divergence from the old policy θk+1 to the new pol-
icy θk over the states encounted while running the old policy. Namely, [19] proposes a heuristic
approximation to the solution of the theoretical TRPO problem,

θk+1 = arg max

θ∈Θ

L(θ, θk)

subj. to

¯DKL(θ k θk) ≤ δ,

(11)

where the surrogate advantage function L(θ, θk) quantiﬁes how well the policy πθ performs com-
pared with the old policy πθk using trajectories obtained by running the old policy,

L(θ, θk) =

E
(s,a)∼πθk (cid:20)

πθ(a | s)
πθk (a | s)

R(s, a) −

(cid:18)

E
a∼πθk (·|s)

[R(· | s)]

,

(cid:19)(cid:21)

and

¯DKL(θ k θk) = E

s∼πθk

[DKL(πθ(· | s) k πθk (· | s))] .

Interestingly, by using a linear approximation to the objective function and a quadratic approxima-
tion to the constraint in (11), we get exactly the natural policy gradient [12] update,

θk+1 = arg max

θ∈Θ

subj. to

· (θ − θk)

∇θL(θ, θk)
(cid:12)
(cid:12)
(cid:12)

θ=θk
(θ − θk)T H(θk)(θ − θk) ≤ δ,

(12)

where H is the Hessian matrix of the constraint in (11) with respect to θ. Using duality theory, this
problem has the analytical solution

θk+1 = θk + α

s

2δ
gT H −1g

H −1g,

(13)

where α is determined through backtracking line search and g = ∇θL(θ, θk)
(cid:12)
(cid:12)
(cid:12)

Computing or storing H −1 may be prohibitively expensive, but according to the form of (13), ﬁnding
H −1g is sufﬁcient, and computing or storing this matrix-vector product is relatively cheap. TRPO
uses the conjugate gradient method to obtain H −1g while circumventing computation or storage of
H −1, enabling its use in practical settings.

.

θ=θk

5

5.1.1 Trust Region Policy Optimization as approximated Mirror Descent

[16] shows that the TRPO update with the form

πk+1 = arg max

π

(

x
X

νπk (s)

π(a | s)

R(s, a)

a
X

(cid:18)

P (s′ | s, a)V πk

η=∞(s′) − V πk

η=∞(s) −

+

s′
X

1
η

log

π(a | s)
πk(a | s) !)

(14)

approximates mirror descent and that this update can be expressed in closed form as

πk+1(a | s) ∝ πk(a | s)eη

R(s,a)+Ps′ P (s′

|s,a)V

πk
η=∞(s′

)−V

πk
η=∞(s)

.

[19] claims that the theoretical TRPO updates are monotonic improvements, but do not claim
whether TRPO converges to an optimal policy. However, through duality theory, [16] shows that
the theoretical TRPO updates indeed converge to the optimal policy π∗.

(cid:0)

(cid:1)

5.2 A3C as Dual Averaging

[16] shows that the A3C algorithm [14] aims to optimize the objective

νπk (s)

π(a | s)

R(s, a) +

x
X

a
X

s′
X

P (s′ | s, a)V πk

η=∞(s′) − V πk

η=∞(s) −

1
ηk log πθ(a | s)

,

!

which can be interpreted as a dual-averaging [22, 8] variant of the TRPO objective in (14).

Again through the use of duality theory, [16] conjectures that A3C does not converge and proposes
a modiﬁed method with convergence guarantees.

5.3 Constrained Policy Optimization (CPO)

[1] uses an approximated update similar to (12) but with an additional constraint that is afﬁne in θ
to make safety guarantees that hold in expectation. The dual problem in CPO is similar to that of
TRPO, and the optimal update to θ can be derived analytically through duality theory here as well.

5.4 A Lyapunov-based approach to Safe Reinforcement Learning

[7] puts forth a primal-dual subgradient method that obtains a policy that satisﬁes certain safety
constraints in expectation.

5.5 Guided Policy Search (GPS)

[13] can be used to train complex high-dimensional policies without optimizing the policies directly
in high-dimensional parameter spaces. GPS trains a student policy to imitate a teacher policy that
is also being trained to produce behaviors that the the student can demonstrate. This approach
formulates the Lagrangian of a speciﬁed cost function and takes gradient steps on primal and dual
variables in an alternating fashion.

Furthermore, [15] shows that GPS can be seen as an approximate variant of mirror descent. More
generally, [17, 25] show that the form of the update in GPS can formulated as mirror descent with the
Bregman divergence DS induced by the relative entropy of the stationary state-action distribution
µπ corresponding to the policy π.

6 Conclusion

The study and application of duality theory has led to key advances in the understanding and efﬁ-
ciency of solving optimization problems in a number of ﬁelds. However, duality hasn’t received

6

 
comparable focus in reinforcement learning even though much of the inﬂuential work in RL has
relied on it. We’ve shown that duality arises more often in RL than one may think, with beneﬁts
including increased interpretability, convergence guarantees, and state-of-the-art advances in algo-
rithmic performance and efﬁciency. Going forward we hope others will more strongly consider
duality while addressing reinforcement learning problems.

References

[1]

Joshua Achiam et al. “Constrained policy optimization”. In: Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70. JMLR. org. 2017, pp. 22–31.

[2] Amir Beck and Marc Teboulle. “Mirror descent and nonlinear projected subgradient methods
for convex optimization”. In: Operations Research Letters 31.3 (2003), pp. 167–175.
[3] Richard Bellman. “Dynamic programming”. In: Science 153.3731 (1966), pp. 34–37.
[4] Dimitri P Bertsekas et al. Dynamic programming and optimal control. Vol. 1. 2. Athena sci-

entiﬁc Belmont, MA, 1995.

[5] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge

university press, 2004.

[6] Stephen Boyd et al. “Distributed optimization and statistical learning via the alternating direc-
tion method of multipliers”. In: Foundations and Trends R(cid:13) in Machine learning 3.1 (2011),
pp. 1–122.

[7] Yinlam Chow et al. “A lyapunov-based approach to safe reinforcement learning”. In: Ad-

[8]

[9]

[10]

vances in neural information processing systems. 2018, pp. 8092–8101.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. “Dual averaging for distributed op-
timization: Convergence analysis and network scaling”. In: IEEE Transactions on Automatic
control 57.3 (2011), pp. 592–606.
Ian Goodfellow et al. “Generative adversarial nets”. In: Advances in neural information pro-
cessing systems. 2014, pp. 2672–2680.
Jonathan Ho and Stefano Ermon. “Generative adversarial imitation learning”. In: Advances
in neural information processing systems. 2016, pp. 4565–4573.

[11] Ronald A Howard. “Dynamic programming and markov processes.” In: (1960).
[12] Sham M Kakade. “A natural policy gradient”. In: Advances in neural information processing

systems. 2002, pp. 1531–1538.

[13] Sergey Levine and Vladlen Koltun. “Guided policy search”. In: International Conference on

Machine Learning. 2013, pp. 1–9.

[14] Volodymyr Mnih et al. “Asynchronous methods for deep reinforcement learning”. In: Inter-

national conference on machine learning. 2016, pp. 1928–1937.

[15] William H Montgomery and Sergey Levine. “Guided policy search via approximate mirror

descent”. In: Advances in Neural Information Processing Systems. 2016, pp. 4008–4016.

[16] Gergely Neu, Anders Jonsson, and Vicenç Gómez. “A uniﬁed view of entropy-regularized

[17]

markov decision processes”. In: arXiv preprint arXiv:1705.07798 (2017).
Jan Peters, Katharina Mulling, and Yasemin Altun. “Relative entropy policy search”. In:
Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence. 2010.

[18] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

[19]

John Wiley & Sons, 2014.
John Schulman et al. “Trust region policy optimization”. In: International conference on ma-
chine learning. 2015, pp. 1889–1897.

[20] David Silver et al. “Deterministic policy gradient algorithms”. In: 2014.

[21] Richard S Sutton et al. “Policy gradient methods for reinforcement learning with function ap-
proximation”. In: Advances in neural information processing systems. 2000, pp. 1057–1063.

[22] Lin Xiao. “Dual averaging methods for regularized stochastic learning and online optimiza-

tion”. In: Journal of Machine Learning Research 11.Oct (2010), pp. 2543–2596.

7

[23] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. “Modeling interaction via the principle

of maximum causal entropy”. In: (2010).

[24] Brian D Ziebart et al. “Maximum entropy inverse reinforcement learning.” In: 2008.
[25] Alexander Zimin and Gergely Neu. “Online learning in episodic Markovian decision pro-
cesses by relative entropy policy search”. In: Advances in neural information processing sys-
tems. 2013, pp. 1583–1591.

8

