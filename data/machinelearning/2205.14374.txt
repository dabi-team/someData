Syntax-Guided Program Reduction for
Understanding Neural Code Intelligence Models

Md Rafiqul Islam Rabin
mrabin@uh.edu
University of Houston
Houston, TX, USA

Aftab Hussain
ahussain27@uh.edu
University of Houston
Houston, TX, USA

Mohammad Amin Alipour
maalipou@central.uh.edu
University of Houston
Houston, TX, USA

Abstract

1 Introduction

2
2
0
2

n
u
J

4
1

]
E
S
.
s
c
[

2
v
4
7
3
4
1
.
5
0
2
2
:
v
i
X
r
a

Neural code intelligence (CI) models are opaque black-boxes
and offer little insight on the features they use in making
predictions. This opacity may lead to distrust in their pre-
diction and hamper their wider adoption in safety-critical
applications. Recently, input program reduction techniques
have been proposed to identify key features in the input pro-
grams to improve the transparency of CI models. However,
this approach is syntax-unaware and does not consider the
grammar of the programming language.

In this paper, we apply a syntax-guided program reduc-
tion technique that considers the grammar of the input pro-
grams during reduction. Our experiments on multiple mod-
els across different types of input programs show that the
syntax-guided program reduction technique is faster and
provides smaller sets of key tokens in reduced programs. We
also show that the key tokens could be used in generating
adversarial examples for up to 65% of the input programs.

CCS Concepts: • Software and its engineering → Soft-
ware testing and debugging; • Computing methodolo-
gies → Feature selection.

Keywords: Neural Models of Source Code, Program Reduc-
tion, Feature Engineering, Transparency, Interpretability

ACM Reference Format:
Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin
Alipour. 2022. Syntax-Guided Program Reduction for Understand-
ing Neural Code Intelligence Models. In Proceedings of the 6th ACM
SIGPLAN International Symposium on Machine Programming (MAPS
’22), June 13, 2022, San Diego, CA, USA. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
MAPS ’22, June 13, 2022, San Diego, CA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9273-0/22/06. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

A neural code intelligence (CI) model is a deep neural net-
work that takes a program as input and predicts certain
properties of that program as output, e.g., predicting method
name [3], variable name [5], or type [12] from a program
body. Unfortunately, these models are opaque black-boxes
and it is hard to interpret and debug the results of these
models. Recently, there is an increasing interest in probing
CI models to improve their transparency and identify and
understand their potential flaws. For instance, recent stud-
ies suggest that state-of-the-art CI models do not always
generalize to other experiments [13, 14]; they heavily rely
on specific individual tokens [11, 18] or structures [15, 21],
are prone to learn noise [20] or duplication [1], and are of-
ten vulnerable to semantic-preserving adversarial examples
[16, 22, 27].

Neural CI models, as in other neural models, represent
an input program as continuous distributed vectors that
are computed after training on large volumes of programs.
It makes understanding what input features a model has
learned a very challenging task. For example, Code2Vec
model [8] learns to represent an input program as a sin-
gle fixed-length high dimensional embedding, however, it
is difficult to understand the meaning or characteristics of
each of these dimensions. Attention-based approaches are
commonly applied to find important code elements in a pro-
gram. For example, Bui et al. [9] attempt to identify relevant
code elements by perturbing statements of the program and
combining corresponding attention and confidence scores.
However, the attention-based approach poorly correlates
with key code elements.

Several studies have already been conducted to find rele-
vant input features in models’ inferences. Allamanis et al. [3]
use a set of features from programs and show that extracting
relevant features that capture global context is essential for
learning effective code context. Rabin et al. [21] attempt to
find key input features of a label by manually inspecting
some input programs of that label. However, this manual
inspection does not scale to large datasets with many labels.
Suneja et al. [25] and Rabin et al. [18] use input program
reduction techniques, based on Delta Debugging [28], to find
minimal inputs that preserve the model’s prediction, hence
finding key tokens in the program with respect to the pre-
diction. However, these approaches are syntax-unaware and

 
 
 
 
 
 
MAPS ’22, June 13, 2022, San Diego, CA, USA

Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour

can create a large number of invalid programs as they do not
follow the grammar of the programming language during
the reduction, which in turn can hinder the identification of
the key tokens.

In this paper, we apply a syntax-guided reduction tech-
nique, called Sivand-Perses, to remove irrelevant parts from
an input program. Given a CI model and an input program,
our approach adopts Perses [24] to reduce the input pro-
gram while preserving the model’s prediction. The approach
continues reducing the input program as long as the model
maintains the same prediction on the reduced program as
on the original program. As the syntax-guided technique fol-
lows the syntax of the programming language, it will always
generate valid input programs. Using the syntax informa-
tion also can improve the performance of the reduction, as
the reduction only explores the space of syntactically valid
programs. The main insight is that, by reducing some in-
put programs of a target label, we may better understand
what key input features a CI model learns from the training
dataset.

An experiment with two CI models and four types of
input programs suggests that the syntax-guided Sivand-
DD outperforms the syntax-unaware alternative Sivand-DD
proposed in [18]. While Sivand-Perses always generates
valid programs, Sivand-DD generates only around 10% valid
programs. On average, Sivand-Perses removes 20% more
tokens, takes 70% fewer reduction steps, and spends half of
the reduction time compared to Sivand-DD for reducing an
input program. Furthermore, our results show that we can
find key input features by reducing input programs using
Sivand-Perses, which can provide additional explanation for
a prediction and highlight the importance of input features
in reduced programs by triggering 10% more mispredictions
with 50% fewer adversarial examples.

Contributions. This paper makes the following contribu-
tions:

• We propose a syntax-guided program reduction ap-
proach, Sivand-Perses, for the identification of key
features in neural code intelligence models.

• We evaluate the performance of Sivand-Perses and
compare it with the state-of-the-art technique on two
CI models for the code summarization task.

2 Related Work

There has been some work in the area of code intelligence
that focuses on the understanding of what relevant features
a black-box model learns for correct predictions. While some
works [11, 13, 14, 16, 18, 22, 25, 27] study the reliance of
models on specific features, many works [3, 9, 18, 21, 25, 26]
focus on finding relevant features for explaining models’
prediction.

2.1 Learning Representation of Source Code

An input program is usually represented as vector embed-
dings for processing and analyzing by neural models. Alla-
manis et al. [2] introduced a framework that processed token
sequences and abstract syntax trees of code to represent the
raw programs. Alon et al. [8] proposed an attention-based
neural model that uses a bag of path-context from abstract
syntax tree for representing any arbitrary code snippets. Alla-
manis et al. [5] constructed data and control flow graphs from
programs to encode a code snippet. Hellendoorn et al. [12]
proposed an RNN-based model using sequence-to-sequence
type annotations for type suggestion. There are some sur-
veys on the taxonomy of models that exploit source code
analysis [4, 23]. Chen and Monperrus [10] also provide a
survey that includes the usage of code embeddings based
on different granularities of programs. However, these mod-
els are often black-box and do not provide any insight on
the meaning or characteristic of learned embeddings. What
features or patterns these embeddings represent are largely
unknown. In this work, we extract key input features that a
model learns for predicting a target label as an explanation
of learned embeddings.

2.2 Reliance on Specific Features

Models often learn irrelevant features, simple shortcuts, or
even noise for achieving target performance. Compton et al.
[11] show that the code2vec embeddings highly rely on vari-
able names and cannot embed an entire class rather than an
individual method. They investigate the effect of obfuscation
on improving code2vec embeddings that better preserves
code semantics. They retrain the code2vec model with obfus-
cated variables to forcing it on the structure of code rather
than variable names and aggregate the embeddings of all
methods from a class. Following the generalizability of word
embeddings, Kang et al. [13] assess the generalizability of
code embeddings in various software engineering tasks and
demonstrate that the learned embeddings by code2vec do
not always generalize to other tasks beyond the example
task it has been trained for. Rabin et al. [16] and Yefet et al.
[27] demonstrate that the models of code often suffer from
a lack of robustness and are vulnerable to adversarial ex-
amples. They mainly introduce small perturbations in code
for generating adversarial examples that do not change any
semantics and find that the simple renaming, adding or re-
moving tokens changes model’s predictions. Suneja et al.
[25] uncover the model’s reliance on incorrect signals by
checking whether the vulnerability in the original code is
missing in the reduced minimal snippet. They find that model
captures noises instead of actual signals from the dataset for
achieving high predictions. Rabin et al. [18] demonstrates
that models often use just a few simple syntactic shortcuts for
making prediction. Rabin et al. [20] later show that models
can fit noisy training data with excessive parameter capacity

Syntax-Guided Program Reduction for Understanding Neural Code Intelligence Models

MAPS ’22, June 13, 2022, San Diego, CA, USA

and thus suffer in generalization performance. As models
often learn noise or irrelevant features for achieving high
prediction performance, the lack of understanding of what
input features models learn would hinder the trustworthi-
ness to correct classification. Such opacity is substantially
more problematic in critical applications such as vulnerabil-
ity detection or automated defect repair. In this work, we
extract key input features for CI models in order to provide
better transparency and explanation of predictions.

2.3 Extracting Relevant Input Features

Several kinds of research have been done in finding relevant
input features for models of source code. Allamanis et al. [3]
exhibit that extracting relevant features is essential for learn-
ing effective code context. They use a set of hard-coded fea-
tures from source code that integrate non-local information
beyond local information and train a neural probabilistic lan-
guage model for automatically suggesting names. However,
extracting hard-coded features from source code may not
be available for arbitrary code snippets and in dynamically
typed languages. Bui et al. [9] propose a code perturbation
approach for interpreting attention-based models of source
code. It measures the importance of a statement in code by
deleting it from the original code and analyzing the effect on
predicted outputs. However, the attention-based approach
often poorly correlates with key elements and suffers from
a lack of explainability. Rabin et al. [21] attempt to find key
input features of a label by manually inspecting some input
programs of that label. They extract handcrafted features for
each label and train simple binary SVM classification models
that achieve highly comparable results to the higher dimen-
sional code2vec embeddings for the method naming task.
However, the manual inspection cannot be applied to a large
dataset. Wang et al. [26] recently propose a mutate-reduce
approach to find key features in the code summarization
models considering valid programs. Suneja et al. [25] and
Rabin et al. [18] apply a syntax-unaware program reduction
technique, Delta Debugging [28], to find the minimal snippet
which a model needs to maintain its prediction. By removing
irrelevant parts to a prediction from the input programs, the
authors aim to better understand key features in the model
inference. However, the syntax-unaware approach creates a
large number of invalid programs during the reduction as it
does not follow the syntax of programs, thus increases the
total steps and time of reduction. In this work, we apply a
syntax-guided program reduction technique that overcomes
the overhead raised by the syntax-unaware technique.

3 Design and Implementation

This section describes our approach of extracting input fea-
tures for code intelligence (CI) models by syntax-guided
program reduction. We use Perses [24] as the syntax-guided
program reduction technique in our study. We first provide

an overview of how Perses works and then describe how we
adopt it in the workflow of our Sivand-Perses approach.

Perses. Sun et al. [24] have proposed the framework for
syntax-guided program reduction called Perses. Given an
input program, the grammar of that programming language,
and the output criteria, Perses reduces the input program
with respect to the grammar while preserving the output
criteria. It mainly follows the steps below:

• It first parses the input program into a parse tree by

normalizing the definition of grammar.

• Then it traverses the tree and determines whether a
tree node is deletable (e.g. whether it follows the gram-
mar and preserves the output criteria). If yes, it prunes
the sub-tree from that node and generates a valid re-
duced program, else it ignores that node and avoids
generating invalid programs. Thus, in each iteration
of reduction, it ensures generating syntactically valid
program variants that preserves the same output cri-
teria.

• Next, the deletion of one node may enable the deletion
of another node. Therefore, Perses is repeatedly ap-
plied to the reduced program until no more tree nodes
can be removed – a process known as fixpoint mode
reduction.

• The final reduced program is called 1-tree-minimal,
and any further attempts to reduce the program would
generate an invalid program or change the output
criteria.

For supporting a programming language data, the syntax-
guided technique leverages knowledge about program syn-
tax for avoiding generating syntactically invalid programs.
We integrate Perses as a black-box framework into Sivand-
Perses for extracting input features of CI models.

Workflow. Figure 1 depicts a high-level view of the work-
flow in Sivand-Perses. Given a set of input programs, our
approach reduces each input program using Perses while pre-
serving the same prediction by the CI model. The approach
removes irrelevant parts from an input program and keeps
the minimal code snippet that the CI model needs to preserve
its prediction. The main insight is that, by reducing some
input programs of a target label, we can identify key input
features of the CI model for that target label. Our approach
follows the steps below:

• Given an input program 𝑃 and a CI model 𝑀, our
approach first records the prediction 𝑦 (i.e., predicted
method name) given by the CI model 𝑀 on the input
program 𝑃, such as 𝑦 = 𝑀 (𝑃).

• Using Perses, we then generate a candidate reduced
program 𝑅 ′ by removing some nodes from the tree of
the input program 𝑃, such as 𝑅 ′ = 𝑃𝑒𝑟𝑠𝑒𝑠 (𝑃).

• If the candidate reduced program 𝑅 ′ does not hold the
same prediction 𝑦 by the CI model 𝑀 (i.e., 𝑦 ≠ 𝑀 (𝑅 ′)),

MAPS ’22, June 13, 2022, San Diego, CA, USA

Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour

Figure 1. Workflow of Sivand-Perses approach.

we reject this candidate program and create another
candidate program by removing some other nodes
from the tree of the input program.

• If the candidate reduced program 𝑅 ′ preserves the
same prediction 𝑦 by the CI model 𝑀 (i.e., 𝑦 = 𝑀 (𝑅 ′)),
we continue reduction and iteratively search for the
final reduced program 𝑅 that produces the same pre-
diction 𝑦.

• The final reduced program is 1-tree-minimal, which
contains the key input features that the CI model must
need for making the correct prediction 𝑦.

After reducing a set of input programs of a target label,
we extract the node type and token value from the abstract
syntax tree (AST) of each reduced program. Every extracted
element from reduced programs is considered as a candidate
input feature. The most common elements are identified as
label-specific key features and other uncommon elements
are identified as input-specific sparse features.

Implementation. Our approach is model-agnostic and can
be applied for various tasks and programming datasets. In
this paper, for experimentation of our approach, we study
two well-known code intelligence models (Code2Vec and
Code2Seq), a popular code intelligence task (MethodName)
and one commonly used programming language dataset
(Java-Large) with different types of input programs.

Task. We use the method name prediction (MethodName
[3]) task in this study. This task is commonly used by re-
searchers in the code intelligence domain for various appli-
cations such as code summarization [3, 6], representation
learning [7, 8], neural testing [13, 16, 27], feature extraction
[18, 26], and so on [4, 23]. In the MethodName task, a model
attempts to predict the name of a method from its body. For
example, given the code snippet “void f(int a, int b)
{int temp = a; a = b; b = temp;}”, the Code2Vec model
correctly predicts the method’s name as “swap”.

Models. We use the Code2Vec [8] and Code2Seq [7] code
intelligence models for MethodName task. Both models
use paths from abstract syntax trees (AST) to encode a pro-
gram. Given a sample expression “a = b;”, an example of

path context in AST is “a, <NameExpr ↑ AssignExpr ↓
IntegerLiteralExpr>, b”.

• Code2Vec. This model extracts a bag of path-contexts
from the AST of the program where each path-
context includes a pair of terminal nodes and the
corresponding path between them. The model learns
embeddings of these path-contexts during training
and uses an attention mechanism to aggregate
multiple path-contexts to a single code vector. The
code vector is used as a representation of the program
for making a prediction.

• Code2Seq. This model also extracts a bag of
path-contexts from the AST of the program but it
sub-tokenizes each path-context. The Code2Seq
model uses a bi-directional LSTM to encode paths
node-by-node, and another LSTM to decode a target
sequence one-by-one.

Dataset. For the MethodName task, we use the Java-
Large dataset [7]. This dataset contains a total of 9, 500 Java
projects from GitHub, where 9, 000 projects are for the train-
ing set, 200 projects for the validation set, and 300 projects
for the test set. Using training set and validation set, we train
both the Code2Vec and Code2Seq models.

Input Types. The dataset from GitHub is often imbal-
anced and contains different sizes and frequencies of input
programs. Therefore, we choose different types of input pro-
grams from the Java-Large test set to evaluate the effec-
tiveness of our approach in terms of reduction and feature
extraction.

• Frequent Methods: We randomly sample a total of
100 input programs from the top-10 most occurring
method names.

• Rare Methods: We randomly sample a total of 100 input
programs from the least occurring method names.
• Smaller Methods: We randomly sample a total of 100
input programs that contains less than 10 lines of code.
• Larger Methods: We randomly sample a total of 50
input programs that have around 100 lines of code.

Syntax-Guided Program Reduction for Understanding Neural Code Intelligence Models

MAPS ’22, June 13, 2022, San Diego, CA, USA

Moreover, to demonstrate the key input features, we select
correctly predicted input programs from the ten most fre-
quent labels of the Java-Large test set for feature extraction.
Those labels (methods) are: equals, main, setUp, onCreate,
toString, run, hashCode, init, execute, and get.

Syntax-unaware Reduction Technique. We

use
Sivand-DD [18] in this study which uses the Delta Debug-
ging algorithm as the syntax-unaware program reduction
technique. Zeller and Hildebrandt [28] have proposed the
Delta Debugging algorithm to reduce the size of an input
program. The algorithm iteratively splits an input program
into multiple candidate programs by removing parts of the
input program. The algorithm then checks if any resulting
candidate program preserves the prediction of the model
on the original input program. When the algorithm finds
a candidate satisfying the property, it uses the candidate
as the new base to be reduced further. Otherwise, the
algorithm increases the granularity for splitting, until it
determines that the input program cannot be reduced
further. Considering the input reduction type (token or
char), we use the following two variations of Sivand-DD:

• Sivand-Token: In this token level reduction approach,
Sivand-DD reduces the size of an input program token
by token. We mostly use the Sivand-Token as the
default baseline for Sivand-DD in this study.

• Sivand-Char: In this char level reduction approach,
Sivand-DD reduces the size of an input program char
by char. We use the Sivand-Char approach to provide
an additional explanation in Section 4.3 and Figure 3.
Rabin et al. [17, 18] described in more detail how the
Sivand-DD technique is applied in the workflow of reducing
input programs for CI models.
Artifacts. We have publicly shared the artifacts of this study
at https://github.com/mdrafiqulrabin/CI-DD-Perses.

4 Results

In this section, we present the result of our experiments on
the Code2Vec and Code2Seq models and the Java-Large
dataset for different input types.

4.1 Comparative Analysis

Here, we provide a systematic comparison between the
syntax-guided program reduction technique and the
syntax-unaware program reduction technique. In particular,
we compare the syntax-guided Sivand-Perses and the
syntax-unaware Sivand-DD in terms of token reduction,
reduction steps and reduction time.
4.1.1 Token Reduction. The goal of Sivand-Perses and
Sivand-DD is to remove irrelevant tokens from an input
program as much as possible while preserving the same
prediction of the CI model. Figure 2a shows their such abil-
ity in reducing the size of the original input programs for

different input types. We can see that, for all input types,
Sivand-Perses reduces more tokens from input programs
than Sivand-DD. On average, Sivand-Perses removes 20%
more tokens from input programs than Sivand-DD. The
difference is most significant (around 30%) in Large input
types and less significant (around 5%) in Rare input types.
This result suggests that Sivand-Perses is more powerful
than Sivand-DD in reducing the size of an input program.

4.1.2 Reduction Steps. The reduction is applied repeat-
edly to an input program until finding the final minimal
program, from where no more tokens can be removed. From
Figure 2b, we can see that Sivand-Perses on average can
reach the final minimal program within 5 reduction steps.
However, Sivand-DD makes around 20 reductions in Fre-
qent-Rare-Small input types and more than 50 reductions
in Large input type, to reach the final minimal program. The
Sivand-DD reduces an input program by a sequence of to-
kens and backtracks if the reduced program is invalid, where
Sivand-Perses can prune an entire sub-tree from the AST
of the program and always generates a valid reduced pro-
gram following grammar. Thus, Sivand-Perses takes a much
lower number of reduction steps than Sivand-DD to reach
the final minimal program.

4.1.3 Reduction Time. We now compare the average
time taken by Sivand-Perses and Sivand-DD for reducing
an input program. As Sivand-DD takes excessive invalid
steps, Sivand-Perses is expected to spend less time
for program reduction. Figure 2c shows Sivand-Perses
reduces an input program faster than Sivand-DD, for all
input types and, especially so for Large input type. In
Freqent-Rare-Small input types, both Sivand-Perses
and Sivand-DD spend less than 2 minutes to reduce an
input program and comparatively Sivand-Perses takes 30
seconds less time than Sivand-DD. In Large input types,
Sivand-DD spends around 17 minutes for the reduction of
a large program but Sivand-Perses takes only 8 minutes,
which is around 50% less than Sivand-DD.
(cid:15)

(cid:12)

Observation 1: Sivand-Perses allows more token re-
moval than Sivand-DD and always creates valid can-
didate programs. Compared to Sivand-DD, Sivand-
Perses is more likely to reach the final minimal pro-
gram in a smaller number of reduction steps, which
decreases the total reduction time.

(cid:14)

(cid:13)

4.2 Key Input Features

Here, we provide the summary of extracted input features
that CI models learn from training dataset for predicting
the target method name. In our experiment, we consider all
tokens in reduced programs as candidate tokens for input
features. We define a label-specific key feature as a candidate
token that appears in at least 50% of the reduced programs,

MAPS ’22, June 13, 2022, San Diego, CA, USA

Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour

(a) Token Reduction

(b) Reduction Steps

(c) Reduction Time

Figure 2. Comparison between Sivand-DD (blue bar) and Sivand-Perses (orange bar).

Table 1. Summary of input features in Top-10 methods.

Table 2. Key input features in Top-10 methods.

Method

Model

Sivand-DD

Sivand-Perses
Candidate Key Candidate Key

equals

main

setUp

onCreate

toString

run

hashCode

init

execute

get

Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)
Code2Vec
Code2Seq
(both)

Top-10

(both)

30
28
30
27
27
28
36
27
41
31
24
34
21
22
23
30
31
36
13
14
15
30
18
35
20
16
24
56
23
58
324

8
8
8
5
5
5
4
4
4
5
4
5
4
3
4
5
6
6
5
5
5
3
1
3
5
3
6
6
0
6
52

12
10
12
21
4
21
13
13
19
20
14
24
18
18
25
22
13
27
6
12
13
29
10
33
14
7
14
49
16
50
238

5
5
6
5
3
5
5
1
5
4
3
4
5
2
5
5
2
5
5
4
5
3
1
3
5
3
5
4
0
4
47

where other infrequent tokens are input-specific sparse fea-
tures. For brevity and page limit, we only show the Top-10
most frequent methods in Table 1 and Table 2.

Method

Model

Reduction

Key Input Features

equals

main

setUp

onCreate

toString

run

hashCode

init

execute

get

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD
Sivand-Perses
Sivand-DD

if, boolean, Object, o, obj, other, instanceof, Stock
boolean, Object, return, o, obj
if, boolean, Object, obj, other, o, instanceof, Stock
boolean, Object, Override, o, obj
args, void, String, Exception, throws
void, String, args, System, Exception
args, void, String, Exception, throws
void, String, args
void, throws, Exception, setUp
void, throws, Exception, super, setUp
void, throws, Exception, setUp
void
void, savedInstanceState, Bundle, onCreate, if
void, savedInstanceState, super, onCreate
void, savedInstanceState, onCreate, Bundle
void, super, onCreate
String, if, toString, sb
String, Override, StringBuilder, sb, return
String, toString, if
String, return
void, try, catch, 0, x
void, Override, try, catch, x
void, try, catch, 0, Override, x
void, Override
int, hashCode, 0, result, null
int, Override, result, final, prime
int, hashCode, result, 0, null
int, result, Override, prime
void, throws, ServletException
void, throws, ServletException
void
void
void, throws, BuildException, execute, context
void, throws, BuildException, super, execute
void, execute, super
void, super, execute
if, T, null, return, key, Object

Sivand-Perses T, throw, key, return

Sivand-DD

None
Sivand-Perses None

From Table 1, considering both Code2Vec and Code2Seq
models, we can see that both Sivand-Perses and Sivand-
DD identify around 50 tokens, in total, as label-specific key
features in Top-10 methods. However, Sivand-DD contains
a total of 324 candidate tokens in reduced programs, which is

Syntax-Guided Program Reduction for Understanding Neural Code Intelligence Models

MAPS ’22, June 13, 2022, San Diego, CA, USA

1.36x times higher than Sivand-Perses that contains a total
of 238 candidate tokens. In some methods, i.e., ‘equals’ and
‘setUp’, the total number of candidate tokens in Sivand-DD
reduced programs is almost 2x times higher than the candi-
date tokens in Sivand-Perses reduced programs. This may
suggest that the tokens found from the reduced programs of
Sivand-DD are more input-specific while the tokens found
from the reduced programs of Sivand-Perses are more label-
specific.

Furthermore, Table 2 shows the label-specific key features
(sorted by their frequency of occurrences in reduced
programs) extracted by Sivand-DD and Sivand-Perses
from their reduced programs. These key input features from
reduced programs can help to understand the prediction of
the CI model for a target label. For example, Sivand-DD
and Sivand-Perses reveal that “void, args, String,
Exception” are key features for the ‘main’ method. It may
highlight that a sample input program containing those
tokens is more likely to be predicted as the ‘main’ method
by CI models. However, the key feature of the Code2Seq
model for the ‘init’ method is only “void” as it is the only
common token in the reduced programs. Moreover, we did
not find any common token in the reduced programs of the
‘get’ method for the Code2Seq model. Thus, by extracting
key features from reduced programs of a target label, we
may also get an intuition about whether a CI model is
learning a label correctly or not.
(cid:11)

(cid:8)

Observation 2: According to our results, Sivand-
Perses reveals more label-specific key features in its
syntax-guided reduced programs, while Sivand-DD
contains more input-specific sparse features in its
syntax-unaware reduced programs.

(cid:10)

(cid:9)

4.3 Multiple Explanations for a Specific Prediction

Different program simplification approaches, i.e., Sivand-DD
and Sivand-Perses, provide a different set of key features
for a target label by a CI model (Table 2). Those different
features can help us find multiple explanations for a specific
prediction. For instance, Code2Seq predicts the code snippet
in Figure 3a as the ‘main’ method. Sivand-DD with char-
based program reduction (Sivand-Char) gives the minimal
program in Figure 3b, that Code2Seq can predict as main.
We can see the presence of the Main identifier in the method
body of Figure 3b which is one of the key tokens for the
target prediction. On the other hand, the Sivand-DD with
token-based program reduction (Sivand-Token) gives the
minimal program in Figure 3c, which suggests the argument
args has an important role in the target prediction. However,
with the AST-based program reduction (Sivand-Perses), the
minimal program is shown in Figure 3d which highlights the
signature of the method, for which Code2Seq still can predict

(a) A sample input program:

public static void f( String [] args ) {

System . setProperty (

Constants . DUBBO_PROPERTIES_KEY ,
" conf / dubbo . properties ");

Main . main ( args );

}

(b) Sivand-Char reduced program:

d f( Sg [] r){y(C , "" ); Main ( ar ) ;}

(c) Sivand-Token reduced program:

void f( String [] args ) {( " ") ;( args ) ;}

(d) Sivand-Perses reduced program:

void f( String args ) { }

Figure 3. A sample input program and corresponding reduced
programs for different program reduction techniques.

the same target label. Having these multiple explanations
can improve the transparency of the models’ inferences.
(cid:7)

(cid:4)

Observation 3: Different program reduction tech-
niques may provide additional explanations for a better
understanding of a specific prediction.

(cid:6)

(cid:5)

4.4 Key Targeted Adversarial Attacks on Models

Here, we evaluate the importance of key input features in
programs by evaluating the adversarial robustness [16, 27]
of CI models where we change the extracted key features.
We generate adversarial examples by applying semantic-
preserving variable renaming transformation on programs,
similar to [16], where we separately change each variable
and all of its occurrences in the program with token var. We
particularly compare the prediction of CI models before and
after the variable renaming. In this experiment, we gener-
ate three types of adversarial sets: original adversarial set,
key adversarial set, and reduced adversarial set. In original
adversarial set, we target the original input programs and
generate candidate transformed programs by considering all
variables. In key adversarial set, we also target the original in-
put programs but generate candidate transformed programs
by considering variables that occur in the key feature list.
In reduced adversarial set, we target the reduced input pro-
grams for generating candidate transformed programs. The
results of change in prediction (misprediction) for variable
renaming transformation are shown in Table 3.

According to Table 3, on average, the number of generated
candidate transformed programs in original adversarial set is
around 3x times higher than the original programs, however,

MAPS ’22, June 13, 2022, San Diego, CA, USA

Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour

Table 3. Adversarial evaluation with key input features.

Reduction

Model

Adversarial Set #Original #Transformed

Misprediction

Sivand-DD

Sivand-Perses

Code2Vec

Code2Seq

Code2Vec

Code2Seq

Original
Key
Reduced
Original
Key
Reduced

Original
Key
Reduced
Original
Key
Reduced

328

287

320

280

1148
722
379
836
530
267

911
320
253
658
211
160

#

135
107
117
109
102
134

97
58
118
93
80
104

%

11.76
14.82
30.87
13.04
19.25
50.19

10.65
18.12
46.64
14.13
37.91
65.00

only around 12% of them trigger mispredictions. Next, the
number of generated candidate transformed programs in key
adversarial set is around 1.5x times higher than the original
programs and trigger around 22% mispredictions. Although
the key adversarial set contains almost 50% less candidate
transformed programs than the original adversarial set, they
trigger around 10% more mispredictions. On the other hand,
the reduced programs are the minimal programs that CI mod-
els keep for preserving their target predictions. Therefore,
the number of generated candidates transformed programs
in reduced adversarial set is the lowest as there are fewer
tokens to apply transformations. However, the transforma-
tion on reduced programs is more powerful and triggers
the highest percentage of mispredictions, up to 65%. More-
over, comparing between Sivand-DD and Sivand-Perses,
in most cases, Sivand-Perses generated candidates trans-
formed programs show a higher rate of misprediction than
Sivand-DD.(cid:15)

(cid:12)

Observation 4: The adversarial programs based on
key input features trigger 10% more mispredictions
with 50% fewer candidates. The Sivand-Perses gener-
ated candidate programs are more vulnerable to adver-
sarial transformation than Sivand-DD, thus, highlight-
ing the importance of key input features in programs.

(cid:14)

(cid:13)

5 Threats to Validity and Future Plan
Evaluation. We evaluated our approach for MethodName
task with two CI models, four input types of randomly se-
lected input programs, and Top-10 most frequent method
names. Despite our best effort, it is possible that experiments
with different models, tasks, and datasets may produce differ-
ent results. Our further plan includes a detailed study with a
variety of models, tasks, and larger datasets.

Challenges. One challenge to integrating Perses is that it
loads the model in each reduction step while Delta Debug-
ging loads the model once at the beginning of reduction. For
a fair comparison between them, we only consider the pro-
gram reduction time and ignore the model loading time. We
are working on optimizing the model loading time for Perses.
Another challenge to integrating Delta Debugging, when
there are multiple subsets that hold the same target criteria,
is that Delta Debugging sometimes gets stuck at that point.
To keep the reduction process working, we temporarily used
a timer to stop the current step and jump to the next step.

6 Conclusion

In this paper, we apply the syntax-guided program reduction
technique, Sivand-Perses, for reducing an input program
while preserving the same prediction of the CI model. The
goal is to improve the transparency of models by extract-
ing key input features from reduced programs. We evaluate
Sivand-Perses on two popular CI models across four types
of input programs for the method name prediction task. Our
results suggest that the syntax-guided program reduction
technique (Sivand-Perses) significantly outperforms the
syntax-unaware program reduction technique (Sivand-DD)
in reducing different input programs. Moreover, we extract
key input features that CI models learn for a target label,
by reducing some input programs of that label. The result
shows that Sivand-Perses keeps more label-specific key in-
put features in its syntax-guided reduced programs than in
Sivand-DD’s syntax-unaware reduced programs. We also
observe that different program reduction techniques may
provide additional explanations for a better understanding
of a specific prediction.

Syntax-Guided Program Reduction for Understanding Neural Code Intelligence Models

MAPS ’22, June 13, 2022, San Diego, CA, USA

7 Data Availability Statement

The proposed prediction-preserving program reduction
framework for CI models and the corresponding reduced
data using Perses and DD algorithms that support the
findings of this study are openly available via Zenodo at
10.5281/zenodo.6630188 [19].

8 Broader Impact

In this section, we discuss the potential misuses and negative
impacts of our approach. Our work focuses on improving
the transparency of neural code intelligence models by re-
ducing input programs with a syntax-guided technique and
extracting key features that models learn from input pro-
grams. Therefore, in the negative scenario, attackers can try
to force a model to make mispredictions by applying the key
targeted adversarial attack (section 4.4), thus may hamper
the wider adoption of the model. However, our work also
highlights multiple explanations for a prediction (section 4.3),
which can motivate researchers to interpret and improve the
robustness of a model against those malicious activities.

References

[1] Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in
Machine Learning Models of Code. In Proceedings of the ACM SIGPLAN
International Symposium on New Ideas, New Paradigms, and Reflections
on Programming and Software (Athens, Greece) (Onward! 2019). ACM
New York, NY, USA, 143–153. https://doi.org/10.1145/3359591.3359735
[2] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton.
2014. Learning Natural Coding Conventions. In Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering (Hong Kong, China) (FSE 2014). Association for Computing
Machinery, New York, NY, USA, 281–293. https://doi.org/10.1145/
2635868.2635883

[3] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton.
2015. Suggesting Accurate Method and Class Names. In Proceedings of
the 10th Joint Meeting on Foundations of Software Engineering (Bergamo,
Italy) (ESEC/FSE 2015). Association for Computing Machinery, New
York, NY, USA, 38–49. https://doi.org/10.1145/2786805.2786849
[4] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles
Sutton. 2018. A Survey of Machine Learning for Big Code and
Naturalness. In ACM Computing Surveys, Vol. 51. Association for
Computing Machinery, New York, NY, USA, Article 81, 37 pages.
https://doi.org/10.1145/3212695

[5] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi.
2018. Learning to Represent Programs with Graphs. In International
Conference on Learning Representations (Vancouver, BC, Canada) (ICLR
2018). OpenReview.net, Open Access. https://openreview.net/forum?
id=BJOFETxR-

[6] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. 2016. A
Convolutional Attention Network for Extreme Summarization of
Source Code. In Proceedings of the 33nd International Conference on
Machine Learning (New York, NY, USA) (ICML 2016). Proceedings
of Machine Learning Research (PMLR), Open Access, 2091–2100.
http://proceedings.mlr.press/v48/allamanis16.html

[7] Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq: Generat-
ing Sequences from Structured Representations of Code. In Interna-
tional Conference on Learning Representations (New Orleans, Louisiana,
United States) (ICLR 2019). OpenReview.net, Open Access. https:
//openreview.net/forum?id=H1gKYo09tX

[8] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019.
Code2vec: Learning Distributed Representations of Code. In Pro-
ceedings of the ACM on Programming Languages (Cascais, Portugal)
(PACMPL 2019, Vol. 3). Association for Computing Machinery, New
York, NY, USA, 40:1–40:29. https://doi.org/10.1145/3290353

[9] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2019. AutoFocus: Inter-
preting Attention-Based Neural Networks by Code Perturbation. In
Proceedings of the 34th IEEE/ACM International Conference on Auto-
mated Software Engineering (San Diego, CA, USA) (ASE 2019). IEEE
Press, New York, NY, USA, 38–41. https://doi.org/10.1109/ASE.2019.
00014

[10] Zimin Chen and Martin Monperrus. 2019. A Literature Study of
Embeddings on Source Code. arXiv:1904.03061 [cs.LG] https://arxiv.
org/abs/1904.03061

[11] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020.
Embedding Java Classes with Code2vec: Improvements from Variable
Obfuscation. In Proceedings of the 17th International Conference on
Mining Software Repositories (Seoul, Republic of Korea) (MSR 2020).
Association for Computing Machinery, New York, NY, USA, 243–253.
https://doi.org/10.1145/3379597.3387445

[12] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis
Allamanis. 2018. Deep Learning Type Inference. In Proceedings of the
26th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (Lake Buena
Vista, FL, USA) (ESEC/FSE 2018). Association for Computing Machinery,
New York, NY, USA, 152–162. https://doi.org/10.1145/3236024.3236051
[13] Hong Jin Kang, Tegawendé F. Bissyandé, and David Lo. 2019. Assessing
the Generalizability of Code2vec Token Embeddings. In Proceedings
of the 34th IEEE/ACM International Conference on Automated Software
Engineering (San Diego, CA, USA) (ASE 2019). IEEE Press, New York,
NY, USA, 1–12. https://doi.org/10.1109/ASE.2019.00011

[14] Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2020. Evalua-
tion of Generalizability of Neural Program Analyzers under Semantic-
Preserving Transformations. arXiv:2004.07313 [cs.SE] https://arxiv.
org/abs/2004.07313

[15] Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2021.
Code2Snapshot: Using Code Snapshots for Learning Representations
of Source Code. arXiv:2111.01097 [cs.SE] https://arxiv.org/abs/2111.
01097

[16] Md Rafiqul Islam Rabin, Nghi D.Q. Bui, Ke Wang, Yijun Yu, Lingxiao
Jiang, and Mohammad Amin Alipour. 2021. On the generalizability of
Neural Program Models with respect to semantic-preserving program
transformations. In Information and Software Technology, Vol. 135.
Elsevier, Amsterdam, Netherlands, 106552. https://doi.org/10.1016/j.
infsof.2021.106552

[17] Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin
Alipour. 2021. Artifact for Article (SIVAND): Understanding Neural
Code Intelligence Through Program Simplification. ACM Digital Li-
brary, ESEC/FSE (2021). https://doi.org/10.1145/3462296

[18] Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin
Alipour. 2021. Understanding Neural Code Intelligence through Pro-
gram Simplification. In Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (Athens, Greece) (ESEC/FSE 2021).
Association for Computing Machinery, New York, NY, USA, 441–452.
https://doi.org/10.1145/3468264.3468539

[19] Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour.
2022. Artifact for Article (CI-DD-Perses): Syntax-Guided Program
Reduction for Understanding Neural Code Intelligence Models. Zenodo,
MAPS (2022). https://doi.org/10.5281/zenodo.6630188

[20] Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour,
and Vincent J. Hellendoorn. 2021. Memorization and Generalization
in Neural Code Intelligence Models. arXiv:2106.08704 [cs.LG] https:
//arxiv.org/abs/2106.08704

MAPS ’22, June 13, 2022, San Diego, CA, USA

Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour

[21] Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, and
Mohammad Amin Alipour. 2020. Towards Demystifying Dimensions
of Source Code Embeddings. In Proceedings of the 1st ACM SIGSOFT
International Workshop on Representation Learning for Software En-
gineering and Program Languages (Virtual, USA) (RL+SE&PL 2020).
Association for Computing Machinery, New York, NY, USA, 29–38.
https://doi.org/10.1145/3416506.3423580

[22] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour.
2019. Testing Neural Program Analyzers. 34th IEEE/ACM International
Conference on Automated Software Engineering (Late Breaking Results-
Track) (2019). https://arxiv.org/abs/1908.10711

[23] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and
Federica Sarro. 2021. A Survey on Machine Learning Techniques for
Source Code Analysis. arXiv:2110.09610 [cs.SE] https://arxiv.org/abs/
2110.09610

[24] Chengnian Sun, Yuanbo Li, Qirun Zhang, Tianxiao Gu, and Zhendong
Su. 2018. Perses: Syntax-Guided Program Reduction. In Proceedings of
the 40th International Conference on Software Engineering (Gothenburg,
Sweden) (ICSE 2018). Association for Computing Machinery, New York,
NY, USA, 361–371. https://doi.org/10.1145/3180155.3180236

[25] Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim A. Laredo, and Alessan-
dro Morari. 2021. Probing Model Signal-Awareness via Prediction-
Preserving Input Minimization. In Proceedings of the 29th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA,
945–955. https://doi.org/10.1145/3468264.3468545

[26] Yu Wang, Fengjuan Gao, and Linzhang Wang. 2021. Demystifying
code summarization models. arXiv:2102.04625 [cs.LG] https://arxiv.
org/abs/2102.04625

[27] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial Exam-
ples for Models of Code. In Proceedings of the ACM on Program-
ming Languages (Virtual, USA) (PACMPL 2020, Vol. 4). Association
for Computing Machinery, New York, NY, USA, 162:1–162:30. https:
//doi.org/10.1145/3428230

[28] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and Isolating
Failure-Inducing Input. In IEEE Transactions on Software Engineering
(Virtual), Vol. 28. IEEE Press, New York, NY, USA, 183–200. https:
//doi.org/10.1109/32.988498

