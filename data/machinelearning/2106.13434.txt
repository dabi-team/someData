1
2
0
2

g
u
A
3

]

C
O
.
h
t
a
m

[

2
v
4
3
4
3
1
.
6
0
1
2
:
v
i
X
r
a

Binary Matrix Factorisation and Completion via
Integer Programming

Oktay G¨unl¨uk
Cornell University, ong5@cornell.edu
Raphael A. Hauser, R´eka ´A. Kov´acs
University of Oxford, The Alan Turing Institute, hauser@maths.ox.ac.uk, reka.kovacs@maths.ox.ac.uk

Binary matrix factorisation is an essential tool for identifying discrete patterns in binary data. In this paper
we consider the rank-k binary matrix factorisation problem (k-BMF) under Boolean arithmetic: we are given
an n × m binary matrix X with possibly missing entries and need to ﬁnd two binary matrices A and B of
dimension n × k and k × m respectively, which minimise the distance between X and the Boolean product of
A and B in the squared Frobenius distance. We present a compact and two exponential size integer programs
(IPs) for k-BMF and show that the compact IP has a weak LP relaxation, while the exponential size IPs
have a stronger equivalent LP relaxation. We introduce a new objective function, which diﬀers from the
traditional squared Frobenius objective in attributing a weight to zero entries of the input matrix that is
proportional to the number of times the zero is erroneously covered in a rank-k factorisation. For one of
the exponential size IPs we describe a computational approach based on column generation. Experimental
results on synthetic and real word datasets suggest that our integer programming approach is competitive
against available methods for k-BMF and provides accurate low-error factorisations.

Key words : binary matrix factorisation, binary matrix completion, column generation, integer

programming

MSC2000 subject classiﬁcation : 90C10
OR/MS subject classiﬁcation : Integer Programming
History :

1. Introduction. For a given binary matrix X ∈ {0, 1}n×m and a ﬁxed positive integer k,
the rank-k binary matrix factorisation problem (k-BMF) is concerned with ﬁnding two matrices
A ∈ {0, 1}n×k, B ∈ {0, 1}k×m such that the product of A and B is a binary matrix closest to X in
the squared Frobenius norm. One can deﬁne diﬀerent variants of this problem depending on the
underlying arithmetic used when computing the product of the matrices. In this paper we focus
on solving k-BMF under Boolean arithmetic where the product of the binary matrices A and B
is computed by (i) interpreting 0s as false and 1s as true, and (ii) using logical disjunction (∨)
in place of addition and logical conjunction (∧) in place of multiplication. Observe that Boolean
multiplication (∧) coincides with standard multiplication on binary input, hence we adopt the
notation a b in place of a ∧ b in the rest of the paper. We therefore compute the Boolean matrix
product of A and B as:

Z = A ◦ B ⇐⇒ zij =

(cid:95)

(ai(cid:96) b(cid:96)j).

(cid:96)

Note that Boolean matrix multiplication can be equivalently written as zij = min{1, (cid:80)
(cid:96) ai(cid:96)b(cid:96)j}
using standard arithmetic summation. The problem then becomes computing matrices A and B
whose Boolean product Z best approximates the input matrix X.

Our motivation for this study comes from data science applications where rows of the matrix
X correspond to data points and columns correspond to features. In these applications low-rank
matrix approximation is an essential tool for dimensionality reduction which helps understand the
data better by exposing hidden features. Many practical datasets contain categorical features which
can be represented by a binary data matrix using unary encoding. For example, consider a data

1

 
 
 
 
 
 
2

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

matrix X below (inspired by [33]), where rows correspond to patients and columns to symptoms,
xij = 1 indicating patient i presents symptom j:

X =









1 1 0
1 1 1
0 1 1

X = A ◦ B =





1 0
1 1
0 1



 ◦

(cid:21)
(cid:20)1 1 0
0 1 1

.

(1)

In this example matrix A ◦ B describes X exactly using 2 derived features where the rows of B
specify how the original features relate to the 2 derived features, and the rows of A give the derived
features of each patient. In other words, factor matrix B reveals that there are 2 underlying diseases
that cause the observed symptoms: Disease α is causing symptoms 1 and 2, and disease β is causing
symptoms 2 and 3. Matrix A reveals that patient 1 has disease α, patient 3 has β and patient 2
has both.

We note that it is also possible to use classical methods such as singular value decomposition
(SVD) [12] or non-negative matrix factorisation (NMF) [24] to obtain low-rank approximations of
X but the resulting factor matrices or their product would typically not be binary unlike BMF [32].
To demonstrate this we next give the best rank-2 SVD and NMF approximations of the matrix X
in (1), respectively:

X ≈





1.21 0.71
1.21 0.00
1.21 −0.71





(cid:20)0.00 0.71 0.50
0.71 0.00 −0.71

(cid:21)

,

X ≈





1.36 0.09
1.05 1.02
0.13 1.34





(cid:20)0.80 0.58 0.01
0.00 0.57 0.81

(cid:21)

.

(2)

Note that neither of these rank-2 approximations provide a clear interpretation. The rank-2 NMF of
X suggests that symptom 2 presents with lower intensity in both α and β, an erroneous conclusion
(caused by patient 2) that could not have been learned from data X which is of “on/oﬀ” type.

We note that in addition to healthcare applications, BMF-derived features of data have also been
shown to be interpretable in biclustering gene expression datasets [44], role based access control
[28, 29] and market basket data clustering [25].

1.1. Complexity and related work. The Boolean rank [34, 15] of a binary matrix X is
deﬁned to be the smallest integer r for which there exist binary matrices A and B such that
X = A ◦ B. In an equivalent deﬁnition, the Boolean rank of X is the minimum value of r for which
it is possible to factor X into a Boolean combination of r rank-1 binary matrices

X =

r
(cid:95)

(cid:96)=1

a(cid:96) b(cid:62)
(cid:96)

for a(cid:96) ∈ {0, 1}n, b(cid:96) ∈ {0, 1}m. Occasionally, the Boolean rank is also referred to as the rectangle
cover number, and rank-1 binary matrices a(cid:96)b(cid:62)
(cid:96) are called rectangle matrices or simply rectangles
[6].

Interpreting X as the node-node incidence matrix of a bipartite graph G(X) with n vertices
on the left and m vertices on the right, the problem of computing the Boolean rank of X is in
one-to-one correspondence with ﬁnding a minimum edge covering of G(X) by complete bipartite
subgraphs (bicliques)[34]. Since the biclique cover problem is NP-hard [35, Theorem 8.1],[10, Prob-
lem GT18], and hard to approximate [43, 4], computing the Boolean rank is hard as well. Finding
an optimal rank-k binary factorisation of X under Boolean arithmetic has a graphic interpretation
of minimizing the number of errors in an approximate covering of G(X) by k bicliques which are
allowed to overlap. In the rank-1 case the Boolean arithmetic coincides with standard arithmetic
and 1-BMF can be interpreted as computing a maximum weight biclique on the complete bipartite
graph Kn,m whose edges that are in G(X) have weight 1 and others weight −1. The maximum

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

3

edge biclique problem with edge weights in {−1, 1} is NP-hard [11], hence even the computation
of a rank-1 BMF is computationally challenging.

Due to the hardness results, the majority of methods developed for BMF rely on heuristics. The
earliest heuristic for BMF, Proximus [22, 21], computes BMF under standard arithmetic using a
recursive partitioning idea and computing 1-BMF at each step. Since Proximus, much research has
focused on computing eﬃcient and accurate methods for 1-BMF. [41] proposes an integer program
(IP) for 1-BMF and several relaxations of it, one of which leads to a 2-approximation, while [42]
provides a rounding based 2-approximation. In [3] an extension of the Proximus framework is
explored which uses the formulations from [41] to compute 1-BMF at each step. k-BMF under
Boolean arithmetic is explicitly introduced in [32, 33], along with a heuristic called ASSO, which is
based on an association rule-mining approach. ASSO is further improved in [1] into an alternating
iterative heuristics. Another approach based on an alternating style heuristic is explored in [44]
to solve a non-linear unconstrained formulation of k-BMF with penalty terms in the objective for
non-binary entries.

In [28, 29] a series of integer programs for k-BMF and exact BMF are introduced. These IPs have
exponentially many variables and constraints and require an explicit enumeration of the 2m possible
binary row vectors for factor matrix B. To tackle the exponential explosion of rows considered, a
heuristic row generation using association rule mining and subset enumeration is developed. An
exact linear IP for k-BMF with polynomially many variables and constraints is presented in our
previous work [19]. This model uses McCormick envelopes [31] to linearize the quadratic terms
coming from the matrix product. We note that both of these integer programs for k-BMF, as well
as any other element-wise models can be naturally applied in the context of rank-k binary matrix
completion by simply setting the objective coeﬃcients corresponding to missing entries to 0.

1.2. Our contribution.

In this paper, we present a comprehensive study on integer program-
ming methods for k-BMF. We examine three integer programs in detail: our compact formulation
introduced in [19], the exponential formulation of [28] and a new exponential formulation which
we introduced in a preliminary version of this paper in [20]. We prove several results about the
strength of LP-relaxations of the three formulations and their relative comparison. In addition,
we show that the new exponential formulation overcomes several limitations of earlier approaches.
In particular, it does not suﬀer from permutation symmetry and it does not rely on heuristically
guided pattern mining. Moreover, it has a stronger LP relaxation than that of [19]. On the other
hand, our new formulation has an exponential number of variables which we tackle using a column
generation approach that eﬀectively searches over this exponential space without explicit enumer-
ation, unlike the complete enumeration used for the exponential size model of [28]. In addition, we
introduce a new objective function for k-BMF under which the problem becomes computationally
easier and we explore the relationship between this new objective function and the original squared
Frobenius distance. Finally, we demonstrate that our proposed solution method is able to prove
optimality for smaller datasets, while for larger datasets it provides solutions with better accuracy
than the state-of-the-art heuristic methods. In addition, the entry-wise modelling of k-BMF in our
formulations naturally extends to handle matrices with missing entries and perform binary matrix
completion, we illustrate this way of application experimentally.

The rest of this paper is organised as follows. In Section 2 we detail the three IP formulations
for k-BMF and prove several results about their LP-relaxations. In Section 3, we introduce a new
objective function and explore its relation to the original squared Frobenius objective. In Section
4 we detail a framework based on the large scale optimisation technique of column generation for
the solution of our exponential formulation and discuss heuristics for the arising pricing problems.
Finally, in Section 5 we demonstrate the practical applicability of our approach on several artiﬁcial
and real world datasets.

4

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

2. Formulations. Given a binary matrix X ∈ {0, 1}n×m and a ﬁxed positive integer k (cid:28)
min(n, m) we wish to ﬁnd two binary matrices A ∈ {0, 1}n×k and B ∈ {0, 1}k×m so that (cid:107)X − Z(cid:107)2
F
is minimised, where Z is the product of A and B and (cid:107) · (cid:107)F denotes the Frobenius norm. Let
E = {(i, j) : xij = 1} ⊂ [n] × [m] denote the index set of nonzero entries of X where [n] := {1, . . . , n}.
Both X and Z being binary matrices, the squared Frobenius and the entry-wise (cid:96)1 norm coincide
and we may expand the objective function to get a linear expression

(cid:107)X − Z(cid:107)2

F =

n
(cid:88)

m
(cid:88)

i=1

j=1

|xij − zij| =

(cid:88)

(1 − zij) +

(cid:88)

zij.

(i,j)∈E

(i,j)(cid:54)∈E

(3)

(i,j)∈E(1 − zij) + (cid:80)

For an incomplete binary matrix X with missing entries, the above objective is slightly changed to
(cid:80)
(i,j)∈E zij where E = {(i, j) : xij = 0}, to emphasise that E ∪ E (cid:54)= [n] × [m], and
the factorisation error is only measured over known entries. In the following sections we present
three diﬀerent integer programs for k-BMF all with the above derived linear objective function.

2.1. Compact formulation. We start with a formulation that uses a polynomial number of

variables and constraints where we denote the McCormick envelope [31] of a, b ∈ [0, 1] by

M C(a, b) = {y ∈ R : 0 ≤ y, a + b − 1 ≤ y, y ≤ a, y ≤ b}.

(4)

Note that if a, b ∈ {0, 1} then M C(a, b) only contains the point ab ∈ {0, 1} corresponding to the
product of a and b. The following Compact Integer linear Program (CIP) models the entries of
matrices A, B, Z directly via binary variables ai(cid:96), b(cid:96)j and zij respectively (for i ∈ [n], (cid:96) ∈ [k], j ∈ [m])
and uses McCormick envelopes to avoid the appearance of quadratic terms that would correspond
to the constraints yi(cid:96)j = ai(cid:96)b(cid:96)j,

(CIP)

ζCIP = min
a,b,y,z

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

zij

(i,j)∈E

s.t. yi(cid:96)j ≤ zij ≤

k
(cid:88)

l=1

yilj

yi(cid:96)j ∈ M C(ai(cid:96), b(cid:96)j)
ai(cid:96), b(cid:96)j, zij ∈ {0, 1}

i ∈ [n], j ∈ [m], (cid:96) ∈ [k],

i ∈ [n], j ∈ [m], (cid:96) ∈ [k],
i ∈ [n], j ∈ [m], (cid:96) ∈ [k].

(5)

(6)

(7)
(8)

Constraints (6) encode Boolean matrix multiplication, while a simple modiﬁcation of the model
in which constraints (6) are replaced by zij = (cid:80)k
(cid:96)=1 yi(cid:96)j models k-BMF under standard arithmetic.
The McCormick envelopes in constraints (7) ensure that for ai(cid:96), b(cid:96)j ∈ {0, 1}, yi(cid:96)j are binary variables
taking the value ai(cid:96)b(cid:96)j. Due to the objective function, constraints (6) and the binary nature of
yi(cid:96)j, the binary constraints on variables zij may be relaxed to zij ∈ [0, 1] without altering optimal
solutions of the formulation.

The LP relaxation of CIP (CLP) is obtained by replacing constraints (8) by ai(cid:96), b(cid:96),j, zij ∈ [0, 1].
For k = 1, we have zij = yi1j and the feasible region of CIP is the Boolean Quadric Polytope (BQP)
over a bipartite graph [36]. The LP relaxation of BQP has half-integral vertices [36], which implies
that CLP for k = 1 has half-integral vertices as well. One can show that in this case, a simple
rounding in which fractional values of CLP are rounded down to 0 gives a 2-approximation to
1-BMF [42]. This however, does not apply for k > 1. We next show that CLP for k > 1 has an
objective function value 0.

Proposition 1. Given a binary matrix X ∈ {0, 1}n×m, CLP has optimal objective value 0 for

k > 1. Moreover, for k > 2 CLP has at least k|E| + 1 vertices with objective value 0.

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

5

Proof. For each (i, j) ∈ E let L(i,j) ⊆ [k] such that |L(i,j)| ≥ 2 and consider the point

ai(cid:96) =

yi(cid:96)j =

1
2
(cid:40) 1
2
0

i ∈ [n], (cid:96) ∈ [k],

(i, j) ∈ E, (cid:96) ∈ L(i,j)
otherwise,

b(cid:96)j =

zij =

1
2
(cid:40)
1
0

(cid:96) ∈ [k], j ∈ [m],

(i, j) ∈ E,
otherwise.

2 , 1

2 ) = [0, 1

2 implies that yi(cid:96)j ∈ M C( 1

For all (i, j) ∈ [n] × [m] and (cid:96) ∈ [k], setting ai(cid:96) = b(cid:96)j = 1
2 ] and
(cid:80)k
l=1 yilj ≥ 1 holds for all (i, j) ∈ E, hence this point gives a feasible solution to CLP with objective
value 0. For k = 2, we can only set L(i,j) = [2] for all (i, j) ∈ E, hence the above construction leads to
a single unique point. For k > 2 however, as the choice of L(i,j)’s is arbitrary, there are many feasible
points with objective value 0 of this form. As each of these points can diﬀer at only k |E| entries
corresponding to entries yi(cid:96)j for (i, j) ∈ E, (cid:96) ∈ [k], there are at most k |E| + 1 aﬃnely independent
points among them. Next we present k |E| + 1 aﬃnely independent points of this form. Since the
objective value is 0 at these points, they must lie on a face of dimension at least k |E| and this face
must have at least k |E| + 1 vertices of CLP with objective value 0. For each (i, j)∗ ∈ E and (cid:96)∗ ∈ [k],
letting L(i,j) = [k] for all (i, j) ∈ E \ {(i, j)∗} and L(i,j)∗ = [k] \ {(cid:96)∗} provides k |E| diﬀerent points of
the above form. Each such point has exactly one entry yi(cid:96)j along the indices (i, j) ∈ E, (cid:96) ∈ [k] which
is zero. Hence the matrix whose columns correspond to these k |E| points has a square submatrix
of the form 1
2 (Jk|E| − Ik|E|) corresponding to entries yi(cid:96)j for (i, j) ∈ E, (cid:96) ∈ [k], where Jt is the all
ones matrix of size t × t and It is the identity matrix of size t. Since matrix Jt − It is nonsingular,
the k |E| points are linearly independent. In addition, letting L(i,j) = [k] for all (i, j) ∈ E gives an
additional point for which yi(cid:96)j = 1
2 for all (i, j) ∈ E, (cid:96) ∈ [k], hence the corresponding part of this
point is 1
2 (Jk|E| − Ik|E|), we get the nonsingular matrix
− 1

2 1 from the columns of 1
2 Ik|E|, hence the k |E| + 1 above constructed points are aﬃnely independent. (cid:3)
The above result suggests that unless the factorisation error is 0 i.e. the input matrix is of Boolean
rank less than or equal to k, before improving the LP bound of CIP many fractional vertices need
to be cut oﬀ. Furthermore, for k > 1, any feasible rank-k factorisation A ◦ B and a permutation
matrix P ∈ {0, 1}k×k provide another feasible solution AP ◦ P (cid:62)B with the same objective value.
Hence, CIP is highly symmetric for k > 1. These properties of CIP make it unlikely to be solved to
optimality for k > 1 in a reasonable amount of time for a large matrix X, though some symmetries
may be broken by enforcing lexicographic ordering of rows of B. For small matrices however, CIP
constitutes the ﬁrst approach to get optimal solutions to k-BMF.

2 1. Now subtracting 1

2.2. Exponential formulation I. Any n × m Boolean rank-k matrix can be equivalently
written as the Boolean combination of k rank-1 binary matrices (cid:87)k
(cid:96) for some a(cid:96) ∈ {0, 1}n, b(cid:96) ∈
{0, 1}m. This suggest to directly look for k rank-1 binary matrices instead of introducing variables
for all entries of factor matrices A and B. The second integer program we detail for k-BMF relies
on this approach by considering an implicit enumeration of rank-1 binary matrices. Let R denote
the set of all rank-1 binary matrices of dimension n × m and let R(i,j) denote the subset of rank-1
matrices of R which have the (i, j)-th entry equal to 1,

(cid:96)=1 a(cid:96)b(cid:62)

R := {ab(cid:62) : a ∈ {0, 1}n, b ∈ {0, 1}m, a, b (cid:54)= 0} ⊂ {0, 1}n×m,
i ∈ [n], j ∈ [m].

R(i,j) := {ab(cid:62) ∈ R : ai = bj = 1} ⊂ R

(9)
(10)

Introducing a binary variable qr for each rank-1 matrix r in R and variables zij corresponding to
the known entries of the X, we obtain the following Master Integer linear Program (MIP),

(MIPF)

ζMIP = min
z,q

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

zij

(i,j)∈E

(11)

6

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

s.t. zij ≤

(cid:88)

(cid:88)

qr

r∈R(i,j)

qr ≤ k zij

r∈R(i,j)
(cid:88)

qr ≤ k

r∈R
zij, qr ∈ {0, 1}

(i, j) ∈ E

(i, j) ∈ E

(i, j) ∈ E ∪ E, r ∈ R

(12)

(13)

(14)

(15)

The objective, as before, measures the factorisation error in squared Frobenius norm, and subscript
F in MIPF stands for Frobenius. Constraints (12) and (13) enforce Boolean matrix multiplication:
zij takes value 1 if there is at least one active rank-1 binary matrix that covers entry (i, j), otherwise
it takes value 0. Notice, that due to the diﬀerence in sign of objective coeﬃcients for variables zij
with (i, j) ∈ E and (i, j) ∈ E it is enough to declare constraints (12) and (13) for indices (i, j) ∈ E
and (i, j) ∈ E respectively. Constraint (14) ensures that at most k rank-1 binary matrices are active
and hence we get a rank-k factorisation of X. Observe that constraints (12) together with qr being
binary imply that zij automatically takes binary values for (i, j) ∈ E, and due to the objective
function it always takes the value at its upper bound, hence zij ∈ {0, 1} may be replaced by zij ≤ 1
for all (i, j) ∈ E without altering the optimum. In contrast, zij for (i, j) ∈ E need to be explicitly
declared binary as otherwise, if there are some active rank-1 matrices (qr > 0) which cover a zero
of X (r ∈ R(i,j), (i, j) ∈ E) then variable zij corresponding to that zero takes the possibly fractional
value 1
qr. One can also consider a strong formulation of MIPF with exponentially many
k
constraints, in which constraints (13) are replaced by qr ≤ zij for all r ∈ R(i,j) and (i, j) ∈ E.

r∈R(i,j)

(cid:80)

The LP relaxation of MIPF (MLPF) is obtained by replacing the integrality constraints by
zij, qr ∈ [0, 1]. Unlike CLP, the optimal objective value of MLPF (ζMLP) is not always zero. By
comparing the rank of the factorisation, k to the isolation number of the input matrix X we can
deduce when MLPF will take non-zero objective value. We next give an extension of the deﬁnition
of isolation number for binary matrices presented in [34, Section 2.3].

Definition 1. Let X be a binary matrix with possibly missing entries. A set S ⊆ E = {(i, j) :
xij = 1} is said to be an isolated set of ones if whenever (i1, j1), (i2, j2) are two distinct members
of S then (a) i1 (cid:54)= i2, j1 (cid:54)= j2 and (b) (i1, j2) ∈ E or (i2, j1) ∈ E or both. The size of the largest
cardinality isolated set of ones of X is denoted by i(X) and is called the isolation number of X.
From the deﬁnition it follows that members of an isolated set of ones cannot be covered by a
common rank-1 submatrix, and hence the isolation number provides a lower bound on the Boolean
rank. The following result shows that MLPF must have non-zero objective value whenever k, the
rank of the factorisation, is chosen so that it is strictly smaller than the isolation number.

Proposition 2. Let X have isolation number i(X) > k, then ζMLP ≥ 1

k (i(X) − k).

Proof. Let S be an isolated set of ones of X of cardinality i(X). We will establish a feasible

solution to the dual of MLPF (MDPF) with objective value 1

k (i(X) − k) implying the result.

Let us apply a change of variables ξij = 1 − zij for (i, j) ∈ E for the ease of avoiding the constant
term in the objective function of MLPF. Then the bound constraints of MLPF can be written as
ξij ≥ 0 for (i, j) ∈ E, zij ≥ 0 for (i, j) ∈ E and qr ≥ 0, r ∈ R as the objective function is minimising
both ξij and zij and we have the cardinality constrains on qr. Associating dual variables pij ≥ 0
(i, j) ∈ E with constraints (cid:80)
qr + ξij ≥ 1, sij ≥ 0 (i, j) ∈ E with constraints (13) and µ ≥ 0
with constraint (14), the Master Dual Program (MDPF) of MLPF is

r∈Ri,j

(MDPF) ζMDP = max
p,s,µ

(cid:88)

(i,j)∈E

pij − k µ

(16)

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

7

s.t.

(cid:88)

pij −

(cid:88)

sij ≤ µ

R ∈ R

(17)

(i,j)∈E∩supp(R)

(i,j)∈E∩supp(R)
0 ≤ pij ≤ 1
1
k

0 ≤ sij ≤
0 ≤ µ,

(i, j) ∈ E

(i, j) ∈ E

(18)

(19)

(20)

where supp(R) = {(i, j) : rij = 1}.

Let sij = 1

k for (i, j) ∈ E and let pij = 1

k for (i, j) ∈ S and pij = 0 for all other (i, j) ∈ E \ S. The
bound constraints on pij and sij are satisﬁed then. It remains to choose µ ≥ 0 such that we satisfy
constraint (17) for all rank-1 binary matrices R ∈ R. Let R ∈ R be a submatrix of X, so we have
|E ∩ supp(R)| = 0. Then by the deﬁnition of isolated sets of ones, R can contain at most one element
from S and hence we have | supp(R) ∩ S| ≤ 1. This tells us that for any µ ≥ 1
k , constraint (17) is
satisﬁed for all R ∈ R that is a submatrix of X. Now let R ∈ R be a rank-1 binary matrix which
covers at least one zero entry of X. Then R may contain more than one element from S. However,
(cid:1)-many
if it contains more than one element from S then it must also contain at least (cid:0)| supp(R)∩S|
zeros as for any two distinct elements (i1, j1), (i2, j2) in S we have (i1, j2) ∈ E or (i2, j1) ∈ E by the
deﬁnition of isolated set of ones. Hence, for all R ∈ R such that |E ∩ supp(R)| > 0, constraint (17)
satisﬁes

2

1
k

|S ∩ supp(R)| −

1
k

|E ∩ supp(R)| ≤

1
k

|S ∩ supp(R)| −

(cid:18)|S ∩ supp(R)|
2

(cid:19)

1
k

≤

1
k

.

(21)

Thus we can set µ = 1
non-zero bound on MLPF for all k < i(X). (cid:3)

k to get the objective value 1

k (i(X) − k) ≤ ζMDP = ζMLP, which provides a

The following example shows that we cannot strengthen Proposition 2 by replacing the condition

k < i(X) with the requirement that k has to be strictly smaller than the Boolean rank of X.

Example 1. Let X = J4 − I4, where J4 is the 4 × 4 matrix of all 1s and I4 is the 4 × 4 identity
matrix. One can verify that the Boolean rank of X is 4 and its isolation number is 3. For k = 3,
the optimal objective value of MLPF is 0 which is attained by a fractional solution in which the
following 6 rank-1 binary matrices are active with weight 1
2 .







q1 = 1
2
0 0 0 0
1 0 1 0
0 0 0 0
1 0 1 0













q2 = 1
2
0 1 1 0
0 0 0 0
0 0 0 0
0 1 1 0













q3 = 1
2
0 1 0 1
0 0 0 0
0 1 0 1
0 0 0 0













q4 = 1
2
0 0 0 0
1 0 0 1
1 0 0 1
0 0 0 0













q5 = 1
2
0 0 1 1
0 0 1 1
0 0 0 0
0 0 0 0













q6 = 1
2
0 0 0 0
0 0 0 0
1 1 0 0
1 1 0 0







2.3. Exponential formulation II. For t ∈ [2m − 1] let βt ∈ {0, 1}m be the vector denoting
the binary encoding of t and note that these vectors give a complete enumeration of all non-zero
binary vectors of size m. Let βtj denote the j-th entry of βt. In [28], the authors present the
following Exponential size Integer linear Program (EIP) formulation using a separate indicator
variable dt for each one of these exponentially many binary vectors βt,

(EIP)

ζEIP = min
α,z,d

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

zij

(i,j)∈E

2m−1
(cid:88)

t=1

αitβtj

αitβtj ≤ kzij

s.t. zij ≤

2m−1
(cid:88)

t=1

(22)

(i, j) ∈ E,

(23)

(i, j) ∈ E,

(24)

8

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

2m−1
(cid:88)

dt ≤ k

t=1
αit ≤ dt
zij, dt, αit ∈ {0, 1}

i ∈ [n], t ∈ [2m − 1],
(i, j) ∈ E ∪ E, t ∈ [2m − 1].

(25)

(26)
(27)

The above formulation has an exponential number of variables and constraints but it is an integer
linear program as βtj are input parameters to the model. Let ELP be the LP relaxation of EIP.
Observe that due to the objective function the bound constraints in ELP may be simpliﬁed to
zij, αit, dt ≥ 0 for all i, j, t and zij ≤ 1 for (i, j) ∈ E without changing the optimum. To solve EIP
or ELP explicitly, one needs to enumerate all binary vectors βt, which is possible only up to
a very limited size. To the best of our knowledge, no method is available that avoids explicit
enumeration and can guarantee the optimal solution of EIP. Previous attempts at computing a
rank-k factorisation via EIP all relied on working with only a small heuristically chosen subset of
vectors βt [28, 29]. However, if there was an eﬃcient method to solve ELP, the following result
shows it to be as strong as the LP relaxation of MIPF.

Proposition 3. The optimal objective values of ELP and MLPF are equal.

Proof. Note that due to constraints (12) and (13) in MLPF and constraints (23) and (24) in
ELP, it suﬃces to show that for any feasible solution αit, dt of ELP one can build a feasible solution
qr of MLPF for which (cid:80)2m−1

qr, and vice-versa.

t=1 αitβtj = (cid:80)

First consider a feasible solution αt ∈ Rn, dt ∈ R (for t ∈ [2m − 1]) to ELP and note that by
constraint (26) we have 0 ≤ αit ≤ dt for all i ∈ [n] and t ∈ [2m − 1]. We can therefore express each
αt as a convex combination of binary vectors in {0, 1}n scaled by dt,

r∈R(i,j)

αt = dt

2n−1
(cid:88)

s=1

λs,t as as ∈ {0, 1}n \ {0},

2n−1
(cid:88)

s=1

λs,t ≤ 1, λs,t ≥ 0,

s ∈ [2n − 1]

(28)

where as denotes the binary encoding of s. Note that we do not require λs,t’s to add up to 1 as we
exclude the zero vector. We can therefore rewrite the solution of ELP as follows

2m−1
(cid:88)

t=1

αtβ(cid:62)

t =

2m−1
(cid:88)

2n−1
(cid:88)

t=1

s=1

dt λs,t asβ(cid:62)

t =

2m−1
(cid:88)

2n−1
(cid:88)

t=1

s=1

qs,tasβ(cid:62)

t where qs,t := dt λs,t.

(29)

Now it is easy to see that asβ(cid:62)
we get (cid:80)2n−1

(cid:80)2m−1
t=1

s=1

t ∈ R and since (cid:80)2m−1

t=1 dt ≤ k holds in any feasible solution to ELP,

qs,t ≤ k, which shows that qs,t is feasible for MLPF.

qs,tasβ(cid:62)
t

(cid:80)2m−1
t=1

The construction works backwards as well, as any feasible solution to MLPF can be written as
for some rank-1 binary matrices asβ(cid:62)
t ∈ R and corresponding variables
s=1 qs,t as and dt := maxi∈[n] αit to satisfy αit ≤ dt. Then since we started
t=1 dt ≤ k is satisﬁed

(cid:80)2n−1
s=1
qs,t ≥ 0. Now let αt := (cid:80)2n−1
from a feasible solution to MLPF, we have (cid:80)2n−1
too. (cid:3)

qs,t ≤ k and hence (cid:80)2m−1

(cid:80)2m−1
t=1

s=1

3. Working under a new objective In the previous section, we presented formulations for
k-BMF which measured the factorisation error in the squared Frobenius norm, which coincides
with the entry-wise (cid:96)1 norm as showed in Equation (3). In this section, we explore another objective
function which introduces an asymmetry between how false negatives and false positives are treated.
Whenever a 0 entry is erroneously covered in a rank-k factorisation, it may be covered by up to k
rank-1 binary matrices. Our new objective function attributes an error term to each 0 entry which

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

9

is proportional to the number of rank-1 matrices covering that entry. As previously, by denoting
Z = A ◦ B a rank-k factorisation of X, the new objective function is

ζ(ρ) =

(cid:88)

(1 − zij) + ρ

(cid:88)

k
(cid:88)

ai(cid:96)b(cid:96)j.

(i,j)∈E

(i,j)∈E

(cid:96)=1

(30)

(cid:80)k

Note that the constraints ai(cid:96)b(cid:96)j ≤ zij ≤ (cid:80)k
(cid:96)=1 ai(cid:96)b(cid:96)j encoding Boolean matrix multiplication imply
that 1
(cid:96)=1 ai(cid:96)b(cid:96)j. Therefore, denoting the original squared Frobenius norm
k
objective function in Equation (3) by ζF , for any X and rank-k factorisation Z of X the following
relationship holds between ζF and ζ(1), ζ( 1

(cid:96)=1 ai(cid:96)b(cid:96)j ≤ zij ≤ (cid:80)k

k ),

ζF ≤ ζ(1) ≤

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

(i,j)∈E

k zij ≤ k ζF

and

1
k

ζF ≤ ζ(

1
k

) ≤ ζF .

(31)

We next show that this new objective function ζ(ρ) with ρ = 1 can overestimate the original
objective ζF by a factor of k. But ﬁrst, we need a technical result which shows that whenever the
input matrix X contains repeated rows or columns we may assume that an optimal factorisation
exists which has the same row-column repetition pattern.

Lemma 1 (Preprocessing). Let X contain some duplicate rows and columns. Then there
exists an optimal rank-k binary matrix factorisation of X under objective ζF (or ζ(ρ)) whose rows
and columns corresponding to identical copies in X are identical.

Proof. Since the transpose of an optimal rank-k factorisation is optimal for X (cid:62), it suﬃces to
consider the rows of X. Furthermore, it suﬃces to consider only one set of repeated rows of X,
so let I ⊆ [n] be the index set of a set of identical rows of X. We then need to show that there
exists an optimal rank-k factorisation whose rows indexed by I are identical. Let Z = A ◦ B be an
optimal rank-k factorisation of X under objective ζF . For all i1, i2 ∈ I we must have
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(1 − zij) +

zij =

(1 − zij) +

zij

(32)

j:(i1,j)∈E

j:(i1,j)∈E

j:(i2,j)∈E

j:(i2,j)∈E

as otherwise replacing Ai,: for each i ∈ I with row Ai∗,: where i∗ ∈ I is a row index for which the
above sum is minimised leads to a smaller error factorisation. Then since (32) holds, replacing Ai,:
for each i ∈ I with row Ai∗,: for any i∗ ∈ I leads to an optimal solution of the desired property. Sim-
ilarly, if Z is an optimal factorisation under objective ζ(ρ), then for all i1, i2 ∈ I the corresponding
objective terms must equal and hence an optimal solution of the desired property exists. (cid:3)

This result implies that whenever the input matrix X contains repeated rows or columns we
may solve the following problem on a smaller matrix instead. Let X (cid:48) ∈ {0, 1}n(cid:48)×m(cid:48) be the binary
matrix obtained from X by replacing each duplicate row and column by a single representative
and let r ∈ Zn(cid:48)
+ be the counts of each unique row and column of X (cid:48) in X respectively.
Let E(cid:48) and E(cid:48) denote the non-zero and zero entry index sets of X (cid:48) respectively. By Lemma 1 an
optimal rank-k factorisation Z (cid:48) = A(cid:48) ◦ B(cid:48) of X (cid:48) under the updated objective function

+ and c ∈ Zm(cid:48)

(cid:88)

ζ (cid:48)
F :=

(i,j)∈E(cid:48)

ri cj (1 − z(cid:48)

ij) +

(cid:88)

ri cj z(cid:48)
ij

(i,j)∈E(cid:48)

(33)

(or ζ (cid:48)(ρ) := (cid:80)
sation of X under the original objective function ζF (or ζ(ρ)).

(i,j)∈E(cid:48) ri cj (1 − z(cid:48)

ij) + ρ (cid:80)

(i,j)∈E(cid:48) ri cj

(cid:96)=1 a(cid:48)

i(cid:96)b(cid:48)

(cid:80)k

(cid:96)j) leads to an optimal rank-k factori-

Proposition 4. For each positive integer k there exists a matrix X(k) for which the optimal

rank-k binary matrix factorisations under objectives ζF and ζ(1) satisfy ζ(1) = k ζF .

10

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

Proof. The idea behind the proof is to consider a matrix Z(k) of exact Boolean rank-k in which
all the k rank-1 components (rectangles) overlap at a unique middle entry and then replace this
entry with a 0 to obtain X(k). Now X(k) and Z(k) are exactly at distance 1 in the squared
Frobenius norm and hence Z(k) is a rank-k factorisation of X(k) with objective value ζF = 1. On
the other hand, since exactly k rectangles cover the entry at which X(k) and Z(k) diﬀer, if Z(k)
is taken as a rank-k factorisation of X(k) under objective ζ(1) it incurs an error of size k. Figure
1 shows the idea how to build such a X(k) for k = 2, 4, 6. Each colour corresponds to a rank-1
component and white areas correspond to 0s. We ﬁrst consider the case when k is even. For k = 2

Figure 1. Example matrices for which ζ(1) = k ζF

(a) k = 2

(b) k = 4

(c) k = 6

take the symmetric matrix X(2) as in Equation (34) which corresponds to Figure 1a. Since X(2)
has repeated rows and columns, according to Lemma 1 we may simplify the problem by replacing
X(2) by X (cid:48)(2) and recording a weight vector for the rows and columns which indicate how many
times each row and column is repeated. This weight vector is then used to update each entry in
the objective function with the corresponding weight. For X(2) the row and column weight vectors
coincide as X(2) is symmetric and we denote it by w(2).



(cid:62)

X(2) =

























1 1 0
1 1 0
1 1 0
1 0 1
0 1 1
0 1 1
0 1 1

◦













1 0 0
1 0 0
1 0 0
0 1 0
0 0 1
0 0 1
0 0 1











=













1 1 1 1 0 0 0
1 1 1 1 0 0 0
1 1 1 1 0 0 0
1 1 1 0 1 1 1
0 0 0 1 1 1 1
0 0 0 1 1 1 1
0 0 0 1 1 1 1













⇒ X (cid:48)(2) =







1 1 0
1 0 1
0 1 1

 with w(2) =



3
1


3

(34)

The Boolean rank of X(2) is 3, which one can conﬁrm by looking at a size 3 isolated set of ones
(shadowed entries) and an exact rank-3 factorisation shown in Equation (34). Let Z(2) be obtained
from X(2) by replacing the 0 at entry (4, 4) by a 1. Z(2) clearly has Boolean rank 2, hence it is
a feasible rank-2 factorisation of X(2). Under objective ζF Z(2) incurs an error of size 1, which is
optimal as ζF ≥ 1 by X(2) being of Boolean rank-3. On the other hand, under objective ζ(1) Z(2)
has objective value 2 as the middle entry is covered twice. To see that Z(2) is optimal under ζ(1)
observe that every entry in X (cid:48)(2) apart from the middle entry has weight strictly greater than 2.
Hence not covering a 1 of X (cid:48)(2) or covering a 0 diﬀerent from the middle entry incurs an error
strictly greater than 2.

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

11

For k > 2 even let us give a recipe to construct a symmetric matrix X (cid:48)(k) and corresponding
2 − 1 and let the following (4t + 3) × (4t + 3) matrix be X (cid:48)(k), where
1 0 ])

weight vector w(k). Let t = k
It is the identity matrix of size t × t, ˜It is the reverted identity matrix of size t × t (so ˜I2 = [ 0 1
and Jt is the all ones matrix of size t × t,














X (cid:48)(k) =

˜It

It

1t
1 1(cid:62)
t 1
1t Jt 1t

t 0 1(cid:62)

1(cid:62)
t 1 1(cid:62)
˜It

1t Jt 1t
1 1(cid:62)
t 1

˜It
t 1 1(cid:62)

t

˜It 1t

It














, w(k) =













(k + 1)1t
(k + 1)
(k + 1) 1t
1
(k + 1) 1t
(k + 1)
(k + 1)1t













, A(cid:48)(k) =



It

1
1t

˜It
t 1 1 1(cid:62)
1(cid:62)
˜It
1t
1

t











It













.

X (cid:48)(k) has isolation number i(X (cid:48)(k)) ≥ 2t + 3 = k + 1 (indicated by the shadowed entries), so no
rank-k factorisation can have zero error. Let Z (cid:48)(k) be obtained from X (cid:48)(k) by replacing the middle
0 by a 1 and let its weight vector be the same as of X (cid:48)(k). The Boolean rank of Z (cid:48)(k) is then at most
k as Z (cid:48)(k) = A(cid:48)(k) ◦ A(cid:48)(cid:62)(k) is an exact factorisation and A(cid:48)(k) is of dimension (4t + 3) × k. This
factorisation is illustrated in Figure 1 for k = 4, 6. Therefore Z (cid:48)(k) is a feasible rank-k factorisation
of X (cid:48)(k). Now Z (cid:48)(k) under objective function ζF has error 1 and hence it is optimal. In contrast,
Z (cid:48)(k) evaluated under objective ζ(1) has error k as the middle 0 is covered k times and it has
weight 1. To see that Z (cid:48)(k) is optimal under ζ(1) as well, note that all entries of X (cid:48)(k) apart from
the middle 0 have weight strictly greater than k. Therefore, any other rank-k factorisation which
does not cover a 1 or covers a 0 which is not the middle 0, incurs an error strictly greater than k,
and hence Z (cid:48)(k) is optimal under objective ζ(1) with value k · ζF .

For k = 1, all 1-BMFs satisfy ζF = ζ(1) by deﬁnition. For k > 1 odd, we can obtain X (cid:48)(k) and
w(k) from X (cid:48)(k + 1) and w(k + 1) by removing the ﬁrst row and column of X (cid:48)(k + 1) and the
corresponding ﬁrst entry of w(k + 1). For X (cid:48)(k) then, the same reasoning holds as for k even. (cid:3)
While Proposition 4 shows that ζ(1) can be k times larger than the Frobenius norm objective
ζF , the matrices in the proof are quite artiﬁcial, and in practice we observe that not many zeros
are covered by more than a few rank-1 matrices. Therefore it is worth considering the previously
introduced formulations for k-BMF with the new objective ζ(ρ).

Let us denote a modiﬁcation of formulation MIPF with the new objective function ζ(ρ) as MIP(ρ)

and use the transformation ξij = 1 − zij for (i, j) ∈ E to get

(MIP(ρ)) ζMIP(ρ) = min
ξ,q

s.t.

(cid:88)

ξij + ρ

(cid:88)

(cid:88)

qr

(i,j)∈E
(cid:88)

(i,j)∈E
qr + ξij ≥ 1

r∈R(i,j)

r∈R(i,j)
(cid:88)

qr ≤ k

r∈R
ξij ≥ 0, qr ∈ {0, 1}

(35)

(i, j) ∈ E

(36)

(i, j) ∈ E, r ∈ R.

(37)

(38)

One of the imminent advantages of using objective ζ(ρ) is that we need only declare variables for
entries (i, j) ∈ E and can consequently delete the weak constraints (13) from the formulation. The
LP relaxation of MIP(ρ) (MLP(ρ)) is obtained by giving up on the integrality constraints on qr
and observing that without loss of generality we can simply write qr ≥ for all r ∈ R. We next show
that the optimal solutions of the LP relaxation of MIPF and MLP(ρ) with ρ = 1

k coincide.

Proposition 5. The optimal solutions of the LP relaxations MLPF and MLP( 1

k ) coincide.

12

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

(cid:80)

r∈R(i,j)

value 1
k
second terms in the objective function (11) of MIPF and (35) of MLP( 1

Proof. It suﬃces to observe that as MLPF is a minimisation problem, each zij (i, j) ∈ E takes the
qr in any optimal solution to MLPF due to constraint (13). This implies that the
k ) have the same value. (cid:3)
k ) that has fewer variables and constraints than MLPF.
In addition, for all ρ > 0, a corollary of Proposition 2 holds by looking at the dual of MLP(ρ)
(MDP(ρ)). Let us associate variables pij for (i, j) ∈ E to constraints (36) and variable µ to constraint
(37). Then the dual of MLP(ρ) is:

Therefore one may instead solve MLP( 1

(MDP(ρ)) ζMDP(ρ) = max
p,µ

s.t.

(cid:88)

pij − k µ

(39)

(i,j)∈E

(cid:88)

pij − µ ≤ ρ |E ∩ supp(R)|

R ∈ R

(40)

(i,j)∈E∩supp(R)
µ ≥ 0, pij ∈ [0, 1]

(i, j) ∈ E

(41)

where supp(R) = {(i, j) : rij = 1}.

Corollary 1. Let X have isolation number i(X) > k. Then for all ρ > 0, MLP(ρ) has objective

value at least ρ (i(X) − k).

Proof. The proof is a simple modiﬁcation of Proposition 2’s proof. The dual of MLP(ρ) (MDP(ρ))
diﬀers from MDPF by having the constant value ρ instead of dual variables sij and constraints (40)
instead of (17). Therefore setting pij = ρ for all (i, j) ∈ S and 0 otherwise (where S is a maximum
isolated set of ones of X), and µ = ρ gives the required bound of ρ (i(X) − k). (cid:3)

4. Computational approach.

It is clearly not practical to solve the master integer program
MIP(ρ) or its LP relaxation MLP(ρ) explicitly as the formulation has an exponential number of
variables. Column generation (CG) is a well-known technique to solve large LPs iteratively by
only considering the variables which have the potential to improve the objective function [2]. The
column generation procedure is initialised by solving a Restricted Master LP (RMLP) which has a
small subset of the variables of the full problem. The next step is to identify a missing variable with
negative reduced cost to be added to RMLP. To avoid considering all missing variables explicitly,
a pricing problem is formulated and solved. The solution of the pricing problem either returns a
variable with negative reduced cost and the procedure is iterated; or proves that no such variable
exists and hence the solution of RMLP is optimal for the full MLP. In this section, we detail how
CG technique can be used to solve the LP relaxation of MIP(ρ) iteratively.

Each Restricted MLP(ρ) (RMLP(ρ)) has the same number of constraints as the full MLP(ρ)
and all variables ξij for (i, j) ∈ E but it only has a small subset of variables qr for r ∈ R(cid:48) ⊂ R
where |R(cid:48)| (cid:28) |R|. Recall that each variable qr corresponds to a rank-1 binary matrix r ∈ R which
determines the coeﬃcients of qr in the constraints as well as the objective function. Hence at every
iteration of the CG procedure we either need to ﬁnd a rank-1 binary matrix for which the associated
variable has a negative reduced cost, or, prove that no such matrix exists.

4.1. The pricing problem. At the ﬁrst iteration of CG, RMLP(ρ) may be initialised with
R(cid:48) = ∅ or can be warm started by identifying a few rank-1 matrices in R using a heuristic. After
solving the RMLP(ρ) to optimality, one obtains an optimal dual solution [p∗, µ∗] to the current
RMLP(ρ). To identify a missing variable qr that has negative reduced cost, we solve the following
pricing problem (PP):

(PP) ω(µ∗, p∗) = µ∗ − max
a,b,y

(cid:88)

p∗
ijyij − ρ

(cid:88)

yij

(i,j)∈E

(i,j)∈E

s.t. yij = aibj,

ai, bj ∈ {0, 1},

i ∈ [n], j ∈ [m].

(42)

(43)

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

13

PP may be formulated as an integer linear program (IPPP) by using McCormick envelopes [31]
(see Section 2.1) to linearise the quadratic constrains to yij ∈ M C(ai, bj). The objective of PP
depends on the current dual solution [p∗, µ∗] and its optimal solution corresponds to a rank-1 binary
matrix ab(cid:62) = r ∈ R whose corresponding variable qr in MLP(ρ) has the smallest reduced cost. If
ω(µ∗, p∗) ≥ 0, then the current RMLP(ρ) does not have any missing variables with negative reduced
cost and consequently the current solution of RMLP(ρ) is optimal for MLP(ρ). If ω(µ∗, p∗) < 0, then
the variable qr associated with the rank-1 binary matrix r = ab(cid:62) is added to the next RMLP(ρ)
and the procedure is iterated. Moreover, any feasible solution to PP with a negative reduced cost
can (also) be added to the RMLP(ρ) to continue the procedure. CG terminates with a proof of
optimality if at some iteration we have ω(µ∗, p∗) ≥ 0.

4.2. Solving the master integer program. After the CG process, if the optimal solution
of MLP(ρ) is integral, then it also is optimal for MIP(ρ). However, if it is fractional, then this
solution only provides a lower bound on the optimal value of MIP(ρ). In this case we obtain an
integer feasible solution by solving a Restricted MIP(ρ) (RMIP(ρ)) over the rank-1 binary matrices
generated by the CG process applied to MLP(ρ). This integer feasible solution is optimal for
MIP(ρ) provided that the objective value of RMIP(ρ) is equal to the ceiling of the objective value
of MLP(ρ). If this is not the case, one needs to embed CG into a branch-and-bound tree [30] to
solve MIP(ρ) to optimality, which is a relatively complicated process and we do not consider it in
this paper.

4.3. Computing lower bounds. Note that even if the CG procedure is terminated prema-
turely, one can still obtain a lower bound on MLP(ρ) and therefore on MIP(ρ) by considering the
dual of MLP(ρ). Let the objective value of of the current RMLP(ρ) be

ζRMLP(ρ) =

(cid:88)

ξ∗
ij + ρ

(cid:88)

(cid:88)

(cid:88)

q∗
r =

ij − kµ∗
p∗

(i,j)∈E

(i,j)∈E

r∈R(i,j)

(i,j)∈E

(44)

ij, q∗

where [ξ∗
r ] is the optimal solution of RMLP(ρ) and [p∗, µ∗] is the corresponding optimal dual
solution which does not necessarily satisfy all of the constraints (40) for MDP(ρ). Now assume
that we solve PP to optimality and obtain a rank-1 binary matrix with a negative reduced cost,
ω(µ∗, p∗) < 0. In this case, we can construct a feasible solution [p, µ] to MDP(ρ) by setting p := p∗
and µ := µ∗ − ω(µ∗, p∗) and obtain the following bound on the optimal value ζMLP(ρ) of MLP(ρ),

ζMLP(ρ) ≥

(cid:88)

pij − k µ =

(cid:88)

(i,j)∈E

(i,j)∈E

ij − k (µ∗ − ω(µ∗, p∗)) = ζRMLP(ρ) + k ω(µ∗, p∗).
p∗

(45)

If we do not have the optimal solution to PP but have a lower bound ω(µ∗, p∗) on it, ω(µ∗, p∗)
can be replaced by ω(µ∗, p∗) in Equation (45) and the bound on MLP(ρ) still holds. Furthermore,
this lower bound on MLP(ρ) naturally provides a valid lower bound on MIP(ρ), thus giving us a
bound on the optimality gap.

4.4. Column generation for MLPF. The CG approach is described above as applied to
the LP relaxation of MIP(ρ). To apply CG to MLPF only a small modiﬁcation needs to be done.
The Restricted MLPF provides dual variables for constraints (13) which are used in the objective
of PP for coeﬃcients of yij (i, j) ∈ E.

We note that CG cannot be used to solve the LP relaxation of the strong formulation of MIPF
in which constraints (13) are replaced by exponentially many constraints qr ≤ zij for all r ∈ R(i,j)
and (i, j) ∈ E. This is due to the fact that CG could cycle and generate the same column over
and over again. For example, consider applying CG to solve the strong formulation of MLPF and

14

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

start with the rank-1 binary matrix of all 1s as the ﬁrst column associated with variable q1. The
objective value of the corresponding Restricted MLPF would be ζ (1)
RMLP = 0 + |E| for the solution
vector [ξ(1), z(1), q(1)] = [0, 1, 1] as all entries of the input matrix are covered. Adding the same
rank-1 binary matrix of all 1s in the next iteration and setting [q1, q2] = [ 1
2 ], allows us to keep
ξ(2) = 0 but reduce the value of z(2) to 1
2 |E|. Therefore,
repeatedly adding the same matrix of all 1s for t iterations, the objective function would become
ζ (t)
t |E| for the solution vector [ξ(t), z(t), q(t)] = [0, 1
RMLP = 0 + 1
t 1]. Consequently, as t → ∞ we
would have ζ (t)
RMLP → 0 and during the column generation process we repeatedly generate the same
rank-1 binary matrix.

2 1 to obtain an objective value ζ (2)

2 , 1
RMLP = 0 + 1

t 1, 1

4.5. An alternative formulation of the pricing problem. Generating rank-1 binary
matrices with negative reduced cost eﬃciently is at the heart of the CG process. For both MLP(ρ)
and MLPF, the pricing problem is a Bipartite Binary Quadratic Program (BBQP) which is NP-hard
in general [14, 37]. Hence for large X it may take too long to solve PP to optimality via formulation
IPPP at each iteration. Introducing H an n × m matrix with hij = p∗
ij ∈ [0, 1] for (i, j) ∈ E, hij = −ρ
for (i, j) ∈ E and hij = 0 for (i, j) (cid:54)∈ E ∪ E, PP can be written in standard form as

(QPPP) ω(µ∗, p∗) = µ∗ −

max
a∈{0,1}n,b∈{0,1}m

a(cid:62)Hb.

(46)

This explicit quadratic form QPPP is more intuitive for thinking about heuristics than formulation
IPPP. If a heuristic approach to PP returns a rank-1 binary matrix with negative reduced cost,
then it is valid to add this heuristic solution as a column to the next RMLP. [14] presents several
heuristics for BBQP along with a simple but powerful greedy algorithm. In Appendix A we detail
this greedy algorithm and some variants of it which we use to provide a warm start to PP at every
iteration of CG in Section 5.2.

5. Experiments. The integer programs and column generation approach introduced in the
previous sections provide a framework for computing k-BMF with dual bounds. In this section, we
present some experimental results to demonstrate the practical applicability of integer program-
ming to obtain low-error factorisations. More speciﬁcally we detail our pricing strategies during
the column generation process and present a thorough comparison of models MIPF, MIP(ρ) and
CIP on synthetic and real world datasets. Our code and data can be downloaded from [18].

5.1. Data.

If X contains rows (or columns) of all zeros, deleting these rows (or columns)
leads to an equivalent problem whose solution A and B can easily be translated to a solution for
the original problem by inserting a row of zeros to A (respectively a column of zeros to B) in the
corresponding place. In addition, if X contains duplicate rows or columns, by Lemma 1 there is
an optimal rank-k factorisation which has the same row-column repetition pattern as X. Hence
we solve the problem on a smaller matrix X (cid:48) which is obtained from X by keeping only one copy
of each row and column, and use an updated objective function in which every entry is weighted
proportional to the number of rows and columns it is contained in X.

5.1.1. Synthetic data. We build our dataset of binary matrices with prescribed sparsity and
Boolean rank as follows. To get a matrix X ∈ {0, 1}n×m with Boolean rank at most κ, ﬁrst we
randomly generate two binary matrices ˜A, ˜B of dimension n × κ and κ × m, then compute their
Boolean product to get X. This ensures X has Boolean rank at most κ. To obtain a certain sparsity
for X, we control the probability of entries of ˜A, ˜B being zero. More speciﬁcally, if we generate
˜ai(cid:96), ˜b(cid:96)j to be zero with probability p, then xij = (cid:87)κ
˜b(cid:96)j is zero with probability (1 − (1 − p)2)κ.
Hence, to obtain X with σ percent of zeros, we need to generate entries of ˜A, ˜B to be zero with
probability p = 1 −

1 − (σ/100) 1
κ .

(cid:96)=1 ˜ai(cid:96)

(cid:113)

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

15

We generate matrices as described above with m = 20 columns and κ = 10. The number of rows
(n) is set to be 20, 35 or 50. For each of the three dimensions (20 × 20, 35 × 20, 50 × 20), we generate
10 sparse matrices with 75% zeroes and 10 normal matrices with 50% zeroes, corresponding to 10
diﬀerent seed settings in the random number generation. We call this initial set of 2 · 3 · 10 matrices
the clean matrices. Next, we create a set of noisy matrices from the clean matrices by randomly
ﬂipping 5% of the entries of each matrix. The noisy matrices are not necessarily of Boolean rank at
most κ = 10, but they are at most 0.05 · n · m squared Frobenius distance away from a Boolean rank
10 matrix. Therefore, our test bed consists of 120 matrices corresponding to 2 noise level settings
(noisy or clean), 2 sparsity levels (sparse or normal ), 3 dimensions (20 × 20, 35 × 20, 50 × 20) and
10 random seeds. Applying the preprocessing steps to our synthetic dataset achieves the largest
dimension reduction on clean matrices, while the dimension of noisy matrices scarcely changes. A
table summarising the parameters used to generate our data can be found in the Appendix B.

5.1.2. Real world data. We work with eight real world categorical datasets that were down-
loaded from online repositories [8, 23]. In general if a dataset has a categorical feature C with N
discrete options vj, (j ∈ [N ]), we convert feature C into N binary features Bj (j ∈ N ) so that if
the i-th sample takes option vj for C that is (C)i = vj, then we have (Bj)i = 1 and (B(cid:96))i = 0 for
all (cid:96) (cid:54)= j ∈ [N ]. This technique of binarisation of categorical columns has been applied in [19] and
[1]. If a row i has a missing value in the column of feature C, we leave the corresponding binary
feature columns with missing values in row i. Table 1 shows a short summary of the resulting
full-binary datasets used, in-depth details on converting categorical columns into binary, missing
value treatment and feature descriptions can be found in Appendix C.

Table 1. Summary of binary real world datasets

zoo

tumor

hepatitis

heart

lymp

audio

apb

votes

n × m
# missing
%1s

101 × 17 339 × 24 155× 38 242× 22 148×44 226 × 92 105× 105 435 ×16

0
44.3

670
24.3

334
47.2

0
34.4

0
29.0

899
11.3

0
8.0

392
49.2

5.2. Testing the computational approach to exponential formulation I. Since the
eﬃciency of CG greatly depends on the speed of generating columns, let us illustrate the speed-
up gained by using heuristics to solve the pricing problem. At each iteration of CG, 30 heuristic
solutions are computed via the heuristics detailed in Appendix A in order to obtain initial feasible
solutions to PP. Under exact pricing, the best heuristic solution is used as a warm start and IPPP
is solved to optimality at each iteration using CPLEX [7]. In simple heuristic (heur ) pricing, if
the best heuristic solution to PP has negative reduced cost then it is directly added to the next
RMLP(ρ). If at some iteration, the best heuristic column does not have negative reduced cost,
CPLEX is used to solve IPPP to optimality for that iteration. The multiple heuristic (heur multi )
pricing strategy is a slight modiﬁcation of the simple heuristic strategy, in which at each iteration
all columns with negative reduced cost are added to the next RMLP(ρ).

Figure 2 indicates the diﬀerences between pricing strategies when solving MLP(1) via CG for k =
5, 10 on the zoo dataset. The primal objective value of MLP(1) (decreasing curve) and the value of
the dual bound (increasing curve) computed using the formula in Equation (45) are plotted against
time. Sharp increases in the dual bound for heuristic pricing strategies correspond to iterations in
which CPLEX was used to solve IPPP, as for the evaluation of the dual bound on MLP(1) a lower
bound on ω(µ∗, p∗) is needed which heuristic solutions do not provide. While we observe a tailing

16

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

Figure 2. Comparison of pricing strategies for solving MLP(1) on the zoo dataset

oﬀ eﬀect [30] on all three curves, both heuristic pricing strategies provide a signiﬁcant speed-up
from exact pricing, adding multiple columns at each iteration being the fastest.

In order for CG to terminate with a certiﬁcate of optimality, at least one pricing problem has
to be solved to optimality. Unfortunately for larger datasets we cannot expect this to be achieved
in a short amount of time. Therefore, we change the multiple heuristic pricing strategy to get a
pricing strategy that we use in the rest of the experiments as follows. We impose an overall ﬁxed
time limit on the CG process and use the barrier method in CPLEX as the LP solver for RMLP
at each iteration. At each iteration of CG, we add up to 2 columns with the most negative reduced
cost to the next RMLP. If at an iteration, heuristics for PP do not provide a column with negative
reduced cost and CPLEX has to be used to improve the heuristic solution, we do not solve IPPP
to optimality but abort CPLEX after 25 seconds if a column with negative reduced cost has been
found. While these modiﬁcations result in a speed-up, they reduce the chance of obtaining a strong
dual bound. In case we wish to focus more on computing a stronger dual bound on MLP, we may
continue solving IPPP via CPLEX even when a heuristic negative reduced cost solution is available.

5.2.1. MLP(1) vs MLPF.

In this section we compare the LP relaxations of MIP(1) and
MIPF. According to Proposition 5 the optimal solution of MLPF is equivalent to MLP( 1
k ) and
hence we solve MLP( 1
k ) which has fewer variables and constraints than MLPF. To solve MLP(1)
and MLP( 1
k ), we start oﬀ from 0 rank-1 binary matrices so R(cid:48) = ∅ in the ﬁrst RMLP and set a
total time limit of 600 seconds, so we either solve MLP to optimality under 600 seconds or run out
of time and compute the gap between the last RMLP and the best dual bound MDP according to
formula 100(ζRMLP − ζMDP)/ζRMLP. As MLP(1) and MLP( 1
k ) correspond to the LP relaxations of
MIP(1) and MIPF with integral objective coeﬃcients, any fractional dual bound may be rounded
up to give a valid bound on the master IP. Therefore, we stop CG whenever the ceiling of the dual
bound reaches the objective value of RMLP.

Figure 3. Time taken in seconds to solve MLP(1) and MLP( 1

k ) via CG on synthetic data

Figure 3 shows the time taken in seconds on a logarithmic scale to solve MLP(1) and MLP( 1
k ) via
CG for k = 2, 4, . . . , 10 on the synthetic matrices. Each line corresponds to the average taken over
10 instances with the same dimension, sparsity and noise level. Blue lines correspond to matrices
of dimension 20 × 20, red to 35 × 20 and green to 50 × 20. Solid lines are used for MLP(1) and

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

17

dashed for MLP( 1
k ). First, we observe that it is signiﬁcantly faster to solve both MLPs on sparse
and clean matrices as opposed to normal and noisy ones of the same dimension. Preprocessing is
more eﬀective in reducing the dimension for clean matrices in comparison to noisy ones (see Table
6 in Appendix B) which explains why noisy instances take longer. In addition, both MLP(1) and
MLP( 1
k ) have a number of variables and constraints directly proportional to non-zero entries of the
input matrix, hence a sparse input matrix requires a smaller problem to be solved. Second, we see
that k = 10 are solved somewhat faster. This can be explained by all matrices in our test bed being
generated to have Boolean rank at most 10. For a rank-10 factorisation of clean matrices without
noise we get 0 factorisation error under both models MIP(1) and MIPF and hence LP relaxation
objective value 0. For noisy matrices we observe the error to be in line with our expectation of
0.05 · n · m. We observe that in some cases it takes signiﬁcantly longer to solve MLP( 1
k ), and in all
ten instances of 50 × 20 normal -noisy matrices MLP( 1
k ) for k = 6 runs out of the time budget of
600 sec. In the experiments, we see the amount of time CG takes is directly proportional to the
number of columns generated, MLP( 1

k ) generating signiﬁcantly more columns than MLP(1).

5.2.2. Obtaining integral solutions. Once we obtain some rank-1 binary matrices (i.e.
columns) via CG applied to a master LP, we can obtain an integer feasible solution by solving
either of the master IPs over the columns available. Here we explore obtaining integer feasible
solutions by solving MIP(1) and MIPF over the columns generated by formulations MLP(1) and
MLP( 1
k ). We use CPLEX as our integer program solver and set a total time limit of 300 seconds.

Figure 4. Factorisation error in (cid:107) · (cid:107)2

F of integral solutions by MIP(1) from columns by MLP(1) and MLP( 1
k )

Figure 4 shows the factorisation error in (cid:107) · (cid:107)2

F of integer feasible solutions obtained by solving
MIP(1) over columns generated by MLP(1) and MLP( 1
k ). As previously, each line corresponds to
the average taken over 10 matrices with same dimension, sparsity and noise level. Solid lines are
used to denote where the columns used were generated by MLP(1) and dashed where by MLP( 1
k ).
Comparing the error values of the dashed and solid lines we draw a crucial observation: columns
generated by MLP(1) seem to be a better basis for obtaining low-error integer feasible solutions
than columns by MLP( 1
k ). We suspect this is the case as in the majority of rank-k factorisations
most entries are only covered by a few rank-1 binary matrices whereas MLP( 1
k ) favours rank-
1 matrices which heavily cover 0 entries of the input matrix. This is because the coeﬃcient in
MLP( 1
k ×(number of
rank-1 matrices covering (i, j)), hence it is cheaper for MLP( 1
k ) to cover a 0 by a few (less than
k) rank-1 matrices than to leave any 1s uncovered. We also conducted a set of experiments using
formulation MIPF and we see that the factorisation error when using formulation MIP(1) to obtain
the integral solutions is extremely close to that of MIPF, see Appendix D Tables 7 and 8 for the
precise diﬀerence in the factorisation error between the two master IPs.

k )’s objective function corresponding to a zero entry at position (i, j) is only 1

Figure 5 shows the time taken to solve the master IPs on columns generated by MLP(1). We
observe that MIP(1) takes notably faster to solve than MIPF and on most normal-noisy matrices
MIPF runs out of the time budget of 300 seconds. Solving both master IPs on columns by MLP( 1
k )
also shows us that while solving MIP(1) over a larger set of columns adds only a few seconds for

18

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

Figure 5. Time taken in seconds to solve MIP(1) and MIPF on columns generated by MLP(1)

most instances, MIPF runs out of the time budget of 300 secs in about half the cases, see Appendix
Table 8. These observations suggest using MIP(1) to ﬁnd integer feasible solutions in the future as
the solution quality is extremely close to that of MIPF but at a fraction of computational eﬀort.

5.3. Accuracy and speed of the IP Formulations.

In this section we computationally
compare the integer programs introduced in Section 2 and 3. CIP due to its polynomial size can
be directly given to a general purpose IP solver like CPLEX and we set a time limit of 600 seconds
on its running time. We expect solution times for CIP to grow proportional to k and density of
X according to Proposition 1. Similarly, we may try to attack the exponential formulation EIP
directly by CPLEX. Since however EIP requires the complete enumeration of 2m binary vectors for
an input matrix X of size n × m we can only solve its root LP under 600 seconds in a very few cases.
For these few cases however, we observe the objective value of ELP to agree with MLP( 1
k ), which
gives an experimental conﬁrmation of Proposition 3. In the following experiments, formulation
MIPF is used on columns generated by MLP( 1
k ), while MIP(1) on columns by MLP(1). The ﬁnal
solution of MIP(1) is evaluated under the original (cid:107) · (cid:107)2
F objective and that error is reported. As
previously, the master LPs are solved with a time limit of 600 seconds and the master IPs with an
additional time limit of 300 seconds.

Table 2 shows the factorisations error in (cid:107) · (cid:107)2

F obtained by MIPF, MIP(1) and CIP and Table
3 shows the corresponding solution times in seconds. Each row of Table 2 and 3 corresponds to
the average of 10 synthetic matrices of the same size, sparsity and noise. The lowest error results
are indicated in boldface. We observe that MIP(1) provides the lowest error factorisation in most
cases, but CIP gives the lowest error when only looking at k = 2. The signiﬁcantly higher error
values of MIPF are due to the lower quality columns generated by MLP( 1
k ) on which it is solved.
We emphasise that we do not do branch-and-price when solving MIP(1) or MIPF. Table 3 shows
that MIP(1) is the fastest in all cases, while CIP runs out of its time limit on all noisy instances
for k = 5, 10. In conclusion, CIP provides very accurate solutions for k = 2 but it is slower to solve
than MIP(1), while for larger k’s MIP(1) dominates in both accuracy and speed.

5.4. Binary matrix completion.

In this section we explore how successful our approach is
at recovering missing entries of incomplete binary matrices. We create an incomplete dataset of our
synthetic matrices by deleting 5, 10, . . . , 30% of the entries of each matrix. This way, after computing
a rank-k factorisation of the incomplete matrix, we can easily compare to the corresponding original
matrix to see how many of the entries we have recovered successfully. Since our synthetic matrices
are generated to be of Boolean rank at most 10, we cannot expect to recover all the entries by a
rank-k completion with k < 10 and thus we perform the experiments with k = 10.

Figure 6 shows the reconstruction percentage against the percentage of missing entries when
solving MIP(1) on columns generated by MLP(1) on the incomplete matrices. As previously, the
three colours correspond to dimensions of the matrices: green to 50 × 20, red to 35 × 20 and blue
to 20 × 20. We deﬁne the percentage of reconstruction as 100 ∗ (1 − (cid:107)X − A ◦ B(cid:107)2
F ) where
X is the original complete matrix and A ◦ B is the rank-k factorisation of the incomplete matrix.
As expected the recovery percentage decreases with the percentage of missing entries and clean

F /(cid:107)X(cid:107)2

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

19

Table 2. Factorisation error in (cid:107) · (cid:107)2

k=2

F of solutions obtained via formulations MIPF, MIP(1) and CIPF
k=10

data
(n-sparsity-noise) MIPF MIP(1) CIP MIPF MIP(1) CIP MIPF MIP(1) CIP
0.0
20-sparse-clean
10.3
20-sparse-noisy
0.0
20-normal-clean
11.2
20-normal-noisy

47.4
59.3
68.7
77.2

16.7
30.7
26.5
40.1

0.0
10.2
0.3
10.7

16.6
30.3
27.7
40.2

47.4
59.5
70.0
78.9

49.6
64.0
75.0
84.6

0.0
11.2
0.3
11.2

20.8
42.6
30.6
47.3

k=5

35-sparse-clean
35-sparse-noisy
35-normal-clean
35-normal-noisy

50-sparse-clean
50-sparse-noisy
50-normal-clean
50-normal-noisy

90.9
113.4
134.2
153.6

84.7
107.5
125.0
143.1

136.0
126.1
166.2 156.5
198.0
215.1
218.6
237.2

39.1
84.7
106.9 84.4
121.7 64.5
139.1 101.7

125.6 61.4
156.7 135.0
194.3 106.1
214.2 168.6

34.5
60.5
54.1
80.3

50.6
89.8
91.0
123.9

34.9
61.7
53.4
81.7

0.1
28.4
0.0
31.1

0.1
51.5
49.6
93.9
95.0
0.0
123.4 62.2

0.0
23.3
0.0
25.5

0.0
36.7
0.0
44.3

0.0
27.1
0.0
31.1

0.0
41.4
0.0
61.3

k=2

Table 3. Time in seconds to obtain solutions in Table 2 via formulations MIPF, MIP(1) and CIPF
k=10

data
(n-sparsity-noise) MIPF MIP(1) CIP MIPF MIP(1) CIP MIPF MIP(1) CIP
1.9
20-sparse-clean
602.9
20-sparse-noisy
15.8
20-normal-clean
602.0
20-normal-noisy

1.6
4.6
21.8
233.7
303.2
56.2
295.5 336.6

169.7
601.6
600.3
600.8

1.1
2.7
15.2
31.3

0.7
10.9
3.3
65.2

0.4
0.8
5.4
17.6

0.4
1.8
1.0
8.0

0.4
0.6
3.5
5.4

k=5

35-sparse-clean
35-sparse-noisy
35-normal-clean
35-normal-noisy

50-sparse-clean
50-sparse-noisy
50-normal-clean
50-normal-noisy

4.0
12.1
76.0
195.3

2.6
28.1
362.0
601.6

0.8
1.9
14.2
31.8

0.6
2.2
46.8
194.8

17.3
108.4
147.8 514.0
188.6 378.5
589.7 739.3

21.9
176.3
285.4 827.7
509.9 692.1
578.2 903.9

0.9
6.4
21.8
132.1

1.1
6.6
153.6
341.1

1.9

449.8
602.3 275.1
600.8
23.2
600.7 394.7

3.8

519.9
602.3 523.9
602.1 187.2
601.0 649.8

0.5
6.8
1.6
45.3

0.7
6.9
2.5
146.2

5.3
605.2
80.6
602.4

12.9
605.1
139.4
601.6

matrices are better recovered than noisy ones. All in all, we see a very high percentage of the
entries can be recovered by MIP(1).

Figure 6. Rank-10 binary matrix completion of artiﬁcial matrices with 5 − 30% missing entries

20

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

5.5. Comparing integer programming approaches against heuristics.

In this section,
we compare our integer programming approaches against the most widely used k-BMF heuristics
on real-world datasets. The heuristic algorithms we evaluate include the ASSO algorithm [32, 33],
the alternating iterative local search algorithm (ASSO++) of [1] which uses ASSO as a starting
point, and the penalty objective formulation (pymf) of [44] via the implementation of [39]. We also
compute rank-k NMF and binarise it with a threshold of 0.5. The exact details and parameters
used in the computations can be found in Appendix E. In addition, we use a new heuristic which
sequentially ﬁnds k rank-1 binary matrices using any heuristic for Bipartite Binary Quadratic
Programming as a subroutine. We refer to this heuristic outlined in Algorithm 1 as k-Greedy as
the subroutine we use to compute the rank-1 binary matrices is the greedy algorithm of [14].

Algorithm 1: Greedy algorithm for k-BMF (k-Greedy)
Input: X ∈ {0, 1}n×m, k ∈ Z+.
Set H ∈ {−1, 0, 1}n×m to hij = 2xij − 1 for (i, j) ∈ E ∪ E and hij = 0 otherwise.
for (cid:96) ∈ [k] do
a, b = BBQP(H) // compute a rank-1 binary matrix via any algorithm for BBQP
A:,(cid:96) = a
B(cid:96),: = b(cid:62)
H[ab(cid:62) == 1] = 0 // set entries of H to zero that are covered
end
Output: A ∈ {0, 1}n×k, B ∈ {0, 1}k×m

We solve CIP using CPLEX with a time limit of 20 mins and provide the heuristic solution of
k-Greedy as a warm start to it. The column generation approach results are obtained by generating
columns for 20 mins using formulation MLP(1) with a warm start of initial rank-1 binary matrices
obtained from k-Greedy, then solving MIP(1) over the generated columns with a time limit of 10
mins. Table 4 shows the factorisation error in (cid:107) · (cid:107)2
F after evaluating the above described methods
on all real-world datasets without missing entries for k = 2, 5, 10. The best result for each instance
is indicated in boldface. We observe that CG provides the strictly smallest error for 8 out of 12
instances.

Table 4. Comparison of factorisation error in (cid:107) · (cid:107)2

F for two IP based methods and ﬁve k-BMF heuristics

MIP(1) CIP ASSO++ k-Greedy pymf ASSO NMF

k=2

k=5

k=10

zoo
heart
lymp
apb

zoo
heart
lymp
apb

zoo
heart
lymp
apb

272

271
1185 1187
1192 1184
776
776

126
129
738
737
982 1026
688
684

39
425
728
573

72
529
829
605

276
1187
1202
776

133
738
1039
694

55
419
812
591

323
274
1187 1241
1201 1225
794
776

218
738

153
813
1053 1067
733

688

175
565
859
606

80
483
952
611

367
1251
1352
778

354
887
1484
719

377
694
1525
661

295
1273
1427
820

135
1190
1112
729

319
896
1102
660

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

21

While integer programming based approaches are able to handle missing entries by simply setting
the objective coeﬃcients of the missing entries to 0, the k-BMF heuristics ASSO, ASSO++ and
pymf cannot so simply be adjusted. Non-negative matrix factorisation however, has an available
implementation that can handle missing entries [26, 27]. Our next experiment compares our integer
programming approaches against k-Greedy and NMF on the real datasets that have missing entries.
Table 5 shows the results with the lowest error results indicated in boldface. For k = 2, k-Greedy
provides very accurate solutions which MIP(1) and CIP fail to improve on in 3 out of 4 instances.
For k = 5, 10 however, MIP(1) produces notably lower error factorisations than the other methods.

Table 5. Comparison of factorisation error in (cid:107) · (cid:107)2

F for real-world data with missing entries

MIP(1) CIP k-Greedy NMF

k=2

k=5

k=10

tumor
hepatitis
audio
votes

tumor
hepatitis
audio
votes

tumor
hepatitis
audio
votes

1352 1352
1264 1344
1419 1419
1246 1246

962

993
1138 1229
1064 1078
853

779

514
632
907 1048
881
765
701
240

1352 1792
1416
1346
1419 2361
1246 1268

1004
1238
1094
853

646
1056
881
706

1832
1618
2361
2353

1949
2159
2361
3189

6. Conclusions and further work.

In this paper we investigated the rank-k binary matrix
factorisation problem from an integer programming perspective. We analysed a compact and two
exponential size integer programming formulations for the problem and made a comparison on the
strength of the formulations’ LP-relaxations. We introduced a new objective function, which slightly
diﬀers from the traditional squared Frobenius objective in attributing a weight to zero entries of the
input matrix that is proportional to the number of times the zero is erroneously covered in a rank-k
factorisation. In addition, we discussed a computational approach based on column generation to
solve one of the exponential size formulations and reported several computational experiments to
demonstrate the applicability of our formulations on real world and artiﬁcial datasets.

Future research directions that could be explored include developing faster exact algorithms for
the pricing problem and once the pricing problems are solved more eﬃciently, a full branch-and-
price implementation would be interesting to explore.

Appendix A: Heuristics for the pricing problem. The greedy algorithm of [14] to solve
the Bipartite Binary Quadratic Program in Equation (46) aims to set entries of a and b to 1 which
correspond to rows and columns of H with the largest positive weights. In the ﬁrst phase of the
algorithm, the row indices i of H are put in decreasing order according to their sum of positive
entries, so γ+
i ≥ γ+
j=1 max(0, hij). Then sequentially according to this ordering,
ai is set to 1 if (cid:80)m
(cid:96)=1 a(cid:96)h(cid:96)j) and 0 otherwise. In the second
phase, bj is set to 1 if (a(cid:62)H)j > 0, 0 otherwise. An eﬃcient implementation of the greedy algorithm
due to [14] is given in Algorithm 2.

:= (cid:80)m
(cid:96)=1 a(cid:96)h(cid:96)j) < (cid:80)m

i+1 where γ+
j=1 max(0, (cid:80)i−1

j=1 max(0, (cid:80)i

i

22

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

i+1.

i ≥ γ+

Algorithm 2: Greedy Algorithm for BBQP
Input: H ∈ Rn×m
Phase I. Order i ∈ [n] so that γ+
Set a = 0n, s = 0m.
for i ∈ [n] do
f0 = (cid:80)m
f1 = (cid:80)m
if f0 < f1 then
Set ai = 1, s = s + hi
end
end

j=1 max(0, sj)
j=1 max(0, sj + hij)

Phase II.
Set b = 0m.
for j ∈ [m] do
if (a(cid:62)H)j > 0 then
Set bj = 1
end
end
Output: a ∈ {0, 1}n, b ∈ {0, 1}m

= γ+
i2

There are many variants of Algorithm 2 one can explore. First, the solution greatly depends on
the ordering of i’s in the ﬁrst phase. If for some i1 (cid:54)= i2 we have γ+
, comparing the sum of
i1
negative entries of rows i1 and i2 can put more “inﬂuential” rows of H ahead in the ordering. Let
us call this ordering the revised ordering and the one which only compares the positive sums as the
original ordering. Another option is to use a completely random order of i’s or to apply a small
perturbation to sums γ+
i to get a perturbed version of the revised or original ordering. None of the
above ordering strategies clearly dominates the others in all cases but they are fast to compute
hence one can evaluate all ﬁve ordering strategies (original, revised, original perturbed, revised
perturbed, random) and pick the best one. Second, the algorithm as presented above ﬁrst ﬁxes
a and then b. Changing the order of ﬁxing a and b can yield a diﬀerent result hence it is best
to try for both H and H (cid:62). In general, it is recommended to start the ﬁrst phase on the smaller
dimension [14]. Third, the solution from Algorithm 2 may be improved by computing the optimal
a with respect to ﬁxed b. This idea then can be used to ﬁx a and b in an alternating fashion and
stop when no changes occur in either. We summarise this alternating heuristic in Algorithm 3

Algorithm 3: Alternating Heuristic for BBQP
Input: H ∈ Rn×m, a(0) ∈ {0, 1}n, b(0) ∈ {0, 1}m.
for (cid:96) = 1, 2, . . . do
a((cid:96))[Hb((cid:96)−1) > 0] = 1
a((cid:96))[Hb((cid:96)−1) ≤ 0] = 0
if a((cid:96)) == a((cid:96)−1) then
Break
end
b((cid:96))[(a((cid:96)))(cid:62)H > 0] = 1
b((cid:96))[(a((cid:96)))(cid:62)H ≤ 0] = 0
if b((cid:96)) == b((cid:96)−1) then
Break
end
end
Output: a((cid:96)) ∈ {0, 1}n, b((cid:96)) ∈ {0, 1}m

In Section 5.2 we use the above described heuristics for the pricing problem in column generation.
At each iteration of the column generation procedure, 30 variants of Algorithm 2 are computed to
obtain an initial feasible solution to PP. The 30 variants of the greedy algorithm use the original

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

23

and revised ordering, their transpose and perturbed version and 22 random orderings. All greedy
solutions are improved by the alternating heuristic until no further improvement is found.

Appendix B: Synthetic data. Table 6 gives a summary of the parameters used to generate
our synthetic dataset. For a synthetic binary matrix X, n × m is the dimension of X, κ is the
Boolean rank which was used to generate X, and n(cid:48) × m(cid:48) is the dimension obtained after removing
zero and duplicate row and columns of X.

Table 6. Parameters of the synthetic dataset

(n-sparsity-noise) n × m κ 0s% noise% #instances n(cid:48) × m(cid:48)

20-sparse-clean
20-sparse-noisy
20-normal-clean
20-normal-noisy

35-sparse-clean
35-sparse-noisy
35-normal-clean
35-normal-noisy

50-sparse-clean
50-sparse-noisy
50-normal-clean
50-normal-noisy

20 × 20 10

35 × 20 10

50 × 20 10

75

50

75

50

75

50

0
5
0
5

0
5
0
5

0
5
0
5

10

10

10

14 × 15
19 × 19
18 × 18
19 × 20

22 × 15
31 × 19
29 × 18
34 × 20

30 × 15
45 × 20
40 × 18
48 × 20

Appendix C: Real world data. The following datasets were used in the experiments:

• The Zoo dataset (zoo) [9] describes 101 animals with 16 characteristic features. All but one
feature is binary. The categorical column which records the number of legs an animal has, is
converted into two new binary columns indicating if the number of legs is less than or equal or
greater than four. The size of the resulting fully binary matrix is 101 × 17.

• The Primary Tumor dataset (tumor ) [16] contains observations on 17 tumour features detected
in 339 patients. The features are represented by 13 binary variables and 4 categorical variables
with discrete options. The 4 categorical variables are converted into 11 binary variables rep-
resenting each discrete option. Two missing values in the binary columns are left as missing
values. The ﬁnal dimension of the binary matrix is 339 × 24 with 670 missing values.

• The Hepatitis dataset (hepat) [13] consists of 155 samples of medical data of patients with
hepatitis. The 19 features of the dataset can be used to predict whether a patient with hepatitis
will live or die. 6 of the 19 features take numerical values and are converted into 12 binary
features corresponding to options: less than or equal to the median value, and greater than the
median value. The column that stores the sex of patients is converted into two binary columns
corresponding to labels man and female. The remaining 12 columns take values yes and no and
are converted into 24 binary columns. The missing values in the raw dataset are left as missing
in the binary dataset as well. The ﬁnal dimension of the binary matrix is 155 × 38 with 334
missing values.

• The SPECT Heart dataset (heart) [5] describes cardiac Single Proton Emission Computed
Tomography images of 267 patients by 22 binary feature patterns. 25 patients’ images contain
none of the features and are dropped from the dataset, hence the ﬁnal dimension of the binary
matrix is 242 × 22.

24

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

• The Lymphography dataset (lymp) [17] contains data about lymphography examination of 148
patients. 8 features take categorical values and are expanded into 33 binary features representing
each categorical value. One column is numerical and we convert it into two binary columns
corresponding to options: less than or equal to median value, and larger than median value. The
ﬁnal dimension of the fully binary matrix is 148 × 44.

• The Audiology Standardized dataset (audio) [38] contains clinical audiology records on 226
patients. The 69 features include patient-reported symptoms, patient history information, and
the results of routine tests which are needed for the evaluation and diagnosis of hearing disorders.
9 features that are categorical valued are binarised into 34 new binary variables indicating if
a discrete option is selected. The missing values in the raw dataset are left as missing in the
binary dataset as well. The ﬁnal dimension of the binary matrix is 226 × 92 with 899 missing
values.

• The Amazon Political Books dataset (books) [23] contains binary data about 105 US politics
books sold by Amazon.com. Columns correspond to books and rows represent frequent co-
purchasing of books by the same buyers. The dimension of the binary matrix is 105 × 105.
• The 1984 United States Congressional Voting Records dataset (votes)[40] includes votes for
each of the U.S. House of Representatives Congressmen on the 16 key votes identiﬁed by the
CQA. The 16 categorical variables taking values of “voted for”, “voted against” or “did not
vote”, are converted into 16 binary features taking value 1 for “voted for”, value 0 for “voted
against” and a missing value indicates “did not vote”. The ﬁnal dimension of the binary matrix
is 435 × 16 with 392 missing values.

Appendix D: Obtaining integer feasible solutions.

In this section we give additional
numerical results supporting our conclusions drawn in Section 5.2.2. Table 7 shows the factorisa-
tion error measured in (cid:107) · (cid:107)2
F of integer feasible solutions obtained by solving MIP(1) and MIPF
over columns generated by MLP(1). MIP(1) takes signiﬁcantly faster to solve than MIPF but the
absolute diﬀerence in error between solutions produced by MIP(1) and MIPF is at most 1, except
for the last row in column k = 5 where MIPF runs out of the time budget of 300 seconds and
produces higher error solutions than MIP(1).

k ). Since MLP( 1

Table 8 shows the result of an analogous experiment where the columns used are generated by
MLP( 1
k ) is slower to solve than MLP(1), more columns are generated during CG
and the master IPs have a harder task on selecting k columns from a larger set of columns in
Table 8. However, while solving MIP(1) over a larger set of columns adds only a few seconds for
most instances, MIPF runs out of the time budget of 300 secs in about half the cases. This is also
demonstrated in the error diﬀerence, with solutions by MIP(1) having smaller error than solutions
by MIPF in most cases.

Appendix E: Heuristics for k-BMF. The following methods were evaluated for the com-

parison in Tables 4 and 5.
• For the alternating iterative local search algorithm of [1] (ASSO++) we obtained the code from
the author’s github page, see the reference. The code implements two variants of the algorithm
and we report the smaller error solution from two variants of it.

• For the method of [44], we used a python implementation in the package pymf, see [39] and we

ran it for 10000 iterations.

• We evaluated the heuristic method ASSO [32] which depends on a parameter and we report the
best results across nine parameter settings (τ ∈ {0.1, 0.2, . . . , 0.9}). The code was obtained form
the webpage of the author: https://people.mpi-inf.mpg.de/ pmiettin/src/DBP-progs/.
We observe that ASSO does not return monotone solutions and sometimes we get a higher error
solution for a higher value of k.

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

25

Table 7. Error in (cid:107) · (cid:107)2

data
(n-sparsity-noise) MIP(1)

F (and runtime in seconds) of integer solutions by MIP(1) and MIPF on columns by MLP(1)
k=5

k=10

k=2

20-sparse-clean
20-sparse-noisy
20-normal-clean
20-normal-noisy

35-sparse-clean
35-sparse-noisy
35-normal-clean
35-normal-noisy

50-sparse-clean
50-sparse-noisy
50-normal-clean
50-normal-noisy

47 (0.0)
59 (0.0)
70 (0.0)
78 (0.1)

MIPF
47 (0.0)
59 (0.0)
69 (0.3)
78 (0.9)

84 (0.1)
84 (0.0)
107 (0.0) 107 (0.1)
125 (0.4) 124 (2.2)
143 (0.6) 141 (4.9)

MIP(1)

16 (0.0)
30 (0.0)
27 (0.1)
40 (0.5)

34 (0.0)
60 (0.0)
54 (0.8)
80 (4.1)

MIPF
16 (0.0)
30 (0.0)
27 (2.7)
39 (76.5)

MIP(1)

0 (0.0)
10 (0.0)
0 (0.0)
10 (0.5)

MIPF
0 (0.0)
10 (0.0)
0 (0.0)
10 (3.4)

34 (0.1)
60 (0.6)
53 (154.8)
80 (245.4)

0 (0.0)
23 (0.1)
0 (0.0)
25 (2.0) 24 (114.2)

0 (0.0)
23 (0.2)
0 (0.1)

126 (0.0) 126 (0.0)
156 (0.0) 156 (0.1)
198 (1.4) 197 (8.2)
218 (2.2) 218 (41.4) 123 (39.7) 126 (271.1) 44 (10.1) 44 (165.8)

50 (0.1)
89 (0.2)
91 (173.4)

50 (0.0)
89 (0.0)
91 (30.9)

0 (0.0)
36 (0.2)
0 (0.1)

0 (0.0)
36 (0.0)
0 (0.1)

Table 8. Error in (cid:107) · (cid:107)2

F (and runtime in seconds) of integer solutions by MIP(1) and MIPF on columns by MLP( 1
k )
k=5

k=10

k=2

data
(n-sparsity-noise) MIP(1)

20-sparse-clean
20-sparse-noisy
20-normal-clean
20-normal-noisy

35-sparse-clean
35-sparse-noisy
35-normal-clean
35-normal-noisy

50-sparse-clean
50-sparse-noisy
50-normal-clean
50-normal-noisy

50 (0.0)
64 (0.0)
76 (0.2)
85 (0.3)

MIPF
50 (0.2)
64 (0.6)
75 (3.9)
85 (6.3)

91 (1.5)
91 (0.0)
113 (3.1)
114 (0.1)
136 (1.0) 134 (19.1)
154 (58.9)
154 (1.6)

MIP(1)

21 (0.0)
42 (0.1)
30 (0.5)
47 (1.2)

39 (0.2)
81 (0.5)
61 (2.0)
93 (6.2)

MIPF
21 (2.6)
43 (219.0)
31 (289.6)
47 (300.4)

MIP(1)

0 (0.0)
11 (0.2)
0 (0.1)
11 (0.6)

MIPF
0 (0.0)
11 (6.3)
0 (0.2)
11 (54.2)

0 (0.1)
39 (98.9)
28 (0.3)
84 (300.7)
65 (300.8)
0 (0.8)
102 (301.3) 28 (2.1) 31 (301.0)

0 (0.1)
28 (229.9)
0 (11.9)

136 (0.8)
166 (6.5)

61 (0.2)
137 (0.0)
128 (0.7) 135 (301.5) 46 (0.6) 50 (301.5)
167 (0.1)
215 (2.2) 215 (131.6) 100 (34.4) 106 (302.1)
0 (153.7)
238 (5.7) 237 (226.4) 149 (95.8) 169 (302.9) 51 (39.4) 62 (302.5)

61 (160.0)

0 (0.8)

0 (0.2)

0 (0.8)

• We computed rank-k non-negative matrix factorisation (NMF) and binarise it by a thresh-
old of 0.5: after an NMF is obtained, values greater than 0.5 are set to 1, otherwise to 0.
For the computation of NMF we used the function non negative factorization from the
sklearn.decomposition module in python when the binary matrix has no missing entries, and
for incomplete binary matrices we used the Matlab implementation in [27, 26].

• The heuristic k-greedy algorithm was ran with 70 random seeds and the subroutine for BBQP
used the greedy and alternating algorithms for BBQP given in Algorithms 2, 3. In addition, the
k-greedy algorithm can be run on a preprocessed or original matrix and we tried both ways.
For each instance the lowest error factorisation is reported.

Acknowledgments. During the completion of this work R.A.K was supported by a doctoral

scholarship from The Alan Turing Institute and the Oﬃce for National Statistics.

References

26

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

[1] Barahona F, Goncalves J (2019) Local search algorithms for binary matrix factorization. URL https:
//github.com/IBM/binary-matrix-factorization/blob/master/code, last accessed on 2020-04-21.

[2] Barnhart C, Johnson EL, Nemhauser GL, Savelsbergh MWP, Vance PH (1998) Branch-and-price: Col-
umn generation for solving huge integer programs. Operations Research 46(3):316–329, URL http:
//dx.doi.org/10.1287/opre.46.3.316.

[3] Beckerleg M, Thompson A (2020) A divide-and-conquer algorithm for binary matrix completion. Linear
Algebra and its Applications 601:113–133, ISSN 0024-3795, URL http://dx.doi.org/10.1016/j.laa.
2020.04.017.

[4] Chalermsook P, Heydrich S, Holm E, Karrenbauer A (2014) Nearly tight approximability results for
minimum biclique cover and partition. Schulz AS, Wagner D, eds., Algorithms - ESA 2014, 235–246
(Berlin, Heidelberg: Springer Berlin Heidelberg), ISBN 978-3-662-44777-2.

[5] Cios KJ, Kurgan LA (2001) Uci machine learning repository: Spect heart data. URL https://archive.

ics.uci.edu/ml/datasets/spect+heart, last accessed on 2020-06-11.

[6] Conforti M, Cornuejols G, Zambelli G (2014) Integer Programming (Springer Publishing Company,

Incorporated), ISBN 3319110071, 9783319110073.

[7] CPLEX Optimization (2018) Using the CPLEX Callable Library, V.12.8. CPLEX Optimization, Inc.,

Incline Village, NV.

[8] Dua D, Graﬀ C (2017) UCI machine learning repository. URL http://archive.ics.uci.edu/ml, last

accessed on 2020-06-11.

[9] Forsyth R (1990) Uci machine learning repository: Zoo data set. URL http://archive.ics.uci.edu/

ml/datasets/Zoo, last accessed on 2020-06-11.

[10] Garey MR, Johnson DS (1979) Computers and Intractability: A Guide to the Theory of NP-Completeness

(New York, NY, USA: W. H. Freeman & Co.), ISBN 0716710455.

[11] Gillis N, Vavasis SA (2018) On the complexity of robust pca and l1-norm low-rank matrix approximation.
Mathematics of Operations Research 43(4):1072–1084, URL http://dx.doi.org/10.1287/moor.2017.
0895.

[12] Golub GH, Van Loan CF (1996) Matrix Computations (USA: Johns Hopkins University Press), 3rd

edition, ISBN 0801854148.

[13] Gong G (1988) Uci machine learning repository: Hepatitis data set. URL https://archive.ics.uci.

edu/ml/datasets/Hepatitis, last accessed on 2020-06-11.

[14] Karapetyan D, Punnen AP (2013) Heuristic algorithms for the bipartite unconstrained 0-1 quadratic

programming problem. arXiv 1210.3684.

[15] Kim K (1982) Boolean Matrix Theory and Applications. Monographs and textbooks in pure and applied

mathematics (Dekker), ISBN 9780824717889.

[16] Kononenko I, Cestnik B (1988) Uci mach. learn. rep.: Primary tumor domain. URL https://archive.

ics.uci.edu/ml/datasets/Primary+Tumor, last accessed on 2020-06-11.

[17] Kononenko I, Cestnik B (1988) Uci machine learning repository: Lymphography data set. URL https:

//archive.ics.uci.edu/ml/datasets/Lymphography, last accessed on 2020-06-11.

[18] Kovacs RA (2021) Code for binary matrix factorisation and completion via integer programming. URL

https://github.com/kovacsrekaagnes/rank_k_BMF.

[19] Kovacs RA, Gunluk O, Hauser RA (2017) Low-rank boolean matrix approximation by integer pro-
gramming. NIPS, 1–5, Optimization for Machine Learning Workshop, https://opt-ml.org/papers/
OPT2017_paper_34.pdf.

[20] Kovacs RA, Gunluk O, Hauser RA (2021) Binary matrix factorisation via column generation. Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence 35(5):3823–3831, URL https://ojs.aaai.org/
index.php/AAAI/article/view/16500.

[21] Koyut¨urk M, Grama A (2003) Proximus: A framework for analyzing very high dimensional discrete-
attributed datasets. Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

27

Discovery and Data Mining, 147–156, KDD ’03 (New York, NY, USA: Association for Computing
Machinery), ISBN 1581137370, URL http://dx.doi.org/10.1145/956750.956770.

[22] Koyut¨urk M, Grama A, Ramakrishnan N (2002) Algebraic techniques for analysis of large discrete-valued
datasets. Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge
Discovery, 311–324, PKDD ’02 (Berlin, Heidelberg: Springer-Verlag), ISBN 3540440372.

[23] Krebs V (2008) Amazon political books. URL http://moreno.ss.uci.edu/data.html#books, last

accessed on 2020-06-11.

[24] Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature

401(6755):788–791, URL http://dx.doi.org/10.1038/44565.

[25] Li T (2005) A general model for clustering binary data. Proceedings of the Eleventh ACM SIGKDD
International Conference on Knowledge Discovery in Data Mining, 188–197, KDD ’05 (New York, NY,
USA: Association for Computing Machinery), ISBN 159593135X.

[26] Li Y, Ngom A (2012) The non-negative matrix factorization toolbox for biological data mining. Source

Code for Biology and Medicine 8:10 – 10.

[27] Li Y, Ngom A (2013) The non-negative matrix factorization toolbox in matlab (the nmf matlab toolbox).

URL https://sites.google.com/site/nmftool/, last accessed on 2021-07-16.

[28] Lu H, Vaidya J, Atluri V (2008) Optimal boolean matrix decomposition: Application to role engineering.
Proceedings of the 2008 IEEE 24th International Conference on Data Engineering, 297–306, ICDE ’08
(Washington, DC, USA: IEEE Computer Society), URL http://dx.doi.org/10.1109/ICDE.2008.
4497438.

[29] Lu H, Vaidya J, Atluri V (2014) An optimization framework for role mining. Journal of Computer

Security 22(1):1–31, ISSN 0926-227X.

[30] L¨ubbecke ME, Desrosiers J (2005) Selected topics in column generation. Operations Research

53(6):1007–1023, URL http://dx.doi.org/10.1287/opre.1050.0234.

[31] McCormick GP (1976) Computability of global solutions to factorable nonconvex programs: Part i —
convex underestimating problems. Mathematical Programming 10(1):147–175, URL http://dx.doi.
org/10.1007/BF01580665.

[32] Miettinen P, Mielik¨ainen T, Gionis A, Das G, Mannila H (2006) The discrete basis problem. F¨urnkranz
J, Scheﬀer T, Spiliopoulou M, eds., Knowledge Discovery in Databases: PKDD 2006, 335–346 (Berlin,
Heidelberg: Springer Berlin Heidelberg), ISBN 978-3-540-46048-0.

[33] Miettinen P, Mielik¨ainen T, Gionis A, Das G, Mannila H (2008) The discrete basis problem. IEEE
Transactions on Knowledge and Data Engineering 20(10):1348–1362, ISSN 1041-4347, URL http://
dx.doi.org/10.1109/TKDE.2008.53.

[34] Monson SD, Pullman NJ, Rees R (1995) A survey of clique and biclique coverings and factorizations of

(0,1)–matrices. Bulletin – Institute of Combinatorics and its Applications 14:17–86, ISSN 1183-1278.

[35] Orlin J (1977) Contentment in graph theory: Covering graphs with cliques. Indagationes Mathemati-
cae (Proceedings) 80(5):406–424, ISSN 1385-7258, URL http://dx.doi.org/10.1016/1385-7258(77)
90055-5.

[36] Padberg M (1989) The boolean quadric polytope: Some characteristics, facets and relatives. Mathemat-

ical Programming 45(1):139–172, URL http://dx.doi.org/10.1007/BF01589101.

[37] Peeters R (2003) The maximum edge biclique problem is np-complete. Discrete Applied Mathematics
131(3):651–654, ISSN 0166-218X, URL http://dx.doi.org/10.1016/S0166-218X(03)00333-0.
[38] Quinlan R (1992) Uci machine learning repository: Audiology (standardized) data set. URL http:
//archive.ics.uci.edu/ml/datasets/audiology+(standardized), last accessed on 2020-06-11.

[39] Schinnerl C (2017) Pymf

- python matrix factorization module. URL https://github.com/

ChrisSchinnerl/pymf3, last accessed on 2021-03-11.

[40] Schlimmer J (1987) Uci machine learning repository: 1984 US Cong. Voting Records Database.
URL https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records, last accessed
on 2020-06-11.

28

G¨unl¨uk, Hauser and Kov´acs: Binary Matrix Factorisation and Completion via Integer Programming
Article submitted to Mathematics of Operations Research

[41] Shen BH, Ji S, Ye J (2009) Mining discrete patterns via binary matrix factorization. Proceedings of
the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 757–766,
KDD ’09 (New York, NY, USA: Association for Computing Machinery), ISBN 9781605584959, URL
http://dx.doi.org/10.1145/1557019.1557103.

[42] Shi Z, Wang L, Shi L (2014) Approximation method to rank-one binary matrix factorization. 2014
IEEE International Conference on Automation Science and Engineering (CASE), 800–805, URL http:
//dx.doi.org/10.1109/CoASE.2014.6899417.

[43] Simon HU (1990) On approximate solutions for combinatorial optimization problems. SIAM Journal

on Discrete Mathematics 3(2):294–310, URL http://dx.doi.org/10.1137/0403025.

[44] Zhang Z, Li T, Ding C, Zhang X (2007) Binary matrix factorization with applications. Proceedings of
the 2007 Seventh IEEE International Conference on Data Mining, 391–400, ICDM ’07 (USA: IEEE
Computer Society), URL http://dx.doi.org/10.1109/ICDM.2007.99.

