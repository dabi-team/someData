9
1
0
2

t
c
O
1
2

]
I

A
.
s
c
[

1
v
2
7
4
9
0
.
0
1
9
1
:
v
i
X
r
a

Under consideration for publication in Theory and Practice of Logic Programming

1

A Logic-Based Framework Leveraging
Neural Networks for Studying the Evolution of
Neurological Disorders

FRANCESCO CALIMERI, FRANCESCO CAUTERUCCIO, LUCA CINELLI
DEMACS, University of Calabria, Italy
(e-mail: {calimeri,cauteruccio,cinelli}@mat.unical.it)

ALDO MARZULLO
DEMACS, University of Calabria, Italy -
CREATIS; CNRS UMR5220; INSERM U1206; Universit´e de Lyon, Universit´e Lyon 1, INSA-Lyon, Villeurbanne, France
(e-mail: marzullo@mat.unical.it)

CLAUDIO STAMILE
CREATIS; CNRS UMR5220; INSERM U1206; Universit´e de Lyon, Universit´e Lyon 1, INSA-Lyon, Villeurbanne, France
(e-mail: stamile@creatis.insa-lyon.fr)

GIORGIO TERRACINA
DEMACS, University of Calabria, Italy
(e-mail: terracina@mat.unical.it)

FRANC¸ OISE DURAND-DUBIEF
Hˆopital Neurologique, Service de Neurologie A
Hospices Civils de Lyon, Bron - France -
CREATIS; CNRS UMR5220; INSERM U1206; Universit´e de Lyon, Universit´e Lyon 1, INSA-Lyon, Villeurbanne, France
(e-mail: francoise.durand-dubief@chu-lyon.fr)

DOMINIQUE SAPPEY-MARINIER
CERMEP - Imagerie du Vivant;
Universit´e de Lyon, Bron, France -
CREATIS; CNRS UMR5220; INSERM U1206; Universit´e de Lyon, Universit´e Lyon 1, INSA-Lyon, Villeurbanne, France
(e-mail: dominique.sappey-marinier@univ-lyon1.fr)

submitted 1 January 2003; revised 1 January 2003; accepted 1 January 2003

Abstract

Deductive formalisms have been strongly developed in recent years; among them, Answer Set Programming
(ASP) gained some momentum, and has been lately fruitfully employed in many real-world scenarios.
Nonetheless, in spite of a large number of success stories in relevant application areas, and even in industrial
contexts, deductive reasoning cannot be considered the ultimate, comprehensive solution to AI; indeed,
in several contexts, other approaches result to be more useful. Typical Bioinformatics tasks, for instance
classiﬁcation, are currently carried out mostly by Machine Learning (ML) based solutions.

In this paper, we focus on the relatively new problem of analyzing the evolution of neurological disorders.
In this context, ML approaches already demonstrated to be a viable solution for classiﬁcation tasks; here,
we show how ASP can play a relevant role in the brain evolution simulation task. In particular, we propose
a general and extensible framework to support physicians and researchers at understanding the complex
mechanisms underlying neurological disorders. The framework relies on a combined use of ML and ASP,

 
 
 
 
 
 
2

F. Calimeri et al.

and is general enough to be applied in several other application scenarios, which are outlined in the paper.
Under consideration in Theory and Practice of Logic Programming (TPLP)

KEYWORDS: Logic Programming, Answer Set Programming, Rule-Based Systems, Deductive Reasoning,
Neural Networks, Neurological Disorders, Machine Learning, Declarative Formalisms, Bioinformatics

1 Introduction

After a number of works on automatic theorem proving and artiﬁcial intelligence, the work on
logic programming took actually off in the 1970’s, with the aim of obtaining automated deduction
systems. Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel¨a 1999; Lonc
and Truszczynski 2006) is one of the several formalisms that stemmed out of such research
efforts, and during the years it turned out to be a powerful declarative formalism for knowledge
representation and reasoning (KRR). After more than twenty-ﬁve years of scientiﬁc research,
the theoretical properties of the language are considered to be well understood, and even if the
community is still very active on several extensions, the solving technology, as witnessed by the
availability of a number of robust and efﬁcient systems (Gebser et al. 2017; Gebser et al. 2018),
is mature for practical applications (Brooks et al. 2007; Terracina et al. 2008; Gebser et al. 2011;
Manna et al. 2015). In the latest years, ASP has been indeed employed in many different domains,
and used for the development of enterprise and industrial-level applications (Calimeri and Ricca
2013; Leone and Ricca 2015; Erdem et al. 2016), fostered by the release of a variety of proper
development tools and interoperability mechanisms for allowing interaction and integration with
external systems (Ricca 2003; Calimeri et al. 2007; Febbraro et al. 2011; Febbraro et al. 2012;
Thimm 2014; Sch¨uller and Weinzierl 2015; Gebser et al. 2014; Eiter et al. 2016; Rath and Redl
2017; Calimeri et al. 2017a; Eiter et al. 2018; Calimeri et al. 2019).

Nevertheless, even though Answer Set Programming, and deductive approaches via logic for-
malisms in general, are of wide use for Artiﬁcial Intelligence (AI) applications, they are not the
ultimate, comprehensive solution to AI, as some kind of problems can be hardly encoded by logic
rules. This can be due to several reasons: the nature of the problem, the lack of proper develop-
ment tools, and severe performance issues even for the best performing systems. Bioinformatics
is a prime example, as both related data (think, for instance, of biomedical images, temporal
data, etc.) and relevant tasks (think, for instance, of classiﬁcation) are not naturally approachable
with deductive strategies. In such research area, and similar ones, different AI-based strategies
have been employed. Lately, approaches relying on Machine Learning (ML) and Artiﬁcial Neu-
ral Networks (ANNs) (Haykin 1998; Goodfellow et al. 2016) are on the rise, both because of the
great results achieved by the research community in the latest years and because they better deal
with the data and the nature of the tasks of interest.

Basically, with ANNs the problem is not actually modeled, and its structure remains almost
unknown; rather, approaches progressively learn, usually by examples, the best answers to pro-
vide in presence of speciﬁc inputs. ANNs can learn and model non-linear complex relationships,
and even generalize acquired knowledge in order to infer new properties over unseen data; more-
over, once trained, a neural network can be extremely fast in providing answers to instances of
complex problems. Unfortunately, obtained results have only statistical signiﬁcance; it is note-
worthy, indeed, that the main weakness of ANNs is in general their incompleteness, since their
precision may strongly depend on the training phase and on the quality of the training data. The

Theory and Practice of Logic Programming

3

logic they use can be sound, yet proven incorrect by further observations. Hence, clearly, as for
deductive reasoning, not all problems can be properly solved by ANNs.

This paper focuses on some opportunities provided by a combined use of ASP and ANN. As
a motivating scenario, we concentrate on a relevant bioinformatics problem, namely the study of
neurological disorders and, speciﬁcally, on the Multiple Sclerosis (MS) disease. To the best of
our knowledge, computer science research on this topic mainly focused on the identiﬁcation of
the disease and, possibly, on the detection of its severity. Actually, in order to clinically analyze
disease evolution, long periods of observations, even decades, are needed. Obviously, a tool
supporting physicians in accelerating the analysis process, e.g. via simulations, would be of great
beneﬁt. Notably, it has been demonstrated by several independent studies that there is a strong
correlation between the variations of the structure of the connections among neurons (also called
connectome) with possible insurgence of several neurological disorders (Bargmann and Marder
2013). Hence, it would be of high interest to simulate the course of the disease by simulating
brain connections degradation, in order to understand which kind of modiﬁcations might mostly
determine an evolution of the disease into a worst state, or which recovery processes might induce
a remission state. Unfortunately, this simulation represents a non-trivial challenge due to various
reasons, including the fact that the actual mechanisms guiding the evolution of the pathology are
still largely unknown.

Interestingly, the connectome can be fruitfully represented by means of a graph; hence, a pos-
sible solution would be to simulate the progress of the pathology by means of a set of custom-
deﬁned rules for modelling the evolution of the brain structure, which may involve a certain
background knowledge. An effective tool for the experts could consist of a comprehensive en-
vironment which allows to dynamically detect minimal alterations of brain connections, based
on speciﬁc guidelines, that induce a change of state in the disease. The possible change of state
can, in turn, be detected by neural networks, exploring latent relations learnt from samples. It
is worth pointing out that, to the best of our knowledge, a tool providing these features is not
available yet; moreover, simulating manually brain alterations is not an easy task and it is almost
impossible to manually detect signiﬁcant brain substructures.

In this context, ASP can play a relevant role. In fact, simulation tasks could be in principle de-
signed with any programming paradigm; nevertheless, since it is still not clear which changes to
which graph properties are likely to impact the evolution of the disease, a try-and-check method-
ology is necessary. This implies writing ad-hoc simulation machineries for each of them. Clearly,
a declarative methodology allows for compact and clear deﬁnitions as well as fast prototyping:
ASP paves the way to easy deﬁnition of rules for the identiﬁcation of brain substructures that can
be of interest for the analyst.

Interestingly, it has been shown (Kocevar et al. 2016) that it is important to look at minimal
graph changes allowing to reach a certain goal in some graph variations, such as density or assor-
tativity; this implies that most of the tasks to be carried out would actually involve optimization.
The use of ASP for optimization problems is still relatively less popular than its use for deci-
sion problems, even though it has been proved to be perfectly suited for them; moreover, the
formulas for the computation of graph metrics can be in general not easy to be written by rules
only. This increases the interest in the application of recent extensions of ASP systems, such as
I-DLV/DLV2 (Calimeri et al. 2017b; Adrian et al. 2018), DLVHEX (Eiter et al. 2016; Calimeri
et al. 2016; Eiter et al. 2018), etc., which allow both to solve optimization problems and the in-
tegration of external computation sources within the ASP program. The potentialities of ASP in
these two areas are pointed out in the rest of the paper.

4

F. Calimeri et al.

The main contribution of the paper consists of a framework for studying the evolution of
neurological disorders by simulating variations in the connectome. The framework relies on two
main modules: an ANN that classiﬁes a given connectome with respect to disorder stages, and
a logic program (speciﬁed via ASP) that allows to perform non-deterministic variations on a
given connectome that guarantee ﬁxing some graph parameters under study. The framework
iterates between the ANN and the ASP modules to simulate possible evolutions of a neurological
disorder.

The present paper elaborates on, and extends, our previous work presented in (Calimeri et al.

2018) as follows:

• it presents a thorough analysis on related work;
• it extends the general framework with new modules and new functionalities;
• it introduces two new actualizations of the framework, thus showing three use cases bene-

ﬁting of the framework that are also biologically relevant;

• it improves the ANN classiﬁer for MS in order to provide insights on the classiﬁcation

activity, thus allowing a more reﬁned combined use of ASP and ANN;

• it signiﬁcantly extends the experimental activity, now including the new use cases and a

speciﬁc performance analysis;

• it introduces a web tool implementing the proposed framework and developed to support

physicians in their analyses via the framework;

• it overviews two more applications possibly beneﬁting of specializations of our general

framework.

The remainder of the paper is structured as follows. In Section 2 we discuss related work, and
in Section 3 we recall some preliminary notions both from the biomedical and ASP contexts. In
Section 4 we introduce the general framework and discuss the role ASP and ANN play therein,
while Section 5 presents three actualizations of the framework. Section 6 describes the experi-
ments we carried out to assess potentiality and applicability of the approach. Section 7 introduces
the web tool we developed based on the presented work, whereas Section 8 surveys some other
applications possibly beneﬁting of the presented framework. Eventually, in Section 9, we draw
our conclusions and outline some future work.

2 Related Works

In this section we outline related literature. Since, to the best of our knowledge, there is no
proposal available, to date, that simulates the evolution of neurological disorders by a combined
use of ASP and ANN, we concentrate our attention on works exploiting together, to different
extents, declarative formalisms and ML solutions.

Some works have been carried out to integrate data-driven solutions into declarative systems
with the aim of increasing performance; for instance, such solutions are used for inductively
choosing conﬁgurations, algorithms selection, and proper coupling of subsystems (Gebser et al.
2011; Maratea et al. 2014; Fusc`a et al. 2017). In some proposals, see for instance SMT (Cok et al.
2015; Barrett et al. 2016; Barrett and Tinelli 2018; Barrett et al. 2013) or CASP (Baselice et al.
2005; Mellarkod et al. 2008; Balduccini and Lierler 2017; Lierler and Susman 2017; Shen and
Lierler 2018; Arias et al. 2018), the logic solver can select statements that should be checked by
external theory/numerical solvers, so that the next steps carried out by the logic solver depend on

Theory and Practice of Logic Programming

5

the answers produced by the external ones. Some works mix statistical analysis and ASP (Gel-
fond 2010; Nickles and Mileo 2014; Beck et al. 2015): here, the aim is to extend logic programs
with probabilistic reasoning, either by a direct integration or by embedding external predicates.
Other approaches are related to the use of methods that “guide” the reasoning, the generation of
logic programs or other optimizations (Law et al. 2015; Law et al. 2016; Chabierski et al. 2017;
Dodaro and Ricca 2018); most of them are still at a preliminary stage.

In the context of ASP, some proposals allow ASP systems to access external sources of compu-
tation and even value invention (Calimeri et al. 2007; Redl 2016; Kaminski et al. 2017; Calimeri
et al. 2017b; Eiter et al. 2018; Alviano et al. 2019; Gebser et al. 2019), making them to impact
semantics computation to different extents. In particular, thanks to extended built-in constructs,
it is possible to invoke external functions and deﬁne custom constraints; via such invocations,
one might in principle place a call even to an ANN from an ASP program.

In the last decades, ANNs have become one of the most powerful machine learning tools for
solving complex problems. Especially in visual domains, they achieved impressive results in
object detection, recognition and classiﬁcation (Goodfellow et al. 2016). An ANN is typically
represented as a composition of functions, each computing a nonlinear weighted combination of
its input, which constitute the neural structure, and are organized in layers. Starting from sam-
ples, the learning algorithm iteratively reﬁnes the network parameters θ in order to approximate
a target function f . For example, in the particular case of classiﬁcation, the ANN learns how
to approximate a function y = f (x, θ ) which maps an input x to a category whose label is y.
However, the approximation given by the ANN does not provide any insight on the form of f ,
meaning that there is no interpretable connection between the parameters and the target function.
This is one of the main causes for the interpretation of ANN being an open problem.

Several attempts have been made for interpreting the behavior of Neural Networks, also using
declarative approaches (Zhang and Zhu 2018), and to incorporate symbolic knowledge into neu-
ral networks, resulting in a class of networks known as knowledge-based neural networks (Towell
and Shavlik 1993). In the context of network interpretation, Zhang et al. used explanatory graphs
and decision trees to create interpretable rules describing convolutional neural network (CNN)
features (Zhang et al. 2018; Zhang et al. 2018). Furthermore, based on a semantic And-Or rep-
resentation, Zhang et al. also proposed a method to use active question-answering to assign a
semantic meaning to neural patterns in convolutional layers of a pre-trained CNN and built a
model for hierarchical object understanding (Zhang et al. 2017). Hu et al. proposed a framework
using logic rules to obtain more meaningful network representations by constructing an iterative
distillation method that transfers the structured information of logic rules into the weights of
neural networks (Hu et al. 2016).

In this section we report some preliminary background knowledge; we address the biomedical
background ﬁrst, before overviewing some basic knowledge on ASP.

3 Preliminaries

3.1 Background on neurological disorders

The incidence of neurological disorders is constantly growing, also because population is aging
on the average in most countries; hence, the efforts to design approaches capable of determin-
ing the onset of these disorders and monitoring their course in patients are intensifying (Duun-

6

F. Calimeri et al.

Henriksen et al. 2012; Hornero et al. 2009; Wieser et al. 2006). Furthermore, the tools supporting
neurologists in their activities are becoming more complex and sophisticated (think, for instance,
of magnetic resonance imaging (MRI) or of new electroencephalograms (EEG) with 256 elec-
trodes, instead of the classical ones with 19 electrodes). These important advances foster the need
for handling new data formats, like images and temporal series, that are hard to be analyzed by
human experts. In these scenarios, automatic tools for the analysis are becoming more and more
essential.

In many neurological investigations, a key role is played by the connections between brain
areas, that can be studied by means of Magnetic Resonance Imaging (MRI); graph theory, and
speciﬁcally network analysis tools, may hence provide insights to evaluate the health state of the
brain. A challenging issue is to ﬁnd suitable representations of brain areas as a network, and then
proper tools for interpreting them.

Multiple Sclerosis (MS) is a chronic disease of the central nervous system that disrupts the ﬂow
of information within the brain, and between brain and body. In 2015, about 2.3 million people
resulted as affected, globally, with rates varying widely among different regions and different
populations (Vos et al. 2016). In the majority of cases (with a probability of 85%), the disease
starts with a ﬁrst acute episode, called Clinically Isolated Syndrome (CIS), that evolves into the
Relapsing-Remitting (RR) form; RR patients will then evolve into the Secondary Progressive
(SP) form after about 10–20 years (Stamile et al. 2015; Kocevar et al. 2016). Remaining 15%
of subjects start directly with the Primary Progressive (PP) form. MS diagnosis has been revo-
lutionized in the last 30 years by the introduction of MRI. Advanced MRI strategies, including
magnetization transfer, spectroscopy, and Diffusion Tensor Imaging (DTI), together with classi-
cal structural MRI modalities like T1, were successful in detecting alterations and contributed to
our understanding of the pathological mechanisms occurring in normal appearing white matter
(WM). Indeed, if T1 provides detailed information about the anatomical structure of the gray
matter (GM) brain regions, DTI allows to detect the WM connections between those regions.

Determining the current clinical proﬁle of a patient has a major impact on the treatment she
gets; unfortunately, it is not an easy task to be carried out with automatic tools. In the litera-
ture, it has been shown that starting from suitable images of the brain, it is possible to extract
a structural connectivity matrix, called connectome, representing the map of neural connections
in the brain at hand (Rubinov and Sporns 2010; Kocevar et al. 2016). This graph can be ex-
pressed as G = (V, E, ω), where nodes V represent mapped brain regions, edges E represent
connections between them and edge weights ω express the strength of these connections. Re-
cent approaches (Ion-Margineanu et al. 2017) exploit state-of-the-art classiﬁers (such as Support
Vector Machines, Linear Discriminant Analysis and Convolutional Neural Networks) to clas-
sify Multiple Sclerosis courses using features extracted from Magnetic Resonance Spectroscopic
Imaging (MRSI) combined with brain tissue segmentations of grey matter, white matter, and
lesions. Beyond the identiﬁcation of the current clinical proﬁle, predicting a patient’s evolution
and response to a therapy based on clinical, biological, and imaging markers still represents a
challenge for neurologists.

3.2 Background on Answer Set Programming

The term “Answer Set Programming” was introduced by Vladimir Lifschitz to denote a declar-
ative programming methodology (Lifschitz 1999); concerning terminology, ASP is sometimes
used in a broader sense, referring to any declarative formalism which represents solutions as

Theory and Practice of Logic Programming

7

sets. However, the more frequent understanding is the one adopted in this article, which dates
back to (Gelfond and Lifschitz 1991). For a more thorough introductory material on ASP, we re-
fer the reader to (Baral 2003; Gelfond and Leone 2002; Lifschitz 1999; Marek and Truszczy´nski
1999); in the following, we brieﬂy recall syntax and semantics of the formalism.

The language of ASP is based on rules, allowing (in general) for both disjunction in rule heads
and nonmonotonic negation in the body. It is worth recalling that a signiﬁcant amount of work
has been carried out by the scientiﬁc community for extending the basic language, in order to
increase the expressive power and improve usability of the formalism. This has led to a variety of
ASP “dialects”, supported by a corresponding variety of ASP systems1 that only share a portion
of the basic language; notably, the community relatively recently agreed on the deﬁnition of a
standard input language for ASP systems, namely ASP-Core-2 (Calimeri et al. 2012), which
is also the ofﬁcial language of the ASP Competition series (Calimeri et al. 2012; Gebser et al.
2016; Gebser et al. 2017; Gebser et al. 2019); it features most of the advanced constructs and
mechanisms with a well-deﬁned semantics that have been introduced and implemented in the
latest years.

For the sake of simplicity, we next focus on the basic aspects of the language; for a complete
reference to the ASP-Core-2 standard, and further details about advanced ASP features, we refer
the reader to (Calimeri et al. 2012) and the vast literature.

A variable or a constant is a term. Variables are denoted by strings starting with some up-
percase letter, while constants can either be integers, strings starting with some lowercase letter
or quoted strings. An atom is a(t1, . . . , tn), where a is a predicate of arity n and t1, . . . ,tn are
terms. A literal is either a positive literal p or a negative literal not p, where p is an atom. A
disjunctive rule (or simply rule, for short) r is a formula of the form: a1 | · · · | an :– b1, · · · , bk,
not bk+1, · · · , not bm., where a1, · · · , an, b1, · · · , bm are atoms and n ≥ 0, m ≥ k ≥ 0. The dis-
junction a1 | · · · | an is the head of r, while the conjunction b1, ..., bk, not bk+1, ..., not bm is
the body of r. A rule without head literals (i.e. n = 0) is usually referred to as an integrity con-
straint. If the body is empty (i.e. k = m = 0), it is called a fact. H(r) denotes the set {a1, ..., an}
of the head atoms, and B(r) the set {b1, ..., bk, not bk+1, . . . , not bm} of the body literals. B+(r)
(resp., B−(r)) denotes the set of atoms occurring positively (resp., negatively) in B(r). A rule r
is safe if each variable appearing in r appears also in some positive body literal of r.

An ASP program P is a ﬁnite set of safe rules. An atom, a literal, a rule, or a program is
ground if no variables appear in it. According to the database terminology, a predicate occurring
only in facts is referred to as an EDB predicate, all others as IDB predicates; the set of facts of
P is denoted by EDB(P).

The Herbrand Universe and the Herbrand Base of P are deﬁned in the standard way and
denoted by UP and BP , respectively. Given a rule r occurring in P, a ground instance of r is
a rule obtained from r by replacing every variable X in r by σ (X), where σ is a substitution
mapping the variables occurring in r to constants in UP ; ground(P) denotes the set of all the
ground instances of the rules occurring in P.

An interpretation of P is a set of ground atoms, that is, an interpretation is a subset I of BP .
A ground positive literal A is true (resp., false) w.r.t. I if A ∈ I (resp., A (cid:54)∈ I). A ground negative
literal not A is true w.r.t. I if A is false w.r.t. I; otherwise not A is false w.r.t. I. Let r be a ground

1 During the years, the scientiﬁc community has been very active, and many ASP systems have been released relying on
different algorithms and solving technologies; we refer the reader to the latest available report on the ASP competition
series (Gebser et al. 2019), the therein reported references and the vast literature.

8

F. Calimeri et al.

rule in ground(P). The head of r is true w.r.t. I if H(r) ∩ I (cid:54)= /0. The body of r is true w.r.t. I if all
body literals of r are true w.r.t. I (i.e., B+(r) ⊆ I and B−(r) ∩ I = /0) and is false w.r.t. I otherwise.
The rule r is satisﬁed (or true) w.r.t. I if its head is true w.r.t. I or its body is false w.r.t. I. A model
of P is an interpretation M of P such that every rule r ∈ ground(P) is true w.r.t. M. A model
M of P is minimal if no model N of P exists such that N is a proper subset of M. The set of all
minimal models of P is denoted by MM(P).

Given a ground program P and an interpretation I, the reduct of P w.r.t. I is the subset P I of
P, which is obtained from P by deleting rules in which a body literal is false w.r.t. I. Note that
the above deﬁnition of reduct, proposed in (Faber et al. 2004), simpliﬁes the original deﬁnition
of Gelfond-Lifschitz (GL) transform (Gelfond and Lifschitz 1991), but is fully equivalent to the
GL transform for the deﬁnition of answer sets (Faber et al. 2004). Let I be an interpretation of a
program P. I is an answer set (or stable model) of P if I ∈ MM(P I) (i.e., I is a minimal model
of the program P I) (Przymusinski 1991; Gelfond and Lifschitz 1991). The set of all answer sets
of P is denoted by ANS(P).

Example 3.1
In order to appreciate declarativity and expressiveness of ASP, let us consider the well-known
NP-complete 3-Coloring problem. Given a graph, we must decide whether there exists an assign-
ment of one out of three colors (red, green, or blue, for instance) to each node such that adjacent
nodes always have different colors. If we suppose that the graph is represented by a set of facts
F consisting of instances of the unary predicate node(X) and of the binary predicate arc(X,Y ),
then the following ASP program (in combination with F) describes all 3-colorings (as answer
sets) of that graph.

r1: color(X,red) | color(X,green) | color(X,blue) :- node(X).
r2: :- color(X1, C), color(X2, C), arc(X1, X2).
Rule r1 expresses that each node must either be colored red, green, or blue2; due to mini-
mality of answer sets, a node cannot be assigned more than one color. The subsequent integrity
constraint checks that no pair of adjacent nodes (connected by an arc) is assigned the same color.
Thus, there is a one-to-one correspondence between the solutions of the 3-Coloring problem
and the answer sets of F ∪ {r1, r2}: the graph is 3-colorable if and only if F ∪ {r1, r2} has some
answer set.

4 Framework and Methodology

In this section we present a framework to support the analysis of neurological disorders evolution.
It is worth noting that the framework is intended to be rather general, and can be adapted to
several kind of disorders and, as it will be shown in Section 8, it can be even extrapolated to
other scenarios. Here, we focus on MS disorder. We ﬁrst introduce the general workﬂow of the
framework, and then illustrate its components in abstract terms. In Section 5 we consider different
use cases, and provide some specializations of the various modules.

The general workﬂow is presented in Figure 1. Intuitively, it takes a brain representation as in-
put, classiﬁes the current stage of the disease, and then simulates the effects of the disease course
by “damaging the brain”; the newly obtained brain representation is then classiﬁed, and used for

2 The same piece of knowledge can be equivalently expressed by means of choice rules, see (Calimeri et al. 2012).

Theory and Practice of Logic Programming

9

Fig. 1: Architecture of the proposed framework

the next steps. These steps are iterated until some condition holds or until a misclassiﬁcation is
detected; in the ﬁrst case, the overall set of results is provided as output; in the latter case, the
execution of the framework is aborted.

More in detail, the framework takes as input a graph representation of the brain of a patient,
which can be obtained starting from a combination of MRI acquisition methods. As pointed out
in Section 3, this is expected to be a weighted graph G0 = (V, E0, ω0) representing the brain
connectome of the patient; this graph is then processed by the Classiﬁer module.

The Classiﬁer module can be formally modelled as a function χ : G → R4. In particular, it
takes as input the graph Gi of the i-th iteration of the framework, which represents the possibly
modiﬁed brain connectome of a patient, and outputs four real values, each indicating the proba-
bility of the input graph of expressing a speciﬁc MS stage, namely Resulti = (PCISi, PRRi, PPPi, PSPi).
Here, PCISi (resp., PRRi, PPPi, PSPi) corresponds to the probability of Gi of representing a CIS
(resp., a RR, a PP, a SP) MS stage (see Section 3). The stage associated with Gi is implicitly the
one showing the highest probability.

As previously pointed out, the classiﬁcation task can be carried out in several ways. One
possibility is to make use of results from graph theory (Rubinov and Sporns 2010; Kocevar
et al. 2016; Shovon et al. 2017); however, despite network analysis applied on brain connectivity
represents a powerful tool, it is not possible yet to deﬁne precise biomarkers to classify subjects,
especially in the MS context. As far as current classiﬁcation methods for MS are concerned, there
is a clear distinction between approaches that are based on ANN and those that use different
learning methods, such as Support Vector Machines (SVM). ANN demonstrated to be one of the
most promising tools for the analysis and classiﬁcation of images, and has been used in a wide
range of applications even if, to date, image analysis via ANN in the context of MS has been
exploited mostly for the identiﬁcation of MS lesions rather than MS proﬁle classiﬁcation. As
usually done in similar contexts, in the approaches studying MS via ANN the ground truth for
training purposes is obtained by annotations on the data directly provided by human experts. The
ANN training phase is a pre-processing step propaedeutic to the application of the framework.

Depending on the actual implementation of the classiﬁer, this module may also provide some
insights on the classiﬁcation process that could be of use in the subsequent steps. This case
will be detailed in Section 5. Intuitively, the speciﬁc ANN we adopt in the specialization of
our framework allows to leverage the topology of the graph for the classiﬁcation process and
to compute a meaningful coefﬁcient for each graph edge, in the form (x, y, impxy), representing
the importance of the edge (x, y) in the classiﬁcation task. These insights can be exploited in the

‹Gi, Resulti›G0ClassifierASP Input FormatterBrain EvolutionSimulationClassifierInput FormatterGi+1ClassificationValidityCheckGi+1ExitConditionDegradationCoefficientsClassificationInsightsyesnoResultiGiGi‹Gi-1, Resulti-1›‹Gi, Resulti›ok‹Gi, Resulti›failSTOP: InvalidResultSet{‹G0, Result0›,‹G1, Result1›,‹Gn, Resultn›}…10

F. Calimeri et al.

simulation of brain evolution and are expressed in Figure 1 by the direct connection between the
Classiﬁer module and the Brain Evolution Simulation module.

The output of the classiﬁcation is veriﬁed by a Classiﬁcation Validity Checker. One of the
recently arising research issues in classiﬁcation tasks is the automatic check of validity of results.
Indeed, veriﬁcation of ANN results is receiving increasing interest, and it is currently a very
active research area (Pulina and Tacchella 2010; Kouvaros and Lomuscio 2018; Leofante et al.
2018). However, given the nature of the problem we are addressing in this paper, we can simplify
this task by resorting to rules and constraints that model domain knowledge.

In particular, if certain clinical evidence is available for a patient, this can be used to limit
classiﬁcation alternatives. As an example, it is well known that a brain model obtained by a
severe disruption of its previous structure cannot induce a remission of the pathology. In other
words, given a brain structure Gi−1 classiﬁed, e.g., as RR, and given its modiﬁed version Gi
obtained by a strong reduction of arcs and/or weights, if Gi is classiﬁed as CIS, i.e., a remission
is hypothesized from Gi−1 to Gi, the classiﬁcation is evidently wrong and must be discarded.
This kind of checks can be easily carried out through suitable sets of rules and constraints; these
will be presented in detail in Section 5. As a side note, if the starting input graph G0 is annotated
with the ground truth provided by an expert on the right classiﬁcation of the initial MS stage, this
step can stop the process at the very ﬁrst iteration if the classiﬁcation result disagrees with the
ground truth. In order to provide the input to the checker in the proper formalism, an ASP Input
Formatter module stands between the classiﬁer and the checker. It is in charge of translating the
output provided by the classiﬁer and the current graph into ASP facts.

The Classiﬁcation Validity Checker can be modelled in abstract terms as a function ν : G ×
R4 × G × R4 → {“OK”,“FAIL”} which takes as input two graphs, Gi−1 and Gi and the corre-
sponding classiﬁcation results Resulti = (PCISi, PRRi, PPPi, PSPi) and Resulti−1 = (PCISi−1, PRRi−1,
PPPi−1 , PSPi−1); it provides as output one among the two possible values “OK” or “FAIL”, de-
pending on the outcome of the check. If the Classiﬁcation Validity Checker returns “FAIL”, the
iterative process is immediately stopped and current and previous results are invalidated. On the
contrary, if the checker returns “OK”, the execution of the framework proceeds to the next steps.
In particular, a generic Exit Condition is subsequently checked in order to verify whether it
is necessary to proceed with the next iteration of the framework or not. The deﬁnition of such
condition strongly depends on the objective of the analysis. As an example, it could be interesting
to check if a certain target probability is reached for a certain MS stage, or if a certain degree of
disruption of the original graph has been induced in the last step, or simply if the required number
of iterations has been carried out. If the exit condition is veriﬁed, the execution is stopped and
the set {(cid:104)G0, Result0(cid:105), (cid:104)G1, Result1(cid:105), . . . (cid:104)Gn, Resultn(cid:105)} of graphs and corresponding classiﬁcation
results are provided as output. Otherwise, the execution proceeds with the next brain evolution
simulation step.

The Brain Evolution Simulation module can be formally modelled as a function µ : G × 2E ×
2E → G ; here, G is the set of all possible graphs, whereas 2E represents all possible sets of triples
of the form (x, y, cxy), where x and y are nodes of the graph and cxy is a label. In particular, µ takes
as input the current graph Gi, and two sets of triples (x, y, impxy), and (x, y, dcxy) which convey,
respectively, information on the importance of each edge for the classiﬁcation task, whenever
provided by the classiﬁer, and information on how to modify edge weights during the simulation.
We formally introduce and specialize these sets is Section 5. µ provides as output a new graph
Gi+1 which represents a simulated evolution of brain structure.

Theory and Practice of Logic Programming

11

Some metrics over graphs representing brain structures have been considered in previous stud-
ies on MS (Rubinov and Sporns 2010); however, it is still unclear how these metrics inﬂuence
the progress of the disease. In our framework, the Brain Evolution Simulation module is used
to explore a wide variety of graph modiﬁcation criteria. In this context, ASP plays a very rele-
vant role as a fast and effective tool for the deﬁnition and identiﬁcation of subgraphs satisfying
some predeﬁned property that could be involved in MS course; in some cases, the identiﬁca-
tion of such subgraphs may involve the solution of optimization problems, whose coding can
be signiﬁcantly time-consuming in other programming paradigms. In our framework, the Brain
Evolution Simulation module is composed of an ASP program of choice that, given the graph
Gi, ﬁrst deﬁnes a connectome modiﬁcation criterion described by a set of edges to modify and
then produces the new graph Gi+1. The corresponding ASP program(s) enjoy the nice properties
of such a declarative formalism, resulting very ﬂexible and easy to adapt to small changes in the
desiderata.

Each ASP program is coupled with an extensional knowledge base consisting of a set of facts
representing nodes and edges of Gi, and identiﬁes a set of atoms which represent the set of
edges E(cid:48) to be modiﬁed. Given that, in our context, edge weights are related to the number of
ﬁbers linking two points in the brain, an edge (x, y, w) with w = 0 is considered inactive and
not contributing to the network, i.e., the corresponding nodes are considered not connected. If
available, the choice of E(cid:48) can also be guided by the information about the importance of each
edge for the classiﬁcation task; this information is expressed by the ﬁrst set of triples (x, y, impxy)
provided as input along Gi.

As already noted, different ways of altering the brain structure using E(cid:48) can be devised. A
basic altering method could consist in simply removing these arcs; this corresponds to set the
weight of each edge in E(cid:48) to 0. However, edge weights play an important role for the structure
itself and, from a biological point of view, the strength of the connections (expressed by edge
weights in our model) progressively decreases while the brain degenerates. As a consequence,
possible evolution strategies might include progressive variations of selected edge weights, as
expressed by the second set of triples (x, y, dcxy) provided as input. All the edges not included in
E(cid:48) are simply copied into Gi+1.

Examples of interesting criteria for identifying the set of edges to be modiﬁed, and that will

be detailed in Section 5, are reported next:

(i) Max Clique: contains the greatest subset of vertices in G such that every two distinct ver-

tices in the clique are connected by an edge;

(ii) k-hub: the set of k nodes having the highest degree;
(iii) Min Vertex Cover: the smallest set of vertices MVC such that each edge of the graph is

incident onto at least one vertex of the set;

(iv) Density reduction: the minimal set of edges that, if removed, allows a reduction of the

graph density by a given amount;

(v) Assortativity increase: the minimal set of edges that, if removed, allows an increase of the

graph assortativity by a given amount.

It is important to note that, as already mentioned and as it will be clearer in the following,
switching between these properties or slightly modifying the criteria in ASP requires just to
change a few rules; on the contrary, using a classical imperative programming scheme, it would
require to rewrite and adapt source code that can be signiﬁcantly harder to maintain.

The newly obtained graph Gi+1 is given as input back to the Classiﬁer; the ASP representa-

12

F. Calimeri et al.

tion of Gi+1 is translated back into the format required by the Classiﬁer by the Classiﬁer Input
Formatter module.

5 Specializations of the framework

In this section we specialize the framework to three biologically relevant settings, upon which
we also carry out some experiments in Section 6. Some of the modules are implemented in the
same way for all the three specializations, and are hence described ﬁrst.

Before the actual description, it is worth pointing out, once again, that the goal of the present
work is not to provide clinical validation of some kind of results; rather, we want to show the
potential of the herein proposed framework that, thanks to the combined use of ASP and ANN,
can help experts in studying the evolution of the disease from different perspectives.

5.1 From MRI to Graphs

This is a pre-processing step needed to transform brain images into graphs. Starting from a
combination of MRI acquisition methods like T1 and DTI (see Section 3.1), it is possible to
extract structural connectivity matrices, representing topological features of the brain (Rubinov
and Sporns 2010; Kocevar et al. 2016). The graph generation pipeline used to extract a brain
network can be divided in three main steps:

1. Parcellation of the cortical and sub-cortical grey matter (GM) is performed on 3D T1-
weighted images in order to label T1 voxels in four groups [white matter (WM), cortical
GM, sub-cortical GM, cerebro-spinal ﬂuid (CSF)] and then to deﬁne graph nodes.

2. Diffusion weighted MRI images are preprocessed using correction of Eddy-current dis-
tortions (Jenkinson et al. 2012) and skull stripping. Probabilistic streamline tractography
(Tournier et al. 2012) on diffusion images is then applied to generate ﬁber-tracks in voxels
labeled as WM voxels.

3. The connectivity matrix A ∈ Rq×q (q = 84) is generated for each subject. More formally,
A represents the adjacency matrix of the weighted undirected graph G = (V, E, ω) where V
is the set containing the segmented GM brain regions (with |V | = q), E is the set of graph
edges deﬁned as:

E = {{i, j} | ω(i, j) > 0, 1 ≤ i, j ≤ q}
and ω : N2 → [0, 1] is a function that measures the strength of the connection between
a pair of nodes by summing the number of streamlines connecting them and scaling this
number in the range [0, 1].3

Figure 2 illustrates the whole process.

5.2 Specialization of the Classiﬁer

The Classiﬁcation of MS patients in their respective clinical forms is achieved by means of a
slightly modiﬁed version of BrainNetCNN (Kawahara et al. 2017) we speciﬁcally deﬁned for
this work. BrainNetCNN is a convolutional neural network (CNN) framework which operates

3 It is worth observing that, since current ASP systems do not support real numbers, the ASP Input Formatter scales

values in the real interval [0, 1] into integer values between 0 and 100.

Theory and Practice of Logic Programming

13

Fig. 2: Illustration of the graph creation steps: (A) T1 and diffusion weighted MR images are
used to generate cortical parcellation and ﬁber tractography (B), which are combined to generate
connectivity matrix (C)

on brain connectivity. Differently from the spatially local convolutions done by traditional CNN,
BrainNetCNN is designed to exploit topological locality of structural brain networks. This result
is achieved by using two novel operators: the Edge2Edge layer, which performs a convolution
over the weights of edges that share nodes together, and the Edge2Node layer, which computes,
for each node i, a weighted combination of the incoming and outgoing weights of edges con-
nected to i.

The BrainNetCNN architecture parameters originally proposed by Kawahara et al. were kept
unchanged since they already showed promising results in predicting clinical neurodevelopmen-
tal outcomes from brain networks. The model takes as input the adjacency matrix representation
of a graph G (the brain connectivity matrix) and outputs a probability value for each MS stage.
The architecture is composed of two Edge2Edge layers, which process the input using 32 ﬁlters,
followed by two Edge2Node layers with 64 and 256 ﬁlters. Then, two fully connected layers of
size 128 and 30 are applied. Both convolutional and fully connected layers use Leaky ReLU (α =
0.33) activation function. Finally, a fully connected layer of size 4 (the output layer) with softmax
activation is used to perform the classiﬁcation.

In Section 6.1.1 we show that this new version of the classiﬁer considerably increases reliabil-
ity with respect to previous proposals (Calimeri et al. 2018). This is important in our framework,
as the precision of the classiﬁcation over unseen samples is fundamental in studying MS evolu-
tion over brain alterations. Moreover, as it will be speciﬁed in Section 5.5.3, this ANN allows to
compute a coefﬁcient for each graph edge that represents its importance in the classiﬁcation task.

5.3 Specialization of the Classiﬁcation Validity Checker

The Classiﬁcation Validity Checker is implemented via an ASP encoding. Due to pace reasons,
we refrain from discussing in detail this and the following encodings, but we point out that they
comply with the ASP-Core-2 standard; the interested reader can refer to Section 3.2 and the vast
literature on Answer Set Programming for more details.

For the purpose of this work we implement a very basic checker that can be easily enriched
with more domain speciﬁc knowledge; this is shown in Figure 3. In particular, it takes as input
two graphs Gi and Gi−1 encoded by a set of facts of the form edge(X,Y,W) and edge 1(X,Y,W),
respectively; it takes as input also a threshold T , used to determine whether a severe disruption

14

F. Calimeri et al.

e.g., result("CIS", 90). result("RR", 25). result("PP", 15). result("SP", 10).

% input: facts of the form "edge(X,Y,W)" and "edge_1(X,Y,W)" as the input graphs
% input: facts of the form "result(STAGE,P)",
%
%
% input: facts of the form "result_1(STAGE,P)",
representing the classificatoin results for the previous graph
%
% input: a fact of the form "Th(T)" indicating the minimum number of removed arcs
representing a severe disruption according to domain knowledge
%

representing the classification results for the current graph

% determine if a severe disruption occurred in the last iteration
severedisruption :- #count{X,Y: edge_1(X,Y,W1), edge(X,Y,W), W1>0, W=0}>T, Th(T).

% check for validity of the classification step
check("FAIL") :- severedisruption, result("CIS",R_CIS), result_1(S,R_1), S!="CIS",

#max{ R : result(_,R)} = R_CIS,

#max{ R : result_1(_,R)} = R_1.

check("OK") :- not check("FAIL").

Fig. 3: An ASP encoding for the Classiﬁcation Validity Checker.

occurred and the results obtained by the classiﬁer at steps i and i − 1, encoded as facts of the form
result(STAGE, P) and result 1(STAGE, P), where STAGE ∈ {“CIS”, “RR”,“PP”, “SP”}
and P represents the probability computed for the corresponding stage by the classiﬁer.

The encoding ﬁrst determines whether a severe disruption occurred in the brain model between
Gi−1 and Gi by counting the number of arcs that have been removed from Gi−1 and comparing
it with a threshold. If this happens, it checks whether a known impossible transition has been
inferred by the classiﬁer between step i − 1 and step i. As an example, in presence of a severe
disruption, it is well known that a transition from RR (resp., from PP, or SP) to CIS is biologically
implausible (Lublin et al. 2014). If a known impossible transition is detected, the computed
answer set contains check("FAIL"); otherwise, the answer set contains check("OK").

5.4 Specialization of the Exit Condition

The exit condition can be specialized in several ways. As an example, it can stop iterations as
soon as it detects that the classiﬁer predicts a transition from one MS stage to another for the cur-
rent patient, for instance when a patient starts from a CIS stage and, after some modiﬁcations to
the brain structure, she is classiﬁed as RR. This would imply that modiﬁcations simulated to the
brain structure are sufﬁcient to simulate a transition in the pathology of the patient. Analogously,
the exit condition can stop the iterations of the framework whenever the difference between pre-
dicted probabilities become very low (i.e., below a certain threshold) from one iteration to the
other; this may imply that the last modiﬁcations simulated on the brain structure are no more
informative.

In our framework, we exploited a simple exit condition which stops the execution after a

certain number of iterations.

5.5 Specializations for three different use cases

5.5.1 Specialization for studying structural properties

A ﬁrst interesting use case for our framework is the study of the impact of graph structural prop-
erties, and their modiﬁcation, in the evolution of MS. The aim is to determine whether there is
a latent relation between the presence/absence of particular graph structures in the connectome

Theory and Practice of Logic Programming

15

and the stage of the MS disease. In particular, we are interested in verifying if and how modiﬁ-
cations on the connectome of a patient, simulated by modiﬁcations on the graph representing it,
can modify the classiﬁcation returned by the ANN. Observe that, as a side effect, understanding
these relations could provide at least partial motivations for ANN classiﬁcations; this is still an
open issue in ANN.

In this use case, the Brain Evolution Simulation module must be specialized to detect the
structural property of interest, and in particular the set of edges representing it, and to generate a
new connectome by modifying selected edges.

Interesting criteria and returned edges are the following: (i) Max Clique, i.e., the greatest subset
of vertices in G such that every two distinct vertices in the set are adjacent; in this case, the
module modiﬁes the edges E(cid:48) linking the vertices in the clique. (ii) Independent Set, i.e., the
greatest subset of vertices in G such that no two vertices in the set are adjacent; in this case the
module modiﬁes the edges E(cid:48) having exactly one vertex in the independent set. (iii) Max-degree
node, i.e., the node showing the maximum degree in G; in this case the module modiﬁes the
edges connected to it. (iv) k-hub, i.e., the set of k nodes having the highest degree; in this case
the module modiﬁes the edges connected to the k-hub. (v) Min Vertex Cover, i.e., the smallest set
of vertices MVC such that each edge of the graph is incident onto at least one vertex of the set;
in this case the module modiﬁes the edges E(cid:48) such that both vertices of the edge are in MVC.

Each version of the Brain Evolution Simulation module is then obtained by a suitable ASP
encoding detecting the property and the corresponding edges. As an example, Figure 4 reports
an ASP encoding for the Max Clique problem.

Intuitively, the program “guesses” the nodes that belong to a clique in the graph Gi by means

of the choice rule:

{clique(X)} :- node(X)

and then checks, by means of the strong constraint:

:- clique(X), clique(Y), X < Y, not activeEdge(X,Y)

that the inclusion of two unconnected nodes in the candidate clique set is forbidden. Cardinality
of the clique is maximized using the weak constraint

:~ node(X), not clique(X). [1@1,X]

that penalizes the exclusion of a node in the candidate clique set.

The set of the edges connecting the nodes within the resulting clique is represented by the ex-
tension of predicate e(X,Y,W), which is built according to the rule e(X,Y,W) :- edge(X,Y,W),
clique(X), clique(Y). The new modiﬁed graph Gi+1 is built with the last two rules appear-
ing in the encoding. In particular, all the edges that must not be modiﬁed are just copied in the
new graph Gi+1 (see the last but one rule). The last rule simulates the progressive disrupting
process of the MS disease on the portion of brain connectome identiﬁed by the extension of
predicate e(X,Y,W); speciﬁcally, we designed it in order to act as a degradation function on
the weights of selected edges; this simulates a degradation in the strength of the connections.
In particular, given the initial graph G0 a degradation coefﬁcient is computed for each edge
(x, y, wxy) in G0 as dxy = wxy × p, where p is a percentage of degradation, set as a parameter
for the experimentation. Degradation coefﬁcients are given as input to the program as facts of
the form dc(X,Y,D) and computed as a preprocessing step before starting framework iterations.
Then, each edge e(X,Y,Wxy) generates in Gi+1 an edge edge1(X,Y,max{Wxy-Dxy,0}) (see
the last rule). Here, a weight set to 0 means a deletion of the edge from the resulting graph (it
will no longer be an activeEdge), and consequently a complete disruption of the correspond-
ing connection; in this case, the subsequent iterations and the corresponding ASP programs will

16

F. Calimeri et al.

% input: facts of the form "node(X)" and "edge(X,Y,W)" as the input graph
% input: facts of the form "dc(X,Y,D)" as degradation coefficients for all edges
% input: support atom "zero(0)"

% guess the clique
{ clique(X) } :- node(X).

% take into account only active edges
activeEdge(X,Y) :- edge(X,Y,W), W>0.
:- clique(X), clique(Y), X < Y, not activeEdge(X,Y).

% maximize clique cardinality
:~ node(X), not clique(X). [1@1,X]

% edges to be modifed in the new graph
e(X,Y,W) :- edge(X,Y,W), clique(X), clique(Y).

% define the new graph
edge1(X,Y,W) :- edge(X,Y,W), not e(X,Y,W).
edge1(X,Y,NW) :- e(X,Y,W),

#max{K : e(X,Y,W), dc(X,Y,D), K=W-D ; 0 : zero(0)} = NW.

Fig. 4: An ASP encoding for the Max Clique problem.

no longer consider this edge as belonging to the graph. In our experiments, we considered both
p = 25% and p = 50%.

The value of p heuristically determines the intensity of degradation applied to the strength of
the connections in one iteration and, consequently, the velocity of degradation of the connec-
tome through the iterations of the framework. Thus, the choice of p determines the velocity of
the simulation. In particular, if we assume that p = 25% and that the same edge is chosen at
each iteration4, it takes four iterations to virtually remove it from the connectome. Analogously,
when p = 50%, and the same edge is chosen at each iteration, two iterations of the framework
are sufﬁcient to virtually remove it. As a consequence, the choice of p strictly depends on the
granularity of degradations one wants to study and on the maximum number of iterations of the
framework that one wants to carry out.

All ASP encodings for considered criteria can be found at https://www.mat.unical.it/

calimeri/material/mix-lp-nn/.

5.5.2 Specialization for studying graph metrics

In (Kocevar et al. 2016) a relationship between some graph metrics, e.g., Density and Assortativ-
ity, and the MS stage of a patient has been clinically demonstrated (see Figure 5 for a summary).
It is then interesting to evaluate whether the evolution of the disease could be also related to pro-
gressive modiﬁcations (decrease/increase) of such metrics obtained by modiﬁcations in the cor-
responding graphs. It is worth pointing out that a change in these metrics is not always related to
speciﬁc substructures and, generally, depends on presence/absence of edges. As a consequence,
the setting introduced in the previous section cannot be applied in this new context. Moreover,
while searching for a variation in a metric, as an example a decrease in density, it is important

4 Observe that this is not obvious, and it strongly depends on both the studied property and the conﬁgurations of the

other edges.

Theory and Practice of Logic Programming

17

Fig. 5: Summary of results on the correlation between Density and Assortativity and MS stages.

to avoid trivial modiﬁcations such as the removal of all the edges. It is also worth noting that,
for some metrics such as assortativity, the removal of edges may induce either a decrease or an
increase in the property. As a consequence, it is crucial to look at minimal graph changes al-
lowing to reach a certain goal in metric variation. Also in this context, the expressiveness and
compactness of ASP and its extensions allow for very elegant and readable encodings even for
optimization problems, i.e., the identiﬁcation of minimal changes involving the computation of
complex metrics.

Among the metrics considered in (Kocevar et al. 2016), we focus on Density and Assortativity.

In order to keep the paper self contained, we next recall the basic deﬁnition of these metrics.

The density d of a graph G = (cid:104)V, E(cid:105) is deﬁned as:

where |E| is the number of edges in G and |V | represents the number of nodes in G.

d =

|E|
|V |(|V | − 1)

18

F. Calimeri et al.

Assortativity measures the similarity of connections in the graph with respect to the node
degree. In particular, the formula for computing the assortativity degree of a graph G is deﬁned
as (Newman 2002):

r =

∑xy(xy(exy − axby))
σaσb

where x and y are values of node degrees for G, exy is the fraction of all edges in G joining
vertices having degree values x and y, and ax = ∑y exy, by = ∑x exy. Moreover, σa and σb are the
standard deviations of the distributions ax and by.

The formulas above, especially the one for assortativity, show that the computation of graph
metrics can be in general not easy to be carried out using only rules in an ASP program. As
a consequence, we resort to recent extensions of ASP systems, such as I-DLV/DLV2 (Calimeri
et al. 2017b; Adrian et al. 2018), DLVHEX (Eiter et al. 2016; Calimeri et al. 2016; Eiter et al.
2018), etc., which allow the integration of external computation sources within the ASP program.
In particular, the problem at hand requires to send a (possibly guessed) entire graph, i.e., a set
of edges, to an external source of computation. The ASP standardization group has not released
standard language directives yet for such features; here, we make use of syntax and semantics of
DLVHEX (Eiter et al. 2016), while a slightly different formulation must be used to comply with
I-DLV/DLV2 (Calimeri et al. 2017b; Adrian et al. 2018) or clingo (Gebser et al. 2019) syntax.

Figure 6 shows the specialization of the Brain Evolution Simulation module for deriving the
minimal changes to perform on a graph Gi in order to obtain a decrease in the measure of a
certain property (by 10% by edge removal in the example).

In particular, the program of Figure 6 ﬁrst deﬁnes the current value of the metric on the in-
put graph; this is done with the support of a call (via “external” atom &computeMetric) to an
external function written in an imperative programming language, e.g., Python. Then, it deﬁnes
the target value for the metric. Hence, the program reports the guess for a subgraph that satisﬁes
the goal, where the number of removed edges, expressed by the edges not in the subgraph, is
minimized by the weak constraint. Finally, the new graph Gi+1 is generated by the last two rules.
Speciﬁcally, edges of Gi in the guessed subgraph are copied into Gi+1, whereas all the other
edges are removed by setting their weight to 0. The changes to be performed on the program for
analyzing an increase of the metric are straightforward.

5.5.3 Specialization for exploiting ANN insights

As previously pointed out, the speciﬁc ANN presented in Section 5.2, leverages the topology
of the graph for the classiﬁcation process. Furthermore, the particular structure of the network
allows to estimate a meaningful coefﬁcient for each graph edge representing the importance of
that edge in the classiﬁcation task. Speciﬁcally, in order to deﬁne an importance coefﬁcient for
each input edge of the brain connectivity, we used the method formerly proposed by Simonyan et
al. (Simonyan et al. 2013) and already applied to BrainNetCNN by Kawahara et al. (Kawahara
et al. 2017).

Let pc(G) be the score assigned to the class c by the trained classiﬁcation layer of the ANN for
a given graph G = (V, E, ω). Then, the contribution of each edge e ∈ E can be estimated based on
its inﬂuence to the score pc(G). More in detail, the edge importance is computed by the partial
derivative δ pc(G)

for each edge e ∈ E by backpropagation (Simonyan et al. 2013).
Then, given a graph G, let us assume we can derive from the output of the Classiﬁer module a
set of facts of the form imp(X,Y,P), where P is the importance in the ANN of the edge (X,Y )

δ e

Theory and Practice of Logic Programming

19

% input: facts of the form "node(X)" and "edge(X,Y,W)" as the input graph
% compute the starting value of the metric and the corresponding threshold
target(X) :- &computeMetric[activeEdge](Y), X=Y/90.

% take into account only active edges
activeEdge(X,Y) :- edge(X,Y,W), W>0.

% guess the new graph
0 < { in(X,Y): activeEdge(X,Y) }.

% check that the goal is reached
:- &computeMetric[in](X), target(K), X > K.

% minimize removed edges
:~ activeEdge(X,Y), not in(X,Y). [1@1,X,Y]

% define the new graph
edge1(X,Y,W) :- in(X,Y), edge(X,Y,W).
edge1(X,Y,0) :- edge(X,Y,W), not in(X,Y).

Fig. 6: An ASP encoding for analyzing the decrease of a graph metric.

given by the formula introduced above. It is strongly interesting to study the impact over MS
clinical course of both structural properties over graphs and graph metrics also taking into ac-
count these insights from the ANN. Indeed, determining that there are sets of edges that could be
ignored without losing important information or, conversely, that it is possible to focus on a small
subset of edges, could both signiﬁcantly help experts in their analyses and reduce computational
requirements.

The specialization of the framework taking into account edge importance is quite straight-
forward. In particular, let us use a threshold T to distinguish between important (P ≥ T ) and
unimportant (P ≤ T ) edges. Consider the ASP program shown in Figure 7; this is substantially
the same as the one shown in Figure 4 for studying cliques, except for the second rule, which
forces important edges (if P ≥ T is used) or unimportant edges (if P ≤ T is used) to belong to the
clique. The threshold T can be dynamically set. Observe that putting unimportant edges in the
clique, means that the graph will be modiﬁed on unimportant parts only (as judged by the ANN).
Differently from what we have shown in Figure 4, the last two rules state that, in the new graph,
the weights of edges identiﬁed by the clique are set to 0 and, hence removed.

Analogously, Figure 8 shows the specialization for graph metrics by minimizing/maximizing
the use of important edges in reaching the goal for graph metrics variation. Again, there is a
minimal difference consisting in the introduction of a weak constraint that minimizes the use of
important/unimportant edges.

6 Experiments

This section reports about the experiments we carried out in order to assess the proposed frame-
work and its specializations. The section is organized in three parts. The aim of the ﬁrst part is
to show the ﬂexibility of the framework, and to adapt the analysis to different perspectives. In
particular, we focus on analyzing how classiﬁcation probabilities vary during the brain evolution
simulation, in order to verify appropriateness of the approach in the biomedical context herein
discussed. The second part speciﬁcally focuses on performance, while the third part is devoted

20

F. Calimeri et al.

% input: facts of the form "node(X)" and "edge(X,Y,W)" as the input graph
% input: facts of the form "imp(X,Y,P)" denoting the importance of edge(X,Y,W)
% input: a fact of the form Th(T) used to discriminate between important and
%
% guess the clique
{ clique(X) } :- node(X).

unimportant edges

% take into account only active edges
% if P <= T unimportant edges are in the clique,
% if P >= T important edges are in the clique
activeEdge(X,Y) :- edge(X,Y,W), W>0, imp(X,Y,P), Th(T), P <= T.
:- clique(X), clique(Y), X < Y, not activeEdge(X,Y).

% maximize clique cardinality
:~ node(X), not clique(X). [1@1,X]

% edges to be modifed in the new graph
e(X,Y,W) :- edge(X,Y,W), clique(X), clique(Y).

% define the new graph
edge1(X,Y,W) :- edge(X,Y,W), not e(X,Y,W).
edge1(X,Y,0) :- e(X,Y,W).

Fig. 7: An ASP encoding for the Max Clique problem maximizing the use of impor-
tant/unimportant edges.

% input: facts of the form "node(X)" and "edge(X,Y,W)" as the input graph
% input: facts of the form "imp(X,Y,P)" denoting the importance of edge(X,Y,W)
% input: a fact of the form Th(T) used to discriminate between important and
%
% compute the starting value of the metric and the corresponding threshold
target(X):- &computeMetric[activeEdge](Y), X=Y/90.

unimportant edges

% take into account only active edges
activeEdge(X,Y):- edge(X,Y,W), W>0.

% guess the new graph
0 < { in(X,Y): activeEdge(X,Y)}.

% check that the goal is reached
:- &computeMetric[in](X), target(K), X > K.
:~ activeEdge(X,Y), not in(X,Y). [1@2,X,Y]

% In the next constraint,
% if P >= T maximizes removal of unimportant edges,
% if P <= T maximizes removal of important edges
:~ activeEdge(X,Y), not in(X,Y), imp(X,Y,P), Th(T), P >= T. [1@1,X,Y]

% define the new graph
edge1(X,Y,W) :- in(X,Y), edge(X,Y,W).
edge1(X,Y,0) :- edge(X,Y,W), not in(X,Y).

Fig. 8: An ASP encoding for analyzing the decrease of a graph metric maximizing the use of
important/unimportant edges.

to the discussion of obtained results and to outline some lessons learned thanks to experiments
outcome.

Theory and Practice of Logic Programming

21

6.1 Experiments on the application of the framework to simulate MS evolution

In the following, we ﬁrst introduce the dataset and the preprocessing steps. Then, we report and
discuss obtained results for each specialization introduced in Section 5. It is worth noting that
we developed also a web tool for supporting experts in carrying out their analyses online through
our framework; such tool will be discussed in Section 7.

6.1.1 Dataset Description and Preprocessing Steps

Data have been obtained from subjects recruited from the MS clinic of Lyon Neurological Hos-
pital (France). This prospective study was approved by the local ethics committee (CPP Sud-Est
IV) and the French national agency for medicine and health products safety (ANSM). Written
informed consent was obtained from all subjects. Diagnosis and disease course, constituting the
ground truth, were established according to the McDonald’s criteria (Lublin et al. 2014; McDon-
ald et al. 2001).

Structural connectivity matrices were extracted for each subject. A total of 578 samples (dis-
tributed into the four aforementioned categories as 63 CIS, 199 RR, 190 SP, 126 PP, respectively)
were considered for the experiments overall, and for each sample the corresponding graph G has
been extracted as explained in Section 5.1. Ground truth on the correct MS stage of all the sam-
ples is available, as it has been provided by expert physicians. Each graph consists of 84 vertices
with an average of 2036.31 ± 139.19 edges for the samples in CIS, 1951.25 ± 235.43 in RR,
1634.56 ± 315.27 in SP and 1760.96 ± 293.58 in PP.

The ANN introduced in Section 5.2 has been trained before starting the experiments on the
framework, by cross validation with 3 folds, using 70% of the samples in each fold for training
the model and the remaining 30% as test set for validation. The quality of the classiﬁcation
was evaluated by means of the average Precision, Recall and F-Measure achieved during the
cross validation, as usual in the literature. The proposed ANN was trained using Adam (Kingma
and Ba 2014) with learning rate 0.001. Early Stopping was used to prevent overﬁtting. Average
evaluation of the cross validation is shown in Table 1. It can be observed how the ANN we
designed for this work is particularly effective in determining the right stage of the pathology
under consideration. This is a crucial factor in the framework, as studying the impact of the
variations in the connectome on the course of the disease requires a very high precision in the
classiﬁcation step. Observe that, even if augmenting the classiﬁcation accuracy is beyond the
scope of this work, the use of well-performing networks allows to obtain more reliable results.

Notably, the new classiﬁcation model adopted in this paper allows to reach an average F-
Measure of 88%, which represents a signiﬁcant improvement with respect to the 80% reached
in (Calimeri et al. 2018) for the same quality measure. It is important to point out that results
shown in Table 1 are slightly lower than previous results obtained in (Calimeri et al. 2018); nev-
ertheless, we consider these slightly lower values deﬁnitely acceptable, as the new ANN used
in the present work allows a meaningful and more interpretable representation of the classiﬁca-
tion process, and this is particularly useful in the new version of the framework. Indeed, Brain-
NetCNN leverages the topological locality of structural brain networks, thus performing more
meaningful operations on the graph structure with respect to the previous approach (Calimeri
et al. 2018) and allowing to compute edge importance. Furthermore, high-level features learned
by BrainNetCNN have been already discussed in the literature in the context of the anatomy and
function of the developing pre-term infant brain.

22

F. Calimeri et al.

Table 1: Average Precision, Recall and F-Measure (± standard deviation) achieved during cross
validation (3 folds). Results are computed per class (CIS, PP, RR, SP) and with respect to all the
classes (Tot).

Precision

Recall

F-Measure

CIS
PP
RR
SP

0.76 (±0.12)
0.91 (±0.04)
0.89 (±0.03)
0.90 (±0.06)

0.88 (±0.13)
0.69 (±0.17)
0.94 (±0.06)
0.93 (±0.02)

0.81 (±0.10)
0.78 (±0.10)
0.91 (±0.02)
0.92 (±0.03)

Tot

0.89 (±0.01)

0.88 (±0.01)

0.88 (±0.02)

After the training phase, in order to keep experiments on the overall framework coherent,
before starting the tests on the framework we ﬁltered out some input samples, relying on the
ground truth provided by physicians. In particular, we ﬁltered out input samples misclassiﬁed by
the trained ANN at the very ﬁrst classiﬁcation; this way, we avoid to propagate initial classiﬁca-
tion errors in the framework. As a consequence, a total of 55 CIS, 189 RR, 187 SP and 109 PP
correctly classiﬁed samples have been actually fed as input to the framework for the tests.

6.1.2 Experiments for studying structural properties

In the context of this analysis, we are interested in studying the possible variations of each stage
of the MS clinical course by modifying the connectome of a patient, according to the structural
properties introduced in Section 5.5.1. For the sake of presentation and space constraint, we
discuss in detail the results of a subset of the experiments we carried out, namely Max Clique,
Min Vertex-Cover, and k-hub, with p = 50%. Nevertheless, the complete set of results is available
at https://www.mat.unical.it/calimeri/material/mix-lp-nn/.

Figures 9, 10, and 11 report the overall results for this specialization. For each starting stage
of the pathology, we report the probability values (indicated by a group of four vertical bars, for
each iteration of the framework), computed by the ANN. In particular, from left to right, one can
observe the variation of the average probability values for each class. As an example, the ﬁrst bar
in the leftmost group of the ﬁrst bar chart in Figure 9 represents the probability associated with
the CIS stage for a CIS classiﬁed patient. The same bar in subsequent groups shows variations of
this probability through iterations 1–4 with the ASP program shown in Figure 4; the same bar in
the chart below shows variations of the same probability for patients formerly classiﬁed as RR.
In order to evaluate the signiﬁcance of obtained results, we considered also a random test for
each test case, designed as follows: at each iteration, given the number n of edges identiﬁed by the
Brain Evolution Simulation module, we generate a parallel modiﬁed graph choosing n random
edges to be modiﬁed that are not related to the structure under consideration. In other words, we
are interested in evaluating whether the variations in classiﬁcation results depend on the structure
of the modiﬁed portion of the connectome, or simply on the number of varied edges. Outcomes
of random tests are reported on the right side of Figures 9, 10, and 11.

Results show interesting variations, when testing the framework with the Max Clique crite-
rion (Figure 9). In particular, it is worth noting that Max Clique seems to affect mostly the CIS
stage, as CIS probability values signiﬁcantly decrease. Interestingly, this behaviour seems not

Theory and Practice of Logic Programming

23

to be simply related to the amount of modiﬁed edges: random tests show substantially constant
probability values across iterations. More interestingly, the aforementioned behaviour is not ob-
servable for the other stages RR, SP and PP, where the alteration of cliques does not actually
induce signiﬁcant changes. This absence of variations is not related to the absence of cliques to
change, or to their different cardinalities; indeed, the number of edges modiﬁed in all stages are
comparable being on average 258.37 ± 34.30 from iteration 1 to iteration 2, and 130.38 ± 5.34
from iteration 3 to iteration 4. The results for the CIS starting stage also show that probabilities
of PP actually increase through iterations, even if not sufﬁciently enough to allow a guess over a
change of state.

As far as Min Vertex Cover is concerned (Figure 10), signiﬁcant variations can be observed
in all stages. However, we observe in this case also a signiﬁcant decrease of the probability of
election in random tests, especially in the ﬁrst iterations. This is mainly due to the high number
of modiﬁed edges, being on average 1490.45 ± 147.12 from iteration 1 to iteration 2, i.e. 41.59%
of the total. In the subsequent iterations, very small minimum vertex cover could be identiﬁed
in the modiﬁed graphs due to the low number of remaining edges; as a consequence, very few
edges (about 0.003% of the total) are modiﬁed and, thus, very small variations on probabilities
are detected.

However, if we concentrate on non random tests, we observe a quite different and interesting
behaviour. In particular, at iteration 2 the probability of RR is always the highest suggesting that
(the absence of) this sub-structure might characterize the RR class, and calls for further studies.
Finally, the k-hub sub-structure (see Figure 11) can be considered as a counter-example of
previous results. In fact, even if the number of modiﬁed edges is signiﬁcant and comparable
with Max Clique, i.e., 301.47 ± 22, 82 from iteration 1 to iteration 2, and 202, 99 ± 26, 47 from
iteration 3 to iteration 4, probability values across iterations are almost constant and very similar
to the random tests. This leads us to hypothesize that k-hub sub-structures are not characterizing
any stage of the disease.

6.1.3 Experiments for studying graph metrics

In the context of this analysis, we are interested in studying the possible variations of each stage
of the MS clinical course by modifying the connectome of a patient, according to the metrics
introduced in Section 5.5.2. In particular, in this section, we discuss the results obtained for
Density and Assortativity. It is worth pointing out that current versions of state-of-the-art ASP
systems have not been able to reasonably scale over the graphs involved in the following tests;
as a consequence, after proving the viability of the approach over small examples, we simulated
the behavior of the ASP-based module via ad-hoc heuristic algorithms.

Figure 12 shows the results obtained for Density; recall that, at each iteration, we reduce
the density of each graph by 10% by removing edges. From the analysis of Figure 12(a) it is
possible to observe a generalized reduction of the probability of the initial classiﬁcation through
the iterations. Only for the CIS stage, however, this is more remarkable, even if a clear transition
cannot be observed from this ﬁgure. In order to better analyze this result, we computed (see
Figure 12(b)) the average probabilities obtained by isolating the cases in which a transition from
CIS to PP is observed through the iterations (these are 39.53% of the total) and the cases in which
a transition from CIS to RR is observed through the iterations (these are 62.79% of the total).
We point out that some of the graphs presented both transitions through the iterations; they have
been considered in both cases. The analysis of Figure 12(b) shows indeed that there is quite an

24

F. Calimeri et al.

(a) Clique (normal)

(b) Clique (random)

Fig. 9: Results for Clique (iterations i = 0..4).

interesting evidence of these transitions in the last iterations; ﬁrst of all, these results partially
conﬁrm the results obtained from studies presented in (Kocevar et al. 2016), pointing out that the
Density of a graph characterizes the stage of MS. Moreover, these results provide more insight
in the potential evolution of the disease. In fact, depending on the area of the brain that is altered,
the transition from CIS can evolve in RR stage. This calls for further studies on this aspect.

In order to better analyze these results, we computed the average number of edges removed at
each iteration for the two transitions separately (CIS-PP and CIS-RR). Results, reported in Table
2, show that even a low number of removed edges, if properly selected, may induce a signiﬁcant
change in MS stage.

Results obtained for Assortativity are shown in Figure 13. Recall that, in this case, we modiﬁed
the graphs in order to obtain an increase in the property of 10% at each iteration by removing
edges. As far as these results are concerned, we observe an almost complete independence of the
computed probabilities on this property through the iterations. Observe that the ﬁnal variation
of assortativity at the last iteration is about 40% of its initial value; as a consequence, this result
cannot be motivated by a low variation in the property itself. As a matter of fact, the number of
edges to be removed in order to reach the variation goal on assortativity was indeed extremely
small. As an example, only 27.67 ± 15.94 edges have been removed on average through the
four iterations for CIS patients, and only 24.26 ± 16.07 edges for PP patients. This result can be

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PP012340.00.20.40.60.81.0stage CISRandomCISRRSPPP012340.00.20.40.60.81.0stage RRRandom012340.00.20.40.60.81.0stage SPRandom01234Iteration0.00.20.40.60.81.0stage PPRandomTheory and Practice of Logic Programming

25

(a) Min Vertex-Cover (normal)

(b) Min Vertex-Cover (random)

Fig. 10: Results for Min Vertex-Cover (iterations i = 0..4).

Table 2: Average number of removed edges (± standard deviation) for reducing Density of 10%
at each iteration.

Iteration

CIS-PP

CIS-RR

1
2
3
4

190.65 (±15.82)
171.47 (±14.19)
154.47 (±12.87)
138.94 (±11.48)

199.48 (±11.15)
179.52 (±9.96)
161.67 (±9.00)
145.41 (±8.14)

Tot

655.53 (±54.35)

686.07 (±38.23)

linked to the ones obtained for Density where the speciﬁcity of removed edges is probably more
important than the overall properties of the corresponding graphs. And this calls for a deeper
analysis on the role of speciﬁc subsets of edges in the classiﬁcation process, which is precisely
what we analyze in the next section.

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PP012340.00.20.40.60.81.0stage CISRandomCISRRSPPP012340.00.20.40.60.81.0stage RRRandom012340.00.20.40.60.81.0stage SPRandom01234Iteration0.00.20.40.60.81.0stage PPRandom26

F. Calimeri et al.

(a) k-hub (normal)

(b) k-hub (random)

Fig. 11: Results for k-hub (iterations i = 0..4).

6.1.4 Experiments for studying ANN insights

In Section 5.5.3 we introduced the concept of importance of an edge for the classiﬁcation pur-
poses, computed by exploiting the peculiar properties of the adopted ANN. In particular, we
introduced a specialization of our framework dealing with edge importance.

Before analyzing our tests coupling graph structures or graph metrics with edge importance,
we ﬁrst consider the impact of the importance degree through the following two simple tests.
Assume edges are ordered by importance, in descending order: (i) remove, one by one, the edges
starting from the most important ones; (ii) remove, one by one, the edges starting from the less
important ones. Each time an edge is removed, the classiﬁcation task is carried out again and
results plotted for each stage. Results are shown in Figure 14. Consider the results in Figure
14(b) ﬁrst; from the analysis of this ﬁgure, it is manifest that removing unimportant edges is
completely irrelevant for the classiﬁcation result at any stage. And this is true up to about 1500
removed edges. On the contrary, removing even very few important edges may strongly affect
classiﬁcation results (see Figure 14(a)).

These considerations call for a deeper analysis of the two settings tested above. In particular, it
would be of great interest for an expert both knowing that there are edges he/she may completely

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PP012340.00.20.40.60.81.0stage CISRandomCISRRSPPP012340.00.20.40.60.81.0stage RRRandom012340.00.20.40.60.81.0stage SPRandom01234Iteration0.00.20.40.60.81.0stage PPRandomTheory and Practice of Logic Programming

27

(a) Results for all stages

(b) Detail for CIS transitions

Fig. 12: Results for Density (iterations i = 0..4).

disregard in the analysis, and that there are edges that need more attention for stage variation
analyses.

In particular, given the specializations for the Brain Evolution Simulation module introduced
in Section 5.5.3, we set a threshold such that the top 40% of edges having the highest importance
are considered as important, whereas the remaining edges are considered non important.

Figure 15(a) shows results for Max Clique when unimportant edges only are allowed in the
cliques. The number of altered edges is comparable to the one obtained for the previous tests
on Max Clique. Figure 15(a) clearly conﬁrms that altering graph sub-structures using unimpor-
tant edges only, provides no apparent modiﬁcations in the classiﬁcation. As a consequence, it
conﬁrms the fact that experts may completely disregard at least 60% of edges in their analyses.
On the contrary, Figure 15(b) reports results on Density when only important edges are re-
moved. In this case, results show that when a huge amount of important edges is removed, the
ANN becomes almost unable to perform a reliable classiﬁcation. As a consequence, this rein-
forces the intuition that there are very few important, and let’s say critical, edges guiding tran-
sitions between MS stages. In Section 7 we show how we took these preliminary results into
account in order to provide experts with a powerful analysis tool.

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PP012340.00.20.40.60.81.0CIS-PPCISRRSPPP01234Iteration0.00.20.40.60.81.0CIS-RR28

F. Calimeri et al.

(a) Assortativity

Fig. 13: Results for Assortativity (iterations i = 0..4).

6.2 Experiments on performances

In this section we present the results of a series of tests aimed at providing support for a perfor-
mance analysis of the system. We ﬁrst analyze execution times of the entire framework, then we
single out the role of the main modules in the overall execution times. Eventually, we focus on
the ASP-based Brain Evolution Simulation module.

All tests have been carried out on a Linux machine 4.15.0 − 20-generic #21-Ubuntu, with an
Intel(R) Core(TM) i7-4770 CPU @3.40Ghz and 15.6 GB of RAM. As for the grounder and the
solver, we coupled I-DLV (version 1.1.0) and WASP (version 2.0). The solver parameters have
been set to --silent and --printonlyoptimum; this means, that the computation is stopped
at the ﬁrst optimum found for optimization problems.

When not differently speciﬁed, we used the dataset introduced in Section 6.1.1. It is worth
pointing out that the aim of these tests is not an assessment of ASP solvers and their performance
(as extensively done in related literature (Calimeri et al. 2012; Gebser et al. 2016; Gebser et al.
2017; Gebser et al. 2019)), but rather to assess the applicability of the proposed framework to the
herein considered context and related ones.

In a ﬁrst series of experiments, we measured the execution times of one iteration of the frame-
work, i.e., involving one step in the brain evolution simulation. We separately considered the
three structural properties Max Clique, Min Vertex-Cover, and k-hub discussed above and, as for

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PPTheory and Practice of Logic Programming

29

(a) Starting from the important edges

(b) Starting from the less important edges

Fig. 14: Variation of classiﬁcation results removing important/unimportant edges.

Max Clique, we tested also the ASP programs altering both unimportant and important edges5.
Results are shown in Figure 16. In order to verify whether the starting stage inﬂuences per-
formance or not, we highlighted running times for each property and for each stage; times are
averaged over all the samples grouped by MS stage and standard deviation is also shown in the
ﬁgure. From the analysis of Figure 16, we can observe that (a) the execution time of one iteration
is particularly small for all properties, always signiﬁcantly lower than one second; (b) there is no
actual correlation between starting stages and performance; (c) interestingly, considering impor-
tant/unimportant edges reduces average execution times, as the dimension of the graphs the ASP
program works on is reduced, in terms of edges.

We then evaluated the potential impact on running times due to subsequent iterations. Fig-
ure 17 reports execution times averaged over all the stages for four subsequent iterations; it
is easy to see that ﬂuctuations of running time among iterations are negligible except for Min
Vertex-Cover where, as discussed in Section 6.1, at the third iteration a very low number of
edges remains in the modiﬁed graphs.

5 Recall that in Section 6.1 we set a threshold such that the top 40% of edges having the highest relevance are considered
as important, whereas the remaining edges are considered as unimportant. It is worth mentioning again that information
about importance of edges is directly provided by the Classiﬁcation module.

0501001502002503000.00.20.40.60.81.0stage CISCISRRPPSP0501001502002503000.00.20.40.60.81.0stage RRCISRRPPSP0501001502002503000.00.20.40.60.81.0stage SPCISRRPPSP050100150200250300N. of removed edges0.00.20.40.60.81.0stage PPCISRRPPSP02505007501000125015000.00.20.40.60.81.0stage CISCISRRPPSP02505007501000125015000.00.20.40.60.81.0stage RRCISRRPPSP02505007501000125015000.00.20.40.60.81.0stage SPCISRRPPSP0250500750100012501500N. of removed edges0.00.20.40.60.81.0stage PPCISRRPPSP30

F. Calimeri et al.

(a) Clique - Altering unimportant edges

(b) Density - Removing important edges

Fig. 15: Analyzing structural properties and graph metrics considering important/unimportant
edges.

In a further series of experiments, we considered the impact of each module of the frame-
work in the running time of one iteration. We take into account the three main modules, namely
the Classiﬁer, the Classiﬁer Validity Checker and the Brain Evolution Simulation modules. Re-
sults shown in Figure 18 clearly point out that the main load of computation is on the Brain
Evolution Simulation module. Obviously, both Classiﬁer and Classiﬁcation Validity Checker ex-
ecution times are independent from the graph property under examination; interestingly, they are
both signiﬁcantly faster than the simulation task. Higher execution times for Max Clique and Min
Vertex-Cover with respect to k-hub depend on the deterministic nature of the encoding for k-hub.
The same considerations carried out in the previous tests when including important/unimportant
edges for Max Clique are still valid in this test.

The scalability of the ASP part of the system has been tested over graphs of increasing size.
First of all, we measured running times of the Brain Evolution Simulation module over a set
of simulated graphs possibly representing connectome; in particular, we ﬁxed the number of
nodes (84 in our tests, coherently with the technique described in Section 5.1) and we randomly
generated graphs with increasing number of edges up to a complete graph. It is worth recalling
that, as pointed out in Section 6.1.1, the average number of edges in graphs corresponding to
real connectome is around 2000. Results are reported in Figure 19; each data point is the average

012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PP012340.00.20.40.60.81.0stage CISCISRRSPPP012340.00.20.40.60.81.0stage RR012340.00.20.40.60.81.0stage SP01234Iteration0.00.20.40.60.81.0stage PPTheory and Practice of Logic Programming

31

Fig. 16: Execution times for one iteration of the framework, considering the three structural
properties Max Clique, Min Vertex-Cover, and k-hub. Bottom graphs show execution times for
one iteration of the framework on Max Clique considering either important or unimportant edges.

Fig. 17: Execution times for four iteration of the framework.

running time of 10 different executions over random graphs having the same number of nodes
and edges. Via this ﬁgure, it is possible to observe that all the tested ASP programs for studying
structural properties are solvable with execution times always below one second on any potential
graph representing a connectome. There are obviously small variations between different samples
and properties; nonetheless, the ﬁgure shows that any connectome can be easily managed by our
approach in order to study structural properties.

CISRRSPPP0.00.20.40.60.81.0Time (sec)Max CliqueCISRRSPPP0.00.20.40.60.81.0Min Vertex CoverCISRRSPPP0.00.20.40.60.81.0k-hubCISRRSPPPStage0.000.050.100.150.200.250.300.350.40Time (sec)Max Clique (altering not important edges)CISRRSPPPStage0.000.050.100.150.200.250.300.350.40Max Clique (altering important edges)1234Iteration0.050.100.150.200.250.30Time (sec)Max CliqueMin Vertex Coverk-hubMax Clique (not imp)Min Clique (imp)32

F. Calimeri et al.

Fig. 18: Impact of each module of the framework in the running time of one iteration.

As a further scalability test, we generated graphs with increasing number of nodes; as for
the number of edges in these graphs, we measured the average number of edges in the graphs
representing real connectome and we kept the same proportion of edges for each generated graph.
Results are shown in Figure 20 for graphs up to 700 nodes; again, each data point is the average
running time of 10 different executions over random graphs having the same number of nodes
and edges. In this case, it is possible to observe that, while the number of nodes is around 100,
running times for all the problems are reasonable; when the number of nodes grows further,
the combinatorial explosion of programs including non-deterministic choice rules is reﬂected
in rapidly increasing running times (indeed, as an example, Max Clique is reported as an hard
problem in the ASP Competition series (Gebser et al. 2016; Gebser et al. 2017; Gebser et al.
2019)); Min Vertex-Cover is affected ﬁrst by this issue. In fact, we observed that, on the machine
used, it may require more than one hour of computation for determining the Min Vertex-Cover
on a graph with around 150 nodes or ﬁnding the Max Clique on a graph with around 500 nodes.
It is interesting to observe that, again, considering only important/unimportant edges allows to
move forward the limit of computation. Indeed, the lower number of considered edges simpliﬁes
the graph and the execution time is faster; as it was expected, considering important edges (40%
of the total) allows a further improvement with respect to the not important edges (60% of the
total). Notably, k-hub scales very well on tested graphs.

These results pose some questions on the applicability of ASP solutions, and generally of
exact solutions for optimization problems, in contexts different from the one studied in this paper,
where the graphs to be handled become very large and non-deterministic reasoning over the graph
is needed. In these contexts, heuristic algorithms, not spanning the entire search space, might
be more efﬁcient. However, as previously pointed out, compactness, versatility, and declarative
nature of ASP allow for a fast prototyping, and make it an excellent tool for testing numerous
alternative graph properties; in those contexts where input is represented by large graphs, one
may think of applying ASP based solutions on small sample graphs, in order to identify the most

ClassifierCheckerBrain Evo. Sim.0.000.050.100.150.200.250.30Time (sec)Max CliqueClassifierCheckerBrain Evo. Sim.0.000.050.100.150.200.250.30Min Vertex CoverClassifierCheckerBrain Evo. Sim.0.000.050.100.150.200.250.30k-hubClassifierCheckerBrain Evo. Sim.0.000.050.100.150.200.250.30Time (sec)Max Clique (altering not important edges)ClassifierCheckerBrain Evo. Sim.0.000.050.100.150.200.250.30Max Clique (altering important edges)Theory and Practice of Logic Programming

33

Fig. 19: Running time of the Brain Evolution Simulation module on simulated graphs with in-
creasing number of edges.

Fig. 20: Running time of the Brain Evolution Simulation module on simulated graphs with in-
creasing number of nodes.

promising properties and, then, implementing them with other ad-hoc, more efﬁcient, solutions
in order to study the problem on real graphs. The adoption of ANN insights on important edges
shown in this paper may be of signiﬁcant help in this task; indeed, leveraging the most important
edges only allows to work on smaller but still signiﬁcant graphs.

Furthermore, we do believe that applications like the one herein described can signiﬁcantly
motivate the scientiﬁc community, especially the one working on ASP, at improving performance
of systems for their use in real-world applications.

6.3 Discussion

The tests presented in this section allow us to draw some interesting considerations. First of all,
all tests presented in Section 6.1 actually proved the appropriateness of the approach for studying
the evolution of MS; also, simplicity and high versatility in deﬁning, setting up and carrying out
a wide variety of tests showed how crucial is the role played by ASP. We also provided some
experts with the system for a ﬁrst view, and they have been quite impressed by the possibility of
simulating brain evolution so easily.

On the ASP side, we can also say that what discussed in Section 6.2 conﬁrms that expres-
siveness and compactness of the language make it perfectly suitable to address a wide variety

50100020003000400050006000N. of edges0.00.20.40.60.81.0Time (sec)Max CliqueMin VertexCoverk-hubMax Clique (not imp)Max Clique (imp)100200300400500600700N. of nodes010002000300040005000Time (sec)Max CliqueMin Vertex Coverk-hubMax Clique (not imp)Max Clique (imp)34

F. Calimeri et al.

of problems on graphs. Moreover, language extensions signiﬁcantly expanded the range of ap-
plicability of ASP; as an example, weak constraints allowed us to easily express optimization
problems and, analogously, the recently introduced possibility of placing external function calls
into a logic program, with a predicate as function parameter instead of a single variable (see,
e.g., the encoding in Figure 6), allowed us to keep the encoding simple and elegant even with
the inclusion of complex graph metrics computation. Furthermore, when the ASP program does
not include choice rules, actual ASP implementations can deal with very large graphs, and scale
deﬁnitely well. Unfortunately, on the downside, the major weakness of current ASP systems be-
comes apparent when a combinatorial explosion of the problem occurs. In particular, while we
have shown that the system is fully capable of addressing structural properties on connectome,
we experienced that addressing large graphs is possible only to some extent.

Moreover, as pointed out in Section 6.1.3, when dealing with graph metrics, since the non-
deterministic choice is carried out on edges instead of nodes, and since the metrics need to
be computed on the entire guessed graph, current versions of state-of-the-art ASP systems do
not reasonably scale over the connectome graphs. In particular, we observed that systems incur
in out-of-memory or exceed time limit (more than one hour) much earlier with respect to the
tests focusing on structural graph properties. Intuitively, the problem is that the systems need to
generate all possible guesses on potential graphs, before computing the corresponding metrics. In
order to exclude external function calls as the potential bottleneck in this case, we also checked
a version of the program for controlling graph density variations using aggregates only; while
avoiding external function calls allowed us to reduce memory issues, we encountered similar
scalability issues on connectome.

The experience above calls for the need of some extra features of ASP systems, e.g., extending
their solving capabilities with custom heuristics and propagators; some work in this direction is
currently ongoing (see (Dodaro and Ricca 2018) and references therein). However, the applica-
bility of these approaches in our context is not straightforward. As a matter of fact, even taking
the possibility of specifying suitable propagators into account, it is not always possible to easily
deﬁne model generation guiding rules; let us think, for instance, to assortativity, where it is not
clear how to guide the edge selection in order to imply a decrease in the property. Moreover, the
problem is even more complex if we consider that the combinatorial explosion of this problem is
coupled with an optimization task.

However, it is worth stressing again the potential role of important/unimportant edges in en-
compassing, to some extent, scalability issues in our general framework. In fact, we have ﬁrst
shown in Sections 6.1.4 (see speciﬁcally Figure 14) that removing even a high percentage of
unimportant edges does not affect the classiﬁcation quality. We have then shown in Section 6.2
(see speciﬁcally Figure 20) that limiting the computation on (a small number of) important edges
only signiﬁcantly reduces performance issues, thus extending the dimensions of graphs that can
be managed.

To the best of our knowledge, this is the ﬁrst work showing how to exploit the importance of
an edge in graph-based classiﬁcation tasks in order to boost reasoning capabilities over graphs.
In our opinion, this result deserves further investigations in future works, and can stimulate the
research community in looking for new ASP program evaluation optimizations.

Theory and Practice of Logic Programming

35

Fig. 21: Screenshot of the integrated web environment

7 Integrated Web Tool

In this section we present an integrated web tool that has been developed in order to implement
the framework introduced in this paper and make it actually usable. The tool is available online
at https://brainmsa.mat.unical.it. The integrated environment provides a user-friendly
interface that shows analysis results in real time. The main objective of the tool is to help physi-
cians, typically neurologists that are not likely to be ANN/ASP experts, to study the evolution of
MS through the application of the proposed framework, but also by manual inspection of brain
modiﬁcations.

The input to the tool is expected to be a graph representation of the brain, obtained as described
in Section 5.1. A 3D environment showing the connectome is then generated, as shown in Fig-
ure 21. The graph accurately reﬂects the shape of a human brain, so that it is possible to identify
which nodes belong to a speciﬁc brain area. Usual rotation, translation and zooming operations
are available for inspecting the brain structure. Edge colors depend on the corresponding weight,
so as to provide a visual representation of connection strength.

The tool provides pre-deﬁned specializations for the various modules of the framework. For
some of them, such as the Brain Evolution Simulation module, the user can choose one among
different ASP programs already available, or she/he can provide other programs personally de-
veloped for speciﬁc purposes. It allows also to choose an empty program, in order to apply only
manual modiﬁcations as explained below.

By launching the classiﬁcation and asking for one single iteration of the framework, the user
can immediately check (see Figure 21) the new prediction on the right panel, and the 3D brain
representation is updated with the applied modiﬁcations. If the number of required iterations is
more than one, the right panel shows the graphs for each iteration (similarly to the ones presented
in Section 6), whereas only the last 3D brain representation is shown.

Once the user carried out several runs, it is possible to have a general overview of obtained re-
sults by clicking the History button. In this case, the page shown in Figure 22 is presented; it ﬁrst
shows a boxplot for each stage of MS that summarizes the overall probability values returned by
the classiﬁer during the current test session. Moreover, the detailed history of prediction results
computed on the current connectome is also provided. Finally, for each prediction, the system
provides also a heat map representing the adjacency matrix of the corresponding graph; this can
be useful to see distribution of edges and weights at a glance.

36

F. Calimeri et al.

Fig. 22: An example of how the environment encompasses results.

Besides the implementation of the framework, the tool provides also some more functionalities
helping experts to carry out manual and more reﬁned analyses. Speciﬁcally, ﬁrst of all, the user
can manually select the set of edges to modify in a brain evolution step. This selection can be
either exploited in substitution of the Brain Evolution Simulation module (if the user chooses the
empty ASP program) or it can be seen as a pre-processing step on the connectome, if one of the
specializations of the Brain Evolution Simulation module is chosen.

Furthermore, the user can modify the visualization of the connectome, based on edge impor-
tance. In particular, if at least one classiﬁcation has been carried out on the connectome, edge
importance, as introduced in Section 5.5.3, is available. Then, users can hide or show edges,
based on their importance, by using a slider. As a consequence, manual inspection on the con-
nectome can be greatly simpliﬁed, allowing the user to concentrate her/his attention on important
edges only.

8 Specialization of the framework to other scenarios

In order to show the generality of the proposed framework, in this section we present some
additional application scenarios it can be specialized to in a quite straightforward way. In par-

Theory and Practice of Logic Programming

37

ticular, we ﬁrst overview some additional biomedical contexts and then we show the application
of the framework to a very different application scenario, namely inﬂuence prediction in social
networks.

8.1 Specialization of the framework to other neurological disorders

It has been proved by several independent studies that there is a strong correlation between the
variations of connections among neurons and the function of the brain and, consequently, with
possible insurgence of several neurological disorders (Bargmann and Marder 2013). As an ex-
ample, in the Alzheimer Disease researchers observed a decrease in the connectivity, associated
with changes in the hippocampus (Lenka et al. 2015); altered connectivity has been observed
also in the Parkinson disease (Lenka et al. 2015); similarly, an increased connectivity associated
with changes in the amygdala have been associated with anxiety disorder (Stein et al. 2007). In
all such contexts, the analysis of brain connections and their variations can provide signiﬁcant
insights in the knowledge of disease evolution.

Let us concentrate on the Alzheimer Disease (hereafter, AD); it is well known that, at early
stages, this disease appears as Mild Cognitive Impairment (hereafter, MCI) but not all patients
with MCI subsequently develop AD (Petersen 2004). However, it is also known that in MCI pro-
gression towards AD a key role is played by the loss of connectivity among the different cortical
areas. Thus, a large variety of approaches aiming at characterizing both MCI and AD have been
proposed in the literature (Hornero et al. 2009; Jeong 2004). Some of them are based on the
analysis of electroencephalograms (EEG) data; this is a less invasive observation method than
MRI. In this case, the analysis can be based on a graph, where each node represents an electrode,
and the weight of each edge expresses the similarity degree between the signals registered by the
corresponding electrodes (see (Cauteruccio et al. 2019)).

Our framework can be specialized quite straightforwardly to the analysis of MCI evolution in
order to study key factors determining its progress to AD. In fact, it is sufﬁcient to specialize the
Classiﬁer module with any approach developed to classify patients in one of the Healthy, MCI,
or AD stages. The Brain Evolution Simulation module can be then specialized with a proper ASP
program that identiﬁes subgraphs corresponding to (variations of) some property of interest; as an
example, in (Cauteruccio et al. 2019) it has been shown that interesting graph properties related
to AD are network density and clustering coefﬁcient. Here, again, the Classiﬁcation Validity
Check can refute the classiﬁcation outcome if a signiﬁcant degradation in the graph corresponds
to a predicted remission of the disease; indeed, this situation is not biologically relevant.

8.2 Specialization of the framework in the context of Social Networks

The work presented in (Wu et al. 2019) surveys several contexts where ANNs are applied to
graph data; indeed, there is increasing interest in extending deep learning approaches for this kind
of data. Interesting applications include, but are not limited to, e-commerce and recommender
systems, citation networks, social networks, trafﬁc analysis, drug discovery, adversarial attack
prevention, and event detection. All of these problems can beneﬁt from the application of our
framework in the identiﬁcation of potentially relevant graph properties.

In order to provide an example, we focus next on one of them, namely inﬂuence prediction in
social networks (Qiu et al. 2018). A social network can be represented as a graph G = (V, E),
where V denotes the set of users and E denotes the set of relationships between them. Each user

38

F. Calimeri et al.

in a social network performs social actions towards other users; these can be suitably summarized
as edge labels between the nodes corresponding to the involved users. Social inﬂuence commonly
refers to the phenomenon that the opinions and actions of a user are affected by others. In many
applications, such as advertising and recommendation, it is crucial to predict the social inﬂuence
of each user.

In (Qiu et al. 2018) a neural network-based approach is proposed to predict the action status
of a user given the action statuses of both near neighbors and local structural information. The
list of action statuses strictly depends on the kind of social network under analysis. As an ex-
ample it can be a “retweet” action in Twitter or a citation action in academic social networks.
The input network is then fed to the ANN which outputs a two-dimension representation for
each user indicating the action status prediction, which is then exploited for the social inﬂuence
computation.

In this contexts it would be of great relevance to study the evolution of social inﬂuence with
respect to modiﬁcations on the social graph. As an example, it would be interesting to ﬁnd the
minimal graph modiﬁcations required to increase the social inﬂuence in the network. Our frame-
work can be specialized quite straightforwardly also to this context. In particular, the Classiﬁer
module can be specialized with the approach presented in (Qiu et al. 2018) in order to predict
action statuses and, thus, social inﬂuence. The Brain Evolution Simulation module, which in this
context could be more appropriately renamed as Graph Evolution Simulation, can be specialized
with the ASP program of choice identifying the minimal changes to be applied on the input graph
in order to reach some target value of social inﬂuence. Even if the complete development of this
speciﬁc application is out of the scope of this paper, it is easy to see that the Classiﬁcation Valid-
ity Checker can be easily encoded with speciﬁc rules allowing to detect wrong classiﬁcations in
action statuses.

9 Conclusion

This paper introduced a general and extensible framework pointing out opportunities provided by
a combined use of ASP and ANN. In particular, we grounded the framework in order to provide
an effective support for neurologists in studying the evolution of neurological disorders.

We have shown that a mixed use of ASP and ANNs can be of signiﬁcant impact both in bioin-
formatics and other research ﬁelds. Indeed, logic-based modules greatly simplify the exploration
of different, possibly complex, variations in the structure of the connectome, and ANNs allow
to immediately check the potential impact of such variations on the course of the disease. We
provided three specializations of the general framework and tested them on real data to show
the effectiveness of the proposed approach. Extensive tests proved the potential impact of the
framework on the discovery process and some limitations of current ASP solvers. Based on this
experience, we developed a web tool allowing even non experts to explore the connectome and
test the impact of its variations on the course of the disease.

We believe that the results are encouraging; moreover, they further motivate the already run-
ning research activities for optimizing ASP program evaluations. Finally, obtained results provide
us with a solid basis for encouraging the communities of both ASP and ANN areas to identify
more contexts where a mixed use of these tools can lead to signiﬁcant beneﬁts.

As far as future work is concerned, we plan to specialize the presented framework on the

application contexts outlined in Section 8.

Theory and Practice of Logic Programming

39

10 Acknowledgements

The authors would like to thank the Editors for their comments and support during the review
process, and the anonymous reviewers for their helpful and constructive comments that greatly
helped at improving the ﬁnal version of the paper.

This work was partially supported by: (i) the Italian Ministry for Economic Development
(MISE) under the project “Smarter Solutions in the Big Data World”, funded within the call
“HORIZON2020” PON I&C 2014-2020 (CUP B28I17000250008); and (ii) the Italian Ministry
of Education, Universities and Research (MIUR) and the Presidency of the Council of Ministers
under project “Declarative Reasoning over Streams” (CUP H24I17000080001) within the call
Progetti di ricerca di Rilevante Interesse Nazionale “PRIN” 2017, project code 2017M9C25L 001.

References

ADRIAN, W. T., ALVIANO, M., CALIMERI, F., CUTERI, B., DODARO, C., FABER, W., FUSC `A, D.,
LEONE, N., MANNA, M., PERRI, S., RICCA, F., VELTRI, P., AND ZANGARI, J. 2018. The ASP system
DLV: advancements and applications. KI 32, 2-3, 177–179.

ALVIANO, M., AMENDOLA, G., DODARO, C., LEONE, N., MARATEA, M., AND RICCA, F. 2019. Eval-
uation of disjunctive programs in WASP. In Logic Programming and Nonmonotonic Reasoning - 15th
International Conference, LPNMR 2019, Philadelphia, PA, USA, June 3-7, 2019, Proceedings, M. Bal-
duccini, Y. Lierler, and S. Woltran, Eds. Lecture Notes in Computer Science, vol. 11481. Springer, 241–
255.

ARIAS, J., CARRO, M., SALAZAR, E., MARPLE, K., AND GUPTA, G. 2018. Constraint answer set pro-

gramming without grounding. TPLP 18, 3-4, 337–354.

BALDUCCINI, M. AND LIERLER, Y. 2017. Constraint answer set solver EZCSP and why integration

schemas matter. TPLP 17, 4, 462–515.

BARAL, C. 2003. Knowledge Representation, Reasoning, and Declarative Problem Solving. Cambridge

University Press, New York, NY, USA.

BARGMANN, C. AND MARDER, E. 2013. From the connectome to brain function. Nature Methods 10,

483. Nature Publishing Group.

BARRETT, C., FONTAINE, P., AND TINELLI, C. 2016. The Satisﬁability Modulo Theories Library (SMT-

LIB). www.SMT-LIB.org.

BARRETT, C. W., DETERS, M., DE MOURA, L. M., OLIVERAS, A., AND STUMP, A. 2013. 6 years of

SMT-COMP. J. Autom. Reasoning 50, 3, 243–277.

BARRETT, C. W. AND TINELLI, C. 2018. Satisﬁability modulo theories. In Handbook of Model Checking.,

E. M. Clarke, T. A. Henzinger, H. Veith, and R. Bloem, Eds. Springer, 305–343.

BASELICE, S., BONATTI, P. A., AND GELFOND, M. 2005. Towards an integration of answer set and
In Logic Programming, 21st International Conference, ICLP 2005, Sitges, Spain,
constraint solving.
October 2-5, 2005, Proceedings, M. Gabbrielli and G. Gupta, Eds. Lecture Notes in Computer Science,
vol. 3668. Springer, 52–66.

BECK, H., DAO-TRAN, M., EITER, T., AND FINK, M. 2015. LARS: A logic-based framework for ana-

lyzing reasoning over streams. In AAAI. AAAI Press, 1431–1438.

BROOKS, D. R., ERDEM, E., ERDOGAN, S. T., MINETT, J. W., AND RINGE, D. 2007. Inferring phylo-

genetic trees using answer set programming. J. Autom. Reasoning 39, 4, 471–511.

CALIMERI, F., CAUTERUCCIO, F., MARZULLO, A., STAMILE, C., AND TERRACINA, G. 2018. Mixing
In RuleML+RR.

logic programming and neural networks to support neurological disorders analysis.
Lecture Notes in Computer Science, vol. 11092. Springer, 33–47.

CALIMERI, F., COZZA, S., AND IANNI, G. 2007. External sources of knowledge and value invention in

logic programming. Ann. Math. Artif. Intell. 50, 3-4, 333–361.

40

F. Calimeri et al.

CALIMERI, F., FABER, W., GEBSER, M., IANNI, G., KAMINSKI, R., KRENNWALLNER, T., LEONE, N.,

RICCA, F., AND SCHAUB, T. 2012. ASP-Core-2: Input language format.

CALIMERI, F., FINK, M., GERMANO, S., HUMENBERGER, A., IANNI, G., REDL, C., STEPANOVA, D.,
TUCCI, A., AND WIMMER, A. 2016. Angry-hex: An artiﬁcial player for angry birds based on declarative
knowledge bases. IEEE Trans. Comput. Intellig. and AI in Games 8, 2, 128–139.

CALIMERI, F., FUSC `A, D., GERMANO, S., PERRI, S., AND ZANGARI, J. 2019. Fostering the use of declar-
ative formalisms for real-world applications: The embasp framework. New Generation Comput. 37, 1,
29–65.

CALIMERI, F., FUSC `A, D., PERRI, S., AND ZANGARI, J. 2017a. External computations and interoper-
ability in the new DLV grounder. In AI*IA 2017 Advances in Artiﬁcial Intelligence - XVIth International
Conference of the Italian Association for Artiﬁcial Intelligence, Bari, Italy, November 14-17, 2017, Pro-
ceedings, F. Esposito, R. Basili, S. Ferilli, and F. A. Lisi, Eds. Lecture Notes in Computer Science, vol.
10640. Springer, 172–185.

CALIMERI, F., FUSC `A, D., PERRI, S., AND ZANGARI, J. 2017b. I-DLV: the new intelligent grounder of

DLV. Intelligenza Artiﬁciale 11, 1, 5–20.

CALIMERI, F., IANNI, G., KRENNWALLNER, T., AND RICCA, F. 2012. The answer set programming

competition. AI Magazine 33, 4, 114–118.

CALIMERI, F., MARZULLO, A., STAMILE, C., AND TERRACINA, G. 2018. Graph based neural networks
for automatic classiﬁcation of multiple sclerosis clinical courses. In 26th European Symposium on Arti-
ﬁcial Neural Networks, ESANN 2018, Bruges, Belgium, April 25-27, 2018.

CALIMERI, F. AND RICCA, F. 2013. On the Application of the Answer Set Programming System DLV in

Industry: a Report from the Field. Book Reviews 2013, 03, 1–16.

CAUTERUCCIO, F., LO GIUDICE, P., TERRACINA, G., URSINO, D., MAMMONE, N., AND MORABITO,
F. 2019. A new network-based approach to investigating neurological disorders. International Journal
of Data Mining, Modelling and Management 11, 315–349. Inderscience.

CHABIERSKI, P., RUSSO, A., LAW, M., AND BRODA, K. 2017. Machine comprehension of text using
combinatory categorial grammar and answer set programs. In COMMONSENSE. CEUR Workshop Pro-
ceedings, vol. 2052. CEUR-WS.org.

COK, D. R., STUMP, A., AND WEBER, T. 2015. The 2013 evaluation of SMT-COMP and SMT-LIB. J.

Autom. Reasoning 55, 1, 61–90.

DODARO, C. AND RICCA, F. 2018. The external interface for extending wasp. Theory and Practice of

Logic Programming, 1–24.

DUUN-HENRIKSEN, J., MADSEN, R., REMVIG, L., THOMSEN, C., SORENSEN, H., AND KJAER, T.
2012. Automatic detection of childhood absence epilepsy seizures: toward a monitoring device. Pediatric
Neurology 46, 5, 287–292.

EITER, T., FINK, M., IANNI, G., KRENNWALLNER, T., REDL, C., AND SCH ¨ULLER, P. 2016. A model
building framework for answer set programming with external computations. TPLP 16, 4, 418–464.
EITER, T., GERMANO, S., IANNI, G., KAMINSKI, T., REDL, C., SCH ¨ULLER, P., AND WEINZIERL, A.

2018. The DLVHEX system. KI - K¨unstliche Intelligenz 32, 2-3 (August), 187–189.

EITER, T., GERMANO, S., IANNI, G., KAMINSKI, T., REDL, C., SCH ¨ULLER, P., AND WEINZIERL, A.

2018. The DLVHEX system. KI 32, 2-3, 187–189.

EITER, T., REDL, C., AND SCH ¨ULLER, P. 2016. Problem solving using the HEX family. In Computational
Models of Rationality, Essays dedicated to Gabriele Kern-Isberner on the occasion of her 60th birthday,
C. Beierle, G. Brewka, and M. Thimm, Eds. College Publications, 150–174.

ERDEM, E., GELFOND, M., AND LEONE, N. 2016. Applications of answer set programming. AI Maga-

zine 37, 3, 53–68.

FABER, W., LEONE, N., AND PFEIFER, G. 2004. Recursive aggregates in disjunctive logic programs:
In Proceedings of the 9th European Conference on Artiﬁcial Intelligence
Semantics and complexity.
(JELIA 2004), J. J. Alferes and J. Leite, Eds. Lecture Notes on Artiﬁcial Intelligence (LNAI), vol. 3229.
Springer Verlag, 200–212.

Theory and Practice of Logic Programming

41

FEBBRARO, O., LEONE, N., GRASSO, G., AND RICCA, F. 2012. JASP: A framework for integrating
answer set programming with java. In Principles of Knowledge Representation and Reasoning: Proceed-
ings of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14, 2012, G. Brewka,
T. Eiter, and S. A. McIlraith, Eds. AAAI Press.

FEBBRARO, O., REALE, K., AND RICCA, F. 2011. Aspide: Integrated development environment for an-
swer set programming. In Logic Programming and Nonmonotonic Reasoning - 11th International Con-
ference, LPNMR 2011, Vancouver, Canada, May 16-19, 2011. Proceedings. Lecture Notes in Computer
Science, vol. 6645. 317–330.

FUSC `A, D., CALIMERI, F., ZANGARI, J., AND PERRI, S. 2017. I-DLV+MS: preliminary report on an
In RCRA@AI*IA. CEUR Workshop Proceedings, vol. 2011. CEUR-

automatic ASP solver selector.
WS.org, 26–32.

GEBSER, M., KAMINSKI, R., KAUFMANN, B., AND SCHAUB, T. 2014. Clingo = ASP + control: Prelim-
inary report. In Technical Communications of the Thirtieth International Conference on Logic Program-
ming (ICLP’14), M. Leuschel and T. Schrijvers, Eds. Vol. arXiv:1405.3694v1. Theory and Practice of
Logic Programming, Online Supplement.

GEBSER, M., KAMINSKI, R., KAUFMANN, B., AND SCHAUB, T. 2019. Multi-shot ASP solving with

clingo. TPLP 19, 1, 27–82.

GEBSER, M., KAMINSKI, R., KAUFMANN, B., SCHAUB, T., SCHNEIDER, M. T., AND ZILLER, S. 2011.
In LPNMR. LNCS, vol. 6645.

A portfolio solver for answer set programming: Preliminary report.
Springer, 352–357.

GEBSER, M., LEONE, N., MARATEA, M., PERRI, S., RICCA, F., AND SCHAUB, T. 2018. Evaluation
techniques and systems for answer set programming: a survey. In Proceedings of the Twenty-Seventh
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Swe-
den., J. Lang, Ed. ijcai.org, 5450–5456.

GEBSER, M., MARATEA, M., AND RICCA, F. 2016. What’s hot in the answer set programming competi-
tion. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA., D. Schuurmans and M. P. Wellman, Eds. AAAI Press, 4327–4329.

GEBSER, M., MARATEA, M., AND RICCA, F. 2017. The sixth answer set programming competition. J.

Artif. Intell. Res. 60, 41–95.

GEBSER, M., MARATEA, M., AND RICCA, F. 2019. The seventh answer set programming competition:

Design and results. CoRR abs/1904.09134.

GEBSER, M., SCHAUB, T., THIELE, S., AND VEBER, P. 2011. Detecting inconsistencies in large biological
networks with answer set programming. Theory and Practice of Logic Programming 11, 2-3, 323360.
GELFOND, M. 2010. Knowledge representation language p-log - A short introduction. In Datalog. LNCS,

vol. 6702. Springer, 369–383.

GELFOND, M. AND LEONE, N. 2002. Logic Programming and Knowledge Representation – the A-Prolog

perspective . Artiﬁcial Intelligence 138, 1–2, 3–38.

GELFOND, M. AND LIFSCHITZ, V. 1991. Classical Negation in Logic Programs and Disjunctive

Databases. New Gen. Comput. 9, 365–385.

GOODFELLOW, I. J., BENGIO, Y., AND COURVILLE, A. C. 2016. Deep Learning. Adaptive computation

and machine learning. MIT Press.

HAYKIN, S. 1998. Neural Networks: A Comprehensive Foundation, 2nd ed. Prentice Hall PTR, Upper

Saddle River, NJ, USA.

HORNERO, R., AB ´ASOLO, D., ESCUDERO, J., AND G ´OMEZ, C. 2009. Nonlinear analysis of elec-
troencephalogram and magnetoencephalogram recordings in patients with Alzheimer’s disease. Philo-
sophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sci-
ences 367, 1887, 317–336. The Royal Society.

HU, Z., MA, X., LIU, Z., HOVY, E. H., AND XING, E. P. 2016. Harnessing deep neural networks with
logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,
ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.

42

F. Calimeri et al.

ION-MARGINEANU, A., KOCEVAR, G., STAMILE, C., SIMA, D. M., DURAND-DUBIEF, F., HUFFEL,
S. V., AND SAPPEY-MARINIER, D. 2017. A comparison of machine learning approaches for classifying
In ICANN (2). LNCS, vol. 10614.
multiple sclerosis courses using MRSI and brain segmentations.
Springer, 643–651.

JENKINSON, M., BECKMANN, C. F., BEHRENS, T. E. J., WOOLRICH, M. W., AND SMITH, S. M. 2012.

FSL. NeuroImage 62, 2, 782–790.

JEONG, J. 2004. EEG dynamics in patients with Alzheimer’s disease. Clinical neurophysiology 115, 7,

1490–1505. Elsevier.

KAMINSKI, R., SCHAUB, T., AND WANKO, P. 2017. A tutorial on hybrid answer set solving with clingo.
In Reasoning Web. Semantic Interoperability on the Web - 13th International Summer School 2017, Lon-
don, UK, July 7-11, 2017, Tutorial Lectures, G. Ianni, D. Lembo, L. E. Bertossi, W. Faber, B. Glimm,
G. Gottlob, and S. Staab, Eds. Lecture Notes in Computer Science, vol. 10370. Springer, 167–203.

KAWAHARA, J., BROWN, C. J., MILLER, S. P., BOOTH, B. G., CHAU, V., GRUNAU, R. E., ZWICKER,
J. G., AND HAMARNEH, G. 2017. Brainnetcnn: Convolutional neural networks for brain networks;
towards predicting neurodevelopment. NeuroImage 146, 1038–1049.

KINGMA, D. P. AND BA, J. 2014. Adam: A method for stochastic optimization. CoRR abs/1412.6980.
KOCEVAR, G., STAMILE, C., HANNOUN, S., COTTON, F., VUKUSIC, S., DURAND-DUBIEF, F., AND
SAPPEY-MARINIER, D. 2016. Graph theory-based brain connectivity for automatic classiﬁcation of
multiple sclerosis clinical courses. Frontiers in Neuroscience 10, 478.

KOUVAROS, P. AND LOMUSCIO, A. 2018.

Formal veriﬁcation of cnn-based perception systems.

CoRR abs/1811.11373.

LAW, M., RUSSO, A., AND BRODA, K. 2015. Learning weak constraints in answer set programming.

TPLP 15, 4-5, 511–525.

LAW, M., RUSSO, A., AND BRODA, K. 2016.
dependent examples. TPLP 16, 5-6, 834–848.

Iterative learning of answer set programs from context

LENKA, A., NADUTHOTA, R., JHA, M., R, R. P., PRAJAPATI, A., JHUNJHUNWALA, K., SAINI, J., YA-
DAV, R., BHARATH, R., AND PAL, P. 2015. Freezing of gait in parkinsons disease is associated with
altered functional brain connectivity. Parkinsonism & Related Disorders 24, 100–106. Elsevier.

LEOFANTE, F., NARODYTSKA, N., PULINA, L., AND TACCHELLA, A. 2018. Automated veriﬁcation of

neural networks: Advances, challenges and perspectives. CoRR abs/1805.09938.

LEONE, N. AND RICCA, F. 2015. Answer set programming: A tour from the basics to advanced devel-
opment tools and industrial applications. In Web Reasoning and Rule Systems - 9th International Con-
ference, RR 2015, Berlin, Germany, August 4-5, 2015, Proceedings. Lecture Notes in Computer Science
(LNCS). Springer Verlag, 308–326.

LIERLER, Y. AND SUSMAN, B. 2017. On relation between constraint answer set programming and satis-

ﬁability modulo theories. TPLP 17, 4, 559–590.

LIFSCHITZ, V. 1999. Answer Set Planning. In Proceedings of the 16th International Conference on Logic
Programming (ICLP’99), D. D. Schreye, Ed. The MIT Press, Las Cruces, New Mexico, USA, 23–37.
LONC, Z. AND TRUSZCZYNSKI, M. 2006. Computing minimal models, stable models and answer sets.

TPLP 6, 4, 395–449.

LUBLIN, F. D., REINGOLD, S. C., COHEN, J. A., CUTTER, G. R., SØRENSEN, P. S., THOMPSON, A. J.,
WOLINSKY, J. S., BALCER, L. J., BANWELL, B., BARKHOF, F., ET AL. 2014. Deﬁning the clinical
course of multiple sclerosis: the 2013 revisions. Neurology 83, 3, 278–286.

MANNA, M., RICCA, F., AND TERRACINA, G. 2015. Taming primary key violations to query large incon-
sistent data via ASP. Theory and Practice of Logic Programming (TPLP).. Cambridge University Press,
UK. 15 (4-5), 696–710.

MARATEA, M., PULINA, L., AND RICCA, F. 2014. A multi-engine approach to answer-set programming.

Theory and Practice of Logic Programming 14, 6, 841–868.

MAREK, V. W. AND TRUSZCZY ´NSKI, M. 1999. Stable Models and an Alternative Logic Programming
In The Logic Programming Paradigm – A 25-Year Perspective, K. R. Apt, V. W. Marek,

Paradigm.
M. Truszczy´nski, and D. S. Warren, Eds. Springer Verlag, 375–398.

Theory and Practice of Logic Programming

43

MCDONALD, W. I., COMPSTON, A., EDAN, G., GOODKIN, D., HARTUNG, H.-P., LUBLIN, F. D., MC-
FARLAND, H. F., PATY, D. W., POLMAN, C. H., REINGOLD, S. C., ET AL. 2001. Recommended
diagnostic criteria for multiple sclerosis: guidelines from the international panel on the diagnosis of mul-
tiple sclerosis. Annals of Neurology: Ofﬁcial Journal of the American Neurological Association and the
Child Neurology Society 50, 1, 121–127.

MELLARKOD, V. S., GELFOND, M., AND ZHANG, Y. 2008.

Integrating answer set programming and

constraint logic programming. Ann. Math. Artif. Intell. 53, 1-4, 251–287.

NEWMAN, M. E. J. 2002. Assortative mixing in networks. Phys. Rev. Lett. 89, 208701.
NICKLES, M. AND MILEO, A. 2014. Web stream reasoning using probabilistic answer set programming.

In RR. LNCS, vol. 8741. Springer, 197–205.

NIEMEL ¨A, I. 1999. Logic Programming with Stable Model Semantics as Constraint Programming

Paradigm. Annals of Mathematics and Artiﬁcial Intelligence 25, 3–4, 241–273.

PETERSEN, R. 2004. Mild cognitive impairment as a diagnostic entity. Journal of internal medicine 256, 3,

183–194. Wiley Online Library.

PRZYMUSINSKI, T. C. 1991. Stable Semantics for Disjunctive Programs. New Gen. Comput. 9, 401–424.
PULINA, L. AND TACCHELLA, A. 2010. An abstraction-reﬁnement approach to veriﬁcation of artiﬁcial
neural networks. In Computer Aided Veriﬁcation, T. Touili, B. Cook, and P. Jackson, Eds. Springer Berlin
Heidelberg, Berlin, Heidelberg, 243–257.

QIU, J., TANG, J., MA, H., DONG, Y., WANG, K., AND TANG, J. 2018. Deepinf: Social inﬂuence pre-
diction with deep learning. In Proc. of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 2110–2119. ACM. New York, NY, USA.

RATH, J. AND REDL, C. 2017. Integrating answer set programming with object-oriented languages. In
Practical Aspects of Declarative Languages - 19th International Symposium, PADL 2017, Paris, France,
January 16-17, 2017, Proceedings, Y. Lierler and W. Taha, Eds. Lecture Notes in Computer Science, vol.
10137. Springer, 50–67.

REDL, C. 2016. The dlvhex system for knowledge representation: recent advances (system description).

TPLP 16, 5-6, 866–883.

RICCA, F. 2003. A java wrapper for DLV. In Answer Set Programming, Advances in Theory and Imple-
mentation, Proceedings of the 2nd Intl. ASP’03 Workshop, Messina, Italy, September 26-28, 2003, M. D.
Vos and A. Provetti, Eds. CEUR Workshop Proceedings, vol. 78. CEUR-WS.org.

RUBINOV, M. AND SPORNS, O. 2010. Complex network measures of brain connectivity: Uses and inter-

pretations. NeuroImage 52, 3, 1059–1069.

SCH ¨ULLER, P. AND WEINZIERL, A. 2015. Answer set application programming: a case study on tetris. In
Proceedings of the Technical Communications of the 31st International Conference on Logic Program-
ming (ICLP 2015), Cork, Ireland, August 31 - September 4, 2015., M. D. Vos, T. Eiter, Y. Lierler, and
F. Toni, Eds. CEUR Workshop Proceedings, vol. 1433. CEUR-WS.org.

SHEN, D. AND LIERLER, Y. 2018. Smt-based constraint answer set solver EZSMT+ for non-tight pro-
grams. In Principles of Knowledge Representation and Reasoning: Proceedings of the Sixteenth Inter-
national Conference, KR 2018, Tempe, Arizona, 30 October - 2 November 2018., M. Thielscher, F. Toni,
and F. Wolter, Eds. AAAI Press, 67–71.

SHOVON, M. H. I., NANDAGOPAL, N., VIJAYALAKSHMI, R., DU, J. T., AND COCKS, B. 2017. Directed
connectivity analysis of functional brain networks during cognitive activity using transfer entropy. Neural
Processing Letters 45, 3 (Jun), 807–824.

SIMONYAN, K., VEDALDI, A., AND ZISSERMAN, A. 2013. Deep inside convolutional networks: Visual-

ising image classiﬁcation models and saliency maps. CoRR abs/1312.6034.

STAMILE, C., KOCEVAR, G., COTTON, F., HANNOUN, S., DURAND-DUBIEF, F., FRINDEL, C.,
ROUSSEAU, D., AND SAPPEY-MARINIER, D. 2015. A longitudinal model for variations detection in
white matter ﬁber-bundles. 2015 International Conference on Systems, Signals and Image Processing
(IWSSIP), 57–60.

STEIN, M., SIMMONS, A., FEINSTEIN, J., AND PAULUS, M. 2007. Increased amygdala and insula acti-
vation during emotion processing in anxiety-prone subjects. The American journal of psychiatry 164 2,
318–327. Psychiatry Online.

44

F. Calimeri et al.

TERRACINA, G., LEONE, N., LIO, V., AND PANETTA, C. 2008. Experimenting with recursive queries in
database and logic programming systems. Theory and Practice of Logic Programming (TPLP). Available
on-line at http://arxiv.org/abs/0704.3157. Cambridge University Press, UK 8(2), 129–165.

THIMM, M. 2014. Tweety - a comprehensive collection of java libraries for logical aspects of artiﬁcial intel-
ligence and knowledge representation. In Proceedings of the 14th International Conference on Principles
of Knowledge Representation and Reasoning (KR’14).

TOURNIER, J., CALAMANTE, F., AND CONNELLY, A. 2012. Mrtrix: Diffusion tractography in crossing

ﬁber regions. Int. J. Imaging Systems and Technology 22, 1, 53–66.

TOWELL, G. G. AND SHAVLIK, J. W. 1993. Extracting reﬁned rules from knowledge-based neural net-

works. Machine Learning 13, 71–101.

VOS, T., ALLEN, C., ARORA, M., BARBER, R., BHUTTA, Z., AND BROWN, A. 2016. Gbd 2015 disease
and injury incidence and prevalence collaborators. global, regional, and national incidence, prevalence,
and years lived with disability for 310 diseases and injuries, 1990-2015: a systematic analysis for the
global burden of disease study 2015. Lancet 388, 10053, 1545–1602.

WIESER, H., SCHINDLER, K., AND ZUMSTEG, D. 2006. EEG in Creutzfeldt–Jakob disease. Clinical

Neurophysiology 117, 5, 935–951. Elsevier.

WU, Z., PAN, S., CHEN, F., LONG, G., ZHANG, C., AND YU, P. 2019. A comprehensive survey on graph

neural networks. CoRR abs/1901.00596.

ZHANG, Q., CAO, R., ZHANG, S., EDMONDS, M., WU, Y. N., AND ZHU, S. 2017. Interactively trans-

ferring CNN patterns for part localization. CoRR abs/1708.01783.

ZHANG, Q., WU, Y. N., AND ZHU, S. 2018. Interpretable convolutional neural networks. In 2018 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June
18-22, 2018. IEEE Computer Society, 8827–8836.

ZHANG, Q., YANG, Y., WU, Y. N., AND ZHU, S. 2018.

Interpreting cnns via decision trees.

CoRR abs/1802.00121.

ZHANG, Q. AND ZHU, S. 2018. Visual interpretability for deep learning: a survey. Frontiers of IT &

EE 19, 1, 27–39.

