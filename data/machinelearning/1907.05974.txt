9
1
0
2

l
u
J

2
1

]

M
D
.
s
c
[

1
v
4
7
9
5
0
.
7
0
9
1
:
v
i
X
r
a

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT∗

LUCAS LAIRD† , RICHARD C. TILLQUIST‡ , STEPHEN BECKER† , AND MANUEL E.
LLADSER† §

Abstract. A subset of vertices in a graph is called resolving when the geodesic distances to
those vertices uniquely distinguish every vertex in the graph. Here, we characterize the resolvability
of Hamming graphs in terms of a constrained linear system and deduce a novel but straightforward
characterization of resolvability for hypercubes. We propose an integer linear programming method
to assess resolvability rapidly, and provide a more costly but deﬁnite method based on Gr¨obner
bases to determine whether or not a set of vertices resolves an arbitrary Hamming graph. As proof
of concept, we identify a resolving set of size 77 in the metric space of all octapeptides (i.e., proteins
composed of eight amino acids) with respect to the Hamming distance; in particular, any octamer
may be readily represented as a 77-dimensional real-vector. Representing k-mers as low-dimensional
numerical vectors may enable new applications of machine learning algorithms to symbolic sequences.

Key words. graph embedding, Gr¨obner basis, Hamming distance, Hamming graph, hypercube,
integer linear programming, metric dimension, multilateration, resolving set, symbolic data science

AMS subject classiﬁcations. 05C12, 05C50, 05C62, 68R10, 90C35, 92C40

1. Introduction. In what follows, k ≥ 1 and a ≥ 2 are ﬁxed integers, and we
refer to elements in the set V := {0, . . . , a − 1}k as k-mers, which we represent either
as strings or row vectors depending on the context.

The Hamming distance between two k-mers u and v, from now on denoted as
d(u, v), is the number of coordinates where the k-mers diﬀer, and is a valid metric.
The Hamming graph Hk,a has V as its vertex set, and two k-mers u and v are adjacent
(i.e. connected by an undirected edge) if and only if d(u, v) = 1, i.e. u and v diﬀer at
exactly one coordinate. As a result, the (geodesic) distance between two vertices in
Hk,a is precisely their Hamming distance (see Figure 1). The literature refers to the
Hamming graph with a = 2 as the (k-dimensional) hypercube.

A non-empty set R ⊆ V is called resolving when for all u, v ∈ V, with u (cid:54)= v,
there exists r ∈ R such that d(u, r) (cid:54)= d(v, r). In other words, R multilaterates V.
For instance, V resolves Hk,a because d(u, v) = 0 if and only if u = v. Equivalently,
R ⊆ V is resolving if and only if the transformation Φ : V → R|R| deﬁned as Φ(v) :=
(d(v, r))r∈R is one-to-one. In particular, the smaller a resolving set of Hk,a, the lower
the dimension needed to represent k-mers as points in a Euclidean space, which may
be handy e.g. to represent symbolic data numerically for machine learning tasks [29].
The metric dimension of Hk,a, which we denote β(Hk,a), is deﬁned as the size of
a minimal resolving set in this graph [17, 27]. For instance, β(H1,a) = (a − 1) because
H1,a is isomorphic to Ka−1, the complete graph on (a − 1) vertices [5]. Unfortunately,
computing the metric dimension of an arbitrary graph is a well-known NP-complete
problem [6, 13, 20], and it remains unknown if this complexity persists when restricted
to Hamming graphs. In fact, the metric dimension of hypercubes is only known up
to dimension k = 10 [17], and values have been conjectured only up to dimension
k = 17 [24]—see OEIS sequence A303735 for further details [25].

∗

Funding: This work has been partially funded by the NSF grant No. 1836914.
†Department of Applied Mathematics, University of Colorado, Boulder.
‡Department of Computer Science, University of Colorado, Boulder.
§Corresponding author. (manuel.lladser@colorado.edu)

1

 
 
 
 
 
 
2

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

Fig. 1. Visual representation of H1,3, H2,3, and H3,3. Blue-colored vertices form minimal

resolving sets in their corresponding Hamming graph.

Integer linear programming (ILP) formulations have been used to search for min-
imal resolving sets [5, 9]. In the context of Hamming graphs, a potential resolving
set R is encoded by a binary vector y of dimension ak such that yj = 1 if j ∈ R and
yj = 0 if j ∈ V \ R. One can then search for a minimal resolving set for Hk,a by
solving the ILP [5]:

min
y

(1.1)

subject to

(cid:88)

j∈V
(cid:88)

yj

|d(u, j) − d(v, j)| · yj ≥ 1, ∀u (cid:54)= v ∈ V

j∈V
yj ∈ {0, 1}, ∀j ∈ V.

The ﬁrst constraint ensures that for all pairs of diﬀerent vertices u and v, there is
some j ∈ R such that |d(u, j) − d(v, j)| > 0, hence R resolves Hk,a. The objective
penalizes the size of the resolving set. (A variant due to [9] is similar but stores ak
copies of a binary version of the distance matrix of the graph.) One downside of
this formulation is that forming the distance matrix of Hk,a requires O(a2k) storage,
as well as signiﬁcant computation. Moreover, standard approaches to reduce the
computation below O(a2k), such as fast multipole methods [15] and kd-trees [2], do
not obviously apply. Even if one could compute all pairwise distances between nodes,
simply storing the distance matrix is impractical. To ﬁx ideas, the graph H8,20—
which is associated with octapeptides (see section 6)—has 208 nodes, so storing the
distance matrix with log2(8) = 3 bits per entry and taking advantage of symmetry
would require 3(cid:0)208

(cid:1) bits, or approximately a prohibitive 123 exabytes.
Due to the above diﬃculties, other eﬀorts have focused on ﬁnding small resolving
sets rather than minimal ones. When ak is small, resolving sets for Hk,a may be
determined using the so-called Information Content Heuristic (ICH) algorithm [18],
or a variable neighborhood search algorithm [24]. Both approaches quickly become
intractable with increasing k. However, the highly symmetric nature of Hamming
graphs can be taken advantage of to overcome this problem. Indeed, recent work [29]
has shown that β(Hk,a) ≤ β(Hk−1,a) + (cid:98)a/2(cid:99); in particular, β(Hk,a) ≤ (k − 1)(cid:98)a/2(cid:99) +
(a − 1) i.e., just O(k) nodes are enough to resolve all the ak nodes in Hk,a. Moreover,
one can ﬁnd a resolving set of size O(k) in only O(ak2) time [29].

2

This manuscript is based on the recent Bachelor’s thesis [21], and has two overar-
ching goals. First, it aims to develop practical methods for certifying the resolvability,
or lack thereof, of subsets of nodes in arbitrary Hamming graphs. So far, this has been

102021000012021      2212       11         212       210      211010011012111110112201200202   120   002          001          000         021         020        022        102100101  222         221         220          121         122RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

3

addressed for hypercubes in the literature [1] but remains unexamined for arbitrary
values of the parameter a. While our work does not directly address the problem
of searching for minimal resolving sets, verifying resolvability is a key component of
any such search and may shed new light on the precise metric dimension of Hk,a in
future investigations. Second, this paper aims also to exploit said characterization
to remove unnecessary nodes—if any—in known resolving sets. This problem, which
is infeasible by brute force when ak is large, has not received any attention in the
literature despite being crucial for the embedding of k-mers into the Euclidean space
of a lowest possible dimension.

The paper is organized as follows. Our main theoretical results are presented
ﬁrst in section 2. Theorem 2.2 provides the foundation from which we address the
problem of verifying resolvability in Hamming graphs and implies a new characteri-
zation of resolvability of hypercubes (Corollary 2.4). An illustrative example shows
the utility of Theorem 2.2 but raises several practical challenges in its implementation
on large Hamming graphs. Section 3 describes a computationally demanding veriﬁ-
cation method based on Gr¨obner bases that is nevertheless more eﬃcient than the
brute force approach and determines with certainty whether or not a set of nodes in
Hk,a is resolving. Computational issues are addressed in section 4 with a novel ILP
formulation of the problem. This approach is fast but stochastic and hence has the
potential to produce false positives or false negatives. Section 5 compares the run
time of these methods against a brute force approach across small Hamming graphs.
Combining the techniques from sections 3 and 4, section 6 presents a simple approach
to discovering and removing redundant nodes in a given resolving set. This approach
allows us to improve on previous bounds on the metric dimension of the Hamming
graph H8,20. Finally, two appendices provide background information about Gr¨obner
bases and linear programming.

All code used in this manuscript is available on GitHub (https://github.com/

hamming-graph-resolvability/Hamming Resolvability).

2. Main results. In what follows Tr(A) denotes the trace of a square matrix A,
B(cid:48) the transpose of a matrix or vector B, and vec(C) the column-major ordering of
a matrix C i.e. the row vector obtained by appending from left to right the entries in
each column of C. For instance:

vec

(cid:18)(cid:20) a b
d
c

(cid:21)(cid:19)

= (a, c, b, d).

In addition, ¯D denotes the ﬂip of the entries in a binary matrix (or vector) D, that is
0 is mapped to 1, and vice versa.

The one-hot encoding of a k-mer v is deﬁned as the binary matrix V of dimension
(a × k) such that V [i, j] = 1 if and only if (i − 1) = v[j] (the oﬀset in i is needed since
the reference alphabet is {0, ..., a − 1} instead of {1, . . . , a}). Here, V [i, j] denotes
the entry in row-i and column-j of the matrix V , and similarly v[j] denotes the j-th
coordinate of the vector v. We also follow the convention of capitalizing k-mer names
to denote their one-hot encodings.

Our ﬁrst result links one-hot encodings of k-mers with their Hamming distance.

Note this result applies to any alphabet size, not just binary.

Lemma 2.1. If u, v are k-mers with one-hot encodings U, V , respectively, then

d(u, v) = k − Tr(U (cid:48)V ); in particular, d(u, v) = Tr(U (cid:48) ¯V ).

Proof. Let Ui and Vi be the i-th column of U and V , respectively. Clearly,
if u[i] = v[i] then (cid:104)Ui, Vi(cid:105) = 1, and if u[i] (cid:54)= v[i] then (cid:104)Ui, Vi(cid:105) = 0, because all

4

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

but one of the entries in Ui and Vi vanish and the non-vanishing entries are equal
to 1. As a result, Tr(U (cid:48)V ) = (cid:80)k
i=1(cid:104)Ui, Vi(cid:105) counts the number of positions where
u and v are equal; in particular, d(u, v) = k − Tr(U (cid:48)V ). Finally, observe that if
1a×k denotes the (a × k) matrix with all entries equal to 1 then Tr(U (cid:48)1a×k) = k
because every row of U (cid:48) has exactly one 1 and all other entries vanish. As a result,
d(u, v) = Tr(U (cid:48)(1a×k − V )) = Tr(U (cid:48) ¯V ), as claimed.

We can now give a necessary and suﬃcient condition for a subset of nodes in an

arbitrary Hamming graph to be resolving.

Theorem 2.2. Let v1, . . . , vn be n ≥ 1 k-mers and V1, . . . , Vn their one-hot en-

codings, respectively, and deﬁne the (n × ak) matrix with rows

(2.1)

A :=






vec(V1)
...
vec(Vn)




 .

Then R := {v1, . . . , vn} resolves Hk,a if and only if 0 is the only solution to the
linear system Az = 0, with z a column vector of dimension ak, satisfying the follow-
ing constraints:
if z is parsed into k consecutive but non-overlapping subvectors of
dimension a, namely z = ((z1, . . . , za), (za+1, . . . , z2a), ..., (z(k−1)a+1, . . . , zka))(cid:48), then
each subvector is the diﬀerence of two canonical vectors.

Proof. Before showing the theorem observe that, for any pair of matrices A and
B of the same dimension, Tr(A(cid:48)B) = (cid:104)vec(A), vec(B)(cid:105), where (cid:104)·, ·(cid:105) is the usual inner
product of real vectors.

Consider k-mers x and y, and let X and Y be their one-hot encodings, respec-
tively. Due to Lemma 2.1, d(vi, x) = d(vi, y) if and only if Tr(V (cid:48)
i (X − Y )) = 0 i.e.
(cid:104)vec(Vi), vec(X − Y )(cid:105) = 0. As a result, the set R does not resolve Hk,a if and only if
there are k-mers x and y such that Az (cid:54)= 0, where z := vec(X)−vec(Y ). Note however
that each column X and Y equals a canonical vector in Ra; in particular, if we parse
vec(X) and vec(Y ) into k subvectors of dimension a as follows: vec(X) = (x1, . . . , xk)
and vec(Y ) = (y1, . . . , yk), then z = (x1 − y1, . . . , xk − yk) with x(cid:48)
i canonical
vectors in Ra. This shows the theorem.

i and y(cid:48)

2.1. Illustrative Example. In H2,3 consider the set of nodes R0 = {02, 11}.

From Theorem 2.2, R0 resolves H2,3 if and only if A0z = 0, with

(2.2)

A0 =

(cid:20)1 0 0
0 1 0

0
0

0
1

(cid:21)

1
0

,

has no non-trivial solution z which satisﬁes the other constraints in the theorem
when writing z = (cid:0)(z1, z2, z3), (z4, z5, z6)(cid:1)(cid:48)
. Something useful to note about this
decomposition is that if a subvector of z has two identical entries, then all the entries
in that subvector must vanish.

Note that A0 is already in its reduced row echelon form [26], and has two pivots:
z1 = −z6 and z2 = −z5. Seeking non-trivial solutions to the constrained linear system,
we examine permissible values for z5 and z6:

(a) If z5 = −1 then we must have (z4, z6) ∈ {(0, 1), (1, 0)}. Furthermore, if
z6 = 1 then (z1, z2, z3) = (−1, 1, 0), but if z6 = 0 then (z1, z2, z3) = (0, 1, −1).
Consequently, z = (−1, 1, 0, 0, −1, 1) and z = (0, 1, −1, 1, −1, 0) solve the
constrained system.

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

5

(b) Similarly, we ﬁnd that z = (−1, 0, 1, −1, 0, 1) and z = (1, 0, −1, 1, 0, −1) solve

the constrained system when we assume that z5 = 0.

(c) Finally, z = (1, −1, 0, 0, 1, −1) and z = (0, −1, 1, −1, 1, 0) are also found to

solve the constrained system when we impose that z5 = 1.

Having found at least one non-trivial solution to the constrained linear system, we
conclude that R0 does not resolve H2,3. (The found z’s are in fact the only non-trivial
solutions.)

From the proof of Theorem 2.2, we can also determine pairs of vertices in H2,3
which are not resolved by R0. Indeed, using the non-trivial solutions found above we
ﬁnd that 12 and 01, 21 and 10, and 00 and 22 are the only pairs of nodes in H2,3
which are unresolved by R0. In particular, because the distances between the nodes
in each pair and 22 are diﬀerent, R1 := R0 ∪ {22} resolves H3,2.

We can double-check this last assertion noticing that the reduced echelon form of

the matrix A1 associated with R1 is

(2.3)

rref(A1) =


1
0

0

0
1
0

0
0
1

0
0
0

0
1
0



 .

1
0
1

In particular, z1 = −z6, z2 = −z5, and z3 = −z6. The ﬁrst and third identity imply
that z1 = z2, hence (z1, z2, z3) = (0, 0, 0). This together with the ﬁrst and second
identity now imply that (z4, z5, z6) = (0, 0, 0). So, as anticipated, z = 0 is the only
solution to the constrained linear system A1z = 0.

In general, if the reduced row echelon form of the matrix given by Theorem 2.2
has j free variables, then there could be up to 3j possible solutions to the associated
linear system, each of which would have to be checked for the additional constraints.
This exhaustive search could be very time consuming if not impossible. Handling
the linear system constraints more systematically and eﬃciently is the motivation for
sections 3 and 4.

2.2. Specializations to Hypercubes. In [1] a necessary and suﬃcient condi-
tion for the resolvability of hypercubes is provided exploiting that d(u, v) = (cid:107)u − v(cid:107)2
2
when u and v are binary k-mers. Next, we reproduce this result using our framework
of one-hot encodings instead.

Corollary 2.3. [1, Theorem 2.2] Let R = {v1, . . . , vn} be a set of n ≥ 1 binary

k-mers, and deﬁne the (n × k) matrix with rows

B :=






v1 − ¯v1
...
vn − ¯vn




 .

Then, R resolves Hk,2 if and only if ker(B) ∩ {0, ±1}k = {0}.

Proof. Let


 A1 A2

A =

. . . A2k−1 A2k





be the (n×2k) matrix with columns A1, . . . , A2k given by Theorem 2.2 for R. It follows
that R resolves Hk,2 if and only if Az = 0, with z = ((x1, y1), . . . , (xk, yk))(cid:48) ∈ {0, ±1}2k
and (xi + yi) = 0 for each i = 1, . . . , k, has only a trivial solution. Note however that

6

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

Az = By, where


 (A2 − A1)

B :=

y := (y1, . . . , yk)(cid:48).

. . .

(A2k − A2k−1)



 ;

Therefore R is resolving if and only if By = 0, with y ∈ {0, ±1}k, has only a trivial
solution. But recall from Theorem 2.2 that the rows of A are the column-major
orderings of the one-hot encodings of the binary k-mers in R. In particular, using
·
(cid:75)
to denote Iverson brackets, we ﬁnd that the row in B associated with v ∈ R is:

(cid:74)

(cid:16)

v[1] = 1
(cid:75)

(cid:74)

−

(cid:74)

v[1] = 0

, . . . ,
(cid:75)

v[k] = 1
(cid:75)

(cid:74)

−

v[k] = 0
(cid:74)

(cid:17)

(cid:75)

= v − ¯v,

from which the corollary follows.

We can provide an even simpler characterization of sets of k-mers that resolve
the hypercube, provided that 1k := (1, . . . , 1) is one of them. This seemly major
assumption is only superﬁcial. Indeed, hypercubes are transitive; that is, given any
two binary k-mers there is an automorphism (i.e., a distance preserving bijection
σ : {0, 1}k → {0, 1}k) that maps one into the other [29, §3.1]. Hence, given any set
R of binary k-mers there is an automphism σ such that 1k ∈ σ(R). In particular,
because R is resolving if and only if σ(R) is resolving, one can assume without any
loss of generality that 1k is an element of R.

Corollary 2.4. Let R = {v1, . . . , vn} be a set of n binary k-mers such that

1k ∈ R, and deﬁne the (n × k) matrix with rows

C :=




 .






v1
...
vn

Then, R resolves Hk,2 if and only if ker(C) ∩ {0, ±1}k = {0}.

Proof. Note that for all binary k-mers v: (v + ¯v) = 1k; in particular, (v − ¯v) =

(2v − 1k). Hence, if B is as given in Corollary 2.4 and C as deﬁned above then

Bz = 0 if and only if Cz = (cid:104)1k, z(cid:105)






1/2
...
1/2




 .

But, because 1k ∈ R and 2 · 1k − 1k = 1k, one of the entries in Bz equals (cid:104)1k, z(cid:105).
Since the entries in Cz are proportional to (cid:104)1k, z(cid:105), Bz = 0 if and only if Cz = 0, from
which the corollary follows.

3. Polynomial Roots Formulation. In this section, we express the constraints
of the linear system in Theorem 2.2 as roots of a multi-variable polynomial system,
and we reveal various properties about this system which can drastically reduce the
complexity of determining whether a subset of nodes resolves or not Hk,a.

In what follows, for any given non-empty set P of polynomials in a possibly multi-
variable z, {P = 0} denotes the set of z’s such that p(z) = 0, for each p ∈ P . Unless
otherwise stated, we assume that z has dimension ka, i.e. z = (z1, . . . , zka).

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

7

Consider the polynomial sets

P1 :=

(3.1)

P2 :=

P3 :=

(cid:110)










z3
i − zi, for i = 1, . . . , ka

(cid:111)
;

ia
(cid:88)

zj, for i = 1, . . . , k






;

j=(i−1)a+1

(cid:16)

2 −

ia
(cid:88)

(cid:17)

·

z2
j

ia
(cid:88)

z2
j , for i = 1, . . . , k

j=(i−1)a+1

j=(i−1)a+1






.

Our ﬁrst result characterizes the constraints of the linear system in Theorem 2.2 in
terms of the roots of the above polynomials. Ahead, unless otherwise stated:

(3.2)

P := (P1 ∪ P2 ∪ P3).

Lemma 3.1. z ∈ {P = 0} if and only if when parsing z into k consecutive but
non-overlapping subvectors of dimension a, each subvector is the diﬀerence of two
canonical vectors.

Proof. The polynomials in P1 enforce that each entry in z must be a −1, 0, or 1,
while the polynomials in P2 enforce that there is a (−1) for every 1 in each subvector
of z. Finally, the polynomials in P3 enforce that each subvector of z has exactly two
non-zero entries or no non-zero entries. Altogether, z ∈ P if and only if each subvector
is identically zero, or it has exactly one 1 and one (−1) entry and all other entries
vanish, i.e. each subvector of z is the diﬀerence of two canonical vectors in Ra.

The following is now an immediate consequence of this lemma and Theorem 2.2.
Corollary 3.2. Let R be a set of nodes in Hk,a and A the matrix given by

equation (2.1). Then, R resolves Hk,a if and only if ker(A) ∩ {P = 0} = {0}.

Our focus in what remains of this section is to better characterize the non-trivial
roots of the polynomial system {P = 0}. To do so, we rely on the concepts of
polynomial ideals and (reduced) Gr¨obner bases, and the following fundamental result
from algebraic geometry. For a primer to these and other concepts on which our
results rely see Appendix A.

Theorem 3.3. (Hilbert’s Weak Nullstellensatz [8, §4.1].) For any non-empty
ﬁnite set of polynials P , {P = 0} = ∅ if and only if {1} is the reduced Gr¨obner basis
of I(P ), the ideal generated by P .

Deﬁne for each i = 1, . . . , k:

(3.3)

Bi :=

(cid:110)

z3
j − zj, for j = (i − 1)a + 1, . . . , ia

(cid:111)

(cid:91)






ia
(cid:88)

(cid:16)

2 −

zj,

ia
(cid:88)

(cid:17)

·

z2
j

ia
(cid:88)

j=(i−1)a+1

j=(i−1)a+1

j=(i−1)a+1






.

z2
j

Observe that Bi is a set of polynomials in (z(i−1)a+1, . . . , zia), i.e. the i-th subvector
of z; in particular, each of these polynomials may be regarded as a function of z, and
B1, . . . , Bk partition P , i.e. P = (cid:116)k
i=1Bi. Accordingly, we call Bi the i-th block of P ,
and denote the reduced Gr¨obner basis of Bi as Gi. The computational advantage of
these observations is revealed by the following results.

8

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

Lemma 3.4. G = ∪k

i=1Gi is the reduced Gr¨obner bases of P in equation (3.2).

Furthermore, Gi may be obtained from G1 using the change of variables:

(3.4)

(z1, . . . , za) −→ (z(i−1)a+1, . . . , zia).

Proof. The case with k = 2 follows from [8, Proposition 2] due to the fact that no
variable and hence no polynomial is shared between the blocks of P . A straightforward
inductive argument in k ≥ 2 then shows that ∪k
i=1Gi is the reduced Gr¨obner bases of
P . Finally, note that B1 is up to the change of variables in equation (3.4) identical to
Bi; in particular, since Buchberger’s algorithm (Algorithm A.1) and the Gr¨obner basis
reduction algorithm (Algorithm A.2) build upon polynomial division, the reduced
Gr¨obner bases of Bi may be obtained from that of B1 using the same change of
variables.

Lemma 3.5. The reduced Gr¨obner bases of B1 under the lexicographic ordering is

(3.5)

G1 =

(cid:40) a

(cid:88)

(cid:41)

zi

(cid:91)

{z3

i − zi}

(cid:91)

{z2

i zj + ziz2
j }

(cid:91)

{zizjz(cid:96)}.

i=1

2≤i≤a

2≤i<j≤a

2≤i<j<(cid:96)≤a

Proof. Let G be the set of polynomials on the right-hand side above. Since G
depends on a but not on the parameter k of Hk,a, and the identity for a ∈ {2, 3, 4}
can be checked using algorithms A.1 and A.2, without loss of generality we assume in
what follows that a > 5.

Since reduced Gr¨obner basis are unique, it suﬃces to show that (i) I(G) = I(B1);
and that for all f, g ∈ G: (ii) the reduction of Spoly(f, g) by G is 0; (iii) LC(f ) = 1;
and (iv) if f ∈ G \ {g} then no monomial of f is divisible by LM (g). We omit the
tedious but otherwise straightforward veriﬁcation of properties (ii) and (iv). Since
property (iii) is trivially satisﬁed, it only remains to verify property (i).

i zj + ziz2

i − zi = zi(zi − 1)(zi + 1), z3

To prove I(G) = I(B1), it suﬃces to show that {G = 0} = {B1 = 0}. Indeed,
the polynomials of the form zizjz(cid:96) imply that if z ∈ {G = 0} then (z2, . . . , za−1) has
at most two non-zero coordinates. In the case two of these coordinates are non-zero,
say zi and zj, the polynomials z3
j − zj = zj(zj − 1)(zj + 1),
and z2
j = zizj(zi + zj) imply that (zi, zj) = (1, −1) or (zi, zj) = (−1, 1);
in particular, because we must have (cid:80)a
Instead, if exactly one
(cid:96)=1 z(cid:96) = 0, z1 = 0.
of the coordinates in (z2, . . . , za−1) is non-zero, say zj, then the polynomial (cid:80)a
(cid:96)=1 z(cid:96)
together with z3
j − zj imply that (z1, zj) = (1, −1) or (z1, zj) = (−1, 1). Finally,
if (z2, . . . , za−1) = 0 then the polynomial (cid:80)a
In all of
these three exhaustive cases, it follows that (z1, . . . , za) is identically zero, or it has
exactly one 1 and one (-1) coordinate and all other coordinates vanish; in other words,
(z1, . . . , za) is a diﬀerence of two canonical vectors in Ra. Since this is precisely the
constraint imposed on this subvector of z by the polynomials in B1, we obtain that
{G = 0} = {B1 = 0} i.e. I(G) = I(B1).

(cid:96)=1 z(cid:96) implies that z1 = 0.

A minor issue for using the Weak Nullstellensatz in our setting is that the poly-
nomials in P have no constant terms; in particular, 0 ∈ {P = 0}. To exclude this
trivial root, observe that if z ∈ P then (cid:80)ia
j ∈ {0, 2}, for each i = 1, . . . , k.
As a result, if z is a non-trivial root of {P = 0} then (cid:80)ka
j = 2i for some i. This

j=(i−1)a+1 z2

j=1 z2

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

9

motivates to introduce the auxiliary polynomial:

(3.6)

f (z) :=

(cid:16) ka
(cid:88)

(cid:17)

,

z2
j

j=1

so that R resolves Hk,a if and only if ker(A) ∩ {P = 0} ∩ {f − 2i = 0} = ∅ for all
i = 1, . . . , k.

Lemma 3.6. Consider a (ﬁnite) reduced Gr¨obner basis G (cid:54)= {1} and a polynomial

f . If f G−→ r then, for each c ∈ R, (f + c) G−→ (r + c).

Proof. Let G = {g1, . . . , gn}. Without loss of generality ﬁx a constant c (cid:54)= 0.
Note that G contains no constant polynomial (except for 0) because G (cid:54)= {1} hence
1 /∈ G. As a result, the leading monomial of each gi does not divide c, hence c G−→ c.
Since f G−→ r, and reductions by a Gr¨obner basis are unique, (f + c) G−→ (r + c) as
claimed.

The following is now a direct consequence of the lemma.
Corollary 3.7. Let G be the reduced Gr¨obner basis of P in equation (3.2). If f
is as deﬁned in (3.6) and f G−→ r then, for each i = 1, 2, . . . , k, (f − 2i) G−→ (r − 2i).

The results from this section allow for a computational method for checking re-
solvability on Hk,a. Lemmas 3.4 and 3.5 are used to construct the reduced Gr¨obner
basis G directly, and Corollary 3.7 eﬃciently removes the trivial solution from consid-
eration in the criteria provided by Theorem 3.3. Altogether these results signiﬁcantly
reduce the number of polynomial reductions required to assess the resolvability of a
set of nodes on Hk,a.

3.1. Illustrative Example (Continuation). We saw in Subsection 2.1 that
R0 = {02, 11} does not resolve H2,3 whereas R1 = R0 ∪ {22} = {02, 11, 22} does. We
can double-check these assertions using Corollary 3.2 as follows.

First, recall that for H2,3 the variable z is 6-dimensional and should be decom-
posed in the form z = (cid:0)(z1, z2, z3), (z4, z5, z6)(cid:1). Next, the kernel of the matrix given
by the corollary for R0 (denoted as A0, see Eq. (2.2)) is described by the linear system:

(cid:26) z1 + z6 = 0;
z2 + z5 = 0.

On the other hand, the roots in {P = 0} given by Corollary 3.2 correspond to the
polynomial system:






1 − z1;
2 − z2;
3 − z3;

0 = z3
0 = z3
0 = z3
0 = z1 + z2 + z3;
1 − z2
0 = (2 − z2
0 = z3
4 − z4;
0 = z3
5 − z5;
0 = z3
6 − z6;
0 = z4 + z5 + z6;
4 − z2
0 = (2 − z2

2 − z2

3) · (z2

1 + z2

2 + z2

3);

5 − z2

6) · (z2

4 + z2

5 + z2

6);

10

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

where the horizontal line distinguishes between the ﬁrst and second block of P (see
Eq. (3.3)). Finally, recall the auxiliary polynomial given by equation (3.6):

f (z) = z2

1 + z2

2 + z2

3 + z2

4 + z2

5 + z2
6.

Assuming the lexicographic order over the monomials, one can determine that the
reduced Gr¨obner basis of {A0z} ∪ P ∪ {f − 2} is {1}; in particular, ker(A0) ∩ {P =
0} ∩ {f = 2} = ∅. On the other hand, because the reduced Gr¨obner basis of {A0z} ∪
P ∪ {f − 2} is {z1 + z6, z2 + z5, z3 − z5 − z6, z4 + z5 + z6, z2
6 − z6}, it
follows that ker(A0) ∩ {P = 0} ∩ {f = 4} (cid:54)= ∅ i.e. ker(A0) ∩ {P = 0} has a non-trivial
solution. Consequently, R0 does not resolve H2,3.

5 + z5z6 + z2

6 − 1, z3

To conﬁrm that R1 = R0 ∪ {22} does resolve H2,3, note that we only need to
add the equation z3 + z6 = 0 to the previous linear system (the full linear system
is now described by the matrix A1, see Eq. (2.3)). Using our code, we ﬁnd that
ker(A1) ∩ {P = 0} ∩ {f − 2} = ∅ and also that ker(A1) ∩ {P = 0} ∩ {f − 4} = ∅
because the associated reduced Gr¨obner basis are both equal to {1}. As a result,
ker(A1) ∩ {P = 0} has no non-trivial solution, i.e. R1 resolves H2,3.

4. Novel Integer Linear Programming Formulation. For some background

about Integer Linear Programming (ILP), see Appendix B.

In contrast to the ILP approaches of [5, 9], our ILP formulation checks the resolv-
ability of a given set rather than searching for minimal resolving sets. Furthermore,
it does not pre-compute the distance matrix of a Hamming graph. As before, ﬁx Hk,a
and a subset of vertices R. Letting z = (z1, . . . , zka) and using the polynomial set P
from equation (3.2), we leverage Lemma 3.1 (with A as in equation (2.1), each row
corresponding to a vertex in R) to reformulate Theorem 2.2 as follows:

(4.1) R does not resolve Hk,a ⇐⇒ ∃z (cid:54)= 0 such that z ∈ ker(A) ∩ {P = 0}.

To formulate this as an ILP, we use the following result.
Lemma 4.1. Deﬁne

I :=




k
(cid:92)

i=1



z ∈ Zak such that

ia
(cid:88)

zj = 0 and

ia
(cid:88)

j=(i−1)a+1

j=(i−1)a+1

|zj| ≤ 2






.

Then I is the intersection of a closed convex polyhedron with the integer lattice Zak,
and z ∈ {P = 0} if and only if z ∈ I.

Proof. Since the intersection of convex sets is convex, and the intersection of a
ﬁnite number of polyhedra is a polyhedron, it follows from standard arguments that

J1 :=

J2 :=








k
(cid:92)

i=1

k
(cid:92)

i=1



z ∈ Rak such that

z ∈ Rak such that

ia
(cid:88)

j=(i−1)a+1

ia
(cid:88)

j=(i−1)a+1

zj = 0






;

|zj| ≤ 2






;

are convex subsets of Rak, and J1 is a polyhedron. We claim that J2 is also a
polyhedron, for which it suﬃces to check that each set in the intersection that deﬁnes
it is a polyhedron. Without loss of generality, we do so only for the case with i =

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

11

1. Indeed, because {z ∈ Rak such that (cid:80)a
coordinate sign ﬂips, we have that

j=1 |zj| ≤ 2} is invariant under arbitrary






z ∈ Rak such that

a
(cid:88)

j=1

|zj| ≤ 2






(cid:92)

=

w∈{−1,1}ak






z ∈ Rak such that

a
(cid:88)

j=1

wjzj ≤ 2






,

which implies that J2 is also a polyhedron. Since I = (J1 ∩ J2 ∩ Zak), the ﬁrst part
of the lemma follows.

From the proof of Lemma 3.1 it is immediate that {P = 0} ⊂ I. To show the
converse inclusion, observe that {P = 0} = ∩k
i=1{Bi = 0} where the Bi’s are as
deﬁned in equation (3.3). To complete the proof, it suﬃces therefore to show that
Ii ⊂ {Bi = 0}, where

Ii :=






z ∈ Zak such that

ia
(cid:88)

zj = 0 and

ia
(cid:88)

|zj| ≤ 2

j=(i−1)a+1

j=(i−1)a+1






.

j=1 |zj| ≤ 2 implies that |zj| ∈ {0, 1, 2} for j = 1, . . . , a.

Indeed, if z ∈ I1 then, because the coordinates of z are integers, the condition
(cid:80)a
If |zj| = 2 for some j
then (cid:80)a
j=1 zj = ±2, which is not possible. Thus zj ∈ {0, ±1} for j = 1, . . . , a; in par-
j − zj = 0. On the other hand, the condition (cid:80)a
ticular, z3
j=1 zj = 0 implies that the
number of 1’s and (-1)’s in (z1, . . . , za) balance out; in particular, since (cid:80)a
j=1 |zj| ≤ 2,
either (z1, . . . , za) vanishes, or it has exactly one 1 and one (-1) entry and all other
entries vanish; in particular, (2 − (cid:80)a
j = 0. Thus, z ∈ {B1 = 0}. The
case for i > 1 is of course the same.

j ) · (cid:80)a

j=1 z2

j=1 z2

Remark 4.2. With current ILP solvers, one can impose that z ∈ {0, ±1}ak simply
as |zi| ≤ 1 for i = 1, . . . , ak. On the other hand, while a constraint like (cid:80)a
j=1 |zj| ≤ 2
is clearly polyhedral, it is not in the form of an aﬃne equality or inequality suitable
for ILP solvers. Nevertheless, standard reformulation techniques can convert this into
a set of aﬃne equalities and inequalities in a higher dimensional space. For example,
in the product space with variables (˜z, w), we can write the constraint as (cid:80)a
j=1 wj ≤ 2
and |˜zj| ≤ wj (i.e., ˜zj ≤ wj and −˜zj ≤ wj), which leads to an equivalent formulation of
the original ILP. One may handle such reformulations automatically using the Matlab
package CVX [14].

It only remains to encode the fact that we look for a nonzero root in {P = 0},

which we do via the ILP in the following theorem:

Theorem 4.3. A subset of vertices R is not resolving on Hk,a if and only if the

solution to the following ILP is less than zero:

min
z∈Rak

ak
(cid:88)

j=1

2jzj

(4.2)

subject to Az = 0 and z ∈ I,

where A is deﬁned in equation (2.1).

Proof. Using equation (4.1) and Lemma 4.1, it remains to show that the objective
function is less than zero if and only if there is a non-zero feasible z. Suppose there is
not a non-zero feasible z. Clearly z = 0 is feasible, hence it is the only feasible point

12

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

for the ILP, and the objective value is zero. Now suppose there is some non-zero
feasible z. Let j(cid:48) be the largest non-zero coordinate. Then because (cid:80)j(cid:48)−1
j=1 2j < 2j(cid:48)
,
and because each entry is bounded |zj| ≤ 1, the objective value at this z is non-zero.
If the objective value is negative, this proves the value of the ILP is negative; if the
objective value is positive, then observe that (−z) is also feasible and has a negative
objective value, and hence the value of the ILP is negative.

Remark 4.4. If the solution to the ILP is less than zero and hence R is not a
resolving set, then each optimal vector z is the diﬀerence of the column-major ordering
of the one-hot encodings of two k-mers which are not resolved by R; in particular, a
vector that resolves these k-mers needs to be added to R to resolve Hk,a.

4.1. Practical formulations and roundoﬀ error. When ak is small, it is
feasible to directly solve the ILP in equation (4.2). One issue with larger values of
ak, besides an obvious increase in run-time, is that the values of 2j in the objective
function quickly lead to numerical overﬂow. A simple ﬁx is to replace each coeﬃ-
cient cj = 2j with an independently drawn realization of a standard normal random
variable N (0, 1). Since these new coeﬃcients are independent of the feasible set, if
the latter is truly larger than {0}, the probability that the entire feasible set is in
the null-space of the linear function (cid:80)ak
j=1 cjzj is zero. Of course, again due to ﬁnite
machine precision, this otherwise almost surely exact method may only be approxi-
mate. Admittedly, when running the ILP with the random coeﬃcients cj’s, ﬁnding an
undoubtedly negative solution to the ILP would certify that the set R is not resolving.
However, if the solution is just slightly negative or vanishes within machine precision,
the assessment about R should be taken with a grain of salt. In this case, one should
draw a new set of random coeﬃcients and re-run the ILP to reassess the resolvability
of R.

Another consideration is that the ILP solver wastes time ﬁnding a feasible point
with the smallest possible objective, when we only care if there is a feasible point with
objective smaller than 0. Thus we could solve the feasiblity problem

Find

z ∈ Rak

subject to Az = 0 and z ∈ I and (cid:104)c, z(cid:105) < 0

where cj = 2j or cj ∼ N (0, 1) as discussed above.
(Feasibility problems can be
encoded in software by minimizing the 0 function.) Unfortunately this is not an ILP
because {c | (cid:104)c, z(cid:105) < 0} is not a closed set. We can partially ameliorate this by solving

Find

z ∈ Rak

(4.3)

subject to Az = 0 and z ∈ I and (cid:104)c, z(cid:105) ≤ −δ

where δ > 0 is a small number (our code uses δ = 10−3). Finding a feasible point
z is then proof that the set R does not resolve Hk,a.
If the solver says the above
problem is infeasible, it could be that δ was too large and hence the computation was
inconclusive. In this case, one could run the slower program (4.2).

5. Computational Complexity Experiments. The theoretical framework
and algorithms proposed in this paper provide a novel way of approaching resolv-
ability on Hamming graphs. To show the computational feasibility and practicality
of our methods, we compare the average run-time of both the ILP and Gr¨obner basis
algorithms against the brute force approach for checking resolvability. Our experi-
ments use Python 3.7.3 and SymPy version 1.1.1 [23], and the commercial ILP solver
gurobi ver. 7.5.2 [16].

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

13

(k, a)

(2,2)
(2,4)
(3,3)
(5,2)
(3,5)
(5,3)
(8,2)
(4,4)
(5,5)
(12,2)
(6,4)
(8,3)

ak

4
16
27
32
125
243
256
256
3125
4096
4096
6561

Brute Force

Gr¨obner Basis

ILP

Mean

SD

Mean

SD

Mean

SD

3.88 × 10−5
2.47 × 10−4
5.02 × 10−4
6.61 × 10−4
8.98 × 10−3
2.78 × 10−2
2.85 × 10−2
3.13 × 10−2
5.19
6.28
7.78
2.02 × 101

1.51 × 10−6
6.83 × 10−5
2.45 × 10−4
3.29 × 10−4
5.38 × 10−3
1.96 × 10−2
2.21 × 10−2
1.97 × 10−2
3.17
5.34
4.65
1.40 × 101

6.79 × 10−3
2.25 × 10−2
2.83 × 10−2
3.14 × 10−2
1.12 × 10−1
1.22 × 10−1
9.87 × 10−2
1.27 × 10−1
4.00
2.93 × 10−1
7.73 × 10−1
1.12 × 101
Table 1

1.06 × 10−3
2.59 × 10−3
7.92 × 10−3
5.27 × 10−3
2.91 × 10−2
7.88 × 10−2
1.96 × 10−2
3.90 × 10−2
3.54
7.24 × 10−2
3.67 × 10−1
1.47 × 101

1.28 × 10−1
1.16 × 10−1
1.21 × 10−1
1.28 × 10−1
1.37 × 10−1
1.20 × 10−1
1.17 × 10−1
1.37 × 10−1
1.35 × 10−1
1.24 × 10−1
1.52 × 10−1
1.62 × 10−1

3.53 × 10−3
4.84 × 10−3
8.12 × 10−3
3.91 × 10−3
7.02 × 10−3
8.12 × 10−3
1.59 × 10−3
9.58 × 10−3
1.09 × 10−2
2.39 × 10−3
8.99 × 10−3
1.41 × 10−2

Time in seconds required to determine resolvability for each technique. Fifty resolving and ﬁfty
non-resolving sets, selected uniformly at random, were considered for each Hamming graph Hk,a.
Means and standard deviations consider ﬁve replicates per set.

Fig. 2. Data from Table 1 with lines of best ﬁt (on log-transformed data) for each method.

In the Table 1, we present the average run-time and standard deviation of the
algorithms on reference test sets for Hamming graphs of increasing sizes. Figure 2
displays the mean run-times as a function of the graph size, and best linear ﬁt for each
method. As seen in the table and ﬁgure, the brute force approach is faster on only
the smallest Hamming graphs (with fewer than 1000 nodes) whereas the ILP solution
is exceptionally fast even as the Hamming graph grows to more than 6000 nodes. For
small problems, the time taken to solve the ILP is likely dominated by the overhead
cost of using CVX to recast the ILP into standard form. The run-time results show a
promising improvement in computational time over the brute force approach which
will only become more pronounced on massive Hamming graphs. Additionally, the
brute force approach is infeasible on these larger graphs due to signiﬁcant memory
costs.

The ILP algorithm is exceptionally quick, beating all other methods for Hamming
graphs with more than 1000 nodes, but it cannot guarantee that a set is resolving.
The Gr¨obner basis algorithm by contrast is much slower on average but is a determin-
istic method of showing resolvability. ILP can be used to quickly determine possible
resolving sets which are then veriﬁed by the Gr¨obner basis algorithm. In this way,
the two methods are symbiotic and cover each other’s weaknesses. We illustrate this
in the next section.

6. Low-dimensional Protein Representations. Symbolic information per-
vades modern data science. With the advent and popularization of high-throughput
sequencing assays, this is particularly true in the ﬁeld of computational biology where

10110210310410-310-210-1100101102Runtime (seconds)Brute ForceGrobner BasisILP14

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

large volumes of biological sequence data have become critical for studying and un-
derstanding the behavior of cells. Analysis of these sequences, however, presents
signiﬁcant challenges. One major issue is that many powerful analysis techniques
deal with numeric vectors, not arbitrary symbols. As a result, biological sequence
data is typically mapped to a real space before such methods are applied. Two of
the most common mappings use K-mer count [22] and one-hot encodings (also called
binary vectors) [4]. K-mer count vectors represent symbolic sequences by their counts
of each possible K-mer.

Resolving sets can be used to deﬁne low-dimensional mappings as well. To ﬁx
ideas we focus on octapeptides, that is proteins composed of 8 amino acids. With a
total of 20 possible amino acids (which we represent as a,r,n,d,c,q,e,g,h,i,l,k,m,f,
p,s,t,w,y,v) and imposing the Hamming distance across these sequences, we have
the Hamming graph H8,20. This graph is massive. It has 25.6 billion vertices and
roughly 1.9 trillion edges rendering most methods of discovering small resolving sets,
including the ICH algorithm, useless. Utilizing a constructive algorithm, a resolving
set of size 82, which we call R, was discovered for H8,20 in [29]. However, it is not
known whether R contains a proper subset that is still resolving. Here, we address
this problem applying the results of sections 3 and 4.

Starting with lower and upper bounds L = 1 and U = 82 respectively, we im-
plement a binary search for β(H8,20). With s = L+U
as the current subset size to
2
check, up to 1000 subsets of R are selected at random. The ILP approach (section 4)
then provides an eﬃcient method for testing the feasibility problem outlined in The-
orem 2.2 for these subsets. If any subset passes this test, the upper bound is set to s.
Otherwise, s becomes the lower bound. This process is repeated until L = (U − 1).
Following this procedure, we found the following set of size 77:

r :=






aaaraaaa,
ccciaaaa,
dddgaaaa,
eeemmmmf,
gggpsaaa,
hhhwaaaa,
iiiyyyyy,
nnnccccq,
qpkaaaaa,
qyeaaaaa,
sisaaaaa,

arwaaaaa,
cnsaaaaa,
dhfaaaaa,
eeemmmmm,
gggsaaaa,
hpvaaaaa,
kkkaaaaa,
nnncccqa,
qqqkaaaa,
rrrdaaaa,
svtaaaaa,

ccchhhhh,
dddeeeee,
eagaaaaa,
ﬀfaaaaa,
hhhttttt,
iiivaaaa,
klqaaaaa,
nnnccqaa,
qqqlkaaa,
rrrndaaa,
ttcaaaaa,

ccchhhhi,
dddeeeeg,
eeefaaaa,
gggppppp,
hhhttttw,
iiiyvaaa,
lllaaaaa,
nnncqaaa,
qqqllkaa,
rrrnndaa,
vfraaaaa,

ccchhhia,
dddeeega,
eeemfaaa,
gggpppps,
hhhtttwa,
iiiyyvaa,
mkyaaaaa,
nnnqaaaa,
qqqlllka,
rrrnnnda,
wmpaaaaa,

ccchhiaa,
dddeegaa,
eeemmfaa,
gggpppsa,
hhhttwaa,
iiiyyyva,
mmmaaaaa,
nstaaaaa,
qqqllllk,
rrrnnnnd,
wwdaaaaa,

ccchiaaa,
dddegaaa,
eeemmmfa,
gggppsaa,
hhhtwaaa,
iiiyyyyv,
nnnccccc,
pppaaaaa,
qqqlllll,
rrrnnnnn,
yglaaaaa






.

Since the ILP formulation does not guarantee that this set is resolving, we veriﬁed the
result using a parallelized version of the Polynomial Roots Formulation (section 3) so
that the Gr¨obner bases of multiple auxiliary polynomials (Eq. (3.6)) may be deter-
mined simultaneously. Thus, we have found a set r ⊂ R of size 77 that resolves H8,20;
in particular, β(H8,20) ≤ 77, which improves the bound of [29], and all 25.6 billion
octapeptides may be uniquely represented with only 77 dimensions. In contrast, a 2-
mer count vector representation would require 400 dimensions and a one-hot encoding
160 dimensions.

Remark 6.1. We replicated the veriﬁcation of r as a resolving set of H8,20 using
our Polynomial Roots Formulation 10 times across 32 computer cores. Overall, a
maximum of approximately 380 megabytes of memory per core (SD ∼ 0.5 MB) and 6
hours and 20 minutes (SD ∼ 142 s) were required to demonstrate the resolvability of
r. Memory usage was determined using the Slurm workload manager sacct command
and maxRSS ﬁeld, while time was measured using Python’s time module.

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

15

Appendix A. Gr¨obner Basis.
In what follows, z = (z1, . . . , zd) is a d-dimensional variable and, unless otherwise

stated, polynomials are functions of z with real coeﬃcients.

A.1. Polynomial Ideals. A polynomial ideal I is a non-empty set of polyno-

mials with the property that if f, g ∈ I and c ∈ R then cf, f + g, f g ∈ I.

The polynomial ideal associated with a non-empty and ﬁnite set P = {p1, . . . , pm}

of polynomials in the variable z is the set deﬁned as

I(P ) :=

(cid:40) m
(cid:88)

i=1

qi · pi, with q1, . . . , qm polynomials in z

.

(cid:41)

Polynomial ideals are useful to characterize the complex numbers z such that

pi(z) = 0 for all i = 1, . . . , m. Indeed, z ∈ {P = 0} if and only if z ∈ {I(P ) = 0}.

A.2. Monomial Orderings. A monomial (in the variable z) is any product of
d , where a1, . . . , ad ≥ 0 are integers. This product is often written

1 · · · zad

the form za1
za with a = (a1, . . . , ad).

A monomial ordering is a total ordering of the monomials such that (i) if za < zb

then, for any monomial zc, za+c < zb+c; and (ii) 1 < za when za (cid:54)= 1.

i=1 ai < (cid:80)d

i=1 bi; or (ii) (cid:80)d

A common example of monomial ordering is the so-called lexicographic order.
Under this ordering, za < zb if there is an index i such that aj = bj for all 1 ≤ j < i
but ai < bi. Another example is the graded lexicographic order under which za < zb
if either (i) (cid:80)d
i=1 bi but za is smaller than zb
under the lexicographic order. Both of these orderings can be reversed giving the
reversed lexicographic order and the graded reverse lexicographic order, respectively.
In what remains of Appendix A, a ﬁxed monomial ordering is assumed.
In particular, each non-zero polynomial p may be uniquely written in the form p =
(cid:80)t
i=1 cimi, where t ≥ 1 is an integer, c1, . . . , ct are real coeﬃcients, and m1 > · · · > mt
are monomials in descending order. This allows us to deﬁne LM(p) := m1 (the leading
monomial of p), LC(p) := c1 (the leading coeﬃcient of p), and LT(p) := c1m1 (the
leading term of p).

i=1 ai = (cid:80)d

A.3. Polynomial Reductions. For a given non-empty set P of polynomials,
every polynomial f can be represented in the form f = (r + g), with r, g polynomials
such that g ∈ I(P ). Representing f in this form is called reducing f by P . The
term r is called the reduction of f by P ; which is expressed in writing as f P→ r.
Observe that reductions are typically not unique. This is because, if f = (r + g), with
g ∈ I(P ), then f = (r − h) + (g + h) for any polynomial h, however, (g + h) ∈ I(P )
when h ∈ I(P ).

Reductions can be computed using multivariate long-division as follows. First,
set r = 0 and g = f , and look for the smallest index i such that LM (pi) divides
LM (g).
If such an index exists, set g = g − LT (g) · pi/LT (pi) so that the LT (g)
and the LT (pi) cancel. Otherwise, set g = g − LT (g) and r = r + LT (g). Continue
this process until g = 0. This will produce a remainder r where no monomial of r is
divisible by any LM (pi).

In general, reductions of the form f P→ r with r /∈ I(P ) are also not unique.
This is because the polynomial r obtained by the long-division depends on the order
in which the polynomials in P are indexed. This lack of uniqueness is the primary
motivation for Gr¨obner bases.

16

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

A.4. Buchberger’s Criterion. In this section we give a characterization of

Gr¨obner bases due to Buchberger [7].

The least common multiple between za and zb is the monomial LCM(za, zb) := zc

with c = (max{a1, b1}, . . . , max{ad, bd}).

Given two polynomials p and q such that LCM(LM(p), LM(q)) = zc, their S-

polynomial is deﬁned as

Spoly(p, q) := zc ·

(cid:18) p

LT(p)

−

q
LT(q)

(cid:19)

.

A non-empty set G = {g1, . . . , gn} of polynomials is called a Gr¨obner basis for a

polynomial ideal I if (i) I(G) = I, and (ii) Spoly(gi, gj) G→ 0 for all gi, gj ∈ G.

Gr¨obner bases have the following property with regards to reductions: f G→ 0 if

and only if f ∈ I(G), otherwise f G→ r for some r /∈ I(G).

A Gr¨obner basis G is called reduced if for all gi ∈ G, (iii) LC(gi) = 1, and (iv)

for all gj ∈ G \ {gi}, no monomial of gj is divisible by LM (gi).

Unlike Gr¨obner bases, the reduced Gr¨obner basis of a polynomial ideal is unique.

A.5. Buchberger’s Algorithm. This is a method for generating a Gr¨obner
basis for a polynomial ideal I(P ) based on Buchberger’s criterion (see Algorithm A.1).
The key idea of the algorithm is to add to the initially empty Gr¨obner basis S-
polynomials of pairs in P which do not reduce to 0. This by construction satisﬁes
Buchberger’s criterion and hence produces a Gr¨obner basis.

Buchberger’s Algorithm, however, does not necessarily produce a reduced Gr¨obner
basis. Such a reduction can be computed using Algorithm A.2. This algorithm is the
simplest but also least eﬃcient for computing Gr¨obner bases.

The computation of Gr¨obner bases is an active ﬁeld of research with many diﬀer-
ent approaches. There are matrix reduction based algorithms, such as Faug´ere’s F4
algorithm [11], as well as signature based algorithms, like Faug´ere’s F5 algorithm and
its variants F5C and F5B [12, 10, 28].

Algorithm A.1 Buchberger’s algorithm for computing a Gr¨obner basis

Input: P , >
G = P
SP = {Spoly(gi, gj)|∀i < j, gi, gj ∈ G}
while SP not empty do

Select S ∈ SP
SP = SP \ {S}
r = S G→ r
if r (cid:54)= 0 then

Add Spoly(r, gi) to SP for each gi ∈ G
G = G ∪ {r}

end if
end while
return G

Appendix B. Linear Programming.
A linear program (LP) is any optimization problem that minimizes or maximizes
a linear objective function over a (possibly unbounded) closed convex polyhedron in
Euclidean space, i.e., a ﬁnite number of aﬃne equalities and inequalities (but not

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

17

Algorithm A.2 Algorithm for reducing a Gr¨obner basis

Input: G, >
For each gi ∈ G, gi = gi
for gi ∈ G do

LC(gi)

H = G \ {gi}
r = gi
if r (cid:54)= 0 then

H→ r

gi = r

else

G = G \ {gi}

end if
end for
return G

strict inequalities). In the combinatorial model of complexity, LP’s are known to be
solvable in polynomial time [19]. If the variables are constrained to be integers, the
LP is an integer linear program (ILP), or more generally, if some of the variables are
constrained to be integers and others are not, it is a mixed integer linear program
(MILP); one usually does not make the distinction between ILP and MILP, since all
ILP solvers also solve MILP.

Unlike LPs, ILP can encode NP-Hard decision problems, and thus in the worst
case they are intractable for large problems. However, because ILPs usually admit re-
laxation (e.g. dropping some constrains), one can ﬁnd upper and lower bounds on the
value of an ILP, and thus, when the bounds meet, produce a certiﬁcate of optimality.
Standard techniques such as branch-and-bound produce a large tree of diﬀerent relax-
ations of the ILP, each node a LP and thus solvable. Once all leaf nodes are visited,
one has a guarantee of the optimal solution, but there may be combinatorially many
leaf nodes. Modern ILP solvers use clever heuristics to determine the order in which
to traverse the tree, and with a good choice, entire branches of the tree can quickly
be pruned. Thus ILP solvers combine the speed of heuristics with provable optimal-
ity. Furthermore, because ILPs are common in industry, there are many high-quality
commercial solvers with excellent implementations such as gurobi [16].

Because in the worst-case ILPs can take exponentially long to solve, they are
sometimes overlooked in the mathematician’s toolbox. Nevertheless, because they are
general, they are the focus of much research and thus the software to solve them con-
tinues to improve rapidly. To quote Dimitris Bertsekas, “in the last twenty-ﬁve years
(1990–2014), algorithmic advances in integer optimization combined with hardware
improvements have resulted in an astonishing 200 billion factor speedup in solving
MIO [mixed-integer optimization] problems” [3]. Thus the main challenge is recog-
nizing when a problem with a combinatorial ﬂavor can be recast as an ILP.

Acknowledgments. This research was partially funded by NSF grant No.
1836914. The authors acknowledge the BioFrontiers Computing Core at the Uni-
versity of Colorado Boulder for providing High-Performance Computing resources
(funded by NIH grant No. 1S10OD012300), supported by BioFrontiers IT group.

[1] A. F. Beardon, Resolving the Hypercube, Discrete Applied Mathematics, 161 (2013), pp. 1882–

1887.

REFERENCES

18

L. LAIRD, R. C. TILLQUIST, S. BECKER, AND M. E. LLADSER

[2] J. L. Bentley, Multidimensional binary search trees used for associative searching, Commu-

nications of the ACM, 18 (1975), pp. 509–517.

[3] D. Bertsimas, Statistics and machine learning via a modern optimization lens. INFORMS
plenary lecture, http://meetings2.informs.org/sanfrancisco2014/plenary.html, 2014.
[4] Y.-D. Cai, K.-Y. Feng, Y.-X. Li, and K.-C. Chou, Support vector machine for predicting

α-turn types, Peptides, 24 (2003), pp. 629–630.

[5] G. Chartrand, L. Eroh, M. A. Johnson, and O. R. Oellermann, Resolvability in graphs
and the metric dimension of a graph, Discrete Applied Mathematics, 105 (2000), pp. 99–
113.

[6] S. A. Cook, The complexity of theorem-proving procedures, in Proceedings of the Third Annual

ACM Symposium on Theory of Computing, ACM, 1971, pp. 151–158.

[7] D. Cox, J. Little, and D. O’Shea, Using Algebraic Geometry, vol. 1 of Graduate Texts in

Mathematics, Springer-Verlag New York, 1998.

[8] D. A. Cox, J. Little, and D. O’Shea, Ideals, Varieties, and Algorithms: An Introduction
to Computational Algebraic Geometry and Commutative Algebra, Springer International
Publishing, 2015, https://doi.org/10.1007/978-3-319-16721-3 2, https://doi.org/10.1007/
978-3-319-16721-3 2.

[9] J. Currie and O. R. Oellermann, The metric dimension and metric independence of a
graph, Journal of Combinatorial Mathematics and Combinatorial Computing, 39 (2001),
pp. 157–168.

[10] C. Eder and J. E. Perry, F5C: A variant of Faug`ere’s F5 algorithm with re-
duced Gr¨obner bases, J. Symb. Comput., 45 (2010), pp. 1442–1458, https://doi.org/
https://doi.org/10.1016/j.jsc.2010.06.019, http://www.sciencedirect.com/science/article/
pii/S0747717110000994. MEGA’2009.

[11] J.-C. Faug`ere, A new eﬃcient algorithm for computing Gr¨obner bases F4, Jour-
nal of Pure and Applied Algebra, 139 (1999), pp. 61 – 88, https://doi.org/
https://doi.org/10.1016/S0022-4049(99)00005-5, http://www.sciencedirect.com/science/
article/pii/S0022404999000055.

[12] J. C. Faug`ere, A New Eﬃcient Algorithm for Computing Gr¨oBner Bases Without Reduc-
tion to Zero (F5),
in Proceedings of the 2002 International Symposium on Symbolic
and Algebraic Computation, ISSAC ’02, New York, NY, USA, 2002, ACM, pp. 75–83,
https://doi.org/10.1145/780506.780516, http://doi.acm.org/10.1145/780506.780516.

[13] M. R. Gary and D. S. Johnson, Computers and Intractability: A Guide to the Theory of

NP-completeness, WH Freeman and Company, New York, 1979.

[14] M. Grant and S. Boyd, CVX: Matlab Software for Disciplined Convex Programming, version

2.1, December 2018. Available at http://cvxr.com/cvx.

[15] L. Greengard and V. Rokhlin, A fast algorithm for particle simulations, Journal of compu-

tational physics, 73 (1987), pp. 325–348.

[16] Z. Gu, E. Rothberg, and R. Bixby, Gurobi optimizer reference manual, version 7.5.2, Gurobi

Optimization Inc., Houston, USA, (2018).

[17] F. Harary and R. A. Melter, On the metric dimension of a graph, Ars Combinatoria, 2

(1976), pp. 191–195.

[18] M. Hauptmann, R. Schmied, and C. Viehmann, Approximation complexity of metric dimen-

sion problem, Journal of Discrete Algorithms, 14 (2012), pp. 214–222.

[19] L. G. Khachiyan, A polynomial algorithm in linear programming, in Doklady Academii Nauk

SSSR, vol. 244, 1979, pp. 1093–1096.

[20] S. Khuller, B. Raghavachari, and A. Rosenfeld, Landmarks in graphs, Discrete Applied

Mathematics, 70 (1996), pp. 217–229.

[21] L. Laird, Metric Dimension of Hamming Graphs and Applications to Computational Biology,

bachelor’s thesis, University of Colorado, the United States, 2019.

[22] C. S. Leslie, E. Eskin, and W. S. Noble, The spectrum kernel: A string kernel for SVM

protein classiﬁcation, in Paciﬁc Symposium on Biocomputing, vol. 7, 2002, pp. 566–575.
[23] A. Meurer, C. P. Smith, M. Paprocki, O. ˇCert´ık, S. B. Kirpichev, M. Rocklin, A. Ku-
mar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P.
Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry,
A. R. Terrel, v. Rouˇcka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and A. Sco-
patz, SymPy: symbolic computing in Python, PeerJ Computer Science, 3 (2017), p. e103,
https://doi.org/10.7717/peerj-cs.103.

[24] V. K.-V. N. Mladenovi, J. Kratica and M. angalovi, Variable neighborhood search for
metric dimension and minimal doubly resolving set problems, European Journal of Opera-
tional Research, 220 (2012), pp. 328–337, https://EconPapers.repec.org/RePEc:eee:ejores:
v:220:y:2012:i:2:p:328-337.

RESOLVABILITY OF HAMMING GRAPHS - PREPRINT

19

[25] OEIS Foundation Inc., The On-Line Encyclopedia of Integer Sequences, (2019), http://oeis.

org/A303735.

[26] P. J. Olver and C. Shakiban, Linear Algebraic Systems, Springer International Publishing,
Cham, 2018, pp. 1–74, https://doi.org/10.1007/978-3-319-91041-3 1, https://doi.org/10.
1007/978-3-319-91041-3 1.

[27] P. J. Slater, Leaves of trees, Congressus Numerantium, 14 (1975), pp. 549–559.
[28] Y. Sun and D. Wang, The F5 algorithm in Buchberger’s style, CoRR, abs/1006.5299 (2010),

http://arxiv.org/abs/1006.5299, https://arxiv.org/abs/1006.5299.

[29] R. C. Tillquist and M. E. Lladser, Low-dimensional representation of genomic se-
quences, Journal of Mathematical Biology, (2019), pp. 1–29, https://doi.org/10.1007/
s00285-019-01348-1.

