Design Technology Co-Optimization for
Neuromorphic Computing

Ankita Paul, Shihao Song and Anup Das
Electrical and
Computer Engineering
Drexel University
Philadelphia, PA 19104
Email: {ankita.paul,shihao.song,anup.das}@drexel.edu

1
2
0
2

t
c
O
5
1

]
T
E
.
s
c
[

1
v
1
3
1
8
0
.
0
1
1
2
:
v
i
X
r
a

Abstract—We present a design-technology tradeoff analysis in
implementing machine-learning inference on the processing cores
of a Non-Volatile Memory (NVM)-based many-core neuromorphic
hardware. Through detailed circuit-level simulations for scaled
process technology nodes, we show the negative impact of design
scaling on read endurance of NVMs, which directly impacts their
inference lifetime. At a ﬁner granularity, the inference lifetime
of a core depends on 1) the resistance state of synaptic weights
programmed on the core (design) and 2) the voltage variation
inside the core that is introduced by the parasitic components
on current paths (technology). We show that such design and
technology characteristics can be incorporated in a design ﬂow to
signiﬁcantly improve the inference lifetime.

I. INTRODUCTION

Neuromorphic systems are integrated circuits designed to
mimic the computations in a mammalian brain [1]. They
enable energy-efﬁcient execution of Spiking Neural Networks
(SNN) [2] and therefore, these systems are suitable for im-
plementing machine learning inference tasks for embedded
Systems and Edge devices in Internet-of-Things (IoT). A
neuromorphic system consists of processing cores that imple-
ment neurons and synapses. Multiple such cores are intercon-
nected together using Segmented Bus [3] or Network-on-Chip
(NoC) [4] to design a many-core neuromorphic hardware [5].
Table I illustrates the neuron and synapse capacity of recent
neuromorphic hardware cores.

TABLE I
CAPACITY OF RECENT NEUROMORPHIC SYSTEMS.

ODIN µBrain DYNAPs BrainScaleS SpiNNaker Neurogrid Loihi TrueNorth

[6]

[7]

# Neurons/core

256

336

[8]

256

[9]

512

# Synapses/core 64K

38K

16K

128K

# Cores/chip

# Chips/board

# Neurons

# Synapses

1

1

256

256

1

1

336

336

1

4

1K

65K

1

352

4M

1B

[10]

36K

2.8M

144

56

2.5B

200B

[11]

[12]

[13]

65K

130K

1M

130M 256M

8M

128

16

128

768

1M

100M

16B

100B

4096

4096

4B

1T

along bitlines and wordlines (see Figure 2). Recently, Non-
Volatile Memory (NVM) technologies such as phase-change
memory (PCM) and oxide-based resistive switching random
access memory (OxRRAM) are used to implement the memory
cells in a neuromorphic core due to their low power con-
sumption, CMOS-compatible scaling, and multilevel analog
operations [18].1 In an NVM-based neuromorphic inference
hardware, the resistance state corresponding to the synaptic
weights of an SNN are programmed on the NVM cells.

Unfortunately, NVMs suffer from reliability issues such as
circuit aging, limited endurance, and read disturbance [26]–
[29]. Many of these issues have been addressed recently in the
context of neuromorphic computing [30]–[37]. In this work, we
focus on the read disturbance issues of OxRRAM technology,
where an OxRRAM cell’s resistance state may change after
performing a certain number of read operations [38]–[40].
Therefore, when an OxRRAM-based neuromorphic hardware
is used to implement an inference task, the trained synaptic
weights programmed on the hardware may change after per-
forming a few inference operations. To ensure expected results
from an inference task, the trained weights need to be repro-
grammed on to the hardware periodically. Reprogramming of
synaptic weights to a neuromorphic hardware involves ofﬂining
the hardware and transferring the weights from the host to
the hardware via a bandwidth-limited interconnect. This can
signiﬁcantly increase the overhead, and lower the availability
and reliability of neuromorphic computing.

In this work, we present a design-technology tradeoff analy-
sis in implementing machine learning inference on many-core
neuromorphic hardware. Through circuit-level simulations we
show the negative impact of design scaling on read endurance
of NVMs, which directly impacts their inference lifetime. We
show that the inference lifetime of a crossbar depends on 1)
the resistance state of synaptic weights programmed on the
NVMs and 2) the voltage variation inside the crossbar that is
introduced by the parasitic components on current paths. Such
design and technology characteristics can be incorporated in a
design ﬂow to signiﬁcantly improve the inference lifetime.

A neuromorphic core can be implemented using an analog
crossbar where bitlines and wordlines are organized in a grid
with memory cells connected at their crosspoints to store the
synaptic weights [14]–[17]. Neuron circuits are implemented

1NVMs are also used in classical von Neumann computing to mitigate the

performance and energy bottlenect of DRAM [19]–[25].

 
 
 
 
 
 
II. BACKGROUND
We brieﬂy introduce Spiking Neural Network (SNN) and

their implementation in hardware.

Spiking Neural Networks (SNNs) are the third generation
of neural networks designed using spiking neurons and bio-
inspired learning algorithms [2]. In an SNN, spikes injected
from pre-synaptic neurons raise the membrane voltage of a
post-synaptic neuron (middle sub-ﬁgure of Figure 1). When the
membrane voltage crosses a threshold (Vth), the post-synaptic
neuron emits spikes that propagate to other neurons (right sub-
ﬁgure of Figure 1). SNNs implement some variants of Integrate
and Fire (I&F) neurons with a spike duration ranging from 1
µs to several ms [41]–[43] (left sub-ﬁgure of Figure 1).

Fig. 1. A leaky integrate-and-ﬁre (LIF) neuron. The membrane potential over
time of the neuron (middle). The spike output of the neuron representing its
ﬁring time (right).

can

SNNs

implement many machine

learning
approaches [44]–[51].
In a supervised machine learning,
an SNN is pre-trained with representative data. Machine-
learning inference refers to feeding live data points to this
trained SNN to generate the corresponding output. The quality
of machine learning inference can be expressed in terms
[44], Peak
of accuracy [48], Mean Square Error
Signal-to-Noise Ratio (PSNR) [52], and Structural Similarity
Index Measure (SSIM) [53]. For SNNs, these quality metrics
are deﬁned in terms of the inter-spike interval (ISI) [54]–[57].
If {t1, t2, · · · , tK } denote a neuron’s ﬁring times in the time
interval [0, T ], the average ISI of this spike train is

(MSE)

I =

K
(cid:88)

i=2

(ti − ti−1)/(K − 1).

(1)

A neuromorphic hardware platform is implemented as a
tiled architecture, where tiles are interconnected via a time-
multiplexed shared interconnect. Each tile consists of a neuro-
morphic processing core, which can implement neurons and
synapses of a machine learning model. A common design
practice is to use an analog crossbar to implement a core.

In a crossbar, pre-synaptic neuron circuits are placed on
horizontal wires called wordlines, while post-synaptic neuron
circuits are placed on vertical wires called bitlines. Memory
cells are placed at the crosspoint of each wordline and bitline,
and they implement the synaptic weights of an SNN. The left
subﬁgure of Figure 2 illustrates an N × N crossbar. The right
subﬁgure illustrates the parasitic RC components on the current
path from a pre-synaptic neuron to a post-synaptic neuron
accessing the memory cell (i, j) located at the crosspoint of
ith wordline and jth bitline.

Overall, a neuromorphic hardware enables distributed and
pipelined processing of SNN operations. Additionally, each

Fig. 2. An N × N crossbar showing the parasitic components within.

crossbar in the hardware can implement a maximum of N pre-
synaptic neurons per post-synaptic neuron. Therefore, system-
software frameworks such as NEUTRAMS [58], NeuroX-
plorer [59], Corelet [60], PACMAN [61], and LAVA [62]
consist of 1) a compiler, which partitions an SNN model into
clusters such that the neurons and synapses of each cluster can
be mapped to a crossbar of the hardware, and 2) a run-time
manager, which maps the clusters of an SNN to the cores of a
many-core hardware. To this end, several mapping strategies
have been proposed, including optimizing for energy [63]–
[66], throughput [67]–[70], resource utilization [56], [58], [62],
[71], [72], circuit aging [30]–[34], inference lifetime [39], and
write endurance [35]–[37]. These mapping techniques all use
some variant of the SNN-partitioning approach proposed in
SpiNeMap [64].

Recently, dataﬂow models have been used to analyze per-
formance of SNNs implemented on neuromorphic hardware,
especially using Synchronous Dataﬂow Graphs (SDFGs) [73].2
There are two strategies proposed in literature – the SDF-
SNN [67] and its extended versions [68], [79], [80], which uses
dataﬂow graphs to model an SNN, performing partitioning and
mapping explorations with neurons and synapses directly, and
the DFSynthesizer [69] and its extended version [70], which
uses dataﬂow graphs to only model the clustered SNN, allowing
mapping and scheduling of the clusters (a collection of neurons
and synapses) to the PEs of a neuromorphic hardware.

Emerging non-volatile memory (NVM) technologies such as
phase-change memory (PCM), oxide-based memory (OxRAM),
spin-based magnetic memory (STT-MRAM), and Flash have
recently been used for synaptic storage in crossbars. NVMs are
non-volatile, have high CMOS compatibility, and can achieve
high integration density. Each NVM device can implement both
a single-bit and multi-bit synapse. Because of these properties,
an NVM-based neuromorphic hardware typically consumes
energy that is magnitudes lower than using SRAMs [81]–[83].
However, NVMs also introduce reliability issues [26]. Table II
summarizes them for different NVMs.

We discuss one speciﬁc issue – limited read endurance for

OxRRAM-based neuromorphic hardware.

III. INTRODUCTION TO OXIDE-BASED RESISTIVE RAM

The resistance switching random access memory (OxRRAM)
technology presents an attractive option for implementing the

2SDFGs are commonly used to model streaming applications that are
implemented on a multi-core system and their performance analysis [74]–[78].

Post-Synaptic NeuronsPre-Synaptic NeuronswordlinesbitlinesTABLE II
RELIABILITY ISSUES IN NVMS.

Reliability Issues

NVMs

High-voltage related circuit aging

PCM, Flash

High-current related circuit aging OxRAM, STT-MRAM
Limited read endurance

All

Limited write endurance

All

and is related to the gap gi,j, a0 is the atomic hoping distance,
and γ0 is a ﬁtting constant.

The transition from one of the LRS states is governed by
the lateral ﬁlament growth [38]. This is illustrated in the right
subﬁgure of Figure 4. The time for state transition in the (i, j)th
RRAM cell is given by

ti,j (LRS) = 10−14.7·Vi,j +6.7sec

(3)

synaptic cells of a crossbar due to its demonstrated poten-
tial for low-power multi-level operation and high integration
is composed of an insu-
density [18]. An OxRRAM cell
lating ﬁlm sandwiched between conducting electrodes form-
ing a metal-insulator-metal (MIM) structure (see Figure 3).
Recently, ﬁlament-based metal-oxide OxRRAM implemented
with transition-metal-oxides such as HfO2, ZrO2, and TiO2
has received considerable attention due to their low-power and
CMOS-compatible scaling.

Fig. 3. Operation of an OxRRAM cell with the HfO2 layer sandwiched
between the metals Ti (top electrode) and TiN (bottom electrode). The left
subﬁgure shows the formation of LRS states with the formation of conducting
ﬁlament (CF). This represents logic states 01, 10, and 11. The right subﬁgure
shows the depletion of CF on application of a negative voltage on the TE. This
represents the HRS state or logic 00.

Synaptic weights are represented as conductance of the
insulating layer within each OxRRAM cell. To program an
OxRRAM cell, elevated voltages are applied at the top and
bottom electrodes, which re-arranges the atomic structure of
the insulating layer. Figure 3 shows the High-Resistance State
(HRS) and the Low-Resistance State (LRS) of an OxRRAM
cell. An OxRRAM cell can also be programmed into interme-
diate low-resistance states, allowing its multilevel operations.
In OxRRAM technology, the transition from HRS state is
governed by a sudden decrease of the vertical ﬁlament gap on
application of stress voltage during spike propagation [38]. This
is illustrated in the left subﬁgure of Figure 4 where the vertical
ﬁlament gap is shown to reduce by an amount h. This may
result in a conducting ﬁlament between the two metal layers
causing the resistive state to change from HRS to LRS. The
rate of change of the ﬁlament gap of the OxRRAM cell at the
(i, j)th location in the crossbar is

dgi,j
dt

= −ϑ0·e− Ea

kT sinh

(cid:18) γi,j · a0
L

·

qVi,j
kT

(cid:19)

, where γi,j = γ0−β·

3

gi,j
g0

(2)
In the above equation, t deﬁnes the state transition time, g0 is
the initial ﬁlament gap of the OxRRAM cell, Vi,j is the voltage
applied to the cell, γi,j is the local ﬁeld enhancement factor

Fig. 4. Read disturbances due to structural alteration in an OxRRAM cell. The
left subﬁgure shows a reduction of the conductive ﬁlament gap on application
of a stress voltage. Such changes leads to a reduction in the resistance, i.e., a
read disturbance of HRS state in the cell. The right subﬁgure shows the lateral
growth of the conductive ﬁlament due to application of a stress voltage. This
illustrates read disturbance of LRS state in the cell.

If the state transition time of an OxRRAM cell is 1000 ms,
then a single quasi-static read operating using 1000 ms read
pulse or 1000 read accesses using with 1-ms spike pulses can
lead to an abrupt change in the state of the cell. Therefore, the
endurance of the cell is 1000. If the cell propagates n spikes
during inference of each image, then the inference lifetime is
deﬁned as 1000/n. Formally,

inference lifetime =

read endurance
spikes per image

(4)

IV. DESIGN-TECHNOLOGY TRADEOFF ANALYSIS

The computer memory industry has thus far been primarily
driven by the cost-per-bit metric, which provides the maximum
capacity for a given manufacturing cost. As shown in recent
works [19]–[24], [84], manufacturing cost can be estimated
from the area overhead. To estimate the cost-per-bit of a
neuromorphic core, we investigate the internal architecture of
a crossbar and ﬁnd that a neuron circuit can be designed using
20 transistors and a capacitor [85], while an NVM cell is a 1T-
1R arrangement with a transistor used as an access device for
the cell. Within an N × N crossbar, there are N pre-synaptic
neurons, N post-synaptic neurons, and N 2 synaptic cells. The
total area of all the neurons and synapses of a crossbar is

neuron area = 2N (20T + 1C)
synapse area = N 2(1T + 1R)

(5)

where T stands for transistor, C for capacitor, and R for NVM
cell. The total synaptic cell capacity is N 2, with each NVM
cell implementing 2-bit per synapse. The total number of bits
(i.e., synaptic capacity) in the crossbar is

total bits = 2N 2

(6)

BETEBETEHhTherefore, the cost-per-bit of an NxN crossbar is

cost-per-bit =

2N (20T + 1C) + N 2(1T + 1R)
2N 2

≈

F 2(27 + 2N )
N

,

(7)

where the cost-per-bit is represented in terms of the crossbar
dimension N and the feature size F . Equation 7 provides a back-
of-the-envelope calculation of cost-per-bit. Figure 5 plots the
normalized cost-per-bit for four different process technology
nodes, with the crossbar dimension ranging from 16 to 256.
the cost-per-bit reduces with increase in
We observe that
the dimension of a crossbar, i.e., larger-sized crossbars can
accommodate more bits for a given cost.

Fig. 5. Cost-per-bit analysis of a crossbar.

We now analyze the internal architecture of a crossbar.
Figure 6a shows the current through the memory cells in a
128x128 crossbar. This current variation is due to the difference
in the length of current paths from pre to post-synaptic neurons
in the crossbar, where the length of a current path is measured
in terms of the number of parasitic components on the path.
These current values are obtained for a 65nm technology node
and at 300K temperature corner. As can be clearly seen from
the ﬁgure, current through memory cells on the top-right corner
of the crossbar is lower than those at the bottom-left corner.

Fig. 7. Difference between current on the shortest and the longest path for
different crossbar sizes.

path, which reduces the current on this path compared to the
shortest path for the same amount of spike voltage applied
on both these paths. The current difference increases with
crossbar size because of the increase in the number of parasitic
resistances on the longest current path, which results in larger
injected into its post-
voltage drops,
synaptic neuron. However, larger current variation causes larger
endurance variation as illustrated in Figure 6b. Therefore, larger
crossbar sizes leads to larger endurance variation.

lowering the current

System designers often make a tradeoff between cost-per-bit
and endurance variation. Typically, crossbar sizes of 128 × 128
and 256 × 256 gives the best tradeoff.

V. DESIGN FLOW INCORPORATING
DESIGN-TECHNOLOGY TRADEOFF

An optimized design ﬂow is one, which takes into account
the design and technology characteristics to place neurons and
synapses to a crossbar such that those synapses that propagate
more spikes are mapped to NVMs with higher read endurance.
This is to increase the inference lifetime.

In our prior work [39], we show that such a design ﬂow can
signiﬁcantly improve the inference lifetime. Figure 8 reports
the inference lifetime for 10 applications for the proposed
design ﬂow normalized to SpiNeMap. We observe that through
intelligent synapse mapping, the inference lifetime obtained
using the proposed approach is on average 3.4x higher.

(a) Current map in a 128x128 cross-
bar.

(b) RRAM endurance in a 128x128
crossbar.

Fig. 6. Current map and RRAM endurance in a 128x128 crossbar.

Figure 6b shows the endurance variation of a 128x128
crossbar at 45 nm node and at 30◦C with each RRAM cell
programmed to HRS state. The endurance variation is a direct
result of the current variation in the crossbar.

Figure 7 shows the difference between currents on the
shortest and longest paths for 32x32, 64x64, 128x128, and
256x256 crossbars at 65nm process node. The input spike
voltage of the pre-synaptic neurons is set to generate 50µA on
the longest path. This current value corresponds to the current
needed to read the resistance state of an OxRRAM cell.

We observe that the current injected into the post-synaptic
neuron on the longest path is lower than the current on the
shortest path by 13.3% for 32x32, 25.1% for 64x64, 39.2%
for 128x128, and 55.8% for 256x256 crossbar. This current
difference is because of the higher voltage drop on the longest

Fig. 8.

Inference lifetime normalized to SpiNeMap.

VI. CONCLUSION

A design-technology tradeoff analysis is performed to inves-
tigate inference lifetime of neuromorphic hardware that adopts
NVM as synaptic storage. An essential observation of the
analysis is that the read endurance of an NVM cell depends
on its programmed synaptic weight (design) and the voltage
exposed to the cell (technology). Our analysis also reveals that
voltages exposed to NVM cells inside a crossbar vary due
to the parasitic components on current paths, which leads to
asymmetric read endurance across NVM cells in a crossbar.

1664128256CrossbarSize0.00.51.0NormalizedCost-per-BitProcessTechnologyNode=45nm32nm22nm16nm0326496128Post-synapticneurons1289664320Pre-synapticneurons50556065707580Current(µA)0326496128Post-synapticneurons1289664320Pre-synapticneurons5000100001500020000250003000035000400004500050000RRAMendurance(spikes)32x3264x64128x128256x256050100150Current(µA)576682113longestpathshortestpathLeNetAlexNetVGGHeartClassMLPDigitEdgeDetImgSmoothHeartEstmVisualPursuitRNNDigitAVERAGE123456InferenceLifetimeNormalizedtoSpiNeMap11391102119312381672652650148515011144SpiNeMapProposedFrom detailed circuit-level simulations, we show design scaling
on read endurance of NVMs negatively impacts the inference
lifetime of neuromorphic hardware. In addition, the design ﬂow
that considers asymmetry in read endurance can signiﬁcantly
improve the inference lifetime of neuromorphic hardware.

ACKNOWLEDGMENT

This work is supported by the United States National Science
Foundation Faculty Early Career Development Award CCF-
1942697 (CAREER: Facilitating Dependable Neuromorphic
Computing: Vision, Architecture, and Impact on Programma-
bility) and the United States Department of Energy CAREER
Award DE-SC0022014 (Architecting the Hardware-Software
Interface for Neuromorphic Computers).

REFERENCES

[1] C. Mead, “Neuromorphic electronic systems,” Proc. of the IEEE, 1990.
[2] W. Maass, “Networks of spiking neurons: The third generation of neural

network models,” Neural Networks, 1997.

[3] A. Balaji et al., “Exploration of segmented bus as scalable global

interconnect for neuromorphic computing,” in GLSVLSI, 2019.

[4] X. Liu et al., “Neu-NoC: A high-efﬁcient interconnection network for

accelerated neuromorphic systems,” in ASP-DAC, 2018.

[5] F. Catthoor et al., “Very large-scale neuromorphic systems for biological
signal processing,” in CMOS Circuits for Biological Sensing and Pro-
cessing, 2018.

[6] C. Frenkel et al., “A 0.086-mm2 12.7-pj/sop 64k-synapse 256-neuron
online-learning digital spiking neuromorphic processor in 28-nm CMOS,”
TBCAS, 2018.

[7] J. Stuijt et al., “µBrain: An event-driven and fully synthesizable archi-
tecture for spiking neural networks,” Frontiers in Neuroscience, 2021.
[8] S. Moradi et al., “A scalable multicore architecture with heterogeneous
memory structures for dynamic neuromorphic asynchronous processors
(DYNAPs),” TBCAS, 2017.

[9] J. Schemmel et al., “Live demonstration: A scaled-down version of the

brainscales wafer-scale neuromorphic system,” in ISCAS, 2012.
[10] S. Furber et al., “The SpiNNaker project,” Proc. of the IEEE, 2014.
[11] B. Benjamin et al., “Neurogrid: A mixed-analog-digital multichip system
for large-scale neural simulations,” Proceedings of the IEEE, 2014.
[12] M. Davies et al., “Loihi: A neuromorphic manycore processor with on-

chip learning,” IEEE Micro, 2018.

[13] M. V. Debole et al., “TrueNorth: Accelerating from zero to 64 million

neurons in 10 years,” Computer, 2019.

[14] C. Liu et al., “A spiking neuromorphic design with resistive crossbar,” in

DAC, 2015.

[15] M. Hu et al., “Memristor crossbar-based neuromorphic computing sys-

tem: A case study,” TNNLS, 2014.

[16] M. Hu et al., “Dot-product engine for neuromorphic computing: Pro-
gramming 1T1M crossbar to accelerate matrix-vector multiplication,” in
DAC, 2016.

[17] A. Ankit et al., “TraNNsformer: Neural network transformation for
memristive crossbar based neuromorphic system design,” in ICCAD,
2017.

[18] A. Mallik et al., “Design-technology co-optimization for OxRRAM-based

synaptic processing unit,” in VLSIT, 2017.

[19] S. Song et al., “Enabling and exploiting partition-level parallelism (PALP)

in phase change memories,” TECS, 2019.

[20] O. Mutlu, “Memory scaling: A systems architecture perspective,” in IMW,

2013.

[26] R. M. Shelby et al., “Non-volatile memory as hardware synapse in
neuromorphic computing: A ﬁrst look at reliability issues,” in IRPS, 2015.
[27] L. Larcher et al., “High-κ related reliability issues in advanced non-

volatile memories,” Microelectronics Reliability, 2010.

[28] C.-Y. Lu et al., “Non-volatile memory technology-today and tomorrow,”

in IPFA, 2006.

[29] P. No´e et al., “Phase-change materials for non-volatile memory devices:
from technological challenges to materials science issues,” Semiconductor
Science and Technology, 2017.

[30] S. Song et al., “A case for lifetime reliability-aware neuromorphic

computing,” in MWSCAS, 2020.

[31] A. Balaji et al., “A framework to explore workload-speciﬁc performance
and lifetime trade-offs in neuromorphic computing,” CAL, 2019.
[32] S. Song et al., “Improving dependability of neuromorphic computing with

non-volatile memory,” in EDCC, 2020.

[33] S. Kundu et al., “Special session: Reliability analysis for ML/AI hard-

ware,” in VTS, 2021.

[34] S. Song et al., “Dynamic reliability management in neuromorphic com-

puting,” JETC, 2021.

[35] T. Titirsha et al., “Reliability-performance trade-offs in neuromorphic

computing,” in IGSC Workshops, 2020.

[36] T. Titirsha et al., “Thermal-aware compilation of spiking neural networks

to neuromorphic hardware,” in LCPC, 2020.

[37] T. Titirsha et al., “Endurance-aware mapping of spiking neural networks

to neuromorphic hardware,” TPDS, 2021.

[38] W. Shim et al., “Impact of read disturb on multilevel RRAM based

inference engine: Experiments and model prediction,” in IRPS, 2020.

[39] S. Song et al., “Improving inference lifetime of neuromorphic systems

via intelligent synapse mapping,” in ASAP, 2021.

[40] K.-T. Chen et al., “Non-volatile ferroelectric FETs using 5-nm Hf 0.5
Zr 0.5 O 2 with high data retention and read endurance for 1T memory
applications,” EDL, 2019.

[41] S. Fusi et al., “Collective behavior of networks with linear (VLSI)

integrate-and-ﬁre neurons,” Neural Computation, 1999.

[42] S. Zhang et al., “A pulse-width modulation neuron with continuous

activation for processing-in-memory engines,” in DATE.

IEEE, 2020.

[43] A. J. Leigh et al., “An efﬁcient spiking neuron hardware system based
on the hardware-oriented modiﬁed Izhikevich neuron (HOMIN) model,”
TCAS II: Express Briefs, 2020.

[44] A. Das et al., “Unsupervised heart-rate estimation in wearables with
Liquid states and a probabilistic readout,” Neural Networks, 2018.
[45] E. J. Moyer et al., “Machine learning applications to DNA subsequence

and restriction site analysis,” in SPMB, 2020.

[46] J. P. Dominguez-Morales et al., “Deep spiking neural network model
for time-variant signals classiﬁcation: a real-time speech recognition
approach,” in IJCNN, 2018.

[47] A. Das et al., “Heartbeat classiﬁcation in wearables using multi-layer
perceptron and time-frequency joint distribution of ECG,” in CHASE,
2018.

[48] A. Balaji et al., “Power-accuracy trade-offs for heartbeat classiﬁcation on

neural networks hardware,” JOLPE, 2018.

[49] Q. Yu et al., “A spiking neural network system for robust sequence

recognition,” TNNLS, 2015.

[50] S. Schliebs et al., “Evolving spiking neural network—a survey,” Evolving

Systems, 2013.

[51] M. Dong et al., “Unsupervised speech recognition through spike-timing-
dependent plasticity in a convolutional spiking neural network,” PloS one,
2018.

[52] T. Chou et al., “CARLsim 4: An open source library for large scale, bio-
logically detailed spiking neural network simulation using heterogeneous
clusters,” in IJCNN, 2018.

[53] A. Hore et al., “Image quality metrics: PSNR vs. SSIM,” in ICPR, 2010.
[54] P. A. Cariani, “Temporal coding of sensory information in the brain,”

Acoustical Science and Technology, 2001.

[21] S. Song et al., “Improving phase change memory performance with data

[55] H. Fang et al., “Encoding, model, and architecture: systematic optimiza-

content aware access,” in ISMM, 2020.

tion for spiking neural network in FPGAs,” in ICCAD, 2020.

[22] O. Mutlu et al., “Research problems and opportunities in memory

[56] A. Balaji et al., “PyCARL: A PyNN interface for hardware-software co-

systems,” SUFI, 2015.

simulation of spiking neural network,” in IJCNN, 2020.

[23] S. Song et al., “Exploiting inter- and intra-memory asymmetries for data

[57] M. Kiselev, “Rate coding vs. temporal coding-is optimum between?” in

mapping in hybrid tiered-memories,” in ISMM, 2020.

IJCNN, 2016.

[24] S. Song et al., “Aging-aware request scheduling for non-volatile main

[58] Y. Ji et al., “NEUTRAMS: Neural network transformation and co-design

memory,” in ASP-DAC, 2021.

[25] S. Song et al., “Design methodologies for reliable and energy-efﬁcient

PCM systems,” in IGSC Workshops, 2020.

under neuromorphic hardware constraints,” in MICRO, 2016.

[59] A. Balaji et al., “NeuroXplorer 1.0: An extensible framework for archi-
tectural exploration with spiking neural networks,” in ICONS, 2021.

[60] A. Amir et al., “Cognitive computing programming paradigm: a corelet
language for composing networks of neurosynaptic cores,” in IJCNN,
2013.

[61] F. Galluppi et al., “A hierachical conﬁguration system for a massively

[74] S. Sriram et al., Embedded Multiprocessors; Scheduling and Synchro-

nization, 2000.

[75] L. Jiashu et al., “A design ﬂow for partially reconﬁgurable heterogeneous

multi-processor platforms,” in RSP, 2012.

parallel neural hardware platform,” in CF, 2012.

[76] A. Das et al., Reliable and Energy Efﬁcient Streaming Multiprocessor

[62] C.-K. Lin et al., “Mapping spiking neural networks onto a manycore

Systems. Springer, 2018.

neuromorphic architecture,” in PLDI, 2018.

[63] A. Das et al., “Mapping of local and global synapses on spiking

neuromorphic hardware,” in DATE, 2018.

[77] A. Das et al., “Reliability and energy-aware mapping and scheduling of
multimedia applications on multiprocessor systems,” TPDS, 2015.
[78] A. H. Ghamarian et al., “Throughput analysis of synchronous data ﬂow

[64] A. Balaji et al., “Mapping spiking neural networks to neuromorphic

graphs,” in ACSD, 2006.

hardware,” TVLSI, 2020.

[79] S. Song et al., “A design ﬂow for mapping spiking neural networks to

[65] T. Titirsha et al., “On the role of system software in energy management

many-core neuromorphic hardware,” in ICCAD, 2021.

of neuromorphic computing,” in CF, 2021.

[80] S. Curzel et al., “Automated generation of integrated digital and spiking

[66] A. Balaji et al., “Run-time mapping of spiking neural networks to

neuromorphic machine learning accelerators,” in ICCAD, 2021.

neuromorphic hardware,” JSPS, 2020.

[81] G. W. Burr et al., “Neuromorphic computing using non-volatile memory,”

[67] A. Das et al., “Dataﬂow-based mapping of spiking neural networks on

Advances in Physics: X, 2017.

neuromorphic hardware,” in GLSVLSI, 2018.

[68] A. Balaji et al., “A framework for the analysis of throughput-constraints

of SNNs on neuromorphic hardware,” in ISVLSI, 2019.

[69] S. Song et al., “Compiling spiking neural networks to neuromorphic

hardware,” in LCTES, 2020.

[70] S. Song et al., “DFSynthesizer: Dataﬂow-based synthesis of spiking

neural networks to neuromorphic hardware,” TECS, 2021.

[82] S. Kim et al., “NVM neuromorphic core with 64k-cell (256-by-256)
phase change memory synaptic array with on-chip neuron circuits for
continuous in-situ learning,” in IEDM, 2015.

[83] S. B. Eryilmaz et al., “Device and system level design considerations
for analog-non-volatile-memory based neuromorphic architectures,” in
IEDM, 2015.

[84] D. Lee et al., “Tiered-latency DRAM: A low latency and low cost DRAM

[71] A. Balaji et al., “Enabling resource-aware mapping of spiking neural

architecture,” in HPCA, 2013.

networks via spatial decomposition,” ESL, 2020.

[85] G. Indiveri, “A low-power adaptive integrate-and-ﬁre neuron circuit,” in

[72] A. Balaji et al., “Compiling spiking neural networks to mitigate neuro-

ISCAS, 2003.

morphic hardware constraints”,” in IGSC Workshops, 2020.

[73] E. Lee et al., “Synchronous data ﬂow,” Proceedings of the IEEE, 1987.

