0
2
0
2

b
e
F
9
2

]

G
L
.
s
c
[

2
v
9
4
3
6
0
.
9
0
9
1
:
v
i
X
r
a

Slice-based Learning: A Programming Model for
Residual Learning in Critical Data Slices

Vincent S. Chen, Sen Wu, Zhenzhen Weng, Alexander Ratner, Christopher Ré

vincentsc@cs.stanford.edu, senwu@stanford.edu, zzweng@stanford.edu,

ajratner@stanford.edu, chrismre@cs.stanford.edu

Stanford University

Abstract

In real-world machine learning applications, data subsets correspond to especially
critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous
driving task, and “question” sentences might be important to a dialogue agent’s
language understanding for product purposes. While machine learning models
can achieve high quality performance on coarse-grained metrics like F1-score and
overall accuracy, they may underperform on critical subsets—we deﬁne these as
slices, the key abstraction in our approach. To address slice-level performance,
practitioners often train separate “expert” models on slice subsets or use multi-task
hard parameter sharing. We propose Slice-based Learning, a new programming
model in which the slicing function (SF), a programming interface, speciﬁes critical
data subsets for which the model should commit additional capacity. Any model
can leverage SFs to learn slice expert representations, which are combined with an
attention mechanism to make slice-aware predictions. We show that our approach
maintains a parameter-efﬁcient representation while improving over baselines by up
to 19.0 F1 on slices and 4.6 F1 overall on datasets spanning language understanding
(e.g. SuperGLUE), computer vision, and production-scale industrial systems.

1

Introduction

In real-world applications, some model outcomes are more important than others: for example, a data
subset might correspond to safety-critical but rare scenarios in an autonomous driving setting (e.g.
detecting cyclists or trolley cars [19]) or critical but lower-frequency healthcare demographics (e.g.
bone X-rays associated with degenerative joint disease [27]). Traditional machine learning systems
optimize for overall quality, which may be too coarse-grained; models that achieve high overall
performance might produce unacceptable failure rates on slices of the data. In many production
settings, the key challenge is to maintain overall model quality while improving slice-speciﬁc metrics.

To formalize this challenge, we introduce the notion of slices: application-critical data subsets,
speciﬁed programmatically by machine learning practitioners, for which we would like to improve
model performance. This leads to three technical challenges:

• Coping with Noise: Deﬁning slices precisely can be challenging. While engineers often have a
clear intuition of a slice, typically as a result of an error analysis, translating that intuition into a
machine-understandable description can be a challenging problem, e.g., “the slice of data that
contains a yellow light at dusk.” As a result, any method must be able to cope with imperfect,
overlapping deﬁnitions of data slices, as speciﬁed by noisy or weak supervision.

• Stable Improvement of the Model: Given a description of a set of slices, we want to improve
the prediction quality on each of the slices without hurting overall model performance. Often,

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
Figure 1: Slice-based Learning via synthetically generated data: (a) The data distribution contains
critical slices, s1, s2, that represent a small proportion of the dataset. (b) A vanilla neural network
correctly learns the general, linear decision boundary but fails to learn the perturbed slice boundary.
(c) A user writes slicing functions (SFs), λ1, λ2, to heuristically target critical subsets. (d) The model
commits additional capacity to learn slice expert representations. Upon reweighting slice expert
representations, the slice-aware model learns to classify the ﬁne-grained slices with higher F1 score.

these goals are in tension: in many baseline approaches, steps to improve the slice-speciﬁc model
performance would degrade the overall model performance, and vice-versa.

• Scalability: There may be many slices. Indeed, in industrial deployments of slice-based ap-
proaches, hundreds of slices are commonly introduced by engineers [32]—any approach to
Slice-based Learning must be judicious with adding parameters as the number of slices grow.

To improve ﬁne-grained, i.e. slice-speciﬁc, performance, an intuitive solution is to create a separate
model for each slice. To produce a single prediction at test time, one often trains a mixture of experts
model (MoE) [18]. However, with the growing size of ML models, MoE is often untenable due to
runtime performance, as it could require training and deploying hundreds of large models—one for
each slice. Another strategy draws from multi-task learning (MTL), in which slice-speciﬁc task heads
are learned with hard-parameter sharing [7]. This approach is computationally efﬁcient but may not
effectively share training data across slices, leading to suboptimal performance. Moreover, in MTL,
tasks are distinct, while in Slice-based Learning, a single base task is reﬁned by related slice tasks.

We propose a novel programming model, called Slice-based Learning, in which practitioners provide
slicing functions (SFs), a programming abstraction for heuristically targeting data subsets of interest.
SFs coarsely map input data to slice indicators, which specify data subsets for which we should
allocate additional model capacity. To improve slice-level performance, we introduce slice-residual-
attention modules (SRAMs) that explicitly model residuals between slice-level and the overall task
predictions. SRAMs are agnostic to the architecture of any neural network model that they are added
to—which we refer to as the backbone model—and we demonstrate our approach on state-of-the-art
text and image models. Using shared backbone parameters, our model initializes slice “expert”
representations, which are associated with learning slice-membership indicators and class predictors
for examples in a particular slice. Then, slice indicators and prediction conﬁdences are used in an
attention-mechanism to reweight and combine each slice expert representation based on learned
residuals from the base representation. This produces a slice-aware featurization of the data, which
can be used to make a ﬁnal prediction.

Our work ﬁts into an emerging class of programming models that sit on top of deep learning
systems [19, 30]. We are the ﬁrst to introduce and formalize Slice-based Learning, a key programming
abstraction for improving ML models in real-world applications subject to slice-speciﬁc performance
objectives. Using an independent error analysis for the recent GLUE natural language understanding
benchmark tasks [39], by simply encoding the identiﬁed error categories as slices in our framework,
we show that we can improve the quality of state-of-the-art models by up to 4.6 F1 points, and we
observe slice-speciﬁc improvements of up to 19.0 points. We also evaluate our system on autonomous
vehicle data and show improvements up to 15.6 F1 points on context-dependent slices (e.g., presence
of bus, trafﬁc light) and 2.3 F1 points overall. Anecdotally, when deployed in production systems [32],
Slice-based Learning provides a practical programming model with improvements of up to 40 F1
points in critical test-time slices. On the SuperGlue benchmark [38], this procedure accounts for a 2.7

2

(c) User heuristicallytargets slices SFs (dotted)(a) Synthetic data with critical slices (dashed)(b) Vanilla model errors(d) Slice-aware model errors52.94 F168.75 F191.18 F181.25 F1improvement in aggregate benchmark score using the same architecture as previous state-of-the-art
submissions. In addition to the proposal of SRAMs, we perform an in-depth analysis to explain
the mechanisms by which SRAMs improve quality. We validate the efﬁcacy of quality and noise
estimation in SRAMs and compare to weak supervision frameworks [30] that estimate the quality of
supervision sources to improve overall model accuracy. We show that by using SRAMs, we are able
to produce accurate quality estimates, which leads to higher downstream performance on such tasks
by an average of 1.1 overall F1 points.

2 Related Work

Our work draws inspiration from three main areas: mixture of experts, multi-task learning, and weak
supervision. Jacobs et. al [18] proposed a technique called mixture of experts that divides the data
space into different homogeneous regions, learns the regions of data separately, and then combines
results with a single gating network [37]. This work is a generalization of popular ensemble methods,
which have been shown to improve predictive power by reducing overﬁtting, avoiding local optima,
and combining representations to achieve optimal hypotheses [36]. We were motivated in part by
reducing the runtime cost and parameter count for such models.

Multi-task learning (MTL) models provide the ﬂexibility of modular learning—speciﬁc task heads,
layers, and representations can be changed in an application-speciﬁc, ad hoc manner. Furthermore,
MTL models beneﬁt from the computational efﬁciency and regularization afforded by hard parameter
sharing [7]. There are often also performance gains seen from adding auxiliary tasks to improve
representation learning objectives [8, 33]. While our approach draws high-level inspiration from
MTL, we highlight key differences: whereas tasks are disjoint in MTL, slice tasks are formulated
as micro-tasks that are direct extensions of a base task—they are designed speciﬁcally to learn
deviations from the base-task representation. In particular, sharing information, as seen in cross-stitch
networks [26], requires Ω(n2) weights across n local tasks; our formulation only requires attention
over O(n) weights, as slice tasks operate on the same base task. For example, practitioners might
specify yellow lights and night-time images as important slices; the model learns a series of micro-
tasks—based solely on the data speciﬁcation—to inform how its approach for the base task, object
detection, should change in these settings. As a result, slice tasks are not ﬁxed ahead of time by an
MTL speciﬁcation; instead, these micro-task boundaries are learned dynamically from corresponding
data subsets. This style of information sharing sits adjacent to cross-task knowledge literature in
recent MTL models [35, 42], and we were inspired by these methods.

Weak supervision has been viewed as a new way to incorporate data of varying accuracy sources,
including domain experts, crowd sourcing, data augmentations, and external knowledge bases
[2, 5, 6, 11, 14, 21, 25, 29, 31]. We take inspiration from labeling functions [31] in weak supervision
as a programming paradigm, which has seen success in industrial deployments [2]. In existing weak
supervision literature, a key challenge is to assess the accuracy of a training data point, which is a
function of supervision sources. In this work, we model this accuracy using learned representations
of user-deﬁned slices—this leads to higher overall quality.

Weak supervision and multitask learning can be viewed as orthogonal to slicing: we have observed
them used alongside Slice-based Learning in academic projects and industrial deployments [32].

3 Slice-based Learning

We propose Slice-based Learning as a programming model for training machine learning models
where users specify important data subsets to improve model performance. We describe the core
technical challenges that lead to our notion of slice-residual-attention modules (SRAMs).

3.1 Problem statement

To formalize the key challenges of slice-based learning, we introduce some basic terminology. In
our base task, we use a supervised input, (x ∈ X , y ∈ Y), where the goal is to learn according to
a standard loss function. In addition, the user provides a set of k functions called slicing functions
(SFs), {λ1, . . . , λk}, in which λi : X → {0, 1}. These SFs are not assumed to be perfectly accurate;
for example, SFs may be based on noisy or weak supervision sources in functional form [31]. SFs can

3

Figure 2: Model Architecture: A developer writes SFs (λi=1,...,k) over input data and speciﬁes any
(a) backbone architecture (e.g. ResNet [16], BERT [13]) as a feature extractor. These features are
shared parameters for k slice-residual attention modules (SRAMs); each learns a (b) slice indicator
head which outputs a prediction, qi, indicating which slice the example belongs to, as supervised
by λi. SRAMs also learn a (c) slice expert representation, trained only on examples belonging to
the slice using a (d) shared slice prediction head, which makes predictions, pi, on the original task
schema and is supervised by the masked ground truth labels for the corresponding slice. An attention
mechanism, a, reweights these representations, ri, into a combined, (e) slice-aware representation.
A ﬁnal (f) prediction head makes model predictions based on this slice-aware representation.

come from domain-speciﬁc heuristics, distant supervision sources, or other off-the-shelf models, as
seen in Figure 2. Ultimately, the model’s goal is to improve (or avoid damaging) the overall accuracy
on the base task while improving the model on the speciﬁed slices.

Formally, each of k slices, denoted si=1,...,k, is an unobserved, indicator random variable, and
each user-speciﬁed SF, λi=1,...,k is a corresponding, noisy speciﬁcation. Given an input tuple
(X , Y, {λi}i=1,...,k) consisting of a dataset (X , Y), and k different user-deﬁned SFs λi, our goal is
to learn a model f ˆw(·)—i.e. estimate model parameters ˆw—that predicts P (Y |{si}i=1,...,k, X ) with
high slice-speciﬁc accuracies without substantially degrading overall accuracy.

Example 1 A developer notices that their self-driving car is not detecting cyclists at night. Upon
error analysis, they diagnose that their state-of-the-art object detection model, trained on an auto-
mobile detection dataset (X , Y) of images, is indeed underperforming on night and cyclist slices.
They write two SFs: λ1 to classify night vs. day, based on pixel intensity; and λ2 to detect bicycles,
which calls a pretrained object detector for a bicycle (with or without a rider). Given these SFs, the
developer leverages Slice-based Learning to improve model performance on safety-critical subsets.

Our problem setup makes a key assumption: SFs may be non-servable during test-time—i.e, during
inference, an SF may be unavailable because it is too expensive to compute or relies on private
metadata [1]. In Example 1, the potentially expensive cyclist detection algorithm is non-servable
at runtime. When our model is served at inference, SFs are not necessary, and we can rely on the
model’s learned indicators.

3.2 Model Architecture

The Slice-based Learning architecture has six components. The key intuition is that we will train a
standard prediction model, which we call the base task. We then learn a representation for each slice
that explains how its predictions should differ from the representation of the base task—i.e., a residual.
An attention mechanism then combines these representations to make a slice-aware prediction.

4

(b)(c)(b)(c)no_cyclist(e)def sf_night(x):    return avg(        X.pixels.intensity    ) < 0.3...(f)(a)Backbone (e.g. ResNet)def sf_bike(x):    return “bike” in        object_detector(x)Developer𝛌1𝛌kInput DataPrediction(d)...(a) Backbone architecture(b) Slice indicator head(c) Slice expert representation(d) Shared slice prediction head(e) Slice-aware representation(f) Prediction headSRAMkSRAM1a1p1q1akpkqk...T...r1rk⨂AttentionRepresentation ReweightingReweighting Mechanism

Overall

Performance (F1 score)
s3
s2

s1

UNIFORM
IND. OUTPUT
PRED. CONF.

FULL ATTENTION
(Ind. Output + Pred. Conf.)

77.1
78.1
79.3

82.7

57.1
52.6
61.1

68.6
71.0
69.2

73.6
76.4
78.7

s4

72.0
78.6
78.6

66.7

77.4

89.1

66.7

Figure 3: Architecture Ablation: Using a synthetic, two-class dataset (Figure, left) with four ran-
domly speciﬁed (size, shape, location) slices (Figure, middle), we specify corresponding, noisy SFs
(Figure, right) and ablate speciﬁc model components by modifying the reweighting mechanism for
slice expert representations. We compare overall/slice performance for uniform, indicator output, pre-
diction conﬁdence weighting, and the proposed attention weighting using all components. Our FULL
ATTENTION approach performs most consistently on slices without worsening overall performance.

With this intuition in mind, the six components (Figure 2) are: (a) a backbone, (b) a set of k
slice-indicator heads, and (c) k corresponding slice expert representations, (d) a shared slice
prediction head, (e) a combined, slice-aware representation, and (f) a prediction head. Each
SRAM operates over any backbone architecture and represents a path through components (b)
through (e). We describe the architecture assuming a binary classiﬁcation task (output dim. c = 1):

(a) Backbone: Our approach is agnostic to the neural network architecture, which we call the
backbone, denoted f ˆw, which is used primarily for feature extraction (e.g. the latest transformer for
textual data, CNN for image data). The backbone maps data points x to a representation z ∈ Rd.

(b) Slice indicator heads: For each slice, an indicator head will output an input’s slice membership.
The model will later use this to reweight the “expert” slice representations based on the likelihood
that an example is in the corresponding slice. Each indicator head maps the backbone representation,
z, to a logit indicating slice-membership: {qi}i=1,...,k ∈ R. Each slice indicator head is supervised
by the output of a corresponding SF, λi. For each example, we minimize the multi-label binary cross
entropy loss (LCE) between the unnormalized logit output of each qi and λi: (cid:96)ind = (cid:80)k
i LCE(qi, λi)

(c) Slice expert representations: Each slice representation, {ri}i=1,...,k, will be treated as an
“expert” feature for a given slice. We learn a linear mapping from the backbone, z, to each ri ∈ Rh,
where d(cid:48) is the size of all slice expert representations.

(d) Shared slice prediction head: A shared, slice prediction head, g(·), maps each slice expert
representation, ri, to a logit, {pi}i=1,...,k, in the output space of the base task: g(ri) = pi ∈ Rc,
where c = 1 for binary classiﬁcation. We train slice “expert” tasks using only examples belonging to
the corresponding slice, as speciﬁed by λi. Because parameters in g(·) are shared, each representation,
ri, is forced to specialize to the examples belonging to the slice. We use the base task’s ground truth
label, y, to train this head with binary cross entropy loss: (cid:96)pred = (cid:80)k

i λiLCE(pi, y)

(e) Slice-aware representation: For each example, the slice-aware representation is the combina-
tion of several “expert” slice representations according to 1) the likelihood that the input is in the
slice and 2) the conﬁdence of the slice “expert’s” prediction. To explicitly model the residual from
slice representations to the base representation, we initialize a trivial “base slice” which consists of
all examples so that we have the corresponding indicators, qBASE, and predictors, pBASE.
Let Q = {q1, . . . , qk, qBASE} ∈ Rk+1 be the vector of concatenated slice indicator logits, P =
{p1, . . . , pk, pBASE} ∈ Rc×k+1 be the vector of concatenated slice prediction logits, and R =
{r1, . . . , rk, rBASE} ∈ Rh×k+1 be the k + 1 stacked slice expert representations. We compute our
attention by combining the likelihood of slice membership, Q, and the slice prediction conﬁdence,
which we interpret as a function of the logits—in the binary case c = 1, we use abs(P ) as this
conﬁdence. We then apply a Softmax to create soft attention weights over the k + 1 slice expert

5

Noisy SFs (   )True ClassesTrue SlicesMethod

VANILLA
DP [31]
HPS [7]
MOE [18]

SBL

Performance (F1 score)
S2
S1
Overall

96.56
96.88
96.72
98.48

97.92

52.94
44.12
50.00
88.24

68.75
43.75
75.00
87.50

91.18

81.25

Figure 4: Scaling with hidden feature representation dimensions. We plot model quality versus
the hidden dimension size. The slice-aware model (SBL) improves over hard parameter sharing
(HPS) on both slices at a ﬁxed hidden dimension size, while being close to mixture of experts (MOE).
Note: MOE has signiﬁcantly more parameters overall, as it copies the entire model.

Figure 5: Coping with Noise: We test the robustness of our approach on a simple synthetic example.
In each panel, we show noisy SFs (left) as binary points and the corresponding slice indicator’s output
(right) as a heatmap of probabilities. We show that the indicator assigns low relative probabilities on
noisy (40%, middle) samples and ignores a very noisy (80%, right) SF, assigning relatively uniform
scores to all samples.

representations: a ∈ Rk+1 = Softmax(Q + abs(P )). Using a weighted sum, we then compute the
combined, slice-aware representation: z(cid:48) ∈ Rd(cid:48)

= Ra.

(f) Prediction head Finally, we use our slice-aware representation z(cid:48) as the input to a ﬁnal linear
layer, h(·), which we term the prediction head, to make a prediction on the original, base task. During
inference, this prediction head makes the ﬁnal prediction. To train the prediction head, we minimize
the cross entropy between the prediction head’s output, h(z(cid:48)), and the base task’s ground truth labels,
y: (cid:96)base = LCE(h(z(cid:48)), y).
Overall, the model is trained using loss values from all task heads: (cid:96)train = (cid:96)base + (cid:96)ind + (cid:96)pred. In
Figure 3, we show ablations of this architecture in a synthetic experiment varying the components in
the reweighting mechanism—speciﬁcally, our described attention approach outperforms using only
indicator outputs, only predictor conﬁdences, or uniform weights to reweight the slice representations.

3.3 Synthetic data experiments

To understand the properties of Slice-based Learning (SBL), we validate our model and its com-
ponents (Figure 2) on a set of synthetic data. In the results demonstrated in Figure 1, we construct
a dataset X ∈ R2 with a 2-way classiﬁcation problem in which over 95% of the data are linearly
separable. We introduce two minor perturbations along the decision boundary, which we deﬁne as
critical slices, s1 and s2. Intuitively, examples that fall within these slices follow different distribu-
tions (P (Y|X , si)) relative to the overall data (P (Y|X )). For all models, the shared backbone is
deﬁned as a 2-layer MLP architecture with a backbone representation size d = 13 and a ﬁnal ReLU
non-linearity. In SBL, the slice-expert representation is initialized with the same size: d(cid:48) = 13.

The model learns the slice-conditional label distribution P (Y |si, X) from noisy SF inputs.
We show in Figure 1b that the slices at the perturbed decision boundary cannot be learned in
the general case, by a VANILLA model. As a result, we deﬁne two SFs, λ1 and λ2, to target the slices
of interest. Because our attention-based model (SBL) is slice-aware, it outperforms VANILLA, which

6

Reported  in TableF1 score (S1)0.40.60.81.0Hidden Dim.51015Reported  in TableF1 score (S2)0.40.60.81.0Hidden Dim.51015VanillaDPHPSMoESBL (Ours)0% noiseConfidence0% noise40% noise80% noiseSF (   ) OutputInd. OutputSF (   ) OutputSF (   ) OutputInd. OutputInd. Outputhas no notion of slices (Figure 1d). Intuitively, if the model knows “where” in the 2-dim data space
an example lives (as deﬁned by SFs), it can condition on slice-speciﬁc features as it makes a ﬁnal,
slice-aware prediction. In Figure 5, we observe our model’s ability to cope with noisy SF inputs: the
indicator is robust to moderate amounts of noise by ignoring noisy examples (middle); with extremely
noisy inputs, it disregards poorly-deﬁned SFs by assigning relatively uniform weights (right).

Overall model performance does not degrade. The primary goal of the slice-aware model is to
improve slice-speciﬁc performance without degrading the model’s existing capabilities. We show that
SBL improves the overall score by 1.36 F1 points by learning the proportionally smaller perturbations
in the decision boundary in addition to the more general linear boundary (Figure 4, left). Further, we
note that we do not regress performance on individual slices.

Learning slice weights with features P (Y |si, X) improves over doing so with only supervision
source information P (Y |si). A core assumption of our approach asserts that if the model learns
improved slice-conditional weights via λi, downstream slice-speciﬁc performance will improve.
Data programming (DP) [31] is a popular weak supervision approach deployed at numerous Fortune
500 companies [2, 32], in which the weights of heuristics are learned solely from labeling source
information. We emphasize that our setting provides the model with strictly more information—in
the data’s feature representations—to learn such weights; we show in Figure 4 (right) that increasing
representation size allows us to signiﬁcantly outperform DP.

Attention weights learn from noisy λi to combine slice residual representations. SBL achieves
improvements over methods that do not aggregate slice information, as deﬁned by each noisy λi.
Both the indicator outputs (Q) and prediction conﬁdence (abs(P )) are robustly combined in the
attention mechanism. Even a noisy indicator will be upweighted if the predictions are high conﬁdence,
and if the indicator has high signal, even a slice expert making poor predictions can beneﬁt from
underlying slice-speciﬁc features. We show in Figure 4 that our method improves over HPS, which
is slice-aware, but has no way of combining slice information despite increasingly noisy λi. In
contrast, our attention-based architecture is able to combine slice expert representations, as SBL sees
improvements over VANILLA by 38.2 slice-level F1 averaged across s1 and s2.

SBL demonstrates similar expressivity to MoE with much less cost. With approximately half
as many parameters, SBL comes within 6.25 slice-level F1 averaged across s1 and s2 of MOE
(Figure 4). With large backbone architectures, characterized by M parameters, and a large number of
slices, k, MOE requires a quadratically large number of parameters, because we initialize an entire
backbone for each slice. In contrast, all other models scale linearly in parameters with M .

4 Experiments

Compared to baselines using the same backbone architecture, we demonstrate that our approach
successfully models slice importance and improves slice-level performance without impacting overall
model performance. Then, we demonstrate our method’s advantages in aggregating noisy heuristics,
compared to existing weak supervision literature. We perform all empirical experiments on Google’s
Cloud infrastructure using NVIDIA V100 GPUs.

4.1 Applications

Using natural language understanding (NLU) and computer vision (CV) datasets, we compare our
method to baselines commonly used in practice or the literature to address slice-speciﬁc performance.

4.1.1 Baselines

For each baseline, we ﬁrst train the backbone parameters with a standard hyperparameter search
over learning rate and (cid:96)2 regularization values. Then, each method is initialized from the backbone
weights and ﬁne-tuned for a ﬁxed number of epochs and the optimal hyperparameters.

VANILLA: A vanilla neural network backbone is trained with a ﬁnal prediction head to make
predictions. This baseline represents the de-facto approach used in deep learning modeling tasks; it is
unaware of slices information and neglects to model them as a result.

7

Dataset

COLA (Matthews Corr. [24])

RTE (F1 Score)

CYDET (F1 Score)

Slice Lift

Slice Lift

Param
Inc.

–
12%
12%
100%

Overall (std)

57.8 (±1.3)
57.4 (±2.1)
57.9 (±1.2)
57.2 (±0.9)

VANILLA
HPS [7]
MANUAL
MOE [18]

Max

–
+12.7
+6.3
+20.0

Param
Inc.

–
10%
10%
100%

Overall (std)

67.0 (±1.6)
67.9 (±1.8)
69.4 (±1.8)
69.2 (±1.5)

Max

–
+12.7
+10.7
+10.9

SBL

12%

58.3 (±0.7)

+19.0

10%

69.5 (±0.8)

+10.9

Avg

–
1.1
+0.4
+1.3

+2.5

Avg

–
+2.9
+4.2
+3.9

+4.6

Param
Inc.

–
10%
10%
100%

Overall (std)

39.4 (±-5.4)
37.4 (±3.6)
36.9 (±4.2)
OOM

Slice Lift

Max

Avg

–
+6.3
+6.3
OOM

–
-0.7
-1.7
OOM

10%

40.9 (±3.9)

+15.6

+2.3

Table 1: Application Datasets: We compare our model to baselines averaged over 5 runs with
different seeds in natural language understanding and computer vision applications and note the
relative increase in number of params for each method. We report the overall score and maximum
relative improvement (denoted Lift) over the VANILLA model for each of the slice-aware baselines.
For some trials of MOE, our system ran out of GPU memory (denoted OOM).

MOE: We train a mixture of experts [18], where each expert is a separate VANILLA model trained
on a data subset speciﬁed by the SF, λi. A gating network [37] is then trained to combine expert
predictions into a ﬁnal prediction.
HPS: In the style of multi-task learning, we model slices as separate task heads with a shared
backbone trained via hard parameter sharing. Each slice task performs the same prediction task,
but they are trained on subsets of data corresponding to λi. In this approach, backpropagation from
different slice tasks is intended to encourage a slice-aware representation bias [7, 35].
MANUAL: To simulate the manual effort required to tune slice-speciﬁc hyperparameters, we leverage
the same architecture as HPS and grid search over loss term multipliers, α ∈ {2, 20, 50, 100}, for
underperforming slices based on VANILLA model predictions (i.e. scoreoverall − scoreslice ≥ 5 F1).

4.1.2 Datasets

NLU Datasets. We select slices based on independently-conducted error analyses [20] (Ap-
pendix A1.2). In Corpus of Linguistic Acceptability (COLA) [40], the task is to predict whether
a sentence is linguistically acceptable (i.e. grammatically); we measure performance using the
Matthews correlation coefﬁcient [24]. Natural slices might occur as questions or long sentences,
as corresponding examples might consist of non-standard or challenging sentence structure. Since
ground truth test labels are not available for this task (they are held out in evaluation servers [39]), we
sample to create data splits with 7.2K/1.3K/1K train/valid/test sentences, respectively. To properly
evaluate slices of interest, we ensure that the proportions of examples in ground truth slices are
consistent across splits. In Recognizing Textual Entailment (RTE) [3, 4, 10, 15, 39], the task is to
predict whether or not a premise sentence entails a hypothesis sentence. Similar to COLA, we create
our own data splits and use 2.25K/0.25K/0.275K train/valid/test sentences, respectively. Finally, in a
user study where we work with practitioners tackling the SuperGlue [38] benchmark, we leverage
Slice-based Learning to improve state-of-the-art model quality on benchmark submissions.

CV Dataset. In the image domain, we evaluate on an autonomous vehicle dataset called Cyclist
Detection for Autonomous Vehicles (CYDET) [22]. We leverage clips in a self-driving video
dataset to detect whether a cyclist (person plus bicycle) is present at each frame. We select one
independent clip for evaluation, and the remainder for training; for valid/test splits, we select
alternating batches of ﬁve frames each from the evaluation clip. We preprocess the dataset with an
open-source implementation of Mask R-CNN [23] to provide metadata (e.g. presence of trafﬁc lights,
benches), which serve as slice indicators for each frame.

4.1.3 Results

Slice-aware models improve slice-speciﬁc performance. We see in Table 1 that each slice-aware
model (HPS, MANUAL, MOE, SBL) largely improves over the naive model.

SBL improves overall performance. We also observe that SBL improves overall performance for
each of the datasets. This is likely because the chosen slices were explicitly modeled from error
analysis papers, and explicitly modeling “error” slices led to improved overall performance.

SBL learns slice expert representations consistently. While HPS and MANUAL perform well
on some slices, they exhibit much higher variance compared to SBL and MOE (as denoted by the

8

std. in Table 1). These baselines lack an attention mechanism to reweight slice representations in a
consistent way; instead, they rely purely on representation bias from slice-speciﬁc heads to improve
slice-level performance. Because these representations are not modeled explicitly, improvements are
largely driven by chance, and this approach risks worsening performance on other slices or overall.

SBL improves performance with a parameter-efﬁcient representation. For CoLA and RTE
experiments, we used the BERT-base [13] architecture with 110M parameters; for CyDet, we used
ResNet-18 [16]. For each additional slice, SBL requires a 7% and 5% increase in relative parameter
count in the BERT and ResNet architectures, respectively (total relative parameter increase reported
in Table 1). As a comparison, HPS requires the same relative increase in parameters per slice. MOE
on the other hand, increases relative number of parameters by 100% per slice for both architectures.
With limited increase in model size, SBL outperforms or matches all other baselines, including MOE,
which requires an order of magnitude more parameters.

SBL improves state-of-the-art quality models with slice-aware representations. In a submission
to SuperGLUE evaluation servers, we leverage the same BERT-large architecture as previous
submissions and observe improvements on NLU tasks: +3.8/+2.8 avg. F1/acc. on CB [12], +2.4 acc.
on COPA [34], +2.5 acc. on WiC [28], and +2.7 on the aggregate benchmark score.

4.2 Weak Supervision Comparisons

To contextualize our contributions in the weak supervision literature, we compare directly to Data Pro-
gramming (DP) [29], a popular approach for reweighting user-speciﬁed heuristics using supervision
source information [31]. We consider two text-based relation extraction datasets: Chemical-Disease
Relations (CDR),[41], in which we identify causal links between chemical and disease entities in a
dataset of PubMed abstracts, and Spouses [9], in which we identify mentions of spousal relationships
using preprocessed pairs of person mentions from news articles (via Spacy [17]). In both datasets, we
leverage the exact noisy linguistic patterns and distant supervision heuristics provided in the open-
source implementation of DP. Rather than voting on a particular class, we repurpose the provided
labeling functions as binary slice indicators for our model. We then train our slice-aware model on
the probabilistic labels aggregated from these heuristics.

SBL improves over current weak supervision methods. Treating the noisy heuristics as slicing
functions, we observe lifts of up to 1.3 F1 overall and 15.9 F1 on heuristically-deﬁned slices. We
reproduce the DP [29] setup to obtain overall scores of F1=41.9 on Spouses and F1=56.4 on CDR.
Using Slice-based Learning, we improve to 42.8 (+0.9) and 57.7 (+1.3) F1, respectively. Intuitively,
we can explain this improvement, because SBL has access to features of the data belonging to slices
whereas DP relies only on the source information of each heuristic.

5 Conclusion

We introduced the challenge of improving slice-speciﬁc performance without damaging the overall
model quality, and proposed the ﬁrst programming abstraction and machine learning model to support
these actions. We demonstrated that the model could be used to push the state-of-the-art quality. In
our analysis, we can explain consistent gains in the Slice-based Learning paradigm, as our attention
mechanism has access to a rich set of deep features, whereas existing weak supervision paradigms
have no way to access this information. We view this work in the context of programming models
that sit on top of traditional modeling approaches in machine learning systems.

Acknowledgements We would like to thank Braden Hancock, Feng Niu, and Charles Srisuwananukorn for many
helpful discussions, tests, and collaborations throughout the development of slicing. We gratefully acknowledge
the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), FA86501827882 (ASED),
NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078
(Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation,
NXP, Xilinx, LETI-CEA, Intel, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson,
Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, Google Cloud, Swiss
Re, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, SAP,
VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommen-
dations expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or
endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.

9

References

[1] Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in deploying weak
supervision at industrial scale. In Proceedings of the 2019 International Conference on Management of
Data, pages 362–375. ACM, 2019.

[2] Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alexander Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in deploying
weak supervision at industrial scale. arXiv preprint arXiv:1812.00417, 2018.

[3] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the second
PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6–4. Venice, 2006.

[4] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual

entailment challenge. In TAC, 2009.

[5] Daniel Berend and Aryeh Kontorovich. Consistency of weighted majority votes. In Proceedings of the
27th International Conference on Neural Information Processing Systems, NIPS’14, pages 3446–3454,
Cambridge, MA, USA, 2014. MIT Press.

[6] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of

the eleventh annual conference on Computational learning theory, pages 92–100. ACM, 1998.

[7] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

[8] Hao Cheng, Hao Fang, and Mari Ostendorf. Open-domain name error detection using a multi-task rnn.
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages
737–746, 2015.

[9] David Corney, Dyaa Albakour, Miguel Martinez-Alvarez, and Samir Moussa. What do a million news

articles look like? In NewsIR@ ECIR, pages 42–47, 2016.

[10] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge.

In Machine Learning Challenges Workshop, pages 177–190. Springer, 2005.

[11] Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. Aggregating crowdsourced binary
ratings. In Proceedings of the 22Nd International Conference on World Wide Web, WWW ’13, pages
285–294, New York, NY, USA, 2013. ACM.

[12] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating
projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung, volume 23, pages
107–124, 2019.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[14] T. Mitchell et. al. Never-ending learning. In AAAI, 2015.

[15] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing
textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and
paraphrasing, pages 1–9. Association for Computational Linguistics, 2007.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[17] Matthew Honnibal and Mark Johnson. An improved non-monotonic transition system for dependency
parsing. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages 1373–1378, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

[18] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local

experts. Neural computation, 3(1):79–87, 1991.

[19] Andrej Karpathy. Building the software 2.0 stack, 2019.

[20] Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R Thomas McCoy, Ian Tenney, Alexis
Ross, Tal Linzen, Benjamin Van Durme, et al. Probing what different nlp tasks teach machines about
function word comprehension. arXiv preprint arXiv:1904.11544, 2019.

10

[21] G. S. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning with weakly

labeled data. JMLR, 11(Feb):955–984, 2010.

[22] Alexerand Masalov, Jeffrey Ota, Heath Corbet, Eric Lee, and Adam Pelley. Cydet: Improving camera-
based cyclist recognition accuracy with known cycling jersey patterns. In 2018 IEEE Intelligent Vehicles
Symposium (IV), pages 2143–2149. IEEE, 2018.

[23] Francisco Massa and Ross Girshick. maskrcnn-benchmark: Fast, modular reference implementa-
tion of Instance Segmentation and Object Detection algorithms in PyTorch. https://github.com/
facebookresearch/maskrcnn-benchmark, 2018. Accessed: Feb 2019.

[24] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme.

Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975.

[25] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled

data. In Proc ACL, pages 1003–1011, 2009.

[26] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
3994–4003, 2016.

[27] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratiﬁcation causes
clinically meaningful failures in machine learning for medical imaging. arXiv preprint arXiv:1909.12475,
2019.

[28] Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for evaluating

context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018.

[29] A.J. Ratner, S.H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Ré. Snorkel: Rapid training data creation

with weak supervision. In VLDB, 2018.

[30] Alexander Ratner, Braden Hancock, and Christopher Ré. The role of massively multi-task and weak

supervision in software 2.0. 2019.

[31] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Ré. Data programming:
Creating large training sets, quickly. In Advances in neural information processing systems, pages 3567–
3575, 2016.

[32] Christopher Ré, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. Overton: A data system for

monitoring and improving machine-learned products. 2019.

[33] Marek Rei. Semi-supervised multitask learning for sequence labeling. arXiv preprint arXiv:1704.07156,

2017.

[34] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An

evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.

[35] Sebastian Ruder. An overview of multi-task learning in deep neural networks.

arXiv preprint

arXiv:1706.05098, 2017.

[36] Omer Sagi and Lior Rokach. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining

and Knowledge Discovery, 8(4):e1249, 2018.

[37] Olivier Sigaud, Clément Masson, David Filliat, and Freek Stulp. Gated networks: an inventory. arXiv

preprint arXiv:1512.03201, 2015.

[38] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding
systems. arXiv preprint arXiv:1905.00537, 2019.

[39] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-
task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461,
2018.

[40] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv

preprint arXiv:1805.12471, 2018.

[41] Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Jiao Li, Thomas C
Wiegers, and Zhiyong Lu. Overview of the biocreative v chemical disease relation (cdr) task. In Proceedings
of the ﬁfth BioCreative challenge evaluation workshop, pages 154–166, 2015.

[42] Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation

approach. arXiv preprint arXiv:1605.06391, 2016.

11

A1 Appendix

A1.1 Model Characteristics

We include summarized model characteristics and the associated baselines to supplement Sections 3.3 and 4.1.1.

Method

Slice-aware

VANILLA
HPS
MANUAL
MOE
SBL

(cid:88)
(cid:88)
(cid:88)
(cid:88)

No manual
tuning
(cid:88)
(cid:88)

(cid:88)
(cid:88)

Weighted
slice info.

(cid:88)
(cid:88)
(cid:88)

Avoids copies of
model (M params)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

Num. Params

O(M + r)
O(M + kr)
O(M + kr)
O(kM + kr)
O(M + krd(cid:48))

Table 2: Model characterizations: We characterize each model’s advantages/limitations and com-
pute the number of parameters for each baseline model, given k slices, M backbone parameters,
feature representation z dimension r, and slice expert representation pi dimension d(cid:48).

A1.2 Slicing Function (SF) Construction

We walk through speciﬁc examples of SFs written for a number of our applications.

Textual SFs For text-based applications (COLA, RTE), we write SFs over pairs of sentences for each task.
Following dataset convention, we denote the ﬁrst sentence as the premise and the second as the hypothesis where
appropriate. Then, SFs are written, drawing largely from existing error analysis [20]. For instance, we might
expect certain questions to be especially difﬁcult to formulate in a language acceptability task. So, we write the
following SF to heuristically target where questions:

def SF _wh ere _que sti on ( premise , hypothesis ) :

# triggers if " where " appears in sentence
sentences = premise + hypothesis
return " where " in sentences . lower ()

In some cases, we write SFs over both sentences at once. For instance, to capture possible errors in article
references (e.g. the Big Apple vs a big apple), we specify a slice where multiple instances of the same article
appear in provided sentences:

def S F _ h a s _ m u l t i p l e _ a r t i c l e s ( premise , hypothesis ) :

# triggers if a sentence has more than one occurrence of the same article
sentences = premise + hypothesis
multiple_a = sum ([ int ( x == " a " ) for x in sentences . split () ]) > 1
multiple_an = sum ([ int ( x == " an " ) for x in sentences . split () ]) > 1
multiple_the = sum ([ int ( x == " the " ) for x in sentences . split () ]) > 1
return multiple_a or multiple_an or multiple_the

Image-based SFs For computer vision applications, we leverage image metadata and bounding box at-
tributes, generated from an off-the-shelf Mask R-CNN [23], to target slices of interest.

def SF_bus ( image ) :

# triggers if a " bus " appears in the predictions of the noisy detector
outputs = noisy_detector ( image )
return " bus " in outputs

We note that these potentially expensive detectors are non-servable—they run ofﬂine, and our model uses
learned indicators at inference time. Despite the detectors’ noisy predictions, our model is able to to reweight
representations appropriately.

A1.3 COLA SFs

COLA is a language acceptability task based on linguistics and grammar for individual sentences. We draw from
error analysis which introduces several linguistically imortance slices for language acceptability via a series of

12

challenge tasks. Each task consists of synthetically generated examples to measure model evaluation on speciﬁc
slices. We heuristically deﬁne SFs to target subsets of data corresponding to each challenge, and include the full
set of SFs derived from each category of challenge tasks:

• Wh-words: This task targets sentences containing who, what, where, when, why, how. We exclude why and
how below because the COLA dataset does not have enough examples for proper training and evaluation of
these slices.

def S F _ w h e r e _ i n _ s e n t e n c e ( sentence ) :
return " where " in sentence

def S F_ wh o _i n _s en t en c e ( sentence ) :
return " who " in sentence

def S F _ w ha t _ i n _ s e n te n c e ( sentence ) :
return " what " in sentence

def S F _ w he n _ i n _ s e n te n c e ( sentence ) :
return " when " in sentence

• Deﬁnite-Indeﬁnite Articles: This challenge measures the model based on different combinations of deﬁnite
(the) and indeﬁnite (a,an) articles in a sentence (i.e. swapping deﬁnite for indeﬁnite articles and vice versa).
We target containing multiple uses of a deﬁnite (the) or indeﬁnite article (a, an):

def S F _ h a s _ m u l t i p l e _ a r t i c l e s ( sentence ) :

# triggers if a sentence has more than one occurrence of the same article
m u l t i pl e _ i n d e f in i t e = sum ([ int ( x == " a " ) for x in sentence . split () ]) > 1 or sum ([
int ( x == " an " ) for x in sentence . split () ]) > 1
mu lti ple_ def ini te = sum ([ int ( x == " the " ) for x in sentence . split () ]) > 1

return m u l t i pl e _ i n d e f in i t e or mu lti ple_ def ini te

• Coordinating Conjunctions: This task seeks to measure correct usage of coordinating conjunctions (and,

but, or) in context. We target the presence of these words in both sentences.

def and_in_sentence ( sentence ) :

return " and " in sentence

def but_in_sentence ( sentence ) :

return " but " in sentence

def or_in_sentence ( sentence ) :

return " or " in sentence

• End-of-Sentence: This challenge task measures a model’s ability to identify coherent sentences or sentence
chunks after removing puctuation. We heuristically target this slice by identifying particularly short sentences
and those that end with verbs and adverbs. We use off-the-shelf parsers (i.e. Spacy [17]) to generate
part-of-speech tags.

def SF _sh ort_ sen ten ce ( sentence ) :

# triggered if sentence has fewer than 5 tokens
return len ( sentence . split () ) < 5

# Spacy tagger
def get_spacy_pos ( sentence ) :

import spacy
nlp = spacy . load ( " en_core_web_sm " )
return nlp ( sentence ) . pos_

def SF _en ds_w ith _ve rb ( sentence ) :

# remove last token , which is always punctuation
sentence = sentence [:−1]
return get_spacy_pos ( sentence )[−1] == " VERB "

def S F _ e nd s _ w i t h _ a dv e r b ( sentence ) :

# remove last token , which is always punctuation
sentence = sentence [:−1]
return get_spacy_pos ( sentence )[−1] == " ADVERB "

A1.4 RTE SFs

Similar to COLA, we use challenge tasks from NLI-based error analysis [20] to write SFs over the textual
entailment (RTE) dataset.

13

• Prepositions: In one challenge, the authors swap prepositions in the dataset with prepositions in a manually-
curated list. The list in its entirety spans a large proportion of the RTE dataset, which would constitute a very
large slice. We ﬁnd it more effective to separate these prepositions into temporal and possessive slices.

def S F _ h a s _ t e m p o r a l _ p r e p o s i t i o n ( premise , hypothesis ) :

t e m p o r a l _ p r e p o s i t i o n s = [ " after " , " before " , " past " ]
sentence = premise + sentence
return any ([ p in sentence for p in t e m p o r a l _ p r e p o s i t i o n s ])

def S F _ h a s _ p o s s e s s i v e _ p r e p o s i t i o n ( premise , hypothesis ) :

p o s s e s s i v e _ p r e p o s i t i o n s = [ " inside of " , " with " , " within " ]
sentence = premise + sentence
return any ([ p in sentence for p in p o s s e s s i v e _ p r e p o s i t i o n s ])

• Comparatives: One challenge chooses sentences with speciﬁc comparative words and mutates/negates them.

We directly target keywords identiﬁed in their approach.

def SF _is _com par ati ve ( premise , hypothesis ) :

co mpa rati ve_ wor ds = [ " more " , " less " , " better " , " worse " , " bigger " , " smaller " ]
sentence = premise + hypothesis
return any ([ p in sentence for p in co mpa rat ive_ wor ds ])

• Quantiﬁcation: One challenge tests natural language understanding with common quantiﬁers. We target

common quantiﬁers in both the combined premise/hypothesis and in only the hypothesis.

def is _qu anti fic ati on ( premise , hypothesis ) :

quantifiers = [ " all " , " some " , " none " ]
sentence = premise + hypothesis
return any ([ p in sentence for p in quantifiers ])

def i s _ q u a n t i f i c a t i o n _ h y p o t h e s i s ( premise , hypothesis ) :

quantifiers = [ " all " , " some " , " none " ]
return any ([ p in hypothesis for p in quantifiers ])

• Spatial Expressions: This challenge identiﬁes spatial relations between entities (i.e. A is to the left of B). We
exclude this task from our slices, because such slices do not account for enough examples in the RTE dataset.

• Negation: This challenge task identiﬁes whether natural language inference models can handle negations.

We heuristically target this slice via a list of common negation words from a top result in a web search.

def S F_ co m mo n _n eg a ti o n ( premise , hypothesis ) :

# Words from https :// www . grammarly . com / blog / negatives /
negation_words = [

" no " ,
" not " ,
" none " ,
" no one " ,
" nobody " ,
" nothing " ,
" neither " ,
" nowhere " ,
" never " ,
" hardly " ,
" scarcely " ,
" barely " ,
" doesnt " ,
" isnt " ,
" wasnt " ,
" shouldnt " ,
" wouldnt " ,
" couldnt " ,
" wont " ,
" cant " ,
" dont " ,

]
sentence = premise + hypothesis
return any ([ x in negation_words for x in sentence ])

• Premise/Hypothesis Length: Finally, separate from the cited error analysis, we target different length
hypotheses and premises as an additional set of slicing tasks. In our own error analysis of the RTE model, we
found these represented intuitive slices: long premises are typically harder to parse for key information, and
shorter hypotheses tend to share syntactical structure.

def S F _ s h or t _ h y p o t he s i s ( premise , hypothesis ) :

return len ( hypothesis . split () ) < 5

14

def S F_ lo n g_ h yp ot h es i s ( premise , hypothesis ) :
return len ( hypothesis . split () ) > 100

def SF_short_premise ( premise , hypothesis ) :

return len ( premise . split () ) < 15

def SF_long_premise ( premise , hypothesis ) :
return len ( premise . split () ) > 100

A1.5 CYDET SFs

For the cyclist detection dataset, we identify subsets that correspond to other objects in the scene using a noisy
detector (i.e. an off-the-shelf Mask R-CNN [23]).

# define noisy detector
def noise_detector ( image ) :

probs = mask_rcnn . forward ( image )

# threshold predictions
preds = []
for object in classes :

if probs [ " object " ] > 0.5:

preds . append ( object )

return preds

# Cyclist Detection SFs
def SF_bench ( image ) :

outputs = noisy_detector ( image )
return " bench " in outputs

def SF_truck ( image ) :

outputs = noisy_detector ( image )
return " truck " in outputs

def SF_car ( image ) :

outputs = noisy_detector ( image )
return " car " in outputs

def SF_bus ( image ) :

outputs = noisy_detector ( image )
return " bus " in outputs

def SF_person ( image ) :

outputs = noisy_detector ( image )
return " person " in outputs

def SF_traffic_light ( image ) :

outputs = noisy_detector ( image )
return " traffic light " in outputs

def SF_fire_hydrant ( image ) :

outputs = noisy_detector ( image )
return " fire hydrant " in outputs

def SF_stop_sign ( image ) :

outputs = noisy_detector ( image )
return " stop sign " in outputs

def SF_bicycle ( image ) :

outputs = noisy_detector ( image )
return " bicycle " in outputs

A1.6 Slice-speciﬁc Metrics

We visualize slice-speciﬁc metrics across each application dataset, for each method of comparison. We report
the corresponding aggregate metrics in Figure 1 (below).

In COLA, we see that MOE and SBL exhibit the largest slice-speciﬁc gains, and also overﬁt on the same slice
ends with adverb. In RTE, we see that SBL improves performance on all slices except common negation, where
it falls less than a point below VANILLA. On CYDET, we see the largest gains for SBL on bench and bus
slices—in particular, we are able to improve in cases where the model might able to use the presence of these
objects to make more informed decisions about whether a cyclist is present. Note: because the MOE model on
CYDET encounters an “Out of Memory" error, the corresponding (blue) data bar is not available for this dataset.

15

Figure 6: For each application dataset (Section 4.1) we report all relative, slice-level metrics compared
to VANILLA for each model.

16

who questionwhat questionwhere questionwhen questionhas multiple articlesshort premise colahas buthas andhas orends with verbends with adverb0.200.150.100.050.000.050.100.150.20Relative Improvementsover Vanilla Model (F1)Slice-level Improvements on COLA per BaselineVanilla PerformanceMoEManualHPSSBL (Ours)has temporalprepositionhas possessiveprepositionis comparativeis quantificationis quantificationhypothesisshort hypothesislong hypothesisshort premiselong premisecommon negation0.100.050.000.050.100.15Relative Improvementsover Vanilla Model (F1)Slice-level Improvements on RTE per Baselinebenchtruckcarbuspersontraffic lightfire hydrantstop signbicycle0.150.100.050.000.050.100.150.20Relative Improvementsover Vanilla Model (F1)Slice-level Improvements on CyDet per Baseline