2
2
0
2

y
a
M
1
3

]
L
M

.
t
a
t
s
[

1
v
4
7
0
0
0
.
6
0
2
2
:
v
i
X
r
a

To the Fairness Frontier and Beyond:
Identifying, Quantifying, and Optimizing the Fairness-Accuracy
Pareto Frontier

Camille Olivia Little∗1, Michael Weylandt∗2, and Genevera I. Allen1,3,4

1Department of Electrical and Computer Engineering, Rice University
2University of Florida Informatics Institute
3Department of Computer Science, Rice University
3Department of Statistics, Rice University

June 2, 2022

Abstract

Algorithmic fairness has emerged as an important consideration when developing and deploying
machine learning models to make high-stakes societal decisions. Yet, improved fairness often
comes at the expense of model accuracy. While aspects of the fairness-accuracy tradeoﬀ have
been studied, most work reports the fairness and accuracy of various models separately; this
makes model comparisons nearly impossible without a uniﬁed model-agnostic metric that reﬂects
the Pareto optimal balance of the two desiderata. In this paper, we seek to identify, quantify, and
optimize the empirical Pareto frontier of the fairness-accuracy tradeoﬀ, deﬁned as the highest
attained accuracy at every level of fairness for a collection of ﬁtted models. Speciﬁcally, we
identify and outline the empirical Pareto frontier through our Tradeoﬀ-between-Fairness-and-
Accuracy (taf) Curves; we then develop a single metric to quantify this Pareto frontier through
the weighted area under the taf Curve which we term the Fairness-Area-Under-the-Curve (fauc).
Our taf Curves provide the ﬁrst empirical, model-agnostic characterization of the Pareto frontier,
while our fauc provides the ﬁrst uniﬁed metric to impartially compare model families in terms
of both fairness and accuracy. Both taf Curves and fauc are general and can be employed with
all group fairness deﬁnitions and accuracy measures. Next, we ask: Is it possible to expand the
empirical Pareto frontier and thus improve the fauc for a given collection of ﬁtted models? We
answer in the aﬃrmative by developing a novel fair model stacking framework, FairStacks.
FairStacks solves a convex program to maximize the accuracy of a linear combination of
ﬁtted models subject to a constraint on score-based model bias. We show that optimizing with
FairStacks always expands the empirical Pareto frontier and improves the fauc; we additionally
study other theoretical properties of our proposed approach. Finally, we empirically validate
taf, fauc, and FairStacks through studies on several real benchmark data sets, showing that
FairStacks leads to major improvements in fauc that outperform existing algorithmic fairness
approaches.

Keywords: Fairness AUC; Fairness-Accuracy Tradeoﬀ; Fair Model Stacking; Fairness Pareto

Frontier

∗These authors contributed equally.

1

 
 
 
 
 
 
1

Introduction

Machine learning algorithms are now widely used to help make high-stakes decisions, such as deciding

if an applicant should be approved for a loan or predicting if a convict will commit another crime.

These decisions can have life-altering consequences and many have shown that machine learning

models can be biased and unintentionally discriminate against protected groups [11, 22]. In response

to this major problem, many authors have developed techniques to identify and mitigate bias in

machine learning algorithms [9, 15, 23, 3, 17, 4, 24, 25]. Many of these techniques can dramatically

improve fairness, but often at the expense of model accuracy; this leads to what has been called the
fairness-accuracy tradeoﬀ [45, 46, 48, 34, 1, 31].

When making certain high-stakes decisions, balancing the tradeoﬀ between fairness and accuracy

is absolutely critical. Take, for example, the Federal Fair House Act (FFHA) of 1968. The FFHA

protects people from discrimination when engaging in housing-related activities such as renting

or buying a home or getting a mortgage. The FFHA is designed to prohibit discrimination on

the basis of race, religion, sex, and other protected attributes, yet as of 2020, Black and Latino

mortgage applicants are more likely to be declined than white applicants [37]. When considering

loan applicants, lenders must balance FFHA requirements (fairness) and credit risk (accuracy).

This example demonstrates a special case of more general fairness-accuracy tradeoﬀ often seen in

real-world scenarios.

In this work, our objective is to identify, measure and optimize the empirical Pareto frontier of

the fairness-accuracy tradeoﬀ. This Pareto frontier is deﬁned as the highest attained accuracy at

every level of fairness for a collection of ﬁtted models. We are particularly motivated to deﬁne a

model-agnostic measure that quantiﬁes the fairness-accuracy tradeoﬀ in a single metric in order

for machine learning practitioners to easily compare existing bias mitigation strategies and choose

hyperparameters that balance the tradeoﬀ in a way suits their speciﬁc task. Moreover, we seek

to develop a ﬂexible meta-learner that expands the empirical Pareto frontier for the set of models

by optimizing the accuracy attainable at every level of fairness. Again, our goal is to develop a

model-agnostic approach that will improve fairness for any collection of models. Taken together, these

objectives would provide users simple and practical tools to assess and measure their models’ Pareto

frontier as well as an approach to further expand their models’ frontier and improve fairness.

1.1 Related Work

The existence of a fairness-accuracy tradeoﬀ has been noted and partially characterized in several

previous papers [34, 45, 46, 6, 48]. Zhao and Gordon [45, 46] prove that, when base rates diﬀer

across protected groups, the minimum possible error of any fair classiﬁes is bounded below by the

diﬀerence in base rates. This gives a valuable theoretical bound on the extremal fairness-accuracy

tradeoﬀ, but does not give a concrete proposal for comparing diﬀerent classiﬁers nor guidance on

how to tune particular classiﬁers.

2

On the other hand, Menon and Williamson [34] consider the problem of ﬁnding an optimal decision

rule, after a base classiﬁer has been learned. They show that under a speciﬁc loss function, the

resulting fairness-accuracy Pareto-frontier can be theoretically characterized, but their ﬁnding is not

applicable to general classiﬁers. Additionally, these previous characterizations of the fairness-accuracy

tradeoﬀ are restricted to binary classiﬁcation [45, 46, 34, 6, 48].

A related line of work [1, 32] attempts to explicitly optimize the fairness-accuracy Pareto frontier

in diﬀerent settings. Agarwal et al. [1] propose a model-agnostic method that iteratively calls

a black-box model and reweights or relabels the data to ﬁnd the most accurate fair version of

the input classiﬁer. The user can input any base learner into this method and recover the entire

fairness-accuracy Pareto frontier using their constrained optimization model that minimizes error

subject to a fairness constraint. This method oﬀers a useful way to construct the frontier, but it

takes as input only one class of models at a time. As a result, the quality of the frontier is limited

by the highest achievable accuracy or fairness of the single input base learner. Martinez et al. [32]

formulate group fairness as a multi-objective optimization problem where each group risk is an

objective function. Though this provides a useful way to identify the Pareto classiﬁer that minimizes

risk of the worst performing group, it is not apparent how their method would extend to other group

fairness notions.

Other proposals [26, 43, 41, 31, 3] attempt to ﬁnd the fairest classiﬁer on a certain problem by

modifying existing classiﬁers to reduce discrimination, typically by some sort of fairness regularizer

or constraint. For example, Kamishima et al. [26] introduce a regularization term to penalize

discrimination when formulating a logistic regression classiﬁer. Existing fairness-regularized methods

work well in certain scenarios, but they suﬀer from three main limitations: i) the added fairness

constraint typically yields a non-convex objective, posing signiﬁcant computational challenges; ii) the
approaches are ad hoc and only applicable to speciﬁc model families; and iii) the practical guarantees
associated with these relaxations are often insuﬃcient in practice. [28].

Existing bias mitigation strategies can be generally categorized into pre-processing techniques [24,

17, 4, 8, 21], in-processing techniques [44, 3, 19, 5], and post-processing techniques [20, 36, 25, 29,

10]. Though they work independently from the data and the model, post-processing techniques often

lead to a drastic decrease in accuracy as the results of the trained model are directly altered. To

avoid this, we leverage popular approaches from ensemble learning by using model stacking [40, 16]

to ﬁnd the fairness-accuracy Pareto frontier. Ensemble learning improves performance by reducing

variance of the prediction error by adding more bias [13]: we will show that fairness constraints

can similarly improve performance. To our knowledge, no existing ensemble learning or other

post-processing strategy stacks a set of models to speciﬁcally optimize the empirical fairness-accuracy

Pareto frontier.

3

l
e
d
o
m
e
h
t

n
e
e
w
t
e
b

p
i
h
s
n
o
i
t
a
l
e
r

e
h
t

s
t
h
g
i
l
h
g
i
h
A

l
e
n
a
P

.
s
n
o
i
t
u
b
i
r
t
n
o
c

l
a
c
i
g
o
l
o
d
o
h
t
e
m
d
n
a

l
a
c
i
t
e
r
o
e
h
t

r
o
j
a
m

r
u
o

f
o

s
n
o
i
t
c
i
p
e
D

:
1

e
r
u
g
i
F

h
t
o
b

r
o
f

c
i
r
t
e
m
c
u
a
f

e
h
t

s
e
z
i
l
a
u
s
i
v
B

l
e
n
a
P

;
)
2
-
1

s
n
o
i
t
i
n
ﬁ
e
D
(

s
e
v
r
u
c

i
f
a
t

d
n
a

f
a
t

g
n
i
t
l
u
s
e
r

e
h
t

d
n
a

,
s
r
e
b
m
e
m

l
a
m

i
t
p
o

o
t
e
r
a
P
s
t
i

,

H

s
s
a
l
c

s
e
v
o
r
p
m

i

y
l
t
n
a
c
ﬁ
i
n
g
i
s

)
4

n
o
i
t
c
e
S
(

e
r
u
d
e
c
o
r
p

s
k
c
a
t
S
r
i
a
F
r
u
o
w
o
h

s
t
c
i
p
e
d
C

l
e
n
a
P

;
5

n
o
i
t
c
e
S

n
i

d
e
s
u

t
h
g
i
e
w
-
p
e
t
s

%
0
8

d
n
a
m
r
o
f
i
n
u

e
h
t

s
a
i
b

e
r
o
c
s

d
e
s
o
p
o
r
p

r
u
o

n
e
e
w
t
e
b

p
i
h
s
n
o
i
t
a
l
e
r

e
h
t

s
t
c
i
p
e
d
D

l
e
n
a
P
d
n
a

;
)
1

n
o
i
t
i
s
o
p
o
r
P
(

s
r
e
b
m
e
m
e
l
b
m
e
s
n
e

l
a
u
d
i
v
i
d
n
i

e
h
t

f
o

f
a
t

e
h
t

n
o
p
u

.
s
a
i
b

f
o

s
e
r
u
s
a
e
m
d
r
a
d
n
a
t
s

e
r
o
m
d
n
a

4

1.2 Our Contributions

1. We introduce the Tradeoﬀ-between-Fairness-and-Accuracy ( taf) Curve which outlines the
empirical Pareto frontier consisting of the highest attained accuracy within a collection of ﬁtted
models at every level of fairness. This provides the ﬁrst model-agnostic quantitative and visual
summary of the fairness-accuracy tradeoﬀ for a model family or other collection of models.

2. Next, we provide the ﬁrst uniﬁed, scalar and model-agnostic measure of the empirical fairness-
accuracy tradeoﬀ by computing the weighted area under the taf Curve, termed the Fairness
Area-Under-the-Curve (fauc). Just as auc is used to compare classiﬁers, fauc is the ﬁrst
quantitative metric to impartially compare model families in terms of both accuracy and

fairness.

3. Finally, we seek to expand the empirical Pareto frontier of the collection of ﬁtted models
through a novel convex, bias-constrained model stacking framework called FairStacks. We
show that under certain assumptions, FairStacks expands the frontier and improves the
fauc, leading to a simple strategy to improve both the fairness and accuracy for any collection

of models.

4. We empirically validate our metrics and FairStacks framework on several real benchmark

data sets, showing that with suﬃcient input models, FairStacks dominates all approaches,
providing the highest accuracy at all levels of fairness (i.e., dominating the fairness-accuracy
Pareto frontier).

2

Identifying the Frontier: Tradeoﬀ-between-Fairness-Accuracy Curve

We begin our study of the fairness-accuracy tradeoﬀ by recalling the basic notions of Pareto
improvement, Pareto optimality, and of the Pareto frontier. Speciﬁcally, given two options, C1 and
C2, we say that C1 is a Pareto improvement over C2 if C1 is preferred over C2 by any consumer. An
option C1 is said to be Pareto optional if there exists no attainable Pareto improvement over it. The
set of all Pareto optimal options is known as the Pareto frontier [33]. In this paper, we evaluate
models on two metrics, fairness and accuracy, and the notion of a Pareto optimal model is easily

characterized:

Deﬁnition 1. Given a set of models H, an accuracy metric Accuracy : H → [0, 1], and a fairness

metric Fairness : H → [0, 1] both of which can be written as sums (averages) over individual ob-
servations, we say that h ∈ H is Pareto optimal in H if there does not exist h(cid:48) ∈ H such that
Accuracy(h(cid:48)) ≥ Accuracy(h) and Fairness(h(cid:48)) ≥ Fairness(h) with one inequality strict.

Here, H is any collection of models, Fairness(h) is any metric in [0, 1] that is decreasing in the
bias of model h according to standard fairness deﬁnitions [39], and Accuracy(h) is any measure of
the performance of model h in [0, 1]. The collection of models, H can be created by varying the
hyperparameters of a ﬁxed model family, by an ensemble of models, or by considering a larger

5

collection of possible models for a given problem. In what follows, we also assume that H always
contains a perfectly fair model, i.e., a model such that Fairness(h) = 1. This assumption is trivially
satisﬁed by including the constant (intercept-only) model and exists only to simplify the statements
of our results. In the context of classiﬁcation, most deﬁnitions of accuracy take values in [0, 1]
and can be used for Accuracy(·), while for regression tasks, one may rescale typical losses to be in
[0, 1] (e.g. R2 or e−MSE). Most standard deﬁnitions of bias in algorithmic fairness take values in
[0, 1] and hence we can let Fairness(h) = 1 − Bias(h); as an example, we can deﬁne the fairness for
Demographic Parity [15, 17] as FairnessDP(h) = 1 − |E(h|A = 1) − E(h|A = 0)|, where A denotes
the protected attribute. Overall, the machinery developed in this paper is very general and works
for any deﬁnitions of fairness and accuracy taking values in [0, 1] that follow the notion of more is
better; that is, more accuracy is preferred and more fairness is preferred.

For a given collection of models H, it clearly does not make sense to use a model h that is not
Pareto optimal. Thus, we seek to identify all Pareto optimal models in H. Note that the collection
of all of these optimal models forms the Pareto frontier; if these models are ordered, they also form
an explicit fairness-accuracy tradeoﬀ curve i.e., the empirical Pareto frontier. This motivates the
following:

Deﬁnition 2. Given a ﬁnite collection of models H, the taf curve associated with H is a function
from f ∈ [0, 1] (Fairness) to [0, 1] (Accuracy) such that

tafH(f ) = max
h∈Hf

Accuracy(f ) where Hf = {h ∈ H : Fairness(h) ≥ f } ⊆ H.

Informally, the taf curve is the curve obtained by constructing a (left) step-function interpolation of
the Pareto optimal members of H and represents the best possible accuracy that can be obtained at
a given level of fairness. Algorithm 1 details how, given a set of candidate models, the set of Pareto
optimal models can be identiﬁed and the taf curve constructed in time quasi-linear in the size of H.

Remark 1. Algorithm 1 identiﬁes the set of all Pareto optimal models, H∗ ⊆ H, and critical points
of the taf curve outlining the fair Pareto frontier of H.

This remark follows directly from the deﬁnitions of Pareto optimality and the taf Curve; further

properties of our taf Curve are provided in the Supplement.

As an illustration, consider Panel A and B in Figure 1 (we plot accuracy and fairness in terms of
percentages instead of in [0, 1]). The taf Curve for a collection of ﬁtted models is a step function
where each critical point is a Pareto optimal model. The left end-point of the taf Curve is given
by the highest accuracy obtained by any model in H extended to Fairness(·) = 0; note that this
left end-point is perhaps not practically relevant but follows from the deﬁnition of taf. The right

end-point of the taf Curve is given by the constant model with fairness equal to one. Hence, our taf

Curve provides a way to determine the Pareto optimal models and Pareto frontier of any arbitrary

6

collection of ﬁtted models; this also provides a visual summary of the fairness-accuracy tradeoﬀ for

particular model classes as well as particular data sets.

Algorithm 1 Compute Pareto Optimal Models and taf Curve Points
Input: Set of candidate models H, including H arbitrary models and one perfectly fair model
Output: Ordered set of Pareto Optimal models H∗ ⊆ H and the step function points for taf

1. Pre-Compute: Stably sort H in descending order of fairness:

Fairness(h(0)) ≥ Fairness(h(1)) ≥ Fairness(h(2)) ≥ · · · ≥ Fairness(h(H))

2. Initialize: H∗ = {h(0)} where h(0) is the perfectly fair model
3. Filter: For i = 1, . . . , H:

• If Accuracy(h(i)) > maxh∈H∗ Accuracy(h):

– If Fairness(h(i)) = Fairness(h∗
– Set H∗ = H∗ ∪ {h(i)}

(|H∗|)), set H∗ = H∗ \ {h∗

(|H∗|)}

4. Return:

• Pareto Optimal Models H∗
• taf Curve Points: {(f, a) : f = Fairness(h∗), a = Accuracy(h∗) for all h∗ ∈ H∗}

3 Quantifying the Frontier: Fairness Area-Under-the-Curve (fauc)

Currently, many compare model families by reporting the fairness and accuracy for a single hyper-

parameter. But given that there is often a tradeoﬀ between fairness and accuracy, these metrics

alone are not suﬃcient to determine the superior model or model family. We thus turn to our taf

Curves and ask, can these be used to compare model families or other collections of models? And,

can we summarize our curves in a single, uniﬁed metric to facilitate comparisons? The idea of

using taf Curves, or the empirical Pareto frontier, naturally arises in many contexts: comparing

model families on the same data set, comparing the level of bias for a ﬁxed set of models in two

data sets, or comparing the fairness of a ﬁxed set of models before and after some intervention.

Ideally, these comparisons are trivial, with one taf Curve dominating the other at all fairness levels,

but in practice taf Curves often cross. To address these diﬃculties, we draw inspiration from

ROC curves used to balance precision and recall in classiﬁcation. Here, people commonly use the

Area-Under-the-ROC-Curve (auc) to compare two or more ROC curves; the auc gives a single

metric summarizing the balance of both precision and recall. Inspired by this, we propose to compute

a simple scalar summary of the taf Curve which we call the Fairness-Area-Under-the-Curve (fauc):

Deﬁnition 3. Given a ﬁnite collection of models H, an accuracy metric Accuracy : H → [0, 1],

a fairness metric Fairness : H → [0, 1], and a non-negative (measurable) weight function w, the
w-weighted fauc score of H is faucw(H) =

tafH(f )w(f ) df

∈ [0, 1].

(cid:16)(cid:82)

(cid:16)(cid:82)

(cid:17)

(cid:17)

/

[0,1] w(f ) df

[0,1]

Just like auc, fauc is a score between zero and one with one indicating perfect accuracy at all

levels of fairness. In practice, note that because the empirical taf curve is piecewise constant,

7

computing the fauc score is a trivial (right-handed) Riemannian sum that can be computed in

linear time.

An illustration of our fauc metric is given in Figure 1B. Notice that since the taf Curve deﬁnitionally

extends the highest attained accuracy to left, this level of accuracy could dominate the fauc score.

Since these associated lower levels of fairness often do not correspond to any model in the collection
H, one may prefer to use a weighted fauc, with non-zero weights for only the levels of fairness of
interest or as needed for the particular context. This motivates us to consider how the ﬂexible weight

function could be used to capture particular fairness preferences:

Theorem 1. Suppose there exists a preference relation, (cid:23), on the set of taf curves which is total,
transitive, and continuous. Suppose further that the preference relation (cid:23) is increasing in taf,
that is, if tafG ≥ tafH pointwise then tafG (cid:23) tafH, that the utility of accuracy is preferentially
independent at any ﬁnite collection of fairness levels, and that the utility of constant taf curves
is linear in the accuracy. Then, there exists a non-negative generalized function w such that
tafG (cid:23) tafH ⇔ faucw

H for all model sets G, H.

G ≥ faucw

This result utilizes economic preference theory (see [33, 12]) and uses standard assumptions in this

ﬁeld; the proof and a detailed discussion of the assumptions may be found in the Supplement.

The implications of Theorem 1 are profound: no matter how an individual would choose to trade-oﬀ

accuracy and fairness, there is a fauc variant that completely characterizes their views. For example,

the preferences of an individual who cares only about accuracy and not at all about fairness are
encoded by a point w = δ0 where δ0 is Dirac’s delta and faucδ0(H) = tafH(0) = maxh∈H Accuracy(h).
We expect that, for most individuals, a weight function of the form w(x) = xα1x>β for some
α ≥ 1, β ∈ [0, 1] captures their preferences well, placing a premium on the accuracy of models that
are (nearly-perfectly) fair and forcing adherence to some regulatory lower-bound on fairness, but

we leave the question of precise utility elicitation to future work. Overall, fauc and its weighted

variants provide an intuitive and practical uniﬁed metric that summarizes the whole fairness-accuracy

tradeoﬀ.

3.1

Improving the fauc Score via Randomized Interpolation: tafi and fauci

Randomizing aspects of model behavior has recently been shown to be an eﬀective approach to

improve fairness in ML systems [47] and the taf/fauc framework can also beneﬁt from randomization.
For any two models g, h, we interpret the composite model αg + (1 − α)h for ﬁxed α ∈ [0, 1] as the
randomized procedure which applies g with probability α and h otherwise. When g, h are Pareto
optimal in H, this procedure produces models which are not Pareto dominated by any element
of H. If these randomized combinations are added to H, the resulting taf curve will consist of
the Pareto optimal elements of H linearly interpolated as shown in Figure 1B: for this reason we
refer to the resulting curve as the taf + Interpolation curve, or more simply, tafi, and the area
under it as fauci. These composite models can be considered as the convex hull of H and there is a

8

connection between this convex hull of randomized procedures and the convex geometry of the taf

curve:

Theorem 2. For a ﬁxed collection of models H, tafiH = conv(tafH) = tafconv(H). Hence, we
have tafH ≤ tafiH pointwise and, by extension, faucw

H for any weight function w.

H ≤ fauciw

Here, conv(tafH) refers to the (geometric) convex hull of the taf Curve, A denotes the upper
boundary of a set A, and tafconv(H) refers to taf applied to the set of randomized procedures.
While randomization is guaranteed to improve performance as measured by fauc, we do note

that randomization is sometimes at odds with transparency and auditability requirements, so this

strategy is not universally applicable. Taking inspiration from Theorem 2 and the power of model

combinations to improve fauc, we next ask if we can optimize linear combinations of models to

expand the fair Pareto frontier.

4 Optimizing the Frontier: Fair Model Stacking (FairStacks)

We have developed the taf +fauc framework for identifying and quantifying the empirical Pareto

frontier attained by a collection of ﬁtted models, but we ask: Is it possible to expand the frontier

for these same set of models, perhaps using some meta-learner? For this, we consider a meta-

learner which uses linear combinations of models in a stacked ensemble learning framework. Taking

inspiration from common multi-objective optimization used to learn the Pareto frontier as well as

recent work of this nature in the context of fairness [32], we propose to study the following problem:

arg max
w∈Rk

Accuracy

(cid:33)

wihi(x); y

such that

(cid:32) k

(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Bias

(cid:32) k

(cid:88)

i=1

wihi(x); y

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ τ

(1)

i=1

are the set of learned input models, τ ∈ [0, 1] is a hyperparameter that controls the
Here, {hi(·)}k
level of bias, and Bias(·) = 1 − Fairness(·). For every ﬁxed value of τ , this problem achieves the the
highest possible accuracy among linear combinations of learners h ∈ H; as one varies τ , this problem
parameterizes the taf Curve and the fairness Pareto frontier.

In general, solutions to (1) are not easy to directly compute, as both the Accuracy and Bias functions
may be non-convex in the ensemble weights w. The use of convex surrogates for 0/1 classiﬁcation
accuracy is well-established [2] and we take a similar approach for measures of group fairness. As

Lohaus et al. [28] note, ﬁnding a convex surrogate for fairness is signiﬁcantly more diﬃcult than

for accuracy because fairness depends on both the continuous score output by the model and the
downstream decision function used to map that score onto {0, 1} labels. Notably, this downstream
decision function is often chosen independently of the stacking procedure and may be discontinuous.
To sidestep these diﬃculties, we consider convex relaxations of score-fairness rather than attempting
to constrain the decision fairness directly. As we will show, this transformation has suﬃcient
ﬁdelity to allow us to fully explore the Pareto frontier of (1) while still preserving the advantages of

convexity.

9

We develop a convex notion of score-fairness for two popular group-based fairness deﬁnitions,

Demographic Parity [15, 17] and Equality of Odds [20], and expect our approach to extend to other

deﬁnitions of group fairness as well. To accomplish this, we consider a unifying framework for group

fairness based on “contrast groups,” wherein the diﬀerence in average outcomes of two groups reﬂects

a (potential) bias to be mitigated:

Deﬁnition 4. The score bias of a prediction system ˆh : Rp → [0, 1] with respect to groups G1, G2 is
(cid:103)Bias(ˆh) = E[ˆh(xi)|i ∈ G1] − E[ˆh(xi)|i ∈ G2] where expectations are taken with respect to the empirical
measure of the data. When Gi = {j : Zj = i}, we refer to the score bias as the score deviation from
demographic parity and when Gi = {j : Zj = i, Yj = 1}, we refer to the score bias as the score
deviation from equality of opportunity for some protected attribute Z and true label Y , both by
analogy with their decision (binary label) counterparts.

Due to their linear formulation, score bias measures are particularly well-suited for use in a stacking
problem. Speciﬁcally, if ˆH(·) = (cid:80) wihi(·) for some ﬁxed base learners {hi}, then we have (cid:103)Bias( ˆH) =
(cid:80) wi(cid:103)Bias(ˆh) and, more generally, |(cid:103)Bias( ˆH)| ≤ (cid:80) |wi||(cid:103)Bias(ˆh)|; hence, a weighted ensemble always has
less score bias than its component parts, implying that model ensembles can improve fairness, in

addition to their well-known improvements in accuracy.

Deﬁnition 5. FairStacks, FS (cid:0){hi(·)}k

i=1

(cid:1), is deﬁned as the solution to the following problem:

arg min
w∈Rk

n
(cid:88)

L

(cid:32) k

(cid:88)

j=1

i=1

(cid:33)

wihi(xj); yj

such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
wi(cid:103)Bias(hi)
(cid:12)
(cid:12)

≤ τ

(2)

Notice that (2) is convex for any convex loss function L, including squared error, binomial deviance,
hinge loss, etc. and has only linear constraints, allowing it to be easily solved at scale. Additionally,
if appropriate, L can also include additional regularization terms not related to fairness such as a
ridge penalty which may be used to reduce overﬁtting when the set of base learners is large. Note
that in Problem 2, the range of meaningful values of τ can, and often does, extend beyond the [0, 1]
interval.

In light of the recent ﬁndings of [28] which outline some challenges with convex relaxations of fairness,

it is reasonable to ask: Under what conditions does the FairStacks problem explore the fair Pareto

frontier? While [28] consider pointwise diﬀerences between fairness deﬁnitions and their convex

relaxations, we do not require precise pointwise bounds to fully explore the Pareto frontier. Instead
since we are trying to learn the whole taf curve at all levels of τ , all we require is that there is a
monotonic relationship between score fairness and decision fairness. This is suﬃcient to guarantee

that our approach fully explores the Pareto frontier. The following theorem establishes general

conditions under which this monotonic relationship holds:

Theorem 3. Suppose the predictions of the FairStacks problem are used to generate predicted
labels via a (potentially randomized) decision function δ(η, Z) such that ∆(η) = EZ[δ(η, Z)] is

10

thrice-diﬀerentiable, has second derivative bounded away from zero and inﬁnity, and is monotonically
increasing in η = η(x) = (cid:80)k
i=1 wihi(x). Then, to a second-order approximation and in expectation,
the decision bias with respect to the same groups as (cid:103)Bias is monotonically increasing in τ if

wT
wT

τ Covi∼G1[H(x)]wτ
τ Covi∼G2[H(x)]wτ

→

∆(cid:48)(cid:48)(Ei∼G2[wT
∆(cid:48)(cid:48)(Ei∼G1[wT

τ xi)])
τ xi])

monotonically in τ , where wτ is the solution to the FairStacks problem at constraint level τ ,
H(x) = (h1(x), h2(x), . . . , hp(x)) is the vector of base model predictions at x, and G1, G2 are the
groups used to evaluate both the score bias, (cid:103)Bias, and the decision bias, Bias.

The conditions of Theorem 3 are somewhat diﬃcult to interpret, but a simple suﬃcient condition

is that the covariance of the base learners is equal between the two groups. This assumption is

reasonable in the stacking context where the base learners are learned on the same training set and

can reasonably be expected to have consistent correlation properties across both groups. Theorem

3 is similar to Theorem 1 of [28], but gives monotonicity instead of continuity; conversely, our

result holds only up to a stochastic Taylor approximation and can be weakly violated in ﬁnite

samples, though we have not observed violations outside of intentionally designed counterexamples.

As an empirical example, in Figure 1D, score and decision fairness have a monotonic relationship.

Practically, Theorem 3 ensures that we can explore the entire taf/fauc space attainable from
stacking members of a given class H by solving the FairStacks problem at a ﬁne grid of τ . We hence
avoid the diﬃculties of quantifying how accurate a particular convex relaxation is at a particular
penalty τ by instead considering and making comparisons based on the entire solution set and
curve.

Next, we seek to verify whether FairStacks achieves our stated objective of expanding the Pareto
frontier of the given collection of models, H. As an empirical illustration, Figure 1C shows that
FairStacks leads to a greatly expanded Pareto frontier compared to the frontier (taf Curve)

attained by ensemble members. In fact, the ensemble produced by FairStacks will always exceed

the fauc of the non-stacked ensemble members, as the following proposition notes:

Proposition 1. Given a model class H, let FS(H) be the set of models obtained by solving the
FairStacks problem (2) at all values of τ . Then tafH ≤ tafFS(H) pointwise and, by extension,
faucw(H) ≤ faucw(FS(H)) for any weight function w where the Fairness used to construct taf is
the same score-fairness used in the FairStacks problem. Additionally, under the conditions of
Theorem 3, the same inequalities hold for taf and fauc based on corresponding decision-fairness.

Putting together Proposition 1 and Theorem 2, it is easy to see that increasing the number of
the models in H will always give a FairStacks solution with larger fauc. This motivates us to
consider ever larger collections and varieties of models as inputs to the meta-learner, FairStacks,

as we explore in the next section. Finally, though not the focus of this paper, we also present

theoretical results on the out-of-sample behavior of FairStacks in the Supplement, showing that it

obtains the same attractive exponential concentration properties out-of-sample as other statistical

11

ML techniques.

Figure 2: taf curves from Section 5: Demographic Parity with sensitive attribute noted in panel
title. The FairStacks kitchen-sink (red) outperforms competing approaches, obtaining the highest
taf.

5 Empirical Studies

In this section, we demonstrate the eﬃcacy of the taf/fauc framework for model comparison

and the FairStacks meta-learner for constructing fair and accurate ensembles. Our results

can be found in Table 1 and Figure 2 and clearly demonstrate the eﬀectiveness of the proposed

methodologies. We brieﬂy describe our experiments below, with additional details and experiments

in the Supplement.

Base-Learners. We compare several fair machine learning methodologies and ensemble families.
We implement three fair classiﬁer methods [44, 19, 1] which have explicit fairness-accuracy tradeoﬀ

parameters, depicting them with dashed lines in Figure 2. We also use the FairStacks meta-learner

to construct several weighted ensembles, shown as solid lines in Figure 2: i) random forest trees,
i.e., the elements of a random forest ensemble; ii) an ensemble of 1000 minipatch decision trees,
i.e. decision trees trained on small sub-samples of features and observations [42, 38]; iii) learners
implemented in the Scikit-Learn and XGBoost packages [35, 7]; and iv) a kitchen-sink ensemble
consisting of both the stand-alone methods and the previous ensembles.

12

Benchmark Datasets. We test our methods on ﬁve standard benchmark datasets: i) the Adult
Income dataset [14], binary classiﬁcation with 48,000 observations, 14 features, and 2 binary protected
attributes (race and gender); ii) the Bank dataset [14], binary classiﬁcation with 45,000 observations,
16 features, and one binary protected attribute (age); iii) the COMPAS dataset [27], binary classiﬁca-
tion with 7,000 observations, 13 features, and 2 binary protected attributes (race and gender); iv)
the Default dataset [14], binary classiﬁcation with 30,000 observations, 23 features, and one binary
protected attribute (gender); and the Communities and Crime (C and C) dataset [14], a regression
task with 100 features, 2,000 observations, and one binary protected attribute (race).

Experimental Setup. For each experiment, we split data into a 50% training/25% ensemble
learning/25% test split, and calculate taf and 80% step-weighted fauc scores, using Demographic
Parity [15, 17] for the fairness metric, as deﬁned above. The threshold of FairnessDP < 80% reﬂects
the commonly cited disparate impact threshold used by the U.S. Equal Employment Opportunity

Commision (EEOC); this represents the simplest legal standard for statistical discrimination used

in the U.S. and provides an impartial basis upon which to compare models. Brier scoring [18],

equivalent to mean squared error, was used as the FairStacks loss function to measure probabilistic
calibration of FairStacks predictions; a simple threshold rule at p = 0.5 was used for fairness
assessment. FairStacks ensembles are ﬁt with a ridge ((cid:96)2) penalty, tuned via 5-fold cross validation
within the 25% split, to avoid overﬁtting. For both base learners and FairStacks ensembles, a

grid of 20 values of the tuning parameter was used. Numerical results in Table 1 were obtained

by averaging results over 10 independent data splits; quantities in parentheses are fauc standard

errors.

Summary of Results. Figure 2 shows taf curves for the base learners and ensembles described
above on four classiﬁcation problems. While some fair learners, particularly that of Zhang et al. [44],

are able to obtain high accuracy, our FairStacks framework consistently achieves a higher level of

accuracy across all fairness levels. The beneﬁts of FairStacks are particularly pronounced for larger
ensembles, with the best performance being achieved by the kitchen-sink ensemble on all tasks. Table
1 reports associated fauc scores, highlighting that the high-accuracy and ﬂexible-fairness of the

FairStacks ensembles results in the best fairness-accuracy tradeoﬀ as measured by fauc. Additional

results in the Supplemental Materials visualize taf curves for the data sets not shown in Figure 2,

demonstrate the use of Equality of Opportunity [20], and illustrate an extension of FairStacks to

multiple protected attributes, as well as providing further details of our experiments.

6 Discussion

Impact. We have developed a framework for identifying, quantifying, and optimizing or expanding
the empirical fairness-accuracy Pareto frontier for a collection of ﬁtted models. Our taf Curves

and Fairness AUC (fauc) provide the ﬁrst general, model-agnostic metric that characterize the

fairness-accuracy tradeoﬀ. Just as ROC Curves and the associated auc oﬀer impartial ways to

compare classiﬁers that balance precision and recall, our taf Curves and fauc oﬀer impartial

13

Table 1: Quantative Results for Section 5: Demographic Parity + 80% step-weighted fauc. Methods
in [44, 19, 1] are speciﬁc to classiﬁcation and cannot be applied to the C and C regression task.

Method

Adult

Gender

Race

Bank
Age

COMPAS

Gender

Race

Default
Gender

C and C
Race

Random Forest
Zhang et al. [44]
Grari et al. [19]
Agarwal et al. [1]
FS MP Trees
FS RF Trees
FS Classiﬁers

.623(.014)
–
–
–
.714(.004)
.695(.003)
.689(.002)
FS Kitchen Sink .866(.001) .899(.001) .947(.002) .633(.003) .796(.002) .933(.001) .734(.002)

.524 (.003)
.629(.018)
.641(.009)
.622(.002)
.699(.005)
.773(.004)
.735(.005)

.772(.001)
.836(.020)
.830(.004)
.797(.003)
.851(.001)
.843(.001)
.823(.002)

.557(.004)
.621(.064)
.590(.009)
.598(.004)
.621(.005)
.607(.006)
.609(.004)

.920(.002)
.883(.005)
.877(.013)
.898(.023)
.920(.002)
.942(.006)
.915(.004)

.848(.001)
.778(.002)
.850(.003)
.778(.014)
.856(.002)
.921(.002)
.863(.002)

.830(.001)
.845(.005)
.834(.002)
.841(.010)
.845(.001)
.851(.002)
.816(.001)

ways to compare model families that balance fairness and accuracy. As such, our taf Curves and

fauc provide simple, intuitive, and useful metrics that can be used to impartially compare model

families for machine learning fairness problems. This is an important contribution that ﬁlls a gap

in the existing algorithmic fairness literature, providing an empirical and model-agnostic way to

measure the fairness-accuracy Pareto-frontier. Additionally, our fair model stacking framework,

FairStacks oﬀers a simple post-processing strategy that learns an optimal weighted combination of

ﬁtted models subject to a constraint on bias. By varying levels of the hyperparameter, this approach

outlines a curve that we show expands the empirical Pareto frontier. We also empirically show

that stacking many diverse models in FairStacks leads to major expansions of the frontier that

dominate competing methods. Thus, FairStacks can be viewed as a meta-learner that can be used

to improve both the accuracy and fairness of any other learners; this is then a simple, but powerful

tool in the machine learning arsenal to improve fairness. Overall, our work provides a major social

impact by helping to quantify and impartially compare algorithmic fairness approaches as well as

providing a useful approach to post-process and stack models that improves accuracy at every level

of fairness.

Limitations & Future Work. The taf/fauc framework is based on existing univariate, mean-
based measures of fairness and accuracy and inherits the limitations of those measures. Fundamental

questions surrounding multivalent or multiple protected attributes, intersectionality, and appropriate

metrics for individual fairness are active areas of research and discussion, both in machine learning

and in society more broadly; as these questions are answered, the taf framework may require

extension to novel, more subtle notions of fairness. Similarly, for simple classiﬁcation problems,
[0, 1]-measures of accuracy are natural, but for more complicated tasks, e.g. those arising in ranking
or computer vision, appropriate measures of accuracy are less obvious and may require alterations

to our framework. In practice, fairness and accuracy must be estimated from data [30], but we have

not discussed the statistical properties of the taf/fauc framework that are key to rigorous model

comparisons. Theorem 1 demonstrates that a fauc weight scheme exists for all user preferences,

but does not give guidance on what weight function should be used: this decision is nuanced and

14

likely requires consensus among a wide range of stakeholders, as well as developments in decision

theory necessary to elicit and construct this consensus. A weight scheme chosen without broad input

may overweight the preferences of decision makers already in power. Finally, FairStacks uses a

linear combination of models under a score-based bias constraint speciﬁc to binary groups; while

we have provided suﬃcient conditions under which this leads to improvements in fauc, there may

be situations where tighter, but potentially non-convex, formulations perform better over a wider

range of scenarios. As this brief discussion suggests, the taf/fauc framework is fertile ground for

development of a fuller theory of fairness-accuracy tradeoﬀs.

Acknowledgements

COL acknowledges support from the NSF Graduate Research Fellowship Program under grant number

1842494. MW’s research is supported by an appointment to the Intelligence Community Postdoctoral

Research Fellowship Program at the University of Florida Informatics Institute, administered by

Oak Ridge Institute for Science and Education through an interagency agreement between the U.S.

Department of Energy and the Oﬃce of the Director of National Intelligence. GIA acknowledges

support from JP Morgan Faculty Research Awards and NSF DMS-1554821.

References

[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. “A
Reductions Approach to Fair Classiﬁcation”. ICML 2018: Proceedings of the 35th International
Conference on Machine Learning. Vol. 80. 2018, pp. 60–69. url: https://proceedings.mlr.
press/v80/agarwal18a.html.

[2] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliﬀe. “Convexity, Classiﬁcation, and
Risk Bounds”. Journal of the American Statistical Association 101 (2006), pp. 138–156. doi:
10.1198/016214505000000907.

[3] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael J. Kearns, Jamie
Morgenstern, Seth Neel, and Aaron Roth. “A Convex Framework for Fair Regression”. ArXiv
Pre-Print 1706.02409 (2017). doi: 10.48550/arXiv.1706.02409.

[4] Flavio P Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R Varshney. “Optimized pre-processing for discrimination prevention”. NeurIPS 2017:
Advances in Neural Information Processing Systems. Vol. 30. 2017, pp. 3995–4004. doi: https:
/ / proceedings . neurips . cc / paper / 2017 / hash / 9a49a25d845a483fae4be7e341368e36 -
Abstract.html.

[5] L. Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K. Vishnoi. “Classiﬁcation with
Fairness Constraints: A Meta-Algorithm with Provable Guarantees”. FAT* 2019: Proceedings of
the Conference on Fairness, Accountability, and Transparency. New York, NY, USA: Association
for Computing Machinery, 2019, pp. 319–328. isbn: 9781450361255. doi: 10.1145/3287560.
3287586.

15

[6]

Irene Chen, Fredrik D Johansson, and David Sontag. “Why Is My Classiﬁer Discriminatory?”
NeurIPS 2018: Advances in Neural Information Processing Systems. Vol. 31. 2018. url: https:
/ / proceedings . neurips . cc / paper / 2018 / hash / 1f1baa5b8edac74eb4eaa329f14a0361 -
Abstract.html.

[7] Tianqi Chen and Carlos Guestrin. “XGBoost: A scalable tree boosting system”. KDD ’16:
Proceedings of the 22nd ACM/SIGKDD International Conference on Knowledge Discovery and
Data Mining. 2016, pp. 785–794. doi: 10.1145/2939672.2939785.

[8] Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. “Fair Generative
Modeling via Weak Supervision”. ICML 2020: Proceedings of the 37th International Conference
on Machine Learning. 2020, pp. 1887–1898. url: https://proceedings.mlr.press/v119/
choi20a.html.

[9] Alexandra Chouldechova and Aaron Roth. “The Frontiers of Fairness in Machine Learning”.

ArXiv Pre-Print 1810.08810 (2018). doi: 10.48550/arXiv.1810.08810.

[10] Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil.
“Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classiﬁcation”. NeurIPS
2019: Advances in Neural Information Processing Systems. 2019. url: https://papers.nips.
cc/paper/2019/hash/ba51e6158bcaf80fd0d834950251e693-Abstract.html.

[11] Jeﬀrey Dastin. “Amazon scraps secret AI recruiting tool that showed bias against women”.
Thomson Reuters (2018). url: https://www.reuters.com/article/us-amazon-com-jobs-
automation-insight.

[12] Gerard Debreu. Mathematical Economics: Twenty Papers of Gerard Debreu. Ed. by Werner
Hildenbrand. Econometric Society Monographs. Cambridge University Press, 1983. isbn:
978-0-521-23736-9. doi: 10.1017/CCOL052123736X.

[13] Thomas G. Dietterich. “Ensemble Methods in Machine Learning”. MCS 2000: Proceedings of
the 1st International Workshops on Multiple Classiﬁer Systems. Lecture Notes in Computer
Science. 2000, pp. 1–15. doi: 10.1007/3-540-45014-9_1.

[14] Dheeru Dua and Casey Graﬀ. UCI Machine Learning Repository. 2017. url: http://archive.

ics.uci.edu/ml.

[15] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. “Fairness
through Awareness”. ITCS ’12: Proceedings of the 3rd Innovations in Theoretical Computer
Science Conference. 2012, pp. 214–226. doi: 10.1145/2090236.2090255.

[16] Saso Džeroski and Bernard Ženko. “Is Combining Classiﬁers with Stacking Better than Selecting
the Best One?” Machine Learning (2004). doi: 10.1023/B:MACH.0000015881.36452.6e.
[17] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkata-
subramanian. “Certifying and Removing Disparate Impact”. KDD 2015: Proceedings of the
21th ACM/SIGKDD International Conference on Knowledge Discovery and Data Mining. 2015,
pp. 259–268. doi: 10.1145/2783258.2783311.

16

[18] Tilmann Gneiting and Adrian E. Raftery. “Strictly Proper Scoring Rules, Prediction, and
Estimation”. Journal of the American Statistical Association 102.477 (2007), pp. 359–378. doi:
10.1198/016214506000001437.

[19] Vincent Grari, Boris Ruf, Sylvain Lamprier, and Marcin Detyniecki. “Fair Adversarial Gradient
Tree Boosting”. ICDM 2019: Proceedings of the 2019 IEEE International Conference on Data
Mining. 2019, pp. 1060–1065. doi: 10.1109/ICDM.2019.00124.

[20] Moritz Hardt, Eric Price, and Nathan Srebron. “Equality of Opportunity in Supervised
Learning”. NeurIPS 2016: Advances in Neural Information Processing Systems 29. Vol. 29. 2016.
url: https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-
Abstract.html.

[21] Heinrich Jiang and Oﬁr Nachum. “Identifying and Correcting Label Bias in Machine Learning”.
AISTATS 2020: Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and
Statistics. 2020, pp. 702–712. url: https://proceedings.mlr.press/v108/jiang20a.html.
[22] Surya Mattu Julia Angwin Jeﬀ Larson and Lauren Kirchner. “There’s software used across the
country to predict future criminals. And it’s biased against blacks.” ProPublica (2016). url:
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-
sentencing.

[23] F. Kamiran and T. Calders. “Classifying without discriminating”. IC4 2009: Proceedings of the
2nd International Conference on Computer, Control and Communication. 2009, pp. 1–6. doi:
10.1109/IC4.2009.4909197.

[24] Faisal Kamiran and Toon Calders. “Data preprocessing techniques for classiﬁcation without
discrimination”. Knowledge and Information Systems 33 (2012), pp. 1–33. doi: 10.1007/
s10115-011-0463-8.

[25] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. “Decision Theory for Discrimination-
Aware Classiﬁcation”. ICDM 2012: Proceedings of the IEEE 12th International Conference on
Data Mining. 2012, pp. 924–929. doi: 10.1109/ICDM.2012.45.

[26] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. “Fairness-Aware Classiﬁer
with Prejudice Remover Regularizer”. ECML PKDD 2012: Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. 2012, pp. 35–50. doi: 10.1007/978-
3-642-33486-3_3.

[27] Jeﬀ Larson, Marjorie Roswell, and Vaggelis Atlidakis. COMPAS Recidivism Risk Score Data

and Analysis. 2022. url: https://github.com/propublica/compas-analysis/.

[28] Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. “Too Relaxed to Be Fair”. ICML
2020: Proceedings of the 37th International Conference on Machine Learning. Ed. by Hal
Daumé III and Aarti Singh. Vol. 119. Virtual: PMLR, 2020, pp. 6360–6369. url: https:
//proceedings.mlr.press/v119/lohaus20a.html.

[29] Pranay K. Lohia, Karthikeyan Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush

R. Varshney, and Ruchir Puri. “Bias Mitigation Post-processing for Individual and Group

17

Fairness”. ICASSP 2019: Proceedings of the 2019 IEEE International Conference on Acoustics,
Speech and Signal Processing. 2019, pp. 2847–2851. doi: 10.1109/ICASSP.2019.8682620.
[30] Kristian Lum, Yunfeng Zhang, and Amanda Bower. “De-biasing ‘Bias’ Measurement”. FAccT
’22: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.
2022. doi: 10.48550/arXiv.2205.05770.

[31] Natalia Martinez, Martin Bertran, and Guillermo Sapiro. “Fairness With Minimal Harm:
A Pareto-Optimal Approach For Healthcare”. ArXiv Pre-Print 1911.06935 (2019). doi: 10.
48550/arXiv.1911.06935.

[32] Natalia Martinez, Martin Bertran, and Guillermo Sapiro. “Minimax Pareto Fairness: A Multi
Objective Perspective”. ICML 2020: Proceedings of the 37th International Conference on
Machine Learning. 2020, pp. 6755–6764. url: https : / / proceedings . mlr . press / v119 /
martinez20a.html.

[33] Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory. Oxford

University Press, 1995. isbn: 978-0-195-07340-9.

[34] Aditya Krishna Menon and Robert C Williamson. “The cost of fairness in binary classiﬁcation”.
Proceedings of the 1st Conference on Fairness, Accountability and Transparency. 2018, pp. 107–
118. url: https://proceedings.mlr.press/v81/menon18a.html.

[35] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,

Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vander-

plas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard
Duchesnay. “Scikit-learn: Machine learning in Python”. Journal of Machine Learning Research
12.Oct (2011), pp. 2825–2830. url: https://www.jmlr.org/papers/v12/pedregosa11a.
html.

[36] Geoﬀ Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q. Weinberger. “On
Fairness and Calibration”. NeurIPS 2017: Advances in Neural Information Processing Systems.
Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett. Vol. 30. 2017. url: https://proceedings.neurips.cc/paper/2017/hash/
b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html.

[37] Lincoln Quillian, John J. Lee, and Honoré Brandon. “Racial Discrimination in the U.S. Housing
and Mortgage Lending Markets: A Quantitative Review of Trends, 1976–2016”. Race and Social
Problems 12.1 (2020), pp. 13–28. doi: 10.1007/s12552-019-09276-x.

[38] Mohammad Taha Toghani and Genevera I. Allen. “MP-Boost: Minipatch Boosting via Adaptive
Feature and Observation Sampling”. BigComp 2021: Proceedings of the 2021 IEEE International
Conference on Big Data and Smart Computing. Jeju Island, South Korea, 2021, pp. 75–78.
doi: 10.1109/BigComp51126.2021.00023.

[39] Sahil Verma and Julia Rubin. “Fairness Deﬁnitions Explained”. FairWare 2018: Proceedings
of the 2018 IEEE/ACM International Workshop on Software Fairness. 2018, pp. 1–7. doi:
10.23919/FAIRWARE.2018.8452913.

18

[40] David H Wolpert. “Stacked generalization”. Neural Networks (1992), pp. 241–259. doi: 10.

1016/S0893-6080(05)80023-1.

[41] Yongkai Wu, Lu Zhang, and Xintao Wu. “On Convexity and Bounds of Fairness-Aware Classiﬁca-
tion”. WWW 2019: The World Wide Web Conference. New York, NY, USA: Association for Com-
puting Machinery, 2019, pp. 3356–3362. isbn: 9781450366748. doi: 10.1145/3308558.3313723.
[42] Tianyi Yao and Genevera I. Allen. “Feature Selection for Huge Data via Minipatch Learning”.

ArXiv Pre-Print 2010.08529 (2020). doi: 10.48550/arXiv.2010.08529.

[43] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P. Gummadi.
“Fairness Constraints: Mechanisms for Fair Classiﬁcation”. AISTATS 2017: Proceedings of the
20th International Conference on Artiﬁcial Intelligence and Statistics. 2017, pp. 962–970. url:
https://proceedings.mlr.press/v54/zafar17a.html.

[44] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. “Mitigating Unwanted Biases with
Adversarial Learning”. AIES 2018: Proceedings of the 2018 AAAI/ACM Conference on AI,
Ethics, and Society. 2018, pp. 335–340. doi: 10.1145/3278721.3278779.

[45] Han Zhao and Geoﬀ Gordon. “Inherent Tradeoﬀs in Learning Fair Representations”. NeurIPS
2019: Advances in Neural Information Processing Systems. Vol. 32. 2019, pp. 15675–15685. url:
https://proceedings.neurips.cc/paper/2019/hash/b4189d9de0fb2b9cce090bd1a15e3420-
Abstract.html.

[46] Han Zhao and Geoﬀrey J. Gordon. “Inherent Tradeoﬀs in Learning Fair Representations”.
Journal of Machine Learning Research 23.57 (2022), pp. 1–26. url: https://www.jmlr.org/
papers/v23/21-1427.html.

[47] Shengjia Zhao, Tengyu Ma, and Stefano Ermon. “Individual Calibration with Randomized
Forecasting”. ICML 2020: Proceedings of the 37th International Conference on Machine Learning.
2020, pp. 11387–11397. url: http://proceedings.mlr.press/v119/zhao20e.html.
Indre Zliobaite. “On the relation between accuracy and fairness in binary classiﬁcation”. ArXiv
Pre-Print 1505.05723 (2015). Presented at the 2nd Workshop on Fairness, Accountability, and
Transparency in Machine Learning. doi: 10.48550/arXiv.1505.05723.

[48]

19

Supplementary Materials

A Proofs

A.1 Proofs for Section 2: Identifying the Frontier: Tradeoﬀ-between-Fairness-

Accuracy Curve

The following proposition gives several properties of the taf curve that follow immediately from the

deﬁnition, but that we state here as they are useful for our subsequent discussions:

Proposition 2. taf curves are: i) closed, proper, almost-everywhere continuous, almost-everywhere
diﬀerentiable, and integrable; ii) monotonically decreasing in f ; and iii) monotonically pointwise
increasing in H, i.e., if H ⊆ H(cid:48) then tafH(f ) ≤ tafH(cid:48)(f ) for all f ∈ [0, 1].

Proof. These claims follow almost immediately from the deﬁnition of the taf curve:

ii) Let 0 ≤ f < f (cid:48) ≤ 1. By deﬁnition, we have Hf ⊆ Hf (cid:48) where Hf = {h ∈ H : Fairness(h) ≥ f }.
Because tafH(f ) is maximizing accuracy over nested sets, we have maxh∈Hf Accuracy(f ) ≥
maxh∈Hf (cid:48) Accuracy(f ) and hence taf(f ) ≥ taf(f (cid:48)) as desired.

iii) Note that if H ⊆ H(cid:48), we have Hf ⊆ H(cid:48)
f

for all f , with Hf deﬁned as above. As before,
maximization over a superset guarantees a greater maximum, so tafH (f ) ≤ tafH (cid:48)(f ) for all
f as desired.

i) These standard properties follow from the monotonicity of taf and the fact that both the

domain and range spaces are compact.

A.2 Proofs for Section 3: Quantifying the Frontier: Fairness Area-Under-the-

Curve (fauc)

Theorem 1. Suppose there exists a preference relation, (cid:23), on the set of taf curves which is total,
transitive, and continuous. Suppose further that the preference relation (cid:23) is increasing in taf,
that is, if tafG ≥ tafH pointwise then tafG (cid:23) tafH, that the utility of accuracy is preferentially
independent at any ﬁnite collection of fairness levels, and that the utility of constant taf curves
is linear in the accuracy. Then, there exists a non-negative generalized function w such that
tafG (cid:23) tafH ⇔ faucw

H for all model sets G, H.

G ≥ faucw

Note that the topological restrictions on the preference relation (cid:23) are weak and standard in economic
theory [17, 6]. In essence, they require that it is possible to compare taf curves, that the implied
preferences are not inconsistent, and that the (cid:23) relation is well behaved with respect to sequences of
limits. The assumption that (cid:23) is pointwise increasing essentially encodes that higher accuracy is
preferred to lower accuracy, ceteris paribus, at all points on the taf curve, while the assumption
of preferential independence implies that the utility of increasing accuracy at fairness f does not
depend on accuracy at a diﬀerent fairness level f (cid:48). The assumption that constant taf curves have

20

a linear utility function, i.e. that utility of a single model is linear in its accuracy, can always be
ensured by a monotonic transformation the accuracy measure used.

Proof. The proof of this theorem proceeds in three parts:

1. There exists a continuous utility function v encoding the preference relationship (cid:23)

2. The utility function v is linear

3. The linear utility function v can be written as a weighted sum of points on the taf curve

Part I: Existence of Utility. We begin by noting that L2([0, 1]) is a compact and separable topological
space by standard analytic results and that the set of taf curves can be endowed with the same
topology: speciﬁcally, compactness follows from compactness of [0, 1] and the fact that the space
of almost-everywhere continuous functions on a compact space is itself compact, while separability

follows form standard results building on the denseness of Q in R and using functions deﬁned on Q

to approximate those on R. From here, we invoke Theorem 1 of Mehta [18], noting that the both
the closure and second-countability requirements of that result are satisﬁed by L2([0, 1]) and hence
by the set of taf curves which have the additional properties of almost-everywhere continuity and
montonicity (cf., Proposition 2.) Mehta’s result thus gives us a utility function v which is continuous
with respect to taf curves. We note also that the results of Peleg [20] could also be used here, under

slightly diﬀerent topological assumptions that are essentially equivalent for our purposes.

Part II: Linearity of Utility. Having established existence of the utility function v, we now argue
that it can be written as a linear function of the taf curve. Our argument essentially follows that

of Fuhrken and Richter [9] and we defer discussion of technical details to their paper.1 Essentially,

we note that the existence of an additive utility function on any ﬁnite set of taf points follows

from classical results of Debreu [6]. A limiting argument extends this to taf curves deﬁned on the

countable set Q and the denseness of Q in R and the almost-everywhere continuity of taf curves

suﬃces to extend to our scenario. Hence, we have

v(taf) =

(cid:90)

[0,1]

w(f )˜v(taf(f )) df

for some non-negative generalized function w and some pointwise utility function ˜v, encoding the
agent’s utility of accuracy, independent of fairness, the existence of which is implied by the assumption
of pointwise increase on (cid:23).

Part III: Expressing Utility in terms of fauc. Finally, we note that ˜v(·) can be removed by monotonic
transformation ˜v(cid:48) such that ˜v(cid:48)(˜v(x)) = x for all x ∈ [0, 1]. Because ˜v(cid:48) is monotonic, it does not

1A similar analysis also appears in the study of rational decision making in continuous-time models: see, e.g., the

papers by Harvey and Østerdal [13] and by Hara [11].

21

change the preference relation and hence

v(cid:48)(taf) =

(cid:90)

[0,1]

w(f )˜v(cid:48)(˜v(taf(f ))) df =

(cid:90)

[0,1]

w(f )taf(f ) df ∝ faucw

encodes the same preference relation as (cid:23). After linear rescaling, this gives the desired connection
between (cid:23) and faucw.

Note that in Part III of the proof, we used the fact that accuracy is the same quantity at all points

on the taf curve, so we could use Theorem 4 of Fuhrken and Richter [9] which essentially relates

our problem to that of the utility of an inﬁnite stream. The assumption about utility of constant
curves ensures the existence of ˜v(cid:48). If accuracy at various points was not comparable, we would have
applied Theorem 5 of Fuhrken and Richter [9] and not been able to have a single (eliminable) ˜v(cid:48)(·)
pointwise utility. We also note that the utility considered here is ordinal : that is, speciﬁc values of v
(or of fauc) are only useful for ordering alternatives.

Note that the use of a generalized function w in the preceding proof arises from scenarios where
the preference relation is not everywhere sensitive to taf: essentially, we have to deal with the case
(cid:104)(1, 0, . . . , 0), x(cid:105) → x(0) as the grid mesh used to approximate [0, 1] becomes ﬁner. If we assume that
the utility is sensitive to a non-null set of points of the taf curve, the weight term can be assumed

to be a proper function.

Theorem 2. For a ﬁxed collection of models H, tafiH = conv(tafH) = tafconv(H). Hence, we
have tafH ≤ tafiH pointwise and, by extension, faucw

H for any weight function w.

H ≤ fauciw

Proof. We begin by noting that the ﬁrst equality tafi = conv(taf) follows from basic properties of
convex hulls of polytopes in the Euclidean plane. Speciﬁcally, note that

conv [taf ∪ (0, 0) ∪ (1, 0)]

is created by interpolating the vertices along the edges of the polytope. If we restrict ourselves
to the upper boundary, recalling that A = {(x, y) : y = sup(a,b)∈A:a=x b}, it is clear that we have
tafi = conv(taf).

For the second inequality, we ﬁrst note that any point (f, a) in tafiH can be expressed as a linear
combination of two points (f −, a−) and (f +, a+) corresponding to Pareto optimal elements of H
with the points distinct if f (cid:54)= 0. Let λ ∈ [0, 1] be such that

(cid:33)

(cid:32)
f

a

= λ

(cid:33)

(cid:32)

f −
a−

+ (1 − λ)

(cid:33)

(cid:32)

f +
a+

The existence of this two-point representation is guaranteed by Carathéodory’s Theorem on convex
bodies, restricting attention to the 1-dimensional face of conv(taf) containing (f, a). Now note

22

that if h−, h+ ∈ H are the models with fairness and accuracy (f ±, a±) respectively, the randomized
combination h = λh− + (1 − λ)h+ has the desired fairness and accuracy: speciﬁcally, note that

Accuracy(h) = E[Accuracy(h)]

= E[Z Accuracy(h−) + (1 − Z) Accuracy(h+)]
= E[Z] Accuracy(h−) + E[1 − Z] Accuracy(h+)
= λf − + (1 − λ)f +

= f

as desired for any Accuracy measure which is an average over observations. The same argument holds
for Fairness, showing that every point in tafi is obtained by a convex combination of elements of
H.

A.3 Proofs for Section 4: Optimizing the Frontier: Fair Model Stacking (FairStacks)

As discussed in the main text, by constraining the scores, e.g., the linear predictor η = Xβ term
of logistic regression, rather than the decisions of the FairStacks problem, we are able to retain

convexity and computational tractability. On its own, however, score fairness is not enough to imply

decision fairness: consider two populations where scores are distributed as a point mass at 0.6 for
one group and a equal mixture of two point masses at 0.4 and 0.8 for the other group (η|A = 0 ∼ δ0.6
vs η|A = 1 ∼ 1
2 δ0.8). In both cases, E[η] = 0.6, but if η is used as input to a decision function
which thresholds at 0.5, we see that P [ ˆY = 1|A = 0] = 1 while P [ ˆY |A = 1] = 0.5, resulting in a
signiﬁcant violation of demographic parity.

2 δ0.4 + 1

While trivial, this example highlights what is essentially the only failure mode of score fairness:
mean scores matching while higher moments (e.g., variance) not matching between classes. We
note however that this failure cannot occur in the FairStacks context: for a ﬁxed data generating

process, shrinking the means of the two classes together shrinks all higher moments as well or, at

least, does not pull them apart, resulting in increased decision fairness as the FairStacks score

constraint is tightened.

Theorem 3. Suppose the predictions of the FairStacks problem are used to generate predicted
labels via a (potentially randomized) decision function δ(η, Z) such that ∆(η) = EZ[δ(η, Z)] is
thrice-diﬀerentiable, has second derivative bounded away from zero and inﬁnity, and is monotonically
increasing in η = η(x) = (cid:80)k
i=1 wihi(x). Then, to a second-order approximation and in expectation,
the decision bias with respect to the same groups as (cid:103)Bias is monotonically increasing in τ if

wT
wT

τ Covi∼G1[H(x)]wτ
τ Covi∼G2[H(x)]wτ

→

∆(cid:48)(cid:48)(Ei∼G2[wT
∆(cid:48)(cid:48)(Ei∼G1[wT

τ xi)])
τ xi])

monotonically in τ , where wτ is the solution to the FairStacks problem at constraint level τ ,
H(x) = (h1(x), h2(x), . . . , hp(x)) is the vector of base model predictions at x, and G1, G2 are the

23

groups used to evaluate both the score bias, (cid:103)Bias, and the decision bias, Bias.

Proof. We begin by taking a second-order Taylor-expansion of ∆ around η0 to ﬁnd:

∆(η) = ∆(η0) + ∆(cid:48)(η0)(η − η0) +

1
2

∆(cid:48)(cid:48)(η0)(η − η0)2 + O(∆(cid:48)(cid:48)(cid:48)(η0))

Substituting η0 = wT
have:

τ H(x) and taking expectations over H(x) on both sides (η0 = wT
τ

E[H(x)]) we

E[∆(wT

τ H(x))] = ∆(E[wT

τ H(x)]) +

= ∆(E[wT

τ H(x)]) +

1
2
1
2

∆(cid:48)(cid:48)(E[wT

τ H(x)]) Cov(wT

τ H(x)) + O(∆(cid:48)(cid:48)(cid:48)(wT
τ

E[H(x)])))

∆(cid:48)(cid:48)(E[wT

τ H(x)])wT

τ Cov(H(x))wτ + O(∆(cid:48)(cid:48)(cid:48)(wT
τ

E[H(x)]))

The diﬀerence in this quantity between two sub-groups (G1, G2) gives the expected decision bias
induced by decision rule ∆ for a FairStacks solution of wτ :

Bias(wτ ; G1, G2) =

(cid:122)
∆(EH(x)∼G1 [wT

τ H(x)]) − ∆(EH(x)∼G2 [wT

(cid:123)
τ H(x)])

A
(cid:125)(cid:124)

+

1
2

wT
τ
(cid:124)

(cid:2)∆(cid:48)(cid:48)(EH(x)∼G1 [wT

τ H(x)]) CovH(x)∼G1 (H(x)) − ∆(cid:48)(cid:48)(EH(x)∼G2 [wT

τ H(x)]) CovH(x)∼G2(H(x))(cid:3) wτ
(cid:125)

(cid:123)(cid:122)
B

+ O(∆(cid:48)(cid:48)(cid:48)(wT
τ

E[H(x)])))

Clearly the zeroth-order term, A, is decreasing in τ by monotonicity of ∆ and the FairStacks
constraint, which forces the two group means towards each other. Controlling the second order term,
B, requires slightly stronger assumptions: rearranging

wT
τ

(cid:2)∆(cid:48)(cid:48)(EH(x)∼G1[wT

τ H(x)]) CovH(x)∼G1(H(x)) − ∆(cid:48)(cid:48)(EH(x)∼G2[wT

τ H(x)]) CovH(x)∼G2(H(x))(cid:3) wτ → 0

gives the convergence condition in the statement of the theorem, as desired.

Three special cases of the preceding analysis are worth noting:

I. If the covariances of the base learners are the same across groups, then B reduces to

B = (cid:2)∆(cid:48)(cid:48)(Ex∼G1[wT

τ H(x)]) − ∆(cid:48)(cid:48)(Ex∼G2[wT

τ H(x)])(cid:3) wT

τ Cov(H(x))wτ

which clearly goes to 0 as τ → 0 by the same argument as the zeroth order term. This is the
simple suﬃcient condition stated in the main text.

II. A weaker suﬃcient condition for decision bias to vanish, up to second order, as τ → 0 is for

24

wτ to converge to the nullspace of

∆(cid:48)(cid:48)(Ex∼G1[wT
∆(cid:48)(cid:48)(Ex∼G2[wT

τ H(x)])
τ H(x)])

Covx∼G1(H(x)) − Covx∼G2(H(x))

That is, we do not actually need the covariances to match asymptotically: we only need wτ to
lie in the nullspace of their diﬀerence. In essence, this only requires that the stacked ensemble

is “unaware” of the diﬀerence in variances, not that no diﬀerence exists.

III. If we can bound (cid:107) Covx∼G1(H(x)) − Covx∼G2(H(x))(cid:107)op above by C, then we can bound B

above by

C|∆(cid:48)(cid:48)(Ex∼G1[wT

τ H(x)]| + |∆(cid:48)(cid:48)(Ex∼G1[wT

τ H(x)] − ∆(cid:48)(cid:48)(Ex∼G2[wT

τ H(x)]|(cid:107) Covx∼G2(H(x))(cid:107)op

The second term goes to 0 as τ → 0, but the ﬁrst term is essentially constant in τ . This
highlights the behavior described in the text before this proof: if there is a systemic diﬀerence

in the covariance structure across groups, we cannot guarantee that decision bias goes to

zero. We do note, however, that it is essentially monotonic2 and the qualitative results of the

discussion in Section 4 still hold.

Theorem 3 is similar to Theorem 1 of Lohaus et al. [16], but with three major diﬀerences: i) we

establish (approximate) monotonicity of a speciﬁc constraint as opposed to continuity with respect

to a broader class of constraints; ii) we do not assume strong convexity of the problem; and iii) we
allow an arbitrary decision function δ to be used, rather than a threshold at 0. Conversely, our
result holds only up to a higher-order Taylor approximation and in expectation and can be weakly

violated in ﬁnite samples, though we have not observed violations outside of intentionally designed

counterexamples.

Proposition 1. Given a model class H, let FS(H) be the set of models obtained by solving the
FairStacks problem (2) at all values of τ . Then tafH ≤ tafFS(H) pointwise and, by extension,
faucw(H) ≤ faucw(FS(H)) for any weight function w where the Fairness used to construct taf is
the same score-fairness used in the FairStacks problem. Additionally, under the conditions of
Theorem 3, the same inequalities hold for taf and fauc based on corresponding decision-fairness.

Proof. Let Stack(H) be all linear combinations of models in H. Clearly H (cid:40) Stack(H) so, by
Proposition 2(iii), we have tafH ≤ tafStack(H). To extend this result to FS(H) instead of Stack(H),
we note that for a ﬁxed fairness level τ , Stack(H)τ = FS(H)τ where Hf = {h ∈ H : Fairness(h) ≥ f },
because taking models with bias at most 1 − τ is equivalent to taking models with fairness at least
τ . Since this inequality holds for all τ , we have tafStack(H) = tafFS(H) and hence tafH ≤ tafFS(H)
as desired.

2Speciﬁcally, when we move from B to an upper bound on B, we loose traditional monotonicity and have the
following weaker form of monotonicity: Bias(wτ ; G1, G2) is upper bounded by some sequence κτ such that κτ is
monotonically decreasing as τ → 0.

25

The conditions of Theorem 3 then let us extend this result to suitable decision-fairness measures

because pointwise dominance is preserved under monotonic transformation of the shared abscissa
(x-axis). We note that the conditions of Theorem 3, or conditions substantively equivalent, are
necessary to ensure the fairness measures used in solving FairStacks and in constructing the

resulting taf curves are congruent.

A.4 Out-of-Sample Performance of FairStacks

Theorem 4. Let wτ be the solution to the FairStacks problem, trained on n1, n2 samples from
groups G1, G2 respectively, and let t(x) : Rp → {0, 1} = 1wT
τ H(x)>c be a thresholding (decision) rule
for some ﬁxed c. If t(x) has classiﬁcation accuracy Atrain and bias Btrain on the training data used
to create the ensemble weights, then

A∗ ≤ Atrain + 2

(cid:115)

(cid:115)

B∗ ≤ Btrain + 2

(p + 1) log(2 min{n1, n2} + 1)
min{n1, n2}

(p + 1) log(2 min{n1, n2} + 1)
min{n1, n2}

(cid:115)

(cid:115)

+

+

log 1/δ
min{n1, n2}

log 1/δ
min{n1, n2}

each with probability at least 1 − δ, where A∗, B∗ are the expected out-of-sample classiﬁcation accuracy
and bias respectively.

Furthermore, if the same classiﬁer, t(·), is run on a test sample of n(cid:48)
respectively, then

1, n(cid:48)

2 samples from groups G1, G2

(cid:115)

(cid:115)

Atest ≤ Atrain +

Btest ≤ Btrain +

log 1/(cid:15)

4 min{n(cid:48)

1, n(cid:48)

2}

log 1/(cid:15)

2 min{n(cid:48)

1, n(cid:48)

2}

+ 2

(cid:115)

(cid:115)

+ 2

(p + 1) log(2 min{n1, n2} + 1)
min{n1, n2}

(p + 1) log(2 min{n1, n2} + 1)
min{n1, n2}

(cid:115)

(cid:115)

+

+

log 1/δ
min{n1, n2}

log 1/δ
min{n1, n2}

each with probability at least 1 − δ − (cid:15), where Atest, Btest are the realized test classiﬁcation accuracy
and bias respectively.

Proof. For simplicity, we assume that both groups have n = min{n1, n2} samples in the training data
2} samples in the test data. Marginally tighter results can be obtained by treating
and n(cid:48) = min{n(cid:48)
n1, n2 separately, but at the cost of more cumbersome analysis that yields little additional insight.
Having additional samples will only tighten the concentration results used, so the results claimed

1, n(cid:48)

are true in the general case. With this simpliﬁcation, we reduce the problem to the analysis of a
classiﬁer trained on 2n samples and can use the well-developed tools of empirical risk minimization
and VC dimension; speciﬁcally, we rely on the tools presented by Boucheron et al. [4].

Combining their Equation (3), Theorem 3.4, and the remark at the end of Section 3, we have the

26

general result that, with probability at least 1 − δ,

|P2nf − P f | ≤ 2

sup
f ∈F

(cid:114)

(p + 1) log(2n + 1)
n

+

(cid:114)

log 1/δ
n

where the supremum is taken over all linear threshold functions3, P2n refers to the empirical
expectation of f over the training data and P refers to the expected value under iid sampling.
Substituting this bound into Equation (2) of Boucheron et al. [4], we obtain the ﬁrst set of results in
our theorem: note that nothing in their analysis requires that L actually be the loss function used,
though this is commonly the case in the analysis of classiﬁcation accuracy, so we can use the bias

function here as well without issue.

We now turn to bounds for accuracy and bias on a ﬁnite test set: in this case, we rely on standard
Hoeﬀding-type bounds for concentration of Bernoulli random variables. Speciﬁcally, if we have 2n(cid:48)
samples each of which are correctly classiﬁed with probability A∗, then the empirical classiﬁcation
accuracy Atest is simply the mean of 2n Bernoulli random variables each with probability A∗.
Hoeﬀding’s inequality, which can be found as Theorem 2.2.6 in the book by Vershynin [21], then

implies that

P (Atest − A∗ ≥ t) ≤ e−4t2n(cid:48)

Combining this with the previous result and setting (cid:15) = e−4t2n, we ﬁnd that

Atest ≤ Atrain +

(cid:114)

(cid:114)

log 1/(cid:15)
4n(cid:48) + 2

(p + 1) log(2n + 1)
n

+

(cid:114)

log 1/δ
n

with probability at most 1 − δ − (cid:15) as desired.

For the bias term, we have essentially the same analysis, but with n (arbitrarily paired) observations
each of which have diﬀerent labels (a “bias occurence”) with probability B∗. Repeating the argument
from above, Hoeﬀding’s inequality gives,

P (Btest − B∗ ≥ t) ≤ e−2t2n(cid:48)

Combining this with the previous result and setting (cid:15) = e−2t2n(cid:48), we ﬁnd that

Btest ≤ Btrain +

(cid:114)

(cid:114)

log 1/(cid:15)
2n(cid:48) + 2

(p + 1) log(2n + 1)
n

+

(cid:114)

log 1/δ
n

as desired.

Note that the log(2 min{n1, n2} + 1) terms can be removed at the cost of a worse leading constant:
see Theorem 3.4 of Boucheron et al. [4] for details. Also note that for certain fairness measures, the
“eﬀective sample size” in this result (min{n1, n2}) may be a signiﬁcant underestimate: speciﬁcally, if

3We have an extra VC dimension because we allow for an arbitrary threshold (bias/intercept term) rather than

ﬁxing the threshold at 0.

27

Equality of Opportunity [12] or other fairness measures that depend only on a subset of the samples

are used, the accuracy (depending on the full sample) may concentrate more quickly than these

results would suggest.

B Additional Material for Section 5 - Empirical Studies

B.1 Additional Experimental Details

B.1.1 Data Sets and Preprocessing

In this section, we explain mores speciﬁc details about the data sets used in our experiments.

• UCI Adult Income [8, 14] Target values state whether a person’s income is ≥ $50k given a
series of attributes. Originally, the data set contains 9 categorical features and 6 continuous
features with n = 48, 222 observations (p = 99 total features after one-hot encoding). It has
two binary protected attributes: Gender (male or female) and Race (white or non-white). We
discarded the fnlwgt variable, which indicates the number of people census takers believe that
observation represents.

Note that we used the “classic” version of the Adult Income data set based on the 1994 U.S.

census: Ding et al. [7] note weakness of this data set and give extensions and updates based

on more recent censuses.

• Bank[8, 19] Target values state whether a client has subscribed to a term deposit. The data set
contains p = 16 features (p = 58 after one-hot encoding) on n = 44, 388 clients of a Portuguese
banking institution. The protected attribute is age, encoded as a binary attribute indicating

whether the client’s age is between 33 and 63 years or not. Continuous variables were left as is

and categorical variables were converted into one-hot encoded vectors.

• COMAS Recidivism[15] Target values state whether or not the individual recidivated within
2 years of their most recent crime. This dataset contains p = 13 binary features and n = 5, 278
observations. It has two binary protected attributes: Gender (male or female) and Race (white

or non-white).

We note that this data set is controversial on its own merits and in its application: we refer the

reader to Bao et al. [2] and to references therein for a thorough discussion of the limitations of

this data set.

• Default[8, 22] Target values that state whether an individual will default on payments. This
data set contains p = 23 features and n = 30, 000 observations. Gender is the sensitive
attribute, encoded as a binary attribute, male or female. Continuous variables were left as is

and categorical variables were converted into one-hot encoded vectors.

• Communities and Crime[8] Target values state the normalized per capita violent crime
rates in various communities in the United States. It contains p = 104 features and n = 1, 969

28

observations. Race is the protected attribute encoded as a binary attribute, majority white
community or not. We removed features that had 80% or greater missing values.

B.1.2 Hyper-Parameter Selection and Model Tuning

While we analyze FairStacks in its constrained form (cf., Equation (2)), we use the following
equivalent penalized form in our numerical experiments:

arg min
w∈Rk

n
(cid:88)

L

(cid:32) k

(cid:88)

j=1

i=1

(cid:33)

wihi(xj); yj

+ λ2

(cid:33)2

wi(cid:103)Bias(hi)

(cid:32) k

(cid:88)

i=1

Note that we square the score bias term ((cid:103)Bias) to yield a diﬀerentiable penalty term, making the
penalized form particularly easy to solve using a damped Newton method.

Here λ ∈ R≥0 is a hyper-parameter controlling the degree of bias: higher values of λ give less biased
(more fair) solutions, with λ = ∞ ⇔ τ = 0 giving a perfectly fair ensemble. We use a grid of 20
log-spaced values of λ ∈ [100, 106] to construct taf curves for our FairStacks ensembles. To avoid
overﬁtting our ensemble weights, we also include a ridge-regression type penalty (+ α
) with
α ∈ [102, 107] chosen to optimize the ﬁve-fold cross validated estimate of the fauc score. (Note that
cross-validation was performed within the model stacking step: reported test fauc scores are indeed

2 (cid:107)w(cid:107)2
2

unbiased.)

In our comparisons with the methods of Zhang et al. [23], Agarwal et al. [1], and Grari et al. [10],

we use standard publicly available implementations:

• Adversarial Debiasing - Zhang et al. [23]: Adversarial debiasing attemps to learn a
classiﬁer under an objective function which balances prediction accuracy and an adversary’s

ability to determine the protected attribute(s) from the predictions. We use the implementation
from the AI Fairness 360 [3] package4 and vary the fairness-accuracy tradeoﬀ parameter (the
weight given to the adversary in the composite objective) over a linearly-spaced grid on the
[0, 1] interval. Default settings are used for all other hyper-parameters.

• Reduction-Based Fair Classiﬁcation - Agarwal et al. [1]: The reductions approach to
fair classiﬁcation reweights or relabels that data to ﬁnd the most accurate and fair version
of the input classiﬁer. We use the implementation from the Fairlearn package5 and vary
the fairness-accuracy tradeoﬀ parameter (the weight given to the adversary in the composite
objective) over a linearly-spaced grid on the [0, 1] interval. Default settings are used for all
other hyper-parameters.

• Fair Adversarial Gradient Tree Boosting - Grari et al. [10]: The Fair Adversarial

4https://github.com/Trusted-AI/AIF360
5https://fairlearn.org/v0.5.0/api_reference/fairlearn.reductions.html

29

Gradient Tree Boosting methodology attempts to learn an ensemble of tree-based classiﬁers

using a variant of gradient boosting which balances prediction accuracy with an adversary’s

ability to determine the protected attribute(s) from the predictions. We use the authors’

reference implementation6 and vary the fairness-accuracy tradeoﬀ parameter (the weight given
to the adversary in the boosting objective) over a linearly-spaced grid on the [0, 0.2] interval.
Default settings are used for all other hyper-parameters.

B.1.3 Computational Resources Used

Computational experiments were run on a shared server with a AMD Ryzen Threadripper 3970X

32-Core Processor (64 Logical CPUs / 32 Physical CPUs) at 3700 MHz and 252 GB of memory. No

GPU resources were used. Moderate parallelization (20 threads) was used to perform our simulation
studies, taking approximately 2 days of total wall clock time (< 1000 hours of CPU time) for all
results in this paper.

Note that, because taf identiﬁcation is quasi-linear and the FairStacks meta-learner is convex,

computational costs are dominated by training base learners and not by the proposed methods of

the paper. For the results shown here, Algorithm 1 and FairStacks both took negligible time
(< 10 seconds) in all our experiments, even for our largest ensembles, with time dominated by the
calculations necessary to estimate base learner fairness and accuracy.

B.2 Additional Experimental Results

B.2.1 Demographic Parity - Additional Data Sets

Figure A1 visualizes taf curves for data sets not appearing in Figure 2 of the main text of the paper.
Note that panel C (the C and C data set) has fewer curves because the methods of Zhang et al. [23],
Grari et al. [10], and Agarwal et al. [1] cannot be applied to this regression task. See Figure 2 in the

main text for other data sets considered.

6https://github.com/vincent-grari/FAGTB

30

Figure A1: taf curves for three fair ML tasks not shown in Figure 2: see discussion in Section 5
and Sections B.1.1-B.1.2 for more details of our experimental approach. We see that on all tasks,
the largest FairStacks ensemble (kitchen-sink ) achieves the highest accuracy at all fairness levels,
yielding the highest overall fauc scores (cf., Table 1). Note that the methods of Zhang et al. [23],
Grari et al. [10], and Agarwal et al. [1] are omitted for the regression (C and C) task as those methods
do not natively support regression.

B.2.2

fauc Table: Equality of Opportunity

Our results in Section 5 are presented for Demographic Parity, but the taf/fauc framework and

the FairStacks meta-learner can be used for other fairness measures. In this section, we repeat our

analysis from above, now using Equality of Opportunity (EO) [12] as our fairness metric. Equality
of Opportunity is deﬁned by the criterion E[ ˆY |A = 1, Y = 1] = E[ ˆY |A = 0, Y = 1] and we measure
fairness as deviation from this ideal: FairnessEO = 1 − |E[ ˆY |A = 1, Y = 1] − E[ ˆY |A = 0, Y = 1]|,
where A is the protected attribute, Y is the ground truth label, and ˆY is the predicted label of
a classiﬁer. For our score bias measure in FairStacks, we use the diﬀerence in subgroup means,
where the same subgroups are used as calculating fairness. (cf., Deﬁnition 4).

Table A1 reports the EO-fauc scores of several base learners and ensemble construction strategies.

As before, we use a weight function at 80% to provide an unbiased comparison, while enforcing
conformance with prevailing U.S. legal standards (cf. Table 1). As with our Demographic Parity-based
experiments, we see that the FairStacks framework consistently achieves the highest fauc scores

across all data sets considered: additionally, consistent with our theoretical results, FairStacks ﬁt

31

on the largest set of base learners (kitchen-sink ) obtains the best Pareto frontier.

Table A1: Quantative Results for Section B.2.2: Equality of Opportunity + 80% step-weighted fauc.
We omit the C and C regression task because it is unclear how to deﬁne Equality of Opportunity for
continuous labels.

Method

Adult

Gender

Race

Bank

Age

COMPAS

Gender

Race

Default

Gender

Random Forest

.782(.001)

.814(.001)

.900(.002)

.568(.003)

.557(.003)

.834(.010)

Zhang et al. [23]

.825(.030)

.840(.007)

.872(.004)

.618(.052)

.617(.061)

.845(.003)

Grari et al. [10]

.826(.003)

.821(.004)

.889(.012)

.627(.005)

.628(.004)

.789(.002)

Agarwal et al. [1]

.800(.004)

.802(.002)

.883(.011)

.621(.003)

.629(.003)

.791(.010)

FS MP Trees

.861(.001)

.857(.001)

.914(.004)

.639(.004)

.683(.002)

.891(.002)

FS RF Trees

.852(.001)

.849(.003)

.918(.015)

.622(.004)

.700(.005)

.921(.003)

FS Classiﬁers

.712(.006)
FS Kitchen Sink .871(.001) .863(.001) .929(.003) .641(.004) .738(.003)

.907(.003)

.837(.001)

.839(.002)

.601(.005)

.883(.002)

.944(.001)

B.2.3

fauc Table: Multiple Protected Attributes

Our FairStacks framework can be used to improve fairness for multiple protected attributes

simultaneously by adding a constraint for each protected attribute. Though this sort of intersectional

fairness [5] is not the primary focus of our paper, we give a brief demonstration here.

Table A2 reports fauc scores for both gender- and race-fairness for four diﬀerent FairStacks

ensembles generated from Random Forest trees: an unconstrained ensemble, FairStacks with a

gender-fairness constraint, FairStacks with a race-fairness constraint, and FairStacks with both

constraints. That is, we solve the FairStacks problem with various constraints and compute fauc

scores for each protected attribute separately. As in our other experiments, we use a 80%-step

function weight scheme in calculating fauc.

As can be seen in Table A2, FairStacks always yields better fauc scores for both protected

attributes. As expected, FairStacks achieves a better fauc score for the group used in the

constraint, but doubly-constrained FairStacks is highly competitive in all scenarios. This suggests

that, at least for the data sets considered here, FairStacks can achieve fairness for multiple groups

without signiﬁcant decreases in accuracy. While this result is promising, we leave development

and analysis of a “composite” fauc, quantifying the tradeoﬀ between both fairness measures and

accuracy, to future work.

Additional References

[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. “A
Reductions Approach to Fair Classiﬁcation”. ICML 2018: Proceedings of the 35th International

32

Table A2: Application of FairStacks to Multiple Protected Attributes: fauc scores in columns two
and four are calculated using Gender as the protected attribute, while scores in columns three and
ﬁve use Race as the protected attribute. While the presence of any fairness constraint improves fauc
scores for both protected attributes, the multiply-constrained FairStacks achieves near-optimal
performance in all scenarios considered.

Constraint

Adult

COMPAS

Gender

Race

Gender

Race

None
Gender
Race
Both

.772(.001) .830(.001) .557(.004) .524(.003)
.843(.001) .829(.002) .607(.006) .641(.004)
.817(.002) .851(.002) .592(.005) .773(.004)
.849(.001) .841(.002) .600(.005) .727(.004)

Conference on Machine Learning. Vol. 80. 2018, pp. 60–69. url: https://proceedings.mlr.
press/v80/agarwal18a.html.

[2] Michelle Bao, Angela Zhou, Samantha Zottola, Brian Brubach, Sarah Desmarais, Aaron

Horowitz, Krisitian Lum, and Suresh Venkatasubramanian. “It’s COMPASlicated: The Messy
Relationship between RAI Datasets and Algorithmic Fairness Benchmarks”. NeurIPS-Datasets
2021: Proceedings of the Neural Information Processing Systems Track on Datasets and Bench-
marks 2021. Ed. by J. Vanschoren and S. Yeung. 2021. url: https://datasets-benchmarks-
proceedings.neurips.cc/paper/2021/hash/92cc227532d17e56e07902b254dfad10-Abstract-
round1.html.

[3] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoﬀman, Stephanie Houde,

Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic,

Seema Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha,

Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. “AI Fairness 360:

An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic
Bias”. ArXiv Pre-Print 1810.01943 (2018). doi: 10.48550/arXiv.1810.01943.

[4] Stéphan Boucheron, Olivier Bousquet, and Gábor Lugosi. “Theory of Classiﬁcation: A Survey
of Some Recent Advances”. ESAIM: Probability and Statistics 9 (2005), pp. 323–375. doi:
10.1051/ps:2005018.

[5] Joy Buolamwini and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in
Commercial Gender Classiﬁcation”. FAccT 2018: Proceedings of the 1st Conference on Fairness,
Accountability, and Transparency. Ed. by Sorelle A. Friedler and Christo Wilson. Vol. 81.
Proceedings of Machine Learning Research. 2018, pp. 77–91. url: https://proceedings.mlr.
press/v81/buolamwini18a.html.

[6] Gerard Debreu. Mathematical Economics: Twenty Papers of Gerard Debreu. Ed. by Werner
Hildenbrand. Econometric Society Monographs. Cambridge University Press, 1983. isbn:
978-0-521-23736-9. doi: 10.1017/CCOL052123736X.

[7] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. “Retiring Adult: New Datasets
for Fair Machine Learning”. NeurIPS 2021: Advances in Neural Information Processing Sys-

33

tems 34. Ed. by M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan. 2021, pp. 6478–6490. url: https://proceedings.neurips.cc/paper/2021/hash/
32e54441e6382a7fbacbbbaf3c450059-Abstract.html.

[8] Dheeru Dua and Casey Graﬀ. UCI Machine Learning Repository. 2017. url: http://archive.

ics.uci.edu/ml.

[9] Gebhard Fuhrken and Marcel K. Richter. “Additive Utility”. Economic Theory 1.1 (1991),

pp. 83–105. doi: 10.1007/BF01210575.

[10] Vincent Grari, Boris Ruf, Sylvain Lamprier, and Marcin Detyniecki. “Fair Adversarial Gradient
Tree Boosting”. ICDM 2019: Proceedings of the 2019 IEEE International Conference on Data
Mining. 2019, pp. 1060–1065. doi: 10.1109/ICDM.2019.00124.

[11] Kazuhiro Hara. “Characterization of stationary preferences in a continuous time framework”.
Journal of Mathematical Economics 63 (2016), pp. 34–43. doi: 10.1016/j.jmateco.2015.11.
005.

[12] Moritz Hardt, Eric Price, and Nathan Srebron. “Equality of Opportunity in Supervised
Learning”. NeurIPS 2016: Advances in Neural Information Processing Systems 29. Vol. 29. 2016.
url: https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-
Abstract.html.

[13] Charles M. Harvey and Lars Peter Østerdal. “Discounting models for outcomes over continuous
time”. Journal of Mathematical Economics 48.5 (2012), pp. 284–294. doi: 10.1016/j.jmateco.
2012.07.001.

[14] Ron Kohavi. “Scaling up the accuracy of naive-Bayes classiﬁers: A decision-tree hybrid”. KDD
1996: Proceedings of the 2nd International Conference on Knowledge Discovery and Data
Mining. Vol. 96. 1996, pp. 202–207. doi: 10.5555/3001460.3001502.

[15] Jeﬀ Larson, Marjorie Roswell, and Vaggelis Atlidakis. COMPAS Recidivism Risk Score Data

and Analysis. 2022. url: https://github.com/propublica/compas-analysis/.

[16] Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. “Too Relaxed to Be Fair”. ICML
2020: Proceedings of the 37th International Conference on Machine Learning. Ed. by Hal
Daumé III and Aarti Singh. Vol. 119. Virtual: PMLR, 2020, pp. 6360–6369. url: https:
//proceedings.mlr.press/v119/lohaus20a.html.

[17] Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory. Oxford

University Press, 1995. isbn: 978-0-195-07340-9.

[18] Ghanshyam Mehta. “Topological Ordered Spaces and Utility Functions”. International Economic

Review 18.3 (1977), pp. 779–782. doi: 10.2307/2525961.

[19] Sérgio Moro, Paulo Cortez, and Paulo Rita. “A data-driven approach to predict the success of
bank telemarketing”. Decision Support Systems 62 (2014), pp. 22–31. doi: 10.1016/j.dss.
2014.03.001.

[20] Bezalel Peleg. “Utility Functions for Partially Ordered Topological Spaces”. Econometrica 38.1

(1970), pp. 93–96. doi: 10.2307/1909243.

34

[21] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. 1st. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge Uni-
versity Press, 2018. isbn: 978-1-108-41519-4.

[22]

I-Cheng Yeh and Che-Hui Lien. “The comparisons of data mining techniques for the predictive
accuracy of probability of default of credit card clients”. Expert Systems with Applications 36.2
(2009), pp. 2473–2480. doi: 10.1016/j.eswa.2007.12.020.

[23] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. “Mitigating Unwanted Biases with
Adversarial Learning”. AIES 2018: Proceedings of the 2018 AAAI/ACM Conference on AI,
Ethics, and Society. 2018, pp. 335–340. doi: 10.1145/3278721.3278779.

35

