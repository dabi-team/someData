Deep Policy Iteration with Integer Programming for
Inventory Management

Pavithra Harsha, Ashish Jagmohan, Jayant Kalangnanam, Brian Quanz
IBM Research, Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA

Divya Singhvi
NYU Leonard N. Stern School of Business, NY 10012, USA

Problem Deﬁnition: In this paper, we present a Reinforcement Learning (RL) based framework for opti-

mizing long-term discounted reward problems with large combinatorial action space and state dependent

constraints. These characteristics are common to many operations management problems, e.g., network

inventory replenishment, where managers have to deal with uncertain demand, lost sales, and capacity

constraints that results in more complex feasible action spaces. Our proposed Programmable Actor Rein-

forcement Learning (PARL) uses a deep-policy iteration method that leverages neural networks (NNs) to

approximate the value function and combines it with mathematical programming (MP) and sample aver-

age approximation (SAA) to solve the per-step-action optimally while accounting for combinatorial action

spaces and state-dependent constraint sets. Results: We then show how the proposed methodology can be

applied to complex inventory replenishment problems where analytical solutions are intractable. We also

benchmark the proposed algorithm against state-of-the-art RL algorithms and commonly used replenish-

ment heuristics and ﬁnd that the proposed algorithm considerably outperforms existing methods by as much

as 14.7% on average in various supply chain settings. Managerial Insights: We ﬁnd that this improve-

ment in performance of PARL over benchmark algorithms can be directly attributed to better inventory

cost management, especially in inventory constrained settings. Furthermore, in a simpler back order setting

where optimal replenishment policy is tractable, we ﬁnd that the RL based policy also converges to the

optimal policy. Finally, to make RL algorithms more accessible for inventory management researchers, we

also discuss the development of a modular Python library that can be used to test the performance of RL

algorithms with various supply chain structures. This library can spur future research in developing practical

and near-optimal algorithms for inventory management problems.

Key words : Multi Echelon Inventory Management, Inventory Replenishment, Deep Reinforcement

Learning

History : This paper is under preparation.

2
2
0
2

t
c
O
4
1

]

G
L
.
s
c
[

2
v
5
1
2
2
0
.
2
1
1
2
:
v
i
X
r
a

1.

Introduction

Inventory and supply chain management have seen a tremendous shift over the past decade. With

the advent of online e-commerce, supply chains have become more and more complex and global

with increasingly connected physical ﬂows (Young 2022). Naturally, the cost of managing these

supply chains have increased over the years. Furthermore, the pandemic has lead to an increased

1

 
 
 
 
 
 
2

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

spending in managing such complex supply chains. In fact, a recent Wall Street Journal report states

that US business logistics costs increased as much as 22% year-on-year (Young 2022). One potential

promising direction for managing such complex supply chains is to use Artiﬁcial Intelligence (AI)

based inventory management solutions and explore the beneﬁts it can provide.

AI and Reinforcement learning (RL) has led to considerable breakthroughs in diverse areas such

as games (Mnih et al. 2013), robotics (Kober et al. 2013) and others. RL provides a systematic

framework to solve sequential decision making problems with very limited domain knowledge. In

fact, one can leverage several state-of-the-art, open-source RL methods to learn a very good policy

that maximizes long-run rewards of many sequential decision problems at hand. Therefore, it is

not surprising that RL has recently also been applied to similar problems in various domains such

as healthcare (Yu et al. 2019), supply chains (Gijsbrechts et al. 2018, Oroojlooyjadid et al. 2021,

Sultana et al. 2020) and more. Yet enterprise level operations applications of RL remains limited

and quite challenging.

Consider typical operations management (OM) problems such as inventory management and

network revenue management. These problems are generally characterized by large action spaces,

often well-deﬁned state-dependent action constraints and rewards, and underlying stochastic tran-

sition dynamics. For example, a ﬁrm managing the inventory across a network of nodes in the

supply chain has to decide how much inventory to place across the diﬀerent nodes of the network.

To accomplish this eﬃciently, the ﬁrm has to overcome various challenges. This includes accounting

for (i) the uncertain demand across the nodes in the network; (ii) a large set of locally feasible (and

often combinatorial) actions since the ﬁrm decides on the number of units to allocate to diﬀerent

nodes; (iii) a large number of state-dependent constraints to ensure that the complete allocation

vector is feasible; (iv) the trade-oﬀ between the immediate and the long term reward of actions.

RL methods use an environment, a live or simulated one, to sample the underlying uncertainty

to generate reward trajectories, and in-turn estimates of diﬀerent action policies by understanding

the trade-oﬀ between long term and short term rewards. Nevertheless, large combinatorial action

spaces with state-dependent constraints, as in the case of the OM problems described above, render

enumeration based RL techniques over the action space, computationally intractable. Hence, in

this paper, we present a specialized RL algorithm that resolves these challenges. In particular,

we present a deep-policy iteration method that leverages neural networks (NNs) to approximate

the value function and combines it with mathematical programming (MP) and sample average

approximation (SAA) to solve the per-step-action optimally while accounting for combinatorial

action spaces and state-dependent constraint sets. From an RL perspective, one can view this

as replacing the actor in an actor-critic method with a math-programming based actor. We use

this modiﬁed RL approach to provide benchmark solutions to inventory management problems

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

3

with complexities that make analytical solutions even in simpler networks intractable (e.g. lost

sales, ﬁxed costs, dual sourcing, lead times in multi-echelon networks) and compare them with

state-of-the art RL methods and popular supply chain heuristics.

1.1. Contributions

We make the following contributions through this work:

1. We present a policy iteration algorithm for dynamic programming problems with large action

spaces and underlying stochastic dynamics that we call Programmable Actor Reinforcement Learn-

ing (PARL). In our framework, the value-to-go is represented as a sum of immediate reward and

sum of future discounted rewards. The future discounted rewards are approximated with a NN that

is ﬁtted by generating value-to-go rewards from Monte-Carlo simulations. Then, we represent the

NN and the immediate reward as an integer program which is used to optimize the per-step action.

As the approximation improves, so does the per-step-action, which ensures that the learned policy

eventually converges to the unknown optimal policy. The approach overcomes the enumeration

challenges and allows an easy implementation of known contextual state dependent constraints.

2. We apply the proposed methodology to the problem of optimal replenishment decisions of

a retailer with a network of warehouses and retail stores. Focusing on settings where analytical

tractability is not guaranteed, we analyze and compare the performance of PARL under diﬀer-

ent supply chain network structures (multi-echelon distribution networks with and without dual-

sourcing and with lost sales) and network sizes (single supplier and three retailers, to up to 20

heterogeneous retailers with multiple intermediary warehouses). We ﬁnd that PARL is compet-

itive, and in some cases outperforms, state of the art methods (14.6% improvement on average

across diﬀerent settings studied in this paper). Our numerical experiments provide a comprehensive

benchmark results in these settings using diﬀerent RL algorithms (SAC, TD3, PPO and A2C), as

well as commonly used heuristics in various supply chain settings (speciﬁcally, base stock policies

on an edge and on a serial path). We also perform additional numerical experiments to analyze the

structure of the learned RL based replenishment policies and ﬁnd that (i) in a simpler back-order

setting, the RL policy is near optimal as it also learns an order up-to-policy and (ii) in the other

more complex settings, the higher proﬁtability is on account of improved cost management across

the supply chain network.

3. Finally, we open source our supply chain environment as a Python library to allow researchers

to easily implement and benchmark PARL and other state-of-the-art RL methods in various supply

chain settings. Our proposed library is modular and allows for researchers to design supply chains

networks with varying complexity (multi-echelon, dual sourcing), size (number of retailers and

warehouses) and reward structure (back order and lost sales settings). While our objective is similar

4

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

to Hubbs et al. (2020), we focus on inventory management problems speciﬁcally and provide more

ﬂexibility in the network design. We believe this library will make RL algorithms more accessible

to the OR and inventory management community and spur future research in developing practical

and near-optimal algorithms for inventory management problems.

1.2. Selected Literature Review

Our work is related to the following diﬀerent streams of literature: (1) parametric policies, (2)

approximate dynamic programming (ADP), (3) reinforcement learning (RL) and (4) mathematical

programming based RL. As the literature in these topics, in general and even within the context

of supply chain and inventory management speciﬁcally, is quite vast, we describe the state-of-the

art with select related literature that is most related to the current work.

Parametric policies for inventory management: The topic of inventory management has been

studied for many years and it has a long history. The seminal work of Scarf (1960) shows that for

a single node sourcing from a single supplier with inﬁnite inventory, a constant lead time and an

ordering cost with ﬁxed and variable components, the optimal policy for back-ordered demand has

a (s, S) structure where S is referred to as the order-up-level based on the inventory position (sum

of on-hand inventory plus that in the pipeline, i.e., a collapsed inventory state) and s an inventory

position threshold, below which orders are placed. This (s, S) policy is commonly referred to as

the base stock policy.

For lost-sales demand settings, i.e., wherein demand excess of inventory is lost, when the lead

times are non-zero, the structure of the optimal policy is unknown (Zipkin 2008a,b) even in the

single node setting sourcing from a single retailer. Moreover, base stock policies generally perform

poorly (Zipkin 2008a), except when penalty costs are large, when inventory levels are high and

stock-outs are rare. In general, the optimal policy depends on the full-inventory pipeline, unlike the

state-space collapse that is possible in back order settings, and the complexity grows exponentially

in lead time. In these settings, Huh et al. (2009), Goldberg et al. (2016) respectively prove the

following asymptotic optimality results (1) regarding the basestock policy when the penalties are

high and (2) regarding the constant ordering policies when the lead times are large. Xin (2021)

combines these results to propose a capped base stock policy that is asymptotically optimal as the

lead time increases and shows good empirical performance for smaller lead times.

Sheopuri et al. (2010) prove that the lost sales problem is a special case of dual sourcing problem

(a retail node has access to 2 external suppliers), and hence base stock policies are not optimal,

in general. In the dual sourcing setting, various heuristic policies similar to the lost sales setting

extend the base stock policy with a constant order and/or cap that splits the order between the

two suppliers depending on the inventory position across each have been independently proposed.

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

5

These include the Dual-Index (Veeraraghavan and Scheller-Wolf 2008), Tailored Base-Surge (Allon

and Van Mieghem 2010) and Capped Dual Index policies (Sun and Van Mieghem 2019).

Multi-echelon networks are those wherein there are multiple nodes, stages or echelons that hold

inventory. Base stock policies are optimal only in special cases with back ordered demands without

ﬁxed costs with additional restrictive assumptions such as the a serial chain with back order penalty

at the demand node (Clark and Scarf 1960) or the inability to hold demand in the warehouse in

a 2-echelon distribution network (Federgruen and Zipkin 1984). We refer the reader to de Kok

et al. (2018) for an extensive review multi-echelon models studied based on variety of modeling

assumptions and supply chain network structures.

Despite the non-optimality of base stock policies (the use of (s, S) policies with a collapsed

state space, i.e., via inventory positions), they are popular both in practice and in the literature.
For example, ¨Ozer and Xiong (2008) and Rong et al. (2017) propose competitive heuristics that

compute the order-up to base stock levels for the multi-echelon distribution (tree) networks without

ﬁxed costs but with service level constraints and demand back ordering costs respectively, and show

asymptotic optimality in certain dimensions in the 2-echelon case. Agrawal and Jia (2019) propose

a learning-based method to ﬁnd the best base stock policy in a single node lost sales setting with

regret guarantees. Pirhooshyaran and Snyder (2020) develop a DNN-based learning approach to

ﬁnd the best order up-to levels in each link of a general supply chain network.

In this work, we present a deep RL approach and leverage it to solve a certain class of cost-based

stochastic inventory management problems in settings where parametric optimal policies do not

exist or are unknown, such as the multi-echelon supply chains with ﬁxed costs, capacities, lost sales

and dual sourcing. In these settings, we provide new empirical benchmarks wherein the algorithm

we propose is able to outperform commonplace heuristics that are based on the base-stock policy.

Approximate Dynamic Programming (ADP) and Reinforcement Learning (RL): Our work is

also related to the broad ﬁeld of ADP (Bertsekas 2017, Powell 2007). This generally focuses on

solving the Bellman’s equation (1) and has been an area of active research for many years. ADP

methods typically use an approximation of the value function to optimize over computationally

intractable dynamic programming problems. Popular algorithms can be classiﬁed into two types:

model-aware and model-agnostic. Model-aware approaches use information on underlying system

dynamics (known transition function and immediate reward) to estimate value-to-go and popular

approaches include value iteration and policy iteration. These algorithms essentially start with

arbitrary estimates of the value-to-go and iteratively improve these estimates to eventually converge

to the unknown optimal policy. We refer the interested readers to Gijsbrechts et al. (2018) for an

excellent review of ADP based approaches for inventory management.

6

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

Model-agnostic approaches circumvent the issue of partial or no knowledge of the underlying

system dynamics by trial-and-error using an environment that generates immediate-reward and

state-transitions, given the current state and a selected action. This latter framework is popularly

referred to as Reinforcement Learning (RL) (Sutton and Barto 2018). Note that RL methods

themselves are also categorized into model-based RL and model-free RL wherein in process of

learning the optimal policy via the environment, the transition functions are also learned in the

former method and not learned in the latter method.

In this paper, we focus on model-free RL methods. One of the popular model-free RL algorithms

is Q-learning, wherein the value of each state-action pair is estimated using the collected trajectory

of state and rewards based on diﬀerent actions and then the optimal action is obtained by an

exhaustive search. In classical RL methods, a set of features are chosen and polynomial functions of

those features are used to approximate the value function (Van Roy et al. 1997). This had limited

success initially, but after the deep-learning revolution and the availability of signiﬁcant compute,

the deep-RL (DRL) methods regained signiﬁcant popularity and success as neural-nets were used to

approximate the value function, thereby automating the step of feature and function selection.This

led to many algorithmic breakthroughs including the development of a family of policy gradient

methods called the actor-critic method (Mnih et al. 2016), in which neural networks are used to

approximate both the value function and the policy itself - the policy with an actor network which

encodes a distribution over actions. Despite their successes, DRL and actor-critic approaches suﬀer

from several challenges, such as lack of robust convergence properties, high sensitivity to hyper

parameters, high sample complexity, and function approximation errors in their networks leading

to sub-optimal policies and incorrect value estimation (see, for discussions, Haarnoja et al. 2018,

Maei et al. 2009, Fujimoto et al. 2018, Duan et al. 2016, Schulman et al. 2017, Henderson et al.

2018, Lillicrap et al. 2016). To address these diﬀerent issues, new variations of the actor-critic

approach continue to be proposed, such as Proximal Policy Optimization (PPO), which tries to

avoid convergence to a sub-optimal solution while still enabling substantial policy improvement

per update by constraining the divergence of the updated policy from the old one (Schulman et

al. 2017), and Soft Actor-Critic (SAC), which tries to improve exploration via entropy regulariza-

tion to improve hyper parameter robustness and prevent convergence to bad local optima while

accelerating policy learning overall (Haarnoja et al. 2018).

The current work is complimentary to this literature since we provide a principled way of fac-

toring in known constraints and immediate reward explicitly in training, as opposed to having

to implicitly infer/learn them. Additionally, our framework, as mentioned earlier, can be viewed

as replacing the actor in the actor-critic method with a mixed-integer program. These aspects of

our proposed approach potentially help address the known issues with actor critic methods, such

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

7

as reducing the sample complexity, improving robustness and reducing the risk of convergence to

poor solutions, and removing the dependence on function approximation of the policy network

which may be inaccurate due to over or under ﬁtting or sampling from the data (e.g., due to

under-exploration or sub-optimal convergence).

RL for inventory management: Early work that shows the beneﬁts of RL for multi-echelon

inventory management problems include Van Roy et al. (1997), Giannoccaro and Pontrandolfo

(2002), Stockheim et al. (2003). There has been a recent surge in using DNN-based reinforcement

learning techniques to solve supply chain problems (Gijsbrechts et al. 2018, Oroojlooyjadid et

al. 2021, Sultana et al. 2020, Hubbs et al. 2020). A DNN-based actor-critic method to solve the

inventory management problem was studied in Gijsbrechts et al. (2018) for the case of single

node lost sales and dual sourcing settings, as well as multi-echelon settings, and showed improved

performance in the latter setting. Oroojlooyjadid et al. (2021) show how RL can be used to solve

the classical bear game problem where agents in a serial supply chain compete for limited supply.

More recently Sultana et al. (2020) use a multi-agent actor-critic framework to solve an inventory

management problem for a large number of products in a multi-echelon setting. Similarly, Qi et

al. (2020) develop a practical end-to-end method for inventory management with deep learning.

Hubbs et al. (2020) show the beneﬁt of RL methods over static policies like base stock in a serial

supply chain for a ﬁnite horizon problem. We also refer the interested readers to an excellent

overview and roadmap for using RL for inventory management in Boute et al. (2021). Unlike these

papers, we adopt a mathematical programming-based RL actor and show the beneﬁt over vanilla

DRL approaches in the inventory management setting.

Mathematical programming (MP) based RL actor: MP techniques have recently been used for

optimizing actions in RL settings with DNN-based function approximators and large action spaces.

They leverage MP to optimize a mixed-integer (linear) problem (MIP) over a polyhedral action

space using commercially available solvers such as CPLEX and Gurobi. A number of papers show

how trained ReLU-based DNNs can be expressed as an MP with Tjandraatmadja et al. (2020),

Anderson et al. (2020) also providing ideal reformulations that improve computational eﬃciencies

with a solver. Ryu et al. (2019) propose a Q-learning framework to optimize over continuous action

spaces using a combination of MP and a DNN actor. Delarue et al. (2020), van Heeswijk and

La Poutr´e (2019), Xu et al. (2020) show how to use ReLU-based DNN value functions to optimize

combinatorial problems (e.g., vehicle routing) where the immediate rewards are deterministic and

the action space is vast. In this paper, we apply it to inventory management problems where the

rewards are uncertain.

8

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

2. Model and Performance Metrics

Model and notation: We consider an inﬁnite horizon discrete-time discounted Markov decision

process (MDP) with the following representation: states s ∈ S, actions a ∈ A(s), uncertain random
variable D ∈ Rdim with probability distribution P (D = d|s) that depends on the context state

s, reward function R(s, a, D), distribution over initial states β, discount factor γ and transition

dynamics s(cid:48) = T (s, a, d) where s(cid:48) represents the next state. A stationary policy π ∈ Π is speciﬁed

as a distribution π(.|s) over actions A(s) taken at state s. Then, the expected return of a policy

π ∈ Π is given by J π = Es∼βV π(s) where the value function is deﬁned as

V π(s) =

∞
(cid:88)

t=0

E [γtR(st, at, Dt)|s0 = s, π, P, T ] ,

where the expectation is taken over both the immediate reward as well as the transition state.

The optimal policy that maximizes the long term expected discounted reward is given by π∗ :=

arg maxπ∈Π J π . Finally, by Bellman’s principle, the optimal policy is a unique solution to the

following recursive equation:

V π∗

(s) = max
a∈A(s)

(cid:104)

ED

R(s, a, D) + V π∗

(cid:105)
(T (s, a, D))

.

(1)

As discussed in §1.2, solving (1) directly is computationally intractable due to the curse of dimen-

sionality. We take a hybrid approach where a model determines the immediate reward but value-

to-go is model-free and is determined using trial-and-error in a simulation environment. We discuss

the algorithm in detail next.

2.1. Algorithm

We propose a monte-carlo simulation based policy-iteration framework where the learned policy

is the outcome of a mathematical program which we refer to as PARL: Programming Actor Rein-

forcement Learning (see Algorithm 1 and an illustrative block diagram (Figure 1).

As is common in RL based methods, our framework assumes access to a simulation environment

that generates state transitions and rewards, given an action and a current state. PARL is initialized

with a random policy. The initial policy is iteratively improved over epochs with a learned critic

(or the value function). In epoch j, policy πj−1 is used to generate N sample paths, each of length

T. At every time step, a tuple of {state (sn
from the environment that is then used to estimate the value-to-go function ˆV πj−1

t ), next-state (sn

t ), reward (Rn

t+1)} is also generated
. The value-to-go

θ

function is represented using a neural network parametrized by θ which is generated by solving the

following error minimization problem:

min
θ

N
(cid:88)

T
(cid:88)

(cid:16)

n=1

t=1

Rcum,n
t

− ˆV πj

θ (sn
t )

(cid:17)2

,

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

9

Figure 1

Block diagram illustration of the PARL algorithm

where the target variable Rcum,n
i is the cumulative discounted reward from the state
at time t in sample path n, generated by simulating policy πj−1. Once a ˆV is estimated, the new

i=t γi−tRn

t

:= (cid:80)T

policy using the trained value-to-go function is simply

πj(s) = arg max
a∈A(s)

(cid:104)

ED

R(s, a, D) + γ ˆV πj−1

θ

(T (s, a, D))

(cid:105)

.

(2)

Problem (2) resembles Problem (1) except that the true value-to-go is replaced with an approx-

imate value-to-go. Since, each iteration leads to an updated subsequent improved policy, we call it

a policy iteration approach. Next, we discuss how to solve Problem (2) to get an updated policy

in each iteration.

Problem (2) is hard to solve because of two main reasons. First, notice that ˆV πj−1 is a neural

network which makes enumeration based techniques intractable, especially for settings where the

actions space is large and combinatorial. And second, the objective function involves evaluating

expectation over the distribution of uncertainty D that is analytically intractable to compute. We

next discuss how PARL addresses each of these complexities.

2.2. Optimizing over a neural network

We ﬁrst focus on the problem of maximization of the objective in (2). To simplify the problem we

start by considering the case where demand D is deterministically known to be d. Then, Problem

(2) can be written as

max
a∈A(s)

R(s, a, d) + γ ˆV πj−1

θ

(T (s, a, d)) ,

10

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

where we have removed the expectation over the uncertain demand D. Notice that the decision

variable, a is an input to the value-to-go estimate represented by ˆV . Hence, optimizing over a

involves optimizing over a neural network which is non-trivial. We take a math programming based

approach to solve this problem. First, we assume that the value-to-go function is a trained K-layer

feed forward ReLU-network with input state s and satisﬁes the following equations ∀k = 2, · · · , K,

z1 = s, ˆzk = Wk−1zk−1 + bk−1, zk = max{0, ˆzk}

ˆVθ(s) := cT ˆzK .

Here, θ = (c, {(Wk, bk)}K−1

k=1 ) are the parameters of the value-to-go estimator. Particularly, (Wk, bk)

are the multiplicative and bias weights of layer k and c is the weight of the output layer (see Figure

2). Finally, ˆzk, zk denotes the pre- and post-activation values at layer k. ReLU activation at each

neuron allows for a concise math programming representation that uses binary variables and big-M

constraints (Ryu et al. 2019, Anderson et al. 2020). For completeness, we brieﬂy describe the steps.

Figure 2

A representative NN that takes as an input, a 6-dimensional state space, and considers a 10 layer NN

(including the output layer) with each internal layer containing 9 neurons. Each neuron is deﬁned by

weights (W) and bias (b), except for the output layer that is deﬁned with parameter vector c. The

output of each neuron uses ReLU activation and passes it as an input to the neurons in the subsequent

layer.

Consider a neuron in the neural-network with parameters (w, b). For example, in layer k neuron

i’s parameters are (W i

k, bi

k). Assuming a bounded input x ∈ [l, u], the output z of that neuron can

be obtained by solving following MP representation:

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

P(w, b, l, u) := arg max

0

x,z,y
z ≥ wT x + b

z ≥ 0

z ≤ wT x + b − M −(1 − y)

z ≤ M +y

x ∈ [l, u]

y ∈ {0, 1} .

11

(3)

Here,

M + = max
x∈[l,u]

wT x + b & M − = min
x∈[l,u]

wT x + b ,

are the maximum and minimum outputs of the neuron for any feasible input s. Note that M + and

M − can be easily calculated by analyzing the component-wise signs on w. For example, let

(cid:40)

˜ui =

if wi ≥ 0
ui
li, otherwise.

Similarly, let

(cid:40)

if wi ≥ 0
li
ui, otherwise.
Then, simple algebra yields that M + = wT ˜u + b and M − = wT ˜l + b. starting with the bounded

˜li =

input state s, the upper and lower bounds for subsequent layers can be obtained by assembling the

max{0, M +} and max{0, M −} for each neuron from its prior layer. We will refer to them as [lk, uk]

for every layer k. This MP reformulation of the neural network that estimates the value-to-go is

crucial in our approach. Particularly, since we can also represent the immediate reward directly in

terms of the decision a, and the feasible action set for any state is a polyhedron, we can now use

the machinery of integer programming to solve Problem (2). We will make this connection more
precise in §3 in the context of inventory management.

Next, we discuss how to tackle the problem of estimating expectation in Problem (2).

2.3. Maximizing expected reward with a large action space:

The objective in Problem (2) has an expectation that is taken over the uncertainty D. Note that the

uncertainty in D impacts both the immediate reward as well as the value-to-go via the transition

function. Evaluating this expectation could be potentially hard since D generally has a continuous

distribution and we use a NN based value-to-go approximator. Hence, we take a SAA approach

(Kim et al. 2015) to solve it. Let d1, d2, ..dη denote η independent realizations of the uncertainty

D. Then, SAA posits approximating the expectation as follows

ED[R(s, a, D) + γ ˆV πj−1

θ

(T (s, a, D))] ≈

1
η

η
(cid:88)

i=1

R(s, a, di) + γ ˆV

πη
j−1
θ

(T (s, a, di)) .

12

Hence, Problem (2) becomes

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

ˆπη
j (s) = arg max
a∈A(s)

1
η

η
(cid:88)

i=1

R(s, a, di) + γ ˆV

πη
j−1
θ

(T (s, a, di)) .

(4)

Problem (4) involves evaluating the objective only at sampled points instead of all possible

realizations. Assuming that for any η, the set of optimal actions is non empty, we show that as

the number of samples, η grows, the estimated optimal action converges to the optimal action. We

make this statement precise in Proposition 1. The proof follows through standard results in the

analysis of SAA and is provided in Appendix §A.

Proposition 1. Consider epoch j of the PARL algorithm with a ReLU-network value function

estimate ˆV πj−1

θ

(s) for some ﬁxed policy πj−1. Suppose πj, ˆπη

j are the optimal policies as described

in Problem (2) and its corresponding SAA approximation respectively. Then, ∀ s,

lim
η→∞

ˆπη
j (s) = πj(s).

Proposition 1 shows that the quality of the estimated policy improves as we increase the number

of demand samples. Nevertheless, the computationally complexity of the problem also increases

linearly with the number of samples: for each demand sample, we represent the DNN based value

function estimation using binary variables and the corresponding set of constraints.

Algorithm 1 PARL

1: Initialize with random actor policy π0.
2: for j ∈ [ ˙T ] do

3:

4:

5:

6:

7:

for (epoch) n ∈ [N ] do

Play policy πj−1 for T (1 − (cid:15)) and random action for (cid:15)T steps starting with state sn
Let Rcum,n
t

i and store tuple {sn

} ∀t = 1, .., T .

t ,Rcum,n

i=t γi−tRn

= (cid:80)T

t

0 ∼ β.

end for

Approximate a DNN value function approximator by solving

ˆVj = arg min

θ

N
(cid:88)

T
(cid:88)

n=1

t=1

(Rcum,n
t

− f (sn

t , θ))2

8:

Sample η realizations of the underlying uncertainty D and obtain a new policy (as a lazy

evaluation, as needed) by solving Problem (4)).

9: end for

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

13

Remark 1 (Incorporating Extra Information on the Underlying Uncertainty ).

In many settings, the decision maker might have extra information on the underlying uncertainty

D. In these cases, one can use specialized weighting schemes to estimate the expected value

to go for diﬀerent actions, given a state. For example, consider the case when the uncertainty

distribution P (D = d) is known and independent across diﬀerent dimensions. Let q1, q2, ..qη denote

η quantiles (for example, evenly split between 0 to 1). Also let Fj & fj, ∀j = 1, 2.., dim, denote

the cumulative distribution function and the probability density function of the uncertainty D in

each dimension respectively. Let dij = F −1

j

(qi) & wij = fj(qi), ∀i = 1, 2, .., η, j = 1, 2.., dim denote

the uncertainty samples and their corresponding probability weights. Then, a single realization of

the uncertainty is a dim dimensional vector di = [di1, .., di,dim] with associated probability weight
wpool
i = wi1 ∗ wi2.. ∗ wi,dim. With η realizations of uncertainty in each dimension, in total there are
ηdim such samples. Let Q = {di, wpool
} be the set of demand realizations sub sampled from this

i

set along with the weights (based on maximum weight or other rules) such that |Q| = η. Also let
wQ = (cid:80)

. Then Problem (4) becomes

i∈Q wpool

i

ˆπη
j (s) = arg max
a∈A(s)

(cid:18)

(cid:88)

wi

di∈Q

R(s, a, di) + γ ˆV

πη
j−1
θ

(cid:19)

(T (s, a, di))

,

(5)

where wi = wpool

i

/wQ. The computational complexity of solving the above problem remains the same

as before but since we use weighted samples, the approximation to the underlying expectation

improves.

3.

Inventory Management Application

We now describe the application of PARL to an inventory management problem. We consider a ﬁrm

managing inventory replenishment and distribution decisions for a single product across a network

of stores (also referred to as nodes) with goal to maximize proﬁts while meeting customer demands.

As we will discuss next, our objective is to account for practical considerations in the inventory

replenishment problem by accounting for (i) lead times in shipments; (ii) ﬁxed and variable costs

of shipment; and (iii) lost sales of unfulﬁlled demand. These problems are notoriously hard to
solve and there are very few results on the structure of the optimal policy (see §1.2), but as we

describe next, RL and particularly, PARL can be used to generate well performing policies on

these problems.

3.1. Model and System Dynamics

The ﬁrm’s supply chain network consists of a set of nodes (Λ), indexed by l. For example, these

nodes can denote the warehouses, distribution centers or retail stores of the ﬁrm. Each node in
the supply chain network can produce inventory units (denoted by random variable Dp

l ) and/or

14

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

generate demand (denoted by random variable Dd

l ). Inventory produced at each node can be stored

at the node, or it can be shipped to other nodes in the supply chain. For node l, Ol denotes the

upstream nodes that can ship to node l. At any given time, each node can fulﬁll demand based

on the available on-hand inventory at the node. We assume that any unfulﬁlled demand is lost

(lost-sales setting).

The ﬁrm’s objective is to maximize revenue (or minimize costs) by optimizing diﬀerent fulﬁll-

ment decisions across the nodes of the supply chain. Fulﬁllment at any node can happen both

through transshipment between nodes in the supply chain, or from an external supplier. Each

trans-shipment from node l to l(cid:48) has a deterministic lead time Lll(cid:48) ≥ 0 and is associated with a ﬁxed

cost Kll(cid:48) and a variable cost Cll(cid:48). The ﬁxed cost could be related to hiring trucks for the shipment,

while the variable cost could be related to the physical distance between the nodes. Each node l

has a holding cost of hl. Each inventory unit sold at diﬀerent nodes generates a proﬁt (equivalently

revenue) of pl.

We discuss the system dynamics in detail next. Note that the dependence on the time period t

is suppressed for ease of exposition.

1. In each period t, the ﬁrm observes I, the inventory pipeline vector of all nodes in the supply

chain. The inventory pipeline vector for node l stores the on-hand inventory as well as the inventory

that will arrive from upstream nodes. By convention, we denote I 0

l to be the on-hand inventory at

node l.

2. The ﬁrm makes trans-shipment decision xl(cid:48)l which denotes the inventory to be shipped from

node l(cid:48) to l and incurs a trans-shipment cost (tsc) of

tscl =

(cid:88)

l(cid:48)∈Ol

(cid:2)Kl(cid:48)l1xl(cid:48)l>0 + Cl(cid:48)lxl(cid:48)l

(cid:3) ,

for node l.

3. The available on hand-inventory at each node is updated so as to account for units that are

shipped out, as well as units that arrive from other nodes, and units that are produced at this
node. We let ˜I 0

l denote this intermediate on-hand inventory, which is given by

˜I 0
l = I 0

l + I 1

l + Dp

l +

(cid:88)

l(cid:48)∈Ol

xl(cid:48)l1Ll(cid:48)l=0 −

(cid:88)

xll(cid:48) .

{l(cid:48)∈Λ|l∈Ol(cid:48) }

4. The stochastic demand at each node, Dd
l

is realized and the ﬁrm fulﬁlls demand from the

intermediate on-hand inventory, generating a revenue from sales (rs) of

rsl = pl min{Dd

l , ˜I 0

l } .

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

15

5. Excess inventory (over capacity) gets salvaged, and the ﬁrm incurs holding costs on the left-

over inventory. The ﬁrm incurs holding-and-salvage costs (hsc) given by

hscl = hl min

(cid:26)

¯Ul,

(cid:104) ˜I 0

l − Dd

l

(cid:105)+(cid:27)

+ δl[ ˜I 0

l − Dd

l − ¯Ul]+ ,

where ¯Ul is the storage capacity of node l. We let I (cid:48)0

l

:= min

(cid:26)

¯Ul,

(cid:104) ˜I 0

l − Dd
l

(cid:105)+(cid:27)

for ease of notation.

6. Finally, inventory gets rotated at the end of the time period:

l = I j+1
I (cid:48)j

l +

(cid:88)

l(cid:48)∈Ol

xl(cid:48)l1Ll(cid:48)l=j, ∀ 1 ≤ j ≤ max
l(cid:48)∈Ol

Ll(cid:48)l .

The rotated inventory becomes the inventory pipeline for the next time period. That is, It+1 = I(cid:48)
t.

The problem of maximizing proﬁts can be now written as a MDP. In particular, the state space

s is the pipeline inventory vector I; the action space is deﬁned by the set of feasible actions; the

transition function T is deﬁned by the set of next-state equations (which depend on the distribution

of demand Dd

l ); and the reward from each state is the proﬁt minus the inventory holding and trans-

shipment costs in each period. We can write the optimization problem using Bellman recursion.

Let

Rl(I, x, D) = rsl − tscl − hcl ,

(6)

denote the revenue per node as a function of the pipeline inventory, the trans-shipment decisions,

and the stochastic demand and production. Then, the total revenue generated from the supply

chain in each time period is

∗
R(I, x, D) =

(cid:88)

Rl(I, x, D) .

l∈Λ

Similarly, the optimization problem can be written as

V (I) = max
x∈A(I)

ED[

∗
R(I, x, D) + γV(I(cid:48))] ,

(7)

(8)

where I(cid:48) is implicitly a function of I, x and D. As discussed before, this inventory replenishment

problem takes exactly the same form as the general problem of §2. It can be solved using Bellman

recursion which is unfortunately computationally intractable. Hence, in what follows we will discuss

how the framework developed in §2.1 can be eﬀectively used to solve such problems.

16

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

3.2. PARL for Inventory Management

Recall that the PARL algorithm solves Problem (2) to estimate an approximate optimal policy. In

the inventory management context, this problem becomes

π(I) = arg max

x∈A(I)

1
η

η
(cid:88)

i=1

∗

R(I, x, di) + γ ˆV (I(cid:48)) ,

where as discussed before the next state I(cid:48) is a function of the current state, action as well as the

demand realization. We describe how this problem can be written as an integer program. First,

since we consider the lost-sales inventory setting, we deﬁne auxiliary variables sali that denotes

the number of units sold for node l and demand sample i. Then, constraints

sali ≤ dd
li

sali ≤ ˜I 0
li ,

(9)

ensure that the number of units sold are less than the inventory on-hand and demand. Note that

with discounted reward and time-invariant prices\costs, opportunities for stock hedging in future

time periods due to the presence of a sales variable are not present, and that sales will exactly be

the the minimum of demand and inventory. We also deﬁne auxiliary variables Bli that denotes the

number of units salvaged at node l for demand sample i. Then, constraints

˜I 0
li = I 0

l + I 1

l + dp

li +

(cid:88)

l(cid:48)∈Ol

xl(cid:48)l1Ll(cid:48)l=0 −

(cid:88)

xll(cid:48)

{l(cid:48)∈Λ|l∈Ol(cid:48) }

li − sli − Bli

(cid:48)0
li = ˜I 0
I
(cid:48)j
li = I j+1
I

l +

(cid:88)

l(cid:48)∈Ol

xl(cid:48)l1Ll(cid:48)l=j,

∀ 1 ≤ j ≤ max
l(cid:48)∈Ol

Ll(cid:48)l,

(10)

Bli ≥ 0 ,

capture the next state transition, for each demand realization. Finally, the objective function

has two components: the immediate reward, and the value-to-go. The immediate reward, in terms

of the auxiliary variables can be written as

Rl(I, x, di) = plsali
(cid:124) (cid:123)(cid:122) (cid:125)
rsl

−

(cid:88)

l(cid:48) ∈Ol
(cid:124)

[Kl(cid:48) lgl(cid:48) l + Cl(cid:48) lxl(cid:48) l]

(cid:123)(cid:122)
tscl

(cid:125)

− (hlI
(cid:124)

(cid:48)0
li + δBli)
(cid:125)

(cid:123)(cid:122)
hscl

.

Note that gl(cid:48)l is a binary variable that models the ﬁxed cost of ordering. Next, let I(cid:48)

i denote the
next state under the ith demand realization. Assume that the NN estimator for value-to-go has Ψ

fully connected layers with Nk neurons in each layer with ReLU activation. Let neujk denote the

jth neuron in layer k. Then, the outcome from the neurons in the ﬁrst layer can be represented as

(zj1, yj1) := P(W1k, b1k, I(cid:48)

i, I(cid:48)

i), ∀j ∈ [1, .., N1] .

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

17

The output from this layer becomes the input of the next layer. Hence, let Z1:= [z11, z12, ..z1N1]
denote the outcome of layer 1. Then, the output of each neuron of layer 2 can be now written as

(zj2, yj2) := P(W2k, b2k, Z1, Z1), ∀j ∈ [1, .., N2] .

Note that we have suppressed Z1 from the outcome to de-clutter notation since it is ﬁxed as an

input to the problem (see Problem (3) for details). Continuing this iterative calculation, we have

that ∀k = 2, ..., Ψ − 1

(zjk, yjk) := P(W2k, b2k, Zk−1, Zk−1), ∀j ∈ [1, .., Nk] .

Finally, letting c denote the weight vector of the output layer, we have that

V (I(cid:48)

i) = c(cid:62)ZNΨ ,

where note that the value-to-go is implicitly a function of the decisions x since they impact the

next-state I(cid:48). Using this value-function approximation, the inventory fulﬁllment problem for each

time period can be now written as

max
xl(cid:48)l∈Z+,UL≤x≤UH

1
n

where

∗
R(I, x, di) =

n
(cid:88)

(cid:20) ∗
R(I, x, di) + γcT zΨi

(cid:21)

i=1
(cid:88)

Rl(Il, xl, dli)

l∈Λ
Rl(Il, xl, dli) = rsl + tscl + hscl

sali ≤ dd

li ∀ l ∈ Λ, i,

sali ≤ ˜I 0

li ∀ l ∈ Λ, i,

gll(cid:48) ≤ xll(cid:48)

∀ l ∈ Ol(cid:48) , l(cid:48) ∈ Λ,

xll(cid:48) ≤ U H
˜I 0
li = I 0

ll(cid:48) gll(cid:48)
l + I 1

∀ l ∈ Ol(cid:48), l(cid:48) ∈ Λ,
(cid:88)

l + dp

li +

xl(cid:48)l1Ll(cid:48)l=0 −

(cid:88)

xll(cid:48),

∀ l ∈ Λ, i,

(cid:48)0
li = ˜I 0li − sli − B0
li,
I
(cid:48)j
li = I j+1

l +

(cid:88)

xl(cid:48) l

I

l(cid:48) ∈Ol

l(cid:48)∈Ol

{l(cid:48)∈Λ|l∈Ol(cid:48) }

∀ l ∈ Λ, i,

1L

(cid:48)

l

l

=j − Bj
li,

∀ 1 ≤ j ≤ max
l(cid:48) ∈Ol

Ll(cid:48) l, l ∈ Λ, i,

Bj

li ≥ 0,

∀ j = 0, · · · , max
l(cid:48)∈Ol

Ll(cid:48)l, l ∈ Λ, i,

(cid:48)
(z2qi, y2qi) ∈ P(W1q, b1q, I

i, I

(cid:48)

i) ∀q ∈ N1,

(zk+1,qi, yk+1,qi) ∈ P(Wkq, bkq, zk,i, zk,i) ∀q ∈ Nk, k = 2, · · · , Ψ − 1.

(11a)

(11b)

(11c)

(11d)

(11e)

(11f)

(11g)

(11h)

(11i)

(11j)

(11k)

(11l)

(11m)

18

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

Remark 2 (Alternate Q-Learning Based Approach: ). We note that

the proposed
PARL methodology is based on using the value function estimate ˆV (s) to optimize actions. Alter-

natively, one could consider solving the following problem

πj(s) = arg max
a∈A(s)

ˆQπj−1(s, a) ,

(12)

where ˆQπj−1(s, a) is the estimated expected discounted reward from selecting action a when in

state s. We decided not to pursue the Q-learning approach due to multiple reasons. First and

foremost is because of the larger size of the neural network needed to estimate the Q values. In

particular, assume that the supply chain network is fully connected and that in each time period,

the centralized planner decides on the number of units to be shipped from each supply chain
node to another. Then, the action decision a ∈ R|Λ|2. Hence, the input layer of the neural network

approximating the Q-function will be of size |I| + |Λ|2. In comparison, the the input layer in the

value function based approach we propose is of size |I|, which is much smaller. Note that larger

the input space, larger is the DNN network size used for approximation. Since the number of

integer variables increase linearly with the network size, smaller networks are preferred to ensure

computational feasibility. Second, in a Q-learning based approach, directly modeling the immediate

reward information is infeasible unless one solves two optimization problems per action step. Next,

we present results from various numerical experiments using the proposed algorithm.

4. Numerical Results

In this section we present numerical results on the performance of the proposed PARL algorithm on

various supply chain settings. The objective is two-fold: (i) to benchmark the proposed algorithm

and compare its performance with state-of-the-art RL and inventory management policies, and (ii)

to discuss the usage of a open-source Python library to create inventory management simulation

environments with implementation of various RL algorithms, for easy benchmarking in supply

chain applications.

We model a representative supply chain network in this environment with at most three diﬀerent

node types:

• Supplier: Supplier node (denoted by S) in the network produce (or alternatively order) inven-

tory to be distributed across the network. For example, these can be big or manufacturing sites

that produce inventory units or a port where inventory from another country lands.

• Retailer: Retailer nodes (denoted by R) in the network consume inventory by generating

demand for the product. For example, these can be retail stores that directly serve customers who

are interested in purchasing the inventory units.

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

19

Figure 3

Example of diﬀerent multi-echelon supply chain networks. In 1S-3R, a single supplier node serves a

set of 3 retail nodes directly. In 1S-2W-3R, the supplier node serves the retail nodes through two

warehouses. In 1S-2W-3R (dual sourcing), each retail nodes can is served by two distributors.

• Warehouse: Warehouse nodes (denoted by W) are intermediate nodes that connect supplier

node to retailer nodes. They hold inventory and ship it to downstream retail nodes.

Each of the nodes are associated with holding costs, holding capacities and spillage costs, while

retailers are additionally associated with price, demand uncertainties and a lost-sales/backorder

demand type, and suppliers with production uncertainties. The directed link between the nodes

forms the supply chain network. Each link is associated with order costs, lead time and maximum

order quantity. The environment executes on the ordering and distribution actions speciﬁed by the

policy by ﬁrst ensuring its feasibility using a proportional fulﬁllment scheme (as it cannot send

more than the inventory in a node), samples the uncertainties, accumulates the reward (the revenue

from fulﬁllment less the cost of ordering and holding), and returns the next state.

With this overview, we start by ﬁrst describing the various supply chain networks that we will

consider for the numerical study.

4.1. Network structure and settings used for numerical experiments

We consider 3 diﬀerent multi-echelon supply chain network structures, inspired from real-world

retail distribution networks.

1. Two-echelon networks: This network consists of one supplier that is connected with a set of

heterogeneous retailers that diﬀer from one another in terms of the holding costs, as well as the

lead time to ship from the supplier to the retailer. We consider networks with varying number of

retailers (3, 10 and 20 retailers), as well as a setting with high-versus-low production. This is done

so as to test the performance of the algorithms in resource constrained settings where inventory

units available to be shipped could be lesser than the demand generated across the network. Hence,

the policy has to decide where to ship available inventory units. This setting is particularly inspired

from fast-fashion retail where the number of units per stock keeping unit (SKU) is very low. These

settings are henceforth referred to as the 1S-mR networks. We speciﬁcally study the 1S-3R, 1S-10R,

1S-20R in the low production setting and 1S-3R-High for the relatively higher production setting.

20

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

2. Tree-distribution networks: We study distribution networks with a tree-structure, speciﬁcally

networks consisting of one supplier, two intermediate warehouses, and three retailers that are each

served from one of the two warehouses. This models networks where in retailers own big warehouses

that are geographically dispersed with each serving a set of retailer stores nearby. We analyze

two diﬀerent settings: 1S-2W-3R where the supplier inventory is constrained, and 1Sinf-2W-3R which

is the more traditional inﬁnite inventory setting, where the supplier is not constrained by the

inventory. Note that only when the supplier inventory is inﬁnite, denoted by Sinf, we do not include

its inventory level as part of the state space.

3. Distribution networks with dual-sourcing: This setting is similar to the prior setting, except

that the retail store is connected to not a single, but to multiple warehouses, and in this case two.

It models multi-echelon supply chains where retailers have big warehouses, sometimes farther away

from demand centers with longer lead times and smaller regional warehouses that also serve the

demand with shorter lead times but relatively higher costs. Hence, the retailer has to decide not

only how much inventory to store, but also where to ship this inventory from amongst the two

warehouses. We refer to this settings as 1S-2W-3R (DS).

In Table 1 we present diﬀerent parameters of the supply chain in each setting. We discuss some

salient features of these settings below and we follow it by additional detail related to notations

and other speciﬁcs to better understand the table.

1. Highly uncertain demand: In many real world settings (e.g., fast-fashion retail), demand for

products is highly uncertain. Hence, we let demand distribution to have a low signal-to-noise ratio.

2. Non-symmetric retailers: The retailer node parameters are selected so that they diﬀer in

terms of (i) the holding costs (ii) the lead time from the upstream warehouse. This asymmetry

implies that commonly used heuristics such as learning an approximate policy for one retailer, and

identically applying it to other retailers would be highly sub-optimal.

3. Fixed ordering costs, holding capacities, non-zero leadtimes and lost sales: All these set-

tings are very common in practice. For example, in retail B2C distribution networks, retailers lose

sales opportunities if the item is not in the shelf. There is also limited shelf space compared to

upstream warehouses. Furthermore, these are settings where optimal policies structures have not

been characterized and analytical tractability is not guaranteed.

In all the settings, we assume deterministic, constant per-period production and that the vari-

ability is only in the demand. The parameters are provided by node type - Retailer (R), Supplier

(S or Sinf) and Warehouse (W) - and then by links between them. Whenever they are provided in

a list format, they correspond to the retailers and warehouses in a chronological order (i.e., R1,

R2, R3 or W1, W2). Also, when there are more nodes or links than the parameters (few elements in

the list speciﬁed in the table), it implies that the parameters list in repeated in a cyclic fashion.

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

21

Parameters

1S-3R-High 1S-3R

1S-10R

1S-20R

1S-2W-3R 1S-2W-3R (DS)

1Sinf-2W-3R

Retailer demand distribution

[N(2,10)]

[N(2,10)] [N(2,10)] [N(2,10)] [N(2,10)] [N(2,10)]

[N(2,10)]

Retailer revenue per item

[50]

[50]

[50]

[50]

[50]

[50]

Retailer holding cost

[1,2,4]

[1,2,4]

[1,2,4,8]

[1,2,4,8]

[1,2,4]

[1,2,4]

Retailer holding capacity

[50]

Supplier production qty per step

15

Supplier holding capacity

Warehouse holding cost

Warehouse holding capacity

100

-

-

[50]

10

100

-

-

[50]

25

150

-

-

[50]

40

300

-

-

Spillage cost at S,W,R

[10]

[10]

[10]

[10]

[50]

10

100

[0.5]

[150]

[10]

[50]

10

100

[0.5, 0.1]

[150]

[10]

[50]

[1,2,4]

[50]

100

500

[0.5]

[150]

[10]

Lead time (S or W to R)

[1,2,3]

[1,2,3]

[1,2,3]

[1,2,3]

[1,2,3]

[(1,5),(2,6),(3,7)] [1,2,3]

Lead time (S to W)

Fixed order cost (S or W to R)

Fixed order cost (S to W)

Variable order cost (any link)

Maximum order (any link)

Initial inventory distribution
(node or link)

-

[50]

-

[0]

[50]

-

[50]

-

[0]

[50]

-

[50]

-

[0]

[50]

-

[50]

-

[0]

[50]

[2]

[50]

[0]

[0]

[50]

[2]

[50]

[0]

[0]

[50]

[2]

[50]

[0]

[20] for S-W

[50]

[U(0,4)]

[U(0,4)]

[U(0,4)]

[U(0,4)]

[U(0,4)]

[U(0,4)]

[U(0,4)]

Table 1

Environment parameters for diﬀerent supply chains studied.

For example the lead time (S or W to R) for the environment 1S-10R is given by [1,2,3] and this

implies that the lead time for links [S-R1, S-R2,....,S-R10] is (1,2,3,1,2,3,1,2,3,1). The notation for

the distributions used are N (µ, σ) for a normal distribution with mean µ and standard deviation σ

and U (a, b) discrete uniform between a and b. Note that because demand is discrete and positive,

when we use a normal distribution, we round and take the positive parts of the realizations. The

lead-time list has a tuple representation in the dual-sourcing setting to represent the lead time of

a retailer from the two diﬀerent warehouses. For example (1,5) in the list represents the lead time

for W1-R1 and W2-R2.

Next, we discuss diﬀerent benchmark algorithms that we tested in this paper.

4.2. Benchmarks

We compare PARL with four state-of-the-art, widely used RL algorithms: PPO (Schulman et al.

2017), TD3 (Fujimoto et al. 2018), SAC (Haarnoja et al. 2018), and A2C (Mnih et al. 2016).

For the RL algorithms we used the tested and reliable implementations provided by Stable-

Baselines3 (Raﬃn et al. 2019), under the MIT License. We made all our environment compatible

with OpenAI Gym (Brockman et al. 2016) and to implement PARL we built on reference implemen-

tations of PPO provided in SpinningUp (Achiam 2018) (both MIT License). We ran RL baselines

on a 152 node X 26 (average) CPU cluster (individual jobs used 1 CPU and max <1GB RAM),

and PARL on a 13 nodes X 48 (average) CPU cluster (individual PARL job uses 16 CPUs for

trajectory parallelization and CPLEX computations and average <4GB RAM). We use version

12.10 of CPLEX with a time constraint of 60s per decision step with 2 threads.

22

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

We additionally compare PARL with commonly used order-upto heuristics in supply chains:

a popularly used (s, S) base stock (BS) policy (Scarf 1960), here implemented for each link (see

Appendix B.1 for implementation details); and a decomposition-aggregation (DA) heuristic (Rong

et al. 2017) to evaluate echelon order-up to policies (S, S) for a serial path in near closed form

(see Appendix B.2 for implementation details). The latter heuristic is designed for tree-networks

with backordered demand and no-ﬁxed costs in the presence of inﬁnite supply and it has near-

closed form expressions with assymptotic guarantees under those settings. In the former heuristic,

a simulation based grid search using the environment is performed to identify the best (s, S) pair

for every link assuming inﬁnite supply but with all other complexities (lost sales and ﬁxed costs)

in-tact. In fact, due to these diﬀerences, we believe that the BS heuristic tends to outperform the

DA heuristic in the settings we study, as we will see in the next section.

4.3. Testing Framework

Evaluating RL algorithms can be tricky as most methods have a large number of hyper parameters

or conﬁgurations that could be tuned, and care must also be taken to capture the actual utility

of an RL method via the evaluation and avoid misleading conclusions occurring due to improper

evaluation (Henderson et al. 2018, Agarwal et al. 2021). In particular, since the same simulation

environments are used to train and evaluate (test) a model, if a model is chosen based on its

performance on that environment during a particular training run and the exact same model

(model weights) used to then evaluate on the same environment for test scoring, this can lead to

misleading conclusions. The particular model class and set of hyper parameters may only have

performed so well due to random chance (i.e., the random initialization, action sampling, and

environment transitions) during the particular training run, and due to selecting the best result

throughout training. Nevertheless, this may not at all characterize how well diﬀerent models or

hyper parameters may enable learning good policies in general or work in practice. It is therefore

important to use separate model hyper parameter selection, training and evaluation runs for each

model. Additionally it is important to use multiple runs in each case to characterize the distribution

of results one can expect with a particular algorithm. We follow such rigorous procedures with our

testing framework and in the results reported here.

Speciﬁcally, for a given RL algorithm, we ﬁrst use a common technique of random grid search

over the full set of hyper parameters to narrow down a smaller set of key tuning hyper parameters

and associated grid of values for a given domain (using multiple random runs with diﬀerent random

seeds per hyper parameter combination). Then, for each environment and RL method, we use

multiple random training runs (10 in this paper) for each hyper parameter combination to evaluate

each hyper parameter combination. We then select the set of hyper parameters giving the best

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

23

average result across the multiple random runs as the best hyper parameters to use for that method

and environment. Finally, we perform the evaluation runs. For the selected best hyper parameters

for a method and environment, we run a new set of 10 random training runs with new randomly

instantiated models to get our ﬁnal set of trained models. Then for each of the 10 models we evaluate

the model on multiple randomly initialized episodes of multiple steps (20 episodes and 256 used in

our experiments) to get a mean reward per model run. Then given the set of mean rewards from

multiple model runs, we report the overall mean, median, and standard deviation of mean rewards

for the particular model (model class and set of best hyper parameters) and environment - which

characterizes what kind of performance we would expect to see for that given RL model approach

on that environment, and how it varies through random chance due to random initialization and

actions/ trajectories). This gives a more thorough and accurate evaluation and comparison of the

diﬀerent RL algorithms. Additionally we performed extensive hyper parameter tuning and focused

on bringing out the best performance of the baseline DRL models - searching over 4700 hyper

parameter combinations with the initial random grid search to narrow down smaller grids, and

using ﬁnal grids of 32-36 hyper parameter combinations (varying 3 or more hyper parameters) per

method to perform ﬁnal hyper parameter optimization per environment and method. Note that
more details on the hyper parameter tuning is provided in §4.4.3.

4.4. Performance, policy analysis and parameter tuning

We start by ﬁrst presenting the results of diﬀerent algorithms in diﬀerent settings and then give

more details on the learned policy by diﬀerent RL algorithms, as well as the hyper parameter

tuning performed for each algorithm.

4.4.1. Performance:

In Table 2, we present the average per step reward (over test runs) of

the diﬀerent algorithms and compare them to PARL in 7 diﬀerent settings described earlier. We

additional provide the percentage improvement over two widely used methods: PPO and BS. We

observe that PARL is a top performing method, in fact it outperforms all benchmark algorithms

in all but one setting, 1Sinf-2W-3R. On average across the diﬀerent supply chain settings we study,

PARL outperforms the best performing RL algorithm by 14.7% and the BS policy by 45%.

Notably, the improvements are higher in supply chain settings that are more complex (1S-20R,

1S-10R, 1S-2W-3R and 1S-2W-3R (DS)) amongst the settings tested in the paper. While in the 10R and

20R settings, the retailer has to optimize decisions over a larger network with larger action space,

1S-2W-3R and 1S-2W-3R (DS) are multi-echelon settings with more complex supply chain structure.

Similarly, in the 1S-3R setting, the supplier is more constrained than the 1S-3R-High setting, which

makes the inventory allocation decision more complex. PARL’s ability to explicitly factor in known

24

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

state-dependent constraints enables it to out-perform other methods in these settings. Settings 1S-

3R-High and 1Sinf-2W-3R have relatively high supplier production, where BS and related heuristics

like DA are known to work well. Here PARL is on par with the BS heuristic (within one standard

deviation of the BS heuristic’s performance) and out-performs other RL methods. In these settings,

the combination of surplus inventory availability, coupled with low holding cost and high demand

uncertainty, encourage holding high inventory levels over long time-horizons. This makes reward

attribution for any speciﬁc ordering action harder, and thus these settings are harder to learn for

RL algorithms.

Setting

1S-3R-High

1S-3R

1S-10R

1S-20R

1S-2W-3R

1S-2W-3R (DS)

1Sinf-2W-3R

SAC
478.8 ± 8.5
478.3
398.0 ± 3.2
398.3
870.5 ± 68.9
905.8
1216.1 ± 25.2
1221.3
374.2 ± 3.7
375.0
344.1 ± 20.6
346.1
40.6 ± 59.8
4.2

TD3
374.7 ± 15.7
374.1
329.6 ± 45.2
311.7
744.4 ± 71.4
766.3
1098.0 ± 43.3
1105.8
361.1 ± 15.4
362.7
259.3 ± 32.3
262.8
21.0 ± 57.0
4.26

PPO
499.4 ± 5.7
500.2
397.0 ± 1.6
397.4
918.3 ± 24.7
919.2
1072.6 ± 63.4
1059.3
377.5 ± 3.7
377.5
387.8 ± 5.3
388.9
136.8 ± 18.4
136.0

A2C
490.8 ± 8.9
490.1
392.4 ± 4.4
392.87
768.1 ± 40.5
773.52
1117.3 ± 37.5
1114.4
360.2 ± 23.2
365.3
327.5 ± 32.7
322.61
62.9 ± 33.7
69.1

BS
513.3 ± 5.9
513.0
313.7 ± 3.1
314.3
660.5 ± 2.1
659.9
862.7 ± 3.0
862.9
300.8 ± 5.4
302.2
166.2 ± 3.8
166.4
208.8 ± 5.2
209.9

DA
474.0 ± 4.6
473.5
303.2 ± 2.2
303.8
651.9 ± 1.6
652.4
851.9 ± 1.5
852.2
287.2 ± 1.9
287.1
158.3 ± 1.7
158.0
163.2 ± 4.8
162.6

PARL
514.8 ± 5.3
514.3
400.3 ± 3.3
400.8
1006.3 ± 29.5
1015.7
1379.2 ± 190.1
1434.3
398.3 ± 2.5
399.7
405.4 ± 2.0
405.9
206.1 ± 9.0
207.9

PARL over PPO / BS

3.1% / 0.3%

0.8 % / 27.6%

9.6% / 52.3%

28.5% / 59.9%

5.5% / 32.4%

4.5% / 143.9%

50.6% / -1.3%

Table 2

Average per-step-reward with standard deviation and median (next line) of diﬀerent benchmark

algorithms, averaged over diﬀerent testing runs. We bold all top performing methods: those with performance not

statistically signiﬁcantly worse than the best method, using one standard deviation.

We also analyze the rate of learning of diﬀerent algorithms during training. In Figure 4, we

plot the average per-step reward over training steps from 3 diﬀerent environments. We ﬁnd that

in each case, the PARL actor performs much worse in the initial, very early training steps on

account of optimizing over a poorly trained critic. Once the critic improves in accuracy, PARL is

able to recover a very good policy during training. Furthermore, we generally observe improved

sample complexity of PARL compared to the DRL baselines from these learning curves. That is,

we observe that PARL often converges to its ﬁnal high-reward solution in fewer total steps with

the environment (in some cases much fewer steps) compared to the DRL baselines.

Finally, we report algorithm run-times. The average per-step run time of the PARL algorithm is

0.178, 0.051, 0.050, 0.089, 0.051, 0.044 and 0.042 seconds in the 1S-3R-High, 1S-3R, 1S-10R, 1S-20R,

1S-2W-3R, 1S-2W-3R (DS) and 1Sinf-2W-3R settings, respectively. The average per step run time is

highest in the 1S-3R-High setting. This is due to the larger feasible action set in this setting. In all

other settings, the run time remains below 0.10 seconds. We note that during training, we use 8

parallel environments to gather training trajectories, and use 2 CPLEX threads per environment.

The run time can improve further by increasing parallelization. In contrast, the average per-step

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

25

(a) 1S-3R

(b) 1S-2W-3R

(c) 1S-2W-3R (DS)

Figure 4

Learning curves of PARL and benchmark algorithms during training runs.

run time of PPO (the DRL algorithm that performs the best in most settings) is 0.007, 0.005, 0.010,

0.008, 0.008, 0.008, and 0.007 seconds respectively. Clearly, PPO outperforms PARL in terms of

run-time. This is because while per-step action in PARL is an outcome of an integer-program, DRL

algorithms take gradient steps that are computationally much faster.

4.4.2. Analyzing learned replenishment policy from RL algorithms: The numerical

results from the experiments above show that RL algorithms can provide substantial gains in

complex supply chain settings. Naturally, the next important question is how are RL algorithms

achieving such improved performance and how good are these policies? We answer both these

questions in what follows.

Why are RL algorithms performing better in some settings?:

In Figure 7, we plot the costs,

revenue and reward breakdown of diﬀerent algorithms in diﬀerent supply chain network settings . In

the 1S-3R-High setting (see Figure 5a), we ﬁnd that while PARL incurs lower total costs (ordering

plus holding), vut nevertheless all algorithms perform equally well in terms of the overall reward

since BS and PPO are able to compensate for higher inventory costs with higher revenue. Similar

insights continue to hold in the other high inventory 1Sinf-2W-3R setting (see Figure 5c), with the

exception of the PPO based policy, whose performance deteriorates in comparison to the other

policies. This is on account of lost sales which results in lower revenue and hence rewards. As we

decrease available inventory (see Figure 5b), the performance of the BS policy deteriorates since

it incurs high ordering costs due to proportional fulﬁllment. Note that in this setting, both PARL

and PPO are better able to trade-oﬀ between costs and revenue. As we increase the complexity of

the underlying network, PARL’s performance improves mainly due to lower costs. In particular,

in the largest network with 20 retailers (see Figure 5d), PARL incurs much lower ordering cost in

comparison to the other algorithms. Nevertheless, its holding costs are higher than both the other

algorithms. This in turn implies that PARL makes fewer orders (but with larger order sizes) which

ensure low overall inventory costs. Cost and reward comparison for other settings are provided in
§B of the Appendix.

26

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

(a) 1S-3R-High

(b) 1S-3R

(c) 1Sinf-2W-3R

(d) 1S-20R

Figure 5

Breakdown of rewards in test across BS, PPO and PARL algorithms. Note that ordering costs include

ﬁxed and variable costs of ordering, revenue refers to the revenue earned from sales and reward refers
to the revenue net costs incurred (see §3 and Table 1 for more details).

Bench-marking with known optimal policy: All the settings considered so far in the paper have

no analytical optimal solution. Hence, while PARL performs better than known heuristics, its

performance might still be far from the optimal policy. Therefore, to compare PARL’s performance

with an optimal policy and to easily visualize the learnt policy, in what follows, we consider a

simpliﬁed setting with one retailer and one supplier with inﬁnite inventory (1Sinf-1R) with back-

ordered demands, non-zero lead time, and no-ﬁxed costs (see Appendix B.3 for full details on the

diﬀerent parameters). In this setting, it is well known that the optimal policy is a order up-to base-

stock policy (i.e., order in each period to maintain an inventory position, which is on-hand plus

pipeline, up to this level). The optimal policy is easy to compute as it has a closed form solution.

Therefore, we compare PARL’s learnt policy with the optimal underlying base stock policy. For the
chosen parameters, in this setting, the order up-to-level is 27 (see details of this calculation in §B.3

of the Appendix). In Figure 6, we present the learned policy from two diﬀerent DRL algorithms.

On the x-axis we plot the inventory position (sum of on-hand and pipeline inventory), and on the

y-axis we plot the optimal action taken by the policy. Notice that interestingly, the optimal policy

learned by the DRL algorithms is also an order-up-to policy. While this result is not surprising for

existing DRL algorithms, especially given the recent success of using oﬀ-the-shelf RL methods for

inventory management, the analysis highlights the near optimal performance of PARL and also

demonstrates that the learned optimal policy mimics the optimal parametric policy in this setting.

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

27

Figure 6

Comparing the optimal ordering policy of PARL and SAC in the 1Sinf -1R backorder setting. Here, the

optimal static policy is an order-up to policy with 27 units being the order upto level. Observe that

both PARL and SAC are able to learn the optimal order-up-to policy across various on-hand inventory

states observed in test (99.9% of the time). Higher variance in comparison to SAC for PARL can

be attributed to its deterministic policy structure that optimizes over the learned critic with a four

dimensional input state.

Having discussed the overall performance of the various benchmark algorithms, we discuss hyper-

parameter tuning next, that plays a very important role in the performance of diﬀerent algorithms.

4.4.3. Hyper-parameter tuning of RL algorithms: Given the number of hyperparame-

ters that need to be selected for running RL algorithms, we divide them in two sets: (i) parameters

that were ﬁxed to be the same across diﬀerent benchmark algorithms and (ii) parameters that were

tuned as we trained diﬀerent benchmark algorithms.

Fixed hyper parameters: Here we report the ﬁxed set of hyper parameters used by all methods.

We ﬁx batch size, the NN architecture, the neuron activation function, the state space represen-

tation1, the action representation, as well as the epoch length for generating trajectories. These

were determined based on two factors: (1) the commonly used settings across the RL literature

(for example 64x642 NN architecture and batch size of 64 is most commonly used across many

1 States and actions can be represented in diﬀerent ways, which aﬀect modeling outcomes and so can be treated as a
hyper parameter. Discrete means each possible state (i.e., combination of inventory values) or action (i.e., combination
of order actions per entity) is given a unique index (so a policy network would output a probability for each possible
index - i.e., state or action variable value combination) and a policy network gives a probability for each discrete
action. Multi-discrete means each diﬀerent state or action variable is encoded with its own set of discrete values and
the policy network gives a probability for each state of each variable (e.g., output a probability over a discrete set for
each order action to be taken). Finally, continuous (and normalized continuous) means the state and action variables
are treated as continuous values - so the policy network outputs a single value for each action variable (i.e., each
order action) and the probability is modeled via a continuous distribution, most often Gaussian.
2 64x64 represents a NN with two layers, each of fully connected 64 neurons.

19202122232425Total inventory23456789Order1s1r backorder - Orders vs. total inventoryPARLSAC28

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

diﬀerent problems and methods), and by sampling random combinations from a large grid of hyper

parameters and comparing results trends to narrow down the set of hyper parameters to consider

to consistently well-performing values and reasonable ranges.

As such, this was an iterative process where we tried a range of hyper parameters, then reﬁned.

For example, for the network architecture size, we also tried larger sizes including 128x128, 512x512,

1024x1024, 128x128x128, 512x256, 1024x512, 1024x512x256, 128x32, 512x128, 512x256x64. Nev-

ertheless, larger sizes did not see considerable improvement, but led to signiﬁcant increase in the

run-time. Similarly, we tried batch sizes of 32, 64, and 128. But, we found no signiﬁcant diﬀerences

in the performance and hence selected the most commonly used 64 as the batch size. To select the

activation function, we tested both ReLU and tanh activation. We found that ReLU activation per-

forms as good or better than tanh (overall it gave close but slightly better results). Hence, we ﬁxed

the activation to ReLU across methods for fair comparison, and for ease of implementation with

more eﬃcient optimization methods (Glorot et al. 2011, Hara et al. 2015). Similarly, for the state

and action space representation, we tested discrete, multi-discrete and continuous representations

(normalized between -1 and 1). We found that both continuous state and action representations

work best in these settings.

Finally, we also comment on some algorithm speciﬁc hyper parameters. For the number of internal

training iterations per collected buﬀer (PPO-speciﬁc setting) we tried 10, 20, and 40, and 70 and

found the best results with 10-20, so ﬁxed this parameter to be 20. For the number of steps per

epoch / update (PPO and A2C-speciﬁc setting) we tried 512, 1024 and 2048 and found the larger

number to give better results generally so ﬁxed this to 2048. For PPO, we also found early stopping

the policy update per epoch, based on KL-divergence threshold of default 0.15, to consistently

provide better results than not using this. The ﬁnal set of ﬁxed hyper-parameters used across all

experiments, models, runs, and environments are given in Table 3.

Table 3

Batch size Net

(hidden
layers
net)
64x64

64

arch.

Fixed set of hyper-parameters used for all methods
Action repre-
sentation

Activation State

sentation

repre-

Epoch
length

per

ReLU

continuous
(normalized)

continuous
(normalized)

2048

Hyper-parameter search: Next, we discuss the set of hyper-parameters that were tuned for each
supply chain setting during training for diﬀerent algorithms. Table 6 of Appendix §C describes
the set of parameters for PPO and A2C. Similarly, Table 7 of Appendix §C describes the set of

parameters for SAC and TD3. For PARL, a common set of hyper-parameters were used across all

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

29

settings, with the exception of the discount factor, γ, for the inﬁnite supplier inventory setting,

since we observed universally higher gamma being necessary for the baseline DRL methods (see

Table 5). The discount factor was set to 0.75 (0.99 for the inﬁnite supplier inventory setting only),

learning-rate was set to 0.001, and the sample-averaging approach used was quantile sampling

with 3 demand-samples per step. In each case, this set of hyper parameters was then used for the

evaluation of the RL model - by retraining 10 diﬀerent times with diﬀerent random seeds using

those best hyper parameters for each method, and reporting statistics on the 20-episode evaluations

of the best epoch model across the 10 runs. Besides the parameters mentioned here, all other

parameters were set at their default values in the Stable Baselines 3 implementation (see API3 for

more details). Note that our experiments revealed that standard gamma value of 0.99 consistently

gave much poorer results than smaller gamma values for most environments (we experimented with

0.99, 0.9, 0.85, 0.8 and 0.75) - so we included smaller gamma values in our hyper-parameter grids

but still kept the option of the traditionally used 0.99 gamma for the benchmark RL methods in

case they were able to factor in longer-term impact better. Finally, note, default optimizers are

used: ADAM (Kingma and Ba 2015) for all except A2C which uses RMSprop (Hinton et al. 2012)

by default.

Evaluation of hyper parameters:

In Table 5 of Appendix §C, we show the selected set of best

hyper parameters used for each benchmark RL method and environment. These were selected based

on the parameter setting that gave the best average reward (maximum over the training epochs),

averaged across 10 diﬀerent model runs for that hyper-parameter combination.

4.5. Open source tool for bench-marking

In this section, we brieﬂy discuss the development and usage of an open-source Python library that

can be used to facilitate the development and benchmarking of reinforcement learning on diverse

supply chain problems.4

The library is designed to make it easy to deﬁne arbitrary, customizable supply chain environ-

ments (such as used in this paper) and to easily plug in diﬀerent RL algorithms to test them on the

environments. Diﬀerent supply chain conﬁgurations can easily be deﬁned via code or conﬁguration

ﬁles, such as varying supply chain network structures, capacities, holding costs, lead times, demand

distributions, etc. For example the conﬁguration ﬁle for the library deﬁning the 1S-2W-3R supply

chain environment used in this paper, with one supplier, 2 warehouses and three retailers, under a

lost sales setting is shown in Listing 1.

3 https://stable-baselines3.readthedocs.io/en/master/
4 We plan to release the library as open source upon publication.

30

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

1 [conf_type]
2 conf_type = graph # Specify network via graph or list
3
4 [env_params]
5 env_type = pdr # pdr (define producers, distributors, and retailers) or 1sMr
6 state_rep = N # Normalized continuous state representation
7 action_rep = MD # Multi-discrete action representation
8 quant = 1 # Order action quantization amount
9 reset_max_entity_inv = 4 # Max initial inventory randomly generated on reset
10 reset_max_connection_inv = 4
11 back_order = False #Set to True for back order at retailers setting instead of lost sales
12
13
14 [supply_chain_general_params]
15 max_order_action = 50 # Maximum order amount
16
17 # Next, for each of producers, distributors and retailers, define list of IDs and

associated lists of settings for each

18 # Note: default distribution for all entities is (truncated, rounded) stationary Gaussian
- other distributions can be easily specified and plugged in (Poisson, non-

stationary versions, and inventory-dependent are already available options)

19
20 [supply_chain_producer_params]
21 id_list = P1
22 prod_daily_prod_avg_list = 10 # Mean production per producer
23 prod_daily_prod_std_list = 0. # Std. dev. of production per producer
24 holding_cost_list = 0
25 holding_capacity_list = 100
26 overorder_penalty_list = 0
27 max_start_inv = -1 # if below 0, set equal to prod_holding_cap
28
29 [supply_chain_distributor_params]
30 id_list = D1, D2
31 holding_cost_list = 0.5, 0.1
32 holding_capacity_list = 150, 150
33 overorder_penalty_list = 10, 10
34 max_start_inv = 60, 60
35
36 [supply_chain_retailer_params]
37 id_list = R1, R2, R3
38 demand_avg_list = 2, 2, 2 # Mean demand per retailer
39 demand_std_list = 10, 10, 10 # Std. dev. of demand per retailer
40 revenue_list = 50, 50, 50
41 holding_cost_list = 1, 2, 4
42 overorder_penalty_list = 10, 10, 10
43 holding_capacity_list = 50, 50, 50
44 max_start_inv = 12, 12, 12
45 backorder_penalty_list = 0, 0, 0
46
47 [supply_chain_connection_params]
48 # Define connections and their parameters (costs and lead times) between defined entities
49 upstream_id_list = P1, P1, D1, D1, D1, D2, D2, D2
50 downstream_id_list = D1, D2, R1, R2, R3, R1, R2, R3
51 L_list = 2, 2, 1, 2, 3, 5, 6, 7 # Specify lead times for each connection
52 order_cost_per_item_list = 0, 0, 0, 0, 0, 0, 0, 0

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

31

53 order_cost_fixed_list = 0, 0, 50, 50, 50, 50, 50, 50
54 max_start_inv = 6, 6, 6, 6, 6, 6, 6, 6

Listing 1: 1S-2W-3R with dual sourcing environment conﬁguration example

This library is based around OpenAI Gym environment API (Brockman et al. 2016), so that stan-

dard RL implementations, like Stable Baselines 3 (Raﬃn et al. 2019) used here, can work directly

with deﬁned environments. Listing 2 shows an example of instantiating the 1S-2W-3R environment

and exercising it - taking actions and getting rewards and next states from the environment, and

loading and training a DRL baseline with the environment. This also demonstrates how the library

supports easily specifying diﬀerent representation encodings for the states and actions (common

representation as mentioned in an earlier footnote: discrete, multi-discrete, continuous, and normal-

ized continuous). Additionally, this example illustrates enabling logging so all states and inventory

levels, actions and orders, rewards, demands, and costs per network entity are logged and can be

exported after taking actions in the environment.

1 from supply_chain_rl . run . env_setup_from_cfg import get_env_fun
2 from stable_ba selines3 import PPO
3 from supply_chain_rl . model . parl import PARL
4
5 # Instantiate the environment with normalized continuous action representations
6 env_function = get_env_fun ( " env . cfg " , action_rep = " n " )
7 env = env_function ()
8
9 # Instantiate a baseline DRL model or PARL model - randomly initialized
10 # model = PPO ( policy =" MlpPolicy " , env = env )
11 model = PARL ( env = env )
12
13 # Fit the model to the env
14 model . learn ( total_timesteps =1 e6 )
15
16 # Evaluate model for some number of steps and log costs and rewards
17 # Reset environment , send action from model to env to get reward and next state
18 state = env . reset ()
19 env . logging_enable ()
20
21 for step in range (256) :
22
23
24
25
26 # Export log of evaluation to a file :
27 env . logging_w rite_j so n ( " trajectory_log . json " )

action , _ = model . predict ( state )
state , reward , done , info = env . step ( action )
print ( step , reward )

Listing 2: Example Python code using the library to load a supply chain environment and train
and evaluate PARL or a baseline RL algorithm on the environment.

32

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

To the best of our knowledge such a reinforcement learning library and set of environments

focused on broadly enabling supply chain environments for reinforcement learning does not cur-

rently exist. OpenAI provides the Gym library which provides an interface for RL environments and

a set of video game environments. Various diﬀerent libraries exist with RL algorithms that leverage

video game environments. However, there are no libraries focused on RL for supply chain manage-

ment that facilitate easy creation of diﬀerent supply chain management environments. There is an

open source library that provides a set of general environments for a variety of operations research

problems, ORGym, but it is more broadly focused on OR as a whole and only includes a couple

environments for supply chains under speciﬁc settings. We believe our framework is complemen-

tary to this and essentially - providing a full set of environments for diﬀerent practical common

supply chain scenarios as well as the capability to ﬂexibly deﬁne custom environments and specify

environment variations of interest, from diﬀerent network, cost, and fulﬁllment structures, to dif-

ferent demand distributions including non-stationary and supply chain settings including lost sales

settings.

5. Conclusions and future research directions

Reinforcement learning has lead to considerable break-throughs in diverse areas such as robotics,

games and many others in the past decade. In this work, we present a RL based approach to solve

some analytically intractable problems in supply chain and inventory management. Many real world

problems have large combinatorial actions and state-dependent constraints. Hence, we propose the

PARL algorithm that uses integer programming and SAA to account for underlying stochasticity

and provide a principled way of optimizing over large action spaces. We then discuss the application

of PARL to inventory replenishment and distribution decision across a supply chain network. We

demonstrate PARL’s superior performance on diﬀerent settings which incorporate some real-world

complexities including, heterogeneous demand across the demand nodes in the network, supply

lead times, and lost sales. Finally, to make the work more accessible, we also detail the development

of a Python library that allows easly implementation of various benchmark RL algorithms along

with PARL for diﬀerent inventory management problems. This work also opens up various avenues

for future research. First and foremost, by making RL algorithms more accessible via the Python

library, we believe that researchers will be able to easily benchmark and also propose new RL

inspired algorithms for other complex supply chain problems. For example, one potential interesting

research direction is to use this methodology in settings where complete demand distribution is

unknown. In these cases, one can use historical sales data to estimate demand distributions and

improve these forecasts over time using online learning techniques. The improved forecasts can

then be directly used for generating replenishment policies using the proposed framework. Another

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

33

interesting direction could be to extend this framework for order fulﬁllment, wherein the demand is

realized and the decision is where to fulﬁll it from. Finally, since IP based methods still suﬀer from

longer run times, coming up with alternate near-optimal formulations that are computationally

eﬃcient, remains an open area of research.

References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. https://github.com/openai/spinningup,

2018.

Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep
reinforcement learning at the edge of the statistical precipice. Advances in neural information processing
systems, 34:29304–29320, 2021.

Shipra Agrawal and Randy Jia. Learning in structured mdps with convex cost functions: Improved regret
bounds for inventory management. In Proceedings of the 2019 ACM Conference on Economics and
Computation, pages 743–744, 2019.

Gad Allon and Jan A Van Mieghem. Global dual sourcing: Tailored base-surge allocation to near-and oﬀshore

production. Management Science, 56(1):110–124, 2010.

Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong mixed-
integer programming formulations for trained neural networks. Mathematical Programming, pages 1–37,
2020.

Dimitri Bertsekas. Dynamic programming and optimal control: Volume I and II. Athena scientiﬁc, 2017.

Robert N Boute, Joren Gijsbrechts, Willem van Jaarsveld, and Nathalie Vanvuchelen. Deep reinforcement

learning for inventory control: A roadmap. European Journal of Operational Research, 2021.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech

Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.

Andrew J Clark and Herbert Scarf. Optimal policies for a multi-echelon inventory problem. Management

science, 6(4):475–490, 1960.

Ton de Kok, Christopher Grob, Marco Laumanns, Stefan Minner, J¨org Rambau, and Konrad Schade. A
typology and literature review on stochastic multi-echelon inventory models. European Journal of
Operational Research, 269(3):955–983, 2018.

Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja. Reinforcement learning with combinatorial
actions: An application to vehicle routing. Advances in Neural Information Processing Systems, 33,
2020.

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement
In International conference on machine learning, pages 1329–1338.

learning for continuous control.
PMLR, 2016.

Awi Federgruen and Paul Zipkin. Approximations of dynamic, multilocation production and inventory

problems. Management Science, 30(1):69–84, 1984.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic

methods. In International Conference on Machine Learning, pages 1582–1591, 2018.

Ilaria Giannoccaro and Pierpaolo Pontrandolfo. Inventory management in supply chains: a reinforcement

learning approach. International Journal of Production Economics, 78(2):153–161, 2002.

Joren Gijsbrechts, Robert N Boute, Jan A Van Mieghem, and Dennis Zhang. Can deep reinforcement learning
improve inventory management? performance and implementation of dual sourcing-mode problems.
Performance on Dual Sourcing, Lost Sales and Multi-Echelon Problems, 2018.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Proceedings of
the fourteenth international conference on artiﬁcial intelligence and statistics, pages 315–323. JMLR
Workshop and Conference Proceedings, 2011.

34

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

David A Goldberg, Dmitriy A Katz-Rogozhnikov, Yingdong Lu, Mayank Sharma, and Mark S Squillante.
Asymptotic optimality of constant-order policies for lost sales inventory models with large lead times.
Mathematics of Operations Research, 41(3):898–913, 2016.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, edi-
tors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 1861–1870. PMLR, 10–15 Jul 2018.

Kazuyuki Hara, Daisuke Saito, and Hayaru Shouno. Analysis of function of rectiﬁed linear unit used in deep
learning. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1–8, 2015.

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep
reinforcement learning that matters. In Proceedings of the AAAI conference on artiﬁcial intelligence,
volume 32, 2018.

Geoﬀrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6e-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.

Christian D Hubbs, Hector D Perez, Owais Sarwar, Nikolaos V Sahinidis, Ignacio E Grossmann, and John M
Wassick. Or-gym: A reinforcement learning library for operations research problems. arXiv preprint
arXiv:2008.06319, 2020.

Woonghee Tim Huh, Ganesh Janakiraman, John A Muckstadt, and Paat Rusmevichientong. Asymptotic
optimality of order-up-to policies in lost sales inventory systems. Management Science, 55(3):404–420,
2009.

Sujin Kim, Raghu Pasupathy, and Shane G Henderson. A guide to sample average approximation. Handbook

of simulation optimization, pages 207–243, 2015.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The Interna-

tional Journal of Robotics Research, 32(11):1238–1274, 2013.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR, 2016.

Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard S Sutton.
Convergent temporal-diﬀerence learning with arbitrary smooth function approximation. Advances in
neural information processing systems, 22, 2009.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,
2013.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
In
International conference on machine learning, pages 1928–1937. PMLR, 2016.

Afshin Oroojlooyjadid, MohammadReza Nazari, Lawrence V Snyder, and Martin Tak´aˇc. A deep q-network
for the beer game: Deep reinforcement learning for inventory optimization. Manufacturing & Service
Operations Management, 2021.

¨Ozalp ¨Ozer and Hongxia Xiong. Stock positioning and performance estimation for distribution systems with

service constraints. Iie Transactions, 40(12):1141–1157, 2008.

Mohammad Pirhooshyaran and Lawrence V Snyder. Simultaneous decision making for stochastic multi-
arXiv preprint

echelon inventory optimization with deep neural networks as decision makers.
arXiv:2006.05608, 2020.

Warren B Powell. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703.

John Wiley & Sons, 2007.

Meng Qi, Yuanyuan Shi, Yongzhi Qi, Chenxin Ma, Rong Yuan, Di Wu, and Zuo-Jun Max Shen. A practical
end-to-end inventory management model with deep learning. Available at SSRN 3737780, 2020.

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

35

Antonin Raﬃn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann.

Stable Baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.

Ying Rong, Z¨umb¨ul Atan, and Lawrence V Snyder. Heuristics for base-stock levels in multi-echelon distri-

bution networks. Production and Operations Management, 26(9):1760–1777, 2017.

Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, and Craig Boutilier. Caql:

Continuous action q-learning. arXiv preprint arXiv:1909.12397, 2019.

Herbert Scarf. The optimality of (s, s) policies in the dynamic inventory problem. 1960.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-

tion algorithms. arXiv preprint arXiv:1707.06347, 2017.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic programming:

modeling and theory. SIAM, 2014.

Alexander Shapiro. Monte carlo sampling methods. Handbooks in operations research and management

science, 10:353–425, 2003.

Anshul Sheopuri, Ganesh Janakiraman, and Sridhar Seshadri. New policies for the stochastic inventory

control problem with two supply sources. Operations research, 58(3):734–745, 2010.

Tim Stockheim, Michael Schwind, and Wolfgang Koenig. A reinforcement learning approach for supply chain

management. In 1st European Workshop on Multi-Agent Systems, Oxford, UK, 2003.

Nazneen N Sultana, Hardik Meisheri, Vinita Baniwal, Somjit Nath, Balaraman Ravindran, and Harshad
Khadilkar. Reinforcement learning for multi-product multi-node inventory management in supply
chains. arXiv preprint arXiv:2006.04037, 2020.

Jiankun Sun and Jan A Van Mieghem. Robust dual sourcing inventory management: Optimality of capped
dual index policies and smoothing. Manufacturing & Service Operations Management, 21(4):912–931,
2019.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma.
The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network veriﬁ-
cation. arXiv preprint arXiv:2006.14076, 2020.

Wouter van Heeswijk and Han La Poutr´e. Approximate dynamic programming with neural networks in

linear discrete action spaces. arXiv preprint arXiv:1902.09855, 2019.

Benjamin Van Roy, Dimitri P Bertsekas, Yuchun Lee, and John N Tsitsiklis. A neuro-dynamic programming
approach to retailer inventory management. In Proceedings of the 36th IEEE Conference on Decision
and Control, volume 4, pages 4052–4057. IEEE, 1997.

Senthil Veeraraghavan and Alan Scheller-Wolf. Now or later: A simple policy for eﬀective dual sourcing in

capacitated systems. Operations Research, 56(4):850–864, 2008.

Linwei Xin. Understanding the performance of capped base-stock policies in lost-sales inventory models.

Operations Research, 69(1):61–70, 2021.

Shenghe Xu, Shivendra S Panwar, Murali Kodialam, and TV Lakshman. Deep neural network approximated
In Proceedings of the AAAI Conference on

dynamic programming for combinatorial optimization.
Artiﬁcial Intelligence, volume 34, pages 1684–1691, 2020.

Liz Young. Companies face rising supply-chain costs amid inventory challenges, Jun 2022.

Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv preprint

arXiv:1908.08796, 2019.

Paul Zipkin. Old and new methods for lost-sales inventory systems. Operations Research, 56(5):1256–1263,

2008.

Paul Zipkin. On the structure of lost-sales inventory models. Operations research, 56(4):937–944, 2008.

36

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

Appendix A: Proof of Proposition 1

Proof of Proposition 1: Consider any state s and let g(s, a, d) = R(s, a, d) + γ ˆV πj−1
(T (s, a, d)). We start by
showing that gη(s, a, d) uniformly converges to E[g(s, a, D)] with probability 1. We prove this result by proving

θ

two main properties of g(s, a, d): (i) g(s, a, d) is continuous in a for almost every d ∈ D, and (ii) g(s, a, d) is

dominated by an integrable function. To prove (ii), we show that g(s, a, d) ≤ C < ∞ w.p. 1 ∀a ∈ A(s).

First, notice that g(s, a, d) is an aﬃne function of the immediate reward R(s, a, d) and NN approximation

of the value-to-go function. By assumption, the immediate reward follows these properties. Hence, to show

these properties for g(s, a, d), we only need to illustrate that the value-to-go estimation also follows these

properties.

Consider the value-to-go approximation, simply denoted as ˆVθ(T (s, a, d)) with θ = (c, {(Wk, bk)}K−1
k=1 )
denoting the parameters of the K-layer ReLU-network. As T (s, a, d) is continuous and ˆVθ(s) is continuous,
ˆVθ(T (s, a, d)) is continuous. Note that T (s, a, d) lies in a bounded space for any realization of the uncer-

tainty d. Furthermore, since the parameters of the NN θ are bounded, the outcome of each hidden layer,

and subsequently the outcome of the NN are also bounded. This proves that the NN is uniformly dominated

by an integrable function. Then, following Proposition 8 of Shapiro (2003), we have uniform convergence of
gη(s, a, d) to E[g(s, a, D)] w.p. 1. Finally, convergence of the optimal solution follows from a direct application

of Theorem 5.3 of Shapiro et al. (2014), where we have used the fact that for all s the set of feasible actions

is a bounded polyhedron A(s) and that for any η, the set of optimal actions ˆπη(s) is non-empty. This proves
the ﬁnal result. (cid:3)

While the above proof assumes that the action space is continuous, one can extend the results in the case

of discrete action spaces as well. See Kim et al. (2015) for a discussion on the techniques used for extending

the analysis to this setting.

Appendix B: More details on benchmark algorithms and parameter settings

B.1. Base Stock heuristic policy

The seminal work of Scarf (1960) shows the optimality of the parametric (s, S) base stock policies for retail

nodes with inﬁnite capacity upstream supplier in backordered demand settings. This optimality holds even

in the case with ﬁxed costs and constant lead times. In this policy, if I is the inventory pipeline vector for a
ﬁrm, the inventory position is deﬁned as IP = (cid:80)L

i=0 I j, where L is the lead time from the supplier, and the

order quantity is max{0, S − IP } as long as IP <= s and 0 otherwise.

Due to the popularity of these policies, we implement a heuristic base stock policy for the multi-echelon

networks we study as follows. At the high level, we construct a base stock policy for every link assuming

the upstream entity’s supply is unconstrained. For the 2 echelon 1S − nR environments, we identify the

best base stock policy via grid search for each link using a 1S − 1R environment with unconstrained supply.

The reason we do a grid search is because we are in the lost sales setting. Note that we observe that the

constrained supply setting also yields very similar results and hence restrict ourselves to the unconstrained

supply setting. For the 3-echelon environments, we use the same strategy for the W − R links but computing

inventory positions IP based on the lead time for that link (note that inventory pipelines can be longer than

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

37

leadtime in the dual sourcing setting). For the S − W links we use environments that treat the warehouse as

a retailer with demand equal to the sum of the downstream (lead time) retail demands to ﬁnd the optimal

parameters for that link.

B.2. Decomposition-Aggregation (DA) heuristic (Rong et al. 2017)

In this section, we describe our implementation of the DA heuristic. Note that we adaptation of the DA

heuristic to the case of a Normal demand distribution as the authors discuss the method in the case of a

] where

br
br +hr
√

Poisson demand distribution.

For 1S-nR environments, for every retailer r compute its respective order up to level Sr = F −1
Dr [

F −1

Dr is the inverse cumulative demand distribution (cdf) of the random variable Dr ∼ (µr(Lr +1), σr

Lr + 1).

Here br is the retailer revenue per item less the variable ordering cost from the supplier and hr is the holding

cost at the retailer. In every period, the retailer orders Sr − IPr where IPr is the retailer’s inventory position

which is the sum of the retailer’s on-hand inventory and that in the pipeline vector.

For the 1S-2W-nR environments, we ﬁrst decompose by sample paths 1S − 1W − 1R.

For each such sample path, we compute Sr = F −1

Dr is the inverse cdf of the random
Lr + 1). Here br is the retailer revenue per item less the variable ordering

br +hr

] where F −1

Dr [ br +hwr

√

variable Dr ∼ N (µr(Lr + 1), σr

cost from the supplier, hr is the holding cost at the retailer and hwr is the holding cost of the warehouse in

the sample path of interest.

We then compute qwr = F

(cid:104)

0.5F −1 (cid:104) br

br +hr

(cid:105)

+ 0.5F −1 (cid:104)

br
br +hwr

(cid:105)(cid:105)

where F and F −1 refers to the cdf and

inverse cdf of the standard normal distribution N(0,1). We use this to compute echelon order up to level of the

warehouse Swr = F −1
Dwr
shortfall is computed which is QDwr r (swr ) = EDwr r [Dwr r − swr ]+ where swr = Swr − Sr.

[qwr ] where Dwr is distributed as N(µr(Lr + Lw + 1), σr

√

Lr + Lw + 1). An expected

Next we aggregate across sample paths to recompute the order up to level at common warehouse w

using a back-order matching method described as follows: Sw = Q−1
Dw
(cid:17)

r|wr =w µrLw,

r|wr =w σ2

r Lw

and Q−1

Dw (y) = min {S|EDw [Dw − S]+ ≤ y}.

(cid:16)(cid:80)

N

(cid:113)(cid:80)

(cid:16)(cid:80)

r|wr =w QDwr r (swr )

(cid:17)

where Dw ∼

In every period, the retailer orders Sr − IPr from the warehouse and the warehouse orders Sw − IPw from

the supplier where IPr, IPw are the retailer’s and warehouse’s respective inventory positions which is the

sum of the on-hand inventory and that in the pipeline vector.

B.3. Parameters and policy for the 1Sinf -1R environment

The 1Sinf -1R environment is a setting with back ordered demand and no-ﬁxed costs. The lead time (L) is

4, the holding cost (h) is 0.8 and the back order penalty (b) is 7. The demand (D) is assumed to be normal

N [µ, σ] where µ = 5 and σ = 0.8. In this setting the optimal policy is an order-up to policy with closed form

solution F −1
D

(cid:17)

(cid:16) b
b+h

which is 26.48. As the environment only allows ordering integer quantities, we tested

both 26 and 27 order up-to levels and 27 outperformed on average across 10 (identical) model runs each with

20 test runs with trajectory length of 10K steps.

38

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

B.4. Comparison of quantile and random sampling in PARL

Here we compare the use of quantile sampling and random sampling to generate realizations of the uncertainty

in (4). For the ﬁve settings under consideration, we compare the per-step reward and per-step training time

across the two sampling approaches.

As can be seen in Table 4, random sampling yields per-step rewards which are close to those obtained via

quantile sampling. In terms of training time, random sampling is slower in certain settings (e.g. 1S-3R-High

and 1S-10R), with higher per-step train-time average and variance.

Setting

1S-3R-High

1S-3R

1S-10R

1S-2W-3R

1S-2W-3R (DS)

PARL-quantile
per-step reward
514.8 ± 5.3
514.3
400.3 ± 3.3
400.8
1006.3 ± 29.5
1015.7
398.3 ± 2.5
399.7
405.4 ± 2.0
405.9

PARL-random
per-step reward
505.3 ± 11.0
505.1
399.5 ± 2.8
400.8
1005.4 ± 21.1
1007.3
395.3 ± 3.1
395.9
398.9 ± 9.7
402.0

PARL-quantile
per-step train-time (s)

PARL-random
per-step train-time (s)

0.178 ± 0.06

0.457 ± 0.26

0.051 ± 0.01

0.053 ± 0.01

0.089 ± 0.03

0.12 ± 0.07

0.051 ± 0.01

0.050 ± 0.01

0.044 ± 0.01

0.043 ± 0.01

Table 4

Comparison of quantile and random sampling in PARL.

B.5. Cost and reward comparison for other settings

(a) 1S-10R

(b) 1S-2W-3R (DS)

(c) 1S-2W-3R

Figure 7

Breakdown of rewards in test across BS, PPO and PARL algorithms for settings not discussed in the

main text. Note that ordering costs include ﬁxed and variable costs of ordering, revenue refers to the
revenue earned from sales and reward refers to the revenue net costs incurred (see §3 and Table 1 for

more details).

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

39

Appendix C: Parameter Tuning Details

Table 5

Best hyper parameters selected for each environment and method, for the benchmark RL methods.

See Tables 6 and 7 for hyper parameter abbreviations.

SAC

TD3

PPO

A2C

method
setting

1S-3R-High G=0.9

1S-3R

1S-10R

1S-20R

1S-2W-3R

1S-2W-3R
(DS)

LR=0.01
EO=True
G=0.75
LR=0.003
EO=True
G=0.8
LR=0.003
EO=True
G=0.9
LR=0.003
EO=False
G=0.8
LR=0.003
EO=False
G=0.9
LR=0.0003
EO=True

1Sinf-2W-3R G=0.99

LR=0.003
EO=False

G=0.9
LR=0.0003
EO=False
G=0.8
LR=0.0003
EO=False
G=0.9
LR=0.0003
EO=True
G=0.75
LR=0.0003
EO=False
G=0.9
LR=0.0003
EO=False
G=0.9
LR=0.0003
EO=True
G=0.99
LR=0.003
EO=False

G=0.9
LR=0.003
VFC=1.0
G=0.8
LR=0.003
VFC=1.0
G=0.8
LR=0.003
VFC=1.0
G=0.9
LR=0.010
VFC=3.0
G=0.8
LR=0.003
VFC=1.0
G=0.75
LR=0.003
VFC=3.0
G=0.99
LR=0.010
VFC=0.5

G=0.8
LR=0.003
VFC=0.5
G=0.8
LR=0.003
VFC=0.5
G=0.9
LR=0.003
VFC=1.0
G=0.9
LR=0.010
VFC=0.5
G=0.8
LR=0.003
VFC=3.0
G=0.8
LR=0.003
VFC=0.5
G=0.99
LR=0.003
VFC=3.0

Table 6

Tuning hyper parameters and additional ﬁxed hyper parameters for PPO and A2C - we vary γ,

learning rate, and value function coeﬃcient - resulting in 36 hyper parameter combinations
Value(s)
0.99, 0.9, 0.80, 0.75
0.01, 0.003, 0.0003
0.5, 1.0, 3.0
2048
0.5
0.95 (PPO) and 1.0
(A2C) (defaults)

Hyper Parameters for PPO and A2C
Discount Factor - γ (G)
Learning rate (LR)
Value function coeﬃcient (in loss) (VFC)
Number of steps to run per update (epoch length)
Max gradient norm (for clipping)
GAE lambda (trade-oﬀ bias vs. variance for Generalized Advantage
Estimator)
Number of epochs to optimize surrogate loss (internal train iterations
per update - PPO only)
KL divergence threshold for policy update early stopping per epoch
(PPO only)
Clip range (PPO only)
RMSprop epsilon (A2C only)

20

0.15
(“target kl”=0.1)
0.2
1e-05

40

Harsha et al.: Deep Policy Iteration for Inventory Management
00(0), pp. 000–000, © 0000 INFORMS

Table 7

Tuning hyper parameters and additional ﬁxed hyper parameters for SAC and TD3 - we vary gamma,

learning rate, and exploration options - resulting in 32 hyper parameter combinations

Hyper Parameters for SAC and TD3
Discount Factor - γ (G)

Learning rate (LR)

Use generalized State Dependent Exploration vs. Action Noise Explo-
ration (SAC) or Action Noise vs. not (TD3) (EO)
Tau (soft update coeﬃcient)
Replay buﬀer size
Entropy regularization coeﬃcient (SAC only)

Value(s)
0.99, 0.9, 0.80, 0.75
0.01, 0.003, 0.0003,
0.00003

True, False

0.005
105
auto

