Learning Semantic Parsers from Denotations with Latent
Structured Alignments and Abstract Programs

Bailin Wang, Ivan Titov and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bailin.wang@ed.ac.uk, {ititov,mlap}@inf.ed.ac.uk

9
1
0
2

p
e
S
9

]
L
C
.
s
c
[

1
v
5
6
1
4
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

Semantic parsing aims to map natural lan-
guage utterances onto machine interpretable
meaning representations, aka programs whose
execution against a real-world environment
produces a denotation. Weakly-supervised
semantic parsers are trained on utterance-
denotation pairs treating programs as latent.
The task is challenging due to the large search
space and spuriousness of programs which
may execute to the correct answer but do not
generalize to unseen examples. Our goal is
to instill an inductive bias in the parser to
help it distinguish between spurious and cor-
rect programs. We capitalize on the intu-
ition that correct programs would likely re-
spect certain structural constraints were they
to be aligned to the question (e.g., program
fragments are unlikely to align to overlapping
text spans) and propose to model alignments
as structured latent variables. In order to make
the latent-alignment framework tractable, we
decompose the parsing task into (1) predicting
a partial “abstract program” and (2) reﬁning
it while modeling structured alignments with
differential dynamic programming. We ob-
tain state-of-the-art performance on the WIK-
ITABLEQUESTIONS and WIKISQL datasets.
When compared to a standard attention base-
line, we observe that the proposed structured-
alignment mechanism is highly beneﬁcial.

1

Introduction

Semantic parsing is the task of translating natural
language to machine interpretable meaning repre-
sentations. Typically, it requires mapping a natural
language utterance onto a program, which is exe-
cuted against a knowledge base to obtain an an-
swer or a denotation. Most previous work (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007; Lu et al., 2008; Jia and Liang, 2016) has
focused on the supervised setting where a model
is learned from question-program pairs. Weakly

Figure 1: After generating an abstract program for
a question, our parser ﬁnds alignments between slots
(with preﬁx #) and question spans. Based on the align-
ment, it instantiates each slot and a complete program
is executed against a table to obtain a denotation.

supervised semantic parsing (Berant et al., 2013;
Liang et al., 2011) reduces the burden of anno-
tating programs by learning from questions paired
with their answers (or denotations).

Two major challenges arise when learning from
denotations: 1) training of the semantic parser re-
quires exploring a large search space of possible
programs to ﬁnd those which are consistent, and
execute to correct denotations; 2) the parser should
be robust to spurious programs which acciden-
tally execute to correct denotations, but do not re-
ﬂect the semantics of the question. In this paper,
we propose a weakly-supervised neural semantic
parser that features structured latent alignments to
bias learning towards correct programs which are
consistent but not spurious.

Our intuition is that correct programs should re-

Question:(cid:21)(cid:27)(cid:34)(cid:25)(cid:14)(cid:26)(cid:36)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:25)(cid:18)(cid:17)(cid:14)(cid:24)(cid:30)(cid:17)(cid:22)(cid:17)(cid:31)(cid:21)(cid:18)(cid:26)(cid:14)(cid:31)(cid:22)(cid:27)(cid:26)(cid:27)(cid:19)(cid:9)(cid:32)(cid:29)(cid:23)(cid:18)(cid:36)(cid:34)(cid:22)(cid:26)(cid:8)AbstractProgram:(cid:30)(cid:18)(cid:24)(cid:18)(cid:16)(cid:31)(cid:3)(cid:2)(cid:29)(cid:27)(cid:34)(cid:13)(cid:30)(cid:24)(cid:27)(cid:31)(cid:5)(cid:2)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:13)(cid:30)(cid:24)(cid:27)(cid:31)(cid:4)Correct	Program:(cid:30)(cid:18)(cid:24)(cid:18)(cid:16)(cid:31)(cid:3)(cid:11)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:26)(cid:14)(cid:31)(cid:22)(cid:27)(cid:26)(cid:7)(cid:9)(cid:32)(cid:29)(cid:23)(cid:18)(cid:36)(cid:12)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:4)InstantiationExecutionDenotation:0SpuriousPrograms:(cid:30)(cid:18)(cid:24)(cid:18)(cid:16)(cid:31)(cid:3)(cid:1)(cid:28)(cid:29)(cid:18)(cid:33)(cid:22)(cid:27)(cid:32)(cid:30)(cid:1)(cid:3)(cid:14)(cid:29)(cid:20)(cid:25)(cid:14)(cid:35)(cid:3)(cid:11)(cid:14)(cid:24)(cid:24)(cid:13)(cid:29)(cid:27)(cid:34)(cid:30)(cid:12)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:4)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:4)(cid:4)(cid:30)(cid:18)(cid:24)(cid:18)(cid:16)(cid:31)(cid:3)(cid:14)(cid:29)(cid:20)(cid:25)(cid:22)(cid:26)(cid:3)(cid:11)(cid:14)(cid:24)(cid:24)(cid:13)(cid:29)(cid:27)(cid:34)(cid:30)(cid:12)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:4)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:30)(cid:22)(cid:24)(cid:33)(cid:18)(cid:29)(cid:4)InconsistentProgram:(cid:30)(cid:18)(cid:24)(cid:18)(cid:16)(cid:31)(cid:3)(cid:11)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:26)(cid:14)(cid:31)(cid:22)(cid:27)(cid:26)(cid:7)(cid:9)(cid:32)(cid:29)(cid:23)(cid:18)(cid:36)(cid:12)(cid:5)(cid:16)(cid:27)(cid:24)(cid:32)(cid:25)(cid:26)(cid:6)(cid:26)(cid:14)(cid:31)(cid:22)(cid:27)(cid:26)(cid:4) 
 
 
 
 
 
spect certain constraints were they to be aligned to
the question text, while spurious and inconsistent
programs do not. For instance, in Figure 1, the
answer to the question (“0”) can be obtained by
executing the correct program which selects the
number of Turkey’s silver medals. However, the
same answer can be also obtained by the spurious
programs shown in the ﬁgure.1 The spurious pro-
grams differ from the correct one in that they re-
peatedly use the column “silver”. Whereas, in the
question, the word “silver” only refers to the tar-
get column containing the answer; it also mistak-
enly triggers the appearance of the column “silver”
in the row selection condition. This constraint,
i.e., that a text span within a question cannot trig-
ger two semantically distinct operations (e.g., se-
lecting target rows and target columns) can pro-
vide a useful inductive bias. We propose to cap-
ture structural constraints by modeling the align-
ments between programs and questions explicitly
as structured latent variables.

Considering the large search space of possible
programs, an alignment model that takes into ac-
count the full range of correspondences between
program operations and question spans would be
very expensive. To make the process tractable, we
introduce a two-stage approach that features ab-
stract programs. Speciﬁcally, we decompose se-
mantic parsing into two steps: 1) a natural lan-
guage utterance is ﬁrst mapped to an abstract pro-
gram which is a composition of high-level opera-
tions; and 2) the abstract program is then instanti-
ated with low-level operations that usually involve
relations and entities speciﬁc to the knowledge
base at hand. This decomposition is motivated by
the observation that only a small number of sensi-
ble abstract programs can be instantiated into con-
sistent programs. Similar ideas of using abstract
meaning representations have been explored with
fully-supervised semantic parsers (Dong and La-
pata, 2018; Catherine Finegan-Dollak and Radev,
2018) and in other related tasks (Goldman et al.,
2018; Herzig and Berant, 2018; Nye et al., 2019).
For a knowledge base in tabular format, we ab-
stract two basic operations of row selection and
column selection from programs: these are han-
dled in the second (instantiation) stage. As shown
in Figure 1, the question is ﬁrst mapped to the ab-
stract program “select (#row slot, #column slot)”

1The ﬁrst program can be paraphrased as: ﬁnd the row
with the largest number of silver medals and then select the
number of silver medals from the previous row.

whose two slots are subsequently instantiated with
ﬁlter conditions (row slot) and a column name
(column slot). During the instantiation of abstract
programs, each slot should refer to the question to
obtain its speciﬁc semantics. In Figure 1, row slot
should attend to “nation of Turkey” while col-
umn slot needs to attend to “silver medals”. The
structural constraint discussed above now corre-
sponds to assuming that each span in a question
can be aligned to a unique row or column slot.
Under this assumption, the instantiation of spuri-
ous programs will be discouraged. The uniqueness
constraint would be violated by both spurious pro-
grams in Figure 1, since “column:silver” appears
in the program twice but can be only aligned to the
span “silver medals” once.

The ﬁrst stage (i.e., mapping a question onto an
abstract program) is handled with a sequence-to-
sequence model. The second stage (i.e., program
instantation) is approached with local classiﬁers:
one per slot in the abstract program. The classi-
ﬁers are conditionally independent given the ab-
stract program and a latent alignment. Instead of
marginalizing out alignments, which would be in-
tractable, we use structured attention (Kim et al.,
2017), i.e., we compute the marginal probabili-
ties for individual span-slot alignment edges and
use them to weight the input to the classiﬁers. As
we discuss below, the marginals in our constrained
model are computed with dynamic programming.
We perform experiments on two open-domain
question answering datasets in the setting of learn-
ing from denotations. Our model achieves an ex-
ecution accuracy of 44.5% in WIKITABLEQUES-
TIONS and 79.3% in WIKISQL, which both sur-
pass previous state-of-the-art methods in the same
weakly-supervised setting.
In WIKISQL, our
parser is better than recent supervised parsers that
are trained on question-program pairs. Our contri-
butions can be summarized as follows:

• we introduce an alignment model as a means
of differentiating between correct and spuri-
ous programs;

• we propose a neural semantic parser that per-
forms tractable alignments by ﬁrst mapping
questions to abstract programs;

• we achieve state-of-the-art performance on

two semantic parsing benchmarks.2

2Our code is available at https://github.com/

berlino/weaksp_em19.

Although we use structured alignments to
mostly enforce the uniqueness constraint de-
scribed above, other types of inductive biases can
be useful and could be encoded in our two-stage
framework. For example, we could replace the
uniqueness constraint with modeling the number
of slots aligned to a span, or favor sparse align-
ment distributions. Crucially, the two-stage frame-
work makes it easier to inject prior knowledge
about datasets and formalisms while maintaining
efﬁciency.

2 Background

Given knowledge base t, our task is to map a nat-
ural utterance x to program z, which is then exe-
cuted against a knowledge base to obtain denota-
tion [[z]]t = d. We train our parser only based on d
without access to correct programs z∗. Our exper-
iments focus on two benchmarks, namely WIK-
ITABLEQUESTIONS (Pasupat and Liang, 2015)
and WIKISQL (Zhong et al., 2017) where each
question is paired with a Wikipedia table and a
denotation. Figure 1 shows a simpliﬁed example
taken from WIKITABLEQUESTIONS.

2.1 Grammars

Executable programs z that can query tables are
deﬁned according to a language. Speciﬁcally, the
search space of programs is constrained by gram-
mar rules so that it can be explored efﬁciently. We
adopt the variable-free language of Liang et al.
(2018) and deﬁne an abstract grammar and an in-
stantiation grammar which decompose the gener-
ation of a program in two stages.3

The ﬁrst stage involves the generation of an
abstract version of a program which, in the sec-
ond stage, gets instantiated. Abstract programs
only consider compositions of high-level func-
tions, such as superlatives and aggregation, while
low-level functions and arguments, such as ﬁlter
conditions and entities, are taken into account in
the next step. In our table-based datasets, abstract
programs do not include two basic operations of
querying tables: row selection and column selec-
tion. These operations are handled at the instan-
In Figure 1 the abstract program
tiation stage.
has two slots for row and column selection, which
are ﬁlled with the conditions “column:nation =
Turkey” and “column:silver” at the instantiation

3We also extend their grammar to additionally support op-
erations of conjunction and disjunction. More details are pro-
vided in the Appendix.

Figure 2: An abstract program and its derivation tree.
Capitalized words indicate types of function arguments
and their return value.

stage. The two stages can be easily merged into
one step when conducting symbolic combinatorial
search. The motivation for the decomposition is to
facilitate the learning of our neural semantic parser
and the handling of structured alignments.

Abstract Grammar Our abstract grammar has
ﬁve basic types: ROW, COLUMN, STRING, NUM-
BER, and DATE; COLUMN is further sub-typed
into STRING COLUMN, NUMBER COLUMN, and
DATE COLUMN; other basic types are augmented
with LIST to represent a list of elements like
LIST[ROW]. Arguments and return values of
functions are typed using these basic types.

Function composition can be deﬁned recur-
sively based on a set of production rules, each cor-
responding to a function type. For instance, func-
tion ROW → ﬁrst(LIST[ROW]) selects the ﬁrst
row from a list of rows and corresponds to produc-
tion rule “ROW → ﬁrst”.

The abstract grammar has two additional types
for slots (aka terminal rules) which correspond to
row and column selection:

LIST[ROW] → #row slot
COLUMN → #column slot

An example of an abstract program and its
derivation tree is shown in Figure 2. We lin-
earize the derivation by traversing it in a left-to-
right depth-ﬁrst manner. We represent the tree
in Figure 2 as a sequence of production rules:
“ROOT → STRING, STRING → select, ROW →
ﬁrst”, LIST[ROW] → #row slot, COLUMN → #col-
umn slot”. The ﬁrst action is always to select the
return type for the root node.
Given a speciﬁc table t,

the abstract gram-
mar Ht will depend on its column types. For ex-
ample, if the table does not have number cells,
“max/min” operations will not be executable.

Instantiation Grammar A column slot is di-
rectly instantiated by selecting a column; a row

STRING      select (ROW, COLUMN) COLUMN      #column_slot ROW      first (LIST[ROW]) LIST[ROW]      #rows_slot Abstract Program: select (first (#rows_slot) #column_slot) Derivation Tree:slot
is ﬁlled with one or multiple conditions
(COND) which are joined together with conjunc-
tion (OR) and disjunction (AND) operators:

COND → COLUMN OPERATOR VALUE
#row slot → COND (AND COND)*
#row slot → COND (OR COND)*

where OPERATOR ∈ [>, <, =, ≥, ≤] and VALUE
is a string, a number, or a date. A special condition
#row slot → all rows is deﬁned to signify that a
program queries all rows.

2.2 Search for Consistent Programs

A problematic aspect of learning from denotations
is that, since annotated programs are not available
(e.g., for WIKITABLEQUESTIONS), we have no
means to directly evaluate a proposed grammar.
As an evaluation proxy, we measure the coverage
of our grammar in terms of consistent programs.
Speciﬁcally, we exhaustively search for all consis-
tent programs for each question in the training set.
While the space of programs is exponential, we
observed that abstract programs which are instan-
tiated into correct programs are not very complex
in terms of the number of production rules used to
generate them. As a result, we impose restrictions
on the number of production rules which can ab-
stract programs, and in this way the search process
becomes tractable.4

We ﬁnd that 83.6% of questions in WIK-
ITABLEQUESTIONS are covered by at least one
consistent program. However, each question even-
tually has 200 consistent programs on average
and most of them are spurious. Treating them as
ground truth poses a great challenge for learning
a semantic parser. The coverage for WIKISQL is
96.6% and each question generates 84 consistent
programs.

Another important observation is that there is
only a limited number of abstract programs that
can be instantiated into consistent programs. The
number of such abstract programs is 23 for WIK-
ITABLEQUESTIONS and 6 for WIKISQL, sug-
gesting that there are a few patterns underlying
several utterances. This motivates us to design a
semantic parser that ﬁrst maps utterances to ab-
stract programs. For the sake of generality, we do
not restrict our parser to abstract programs in the
training set. We elaborate on this below.

4Details are provided in the Appendix.

3 Model

After obtaining consistent programs z for each
question via ofﬂine search, we next show how to
learn a parser that can generalize to unseen ques-
tions and tables.

3.1 Training and Inference

Our learning objective J is to maximize the log-
likelihood of the marginal probability of all con-
sistent programs, which are generated by mapping
an utterance x to an interim abstract program h:

J = log{

(cid:88)

p(h|x, t)

h∈Ht

(cid:88)

[[z]]=d

p(z|x, t, h)}.

(1)

During training, our model only needs to focus
on abstract programs that have successful instanti-
ations of consistent programs and it does not have
to explore the whole space of possible programs.

At test time, the parser chooses the program ˆz

with the highest probability:

ˆh, ˆz = arg max
h∈Ht, z

p(h|x, t)p(z|x, t, h).

(2)

For efﬁciency, we only choose the top-k abstract
programs to instantiate through beam search. ˆz is
then executed to obtain its denotation as the ﬁnal
prediction.

Next, we will explain the basic components of
our neural parser. Basically, our model ﬁrst en-
codes a question and a table with an input encoder;
it then generates abstract programs with a seq2seq
model; and ﬁnally, these abstract programs are in-
stantiated based on a structured alignment model.

3.2

Input Encoder

Each word in an utterance is mapped to a dis-
tributed representation through an embedding
Following previous work (Neelakantan
layer.
et al., 2017; Liang et al., 2018), we also add an
indicator feature specifying whether the word ap-
pears in the table. This feature is mapped to a
learnable vector. Additionally, in WIKITABLE-
QUESTIONS, we use POS tags from the CoreNLP
annotations released with the dataset and map
them to vector representations. The ﬁnal represen-
tation for a word is the concatenation of the vec-
tors above. A bidirectional LSTM (Hochreiter and
Schmidhuber, 1997) is then used to obtain a con-
textual representation li for the ith word.

A table is represented by a set of columns. Each
column is encoded by averaging the embeddings

of words under its column name. We also have a
column type feature (i.e., number, date, or string)
and an indicator feature signaling whether at least
one entity in the column appears in the utterance.

3.3 Generating Abstract Programs

Instead of extracting abstract programs as tem-
plates, similarly to Xu et al. (2017) and Catherine
Finegan-Dollak and Radev (2018), we generate
them with a seq2seq model. Although template-
based approaches would be more efﬁcient in prac-
tice, a seq2seq model is more general since it
could generate unseen abstract programs which
ﬁxed templates could not otherwise handle.

Our goal here is to generate a sequence of
production rules that lead to abstract programs.
During decoding, the hidden state gj of the j-
th timestep is computed based on the previous
production rule, which is mapped to an embed-
ding aj−1. We also incorporate an attention mech-
anism (Luong et al., 2015) to compute a contex-
tual vector bj. Finally, a score vector sj is com-
puted by feeding the concatenation of the hidden
state and context vector to a multilayer perceptron
(MLP):

gj = LSTM(gj−1, aj−1)
bj = Attention(gj, l)
sj = MLP1([gj; bj])
p(aj|x, t, a<j) = softmaxaj (sj)

(3)

where the probability of production rule aj is
computed by the softmax function. According to
our abstract grammar, only a subset of production
rules will be valid at the j-th time step. For in-
stance, in Figure 2, production rule “STRING →
select” will only expand to rules whose left-hand
side is ROW, which is the type of the ﬁrst argu-
ment of select. In this case, the next production
rule is “ROW → ﬁrst”. We thus restrict the nor-
malization of softmax to only focus on these valid
production rules. The probability of generating an
abstract program p(h|x, t) is simply the product of
the probability of predicting each production rule
(cid:81)

j p(aj|x, t, a<j).
After an abstract program is generated, we need
to instantiate slots in abstract programs. Our
model ﬁrst encodes the abstract program using a
bi-directional LSTM. As a result, the representa-
tion of a slot is contextually aware of the entire
abstract program (Dong and Lapata, 2018).

3.4

Instantiating Abstract Programs

To instantiate an abstract program, each slot must
obtain its speciﬁc semantics from the question. We
model this process by an alignment model which
learns the correspondence between slots and ques-
tion spans. Formally, we use a binary alignment
matrix A with size m × n × n, where m is the
number of slots and n is the number of tokens.
In Figure 1, the alignment matrix will only have
A0,6,8 = 1 and A1,2,3 = 1 which indicates that
the ﬁrst slot is aligned with “nation of Turkey”,
and the second slot is aligned with “silver medals”.
The second and third dimension of the matrix rep-
resent the start and end position of a span.

We model alignments as discrete latent vari-
ables and condition the instantiation process on the
alignments as follows:

(cid:80)

p(z|x, t, h) =

[z]=d

(cid:80)
A

p(A|x, t, h) (cid:80)
[z]=d

p(z|x, t, h, A).

(4)

We will ﬁrst discuss the instantiation model
p(z|x, t, h, A) and then elaborate on how to avoid
marginalization in the next section. Each slot in
an abstract program can be instantiated by a set
of candidates following the instantiation grammar.
For efﬁciency, we use local classiﬁers to model the
instantiation of each slot independently:

p(z|x, t, h, A) =

(cid:89)

s∈S

p(s → c|x, t, h, A),

(5)

where S is the set of slots and c is a candidate fol-
lowing our instantiation grammar. “s → c” repre-
sents the instantiation of slot s into candidate c.

Recall that there are two types of slots, one for
rows and one for columns. All column names
in the table are potential instantiations of column
slots. We represent each column slot candidate by
the average of the embeddings of words in the col-
umn name. Based on our instantiation grammar
in Section 2.1, candidates for row slots are repre-
sented as follows: 1) each condition is represented
with the concatenation of the representations of a
column, an operator, and a value. For instance,
condition “string column:nation = Turkey” in Fig-
ure 1 is represented by vector representations of
the column ‘nation’, the operator ‘=’, and the en-
tity ‘Turkey’; 2) multiple conditions are encoded
by averaging the representations of all conditions
and adding a vector representation of AND/OR to
indicate the relation between them.

For each slot, the probability of generating a
candidate is computed with softmax normalization
on a score function:

the marginalization by moving the outside expec-
tation directly inside over A. As a result, we in-
stead optimize the following objective:

p(s → c|x, t, h, A) ∝ exp{MLP([s; c])},

(6)

where s is the representation of the span that slot s
is aligned with, and c is the representation of can-
didate c. The representations s and c are concate-
nated and fed to a MLP. We use the same MLP ar-
chitecture but different parameters for column and
row slots.

3.5 Structured Attention

We ﬁrst formally deﬁne a few structural con-
straints over alignments and then explain how to
incorporate them efﬁciently into our parser.

The intuition behind our alignment model is that
row and column selection operations represent dis-
tinct semantics, and should therefore be expressed
by distinct natural language expressions. Hence,
we propose the following constraints:

Unique Span In most cases, the semantics of a
row selection or a column selection is expressed
uniquely with a single contiguous span:

∀k ∈ [1, |S|],

(cid:88)

i,j

Ak,i,j = 1,

(7)

where |S| is the number of slots.

No Overlap Spans aligned to different slots
should not overlap. Formally, at most one span
that contains word i can be aligned to a slot:

∀i ∈ [1, n],

(cid:88)

k,j

Ak,i,j ≤ 1.

(8)

As an example, the alignments in Figure 1 fol-
low the above constraints.
Intuitively, the one-
to-one mapping constraint aims to assign distinct
and non-overlapping spans to slots of abstract pro-
grams. To further bias the alignments and improve
efﬁciency, we impose additional restrictions: (1) a
row slot must be aligned to a span that contains
an entity since conditions that instantiate the slot
would require entities for ﬁltering; (2) a column
slot must be aligned to a span with length 1 since
most column names only have one word.

Marginalizing out all A in Equation (4) would
be very expensive considering the exponential
number of possible alignments. We approximate

J ≈ log (cid:8) (cid:88)

p(h|x, t)

(cid:88)

p(z|x, t, h, E[A])(cid:9),

(9)

h∈Ht

[[z]]=d

where E[A] are the marginals of A with respect to
p(A|x, t, h).

The idea of using differentiable surrogates for
discrete latent variables has been used in many
other works like differentiable data structures
(Grefenstette et al., 2015; Graves et al., 2014) and
attention-based networks (Bahdanau et al., 2015;
Kim et al., 2017). Using marginals E[A] can be
viewed as structured attention between slots and
question spans.

The marginal probability of the alignment ma-
trix A can be computed efﬁciently using dynamic
programming (see T¨ackstr¨om et al. 2015 for de-
tails). An alignment is encoded into a path in
a weighted lattice where each vertex has 2|S|
states to keep track of the set of covered slots.
The marginal probability of edges in this lattice
can be computed by the forward-backward al-
gorithm (Wainwright et al., 2008). The lattice
weights, represented by a scoring matrix M ∈
Rm×n×n for all possible slot-span pairs, are com-
puted using the following scoring function:

Mk,i,j = MLP2([r(k); span[i : j]]),

(10)

where r(k) represents the kth slot and span[i : j]
represents the span from word i to j. Recall that
we obtain r(k) by encoding a generated abstract
program. A span is represented by averaging the
representations of the words therein. These two
representations are concatenated and fed to a MLP
to obtain a score. Since E[A] is not discrete any-
more, the aligned representation of slot s in Equa-
tion (6) becomes the weighted average of repre-
sentations of all spans in the set.

4 Experiments

We evaluated our model on two semantic parsing
benchmarks, WIKITABLEQUESTIONS and WIK-
ISQL. We compare against two common baselines
to demonstrate the effectiveness of using abstract
programs and alignment. We also conduct detailed
analysis which shows that structured attention is
highly beneﬁcial, enabling our parser to differen-
tiate between correct and spurious programs. Fi-
nally, we break down the errors of our parser so as

to examine whether structured attention is better at
instantiating abstract programs.

4.1 Experimental Setup

Datasets WIKITABLEQUESTIONS
contains
2,018 tables and 18,496 utterance-denotation
pairs. The dataset is challenging as 1) the tables
cover a wide range of domains and unseen tables
appear at test time; and 2) the questions involve
a variety of operations such as superlatives, com-
parisons, and aggregation (Pasupat and Liang,
2015). WIKISQL has 24,241 tables and 80,654
The questions are
utterance-denotation pairs.
logically simpler and only involve aggregation,
column selection, and conditions. The original
dataset is annotated with SQL queries, but we
only use the execution result for training. In both
datasets, tables are extracted from Wikipedia and
cover a wide range of domains.

Entity extraction is important during parsing
since entities are used as values in ﬁlter condi-
tions during instantiation. String entities are ex-
tracted by string matching utterance spans and ta-
In WIKITABLEQUESTIONS, numbers
ble cells.
and dates are extracted from the CoreNLP anno-
tations released with the dataset. WIKISQL does
not have entities for dates, and we use string-based
normalization to deal with numbers.

Implementation We obtained word embed-
dings by a linear projection of GloVe pre-trained
embeddings (Pennington et al., 2014) which were
ﬁxed during training. Attention scores were com-
puted based on the dot product between two vec-
tors. Each MLP is a one-hidden-layer perceptron
with ReLU as the activation function. Dropout
(Srivastava et al., 2014) was applied to prevent
overﬁtting. All models were trained with Adam
(Kingma and Ba, 2015). Implementations of ab-
stract and instantiation grammars were based on
AllenNLP (Gardner et al., 2017).5

4.2 Baselines

Aside from comparing our model against previ-
ously published approaches, we also implemented
the following baselines:

Typed Seq2Seq Programs were generated us-
ing a sequence-to-sequence model with attention
(Dong and Lapata, 2016). Similarly to Krishna-
murthy et al. (2017), we constrained the decod-

5Please refer to the Appendix for the full list of hyperpa-

rameters used in our experiments.

Supervised by Denotations Dev. Test

Pasupat and Liang (2015)
Neelakantan et al. (2017)
Haug et al. (2018)
Zhang et al. (2017)
Liang et al. (2018)
Dasigi et al. (2019)
Agarwal et al. (2019)

Typed Seq2Seq
Abstract Programs

37.1
37.0
34.1
34.2
— 34.8
43.7
40.4
43.1
42.3
43.9
42.1
44.1
43.2

37.3

38.3

f.w. standard attention
f.w. structured attention

39.4
43.7

41.4
44.5

Table 1:
f.w. stands for slots ﬁlled with.

Results on WIKITABLEQUESTIONS.

ing process so that only well-formed programs are
predicted. This baseline can be viewed as merging
the two stages of our model into one stage where
generation of abstract programs and their instanti-
ations are performed with a shared decoder.

Standard Attention The aligned representation
of slot s in Equation (6) is computed by a standard
attention mechanism: s = Attention(r(s), l)
where r(s) is the representation of slot s from
abstract programs. Each slot is aligned indepen-
dently with attention, and there are no global struc-
tural constraints on alignments.

4.3 Main Results

For all experiments, we report the mean accuracy
of 5 runs. Results on WIKITABLEQUESTIONS are
shown in Table 1. The structured-attention model
achieves the best performance, compared against
the two baselines and previous approaches. The
standard attention baseline with abstract programs
is superior to the typed Seq2Seq model, demon-
strating the effectiveness of decomposing seman-
tic parsing into two stages. Results on WIKISQL
are shown in Table 2. The structured-attention
model is again superior to our two baseline mod-
els. Interestingly, its performance surpasses previ-
ously reported weakly-supervised models (Liang
et al., 2018; Agarwal et al., 2019) and is on par
even with fully supervised ones (Dong and Lap-
ata, 2018).

The gap between the standard attention base-
line and the typed Seq2Seq model is not very
large on WIKISQL, compared to WIKITABLE-
QUESTIONS. Recall from Section 2.2 that WIK-

Supervised by Programs

Dev. Test

Models

WTQ WIKISQL

Zhong et al. (2017)
Wang et al. (2017)
Xu et al. (2017)
Huang et al. (2018)
Yu et al. (2018)
Sun et al. (2018)
Dong and Lapata (2018)
Shi et al. (2018)

60.8
67.1
69.8
68.3
74.5
75.1
79.0
84.0

59.4
66.8
68.0
68.0
73.5
74.6
78.5
83.7

Supervised by Denotations Dev. Test

Liang et al. (2018)
Agarwal et al. (2019)

Typed Seq2Seq
Abstract Programs

72.2
74.9

72.1
74.8

74.5

74.7

f.w. standard attention
f.w. structured attention

75.2
79.4

75.3
79.3

Table 2: Results on WIKISQL. f.w.: slots ﬁlled with.

ISQL only has 6 abstract programs that can be
successfully instantiated. For this reason, our de-
composition alone may not be very beneﬁcial if
coupled with standard attention. In contrast, our
structured-attention model consistently performs
much better than both baselines.

We report scores of ensemble systems in Ta-
ble 3. We use the best model which relies on
abstract programs and structured attention as a
base model in our ensemble. Our ensemble sys-
tem achieves better performance than Liang et al.
(2018) and Agarwal et al. (2019), while using the
same ensemble size.

4.4 Analysis of Spuriousness

To understand how well structured attention can
help a parser differentiate between correct and
spurious programs, we analyzed the posterior dis-
tribution of consistent programs given a denota-
tion: p(z|x, t, d) where [[z]] = d.

WIKISQL includes gold-standard SQL anno-
tations, which we do not use in our experiments
but exploit here for analysis. Speciﬁcally, we con-
verted the annotations released with WIKISQL to
programs licensed by our grammar. We then com-
puted the log-probability of these programs ac-
cording to the posterior distribution as a measure
of how well a parser can identify them amongst all
consistent programs log (cid:80)
z∗ p(z∗|x, t, d), where
z∗ denotes correct programs. The average log-

Liang et al. (2018)
Agarwal et al. (2019)
Our model

46.3 (10)
46.9 (10)
47.3 (10)

74.2 (5)
76.9 (5)
81.7 (5)

Table 3: Results of ensembled models on the test set;
ensemble sizes are shown within parentheses.

Error Types

standard

structured

Abstraction Error
Instantiation Error
Coverage Error

19.2
41.5
39.2

20.0
36.2
43.8

Table 4: Proportion of errors on the development set in
WIKITABLEQUESTIONS.

probability assigned to correct programs by struc-
tured and standard attention is -0.37 and -0.85, re-
spectively. This gap conﬁrms that structured atten-
tion can bias our parser towards correct programs
during learning.

4.5 Error Analysis

We further manually inspected the output of our
structured-attention model and the standard atten-
tion baseline in WIKITABLEQUESTIONS. Specif-
ically, we randomly sampled 130 error cases in-
dependently from both models and classiﬁed them
into three categories.

Abstraction Errors
If a parser fails to generate
an abstract program, then it is impossible for it to
instantiate a consistent complete program.

Instantiation Errors These errors arise when
abstract programs are correctly generated, but are
mistakenly instantiated either by incorrect column
names or ﬁlter conditions.

Coverage Errors These errors arise from im-
plicit assumptions made by our parser: a) there
is a long tail of unsupported operations that are
not covered by our abstract programs; b) if enti-
ties are not correctly identiﬁed and linked, abstract
programs cannot be correctly instantiated.

Table 4 shows the proportion of errors attested
by the two attention models. We observe that
structured attention suffers less from instantia-
tion errors compared against the standard attention
baseline, which points to the beneﬁts of the struc-
tured alignment model.

5 Related Work

Neural Semantic Parsing We follow the line
of work that applies sequence-to-sequence models
(Sutskever et al., 2014) to semantic parsing (Jia
and Liang, 2016; Dong and Lapata, 2016). Our
work also relates to models which enforce type
constraints (Yin and Neubig, 2017; Rabinovich
et al., 2017; Krishnamurthy et al., 2017) so as
to restrict the vast search space of potential pro-
grams. We use both methods as baselines to show
that the structured bias introduced by our model
can help our parser handle spurious programs in
the setting of learning from denotations. Note that
our alignment model can also be applied in the su-
pervised case in order to help the parser rule out
incorrect programs.

Earlier work has used lexicon mappings (Zettle-
moyer and Collins, 2007; Wong and Mooney,
2007; Lu et al., 2008; Kwiatkowski et al., 2010)
to model correspondences between programs and
natural language. However, these methods cannot
generalize to unseen tables where new relations
and entities appear. To address this issue, Pasupat
and Liang (2015) propose a ﬂoating parser which
allows partial programs to be generated without
being anchored to question tokens.
In the same
spirit, we use a sequence-to-sequence model to
generate abstract programs while relying on ex-
plicit alignments to instantiate them. Besides se-
mantic parsing, treating alignments as discrete la-
tent variables has proved effective in other tasks
like sequence transduction (Yu et al., 2016) and
AMR parsing (Lyu and Titov, 2018).

Learning from Denotations To improve the
efﬁciency of searching for consistent programs,
Zhang et al. (2017) use a macro grammar induced
from cached consistent programs. Unlike Zhang
et al. (2017) who abstract entities and relations
from logical forms, we take a step further and
abstract the computation of row and column se-
lection. Our work also differs from Pasupat and
Liang (2016) who resort to manual annotations
to alleviate spuriousness.
Instead, we equip our
parser with an inductive bias to rule out spuri-
ous programs during training. Recently, reinforce-
ment learning based methods address the com-
putational challenge by using a memory buffer
(Liang et al., 2018) which stores consistent pro-
grams and an auxiliary reward function (Agarwal
et al., 2019) which provides feedback to deal with
spurious programs. Guu et al. (2017) employ vari-

ous strategies to encourage even distributions over
consistent programs in cases where the parser has
been misled by spurious programs. Dasigi et al.
(2019) use coverage of lexicon-like rules to guide
the search of consistent programs.

6 Conclusions

In this paper, we proposed a neural semantic
parser that learns from denotations using abstract
programs and latent structured alignments. Our
parser achieves state-of-the-art performance on
two benchmarks, WIKITABLEQUESTIONS and
WIKISQL. Empirical analysis shows that the in-
ductive bias introduced by the alignment model
helps our parser differentiate between correct and
spurious programs. Alignments can exhibit dif-
ferent properties (e.g., monotonicity or bijectiv-
ity), depending on the meaning representation lan-
guage (e.g., logical forms or SQL), the deﬁnition
of abstract programs, and the domain at hand. We
believe that these properties can be often captured
within a probabilistic alignment model and hence
provide a useful inductive bias to the parser.

Acknowledgements

We would like to thank the anonymous reviewers
for their valuable comments. We gratefully ac-
knowledge the support of the European Research
Council (Titov: ERC StG BroadSem 678254; La-
pata: ERC CoG TransModal 681760) and the
Dutch National Science Foundation (NWO VIDI
639.022.518).

References

Rishabh Agarwal, Chen Liang, Dale Schuurmans, and
Mohammad Norouzi. 2019. Learning to generalize
from sparse and underspeciﬁed rewards. In Proc. of
ICML.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proc. of EMNLP.

Li Zhang Karthik Ramanathan Sesh Sadasivam
Jonathan
Im-
In

Rui Zhang Catherine Finegan-Dollak,
K. Kummerfeld and Dragomir Radev. 2018.
proving text-to-sql evaluation methodology.
Proc. of ACL.

Pradeep Dasigi, Matt Gardner, Shikhar Murty, Luke
Iterative

Zettlemoyer, and Eduard Hovy. 2019.

search for weakly supervised semantic parsing. In
Proc. of NAACL.

Li Dong and Mirella Lapata. 2016. Language to logical

form with neural attention. In Proc. of ACL.

Li Dong and Mirella Lapata. 2018. Coarse-to-ﬁne de-
coding for neural semantic parsing. In Proc. of ACL.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. Allennlp: A deep semantic natural language
processing platform.

Omer Goldman, Veronica Latcinnik, Ehud Nave, Amir
Globerson, and Jonathan Berant. 2018. Weakly su-
pervised semantic parsing with abstract examples.
In Proc. of ACL.

Alex Graves, Greg Wayne,

2014. Neural turing machines.
arXiv:1410.5401.

and Ivo Danihelka.
arXiv preprint

Edward Grefenstette, Karl Moritz Hermann, Mustafa
Suleyman, and Phil Blunsom. 2015. Learning to
In Proc. of
transduce with unbounded memory.
NeurIPS, pages 1828–1836.

Kelvin Guu, Panupong Pasupat, Evan Zheran Liu,
From language to pro-
and Percy Liang. 2017.
grams: Bridging reinforcement learning and maxi-
mum marginal likelihood. In Proc. of ACL.

Till Haug, Octavian-Eugen Ganea,

and Paulina
Neural multi-step reasoning
Grnarova. 2018.
for question answering on semi-structured tables.
ECIR.

Jonathan Herzig and Jonathan Berant. 2018. Decou-
pling structure and lexicon for zero-shot semantic
parsing. In Proc. of EMNLP.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen
tau Yih, and Xiaodong He. 2018. Natural language
to structured query generation via meta-learning. In
Proc. of NAACL.

Robin Jia and Percy Liang. 2016. Data recombination

for neural semantic parsing. In Proc. of ACL.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured attention networks.
In Proc. of ICLR.

Diederik P Kingma and Jimmy Ba. 2015. Adam:
In Proc. of

A method for stochastic optimization.
ICLR.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order uniﬁcation. In Proc. of EMNLP.

Chen Liang, Mohammad Norouzi, Jonathan Berant,
Quoc V Le, and Ni Lao. 2018. Memory augmented
policy optimization for program synthesis and se-
mantic parsing. In Proc. of NeurIPS.

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Proc. of ACL.

Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of EMNLP.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
In Proc. of
based neural machine translation.
EMNLP.

Chunchuan Lyu and Ivan Titov. 2018. Amr parsing as
graph prediction with latent alignment. In Proc. of
ACL.

Arvind Neelakantan, Quoc V Le, Martin Abadi, An-
drew McCallum, and Dario Amodei. 2017. Learn-
ing a natural language interface with neural pro-
grammer. In Proc. of ICLR.

Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and
Armando Solar-Lezama. 2019. Learning to infer
program sketches. Proc. of ICML.

P. Pasupat and P. Liang. 2015. Compositional semantic
parsing on semi-structured tables. In Proc. of ACL.

Panupong Pasupat and Percy Liang. 2016.

Inferring

logical forms from denotations. In Proc. of ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proc. of EMNLP.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In Proc. of ACL.

Tianze Shi, Kedar Tatwawadi, Kaushik Chakrabarti,
Yi Mao, Oleksandr Polozov, and Weizhu Chen.
text-to-sql
Incsql: Training incremental
2018.
arXiv
parsers with non-deterministic oracles.
preprint arXiv:1809.05054.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
In Proc. of
straints for semi-structured tables.
EMNLP.

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Gui-
hong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and
Ming Zhou. 2018. Semantic parsing with syntax-
and table-aware sql generation. In Proc. of ACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proc. of NeurIPS.

Oscar T¨ackstr¨om, Kuzman Ganchev, and Dipanjan
Das. 2015. Efﬁcient inference and structured learn-
ing for semantic role labeling. TACL.

Martin J Wainwright, Michael I Jordan, et al. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends R(cid:13) in Ma-
chine Learning, 1(1–2):1–305.

Chenglong Wang, Marc Brockschmidt, and Rishabh
Singh. 2017. Pointing out SQL queries from text.
Technical report, MSR.

Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proc. of ACL.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
arXiv preprint
without reinforcement
arXiv:1711.04436.

learning.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proc. of ACL.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online
segment to segment neural transduction. In Proc. of
EMNLP.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018. Typesql: Knowledge-based
type-aware neural text-to-sql generation. In Proc. of
NAACL.

Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to log-
ical form. In Proc. of EMNLP/CoNLL.

Luke S Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: structured
classiﬁcation with probabilistic categorial gram-
mars. In Proc. of UAI.

Yuchen Zhang, Panupong Pasupat, and Percy Liang.
2017. Macro grammars and holistic triggering for
efﬁcient semantic parsing. In Proc. of EMNLP.

Victor Zhong, Caiming Xiong, and Richard Socher.
Seq2sql: Generating structured queries
2017.
from natural language using reinforcement learning.
CoRR, abs/1709.00103.

A Grammars

We created our grammars following Zhang et al.
(2017) and Liang et al. (2018). Compared with
Liang et al. (2018), we additionally support dis-
junction(OR) and conjunction(AND). Some func-
tions are pruned based on their effect on coverage,
which is the proportion of questions that obtain at

least one consistent program. “same as” function
(Liang et al., 2018) is excluded since it introduces
too many spurious programs while contributing
little to the coverage. For the same reason, con-
junction(AND) is not used in WIKITABLEQUES-
TIONS and disjunction(OR) is not used in WIK-
ISQL.

We also include non-terminals of function types
in production rules (Krishnamurthy et al., 2017).
For instance, function “ROW → ﬁrst(LIST[ROW])”
selects the ﬁrst row from a list of rows and will
lead to the production rule “ROW → ﬁrst”. In the
paper, we eliminate the function type for simplic-
ity. Practically, we use two production rules to
represent the function: ROW → <ROW: LIST[ROW] > and
<ROW:LIST[ROW]> → ﬁrst , where < ROW: LIST[ROW] > is
an abstract function type.

B Search for Consistent Programs

We enumerate all possible programs in two stages
using the abstract and instantiation grammars. To
constrain the space and make the search process
tractable, we restrict the maximal number of pro-
duction rules for generating abstract programs dur-
ing the ﬁrst stage. It is based on the observation
that the abstract programs which can be success-
fully instantiated into correct programs are usu-
ally not very complex. In other words, the con-
sistent programs that are instantiated by long ab-
stract programs are very likely to be spurious. For
instance, programs like “select( previous( next( previ-
ous(argmax [all rows], column:silver) column:bronze)” are
unlikely to have a corresponding question. Specif-
ically, we set the maximal number of production
rules for generating abstract programs to 6 and
9, which leads to search time of around 7 and
10 hours for WIKISQL and WIKITABLEQUES-
TIONS respectively, using a single CPU. Note that
this needs to be done only once.

C Hyperparameters

Models used in WIKITABLEQUESTIONS and
WIKISQL share similar hyperparameters which
are listed in Table 5. Our input embeddings are
obtained by a linear projection from the ﬁxed
pre-trained embedding (Pennington et al., 2014).
Word Indicator refers to the indicator feature of
whether a word appears in the table; Column In-
dicator refers to the indicator feature of whether at
least one entity in a column appears in the ques-
tion. All MLPs mentioned in the paper have the

Hyperparameters

WTQ WIKISQL

Input Fixed Embedding Size
Input Linear Projection Size
POS Embedding Size
Production Rule Embedding Size
Column Type Embedding Size
Word Indictor Embedding Size
Column Indictor Embedding Size
Operator Embedding Size
Encoder Hidden Size
Encoder Dropout
Decoder Hidden Size
AP Encoder Hidden Size
AP Encoder Dropout
MLP Hidden Size
MLP Dropout

300
256
64
436
16
16
16
128
256
0.45
218
218
0.25
436
0.25

300
256
-
328
16
16
16
128
256
0.35
164
164
0.25
328
0.2

Table 5: Hyperparameters for WTQ (WIKITABLE-
QUESTIONS) and WIKISQL. AP Encoder is the en-
coder representing the abstract programs we generate.
.

same hidden size and dropout rate. During decod-
ing, we choose the top-6 abstract programs to in-
stantiate via beam search.

D Alignments

If a row slot is instantiated with the special con-
dition ‘all rows’, then it is possible that the se-
mantics of this slot is implicit. For instance, the
question “which driver completed the least num-
ber of laps? ” should ﬁrst be mapped to the ab-
stract program “select (argmin (#row slot, #column slot),
#column slot )” which is then instantiated to the cor-
rect program “select (argmin (all rows, column:laps) col-
umn:driver)”. The row slot in the abstract program
is instantiated with ‘all rows’, but this is not ex-
plicitly expressed in the question.

To make it compatible with our constraints in
Equation (6) and (7), we add a special token
“ALL ROW” at the end of each question.
If a
row slot is aligned with this token, then it is ex-
pected to be instantiated with the special condi-
tion ‘all rows’. Speciﬁcally, this special token is
mapped to a learnable vector during instantiations.
Our alignment needs to learn to align this special
token with a row slot if this row slot should be in-
stantiated with the condition ‘all rows’.

