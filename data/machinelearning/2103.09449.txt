1
2
0
2

r
a

M
7
1

]
l
l
a
h
-
s
e
m

.
t
a
m
-
d
n
o
c
[

1
v
9
4
4
9
0
.

3
0
1
2
:
v
i
X
r
a

Universal image segmentation for optical
identiﬁcation of 2D materials
Randy M. Sterbentz1, Kristine L. Haley1, and Joshua O. Island1,*

1Department of Physics and Astronomy, University of Nevada Las Vegas, Las Vegas, Nevada 89154, USA
*jisland@physics.unlv.edu

ABSTRACT

Machine learning methods are changing the way data is analyzed. One of the most powerful and widespread applications of
these techniques is in image segmentation wherein disparate objects of a digital image are partitioned and classiﬁed. Here
we present an image segmentation program incorporating a series of unsupervised clustering algorithms for the automatic
thickness identiﬁcation of two-dimensional materials from digital optical microscopy images. The program identiﬁes mono-
and few-layer ﬂakes of a variety of materials on both opaque and transparent substrates with a pixel accuracy of roughly 95%.
Contrasting with previous attempts, application generality is achieved through preservation and analysis of all three digital color
channels and Gaussian mixture model ﬁts to arbitrarily shaped data clusters. Our results provide a facile implementation of
data clustering for the universal, automatic identiﬁcation of two-dimensional materials exfoliated onto any substrate.

1 Introduction

Image segmentation has become an essential technique in ﬁelds from medical imaging1–3 and autonomous driving4 to robotic
perception5 and image compression6–8. Through unsupervised segmentation of large data sets, trained algorithms can recognize
and predict elements of new images. An appealing application of image segmentation is in the thickness identiﬁcation of
two-dimensional (2D) materials from their digital optical microscopy images. Current ﬂake detection methods rely heavily
on identiﬁcation by trained researchers, a human-learning process, in which ﬂakes are identiﬁed by their contrast difference
on a substrate after signiﬁcant trial and error. Automatic thickness identiﬁcation would relieve this tedious, time-consuming
screening process and possibly improve identiﬁcation accuracy.

The easiest implementation of image segmentation for 2D materials is by thresholding. This is performed by analyzing
image contrast, from reﬂectance or transmittance for example, and partitioning regions of an image based on contrast level
difference. This technique has been widely and successfully employed in the identiﬁcation and characterization of exfoliated 2D
materials9–19. Thresholding techniques, while easily implemented, suffer from inaccuracy when contrast differences become
relatively small, for example a single layer of graphene on a silicon/silicon dioxide (Si/SiO2) substrate, and can be highly
dependent on precise experimental conditions hindering universal application.

Recently, a variety of more advanced machine learning techniques have emerged to automate and improve the process of
identifying exfoliated 2D materials20–30. These include techniques based on neural networks and data clustering but have been
primarily applied to opaque substrates and almost entirely to standard Si/SiO2. Transparent substrates are commonly used for
exfoliation or experiments on 2D materials11, 31–35. A method that can be applied to identify the thickness of any material on
any substrate is highly desirable.

Here we present an open source program36 written in Python 3 to automatically identify the thickness of exfoliated 2D ﬂakes
which can be universally applied to different materials and substrates. We combine three well-established clustering techniques
to form a training script to segment layers of a ﬂake, manually label the layers, and then use that training to test thicknesses
of other ﬂakes. The program presents roughly a 95% pixel accuracy for graphene and transition metal dichalcogenides on
silicon/silicon dioxide and polydimethylsiloxane (PDMS) substrates. Importantly, no change to the program’s adjustable
parameters are needed to identify different materials on different substrates, allowing simple and universal application to any
material/substrate combination.

2 Overview of the program

An overview of the program applied to graphene on Si/SiO2 is shown in Figure 1. The training stage begins with a set of optical
microscopy images cropped to few-layer ﬂakes whose thicknesses are determined using optical contrast methods9. Figure 1(a)
shows a cropped optical image of a few-layer graphene ﬂake on a Si/SiO2 (300 nm SiO2) substrate with each layer thickness
labeled (1-4 layers). A scatter plot of the red, green, and blue (RGB) channel values (normalized to a range 0-1) for each pixel

 
 
 
 
 
 
Figure 1. Overview of the program composed of training and testing stages. (a-b) Raw optical image of a few-layer
graphene ﬂake on 300 nm of SiO2 (a) and its corresponding scatter plot of each pixel in RGB color space (b). The scatter plot
data points are colored to match their RGB value. (c-d) The result after preprocessing the image in panel (a). The preprocessing
reveals well-deﬁned clusters in the associated scatter plot (d). (e-f) A color plot of pixel-cluster association (e) and
corresponding scatter plot (f). Each pixel is colored based on its most probable data cluster identity. (g-h) Raw optical image of
few-layer graphene ﬂake with unknown thickness (g) used for testing and its corresponding RGB scatter plot (h). (i-j) Crop of
panel (g) around the ﬂake of interest (i) and corresponding scatter plot after the testing stage (j).

in the image is shown in Figure 1(b). The data points have been colored according to their RGB values. At this stage, the scatter
plot shows only broad features that can be generally associated with the substrate (pink) and few-layer graphene (purple) but no
clear correspondence to individual thicknesses can be made. The raw image is preprocessed using a bilateral ﬁlter to reduce
noise and a background normalization using a planar ﬁt. The result, after compression to roughly 10,000 total pixels, is shown
in Figure 1(c). Preprocessing reveals the individual clusters of data in the scatter plot (Figure 1(d)) associated with the substrate
and ﬂake layers.

The location and distribution of these clusters in RGB space are found using a series of unsupervised clustering techniques
summarized here and detailed further below. The centers are ﬁrst located using mean shift and density-based spatial clustering.
Once the centers are identiﬁed, ﬁt characteristics such as the weight, mean position, and distribution of each cluster are found
using a Gaussian mixture model. An image of the result of the ﬁtting algorithm is shown in Figure 1(e). The pixels in the
color plot have been colored according to the ﬁt results and the scatter plot (Figure 1(f)) shows these ﬁt assignments in more
detail. Once the cluster characteristics are extrapolated for several training images, a master catalogue is created that ties the ﬁt
clusters to the predetermined ﬂake thicknesses in our training images.

To determined accuracy of the training, we test the master catalogue on a set of images with identiﬁed thicknesses. An
example of the testing results for graphene on Si/SiO2 is shown in Figures 1(g-j). In the testing stage, untrained optical
images (Figure 1(g-h)) are ﬁrst preprocessed using the same procedure as in training but without cropping. The preprocessed
images are then checked against the master catalogue for ﬂake thickness assignment given the pixel location in RGB space.
Figure 1(i) shows the result (cropped to show detail of ﬂake of interest) with layer thicknesses identiﬁed by the color bar. The
associated scatter plot in Figure 1(j) shows the corresponding clusters and layer assignments. In the following section we detail
the implemenation of the clustering algorithms before presenting results of our program performed on other materials and
substrates.

3 Unsupervised clustering algorithms

Our training script incorporates three clustering algorithms to identify the center of the data clusters and ﬁt their distributions.
Without explicitly knowing the number of clusters (layers) in the image, the script begins with an unsupervised method of
determining the seed number. We use mean shift37–39 and density-based40, 41 algorithms to ﬁrst ﬁnd these cluster centers which
are then fed to a Gaussian mixture model for ﬁtting arbitrary ellipsoidal distributions.

Mean shift is an unsupervised machine learning algorithm that locates centers of high data density. The algorithm begins
by populating color space with an array of points, referred to as “mean points” ((cid:126)ρk). Figure 2(a) shows the same data as in
Figure 1(d) but with red closed circles indicating the initial positions of the equally spaced mean points (an 8 × 8 × 8 array).

2/S-2

Figure 2. Mean shift and DBSCAN clustering for identiﬁcation of cluster centers. (a) The same data as in Figure 1(d)
with red closed circles showing the initial positions of the mean points in the mean shift algorithm. (b) Scatter plot after one
cycle of mean shift; outlier mean points have been deleted and others have moved towards their local density maxima. (c) The
ﬁnal state of the mean points after they have converged to their maxima. (d) The RGB pixels plotted with the identiﬁed cluster
centers (colored closed circles) after the DBSCAN algorithm.

The next step groups all data points within a deﬁned radius (ε) of each mean point together. We deﬁne ε to be just large enough
to overlap with its nearest neighbors. The average location of the data pixels within ε of a given mean point becomes the new
position of that mean point ((cid:126)ρ (cid:48)

k) after one iteration of the algorithm. This is calculated by:

(cid:126)ρ (cid:48)

k =

1
M ∑

i

(cid:126)xi for |(cid:126)xi −(cid:126)ρk| < ε,

(1)

where (cid:126)xi is the position of each data point and M is the total number of data points within ε of (cid:126)ρk. In this way, each mean point
gradually shifts towards higher densities of data. Figure 2(b) shows one iteration of the algorithm. Several points have moved
to their new mean positions according to the data within ε of each (cid:126)ρk.

Mean shift is computationally slow, having to calculate the distance between every data point (≈10,000 pixels) and every
mean point (initially 512). To increase efﬁciency, mean points that have no data within ε after the ﬁrst cycle, and thus make no
contribution towards a data cluster, are deleted. Additionally, mean points may approach their local maximum at different rates.
Per mean point, as soon as the number of data points within their radius starts to decrease, they are turned off and no longer
involved in future calculations. Figure 2(c) shows the ﬁnal state of the algorithm where all mean points have converged to their
local density maxima. After this, outliers are removed before moving to the next algorithm.

Once mean shift is complete, several mean points will themselves be clustered in color space and some mean points will
have converged to outliers. Due to the ellipsoidal shape of the clusters in RGB space after preprocessing, the mean points will
tend to lie along lines. An efﬁcient algorithm for grouping these lines is Density-Based Spatial Clustering of Applications with
Noise (DBSCAN)40, 41. DBSCAN groups data together by following the trajectory of nearby points. The algorithm starts by
“visiting” a random mean point. A radius (we ﬁnd ε/2 works well) around it is checked for other mean points. If none are
found, the starting point is labeled an outlier. If there are neighbors, they are grouped together. One of the other points in this
group is visited next, checking the same radius around itself to ﬁnd new points to add to the group. This repeats until no new
points are added to the group and every point within the group has been visited. Once the group is ﬁnished, a new group starts
at a randomly chosen mean point and the process repeats. The centers of each group are found by averaging their respective
mean points. Figure 2(d) shows the result after running the DBSCAN algorithm on the mean points in Figure 2(c).

The combination of mean shift and DBSCAN presents an unsupervised method of determining how many clusters are in a
given image and their centers. Following this step, this information can be used to seed a more powerful clustering technique
for data with ellipsoidal distributions. The popular K-means clustering technique42–44 for example is undesirable here as it
assumes spherical clusters. Instead, we use a multivariate Gaussian Mixture Model (GMM)2, 45–49 that allows ﬁtting of data
with arbitrary normal distributions. This expands application of the program by automatically handling new materials and

3/S-2

substrate combinations that may have different cluster distributions in RGB space.

In the GMM, each ﬁtting ellipsoid has three characteristics developing throughout the process: the weight (φk, deﬁning the
number of data points near ellipsoid k), the centroid ((cid:126)µk, deﬁning the mean of the data points belonging to ellipsoid k), and
the covariance matrix (Σk, deﬁning the shape and orientation of ellipsoid k in RGB space). These characteristics are used to
calculate the probability γik of a data point (cid:126)xi belonging to ellipsoid k. This probability is given by:

γik =

φkN ((cid:126)xi,(cid:126)µk, Σk)
j=1 φ jN ((cid:126)xi,(cid:126)µ j, Σ j)

∑K

,

(2)

where K is the total number of clusters and N is the three-variable (for 3-dimensional RGB space) Gaussian distribution given
by:

N ((cid:126)xi,(cid:126)µk, Σk) = [(2π)3|Σk|e((cid:126)xi−(cid:126)µk)T Σ−1

k ((cid:126)xi−(cid:126)µk)]− 1

2

The weights, means, and covariance matrices used in these relations are calculated through:

φk =

1
N

N
∑
i=1

γik,

(cid:126)µk =

ΣN
i=1γik(cid:126)xi
ΣN
i=1γik

,

Σk =

∑N

i=1 γik|(cid:126)xi −(cid:126)µk|2
i=1 γik

∑N

.

(3)

(4)

(5)

(6)

First we initialize each of the ﬁtting ellipsoids by setting all initial weights to 1/K. The centroids are taken directly from the
results of DBSCAN (cid:126)µ = (cid:126)ρ. The covariance matrices are initialized from the centroids using Equation 6 with γik = 1. Figure
3(a) shows the initialization of the ﬁtting ellipsoids for our example few-layer graphene data set from Figure 1(d) and Figure 2.
The ellipsoids have been scaled to a 95% conﬁdence level.

An unsupervised machine learning algorithm, referred to as expectation-maximization (EM), is used to further optimize
the ellipsoid parameters and ﬁt the data. The expectation step determines γik based on the initialized weights, centroids, and
covariance matrices calculated above. The maximization step uses these probabilities to re-calculate each cluster’s weight,
centroid, and covariance matrix. These two steps iterate and gradually the ellipsoid parameters converge. Figure 3(b) shows
the algorithm results after 2 cycles and Figure 3(c) shows the results after 30 cycles. After 30 cycles, the ellipsoids resemble
the distributions of the data with several small tight ellipsoids corresponding to the substrate and 1-4 layers of graphene, and
two larger ellipsoids (purple and blue) accounting for noise. The max change of all cluster’s weights between maximization
steps (∆φk < 0.0001) is used to deﬁne convergence and end the algorithm. Figure 3(d) shows the results of the algorithm after
convergence (total 61 cycles) for this data set. Note that the large purple and blue ellipsoids are a product of over-ﬁtting the
data (ﬁtting 7 ellipsoids to 5 data clusters). These ellipsoids do not contribute to the master catalogue, but are important for
ﬁtting data points associated with thicker layers (> 4) and outliers. The over ﬁtting also allows the primary ellipsoids to conﬁne
themselves to the core of their data clusters. Once convergence has been reached, only ellipsoids that ﬁt well to known layer
thicknesses are added to a catalogue.

The training process is repeated for multiple ﬂakes of the same material and substrate (we trained around 5 for each
material/substrate combination), saving their ellipsoid characteristics into the same catalogue. A master catalogue is then
created by averaging together the characteristics of ellipsoids with like-thickness. This master catalogue is the tool with which
we can test other images to determine their ﬂake layer thicknesses (Figure 1(i-j)).

4 General application to other materials and substrates

Our script can be universally applied to the identiﬁcation of other 2D material thicknesses on opaque and transparent substrates.
This generality is achieved by analyzing all three dimensions of the color-space data and ﬁtting the resulting clusters of arbitrary
shape with our GMM-EM algorithm. Importantly, no change in the adjustable parameters (ε or GMM convergence) are required
for the following results.

Figure 4 displays the power of this generality by identifying the layer thickness of two additional materials, molybdenum
disulﬁde (MoS2) and molybdenum diselenide (MoSe2), on opaque (Si/SiO2) and transparent (polydimethylsiloxane (PDMS))

4/S-2

Figure 3. Cluster ﬁtting with a Gaussian mixture model (GMM). The scatter plot from Figure 1(d) is superimposed with
95% conﬁdence ellipsoids based on the ﬁt characteristics of the GMM-EM algorithm. (a) The initialized ellipsoids show little
correspondence to the underlying data. (b) After two cycles of expectation-maximization, the ellipsoids better resemble the
data clusters. (c) After 30 cycles, some ellipsoids have nearly converged on their data clusters. (d) The convergence condition is
reached after 61 cycles.

substrates. MoS2 on Si/SiO2 (Figure 4(a-e)) presents clusters very similar to those of graphene on Si/SiO2 but they are separated
further in RGB space (Figure 4(b)). Further training for this material/substrate combination would improve our testing results
which only identify layer thicknesses of 1 and 2 (Figure 4(d-e)). From the covariance matrices we note that while all the data
clusters are technically triaxial ellipsoids (none of the semi-axes are equal), the clusters for materials on Si/SiO2 are roughly
prolate spheroids with one axis (blue) an order of magnitude larger than the other two semi-axes (red and green).

MoS2 on PDMS (Figure 4(f-j)) presents clusters again extending along the blue axis, though not as strongly as materials on
Si/SiO2. The clusters are similarly well-separated in RGB space as they are for MoS2 on Si/SiO2. Testing for this set identiﬁes
monolayer, bilayer and trilayer thicknesses (Figure 4(j)). Finally, MoSe2 on PDMS presents the most spherical ellipsoids of our
investigation still slightly extending along blue, (Figure 4(k-o)) and mono- through trilayer thicknesses are easily identiﬁed
(Figure 4(o)).

5 Discussion

Our investigation focuses on the development of a program that can be universally applied to different 2D materials and sub-
strates. This requirement invariably introduces computation time when compared with other recent segmentation methods22, 50.
The training time, for example, reported in Ref.50 for the entire program is roughly 31 hours. Computation times for the
training stage of our program depend on the image composition. A single layer image can take about ten minutes but images
with multilayer ﬂakes (more clusters) take as long as 5 hours. Our program results here are from training sets of roughly 10
images corresponding to about 10 hour computation time. However, this is a single event time cost because once the master
catalogue is trained for a particular material and substrate combination, it can then be used repeatedly in the testing step, which
is more efﬁcient.

Image testing requires roughly one minute to identify layer thicknesses of new images. Computation time is sufﬁciently short
for testing because image pixels are simply compared with the master catalogue. This time would allow in-situ identiﬁcation
of ﬂakes from images taken by human inspection of a substrate. The time spent scanning between images can take several
minutes. The time may also be sufﬁcient for an automated scanning system such as that presented in ref.51. Improvements in
computation time may be sought through further image compression or possibly reducing the testing step to two dimensions of
the three-dimensional RGB space, possibly blue and either green or red, similar to algorithms presented in ref.23. Although, the
dropped color dimension would have to be identiﬁed for a particular material/substrate combination.

For each material/substrate combination investigated in this study, the pixel accuracy was determined by creating a ground
truth image and comparing it, pixel-by-pixel, with the testing images (see Figure S1 in the supplemental materials for details).
Pixel accuracy was slightly better for materials on PDMS but overall the program achieves an average accuracy of 95% for the

5/S-2

Figure 4. General thickness identiﬁcation of 2D materials on opaque and transparent substrates. (a-c) An example
training process for MoS2 ﬂakes exfoliated onto Si/SiO2 substrates. (a) Raw optical image before preprocessing. (b) RGB
scatter plot of the identiﬁed clusters with pixels colored according to the cluster they belong to. (c) Reconstructed image of the
MoS2 ﬂake in (a) after the training step. (d-e) Testing process for MoS2 on Si/SiO2. (d) Raw optical image of an MoS2 ﬂake on
Si/SiO2. (e) Layer identiﬁcation after testing the image in (d). (f-h) An example training process for MoS2/PDMS. (i-j) An
example testing process for MoS2/PDMS. (k-m) An example training process for MoSe2/PDMS. (n-o) An example testing
process for MoSe2/PDMS.

materials and substrates investigated in this study. This pixel accuracy is comparable to that achieved in studies based on much
larger training sets. Ref.50 reports pixel accuracy of 97% from a training set of 917 images. Based on these results, normalized
confusion matrices for each combination were calculated showing the individual layer accuracy as well. Finally, we note that a
clear advantage to our approach is the simplicity of our program which relies on well-known and proven clustering techniques
with relatively high pixel accuracy from small training sets.

6 Conclusion

Summarizing, we have presented a code for the automatic identiﬁcation of ﬂake thicknesses that can be universally applied to a
variety of 2D materials and substrates. The algorithm analyzes data clusters in RGB space of preprocessed optical microscopy
images. It can accurately identify mono- and few-layer thicknesses with a pixel accuracy of 95%. We anticipate the program
will be of use for a wide variety of materials and substrates for the continued interest and investigation into the properties and
characteristics of 2D materials.

7 Acknowledgements

The authors thank Jeffery Cloninger for technical support and Najme S. Taghavi and Dr. Andres Castellanos-Gomez for optical
images of materials on PDMS.

8 Author contributions statement

J.O.I. conceived the project. R.M.S. coded the program with guidance from J.O.I.. R.M.S. and K.L.H. provided optical images
of 2D materials. All authors wrote and reviewed the manuscript.

6/S-2

9 References

References

1. Clarke, L. P. et al. MRI segmentation: Methods and applications. Magn. Reson. Imaging 13, 343–368, DOI: 10.1016/

0730-725X(94)00124-L (1995).

2. Ji, Z. et al. Fuzzy Local Gaussian Mixture Model for Brain MR Image Segmentation. IEEE Transactions on Inf. Technol.

Biomed. 16, 339–347, DOI: 10.1109/TITB.2012.2185852 (2012).

3. Forouzanfar, M., Forghani, N. & Teshnehlab, M. Parameter optimization of improved fuzzy c-means clustering algorithm
for brain MR image segmentation. Eng. Appl. Artif. Intell. 23, 160–168, DOI: 10.1016/j.engappai.2009.10.002 (2010).

4. Feng, D. et al. Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets,
Methods, and Challenges. IEEE Transactions on Intell. Transp. Syst. 1–20, DOI: 10.1109/TITS.2020.2972974 (2020).

5. Schwarz, M., Milan, A., Periyasamy, A. S. & Behnke, S. RGB-D object detection and semantic segmentation for
autonomous manipulation in clutter. The Int. J. Robotics Res. 37, 437–451, DOI: 10.1177/0278364917713117 (2018).

6. Ratakonda, K. & Ahuja, N. Lossless image compression with multiscale segmentation. IEEE Transactions on Image

Process. 11, 1228–1237, DOI: 10.1109/TIP.2002.804528 (2002).

7. Sanderson, H. & Crebbin, G. Image segmentation for compression of images and image sequences. IEE Proc. - Vision,

Image Signal Process. 142, 15–21, DOI: 10.1049/ip-vis:19951681 (1995).

8. Shen, L. & Rangayyan, R. A segmentation-based lossless image coding method for high-resolution medical image

compression. IEEE Transactions on Med. Imaging 16, 301–307, DOI: 10.1109/42.585764 (1997).

9. Li, H. et al. Rapid and Reliable Thickness Identiﬁcation of Two-Dimensional Nanosheets Using Optical Microscopy. ACS

Nano 7, 10344–10353, DOI: 10.1021/nn4047474 (2013).

10. Bing, D. et al. Optical contrast for identifying the thickness of two-dimensional materials. Opt. Commun. 406, 128–138,

DOI: 10.1016/j.optcom.2017.06.012 (2018).

11. Taghavi, N. S. et al. Thickness determination of MoS2, MoSe2, WS2 and WSe2 on transparent stamps used for deterministic

transfer of 2D materials. Nano Res. 12, 1691–1695, DOI: 10.1007/s12274-019-2424-6 (2019).

12. Ni, Z. H. et al. Graphene Thickness Determination Using Reﬂection and Contrast Spectroscopy. Nano Lett. 7, 2758–2763,

DOI: 10.1021/nl071254m (2007).

13. Jung, I. et al. Simple Approach for High-Contrast Optical Imaging and Characterization of Graphene-Based Sheets. Nano

Lett. 7, 3569–3575, DOI: 10.1021/nl0714177 (2007).

14. Wang, X., Zhao, M. & Nolte, D. D. Optical contrast and clarity of graphene on an arbitrary substrate. Appl. Phys. Lett. 95,

081102, DOI: 10.1063/1.3212735 (2009).

15. Zhang, H. et al. Optical thickness identiﬁcation of transition metal dichalcogenide nanosheets on transparent substrates.

Nanotechnology 28, 164001, DOI: 10.1088/1361-6528/aa6133 (2017).

16. Yu, Y. et al. Investigation of multilayer domains in large-scale CVD monolayer graphene by optical imaging. J. Semicond.

38, 033003, DOI: 10.1088/1674-4926/38/3/033003 (2017).

17. Wang, Y. Y. et al. Thickness identiﬁcation of two-dimensional materials by optical imaging. Nanotechnology 23, 495713,

DOI: 10.1088/0957-4484/23/49/495713 (2012).

18. Rubio-Bollinger, G. et al. Enhanced Visibility of MoS2, MoSe2, WSe2 and Black Phosphorus: Making Optical Identiﬁca-
tion of 2D Semiconductors Easier. Electronics 4, 847–856, DOI: 10.3390/electronics4040847 (2015). 1511.05474.

19. Jessen, B. S. et al. Quantitative optical mapping of two-dimensional materials. Sci. Reports 8, 6381, DOI: 10.1038/

s41598-018-23922-1 (2018).

20. Yang, J. & Yao, H. Automated identiﬁcation and characterization of two-dimensional materials via machine learning-based
processing of optical microscope images. Extrem. Mech. Lett. 39, 100771, DOI: 10.1016/j.eml.2020.100771 (2020).

21. Wu, B., Wang, L. & Gao, Z. A two-dimensional material recognition image algorithm based on deep learning. In
2019 International Conference on Information Technology and Computer Application (ITCA), 247–252, DOI: 10.1109/
ITCA49981.2019.00061 (2019).

22. Masubuchi, S. et al. Deep-learning-based image segmentation integrated with optical microscopy for automatically

searching for two-dimensional materials. npj 2D Mater. Appl. 4, 1–9, DOI: 10.1038/s41699-020-0137-z (2020).

7/S-2

23. Masubuchi, S. & Machida, T. Classifying optical microscope images of exfoliated graphene ﬂakes by data-driven machine

learning. npj 2D Mater. Appl. 3, 1–7, DOI: 10.1038/s41699-018-0084-0 (2019).

24. Lin, X. et al. Intelligent identiﬁcation of two-dimensional nanostructures by machine-learning optical microscopy. Nano

Res. 11, 6316–6324, DOI: 10.1007/s12274-018-2155-0 (2018).

25. Li, Y. et al. Rapid identiﬁcation of two-dimensional materials via machine learning assisted optic microscopy. J.

Materiomics 5, 413–421, DOI: 10.1016/j.jmat.2019.03.003 (2019).

26. Greplova, E. et al. Fully Automated Identiﬁcation of Two-Dimensional Material Samples. Phys. Rev. Appl. 13, 064017,

DOI: 10.1103/PhysRevApplied.13.064017 (2020).

27. Cellini, F., Lavini, F., Berger, C., de Heer, W. & Riedo, E. Layer dependence of graphene-diamene phase transition in
epitaxial and exfoliated few-layer graphene using machine learning. 2D Mater. 6, 035043, DOI: 10.1088/2053-1583/ab1b9f
(2019).

28. Hu, X., Qiu, C. & Liu, D. Rapid thin-layer ws 2 detection based on monochromatic illumination photographs. Nano Res.

1–6 (2020).

29. Dong, X. et al. 3d deep learning enables accurate layer mapping of 2d materials. ACS nano (2021).

30. Aleithan, S. H. & Mahmoud-Ghoneim, D. Toward automated classiﬁcation of monolayer versus few-layer nanomaterials

using texture analysis and neural networks. Sci. Reports 10, 1–8 (2020).

31. Castellanos-Gomez, A. et al. Deterministic transfer of two-dimensional materials by all-dry viscoelastic stamping. 2D

Mater. 1, 011002, DOI: 10.1088/2053-1583/1/1/011002 (2014).

32. Island, J. O. et al. Precise and reversible band gap tuning in single-layer MoSe2 by uniaxial strain. Nanoscale 8, 2589–2593,

DOI: 10.1039/C5NR08219F (2016).

33. Lippert, S. et al. Inﬂuence of the substrate material on the optical properties of tungsten diselenide monolayers. 2D Mater.

4, 025045, DOI: 10.1088/2053-1583/aa5b21 (2017).

34. Nguyen, D. C. et al. Visibility of hexagonal boron nitride on transparent substrates. Nanotechnology 31, 195701, DOI:

10.1088/1361-6528/ab6bf4 (2020).

35. Chu, H. et al. Linear Magnetoelectric Phase in Ultrathin ${\mathrm{MnPS}}_{3}$ Probed by Optical Second Harmonic

Generation. Phys. Rev. Lett. 124, 027601, DOI: 10.1103/PhysRevLett.124.027601 (2020).

36. Sterbentz, R. M. Universal image segmentation. https://github.com/islandlab-unlv/Universal-image-segementation (2020).

37. Comaniciu, D. & Meer, P. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern

Analysis Mach. Intell. 24, 603–619, DOI: 10.1109/34.1000236 (2002).

38. Zhou, Y.-m., Jiang, S.-y. & Yin, M.-l. A Region-Based Image Segmentation Method with Mean-Shift Clustering
Algorithm. In 2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery, vol. 2, 366–370, DOI:
10.1109/FSKD.2008.363 (2008).

39. Zhou, H., Wang, X. & Schaefer, G. Mean Shift and Its Application in Image Segmentation.

In Kwa´snicka, H. &
Jain, L. C. (eds.) Innovations in Intelligent Image Analysis, Studies in Computational Intelligence, 291–312, DOI:
10.1007/978-3-642-17934-1_13 (Springer, Berlin, Heidelberg, 2011).

40. Ester, M., Kriegel, H.-P., Sander, J. & Xu, X. A density-based algorithm for discovering clusters in large spatial databases
with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD’96,
226–231 (AAAI Press, Portland, Oregon, 1996).

41. Xu, R. & Wunsch, D. Survey of clustering algorithms. IEEE Transactions on Neural Networks 16, 645–678, DOI:

10.1109/TNN.2005.845141 (2005).

42. Celebi, M. E., Kingravi, H. A. & Vela, P. A. A comparative study of efﬁcient initialization methods for the k-means

clustering algorithm. Expert. Syst. with Appl. 40, 200–210, DOI: 10.1016/j.eswa.2012.07.021 (2013).

43. Dhanachandra, N., Manglem, K. & Chanu, Y. J. Image Segmentation Using K -means Clustering Algorithm and Subtractive

Clustering Algorithm. Procedia Comput. Sci. 54, 764–771, DOI: 10.1016/j.procs.2015.06.090 (2015).

44. Dhanachandra, N. & Chanu, Y. J. A Survey on Image Segmentation Methods using Clustering Techniques. Eur. J. Eng.

Res. Sci. 2, 15–20, DOI: 10.24018/ejers.2017.2.1.237 (2017).

45. Gupta, L. & Sortrakul, T. A gaussian-mixture-based image segmentation algorithm. Pattern Recognit. 31, 315–325, DOI:

10.1016/S0031-3203(97)00045-9 (1998).

8/S-2

46. Nguyen, T. M. & Wu, Q. M. J. Dirichlet Gaussian mixture model: Application to image segmentation. Image Vis. Comput.

29, 818–828, DOI: 10.1016/j.imavis.2011.09.001 (2011).

47. Permuter, H., Francos, J. & Jermyn, I. A study of Gaussian mixture models of color and texture features for image

classiﬁcation and segmentation. Pattern Recognit. 39, 695–706, DOI: 10.1016/j.patcog.2005.10.028 (2006).

48. Ribeiro, H. L. & Gonzaga, A. Hand Image Segmentation in Video Sequence by GMM: A comparative analysis. In 2006
19th Brazilian Symposium on Computer Graphics and Image Processing, 357–364, DOI: 10.1109/SIBGRAPI.2006.23
(2006).

49. Santosh, D., Venkatesh, P. N., Poornesh, P., Rao, L. N. & Kumar, N. Tracking Multiple Moving Objects Using Gaussian

Mixture Model (2013).

50. Han, B. et al. Deep-Learning-Enabled Fast Optical Identiﬁcation and Characterization of 2D Materials. Adv. Mater. n/a,

2000953, DOI: 10.1002/adma.202000953 (2020).

51. Masubuchi, S. et al. Autonomous robotic searching and assembly of two-dimensional crystals to build van der Waals

superlattices. Nat. Commun. 9, 1413, DOI: 10.1038/s41467-018-03723-w (2018).

9/S-2

10 Supplemental Materials: Universal image segmentation for optical identiﬁcation of

2D materials

1 Python code and packages

The software was written in Python 3.7.3 and includes the following packages: scipy==1.5.2, matplotlib==3.0.3, numpy==1.16.2,
and opencv-python==4.2.0.34. The full Python code is available at https://github.com/islandlab-unlv/Universal-image-
segementation.

2 Pixel accuracy and confusion matrices

In order to measure the ﬁdelity of our program, we created confusion matrices by comparing the testing results (predictions)
with manually recolored images based on layer thickness (ground truths). Figure S1(a) shows a ground truth image for a
graphene ﬂake on Si/SiO2 and Figure S1(b) shows the testing result using the same colorscale. Figure S1(c) shows the raw
counts for the confusion matrix of graphene on Si/SiO2. Element i j in the confusion matrix is the number of pixels predicted
to be i-layer that are known to be j-layer. The diagonal thus is where the program predicted the layer thickness correctly.
We calculate the pixel accuracy by taking the ratio of the trace of the matrix with the sum of all elements in the matrix. For
graphene on SiO2, we calculate a pixel accuracy of 96.7%. For the other test samples in Figure S1, we calculate the following
pixel accuracies: MoS2/SiO2 in Figure S1(e-h) (94.7%), MoS2/PDMS in Figure S1(i-l) (98.0%), MoSe2/PDMS in Figure
S1(m-p) (96.7%).

Normalizing a confusion matrix by the true labels elucidates how well the program can predict each layer thickness.
Columns 0-2 of Figure S1(d), for example, state we can successfully identify 99.0% of the Si/SiO2 substrate, 97.3% of
the monolayer graphene regions, and 94.3% of the bilayer graphene regions. It appears to falter with tri- and quadlayer
identiﬁcation, but this is a result of the necessary preprocessing on the test image. Despite qualities of bilateral ﬁltering as an
edge-preserving ﬁlter, we always had some blurring at the ﬂake-thickness boundaries while applying it. As a result, narrow
strips of a layer thickness could be blurred away, as is the case with the thin stripes of trilayer present in the graphene/SiO2 test
image (light grey stripes at the bottom left of panels (a) and (b) of Figure S1). These confusion matrices were created off of a
single test image per material/substrate tested, and are beholden to the biases that arise from those individual samples. With
ﬁner tuning of the bilateral ﬁlter and a larger test set, this blurring should be mitigated.

S-1/S-2

Figure S1. Pixel accuracy testing. (a-d) Ground truth (a) and prediction (b) images from testing for a graphene ﬂake on a
Si/SiO2 substrate. Raw pixel count confusion matrix for graphene on SiO2 (c), and the same confusion matrix normalized by
column (normalized to true label) (d). (e-h) Ground truth (e) and prediction (f) images from testing for a MoS2 ﬂake on a
Si/SiO2 substrate. Raw pixel count confusion matrix for MoS2 on SiO2 (g), and the same confusion matrix normalized by
column (normalized to true label) (h). (i-l) Ground truth (i) and prediction (j) images from testing for a MoS2 ﬂake on a PDMS
substrate. Raw pixel count confusion matrix for MoS2 on PDMS (k), and the same confusion matrix normalized by column
(normalized to true label) (l). (m-p) Ground truth (m) and prediction (n) images from testing for a MoSe2 ﬂake on a PDMS
substrate. Raw pixel count confusion matrix for MoSe2 on PDMS (o), and the same confusion matrix normalized by column
(normalized to true label) (p).

S-2/S-2

