1
2
0
2

v
o
N
1

]
L
C
.
s
c
[

1
v
3
9
1
1
0
.
1
1
1
2
:
v
i
X
r
a

Transformers for prompt-level EMA non-response prediction

SUPRIYA NAGESH, Georgia Insistute of Technology, USA
ALEXANDER MORENO, Georgia Institute of Technology, USA
STEPHANIE M. CARPENTER, University of Michigan, USA
JAMIE YAP, University of Michigan, USA
SOUJANYA CHATTERJEE, University of Memphis, USA
STEVEN LLOYD LIZOTTE, University of Utah, USA
NENG WAN, University of Utah, USA
SANTOSH KUMAR, University of Memphis, USA
CHO LAM, University of Utah, USA
DAVID W. WETTER, University of Utah, USA
INBAL NAHUM-SHANI, University of Michigan, USA
JAMES M. REHG, Georgia Institute of Technology, USA

Ecological Momentary Assessments (EMAs) are an important psychological data source for measuring current cognitive states,
affect, behavior, and environmental factors from participants in mobile health (mHealth) studies and treatment programs.
Non-response, in which participants fail to respond to EMA prompts, is an endemic problem. The ability to accurately predict
non-response could be utilized to improve EMA delivery and develop compliance interventions. Prior work has explored
classical machine learning models for predicting non-response. However, as increasingly large EMA datasets become available,
there is the potential to leverage deep learning models that have been effective in other fields. Recently, transformer models
have shown state-of-the-art performance in NLP and other domains. This work is the first to explore the use of transformers for
EMA data analysis. We address three key questions in applying transformers to EMA data: 1. Input representation, 2. encoding
temporal information, 3. utility of pre-training on improving downstream prediction task performance. The transformer
model achieves a non-response prediction AUC of 0.77 and is significantly better than classical ML and LSTM-based deep
learning models. We will make our a predictive model trained on a corpus of 40K EMA samples freely-available to the research
community, in order to facilitate the development of future transformer-based EMA analysis works.
Manuscript under review at ACM IMWUT

ACM Reference Format:
Supriya Nagesh, Alexander Moreno, Stephanie M. Carpenter, Jamie Yap, Soujanya Chatterjee, Steven LLoyd Lizotte, Neng
Wan, Santosh Kumar, Cho Lam, David W. Wetter, Inbal Nahum-Shani, and James M. Rehg. 2022. Transformers for prompt-level
EMA non-response prediction. 1, 1 (January 2022), 20 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Authors’ addresses: Supriya Nagesh, Georgia Insistute of Technology, USA, snagesh7@gatech.edu; Alexander Moreno, Georgia Institute
of Technology, USA; Stephanie M. Carpenter, University of Michigan, USA; Jamie Yap, University of Michigan, USA; Soujanya Chatterjee,
University of Memphis, USA; Steven LLoyd Lizotte, University of Utah, USA; Neng Wan, University of Utah, USA; Santosh Kumar, University
of Memphis, USA; Cho Lam, University of Utah, USA; David W. Wetter, University of Utah, USA; Inbal Nahum-Shani, University of Michigan,
USA; James M. Rehg, Georgia Institute of Technology, USA.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from
permissions@acm.org.
© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/1-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: January 2022.

 
 
 
 
 
 
2

• Nagesh, et al.

1

INTRODUCTION

Mobile health (mHealth) technology is a promising tool for health behavior change and maintenance with a
broad array of applications, including smoking cessation [33], physical activity [20], stress management [24] and
medication adherence [30]. mHealth data sources such as wearable sensors, self-reports, GPS, etc. provide key
insights into the contextual and behavioral factors that influence health outcomes, through the ability to collect
data from participants in real-time in the field environment. A particularly valuable source of data comes from
ecological momentary assessments (EMAs), in which participants answer questions about their mental state,
behaviors, and other factors by completing questionnaires, typically multiple times per day. EMA data provides
unique insights which are difficult to glean from other sensing modalities, and is widely-used as a result. It can
be used to assess the risk of adverse behaviors, trigger interventions, or estimate treatment effects.

A major challenge in EMA data collection is participant non-response, which arises when users fail to complete
a survey when prompted. Non-response is problematic for three reasons. First, it reduces the statistical power
when testing hypotheses using mHealth data. Second, if the non-response is systematic, then it is likely to be
missing not at random (MNAR), a form of bias which is difficult to correct for. Third, missing EMA samples make
it more challenging to assess time-varying contextual variables such as emotions, environments, and behaviors.
Non-response can have many causes. For example, participants who are driving or in a meeting may not be
available to respond to the prompt. Non-response can also arise due to a lack of participant engagement or
motivation. A predictive model that could identify moments when non-response is likely would be a useful tool
in improving EMA yields. A predictive model could be used by the EMA scheduler to deliver prompts when
the likelihood of a response is higher, and it could also be used to trigger additional incentives or compliance
interventions designed to improve the EMA response rate.

The goal of this paper is to develop a machine learning model that can predict the future risk of non-response
from the history of EMA responses. Prior work on studying non-response [2, 8, 27, 28, 38, 44] has focused on
identifying effective predictors using classical machine learning methods. Given the variety of factors that can
contribute to nonresponse, a data-driven feature learning approach is attractive. Only a few prior works have used
deep learning (DL) for EMA data modeling [26, 41], in contrast to other mHealth data types such as accelerome-
try [3, 14, 34, 47], and no prior works have used DL for non-response prediction. Recently, transformers [46] have
emerged as a powerful new class of tools for modeling sequential observations. Following their initial success in
NLP [9, 31], transformers have proven effective in computer vision[11] [32] [17] and speech recognition [29],
among other domains. Sequences of EMA observations differ significantly from NLP and time series data in that
the arrival times are irregularly-sampled and important to model (e.g. EMA responses which are closer together
in time are more likely to be correlated). We are the first to explore the use of transformers for EMA data analysis in
general, and non-response prediction in particular.

We address three issues in applying transformers to EMA data modeling. The first is the choice of input
representation. While many applications require domain-specific input embeddings (e.g. word embeddings in the
case of NLP), we find that the fixed length EMA response vector itself is an effective input representation. The
second issue has to do with the method for encoding observation times. Positional encoding introduced in [46]
adds a vector to each input embedding which provides a global encoding of the position of each word. Similarly,
we find that explicitly encoding the EMA times along with the responses improves performance. However, we
find that concatenating the positional encoding is more effective than adding it. The third issue is the utility of
pre-training in improving prediction performance. The BERT architecture for NLP tasks [9] demonstrated the
effectiveness of pre-training a transformer-based model on a large unlabeled corpus prior to fine-tuning it with
labels on a smaller task-specific training dataset. We designed a self-supervised pre-training task based on EMA
imputation and evaluated its effectiveness for non-response prediction. We found that pre-training produced a
small performance benefit which was not statistically-significant. We hypothesize that this approach may be

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

3

more effective in the future as larger EMA datasets become available. We present visualizations of the learned
transformer representation that suggest that it encodes structure in the EMA response data which is meaningful
for non-response prediction.

In summary, this paper makes the following contributions:

• This is the first work to explore the utility of transformer models for sequences of EMA response data.
We present a transformer architecture for predicting non-response to EMA prompts using the history of
EMA responses. The transformer model achieves an AUC of 0.77 for predicting future non-response and is
significantly more accurate than both classical ML models and DL models based on the LSTM architecture.
• We present the design decisions that yield effective transformers for EMA sequence analysis, investigating
input and positional encoding methods and identifying the most effective strategies. We present visualiza-
tions that illustrate the ability of the transformer to learn meaningful representations for non-response
prediction.

• We design a self-supervised pre-training task to learn the structure within EMA sequences. We evaluate
the utility of pre-training and report a modest performance gain which is not statistically-significant.

2 RELATED WORK

There are three bodies of prior work which are most closely-related to this paper: 1) Analysis and prediction
of EMA non-response, as well as the related topics of interruptability and availability; 2) Use of deep learning
models to analyze EMA data; and 3) Transformer models for electronic health record (EHR) data, which shares
with our task the need to model irregularly-sampled data. We discuss each of these topics in detail.

2.1 Analyzing and predicting EMA non-response

A significant body of prior work analyzes the factors that are related to non-response to EMA prompts. [8, 38]
identify the factors that have a significant effect on non-response (which they term compliance, adherence,
engagement). [37, 44] study EMA response rates and determine the feasibility of using EMA as a research tool
based on the response rates. [37] further underscores the importance of differentiating between human factors and
factors related to technology in non-response while reporting response rates (which they refer to as adherence
level). Two recent review papers on EMA non-response, [15] and [28], provide additional evidence for the
importance of the problem. [28] reviews studies involving patients with chronic pain, while [15] reviews studies
related to substance abuse. In contrast to the current study, these prior works do not address the development of
a predictive model for non-response to EMA.

Two recent works [2, 27] have focused on predicting participant non-response. Both works use contextual
factors (such as location, activity, etc.) in a predictive model. One common factor among all these prior works is
their use of classical machine learning models for analyzing and predicting EMA non-response. We share with
these prior works an investigation into the predictive power of various contextual factors and mental states (e.g.
emotions). At the same time, our work is uniquely-distinguished by its focus on developing transformer models
for non-response prediction in order to exploit the benefits of feature learning in modeling complex sequential
data.

The tasks of assessing interruptibility and availability in mHealth are related to our problem of non-response
prediction. A representative example of availability modeling is Sarkar et. al. [36], which developed a classifier
that combined mobile sensor data with past EMA responses to classify whether or not a participant is available
at the current moment in time. The topic of interruptibility has been widely-explored in the context of intelligent
notification systems [1, 12, 13, 25]. The goal of these works is to design a system that delivers notifications at
opportune moments based on contextual factors. The focus of [25] is the optimization of the user experience.
[12] presents a reinforcement learning based method for scheduling notifications. This is similar to the study

, Vol. 1, No. 1, Article . Publication date: January 2022.

4

• Nagesh, et al.

of receptivity to mHealth interventions in [5, 21], where the goal is to determine opportune moments using
contextual factors (such as activity, location, phone battery, etc.). The topics of availability, receptivity, and
interruptibility prediction are critically important for avoiding unnecessary participant burden and considering
external contextual factors in determining availability. In contrast, our focus is on developing a predictive model
for non-response based on feature learning derived from factors such as participant mental states and emotions,
and their history of EMA responses.

An additional related topic is participant disengagement, which manifests as a steady decline over time in the
participation of a user in a study or treatment program [6, 22], often resulting in loss to follow-up [10]. The focus
of these works is on longitudinal analyses and long-term study outcomes. In contrast, our focus is on quantifying
the short-term risk for non-response at the EMA prompt level. Such a capability could inform the design of
interventions to maximize the utility of EMA as a measurement tool, which is distinct from the important task of
improving long-term participant engagement.

A final related topic is in the domain of ePROs (electronic patient-reported outcomes). ePROs are patient-
provided information about symptoms, side effects, drug timing and other questions during a clinical trial [7].
ePROs generally lack the momentary, frequent sampling found in our EMA dataset. The extension of our work to
developing transformer models for sequences of ePRO data is an interesting avenue for future work.

2.2 Deep models for EMA data

There are two prior works that develop deep models for prediction tasks using EMA data [26, 41]. In [41], the
authors propose a recurrent neural network (RNN) for forecasting depressed mood using the history of EMA data.
In [26], the focus is on predicting short term mood developments from EMA data using an RNN. In addition, there
are numerous works that analyze EMA data using classical statistical and machine learning tools, such as logistic
regression and SVMs [18, 19, 35, 40, 45]. The current article differs from these prior works in two ways. First, we
address the problem of predicting whether the next prompt will result in an EMA response, which is distinct
from the task of predicting the responses themselves, as in the case of predicting self-reported mood. Second,
we develop a transformer model for EMA sequences and analyze its utility for predicting EMA non-response.
Transformer models have been shown to deliver state of the art results in fields such as NLP [46][9] and computer
vision. We extend this class of models to the EMA setting.

We note that there has been significant work on using DL models to analyze clinical data such as Electronic
Health Records (EHR), a domain with some similarity to EMA analysis. While EHR data is diverse, it includes
categorical variables that capture clinical states, which is analogous to EMA response data. Two representative
works that use classical sequential DL models for EHR analysis are [4, 16]. Both works use an attention layer
with a recurrent temporal model (an RNN) for EHR sequence analysis. In contrast, our focus is on the exploration
of transformer-style models for irregularly-sampled EMA data.

2.3 Transformers for Electronic Health Records Data

Based on the success of transformer models on NLP tasks [46][9][42], recent works have explored their application
to a broad range of other domains, including the analysis of EHR data. EHR analysis includes several prediction
tasks, such as length of stay, mortality, and sepsis onset, which share our focus on predictive modeling from
irregularly-sampled data. One representative work is [39], which applies transformer models to irregularly
sampled clinical data. In [23], a BERT-style model is developed using a pre-training task that is appropriate for
irregularly sampled diagnosis codes.

There are several significant differences between EHR and EMA analysis. First, EHR datasets consist primarily
of categorical observations (e.g. diagnostic codes) and real-valued biomarker measurements, while EMA data
consists primarily of ordinal vectors. Second, in EHR datasets only a subset of possible observations are available

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

5

at any point in time, whereas for EMA it tends to be all or nothing (participants either respond to the prompt and
answer all of the items or fail to respond at all). Third, EHR data contains many more variables and data item
types in comparison to EMA. Given these differences, it is unlikely that findings from modeling EHR data will
transfer in any significant way to EMA data analysis.

3 STUDY PROTOCOL AND DATASET

The dataset comes from a study that examines the influence of intrapersonal and contextual factors on smoking
lapse among African American smokers. Data was collected from multiple modalities including EMA prompts,
on body sensors, and location from GPS.

The study participants carried a smartphone provided to them with the study software installed. The mobile
app delivered EMA prompts and collected real time continuous data in the participant’s natural environment
from multiple sensors. Data was processed in real time on the smartphone and machine learning algorithms
were used to extract biomarkers corresponding to specific behavioral and physiological indicators of smoking
and stress. In this analysis, we focus on the EMA data, as this provides a rich set of items that capture aspects of
contextual and mental state, and is also the most widely-collected datatype in health applications.

We now describe the EMA collection process. In order to begin and end triggering EMAs for the day, participants
had to press a button indicating start and end of day. Participants were prompted by the phone app to complete
three types of Ecological Momentary Assessments (EMAs) on their smartphones during the study - random EMA,
stress triggered EMA, smoking triggered EMA. On each day, a participant was prompted with an average of four
random EMAs. After the day start button was pressed, the day was divided into 4 equal blocks of time. In each
block, the phone app checked for the ‘participant availability,’ determined by the battery level (being above 10%),
whether the participant was driving, and if the participant had enabled a ‘do not disturb’ option. The ‘do not
disturb’ mode could be used by participants to stop receiving any EMA prompts when they were unavailable.
The data collected from the sensors was used to determine smoking events and events of stress. In case of these
events, the phone app checked for the same conditions for firing an EMA and triggered a smoking EMA or stress
EMA. In our work, we are interested in predicting non-response to the random EMAs.

Figure 1 shows the interface for an EMA notification and the UI while responding to some example survey
questions. Once a notification was triggered, the participant could either: 1. Accept the notification and begin
answering the survey by clicking ‘OK’, 2. Dismiss the notification by clicking ‘Cancel’, 3. Snooze the notification
and receive it again after 10 minutes.

The dataset consists a total of 255 participants, after excluding participants who dropped out of the study. The
participants range between age 20 to 82 (mean 51 ± 12 years) and we have a roughly balanced split between
the male and female subjects. The data collection process spanned two contiguous weeks (4 days pre-smoking-
cessation through 10 days post-smoking-cessation). Over the course of the study a total of 9043 random EMAs
were triggered and 5636 of them were completed (average compliance rate of 62.8%).

4 METHODOLOGY
4.1 Non-response prediction problem framing

For our analysis, we are interested in predicting response to random EMAs. From here on, we use the term EMA
to refer to random EMA.

Consider a set of 𝑛 participants indexed as 𝑖 = 1, 2, · · ·, 𝑛. Each participant then has EMAs (observations)
indexed by 𝑗 = 1, 2, · · ·, 𝑛𝑖 , where 𝑛𝑖 is the number of observations (EMAs) for participant 𝑖. We design a model to
use a sequence of N EMAs as input and predict if the (N+1)𝑡ℎ EMA is completed. See Figure 2 for an illustration

, Vol. 1, No. 1, Article . Publication date: January 2022.

6

• Nagesh, et al.

Fig. 1. (a) EMA notification on the study phone (b) Survey question: angry (c) Survey question: relaxed.

Fig. 2. Sliding window approach used to create our feature vector sequence and classification label. N is the sequence length
and feature vectors from N consecutive EMAs are concatenated and the prediction label 𝑌 is the compliance to the (𝑁 + 1)th
EMA.

of the setup. We frame this as a binary classification problem where our label is

𝑌𝑗 =

(cid:40)

1
0

if 𝑗𝑡ℎ EMA is completed
if 𝑗𝑡ℎ EMA is missed

The feature vector derived from the 𝑗𝑡ℎ EMA is denoted as 𝑋 𝑗 . Section 4.2 describes the process to create 𝑋 𝑗

from the EMA data. Here, we assume that 𝑋 𝑗 is a feature vector consisting of 𝐾 features.

We developed models under two scenarios. In the first, we used a sequence length of one, meaning that for
each EMA we predict the compliance for the next EMA prompt. In the second scenario, we used a sliding window
(of length 𝑁 ) approach as shown in Figure 2 to compute the feature sequence and the corresponding binary label
for classification. In this case, we use a transformer architecture to perform feature learning, and predict next
EMA compliance from a sequence of feature vectors. For our experiments, we use a sequence length N = 5, 10, 15,
and 25.

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

7

4.2 Feature construction

We now describe our approach to feature engineering for the task of compliance prediction. We use a set of raw
features derived directly from the EMA response along with meta-data logged as part of the study. These are
listed in Table 1 under the column ‘Features’. The raw features are obtained from the following sources:

(1) Positive and negative affect, smoking urge: Self-reported
(2) Time, compliance: EMA logs
Additionally, we construct summary features using the history of the raw features. These are listed in Table 1
under the column ‘Summary features’. There are two types of summary features constructed, they are designed
to: 1) Capture the completion history summary (long term and short term); 2) Capture the variance in the positive
and negative affect, and completion pattern.

The long-term completion rate feature is computed to capture the trait of the participant. Trait refers to the
baseline trend of compliance for that participant. Here, we compute the ratio of total EMAs completed over the
total number of EMAs triggered until the current EMA. The short-term completion rate feature, which captures
the state of the participant is computed by the ratio of the number of EMAs completed on the previous day to
the total number of EMAs triggered on the previous day. The state refers to temporally local changes in the
compliance, and this computed feature attempts to capture the state for each participant.

Long-term completion rate (CR) =

(cid:40) (cid:205)𝑗

𝑖=1
𝑗

𝑌𝑖

if 𝑗 ≠ 0
if 𝑗 = 0

0
(cid:40) #EMA completed on day (𝑑 − 1)
𝑛𝑑−1

0

if 𝑛𝑑−1 ≠ 0
if 𝑛𝑑−1 = 0

Short-term completion rate (CR) =

where 𝑑 is the day the current EMA is triggered, 𝑛𝑑−1 is the total number of EMAs triggered on day 𝑑 − 1.

The variance feature is computed for the positive and negative affect and smoking urge. The variance feature
for each covariate for the 𝑗𝑡ℎ EMA is computed as the variance of the covariate until the 𝑗𝑡ℎ EMA. For example,
the variance of the positive affect ‘Happy’ computed for the 𝑗𝑡ℎ EMA is the variance in response to the question
‘Happy’ for EMA 1 to EMA 𝑗.

Type

Positive affect

Negative affect

Compliance

Features
Enthusiastic
Happy
Relaxed
Bored
Sad
Angry
Restless
Urge
Current EMA status

Value

Summary features

Likert scale (1-5) Variance of each item

Likert scale (1-5) Variance of each item

Binary

Long term CR
Short term CR

Table 1. List of all features used. The raw features derived from each EMA prompt are listed in the column ‘Features’.
We compute additional summary features from the history of the raw features. These are listed in the column ‘Summary
features’.

, Vol. 1, No. 1, Article . Publication date: January 2022.

8

• Nagesh, et al.

4.3 EMA Transformer model

Transformer models are very popular sequence models for NLP tasks [42][46][9]. In our work, we are modeling a
sequence of EMA responses using this class of models. We would like to answer the following questions in the
context of EMA data:

• How are the EMA responses represented in the transformer?
• How is the positional information handled? EMAs are irregularly sampled through the day. How can this

information be captured in the model?

• What kind of pre-training tasks can be designed to learn the structure of EMA sequences?

We describe some background about transformers in NLP below. We then describe at our attempt at answering

the questions in the context of EMA sequences.

4.3.1 Background. The transformer is a sequence modeling architecture based entirely on attention proposed in
[46]. A self-attention mechanism is a mapping between pairs of words in a sentence/input points in a sequence
to the output. The scaled dot product attention mechanism introduced in [46] is computed as

Attention(𝑄, 𝐾, 𝑉 ) = softmax

(cid:19)

𝑉

(cid:18) 𝑄𝐾 ⊤
√
𝑑𝑘

where 𝑄, 𝐾, 𝑉 are the query, key, and value matrices computed as a projection of the input sequence 𝑋 into
query, key and value spaces. 𝑄 = 𝑋𝑊 𝑄, 𝐾 = 𝑋𝑊 𝐾, 𝑉 = 𝑋𝑊 𝑉 . The matrix multiplication 𝑄𝐾 ⊤ computes pairwise
inner products between every query and key vector pair. The value vector is weighted by this attention matrix.
Multihead attention performs the attention mechanism described above in ℎ different feature spaces, where ℎ
is the number of heads. The attention is computed on the key, query, value matrices projected with ℎ different
learned projections and concatenated together.

Multihead(𝑄, 𝐾, 𝑉 ) = Concat(ℎ𝑒𝑎𝑑1, ℎ𝑒𝑎𝑑2, · · ·ℎ𝑒𝑎𝑑ℎ)𝑊 𝑜

where ℎ𝑒𝑎𝑑𝑖 = Attention(𝑄𝑊 𝑄

𝑖

, 𝐾𝑊 𝐾
𝑖

, 𝑉𝑊 𝑉

𝑖 ) and 𝑊 𝑜 projects the concatenated output back to the original size.

Two initial operations are performed on a sentence prior to computing attention:
(1) Input embedding: learned embeddings are used to convert words to vectors of dimension 𝑑𝑚𝑜𝑑𝑒𝑙 .
(2) Positional encoding: since the transformer model does not contain any form of recurrence, information
about the position of different words is added to the input representation. Sine and cosine embeddings are
computed as shown below and added to the input representation. Here 𝑝𝑜𝑠 corresponds to the position of a
word in a sentence.

𝑃𝐸 (𝑝𝑜𝑠,2𝑖) = 𝑠𝑖𝑛(𝑝𝑜𝑠/10002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙 )
𝑃𝐸 (𝑝𝑜𝑠,2𝑖+1) = 𝑐𝑜𝑠 (𝑝𝑜𝑠/10002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙 )

4.3.2 EMA setting. One way to think about the scaled dot product attention, discussed in [43] is noting we can
think of exp(𝑄𝐾𝑇 ) the numerator of the softmax as evaluating kernels between a set of query and key points.
This then gives us the interpretation that self-attention can be decomposed into several steps: 1) using the keys
to construct a kernel average smoother of the value function, where the domain of this function is a vector
summarizing both observations and times (via the positional embeddings) 2) evaluating the kernel smoother at
the query points in order to obtain a fixed length vector 3) using that fixed length vector as input to a neural
network. For EMAs, one can think of this as constructing a kernel regression function to describe the EMA
observation trajectory via the keys and values and evaluating this function using the observed EMAs and their

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

9

time points via the queries. This then gives a summary of the EMA history in a fixed length vector for input into
a neural network.

There are two main differences between a sequence of words and a sequence of EMA responses: 1. EMA
responses are ordinal and responses are already in a vector form, 2. Continuous time associated with EMA
responses: e.g., a sequence of 4 EMAs could have been completed at 10 AM, 11.30 AM, 3 PM, 4 PM respectively.
We account for these two differences architecturally in this manner:

(1) Input embedding: The feature vector computed for each EMA is used directly as the input embedding. For

a sequence of N EMAs, we represent the input embeddings as 𝑋1, 𝑋2, · · ·𝑋𝑁 .

(2) Positional/time encoding: A sequence of EMAs has a sequence of discrete positions and continuous times
associated with it. For example, for a sequence of N EMAs with input embeddings 𝑋1, 𝑋2, · · ·𝑋𝑁 , the
corresponding discrete positions are 1, 2, · · ·𝑁 and the continuous time associated is 𝑡1, 𝑡2, · · ·𝑡𝑁 . We encode
this information into the input to the self-attention mechanism. We compare two different ways of encoding
the position/time representation:

(a) Addition: The embeddings from the position/time values are computed using the sine and cosine functions
described in Section 4.3.1. Given a sequence of EMA input embeddings 𝑋 , position 𝑝𝑜𝑠, time 𝑡, the input
to multihead attention when encoding the discrete position and continuous time values respectively are:

Input to multihead attentionpos = 𝑋 + 𝑃𝐸 (𝑝𝑜𝑠)
Input to multihead attentiontime = 𝑋 + 𝑃𝐸 (𝑡)

(b) Concatenation: The embeddings from the position/time values are computed and concatenated with the
input embeddings. Given a sequence of EMA input embeddings 𝑋 , position 𝑝𝑜𝑠, time 𝑡, the input to
multihead attention when encoding the discrete position and continuous time values respectively are:

Input to multihead attentionpos =

Input to multihead attentiontime =

where

(cid:21)

(cid:20) 𝑋
𝑃𝐸 (𝑡)

=

(cid:21)

(cid:20) 𝑋1
𝑃𝐸 (𝑡1)

,

(cid:21)

(cid:20) 𝑋2
𝑃𝐸 (𝑡2)

, · · ·,

(cid:21)

(cid:20) 𝑋𝑁
𝑃𝐸 (𝑡𝑁 )

4.4 Self-supervised pre-training for transformer

(cid:21)

(cid:20)

𝑋
𝑃𝐸 (𝑝𝑜𝑠)
(cid:20) 𝑋
𝑃𝐸 (𝑡)

(cid:21)

The goal of pre-training a transformer model in a self-supervised manner is so that it can learn the structure in
the data, which can be useful for other downstream tasks. In EMA, we might be interested in some particular
prediction problem like predicting the probability of a person drinking alcohol. Since we are limited by the
amount of labeled data, pre-training aims to leverage self-supervised learning from a lare corpus of EMA data. If
we have a large EMA corpus, we can train a model that can learn the structure between different EMA items
and the temporal structure in the data. The model can then be fine-tuned for the prediction task that we are
interested in. In this paper, we are evaluating the utility of self-supervised pre-training of transformers with EMA
data. Based on the findings from BERT [9], we envision that self-supervised pre-training might be an attractive
strategy.

4.4.1 Background. [9] introduced BERT, which is designed to pre-train bidirectional representations from a large
corpus of text in a self-supervised manner. This is done by first pre-training BERT (a bidirectional transformer
model) on two tasks: 1. Masked language modeling (MLM), 2. Next sentence prediction (NSP). In the MLM task,

, Vol. 1, No. 1, Article . Publication date: January 2022.

10

• Nagesh, et al.

some words in the input sentence are replaced by a MASK token. The model is trained to impute these words
correctly. In the NSP task, a pair of sentences is provided as the input and the model is trained to recognize if the
second sentence is a valid ‘next sentence’. The exact description of the pre-training can be found in [9].

The idea behind pre-training BERT in this manner is to learn the structure in language: structure of words
within a sentence (MLM) and structure at the sentence level (NSP) without having any specialized labels. Once it
has learned the structure, a pre-trained BERT can be used with an additional linear layer for other downstream
tasks.

4.4.2 Pre-training: EMA transformer. We design a masked EMA imputation task that similar to the MLM task.
The features described in Section 4.2 are constructed for each EMA prompt. We then obtain fixed length sequences
of EMA features using a sliding window. Let the sequence of EMA features be 𝑋1, 𝑋2, · · ·𝑋𝑁 where 𝑁 is the
sequence length and 𝑋𝑖 is the feature vector computed for the 𝑖𝑡ℎ EMA in the sequence. 𝑋𝑖 consists of features
corresponding to the positive and negative affect (emotion items) and compliance history.

The goal of the masked EMA imputation task is to mask out responses to some emotion items in the sequence,
and learn to reconstruct the response to these items. This will help the model learn the structure between the
different emotion items and their temporal pattern. The number of emotion items masked is pre-determined by
us and the positions where the response is masked is chosen at random. We mask out the emotion item(s) in 15%
of the sequence, determined randomly. However, the masked imputation task is for pre-training purposes only.
The input sequence does not contain mask tokens during a downstream fine-tuning task. To account for this,
after a particular position is chosen for masking, we replace the input with the mask token 80% of the time. The
input value is retained as is 10% of the time and changed to a random value 10% of the time. This is similar to the
masking approach in MLM in [9].

For example, consider EMA sequences of length 10. Suppose we choose to mask out the emotion item ‘Happy’,
we randomly choose 2 positions in the sequence at random. For each position with probability 80%, we replace
the EMA response to the question ‘Happy’ in the input sequence with the mask token. The EMA transformer
model is then trained to impute the actual value of the response to the question ‘Happy’ in these places. A model
pre-trained in this manner will learn the structure in the EMA data and across the emotion items. There are
several pre-training tasks possible based on the choice of the emotion items we mask out. Figure 3 shows the two
extreme ends of tasks possible. In the first case (a), one emotion item is masked out. In the second case (b), all
the emotion items are masked out. The model then has to reconstruct responses to all questions in the masked
positions. There are also intermediate tasks possible where we mask out some of the emotion items. Note that
the values masked are always the response to emotion items. We do not mask out the compliance history results
for the masked imputation task.

Once we pre-train an EMA transformer model, we add a linear layer to it and fine-tune it for the non-response

prediction task as shown in Figure 4.

4.5 Baseline models for comparison

We consider two non temporal models - Logistic regression, Support Vector Machine (SVM) and compare their
performance to ours. We also compare the performance of our model to a vanilla LSTM and an attention LSTM
[16] architecture.

4.6 Model development

Our model was implemented in PyTorch. The input data was converted to a 3D format with the dimensions as
number of subjects, number of time steps, feature input size. We validate the model and perform grid search for
hyperparameter tuning using a separate validation data set (containing data from 10% of the subjects). The model
architecture is the standard transformer [46] encoder architecture. Our architecture consists of 6 encoder layers,

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

11

Fig. 3. Self-supervised EMA masked imputation tasks. Here we assume that there are 𝐾 emotion items in each EMA. The
input sequence here is depicting only the responses to emotion items. We do not mask out other features in the input
sequence. (a) The first task shown is to impute one emotion item at a time in 15% of the sequence positions that are randomly
masked. For example, the value of the emotion ‘Happy’ can be masked off in some positions of the sequence and the task is
to impute this value correctly. Note that we explore imputing each emotion item one at a time as a pre-training task and
evaluate the downstream non-response prediction performance. There are intermediate tasks possible such as masking out 3
emotion items, 4 emotion items, etc. (b) The second task is to impute all emotion items in 15% of the sequence.

8 attention heads per encoder layer. The dimension of the key, query and value vectors is 64. The scores reported
in the results section include 5 fold cross validation results for prediction.

5 EXPERIMENTAL RESULTS

In our analysis, we report performance by splitting the data across subjects. We perform 5 fold cross-subject
validation, by training the model on data from a set of subjects and testing on data from the held out subjects.

5.1 Predicting non-response to the next EMA using the current EMA response

We first evaluate the performance of predicting next EMA compliance when the sequence length N = 1 using non
temporal models(using features from one EMA to predict compliance to the next). We perform prediction using
two sets of features: 1) Raw features only, 2) raw features and summary features which are described in Table 1.

, Vol. 1, No. 1, Article . Publication date: January 2022.

12

• Nagesh, et al.

Fig. 4. Pre-training the EMA transformer model with an EMA imputation task. The pre-trained model is then fine-tuned
for the non-response prediction task. This pre-training strategy is similar to BERT where the model is pre-trained in a
self-supervised manner on a large text corpus and the model is fine-tuned for downstream tasks.

The area under the ROC curve (AUC) score for this analysis is presented in Table 2. We see that including the
computed summary features result in improved performance. The summary features capture the summary of the
previous EMA responses. In the next subsection, we report the performance modeling a sequence of EMA responses to
better capture the history.

Model
Logistic regression
SVM (RBF kernel)

Raw features only Raw features & summary features

0.63 ± 0.02
0.64 ± 0.02

0.71 ± 0.02
0.71 ± 0.02

Table 2. Average 5 fold cross validation AUC for predicting next EMA compliance using features from the current EMA (N =
1). We use two sets of features: 1) raw features only, 2) raw features and summary features. These features are listed in Table
1.

5.2 Predicting non-response to the next EMA using a sequence of EMA responses

We present results for predicting non-response to the next EMA prompt using a sequence of EMA responses.
The features described in Table 1 corresponding to each EMA is computed. We then use a sliding window to
obtain EMA sequences of a fixed length (N) and the non-response label for the next EMA prompt. The results are
reported in Table 3. We see that the deep models show an improvement over logistic regression. The transformer
model performs the best and particularly shows an improvement in modeling long sequences (N = 15, 25). We next
present the results using the transformer model with pre-training. The EMA transformer model is pre-trained as

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

13

described in Section 4.4. The model is then fine-tuned for the non-response prediction task and the results are in
Table 4.

Model

N = 5

Logistic regression 0.70 ± 0.03
0.74 ± 0.02
Vanilla LSTM
Attention LSTM 0.73 ± 0.02
0.75 ± 0.02
EMA transformer

N = 10
0.66 ± 0.02
0.74 ± 0.01
0.73 ± 0.02
0.76 ± 0.01

N = 15
0.65 ± 0.02
0.73 ± 0.02
0.72 ± 0.02
0.76 ± 0.01

N = 25
0.58 ± 0.02
0.72 ± 0.01
0.71 ± 0.01
0.75 ± 0.01

Table 3. Average 5 fold cross validation AUC for predicting non-response to next EMA using a sequence of N EMAs. The
transformer model here is directly trained for the task of non-response prediction without any pre-training. Note that the
transformer model here is learned directly on the non-response prediction task without any pre-training.

Model
EMA transformer

N = 5
0.75 ± 0.01

N = 10
0.77 ± 0.01

N = 15
0.77 ± 0.01

N = 25
0.75 ± 0.02

Table 4. EMA transformer with self-supervised pre-training. The pre-training task is to impute one item (‘Happy’) in the
masked positions in the sequence. This pre-training task performed the best compared to the others.

5.3 Results with different pre-training tasks

The results presented in Table 4 are with the pre-training task of imputing one emotion item. In Section 4.4 we
describe pre-training tasks ranging from imputing one emotion item at a time to imputing all emotion items. For
example, in a sequence of 10 EMAs we mask the response to the emotion ‘Happy’ at positions 1 and 5. The model
is then trained to impute the value of ‘Happy’ at these positions. We compare different pre-training tasks and the
final fine-tuning performance to the non-response prediction task in Table 5. We see that the performance does
not vary much with the pre-training task.

Pre-training task
One item
All items
Imputing 5 items

N = 15
0.76 ± 0.01
0.75 ± 0.02
0.76 ± 0.02

Table 5. Cross validation AUC for predicting non-response to next EMA prompt using a sequence of N EMAs. The transformer
model is pretrained on different tasks here: 1. Imputing one emotion item at a time, 2. Imputing all the emotion items, 3.
Imputing five emotion items.

5.4 Learned attention weights

The transformer encoder layers each perform multihead attention. The attention operation as described previously
involves computing 𝐴 = softmax(
). The matrix 𝐴 is multiplied with the value matrix 𝑉 . The columns of 𝐴
determine the weight or scaling factor for each row in 𝑉 which corresponds to the position in the EMA sequence.
We compute the matrix 𝐴 from each encoder layer and plot the attention weight corresponding to each position
in an EMA sequence of length 5. Figure 5 illustrates the weight on the different EMA positions in the sequence

𝑄𝐾 ⊤
√
𝑑𝑘

, Vol. 1, No. 1, Article . Publication date: January 2022.

14

• Nagesh, et al.

Fig. 5. Attention weight across the different encoder layers in the transformer on the different EMA responses in a sequence
of 5 EMAs in predicting non-response to the 6𝑡ℎ EMA. The x-axis contains the lag of the EMA with respect to the 6𝑡ℎ EMA.
For example, Lag 1 is the most recent EMA.

Fig. 6. Attention weights on the different features in the input layer (first encoder layer). This provides an interpretation of
the feature importance for the task of non-response prediction.

for predicting non-response to the next EMA. We also present the attention weights on the input features. We
visualize the weight corresponding to each feature while computing the value matrix (with 𝑊 𝑉 ). We present
this analysis only for the first encoder layer since the input to subsequent layers is a linear combination of the
input features and hence not interpretable at the feature level. This result is presented in Figure 6. We discuss the
interpretation of these results in Section 6.

6 DISCUSSION
6.1 Predicting non-response to EMA prompts

We have defined a forecasting problem for prompt level EMA compliance. We evaluate the performance of
classical statistical and machine learning models such as logistic regression and SVM (RBF kernel) for predicting
next EMA compliance (compliance to (𝑁 + 1)𝑡ℎ EMA) using the current EMA (𝑁 𝑡ℎ EMA) features. Our feature
vector summarizes the EMA history by using average completion and affect variance. We perform an experiment
to remove all summary features computed from the EMA items. In this setting, we use only the information
contained in one EMA response to predict next EMA compliance. This reduces performance substantially, as

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

15

shown in Table 2 column ‘Raw features only’. These findings suggest that the EMA history summary is important
for predicting next EMA compliance.

Next, we explicitly incorporate the history of EMAs into the prediction model by using a fixed sequence of
EMAs (sequence of 𝑁 EMAs) to predict next EMA ((𝑁 + 1)𝑡ℎ EMA) compliance. We develop a transformer based
model for the task of forecasting using the sequence of EMA features. We see in table 3 that using a sequence of
EMAs improves performance (in LR and SVM when compared to using N = 1). Among the models predicting
compliance from the sequence of EMAs, the EMA transformer performs the best.

We hypothesize that the modest improvement in performance with deep learning models over classical models
is due to the small size of our dataset. This prompts a couple of future directions for our work: 1) exploring
domain adaptation methods to bridge the gap between several mHealth datasets and increasing the training data,
2) exploring transfer learning from other data domains (EHR,NLP) to improve the performance of deep models.

6.2 Temporal/positional encoding for EMA input to transformers

Since transformer models do not contain recurrence, the notion of position of words in a sentence is lost.
The positional information is injected into the input sequence through positional encoding. In the canonical
transformer works such as [46] and [9], positional encoding is performed by adding a function of the positional
information to the input embedding.

where 𝑃𝐸 (𝑝𝑜𝑠) are sine and cosine functions of the position.

Input to multihead attention = Input embedding + 𝑃𝐸 (𝑝𝑜𝑠)

In the case of EMAs, we have a more continuous notion of time when each EMA is triggered. Encoding the
continuous time values into the input would provide a higher resolution of information than just the positions of
each EMA. In the standard NLP literature, positional information is embedded by adding sine and cosine functions
of the position to the input. We believe that the additive positional encoding strategy (used in NLP) is sub-
optimal for EMA data where we have much smaller datasets. The idea here being that adding temporal/positional
information changes the input values and the model has to learn to differentiate the effect of position/time and
the actual input variation. In the case of NLP where datasets are larger, we hypothesize that the model can learn
this distinction better.

We encode temporal/positional information by concatenating it with the input embeddings.

Input to multihead attention =

(cid:20)Input embedding
𝑃𝐸 (𝑡)

(cid:21)

The results for predicting non-response with different temporal/positional encoding strategies are shown in
Table 6. This table also contains results when the size of the dataset used for pre-training the model varies. In the
first two cases (smaller dataset), we see that concatenation performs better than additive encoding. In the third
case (using slightly more data for pre-training), we see that the performance with additive positional/temporal
encoding improves. We visualize the test results (imputation error) for the pre-training task of masked imputation
in Figure 7. The imputation error is the test error for imputing each emotion item. The results with four different
positional encoding strategies are shown: Positional encoding (add), Temporal encoding (add), Positional encoding
(concat), Temporal encoding (concat). We see that concatenation is always better than addition for the purpose of
encoding temporal information.

6.3 Self-supervised pre-training of EMA transformer

Pre-training transformer models in a self-suppervised manner has been shown to be beneficial in other domains
such as NLP. The BERT model is pre-trained on a large corpus of text to learn structure in sentences. This model
is then fine-tuned for new tasks where labeled datasets are of smaller sizes. Such a capability would be beneficial

, Vol. 1, No. 1, Article . Publication date: January 2022.

16

• Nagesh, et al.

Pre-training dataset Positional (concat) Temporal (concat) Temporal (additive) Positional (additive)
0.77 ± 0.01
0.77 ± 0.01
0.76 ± 0.01
Table 6. Positional encoding strategies. All results reported are for N = 15

Dataset A only
Dataset B only
Dataset A + B

0.76 ± 0.01
0.76 ± 0.01
0.76 ± 0.01

0.73 ± 0.02
0.75 ± 0.01
0.75 ± 0.01

0.70 ± 0.01
0.73 ± 0.01
0.75 ± 0.01

Fig. 7. Pre-training task test performance (imputation error) when pre-trained with varying amount of data. We compare
different temporal/positional encoding methods. We see that overall, concatenation performs better (lower error) than
addition.

in fields such as mHealth where labeled datasets are difficult to collect. We evaluate the utility of pre-training
and find a small improvement in the non-response prediction performance. Table 4 reports the AUC scores
with a pre-trained transformer model. We see an improvement over the results in Table 3. However, this isn’t
a statistically significant improvement. We hypothesize that this is due to the small size of our datasets. The
question of the utility pre-training with EMA sequences as in the NLP setting remains a question for future work.
However, the preliminary results that we obtain show that pre-training might have the potential to result in
significant improvements with larger datasets.

6.4

Interpreting the learned attention weights

Figure 5 presents a visualization of the attention weights learned by the different transformer encoder layers
on the EMA positions within a sequence. This corresponds to the importance of each EMA position within
a sequence of 𝑁 EMAs in predicting non-response to the (𝑁 + 1)𝑡ℎ EMA. We see that Lag 1 has the highest
weight across all of the encoder layers. Lag 1 corresponds to the most recent EMA, to the EMA for which we
are predicting non-response. This is intuitive, as the most recent EMA is likely to be most closely-related to the
paricipant’s current mental state. We see an interesting pattern in the attention weights corresponding to the
other lags. The mean of the lag 2, 3, and 4 weights are similar, but the weight corresponding to lag 5 is higher.
One possible explanation for this trend is that since there are 4 EMAs on average per day, an EMA at lag 5

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

17

would correspond to the same time window as the current prediction task, but on the previous day. This may be
capturing aspects of the participant’s daily routine that are relevant to their response or non-response. More
explicit methods for learning features derived from daily routines, diurnal rhythms, and related patterns is an
interesting topic for future work.

In Figure 6 we visualize the attention weights on the different features in the first encoder layer. This corresponds
to the relative importance of different types of features in predicting non-response to the next EMA prompt. We
see that time is an important feature in predicting non-response. This is also aligned with our finding that the
way we encode time in the input affects the performance of the transformer model. We also find that the emotion
features Enthusiastic, Angry, and Bored have higher attention weights in comparison to other emotion items.
We believe this is reasonable, as these are examples of strongly positive and negative emotions, which could
influence response behavior, as well as the feeling of boredom which may correlate with a lack of engagement.
The completion feature captures the detailed pattern of completion to each EMA in the sequence, while CR long
and short are the average completion rate features that capture a summary of the pattern of completion. We see
in Figure 6 that the detailed pattern of completion is more useful in predicting non-response when compared to
the long and short summaries of completion. This suggests that the model gets significant benefit from modeling
the more detailed patterns in the response history. Collectively, these visualizations provide qualitative evidence
that the transformer model is capable of learning meaningful structure from the sequence of EMA responses. The
ability to identify and visualize the feature interactions learned by the transformer can be a potentially useful
capability for domain scientists who are interested in designing related interventions.

7 CONCLUSION

In this paper, we present a transformer architecture for modeling EMA sequences to predict non-response to
future EMA prompts. Existing work on analyzing and predicting non-response have used classical machine
learning models for this task. We are the first to explore the use of transformers for modeling sequences of
irregularly sampled EMA responses and predicting non-response to future EMA prompts. We address three issues
in this work: 1. Choice of the input representation for EMA sequences, 2. encoding the temporal information into
the input, 3. analyzing the utility of self-supervised pre-training on EMA data for improving the non-response
prediction task. We find that the transformer model achieves a classification AUC of 0.77 and outperforms both
classical ML and LSTM based DL models. We find that the design choice for positional/temporal encoding affects
the performance of the model. We find that concatenating the temporal information leads to better performance
when compared to the standard practice of adding the positional embedding. We design a self-supervised pre-
training task on EMA sequences and find that it leads to a modest improvement that is not statistically significant.
We present visualizations of the learned attention weights that illustrate the ability of the transformer to learn
meaningful representations. We will make the predictive model trained from a corpus of 40K EMA samples freely
available to the research community. An important future step will be using these prompt level compliance
forecasts to inform the timing of compliance interventions.

ACKNOWLEDGMENTS
REFERENCES

[1] Samaneh Aminikhanghahi, Maureen Schmitter-Edgecombe, and Diane J Cook. 2019. Context-aware delivery of ecological momentary

assessment. IEEE Journal of Biomedical and Health Informatics 24, 4 (2019), 1206–1214.

[2] Mehdi Boukhechba, Lihua Cai, Philip I Chow, Karl Fua, Matthew S Gerber, Bethany A Teachman, and Laura E Barnes. 2018. Contextual
analysis to understand compliance with smartphone-based ecological momentary assessment. In Proceedings of the 12th EAI International
Conference on Pervasive Computing Technologies for Healthcare. 232–238.

[3] Yuqing Chen and Yang Xue. 2015. A deep learning approach to human activity recognition based on single accelerometer. In 2015 IEEE

International Conference on Systems, Man, and Cybernetics. IEEE, 1488–1492.

, Vol. 1, No. 1, Article . Publication date: January 2022.

18

• Nagesh, et al.

[4] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart. 2016. Retain: An interpretable
predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems. 3504–3512.
[5] Woohyeok Choi, Sangkeun Park, Duyeon Kim, Youn-kyung Lim, and Uichin Lee. 2019. Multi-stage receptivity model for mobile
just-in-time health intervention. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3, 2 (2019), 1–26.
[6] James Clawson, Jessica A Pater, Andrew D Miller, Elizabeth D Mynatt, and Lena Mamykina. 2015. No longer wearing: investigating the
abandonment of personal health-tracking technologies on craigslist. In Proceedings of the 2015 ACM International Joint Conference on
Pervasive and Ubiquitous Computing. 647–658.

[7] Stephen Joel Coons, Sonya Eremenco, J Jason Lundy, Paul O’Donohoe, Hannah O’Gorman, and William Malizia. 2015. Capturing
patient-reported outcome (PRO) data electronically: the past, present, and promise of ePRO measurement in clinical trials. The
Patient-Patient-Centered Outcomes Research 8, 4 (2015), 301–309.

[8] Delphine S Courvoisier, Michael Eid, and Tanja Lischetzke. 2012. Compliance to a cell phone-based ecological momentary assessment

study: The effect of time and personality characteristics. Psychological assessment 24, 3 (2012), 713.

[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for

language understanding. arXiv preprint arXiv:1810.04805 (2018).

[10] Katie L Druce, William G Dixon, and John McBeth. 2019. Maximizing engagement in mobile health studies: lessons learned and future

directions. Rheumatic Disease Clinics 45, 2 (2019), 159–172.

[11] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. 2019. Video action transformer network. In Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition. 244–253.

[12] Bo-Jhang Ho, Bharathan Balaji, Mehmet Koseoglu, Sandeep Sandha, Siyou Pei, and Mani Srivastava. 2020. Quick Question: Interrupting

Users for Microtasks with Reinforcement Learning. arXiv preprint arXiv:2007.09515 (2020).

[13] Bo-Jhang Ho, Bharathan Balaji, Nima Nikzad, and Mani Srivastava. 2017. Emu: engagement modeling for user studies. In Proceedings
of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International
Symposium on Wearable Computers. 959–964.

[14] Wenchao Jiang and Zhaozheng Yin. 2015. Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks.
In Proceedings of the 23rd ACM International Conference on Multimedia (Brisbane, Australia) (MM ’15). Association for Computing
Machinery, New York, NY, USA, 1307–1310. https://doi.org/10.1145/2733373.2806333

[15] Andrew Jones, Danielle Remmerswaal, Ilse Verveer, Eric Robinson, Ingmar HA Franken, Cheng K Fred Wen, and Matt Field. 2019.

Compliance with ecological momentary assessment protocols in substance users: a meta-analysis. Addiction 114, 4 (2019), 609–619.

[16] Deepak A Kaji, John R Zech, Jun S Kim, Samuel K Cho, Neha S Dangayach, Anthony B Costa, and Eric K Oermann. 2019. An attention

based deep learning model of clinical events in the intensive care unit. PloS one 14, 2 (2019), e0211057.

[17] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. 2021. Transformers in

Vision: A Survey. arXiv preprint arXiv:2101.01169 (2021).

[18] Heejung Kim, SungHee Lee, SangEun Lee, Soyun Hong, HeeJae Kang, and Namhee Kim. 2019. Depression Prediction by Using Ecological
Momentary Assessment, Actiwatch Data, and Machine Learning: Observational Study on Older Adults Living Alone. JMIR mHealth and
uHealth 7, 10 (2019), e14149.

[19] Zachary King, Judith Moskowitz, Laurie Wakschlag, and Nabil Alshurafa. 2018. Predicting Perceived Stress Through Mirco-EMAs and
a Flexible Wearable ECG Device. In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on
Pervasive and Ubiquitous Computing and Wearable Computers. 106–109.

[20] Predrag Klasnja and et.al. 2019. Efficacy of contextually tailored suggestions for physical activity: A micro-randomized optimization

trial of HeartSteps. Annals of Behavioral Medicine 53, 6 (2019), 573–582.

[21] Florian Künzler, Varun Mishra, Jan-Niklas Kramer, David Kotz, Elgar Fleisch, and Tobias Kowatsch. 2019. Exploring the State-of-
Receptivity for mHealth Interventions. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3, 4 (2019),
1–27.

[22] Amanda Lazar, Christian Koehler, Joshua Tanenbaum, and David H Nguyen. 2015. Why we use and abandon smart devices. In Proceedings

of the 2015 ACM international joint conference on pervasive and ubiquitous computing. 635–646.

[23] Yikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy, Yajie Zhu, Kazem Rahimi,
and Gholamreza Salimi-Khorshidi. 2020. BEHRT: transformer for electronic health records. Scientific reports 10, 1 (2020), 1–12.
[24] Eric Mayor and Liudmila Gamaiunova. 2015. Mobile device-based mindfulness intervention promotes emotional regulation during
anticipatory stress. In Proceedings of the 5th EAI International Conference on Wireless Mobile Communication and Healthcare. 258–262.
[25] Abhinav Mehrotra and Mirco Musolesi. 2017. Intelligent notification systems: A survey of the state of the art and research challenges.

arXiv preprint arXiv:1711.10171 (2017).

[26] Adam Mikus, Mark Hoogendoorn, Artur Rocha, Joao Gama, Jeroen Ruwaard, and Heleen Riper. 2018. Predicting short term mood
developments among depressed patients using adherence and ecological momentary assessment data. Internet interventions 12 (2018),
105–110.

, Vol. 1, No. 1, Article . Publication date: January 2022.

Transformers for prompt-level EMA non-response prediction •

19

[27] Varun Mishra, Byron Lowens, Sarah Lord, Kelly Caine, and David Kotz. 2017. Investigating contextual cues as indicators for EMA
delivery. In Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the
2017 ACM International Symposium on Wearable Computers. 935–940.

[28] Masakatsu Ono, Stefan Schneider, Doerte U Junghaenel, and Arthur A Stone. 2019. What affects the completion of ecological momentary
assessments in chronic pain research? An individual patient data meta-analysis. Journal of medical Internet research 21, 2 (2019), e11398.
[29] Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, and Shiva Sundaram. 2020. Multimodal and Multiresolution Speech
Recognition with Transformers. In Proceedings of 58th Annual Meeting of the Association for Computational Linguistics (ACL 20). 2381–2387.
https://doi.org/10.18653/v1/2020.acl-main.216

[30] Jamie Yea Eun Park, Jenny Li, Alyssa Howren, Nicole Wen Tsao, and Mary De Vera. 2019. Mobile phone apps targeting medication

adherence: quality assessment and content analysis of user reviews. JMIR mHealth and uHealth 7, 1 (2019), e11919.

[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.

(2018).

[32] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision Transformers for Dense Prediction. arXiv preprint arXiv:2103.13413

(2021).

[33] William T Riley, Daniel E Rivera, Audie A Atienza, Wendy Nilsen, Susannah M Allison, and Robin Mermelstein. 2011. Health behavior
models in the age of mobile interventions: are our theories up to the task? Translational behavioral medicine 1, 1 (2011), 53–71.
[34] Charissa Ann Ronao and Sung-Bae Cho. 2016. Human activity recognition with smartphone sensors using deep learning neural

networks. Expert systems with applications 59 (2016), 235–244.

[35] Koustuv Saha, Larry Chan, Kaya De Barbaro, Gregory D Abowd, and Munmun De Choudhury. 2017. Inferring mood instability on
social media by leveraging ecological momentary assessments. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
Technologies 1, 3 (2017), 1–27.

[36] Hillol Sarker, Moushumi Sharmin, Amin Ahsan Ali, Md Mahbubur Rahman, Rummana Bari, Syed Monowar Hossain, and Santosh
Kumar. 2014. Assessing the availability of users to engage in just-in-time intervention in the natural environment. In Proceedings of the
2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing. 909–920.

[37] Mariya P Shiyko, Seth Perkins, and Linda Caldwell. 2017. Feasibility and adherence paradigm to ecological momentary assessments in

urban minority youth. Psychological assessment 29, 7 (2017), 926.

[38] Alexander W Sokolovsky, Robin J Mermelstein, and Donald Hedeker. 2014. Factors predicting compliance to ecological momentary

assessment among adolescent smokers. nicotine & tobacco research 16, 3 (2014), 351–358.

[39] Huan Song, Deepta Rajan, Jayaraman Thiagarajan, and Andreas Spanias. 2018. Attend and diagnose: Clinical time series analysis using

attention models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.

[40] Gerasimos Spanakis, Gerhard Weiss, and Anne Roefs. 2016. Enhancing Classification of Ecological Momentary Assessment Data Using

Bagging and Boosting. In 2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 388–395.

[41] Yoshihiko Suhara, Yinzhan Xu, and Alex’Sandy’ Pentland. 2017. Deepmood: Forecasting depressed mood based on self-reported histories

via recurrent neural networks. In Proceedings of the 26th International Conference on World Wide Web. 715–724.

[42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732

(2020).

[43] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection:

A Unified Understanding of Transformer’s Attention via the Lens of Kernel. arXiv preprint arXiv:1908.11775 (2019).

[44] Caitlin M Turner, Phillip Coffin, Deirdre Santos, Shannon Huffaker, Tim Matheson, Jason Euren, Anna DeMartini, Chris Rowe, Steven
Batki, and Glenn-Milo Santos. 2017. Race/ethnicity, education, and age are associated with engagement in ecological momentary
assessment text messaging among substance-using MSM in San Francisco. Journal of substance abuse treatment 75 (2017), 43–48.
[45] Kasper van Mens, CWM de Schepper, Ben Wijnen, Saskia J Koldijk, Hugo Schnack, Peter de Looff, Joran Lokkerbol, Karen Wetherall,
Seonaid Cleare, Rory C O’Connor, et al. 2020. Predicting future suicidal behaviour in young adults, with different machine learning
techniques: a population-based longitudinal study. Journal of Affective Disorders (2020).

[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

Attention is all you need. arXiv preprint arXiv:1706.03762 (2017).

[47] Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, and Shonali Krishnaswamy. 2015. Deep convolutional neural networks on
multichannel time series for human activity recognition. In Twenty-Fourth International Joint Conference on Artificial Intelligence.

A APPENDIX
A.1 Supplementary figures

, Vol. 1, No. 1, Article . Publication date: January 2022.

20

• Nagesh, et al.

Fig. 8. Histogram of the time between two consecutive EMA prompts. We see that the time difference is around 4 hours
in most cases. The exceptions are when the app determines that the participant is unavailable and triggers an EMA at a
different time.

Fig. 9. Demographics distribution in our dataset. (a) Age (b) Gender

, Vol. 1, No. 1, Article . Publication date: January 2022.

