Accelerating Generalized Benders Decomposition
for Wireless Resource Allocation

Mengyuan Lee, Ning Ma, Guanding Yu, and Huaiyu Dai

1

0
2
0
2

t
c
O
5
1

]
T
I
.
s
c
[

2
v
4
9
2
1
0
.
3
0
0
2
:
v
i
X
r
a

Abstract—Generalized Benders decomposition (GBD) is a glob-
ally optimal algorithm for mixed integer nonlinear programming
(MINLP) problems, which are NP-hard and can be widely found
in the area of wireless resource allocation. The main idea of
GBD is decomposing an MINLP problem into a primal problem
and a master problem, which are iteratively solved until their
solutions converge. However, a direct implementation of GBD
is time- and memory-consuming. The main bottleneck is the
high complexity of the master problem, which increases over the
iterations. Therefore, we propose to leverage machine learning
(ML) techniques to accelerate GBD aiming at decreasing the
complexity of the master problem. Speciﬁcally, we utilize two
different ML techniques, classiﬁcation and regression, to deal
with this acceleration task. In this way, a cut classiﬁer and a
cut regressor are learned, respectively, to distinguish between
useful and useless cuts. Only useful cuts are added to the master
problem and thus the complexity of the master problem is
reduced. By using a resource allocation problem in device-to-
device communication networks as an example, we validate that
the proposed method can reduce the computational complexity
of GBD without loss of optimality and has good generalization
ability. The proposed method is applicable for solving various
MINLP problems in wireless networks since the designs are
invariant for different problems.

Index Terms—Machine learning, generalized Benders decom-
position, device-to-device communications, resource allocation,
mixed integer nonlinear programming

I. INTRODUCTION

Resource allocation is a popular and important topic in
wireless networks [1]. However, wireless resource allocation
problems generally involve two kinds of variables, i.e., the
continuous and discrete ones. Therefore, most of them are
formulated as mixed integer nonlinear programming (MINLP)
problems, which can be widely found in various existing works
[2]–[6].

Unfortunately, MINLP problems are generally NP-hard and
no efﬁcient globally optimal algorithm exists yet. Available
globally optimal algorithms for MINLP problems, such as the
branch and bound (B&B) algorithm [7] and the generalized
Benders decomposition (GBD) [8], suffer from the exponential
worst-case computational complexity. Therefore, sub-optimal

M. Lee and G. Yu are with College of Information Science and Elec-
tronic Engineering, Zhejiang University, Hangzhou 310027, China. e-mail:
{mengyuan lee, yuguanding}@zju.edu.cn. (Corresponding author: Guanding
Yu)

N. Ma is with the College of Computer Science and Technology, Zhejiang

University, Hangzhou 310027, China. e-mail: 3170101236@zju.edu.cn.

H. Dai

is with the Department of Electrical and Computer Engineer-
ing, North Carolina State University, Raleigh, NC 27606 USA. e-mail:
hdai@ncsu.edu.

This work was done while M. Lee was a visiting PhD student at NC State

University.

and heuristic algorithms of low complexity are widely used
for resource allocation in wireless networks based on various
mathematical
techniques, such as game theory [9], graph
theory [10], greedy search [11], and some other iterative
methods [12]. However, their performance has no guarantee
because the gaps between the optimal solutions and the sub-
optimal ones are difﬁcult to control.

Inspired by the success of machine learning (ML) in
the wireless communications community has
many ﬁelds,
recently turned to ML to obtain more efﬁcient methods for
resource allocation problems, such as power allocation [13]–
[18], link scheduling [19], [20], and user association [21]. All
aforementioned works can be classiﬁed into three different
learning paradigms. The ﬁrst one is the end-to-end learning
paradigm. For this learning paradigm, the input/output relation
of a given resource optimization problem is regarded as a
“black box”, which is directly learned by ML techniques
especially the deep neural networks (DNNs). However, this
learning paradigm is effective for resource allocation with
only one kind of output variables [13], [18]–[20] and does
not work well for MINLP problems. The second one is the
reinforcement learning paradigm. Because labeled samples
that are difﬁcult to obtain in wireless networks are not needed
in this learning paradigm, it is widely used in many wire-
less resource allocation scenarios [14], [15], [21]. However,
problem-dependent designs are needed and how to choose
appropriate designs is tricky, which needs iterative processes
of trial and error. The third one is the algorithm speciﬁc
learning paradigm, which aims at accelerating some existing
algorithms for optimization problems. Speciﬁcally, it uses ML
techniques to exploit the speciﬁc algorithm structures. For
example, authors in [16], [17] have utilized imitation learning
to learn the best prune policy to accelerate the B&B algorithm.
It can achieve satisfactory performance even without problem-
dependent designs. However, the accelerated B&B algorithm
in [16], [17] suffers from loss of optimality.

To circumvent problem-dependent design and avoid loss of
optimality, we propose to use ML techniques to accelerate
the GBD algorithm in this paper. GBD algorithm is widely
used for resource allocation in wireless networks [41]–[44]. It
performs equally well as the B&B algorithm but has a simpler
algorithm structure. The main idea of GBD is decomposing an
MINLP problem into a primal problem and a master problem.
These two problems are iteratively solved until their solutions
converge. At each iteration, the primal problem is ﬁrst solved,
and a cut, i.e., a new constraint, is generated according to
is added to the master problem.
its result. Then the cut
Finally, the master problem is solved. Over the iterations,

 
 
 
 
 
 
the number of added cuts increases and the master problem
becomes more and more time-consuming. Therefore, the high
complexity of the master problem is the main bottleneck
of GBD. To accelerate GBD, many mathematical methods
have been proposed. One of the widely used methods is
the multi-cut GBD [22], where multiple cuts instead of one
cut are generated at each iteration to reduce the number of
required iterations. In this way, fewer master problems are
needed to be solved and the total computational time can be
reduced. However, not all the cuts generated during multi-cut
GBD contribute equally to the convergence. Some cuts would
not bring much change to the convergence but increase the
complexity of master problem. Therefore, we propose to use
ML techniques to distinguish between useful and useless cuts,
and only add useful cuts to the master problem. In this way,
the complexity of master problems increases in a slower speed
and GBD can be further accelerated. Speciﬁcally, we utilize
two different ML techniques, classiﬁcation and regression, to
deal with this acceleration task. Then we design ﬁve features
to characterize the generated cuts. These features are invariant
and applicable for different resource allocation problems. We
also propose different data collection methods, label deﬁnition
methods, as well as training and testing processes for these two
kinds of ML techniques. In this way, a cut classiﬁer and a cut
regressor can be learned in the supervised manner, respec-
tively. Finally, we use a joint subcarrier and power allocation
problem in device-to-device (D2D) communication networks
as an example to verify the effectiveness and generalization
ability of the proposed method.

Actually, cut deletion or clean-up strategies are important
when multiple cuts are added to the master problems at each
iteration [23]. Most existing works use mathematical methods
to develop cut clean-up strategies. A similar work that has
leveraged ML techniques to develop cut clean-up strategies can
be found in [24], where the authors have accelerated Benders
decomposition [25] especially for the two-stage stochastic
programming problems. Speciﬁcally, the authors have used
ML techniques to evaluate cuts and their proposed method
is instance-speciﬁc, which means a speciﬁc model is needed
for each instance and the training process is conducted online.
However, reusing models for different problems and ofﬂine
training are generally preferred in wireless networks. More-
over, the two-stage stochastic programming problems are very
different from the MINLP problems and the cuts generated
in GBD are more difﬁcult to be characterized than those in
Benders decomposition. Therefore, the method in [24] may
not be directly applied for wireless resource allocation.

In brief, our main contributions are summarized as follows.
• We incorporate ML techniques to accelerate GBD. The
proposed method is a practically efﬁcient and globally opti-
mal algorithm. It is applicable for various MINLP resource
allocation problems in wireless networks,
thus avoiding
tricky problem-dependent designs in individual cases. Ex-
tensive simulation results suggest that the proposed method
can reduce the time complexity of GBD without loss of
optimality and has strong generalization ability.

• As a universal deﬁnition for useful cuts is not available [26],
the obtained binary labels for generated cuts are inevitably

2

noisy, which has a signiﬁcant impact on the performance.
To alleviate the impact of noise, we propose to utilize two
different ML techniques, i.e, classiﬁcation and regression,
to develop cut clean-up strategies and accelerate the GBD
algorithm. Moreover, simulation is conducted to analyze
and compare the advantages and disadvantages of these two
methods.

• To solve these two ML problems, we carefully design ﬁve
problem-independent features to describe the cuts generated
in GBD, together with the corresponding data collection
methods, label deﬁnition methods, as well as training and
testing processes.

• Moreover, based on theoretical analysis on noisy labels in
binary classiﬁcation, we provide some interesting insights
for choosing hyperparameters even without knowing the
accurate noise in the training samples, thus avoiding the
time-consuming process of trial and error in the traditional
hyperparameter selection procedure.
The rest of this paper is organized as follows. In Section
II, we introduce the general mathematical formulation for
wireless resource allocation and give a brief introduction to
GBD. In Section III, we use ML techniques to accelerate GBD.
The testing results of the proposed method are presented in
Section IV. Finally, we conclude this paper in Section V.

II. GBD FOR WIRELESS RESOURCE ALLOCATION

In this section, we ﬁrst introduce the general mathematical
formulation for wireless resource allocation problem. Then,
we introduce the basic idea of the classic GBD algorithm.
Finally, we analyze the drawbacks of GBD and introduce one
improved variant, i.e., the multi-cut GBD.

A. Problem Formulation for Wireless Resource Allocation

Resource allocation in wireless networks generally involves
two kinds of variables: the continuous ones such as data rate
and transmit power, and the discrete ones such as user selec-
tion and subcarrier assignment. Thus, many wireless resource
allocation problems can be formulated as the following generic
form of mixed integer optimization1

subject to

min
{x,y}

f (x, y),

G(x, y) ≤ 0,

x ∈ X ,

y ∈ Y,

(1)

(1a)

(1b)

(1c)

where f (·, ·) is the objective function such as energy con-
sumption and communication delay, x is the vector of n1
continuous variables, y is the vector of n2 discrete variables,
and G(·, ·) represents the vector of constraints deﬁned on
X × Y ⊆ Rn1 × Zn2, such as power constraints and data
rate requirements.

Problem (1) can be widely found in the study of wireless
networks, such as joint user association and power allocation

1Maximization problems can be transformed to minimization problems in

a straightforward way.

3

in carrier aggregation systems [2], optimal mode selection in
D2D networks [3], resource allocation in multi-user mobile-
edge computation ofﬂoading [4], trajectory planning and re-
course allocation for unmanned aerial vehicles [5], as well
as joint power allocation and relay selection in amplify-and-
forward cooperative communication system [6].

Generally, Problem (1) is NP hard and it is computationally
challenging to obtain its optimal solution. According to the
linearity of f (·, ·) and G(·, ·) with ﬁxing y, Problem (1) can
be further categorized into two kinds: mixed integer linear pro-
gramming (MILP) problems and MINLP problems. For MILP
problems, both f (·, ·) and G(·, ·) with ﬁxing y are supposed to
be linear functions. Unfortunately, this condition is not met by
most wireless resource allocation problems. Therefore, MILP
problems are less often encountered than MINLP problems
in wireless networks. The optimal algorithms for solving the
MINLP problems include the B&B algorithm and the GBD
algorithm. Generally, GBD has a simpler algorithm structure
than the B&B algorithm. In the following, we introduce the
basic idea of the GBD algorithm for MINLP problems.

B. Brief Introduction of GBD

The main idea of GBD is decomposing an MINLP prob-
lem into two sub–problems: a primal problem and a master
problem. These two problems are iteratively solved until their
solutions converge. To guarantee the convergence and the
global optimality of the GBD algorithm, the MINLP problem
in (1) should satisfy two conditions [8]:
• Convexity: the problem should be convex on x given the

discrete variables y;

upper bound to the original Problem (1). Given that Problem
(2) is convex, solving the problem also provides the optimal
Lagrange multipliers vector, i.e., the optimal dual variables
vector, µ(i), for constraint (2a). Then the Lagrange function
can be written as

L(x(i), y, µ(i)) = f (x(i), y) + µ(i)T

G(x(i), y).

On the other hand, if Problem (2) is infeasible, we add the
iteration index i into the infeasible primal problem index set,
I, as a new element and turn to an l1-minimization feasibility-
check problem as follows [8]

subject to

min
{x,α}

α,

G(x, y(i−1)) ≤ α,

x ∈ X ,

α ≥ 0.

(3)

(3a)

(3b)

(3c)

Problem (3) is named as feasibility-check problem because
its result reﬂects the feasibility of Problem (2). Speciﬁcally,
α = 0 means a feasible point of Problem (2) has been
found; otherwise, Problem (2) is infeasible. As mentioned
above, we only turn to Problem (3) when Problem (2) is
infeasible. Therefore, α = 0 will not happen in the GBD
algorithm. By solving the feasibility-check problem, we can
get the Lagrange multiplier vector λ(i) for constraints (3a).
The Lagrange function resulting from the infeasible primal
problem is deﬁned as

¯L(x(i), y, λ(i)) = λ(i)T

(G(x(i), y) − α).

• Linear separability: the problem should be linear on y given

the continuous variables x.

Note that an upper bound is only obtained from a feasible
primal problem.

Otherwise, GBD algorithm may fail to converge or converge
to a local optimum. Note that linear separability is satisﬁed by
many wireless resource allocation problems, such as Problem
(6) in Section IV.A. In this paper, we only focus on the MINLP
problems that satisfy the above two conditions, for which the
GBD algorithm always convergences to the global optimum.
As for the non-convex MINLP problems, many variations of
GBD have been proposed [27]–[29]. How to accelerate those
modiﬁed GBD algorithms is interesting for future work.

For a convex and linear separable MINLP problem, the
primal problem is also convex and can be solved by vari-
ous convex optimization techniques. Speciﬁcally, the primal
problem at the i-th iteration ﬁnds the optimal x by ﬁxing the
discrete variables y on a speciﬁc value y(i−1) (or a given
initial y(0)) and can be written as

subject to

f (x, y(i−1)),

min
x∈X

G(x, y(i−1)) ≤ 0.

(2)

(2a)

Note that some speciﬁc value y(i−1) may lead to an infeasible
primal problem (2). If Problem (2) is feasible, the correspond-
ing solution is denoted as x(i) and we add the iteration index
i into the feasible primal problem index set, F , as a new
element. Clearly, the optimal value f (x(i), y(i−1)) provides an

As for the master problem, it is obtained based on nonlinear
convex duality theory. Considering the Variant 2 of GBD (V2-
GBD) [8] without loss of generality, a relaxed master problem
is given by

min
{y,η}

η,

(4)

subject to

y ∈ Y,
η ≥ L(x(j), y, µ(j)), ∀j ∈ F , µ(j) (cid:23) 0,
0 ≥ ¯L(x(j), y, λ(j)), ∀j ∈ I,

(4a)

(4b)

(4c)

where constraints (4b) and (4c) are referred to as the optimality
and feasibility cuts, respectively. Problem (4) is an MILP
problem and can be solved by various MILP solvers, such
as CPLEX2. Note that CPLEX can be also used for MINLP
it can only solve the mixed integer
problems. However,
second-order cone programming and mixed integer quadratic
or quadratically constrained programming problems, which
only cover a small part of the resource allocation problems
in wireless networks. The relaxed master problem provides
a lower bound, η∗(i), to the original Problem (1) and its
solution, y(i), is used to generate the primal problem in the
next iteration. The whole procedure of GBD is summarized in

2https://www.ibm.com/analytics/cplex-optimizer

Table I.

TABLE I
GENERALIZED BENDERS DECOMPOSITION

Set iteration index, i = 0.
Set maximum iteration number, M .
Set tolerance, ∆.
Select an initial value for y(0).
U BD(0) = ∞, LBD(0) = −∞.

Algorithm 1 Generalized Benders Decomposition
1: initialization
2:
3:
4:
5:
6:
7: while |(U BD(i) − LBD(i))/LBD(i)| > ∆ and i < M do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end while

i = i + 1.
Solve the primal problem (2) by ﬁxing y as y(i−1).
if the primal problem is feasible then
Obtain the optimal solution x(i).
Obtain L(x(i), y, µ(i)) and an optimality cut C (i).
Set U BD(i) = min(U BD(i−1), f (x(i), y(i−1))).

end if
Add C (i) into the relaxed master problem (4) .
Solve the relaxed master problem (4) to obtain η
Set LBD(i) = η

Solve the feasibility-check problem (3).
Obtain ¯L(x(i), y, λ(i)) and a feasibility cut C (i).

else

∗.

∗ and y(i).

C. Multi-cut GBD

A direct implementation of the classical GBD may require
excessive computing time and memory. Many works have
been dedicated to exploring ways to improve the convergence
speed of the algorithm by reducing the required time for each
iteration or the number of required iterations. The former goal
can be achieved by improving the procedure used to solve
the primal and relaxed master problems at each iteration,
which needs to be designed according to speciﬁc optimization
problems. On the other hand, the latter goal can be achieved
by improving the quality of the generated cuts, which is
applicable for all optimization problems. In the following, we
introduce an acceleration method of this kind named multi-cut
GBD [22], which is the basis of our ML-based acceleration
method to be introduced in Section III.

The key idea of multi-cut GBD is generating a number
of cuts at each iteration to reduce the number of required
iterations. The detailed procedure of the multi-cut GBD is
summarized in Table II. The main difference between Algo-
rithms I and II is that a set S of discrete solutions, instead of
only one solution, is obtained while solving the relaxed master
problem (4). The set S includes both optimal and suboptimal
solutions for Problem (4). Generally, the size of S is set as
a given constant, S. To get S , we ﬁrst use MILP solvers to
get all feasible solutions for Problem (4). Then we add the
solution with the smallest gap between its objective value, η,
and the optimum value, η∗, one by one into S until |S | = S.
Note that S should not be set large. Otherwise, there may
not exist enough feasible solutions. If the number of existing
solutions is less than S, we will collect all existing solutions
as the set S . In this way, we can get |S | primal problems
by ﬁxing the discrete variables, y, in the Problem (2) as the
solutions in set S . By solving the |S | primal problems, |S |
cuts can be obtained and added to the relaxed master problem
(4). Then Problem (4) is solved and a new set S is obtained,

4

Set iteration index, i = 0.
Set maximum iteration number, M .
Set tolerance, ∆.
Select the initial solution set as S = {y(0)}.
U BD(0) = ∞, LBD(0) = −∞.

TABLE II
MULTI-CUT GENERALIZED BENDERS DECOMPOSITION
Algorithm 2 Multi-cut Generalized Benders Decomposition
1: initialization
2:
3:
4:
5:
6:
7: while |(U BD(i) − LBD(i))/LBD(i)| > ∆ and i < M do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

i = i + 1, s = 0.
while |S | 6= 0 do
s = s + 1.
ys ← pop out the best solution from S .
Solve the primal problem (2) by ﬁxing y as ys.
if the primal problem is feasible then
Obtain the optimal solution x(i).
Obtain L(x(i), y, µ(i)) and an optimality cut C (i)
s .
Set U BD(i) = min(U BD(i−1), f (x(i), ys)).

Solve the feasibility-check problem (3).
Obtain ¯L(x(i), y, λ(i)) and a feasibility cut C (i)
s .

into the relaxed master problem (4) .

end while
Solve problem (4) to obtain the optimal η
multiple solutions for y.
Set LBD(i) = η∗.

end if
Add C (i)

else

s

∗ and a set S of

24:
25: end while

which is used to generate new |S | primal problems in the
next iteration.

According to [22], the multi-cut GBD does not change the
optimal solution of the GBD algorithm for convex MINLP
problems. The multiple cuts generated in the multi-cut GBD
can improve the obtained lower bounds when solving the
relaxed master problem. Therefore, the total number of re-
quired iterations decreases and fewer relaxed master problems
need to be solved as well. On the other hand, more primal
problems need to be solved in multi-cut GBD than in classical
GBD. Fortunately, the |S | primal problems are independent
and can be solved in parallel while implementing multi-cut
GBD, which will not bring additional overhead in terms of
computational time if parallel computing is available. Overall,
the total computational time can be expected to decrease when
the multi-cut GBD is deployed, according to [22].

However, more cuts are added to the relaxed master problem
at each iteration in the multi-cut GBD compared with the
classical GBD. Given the fact that the relaxed master problem
will be more time-consuming as more cuts are added, the
main overhead of multi-cut GBD is the average time consumed
by solving the relaxed master problem. Note that, among all
the cuts generated at each iteration in multi-cut GBD, some
cuts do not contribute much change on the lower bound but
would increase the complexity of the relaxed master problem.
We name these cuts as useless cuts and the others as useful
cuts. Actually, only useful cuts need to be added into the
relaxed master problem at each iteration. If we can distinguish
between useful and useless cuts and only add useful cuts
to the relaxed master problem at each iteration, the average
complexity of the relaxed master problems in multi-cut GBD
will decrease, which further accelerates the whole algorithm.
According to [23], there is no need to optimally solve Problem

5

(4) at each iteration to produce cuts for the convex MINLP
problems. Optimality can be guaranteed even though we use
the cuts generated from sub-optimal solutions of Problem (4).
Therefore, our proposed methods can still achieve optimality
for the convex MINLP problems by only adding useful cuts
to the relaxed master problem. In the following, we propose
to use ML techniques to achieve this goal.

III. MACHINE LEARNING BASED GBD

As mentioned above, we aim to accelerate GBD by distin-
guishing between useful and useless cuts. In this section, we
ﬁrst provide an overview of the ML approach. Speciﬁcally, we
utilize two ML techniques to deal with the acceleration task.
Then we design features, introduce data collection and label
deﬁnition, and propose the training and testing processes for
theses two ML techniques, respectively. Finally, we analyze
the computational complexity and space complexity of the
proposed methods.

A. Overview of the Machine Learning Approach

Clearly, the deﬁnition of useful and useless cuts is vague
up to this point. Actually, there is no reliable way to identify
useless cuts [26]. If we use the vague terms to label a cut,
we can only quantify the degree to which the cut satisﬁes the
concept of useful or useless with a membership value in [0, 1].
The membership value corresponds to some continuous per-
formance indicators that can be directly collected by running
the GBD algorithm. Given that our ultimate goal is to only
add useful cuts to the relaxed master problem, the continuous
membership values need to be transformed into the binary
labels. The transformation rule is handcrafted and heuristic,
which will induce noise in the binary label of each cut. Gen-
erally, noisy labels have a signiﬁcant impact on performance
and need to be carefully dealt with. Therefore, we propose
to utilize two different ML techniques, i.e., classiﬁcation and
regression, to deal with this acceleration task. Classiﬁcation
and regression are two classical techniques in ML. The task
of classiﬁcation is predicting a discrete label and that of
regression is predicting a continuous quantity. Moreover, the
algorithms and evaluation metrics of them are also different.
In the following, we introduce how to solve the acceleration
task using these two ML techniques, respectively. We also
analyze and compare their advantages and disadvantages in
detail using the simulation results in Section IV.

B. “Cut Classiﬁer” Machine Learning Algorithm

As mentioned above, classiﬁcation aims to predict the dis-
crete labels. Therefore, by using the classiﬁcation technique,
we directly train a model, i.e., a cut classiﬁer, to predict the
binary label of each cut.

1) Feature Design: Feature design is very important for cut
classiﬁer training. To achieve better performance in the ML
based GBD algorithm, we need to dig out proper features that
are closely related to the generated cuts. Given that our pro-
posed method is expected to be used in a large class of mixed
integer wireless resource allocation problems, the proposed

features are supposed to be invariant in different problems,
i.e., to be problem-independent. Therefore, we only focus on
the structure of GBD and do not take into consideration the
speciﬁc features of resource allocation problems and wireless
networks. In this way, the designed features consist of the
following ﬁve categories.

• Cut optimality indicator: As mentioned in Section II, cuts
are divided into two kinds: optimality cut and feasibility cut.
We set the cut optimality indicator as 1 for the optimality
cut and 0 for the feasibility one. The optimality cut is
derived from the feasible primal problem (2), while the
feasibility cut is derived from the feasibility-check problem
(3). As mentioned before, the upper bound is obtained only
from the feasible primal problem while the feasibility-check
problem does not contribute to the convergence. Therefore,
the optimality cut is preferred than the feasibility cut.

• Cut violation: Cut violation has been ﬁrst proposed in [24]
for Benders decomposition. It can also be used in this work
with some modiﬁcations. Cut violation means how large
the feasible region of the relaxed master problem is cut
off by adding a cut. For a cut generated at the current
solution (x(i), y(i−1), η(i−1)), the cut violation is deﬁned
as L(x(i), y(i−1), µ(i)) − η(i−1) if the cut is an optimality
cut. And for the feasibility cut, its cut violation is deﬁned
as ¯L(x(i), y(i−1), λ(i)). Larger cut violation means that the
new feasible region after adding the cut is far from the
current solution, which is expected to bring about larger
change on the lower bound and thus is preferred.

• Cut repeat: A cut may be generated more than once during
the multi-cut GBD algorithm. As mentioned above, we col-
lect both optimal and sub-optimal solutions for the relaxed
master problem at each iteration during the multi-cut GBD
algorithm. A speciﬁc y may be the sub-optimal solution for
the relaxed master problems in different iterations, thus its
corresponding cut might be generated more than once. For
a cut generated at iteration i, its cut repeat is deﬁned as
how many times this cut has been generated until the i-th
iteration. Cut repeat is a very important feature. Repeatedly
adding existing cuts may lead to redundancy without any
contribution to the change of lower bound.

• Cut depth: For a cut generated at iteration i, its cut depth is i.
Because the objective value of the relaxed master problem,
η, increases in a decreasing speed over iterations, we cannot
expect the change amount of η brought by the cuts generated
at different iterations to be equally large. On the other
hand, usually fewer cuts need to be added to the relaxed
master problem over iterations. Therefore, cut depth is also
a necessary feature for the generated cuts.

• Cut order: While solving the relaxed master problem in
multi-cut GBD, multiple solutions for the discrete variables,
y, are generated. Each solution corresponds to a cut. As
mentioned in Section II.C, at each iteration, we ﬁrst generate
all feasible solutions for the relaxed master problem, and
then add the solution with the smallest gap value, η−η∗, into
the solution set S one by one until |S | = S or all feasible
solutions are already added into S . In this way, different
solution is added into S in a different order, which is also

TABLE III
DATA COLLECTION METHOD FOR CUT CLASSIFIER

initialization

Set training dataset: T = ∅.

while |(U BD(i) − LBD(i))/LBD(i)| > ∆ and i < M do

Set iteration index, i = 0.
Set maximum iteration number, M .
Set tolerance, ∆.
Select the initial solution set as S = {y(0)}.
U BD(0) = ∞, LBD(0) = −∞.

Algorithm 3 Data Collection Method for Cut Classiﬁer
1: initialization
2:
3: for problem instance Q in Q do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

i = i + 1.
y(i−1) ← randomly pop out a solution from S .
Solve the primal problem (2) by ﬁxing y as y(i−1).
if the primal problem is feasible then
Obtain the optimal solution x(i).
Obtain L(x(i), y, µ(i)) and an optimality cut C (i).
Set U BD(i) = min(U BD(i−1), f (x(i), y(i−1))).

Solve the feasibility-check problem (3).
Obtain ¯L(x(i), y, λ(i)) and a feasibility cut C (i).

end if
Add C (i) into the relaxed master problem (4) .
Solve problem (4) to obtain the optimal η∗ and a set S
of multiple solutions for y.
Set LBD(i) = η
CI (i) = LBD(i) − LBD(i−1).
if i > 1 then

else

∗.

if CI (i)/CI (i+1) > θ then

T ← {F eature(C (i−1)), 1}.

T ← {F eature(C (i−1)), 0}.

else

24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35: end for
36: return training set T .

end if

end if
end while
T ← {F eature(C (i−1)), 1}.

deﬁned as its corresponding cut’s order. To some extent, cut
order can reﬂect the quality of the cut.

With the above ﬁve features, each cut is described by a ﬁve-

dimensional vector, which is then input to the cut classiﬁer.

2) Data Collection and Label Deﬁnition: When we focus
on a speciﬁc resource allocation problem, we can generate
a training problem set, Q, which includes different problem
instances. Then, we propose to run a modiﬁed GBD algorithm
on each problem instance in Q to collect the generated cuts as
the training samples. Speciﬁcally, a set S of multiple discrete
solutions are obtained from the relaxed master problem at each
iteration of the modiﬁed GBD. Then we only randomly choose
one solution from S and ﬁx the discrete variables, y, as the
selected solution to get the primal problem (2). We solve this
primal problem and get a cut. Finally, we add the cut to the
relaxed master problem and solve it. The above process is
different from the multi-cut GBD, where multiple cuts are
derived at each iteration. It is also different from the classical
GBD, where the cut at each iteration is always derived from
the optimal solution of the relaxed master problem and lacks
variety.

Because we want to learn the cut classiﬁer in a supervised
manner, both the features and the label for each cut should

6

True Label

Defined Label

0 (useless)

0 (useless)

!"
!#

1 (useful)

1 (useful)

Fig. 1. Relations between the true labels and the deﬁned labels.

be collected during the data collection process. As mentioned
above, the binary label of each cut, denoted as BL, cannot
be directly collected by running the GBD algorithm. It is
transformed from some continuous performance indicators
with handcrafted rules. In our work, we assign BL as 1 for a
useful cut and 0 for a useless one. Here, we denote the change
amount of the objective value of the relaxed master problem,
η, before and after adding a certain cut at the i-th iteration as
CI (i). We use CI (i) as the continuous performance indicator
of the cut derived at the i-th iteration. As mentioned before,
the objective value of the relaxed master problem increases in
a decreasing speed over the iterations. Therefore, we prefer the
cut that can bring a large CI as compared to the cut in the next
iteration. In this way, the binary label of the cut derived at the
i-th iteration, BL(i), is assigned as 1 if CI (i)/CI (i+1) > θ,
and as 0 otherwise. Meanwhile, the binary label of the cut
derived at the last iteration is always set as 1. In summary,
the details of the data collection method for cut classiﬁer are
shown in Table III.

As mentioned in Section III.A, the handcrafted transforma-
tion rule is heuristic and will induce noise in the binary labels.
The relation between the true labels and the deﬁned labels
can be illustrated as Fig. 1. To alleviate the impact of noise,
choosing an appropriate threshold, θ, is very important. We
initially set threshold θ as 1 and adjust it according to the
output performance. According to [30], the existence of noise
will not change the misclassiﬁcation probability if α0 = α1. It
suggests that we can avoid the inﬂuence of noise by achieving
a balance between α0 and α1. Clearly, a larger threshold means
fewer cuts are deﬁned as useful, which leads to a larger α1
and a smaller α0. Meanwhile, a smaller threshold leads to the
opposite result. Based on these facts, we can carefully adjust
θ to achieve the balance between α0 and α1 and alleviate the
impact of noise although we do not know the exact values
of error rates. If the accuracy of the learned classiﬁer is not
satisfactory because of α0 6= α1, we may try a larger θ and a
smaller one at the same time since we do not know which error
is bigger. One of the new thresholds will decrease |α0 − α1|,
which brings about better performance. Otherwise, the one
adopted now is already the best threshold and we need to try
other methods for performance improvement.

3) Training Process: After collecting the training dataset T
using Algorithm III, we can adopt some well-known classiﬁca-
tion techniques, such as support vector machine (SVM) [31],
linear discriminant analysis (LDA) [32], and logistic regression
[33], to learn the cut classiﬁer. Given the fact that the number
of the useless cuts is generally greater than that of the useful
cuts, the collected training dataset T is imbalanced, which
will inﬂuence the performance of the learned cut classiﬁer.
Therefore, we use the undersampling technique [34] on the
collected dataset T before training. This technique uses a

subset of the majority class for training. Since many majority
class samples are ignored, the training set becomes more bal-
anced and is denoted as ˆT . Then we use the aforementioned
ML classiﬁcation techniques with sampled training dataset ˆT
and plot the receiver operating characteristic (ROC) curves to
evaluate the diagnostic ability of learned cut classiﬁers with
different classiﬁcation techniques. We choose the best one for
the following testing process.

The overhead of the data collection process and the training
process consists of the following three parts: the time con-
sumed by collecting the training dataset T from the training
problem set Q, the memory consumed by storing T , and
the time consumed by learning the cut classiﬁer from T . As
mentioned above, the classiﬁcation techniques we adopt are all
from the classical ML ﬁeld. They only need a small number of
training samples to get good models and converge fast, which
is different from the deep learning technique. In this way, the
overhead of collecting and storing T is modest, and the time
consumed by learning the cut classiﬁer is also negligible.

We shall note that the training process of our proposed
method is ofﬂine. Moreover, the proposed method has general-
ization ability on different aspects and a learned cut classiﬁer
can be used for more than one applications, which will be
validated by the simulation results in Section IV. In this way,
the overhead of the training process may be amortized in
practice over different applications. In summary, the overhead
of the data collection process and the training process will
not bring negative effects on the performance of our proposed
method and we will not take this overhead into consideration
for the following testing process.

4) Testing Process: The testing process is very similar to
the multi-cut GBD in Table II. The only difference is that we
need to use the learned cut classiﬁer to determine whether the
cut is useful before adding the generated cuts into the relaxed
master problem. We add it to the relaxed master problem
only if the cut is classiﬁed as useful. However, if all the |S |
generated cuts are labeled as useless cuts by the cut classiﬁer,
we switch to the classical GBD and add the cut generated
by the optimal solution y∗. The detailed testing process is
summarized in Table IV.

C. “Cut Regressor” Machine Learning Algorithm

Different from the cut classiﬁer that directly predicts the
binary label, the task of regression is predicting a continuous
quantity. Therefore, by using the regression technique, we
train a model, i.e., a cut regressor, to predict the continuous
performance indicator of each cut.

1) Feature Design: In Section III.B, ﬁve features closely
related to the generated cuts are proposed, which are invariant
for different problems. We still use these ﬁve features for
training cut regressor.

2) Data Collection and Label Deﬁnition: For the “cut
regressor” ML problem, we propose to collect training dataset
by directly running each problem instance in the training
problem set, Q, with the multi-cut GBD in Table II. Since the
goal of cut regressor is learning the continuous performance
indicators instead of the binary labels of each cut, the label

7

Set iteration index, i = 0.
Set maximum iteration number, M .
Set tolerance, ∆.
Select the initial solution set as S = {y(0)}.
U BD(0) = ∞, LBD(0) = −∞.

i = i + 1, s = 0, ncut = 0.
while |S | 6= 0 do
s = s + 1.
ys ← pop out the best solution from S .
Solve the primal problem (2) by ﬁxing y as ys.
if the primal problem is feasible then
Obtain the optimal solution x(i).
Obtain L(x(i), y, µ(i)) and get an optimality cut C (i)
s .
Set U BD(i) = min(U BD(i−1), f (x(i), ys)).

TABLE IV
TESTING PROCESS WITH CUT CLASSIFIER πc OR CUT REGRESSOR πr
Algorithm 4 Testing Process with Cut Classiﬁer πc or Cut
Regressor πr
1: initialization
2:
3:
4:
5:
6:
7: while |(U BD(i) − LBD(i))/LBD(i)| > ∆ and i < M do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:

Using πc or πr to decide whether C (i)
If useful, add it into the relaxed master problem (4).
ncut = ncut + 1, s = s − 1.

Solve the feasibility-check problem (3).
Obtain ¯L(x(i), y, λ(i)) and get a feasibility cut C (i)
s .

end while
if ncut = 0 then
Add C (i)

end if
end while
while s > 0 do

into the relaxed master problem (4).

end if
Solve problem (4) to obtain the optimal η
multiple solutions for y.
Set LBD(i) = η∗.

∗ and a set S of

is useful.

else

1

s

31:
32: end while

s . ACI (i)
s

for each collected data should be the continuous performance
indicator. Different from the data collection process for cut
classiﬁer, multiple training samples are generated at each
iteration. In this case, we cannot use CI (i)/CI (i+1) as the
continuous performance indicators any more, because samples
generated at the same iteration are related and we need to
take their relations into consideration. To deal with this issue,
we order the cuts generated at the same iteration using the
same method mentioned in Section III.B and propose two
new indicators. The ﬁrst one is the change amount of the
objective value of the relaxed master problem, η, before and
after adding the ﬁrst s cuts at the i-th iteration, denoted as
ACI (i)
reﬂects the cumulative inﬂuence of the ﬁrst
s cuts and does not decrease with s. The second one is the
change amount of the objective value of the relaxed master
problem, η, before and after adding all |S | cuts at the i-th
iteration, denoted as CT (i). To get normalized labels that are
preferred in ML, we introduce a new variable CR(i)
as the
s
performance indicator for the s-th cut at the i-th iteration,
which is deﬁned as the ratio between ACI (i)
and CT (i), i.e.,
s
s /CT (i). As mentioned above, ACI (i)
CR(i)
does
s
not decrease with s. Therefore, CR(i)
also does not decrease
s
with s and its’ value varies from 0 to 1. During the data
collection process, we collect aforementioned ﬁve features and
the proposed performance indicator, CR(i)
s , as the label for the
s-th cut at the i-th iteration.

s = ACI (i)

3) Training Process: After collecting the training dataset,
we can adopt some well-known regression techniques, such
linear regression [36], ridge regression
as extra tree [35],
[37], lasso regression [38] and SVM regression [31], to learn
the cut regressor. Note that there is no data imbalance issue
for regression problems and the undersampling process is
not needed before training. To evaluate how well the model
ﬁts the data, we check the R-squared score [39] of each
regression model. Suppose that we have a training set T =
{(u(n), z(n)); n = 1, ..., N } consisting of N training samples,
where u(n) is the vector of input features and z(n) is the label.
Then the R-squared score is deﬁned as

,

P

P

P

R2 = 1 −

N
n=1(ˆz(n) − z(n))2
N
n=1(¯z − z(n))2
is the predicted result of sample (u(n), z(n))
where ˆz(n)
N
n=1 z(n)/N . The R-squared score indicates the
and ¯z =
percentage of the response variable variation that is explained
by the regression model, and therefore it is always between
0 and 1. Generally, the larger the R-squared score is, the
better the regression model ﬁts the data. Therefore, we choose
the regression model with the largest R-squared score for the
following testing process. In addition, the overhead of the data
collection process and the training process for the cut regressor
is similar to that of the cut classiﬁer, which is negligible and
will not be considered during the following testing process.

4) Testing Process and Handcrafted Label Transformation
Rule: The testing process of the cut regressor is also very
similar to that of the cut classiﬁer. The difference is that we can
only get the predicted continuous indicator for each cut instead
of directly knowing whether the cut is useful before adding the
generated cuts into the relaxed master problem. To transform
the continuous indicator into the binary label BL ∈ {0, 1}
during the testing process, we propose the following rule:

BL(i)

s =

0,
1,

(

s > 1 and ˆCR
otherwise,

(i)
s = ˆCR

(i)
s−1 = 1,

(5)

(i)
s

where ˆCR
is the predicted continuous performance indicator
for the s-th cut at the i-th iteration. This rule can be explained
as follows. During the multi-cut GBD, we always ﬁrst use
the best solution from S . Therefore, when s = 1, the cut is
generated from the optimal solution y∗. Given that at least
one cut should be added to the relaxed master problem, the
cut with s = 1 must be deﬁned as a useful cut. Meanwhile,
CR(i)
s ∈ [0, 1] and it does not decrease with s. An example
of 8-cut GBD is shown in Fig. 2, where CR(i)
increases with
s
s and quickly reaches 1. Deﬁne s(i)
F as the critical point such
that CR(i)
s with
s ≤ s(i)
F , it contributes to the change of the objective function
of the relaxed master problem and should be deﬁned as a
useful cut. We call the interval [1, s(i)
F ] as the useful cut region.
s with s > s(i)
Meanwhile, the cut C(i)
F does not contribute to
the change of the objective function of the relaxed master
problem and should be deﬁned as a useless cut. We call the
interval (s(i)
F , S] as the useless cut region. It is easy to verify

F . Then, for the cut C(i)

s = 1 when s ≥ s(i)

8

(i)
(i−1)
that ˆCR
s = ˆCR
s
useless cut region.

= 1 is an equivalent condition for the

Useful Cut Region

%&’
!(

Useless Cut Region

’
&
$%
#
"

r
o
t
a
c
i
d
n
I

e
c
n
a
m
r
o
f
r
e
P
s
u
o
u
n
i
t
n
o
C

Fig. 2. An example of the relation between CR(i)

s

and s.

Cut order !

With the predicted continuous indicator and the label trans-
formation rule in (5), we can determine whether a cut is useful.
And only useful cuts are added to the relaxed master problem.
Note that the situation where all the |S | generated cuts are
labeled as useless cuts will not happen in the testing process
for the cut regressor, because we always deﬁne the cut with
s = 1 as a useful one in the transformation rule. The detailed
testing process is also summarized in Table IV.

D. Computational Complexity and Space Complexity Analysis

In this subsection, we analyze the computational complexity

and space complexity of our proposed method, respectively.

The computational complexity analysis of GBD algorithm
is still open in the literature. Given the NP-hardness of the
considered MINLP problems, the worst-case computational
complexity of the proposed algorithm still grows exponen-
tially with the problem size, while the average computational
complexity is usually unknown despite of some special cases.
Therefore, one may pay more attention to testing the practical
computational complexity of an accelerated GBD algorithm in
general. The practical computational complexity is obtained by
doing simulations, which is further discussed in Section IV.

To analyze the space complexity, we notice that the most
memory-consuming part of the GBD algorithm lies in the
relaxed master problem because the cumulative number of
cuts added to the relaxed master problem, denoted as Sc,
increases over the iterations. Therefore, the space complexity
of GBD is directly related to Sc. Furthermore, each cut is
generated by sequentially solving the primal and the relaxed
master problems, where the value of y from the last iteration
and the value of x from the primal problem are needed to
be stored sequentially. Therefore, the space complexity of the
GBD algorithm can be expressed as O(Sc · max(n1, n2)),
where n1 and n2 are the dimensions of x and y, respectively.
Given n1 and n2, the multi-cut GBD algorithm generally adds
more cuts to the relaxed master problem than the classical
one. Therefore, the multi-cut GBD algorithm has a larger
Sc and is more memory-consuming than the classical one.
On the other hand, our proposed method can discard useless
cuts. As suggested from the following simulation results in
Section IV, the Sc of our proposed method is close to that
of the classical GBD, and much smaller than that of the

multi-cut GBD. Therefore, our proposed method has a similar
space complexity to the classical GBD and a much lower one
compared with the unaccelerated multi-cut GBD algorithm.

IV. TESTING RESULTS

In this section, we test the performance of the proposed ML
based GBD. We use a joint subcarrier and power allocation
problem in D2D networks as an example. We ﬁrst brieﬂy intro-
duce the system model and resource allocation problem. Then
we present the testing results of the proposed method on this
problem. We also pay special attention to the generalization
ability of the proposed method, which is difﬁcult to meet but
is preferred in practice.

A. Resource Allocation Problem in D2D Networks

We adopt the system model and problem formulation in
[17], where an uplink single-cell scenario with K cellular
users (CUs) in a set K and L D2D pairs in a set L is
considered. Each CU connects to the evolved nodeB (eNB)
with an orthogonal channel and each D2D pair transmits data
by reusing the uplink channels of CUs. To optimize the overall
data rate and achieve user fairness, a max-min optimization
problem is formulated. Speciﬁcally, the objective function is
to maximize the minimum data rate among all D2D pairs,
where the transmit power vector of CUs, pC, the transmit
power vector of D2D pairs, pD, and the discrete indicator
vector of subcarrier allocation, ρ, are to be determined.

We consider the following constraints. First, the data rate
of each CU is required to be no less than a given threshold,
RC
min. Second, the power of individual D2D pair is constrained
by an upper bound, P D
max. Third, the power of each CU
is also constrained by an upper bound, P C
max. Finally, each
subcarrier can be reused by at most one D2D pair to limit the
interference. In this way, the mathematical formulation of this
MINLP problem is given by [17]

max
,pD

{pC

,ρ}

RD

l (pC, pD, ρ),

min
l∈L

subject to

ρkl ∈ {0, 1},

∀k ∈ K , l ∈ L ,

ρkl ≤ 1,

∀k ∈ K ,

l∈L
X
ρklpD

kl ≤ P D

max,

∀l ∈ L ,

RC

k∈K
X
k(pC, pD, ρ) ≥ RC
k ≤ P C
pC

max,

∀k ∈ K ,

min,
∀k ∈ K ,

(6)

(6a)

(6b)

(6c)

(6d)

(6e)

where RC
l, respectively. They are given by

k and RD

l are the data rates of CU k and D2D pair

RC

k(pC, pD, ρ) = log(1 +

σ2
N +

RD

l (pC, pD, ρ) =

k∈K
X

P
ρkl log(1 +

),

klhDB
l

pC
khCB
k
l∈L ρklpD
klhD
ρklpD
l
khCD
N + pC
σ2
kl

),

TABLE V
D2D NETWORK PARAMETERS
Parameter

Value

9

Cell Radius
Noise Spectral Density
Path Loss Model for Cellular Links
Path Loss Model for D2D Links
Shadowing Standard Deviation

Maximum Transmitter Power of CU, P C
max
Maximum Transmitter Power of D2D Pair, P D
Minimum Data Rate of CU, RC

min

max

500 m
-174 dBm/Hz
128.1+37.6log(d[km])
148+40log(d[km])
10 dB
20 dBm
20 dBm
2 bit/s/Hz

where σ2
N is the power of the additive white Gaussian noise
(AWGN), hCD, hCB, hD, and hDB are the channel power
gains between CU and the receiver of the D2D pair, CU
and the eNB, the transmitter and the receiver of the D2D
pair, and the transmitter of the D2D pair and the eNB,
respectively. Since the data rates of D2D users are independent
of each other, it can be proven that Problem (6) satisﬁes the
convexity and linear separability mentioned in Section II.B.
Detailed discussions of its convexity can be found in [17]
and omitted here to avoid redundancy. Note that the resource
allocation problems in [2], [4] are similar to Problem (6),
which suggests that Problem (6) is a suitable example to verify
the effectiveness of our proposed method.

B. Simulation Setup

1) Parameters for D2D Network: We consider a single-cell
network with a radius of 500 m, where the eNB is located in
the center and all the users are distributed uniformly in the
cell. Some important network parameters are given in Table
V and more details can be found in [17].

2) Parameters for Three Kinds of GBD algorithms: During
the simulation, we compare the performance of classical GBD,
multi-cut GBD, and ML based GBD. For all these three kinds
of GBD algorithms, we set the convergence tolerance for
the upper and lower bounds, ∆, as 0.5%, and the maximum
iteration number, M , as 10,000. We use the CPLEX 12.9 to
obtain the solutions of the relaxed master problems and get the
multiple solutions by using solution pool and setting relevant
parameters of CPLEX. All other codes are implemented in
python 3.6 except the solving process for the primal problem
that is implemented in Matlab. The code is run on the Windows
system with a 3.6 GHz CPU. In the following, we call the
classical GBD as single-cut GBD, and the multi-cut GBD with
S = s as s-cut GBD.

To choose an appropriate value of S for the multi-cut GBD,
the data collection process, and the testing process, we test
the performance of multi-cut GBD with different values of S.
The results are summarized in Table VI, where the average
cut number is deﬁned as the ratio between the cumulative cut
number and the needed iteration number. As mentioned in
Section II.C, there may not exist enough feasible solutions if
S is large. When the number of existing feasible solutions is
smaller than S, we collect all existing solutions as the set S
and |S | < S. Therefore, the average cut number is generally
smaller than S. The multi-cut GBD algorithm with different S
may have similar average cut number and performance. In this
case, a smaller S is usually preferred, which is closer to the
average cut number and better reﬂects the characteristics of the
multi-cut GBD algorithm. On the other hand, the cumulative
time of multi-cut GBD ﬁrst decreases and then ﬂuctuates with

1.0

0.8

0.6

0.4

0.2

e
t
a
R
e
v
i
t
i
s
o
P

e
u
r
T

0.0

0.0

SVM(area = 0.92)
LDA(area = 0.92)
Logistic Regression(area = 0.92)

0.2

0.4

0.6

0.8

1.0

False Positive Rate

Fig. 3. ROC curves for different classiﬁcation techniques.

the increase of S. Therefore, there will exist a critical point
where the average cut number is close to S and the cumulative
time begins to ﬂuctuate. The critical point corresponds to
an appropriate value of S for the baseline multi-cut GBD
algorithm. Speciﬁcally, the critical point for our tested problem
is S = 8. Therefore, we choose S = 8 for the following testing
process and only compare the proposed method with 8-cut
GBD. Note that the value of S is usually pre-determined and
limited to a small range in practice [22]. Moreover, ﬁnding
the critical point of S aims at choosing a baseline for fair
comparison, which is not needed in practice and no overhead
will be incurred.

C. Performance of Machine Learning Based GBD

In this part, we focus on the performance of the proposed
ML based GBD. We set the number of CUs, K, as 5, and
that of D2D pairs, L, as 3. The training and testing problem
sets both include 50 problem instances that are generated by
randomly placing CUs and D2D pairs. Note that 50 problem
instances correspond to thousands of cut samples and we
report the average performance over the testing problem set.
As mentioned in Section III.B, we have adopted three well-
known ML classiﬁcation techniques to learn the classiﬁer.
During the training process, we set the class weights for useful
and useless cuts as 2:1. The ROC curves of the learned cut
classiﬁers with different techniques are shown in Fig. 3. We
usually use the area under the ROC curve to measure how well
a classiﬁer is. The larger the area is, the better the classiﬁer
performs. From the ﬁgure, SVM, LDA, and logistic regression
perform equally well, i.e., the areas under the ROC curves for
all of them achieve 0.92. Thus, we simply use the cut classiﬁer
from SVM in the following test.

As mentioned in Section III.C, we also have adopted ﬁve
popular ML regression techniques to learn the regressor and
used the R-squared scores to evaluate them. We summarize
the R-squared scores for them in Table VII. Generally, re-
gression model with a larger R-squared score is preferred.
Therefore, we use the cut regressor from extra tree in the
following test. From Table VII, linear regression and another
two special linear regression models, i.e., ridge regression
and lasso regression, have very small R-squared scores. The
results suggest that the relations between the features and the
continuous performance indicators are highly non-linear.

After choosing appropriate techniques for the cut classiﬁer
and the cut regressor, respectively, the performance testing

e
g
a
t
n
e
c
r
e
P

1.0

0.8

0.6

0.4

0.2

0.0

10

Single-cut GBD
8-cut GBD
Classification Based GBD
Regression Based GBD

0

20

40

60

80
Iterations

100

120

140

160

(a) Cumulative distribution function of total required iterations.

Single-cut GBD
8-cut GBD
Classification Based GBD
Regression Based GBD

r
e
b
m
u
n

t
u
c

l

e
v
i
t
a
u
m
u
C

140

120

100

80

60

40

20

0

0

20

40

60

80
Iterations

100

120

140

160

(b) Cumulative cut number for relaxed master problems.

Fig. 4. Performance of different GBD algorithms.

0

−50

−100

−150

−200

−250

−300

d
n
u
o
b

r
e
w
o
L
/
r
e
p
p
U

Upper Bound for Single-cut GBD
Upper Bound for 8-cut GBD
Lower Bound for Single-cut GBD
Lower Bound for 8-cut GBD

0

20

40

60
Iterations

80

100

Fig. 5. Convergence performance of single-cut and 8-cut GBD.

results of the proposed method are summarized in Fig. 4
and Table VIII. Speciﬁcally, Fig. 4(a) reﬂects the cumulative
distribution function of the total required iterations. Fig. 4(b)
shows the cumulative number of the cuts added to the relaxed
master problem, which reﬂects the complexity of the relaxed
master problem over the iterations. Table VIII gives the
detailed values of more performance metrics.

Before looking into the performance of the ML based
GBD, we ﬁrst pay attention to the comparison between the
single-cut GBD and the 8-cut GBD. The results in Table
VIII suggest that
the cumulative time for solving relaxed
master problems of multi-cut GBD decreases compared with
single-cut GBD. To ﬁgure out more details, we depict the
convergence performance of the single-cut GBD and the 8-
cut GBD in Fig. 5. From this ﬁgure, we know that the lower
bound of the single-cut GBD, which is related to the relaxed
master problems, increases very slowly. It proves that the main
bottleneck of the single-cut GBD is the time consumed by
solving the relaxed master problems, which accounts for over

 
 
 
 
 
 
S
Needed Iteration Number
Cumulative Cut Number
Average Cut Number
Cumulative Time (s)

TABLE VI
PERFORMANCE OF GBD ALGORITHMS WITH DIFFERENT VALUES OF S
14
20.50
187.39
9.14
1.33

10
21.54
161.72
7.51
1.43

12
20.51
172.73
8.42
1.41

4
28.56
106.57
3.73
2.12

6
25.00
131.49
5.26
1.68

8
23.24
152.37
6.56
1.50

2
43.85
86.68
1.98
2.68

1
73.22
73.22
1.00
2.97

11

16
19.98
197.11
9.86
1.36

20
19.59
218.54
11.16
1.37

TABLE VII
R-SQUARED SCORES FOR DIFFERENT REGRESSION TECHNIQUES

Regression Model
Extra Tree
Linear Regression
Ridge Regression
Lasso Regression
SVM Regression

R-squared Score
0.99
0.08
0.08
0.01
0.81

TABLE VIII
PERFORMANCE OF DIFFERENT GBD ALGORITHMS

Method

Single-cut
GBD
8-cut GBD
Classiﬁcation
Based GBD
Regression
Based GBD

Needed
Iteration
Number

73.22

23.24

23.41

Cut
Number

73.22

152.37

83.49

33.24

92.66

Time (s)

Useful Cut
Recognition
Rate

Useless Cut
Recognition
Rate

2.97

1.50

0.78

0.98

/

/

/

/

99.24%

72.61%

82.35%

80.00%

90% of the total computation time [40]. The multi-cut GBD
can reduce the number of required iterations, i.e., the number
of relaxed master problems need to be solved, as shown in
Fig. 4(a), but it adds more cuts to the relaxed master problems.
Hence, the average complexity of each relaxed master problem
increases as shown in Fig. 4(b). Overall, the multi-cut GBD
can outperform the single-cut GBD by solving fewer but more
complicated relaxed master problems, which suggests that
decreasing the number of the relaxed master problems that
need to be solved plays a more important role in accelerating
GBD than reducing the average complexity of the relaxed
master problems. It also indicates that keeping the useful cuts
is more helpful than discarding the useless cuts, which is the
reason why we set a higher weight to the class of useful cuts
during the training process of the cut classiﬁer.

As for the classiﬁcation based GBD, 99.24% useful cuts
are kept and 72.61% useless cuts are discarded as shown in
Table VIII. Compared with the single-cut GBD, the number
of required iterations decreases by 68.03% while the average
complexity of the relaxed master problem only increases by
14.03%. And the cumulative time for solving master problems
decreases by 73.74%. Compared with the 8-cut GBD, the
average complexity of the relaxed master problem decreases
by 45.21% while the number of required iterations only
increases by 0.73%. And the cumulative time for solving
master problems decreases by 48.00%.

As for the regression based GBD, 82.35% useful cuts are
kept and 80.00% useless cuts are discarded as shown in
Table VIII. Compared with the single-cut GBD, the number
of required iterations decreases by 54.60% while the aver-
age complexity of the relaxed master problem increases by
26.55%. And the cumulative time for solving master problems
decreases by 67.00%. Compared with the 8-cut GBD, the
average complexity of the relaxed master problem decreases
by 39.19% while the number of required iterations increases
by 43.03%. And the cumulative time for solving master
problems decreases by 34.67%.

TABLE IX
COMPARISON BETWEEN CLASSIFICATION AND REGRESSION BASED GBD
Regression Based GBD
Learning continuous
performance indicator

Classiﬁcation Based GBD

Learning binary label

Methodology

Method

Data Collection
Complexity
Useful Cut
Recognition Ability
Useless Cut
Recognition Ability
Optimality
Acceleration Performance

High

Good

Limited

Low

Limited

Good

Globally optimal
Strong

Globally optimal
Good

Compared with the accelerated B&B algorithm in [16], [17],
the proposed accelerated GBD algorithm can always achieve
the global optimum while the accelerated B&B algorithm is
a suboptimal one. On the other hand, from the testing results
on the scenario with 5 CUs and 3 D2D pairs, our proposed
method outperforms the accelerated B&B algorithm in terms
of practical computational complexity. The accelerated GBD
is about twice as fast as the accelerated B&B algorithm.
However, the worst-case complexity of both methods is still
exponential due to the NP-harness of MINLP problems.

D. Comparison Between Classiﬁcation and Regression Based
GBD

In this part, we discuss the advantages and disadvantages of
the two proposed ML based GBD. The detailed comparison
between these two methods is summarized in Table IX. In
terms of methodology, the classiﬁcation based GBD directly
learns the binary labels while the regression based GBD
learns the continuous performance indicators. In addition, the
data collection method of the classiﬁcation based GBD is
actually a variant of single-cut GBD. However, the regression
based GBD uses multi-cut GBD to collect training data. As
mentioned above, the single-cut GBD is more time-consuming
than the multi-cut GBD. Therefore, the data collection method
of classiﬁcation based GBD is more time-consuming than
that of the regression based GBD. Moreover, both methods
can accelerate GBD without loss of optimality. As for the
acceleration performance, the classiﬁcation based GBD per-
forms slightly better than the regression based GBD. There
are two possible reasons. On the one hand, the gap between
the error rates α0 and α1, i.e., |α0−α1|, may be very small. As
mentioned in Section III.B, the existence of the noise will not
inﬂuence the classiﬁcation accuracy if α0 = α1. Therefore,
the cut classiﬁer can tolerate the labeling noise and has good
performance. On the other hand, the cut classiﬁer has a higher
useful cut recognition rate while the cut regressor has a higher
useless cut recognition rate. As mentioned above, keeping
the useful cuts is more helpful than discarding the useless
cuts. Therefore, the classiﬁcation based GBD outperforms the
regression based GBD in terms of acceleration performance.

E. Generalization Ability of Machine Learning Based GBD

the network parameters change.

12

Generalization ability is a very important property of ML
techniques, especially for our proposed method that is ex-
pected to be applicable to various mixed integer wireless
resource allocation problems. In this part, we test the general-
ization ability on three different aspects: size aspect, parameter
aspect, and problem aspect. Because the cut classiﬁer slightly
outperforms the cut regression in terms of acceleration perfor-
mance, we only focus on the generalization ability of the cut
classiﬁer in the following.

1) Generalization Ability on Size Aspect: First, we test the
generalization ability on the size aspect. Speciﬁcally, we test
how the cut classiﬁer learned on small-scale scenario performs
on the testing problem sets with larger-scale scenarios. We
use the cut classiﬁer learned in Section IV.C, and the testing
problem set of each scale includes 50 problem instances. The
results are summarized in Table X. We add three new metrics
to evaluate the generalization ability of the proposed method.
The ﬁrst one is the normalized needed number of iterations
with respect to that of the multi-cut GBD algorithm. If we
can keep all the useful cuts, this metric will be very close to
1. The second one is the normalized cumulative cut number
added to the relaxed master problems with respect to that of
the single-cut GBD algorithm. If we can discard all the useless
cuts, this metric will also be very close to 1. The last one
is the speedup in running time with respect to the multi-cut
GBD algorithm, which corresponds to the ultimate goal of
the proposed method. Note that the worst-case computational
complexity of GBD is exponential, i.e., O(2KL), and thus we
use KL to characterize the problem’s complexity. In Table
X, KL ranges from 15 to 50. The results suggest that the
proposed method has a good generalization ability on the
size aspect for scenarios with KL ≤ 27. When we increase
the problem size over 27, while the useful cut recognition
rate remains excellent, the useless cut recognition rate drops
signiﬁcantly. Speciﬁcally, the useless cut recognition rate drops
to 6.43% for the scenario with 10 CUs and 5 D2D pairs.
In this case, the accelerated GBD keeps almost all the cuts
and performs similarly to the multi-cut GBD. Therefore, the
speedup to multi-cut GBD is quite modest. Overall, the result
suggests that the generalization ability on the size aspect is
limited to a certain range, which is commonly observed for
ML based resource allocation methods [13]–[17], [19]–[21].
2) Generalization Ability on Parameter Aspect: Further-
more, we test
the generalization ability on the parameter
aspect. We still use the cut classiﬁer in Section IV.C but the
D2D network parameters of the testing samples are different
from those in Table V. Speciﬁcally, we change the cell radius
of the D2D network, the maximum transmit power of CU,
and the minimum data rate requirement of CU, respectively.
The testing problem set of each different parameter setting
still includes 50 problem instances and the testing results are
summarized in Table XI. From Table XI, it is clear that the
proposed method also has a strong generalization ability on the
parameter aspect. All the performance metrics remain almost
the same for scenarios with different parameter settings. The
result suggests that we do not need to train a new model while

3) Generalization Ability on Problem Aspect: Finally, we
test the generalization ability on the problem aspect. Speciﬁ-
cally, we aim to test the performance of the proposed method
on problems with different objective functions. Note that the
testing problems with new objective functions still need to
satisfy the convexity and linear separability mentioned in
Section II.B. We still use the cut classiﬁer learned in Section
IV.C. The objective function of the training problems is to
maximize the minimum data rate among all the D2D pairs as
mentioned in Section IV.A. However, the objective functions
of the testing problems are different from that. Speciﬁcally, we
choose two kinds of testing problems. The objective function
of the ﬁrst one is to maximize the sum rate of all the D2D
pairs, i.e.,

max
,pD

{pC

,ρ}

l∈L
X

RD

l (pC, pD, ρ),

and that of the second one is to maximize the weighted sum
rate of all the D2D pairs, i.e.,

max
,pD

{pC

,ρ}

l∈L
X

ωlRD

l (pC, pD, ρ),

where ωl denotes the weight for the l-th D2D pair and is
generally selected according to priority in advance. In this
test, ωl is simply randomly generated according to the uniform
distribution between (0, 1).

During the test, we set the number of CUs, K, as 5, and
that of D2D pairs, L, as 3. All other parameter settings are
the same as those in Table V. We still report the average
performance over 50 testing problem instances and the results
are summarized in Table XII. The results suggest that the
proposed method performs well for scenarios with different
objective functions and has a strong generalization ability on
the problem aspect.

Based on the testing results in Tables X, XI, and XII, we
conclude that our proposed method has limited generalization
ability on the size aspect but good generalization ability on
both the parameter and problem aspects. Therefore, one can
start by solving a small convex problem, and then generalize
it to other similar convex problems with different objective
functions and parameter settings within certain scales. In
this way, one can avoid generating training samples for new
problems, which is part of the overhead mentioned in Section
III.B.

V. CONCLUSION

GBD is a widely used globally optimal algorithm for
MINLP resource allocation problems in wireless networks.
However, a direct
implementation of GBD suffers from
high complexity due to the time-consuming master problems.
Therefore, this paper incorporates ML techniques to accelerate
GBD by decreasing the complexity of the master problems.
Speciﬁcally, we deal with the acceleration task with two
different ML techniques: classiﬁcation and regression. In this
way, a cut classiﬁer and a cut regressor are learned in the
supervised manner, respectively. The learned models can dis-
tinguish between useful and useless cuts, and only useful cuts

TABLE X
GENERALIZATION PERFORMANCE ON LARGER-SCALE SCENARIOS

Testing (K, L)
Useful Cut Recognition Rate
Useless Cut Recognition Rate
Normalized Needed Iteration Number
Normalized Cumulative Cut Number
Speedup to Multi-cut GBD

(5, 3)
99.24%
72.61%
1.00
1.14
1.92x

(9, 2)
99.77%
75.58%
1.00
1.10
1.69x

(6, 4)
99.51%
78.09%
1.07
1.15
1.72x

(7, 3)
99.53%
77.20%
1.00
1.12
1.73x

(9, 3)
99.19%
73.64%
1.15
1.22
1.64x

(8, 4)
99.25%
42.95%
1.05
1.30
1.45x

(10, 5)
99.03%
6.43%
1.03
1.37
1.12x

13

TABLE XI
PERFORMANCE WITH DIFFERENT D2D NETWORK PARAMETERS

Testing
Parameter Setting
Useful Cut
Recognition Rate
Useless Cut
Recognition Rate
Normalized Needed
Iteration Number
Normalized Cumulative
Cut Number
Speedup to
Multi-cut GBD

Original
Parameters

Cell Radius
as 750 m

P C
max as
22 dBm

RC
min as
1.5 bit/s/Hz

99.24%

99.40%

99.93%

98.84%

72.61%

73.21%

72.42%

72.10%

1.00

1.14

1.02

1.15

1.00

1.12

1.00

1.10

1.92x

1.82x

1.69x

1.88x

TABLE XII
PERFORMANCE WITH DIFFERENT OBJECTIVE FUNCTIONS

Objective Function

Max-min
Problem

Maximize
Sum Rate

Useful Cut Recognition Rate
Useless Cut Recognition Rate
Normalized Needed Iteration Number
Normalized Cumulative Cut Number
Speedup to Multi-cut GBD

99.24%
72.61%
1.00
1.14
1.92x

99.82%
78.07%
1.00
1.02
1.88x

Maximize
Weighted
Sum Rate
99.79%
81.40%
1.03
1.01
1.82x

are added to the master problem. Therefore, the complexity
of the master problem decreases and GBD can be further
accelerated. By using a resource allocation problem in D2D
communication networks as an example, we verify that the
proposed method can reduce the computational complexity of
GBD without loss of optimality. Meanwhile, all designs of the
proposed method are invariant for different problems, and thus
it has good generalization abilities on different aspects, which
is a preferred property in wireless networks.

This paper is just a very ﬁrst attempt to accelerate the
solving process of the MINLP resource allocation tasks cir-
cumventing problem-dependent designs. Some open problems
remain for future work. First, for MINLP problems that do
not satisfy the convexity condition, modiﬁcations on GBD
are needed to guarantee optimality. How to generalize our
proposed method to these modiﬁed GBD algorithms is an
interesting problem worth further investigation. Secondly, our
proposed method is implemented in the supervised manner
where labeled training samples are needed. However, labeled
training samples are usually difﬁcult to obtain in wireless
networks. How to decrease the number of needed training
samples is an interesting research problem. Moreover, all
designs in our proposed acceleration method are heuristic,
such as the features proposed in Section III.B. These heuristic
designs will not inﬂuence the optimality of the accelerated
GBD, but there is still room to further speed up the GBD
algorithm. Adding some other problem-independent features
in the proposed method may lead to better performance. On
the other hand, while focusing on a certain resource allocation
problem, some problem-dependent features may help reinforce
the performance. Using Problem (6) as an example, the D2D
/L, can be used
pair activation ratio, deﬁned as
as a problem-dependent feature for the cut generated at the
current solution (pC(i), pD(i), ρ(i−1)). Higher D2D pair acti-

l∈L ρ(i−1)

P

l

vation ratio generally leads to higher overall throughputs. Un-
fortunately, there is no general method to design such problem-
dependent features. Usually trial and error is inevitable to ﬁnd
out the key parameters related to the optimization goal in
the speciﬁc wireless network. Finally, our proposed cut-clean
up strategies are still heuristic. One can consider designing
other effective methods to further accelerate GBD, and even
adopting new ML techniques for wireless MINLP resource
allocation problems.

REFERENCES

[1] Z. Han and K. R. Liu, Resource allocation for wireless networks: Basics,
techniques, and applications. Cambridge, U.K.: Cambridge Univ. Press,
2008

[2] Q. Chen, G. Yu, R. Yin, and G. Y. Li, “Energy-efﬁcient user association
and resource allocation for multistream carrier aggregation,” IEEE Trans.
Veh. Technol., vol. 65, no. 8, pp. 6366–6376, Aug. 2016.

[3] M. Kl¨ugel and W. Kellerer, “Optimal mode selection by cross-layer de-
composition in D2D cellular networks,” IEEE Trans. Wireless Commun.,
vol. 19, no. 4, pp. 2528–2542, Apr. 2020.

[4] C. You, K. Huang, H. Chae, and B. H. Kim, “Energy-efﬁcient resource
allocation for mobile-edge computation ofﬂoading,” IEEE Trans. Wireless
Commun., vol. 16, no. 3, pp. 1397–1411, Mar. 2017.

[5] M. Samir, S. Sharafeddine, C. Assi, T. Nguyen, and A. Ghrayeb, “UAV
trajectory planning for data collection from time-constrained IoT devices,”
IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 34–46, Jan. 2020.
[6] I. Ahmed, A. Ikhlef, R. Schober, and R. Mallik, “Joint power allocation
and relay selection in energy harvesting AF relay systems,” IEEE Wireless
Commun. Lett., vol. 2, no. 2, pp. 239–242, Apr. 2013.

[7] A. H. Land and A. G. Doig, “An automatic method of solving discrete
programming problems,” Econometrica, J. Econometric Soc., vol. 28, no.
3, pp. 497–520, Jul. 1960.

[8] A. M. Geoffrion, “Generalized benders decomposition,” J. Optim. Theory

Applicat., vol. 10, no. 4, pp. 237–260, Apr. 1972.

[9] H. Nguyen, M. Hasegawa, and W. Hwang, “Distributed resource al-
location for D2D communications underlay cellular networks,” IEEE
Commun. Lett., vol. 20, no. 5, pp. 942–945, May 2016.

[10] A. N. Zaki and A. O. Fapojuwo, “Optimal and efﬁcient graph-based
resource allocation algorithms for multiservice frame-based OFDMA
networks,” IEEE Trans. Mobile Comput., vol.10, no. 8, pp. 1175–1186,
Aug. 2011.

[11] X. Wu et al., “FlashLinQ: A synchronous distributed scheduler for peer-
to-peer ad hoc networks,” IEEE/ACM Trans. Netw., vol. 21, no. 4, pp.
1215–1228, Aug. 2013.

[12] Q. Xu et.al., “Energy-efﬁcient resource allocation for heterogeneous
services in OFDMA downlink networks: Systematic perspective,” IEEE
Trans. Veh. Technol., vol. 63, no. 5, pp. 2071–2082, Jun. 2014.

[13] H. Sun et.al., “Learning to optimize: Training deep neural networks for
interference management,” IEEE Trans. Signal Process., vol. 66, no. 20,
pp. 5438–5453, Oct. 2018.

[14] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning based
resource allocation for V2V communications,” IEEE Trans. Veh. Technol.,
vol. 68, no. 4, pp. 3163–3173, Apr. 2019.

[15] K. K. Nguyen et. al., “Non-cooperative energy efﬁcient power allocation
game in D2D communication: A multi-agent deep reinforcement learning
approach,” IEEE Access, vol. 7, pp. 100480–100490, Jul. 2019.

[16] Y. Shen et.al., “LORM: Learning to optimize for resource management
in wireless networks with few training samples,” IEEE Trans. Wireless
Commun., vol. 19, no. 1, pp. 665–679, Jan. 2020.

[17] M. Lee, G. Yu, and G. Y. Li, “Learning to branch: Accelerating resource
allocation in wireless networks,” IEEE Trans. Veh. Technol., vol. 69, no.
1, pp. 958–970, Jan. 2020.

[18] Q. Hu, Y. Cai, Q. Shi, K. Xu, G. Yu, and Z. Ding, “Iterative algorithm
induced deep-unfolding neural networks: Precoding design for multiuser
MIMO systems,” IEEE Trans. Wireless Commun., to be published.

14

[19] W. Cui et.al., “Spatial deep learning for wireless scheduling,” IEEE J.

Sel. Areas Commun., vol. 37, no. 6, pp. 1248–1261, Jun. 2019.

[20] M. Lee et.al., “Graph embedding based wireless link scheduling with

few training samples,” arXiv preprint arXiv:1906.02871, 2019.

[21] N. Zhao et. al., “Deep reinforcement learning for user association and
resource allocation in heterogeneous cellular networks”, IEEE Trans.
Wireless Commun., vol. 18, no. 11, pp. 5141–5152, Nov. 2019.

[22] L. Su et.al., “Computational strategies for improved MINLP algorithms,”

Comput. Chem. Eng., vol. 75, pp. 40–48, Apr. 2015.

[23] R. Rahmaniani et.al., “The Benders decomposition algorithm: A litera-
ture review,” Eur. J. Oper. Res., vol. 259, no. 3, pp. 801–817, Jun. 2017.
[24] H. Jia and S. Shen, “Benders cut classiﬁcation via support vector
machines for solving two-stage stochastic programs,” arXiv preprint
arXiv:1906.05994, 2019.

[25] J. F. Benders, “Partitioning methods for solving mixed variables pro-
gramming problems,” Numer. Math., vol. 4, no.1, pp. 238–252, Dec. 1962.
[26] A. Ruszczy´nski, “Decomposition methods,” Handbooks Oper. Res.

Manag. Sci.. Dordrecht, vol. 10, pp. 141–211, Jan. 2003.

[27] X. Li, Y. Chen, and P. Barton, “Nonconvex generalized Benders de-
composition with piecewise convex relaxations for global optimization of
integrated process design and operation problems,” Ind. Eng. Chemistry
Res., vol. 51, no. 21, pp. 7287–7299, May 2012.

[28] X. Li, A. Tomasgard, and P. I. Barton, “Nonconvex generalized Ben-
ders decomposition for stochastic separable mixed-integer nonlinear pro-
grams,” J. Optimiz. Theory App., vol. 151, no. 3, pp. 425–454, Dec. 2011.
[29] S. M. Frank and S. Rebennack, “Optimal design of mixed ac-dc
distribution systems for commercial buildings: A nonconvex generalized
benders decomposition approach,” Eur. J. Oper. Res., vol. 242, no. 3, pp.
710–729, May 2015.

[30] P. A. Lachenbruch, “Discriminant analysis when the initial samples are
misclassiﬁed II: Non-random misclassiﬁcation models,” Technometrics,
vol. 16, no. 3, pp. 419–424, Aug. 1974.

[31] L. Wang, Support vector machines: Theory and applications. Berlin,

Germany: Springer, Jun. 2005.

[32] S. Balakrishnama and A. Ganapathiraju, “Linear discriminant analysis -

A brief tutorial,” Inst. Signal Inf. Process., pp. 1–8, Mar. 1998.

[33] D. G. Kleinbaum, Logistic regression: A self-learning text. New York:

Springer-Verlag, Aug. 2002.

[34] C. Drummond and R. C. Holte, “C4.5, class imbalance, and cost
sensitivity: Why under-sampling beats over-sampling,” in Proc. Working
Notes ICML Workshop Learn. Imbalanced Data Sets, Washington DC,
2003.

[35] P. Geurts, D. Ernst, and L. Wehenkel, Extremely randomized trees.

Machine learning, Apr. 2006.

[36] G. A. F. Seber and A. J. Lee, Linear regression analysis. New York,

NY, USA: Wiley, 2003.

[37] A. E. Hoerl et. al., “Ridge regression: Some simulations,” Commun.

Statist., vol. 4, no. 2, pp. 105–123, Jan. 1975.

[38] C. Hans, “Bayesian lasso regression,” Biometrika, vol. 96, no. 4, pp.

835–845, Dec. 2009.

[39] S. Glantz and B. Slinker, “Primer of applied regression and analysis of

variance.” McGraw-Hill/Appleton & Lange, Apr. 1990.

[40] T. L. Magnanti and R. T. Wong, “Accelerating Benders decomposition:
Algorithmic enhancement and model selection criteria,” Oper. Res., vol.
29, no. 3, pp. 464–484, Jun. 1981.

[41] A. Ibrahim et.al., “Using Bender’s decomposition for optimal power
control and routing in multihop D2D cellular systems,” IEEE Trans.
Wireless Commun., vol. 18, no. 11, pp. 5050–5064, Nov. 2019.

[42] A. Ibrahim, O. A. Dobre, T. M. N. Ngatched, and A. G. Armada, “Ben-
der’s decomposition for optimization design problems in communication
networks,” IEEE Netw., vol. 34, no. 3, pp. 232–239, May/Jun. 2020.
[43] H. Meshgi, D. Zhao, and R. Zheng, “Optimal resource allocation in
multicast device-to-device communications underlaying LTE networks,”
IEEE Trans. Veh. Technol., vol. 66, no. 9, pp. 8357–8371, Sep. 2017.
[44] R. Ramamonjison et.al., “Joint optimization of clustering and cooper-
ative beamforming in green cognitive wireless networks,” IEEE Trans.
Wireless Commun., vol. 13, no. 2, pp. 982–997, Feb. 2014.

