2
2
0
2

b
e
F
7
2

]

G
L
.
s
c
[

2
v
8
3
5
5
1
.
0
1
1
2
:
v
i
X
r
a

Model Fusion of Heterogeneous Neural Networks via
Cross-Layer Alignment

Dang Nguyen(cid:5) Khai Nguyen† Nhat Ho† Dinh Phung‡ Hung Bui(cid:5)

Monash University‡; VinAI Research(cid:5); University of Texas, Austin†
March 1, 2022

Abstract

Layer-wise model fusion via optimal transport, named OTFusion, applies soft neuron as-
sociation for unifying diﬀerent pre-trained networks to save computational resources. While
enjoying its success, OTFusion requires the input networks to have the same number of lay-
ers. To address this issue, we propose a novel model fusion framework, named CLAFusion,
to fuse neural networks with a diﬀerent number of layers, which we refer to as heterogeneous
neural networks, via cross-layer alignment. The cross-layer alignment problem, which is an
unbalanced assignment problem, can be solved eﬃciently using dynamic programming. Based
on the cross-layer alignment, our framework balances the number of layers of neural networks
before applying layer-wise model fusion. Our experiments indicate that CLAFusion, with an
extra ﬁnetuning process, improves the accuracy of residual networks on CIFAR10 and CIFAR100
datasets. Furthermore, we explore its practical usage for model compression and knowledge
distillation when applying to the teacher-student setting.

1

Introduction

The ubiquitous presence of deep neural networks raises the question, “Can we combine the knowledge
of two and more deep neural networks to improve performance?" As one of the earliest successful
methods, ensemble learning aggregates the output over a collection of trained models, which is
referred to as an ensemble, to improve the generalization ability [10, 37]. However, it is expensive
in terms of computational resources because it requires storing and running many trained neural
networks during inference. To overcome the previous hardness, we need to ﬁnd a single neural
network that can inherit the knowledge from multiple pre-trained neural networks and have a small
size at the same time. To our best knowledge, there are two popular approaches for this purpose:
knowledge distillation and model fusion.

The ﬁrst approach, knowledge distillation, is a machine learning technique that transfers knowledge
from large networks (teachers) into a smaller one (student) [12]. The smaller network is trained
by the task-speciﬁc loss and additional distillation losses that encourage the student network to
mimic the prediction of the teachers. Knowledge distillation is well-known as an eﬃcient model
compression and acceleration technique [9]. The distillation process, however, is computationally
expensive because it runs forward inference for both teacher and student networks at each training
epoch. In addition, simply distilling into a randomly initialized student network, in most cases, does
not yield high performance, thus a good initialization of the student network is needed [30].

The second line of work is called model fusion which is the problem of merging a collection of
pre-trained networks into a uniﬁed network [33, 26]. The simplest technique in model fusion is
vanilla averaging [31, 27], which computes the weighted average of pre-trained network parameters

1

 
 
 
 
 
 
without the need for retraining. Their methods fuse neural networks with the same architecture
without considering the permutation invariance nature of neural networks. To mitigate this problem,
the idea of solving a neuron alignment problem before applying weight averaging is discussed
in [1, 21]. Recently, there are two concurrent works that beneﬁt from the idea of neuron alignment.
FedMA [33] formulated and solved the assignment problem to ﬁnd a permutation matrix that undoes
the permutation of weight matrices. While OTFusion [26] viewed the problem through the lens of
optimal transport [22, 14] and utilized the transport map to align the weight matrices. Though both
methods can fuse multiple neural networks, their applications are limited to networks with the same
number of layers.

Contributions. In this paper, we propose a model fusion framework for fusing heterogeneous neural
networks, namely, unequal width and unequal depth neural networks, which is named as Cross-Layer
Alignment Fusion (CLAFusion). CLAFusion consists of three parts: cross-layer alignment, layer
balancing method, and layer-wise model fusion method. In summary, our main contributions are
two-fold:

1. We formulate the cross-layer alignment as an assignment problem using layer representation and
layer similarity, then propose a dynamic programming-based algorithm to solve it. Next, we present
two natural and fast methods to balance the number of layers between two networks. Furthermore,
we discuss the application of our framework for the cases of more than two networks.

2. Our experiments demonstrate the eﬃciency of CLAFusion on diﬀerent setups. The fused model
from CLAFusion serves as an eﬃcient initialization when training residual networks. In addition,
the components of CLAFusion can be successfully applied to the heterogeneous model transfer task.
Moreover, CLAFusion shows potential applications for model compression and knowledge distillation
in the teacher-student setting.

2 Background

In this section, we ﬁrst recall the layer representation and layer similarity that have been widely
used in applications. Then, we review available model fusion methods and their limitations. Finally,
we discuss OTFusion and the challenge of fusing heterogeneous neural networks.

2.1 Layer Representation and Layer Similarity

A common layer representation is the matrix of activations [16]. Activation matrix, also known as
activation map, can be used as a type of knowledge to guide the training in knowledge distillation [9].
Another choice is the weight matrix of the trained neural networks, which has been shown to perform
well on intra-network comparison tasks [23].

The neural network exhibits the same output when its neurons in each layer are permuted, this
is so-called the permutation invariance nature of neural network parameters. Due to the permu-
tation invariance property, a meaningful layer similarity index should be invariant to orthogonal
transformations [16]. Their proposed Centered Kernel Alignment (CKA) eﬀectively identiﬁes the
relationship between layers of diﬀerent architectures.

2

2.2 Model fusion

Despite showing good empirical results, vanilla averaging [31, 27] only works in the case when the
weights of individual networks are relatively close in the weight space. As an eﬀective remedy, several
works on model fusion considered performing permutation of neurons in hidden layers before applying
vanilla averaging. FBA-Wagging [1], Elastic Weight Consolidation [20], and FedMA [33] formulate
the neuron association problem as an assignment problem and align the neurons. Nevertheless, those
variants are not generalized to heterogeneous neural networks.

There is limited literature devoted to the discussion of fusing heterogeneous neural networks.
NeuralMerger [5] is one of the few attempts to deal with this setting. However, there are two key
diﬀerences in NeuralMerger from our CLAFusion. First, their cross-layer alignment is hand-crafted
and dedicated to speciﬁc architectures. Secondly, they decompose weights into lower dimensions and
use vector quantization to merge the weights. On the other hand, we introduce a systematic way to
solve the cross-layer alignment problem, and combining weights is done using the layer-wise model
fusion method.

2.3 Model fusion via Optimal Transport

Recently, [26] proposed OTFusion, which solves the neuron association problem based on free-
support barycenters [6]. OTFusion leads to a noticeable improvement over vanilla averaging when
all individual networks have the same architecture. One advantage of OTFusion over other vanilla
averaging-based model fusion methods is that it can fuse networks with diﬀerent widths (i.e., number
of neurons) thanks to the nature of optimal transport.

Issues of OTFusion. OTFusion, however, is still a layer-wise approach that cannot be directly
employed for heterogeneous neural networks. We need to ﬁnd two corresponding layers, matching
their neurons before averaging weight matrices. There are two challenges to this scheme. Firstly,
the one-to-one mapping between layers is not available in advance. A naive one-to-one mapping of
layers with the same index as the layer-wise approach may cause a mismatch when the numbers of
layers are diﬀerent. For example, we analyze two VGG conﬁgurations [25, Table 1] with diﬀerent
depths: VGG11 and VGG13. The second convolution layer of VGG11 has 128 channels and an input
image of size 112 × 112. While that of VGG13 has 64 channels and an input image of size 224 × 224.
Secondly, assume that we have successfully found a one-to-one mapping, there still necessitates
a special treatment for remaining layers that has no counterparts. Simply removing those layers
reduces the performance of the deeper network, this may cumulatively degrade the fused model.

3 Cross-layer Alignment Model Fusion

To overcome the challenges of OTFusion, in this section, we propose Cross-layer Alignment Model
Fusion (in short, CLAFusion) framework for fusing heterogeneous neural networks.

n
(cid:74)

as a set of integers {1, . . . , n} and In as an identity matrix of size n × n.
Notation. We deﬁne
We use A, B to denote the individual models and F to denote the fused model. l indicates the layer
index. Layer l of model A has p(l)
A . The pre-activation and
activation vector at layer l of model A are denoted as z(l)
A , respectively. The weight
matrix between layers l and l − 1 of model A is W (l,l−1)
. The following equations hold between two

A neurons and an activation function f (l)

A ∈ Rp(l)

A , x(l)

(cid:75)

A

3

Figure 1: CLAFusion strategy:
In the ﬁrst step, the cross-layer alignment problem is solved for two pre-trained
models. Two corresponding layers are surrounded by two rounded rectangles of the same color. Based on the optimal
mapping obtained in the previous step, CLAFusion balances the number of layers (adding layers in this ﬁgure). The
red circles and lines indicate the newly added neurons and weights (Zero weights are omitted). Finally, CLAFusion
applies a layer-wise model fusion method to produce the ﬁnal output.

consecutive layers of model A (we omit the bias terms here).

x(l) = f (l)

A (z(l)

A ) = f (l)

A (W (l,l−1)

A

x(l−1)).

(1)

We stack the pre-activation vector over t samples to form a t−row pre-activation matrix. Let
B ∈ Rt×p(l)
A ∈ Rt×p(l)
Z(l)
A denote a matrix of pre-activations at layer l of model A for t samples, and Z(l)
B
denote a matrix of pre-activations at layer l of model B for the same t samples. The activation
A , X (l)
matrices X (l)
B are obtained by applying the corresponding activation functions element-wise to
the pre-activation matrices.

Problem setting. Hereafter, we consider fusing two feed-forward networks A and B of the same
architecture family. Two networks have the same input and output dimension but a diﬀerent number
of hidden layers. In each network, the hidden layer has an activation function of ReLU, which is the
general setting in modern architectures. Additional work is required to handle bias terms and the
batch normalization layer properly so we leave them for future work. Let m and n be the number of
hidden layers of models A and B, respectively. Taking the input and output layers into account, the
numbers of layers are m + 2 and n + 2, respectively. Without loss of generality, assume that m ≥ n.
The layer index of model A and B are {0, 1, . . . , m + 1} and {0, 1, . . . , n + 1}, respectively.

General strategy. Our framework has three components, which are illustrated in Figure 1. The
ﬁrst part is a cross-layer alignment which is a one-to-one mapping from hidden layers of model B to
hidden layers of model A. The second part is a layer balancing method that equalizes the number of
layers of two models based on the cross-layer alignment. The last part is a layer-wise model fusion
method that is applied to same-sized models. The third part of our framework adopts any model
fusion methods that can fuse two neural networks with the same number of layers.

3.1 Cross-layer alignment

Cross-layer alignment (CLA) is a nontrivial problem that also arises in the context of the feature-
based knowledge distillation approach [35, 29, 3]. CLA requires ﬁnding a one-to-one function that
matches similar layers between two models. Despite considering CLA, prior works are based on the
hand-crafted layer association, which causes a lack of generalization. To address this matter, we

4

Fused modelCross-layeralignmentLayerbalancingLayer-wisemodel fusionInputOutputModel AModel BModel AModel BModel AModel BFigure 2: Cross-layer alignment example: Two pre-trained neural networks are given in (a). Model A has 4
hidden layers of size 5, 4, 3, 1 while model B has 3 hidden layers of size 3, 2, 1. (b) shows the cost matrix (squared
diﬀerence) between layer representations (number of neurons) for hidden layers of two networks. Three color cells
represent the solution of the CLA problem. Note that the upper-left and lower-right cells are automatically chosen by
the constraints on the ﬁrst and last hidden layers. (c) visualizes the optimal mapping. Two rounded rectangles of the
same colors represent two corresponding layers in the optimal CLA.

introduce an eﬃcient method for solving the CLA problem. Because the input and output layers of
the two models are identical in both dimension and functionality, we only align their hidden layers.
In addition, the ﬁrst and last hidden layers usually play a critical role in the model performance [36].
Therefore, we directly match the ﬁrst and last hidden layers of the two models. In case n = 1, we
prioritize the ﬁrst layer over the last layer. The importance of this constraint in CLA is studied in
Appendix B. Furthermore, two networks are feed-forward, so the mapping is necessary to keep the
sequential order of layers. All in all, we formulate the CLA problem as follows.

A , . . . , L(m)

Deﬁnition 1. Assume that we have two layer representations for hidden layers of two models as,
B , . . . , L(n)
LA = {L(1)
B }. Let C ∈ Rm×n denote the cost matrix between
them, i.e., Ci,j = d(L(i)
where d is a layer dissimilarity. The optimal CLA
, j ∈
m
(cid:75)
(cid:75)
(cid:74)
is a strictly increasing mapping a :
satisfying a(1) = 1, a(n) = m and minimizing
m
n
S(n, m) = (cid:80)n
(cid:75)
(cid:74)
(cid:74)
i=1 Ca(i),i.

A } and LB = {L(1)
B ), i ∈

A , L(j)

n
(cid:74)
(cid:55)→

(cid:75)

Discussion. The above problem is a special case of the (linear) unbalanced assignment problem [24].
The condition of strictly increasing ensures the sequential order of layers when matching. If we
have two increasing layer representations in 1-D, the problem can be interpreted as the Partial
Optimal Transport in 1-D problem with uniform weights. It can be solved eﬃciently in O(mn) using
the proposed method in [2]. However, the increasing constraint on layer representations is quite
strict and layer representations might not be in 1-D in general. On the other hand, the unbalanced
assignment problem can be solved using the generalization of the Hungarian algorithm [18]. The
time complexity of the Hungarian algorithm in our CLA problem is O(mn2). The importance of
CLA in our framework is demonstrated in Appendix B where we compare the performance of the
fused model obtained using the optimal mapping with those from other mappings.

Cross-layer alignment algorithm. We propose an eﬃcient algorithm based on dynamic
programming to solve the CLA problem. Algorithm 1 details the pseudo-code for our algorithm.
Given the cost matrix, its time complexity is only O(mn). Note that the optimal alignment from
Deﬁnition 1 is not necessarily unique so we choose one which is obtained by backtracking.

5

14914Model AModel BInput1234Output41694400123412301(a) Pretrained models(b) Cost matrix(c) Optimal mappingModel AModel BAlgorithm 1 Cross-layer alignment algorithm

Input: C = [Ci,j]i,j
S(i, i) ← (cid:80)i
l=1 Cl,l, i ∈
m
S(0, j) ← 0, j ∈
(cid:75)
(cid:74)
for i = 1 to n do

n
(cid:74)

(cid:75)

and S(i, 0) ← 0, i ∈

n
(cid:74)

(cid:75)

for j = i + 1 to m do

S(i, j) ← min{S(i, j − 1), S(i − 1, j − 1) + Cj,i}

end for

end for
a(1) ← 1; a(n) ← m; i ← n − 1; j ← m − 1
while i ≥ 2 do

while j ≥ i + 1 and S(i, j) = S(i, j − 1) do

j ← j − 1
end while
a(i) = j; i ← i − 1; j ← j − 1

end while
Output: a

Layer representation. An appropriate representation for hidden layers is the critical element in
the CLA problem. We list three possible layer representations for hidden layers of two models. The
ﬁrst representation is the number of neurons in each layer. This encourages two layers that have
a similar number of neurons to match with each other. The second choice is the (pre-)activation
matrix which is widely used for cross-layer comparison. The third alternative is the weight matrix of
the pre-trained model. Next, we discuss the choice of the cost function (layer dissimilarity). For
the ﬁrst representation, the squared diﬀerence is used. To measure the discrepancy between two
(pre-)activation matrices we choose the dissimilarity index based on CKA while another option is
the Wasserstein distance. Finally, the cosine distance, Kullback–Leibler divergence, and Wasserstein
distance can be utilized in the weight matrix representation [23].

CLA for Convolutional neural network (CNN). Following NeuralMerger, we do not match
fully connected layers and convolution layers. Naturally, a convolution layer has diﬀerent functionality
from a fully connected layer. In addition, there is a lack of an appropriate way to combine the weights
of a fully connected layer and a convolution layer. In this paper, we consider CNN architecture which
consists of consecutive convolution layers followed by fully connected layers such as VGG [25] and
RESNET [11]. The key idea is to solve the CLA for same-type layers separately then combine two
mappings. We further break the convolution part into smaller groups and ﬁnd the mapping for each
pair of groups. In particular, we divide into 5 groups separated by the max-pooling layers in VGG.
In RESNET, we group consecutive blocks with the same number of channels, a.k.a. stage, and in
each stage, align blocks instead of layers. For the block representation, we choose the (pre-)activation
matrix of the second convolution layer in each block.

3.2 Layer balancing methods

As mentioned above, some layers in model A may have no counterparts in model B in the optimal
mapping. Naturally, we have two opposite directions: reduce layers in model A or add layers into

6

Figure 3: Layer balancing examples:
In (a), a new layer is added between hidden layers 1 and 2 of model B.
Newly added neurons and weights are in red (We omit zero weights). The new weight matrix between layers 1 and 2
of model B is an identity matrix whose size equals the number of neurons at layer 1. While the old weight matrix
between layers 1 and 2 in model B becomes the new weight matrix between layers 2 and 3. In (b), hidden layer 2
of model A is removed. The red lines indicate the new weight matrix which is calculated based on two old weight
matrices and the activation matrix of the removed layer.

model B. Assume that we have already balanced the number of layers up to layer l of model B. At
layer l + 1 of model B, there are two possibilities. If a(l + 1) − a(l) = 1, the layer-wise fusion method
is ready to apply up to layer l + 1. If a(l + 1) − a(l) > 1, we can either merge layers between layers
a(l) and a(l + 1) of model A or add layers between layers l and l + 1 of model B. Next, we discuss
two methods in the case a(l + 1) − a(l) = 2. When a(l + 1) − a(l) > 2, we can repeat the same
method a(l + 1) − a(l) − 1 times.

Add layers into model B. We add a layer l(cid:48) between layers l and l + 1 of model B. The new
weight matrices are deﬁned as W (l(cid:48),l)
B . The new
activation function is deﬁned as f (l(cid:48))
from Equation 1 we have

and W (l+1,l(cid:48))
B
, which is ReLU. Because x(l)

B ← I
B ← f (a(l)+1)

B (cid:23) 0 for all l ∈

← W (l+1,l)
B

∈ Rp(l+1)

B ×p(l)

n
(cid:74)

p(l)
B

,
(cid:75)

A

B = f (l(cid:48))
x(l(cid:48))

B (W (l(cid:48),l)

B x(l)

B ) = ReLU (x(l)

B ) = x(l)
B .

Therefore, the information of model B remains unchanged after adding layer l(cid:48). Note that the new
layer is just an identity mapping, which is a trick that has been used in RESNET and NET2NET [4].
Also discussed in NET2NET, the adding layers method is a function-preserving transformation that
allows us to generate a valuable initialization for training a larger network. We later examine this
hypothesis in our experiments.

Merge layers in model A. We merge layer a(l) + 1 into layer a(l) of model A by directly
connecting layer a(l) to layer a(l + 1). Because f (a(l)+1)
is ReLU, the new weight matrix can be
written as

A

W (a(l+1),a(l))

= W (a(l+1),a(l)+1)

A

D(a(l)+1)

A

W (a(l)+1,a(l))

A

,

×p(a(l)+1)

A

where D(a(l)+1)
is an input-dependent diagonal matrix with 0s and 1s on its
diagonal. The ith entry in the diagonal has a value of 1 if the ith entry of z(a(l)+1)
is positive and 0
otherwise. Because the actual sign of neuron i at layer a(l) + 1 varies by the input, we provide a
has a pre-activation vector over t samples
simple estimation. Given that the neuron i ∈

A

A

A
∈ Rp(a(l)+1)

A

p(a(l)+1)
A
(cid:74)

(cid:75)

7

(a) Adding layers(b) Merging layers3Model AModel BModel AModel BInput12Output4as z.,i ∈ Rt, which is the ith column of the pre-activation matrix Z(a(l)+1)
neuron i using either sign of sum (sgn (cid:80)t
have positive activation values, 0 otherwise).

. We estimate the sign of
j=1 zj,i) or sign of majority (i.e., 1 if at least t/2 samples

A

Pros and cons of balance methods. An advantage of merging layers is that the fused model
has fewer layers. However, merging layers degrades the accuracy of model A and it is slower than
adding layers method that does not involve any complex calculations. On the ﬂip side, adding
layers does not aﬀect the accuracy of model B but results in a deeper fused model. Surprisingly,
merging layers shows comparatively better performance than adding layers in our experiments on the
multilayer perceptron (MLP). Performance comparison between the two methods will be provided in
Appendix B and C.1. Note that we can use more sophisticated model compression methods instead
of the merging layers method. Similarly, it is possible to replace the adding layers method with a
more advanced network expanding technique [34]. Here, we just introduce two natural and fast ways
to demonstrate our framework.

Balancing the number of layers for CNN. The same merging method does not work in the
case of CNN. On the other hand, the adding layers method can be easily applied for convolution
layers. For VGG, we can set all ﬁlters of a new convolution layer to identity kernels. For RESNET,
we add a new block in which all ﬁlters of two convolution layers become zero kernels while the
short-cut connection remains as the identity mapping.

3.3 Extension to multiple neural networks

Solving the CLA problem for multiple neural networks is a non-trivial task. It can be formulated as a
multi-index assignment problem [28, 13]. In addition, it has some connections to the multi-marginal
optimal partial optimal transport [7, 15]. Therefore, it is an interesting direction for our future work.
Here we only propose a simple yet eﬀective approach to extend CLAFusion to the case of multiple
networks. Consider K pre-trained models {Mi}K
k=1. Our approach is similar to what they did in
OTFusion [26, Section 4]. Starting with an estimation of the fused model MF , we apply the ﬁrst
and second parts of CLAFusion K times to depth-align K pre-trained models with respect to the
fused model. After that, we apply OTFusion to those depth-aligned networks to produce the ﬁnal
weights for the fused model. The choice of MF plays an important role in this approach. However,
it is unclear how it is chosen in OTFusion. In our experiments, the network with the most number
of layers is chosen as the initialization for MF .

4 Experiments

In this section, we showcase the practical usage of CLAFusion in diﬀerent setups. We ﬁrst
Outline.
fuse a RESNET34 and a RESNET18 trained on CIFAR10 and CIFAR100 datasets [17]. Secondly,
we incorporate CLA into the framework of heterogeneous model transfer to boost the performance.
Finally, our method is applied to the teacher-student setting in which VGG architectures are utilized.
In all experiments, OTFusion is adopted as the layer-wise model fusion in the third step of our
framework. The detailed settings including training hyperparameters, used assets, and computational
resources are deferred to Appendix A. We further conduct ablation studies to validate the eﬀectiveness
of cross-layer alignment and compare two layer balancing methods in Appendix B. In Appendix C,
we provide additional experimental results including skill transfer on the synthetic dataset and

8

Table 1: The results of fusing and ﬁnetuning RESNET34 (MA) and RESNET18 (MB) on CIFAR10
and CIFAR100 datasets. Each entity shows the average classiﬁcation accuracy over 5 seeds. The
best performing initialization in each row is in bold. The detailed results are reported in Table 14.

Dataset

MA MB

Ensemble MF

Finetune

Learning

MA MB

MF MB depth-aligned Random RESNET34

CIFAR10
CIFAR100

93.31
65.93

92.92
65.33

93.81
68.49

65.72
27.93

93.52
66.87

93.29 93.67
66.19 67.59

93.24
66.48

92.00
64.59

Table 2: The results of fusing and ﬁnetuning more than 2 residual networks on CIFAR10 and
CIFAR100 datasets. Pre-trained models are alternately RESNET34 and RESNET18. Each entity
shows the average classiﬁcation accuracy over 5 seeds. The best performing initialization in each
row is in bold. The detailed results are reported in Tables 15 and 16.

Dataset

Pre-trained models

Ensemble MF

Finetune

CIFAR10

93.31, 92.92, 93.16, 92.83
93.31, 92.92, 93.16, 92.83, 93.18, 92.86

CIFAR100

65.93, 65.33, 65.97, 65.28
65.93, 65.33, 65.97, 65.28, 65.89, 65.30

94.17
94.22

70.27
70.62

31.76
23.75

93.52, 93.29, 93.41, 93.20
93.76
93.52, 93.29, 93.41, 93.20, 93.49, 93.24 93.78

8.47
4.39

66.87, 66.19, 66.77, 65.96
68.92
66.87, 66.19, 66.77, 65.96, 66.56, 65.92 69.40

Learning

Pre-trained models

MF

supplementary results for experiments in the main text as well as highlight the computational
advantage of CLAFusion.

4.1 Generate an eﬃcient initialization

Following the experiments conducted in [26, Section 5.3], CLAFusion can be used
Settings.
to generate an initialization when training the larger neural network. We apply CLAFusion to
pre-trained RESNET34 and RESNET18 on CIFAR10 and CIFAR100 datasets. Since our purpose is
not to obtain state-of-the-art performance, we use the same architecture for both datasets. Layer
representation is the pre-activation matrix of 200 samples while the cost function is calculated by
subtracting the linear CKA from 1. After fusing, we also perform ﬁnetuning to improve the accuracy
as observed in previous model fusion works [5, 26]. The ﬁnetuning hyperparameters, which are
adopted from [26, Appendix S3.1.3], are reported in Table 7.

Baselines. We use NET2NET to generate a RESNET34 model, referred to as MB depth-aligned,
which preserves the performance of MB. To compare with our method, we ﬁnetune the pre-trained
models, MB depth-aligned, and a RESNET34 from scratch. As a reference, we report the result of
ensemble learning that calculates the average predictions over all individual models.

Results. As can be seen from Table 1 retraining from the fused model gains accuracies of 93.67
and 67.59, which are the highest among all initializations on each dataset. This demonstrates the
advantage of CLAFusion over NET2NET when initializing a large model from a pre-trained small
model. Because our method allows combining the knowledge from the pre-trained large model to
generate a better initialization, rather than solely relying on the knowledge of the small model.
Although ensemble learning achieves higher performance, it comes at the cost of a nearly 50%
increase in both memory and inference time according to Table 20.

9

Table 3: The results of heterogeneous model transfer from RESNET18 to RESNET34. Each entity
shows the average classiﬁcation accuracy ± standard deviation over 5 seeds. The best performing
method in each row is in bold. The detailed results can be found in Table 18.

Dataset

CIFAR10

Method

HMT

HMT + Naive avg HMT + OTFusion HMT + CLA + OTFusion

Transfer
Transfer + Finetune

10.79 ± 1.01
93.10 ± 0.18

CIFAR100

Transfer
Transfer + Finetune

2.70 ± 0.45
65.15 ± 0.24

37.49 ± 4.59
93.38 ± 0.19

10.79 ± 1.36
66.42 ± 0.25

18.32 ± 1.76
93.66 ± 0.14

5.14 ± 1.22
67.23 ± 0.16

61.29 ± 3.69
93.86 ± 0.14

12.56 ± 2.79
67.83 ± 0.21

More than 2 models. We consider the same settings as in the 2-model scenario but fuse 4 or 6
models instead. It can be seen clearly from Table 2 that the fused model, after ﬁnetuning, consistently
yields the best performance on both datasets. On CIFAR100, for example, the classiﬁcation accuracy
increases remarkably over ﬁnetuning pre-train models by a margin of at least 2.05 and 2.53 for the
cases of 4 and 6 models, respectively. In terms of computational resources, the gap between our
model fusion method and ensemble learning is pushed even further. Table 20 shows that ensemble
learning uses approximately 3 and 4.5 times as many resources as CLAFusion when combining 4 and
6 pre-trained RESNET models, respectively.

4.1.1 Heterogeneous model transfer

Settings.
Heterogeneous model transfer [32] is a branch of model transfer that deals with
heterogeneous neural networks. It is contrasted to homogeneous model transfer, also known as
transfer learning, in which the same network architecture is used in both the pre-training and
ﬁnetuning phase. In this experiment, we apply CLA and OTFusion to the heterogeneous model
transfer method (in short, HMT). We transfer the pre-trained RESNET18 to the pre-trained
RESNET34 using diﬀerent methods, then ﬁnetune and compare their performance. We use the same
pre-trained models, layer representation, layer dissimilarity, and ﬁnetuning hyperparameters as in
Section 4.1.

Baselines. First, the heterogeneous model transfer method (HMT) in the original paper [32] is
used. Model parameters of the pre-trained RESNET18 are transferred to the pre-trained RESNET34.
The weights that are not transferred are set to the weights of the pre-trained RESNET34. In the
second method (HMT + Naive avg), after transferring we apply naive averaging to the transferred
RESNET34, which is the result of HMT, and the pre-trained RESNET34. Naive averaging is replaced
with OTFusion in the third method (HMT + OTFusion). Diﬀerent from the third method, the last
method utilizes the optimal mapping from the CLA problem instead of the longest chain proposed
in [32] for the layer-to-layer transfer.

Results. Table 3 demonstrates the average performance of diﬀerent transfer methods. Using
CLA improves the performance over the longest chain substantially, with a signiﬁcant gap of 42.97
and 7.42 between the third and fourth methods on CIFAR10 and CIFAR100 datasets, respectively.
After ﬁnetuning, the combination of CLA and OTFusion leads to an improvement of 0.76 and 2.68
compared to using HMT only on CIFAR10 and CIFAR100 datasets, respectively.

10

Table 4: The results of fusing teacher and student VGG models on the CIFAR10 dataset. The best
performing student model is in bold. More results can be found in Table 17.

# Params

Teacher

Students

Finetune

(MA, MB, MF )

MA

MB MF MA MB

MF MB depth-aligned

(33M, 3M, 3M)

92.70

89.92

82.66

92.65

89.89 90.96

90.84

Table 5: Knowledge distillation from teacher MA into student model MB. The "Best“ row shows the
best performance for all combinations of hyperparameters. The "Average" row illustrates the mean
± standard deviation of the best accuracies obtained at diﬀerent temperatures. The best performing
initialization in each row is in bold. The detailed results are reported in Table 19.

Distillation initialization

Random MB

Best
Average

89.10
88.41 ± 0.77

MB

90.98

MF

91.43

90.73 ± 0.43 91.16 ± 0.25

MB depth-aligned Random MF

91.24
91.09 ± 0.19

88.84
88.10 ± 0.68

4.2 Teacher-student fusion

Settings.
In this experiment, we transfer the knowledge from the pre-trained teacher model to
the student model. We train two VGG models on the CIFAR10 dataset: model A has VGG13
architecture with a double number of channels at each layer while model B has VGG11 architecture
with a half number of channels at each layer. After fusing the teacher and student models, we
retrain the fused model and compare it to the retraining result of the student model. The layer
measure and the layer dissimilarity are identical to Section 4.1. We adopt the best hyperparameters
as reported in [26, Appendix S11] for retraining. All ﬁnetuning hyperparameters are reported in
Table 8. Additional experiments with teacher and student RESNET models on CIFAR100 are given
in Appendix C.4.

Results. The classiﬁcation accuracies are summarized in Table 4. After retraining, the fused model
yields better performance than retraining other student models. The compression ratio between the
fused model and the teacher is about 11 at a cost of reducing 1.74% accuracy. This suggests that
CLAFusion can act as a network pruning method to compress a heavy model into a lightweight one.

4.2.1 Knowledge distillation

Settings.
It has been shown that pre-trained distillation [30], which performs knowledge distillation
from pre-trained large teacher models to a pre-trained student model, outperforms both knowledge
distillation to a randomly initialized student and the conventional pre-training + ﬁnetuning scheme.
As discussed in OTFusion, the fused model can be used as an eﬃcient initialization for knowledge
distillation. To examine the same application of CLAFusion, we perform knowledge distillation to
the above pre-trained VGG models. We employ the method in [12] to match the logit distribution of
the student model to that of the teacher model. For initialization, we consider ﬁve diﬀerent choices
for the student model: (a) randomly initialized MB, (b) MB, (c) MF , (d) MB depth-aligned, (e)
randomly initialized MF . We sweep over a set of hyperparameters to choose the best combination
that maximizes the accuracy of the student model. The sets of hyperparameters for distillation also

11

follows the setting in [26, Appendix S12]: temperature T = {20, 10, 8, 4, 1} and loss-weight factor
γ = {0.05, 0.1, 0.5, 0.7, 0.95, 0.99}. For a fair comparison, the hyperparameters for training student
models are identical to ﬁnetune hyperparameters in Section 4.2.

Results. The results for knowledge distillation are reported in Table 5. Using the fused model as an
initialization achieves the best performance among diﬀerent choices of initializations. It showcases
the application of CLAFusion as an eﬃcient initialization for knowledge distillation. If averaging over
diﬀerent temperatures, retraining the fused model (90.96) works better than pre-trained distillation
(90.73). Even the best accuracy of pre-trained distillation into the pre-trained student model (90.98)
is slightly higher than that of retraining the fused model but comes with the cost of hyperparameter
tuning. Moreover, as reported in Table 21, an epoch in pre-trained distillation takes 12.22 seconds
while that of ﬁnetuning only costs 7.48 seconds, which is around 1.63x speedup. It further strengthens
the claim that CLAFusion in combination with ﬁnetuning can act as a reliable model compression
method.

5 Conclusion

In the paper, we have presented a framework for model fusion. Our CLAFusion extends layer-
wise model fusion methods to the setting of heterogeneous neural networks by solving a cross-layer
alignment problem, followed by a layer balancing step. Finetuning the fused network from CLAFusion
achieves a better accuracy for RESNET trained on CIFAR10 and CIFAR100 datasets. In addition,
CLAFusion can be incorporated into the heterogeneous model transfer framework to improve
performance. Furthermore, it shows potential applications for model compression and knowledge
distillation.

Supplement to “Model Fusion of Heterogeneous Neural Networks
via Cross-Layer Alignment”

In this supplementary material, we ﬁrst present the common settings, used assets, and computational
resources in Appendix A. Next, we illustrate the eﬃciency of CLA as well as the importance of
constraints on the ﬁrst and last layers in Appendix B. Finally, we provide additional results of our
experiments in Appendix C.

A Experiment settings

In all experiments, we use OTFusion to fuse same-length models after aligning and balancing in
the ﬁrst two steps of CLAFusion. Following the same alignment strategy in [26, Appendix S1],
OTFusion is computed using the activation-based alignment strategy in which the pre-activation
matrix instead of the activation matrix is used for the neuron measure. The hyperparameters for
pre-trained models are summarized in Table 6. Without any further speciﬁcation, the random
seed for training or retraining is set to its default value. The accuracy of pre-trained MLP is the
accuracy of the last epoch. While the accuracies of VGG and RESNET are reported as the best
performing checkpoint. For all ﬁnetuning experiments, we always chose the best record among all
epochs. The hyperparameters for ﬁnetuning RESNET and VGG models are reported in Tables 7
and 8, respectively.

12

Table 6: The hyperparameters for training diﬀerent network architectures.

Number of epochs
Training batch size
Test batch size
Optimizer
Initial LR
Momentum
Weight decay
LR decay factor
LR decay epochs
Default seed
5 seeds

MLP

10
64
1000
SGD
0.01
0.5

0
0,1,2,3,4

VGG

ResNet

300
128
1000
SGD
0.05
0.9
0.0005
2
30,60,. . .,270
42
40,41,42,43,44

300
256
1000
SGD
0.1
0.9
0.0001
10
150,250
42
40,41,42,43,44

Table 7: Finetuning RESNET hyperparameters.

Table 8: Finetuning VGG hyperparameters.

Number of epochs
Training batch size
Test batch size
Optimizer
Initial LR
Momentum
Weight decay
LR decay factor
LR decay epochs

120
256
1000
SGD
0.1
0.9
0.0001
2
20,40,60,80,100

Number of epochs
Training batch size
Test batch size
Optimizer
Initial LR
Momentum
Weight decay
LR decay factor
LR decay epochs

120
128
1000
SGD
0.01
0.9
0.0005
2
20,40,60,80,100

Open source code. We adapt the oﬃcial implementation of OTFusion1 [26] in our implementation.
For computing the optimal transport, we use Python Optimal Transport (POT) library2 [8]. For
model transfer, the source code of the original paper3 [32] is utilized.

Neural network architectures. Abusing the notation in VGG’s paper [25], we represent several
CNN architectures used in the experiments as follows.
VGG13 doub: Input layer → conv3-64 → conv3-128 → maxpool → conv3-256 → conv3-256 →
maxpool → conv3-512 → conv3-512 → maxpool → conv3-1028 → conv3-1028 → maxpool →
conv3-1028 → conv3-512 → maxpool → Output layer

VGG11 half :
Input layer → conv3-64 → maxpool → conv3-64 → maxpool → conv3-128 →
conv3-128 → maxpool → conv3-256 → conv3-256 → maxpool → conv3-256 → conv3-512 →
maxpool → Output layer

VGG13 stud :
Input layer → conv3-64 → conv3-64 → maxpool → conv3-64 → conv3-64 →
maxpool → conv3-128 → conv3-128 → maxpool → conv3-256 → conv3-256 → maxpool →
conv3-256 → conv3-512 → maxpool → Output layer

1https://github.com/sidak/otfusion
2https://github.com/PythonOT/POT
3https://anonymous.4open.science/r/6ab184dc-3c64-4fdd-ba6d-1e5097623dfd/a_hetero_model_transfer.py

13

Table 9: The optimal CLA for diﬀerent combinations of layer representation and cost function when
fusing 2 MLPs on the MNIST dataset. Experiments were run with 5 diﬀerent seeds. Changing the
random seed did not alter the optimal mappings except for two cases which are in red.

Layer representation

Cost function

Both

Last layer constraint First layer constraint

None

Number of neurons
Pre-activation matrix
Pre-activation matrix Wasserstein distance

squared diﬀerence
1 - linear CKA

[1, 2, 5]
[1, 2, 5]
[1, 2, 5]

[1, 2, 5]
[1, 2, 5]
[1, 2, 5]

[1, 2, 3]
[1, 2, 3]
[1, 2, 3]

[1, 2, 3]
[1, 2, 3]
[1, 2, 3]

RESNET34 :
Input layer → conv3-64 → conv3-64 → conv3-64 → conv3-64 → conv3-64 →
conv3-64 → conv3-64 → conv3-128 → conv3-128 → conv1 − 128 → conv3-128 → conv3-128 →
conv3-128 → conv3-128 → conv3-128 → conv3-128 → conv3-256 → conv3-256 → conv1 − 256 →
conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 →
conv3-256 → conv3-256 → conv3-256 → conv3-512 → conv3-512 → conv1 − 512 → conv3-512 →
conv3-512 → conv3-512 → conv3-512 → Output layer

RESNET18 :
Input layer → conv3-64 → conv3-64 → conv3-64 → conv3-64 → conv3-64 →
conv3-128 → conv3-128 → conv1 − 128 → conv3-128 → conv3-128 → conv3-256 → conv3-256 →
conv1 − 256 → conv3-256 → conv3-256 → conv3-512 → conv3-512 → conv1 − 512 → conv3-512 →
conv3-512 → Output layer

Note that the average-pooling layer is skipped in the representation. In addition, the last fully-
connected layers, which are usually called classiﬁer layers, are merged into the output layer. In
RESNET models, convolution layers in red color with a kernel size of 1 is the shortcut layer that
connects two stages of diﬀerent channel sizes. These layers are represented by dotted shortcuts in
Figure 3 in RESNET’s paper [11].

Computing devices. All experiments are done on 1 NVIDIA A100 40GB GPU.

B Ablation studies

In this section, we show that the optimal CLA is superior to other strictly increasing mappings by
comparing the performance of the fused model which is obtained by applying the second and third
steps of CLAFusion with the corresponding cross-layer mapping. We ﬁrst study the importance of
CLA and two constraints to the classiﬁcation accuracy of the fused model when the pre-trained
models are fully connected neural networks. In the latter part, we validate the advantage of CLA in
fusing convolutional neural networks.

B.1 Fully connected neural networks

Settings. We compare the performance of diﬀerent combinations of layer representation and layer
balancing methods in our framework. To prove the eﬃciency of the CLA step, we compare the result
of fusing two models using diﬀerent mappings. We also consider removing the constraint on the ﬁrst
and last hidden layers. We train two MLPs on the MNIST dataset [19] in 5 diﬀerent seeds. Model A
has 5 hidden layers of size 400, 200, 100, 50, 25 while model B has 3 hidden layers of size 400, 200,
100 as the following representation.

14

Table 10: Performance comparison between diﬀerent combinations of mapping and balancing methods
when fusing 2 MLPs on the MNIST dataset. Experiments were run with 5 diﬀerent seeds. The
mapping obtained from the CLA step and the best performing mapping in columns MF are in bold.

Mapping

MA

MB

MF

[1, 2, 3]
[1, 2, 4]
[1, 2, 5]
[1, 3, 4]
[1, 3, 5]
[1, 4, 5]
[2, 3, 4]
[2, 3, 5]
[2, 4, 5]
[3, 4, 5]

96.95 ± 0.18

97.75 ± 0.04

Add

Merge

92.33 ± 2.35
93.90 ± 1.57
93.41 ± 1.56
92.48 ± 2.07
92.49 ± 2.64 94.00 ± 1.36
92.21 ± 1.86
90.69 ± 2.62
93.04 ± 2.44
90.88 ± 2.61
93.06 ± 2.07
90.98 ± 2.35
51.26 ± 6.41
82.21 ± 1.79
56.56 ± 5.87
83.14 ± 2.18
54.16 ± 6.01
83.00 ± 2.01
56.31 ± 4.79
79.49 ± 2.37

Model A: Input layer → FC-400
(cid:124) (cid:123)(cid:122) (cid:125)
layer 1

→ FC-200
(cid:124) (cid:123)(cid:122) (cid:125)
layer 2

→ FC-100
(cid:124) (cid:123)(cid:122) (cid:125)
layer 3

→ FC-50
(cid:124) (cid:123)(cid:122) (cid:125)
layer 4

→ FC-25
(cid:124) (cid:123)(cid:122) (cid:125)
layer 5

→ Output layer

Model B: Input layer → FC-400
(cid:124) (cid:123)(cid:122) (cid:125)
layer 1

→ FC-200
(cid:124) (cid:123)(cid:122) (cid:125)
layer 2

→ FC-100
(cid:124) (cid:123)(cid:122) (cid:125)
layer 3

→ Output layer

(cid:55)→

n
(cid:74)

The optimal mapping of CLA. The cross-layer mappings obtained using Algorithm 1 with
diﬀerent combinations are given in Table 9. According to Deﬁnition 1 in Section 3.1, the optimal
mapping is strictly increasing mapping a :
where n and m are the number of hidden layers
m
(cid:75)
(cid:74)
of the shallower and deeper networks, respectively. A mapping a is represented as [a(1), a(2), . . . , a(n)]
where a(i) = j indicates that layer i of model B is aligned to layer j of model A. In this case, we
have m = 5 and n = 3 and the mapping [1, 2, 5] means that a(1) = 1, a(2) = 2, and a(3) = 5.
We observe that changing the random seed barely aﬀects the result of CLA in this setting. When
keeping both ﬁrst and last layer constraints, all three combinations yield the same mappings. All
three combinations share the same mappings in almost all cases even if none of the two constraints
are placed4. In this case, the ﬁrst layer constraint does not aﬀect the results of the optimal mapping
in all combinations.

(cid:75)

The eﬀectiveness of the optimal mapping. We fuse two MLPs for all 10 possible mappings
between two models A and B. For the layer balancing method, we try both adding layers and merging
layers (the sign of sum estimation). The average performance of pre-trained models and the fused
model across 5 random seeds are summarized in Table 10. On either choice of the layer balancing
method, the best accuracy of the fused model is achieved when imposing both the ﬁrst and last layer
constraints. Removing the last layer constraint decreases slightly the performance of the fused model.
When the ﬁrst layers of the two models do not match each other in the last 4 rows, the accuracy
drops dramatically. Because an input image may consist of negative cell values due to normalization,
adding a new layer as the ﬁrst hidden layer does not maintain the accuracy of the shallower model.

4When the random seed is 2, the optimal mapping for "pre-activation matrix“ + "1 - linear CKA“ is [1, 2, 5] instead

of [1, 2, 3].

15

Therefore, it is necessary to match the ﬁrst and last hidden layers of the two models. Comparing
layer balancing methods, the merging layers method runs slower but interestingly leads to a higher
accuracy than the adding layers in this situation.

B.2 Convolutional neural networks

In this section, we conduct ablation studies on two common CNN architectures including VGG and
RESNET. For solving the CLA problem, layer representation is the pre-activation matrix of 200
samples while the cost function is calculated by subtracting the linear CKA from 1. In terms of the
layer balancing method, adding layers is chosen. After fusing two models, we perform ﬁnetuning
with hyperparameters in Tables 7 and 8.

B.2.1 VGG architecture

Settings. We consider the same two VGG models used in Section 4.2 on the CIFAR10 dataset. Using
the same notation as in VGG’s paper [25], the architectures of the two models can be represented as
follows.

(cid:104)

conv3-64
(cid:123)(cid:122)
(cid:125)
(cid:124)
layer 1

→ conv3-128
(cid:125)

(cid:123)(cid:122)
layer 2

(cid:124)

(cid:105)

→ maxpool →

(cid:105)

→ maxpool →

(cid:104)

conv3-256
(cid:124)
(cid:123)(cid:122)
(cid:125)
layer 3

(cid:105)

→

→ conv3-256
(cid:125)

(cid:123)(cid:122)
layer 4

(cid:124)

(cid:105)

→ maxpool →

(cid:104)

conv3-1028
(cid:124)
(cid:123)(cid:122)
(cid:125)
layer 7

→ conv3-1028
(cid:125)

(cid:123)(cid:122)
layer 8

(cid:124)

conv3-512
(cid:124)
(cid:125)
(cid:123)(cid:122)
layer 5

→ conv3-512
(cid:125)

(cid:123)(cid:122)
layer 6

→ maxpool → Output layer

(cid:105)

→ maxpool →

→ maxpool →

(cid:104)

conv3-64
(cid:124)
(cid:123)(cid:122)
(cid:125)
layer 1
(cid:104)

conv3-256
(cid:124)
(cid:123)(cid:122)
(cid:125)
layer 5

(cid:105)

→ conv3-256
(cid:125)

(cid:123)(cid:122)
layer 6

(cid:124)

(cid:104)

(cid:105)

conv3-64
(cid:124)
(cid:125)
(cid:123)(cid:122)
layer 2

→ maxpool →

conv3-128
(cid:124)
(cid:125)
(cid:123)(cid:122)
layer 3
(cid:105)

→

→

→ conv3-512
(cid:125)

(cid:123)(cid:122)
layer 8

(cid:124)

(cid:104)

conv3-256
(cid:124)
(cid:123)(cid:122)
(cid:125)
layer 7

Model A: Input layer →

maxpool →

(cid:104)

(cid:124)
(cid:105)

(cid:104)

(cid:104)

conv3-1028
(cid:124)
(cid:125)
(cid:123)(cid:122)
layer 9

→ conv3-512
(cid:125)

(cid:123)(cid:122)
layer 10

(cid:124)

Model B : Input layer →

(cid:105)

conv3-128
(cid:123)(cid:122)
(cid:125)
(cid:124)
layer 4

→ maxpool →

maxpool → Output layer

Note that following the oﬃcial implementation of OTFusion, we did not modify the number of
channels of the ﬁrst and last convolution layers.

The optimal mapping of CLA. Because two models have identical input, output, and fully
connected layers, we only need to consider matching their convolution layers. As discussed in
Section 3.1, each VGG model is divided into 5 groups which are separated by the max-pooling layers
and enclosed in square brackets in the above representation. For the last three pairs of groups, the
optimal mappings are trivial because each pair has the same number of layers. Abusing the notation
. Therefore, we only need to solve the
5
in Deﬁnition 1 in Section 3.1, we have a(3 + i) = 5 + i, i ∈
(cid:75)
(cid:74)
CLA problem for the ﬁrst two groups, i.e., to ﬁnd the values of a(1) and a(2). In this case, we again
observe that the optimal mapping remains the same even if changing the random seed.

The eﬀectiveness of the optimal mapping. Other than the optimal mapping obtained from
CLA, we fuse two VGG models for 3 intra-group mappings and 2 additional inter-group mappings.
Here, intra-group means that layers in the ﬁrst(second) group of model B are only aligned to layers in
the ﬁrst(second) group of model A. While inter-group means that layers in the ﬁrst or second group
of model B can be aligned to layers in the both ﬁrst and second groups of model A. In essence, the

16

Table 11: Performance comparison between diﬀerent mappings when fusing 2 VGG models on the
CIFAR10 dataset. Experiments were run with 5 diﬀerent seeds. To make the notation succinct, the
mapping only indicates the values of a(1) and a(2). The mapping obtained from the CLA step is in
bold while two additional inter-group mappings are in red. The best performing mapping in columns
MF is in bold.

Mapping

MA

MB

MF

[1, 2]
[1, 3]
[1, 4]
[2, 3]
[2, 4]
[3, 4]

92.65 ± 0.16

89.72 ± 0.12

Before ﬁnetuning After ﬁnetuning

62.76 ± 1.92
82.03 ± 0.50
79.37 ± 1.05
46.64 ± 2.23
49.63 ± 2.80
46.93 ± 1.99

90.56 ± 0.12
90.64 ± 0.17
90.37 ± 0.20
89.32 ± 0.15
89.03 ± 0.19
84.62 ± 0.16

inter-group mappings are discouraged due to the inconsistency of the input image size as discussed in
Section 2.3. In this experiment only, we consider them for comparison purposes. Table 11 illustrates
the average performance of the fused model for diﬀerent mappings. Before ﬁnetuning, it can be seen
clearly that the optimal CLA shows a clear margin of at least 2.66% in the performance over other
mappings. Similar to the case of MLPs, the performance falls remarkably when not aligning the ﬁrst
layers of two models in the last 3 rows. In terms of the performance after ﬁnetuning, the optimal
CLA still beats other mappings, resulting in an accuracy of 90.64 on average.

B.2.2 RESNET architecture

Settings. We consider the same two RESNET models used in Section 4.1 on the CIFAR10 dataset.
Using the same notation as in VGG’s paper [25], the architectures of the two models can be
represented as follows.

RESNET34 : Input layer → conv3-64 →

(cid:104)
(conv3-64 → conv3-64) → (conv3-64 → conv3-64) →

(cid:105)
(conv3-64 → conv3-64)

→

(cid:104)

(conv3-128 → conv3-128 → conv1 − 128) → (conv3-128 → conv3-128)
(cid:105)

(cid:104)
→ (conv3-128 → conv3-128) → (conv3-128 → conv3-128)
(conv3-256 → conv3-256 →
conv1 − 256) → (conv3-256 → conv3-256) → (conv3-256 → conv3-256) → (conv3-256 → conv3-256)
(cid:104)
(conv3-512 → conv3-512 →
→ (conv3-256 → conv3-256) → (conv3-256 → conv3-256)

→

→

(cid:105)

(cid:105)
conv1 − 512) → (conv3-512 → conv3-512) → (conv3-512 → conv3-512)

→ Output layer

(cid:104)

(cid:105)
(conv3-64 → conv3-64) → (conv3-64 → conv3-64)

→

RESNET18 : Input layer → conv3-64 →
(cid:104)

(cid:105)
(conv3-128 → conv3-128 → conv1 − 128) → (conv3-128 → conv3-128)

(cid:104)
(conv3-256 →

→

conv3-256 → conv1 − 256) → (conv3-256 → conv3-256)

(cid:105)

(cid:104)

→

(conv3-512 → conv3-512 →

conv1 − 512) → (conv3-512 → conv3-512)

(cid:105)

→ Output layer

The optimal CLA. Each RESNET model is also divided into 5 stages which are enclosed in

17

Table 12: Performance comparison between diﬀerent mappings when fusing 2 RESNET models on
the CIFAR10 dataset. Experiments were run with 5 diﬀerent seeds. The term “CLA woc" stands for
CLA without the last layer constraint. The best performing mapping in columns MF is in bold.

Mapping

MA

MB

MF

Naive
Random
CLA woc
CLA

93.31 ± 0.13

92.92 ± 0.20

Before ﬁnetuning After ﬁnetuning

62.43 ± 3.08
64.04 ± 3.01
65.58 ± 2.61
65.72 ± 2.33

93.59 ± 0.18
93.61 ± 0.12
93.60 ± 0.14
93.67 ± 0.07

square brackets in the above representation. And each building block is enclosed in parentheses.
As discussed in Section 3.1, in each stage we ﬁnd the optimal mapping for blocks instead of layers.
Because each stage of RESNET18 has only two blocks, the ﬁrst block is assigned to the ﬁrst block
while the second one is assigned to the last block in the same stage of RESNET34.

The eﬀectiveness of the optimal mapping. We compare the optimal CLA with three baseline
mappings. The ﬁrst baseline is the naive mapping of blocks with the same index, i.e., the ith block of
RESNET18 is aligned to the ith block in the same stage of RESNET34. The second one is random
mapping which satisﬁes the ﬁrst layer constraint. Because the total number of strictly increasing
mappings is quite large, we only sample 5 random mappings for each random seed and take their
average performance. The last mapping is the one that is obtained by solving the CLA problem
but removing the last layer constraint. Table 12 compares the classiﬁcation accuracy of the fused
model on diﬀerent types of mapping. Before ﬁnetuning, using the optimal CLA outperforms the
naive and random mappings with a wide margin of 3.29 and 1.68, respectively. Removing the last
layer constraint decreases the performance slightly but still yields better performance than the other
two. After ﬁnetuning, the optimal CLA produces slightly higher accuracy than the other mappings.
In terms of baselines, three mappings have a comparative performance of approximately 93.60%.

C Additional experimental results

We ﬁrst apply CLAFusion to the skill transfer experiment [26, Section5.1] but for heterogeneous
neural networks.
In the subsequent sections, we report the detailed results for diﬀerent runs
of experiments in the main text. Finally, we showcase the eﬃciency of CLAFusion in terms of
computational resources.

C.1 Skill transfer

Settings.
In skill transfer, we aim to obtain a single model that can inherit both overall and
specialized skills from the two individual models. We adopt the same heterogeneous data-split
technique as in [26, Section 5.1] for two MLPs A and B. The MNIST dataset is separated into two
datasets. The dataset for model A has all images of label 4 and 10% images of other labels while that
for model B contains the rest. We train an MLP with 3 hidden layers of size 400, 200, 100 for model
A and an MLP with 4 hidden layers of size 400, 200, 100, 50 for model B. The pre-activation matrix is

18

(a) Adding layers

(b) Merging layers

Figure 4: The average performance of skill transfer using (a) adding layers and (b) merging layers as the layer
balancing method. Experiments were run with 5 diﬀerent seeds.

Table 13: Performance comparison between two layer balancing methods on skill transfer task.
Experiments were run with 5 diﬀerent seeds. The best performing result in each row is in bold.

Seed

wB

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0
1
2
3
4

86.10
91.60
90.80
91.94
93.34

87.06
91.81
92.07
92.31
93.31

88.23 78.64
85.63
91.23
92.46 84.06
79.58
91.39
81.70
91.56

73.48
77.20
82.53
71.85
77.45

69.70
77.99
83.21
71.77
78.46

70.45
81.59
85.01
78.31
82.15

75.91
84.85
86.83
83.63
85.00

81.38
86.88
87.76
85.79
86.62

85.36
87.48
88.01
87.01
87.37

87.68
87.29
87.83
87.51
87.58

Avg

90.76

91.31

90.97

81.92

76.50

76.23

79.50

83.24

85.69

87.05

87.58

0
1
2
3
4

86.10
91.60
90.80
91.94
93.34

87.19
92.23
91.98
92.66
93.80

78.51
87.10
91.83
89.08
92.30 86.00
92.76 88.13
90.03
93.18

63.66
78.69
81.27
71.65
80.24

56.74
76.04
79.42
65.90
79.27

53.31
77.34
80.09
64.20
81.01

53.88
81.44
82.95
67.39
82.52

57.65
84.87
85.46
72.91
83.91

66.55
86.39
85.61
76.06
84.68

76.35
86.60
85.04
77.40
85.09

Avg

90.76

91.57

91.43

86.35

75.10

71.47

71.19

73.64

76.96

79.86

82.10

Adding layers

Merging layers

obtained by running inference for 400 samples. We use the number of neurons as layer representation
and adding layers method for balancing. The weight proportion of model B, which is referred to as
wB (also known as fusion rate in [27]), is swept over 11 diﬀerent values: wB = {0.0, 0.1, . . . , 1.0}.

Results. Figure 4(a) shows that when the weight proportion of model B is small (0.1 or 0.2),
the fused model improves over both individual models. The reason is that the specialist model A
has higher accuracy than the generalist model B. When distributing the weight proportion fairly
(0.5), the fused model performs worst due to the eﬀect of heterogeneous training data. The fused
model has the best accuracy of 91.31%, which is higher than those of models A (90.76%) and B
( 87.58%). This claims that the knowledge has been transferred successfully to the fused model
without retraining. Furthermore, the detailed results of skill transfer for 5 diﬀerent random seeds are
reported in Table 13. The specialist model A generally has higher accuracy than the generalist model

19

0.00.20.40.60.81.0Weight proportion of model B767880828486889092Global test accuracyMMAMB depth-aligned0.00.20.40.60.81.0Weight proportion of model B72.575.077.580.082.585.087.590.092.5Global test accuracyMMAMB depth-alignedTable 14: The results of ﬁnetuning RESNET from diﬀerent choices of initialization across 5 diﬀerent
seeds. The best performing initialization in each row is in bold.

Dataset

Seed MA MB

Ensemble MF

Finetune

Learning

MA MB

MF MB depth-aligned Random RESNET34

CIFAR10

CIFAR100

40
41
42
43
44

93.42
93.37
93.31
93.39
93.06

93.21
92.90
93.05
92.78
92.65

Avg

93.31

92.92

40
41
42
43
44

66.08
66.21
65.89
65.85
65.61

65.19
65.22
65.04
66.17
65.03

Avg

65.93

65.33

94.10
93.81
93.94
93.71
93.49

93.81

68.44
68.58
68.39
68.98
68.05

68.49

67.83
63.66
68.30
66.45
62.35

93.50
93.61
93.61
93.50
93.37

93.47 93.61
93.26 93.74
93.36 93.71
93.30 93.73
93.05 93.55

65.72

93.52

93.29 93.67

29.44
28.92
26.62
27.35
27.31

66.73
67.32
66.77
67.12
66.51

66.47 67.71
66.33 67.54
65.91 67.16
66.35 68.05
65.90 67.50

27.93

66.87

66.19 67.59

93.47
93.32
93.43
93.00
92.97

93.24

66.23
66.45
66.35
67.23
66.16

66.48

91.95
91.93
92.25
91.76
92.12

92.00

64.73
64.24
64.66
65.67
63.65

64.59

Table 15: The results of fusing and ﬁnetuning 4 RESNET from diﬀerent choices of initialization
across 5 diﬀerent seeds. The best performing initialization in each row is in bold.

Dataset

Seed

Pre-trained models

Ensemble MF

Finetune

(RESNET34, RESNET18) x2

Learning

Pre-trained models

MF

CIFAR10

CIFAR100

40
41
42
43
44

93.42, 93.21, 93.37, 92.90
93.37, 92.90, 93.31, 93.05
93.31, 93.05, 93.39, 92.78
93.39, 92.78, 93.06, 92.65
93.06, 92.65, 92.66, 92.76

Avg

93.31, 92.92, 93.16, 92.83

40
41
42
43
44

66.08, 65.19, 66.21, 65.22
66.21, 65.22, 65.89, 65.04
65.89, 65.04, 65.85, 66.17
65.85, 66.17, 65.61, 65.03
65.61, 65.03, 66.27, 64.92

Avg

65.93, 65.33, 65.97, 65.28

94.36
94.20
94.10
94.11
94.06

94.17

70.24
70.50
70.46
70.16
69.98

70.27

34.55
32.35
34.01
34.93
22.98

93.50, 93.47, 93.61, 93.26 93.97
93.61, 93.26, 93.61, 93.36 93.72
93.61, 93.36, 93.50, 93.30 93.81
93.50, 93.30, 93.37, 93.05 93.73
93.37, 93.05, 92.94, 93.04 93.56

31.76

93.52, 93.29, 93.41, 93.20 93.76

8.28
8.02
7.75
8.51
9.77

8.47

66.73, 66.47, 67.23, 66.33 68.89
67.23, 66.33, 66.77, 65.91 68.70
66.77, 65.91, 67.12, 66.35 68.77
67.12, 66.35, 66.51, 65.90 69.08
66.51, 65.90, 66.24, 65.29 69.14

66.87, 66.19, 66.77, 65.96 68.92

B seeds because it has been trained on all 10 labels. In 4 out of 5 seeds, the fused model improves
over both individual models when the weight proportion of model B is small (wB = {0.1, 0.2}).

Comparison between two layer balancing methods. For comparison purposes, we further
conduct the same procedure but replace adding layers by merging layers (the sign of sum estimation).
Similar to the previous experiment, the fused model attains the best performance when the weight
proportion of model B is small (wB = {0.1, 0.2}). Figure 4 illustrates the average performance of
two layer balancing methods on the skill transfer task. When averaging over 5 random seeds, both
methods perform best if wB = 0.1. The best accuracy of merging layers (91.57) is slightly higher
than that of adding layers (91.31) even though the accuracy of MB depth-aligned drops from 87.58
to 82.10.

20

C.2 Generate an eﬃcient initialization

Results. Finetuning from the fused model always improves the accuracy over ﬁnetuning from
pre-trained models as illustrated in Table 14. In comparison with other initializations, it yields
the best performance in all 5 seeds on both datasets. MB depth-aligned, which is the result of
NET2NET [4] operation, performs better than ﬁnetuning from scratch but only leads to comparable
performance as ﬁnetuning from MB.

More than 2 models. Tables 15 and 16 detail the results in the multiple-model scenario. Fusing
4 RESNET results in the initialization with a signiﬁcant decrease in the accuracy compared to
the two-model scenario. However, there is an increase in the ﬁnal performance after ﬁnetuning,
with a ﬁgure growing from 93.67 to 93.76 on the CIFAR10 dataset and from 67.59 to 68.92 on the
CIFAR100 dataset. In addition, CLAFusion produces the best initialization in all 5 runs. For the
6-model case, CLAFusion also yields more favorable initialization in every run on both CIFAR10
and CIFAR100 datasets. It is worth mentioning that increasing the number of pre-trained models
does increase the performance of ensemble learning and the fused model after ﬁnetuning.

Table 16: The results of fusing and ﬁnetuning 6 RESNET from diﬀerent choices of initialization
across 5 diﬀerent seeds. The best performing initialization in each row is in bold.

Dataset

Seed

Pre-trained models

Ensemble MF

Finetune

CIFAR10

CIFAR100

(RESNET34, RESNET18) x3

Learning

Pre-trained models

MF

40
41
42
43
44

93.42, 93.21, 93.37, 92.90, 93.31, 93.05
93.37, 92.90, 93.31, 93.05, 93.39, 92.78
93.31, 93.05, 93.39, 92.78, 93.06, 92.65
93.39, 92.78, 93.06, 92.65, 92.66, 92.76
93.06, 92.65, 92.66, 92.76, 93.46, 93.05

Avg

93.31, 92.92, 93.16, 92.83, 93.18, 92.86

40
41
42
43
44

66.08, 65.19, 66.21, 65.22, 65.89, 65.04
66.21, 65.22, 65.89, 65.04, 65.85, 66.17
65.89, 65.04, 65.85, 66.17, 65.61, 65.03
65.85, 66.17, 65.61, 65.03, 66.27, 64.92
65.61, 65.03, 66.27, 64.92, 65.82, 65.34

Avg

65.93, 65.33, 65.97, 65.28, 65.89, 65.30

94.28
94.19
94.13
94.14
94.36

94.22

70.47
70.83
70.66
70.53
70.63

70.62

26.32
24.22
24.37
23.78
20.08

93.50, 93.47, 93.61, 93.26, 93.61, 93.36 93.84
93.61, 93.26, 93.61, 93.36, 93.50, 93.30 93.84
93.61, 93.36, 93.50, 93.30, 93.37, 93.05 93.63
93.50, 93.30, 93.37, 93.05, 92.94, 93.04 93.69
93.37, 93.05, 92.94, 93.04, 93.41, 93.34 93.89

23.75

93.52, 93.29, 93.41, 93.20, 93.49, 93.24 93.78

4.14
4.63
4.22
4.87
4.09

4.39

66.73, 66.47, 67.23, 66.33, 66.77, 65.91 68.80
67.23, 66.33, 66.77, 65.91, 67.12, 66.35 69.34
66.77, 65.91, 67.12, 66.35, 66.51, 65.90 69.72
67.12, 66.35, 66.51, 65.90, 66.24, 65.29 69.56
66.51, 65.90, 66.24, 65.29, 66.18, 66.16 69.57

66.87, 66.19, 66.77, 65.96, 66.56, 65.92 69.40

C.3 Heterogeneous model transfer

Settings. The ﬁnetuning hyperparameters are similar to those for ﬁnetuning RESNET models in
Appendix C.2. However, we observe that retraining at the initial learning rate of 0.1 in some cases
causes the target model after transferring to diverge. Hence, we additionally retrain at the initial
learning rate of 0.05 and report the best performing one between two initial learning rates.

Results. Table 18 details the results of 4 diﬀerent model transfer methods. On the CIFAR10
dataset, combining CLA and OTFusion with HMT always results in a great boost, at least 16.27
(seed 42), in the accuracy of the target model after transferring. After the ﬁnetuning phase, it gives
the best performance in 4 out of 5 random seeds. In terms of the CIFAR100 dataset, incorporating
CLAFusion into HMT leads to the best performance in 5 out of 5 runs.

21

Table 17: Finetuning teacher-student models across 5 diﬀerent seeds. The best performing student
model in each row is in bold. The results of VGG reported in Table 4 are run with the random seed
of 42.

Dataset +

Seed MA MB

Ensemble MF

Retrain

Architecture

CIFAR10 +
VGG

CIFAR100 +
RESNET

Learning

MA MB

MF MB depth-aligned

40
41
42
43
44

92.71
92.34
92.70
92.79
92.70

89.68
89.61
89.92
89.78
89.62

Avg

92.65

89.72

40
41
42
43
44

66.08
66.21
65.89
65.85
65.61

61.68
61.57
61.36
61.56
61.46

Avg

65.93

61.53

92.53
92.68
92.86
92.82
92.51

92.68

67.14
67.11
67.53
67.14
66.94

67.17

81.66
82.59
82.66
81.79
81.43

92.67
92.50
92.65
92.77
92.56

89.81 90.64
89.57 90.59
89.89 90.96
90.10
90.55
89.46 90.44

82.03

92.63

89.77 90.64

28.72
27.98
29.31
28.03
28.30

66.73
67.23
66.77
67.12
66.51

63.10 64.89
62.95 64.59
62.97 65.10
62.23 63.83
62.59 63.85

28.47

66.87

62.77 64.45

90.54
90.42
90.84
90.61
90.19

90.52

63.54
62.94
62.48
62.96
62.84

62.95

C.4 Teacher-student fusion

Settings. We conduct an additional setting, which is fusing teacher and student RESNET models on
the CIFAR100 dataset. Teacher model A has RESNET34 architecture while model B has RESNET18
architecture with a half number of channels at each layer. The architectures of MA, MB, and MF ,
which are in turn RESNET34, RESNET18 half, and RESNET34 half, can be represented as follows.

Input layer → conv3-64 → conv3-64 → conv3-64 → conv3-64 → conv3-64 →
RESNET34 :
conv3-64 → conv3-64 → conv3-128 → conv3-128 → conv1 − 128 → conv3-128 → conv3-128 →
conv3-128 → conv3-128 → conv3-128 → conv3-128 → conv3-256 → conv3-256 → conv1 − 256 →
conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 → conv3-256 →
conv3-256 → conv3-256 → conv3-256 → conv3-512 → conv3-512 → conv1 − 512 → conv3-512 →
conv3-512 → conv3-512 → conv3-512 → Output layer

RESNET18 half : Input layer → conv3-32 → conv3-32 → conv3-32 → conv3-32 → conv3-32 →
conv3-64 → conv3-64 → conv1 − 64 → conv3-64 → conv3-64 → conv3-128 → conv3-128 →
conv1 − 128 → conv3-128 → conv3-128 → conv3-256 → conv3-256 → conv1 − 256 → conv3-256 →
conv3-256 → Output layer

RESNET34 half : Input layer → conv3-32 → conv3-32 → conv3-32 → conv3-32 → conv3-32 →
conv3-32 → conv3-32 → conv3-64 → conv3-64 → conv1 − 64 → conv3-64 → conv3-64 → conv3-64
→ conv3-64 → conv3-64 → conv3-64 → conv3-128 → conv3-128 → conv1 − 128 → conv3-128 →
conv3-128 → conv3-128 → conv3-128 → conv3-128 → conv3-128 → conv3-128 → conv3-128 →
conv3-128 → conv3-128 → conv3-256 → conv3-256 → conv1 − 256 → conv3-256 → conv3-256 →
conv3-256 → conv3-256 → Output layer

Other settings for running CLAFusion and ﬁnetuning are similar to experiments in Section 4.1.

Results. Table 17 illustrates the ﬁnetuning result of teacher and student models across 5 diﬀerent

22

Table 18: The results of model transfer from MB (RESNET18) to MA (RESNET34) across 5 diﬀerent
seeds. The best performing method in each row is in bold.

Dataset

CIFAR10

Method

Transfer

Transfer + Finetune

CIFAR100

Transfer

Transfer + Finetune

Seed HMT HMT + Naive avg HMT + OTFusion HMT + CLA + OTFusion

40
41
42
43
44

9.55
10.13
10.45
12.44
11.37

Avg

10.79

40
41
42
43
44

93.29
93.14
93.08
93.20
92.77

Avg

93.10

40
41
42
43
44

Avg

40
41
42
43
44

2.98
2.33
3.03
3.17
2.00

2.70

64.78
65.25
65.15
65.53
65.06

Avg

65.15

37.80
37.66
43.44
39.24
29.33

37.49

93.63
93.28
93.16
93.27
93.58

93.38

9.69
10.92
12.90
11.44
9.00

10.79

65.96
66.39
66.56
66.71
66.46

66.42

20.17
17.66
19.25
15.20
19.30

18.32

93.61
93.69
93.78
93.80
93.41

93.66

7.32
4.38
4.13
5.65
4.22

5.14

67.10
67.37
67.22
67.44
67.00

67.23

62.94
62.27
60.86
65.74
54.63

61.29

93.75
94.09
93.68
93.92
93.84

93.86

12.69
14.49
7.33
15.35
12.92

12.56

67.74
67.95
67.51
68.13
67.80

67.83

seeds. The fused model MF is the most productive initialization of student models in at least 4
out of 5 seeds. Retraining the fused model (MF ) always yields higher accuracy than continuing
training the student model (MB), leading to average improvements of 0.87 and 1.68 on CIFAR10
and CIFAR100 datasets, respectively.

Discussion. At ﬁrst glance, the ﬁrst two steps of our approach may resemble the NET2NET
operations. Although both approaches increase the depth of the student network and use the deeper
student as a good initialization, there are two major diﬀerences between ours and NET2NET. Firstly,
we present a systematic way of ﬁnding the location to add the identity mappings for diﬀerent types of
network architectures while NET2NET is manually designed for a speciﬁc type of network. Secondly,
we do not enlarge the width of the student network (equivalently, use only NET2DEEPERNET
operation) so that the student network remains more compact than the teacher network.

C.5 Knowledge distillation

In this experiment, we apply knowledge distillation to pre-trained models of seed 42 in Table 17.

Results. The results of distilling the teacher model into the student model are given in Table 19.
Knowledge distillation from the fused model leads to the best accuracy in no fewer than 4 out
of 5 temperatures. In addition, both its average and best performance are the highest among all
initializations. This suggests that CLAFusion can also be used to generate an eﬃcient initialization

23

Table 19: Distillation results for diﬀerent temperatures. For each type of architecture, the entity in
the ﬁrst ﬁve rows is the best accuracy obtained by varying the loss-weight factor while the last two
rows report the best and average performance across 5 diﬀerent temperatures. The best performing
initialization in each row is in bold.

Dataset +

Temperature

Distillation initialization

Architecture

CIFAR10 +
VGG

CIFAR100 +
RESNET

T

20
10
8
4
1

Best
Avg

20
10
8
4
1

Best
Avg

Random MB

MB

MF

MB depth-aligned Random MF

88.99 (0.70)
89.10 (0.70)
88.91 (0.70)
87.89 (0.70)
87.14 (0.10)

90.94 (0.70) 91.29 (0.10)
90.97 (0.70) 91.23 (0.50)
90.98 (0.70) 91.43 (0.70)
90.81 (0.99) 91.14 (0.50)
90.69 (0.05)
89.97 (0.10)

89.10
88.41

90.98
90.73

91.43
91.16

63.19 (0.50)
63.99 (0.50)
63.01 (0.50)
62.41 (0.50)
61.64 (0.10)

65.87 (0.95) 66.56 (0.70)
66.20 (0.95) 66.79 (0.70)
66.07 (0.99) 66.50 (0.70)
64.54 (0.50) 65.26 (0.70)
63.60 (0.10) 64.43 (0.05)

63.99
62.85

66.20
65.26

66.79
65.92

91.14 (0.70)
91.23 (0.10)
91.24 (0.70)
91.10 (0.50)
90.73 (0.05)

91.24
91.09

66.04 (0.70)
66.31 (0.99)
66.08 (0.99)
64.52 (0.50)
63.94 (0.50)

66.31
65.38

88.20 (0.10)
88.84 (0.50)
88.76 (0.50)
87.65 (0.05)
87.04 (0.05)

88.84
88.10

64.37 (0.50)
64.84 (0.95)
64.63 (0.70)
62.79 (0.50)
62.01 (0.05)

64.84
63.73

Table 20: The number of parameters and inference time on the CIFAR10 dataset for diﬀerent
experiments. The inference time is the time taken in seconds to run inference on the whole test
dataset.

Experiment

Architecture

# Params

Inference time

Pre-trained models

MF

Pre-trained models MF Ensemble Pre-trained models MF Ensemble

Fuse 2 RESNET RESNET34, RESNET18
RESNE34
Fuse 4 RESNET (RESNET34, RESNET18) x2 RESNE34
Fuse 6 RESNET (RESNET34, RESNET18) x3 RESNE34

21M, 11M
(21M, 11M) x2
(21M, 11M) x3

21M
21M
21M

32M
64M
96M

1.78, 1.08
(1.78, 1.08) x2
(1.78, 1.08) x3

1.78
1.78
1.78

2.72
5.30
7.88

for pre-trained distillation. Moving onto other initialization, MB depth-aligned, which is obtained
from the NET2NET operation, is the second-best choice, followed by the pre-trained student model
MB. Distilling knowledge into either randomly initialized student models, however, yields a much
lower accuracy. This result is consistent with the ﬁndings in [30].

C.6 Computational eﬃciency of model fusion

In this section, we demonstrate that CLAFusion enhances the computational eﬃciency substantially
over ensemble learning and knowledge distillation in all experiments.

Table 20 illustrates the model size and inference time on the CIFAR10 test dataset of all models
in Section 4.1. As discussed in Section 1, ensemble learning is computationally expensive because
it stores and runs all pre-trained models during the inference stage. As the number of pre-trained
models increases, memory consumption and inference time of ensemble learning grow up dramatically.
In particular, the number of parameters multiplies by the same factor as the number of pre-trained
models, increasing from 32M to 96M. Though rising at a lower speed, the inference time still increases

24

Table 21: Training time for ﬁnetuning and knowledge distillation in the teacher-student setting. The
training time is the time taken in seconds to run one epoch of training on the whole train dataset.

Experiment

Architecture

# Params

Finetuning

Distillation

MA

MB

MF

MA MB MF MA MB MF MB MF

Teacher-student VGG

VGG13 doub

VGG11 half

VGG13 stud

Teacher-student RESNET RESNET34 RESNET18 half RESNET34 half

33M 3M 3M 11.14
21M 3M 5M 31.73

7.40
7.50

7.48
12.05

12.22
36.39

13.86
41.36

by almost 190%, from 2.72 to 7.88. On the other hands, the fused model produced by CLAFusion
remains compact no matter how many pre-trained models are fused.

Table 21 compares the training time between ﬁnetuning and knowledge distillation in the teacher-
student setting. It is undoubtedly that knowledge distillation runs much more slowly than ﬁnetuning,
causing a rise of at least 65% if using the same VGG initialization. In terms of RESNET architecture,
the speedup is even more signiﬁcant, with increases of 385% and 243% for MB and MF , respectively.
The convincing reason is that in knowledge distillation the input data is fed in the forward direction
through both teacher and student models while ﬁnetuning only runs forward inference for the student
model whose size is much smaller than that of the teacher model.

References

[1] S. Ashmore and M. Gashler. A method for ﬁnding similarity between multi-layer perceptrons
by forward bipartite alignment. In 2015 International Joint Conference on Neural Networks
(IJCNN), pages 1–7. IEEE, 2015. (Cited on pages 2 and 3.)

[2] N. Bonneel and D. Coeurjolly. Sliced partial optimal transport. ACM Transactions on Graphics

(Proceedings of SIGGRAPH), 38(4), Jul 2019. (Cited on page 5.)

[3] D. Chen, J. Mei, Y. Zhang, C. Wang, Z. Wang, Y. Feng, and C. Chen. Cross-layer distillation
with semantic calibration. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
pages 7028–7036, 2021. (Cited on page 4.)

[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge transfer.
In 4th International Conference on Learning Representations, 2016. (Cited on pages 7 and 21.)

[5] Y.-M. Chou, Y.-M. Chan, J.-H. Lee, C.-Y. Chiu, and C.-S. Chen. Unifying and merging
well-trained deep neural networks for inference stage. In Proceedings of the 27th International
Joint Conference on Artiﬁcial Intelligence, pages 2049–2056. AAAI Press, 2018. (Cited on pages 3
and 9.)

[6] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In International

conference on machine learning, pages 685–693. PMLR, 2014. (Cited on page 3.)

[7] A. Figalli. The optimal partial transport problem. Archive for rational mechanics and analysis,

195(2):533–560, 2010. (Cited on page 8.)

[8] R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel,
A. Corenﬂos, K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotoma-
monjy, I. Redko, A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and

25

T. Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1–8,
2021. (Cited on page 13.)

[9] J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International

Journal of Computer Vision, Mar 2021. (Cited on pages 1 and 2.)

[10] L. K. Hansen and P. Salamon. Neural network ensembles. IEEE transactions on pattern analysis

and machine intelligence, 12(10):993–1001, 1990. (Cited on page 1.)

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. (Cited
on pages 6 and 14.)

[12] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531, 2015. (Cited on pages 1 and 11.)

[13] G. Huang and A. Lim. A hybrid genetic algorithm for three-index assignment problem. In
The 2003 Congress on Evolutionary Computation, 2003. CEC’03., volume 4, pages 2762–2768.
IEEE, 2003. (Cited on page 8.)

[14] L. V. Kantorovich. On the translocation of masses. Journal of mathematical sciences, 133(4):1381–

1382, 2006. (Cited on page 2.)

[15] J. Kitagawa and B. Pass. The multi-marginal optimal partial transport problem. In Forum of

Mathematics, Sigma, volume 3. Cambridge University Press, 2015. (Cited on page 8.)

[16] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations
revisited. In International Conference on Machine Learning, pages 3519–3529. PMLR, 2019.
(Cited on page 2.)

[17] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University

of Toronto, Toronto, Ontario, 2009. (Cited on page 8.)

[18] H. W. Kuhn. The Hungarian method for the assignment problem. Naval research logistics

quarterly, 2(1-2):83–97, 1955. (Cited on page 5.)

[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. (Cited on page 14.)

[20] M. I. Leontev, V. Islenteva, and S. V. Sukhov. Non-iterative knowledge fusion in deep convolu-
tional neural networks. Neural Processing Letters, 51(1):1–22, Jul 2019. (Cited on page 3.)

[21] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. E. Hopcroft. Convergent learning: Do diﬀerent
neural networks learn the same representations? In Workshop on Feature Extraction: Modern
Questions and Challenges, pages 196–212, 2015. (Cited on page 2.)

[22] G. Monge. Mémoire sur la théorie des déblais et des remblais. Histoire de l’Académie Royale

des Sciences de Paris, 1781. (Cited on page 2.)

[23] J. O’Neill, G. V. Steeg, and A. Galstyan. Layer-wise neural network compression via layer
fusion. In Asian Conference on Machine Learning, pages 1381–1396. PMLR, 2021. (Cited on
pages 2 and 6.)

26

[24] L. Ramshaw and R. E. Tarjan. On minimum-cost assignments in unbalanced bipartite graphs.

HP Labs, Palo Alto, CA, USA, Tech. Rep. HPL-2012-40R1, 2012. (Cited on page 5.)

[25] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, 2015. (Cited on pages 3,
6, 13, 16, and 17.)

[26] S. P. Singh and M. Jaggi. Model fusion via optimal transport. Advances in Neural Information

Processing Systems, 33, 2020. (Cited on pages 1, 2, 3, 8, 9, 11, 12, 13, and 18.)

[27] J. Smith and M. Gashler. An investigation of how neural networks learn from the experiences
of peers through periodic weight averaging. In 2017 16th IEEE International Conference on
Machine Learning and Applications (ICMLA), pages 731–736. IEEE, 2017. (Cited on pages 1, 3,
and 19.)

[28] F. C. Spieksma. Multi index assignment problems: complexity, approximation, applications. In

Nonlinear assignment problems, pages 1–12. Springer, 2000. (Cited on page 8.)

[29] F. Tung and G. Mori. Similarity-preserving knowledge distillation.

In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 1365–1374, 2019. (Cited on
page 4.)

[30] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova. Well-read students learn better: On the
importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019. (Cited on
pages 1, 11, and 24.)

[31] J. Utans. Weight averaging for neural networks and local resampling schemes. In Proc. AAAI-96
Workshop on Integrating Multiple Learned Models. AAAI Press, pages 133–138. Citeseer, 1996.
(Cited on pages 1 and 3.)

[32] G. Wang, J. Lai, W. Liang, and G. Wang. Heterogeneous model transfer between diﬀerent
neural networks. Submission at International Conference on Learning Representations, 2021.
(Cited on pages 10 and 13.)

[33] H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni. Federated learning with
matched averaging. In International Conference on Learning Representations, 2020. (Cited on
pages 1, 2, and 3.)

[34] T. Wei, C. Wang, Y. Rui, and C. W. Chen. Network morphism. In International Conference on

Machine Learning, pages 564–572. PMLR, 2016. (Cited on page 8.)

[35] S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance
of convolutional neural networks via attention transfer. In International Conference on Learning
Representations, 2017. (Cited on page 4.)

[36] C. Zhang, S. Bengio, and Y. Singer. Are all layers created equal? In Workshop on Identifying

and Understanding Deep Learning Phenomena, 2019. (Cited on page 5.)

[37] Z.-H. Zhou. Ensemble methods: Foundations and algorithms. CRC press, 2012. (Cited on page 1.)

27

