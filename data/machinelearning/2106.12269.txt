Improved Acyclicity Reasoning for Bayesian Network Structure Learning with
Constraint Programming

Fulya Tr¨osser1∗ , Simon de Givry1 and George Katsirelos2
1Universit´e de Toulouse, INRAE, UR MIAT, F-31320, Castanet-Tolosan, France
2UMR MIA-Paris, INRAE, AgroParisTech, Univ. Paris-Saclay, 75005 Paris, France
{fulya.ural, simon.de-givry}@inrae.fr, gkatsi@gmail.com

1
2
0
2

n
u
J

3
2

]
I

A
.
s
c
[

1
v
9
6
2
2
1
.
6
0
1
2
:
v
i
X
r
a

Abstract

Bayesian networks are probabilistic graphical mod-
els with a wide range of application areas includ-
ing gene regulatory networks inference, risk anal-
ysis and image processing. Learning the structure
of a Bayesian network (BNSL) from discrete data
is known to be an NP-hard task with a superexpo-
nential search space of directed acyclic graphs. In
this work, we propose a new polynomial time algo-
rithm for discovering a subset of all possible cluster
cuts, a greedy algorithm for approximately solving
the resulting linear program, and a generalised arc
consistency algorithm for the acyclicity constraint.
We embed these in the constraint programming-
based branch-and-bound solver CPBayes and show
that, despite being suboptimal, they improve per-
formance by orders of magnitude. The resulting
solver also compares favourably with GOBNILP, a
state-of-the-art solver for the BNSL problem which
solves an NP-hard problem to discover each cut and
solves the linear program exactly.

1 Introduction
Towards the goal of explainable AI, Bayesian networks offer
a rich framework for probabilistic reasoning. Bayesian Net-
work Structure Learning (BNSL) from discrete observations
corresponds to ﬁnding a compact model which best explains
the data. It deﬁnes an NP-hard problem with a superexponen-
tial search space of Directed Acyclic Graphs (DAG). Several
constraint-based (exploiting local conditional independence
tests) and score-based (exploiting a global objective formula-
tion) BNSL methods have been developed in the past.

Complete methods for score-based BNSL include dynamic
[Silander and Myllym¨aki, 2006],
heuristic
programming
Fan and Yuan, 2015],
[Yuan and Malone, 2013;
search
maximum satisﬁability [Berg et al., 2014],
branch-and-
cut [Bartlett and Cussens, 2017] and constraint program-
ming [van Beek and Hoffmann, 2015]. Here, we focus on
the latter two.

GOBNILP [Bartlett and Cussens, 2017] is a state-of-the-
art solver for BNSL. It implements branch-and-cut in an in-

∗Contact Author

teger linear programming (ILP) solver. At each node of the
branch-and-bound tree, it generates cuts that improve the lin-
ear relaxation. A major class of cuts generated by GOBNILP
are cluster cuts, which identify sets of parent sets that cannot
be used together in an acyclic graph. In order to ﬁnd cluster
cuts, GOBNILP solves an NP-hard subproblem created from
the current optimal solution of the linear relaxation.

CPBayes [van Beek and Hoffmann, 2015] is a constraint
programming-based (CP) method for BNSL. It uses a CP
model
that exploits symmetry and dominance relations
present in the problem, subproblem caching, and a pattern
database to compute lower bounds, adapted from heuris-
tic search [Fan and Yuan, 2015].
van Beek and Hoffmann
showed that CPBayes is competitive with GOBNILP in many
In contrast to GOBNILP, the inference mech-
instances.
anisms of CPBayes are very lightweight, which allows it
to explore many orders of magnitude more nodes per time
unit, even accounting for the fact that computing the pattern
databases before search can sometimes consume considerable
time. On the other hand, the lightweight pattern-based bound-
ing mechanism can take into consideration only limited in-
formation about the current state of the search. Speciﬁcally,
it can take into account the current total ordering implied by
the DAG under construction, but no information that has been
derived about the potential parent sets of each vertex, i.e., the
current domains of parent set variables.

In this work, we derive a lower bound that is computation-
ally cheaper than that computed by GOBNILP. We give in
Section 3 a polynomial-time algorithm that discovers a class
of cluster cuts that provably improve the linear relaxation. In
Section 4, we give a greedy algorithm for solving the linear
relaxation, inspired by similar algorithms for MaxSAT and
Weighted Constraint Satisfaction Problems (WCSP). Finally,
in Section 5 we give an algorithm that enforces generalised
arc consistency on the acyclicity constraint, based on pre-
vious work by van Beek and Hoffmann, but with improved
complexity and practical performance. In Section 6, we show
that our implementation of these techniques in CPBayes leads
to signiﬁcantly improved performance, both in the size of the
search tree explored and in runtime.

2 Preliminaries
We give here only minimal background on (inte-
program-
ger)

programming

constraint

linear

and

 
 
 
 
 
 
and

ming,
reader
refer
[Papadimitriou and Steiglitz, 1998;
for more.

the

to

existing
literature
Rossi et al., 2006]

Constraint Programming
A constraint satisfaction problem (CSP) is a tuple hV, D, Ci,
where V is a set of variables, D is a function mapping vari-
ables to domains and C is a set of constraints. An assignment
A to V ′ ⊆ V is a mapping from each v ∈ V ′ to D(v). A
complete assignment is an assignment to V . If an assignment
maps v to a, we say it assigns v = a. A constraint is a pair
hS, P i, where S ⊆ V is the scope of the constraint and P is
a predicate over QV ∈S D(V ) which accepts assignments to
S that satisfy the constraint. For an assignment A to S′ ⊇ S,
let A′|S be the restriction of A to S. We say that A satisﬁes
c = hS, P i if A|S satisﬁes c. A problem is satisﬁed by A if
A satisﬁes all constraints.

For a constraint c = hS, P i and for v ∈ S, a ∈ D(v),
v = a is generalized arc consistent (GAC) for c if there exists
an assignment A that assigns v = a and satisﬁes c. If for all
v ∈ S, a ∈ D(v), v = a is GAC for c, then c is GAC. If
all constraints are GAC, the problem is GAC. A constraint is
associated with an algorithm fc, called the propagator for c,
that removes (or prunes) values from the domains of variables
in S that are not GAC.

CSPs are typically solved by backtracking search, using
propagators to reduce domains at each node and avoid parts
of the search tree that are proved to not contain any solutions.
Although CSPs are decision problems, the technology can be
used to solve optimization problems like BNSL by, for ex-
ample, using branch-and-bound and embedding the bounding
part in a propagator. This is the approach used by CPBayes.

Integer Linear Programming
A linear program (LP) is the problem of ﬁnding

min{cT x | x ∈ Rn ∧ Ax ≥ b ∧ x ≥ 0}

where c and b are vectors, A is a matrix, and x is a vector
of variables. A feasible solution of this problem is one that
satisﬁes x ∈ Rn∧Ax ≥ b∧x ≥ 0 and an optimal solution is a
feasible one that minimizes the objective function cT x. This
can be found in polynomial time. A row Ai corresponds to
an individual linear constraint and a column AT
j to a variable.
The dual of a linear program P in the above form is another
linear program D:

max{bT y | y ∈ Rm ∧ AT y ≤ c ∧ y ≥ 0}

where A, b, c are as before and y is the vector of dual vari-
ables. Rows of the dual correspond to variables of the primal
and vice versa. The objective value of any dual feasible so-
lution is a lower bound on the optimum of P . When P is
satisﬁable, its dual is also satisﬁable and the values of their
optima meet. For a given feasible solution ˆx of P , the slack
of constraint i is slackˆx(i) = AT
i x−bi. Given a dual feasible
solution ˆy, slackD
ˆy (i) is the reduced cost of primal variable
i, rcˆy(i). The reduced cost rcˆy(i) is interpreted as a lower
bound on the amount that the dual objective would increase
over bT ˆy if xi is forced to be non-zero in the primal.

An integer linear program (ILP) is a linear program in
which we replace the constraint x ∈ Rn by x ∈ Zn and it
is an NP-hard optimization problem.

Bayesian Networks
A Bayesian network is a directed graphical model B =
hG, P i where G = hV, Ei is a directed acyclic graph (DAG)
called the structure of B and P are its parameters. A BN
describes a normalised joint probability distribution. Each
vertex of the graph corresponds to a random variable and
presence of an edge between two vertices denotes direct con-
ditional dependence. Each vertex vi is also associated with
a Conditional Probability Distribution P (vi | parents(vi)).
The CPDs are the parameters of B.

The approach which we use here for learning a BN from
data is the score-and-search method. Given a set of mul-
tivariate discrete data I = {I1, . . . , IN }, a scoring func-
tion σ(G | I) measures the quality of the BN with un-
derlying structure G. The BNSL problem asks to ﬁnd a
structure G that minimises σ(G | I) for some scoring
function σ and it is NP-hard [Chickering, 1995]. Several
scoring functions have been proposed for this purpose, in-
cluding BDeu [Buntine, 1991; Heckerman et al., 1995] and
BIC [Schwarz, 1978; Lam and Bacchus, 1994]. These func-
tions are decomposable and can be expressed as the sum
of local scores which only depend on the set of parents
(from now on, parent set) of each vertex: σF (G | I) =
Pv∈V σv
In
this setting, we ﬁrst compute local scores and then com-
pute the structure of minimal score. Although there are
potentially an exponential number of local scores that have
to be computed, the number of parent sets actually con-
sidered is often much smaller, for example because we re-
strict the maximum cardinality of parent sets considered or
we exploit dedicated pruning rules [de Campos and Ji, 2010;
de Campos et al., 2018]. We denote P S(v) the set of candi-
date parent sets of v and P S−C(v) those parent sets that do
not intersect C. In the following, we assume that local scores
are precomputed and given as input, as is common in similar
works. We also omit explicitly mentioning I or F , as they are
constant for solving any given instance.

F (parents(v) | I) for F ∈ {BDeu, BIC}.

Let C be a set of vertices of a graph G. C is a violated
cluster if the parent set of each vertex v ∈ C intersects C.
Then, we can prove the following property:
Property 1. A directed graph G = hV, Ei is acyclic if and
only if it contains no violated clusters, i.e., for all C ⊆ V ,
there exists v ∈ C, such that parents(v) ∩ C = ∅.

The GOBNILP solver [Bartlett and Cussens, 2017] formu-

lates the problem as the following 0/1 ILP:

min X

σv(S)xv,S

v∈V,S⊆V \{v}

s.t. X

xv,S = 1

S∈P S(v)

(1)

∀v ∈ V

(2)

X
v∈C,S∈P S−C (v)

xv,S ∈ {0, 1}

xv,S ≥ 1

∀C ⊆ V

(3)

∀v ∈ V, S ∈ P S(v)

(4)

Algorithm 1: Acyclicity Checker
acycChecker (V, D)
order ← {}
changes ← true
while changes do

changes ← f alse
foreach v ∈ V \ order do

if ∃S ∈ D(v) s.t. (S ∩ V ) ⊆ order then

1

order ← order + v
changes ← true

return order

The CPBayes

This ILP has a 0/1 variable xv,S for each candidate parent
set S of each vertex v where xv,S = 1 means that S is the par-
ent set of v. The objective (1) directly encodes the decompo-
sition of the scoring function. The constraint (2) asserts that
exactly one parent set is selected for each random variable.
Finally, the cluster inequalities (3) are violated when C is a
violated cluster. We denote the cluster inequality for cluster
C as cons(C) and the 0/1 variables involved as varsof (C).
As there is an exponential number of these, GOBNILP gen-
erates only those that improve the current linear relaxation
and they are referred to as cluster cuts. This itself is an NP-
hard problem [Cussens et al., 2017], which GOBNILP also
encodes and solves as an ILP. Interestingly, these inequali-
ties are facets of the BNSL polytope [Cussens et al., 2017],
so stand to improve the relaxation signiﬁcantly.
solver

[van Beek and Hoffmann, 2015]
models BNSL as a constraint program. The CP model has a
parent set variable for each random variable, whose domain
is the set of possible parent sets, as well as order variables,
which give a total order of the variables that agrees with
the partial order implied by the DAG. The objective is the
same as (1).
It includes channelling constraints between
the set of variables and various symmetry breaking and
dominance constraints.
It computes a lower bound using
two separate mechanisms: a component caching scheme
and a pattern database that is computed before search and
holds the optimal graphs for all orderings of partitions
of the variables. Acyclicity is enforced using a global
constraint with a bespoke propagator. The main routine
of the propagator is acycChecker (Algorithm 1), which
returns an order of all variables if the current set of domains
of the parent set variables may produce an acyclic graph, or
a partially completed order if the constraint is unsatisﬁable.
This algorithm is based on Property 1.

Brieﬂy, the algorithm takes the domains of the parent set
variables as input and greedily constructs an ordering of the
variables, such that if variable v is later in the order than v′,
then v /∈ parents(v′)1. It does so by trying to pick a parent
set S for an as yet unordered vertex such that S is entirely
contained in the set of previously ordered vertices2.
If all
assignments yield cyclic graphs, it will reach a point where

1We treat order as both a sequence and a set, as appropriate.
2When propagating the acyclicity constraint it always holds that
a ∩ V = a, so this statement is true. In section 3.1, we use the
algorithm in a setting where this is not always the case.

all remaining vertices are in a violated cluster in all possible
graphs, and it will return a partially constructed order. If there
exists an assignment that gives an acyclic graph, it will be
possible by property 1 to select from a variable in V \ order
a parent set which does not intersect V \ order, hence is a
subset of order. The value S chosen for each variable in
line 1 also gives a witness of such an acyclic graph.

An immediate connection between the GOBNILP and CP-
Bayes models is that the ILP variables xv,S, ∀S ∈ P S(v) are
the direct encoding [Walsh, 2000] of the parent set variables
of the CP model. Therefore, we use them interchangeably,
i.e., we can refer to the value S in D(v) as xv,S.

3 Restricted Cluster Detection
One of the issues hampering the performance of CPBayes is
that it computes relatively poor lower bounds at deeper levels
of the search tree. Intuitively, as the parent set variable do-
mains get reduced by removing values that are inconsistent
with the current ordering, the lower bound computation dis-
cards more information about the current state of the problem.
We address this by adapting the branch-and-cut approach of
GOBNILP. However, instead of ﬁnding all violated cluster
inequalities that may improve the LP lower bound, we only
identify a subset of them.

Consider the linear relaxation of the ILP (1)– (4), restricted
to a subset C of all valid cluster inequalities, i.e., with equa-
tion (4) replaced by 0 ≤ xv,S ≤ 1∀v ∈ V, S ∈ P S(v) and
with equation (3) restricted only to clusters in C. We denote
this LPC. We exploit the following property of this LP.
Theorem 1. Let ˆy be a dual feasible solution of LPC with
dual objective o. Then, if C is a cluster such that C /∈ C
and the reduced cost rc of all variables varsof (C) is greater
than 0, there exists a dual feasible solution ˆy of LPC∪C with
dual objective o′ ≥ o + minrc(C) where minrc(C) =
minx∈varsof (C) rcˆy(x).

Proof. The only difference from LPC to LPC∪C is the ex-
tra constraint cons(C) in the primal and corresponding dual
variable yC . In the dual, yC only appears in the dual con-
straints of the variables varsof (C) and in the objective, al-
ways with coefﬁcient 1. Under the feasible dual solution
ˆy∪{yC = 0}, these constraints have slack at least minrc(C),
by the deﬁnition of reduced cost. Therefore, we can set
ˆy = ˆy ∪ {yC = minrc(C)}, which remains feasible and
has objective o′ = o + minrc(C), as required.

Theorem 1 gives a class of cluster cuts, which we call RC-
clusters, for reduced-cost clusters, guaranteed to improve the
lower bound. Importantly, this requires only a feasible, per-
haps sub-optimal, solution.

Example 1 (Running example). Consider a BNSL instance
with domains as shown in Table 1 and let C = ∅. Then, ˆy = 0
leaves the reduced cost of every variable to exactly its primal
objective coefﬁcient. The corresponding ˆx assigns 1 to vari-
ables with reduced cost 0 and 0 to everything else. These are
both optimal solutions, with cost 0 and ˆx is integral, so it is
also a solution of the corresponding ILP. However, it is not a
solution of the BNSL, as it contains several cycles, including

2

0

1

Variable Domain Value Cost
0
0
6
0
10
0
5
0
1
2
3

{2}
{2, 4}
{}
{1, 3}
{}
{0}
{}
{2, 3}
{3}
{2}
{}

3

4

Table 1: BNSL instance used as running example.

Algorithm 2: Lower bound computation with RC-
clusters
lowerBoundRC (V, D, C)
ˆy ← DualSolve(LPC(D))
while True do

C ← V \ acycChecker(V, Drc
if C = ∅ then

C,ˆy)

2

3

return hcost(ˆy), Ci

C ← minimise(C)
C ← C ∪ {C}
ˆy ← DualImprove(ˆy, LPC(D), C)

C = {0, 2, 3}. The cluster inequality cons(C) is violated in
the primal and allows the dual bound to be increased.

We consider the problem of discovering RC-clusters within
the CP model of CPBayes. First, we introduce the nota-
tion LPC(D) which is LPC with the additional constraint
xv,S = 0 for each S /∈ D(v). Conversely, Drc
C,ˆy is the set
of domains minus values whose corresponding variable in
LPC(D) has non-zero reduced cost under ˆy, i.e., Drc
C,ˆy = D′
where D′(v) = {S | S ∈ D(v) ∧ rcˆy(xv,S) = 0}. With
this notation, for values S /∈ D(v), xv,S = 1 is infeasible in
LPC(D), hence effectively rcˆy(xv,S) = ∞.
Theorem 2. Given a collection of clusters C, a set of domains
D and ˆy, a feasible dual solution of LPC(D), there exists
an RC-cluster C /∈ C if and only if Drc
C,ˆy does not admit an
acyclic assignment.

Proof. (⇒) Let C be such a cluster. Since for all xv,S ∈
varsof (C), none of these are in Drc
C,ˆy, so cons(C) is violated
and hence there is no acyclic assignment.

(⇐) Consider once again acycChecker, in Algorithm 1.
When it fails to ﬁnd a witness of acyclicity, it has reached
a point where order ( V and for the remaining variables
C = V \ order, all allowed parent sets intersect C. So if
acycChecker is called with Drc
C,ˆy, all values in varsof (C)
have reduced cost greater than 0, so C is an RC-cluster.

Theorem 2 shows that detecting unsatisﬁability of Drc
C,ˆy is
enough to ﬁnd an RC-cluster. Its proof also gives a way to
extract such a cluster from acycChecker.

Algorithm 2 shows how theorems 1 and 2 can be used
It is given the current set of

to compute a lower bound.

domains and a set of clusters as input.
It ﬁrst solves the
dual of LPC(D), potentially suboptimally. Then, it uses
acycChecker iteratively to determine whether there exists
an RC-cluster C under the current dual solution ˆy.
If that
cluster is empty, there are no more RC-clusters, and it termi-
nates and returns a lower bound equal to the cost of ˆy under
LPC(D) and an updated pool of clusters. Otherwise, it min-
imises C (see section 3.1), adds it to the pool of clusters and
solves the updated LP. It does this by calling DualImprove,
which solves LPC(D) exploiting the fact that only the cluster
inequality cons(C) has been added.
Example 2. Continuing our example, consider the behav-
ior of acycChecker with domains Drc
∅,ˆy after the initial dual
solution ˆy = 0. Since the empty set has non-zero reduced
cost for all variables, acycChecker fails with order = {},
hence C = V . We postpone discussion of minimization for
now, other than to observe that C can be minimized to C1 =
{1, 2}. We add cons(C1) to the primal LP and set the dual
variable of C1 to 6 in the new dual solution ˆy1. The reduced
costs of x1,{} and x2,{} are decreased by 6 and, importantly,
In the next iteration of lowerBoundRC,
rcˆy1 (x1,{}) = 0.
acycChecker is invoked on Drc
{C1},ˆy1 and returns the clus-
ter {0, 2, 3, 4}. This is minimized to C2 = {0, 2, 3}. The
parent sets in the domains of these variables that do not in-
tersect C2 are x2,{} and x3,{}, so minrc(C2) = 4, so we add
cons(C2) to the primal and we set the dual variable of C2
to 4 in ˆy2. This brings the dual objective to 10. The reduced
cost of x2,{} is 0, so in the next iteration acycChecker runs
on Drc
{C1,C2},ˆy2 and succeeds with the order {2, 0, 3, 4, 1}, so
the lower bound cannot be improved further. This also hap-
pens to be the cost of the optimal structure.

Theorem 3. Algorithm 2 terminates but is not conﬂuent.

Proof. It terminates because there is a ﬁnite number of cluster
inequalities and each iteration generates one. In the extreme,
all cluster inequalities are in C and the test at line 3 succeeds,
terminating the algorithm.

To see that it is not conﬂuent, consider an example with 3
clusters C1 = {v1, v2}, C2 = {v2, v3} and C3 = {v3, v4}
and assume that the minimum reduced cost for each cluster is
unit and comes from x2,{4} and x3,{1}, i.e., the former value
has minimum reduced cost for C1 and C2 and the latter for C2
and C3. Then, if minimisation generates ﬁrst C1, the reduced
cost of x3,{1} is unaffected by DualImprove, so it can then
discover C3, to get a lower bound of 2. On the other hand,
if minimisation generates ﬁrst C2, the reduced costs of both
x2,{4} and x3,{1} are decreased to 0 by DualImprove, so
neither C1 nor C3 are RC-clusters under the new dual solution
and the algorithm terminates with a lower bound of 1.

Related Work. The idea of performing propagation on the
subset of domains that have reduced cost 0 has been used
in the VAC algorithm for WCSPs [Cooper et al., 2010]. Our
method is more light weight, as it only performs propagation
on the acyclicity constraint, but may give worse bounds. The
bound update mechanism in the proof of theorem 1 is also
simpler than VAC and more akin to the “disjoint core phase”
in core-guided MaxSAT solvers [Morgado et al., 2013].

3.1 Cluster Minimisation

It is crucial for the quality of the lower bound produced by Al-
gorithm 2 that the RC-clusters discovered by acycChecker
are minimised, as the following example shows. Empirically,
omitting minimisation rendered the lower bound ineffective.

Example 3. Suppose that we attempt to use lowerBoundRC
without cluster minimization.
Then, we use the cluster
given by acycChecker, C1 = {0, 1, 2, 3, 4}. We have
minrc(C1) = 3, given from the empty parent set value of
all variables. This brings the reduced cost of x4,{} to 0.
It then proceeds to ﬁnd the cluster C2 = {0, 1, 2, 3} with
minrc(C2) = 2 and decrease the reduced cost of x3,{} to
0, then C3 = {0, 1, 2} with minrc(C3) = 1, which brings
the reduced cost of x1,{} to 0. At this point, acycChecker
succeeds with the order {4, 3, 1, 2, 0} and lowerBoundRC re-
turns a lower bound of 6, compared to 10 with minimization.
The order produced by acycChecker also disagrees with the
optimum structure.

Therefore, when we get an RC-cluster C at line 2 of algo-
rithm 2, we want to extract a minimal RC-cluster (with re-
spect to set inclusion) from C, i.e., a cluster C′ ⊆ C, such
that for all ∅ ⊂ C′′ ⊂ C′, C′′ is not a cluster.

Minimisation problems like this are handled with an ap-
propriate instantiation of QuickXPlain [Junker, 2004]. These
algorithms ﬁnd a minimal subset of constraints, not variables.
We can pose this as a constraint set minimisation problem by
implicitly treating a variable as the constraint “this variable is
assigned a value” and treating acyclicity as a hard constraint.
However, the property of being an RC-cluster is not mono-
tone. For example, consider the variables {v1, v2, v3, v4}
and ˆy such that the domains restricted to values with 0 re-
duced cost are {{v2}}, {{v1}}, {{v4}}, {{v3}}, respectively.
Then {v1, v2, v3, v4}, {v1, v2} and {v3, v4} are RC-clusters.
but {v1, v2, v3} is not because the sole value in the do-
main of v3 does not intersect {v1, v2, v3}. We instead min-
imise the set of variables that does not admit an acyclic so-
lution and hence contains an RC-cluster. A minimal un-
satisﬁable set that contains a cluster is an RC-cluster, so
this allows us to use the variants of QuickXPlain. We fo-
cus on RobustXPlain, which is called the deletion-based al-
gorithm in SAT literature for minimising unsatisﬁable sub-
sets [Marques-Silva and Menc´ıa, 2020]. The main idea of the
algorithm is to iteratively pick a variable and categorise it as
either appearing in all minimal subsets of C, in which case
we mark it as necessary, or not, in which case we discard
it. To detect if a variable appears in all minimal unsatisﬁ-
able subsets, we only have to test if omitting this variable
yields a set with no unsatisﬁable subsets, i.e., with no vio-
lated clusters. This is given in pseudocode in Algorithm 3.
This exploits a subtle feature of acycChecker as described
in Algorithm 1: if it is called with a subset of V, it does not
try to place the missing variables in the order and allows par-
ent sets to use these missing variables. Omitting variables
from the set given to acycChecker acts as omitting the con-
straint that these variables be assigned a value. The com-
plexity of MinimiseCluster is O(n3d), where n = |V | and
d = maxv∈V |D(v)|, a convention we adopt throughout.

Algorithm 3: Find a minimal RC-cluster subset of C
MinimiseCluster (V, D, C)
N = ∅
while C 6= ∅ do
Pick c ∈ C
C ← C \ {c}
C′ ← V \ acycChecker(N ∪ C, D)
if C′ = ∅ then

N ← N ∪ {c}

else

C ← C′ \ N

return N

4 Solving the Cluster LP

Solving a linear program is in polynomial time, so in principle
DualSolve can be implemented using any of the commercial
or free software libraries available for this. However, solving
this LP using a general LP solver is too expensive in this set-
ting. As a data point, solving the instance steel BIC with
our modiﬁed solver took 25,016 search nodes and 45 sec-
onds of search, and generated 5, 869 RC-clusters. Approx-
imately 20% of search time was spent solving the LP using
the greedy algorithm that we describe in this section. CPLEX
took around 70 seconds to solve LPC with these cluster in-
equalities once. While this data point is not proof that solving
the LP exactly is too expensive, it is a pretty strong indicator.
We have also not explored nearly linear time algorithms for
solving positive LPs [Allen-Zhu and Orecchia, 2015].

Our greedy algorithm is derived from theorem 1. Observe
ﬁrst that LPC with C = ∅, i.e., only with constraints (2) has
optimal dual solution ˆy0 that assigns the dual variable yv of
PS∈P S(v) xv,S = 1 to minS∈P S(v) σv(S). That leaves at
least one of xv,S, S ∈ P S(v) with reduced cost 0 for each
v ∈ V . DualSolve starts with ˆy0 and then iterates over C.
Given ˆyi−1 and a cluster C, it sets ˆyi = ˆyi−1 if C is not
an RC-cluster. Otherwise, it increases the lower bound by
c = minrc(C) and sets ˆyi = ˆyi−1 ∪ {yC = c}. It remains to
specify the order in which we traverse C.

We sort clusters by increasing size |C|, breaking ties by
decreasing minimum cost of all original parent set values
in varsof (C). This favours ﬁnding non-overlapping cluster
cuts with high minimum cost. In section 6, we give experi-
mental evidence that this computes better lower bounds.

DualImprove can be implemented by discarding previ-
ous information and calling DualSolve(LPC(D)). Instead,
it uses the RC-cluster C to update the solution without revis-
iting previous clusters.

In terms of implementation, we store varsof (C) for each
cluster, not cons(C). During DualSolve, we maintain the
reduced costs of variables rather than the dual solution, oth-
erwise computing each reduced cost would require iterating
over all cluster inequalities that contain a variable. Speciﬁ-
cally, we maintain ∆v,S = σv(S) − rcˆy(xv,S). In order to
test whether a cluster C is an RC-cluster, we need to com-
pute minrc(C). To speed this up, we associate with each
stored cluster a support pair (v, S) corresponding to the last

minimum cost found. If rcˆy(v, S) = 0, the cluster is not an
RC-cluster and is skipped. Moreover, parent set domains are
sorted by increasing score σv(S), so S ≻ S′ ⇐⇒ σv(S) >
σv(S′). We also maintain the maximum amount of cost
transferred to the lower bound, ∆max
= maxS∈D(v) ∆v,S
for every v ∈ V . We stop iterating over D(v) as soon as
σv(S) − ∆max
is greater than or equal to the current mini-
mum because ∀S′ ≻ S, σv(S′) − ∆v,b ≥ σv(S) − ∆max
.
In practice, on very large instances 97.6% of unproductive
clusters are detected by support pairs and 8.6% of the current
domains are visited for the rest3.

v

v

v

To keep a bounded-memory cluster pool, we discard fre-
quently unproductive clusters. We throw away large clusters
with a productive ratio
#productive+#unproductive smaller
than
1,000 . Clusters of size 10 or less are always kept because
they are often more productive and their number is bounded.

#productive

1

5 GAC for the Acyclicity Constraint

van

Beek

showed

Previously,
and
Hoffmann[van Beek and Hoffmann, 2015]
that
using acycChecker as a subroutine, one can construct a
GAC propagator for the acyclicity constraint by probing,
i.e., detecting unsatisﬁability after assigning each individual
value and pruning those values that lead to unsatisﬁability.
acycChecker is in O(n2d), so this gives a GAC propagator
in O(n3d2). We show here that we can enforce GAC in time
O(n3d), a signiﬁcant improvement given that d is usually
much larger than n.

Suppose acycChecker ﬁnds a witness of acyclicity and
returns the order O = {v1, . . . , vn}. Every parent set S of
a variable v that is a subset of {v′ | v′ ≺O v} is supported
by O. We call such values consistent with O. Consider now
S ∈ D(vi) which is inconsistent with O, therefore we have to
probe to see if it is supported. We know that during the probe,
nothing forces acycChecker to deviate from {v1, . . . , vi−1}.
So in a successful probe, acycChecker constructs a new or-
der O′ which is identical to O in the ﬁrst i − 1 positions and
in which it moves vi further down. Then all values consistent
with O′ are supported. This suggests that instead of probing
each value, we can probe different orders.

Acyclicity-GAC, shown in Algorithm 4, exploits this in-
sight. It ensures ﬁrst that acycChecker can produce a valid
order O. For each variable v, it constructs a new order O′
from O so that v is as late as possible. It then prunes all par-
ent set values of v that are inconsistent with O′.
Theorem 4. Algorithm 4 enforces GAC on the Acyclicity con-
straint in O(n3d).

Proof. Let v ∈ V and S ∈ D(v). Let O = {O1, . . . , On}
and Q = {Q1, . . . , Qn} be two valid orders such that O
does not support S whereas Q does. It is enough to show
that we can compute from O a new order O′ that supports
S by pushing v towards the end. Let Oi = Qj = v and
let Op = {O1, . . . , O(i−1)}, Qp = {Q1, . . . , Q(j−1)} and
Os = {Oi+1, . . . , On}.

3See the supplementary material for more.

Algorithm 4: GAC propagator for acyclicity
Acyclicity-GAC (V, D)
O ← acycChecker(V, D)
if O ( V then

return Failure
foreach v ∈ V do

changes ← true
i ← O−1(v)
pref ix ← {O1, . . . , Oi−1}
while changes do

4

changes ← f alse
foreach w ∈ O \ (pref ix ∪ {v}) do

if ∃S ∈ D(w) s.t. S ⊆ pref ix then

pref ix ← pref ix ∪ {w}
changes ← true
Prune {S | S ∈ D(v) ∧ S * pref ix}

return Success

Let O′ be the order Op followed by Qp, followed by v,
followed by Os, keeping only the ﬁrst occurrence of each
variable when there are duplicates. O′ is a valid order: Op
is witnessed by the assignment that witnesses O, Qp by the
assignment that witnesses Q, v by S (as in Q) and Os by the
assignment that witnesses O. It also supports S, as required.
Complexity is dominated by repeating O(n) times the loop
at line 4, which is a version of acycChecker so has complex-
ity O(n2d) for a total O(n3d).

6 Experimental Results
6.1 Benchmark Description and Settings
The datasets come from the UCI Machine Learning Reposi-
tory4, the Bayesian Network Repository5, and the Bayesian
Network Learning and Inference Package6. Local scores
were computed from the datasets using B. Malone’s code7.
BDeu and BIC scores were used for medium size instances
(less than 64 variables) and only BIC score for large instances
(above 64 variables). The maximum number of parents was
limited to 5 for large instances (except for accidents.test
with maximum of 8), a high value that allows even learning
complex structures [Scanagatta et al., 2015]. For example,
jester.test has 100 random variables, a sample size of
4, 116 and 770, 950 parent set values. For medium instances,
no restriction was applied except for some BDeu scores (limit
sets to 6 or 8 to complete the computation of the local scores
within 24 hours of CPU-time [Lee and van Beek, 2017]).

We have modiﬁed the C++ source of CPBayes v1.1 by
adding our lower bound mechanism and GAC propagator.
We call the resulting solver ELSA and have made it publicly
available. For the evaluation, we compare with GOBNILP
v1.6.3 using SCIP v3.2.1 with cplex v12.7.0. All compu-
tations were performed on a single core of Intel Xeon E5-
2680 v3 at 2.50 GHz and 256 GB of RAM with a 1-hour

4http://archive.ics.uci.edu/ml
5http://www.bnlearn.com/bnrepository
6https://ipg.idsia.ch/software.php?id=132
7http://urlearning.org

Instance
carpo100 BIC
alarm1000 BIC
ﬂag BDe
wdbc BIC
kdd.ts
steel BIC
kdd.test
mushroom BDe
bnetﬂix.ts
plants.test
jester.ts
accidents.ts
plants.valid
jester.test
bnetﬂix.test
bnetﬂix.valid
accidents.test

|V | P |ps(v)| GOBNILP
0.6
60
1.2
37
4.4
29
99.8
31
327.6
64
28
†
64
1521.7
23
†
100
†
69
†
100
†
1274.0
111
69
†
100
†
100
†
100
†
4975.6
111

424
1003
1325
14614
43584
93027
152873
438186
446406
520148
531961
568160
684141
770950
1103968
1325818
1425966

CPBayes
78.5 (29.7)
204.2 (172.9)
19.0 (18.1)
629.8 (576.6)
†
1270.9 (1218.9)
†
176.4 (56.0)
629.0 (431.4)
†
†
†
†
†
3525.2 (3283.8)
1456.6 (1097.0)
†

ELSA
40.6 (0.0)
27.8 (0.7)
0.9 (0.1)
48.9 (1.6)
1314.5 (158.2)
98.0 (49.2)
1475.3 (120.6)
135.4 (33.7)
1065.1 (878.4)
18981.9 (17224.0)
10166.0 (9697.9)
2238.7 (904.5)
12347.6 (8509.7)
17637.8 (16979.2)
8197.7 (7975.6)
9282.0 (8950.3)
3661.7 (641.5)

ELSA \ GAC
40.7 (0.0)
28.8 (1.5)
0.9 (0.1)
49.1 (1.7)
1405.4 (239.5)
99.2 (50.1)
1515.9 (128.5)
137.0 (35.0)
1111.4 (931.0)
30791.2 (29073.0)
14915.9 (14470.1)
2260.3 (986.1)
19853.1 (15963.1)
21284.0 (20661.9)
8057.3 (7841.4)
10220.5 (9898.4)
4170.1 (1213.6)

ELSA chrono
40.6 (0.0)
29.9 (2.7)
1.3 (0.5)
50.3 (3.1)
1663.2 (512.4)
130.0 (81.2)
1492.4 (109.5)
133.7 (31.9)
1132.4 (936.3)
†
23877.6 (23325.7)
2221.1 (904.8)
†
†
7915.0 (7686.3)
9619.7 (9257.4)
3805.2 (687.6)

Table 2: Comparison of ELSA against GOBNILP and CPBayes. Time limit for instances above the line is 1h, for the rest 10h. Instances are
sorted by increasing total domain size. For variants of CPBayes we report in parentheses time spent in search, after preprocessing ﬁnishes. †
indicates a timeout.

(resp. 10-hour) CPU time limit for medium (resp.
large)
size instances. We used default settings for GOBNILP with
no approximation in branch-and-cut (limits/gap = 0). We
used the same settings in CPBayes and ELSA for their pre-
processing phase (partition lower bound sizes lmin, lmax and
local search number of restarts rmin, rmax). We used two
different settings depending on problem size |V |:
lmin =
20, lmax = 26, rmin = 50, rmax = 500 if |V | ≤ 64, else
lmin = 20, lmax = 20, rmin = 15, rmax = 30.

6.2 Evaluation

In Table 2 we present the runtime to solve each instance to
optimality with GOBNILP, CPBayes, and ELSA with default
settings, without the GAC algorithm and without sorting the
cluster pool (leaving clusters in chronological order, rather
than the heuristic ordering presented in Section 4). For the in-
stances with kV k ≤ 64 (resp. > 64), we had a time limit of 1
hour (resp. 10 hours). We exclude instances that were solved
within the time limit by GOBNILP and have a search time of
less than 10 seconds for CPBayes and all variants of ELSA.
We also exclude 8 instances that were not solved to optimality
by any method. This leaves us 17 instances to analyse here
out of 69 total. More details are given in the supplemental
material, available from the authors’ web pages.

to

to

proven

GOBNILP. CPBayes
competitive
be

Comparison
al-
was
ready
to GOBNILP
[van Beek and Hoffmann, 2015]. Our results in Table 2
conﬁrm this while showing that neither is clearly better.
When it comes to our solver ELSA, for all the variants, all
instances solved within the time limit by GOBNILP are
solved, unlike CPBayes. On top of that, ELSA solves 9 more
instances optimally.

Comparison to CPBayes. We have made some low-level
performance improvements in preprocessing of CPBayes, so
for a more fair comparison, we should compare only the

search time, shown in parentheses. ELSA takes several or-
ders of magnitude less search time to optimally solve most
instances, the only exception being the bnetflix instances.
ELSA also proved optimality for 8 more instances within the
time limit.

Gain from GAC. The overhead of GAC pays off as the in-
stances get larger. While we do not see either a clear im-
provement nor a downgrade for the smaller instances, search
time for ELSA improves by up to 47% for larger instances
compared to ELSA \ GAC.

Gain from Cluster Ordering. We see that the ordering
heuristic improves the bounds computed by our greedy dual
LP algorithm signiﬁcantly. Compared to not ordering the
clusters, we see improved runtime throughout and 3 more in-
stances solved to optimality.

7 Conclusion

We have presented a new set of inference techniques for
BNSL using constraint programming, centered around the ex-
pression of the acyclicity constraint. These new techniques
exploit and improve on previous work on linear relaxations of
the acyclicity constraint and the associated propagator. The
resulting solver explores a different trade-off on the axis of
strength of inference versus speed, with GOBNILP on one
extreme and CPBayes on the other. We showed experimen-
tally that the trade-off we achieve is a better ﬁt than either ex-
treme, as our solver ELSA outperforms both GOBNILP and
CPBayes. The major obstacle towards better scalability to
larger instances is the fact that domain sizes grow exponen-
tially with the number of variables. This is to some degree
unavoidable, so our future work will focus on exploiting the
structure of these domains to improve performance.

[Lam and Bacchus, 1994] Wai Lam and Fahiem Bacchus.
Using new data to reﬁne a bayesian network. In Proc. of
UAI, pages 383–390, 1994.

[Lee and van Beek, 2017] Colin Lee and Peter van Beek. An
experimental analysis of anytime algorithms for bayesian
In Advanced Methodologies
network structure learning.
for Bayesian Networks, pages 69–80, 2017.

[Marques-Silva and Menc´ıa, 2020] Jo˜ao Marques-Silva and
Carlos Menc´ıa. Reasoning about inconsistent formulas.
In Christian Bessiere, editor, Proc. of IJCAI-2020, pages
4899–4906, 2020.

[Morgado et al., 2013] Ant´onio Morgado, Federico Heras,
Mark H. Lifﬁton, Jordi Planes, and Jo˜ao Marques-Silva.
Iterative and core-guided MaxSAT solving: A survey and
assessment. Constraints An Int. J., 18(4):478–534, 2013.
[Papadimitriou and Steiglitz, 1998] Christos H Papadim-
itriou and Kenneth Steiglitz. Combinatorial optimization:
algorithms and complexity. Courier Corporation, 1998.
[Rossi et al., 2006] Francesca Rossi, Peter Van Beek, and
Toby Walsh. Handbook of constraint programming. El-
sevier, 2006.

[Scanagatta et al., 2015] Mauro

P
de Campos, Giorgio Corani, and Marco Zaffalon. Learn-
ing bayesian networks with thousands of variables. Proc.
of NeurIPS, 28:1864–1872, 2015.

Scanagatta,

Cassio

[Schwarz, 1978] Gideon Schwarz. Estimating the dimension
of a model. The Annals of Statistics, 6(2):461–464, 1978.
[Silander and Myllym¨aki, 2006] Tomi Silander and Petri
Myllym¨aki. A simple approach for ﬁnding the globally
optimal bayesian network structure. In Proc. of UAI’06,
Cambridge, MA, USA, 2006.

[van Beek and Hoffmann, 2015] Peter van Beek and Hella-
Franziska Hoffmann. Machine learning of bayesian net-
In Proc. of Inter-
works using constraint programming.
national Conference on Principles and Practice of Con-
straint Programming, pages 429–445, Cork, Ireland, 2015.
In Proc. of the
Sixth International Conference on Principles and Practice
of Constraint Programming, pages 441–456, 2000.

[Walsh, 2000] Toby Walsh. SAT vs CSP.

[Yuan and Malone, 2013] Changhe Yuan and Brandon Mal-
one. Learning optimal bayesian networks: A shortest path
perspective. J. of Artiﬁcial Intelligence Research, 48:23–
65, 2013.

Acknowledgements
We thank the GenoToul (Toulouse, France) Bioinformatics
platform for its support. This work has been partly funded
by the “Agence nationale de la Recherche” (ANR-16-CE40-
0028 Demograph project and ANR-19-PIA3-0004 ANTI-
DIL chair of Thomas Schiex).

References
[Allen-Zhu and Orecchia, 2015] Zeyuan Allen-Zhu

and
Lorenzo Orecchia. Nearly-linear time positive LP solver
with faster convergence rate. In Proc. of the Forty-Seventh
Annual ACM Symposium on Theory of Computing,
STOC’15, page 229–236, New York, NY, USA, 2015.

[Bartlett and Cussens, 2017] Mark Bartlett

James
Cussens. Integer linear programming for the bayesian net-
work structure learning problem. Artiﬁcial Intelligence,
pages 258–271, 2017.

and

[Berg et al., 2014] Jeremias Berg, Matti J¨arvisalo, and Bran-
don Malone.
Learning optimal bounded treewidth
bayesian networks via maximum satisﬁability. In Artiﬁcial
Intelligence and Statistics, pages 86–95. PMLR, 2014.

[Buntine, 1991] Wray Buntine.

Theory reﬁnement on
bayesian networks. In Proc. of UAI, pages 52–60. Else-
vier, 1991.

[Chickering, 1995] David Maxwell Chickering. Learning
bayesian networks is NP-Complete. In Proc. of Fifth Int.
Workshop on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pages 121–130, Key West, Florida, USA, 1995.
[Cooper et al., 2010] Martin C Cooper, Simon de Givry,
Martı S´anchez, Thomas Schiex, Matthias Zytnicki, and
Tomas Werner. Soft arc consistency revisited. Artiﬁcial
Intelligence, 174(7-8):449–478, 2010.

[Cussens et al., 2017] James Cussens, Matti

J¨arvisalo,
Janne H Korhonen, and Mark Bartlett. Bayesian network
structure learning with integer programming: Polytopes,
facets and complexity. Journal of Artiﬁcial Intelligence
Research, 58:185–229, 2017.

[de Campos and Ji, 2010] Cassio Polpo de Campos and
Qiang Ji. Properties of bayesian dirichlet scores to learn
bayesian network structures. In Proc. of AAAI-00, Atlanta,
Georgia, USA, 2010.

[de Campos et al., 2018] Cassio P de Campos, Mauro
Scanagatta, Giorgio Corani, and Marco Zaffalon. Entropy-
based pruning for learning bayesian networks using BIC.
Artiﬁcial Intelligence, 260:42–50, 2018.

[Fan and Yuan, 2015] Xiannian Fan and Changhe Yuan. An
improved lower bound for bayesian network structure
learning. In Proc. of AAAI-15, Austin, Texas, 2015.

[Heckerman et al., 1995] David Heckerman, Dan Geiger,
and David M Chickering. Learning bayesian networks:
The combination of knowledge and statistical data. Ma-
chine learning, 20(3):197–243, 1995.

[Junker, 2004] Ulrich Junker. Preferred explanations and re-
laxations for over-constrained problems. In Proc. of AAAI-
04, pages 167–172, San Jose, California, USA, 2004.

