0
2
0
2

n
u
J

0
1

]

G
L
.
s
c
[

1
v
5
2
7
5
0
.
6
0
0
2
:
v
i
X
r
a

Bayesian Experience Reuse for Learning from
Multiple Demonstrators

Michael Gimelfarb∗
Mechanical and Industrial Engineering
University of Toronto
mike.gimelfarb@mail.utoronto.ca

Scott Sanner
Mechanical and Industrial Engineering
University of Toronto
ssanner@mie.utoronto.ca

Chi-Guhn Lee
Mechanical and Industrial Engineering
University of Toronto
cglee@mie.utoronto.ca

Abstract

Learning from demonstrations (LfD) improves the exploration efﬁciency of a learn-
ing agent by incorporating demonstrations from experts. However, demonstration
data can often come from multiple experts with conﬂicting goals, making it difﬁcult
to incorporate safely and effectively in online settings. We address this problem in
the static and dynamic optimization settings by modelling the uncertainty in source
and target task functions using normal-inverse-gamma priors, whose corresponding
posteriors are, respectively, learned from demonstrations and target data using
Bayesian neural networks with shared features. We use this learned belief to derive
a quadratic programming problem whose solution yields a probability distribution
over the expert models. Finally, we propose Bayesian Experience Reuse (BERS) to
sample demonstrations in accordance with this distribution and reuse them directly
in new tasks. We demonstrate the effectiveness of this approach for static optimiza-
tion of smooth functions, and transfer learning in a high-dimensional supply chain
problem with cost uncertainty.

1

Introduction

Learning from demonstrations (LfD) is a powerful approach to incorporate advice from experts in
the form of demonstrations in order to speed up learning on new tasks. However, existing work
in LfD assumes that demonstrations are generated from a single near-optimal agent with a single
goal [3, 14, 38]. In practice, however, data may be available from multiple sub-optimal agents with
conﬂicting goals. For example, when learning to operate a vehicle autonomously using human driver
demonstrations [9], different drivers can have different goals and needs (e.g. destinations, priorities
for safety or time) and experience levels. Relying on demonstrators whose goals are misaligned with
the new target task can lead to unintended behaviors, which can be dangerous in performance-critical
applications, and can be minimized by learning to trust the most relevant demonstrators.

In this paper, we focus on using LfD with multiple conﬂicting demonstrators for solving static and
dynamic optimization problems. In order to measure the similarity between source and target task
goals, we adopt a Bayesian framework. We ﬁrst parameterize source and target task reward functions
as linear functions in a common feature space, whose uncertainty is modeled using Normal-Inverse-
Gamma priors that are updated to posterior distributions using source and target demonstrations.

∗Afﬁliate to Vector Institute of Artiﬁcial Intelligence, Toronto

Preprint. Under review.

 
 
 
 
 
 
Taking a fully Bayesian treatment allows us to avoid premature convergence and is more robust to
non-stationary data [8]. To jointly learn the posteriors and feature space, we build a Bayesian neural
network with a shared encoder and multiple heads, one for each source and target task. We then
formulate a quadratic programming problem (QP) whose solution yields a probability distribution
over the source demonstrators. This allows us to trade-off the mean and variance of the uncertainty in
source and target task goals in a principled manner.

In order to transfer demonstrations to new tasks in LfD, one approach is to pre-train the learner directly
from the source data [15], or learn and reuse auxiliary representations from the source data such as
policies [16, 45]. However, the former can be ineffective when demonstrations assume conﬂicting
goals, while meaningful policies may be difﬁcult to solicit from the latter when demonstrators are
limited, sub-optimal or exploratory in nature [32, 38, 47]. On the other hand, experience collected
from a failed or inexperienced demonstrator can be just as valuable as an experienced one [21].
We present an algorithm called Bayesian Experience Reuse (BERS), for directly reusing multiple
demonstrations in a way that is consistent with the learned weighting over the source and target task
goals (Algorithm 1). We demonstrate the efﬁcacy of our approach for optimizing static functions, and
continuous control of a supply chain using reinforcement learning with high-dimensional actions.

1.1 Related Work

Most papers in LfD incorporate demonstrations from a single optimal or near-optimal expert with
a single goal [3, 14, 38]. Some papers in the area of LfD for reinforcement learning (RLfD) relax
this assumption to a single sub-optimal demonstrator, using pre-training [15, 19, 22], reward shaping
[13, 42], ranking demonstrations [51], Bayesian inference [36], or other approaches [45] for transfer.
However, the aforementioned papers cannot learn from multiple demonstrators with conﬂicting goals.
On the other hand, papers on this topic typically assume multiple near-optimal demonstrators, so that
recovering policies [16, 28, 29], value functions [20, 27] or successor features [6, 7] is possible. In
this paper, we show how sub-optimal demonstrators with conﬂicting goals can be ranked according to
their alignment with the target task goal in a safe Bayesian manner, and reused directly by the target
agent without learning auxiliary representations such as policies or value functions. Our approach
is related to encoder sharing [18, 34, 48, 49] and uncertainty quantiﬁcation of rewards or policies
[5, 12, 25, 46], but our problem setting and methodology are different.

2 Background

2.1 Reinforcement Learning

,

(cid:105)

S

A

A

(cid:104)S

, P, R, γ

, where:

Complex decision-making can be summarized in a Markov decision process (MDP), formally
(s) is a set of possible actions
deﬁned as a ﬁve-tuple
is a set of states,
s, a) gives the next-state distribution upon taking action a in state s, R(s, a, s(cid:48))
in state s, P (
·|
is the reward received upon transition to state s(cid:48) after taking action a in state s, and γ
[0, 1]
is a discount factor that gives smaller priority to rewards realized further into the future. The
objective of an agent is to ﬁnd a policy µ :
that maximizes the long-run expected return
Qµ(s, a) = E[(cid:80)∞
st, at).
µ(st) and st+1
When P and R are known, an optimal policy can be found exactly using dynamic programming
[35, 37]. In the reinforcement learning setting, neither P nor R are known to the decision-maker.
Instead, the agent interacts with the environment using a randomized exploration policy µe, collecting
reward and observing the state transitions [43]. In order to learn optimal policies in this setting,
temporal difference methods ﬁrst learn Q(s, a) and then recover an optimal policy [31, 52], while
policy gradient methods parameterize and recover an optimal policy µ∗ directly [44]. Actor-critic
methods learn and use both a critic Q(s, a) and actor policy µ(s) simultaneously [26].

t=0 γtR(st, at, st+1)

s0 = s, a0 = a], where at

S → A

P (

∼

∼

∈

·|

|

2.2 Common Feature Representations

In this paper, we consider the regression problem associated with learning a multivariate function
y :
. For instance, in the RL setting, we estimate reward functions where
Rd, a function y can be
Rd is a ﬁxed vector.

X
X
expressed as a linear combination y(x) = φ(x)(cid:62)w,

and y(x) = R(s, a, s(cid:48)). Given a feature map φ :

R on a domain

S × A × S

, where w

X →
=

x

∀

∈ X

X →
∈

2

We are interested in the transfer learning problem between similar tasks in a domain
common

, deﬁned formally as

X

φ on a

M

φ = (cid:8)y :

w

d s.t. y(x) = φ(x)(cid:62)w,

x

R

(cid:9) .

∃

∈

M

M
φ could include all MDPs with shared dynamics and different rewards Ri. In
In the RL setting,
(1), we have explicitly assumed that the (unknown) state features φ are shared among tasks. This is
φ, we may trivially
not a restrictive assumption in practice, as given a set of tasks T1, T2 . . . Tn
deﬁne φk(x) = yTk (x),
for each k = 1, 2 . . . n. The main challenge in this paper is to
simultaneously learn a suitable common latent feature embedding φ and posterior distributions for
y(x), and leverage this uncertainty for measuring task similarity in

∈ M

∈ X

∈ X

φ.

x

∀

∀

(1)

M

3 Bayesian Learning from Multiple Demonstrators

The agent is presented with sets of demonstrations

1,

2, . . .

N from source tasks T1, T2, . . . TN

φ, consisting of labeled pairs (xt, yt), and would like to solve a new target task Ttarget

∈
D
φ.
M
In order to make optimal use of the source tasks during training, the agent should learn to favor
demonstrators whose underlying reward representation is closest to the target reward.

∈ M

D

D

3.1 Bayesian Linear Regression with Common Feature Representations

In order to tractably learn these reward representations, we learn a shared feature space ˆφ
φ, to-
gether with Bayesian regressors P(wi
)
|D
for the target task such that yi(x) = φ(x)(cid:62)wi and ytarget(x) = φ(x)(cid:62)wtarget approximately hold
for all i and all realizations x
R. As we will soon show, the sharing of the features φ is
crucial to allow meaningful comparisons between source weights wi and target weights w.

i) for the source task and a Bayesian regressor P(wtarget

∈ X

, y

|D

≈

∈

In order to tractably learn the features φ as well as the corresponding posterior distributions, we
parameterize φ(x)
φθ(x) using a deep neural network (encoder) with weight parameters θ, and
model w1, . . . wN , wtarget using the normal-inverse-gamma (NIG) conjugate prior:

≈

εi

∼ N

(0, σ2

i ),

wi

yi(x) = φθ(x)(cid:62)wi + εi,
i Λ−1
σ2
i ∼
i

(µi, σ2

),

∼ N

InvGamma(αi, βi).

The joint posterior P(wi, σ2

i |D

i) can be shown to factor as

P(wi, σ2

i)

i |D
) and σ2

(µi, σ2

i Λ−1
i

P(wi

σ2
i ,
|

∝

D

i) P(σ2

i),

i |D

InvGamma(αi, βi), where:

where wi

σ2
i ,
|

i
∼ N
D
Λi = Λ0

i + Φ(cid:62)
i Φi,
1
i
2 |D

i +

,
|

αi = α0

i
i |D
∼
(cid:0)Λ0
µi = Λ−1
1
2

βi = β0

i +

i

i µ0
(cid:0)y(cid:62)

i + Φ(cid:62)

i yi

(cid:1) ,

i yi + (µ0

i )(cid:62)Λ0

i µ0

µ(cid:62)

i Λiµi

(cid:1) ,

i −

(2)

(3)

(4)

and where Φi is the set of state features and yi is the set of observations of yi in
i [8]. We have
also assumed that, conditional on data
i are mutually independent
between tasks, a very mild assumption in practice. Adapting the methodology of Snoek et al. [40], we
update θ by gradient ascent on the marginal log-likelihood function for each head i, deﬁned here as

i, weights wi and variances σ2

D

D

log P(yi

Xi) =

|

i

π+log Γ(α0
i )
|

−

|D

i log β0
a0

i +

1
2

log

Λ0
|

i |−

log Γ(αi)+αi log βi

1
2

−

log

Λi
|

|

, (5)

where the key quantities are provided by (4) and depend implicitly on θ through Φi.

Using this model, parameter sharing allows the learned features φ to be reﬁned and transferred
seamlessly from source to target tasks, or as new source and target tasks are incorporated, and
provides an additional source of knowledge for transfer. However, our main contribution is to use the
posterior distributions over task goals, wi and wtarget, to transfer experiences between tasks.

3

3.2 Source Task Selection via Quadratic Programming

In order to derive a Bayesian decision rule for source task selection, we ﬁrst observe that source
tasks that are most similar to the target task, and hence those that lead to better transfer, should
have wi closest to the true target wtarget. In our setting, we instead have uncertain estimates for wi
and wtarget modelled as multivariate normal random variables. We therefore look for a weighting
(cid:80)N
i=1 aiwi that is closest in expectation to wtarget with respect to the uncertainty in these estimates.
More speciﬁcally, suppose that we have computed the posterior distributions of wi for each i =
1, 2 . . . N and wtarget from past data. We would like to weight the wi in such a way that the weighted
sum (cid:80)

i aiwi is closest to wtarget in expectation, or in other words, solve

(cid:34)

min
a∈P

E

(cid:107)

wtarget

−

aiwi

(cid:35)

,

(cid:12)
(cid:12)
(cid:12) D

2
2
(cid:107)

N
(cid:88)

i=1

(6)

P

D

is the union of all source and target data and

where
would like to be able to sample source tasks according to a, so we take
the set of probability distributions on
problems [33, 50, 53], we may choose

is a convex polyhedron. Speciﬁcally, we
0
,
=
}
. In other applications, such as static regression
}

≥
1, 2 . . . N
{
= RN or put additional constraints on expert selection.
P

a : 1(cid:62)a = 1, a

1, . . . σ2

To derive a closed form for (6), ﬁrst note that, given the true values of the variances v =
[σ2
target], the random variables w1, . . . wN , wtarget follow normal distributions with
respective means µ1 . . . µN , µtarget and covariances σ2
targetΣtarget (equation
(3)). Then, for any a
(cid:80)

N ΣN , σ2
1Σ1, . . . σ2
(cid:80)
i aiwi

RN , it is easy to check that wtarget
i Σi), and hence that

targetΣtarget + (cid:80)

(µtarget

D ∼ N

N , σ2

v,

−

−

P

∈

{

|

i aiµi, σ2
(cid:104)
E
(cid:107)

wtarget

i a2
i σ2
(cid:105)

(cid:88)

−

i

aiwi

v,

2
2 |
(cid:107)

D

= tr(σ2

targetΣtarget +

(cid:88)

i σ2
a2

i Σi) +

µtarget

(cid:107)

(cid:88)

−

i

aiµi

2
2
(cid:107)

= σ2

targettr(Σtarget) +

i
(cid:88)

i

i σ2
a2

i tr(Σi) +

µtarget
(cid:107)

−

(cid:88)

i

aiµi

2
2.
(cid:107)
(7)

Then, taking expectation of (7) with respect to the variance terms v,

(cid:104)
E

wtarget
(cid:107)

−

(cid:88)

i

aiwi

(cid:105)

(cid:12)
(cid:12)

2
2

(cid:107)

D

(cid:34)

(cid:104)
E

= E

wtarget
(cid:107)

−

(cid:88)

i

aiwi

(cid:12)
(cid:12) v,

2
2

(cid:107)

D

(cid:105) (cid:12)
(cid:12)
(cid:12) D
(cid:88)

(cid:35)

aiµi

2
2
(cid:107)

−

i

∝

=

(cid:88)

i

N
(cid:88)

i=1

i E[σ2
a2

i |D

] tr(Σi) +

µtarget
(cid:107)

a2
i

(cid:19)

(cid:18) βi
αi

−

1

tr(Σi) +

µtarget
(cid:107)

−

N
(cid:88)

i=1

aiµi

2
2,
(cid:107)

(8)

where in the last step we used (2) and the expectation of InvGamma(αi, βi), and ignored all terms
independent of any decision variables a. Hence, we have shown that optimizing the expected weighted
mean-squared error (6) is equivalent to optimizing the weighted mean-squared error of the posterior
means plus a penalty term equal to the product of the noise and posterior variances.

To simplify (8) further, we deﬁne M = [µ1 . . . µN ]
with entries
following quadratic programming formulation of our problem:

RN ×N the diagonal matrix
βi
αi−1 tr(Σi), i = 1, 2 . . . N . Rewriting (8) in this new notation, we can obtain the

Rd×N and S

∈

∈

min
a

subject to

µ(cid:62)Ma +

−
1(cid:62)a = 1,
0.
a

(cid:23)

a(cid:62)(M(cid:62)M + S)a

1
2

(9)

Here, M(cid:62)M + S is positive deﬁnite, since it is the sum of the positive semi-deﬁnite M(cid:62)M and the
positive deﬁnite S. Hence, the above QP problem can be formulated and solved exactly using an
off-the-shelf solver in polynomial time in N [11], which does not grow with the feature dimension

4

and can be applied in online settings. Hence, the QP (9)
d nor the number of demonstrations
remains tractable when the domain complexity is high or the number of demonstrations is large.
This is not necessarily the case if omitting the second order terms S, since the QP may become
rank-deﬁcient and thus lack a unique solution. We can therefore interpret S as a data-dependent
Bayesian regularizer for the QP solution.

|D

|

i

In the case where N is considerably large, the solution to (9) can instead be approximated using
neural networks [1, 10]. Alternatively, since the posterior updates are relatively smooth (as we
demonstrate experimentally), the solution of (9) should not change signiﬁcantly between consecutive
iterations, in which case warm starts may be particularly effective [17].

3.3 Bayesian Experience Reuse (BERS) for LfD

When transferring demonstrations from a single source, a simple yet effective approach is to pre-train
or seed the target learning agent with the demonstrations prior to target task training [15]. When
data originates from multiple demonstrators with differing goals, pre-training is no longer possible.
Instead, we propose to train the target agent on the source data while concurrently learning the target
task. In this manner, the source demonstrations provide an effective exploration bonus by allowing
the agent to observe demonstrations from other tasks with similar goals.

More speciﬁcally, in each iteration of the target learning phase, we sample a source task Ti
∈ M
according to the probability distribution a obtained from the quadratic programming problem (9),
and train the target agent on experiences drawn from the corresponding source data
i. In order
for the target agent to improve beyond the source task solution and generalize to the new task, the
agent must eventually learn from target demonstrations rather than the source data. In order to do
this, we apply the framework of Probabilistic Policy Reuse [16, 29] in our setting. Speciﬁcally in
each training episode m, the target agent is trained on source data with probability pm, and otherwise
pm, where pm is annealed to zero over time. The choice of pm
uses target data with probability 1
is typically problem dependent, but may also be annealed based on the QP solution.

−

D

φ

The full implementation of our algorithm, Bayesian Experience Reuse (BERS), is illustrated in
Figure 1 in a transfer learning setting. The left ﬁgure provides a visualization of the process used to
N
i=1 and µ, Λ, α, β are
weight source tasks. Here, Bayesian heads with parameters
maintained for source and target tasks respectively, while sharing the encoder parameters θ, and then
used to construct the quadratic programming problem (9). The outputs ˆyi and ˆytarget can also be
used for making predictions (e.g. in regression tasks); in this paper we focus on using the posterior
distributions over task goals, and the corresponding QP solution, to transfer source demonstrations in
a Bayesian framework.

µi, Λi, αi, βi

}

{

In the pseudo-code on the right, we have deﬁned Obase to be a base learning algorithm for solving the
target task, which is assumed to be a static or dynamic (RL) optimization algorithm in this work. We
pre-train the ﬁrst N heads on source data to learn features and posteriors for source task goals. At the
end of each episode, we train all source and target heads on source and target demonstrations to reﬁne
the features and posteriors for all tasks. With simple modiﬁcations, this algorithm can also be applied
in multi-task settings, by maintaining one QP solution per task. The bottleneck of this procedure is
the solution of the QP, which can be addressed using various approximation methods outlined above.

4 Empirical Evaluation

In order to demonstrate the effectiveness of BERS, we apply it to a static optimization problem and
continuous control of a supply chain.

4.1 Static Function Optimization

We ﬁrst consider the problem of minimizing a smooth function [24]. Transfer learning can be useful
in this setting because the known solution of one smooth function can be used as a starting point
or seed in the search for the minimum of another similar function. More speciﬁcally, we use the
10-D Rosenbrock, Ackley and Sphere functions as source tasks, whose global optimum solutions
have been shifted to various locations. As target task, we use each source task as the ground truth
as well as the Rastrigin function. The Sphere and Rastrigin functions are locally similar around

5

Algorithm 1 Bayesian Experience Reuse (BERS)

Require: {Di}N

i=1, TN +1 = Ttarget ∈ Mφ, Obase, DN +1 =

Dtarget = ∅, θ, {µi, Λi, α1, βi}N +1
pre-train θ, {µi, Λi, α1, βi}N
for episode m = 1, 2, . . . do

i=1 on {Di}N

i=1 , pm, a

i=1 using (4), (5)

for step t = 1, 2, . . . T of episode m do

explore Ttarget using Obase and collect d = (x, y)
Dtarget = Dtarget ∪ d
if Bernoulli(pm) = 1 then

sample it ∼ a and experience B ⊂ Dit

else

sample experience B ⊂ Dtarget

end if
train Obase on B

end for
train θ, {µi, Λi, α1, βi}N +1
i=1 on {Di}N +1
solve QP (9) (using, e.g. [2]) and set a to the solution

i=1 using (4), (5)

Figure 1: Left: bayesian multi-headed neural-linear model with shared encoder (MLP) and aggregated
QP decision layer. Right: pseudo-code implementation for transfer learning for static and dynamic
optimization (BERS).

end for
return Obase

the optimum, so a successful transfer experiment should learn to transfer knowledge between them.
We consider optimizing the Rastrigin function in both transfer and multi-task settings. As the base
learning agent Obase in Algorithm 1, we use Differential Evolution (DE) [41], a commonly-used
evolutionary algorithm for global optimization of black-box functions 2.

We compare BERS (Ours) to the UCB algorithm [4] with asymptotically optimal convergence (UCB),
where the reward is improvement in best solution value between consecutive iterations. We also
compare to the equal weighting of demonstrators (Equal), individual demonstrators (S1, S2. . . ), and
standard DE (None). Figure 2 illustrates the function value of the best solution and the QP solution in
each iteration. Figure 3 tracks the movement of the posterior means of wi and wtarget during target
training for a simpliﬁed model with d = 2 for visualization.

(a) Rosenbrock

(b) Ackley

(c) Sphere

(d) Rastrigin

(e) Rastrigin-MT

Figure 2: Best function ﬁtness (top row) and weights assigned to source tasks (bottom row) over
generations using transfer and multi-task (MT) learning for function optimization, with each source
and target task as ground truth. Averaged over 20 trials with shaded standard error bars.

4.2 Continuous Control of a Supply Chain Network

The second problem is continuous control of a supply chain network as illustrated in Figure 4a. Here,
the nodes include a central factory and K = 6 warehouses (denoted A, B . . . F), and edges represent

2Another option is to use Bayesian optimization (BO). However, we believe this is more suitable for expensive

functions with relatively low numbers of local optima, such as for hyper-parameter optimization.

6

x...φ1φ2φdφθ(x)µ1,Σ1α1,β1w1σ21......µN,ΣNαN,βNwNσ2Nµ,Σα,βwtargetσ2target...ˆy1(x)ˆyN(x)ˆytarget(x)mina−µTMa+12aT(MTM+S)as.t.1Ta=1,a(cid:23)0.a−∇θlogP(yi|Xi)0.00.51.01.5batch number1e2012345best fitness obtainedOursEqualNoneRosenbrockAckleySphereUCB012345batch number1e20.00.20.40.60.8probabilityRosenbrockAckleySphere0.00.51.01.5batch number1e2012345best fitness obtainedOursEqualNoneRosenbrockAckleySphereUCB012345batch number1e20.00.20.40.60.8probabilityRosenbrockAckleySphere0.00.20.40.60.81.0batch number1e201234best fitness obtainedOursEqualNoneRosenbrockAckleySphereUCB012345batch number1e20.00.20.40.60.81.0probabilityRosenbrockAckleySphere012345batch number1e202468best fitness obtainedOursEqualNoneRosenbrockAckleySphereUCB012345batch number1e20.00.20.40.60.8probabilityRosenbrockAckleySphere12345batch number1e20246best fitness obtainedOursEqualNoneUCB012345batch number1e20.00.20.40.60.8probabilityRosenbrockAckleySphere(a) Rosenbrock

(b) Ackley

(c) Sphere

(d) Rastrigin

Figure 3: For function optimization with each source and target task as the ground truth, shows the
evolution of the posterior mean of each wi and wtarget during training for the simpliﬁed experiment
with latent dimension d = 2. As shown here, the target mean eventually converges to the correct
source task mean.

possible transportation routes along which inventory can ﬂow. A centralized controller must decide,
at each discrete time step, how many units of a good to manufacture (up to 35 per period), and
how many to ship from the factory to each warehouse and between warehouses. Furthermore, the
cost of dispatching a truck depends on the source and destination. The controller identiﬁes three
likely scenarios (Scenarios 1, 2, and 3), shown in Figure 4a, where solid arrows indicate the cheapest
routes with cost 0.03 and dark and light grey arrows indicate costly routes with costs 1.50 and 3.00.
A prudent agent must learn to take advantage of the “cheapest" routes through the network while
simultaneously learning optimal manufacturing and order quantities.

The state includes the current stock in the factory and warehouses. Actions are modelled as follows:
(1) one continuous action for production as a proportion of the maximum; (2) one set of K + 1
actions for proportions of factory stock to ship to each warehouse (including to keep at the factory);
and (3) one set of K actions per warehouse, for proportions of warehouse stocks to ship to all other
warehouses (including itself). This leads to a 2 + K + K 2 = 44-dimensional action space. In order
to tractably solve this problem, we use the actor-critic algorithm DDPG [30] as the base learning
agent Obase. The actor network is shown in Figure 4b, using sparse encoding for states and sigmoid/
softmax output activation to satisfy stock constraints. The critic network has a similar structure.

We evaluate BERS (Ours) against prioritized experience replay [23, 39] with pre-loaded demon-
strations from all source tasks (PER), and HAT [45] combined with a state-of-the-art policy reuse
algorithm [29] (PPR). In the latter case, source policies are trained using the same architecture as the
actor network in Figure 4b for 50 epochs using a cross-entropy loss. Figure 5 illustrates the total test
proﬁt obtained, as well as the weights assigned by the QP problems to the source tasks. Figure 6 how
the posterior weights of the target task approach the weights of the ground truth.

Scenario 1

Scenario 2

Scenario 3

Target

(a) Transport cost visualization.

(b) Actor network.

Figure 4: Source and target task conﬁguration and actor network used for the Supply Chain domain.

7

w112345678w25.02.50.02.55.07.510.012.5w30.51.01.52.02.5Rosen.AckleySphereTargetw11086420w2642024w30.250.500.751.001.251.501.752.00RosenbrockAckleySphereTargetw164202468w212108642w30.250.500.751.001.251.501.752.00RosenbrockAckleySphereTargetw1123456789w212.510.07.55.02.50.02.55.0w30.51.01.52.02.5Rosen.AckleySphereTargetABCDEFABCDEFABCDEFABCDEF...qfactoryqWH1qWH2qWHKStatest10...000...1...01...0......Fully-connected2hiddenlayers300neuronsReLUSoftmaxK+1SoftmaxK+1SoftmaxK+1...SoftmaxK+1ABCDEFABCDEFABCDEF...ABCDEFaproduceafactory→WH1...KaWH1→WH1...KaWH2→WH1...K...aWHK→WH1...K(a) Scenario 1

(b) Scenario 2

(c) Scenario 3

(d) Target Task

Figure 5: Total testing proﬁt per episode (left) and weights assigned to source tasks (right) over
epochs using soft pre-training of DDPG for the Supply Chain problem, with each source and target
task as ground truth. Averaged over 5 trials with shaded standard error bars.

(a) Scenario 1

(b) Scenario 2

(c) Scenario 3

(d) Target

Figure 6: For Supply Chain control with each source and target task as the ground truth, shows the
posterior marginal distributions of wi and wtarget during training on the target task (after 0, 10, 50,
100 and 200 episodes). Over time, the target features begin to concentrate on the corresponding
values for the correct source task.

4.3 Discussion

On the optimization task, BERS achieves a small performance gap relative to the single best expert,
since it can quickly identity the most suitable source task (Figure 2). While UCB is a strong baseline,
BERS ﬁnds the solution in less iterations and with less variability. As expected, the QP solution
favours the Sphere function as the source task when solving the Rastrigin function, because they are
structurally similar functions, which leads to a quicker identiﬁcation of the global minimum.

On the supply chain task, BERS also achieves results that are similar to the single best expert, and
does better than PPR (while the asymptotic performance on Scenario 1 is similar, BERS achieves
better jump-start performance). Both BERS and PPR are able to quickly surpass the performance
of the exploration policy that generated the source data (shown as a horizontal line in Figure 5),
whereas PER was not able to obtain satisfactory performance. We believe the latter is true because
PER prioritizes experiences by TD error, which is not suitable when the rewards are sampled from
different tasks, while BERS learns a common feature embedding to allow consistent comparison
between tasks (Figure 3). We also believe that if the source data was considerably less optimal,
then the performance gap between PPR and BERS would be greater. On the target scenario, it is
interesting to see that the weights assigned to Scenarios 1 and 2 are roughly equal, which makes
sense as the target task shares similarities with both scenarios (Figure 4a). By mixing two source
tasks, BERS was able to perform substantially better on the target task than the two source tasks (S2
and S3) in isolation. Also by adopting a fully Bayesian treatment, we obtain smooth convergence of
the task weights on all experiments.

8

012345batch number1e40500100015002000total profitOursPERPPREqualS1S2S3NoneSource012345batch number1e40.00.20.40.60.8probabilityScenario 1Scenario 2Scenario 3012345batch number1e40500100015002000total profitOursPERPPREqualS1S2S3NoneSource012345batch number1e40.00.20.40.60.81.0probabilityScenario 1Scenario 2Scenario 3012345batch number1e40500100015002000total profitOursPERPPREqualS1S2S3NoneSource012345batch number1e40.00.20.40.60.8probabilityScenario 1Scenario 2Scenario 3012345batch number1e40500100015002000total profitOursPERPPREqualS1S2S3None012345batch number1e40.00.20.40.6probabilityScenario 1Scenario 2Scenario 3S1S2S3Targettask1050510feature weightsS1S2S3Targettask10505101520feature weightsS1S2S3Targettask1050510feature weightsS1S2S3Targettask151050510feature weights5 Conclusion

We studied the problem of LfD with multiple sub-optimal demonstrators with different goals. To solve
this problem, we proposed a multi-headed Bayesian neural network with a shared encoder to efﬁciently
learn consistent representations of the source and target reward functions from the demonstrations.
Reward functions were parameterized as linear models, whose uncertainty was modeled using
Normal-Inverse-Gamma priors and updated using Bayes’ rule. A quadratic programming problem
was formulated to rank the demonstrators while trading off the mean and variance of the uncertainty
in the learned reward representations, and Bayesian Experience Reuse (BERS) was proposed to
incorporate demonstrations directly when learning new tasks. Empirical results for static function
optimization and RL suggest that our approach can successfully transfer experience from conﬂicting
demonstrators in diverse tasks.

Broader Impact

This work presents a general framework for reusing demonstrations data available in general static
and dynamic optimization problems. It could lead to positive societal consequences in terms of safety
and reliability of automated systems, such as medical diagnosis systems that often require pre-training
from human experts, or driver-less cars that learn from human mistakes to avoid similar situations
in the real world, potentially saving lives in the process. On the other hand, because our approach
requires the learning of the (latent) intentions of users and their goals, there could potentially be
privacy concerns depending on how the trained agent decisions are implemented or published. This
is particularly true shortly after commencement of training, when the learning agent can resemble
one or more demonstrators in their decision making. Our framework is also based on deep learning
methodologies, and so it naturally inherits some of the risks prevalent in that work (although our
work also tries to address some of these issues by using a Bayesian approach). For example, data
that consists of only exceptional situations (such as pre-training of a search and rescue robot), may
consist of many outliers. Such data sets must sometimes undergo pre-processing to remove the effects
of outliers or other features that could skew the algorithm and lead to the wrong conclusions. This
could be catastrophic in mission critical operations where every decision counts. However, by using
prudent machine learning practices, we believe the beneﬁts of this work outweigh the risks.

Acknowledgments and Disclosure of Funding

This work was funded by a DiDi Graduate Student Award. The authors would like to thank Jihwan
Jeong for his Python code for training the neural-linear network and discussions that helped improve
this paper.

References

[1] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In

International Conference on Machine Learning, pages 136–145. JMLR. org, 2017.

[2] M Andersen, J Dahl, and L Vandenberghe. Cvxopt: Python software for convex optimization, version 1.1,

2015.

[3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from

demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.

[4] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning

Research, 3(Nov):397–422, 2002.

[5] Hamoon Azizsoltani, Yeo Jin Kim, Markel Sanz Ausin, Tiffany Barnes, and Min Chi. Unobserved is not
equal to non-existent: using gaussian processes to infer immediate rewards across contexts. In Proceedings
of the 28th International Joint Conference on Artiﬁcial Intelligence, pages 1974–1980. AAAI Press, 2019.

[6] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David
Silver. Successor features for transfer in reinforcement learning. In Advances in Neural Information
Processing Systems, pages 4055–4065, 2017.

9

[7] Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz,
Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and
generalised policy improvement. In International Conference on Machine Learning, pages 501–510, 2018.

[8] Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.

[9] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal,
Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving
cars. arXiv preprint arXiv:1604.07316, 2016.

[10] Abdesselam Bouzerdoum and Tim R Pattison. Neural network for quadratic optimization with bound

constraints. IEEE Transactions on Neural Networks, 4(2):293–304, 1993.

[11] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university

press, 2004.

[12] Daniel S. Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum. Safe imitation learning via fast

bayesian reward inference from preferences, 2020.

[13] Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew E Taylor, and Ann Nowé.
In Twenty-Fourth International Joint

Reinforcement learning from demonstration through shaping.
Conference on Artiﬁcial Intelligence, 2015.

[14] Sonia Chernova and Andrea L Thomaz. Robot learning from human teachers. Synthesis Lectures on

Artiﬁcial Intelligence and Machine Learning, 8(3):1–121, 2014.

[15] Gabriel V Cruz Jr, Yunshu Du, and Matthew E Taylor. Pre-training neural networks with human demon-

strations for deep reinforcement learning. arXiv preprint arXiv:1709.04083, 2017.

[16] Fernando Fernández, Javier García, and Manuela Veloso. Probabilistic policy reuse for inter-task transfer

learning. Robotics and Autonomous Systems, 58(7):866–871, 2010.

[17] Hans Joachim Ferreau, Hans Georg Bock, and Moritz Diehl. An online active set strategy to overcome
the limitations of explicit mpc. International Journal of Robust and Nonlinear Control: IFAC-Afﬁliated
Journal, 18(8):816–830, 2008.

[18] Yannis Flet-Berliac and Philippe Preux. Merl: Multi-head reinforcement learning. In NeurIPS 2019-Deep

Reinforcement Learning Workshop, 2019.

[19] Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning from

imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.

[20] Michael Gimelfarb, Scott Sanner, and Chi-Guhn Lee. Reinforcement learning with multiple experts: A
bayesian model combination approach. In Advances in Neural Information Processing Systems, pages
9528–9538, 2018.

[21] Daniel H Grollman and Aude Billard. Donut as i do: Learning from failed demonstrations. In 2011 IEEE

International Conference on Robotics and Automation, pages 3804–3809. IEEE, 2011.

[22] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John
Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, 2018.

[23] Yuenan Hou, Lifeng Liu, Qing Wei, Xudong Xu, and Chunlin Chen. A novel ddpg method with prioritized
experience replay. In 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC),
pages 316–321. IEEE, 2017.

[24] Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation
problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2):150–194,
2013.

[25] David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, and Sebastian
Tschiatschek. Successor uncertainties: exploration and uncertainty in temporal difference learning. In
Advances in Neural Information Processing Systems, pages 4509–4518, 2019.

[26] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms.

In Advances in Neural Information

Processing Systems, pages 1008–1014, 2000.

[27] Guohao Li, Matthias Mueller, Vincent Casser, Neil Smith, Dominik L Michels, and Bernard Ghanem. Oil:

Observational imitation learning. arXiv preprint arXiv:1803.01129, 2018.

10

[28] Mao Li and D Kudenko. Reinforcement learning from multiple experts demonstrations. In Workshop on

Adaptive Learning Agents (ALA) at the Federated AI Meeting, volume 18, 2018.

[29] Siyuan Li and Chongjie Zhang. An optimal online method of selecting source policies for reinforcement

learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[30] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

[31] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.

[32] Monica N Nicolescu and Maja J Mataric. Natural methods for robot task learning: Instructive demon-
strations, generalization and practice. In Proceedings of the second international joint conference on
Autonomous agents and multiagent systems, pages 241–248, 2003.

[33] David Pardoe and Peter Stone. Boosting for regression transfer. In Proceedings of the 27th International

Conference on Machine Learning, pages 863–870, 2010.

[34] Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cédric Archambeau. Scalable hyperparameter
transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855, 2018.

[35] Warren B Powell. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703.

John Wiley & Sons, 2007.

[36] Bob Price and Craig Boutilier. A bayesian approach to imitation in reinforcement learning. In Twenty-

Fourth International Joint Conference on Artiﬁcial Intelligence, pages 712–720, 2003.

[37] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &

Sons, 2014.

[38] Stefan Schaal. Learning from demonstration. In Advances in Neural Information Processing Systems,

pages 1040–1046, 1997.

[39] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv

preprint arXiv:1511.05952, 2015.

[40] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa
Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural networks. In
International Conference on Machine Learning, pages 2171–2180, 2015.

[41] Rainer Storn and Kenneth Price. Differential evolution–a simple and efﬁcient heuristic for global optimiza-

tion over continuous spaces. Journal of Global Optimization, 11(4):341–359, 1997.

[42] Halit Bener Suay, Tim Brys, Matthew E Taylor, and Sonia Chernova. Learning from demonstration for
shaping through inverse reinforcement learning. In Proceedings of the 2016 International Conference on
Autonomous Agents & Multiagent Systems, pages 429–437, 2016.

[43] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

[44] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems, pages 1057–1063, 2000.

[45] Matthew E Taylor, Halit Bener Suay, and Sonia Chernova. Integrating reinforcement learning with human
demonstrations of varying ability. In The 10th International Conference on Autonomous Agents and
Multiagent Systems, pages 617–624, 2011.

[46] Sanjay Thakur, Herke van Hoof, Juan Camilo Gamboa Higuera, Doina Precup, and David Meger. Uncer-
tainty aware learning from demonstrations in multiple contexts using bayesian neural networks. In 2019
International Conference on Robotics and Automation (ICRA), pages 768–774. IEEE, 2019.

[47] Michael van Lent and John E Laird. Learning procedural knowledge through observation. In Proceedings

of the 1st international conference on Knowledge capture, pages 179–186, 2001.

[48] Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang.
Hybrid reward architecture for reinforcement learning. In Advances in Neural Information Processing
Systems, pages 5392–5402, 2017.

11

[49] Giulia Vezzani, Abhishek Gupta, Lorenzo Natale, and Pieter Abbeel. Learning latent state representation

for speeding up exploration. arXiv preprint arXiv:1905.12621, 2019.

[50] Boyu Wang, Jorge Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the performance
gap between domains. In Advances in Neural Information Processing Systems, pages 10644–10654, 2019.

[51] Zhaodong Wang and Matthew E Taylor. Improving reinforcement learning with conﬁdence-based demon-
strations. In Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence, pages 3027–3033,
2017.

[52] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279–292, 1992.

[53] Yi Yao and Gianfranco Doretto. Boosting for transfer learning with multiple sources. In 2010 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, pages 1855–1862. IEEE,
2010.

12

Appendix

Hyper-Parameters for the Neural-Linear Model

(0, I), σ2

∼ N

i ∼

We set wi
InvGamma(1, 1). For the encoder, we set d = 20, use L2 regularization
with penalty λ = 10−4, a learning rate of 10−4, and initialize weights using Glorot uniform. The
encoder has two hidden layers with 200 ReLU units each (300 in the ﬁrst layer for the supply chain
problem), and tanh outputs. We also include a constant bias term in the feature map φ when learning
w. Prior to transfer, we ﬁrst train the neural linear model on 4000 batches of size 64 sampled
uniformly from source data. During target training, we train on batches of size 64 sampled from
source and target data after each iteration (generation for optimization, episode for supply chain) to
avoid catastrophic forgetting of features (one batch for optimization and 20 for supply chain). QPs
are solved from scratch at the end of each iteration using the cvxopt package [2].

Details for the Static Optimization Benchmark

Problem Setting: The four test functions considered in this paper are as follows:

fRosenbrock(x) =

D−1
(cid:88)

i=1

100[(xi+1

i )2 + (1
x2

xi)2]

−

−
(cid:118)
(cid:117)
(cid:117)
(cid:116)

0.2

fAckley(x) =

20 exp

−





−

fSphere(x) =

D
(cid:88)

(xi + 2)2

i=1



x2
i



1
D

D
(cid:88)

i=1

(cid:32)

1
D

D
(cid:88)

i=1

exp

−

(cid:33)

cos(2πxi)

+ 20 + exp (1)

fRastrigin = 10D +

D
(cid:88)

[(xi + 2)2

i=1

−

10 cos(2π(xi + 2))],

RD and restricted to xi

where x = [x1, x2 . . . xD]
4, 4] for i = 1, 2 . . . D and we set
∈
D = 10 for all experiments. The global minimums of the functions are, respectively, as follows:
x∗
Ackley = 0D, x∗
Rosenbrock = 1D, x∗
1D. For Rosenbrock,
2
−
Sphere and Rastrigin functions, we transform outputs using y
√y so they become, approximately,
equally scaled (for Rosenbrock we subsequently also divide by 10), to demonstrate whether we can
actually “learn" each function from the data rather than distinguish it according to scale alone.

[
−
1D and x∗
Rastrigin =

Sphere =

(cid:55)→

×

×

−

∈

2

Solver Settings: To optimize all functions, we use the Differential Evolution (DE) algorithm [41].
The pseudo-code of this algorithm is outlined in Algorithm 2.

Here, CR is the crossover probability and denotes the probability of replacing a coordinate in an
existing solution with the candidates (crossover), F is the differential weight and controls the strength
of the crossover, and N P is the size of the population (larger implies a more global search). We
use standard values CR = 0.7, F = 0.5 and N P = 32 for reporting experimental results, unless
4, +4] for all coordinates; all points that are
indicated otherwise. The search bounds are also set to [
generated during the initialization of the population and the crossover are clipped to this range.

−

Experimental Details: We consider both transfer and multi-task learning settings. For the transfer
experiment, we use the Rosenbrock, Ackley and Sphere functions as the source tasks, and the
Rastrigin function as the target task. We also consider each of the source tasks as the ground truth to
see whether we can identify the correct source task in an ideal controlled setting. In the multi-task
setting, we did not observe any advantages of our algorithm nor the baselines when solving the
Rosenbrock, Ackley or Sphere functions, so we focus on the quality of solution obtained for the
Rastrigin function only. Transfer learning: We ﬁrst generate demonstrations (xi, f (xi)) from each
source function f using DE (Algorithm 2) and conﬁguration above until a ﬁtness of 0.15 is achieved.
Respectively, these have sizes 17693, 6452 and 5853 for each source task. We further transform
outputs for training and prediction with the neural-linear model using a log-transform y
log(1 + y)
to limit the effects of outliers and stabilize the training of the model. When solving the target

(cid:55)→

1

Algorithm 2 Differential Evolution (DE)

Require: f : R

D → R, CR ∈ [0, 1], F ∈ [0, 2], N P ≥ 4

initialize N P points P uniformly at random in the search space
for generation m = 1, 2, . . . do

for agent x ∈ P do

randomly select three candidates a, b, c ∈ P that are distinct from each other and from x
pick a random index R ∈ {1, 2 . . . D}
for i = 1, 2 . . . D do
pick ri ∼ U (0, 1)
if ri < CR or i = R then

set yi = ai + F (bi − ci)

else

set yi = xi

end if
if f (y) ≤ f (x) then

replace x in P with y

end if
end for

end for

end for
return the agent in P with the least function value

B

is a singleton set containing the best solution from the source data, and Obase is
task, the batch
trained by replacing the ﬁrst of the three sampled candidates prior to crossover with probability
pm = 0.99m, where m is the index of the current generation (following the structure of Algorithm 1).
This guarantees that the new swarm will possess the traits of the source solutions with high probability,
but still maintain diversity so the solution can be improved further. Multi-task learning: We solve
all source and target functions simultaneously, sharing the best solutions between them using the
mechanisms outlined above for the transfer learning experiment. One QP is maintained for each task
to learn weightings over the other tasks excluding itself. Here, we set pm = 0.3, so that the best
obtained solutions can be consistently shared between tasks over time.

Details for the Supply Chain Benchmark

Problem Setting: Demand in units per time step is dt,i
7, 6, 6, 5, 5, 5
}
for warehouses (factory demand is zero) and is not backlogged but lost forever during shortages. The
unit selling price is 0.6, the unit production cost is 0.1, the unit storage cost per time step is 0.03 and
the maximum storage capacity is capped at 50 for the factory and each warehouse. A truck can ship
only 4 units, but the controller can dispatch an unlimited number of trucks from any location.

Poisson(µi), where µi is

∼

{

Solver Settings: We use the Deep Deterministic Policy Gradient (DDPG) Algorithm [30] to
solve this problem. The critic network has 300 hidden units per layer. We also use L2 penalty
λ = 10−4, learning rates 10−4 and 2
10−3)
×
initialization for weights in output layers, discount factor γ = 0.96, horizon T = 200, randomized
(0, σ2
replay with capacity 10000, batch size of 32, and explore using independent Gaussian noise
t ),
where σt is annealed from 0.15 to 0 linearly over 50000 training steps.

10−4 for actor and critic respectively, U (

10−3, 3

3
−

N

×

×

Experimental Details: We consider the transfer from multiple source tasks (Scenarios 1, 2 and 3)
to a single target task (Target Task) as indicated in Figure 4a. We also consider each source task as a
ground truth. To collect source data, we train DDPG with the above hyper-parameters on each source
task for 30000 time steps, then randomly sub-sample 10000 observations. This last step demonstrates
whether our approach can learn stable representations with limited data and incomplete exploration
trajectories. Since regression is sensitive to outliers, when training the neural linear model, we remove
2.5% of the samples with the highest and lowest rewards (we also exclude observations collected
from the target task that lie outside any of these bounds). In order to implement transfer, we set
pm = 0.95m, where m is the current episode number. This provides a reasonable balance between
exploration of source data and exploitation of target data. In accordance with Algorithm 1, we sample
batches

of size 32 uniformly at random from the source data.

B

2

Details for Prioritized Experience Replay

Experimental Details: We evaluated the performance of Prioritized Experience Replay [39] (PER)
on the Supply Chain benchmark. In summary, PER is a replay buffer that ranks experiences using the
Bellman error, deﬁned for the DDPG algorithm for a transition (s, a, r, s(cid:48)) as

where Q is the critic network, and Q(cid:48) and µ(cid:48) are the target critic and target actor networks, respectively.

δ = r + γQ(cid:48)(s(cid:48), µ(cid:48)(s(cid:48)))

Q(s, a),

−

In order to use PER to transfer experiences, we load all source demonstrations into the replay buffer
prior to target task training with initial Bellman error δ = 1. The source demonstrations are ﬁrst
combined into a single data set and shufﬂed. The capacity of the buffer is then ﬁxed to the total
number of source demonstrations from all tasks (around 28,000 examples). During target training,
observed transitions are immediately added to the replay buffer and override the oldest experience.
In this way, the agent is able to maximize the use of the source data at the beginning of training but
eventually shift emphasis to target task data.

Detailed Analysis: Figure 7 demonstrates the composition of each batch (of size 32) according
to source (Scenario 1, Scenario 2, Scenario 3, Target Task) for each ground truth. As illustrated
in all four plots, in early stages of training, batches are predominantly composed of source data,
while in later stages, they consist entirely of target data. Figure 7 shows that PER is unable to favor
demonstrations from the source task that correspond to the ground truth, in all four problem settings.
One possible explanation of this is that an implicit assumption of PER is violated, namely that
experiences are drawn from the same distribution of rewards, whereas in our experiment, experiences
can come from tasks with contradictory goals. In this case, experiences corresponding to the ground
truth do not necessarily have larger Bellman error (in fact, the opposite may be true).

(a) Scenario 1

(b) Scenario 2

(c) Scenario 3

(d) Scenario 4

Figure 7: Shows the composition of each batch over time sampled from the prioritized replay (PER)
according to whether the sample came from the source or target task data, for each ground truth. PER
is unable to rank experiences correctly to match the context.

3

010K20K30Kbatch number032sourceS1S2S3Target010K20K30Kbatch number032sourceS1S2S3Target010K20K30Kbatch number032sourceS1S2S3Target010K20K30Kbatch number032sourceS1S2S3Target