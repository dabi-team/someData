0
2
0
2

n
u
J

9
2

]

G
L
.
s
c
[

1
v
3
5
7
5
1
.
6
0
0
2
:
v
i
X
r
a

Neural Time Warping For Multiple Sequence Alignment

Keisuke Kawano, Takuro Kutsuna, Satoshi Koide

Toyota Central R&D Labs. Inc., Nagakute, Aichi, Japan

Abstract

Multiple sequences alignment (MSA) is a traditional and challenging task for time-series analyses.
The MSA problem is formulated as a discrete optimization problem and is typically solved by dynamic
programming. However, the computational complexity increases exponentially with respect to the number
of input sequences. In this paper, we propose neural time warping (NTW) that relaxes the original MSA
to a continuous optimization and obtains the alignments using a neural network. The solution obtained
by NTW is guaranteed to be a feasible solution for the original discrete optimization problem under
mild conditions. Our experimental results show that NTW successfully aligns a hundred time-series
and signiﬁcantly outperforms existing methods for solving the MSA problem. In addition, we show a
method for obtaining average time-series data as one of applications of NTW. Compared to the existing
barycenters, the mean time series data retains the features of the input time-series data.

1 Introduction

Time-series alignment is one of the most fundamental operations in processing sequential data, and has
various applications including speech recognition [1], computer vision [2, 3], and human activity analysis [4, 5].
Alignment can be written as a discrete optimization problem that determines how to warp each sequence to
match the timing of patterns in the sequences. Dynamic time warping (DTW) [6] is a prominent approach,
which ﬁnds the optimal alignment using the dynamic programming technique. However, for multiple sequence
alignment (MSA), it is diﬃcult to utilize DTW because the computational complexity increases exponentially
with the number of time-series data. In this paper, we propose a new algorithm, neural time warping (NTW)
to solve MSA, which can even be utilized for one hundred time-series data.

An important idea for MSA is to relax the discrete optimization problem to a continuous optimization
problem with the warpings modeled by combining basis functions [7, 8]. Generalized time warping (GTW) [7]
has been proposed to express warpings by a linear combination of predeﬁned basis functions. However, GTW
requires a large number of basis functions to express complex warpings. Trainable time warping (TTW) [8]
models the warpings using discrete sine transform (DST), i.e., sine functions as the basis functions. TTW
controls the non-linearity of the warpings by limiting the number of DST coeﬃcients to avoid sudden changes
in the warpings. The solution of the original problem can be obtained by converting the solution of the relaxed
continuous optimization. However, the converted warpings of the existing methods may not be valid solutions
for the original MSA problems. In contrast, NTW guarantees the validity; as such, NTW has the ability to
express ﬂexible warpings.

Neural networks have been successfully applied to various types of applications, and recently some neural
network-based alignment methods have been proposed [9, 10]. NeuMATCH [9] is a model that outputs
alignments for the matching of videos and texts. NeuralWarp [10] predicts whether or not to align frames of
the sequence using embedded features. However, these methods are supervised learning-based approaches,
i.e., they require many true alignments, while NTW aligns the sequences optimizing a neural network in an
unsupervised manner for each MSA problem.

One of applications of time-series alignment is to ﬁnd a representative time-series from a large number of
time-series data. DTW Barycenter Averaging (DBA) [11] provides DTW barycenters, which minimizes the
sum of the DTW discrepancies to the input time-series. DBA repeats (1) aligning each time-series to the

1

 
 
 
 
 
 
current barycenter sequence and (2) updating the barycenter alternately. Unlike DTW barycenters, we can
easily obtain statistics (e.g., averages and standard deviation) of the aligned time-series using NTW.

Contributions: For the executable and accurate MTSAs, we propose NTW, which models ﬂexible
warpings with a neural network, thereby guaranteeing the feasibility after discretization (Section 2). To
mitigate poor local minima problems that often occur in the gradient-based optimization of MSA problems,
we propose an annealing method (Section 2.5), which aligns only the low-frequency parts in the early steps
of the optimization, and then gradually aligns the high-frequency parts. Furthermore, we demonstrate the
advantage of NTW against existing methods using UCR time series classiﬁcation archive [12] (Section 3).
Lastly, we show some examples of averaged time-series data obtained from aligned sequences (Section 3.3).

Notations: We denote {0, . . . , N } as

N

(cid:74)

. 1N ∈ RN is a N dimensional vector of ones.
(cid:75)

2 Neural Time Warping

2.1 Problem setting

Given N time-series, xi = [x(0)
optimization problem, which minimizes the sum of distances between warped time-series:

] ∈ RTi+1, i = 1, . . . , N , the MSA problem can be formulated as the

, . . . , x(Ti)

i

i

min
τ ∈T

Z
(cid:88)

N
(cid:88)

N
(cid:88)

z=0

i=1

j=1

d(ˆxi(z; τi), ˆxj(z; τj)).

(1)

Here, T is a set of warpings τ = [τ1, . . . , τN ], which satisﬁes the following constraints for all τi :

(cid:55)→

Z

(cid:74)

(cid:75)

(cid:74)

τi(z) ≤ τi(z + 1),
τi(z + 1) − τi(z) ≤ 1, (continuity)
τi(0) = 0, τi(Z) = Ti. (boundary conditions)

(monotonically nondecreasing)

Ti

.

(cid:75)
(2)

(3)

(4)

t=0 x(t)

ˆxi(z; τi) := (cid:80)Ti
(cid:55)→ R) corresponding to a warped time-series of xi
Z
i δ(t − τi(z)) denotes a function (
(cid:74)
by τi, where δ is the delta function. d is a distance function between frames (e.g., Euclidean distance) and
the length of the warped time-series is Z + 1. Note that we can extend the alignment problem to multi-
dimensional time-series using an appropriate distance function d [3]. DTW [6] obtains the optimal τ and Z
with the dynamic programming technique. However, the computational complexity of DTW is O((cid:81)N
i=1 Ti),
thereby making it diﬃcult to apply DTW to the MSA problem.

(cid:75)

2.2 Continuous warpings

To reduce the computational complexity, NTW relaxes the discrete optimization problem to a continuous
optimization problem, and searches the optimal continuous warpings τ (cid:48)(s) = [τ (cid:48)
N (s)] : [0, S] (cid:55)→
[0, 1]N instead of τ . Note that any positive value can be assigned to S without a loss of generality. First, we
interpolate the input time-series with sinc interpolation:

1(s), . . . , τ (cid:48)

i(t(cid:48)) :=
x(cid:48)

Ti(cid:88)

t=0

x(t)
i sinc (t − t(cid:48)Ti) , sinc(t) :=

(cid:40) sin(πt)
t

1

t (cid:54)= 0
t = 0,

where we assume t(cid:48) ∈ [0, 1]. With a continuous warping τ (cid:48)
i(τ (cid:48)
x(cid:48)

i (s)) [8]. For clarity, we denote the warpings of the original MSA problem as discrete warpings.
Given a continuous warping τ (cid:48), a discrete warping ˜τi of length Z + 1 can be calculated in the following
way: (1) prepare monotonically increasing points [s0, . . . , sZ] such that sz ∈ [0, S], s0 = 0, and sZ = S, (2)
calculate the discrete warping ˜τi for i = 1, . . . , N by ˜τi(z) := (cid:98)Tiτ (cid:48)
i (sz)(cid:99), where (cid:98)·(cid:99) is a ﬂoor function. In the
following sections, we assume sz to be at regular intervals in [0, S], i.e., sz = z
. A discrete
Z S for z ∈
warping obtained from a continuous warping by the above procedure is called a sampled warping.

i (s), the warped time-series can be written as

Z

(cid:74)

(cid:75)

2

2.3 Modeling of the warpings

It is necessary to model the warpings τ (cid:48) with a ﬂexible function that can operate even when optimal warpings
have complex forms. However, it is diﬃcult to guarantee that the sampled warpings satisfy the alignment
constraints (2–4) with the ﬂexible function. NTW carefully models τ (cid:48)
θ using the orthogonal basis ek ∈
RN , k = 1, . . . , N , and a non-linear function φθ(s) : [0, S] (cid:55)→ RN −1 with its parameters θ as follows.

τ (cid:48)
θ(s) := se1 + s (S − s)

N −1
(cid:88)

[φθ(s)]kek+1,

k=1

(5)

1N . The following proposition holds for any functions φθ in Eq.(5).

√

N , the sampled warpings ˜τi obtained from τ (cid:48)

θ in Eq.(5) satisﬁes the boundary

where e1 = 1√
N

Proposition 1. If we set S =
condition (4).

In the followings, we assume S =

√

N . Then the following proposition holds:

Proposition 2. If τ (cid:48)
θ,i(sz+1) − τ (cid:48)
sampled warping ˜τ obtained from τ (cid:48)

θ,i(sz) ≥ 0 (∀i ∈ {1, . . . , N }, ∀z ∈
θ in Eq.(5) satisﬁes the monotonicity (2) and the continuity (3).

) and Z ≥ N maxN

Z

(cid:75)

(cid:74)

j=1 Tj, then the

Proof. From the ﬁrst assumption, the sampled warpings obviously satisfy the monotonicity (2). By consid-
ering the regular intervals sz+1 − sz =

√
N
Z , we have

N
(cid:88)

i=1

N
(cid:88)

i=1

=

θ,i(sz+1) − τ (cid:48)
τ (cid:48)

θ,i(sz)

√

N
Z

e1,i +

√

(cid:2)sz+1(

N −1
(cid:88)

k=1

N − sz+1)φθ(sz+1)

√

− sz(

N − sz)φθ(sz)(cid:3)

k

N
(cid:88)

i=1

ek+1,i.

(6)

We also have (cid:80)N
we have (cid:80)N
holds for all i. By multiplying Ti, for all i, we obtain

i=1 ek+1,i = 0, ∀k ∈ {1, . . . , N − 1} because ek+1 are orthogonal to e1 = 1√
N
θ,i(sz+1) − τ (cid:48)
θ,i(sz+1) − τ (cid:48)

Z . Furthermore, from τ (cid:48)

θ,i(sz) ≥ 0, τ (cid:48)

θ,i(sz+1) − τ (cid:48)

θ,i(sz) = N

i=1 τ (cid:48)

1N . Therefore,
θ,i(sz) ≤ N
Z

Ti(τ (cid:48)

θ,i(sz+1) − τ (cid:48)

θ,i(sz)) ≤

TiN
Z

≤

Ti
maxN
j=1 Tj

≤ 1.

(7)

Because a − b ≤ 1 ⇒ (cid:98)a(cid:99) − (cid:98)b(cid:99) ≤ 1 holds in general, we obtain the continuity (3) as

˜τi(sz+1) − ˜τi(sz) = (cid:98)Tiτ (cid:48)

θ,i(sz+1)(cid:99) − (cid:98)Tiτ (cid:48)

θ,i(sz)(cid:99) ≤ 1.

Instead of Eq.(5), if we employ another model, e.g.,

τ (cid:48)
θ(s) = s

(cid:16)√

(cid:17) N
(cid:88)

N − s

[ψ(s)]kek +

k=1

s
S

with ψ : R (cid:55)→ RN , the sampled warpings satisfy the boundary condition (4), but not necessarily the con-
tinuity (3). Similar to the alternative model, a discrete warping obtained with TTW [8] also satisﬁes the
boundary condition (4) and the monotonicity (2), but does not always satisfy the continuity (3). A discrete
alignment obtained with GTW [7] does not always satisfy the boundary condition (4) and the continuity (3).
In contrast, NTW guarantees that the obtained discrete alignment satisﬁes the conditions (2–4) as long as
τ (cid:48)
θ satisﬁes the condition described in Proposition 2. In NTW, a neural network is employed as the non-
linear functions for the warpings. The network parameters can be learned with end-to-end gradient-based
optimization.

3

2.4 Optimization

From Proposition 2, we consider the following continuous optimization problem for NTW.

√

N

(cid:90)

N
(cid:88)

N
(cid:88)

min
θ

s=0
i=1
θ,i(sz+1) ≥ τ (cid:48)

s.t., τ (cid:48)

j=1

(x(cid:48)

i(τ (cid:48)

θ,i(s)) − x(cid:48)

j(τ (cid:48)

θ,j(s)))2ds,

(8)

θ,i(sz), (∀i ∈ {1, ..., N }, ∀z ∈

Z
(cid:74)

).
(cid:75)

To utilize recent optimization techniques for neural networks [13], we employ the following unconstrained
optimization problem with a penalty term r.

√

N

(cid:90)

N
(cid:88)

N
(cid:88)

s=0

i=1

j=1

min
θ

(x(cid:48)

i(τ (cid:48)

θ,i(s)) − x(cid:48)

j(τ (cid:48)

θ,j(s)))2ds + λr(τ (cid:48)

θ),

r(τ (cid:48)

θ) :=

N
(cid:88)

Z−1
(cid:88)

i=1

z=0

max (cid:0)τ (cid:48)

θ,i(sz+1) − τ (cid:48)

θ,i(sz), 0(cid:1) ,

(9)

where λ is a hyperparameter for the penalty term. Using a large enough λ, the solutions satisfying the
monotonicity on sz can be obtained. We approximate the integration by the trapezoidal rule because s is a
one dimensional variable. Note that Proposition 2 hold as long as r(τ (cid:48)
θ) = 0. Finally, we obtain the following
objective function for NTW,

min
θ

Z(cid:48)−1
(cid:88)

N
(cid:88)

N
(cid:88)

(x(cid:48)

i(τ (cid:48)

θ,i(sz(cid:48)+1)) − x(cid:48)

j(τ (cid:48)

θ,j(sz(cid:48))))2ds + λr(τ (cid:48)

θ),

z(cid:48)=0

i=1

j=1

r(τ (cid:48)

θ) :=

N
(cid:88)

Z−1
(cid:88)

i=1

z=0

max (cid:0)τ (cid:48)

θ,i(sz+1) − τ (cid:48)

θ,i(sz), 0(cid:1) ,

(10)

where z(cid:48) is the sampling points for the trapezoidal rule.

The objective function is calculated by GPU parallel computing. Moreover, it can be easily implemented
with modules for tensor operations (e.g., PyTorch [14]) and the gradients of the parameters θ can be obtained
by automatic diﬀerentiation [15]. When GPU memory is not enough, mini-batch optimization can be used
for NTW.

2.5 Annealing method

A further problem of the optimization for NTW is the presence of many poor local minima, which correspond
to align only part of the high-frequency components of the input time-series. To mitigate this problem, we
extend the sinc function with an annealing parameter α as

sinc(cid:48)(t; α) =

(cid:40) sin(απt)
αt

1

t (cid:54)= 0
t = 0.

(11)

When α = 1, sinc(cid:48) is equivalent to the original sinc function. Figure 1(a) shows an example of interpolated
time-series varying the annealing parameter α. As shown in Figure 1(a), when α (cid:29) 1, the interpolated
time-series represent only the low-frequency components of the original time-series. By gradually annealing
α from a suﬃciently large value to 1, the optimization ﬁrst aligns the low-frequency components of the time-
series and then gradually aligns the high-frequency components. To demonstrate the annealing technique,
Figure 1(b) shows an example of loss histories (i.e., the values of the objective function in the problem (8))
during the optimizations. For the “w/o annealing” setting, the annealing parameter α is ﬁxed at 1 during
the optimization, while we initialized α to 100 then multiplied 0.99 in each update for the “with annealing”
setting. Details of the implementation are presented in Section 3. As the result shows, NTW without the
annealing easily falls into local minima, while NTW with the annealing converges to better solutions.

4

(a) Interpolated time-series varying an-
nealing parameters (50words dataset, la-
bel 33 [12]).

(b) Loss histories of NTW, with and
without annealing technique.

Figure 1: Examples for annealing method

2.6 Warped Average of Time-Series Data

Given N aligned time-series, x(cid:48)
For example, the average time-series can be obtained as ¯x := 1
N

i ∈ RZ, i = 1, . . . , N , we can easily obtain statistics of the aligned time-series.
i=1 x(cid:48)
i. Similarly, the standard deviation
i(˜τi(s)) − ¯x(s))2. We call ¯x(s) and xSD(s) warped average

i=1 (x(cid:48)

(cid:80)N

(cid:80)N

(cid:113)

1
N

of the time-series data is xSD(s) :=
and warped standard deviation, respectively.

3 Experiments

3.1 Dataset

In this section, we compare NTW with existing methods for MSA, namely GTW [7] and TTW [8] using the
UCR time series classiﬁcation archive (2015) [12]. We also employ UCR time series classiﬁcation archive
(2015) for comparing warped averages acquired by NTW with barycenters.

This archive includes 85 datasets of time series, such as human actions and medical imaging. One dataset
contains up to 60 class labels and is divided into train and test parts. For the evaluation, we use all training
splits of each class in each dataset if they have more than one time-series data (642 sets in total). Due to
limited computational resources, we randomly sampled 100 time-series from a set containing more than 100
time-series data.

3.2 Multiple Sequences Alignment

Evaluation metric: We cannot evaluate the accuracies of MSA directly, because it is diﬃcult to obtain
the true alignments for a large number of time-series data. Following [8], we employed the barycenter loss;

Lb :=

1
N Z

N
(cid:88)

i=1



DDTW

xi,



x(cid:48)
j(˜τj)

 ,

1
N

N
(cid:88)

j=1

(12)

where DDTW denotes the DTW discrepancy [6].

To compare validities of the sampled warpings, we also employ the following validity scores, which measure

5

0100200024=1=25=10005001000# updates0.20.40.6losswith annealingw/o annealingTable 1: Average barycenter losses Lb (10−2).
NTW TTW (best K) TTW(4) TTW(16) GTW

1.515

1.645

1.715

1.812

1.600

Table 2: Average validity scores (%)

NTW TTW(4) TTW(16) GTW

Vmono
Vcont
Vbound

100.0
100.0
100.0

100.0
91.3
100.0

100.0
85.7
100.0

100.0
95.8
84.8

the ratios of frames satisfying the constraints of MSA (2–4), respectively;

Vmono :=

Vcont :=

1
N Z

1
N Z

Z−1
(cid:88)

N
(cid:88)

z=0

i=1

Z−1
(cid:88)

N
(cid:88)

z=0

i=1

I(˜τi(z + 1) ≥ ˜τi(z))

I(˜τi(z + 1) − ˜τi(z) ≤ 1)

Vbound :=

1
2

N
(cid:88)

i=1

(I(˜τi(0) = 0) + I(˜τi(Z) = Ti)) ,

(13)

(14)

(15)

where I(cond) is 1 when cond is true, and 0 otherwise.

Implementations: We implemented NTW using PyTorch (version 1.2) [14] and experimented with
GPUs (NVIDIA TITAN RTX) (See Appendix A for more details). The neural network is a four-layered,
fully connected network with ReLU activations [16] and skip connections [17], where the number of the
nodes are 1-512-512-1025-[N − 1], and all weights and biases are initialized to zero. Via this initialization,
NTW initializes the warpings uniformly (i.e., all frames are uniformly warped to align the lengths of the
time-series). See Section A for more details. We updated the parameter 1000 times using Adam [13] with its
default parameters in the PyTorch [14] implementation except for the learning rate of 0.0001. The penalty
parameter is set to λ = 1000, with which the solutions of NTW almost satisfy the monotonicity condition as
shown in the following section. The number of sampling points for the trapezoidal rule is the same as the
length of the input time-series. The annealing parameter α is initialized to 100 and multiplied by 0.99 after
every update while it is larger than 1. For more details, see Section B.

For TTW [8] and GTW [7], we employed MATLAB implementations provided by the corresponding
authors1. As it was reported in [8] that the performance of TTW varies depending on the number of DST
coeﬃcients K, we employed two models for TTW (K = 4 and 16).

Results: Table 1 and Figure 2 show some examples of MSA results and average values of barycenter
losses, respectively; we chose better results of TTWs (K = 4 and K = 16) in each set for TTW (best K).
We conducted a pair-wise t-test (one-sided) over the 642 results. As shown in the Table 1, NTW signiﬁcantly
outperformed both TTW and GTW (p-value<0.02) on the barycenter losses. The examples of MSA in
Fig 2 indicate that NTW has enough ﬂexibility to approximate the complex warpings even when the number
of time-series is 100 (uWaveGestureLibrary Y (8) and yoga (2)). Table 2 shows the average values of the
validity scores, which indicate the results of NTW satisﬁed the continuity (3) and boundary conditions (4)
for all MSAs. Note that the results of NTW rarely (0.0003% on average) broke the monotonicity because
the unconstrained optimization (9) was used instead of constrained optimization (8).

1TTW: https://github.com/soheil-khorram/TTW,

GTW: https://github.com/LaikinasAkauntas/ctw

6

Figure 2: Examples of multiple sequence alignment results. Titles are dataset names and labels.

3.3 Warped Average and Barycenters

We then visually compare the warped averages acquired by MTSAs and the DTW barycenter. To calculate
the warped average of time-series data for NTW, TTW and GTW, we utilize the aligned time-series data
obtained in the previous section. For DBA [11], we employ the tslearn implementation [18].

In Figure 3, some examples of warped averages obtained by NTW, TTW and GTW and DTW barycenters
are shown. As the ﬁgure indicates, the warped averages obtained by NTW retain the characteristics of
the input time-series data. Although the DTW discrepancy between the input time-series data and the
barycenters are small, the DTW barycenters obtained by DBA have the sharp peaks that do not exist in
the input time-series data. Note that the DTW discrepancies between the DTW barycenters and the input
time-series data are smaller than the DTW discrepancies between the warped averages and the input time-
series data. This is because the sharp peaks can be warped to be matched to the input time-series data
when calculating the DTW discrepancies. The average time-series obtained with TTW and GTW could not
represent the characteristics of the input time-series data for some datasets (e.g., Lighting7 (0)) because of
the failure of the alignments.

4 Conclusion

We proposed a new method for MSA, neural time warping (NTW), which enabled alignments for as many as
100 time-series. NTW modeled the warpings as outputs of a neural network optimized for each MSA problem.
The solution of NTW is guaranteed to be a valid solution to the original discrete problem under mild condi-
tions. We also proposed the annealing method to mitigate the problem of poor local minima. Experiments
using UCR time series classiﬁcation archive showed that NTW outperformed the existing multiple alignment
methods. The warped averages obtained by NTW retain the characteristics of the input time-series data.

7

DataNTWTTW(4)GTW50words (33)InlineSkate (5)Lighting7 (0)yoga (2)TTW(16)uWaveGestureLibrary_Y (8)Figure 3: Examples of warped average and barycenters. Titles are dataset names and labels. The lines
represent the warped averages (blue: NTW, green and red: TTW, and purple:GTW) and DTW barycenter
(orange:DBA). The area denotes warped standard deviations. Note that we cannot obtain warped standard
deviations for DTW barycenters.

References

[1] Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, and Shigeki Sagayama, “Dynamic time-alignment
kernel in support vector machine,” in Advances in neural information processing systems (Neurips),
2002, pp. 921–928.

[2] Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and Juan Carlos Niebles, “D3TW: Discriminative
diﬀerentiable dynamic time warping for weakly supervised action alignment and segmentation,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.

[3] George Trigeorgis, Mihalis A Nicolaou, Stefanos Zafeiriou, and Bjorn W Schuller, “Deep canonical
time warping,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp.
5110–5118.

[4] Yaser Sheikh, Mumtaz Sheikh, and Mubarak Shah, “Exploring the space of a human action,” in

International Conference on Computer Vision (ICCV). IEEE, 2005, vol. 1, pp. 144–149.

[5] Alexei Gritai, Yaser Sheikh, Cen Rao, and Mubarak Shah, “Matching trajectories of anatomical land-
marks under viewpoint, anthropometric and temporal transforms,” International journal of computer
vision, vol. 84, no. 3, pp. 325–343, 2009.

[6] Hiroaki Sakoe and Seibi Chiba, “Dynamic programming algorithm optimization for spoken word recog-

nition,” 1978, vol. 26, pp. 43–49, IEEE.

[7] Feng Zhou and Fernando De la Torre, “Generalized time warping for multi-modal alignment of human
motion,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp.
1282–1289.

[8] Soheil Khorram, Melvin G McInnis, and Emily Mower Provost, “Trainable time warping: Aligning
time-series in the continuous-time domain,” in IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2019, pp. 3502–3506.

8

DataNTWTTW(4)GTW50words (33)InlineSkate (5)Lighting7 (0)yoga (2)TTW(16)uWaveGestureLibrary_Y (8)DataNTWTTW(4)GTW50words (33)InlineSkate (5)Lighting7 (0)yoga (2)TTW(16)uWaveGestureLibrary_Y (8)DBA[9] Pelin Dogan, Boyang Li, Leonid Sigal, and Markus Gross, “A Neural Multi-sequence Alignment TeCH-
nique (NeuMATCH),” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.

[10] Josif Grabocka and Lars Schmidt-Thieme, “NeuralWarp: Time-series similarity with warping networks,”

arXiv preprint arXiv:1812.08306, 2018.

[11] Fran¸cois Petitjean, Alain Ketterlin, and Pierre Gan¸carski, “A global averaging method for dynamic time
warping, with applications to clustering,” Pattern Recognition, vol. 44, no. 3, pp. 678–693, 2011.

[12] Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, and
Gustavo Batista, “The UCR Time Series Classiﬁcation Archive,” July 2015, www.cs.ucr.edu/~eamonn/
time_series_data/.

[13] Diederick P Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” in International

Conference on Learning Representations (ICLR), 2015.

[14] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer, “Automatic diﬀerentiation in PyTorch,” in
Neurips Autodiﬀ Workshop, 2017.

[15] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeﬀrey Mark Siskind,
“Automatic diﬀerentiation in machine learning: a survey,” Journal of machine learning research, vol.
18, no. 153, 2018.

[16] Vinod Nair and Geoﬀrey E Hinton, “Rectiﬁed linear units improve restricted boltzmann machines,” in

International conference on machine learning (ICML), 2010, pp. 807–814.

[17] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,”
in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 2261–2269.

[18] Romain Tavenard, Johann Faouzi, Gilles Vandewiele, Felix Divo, Guillaume Androz, Chester Holtz,
Marie Payne, Roman Yurchak, Marc Rußwurm, Kushal Kolar, and Eli Woods, “tslearn: A machine
learning toolkit dedicated to time-series data,” 2017, https://github.com/tslearn-team/tslearn.

9

A Architecture of NTW

Figure 4 illustrates the architecture of NTW employed for the experiments in Section 3.

Figure 4: Architecture of NTW employed for the experiments in Section 3

10

FCN +ReLUFCN +ReLU1Input (s)512512FCNConcatenate1025Fully connected + ReLUFully connectedEq. (5)Calc Eq. (5)N-1NOutputB Pseudo-code of NTW

For reproducibility, we show the PyTorch-like pseudo-code of NTW below.

c l a s s SincWarping ( nn . Module ) :

d e f

i n i t

( s e l f , w a r p e d l e n g t h ,

t e m p e r a t u r e ,

t e m p e r a t u r e m u l t i p l i e d=None ) :

s e l f ) .

s u p e r ( SincWarping ,
s e l f . i n i t i a l t e m p e r a t u r e = t e m p e r a t u r e
s e l f . t e m p e r a t u r e = t e m p e r a t u r e
s e l f . t e m p e r a t u r e m u l t i p l i e d = t e m p e r a t u r e m u l t i p l i e d
s e l f . w a r p e d l e n g t h = w a r p e d l e n g t h

i n i t

( )

d e f

f o r w a r d ( s e l f , Xs ,
N, Tx , d = Xs . s h a p e
a r a n g e T x = t o r c h . a r a n g e ( Tx , d e v i c e=Xs . d e v i c e ) . f l o a t ( )

t a u ) :

t s = ( ta u ∗ ( Tx − 1 ) ) . u n s q u e e z e ( 2 ) . expand (N,
temp = np . p i ∗ ( t s ∗ 1 / s e l f . t e m p e r a t u r e ) + 1 e−7
s i n c r e t = t o r c h . s i n ( temp ) / ( temp )

s e l f . w a r p e d l e n g t h , Tx ) − a r a n g e T x

warped Xs = t o r c h . matmul (

s i n c r e t , Xs ) / s e l f . t e m p e r a t u r e

i f

s e l f . t r a i n i n g :

i f

s e l f . t e m p e r a t u r e m u l t i p l i e d i s no t None :

s e l f . t e m p e r a t u r e = max ( s e l f . t e m p e r a t u r e ∗ s e l f . t e m p e r a t u r e m u l t i p l i e d , 1 )

r e t u r n warped Xs

c l a s s NTW( nn . Module ) :

d e f

i n i t

( s e l f , N,

i n i t i a l t e m p e r a t u r e ,
t e m p e r a t u r e m u l t i p l i e d ,
w a r p e d l e n g t h ) :

s u p e r (NTW,

s e l f ) .

i n i t

( )

s e l f . w a r p e d l e n g t h = w a r p e d l e n g t h
s e l f . warp = SincWarping ( w a r p e d l e n g t h ,

i n i t i a l t e m p e r a t u r e ,

t e m p e r a t u r e m u l t i p l i e d )

s e l f . n e t = Net (N − 1 )
d = t o r c h . z e r o s (N − 1 , N)
f o r n i n r a n g e (N − 1 ) :

d [ n , n ] = 1

d = t o r c h . c a t ( [ t o r c h . o n e s (N ) . u n s q u e e z e ( 0 ) , d ] )
s e l f . b a s e s = nn . Parameter (− t o r c h . q r ( d . t ( ) ) [ 0 ] ,
s e l f . n e t o u t 2 f z = lambda n e t o u t p u t s ,

s z , b a s e s :

r e q u i r e s g r a d=F a l s e )

( t o r c h . c a t ( [ s z , n e t o u t p u t s ] , 1 ) @ b a s e s . t ( ) ) . t ( )

d e f

f o r w a r d ( s e l f , Xs ) :
N = Xs . s h a p e [ 0 ]
s z = t o r c h . l i n s p a c e ( 0 , N∗ ∗ 0 . 5 ,
n e t o u t p u t s = s e l f . n e t ( s z )
t a u = s e l f . n e t o u t 2 f z ( n e t o u t p u t s ,

s z ,

s e l f . b a s e s )

s e l f . w a r p e d l e n g t h ) . r e s h a p e ( −1 , 1 ) . t o ( Xs . d e v i c e )

r e t u r n s e l f . warp ( Xs ,

t au ) ,

t a u

# main . py
1 . # d e f i n e a c l a s s o f n e u r a l network f o r NTW
2 . Xs = l o a d d a t a ( )
3 . ntw = NTW( ∗ ∗ a r g s )
4 . w h i l e n ot c o n v e r g e
warped Xs ,
5 .
( l o s s ( warped Xs ) + p e n a l t y ( t a u ) ) . backward ( )
6 .
7 .
o p t i m i z e r . s t e p ( )
8 . o u t p u t warped Xs

t a u = ntw ( Xs )

11

