Draft version May 5, 2021
Typeset using LATEX default style in AASTeX63

1
2
0
2

y
a
M
3

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
6
0
1
1
0
.
5
0
1
2
:
v
i
X
r
a

Star-Galaxy Image Separation with Computationally Eﬃcient Gaussian Process Classiﬁcation

Amanda L. Muyskens,1 Im`ene R. Goumiri,2 Benjamin W. Priest,3 Michael D. Schneider,2
Robert E. Armstrong,2 Jason M. Bernstein,1 and Ryan Dana4

1Engineering Division, Lawrence Livermore National Laboratory
Livermore, CA 94550, USA
2Physics Division, Lawrence Livermore National Laboratory
Livermore, CA 94550, USA
3Center for Applied Scientiﬁc Computing, Lawrence Livermore National Laboratory
Livermore, CA 94550, USA
4Computing Division, Lawrence Livermore National Laboratory
Livermore, CA 94550, USA

ABSTRACT

We introduce a novel method for discerning optical telescope images of stars from those of galaxies
using Gaussian processes (GPs). Although applications of GPs often struggle in high-dimensional data
modalities such as optical image classiﬁcation, we show that a low-dimensional embedding of images
into a metric space deﬁned by the principal components of the data suﬃces to produce high-quality
predictions from real large-scale survey data. We develop a novel method of GP classiﬁcation hyperpa-
rameter training that scales approximately linearly in the number of image observations, which allows
for application of GP models to large-size Hyper Suprime-Cam (HSC) Subaru Strategic Program data.
In our experiments we evaluate the performance of a principal component analysis (PCA) embed-
ded GP predictive model against other machine learning algorithms including a convolutional neural
network and an image photometric morphology discriminator. Our analysis shows that our meth-
ods compare favorably with current methods in optical image classiﬁcation while producing posterior
distributions from the GP regression that can be used to quantify object classiﬁcation uncertainty.
We further describe how classiﬁcation uncertainty can be used to eﬃciently parse large-scale survey
imaging data to produce high-conﬁdence object catalogs.

Keywords: star-galaxy — catalogs — surveys — machine learning — gaussian processes — neural

networks

1. INTRODUCTION

The production of accurate maps of the cosmos from wide-ﬁeld optical sky surveys depends fundamentally on our
ability to produce reliable catalogs of stars and galaxies from photometric imaging. Given a pure galaxy catalog,
measures of galaxy clustering Desjacques et al. (2018) and gravitational lensing shear Kilbinger (2015) can inform
models of cosmology and galaxy evolution. However, obtaining competitive model constraints from existing and
future surveys requires selecting galaxy samples that span most of the visible sky down to magnitude limits well
below that of the best stellar catalogs. When faint stars are erroneously classiﬁed as galaxies, large-angle galaxy
clustering statistics can be biased by the low-order angular moments of the Milky Way stellar spatial distribution.
In measuring cosmic shear from galaxies, stellar contamination in the galaxy sample can systematically reduce the
large-angle shear correlations and increase the large-angle shear-galaxy cross-correlations, which both bias inferences of
the cosmological model. Cosmic shear also requires accurate knowledge of the point spread function (PSF), typically
derived from a sample of stars, that can be biased if misclassiﬁed galaxies are used to derive the PSF. Studies of

Corresponding author: Amanda L. Muyskens
muyskens1@llnl.gov

 
 
 
 
 
 
2

galactic archeology and discovery of new satellites of the Milky Way are also biased by galaxy sample contamination
in the stellar catalog Drlica-Wagner et al. (2015).

With the largest-area optical sky surveys such as Pan-STARRS Flewelling (2018), the Dark Energy Survey (DES) Ab-
bott et al. (2019), the Rubin Observatory Legacy Survey of Space and Time (LSST) Ivezi´c et al. (2019), or Euclid Ami-
aux et al. (2012), star-galaxy mis-classiﬁcations can introduce erroneous signals in the search for relativistic cluster-
ing Yoo (2014), primordial non-Gaussianity Desjacques & Seljak (2010), dark energy clustering Hu & Scranton (2004),
or modiﬁed gravity Renk et al. (2016) constraints. Detection of any such eﬀects beyond the standard cosmological
model could be among the most impactful scientiﬁc results from these surveys if the star-galaxy sample selection can
be improved well beyond that shown in recent works (e.g., Huang et al. 2017; Sevilla-Noarbe et al. 2018). Excess galaxy
clustering detections in the past (e.g., Sawangwit et al. 2011; Thomas et al. 2011) are not yet reliable as indicators of
new physics without further characterization of systematic errors, including star-galaxy mis-classiﬁcations.

Direct methods for star-galaxy image classiﬁcation based upon image summary information suﬃce for relatively
bright objects. For instance, the morphological approach Vasconcellos et al. (2011b); Slater et al. (2020) compares
the apparent shape of the source with the point spread function (PSF), and is employed in the Dark Energy Survey
(DES) The Dark Energy Survey Collaboration (2005); Abbott et al. (2018). The Hyper Surpime-Cam (HSC) Miyazaki
et al. (2018); Aihara et al. (2017) survey similarly compares the ﬂux ratio of a galaxy model to a model using the PSF.
Stars and galaxies also exhibit diﬀerent spectral energy distributions. Pollo, A. et al. (2010) exploit this fact to cluster
images in color space based upon the far-infrared data obtained from astronomical surveys. Many investigators have
also used spectral and morphological features to devise supervised learning approaches using decision trees Vasconcellos
et al. (2011a); Sevilla-Noarbe & Etayo-Sotos (2015) and ensemble methods such as random forests Kim et al. (2015).
The simple approaches using hand-crafted features utilized in the past often fail to correctly classify the faintest
images in modern surveys, as they comparatively contain less information and more noise than earlier datasets.
Furthermore, the sheer volume of data demands automation with minimal human intervention.

Labels for classifying faint star and galaxy images can be obtained from overlapping space-based imaging where
the better resolution allows more reliable detection of resolved galaxy features. However, the Hubble Space Telescope
(HST) has a narrow ﬁeld of view that limits the available labeled data to small subsets of the areas of ground-based
surveys. The upcoming ESA Euclid survey will cover a wide area, but will not reach the limiting magnitudes of most
ground-based wide-ﬁeld surveys. The upcoming NASA Roman survey will reach a limiting magnitude comparable to
that of LSST, but will only cover roughly 10% of the LSST footprint. Citizen science projects such as Galaxy Zoo can
be another source of labeled classiﬁcations.

The volume and complexity of the data suggest the use of representation learning - where a machine learning model
learns an appropriate feature representation in addition to performing prediction. Deep convolution neural networks
(CNNs) are representation learning models that have become very popular in image processing applications due to
their ability to capture and exploit local contours in arrays of adjacent pixels Krizhevsky et al. (2012). Neural networks
(non-convolutional) were previously applied to the star-galaxy classiﬁcation problem by Odewahn et al. (1992); Sevilla-
Noarbe et al. (2018). Recently Kim & Brunner (2016) showed that CNNs compete with random forest classiﬁers on
separating star and galaxy images. Alternatively, kernel models such as support vector machines (SVMs) and Gaussian
processes (GPs) are representation learning models that eschew the explicit construction of a feature map - like CNNs
- and instead rely on a kernel, which speciﬁes a similarity function in a representation space that is nonlinearly related
to the data space. Fadely et al. (2012) used SVMs to solve the star-galaxy separation problem, and in Goumiri
et al. (2020), GPs are utilized to demonstrate that a low-dimensional embedding of the images result in improved
GP classiﬁcation performance. While GPs are very attractive due to their fully-Bayesian inference model, the limited
expressiveness of popular kernel functions as well as the computational scaling compared to CNNs has slowed their
application to high-dimensional domains such as image processing Bradshaw et al. (2017).

In this document we demonstrate the viability of GPs as a tool to solve the star-galaxy separation problem by
extending the work presented in Goumiri et al. (2020). We describe a normalization scheme of pre-processing the
images. Then, we overcome the high dimensionality of the data by way of principle component analysis (PCA),
dramatically restricting the dimensionality of the structured image data to only its most salient features. We describe
a novel method of GP classiﬁcation hyperparameter training that exploits sparsity in the procedure of GP prediction
that incorporates an approximate nearest neighbor algorithm based on a similar method presented for GP regression
in Muyskens et al. (2021). Using this new method, we show that GPs trained on the embedded data are both
computationally eﬃcient and accurate in both small data and large data regimes. We further show that even when

Star-Galaxy Separation with Gaussian Processes

3

optical data is available in multiple ﬁlters, images from a single ﬁlter are as good as those produced from the full
dataset. We also show that the joint PCA-GP model compares favorably to other machine learning baselines on
both the full and PCA-reduced data. Further, our method outperforms the morphological discriminator from the
HSC pipeline even for very few training images, and demonstrates a large improvement for many training images.
Finally we show how we can make use of the full posterior distribution returned by Gaussian processes to interpret the
prediction and quantify uncertainty. By using this uncertainty, one can identify a small class of less-accurate images
that can be veriﬁed by non-automated methods. This process further elevates the performance of the classiﬁer and
describes procedure which to parse and label large batches of star-galaxy images reliably incorporating automation.

2. STAR-GALAXY IMAGE DATA

The data used in this analysis is from the ﬁrst public release of the HSC Subaru Strategic Program Aihara et al.
(2017), the deepest ongoing large-scale survey.
In particular, we selected a training set of star and galaxy images
from the UltraDeep COSMOS ﬁeld which is smaller in area but was observed many more times making it signiﬁcantly
deeper than the main survey. The COSMOS ﬁeld overlaps space-based higher-resolution imaging from the Hubble
Space Telescope (HST) so we cross match the HSC objects to the HST objects that have been labelled as stars or
galaxies by Leauthaud et al. (2007), who identiﬁed stars by looking for the stellar locus in the 2D space of magnitude
and peak surface brightness. While they claim their classiﬁcation to be reliable up to magnitude ∼ 25, beyond which
point sources and galaxies appear identical breaking down their classiﬁcation, the HSC data is quite a bit deeper (∼ 27
in the i ﬁlter), so we must be careful in interpreting results at the faint end.

Figure 1. Examples of normalized i band images (top row) and their corresponding point spread functions (bottom row). a.
and b. are star example observations, and c. and d. are galaxy observations. The star examples are more symmetical, but star
is diﬃcult to distinguish visually from the galaxy example in c. Similarly, galaxy example observation d. although subtly
b.
asymmetric, is more similar in appreance to star a. than galaxy example c.

Of the ﬁve ﬁlters, corresponding to diﬀerent frequency bands that equip HSC, we consider data from the g, r, i,
and z band and exclude the y band data because it is signiﬁcantly shallower than the other bands. For each ﬁlter we
extract images from the deblended HSC images, and reject images that have a signal-to-noise ratio lower than 10 in all
four bands, so as to remove spurious sources and more closely match the HST catalogs. Finally, to get rid of remaining
artifacts and junk objects, we ensure that the ratio of the χ2 value for a circular area of interest (r < 2 FWHMPSF)
over the chi squared value of an annulus region around it (5 FWHMPSF < r < 10 FWHMPSF) must be lower than 4,
where FWHMPSF is the Full Width at Half Maximum (FWHM) of the point spread function (PSF). Example images
and point spread functions for the i band can be seen in 1.

The methodology employed here deﬁnes a classiﬁer that assigns images of celestial objects to the “star” or “galaxy”
labels. The classiﬁer is based upon a workﬂow that processes images and assigns to them conditional probabilities
that they depict stars. Thresholding this probability obtains the ﬁnal label prediction.

3. METHODOLOGY

4

We describe the full workﬂow in detail throughout the remainder of this section. Section 3.1 describes a normalization
and dimensionality reduction procedure using principal component analysis (PCA). Section 3.2 deﬁnes a GP classiﬁer
that accepts this PCA-reduced data and interpolates class labels in terms of a latent predictor. Section 3.3 provides a
novel cross-validation-based hyperparameter estimation training procedure for the GP classiﬁcation model, as well as
providing a scalable implementation leveraging the nearest neighbors structure of the data. Section 3.4 discusses how
to use the GP posterior variance to identify ambiguous images that the model classiﬁes with low conﬁdence. Finally, we
validate our methodology in Section 4 by training such a classiﬁer on a subset of the pre-labeled images and evaluating
its accuracy on the remaining images. Further, we compare to baselines and demonstrate the performance in several
numerical studies of the data.

3.1. Normalization and Dimensionality Reduction

We describe in this section a normalization scheme that produces favorable performance on the star-galaxy classiﬁ-
cation for all methods, although other choices are possible. For a single object, we consider at most 8 images, where
the g, r, i, and z bands each contain a photometric image as well as a point spread function (PSF) image produced
by the HSC pipeline. Further, we crop images to the most central 26 × 26 pixels per image. We then independently
normalize each of these 8 image classes.

We assume throughout n training examples and m test examples. Let W (0)

j,k ∈ R676 be the pixel vector deﬁned by

the ﬂattened image associated with the kth band of the jth image, where k = 1, 2, . . . , 8 and j = 1, 2, . . . , (n + m).

We normalize in two stages. First we remove the background from each image independently,

W 1

j,k = W 0

j,k − min(W 0

j,k)1676.

(1)

Here, 1(cid:96) = {1}(cid:96) is the vector of ones. Then, we normalize the values within each image vector to have at maximum
value of 1,

Then we form a full matrix W ∈ R(n+m)×5408 such that

Wj,k =

W 1
j,k
max(W 1

.

j,k)

(2)

(3)

W =









W T
1,1
W T
2,1
...

W T
1,2
W T
2,2
...

W T

(n+m),1 W T

(n+m),2

... W T
1,8
... W T
2,8
...
. . .
... W T

(n+m),8









Next we reduce the dimensionality of the input data by performing a principal components analysis. For the 8 image
classes, we have a total of 8 × 262 = 5, 408 pixels for each object. We also consider single band images, including both
their photometric image and the corresponding PSF image. These single-band datasets correspond to (n + m) × 1352
vertical slices of W .

Both W and its single-band variants are of a signiﬁcantly higher dimension than is typically considered reasonable to
ﬁt with a GP without dimension reduction. For the rest of this document we will more generally deﬁne W to contain any
subset of vertical slices of the 8 image types for each object included and therefore have dimensions (n + m) × N . Then,
since W T W is proportional to the sample covariance matrix of W , we perform the eigendecomposition W T W = P ΛP T ,
where Λ is a diagonal matrix that contains the sorted eigenvalues and P is a matrix that contains the corresponding
eigenvectors. Deﬁne PL to be a N × L matrix comprised of columns of the ﬁrst L eigenvectors. Then we can compute
our data embedding as W PL ∈ R(n+m)×L.

Performing the full eigendecomposition of such a large matrix is impractical on a typical computer without some
approximation. We approximate the largest eigenvalues and their corresponding eigenvectors using the methods in
Lehoucq et al. (1998). This is implemented using the “eigsh” function in the SciPy Python package Virtanen et al.
(2020). Figure 2 shows several example principal components of the image data for the i band images. We see that the
ﬁrst principal components are symmetrical large-scale features, and as the component number increases, they model
non-symmetric, high-frequency features.

The matrix W PL ∈ R(n+m)×L describes n training images and m testing images that have been embedded into L
dimensions. Let Xtrain = {x1, . . . , xn} be the rows of W PL that correspond to the n embedded training images. These

Star-Galaxy Separation with Gaussian Processes

5

Figure 2. PCA component variables for components 1, 5, 25, 50 (left to right) for both the image (top) and point spread
function (bottom) for the i channel.

images are associated with ground truth class labels z ∈ {−1, +1}n such that zi = +1 when image xi is known to be a
star and zi = −1 otherwise. Similarly, X ∗
m} are the rows of W PL corresponding to the m embedded
testing images.

test = {x∗

1, . . . , x∗

3.2. Gaussian Processes

After the data is embedded on a low-dimensional space, we employ Gaussian Processes (GPs) in order to build a
star-galaxy discrimination model. GPs allow fully-Bayesian inference and quantify uncertainty for a response variable
with a Gaussian posterior distribution. Typically, GPs are used to model continuous response variables, but they
can also be used for classiﬁcation by modeling a latent variable that relates to the diﬀerent classes, see Chapter 3 of
Rasmussen & Williams (2005). For example, with logistic GP regression the logit of the conditional class probabilities
is modeled as a latent Gaussian process. Here we take a simpler approach where the classes are modeled as the sign
of a latent GP. The challenges are to infer the latent Gaussian process from the training data and to predict the GP,
and therefore class label, at a new training point.

In particular, we employ a GP here as a prior distribution over a discrimination function fθ : RL → R, where L is the
embedding dimension of the images described in Section 3.1. Here θ are hyperparameters that will be deﬁned later.
The GP distribution is described by a mean function, m(·), and a positive deﬁnite covariance function, kθ(·, ·). The
notation fθ ∼ GP(m(·), kθ(·, ·)) is used to indicate that a function f has a GP distribution with mean function zero
and covariance function kθ. By convention, the mean function is assumed to be known and so m(·) ≡ 0 is assumed
without a loss of generality.

Let z = (z1, . . . , zn)(cid:48) denote the known class labels and let fθ(xi) denote the corresponding value of the latent

discriminator at input location xi. The class labels are related to the latent GP as

zi = sign(fθ(xi)) =




+1 fi(θ) > 0



−1

fi(θ) < 0.

(4)

Tuning the hyperparameters θ conditioned on observations require estimation whose details we defer to Section 3.3.

6

We will assume that f ∈ Rn constitute evaluations of a continuous, surrogate discrimination function fθ : RL → R on
Xtrain = {x1, . . . , xn}. Further, we assume that y are the “observed” (prior to class label thresholding by application
of Equation (4)) realizations of fθ on Xtrain perturbed by homoscedastic Gaussian noise (cid:15). We seek to interpolate
fθ’s response f∗ ∈ Rm to the unknown testing data X ∗
test = {x∗
m}. The assumption that fθ ∼ GP(0, kθ(·, ·))
imposes the following Bayesian prior model on f , the true evaluations of f on Xtrain:

1, . . . , x∗

= f + (cid:15),

y
σ
f = [fθ(x1), . . . , fθ(xn)](cid:62) ∼ N (0, Kﬀ ),
(cid:15) ∼ N (0, τ 2In).

(5)

Here Kﬀ is an n × n positive deﬁnite covariance matrix on the training data whose (i, j)th element is kθ(xi, xj), and
τ 2 is the variance of the unbiased homoscedastic noise. The deﬁnition of GP regression then speciﬁes that the joint
distribution of all training and testing responses y and f ∗ is given by

(cid:35)

(cid:34)

y
f∗

(cid:32)

(cid:34)

= N

0, σ2

Kﬀ + τ 2In Kf ∗
K∗∗

K∗f

(cid:35)(cid:33)

.

(6)

Here Kf ∗ = K (cid:62)
of Kf ∗ is k(xi, x∗
Finally, we are able to compute the posterior distribution of the testing response f ∗ on X ∗

∗f is the cross-covariance matrix between the training and testing data; that is, the (i, j)th element
j ).

j ). Similarly, K∗∗ is the covariance matrix of the testing data, and has (i, j)th element kθ(x∗

i , x∗

test as

f ∗ | Xtrain, X ∗

test, y ∼ N (¯f ∗, σ2C),

¯f ∗ ≡ K∗f (Kﬀ + τ 2In)−1y,
C ≡ K∗∗ − K∗f (Kﬀ + τ 2In)−1Kf ∗.

(7)

Equation (7) gives the posterior mean ¯f ∗ in closed form. However, this equation depends upon y, the hypothetically
observed perturbed realizations of the surrogate model f , whereas we only have access to the thresholded class indica-
tors z. Some sources such as Rasmussen & Williams (2005) estimate y using a Laplace approximation. However, this
approximation requires repeated covariance matrix solves that are impractical with large training data. Accordingly,
we identify z for y in Equation (7) and use the {−1, +1} labels directly as regression targets, which is similar to
least-squares GP classiﬁcation (see Chapter 6.5 of Rasmussen & Williams (2005)). Identifying y with z gives us an
alternative solution for the posterior mean ¯f ∗:

¯f ∗ ≡ K∗f (Kﬀ + τ 2In)−1z.

(8)

We obtain ¯f ∗ using Equation (8) and reverse the latent variable encoding using Equation (4), learning the predicted
class labels of the training data z∗ ∈ {−1, +1}m. Demonstrations of the classiﬁcation induced by this substitution in a
one-dimensional numerical example is in Figure 3. The ith test image corresponding to input x∗
i is classiﬁed as a star if
the associated latent GP mean is positive and is classiﬁed as a galaxy otherwise. Section 3.4 will additionally describe
using the posterior variance to identify images that the model predicts with low conﬁdence, leading to uncertain class
predictions.

The posterior distribution given in Equations (7) and (8) depends on the choice of kernel function. We select the
Mat´ern kernel, which is a stationary and isotropic kernel that is commonly used in the spatial statistics GP literature
due to its ﬂexibility and favorable properties Stein (2012).
In particular, the Mat´ern kernel is used here since it
allows greater control over the smoothness of the GP than the commonly-used radial basis function kernel. A general
expression for the kernel is

kMat´ern(x, x(cid:48)) = σ2 21−ν
Γ(ν)

(cid:32)√

2ν

(cid:107)x − x(cid:48)(cid:107)2
2
(cid:96)

(cid:33)ν

(cid:32)√

2ν

(cid:107)x − x(cid:48)(cid:107)2
2
(cid:96)

(cid:33)
,

Kν

(9)

where ν > 0 is a smoothness parameter, (cid:96) > 0 is a correlation-length scale hyperparameter, σ2 > 0 is a scale
parameter, Γ is the Gamma function, and Kν(·) is a modiﬁed Bessel function of the second kind. We will optimize
these hyperparameters in our analysis rather than ﬁxing them for convenience. Furthermore, using the previously

Star-Galaxy Separation with Gaussian Processes

7

Figure 3. 1-D example of our GP classiﬁer with various Mat´ern kernel hyperparameters. Lines represent ¯f ∗ and X observations
where ¯f ∗ < 0 are classiﬁed as one class (galaxy), and those with ¯f ∗ > 0 are the other (star).

discussed normalization and dimensionality reduction procedures, the authors observed similar performance from this
kernel, the radial basis function kernel, and the neural network Gaussian process (NNGP) kernel Yang & Salman
(2019), but these results are omitted.

Given a set of hyperparameters, predictions can be obtained as described in Section 3.2. However, the hyperparame-
ters should be selected from the data to provide optimal predictions. The next section discusses a cross-validation-based
approach to hyperparameter estimation that is computationally eﬃcient for large training data sizes.

3.3. Hyperparameter Estimation

GP hyperparameters are typically estimated using a likelihood-based approach such as maximum likelihood esti-
mation or Bayesian calibration. However, the task is challenging in part because evaluating the likelihood is compu-
tationally expensive (O(n3)) and the optimization problem is non-convex. Cross-validation using a grid search can
be employed, but prediction still involves forming the kernel matrix and inverting a large matrix, which can also
be prohibitively expensive, and limit the optimized parameter values. Models such as Fuentes (2001) and Gramacy
et al. (2007) seek to improve computational eﬃciency in hyperparameter optimization by partitioning the domain and
asserting independence over the partitions. However, the partitioning can lead to discontinuous predictions and the
hyperparameter estimates can be poor if there is signiﬁcant correlation across partition boundaries. Other methods
make low-rank (Banerjee et al. (2008)) or sparsifying assumptions (Kaufman et al. (2008)) that allow for faster ap-
proximate estimation. A comprehensive review of computationally inexpensive GP-like methods are in Heaton et al.
(2019).

Models such as Gramacy & Apley (2015), utilize only local training observations to ﬁt independent GP models for
each prediction location using maximum likelihood estimation. This method is extremely computationally eﬃcient
and parallelizable, but the estimates can be inaccurate with only a few correlated local observations to estimate a
set of hyperparameters. We improve on this type of local estimation method in two primary ways. First, we avoid
maximum likelihood in favor of cross validation because the kriging weights (K∗f (Kﬀ + τ 2In)−1) are sparser than the
correlation function (9). Therefore, the predictions using only local data is an improved approximation over a local
likelihood approximation. Second, because we assume an overall stationary GP model, we borrow information across
spatial locations to improve robustness of the hyperparameter estimates. Further, in a stationary GP, a new model

8

is not necessary for each prediction location so a large number of prediction can be made eﬃciently. This method
is a novel extension of the approximate leave-one-out cross-validation approach for GP regression in Muyskens et al.
(2021) extended to the more complex GP classiﬁcation estimation problem.

Cross-validation seeks parameter values to maximize out-of-sample classiﬁcation accuracy. To formally describe
the procedure, let θ denote the hyperparameters that require estimation and kθ(·, ·) a GP kernel of interest. In the
Mat´ern kernel, deﬁne θ = (σ2, ν, (cid:96), τ 2)T . Here we omit estimation of σ2 because predictions of fi(θ) do not depend on
σ2. Therefore, a diﬀerent estimation method to deﬁne the uncertainty quantiﬁcation of the classiﬁcation prediction
will be used outlined in the next section. Let ˆzi be the ith leave-one-out class label prediction of zi given the set
of all training points excluding xi. ˆzi, is obtained in two steps. First, let x−i = {x1, x2, . . . , xi−1, xi+1, . . . , xn} and
z−i = {z1, z2, . . . , zi−1, zi+1, . . . , zn} be the observations and labels excluding the ith training point. Then we modify
Equation 8 to obtain the mean GP prediction by regressing the training labels on the corresponding inputs,

ˆfθ(xi) = ¯fi = Ki,−iK −1

−i.−iz−i.

(10)

Here Ki,−i is the cross-covariance between xi and x−i, while K−i,−i is the covariance among points in x−i, both
in terms of kθ(·, ·) as in Equation (6). We then use Equation 4 to round ˆfθ(xi) to the nearest class label ˆzi. Note
that the GP classiﬁer can be interpreted as a special case of a technique called kernel classiﬁcation, but the direct
correspondence is not given here to avoid unnecessary equations.

Although many criterion for classiﬁcation accuracy are possible, we select the cross-entropy loss, also referred to as
. δ takes a point

the log loss. Deﬁne δ : R → [0, 1]2 as a “padded” softmax function, where δ(a) =
ea+e−a ,
prediction in R and converts it into a probability distribution. Then the cross entropy loss is

e−a
ea+e−a

(cid:16) ea

(cid:17)

Q(θ) = −

n
(cid:88)

i=1

(cid:26) zi + 1
2

log [δ (fθ(xi))0] +

1 −

(cid:18)

(cid:19)

zi + 1
2

(cid:27)

log [δ (fθ(xi))1]

.

The hyperparameters are estimated as the maximizer of the leave-one-out cross-validation accuracy

ˆθ = arg max

−Q(θ).

θ

(11)

(12)

The loss function (11) is maximized using the ”optimize” function from the SciPy Python package, but other choices
are possible.

When there are a large number of observations, the cross-validation optimization is more computationally expensive
than traditional maximum likelihood estimation; the procedures have complexity O(n4) and O(n3), respectively.
Hence, local kriging is used in place of full kriging, so that only the q nearest neighbors to the prediction location are
considered, where q (cid:28) n. The complexity of the local kriging approach is O(nq3), which is linear in the sample size n
and tractable compared to the cubic scaling of likelihood-based optimization approaches.

Let xNi be the set of training observations nearest to xi, and let zNi be their corresponding labels. Similar to (10),

the ith nearest neighbors GP prediction is

f N N
θ

(xi) = Ki,Ni K −1

Ni,Ni

zNi,

(13)

where Ki,Ni is the cross-covariance between xi and xNi, while KNi,Ni is the covariance among points in xNi, similar
to Equations (6) and (10). Hyperparameter optimization proceeds following Equation (12), substituting f N N
(xi) for
fθ(xi).

θ

Batching, a common technique utilized in machine learning, allows us to obtain further computational eﬃciency. In
traditional batching, random locations are selected individually or in groups from the training data. The loss function
in (11) is then replaced by a summation over a subset of the training data whose indices are collected in a set denoted
B. That is, the loss function obtained with batching is

QB(θ) = −

(cid:26) zi + 1
2

(cid:88)

i∈B

log (cid:2)δ (cid:0)f N N

θ

(xi)(cid:1)

(cid:3) +

0

(cid:18)

1 −

(cid:19)

zi + 1
2

log (cid:2)δ (cid:0)f N N

θ

(xi)(cid:1)

(cid:27)
(cid:3)

.

1

(14)

where the hyper-parameter estimate is obtained by maximizing this function as in (12). This approximation introduces
some variability into the optimization problem because the batch indices are randomly selected, but the computational

Star-Galaxy Separation with Gaussian Processes

9

savings can be signiﬁcant if |B| (cid:28) n. If n is small, then there is little value to be gained with batching and all of the
data can be used for hyperparameter estimation.

We conclude our discussion on hyperparameter estimation with a comment on how batching is performed here within
the nearest neighbors framework. If batched observations are randomly selected from the data and only this data is
used in training, the nearest neighbors of each batched observation will be artiﬁcially distant from one another. This
structure does not well-represent the densely-sampled dataset, and therefore the hyperparameter selection would be
biased in ﬁtting low-frequency correlations, but be unable to model the more important high-frequency correlations.
To avoid this problem, all points in the original training set are considered as possible nearest neighbors to the points
included in the batch. Hence, more data is ultimately used to estimate the hyperparameters than is included directly
in the batch. Furthermore, points are excluded from being included in a batch if all their neighbors are the same class
since that point will have the same contribution to the loss function regardless of the hyperparameters or kernel choice.
Given a trained GP classiﬁer, class label predictions can be made for a new image. However, in addition to a class
label prediction, the GP also outputs a Gaussian distribution that contains information about the conﬁdence of the
prediction. We continue with a discussion of how to use this information to assess whether an image is a star or galaxy
with conﬁdence, or whether the image class is ambiguous.

3.4. Ambiguous Classiﬁcation

Although uncertainty quantiﬁcation for predictions is deﬁned in Gaussian processes through conditional distributions,
it is unclear how to utilize this latent information for classiﬁcation uncertainty quantiﬁcation. We describe a method
that utilizes the latent uncertainties through the context of statistical hypothesis testing. Using this method, we include
a third classiﬁcation label we refer to as ”ambiguous” where the latent Gaussian process is non-signiﬁcantly diﬀerent
than the class cutoﬀ value, which is 0 in our case. By removing these images from the star and galaxy designations, we
are able to improve performance of the of the classiﬁer, and identify the images that may need manual review, further
imaging, or other veriﬁcation methods or increase our reliability in the object’s classiﬁcation.

For demonstration, assume that if the ith training observation is a galaxy, zi = −1 and if it is a star zi = +1. Then

if on the test set, the latent predictions are less than 0, we predict it is a galaxy, and if it is greater than 0, a star.

Consider the hypotheses

H0 : fi = 0
H1 : fi (cid:54)= 0

(15)

Ideally, if fi < 0, this would correctly identify the ith observation as a galaxy, and we would hope to reject H0 and
accept the alternative. This would mean that we are conﬁdent in the correct identiﬁcation of the observation as a
galaxy. Inversely, if fi > 0, we would hope to fail to reject H0 since this is the incorrect conclusion. In this testing
formulation, a Type I error (α) is when 0 is not contained in the interval, but the classiﬁcation is incorrect. Similarly,
a Type II error (1 − β) is when 0 is contained in the interval, but the prediction would be in the correct classiﬁcation.
We can formally test this hypothesis by creating latent prediction intervals given the Gaussian process standard error.
For each testing location, we compute the following prediction interval using local observations

fθ(xi) ± 1.96 ∗ (cid:98)σ2Cii,
where Cii is deﬁned as the element in the ith row and ith column of C deﬁned in Equation (7) with optimized
hyperparameter values. If this interval contains 0, we classify it as ambiguous, but otherwise use the classiﬁcation
given by the sign of the prediction as previously described.

(16)

However, the computation of these intervals requires an estimate of the prediction variance σ2 we were unable to
estimate via cross-validation. After the other hyperparameters in θ have been estimated as in the previous section, we
compute the above intervals for leave-one-out predictions using local predictions. Then, we minimize a ﬁxed function
of the Type I (α) and Type II (1 − β) errors in these tests to estimate σ2.

Generally as the number of Type I errors increase, the number of Type II errors decrease and vice versa. Note that
because there are many more correct classiﬁcations in our method, estimation of the Type II error is more accurate
than that of the Type I error, but a Type I error is considered more severe. Deﬁning a category of ”ambiguous” images
is ultimately a balance of these errors, Further, the more ambiguously classiﬁed images, the higher the accuracy of
the remaining classiﬁcation. However, this requires manual review of more ambiguous images. Therefore, based on

10

available resources, diﬀerent choices may be made as to how many images could reasonably be classiﬁed as such. We
present several example functions that yield a range of possible values.

1. α + 1 − β.

2. 2α + 1 − β.

3. 4α + 1 − β.

4. 10α + 1 − β.

5. ni ∗ α + (1 − β) ∗ nc,

where nc is the number of correctly identiﬁed images, and ni is the number of incorrectly classiﬁed images. Other
choices are possible, but we found minimizing these functions gave a variety of estimates of σ2.

4. RESULTS AND DISCUSSION

In this section, we perform a serious of numerical studies to evaluate the accuracy of our method in the classiﬁcation
of stars and galaxies. Throughout this section, we randomly sample training and testing images from our available
dataset so that there are equal numbers of star and galaxy images. In total, the dataset has 31,798 labeled objects,
where 13,133 objects are stars, and 18,665 objects are galaxies.
In all studies, we allow the testing dataset to be
1,000 images, equally sampled from the available stars and galaxies. The primary metric for success we report is the
accuracy of the metric:

1
1000

1000
(cid:88)

i=1

1{ ˆzi = zi},

(17)

where 1 is an indicator function that is 1 when the classiﬁcation of the model matches the true classiﬁcation and 0
otherwise. This out-of-sample testing data accuracy procedure is then repeated for 100 simulation iterations where
the training and testing samples are randomly re-drawn, normalized, embedded, and the classiﬁers are retrained
independently in each iteration.

Figure 4. The i-band images produce the best prediction. The g-band channel produces the worst classiﬁcations. In all other
cases, including the PSF images in addition to the primary star or galaxy image improves classiﬁcation accuracy.

Star-Galaxy Separation with Gaussian Processes

11

First, we formally study the inﬂuence utilizing data from various bands and point spread images in the classiﬁcation
of stars and galaxies. For training set sizes {500, 2000, 4000, 6000, 8000, 10000}, we compute the accuracy of a classiﬁer
where the PCA reduction is performed on various subsets of the total available images of each object. The results of
this simulation are in Figure 4. These results demonstrate that g and only classiﬁcation is signiﬁcantly worse than
all other data conﬁgurations. In all other bands, including the point spread function images improves the accuracy
of the classiﬁers. Overall, using only the i band produces the most accurate predictions. This is not surpising as
HSC prioritized the i band when data collection was good. Although the magnitude of this improvement is small, we
conclude that data from this band is suﬃcient. Therefore, in all other numerical studies, we consider data only from
this band.

Figure 5. Left: Star-galaxy classiﬁcation accuracy versus training sample size. Several classiﬁcation algorithms are shown by
the diﬀerent colors and line types. See the text for the description of each algorithm. Right: Computational runtimes for the
combined operations of training and testing the classiﬁers as measured on a personal computer on a single core. The Mat´ern
kernel is more accurate than CNNs for small sample sizes. Both methods linearly scale, but the slope of the CNN computation
is lower than that of the Mat´ern.

Next, we compare our method to other machine learning methods as well as the extendedness morphological classiﬁer
from the HSC pipeline Bosch et al. (2018). We test the accuracy of these classiﬁers for a wide range of training sample
sizes. In comparison to our GP method, we ﬁrst consider the morphological classiﬁer. It classiﬁes stars and galaxies
based on the magnitude diﬀerence between a PSF and galaxy model. For a star the diﬀerence should be zero, while
a galaxy will have a signiﬁcant deviation from zero. A hard cut of magP SF − maggalaxy = 0.0164 is used to set the
boundary between the two cases. While the choice of cut was not optimized for any particular science use case, it
gives a baseline to compare against.

Next we consider a convolution neural network classiﬁcation model. This is the generally accepted state-of-the-art
machine learning model for image classiﬁcation tasks. We implement this method via the “Keras” Python package
with the architecture give in the Appendix. We utilize a similar cross-entropy loss function as our Gaussian process
method, with 30 epochs. To provide other comparisons, we consider both a logistic regression (LR) and dense neural
network (DNN) applied to the same PCA reduced data ﬁt by our GP method.

The results of this study are in Figure 5 plotted on the log scale of sample size. The morphological classiﬁer is
constant in training size because it does not rely on training data. In low training sample sizes, all methods other
than the GP classiﬁer perform worse than the morphological classiﬁer. However, in training sample sizes, all machine
learning methods perform signiﬁcantly better than the morphological classiﬁer (approximately 0.08 improvement), and
the diﬀerences between the methods is small. In terms of training computing time, the CNN is by far much more
expensive than the other methods. Although the training time of our GP classiﬁer is more than that of the other faster
methods, the scaling is much improved over the traditional O(n3) scaling. In order to exlore the minimum sample size

12

needed to outperform the morphological classiﬁer, we explore the accuracy of our method with extremely small sample
sizes in Figure 6. The mean accuracy of the GP classifer exceeds the morphological classiﬁer accuracy of 0.9 at around
80 training observations. This means that only approximately 0.3% of the total 31,798 object observations need to be
labeled in order to demonstrate classiﬁcation improvement on the morhpological classiﬁer with our GP method.

Figure 6. Extremely small sample GP classiﬁer accuracy results and 90% empirical conﬁdence intervals over 100 simulation
iterations. The smallest sample size is n = 4, where only 2 star and 2 galaxy observations are provided. The mean accuracy of
the GP classifer exceeds the morphological classiﬁer accuracy of 0.9 at around 80 training observations.

We further compare our GP classiﬁer and the morphological classiﬁer in Figure 7. For each noted signal-to-noise
ratio, we excluded images that were below that thresholded value and estimated the accuracy using only 1,000 training
star-galaxy images randomly sampled from the images of suﬃcient signal. Although the morphological classiﬁer does
well at identifying galaxies, in low signal-to-noise images, it does a poor job identifying stars. Our GP method performs
well at identifying for both star and galaxy images even in low signal-to-noise images.

Finally, we summarize results of our uncertainty quantiﬁcation in Table 4. In all cases, we demonstrate improved
star-galaxy classiﬁed accuracy by determining between approximately 4 and 21 images as ambiguous out of a total
of 1,000 testing images. Although the accuracy of the ambiguously classiﬁed testing images was better than 0.5, its
average performance is signiﬁcantly below that of the star-galaxy classiﬁed images. In future piplines that automate
the star-galaxy designation for large amounts of data, employing this method would allow for improvement in accurate
classiﬁcation by indicating the ”ambiguous” images that should be veriﬁed by a human operator. This represents a
signiﬁcant improvement over using the same human operator resources to verify the classiﬁcation of random images
in the prediction set.

As a continuation of our uncertainty analysis, we demonstrate traditional classiﬁcation of the receiver operating
characteristic (ROC) curve in Figure 8. In these plots, the true positive rate and false positive rates are compared
for a variety of cutoﬀ values to compare to the ¯f ∗ predictions. In this case, deﬁne the true positive rate to be the
proportion of galaxies that are correctly classiﬁed as such, and the false positive rate as the 1- the proportion of stars
that are correctly classiﬁed. A perfect classiﬁer would have a true positive rate of 1 and a false positive rate of 0,
which would yield an ”area under the curve” or AUC of 1. Averaged across 100 simulation iterations for 5,000 training
samples, the AUC is estimated to be approximately 0.985 in Figure 8.

We have shown that applying Gaussian processes, in particular with a Mat´ern kernel, to reduced-order images of
stars and galaxies allows classifying images with great accuracy and eﬃciency. Our classiﬁcation method outperforms
standard methods like morphological classiﬁers as well as other machine learning methods such as CNNs, while
providing a higher degree of interpretability of the predictions and requiring fewer training samples. We have ﬁrst

5. CONCLUSION

Star-Galaxy Separation with Gaussian Processes

13

Figure 7. Comparison of Gaussian process (GP) and the morphological classiﬁer (MC) over signal to noise ratios broken out
by star and galaxy accuracy. ”Mean” represents the accuracy reported in other analyses, which is the mean performance across
stars and galaxies.

(cid:99)σ2 Optimizer

α + 1 − β

2α + 1 − β

4α + 1 − β

10α + 1 − β

ni ∗ α + (1 − β) ∗ nc

# Ambiguous
Ambiguous Accuracy
Classiﬁed Accuracy
# Ambiguous
Ambiguous Accuracy
Classiﬁed Accuracy
# Ambiguous
Ambiguous Accuracy
Classiﬁed Accuracy
# Ambiguous
Ambiguous Accuracy
Classiﬁed Accuracy
# Ambiguous
Ambiguous Accuracy
Classiﬁed Accuracy
All testing Observations Accuracy

n=1,000 n=5,000 n=10,000 n=15,000 n=20,000
5.5
0.790
0.970
15.0
0.788
0.972
15.1
0.789
0.972
15.1
0.789
0.972
6.7
0.798
0.970
0.969

5.3
0.792
0.967
13.0
0.790
0.968
13.0
0.791
0.968
13.0
0.791
0.968
6.1
0.780
0.967
0.966

4.3
0.641
0.954
19.1
0.748
0.956
21.0
0.754
0.956
21.0
0.753
0.956
5.8
0.628
0.954
0.952

4.5
0.742
0.968
10.9
0.763
0.969
11.1
0.761
0.969
11.1
0.761
0.969
5.5
0.739
0.968
0.967

4.2
0.681
0.965
10.8
0.753
0.966
11.0
0.754
0.966
11.0
0.754
0.966
6.8
0.711
0.966
0.964

Table 1. Results from ambiguous classiﬁcation via Gaussian Process uncertainties for 1,000 testing observations.

demonstrated that by normalizing and PCA embedding the data, we can apply GP weights to interpolate masked
classiﬁcation predictions on images of new objects. These results could inform future astronomical image processing
strategies and software pipelines for large sky surveys in the future.

14

Figure 8. ROC curve of our GP classiﬁer with 5,000 training observations averaged over 100 simulation iterations. Red point
represents our classiﬁer with 0 threshold, and area under the curve is 0.985.

We developed a novel method of GP classiﬁcation hyperparameter estimation that allows for nearly linear order
of estimation (as opposed to traditional O(n3)). This allows our methods to be eﬃciently trained on more images
than traditional maximum likelihood methods, allowing our methods to use the labeled resources of the large Rubin
Observatory data to apply to new large-scale studies. Since our methods parallelize computation into small, local
computations, there is opportunity in future work to scale up these methods on HPC systems.

In our studies, we consider balanced training and testing data, where there are equal numbers of stars and galaxies
in our samples.
In this case, our assumption of a mean-zero GP is reasonable, since we have equal z = [−1, 1]
samples. However, to train on all labeled star-galaxy images for future surveys, these assumptions must be relaxed.
Further research must be done in order to explore behavior of this classiﬁer in the unbalanced training data case, and
whether a change in mean function could compensate for this eﬀect. Further, our models consider only the isotropic,
stationary Mat´ern kernel. Future work could consider anisotropic eﬀects where distances in the PCA components
are treated diﬀerently. Future work could also be pursued in generalizing our scalable estimation method to non-
stationary through a non-stationary mean or kernel function. Finally, although we demonstrate the eﬀectiveness and
eﬃciency of our classiﬁcation method on star-galaxy separation, this method could be more generally applied to other
classiﬁcation problems in astronomy and other ﬁelds. For example, one could consider applying these methods to other
space situational awareness applications such as non-resolved object characterization.

ACKNOWLEDGMENTS

This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National
Laboratory under Contract DE-AC52-07NA27344 with IM release number LLNL-JRNL-821985. Funding for this work
was provided by LLNL Laboratory Directed Research and Development grant 19-SI-004.

This document was prepared as an account of work sponsored by an agency of the United States government. Neither
the United States government nor Lawrence Livermore National Security, LLC, nor any of their employees makes any
warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or
usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe
privately owned rights. Reference herein to any speciﬁc commercial product, process, or service by trade name,
trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or

Star-Galaxy Separation with Gaussian Processes

15

favoring by the United States government or Lawrence Livermore National Security, LLC. The views and opinions
of authors expressed herein do not necessarily state or reﬂect those of the United States government or Lawrence
Livermore National Security, LLC, and shall not be used for advertising or product endorsement purposes.

REFERENCES

Abbott, T., Abdalla, F., Avila, S., et al. 2019, Physical

Hu, W., & Scranton, R. 2004, Physical Review D, 70,

Review D, 99, 123505

123002

Abbott, T. M. C., Abdalla, F. B., Allam, S., et al. 2018,
The Astrophysical Journal Supplement Series, 239, 18,
doi: 10.3847/1538-4365/aae9f0

Huang, S., Leauthaud, A., Murata, R., et al. 2017,

Publications of the Astronomical Society of Japan, 70,

doi: 10.1093/pasj/psx126

Aihara, H., Armstrong, R., Bickerton, S., et al. 2017,

Ivezi´c, ˇZ., Kahn, S. M., Tyson, J. A., et al. 2019, The

Publications of the Astronomical Society of Japan, 70,
doi: 10.1093/pasj/psx081

Amiaux, J., Scaramella, R., Mellier, Y., et al. 2012, in

Space Telescopes and Instrumentation 2012: Optical,
Infrared, and Millimeter Wave, Vol. 8442, International
Society for Optics and Photonics, 84420Z

Banerjee, S., Gelfand, A. E., Finley, A. O., & Sang, H.

2008, Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 70, 825

Bosch, J., Armstrong, R., Bickerton, S., et al. 2018,

Publications of the Astronomical Society of Japan, 70, S5

Bradshaw, J., Matthews, A. G. d. G., & Ghahramani, Z.

2017, arXiv preprint arXiv:1707.02476

Desjacques, V., Jeong, D., & Schmidt, F. 2018, Physics

Reports, 733, 1,
doi: https://doi.org/10.1016/j.physrep.2017.12.002

Desjacques, V., & Seljak, U. 2010, Classical and Quantum

Gravity, 27, 124011

Drlica-Wagner, A., Bechtol, K., Rykoﬀ, E. S., et al. 2015,

ApJ, 813, 109, doi: 10.1088/0004-637X/813/2/109

Fadely, R., Hogg, D. W., & Willman, B. 2012, The

Astrophysical Journal, 760, 15

Flewelling, H. 2018, in American Astronomical Society

Meeting Abstracts# 231, Vol. 231, 436–01

Fuentes, M. 2001, Environmetrics: The oﬃcial journal of

the International Environmetrics Society, 12, 469

Goumiri, I. R., Muyskens, A. L., Schneider, M. D., Priest,

B. W., & Armstrong, R. E. 2020, Star-Galaxy Separation
via Gaussian Processes with Model Reduction.
https://arxiv.org/abs/2010.06094

Gramacy, R. B., & Apley, D. W. 2015, Journal of
Computational and Graphical Statistics, 24, 561

Astrophysical Journal, 873, 111

Kaufman, C. G., Schervish, M. J., & Nychka, D. W. 2008,

Journal of the American Statistical Association, 103,

1545

Kilbinger, M. 2015, Reports on Progress in Physics, 78,

086901

Kim, E. J., & Brunner, R. J. 2016, Monthly Notices of the

Royal Astronomical Society, stw2672

Kim, E. J., Brunner, R. J., & Carrasco Kind, M. 2015,

Monthly Notices of the Royal Astronomical Society, 453,

507

Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, in

Advances in neural information processing systems,

1097–1105

Leauthaud, A., Massey, R., Kneib, J.-P., et al. 2007, The

Astrophysical Journal Supplement Series, 172, 219,

doi: 10.1086/516598

Lehoucq, R. B., Sorensen, D. C., & Yang, C. 1998,

ARPACK users’ guide: solution of large-scale eigenvalue

problems with implicitly restarted Arnoldi methods

(SIAM)

Miyazaki, S., Komiyama, Y., Kawanomoto, S., et al. 2018,

Publications of the Astronomical Society of Japan, 70,

S1, doi: 10.1093/pasj/psx063

Muyskens, A., Priest, B., Goumiri, I., & Schneider, M.

2021, MuyGPs: Scalable Gaussian Process

Hyperparameter Estimation Using Local

Cross-Validation. https://arxiv.org/abs/2104.14581

Odewahn, S., Stockwell, E., Pennington, R., Humphreys,

R. M., & Zumach, W. 1992, in Digitised Optical Sky

Surveys (Springer), 215–224

Gramacy, R. B., et al. 2007, Journal of Statistical Software,

Pollo, A., Rybka, P., & Takeuchi, T. T. 2010, A&A, 514,

19, 6

A3, doi: 10.1051/0004-6361/200913428

Heaton, M. J., Datta, A., Finley, A. O., et al. 2019, Journal
of Agricultural, Biological and Environmental Statistics,
24, 398

Rasmussen, C. E., & Williams, C. K. I. 2005, Gaussian

Processes for Machine Learning (Adaptive Computation

and Machine Learning) (The MIT Press)

16

Renk, J., Zumalacarregui, M., & Montanari, F. 2016,

Stein, M. L. 2012, Interpolation of spatial data: some

Journal of Cosmology and Astroparticle Physics, 2016,

040

Sawangwit, U., Shanks, T., Abdalla, F., et al. 2011,

Monthly Notices of the Royal Astronomical Society, 416,

3033

theory for kriging (Springer Science & Business Media)

The Dark Energy Survey Collaboration. 2005, arXiv

e-prints, astro. https://arxiv.org/abs/astro-ph/0510346
Thomas, S. A., Abdalla, F. B., & Lahav, O. 2011, Physical

review letters, 106, 241301

Vasconcellos, E., De Carvalho, R., Gal, R., et al. 2011a,

Sevilla-Noarbe, I., & Etayo-Sotos, P. 2015, Astronomy and

The Astronomical Journal, 141, 189

Computing, 11, 64

Sevilla-Noarbe, I., Hoyle, B., March˜a, M., et al. 2018,

Monthly Notices of the Royal Astronomical Society, 481,

5451

Vasconcellos, E. C., de Carvalho, R. R., Gal, R. R., et al.

2011b, The Astronomical Journal, 141, 189,
doi: 10.1088/0004-6256/141/6/189

Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020,

Nature Methods, 17, 261, doi: 10.1038/s41592-019-0686-2

Slater, C. T., Ivezi´c, ˇZ., & Lupton, R. H. 2020, The

Yang, G., & Salman, H. 2019, arXiv preprint

Astronomical Journal, 159, 65,

doi: 10.3847/1538-3881/ab6166

arXiv:1907.10599

Yoo, J. 2014, Classical and Quantum Gravity, 31, 234001

Star-Galaxy Separation with Gaussian Processes

17

APPENDIX

.1. Convolutional Neural Network

Convolutional neural networks are a class of deep neural networks which are commonly used for image classiﬁcation,

where neurons in hidden layers are only connected to a localized subset of neurons from the previous layer.

The CNN analysis was implemented in R using the Keras package. The network architecture summary provided by
the package is shown below. The two dropout layers have rates of 20%. The convolutional and dense layers utilize
the ReLU activation functions, and the output layer employs the softmax activation function so that the output is a
probability value in (0, 1). Table 2 provides details on the CNN training.

Setting

Value

Learning Rate
Validation Split
Decay Factor
Optimizer
Loss Function
Batch Size
Epochs

1
0.2
0.95
Adam
Categorical cross-entropy
256
30

Table 2. Details on CNN training.

0

544

18496

Param #

(None, 2, 2, 2, 64)

(None, 6, 6, 2, 32)

(None, 6, 6, 2, 64)

(None, 26, 26, 2, 32)

Model: "sequential_3"
___________________________________________________________________________________________
Layer (type)
Output Shape
===========================================================================================
conv3d_5 (Conv3D)
___________________________________________________________________________________________
max_pooling3d_5 (MaxPooling3D)
___________________________________________________________________________________________
conv3d_4 (Conv3D)
___________________________________________________________________________________________
max_pooling3d_4 (MaxPooling3D)
___________________________________________________________________________________________
(None, 512)
flatten_13 (Flatten)
___________________________________________________________________________________________
dense_59 (Dense)
(None, 400)
___________________________________________________________________________________________
(None, 400)
dropout_29 (Dropout)
___________________________________________________________________________________________
dense_58 (Dense)
(None, 200)
___________________________________________________________________________________________
(None, 200)
dropout_28 (Dropout)
___________________________________________________________________________________________
dense_57 (Dense)
(None, 100)
___________________________________________________________________________________________
dense_56 (Dense)
===========================================================================================
Total params: 324,742
Trainable params: 324,742

(None, 2)

205200

20100

80200

202

0

0

0

0

18

Non-trainable params: 0
___________________________________________________________________________________________

