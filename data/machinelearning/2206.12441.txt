2
2
0
2

n
u
J

4
2

]

G
L
.
s
c
[

1
v
1
4
4
2
1
.
6
0
2
2
:
v
i
X
r
a

Joint Representation Training in Sequential Tasks
with Shared Structure

Aldo Pacchiano
Microsoft Research, NYC
apacchiano@microsoft.com

Oﬁr Nachum
Google Research
ofirnachum@google.com

Nilesh Tripuraneni
UC Berkeley
nilesh_tripuraneni@berkeley.edu

Peter Bartlett
UC Berkeley
peter@berkeley.edu

Abstract

Classical theory in reinforcement learning (RL) predominantly focuses on the sin-
gle task setting, where an agent learns to solve a task through trial-and-error ex-
perience, given access to data only from that task. However, many recent em-
pirical works have demonstrated the signiﬁcant practical beneﬁts of leveraging a
joint representation trained across multiple, related tasks. In this work we the-
oretically analyze such a setting, formalizing the concept of task relatedness as
a shared state-action representation that admits linear dynamics in all the tasks.
We introduce the Shared-MatrixRL algorithm for the setting of Multitask Ma-
trixRL [32]. In the presence of P episodic tasks of dimension d sharing a joint
d low-dimensional representation, we show the regret on the the P tasks can
r
be improved from O
over N
episodes of horizon H. These gains coincide with those observed in other linear
models in contextual bandits and RL [31, 17]. In contrast with previous work that
have studied multi task RL in other function approximation models, we show that
in the presence of bilinear optimization oracle and ﬁnite state action spaces there
exists a computationally efﬁcient algorithm for multitask MatrixRL via a reduc-
tion to quadratic programming. We also develop a simple technique to shave off a
?H factor from the regret upper bounds of some episodic linear problems.

P Hd?N H
p

?N H
q

HP ?rd

Hd?rP

to O

`

!

pp

q

q

1 Introduction

Reinforcement learning (RL) is about learning via doing – learning to solve a sequential decision-
making task where the only information about the task is obtained via trial-and-error. Accordingly,
the underlying assumptions made in RL are typically minimal. Beyond what can be learned from
trial-and-error experience, the learner’s structural prior on the underlying task is commonly restricted
to a small set of Markov assumptions [26]: namely, that the task is of a sequential nature, with the
task reward and state transition dynamics at each step determined by an (unknown) Markov process.

The simplicity of this setting, which forms the basis of a rich and diverse literature [9, 27], stands in
contrast to the complexity of many real-world settings, where one has access to data from multiple,
related tasks. In these situations, experience from one task can often be leveraged to accelerate
learning in another. For example, when humans are confronted with learning a new video game,
we naturally draw on previous experience and knowledge from playing other games, even if the
dynamics and rewards across the games are not the same.

In line with this intuition, there exist a number of empirical works which demonstrate how expe-
rience can be gathered from multiple tasks to accelerate RL over learning these tasks in isolation.

 
 
 
 
 
 
For example in robotics [33, 19], such approaches are key to avoiding an expensive blow-up in the
sample complexity of required, real-world interactions. A popular paradigm to jointly use experi-
ence from multiple tasks is by way of learning a shared low-dimensional representation. Namely,
the observations of each task are individually embedded into a common low-dimensional space, and
learning occurs jointly in this space. [28, 12].

Despite these empirical successes, theoretical explorations to understand the beneﬁts of such joint
training in RL have been limited. While in the supervised learning literature, the beneﬁt of multi-
task training is well-studied [e.g., 6, 7, 8, 13, 29, 30], obtaining a similar understanding in the setting
of RL is more challenging. For one, a sufﬁciently ﬂexible yet useful notion of “task relatedness” is
difﬁcult to formulate in RL, which involves both rewards and transition dynamics. Secondly, an
algorithm using such a relatedness measure must carefully balance exploration and exploitation,
while appropriately handling inevitable inaccuracies in the learned representation and how these can
compound over the horizon.

Given these existing shortcomings in the literature, in this work we aim to theoretically analyze the
beneﬁt of learning joint representations for multi-task RL. We begin by formalizing the underlying
similarity – i.e., task relatedness – between multiple tasks. Leveraging recent results on linearly
factored or low-rank MDPs [3, 32, 24], we assume that there exists a state-action representation such
that all tasks admit linear transition dynamics with respect to this representation. In this setting, any
one task may exhibit distinctly different dynamics from the remaining tasks while still maintaining
a common and learnable structure. Under such a shared representation, we quantify the beneﬁt – in
terms of regret – given by using a sufﬁciently accurate approximate representation, and we pair this
result with an online algorithm for simultaneously learning and using such a representation. Our
results provide a clear understanding of the trade-offs associated with leveraging a jointly learned
representation in the setting of RL as a function of the dimension of the shared representation, the
number of tasks, and the dimension of the raw state observations (see our main results in Section 4).
We show that in the presence of bilinear optimization oracle there exists a computationally efﬁcient
algorithm for multitask MatrixRL via a reduction to quadratic programming (see Section 5). This
is in contrast with previous work that have studied multi task RL in other function approximation
models such as [17]. We develop a general regret analysis technique to shave off a ?H factor from
the the regret upper bounds of episodic linear problems and apply it both to the original MatrixRL
rates (see Section 3) as well as Shared-MatrixRL (see Section 4).

2 Related Work

As mentioned above, multi-task learning is well-studied in the supervised learning literature. The
predominant mechanism for performing multi-task learning in these previous works is analogous
to our own, namely, parameterizing a classiﬁer as a composition of two functions, one of which is
common to all tasks and another which is unique to each class [6, 21, 13, 29, 30]. As in our own
work, these previous works generally rely on an assumption that an optimal hypothesis with the
desired compositional form exists, although some work has explored alternative assumptions [8].

More closely related to our own setting is the work of D’Eramo et al. [12], which theoretically
analyze approximate dynamic programming in the context of multi-task learning. In this setting, an
approximate value function is learned, with a common representation used to parameterize this value
function. However, it is important to note that approximate dynamic programming is distinct from
RL, as it ignores the difﬁcult exploration problem associated with learning from one’s own collected
data. In contrast, our analysis is speciﬁcally tailored to an online learning scenario: where one of
the main challenges is deriving multi-task learning bounds which carefully balance exploration and
exploitation jointly across all tasks.

Works that do consider the online learning setting include Yang et al. [31] and Hu et al. [17]. The
ﬁrst of these considers a linear bandit setting and is not immediately applicable to RL; moreover,
this work imposes additional structural conditions on the linear features of the bandit problem which
effectively require the action features to sufﬁciently cover all possible directions. The second work
by Hu et al. [17] is closer to ours and considers the linear RL setting. Our ‘sharedness’ assumptions
and results are a generalization of those studied in Hu et al. [17]. Because Hu et al. [17] studies a
value-based approach, the assumption is that all the underlying linear parameter weights of the task’s
Q-functions lie in (or near) the same subspace. Because we are studying a model based approach

2

we move beyond this sharedness structure to instead study the setting where the task model matrices
share a common factorization. This is the reason our bounds have a dependence on d1 as well as
on d and r. Our regret guarantees are similar to those in Hu et al. [17], despite taking a different
approach with distinct derivations. We don’t see this as a limitation but rather as an indication to the
wider research community that there is a potential opportunity to develop a unifying analysis of RL
methods in the presence of shared representation learning that could subsume both value-based and
model-based methods. We believe this to be an interesting and exciting avenue for future research.
Moreover, we show (see Section 5) that as long as we have access to an oracle for joint least squares
matrix factorization, the optimization problem required to ﬁnd the policy to execute at time t can be
solved efﬁciently. This is in stark contrast with other approaches where even solving the necessary
joint optimization problem over the task family to ﬁnd the policy to execute at any given time can
be an intractable problem. Recently, other works [11, 4, 20] have explored more general versions of
our shared task assumptions where the learner may have access only to a family of representation
functions and is tasked with learning a viable representation while interacting with multiple tasks at
once. They show that shared representation learning is advantageous when compared to learning a
single representation per task. These are very close in spirit to the study we present here and should
be thought of as a successor works to ours.

In this work we consider a model for task relatedness inspired by [30], where we assume the under-
lying model of the MDP dynamics have a shared low rank representation. Other models of the rela-
tionship between related tasks are possible. Most notably Müller and Pacchiano [23] and Moskovitz
et al. [22]: in Müller and Pacchiano [23] the authors consider the question of learning an appropri-
ate ‘bias’ vector for regularizing the MatrixRL algorithm. This allows them to show that in case
the variance of the models in the family is small, performance (in this case measured in the form
of regret) in a test task can be substantially better. The authors of [22] tackle a similar issue. In
their work they show that under the assumption that the optimal policies are similar across tasks in
the family, it is possible to learn a useful default policy such that a policy gradient algorithm that
regularizes towards it can learn an optimal policy for a target task much more efﬁciently than an
algorithm regularizing towards the uniform policy. We leave the task of generalizing our work to the
setting of a set or distribution train tasks for the purpose of solving a test task for future work.

3 Preliminaries

Formally, we consider the setting of episodic reinforcement learning proposed in [32] where an
an agent explores an MDP
with state space S, action space A and known reward
0, 1
function r : S

S, A, P, r, H
p
whose transition dynamics are given by the feature embedding,
s

Ñ r

A

ˆ

q

s, a
φ
p
The learner receives a noiseless reward r
which for simplicity we assume is known. All in-
teractions between a policy and the MDP is of length H. For any policy π, state s, action a and
we deﬁne V π
as the value and Q functions of policy π. Our objective is to
s
h
h p
design algorithms with small regret, deﬁned as

, Qπ
hp
q

q “
s, a
p

˜s
s, a
|
p

JM
q

˜s
q
p

s, a

P r

H

ψ

q

q

s

‹

P

N

R

N H
p

q “

V π‹
1

sn,1
p

q ´

V πn
1

sn,1
p

,
q

1
n
ÿ
“

Where π
‹
are the initial states during the nth episode.

corresponds to the optimal policy, πn is the algorithm’s policy during time-step n and sn,1

The algorithm in [32] works by building an estimator
collected so far. We use the notation t
st, at, ˜st
the state-action-state triplets
p
features by:

n, h
“ p
q
where ˜st
q

“

st
Ă
`

(i.e. episode n

Mn of the matrix M

at time n using the data
H), to denote
1. For simplicity we denote the associated

‹
N and stage h

ď

ď

φt

st, at
φ
p

q P

“

Rd, ψt

ψ

˜st
p

q P

“

Rd1

and M

Rd

d1

.

ˆ

‹ P

d1

S

P

|ˆ

as the matrix whose rows equal ψ

R|
Denote Ψ
For any matrix B we use B
the Frobenius norm of a matrix and
} ¨ }8
following assumptions regarding the norms of M

to refer to B’s i
2,

s
p
´
the l2 and l

} ¨ }

:, i

q

s

r

for all s

“
P
th column. We use the notation

S and let Kψ

˜s
˜s
ψ
˜s ψ
J.
q
q
p
p
F to denote
norms of a vector. We will make the

} ¨ }
ř

and the feature maps φ and ψ.

8

‹

3

Assumption 3.1 (Boundedness). The feature maps φ and ψ satisfy
M
and
‹r
}
therefore
}

S for all s, a
?d1S

ď
F
ď

A and i

:, i
M

s}
‹}

P r

d1

ˆ

S

P

s

2

Lψ
2
and some known values Lφ, Lψ and S. And

s, a
φ
p

s
p

Lφ,

q}

q}

ď

ď

ψ

}

}

2

We also consider the following two assumptions on feature regularity, both present in [32].

Assumption 3.2. [Feature Regularity] For all v

C1ψ, where
columns).

Y

2,

}

}

8 “

maxi

bř

P
i,j is the 2,

j Y2

8

S

R|

|,

ΨJv

Cψ

v

, and

ΨK´

}

8 ď
}8
norm (inﬁnity norm over the l2 norm of Y’s

}8 ď

}

}

1
ψ }

2,

We will also prove sharper results under a more reﬁned feature regularity assumption,

Assumption 3.3. [Stronger Feature Regularity] For all v
ΨK´
8 ď
}
l2 norm of Y’s columns).

C1ψ, where

1
ψ }

j Y2

maxi

8 “

Y

2,

2,

}

}

R|
i,j is the 2,

P

8

S

bř
As it is explained in [32], this assumption can be satisﬁed when Ψ is a set of sparse features or if Ψ
is a set of highly concentrated features.

2

ΨJv

|,
norm (inﬁnity norm over the

, and

Cψ

}8

ď

v

}

}

}

The matrix estimator

Mn considered by [32] equals:

Ă

Mn

Σn

“ r

1

´

s

φn1,hψJn1,hK´

1
ψ .

n1

n,h
ÿ

ă

ď

H

Where

Ă

Σn

λI

`

“

n1

n,h
ÿ

ă

ď

H

φn1,hφJn1,h

and Σn,h
problem:

“

Σn

`

h1

ă

h φn,h1φJn,h1 . It is easy to see that

Mn is the solution to the ridge regression

ř
Mn

arg min
M

“

Ă

ψJn1,hK´

1
ψ ´

Ă
φJn1,hM

2
2

}

M

λ
}

}

`

2
F .

(1)

H }

n1

n,h
ÿ

ă

ď

It can be shown that with high probability and for all t simultaneously all
M

.

Mn lie in a vicinity of

‹

Lemma 3.4. For all δ

0, 1

with probability at least 1

δ for all n

N simultaneously,

Ă

P p
M

q
Rd

:

ˆ

d1

M

‹ P t

2
1
{
q
2
1
{
“
q
2,1 denotes the l1 norm of the l2 norm of the columns of B while

M
p
M
p

Mn
Ă

d1βn
a

‹ P t

Mn

Σn

Σn

u
:

Rd

M

M

a

2,1

}p

}p

q}

q}

ď

“

´

´

ď

d1

P

P

d1

u

ˆ

F

:

:

U1,2
n .
UF
n .

´

P
βn

Where

B

}

}

the Frobenius norm, ?βn

Ă

“

R

d log

d1

`

c

´

d1nHL2
δ

φ{

λ

?λS and R

“ }

`

¯

B

F corresponds to

}
1
K´
ψ }

}
Lψ

SLφ.

`

The proof of Lemma 3.4 can be found in Appendix A. We can make use of Lemma 3.4 to show a
regret guarantee for the MatrixRL algorithm from [32] (see Algorithm 1). Let’s revisit the optimistic
value function construction of the MatrixRL algorithm,

s, a

S

@p

Qn,h

q P

ˆ

s, a
p

q “

A : Qn,H
s, a
r
p

q `

1

s, a
p
`
max
1,2
U
n

M

q “
s, a
φ
p

0 and

h

H

@

P r

JMΨJVn,h
q

`

s

:

1

(2)

P

where

Vn,h

s
p

q “

Π
0,H
r

s

max
a

Qn,h

s, a
p

s, a, n, h.

@

q
ı

”

Π
0,H
r

s

denotes the coordinate-wise clipping/projection operator onto the

0, H

interval.

s
δ where Lemma 3.4 holds as E. We’ll be

r

Let’s deﬁne the “good" event of probability at least 1
´
making heavy use of the following ‘determinant lemma’,

4

S, A, P, s0, r, H
.
q

“ p
0, 1
d1
.

, features φ : S
q

ˆ

A

Ñ

Rd

be given by Equation 2 using U1,2

n and βn as in Lemma 3.4.

Algorithm 1 MatrixRL.

1: Input: An episodic MDP environment M

, probability parameter δ
d, M1

0

Rd

P p
ˆ

Ð

P

Rd
ˆ
, N :

I
Ð
1,
“
Mn.

P
¨ ¨ ¨

Rd1

Ñ

and ψ : S
2: Initialize: Σ1
3: for episode n
Solve for
4:
Qn,h
Let
5:
u
For stage h
6:
Ă
7:
8:
9:
10: Σn
Ð
11: Compute

Σn
Mn

`

t

`

1

Ă

“

¨ ¨ ¨

, H:

1,
Let the current state be sn,h .
arg maxa
Play action an,h
“
Record the next state sn,h
1.
H φn,hφJn,h.

`

h

ď
1 using (1).
ř

`

A Qn,h

sn,h, a
p

q

.

P

Lemma 3.5 (Determinant Lemma).
x1, . . . , xM
for ℓ

Rd such that kxqk2
deﬁne Dℓ :

P
2, . . . , M

1

ď
λI

P t

`

u

(Lemma C.3 from [25]) For any sequence of vectors
λI and
L for all q
ℓ
´
q
“

. Given a λ
1
1 xqxJq . Then for all M

0 deﬁne D1 :
N and b

ě
P

P r

N

ą

“

0

s

`
“
ř
DM
det
1
p
q
`
λI
det
q ˙
p

d log

ď

1
ˆ

`

M L2
λd

.

˙

(3)

log

ˆ

and

M

1
q
ÿ
“

min

b,

!

xq

}

2
D´1
q

}

)

1
ď p

b

d log
q

`

M L2
λd

.

˙

1
ˆ

`

Our ﬁrst result is to derive a sharper regret guarantee for the MatrixRL algorithm than in [32],
Theorem 3.6. The regret satisﬁes,

R

N H
p

q ď

8H

N H log

d

ˆ

6 log N H
δ

2LφHd

c

γN
λ

log

1
˜

`

1. Under Assumption 3.2,

g
f
f
e

`

˙
N L2
φ
λd ¸

2

2γN N Hd log

N HL2
φ
λd ¸`

1

˜

`

?γN

“

2CψHd1

βN

2CψHd1

“

a

d log

˜

R

¨

˝

g
f
f
e

d1

`

d1N HL2
λ
φ{
δ

?λS

¸ `

˛

‚

2. Under the stronger Assumption 3.3,

?γN

“

2CψH

d1βN

2CψH?d1

“

with probability at least 1

a

2δ.

´

d log

˜

R

¨

˝

g
f
f
e

d1

`

d1N HL2
λ
φ{
δ

?λS

¸ `

˛

‚

The proof of Lemma 3.6 can be found in Appendix A.4. In contrast with the regret guarantees
2 as opposed to H 2. We achieve this by using the
of [32], our bounds have a dependence on H 3
{
following “lazy" version of the commonly used determinant lemma in the bandits/RL literature.

xn,h
Lemma 3.7. Let xn,h
family of positive semideﬁnite matrices for n

P

}

R ˜d satisfying

L for some ˜d

N and 1

} ď
P

h

ď

ď

N and let Dn,h

˜d be a
P
H such that λI ĺ Dn,h ĺ Dn1,h1

R ˜d
ˆ

P

5

if
n, h
p
Dn

“

q ď p
Dn
´

n1, h1
1,H and D1

q

“

in the lexicographic order (i.e. n1

n or h1

λI. The following inequalities hold,

ą

h when n

“

ě

n1). Deﬁne

N

H

N

H

xn,h

}D´1

n ď

1
n
ÿ
“

1 }

h
ÿ
“

1
n
ÿ
“

1
h
ÿ
“

xn,h

2

}

}D´1

n,h `

2HL
?λ

log

1

DN
det
q
p
`
λI
det
q ˙
p

.

ˆ

The proof of Lemma 3.7 can be found in Appendix A.1. As a corollary of Lemma 3.7,
Corollary 3.8. The following inequalities hold,

N

H

N

H

φn,h

}Σ´1

n ď

1
n
ÿ
“

1 }

h
ÿ
“

1
n
ÿ
“

1
h
ÿ
“

φn,h

2

}

}Σ´1

n,h `

2LφHd
?λ

log

1

˜

`

N HL2
φ
λd ¸

.

(4)

(5)

Proof. As an immediate consequence of Lemma 3.7 by setting xn,h

φn,h and Dn,h

Σn,h,

“

“

N

H

N

H

φn,h

}Σ´1

n ď

1 }

φn,h

2

}

}Σ´1

n,h `

1
h
ÿ
“

1
n
ÿ
“

1
n
ÿ
“

h
ÿ
“

Equation 3 from Lemma 3.5 implies,

2HL
?λ

log

1

ΣN
det
p
q
`
λI
det
q ˙
p

ˆ

log

1

ΣN
det
p
q
`
λI
det
q ˙
p

ˆ

d log

1
˜

`

ď

N HL2
φ
λd ¸

.

The result follows.

Corollary 3.8 allows us to transform a sum of inverse Σ´
n,h norms. This
transformation comes at the cost of a 2 factor and a logarithmic cost with a dH multiplier. Since
hides logarithmic factors,
it can be shown that
O
N
n

?dN H
p
?dN H
. This allows us to save a ?H factor in our
we conclude that
q
p
r
ﬁnal regret bound. Lemma 3.7 and Corollary 3.8 can be applied to any episodic linear setting and
can be used to shave off a ?H factor form other episodic stationary linear models beyond MatrixRL.

n to a sum of inverse Σ´

n,h “
O

}
“
φn,h

}Σ´1

}Σ´1

where

n “

φn,h

ř
1
“

ř
1
“

p¨q

H
h

H
h

N
n

ř

ř

O

r

“

}

q

1

1

r

1

1

4 Shared Structure Model

In this work we are concerned with understanding conditions under which sequential learning can
be made more sample-efﬁcient when simultaneously training in the presence of several related tasks.
In contrast with other works that are concerned with the problem of learning from a set of related
source tasks before engaging with a new target task, we are interested in understanding what beneﬁts
can be derived simultaneously from joint representation training across multiple RL problems. We
borrow the subspace sharedness model from [31] and generalize it from the setting of linear bandits
to the previously described MatrixRL setting. We begin by assuming the learner has access to
p
P tasks encoded by the matrices
1. We make
q
u
r is a projection
to satisfy Assumption 3.1, so

1 with known reward functions
p
q

where B
p
q

p
q
‹ “

p
q
‹ u

rp
t
Rd

Ap
‹

the assumption the transitions factorize as Mp
Rr
operator1 and Ap
ˆ
‹
p
?d1S.
Ap
Mp
q
‹ }

P
F
ď

p
q
‹ }

}
We are interested in designing an algorithm that bounds the “shared regret", deﬁned as

. We require all of the matrices Mp
‹

‹ P

“ }

Mp

p
q

B

P
p

P
p

d1

t

“

ˆ

“

F

‹

RP

N H
p

q “

N

P

1
n
ÿ
“

1
p
ÿ
“

V πppq

‹

1

p
sp
qn,1
p

q ´

V πppq

n

1

p
sp
qn,1
p

,
q

where sp

p
qn,1 is the starting state for task p in epsisode n, πp

p
qn is the policy used by task p during
is the optimal policy of task p. Notice that instead of optimizing the usual form

epsiode n, and πp
‹

p
q

1Recall that a linear operator P is a projection if P2v

Pv.

“

6

of the single task regret, here we are interested in minimizing the aggregate regret incurred across
all tasks. The learner’s objective is to leverage the shared structure among the tasks to incur a regret
RP
smaller then what is obtained by learning each task in isolation–a shared regret equal to
P times the single-task MatrixRL regret upper bound.

N H
p

q

‹

, they would be able to use projected features of the form ˜φ
s, a
p

In this framework, the transition dynamics across MDPs are coupled because the agent’s feature
If the learner had
embedding of state-action pairs lie in a common low-dimensional subspace.
knowledge of B
s, a
φ
in
p
their exploration. This would allow the learner to incur regret scaling only in r, independently of d.
Although it is impossible to completely eliminate the d-dependence without apriori knowledge of
B
, we show that in some cases it is possible to improve the d-dependence. Our main result can be
summarized as follows,
Theorem 4.1 (Informal). There exists an algorithm for joint learning over a set of related tasks
Mp

that achieves a regret of

q “

Ap

B

B

q

‹

‹

p
q
‹ “

p
q
‹ up

P

Pr

s

‹

t

N H
p
´´
O hides logarithmic factors
with high probability, where

q “

RP

O

`

Hd?rP

HP ?rd

?N H

,

¯

¯

r

1 trajectories
Recall that for an isolated task in order to recover an estimator
of horizon H we solve d1 independent ridge regression problems (one per column) as deﬁned by
Equation 1.

M of M

given n

´

r

‹

Ă

In the multi-task setting with shared structure, we instead consider the following quadratic objective
1 parameters with that of the shared
that weaves together the estimation of the task-speciﬁc
B projection matrix.2

Ap

P
p

u

t

“

arg min
Pd,r,
B
P
?d1S,
,
¨¨¨

}

ApP q

F

B, Ap
p

1
q,

¨ ¨ ¨

P

, Ap

q

q

?d1S

F

}

ď

Ap1q

}

F

}

ď

(6)

2

J

BAp

p
q

F

B, Ap
p

1
q,

¨ ¨ ¨

P

, Ap

q

q “

Ap

p
q

λ
}

p

P
ÿ
Pr

s

2
F `

}

ψp

p
qn1,h

J K´

1
ψ ´

φp

p
qn1,h

n1

2
¯›
›
›
r projection matrices with r orthonormal columns and
›

H ›
›
›
›

n,h
ÿ

¯

´

´

´

¯

ă

ď

q is the Frobenius ball of radius ?d1S in the space of matrices Rr
p

Where Pd,r corresponds to the set of all d
the search space for Ap
Notice that by virtue of the orthogonality of B’s columns (i.e. BJB
BAp
}
for the shared projection matrix and the low rank dynamics matrices for each of the tasks p
1,

Ir ) the regularizer satisﬁes
to refer to the resulting estimators

, P right before the nth batch of P trajectories is collected.

2
F . We use the notation

2
F “ }

1
qn ,

Bn,

P
qn

Ap

Ap

Ap

¨ ¨ ¨

p
q

p
q

ˆ

“

“

d1

ˆ

}

}

,

.

r

r

r

¨ ¨ ¨

We start by proving a series of data dependent bounds on the estimates
serve as the analogous shared-structure versions of Lemma 3.4.

B,

1
qn ,

Ap

¨ ¨ ¨

Ap

P
qn

,

that will

Bn,

r
r
1
Ap
qn ,

¨ ¨ ¨

r
P
qn

Ap

,

and the true

r

r

r

β1
nH p

δ

q `

pPrP s
ÿ

λ

Appq
‹ }

}

2
F

Now we show a bound for the data-dependent distance between
parameters B
.
Lemma 4.2. For any δ
the following bound holds,
2

, Ap
‹
0, 1

, Ap
‹

¨ ¨ ¨

1
q

P

‹

,

2

q

λ

Appq
n

P p
1
2

F `
›
›
›

q
Σppq
n

1{2

¯

›
´
›
›
δ for all n
›
´

P

Appq
n

Bn

B‹Appq
´

‹ ´
N and where
r

r

F ď
¯›
›
›
›

pPrP s
ÿ

›
›
› r

δ
β1nH p

with probability at least 1
b2
2R2 `
2
p

q “
12R2
p

2 log log

LφS

`

`

1

b

nHP
p

`

q
2Our results will also be true when the ψ, φ maps are task-dependent. In this case, the only change to our

q
´

qq `

q `

q p

`

`

`

¯

3

log

dr

rd1P

log nHP

log 2RLφ

log

5S
p

results would require making Kψ task-dependent.

1
δ ` p

7

And b

“

2Rd1SLψ.

The proof of Lemma 4.2 can be found in Appendix B.1. In contrast with the results of Lemma 3.4,
the guarantees of Lemma 4.2 apply to the sum of the errors across all P tasks. As we’ll see in
the coming discussion this is the main source of difﬁculties in designing a reinforcement learning
algorithm that successfully makes use of this result to construct optimistic value functions. We can
Bn and
use Lemma 4.2 to obtain the following high probability conﬁdence interval jointly around
Ap

1, which is one of our main results:

p
qn

P
p

u

t
Lemma 4.3. For any δ

“

r

0, 1

P p

q

with probability at least 1

δ for all n

N simultaneously,

r

2

´
2
1
{

P

´

Σp

p
qn

BAp

p
q

Bn

Ap

p
qn

p ›
ÿ
›
›
›

´

Σp

p
qn

¯
2
1
{

´

Mp

p
q

r
r
p
Ap
Bn
qn

´

F ď

γn

δ
p

q+

¯›
›
2
›
›
F ď

γn

δ
p

q+

Mp

p
q

P
p

u

“

1 s.t.

Ď #t

´
UF
n p
loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon
r

2P ?d1Sλ and β1n is deﬁned as in Lemma 4.2.

¯
:

r

r

“

δ

q

p ›
´
ÿ
›
›
›

¯›
›
›
›

Mp

p
q
‹ “

B

p
q

Ap
‹

‹

!

P

p

“

1 P #t

)

BAp

p
q

1 s.t.

P
p

“

u

where γn

δ
p

q “

2β1np
δ

q `

Proof. Lemma 4.2 implies that with probability at least 1

δ for all n

2

´

N,

P

2

λ

Appq
n

1{2

1
2

Σppq
n

B‹Appq
›
´
´
›
›
›
?d1S, this implies that

F `
›
›
›

¯

‹ ´

pPrP s
ÿ
p
q
‹ }

Ap

}

F

›
›
› r
ď

Since

β1
nH p

δ

q `

pPrP s
ÿ

λ

Appq
‹ }

}

2
F

Bn

Appq
n

r

r

2

F ď
¯›
›
›
›

2
1
{

B

Σp

p
qn

¯

´

P
ÿ
Pr
The result follows.

p

´

s ›
›
›
›

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

r

r

F ď
¯›
›
›
›

2β1nHp
δ

q `

2P ?d1Sλ.

From here on we use the name E 1 to deﬁne the event of Lemma 4.3 where the sum of the square of
the conﬁdence intervals across all tasks is bounded by γn

δ.

1

δ
p

. Lemma 4.3 implies P
q

E 1
p

q ě

´

p
q, s0, r, H

, features φp
q

p
q :

Algorithm 2 Shared-MatrixRL.
1: Input: Episodic MDP environments

S

A

ˆ
Ñ
2: Initialize:
t
3: For episode n
4:

Rd and ψp
p
Σp
q1

Ð
1,

“

¨ ¨ ¨

p
q : S
Rd
I
P
, N :

Ñ
d
ˆ

Rd1
P
p

u

“

Solve Problem 6 and compute
1 be given by Qp
Qp
Let
t
where,

qn,hu

P
p

“

p

t

Mp

p
q
up
s “ p
, probability parameter δ
.
1,

Mp

p
q1

Rd

0

d1

Pr

P

ˆ

S, A, Pp
0, 1

P p

.
q

t

P

u

Ð
1
qn ,

Ap
Bn,
p
s, a
qn,hp
r
r
arg max
UF

q “

P
p“1P

u

Mppq

t

n p

δ

p
q ÿ

¨ ¨ ¨
Qp

P
qn .
s, a,

Ap
,
p
qn,hp
r

Mp

p
qn

P
p

“

u

1

q

t

p

Mp

qn u

P
p

“

1

“

t

V p

p
qn,1

p
Ď
sp
qn,1,
p

t

Mp

p
q

P
p

“

1

.
q

u

(7)

Ď
1 is the set of ﬁrst states seen at the start of their episodes by all tasks.
p
“
, P :

r

Where
For p

p
sp
qn,1
1,
For stage h

u
¨ ¨ ¨

t
“

1,
Let the current state be sp

, H :

¨ ¨ ¨

“

Play action ap

p

qn,h “

Record the next state sp

Update Σp

p
qn
`

p
qn

Σp

1

Ð

p
qn,h.
arg maxa
p
1.
qn,h

`

`

H

h

ď

ř

A Qp

p

qn,hp

p
qn,h, a
sp

.
q

P

φp

p
qn,h

φp

p
qn,h

J for all p

´

¯ ´

¯

P

.

s

P r

8

5:
6:

7:
8:
9:

10:

11:

12:

13:

We now introduce the Shared-MatrixRL algorithm. In contrast with the simple MatrixRL in Al-
gorithm 1, Shared-MatrixRL makes use of a shared conﬁdence interval for the
matrices. We deﬁne the following optimistic Q

functions for the task family,

Bn,

p
qn

Ap

P
p

t

u

“

1

A : Qp

p
qn,H

Mp

p
q

and

s, a

S

s

P

Pr

ˆ

@t

Qp

q P

s, a,

@p
p
Mp
q

up
p
qn,hp
t
p
p
Mp
where V p
qn,h
q
p
p under model Mp
q. For all s, a, n, h,

up
P
p

p
q

pt

rp

Pr

u

“

`

q

P

1

1

sp, ap
p

sq “
is a vector of dimension

q `

S
|

|

´
1

`

1

s, a,
p
φp

p
q

Mp

p
q

P
p

t
sp, ap
p

u
“
JMp
q

q “
p
q

0

p

@

Ψp

p
q

P

r
and

s

P r
J V p

p
qn,h

1

H

r
h
P r
P
p
q
p

@
Mp

:

s

1

q
corresponding to the value functions of task

pt

´

¯

u

“

`

p

s,
V p
qn,h p

t

Mp

p
q

P
p

“

1

u

q “

Π
0,H
r

s

max
a

”

p

Qp

qn,hp

s, a,

Mp

p
q

P
p

“

1

u

t

.

q
ı
and value functions

The deﬁnition of

the parametric Q functions Qp

p

V p
s,
qn,h p
Equation 7. We deﬁne the optimistic value functions as,

Mp

t

u

“

q

1

p
q

P
p

p

qn,hp

s, a,

Mp

p
q

t

P
p

u

1

q

“

is required to deﬁne the joint optimistic objective for the set of P tasks of

p

Qp

s
V p
qn,h p
qn,hp
The optimization problem of Equation 7 requires to solve for

qn,hp

qn u

s, a,

s, a

q “

,
q

Qp

Mp

t

“

1

p

P
p

p

p

p

s,
V p
qn,h p

p

Mp

qn u

P
p

t

1

q

“
1 optimizes the sum of values
of the P tasks at the beginning of the nth episode. This

Ď

P
p

t

u

“

q “
p
Mp
qn

as ‘seen’ from the initial states
form of optimism is required to ensure the constraint

t

s

p
sp
qn,1

Ď
P
up

Pr

Mp

p
q

t

u

Ď
P
1
p

“

UF
δ
n p

q

Ă

is satisﬁed.

p
Mp
qn
t
p
Mp
qn
Ď

u
P
p

P
p

“
1

Limitations. Shared-MatrixRL works in a similar way to the single task Matrix RL algorithm; a
policy is executed in each of the component tasks based on a series of optimistic Q values. The data
collected by the learner is then used to update the component models via Equation 6. The chief dif-
ference in our approach to the multi task setting lies in the deﬁnition of the shared Q functions. This
is what allows us to make use of the shared conﬁdence interval of Lemma 4.3. Unfortunately this
1 is intractable since it requires the com-
means the computation of the ‘optimistic models’

Ď

r

p

Mp

p
qn

s, a,

qn,hp

for all feasible values of

putation and storage of the Q values Qp

q

“

“

t

u

t

t

u

Pr

P
p

Ď

up

p
qn

p
qn

Mp

Mp

p
sp
qn,1

P
s
1. This situation is not as severe as it seems since the computation of
and then solve for
the optimistic Q functions in the original MatrixRL algorithm (and even in the OFUL algorithm for
linear bandits [1]) is also an intractable problem. Another potential drawback of Algorithm 2 is its
1. An astute reader may posit it to be
requirement to have knowledge of the initial states
possible to overcome this issue by using Thompson Sampling [5, 2]. In this case we would sample
1 from block gaussian distribution where each block is centered around
a set of models
each
1. Unfortunately,
this strategy would cause the degradation of the regret upper bound to a level that is not competitive
with the strategy of solving each task independently. We leave the removal of the assumption on
p
sp
qn,1
t
In order to prove the Shared-MatrixRL satisﬁes a satisfactory sublinear regret guarantee we start
by showing optimism holds for the shared representations parameterized by
Lemma 4.4 (Optimism). Whenever E 1 holds,

p
qn . Sampling from this posterior does not require knowledge of

1 as future work.

p
sp
qn,1

Ă
P
p

Mp

Mp

p
qn

Ď

1.

P
p

P
p

P
p

P
p

u

t

u

u

u

t

u

t

t

“

“

“

“

“

V πppq

‹

1

p
sp
qn,1
p

q ď

p

P
ÿ
Pr

s

V p

p
qn,1

p
sp
qn,1
p

.
q

p

P
ÿ
Pr

s

Ď

Proof. Since

the deﬁnition of

Mp

p
qn

t

V p

p
qn,1

p
sp
qn,1
p

q “

Vn,1

p
qn,1,
sp

p

Mp

qn u

t

P
p

“

1

¯

´

P
p

u
“
Vn,1

1 implies that,

p
qn,1,
sp
p

t

p

Mp

qn u

s

Ď

Ď
P
p
ÿ
Pr

P
p

“

1

q ě

p

P
ÿ
Pr

s

9

Ď
Vn,1

p
qn,1,
sp
p

B

Ap

p
q
‹ u

‹

t

P
p

“

1

q

Since Vn,1

p
qn,1, B
sp
p

‹

,

Ap

p
q
‹ u

t

P
p

“

1

q “

V πppq

‹

1

p
sp
qn,1
p

, the result follows.
q

Similarly we can use our conﬁdence interval bounds to prove the following bound on the bellman
error.
Lemma 4.5. If Assumption 3.3 holds and E 1 is true then for h

H

,

p

Qp

qn,hp

p
qn,h, ap
sp

p

qn,hq ´

p

p
qn,h, ap
sp
p

qn,hq `

Pp

p
q

r

´

p

P
ÿ
Pr

s

p¨|

s
P r
p
qn,h, ap
sp

p

qn,hq

JV p

p
qn,h

`

1

¯

´1

ď

2CψH

γn

d

δ
p

q

p

P
ÿ
Pr

s

2

p

φp
qn,h}

}

Σppq
n

´

¯

The proof of Lemma 4.5 can be found in Appendix B.2. Having established that optimism holds, we
can use a similar set of techniques as in the proof of Theorem 3.6 to show a regret guarantee. First
we derive Corollary 4.6, an equivalent version to Corollary 3.8. This allows us to maintain the ?H
factor improvement in the multitask setting. This result is a consequence of Lemma 3.7.
Corollary 4.6. The following inequalities hold,

N

H

1
n
ÿ
“

h
ÿ
“

1 d ÿ

Pr

p

P

2

p

φp
qn,h}

}

Σppq
n

´1

ď

s

´

¯

N

H

2

1
n
ÿ
“

1
h
ÿ
“

d ÿ
P
p
Pr

s

2

p

φp
qn,h}

}

Σppq
n,h

´1

`

2LφHdP
?λ

log

1

˜

`

´

¯

N HL2
φ
λd ¸
(8)

.

P

¨ ¨ ¨

, φp

qn,hq

where φp

Proof. Deﬁne N HP variables xn,h,p
1
qn,h,
φp
p
In this case, Dn,h is a block diagonal matrix (with d
xn,h
φp
qn,h}
}
a consequence of Lemma 3.7,

p
qn,h is located in the p

ˆ
´1. By deﬁnition

}D´1

cř

n,h “

Σppq
n,h

s }

´

P

Pr

P

´

¯

p

p

2

}

RdP ordered lexicographically and satisfying xn,h

th d dimensional slot of xn,h for all p

“
P
.
s
d diagonal blocks equal to Σn,h) such that
xn,h
´1. As

P r

p

2

p

P

Pr

s }

φp
qn,h}

}D´1

n “

Σppq
n

´

¯

cř

N

H

1
n
ÿ
“

h
ÿ
“

1 d ÿ

Pr

p

P

2

p

φp
qn,h}

}

Σppq
n

´1

ď

s

´

¯

N

H

2

1
n
ÿ
“

1
h
ÿ
“

d ÿ
P
p
Pr

s

2

p

φp
qn,h}

}

Σppq
n,h

´1

`

´

¯

2LφH
?λ

log

DN
det
1
p
`
λIdP
det
p

q
q ˙

.

ˆ

Where we have used the notation Is to denote the s
p
of DN
Σp
1 det
qN
p
`

DN
1 we see that det
p

q “

P
p

`

`

“

1

ˆ
1
q

s dimensional identity matrix. By deﬁnition
and therefore,

N HL2
φ
λd ¸
Where the last inequality follows from Equation 3 in Lemma 3.5. The result follows.

DN
det
1
p
`
λIdP
det
p

p
Σp
det
qN
p
`
λId
det
p

q
q ¸ ď

q
q ˙

P d log

1
p
ÿ
“

log

log

˜

˜

`

“

ˆ

1

1

ś
P

.

Similar to Corollary 3.8, the result of Corollary 4.6 allows us to transform inverse norms deﬁned

1

´

p
qn

Σp

by the matrices
multiplicative cost plus a logarithmic term with a dHP multiplier.
Theorem 4.7. The regret of Shared-MatrixRL satisﬁes,

, into inverse norms deﬁned by the matrices

¯

´

1

Σp

p
qn,h

´

´

¯

, at a constant

RP

N H
p

q ď

H

N HP log

d

6 log N H
δ

ˆ

`

˙

4CψH 2dP

L2
φ
?λ ¸

1
˜

`

log

1

˜

`

N HL2
φ
λd ¸

δ
p

N HP d
q

1

`

˜

L2
φ
?λ ¸

log

1
˜

`

N HL2
φ
λd ¸

.

2CψH

γN

g
f
f
e

With probability at least 1

2δ.

´

10

γN

δ
p

q`

a

The proof can be found in Appendix B. Since γN
`
ignoring polynomial dependencies on d1) Theorem 4.7 implies,
Corollary 4.8. The regret of Algorithm 2 satisﬁes,

q «

δ
p

dr

rP (up to logarithmic factors and

O

H?N HP

RP

N H
p

q ď
With probability at least 1

´

`
2δ.

r

H?dr

`

rP ?N HP d

¯

O

“

Hd?rP

´´

`

HP ?rd

?N H

.

¯

¯

r

q

!

O

d and r

HdP ?N H
p

´
achieved by using the Ma-
This result improves upon the shared regret of order
trixRL algorithm to learn each task independently. Interestingly, learning the tasks’ shared structure
P . To explain this phenomenon observe that the
only becomes beneﬁcial when r
!
degrees of freedom (i.e. the number of parameters to learn) in Shared-MatrixRL equals dr
P r.
The degrees of freedom of running P independent copies of MatrixRL in contrast equals dP d1.
For shared representation learning to be more efﬁcient than learning each task alone, we require
dP d1. This is why for Shared-MatrixRL learning to be truly beneﬁcial (and at-
dr
`
tain a smaller regret upper bound than running P tasks independently) we require dr
dP d1 and
!
P rd1
r, learning the shared
!
matrix B
may require more data than learning the dP d1 parameters of estimating the models for
all P tasks independently. Although we have not developed a lower bound for the speciﬁc Ma-
trixRL setting, the results of Yang et al. [31] provide evidence to posit the regret upper bound for
Shared-MatrixRL in Theorem 4.7 is optimal.

dP d1. For example when the number of tasks is small and P

P rd1

`

!

!

r

‹

5 Computationally Efﬁcient Shared-MatrixRL

P
Algorithm 2 has two computationally intensive components. First, solving for
qn
and second, solving for Equation 7. The ﬁrst objective may be difﬁcult to solve because it involves
solving a bilinear quadratic optimization problem. The second one can prove even more challenging
r
ﬁrst because it requires a way to ‘store’ the parametric value functions V p
(these
functions may be highly non-linear), and second because solving for Equation 7 involves optimizing
a non-convex objective.

r
s,
p

r
Mp

p
qn,1

Bn,

Ap

Ap

¨ ¨ ¨

p
q

P
p

t

u

“

q

1

,

1
qn ,

In this section we show that, given access to a computational oracle for Problem 6 and assuming
S, A are ﬁnite, there exists a computationally efﬁcient procedure for solving for the joint optimistic
objective of Equation 7 of Algorithm 2. As it is mentioned in the discussion surrounding Equation 7
of [32], the conﬁdence bonus of Equation 2 can be substituted by

Qn,h

s, a
p

q “

r

s, a
p

q `

s, a
φ
p

J
q

MnΨJVn,h

1

`

`

2LΨH

βn

s, a
φ
p

}

q}Σ´1

n

This corresponds to explicitly solving for the optimistic model maximizing the Q values at state
p
q be a set of P conﬁdence radii. In the multi-task
action pair
q
setting, let’s consider enforcing,

Ă
and in-episode time h. Let τ p

s, a
p

a

p

Mp

qn P

2
1
{

Σp

p
qn

Mp

p
q

Bn

Ap

p
qn

´

τ p

p
q

:

UF
n p

“

δ, p, τ p

p
q

q

F ď

P
p

"›
´
›
r
›
δ
If
, we can allow for the per-state maximization of the optimistic models as in
›
q
p
the single task setting (see Equation 2) and obviate solving for problem 7 in Algorithm 2. If we call
ř
p
Mp
qn

achieving the argmax in the deﬁnition Qp

¯›
›
›
›

2
Ď
q

τ p
p

s, a, τ p

γn

p
q

p
q

p
q

ď

*

´

¯

r

r

“

p

1

s, a
p
s, a
r
τ p
p
p
Ď
q
conﬁdence radii for model p to be upper bounded by τ p

the model in UF
δ, p, τ p
n p
s, a
φ
p

JMΨJV p
q

q
maxM

δ,p,τ ppq

p
qn,h

q `

UF

n p

`

q

P

1

q “
p
. This is because restricting the individual
q
q
p
q for all state action pairs ensures that,

qn,hp

p

Qp

qn,hp

p
qn,h, ap
sp

p
qn,h, τ p

p
q

q ´

r

´

p

p
qn,h, ap
sp
p

qn,hq `

Pp

p
q

sn,h, an,h

p¨|

p
qn,h

JV p
q

`

1

τ p
p

p

P
ÿ
Pr

s

p
q

q
¯
p
qn,h

ď

ď

p

P
ÿ
Pr

s ›
´
›
›
›
Cψ

p

P
ÿ
Pr

s

›
›
›

φp

p
qn,h

J

p

Mp

qn ´

p
q

Mp
‹

Ψp

p
q

J V p

p
q

1

τ p
p

`

¯
p
qn,h

V p

´

Ď
τ p
p

1

`

2 ›
´
›
›
›
J

¯›
›
›
p
›
qn,h

φp

¯
p

Mp

qn ´

´

¯

´

Ď

p
q

q
›
›
›

8 ›
›
›
›

p
q

Mp
‹

2
¯›
›
›
›

2

q
›
›
›
›

11

p
q

τ p

P
If
p
u
Lemma 4.5 imply,

t

“

1 are deﬁned such that

2

τ p

p
q

P
p

“

1

γn

δ
p

q

ď

the same arguments as in the proof of

p

Qp

qn,hp

p
qn,h, ap
sp

p
qn,h, τ p

p

P
ÿ
Pr

s

ř

p
q

q ´

˘
`
p
qn,h, ap
sp
r
p

p

´

qn,hq `

Pp

p
q

sn,h, an,h

p¨|

p
qn,h

JV p
q

`

p
q

1

τ p
p

q
¯

´1

2

p

φp
qn,h}

}

Σppq
n

´

¯

p
q

1

τ p
p

q`

`

2CψH

γn

d

δ
p

q

p

P
ÿ
Pr

s

ď

J
q

r
Σppq
n

´

´1
r
¯

Qp

p

qn,hp

s, a, τ p

p
q

The Q functions Qp

p

,
qn,hp¨

p
q

, τ p
¨

q

satisfy,

p

Qp

qn,hp

s, a, τ p

p
q

q “

p
q

rp

s, a
p
2LΨHτ p

φp

p
q

q `
p
q

}

φp

p
q

s, a
p
s, a
p

q}

Bn

Ap

p
qn

Ψp

p
q

J V p

p
qn,h

´

¯

Where

V p

p
qn,h

`

p
q

1

τ p
p

q “

Π
0,H
r

s

max
a

”

If S, A are ﬁnite sets, then for any ﬁxed set of thresholds
be expressed as the solution to a linear program in the variables V p

τ p

t

u

quadratic constraint of the form
tasks p

P

p

P

Pr

s

P r

s

2

τ p

p
q

`

˘

γn

δ
p

q

ď

becomes the convex Quadratically Constrained Linear Program (QCLP),

s, a, n, h.

@

p
q

q
ı
, solving for Qp
can
p
1 and Qp
qn,h. By adding a
the resulting optimization problem over all

s, a, τ p
p

p
qn,h

p
q

p
q

`

q

max
P
p
s
Pr

V p

p
qn,1

p
qn,1, τ p
sp
p

p
q

q

1
p
ÿ
“

P
ÿ
Pr
operations to arrive at an

p

1
N H

s.t.

2

τ p

p
q

γn

δ
p

,
q

ď

and thus it will take poly
lem. This is enough to guarantee optimism up to an overall error of order
˘
in Chapter 4 of [10] on how to solve QCLP problems efﬁciently.

`

¯

s ´
1
N 2H2 approximate solution for this prob-
1
N H . See the discussion

ř
P

6 Conclusion

Hd?rP

HP ?rd

In this work we are the ﬁrst to analyze the problem of joint training across a set of related Markov
Decision Processes. We show that when the training tasks’ transition dynamics can be embedded
in a common low-dimensional subspace of dimension r, a joint training algorithm can obtain regret
– the regret of learning each task
O
separately ignoring the shared task structure. Our training method solves a quadratic optimization
r
problem that jointly penalizes the shared and task-dependent model parameters (see Equation 6).
We expect the techniques we have introduced in this work, including the multitask least squares ob-
jective of Equation 6 and the parametric Q functions Qp
, to have applications
in other MDP models with function approximation–such as Linear MDPs [18, 34] amongst others.

HdP ?N H
p

as opposed to

?N H

qn,hp

s, a,

Mp

´´

up

p
q

O

`

sq

¯

¯

r

Pr

t

q

P

p

Acknowledgments and Disclosure of Funding

This was conducted as a result of the Google BAIR Commons program at UC Berkeley.

References

[1] Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic ban-

dits. Advances in neural information processing systems, 24:2312–2320, 2011.

[2] M. Abeille and A. Lazaric. Linear thompson sampling revisited. In Artiﬁcial Intelligence and

Statistics, pages 176–184. PMLR, 2017.

12

[3] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. Flambe: Structural complexity and

representation learning of low rank mdps. arXiv preprint arXiv:2006.10814, 2020.

[4] A. Agarwal, Y. Song, W. Sun, K. Wang, M. Wang, and X. Zhang. Provable beneﬁts of repre-

sentational transfer in reinforcement learning. arXiv preprint arXiv:2205.14571, 2022.

[5] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International conference on machine learning, pages 127–135. PMLR, 2013.

[6] J. Baxter. Learning internal representations. In Proceedings of the eighth annual conference

on Computational learning theory, pages 311–320, 1995.

[7] J. Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence research, 12:

149–198, 2000.

[8] S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In Learn-

ing theory and kernel machines, pages 567–580. Springer, 2003.

[9] D. P. Bertsekas. Reinforcement learning and optimal control. Athena Scientiﬁc Belmont, MA,

2019.

[10] S. Boyd, S. P. Boyd, and L. Vandenberghe. Convex optimization. Cambridge university press,

2004.

[11] Y. Cheng, S. Feng, J. Yang, H. Zhang, and Y. Liang. Provable beneﬁt of multitask representa-

tion learning in reinforcement learning. arXiv preprint arXiv:2206.05900, 2022.

[12] C. D’Eramo, D. Tateo, A. Bonarini, M. Restelli, and J. Peters. Sharing knowledge in multi-task
deep reinforcement learning. In International Conference on Learning Representations, 2019.

[13] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. Few-shot learning via learning the

representation, provably. arXiv preprint arXiv:2002.09434, 2020.

[14] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bi-
arXiv preprint

linear classes: A structural framework for provable generalization in rl.
arXiv:2103.10897, 2021.

[15] S. R. Howard, A. Ramdas, J. McAuliffe, J. Sekhon, et al. Time-uniform chernoff bounds via

nonnegative supermartingales. Probability Surveys, 17:257–317, 2020.

[16] S. R. Howard, A. Ramdas, J. McAuliffe, and J. Sekhon. Time-Uniform, Nonparametric,
Nonasymptotic Conﬁdence Sequences. The Annals of Statistics, 49(2):1055–1080, 2021.

[17] J. Hu, X. Chen, C. Jin, L. Li, and L. Wang. Near-optimal representation learning for linear
In International Conference on Machine Learning, pages 4349–4358.

bandits and linear rl.
PMLR, 2021.

[18] K.-S. Jun, R. Willett, S. Wright, and R. Nowak. Bilinear bandits with low-rank structure. arXiv

preprint arXiv:1901.02470, 2019.

[19] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and
K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv
preprint arXiv:2104.08212, 2021.

[20] R. Lu, A. Zhao, S. S. Du, and G. Huang. Provable general function class representation learning

in multitask bandits and mdps. arXiv preprint arXiv:2205.15701, 2022.

[21] A. Maurer. Bounds for linear multi-task learning. The Journal of Machine Learning Research,

7:117–139, 2006.

[22] T. Moskovitz, M. Arbel, J. Parker-Holder, and A. Pacchiano. Towards an understanding of
In International Conference on Artiﬁcial

default policies in multitask policy optimization.
Intelligence and Statistics, pages 10661–10686. PMLR, 2022.

13

[23] R. Müller and A. Pacchiano. Meta learning mdps with linear transition models. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 5928–5948. PMLR, 2022.

[24] O. Nachum and M. Yang. Provable representation learning for imitation with contrastive

fourier features. arXiv preprint arXiv:2105.12272, 2021.

[25] A. Pacchiano, C. Dann, C. Gentile, and P. Bartlett. Regret bound balancing and elimination for

model selection in bandits and rl. arXiv preprint arXiv:2012.13045, 2020.

[26] M. L. Puterman. Markov decision processes. Handbooks in operations research and manage-

ment science, 2:331–434, 1990.

[27] R. S. Sutton. Introduction: The challenge of reinforcement learning. In Reinforcement Learn-

ing, pages 1–3. Springer, 1992.

[28] Y. W. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pas-
canu. Distral: Robust multitask reinforcement learning. arXiv preprint arXiv:1707.04175,
2017.

[29] N. Tripuraneni, M.
The importance of
M. F. Balcan,
Systems, volume 33, pages 7852–7862. Curran Associates,
https://proceedings.neurips.cc/paper/2020/file/59587bffec1c7846f3e34230141556ae-Paper.pdf.

learning:
Jin.
In H. Larochelle, M. Ranzato, R. Hadsell,
Information Processing
URL

editors, Advances in Neural

Jordan,
task diversity.

On the theory of

and H. Lin,

Inc., 2020.

and C.

transfer

[30] N. Tripuraneni, C. Jin, and M. I. Jordan.

Provable meta-learning of linear representa-
In M. Meila and T. Zhang, editors, Proceedings of the 38th International Con-
tions.
ference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139
of Proceedings of Machine Learning Research, pages 10434–10443. PMLR, 2021. URL
http://proceedings.mlr.press/v139/tripuraneni21a.html.

[31] J. Yang, W. Hu, J. D. Lee, and S. S. Du. Impact of representation learning in linear bandits. In

International Conference on Learning Representations, 2020.

[32] L. Yang and M. Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pages 10746–10756. PMLR,
2020.

[33] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A
benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on
Robot Learning, pages 1094–1100. PMLR, 2020.

[34] D. Zhou, Q. Gu, and C. Szepesvari. Nearly minimax optimal reinforcement learning for linear
In Conference on Learning Theory, pages 4532–4576.

mixture markov decision processes.
PMLR, 2021.

A Proofs from Section 3

A.1 Proof of Lemma 3.7

R ˜d satisfying

Lemma 3.7. Let xn,h
xn,h
family of positive semideﬁnite matrices for n
if
n, h
p
Dn

N and 1
in the lexicographic order (i.e. n1

n1, h1
1,H and D1

} ď
P

λI. The following inequalities hold,

ď
ą

P

}

q

L for some ˜d

N and let Dn,h

˜d be a
P
H such that λI ĺ Dn,h ĺ Dn1,h1
h
ď
n1). Deﬁne
n or h1

h when n

R ˜d
ˆ

P

ě

“

q ď p
Dn
´

“

“

N

H

N

H

xn,h

}D´1

n ď

1
n
ÿ
“

1 }

h
ÿ
“

1
n
ÿ
“

1
h
ÿ
“

xn,h

2

}

}D´1

n,h `

2HL
?λ

log

1

DN
det
p
q
`
λI
det
q ˙
p

.

ˆ

(4)

14

Proof. Deﬁne en,h

xn,h

}D´1

n ě

2

}

1

}
´

1

“
xn,h

}
´
}D´1
n,h

¯

xn,h

}D´1
. Thus,

n ď

xn,h

2

}

}D´1

n,h

¯

. We deﬁne ec

n,h “

1

´

en,h

“

N

H

N

H

N

H

xn,h

}D´1

n “

1
n
ÿ
“

1 }

h
ÿ
“

1
n
ÿ
“
N

1
h
ÿ
“
H

en,h

}

xn,h

}D´1

n `

ec
n,h}

xn,h

}D´1

n

1
n
ÿ
“
H

N

1
h
ÿ
“
ec
n,h

L
?λ

.

1
h
ÿ
“
Where the inequality holds because en,h

bound

xn,h

}

(setting B

L
?λ
n ď
1
n and C

}D´1
D´
“

D´

“

holds. If ec

2

xn,h

}

}D´1

n,h `

ď

xn,h

1
n
ÿ
“

1
h
ÿ
“
}D´1
}D´1
}
1 (and therefore
}D´1
n ě
1
n,h. These satisfy B ľ C ą 0) implies

1
n
ÿ
“
xn,h

}
xn,h

n,h “

n ď

n,h

2

}

and because for all n, h the

xn,h

2

}

}D´1

n,h

), Lemma C.4,

4

ď

xn,h

}
xn,h

}

2
D´1
}
n
2
D´1
n,h

}

xJ

sup
0
x

ď

1

D´
n
1
˘
n,h
¯

‰

xJ

D´
`
´
. Notice that HEn

x
x ď

1
D´
det
n q
p
1
D´
det
n,hq
p

Dn,h
det
p
Dn
det
p

q
q

.

“

h s.t. ec

n,h “

ř

u “ H

h ec

n,h for all n. For all n denote by
1 so that

we deﬁne hn

H

ě
1

“

`

h ec

1

1
Deﬁne En
“
h s.t ec
min
hn
´ř
n,h “
“
t
Dn
Dn,hn “
1.
The following telescoping relation holds.

n,h ě
1
.
u

In case
¯

t

`

N

1
n
ź
“

Dn,hn q
det
p
Dn
det
p
q
4En

ě

looooomooooon
Therefore it must be the case that,

1

Dn
det
p
q
Dn,hn q
det
p

`

det

DN
p
`
D1
det
p

q

1

q

1

DN
det
p
`
λI
det
q
p

q

“

“

¨

1

ě
looooomooooon

n En

4

ř

ď

1

DN
det
p
`
λI
det
q
p

q

.

Thus,

1 En

N
n

“

ď

ř

2 log

´
N

det
p
det
p

DN `1
λI

q

H

¯

ec
n,h ď

H

1
n
ÿ
“

1
h
ÿ
“

The result follows.

. Finally, this implies,

N

1
n
ÿ
“

En

ď

2H log

1

DN
det
q
p
`
λI
det
q ˙
p

.

ˆ

Recall that we denote the ’good’ event that all conﬁdence intervals
as E.

U1,2
n u

t

n hold at all times n

N

P

A.2 Proof of Lemma 3.4

Lemma 3.4. For all δ

0, 1

with probability at least 1

δ for all n

N simultaneously,

P p
M

q
Rd

:

ˆ

d1

M

‹ P t

2
1
{
q
2
1
{
q
“
2,1 denotes the l1 norm of the l2 norm of the columns of B while

M
p
M
p

Mn
Ă

d1βn
a

‹ P t

Mn

Σn

Σn

u
:

Rd

M

M

a

2,1

}p

}p

q}

q}

´

ď

“

ď

´

d1

P

P

d1

u

ˆ

F

:

:

U1,2
n .
UF
n .

´

P
βn

Where

B

}

}

the Frobenius norm, ?βn

Ă

?λS and R

“ }

`

¯

B

F corresponds to

}
1
K´
ψ }

}
Lψ

SLφ.

`

“

R

d log

d1

`

c

´

d1nHL2
δ

φ{

λ

15

:, i

J
q

K´

M‹

M‹

, i

s ´

φJn1,h p

Proof. By deﬁnition, E

1
sn1,h, an1,h
ψ ψn1,h
|
”
. Notice that,
r
s
Dn1,h
|
@
N
Dn1,h
, this implies
random variables are conditionally subgaussian with parameter R. Problem 1 can

Observe that E
r
ă 8
all the Dn1,h
i
q
p
be decomposed into d1 independent Ridge Regression problems for each column of M

1
ψ }
`
0 for all i

s
P r
. Since R

i
p
Fn1,h

J φn1,h
q

. Let Dn1,h

ψJn1,hK´

1
ψ r

“
, n1

SLφ :

q| ď }

s
H

K´

d
s

s “

“ p

q “

Rd1

Lψ

i
p

i
p

P r

P r

P r

P r

R.

, h

, h

n1

H

N

q|

P

ı

s

s

.

,

:

Mn

:, i

arg min
M

r

s “

2 `
n,h
›
ÿ
›
›
As a consequence of Theorem C.2 we see that with probability at least 1

H ›
›
›

s ´

Ă

n1

ă

ď

s

r

δ,

ψJn1,hK´

:, i

φJn1,hM

:, i

1
ψ r

2

‹
2
2 .
s}

M

λ

}

r

:, i

´

Mn

}

:, i

r
1{2

s ´
Mn

M

:, i

Σn

s}
‹r
:,i
M‹r

“}p

q

Σn
:,i
Ă
sq}
r
looooooooooooomooooooooooooon
d1

p
and all n

N

s´

Ă

P r

s

P r

s

For all i

. Therefore,

d log

˜

R

ď

2

g
f
f
e

d1 `

d1nHL2
φ{
δ

λ

?λS :

“

¸ `

βn.

a

Σn
p

2
1
{
q

›
›
›

M

‹

´

Mn

´

Ă

¯›
›
›

d1

βn,

2,1 ď

a

A.3 Regret Guarantees for MatrixRL

Σn
p

2
1
{
q

›
›
›

M

‹

´

Mn

´

Ă

¯›
›
›

F ď

d1βn

a

Lemma A.1 (Optimism). Suppose E holds. Then for all h

H

and

P r

s

s, a
p

q P

S

ˆ

A, we have

Q‹hp
Proof. The same argument as in Lemma 4 of [32] can be used to show this result.

s, a
p

Qn,h

s, a

q ď

q

Next we show that the conﬁdence balls U1,2
the estimation error.
Lemma A.2. For any M

n we have,

U1,2

P

n and UF

n can be used to give a strong upper bound for

For any M

UF
n ,

P

}

}

φJs,a

M

´

Mn

´

φJs,a

M

´

´

¯

¯

Ă

Mn

Ă

d1

1

}

ď

φs,a

βn

}

}Σ´1

n

a

2

}

ď

d1βn

φs,a

}

}Σ´1

n

Proof. The following inequalities hold,

}

φJs,a

M

´

Mn

´

Ă

1

}

¯

Similarly,

“ }

φJs,a p
φJs,a p
ď }
βn
d1

ď

a

2
1
{

´
q

Σn

p

Σn

2

} p

Σn

´
q
φs,a

}

2
1
{

}
}Σ´1

n

2
1
{
q
Σn

M

Mn

´
2
1
{
q

´
M

Ă
´

´

1

}
¯
Mn

¯

Ă

2,1

}

}

φJs,a

M

´

Mn

´

Ă

2

}

“ }

¯

ď }

ď

a
φJs,a p
φJs,a p
d1βn

2
1
{

q
Σn

Σn

2
1
{

´
q

Σn
p

2

} p

Σn

´
q
φs,a

}

2
1
{

}
}Σ´1

n

M

Mn

2

}
¯
Mn

´
M

Ă
´

´
2
1
{
q

´

F

}

¯

Ă

a

16

The following lemma holds,
Lemma A.3. Suppose that E holds. Then for h

H

s

P r

, we have,

1. Under Assumption 3.2,

Qn,h

sn,h, an,h
p

q `
2. Under the stronger Assumption 3.3,

q ´

`

r

sn,h, an,h
p

P

sn,h, an,h

p¨|

JVn,h
q

`

1

ď

2CψHd1

βn

˘

a

Qn,h

sn,h, an,h
p

q ´

r

sn,h, an,h
p

q `

P

sn,h, an,h

p¨|

JVn,h
q

`

1

ď

2CψH

d1βn

φn,h

}Σ´1

n

φn,h

}Σ´1

n

}

}

.

.

Proof. Let
tion 3.2,

M

“

arg maxM

`
U

P

1,2
n

φJn,hMΨJVn,h

`

1. We start by proving the result under Assump-

˘

a

Ă
Qn,h

sn,h, an,h
p

q ´

“

r

sn,h, an,h
p
φJn,h
`

M

´

q `
M

‹

P

sn,h, an,h

p¨|
ΨJVn,h

1

`

JVn,h
q

`

1

˘

´
φJn,h

M
Ă
´
Ă
Vn,h
`

M

¯
‹

´

¯
φJn,h

1

}8}

Cψ

}

CψH

φJn,h

M

}

}

´
φJn,h

´

´

M
Ă
´
βn

CψH

ď }
i
q
p
ď
ii
p
q
ď

ď
ii
p
q
ď

1

ΨJVn,h

}

1

`

}8

}

M

1

}

‹

¯

´

1

}

M

´
M

Ă
‹

¯
Mn

¯
Ă
2CψHd1
}Σ´1
follows by Assumption 3.2, and inequality

Ă
φn,h
}

´

n

1

}

` }

φJn,h

Mn

´

Ă

M

´

1

}

¯

‹

¯

i
Inequality
p
bounded by H. Finally

q

1 is
is a consequence of conditioning on E and suing the deﬁnition of U1,2
n .

holds because the range of Vn,h

ii
p

a

`

q

If instead Assumption 3.3 holds,

iii
p

q

Qn,h

sn,h, an,h
p

q ´

“

r

sn,h, an,h
p
φJn,h
`

M

´

q `
M

‹

P

sn,h, an,h

p¨|
ΨJVn,h

1

`

JVn,h
q

`

1

˘

2

ΨJVn,h

}

1

2

}

`

}

M

2

}

‹

¯

´

2

´
φJn,h

M

¯
‹

M
Ă
´
Ă
Vn,h
`

´

1

φJn,h

}

}8}
M

Cψ

}
CψH

CψH

´
φJn,h

}

Ă
´
d1βn

ď }
i
p
q
ď

ď

ď
ii
p
q
ď

¯
φJn,h

M

´
M
Ă
}
‹
¯
Mn

´

´
M

2

}

` }

φJn,h

Mn

´

Ă

M

´

2

}

¯

‹

¯

¯
Ă
}Σ´1
n
follows by Assumption 3.3, and inequality

´
2CψH

Ă
φn,h
}

a

ii
p

q

Inequality

i
p

q

A.4 Proof of Theorem 3.6

Theorem 3.6. The regret satisﬁes,

R

N H
p

q ď

8H

N H log

d

ˆ

6 log N H
δ

2

2γN N Hd log

N HL2
φ
λd ¸`

1

˜

`

2LφHd

c

γN
λ

log

1
˜

`

g
f
f
e

`

˙
N L2
φ
λd ¸

17

1. Under Assumption 3.2,

?γN

“

2CψHd1

βN

2CψHd1

“

a

d log

˜

R

¨

˝

g
f
f
e

d1

`

d1N HL2
λ
φ{
δ

?λS

¸ `

˛

‚

2. Under the stronger Assumption 3.3,

?γN

“

2CψH

d1βN

2CψH?d1

“

with probability at least 1

a

2δ.

´

d log

˜

R

¨

˝

g
f
f
e

d1

`

d1N HL2
λ
φ{
δ

?λS

¸ `

˛

‚

Proof. Let’s start by conditioning on E. Let ?γn
?γn

2CψH?d1βn if the stronger Assumption 3.3 holds instead.

“

2CψHd1βn if Assumption 3.2 holds and

N H
p

q “

N
n

“

1 V ‹1

sn,1
p

q ´

V πn
1

sn,1
p

. The optimism property of Lemma A.1 implies
q

“
Recall that R
that,

R

N H
p

q

i
p
q
ď

ii
p
q
ď

ř

Vn,1

sn,1
p

q ´

V πn
1

sn,1
p

q

N

1
n
ÿ
“
N

?γn

φn,1

}

}Σ´1

n `

P

sn,h, an,h

p¨|

J
q

Vn,2
p

´

V πn
2

q

1
n
ÿ
“
N

?γn

φn,1

}

}Σ´1

n `

P

sn,h, an,h

p¨|

J
q

Vn,2
p

´

V πn
2

Vn,2

q ´ p

sn,2
p

q ´

V πn
2

sn,2
p

qq

`

δn,2

“

1
n
ÿ
“

Vn,2

sn,2
p

q ´

V πn
2

sn,2
looooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooon
p

q

holds by Optimism. Inequality

holds by Lemma A.3. A recursive application of

i
Inequality
q
p
this decomposition yields,

ii
p

q

R

N H
p

q ď

N

H

1
n
ÿ
“

1
h
ÿ
“

?γn

φn,h

}

}Σ´1

n `

δn,h

ď

?γN

N

H

ˆ

1
n
ÿ
“

1 }

h
ÿ
“

φn,h

δn,h

}Σ´1

n

`

I

Where the right inequality holds because γn is increasing (in n). The sum
easily be bounded by invoking Lemma C.1 by observing that
probability at least 1

1 δn,h can
4H for all n, h. Thus, with
ř

δ for all n,

δn,h
|

| ď

ř

“

“

loooooooooomoooooooooon
1

N
n

H
h

´

N

H

1
n
ÿ
“

1
h
ÿ
“

δn,h

8H

ď

N H log

d

6 log N H
δ

.
˙

ˆ

We proceed to bound I. By Corollary 3.8, equation 5,

I

ď

N

H

1
n
ÿ
“

1
h
ÿ
“

φn,h

2

}

}Σ´1

n,h `

2LφHd
?λ

log

1

˜

`

N HL2
φ
λd ¸

.

By Lemma 3.5,

N

H

g
f
1
n
ÿ
f
“
e
We then conclude that with probability at least 1

1
h
ÿ
“

n,h ď

}

2

φn,h

}Σ´1

2

2N Hd log

δ, whenever E holds,

N HL2
φ
λd ¸

.

1

˜

`

´

18

R

N H
p

q ď

8H

N H log

d

ˆ

6 log N H
δ

2

2γN N Hd log

N HL2
φ
λd ¸`

1

˜

`

2LφHd

c

γN
λ

log

1
˜

`

g
f
f
e

`

˙
N L2
φ
λd ¸

Since E holds with probability at least 1

´

δ the result follows by a simple union bound.

B Proofs of Section 4

1 trajectories
Recall that for an isolated task in order to recover an estimator
of horizon H we run d1 independent Ridge Regression problems (one per column) as deﬁned by
Equation 1.

M of M

given n

´

‹

Ă

Mn

:, i

r

s “

arg min
M

Ln

M
p

.
q

Ă

r

ψJn1,hK´

1
ψ r

:, i

s ´

φJn1,hM

:, i

r

s

M

λ

}

r

:, i

2
2
s}

2

2 `
›
›
›

Where

Ln

M
p

q “

r

n1

n,h
ÿ

ă

ď

H ›
›
›

In order to avoid notational clutter let’s call
be rewritten as,

p

ψp
qn1,h “
r

K´

1
ψ ψp

p
qn1,h. Under this notation, problem 6 can

.

arg min

B

P

Pd,r ,Ap1q,

,ApP q

P

¨¨¨

Rrˆd1

F

B, Ap
p

1
q,

¨ ¨ ¨

P

, Ap

q

q

F

B, Ap
p

1
q,

¨ ¨ ¨

P

, Ap

q

q “

Ap

p
q

λ
}

2
F `

}

p

P
ÿ
Pr

s

H }

n1

n,h
ÿ

ă

ď

(9)

2
2.

BAp

p
q

´

¯

p

J φp

qn1,h}

p

ψp
qn1,h ´
r

We will make use of the following standard bound on the covering number of the l2 ball.
Lemma B.1. For any ε
x
x
r
}

s
is upper bounded by

covering number of the Euclidean ball in Rd with radius

P p
2
ď

0, 1
r

0 i.e..

2r
`
ε

Rd :

the ε

´

ą

P

u

t

}

d

1

.

(See for example Lemma D.1 in [14] )

`

˘

We will also make use of the following bound on the covering number of the space of
1
B, Ap
q,
p
B, Ap1q,

matrices under the norm,

¯B, ¯Ap1q,

, ¯Appq

, Appq

¯Ap1q

Ap1q

, Ap

¯Appq

Appq

max

¨ ¨ ¨

p
q

uq

¯B

B

,

,

,

}p

¨ ¨ ¨

uq ´ p

¨ ¨ ¨

uq} “

´

}

}

´

}

¨ ¨ ¨

}

´

}

´

}

¯

0, 1

Lemma B.2. For any ε
norm over l2 norms of the set of matrices
matrix with r orthonormal columns and Ap
is upper bounded by
d1
, i

S for all i

the ε

P p

´

q

2

B, Ap
p
p
q

P

s}

ď

P r

s

covering number under the norm computed by taking the l

1
q,
Rr

¨ ¨ ¨
ˆ

p
q

, Ap
such that B
P
d1 are such that Mp
p
q

q

Rd

“

8
r is a projection
ˆ
p
M
BAp
:
q and

}

r

3
ε

ˆ

d

r

ˆ

1

˙

ˆ

`
ε

2S

d1

r

ˆ

ˆ

P

˙

Proof. Observe that B
B

Rd

X

ˆ

P

P

ing number of

r s.t.
X

P
X
:, i
r
}
s}
Rd
r s.t.
ˆ

P

ď
X
r

}

:, i

i
@
2

s

P r
1

s}

ď

Rd

r. The r columns of B form an orthonormal set. Observe that
. The results of Lemma B.1 imply that the cover-

1

r

ˆ
2

i
(
@

r

s

P r

(

19

is upper bounded by

d

r

.

ˆ

3
ε

`

˘

 
 
Recall we are working under the assumption that for all columns i
S for some (known) S
have

:, i

0.

2

Mp

p
q

d1

s

P r

of Mp

p
q

“

BAp

p
q, we

}

r

s}

ď

This implies the rows of Ap
ε

covering number of the set

p
q satisfy
1
Ap
q,

}

Ap

´
Putting these together yields an upper bound of
1
q,
matrices.
of the space of

, Ap

¨ ¨ ¨

p
q

`

B, Ap
p

¨ ¨ ¨

uq

ą
i, :
P

p
q
r
, Ap

2

ď
of

s}
q

S. This in turn implies an upper bound for the
1

d1

P

r

2S
`
ε

ˆ

ˆ

.

˘

3
ε

r

d
`
ˆ

1

2S
˘
`
ε

d1

r

ˆ

ˆ

P

for the ε

covering number

´

`

˘

`

˘

B.1 Proof of Lemma 4.2

Lemma 4.2. For any δ

0, 1

the following bound holds,

λ

Appq
n

pPrP s
ÿ

›
›
› r

2

P p
1
2

F `
›
›
›

q
Σppq
n

1{2

¯

›
´
›
›
δ for all n
›
´

P

B‹Appq
´

‹ ´

Bn

Appq
n

r
N and where

r

2

F ď
¯›
›
›
›

β1
nH p

δ

q `

pPrP s
ÿ

λ

Appq
‹ }

}

2
F

b

2 log log

2
p

nHP
p

qq `

log

3

`

1
δ ` p

dr

rd1P

log

q p

5S
p

q `

`

log nHP

`

log 2RLφ

q

¯

with probability at least 1
b2
2R2 `

β1nH p
δ

LφS

`

`

1

q “
12R2
p
And b

“

q
`
´
2Rd1SLψ.

Proof. For readability, recall that

δ
β1nH p

q “

LφS

`
rd1P

1

`
dr
p

`

q p

b2
2R2 ` p
5S
log
p

q `

12R2

b

2 log log

q
`
´
log nHP

`

2
nHP
p
p
log 2RLφ

q

¯

log

3

`

qq `

1
δ `

By deﬁnition

Bn,

1
qn ,

Ap

Ap

P
qn

,

satisfy:

¨ ¨ ¨

r
p
qn }

Ap

r

2
F `

λ
}

r

p

P
ÿ
Pr

s

n1

n,h
ÿ

ă

ď

r

H }

p

ψp
qn1,h ´
r

´

ď

p

P
ÿ
Pr

s

Bn

Ap

p
qn

p

J φp

qn1,h}

2
2

r
Ap

r
λ
}

p
q
‹ }

¯
2
F `

H }

n1

n,h
ÿ

ă

ď

p

ψp
qn1,h ´
r

´

B

p
q

Ap
‹

‹

¯

p

J φp

qn1,h}

2
2.

Let’s write

p

ψp
qn1,h “
p
B
qn1,h
r
¯

´

J

B

‹
p
q

´
Ap
‹
‹

p
q

Ap
‹

J

¯
:, i
r

φp

, i
the formula above and rearranging the resulting terms,
¯

s ´

s P

´

p

J φp

qn1,h `
Rd1

Dp

p
qn1,h. The random variable Dp

1
ψ r
is conditionally zero mean. Substituting this deﬁnition in

i
qn1,hp

J K´

p
qn1,h

q “

ψp

¯

´

p

:

λ
}

p

P
ÿ
Pr

s

p

Ap

qn }

r

2
F `

H }

B

´

n1

n,h
ÿ

ă

ď

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

p

J φp

qn1,h}

2

ď

¯

λ
}

Ap

p
q
‹ }

2
F `

n1

p

P
ÿ
Pr

s

Dp

p
qn1,h,

Bn

p

Ap

qn ´

B

p
q

Ap
‹

‹

J φp

p
qn1,h

(10)

¯

F

n,h
ÿ

ă

ď

H

B

´

r

r
2

r
p
q
p
loooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon

r

I

In order to bound the last expression we make use of a covering argument.

20

s

P

P r

Let p
number to be deﬁned later. Let’s pick a ﬁxed element

and let’s focus on obtaining a high probability bound of term I
.
q
H

1
q,
¨ ¨ ¨
N and h

B, Ap
p

Let’s re-index time and instead use tuples n, h with n
with a lexicographic
ordering and use the natural task ordering provided by the task indexes and let’s deﬁne the martingale
p

. Let ε
q

0, 1

P p

be a

q

, Ap

p
p

P r

P

P

s

p

p

q

where Z p

qn,h “ x

Dp

p
qn,h,

BAp

p
q

p
q

B

Ap
‹

‹

´

J φp

qn,hy

for all

difference sequence

p

P

s

P r

. Recall that Dp

t

Z p
qn,hun,h
ď
Rd1
qn1,h P

p

H,p

P

s

Pr
and taht

´

¯

p

Dp
|

i
qn1,hp

K´

1
ψ }

Lψ

`

q| ď }

SLφ :

R.

“

n1

@

N

, h

s

H

, i

s

d1

.

s

P r

P r

P r

p

.

P

“

b for all p

2Rd1SLφ

Observe that

Z p
qn,h| ď
|
It is easy to see that E
(and for all p
P
s
P r
N and h
pairs n
n, h
p
P r
1
n1, h1
to denote the preceding point in the lexicographic order to the pair
p
also hold true for the boundary points so that

P r
0 whenever h
). For simplicity we use the notation
ı

´
to denote all the integer
in the lexicographic order. We use the notation
. This will
q

1 and E
n1, h1

p
Fn
qn,1
|
n, h

that are less than

n1, h1
p

Z p
qn,h|

0 when h

”
q ď p

Fn,h

P
´

”
H

1, H

1,H

Z p

tp

qu

“

ą

“

“

1

ı

´

q

s

s

p

1

1

n1

´
the Cauchy-Schwartz inequality implies the second moments of

q “ p

´

q
n1, 1
p

.
q

A simple application of
Z p
qn,hu

n,h

t

ď

p

h satisfy the following bound for all p

P

s

P r

E

Z p

p
qn,h

„´

2

¯

Fn,h
|

´

1

ď



R2

BAp

p
q

›
´
›
›
›

B

p
q

Ap
‹

‹

´

¯

J φp

p
qn1,h

2

›
›
›
›

(11)

1
ψ }

p

`

K´

Lψ

“ }

Where R
SLφ is the probability one upper bound on the magnitude of D deﬁned in
Lemma 3.4. We will apply the Empirical Bernstein bound of Lemma C.3 to the martingale sequence
Z p
qn,hun,h
t
Let Wn,h
P
Pr
p
of Equation 11 we conclude that

be the variance process. As a consequence

Z p
qn1,h1q
p

Varn1

n1,h1

1,h1

H,p

qďp

n,h

“

Pr

´

´

ď

P

p

p

1

.

q

s

s

ř

ř

Wn,h

i
p
q
ď

ď

E

Z p

p
qn1,h1

n1,h1
ÿp
qďp

n,h

P
p
q ÿ
Pr

s

„´

2

¯

Fn1,h1
|

1

´



n1,h1
ÿp
qďp

n,h

P
p
q ÿ
Pr

s

R2

BAp

p
q

›
´
›
›
›

B

p
q

Ap
‹

‹

´

¯

J φp

p
qn1,h

2

›
›
›
›

(12)

Where inequality
We are ready to use the bound of Lemma C.3 (with c

i
p

q

holds because the variance is upper bounded by the second moment.

By equating Sn,h

b).

Z p

“
p
qn1,h1 in the deﬁnition of the problem and using the upper bound on Wn,h
H

with probability at least 1

δ for all

0, 1

“

N

´

n, h
p

q P

ˆ r

s

P p

q

p

n,h

n1,h1

qďp

p
Pr
from Equation 12 yields that for all δ
ř
and all p

P

P

q

s

ř
.
s

P r

21

Sn,h

1.44

max
Wn,h, m
p

d

q

ď

1.4 log log

ˆ

0.41c

1.4 log log

max

2
ˆ

ˆ
2.5

ˆ
4R2

q ` p

˚

ˆ
Wn,h, m
p

`
1
4R2 max
1
Wn,h, b2
4R2 max
p
b2
Wn,h
2R2 ` p
4R2 `

2.5

i
q
p
ď
ii
p
q
ď

ď

log

`

5.2
δ

˙

2
ˆ
ˆ
Wn,h
m

max

Wn,h
m

, 1

ˆ

, 1

˙˙˙

log

`

`

0.41b

q

1.4 log log

ˆ

˙˙˙
5.2
δ

˙
2
ˆ

ˆ

max

Wn,h
m

, 1

ˆ

˙˙˙

log

`

5.2
δ

˙

2.5

q ` p

4R2

˚

0.41b

q

`

1.4 log log

ˆ

2
p

nHP
p

4R2

˚

0.41b

q

`

1.4 log log

ˆ

2
p

nHP
p

qq `

log

qq `
5.2
δ

log

˙

5.2
δ

˙

(13)

Where inequality
by setting m

holds because for all α, β

α2
ii
b. Inequality
p
2Rd1SLψ). Observe now that by Eequation 12 we have that

β2, and c

R, αβ

q
b2 (recall b

i
p

`

ď

“

P

holds

q

“

“

Wn,h
4R2 ď

1
4

n1,h1
ÿp
qďp

n,h

P
p
q ÿ
Pr

s ›
´
›
›
›

BAp

p
q

B

p
q

Ap
‹

‹

´

¯

J φp

p
qn1,h

2

.

›
›
›
›

Let ε1

ą

0 and deﬁne ε

ε1
2RLφ

.

1
q,

P

, Ap

q

q

¨ ¨ ¨

there exists an element of the cover

¯B, ¯Ap
p

1
q,

¨ ¨ ¨

, ¯Ap

P

q

q

“
B, Ap
p

Notice that for any
such that

And therefore

BAp

p
q

´

¯B ¯Ap

p
q

J φp

p
qn1,h

¯

›
›
›
›

›
´
›
›
›

ε1

ď

p

@

P

s

P r

BAp

p
q

B

p
q

Ap
‹

‹

´

J φp

p
qn1,h

¯

›
ˇ
´
ˇ
›
›
ˇ
›
ˇ

´

›
›
›
›

›
›
›
›

p
q

¯B ¯Ap
´

B

p
q

Ap
‹

‹

´

J φp

p
qn1,h

¯

›
ˇ
›
ˇ
ˇ
›
ˇ
›

ε1

ď

p

@

P

s

P r

(14)

Let ε1

1
nHP , δ

0, 1

and δ

“

P p

q
in the covering that is closest to the random point
probability at least 1
δ

ε q

“

p

p

3

dˆr

δ
1`2S
ε q

rˆd1ˆP . Denote as

Bn,

1
qn ,

Ap

¨ ¨ ¨

Bn,

1
qn ,

Ap

¨ ¨ ¨

Ap

P
qn

,

as the point

,

P
Ap
qn . We can conclude that with
s
s

s

´

r

r

r

22

p

P
ÿ
Pr

s

I

p
p

q “

i
q
p
ď

2

Dp

p
qn1,h,

Bn

p

Ap

qn ´

B

p
q

Ap
‹

‹

J φp

p
qn1,h

¯

F

r
Ap

p

qn ´

B

p
q

Ap
‹

‹

J φp

p
qn1,h

´

r
Bn

n1

n,h
ÿ

ă

ď

H

p

P
ÿ
Pr

s

B

2

Dp

p
qn1,h,

H

n,h
ÿ

ď

p

P
ÿ
Pr

s

B

´

2nHRLφP ε

`

F

2

`

›
›
›
›
qq `
2

2nHRLφP ε

`

log

5.2
δ

˙

n1

ă
1
2

ii
p
q
ď

iii
q

p

ď

n1,h1
ÿp
qďp
b2
2R2 ` p
1
2

n1,h1
ÿp
qďp
b2
2R2 ` p

¯

Bn

s
Ap

p

s
qn ´

J φp

p
qn1,h

B

p
q

Ap
‹

‹

¯

1.4 log log

2
p

nHP
p

n,H

2.5

˚

P
p
q ÿ
Pr
4R2

´

s ›
›
›
›
`

s
s
0.41b

Bn

Ap

qn ´

J φp

p
qn1,h

B

p
q

Ap
‹

‹

¯

n,H

2.5

˚

P
p
q ÿ
Pr
4R2

s ›
´
›
›
›
`

r
r
0.41b

1.4 log log

q

ˆ
p

q

ˆ
p

1
2

“

n1,h1
ÿp
qďp
b2
2R2 ` p

n,H

2.5

´

s ›
P
p
q ÿ
›
Pr
›
4R2
›
`

˚

r
r
0.41b

q

Bn

Ap

qn ´

B

p
q

Ap
‹

‹

1.4 log log

ˆ

2
p

›
›
›
›
qq `
2

log

5.2
δ

˙

1

`

`

LφS

`

log

qq `

5.2
δ

.

˙

2
p

nHP
p

J φp

p
qn1,h

¯

›
›
›
›
nHP
p

2nHRLφP ε

nHLφSP ε1

`

`

`

Bn and

holds because

i
Inequality
q
p
over the l2 norms of collections of matrices of the form
1. Inequality
the ε
ε1

ii
p
iii
cover. Inequality
´
p
1
ε1
nHP and ε
(recall b
2RLφ

norm
B, A1,
and holds with probability
¨ ¨ ¨
p
holds as a consequence of Equation 13 and an application of the union bound over
s
is a consequence of Equation 14. Where the last equality holds because

Bn are ε close in the norm resulting of computing the l

2Rd1SLψ).

, AP

r

8

q

q

q

“

“

Notice that δ

3

ε q
p
2S
log
`
ε
p
`
1
P
B, Ap
q,
q
p
q
above yields that with probability at least 1

q ˚ p
¨ ¨ ¨

“
dr
, Ap

δ
1`2S
ε

dˆr

rˆd1ˆP ď
q

q `
p
rd1P
. Having applied the union bound over the ε cover over tuples
q
in the previous discussion and plugging in the deﬁnition of δ in the display

dˆr`rˆd1ˆP and therefore log
q

3`2S
ε

1
p

log

ď

˘

`

δ

{

p

δ

1
δ

“

3

δ

´

I

p
p

q ď

1
2

p

P
ÿ
Pr

s

n,H

n1,h1
ÿp
qďp
4R2
2.5
p
dr
p

`

rd1P

˚

P
p
q ÿ
Pr

s ›
´
›
›
›
0.41b
q
´
3
`
p

log

q p

`

2

J φp

p
qn1,h

`

›
›
›
›
log 5.2

LφS

1

`

`

b2
2R2 `

log

`

1
δ `

B

p
q

Ap
‹

‹

¯

Bn

Ap

p

qn ´

r
r
1.4 log log

nHP
p

2
p
log nHP

`

qq `
log 2RLφ

2S

q `

q

¯

Let the radius β1nH p
δ
LφS
δ
β1nH p

q “

1

q

be deﬁned as,

b2
2R2 ` p
3
log
p

2.5

4R2

0.41b

1.4 log log

2

nHP
p

log 5.2

log

1
δ `

˚

`
rd1P

`
dr
p
2Rd1SLψ. Combining this upper bound with Equation 10 we obtain that with probability
δ:

q
´
log 2RLφ

`
log nHP

qq `

q `

2S

q p

`

`

`

`

¯

q

p

Where b
at least 1

“
´

λ
}

p

P
ÿ
Pr

s

p

Ap

qn }

r

2
F `

1
2

H }

B

´

n1

n,h
ÿ

ă

ď

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

r

r

23

¯

p

J φp

qn1,h}

2

δ
β1nH p

ď

q `

λ
}

Ap

p
q
‹ }

2
F

r

p

P
ÿ
Pr

s

By deﬁnition

B

´

H ›
›
›
›

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

J φp

p
qn1,h

¯

r

r

2
1
{

B

Σp

p
qn

¯

´

2

›
›
›
›

“

´

›
›
›
›

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

r

r

2

.

F
¯›
›
›
›

n1

n,h
ÿ

ă

ď

And therefore

p

P
ÿ
Pr

s

λ

Ap

p
qn

›
›
› r

›
›
›

2

F `

1
2

2
1
{

Σp

p
qn

¯

´

›
´
›
›
›

B

Ap

p
q
‹ ´

‹

Bn

Ap

p
qn

r

r

2

F ď
¯›
›
›
›

β1nH p
δ

q `

p
q

Ap
‹

λ

›
›
›

2

F
›
›
›

p

P
ÿ
Pr

s

We can derive a version of Lemma A.3 adapted to the Shared structure setting in this Section.

B.2 Proof of Lemma 4.5

Lemma 4.5. If Assumption 3.3 holds and E 1 is true then for h

p

Qp

qn,hp

p
qn,h, ap
sp

p

qn,hq ´

p

p
qn,h, ap
sp
p

qn,hq `

Pp

p
q

r

´

p

P
ÿ
Pr

s

p¨|

ď

2CψH

γn

d

δ
p

q

p

P
ÿ
Pr

s

Proof. If Assumption 3.3 holds,

H

,

P r

s
p
qn,h, ap
sp

p

qn,hq

JV p

p
qn,h

`

2

p

φp
qn,h}

}

Σppq
n

´

¯

1

¯

´1

p

Qp

qn,hp

p
qn,h, ap
sp

p

qn,hq ´

p

p
qn,h, ap
sp
p

qn,hq `

Pp

p
q

sn,h, an,h

p¨|

p
qn,h

JV p
q

`

1

r

´

p

P
ÿ
Pr

s

¯
J V p

p
qn,h

Ψp

p
q

φp

p
qn,h

J

p

Mp

qn ´

p
q

Mp
‹

p

P
ÿ
Pr

´

s ›
›
›
›
Cψ

´

Ď

¯
p
qn,h

V p

2 ›
¯›
´
›
›
›
›
›
›
p
Mp
qn ´

1

`

´

8 ›
›
›
›
J

›
›
›
p
qn,h

φp

φp

p
qn,h

J

¯

´

p

Mp

qn ´

Ď
Mp
‹

p
q

¯

p
q

Mp
‹

2

2
¯›
›
›
›

`

1

2

›
›
›
›

ď

i
p
q
ď

ii
p
q
ď

iii
q

p

ď

p

P
ÿ
Pr

s

CψH

›
›
›

p

P
ÿ
Pr

CψH

¯›
›
›
›
p
qn

¯
p
qn,h

φp

´

Ď
J

´

¯

´

s ›
´
›
›
›
s ˆ›
›
›
›

p

P
ÿ
Pr

p

Mp

qn ´

Mp

Ď

Ă

φp

p
qn,h

J

´

¯

´

2 `
¯›
›
›
›

›
›
›
›

p

Mp

qn ´

Ă

p
q

Mp
‹

2˙
¯›
›
›
›

i
Inequality
p
bounded by H. Finally
the right hand side of the last inequality,

iii
p

q

q

follows by Assumption 3.2, and inequality

1 is
is a consequence of the triangle inequality. We focus now on bunding

holds because the range of Vn,h

ii
p

`

q

φp

p
qn,h

J

p

Mp

qn ´

Mp

p
qn

φp

p
qn,h

J

p

Mp

qn ´

p
q

Mp
‹

p

P
ÿ
Pr

´

s ˆ›
›
›
›

¯

´

p

P
ÿ
Pr

s

p

Ď
φp
qn,h}

}

Ă
Σppq
n

´

¯

2 `
¯›
›
›
›

´

›
›
›
›
p
Σp
qn

´1

ˆ›
´
›
›
›

¯
2
1
{

¯

´

24

´

Mp

Ă
p
qn ´

Mp

p
qn

Ď

Ă

ď

2
1
{

Σp

p
qn

2˙

¯›
›
›
›
F `

¯›
›
›
›

›
´
›
›
›

¯

´

Ă

p

Mp

qn ´

p
q

Mp
‹

F ˙

¯›
›
›
›

Let’s bound each of the two summands.

φppq
n,h}

}

´1

Σppq
n

´

¯

pPrP s
ÿ

1{2

Σppq
n

¯

´

´

›
›
›
›

Mppq

n ´

Mppq
n

Ď

Ă

¯›
›
›
›

F ď

pPP }

d ÿ

φppq
n,h}

2

´1

Σppq
n

´

¯

ď

pPP }

d ÿ

ď

pPP }

d ÿ

φppq
n,h}

2
Σppq
n

´1

´

¯

φppq
n,h}

2
Σppq
n

´1

´

¯

a

1{2

Σppq
n

Mppq
n

¯

1{2

´

Ď
Mppq
n

Σppq
n

Mppq
n

Ă
Mppq
n

´

´

¯

´

Ď

Ă

2

F
¯›
›
›
2
›

F
¯›
›
›
›

g
f
f
e

pPrP s ›
´
ÿ
›
›
›

g
f
f
e

pPrP s ›
´
ÿ
›
›
›
q

γn

δ

p

The ﬁrst inequality holds by Holder and the last inequality holds because
UF
n . Similarly,

Mp

1
qn ,

¨ ¨ ¨

Mp

P
qn

,

is in

r

pPrP s
ÿ

φppq
n,h}

}

Σppq
n

´1

}

´

¯

1{2

Σppq
n

´

¯

´

Mppq

n ´

Mppq
‹

Ă

F

}

ď

¯

d ÿ
pPrP s

ď

ď

pPrP s

d ÿ

d ÿ
pPrP s

Ď

´1

¯

g
f
f
e

pPrP s ›
´
ÿ
›
›
›

φppq
n,h}

2
Σppq
n

}

´

2

φppq
n,h}

}

´1

Σppq
n

´

¯

φppq
n,h}

2
Σppq
n

}

´1

g
f
f
e

pPrP s ›
´
ÿ
›
›
›
q

γn

δ

p

´

¯

a

Ď

1{2

¯

´

1{2

Σppq
n

Σppq
n

Mppq
n

Ă
Mppq
n

¯

´

Ă

Mppq
‹

Mppq
‹

´

´

2

F
¯›
›
›
2
›

F
¯›
›
›
›

The ﬁrst inequality holds by Holder and the last inequality holds because with high probability
Mp
‹

, Mp
‹

is in

1
q

P

,

q

UF
δ
n p

.
q

¨ ¨ ¨

The result follows.

r

B.3 Proof of Theorem 4.7

Theorem 4.7. The regret of Shared-MatrixRL satisﬁes,

RP

N H
p

q ď

H

N HP log

d

6 log N H
δ

ˆ

`

˙

4CψH 2dP

L2
φ
?λ ¸

1
˜

`

log

1

˜

`

N HL2
φ
λd ¸

δ
p

N HP d
q

1

`

˜

L2
φ
?λ ¸

log

1
˜

`

N HL2
φ
λd ¸

.

2CψH

γN

g
f
f
e

γN

δ
p

q`

a

With probability at least 1

2δ.

´

Proof. Recall the shared regret equals RP
optimism property of Lemma 4.4 implies that,

N H
p

q “

N
n

“

1

ř

ř

25

V πppq

‹

1

p
sp
qn,1
p

q ´

V πppq

n

1

p
sp
qn,1
p

. The
q

s

p

P

Pr

V p

p
qn,1

p
sp
qn,1
p

q ´

V πppq

n

1

p
sp
qn,1
p

q

RP

N H
p

q ď

i
p
q
ď

N

1
n
ÿ
“
N

p

P
ÿ
Pr

s

2CψH

1
n
ÿ
“
N

´

2CψH

γn

δ
p

q

a

d ÿ
P
p
Pr

s

2

p

φp
qn,h}

}

Σppq
n

´1

`

´

¯

p

P
ÿ
Pr

s

Pp

p
q

p
qn,h, ap
sp

p

J

qn,hq

p¨|

V πppq

n

2

V p

p
qn,2

´

´

¯ ¯

2

p

φp
qn,h}

}

Σppq
n

´1

`

“

1
n
ÿ
“

γn

δ
p

a
p
qn,h, ap
sp

q
d ÿ
P
p
Pr
V p

J

s

p
qn,2

p

qn,hq

p¨|

´
Pp

p
q

¯

´

V πppq

n

2

´

Vn,2

p
sp
qn,2
p

q ´

V πppq

n

2

p
sp
qn,2
p

´

`

q
¯

p

P
ÿ
Pr

s

p

P
ÿ
Pr

s

´

¯
δppq
n,2
loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon
Vn,2

V πppq

´

2

n

p
sp
qn,2
p

q ´

p
sp
qn,2
p

q
¯

Where

i
p

q

is a consequence of Lemma 4.5. A recursive use of this decomposition yields,

RP

N H
p

q ď

N

H

1
n
ÿ
“
N

1
h
ÿ
“
H

2CψH

γn

δ
p

a

q
d ÿ
P
p
Pr

s

2

p

φp
qn,h}

}

Σppq
n

´1

`

´

¯

ď

1
n
ÿ
“

1
h
ÿ
“

2CψH

γN

δ
p

a

2

p

φp
qn,h}

}

Σppq
n

´1

`

´

¯

q
d ÿ
P
p
Pr

s

I

p
δp
qn,h

p
δp
qn,h

p

P
ÿ
Pr

s

p

P
ÿ
Pr

s

Where the last inequality holds because γn is increasing in n.

loooooooooooooooooooooooooooomoooooooooooooooooooooooooooon

The sum
p

δp
qn,h| ď
|

N
n

1

“

p

P

Pr

s

“

H
h

1 δp

p
qn,h can easily be bounded by invoking Lemma C.1 by observing that

4H for all n, h, p. Thus, with probability at least 1
ř

ř

ř

δ for all n,

´

N

H

1
n
ÿ
“

1
h
ÿ
“

p

P
ÿ
Pr

s

p

δp
qn,h ď

8H

N HP log

d

6 log N H
δ

.
˙

ˆ

By Corollary 4.6,

N

H

1
n
ÿ
“

h
ÿ
“

1 d ÿ

Pr

p

P

2

p

φp
qn,h}

}

Σppq
n

´1

ď

s

´

¯

N

H

2

1
n
ÿ
“

1
h
ÿ
“

d ÿ
P
p
Pr

s

2

p

φp
qn,h}

}

Σppq
n,h

´1

`

2LφHdP
?λ

log

1

˜

`

N HL2
φ
λd ¸

.

´

¯

By Cauchy-Schwartz,

N

H

1
n
ÿ
“

h
ÿ
“

1 d ÿ

Pr

p

P

2

p

φp
qn,h}

}

´1

Σppq
n,h

s

´

¯

ď g
f
f
e

N

H

N H

1
n
ÿ
“

1
h
ÿ
“

p

P
ÿ
Pr

s

2

p

φp
qn,h}

}

´1

Σppq
n,h

´

¯

By Lemma 3.5 (setting b

Lφ
?λ

),

“

N

H

1
n
ÿ
“

1
h
ÿ
“

p

P
ÿ
Pr

s

2

p

φp
qn,h}

}

2
Σ´1

n,h ď

P

1

˜

`

L2
φ
?λ ¸

d log

1

˜

N HL2
φ
λd ¸

`

26

Therefore

N

H

1
n
ÿ
“

h
ÿ
“

1 d ÿ

Pr

p

P

2

p

φp
qn,h}

}

´1

Σppq
n

s

´

¯

N HP

ď g
f
f
e
2LφHdP
?λ

L2
φ
?λ ¸

1

˜

`

d log

1

˜

N HL2
φ
λd ¸`

`

log

1
˜

`

N HL2
φ
λd ¸

.

We then conclude that with probability at least 1

δ, whenever E 1 holds,

´

RP

N H
p

q ď

H

N HP log

d

6 log N H
δ

ˆ

`

˙

N HP

L2
φ
?λ ¸

1

˜

`

d log

1

˜

N HL2
φ
λd ¸`

`

2CψH

γN

δ
p

a
4CψLφH 2dP
?λ

qg
f
f
e

γN

a

δ
p

q

log

1

˜

`

N HL2
φ
λd ¸

.

C Supporting Technical Lemmas

Lemma C.1. Let
0, 1
let δ

8t
“
. Then with probability 1

1 be a martingale difference sequence with
δ for all N

Xt

N

t

u

Xn
|

| ď

ζ and E

Xn

r

s “

0, and

P p

s

´

P

Xn

2ζ

ď

N log

d

6 log N
δ

.
˙

ˆ

N

1
t
ÿ
“

Proof. Observe that |
[15, Equation (11)] we ﬁnd that

xn
|ζ ď

1. By invoking a time-uniform Hoeffding-style concentration inequality

P

n

P

«@

N :

N

1
t
ÿ
“

Xn
ζ ď

1.7

N

d

log log

ˆ

T
p

q `

0.72 log

5.2
δ

ˆ

δ.

1

´

˙˙ﬀ ě

Rounding up the constants for the sake of simplicity we get

P

t

P

«@

N :

which establishes our claim.

T

1
t
ÿ
“

Xt

2ζ

T

d

ď

log

ˆ

ˆ

6 log
δ

T
p

q

δ,

1

´

˙˙ﬀ ě

It can be shown that with high probability and for all t simultaneously all
for that we will make use of the following Theorem,
xi
Theorem C.2. [Theorem 1 in [1]] Let
u
be response random variables satisfying yℓ
subgaussian. Let
and ηℓ is R
t
responses
´
i
“

`
θt be the ridge regression estimator of θ

1
1, and regularizer λ

Rm with
xℓ

xi
}
ηℓ where

0. For all t

1
Ă
θJ
‹

} ď
θ
}

8i
“
“

´
yi
u

} ď

N,

P

t

t

‹

Mn lie in a vicinity of M,

L for all i
Ă

n

P r
S such that E

. And let
ηℓ
Fℓ
|
r
with covariates

s

yi
t
1
´
xi
t

1
8i
u
“
0
s “
1
t
1,
´
i
u
“

p

ą
(cid:13)
ˆθt
(cid:13)
(cid:13)

θ‹

´

(cid:13)
(cid:13)
(cid:13)Vt ď

βt

δ
p

q

with probability at least 1

T
t
“

1 xixJi .

ř

δ. Where βt

δ
p

q “

´

a

R

m log

c

´

1

`

tL2
δ

λ
{

`

¯

?λS and Vt

λI

`

“

27

Lemma C.3 (Uniform empirical Bernstein bound). In the terminology of Howard et al. [16], let
t
0 and variance process Wt. Then with
St
i
“
probability at least 1

1 Yi be a sub-ψP process with parameter c

δ for all t

ą

“

N

ř

´

P

1.44

St

ď

max

d

Wt, m
p

q

1.4 log log

ˆ

2
ˆ

`

0.41c

1.4 log log

ˆ

2
ˆ

max

ˆ

ˆ

where m

ą

0 is arbitrary but ﬁxed.

max

Wt
m

ˆ

, 1

, 1

`

˙˙˙

ˆ
Wt
m

˙˙˙
5.2
δ

log

log

`

5.2
δ

˙

˙

The following lemma relates the values of quadratic forms of Positive Semi-deﬁnite Matrices with
the ratio of their determinants.
Lemma C.4. If B ľ C ą 0 be d

d dimensional matrices then,

ˆ

xJBx
xJCx ď

sup
0
x

‰

B
det
p
C
det
p

q
q

.

C´

2y. Then
1
{

Proof. Given any y

Rd let x

P

“

xJBx
xJCx “

sup
0
y

‰

sup
0
x

‰

yJC´

2y
1
{

2BC´
1
{
kyk2
2

“

(cid:13)
(cid:13)
(cid:13)C´

2BC´
1
{

2(cid:13)
1
(cid:13)
{
(cid:13)op

by the deﬁnition of the operator norm. Recall that by assumption B
C´

I ľ 0, and hence all the eigenvalues of C´

2BC´
1
{

2BC´
1
{

2
1
{

2 are at least 1. Thus
1
{

´

C ľ 0 therefore

´

xJBx
xJCx ď

(cid:13)
(cid:13)
(cid:13)C´

2BC´
1
{

2(cid:13)
1
(cid:13)
{
(cid:13)op ď

C´
det
p

2BC´
1
{

2
1
{

sup
0
x

‰

where the last equality follows
2
1
{

2BC´
1
{

C´
det
p

. This completes the proof.
q

since

det
B
q
p
det
C
q “
p

C´
det
p

2
1
{

q

2
1
{

C´
p

q “

q “

,

B
det
p
C
det
p
B
det
p

q

q
q
det

28

