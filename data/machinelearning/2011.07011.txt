1
2
0
2

b
e
F
9
1

]

Y
S
.
s
s
e
e
[

2
v
1
1
0
7
0
.
1
1
0
2
:
v
i
X
r
a

Imposing Robust Structured Control Constraint on
Reinforcement Learning of Linear Quadratic Regulator

Sayak Mukherjee, Thanh Long Vu ∗

Abstract

This paper discusses learning a structured feedback control to obtain suﬃcient robustness
to exogenous inputs for linear dynamic systems with unknown state matrix. The structural
constraint on the controller is necessary for many cyber-physical systems, and our approach
presents a design for any generic structure, paving the way for distributed learning control. The
ideas from reinforcement learning (RL) in conjunction with control-theoretic suﬃcient stability
and performance guarantees are used to develop the methodology. First, a model-based frame-
work is formulated using dynamic programming to embed the structural constraint in the linear
quadratic regulator (LQR) setting along with suﬃcient robustness conditions. Thereafter, we
translate these conditions to a data-driven learning-based framework - robust structured rein-
forcement learning (RSRL) that enjoys the control-theoretic guarantees on stability and con-
vergence. We validate our theoretical results with a simulation on a multi-agent network with
6 agents.

Keywords: Reinforcement Learning, Structured Learning, Distributed Control, Stability Guaran-
tee, Robust Learning, Linear Quadratic Regulator.

1

Introduction

Automatic control researchers over the last few decades heavily investigated control designs for
large-scale and interconnected systems. Distributed control solutions are much more feasible and
computationally cheap instead of performing full-scale centralized control designs for such scenarios.
In many cyber-physical systems the communication infrastructure that helps to implement feedback
controllers may have their own limitations. Many of such computational and practical bottlenecks
have triggered a ﬂourish in the research of distributed and structure control designs. Optimal
control laws that can capture decentralized [1] or a more generic distributed [6] structure has been
investigated under notions of quadratic invariance (QI) [20,27], structured linear matrix inequalities
(LMIs) [19, 21], sparsity promoting optimal control [8] etc. [11, 12] discussed a structural feedback
gain computation for discrete-time systems with known dynamics.

However, the above research works have been conducted with the assumption that the dynamic
model of the system is known to the designer. In practice, accurate dynamic model of large-scale
systems may not always be known, for example, the U.S. Eastern Interconnection transmission
grid model consists of 70, 000 buses making tractable model-based control designs much diﬃcult.
Moreover, the dynamic system may also be coupled with unmodeled dynamics from coupled pro-
cesses, parameter drift issues etc. which motivated research works toward learning control gains
from system trajectory measurements. In recent times, ideas from machine learning and control

∗S. Mukherjee and T. L. Vu are with the Optimization and Control Group, Paciﬁc Northwest National Laboratory

(PNNL), Richland, WA, USA. Emails: (sayak.mukherjee, thanhlong.vu)@pnnl.gov.

1

 
 
 
 
 
 
theory are uniﬁed under the notions of reinforcement learning (RL) [29]. In the RL framework, the
controller tried to optimize its policies based on the interaction with the system or environment to
achieve higher rewards. Traditionally RL techniques have been developed for the Markov Decision
Process (MDP) [2, 26, 33] based framework, and has since been the driving force to develop lots of
advanced algorithmic sequential decision making solutions. On a slightly diﬀerent path, to incorpo-
rate much more rigorous and control theoretic guarantees in the context of dynamic system control
where systems are modeled by diﬀerential equations, research works such as [14, 17, 22, 25, 30, 31]
brought together the the good from the worlds of optimal and adaptive control along with the ma-
chine learning ingredients, sometimes under the notions of adaptive dynamic programming (ADP).
More variants of data-driven control research such as data-dependent linear matrix inequalities [4],
predictive data-driven control [3], analyzing sample complexity [5], analysis on the characteris-
tics of direct and indirect data-driven control [7] to name a few, have been reported. Learning
control designs have been reported for systems with partially or completely model free designs.
Works such as [15, 16, 23, 32] present robust ADP/RL designs for dynamic system control. In this
article, we concentrate on the learning control designs from the dynamic system viewpoint, and
present a novel structured optimal control learning solution along with incorporating robustness
for continuous-time linear systems with unknown state matrix.

Although reference [34] present a survey on multi-agent and distributed RL solutions in the
context of the MDPs, the area of distributed and structured learning control designs from the
dynamic system viewpoint is still less explored. In [25], a projection based reduced-dimensional RL
variant have been proposed for singularly perturbed systems, [13] presents a decentralized design
for a class of interconnected systems, [10] presents a structured design for discrete-time time-
varying systems, [9] presents a distributed learning for sparse linear systems in the recent times.
Continuing in these lines of research, this article presents a robust structured optimal control
learning methodology using ideas from ADP/RL. We have extended and connected some classical
results that provide a bridge along our path to eventually formulating the novel RL solutions for
the robust structured design, which is the core contribution of this article.

We ﬁrst consider the dynamic system without any exogenous perturbations, and formulate a
model-based structural optimal control solution for continuous-time LTI systems using dynamic
programming. Thereafter, we perform the robustness analysis in presence of exogenous inputs and
provide guarantees with suﬃcient stability conditions. Subsequently, these model-based formula-
tions are translated to a data-driven gain computation framework - RSRL that can encapsulate
the structural and the robustness constraints along with enjoying the stability, convergence, and
sub-optimality guarantees. It is worthy to note that the “robustness” of this structural feedback
control to the exogenous inputs is showed in twofold. First, if the exogenous inputs are entirely
unknown, then the closed-loop system is input-to-state stable to the exogenous inputs. Second, if
the exogenous inputs are bounded and intermittently measurable, i.e., the exogenous inputs mea-
surement is available at-least for some disjoint intervals, then the closed-loop system is globally
asymptotically stable. This implies that the intermittently measurable exogenous inputs are fully
compensated to compute the structured feedback controls. We validate our design on a multi-agent
dynamic network with 6 agents.

2 Model and Problem formulation

We consider a perturbed linear time-varying (LTV) continuous-time dynamic system as

˙x = Ax + B(u + ζ(x, t)), x(0) = x0,

(1)

2

where x ∈ Rn, u ∈ Rm are the states and control inputs. The perturbation is caused due to the
inﬂuence of the exogenous input ζ(x, t) ∈ Rm. The function ζ(x, t) represents a functional coupling
of the dynamic system with some extraneous processes which can inﬂuence the dynamic system.
We consider the input matrix B for the control and the exogenous input to be same, i.e., we use
matched input ports. We, hereby, make the following set of assumptions.
Assumption 1: The dynamic state matrix A is unknown, and the input matrix B is assumed to
be known.

Thereafter, we characterize the nature of the coupling variable ζ(x, t) with the following as-

sumption.
Assumption 2: The exogenous input measurements are available at-least for some disjoint inter-
i=0[ti, ti + δt] (cid:84)[0, t] where t is the current time and δt is a small time increment [15], and
vals (cid:83)∞
satisfy the boundedness property given as:

(cid:107)ζ(x, t)(cid:107)2 ≤ α(cid:107)x(cid:107)2, α > 0.

(2)

In our numerical example, we considered ζ(x, t) to be of structure l(t)x(t) with (cid:107)l(t)(cid:107) ≤ α. With
this unknown state matrix, and the presence of the bounded disturbance as given in Assumption
2, we would like to learn an optimal feedback gain u = −Kx. However, instead of unrestricted
control gain K ∈ Rm×n, we impose some structure on the gain. We would like to have K ∈ K,
which we call structural constraint, where K is the set of all structured controllers such that:
K := {K ∈ Rm×n | F (K) = 0}.

(3)

Here F (.) is the matricial function that can capture any structure in the control gain matrix. This
will able to implement non-zero control communication links in the K matrix. We now make the
following assumption on the control gain structure K.
Assumption 3: The communication structure required to implement the feedback control is known
to the designer, and is sparse in nature.

This assumption means that the structure K is known. This captures the limitations in the
feedback communication infrastructure. For many network physical systems, the communication
infrastructure can be already existing, for example, in some peer-to-peer architecture, agents can
only receive feedback from their neighbors. Another very commonly designed control structure is of
block-decentralized nature where local states are used to perform feedback control. Therefore, our
general constraint set will encompass all such scenarios. We also make the standard stabilizability
assumption.
Assumption 4: The pair (A, B) is stabilizable and (A, Q1/2) is observable, where Q (cid:31) 0 is
responsible to penalize the states as described in the problem statement.

We can now state the problem statement as follows.

P. For system (1) satisfying Assumptions 1, 2, 3, 4, learn robust structured control policies u =
−Kx, where K belongs to the class of structural control K described by (3), so that the closed-loop
system is stable and the following objective is minimized

J(x(0), uK) =

(cid:90) ∞

0

(xT Qx + uT Ru)dτ.

(4)

3 Structured Reinforcement Learning and Robustness

To develop the learning control design we will take the following route. First, we consider an
unperturbed dynamic system, i.e.,
˙x = Ax + Bu, and then formulate a framework where we can
impose the structural constraint. Thereafter, we will consider the perturbation caused due to
ζ(x, t), and then reﬁne the framework to add robustness guarantee.

3

3.1 Structural Constraint on the Feedback Control Gain

We start with the unperturbed dynamic system:

˙x = Ax + Bu, x(0) = x0,

(5)

and we want to design the control u = −Kx such that K ∈ K. We will ﬁrst formulate a model-
based solution of the optimal control problem via a modiﬁed Algebraic Riccati Equation (ARE)
in continuous-time that can solve the structured optimal control when all the state matrices are
known. The scenario with structural constraint without any exogenous inputs has been presented
in our recent work [24], in this article we present the detail formulation for better readability. We
now present a very important model-based theorem as follows.
Theorem 1: For any matrix L ∈ Rm×n, let P (cid:31) 0 be the solution of the following modiﬁed Riccati
equation

AT P + P A − P BR−1BT P + Q + LT RL = 0.

Then, the control gain

K = K(L) = R−1BT P − L,

(6)

(7)

will ensure closed-loop stability of (5), i.e., A − BK ∈ RH∞ without any external perturbation. (cid:3)
Proof: See Appendix.

At this point, we investigate closely the matricial structure constraint. Let IK denotes the
indicator matrix for the structured matrix set K where this matrix contains element 1 whenever
the corresponding element in K is non-zero. The structural constraint is simply written as:

F (K) = K ◦ I c

K = 0.

(8)

Here, ◦ denotes the element-wise/ Hadamard product, and I c
K is the complement of IK. We,
hereafter, state the following theorem on the choice of L to impose structure on K. This follows
the similar form of discrete-time condition of [11, 12].
Theorem 2: Let L = F (φ(P )) where φ(P ) = R−1BT P. Then, the control gain K = K(L) designed
as in Theorem 1 will satisfy the structural constraint F (K) = 0.
Proof : We have,

K = φ(P ) − L,

= φ(P ) − F (φ(P )),
= φ(P ) − φ(P ) ◦ I c
K,
= φ(P ) ◦ (1m×n − I c
= φ(P ) ◦ IK ∈ K.

K),

(9)

(10)

(11)

(12)

(13)

(cid:3)
This concludes the proof.
The implicit assumption here is the existence of the solution of the modiﬁed ARE (6). Once such
solution exists, we will eventually learn the structured control gains. The next theorem describes
the sub-optimality of the feasible structured solution with respect to the unconstrained objective.
Theorem 3: The diﬀerence between the feasible optimal structured control objective value J and
the optimal unstructured objective ¯J is bounded as:

(cid:107)J − ¯J(cid:107) ≤

l
2g

(cid:107)(xT

0 ⊗ xT

0 )(cid:107),

4

(14)

for any feasible control structure if
V : Rn → Rn is a operator deﬁned as:

√

lg(cid:15) < l/2. Here g = (cid:107)BR−1BT (cid:107)2, l = (cid:107)V −1(cid:107)−1, where

VW = (A − BR−1BT )T W + W (A − BR−1BT ).

(15)

(cid:3)
Proof: Let the unstructured solution of the ARE be denoted as ¯P , then the unstructured objective
value is ¯J = xT
¯P x0, whereas, the learned structured control will result into the objective J =
0
xT
0 P x0, therefore we have,

(cid:107)J − ¯J(cid:107) = (cid:107)(xT
≤ (cid:107)(xT

0 ⊗ xT
0 ⊗ xT

0 )vec(P − ¯P )(cid:107)
0 )(cid:107)(cid:107)(P − ¯P )(cid:107)F

Following [28][Theorem 3] we use g, and l as deﬁned in the Theorem 3. With (cid:15) = (cid:107)LT RL(cid:107)
have if

lg(cid:15) < l/2,

√

l

(cid:107)(P − ¯P )(cid:107)F ≤

=

2l(cid:15)(l − (cid:112)l2 − 4lg(cid:15))
4lg(cid:15)

=

2l(cid:15)
l + (cid:112)l2 − 4lg(cid:15)
(l − (cid:112)l2 − 4lg(cid:15))
2g

<

l
2g

.

As such, the diﬀerence between the optimal values J and ¯J is bounded by

(cid:107)J − ¯J(cid:107) ≤

l
2g

(cid:107)(xT

0 ⊗ xT

0 )(cid:107)

(16)

, we will

(17)

for any structure of the control. We note that g and l are not dependent on the control structure.
Therefore, the inequality (17) indicates that the diﬀerence between the optimal control value J
with K ∈ K, and optimal unstructured control value ¯J is linearly bounded by the initial value
(cid:3)
(cid:107)(xT

0 )(cid:107) for any control structure.

0 ⊗ xT

3.2 Robustness

To this end, we have formulated a modiﬁed algebraic Riccati equation that can restrict the optimal
control solutions to the structural constraint. We will now investigate the robustness guarantees
and necessary modiﬁcations for the system (1) with ζ(x(t), t).
Theorem 4: For any bounded exogenous input ζ(t), the structural control u = −Kx where K ∈ K
computed following Theorems 1 and 2 will result into the system (1) to be input-to-state stable
(ISS) with respect to ζ(t).
Proof: The closed-loop dynamics with control u = −Kx, K ∈ K is given by:

Therefore, we have

˙x = (A − BK)x + Bζ(t), x(t0) = x0.

x(t) = e(A−BK)(t−t0)x0 +

(cid:90) t

t0

e(A−BK)(t−τ )Bζ(τ )dτ

(18)

(19)

As A − BK is stable we have, (cid:107)e(A−BK)(t−t0)(cid:107) ≤ ke−λ(t−t0), k > 0, λ > 0. Subsequently, we can
bound (cid:107)x(t)(cid:107) as,

(cid:107)x(t)(cid:107) ≤ ke−λ(t−t0)(cid:107)x(t0)(cid:107) +

k(cid:107)B(cid:107)
λ

supτ ∈[t0,t](cid:107)ζ(τ )(cid:107).

(20)

5

Therefore, with the global asymptotic stability of the structured control, we can conclude as long
(cid:3)
as (cid:107)ζ(t)(cid:107) is bounded, the closed-loop is ISS with respect to ζ(t).
If the exogenous inputs ζ(x, t) are intermittently measurable as in Assumption 2, then they can
be entirely compensated by the structural feedback control to ensure the global asymptotic stability
of the closed-loop system, as in the following theorem which will be necessary when developing the
data-driven algorithm. Please note that although the ISS condition in Theorem 4 can be ensured
when bounded ζ(t) measurements are not available, as the control learning will be dependent on
the state trajectories that are perturbed by the exogenous inputs, we will require the measurements
of the exogenous inputs.
Theorem 5: With assumption 2, Let P (cid:31) 0 be the solution of the following Riccati equation

(A + βI)T P + P (A + βI) − P BR−1BT P + Q + LT RL = 0, β > 0,

(21)

α2λ2

max(R)
λmin(R)

+

then the control u = −R−1BT P x+Lx will ensure closed-loop stability of (1) with Q (cid:23) (

2αd)I, d ≥ max((cid:107)L(cid:107)) and R (cid:31) 0, where λmax(R) and λmin(R) are the maximum and minimum
(cid:3)
eigenvalues of R.
Proof: See Appendix.
We note that the implicit assumption here is the existence of the solution of the modiﬁed ARE
(6) where L = F (φ(P )) and φ(P ) = R−1BT P. It is still an open question on the necessary and
suﬃcient condition on the structure F (K) for the existence of this solution. However, once the
solution exists, we can iteratively compute it and the associated control gain K using the algorithm
formulated in the next section.

4 Reinforcement Learning Algorithm

The gain can be iteratively computed using the following model-based algorithm as follows.
Theorem 6: Let K0 be such that A − BK0 is Hurwitz. Then, for k = 0, 1, . . .
1. Solve for Pk(Policy Evaluation) :

AT

k Pk + PkAk + KT

k RKk + Q + 2βPk = 0, Ak = A − BKk.

2. Update the control gain (Policy update):

Kk+1 = R−1BT Pk − F (φ(Pk)), φ(Pk) = R−1BT Pk.

(22)

(23)

Then A − BK is Hurwitz and Kk ∈ K and Pk converge to structured K ∈ K, and P as k → ∞. (cid:3)
Proof: The theorem is formulated by taking motivation from the Kleinman’s algorithm [18] that
has been used for unstructured feedback gain computations. Comparing with the Kleinman’s
algorithm, the control gain update step is now modiﬁed to impose the structure. With some
mathematical manipulations it can be shown that using Kk = BT Pk − Fk, where Fk = BT Pk ◦ I c
K,
we have,

(A − BKk)T Pk + Pk(A − BKk) + KT
AT Pk + PkA − PkBR−1BT Pk + F T

k RKk =

k RFk = −Q − 2βPk.

(24)

(25)

Therefore, (22) is equivalent to (21) for the kth iteration. As we shown the stability and convergence
via dynamic programming and Lyapunov analysis in Theorem 1, considering the equivalence of
(cid:3)
Theorem 1 with this iterative version, the theorem can be proved.

6

Although, we have formulated an iterative solution to compute structured feedback gains, the
algorithm is still model-dependent. We, hereby, start to move into a state matrix agnostic design
using reinforcement learning. We write (1) incorporating u = −Kkx, Kk ∈ K as

˙x = Ax + Bu + Bζ = (A − BKk)x + B(Kkx + u) + Bζ.

(26)

We explore the system by injecting a probing signal u = u0 such that the system states do not
become unbounded [16]. For example, following [16] one can design u0 as a sum of sinusoids.
Thereafter, we consider a quadratic Lyapunov function xT P x, P (cid:31) 0, and we can take the time-
derivative along the state trajectories, and use Theorem 6 to alleviate the dependency on the state
matrix A.

d
dt

(xT Pkx) = xT (AT

k Pk + PkAk)x+

2(Kkx + u0)T BT Pkx + 2(ζ T BT Pk)x,

= −xT ( ¯Qk + 2βPk)x + 2(Kkx + u0)T R(Ki(k+1) + Fk)x + 2(ζ T BT Pk)x,

where, ¯Qk = Q + KT

k RKk. Therefore, starting with an arbitrary control policy u0 we have,

(t+T )Pkx(t+T ) − xT
xT
(cid:90) t+T

t Pkxt +

=

(−xT ¯Qkx + 2(ζ T BT Pk)x)dτ.

(cid:90) t+T

t

xT
i 2βPkxidτ − 2

(cid:90) t+T

t

((Kkx + u0)T (Kk+1 + Fk)x)dτ

(27)

(28)

t

We, thereafter, solve (28) by formulating an iterative algorithm using the measurements of the
state and control trajectories. The algorithm basically formulates an iterative least-squares prob-
lem to solve for the structured gain. The design will require to gather data matrices D =
{Sxx, Txx, Txu0, Txζ} for suﬃcient number of time samples (discussed shortly) where ⊗ denotes
the Kronecker product and a|t2
t1

= a(t2) − a(t1) as follows:

,

· · ·

, x ⊗ x|tl+T

(cid:105)T

,

Sxx =

Txx =

Txu0 =

Txζ =

(cid:104)

t1

x ⊗ x|t1+T
(cid:104)(cid:82) t1+T
t1
(cid:104)(cid:82) t1+T
t1
(cid:104)(cid:82) t1+T
t1

(x ⊗ x)dτ,

(x ⊗ u0)dτ,

· · ·

· · ·

(x ⊗ ζ)dτ,

· · ·

tl
, (cid:82) tl+T
tl
, (cid:82) tl+T
tl
, (cid:82) tl+T
tl

(cid:105)T

(x ⊗ x)dτ

(x ⊗ u0)dτ

(x ⊗ ζ)dτ

(cid:105)T

.

,

(cid:105)T

,

(29)

(30)

(31)

(32)

Algorithm 1 presents the steps to compute the structured feedback gain K ∈ K without knowing

the state matrix A.
Remark 1: If A is Hurwitz, then the controller update iteration in (33) can be started without
any stabilizing initial control. Otherwise, stabilizing K0 is required, as commonly encountered in
the RL literature [16]. This is mainly due to its equivalence with modiﬁed Kleinman’s algorithm
in Theorem 6.
Remark 2: The rank condition dictates the amount of data sample needs to be gathered. For
this algorithm we need rank(Txx Txu0 Txζ) = n(n + 1)/2 + |K| + nm, where |K| is the number
of non-zero elements in the structured feedback control matrices. This is based on the number of
unknown variables in the least squares. The number of data samples can be considered to be twice
this number to guarantee convergence.

7

Algorithm 1 Robust Structured Reinforcement Learning (RSRL) Control
1. Gather suﬃcient data: Store data (x, ζ and u0) for interval (t1, t2, · · · , tl), ti − ti−1 = Tstep. Then construct the
following data matrices D = {Sxx, Txx, Txu0 , Txζ} such that rank(Txx Txu0 Txζ) = n(n + 1)/2 + |K| + nm. Select

+ 2αd)I, where d ≥ max((cid:107)F (cid:107)) is estimated from the allowable implementation gains.

Q (cid:23) (

α2λ2

max(R)

λmin(R)

2. Controller update iteration : Starting with a stabilizing K0, Compute K iteratively (k = 0, 1, · · · ) using the
following iterative equation
for k = 0, 1, 2, ..

A. Solve for Pk, and Kk+1 + Fk:

(cid:2)Sxx + 2βTxx −2Txx(In ⊗ K T
k R) − 2Txu0 (In ⊗ R) −2Txζ
(cid:124)
(cid:123)(cid:122)
Θk

(cid:3)

(cid:125)





vec(Pk)
vec(K(k+1) + Fk)
vec(BT Pk)

B. Compute Fk = R−1BT Pk ◦ I c
C. Update the gain Kk+1.
D. Terminate the loop when ||Pk − Pk−1|| < ς, ς > 0 is a small threshold.
endfor

K using the feedback structure matrix.

3. Applying K on the system : Finally, apply u = −Kx, K ∈ K, and remove u0.


 = −Txxvec( ¯Qk)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Φk

.

(33)

The condition on Q in the Theorem 5 ensures robustness of the design. Although, the designer
can estimate max((cid:107)L(cid:107)), during the initial iterations, the numerical values may not satisfy the
condition on Q, and therefore, the designer may need to re-tune the parameter β.
Theorem 7: Performing Algorithm 1 using x(t), u(t), and ζ(t) will recover the structured K ∈ K,
and P corresponding to Theorem 6 for (1).
Proof: Performing Algorithm 1 using x(t), u(t), and ζ(t) is equivalently solving the trajectory
relationship (28). As (28) has been constructed using Theorem 6, then any solution from Theorem
6 will satisfy the kth iteration of the following equation:



Θk



vec(Pk)
vec(K(k+1))
vec(BT Pk)



 = Φk.

(34)

Hence, any solution of Theorem 6 is also a solution of (34). When the condition rank(Txx Txu0 Txζ) =
n(n+1)/2+|K|+nm is satisﬁed, Θk will have full column rank. As such, equation (34) has a unique
solution [vec(Pk), vec(K(k+1)), vec(BT Pk)]T , from which we have a unique solution Pik, Ki(k+1).
Since this solution is unique, it is also the solution Pk, K(k+1) of theorem 6. Considering this equiv-
alence of the Algorithm 1 with the modiﬁed Kleinman update in Theorem 6, we can conclude that
(cid:3)
the structured K ∈ K, and P can be recovered using Algorithm 1.

8

5 Numerical Example

Figure 1: An example of structured feedback for agent 1 with exogeneous inputs

We consider a multi-agent network with 6 agents following the interaction structure shown in Fig.
1. We consider each agent to follow a consensus dynamics with its neighbors such that:

(cid:88)

˙xi =

j∈Ni,i(cid:54)=j

αij(xj − xi) + ui + ζi(x, t), xi(0) = xi0,

(35)

where αij > 0 are the coupling coeﬃcients. We consider the state and input matrix to be:

A =









−5
2
2 −6
3
0
0
0

3
0
0 −5
0
1
3

0
0
2
2 −2
0
0

0
1
0
0
0 −4
0

0
3
0
0
3
3 −6









, B = I6.

(36)

We thereafter consider the exogeneous input perturbation to be ζi(x, t) = −0.3 · sin(t)xi, with the
estimate of α to be 0.5. We also set β = 1, and d = 2.4 which make sure that the condition
on the Q can be satisﬁed. We consider Q = (3 + α2 + 2α · d)I6. The dynamics given as above
is generally referred to as a Laplacian dynamics with A.1n = 0 resulting into a zero eigenvalue.
We would like the controller to improve the damping of the eigenvalues closer to instability. The
eigenvalues of the system are −10.00, −8.27, −6.00, −3.00, −0.72, 0.00. We choose initial conditions
as [0.3, 0.5, 0.4, 0.8, 0.9, 0.6]T . We consider an arbitrary sparsity pattern, with the assumption that
the states of all the agents can be measured. We experiment with the following structure constraint:
IK(1, 2) = 0, IK(1, 6) = 0, IK(2, 4) = 0, IK(2, 6) = 0, IK(3, 4) = 0, IK(3, 5) = 0, IK(4, 1) = 0, IK(4, 2) =
0, IK(5, 4) = 0, IK(5, 3) = 0, IK(6, 4) = 0, IK(6, 1) = 0.

Here we have n = 6, m = 6, and number of non-zero elements of K is 24, therefore, we require
to gather data for at-least 2(n(n + 1)/2 + |K| + nm) samples, which is 162 data samples. We use
the time step to be 0.01s with an exploration of 2s. The iteration for K and P took around 0.03
s on an average in Matlab19a with a Macbook laptop of Catalina OS, 2.8 GHz Quad-Core Intel
Core i7 with 16 GB RAM. During exploration, we have used sum of sinusoids based perturbation
signal to persistently excite the network. Please note that the majority of the learning is spent
on the exploration of the system because of the requirement of persistent excitation and the least
square iteration is a order of magnitude smaller in comparison to the exploration time. With
faster processing units, the least square iteration can be made much faster. Fig. 2 shows the state

9

123456Physical couplingFeedbackcommunicationExogenouscouplingFigure 2: State trajectories during exploration (till 2 s) and control implementation

Figure 3: P convergence

Figure 4: K convergence

trajectories of the agents during exploration, and also with control implementation phase. The
structured control gain learned in this scenario is given as:

KK =











0.0000
1.7997
1.6876
0.2667
0.4171
0.0409
0.0000 −0.0000
0.2021
0.0170
0.4038
0.0000

0.0000
0.0170
0.4171
0.0449
0.0000
0.0409 −0.0000
0.2021
0.0029
1.8017 −0.0000 −0.0000
2.2384 −0.0001 −0.0001
0.3222
0.4402
1.9267
0.0000
0.0000
1.7330
0.4402
0.0029 −0.0000











(37)

The total cost comes out to be 1.0742 units. Fig. 3-4 show that the P and K iteration
converges after around 5 iterations (using Frobenius norm). The solution also closely matches with
the model-based computation. On the other hand, the unstructured optimal control solution is
given as:

Kunstruc =











1.6629
0.2407
0.3768
0.0424
0.0163
0.0379

0.2407
1.5507
0.0379
0.0021
0.1839
0.3617

0.3768
0.0379
1.6659
0.2922
0.0009 −0.0001
0.0032 −0.0002

0.0379
0.0163
0.0424
0.3617
0.1839
0.0021
0.2922
0.0032
0.0009
2.0406 −0.0001 −0.0002
0.3975
1.7785
1.5768
0.3975











,

(38)

with the optimal cost 1.068 units. With the structured solution KK, the damping of the closed-loop
poles is improved as they are placed at −16.6912, −15.1893, −14.0494, −13.2785, −12.2264, −12.5005.
This example brings out various intricacies of the algorithm and validating our theoretical results.

10

6 Conclusions

This paper presented a reinforcement learning-based robust optimal control design for linear sys-
tems with unknown state dynamics when the control is subjected to a structural constraint. We
ﬁrst formulate an extended algebraic Riccati equation (ARE) from the model-based analysis en-
compassing dynamic programming and robustness analysis with suﬃcient stability and convergence
guarantees. Subsequently, an policy iteration based RL algorithm is formulated using the previous
model-based results that continue to enjoy the rigorous guarantees and can compute the structured
sub-optimal gains using the trajectory measurements of states, controls, and exogenous inputs. The
sub-optimality of the learned structured gain is also quantiﬁed by comparing it with the uncon-
strained optimal solutions. Simulations on a multi-agent network with constrained communication
infrastructure along with the exogenous inﬂuences at each agent substantiate our theoretical and al-
gorithmic formulations. Future research will look into investigating the feasibility and methodology
of robust design variants when the measurements of exogenous inputs are not available by exploit-
ing some underlying knowledge about the exogenous input and its corresponding gain through the
dynamic system.

References

[1] L. Bakule. Decentralized control: an overview. Annual reviews in control, 32:87–98, 2008.

[2] D. P. Bertsekas. Dynamic Programming and Optimal Control: Approximate Dynamic Pro-

gramming, 4th ed. Athena Scientiﬁc, Belmont, MA, USA., 2012.

[3] Jeremy Coulson, John Lygeros, and Florian D¨orﬂer. Data-enabled predictive control: In the
In 2019 18th European Control Conference (ECC), pages 307–312.

shallows of the deepc.
IEEE, 2019.

[4] Claudio De Persis and Pietro Tesi. Formulas for data-driven control: Stabilization, optimality,

and robustness. IEEE Transactions on Automatic Control, 65(3):909–924, 2019.

[5] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample
complexity of the linear quadratic regulator. Foundations of Computational Mathematics,
pages 1–47, 2019.

[6] F. Deroo. Control of interconnected systems with distributed model knowledge. PhD thesis,

TU Munchen, Germany, 2016.

[7] Florian D¨orﬂer, Jeremy Coulson, and Ivan Markovsky. Bridging direct & indirect data-driven
control formulations via regularizations and relaxations. arXiv preprint arXiv:2101.01273.

[8] M. Fardad, F. Lin, and M. R. Jovanovi´c. Sparsity-promoting optimal control for a class of
distributed systems. In Proceedings of the 2011 American Control Conference, pages 2050–
2055, 2011.

[9] Salar Fattahi, Nikolai Matni, and Somayeh Sojoudi. Eﬃcient learning of distributed linear-
quadratic control policies. SIAM Journal on Control and Optimization, 58(5):2927–2951, 2020.

[10] L. Furieri, Y. Zheng, and M. Kamgarpour. Learning the globally optimal distributed lq regu-

lator. arXiv:1912.08774v3, 2020.

11

[11] J. C. Geromel. Structural constrained controllers for linear discrete dynamic systems. IFAC

Proceedings Volumes, 17(2):435 – 440, 1984. 9th IFAC World Congress.

[12] J.C. Geromel, A. Yamakami, and V.A. Armentano. Structural constrained controllers for
discrete-time linear systems. Journal of optimization theory and applications, 61(1):73–94,
1989.

[13] Y. Jiang and Z. Jiang. Robust adaptive dynamic programming for large-scale systems with
an application to multimachine power systems. IEEE Transactions on Circuits and Systems
II: Express Briefs, 59(10):693–697, Oct 2012.

[14] Y. Jiang and Z.-P. Jiang. Computational adaptive optimal control for continuous-time linear

systems with completely unknown dynamics. Automatica, 48:2699–2704, 2012.

[15] Y. Jiang and Z. P. Jiang. Robust adaptive dynamic programming. In F. Lewis and D. Liu, ed-
itors, Reinforcement Learning and Approximate Dynamic Programming for Feedback Control.
IEEE Press, 2013.

[16] Y. Jiang and Z.-P. Jiang. Robust Adaptive Dynamic Programming. Wiley-IEEE press, 2017.

[17] B. Kiumarsi, K.G. Vamvoudakis, H. Modares, and F.L. Lewis. Optimal and autonomous
control using reinforcement learning: A survey. IEEE Trans. on Neural Networks and Learning
Systems, 2018.

[18] D. Kleinman. On an iterative technique for riccati equation computations. IEEE Trans. on

Automatic Control, 13(1):114–115, 1968.

[19] C. Langbort, R. S. Chandra, and R. D’Andrea. Distributed control design for systems intercon-
nected over an arbitrary graph. IEEE Transactions on Automatic Control, 49(9):1502–1519,
2004.

[20] L. Lessard and S. Lall. Quadratic invariance is necessary and suﬃcient for convexity.

In

Proceedings of the 2011 American Control Conference, pages 5360–5362, 2011.

[21] P. Massioni and M. Verhaegen. Distributed control for identical dynamically coupled systems:

A decomposition approach. IEEE Transactions on Automatic Control, 54(1):124–135, 2009.

[22] S. Mukherjee, H. Bai, and A. Chakrabortty. On model-free reinforcement learning of reduced-
order optimal control for singularly perturbed systems. In IEEE Conference on Decision and
Conrol 2018, Miami, FL, USA.

[23] S. Mukherjee, H. Bai, and A. Chakrabortty. On robust model-free reduced-dimensional re-
In 2020 American Control

inforcement learning control for singularly perturbed systems.
Conference (ACC), pages 3914–3919, 2020.

[24] S. Mukherjee and T.L. Vu. Reinforcement learning of structured control for linear systems

with unknown state matrix. arXiv preprint arXiv:2011.01128, 2020.

[25] Sayak Mukherjee, He Bai, and Aranya Chakrabortty. Reduced-dimensional reinforcement

learning control using singular perturbation approximations. Automatica, 126:109451, 2021.

[26] W.B. Powell. Approximate dynamic programming. Wiley, 2007.

12

[27] M. Rotkowitz and S. Lall. A characterization of convex problems in decentralized controlast.

IEEE Transactions on Automatic Control, 51(2):274–286, 2006.

[28] J. Sun. Perturbation theory for algebraic riccati equations. SIAM Journal on Matrix Analysis

and Applications, 19:39–65, 1998.

[29] R.S. Sutton and A.G. Barto. Reinforcement learning - An introduction. MIT press, Cambridge,

1998, 1998.

[30] K.G. Vamvoudakis. Q-learning for continuous-time linear systems: A model-free inﬁnite hori-

zon optimal control approach. Systems and Control Letters, 100:14–20, 2017.

[31] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F.L. Lewis. Adaptive optimal control for
continuous-time linear systems based on policy iteration. Automatica, 45:477–484, 2009.

[32] Ding Wang, Haibo He, and Derong Liu. Adaptive critic nonlinear robust control: A survey.

IEEE transactions on cybernetics, 47(10):3429–3451, 2017.

[33] C. Watkins. Learning from delayed systems. PhD thesis, King’s college of Cambridge, 1989.

[34] K. Zhang, Z. Yang, and T. Ba¸sar. Multi-agent reinforcement learning: a selective overview of

theories and algorithms. arXiv 1911.10635, 2019.

7 A1. Proof of Theorem 1

We look into the optimal control solution of the unperturbed dynamic system (5) with the objective
(2) using dynamic programming (DP) such that we can ensure theoretical guarantees. We assume
at time t, the state is at x = x1. We deﬁne the ﬁnite time optimal value function with the
unconstrained control as:

Vt(x1) = minu

(cid:90) T

t

(xT Qx + uT Ru)dτ,

(39)

with x(t) = x1, ˙x = Ax + Bu. Staring from state x1, the optimal Vt(x1) gives the minimum
LQR cost-to-go. Now as the value function is quadratic, we can write it in a quadratic form as,
Vt(x1) = xT
1 Ptx1, Pt (cid:31) 0. We, next, look into a small time interval [t, t + h], where h > 0 is small,
and in this small time interval we assume that the control is constant at u = u1 and is optimal.
Then cost incurred over the interval [t, t + h] is

U1 =

(cid:90) t+h

t

(xT Qx + uT Ru)dτ ≈ h(xT

1 Qx1 + uT

1 Ru1).

Also, the control input evolves the states at time t + h to,

x(t + h) = x1 + h(Ax1 + Bu1).

Then, the minimum cost-to-go from x(t + h) is:

Vt+h(x(t + h)) = (x1 + h(Ax1 + Bu1))T Pt+h(x1

+ h(Ax1 + Bu1)).

13

(40)

(41)

(42)

Expanding Pt+h as (Pt + h ˙Pt) we have,

Vt+h(x(t + h)) =
(x1 + h(Ax1 + Bu1))T (Pt + h ˙Pt)(x1 + h(Ax1 + Bu1)),
≈ xT
1 Pt(Ax1 + Bu1)
+ xT
1

1 Ptx1 + h((Ax1 + Bu1)T Ptx1 + xT
˙Ptx1).

Therefore, the total cost U ,

Vt(x1) = U = U1 + Vt+h(x(t + h)),
1 Qx1 + uT
1 Ptx1 + h(xT
1 Pt(Ax1 + Bu1) + xT
1

= xT
+ xT

1 Ru1 + (Ax1 + Bu1)T Ptx1
˙Ptx1).

(43)

(44)

(45)

(46)

If the control u = u1 is optimal then the total cost must be minimized. Minimizing Vt(x1) over u1
we have,

uT
1 R + xT
1 PtB = 0,
u1 = −R−1BT Ptx1.

(47)

(48)

Now this gives us an optimal gain K = R−1BT Pt which solves the unconstrained LQR. However,
we are not interested in the unconstrained optimal gain, as that cannot impose any structure per
se. In order to impose structure in the feedback gains, the feedback control will have to deviate
from the optimal solution of R−1BT Pt, and following [11], we introduce another matrix L ∈ Rm×n
such that,

K + L = R−1BT Pt,
K = R−1BT Pt − L.

(49)

(50)

The matrix L will help us to impose the structure, i.e., K ∈ K, which we will discuss later.
Therefore, the structured implemented control is given by,

u1 = −Kx1 = −R−1BT Ptx1 + Lx1.

(51)

We have u1 ∈ uK, where uK is the set of all control inputs when following K ∈ K. Now, with slight
abuse of notation, we denote the matrix Pt to be the solution corresponding to the structured
optimal control. The Hamilton-Jacobi equation with the structured control is given by,

V K∈K
(x1) ≈ minu1∈uKU,
t
1 Ptx1 ≈ minu1∈uK(xT
xT

1 Ptx1 + h(xT
+ (Ax1 + Bu1)T Ptx1 + xT
xT
1

˙Ptx1)).

1 Qx1 + uT
1 Pt(Ax1 + Bu1)+

1 Ru1

Putting (51), neglecting higher order terms, and after simplifying we get,

− ˙Pt = AT Pt + PtA − PtBR−1BT Pt + Q + LT RL.

For steady-state solution, we will have,

AT P + P A − P BR−1BT P + Q + LT RL = 0.

14

(52)

(53)

(54)

(55)

This proves the modiﬁed Riccati equation of the theorem. Now let us look into the stability of the
closed-loop system with the gain K = R−1BT P − L. We can consider the Lyapunov function:

Therefore, the time derivative along the closed-loop trajectory of (5) is given as,

W = xT P x, P (cid:31) 0.

˙W = xT P ˙x + ˙xT P x,

= xT P (Ax + Bu) + (Ax + Bu)T P x,
= xT P (Ax + B(−R−1BT P x + Lx)) + .
(Ax + B(−R−1BT P x + Lx))T P x,
= xT [P A + AT P − P BR−1BT P − P BR−1BT P

+ P BL + LT BT P ]x,

= xT [−Q − P BR−1BT P + 2P BL − LT RL]x,
= xT [−Q − (P BR−1 − LT )R(P BR−1 − LT )T ]x.

(56)

(57)

(58)

(59)

(60)

(61)

(62)

Now as R is positive deﬁnite, the terms of form X T RX are at-least positive semi-deﬁnite. Therefore,
we have,

˙W ≤ −xT Qx.

(63)

This ensures the stability of the closed-loop system (5). Since the linear system (5) is autonomous
and (A, Q1/2) is observable, the globally asymptotic stability can be proved by using the LaSalle’s
(cid:3)
invariance principle. This completes the proof.

8 A2. Proof of Theorem 5

We start by considering the quadratic Lyapunov function W = xT P x, P (cid:31) 0. The closed-loop
dynamics is now given by,

˙x = (A − B(R−1BT P − L)x) + Bζ(x, t).

(64)

Using Assumption 2, the derivative of the Lyapunov function along the closed-loop trajectories will
satisfy

˙W = xT P (Ax + B(−R−1BT P x + Lx)) + .

(Ax + B(−R−1BT P x + Lx))T P x + 2xT P Bζ(x, t),
= xT [P A + AT P − P BR−1BT P − P BR−1BT P

+ P BL + LT BT P ]x + 2xT P Bζ(x, t),

= xT [−Q − P BR−1BT P + 2P BL − LT RL − 2βP ]x + 2xT P Bζ(x, t),
≤ xT [−Q − P BR−1BT P + 2P BL − LT RL − 2βP ]x + 2α(cid:107)BT P x(cid:107)(cid:107)x(cid:107),

(65)

(66)

(67)

(68)

(69)

Denote p = R−1BT P x − Lx, then, BT P x = R(p + Lx). Hence, 2α(cid:107)BT P x(cid:107)(cid:107)x(cid:107) = 2α(cid:107)R(p +

Lx)(cid:107)(cid:107)x(cid:107) ≤ 2αλmax(R)((cid:107)p(cid:107) + (cid:107)Lx(cid:107))(cid:107)x(cid:107). On the other hand

pT Rp = xT P BR−1BT P x − xT P BLx − xT LT BT P x + xT LT RLx

= xT P BR−1BT P x − 2xT P BLx + xT LT RLx.

(70)

(71)

15

Continuing with the computation of

˙W we have

˙W ≤ xT [−Q − 2βP ]x − pT Rp + 2αλmax(R)(cid:107)p(cid:107)(cid:107)x(cid:107) + 2αλmax(R)(cid:107)L(cid:107)(cid:107)x(cid:107)2,

≤ −xT (Q − 2αλmax(R)(cid:107)L(cid:107)I)x − λmin(R)(cid:107)p(cid:107)2 + 2αλmax(R)(cid:107)p(cid:107)(cid:107)x(cid:107) − 2βλmin(P )(cid:107)x(cid:107)2

(72)

(73)

Using Q − 2α(cid:107)L(cid:107)I (cid:23)

α2λ2

max(R)
λmin(R)

I we have,

˙W ≤ −λmin(R)(

αλmax(R)
λmin(R)

(cid:107)x(cid:107) − (cid:107)p(cid:107))2 − 2βλmin(P )(cid:107)x(cid:107)2 ≤ −2βλmin(P )(cid:107)x(cid:107)2 ≤ 0

(74)

Using Barbalat’s lemma, it is straightforward to show that the time-varying closed-loop system (1)
is globally exponentially stable at the origin.

For implementation, as we know the structure of the feedback control, and it is practical to
assume that we know the upper limits of the designable gains, we can set d ≥ max((cid:107)L(cid:107)), and select

Q (cid:23) (

α2λ2

max(R)
λmin(R)

+ 2αd)I. This completes the proof.

(cid:3)

16

