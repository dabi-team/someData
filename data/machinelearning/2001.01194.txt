0
2
0
2

v
o
N
0
3

]
T
S
.
h
t
a
m

[

3
v
4
9
1
1
0
.
1
0
0
2
:
v
i
X
r
a

CUTOFF FOR EXACT RECOVERY OF GAUSSIAN MIXTURE MODELS

XIAOHUI CHEN AND YUN YANG

Abstract. We determine the information-theoretic cutoﬀ value on separation of cluster
centers for exact recovery of cluster labels in a K-component Gaussian mixture model with
equal cluster sizes. Moreover, we show that a semideﬁnite programming (SDP) relaxation
of the K-means clustering method achieves such sharp threshold for exact recovery without
assuming the symmetry of cluster centers.

1. Introduction
Let X1, . . . , Xn be a sequence of independent random vectors in Rp sampled from a K-
component Gaussian mixture model with K 6 n. Speciﬁcally, we assume that there exists a
partition G∗1, . . . , G∗K of the index set [n] :=

}
N (0, σ2Ip),
(1)
Rp are the unknown cluster centers and σ2 > 0 is the common noise
where µ1, . . . , µK ∈
variance. For simplicity, we assume that σ2 is known. Our main focus of this paper is
to investigate the problem of optimal exact recovery for the true partition (or clustering)
structure G∗1, . . . , G∗K .

1, . . . , n
{
εi

Xi = µk + εi,

such that if i

G∗k, then

i.i.d.
∼

∈

For each partition G1, . . . , GK of [n], let H = (hik)

matrix of the observation Xi to the cluster k, i.e.,

n
0, 1
}

×

∈ {

K be the binary assignment

hik =

(cid:26)

if i

1,
Gk,
0, otherwise,

∈

for i

[n], k

[K].

∈

∈

Since each row of H contains exactly one nonzero entry, there is one-to-one mapping (up to
assignment labeling) between the partition and the assignment matrix. Thus recovery of the
true clustering structure is equivalently to recovery of the associated assignment matrix.

Given the data matrix Xp

n = (X1, . . . , Xn), the optimal estimator that maximizes the
probability of recovering the clustering labels correctly is the maximum a posteriori (MAP)
estimator. If the label assignment is uniformly random, then the MAP estimator is equivalent
to the maximum likelihood estimator (MLE), where the log-likelihood function is given by

×

ℓ(H, µ1, . . . , µK) =

np
2

−

log(2πσ2)

1
2σ2

−

Then the MLE corresponds to the solution of

n

K

n

K

Xi=1

Xk=1

hikk

Xi −

2
µkk
2.

Xi −
subject to the constraint that H is an assignment matrix.

min
H, µ1,...,µK

hikk

Xi=1

Xk=1

2
2

µkk

(2)

Date: First arXiv version: January 5, 2020. This version: December 1, 2020.
Key words and phrases. K-means, Gaussian mixture models, semideﬁnite relaxation, exact recovery, sharp

threshold, optimality.

1

 
 
 
 
 
 
2

XIAOHUI CHEN AND YUN YANG

Since we focus on the recovery of the true clustering structure G∗1, . . . , G∗K , we may ﬁrst

proﬁle the “nuisance parameters” µ1, . . . , µK, whose MLEs are given by

ˆµk =

n
i=1 hikXi
n
i=1 hik

1

=

Xi,

Gk| Xi
n
i=1 hik denotes the cardinality of the k-th cluster. Substituting ˆµk into (2),
where
we see that the MLE for H (and thus for G1, . . . , GK ) is the solution of the constrained
combinatorial optimization problem:

Gk|
|

P

P

P

Gk

=

∈

|

max
G1,...,GK

K

1

Gk| Xi,j

∈

Gk

|

Xk=1

Xi, Xji
h

K

subject to

Gk = [n],

(3)

Gk=1

where

denotes the disjoint union.

⊔

It is now clear that, under the Gaussian mixture model, the MLE in (3) is equivalent to
the classical K-means clustering method [45], which minimizes the total intra-cluster squared
Euclidean distances. Since the K-means clustering problem is known to be worst-case NP-
hard [16, 46], one can expect that a polynomial-time algorithm for computing the MLE of
the clustering structure with exact solutions only exists in certain cases. Because of this
computational barrier of the original K-means problem, various computationally tractable
approximation algorithms are proposed in literature.

A widely used algorithm for solving the K-means is Lloyd’s algorithm [43], which is an it-
erative algorithm that sequentially reﬁnes the partition structure to ensure that the K-means
objective function is monotonically decreasing. Lloyd’s algorithm has a similar nature as the
classical expectation-maximization (EM) algorithm [18] in that, while the EM implicitly per-
forms soft clustering at every E-step, Lloyd’s algorithm does hard clustering at each iteration
via the Voronoi diagram.

Given a suitable initialization (such as the spectral clustering method [34]), it is shown
in [44] that the clustering error for Lloyd’s algorithm converges to zero exponentially fast,
provided that

where n = mink

[K] |
∈

G∗k|

∆2 := min

2 > Cσ2 Kn
µk −
n
is the minimal cluster size and a

=l6K k

µlk

16k

2

1

Kp
n
(cid:16)
b = max(a, b).

∨

(cid:17)

,

∨

Separation lower bound in (4) is not sharp (in the high-dimensional setting when p

In the simplest symmetric two-component Gaussian mixture model:

Xi = µηi + εi,

(4)

n).

≫

where ηi = 1, i
algorithm that achieves the sharp threshold on
which is given by

G∗1 and ηi =

1, i

−

∈

∈

G∗2, [52] proposes a simple iterative thresholding
2
2 for exact recovery with high probability,
k

µ
k

σ2

1 +

1 +

(cid:18)

r

2p
n log n

(cid:19)

log n.

(5)

It should be noted that the algorithm in [52] critically depends on the symmetry of the
Gaussian centers (i.e., µ and
µ) and it is structurally diﬃcult to extend such algorithm
with maintained statistical optimality to a general K-component Gaussian mixture model
without assuming the centers are equally spaced.

−

Another active line of research focuses on various convex relaxed versions of the K-means
problem that is solvable in polynomial-time [57, 49, 42, 23, 59, 27, 12]. The best known
rate of convergence achieved by the semideﬁnite programming (SDP) relaxed K-means for

6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

3

the Gaussian mixture model (1) is given by [27]. Speciﬁcally, it is shown therein that mis-
classiﬁcation errors of the SDP originally proposed in [57] for relaxing the K-means has the
SNR2), where the signal-to-noise ratio is deﬁned as
exponential rate of convergence exp(
∆2
σ2 ∧

n
n
and a
In particular, the exponential rate implies that exact recovery is
achieved by the SDP relaxed K-means with high probability in the equal cluster size case
n = n/K if minimal separation of cluster centers satisﬁes the lower bound

n∆4
pσ4 > c

·
−
SNR2 =

b = min(a, b).

(6)

C

∧

∆2 > Cσ2

1

Kp
n log n !

∨ s

log n.

(7)

Now comparing (7) with the optimal exact results (5) in the special symmetric two-

component Gaussian mixture model, it is natural to ask the following question:

does the SDP relaxed K-means clustering method achieve a sharp threshold for
exact recovery of the general K-component Gaussian mixture model?

To the best knowledge of ours, this is an open question in literature. In this paper, we
provide an aﬃrmative answer to this question: we show that there is an SDP relaxation of
the K-means clustering method (given in (11) below) achieving the exact recovery with high
probability if ∆2 > (1 + α)∆

, where

2

2

∆

= 4σ2

1 +

1 +

s

Kp
n log n !

log n.

(8)

2

−

In addition, if ∆2 6 (1
, then the probability of exact recovery for any estimator vanishes
α)∆
to zero under the equal cluster size scenario. Thus ∆
yields the information-theoretic cutoﬀ
value on the minimal separation of cluster centers for exact recovery of the K-component
Gaussian mixture model, and the SDP relaxation for the K-means is minimax-optimal in the
sense that sharp phase transition of the probability of wrong recovery from zero to one occurs
2
at the critical threshold given by the ∆

2

.

1.1. Related work. There is a vast literature studying the clustering problem on the Gauss-
ian mixture model, or more generally ﬁnite mixture models. Regarding clustering labels
as missing data, parameter estimation is often carried out by the EM algorithm [18, 24].
The EM algorithm has been extensively studied in the statistics and machine learning lit-
erature [13, 9, 69, 35, 70, 17, 19, 68, 20]. Optimal rate of convergence for estimating the
mixing distribution in ﬁnite mixture models is derived in [13]. Consistency of the K-means
estimation of the clustering centers is studied in [45, 58], without concerning the compu-
tational complexity. Computationally eﬃcient algorithms for solving the K-means include
Lloyd’s algorithm [43, 44] and convex relaxations [57, 49, 42, 23, 59, 27, 12, 6]. Other popular
clustering methods include the spectral clustering [48, 54, 61, 1, 37, 7, 64, 65] and variants of
the K-means [5, 55, 53, 15, 14]. Analysis under the mixture models has also been done under
other clustering models such as the stochastic ball models [53, 6, 23].

Parallel to the (mixture) model-based clustering framework, there are many similar methods
and algorithms proposed for community detection in network data based on the stochastic
block model (SBM) [32, 21]. Successful algorithms for community detection, partial and exact
recovery under the SBM have been extensively studied in literature – these include spectral
algorithms [36, 47, 39], SDP relaxations [22, 4, 29, 30, 28, 10, 41], among others [51, 50].

 
 
4

XIAOHUI CHEN AND YUN YANG

1.2. Notation. Let 1n be the n
size, let
A, B
h
ﬁx the notation nk =
cluster size.

1 vector of all ones. For two matrices A and B of the same
= tr(AT B) be the usual inner product. Throughout the rest of the paper, we
[K] nk as the minimal
∈

, and n = mink

, m = min16k

2nknl
nk+nl

G∗k|
|

=l6K

×

i

n

o

In this section, we state our main result on the information-theoretic cutoﬀ value of the

exact recovery of the Gaussian mixture model in (1).

2. Main result

Theorem 2.1 (Separation upper bound for exact recovery via SDP relaxation). If there exist
constants δ > 0 and β

(0, 1) such that

and

with

∈
(1

log n >

β)2

−
β2

C1n
m

,

δ 6

β2

β)2

(1

−

C2
K

,

m >

4(1 + δ)2
δ2

,

∆2 >

4σ2(1 + 2δ)

β)2

(1

−

1 +

1 +

s

β)2
(1
−
(1 + δ)

p
m log n

+ C3Rn

log n

!

Rn =

(1

β)2

−
(1 + δ) log n

√p log n
n

+

log n
n

(cid:18)

,

(cid:19)

then the SDP in (11) achieves exact recovery with probability at least 1
Ci, i = 1, 2, 3, 4, are universal constants.

C4 K 2 n−

δ, where

−

The following corollary is a direct consequence (and a special case) of Theorem 2.1 when

the cluster sizes are equal.

2
2
2, and ∆

−

µlk

µk −

C2(log n)−

=l6K k
2

To derive a lower bound, we focus on the equal size case where clusters

Corollary 2.2. Let α > 0, ∆2 = min16k
be deﬁned in (8). Suppose
that the cluster sizes are equal and K 6 C1 log(n)/ log log(n) for some small constant C1 > 0
depending only on α. If ∆2 > (1 + α)∆
, then the SDP in (11) achieves exact recovery with
probability at least 1

c3, where C2, c3 are constants depending only on α.
K
k=1 have
G∗k}
{
roughly the same sizes. More precisely, recall that our unknown parameters are the cluster
indicating variables H =
are the unknown cluster
K log(n)/n for some suﬃciently large constant C > 0. Here, we consider
sizes. Let δn = C
[K] that allows a small ﬂuctuation on the community
nk ∈
size in establishing the lower bound. Particularly, we deﬁne the (localized) parameter space
as

δn) n/K, (1+δn) n/K] for k

, and
[K]
}

hik : i
{

nk : k
{

[K]
}

[n], k

[(1

p

−

∈

∈

∈

∈

Θ

n, K, ∆

=

(cid:0)

(cid:1)

n

nk :=

,
hik}
{

µk}
{
(cid:1)
δn)

−

n
K

(1

n(cid:0)
hik ∈

: hik ∈ {

, µk ∈
0, 1
}

Rp,

K

hik = 1,

Xk=1
µlk

, (1 + δn)

,

µk −
k

> ∆,

i

∀

∈

[n] and

(k, l)

∀

∈

[K]2, k

= l

.

n
K

Xi=1

i
Theorem 2.3 (Separation lower bound for exact recovery: equal cluster size case). Let α
(0, 1). If ∆2 6 (1

α)∆

h

2

o

∈

−

inf
ˆhik}
{

(H,µ)

∈

and K 6 log n, then we have
ˆhik 6

P(H,µ)

= hik, i

∈

sup
Θ(n,K,∆)

[n], k

(cid:0)

[K]

> 1

∈

cKn−

1,

−

(cid:1)

6
 
6
6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

5

for

.
hik}
{

where c > 0 is a constant depending only on α and the inﬁmum is over all possible estimators
ˆhik}
{
Corollary 2.2 and Theorem 2.3 together imply that in the equal cluster size case when
K , the SDP relaxation (11) for the K-means is minimax-optimal in
n1 = n2 =
the sense that sharp phase transition of the probability of wrong recovery from zero to one
2
occurs at the critical threshold given by the ∆

= nK = n

in (8).

· · ·

3. Semidefinite programming relaxation: primal and dual

In this section, we describe the SDP relaxation of the K-means that achieves the cutoﬀ
value of the exact recovery and outline the strategy of showing that the SDP solution uniquely
recovers the true clustering structure by a dual certiﬁcate argument via the primal-dual con-
struction. We remark that similar primal-dual analyses are done in [42, 33].
GK |−
|

Let A = X T X be the aﬃnity matrix and B = diag(
G1|−
|

1). Then we can

1, . . . ,

reparametrize (3) as

A, HBH T

max
H h

i

subject to H

n
0, 1
}

∈ {

K , H1K = 1n,

×

(9)

which is a mixed integer program with a nonlinear objective function [57, 31]. If the cluster
centers µ1, . . . , µK are properly separated, then the aﬃnity matrix A from the data has an
approximate block diagonal structure (up to a permutation of the data index).

Changing variable Z = HBH T , we observe that the n

the following properties:

n symmetric matrix Z satisﬁes

×

(P1) positive semideﬁnite (psd) constraint: Z
(P2) non-negative (entrywise) constraint: Z > 0, i.e., Zij > 0 for all i, j
(P3) unit row-sum constraint: Z1n = 1n;
(P4) trace constraint: tr(Z) = K.

(cid:23)

0;

[n];

∈

Since Z is symmetric, properties (P2) and (P3) automatically ensure that Z is a stochastic
matrix Z1n = Z T 1n = 1n. Given any clustering structure G1, . . . , GK , we may consider the
associated cluster membership matrix:

Gk|
1/
|
0

if i, j
Gk
∈
otherwise

.

Zij =

(cid:26)

(10)

Thus to recover the true clustering structure G∗1, . . . , G∗K , it suﬃces to compare the estimated
membership matrix and the true one Z ∗.

After the change-of-variables, the objective function in (9) becomes linear in Z. Then we

use the solution ˆZ of the following (convex) SDP to estimate Z ∗:

ˆZ = argmaxZ

A, Z

CK h
∈

,
i

(11)

where

CK =

Z

Rn

n

×

Z

0, Z T = Z, tr(Z) = K, Z1n = 1n, Z > 0

.

∈

n

(cid:23)

(cid:12)
(cid:12)
(cid:12)

Note that the above SDP is ﬁrst proposed in [57] and later studied in [27, 15, 14]. For
spherical Gaussians (i.e., the noise covariance matrix is proportional to the identity matrix),
since the SDP relaxation (11) does not require the knowledge of the noise variance σ2 and
the partition information other than the number of clusters K, it in fact can handle the more
general case of unequal cluster sizes.

o

6

XIAOHUI CHEN AND YUN YANG

Remark 3.1 (Adaptation to the number of clusters K). The SDP in (11) can be made adaptive
to the unknown number of cluster K. When the number of clusters K is unknown, the
constraint tr(Z) = K in the SDP (11) can be lifted to a penalization term in its objective
function, i.e., we solve

˜Zλ := arg max

A, Z

{h

i −

λtr(Z) : Z

0, Z T = Z, Z1n = 1n, Z > 0
,
}

(cid:23)

(12)

where λ > 0 is a regularization parameter. This is the regularized K-means proposed by [12,
59] and analyzed by [14] in the manifold clustering setting. Using the same existing argument
for proving the separation upper bound in Section 4, we see that with the λ choice being

σ2(√n + √p +

2 log n )2+Cβ−

1σ2 (n + K log n + (1

p

6 λ 6 pσ2 +

β
4

β)Kδ

pm log n)

−

m∆2,

p

(13)

−

CK 2n−

then under the same conditions in Theorem 2.1, ˜Zλ = Z ∗ achieves exact recovery with prob-
δ. Note that a larger signal-to-noise ratio ∆2/σ2 permits a wider
ability at least 1
allowable range for λ to achieve exact recovery, and our conditions in Theorem 2.1 ensures
the existence of at least one such λ. The idea for ˜Zλ to achieve the sharp threshold (i.e.,
the separation upper bound) is that the SDP giving ˆZ in (11) and its regularized version
in (12) have the same Lagrangian form and the dual problem. Thus we need only to extract
the regime of the regularization parameter λ in (13) that ensures a successful dual certiﬁcate
construction as characterized in ˆZ (Section 4). In particular, the dual certiﬁcate constructed
for ˆZ is a convenient choice of λ♯ that falls into the region (13) with high probability.
In
addition, [14] provides a practical method for adaptively tuning this regularization parameter
(cid:4)
λ.

Note that Z ∗ is a rank-K block diagonal matrix, and for any Z

CK, due to the psd
∈
. Then the SDP in (11) can be eﬀectively
constraint, tr(Z) equals to the nuclear norm
Z
k
viewed as a low-rank matrix denoising procedure for the data aﬃnity matrix A by ﬁnding
its optimal matching from all feasible “rank-K” stochastic matrices proxied by the trace
constraint.

k∗

On the other hand, the SDP solutions are not integral in general. If this is the scenario,
then the standard relaxing-and-rounding paradigm [66] can be used to round the SDP solution
back to a point in the feasible set of the original discrete optimization problem (3). In our
case, we can apply the K-means clustering to the top K-eigenvectors of ˆZ as a rounding
procedure to extract the estimated partition structure ˆG1, . . . , ˆGK .

However, it is observed that the rounding step is not always necessary and solution to the
clustering problem (3) can be directly recovered from solving the relaxed SDP problems when
the separation of cluster centers is large, which is sometimes referred to the exact recovery
or hidden integrality phenomenon [6, 23]. This motivates the question we asked earlier in
Section 1 that when and to what extend the SDP relaxation can in fact produce the exact
recovery. The rest of the paper is devoted to characterize the precise cutoﬀ value on the
separation of cluster centers that yields the exact recovery.

SHARP THRESHOLD OF GAUSSIAN MIXTURES

7

3.1. Dual problem. To analyze the exact recovery property of ˆZ, we ﬁrst derive the dual
problem for the (primal) SDP problem in (11). Let

(Z, Q, λ, α, B) = tr(AZ) + tr(QZ) + λ(K

L

−

tr(Z)) + αT

= (λK + αT 1n) + tr

A + Q

be the Lagrangian function, where Qn
are the Lagrangian multipliers. Consider the max-min problem:

0, αn

×

×

(cid:26)(cid:20)
n (cid:23)

Z + Z T
2

1n

+ tr(BZ)

(cid:18)
λIdn + B

1n −
1
2

−

−
1 = (α1, . . . , αn)T , Bn

(cid:19)
(1nαT + α1T
n )
(cid:21)
n > 0, and λ

(cid:27)

Z

×

R

∈

max
∈

Rn×n

Z

Q

(cid:23)

min
R,α
∈

∈

0,λ

Rn,B>0 L

(Z, Q, λ, α, B),

where the maximum over Z is unconstrained. If Z is not primal feasible for the SDP prob-
lem (11), then

Q

0,λ

Rn,B>0 L

min
R,α
∈

(Z, Q, λ, α, B) =

.

−∞

(cid:23)
For example, consider tr(Z)
On the other hand, if Z is feasible for (11), then

= K and choose λ =

∈

c
tr(Z) with an arbitrarily large c > 0.

K

−

−

where the equality is attained if for example Q = B = 0. Then,

tr(AZ) 6

Q

(cid:23)

min
R,α
∈

∈

0,λ

Rn,B>0 L

(Z, Q, λ, α, B),

tr(AZ) 6 max

max
CK
Z
∈

6

min
R,α
∈

∈

∈

Z

Rn×n

0,λ

Q
(cid:23)
min
R,α
Q
(cid:23)
∈
1
2 (1nαT + α1T
n )

0,λ

Rn,B>0

∈

Rn,B>0 L
max
∈

Rn×n L

Z

= 0, then

Similarly, if A + Q

λIdn + B

−

−

(Z, Q, λ, α, B)

(Z, Q, λ, α, B).

max
∈

Rn×n

Z

tr

A + Q

(cid:26)(cid:20)

λIdn + B

−

1
2

−

(1nαT + α1T
n )
(cid:21)

Z

(cid:27)

=

,

∞

which is avoided by the minimization over the Lagrangian multipliers. Thus with Q = λIdn +
1
2 (1nαT + α1T
n )

A, we have

B

−

−

tr(AZ) 6

max
CK
Z
∈

R,α

λ

∈

min
Rn,B
∈

Rn×n

∈

λK + αT 1n : B > 0, λIdn +

(cid:26)

1
2

(1nαT + α1T
n )

B

A

−

−

(cid:23)

,

0
(cid:27)

which is the weak duality between the primal SDP problem (11) and its dual problem:

R,α

minλ
∈
∈
subject to B > 0,

Rn,B

∈

Rn×n

λK + αT 1n}
{

Moreover, the duality gap is given by

λIdn +

1
2

(1nαT + α1T
n )

B

A

−

−

(cid:23)

0.

λK + αT 1n −

tr(AZ) = λ tr(Z) + αT Z + Z T

tr(AZ)

2

1n −
(1nαT + α1T
n )

A

−

−

B

Z

+ tr(BZ)

(cid:21)

(cid:27)

= tr

λIdn +

(cid:26)(cid:20)
> tr(BZ) > 0.

1
2

(14)

(15)

6
6
8

XIAOHUI CHEN AND YUN YANG

3.2. Optimality conditions: primal-dual construction. Let 1G∗
such that it is equal to 1nk on G∗k and zero otherwise. To show that

k

be the n

1 vector

×

1
n1 Jn1
0
...
0

0
1
n2 Jn2
. . .

· · ·

· · ·
· · ·

· · ·
0

0
0
...
JnK

1
nK



=






Z ∗ = 





1
nk

K

Xk=1

1G∗

k

1T
G∗
k

(16)

is the solution of the primal SDP problem (11), we need the duality gap (15) is zero at Z = Z ∗.
To this end, we need to construct a dual certiﬁcate (λ, α, B) such that:
(C1) B > 0;
(C2) Wn := λIdn + 1
(C3) tr(WnZ ∗) = 0;
(C4) tr(BZ ∗) = 0.
Note that (C1) and (C2) are dual feasibility constraints, while (C3) and (C4) are the optimality
conditions (i.e., complementary slackness) corresponding to the zero duality gap in (15). In
particular, (C4) implies that BG∗

2 (1nαT + α1T
n )

= 0 for all k

[K].

−

−

(cid:23)

0;

B

A

To ensure that Z ∗ is the unique solution of the SDP problem (11), we observe that Z ∗ is

kG∗

k

∈

the only feasible matrix to the SDP (11) satisfying the block diagonal structure

Z (1)
0
...
0








0
Z (2)
. . .

· · ·

· · ·
· · ·

· · ·
0 Z (K)

0
0
...



,






l

kG∗

= 0 for all distinct pair (k, l)

Indeed, since each block Z (k) satisﬁes
i.e., ZG∗
1/21nk ) is one eigenvalue-eigenvector pair of Z (k) and the
Z (k)1nk = 1nk and is psd, (1, nk−
K
trace of Z (k) is at least 1. On the other hand, due to the trace constraint
k=1 tr(Z (k)) =
tr(Z) = k, we then must have tr(Z (k)) = 1. In addition, 1 is its only nonzero eigenvalue with
eigenvector nk−

1/21nk . Consequently, Z (k) must take the form of n−

[K]2.

P

1
k Jnk .

Given the above block diagonal structure and tr(BZ ∗) = 0, we conclude that Z ∗ is the

∈

unique solution to the SDP (11) if
(C5) BG∗
in addition to the optimality conditions (C1)-(C4).

> 0 for all distinct pair (k, l)

[K]2,

kG∗

∈

l

4. Proof of Theorem 2.1

In this section, we show that a dual certiﬁcate described in Section 3.2 can be successfully
0 and

constructed with high probability, thus proving Theorem 2.1. First, observe that Wn (cid:23)
tr(WnZ ∗) = 0 imply that

Wn1G∗

= 0 for all k

[K].

k
= 0 imply that for each distinct pair (k, l)

∈

The last display together with BG∗
kG∗
1
2

λ1nk +

k

1nk (

G∗
Xi
k
∈
αG∗

l

1
2

1nl(

αi) +

G∗
Xi
k
∈

1
2

αi) +

1
2

αG∗

k

nk = AG∗

kG∗

k

nk −

AG∗

l G∗

k

1nk = BG∗

l G∗

k

1nk ,

1nk ,

[K]2,

∈

(17)

(18)

(19)

SHARP THRESHOLD OF GAUSSIAN MIXTURES

9

where αT = (αT
G∗
1

, . . . , αT
G∗
K

). From (18), we get

αi =

1
nk

G∗
Xi
k
∈

1T
nk AG∗

kG∗

k

1nk −

λ.

Substituting the last equation back into (18), we get
1
n2
k

1nk −
Next we construct a solution of B for (19). For k

1nk −

λ
nk

2
nk

αG∗

AG∗

kG∗

=

k

k

= l, we have

1nk (1T

nk AG∗

kG∗

k

1nk ).

(20)

BG∗

l G∗

k

1nk =

nl + nk
2nl

−

λ1nl+

1
2nk

(1T

nk AG∗

kG∗

k

1nk )1nl+

nk
nl

AG∗

l G∗

l

1nl−

nk
2n2
l

(1T

nlAG∗

l G∗

l

1nl)1nl−

AG∗

l G∗

k

1nk .

In particular, for j

[BG∗

l G∗

k

1nk ]j =

=

=

G∗l ,
∈
nl + nk
2nl

λ +

1
2nk

nl + nk
2nl
nl + nk
2nl

λ +

λ +

nk
2
nk
2

−

−

−
1

XT

s Xt +

nk
nl

G∗
Xt
l
∈

XT
j Xt −

nk
2n2
l

G∗
Xs,t
l
∈

XT
s Xt −

G∗
Xt
k
∈

XT

j Xt

XT

l Xl) + nkXT

Xjk

2
2 − k

Xl −

j (Xl −
2
Xjk
2),

Xk)

(21)

G∗
Xs,t
k
∈
(XT
k Xk −
Xk −
(
k

Xi is the empirical mean of data points in the k-th cluster. Without
where Xk = n−
k
loss of generality, we may take a symmetric B (i.e., BT = B) and then construct B as
block-wise rank-one matrix satisfying the above row sum constraint (21):

G∗
k

P

∈

i

B♯
G∗

l G∗

k

=

BG∗

k

l G∗
1T
G∗
l

1G∗

k

BG∗

1T
G∗
l
l G∗

k

BG∗
1G∗

k

l G∗

k

for each distinct pair (k, l)

∈

[K]2, and B♯
G∗

kG∗

k

= 0.

(22)

G∗l

c(k,l)
j

For notational simplicity, let us denote the column sums and row sums of matrix BG∗
by c(k,l) =
t(k,l) =

: j
∈
c(k,l)
=
G∗
(cid:0)
j
j
l
∈
/t(k,l). For convenience, we also deﬁne r(k,k)
c(k,l)
]ij = r(k,l)
P
P
i
j
G∗k, so that BG∗
kG∗

be the total sum, then the construction in (22) becomes
= t(k,k) = 0 for all

, respectively. In addition, by letting

and r(k,l) =
r(k,l)
i

[K] (deﬁne 0/0 = 0).

= 0 for all k

= c(k,k)
j

[BG∗
i, j

r(k,l)
i

in (22)

∈
Recall that to ensure uniqueness, we need to choose λ such that B♯
G∗

kG∗

l G∗

G∗k

G∗
k

(cid:1)
i
∈

: i

∈

∈

(cid:1)

(cid:0)

k

k

i

l

pair (k, l)

∈

[K]2, which is, in view of (21), guaranteed whenever

λ < min
16k

=l6K

nlnk
nl + nk

min
G∗
j
l
∈

Xk −
(
k

Xjk

2
2 − k

Xl −

Xjk

2
2).

(cid:26)

B

(cid:27)
On the other hand, we require that λ is not too small since Wn = λIdn + 1
−
A
0. To identify the right λ, we will employ the following lemma that provides some
−
high probability lower bounds that will be useful for bounding from below the column sums
c(k,l)
under proper separation conditions on the
G∗k}
G∗l }
j
{
Gaussian centers. Recall that

2 (1nαT + α1T
n )

and row sums

r(k,l)
i
{

: j

: i

(cid:23)

∈

∈

kG∗

l

> 0 for all distinct

.

(23)

∆ = min
16k

=l6K k

µk −

µlk

and m = min

16k

=l6K

2nknl
nk + nl

.

o

n

Note that ∆ is the minimum separation between the cluster centers and m quantiﬁes the
“minimum” cluster size in the pairwise sense.

6
6
6
6
10

XIAOHUI CHEN AND YUN YANG

Lemma 4.1 (Separation bound on the Gaussian centers). Let δ > 0 and 1 > β > 0. If there
exists a suﬃciently large universal constant c1 > 0 such that

∆2 >

4σ2(1 + 2δ)

β)2

(1

−

1 +

1 +

s

β)2
(1
−
(1 + δ)

p
m log n

+ c1Rn

log n

!

(24)

with

Rn =

then as long as m > 4(1 + δ−

−

(1

β)2
(1 + δ) log n   p
1)2,

p log(nK)
n

+

log(nK)
n

,

!

P

Xi −
k
(cid:16)

2

Xlk

Xi −

− k

2 >

Xkk

nk + nl
nknl

σ2p + β

µk −
k

2

µlk

−

rkl,

for all distinct pairs (k, l)

[K]2 and i

∈

G∗k

6

K 2
nδ +

8
n

,

2p log(nK) +

log(nK).

(cid:17)
4σ2
nk

∈
+ 2σ2 nk + nl
nknl

p

where rkl = 2σ

2 log(nK)
nl

s

µk −
k

µlk

If the conditions of Lemma 4.1 holds, then according to this lemma we may choose

λ♯ = pσ2 +

β
4

m∆2,

(25)

G∗k,

∈

so that it holds with probability at least 1
j

δ

2n−

−

−

10n−

1 that for all k, l

[K], k

= l, i

∈

G∗l ,

∈
r(k,l)
i

β
2

>

c(k,l)
j >

µk −

2,
µlk
µlk
µk −
nl k
1)2. This implies B♯
as long as m > 4(1 + δ−
kG∗
G∗
ﬁx such a choice for λ in the rest of the proof.
1G∗
Denote ΓK = span
{

nk k

l

β
2

2

and t(k,l) >

µk −
> 0 for any distinct pair (k, l)

nknl k

β
2

2,
µlk

(26)

[K]2. We

∈

k

∈

: k

[K]
of Rn spanned by the vectors 1G∗
1 , . . . , 1G∗
1G∗
{
eigenvectors of Wn associated to the zero eigenvalues. Thus to ensure Wn (cid:23)
to check that: for any v = (v1, . . . , vn)T

}⊥ be the orthogonal complement of the linear subspace
are
[K]
}
k
0, we only need

∈
vT Wnv > 0.
Our next task is to derive a high probability lower bound for the quadratic form vT Wnv.
Plugging the deﬁnition of Wn, we write

. In view of (17), we see that

ΓK such that

k2 = 1,

v
k

: k

∈

K

vT Wnv = λ

v
k

2 +
k

1
2

(vT 1nαT v + vT α1T v)

−

K

XT
i Xjvivj −

vT B♯v.

Since vT 1G∗

k

= 0 or

i

G∗
k

∈

P

where S(v) :=
k
Xi = µk + εi for i

K
k=1
i
∈
G∗k, we have

G∗
k

P

P
∈

vi = 0 for all k

[K] and v

G∗
G∗
Xk,l=1 Xi
k Xj
l
∈
∈
ΓK, we get

∈

∈
v
k

vT Wnv = λ
2 and T (v) = vT B♯v. Recall the clustering model (1):
2
Xivik

T (v),

S(v)

2
k

−

−

Xivi = µk

vi +

εivi =

εivi.

G∗
Xi
k
∈

G∗
Xi
k
∈

G∗
Xi
k
∈

G∗
Xi
k
∈

 
6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

11

so that

n

n

S(v) =

εiεT

j vivj

Xj=1
is a quadratic form in v. Therefore, for each v
bounded by the largest singular value of the Gram matrix Gn =
S(v) = vT

Xi=1

v 6

∈

ΓK satisfying

T

T

Ekop =

2
op, where matrix
kEk

E

E

kE

E
has i.i.d. N (0, σ2) entries. Applying Lemma 8.2, we can reach

∈

= (ε1, ε2, . . . , εn)

Rp

n

×

v
k
εiεT
j

= 1, S(v) can be
, so that

[n]

k
: i, j

∈

(cid:8)

(cid:9)

P

v

max
v
ΓK ,
k

=1

∈
Now we analyze the last term T (v).

(cid:16)

k

S(v) > σ2(√n + √p + √2t )2

6 e−

t,

t > 0.

∀

(cid:17)

Lemma 4.2 (Bound on T (v)). Assume the separation condition (24) in Lemma 4.1 and con-
sider the choice of λ as (25). We have for any δ > 0,

P

(cid:18)

T (v)
|
|

> Cβ−

1σ2

n + K log n + (1

(cid:0)

β)Kδ

mp log n

−

v
k

2,
k

p

(cid:1)

v

ΓK

¯εk : k
{

∈
∀
(cid:12)
(cid:12)
δ + 10n−
6 4K 2n−
(cid:12)
(cid:12)

[K]
}
(cid:19)

∈
1.

By combining previous bounds on

P

v, Wnv
h
(cid:18)

i

6 λ

−

σ2(√n + √p +

and

S(v)
|
|
2 log n )2

p

T (v)
|
|
1σ2

Cβ−

−

together, we obtain

n + K log n + (1

−

β)Kδ

mp log n

,

p
6 (5K 2 + 1) n−

δ.

= 1

(cid:1)

(cid:0)
ΓK ,

v
k

k

v

∀

∈

(cid:19)

Combining this with our constructions (25) for λ♯, (20) for α♯ and (22) for B♯ and all previ-
ous analysis, we obtain that (λ♯, α♯, B♯) will be a dual certiﬁcate that satisﬁes (1)–(5) with
probability at least 1

(5K 2 + 1) n−

δ if

−
σ2(√n + √p +

2 log n )2 + Cβ−

1σ2 (n + K log n + (1

β)Kδ

pm log n)

6 pσ2 +

p
β
m
4

4σ2(1 + 2δ)

β)2

(1

−

1 +

1 +

s

β)2
(1
−
(1 + δ)

which is true if for some universal constants C, c > 0,

−
p
m log n

p
+ c1Rn

!

log n.

(27)

(1

log n >

β)2

−
β2

Cn
m

,

and δ 6

β2

(1

β)2

c
K

.

−
5. Proof of Theorem 2.3

The ﬁrst step is to reduce the worst-case misclassiﬁcation risk to the average-case risk by
putting a prior πH over H =
with (hi1, . . . , hiK ) being i.i.d. following the multinomial
distribution with one trial and probability vector (n/K, . . . , n/K). By the classical Chernoﬀ
bound we have

hik}
{

P

πH

nk :=

(cid:16)

n

Xi=1

hik ∈

h

(1

−

δn)

n
K

, (1 + δn)

n
K

i

, k

∈

[K]

> 1

(cid:17)

n−

1,

−

(28)

 
12

XIAOHUI CHEN AND YUN YANG

by choosing the constant C in δn large enough. As a consequence, we have
ˆhik 6
E

sup
Θ(n,K,∆)

= hik, i

[n], k

(H,µ)

(H,µ)

[K]

∈

P

(cid:0)

∈

∈
πH P(H,µ)

= hik, i

[n], k

sup

inf
ˆhik}
{
> inf
ˆhik}
{

µk−

k

µlk

>∆,

(k,l)

[K]2,k

=l

∀

∈

ˆhik 6

(cid:0)

(cid:1)
∈

[K]

∈

n−

1.

−

(cid:1)

Conditioning on the event that n1 + n2 points belong to the ﬁrst two clusters, the problem of
correctly classifying all n samples into K clusters is always not easier than correctly classifying
the n1 + n2 points into the ﬁrst and second clusters, that is,

ˆhik 6

ˆhik 6
(cid:0)

(cid:1)

(cid:0)

∈

∈

∈

∈

[2]

i
{

[K]

[n], k

G2, k

> P(H,µ)

= hik, i

= hik, i

P(H,µ)
G1 ∪
denote the k-th cluster. Now we apply the
[n] : hik = 1
where recall that Gk =
}
following minimax lower bound Lemma 5.1 proved in Section 5.1 below for two clusters G1
and G2 conditioning on their total sizes n1 + n2,
ˆhik 6
(cid:0)

for some c > 0, where ˜π12 denote the conditional prior distribution of
G1 ∪
G2, k =
∈
n1+n2. Here
1, 2
1, 2
}
{
}
1, the
we have used the high probability bound (28) so that with probability at least 1
n−
separation ∆ satisﬁes

given the total sample size n1 + n2 of G1 ∪

G2, which is uniform over

(cid:1)
ˆhik, i
{

˜π12P(H,µ)

sup
µ2k

= hik, i

G1 ∪

ˆhik,i
{

cK/n,

G2, k

µ1−

G2,k=1,2

> 1

inf

[2]

>∆

−

−

G1

∈

∈

∈

E

(cid:1)

∪

∈

k

}

,

∆2 6 4(1

−

α/2)σ2

1 +

1 +

s

2p
(n1 + n2) log(n1 + n2) !

log n.

Note that the proof of Lemma 5.1 also reduces the worst-case bound to the average-case
bound, where the prior on the cluster label is uniform as the conditional distribution ˜π12
given the total size n1 + n2. Putting all pieces together and using K 6 log n give a proof of
the claimed result.

5.1. Lower bound for K = 2. Now we prove an information-theoretic limit for exact recov-
ery of clusters labels in a symmetric two-component Gaussian mixture model,

Xi = ηiµ + σεi,

εi

N (0, Ip),

i = 1, . . . , n,

(29)

i.i.d.
∼

−

1, 1
}

is the label for the ith observation indicating which component it comes from.

µ are unknown centers of the two symmetric Gaussian components, and

where µ and
ηi ∈ {−
Lemma 5.1 (Separation lower bound for exact recovery: K = 2). Let α
(0, 1). Consider the
symmetric two-component Gaussian mixture model in (29) with an independent Rademacher
prior distribution on ηi. If ∆2 6 (1
, then

α)∆

∈

2

inf
ˆη

−
sup
>∆/2

µ
k

k

P(ˆη

= η) > 1

cn−

1,

−

(30)

where c > 0 is a constant depending only on α and the inﬁmum is over all possible estimators
ˆη for η =

ηi}
{

n
i=1 ∈ {±

n.
1
}

Remark 5.2. Our Lemma 5.1 is stronger than the exact recovery notation in [52] and the
probability of wrong recovery lower bound in (30) does not follows from the lower bound
therein for the expected Hamming distance loss in the symmetric two-component Gaussian
mixture model. Moreover, complementing the upper bound in Corollary 2.2, the lower bound
is sharply optimal in the sense that the probability of wrong recovery is arbitrarily close to

6
 
6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

13

one (rather than just bounded away from zero) if the separation signal size ∆2 is below the
2
(cid:4)
cutoﬀ value ∆

.

Proof of Lemma 5.1. To prove the lower bound, we follow the same setup as in the lower
bound proof in [52] by placing a N (0, κ2
nIp) prior on µ and an independent Rademacher prior
on η. Note that algorithm that maximizing the probability of reconstructing labels correctly
X). Since the prior label
is the maximum a posterior (MAP) estimator
assignment is uniform, MAP is in particular equivalent to maximum (integrated) likelihood
estimator (MLE) after integrating out µ, i.e.,
η)
is viewed as a function of η. Speciﬁcally, the maximum (integrated) likelihood function can
be computed as follows
e

η = argmaxη p(η

η = argmaxη L(η

X) where L(η

X) = p(X

e

|

|

|

|

n

p(Xi |

µ, ηi) p(µ) dµ

X) =

L(η

|

Rp

Z

∝

Rp

Z

exp

∝

Yi=1
exp

1
2

n

(cid:16)

−

n
n
σ2 +

1
2κ2

n k

µ

dµ

2
k

o

n

1
2σ2

Xi −
k

ηiµ

2
k

−

Xi=1
−

1

1
κ2
n

(cid:17)

1
σ2

n

ηiXi

2

.

Xi=1

o

(cid:13)
(cid:13)
(cid:13)
2, or equivalently,

(cid:13)
(cid:13)
(cid:13)

(cid:10)
n

=i
Xj

We can see from the last expression that the MLE fails if there exists some i
ηiXi +
Therefore,
(cid:13)
P
(cid:13)

n
=i ηjXj
j

n
=i ηjXj
j

ηiXi +

ηiXi,

2 <

P

(cid:13)
(cid:13)

−

P

[n] such that
∈
n
=i ηjXj
< 0.
j

(cid:11)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
= η) = P(
η

P(
η

inf
bη

= η) > P

i

∈

∃

(cid:16)

[n], such that

ηiXi,

ηjXj

< 0

.

(31)

(cid:11)
e
n
Without loss of generality, we assume σ = 1. Let εn = n−
i=1 εi be the sample average of
the noise. Since (η1ε1, . . . , ηnεn) has the same joint distribution as (ε1, . . . , εn), we can write

b

(cid:10)

1

(cid:17)

P

n

=

1

−

1

(cid:10)
εi −
D

n

ηiXi,

ηjXj

=

µ + ηiεi, µ +

1

=i
Xj
εn, µ +

n

n

−
=:Ri,1

(cid:11)

1

(cid:10)

εn

+

E

2
µ
k
k

+

n

n

−

n

1

=i
Xj
2

−
εnk

1 k

ηiεi

d=

µ + εi, µ +

(cid:11)
1

−

D
εik

2

+

2n
n

−
−

−

n

1 k

n

n

¯εn −

1

−
µ, εni −

1
1 h

1

−

n

1

εi

1

E
µ, εii

.

1 h

n

−

=:Ri,2

{z

}

|

}

=:Ri,3

{z

Bound Ri,3. Let βn > 0 and

{z

|

}

|

6 √n

6 √n

−

−

√n

√n

µ, εni|
|h
µ, εni|
|h

B1 =
B1 =
e

(cid:8)

(cid:8)

[n] |h

2, max
µ
1βnk
k
i
∈
µ
, max
1βnk
k
i
∈

[n] |h

µ, εii|
µ, εii|

6 √n

6 √n

,

2
k

µ
1βnk
−
µ
1βnk
k
(cid:9)

−

(cid:9)

.

B

By the standard tail inequality of the Gaussian random variable and union bound, we have
P(
cnβ2
c
2)
c
1) 6 min
1) 6 min
for some univer-
n)
1, n exp(
}
−
{
}
k
µ
6 3βnk
In addition, we have maxi
sal constant c > 0.
B1 and
Ri,3|
[n] |
∈
µ
6 3βnk
Ri,3|
maxi

2 on the event
k

1, n exp(
{

on the event

cnβ2
nk

and P(

[n] |
∈

−

µ

B

e

k

B1.
e

6
6
6
6
6
6
6
6
14

XIAOHUI CHEN AND YUN YANG

Bound Ri,2. By tail inequalities of the chi-square random variable in Lemma 8.1, we have
for any t > 0 and i

[n],

∈
p

P(

2

εik
k

−
Thus we have P(

(cid:12)
(cid:12)
B2 =

B
εnk
k

2

n

p

6 2

−

> 2√pt + 2t) 6 2e−

t

and P(

n

ε
k

2
k

−

p

> 2√pt + 2t) 6 2e−

t.

(cid:12)
c
2) 6 4n−
(cid:12)

1, where

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:8)(cid:12)
(cid:12)
On the event

p
(cid:12)
(cid:12)
B2, we have maxi

[n] |
∈

Ri,2|

p log n + 2 log n, max
[n]

i

εik
k
(cid:12)
(cid:12)
6 6(√p log n + log n)/(n

−

(cid:12)
(cid:12)

p

∈

2

6 2

2p log n + 4 log n

.

p
1).

−

(cid:9)

Analyze Ri,1. From elementary calculations, we have that the conditional joint distribution
of Ui : =

, i = 1, . . . , n, given εn is

εi −

εn, µ + n
n
−

1 εn

E

D
U1
U2
...
Un

,

n







µ +

εn ∼

(cid:12)
(cid:12)
(cid:12)

(cid:12)

(cid:12)

(cid:12)
(cid:12)

(cid:12)
Conditioning on εn, let
(cid:12)
εn) = E(Z 2
E(U 2
i |
i |
inequality (cf. Theorem 7.2.9 in [63]) we have

0
0

...


0






n
i=1 be i.i.d. N
Zi}
{
εn) and E((Ui −
Uj)2

N 










(cid:13)
(cid:13)
(cid:13)

εn

−

n

1

(cid:13)
(cid:13)
(cid:13)

1

1

n−
−
1
n−
−
...
n−

1

−
1)

2








−
1
−

1

n−
n−
...
n−

1

1

2

−
µ+ n
1 εn
n
−
Zj)2

n−
0, (1
−
εn) > E((Zi −
(cid:13)
(cid:0)
|
(cid:13)

−
−

1

1

n−
n−
...
n−

1

−

· · ·
· · ·
. . .

· · ·





.

1











random variables. Since
[n], by Slepian’s

εn) for i, j
(cid:1)

|

(cid:13)
(cid:13)

∈

P

max
[n]
i
∈

Ui > t

εn

> P

max
[n]
i
∈

Zi > t

εn

= 1

1

−

−

(cid:16)
(cid:0)
Combining the previous three terms with (31), we obtain that

(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:1)(cid:17)

(cid:12)
(cid:12)

P

Z1 > t

εn

n

,

R.

t

∈

P(
η

= η) > 1

= 1

b

−

−

P

(cid:16)

P

Ri,1 + Ri,2 + Ri,3 > 0,

i

∀

∈
2 + Ri,2 + Ri,3,
k

Ui 6

µ
k

[n]

(cid:17)
∈

i

∀

[n]

1

1

>




(cid:16)
−

−

2 + 6(√p log n+log n)
µ
Ui 6 (1 + 3βn)
,
n
k
k
−
+ 6(√p log n+log n)
2 + 3βnk
µ
n
k
−

µ
k

Ui 6

k

1

,

−

−

(cid:17)
1

−
P

P

(cid:16)

(cid:16)

For γn > 0, we deﬁne



i

∀

i

∀

∈

∈

[n]

(cid:17)
[n]

(cid:17)

on

on

B1 ∩ B2
B1 ∩ B2

e

.

(32)

B3 =

2
µ
k
k
(cid:8)(cid:12)
With the prior distribution µ
N (0, 4−
(cid:12)
follows from the proof of Theorem 5 in [52] (cf. equation (28)) that P(
n/32),
provided 4νn 6 γn 6 1. Moreover, using the lower tail bound of the chi-square random
variable in Lemma 8.1, we have P(

νn) and νn =
c
3) 6 2 exp(
q

−
(cid:12)
nIp) where κ2
1κ2
(cid:12)

n∆2
4p log2 n
pγ2

6 ∆2γn/4

∆2/4

(cid:9)
4p(1

n =

, it

∆2

−

∼

B

−

.

n/4), where

c
4) 6 exp(

−
2 >

pθ2
p
n

B
B4 =

εnk
k

(cid:8)

(1

θn)

.

−

(cid:9)

To analyze the right-hand side of (32), we ﬁrst consider the higher dimensional case where
p > log2 n. In such regime, we divide further into three cases depending on the separation
signal size as following.

6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

15

Medium signal size case:
joint distribution of (U1, . . . , Un) given εn, we can bound on

n . Since

q

n

3/2
√n < ∆ < 2

p log n

2 log

(U1, . . . , Un) has the same

µ
Ui 6 (1 + 3βn)
k

−

2 +
k

−
B1 ∩ B2,
i
,

6(√p log n + log n)

n

1
∀
6(√p log n + log n)

−

∈

[n]

(cid:17)

max
[n]
i
∈

µ
Ui 6 (1 + 3βn)
k

2 +
k

P

1

−

µ
Z1 > (1 + 3βn)
k

2 +
k

h

(cid:16)

n

1
(cid:17)
6(√p log n + log n)

−

n

1

−

n

.

εn

|

(cid:17)i

N (0, 1) be the standard Gaussian random variable. Thus, on the event

P

(cid:16)

=P

(cid:16)
6 E

4

i=1 Bi, we

T

Let Z
have

∼

where

P

µ
Z1 > (1 + 3βn)
k

2 +
k

6(√p log n + log n)

εn

1

n

−

|
2 + 6(√p log n+log n)
µ
(1 + 3βn)
n
1
k
k
−
2 + n2
2 + 2n
n
(n
k
−
−
(1 + 3βn)(1 + γn)∆2 + 24(√p log n+log n)

εnk
k

µ
k

1 h

q

1)2

−

1
n

1

(cid:17)

µ, εni

n

−

1
n −

2βn)(1

1
−
γn)∆2 + 16p
1 (1
−

θn) 

(1 + 3βn)(1 + γn)∆2 + 24(√p log n+log n)

−

−

n

4(1

εn

|



= Φc(Vn),

.

(cid:16)

=P

Z >






q

>P

Z >





q

Vn =

θn)
−
and Φc(t) = P(Z > t). Combining all pieces together, we obtain that

2βn)(1

4(1

q

−

−

1
n −

n

1
−
γn)∆2 + 16p
1 (1
−

n

P(
η

= η) > 1

[1

−

−

(1

−

rn)Φc(Vn)]n

rn

−

(33)

provided 4νn 6 γn 6 1, where
e
cnβ2

1, n exp(
rn = min
{
Note that ∆ < 2

−
n , implying ν2

n(1

p log n

−

q

β2
n =

1
log n

,

n 6 1

log n . We choose
16
log n

γ2
n =

,

θ2
n =

1
log n

.

n)∆2)
γ2
}

+ 4n−

1 + 2 exp(

−

pγ2

n/32) + exp(

−

pθ2

n/4).

Since p > log2 n and ∆ > 2 log

3/2

n

√n

, we have

−

By using the fact that log Φc(t)
δ) log n for any δ

2(1

∼ −

(0, 1), then for n > 2c, we have

→ ∞

P(
η

= η) > 1

−

[1
(1
−
−
t2/2 as t

cn−

1)Φc(Vn)]n
, we can conclude that as long as Vn 6

cn−

1.

−

p

P(
η

= η) > 1

−

1

1

−

cn−
δ

−

−
n1

cn−

1 > 1

nδ/2

e−

−

cn−

1 > 1

c′n−

1,

−

−

−

where c′ is a constant depending on δ. The condition Vn 6
∆2 6 (1
x > 0.

2(1
for some δ := δ(α) and by inverting the function x

e
α)∆

p

−

2

−
7→

δ) log n is implied by
4x + 16p/n for

x/

1

n

(cid:17)

e
∈

(cid:16)

p

6
6
6
16

XIAOHUI CHEN AND YUN YANG

Low signal size case: ∆ 6 2 log

3/2

n

√n

. The argument is similar to the medium signal size

case, so we only sketch the proof. On the event

P

(cid:16)
6 E

max
[n]
i
∈

Ui 6

µ
k

2 + 3βnk
µ
k

k

+

P

1

−

Z1 >

(cid:16)

µ
k

2 + 3βnk
µ
k

k

+

B4, we have

T

B3

B2

B1
6(√p log n + log n)
T
e
n

1
(cid:17)
6(√p log n + log n)

T

−

n

εn

|

(cid:17)i

n

1

−

εn

|

(cid:17)

6(√p log n + log n)

n

1

−
+ 6(√p log n+log n)
n
−
2

1

+

2 + 3βnk
µ
k
k
2 + 3βnk
µ
µ
k
k
k
2 + n2
1
µ
n
(n
k
k
−
q

1

εnk
k
−
(1 + γn)∆2 + 6βn√1 + γn∆ + 24(√p log n+log n)

µ
2βnk

1)2

−

k



1

n

εn

|

4(1

−

1
n )(1

−

γn)∆2 + 16p
−

n

1 (1

θn)

−

−

−
16βn√1

γn∆ 


−

and

h

P

Z1 >

>P

>P

(cid:16)










Z >

Z >

where

µ
k

q

q

Vn =

(1 + γn)∆2 + 6βn√1 + γn∆ + 24(√p log n+log n)

4(1

1
n )(1

γn)∆2 + 16p
−

n

1 (1

−

−

θn)

−

−

n

1

−
16βn√1

γn∆

−

q
Combining all pieces together, we obtain that

= Φc(Vn),

.

P(
η

= η) > 1

[1

−

−

(1

−

rn)Φc(Vn)]n

rn,

−

provided 4νn 6 γn 6 1, where
e

rn = min

1, n exp(
{

−

Now we choose

cnβ2
n)
}

+ 4n−

1 + 2 exp(

−

pγ2

n/32) + exp(

−

pθ2

n/4).

β2
n =

log2 n
n

,

γ2
n =

16
log n

,

θ2
n =

1
log n

.

, then βn∆ 6 p/n (recall p > log2 n) and there exists a sequence

If 2 log
ξn →

2

n

n < ∆ 6 2 log
0 as n

√n

such that

3/2

n

→ ∞

Vn 6 (1 + ξn)(∆/2 + 3βn + ξn) = o(1),

which implies that P(
η

= η) > 1

cn−

1. If ∆ 6 2 log
n

2

n

, then

−

e

Vn 6 (1 + ξn)

and P(
η

= η) > 1

cn−

1.

−

= o(1)

3∆βn
p/n
2

p

e

High signal size case: 2
2
p/n = o(∆2) and p = O(n). Then the sharp threshold ∆

n 6 ∆ 6 √1

q

−

p log n

α ∆. Note that in this regime, we have

= 8(1 + o(1)) log n, which is

6
6
6
SHARP THRESHOLD OF GAUSSIAN MIXTURES

17

asymptotically independent of p. Thus we place an (essentially one-dimensional) point mass
prior on µ at (∆/2, 0, . . . , 0)T

Rp. A similar calculation yields

∈

1
σ2

L(η

X)

exp

|
∝
= η) > P

n
i

P(
η

n

µ,

ηiXi

,

and

Xi=1
Eo
D
[n], such that

µ, ηiXi

< 0

.

∈

∃
n
i=1 are i.i.d. random variables with
(cid:0)
2 +
k

µ, ηiεii
h

e
> 0

= P

> 0

(cid:10)
= P(Z >

(cid:11)

(cid:1)

µ
) = 1
k

−k

−

Φc(∆/2),

n > 1

−

1

(cid:16)

(cid:1)

−

1
n1

−

δ

(cid:17)

n

> 1

−

nδ

e−

> 1

cn−

1,

−

for some δ depending only on α. Here the constant c

Since

(cid:8)

µ, ηiXii
h
P
µ, ηiXi
(cid:9)
h

we have

(cid:0)
P(
η

(cid:11)

(cid:1)

= η) > 1

1

−

−

µ
k
(cid:0)
Φc(∆/2)

when ∆2 6 4 (2
depending only on δ (and thus only on α).

(cid:0)
δ) log n 6 (1

α)∆

−

−

(cid:1)
2

e

Finally, we deal with the lower dimensional case where p < log2 n. In such regime, we also
= 8(1 + o(1)) log n. Following the same argument as in the high signal size case
(cid:4)

have ∆
under p > log2 n, we conclude that P(
η

= η) > 1

1.

2

cn−

−

6. Discussions
e

In this paper, we characterized the information-theoretic sharp threshold for exact recovery
of Gaussian mixture models. There are still some interesting open questions, which we list
below.

General noise covariance matrix. The SDP relaxation in (11) does not require to know
the noise variance σ2 only in the spherical Gaussian case, i.e., the noise εi has i.i.d. N (0, σ2Ip)
distribution. Consider the general covariance matrix case εi ∼
N (0, Σ) when Σ is not nec-
1/2Xi to make
essarily spherical.
the noise spherical and the sharp threshold in (8) holds and reads in terms of the minimal
Mahalanobis distance

If Σ is known, then we can ﬁrst apply the transform Σ−

∆2 = min

16j<k6K

d2
Σ(µj, µk) = 4

1 +

1 +

s

Kp
n log n !

log n,

e

where d2
µk). If Σ is unknown, [26] showed that in the K = 2
Σ(µj, µk) = (µj −
case the misclassiﬁcation probability of the Bayes classiﬁer decays exponentially fast in the
Mahalanobis distance d2
µ2)T Σ−
. Thus we
conjecture that:

1(µj −
Σ(µ1, µ2) = (µ1 −

µ2) rather than ∆2

σ2 = ∆2

1(µ1 −

µk)T Σ−

Σ

op

k

k

there is a sharp threshold for exact recovery under the general unknown co-
variance matrix given by

∆2 above.

e

Average-case algorithmic hardness in multiple clusters. Both our upper and lower
bounds for exact recovery in Corollary 2.2 and Theorem 2.3 require the number of clus-
ters K = O(log n). We argue that this condition is likely to be necessary for achieving
the sharp threshold of exact recovery. Consider the balanced spherical Gaussian mixture
model with common noise variance and multiple communities for K > 3.
It is shown in
[11] that: (i) detection and partial (i.e., correlated) recovery are information-theoretically

6
6
6
 
18

XIAOHUI CHEN AND YUN YANG

possible if ρ > 2

pK log K
n

+ 2 log K; (ii) detection and partial recovery are impossible if

2p(K

q
1) log(K

1)

−

, where ρ is the squared signal-to-noise ratio in the Gaussian mix-
ρ <
ture model (an equivalent quantity of ∆2/σ2 in our notation). In contrast, it is also known
from [11, 67] that spectral methods have correlated recovery with the true community labels if

q

−

n

p
n (K

−

q

and only if ρ >
1). The phase transition of spectral methods is a direct consequence
of the BBP phase transition in the random matrix theory [8, 56]. Thus for ﬁxed K, there
is no gap (modulo constants) between computation and information theoretic thresholds. In
addition, a suﬃcient condition for partial recovery of the same SDP as in our paper is given
by ∆2
p/n)K in [27]. Based on evidence from statistical physics, it is conjectured
by [40] (and remains as an open problem) that the computational threshold coincides with
the spectral methods for partial recovery for large K, thus suggesting there is a computation-
ally hard regime where no polynomial time algorithm can attain the information-theoretic
threshold when K

σ2 & (1 +

p

.

Now turning into exact recovery. Recall that our result shows that the information-

→ ∞

theoretical threshold is

2

∆
σ2 = 4

log n +

log2 n +

r

Kp log n
n

,

!

which is achieved by an SDP when K . log(n)/ log log(n). Thus, in such growth region of K,
there is no computational hardness for exact recovery, which is a similar scenario in the partial
/σ2 is larger (modulo constants)
recovery case when K = O(1). Note that the threshold ∆
than the partial recovery suﬃcient condition for the spectral methods and the SDP, which
in turn is strictly larger than its necessary condition (i.e., information-theoretic threshold) as
K

2

→ ∞

. Hence, we propose the following conjecture:
for K
case exact recovery information-theoretic threshold.

≫

log n, there is no polynomial time algorithm can achieve the average-

If this conjecture is true, then our current regime K . log(n)/ log log(n) where the SDP
log(n) would be an algorithmic
achieves the information-theoretic limit is sharp, i.e., K
hardness for exact recovery. The conjecture also implies that transition of hardness of clus-
tering Gaussian mixture models from partial recovery and exact recovery is from O(1) and
O(log n), respectively.

≍

Unbalanced communities. Corollary 2.2 and Theorem 2.3 together imply that in the
K , the SDP relaxation (11) for the
equal cluster size case when n1 = n2 =
K-means is minimax-optimal in the sense that sharp phase transition of the probability of
2
in (8). It
wrong recovery from zero to one occurs at the critical threshold given by the ∆
remains an interesting open question whether the separation gap ∆ is sharp when cluster sizes
are unbalanced.

= nK = n

· · ·

7. Proof of key lemmas

7.1. Proof of Lemma 4.1. Without loss of generality, we may assume σ = 1. Denote
(i)
θ = µk −
kl , where

µl and deﬁne the event

k,l,i A

=

(i)
kl =

A

Xi −
k

n

A
2

Xlk

T
Xi −

2 >

Xkk

− k

nk + nl
nknl

p + β

θ
k

2
k

−

rkl

,

o

 
SHARP THRESHOLD OF GAUSSIAN MIXTURES

19

with the index (k, l, i) ranging over all distinct pairs (k, l)

[K]2 and all i

∈

G∗k and

∈

rkl = 2

s

2 log(nK)
nl

θ
k

k

+ 2

nk + nl
nknl

2p log(nK) +

4
nk

log(nK).

G∗k and k
2
εlk
εik
k

−

2

− k

(cid:18)

p
[K]. We can write
∈
εi −
nk −

2 =
2

εkk
1
nk (cid:19)

2
}k

εk
k

θ
h

i
\{

−

−

−
θ, εli
2
h

εl + εk, θ

εl + 2εi −

εki

2

Recall that Xi = µk + εi for each i
Xi −
εlk
k

Xi −
Xlk
k
θ
=
k

Xkk
1
2 +
nk (cid:18)

− k
2 +
k

2 =

2

∈
θ + εi −
k
1
nk (cid:19)
1
εk
nk (cid:19)
G∗

2

i
k\{

}

∈

−
nk −

εl +

(cid:18)
1

j

1)−

i
\{

, εi

}

,

+

i
\{

B

+ 2

θ

−

}

*
= (nk −
(i)
εlk
kl =
k
εk
k
εl −
k
θ, (1
h

n

i
\{

where εk

εj. Set ζn = 2 log(nK) and deﬁne

P
2 > n−
l

1

(p

2

2 > p

2

pζn,

pζn),

εik
k
1(p + 2

1

−

p
1)−

p
pζn + 2ζn),

−
2 6 (nk −
}k
k )2εk
n−
(1
−
k )2εk
n−
are mutually independent. Thus conditional on εl and εk

θ, εli
h
1
k )(p + 2

1
2 6 (n−
p
l + n−

q
pζn + 2ζn),

p
θ
k )ζnk

}k
εli

θ
l ζnk

1
l + n−

k
o

2(n−

2n−

} −

,
k

i
\{

i
\{

q

−

6

6

1

1

1

.

(34)

(35)

, we

i
\{

}

Note that εi, εl, and εk
have

i
\{

}

k )2εk
n−
Then, on the event where (34) and (35) hold, we can bound

εii ∼

εl + (1

θ
h

i
\{

N

−

−

−

−

0,

(cid:16)

θ

}

1

,

εl + (1

U ∗ := P

θ
2
h

εl + (1

1

k )2εk
n−

,

i
\{

}

−

−

(cid:16)
= Φc





2

q

θ
k

2 + 2
θ, (1
h
k

−

1
k )2εk
n−

1

k )2εk
n−

i
\{

}

−

.

(cid:17)

2

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
> (1

−

εii
(1

θ
β)
k

2
k

−

εl, εk

i
\{

}

(cid:17)

2
θ
β)
k
k
εli

} −

+

−
i
\{

(cid:12)
(cid:12)
(cid:12)
εl −
(1
k

1
k )2εk
n−

i
\{

−

2 

}k



6 Φc 

−
θ
k )ζnk
where Φc(t) denotes the tail probability P(Z > t) for a standard normal random variable Z.
1. Under the separation condition (24) on the Gaussian centers,
Note that n−
we have

1
k )(p + 2√pζn + 2ζn)

θ
2
rk

1
l + n−

1
l + n−

2 + 2
k

2(n−









q



k

1

,

1
1
k 6 2m−
l + n−
2 > 8 log n and
k

θ
k

(1

2
θ
β)
k
k
+ (n−

Thus, on events (34) and (35), we have

2(n−

1
l + n−

1

θ
k )ζnk

2
√m k

θ

2.
k

6

k

2
q

U ∗ 6 Φc



2



q

(1 + 2

θ
√m )
k

(1

2
θ
β)
k
−
k
k )(p + 2√pζn + 2ζn) 
1
2 + (n−
l + n−
k


1

.

20

XIAOHUI CHEN AND YUN YANG

Now under the separation condition (24) and noting that (1 + δ)(1 + 2
that

√m ) 6 1 + 2δ, we see

where

Hence we get

β)2

(1

−

8(1 + δ) log n k

θ

1 +

4
k

−

(cid:18)

2
√m

(cid:19)

θ
k

2
k

−

r1 > 0,

r1 = p(n−

1
l + n−

1
k ) + 2(

pζn + ζn)(n−

1
l + n−

1
k ).

p

U ∗ 6 Φc(

2(1 + δ) log n) 6 n−

(1+δ),

x2/2 for
where the second inequality follows from the standard Gaussian tail bound Φc(x) 6 e−
x > 0. In addition, applying the probability tail bounds for χ2 distributions in Lemma 8.1,
we have P(

) 6 6/(n2K 2). Now putting pieces together, we have
c

(i)
kl

p

B

c

c

P(

c) 6

A

P(

(i)
kl

A

(i)

kl ) + P(
B

(i)
kl

)

∩ B

X16k

G∗
=l6K Xi
k

∈

6

6

∈

X16k
K 2
nδ +

G∗
=l6K Xi
k
8
n

.

E [U ∗1((34), (35) hold)] +

6
n

7.2. Proof of Lemma 4.2. Without loss of generality, we may assume σ = 1. Recall that
are denoted by c(k,l) =
G∗l
the column sums and row sums of matrix BG∗
and
kG∗
∈
r(k,l)
c(k,l)
r(k,l) =
=
G∗
(cid:0)
i
i
j
k
∈
c(k,l)
]ij = r(k,l)
total sum, and the construction of B♯ in (22) can be written as [B♯
P
l G∗
G∗
j
ΓK, we may write
for any distinct pair (k, l)

, respectively. In addition, t(k,l) =

is the
(cid:1)
/t(k,l),

r(k,l)
i

c(k,l)
j

G∗k

: j

: i

G∗
l

P

∈

(cid:1)

(cid:0)

∈

j

k

i

l

[K]2. Under this notation, for each v
∈
r(k,l)
i

K

∈

c(k,l)
j
t(k,l)

vi vj =

vi r(k,l)
i

Xk=1 Xl
Using once again the property

G∗
G∗
=k Xi
k Xj
l
∈
∈

=k (cid:26)

Xk=1 Xl
vi = 0 for all k

G∗
(cid:16) Xi
k
∈

G∗
(cid:17)(cid:16) Xj
l
∈
[K], we can simplify

i

G∗
k

∈

vj c(k,l)

j =

vj

P
−

nl + nk
2nl

λ +

nk
2

2

Xkk
(
k

Xlk

2) + nkh

Xj, Xl −

− k

vj c(k,l)
j

.
(cid:17)(cid:27)

Xki
(cid:21)

K

T (v) =

G∗
Xj
l
∈

1
t(k,l)

∈

G∗
(cid:20)
Xj
l
∈
Xl −
=nkh

=nkh

Xl −

=nkh

µl −

vjXji

vjεji

Xk,

Xk,

G∗
Xj
l
∈

G∗
Xj
l
∈
µk + εl −

εk,

vj εji
.

G∗
Xj
l
∈
εl,
µl + εk −
c(k,l)
j =

−

= nlh

µk −
vivj r(k,l)
i

i

G∗
k

. Then

vi εii
nknl(T1,kl + T2,kl + T3,kl),

P

∈

Similarly,

vi r(k,l)
i

i

G∗
k

∈

P

G∗
G∗
Xi
k Xj
l
∈
∈

6
6
6
6
and

where

SHARP THRESHOLD OF GAUSSIAN MIXTURES

21

T (v) =

−

K

Xk=1 Xl

=k (cid:26)

nknl
t(k,l)

(T1,kl + T2,kl + T3,kl)

,

(36)

(cid:27)

T1,kl(v) =

G∗
(cid:20) Xi
k

∈

vih

µk −

µl, εii
(cid:21)

·

vjh

µk −

,

µl, εji
(cid:21)

G∗
(cid:20) Xj
l

∈

T2,kl(v) =

G∗
(cid:20) Xi
k

∈

vih

εk −

εl, εii
(cid:21)

·

vjh

εk −

,

εl, εji
(cid:21)

G∗
(cid:20) Xj
l

∈

T3,kl(v) =

G∗
(cid:20) Xi
k

∈

vih

µk −

µl, εii
(cid:21)

+

G∗
(cid:20) Xi
k

∈

·

∈

G∗
(cid:20) Xj
l
εk −
vih

vjh

εk −

εl, εji
(cid:21)

εl, εii
(cid:21)

·

G∗
(cid:20) Xj
l

∈

vjh

µk −

.

µl, εji
(cid:21)

To bound these three terms, we will use the following lemma, whose proof is deferred to the
end of this section.

Lemma 7.1 (Uniform high probability bounds for random ﬂuctuation terms). For any δ > 0,
it holds with probability at least 1
ΓK and any distinct pair
(k, l)

δ that for any v

4K 2n−

[K]2,

−

∈

∈

vih

µk −

6

µk −
k

µlk

nk +

2nk log n + 2 log n

1/2

1/2

v2
i

,

(37)

G∗
Xi
k
∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)

G∗
Xi
k
∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)

G∗
Xi
k
∈
1
εk

nk −
nk

vi

G∗
Xi
k
∈

D

(cid:12)
(cid:12)
(cid:12)
(cid:12)

µl, εii
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

vi k

εik

(cid:12)
(cid:12)
(cid:12)
(cid:12)
εl, εii

vih

, εi

i
\{

}

E(cid:12)
(cid:12)
(cid:12)
(cid:12)

for some universal constant C > 0.

(cid:0)

p

6 Cp1/2[n1/2

k + log2(n) ]

G∗
(cid:16) Xi
k
∈

6 C

(p + log n) nk
nl

s

6 Cp1/2(δ log n)1/2

v2
i

G∗
(cid:16) Xi
k
∈
v2
i

(cid:17)
1/2

,

G∗
(cid:16) Xi
k
∈

(cid:17)

G∗
(cid:16) Xi
k
∈

(cid:17)

(cid:1)

1/2

,

v2
i

(cid:17)

1/2

,

(38)

(39)

(40)

Bound T1,kl: By applying the Cauchy-Schwarz inequality and inequality (37), we can bound

T1,kl(v)
|
|

=

µk −
6
k

µlk

2

G∗
Xi
k
∈

(cid:12)
(cid:12)
(cid:12)

vih

µk −

1/2

v2
i

µl, εii

vjh

µk −

G∗
Xj
l
∈
1/2
v2
j

nk +

µl, εji
(cid:12)
(cid:12)
(cid:12)

2nk log n + 2 log n

G∗
(cid:16) Xj
l
∈
Throughout the proof, we can always work under the event

G∗
(cid:16) Xi
k
∈

p

(cid:17)

(cid:17)

(cid:0)

1/2

nl +

2nl log n + 2 log n

(cid:1)

(cid:0)

p

1/2.

(cid:1)

t(k,l) > βnknlk
{

µk −

2/2 for all distinct pairs (k, l)

µlk

[K]2 and i

∈

,
G∗k}

∈

(41)

6
22

XIAOHUI CHEN AND YUN YANG

which according to the choice of λ♯ in (25) after Lemma 4.1, holds with probability at least
1. Under this event, we get a uniform bound for ﬁrst sum of T1,kl’s in the
1
decomposition (36) of T (v) for all v

K 2n−

8n−

−

−

δ

ΓK:

∈

nknk
t(k,l)

K

(cid:12)
(cid:12)
Xk,l=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

T1,kl(v)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

6

2
β

K

(cid:26)

G∗
Xk=1 (cid:16) Xi
k
K

∈

1/2

nk +

2nk log n + 2 log n

(cid:17)

(cid:0)

p

1/2

nl +

2nl log n + 2 log n

v2
i

v2
j

1/2

(cid:27)

(cid:1)

1/2

·

(cid:26)

(a)
6

2
β

G∗
Xl=1 (cid:16) Xj
l
K

∈

(cid:17)
1/2

(cid:0)

p

K

(cid:27)

(cid:1)

(cid:18)

G∗
Xk=1 Xi
k
∈
K

v2
i

(cid:19)

1/2

·

(cid:18)

Xk=1
K

(cid:0)

nk +

2nk log n + 2 log n

p

1/2

(cid:19)
(cid:1)

1/2

·

(cid:18)
2
β

(cid:0)

6

G∗
Xl=1 Xj
l
∈
n +

p

v2
j

(cid:19)

·

(cid:18)

Xl=1

(cid:0)

2nK log n + 2K log n

nl +

2nl log n + 2 log n

.

(cid:19)

(cid:1)

p
v
k

2,
k

(cid:1)

where step (a) is due to the Cauchy-Schwarz inequality, and the last step uses the identity

K
k=1 nk = n and inequality

K
k=1 √nk 6

K

K
k=1 nk.

Bound T2,kl: Due to the symmetry, we only need to analyze the ﬁrst sum in T2,kl, which can
P
be further decomposed as

P

P

q

vih

εk −

εl, εii

=

G∗
Xi
k
∈

G∗
Xi
k
∈

=

vi

D
vi
nk k

1
nk

εi +

nk −
nk

1

εk

εl, εi

i
\{

} −

2

εjk

−

vih

εl, εii

+

G∗
Xi
k
∈

G∗
Xi
k
∈

=: G1(v) + G2(v) + G3(v),

E

vi

G∗
Xi
k
∈

D

1

nk −
nk

εk

i
\{

}

, εi

E

where the three terms G1(v), G2(v) and G3(v) are respectively bounded by using inequali-
ties (38), (39) and (40) in Lemma 7.1. Therefore, we can reach

vih

εk −

εl, εii

6 C

(cid:18)r

G∗
Xi
k
∈

p
nk

+

log2(n)√p
nk

(p + log n) log nk
nl

+

s

+

δp log n

1/2

v2
i

p

G∗
(cid:19)(cid:16) Xi
k

∈

(cid:17)

6 C ′

δp log n + log2(n)

(cid:18)

p

r

p
n

1/2

.

v2
i

G∗
(cid:19)(cid:16) Xi
k

∈

(cid:17)

This implies the following bound on T2,kl due to the symmetry,

T2,kl(v)
|
|

6 C ′′

δp log n +

(cid:18)

p log4(n)
n

1/2

v2
i

1/2

.

v2
j

G∗
(cid:19)(cid:16) Xi
k

∈

(cid:17)

G∗
(cid:16) Xj
l
∈

(cid:17)

SHARP THRESHOLD OF GAUSSIAN MIXTURES

23

Then we may obtain by using the lower bound condition in Lemma 4.1 as
C1(1

(1 + δ) p log n/m that under the event (41),

β)−

1

−

µk −
k

2 >

µlk

K

K

1/2

v2
i

1/2

v2
j

G∗
Xk=1 (cid:16) Xi
k

∈

(cid:17)

(cid:19)(cid:18)

G∗
Xl=1 (cid:16) Xj
l

∈

(cid:17)

(cid:19)

nknk
t(k,l)

K

(cid:12)
(cid:12)
Xk,l=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p
T2,kl(v)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

6

C2(1
−
β

C2(1

6

−
β

β)

δ

mp log n +

(cid:16)
β)K

p

p

δ

mp log n +

mp log7 n
n

(cid:17)(cid:18)
mp log7 n
n

p

where the last step is due to the Cauchy-Schwarz inequality.

2,
k

v
k
(cid:17)

Bound T3,kl: Note that term

satisﬁes

(cid:16)

p

T3,kl(v)
|
|
µk −

vih

T3,kl(v)
|
|

6

1
2

µl, εii
(cid:19)

2

+

2

1
2

+

vih

εk −

εl, εii
(cid:19)

2

vjh

µk −

µl, εji
(cid:19)

vjh

εk −

εl, εji
(cid:19)

2

.

∈

G∗
(cid:18) Xj
l
1
2

G∗
(cid:18) Xj
l

∈

∈

G∗
(cid:18) Xi
k
1
2

+

G∗
(cid:18) Xi
k

∈

T3,kl(v)
|
|

can be bounded by the sum of the upper bounds for

T1,kl(v)
|
|

and

Therefore,
T2,kl(v)
.
|
|

Putting all pieces together, we can ﬁnally reach

T (v)
|
|

6

6

K

K

T1,kl

+

nknl
t(k,l)

K

T2,kl

+

nknl
t(k,l)

nknl
t(k,l)

T3,kl

(cid:12)
=k
Xk=1 Xl
(cid:12)
(cid:12)
C3
(cid:12)
2
β k
k

v

(cid:16)

=k
Xk=1 Xl

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n + K log n + (1

β)Kδ

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
mp log n +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=k
Xk=1 Xl
mp log7 n
n

p

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:17)

p

7.3. Proof of Lemma 7.1. We can apply the Cauchy-Schwarz inequality to obtain

µk −
k

µlk

−

1

G∗
Xi
k
∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Since

vih

µk −

µl, εii
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µk −
µk −
k

µl
µlk

D

, εi

E

6

1/2

v2
i

G∗
(cid:16) Xi
k
∈

(cid:17)

G∗
(cid:16) Xi
∈

k D

µk −
µk −
k

µl
µlk

2

1/2

.

, εi

E

(cid:17)

i.i.d.
∼

N (0, 1),

i = 1, . . . , n,

we obtain by Lemma 8.1 and a union bound argument that with probability at least 1

K 2n−

1,

−

µk −
µk −
k

µl
µlk

2

, εi

E

G∗
Xi
∈

k D

6 nk +

2nk log n + 2 log n for all k, l

[K].

∈

p
A combination of the preceding two displays yields the ﬁrst claimed inequality (37).

i

∈
i

G∗
k
G∗
k

vi = 0 for any v
ΓK, we can also write the left hand side of inequality (38) as
Since
G1(vk) =
εik
vi(
p), which can be viewed as a centered empirical process indexed by
P
k
Rnk, the restriction v↾G∗
vk ∈
ΓK onto G∗k. We may assume without loss of generality
∈
vkk
v↾G∗
that vk ∈
. By Theorem 4 in [2], there exists a universal
= 1
}
k
{
constant C such that for any t > 0,

∈
P
Vk :=

of v
ΓK,

: v

−

∈

∈

2

k

k

P

G1k
k
(cid:16)

Vk > 2 E[
G1k
k

Vk ] + t

6 exp

(cid:17)

t2
3τ 2
1

(cid:17)

−

(cid:16)

+ 3 exp

t
M1kψ1
k

.

(cid:17)

−

C

(cid:16)

6
6
6
24

XIAOHUI CHEN AND YUN YANG

Vk = supvk∈
where
G1k
k
M1 = maxi
maxvk∈
G∗
Vk |
k
Lemma 2.2.2 in [60] and Lemma 8.3, we have

G1(vk)
Vk |
|
2
εik
vi(
−
k

and τ 2
p)
|

6 maxi

1 = supvk∈
Vk
2
εik
G∗

k |k

∈

∈

2

v2
i

E[
εik
k

p]2 6 2p, and
G∗
i
−
k
∈
. By the maximal inequality in
p
P
|
−

M1kψ1 6 C log(nk) max
k

G∗

i

2

εik

p

−

k kk

∈

By the Cauchy-Schwarz inequality, we have for all v

kψ1 6 Cp1/2 log(nk).
Vk,

∈

2

εik
vi(
k

−

p)

6

1/2

vi

G∗
Xi
k
∈

(cid:17)
Then Jensen’s inequality implies that

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

G∗
(cid:16) Xi
k
∈

2

εik
(
k

−

p)2

G∗
(cid:16) Xi
k
∈

1/2

6

(cid:17)

G∗
(cid:16) Xi
k
∈

2

εik
(
k

−

p)2

1/2

.

(cid:17)

Thus with probability at least 1

E[
G1k
k

Vk ] 6

2

E(
εik
k

−

G∗
h Xi
k
∈
1, we have
4n−
Vk 6 Cp1/2[n1/2

−
G1k
k

1/2

p)2

= (2nkp)1/2.

i

k + log2(n)],

(42)

which entails the second claimed inequality (38).

Next we prove the third claimed inequality. Note that conditional on εl, G2(vk) :=
Vk.

εl,
h
By the Borell-Sudakov-Tsirel’son inequality (cf. Theorem 2.5.8 in [25]), we have

v2
i ) is a centered Gaussian process indexed by vk ∈

viεii ∼

εlk
k

N (0,

G∗
k

G∗
k

∈

∈

2

i

i

P

P

G2k
k
(cid:16)
2 supvk∈
Vk

P
Vk > E[
G2k
k
v2
i 6

Vk |
εlk
k

where τ 2
εlk
G∗
k
k
Corollary 2.2.8 in [60]) yields that

2 =

∈

i

P

εl] + τ2

2 log n

εl

6 n−

1,

2. Then Dudley’s entropy integral bound (cf.

p

(cid:17)

(cid:12)
(cid:12)
(cid:12)

εl] 6 C
where we have used the fact that the ε-covering entropy of the unit sphere in Rnk is at most
Cnk log(1/ε) for any ε

εlk
k

Vk |

E[
G2k
k

n1/2
k ,

(0, 1). Combining the last two displays with the inequality
∈
P(
εlk
k

p log n + 2 log n)) 6 n−

2 > n−
l

(p + 2

1

and a union bound argument, we get with probability at least 1

p

−

1,
K 2n−

1,

implying the third inequality (39).

G2k
k

Vk 6 C

s

(p + log n)nk
nl

,

(43)

Now we prove the last inequality. Note that
nk −
nk

1
nk

, εi

i
\{

εk

vi

=

1

}

G∗
Xi
k
∈

D

E

G∗
(i,j)
X{
∈

k:i

vih

εi, εji

=:

1
nk

U1(vk),

=j

}

where

U1(vk) =

1
2

εi, εji
(vi + vj)
h

=j
is a degenerate U -process of order two. To simplify the notation, we may assume G∗k =
1, . . . , nk}
{

in the rest of this proof. Applying Lemma 8.4 with

k:i

}

G∗
(i,j)
X{
∈

=

A
{

A

Idp |

⊗

A =

aij}i,j
{

[nk], aij = (vi + vj)/2, vk ∈

∈

Vk}
,

6
6
6 sup

A

A k

ε
Idpkopk

k

⊗

6 n1/2
ε
l k

.
k

6 4nk E[
ε
k

2] = 4n2
k

kp.

SHARP THRESHOLD OF GAUSSIAN MIXTURES

25

we get

where

P

> t

E[
U1k
U1k
Vk ]
Vk −
k
k
(cid:16)(cid:12)
(cid:12)
(cid:12)
(cid:12)
Vk U1(vk), εT = (εT
Vk = supvk∈
U1k
(cid:12)
(cid:12)
k
= E

(cid:17)

6 2 exp

(A

ε
k

kA

sup
A

h
By the Cauchy-Schwarz inequality,

C min

t2
ε
k
nk ), and

(cid:16)

2
k
A

−
h
1 , . . . , εT

,

supA

t
A

∈A k

Idpkop

⊗

(cid:17)i

Idp + AT

⊗

.

i

Idp)ε
(cid:13)
(cid:13)
(cid:13)

⊗

(cid:13)
(cid:13)
(cid:13)
vi + vj
2

nl

uiuj

Xi,j=1

nk

6 max
u
=1
k

k

nk

1/2

nk

u2
i

v2
i

nk

1/2

1/2

u2
j

k 6 n1/2
n1/2
k .

(cid:16)

Xi=1

(cid:17)

(cid:16)

Xi=1

(cid:17)

(cid:16)

Xj=1

(cid:17)

kop = max
A
k

=1

u

k

k

uT Au = max
=1

u

k

k

nk

= max
u
=1
k

k

Xi=1
(cid:16)
Idpkop =

Since

A
k

⊗

uivi

uj

Xj=1
(cid:17)
(cid:17)(cid:16)
kop, we have
A
k
Idp)ε
(A
sup
A k

⊗

k

Then Jensen’s inequality yields that
2
k
A

ε
k
Vk ], we note that

To bound E[
U1k
k
=

U1(vk)
|
|

nk

nk

1/2

nk

vj

εj,

εj

6

v2
j

εj,

εj

1/2

2

Xj=1
nk

(cid:12)
(cid:12)
(cid:12)
(cid:12)

6

D

εj,

=j,i
Xi
∈

[nk]

εj

E(cid:12)
(cid:12)
(cid:12)
2
(cid:12)

1/2

(cid:18)

Xj=1

(cid:19)

(cid:18)

Xj=1 D

=j,i
Xi
∈

[nk]

(cid:19)

E

.

=j,i
Xi
∈
From Jensen’s inequality and the independence between εj and

Xj=1 D

[nk]

E

(cid:18)

(cid:19)

i

=j,i

E[
U1k
k

Vk ] 6

nk

E

εj,

(cid:18)

Xj=1

(cid:20)D

Thus we see that with probability at least 1

1/2

2

=

εj

nk

E

E

(cid:21)(cid:19)

Xj=1

(cid:20)(cid:13)
(cid:13)
(cid:13)
2n−
Vk 6 Cnkp1/2(δ log n)1/2,

(cid:18)
δ,

−

=j,i
Xi
∈

[nk]

U1k
k

which implies the last claimed inequality (40).

[nk] εj, we have
∈
1/2

2

6 nkp1/2.

P

=j,i
Xi
∈

[nk]

εj

(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

Lemma 8.1 (Tail bound for χ2 distributions). If Z

N (0, Idp), then for all t > 0,

8. Supporting lemmas

∼

P(
2 > p + 2√pt + 2t) 6 e−
Z
k
k
P(
2√pt) 6 e−
Z
k

−

t,
t.

2 6 p
k
Proof of Lemma 8.1. See Lemma 1 in [38].

Lemma 8.2 (Deviation of Gaussian random matrices). If
then

E ∈

Rp

×

n has i.i.d. N (0, σ2) entries,

P

kEkop > σ(√n + √p + √2t )
(cid:1)
(cid:0)

6 e−

t,

t > 0.

∀

(44)

(cid:4)

6
6
6
6
6
6
26

XIAOHUI CHEN AND YUN YANG

Proof of Lemma 8.2. See Corollary 5.35 in [62].

(cid:4)

Lemma 8.3. Let ε1, ε2 be i.i.d. N (0, Idp). Then there exists a universal constant C such that

2

p

ε1, ε2ikψ1 6 Cp1/2.
ε1k
kh
p
Proof of Lemma 8.3. Note that
j=1 ε1jε2j , and each additive component ε1jε2j
ε2j kψ2 = 1 (cf. Lemma 2.7.7 in [63]). By
is sub-exponential with
Bernstein’s inequality (cf. Theorem 2.8.2 in [63]), there exists a universal constant C1 such
that for any t > 0,

kψ1 +
−
ε1, ε2i
=
h
ε1jε2j kψ1 6
ε1j kψ2k
P
k
k

kk

P(

ε1, ε2i|
|h

> t) 6 2 exp[

C1 min(t2/p, t)].

−

Let C be a large positive real number. By integration-by-parts and change-of-variables, we
have

ε1, ε2i|
|h
C

E

exp

h

(cid:16)

(cid:17)i

=

=

=

1

Z

1

Z

0

Z

62

∞

P

exp

ε1, ε2i|
|h
C

> t

dt

(cid:16)

(cid:16)

∞

P

∞

P

(cid:16)
ε1, ε2i|
|h

(cid:17)

> C log t

(cid:17)
dt

ε1, ε2i|
|h

> Cx

(cid:17)
ex dx
(cid:17)
p x2+x dx + 2

2

∞

C1C

(cid:16)
p/C
e−

e−

(C1C

1)x dx

−

0

Z

p
4C1C

62e

πp
C1C 2 +

2
C1C

2

r

p/C

(C1C

e−

2) p

C .

−

Z

2

−

Thus if we take C = Kp1/2 for some large enough universal constant K > 0, then

E

ε1, ε2i|
|h
C
ε1, ε2ikψ1 6 Kp1/2. The ψ1 norm bound for
kh

6 2,

exp

(cid:17)i

(cid:16)

h

which implies that
similar lines.

2

ε1k
k

−

p follows from
(cid:4)

Lemma 8.4 (Uniform Hanson-Wright inequality for Gaussian quadratic forms). Let ε
N (0, Idp) and

p matrices. Consider the random variable

be a bounded class of p

∼

A

×
(εT Aε

E[εT Aε]).

−

Z = sup

A

∈A

Then there exists a universal constant C such that for any t > 0,

P(
Z
|
−
= E[supA

where

ε
k

kA

(A + AT )ε

].
k

∈A k

E[Z]
|

> t) 6 2 exp

C min

−

h

t2
ε
k

2
k
A

,

(cid:16)

supA

t

A

kop

∈A k

,

(cid:17)i

Proof of Lemma 8.4. Note that the standard Gaussian random vector ε satisﬁes the concen-
tration inequality

for any t > 0 and every 1-Lipschitz function ϕ : Rp
2.5.7 in [25]). Then the lemma follows from Theorem 2.10 in [3].

→

∞

(cf. Theorem
(cid:4)

P(
ϕ(ε)
|

E[ϕ(ε)]
|

−

> t) 6 2 exp(

t2/2)
−
R such that E[
ϕ(ε)
] <
|
|

SHARP THRESHOLD OF GAUSSIAN MIXTURES

27

Acknowledgement

The authors would like to thank two anonymous referees and the Associate Editor Prof.
Stephane Boucheron for their many constructive comments. X. Chen’s research was supported
in part by NSF CAREER Award DMS-1752614, UIUC Research Board Award RB18099, and
a Simons Fellowship. Y. Yang’s research was supported in part by NSF DMS-1810831. X.
Chen acknowledges that part of this work was carried out in the Institute for Data, System,
and Society (IDSS) at Massachusetts Institute of Technology.

References

[1] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In Peter
Auer and Ron Meir, editors, Learning Theory, pages 458–469, Berlin, Heidelberg, 2005. Springer Berlin
Heidelberg.

[2] Radoslaw Adamczak. A tail inequality for suprema of unbounded empirical processes with applications to

markov chains. Electron. J. Probab., 13:1000–1034, 2008.

[3] Radoslaw Adamczak. A note on the hanson-wright inequality for random vectors with dependencies.

Electron. Commun. Probab., 20:13 pp., 2015.

[4] Arash A. Amini and Elizaveta Levina. On semideﬁnite relaxations for the block model. Ann. Statist.,

46(1):149–179, 2018.

[5] David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings
of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, pages 1027–1035,
Philadelphia, PA, USA, 2007. Society for Industrial and Applied Mathematics.

[6] Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, and
Rachel Ward. Relax, no need to round: Integrality of clustering formulations. In Proceedings of the 2015
Conference on Innovations in Theoretical Computer Science, ITCS ’15, pages 191–200, New York, NY,
USA, 2015. ACM.

[7] Pranjal Awasthi and Or Sheﬀet. Improved spectral-norm bounds for clustering. In Anupam Gupta, Klaus
Jansen, Jos´e Rolim, and Rocco Servedio, editors, Approximation, Randomization, and Combinatorial Op-
timization. Algorithms and Techniques, pages 37–49, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
[8] Jinho Baik, G´erard Ben Arous, and Sandrine P´ech´e. Phase transition of the largest eigenvalue for nonnull

complex sample covariance matrices. Ann. Probab., 33(5):1643–1697, 09 2005.

[9] Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algorithm:

From population to sample-based analysis. Ann. Statist., 45(1):77–120, 02 2017.

[10] Afonso S. Bandeira. Random laplacian matrices and convex relaxations. Foundations of Computational

Mathematics, 18(2):345–379, 2018.

[11] J. Banks, C. Moore, R. Vershynin, N. Verzelen, and J. Xu. Information-theoretic bounds and phase tran-
sitions in clustering, sparse pca, and submatrix localization. IEEE Transactions on Information Theory,
64(7):4872–4894, 2018.

[12] Florentina Bunea, Christophe Giraud, Martin Royer, and Nicolas Verzelen. PECOK: a convex optimization

approach to variable clustering. arXiv:1606.05100, 2016.

[13] Jiahua Chen. Optimal rate of convergence for ﬁnite mixture models. Ann. Statist., 23(1):221–233, 02 1995.
[14] Xiaohui Chen and Yun Yang. Diﬀusion k-means clustering on manifolds: provable exact recovery via
semideﬁnite relaxations. Applied and Computational Harmonic Analysis, to appear (arXiv:1903.04416v4),
2020.

[15] Xiaohui Chen and Yun Yang. Hanson-wright inequality in hilbert spaces with application to k-means

clustering for non-euclidean data. Bernoulli, 27(1):586–614, 2021.

[16] Sanjoy Dasgupta. The hardness of k-means clustering. Technical Report CS2007-0890, University of Cal-

ifornia, San Diego, 2007.

[17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suﬃce for mixtures
of two gaussians. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning
Theory, volume 65 of Proceedings of Machine Learning Research, pages 704–710, Amsterdam, Netherlands,
2017. PMLR.

[18] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em
algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1–38, 1977.

28

XIAOHUI CHEN AND YUN YANG

[19] Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Michael I. Jordan, Martin J. Wainwright, and Bin Yu. Singu-

larity, misspeciﬁcation, and the convergence rate of em. arXiv:1810.00828v1, 2018.

[20] Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Michael I. Jordan, Martin J. Wainwright, and Bin Yu. Chal-

lenges with em in application to weakly identiﬁable mixture models. arXiv:1902.00194v1, 2019.

[21] M.E. Dyer and A.M. Frieze. The solution of some random np-hard problems in polynomial expected time.

Journal of Algorithms, 10(4):451–489, 1989.

[22] Abbe Emmanuel, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block model.

IEEE Transactions on Information Theory, 62(1):471–487, 2016.

[23] Yingjie Fei and Yudong Chen. Hidden integrality of sdp relaxation for sub-gaussian mixture models.

arXiv:1803.06510, 2018.

[24] Chris Fraley and Adrian Raftery. Model-based clustering, discriminant analysis, and density estimation.

Journal of American Statistical Association, 97(458):611–631, 2002.

[25] Evarist Gin´e and Richard Nickl. Mathematical Foundations of Inﬁnite-Dimensional Statistical Models.

Cambridge University Press, 2016.

[26] Christophe Giraud. Introduction to high-dimensional statistics. Volume 139 of Monographs on Statistics

and Applied Probability. CRC Press, Boca Raton, FL, 2015.

[27] Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed kmeans.

arXiv:1807.07547v3, 2018.

[28] Olivier Gu´edon and Roman Vershynin. Community detection in sparse networks via Grothendieck’s in-

equality. Probability Theory and Related Fields, 165:1025–1049, 2016.

[29] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semideﬁnite

programming. IEEE Transactions on Information Theory, 62(5):2788–2797, 2016.

[30] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semideﬁnite

programming: Extensions. IEEE Trans. Inf. Theor., 62(10):5918–5937, 2016.

[31] Pierre Hansen and Brigitte Jaumard. Cluster analysis and mathematical programming. Mathematical

Programming, 79(1):191–215, Oct 1997.

[32] P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks,

5:109–137, 1983.

[33] Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, and Soledad Villar. On the tightness of an SDP

relaxation of k-means. arXiv e-prints, page arXiv:1505.04778, May 2015.

[34] Ravindran Kannan and Santosh Vempala. Spectral algorithms. Found. Trends Theor. Comput. Sci., 4:157–

288, March 2009.

[35] Jason M. Klusowski and W. D. Brinda. Statistical guarantees for estimating the centers of a two-component

gaussian mixture by em. arXiv:1608.02280v1, 2016.

[36] Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov´a, and
Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of
Sciences, 110(52):20935–20940, 2013.

[37] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In Pro-
ceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS ’10, pages
299–308, Washington, DC, USA, 2010. IEEE Computer Society.

[38] B. Laurent and Massart P. Adaptive estimation of a quadratic functional by model selection. Ann. Statist.,

28(5):1302–1338, 2000.

[39] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models. Ann.

Statist., 43(1):215–237, 2015.

[40] Thibault Lesieur, Caterina De Bacco, Jess Banks, Florent Krzakala, Cris Moore, and Lenka Zdeborov´a.
Phase transitions and optimal algorithms in high-dimensional gaussian mixture clustering. In Commu-
nication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on, pages 601–608.
IEEE, 2016.

[41] Xiaodong Li, Yudong Chen, and Jiaming Xu. Convex relaxation methods for community detection.

arXiv:1810.00315, 2018.

[42] Xiaodong Li, Yang Li, Shuyang Ling, Thomas Stohmer, and Ke Wei. When do birds of a feather ﬂock

together? k-means, proximity, and conic programming. arXiv:1710.06008, 2017.

[43] Stuart Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28:129–137,

1982.

[44] Yu Lu and Harrison Zhou. Statistical and computational guarantees of lloyd’s algorithm and its variants.

arXiv:1612.02099, 2016.

SHARP THRESHOLD OF GAUSSIAN MIXTURES

29

[45] J.B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. Proc. Fifth

Berkeley Sympos. Math. Statist. and Probability, pages 281–297, 1967.

[46] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-means problem is np-hard.
In Sandip Das and Ryuhei Uehara, editors, WALCOM: Algorithms and Computation, pages 274–285,
Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.

[47] Laurent Massouli´e. Community detection thresholds and the weak ramanujan property. In Proceedings of
the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC ’14, pages 694–703, New York,
NY, USA, 2014. ACM.

[48] Marina Meila and Jianbo Shi. Learning segmentation by random walks. In In Advances in Neural Infor-

mation Processing Systems, pages 873–879. MIT Press, 2001.

[49] Dustin G. Mixon, Soledad Villar, and Rachel Ward. Clustering subgaussian mixtures by semideﬁnite

programming. arXiv:1602.06612v2, 2016.

[50] Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction and optimal

recovery of block models. Ann. Appl. Probab., 26(4):2211–2256, 2016.

[51] Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for the planted bisection model.

Electron. J. Probab., 21:24 pp., 2016.
[52] Mohamed Ndaoud. Sharp optimal

arXiv:1812.08078v2, 2019.

recovery in the

two component gaussian mixture model.

[53] Abhinav Nellore and Rachel Ward. Recovery guarantees for exemplar-based clustering. Inf. Comput.,

245(C):165–180, 2015.

[54] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In

Advances in Neural Information Processing Systems, pages 849–856. MIT Press, 2001.

[55] Rafail Ostrovsky, Yuval Rabani, Leonard J. Schulman, and Chaitanya Swamy. The eﬀectiveness of lloyd-

type methods for the k-means problem. J. ACM, 59(6):28:1–28:22, January 2013.

[56] Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance model.

Statistica Sinica, 17:1617–1642, 2007.

[57] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite programming. SIAM

J. OPTIM, 18(1):186–205, 2007.

[58] David Pollard. Strong consistency of k-means clustering. Ann. Statist., 9(1):135–140, 01 1981.
[59] Martin Royer. Adaptive clustering through semideﬁnite programming. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 1795–1803. Curran Associates, Inc., 2017.

[60] Aad van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes: With Applications

to Statistics. Springer, 1996.

[61] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J. Comput. Syst.

Sci, 68:2004, 2004.

[62] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, pages 210–268. Cam-

bridge University Press, 2012.

[63] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
[64] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, 2007.
[65] Ulrike von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clustering. Annals of

Statistics, 36(2):555–586, 2008.

[66] David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms. Cambridge Uni-

versity Press, New York, NY, USA, 1st edition, 2011.

[67] Yihong Wu and Jiaming Xu. Statistical problems with planted structures: Information-theoretical and

computational limits. arXiv:1806.00118v2, 2018.

[68] Yihong Wu and Harrison Zhou. Randomly initialized EM algorithm for two-component Gaussian mixture

achieves near optimality in O(√n) iterations. arXiv:1908.10935v1, 2019.

[69] Ji Xu, Daniel Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two
gaussians. In Proceedings of the 30th International Conference on Neural Information Processing Systems,
NIPS’16, pages 2684–2692, USA, 2016. Curran Associates Inc.

[70] Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence of gradient em on multi-component
mixture of gaussians. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, NIPS’17, pages 6959–6969, USA, 2017. Curran Associates Inc.

30

XIAOHUI CHEN AND YUN YANG

Xiaohui Chen
Department of Statistics
University of Illinois at Urbana-Champaign
725 S. Wright Street, Champaign, IL 61820
E-mail: xhchen@illinois.edu
URL: http://publish.illinois.edu/xiaohuichen/

Yun Yang
Department of Statistics
University of Illinois at Urbana-Champaign
725 S. Wright Street, Champaign, IL 61820
E-mail: yy84@illinois.edu
URL: https://sites.google.com/site/yunyangstat/

