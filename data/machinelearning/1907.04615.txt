1
2
0
2

b
e
F
4
1

]

O
C

.
t
a
t
s
[

3
v
5
1
6
4
0
.
7
0
9
1
:
v
i
X
r
a

Probabilistic programming for birth-death models of evolution
using an alive particle ﬁlter with delayed sampling

Jan Kudlicka
Uppsala University
Uppsala, Sweden

Lawrence M. Murray
Uber AI Labs
San Francisco, CA, USA

Fredrik Ronquist
Swedish Museum
of Natural History
Stockholm, Sweden

Thomas B. Sch¨on
Uppsala University
Uppsala, Sweden

Please cite this version:

J. Kudlicka, L. M. Murray, F. Ronquist, and T. B. Sch¨on. Probabilistic programming for birth-death models of evolution using
an alive particle ﬁlter with delayed sampling. In Conference on Uncertainty in Artiﬁcial Intelligence, 2019.

@inproceedings{kudlicka2019probabilistic,

title={Probabilistic programming for birth-death models of evolution using

an alive particle filter with delayed sampling},

author={Kudlicka, Jan and Murray, Lawrence M. and Ronquist, Fredrik and

Sch\"on, Thomas B.},

booktitle={Conference on Uncertainty in Artificial Intelligence},
year={2019}

}

Abstract

We consider probabilistic programming for birth-death models of evolution and introduce a new widely-
applicable inference method that combines an extension of the alive particle ﬁlter (APF) with automatic Rao-
Blackwellization via delayed sampling. Birth-death models of evolution are an important family of phylogenetic
models of the diversiﬁcation processes that lead to evolutionary trees. Probabilistic programming languages
(PPLs) give phylogeneticists a new and exciting tool: their models can be implemented as probabilistic programs
with just a basic knowledge of programming. The general inference methods in PPLs reduce the need for external
experts, allow quick prototyping and testing, and accelerate the development and deployment of new models. We
show how these birth-death models can be implemented as simple programs in existing PPLs, and demonstrate
the usefulness of the proposed inference method for such models. For the popular BiSSE model the method
yields an increase of the effective sample size and the conditional acceptance rate by a factor of 30 in comparison
with a standard bootstrap particle ﬁlter. Although concentrating on phylogenetics, the extended APF is a general
inference method that shows its strength in situations where particles are often assigned zero weight. In the case
when the weights are always positive, the extra cost of using the APF rather than the bootstrap particle ﬁlter
is negligible, making our method a suitable drop-in replacement for the bootstrap particle ﬁlter in probabilistic
programming inference.

1 Introduction

The development of new probabilistic models of evolution is an important part of statistical phylogenetics. These
models require inference algorithms that are able to cope with increased model complexity as well as the larger
amount of observational data available today. Experts from several ﬁelds typically need to be involved, both to
design bespoke inference algorithms, and to implement the new models and the inference algorithms in existing
software or to develop new software from scratch. Probabilistic programming languages (PPLs) (e.g., Goodman

1

 
 
 
 
 
 
et al., 2008; Tolpin et al., 2016; Mansinghka et al., 2014; Paige and Wood, 2014) have the potential to accelerate
this: generative models are speciﬁed as simple programs and compiled into executable applications that include
general inference engines. Writing models in PPLs requires just basic programming skills, and thus allows quick
prototyping and testing.

Quite a few software applications for statistical phylogenetics exist today, including the popular MrBayes (Huelsen-
beck and Ronquist, 2001) and BEAST (Drummond and Rambaut, 2007). They typically take a Bayesian approach
and implement Markov chain Monte Carlo inference (see review by Nascimento et al., 2017). Most of these ap-
plications do not allow the user to specify models outside of a predeﬁned model space, which can be quite narrow.
Even when adding new models is possible, it is usually a challenging task requiring not only good programming
skills but also detailed knowledge of the software design and implementation.

Statistical phylogeneticists recognize the beneﬁts of software that supports the addition of new models and in-
ference methods. For example, the design of BEAST 2 (Bouckaert et al., 2014) allows users to create and use
custom modules. RevBayes (H¨ohna et al., 2016) goes even further: it uses a domain-speciﬁc probabilistic pro-
gramming language for phylogenetics based on probabilistic graphical models (e.g., Koller and Friedman, 2009).
However, the language is not Turing-complete, which means it has some limitations. For example it does not
allow unbounded recursion.

In this paper we concentrate on birth-death models of evolution, an important family of phylogenetic models. In
these models, births correspond to lineage splits (speciation events) and deaths to extinction events. These models
specify probability distributions of evolutionary trees and the task is to infer model parameters given a part of a
complete tree that represents evolution of currently living species.

We take a step toward using PPLs in statistical phylogenetics: our main contribution is a new general inference
algorithm based on an extension of the alive particle ﬁlter (APF) (Del Moral et al., 2015) combined with automatic
Rao-Blackwellization via delayed sampling (Murray et al., 2018). We also show how to implement birth-death
models of evolution in existing PPLs, and show the usefulness of our inference algorithm for such models. Inter-
estingly, by using this algorithm we avoid sampling of birth and death rates. We believe that the algorithm may
be of interest for other models with highly-informative observations. Finally, we prove that the estimator of the
marginal likelihood in the extended APF is unbiased.

The rest of the paper is organized as follows: in Section 2 we give a brief recapitulation of basic concepts in
evolution and introduce probabilistic programming in more detail. We derive our inference algorithm and show
how phylogenetic birth-death algorithms can be implemented in PPLs in Section 3. We give implementations
of two well-known phylogenetic birth-death models and compare several general inference algorithms for these
models in Section 4. We offer some conclusions and ideas for future research in Section 5.

2 Background

2.1 Speciation, extinction and phylogenies

There are two types of events that play a signiﬁcant role in the evolution of any species.

• Speciation occurs when the population of one species splits and eventually forms two new species.

• Extinction occurs when the whole population of one species dies out. Species that are not extinct, i.e.,

species with individuals alive at the present time, are called extant.

In phylogenetics, the before present (BP) time is usually used for dating, i.e., if an event happened at time τ it
means it happened τ time units ago.

The result of an evolutionary process is a binary tree called the complete phylogeny. A very simple example of a
complete phylogeny is depicted in Figure 1a. The nodes represent events and species at signiﬁcant times:

• the root node represents the most recent common ancestor (MRCA) of all species of interest,

• an internal node represents a speciation event,

• a leaf at τ = 0 (i.e. the present time) represents an extant species,

2

(a) Complete tree

(b) Reconstructed tree

Observed
speciation

Root
(MRCA)

Unobserved
(hidden)
speciation

Pruning

Augmentation

A
C
R
M
τ

τ

e
m
T

i

0

Extinction

Extant species

Hidden (unobserved) subtree

Figure 1: Complete (on the left) vs. reconstructed (on the right) tree. The reconstructed tree shows only the
evolution of the extant species.

• a leaf at τ > 0 (i.e. in the past) represents an extinction event.

The length of edges—or branches as they are called in phylogenetics—is the difference between the time of the
parent and the child node.

The reconstructed phylogeny is obtained from a complete tree by removing all subtrees that involve only extinct
species. We will refer to this as pruning. An example of a reconstructed tree is depicted in Figure 1b.

The reconstructed phylogeny represents the evolution of the extant species and only contains information that can
be observed directly (the extant species) or reconstructed by statistical analysis of the DNA sequences of extant
species (the topology of the tree and the times of the speciation events).

2.2 Probabilistic programming

The development of new probabilistic models and inference algorithms is a time-consuming and possibly error-
prone process that usually requires skilled experts in probability, statistics and computer science. Probabilistic
programming is a relatively new approach to solve this problem: generative models are expressed as computer
programs in probabilistic programming languages (PPLs) with support for random variables and operations on
them. Integral to PPLs are general inference engines that perform the inference in such programs. These engines
estimate the distribution of all latent random variables conditioned on the observed data and use it to answer the
queries of interest.

PPLs allow us to deﬁne and initialize random variables with a given distribution, for example:

x ∼ Normal(0, 1)

The program may use random variables as though any ordinary variable, and control the ﬂow of the execution as
shown in the following example:

if x > 0.5 then

y ∼ Normal(x, 1)

else

y ∼ Exponential(1)

end if

Depending on the PPL, conditioning on the observed data might be speciﬁed explicitly or implicitly. The former
means that conditioning on the observed data is a part of the program, for example:

x ∼ Normal(0, 1)
observe 0.892 ∼ Normal(x, 1)

3

The latter implies that observed values of random variables are not part of the program, but instead speciﬁed at
run time (e.g. as arguments).

The main general inference methods used in PPLs are adaptations of various inference algorithms for the universal
setting described below, including Markov chain Monte Carlo (MCMC) (Metropolis et al., 1953; Hastings, 1970),
sequential Monte Carlo (SMC) (Gordon et al., 1993; Del Moral et al., 2006), and Hamiltonian Monte Carlo (HMC)
(Neal, 2011).

There already exist quite a few PPLs today based on different programming paradigms, for example functional
PPLs like Anglican (Tolpin et al., 2016) and Venture (Mansinghka et al., 2014); imperative Probabilistic C (Paige
and Wood, 2014), Turing (Ge et al., 2018), Stan (Carpenter et al., 2017), Edward (Tran et al., 2016) and Pyro
(Bingham et al., 2019); and object-oriented Birch (Murray and Sch¨on, 2018).

2.3 Programmatic model and SMC

The execution of a probabilistic program can be modeled using a programmatic model (Murray and Sch¨on, 2018).
Let {Vi}i denote a countable set of all random variables, both latent and observed, in a probabilistic program. This
set might be inﬁnite due to loops and recursion. During execution, whenever a random variable Vi is encountered,
its realization vi is drawn from a distribution associated with it.

Multiple executions of the program might in general encounter different subsets of the random variables (e.g., due
to using random variables in conditional expressions) and encounter them in a different order (with the exception
of the ﬁrst one). For each random variable Vj not encountered during the execution we set vj = ⊥ (the symbol
⊥ represents an undeﬁned value). We will however assume that any execution encounters all observed random
variables and that these observations are encountered in the same order.

Let σ denote a sequence of indices into {Vi} specifying the order in which the random variables are encountered
during an execution of the program, and let |σ| denote the length of this sequence. Also, let {vi}i∈σ denote
the realizations of the random variables indexed by σ, i.e., vσ[1], . . . , vσ[|σ|]. In a similar manner, we will use
{Vi = vi}i∈σ to denote Vσ[1] = vσ[1], . . . , Vσ[|σ|] = vσ[|σ|].

The index of the k-th encountered variable is given by a deterministic function Ne (for next) of the realizations of
the previously encountered random variables, so that

σ[k] = Ne({vi}i∈σ[1:k−1]),

where σ[1:k−1] denotes the indices of the ﬁrst k − 1 encountered random variables. The function Ne is uniquely
deﬁned by the probabilistic program. If there are no more random variables to encounter, Ne returns ⊥.

The k-th encountered random variable, Vσ[k], is sampled from

Vσ[k] ∼ pσ[k](·| Pa({vi}i∈σ[1:k−1])),

where pσ[k] is the distribution speciﬁed by the program, Pa (for parents) is a deterministic function returning the
parameters of this distribution, and again, it is a function of the realizations of the previously encountered random
variables.

The joint distribution function encoded by the program can be given recursively (starting with σ = []):

p({vi}i(cid:54)∈σ|σ, {Vj = vj}j∈σ) =






p({vi}i(cid:54)∈σ(cid:48)|σ(cid:48), {Vj = vj}j∈σ(cid:48)) × pκ(vκ| Pa({vj}j∈σ))

if κ (cid:54)= ⊥,

1

0

if κ = ⊥ ∧ ∀i (cid:54)∈ σ : vi = ⊥,

otherwise,

where κ = Ne({vi}i∈σ) and σ(cid:48) is obtained from σ by appending κ. The ﬁrst case is the conditional probability
chain rule, the remaining cases cover the situation where there are no more random variables to encounter.

We wish to sample from the posterior distribution p({vi}i(cid:54)∈γ|Vγ[1] = y1, . . . , Vγ[T ] = yT ), where T denotes the
number of observations, yt denotes the t-th observation and γ denotes the sequence of indices of the observed
random variables in {Vi}. The sequential nature of the joint distribution allows us to employ Sequential Monte
Carlo methods (Del Moral et al., 2006) to sample from this posterior distribution, including the bootstrap particle
ﬁlter (BPF) summarized in Algorithm 1. For the sake of brevity we have assumed that the last observation is also
the last encountered random variable. In the pseudocode, Cat() denotes the categorical distribution with the given

4

Algorithm 1 Bootstrap particle ﬁlter (BPF).

for n = 1 to N do
v(n)
0 ← ∅; w(n)
end for
for t = 1 to T do

0 ← 1

t−1}N

m=1)

t−1/ (cid:80)N

for n = 1 to N do
l=1 w(l)
a ∼ Cat({w(m)
v(n)
t ← PROPAGATE(v(a)
t−1)
t ← pγ[t](yt| Pa(v(n)
w(n)
))
v(n)
t
end for

[γ[t]] ← yt

t

end for

function PROPAGATE(v)

κ ← Ne(v)
while κ (cid:54)∈ γ do

v[κ] ∼ pκ(·| Pa(v))
κ ← Ne(v)

end while
return v
end function

(cid:46) Initialize

(cid:46) Resample
(cid:46) Propagate
(cid:46) Weigh

(cid:46) Run until next observe

event probabilities. Variables denoted by v are associative arrays (also known as maps or dictionaries) used to
store the realizations of random variables (v[i] denotes the realization of Vi). The PROPAGATE function runs the
program until it encounters an observation.

Samples from the joint distribution and the corresponding weights can be used to estimate the expected value of a
test function h of interest:

(cid:98)E[h] =

(cid:80)

(cid:16)

v(n)
T

(cid:17)

,

n w(n)
T h
(cid:80)
n w(n)

T

as well as to estimate the marginal likelihood p(y1:T ):

(cid:98)Z =

T
(cid:89)

t=1

1
N

N
(cid:88)

n=1

w(n)
t

.

3 Methods

3.1 Extended alive particle ﬁlter

In the bootstrap particle ﬁlter, each particle is propagated by simulating the prior, and may make random choices
that lead to a state with zero weight. In phylogenetic birth-death models this happens quite often: when simulating
the evolution of subtrees that must ultimately become extinct, if any species happen to survive to the present time,
the particle is assigned zero weight. In extreme cases, all particles have zero weight, and the BPF degenerates.

Del Moral et al. (2015) considered this problem in a setting with indicator potentials (such as in approximate
Bayesian computation), i.e. all weights being either zero or one. They proposed a modiﬁcation of the BPF, where
the resampling and propagation steps are repeated for particles that have weight zero until all particles have weight
one. Details of the resulting alive particle ﬁlter (APF) as well as proofs of some of its theoretical properties can
be found in Del Moral et al. (2015).

Although the original APF was designed speciﬁcally for indicator potentials, we have extended the algorithm to
work with importance weights, see Algorithm 2 (the PROPAGATE function is the same as in Algorithm 1). The
APF requires N + 1 particles rather than N in order to estimate the marginal likelihood without bias.

5

Algorithm 2 Alive particle ﬁlter (APF).

for n = 1 to N do
v(n)
0 ← ∅; w(n)
end for
for t = 1 to T do

0 ← 1

Pt ← 0
for n = 1 to N + 1 do

t−1}N

m=1)

repeat

t−1/ (cid:80)N

a ∼ Cat({w(m)
l=1 w(l)
v(n)
t ← PROPAGATE(v(a)
t−1)
Pt ← Pt + 1
t ← pγ[t](yt| Pa(v(n)
w(n)
until w(n)
v(n)
t
end for

t > 0
[γ[t]] ← yt

))

t

(cid:46) Initialize

(cid:46) Resample

(cid:46) Propagate

(cid:46) Weigh

end for

At the t-th observe statement, if the weight of a particle is zero, the resampling and propagation steps are repeated.
This procedure is repeated until the weights of all N + 1 particles are positive. The APF counts the total number
of propagations Pt for each observation. The algorithm never uses the states or weights of the N + 1-th particle,
but propagations made using this particle are included in Pt, and used to calculate the unbiased estimate of the
marginal likelihood p(y1:T ):

(cid:98)Z =

T
(cid:89)

t=1

(cid:80)N

n=1 w(n)
t
Pt − 1

.

The proof of unbiasedness can be found in Appendix A in the supplementary material.

Unbiasedness of the marginal likelihood estimate opens for the possibility to use the APF within particle Markov
chain Monte Carlo methods.

3.2 Birth-death models as probabilistic programs

Phylogentic birth-death models constitute a family of models where speciation (birth) events and extinction (death)
events occur along the branches of a phylogenetic tree. Typically, the waiting times between events are exponen-
tially distributed. In general, the rates of these exponential distributions do not remain constant but rather change
continuously, discontinuously, or both. Some models assume that these rates further depend on a state variable
that itself evolves discontinuously along the tree; in some cases the value of this state variable is given for the
extant species.

The constant-rate birth death (CRBD) model (Kendall, 1948) is the simplest birth-death model, where the speci-
ation rate λ and extinction rate µ remain constant over time. Pseudocode for generating phylogenetic trees using
the CRBD model can be found in Appendix B in the supplementary material.

Phylogenetic trees are unordered (i.e. there is no speciﬁc ordering of the children of each internal node) and usually
include the labels for the extant species. To derive the likelihood of a complete labeled phylogenetic tree T , let
us ﬁrst assume that the tree is ordered and unlabeled. Let Tr denote the subtree rooted at the node r, and Ch(r)
denote the children of this node. The likelihood of the subtree Tr can be expressed recursively (we have dropped
conditioning on λ and µ in the notation for brevity):

p(Tr) =






(cid:81)

p(Tc)

c∈Ch(r)
λe−(λ+µ)∆r (cid:81)

if r is the root node,

p(Tc)

if r is a speciation,

c∈Ch(r)

µe−(λ+µ)∆r

if r is an extinction,

e−(λ+µ)∆r

if r is an extant species,

6

where ∆r is the length of the branch between the node r and its parent. If r is a speciation event, no extinction
occurs along the branch (factor e−µ∆r ) and the speciation happens after a waiting time ∆r (factor λ exp−λ∆r ).
If r is an extinction event, no speciation occurs along the branch (factor e−λ∆r ) and the extinction occurs after
waiting time ∆r (factor µe−µ∆r ). Finally, if r is an extant species, neither extinction nor speciation occurs along
the branch.

The likelihood of the complete, unordered and labeled phylogeny T is given by

p(T ) =

2S+1
C!

p(Troot),

where S is the number of speciation events (excluding the root) and C is the number of extant species. The factor
2S+1 represents the number of all possible orderings of the tree and there are C! permutations of the labels of the
extant species. The likelihood can be conveniently written as

p(T ) =

2S+1
C!

λSµX e−(λ+µ)L,

where we have introduced L to denote the sum of all branch lengths and X to denote the number of extinction
events. The likelihood in other birth-death models that admit varying rates and/or include the state can be derived
in a similar way.

The task of interest is to infer model parameters given a reconstructed tree. Recall that this tree is a part of the
complete tree corresponding to the extant species and their ancestors. A naive approach to inference is to simulate
trees from the generative model, prune back the extinct subtrees, and reject those for which the pruned tree does
not equal the observed tree. Such an approach always results in rejection.

Instead, we turn the problem upside down: starting with the observed tree and augmenting it with unobserved
information to obtain a complete tree. Recalling Figure 1, the observed tree is traversed in depth-ﬁrst order. Along
each branch, the generative model is used to simulate:

• changes to the state (in models with state),

• changes to the speciation and extinction rates,

• hidden speciation events.

For each of the hidden speciation events, the model simulates the evolution of the new species (i.e. a hidden
subtree). If any portion of a hidden subtree survives to the present time, the weight is set to zero. If not, the current
weight is doubled, since there are two possible orderings of the children created by a hidden speciation event on
an observed branch.

If the examined branch ends with a speciation event, the algorithm observes 0 ∼ Exponential(λ). Finally, as
there were no extinction events along the processed branch, the algorithm observes 0 ∼ Poisson(µ∆). In models
with state, if the branch ends at τ = 0 (i.e. the present time) we also condition on the simulated state being equal
to the observed state. We trigger resampling at the end of each branch.

Let us return to the CRBD model in light of the discussion above. The likelihood of a proposed complete tree T (cid:48)
that is compatible with the observation (i.e. without any extant species in the hidden subtrees) is given by

q(T (cid:48)) = λH (cid:48)

e−λLobs × 2S(cid:48)

λS(cid:48)

µX (cid:48)

e−(λ+µ)L(cid:48)

,

where H (cid:48) denotes the number of all simulated hidden speciation events along the observed tree, Lobs the sum
of the branch lengths in the observed tree, S(cid:48) the number of speciation events in all hidden subtrees, X (cid:48) the
number of extinction events and ﬁnally L(cid:48) denotes the sum of the branch lengths in the hidden subtrees. The
factor λH (cid:48)
e−λLobs is related to the hidden speciation events, and the rest is the combined likelihood of all hidden
subtrees.

The weight of the proposal T (cid:48) is given by

w(T (cid:48)) =

2Sobs+1
C!

× 2H (cid:48)

× λSobs × e−µLobs,

where Sobs is the number of observed speciation events. The factor 2Sobs+1/C!, related to the number of pos-
sible orderings and the number of labeling permutations, is used as the initial weight of each proposal. The

7

factor 2H (cid:48)
corresponds to doubling the weight for each hidden subtree, the factor λSobs is due to observing 0 ∼
Exponential(λ) at all observed speciations, and ﬁnally the factor e−µLobs is due to observing 0 ∼ Poisson(µ∆)
for all observed branches.

Multiplying the likelihood q(T (cid:48)) and the weight w(T (cid:48)) of the proposal and summing the event numbers and the
branch lengths we get

q(T (cid:48))w(T (cid:48)) = p(T (cid:48)).

The factor 2Sobs+1/C! in w(T (cid:48)) is constant for all proposals and we will omit it from the weight in the algorithms
and experiments.

3.3 Delayed sampling of the rates

In a Bayesian setting, the parameters are associated with a prior distribution. Using the gamma distribution
(or the exponential distribution as its special case) as a prior for the rates of speciation, extinction and state
change is mathematically convenient since the gamma distribution is a conjugate prior for both the Poisson and the
exponential likelihood. Instead of sampling these parameters from the prior distribution before running the particle
ﬁlter (which we refer to as immediate sampling), we can exploit the conjugacy, which allows us to marginalize
out the parameters and sample them after running the particle ﬁlter. Exploiting the conjugacy in a probabilistic
program can be automated by an algorithm known as delayed sampling (Murray et al., 2018).

Consider the following prior:

ν ∼ Gamma(k, θ),

with k ∈ N. When the program needs to make a draw from a Poisson distribution

n ∼ Poisson(ν∆),

it can instead make a draw from the marginalized distribution:

n ∼ NegativeBinomial

k,

(cid:18)

1
1 + ∆θ

(cid:19)

,

where NegativeBinomial(k, p) is the negative binomial distribution counting the number of failures given the
number of successes k and the probability of success p in each trial. The distribution for ν is then updated to the
posterior distribution according to

(cid:18)

ν ∼ Gamma

k + n,

θ
1 + ∆θ

(cid:19)

.

Similarly for variables distributed according to the exponential distribution, instead of drawing

∆ ∼ Exponential(ν),

the program makes a draw from the marginalized distribution:

∆ ∼ Lomax

(cid:19)

, k

,

(cid:18) 1
θ

where the ﬁrst parameter denotes the scale and the second the shape of the Lomax distribution, and the distribution
for ν is then updated to

(cid:18)

ν ∼ Gamma

k + 1,

θ
1 + ∆θ

(cid:19)

.

Using this strategy there is actually no need to sample the rates at all; all draws involving these rates are replaced
by draws from the negative binomial and the Lomax distributions with a consequent update of the rate distribution.
Details of these conjugacy relationships can be found in Appendix C in the supplementary material.

8

40

20

0

0

λ
µ

0.05

0.1

0.15

0.2

Figure 2: The posterior distributions for the speciation and extinction rates for the cetaceans using the CRBD
model.

4 Experiments

We implemented the inference algorithms described above in the probabilistic programming language Birch (Mur-
ray and Sch¨on, 2018) and added support for the conjugacy relationships described in the previous section, so that
Birch can provide automated delayed sampling for these. We also implemented two phylogenetic birth-death
models, described in Sections 4.1 and 4.2 below.

We ran the inference for these models using different combinations of the inference method, the sampling strategy
(immediate or delayed) and different number of particles N . For each combination we executed the program M
times, collected the estimates { (cid:98)Zm}M
m=1 of the marginal likelihood and calculated the relative effective sample
size (RESS):

RESS =

1
M

(cid:16)(cid:80)M

m=1 (cid:98)Zm
m=1 (cid:98)Z 2
m

(cid:80)M

(cid:17)2

,

as well as the conditional acceptance ratio (CAR) (see Murray et al., 2013 for more detail):

CAR =

1
M

(cid:32)
2

M
(cid:88)

i=1

(cid:33)

ci − 1

,

where ci is the sum of the i smallest elements in { (cid:98)Zm}m. We also calculated the sample variance var log (cid:98)Z.

For the experiments with the APF we also compared the total number of propagations with the number of propa-
gations in the BPF by calculating

ρ =

(cid:80)M

m=1 Pm
M N T

,

where Pm is the number of all propagations made during the m-th execution and T is the number of branches in
the observation. Note that N T is the number of propagations in the BPF.

4.1 Constant-rate birth death model

Pseudocode for the probabilistic program implementing the constant-rate birth death (CRBD) model is listed as
Algorithm 3. To sample speciation events along a branch we ﬁrst sample a number of events from a Poisson
distribution and then sample the time of each event from a uniform distribution. The implementation in Birch can
be found in the supplementary material.

We used the phylogeny of cetaceans (Steeman et al., 2009) as the observation. This phylogeny (Figure 5 in
the supplementary material) represents the evolution of whales, dolphins and porpoises and contains 87 extant
species. We used Gamma(1, 1) as the prior for both the speciation and extinction rate. The results of experiments
comparing BPF and APF with immediate or delayed sampling for different number of particles N , and running
M = 200 executions for each combination, are summarized in Table 1 and Figure 3a.

When using delayed sampling, the speciation and extinction rates are never sampled; the rates are instead rep-
λ , k(m)
resented by gamma distributions with parameters that are updated during the execution. Let k(m)

λ , θ(m)

µ

9

Algorithm 3 CRBD model as a probabilistic program.

Input:

• T – a pre-ordered list of nodes in the observation
• kλ, θλ – the shape and scale of the prior Gamma distribution for λ (kλ ∈ N)
• kµ, θµ – the shape and scale of the prior Gamma distribution for µ (kµ ∈ N)

λ ∼ Gamma(kλ, θλ)
µ ∼ Gamma(kµ, θµ)
for all r ∈ T do

if r is the root then

continue

end if
chs ∼ Poisson(λ∆r)
for i ← 1 to chs do

τ ∼ Uniform(τr, τr + ∆r)
if BRANCHSURVIVES(τ ) then

set the weight to 0 and return

end if
double the weight

end for
if r has children then

observe 0 ∼ Exponential(λ)

end if
observe 0 ∼ Poisson(µ∆r)

end for

function BRANCHSURVIVES(τ, λ, µ)

∆ ∼ Exponential(µ)
if ∆ ≥ τ then
return true

end if
cb ∼ Poisson (λ∆)
for i ← 1 to cb do

τ (cid:48) ∼ Uniform(τ − ∆, τ )
if BRANCHSURVIVES(τ (cid:48), λ, µ) then

return true

end if
end for
return false

end function

µ

and θ(m)
denote the ﬁnal values of these parameters for a particle drawn from all particles in the m-th run with
the probabilities proportional to their ﬁnal weights. The posterior distributions for λ and µ can be estimated by
mixtures of gamma distributions:

λ ∼

µ ∼

1
m=1 (cid:98)Zm

(cid:80)M

1
m=1 (cid:98)Zm

(cid:80)M

M
(cid:88)

m=1

M
(cid:88)

m=1

(cid:98)Zm Gamma

(cid:98)Zm Gamma

(cid:16)

(cid:16)

λ , θ(m)
k(m)

λ

µ , θ(m)
k(m)

µ

(cid:17)

(cid:17)

,

.

Figure 2 depicts the posterior distributions for the speciation and extinction rates estimated using M = 1000 runs
of the APF with N = 4096 particles.

10

4.2 Binary-state speciation and extinction model

The binary-state speciation and extinction (BiSSE) model (Maddison et al., 2007) introduces a binary state for
species, denoted by s ∈ {0, 1}. Each state has its own (but constant) speciation and extinction rates, denoted
by λs and µs. The waiting time between switching state is exponentially distributed with rates q01 for switching
from state 0 to state 1, and q10 from state 1 to state 0. In our experiments we made a (common) assumption that
q01 = q10 = ς.

We used the same observation as Rabosky and Goldberg (2015), i.e., we extended the cetacean phylogeny with the
state variable related to the body length of cetaceans obtained from Slater et al. (2010). Data are available for 74
of the 87 extant species. The binary state 0 and 1 refers to the length being below and above the median. Again we
used Gamma(1, 1) as the prior for λ0, λ1, µ0 and µ1, and Gamma(1, 10/820.28) as the prior for ς (the number
in the denominator is the sum of all branch lengths). The initial state at the root is drawn from {0, 1} with equal
probabilities. The results for experiments comparing the BPF and the APF with immediate or delayed sampling
for different number of particles N , and running M = 200 executions for each combination, are summarized in
Table 2 and Figure 3b. When running the experiments using the BPF and immediate sampling, a certain fraction
of the executions degenerated—from 25% of the executions with 1024 particles down to 1.5% of the executions
with 4096 particles. These executions were excluded when calculating var log (cid:98)Z.

Our implementation of the BiSSE model can be found in the supplementary material.

5 Discussion and conclusion

In this paper we introduced a new general inference method for probabilistic programming combining an extended
alive particle ﬁlter (APF) with delayed sampling, and proved that the resulting estimate of the marginal likelihood
is unbiased. We showed how phylogenetic birth-death models can be implemented in probabilistic programming
languages, in particular, we considered two models—CRBD and BiSSE and their implementation in the proba-
bilistic programming language Birch. We showed the strength of this inference method for these models compared
to the standard bootstrap particle ﬁlter (BPF) (Tables 1 and 2, and Figures 3a and 3b): for the BiSSE model using
8192 particles we increased RESS approximately 29 times, CAR approximately 30 times and lowered var log (cid:98)Z
more than 1150 times at the cost of running 3 times more propagations.

The extended APF is a suitable drop-in replacement for the BPF for black-box probabilistic programs. If a program
produces only positive weights, the APF produces the same result as the BPF at the overhead of just one extra
particle, used to estimate the marginal likelihood. On the other hand, if the program can produce zero weights,
the APF behaves much more reasonably than the BPF: resampling and propagation are repeated until all particles
have positive weight. This may seem equivalent to using the BPF with a higher number of particles (ρ times more
to be precise), but this is not the case: the number of propagations Pt is not the same throughout the execution,
but rather adapted dynamically for each t. This simpliﬁes the tuning of the number of particles for such models.

The learning of rates in birth-death models sits in the context of broader problems in phylogenetics, such as the
learning of trees. Interesting future work would be to consider whether models and methods for learning rates can
be combined with models and methods for learning tree structures for joint inference.

Acknowledgements

The authors wish to thank Johannes Borgstr¨om, Viktor Senderov and Andreas Lindholm for their useful feedback.
This research was ﬁnancially supported by the Swedish Foundation for Strategic Research (SSF) via the project
ASSEMBLE and by the Swedish Research Council grants 2013-4853, 2014-05901 and 2017-03807.

References

E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh, P. Szerlip, P. Horsfall,
and N. D. Goodman. Pyro: Deep universal probabilistic programming. Journal of Machine Learning Research,
20(28):1–6, 2019.

R. Bouckaert, J. Heled, D. K¨uhnert, T. Vaughan, C.-H. Wu, D. Xie, M. A. Suchard, A. Rambaut, and A. J.
Drummond. BEAST 2: A software platform for Bayesian evolutionary analysis. PLOS Computational Biology,
10(4):e1003537, 2014.

11

-285

-290

(cid:98)Z
g
o
l

-295

-300

-325

-350

(cid:98)Z
g
o
l

-375

-400

Method

BPF immediate

APF immediate

BPF delayed

APF delayed

Method

BPF immediate

APF immediate

BPF delayed

APF delayed

512

1024

2048

4096

1024

2048

4096

8192

N

(a) CRBD

N

(b) BiSSE

Figure 3: Box plot of log (cid:98)Z for the CRBD (on the left) and BiSSE (on the right) model for different number of
particles (N ) and methods. The lower and upper hinges correspond to the ﬁrst and third quartile, whereas the
midhinge corresponds to the median. Values outside of the interquartile range are shown as dots. The horizontal
dashed line shows the true value of log Z for the CRBD model.

Table 1: Summary of the results of the experiments with the CRBD model using the cetacean phylogeny as the
observation, priors λ, µ ∼ Gamma(1, 1), and M = 200.

Bootstrap particle ﬁlter (BPF)

Alive particle ﬁlter (APF)

Immediate sampling

Delayed sampling

N RESS CAR
0.04
0.02
0.12
0.11
0.17
0.14
0.23
0.18

512
1024
2048
4096

var RESS CAR
0.23
0.13
0.35
0.28
0.47
0.47
0.63
0.67

334.2
117.5
52.2
17.2

var
31.6
12.9
8.7
0.7

Immediate sampling
ρ RESS CAR
0.15
0.18
0.32
0.42

0.11
0.14
0.29
0.36

var
50.7
20.2
7.6
2.7

1.8
1.8
1.7
1.7

Delayed sampling
ρ RESS CAR var
2.7
0.8
0.3
0.2

0.40
0.54
0.73
0.84

0.46
0.55
0.69
0.76

1.7
1.7
1.7
1.7

Table 2: Summary of the results of the experiments with the BiSSE model using the cetacean phylogeny ex-
tended with information about the body length as the observation, priors λ0, λ1, µ0, µ1 ∼ Gamma(1, 1) and
ς ∼ Gamma(1, 10/820.28), and M = 200.

Bootstrap particle ﬁlter (BPF)

Alive particle ﬁlter (APF)

Immediate sampling

Delayed sampling

N RESS CAR
0.01
0.01
0.01
0.01
0.01
0.01
0.02
0.02

1024
2048
4096
8192

var RESS CAR
0.09
0.06
0.15
0.09
0.27
0.22
0.35
0.28

3382.2
2954.0
1894.1
968.4

var
72.4
22.2
7.6
6.1

12

Immediate sampling
ρ RESS CAR
0.01
0.02
0.01
0.03

0.01
0.02
0.01
0.02

var
2294.9
1044.5
614.3
160.9

10.0
6.6
5.9
3.9

Delayed sampling
ρ RESS CAR var
4.8
2.9
1.3
0.8

0.10
0.14
0.34
0.54

0.21
0.27
0.43
0.55

3.1
3.1
3.1
3.0

B. Carpenter, A. Gelman, M. D. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li, and

A. Riddell. Stan: A probabilistic programming language. Journal of Statistical Software, 76(1), 2017.

P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society:

Series B (Statistical Methodology), 68(3):411–436, 2006.

P. Del Moral, A. Jasra, A. Lee, C. Yau, and X. Zhang. The alive particle ﬁlter and its use in particle Markov chain

Monte Carlo. Stochastic Analysis and Applications, 33(6):943–974, 2015.

A. J. Drummond and A. Rambaut. BEAST: Bayesian evolutionary analysis by sampling trees. BMC Evolutionary

Biology, 7(1):214, 2007.

H. Ge, K. Xu, and Z. Ghahramani. Turing: A language for ﬂexible probabilistic inference. In Proceedings of the
Twenty-First International Conference on Artiﬁcial Intelligence and Statistics, volume 84, pages 1682–1690,
2018.

N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church: a language for
generative models. In Proceedings of the 24th Conference in Uncertainty in Artiﬁcial Intelligence, pages 220–
229, 2008.

N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian Bayesian state

estimation. In IEE Proceedings on Radar and Signal Processing, volume 140, pages 107–113, 1993.

W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):

97–109, 1970.

S. H¨ohna, M. J. Landis, T. A. Heath, B. Boussau, N. Lartillot, B. R. Moore, J. P. Huelsenbeck, and F. Ronquist.
RevBayes: Bayesian phylogenetic inference using graphical models and an interactive model-speciﬁcation
language. Systematic Biology, 65(4):726–736, 2016.

J. P. Huelsenbeck and F. Ronquist. MRBAYES: Bayesian inference of phylogenetic trees. Bioinformatics, 17(8):

754–755, 2001.

D. G. Kendall. On the generalized “birth-and-death” process. The Annals of Mathematical Statistics, 19(1):1–15,

1948.

D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT Press, 2009.

W. P. Maddison, P. E. Midford, and S. P. Otto. Estimating a binary character’s effect on speciation and extinction.

Systematic Biology, 56(5):701–710, 2007.

V. Mansinghka, D. Selsam, and Y. Perov. Venture: a higher-order probabilistic programming platform with

programmable inference. arXiv preprint arXiv:1404.0099, 2014.

N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by

fast computing machines. The Journal of Chemical Physics, 21(6):1087–1092, 1953.

L. M. Murray and T. B. Sch¨on. Automated learning with a probabilistic programming language: Birch. Annual

Reviews in Control, 46:29–43, 2018.

L. M. Murray, E. M. Jones, and J. Parslow. On disturbance state-space models and the particle marginal

metropolis-hastings sampler. SIAM/ASA Journal on Uncertainty Quantiﬁcation, 1(1):494–521, 2013.

L. M. Murray, D. Lund´en, J. Kudlicka, D. Broman, and T. B. Sch¨on. Delayed sampling and automatic Rao-
Blackwellization of probabilistic programs. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 1037–1046, 2018.

F. F. Nascimento, M. dos Reis, and Z. Yang. A biologist’s guide to Bayesian phylogenetic analysis. Nature

Ecology & Evolution, 1(10):1446–1454, 2017.

R. M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2(11):2, 2011.

B. Paige and F. Wood. A compilation target for probabilistic programming languages. In International Conference

on Machine Learning, pages 1935–1943, 2014.

13

M. K. Pitt, R. dos Santos Silva, P. Giordani, and R. Kohn. On some properties of Markov chain Monte Carlo

simulation methods based on the particle ﬁlter. Journal of Econometrics, 171(2):134–151, 2012.

D. L. Rabosky and E. E. Goldberg. Model inadequacy and mistaken inferences of trait-dependent speciation.

Systematic Biology, 64(2):340–355, 2015.

G. J. Slater, S. A. Price, F. Santini, and M. E. Alfaro. Diversity versus disparity and the radiation of modern
cetaceans. Proceedings of the Royal Society of London B: Biological Sciences, 277(1697):3097–3104, 2010.

M. E. Steeman, M. B. Hebsgaard, R. E. Fordyce, S. Y. Ho, D. L. Rabosky, R. Nielsen, C. Rahbek, H. Glenner, M. V.
Sørensen, and E. Willerslev. Radiation of extant cetaceans driven by restructuring of the oceans. Systematic
Biology, 58(6):573–585, 2009.

D. Tolpin, J.-W. van de Meent, H. Yang, and F. Wood. Design and implementation of probabilistic programming
language Anglican. In Proceedings of the 28th Symposium on the Implementation and Application of Functional
programming Languages, pages 6:1–6:12. ACM, 2016.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Edward: A library for probabilistic

modeling, inference, and criticism. arXiv preprint arXiv:1610.09787, 2016.

14

SUPPLEMENTARY MATERIAL

15

f1(x1|x0)

f2(x2|x1)

f3(x3|x2)

ft(xt|xt−1)

ft+1(xt+1|xt)

fT (xT |xT −1)

x0

x1

x2

· · ·

xt

· · ·

xT

g1(y1|x1)

g2(y2|x2)

gt(yt|xt)

gT (yT |xT )

y1

y2

yt

yT

Figure 4: Graphical model of the state space model.

A Proof of the unbiasedness of the marginal likelihood estimator of the

extended APF

In this section we prove that the marginal likelihood estimator

w(n)
t

N
(cid:80)
n=1
Pt − 1

,

T
(cid:89)

t=1

(cid:98)Z =

produced by the extended alive particle ﬁlter (APF) for the state space model (Figure 4)

x0 ∼ p(x0),
xt ∼ ft(xt|xt−1), for t = 1, 2, . . . , T,
yt ∼ gt(yt|xt),

is unbiased in the sense that E[ (cid:98)Z] = p(y1:T ).
The structure of our proof is similar to that of Pitt et al. (2012) for the Auxiliary Particle Filter. Let Ft =
{x(n)
n=1 denote the internal state of the particle ﬁlter, i.e., the states and weights of all particles, at time t.
t

t }N

, w(n)

Lemma 1.

E

(cid:34) (cid:80)N

n=1 w(n)
t
Pt − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

Ft−1

=

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

yt

p

Proof. In the interest of brevity, we will omit conditioning on Ft−1 in the notation. For each particle, the APF
constructs a candidate sample x(cid:48) by drawing a sample from {x(n)
t−1} with the probabilities proportional to the
weights {w(n)

t−1} and propagating it forward to time t such that

x(cid:48) ∼

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

x(cid:48)(cid:12)
(cid:12)x(n)
(cid:12)

t−1

(cid:17)

.

ft

If gt(yt|x(cid:48)) = 0, the candidate sample is rejected and the procedure is repeated until acceptance (when gt(yt|x(cid:48)) >
0).
Let At = {x(cid:48) : gt(yt|x(cid:48)) > 0}. The acceptance probability pAt is then given by

(cid:90)

pAt =

1At(x(cid:48))

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

x(cid:48)(cid:12)
(cid:12)x(n)
(cid:12)

t−1

(cid:17)

ft

dx(cid:48),

where 1 denotes the indicator function.

The accepted samples are distributed according to the following distribution:

xt ∼

1At(xt)
pAt

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

xt

ft

16

The expected value of the weight wt = gt(yt|xt) of an accepted sample is given by

(cid:90)

E[wt] =

gt(yt|xt)

1At(xt)
pAt

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

xt

ft

dxt.

The factor 1At(xt) can be omitted since 1At(xt) = 0 ⇔ gt(yt|xt) = 0, resulting in

E[wt] =

(cid:90)

1
pAt

N
(cid:88)

n=1

(cid:80)N

w(n)
t−1
m=1 w(m)
t−1
w(n)
(cid:90)
t−1
m=1 w(m)
w(n)
t−1
m=1 w(m)

t−1

t−1

p

(cid:80)N

(cid:80)N

=

=

1
pAt

1
pAt

N
(cid:88)

n=1

N
(cid:88)

n=1

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

xt

ft

gt(yt|xt)dxt

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

xt

ft

gt(yt|xt)dxt

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

yt

The APF repeats drawing new samples until N + 1 samples have been accepted. The total number of draws
of candidate samples at time t, Pt, is itself a random variable distributed according to the negative binomial
distribution

P (Pt = D) =

(cid:18) D − 1

(N + 1) − 1

(cid:19)

pN +1
At

(1 − pAt)D−(N +1)

with the support D ∈ {N + 1, N + 2, N + 3, . . . }.
Finally, using the fact that E[wt] does not depend on the value of Pt,

E

(cid:34) (cid:80)N

n=1 w(n)
t
Pt − 1

(cid:35)

=

∞
(cid:88)

D=N +1

= N E[wt]

= N E[wt]

N E[wt]
D − 1

∞
(cid:88)

D=N +1
∞
(cid:88)

∞
(cid:88)

N E[wt]
D − 1

(cid:19)

(cid:18)D − 1
N

pN +1
At

(1 − pAt)D−(N +1)

P (Pt = D) =

1
D − 1

(cid:18)D − 1
N

D=N +1
(cid:19)

pN +1
At

(1 − pAt)D−(N +1)

1
D − 1

(D − 1)(D − 2)!
N (N − 1)!(D − (N + 1))!

pN +1
At

(1 − pAt)D−(N +1)

= E[wt]pN +1

At

D=N +1
∞
(cid:88)

D=N +1

(cid:18) D − 2

(cid:19)

D − (N + 1)

(1 − pAt)D−(N +1)

(using the binomial theorem)

= E[wt]pN +1

At

p−N
At

=

1
pAt

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

yt

p

pAt

=

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

yt

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

p

Lemma 2.





E

(cid:80)N

n=1 w(n)
t p

(cid:16)

yt+1:t(cid:48)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t

Pt − 1



Ft−1

 =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

p

yt:t(cid:48)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

17

Proof. Similar to the proof of Lemma 1 we have that

E[wtp(yt+1:t(cid:48)|xt)] =

(cid:90)

1
pAt

N
(cid:88)

n=1

(cid:80)N

w(n)
t−1
m=1 w(m)
t−1
w(n)
(cid:90)
t−1
m=1 w(m)
w(n)
t−1
m=1 w(m)

t−1

t−1

p

(cid:80)N

(cid:80)N

=

=

1
pAt

1
pAt

N
(cid:88)

n=1

N
(cid:88)

n=1

(cid:16)

yt:t(cid:48)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

xt

ft

(cid:16)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

xt

ft

gt(yt|xt)p(yt+1:t(cid:48)|xt)dxt

gt(yt|xt)p(yt+1:t(cid:48)|xt)dxt

and using this result we have that





E

(cid:80)N

n=1 w(n)
t p

(cid:16)

yt+1:t(cid:48)

(cid:12)
(cid:12)x(n)
(cid:12)

t

Pt − 1

(cid:17)



 =

∞
(cid:88)

D=N +1

N E[wtp(yt+1:t(cid:48)|xt)]
D − 1

(cid:18)D − 1
N

(cid:19)

pN +1
At

(1 − pAt)D−(N +1)

= N E[wtp(yt+1:t(cid:48)|xt)]

∞
(cid:88)

D=N +1

1
D − 1

(cid:18)D − 1
N

(cid:19)

pN +1
At

(1 − pAt)D−(N +1)

= N E[wtp(yt+1:t(cid:48)|xt)]

pAt
N

= N

1
pAt

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

p

yt:t(cid:48)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

(cid:17) pAt
N

=

N
(cid:88)

n=1

w(n)
t−1
m=1 w(m)

t−1

(cid:80)N

(cid:16)

p

yt:t(cid:48)

(cid:17)

(cid:12)
(cid:12)x(n)
(cid:12)

t−1

.

(cid:16)

p

yt−h:t

(cid:12)
(cid:12)x(n)
(cid:12)

t−h−1

(cid:17)

.

Lemma 3.

(cid:34) t
(cid:89)

E

t(cid:48)=t−h

(cid:80)N

n=1 w(n)
t(cid:48)
Pt(cid:48) − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

Ft−h−1

=

N
(cid:88)

n=1

t−h−1

w(n)
m=1 w(m)

(cid:80)N

t−h−1

Proof. By induction.

The base step for h = 0 was proved in Lemma 1.

In the induction step, let us assume that the equality holds for h and prove it for h + 1:

(cid:34)

E

t
(cid:89)

t(cid:48)=t−h−1

(cid:80)N

n=1 w(n)
t(cid:48)
P (cid:48)
t − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:34)

Ft−h−2

= E

E

(cid:34) t
(cid:89)

t(cid:48)=t−h

(cid:80)N

n=1 w(n)
t(cid:48)
P (cid:48)
t − 1

(cid:12)
(cid:12)
(cid:12)
Ft−h−1
(cid:12)
(cid:12)

(cid:35) (cid:80)N

n=1 w(n)
t−h−1
Pt−h−1 − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

Ft−h−2

= E

(cid:34) N
(cid:88)

(using the induction assumption)
w(n)
m=1 w(m)
w(n)

n=1
(cid:34) N
(cid:88)

t−h−1

t−h−1

(cid:80)N

(cid:16)

(cid:16)

p

t−h−1
Pt−h−1 − 1

p

yt−h:t

= E

n=1
(using Lemma 2)
w(n)
m=1 w(m)

N
(cid:88)

t−h−2

(cid:80)N

n=1

=

t−h−2

(cid:16)

p

yt−h−1:t

(cid:12)
(cid:12)x(n)
(cid:12)

t−h−2

(cid:17)

.

yt−h:t

(cid:12)
(cid:12)x(n)
(cid:12)

t−h−1

(cid:17) (cid:80)N

n=1 w(n)
t−h−1
Pt−h−1 − 1

(cid:35)

Ft−h−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)x(n)
(cid:12)

t−h−1

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

Ft−h−2

Theorem 1.



E





T
(cid:89)

t=1

w(n)
t

N
(cid:80)
n=1
Pt − 1







= p(y1:T ).

18

Proof. Using Lemma 3 with t = T, h = T − 1 and

(cid:34)

E

1
N

N
(cid:88)

n=1

(cid:35)

(cid:17)

(cid:16)

p

y1:T

(cid:12)
(cid:12)x(n)
(cid:12)

0

= p(y1:T ).

B Generative model for CRBD

The pseudocode for generating phylogenetic trees using the CRBD model is listed in Algorithm 4.

Algorithm 4 Pseudocode for generating trees using the CRBD model.
function CRBD(τorig)

return (τorig, {CRBD’(τorig)})

end function

function CRBD’(τ )

∆ ∼ Exponential(λ + µ)
τ (cid:48) ← τ − ∆
if τ (cid:48) < 0 then

return (0, ∅)

(cid:16)

end if
e ∼ Cat
if e = 1 then

p1 = λ

λ+µ , p2 = µ

λ+µ

(cid:17)

return (τ (cid:48), {CRBD’(τ (cid:48)), CRBD’(τ (cid:48))})

else

return (τ (cid:48), ∅)

end if
end function

C Relevant conjugacy relationships

C.1 Negative binomial and Lomax distribution

Negative binomial distribution

Parameters: number of successes k > 0 before the experiment is stopped, probability of success p ∈ (0, 1)

Probability mass function:

f (r|k, p) =

(cid:18)r + k − 1
k − 1

(cid:19)

pk(1 − p)r for r ∈ N ∪ {0},

where r is the number of failures.

Lomax distribution

Parameters: scale λ > 0, shape α > 0

Probability density function:

f (∆|λ, α) =

(cid:19)−(α+1)

(cid:18)

α
λ

1 +

∆
λ

for ∆ ≥ 0

19

C.2 Conjugacy relationships

Gamma-Poisson mixture

Prior distribution: ν ∼ Gamma(k, θ) with the probability density function

f (ν|k, θ) =

1

Γ(k)θk νk−1e−ν/θ for ν > 0

Likelihood: n ∼ Poisson(ν∆) with the probability mass function

f (n|λ) =

λn
n!

e−λ for n ∈ N ∪ {0},

where λ = ν∆.
Prior predictive distribution (k ∈ N):

f (n|k, θ) =

=

(cid:90) ∞

1

0

Γ(k)θk νk−1e−ν/θ ×
(cid:18) 1
∆n
n!(k − 1)!θk
θ

+ ∆

(ν∆)n
n!
(cid:19)−(n+k)

e−ν∆dν =

νn+k−1e−ν(1/θ+∆)dν

∆n
n!(k − 1)!θk
(cid:18)n + k − 1
k − 1

(cid:90) ∞

0
(cid:19) (cid:18) 1

(cid:19)k (cid:18)

(cid:19)n

1 −

1
1 + ∆θ

1 + ∆θ

(n + k − 1)! =

n|k, θ ∼ NegativeBinomial

k,

(cid:18)

(cid:19)

1
1 + ∆θ

Posterior distribution:
1

f (ν|n) ∝

Γ(k)θk νk−1e−ν/θ ×
(cid:18)
θ
1 + ∆θ

k + n,

(ν∆)n
n!
(cid:19)

ν|n ∼ Gamma

e−ν∆ ∝ νk+n−1e−ν(1/θ+∆) = ν(k+n)−1e−ν/(

θ

1+∆θ )

Gamma-exponential mixture

Prior distribution: ν ∼ Gamma(k, θ)

Likelihood: ∆ ∼ Exponential(ν) with the probability density function

f (∆|ν) = νe−ν∆ for ∆ ≥ 0

Prior predictive distribution:

f (∆|k, θ) =

(cid:90) ∞

0

Γ(k)θk νk−1e−ν/θ × νe−ν∆dν =
(cid:19)−(k+1)

+ ∆

(cid:19)

Γ(k + 1) =

=

1
Γ(k)θk

1
Γ(k)θk
(cid:18) 1
θ

k
θk

∆|k, θ ∼ Lomax

, k

1

(cid:18) 1
θ
(cid:18) 1
θ

(cid:90) ∞

0

νke−ν(1/θ+∆)dν

(cid:19)−(k+1)

= kθ(1 + ∆θ)−(k+1)

+ ∆

Posterior distribution:
1

f (ν|∆) ∝

Γ(k)θk νk−1e−ν/θ × νe−ν∆ ∝ νke−ν(1/θ+∆) = ν(k+1)−1e−ν/(

θ

1+∆θ )

ν|∆ ∼ Gamma

k + 1,

(cid:18)

(cid:19)

θ
1 + ∆θ

D Source code

Birch is available at
https://birch-lang.org/

The source code for the CRBD and BiSSE models is available at
https://github.com/kudlicka/paper-2019-probabilistic

20

Figure 5: Phylogeny of cetaceans (whales, dolphins and porpoises).

21

4.0Balaenoptera physalusMegaptera novaeangliaeZiphius cavirostrisBerardius arnuxiiMesoplodon peruvianusOrcaella brevirostrisBalaenoptera acutorostrataInia geoffrensisCephalorhynchus hectoriGrampus griseusBalaenoptera omuraiMonodon monocerosLissodelphis borealisLagenorhynchus obliquidensHyperoodon ampullatusDelphinus delphisPeponocephala electraLagenorhynchus australisMesoplodon stejnegeriDelphinapterus leucasTasmacetus shepherdiOrcinus orcaLagenorhynchus obscurusStenella attenuataTursiops truncatusPhyseter catodonEubalaena australisKogia brevicepsStenella clymeneMesoplodon bowdoiniMesoplodon bidensPlatanista minorStenella coeruleoalbaEschrichtius robustusPhocoena sinusBalaenoptera bonaerensisMesoplodon ginkgodensMesoplodon mirusPhocoena dioptricaKogia simusSousa chinensisEubalaena glacialisGlobicephala macrorhynchusMesoplodon carlhubbsiMesoplodon traversiiCephalorhynchus heavisidiiTursiops aduncusBalaenoptera borealisPhocoena phocoenaCephalorhynchus commersoniiBalaenoptera brydeiCaperea marginataBalaenoptera musculusNeophocaena phocaenoidesMesoplodon europaeusBalaenoptera edeniStenella frontalisPontoporia blainvilleiLissodelphis peroniiLagenodelphis hoseiPhocoenoides dalliPhocoena spinipinnisEubalaena japonicaMesoplodon layardiiMesoplodon hectoriPseudorca crassidensHyperoodon planifronsMesoplodon perriniStenella longirostrisGlobicephala melasLagenorhynchus albirostrisFeresa attenuataIndopacetus pacificusLagenorhynchus crucigerSotalia fluviatilisPlatanista gangeticaDelphinus capensisBalaena mysticetusSotalia guianensisMesoplodon grayiSteno bredanensisMesoplodon densirostrisBerardius bairdiiLipotes vexilliferDelphinus tropicalisLagenorhynchus acutusCephalorhynchus eutropia