2
2
0
2

n
u
J

5
1

]
L
M

.
t
a
t
s
[

1
v
0
3
6
7
0
.
6
0
2
2
:
v
i
X
r
a

Rethinking Initialization of the Sinkhorn Algorithm

James Thornton ∗
University of Oxford
james.thornton@stats.ox.ac.uk

Marco Cuturi
Apple
cuturi@apple.com

Abstract

Computing an optimal transport (OT) coupling between distributions plays an
increasingly important role in machine learning. While OT problems can be solved
as linear programs, adding an entropic smoothing term is known to result in solvers
that are faster and more robust to outliers, differentiable and easier to parallelize.
The Sinkhorn ﬁxed point algorithm is the cornerstone of these approaches, and,
as a result, multiple attempts have been made to shorten its runtime using, for
instance, annealing, momentum or acceleration. The premise of this paper is that
initialization of the Sinkhorn algorithm has received comparatively little attention,
possibly due to two preconceptions: as the regularized OT problem is convex, it
may not be worth crafting a tailored initialization as any is guaranteed to work;
secondly, because the Sinkhorn algorithm is often differentiated in end-to-end
pipelines, data-dependent initializations could potentially bias gradient estimates
obtained by unrolling iterations. We challenge this conventional wisdom and show
that carefully chosen initializations can result in dramatic speed-ups, and will not
bias gradients which are computed with implicit differentiation. We detail how
initializations can be recovered from closed-form or approximate OT solutions,
using known results in the 1D or Gaussian settings. We show empirically that
these initializations can be used off-the-shelf, with little to no tuning, and result in
consistent speed-ups for a variety of OT problems.

1

Introduction

The optimal assignment problem and its generalization, the optimal transport (OT) problem, play
an increasing role in the modern machine learning (ML) literature. These problems deﬁne the
Wasserstein geometry (Santambrogio, 2015; Peyré et al., 2019), which is routinely used as a loss
function in imaging (Schmitz et al., 2018; Janati et al., 2020), but also used to reconstruct correspon-
dences between datasets, as for instance in domain adaptation (Courty et al., 2014, 2017), single-cell
genomics (Schiebinger et al., 2019) or computer vision (Carion et al., 2020). Some recent applica-
tions use optimal matches as an intermediate representation, as in self-supervised learning (Caron
et al., 2020), balanced attention (Sander et al., 2022), parameterized matching (Sarlin et al., 2020),
differentiable sorting and ranking (Adams and Zemel, 2011; Cuturi et al., 2019, 2020; Xie et al.,
2020a), differentiable resampling (Corenﬂos et al., 2021) and clustering (Genevay et al., 2019).

Sinkhorn as a subroutine for OT. A striking feature of all the approaches outlined above is that
they do not rely on the linear programming formulation of OT (Ahuja et al., 1988, §9-11), but use
instead an entropy regularized formulation (Cuturi, 2013; Peyré et al., 2019). That formulation is
typically solved by the the Sinkhorn algorithm (1967), which has gained popularity for its versatility,
efﬁciency and differentiability.

Ever Faster Sinkhorn. The Sinkhorn algorithm entails a ﬁxed-point iteration that updates two dual
variables alternatively. Because the runtime of Sinkhorn is a bottleneck for many methods that use
it as an inner routine, several attempts at speeding it up have been proposed. These improvements

∗Work done during an internship at Apple.

 
 
 
 
 
 
can be split into two lines of works: either by aiming for faster kernel matrix-vector multiplications,
using properties of the considered geometric setups (Solomon et al., 2015; Altschuler et al., 2019;
Scetbon and Cuturi, 2020), or by reducing the total number of iterations needed for convergence,
with a more holistic view of the algorithm. We show that all of these approaches complement our
work, and consider in particular the annealing schedule for the entropic regularization parameter
proposed by Kosowsky and Yuille (1994) and further reﬁned in (Schmitzer, 2019; Xie et al., 2020b),
as well as over-relaxed iterations that use momentum (Thibault et al., 2021; Lehmann et al., 2021),
and Anderson acceleration (1965), as considered in (Chizat et al., 2020).

Initialization as a Blind Spot. The acceleration methods proposed above all default to initializing
dual vectors at 0 (or scaling vectors as 1). Given the growing demand for faster Sinkhorn layers from
recent ML applications, the initialization of dual potentials presents an overlooked opportunity for
acceleration. To our knowledge, initialization has only been explored in a few restricted setups, such
as semi-discrete settings in 2/3D (Meyron, 2019), or to initialize the discrete Wasserstein barycenter
problem (Cuturi and Peyré, 2015).

Contributions. We propose off-the-shelf initializers that should be generic and robust enough to help
practitioners consistently across applications. We note that concurrent work by Amos et al. (2022)
proposes initializers that are trained and informed by a dataset of measures, and formulated as an
amortized problem. Our and their approach complement each other, in the sense that our initializers
require little to no hyperparameter tuning at all. We leverage instead closed-form solutions for primal
OT to obtain dual initializations.

• We ﬁrstly target the differentiable 1D OT problem that has been used for differentiable ranking
and soft top-k estimators. We recover dual vectors for the non-regularized problem using known
optimal primal solutions derived from sorting and by calling a novel variation of the primal-dual
solver for a few iterations. We show that initializing Sinkhorn with these dual vectors results in
orders of magnitude improvements for differentiable sorting and ranking.

• We consider the 2-Wasserstein setup in Rd, with the ground cost set to be the squared Euclidean
distance and consider Gaussian approximations. We recover dual vectors using the Brenier theorem
(1987) and the known closed-form transport map for the Gaussian case. We extend this approach
to leverage closed-form approximations of OT potentials for mixtures of Gaussians. We show that
in the regime where d (cid:28) n, i.e. where the number of points is much larger than dimension (the
typical bottleneck for Sinkhorn applications), we can recover approximations for dual potentials
that are consistently effective and cheap to compute.

2 Background on Entropy Regularized Transport

2.1 Entropy Regularized Optimal Transport and the Sinkhorn Algorithm
Given two discrete probability measures µ = (cid:80)n
i=1 aiδxi and ν = (cid:80)m
in P(Rd),
where a = (a1, . . . , an), b = (b1, . . . , bm) are probability weights and (x1, . . . , xn) ∈ Rd×n,
(y1, . . . , ym) ∈ Rd×m, the entropy regularized optimal transport problem between µ and ν parame-
terized by ε ≥ 0 with cost function c : Rd × Rd → R has two equivalent formulations,

j=1 bjδyj

min

(cid:104)P, C(cid:105) − εH(P), max

Eε(f , g):= f T a + gT b − ε(cid:104)ef /εKeg/ε(cid:105) + ε , (1)

P∈Rn×m
+

,P1m=a,PT 1n=b

f ∈Rn,g∈Rm

where C = [c(xi, yj)]i,j = [ci,j]i,j, and K = e−C/ε is the kernel. While (f , g) are unconstrained for
ε > 0, the regularization term converges as ε → 0 to an indicator function that requires fi + gj ≤ ci,j.

The Sinkhorn algorithm (Alg.
1) is a se-
quence of updates designed to cancel the gradients
∇1Eε(f , g) (line 4) and ∇2Eε(f , g) (line 5) of the
energy Eε in (1) (this interpretation is only valid
if the momentum factor ω is set to 1, its default
value). These updates use the line-wise soft-min
operator minε, deﬁned for a matrix S = [Si,j] as
minε(S) = [−ε log (cid:0)1T e−Si,·/ε(cid:1)]i and the tensor
addition notation f ⊕ g = [fi + gj]i,j. The runtime
of the Sinkhorn algorithm hinges on several factors, chieﬂy among them the choice of ε. It is

Algorithm 1: Sinkhorn Algorithm
1: Input: a, b, C, ε > 0, ω > 0, f (0), g(0).
2: Initialize: f ← f (0), g ← g(0)
3: while not converged do
4:
5:
6: end while
7: Return f , g

f ← ω(ε log a − minε(C − f ⊕ g)) + f
g ← ω(ε log b−minε(CT −g⊕f ))+g

2

reported in the literature that hundreds of iterations are typically required when using fairly small
regularization ε and a reasonable convergence threshold (e.g. 500 in Salimans et al. 2018, App.B).
These scalability issues are compounded in advanced applications whereby multiple Sinkhorn layers
are embedded in a single computation or batched across examples (Cuturi et al., 2019; Xie et al.,
2020a; Cuturi et al., 2020). To mitigate runtime issues, popular acceleration techniques such as
ﬁxed (Thibault et al., 2021) or adaptive Lehmann et al. (2021) momentum approaches, as well as
Anderson acceleration (Chizat et al., 2020) have been considered. In principle, these improvements
help Sinkhorn converge faster, regardless of the quality of the initializers f (0) and g(0). In practice,
however, most acceleration methods are known to work well when initialized not too far away from
optima (d’Aspremont et al., 2021), thereby reinforcing the need to obtain better guesses for f (0)
and g(0). Although the distance between initializations f (0), g(0) and their optimal values plays an
important role in the runtime analysis of the Sinkhorn algorithm (Franklin and Lorenz, 1989), these
vectors are initialized to 0n, 0m in most implementations (Flamary et al., 2021; Cuturi et al., 2022).

2.2 Closed-Form Expressions in Optimal Transport

(cid:90)

This work draws inspiration from known closed-forms for
unregularized OT, to then compute dual initializations f (0)
and g(0). Some of these closed forms rely on the Monge for-
mulation of OT, recalled for completeness for two measures
µ, ν ∈ P(Rd) in (2), using the push-forward (cid:93) notation, as
well on the dual formulation of OT, recalled in (3) in the case
where the cost is the squared Euclidean distance and denoting f ∗ the Legendre transform of f .
We review two relevant cases, where either or both an optimal coupling, namely a P(cid:63) exists for ε = 0
in the primal formulation of (1)), or an optimal map T (cid:63) to (2) can be obtained in closed form. We
show in §3 how these solutions can be leveraged to recover vectors of dual variables.

c(x, T (x))dµ(x). (2)

min
T :Rd→Rd
T(cid:93)µ=ν

max
f :Rd→R, cvx

(cid:90)
f dµ +

f ∗dν .

(3)

(cid:90)

One dimensional considerations As outlined in (Santambrogio, 2015, Chap.3) and (Chiappori et al.,
2017), in the case where dimension d = 1, and if the cost function c is such that −c is supermodular
(i.e. ∂c/∂x∂y < 0) then both a solution P(cid:63) to (1) for ε = 0 and T (cid:63) to (2) can be recovered in closed
form. Writing σ, ρ for sorting permutations on µ and ν, namely ensuring xσ1 ≤ · · · ≤ xσn and
yρ1 ≤ . . . yρm, then the solution P(cid:63) is the so-called north-west corner solution (Peyré et al., 2019,
§3.4.2) NW(aσ, bρ), where aσ and bρ are the weight vectors a, b permuted according to σ and ρ
respectively. Similarly, a Monge map can be recovered through the cumulative distribution functions
of µ, ν Fµ, Fν. In that case, writing F −1
for the generalized inverse of Fν (the quantile function of
ν) one has that the transport map T (cid:63) is given by T (cid:63)(x) = F −1

◦ Fµ(x).

ν

ν

Gaussian. The transport map T solving the Monge problem (2) from a non-degenerate Gaussian
measure µ = N (mµ, Σµ) to another Gaussian ν = N (mν, Σν) can also be recovered in closed-
form as T (cid:63)(x) := A(x − mµ) + mν, where A = Σ− 1
µ . The Brenier potential
corresponding to that map is a quadratic form ϕ(x) = 1
2 xT Ax + (mν − Amµ)T x (deﬁned up to
an arbitrary constant), to recover ∇ϕ = T (cid:63). Finally, the optimal transport cost itself between the
measures is known as the Bures-Wasserstein distance and is equal to

2 Σ− 1

µ ΣνΣ

µ (Σ

µ ) 1

1
2

1
2

2

2

W 2

2 (N (mµ, Σµ), N (mν, Σν)) = (cid:107)mµ − mν(cid:107)2 + tr(Σµ + Σν − 2(Σ

µ ΣνΣ

1
2

1
2

µ )

1
2 ).

(4)

3 From Primal Solutions to Dual Sinkhorn Initializations

(cid:107)f ((cid:96)) − f (cid:63)(cid:107)var ≤ (cid:107)f (0) − f (cid:63)(cid:107)varλ(K)2(cid:96)

On the beneﬁt of starting closer to the solution.
While the Sinkhorn algorithm does converge for any
initialization (the dual problem is concave and unconstrained), results on the convergence of Sinkhorn
potentials (Peyré et al., 2019, Remark 4.14) yield, denoting f ((cid:96)) the vector f obtained after running
Algorithm 1 for exactly (cid:96) iterations, the bound given in (5) where deviation is measured using the
variation norm and λ(K) is a type of conditioning number of K (see (Peyré et al., 2019, Theorem
4.1)), which depends on the range of values of the cost matrix C, and therefore the spatial dispersion
of both the xi and yj, as well as the regularization strength ε. One has that 0 < λ(K) < 1, and the
Sinkhorn algorithm runs into convergence issues as λ(K) gets closer to 1. The motivation to obtain a
better initializer is therefore found in targeting the initial gap in (cid:107)f (0) − f (cid:63)(cid:107)var through other means.

(5)

3

Two or one Dual Initializations. While Algorithm 1 mentions two dual initial vectors (f (0), g(0)), a
careful inspection of the updates show that it is in fact equivalent to only supply a single dual variable:
when starting with an iteration updating g, only f (0) is needed (the reference to g is only there for
numerical stability). Conversely, only g(0) is required when updating f . Since only either is needed,
we choose by default to initialize the smallest vector of the two when n (cid:54)= m.

Differentiability and Dual initialization. The simplest way to differentiate the output of the
Sinkhorn ﬁxed-point algorithm is to unroll its iterations (Adams and Zemel, 2011; Hashimoto
et al., 2016; Genevay et al., 2018, 2019; Cuturi et al., 2019; Caron et al., 2020). This approach
has, however, several drawbacks: its memory footprint grows with the number of iterations needed
to converge, and it forbids using non-differentiable acceleration methods such as Anderson accel-
eration or adaptive momentum. As a result, a more recent body of work advocates using implicit
differentiation instead (Luise et al., 2018; Cuturi et al., 2020; Xie et al., 2020b). At the cost of
solving a linear system, this approach bypasses memory storage issues and permits the use of more
advanced optimization strategies. Because of these practical advantages, implicit differentiation
should be preferred to unrolling for most practical applications, and this is likely to be more so as
better optimization recipes are applied to the Sinkhorn problem. It is, for instance the default setting
in (Cuturi et al., 2022).

3.1 Recovering Dual Potentials from an Optimal Primal

Properties of the optimal primal P(cid:63). Taking the 1D case as motivation, we present a method to
recover optimal dual potentials f (cid:63), g(cid:63) from an optimal primal solution P(cid:63). To that end, one can cast
an OT problem as a min-cost-ﬂow problem on a bipartite graph G = (V, E), with vertices composed
of source nodes S = {1, . . . , n} and target nodes T = {1(cid:48), . . . , m(cid:48)}, V = S ∪ T , and edge set
E = {(i, j(cid:48)), i = 1, . . . , n; j = 1, . . . , m} linking them. The KKT conditions state that, writing
E(P) = {(i, j(cid:48))|Pi,j > 0} one has that the graph (V, E(P(cid:63))) is necessarily a forest (Peyré et al., 2019,
Prop. 3.4). We write T1, . . . , TK for the K trees forming that forest, where 1 ≤ K ≤ min(n, m),
and write tk for their size. We use the lexicographic order to deﬁne the root node of each tree, chosen
to be the smallest source node s(k) contained in Tk. For convenience, we assume that trees are
ordered following s(k), and therefore that T1 has 1 as its root node. For each tree k, we introduce
pk = (pk
tk−1) for a pre-order breadth-ﬁrst-traversal of Tk originating at s(k), enumerating
tk − 1 edges, namely pairs in S × T or S × T , guaranteed to be such that any parent node in the tree
is visited before its descendants. ι(j) denotes the smallest source index i such that (i, j(cid:48)) ∈ E(P(cid:63)).

1, . . . , pk

Complementary and Feasibility Constraints.
Complementary slackness provides a set of
n + m − K linear equations (6), while feasi-
bility constraints are given in (7).
i + g(cid:63)

(i, j(cid:48)) ∈ E(P(cid:63)) ⇔ f (cid:63)

j = ci,j ,

(6)

∀i ≤ n, j ≤ m, fi + gj ≤ ci,j .
(7)
For the special case K = 1, which happens
for instance when n and m are co-primes and
weights are uniform, the set of linear equations
(6) sufﬁces to recover the n + m dual vari-
ables, with the convention that the ﬁrst entry
be 0. When, on the contrary, K > 1, that set of
n + m − K equations is no longer sufﬁcient. For
example, K = n = m for the optimal assign-
ment problem, in which (V, E(P(cid:63))) describes
a set of n isolated trees, and only n equality re-
lations are available for 2n variables. In such
cases, one must additionally use the feasibility
constraint (7) to obtain optimal dual variables
(Peyré et al., 2019, Prop 3.3).
The c-transform gc
i := minj ci,j − gj can be
used to enforce constraints (7), however, it may
no longer satisfy the complementary condition

for k ∈ {2, . . . , K} do

Algorithm 2: Recover dual from primal
1: Input: Cost matrix C and graph (V, E(P(cid:63)))
2: Initialize: f = 0.
3: while not converged do
4:
5:
6:
7:
8:
end for
9:
10: end while
11: Return f

end for
for k ∈ {1, . . . , K} do

fs(k) ← minj cs(k),j − cι(j),j + fι(j)

f ← UPDATETREE(C, f , k)

Algorithm 3: UPDATETREE
1: Input: Cost matrix C, f , tree index k
2: for e = (a, b) ∈ pk do
if a ∈ S, b ∈ T then
3:
g ← ca,b − fa
4:
5:
6:
end if
7:
8: end for
9: Return f

fa ← ca,b − g

else

4

(6). This is remedied by updating all source nodes i in tree k by starting from s(k) as detailed in
Algorithm 3. Repeated application of these updates, Algorithm 2, guarantees convergence.
Lemma 1. Given the optimal coupling matrix P∗ solving OT problem (1) with ε = 0, the procedure
deﬁned in Algorithm 2 converges to the optimal dual potentials for dual problem (3).

The proof is provided in Appendix E, and uses the fact that Algorithm 2 is a primal-dual method
(Dantzig et al., 1956), tweaked because the primal solution P(cid:63) is known.

3.2 Faster Differentiable Sorting and Ranking using Dual Initializers

Sorting as an OT problem. Given any cost function c : R × R → R such that ∂c/∂x∂y < 0,
i=1 ∈ Rn can be recovered from an optimal transport matrix P(cid:63) computed
sorting an array x = (xi)n
between uniformly weighted x to a uniformly weighted measure of increasing numbers, for instance
y = (1, 2, . . . , n). Cuturi et al. (2019) show that the vector of ranks of x can be recovered as nP(cid:63)z
where z = (1, 2, ..., n), whereas the sorted vector of x is nP(cid:63)T x.

Differentiable sorting. A differentiable and fractional soft sorting/ranking operator can be derived
from entropy regularized transport couplings, between input values x and a vector of increasing
target values y (Cuturi et al., 2019), using a solution Pε to (1) to form nPεz and nPT
ε x. This “soft”
formulation has the added ﬂexibility that the size m and values of weights a, b can be adjusted. A
practical drawback of that approach is that the number of Sinkhorn iterations needed for the coupling
to converge can be large, see Figure 3 and further results in Appendix B.1.

Dual 1D Initializers. When considering the case where the target family y has size n and uniform
weights, the sorting problem corresponds to an optimal assignment problem for which K = n. Recall
sorting permutation σ, one can therefore construct complementary potentials efﬁciently by setting
gj = cσ−1(j),j − fσ−1(j) for each j and computing potentials by iterating only the c-transform from
Algorithm 2, as the second update is redundant. This update can be applied coordinate by coordinate
or vectorized. We refer to this latter approach as DUALSORT (see Algorithm 4 in Appendix E).
DUALSORT recovers dual potentials with a O(n2) complexity. Moreover, 2 inner iterations of
DUALSORT have the same computational complexity as 1 block iteration of the Sinkhorn algorithm.
We observe empirically that Algorithm 2 converges extremely quickly, as discussed in the Appendix,
and we set a predeﬁned limit of 3 number of iterations.

3.3 Computing Dual Initializers from Gaussian Approximations

We explore in this section the possibilities offered by Gaussian approximations to obtain cheap
initializers for the Sinkhorn problem, in the so-called 2-Wasserstein setting, where c(x, y) = (cid:107)x−y(cid:107)2
2.

From optimal potentials to dual initializers. Through
Brenier’s theorem, one can easily recover a dual potential
f (cid:63) solving the dual problem (3) between Gaussian mea-
sures N (mµ, Σµ) and N (mν, Σν). The dual potential
reads in closed form:
2 (cid:107)x(cid:107)2 − 1

2 (x − mµ)T A(x − mµ) − mT

f (cid:63)(x) = 1

ν x,

i=1 aiδxi and ν = (cid:80)m

Figure 1: Transport map (black) from
Gaussian approximations (dashed) of S-
curve (green) and two-moons (red)

where A is deﬁned in §2. We use this approximation
to initialize Sinkhorn potentials as follows. Given two
discrete measures µ = (cid:80)n
j=1 bjδyj
where xi, yj ∈ Rd, pre-compute ﬁrst their empirical mean and covariance matrices (mµ, Σµ) and
(mν, Σν), and follow by evaluating the potential above on all observed points of the ﬁrst measure
f (0)
i ← f (cid:63)(xi) (or alternatively the second measure if m < n) to seed the Sinkhorn algorithm.
Complexity comparison.
Solving the 2-
Wasserstein problem on the Gaussian approxi-
mations of µ, ν, requires pre-computing means
and covariance matrices O((n + m)d2), before
computing matrix square-roots and their inverse,
which can be both obtained using the Newton-
Schulz iterations (Higham, 2008) at cost O(d3).

Table 1: Gaussian initialization on toy examples
n = m = 1024, d = 2, 200 seeds.

# Iterations (mean ± std)

Init Gaus

Dataset

Init 0

2-moons
S curve / 2-m.
3 Gauss. blobs

120.0 ± 0.0
137.2 ± 16.7
236.0 ± 24.3

11.0 ± 0.0
49.6 ± 14.8
45.4 ± 9.7

5

Obtaining a Gaussian approximation for dual variables to seed Sinkhorn is therefore particularly
relevant in settings where d (cid:28) n, which is the regime where OT can prove the most useful.

Relevance in high-dimensional settings. One may question whether using Gaussian approxima-
tions for non-Gaussian discrete distributions is reasonable in high dimensional, multi-modal settings.
However, we argue that matching ﬁrst and second moments of µ to ν is better than data oblivious
approaches. These Gaussian initializers are shown in our experiments to work at least as well, and
often signiﬁcantly better, than the default null initialization in a variety of cases from toy examples as
shown in Table 1, OT in a latent space as shown in Section 4.3, to word-embeddings as demonstrated
in Section 4.4. The overhead induced by the computations of dual solutions is naturally dictated by
the tradeoff between n (the number of points) and d (their dimension). In all cases considered here
that overhead is negligible, but explored with more care in Appendix C.

3.4 Leveraging Gaussian Mixture Approximations

The Gaussian initialization approach can be extended to Gaussian mixture models (GMMs), pending
further approximations, and at the additional cost of pre-estimating GMMs for each input measure.
By further approximations above, we refer more explicitly to the fact that, unlike for single Gaussians,
we do not have access to closed-form OT solutions between GMMs, but instead only couplings that
return a cost that is an upper-bound on the true Wasserstein distance between two GMMs. We show
how to derive dual initializers from these approximate couplings.

k=1 αk = (cid:80)K

k=1 αkρk and τ = (cid:80)K

OT between GMMs as OT in the space of Gaussian measures. Consider Gaussian mixtures
ρ = (cid:80)K
k=1 βkτk, assuming each ρk and τk is itself a Gaussian measure, and
(cid:80)K
k=1 βk = 1. It was proposed in Chen et al. (2018) to approximate the continuous
OT problem between ρ and τ in the space Rd with the discrete OT problem in the space of mixtures
of Gaussians, where each K-mixture is a discrete measure with K atoms, each atom being a
Gaussian, and the ground cost between them being the pairwise Bures-Wasserstein distance between
2 (ρi, τj)]ij as in (4), resulting in two potentials ˜f and ˜g that solve the
the Gaussians C = [W 2
corresponding regularized K × K OT problem.

Approximating Dual Potentials with
Gaussian Mixtures. Our goal in this section is
to provide an approximation ˆf of f (cid:63), the poten-
tial that would be such that
(cid:90)

(cid:90)

W 2(ρ, τ ) =

f (cid:63)(x)dρ(x) +

(f (cid:63))∗(y)dτ (y).

Such an approximation can be then be used
to initialize potentials, f (0)
i ← ˆf (xi). We
propose to do so by ﬁrst ﬁtting mixtures τ
and ρ to input measures µ = (cid:80)n
i=1 aiδxi and
ν = (cid:80)n
j=1 bjδyj , as advocated by Delon and
Desolneux (2020). Potential vectors ˜f ∈ RK
are then recovered using the Sinkhorn algorithm
on a K × K problem, OT approximation for
GMMs as described above. From those potentials, we propose to compute an approximate ˆf dual
potential function taking inspiration from the following approximation, which is (as in the spirit of
OT for GMMs) only valid for concentrated Gaussians:

Figure 2: Gap between the true dual and the GMM
approximate dual for a pair of measures taken
within the Newsgroup embeddings dataset, as a
function of number of mixture components.

(cid:90)

K
(cid:88)

Rd

k=1

f (cid:63)(x)αkdρk(x) ≈

(cid:90)

K
(cid:88)

Rd

k=1

˜fkαkdρk(x),

with (cid:82) (cid:80)K

k=1 f (cid:63)(x)αkdρk(x) = (cid:82) f (cid:63)(x) (cid:80)K

k=1 αkdρk(x), gives ˆf (x) :=

(cid:80)K

˜fkαkdρk(x)
k=1
(cid:80)K
k=1 αkdρk(x)

.

Intuitively this approximation is a Nadaraya-Watson type interpolator between Gaussian mixture
components, that recovers the optimal Sinkhorn potentials, f (cid:63), in the case where K = n components
with means (xi)i and zero covariance.

6

020406080Index, i0.000.050.100.150.20|̂fi−f*i|25 Components10 Components25 Components50 ComponentsComplexity. Computing OT losses between Gaussian measures supported on Rd has complexity
O(d3) for full covariance matrices, however only O(d) for diagonal covariance matrices, and so
computing the cost matrix for the GMM OT problem would cost O(K 2d3) or O(K 2d). It costs
O(Ln2) to run the Sinkhorn algorithm between pointclouds of size n for L iterations and so the
proposed GMM initialization may provide efﬁciency gains when K 2d (cid:28) n2.

4 Experiments

In this section we illustrate the beneﬁts of our proposed initialization strategies, in particular using
DUALSORT for differentiable sorting and soft-0/1 loss from (Cuturi et al., 2019), and Gaussian
initializers for deep differentiable clustering from (Genevay et al., 2019) and for document similarity
on word embeddings. The purpose of these experiments is to show the beneﬁt of the initializer
and not the performance in the particular tasks or in claiming these tasks are original. With that
in mind, we have not performed extensive network parameter tuning, though we do include some
performance metrics to illustrate that the setups are reasonable. Further experimental details are given
in Appendix B. Experiments were carried out using OTT-JAX (Cuturi et al., 2022) including their
implementation of acceleration methods for comparison.

4.1 Differentiable Sorting

Figure 3 illustrates the dramatic speed-up effect from using the DUALSORT procedure, with just 3
vectorized iterations. In this experiment arrays of size n ∈ {16, 32, 64, 128, 256, 512, 1024} were
sampled from the Gaussian blob dataset (Pedregosa et al., 2011) for 200 different seeds. At each
seed, 1-dimensional Gaussian data is generated from 5 random centers with centers uniformly
distributed in (−10, 10) with standard deviation 3. The Sinkhorn algorithm was then ran with the
proposed initialization, DUALSORT and with the default zero initializer, labelled 0. Other Sinkhorn
acceleration methods were also investigated including Anderson acceleration (And= 5), momentum
(mom. = 1.05), regularization decay (ε decay = 0.8) and adaptive momentum (adapt= 10). Figure
3a compares Sinkhorn algorithm with initialization to Sinkhorn enhanced through other acceleration
method. Figure 3b illustrates the relative-speed up from including initialization along with other
enhancements where speed-up is deﬁned as the ratio of iterations using the zero initializer and the
DUALSORT initializer, hence > 1 indicates an improvement using DUALSORT.

(a) Number of iterations (lower is better)

(b) Relative speed-up (higher is better)

Figure 3: Left: we show that DUALSORT with a default Sinkhorn setup dominates all existing
acceleration methods implemented when run with a default 0 initialization. We plot median, upper
and lower quartiles of iterations needed to converge over 200 seeds for various array sizes (iterations
for DUALSORT include steps for the primal-dual procedure). Right: assuming both the 0 and the
DUALSORT initializers are coupled with acceleration methods, we still observe, no matter what
acceleration is used, very large speedups.

4.2 Soft Error Classiﬁcation

The following experiment demonstrates the differentiability of the soft-sorting and rank-
ing operations as well as how the DUALSORT initializer improves computational perfor-
mance for real tasks. We follow the experimental setup from Cuturi et al. (2019). Let

7

242526272829210Array size, n22242628210212Iterations0  + Default0  + And.=50  + ω mom.=1.050  + adapt.=100  + ε decay=0.8DualSort + Default242526272829210Array size, n2022242628210Speed-upDefaultAnd.=5ω mom.=1.05adapt.=10ε decay=0.8No speed-upFor input x ∈ X ,

hθ : X → RK be a parameterized K-label classiﬁer and R the differentiable ranking oper-
the soft-0/1 loss evaluated at x is
ator described in Section 3.2.
therefore max(0, K − R(hθ(x)). We refer readers to Cuturi et al. (2019) for more detail.
The classiﬁer network from Cuturi et al. (2022) is
used for CIFAR-100, consisting of four CNN layers,
and a fully connected hidden layer with size 512,
more details given in Appendix B.2. Regularization
parameter, ε was set to 0.01 and the network was
trained until convergence over 10 seeds. DUALSORT
initializer was ran with a maximum of 3 iterations,
which, as discussed in Section 3.2, is computationally
cheaper than two Sinkhorn iterations.

Table 2: Soft-Error: CIFAR 100

Sink. Iter. (mean ± std)

Initializer

Accuracy on the evaluation set is shown in Section 4.2
for 300 epochs. It is clear from the accuracy curves
that, as expected, the Sinkhorn initialization proce-
dure does not signiﬁcantly affect accuracy. However, Section 4.2 illustrates that the number of
Sinkhorn iterations required to converge at each iteration is reduced drastically using the DUALSORT
initializer.

Zero
Anderson
Momentum
Adaptive
ε-decay
DualSort
DualSort, Adap.
DualSort, Ande.

17.9 ± 0.1
12.3 ± 0.2
15.7 ± 0.2
15.2 ± 0.2
17.0 ± 0.1
9.7 ± 0.1
10.3 ± 0.1
8.2 ± 0.1

Figure 4: (left) Accuracy of CNN classiﬁer trained with various initializers and Sinkhorn methods in
the soft-error classiﬁer experiment for CIFAR-100 with ε = 0.01. (right) For each method, rolling
median of number of Sinkhorn iterations to converge, as a function of epoch, in addition to quartiles
1 and 3, using 10 runs.

4.3 Differentiable Clustering

We demonstrate the performance improvement from
the Gaussian initializer on the task of deep differ-
entiable clustering, with the experimental setup of
Genevay et al. (2018). Differentiable clustering aims
at producing a latent representation amenable to clus-
tering. This is achieved using a variational autoen-
coder (Kingma et al., 2014) with learnable, discrete
cluster embeddings, and an additional loss term allo-
cating encodings to cluster embeddings using OT.

Table 3: Avg. Sinkhorn iter./training step

Num Iterations (mean ± std)

ε = 0.1

ε = 0.01

Zero
ε-decay
Anderson
Momentum
Adaptive
Gaus
Gaus, Adapt.
Gaus, Ander.

34.2 ± 0.84
33.0 ± 1.0
17.1 ± 0.3
32.6 ± 0.6
21.5 ± 0.2
17.3 ± 0.7
16.7 ± 0.9
9.5 ± 0.29

354.1 ± 7.0
340.5 ± 17.8
844.4 ± 26.2
342.5 ± 3.7
96.6 ± 4.1
196.6 ± 6.7
68.7 ± 1.3
419.9 ± 56.3

For data of dimension dx and latent dimension dz, let
Eθ : Rdx → R2×dz and Dθ : Rdz → Rdx denote an
encoder and decoder respectively, parameterized by θ.
Let µφ ∈ RK×dz denote cluster embeddings for K
clusters. The objective of differentiable clustering is to learn Eθ, Dθ and embeddings µφ ∈ RK×dz .
This may be achieved by minimizing the loss (cid:96)ae(θ) + (cid:96)OT(φ, θ) for each batch of data (xi)i. Here
(cid:96)ae(θ) is the standard variational auto-encoder loss and (cid:96)OT(φ, θ) is the regularized OT loss from
(1) between µ = (cid:80)K
1
n δzi. zi = mi + σiui, where (mi, σi) = Eθ(xi),
ui ∼ N (0dz , Idz ), and ˜xi = Dθ(zi).

K δµφk and ν = (cid:80)n

k=1

i=1

1

8

050100150200250300Epoch0.00.20.40.6Accuracy0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultDualSort + DefaultDualSort + adapt.=10DualSort + And.=5050100150200250300Epoch10152025IterationsWe demonstrate this task for MNIST (Deng, 2012) over 10 seeds. Fully connected networks with 4
hidden layers were used for Eθ and Dθ, where dz = 32 and dx = 784, further experimental details
are given in Appendix B.3.

Figure 5: Differentiable clustering on MNIST dataset, rolling median, quartile 1 and 3, per epoch,
averaged over 10 seeds. Right: ε = 0.01, left: ε = 0.1.

Figure 5 and Table 3 demonstrate that the Gaussian initializer outperforms the zero initialization
for default Sinkhorn and all other combinations of default Sinkhorn plus acceleration techniques.
Performance metrics and samples from the generative model are given in Appendix B.3.

4.4 Word Embeddings

In the following experiment, documents were gathered from the 20 Newsgroup dataset (Lang, 1995)
and each word, (wi)n
i=1, in the vocabulary across documents is embedded using the pre-trained GloVe
i=1 where ei ∈ R50.
word embeddings (Pennington et al., 2014) as (ei)n
Each document can be represented as a his-
togram with weights (ai)n
i=1 corresponding to
word-frequency, νi = (cid:80)n
i=1 aiδei. We com-
pute pairwise OT distances between 50 docu-
ments, resulting in 1225 pairs, and report the
number of Sinkhorn iterations required for con-
vergence for the default zero intializer (0), Gaus-
sian (Gaus) and Gaussian mixture initializers
with full covariance matrices (GMM K) and
K ∈ {10, 25, 50} components. This experiment
was carried out using a subset of the vocabulary
of size n for n ∈ {2 × 103, 5 × 103, 104} using
regularization ε = 0.001. The distribution of
results are shown in Figure 6 illustrating that
improvements can be obtained for a range of K.

Figure 6: Distribution of number of Sinkhorn iter-
ations required for OT between Newsgroup docu-
ments with word embeddings.

5 Conclusion

We have demonstrated how carefully chosen initializations can signiﬁcantly improve the performance
of the Sinkhorn algorithm. We have provided efﬁcient, effective and robust initializers that are
applicable to a wide range of settings, and which may be embedded in end-to-end differentiable
procedures. Initialization is a neglected area of computational OT, and we hope that these promising
results can inspire new researches to address these limitations. In particular, extending these strategies
to non-Euclidean settings provides an interesting research opportunity.

9

020406080100Epoch232425Iterations020406080100Epoch26272829210Iterations0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultGaus + DefaultGaus + adapt.=10Gaus + And.=52000500010000n0200400600Iterations0GausGMM K=10GMM K=25GMM K=50References

Adams, R. P. and Zemel, R. S. (2011). Ranking via sinkhorn propagation. arXiv preprint

arXiv:1106.1925.

Ahuja, R. K., Magnanti, T. L., and Orlin, J. B. (1988). Network ﬂows.

Altschuler, J., Bach, F., Rudi, A., and Niles-Weed, J. (2019). Massively scalable sinkhorn distances

via the nyström method. Advances in neural information processing systems, 32.

Amos, B., Cohen, S., Luise, G., and Redko, I. (2022). Meta optimal transport.

Anderson, D. G. (1965). Iterative procedures for nonlinear integral equations. Journal of the ACM

(JACM), 12(4):547–560.

Bertsimas, D. and Tsitsiklis, J. N. (1997). Introduction to linear optimization, volume 6. Athena

Scientiﬁc Belmont, MA.

Brenier, Y. (1987). Décomposition polaire et réarrangement monotone des champs de vecteurs. CR

Acad. Sci. Paris Sér. I Math., 305:805–808.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-end
object detection with transformers. In European conference on computer vision, pages 213–229.
Springer.

Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. (2020). Unsupervised
learning of visual features by contrasting cluster assignments. Advances in Neural Information
Processing Systems, 33:9912–9924.

Chen, Y., Georgiou, T. T., and Tannenbaum, A. (2018). Optimal transport for gaussian mixture

models. IEEE Access, 7:6269–6278.

Chiappori, P.-A., McCann, R. J., and Pass, B. (2017). Multi-to one-dimensional optimal transport.

Communications on Pure and Applied Mathematics, 70(12):2405–2444.

Chizat, L., Roussillon, P., Léger, F., Vialard, F.-X., and Peyré, G. (2020). Faster wasserstein distance
estimation with the sinkhorn divergence. Advances in Neural Information Processing Systems,
33:2257–2269.

Corenﬂos, A., Thornton, J., Deligiannidis, G., and Doucet, A. (2021). Differentiable particle ﬁltering
via entropy-regularized optimal transport. In International Conference on Machine Learning,
pages 2100–2111. PMLR.

Courty, N., Flamary, R., Habrard, A., and Rakotomamonjy, A. (2017). Joint distribution optimal
transportation for domain adaptation. Advances in Neural Information Processing Systems, 30.

Courty, N., Flamary, R., and Tuia, D. (2014). Domain adaptation with regularized optimal transport.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
pages 274–289. Springer.

Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. Advances in

neural information processing systems, 26.

Cuturi, M., Meng-Papaxanthos, L., Tian, Y., Bunne, C., Davis, G., and Teboul, O. (2022). Optimal
transport tools (ott): A jax toolbox for all things wasserstein. arXiv preprint arXiv:2201.12324.

Cuturi, M. and Peyré, G. (2015). A smoothed dual approach for variational wasserstein problems.

arXiv preprint arXiv:1503.02533.

Cuturi, M., Teboul, O., Niles-Weed, J., and Vert, J.-P. (2020). Supervised quantile normalization for
low rank matrix factorization. In International Conference on Machine Learning, pages 2269–2279.
PMLR.

Cuturi, M., Teboul, O., and Vert, J.-P. (2019). Differentiable ranking and sorting using optimal

transport. Advances in neural information processing systems, 32.

10

Dantzig, G. B., Ford Jr, L. R., and Fulkerson, D. R. (1956). A primal–dual algorithm. Technical

report, RAND CORP SANTA MONICA CA.

d’Aspremont, A., Scieur, D., and Taylor, A. (2021). Acceleration methods. arXiv preprint

arXiv:2101.09545.

Deligiannidis, G., De Bortoli, V., and Doucet, A. (2021). Quantitative uniform stability of the iterative

proportional ﬁtting procedure. arXiv preprint arXiv:2108.08129.

Delon, J. and Desolneux, A. (2020). A wasserstein-type distance in the space of gaussian mixture

models. SIAM Journal on Imaging Sciences, 13(2):936–970.

Deng, L. (2012). The mnist database of handwritten digit images for machine learning research.

IEEE Signal Processing Magazine, 29(6):141–142.

Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L.,
Corenﬂos, A., Fatras, K., Fournier, N., et al. (2021). Pot: Python optimal transport. Journal of
Machine Learning Research, 22(78):1–8.

Franklin, J. and Lorenz, J. (1989). On the scaling of multidimensional matrices. Linear Algebra and

its Applications, 114:717–735.

Genevay, A., Dulac-Arnold, G., and Vert, J.-P. (2019). Differentiable deep clustering with cluster

size constraints. arXiv preprint arXiv:1910.09036.

Genevay, A., Peyré, G., and Cuturi, M. (2018). Learning generative models with sinkhorn divergences.
In International Conference on Artiﬁcial Intelligence and Statistics, pages 1608–1617. PMLR.

Hashimoto, T., Gifford, D., and Jaakkola, T. (2016). Learning Population-Level Diffusions with

Generative Recurrent Networks. volume 33.

Higham, N. J. (2008). Functions of matrices: theory and computation. SIAM.

Janati, H., Bazeille, T., Thirion, B., Cuturi, M., and Gramfort, A. (2020). Multi-subject meg/eeg

source imaging with sparse multi-task regression. NeuroImage, 220:116847.

Kingma, D. P., Mohamed, S., Jimenez Rezende, D., and Welling, M. (2014). Semi-supervised
learning with deep generative models. Advances in neural information processing systems, 27.

Kosowsky, J. and Yuille, A. L. (1994). The invisible hand algorithm: Solving the assignment problem

with statistical physics. Neural networks, 7(3):477–490.

Lang, K. (1995). Newsweeder: Learning to ﬁlter netnews. In Proceedings of the Twelfth International

Conference on Machine Learning, pages 331–339.

Lehmann, T., Von Renesse, M.-K., Sambale, A., and Uschmajew, A. (2021). A note on overrelaxation

in the sinkhorn algorithm. Optimization Letters, pages 1–12.

Luise, G., Rudi, A., Pontil, M., and Ciliberto, C. (2018). Differential properties of sinkhorn
approximation for learning with wasserstein distance. Advances in Neural Information Processing
Systems, 31.

Meyron, J. (2019).

Initialization procedures for discrete and semi-discrete optimal transport.

Computer-Aided Design, 115:13–22.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830.

Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation.

In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.

Peyré, G., Cuturi, M., et al. (2019). Computational optimal transport: With applications to data

science. Foundations and Trends® in Machine Learning, 11(5-6):355–607.

11

Salimans, T., Zhang, H., Radford, A., and Metaxas, D. (2018). Improving GANs using optimal

transport. In International Conference on Learning Representations.

Sander, M. E., Ablin, P., Blondel, M., and Peyré, G. (2022). Sinkformers: Transformers with doubly
stochastic attention. In International Conference on Artiﬁcial Intelligence and Statistics, pages
3515–3530. PMLR.

Santambrogio, F. (2015). Optimal transport for applied mathematicians. Birkhauser.

Sarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich, A. (2020). Superglue: Learning feature
matching with graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR).

Scetbon, M. and Cuturi, M. (2020). Linear time sinkhorn divergences using positive features.

Advances in Neural Information Processing Systems, 33:13468–13480.

Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S.,
Lin, S., Berube, P., et al. (2019). Optimal-Transport Analysis of Single-Cell Gene Expression
Identiﬁes Developmental Trajectories in Reprogramming. Cell, 176(4).

Schmitz, M. A., Heitz, M., Bonneel, N., Ngole, F., Coeurjolly, D., Cuturi, M., Peyré, G., and Starck,
J.-L. (2018). Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear
dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643–678.

Schmitzer, B. (2019). Stabilized sparse scaling algorithms for entropy regularized transport problems.

SIAM Journal on Scientiﬁc Computing, 41(3):A1443–A1481.

Sinkhorn, R. (1967). Diagonal equivalence to matrices with prescribed row and column sums.

American Mathematical Monthly, 74:402–405.

Solomon, J., De Goes, F., Peyré, G., Cuturi, M., Butscher, A., Nguyen, A., Du, T., and Guibas,
L. (2015). Convolutional Wasserstein distances: efﬁcient optimal transportation on geometric
domains. ACM Transactions on Graphics, 34(4):66:1–66:11.

Thibault, A., Chizat, L., Dossal, C., and Papadakis, N. (2021). Overrelaxed sinkhorn–knopp algorithm

for regularized optimal transport. Algorithms, 14(5):143.

Xie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H., Wei, W., and Pﬁster, T. (2020a). Differentiable
top-k with optimal transport. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H., editors, Advances in Neural Information Processing Systems, volume 33, pages 20520–20531.
Curran Associates, Inc.

Xie, Y., Wang, X., Wang, R., and Zha, H. (2020b). A fast proximal point method for computing exact

wasserstein distance. In Uncertainty in artiﬁcial intelligence, pages 433–453. PMLR.

12

A Dual Potential Comparison

Dual potentials f , g are unique up to constant shifts i.e. f − s, g + s for s ∈ R. Therefore, in order to
compare potentials f ∈ Rn, we center f , as f ← f − 1
n

i fi.

(cid:80)

B Further Experimental Details

B.1 Differentiable Sorting Details

Regularization ε = 0.01 was used, as per Cuturi et al. (2019). In this experiment arrays of size
n ∈ {16, 32, 64, 128, 256, 512, 1024} were sampled from the Gaussian blob dataset (Pedregosa et al.,
2011) for 200 different seeds. At each seed, 1-dimensional Gaussian data is generated from 5 random
centers with centers uniformly distributed in (−10, 10) with standard deviation 3.

Baseline acceleration methods (Anderson acceleration, momentum, adaptive momentum, (cid:15) decay)
were considered to augment the Sinkhorn algorithm, using the implementations from Cuturi et al.
(2022). The momentum hyper-parameter ω was set at 1.05 from a grid search of {0.8, 1.05, 1.1, 1.3}.
Adaptive momentum consists of adjusting the momentum parameters every adapt_iters number of
iterations where adapt_iters was set to 10 from a search on {10, 20, 50, 200}. (cid:15) decay consisted
of gradually reducing the regularization term from 5(cid:15) to (cid:15) by a factor of 0.8, from a search of
decay factors from {0.8, 0.95}. The Anderson acceleration parameter was set to 5 from a search on
{3, 5, 8, 10, 15}.

B.2 Soft Error Details

Regularization (cid:15) = 0.01 was used for the soft-error task. The soft 0/1 error objective described in
Cuturi et al. (2019) was used, with a neural network classiﬁer consisting of two CNN blocks with 32
and 64 features respectively, and a hidden layer of hidden size 512. Each CNN block consists of two
CNN layers with 3 × 3 kernel, relu activations between CNN layers and a max pooling layer at the
end of each block. Implementation including neural network architecture was taken from Cuturi et al.
(2022)2. Our proposed method was compared to other acceleration baselines using the same grid of
hyperparameters as described in Appendix B.1. Batch size was set to 64 and learning rate 0.001.

B.3 Differentiable Clustering Details

The experiment was repeated for (cid:15) = 0.1 and (cid:15) = 0.01 and again compared to other acceleration
baselines using the same grid of hyperparameters as described in Appendix B.1. Batch size was set
to 256 and learning rate 0.001.

Latent dimension was set to dz = 32 and MNIST (Deng, 2012) images are of size dx = 28 × 28.The
decoder Dθ : Rdx → R2×dz consists of 4 hidden [512, 512, 256, 256] followed by a ﬁnal linear layer
converting the outputted embedding to a vector of dimension 784. The encoder Eθ : Rdx → R2×dz
consists of 4 hidden layers of depths [512, 512, 256, 256] with relu activations, the ﬁnal embeddings
is mapped to mi ∈ Rdz and logvari ∈ Rdz by two separate linear layers without activations, where
σi = exp (0.5 × logvari). For batch (xi)i, the standard VAE loss (cid:96)ae(θ) = (cid:80)
2 −
0.5 (cid:80)
i − σ2
i ). Recall ˜xi = Dθ(zi) and zi = mi + σiui, ui ∼ N (0dz , Idz ).
As discussed in Genevay et al. (2019), clusters may be used as an
unsupervised classiﬁer and accuracy is reported in Table 4, illustrat-
ing that the clusters are meaningful. In addition, samples from the
clustered latent space may be used to generate new samples as a
form of conditional generation, again shown in Figure 7.

i(1 + 2 ∗ log(σi) − m2

i ||xi − ˜xi||2

Accuracy for each cluster is deﬁned as in Genevay et al. (2019),
as follows. Accuracy for label l in cluster k is by accl,k =
(cid:80)
where ˜yi = arg mink ||zi − µφ,k||2
2 and yi is the
Figure 7: Generated Samples
true label of xi. We write the top label accuracy for each cluster k
as maxl accl,k. When using 10 clusters for 10 labels for MNIST, each cluster’s top label accuracy

i Iyi==l, ˜yi==k
(cid:80)
i Iyi==k

2https://github.com/ott-jax/ott/tree/main/ott/examples/soft_error

13

corresponds to a different label, one cluster for each digit. Table 4 shows that the clusters manage to
capture geometrically meaningful information corresponding to each label.

Table 4: Evaluation Accuracy of trained clustered VAE for MNIST
4
0.80

6
0.68

5
0.61

2
0.42

0
0.91

1
0.66

7
0.64

3
0.56

8
0.90

Digit
Accuracy

9
0.78

C Overhead Analysis

Although timings are highly dependent on hardware and implementation, we provide some experi-
mental examples running on a single V100 GPU and 4 CPUs. This shows that the time overhead
for DualSort and Gaussian initializers are inconsequential relative to speed-up in terms of both time
and iteration count for the savings in Sinkhorn iterations. The Gaussian mixture model (GMM) is
computationally more expensive than the other proposed initializers, however the table below shows
that it can also result in time savings.

C.1 Differentiable Sorting

Table 5: Average time in seconds for DualSort with 3 iterations and Sinkhorn iterations to convergence
over 200 soft sorting problems of dimension n

n

32

64

128

256

512

1024

Initializer

Initialization

Iterations

0
DualSort

0
DualSort

0
DualSort

0
DualSort

0
DualSort

0
DualSort

-
0.0012

-
0.0012

-
0.0012

-
0.0012

-
0.0012

-
0.0012

0.28
0.22

0.22
0.088

0.17
0.066

0.17
0.049

0.13
0.050

0.14
0.058

It can be seen that the DualSort initialization procedure is extremely efﬁcient and does not have
signiﬁcant impact on the total run-time. The timings above are averaged per OT problem over 200
runs with different seeds.

C.2 Gaussian and GMM

In this section we consider timings for the word embedding/ document similarity experiment.

For the GMM initializer, the pre-compute is the average time to compute each GMM (1 per document),
divided by the number of OT problems. Each GMM is reused multiple times, so the cost is split. Each
GMM was computed using scikit-learn (Pedregosa et al., 2011) on CPU, for lack of a convenient
GPU implementation. There exists open-source GPU implementations 3 of Gaussian mixture models
for diagonal component covariance matrices which are signiﬁcantly faster, and may be worth further
investigation for more efﬁcient implementation. Similarly, one may amortize inference in GMMs or
provide a warm-start from a pooled GMM to initialize ﬁtting the GMM. We use the default K-means
initializer from scikit learn. The Initialization ﬁeld reports the time to compute the approximate dual
potentials given the GMM parameters.

3https://github.com/borchero/pycave

14

For the Gaussian initializer, the mean and variance parameters are inexpensive to compute, hence were
not computed and cached but instead computed repeatedly on the ﬂy for each OT problem. Hence
the total initialization compute time is reported in the Initialization column. Further computational
savings could be made by caching the Gaussian parameters for each document. Note that the
dimension for the Gaussian OT approximation is d = 50 and given the Gaussian initialization is
negligible here, it would also be negligible for lower dimensional settings.

Table 6: Time, in seconds, per OT problem split by task, averaged over 1, 225 OT problems, from
each pair of 50 documents from the Newsgroup 20 dataset with a subset of vocabulary of size n.
Total %

Sinkhorn Iter.

Initialization

Pre-compute

Initializer

Init. + Pre-c.

Total

n

2, 000

5, 000

10, 000

0
Gaus
GMM K = 10
GMM K = 25
GMM K = 50

0
Gaus
GMM K = 10
GMM K = 25
GMM K = 50

0
Gaus
GMM K = 10
GMM K = 25
GMM K = 50

-
-
0.0027 × 2
0.0037 × 2
0.0047 × 2

-
-
0.0035 × 2
0.0070 × 2
0.012 × 2

-
-
0.0042 × 2
0.012 × 2
0.022 × 2

-
0.0021
0.013
0.015
0.017

-
0.0030
0.014
0.015
0.018

-
0.0024
0.013
0.015
0.017

0.080
0.060
0.068
0.051
0.037

0.31
0.22
0.26
0.20
0.15

1.0
0.74
0.84
0.67
0.52

0.080
0.062
0.086
0.073
0.063

0.31
0.22
0.28
0.34
0.19

1.0
0.74
0.86
0.71
0.58

0%
3.4%
21%
30%
41%

0%
0.96%
7.1%
41%
21%

0%
0.32%
2.3%
5.6%
10.3%

D Gaussian Potential

In this section we derive explicitly the Gaussian potential. The transport map T solving the
Monge problem (2) from a non-degenerate Gaussian measure µ = N (mµ, Σµ) to another Gaus-
sian ν = N (mν, Σν) can be recovered in closed-form as T (cid:63)(x) := A(x − mµ) + mν, where
A = Σ− 1
µ , see e.g. (Peyré et al., 2019, Chapter 2.6) for a discussion. Brenier’s
theorem (Brenier, 1987) states that this map is uniquely deﬁned as the gradient of a convex function
ϕ, and it can be veriﬁed that T (cid:63)(x) = ∇ϕ(x) where ϕ(x) = 1
The convex function ϕ(x) is related to dual potential f through ϕ(x) = ||x||2

2 (x − mµ)T A(x − mµ) + mT

2 Σ− 1

µ ΣνΣ

µ (Σ

µ ) 1

ν x.

1
2

1
2

2

2

2 − f (x) hence

f (cid:63)(x) =

||x||2
2

−

1
2

(x − mµ)T A(x − mµ) − mT

ν x.

15

E Convergence of Algorithm 2 and DualSort Details

E.1 Proof of Primal Dual Convergence

There are many ways to show the convergence of Algorithm 2. Firstly, Algorithm 2 is a particular
case of the primal dual algorithm (Dantzig et al., 1956), for the much simpler case whereby there are
no cycles in the network ﬂow graph and there is a known, sparse, primal solution.

Recovering optimal dual potentials corresponding to the primal solution is equivalent to ﬁnding any
vector of shortest paths f from a single node e.g. node 0, in the network to each of the other nodes,
see e.g. (Bertsimas and Tsitsiklis, 1997, Theorem 7.17) and (Ahuja et al., 1988, Chapter 9).

Algorithm 2 computes the shortest path using a particular case of a method known as label correcting
(Bertsimas and Tsitsiklis, 1997, Chapter 7). Given there are no cycles, the proposed method recovers
the shortest path by (Bertsimas and Tsitsiklis, 1997, Theorem 7.18) and hence recovers the optimal
dual potentials.

Algorithm 2 exploits the primal solution efﬁciently by correcting all nodes in the same tree, hence the
iterations are dependent on the number of trees and not necessarily the number of nodes.

E.2 DualSort Algorithm

The DUALSORT algorithm simpliﬁes Algorithm 2 and is given sequentially below in Algorithm 4.
Without loss of generality, we assume that xi is rearranged in increasing order, so that the sorting
permutation σ is the identity. Let diag denote the operator used to extract the diagonal of a matrix, so
that diag(C) ∈ Rn and one has [diag(C)]i = ci,i, and write 1 for the vector of size n with all entries
1. The inner loop can be carried out in two different ways, either using a vectorized update or looping
through coordinates one at a time. These two updates are distinct, and we do observe that cycling
through coordinates in Gauss-Seidel fashion converges faster in terms of total number of updates.
However, that perspective misses the fact that vectorized updates utilize more efﬁciently accelerators
from a runtime perspective. Additionally, these updates are equal to, in terms of complexity to the
Sinkhorn iterations, making it easier to discuss the beneﬁts of our initializers. For these reasons, we
use the vectorized=True ﬂag in our experiments.

Algorithm 4: DUALSORT Initializer
1: Input: Cost matrix C, primal solution, P, vectorized ﬂag
2: Initialize: f = 0
3: while not converged do
if vectorized then
4:
f ← minaxis=1
5:
6:
7:
8:
9:
10:
11: end while
12: Return f

(cid:0)C − D(C)1T + f1T (cid:1)

fi ← (minj ci,j − cj,j + fj)

for i ∈ {1, . . . , n} do

end for

end if

else

E.3 Number of DualSort Iterations

Figure 8 illustrates the convergence of the DualSort algorithm when compared to the true potentials
found from linear programming. Visually, from the right plot of Figure 8, the approximate dual is
close to the true dual after just one iteration. However the squared error (left plot) is still large. After
3 iterations, the error is signiﬁcantly reduced and after 10, the error is not noticeable.

Figure 9 shows how the performance of the initializer improves signiﬁcantly from 1 initialization
iteration to 3 or 10 for the CIFAR-100 soft-error classiﬁcation task. Here performance is measured
in how many additional Sinkhorn iterations are required after initialization for convergence. Note
however that empirically there is not much difference between 3 and 10, hence 3 was used in
experiments.

16

Figure 8: Single sample of size 32 from Gaussian blob dataset with 5 centers. Left: squared error
vs true potential by number of DualSort iterations. Right: Potential from linear solver vs DualSort
approximations.

Figure 9: Number of Sinkhorn iterations per training step when using soft error loss for CIFAR-100
classiﬁer. Top: threshold=0.01, bottom: threshold=0.05. Number of vectorized DualSort iterations
1,3,10 (left to right)

F Threshold Analysis

Convergence of each the Sinkhorn for each problem was determined according to a threshold
tolerance, τ , for how close the marginals from the coupling derived from potentials are to the true
marginals. For OT problem between µ = (cid:80)n
j=1 biδyj , and denote potentials
after l Sinkhorn iterations as f (l), g(l), then the corresponding coupling may be written elementwise
as p(l)

i=1 aiδxi and ν = (cid:80)n

i +g(l)
f (l)
j −ci,j
(cid:15)

i,j = exp

and the threshold condition may be written
(cid:88)
(cid:88)
|

(cid:88)
|

p(l)

p(l)

(cid:88)

i,j − ai| +

i,j − bj| < τ.

i

j

j

i

We use τ = 0.01 for speed. But also note that a higher threshold τ = 0.05 leads to faster convergence
without drop in performance, as evidenced in Figure 10 for the soft error classiﬁcation task on
CIFAR-100. Figure 9 also illustrates that the DualSort initializer appears to exhibit relatively better
performance to the zero initialization for a higher convergence threshold.

17

051015202530i0255075100125150175|f*ifi|2DualSort, Num Iter: 1DualSort, Num Iter: 3DualSort, Num Iter: 10051015202530i50250255075100125f*i vs fiDualSort, Num Iter: 1DualSort, Num Iter: 3DualSort, Num Iter: 10Linear Prog050100150200250300Epoch051015202530Iterations0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultDualSort + DefaultDualSort + adapt.=10DualSort + And.=5050100150200250300Epoch051015202530050100150200250300Epoch051015202530050100150200250300Epoch02468101214Iterations0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultDualSort + DefaultDualSort + adapt.=10DualSort + And.=5050100150200250300Epoch02468101214050100150200250300Epoch02468101214Figure 10: Evaluation accuracy through training when using soft error loss for CIFAR-100 classiﬁer.
Top: threshold=0.01, bottom: threshold=0.05. Number of vectorized DualSort iterations 1,3,10 (left
to right)

G Other Details

Societal Impact. We are not aware of any direct negative societal impacts in this work. We
acknowledge that the Sinkhorn algorithm may be used in various applications across compute vision
and tracking with negative impacts, and this work may enable further such applications.
Code. Code for initializers will be incorporated into OTT library (Cuturi et al., 2022).
Open source software and licences. Cuturi et al. (2022) has an Apache licence.

18

050100150200250300Epoch0.00.10.20.30.40.50.60.7Accuracy0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultDualSort + DefaultDualSort + adapt.=10DualSort + And.=5050100150200250300Epoch0.00.10.20.30.40.50.60.7050100150200250300Epoch0.00.10.20.30.40.50.60.7050100150200250300Epoch0.00.10.20.30.40.50.60.7Accuracy0 + ε decay=0.80 + ω mom.=1.050 + And.=50 + adapt.=100 + DefaultDualSort + DefaultDualSort + adapt.=10DualSort + And.=5050100150200250300Epoch0.00.10.20.30.40.50.60.7050100150200250300Epoch0.00.10.20.30.40.50.60.7