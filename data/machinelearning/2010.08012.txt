Neural Function Modules with Sparse Arguments:
A Dynamic Approach to
Integrating Information across Layers

0
2
0
2

t
c
O
5
1

]

G
L
.
s
c
[

1
v
2
1
0
8
0
.
0
1
0
2
:
v
i
X
r
a

Alex Lamb
Universite de Montreal

Anirudh Goyal
Universite de Montreal

Agnieszka Słowik
University of Cambridge

Michael Mozer
Google Research / University of Colorado

Philippe Beaudoin
Element AI

Yoshua Bengio
Mila

Abstract

Feed-forward neural networks consist of a se-
quence of layers, in which each layer performs
some processing on the information from the
previous layer. A downside to this approach is
that each layer (or module, as multiple mod-
ules can operate in parallel) is tasked with
processing the entire hidden state, rather than
a particular part of the state which is most
relevant for that module. Methods which only
operate on a small number of input variables
are an essential part of most programming
languages, and they allow for improved mod-
ularity and code re-usability. Our proposed
method, Neural Function Modules (NFM),
aims to introduce the same structural capabil-
ity into deep learning. Most of the work in the
context of feed-forward networks combining
top-down and bottom-up feedback is limited
to classiﬁcation problems. The key contri-
bution of our work is to combine attention,
sparsity, top-down and bottom-up feedback,
in a ﬂexible algorithm which, as we show,
improves the results in standard classiﬁca-
tion, out-of-domain generalization, generative
modeling, and learning representations in the
context of reinforcement learning.

1

Introduction

Much of the progress in deep learning architectures
has come from improving the ability of layers to
have a speciﬁc focus and specialization. One of the
central drivers of practical progress in deep learn-
ing has been ﬁnding ways to make networks deeper
and to create inductive bias towards specialization.

Indeed, the general trend in state-of-the-art tech-
niques has been from networks with just a few lay-
ers to networks with hundreds or thousands of layers,
where each layer has a much more speciﬁc role. It-
erative inference and generation [Marino et al., 2018,
Jastrzębski et al., 2017, Greﬀ et al., 2019] approaches
also share this motivation: since they only require each
pass to make a small change to an underlying represen-
tation. This has been applied in iterative inference for
generative models [Wu et al., 2019].

Perhaps the best example of this is residual networks
(ResNets) [He et al., 2016a], in which an additive skip-
connection is placed between layers. This allows a
layer to use any other layer via the linear additive skip
connections, yet this is a rigid and inﬂexible communi-
cation between layers. Despite that limitation, residual
networks have now become ubiquitous, and are now
used in almost all networks where computational re-
sources allow for a large number of layers.

Other ideas in feed-forward networks have explored
how to make individual layers more narrowly focused.
In the DenseNet [Huang et al., 2017], each layer takes
as input all previous layers within the same block, con-
catenated together. This reduces the need to store
redundant information in multiple layers, as a layer
can directly use information from any prior layer given
as input, even if it does not directly precede that layer.
Neural ODEs [Chen et al., 2018a] explored a contin-
uous variant of ResNets, in which it can be directly
shown that making each layer perform an even more
narrowly focused computation (in this sense, meaning
a smaller update) can improve accuracy.

While these techniques have greatly improved the gen-
eralization performance of deep networks, in large part
by making individual layers more specialized, we argue
that they are still limited in that they take the entire

 
 
 
 
 
 
Neural Function Modules with Sparse Arguments

= 2). A layer where NFM
Figure 1: An illustration of NFM where a network is ran over the input twice (
is applied sparsely attends over the set of previously computed layers, allowing better specialization as well as
top-down feedback.

K

current program state as input, rather than dynamically
selecting input arguments. To address this limitation,
we take inspiration from computer programs, which
are much richer and more ﬂexible than today’s deep
networks. Computer programs are typically organized
into methods, which perform a speciﬁc task, and only
operate over a speciﬁc set of inputs which are relevant
for that task. This has several advantages over writ-
ing code without these well-contained methods. One
reason is that it leads to better separation of concerns
and modularity. A method which is written once can
be reused in diﬀerent programs, even if the overall pur-
pose of the program is very diﬀerent. Another beneﬁt
to using methods is that it allows the programmer to
avoid accidentally using or changing variables unrelated
to the function. In this way a program could use a
larger number of variables and have a greater amount of
complexity, with a contained risk of introducing bugs.
Another advantage is that mistakes can be isolated
down to a speciﬁc method within the program, which
can lead to a more concentrated and eﬃcient credit
assignment.

Additionally, methods in computer programs have the
property that they are able to take arguments from
the entire program state which is in scope. Thus they
may use either recently computed variables, or vari-
ables computed much earlier in the program. This has
some connection to bottom-up and top-down feedback
signals from cognitive psychology, with recently com-
puted variables or input streams being analogous to
bottom-up signals and higher-level goals or descriptive
variables being analogous to top-down signals. This
idea has seen some work in the deep learning commu-
nity [Zamir et al., 2017], although many deep architec-

tures (such as feed-forward networks) rely exclusively
on bottom-up processing.

Our proposal is to allow modules (corresponding to
computational blocks which can be executed in paral-
lel or in sequence) in a deep network to attend over
the previously computed modules from the network
to construct their input. This proposal, which we call
Neural Function Modules (NFM), is motivated by anal-
ogy with functions in programming languages - which
typically only take a few variables as arguments, rather
than acting upon the entire global state of the pro-
gram. Likewise an NFM module selects the hidden
states from the previously computed modules as its
inputs, and may use its entire hidden state to focus
on these few inputs. The modules which come later in
the network may attend to either that module or the
modules preceding it.

Additionally, to allow the NFM to consider both top-
down and bottom-up signal specialization, we use a
multi-pass setup, in which we run the networks forward
pass multiple times, and allow the NFM to attend over
all of the previously seen passes. In a classiﬁer, the later
parts of the network from earlier passes correspond to
“top-down” feedback, since they come from a network
which has seen and has a compressed representation
of the entire example. Whereas attending to parts of
the network closer to the input correspond to “bottom-
up” feedback. In general we consider a two-pass setup,
but also perform analysis on single-pass and three-pass
setups.

We believe that the largest impact from adding this
structure to deep learning will be in tasks where the
question of when a layer is used is dynamic, which

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

can occur as a result of the data distribution shifting
during training, or the data distribution systematically
diﬀering between training and evaluation. For example,
the former occurs organically while training a GAN
generator, since the objective from the discriminator is
constantly evolving as the adversarial game progresses.

Our newly proposed NFM module captures concepts
of iterative inference, sparse dynamic arguments, and
ﬂexible computation, yet is straightforward to integrate
into a variety of existing architectures. We demonstrate
this by integrating NFM into convolutional classiﬁers,
GAN generators, and VAE encoders. In each case, the
structure of the NFM module is kept the same, but the
details of the integration diﬀer slightly.

Our method oﬀers the following contributions:

• Allowing layers to focus their entire output space
size without limiting the information accessible to
later modules.

• Making modules more specialized, by allowing
them to focus on particular relevant inputs, rather
than the entire hidden state of the network.

• Providing a mechanism to allow for dynamic com-
bination of top-down and bottom-up feedback sig-
nals.

The key contribution of our work is to combine atten-
tion, sparsity, top-down and bottom-up feedback, in a
ﬂexible algorithm which can improve the results in the
standard classiﬁcation, out-of-domain generalization,
generative modeling and reinforcement learning (as
demonstrated in Section 4 ). Most of the work in the
context of feed-forward networks combining top-down
and bottom-up feedback is limited to classiﬁcation
problems. To the best of our knowledge, no work has
combined these ingredients together in a uniﬁed ar-
chitecture, that can be used to show improvements
in various diﬀerent problems, i.e classiﬁcation, genera-
tive modelling, out of distribution generalization and
learning representations in the context of RL.

Our experiments show that these NFM modules can
dynamically select relevant modules as inputs, leading
to improved specialization of modules. As a result,
we show that this leads to improved generalization to
changing task distributions. We also demonstrate its
ﬂexibility, by showing that it improves performance in
classiﬁcation, relational reasoning, and GANs.

2 Related Work

DenseNet: These use concatenation to combine all
of the layers within a block as inputs. The largest dif-
ference between NFM and DenseNet is that NFM uses

sparse attention, whereas DenseNet uses concatenation.
Another key diﬀerence is that NFM uses attention over
many previously seen modules, even if they are of dif-
ferent sizes, whereas DenseNet only uses the handful of
previously computed layers of the same spatial dimen-
sion. Most of the papers that have used DenseNets,
have tested it on mostly computer vision problems (like
classiﬁcation), whereas NFM is a generic module that
can be used for improving systematic generalization in
relational reasoning, for classiﬁcation as well as for gen-
erative modelling. An attentive variant of DenseNets
was also proposed [Kim et al., 2018].

Conditional Computation and Modularity:
Most of the current deep learning systerms are built in
the form of one big network, consisting of a layered but
otherwise monolithic structure, which can lead to poor
adaptation and generalization [Andreas et al., 2016,
Bahdanau et al., 2018,
Santoro et al., 2017a,
Bengio et al., 2019, Goyal et al., 2019].
In order
to address this problem, there have been attempts in
modularizing the network such that a neural network
is composed dynamically from several neural modules,
where each module is meant to perform a distinct
function [Andreas et al., 2016, Shazeer et al., 2017,
Rosenbaum et al., 2017, Goyal et al., 2019].
The
motivation behind methods that use conditional
computation is
to dynamically activate only a
portion of the entire network for each example
[Bengio, 2013, Wu et al., 2018, Fernando et al., 2017,
Recently
McGill and Perona, 2017].
[Hu et al., 2018, Woo et al., 2018, Wang et al., 2018,
Chen et al., 2018b, Veit and Belongie, 2018]
have
proposed to use a learned gating mechanism (either
using reinforcement learning or evolutionary methods)
to dynamically determine when to skip in ResNet in
a context dependent manner to reduce computation
In this line of work, no multiple modules are
cost.
Instead, the whole network is
explicitly deﬁned.
dynamically conﬁgured by selectively activating model
components such as hidden units and diﬀerent layers
for each input example. Most of these works have been
applied to computer vision problems such as image
classiﬁcation or segmentation or object detection.
Our work is more related to such methods that
dynamically decide where to route information but
instead of skipping a particular layer or a unit, NFM
dynamically decide where to query information from
(i.e., which layers to attend to), using sparse attention.
Another key diﬀerence in the proposed method is
that layers can attend to both the bottom-up, as well
as top-down information which has been shown to
improve systematic generalization as also evident in
our experiments. Also, NFM is a generic module
that can be used for other problems whereas most of
these methods have been studied exclusively for image

Neural Function Modules with Sparse Arguments

Table 1: Desiderata and Related Work: showing how our motivations relate to prior work.

Desiderata
Sparse Arguments
Iterative Inference
Dynamically Skipping Layers
Combining Top-Down and Bottom-Up Feedback Deep Boltzmann Machines [Salakhutdinov and Hinton, 2009]

Related Methods
SAB [Ke et al., 2018], Sparse Transformer [Child et al., 2019]
LOGAN [Wu et al., 2019],GibbsNet [Lamb et al., 2017]
SkipNet [Wang et al., 2018]

classiﬁcation.

Transformers: The Transformer architecture uses at-
tention over positions, but only on the previous layer.
NFM attends over layers, making it complementary
with Transformers, yet the methods share a related
motivation of allowing parts of the network to dynami-
cally select their inputs using attention. Future work
can also investigate on integrating NFM module with
transformers.

Sparse Attentive
Sparse Attention in RNNs:
Backtracking (SAB) [Ke et al., 2018] used sparse at-
tention for assigning credit to a sparse subset of time
steps in the context of RNNs leading to eﬃcient credit
assignment as well as eﬃcient transfer. More re-
cently, Recurrent Independent Mechanisms (RIMs)
[Goyal et al., 2019] also use sparse attention for dy-
namically selecting a sparse subset of modules in an
input dependent manner. Both SAB and RIMs uses
sparse-attention in the context of RNNs, where the pro-
posed method uses attention to dynamically integrate
bottom-up as well as top-down information.

3 Neural Function Modules

Our goal is to introduce the idea of Neural Function
Modules (NFM) as a new way of composing layers in
deep learning. To that end, we start with desiderata
motivating our design without going into architectural
details. Next, we give a detailed algorithmic descrip-
tion using a speciﬁc set of tools (such as top-k softmax
[Ke et al., 2018] as a way of implementing sparse atten-
tion), while we note that our concept is more general
than this speciﬁc architecture. We then describe how
NFM can be integrated into various architectures.

3.1 Desiderata

We lay out desiderata motivating our design, in the
hope of deﬁning a class of architectures that share these
goals:

• Creating deep networks in which the commu-
nication between layers is dynamic and state-
dependent, allowing it to be invariant to prior
layers which are not relevant for it;

• To allow a deep neural network to selectively route
information ﬂow around some layers, to break the
bottleneck of sequential processing;

• To allow a model to dynamically combine bottom-
up and top-down information processing using
attention; and

• To introduce more ﬂexibility into deep architec-
tures, by breaking the constraint that each layer
needs to be a suitable input for the following layer.
For example, a layer which destroys ﬁne-grained
spatial information may be diﬃcult to use earlier
in the network, but could be valuable for later
processing in deeper layers.

3.2 Proposed Implementation

We describe our particular implementation of the Neu-
ral Functional Module concept which uses sparse at-
tention, although we view the basic idea as being more
general, and potentially implementable with diﬀerent
tools. In particular, we introduce a new NFM module
which is encapsulated and has its own parameters. It
stores every previously seen layer (from the forward
pass) in its internal state, and uses the current layer’s
state as a query for attending over those previously seen
layers. This is described in detail at in Algorithm 1.

Algorithm 1 Neural Functional Module (NFM)
1: Input: An input x. A number of passes K. A neural
network with N modules for all k ∈ {1...K}: f (1)
, f (2)
,
θk
θk
f (3)
θk

θk

, ...,f (N )
2: M := EmptyList
3: for k = 1 to K do
h(0) := x
4:
for i = 1 to N do
5:
˜M := EmptyList
6:
for j = 1 to i do
7:
8:
9:
10:

end for
R = Attention (i)
θk
h(i) := f (i)
θk
M.append(h(i))

11:
12:
13:
14: end for

end for

˜Mj .append(rescale(Mj), scale(h(i−1)))

(K = ˜M, V = ˜M, Q = h(i−1))

(residual = h(i−1), input = R)

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

3.2.1 Mutli-Head Attention Mechanism

We aimed to use the multi-head attention mechanisms
from Transformers with as few modiﬁcations as possible.
We use linear operations (separate per module) to
compute the key, value, and query for each module.
We then use softmax top-k attention [Ke et al., 2018,
Goyal et al., 2019], in which the attention only attends
over the elements with the top-k highest attention
scores. Then we use one linear layer, followed by an
activation, followed by a linear layer to project back to
the original size. Depending on the problem setting,
we may or may not use batch normalization.

i

We now lay out the structure of the multi-headed at-
tention mechanism which is used in Algorithm 1. The
Attention (i)
function takes a set of keys and values to
θK
be attended-over: K, V , a query q, and a residual con-
nection R. The keys and queries are given dimension
dk and the values are given dimension dv. It is speciﬁc
to the layer index i and the parameters for attention for
the layer i: (Wq, Wk, Wv, Wo1, Wo2, γ) = θ(A)
. These
W refer to weight matrices and γ is a learned scalar
which is initialized to zero at the start of training.
Additionally a k to specify the k for top-k attention
(typically k is set to a value less than ten). An activa-
tion function σ must also be selected (in our case, we
used ReLu). We place a batch normalization module
directly before applying the activation σ.
We set ˆQ = qWq, ˆK = KWk, ˆV = V Wv. Then we
)V . Afterwards the output
compute A = Softmax (
hidden state is computed as h1 = σ(AWo1 )Wo2
. Then
the ﬁnal output from the attention blocked is multiplied
by a γ scalar and added to the residual skip-connection:
h2 = R + γh1. The weighting with a γ scalar was used
in self-attention GANs [Zhang et al., 2018].

ˆQ ˆKT
dk

Default Computation Additionally, we append a
zero-vector to the beginning of the list of keys and
values for the layers to be attended over. Thus, a
querying position on some (or all) of its heads may
elect to look at these zeros rather than reading from
an input element. This is analogous to performing a
function with a number of arguments which is less than
the number of heads.

3.2.2 Rescaling Layers (Automatic Type

Conversion)

When we query from a module of spatial dimension
len(hi), we consider attending over modules which may
have either the same or diﬀerent spatial dimensions
len(hj). For simplicity, we discuss 1D layers, but the
rescaling approach naturally generalizes to states with
2D or 3D structure.

Table 2: Classiﬁcation Results (% Test Accuracy) with
NFM show consistent improvements across tasks and
architectures. All results use Input Mixup with α = 1.0
and the experimental procedure in [Verma et al., 2018]

Methods
Base Arch.
CIFAR-10
PreResNet18
CIFAR-10
PreResNet34
CIFAR-10
PreResNet50
CIFAR-100
PreResNet18
PreResNet34
CIFAR-100
Tiny-Imagenet PreResNet18
PreResNet18
Imagenet

Baseline

0.2
0.1
0.1
0.1
0.3
0.3

96.27
96.79
97.31
78.15
80.13
57.12

±
±
±
±
±
±
76.72

NFM

0.09
0.1
0.2
0.3
0.1
0.2

96.56
97.05
97.58
78.66
80.77
58.32

±
±
±
±
±
±
77.1

Table 3: Results on Atari games with NFM show im-
proved scores with NFM used in the game-image en-
coder.

Game
Ms. Pacman
Alien
Amidar
Q-bert
Crazy Climber

Architecture Baseline

NFM

ResNet18
ResNet18
ResNet18
ResNet18
ResNet18

6432
12500
3923
27433
333242

6300
14382
3948
33253
334323

3384
239
23
2302
2389

±
±
±
±
±

If len(hi) = len(hj), then the two modules are of the
same scale, and no rescaling is performed. If len(hi) >
len(hj), then hj needs to be upsampled to the size of
hi, which we do by using nearest-neighbor upsampling.
The interesting case is when len(hi) < len(hj), which is
when downsampling is required. The simplest solution
would be to use a nearest-neighbor downsampling, but
this would involve arbitrarily picking points from hj
in the downsampling, which could discard interesting
information. Instead, we found it performed slightly
better to use a SpaceToDepth operation to treat all of
the points in the local window of size len(hj )
as separate
len(hi)
positions for the attention. Thus when attending over
a higher resolution module, NFM can pick speciﬁc
positions to attend over.

3.3

Integrating NFM

We have presented an encapsulated NFM module which
is ﬂexible and could potentially be integrated into many
diﬀerent types of networks. Notably, in all of these
integrations, the internal structure of the NFM module
itself remains the same, demonstrating the simplicity
and ﬂexibility of the approach. Additionally, using
NFM introduces no additional loss terms.

4 Experiments

Our experiments have the following goals:

• Demonstrate that Neural Function Modules can

Neural Function Modules with Sparse Arguments

improve results on a wide array of challenging
benchmark tasks, with the goal of demonstrating
the practical utility and breadth of the technique.

• To show that NFM addresses the bottleneck prob-
lem in the size of the hidden layers, by achieving
drastically improved performance when the model
is made narrow, with very few hidden units per
module.

• To show that NFM improves generalization when
the train and test set diﬀer systematically, as a
result of improved specialization of the modules
over sparse functional sub-tasks.

and evaluation, and verify how well the model is able
to generalize. Our reasoning is that if the model learns
sparse functional modules for recognizing the objects,
then it should be able to handle novel numbers of
objects. To keep this task as simple as possible, we
construct synthetic 64x64 images containing multiple
MNIST digits, and we train a convnet with the output
as a multi-label binary classiﬁer for each digit.

For the integration with NFM, we used two passes
(
= 2) and use a top-k sparsity of k = 5. Thus the
K
NFM module is used 8 times with dk = 16, dv = 16,
and 4 heads. For more details and experimental setup,
see Appendix C).

See Appendix A for more details on experimental setup
and hyperparameters.

4.3 Relational Reasoning

4.1 GANs

Intuitively, generating images with structured objects
involves various sparse functional operations - for ex-
ample creating a part such that it is consistent with
another part, or generating a part with particular prop-
erties. Based on this intuition we integrated NFM
into InfoMax GAN [Kwot Sin Lee and Cheung, 2019],
and we modify only the generator of the GAN by inte-
grating NFM (the discriminator and all of the losses
are kept the same). In our integration, we use two
passes
= 2 and we place an NFM module before
each residual block. Thus a total of 8 NFM modules
are integrated. We used dk = 32, dv = 32, and 4 heads
for the attention. For both CIFAR-10 and Imagenet
our base generator architecture is a ResNet18. For
more details, see Appendix F.

K

We used the [Lee and Town, 2020] GAN code base with
default hyperparameters for all of our GAN experi-
ments. We only changed the architecture of genera-
tor to incorporate NFM. Thus it might be the case
that we could have achieved even better performance
if we re-tuned the hyperparameters speciﬁcally for
our the NFM architecture, yet in practice we found
solid improvements even without doing this. A signiﬁ-
cant improvement on GANs using NFM are shown in
Table 4 as measured by Frechet Inception Distance
(FID) [Heusel et al., 2017] and Inception Score (IS)
[Salimans et al., ]. We integrate NFM directly into
the Pytorch Mimicry codebase which contains a variety
of techniques: SNGAN, SSGAN, InfoMax-GAN, and
WGAN-GP. The NFM model outperforms all of these
strong baselines (Table 4).

4.2 Stacked MNIST Generalization

We consider a simple multi-object classiﬁcation task in
which we change the number of object between training

In relational reasoning a model is tasked with recog-
nizing properties of objects and answering questions
about their relations: for example, “is the red ball in
front of the blue ball”. This problem has a clear sparse
functional structure which requires ﬁrst recognizing the
properties of objects from images and then analyzing
speciﬁc relations, and thus we sought to investigate
if NFM could improve results in this domain. We
used the Sort-of-CLEVR [Santoro et al., 2017b] task
to evaluate NFM in the context of visual reasoning
and compositional generalization. We analyzed the
eﬀect of extending a generic convolutional baseline
(CNN_MLP) with NFM (speciﬁcally, NFM-ConvNet;
Table 6). The baseline implementations and dataset
generation follow description in the paper which in-
troduced Sort-of-CLEVR [Santoro et al., 2017b]. Simi-
larly as in Section 4.2, we use two passes (
= 2) and a
top-k sparsity of k = 5. We report mean test accuracy
and standard deviation (over three trials) in Table 6
and Table 7.

K

Images in Sort-of-CLEVR consist of 6 randomly placed
geometrical shapes of 6 possible colors and 2 possible
shapes. There are 10 relational and 10 non-relational
questions per image. Non-relational questions have
two possible answers (random guess accuracy is 50%).
This also applies to relational questions of type 1 and
type 2 (reasoning over distances between the objects).
Relational questions of type 3 (count) have 6 possible
answers. Therefore on average a random baseline for
39%. While Sort-
relational questions has accuracy of
of-CLEVR is a simple dataset in its original form, it can
be made substantially more diﬃcult by introducing a
distribution shift (Table 7). Table 7 shows the results of
omitting N color-shape combinations from the training
set and testing on the original N = 12 combinations.
We include the results of a strong baseline, Relation
Networks (RNs) [Santoro et al., 2017b]. Note that RNs
contain a module speciﬁcally tailored for answering
relational questions in this task.

≈

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

Table 4: Improved generation with GANs (no use of class labels) on CIFAR-10 and Tiny-Imagenet, outper-
forming many strong baselines on Inception Score (IS) and Frechet Inception Distance (FID). We compare our
NFM(InfoMax-GAN) against three external baselines: SNGAN [Miyato et al., 2018], SSGAN [Chen et al., 2019],
and InfoMax-GAN [Kwot Sin Lee and Cheung, 2019].

Methods
SNGAN
SSGAN
InfoMax-GAN
NFM(InfoMax-GAN)

CIFAR-10 FID

16.77
14.65
15.12
13.15

0.04
0.04
0.10
0.06

±
±
±
±

CIFAR IS
0.06
7.97
0.06
8.17
0.08
8.08
0.02
8.34

±
±
±
±

Tiny-Imagenet FID Tiny-Imagenet IS

23.04
21.79
20.68
18.23

0.06
0.09
0.02
0.08

±
±
±
±

8.97
9.11
9.04
9.12

±
±
±
±

0.12
0.12
0.10
0.09

Table 5: Recognizing images with multiple mnist digits:
training on one or three digits (top), one or ﬁve digits
(bottom), provides evidence of improved specialization
over the digit recognition sub-task (test accuracy %).

Methods
Trained (1,3) digits
One Digit
Three Digits
Two Digits
Four Digits
Five Digits
Trained (1,5) digits
One Digit
Five Digits
Two Digits
Three Digits
Four Digits

Baseline

NFM

99.27
87.83
76.7
62.57
24.27

0.05
0.02
1.34
0.04
1.02

±
±
±
±
±

99.22
68.87
74.69
57.11
75.90

0.09
4.25
2.88
3.57
2.48

±
±
±
±
±

99.23
87.78
88.36
66.12
29.48

99.16
71.76
84.01
66.58
79.04

0.03
0.46
0.85
1.86
3.12

0.04
2.79
6.06
5.24
2.37

±
±
±
±
±

±
±
±
±
±

Table 6: Test accuracy on Relational reasoning (Sort-
of-CLEVR) from images.

Task
Relational qst
Non-relational qst

CNN

NFM(CNN)

68.26
71.78

0.61
9.3

±
±

74.95

±
79.37

1.58
2

±

The accuracy of both baselines decreases towards ran-
dom performance with a distribution shift between
training and test data. Our model outperforms the
simple baseline and RNs in the non-relational set of
questions, suggesting that NFM improves the stage
of recognizing object properties even in the presence
of a distribution shift. While generalization to out-of-
distribution samples remains challenging when com-
bined with relational reasoning, NFM might alleviate
the need for additional fully connected layers such as
those used in RNs. For more details regarding the
setup we ask the reader to refer to Appendix D.

4.4 Classiﬁcation and Generalization to

Occlusions

We evaluated NFM on the widely studied classiﬁcation
benchmarks CIFAR-10, CIFAR-100, Tiny-Imagenet,

and Imagenet and found improvements for all of them,
with the goal of demonstrating the utility and versa-
tility of NFM (Table 2). We used a two-pass setup
with k = 5, and we trained both the NFM and baseline
models with Mixup [Zhang et al., 2017]. We integrated
with base architectures of PreActResNet18, PreActRes-
Net34, and PreActResNet50 [He et al., 2016b].

As an ablation study on the classiﬁer, we tried training
with NFM normally, but at test time changed the
attention values to be random (drawn from a Gaussian).
We found that this dramatically hurt results on CIFAR-
10 classiﬁcation (96.5% to 88.5% test accuracy). This
is evidence that the performance of NFM is dependent
on the attention selectively picking modules as inputs.

In addition to improving classiﬁcation results, we found
that classiﬁers trained with NFM had better robustness
to occlusions (not seen at all during training). Evidence
from neuroscience shows that feedback from frontal
(higher) brain areas to V4 (lower brain areas) is criti-
cal in interpreting occluded stimuli. [Fyall et al., 2017].
Additionally research has shown that neural nets with
top-down feedback better handle occluded and clut-
tered displays.
[Spoerer et al., 2017]. Using our nor-
mally trained NFM model on CIFAR-10 with PreAc-
tResNet18, we improve test accuracy with occlusion
boxes of size 16x16, with a single occlusion box per
image, from 82.46% (baseline) to 84.11% (NFM). Our
occlusion used the same parameters as from the Cutout
paper [DeVries and Taylor, 2017]. For more details re-
fer to Appendix B.

4.5 Atari

The Atari 2600 game environment involves learning
to play simple 2D games which often contain small
modules with clear and sparsely deﬁned behavior pat-
terns. For this reason we sought to investigate if
using an encoder with NFM modules could lead to
improved results. All compared methods used the
same ResNet backbone, input preprocessing, and an
action repeat of 4. Our NFM integration used two
passes (
= 2) and top-k sparsity of k = 4. We
chose games that require some degree of planning

K

Neural Function Modules with Sparse Arguments

Table 7: Compositional generalization (Sort-of-CLEVR) to unseen variations, suggesting better specialization
over uncovering attributes and understanding relations.

Number of hold-out combinations
N = 1: Relational qst
N = 1: Non-relational qst
N = 2: Relational qst
N = 2: Non-relational qst
N = 3: Relational qst
N = 3: Non-relational qst
Average

CNN

Relation networks NFM(CNN)

0.47

±
1.63
3.27

57.67
57
±
47
±
54.33

±
20.66

0.47
3
±
1.25

42.33

±
46.99

50.67
48
±
45
±
43.33
40.67
46

0.94

±
0.82
3.56

7.31
±
1.7
±
3.56

±
45.61

54.67
59
±
46
±
57
±
38.33
49.67

1.7
±
0.81
1.41
1.63
3.3
3.3

±
±
50.78

and exploration as opposed to purely reactive ones:
Ms. Pacman, Frostbite, Alien, Amidar, Hero, Q-bert,
Crazy Climber. We choose this set of games, as it
was previously used by [Vezhnevets et al., 2016]. We
integrate NFM into the resnet encoder of a Rainbow
IQN [Dabney et al., 2018, Hessel et al., 2018]. We use
keysize dk of 32, value size dv of 32, 4 heads, two-passes
= 2, and top-k sparsity of k = 4. Thus the NFM
K
module is added at four places in the integration. After
integrating NFM, we kept the same hyperparameters
for training the IQN model. We use exactly the same
setup as in [Lieber, 2019] for RL algorithm, as well
as other hyper-parameters not speciﬁc to the propose
architecture. We show substantially improved results
on four of these ﬁve games (see Table 3 ).

4.6 Analysis of Hyperparameters

On CIFAR-100 classiﬁcation (PreActResNet34) we
jointly varied the keysize, valsize, and number of heads
used for the NFM process (Table 8).

Table 8: Varying the key dimension, value dimension,
top-k sparsity, and number of heads used for the atten-
tion process, when running PreActResNet34 on CIFAR-
100 classiﬁcation. Note that all results outperform the
baseline accuracy of 80.13%.

Heads Top-k
2
2
2
2
2
2
4
4
4
4
4
4

3
3
3
4
4
4
3
3
3
3
4
4

dk
8
8
8
8
8
8
16
16
16
32
16
32

dv Test Accuracy (%)
16
32
64
16
32
64
16
32
64
32
32
64

80.37
80.47
80.41
80.26
80.77
80.52
80.31
80.26
80.27
80.40
80.55
80.33

K

±

±

On the relational reasoning task, we tried using
= 1, and we achieved test accuracy of
one-pass
1.17 on relational questions, which is better
73.07
than the baseline’s 68.26
0.61 accuracy, but worse
= 2 (accuracy of
than the NFM with two-passes
1.58 on relational questions). We also saw
74.95
an improvement thanks to introducing attention spar-
sity. In the reported results, both baseline CNN and
NFM(CNN) use 24 initial channels. We also looked at
the relation between the number of model parameters,
test accuracy and adding/removing NFM in an image
classiﬁcation task (Appendix B).

±

K

5 Conclusion

The central concept behind the success of deep learning
is that models should consist of multiple components
(layers), each performing speciﬁc functions. Many of
the most successful ideas in deep learning have the pur-
pose of allowing individual layers to serve a more spe-
ciﬁc and incremental role. However, we note that most
neural networks still process all of the layers in a ﬁxed
sequence, giving each layer the previous layer as input.
We have instead proposed an alternative setup, which
we called Neural Function Modules (NFM), which is
inspired by functions in programming languages, which
operate over speciﬁc arguments. Our main contribution
lies in a new algorithm design, which connects several
ideas that are important in deep learning (attention,
sparsity, specialized modules, top-down and bottom-up
feedback, long-range dependencies). The proposed im-
plementation of these ideas (Neural Function Modules)
is a generic and highly ﬂexible architecture. While it is
improved by NFM, increased standard test accuracy on
classiﬁcation tasks such as ImageNet was not the main
focus of our work. We have shown that the proposed
method substantially improves the performance across
many diﬀerent tasks (including systematic generaliza-
tion), and we have shown ways in which this opens up
new opportunities for architecture design - by removing
the constraint that each layer must serve as input for
the successive layer.

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

References

[Andreas et al., 2016] Andreas, J., Rohrbach, M., Dar-
rell, T., and Klein, D. (2016). Neural module net-
works. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
39–48.

[Antoniou et al., 2018] Antoniou, A., Słowik, A.,
Crowley, E. J., and Storkey, A. (2018). Dilated
densenets for relational reasoning. arXiv preprint
arXiv:1811.00410.

[Bahdanau et al., 2018] Bahdanau, D., Murty, S.,
Noukhovitch, M., Nguyen, T. H., de Vries, H., and
Courville, A. (2018). Systematic generalization:
what is required and can it be learned?
arXiv
preprint arXiv:1811.12889.

[Bengio, 2013] Bengio, Y. (2013). Deep learning of
representations: Looking forward. In International
Conference on Statistical Language and Speech Pro-
cessing, pages 1–37. Springer.

[Bengio et al., 2019] Bengio, Y., Deleu, T., Rahaman,
N., Ke, R., Lachapelle, S., Bilaniuk, O., Goyal, A.,
and Pal, C. (2019). A meta-transfer objective for
learning to disentangle causal mechanisms. arXiv
preprint arXiv:1901.10912.

[Chen et al., 2018a] Chen, R. T. Q., Rubanova, Y.,
Bettencourt, J., and Duvenaud, D. K. (2018a). Neu-
ral ordinary diﬀerential equations.
In Bengio, S.,
Wallach, H., Larochelle, H., Grauman, K., Cesa-
Bianchi, N., and Garnett, R., editors, Advances in
Neural Information Processing Systems 31, pages
6571–6583. Curran Associates, Inc.

[Chen et al., 2019] Chen, T., Zhai, X., Ritter, M., Lu-
cic, M., and Houlsby, N. (2019). Self-supervised gans
via auxiliary rotation loss.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 12154–12163.

[Chen et al., 2018b] Chen, Z., Li, Y., Bengio, S., and
Si, S. (2018b). Gaternet: Dynamic ﬁlter selection in
convolutional neural network via a dedicated global
gating network. arXiv preprint arXiv:1811.11205.

[Child et al., 2019] Child, R., Gray, S., Radford, A.,
and Sutskever, I. (2019). Generating long sequences
with sparse transformers.

[Dabney et al., 2018] Dabney, W., Ostrovski, G., Sil-
ver, D., and Munos, R. (2018). Implicit quantile
networks for distributional reinforcement learning.
arXiv preprint arXiv:1806.06923.

[DeVries and Taylor, 2017] DeVries, T. and Taylor,
G. W. (2017).
Improved regularization of convo-
lutional neural networks with cutout. arXiv preprint
arXiv:1708.04552.

[Fernando et al., 2017] Fernando, C., Banarse, D.,
Blundell, C., Zwols, Y., Ha, D., Rusu, A. A., Pritzel,
A., and Wierstra, D. (2017). Pathnet: Evolution
channels gradient descent in super neural networks.
arXiv preprint arXiv:1701.08734.

[Fyall et al., 2017] Fyall, A. M., El-Shamayleh, Y.,
Choi, H., Shea-Brown, E., and Pasupathy, A. (2017).
Dynamic representation of partially occluded ob-
jects in primate prefrontal and visual cortex. Elife,
6:e25784.

[Goyal et al., 2019] Goyal, A., Lamb, A., Hoﬀmann, J.,
Sodhani, S., Levine, S., Bengio, Y., and Schölkopf, B.
(2019). Recurrent independent mechanisms. arXiv
preprint arXiv:1909.10893.

[Greﬀ et al., 2019] Greﬀ, K., Kaufmann, R. L., Kabra,
R., Watters, N., Burgess, C., Zoran, D., Matthey, L.,
Botvinick, M., and Lerchner, A. (2019). Multi-object
representation learning with iterative variational in-
ference. arXiv preprint arXiv:1903.00450.

[He et al., 2016a] He, K., Zhang, X., Ren, S., and Sun,
J. (2016a). Deep residual learning for image recogni-
tion. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).

[He et al., 2016b] He, K., Zhang, X., Ren, S., and Sun,
J. (2016b). Identity mappings in deep residual net-
works. In European conference on computer vision,
pages 630–645. Springer.

[Hessel et al., 2018] Hessel, M., Modayil, J., Van Has-
selt, H., Schaul, T., Ostrovski, G., Dabney, W., Hor-
gan, D., Piot, B., Azar, M., and Silver, D. (2018).
Rainbow: Combining improvements in deep rein-
forcement learning. In Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence.

[Heusel et al., 2017] Heusel, M., Ramsauer, H., Un-
terthiner, T., Nessler, B., and Hochreiter, S. (2017).
Gans trained by a two time-scale update rule con-
verge to a local nash equilibrium. In Advances in
neural information processing systems, pages 6626–
6637.

[Hu et al., 2018] Hu, J., Shen, L., Albanie, S., Sun, G.,
and Vedaldi, A. (2018). Gather-excite: Exploiting
feature context in convolutional neural networks. In
Advances in Neural Information Processing Systems,
pages 9401–9411.

Neural Function Modules with Sparse Arguments

[Huang et al., 2017] Huang, G.,

Z., Van
Der Maaten, L., and Weinberger, K. Q. (2017).
Densely connected convolutional networks.
In
Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708.

Liu,

[Jastrzębski et al., 2017] Jastrzębski, S., Arpit, D.,
Ballas, N., Verma, V., Che, T., and Bengio, Y.
(2017). Residual connections encourage iterative
inference. arXiv preprint arXiv:1710.04773.

[Ke et al., 2018] Ke, N. R., GOYAL, A. G. A. P., Bi-
laniuk, O., Binas, J., Mozer, M. C., Pal, C., and
Bengio, Y. (2018). Sparse attentive backtracking:
Temporal credit assignment through reminding. In
Advances in neural information processing systems,
pages 7640–7651.

[Kim et al., 2018] Kim, S., Hong, J., Kang, I., and
Kwak, N. (2018). Semantic sentence matching with
densely-connected recurrent and co-attentive infor-
mation. CoRR, abs/1805.11360.

[Kwot Sin Lee and Cheung, 2019] Kwot Sin Lee, N.-
T. T. and Cheung, N.-M. (2019). Infomax-gan: Mu-
tual information maximization for improved adver-
sarial image generation.

[Lamb et al., 2017] Lamb, A. M., Hjelm, D., Ganin,
Y., Cohen, J. P., Courville, A. C., and Bengio, Y.
(2017). Gibbsnet:
Iterative adversarial inference
for deep graphical models. In Advances in Neural
Information Processing Systems, pages 5089–5098.

[Lee and Town, 2020] Lee, K. S. and Town, C. (2020).
Mimicry: Towards the reproducibility of gan re-
search.

[Lieber, 2019] Lieber, O. (2019). Rltime: A reinforce-
ment learning library for state-of-the-art q-learning.
https://github.com/opherlieber/rltime.

[Marino et al., 2018] Marino, J., Yue, Y., and Mandt,
Iterative amortized inference. arXiv

S. (2018).
preprint arXiv:1807.09356.

[McGill and Perona, 2017] McGill, M. and Perona, P.
(2017). Deciding how to decide: Dynamic routing
in artiﬁcial neural networks. In Proceedings of the
34th International Conference on Machine Learning-
Volume 70, pages 2363–2372. JMLR. org.

[Miyato et al., 2018] Miyato, T., Kataoka, T.,
Koyama, M., and Yoshida, Y. (2018). Spectral
normalization for generative adversarial networks.
arXiv preprint arXiv:1802.05957.

[Rosenbaum et al., 2017] Rosenbaum, C., Klinger, T.,
and Riemer, M. (2017). Routing networks: Adap-
tive selection of non-linear functions for multi-task
learning. arXiv preprint arXiv:1711.01239.

[Salakhutdinov and Hinton, 2009] Salakhutdinov, R.
and Hinton, G. (2009). Deep boltzmann machines. In
Artiﬁcial intelligence and statistics, pages 448–455.

[Salimans et al., ] Salimans, T., Goodfellow,

I.,
Zaremba, W., Cheung, V., Radford, A., and Chen,
X.
Improved techniques for training gans. arxiv
2016. arXiv preprint arXiv:1606.03498.

[Santoro et al., 2017a] Santoro, A., Raposo, D., Bar-
rett, D. G., Malinowski, M., Pascanu, R., Battaglia,
P., and Lillicrap, T. (2017a). A simple neural net-
work module for relational reasoning. In Advances in
neural information processing systems, pages 4967–
4976.

[Santoro et al., 2017b] Santoro, A., Raposo, D., Bar-
rett, D. G., Malinowski, M., Pascanu, R., Battaglia,
P., and Lillicrap, T. (2017b). A simple neural net-
work module for relational reasoning. In Guyon, I.,
Luxburg, U. V., Bengio, S., Wallach, H., Fergus,
R., Vishwanathan, S., and Garnett, R., editors, Ad-
vances in Neural Information Processing Systems 30,
pages 4967–4976. Curran Associates, Inc.

[Shazeer et al., 2017] Shazeer, N., Mirhoseini, A.,
Maziarz, K., Davis, A., Le, Q., Hinton, G., and
Dean, J. (2017). Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538.

[Spoerer et al., 2017] Spoerer, C. J., McClure, P., and
Kriegeskorte, N. (2017). Recurrent convolutional
neural networks: a better model of biological object
recognition. Frontiers in psychology, 8:1551.

[Veit and Belongie, 2018] Veit, A. and Belongie, S.
(2018). Convolutional networks with adaptive in-
ference graphs. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 3–18.

[Verma et al., 2018] Verma, V., Lamb, A., Beckham,
C., Najaﬁ, A., Mitliagkas, I., Courville, A., Lopez-
Paz, D., and Bengio, Y. (2018). Manifold mixup:
Better representations by interpolating hidden states.
arXiv preprint arXiv:1806.05236.

[Vezhnevets et al., 2016] Vezhnevets, A., Mnih, V.,
Osindero, S., Graves, A., Vinyals, O., Agapiou, J.,
et al. (2016). Strategic attentive writer for learning
macro-actions. In Advances in neural information
processing systems, pages 3486–3494.

[Wang et al., 2018] Wang, X., Yu, F., Dou, Z.-Y., Dar-
rell, T., and Gonzalez, J. E. (2018). Skipnet: Learn-
ing dynamic routing in convolutional networks. In
Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 409–424.

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

[Woo et al., 2018] Woo, S., Park, J., Lee, J.-Y., and
So Kweon, I. (2018). Cbam: Convolutional block
attention module. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 3–
19.

[Wu et al., 2019] Wu, Y., Donahue, J., Balduzzi, D.,
Simonyan, K., and Lillicrap, T. (2019). Logan: La-
tent optimisation for generative adversarial networks.

[Wu et al., 2018] Wu, Z., Nagarajan, T., Kumar, A.,
Rennie, S., Davis, L. S., Grauman, K., and Feris, R.
(2018). Blockdrop: Dynamic inference paths in resid-
ual networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
8817–8826.

[Zamir et al., 2017] Zamir, A. R., Wu, T.-L., Sun, L.,
Shen, W. B., Shi, B. E., Malik, J., and Savarese, S.
(2017). Feedback networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 1308–1317.

[Zhang et al., 2017] Zhang, H., Cisse, M., Dauphin,
Y., and Lopez-Paz, D. (2017). mixup: Beyond em-
pirical risk minimization. iclr 2018. arXiv preprint
arXiv:1710.09412.

[Zhang et al., 2018] Zhang, H., Goodfellow,

I.,
Metaxas, D., and Odena, A. (2018). Self-attention
generative adversarial networks.
arXiv preprint
arXiv:1805.08318.

Neural Function Modules with Sparse Arguments

A Hyperparameter Analysis

We performed some additional experiments to measure the importance of a few critical hyperparameters for NFM.

We tried replacing the normal re-scaling process (Section 3.2.2) from NFM and replaced it with a simple nearest-
neighbor re-scaling. On CIFAR-10 PreActResNet18 classiﬁcation, the average test accuracy dropped from 96.56%
to 96.47%

On the relational reasoning task, we tried using one-pass
1.17 on
relational questions, which is better than the baseline’s 68.26
0.61 accuracy, but worse than the NFM with
1.58 on relational questions). We also saw an improvement thanks to
two-passes
introducing attention sparsity. In the reported results, both baseline CNN and NFM(CNN) use 24 initial channels.

= 1, and we achieved test accuracy of 73.07

= 2 (accuracy of 74.95

±

±

±

K

K

B Classiﬁcation Task

We considered classiﬁcation on CIFAR-10, CIFAR-100, Tiny-Imagenet, and Imagenet. We consider variants on
the PreActResNet architecture [He et al., 2016b]. For CIFAR-10, CIFAR-100, Tiny-Imagenet, and Imagenet and
followed the same hyperparameters and conﬁguration as the input mixup baseline in [Verma et al., 2018]. On all
datasets except for Imagenet, we trained for 600 epochs, with a starting learning rate of 0.1, and dropped the
learning rate by 10x at 200 epochs, 400 epochs, and 500 epochs. We averaged our obtained test accuracy over the
last 10 epochs. We used input mixup with a rate of α = 1.0 [Zhang et al., 2017], except on Imagenet, where we
used alpha = 0.5. For these experiments, we used a keysize of 32 and valsize of 32, with 4 heads. We integrated
NFM modules after each residual block. Thus we used 8 NFM modules (4 in the ﬁrst pass and 4 in the second
pass). We used a top-k sparsity for the attention of k = 5.

We also investigated if a single pass of NFM improves the accuracy regardless of the number of parameters
in the model. We used the recent image classiﬁcation task from https://github.com/ElementAI/synbols.
Preliminary results are in Table 9.

Table 9: Default Synbols dataset (100k examples).
Num parameters Num inital planes NFM? Test Accuracy (%)
206k
177k
757k
698k
5M
4M

83.14
81.89
84.03
82.61
87.63
85.2

Yes
No
Yes
No
Yes
No

12
12
24
24
64
64

While more investigation is needed, in image classiﬁcation adding NFM seems to scale better (in terms of test
accuracy) than increasing the model size by adding more layers. Without NFM, the accuracy increases by 0.72%
177k = 521k additional parameters, whereas by adding NFM we get an increase of 1.25% at
at the cost of 698k
177k = 29k parameters. The eﬀect persists for 12, 24, 64 initial planes (Table 9).
the cost of adding 206k

−

−

C Stacked MNIST Generalization

For stacked mnist, we consider a base CNN with nine convolutional layers, with every other layer having a stride
of two. Each convolutional layer had a kernel size of 3. The ﬁrst convolutional layer had 32 channels, which we
doubled every time the resolution is reduced, leading the ﬁnal number of channels to be 512. Each batch contains
a certain number of digits per image, either (1 or 3) digits or (1 or 5) digits. As a result of the batches having
variable characteristics, we elected to remove the batch normalization layer when training on this task.

We trained all models with Adam with a learning rate of 0.001 for 300 epochs, and report the test accuracy from
the epoch with the highest validation accuracy.

The images in our stacked MNIST task are 64x64, and some examples with 5 mnist digits are shown in Figure 2.
Each digit is reduced to a size of 16x16 by nearest-neighbor downsampling and then pasted into a random position
within the frame.

Lamb, Goyal, Słowik, Mozer, Beaudoin, Bengio

Figure 2: Examples of the stacked mnist digit dataset with 5 digits per image.

Neural Function Modules with Sparse Arguments

D Relational Reasoning

Figure 3: A sample from the Sort-of-CLEVR dataset.

Figure 3 shows a sample (image, question) from the Sort-of-CLEVR dataset. Each image is paired with 10
relational and 10 non-relational questions. We use the exact CNN baseline architecture from [Antoniou et al., 2018]
along with the same experimental setup. We train all models for 250 epochs with the Adam optimizer with a
learning rate of 0.001.

E Atari Reinforcement Learning

We integrate NFM into the resnet encoder of a Rainbow IQN [Dabney et al., 2018, Hessel et al., 2018]. We use
keysize dk of 32, value size dv of 32, 4 heads, two-passes
= 2, and top-k sparsity of k = 4. Thus the NFM
module is added at four places in the integration. After integrating NFM, we kept the same hyperparameters for
training the IQN model. We use exactly the same setup as in [Lieber, 2019] for RL algorithm, as well as other
hyper-parameters not speciﬁc to the propose architecture.

K

F Generative Adversarial networks

We integrated NFM into an Infomax-GAN for both CIFAR-10 and Tiny-Imagenet. We elected to integrate NFM
into the generator only, since it is computationally much cheaper than using it in both the generator and the
discriminator, as multiple discriminator updates are done for each generator update. The original Infomax-GAN
32x32 generator consists of a linear layer from the original latents to a 4x4 spatial layer with 256 channels. This
is then followed by three residual blocks and then a ﬁnal convolutional layer. Our integration applies NFM after
this ﬁrst spatial layer and after the output of each residual block. Since we use two passes
= 2, the NFM
module is applied a total of 8 times (4 in each pass).

K

The only change we introduced was the integration of NFM and the two-pass generator. Aside from that, the
hyperparameters for training the GAN are unchanged from [Lee and Town, 2020].

G Computational Resources

Resources Used: It takes about 4 days to train the proposed model on Atari RL benchmark task for 50M
timesteps.

Relationalquestions:1.Whatistheshapeoftheobjectclosesttotheredobject?⇒square2.Whatistheshapeoftheobjectfurthesttotheorangeobject?⇒circle3.Howmanyobjectshavesameshapewiththeblueobject?⇒3Non-relationalquestions:1.Whatistheshapeoftheredobject?⇒Circle2.Isgreenobjectplacedontheleftsideoftheimage?⇒yes3.Isorangeobjectplacedontheupsideoftheimage?⇒no