A High-Performance Sparse Tensor Algebra Compiler in
Multi-Level IR
Luanzheng Guo
Pacific Northwest National
Laboratory
lenny.guo@pnnl.gov

Ruiqin Tian
Pacific Northwest National
Laboratory
ruiqin.tian@pnnl.gov

Jiajia Li
Pacific Northwest National
Laboratory
Jiajia.Li@pnnl.gov

1
2
0
2

b
e
F
9

]

C
D
.
s
c
[

1
v
7
8
1
5
0
.
2
0
1
2
:
v
i
X
r
a

Bin Ren
William & Mary
bren@cs.wm.edu

Gokcen Kestor
Pacific Northwest National
Laboratory, UC Merced
gokcen.kestor@pnnl.gov

ABSTRACT
Tensor algebra is widely used in many applications, such as scien-
tific computing, machine learning, and data analytics. The tensors
represented real-world data are usually large and sparse. There
are tens of storage formats designed for sparse matrices and/or
tensors and the performance of sparse tensor operations depends
on a particular architecture and/or selected sparse format, which
makes it challenging to implement and optimize every tensor op-
eration of interest and transfer the code from one architecture
to another. We propose a tensor algebra domain-specific language
(DSL) and compiler infrastructure to automatically generate kernels
for mixed sparse-dense tensor algebra operations, named COMET.
The proposed DSL provides high-level programming abstractions
that resemble the familiar Einstein notation to represent tensor
algebra operations. The compiler performs code optimizations and
transformations for efficient code generation while covering a wide
range of tensor storage formats. COMET compiler also leverages
data reordering to improve spatial or temporal locality for better
performance. Our results show that the performance of automat-
ically generated kernels outperforms the state-of-the-art sparse
tensor algebra compiler, with up to 20.92x, 6.39x, and 13.9x per-
formance improvement, for parallel SpMV, SpMM, and TTM over
TACO, respectively.

1 INTRODUCTION
Tensor algebra is at the core of numerous applications in scientific
computing, machine learning, and data analytics. Tensors are a
generalization of matrices to any number of dimensions, which
are often large and sparse. Sparse tensors are used to represent
a multifactor or multirelational dataset, and has found numerous
applications in data analysis and mining [36, 54, 64] for health
care [3, 46], natural language processing [15, 51], machine learn-
ing [43, 59], and social network analytics [77], among many others.
Developing optimized kernels for sparse tensor algebra methods
is complicated. First, sparse tensors are often stored in a compressed
form (indexed data structures) and computational kernels needs
to efficiently loop over the nonzero elements of the tensor inputs.
Second, iterating over nonzero elements highly depends on the
particular storage format employed, hence many algorithms exist
to implement the same operation, each targeting a specific for-
mat. Finally, applications may use multiple formats concurrently
throughout the computation and mix different formats in the same

operation to achieve high performance. When tensors with different
storage formats are used in the same operation, there are two op-
tions: converting one (or both) tensor(s), which is time-consuming
especially if the tensor is only used once, or developing an algo-
rithm that can efficiently iterate over both formats simultaneously,
which lacks generality and requires different implementations for
each combination of tensor formats [10, 44].

The current solutions implement ad hoc high-performance ap-
proaches for particular computer architecture and/or format. Most
of these algorithms tackle specific problems and domains and con-
veniently store sparse tensors in a format that exploits the char-
acteristics of the problem. This approach has originated tens of
different formats [10, 18, 40, 50, 62] to represent sparse tensors.
Some of them are storage-efficient for specific inputs [10, 18, 37, 67]
or evenly nonzero distributions across rows/columns [18, 49]; some
are better affiliated to specific tensor computations, e.g., sparse
matrix-vector multiplication [72, 73] versus sparse tensor-matrix
multiplication [10, 61]; others are particularly designed for different
computer architectures, such as CPUs [41, 72] versus GPUs [47, 48].
On the other hand, it is infeasible to manually write optimized code
for each tensor algebra expressions considering the all possible
combinatorial combinations of tensor operations and formats.

To solve the above challenges, we present a sparse tensor algebra
compiler, named COMET, that is agnostic to storage formats: as
opposed to a library of sparse tensor methods, where the methods
are statically defined, a compiler can automatically and dynami-
cally generate efficient tensor algebra kernel specifically optimized
mixed dense/sparse tensor expressions. COMET Domain-Specific
Language (DSL) is a highly-productive language that provides high-
level programming abstractions that resemble the familiar Einstein
notations [20] to represent tensor operations. COMET is based on
the Multi-Level Intermediate Representation (MLIR) [38] frame-
work recently introduced by Google to building reusable and exten-
sible compiler infrastructures. The key benefit of building on top of
MLIR is its built-in performance portability. In the COMET multi-
level Intermediate Representation (IR), domain-specific, application-
dependent optimizations are performed at higher levels of the IR
stack where operations resemble programming languagesâ€™ abstrac-
tions and can be optimized based on the operations semantics.
Generic, architecture-specific optimizations are, instead, performed
at lower-levels, where simpler operations are mapped to the mem-
ory hierarchy and to processorâ€™s registers.

 
 
 
 
 
 
To enable modular code generation with respect to formats and
combination of formats, we employ four storage format attributes
â€“ dense, compressed unique, compressed non-unique, and singleton â€“
which are assigned to each tensor dimension [33]. By properly com-
bining those attributes in each dimension, it is possible to express
common sparse tensor compressed formats, such as COO, CSR,
DCSR, ELLPACK, CSF and Mode-generic. COMET code generation
algorithm analyzes the dimension attributes and produces code to
efficiently iterate over the nonzero elements of the input tensors.
Since the number of storage format attributes is far lower than
all possible combinations of storage formats, the code generation
algorithm is greatly simplified and yet can support most of the
commonly used sparse tensor storage formats and arbitrary combi-
nations of those. This approach lets users not only mix and match
storage format desired for their applications but also can enable
custom formats without modifying the underlying compiler infras-
tructure. Once the loop form of a computation has been generated
at the IR, COMET either lowers the code for sequential or paral-
lel execution. In the former case, COMET produces a high-quality
LLVM IR (which we show in this work has better loop unrolling
and vectorization than an equivalent LLVM IR produced by clang);
in the latter case, instead, COMET lowers code to the async di-
alect for asynchronous task execution based on LLVM co-routines
Compared to hand-tuned libraries [12, 40, 41, 50, 62, 73] and source-
to-source compilers [31â€“33], our approach is more portable, flexible,
and adaptable, as emerging architectures and storage formats can
be added without re-engineering the computational algorithms.
Finally, COMET employs the state-of-the-art data reordering algo-
rithm [42] to increase spatial and temporal locality on a modern
processor.

We evaluated COMET with 2833 sparse matrices and six ten-
sors from the SuiteSparse Suite Matrix Collection [19], FROSTT
Tensor Collection [60] and BIGtensor [26]. Our results show that
COMET can generate efficient code for multi-threaded CPU archi-
tectures from high-level descriptions of the algorithms. Compared
to state-of-the-art high-productivity tensor algebra languages and
compiler, COMET provides on average 2.29x, up to 6.26x, perfor-
mance improvements over TACO compiler for sequential Sparse-
Matrix Dense-Matrix (SpMM). We also show that asynchronous
task execution outperforms OpenMP parallelization, especially for
small input matrices, where runtime overhead is predominant. Our
results show up to 6.39x and 13.9x speedup over TACO for SpMM
and TTM, respectively. Finally, data reordering achieves up to 3.89x
and 7.14x performance improvements for parallel SpMV and SpMM
kernels, respectively, over the original COMET.

To the best of our knowledge, COMET is the first MLIR-based
compiler that integrates generic code generation for arbitrary in-
put formats, data reordering, and automatic parallelization within
the same framework. COMET can improve end-user application
performance while supporting efficient code generation for a wider
range of formats specialized for different application and data char-
acteristics. This paper makes the following contributions:

â€¢ We introduce the COMET DSL, an intuitive yet powerful
and flexible language to implement dense and sparse tensor
algebra algorithms;

2

â€¢ We propose an MLIR-based compiler that automatically gen-
erates efficient sequential and parallel code for a tensor ex-
pression with dense and mixed operands while supporting
the important sparse tensor storage formats.

â€¢ We integrate the state-of-the-art data reordering algorithm

to enhance data locality.

â€¢ We provide an exhaustive experimental evaluation and show
that COMET generally outperforms state-of-the-art tensor
compiler for both sequential and parallel execution.

2 BACKGROUND AND MOTIVATION
There exist various compressed and uncompressed formats to store
sparse matrices and tensors in the literature, including COOrdi-
nate (COO), Compressed Sparse Row (CSR), Double Compressed
Sparse Row (DCSR), ELLPACK, Compressed Sparse Fiber (CSF),
and Mode-Generic [12, 17, 22, 29, 48]. The specific format chosen
to represent data in an application generally depends on the ex-
pected characteristics of the data itself and how these impact other
desired properties, such as performance of a computational kernel
or memory footprint (which is particularly important in the case
of very large, multi-dimensional tensors).

Each format is important for different reasons. COO [5, 58] is
commonly used to store sparse matrices and tensors, such as the
Matrix Market exchange format [2] and the FROSTT sparse ten-
sor format [60]. While COO is the most natural format, it is not
necessarily the most performant format. CSR [70] is for sparse ma-
trices, which compresses row indices as pointers to row beginning
positions to avoid duplicated storage and increase performance
for memory bandwidth-bound computation such as Sparse-Matrix
Dense-Vector (SpMV). DCSR [16] further compresses zero rows by
adding an extra pointer to nonzero rows based on the CSR format.
With an extra level of compression on rows, DCSR is more efficient
than CSR for highly sparse (hypersparse) data. The ELLPACK [29]
format is efficient for matrices that contain a bounded number
of nonzeros per row, such as matrices that represent well-formed
meshes. CSF [62] generalizes the DCSR or CSR matrix format to
high-order tensors that compresses every dimension. Mode-Generic
format [10] is a generic representation of semi-sparse tensors with
one or more dense dimensions stored as dense blocks with the
coordinates of the blocks stored in COO.

An application might need any or even several of these formats
based on its needs, which makes it important to support computa-
tion with various tensor storage formats and their combinatorial
combinations. The main challenge is that the computational kernel
needs to effectively iterate over each sparse input tensor stored in
different storage formats. This problem is especially more compli-
cated for expressions that involve multiple operands.

Because of the large number of storage formats and possible
combinations, most state-of-the-art sparse tensor libraries support
only a few sparse formats (and generally only binary operations) or
convert tensors to an internal storage format, thereby potentially
losing the performance, memory footprint, or other advantages
that a specific format may offer. A compiler, on the other hand,
can automatically generate the efficient code for specific input
formats and their combinations, increasing flexibility, adaptivity
to new formats, and portability to various hardware platforms. To

# T e n s o r D e f i n i t i o n
Tensor < d o u b l e > A ( [ a , b ] , CSR ) ;
Tensor < d o u b l e > B ( [ b , c ] , Dense ) ; # Tensor < d o u b l e > B ( [ b , c ] , { D , D } ) ;
Tensor < d o u b l e > C ( [ a , c ] , Dense ) ; # Tensor < d o u b l e > C ( [ a , c ] , { D , D } ) ;

# Tensor < d o u b l e > A ( [ a , b ] , { D , CU } ) ;

{

# I n d e x L a b e l D e f i n i t i o n
I n d e x L a b e l
I n d e x L a b e l
I n d e x L a b e l

[ a ] = [ ? ] ;
[ b ] = [ ? ] ;
[ c ] = [ 3 2 ] ;

1 d e f main ( )
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

}

# T e n s o r R e a d f i l e O p e r a t i o n
A[ a , b ] = s p a c e _ r e a d ( f i l e n a m e ) ;

# T e n s o r F i l l O p e r a t i o n
B [ b , c ] = 1 . 0 ;
C[ a , c ] = 0 . 0 ;

# T e n s o r C o n t r a c t i o n
C[ a ,

c ] = A[ a , b ]

âˆ— B [ b , c ] ;

Listing 1: An example SPACe program for Sparse Matrix-
times-Dense-Matrix operation.

achieve this goal, two important requirements need to be satisfied:
1) a unified way to represent important sparse storage formats
(Section 4) and 2) an efficient algorithm to generate specific code
for a given expression and its particular input formats (Section 6).

3 COMET OVERVIEW
COMET consists of a DSL for tensor algebra computations, a pro-
gressive lowering process to map high-level operations to low-level
architectural resources, a series of optimizations performed in the
lowering process, and various IR dialects to represent key concepts,
operations, and types at each level of the multi-level IR. This section
reviews the key characteristics of our compiler framework. COMET
is based on the MLIR framework [38], a compiler infrastructure
to build reusable and extensible compilers and IRs. MLIR supports
the compilation of high-level abstractions and domain-specific con-
structs and provides a disciplined, extensible compiler pipeline
with gradual and partial lowering. Users can build domain-specific
compilers and customized IRs (called dialect), as well as combining
existing IRs, opting into optimizations and analysis.

Our previous work focuses on dense high-dimensional tensor
contractions. The compiler reformulates tensor contractions as a
sequence of transpose and matrix-matrix multiplication operations,
then generates efficient code by several code optimizations (e.g.,
loop tiling, micro kernel). The detailed description of previous work
and its performance results for important tensor expressions from
the Northwest Chemistry framework (NWChem) [66] can be found
in [1]. This work, instead, focuses on sparse tensor algebra.

Figure 1 shows an example COMET program for an SpMM op-
eration. The IndexLabel operation defines an index label. It can
assign the size of the index with a scalar number. If the size is
unknown in static time, then use a question mark (?) (Lines 3-5).
The Tensor operation defines new tensors (Lines 8-10); the SpMM
operation is defined at Line 20. In particular, the matrix A is stored
in the CSR format while the matrix B and the result matrix C are
dense. Note that there is no specific operation for SpMM at lan-
guage level, nor the programmer needs to explicitly state the format
of each input tensor while contracting the two tensors. COMET
atomically derives the specific operation from the format of input

Figure 1: COMET execution flow and compilation pipeline

tensors and the index labels. COMET internally annotates each
tensor with storage format attributes, devises the storage formats
used in the contraction, and properly passes this information down
to the IR stack when lowering the code. COMET can generate the
appropriate code according to the input tensor storage formats
(Section 6.2).

The code generation in COMET follows a progressive lower-
ing approach where optimizations are applied at different levels.
Figure 1 shows the compilation pipeline of COMET, where our
contributions are annotated by the dashed box. Users express their
computation in a high-level tensor algebra DSL (Section 5). First,
the COMET DSL is lowered to a Sparse Tensor Algebra (TA) IR, the
first dialect in the COMET IR stack. The language operators, types,
and structures are first mapped to an abstract syntax tree and then
to the TA dialect. The TA dialect contains domain-specific concepts,
such as multi-dimensional tensors, contractions, and tensor expres-
sions. Our compiler framework applies high-level optimizations
and transformation leveraging semantics information carried from
the DSL. For example, COMET tracks the input tensorsâ€™ definitions
and annotates each tensor with storage format attributes on each
dimension, based on the index label definitions.

Next, our compiler lowers the Tensor Algebra (TA) IR code to
lower levels of the IR stack, which follows different paths depending
on the operation and input formats. Dense tensor algebra opera-
tions are lowered first to linear algebra dialect, then to Structured
Control Flow (SCF) dialect, and finally to standard dialect. Sparse
linear algebra operations are lowered to SCF dialect which is a
loop represented in the MLIR framework. At this point, COMET
employs generic optimizations during the lowering steps but also
considers additional information about the final target architecture.
For CPU execution, the code is lowered to the Low-Level Virtual
Machine (LLVM) dialect for sequential execution and async dialect
to models asynchronous execution at a higher-level and then to
proper LLVM IR for final assembly and linking.

3

AlgebraTensor Algebra DSL -Sparse/Dense TA ASTTensorAsyncStructure Control FlowLLVM IRLLVM IRStandardLinear AlgebraSequential executionTTGT, multi-operand expressions, tiling, micro kernelAsyncLLVM CoroutinesParallel executionSparseDenseSparse tensors, support for important storage formats, data reorderingFront/backend dialectOptimization dialect External representation(a) Sparse Matrix

(b) Sparse Tensor

Figure 2: Example matrix and tensor represented in different formats. Each format is a combination of the storage format
attributes.

4 TENSOR STORAGE FORMAT
As reported in Section 2, to support multiple sparse storage formats
a compiler needs a uniform way to represent each tensor in memory.
This internal storage formats need to preserve the characteristics
of the original format, e.g., data compression or performance for
specific sparse patterns, while allowing a unified algorithm to gen-
erate efficient code for each computational expression. COMET
defines a set of storage format attributes for each dimension to
represent various sparse tensor formats. Code generation is then
based on each dimensionâ€™s storage format attributes rather than
the whole format, which greatly reduces the number of formats
and combinations that a compiler needs to support. Importantly,
COMET does not convert the original data layout into a different
storage format. Instead, the storage format attributes are used to
compose meta-data information that describes the original format,
i.e., the data layout of the original format is preserved in memory
and retains the original characteristics (compression, locality, etc.).
Representing every tensor dimension separately has been shown
to be an effective way to generalize tensor storage formats and
support efficient code generation [34]. Representing each dimen-
sion independently makes it easier to manage, adapt, and con-
vert formats and to generate computational kernels uniformly.
COMET defines the following four storage format attributes bor-
rowed from [34, 40, 62]:

Dense (D). This dimension is in the â€œdenseâ€ format, i.e.,
all coordinates in this dimension will be accessed during
the computations. For this format, we only use one scalar
number stored in the pos array to represent the size of this
dimension, such as the row dimension in Figure 2a(3).
Compressed_Unique (CU). This dimension is in a â€œcom-
pressed unique" format, i.e., the coordinates of nonzero ele-
ments in this dimension are compressed, and only the unique
(no duplication) ones are stored in the array crd. It uses an-
other array pos to store the start position of each unique
coordinate, such as the row dimension Figure 2a(4), where

4

the elements 1 and 2 are in the same row, but only one row
coordinate is stored in row_crd array.
Compressed_Nonunique (CN). This dimension is in a â€œcom-
pressed non-unique" format, i.e., all the coordinates of nonzero
elements will be recorded in crd array, and every coordi-
nate in the crd array will be accessed one by one. CN then
stores the start and the length of the crd array to the pos
array, such as the row dimension Figure 2a(2), where all the
row coordinates of the nonzeros are stored in row_crd array,
row_pos only stores the start and the length of the row_crd
array.
Singleton (S). The dimension is in a â€œsingleton" format,
i.e., all the nonzero coordinates are recorded to the array
crd without any other information, such as the column di-
mension Figure 2a(2), only the column coordinates of the
nonzeros are stored in row_crd array.

Internally, each tensor dimension is described by two arrays, a
position (pos) and a coordinate (crd) array. D only uses the pos
array to store the size of the dimension; the compressed storage
format attributes CU and CN use both pos and crd arrays to store
the nonzero coordinates and their positions; S only uses the crd
array to store the nonzero coordinates in the dimension.

Furthermore, Figure 2 shows two examples that store a sparse
matrix and a sparse tensor, respectively, in three formats (COO,
CSR, and DCSR) with the representation of varied storage format
attributes combinations. By properly combining the tensor storage
format attributes, COMET can represent the important sparse stor-
age formats, including COO, CSR, DCSR, BCSR, CSB, ELLPACK,
CSF and Mode-generic, in a uniform way, while retaining each
formatâ€™s characteristics.

5 COMET LANGUAGE DEFINITION
COMET provides a high-level Tensor Algebra DSL that increases
portability and productivity by allowing scientists to reason about
their algorithms implementation in their familiar notation and

3412567103240123columnsrows(1) matrix A(2) A in COO format(3) A in CSR format(4) A in DCSR format070011344row_posrow_crd03011231234567col_crdA valrow (CN)column (S)5024457row_poscol_pos03011231234567col_crdA valrow (D)column (CU)02457col_pos03011231234567col_crdA valcolumn (CU)040134row_posrow (CU)row_crd(1) tensor B(2) B in COO format(3) B in CSF format(4) B in Mode-Generic format070000222row_posrow_crd00110111234567col_crdB valrow (CN)column (S)024row_poscol_pos01011234567col_crdB valrow (CU)column (CU)1002340B val040022row_posrow (CN)row_crd5671234columnsrowstubes0301123tube_crdtube (S)row_crd0202col_poscol_crdtube (CU)0245703011230101col_poscolumn (S)4tube_postube (D)005000067syntax. Specifically, COMET DSL allows scientists 1) to express
concepts and operations in a form that closely resembles their fa-
miliar notations and 2) to convey domain-specific information to
the compiler for better program optimization. For example, our
language represents Einstein mathematical notation and provides
users with an interface to express tensor algebra semantics. The
same COMET program can be lowered to different architectures,
and the lowering steps can follow different optimizations and low-
ering algorithms, allowing COMET to produce high-quality code
for target architectures without excessive burden on the program-
mer (see Section 6). This work extends the COMET tensor algebra
language to support sparse tensor algebra operations and syntax,
the storage formats described in the previous sections.

Furthermore, we extend COMET to support dynamic data types.
As discussed above, Figure 1 shows an example of a COMET pro-
gram. In the COMET language, a tensor object refers to a multi-
dimensional array of arithmetic values that can be accessed by
indices. Range-based index label constructs (IndexLabel) repre-
sent the range of indices expressed through a scalar, a range, or a
range with increment. Index labels can be used both for construct-
ing a tensor or for representing a tensor operation. Different from
the original COMET compiler [1], IndexLabels can now be defined
as static or dynamic. Static IndexLabels explicitly state the size of
the dimension (Line 5) while dynamic IndexLabels (Lines 3 and
4) only indicate that there exists a dimension, but the size will be
determined later on during the execution of the program. Dynamic
and static index labels differ in that dynamic index labels indicate
an unknown size through a question mark (?) operator while static
index labels explicitly state the size of the dimension through a
scalar value.

A tensor is constructed by defined static or dynamic index labels
and by declaring the sparsity of each dimension, according to the
internal storage format described in the previous section. In Figure 1
tensor A is stored in CSR format, while tensors B and C are stored
in dense format. Note that COMET provides convenient notation
to represent the most common tensor storage format, avoiding the
need to specify the storage format for each dimension, as described
in the comments at Lines 8-10. Internally, however, COMET reasons
in terms of sparsity on each dimension when generating code.

In the example COMET program in Figure 1, the tensor A, B, and
C are initialized with a tensor file by space_read(), the constant
value 1.0, and the constant value 0.0, respectively. The function
space_read() first reads a tensor from the file in COO format and
then converts it to our internal storage format (see Section 4) to
represent CSR. We implement space_read() as a runtime function,
and it can be called in the COMET program directly.

The last line in the program performs the SpMM operation. How-
ever, users need not explicitly state that the operation is an SpMM
but can simply use the common tensor contraction * operator.
COMET will infer that the operator refers to an SpMM operation
from the storage format of the input tensors, in this case, a sparse
matrix and a dense matrix, and will generate the proper code to
iterate over the specific storage format through rules generated
from the definition of storage format attributes. Also, note that
COMET employs index labels to determine the type of operation to
perform. For example, the * operator refers to a tensor contraction

Figure 3: Generated sparse tensor algebra dialect for SpMM
operation

if the contraction indices are adjacent or to element-wise opera-
tion otherwise. In Figure 1, the index label b is used as contraction
indices between A and B (adjacent or internal indices), thus the
operator * refers to a tensor contraction. Therefore, COMET can not
only support tensor contraction but are generally applicable to many
other operations as well. Conclusively, the COMET TA language
simplifies writing tensor algebra program by supporting common
programming paradigms and enables users to express high-level
concepts in their familiar notations.

6 COMPILATION PIPELINE
We introduce sparse tensor algebra dialect in MLIR to support
mix dense/sparse tensor algebra computation with a wide range
of storage formats. We use format attributes to represent each
dimension sparsity format in a uniform way in the proposed TA IR.
COMET compiler generates efficient code based on the represented
format attribute per dimension. This section describes the compiler
framework, which consists of two main parts: 1) a sparse MLIR TA
dialect to represent tensor storage formats and operations, and 2)
code generation algorithms to generate efficient serial and parallel
code starting from the proposed TA DSL.

6.1 Sparse Tensor Algebra Dialect
COMET supports a uniform tensor storage format based on the
attributes described in Section 4 and the tensor algebra operations
supported in our DSL. Figure 3 shows the generated tensor algebra
IR for the SpMM program in Listing 1. The rest of this section details
the various operation in the sparse TA dialect.

Static/Dynamic Index Labels. The sparse tensor algebra di-
alect supports two types of index label, static and dynamic. If the
dimension size of the index is known in compile-time, COMET
uses ta.ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥_ğ‘™ğ‘ğ‘ğ‘’ğ‘™_ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ to represent the index label. It has
three operands, which represent the start, end, and step value on
this index. ta.ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥_ğ‘™ğ‘ğ‘ğ‘’ğ‘™_ğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ is used to represent the in-
dex label when the dimension size is unknown in compile time.
ta.ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥_ğ‘™ğ‘ğ‘ğ‘’ğ‘™_ğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ has two operands, the start, and step

5

1  #map0 = affine_map<(d0, d1, d2) -> (d0, d1)>2  #map1 = affine_map<(d0, d1, d2) -> (d1, d2)>3  #map2 = affine_map<(d0, d1, d2) -> (d0, d2)>4  module {5     func @main() {6         %c0 = constant 0 : index7         %c1 = constant 1 : index8         %c32 = constant 32 : index9         %a = "ta.index_label_dynamic"(%c0, %c1) : (index, index) -> !ta.range10       %b = "ta.index_label_dynamic"(%c0, %c1) : (index, index) -> !ta.range11       %c = "ta.index_label_static"(%c0, %c32, %c1) : (index, index, index) -> !ta.range12       %A = "ta.tensor_decl"(%a, %b) {format = ["D", "CU"]} : (!ta.range, !ta.range) -> tensor<?x?xf64>13       %B = "ta.tensor_decl"(%b, %c) {format = ["D", "D"]} : (!ta.range, !ta.range) -> tensor<?x32xf64>14       %C = "ta.tensor_decl"(%a, %c) {format = ["D", "D"]} : (!ta.range, !ta.range) -> tensor<?x32xf64>15       %labeledA = "ta.labeled_tensor"(%A, %a, %b) : (tensor<?x?xf64>, !ta.range, !ta.range) -> tensor<?x?xf64>16       %read_data = "ta.generic_call"() {callee = @space_read,  filename = "dataset.mtx"} : () -> tensor<*xf64>17       %setop = "ta.set_op"(%labeledA, %read_data) {__beta__= 0.000000e+00 : f64} :  (tensor<?x?xf64>, tensor<*xf64>) -> tensor<?x?xf64>18       "ta.fill"(%B) {value = 1.0 : f64} : (tensor<?x32xf64>) -> ()19       "ta.fill"(%C) {value = 0.0 : f64} : (tensor<?x32xf64>) -> ()20       "ta.tc"(%A, %B, %C) {alpha = 1..000000e+00 : f64, beta = 0..000000e+00 : f64,                       format = [["D", "CU"], ["D", "D"], ["D", "D"]],  indexing_maps = [#map0, #map1, #map2]} :                       (tensor<?x?xf64>, tensor<?x32xf64>, tensor<?x32xf64>) -> ()21       "ta.return"() : () -> ()Figure 4: Sparse tensor data structure construction opera-
tion

value on this index. The end value on this index will be known in
runtime.

Sparse Tensor Declaration. In sparse tensor algebra dialect,
the tensor is declared with ta.ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ _ğ‘‘ğ‘’ğ‘ğ‘™ operation. The operands
of ta.ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ _ğ‘‘ğ‘’ğ‘ğ‘™ are the index labels of the tensor. It can contain
an arbitrary number of operands, which means it can declare arbi-
trary dimensional tensor. ta.ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ _ğ‘‘ğ‘’ğ‘ğ‘™ operation also contains
storage format attributes of the tensor in each dimension for sparse
tensors.

Sparse Tensor Operations. The sparse TA dialect also defines
the tensor algebra operations supported by COMET. For exam-
ple, the tensor contraction ta.tc operation for an SpMM com-
putation (shown in line 20 of Figure 3) takes two input tensors
and computes the result of the contraction. The first and second
operands (%A and %B) are input tensors, and the third operand (%C)
is the output tensor. â€œta.sptensor<tensor<?Ã—i32>, tensor<?Ã—i32>,
tensor<?Ã—i32>, tensor<?Ã—i32>, tensor<?Ã—f64>>â€ is the data type
for %A, while â€œtensor<?Ã—32Ã—f64>â€ is the data type for %B, and
â€œtensor<?Ã—32Ã—f64>â€ is the data type for %C. â€œ-> ()â€ represents the
return type which is void.

We introduce formats attribute to extend the original ta.tc to
provide the storage format information of each input tensor. In line
20 of Figure 3, the first tensor is in CSR format, while the second and
third are all Dense tensors. The code in the figure shows that each
input tensors is associated with its storage format information. We
also introduce indexing_maps to ta.tc to represent the indices
of each tensor. The indexing_maps helps propagate indices infor-
mation along with the lowering stack. The tensor expression and
the storage format information will be further propagated down
to the lower level of the IR to provide the format attribute in each
dimension when generating the computational code.

Sparse Tensor Data Type. As described in Section 4, a tensor
ğ‘‡ consists of ğ‘˜ dimensions ğ‘‘ğ‘– for 0 â‰¤ ğ‘– â‰¤ ğ‘˜ âˆ’ 1, where every
dimension ğ‘‘ğ‘– is associated with a uniform storage attribute ğ‘ğ‘– âˆˆ
{D, CU, CN, S}. COMET associates two arrays crd and pos to each
dimension to describe the storage format (meta-data). In the TA
dialect, we define a sparse tensor as a struct data structure, which
contains the nonzero indices in each dimension and their values.
Figure 4 shows how a 2D sparse matrix is represented in our
TA dialect. In Figure 4, ta.sptensor_construct is the function
to construct the sparse tensor struct, which is implemented as an
operation in the TA dialect. The sptensor_construct operation
takes the pos and crd arrays in each dimension (%A1pos, %A1crd,
%A2pos, %A2crd) and the nonzero values (%AVal) as input, and
returns a ta.sptensor type data structure that represents a sparse
tensor in the TA dialect. The tensor types within ta.sptensor
represent the pos and crd arrays corresponding to each dimension

6

of the tensor itself (see Section 4). In the ta.sptensor structure,
the type of %A1pos, %A1crd, %A2pos, %A2crd are tensor<?Ã—i32>,
the type of %Aval is tensor<?Ã—f64>.

6.2 Sparse Code Generation Algorithm
COMET lowers the code from high-level COMET DSL language to
low-level machine code in multiple lowering steps.

DSL Lowering. The first step in our compilation pipeline con-
sists of lowering the high-level COMET DSL into the sparse TA
dialect. Figure 3 shows the TA dialect corresponding to the COMET
code presented in Figure 1. In Figure 3, "ta." represents the ten-
sor algebra dialect. The indexLabel operation in COMET DSL
will be lowered either into a ta.index_label_static operation
or a ta.index_label_dynamic operation (e.g., Lines 9-11 in Fig-
ure 3) based on whether the size of the dimension represented
by the index label is known or unknown at compile time. The
ta.index_Label operation has three parameters (%A, %a, and %b),
which are the start, the end, and the iteration step values in the
dimension represented by the index label. The IndexLabel at Lines
3-4 of Figure 1 has an unknown size, so it will be lowered into
the ta.index_label_dynamic operation, which only contains the
start value of the dimension. The dimension size will be inferred
during the runtime.

Progressive Lowering. Next, the sparse TA dialect is further
translated to lower MLIR dialects. We describe this lowering process
in two parts, early lowering and late lowering .

First, in the early lowering step, COMET lowers all the operations
in the sparse TA dialect, except the ta.tc operation. In particular,
the ta.tensor_decl operation, which declares a tensor, is lowered
into alloc and tensor_load operations, which are standard dialect
operations in std dialect for dense. For sparse, ta.tensor_decl
operations are lowered into more, a composition of alloc and
tensor_load operations for pos and crd arrays to store the co-
ordinates of nonzeros in each dimension, and val array to store
nonzero values. These coordinates of nonzeros are later used by
ta.sptensor_construct operation (Figure 4) to construct a sparse
tensor. To fill the pos, crd, and val arrays, the ta.generic_call
operation is invocated to to call the space_read() function. The
ta.generic_call operation is then lowered to the call operations
in the MLIR std dialect. The ta.fill operation initializes dense ten-
sors with identical values. The ta.fill operation will be lowered
into the fill operation in the MLIR ğ‘™ğ‘–ğ‘›ğ‘ğ‘™ğ‘” dialect. The ta.return
operation returns the function, and is lowered into return op-
eration in the MLIR ğ‘ ğ‘¡ğ‘‘ dialect. The ta.index_label_dynamic
operations is lowered into the ta.index_label_static operation
when the index label is identified from the input file.

Second, in the late lowering step, ta.tc operations are lowered
into the MLIR scf (structure control flow) dialect operations. Fig-
ure 6 describes the lowering algorithm to ta.tc with an example
mix sparse dense tensor contraction operation, where a sparse ten-
sor ğ´ times a dense tensor ğµ, and the output can be either sparse
or dense. The algorithm takes ta.tc as input, and automatically
generates the computational kernel code of a combination of scf
and std dialects. ta.tc is the sparse tensor algebra dialect of the
tensor contraction operation presented at Line 20 in Figure 1. As

 %A = â€œta.sptensor_constructâ€(%A1pos, %A1crd, %A2pos, %A2crd, %Aval) :            (tensor<?xi32>, tensor<?xi32>, tensor<?xi32>, tensor<?xi32>,             tensor<?xf64>)  ->              (!ta.sptensor<tensor<?xi32>, tensor<?xi32>, tensor<?xi32>,               tensor<?xi32>, tensor<?xf64>>)Table 1: Generated code to access nonzeros coordinates

Attr

Corresponding code

D
CU

CN
S

for i from 0 to pos[0] { ... }
for i from pos[m] to pos[m+1]{ idx = crd[i];}
(m: The argument of the upper level loop. m is 0 when the dimension is the
first dimension of the tensor)
for i from pos[0] to pos[1]{idx = crd[i];}
idx = crd[m];

shown at Line 20 in Figure 3 ta.tc operation is lowered based on
the code generation algorithm in Figure 6.

Figure 6 shows COMETâ€™s code generation algorithm that con-
sists of three key steps. This algorithm is general, applicable to
varied tensor algebra operations, and can generate arbitrary index
permutations. Moreover, in contrast to TACO, COMET can gen-
erate sparse output. Take tensor expression ğ¶ğ‘–ğ‘˜ = ğ´ğ‘– ğ‘— âˆ— ğµ ğ‘—ğ‘˜ as an
example, and assume the format of ğ´ is [D, CU], ğµ is [D, D] and ğ¶
is [D, D], respectively. The basic idea of this code generation is as
follows:

Step-I (Line 1 to Line 3) collects both index information as well
as the format attribute of each index. The above sample tensor
expression has three indices (ğ‘ğ‘™ğ‘™-ğ¼ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  = {ğ‘–, ğ‘—, ğ‘˜ }). The order
of these indices matters, and is decided by tensor access orders.
The format attribute of each index is decided by the usage of this
index. If this index appears in dense input tensors only, its format
attribute is D; otherwise, the format attribute is decided by the
corresponding dimension of the sparse tensor. For the above sample
tensor expression, the format attribute of index ğ‘– is D and ğ‘— is CU
(both decided by sparse input tensor ğ´), and ğ‘˜ is D (decided by dense
input tensor ğµ), respectively. After collecting this information, this
algorithm defines three index variables (ğ‘£ğ¼ğ‘‘ğ‘¥ğ´, ğ‘£ğ¼ğ‘‘ğ‘¥ğµ and ğ‘£ğ¼ğ‘‘ğ‘¥ğ¶ )
to access the value array of tensor ğ´, ğµ and ğ¶, respectively (Line 3).
Step-II (Line 4 to Line 19) iterates each index to generate loop
structure code (as the algorithm line starting with "emit" shows).
It leverages the aforementioned definition of each storage format
attribute to find nonzero coordinates in each dimension via pos
and crd arrays (e.g., ğ‘‘_ğ‘ğ‘œğ‘  and ğ‘‘_ğ‘ğ‘Ÿğ‘‘ in the algorithm). Table 1
shows the sample loop code in C language for each format attribute.
Besides generating loop structure code for each index, this step also
updates three index variables (ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ , ğ‘‡ âˆˆ {ğ´, ğµ, ğ¶}) that will be
used for inner-most computation. If the format attribute of an index
(e.g., ğ‘‘) is ğ·, i.e., ğ‘‘ only appears in dense tensors, then ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ =
ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ Ã—ğ‘‘_ğ‘†ğ¼ğ‘ ğ¸+ğ‘ğ‘Ÿğ‘”, whereğ‘‡ denotes all dense tensors that contain
index ğ‘‘, ğ‘ğ‘Ÿğ‘” is the coordinate on index ğ‘‘ (i.e., the argument of the
generated loop for index ğ‘‘), and ğ‘‘_ğ‘†ğ¼ğ‘ ğ¸ is index ğ‘‘â€™s dimension
size. If the format attribute of index ğ‘‘ is sparse (e.g., ğ¶ğ‘ˆ ), this step
handles sparse tensors and dense tensors separately. For sparse
tensors ğ‘‡ that contain index ğ‘‘, ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ = ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ + ğ‘ğ‘Ÿğ‘”, where ğ‘ğ‘Ÿğ‘”
is still the argument of the generated loop for index ğ‘‘. For dense
tensors ğ‘‡ that contain index ğ‘‘, ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ = ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ + ğ‘‘_ğ‘ğ‘Ÿğ‘‘ [ğ‘ğ‘Ÿğ‘”], where
ğ‘‘_ğ‘ğ‘Ÿğ‘‘ is the crd array of index ğ‘‘, and ğ‘‘_ğ‘ğ‘Ÿğ‘‘ [ğ‘ğ‘Ÿğ‘”] is the coordinate.
Step-III (Line 20) generates inner-most computation code to
load values from ğ´[ğ‘£ğ¼ğ‘‘ğ‘¥ğ´] and ğµ [ğ‘£ğ¼ğ‘‘ğ‘¥ğµ], compute their product,
and update ğ¶ [ğ‘£ğ¼ğ‘‘ğ‘¥ğ¶ ], after step-II generates ğ‘£ğ¼ğ‘‘ğ‘¥ğ‘‡ for tensor ğ‘‡
(ğ‘‡ âˆˆ {ğ´, ğµ, ğ¶}).

Figure 5: Lowered scf dialect code example for SpMM in the
CSR format. The right side numbers represent line numbers
in Algorithm 6

Figure 6: Sparse code generation algorithm

6.3 Parallel Code Generation
For sequential execution COMET lowers the scf dialect to the llvm
IR dialect and then to proper LLVM IR for assembly and linking.
For parallel execution, instead, the scf dialect is lowered to the
async dialect (See Figure 1). In details, we developed a pass to
lower scf.for loops to scf.parallel loops and the latter to the
async dialect. The async dialect encapsulates the semantics of an
asynchronous task-based parallel runtime in which computational
tasks are spawn and asynchronously executed by parallel worker
threads. Currently, MLIR supports a task continuation stealing ap-
proach (like Cilk [14]) in which the control is returned to the parent
task after spawning. The dialect provides semantics primitives to
synchronize the execution of tasks. COMET lowers those asyn-
chronous tasks execution primitives to LLVM co-routines in LLVM
IR, which is then passed to the assembler and linker to create a

7

1   %A1SIZE_i32 = load %A1pos[%c0] : memref<?xi32> 2   %A1SIZE = index_cast %A1SIZE_i32 : i32 to index  3   scf.for %i = %c0 to %A1SIZE step %c1 { 4       %next_i = addi %i, %c1 : index   5       %A2pos_start_i32 = load %A2pos[%i] : memref<?xi32>6       %A2pos_start = index_cast %A2pos_start_i32 : i32 to index7       %A2pos_end_i32 = load %A2pos[%next_i] : memref<?xi32>8       %A2pos_end = index_cast %A2pos_end_i32 : i32 to index9       scf.for %arg1 =   %A2pos_start to %A2pos_end step %c1 {10         %j_i32 = load %A2crd[%arg1] : memref<?xi32> 11         %j = index_cast %j_i32 : i32 to index 12         scf.for %k = %c0 to %c32 step %c1 { 13              %Avalue = load %Aval[%arg1] : memref<?xf64> 14              %Bvalue = load %B[%j, %k] : memref<?x32xf64>15              %product = mulf %Avalue, %Bvalue : f6416              %Cvalue_old = load %C[%i, %k] : memref<?x32xf64>17              %Cvalue = addf %Cvalue_old, %product : f6418              store %Cvalue, %C[%i, %k] : memref<?x32xf64>19   }}}6-78-96-720# TensorExpr e.g. Cik=Aij*Bjk; Format e.g.A[D, CU], B[D, D], C[D, D] CodeGen(TensorExpr, Format)      :   1.      Collect all indices from TensorExpr into ğ‘ğ‘™ğ‘™-ğ¼ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  2     Extract format attr for each index, then put them into formats        # Define variables to store coordinates in the value array of each tensor3.    Value-Indices vğ¼ğ‘‘ğ‘¥ğ´, vğ¼ğ‘‘ğ‘¥ğµ, vğ¼ğ‘‘ğ‘¥ğ¶ = 0                # Generate for loops based on the format4.    for ğ‘‘ in ğ‘ğ‘™ğ‘™ğ¼ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  do5.        switch((d))6.             case  ğ·:  emit-for(ğ‘ğ‘Ÿğ‘” = 0 to ğ‘‘_ğ‘ğ‘œğ‘ [0])7.                            vğ¼ğ‘‘ğ‘¥T = vğ¼ğ‘‘ğ‘¥T * d_SIZE + ğ‘ğ‘Ÿğ‘” # T âˆŠ {all tensors that contain index d}                # m is 0 when d is the first index in the input tensor;                # Otherwise, m is the argument of the upper-level loop8.            case ğ¶ğ‘ˆ: emit-for(ğ‘ğ‘Ÿğ‘” = ğ‘‘_ğ‘ğ‘œğ‘ [ğ‘š] to ğ‘‘_ğ‘ğ‘œğ‘ [ğ‘š + 1])9.                           emit-load(ğ‘‘_ğ‘ğ‘Ÿğ‘‘, ğ‘ğ‘Ÿğ‘”)10                          vğ¼ğ‘‘ğ‘¥T += ğ‘ğ‘Ÿğ‘”             # T âˆŠ {all sparse tensors that contain index d}11.                         vğ¼ğ‘‘ğ‘¥T = vğ¼ğ‘‘ğ‘¥T * d_SIZE + d_crd[ğ‘ğ‘Ÿğ‘”]  # T âˆŠ {all dense tensors that contain index d}12.          case  CN: emit-for(ğ‘ğ‘Ÿğ‘” = ğ‘‘_ğ‘ğ‘œğ‘ [0] to ğ‘‘_ğ‘ğ‘œğ‘ [1])13.                          emit-load(ğ‘‘_ğ‘ğ‘Ÿğ‘‘, ğ‘ğ‘Ÿğ‘”)14                           vğ¼ğ‘‘ğ‘¥T += ğ‘ğ‘Ÿğ‘”  # T âˆŠ {all sparse tensors that contain index d}15                           vğ¼ğ‘‘ğ‘¥T = vğ¼ğ‘‘ğ‘¥T * d_SIZE + d_crd[ğ‘ğ‘Ÿğ‘”]  # T âˆŠ {all dense tensors that contain index d}16.          case ğ‘†: ğ‘ğ‘Ÿğ‘” = argument of upper-level loop17.                      emit-load(ğ‘‘_ğ‘ğ‘Ÿğ‘‘, ğ‘ğ‘Ÿğ‘”) 18.                      vğ¼ğ‘‘ğ‘¥T += 0 # T âˆŠ {all sparse tensors that contain index d}19.                      vğ¼ğ‘‘ğ‘¥T = vğ¼ğ‘‘ğ‘¥T * d_SIZE + d_crd[ğ‘ğ‘Ÿğ‘”] # T âˆŠ {all dense tensors that contain index d}               # Generate the loop body of the innermost loop          20    emit operations to do computation for C[vğ¼ğ‘‘ğ‘¥C] += A[vğ¼ğ‘‘ğ‘¥A] *  B[vğ¼ğ‘‘ğ‘¥B]         # Generate load, mul, store operationsbinary. As Figure 7d shows, the MLIR asynchronous runtime intro-
duces relatively low overhead during execution, which improves
performance, especially for small computations.

7 DATA REORDERING
The distribution of the nonzero entries in sparse matrices/tensors
can significantly affect the performance of sparse matrix/tensor
algebra computations. Reordering [27, 42] is the de facto technique
to optimize the memory access pattern caused by uneven data distri-
bution. Different from existing compiler frameworks [30, 32] which
apply reordering to iterations, we apply reordering to matrices and
tensors to optimize their memory access patterns.

We borrow from the reordering algorithm presented in [42]
(LexiOrder), extended it to support sparse matrices, and imple-
mented it in the COMET runtime (tensor_reorder()). The LexiOrder
algorithm is built on top of the doubly lexical ordering algorithm [45,
52] with some optimization techniques to advance its overall effi-
ciency and availability on some concern cases. The basic idea of the
LexiOrder algorithm is to sort a specific dimension (either rows
or columns for matrices) in an iteration using the doubly lexical
ordering algorithm and sort all dimensions in turn across iterations.
The algorithmâ€™s objective is to cluster all nonzero entries around
the diagonal to increase spatial and temporal locality.

8 EVALUATION
In this section we evaluate COMET against state-of-the-art high-
level compiler frameworks and DSL for dense and sparse tensor alge-
bra. Specifically, we compare our results against TACO [34], a tensor
algebra compiler that performs automatic source-to-source trans-
formation from TACO DSL to sequential C++, Parallel OpenMP,
and data-parallel CUDA. For brevity, we evaluated the performance
of selected benchmarks with a single storage format â€“ matrices
(CSR) and tensors (CSF), though our compiler can operate on other
formats as well. All results reported are the average of 25 runs.

8.1 Experimentation Setup
We performed our experiments on a compute node equipped with
two Intel Xeon Gold 6126 sockets running at 2.60GHz. Each CPU
socket consists of 12 processing core (for a total of 24 cores). The
system features 192 GB of DRAM memory. We compiled COMET,
TACO, and all the benchmarks with âˆ’O3 and clang 12.0 and use
the most recent MLIR version at the time of writing this manuscript.
We use as input datasets 2833 matrices and six tensors of different
sizes and shapes chosen from the SuiteSparse Matrix Collection [19],
the FROSTT Tensor Collection [60], and BIGtensor [26]. The SuiteS-
parse Matrix Collection is a growing dataset of sparse matrices in
real-world applications. The dataset is widely used in the numerical
linear algebra community for performance evaluation. The FROSTT
Tensor Collection is a composition of open-source sparse tensor
datasets from various data sources that are difficult to collect. The
BIGtensor dataset is a tensor database that contains large-scale
tensors for large-scale tensor analysis. Our input datasets represent
the most important HPC domains in scientific computing, including
chemistry, structural engineering, various linear solvers, computer
graphics and vision, and molecular dynamics. We provide the de-
scription of the six tensors in Table 2.

Name

NELL-1

NELL-2

delicious-
3d

flickr-3d

vast-2015-
mc1-3d
Freebase-
music[26]

Size

Nonzeros

Domain

x
x

x
x

2,902,330
2,143,368
25,495,389
12,092 x 9184 x
28,818
532,924
17,262,471
2,480,308
319,686
28,153,045
1,607,191
165,427
11,374 x 2
23,344,784
x
223,344,784 x
166

x
x

x

143599552

Natural Language Processing

76879419

Natural Language Processing

140,126,181

Tags from Delicious website

112,890,310

Tages from Flickr website

26,021,854

Theme park attend event

99,546,551

Entries related with music in
Freebase

Table 2: Description of sparse tensors

8.2 Sparse Tensor Operations
We define the sparse tensor operations considered in COMET below.
SpMV. The Sparse Matrix-times-Vector (SpMV or SpMSpV), y =
X Ã— v, is the multiplication of a sparse matrix X âˆˆ Rğ¼1Ã—ğ¼2 with a
dense vector v âˆˆ Rğ¼2 . ğ‘¦ğ‘–1 = (cid:205)ğ¼2

ğ‘–2=1 ğ‘¥ğ‘–1ğ‘–2ğ‘£ğ‘–2 .
SpMM. The Sparse Matrix-times-Matrix (SpMM or SpGEMM),
Y = X Ã— U, is the multiplication of a sparse matrix X âˆˆ Rğ¼1Ã—ğ¼2 with
a dense matrix U âˆˆ Rğ¼2Ã—ğ‘…. ğ‘¦ğ‘–1ğ‘Ÿ = (cid:205)ğ¼2

ğ‘–2=1 ğ‘¥ğ‘–1ğ‘–2ğ‘¢ğ‘–2ğ‘Ÿ .

SpTTV. The Sparse Tensor-Times-Vector (SpTTV) [7] in mode ğ‘›,
Y = X Ã—ğ‘› v, is the multiplication of a sparse tensor X âˆˆ Rğ¼1Ã—ğ¼2Ã—ğ¼3
with a dense vector v âˆˆ Rğ¼ğ‘› , along mode ğ‘›. Given ğ‘› = 1, ğ‘¦ğ‘–2ğ‘–3 =
(cid:205)ğ¼1
ğ‘–1=1 ğ‘¥ğ‘–1ğ‘–2ğ‘–3ğ‘£ğ‘–ğ‘› . This results in a two-dimensional ğ¼2 Ã— ğ¼3 tensor
which has one less dimension.

SpTTM. The Sparse Tensor-Times-Matrix (SpTTM) [7, 35] in
mode ğ‘›, denoted by Y = X Ã—ğ‘› U, is the multiplication of a sparse
tensor X âˆˆ Rğ¼1Ã—ğ¼2Ã—ğ¼3 with a dense matrix U âˆˆ Rğ¼ğ‘›Ã—ğ‘…, along mode ğ‘›.
Mode-1 TTM results in a ğ‘…Ã—ğ¼2Ã—ğ¼3 tensor, and its operation is defined
as ğ‘¦ğ‘Ÿ Â·Â·Â·ğ‘–2ğ‘–3 = (cid:205)ğ¼ğ‘›
ğ‘–ğ‘›=1 ğ‘¥ğ‘–1ğ‘–2ğ‘–3ğ‘¢ğ‘–ğ‘›ğ‘Ÿ . Also, note that ğ‘… is typically much
smaller than ğ¼ğ‘› in low-rank decompositions, typically ğ‘… < 100.

SpMV and SpMM widely appear in applications from scientific
computing, such as direct or iterative solvers [39, 71], to data in-
tensive domains [76], graph analytics [41]. SpTTV and SpTTM are
computational kernels of popular tensor decompositions, such as
the Tucker decomposition [35, 59, 75] , tensor power method [4, 69],
for a variety of applications, including (social network, electrical
grid) data analytics, numerical simulation, machine learning.

8.3 Performance Evaluation
SpMV and SpMM. We measured the performance of COMET and
TACO while running SpMV and SpMM with each of the 2833 matri-
ces for sequential and parallel execution. We present the experimen-
tal results in Figure 7, where COMET and TACO are represented
in red and blue dots respectively. In the plot, the x-axis represents
a matrix (2,833 matrices, ordered by increasing number of nonze-
ros) and the y-axis execution time (lower is better). As we can see
from the plots, COMET achieves better performance than TACO
on sequential SpMM (Figure 7c and parallel SpMV (Figure 7b, and
comparable performance on sequential SpMV and parallel SpMM
(Figures 7a and 7d, respectively). For sequential execution, COMET

8

(a) sequential SpMV

(b) parallel SpMV

(c) sequential SpMM

(d) parallel SpMM

Figure 7: Performance comparison with TACO on CPU.

(a) sequential SpMV-lexi

(b) parallel SpMV-lexi

(c) sequential SpMM-lexi

(d) parallel SpMM-lexi

Figure 8: Performance of Lexi ordering

outperforms TACO by up to 6.26x for SpMM (average 2.29x) and
by up to 2.14x for SpMV (average 0.94x). A comparison of COMET
and TACO generated LLVM IR codes shows that COMET results in
more optimized code with better SIMD (or vectorization) utilization
and loop unrolling. For both SpMV and SpMM, take SpMM as an
example. The utilization of many SIMD instructions in TACO is
only half of that in COMET (e.g., TACO only uses 2 lanes while
COMET uses 4 lanes). COMET unrolls multiple loops by 8 while
TACO unrolls them by 2. Although the generated LLVM IR for
both SpMV and SpMM show similar differences, the effect of better
vectorization and loop unrolling are more evident for larger com-
putation (SpMM). These results highlight one of the major goals
of MLIR and MLIR-based compilers: by leveraging higher-level se-
mantics information and progressive lowering steps, it is possible
to produce a more aggressive and higher-quality LLVM IR that,
eventually, results in higher performance and resource utilization.
For parallel SpMV, COMET achieves an average of 20.92x speedup
over TACO. Especially for small matrices, COMET outperforms
TACO by a significant margin, however, after further inspection,
we realized that this performance difference is due to the overhead
introduced by the underlying parallel runtime. COMET uses an
asynchronous task-based programming model based on LLVM co-
routines while TACO leverages OpenMP. For small computation,
LLVM co-routines introduce less overhead than OpenMP threading
(which is beneficial for larger parallel regions). As we can see from
Figure 7d, when there is enough computation for each OpenMP
thread, the runtime overhead is amortized and both COMET and
TACO perform similarly.

Reordering. By reordering data in memory, COMET attempts
to increase spatial and temporal locality to achieve higher perfor-
mance. The plots in Figure 8 show COMET performance when
reordering data compared to original case (no reordering). Figure 8
shows that, indeed, in many cases there is significant advantage

Figure 9: Visualization comparison of matrices with and
without reordering

of reordering data, with up to 3.41x (average 1.04x), 3.89x (average
1.03x), 7.12x (average 1.12x), and 7.14x (average 1.13x) for SpMV
sequential, SpMV parallel, SpMM sequential, and SpMM parallel,
respectively. However, we also note that there might be signifi-
cant performance degradation, especially for parallel execution. We
further analyzed the reasons for this disparity and identified load
imbalance as the primary source of performance degradation. Our

9

(a) bundle_adj(b) bundle_adj after reordering(c) kron_g500-logn20(d) kron_g500-logn20 after reordering(a) sequential TTV

(b) parallel TTV

(c) sequential TTM

(d) parallel TTM

Figure 10: Performance of tensor operations

reordering algorithm attempts to cluster nonzeros on the top-left
corner of sparse matrices. In an ideal case, after reordering the
nonzeros are distributed around the matrix diagonal.

Figure 9 shows a case in which reordering results in high per-
formance improvements. In this case, the nonzero elements origi-
nally around the first column are distributed around the diagonal.
Figure 9, instead, shows a case in which reordering reduces perfor-
mance. In this case, the nonzeros are clustered around the top-left
corner, thus threads that operate on the top rows have more work
to perform compared to threads that operate on the bottom rows,
which results in load imbalance and performance degradation.

TTV and TTM. We also compare COMET with TACO on TTV
and TTM with six sparse tensors on CPU and multi-threads and
with reordering optimization on and off. Figure 10 illustrates the
experimental results. TACO does not generate parallel code if the
output tensor is stored in sparse format, even if instructed to do so,
thus the results in the Figure for parallel execution are with respect
to sequential execution of the TACO benchmarks. For sequential
TTV, COMET performs comparably to TACO. With reordering,
COMET achieves better performance on four out of six sparse
tensors. For parallel TTV, COMET performs significantly better
than TACO with up to 12.5Ã— and on average 8Ã— speedup. With
reordering, COMETâ€™s performance is degraded on five of six sparse
tensors except for delicious-3d. As for the case of SpMV and SpMM,
we observed similar load imbalance issues. For sequential TTM,
COMET performs better than TACO with up to 3.3Ã— and on average
2.53Ã— speedup. With reordering, COMET achieves better perfor-
mance on three out of six sparse tensors. For parallel TTM, COMET
performs significantly better than TACO with up to 13.9Ã— and on
average 8.13Ã— speedup. With reordering, COMETâ€™s performance is
degraded on five of six sparse tensors except for vast-2015-mc1-3d.
Our results show that reordering tensors have a significant (pos-
itive or negative) impact on performance, more than for matrices.
One possible reason is that the LexiOrder algorithm reorders all
dimensions of data simultaneously, which means the data locality
is the best when accessing all the dimensions in conjunction, as
in conjunction. The sparse tensor operation MTTKRP [42] follows
this behavior to gain a good performance speedup. However, this
does not mean that the indices in every dimension get good locality
when accessing the vector or matrix in TTV or TTM, potentially
leading to low performance. We will investigate alternative reorder-
ing algorithms and adaptive methods in future work.

10

9 RELATED WORK
Compiler for Tensor Algebra. Compiler techniques have been
used to drive irregular computation in tensor algebra [8, 25, 28,
34, 65]. TCE [25] is a compiler optimization framework that fo-
cuses on dense tensor contraction operations in quantum chemistry.
TTC [65] is a compiler framework that carries out a composition of
high-performance tensor transpose strategies for GPUs. TACO [34]
is a compiler that generates code for given tensor algebra expres-
sions and used as a higher-level domain-specific language for tensor
algebra. Kim et al. [28] use similar compiler techniques for high-
performance tensor contractions but focus on its application on
Graphics Processing Unit (GPU)s. Different from existing works, we
develop a high-performance sparse tensor algebra compiler using
MLIR, which supports both serial and parallel code generation and
enables better portability and adaptability.

Domain-specific Libraries for Tensor Algebra. There have
been a collection of tensor algebra libraries developed [21, 23, 24,
53, 55â€“57, 63, 68]. FLAME [24] is a library aiming for the derivation
and implementation of tensor algebra operations on CPUs. Later,
serial linear algebra libraries are extended to run on distributed par-
allel systems [21, 53, 56, 57]. On the other hand, these libraries are
extended to support sparse tensor algebra operations using differ-
ent sparse tensor formats [23, 55, 63]. Tensor algebra libraries favor
scientific computing and are widely utilized in scientific applica-
tion development. By contrast, COMET transparently implements
tensor algebra algorithms per se and can compile most types of
sparse tensor formats and automatically generate efficient code.

Tensor Algebra Optimization. Plenty of work [6, 9, 12, 13, 37,
40, 62, 74] leverage reordering to optimize tensor algebra with re-
spect to distinct tensor formats for different tensor operations and
heterogeneous architectures. Kjolstad et al. [31, 33] reorder loops
of tensor algebra computations to improve the data locality. Smith
et al. [62] use reordering to enable high-performance tensor fac-
torization operations. Yang et al. [74] identify an efficient memory
access pattern for high-performance SpMM operations through
merge-based load balancing and row-major coalesced memory ac-
cess. Other works, such as [11, 18, 40, 47], to name a few, design
high-performance algorithms considering computer architecture
characteristics using techniques like register blocking, cache block-
ing, and reordering. COMET.

10 CONCLUSION
In this work, we present a high-performance sparse tensor algebra
compiler, called COMET, and a high-productive DSL to support
next-generation tensor operations. Our DSL enables high-level pro-
gramming abstractions that resemble the familiar Einstein notation
to express tensor algebra operations. COMET is based on the MLIR
framework, which allows us to build portable, adaptable, and ex-
tensible compilers. COMET provides an effective and efficient code
generation which supports most tensor storage formats through
an internal storage format based on four dimension attributes and
a novel code generation algorithm. Furthermore, we incorporate a
data reordering algorithm to increase the data locality. The evalua-
tion results reveal that COMET outperforms competing for baseline
sparse tensor algebra compiler TACO with up to 20.92x, 6.39x, and
13.9x performance improvement for SpMV, SpMM, and TTM com-
putations respectively. In future work, we plan to extend COMET to
support heterogeneous architectures and to explore alternatives re-
ordering schemes that better adapt to the sparsity patterns observed
in scientific and engineering input sets.

11 ACKNOWLEDGEMENT
This research is supported by PNNL Laboratory Directed Research
and Development Program (LDRD), Data-Model Convergence Ini-
tiative, project DuoMO: A Compiler Infrastructure for Data-Model
Convergence.

REFERENCES
[1] [n.d.]. ([n. d.]).
[2] 2013. National Institute of Standards and Technology. http://math.nist.gov/

MatrixMarket/formats.html.

[3] Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and BÃ¼lent Yener.
2007. Multiway analysis of epilepsy tensors. Bioinformatics 23, 13 (2007), i10â€“i18.
[4] Animashree Anandkumar, Rong Ge, and Majid Janzamin. 2017. Analyzing tensor
power method dynamics in overcomplete regime. The Journal of Machine Learning
Research 18, 1 (2017), 752â€“791.

[5] Hartwig Anzt, Terry Cojean, Chen Yen-Chen, Jack Dongarra, Goran Flegar, Pratik
Nayak, Stanimire Tomov, Yuhsiang M Tsai, and Weichung Wang. 2020. Load-
balancing sparse matrix vector product kernels on GPUs. ACM Transactions on
Parallel Computing (TOPC) 7, 1 (2020), 1â€“26.

[6] Alexander A Auer, Gerald Baumgartner, David E Bernholdt, Alina Bibireata,
Venkatesh Choppella, Daniel Cociorva, Xiaoyang Gao, Robert Harrison, Sriram
Krishnamoorthy, Sandhya Krishnan, et al. 2006. Automatic code generation
for many-body electronic structure methods: the tensor contraction engine.
Molecular Physics (2006).

[7] Brett W. Bader and Tamara G. Kolda. 2007. Efficient MATLAB computations with
sparse and factored tensors. SIAM Journal on Scientific Computing 30, 1 (Dec.
2007), 205â€“231. https://doi.org/10.1137/060676489

[8] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Ab-
durrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, and Saman
Amarasinghe. 2019. Tiramisu: A polyhedral compiler for expressing fast and
portable code. In 2019 IEEE/ACM International Symposium on Code Generation
and Optimization (CGO). IEEE, 193â€“205.

[9] Muthu Baskaran, Benoit Meister, and Richard Lethin. 2014. Low-overhead load-
balanced scheduling for sparse tensor computations. In 2014 IEEE High Perfor-
mance Extreme Computing Conference (HPEC). IEEE.

[10] Muthu Baskaran, BenoÃ®t Meister, Nicolas Vasilache, and Richard Lethin. 2012.
Efficient and scalable computations with sparse tensors. In 2012 IEEE Conference
on High Performance Extreme Computing. IEEE, 1â€“6.

[11] Nathan Bell and Michael Garland. 2008. Efficient sparse matrix-vector multiplica-
tion on CUDA. Technical Report. Nvidia Technical Report NVR-2008-004, Nvidia
Corporation.

[12] Nathan Bell and Michael Garland. 2009.

Implementing sparse matrix-vector
multiplication on throughput-oriented processors. In Proceedings of the conference
on high performance computing networking, storage and analysis.

[13] Aart J. C. Bik and Harry A. G. Wijshoff. 1993. Compilation Techniques for Sparse
Matrix Computations (ICS â€™93). Association for Computing Machinery, New York,
NY, USA, 416â€“424.

11

[14] Robert D Blumofe, Christopher F Joerg, Bradley C Kuszmaul, Charles E Leiserson,
Keith H Randall, and Yuli Zhou. 1995. Cilk: An efficient multithreaded runtime
system. ACM SigPlan Notices 30, 8 (1995), 207â€“216.

[15] Guillaume Bouchard, Jason Naradowsky, Sebastian Riedel, Tim RocktÃ¤schel, and
Andreas Vlachos. 2015. Matrix and tensor factorization methods for natural
language processing. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing: Tutorial Abstracts. 16â€“18.

[16] Aydin Buluc and John R Gilbert. 2008. On the representation and multiplication
of hypersparse matrices. In 2008 IEEE International Symposium on Parallel and
Distributed Processing. IEEE, 1â€“11.

[17] Yuedan Chen, Guoqing Xiao, M Tamer Ã–zsu, Chubo Liu, Albert Y Zomaya,
and Tao Li. 2020. aeSpTV: An adaptive and efficient framework for sparse
tensor-vector product kernel on a high-performance computing platform. IEEE
Transactions on Parallel and Distributed Systems 31, 10 (2020), 2329â€“2345.
[18] Jee W Choi, Amik Singh, and Richard W Vuduc. 2010. Model-driven autotuning
of sparse matrix-vector multiply on GPUs. ACM sigplan notices 45, 5 (2010),
115â€“126.

[19] Timothy A Davis and Yifan Hu. 2011. The University of Florida sparse matrix

collection. ACM Transactions on Mathematical Software (TOMS) (2011).

[20] Albert Einstein. 1923. Die grundlage der allgemeinen relativitÃ¤tstheorie. In Das

RelativitÃ¤tsprinzip. Springer.

[21] Evgeny Epifanovsky, Michael Wormit, Tomasz KuÅ›, Arie Landau, Dmitry Zuev,
Kirill Khistyaev, Prashant Manohar, Ilya Kaliman, Andreas Dreuw, and Anna I
Krylov. 2013. New implementation of high-level correlated methods using a gen-
eral block tensor library for high-performance electronic structure calculations.
Journal of computational chemistry (2013).

[22] Xiaowen Feng, Hai Jin, Ran Zheng, Kan Hu, Jingxiang Zeng, and Zhiyuan Shao.
2011. Optimization of sparse matrix-vector multiplication with variant CSR
on GPUs. In 2011 IEEE 17th International Conference on Parallel and Distributed
Systems. IEEE, 165â€“172.

[23] Matthew Fishman, Steven R White, and E Miles Stoudenmire. 2020. The ITensor
Software Library for tensor network calculations. arXiv preprint arXiv:2007.14822
(2020).

[24] John A Gunnels, Fred G Gustavson, Greg M Henry, and Robert A Van De Geijn.
2001. FLAME: Formal linear algebra methods environment. ACM Transactions
on Mathematical Software (TOMS) (2001).

[25] So Hirata. 2003. Tensor contraction engine: Abstraction and automated parallel
implementation of configuration-interaction, coupled-cluster, and many-body
perturbation theories. The Journal of Physical Chemistry A (2003).

[26] Inah Jeon, Evangelos E. Papalexakis, U Kang, and Christos Faloutsos. 2015.
HaTen2: Billion-scale Tensor Decompositions. In IEEE International Conference
on Data Engineering (ICDE).

[27] Peng Jiang, Changwan Hong, and Gagan Agrawal. 2020. A novel data transfor-
mation and execution strategy for accelerating sparse matrix multiplication on
GPUs. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming. 376â€“388.

[28] Jinsung Kim, Aravind Sukumaran-Rajam, Vineeth Thumma, Sriram Krishnamoor-
thy, Ajay Panyala, Louis-NoÃ«l Pouchet, Atanas Rountev, and Ponnuswamy Sa-
dayappan. 2019. A code generator for high-performance tensor contractions
on gpus. In 2019 IEEE/ACM International Symposium on Code Generation and
Optimization (CGO). IEEE.

[29] David R Kincaid, Thomas C Oppe, and David M Young. 1989. ITPACKV 2D userâ€™s
guide. Technical Report. Texas Univ., Austin, TX (USA). Center for Numerical
Analysis.

[30] Vladimir Kiriansky, Yunming Zhang, and Saman Amarasinghe. 2016. Optimizing
indirect memory references with milk. In Proceedings of the 2016 International
Conference on Parallel Architectures and Compilation. 299â€“312.

[31] Fredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2019.
Tensor algebra compilation with workspaces. In 2019 IEEE/ACM International
Symposium on Code Generation and Optimization (CGO). IEEE, 180â€“192.

[32] Fredrik Kjolstad, Stephen Chou, David Lugato, Shoaib Kamil, and Saman Ama-
rasinghe. 2017. taco: A Tool to Generate Tensor Algebra Kernels. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
943â€“948. https://doi.org/10.1109/ASE.2017.8115709

[33] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras-
inghe. 2017. The Tensor Algebra Compiler. Proc. ACM Program. Lang. 1, OOPSLA,
Article 77 (Oct. 2017), 29 pages. https://doi.org/10.1145/3133901

[34] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras-
inghe. 2017. The tensor algebra compiler. Proceedings of the ACM on Programming
Languages 1, OOPSLA (2017), 1â€“29.

[35] Tamara G Kolda and Brett W Bader. 2009. Tensor decompositions and applications.

SIAM review 51, 3 (2009), 455â€“500.

[36] Tamara G Kolda and Jimeng Sun. 2008. Scalable tensor decompositions for multi-
aspect data mining. In 2008 Eighth IEEE international conference on data mining.
IEEE, 363â€“372.

[37] Kornilios Kourtis, Vasileios Karakasis, Georgios Goumas, and Nectarios Koziris.
2011. CSX: an extended compression format for spmv on shared memory systems.

[61] Shaden Smith and George Karypis. 2017. Accelerating the tucker decomposition
with compressed sparse tensors. In European Conference on Parallel Processing.
Springer, 653â€“668.

[62] Shaden Smith, Niranjay Ravindran, Nicholas D Sidiropoulos, and George Karypis.
2015. SPLATT: Efficient and parallel sparse tensor-matrix multiplication. In 2015
IEEE International Parallel and Distributed Processing Symposium. IEEE, 61â€“70.

[63] Edgar Solomonik, Devin Matthews, Jeff R Hammond, John F Stanton, and James
Demmel. 2014. A massively parallel tensor contraction framework for coupled-
cluster computations. J. Parallel and Distrib. Comput. (2014).

[64] Qingquan Song, Hancheng Ge, James Caverlee, and Xia Hu. 2019. Tensor comple-
tion algorithms in big data analytics. ACM Transactions on Knowledge Discovery
from Data (TKDD) 13, 1 (2019), 1â€“48.

[65] Paul Springer, Aravind Sankaran, and Paolo Bientinesi. 2016. TTC: A tensor
transposition compiler for multiple architectures. In Proceedings of the 3rd ACM
SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array
Programming.

[66] Marat Valiev, Eric J Bylaska, Niranjan Govind, Karol Kowalski, Tjerk P Straatsma,
Hubertus JJ Van Dam, Dunyou Wang, Jarek Nieplocha, Edoardo Apra, Theresa L
Windus, et al. 2010. NWChem: A comprehensive and scalable open-source
solution for large scale molecular simulations. Computer Physics Communications
(2010).

[67] Richard W Vuduc and Hyun-Jin Moon. 2005. Fast sparse matrix-vector multipli-
cation by exploiting variable block structure. In International Conference on High
Performance Computing and Communications. Springer, 807â€“816.

[68] Endong Wang, Qing Zhang, Bo Shen, Guangyong Zhang, Xiaowei Lu, Qing
Wu, and Yajuan Wang. 2014. Intel math kernel library. In High-Performance
Computing on the IntelÂ® Xeon Phiâ„¢. Springer.

[69] Yining Wang, Hsiao-Yu Tung, Alexander Smola, and Animashree Anandkumar.
2015. Fast and guaranteed tensor decomposition via sketching. arXiv preprint
arXiv:1506.04448 (2015).

[70] James B White and Ponnuswamy Sadayappan. 1997. On improving the perfor-
mance of sparse matrix-vector multiplication. In Proceedings Fourth International
Conference on High-Performance Computing. IEEE, 66â€“71.

[71] Samuel Williams, Leonid Oliker, Richard Vuduc, John Shalf, Katherine Yelick,
and James Demmel. 2007. Optimization of sparse matrix-vector multiplication
on emerging multicore platforms. In SCâ€™07: Proceedings of the 2007 ACM/IEEE
Conference on Supercomputing. IEEE, 1â€“12.

[72] Biwei Xie, Jianfeng Zhan, Xu Liu, Wanling Gao, Zhen Jia, Xiwen He, and Lixin
Zhang. 2018. Cvr: Efficient vectorization of spmv on x86 processors. In Proceedings
of the 2018 International Symposium on Code Generation and Optimization. 149â€“
162.

[73] Shengen Yan, Chao Li, Yunquan Zhang, and Huiyang Zhou. 2014. yaSpMV: yet
another SpMV framework on GPUs. Acm Sigplan Notices 49, 8 (2014), 107â€“118.
[74] Carl Yang, AydÄ±n BuluÃ§, and John D Owens. 2018. Design principles for sparse
matrix multiplication on the GPU. In European Conference on Parallel Processing.
Springer.

[75] Tatsuya Yokota and Andrzej Cichocki. 2014. Multilinear tensor rank estimation
via sparse Tucker decomposition. In 2014 Joint 7th International Conference on
Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium
on Advanced Intelligent Systems (ISIS). IEEE, 478â€“483.

[76] Xianyi Zhang, Yunquan Zhang, Xiangzheng Sun, Fangfang Liu, Shengfei Liu,
Yuxin Tang, and Yucheng Li. 2009. Automatic Performance Tuning of SpMV on
GPGPU. HPC Asia, Kaohsiung, Taiwan, China (2009), 173â€“179.

[77] Yin Zhang, Min Chen, Shiwen Mao, Long Hu, and Victor CM Leung. 2014. CAP:
Community activity prediction based on big data analysis. Ieee Network 28, 4
(2014), 52â€“57.

ACM SIGPLAN Notices 46, 8 (2011), 247â€“256.

[38] Chris Lattner, Jacques Pienaar, Mehdi Amini, Uday Bondhugula, River Riddle,
Albert Cohen, Tatiana Shpeisman, Andy Davis, Nicolas Vasilache, and Oleksandr
Zinenko. 2020. MLIR: A Compiler Infrastructure for the End of Mooreâ€™s Law.
arXiv preprint arXiv:2002.11054 (2020).

[39] Benjamin C Lee, Richard W Vuduc, James W Demmel, and Katherine A Yelick.
2004. Performance models for evaluation and automatic tuning of symmetric
sparse matrix-vector multiply. In International Conference on Parallel Processing,
2004. ICPP 2004. IEEE, 169â€“176.

[40] Jiajia Li, Jimeng Sun, and Richard Vuduc. 2018. HiCOO: hierarchical storage of
sparse tensors. In SC18: International Conference for High Performance Computing,
Networking, Storage and Analysis. IEEE, 238â€“252.

[41] Jiajia Li, Guangming Tan, Mingyu Chen, and Ninghui Sun. 2013. SMAT: an
input adaptive auto-tuner for sparse matrix-vector multiplication. In Proceed-
ings of the 34th ACM SIGPLAN conference on Programming language design and
implementation. 117â€“126.

[42] Jiajia Li, Bora UÃ§ar, Ãœmit V Ã‡atalyÃ¼rek, Jimeng Sun, Kevin Barker, and Richard
Vuduc. 2019. Efficient and effective sparse tensor reordering. In Proceedings of
the ACM International Conference on Supercomputing. 227â€“237.

[43] Xupeng Li, Bin Cui, Yiru Chen, Wentao Wu, and Ce Zhang. 2017. Mlog: Towards
declarative in-database machine learning. Proceedings of the VLDB Endowment
10, 12 (2017), 1933â€“1936.

[44] Bangtian Liu, Chengyao Wen, Anand D Sarwate, and Maryam Mehri Dehnavi.
2017. A unified optimization approach for sparse tensor operations on gpus. In
2017 IEEE international conference on cluster computing (CLUSTER). IEEE, 47â€“57.
[45] Anna Lubiw. 1987. Doubly lexical orderings of matrices. SIAM J. Comput. 16, 5

(1987), 854â€“879.

[46] Yuan Luo, Fei Wang, and Peter Szolovits. 2017. Tensor factorization toward

precision medicine. Briefings in bioinformatics 18, 3 (2017), 511â€“514.

[47] Marco Maggioni and Tanya Berger-Wolf. 2013. AdELL: An adaptive warp-
balancing ELL format for efficient sparse matrix-vector multiplication on GPUs.
In 2013 42nd international conference on parallel processing. IEEE, 11â€“20.

[48] Duane Merrill and Michael Garland. 2016. Merge-based parallel sparse matrix-
vector multiplication. In SCâ€™16: Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis. IEEE, 678â€“689.
[49] Alexander Monakov, Anton Lokhmotov, and Arutyun Avetisyan. 2010. Au-
tomatically tuning sparse matrix-vector multiplication for GPU architectures.
In International Conference on High-Performance Embedded Architectures and
Compilers. Springer, 111â€“125.

[50] Israt Nisa, Jiajia Li, Aravind Sukumaran-Rajam, Prasant Singh Rawat, Sriram
Krishnamoorthy, and Ponnuswamy Sadayappan. 2019. An efficient mixed-mode
representation of sparse tensors. In Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis. 1â€“25.
[51] Daniel W Otter, Julian R Medina, and Jugal K Kalita. 2020. A survey of the usages
of deep learning for natural language processing. IEEE Transactions on Neural
Networks and Learning Systems (2020).

[52] Robert Paige and Robert E Tarjan. 1987. Three partition refinement algorithms.

SIAM J. Comput. 16, 6 (1987), 973â€“989.

[53] Jack Poulson, Bryan Marker, Robert A Van de Geijn, Jeff R Hammond, and
Nichols A Romero. 2013. Elemental: A new framework for distributed memory
dense matrix computations. ACM Transactions on Mathematical Software (TOMS)
(2013).

[54] Steffen Rendle, Leandro Balby Marinho, Alexandros Nanopoulos, and Lars
Schmidt-Thieme. 2009. Learning optimal ranking with tensor factorization for tag
recommendation. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. 727â€“736.

[55] Chase Roberts, Ashley Milsted, Martin Ganahl, Adam Zalcman, Bruce Fontaine,
Yijian Zou, Jack Hidary, Guifre Vidal, and Stefan Leichenauer. 2019. Tensornet-
work: A library for physics and machine learning. arXiv preprint arXiv:1905.01330
(2019).

[56] Martin D Schatz, Tze Meng Low, Robert A van de Geijn, and Tamara G Kolda.
2014. Exploiting symmetry in tensors for high performance: Multiplication with
symmetric tensors. SIAM Journal on Scientific Computing (2014).

[57] Martin D Schatz, Robert A Van de Geijn, and Jack Poulson. 2016. Parallel matrix
multiplication: A systematic journey. SIAM Journal on Scientific Computing
(2016).

[58] Naser Sedaghati, Te Mu, Louis-NoÃ«l Pouchet, Srinivasan Parthasarathy, and P
Sadayappan. 2015. Automatic selection of sparse matrix representation on GPUs.
In Proceedings of the 29th ACM on International Conference on Supercomputing.
99â€“108.

[59] Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evange-
los E Papalexakis, and Christos Faloutsos. 2017. Tensor decomposition for signal
processing and machine learning. IEEE Transactions on Signal Processing 65, 13
(2017), 3551â€“3582.

[60] Shaden Smith, Jee W. Choi, Jiajia Li, Richard Vuduc, Jongsoo Park, Xing Liu, and
George Karypis. 2017. FROSTT: The Formidable Repository of Open Sparse Tensors
and Tools. http://frostt.io/

12

