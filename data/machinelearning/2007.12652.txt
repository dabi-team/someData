2
2
0
2

n
u
J

8
2

]

G
L
.
s
c
[

4
v
2
5
6
2
1
.
7
0
0
2
:
v
i
X
r
a

MurTree: Optimal Decision Trees via
Dynamic Programming and Search

Emir Demirović
Anna Lukina
Delft University of Technology
Delft, The Netherlands

Emmanuel Hebrard
LAAS CNRS
Toulouse, France

Jeﬀrey Chan
RMIT University
Melbourne, Australia

James Bailey
Christopher Leckie
Kotagiri Ramamohanarao
University of Melbourne
Melbourne, Australia

Peter J. Stuckey
Monash University and DATA61
Melbourne, Australia

e.demirovic@tudelft.nl
a.lukina@tudelft.nl

hebrard@laas.fr

jeffrey.chan@rmit.edu.au

baileyj@unimelb.edu.au
caleckie@unimelb.edu.au
kotagiri@unimelb.edu.au

peter.stuckey@monash.edu

Abstract

1Decision tree learning is a widely used approach in machine learning, favoured in appli-
cations that require concise and interpretable models. Heuristic methods are traditionally
used to quickly produce models with reasonably high accuracy. A commonly criticised
point, however, is that the resulting trees may not necessarily be the best representation
of the data in terms of accuracy and size. In recent years, this motivated the development
of optimal classiﬁcation tree algorithms that globally optimise the decision tree in contrast
to heuristic methods that perform a sequence of locally optimal decisions. We follow this
line of work and provide a novel algorithm for learning optimal classiﬁcation trees based
on dynamic programming and search. Our algorithm supports constraints on the depth
of the tree and number of nodes. The success of our approach is attributed to a series
of specialised techniques that exploit properties unique to classiﬁcation trees. Whereas
algorithms for optimal classiﬁcation trees have traditionally been plagued by high runtimes
and limited scalability, we show in a detailed experimental study that our approach uses
only a fraction of the time required by the state-of-the-art and can handle datasets with
tens of thousands of instances, providing several orders of magnitude improvements and
notably contributing towards the practical use of optimal decision trees.

Keywords: decision trees, search, dynamic programming, combinatorial optimisation

1. The paper has been published in JMLR’22: https://jmlr.csail.mit.edu/beta/papers/v23/20-520.html.

©2021 Emir Demirović, Anna Lukina, Emmanuel Hebrard, Jeﬀrey Chan, James Bailey, Christopher Leckie,
Kotagiri Ramamohanarao, and Peter J. Stuckey.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

1. Introduction

The combination of simplicity and eﬀectiveness has popularised decision tree models in the
machine learning community. A notable advantage of these models is their interpretability,
in particular when the tree is of small size. Figure 1 shows an example of such a model,
which may be easily understood even by non-experts.

Despite its clear structure, constructing a decision tree to represent the data is a chal-
lenging computational problem (NP-hard). Traditionally, models are built using heuristic
methods, such as CART (Breiman et al. (1984)), which iteratively optimise a local objective
function. While these techniques have shown to be immensely valuable due to their ability
to provide high quality trees in low computational time, the resulting tree is not guaranteed
to be globally optimal, i.e., it may not necessarily be the best representation of the data in
terms of accuracy, size, or other considerations such as fairness.

An alternative to heuristic approaches is to construct optimal decision trees, i.e., the best
possible decision tree according to a given metric. The idea of computing optimal decision
trees dates back to approximately the 1970s when constructing optimal decision trees was
proven to be NP-hard by Hyaﬁl and Rivest (1976).2 As emphasised by Bertsimas and Dunn
(2017), while optimal decision trees have always been desirable, the authors of the CART
algorithm (Breiman et al. (1984)) found that such trees were computationally infeasible at
the time, and hence heuristics were the only option.

Optimal decision trees are enticing for several reasons. It has been observed that a more
accurate representation of the data oﬀers better generalisation on unseen data (Bertsimas
and Dunn (2017); Verwer and Zhang (2017, 2019)). This has been reiterated in our exper-
iments as well. Globally optimal trees are particularly important in socially-sensitive con-
texts, where optimality plays an important role in ensuring fairness (Aghaei et al. (2019)).
In some applications, the goal is to optimise the size of the decision tree representing a given
controller to save memory for embedded devices (Ashok et al. (2020)). Decision trees, in
particular those of smaller size, are desirable for formal methods when verifying properties
of trained controllers (Bastani et al. (2018)), as opposed to more complex machine learning
models. In recent years, there has been growing interest in explainable artiﬁcial intelligence.
The basic premise is that machine learning models, apart from high accuracy, must also
be able to explain their decisions to a (non-expert) human. This is necessary to increase
human trust and reliability of machine learning in complex scenarios that are conventionally
handled by humans. Optimal decision trees of small size naturally ﬁt within the scope of
explainable AI, as their reduced size is more convenient for human interpretation.

Decision tree learning may be deﬁned as a mathematical optimisation program: an
objective function is posed together with a set of constraints that encode the decision tree
structure. An advantage of optimal algorithms over heuristic approaches is that they adhere
precisely to the given speciﬁcation. This allows a clear analysis and assessment of the
suitability of the particular mathematical formulation for a given application. In contrast,
in heuristic methods there is a discrepancy between the target learning problem and the goals
of the heuristic algorithm, i.e., the methods may not directly optimise the tree according
to the globally deﬁned objective, but rather locally optimise a sequence of subproblems

2. Their proof is for the problem of ﬁnding a perfect tree minimising the expected number of feature tests.
However, it can easily be adapted to maximising the accuracy under a constraint on the maximum depth.

2

MurTree: Optimal Decision Trees via Dynamic Programming and Search

with respect to a surrogate metric. This leads to situations where it may be diﬃcult to
make conclusive statements on the learning problem deﬁnition, as the heuristic approach
may not faithfully follow the desired metrics. For example, a speciﬁcation might be deemed
suboptimal not due to a ﬂaw in the deﬁnition, but rather because of the inability of the
heuristic algorithm to optimise according to the speciﬁcation.

Despite the appeal of optimal algorithms for decision trees, heuristic methods are his-
torically the dominant approach due to computational reasons. Indeed, heuristic methods
oﬀer scalable algorithms that produce results in the order of seconds. However, as both
algorithmic techniques and hardware advanced, optimal decision trees have become within
practical reach and attracted growing interest from the research community. In particular,
there has been a surge of successful methods in the past few years. These approaches use
generic optimisation methods, namely integer programming (Bertsimas and Dunn (2017);
Verwer and Zhang (2017, 2019); Aghaei et al. (2019); Zhu et al. (2020)), constraint pro-
gramming (Verhaeghe et al. (2019)), and SAT (Narodytska et al. (2018); Avellaneda (2020);
Janota and Morgado (2020)), and algorithms tailored to the decision tree problem (Nijssen
and Fromont (2010, 2007); Hu et al. (2019); Aglin et al. (2020a); Lin et al. (2020)). The
methods DL8 (Nijssen and Fromont (2007, 2010)) and DL8.5 (Aglin et al. (2020a,b)) are of
particular interest as they can be seen as a starting point for our work. The DL8.5 approach
has been shown to be highly eﬀective, outperforming other approaches when computing full
binary decision trees on binary data, demonstrating the value of specialising methods to
exploit speciﬁc decision tree properties over generic optimisation approaches.

Our Contribution. While previous works use highly related ideas, the presentation
and terminology may diﬀer substantially.
In this work, we unify and generalise success-
ful concepts from the literature by viewing the problem through the lens of a conventional
algorithmic framework, namely dynamic programming and search. We introduce novel algo-
rithmic techniques that reduce computation time by orders of magnitude when compared to
the state-of-the-art. We conduct an experimental study on a wide range of benchmarks from
the literature to show the eﬀectiveness of our approach and its components, and reiterate
that optimal decision trees lead to better generalisation in terms of out-of-sample accuracy.
In more detail, the contributions are as follows:

• MurTree (Section 4), an algorithm for computing optimal classiﬁcation trees. Given
an input dataset and a set of predicates, it computes a decision tree that minimises
the number of misclassiﬁcations using the given predicates. The algorithm allows
constraints on the depth and the number of nodes of the decision tree. The method
may be extended with additional functionality, such as multi-classiﬁcation, regression,
the sparse decision tree objective, lexicographical minimisation of misclassiﬁcation and
size, anytime behaviour, and nonlinear metrics, as discussed in Section 4.8.

• A clear high-level view of the algorithm using conventional algorithmic principles,
namely dynamic programming and search, that uniﬁes and generalises some of the
ideas from the literature (Section 4.1).

• A specialised algorithm for computing the optimal classiﬁcation tree of depth two,
It uses a frequency
which serves as the backbone of our algorithm (Section 4.3).
counting method to avoid explicitly referring to the dataset when constructing the

3

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

tree, substantially reducing the runtime of computing optimal trees. The technique
is further augmented with an incremental technique that takes into account previous
computations, providing orders of magnitude speed-ups. Counting and incremental
construction ideas have been previously used in classical algorithms, such as counting
sort, and in the frequent itemset mining community, e.g., Zaki and Gouda (2003). We
exploit such ideas in the context of decision trees.

• A novel similarity-based mechanism for computing a lower bound on the number of
misclassiﬁcations. The bound is eﬀective in determining that portions of the search
space cannot contain better decision trees than currently found during the search,
which allows the algorithm to prune parts of the search space without needing further
inspection, providing additional speed-ups. The bound is derived by examining previ-
ously computed subtrees and computing a bound on the number of misclassiﬁcations
that must hold in the new search space (Section 4.4).

• Several extensions to DL8.5 (Aglin et al. (2020a)), namely we incorporate the con-
straint on the number of nodes, extend the caching technique to take into account
constraints on both the depth and number of nodes and provide a novel implemen-
tation of two existing caching schemes (Section 4.5), describe an incremental solving
option to allow reusing computation when solving a series of increasingly large decision
trees (Section 4.5.5), which is useful in hyper-parameter tuning, for example, reﬁne
the lower bounding technique on the number of misclassiﬁcations from DL8.5 (Section
4.5.3), and discuss a dynamic node exploration strategy (Section 4.6) that leads to
consistent improvements over a conventional post-order search.

• A detailed experimental study to analyse the eﬀectiveness of our individual techniques
and scalability of our approach, evaluate our approach with respect to the state-of-the-
art optimal classiﬁcation tree algorithms, and compare against heuristic decision tree
and random forest algorithms on out-of-sample accuracy (Section 5). The experimental
results show that our approach provides generalisable trees and exhibits speed-ups of
(several) orders of magnitude when compared to the state-of-the-art.

The rest of the paper is organised as follows.

In the next section, we introduce the
notations and deﬁnitions used throughout the paper. In Section 3, we review the state-of-
the-art for optimal decision trees. Our main contribution is given in Section 4, where we
describe our MurTree algorithm. In Section 5, we conduct a series of empirical evaluations
of our approach and conclude in Section 6.

2. Preliminaries

A feature is a variable that encodes information about an object. We speak of binary,
categorical, and continuous features depending on their domain, i.e., binary, discrete, and
continuous domains. A feature vector is a vector of features. An instance is a pair that
consists of a feature vector and a value representing the class. A class can take continuous
or discrete values. In future text, we assume the class is a discrete value, i.e. we consider
classiﬁcation tasks. A dataset, or simply data, is a set of instances. While features within
a vector may have diﬀerent domains, the i-th feature of each feature vector of the dataset

4

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Figure 1: A Decision Tree for Commuting to Work

shares the same domain. The assumption is that the features describe certain characteristics
about the objects, and the i-th feature of each feature vector refers to the same characteristic.
A decision tree is a machine learning model that takes the form of a tree (see Figure
1). We consider binary trees, i.e., trees that contain nodes with at most two children. We
call leaf and non-leaf nodes classiﬁcation and predicate nodes, respectively. Each predicate
node is assigned a predicate that maps feature vectors to a Boolean value, e.g., “CityBike
available?” is a predicate with a clear yes/no answer. The left and right edges of a predicate
node are associated with the values zero and one, respectively. Each classiﬁcation node is
assigned a ﬁxed class. We note that other variations of decision trees are possible, e.g., more
than two children, but these are not considered in this work.

A decision tree may be viewed as a function that performs classiﬁcation according to
the following recursive procedure. Given a feature vector, it starts by considering the root
node. If the considered node is a classiﬁcation node, its class determines the class of the
feature vector and the procedure terminates. Otherwise, the node is a predicate node, and
the left child node will be considered next if the predicate of the node evaluates to zero, and
otherwise the right child node is selected. The process recurses until a class is determined.
The misclassiﬁcation score of a decision tree on data is the number of instances for which
the classiﬁcation produces an incorrect class considering the data as ground truth.

In practice, the predicates take a special form. For single-variate or axis-aligned decision
trees, which are the focus of this work, predicates only consider a single feature and typically
test whether it exceeds a threshold value. For example, the predicate in Fig. 1 “Have more
considers a single feature representing the available time in minutes
than 30 minutes?”
and tests whether it exceeds the value thirty. We refer to these nodes as feature nodes, as
the predicate depends solely on one feature. Predicates are chosen based on the dataset.
Generalisations of decision trees are straight-forward: multi-variate versions use predicates
that operate on more than one feature, and predicates can be substituted by functions whose
co-domains are of size n, in which case the decision tree is an n-ary tree with an analogous
deﬁnition. These generalisations are not considered in this work.

The depth of a decision tree is the maximum number of feature nodes any instance may
encounter during classiﬁcation. The size of a decision tree is the number of feature nodes.
For example, the decision tree in Fig. 1 has the depth and size equal to two. It follows that
the maximum size of a decision tree with depth d is 2d −1. An alternative size deﬁnition may

5

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

consider the total number of nodes in the tree. Note that these deﬁnitions are equivalent,
as a tree with n predicate nodes has n + 1 classiﬁcation nodes.

The process of decision tree learning seeks to compute a decision tree that minimises a
target metric under a set of constraints. We are primarily concerned with minimising the mis-
classiﬁcation score given a maximum depth and a maximum number of feature nodes. Other
metrics and constraints may also be considered (see Section 4.8). Given the misclassiﬁcation
score, classiﬁcation nodes will be assigned the class that minimises the misclassiﬁcations on
the given dataset. This corresponds to computing the instances that reach the classiﬁcation
node during classiﬁcation and selecting the class of the node according to the majority class.
Following previous optimal decision tree works (Nijssen and Fromont (2007, 2010); Nar-
odytska et al. (2018); Verhaeghe et al. (2019); Aglin et al. (2020a); Hu et al. (2019); Lin
et al. (2020)), we consider the setting where all features are binary. Datasets with contin-
uous and/or categorical features are assumed to be binarised as a preprocessing step. This
corresponds to selecting the set of predicates upfront rather than during algorithm execution
as is standard in heuristic algorithms.

We use special notation for binary datasets, where the domain of features is Boolean,
i.e., {0, 1}. Given a feature vector fv, we write fi ∈ fv and fi ∈ fv if the i-th feature has
value one and zero, respectively. If fi ∈ fv, we say the i-th feature is present in the feature
vector fv (a positive feature), otherwise it is not present (a negative feature). We consider
only predicates that test the presence of a feature, i.e., the predicates Pi(fv) and Pi(fv)
evaluate to one if fi ∈ fv and fi ∈ fv, respectively, and evaluate to zero otherwise. Given
this special form, we simply write fi or fi for the predicates instead of Pi and Pi. The binary
dataset D is partitioned into a positive and negative class of instances based on the classes,
i.e., D = D+ ∪ D−. We consider the partitions as sets of feature vectors since their class is
clear from context, and write D(f ) as the set of instances from D that contain feature f ,
and analogously for multiple features, e.g., D(f1, f2) are the set of instances that contain
both f1 and f2.

3. Literature Review

Historically, the most popular techniques for decision tree learning were based on heuristics
due to their eﬀectiveness and scalability. Examples of these algorithms include CART,
originally proposed by Breiman et al. (1984), and C4.5 by Quinlan (1993). These algorithms
start with a single node, and iteratively expand the tree based on metrics such as information
gain and Gini coeﬃcients, possibly post-processing the obtained decision trees to prune
branches in an eﬀort to reduce overﬁtting. While there is a vast literature on (heuristic)
algorithms for decision trees, in this work, we are primarily concerned with optimal single-
variate decision trees, and hence direct further discussion to such settings.

Bertsimas and Shioda (2007) presented a mixed-integer programming approach for op-
timal decision tree s that worked well on smaller datasets. Mixed-integer programming
formulations with better performance were given by Bertsimas and Dunn (2017) and Ver-
wer and Zhang (2017). These methods encode the optimal decision tree by ﬁxing the tree
depth in advance, creating variables to represent the predicates for each node, and adding
constraints to enforce the decision tree structure. These approaches were later improved by
BinOPT (Verwer and Zhang (2019)), a binary linear programming formulation, that took

6

MurTree: Optimal Decision Trees via Dynamic Programming and Search

advantage of binarising data to reduce the number of variables and constraints required to
encode the problem. Aghaei et al. (2019) used a mixed-integer programming formulation
for optimal decision trees that supported fairness metrics. The authors argued that using
machine learning in socially sensitive contexts may perpetuate discrimination if no special
measures are taken into account. In this instance, optimal decision trees provide the best
tree that balanced accuracy and fairness, albeit with a high computational time when com-
pared to specialised heuristic methods (Kamiran et al. (2010)). Recently, Zhu et al. (2020)
proposed a novel mixed-integer programming formulation based on support vector machines
and a cutting plane technique for optimal multi-variate decision trees, and a ﬂow-based en-
coding has been developed Aghaei et al. (2020). An advantage of declarative approaches is
that adding additional constraints may be straight-forward, however scalability may be an
issue when compared to specialised approaches when considering single-variate trees, e.g.,
DL8.5 (see below) or our method. For more information regarding decision tree optimisa-
tion using mathematical programming, we refer the readers to the survey by Carrizosa et al.
(2021).

Encodings of decision trees using propositional logic (SAT) and constraint programming
have been initially devised by Bessiere et al. (2009). Recently, an improved SAT model has
been proposed by Narodytska et al. (2018), after which several other SAT-related works
have been published (Avellaneda (2020); Janota and Morgado (2020); Schidler and Szeider
(2021)). This line of work deviates from conventional machine learning approaches, as the
aim is to construct the smallest tree that perfectly describes the given dataset, i.e., leads to
zero misclassiﬁcations on the training data, although they can be adapted to the accuracy
criterion via maximum satisﬁability (MaxSAT) Hu et al. (2020). To circumvent scalability
issues, the methods perform subsampling of the data, incrementally construct the encoding,
and/or focus on improving a subtree obtained using a heuristic algorithm.

Nijssen and Fromont (2007, 2010) introduced a framework named DL8 for optimal de-
cision trees that could support a wide range of constraints. They observed that the left
and right subtree of a given node can be optimised independently, introduced a caching
technique to save subtrees computed during the algorithm in order to reuse them at a later
stage, and combined these with ideas from the pattern mining literature to compute optimal
decision trees. DL8 laid a foundation for optimal decision tree algorithms that follow.

Verhaeghe et al. (2019) approached the optimal classiﬁcation tree problem by minimis-
ing the misclassiﬁcations using constraint programming. The independence of the left and
right subtrees from Nijssen and Fromont (2007, 2010) was captured in an AND-OR search
framework. Upper bounding on the number of misclassiﬁcations was used to prune parts of
the search space and their algorithm incorporated an itemset mining technique to speed-up
the computation of instances per node and used a caching technique similar to DL8 (Nijssen
and Fromont (2007, 2010)).

Hu et al. (2019) presented an algorithm that computes the optimal decision tree by con-
sidering a balance between misclassiﬁcations and number of nodes. They apply exhaustive
search, caching, and lower bounding of the misclassiﬁcations based on the cost of adding a
new node to the decision tree. The method was improved and extended by Lin et al. (2020),
providing good performance if the sparsity coeﬃcient, which controls the balance between
accuracy and number of nodes, is suﬃciently high.

7

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Aglin et al. (2020a) developed DL8.5 by combining and reﬁning the ideas from DL8
and the constraint programming approach. Their main addition was an upper bounding
technique, which limited the upper misclassiﬁcation value of a child node once the optimal
subtree was computed for its sibling, and a lower bounding technique, where the algorithm
stored information not only about computed optimal subtrees but also pruned subtrees to
provide a lower bound on the misclassiﬁcations of a subtree. This led to an algorithm that
outperformed previous approaches by a notable margin when optimising the misclassiﬁcation
score under a depth constraint. The method was recently released as a Python library with
further improvements based on sparse bitvectors (Aglin et al. (2020b)).

Exploiting properties speciﬁc to the decision tree learning problem proved to be valuable
in improving algorithmic performance in previous work. In particular, search and pruning
techniques, caching computation for later reuse, and the techniques that take advantage of
the decision tree structure all lead to notable gains in performance. These are the main
reasons for the success of specialised methods over generic frameworks, such as integer
programming and SAT. As there is a signiﬁcant overlap of ideas and techniques used in
related work, we discuss these in more detail in Section 4.1 when presenting the high-level
view of our algorithm.

The above discussion was mainly concerned with single-variate optimal decision tree
algorithms, which are the focus of this work. Other related work includes heuristic meth-
ods for multi-variate trees (Yang et al. (2019)), theoretical analysis of heuristic methods
(Blanc et al. (2020)), a ﬁne-grained computational complexity study (Ordyniak and Szei-
der (2021)), neural networks for decision trees (Kontschieder et al. (2015); Tanno et al.
(2019)), randomised trees (Blanquero et al. (2020)), end-to-end learning of decision trees
(Hehn et al. (2019); Elmachtoub et al. (2020)), and dynamic programming methods to con-
struct decision trees from random forests (Vidal and Schiﬀer (2020)). For more refereces,
we refer the readers to a curated list of decision tree papers by Benedek Rozemberczki:
https://github.com/benedekrozemberczki/awesome-decision-tree-papers.

4. MurTree: Our Algorithm for Optimal Classiﬁcation Trees

Our algorithm computes optimal classiﬁcation trees by exhaustive search. The search space
is exponentially large, but special measures are taken to eﬃciently iterate through all trees,
exploit the overlap between trees, and avoid computing suboptimal decision trees. We
give the main idea of the algorithm, then provide the full pseudocode, and follow up with
individual subsections where we present each individual technique in greater detail.

The following text focusses on optimal classiﬁcation trees that minimise the number of
misclassiﬁed instances for binary datasets and binary classiﬁcation given constraints on the
depth and number of nodes. This serves as the core part of our algorithm. Further extensions
are discussed in Section 4.8, which includes multi-classiﬁcation, regression, optimising the
sparse objective, lexicographically minimising the misclassiﬁcation score and the number of
nodes, anytime behaviour, and optimising nonlinear metrics.

4.1 High-Level Idea

We note two important properties of decision trees:

8

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Property 1 (Independence) Given a dataset D, a feature node partitions the dataset D into
its left and right subtree, such that Dlef t ∩ Dright = ∅ and D = Dlef t ∪ Dright.

Property 2 (Overlap) Given a classiﬁcation node, a set of features encountered on the path
from the root node to the classiﬁcation node, and an instance, the order in which the features
are used to evaluate the instance does not change the classiﬁcation result.

Both properties follow directly from the deﬁnition of decision trees and are emphasised as
they play a major role in designing decision tree algorithms. Property 1 allows computing
the misclassiﬁcation score of the tree as the sum of the misclassiﬁcation scores of its left
and right subtree. As will be discussed, this is important as once a feature node has been
selected, the left and right subtrees can be optimised independently of each other. Property
2 shows there is an overlap between decision trees that share the same features, which is
taken advantage of by caching techniques. For example, once the optimal tree has been
computed for the dataset D(f1, f2), the resulting tree is stored in the cache and reused when
the dataset D(f2, f1) is encountered (see Section 4.5 for more details on caching), since both
D(f1, f2) and D(f2, f1) represent exactly the same subproblem.

The dynamic programming formulation of optimal classiﬁcation trees given in Eq. 1
provides a high-level summary of our algorithm. The input parameters consist of a binary
dataset D with features F, an upper bound on depth d, and an upper bound on the number
of feature nodes n. The output is the minimum number of misclassiﬁcations possible on the
data given the input decision tree characteristics.

T (D, d, n) =






T (D, d, 2d − 1)
T (D, n, n)
min{|D+|, |D−|}
min{T (D(f ), d − 1, n − i − 1)

n > 2d − 1
d > n
n = 0 ∨ d = 0
n > 0 ∧ d > 0

(1)

+ T (D(f ), d − 1, i) : f ∈ F, i ∈ [0, n − 1]}

The ﬁrst and second case in Eq. 1 place a natural limit on the number of feature nodes
and depth to avoid redundancy. The third case captures the situation where the node must
be declared as a classiﬁcation node, i.e., the node is labelled according to the majority
class. The fourth case states that computing the optimal misclassiﬁcation score amounts to
examining all possible feature splits and ways to distribute the feature node count to the
left and right children of the root node. For each combination of a selected feature and node
count distribution to its children, the optimal misclassiﬁcation is computed recursively as
the sum of the optimal misclassiﬁcations of its children. The formulation is exponential in
the depth, feature node limit, and number of features, but with special care, as presented in
the subsequent sections, it is possible to compute practically relevant optimal classiﬁcation
trees within a reasonable time.

Eq. 1 serves as the core foundation of our algorithm. In contrast to previous algorithms,
we take advantage of the structure of decision trees to allow imposing a limit on the number
of nodes as presented in Eq. 1. For example, previous methods either set the number of
nodes to the maximum value given the depth (Aglin et al. (2020a); Avellaneda (2020)), do
not directly limit the number of nodes but instead penalise the objective function for each

9

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

node in the tree (Hu et al. (2019); Lin et al. (2020)), or allow constraints on the number
of nodes but do not make use of decision tree properties (Bertsimas and Dunn (2017);
Narodytska et al. (2018); Verwer and Zhang (2017, 2019)). The last point is particularly
important as the ability to exploit optimal decision tree properties has proven to be essential
in achieving the best performance.

Diﬀerent forms of Eq. 1 were used in some previous work under diﬀerent terminology.
The AND-OR search method (Verhaeghe et al. (2019)), pattern mining approach (Nijssen
and Fromont (2007, 2010); Aglin et al. (2020a)), and the search by Hu et al. (2019) and Lin
et al. (2020) use the independence property of the left and right subtree (Property 1). Those
approaches save computed optimal subtrees (Property 2), which corresponds to memoisation
as an integral part of dynamic programming (Section 4.5). The DL8 papers (Nijssen and
Fromont (2007, 2010)) introduced a general framework with a variety of constraints which
includes, amongst others, constraints on the depth, node count, and fairness. Framing the
problem as a dynamic program dates from the 1970s (e.g., Garey (1972)), but the descrip-
tion in works afterwards deviated as new techniques were introduced. Our contribution is
presenting the problem using conventional dynamic programming notation and algorithms
that respect constraints on the depth of the tree and the number of nodes.

A key component of our algorithm is a specialised method for computing decision trees
of depth at most two. It takes advantage of the speciﬁc decision tree structure by performing
a precomputation on the data, which allows it to compute the optimal decision tree without
explicitly referring to the data. This oﬀers a signiﬁcantly lower computational complexity
compared to the generic case of Eq. 1, but is applicable in practice only to decision trees
of depth two. Thus, rather than following Eq. 1 until the base case, we stop the recursion
once a tree of depth two is required and invoke the specialised method.

A deﬁning characteristic of search algorithms are pruning techniques, which detect areas
of the search that may be discarded without losing optimality. In the case of decision trees,
subtrees may be pruned based on the lower or upper bound3 of the number of misclassi-
ﬁcations of the given subtrees.
If the lower bound shows that the misclassiﬁcations of a
currently considered subtree will result in a value greater than the set upper bound, the
subtree can be pruned, eﬀectively reducing the search space. Note that the upper bound is
always set in a way to exclude trees that have a higher misclassiﬁcation score than the best
tree found so far. The challenge when designing bounding techniques is to ﬁnd the correct
balance between pruning power and the computational time required by the technique.

We introduce a novel similarity-based lower bounding technique (Section 4.4) that derives
a bound based on the similarity of the previously considered subtrees. We use our lower
bounding method in combination with the previous lower bounding approach introduced in
DL8.5 (Aglin et al. (2020a)), which we describe in the following text. Given a parent node,
once the optimal subtree is computed for one of the children, an upper bound can be posed
on the other child subtree based on the best decision tree known for the parent node and
the number of misclassiﬁcations of the optimal child subtree. If a subtree fails to produce a
solution within the posed upper bound, the upper bound is eﬀectively a lower bound that
can be used once the same subtree is encountered again in the search. Our algorithm uses

3. Note that the term ‘upper bound’ is to not meant to be interpreted as an upper bound to the global
problem in the strict mathematical sense, but rather as a value that when exceeded leads to trees that
have more misclassiﬁcations than the currently best known tree during the execution of the algorithm.

10

MurTree: Optimal Decision Trees via Dynamic Programming and Search

a reﬁnement of the described lower bound, which additionally takes into account all lower
bounds of the children of the parent node (Section 4.5.3).

The next subsection describes our techniques in more detail.

4.2 Main Algorithm Description

Algorithm 1 summarises our algorithm. It can be seen as an instantiation of Eq. 1 with
additional techniques to speed-up the computation. In further text, we use the convention
that infeasible trees (denoted with ∅) have an inﬁnite misclassiﬁcation score.

The algorithm takes as input a dataset D consisting of positive D+ and negative D−
instances, branch information (initially empty, see below for details), the depth and the
number of feature nodes, and an upper bound that represents a limit on the number of
misclassiﬁcations before the tree is deemed infeasible, i.e., not of interest for example since
a better tree is known. The output is an optimal classiﬁcation tree respecting the input
constraints on the depth, size, and upper bound, or a ﬂag indicating that no such tree
exists, i.e., the problem is infeasible. The latter occurs as a result of recursive calls (see
further), which pose an upper bound that is necessary to ensure the decision tree has a
lower misclassiﬁcation value than the best tree found so far. The upper bound is initially
set to the misclassiﬁcation score of a single classiﬁcation node for the data and is updated
throughout the execution. We note that a tighter upper bound could be computed by using
a heuristic method at the start. The algorithm proceeds as follows.

(Alg. 1:

lines 2-3) If the upper bound is negative, the algorithm reports infeasibility.
Negative bounds may be a result of the calls in the general case algorithm (Alg. 2 and 3).
(Alg. 1: lines 4-8) If no feature nodes are allowed, the algorithm returns a classiﬁcation
node or reports infeasibility in case the classiﬁcation node misclassiﬁcation exceeds the
upper bound. The method LeafMisclassiﬁcation computes the misclassiﬁcation score of a
classiﬁcation node given a dataset D as min{|D−|, |D+|}, and the method ClassiﬁcationNode
returns a tree consisting of a single classiﬁcation node that minimises the misclassiﬁcation
score on the dataset D, i.e., it assigns the majority class as its label.

(Alg. 1: lines 9-14) After basic tests, the cache is queried to check whether the optimal
subtree has already been computed as part of a previous recursive call.
If the optimal
subtree is present in the cache, it is returned if the optimal subtree meets the upper bound
constraint, otherwise infeasibility is reported. Caching subtrees for trees where the depth is
constrained dates from DL8 (Nijssen and Fromont (2007, 2010)). In our work, the algorithm
additionally caches with respect to the depth and number of node constraints.

(Alg. 1: lines 16-21) Assuming that the optimal subtree is not in the cache, the cache
is updated using our similarity-based lower bound (Section 4.4). Naturally, the new lower
bound will replace the old cached bound only if it is of greater value. In case the lower
bounding procedure happens to recover an optimal solution for the subtree, it is returned
or infeasibility is reported if it exceeds the upper bound (see Section 4.4 for details).

(Alg. 1: lines 22-26) Afterwards, the algorithm attempts to prune based on the (possibly
updated) lower bound stored in the cache, or return a classiﬁcation node if the lower bound
matches the misclassiﬁcation score of the classiﬁcation node.

(Alg. 1:

lines 27-32) After all simpler operations have been performed, the algorithm
tests if the subproblem is a tree of depth at most two. A key aspect of our algorithm is that

11

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

trees of depth at most two are computed using a specialised procedure (Section 4.3). Should
this be the case, the specialised algorithm is used to solve the subtree and store in the cache
the solutions using one, two, and three feature nodes, regardless of the input requirements
(number of nodes and upper bound). This is done since running the specialised algorithm
produces the mentioned solutions as part of its procedure and it may be beneﬁcial to store all
the results. Once the computation is done, the algorithm returns the corresponding subtree
given the input number of nodes if it is within the upper bound limit, otherwise reports
infeasibility.
(Alg. 1:

line 33) Assuming neither of the above conditions took place, the algorithm
reaches the general (fourth) case from Eq. 1, where the search space is exhaustively explored
through a series of overlapping recursions. This is detailed in Alg. 2 and summarised below.
Algorithm 2: General Case. (Alg. 2: line 9) The algorithm considers each feature split.
(Alg. 2: line 10-11) If the current best solution is at the lower bound, the optimal decision
tree has been found and no further splits need to be considered. (Alg. 2: line 12-13) Feature
splits that do not discriminate at least one instance are skipped since they add no value to
the tree. This is summarised as follows.

Deﬁnition 1 (Degenerate Decision Trees) A decision tree is degenerate if it contains at least
one predicate node that does not discriminate a single training instance, i.e., the predicate
returns the same truth value for each of its training instances.

Proposition 2 (Pruning Degenerate Trees) Given a degenerate decision tree with n fea-
ture nodes and misclassiﬁcation score s on the training data, there exists at least one other
decision tree with n(cid:48) < n feature nodes and misclassiﬁcation score s(cid:48) ≤ s.

(Alg. 2: line 14-21) Recall that the number of allowed feature nodes is given as input.
Once a feature has been considered as the root node (line 9), the remaining node budget
is split among its children. For each feature split, the algorithm considers all possible
combinations of distributing the remaining node budget amongst its left and right subtrees.
Note that when considering no other node limit other than the limit imposed by the depth of
the tree, there is only one such combination, i.e., nmax = nmin in Algorithm 2. The algorithm
invokes a subroutine given in Algorithm 3 (see below), which computes the optimal tree
given the tree conﬁguration (the feature of the root and the number of features in children
subtrees), or reports a local lower bound on the solution (Section 4.5.3).

(Alg. 2: line 22-28) Throughout the algorithm the best tree found so far is recorded (line
19). After exhausting all feature splits for the root node, if the best tree found is within the
upper bound limit, the best tree is the optimal decision tree for the considered subproblem
and it is stored in the cache. Otherwise, a lower bound is computed and stored in the cache.
The fact that no tree was found within the upper bound limit implies a lower bound for the
given subproblem is one greater than the input upper bound. The information is stored in
the cache in case the bound is needed in one of the other recursive calls. This bound was
introduced in DL8.5 Aglin et al. (2020a) and we provide a further reﬁnement by taking into
account the local bounds (see the reﬁned bound in Section 4.5.3).

At the end of Algorithm 2, the internal data structures related to our similarity lower
bound are updated using the dataset D (see Section 4.4) before returning the result of the

12

MurTree: Optimal Decision Trees via Dynamic Programming and Search

algorithm, i.e., either the optimal decision tree or an infeasible tree indicating that no such
tree exists within the input speciﬁcation.

Algorithm 3: Subroutine to compute the optimal tree for a given tree conﬁguration. For
a chosen tree conﬁguration (the feature of the root and the number of features in children
subtrees), the algorithm determines the maximum depth of the left and right subtrees based
on the second case of Eq. 1. It then considers which subtree to recurse on ﬁrst. Previous work
in DL8.5 ﬁxed the order by exploring the left before the right subtree. In our algorithm, we
introduce a dynamic strategy that prioritises the subtree with the greater misclassiﬁcation
score of its leaf node (Section 4.6). The intuition is that such a subtree is more likely to
result in a tree with more misclassiﬁcations, and if one subtree has a high misclassiﬁcation
score it increases the likelihood of pruning the other sibling. For example, given a scenario
where there are two children, one that will result a zero misclassiﬁcation score and one that
will result in an infeasible subtree, exploring the later ﬁrst will remove the need to process
the former, whereas processing the nodes in reverse order will require solving both subtrees
instead of only one.

The algorithm then solves the subtrees in the chosen order. When computing the upper
bound of the ﬁrst subtree, the lower bound of the second is taken into account together with
the global upper bound provided. If the ﬁrst subtree is infeasible, a local lower bound is
returned using Eq. 15. Otherwise, the second subtree is computed. If both the left and right
subtree calls successfully terminated, the obtained decision tree is returned as the optimal
tree. Otherwise, a local lower bound is returned.

This concludes the main description of our algorithm. Before proceeding with detailing
each component of our algorithm, we reiterate the diﬀerences between our approach and
DL8.5 (Aglin et al. (2020a)) in light of the technical description given above.

Comparison with DL8.5 (Aglin et al. (2020a)). Algorithm 1 shares a similar layout as
in DL8.5, but there are notable diﬀerences that result in orders of magnitude speed-ups.
The diﬀerences can be summarised as follows: 1) we allow constraining the number of
feature nodes in addition to the depth, which is important in obtaining the smallest optimal
decision, e.g., to improve interpretability, or when hyper-parameter tuning to learn trees that
better generalise on unseen instances (Section 5.4), 2) our specialised algorithm (Section
4.3) is substantially more eﬃcient at computing trees with depth two when compared to
the general algorithm in Algorithm 1 or DL8.5, 3) we propose a new lower bound based on
the similarity with previously computed subtrees to further prune the search space (Section
4.4), reﬁne the previous lower bound (Section 4.5.3), and consider the lower bound when
imposing the upper bound on the subtrees, 4) our cache policy (Section 4.5) is extended to
support the number of feature nodes constraint and allows for incremental solving, allowing
reusing computation when solving trees with new depth or number of nodes, e.g., during
hyper-parameter tuning, 5) we dynamically determine which subtree to explore ﬁrst based
on pruning potential (Section 4.6) rather than use a static strategy, 6) we discuss our novel
implementation of two caching strategies (based on branches and datasets) that leads to a
light-weight cache (Section 4.5), allowing us to take advantage of more advanced hashing
on the dataset rather than on branches as done in DL8.5, and 7) we propose a number of
extensions (see Section 4.8).

13

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Algorithm 1: M urT ree.SolveSubtree(D, B, d, n, U B), the main algorithm loop

input: Dataset D = D+ ∪ D−, branch of the subtree B, depth d ∈ N0, number of feature

nodes n ∈ N0, upper bound on the misclassiﬁcations U B ∈ Z

output: Optimal classiﬁcation tree within the input speciﬁcation that minimises the

misclassiﬁcation score on D

1 begin

// Prune based on the upper bound
if U B < 0 then
return ∅

// Base case, second case (Eq. 1):
if d = 0 ∨ n = 0 then

no feature nodes are possible

if Leaf M isclassif ication(D) ≤ U B then

return Classif icationN ode(D)

else

return ∅

// Use cached subtrees if possible (Section 4.5)
if IsOptimalSubtreeInCache(D, B, d, n) then

T ← RetrieveOptimalSubtreeF romCache(D, B, d, n)
if M isclassif ications(T ) ≤ U B then

return T

else

return ∅

// Update the cache using the similarity-based lower bound (Section 4.4)
// Note that an optimal solution may be found in the process
updated_optimal_solution ← U pdateCacheU singSimilarity(D, B, d, n)
if updated_optimal_solution then

T ← RetrieveOptimalSubtreeF romCache(D, B, d, n)
if M isclassif ications(T ) ≤ U B then

return T

else

return ∅

// Prune if the lower bound exceeds the upper bound, since no tree can be found

within the upper bound requirement (Section 4.5.4)

LB ← RetrieveLowerBoundF romCache(D, B, d, n)
if LB > U B then
return ∅

// If the leaf node is already at the lower bound, no need to look further
if LB = Leaf M isclassif ication(D) then

return Classif icationN ode(D)

// Use Algorithm 4 for small trees from Section 4.3
// Note that the specialised algorithm updates the cache
if d ≤ 2 then

T ← SpecialisedDepthT woAlgorithm(D, B, d, n)
if M isclassif ications(T ) ≤ U B then

return T

else

return ∅

// Fourth case (Eq.
1):
return M urT ree.GeneralCase(D, B, d, n, U B)

exhaustively search using Algorithm 2

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

14

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Algorithm 2: M urT ree.GeneralCase(D, B, d, n, U B), the general (fourth) case
of Eq. 1 used in Algorithm 1

input: Dataset D = D+ ∪ D−, branch of the subtree B, depth d ∈ N0, number of feature

nodes n ∈ N0, upper bound on the misclassiﬁcations U B ∈ Z

output: Optimal classiﬁcation tree within the input speciﬁcation that minimises the

misclassiﬁcation score on D.

1 begin

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

// Use a single classification node as an initial solution
Tbest ← Classif icationN ode(D)
if Leaf M isclassif ication(D) > U B then

Tbest ← ∅

// Find the lower bound stored in cache (Section 4.5.4)
LB ← RetrieveLowerBoundF romCache(D, B, d, n)
// RLB refers to the refined lower bound in Eq. 16
RLB ← ∞
// Compute allowed number of nodes for child subtrees
nmax ← min{2(d−1) − 1, n − 1}
nmin ← (n − 1 − nmax)
for splitting f eature f ∈ F do

// If the current best node is the optimal node, stop
if M isclassif ications(Tbest) = LB then

break

// Nondiscriminary splits should be avoided
if |D(f )| = 0 ∨ |D(f )| = 0 then

continue

for nL ∈ [nmin, nmax] do
nR ← n − 1 − nL
// Impose an upper bound U B(cid:48) that ensures that a feasible tree will have

fewer misclassifications than the best tree found so far Tbest

U B(cid:48) ← min{U B, M isclassif ications(Tbest) − 1}
// Use Algorithm 3 to compute subproblem
(T, LBlocal) ←
M urT ree.SolveSubtreeGivenRootF eature(D, B, f, d, nL, nR, U B(cid:48))
if T (cid:54)= ∅ then
Tbest ← T

else

RLB ← min{RLB, LBlocal}

// Cache the optimal solution...
if M isclassif ications(Tbest) ≤ U B then

StoreOptimalSubtreeInCache(Tbest, D, B, d, n)
// ...or record the lower bound (Section 4.5.3)
else

LB ← max{LB, U B + 1}
if RLB < ∞ then

LB ← max{LB, RLB}

// Store the lower bound in the cache (Section 4.5.3)
StoreLowerBoundInCache(D, B, d, n, LB)
ReplaceDatasetF orSimilarityBound(D, B, d)
return Tbest

15

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Algorithm 3: M urT ree.SolveSubtreeGivenRootF eature(D, B, froot, d, nL, nR, U B):
a subroutine used as part of Algorithm 2

input: Dataset D = D+ ∪ D−, branch of the subtree B, root feature froot ∈ F,
depth d ∈ N0, number of feature nodes in left and right subtree
nL, nR ∈ N0, upper bound on the misclassiﬁcations U B ∈ Z

output: An optimal decision tree with feature froot as its root that satisﬁes the
input speciﬁcation and minimises the misclassiﬁcation score on D. If no
such tree exists, a lower bound on the misclassiﬁcation score is returned.

1 begin

// Get the depth and branches of the children subtrees
dL ← min(d − 1, nL)
dR ← min(d − 1, nR)
(BL, BR) ← GetChildBranches(B, froot)
// Dynamic order:
process left subtree first (Section 4.6)
if Leaf M isclassif ication(D(f root)) > Leaf M isclassif ication(D(froot)) then
U BL ← U B − RetrieveLowerBoundF romCache(D(f root), BR, dR, nR)
TL ← M urT ree(D(f root), BL, dL, nL, U BL)
// No need to compute the right subtree if the left child is infeasible
if TL = ∅ then

LBlocal ← compute local bound (Eq. 15)
return (∅, LBlocal)

U BR ← U B − M isclassif ications(TL)
TR ← M urT ree(D(froot), BR, dR, nR, U BR)
// If both children are feasible, return the optimal solution
if TR (cid:54)= ∅ then

T ← tree with root feature froot and children TL and TR
return (T, M isclassif ications(T ))

else

LBlocal ← compute local bound (Eq. 15)
return (∅, LBlocal)

else

// Dynamic post-order:
Process right subtree analogously as above

process right subtree first (Section 4.6)

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

4.3 Specialised Algorithm for Trees of Depth Two

An essential part of our algorithm is a specialised method for computing optimal decision
trees of depth two. The procedure is repeatedly called in our algorithm, i.e., each time a
tree of at most depth two needs to be optimally solved. In the following, we present an
algorithm that achieves lower complexity than the general algorithm (Eq. 1 and Prop. 3)
when considering trees with depth two.

Prior to presenting our specialised algorithm, we discuss the complexity of computing

decision trees of depth two using Eq. 1 as the baseline.

16

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Proposition 3 Computing the optimal classiﬁcation tree of depth two using Eq. 1 can be
done in O(|D| · |F|2) time.

Assume that splitting the data based on a feature node is done in O(|D|) time. Eq. 1
considers |F| splits for the root and for each feature performs 2 · |F| splits for its children.
This results in 2 · |F|2 splits and an overall runtime of O(|D| · |F|2), proving Proposition
3. In practice, partitioning the dataset based on a feature can be sped-up using bitvector
operations and caching subproblems (Aglin et al. (2020a); Verhaeghe et al. (2019); Hu et al.
(2019)), but the complexity remains as this only impacts the hidden constant in the big-O.
In the following, we present an algorithm with lower complexity and additional prac-
tical improvements which, when combined, reduce the runtime of computing the optimal
classiﬁcation tree of depth two by orders of magnitudes.

Algorithm 4 provides a summary. The input is a dataset D and the output is the
optimal classiﬁcation tree of depth two with three feature nodes that minimises the number
of misclassiﬁed instances.

The specialised procedure computes the optimal decision tree in two phases. In the ﬁrst
step, it computes frequency counts for each pair of features, i.e., the number of instances
in which both features are present. In the second step, it exploits the frequency counts to
eﬃciently enumerate decision trees without needing to explicitly refer to the data. This
provides a substantial speed-up compared to iterating through features and splitting data
as given in the dynamic programming formulation (Eq. 1) for decision trees of depth two.
We now discuss each phase in more detail and present a technique to incrementally compute
the frequency counts. We note that related counting and incremental computation ideas
have been used in classical algorithms, such as counting sort, and frequent itemset mining
methods, e.g., Zaki and Gouda (2003).

4.3.1 Phase One: Frequency counting (Algorithm 4, Lines 2-9)

Let F Q+(fi) and F Q+(fi, fj) denote the frequency counts in the positive instances for a
single feature and a pair of features, respectively. The functions F Q−(fi) and F Q−(fi, fj)
are deﬁned analogously for the negative instances.

A key observation is that based on F Q(fi) and F Q(fi, fj), we may compute F Q(fi),

F Q(fi, fj), F Q(fi, fj), and F Q(fi, fj). This is done as follows:

F Q+(fi) = |D+| − F Q+(fi)

F Q+(fi, fj) = F Q+(fi) − F Q+(fi, fj)

F Q+(fi, fj) = F Q+(fj) − F Q+(fi, fj)

F Q+(fi, fj) = |D+| − F Q+(fi) − F Q+(fj) + F Q+(fi, fj)

(2)

(3)

(4)

(5)

The equations make use of the fact that the features are binary. For example, Eq. 2
states that if the total number of positive instances is |D+| and we computed the frequency
count F Q+(fi), then the frequency count F Q+(fi) is the number of instances in which fi

17

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Algorithm 4: Specialised algorithm for computing optimal classiﬁcation trees of
depth two with three nodes

input: Binary dataset D = D+ ∪ D−
output: Optimal classiﬁcation tree of depth two with three feature nodes that

minimises the misclassiﬁcation score on D

1 begin
2

∀fi : F Q+(fi) ← 0 ∧ F Q−(fi) ← 0
∀fi, fj, i < j : F Q+(fi, fj) ← 0 ∧ F Q−(fi, fj) ← 0
/* Step 1:
for fv ∈ D+ do

construct the frequency counter of positive features */

for fi ∈ fv do

increment F Q+(fi)
for fj ∈ fv s.t. i < j do

increment F Q+(fi, fj)

construct the optimal decision tree based on the frequency counters

F Q− is computed as above using D−
/* Step 2:
F Q+ and F Q− */
/* Compute the best left and right subtrees for each feature */
for fi ∈ F do

for fj ∈ F s.t. i (cid:54)= j do

CS(fi, fj) ← min{F Q+(fi, fj), F Q−(fi, fj)}
CS(fi, fj) ← min{F Q+(fi, fj), F Q−(fi, fj)}
/* Compute branch with fi as root and fj as left child */
M Slef t(fi, fj) ← CS(fi, fj) + CS(fi, fj)
if BestLef tSubtree(fi).misclassif ication > M Slef t(fi, fj) then
BestLef tSubtree(fi).misclassif ication ← M Slef t(fi, fj)
BestLef tSubtree(fi).f eature ← fj

The best right subtree with fi as the root and fj as the right child is
computed analogously as above

/* Compute the best tree by taking the feature with the minimum sum of

misclassification of its children */
best_tree ← argminfi∈F {BestLef tSubtree(fi).misclassif ication +
BestRightSubtree(fi).misclassif ication}
return best_tree

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

does not appear, i.e., the diﬀerence between |D+| and F Q+(fi). Similar reasoning is applied
to the other equations and computing the frequency count F Q− is analogous.

The following proposition summarises the runtime of computing F Q+(fi, fj).

Proposition 4 (Computational Complexity of Phase One) Let m+ denote the maximum
number of features in any single positive instance. Frequency counts F Q+(fi, fj) can be
computed in O(|D+| · m2

+) time with O(F 2) memory.

18

MurTree: Optimal Decision Trees via Dynamic Programming and Search

An eﬃcient way of computing the frequency counts is to represent the feature vector as
a sparse vector, and iterate through each instance in the dataset and increase a counter for
each individual feature and each pair of features. This leads to the proposed complexity
result. The additional memory is required to store the frequency counters, allowing to query
a frequency count as a constant time operation. Note that the pairwise frequency count is
symmetric, i.e., F Q+(fi, fj) = F Q+(fj, fi), which requires only to consider fi and fj in the
frequency count for i < j. This results in a smaller hidden constant in the big-O notation.

4.3.2 Phase Two: Optimal tree computation (Algorithm 4, Lines 10-19)

Recall that a classiﬁcation node is assigned the positive class if the number of positive
instances exceeds the number of negative instances, otherwise the node class is negative.
Let CS(fi, fj) be the classiﬁcation score for a classiﬁcation node with all instances of D
containing both features fi and fj. The classiﬁcation score is then computed as follows.

CS(fi, fj) = min (cid:8)F Q+(fi, fj), F Q−(fi, fj)(cid:9)

(6)

Given a decision tree with depth two, a root node with feature froot, a left and right
children with features flef t and fright, we may compute the misclassiﬁcation score in constant
time assuming the frequency counts are available. Let M Slef t and M Sright denote the
misclassiﬁcation scores of the left or right subtree. The computations are as follows.

M Slef t(froot, flef t) = CS(froot, flef t) + CS(froot, flef t)

M Sright(froot, fright) = CS(froot, fright) + CS(froot, fright)

(7)

(8)

The total misclassiﬁcation score of the tree is the sum of misclassiﬁcations of its children.
As the number of misclassiﬁcation can be computed solely based on the frequency counts,
we may conclude the computational complexity.

Proposition 5 (Computational Complexity of Phase Two) Given the frequency counts F Q+
and F Q−, the optimal subtree tree can be computed in O(|F |2) time with O(|F |) memory.

It follows from Property 1 that given a root node with feature froot, the left and right
subtrees can be optimised independently. Therefore, it is suﬃcient to compute for each
feature its best left and right subtrees, and take the feature with the minimum sum of its
child misclassiﬁcations. To compute the best left and right feature for each feature, the
algorithm maintains information about the best left and right child for each feature found
so far, leading to the memory requirement from Proposition 5. The best features are initially
arbitrarily chosen. Recall that from Property 1 it follows that the left and right subtree can
be optimised independently:

min
flef t,fright∈F

M S(froot, flef t, fright) = min
flef t∈F

M Slef t(froot, flef t)+ min

fright∈F

M Sright(froot, fright)

Therefore, rather than considering triplets of features (froot, flef t, fright),

it iterates
through each pair of features (froot, fchild), computes the misclassiﬁcation values of the
left subtree using Eq. 7, updates the best left child for feature froot, and performs the same

19

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

procedure for the right child. After iterating through all pairs of features, the best left and
right subtree is known for each feature, leading to the proposed complexity. The optimal
decision tree can then be computed by ﬁnding the feature with minimum misclassiﬁcation
cost of its combined left and right misclassiﬁcation.

After discussing each individual phase, we may conclude the overall complexity:

Proposition 6 (Computational Complexity of Depth-2 Decision Trees) Let m be the upper
limit on the number of features in any single positive and negative instance. The number of
operations required to computing an optimal decision tree is O(|D| · m2 + |F|2) using O(F 2)
auxiliary memory.

The result follows by combining Propositions 4 and 5. The obtained runtime is sub-
stantially lower at the expense of using additional memory compared to the dynamic pro-
gramming formulation (Eq. 1) outlined in Proposition 3. Note that instances with binary
features are naturally sparse.
If the majority of instances contain more than half of the
features, then as a preprocessing step all feature values may be inverted to achieve sparsity
without loss of generality. The advantage of our approach is exempliﬁed with lower sparsity
ratios, i.e., cases where each vector contains a small number of features compared to the
total number of features.

There are several additional points to note, which are not shown in Algorithm 4 to keep

the pseudo-code succinct.

The above discussion assumed the feature node limit was set to three. The algorithm
can be modiﬁed for the case of two feature nodes, keeping the same complexity, while in the
case with only one feature node the pairwise computations are no longer necessary leading
to O(|D| · m + |F |) complexity, where m is the upper limit on the number of features in any
single positive and negative instance. As an optimisation technique, each time the algorithm
is invoked, we extract the solutions using one, two, and three nodes and store all of these
in cache (see Section 4.5), regardless of the initial node count. The reasoning is that it is
likely the other node counts will be considered in the future and the extra computation
performed to capture all solutions is negligible. Furthermore, the algorithm is implemented
to lexicographically minimise the misclassiﬁcations and the number of nodes.

To improve the performance in practice, the algorithm iterates through pairs of features
(fi, fj) such that i < j. After updating the current best left and right subtree feature using
fi as the root and fj as the child, the same computation is done using fj as the root and fi as
the child. Compared to the pseudo-code in Algorithm 4, this cuts the number of iterations
by half, but each iteration does twice as much work, which overall results in a speed-up in
practice. Moreover, rather than computing the best tree in a separate loop after computing
the best left and right subtrees for each feature, this is done on the ﬂy by keeping track of
the best subtree encountered so far during the algorithm.

Specialised algorithm for decision trees of depth three. We considered computing decision
trees with depth three using a similar idea. Even though this results in a better big-O
complexity for trees of depth three, albeit requiring O(F 3) memory, our preliminary results
did not indicate practical beneﬁts. Including additional low-level optimisation might improve
the results, but for the time being we leave this as an open question.

20

MurTree: Optimal Decision Trees via Dynamic Programming and Search

4.3.3 Incremental Computation

The specialised method for computing decision trees of depth two is repeatedly called in the
algorithm. For each call, the algorithm is given a diﬀerent dataset that is a result of applying
a split in one of the nodes in the tree. The key observation is that datasets which diﬀer only
in a small number of instances result in similar frequency counts. The idea is to exploit
this by only updating the necessary diﬀerence rather than recomputing the frequency counts
from scratch. Such a strategy resembles techniques used in the frequent itemset community
(Zaki and Gouda (2003)). We further elaborate on the idea used in our algorithm.

The key point is to view the previous dataset Dold and the new dataset Dnew in terms

of their intersection and diﬀerences.

Observation 1 Given two datasets Dnew and Dold, let their diﬀerence be denoted as Din =
Dnew \ Dold and Dout = Dold \ Dnew and their intersection as Dsame = Dnew ∩ Dold. We may
express the datasets as Dnew = Din ∪ Dsame and Dold = Dout ∪ Dsame

We ﬁrst note that set operations can be done eﬃciently for datasets.

Proposition 7 (Computational Complexity of Set Operations on Datasets) Given a dataset
D and two of its subsets Dnew ⊆ D and Dold ⊆ D, the sets Din = Dnew − Dold and Dout =
Dold − Dnew can be computed in O(|Dnew| + |Dold|) time using O(|D|) memory.

The above can be realised by associating each instance of the original dataset D with
a unique ID and storing positive and negative instances in their corresponding positive
and negative arrays sorted by ID. Once these conditions are met, a linear pass through
the datasets may determine the diﬀerences, and accordingly the frequency counts may be
updated incrementally.

Proposition 8 (Computational Complexity of Incremental Frequency Computation) Let m
denote the maximum number of features in any considered instance. Given the frequency
counts F Qold of a previous dataset Dold, a new dataset Dnew, and their diﬀerences Din and
Dout, the frequency counts F Qnew of the new dataset Dnew can be computed in O((|Din| +
|Dout|) · m2) time.

To show the complexity, note the diﬀerence between F Qold and F Qnew.

Observation 2 Let K(F Q) denote the set of instances used to compute the frequency counts
F Q. It follows that K(F Qold) = Dout ∪ Dsame and K(F Qnew) = Din ∪ Dsame.

Consider taking F Qold and applying a series of operations to reach the new frequency
counts F Qnew. The complexity result of Proposition 8 follows from the previous observations
and the following:

Observation 3 The frequency counts F Qold already capture the counts for instances Dsame

Observation 4 The frequency counts F Qold need to be incremented using instances Din

Observation 5 The frequency counts F Qold need to be decremented using instances Dout

21

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Using the incremental update procedure is sensible only if the number of updates required
is small compared to recomputing from scratch. In our algorithm, in each call to compute a
decision tree of depth two, the algorithm incurs an overhead (Proposition 7) to compute the
diﬀerences between the old and new dataset. It proceeds with the incremental computation
if |Din ∪ Dout| < |Dnew|, and otherwise computes from scratch.

and D2

Our algorithm keeps two sets of frequency counters, which are tied to two diﬀerent
. When our method is called, the algorithm uses as a starting point
datasets D1
the previous frequency counter that requires the least number of operations to incrementally
construct the new new frequency counter for the new dataset Dnew. Upon completing the
frequency counter computation, the new counter will replace the chosen old one.

old

old

The overhead of computing the number of operations required is negligible compared to
the overall complexity of computing the optimal tree of depth two (Proposition 6), but the
beneﬁts can be signiﬁcant if the diﬀerence is small. Assuming that two neighbouring features
are similar, two successive features considered for splitting are likely to lead to require only
a small number of modiﬁcations.

The previous paragraph motivates the choice of only storing two frequency counters.
When computing a tree of depth three, the data is split amongst the left and right subtree.
Whereas the data passed to the left and right subtree may be very diﬀerent from one another,
the data passed to the left subtree during the next split may not be substantially diﬀerent
from the data used in current split in the left subtree (similarly for the right subtree). Recall
that the specialised depth two algorithm would be called on the child subtree sequentially. In
order to preserve the frequency counters of both children in between two successive splits,
the heuristic choice was made to store two sets of frequency counters. As shown in the
experimental section, the incremental strategy provides notable runtime reductions.

4.4 Similarity-Based Lower Bounding

We present a novel lower bounding technique that does not rely on the algorithm having
previously searched a speciﬁc path, as opposed to the cache-based lower bound introduced in
the later section. Given a dataset Dnew for a node, the method aims to derive a lower bound
by taking into account a previously computed optimal decision tree using the dataset Dold.
It infers the bound by considering the diﬀerence in the number of instances between the
previous dataset Dold and the current dataset Dnew. The bound is used to prune portions of
the search space that are guaranteed to not contain a better solution than the best decision
tree encountered so far in the search. We note that our approach is related to the lower
bound for decision lists (Angelino et al. (2017)) and diﬀset computation (Zaki and Gouda
(2003)). We present the ideas as an application to decision trees using elementary algebra.
Assume that for both datasets, the depth and the number of allowed feature nodes
requirements are identical. As in the previous section, we deﬁne the sets Din = Dnew \ Dold,
Dout = Dold \ Dnew, and Dsame = Dnew ∩ Dold.

Given the limit on the depth d and number of features nodes n, a dataset Dnew, and a
dataset Dold with T (Dold, d, n) as the misclassiﬁcation score of the optimal decision tree of
Dold (recall Eq. 1), we deﬁne the similarity-based lower bound,

LB(Dnew, Dold, d, n) = T (Dold, d, n) − |Dout|,

(9)

22

MurTree: Optimal Decision Trees via Dynamic Programming and Search

which is a lower bound for the number of misclassiﬁcations of the optimal decision tree for
the dataset Dnew of a tree of depth d with n feature nodes. Formally:

Proposition 9 LB(Dnew, Dold, d, n) ≤ T (Dnew, d, n).

As a result, subtrees with a lower bound greater than its upper bound are immedi-
ately pruned, eﬀectively speeding up the search. To show that Proposition 9 is indeed a
lower bound, let T (D) = T (D, d, n), note that removing Dout from Dold may reduce the
misclassiﬁcation cost by at most |Dout|:

T (Dold) − T (Dold \ Dout) = T (Dold) − T (Dsame) ≤ |Dout|.

Adding instances to Dsame cannot decrease the misclassiﬁcation score T (Dsame):

T (Dold) − |Dout| ≤ T (Dsame).

Combining Eq. 11 and 12 we arrive at:

T (Dnew) = T (Dsame ∪ Din) ≥ T (Dsame)

T (Dold) − |Dout| ≤ T (Dnew)

LB(Dnew, Dold, d, n) ≤ T (Dnew)

(10)

(11)

(12)

(13)

(14)

which shows the derivation of Proposition 9.

As implied in the previous text, a set of previous datasets and their optimal values need
to be kept available for comparison once a new dataset is considered. This give rise to a
trade-oﬀ: keeping more datasets increases the probability of deriving a greater lower bound
at the expense of computational time for each lower bound computation.

Our algorithm stores two datasets for each depth value. When computing the bound
for a new dataset with depth d, the two datasets stored at depth d are used to compute
the similarity-based lower bound, and the stronger bound of two is taken. Once a subtree
has been exhaustively searched with depth d, its corresponding dataset replaces the most
similar dataset stored at depth d. Similarity between datasets is formally computed as
|Dout| + |Din|. The intuition is to ensure that given two successive splits at depth d, the
resulting left and right subtree datasets of the ﬁrst split will be used to compute the bound
of the dataset that come in the next split. This is similar reasoning as in the case of the
incremental computation in the specialised algorithm (see end of Section 4.3.3).

If the datasets Dold and Dnew are equal, then any result for dataset Dold can be directly
used for Dnew. As discussed in the next section, optimal solutions and lower bounds for
subtrees are stored in the cache. When computing a similarity-based lower bound for a new
dataset at depth d, if it is detected that it is equal to one of the two stored datasets at depth
d, then the optimal solution and lower bounds of the stored datasets are fully transferred
to the new dataset. Note that this situation may only occur when using a branch-based
caching (see next section).

As shown in the experimental results (Section 5.2.2), the use of the similarity-based lower

bound reduces the runtime for all datasets, with only a few exceptions.

23

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

4.5 Caching of Optimal Subtrees (Memoisation)

As is common in dynamic programming algorithms, a caching or memoisation table is main-
tained to avoid recomputing subproblems. In our case, information about computed optimal
subtrees is stored. This is used to retrieve a subtree that has already been computed, pro-
vide lower bounds, and reconstruct the optimal decision tree at the end of the algorithm.
Caching has been used in previous works (Nijssen and Fromont (2007, 2010); Aglin et al.
(2020a); Verhaeghe et al. (2019); Hu et al. (2019); Lin et al. (2020)).

We adapt and extend the caching techniques from the literature for our algorithm, i.e.,
our cache takes into account constraints both on depth and the number of nodes. We discuss
two caching techniques, namely branch and dataset caching, that have been introduced
in DL8 (Nijssen and Fromont (2007, 2010)) and (variants) have appeared in later works.
Whereas the diﬀerent techniques were viewed as a trade-oﬀ between computational time
and eﬃciency, we show that in our realisation both techniques take only a small fraction of
the total runtime, allowing us to use dataset caching without incurring notably drawbacks.
Formally, we deﬁne the cache as a mapping of a subtree (represented as a branch or
dataset, see below) to a set of cache entries. Each entry contains information on the lower
bound and/or the optimal root node of the subtree under constraints on the depth and
number of feature nodes, which includes the root feature, the number of feature nodes in its
left and right children, and the misclassiﬁcation score. Initially, the cache is empty and is
populated throughout the algorithm. As we employ a specialised algorithm for depth two
trees, we do not cache the lowest decision tree layer. This leads to fewer subtree entries in the
cache, saving space and increasing eﬃciency. We note that given our techniques described
below, the overhead of look-up a subtree in our cache is kept low.

A key concept is the hash function of a subtree. We discuss the branch and dataset

representations, corresponding hash functions, and our realisation of these ideas.

4.5.1 Subtree Hashing Based on Branches

The key observation is that given a path from the root to any given node, each permutation
of the feature nodes on the path results in the same dataset for the node furthest from the
root, e.g., D(fi)(fj) = D(fj)(fi). This allows representing a path as a set of features, e.g.,
{fi, fj}. The path of a subtree is called a branch. We reiterate that each subtree may be
associated with exactly one branch, while a single branch of length k may be linked to k!
subtrees. This is valuable since it implies that the computation of a single subtree may be
shared amongst each k! subtree associated with the same branch. We note that the branch
representation has been introduced in DL8 using the term itemset.

Our branch-based cache consists of an array of hash tables: each branch of length k is
stored in the k-th hash table. We stored the branch as a sorted array, where features are
converted into integers based on their index and polarity4, i.e., given a feature f with index
i, we assign the value 2i + 1 to the positive feature f and 2i to the negative feature f . We
use a conventional hash function5 on integer arrays within the hash tables, i.e., given an
array A of length n, its hash is computed using Algorithm 5.

4. Such a conversion is borrowed from the SAT solving literature.
5. E.g., see the function template hash_combine in the C++ Boost library.

24

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Algorithm 5: A standard hash function for an array of integers

input: Array of integers A = [a1, a2, ..., an]
output: An integer k representing the hash value of A

1 begin

2

3

4

5

k ← n
for i ∈ [1, n] do

k ← k ⊕ (ai + 0x9e3779b9 + 64k + k/4)

return k

The advantage of a branch representation is its compactness, i.e., each subtree is repre-
sented only using a few features, allowing eﬃcient hash function computation. The downside
is that diﬀerent branches may lead to the same dataset (subproblem), but this will not be
detected when using caching based on branches, leading to unnecessary recomputation.

4.5.2 Subtree Hashing Based on Datasets

An alternative to the branch representation is to use more general representations. This is
desirable since once a lower bound or optimal solution has been computed for a subtree,
the results may be shared amongst any future subtree that optimises exactly the same
subproblem rather than only sharing with subtrees with equivalent branches, alleviating the
drawback of the branch representation. The downside is that more general representations
may be computationally intensive.

DL8 (Nijssen and Fromont (2007, 2010)) proposed to compute the closure of a branch
(itemset): given a branch, its closure is the set of features that appear in all instances of the
dataset corresponding to the branch. The closure is then used as the subproblem represen-
tation. Note that the branch is a subset of its closure. Such an approach correctly identiﬁes
all equivalent subproblems, addressing the issue of the branch representation. The draw-
back is that computing the closure requires additional computation, providing an important
trade-oﬀ that must be considered. A related idea has been recently proposed by Lin et al.
(2020), where the subproblem is represented using a bitset, i.e., the i-th bit is set is the i-th
instance is present.

In our work, we introduce an alternative representation that uses the dataset as the
subproblem representation and discuss several techniques that keep the computational time
of caching low.

At the start of the algorithm, each instance is assigned a unique identiﬁer in the form of
an integer. A dataset contains an array for each class and instances within each array are
kept sorted with respect to their identiﬁer. Given such data structures, determining whether
two datasets D1 and D2 are identical may be done in linear time with respect to the number
of present instances, i.e., O(|D1|).

Our cache consists of an array of hash tables, where datasets with m instances are stored
in the m-th hash table. We use instance identiﬁers and Algorithm 5 to compute the hash
value of a dataset, where the dataset is treated as an array with instance identiﬁers as
integers. The hash is computed only once and stored in the dataset for further use.

In our experiments, the eﬀectiveness of dataset caching showed better performance than

branch caching and the additional memory requirements were not an issue.

25

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

For both caching ideas we introduced a further optimisation to speed-up in practice. For
each array of array of hash tables, the last two calls are stored in a temporary buﬀer. Before
searching in the hash table, the cache ﬁrst looks up the temporary buﬀer in case the item
sought for is already present. This was done since the same cache call may be performed
many times during the algorithm, e.g., after splitting the dataset, the algorithm select the
node allocation to the left and right child, but each cache call will lead to the same cache
entry.

4.5.3 Storing Subtrees and Lower Bounds in the Cache

Information is stored in cache once a subtree has been exhaustively explored. We consider
two scenarios:

Scenario #1: a decision tree has been found within the upper bound. In this situation, the
computed subtree is optimal and the corresponding entry is stored/updated using the root
node assignment as the optimal assignment, the lower bound is set to the misclassiﬁcation
score, noting the depth and feature node limit that was given when computing the subtree.
In the event that the algorithm determines that the minimum misclassiﬁcation score may
be achieved using fewer nodes than imposed by the node limit, we may use the following
proposition to create additional cache entries:

Proposition 10 Let T (D, d, n) be the misclassiﬁcation score of the optimal decision tree
for the dataset D with depth limit d and node limit n. If there exists an n(cid:48) < n such that
T (D, d, n(cid:48)) = T (D, d, n), then T (D, d, i) = T (D, d, n) for i ∈ [n(cid:48), n].

Similar reasoning is used to populate entries in case a smaller depth is used than allowed.
We note that during the algorithm, a given branch or dataset may be only exhaustively
explored once, depending on the subtree representation used in the cache. The next time
a branch or dataset is encountered, its corresponding solution is retrieved from the cache
(Section 4.5.4).

Scenario two: no decision tree has been found within the upper bound. It follows that the
lower bound on the number of misclassiﬁcations for the subtree is at least one greater than
the upper bound. This is the lower bounding reasoning introduced in DL8.5 (Aglin et al.
(2020a)).

In this work, we note a slightly stronger lower bound. Let LB(D, d, n) be the lower
bound for the number of misclassiﬁcations of an optimal decision tree for the dataset D
with n nodes and depth d, i.e., T (D, d, n) ≥ LB(D, d, n). We introduce the following reﬁned
lower bound RLB:

LBlocal(D, f, d, nlef t, nright) = LB(D(f , d − 1, nlef t)) + LB(D(f, d − 1, nright))

(15)

RLB(D, d, n) = min{LBlocal(D, f, d, nlef t, nright) | f ∈ F ∧ nlef t + nright = n − 1} (16)

The bound RLB considers all possible assignments of features and numbers of feature
nodes to the root and its children, and selects the minimum sum of the lower bounds of

26

MurTree: Optimal Decision Trees via Dynamic Programming and Search

its children. It follows that no decision tree may have a misclassiﬁcation score lower than
RLB. We combine RLB with the upper bound to obtain a lower bound for the case where
no decision tree with less than the speciﬁed upper bound U B could be found:

T (D, d, n) ≥ max{RLB(D, d, n), U B + 1}.

(17)

The proposed bound generalises the bound from DL8.5 (Aglin et al. (2020a)), which only
considers the second expression on the right-hand side of Eq. 17 to derive a lower bound
when no tree could be found within the given upper bound.

Once the lower bound has been computed, it is recorded in the cache for the subtree
along with the constraints on the depth and number of feature nodes, and the optimal
assignment is set to null.

4.5.4 Retrieving Subtrees and Lower Bounds from the Cache

When considering a new subtree, the algorithm searches for set of entries corresponding to
the subtree using hash tables, as discussed at the beginning of the section.

A lower bound for the current tree may be inferred from the bounds of the larger tree,

formally summarised in the following proposition.

Proposition 11 Given the dataset D and depth bound d and the maximum number of fea-
ture nodes n, a bound for a larger tree is a bound for the current tree, i.e., ∀n(cid:48) ≥ n, d(cid:48) ≥ d :
LB(D, d(cid:48), n(cid:48)) ≤ LB(D, d, n).

When retrieving a lower bound for trees that have no cache entries, Proposition 11 allows
inferring a lower bound from larger trees that may be stored in the cache. Note that the
lower bounds are nonincreasing with size, i.e.,

LB(D, d(cid:48), n(cid:48)) ≤ LB(D, d, n)

n(cid:48) ≥ n ∧ d(cid:48) ≥ d

(18)

The tightest bound is returned when retrieving the lower bound. If there are no appli-
cable entries in the cache, the trivial lower bound of zero is returned. For example, given
a dataset D(f1, f2) with the node limit set to ﬁve and depth three, if there is no subtree
for the given size in the cache but there is an entry when the node limit was set to six and
seven with the same depth, then the lower bound using six nodes is the tightest valid lower
bound available for the tree with ﬁve nodes.

4.5.5 Incremental Solving

We label the process of querying the algorithm to compute progressively smaller or larger
decision trees as incremental solving. For example, once the algorithm has computed an
optimal decision tree for a given depth and number of nodes, the user may be interested
in a tree with more or fewer nodes. Such situations also occur as part of hyper-parameter
tuning. Our cache naturally supports these types of queries. Computations used for a tree
with a given depth and node count may be reused when the algorithm is run with a diﬀerent
set of depth and node count values.

27

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

4.5.6 Recovering the Optimal Decision Tree

Recall that for each subtree optimally computed, only the root node is stored in the cache.
When necessary, the complete subtree may be reconstructed as a series of queries to the
cache, where each time a single node is retrieved, as introduced in DL8 (Nijssen and Fromont
(2007, 2010)). In our algorithm, there is an exception to the mentioned tree reconstruction
procedure. After solving a tree of depth two, the root node is stored, but not its children.
During the algorithm these are not necessary, but the children are needed when reconstruct-
ing the optimal decision tree found at the end. In this case, the required child nodes are
recomputed using Algorithm 4. This avoids storing an exponential number of entries (re-
call that the number of paths increases exponentially with the depth) which do not serve
a purpose other than the ﬁnal reconstruction. The resulting computational overhead of
recomputing the solutions at the end is negligible compared to the overall execution time.

4.6 Node Selection Strategy

Given a feature for a node and the size allocation for its children, the algorithm decides on
which child node to recurse on ﬁrst. In DL8.5, the algorithm always visits the left subtree
before the right subtree.

Our search strategy is a variant of such post-order traversal, labelled dynamic ordering,
which dynamically decides which child node to visit ﬁrst. The idea is to prioritise the child
node that has (heuristically) the higher number of misclassiﬁcations, which in turn leads to
a higher chance to prune to the other sibling. The potential is roughly approximated by
the number of misclassiﬁcations of its corresponding classiﬁcation node. In our experiments
such a strategy shows consistent improvement over a static post-order traversal used in
DL8.5 (Aglin et al. (2020a)). We considered a more complex approach that selected the
subtree with the larger gap between the upper and lower bound, however this did not leave
to improvements over the dynamic strategy ordering.

Ordering search nodes according to a heuristic is common in combinatorial optimisation,
e.g., variable selection in integer/constraint programming, and in the data mining literature,
e.g., Zaki (2000); Zimmermann and De Raedt (2009). The above idea represents such an
idea applied to optimal decision trees.

4.7 Feature Selection

For a given node, each possible tree conﬁguration (a feature and the size of its children) is
considered one at a time, unless the node is pruned or the optimal solution is retrieved from
the cache (see Subsection 4.5). The order in which tree conﬁgurations are explored may
have an impact on performance, as evidenced in search algorithms in general.

We considered three feature selection strategies from the literature, which order the
features according to the following: 1) Gini coeﬃcient of the features, 2) in order of feature
appearance in the dataset, and 3) randomly order features. As discussed in (Section 5.2.3),
in our experiments the inorder variant was selected as the default option.

28

MurTree: Optimal Decision Trees via Dynamic Programming and Search

4.8 Extensions

The core algorithm has been presented in the previous sections. We now discuss several
extensions that use the presented core algorithm as a basis.

4.8.1 Multi-Classification

To extend the algorithm for multi-classiﬁcation, the key step is to generalise Algorithm
4 to compute frequency counters for each class. Equations analogous to Equations 2—8
are devised to compute the misclassiﬁcation scores. Since classes partition the data, the
complexity results remain valid for multi-classiﬁcation.

4.8.2 Regression

As in multi-classiﬁcation, the main step in extending our method to work with regression is
to adapt the specialised algorithm for computing depth two trees (Algorithm 4). Consider
regression trees where leaf nodes are assigned ﬁxed values that are computed as the average
value of their corresponding training instances. The specialised algorithm for depth two
trees, in addition to the frequency counters, maintains a similar structure where the total
sum of target values of each pair of features is stored, and analogous equations to Equations
2—8 are used. Note that our similarity-based lower bounds, in their current form, would
not be applicable to regression.

4.8.3 Sparse Objective

Apart from minimising the misclassiﬁcation score, the sparse objective is a popular metric
for decision trees, which considers a weighted linear combination of the misclassiﬁcation
score and the number of feature nodes. This objective was used in the original CART paper
(Breiman et al. (1984)) and discussed in some of the other optimal decision tree works
(Bertsimas and Dunn (2017); Hu et al. (2019); Lin et al. (2020)). Formally, the sparse
objective is speciﬁed as follows:

misclassif ications + α × nodes,

(19)

balancing the size of the decision tree against the misclassiﬁcations using the sparse co-
eﬃcient α ∈ N0. The intuition is that adding a node to a decision tree should only be
considered if it leads to a reduction in the misclassiﬁcations by at least α. The depth may
also be penalised in a similar manner.

To support the sparse objective, we perform a sequence of queries to our algorithm,
each time modifying the number of nodes and imposing an upper bound according to the
new number of nodes considered, illustrated in Algorithm 6. Given our caching mechanism,
computations in between calls are reused, even if the sparse weight α is changed (see below).
Note that, as an emerging functionality, running Algorithm 6 with α = 0 considers all
possible trees given the upper values on the depth and number of nodes and stores the
computation in the cache. As a result, the optimal tree given any α may be computed
immediately, as all the necessary subtrees needed for the computation will be already stored
in the cache. This may be considered when tuning a tree for the best sparse coeﬃcient α. In
this case, it is important that the depth is small enough that the algorithm may terminate

29

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Algorithm 6: M urT ree.SolveSparseObjective(D, d, n, α), an algorithm to min-
imise the sparse objective (Eq.19)

input: Dataset D = D+ ∪ D−, depth d ∈ N0, number of feature nodes n ∈ N0, sparse

coeﬃcient α ∈ N0

output: Optimal decision tree within the input speciﬁcation that minimises the sparse

objective (Eq. 19) on dataset D

1 begin

2

3

4

5

6

7

8

Tbest ← Classif icationN ode(D)
for n(cid:48) ∈ [1, n] do

U B ← SparseObjective(Tbest) − (α · n(cid:48)) − 1
T ← M urT ree.SolveSubtree(D, ∅, d, n(cid:48), U B)
if T (cid:54)= ∅ then
Tbest ← T

return Tbest

within reasonable time, as the runtime exponentially increases with respect to the depth.
Otherwise, for larger depths, it may be beneﬁcial to consider particular α values rather
than tuning for all values, since setting the coeﬃcient to a positive value α > 0 contributes
towards reducing the search space, which may assist the search in the case of deeper trees.

4.8.4 Minimising Number of Nodes

We may consider a lexicographical objective, where the aim is to achieve the minimum
misclassiﬁcation score using the least amount of nodes. Note that our presented algorithm
is focussed on minimising misclassiﬁcations within the given constraints on the depth and
number of nodes, but does not necessarily return the smallest tree. To take into account
the lexicographical objective, we may ﬁrst compute the optimal tree, and then query the
algorithm to compute smaller trees using the misclassiﬁcation score as an upper bound.
Recall that computations from one call will be reused in other calls through the cache. This
is summarised in Algorithm 7.

Algorithm 7: M urT ree.SolveSubtreeLexicographically(D, d, n), an algorithm to
compute the tree with minimum misclassiﬁcations using the least amount of nodes

input: Dataset D = D+ ∪ D−, depth d ∈ N0, number of feature nodes n ∈ N0
output: Optimal decision tree that lexicographically minimise the misclassiﬁcation and

then the number of nodes on dataset D

1 begin

2

3

4

5

6

7

8

Tbest ← M urT ree.SolveSubtree(D, ∅, d, n, U B)
U B ← M isclassif ication(Tbest)
for n(cid:48) ∈ [n − 1, 0] do

T ← M urT ree.SolveSubtree(D, ∅, d, n(cid:48), U B)
if T (cid:54)= ∅ then
Tbest ← T

return Tbest

30

MurTree: Optimal Decision Trees via Dynamic Programming and Search

4.8.5 Anytime Behaviour

An anytime algorithm has the property that it may provide a solution at any time during
its execution. Our algorithm discussed in the previous sections has, in some sense, support
for anytime behaviour since a global solution is only registered at the root node. However,
the algorithm may be augmented to save a better solution as soon as it is found, rather
than delay until the root node recursion. The key observation is that any solution, even a
classiﬁcation node, is a solution to the decision tree problem. When processing one child
subtree, the other subtree may be assumed to be a classiﬁcation node for the purposes of
the anytime solution. A separate data structure may be maintained to keep track of the
current tree, incrementally update the misclassiﬁcation score, and update the global solution
should a better solution present itself during search. This incurs an additional overhead,
but it is negligible compared to the other operations, e.g., computing a tree of depth two.
We note that while anytime behaviour may be supported, our algorithm is not designed to
exhibit strong anytime behaviour, but rather minimise the time to exhaustively explore the
search space of all decision trees. Another technique to improve the anytime behaviour is to
consider an iterative deepening approach, where the optimal trees of depth k is computed
before proceeding with trees of depth k + 1 until the desired maximum depth is reached,
possibly using Gini feature selection.

4.8.6 Optimising Nonlinear Metrics

Our algorithm may be extended to support metrics which have a nonlinear relationship
between the misclassiﬁcations of each class, e.g., F1-score. The key idea is to generalise the
method to a bi-objective setting, where the objectives represent the number of misclassiﬁed
instances for each individual class. This allows computing the Pareto front and subsequently
computing the tree that minimises the given nonlinear metric. While this is desirable, the
task of minimises such metrics is signiﬁcantly more computationally expensive compared to
linear metrics such as the misclassiﬁcation score. We refer the interested reader to a separate
article (Demirović and Stuckey (2021)), where we explored this idea in more detail.

5. Computational Study

The goal of this section is to evaluate diﬀerent variants of our algorithm and compare with
the state-of-the-art. With this in mind, we designed three major themes to investigate,
each addressing a unique set of questions: 1) variations and scalability of our approach,
2) eﬀectiveness compared to the state-of-the-art optimal classiﬁcation tree algorithms, and
3) out-of-sample accuracy as compared to heuristically-obtained decision trees and random
forests.

5.1 Datasets and Computational Environment

We use publicly available datasets used in previous works (Bertsimas and Dunn (2017);
Verwer and Zhang (2019); Narodytska et al. (2018); Aglin et al. (2020a); Hu et al. (2019)),
most of which are available from the UCI and CP4IM repositories. The datasets include
85 classiﬁcation problems with a mixture of binary, categorical, and continuous features.
Datasets with missing values were excluded from experimentation. Some benchmarks ap-

31

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

peared in previous works under diﬀerent binarisation techniques or simpliﬁcations, e.g.,
multi-classiﬁcation turned into binary-classiﬁcation using a ‘one-versus-all’ scheme or a sub-
set of the features were removed. We include such variants as separate datasets.

Datasets with categorical and/or continuous features are converted into binary datasets
as a preprocessing step using a supervised discretisation algorithm based on the minimum
description length principle (MDLP) by Fayyad and Irani (1993), eﬀectively converting each
feature into a categorical feature based on the statistical signiﬁcance of the feature values
for the class, and then using a one-hot encoding to binarise the features. This was done
for 22 datasets, while the remaining 63 datasets were already binary. The implementation
of MDLP from the R programming package was used (Kim (2015)). While the chosen
discretisation strategy was suﬃcient for our evaluation purpose, we acknowledge that there
may be better ways of discretising features. We leave the analysis of discretisation strategies
for optimal decision trees for future work.

We partition the datasets into four groups based on the source of the dataset. For this

reason the names displayed do not follow an alphabetical order.

Our code, binarised datasets, and the binarisation script are available online: https:

//bitbucket.org/EmirD/murtree.

Experiments were run on an Intel i-7-8550U CPU with 32 GB of RAM running one
algorithm at a time using one processor. The timeout was set to ten minutes except for
the hyper-parameter tuning where no limit was enforced. In the following, we dedicate a
separate subsection to each of the three major experimental topics.

5.2 Variations of Our Algorithm and Scalability

The aim of this subsection is to investigate variations of our approach, namely:

1. Compare branching and dataset caching.

2. Assess the eﬃciency of incremental frequency and similarity lower bound computation.

3. Analyse the impact of the feature and node selection strategies.

4. Demonstrate the limits and scalability of our approach.

The default setting of our algorithm uses all techniques presented in the paper, i.e.,
incremental frequency computation, incremental solving, similarity lower bounding, in-order
feature selection, dynamic node selection, and the dataset-based cache.

5.2.1 Part One: Cache Variants

We run our algorithm varying the cache strategy (branch- and dataset-based caching: Sec-
tion 4.5). For each combination, we ﬁx the depth of the tree to four and task the algorithms
to compute ﬁfteen optimal decision trees, one tree for each value of n ∈ [1, 15]. Such a
computation task is common in hyper-parameter tuning or when optimising the sparse ob-
jective. The algorithms take advantage of incremental solving, i.e., subproblems stored in
the cache from previous trees are reused.

The results are given in Table 1, where the runtime and number of cache entries for each

setting are shown. Benchmarks where the diﬀerence was insigniﬁcant are excluded.

32

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Table 1: Comparing the eﬃciency of branch and dataset caching. For each dataset, the
number of instances (D), binary features (F), and number of classes (C) are displayed.
Datasets where the diﬀerence between the methods is insigniﬁcant are excluded. The time
represents the number of seconds to compute decision trees with n ∈ [1, 15] feature nodes
with maximum depth four (ﬁfteen trees in total). The number of cache entries is given in
thousands. Bold numbers highlight better runtime.

Branch Cache

Dataset Cache

Name
ionosphere
letter
pendigits
segment
splice-1
vehicle
Statlog_satellite
Statlog_shuttle
appendicitis
australian
backache
cleve
colic
heart-statlog
hepatitis
hungarian
new-throid
promoters

|D|
351
20000
7494
2310
3190
846
4435
43500
106
690
180
303
368
270
155
294
215
106

|F|
445
224
216
235
287
252
385
181
530
1163
475
395
415
381
361
330
334
334

|C| Time Cache Entries Time Cache Entries
2
2
2
2
2
2
6
7
2
2
2
2
2
2
2
2
3
2

250
417
153
11
111
31
507
114
12
1001
48
14
55
12
19
16
35
32

132
264
88
8
99
18
519
124
7
740
34
12
41
9
12
12
32
27

54
11
11
4
35
13
95
17
22
463
72
73
86
66
41
50
39
54

170
32
34
12
44
40
97
17
140
699
120
83
105
77
72
60
56
61

Based on the results, we conclude that the dataset cache leads to the best performance
in terms of runtime. We note that the diﬀerence in eﬃciency is also reﬂected in the number
of cache entries. Dataset caching requires fewer cache entries, indicating that the equiva-
lence between diﬀerence subproblems could be eﬀectively exploited. The exception are two
datasets, where the number of cache entries is similar and branch caching has a slight ad-
vantage. The reason for only a slight diﬀerence is that even though dataset caching requires
more computational time and memory compared to branch caching, the overall diﬀerence is
not signiﬁcant in our implementation compared to the other algorithmic components. We
note that we experimented with a cache based on the closure of a branch, but this incurred
a notable overhead for most instances compared to dataset- and branch-caching.

5.2.2 Part Two: Incremental Frequency Computation and the

Similarty-Based Lower Bound

We run our algorithm tuning on and oﬀ the incremental frequency computation (Section
4.3.3) and the similarity-based lower bound (Section 4.4). This gives rise to a total of four
settings. As before, the depth of the tree is ﬁxed to four and the algorithms compute ﬁfteen
optimal trees by varying the number of nodes from one to ﬁfteen.

The results are given in Table 2, where the runtime is shown for each setting. Benchmarks

where the diﬀerence was insigniﬁcant are excluded.

We draw two main conclusions. First, incremental computation is always beneﬁcial.
The splits of two neighbouring features often only diﬀer in a small number of instances, and
consequently performing minor changes to the previously computed frequency counters saves

33

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Table 2: Comparison to determine eﬀectiveness of the incremental frequency counter com-
putation (‘Inc’) and the similarity-based lower bound (‘SLB’). For each dataset, the number
of instances (D), number of binary features (F), and number of classes (C) are displayed.
Datasets where the eﬀect of similarity-based lower bounding is insigniﬁcant are excluded.
The time represents the number of seconds the algorithms require to compute decision trees
with n ∈ [1, 15] feature nodes with maximum depth four (ﬁfteen trees in total). Timeouts
(1800 seconds) denoted as —. Bold numbers highlight better runtime.

Name
letter
pendigits
segment
default_credit
magic04
Statlog_satellite
Statlog_shuttle
appendicitis
australian
backache
cleve
colic
heart-statlog
hepatitis
hungarian
new-throid
shuttleM

|D|
20000
7494
2310
30000
19020
4435
43500
106
690
180
303
368
270
155
294
215
14500

|F|
224
216
235
44
79
385
181
530
1163
475
395
415
381
361
330
334
691

|C|
2
2
2
4
2
6
7
2
2
2
2
2
2
2
2
3
2

noInc-noSLB inc-noSLB inc-SLB
269
96
9
9
9
516
128
7
788
37
14
43
10
13
15
35
416

810
185
49
7
8
1060
131
32
—
86
65
89
55
30
31
148
1062

237
86
15
7
6
756
88
28
—
77
58
73
49
26
28
143
805

time compared to recomputing from scratch. Second, the lower bound contribute towards
equal or lower runtimes in a most of the benchmarks. Overall, the experiments demonstrate
that it is typically beneﬁcial to include both incremental computation and lower bounding.

5.2.3 Part Three: Feature and Node Selection Strategies

We run our algorithm varying the feature selection strategy (Section 4.7: in order, random,
and sorted according to Gini coeﬃcients) and the node selection strategy (Section 4.6: post-
order and dynamic). As before, the depth of the tree is ﬁxed to four and the algorithms
compute ﬁfteen optimal trees by varying the number of nodes from one to ﬁfteen.

The results are given in Table 3, where the runtime is shown for each setting. Benchmarks

where the diﬀerence was insigniﬁcant are excluded.

We draw several main conclusions. First, random feature selection is never beneﬁcial.
This is partially due to its anti-synergy with our incremental frequency computation and
similarity lower bounding. Second, dynamic node selection is consistently better than a ﬁxed
post-order selection, albeit the performance gains are relatively small compared to the other
algorithmic components. Third, both Gini and in-order feature selection are competitive,
although in-order performs better on slightly more datasets while being much simpler.

5.2.4 Part Four: Scalability

We investigate the sensitivity of our MurTree algorithm with respect to the number of
instances and maximum depth. In Table 4, results are shown for a subset of representation
datasets when our algorithm is run to compute trees of depth ∈ [4, 5] on datasets where

34

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Table 3: Comparison of feature and node selection strategies. For each dataset, the number
of instances (D), number of binary features (F), and number of classes (C) are displayed.
The time represents the number of seconds the algorithms require to compute decision trees
with n ∈ [1, 15] feature nodes with maximum depth four (ﬁfteen trees in total). Datasets
solved under a second or when the diﬀerences between the variants was negligible have been
omitted.

Feature Selection
Node Selection
Name
ionosphere
letter
pendigits
segment
splice-1
vehicle
default_credit
magic04
Statlog_satellite
Statlog_shuttle
australian
backache
cleve
colic
heart-statlog
hepatitis
hungarian
promoters
shuttleM

|D|
351
20000
7494
2310
3190
846
30000
19020
4435
43500
690
180
303
368
270
155
294
106
14500

|F|
2
2
2
2
2
2
4
2
6
7
2
2
2
2
2
2
2
2
2

|C|
2
2
2
2
2
2
4
2
6
7
2
2
2
2
2
2
2
2
2

Gini

Random
Dynamic Dynamic

InOrder
PostOrder Dynamic

Time
154
310
100
16
94
23
9
11
458
140
445
24
10
44
8
12
13
21
466

Time
332
443
133
23
128
30
9
11
665
187
1155
40
17
60
14
18
17
31
468

Time
139
276
93
10
103
22
7
9
518
126
803
37
14
43
11
14
14
30
477

Time
132
264
88
8
99
18
7
9
519
124
740
34
12
41
9
12
12
27
383

instances are duplicated k ∈ [1, 2, 3, 4] times. Similarly as before, each run computes ﬁfteen
and thirty-one trees with varying size, depending on the depth. Depth three trees are
computed in negligible time and are excluded from further consideration.

The results indicate a linear dependency with the number of instances for the majority
of the datasets. As most of the computational time is spent in repeatedly solving optimal
subtrees of depth two (Section 4.3), the ﬁnding is consistent with the theoretical complexity
(Proposition 6). This is a notable improvement over generic optimisation approaches, such
as integer programming or SAT. The latter may exhibit an exponential runtime dependency
on the number of instances as new binary variables are introduced for each instance, and
typically do not consider datasets with more than a thousand instances.

In contrast to the number of instances, the depth has a large impact on the running time.
The number of possible decision trees grows exponentially as the depth increases, which is
reﬂected in the sharp increase of both the time and number of cache entries. For example,
our approach computes depth-three trees within seconds, but the runtimes go up notably
for depth four and ﬁve, e.g., the diﬀerences between depths is in the order of magnitude.
Our proposed techniques manage to delay the exponential blow up, but do not remove it.

The main conclusion of the above discussion is that the bottleneck of the approach is not
necessarily in the number of instances, but rather in the depth of the tree. Note that the
experiments are merely indicative. In practice, however, introducing more instances might
implicitly increase or decrease the number of binary features in the discretisation and have

35

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Table 4: Scalability of our MurTree approach as instances are duplicated two, three and four
times with varying depth. For each dataset, the number of instances (D), number of binary
features (F), and number of classes (C) are displayed. Results for a subset of all datasets
are shown for simplicity. The time represents the number of seconds the algorithms require
to compute decision trees with n ∈ [1, 15] for depth = 4 (ﬁfteen trees) and n ∈ [1, 31] for
depth = 5 (thirty-one trees). The number of cache entries is given in thousands.

Name
anneal
audiology
australian-credit
breast-wisconsin
diabetes
german-credit
heart-cleveland
hypothyroid
kr-vs-kp
mushroom
segment
yeast
biodeg
default_credit
HTRU_2
Ionosphere
appendicitis
hepatitis

|D|
812
216
653
683
768
1000
296
3247
3196
8124
2310
1484
1055
30000
17898
351
106
155

|F|
93
148
125
120
112
112
95
88
73
119
235
89
81
44
57
143
530
361

Time for depth=4

Time for depth=5

|C|
2
2
2
2
2
2
2
2
2
2
2
2
2
4
2
2
2
2

1|D|
1
1
2
1
2
3
1
2
1
5
8
1
2
7
3
3
7
12

2|D|
1
1
3
2
3
5
1
3
2
9
15
3
2
14
6
4
8
13

3|D|
1
1
4
2
4
7
1
5
3
13
21
3
4
21
8
5
9
14

4|D|
2
1
5
3
5
9
2
6
4
18
30
4
4
30
12
6
9
14

cache
2
3
6
3
5
10
3
2
1
5
4
3
8
3
4
15
22
41

1|D|
10
14
76
17
65
160
19
40
17
101
60
35
75
150
100
147
421
850

2|D|
15
20
95
26
91
232
25
60
29
234
105
61
104
463
209
181
447
916

3|D|
20
25
125
37
119
308
31
85
42
345
155
88
135
720
310
213
479
956

4|D|
27
30
160
47
146
381
36
106
54
478
202
108
161
1063
434
254
512
1075

cache
22
79
157
56
118
385
74
27
16
130
44
57
261
81
72
688
1497
3360

an eﬀect on shaping the structure of the dataset, both of which may impact positively or
negatively the running time.

Apart from the depth, another important factor is the number of binary features, which
additionally dictates the number of possible decision trees necessary to explore to ﬁnd the
optimal tree. As the ability of our techniques to prune and reduce computational time
depends on the structure of the dataset, it is diﬃcult to artiﬁcially increase the number
of features and show the dependency. For example, duplicating features would not lead to
conclusive statements on the impact of the number of features on runtime, as our lower
bounding mechanism would trivially prune these features. We instead refer to the computa-
tional complexity of our algorithm from Proposition 6 and the number of possible decision
trees as an indicative measure of the inﬂuence of the number of binary features and sparsity
of the feature vectors on the runtime.

5.2.5 Section Summary

The experimental results conﬁrmed the eﬃciency of our incremental frequency computation
and similarity-based lower bounding approach. Each of the techniques provides a reduction
in terms of runtime. We show that dataset-based caching exhibits equal or better perfor-
mance than branch-based caching across the datasets. Our approach scales approximately
linearly with respect to the number of instances, and the depth of the tree has a large in-
ﬂuence on the runtime, i.e., decision trees of depth three and four are typically computed
within seconds or minutes, respectively, but trees of depth ﬁve are notably more challenging
depending on the dataset. Increasing the number of binary features increases the expected

36

MurTree: Optimal Decision Trees via Dynamic Programming and Search

runtime, but this is diﬃcult to measure as it depends on the eﬀectiveness of the pruning
techniques for the dataset at hand. We found that inspecting features in the order as given
in the dataset was more eﬀective than ordering features according to their corresponding
Gini coeﬃcients, possibly due to the in order feature selection synergies well with incremen-
tal frequency and similarity lower bound computation, but the diﬀerence largely depends on
the dataset. Lastly, our dynamic node selection strategy oﬀered consistent improvements
over a static strategy.

5.3 Comparison Against State-Of-The-Art Optimal Decision Tree Algorithms

Amongst the optimal decision tree methods discussed in Section 3, we consider DL8.5 by
Aglin et al. (2020a) as the main competing method. The rationale is that DL8.5 has been
shown to largely outperform the other techniques based on generic optimisation modelling,
such as integer programming (Verwer and Zhang (2019); Bertsimas and Dunn (2017)) and
constraint programming (Verhaeghe et al. (2019)), when minimising the misclassiﬁcation
score for full binary trees. We now discuss other approaches.

The SAT method by Narodytska et al. (2018) takes a diﬀerent approach: rather than
directly minimising the misclassiﬁcations given a ﬁxed depth, it attempts to construct the
smallest decision in terms of the total number of nodes that perfectly ﬁts the data, i.e., trees
that have a misclassiﬁcation score of zero. As ﬁnding the zero-misclassiﬁcation tree using
the complete dataset was computationally infeasible for SAT, and also prone to overﬁtting,
the authors proposed to subsample datasets by selecting 5-20% of the instances. While
this setting has its merits, it diverges from the goals of our paper. Furthermore, we found
that our algorithm computes the perfect decision tree within seconds on exactly the same
subsampled data used in the SAT paper and as can be seen in tables, we can directly optimise
with the complete datasets.

Other SAT works (Avellaneda (2020); Janota and Morgado (2020); Schidler and Szeider
(2021)) use either the discussed SAT method or BinOpt (Verwer and Zhang (2019)) as a
ground for comparison, but these have been shown to be outperformed other recent works,
e.g., DL8.5, which we further improve upon. The same reasoning holds when comparing to
other generic (optimisation) frameworks such as integer programming. The reason for the
discrepancy in runtime between our approach and SAT is that we provide a highly specialised
procedure that exploits classiﬁcation tree properties, e.g., Properties 1 and 2. One could
argue that declarative approaches are easily extendable with new constraints beyond those
considered here, which may be of interest, but we do not make use of such functionality. For
these reasons, we perform no further comparison with these methods.

Hu et al. (2019) (OSDT) and Lin et al. (2020) (GOSDT) introduce exhaustive search
algorithms optimising the linear combination of the misclassiﬁcation score and number of
nodes in tree (Eq. 19. The pruning mechanism of both works is based on the sparsity
coeﬃcient α from Eq. 19, i.e., the lower bound for each new introduce node is at least α.
As such, the sparsity coeﬃcient α plays a key role in the algorithm: the larger the α, the
faster the algorithms may exhaustively explore the search space. Recall that our approach
optimises the sparse objective using Algorithm 6. We experimented with both approaches,
and rather than providing detailed tables, we summarise our ﬁndings.

37

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

A direct comparison can be made with OSDT using a ten minute timeout with depth
four trees. We found that our algorithm computes optimal trees with the speciﬁed objective
within seconds for the benchmarks used by Hu et al. (2019), whereas their method may
require minutes or timeout. On the majority of our benchmarks, the approach by Hu et al.
(2019) timeouts, unless the sparsity coeﬃcient is set to be suﬃciently high. As presented, our
approach may handle any sparsity coeﬃcient (previous tables may be interpreted as using
α = 0) within the time limit for trees with maximum depth four. Note that optimising with
larger sparsity coeﬃcient values reduces the runtime due to pruning.

The comparison with GOSDT is slightly diﬀerent, since the problem deﬁnition is not the
same. In GOSDT, the algorithm does not directly support limiting the depth or number of
nodes, but instead the structure of the tree is controlled through the objective function and
the sparsity coeﬃcient. To facilitate a fair comparison, we ﬁrst ran GOSDT on 68 datasets
(all but the ’reduced’ benchmarks) to observe the results. We used a time limit of ten
minutes and set the sparsity coeﬃcient to the default value6 of 0.01 · |D|. We observed that
GOSDT timed out on 65% of the 68 datasets. Even though the depth is not limited, on 90%
the (possibly suboptimal) computed tree was of depth at most four, the maximum depth
was seven, and all trees had a small number of nodes. These results are expected since the
goal of the authors of GOSDT was to produce small trees. That said, our MurTree approach
can produce optimal small trees (e.g., depth ∈ {1, 2, 3, 4} and any number of nodes) within
seconds or minutes even when the sparsity coeﬃcient is set to zero. Note that after running
our algorithm using value zero for the sparsity coeﬃcient, the cache will be populated, and
then a tree for any sparsity value may be extracted immediately. Intuitively, the zero-case
coeﬃcient is the worst case, and optimising with greater values of the sparsity coeﬃcient is
beneﬁcial as it oﬀers pruning.

We now proceed with the main comparison with DL8.5.

5.3.1 Comparison with DL8.5 by Aglin et al. (2020a)

The aim is to assess the eﬀectiveness of our MurTree approach with respect to DL8.5, the
state-of-the-art method for optimal decision trees. We evaluate the runtime of both methods
to exhaustively explore the search space: a lower runtime indicates a more eﬀective approach.
DL8.5 optimises the misclassiﬁcation score given a constraint on the depth of the tree.
The number of feature nodes cannot be limited, meaning that full binary trees are considered,
i.e., eﬀectively the number of feature nodes is set to the maximum value given the depth. We
experiment with maximum depths of four and ﬁve. To ensure a fair comparison, i.e., have
both algorithms solve exactly the same problem, we set the maximum number of feature
nodes for our method to 15 for depth four and 31 for depth ﬁve trees. The complete dataset
is given to both algorithms without dividing into the training and test set. Ten minutes is
allocated for each dataset.

Although it is standard practice in machine learning to compare learning algorithms on
out-of-sample accuracy, in this case runtime is more appropriate for evaluating the meth-
ods since both algorithms are optimising the same objective. The out-of-sample accuracy
evaluation of optimal decision trees is reserved for the next section. Note that since the

6. To avoid confusion, we note that our deﬁnition of the sparse objective is based on misclassiﬁcations,

whereas in GOSDT it is based on accuracy. Both deﬁnitions are equivalent.

38

MurTree: Optimal Decision Trees via Dynamic Programming and Search

algorithms discriminate trees solely based on the objective, the resulting trees obtained by
both methods, assuming neither method timed out, will have the same objective value but
may diﬀer in their structure and features selected.

The runtimes, given in Table 5, show that our method is orders of magnitude faster than
DL8.5. This is a signiﬁcant result, as DL8.5 has been previously shown to outperform other
techniques for optimal classiﬁcation trees based on integer and constraint programming by a
large margin. Our results illustrate the advantage of designing and specialising decision tree
optimisation algorithms compared to using oﬀ-the-shelf tools. Both DL8.5 and our MurTree
approach exploit the dynamic programming structure of decision trees, but our method
employs additional techniques to further take advantage of the properties of decision trees.
The reduced runtime contributes greatly towards the application of optimal classiﬁcation
tree methods in practice, especially when tuning is involved (see next section).

5.4 Comparison Against Conventional Algorithms on Out-Of-Sample Accuracy

In this section, we analyse the suitability of our optimal decision trees as out-of-sample
classiﬁers. The aim is to demonstrate that more accurate trees of limited size lead to better
generalisations than what is oﬀered by heuristic approaches. Note that the restricted size of
optimal decision trees plays the role of a regulariser to avoid overﬁtting. The main compari-
son is done against an optimised implementation of CART (Breiman et al. (1984)), a widely
used decision tree learning algorithm. For illustrative purposes, we also make a comparison
with random forests, as a related method that typically improves accuracy over standard
decision tree algorithms at the expense of being less interpretable. As will be discussed, our
experiments further conﬁrm similar empirical ﬁndings (Verwer and Zhang (2019); Bertsimas
and Dunn (2017)). The experiments were run on exactly the same (binarised) benchmarks
for all methods. We use the algorithms provided by the sklearn (Pedregosa et al. (2011))
Python package for machine learning for the other methods.

5.4.1 Hyper-Parameter Tuning

Selecting a good set of parameters is important when evaluating the performance of machine
learning models. Hyper-parameter tuning is performed for all methods using grid search.
Given a model, we compute the average train and test accuracy using stratiﬁed ﬁve-fold
cross-validation for each combination of parameters. The set of parameters that leads to
the best test accuracy is selected. Note that the model is trained on the training sets,
but evaluated on the test sets. The runtime presented includes the time taken to perform
cross-fold validation using all parameters and the time to train a new decision using the
selected parameters on the complete dataset. All methods and parameter conﬁgurations
used exactly the same folds.

5.4.2 Comparison Against Heuristic Decision Trees (CART)

We considered three tuning settings for our MurTree method to analyse the eﬀect of re-
stricting tuning options. The three settings are as follows:

1. MT-F: Only a single parameter conﬁguration is set based on a heuristically obtained
tree. The parameter values are ﬁxed to match those produced by the decision tree com-

39

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Table 5: Comparison of DL8.5 (Aglin et al. (2020a)) and our approach, MurTree. For
each dataset, the number of instances (D), number of binary features (F), and number of
classes (C) are displayed. Entries display runtime in seconds (rounded to nearest integer) to
compute the optimal classiﬁcation tree of depth four and ﬁve. Time limit set to ten minutes.
Timeouts denoted as —

Name

D

F C

Depth=4

Depth=5

anneal
audiology
australian-credit
breast-wisconsin
diabetes
german-credit
heart-cleveland
hepatitis
hypothyroid
ionosphere
kr-vs-kp
letter
lymph
mushroom
pendigits
primary-tumor
segment
soybean
splice-1
tic-tac-toe
vehicle
vote
yeast
ﬁco-binary
bank_conv
biodeg
default_credit
HTRU_2
Ionosphere
magic04
messidor
spambase
Statlog_satellite
Statlog_shuttle
appendicitis
australian
backache
cancer
cleve
colic
haberman
heart-statlog
hepatitis
hungarian
new-throid
promoters
shuttleM

812
216
653
683
768
1000
296
137
3247
351
3196
20000
148
8124
7494
336
2310
630
3190
958
846
435
1484
10459
4521
1055
30000
17898
351
19020
1151
4601
4435
43500
106
690
180
683
303
368
306
270
155
294
215
106
14500

93
148
125
120
112
112
95
68
88
445
73
224
68
119
216
31
235
50
287
27
252
48
89
17
26
81
44
57
143
79
24
132
385
181
530
1163
475
89
395
415
92
381
361
330
334
334
691

DL8.5 MurTree DL8.5 MurTree
4
< 0.5
46
2
83
86
7
< 0.5
38
194
16
—
< 0.5
< 0.5
464
< 0.5
< 0.5
< 0.5
—
< 0.5
277
< 0.5
54
1
1
22
68
30
2
106
< 0.5
268
—
—
422
—
176
5
500
—
4
383
119
194
—
1
—

—
< 0.5
< 0.5 < 0.5
—
—
—
—
—
35
—
—
—
—
7
23
—
11
1
36
—
7
—
26
—
6
15
—
—
—
316
—
5
—
—
—
—
—
—
301
—
—
293
—
—
—
—
—
—

55
2
99
2
383
2
188
2
421
2
326
2
108
2
13
2
104
2
—
2
49
2
—
2
8
2
26
2
—
2
1
2
1
2
3
2
—
2
1
2
—
2
4
2
186
2
1
2
2
2
67
2
155
4
64
2
126
2
2
244
2 < 0.5
—
2
—
6
—
7
—
2
—
2
—
2
16
2
—
2
—
2
14
2
—
2
—
2
—
2
—
3
—
2
—
2

2
1
2
2
< 0.5
< 0.5
2
89
1
296
< 0.5
1
76
< 0.5
< 0.5
< 0.5
133
< 0.5
11
< 0.5
2
< 0.5
< 0.5
1
4
2
1
4
< 0.5
8
320
25
7
386
8
< 0.5
4
17
< 0.5
6
4
4
21
1
42

40

MurTree: Optimal Decision Trees via Dynamic Programming and Search

puted using CART. Note that, strictly speaking, this is not a hyper-tuning approach,
but nevertheless gives insight on the generalisability of optimal decision trees.

2. MT-R: The heuristically obtained decision tree provides an upper bound on the al-
lowed parameter values for tuning,
i.e., given a tree constructed by CART with
depth d and number of feature nodes s, tuning is done with depth ∈ 1, ..., d and
f eature node count ∈ {depth, ..., s}.

3. MT-A: Fully exploit available parameters of our algorithm until depth four,
depth ∈ {1, 2, 3, 4} and num_f eature_nodes ∈ {depth, depth + 1, ..., 2depth − 1}.

i.e.,

The aggregated results on 84 datasets are shown in Figure 2 for the three settings com-

pared to CART, which was tuned using depth ∈ [1, 2, 3, 4].

When considering the MT-F parameter selection strategy, the results are roughly com-
parable to the outcome produced by CART, even though optimal decision trees have been
training accuracy. The mismatch between better training and lack of consistent performance
on the test set indicates that the structure of the tree produced by CART may be suboptimal
for the considered dataset.

The performance is notably diﬀerent when allowing more freedom in parameter selection.
The MT-R parameter selection strategy produces better results for most datasets, while
taking into account all parameter options (MT-A) consistently demonstrates greater out-of-
sample accuracy across the datasets.

The runtime of our MurTree approach is reasonably short for most benchmarks. How-
ever, as expected, CART is much faster, i.e., the runtime was only a fraction of a second for
almost all benchmarks. Nevertheless, the runtime diﬀerence between the methods may be
acceptable for a vast number of application, especially when training time is not a concern.
Overall, we conclude that the trees produced by our MurTree algorithm provide bet-
ter generalisation compared to trees obtained using CART, a classiﬁcation decision tree
algorithm, at the expense of greater runtime.

5.4.3 Comparison Against Random Forests

We show the diﬀerence in both training and test accuracy in Figure 3.

For completeness we show a comparison with tuned random forests using the same sklearn
Python package. A forest of trees is typically more accurate than a single decision tree,
but the resulting model is less concise and more diﬃcult for human interpretation. The
forests were tuned by varying the number of trees in the forest from [10, 50, 100], selecting
the maximum depth from [no_limit, 1, 2, 4], and considering a subset of the features at each
2 |F|, (cid:112)|F|, log2(|F|)], where F is the set of features.
step to evaluate with respect to [|F|, 1
It is evident
that random forests have the advantage on the training set, which translates to the test set.
Nevertheless, for roughly half of the datasets optimal trees of depth four achieve comparable
performance in terms of accuracy. This demonstrates that for some applications optimal
decision trees may be preferred over heuristically trained random forests. The runtime of
random forest is not negligible but still reasonable: majority of the datasets fall into the
10-19 seconds range. We note that a diﬀerent tuning strategy for random forests may lead
to lower/higher runtimes.

41

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

e
c
n
e
r
e
ﬀ
i
d

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

e
c
n
e
r
e
ﬀ
i
d

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

e
c
n
e
r
e
ﬀ
i
d

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

26

71

e
m

i
t
n
u
r

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

12

11

10

5

6

5

2

2

1

1

-2

-3
8
Diﬀerence in accuracy in percentage on the test set

-1

3

0

7

1

2

4

10

0-4

2

5-9

6

1

3

1

10-59

180-299

300-1799

1800+

Runtime ranges in seconds

MT-F: Depth and number of nodes ﬁxed to the values of the CART tree

32

70

e
m

i
t
n
u
r

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

1

10

3

5-9

0-4

4

2

4

1

10-59

60-179

300-1799

1800+

Runtime ranges in seconds

14

12

9

6

3

2

1

1

0

-1
8
3
Diﬀerence in accuracy in percentage on the test set

2

6

7

1

4

1

-5

1

-2

MT-R: The CART tree provides the upper bound on the depth and number of nodes

27

53

17

14

7

6

3

1

1

1

4

-1

e
m

i
t
n
u
r

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

1

19

12

6

6

5

2

0-4

5-9

10-59

60-179

300-1799

1800+

1

0
8
4
Diﬀerence in accuracy in percentage on the test set

2

5

6

7

3

42
MT-A: full tuning, i.e.,depth ∈ {1, 2, 3, 4} and num_nodes ∈ {1, 2, ..., 15}

Runtime ranges in seconds

Figure 2: Performance comparison of our MurTree approach against CART on 84 datasets
using diﬀerence tuning strategies for the depth and number of nodes of the optimal tree.

MurTree: Optimal Decision Trees via Dynamic Programming and Search

26

32

e
c
n
e
r
e
ﬀ
i
d

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

e
c
n
e
r
e
ﬀ
i
d

n
e
v
i
g

h
t
i
w
s
t
e
s
a
t
a
d

f
o

r
e
b
m
u
N

13

9

6

4

4

3

3

1

1

1

1

1

1

1

1

1

2

2

2

2

3

2

1

1

1

13

11

8

5

-26 -24 -23 -19 -16 -14 -13 -12 -11 -10 -8 -7 -6 -5 -4 -3 -2 -1

0

1

-17

-14

Diﬀerence in accuracy in percentage on the training set

-7

-9
1
-3
Diﬀerence in accuracy in percentage on the test set

-4

-1

-2

-5

0

3

2

1

4

Figure 3: Accuracy comparison of our MurTree (MT-A) approach against random forests
on 84 datasets on both the training and test set.

6. Conclusion

We presented MurTree, an algorithm for computing optimal decision trees, i.e., decision
trees that achieve the best representation of the data in terms of the misclassiﬁcation score.
The algorithm is based on dynamic programming and search. Our novel techniques exploit
decision tree properties to provide orders of magnitude speed-ups when compared to the
state-of-the-art. The conducted experimental study shows that optimal decision trees are
desirable as their out-of-sample accuracy is greater than decision trees obtained using a
conventional learning algorithm (CART), while providing concise and interpretable models
within reasonable time for the majority of the benchmarks.

There are several limitations of our algorithm, some of which are shared with other
optimal decision tree algorithms. The depth of the trees is kept relatively low, e.g., depth
four. A low depth is convenient for interpretability, but for some applications, deeper trees
may be necessary, e.g., compactly representing controllers using perfect trees (Ashok et al.
(2020)). We observed that for half of the datasets considered, optimal decision trees provide
comparable performance in terms of out-of-sample accuracy when compared to the more
complex model of random forests, but for the other half of datasets, random forests had
better generalisation. In the setting we consider, the predicates are required to be provided in
advance, i.e., the dataset must be binarised. Given that the algorithm is unlikely to support
tens of thousands of predicates in the current form, a trade-oﬀ must be made between
the runtime and number of predicates when using datasets with continuous or categorical
features. Even though our algorithm provides signiﬁcant speed-ups, traditional heuristic
methods remain much faster. Nevertheless, for most tested datasets our approach produced
optimal trees within seconds or minutes, which may be acceptable for oﬄine applications.

There are several directions for future work. Considering novel metrics to improve the
ability to generalise better on unseen data may be one such direction, or understand which
optimal tree to select out of a set of trees with minimum misclassiﬁcation scores. Analysing

43

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

the eﬀect of supervised discretisation algorithms for binarising the datasets may lead to
additional insights. Furthermore, constructing forests of optimal trees is another research
direction worth considering.

Acknowledgments

We would like to acknowledge the comments of the editor and the anonymous reviewers.
Their commitment to the reviewing process has considerably contributed towards clarity,
accessibility, and correctness of the paper. An anonymous reviewer motivated us to explore
caching based on datasets which led to improvements. Part of this work was done while
Anna Lukina was visiting the Simons Institute for the Theory of Computing.

References

Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair
decision trees for non-discriminative decision-making. In Proceedings of AAAI, 2019.

Sina Aghaei, Andres Gomez, and Phebe Vayanos. Learning optimal classiﬁcation trees:

Strong max-ﬂow formulations. arXiv preprint arXiv:2002.09142, 2020.

Gaël Aglin, Siegfried Nijssen, and Pierre Schaus. Learning optimal decision trees using

caching branch-and-bound search. In Proceedings of AAAI, 2020a.

Gaël Aglin, Siegfried Nijssen, Pierre Schaus, and UCLouvain ICTEAM. Pydl8. 5: a library

for learning optimal decision trees. In Proceedings of IJCAI, 2020b.

Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.
Learning certiﬁably optimal rule lists for categorical data. The Journal of Machine Learn-
ing Research, 2017.

Pranav Ashok, Mathias Jackermeier, Pushpak Jagtap, Jan Křetínsk`y, Maximilian
Weininger, and Majid Zamani. dtcontrol: decision tree learning algorithms for controller
representation. In Proceedings of the International Conference on Hybrid Systems: Com-
putation and Control, 2020.

Florent Avellaneda. Eﬃcient inference of optimal decision trees. In Proceedings of AAAI,

2020.

Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriﬁable reinforcement learning

via policy extraction. In Proceedings of NeurIPS, 2018.

Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7),

2017.

Dimitris Bertsimas and Romy Shioda. Classiﬁcation and regression via integer optimization.

Operations Research, 55(2), 2007.

Christian Bessiere, Emmanuel Hebrard, and Barry O’Sullivan. Minimising decision tree size

as combinatorial optimisation. In Proceedings of CP, 2009.

44

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Guy Blanc, Jane Lange, and Li-Yang Tan. Provable guarantees for decision tree induction:

the agnostic setting. Proceedings of ICML, 2020.

Rafael Blanquero, Emilio Carrizosa, Cristina Molero-Río, and Dolores Romero Morales.
Sparsity in optimal randomized classiﬁcation trees. European Journal of Operational Re-
search, 2020.

Leo Breiman, JH Friedman, RA Olshen, and CJ Stone. Classiﬁcation and regression trees.

Cole Statistics/Probability Series, 1984.

Emilio Carrizosa, Cristina Molero-Río, and Dolores Romero Morales. Mathematical opti-

mization in classiﬁcation and regression trees, 2021.

Emir Demirović and Peter Stuckey. Optimal decision trees for nonlinear metrics. In Pro-

ceedings of AAAI, 2021.

Adam N Elmachtoub, Jason Cheuk Nam Liang, and Ryan McNellis. Decision trees for
decision-making under the predict-then-optimize framework. Proceedings of ICML, 2020.

Usama M. Fayyad and Keki B. Irani. Multi-interval discretization of continuous-valued

attributes for classiﬁcation learning. In Proceedings of IJCAI, 1993.

Michael R Garey. Optimal binary identiﬁcation procedures. SIAM Journal on Applied

Mathematics, 23(2), 1972.

Thomas M Hehn, Julian FP Kooij, and Fred A Hamprecht. End-to-end learning of decision

trees and forests. International Journal of Computer Vision, 2019.

Hao Hu, Mohamed Siala, Emmanuel Hebrard, and Marie-José Huguet. Learning optimal
decision trees with maxsat and its integration in adaboost. In Proceedings of IJCAI, 2020.

Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. In Proceedings

of NeurIPS, 2019.

Laurent Hyaﬁl and Ronald L Rivest. Constructing optimal binary decision trees is NP-

complete. Information Processing Letters, 5(1), 1976.

Mikoláš Janota and António Morgado. Sat-based encodings for optimal decision trees with

explicit paths. In Proceedings of SAT, 2020.

Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree

learning. In IEEE International Conference on Data Mining, 2010.

HyunJi Kim. Package ‘discretization’ in cran-r. https://CRAN.R-project.org/package=

discretization, 2015. [Online; accessed 21-May-2020].

Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep
neural decision forests. In Proceedings of the IEEE International Conference on Computer
Vision, 2015.

45

Demirović, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey

Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and

scalable optimal sparse decision trees. In Proceedings of ICML, 2020.

Nina Narodytska, Alexey Ignatiev, Filipe Pereira, and Joao Marques-Silva. Learning optimal

decision trees with SAT. In Proceedings of IJCAI, 2018.

Siegfried Nijssen and Elisa Fromont. Mining optimal decision trees from itemset lattices. In

Proceedings of SIGKDD, 2007.

Siegfried Nijssen and Elisa Fromont. Optimal constraint-based decision tree induction from

itemset lattices. Data Mining and Knowledge Discovery, 21(1), 2010.

Sebastian Ordyniak and Stefan Szeider. Parameterized complexity of small decision tree

learning. In Proceedings of AAAI, 2021.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12, 2011.

Ross Quinlan. C4.5: Programs for Machine Learning. Kaufmann, 1993.

André Schidler and Stefan Szeider. Sat-based decision tree learning for large data sets. In

Proceedings of AAAI, 2021.

Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori.

Adaptive neural trees. In Proceedings of ICML, 2019.

Hélene Verhaeghe, Siegfried Nijssen, Gilles Pesant, Claude-Guy Quimper, and Pierre Schaus.
Learning optimal decision trees using constraint programming. In Proceedings of CP, 2019.

Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible constraints and

objectives using integer optimization. In Proceedings of CPAIOR, 2017.

Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary linear

program formulation. In Proceedings of AAAI, 2019.

Thibaut Vidal and Maximilian Schiﬀer. Born-again tree ensembles. In Proceedings of ICML,

2020.

Bin-Bin Yang, Song-Qing Shen, and Wei Gao. Weighted oblique decision trees. In Proceed-

ings of the AAAI, 2019.

Mohammed J Zaki and Karam Gouda. Fast vertical mining using diﬀsets. In Proceedings

of SIGKDD, 2003.

Mohammed Javeed Zaki. Scalable algorithms for association mining. IEEE Transactions on

Knowledge and Data Engineering, 12(3), 2000.

Haoran Zhu, Pavankumar Murali, Dzung T Phan, Lam M Nguyen, and Jayant R
Kalagnanam. A scalable mip-based method for learning optimal multivariate decision
trees. In Proceedings of NeurIPS, 2020.

46

MurTree: Optimal Decision Trees via Dynamic Programming and Search

Albrecht Zimmermann and Luc De Raedt. Cluster-grouping: from subgroup discovery to

clustering. Machine Learning, 77(1), 2009.

47

