2
2
0
2

r
a

M
1

]
E
S
.
s
c
[

2
v
1
5
1
4
1
.
2
1
1
2
:
v
i
X
r
a

Cerebro: Static Subsuming Mutant Selection

1

Cerebro: Static Subsuming Mutant Selection

Aayush Garg, Milos Ojdanic, Renzo Degiovanni,
Thierry Titcheu Chekam, Mike Papadakis and Yves Le Traon

Abstract—Mutation testing research has indicated that a major part of its application cost is due to the large number of low utility mutants
that it introduces. Although previous research has identiﬁed this issue, no previous study has proposed any effective solution to the
problem. Thus, it remains unclear how to mutate and test a given piece of code in a best effort way, i.e., achieving a good trade-off
between invested effort and test effectiveness. To achieve this, we propose Cerebro, a machine learning approach that statically selects
subsuming mutants, i.e., the set of mutants that resides on the top of the subsumption hierarchy, based on the mutants’ surrounding code
context. We evaluate Cerebro using 48 and 10 programs written in C and Java, respectively, and demonstrate that it preserves the
mutation testing beneﬁts while limiting application cost, i.e., reduces all cost application factors such as equivalent mutants, mutant
executions, and the mutants requiring analysis. We demonstrate that Cerebro has strong inter-project prediction ability, which is
signiﬁcantly higher than two baseline methods, i.e., supervised learning on features proposed by state-of-the-art, and random mutant
selection. More importantly, our results show that Cerebro’s selected mutants lead to strong tests that are respectively capable of killing 2
times higher than the number of subsuming mutants killed by the baselines when selecting the same number of mutants. At the same
time, Cerebro reduces the cost-related factors, as it selects, on average, 68% fewer equivalent mutants, while requiring 90% fewer test
executions than the baselines.

Index Terms—mutant, mutation, mutation testing, subsuming mutant, mutant prediction, static selection, static mutant selection, static
subsuming mutant selection, static subsuming mutant prediction, encoder-decoder, machine translation, tf-seq2seq

(cid:70)

1 INTRODUCTION

Research and practice with mutation testing has shown
that it can effectively guide developers in improving their
test suite strengths [3], [14], and can be used to reliably
compare test techniques [5], [51]. A key issue though, is that
it is expensive, as a large number of mutants are involved,
the majority of which are of low utility, i.e., they do not
contribute to the testing process [3], [28], [31]. This means
that mutation testers should ﬁlter their mutant sets using
manual analysis to identify equivalent mutants [9], and
perform numerous test executions to discard mutants that
do not provide testing value, i.e., mutants that are detected
by the tests designed to detect other mutants [3], [28], [31].

Working with large real-world systems makes the prob-
lem almost intractable due to the vast numbers of mutants
involved. Test execution overheads alone can limit the
scalability of the technique. For instance, in our experiments,
we needed around 48 hours to execute the mutants for
a single component of the systems we examined. At the
same time the manual effort required by testers is escalated
with larger programs as the number of mutants grows
proportionally to program size.

To reduce application cost, it is imperative to limit the
number of mutants to those that are actually useful, prior
to any manual mutant analysis or test execution. Thus, we
need to identify which mutants are killable in order to limit
the manual effort involved in their identiﬁcation, and also to
identify the mutants that are subsuming (disjoint)1, in order

• A. Garg, M. Ojdanic, R. Degiovanni, M. Papadakis and Y. Le Traon are

with the University of Luxembourg, Luxembourg.

• T. Checkam is with SES, Luxembourg.

to reduce unnecessary computations, and to provide accurate
adequacy measurements [47].

This problem is known as the mutant selection prob-
lem [48] and has been studied in the form of selective muta-
tion [44], [68], i.e., restricting the number of transformations
to be used, with limited success [11], [35]. Though, the key
issue with mutant selection is the simple syntactic-based
nature of the selection process. The issue is that mutants
are introduced everywhere with respect to simple language
operators, e.g., by replacing an operator with another, that
completely ignore the program and particular location
semantics. This operator matching mutant selection has the
unfortunate effect of introducing mutants independent of
their context and program semantics.

We propose Cerebro2, a machine learning technique that
learns to identify interesting mutants given their context. In
particular we learn the associations between mutants and
their surrounding code. Our learning scope is a relatively
small area around the mutation point that differentiates
locally, the mutants that are useful from those that are not.
This allows mutating the program elements to ﬁt best to their
context, instead of mutating entire codebases with every
possible transformation, enabling inter-project predictions.

Cerebro operates at lexical level, with a simple code prepro-
cessing. In particular, a mutant and its surrounding code is
represented as a vector of tokens where all literals and identi-
ﬁers, i.e., user deﬁned variables, types, and method calls, are
replaced with predeﬁned, hence predictable, identiﬁer names.
This allows restricting the related vocabulary and learning
scope to a relatively small ﬁxed size of tokens around the

1. The term disjoint mutants refers to a minimal subset of mutants
that need to be killed in order to reciprocally kill the original set [31],
[46].

2. Cerebro is a ﬁctional device appearing in Marvel comics used by the
X-Men to detect human mutants. More details in https://en.wikipedia.
org/wiki/Cerebro.

 
 
 
 
 
 
Cerebro: Static Subsuming Mutant Selection

2

mutation points. Learning is performed using a powerful
and language-agnostic machine translation technique [8] that
we train on related code fragments and their labels.

We consider useful, the subset of mutants that resides
on top of the subsumption hierarchy and subsumes the
others [34], aka subsuming mutants [28], for the set of all
possible mutant instances produced by a given set of
mutation operators. Mutant M1 subsumes mutant M2 if
every test case detecting M1 also detects M2. This implies
that the tests detecting the subsuming mutant will also detect
the subsumed ones thereby making subsumed mutants
redundant.

We implemented Cerebro and evaluated its ability to
predict (inter-project predictions) subsuming mutants on a
large set of programs, composed of 48 C programs (CoreUtils)
and 10 Java projects (Apache Commons, Joda-Time, and
Jsoup). Our results demonstrate that Cerebro signiﬁcantly
outperforms both, random mutant selection and a supervised
machine learning approach (used by previous research) on
both, C and Java benchmarks.

In particular, our results show that Cerebro signiﬁcantly
outperforms the baselines. In Java projects, Cerebro obtained
2.81 times higher MCC3 values, an improvement of 82%
in F-measure, 68.88% in Precision, and 85.71% in Recall
over the state-of-the-art supervised machine learning. In C
programs, Cerebro obtained 2.76 times higher MCC values,
3.72 times higher precision, and slightly increased Recall
value (4% higher). The improvement measured in F-measure
is approximately 65%.

To put the predictions into a context and understand its
inﬂuence on mutation testing, we also validated Cerebro in
a controlled simulation of the envisioned use case. In
particular, we simulate a scenario where testers are guided by
mutation testing, i.e., they design test cases based on mutants.
Therefore, fewer mutants imply less effort, while stronger
mutants imply stronger tests. Our analysis shows that Cerebro
achieved more than twice the subsuming mutation scores4
in both, C and Java programs that we use. At the same
time Cerebro required signiﬁcantly less effort in terms of
both, analyzed equivalent mutants and test executions. In
C programs, 3.70% of the mutants analyzed by Cerebro are
equivalent, while 55.56% and 53.33% analyzed by random
mutant selection and supervised learning, respectively are
equivalent; Cerebro also required 91% fewer test executions
than random selection and supervised learning, respectively.
In Java programs, Cerebro required the analysis of 41% and
36% fewer equivalent mutants, and 92% and 87% fewer test
executions than random mutant selection and supervised
learning, respectively.

All-in-all our paper makes the following contributions:
1) We present Cerebro, a powerful static subsuming mutant

selection technique.

2) We provide evidence suggesting that Cerebro successfully
predicts subsuming mutants with 0.85 Precision, 0.33
Recall and 0.46 MCC.

3. The Matthews Correlation Coefﬁcient (MCC) [41] is a reliable metric
of the quality of prediction models [56], relevant when the classes are
of very different sizes, e.g. in case of C programs, 10.2% subsuming
mutants (positives) over 89.8% non-subsuming mutants (negatives).

4. Subsuming mutation score (MS*) is the ratio of the killed and the total

number of subsuming mutants.

3) We show that Cerebro signiﬁcantly outperforms the
current state-of-the-art, i.e., random mutant selection
and previously proposed machine learning technique,
by revealing 2 times the subsuming mutants, while
analyzing 64% to 67% fewer equivalent mutants and
requiring 89% to 92% fewer test executions.
The remainder of the paper is organized as follows.
Section 2 introduces preliminary concepts necessary in
subsequent sections. Section 3 describes the envisioned use
case for Cerebro and elaborates on a particular motivating
example. Section 4 describes the approach in detail. Section
5 introduces the research questions and Section 6 details
the experimental setup. The results of our experimental
evaluation are summarized in Section 7. We discuss threats to
validity in Section 9. In Section 8 we also discuss the impact of
the abstraction process and mutants’ context size on Cerebro’s
prediction performance. Finally, we discuss related work in
Section 10, and present our conclusion and future work in
Section 11.

2 BACKGROUND
2.1 Subsuming Mutants

Mutation is a test adequacy criterion in which test require-
ments are represented by mutants that are obtained by
performing slight syntactic modiﬁcations to the original
program. Then, the tester needs to design test cases in
order to kill the mutants, i.e., to distinguish the observable
behavior between the mutant and the original program. Some
mutants cannot be killed as they are functionally equivalent
to the original program. Hence, the quality of a test suite is
measured by the mutation (adequacy) score, a percentage
metric obtained by the ratio of killed mutants over the total
number of (non-equivalent) generated mutants.

Mutation testing is a promising, empirically validated
software testing technique that hasn’t achieved its full
potential yet [48]. It is often considered as computationally
expensive, mainly due to the large number of mutants that
it introduces, which require analysis and execution with
the related test suites. One may notice that the number of
mutants is disproportionate with the number of test cases to
kill them, since one test case can kill several mutants at the
same time. Thus, the effort put into analyzing and executing
mutants that do not help to improve test suites is wasted.
Hence, it is desirable to analyze only the mutants that add
value, i.e., subsuming mutants [3], [28], [31], [34].

Intuitively, subsuming mutants are the minimum subset
of all mutants that when killed, by any possible test suite,
results in killing the entire set of killable mutants. Given
two mutants M1 and M2, it is said that M1 subsumes M2
if every test suite T killing M1 also kills M2. Unfortunately,
identifying subsuming mutants is undecidable as it is not
possible to know a mutant’s behavior under every possible
input. Thus, researchers typically approximate them through
test suites [3], [28], [35], [46], [47].

More precisely, let M1, M2 and T be two mutants and a
test suite, respectively, where T1 ⊆ T and T2 ⊆ T are the set
of tests from T that kill mutants M1 and M2, respectively,
and T1 (cid:54)= ∅ and T2 (cid:54)= ∅, indicating that both M1 and M2
are killable mutants. We will say that mutant M1 subsumes
mutant M2, if and only if, T1 ⊆ T2. In case T1 = T2, we say

Cerebro: Static Subsuming Mutant Selection

3

that mutants M1 and M2 are indistinguishable for T . The set
of mutants which are both killable, and subsumed only by
indistinguishable mutants are called subsuming mutants.

For example, if we have a mutant set of 3 mutants (M1,
M2, and M3) and a test set T = {t1, t2, t3}, where M1 is
killed by T1 = {t1}; M2 is killed by T2 = {t1, t2}; and M3 is
killed by T3 = {t3}. We can notice that every time that we
run a test (t1) to kill mutant M1 we will also kill mutant M2.
However, the opposite does not hold. Thus, we have two
subsuming mutants, i.e., M1 and M3.

Subsuming mutation score (MS*) is the ratio between
killed subsuming mutants over the total number of sub-
suming mutants [47]. Subsuming mutation score has been
proposed [3], [31], [47] as a reliable metric to evaluate the
effectiveness of testing techniques as it does not consider
the presence of subsumed mutants. Subsumed mutants can
artiﬁcially inﬂate the mutation score of a testing technique
and can mislead its apparent ability to detect faults. For
instance, following our previous example, a test suite {t1,
t2} kills 66.7% of all the mutants (i.e., M1 and M2), but 50%
of the subsuming ones (M3 is not killed).

Interestingly, killing subsuming mutants leads to the
killing of all killable mutants, thus, testers needs to focus
mutation analysis on subsuming mutants. The problem
though, is that one needs to know the subsumption relations
between mutants in advance, before starting to analyze the
mutants and designing tests. To deal with this issue, we
introduce Cerebro, a static technique that predicts subsuming
mutants without requiring any dynamic analysis, with the
aim to help testers decide on which mutants to use when
performing mutation-guided test generation [23], [49].

2.2 Machine Translation

Machine Translation can be considered as a transfor-
mation function transform(X ) = Y , where the input
X = {x1 , x2 , . . . , xn } is a set of entities that represents
a component to be transformed, to produce the output
Y = {y1 , y2 , . . . , yn }, which is a set of entities that represent
a transformed (desired) component. In the training phase, the
transformation function learns on the example pairs (X , Y )
available in the training dataset. In our context, X contains
the source code with an annotation that indicates the location
and type of the mutation operator applied, and Y contains
the same information, plus a label that indicates whether the
mutant is subsuming or not.

the

pairs

example

The transformation function is trained to append
the label to a given mutant by training the function
(Code+MutationAnnotation,
on
Code+MutationAnnotation+Label),
where
Code+MutationAnnotation represents the source code
with an annotation in the statement to indicate the mutation
operator type applied. This learned transformation is used
as our prediction model for predicting subsuming mutants.
Among the several machine translation algorithms that
have been suggested over the past years, we use the RNN
Encoder-Decoder which is established and is used by many
recent studies [59], [61], [62].

2.3 RNN Encoder-Decoder architecture

The RNN Encoder-Decoder machine translation is composed
of two major components: an RNN Encoder to encode a

Fig. 1: Cerebro Mutation Testing process. Given a program
P and a mutant set M , Cerebro selects from M a subset
of mutants M (cid:48) to be used for test generation. M (cid:48) is then
used to in Test generation, test execution and mutation score
calculation steps.

sequence of terms x into a vector representation, and an RNN
Decoder to decode the representation into another sequence
of terms y. The model learns a conditional distribution
over an (output) sequence conditioned on another (input)
sequence of terms: P (y1; . . . ; ym|x1; . . . ; xn), where n and
m may differ. For example, given an input sequence x
= Sequencein = (x1; . . . ; xn) and a target sequence y =
Sequenceout = (y1; . . . ; ym), the model is trained to learn
the conditional distribution: P (Sequenceout|Sequencein) =
P (y1; . . . ; ym|x1; . . . ; xn), where xi and yj are space-
separated tokens. A bi-directional RNN Encoder [8] (formed
by a backward RNN and a forward RNN) is considered the
most efﬁcient to create representations as it takes into account
both past and future inputs while reading a sequence [6].

3 USE CASE SCENARIO AND MOTIVATION

3.1 Use Case Scenario

Figure 1 shows an overview of how the testing process is
performed when it is guided by mutation. We adapted this
ﬁgure from the one published in [4, Figure 5.2]. Given a
program P as input, the mutation testing process starts by
creating a set M of mutants forming the test requirements.
Test requirements are satisﬁed when tests kill the mutants.
Since the number of mutants are excessive and form the key
cost factor of mutation testing [48], testers select a subset
M (cid:48) of mutants from M to focus on their analysis. Then,
testers pick a mutant m ∈ M (cid:48) and design a test t capable of
killing m or judge it as equivalent and discard it. The process
is repeated until the design of test is capable of killing a
predeﬁned ratio of mutants (threshold). Finally, the designed
test suite T is used to check the correctness of program
P (w.r.t. test suite T ). If test suite T detects some bug in
program P , then P has to be ﬁxed and the same mutation
testing procedure can again be employed.

It is worth mentioning that there are two major cost
factors in mutation testing, these are the equivalent and
subsumed mutants. This is because they introduce overheads
both during test generation and test execution, leading to
minor test effectiveness improvements. Therefore, to reduce
mutation testing effort while preserving its effectiveness, it
is essential to focus on subsuming mutants.

Mutant GenerationMutant Selection (Cerebro)Test GenerationTest Execution - MS calculationPMPM'PM'PTMS threshold reached?yesnoP correct wrt T?TyesnoFix  PCerebro: Static Subsuming Mutant Selection

4

1
2

3
4

5
6
7
8

int max(int a, int b, int c){

if (a >= b && a >= c) //M0: (a < b && a >= c)
//M1: (a >= b && a > c)
//M2: (a >= b || a >= c)
//M3: (true && a >= c)

return a; //M4: return b;

else if (b >= a && b >= c)//M5: (b < a && b >= c)

//M6: (b >= a && b > c)
//M7: (b >= a || b >= c)
//M8: (false && b >= c)

return b; //M9: return a;

else

return c; //M10: return 0;

}

(a) Code and mutants for function max.

(b) Subsuming Mutants Graph
for function max.

(c) Our motivating example shows that mu-
tants selected by Cerebro lead to stronger test
suites than those designed to kill randomly
selected mutants, when equal number of mu-
tants is analyzed.

Fig. 2: The example shows that by analyzing only the three subsuming mutants M3, M4 and M7 is enough for covering all 9
killable mutants. Particularly, mutants M1 and M6 are equivalents.

Hence, we develop Cerebro, a machine learning technique
that learns from mutants’ surrounding context to predict
which mutants are subsuming. Given the input program,
P and the set M of mutants, Cerebro selects a subset M (cid:48) of
mutants that is probably subsuming (predicted as subsuming
by Cerebro), to be used for mutation testing (to guide testers
and evaluate test effectiveness). Based on M (cid:48), testers and/or
automatic test generation techniques can focus on the few
strong mutants and design effective test cases.

3.2 Motivating Example
Let us consider the code snippet of function max of Figure 2a,
which takes three integers as input and returns the maximum
number among them. Also, consider (for simplicity) that
we have the 11 mutants shown in the ﬁgure. For instance,
mutant M0 mutates sub-expression a >= b of line 2 into a
< b. Similar mutations on relational operations were applied
to produce mutants M1, M3, M5, M6 and M8. Mutants
M2 and M7 replace the conjunction (&&) by the disjunction
(||). While mutants M4, M9 and M10 replace the returned
variable name by other variable name or constant (M10
replaces variable name c by constant 0).

For the sake of the thorough demonstration, we observed
scenarios under the following testing conditions: A test
case invoking max(1,2,0) and expecting 2 as a result,
kills mutant M3, as well as, mutants M0, M2, M5, M8,
and M9. But tests invoking max(2,0,1), max(1,0,2),
and max(0,2,1) will kill mutants M0, M2, M5, M8, and
M9, except M3. Figure 2b shows a graph representation of
the subsumption relation between the 9 killable mutants.
Moreover, Figure 2b shows that M3 subsumes M0, M5, M8
and M2. Particularly notice that mutants M5 and M8 are
indistinguishable, since they are killed by the same tests, and
subsume mutant M9. Although, mutants M1 and M6 are
equivalent.

In summary, mutants M3, M4 and M7 are subsuming,
indicating that in order to kill every killable mutant it is
sufﬁcient to kill only these 3 subsuming mutants.

Cerebro will take as input the program max and the set
of mutants, and it will point to those that are most likely
subsuming. In an ideal scenario, Cerebro would point only
to M3, M4 and M7, but it is possible, as in every machine

learning based technique, that it does some mistakes, i.e.,
incorrect predictions of subsuming mutants, pointing to
some non-subsuming (subsumed or equivalent mutants) as
subsuming.

For instance, consider the case in which Cerebro predicts
M3 and M4 and M10 as subsuming mutants. Therefore,
a tester will incrementally design test cases to kill all the
predicted mutants. Assume that the tester starts by analyzing
mutant M3 and designs a test to kill it, e.g., by invoking
max(1,2,0). This test does not kill the rest of the selected
mutants. The tester then proceeds to analyze the surviving
mutant M4, for which he/she designs a test that invokes
max(2,0,1) to kill it. Finally, the tester designs a test
by invoking max(0,1,2), which kills mutant M10 and
also (non selected) subsuming mutant M7. Notice that this
test suite designed to kill all mutants selected by Cerebro
progressively increments the MS*: ﬁrst test kills subsuming
mutant M3 leading to a MS* of 33.33%; second test kills
subsuming mutant M4, obtaining 66.66% of MS*; and ﬁnally,
third test kills collaterally subsuming mutant M7 leading to
a MS* of 100%.

Consider a scenario in which mutants are selected
randomly. For instance, assume that M9 is the ﬁrst one
to be selected for analysis for which a test case invoking
max(0,2,1) is designed to kill it. This test collaterally kills
mutants M5 and M8, but it does not kill any subsuming
mutant. Then, assume that equivalent mutant M1 is ran-
domly selected, adding no value to the testing process, but
requiring analysis anyway. Afterwards mutant M0 is ran-
domly selected for which a test case invoking max(2,0,1)
is designed to kill it, that fortunately also kills subsuming
mutant M4. Then, mutant M2 is randomly selected for which
the tester designs a test to kill it by invoking max(1,0,2).
This test also kills mutant M10, but no subsuming mutant
is killed. After that, tester randomly selects mutant M3 for
analysis and designs a test by invoking max(1,2,0) to kill
it. This test kills subsuming mutant M3 and also mutant M2.
Finally, mutant M4 is randomly selected for which the tester
designs a test to kill it, by invoking max(2,0,2). Hence, all
subsuming mutants are killed.

In this particular scenario we can observe that MS*
remains at 0% after analyzing the ﬁrst 2 mutants randomly

M4M3M7M0M2M10M5, M8M9MS*Number of analysed mutants1230.330.661.00CerebroRandom456effectiveness differenceeffort differenceCerebro: Static Subsuming Mutant Selection

5

selected, and reaches a MS* of 33.33% after analyzing the
third randomly selected mutant. The analysis of the fourth
selected mutant (non-subsuming) did not add value (MS*
remains the same). Finally, ﬁfth and sixth analyzed mutants
were subsuming, leading to a test suite that obtains MS* of
100% after analyzing 6 mutants.

Figure 2c depicts the progress of MS* obtained by the test
suites when guided by Cerebro and random mutant selection
in the previously described scenarios. Through this example
we demonstrate a case where two approaches analyze the
same number of mutants (same effort) with Cerebro having
higher effectiveness (MS*) than the random mutant selection
baseline. At the same time, in order to reach the same MS*
as Cerebro, random mutant selection needs more effort, i.e., it
will require the analysis of many more mutants than Cerebro
(in the example random baseline analyzed two times more
mutants than Cerebro).

There are several points we want to highlight about the
particular scenarios just described. First, it is essential to
notice that mutants selected by Cerebro will be as close as pos-
sible to subsuming in the subsumption relation. Killing these
(almost subsuming) mutants can help in killing subsuming
mutants predicted as non-subsuming by Cerebro, for instance,
the test that kills subsumed mutant M10, also kills subsuming
mutant M7 that was incorrectly predicted as non-subsuming
by Cerebro. Second, it is also important to notice that Cerebro
selects the least possible number of equivalent mutants,
saving the time of analysis to the tester (in the example,
Cerebro did not predict any equivalent mutant as subsuming).
Third, notice that the prediction performance obtained by
Cerebro does not necessarily reﬂect its effectiveness in practice,
since mutant kills are not independent of one another. While
Cerebro reached 66.66% of Precision and 66.66% of Recall
in the example, in practice, the test suite designed to kill
all selected mutants obtains 100% of subsuming mutation
score (MS*). And fourth, it is worth to study the trade-off
between the effectiveness and effort of the different mutant
selection techniques. We consider all these points in our
empirical evaluation to assess the prediction performance,
effectiveness, and effort required by Cerebro and the related
mutant selection techniques.

4 APPROACH

The main objective of Cerebro is to automatically learn the
silent features/patterns of the context surrounding subsum-
ing mutants without requiring any features deﬁnition and/or
selection by human intervention, that we can use later to
predict if mutants on an unseen source code are likely to
be subsuming or not. Thus, we train a machine translator
(viz. an encoder-decoder model) to identify subsuming
mutants, by feeding it with source code where the statement
(to mutate) is annotated with the mutant type and its
label (subsuming or not). Machine translators have been
successfully used to translate text from one language to
another, as they automatically recognize (i) the features
of the language (to be translated) and (ii) the required
translation (to the desired language). In our case, it is used
to automatically identify the features of subsuming mutants
without any investment of time and/or resources to deﬁne
features.

After training, one can input to the translator, an unseen
mutant (source code where the statement to mutate is
annotated with the mutation annotation). The translator will
append the label to the mutant given as input, to predict
whether it is subsuming or not.

Figure 3 shows an overview of the implementation. For
training, Cerebro takes a set of mutants and their correspond-
ing label. In each mutant source code, the statement (to
mutate) is annotated with the mutation annotation, and the
model learns the label to be appended to this annotation,
that indicates whether the mutant is subsuming or non-
subsuming. We can summarize Cerebro’s pre-processing,
training and testing steps as follows:

1) Abstraction: Producing abstracted code of the actual
source code by removing irrelevant information (e.g.
comments) and replacing user-deﬁned identiﬁers and
literals (e.g. variable names) by predictable tokens;

the pairs

2) Pairs Generation: Generating

(input-
expected output) to be used for training, by adding the
corresponding label into the mutation annotations;
3) Training: Training the machine translator to learn which
label is to be appended to the mutation annotations;
4) Testing: Utilizing the trained translator to predict and
append labels to the mutation annotations present in
unseen mutant source code.
In the remainder of this section we describe each of the

aforementioned phases of our approach, in detail.

4.1 Abstracting the Irrelevant Information

A major challenge in dealing with raw source code is the
huge vocabulary created by the abundance of identiﬁers and
literals used in the code. On such a large scale, vocabulary
may hinder the goal of learning features surrounding the
subsuming mutants. Thus, to reduce vocabulary size, we
abstract source code by replacing user-deﬁned entities with
re-usable identiﬁers.

Figure 4 shows an actual code snippet (Figure 4a)
converted into its abstract representation (Figure 4b). The
purpose of this abstraction is to replace any reference to user-
deﬁned entities (function names, types, goto labels, variable
names and string literals) by identiﬁers that can be reused
across source code ﬁle, hence reducing the vocabulary size.
Thus, our abstraction approach ﬁrst detects user-deﬁned
entities before replacing them with unique identiﬁers (new
IDs).

the

IDs

New

follow

regular

expression
(fn|tp|lb|vr|lr)_(num)+, where num stands
for
numbers 1, 2, 3, . . . assigned in a sequential and positional
fashion based on the occurrence of that entity. All the
user-deﬁned Function names, Type names, Variable names,
Labels, and String Literals are replaced with fn_num, tp_num,
lb_num, vr_num, and lr_num, respectively. Thus, the ﬁrst
function name found receives the ID fn_1, the second
receives the ID fn_2, and so on. If any of these entities
appear multiple times in a source code ﬁle, it is replaced
with the same ID.

Additionally, we remove code comments and add mu-
tation annotations to encode the mutation operator and
the corresponding label (to be learned by the transla-
tor). Our mutation annotations have the general shape

Cerebro: Static Subsuming Mutant Selection

6

Fig. 3: Implementation: Source code is abstracted and attached with mutation annotation to produce mutant annotations.
Model is trained on mutant annotations to further append the label (subsuming/non-subsuming). Trained model is provided
with an unseen mutant annotation to append the label. The appended label acts as the prediction for the unseen mutant
annotation.

1
2
3
4
5
6
7
8
9
10
11
12

...
public String getOptionValue (

final Option option ) {
if ( option == null ) {

return null ;

}
final String [] values =

getOptionValues ( option ) ;

return ( values == null ) ?

null : values [ 0 ] ;

}
...

1
2
3
4
5
6
7
8
9
10
11
12

...
public String fn_3 (
final tp_1 vr_3 ) {
if ( vr_3 == null ) {

return null ;

}
final String [] vr_5 =

fn_4 ( vr_3 ) ;

return ( vr_5 == null ) ?

null : vr_5 [ 0 ] ;

}
...

1
2
3
4
5
6
7
8
9
10
11
12

...
public String fn_3 (
final tp_1 vr_3 ) {
if ( vr_3 == null ) {

return null ; MST[ReturnValsMutator]MSP[

]

}
final String [] vr_5 =

fn_4 ( vr_3 ) ;

return ( vr_5 == null ) ?

null : vr_5 [ 0 ] ;

}
...

Label
S / N

(a) Actual Source Code

(b) Abstracted Code

(c) Mutant Annotation

Fig. 4: Abstraction: Actual Source Code (4a) is abstracted by replacing user-deﬁned entities (Function names, Type names,
Variable names) with tokens (fn_num, tp_num, vr_num) to achieve the Abstracted Code (4b). Mutant annotation (4c) is
generated by adding the Mutation annotation with its corresponding label, i.e., Subsuming (S) or Non-Subsuming (N). The
trained model is used for prediction of unseen mutant annotations.

“MST[+MutationOperator+]MSP[]”, where MST and MSP
denote mutation annotation start and stop, respectively, and
MutationOperator indicates the applied mutation operation
(in green in Figure 4c). Between the last brackets [], our
trained model adds one of the labels S or N, indicating that
the mutant obtained by applying the mutation operation, is
predicted as subsuming or non-subsuming, respectively.

input sequence given to the translator and the expected
output sequence produced by it, is the predicted label S
or N. Using these sequences, we intend to capture as much
code as possible around the mutant without incurring the
exponential increase in training time.

represents

a mutant

4.2 Pairs Generation
The mutation operation (ReturnValsMutator5) shown
in Figure
in which the
4c
sentence return null is
replaced by throw new
java.lang.RuntimeException() exception. Notice that
this mutant is labeled as subsuming in our dataset, since
there is only one test that can kill it, when the input option
is null. Hence for training we consider S as the label to be
learned by the translator to predict this mutant as subsuming.
To do so, we train in pairs (MutantAnnotation, Mu-
tantAnnotation+Label), where the ﬁrst component is the
annotated code shown in Figure 4c, and the second com-
ponent is the same code with the predicted label, i.e.,
MST[ReturnValsMutator]MSP[S] in our case, to indi-
cate that the mutant is subsuming. The resulting text is
arranged in a single sentence to represent a sequence of
space-separated entities (the representation supported by
the machine translator). The only difference between the

5. https://pitest.org/quickstart/mutators/#RETURN_VALS

4.3 Building the Machine Translator

To build our machine translator, we train an encoder-decoder
model that can transform an input sequence to a desired
output sequence. In our representation, a sequence consists
of tokens separated by spaces that ends with a newline
character. Thus, we train the encoder-decoder by feeding it
with pairs of sequences, produced in the previous step. The
translator learns to replicate the abstracted source code with
the mutation annotation and to append the label (S/N) that
will be used as a prediction for the mutant.

We found that training the translator on sequences of
maximum 100 tokens in length is computationally feasible,
but expensive (740 training hours required on a Tesla V100
GPU). Hence, we also experiment with sequences of 50 to-
kens in length and demonstrate that the computation cost of
training the translator can be further contained (360 training
hours required). We name Cerebro trained on sequences of
100 tokens in length as Cerebro-100. Following our naming
convention, we name Cerebro trained on sequences of 50
tokens in length as Cerebro-50.

PRE-PROCESSING MODEL TRAINING  ABSTRACTION  MUTANT ANNOTATION GENERATION SOURCE  CODE ABSTRACTED CODE MUTANT ANNOTATION  ENCODER-DECODER FRAMEWORK TRAINED MODEL TESTING UNSEEN  MUTANT ANNOTATIONS PREDICTION NON-SUBSUMING SUBSUMING SUBSUMING & NON-SUBSUMING MUTANT ANNOTATIONS TRAINED MODEL Cerebro: Static Subsuming Mutant Selection

4.4 Predicting from appended labels

To predict whether or not a certain mutation at a particular
position in an unseen code is subsuming, we abstract the
unseen code followed by sequence generation which results
in abstracted code sequence attached with mutation annota-
tion as depicted in Figure 3. We feed this sequence into the
trained machine translator to yield an output sequence with
an appended label. The appended label acts as a prediction
(subsuming/non-subsuming) for this speciﬁc mutation. If the
translator produces an output sequence with a change other
than appending the predicted label, the input sequence is
predicted as non-subsuming, by default. In our experiments
reported in Section 7, this happened in 4.2% and 0.1% of the
sequences for C and Java programs, respectively.

5 RESEARCH QUESTIONS

We start by checking the prediction ability of Cerebro and ask:
RQ1 Prediction Performance: How effective is Cerebro in pre-

dicting subsuming mutants?

We leverage two datasets, made of C and Java programs,
for which extensive mutation analysis has been performed to
identify subsuming mutants. We reimplemented 2 techniques
that we use as baselines in our analysis. The ﬁrst baseline is
a Random mutant sampling, while the second is a supervised
machine learning method based on manually designed
features that were used by previous work [11] (e.g., data
ﬂow, control ﬂow, etc.). These features are used to train a
binary classiﬁer in order to predict whether a mutant is
subsuming or not. Further details about the baselines can be
found in Section 6.3.

After analyzing the predictions, we turn our attention
to the envisioned application scenario; measuring test effec-
tiveness of the predicted mutants. It is important to check
the application case because a) predictions may select weak
mutants [11] (weak subsuming mutants result in lower test
effectiveness than the strong ones), b) selected mutants may
not be diverse as they may include mutually subsuming
mutants [34], and c) tester beneﬁts are unclear. Thus, we ask:
RQ2 Effectiveness Evaluation: How does Cerebro compare with

the baselines in terms of subsuming mutation score?

We perform a simulation of a mutation testing scenario
where a tester analyzes the selected mutants in order to
generate tests [5], [11], [35]. For test effectiveness, we measure
the subsuming mutation score (MS*) achieved by the tests
that kill the selected mutants. In essence, we evaluate the
guidance offered by the mutants when testers design tests to
kill the selected mutants. It is worth noticing that in this part
of the experiment we control the number of mutants, i.e.,
all techniques analyze the same number of mutants. Such
simulation is typical in mutation testing literature [5], [11],
[35] and aims at quantifying the beneﬁt of an approach over
the other.

Complementary to the previous question, we compare
the effort required by each technique to obtain the same level
of test effectiveness. Hence, we ﬁrst investigate the human
effort measured in terms of the number of mutants analyzed
by the tester, to reach the same subsuming mutation score
using Cerebro and the baselines. Hence, we ask:

7

RQ3 Manual Effort: How many mutants require manual
analysis in order to reach a given level of subsuming
mutation score?
We perform a similar simulation of a testing scenario in
which we measure how many mutants the tester needs to
analyze (generate a test case to kill or judge equivalence),
until he/she obtains a determined subsuming mutation score.
This allows us to quantify the human effort required by each
approach to obtain the same beneﬁt.

Related to the previous question, we also investigate
the number of test executions necessary to reach the same
subsuming mutation score, by following the incremental
process of mutation analysis, i.e., a tester picking a mutant
and analyzing it. If the picked mutant is killable, he/she
generates a test case that kills it, and then checks if the
remaining alive (not analyzed and not killed) mutants
are collaterally killed by the same test (by executing the
generated test on alive mutants). The killed mutants are
removed from the set of alive mutants. Then, we ask:

RQ4 Computational Effort: How many test executions are
required in order to reach a given level of subsuming
mutation score?
We perform a simulation as before, but in this case, every
time that a test is generated, we count the number of test
executions and measure the attained subsuming mutation
score, until we reach a given subsuming mutation score.

6 EXPERIMENTAL SETUP
6.1 Benchmarks and Ground Truth

In order to show that our approach is language agnostic, we
make our evaluation on a set of C and Java programs.

C-Benchmark: To perform our study that requires strong
test suites, we used an independently built dataset from
related work [12]. It includes C programs from the GNU
Coreutils6, that consist of ﬁle, text and shell utility programs
widely used in Unix systems. The data-set is composed of 48
GNU Coreutils (v8.22) programs aka subjects (mentioned in
Table 1), each packaged with an accompanying system test
suite, generated by developers. The size of these programs
ranges from 1,000 to 14,000 lines of code (LOC), with a
median size of 3,500 LOC. For each subject, the data-set
includes a mutant-test killing matrix that records, for each
mutant, a set of tests that kill it.

The mutant-test killing matrices were obtained by gen-
erating mutants using the Mart mutant generation tool [13]
and executing them against large test pools. The test pools
were built by considering developer tests and adding auto-
matically generated tests using a 24 hours run of KLEE [10].
Additionally, mutation-based test suites were automatically
generated using 128 different conﬁgurations of SEMu [12],
each running for 2 hours, and an additional ‘seeded‘ test
generation of KLEE. To reduce the total execution cost,
for each program, the 3 functions that were covered by
the largest number of developer tests were selected for
mutation analysis, i.e., mutants were generated only for these
functions.

We use these mutant-test killing matrices to compute
the mutant subsumption, following the deﬁnition given in

6. https://www.gnu.org/software/coreutils/

Cerebro: Static Subsuming Mutant Selection

Section 2.1, and label each mutant as either subsuming or
non-subsuming. To make the problem as balanced as possible
(to assist in machine learning), we mark as subsuming all
mutants in the top of the hierarchies, including mutually
subsumed mutants.

Needless to say, it is possible to have some noise in
our labeling process in the sense that mutants labeled as
subsuming may be non-subsuming. The data-set reduced
this noise by augmenting the test suites with multiple large
and diverse test suites generated by different state-of-the-art
tools. Please refer to the threat in Section 9 for a related
discussion.

Java-Benchmark: For Java we select a set of well-
tested open source projects from GitHub. We select
projects from the Apache Commons Proper7 repository of
reusable Java components, Joda-Time8 - a date and time
library, and Jsoup9 - an HTML manipulation library. The
set counts 10 projects: commons-cli, commons-codec,
commons-collections, commons-csv, commons-io,
commons-lang, commons-net, commons-text, jsoup,
joda-time. These projects contain up to 284 classes. Table 1
reports the version/commit of each project we used for our
study. Following a similar procedure done for C in [12], we
also build test pools by using developer tests and adding
automatically generated tests by running EvoSuite [23] for
each project with the default running time, but with multiple
coverage metrics10. The mutant-test killing matrices were
obtained using Pitest [17]. For each project, we run the
mutants on the test pools for 48 hours. To reduce execution
time, we select the classes processed during that time lapse.
Table 2 records the total number of mutants, number
(and percentage) of killable and subsuming mutants, and
number of test cases conforming to the mutant-test killing
matrices. Please note that the difference on the ratio of
subsuming mutants with previous research [3], [34], [47]
is due to the inclusion of all mutually subsuming mutants.
As already explained, we include all subsuming mutants to
avoid misleading our learner.

6.2 Equivalent Mutants

Early research on mutation testing has demonstrated that
deciding whether a mutant is equivalent is an undecidable
problem [9]. Mutation testing may produce a mutant that is
syntactically different from the original, yet semantically
identical, aka equivalent mutant [32]. Undecidability of
equivalences means that it is impossible to automatically
discard them all. As a result, the tester may never know
whether he or she has failed to ﬁnd a killing test case because
the mutant is particularly hard to kill, yet remains killable (a
‘stubborn’ mutant [65]), or whether failure to ﬁnd a killing
test case derives from the fact that the mutant is equivalent.
The best options we have are effective algorithms that can
remove most equivalent mutants, e.g., in C data-set [12]
authors applied TCE (Trivial Compiler Equivalence) [25], [32]
to ﬁlter out equivalent and duplicated mutants. Interestingly,
early research on mutation testing [2] has shown that humans

7. https://commons.apache.org
8. https://github.com/JodaOrg/joda-time/
9. https://github.com/jhy/jsoup
10. LINE:BRANCH:MUTATION:OUTPUT:METHOD:CBRANCH

TABLE 1: Benchmark

Project

Web URL

C

8

Version /
Commit

base64, basename,
chcon, chgrp,
chmod, chown,
chroot, cksum,
comm, date,
df, dirname,
echo, expr,
factor, false,
groups, join,
link, logname,
ls, md5sum,
mkdir, mkﬁfo,
mknod, mktemp,
nproc, numfmt,
pathchk, printf,
pwd, realpath,
rmdir, sha256sum,
sha512sum, sleep,
stdbuf, sum,
sync, tee,
touch, truncate,
tty, uname,
uptime, users,
wc, whoami [12]

https://github.com/coreutils/coreutils.git

v8.22

Java

commons-cli

https://github.com/apache/commons-cli.git

6490067

commons-collections

https://github.com/apache/commons-collections.git

d6eeceb

commons-text

https://github.com/apache/commons-text.git

commons-csv

https://github.com/apache/commons-csv.git

commons-lang

https://github.com/apache/commons-lang.git

commons-io

https://github.com/apache/commons-io.git

commons-net

https://github.com/apache/commons-net.git

commons-codec

https://github.com/apache/commons-codec.git

jsoup

joda-time

https://github.com/jhy/jsoup.git

https://github.com/JodaOrg/joda-time.git

26a308f

865872e

2c0429a

c126bdd

33df028

475910a

528ba55

767c94e

TABLE 2: Test Subjects

Language

#Programs

#Mutants

#Killed

#Subsuming

#Testcases

C [12]

Java

48

10

71,850

49,530 (68.9%)

7,358 (10.2%)

136,412

153,823

124,064 (80.6%)

41,219 (26.8%)

21,878

also make many mistakes (approximately 20%) when judging
mutants (as being equivalent or not). This means that it is
unrealistic to expect that automated tools (or testers, in case
of manual test case design) kill all killable mutants.

To make a fair approximation of killable mutants we used
state-of-the-art test generation tools (KLEE [10], SEMu [12],
and EvoSuite [23]), together with mature developer test suites
to identify killable mutants. For the remaining live mutants
(i.e., mutants that are killed neither by developers written
nor automatically generated test suites) we assumed that live
mutants are equivalent. Although, this assumption may have
some impact on our results (refer to Section 8.4 for an analysis
of the impact of this assumption), it allows quantifying the
effort involved by testers in analyzing low utility mutants
when using the current state-of-the-art advances. Moreover,
since Cerebro performs machine learning, it learns from the
employed data. This means that the availability of clean data,
with a clear signal to learn, will allow Cerebro make better
predictions, thereby potentially improving its performance.

6.3 Baselines

We consider 2 baselines. The ﬁrst one is the Random mutant
sampling that samples uniformly from the entire set of
mutants. The second baseline is a Decision Tree classiﬁcation
based on the features proposed by related work [11], [30].

Previous works showed a strong connection between mu-
tant utility and surrounding code (utility captured through

Cerebro: Static Subsuming Mutant Selection

9

CFG, data ﬂows, AST, etc. features). Thus, we use the
mutant features to predict subsuming mutants in both C
and Java. Features belong to 4 categories: Mutant Type
related features, Control-Flow graph related features, Control
and Data dependency related features, and AST related
features. In total we used the 28 features, used by the
related work [11], for the C programs, and implemented
16 of those features for Java11. We excluded features such as
AstChildHasIdentiﬁer and AstChildHasLiteral that we found
unfeasible to implement in the employed tools, i.e., Pitest
works at byte-code level making it difﬁcult to identify the
original source code expression. Nevertheless, the excluded
features were approximated by mutant type.

After extracting the features, following the related
work [11], we trained a stochastic gradient boosted Decision
Tree model by using the same conﬁguration as the related
work [11]. We followed the same validation setup for Cerebro.

6.4 Implementation and Model Conﬁguration

We rely on the srcML tool [18] to convert source code into an
XML format to tag literals, keywords, identiﬁers, comments,
and our mutation annotations. This helps in separating user-
deﬁned identiﬁers and string literals (the largest part of the
vocabulary) from language keywords as srcML supports
C, Java and other languages. Then, we implement the ID
replacement to generate the abstracted code.

We follow the sequence pair generation procedure
mentioned in Section 4.2 to generate sequences from the
abstracted code. These sequences serve as training input
for our encoder-decoder model, which we build using tf-
seq2seq [1], a general-purpose encoder-decoder framework.
Following previous works [61], [62], we conﬁgure our model
with bidirectional encoder. We use a Gated Recurrent Units
(GRU) network [16] to act as the Recurrent Neural Network
(RNN) cell, which was shown to perform better than possible
alternatives (simple RNNs or gated recurrent units) in related
prediction tasks [57]. To achieve good performance with
acceptable model training time, we utilize AttentionLayer-
Bahdanau [7] as our attention class, conﬁgured with 2 layered
AttentionDecoder and 1 layered BidirectionalRNNEncoder,
both with 256 units.

To determine an appropriate number of training epochs,
we conducted a preliminary study involving a validation set,
independent of both, training and test sets that we use in
our evaluation. Here we incrementally train the model, with
checks after every epoch to monitor model training accuracy.
We pursue training the model till the training performance
on the validation set does not improve anymore. We found
15 epochs to be a good default for our validation sets.
Once model training is complete, we follow the procedure
explained in Section 4.4 to predict whether an unseen mutant
annotation sequence is subsuming or not.

The codebase of C and Java programs with mutant infor-
mation, abstracted code, and mutant annotation sequences
that the encoder-decoder model trains on and predict,

11. statementComplexity, expressionComplexity, MutantType, Block-
Depth, CfgDepth, CfgPredNum, CfgSuccNum, NumInBlock, Nu-
mOutDataDeps, NumInDataDeps, NumOutCtrlDeps, NumInCtrlDeps,
AstNodeParentType, NumberOfAstParents, AstNodeType, NumberO-
fAstChildren

with mapping to the original code, are publicly available
at https://github.com/garghub/Cerebro. In addition to our
dataset, we have made available our source code and trained
models as well.

6.5 Experimental Procedure

In the ﬁrst experimental part, we evaluate the prediction
ability of our approach, answering RQ1, while in the second
part, we evaluate cost-effectiveness of Cerebro, answering
RQs2-4.

6.5.1 First Experimental Part

We start by evaluating the prediction performance of Cere-
bro, and the baselines, using four typical metrics, namely,
Precision, Recall, F-measure, and Matthews Correlation Coef-
ﬁcient (MCC) [41]. A confusion matrix is computed for
each one of the studied methods, which stores the correct
and incorrect predictions. Given a subsuming mutant, if it
is predicted as subsuming, then it is a true positive (TP);
otherwise, it is a false negative (FN). Given a non-subsuming
mutant, if it is predicted as non-subsuming, then it is a true
negative (TN); otherwise, it is a false positive (FP). Then
we can use the confusion matrix to quantitatively evaluate
the prediction performance of Cerebro and Decision Trees
prediction models.

Precision =

T P
T P + F P

Recall =

T P
T P + F N

F-measure =

2 × Precision × Recall
Precision + Recall

MCC =

T P × T N − F P × F N
(cid:112)(T P + F P )(T P + F N )(T N + F P )(T N + F N )

Intuitively, Precision is the ratio of mutants truly subsum-
ing among all the mutants predicted as subsuming. Recall
is the ratio of mutants correctly predicted as subsuming
among all the subsuming mutants. F-measure indicates the
weighted harmonic mean of Precision and Recall. Matthews
Correlation Coefﬁcient (MCC) [41] is a reliable metric of the
quality of prediction models [56], that in contrast to the
previous metrics, also takes into account the True Negatives
(correctly predicted non-subsuming mutants). It is generally
regarded as a balanced measure that can be used even
when the dataset is unbalanced, i.e., the classes are of very
different sizes, e.g. in case of C programs, 10.2% subsuming
mutants (Positives) over 89.8% non-subsuming mutants
(Negatives). MCC returns a coefﬁcient between 1 and -1.
An MCC value of 1 indicates a perfect prediction, whereas
a value of -1 indicates a perfect inverse prediction, i.e., a
total disagreement between prediction and reality. An MCC
value equals 0 indicates that the prediction performance is
equivalent to random guessing.

The mutants selected by Cerebro are the ones predicted
as subsuming. For Decision Trees baseline, as it computes a
probability of a mutant being subsuming, we followed the
probability margin convention and considered those mutants
whose predicted probability was higher than 0.5 [11].

Cerebro: Static Subsuming Mutant Selection

10

To assess the performance we perform a inter-project
evaluations. We use 5-folds cross validation, where we evenly
split each benchmark in 5 parts (10 programs and 2 projects
per fold for C and Java benchmark, respectively). Then, for
each benchmark, we repetitively use 1 fold for testing and 4
folds for training (1 part out of 4, is used for validation).

6.5.2 Second Experimental Part

To study the cost and test effectiveness of our approach
and the baselines, we simulate a testing scenario where
a tester selects a subset of mutants, to use for mutation
analysis, and designs tests to kill them. Algorithm 1 provides
the pseudo-code of the simulation process we follow in
our experiments. It takes as input a set M of mutants to
analyze, the test pool P and a target subsuming mutation
score tMS*, and returns a test suite T that kills every
mutant from M (or reaches the pre-speciﬁed subsuming
mutation score). Additionally, it returns the subsuming
mutation score obtained by the test suite T (currMS*),
number of analyzed mutants (analyzedMut), number of
equivalent mutants analyzed (equivMut), and number of
test executions (tExec) required to generate test suite T
during the simulated mutation testing scenario.

Algorithm 1 Pseudo-code of the simulation procedure to
answer RQ2-4.
Input: set of mutants M
Input: test pool P
Input: target subsuming mutation score tMS*
Output: test suite T covering mutants in M
Output: subsuming mutation score currMS* obtained by T
Output: analyzedMut number of analyzed mutants
Output: equivMut number of equivalent mutants analyzed
Output: tExec number of test executions

(cid:46) set of survived mutants

m ← pickNextMutant(C)
analyzedMut++
if the test pool P can kill mutant m then

1: T ← ∅
2: C ← M
3: currMS* ← 0
4: while currMS* < tMS* and ¬isEmpty(C) do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end while
17: return T, currMS*, analyzedMut, equivMut, tExec

t ← randomlyPickTestKilling(m, P)
T ← T ∪ {t}
(cid:46) add test t to the suite
tExec += size(C) (cid:46) run t on mutants from C
remove from set C all mutants killed by t

end if
currMS* ← calculateMS*(M,T)

(cid:46) m is judged as equivalent

equivMut++

else

The simulation starts by picking (pickNextMutant) the
top mutant m, according to the technique used (Cerebro,
Decision Trees, and Random), among survived mutants from
set C (initialized with all mutants from M). It then checks
if there exists some test in the test pool P that kill m (this
process simulates a tester picking, analyzing, and designing
a test to kill a mutant). If no test kills mutant m, we judge it
as equivalent and remove it from C. Otherwise, we randomly

pick one test t from the pool that kills m. Then, we run
the test t on every mutant from C to check if the same test
consequently kills other mutants (killed mutants are then
removed from C). This process continues by taking the next
survived mutant and ﬁnding a test to kill it until every
mutant in C has been killed or until the desired subsuming
mutation score is reached. We run this simulation with
the set of mutants selected by Cerebro, Decision Trees, and
Random, respectively, and use the reported values to compare
their cost-beneﬁt performance for answering RQ2-4. Since
Algorithm 1 includes some random decisions, we repeat this
process 1,000 times for all the approaches.

To answer RQ2, we measure the effectiveness (beneﬁt)
of the approaches in terms of the subsuming mutation score
(MS*), i.e., the ratio between killed and total number of sub-
suming mutants, achieved by the generated test suites when
analyzing the selected mutants. The subsuming mutation
score reduces the inﬂuence of redundant mutants [34], [47].
For assessing the effectiveness of the approaches, we
aim at controlling the number of mutants selected by each
tool. In the case of Cerebro, the mutants selected are the
ones predicted as subsuming by our model. For Decision
Trees baseline, we rank (in descending order) the mutants
according to the predicted probability of being subsuming,
and follow the ranking to pick mutants (from highest
probability to lowest) for analysis. Random baseline randomly
ranks the mutants to be selected. Initially, we consider the
same number of selected mutants for the 3 approaches,
deﬁned as the number of mutants predicted as subsuming
by Cerebro. For instance, if Cerebro predicts 20 mutants as
subsuming, then Decision Trees and Random baselines will
also select the top 20 ranked mutants. Our intention is to
compare the effectiveness reached by each approach, when
the number of selected mutants is equal.

Additionally, we study the number of equivalent mutants
selected by each approach (as these are an important source
of redundancy during mutation testing), as well as, the
required number of mutants selected by the baselines in
order to reach the same subsuming mutation score as Cerebro.
To answer RQ3 and RQ4, we study the effort (cost)
required by each approach in two ways. We measure the
human effort in terms of the number of analyzed mutants,
killable or not, that are presented to testers for analysis
(i.e., either designing a test to kill these or judging these as
equivalent), when applying mutation testing. Intuitively, for
a given set of mutants, the number of analyzed mutants can
be considerably smaller than the entire set’s size because a
test designed by analyzing one mutant can kill other mutants
as well. Hence, we also measure the computational effort in
terms of the number of test executions performed, during the
mutation analysis procedure, i.e., we count the test executions
required at every step where a new test is created. As for
RQ2, here we also study the number of test executions and
the number of mutants that require analysis by the baselines,
to reach the same subsuming mutation score as Cerebro.
7 EXPERIMENTAL RESULTS

7.1 Prediction Performance (RQ1)

Table 3 records the average (and median) performance
metrics. Figure 5 shows the performance comparison in
box plot format showing the distribution of performance

Cerebro: Static Subsuming Mutant Selection

11

Fig. 5: (RQ1) Prediction Performance Comparison: On average, Cerebro-100 outperforms Decision Trees by 2.76 times, and 2.81
times higher MCC in C, and Java Benchmark. Moreover, Cerebro-50 outperforms Decision Trees by 2.29 times, and 2.38 times
higher MCC in C, and Java Benchmark. Overall, Cerebro outperforms by 2.78 times higher MCC than Decision Trees.

TABLE 3: (RQ1) Prediction Performance of Cerebro and
Decision Trees. On average, Cerebro outperforms by 2.78 times
higher MCC than Decision Trees.

Average (and Median) Performance in C-Benchmark

Approach

MCC

F-measure

Precision

Recall

Decision Trees

0.17 (0.18)

0.25 (0.26)

0.25 (0.25)

0.25 (0.27)

Cerebro-50

0.39 (0.40)

0.34 (0.34)

0.82 (0.82)

0.21 (0.22)

Cerebro-100

0.47 (0.47)

0.41 (0.40)

0.93 (0.93)

0.26 (0.25)

Average (and Median) Performance in Java-Benchmark

Approach

MCC

F-measure

Precision

Recall

Decision Trees

0.16 (0.18)

0.28 (0.30)

0.45 (0.48)

0.21 (0.21)

Cerebro-50

0.38 (0.38)

0.42 (0.42)

0.72 (0.73)

0.31 (0.30)

Cerebro-100

0.45 (0.45)

0.51 (0.52)

0.76 (0.73)

0.39 (0.38)

indicators (MCC, F-measure, Precision, and Recall) for both
approaches in C, and Java Benchmarks.

On average, Cerebro obtains a high Precision, i.e., 0.93
and 0.76 (Cerebro-100), and 0.82 and 0.72 (Cerebro-50) in
C and Java benchmarks, respectively. Testers focusing on
mutants selected by Cerebro can be conﬁdent that these are
very likely to be subsuming, providing high utility to the
testing process. On the other hand, Recall achieved is low,
i.e., 0.26 and 0.39 (Cerebro-100), and 0.21 and 0.31 (Cerebro-50)
in C and Java benchmarks, respectively. This indicates that
many subsuming mutants are mistakenly predicted as non-
subsuming by Cerebro. In practice these mutants can still be
collaterally killed by other (mutually subsumed) subsuming
mutants correctly predicted as subsuming by Cerebro (which
is often the case, as we will show when answering RQ2 in
the following section). Needless to say, any complementary
mutation testing and mutant selection technique can be
employed to analyze the remaining mutants that are not
killed by test suites designed to kill mutants selected by
Cerebro.

On comparison with baselines, we observe that Cerebro
clearly achieves much higher prediction performance in com-
parison to Decision Trees in both benchmarks. The differences

are statistically signiﬁcant.12

In C-Benchmark, on average, Cerebro with its MCC of 0.47
(Cerebro-100), and 0.39 (Cerebro-50) outperforms Random (0.0
MCC). Cerebro also outperforms Decision Trees, on average,
with 2.76 times higher MCC and 64% improvement in F-
measure. It is worth mentioning that while Cerebro achieves
3.72 times higher precision than Decision Trees, Cerebro also
offers an improvement of 4% in Recall over Decision Trees.

In Java-Benchmark, on average, Cerebro with its MCC
of 0.45 (Cerebro-100), and 0.38 (Cerebro-50) outperforms
Random (0.0 MCC). Cerebro also outperforms Decision Trees, on
average, with 2.81 times higher MCC, and an improvement
of 82% in F-measure, 68.88% in Precision, and 85.71% in
Recall.

In summary, Cerebro offers an improvement in prediction

capability (MCC) of 2.78 times higher than Decision Trees.

7.2 Effectiveness Evaluation (RQ2)

Figure 6a and 6d show the average subsuming mutation
score (MS*) obtained when selecting the same number of
mutants (by all techniques). In C-Benchmark, on average,
Cerebro-100 obtains an MS* of 87.50%, which is 2.39 and 2.63
times higher MS* than Decision Trees and Random, respectively.
Moreover, Cerebro-50 obtains an MS* of 71.43%, which is 2.02
and 2.17 times higher MS* than Decision Trees and Random,
respectively.

In Java-Benchmark, on average, Cerebro-100 obtains an
MS* of 95.90%, which is twice higher than Decision Trees,
and 69.53% improvement over Random. Moreover, Cerebro-
50 obtains an MS* of 95.66%, which is 2.20 times higher
than Decision Trees, and 83.33% improvement over Random.
The differences are statistically signiﬁcant, according to
the computed p − value. We also compared them with the
Vargha-Delaney A measure ( ˆA12) [63], showing that Cerebro
achieves better MS* than Decision Trees, and Random, in 92.4%,
and 95.7% of the cases.

We also study the selection size needed by Decision Trees
and Random to achieve the same MS* obtained by Cerebro.
For C-Benchmark, Figure 6b shows that while Cerebro-100
selects only 2.35% of the mutants, Decision Trees, and Random

12. We compared the MCC values using Wilcoxon signed-rank test and
obtained a p − value < 5.07e−3 in comparison to Decision Trees. We
also compared the MCC values with the Vargha-Delaney A measure [63]
and observed that in all (100%) cases, Cerebro signiﬁcantly outperforms
baseline techniques.

CJava0.00.20.40.60.81.0MCCCerebro-100Cerebro-50D.TreesCJava0.00.20.40.60.81.0F-measureCerebro-100Cerebro-50D.TreesCJava0.00.20.40.60.81.0PrecisionCerebro-100Cerebro-50D.TreesCJava0.00.20.40.60.81.0RecallCerebro-100Cerebro-50D.TreesCerebro: Static Subsuming Mutant Selection

12

(a) C-Benchmark: For the same mutant
selection size, Cerebro-100 obtains an MS*
of 87.50%, while Decision Trees, and Ran-
dom obtains 36.67%, and 33.33%.
Java-Benchmark: Cerebro obtains, on av-
erage, an MS* of 95.90%, while Decision
Trees, and Random obtains 47.85%, and
56.45%.

(b) C-Benchmark: to reach the same MS*,
Cerebro-100 uses 2.35% of the mutants,
while Decision Trees, and Random 85.42%,
and 87.61%.
Java-Benchmark: Cerebro-100 uses 9.85%
of the mutants, while Decision Trees, and
Random use 44.80%, and 78.97%.

(c) C-Benchmark: Cerebro-100 selects 1.10%
equivalent mutants, while Decision Trees,
and Random select 24.44%, and 26.09%.
Java-Benchmark: 9.95% of mutants selected
by Cerebro-100 are equivalent, whereas
15.11%, and 19.33% of mutants selected by
Decision Trees, and Random are equivalent.

(d) C-Benchmark: For the same mutant
selection size, Cerebro-50 obtains an MS*
of 71.43%, while Decision Trees, and Ran-
dom obtains 34.23%, and 32.88%.
Java-Benchmark: Cerebro-50 obtains, on
average, an MS* of 95.65%, while Deci-
sion Trees, and Random obtains 43.48%,
and 52.17%.

(e) C-Benchmark: to reach the same MS*,
Cerebro-50 uses 2.52% of the mutants,
while Decision Trees, and Random 34.24%,
and 42.37%.
Java-Benchmark: Cerebro-50 uses 11.60%
of the mutants, while Decision Trees, and
Random use 41.77%, and 75.09%.

(f) C-Benchmark: Cerebro-50 selects 4.37%
equivalent mutants, while Decision Trees,
and Random select 24%, and 26.23%.
Java-Benchmark: 5.45% of mutants se-
lected by Cerebro-50 are equivalent, whereas
15.86%, and 19.26% of mutants selected by
Decision Trees, and Random are equivalent.

Fig. 6: (RQ2) Results of the Simulation - Trade off between mutant selection size and MS*.

need to select 85.42% (36.35 times higher), and 87.61% (37.28
times) of the mutants to achieve same MS* as Cerebro. Also,
Figure 6e shows that while Cerebro-50 selects only 2.52% of
the mutants, Decision Trees, and Random need to select 34.23%
(13.57 times higher), and 42.37% (16.79 times) of the mutants,
to achieve same MS* as Cerebro. For Java-Benchmark, while
Cerebro-100 selects 9.85% of the mutants, Decision Trees, and
Random need to select 44.80% (4.55 times higher), and 78.97%
(8.02 times) of the mutants, to achieve same MS* as Cerebro-
100. Also, while Cerebro-50 selects 11.60% of the mutants,
Decision Trees, and Random need to select 41.77% (3.60 times
higher), and 75.09% (6.48 times) of the mutants, to achieve
same MS* as Cerebro-50. We obtained a statistically signiﬁcant
p − value and ˆA12 when compared these values, evidencing
that Cerebro in more than 98.5%, and 99.1% of the cases,
selects fewer mutants than Decision Trees, and Random.

We also measure the percentage of equivalent mutants
selected. For C-Benchmark, Figure 6c shows that 1.10% of

mutants selected by Cerebro-100 are equivalent, whereas
24.44%, and 26.09%, of the mutants selected by Decision Trees,
and Random, are equivalent. Also, Figure 6f shows that 4.37%
of mutants selected by Cerebro-50 are equivalent, whereas
24%, and 26.23%, of the mutants selected by Decision Trees,
and Random, are equivalent. In Java-Benchmark, 9.95% of the
mutants selected by Cerebro-100 are equivalent whereas for
Decision Trees, and Random, 15.11% (51.86% more), and 19.33%
(94.27% more) selected mutants are equivalent. Also, 5.45% of
the mutants selected by Cerebro-50 are equivalent whereas for
Decision Trees, and Random, 15.86% (2.91 times higher), and
19.26% (3.53 times higher) selected mutants are equivalent.
The differences are statistically signiﬁcant. ˆA12 shows that
Cerebro in more than 90%, and 98.4% of the cases selects
fewer equivalent mutants than Decision Trees, and Random.
These results provide evidence that our approach can reduce
signiﬁcantly this long-standing problem of mutation analysis.

 & - D Y D                   0 6  & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   6 H O H F W L R Q  6 L ] H  I R U  6 D P H  0 6  & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   3 U R S R U W L R Q  R I  ( T X L Y D O H Q W  0 X W D Q W & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   0 6  & H U H E U R    '  7 U H H V 5 D Q G R P & - D Y D                   6 H O H F W L R Q  6 L ] H  I R U  6 D P H  0 6  & H U H E U R    '  7 U H H V 5 D Q G R P & - D Y D                   3 U R S R U W L R Q  R I  ( T X L Y D O H Q W  0 X W D Q W & H U H E U R    '  7 U H H V 5 D Q G R PCerebro: Static Subsuming Mutant Selection

13

(a) C-Benchmark: Same number of mu-
tants lead to MS* of 78%, 45.56%, and
41.18% for Cerebro-100, Decision Trees,
and Random.
Java-Benchmark: Cerebro-100 reaches
MS* of 94.90%, whereas Decision Trees,
and Random reach 63.59%, and 55.43%.

(b) C-Benchmark: Cerebro-100 analyzes
1.21% mutants, whereas Decision Trees, and
Random analyze 22.33%, and 22.80% to
reach the same MS* as Cerebro-100.
Java-Benchmark: Cerebro-100
analyze
3.22% mutants, whereas Decision Trees,
and Random analyze 12.07% and 18.05%
to reach same MS*.

(c) C-Benchmark: 3.70%, 53.33%, and
55.56% of the mutants selected by Cerebro-
100, Decision Trees, and Random are equiv-
alent.
Java-Benchmark: 33.48%, 52%, and 57.04%
of the mutants selected by Cerebro-100,
Decision Trees, and Random are equivalent.

(d) C-Benchmark: Same number of mu-
tants lead to MS* of 65.75%, 33.33%,
and 30.77% for Cerebro-50, Decision Trees,
and Random.
Java-Benchmark: Cerebro-50 reaches
MS* of 95.65%, whereas Decision Trees,
and Random reach 53.54%, and 49.83%.

(e) C-Benchmark: Cerebro-50 analyzes
1.02% mutants, whereas Decision Trees, and
Random analyze 11.92%, and 13.17% to
reach the same MS* as Cerebro-50.
Java-Benchmark: Cerebro-50 analyze 2.52%
mutants, whereas Decision Trees, and Ran-
dom analyze 12% and 17.19% to reach
same MS*.

(f) C-Benchmark: 11.31%, 50%, and 50%
of the mutants selected by Cerebro-50,
Decision Trees, and Random are equivalent.
Java-Benchmark: 23.72%, 56.08%, and
57.38% of the mutants selected by Cerebro-
50, Decision Trees, and Random are equiva-
lent.

Fig. 7: (RQ3) Results of the Simulation - Trade off between percentage of mutants analyzed and MS*.

7.3 Number of Analyzed Mutants (RQ3)

Figures 7a and 7d show the average subsuming mutation
score (MS*) obtained by each technique for the same number
of analyzed mutants. In C-Benchmark, on average, Cerebro-
100 achieved an MS* of 78%, which is an improvement of
89.41%, and 71.20% over the MS* of Random, and Decision
Trees, respectively. Moreover, Cerebro-50 achieved an MS*
of 65.75%, which is 2.14 times higher than Random and an
improvement of 97% over Decision Trees In Java-Benchmark,
on average, Cerebro-100 achieved an MS* of 94.90%, an
improvement of 49.24% and 71.21% over Decision Trees and
Random, respectively. Moreover, Cerebro-50 achieved an MS*
of 95.65%, an improvement of 78.65% and 91.94% over
Decision Trees and Random, respectively. The differences are
statistically signiﬁcant, according to the computed p − value
and ˆA12. We observed that Cerebro in more than 96.2%, and
98.4%, of the cases is better than Decision Trees, and Random.
We also study what should be the percentage of mutants

to be analyzed by Decision Trees and Random to achieve the
same MS* as Cerebro. For C-Benchmark, Figure 7b shows that
while Cerebro-100 analyzes 1.21% mutants, Decision Trees, and
Random need to analyze 22.33% (18.45 times higher), and
22.80% (18.84 times higher) of mutants to reach same MS*
as Cerebro-100. Also, Figure 7e shows that while Cerebro-50
analyzes 1.02% mutants, Decision Trees, and Random need
to analyze 11.92% (11.58 times higher), and 13.17% (12.78
times higher) of mutants to reach same MS* as Cerebro-50. In
Java-Benchmark, while Cerebro-100 analyzes 3.22% mutants,
Decision Trees, and Random need to analyze 12.07% (3.75
times higher), and 18.05% (5.61 times higher) of mutants to
reach same MS* as Cerebro-100. Moreover, while Cerebro-50
analyzes 2.52% mutants, Decision Trees, and Random need to
analyze 12.00% (4.76 times higher), and 17.19% (6.82 times)
of mutants to reach same MS* as Cerebro-50. We obtained
a statistically signiﬁcant p − value and ˆA12, showing that
Cerebro in more than 99% of the cases analyzes less mutants
than Decision Trees and Random.

 & - D Y D                   0 6  & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   $ Q D O \ ] H G  0 X W D Q W V  I R U  6 D P H  0 6  & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   ( T Y  0 X W D Q W V  I R U  V D P H  $ Q D O \ ] H G & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   0 6  & H U H E U R    '  7 U H H V 5 D Q G R P & - D Y D                   $ Q D O \ ] H G  0 X W D Q W V  I R U  6 D P H  0 6  & H U H E U R    '  7 U H H V 5 D Q G R P & - D Y D                   ( T Y  0 X W D Q W V  I R U  V D P H  $ Q D O \ ] H G & H U H E U R    '  7 U H H V 5 D Q G R PCerebro: Static Subsuming Mutant Selection

14

(a) C-Benchmark: For same
number of
test executions,
Cerebro-100 obtains an MS* of
74%, while Decision Trees, and
Random obtain 45.45%, and
44.44%.
Java-Benchmark: Cerebro-100
obtains MS* of 95.65%, while
Decision Trees, and Random ob-
tain 57.38%, and 60.40%.

(b) C-Benchmark: Cerebro-100
requires 291 test executions,
while Decision Trees, and Ran-
dom require 3,345, and 3,149 to
reach the same MS* as Cerebro-
100.
Java-Benchmark: 65,741 test
executions are required by
Cerebro-100, while Decision
Trees, and Random require
517,040, and 795,304.

(c) C-Benchmark: For same
number of
test executions,
Cerebro-50 obtains an MS* of
65.52%, while Decision Trees,
and Random obtain 36.20%,
and 36.99%.
Java-Benchmark: Cerebro-50
obtains MS* of 95.65%, while
Decision Trees, and Random ob-
tain 50.41%, and 60.21%.

(d) C-Benchmark: Cerebro-50
requires 125 test executions,
while Decision Trees, and Ran-
dom require 1,785, and 2,182 to
reach the same MS* as Cerebro-
50.
Java-Benchmark: 50,622 test
executions are required by
Cerebro-50, while Decision Trees,
and Random require 560,866,
and 894,494.

Fig. 8: (RQ4) Results of the Simulation - Trade off between number of test executions and MS*.

We also measure the percentage of equivalent mutants
analyzed by each technique. For C-Benchmark, Figure 7c
shows that, on average, Cerebro-100 analyzes 3.70% equiva-
lent mutants, while 53.33% (14.41 times higher), and 55.56%
(15.02 times higher) of the mutants analyzed by Decision Trees,
and Random are equivalent. Also, 7f shows that Cerebro-50
analyzes 11.31% equivalent mutants, while 50% (4.42 times
higher) of the mutants analyzed by Decision Trees and Random
are equivalent. For Java-Benchmark, on average, 33.48% of
the mutants analyzed by Cerebro-100 are equivalent, while
Decision Trees, and Random analyze 52% (55.31% more), and
57.04% (70.37% more) equivalent mutants. Also, 23.72% of
the mutants analyzed by Cerebro-50 are equivalent, while
Decision Trees, and Random analyze 56.08% (2.36 times higher),
and 57.38% (2.42 times higher) equivalent mutants. This
indicates that the baselines suggest the consumption of a
large effort to analyze redundant mutants, in comparison
to Cerebro. The differences are statistically signiﬁcant. ˆA12
suggests that Cerebro in more than 98% of the cases analyzes
fewer equivalent mutants than Decision Trees, and Random.

7.4 Number of Test Executions (RQ4)

Figure 8a and 8c show the average subsuming mutation
score (MS*) when the number of test executions are ﬁxed. In
C-Benchmark, on average, Cerebro-100 achieves an MS* of
74%, outperforming Decision Trees, and Random by 62.82%,
and 66.52% (Decision Trees, and Random achieve 45.45%, and
44.44% of MS*). Also, Cerebro-50 achieves an MS* of 65.52%,
outperforming Decision Trees, and Random by 80.95%, and
77.14% (Decision Trees, and Random achieve 36.21%, and
36.99% of MS*). In Java-Benchmark, on average, Cerebro-100
and Cerebro-50 achieve an MS* of 95.65% in both simulations,
an improvement of approx. 67%, and 58% over Decision Trees,
and Random (Decision Trees, and Random achieve 57.38%, and
60.40% of MS* in ﬁrst simulation when compared against
Cerebro-100, and 50.41%, and 60.21% of MS* in the second
comparison simulation against Cerebro-50). We obtained a
statistically signiﬁcant p − value. Also ˆA12 suggests that

Cerebro in 94.15%, and 95.7%, of the cases is better than
Decision Trees, and Random.

We also measure the test executions required by the
baselines to achieve the same MS* as Cerebro. Figure 8b shows
that, in C-Benchmark, Cerebro-100 requires 291 test executions
(median), while Decision Trees, and Random require 3,345, and
3,149. Also, Figure 8d shows that Cerebro-50 requires 125
test executions (median), while Decision Trees, and Random
require 1,785, and 2,182. This shows that Cerebro-100 is 10-
12 times less and Cerebro-50 is 14-17 times less expensive
(computationally) than the baselines.

In Java-Benchmark, Decision Trees, and Random require
517,040, and 795,304 test executions (median) to achieve the
same MS* as Cerebro-100, for which 65,741 test executions
are required. Moreover, Decision Trees, and Random require
560,866, and 894,494 test executions to achieve the same MS*
as Cerebro-50, for which 50,622 test executions are required.
This shows that the baselines require 7 to 12 times, and 11 to
17 times higher computational effort than Cerebro-100, and
Cerebro-50.

These differences are statistically signiﬁcant. ˆA12 value
indicates that in more than 98.7% of the cases, Cerebro
executes fewer tests than Decision Trees and Random.

8 DISCUSSION

Cerebro is a learning-based method, and thus its performance
depends on a number of parameters and design decisions we
made. To this end, we discuss the key (intuitive) parameters
that make the Machine Translation approach we use effective
(Section 8.1), together with empirical results demonstrating
the potential impact on the model’s performance given the
design decisions of using unabstracted code sequences (Sec-
tion 8.2), sequences with decreased length during training
(Section 8.3), and the impact of assuming unkilled mutants
as equivalent mutants during testing (Section 8.4).

 & - D Y D                   0 6  & H U H E U R     '  7 U H H V 5 D Q G R P &    H     H     H     H     H     H  7 H V W  ( [ H F X W L R Q  I R U  V D P H  0 6  & H U H E U R     '  7 U H H V 5 D Q G R P - D Y D    H     H     H     H     H     H  & H U H E U R     '  7 U H H V 5 D Q G R P & - D Y D                   0 6  & H U H E U R    '  7 U H H V 5 D Q G R P &    H     H     H     H     H     H  7 H V W  ( [ H F X W L R Q  I R U  V D P H  0 6  & H U H E U R    '  7 U H H V 5 D Q G R P - D Y D    H     H     H     H     H     H  & H U H E U R    '  7 U H H V 5 D Q G R PCerebro: Static Subsuming Mutant Selection

TABLE 4: Impact of the abstraction process and sequence
length in Cerebro’s prediction performance: On average, MCC
is decreased by 18% with unabstracted code and decreased
by 24% with sequence length 25.

Average (and Median) Performance in C-Benchmark

Approach

MCC

F-measure

Precision

Recall

Cerebro-100

0.47 (0.47)

0.41 (0.40)

0.93 (0.93)

0.26 (0.25)

Cerebro-50

0.39 (0.40)

0.34 (0.34)

0.82 (0.82)

0.21 (0.22)

Cerebro-unabs

0.32 (0.31)

0.28 (0.27)

0.70 (0.73)

0.17 (0.16)

Cerebro-25

0.30 (0.29)

0.27 (0.28)

0.64 (0.61)

0.17 (0.18)

Average (and Median) Performance in Java-Benchmark

Approach

MCC

F-measure

Precision

Recall

Cerebro-100

0.45 (0.45)

0.51 (0.52)

0.76 (0.73)

0.39 (0.38)

Cerebro-50

0.38 (0.38)

0.42 (0.42)

0.72 (0.73)

0.31 (0.30)

Cerebro-unabs

0.31 (0.34)

0.43 (0.41)

0.56 (0.53)

0.36 (0.38)

Cerebro-25

0.29 (0.32)

0.42 (0.41)

0.51 (0.45)

0.36 (0.37)

8.1 Why Cerebro is a good candidate for subsuming
mutant prediction?

There are three main factors that make Machine Transla-
tion a good candidate for subsuming mutant prediction.
The ﬁrst one is that it learns to select mutants using the
exact local context (entire code snippet composed of 50-100
tokens, represented as a sequence), while previous work
considers AST and data-ﬂow abstractions [11], ignoring the
exact formulation of the code snippet. In a sense, the key
determining factor is the sequence that code tokens appear
in the local context (considered code snippet). The second
reason is that the machine translator includes a powerful
self-attention mechanism, which together with the encoder-
decoder architecture makes the learning resistant to noise
[60], and able to learn out of imbalanced data. Overall,
previous research has shown that this architecture often
makes the best predictions for many NLP tasks [20]. This
is actually the reason why Machine Translation has been
successfully used in code analysis tasks such as mutant
generation, code clone detection, test assertions generation,
etc. The third reason is the diversity of the selected mutants,
i.e., Cerebro selects a few mutants per code block, which
allows eliminating local redundancies, while spreading
testing across the entire code-base.

15

TABLE 5: Impact of noise in evaluation on all approaches’
performance (MS*): Cerebro’s and Decision Trees’ performances
are more or less inversely related to the noise in evaluation.
For Random selection, the performance also deteriorated
in most of the cases, with exceptions of 10% noise in C
benchmark, and 6% and 8% noise in Java benchmark where
Random’s performance improved by 6.48%, and 0.23% and
1.26% improved MS*, respectively.

Performance Change % (Median) in MS* w.r.t. noise
for C-Benchmark

Noise (%)

Cerebro Decision Trees

2% ↓ -2.24%
4% ↓ -3.10%
6% ↓ -4.67%
8% ↓ -5.78%
10% ↓ -7.06%

Random
↓ -3.61% ↓ -13.91%
↓ -8.89%
↓ -3.59%
↓ -0.95%
↓ -3.83%
↓ -8.17%
↓ -7.40%
↑ +6.48%
↓ -6.53%

Performance Change % (Median) in MS* w.r.t. noise
for Java-Benchmark

Noise (%)

Cerebro Decision Trees
↓ -1.17%
↓ -1.89%
↓ -2.76%
↓ -3.76%
↓ -4.63%

2% ↓ -2.19%
4% ↓ -4.55%
6% ↓ -6.15%
8% ↓ -7.50%
10% ↓ -8.61%

Random
↓ -0.16%
↓ -0.39%
↑ +0.23%
↑ +1.26%
↓ -2.80%

8.3 Impact of reducing the sequence length

We also analyzed the impact of reducing the length of
sequences that we use to train our models and how it
affects the model prediction performance (RQ1). In this
experiment, we reduced the sequence length from 50 tokens
per sequence to 25 tokens per sequence. Figure 9 and Table 4
shows the average and median scores achieved by the
models. For simulation details on Effectiveness Evaluation
(RQ2), Number of Analyzed Mutants (RQ3) and Number of
Test Executions (RQ4), please refer to our online repository.
From these results we found that reducing the length of
sequences used by the models to train also deteriorated the
model prediction performance for projects in both C and
Java benchmarks. For C-Benchmark, the model performance
deteriorated by 23.5% in MCC, 22.2% in Precision and
18.1% in Recall. For Java-Benchmark, although we found
an improvement of 18.7% in Recall, the overall performance
deteriorated by 24.7% in MCC and 28.6% in Precision.

8.2 Impact of removing code abstraction

We analyzed the impact of using unabstracted code se-
quences to train our models instead of proposed abstracted
code sequences and how it affects the model prediction
performance (RQ1). In this experiment, we just removed the
code comments and kept everything else as it is. We found a
prediction performance reduction for projects in both C and
Java benchmarks. For C-Benchmark, the model performance
deteriorated by 18.9% in MCC, 14.4% in Precision and
18.1% in Recall. For Java-Benchmark, although we found
an improvement of 15.4% in Recall, the overall performance
deteriorated by 17.9% in MCC and 22.5% in Precision.

8.4 Impact of considering equivalent, mutants that are
impact of potential mistakes in our
subsuming, i.e.,
evaluation

In our experiments, we considered the mutants that were
not killed by our test suite as unkillable a.k.a. equivalent.
Although this being an undecidable problem (as we elab-
orated in Section 6.2), we analyzed the impact of what
would have happened if the mutants that we considered
as equivalent were subsuming instead. Hence, we addressed
this by introducing noise in our evaluation, i.e., we assumed
2% equivalent mutants in our evaluation set as subsuming

Cerebro: Static Subsuming Mutant Selection

16

Fig. 9: Impact of the abstraction process and sequence length in Cerebro’s prediction performance: On average, MCC is
decreased by 18% with unabstracted code and decreased by 24% with sequence length 25.

Fig. 10: Impact of noise in evaluation on all approaches’ performance (MS*): Cerebro’s and Decision Trees’ performances
are more or less inversely related to the noise in evaluation. For Random selection, the performance also deteriorated in
most of the cases, with exceptions of 10% noise in C benchmark, and 6% and 8% noise in Java benchmark where Random’s
performance improved by 6.48%, and 0.23% and 1.26% improved MS*, respectively.

and analyzed the change in performance (MS* achieved)
for all the approaches (Cerebro, Decision Trees and Random).
We gradually increased the noise percentage from 2% till
10% (i.e., 2%, 4%, 6%, 8%, 10%) and analyzed the change in
behaviour for all the approaches (i.e., change in MS*), if it
increases or decreases with increase in noise.

We found that Cerebro’s and Decision Trees’ performances
are more or less inversely related to the noise in evaluation
(Figure 10). Higher the noise, lower the MS* achieved by
both the approaches (with an exception of 10% noise in C
benchmark for Decision Trees where Decision Trees performed
better than in case of 8% noise, as detailed in Table 5).
For Random selection, the performance also deteriorated in
most of the cases, with an exception of 10% noise in C
benchmark, and 6% and 8% noise in Java benchmark where
Random’s performance improved by 6.48%, and 0.23% and
1.26% improved MS*, respectively. Despite the reduction in
performance due to introduced noise, Cerebro still achieves
higher MS* than the baselines (Figure 10).

9 THREATS TO VALIDITY
External Validity: Threats may relate to the subjects we used.
Although our evaluation expands to both C and Java projects
of different sizes, the results may not generalize to other
projects or programming languages. We consider this threat
as low since we have a large sample of programs, i.e., we
perform one of the largest mutation testing studies to date.
Other external threat lies in the operators we used, since
our prediction approach might not work for different types of
mutants. To reduce this threat, we employ modern mutation
tools, for both C and Java that implement a large variety of

mutation operators. For the C-Benchmark, taken from [12],
816 simple operators across 18 categories were considered;
while for creating our Java-Benchmark, we consider the
group “ALL” of mutation operators provided by Pitest [17],
resulting in 112 simple operators across 29 categories.

Internal Validity: Threats may relate to the restriction that
we impose on sequence length, i.e., a maximum of 100 tokens.
This was done to enable reasonable model training time,
approximately 740 hours. Moreover, restricting the sequence
length to 50 assisted to reach an appropriate training time of
360 hours. However, it resulted in a prediction performance
deterioration of approximately 15%, as discussed in Section 8.
Other threats maybe due to the use of machine translation
for classiﬁcation. This choice was made for simplicity, to use
the related framework out of the box, similar to the related
studies [61], [62]. Still a potential “sequence to class classiﬁer”
may yield better results, though such improvements should
be marginal given the low number of unexpected labels we
get, i.e., on average, 2.15% of the mutants do not get a valid
label (4.2% in C and 0.1% in Java).

Threats may also relate to the features we implemented
for training the Decision Trees baseline. We follow the guide-
lines provided in [11], to extract the 16 features for our
Java dataset. Unfortunately, many of the 28 features for C
programs presented in [11] depend on the semantic of the C
language, that we found unfeasible to be replicated in Java.
However, the prediction performance of Decision Trees in Java
are in line with the results obtained for C, indicating that the
impact of this threat is low.

Other internal validity threats could be related to the test
suites we used and the mutants considered as subsuming

CJava0.00.20.40.60.81.0MCCCerebro-100Cerebro-50Cerebro-unabsCerebro-25CJava0.00.20.40.60.81.0F-measureCerebro-100Cerebro-50Cerebro-unabsCerebro-25CJava0.00.20.40.60.81.0PrecisionCerebro-100Cerebro-50Cerebro-unabsCerebro-25CJava0.00.20.40.60.81.0RecallCerebro-100Cerebro-50Cerebro-unabsCerebro-25CJava0.00.20.40.60.81.0MS*Cerebro0% noise2% noise4% noise6% noise8% noise10% noiseCJava0.00.20.40.60.81.0MS*Decision TreesCJava0.00.20.40.60.81.0MS*RandomCerebro: Static Subsuming Mutant Selection

17

and equivalent. To deal with this issue, we used well-tested
programs and state-of-the-art tool to generate extensive
pools of tests (KLEE [10], SEMu [12], and EvoSuite [23]).
Since identifying subsuming and equivalent mutants is
an undecidable problem, in our experimental setup, we
approximate them through an extensive pool of tests. This
has been a typical process followed in related mutation
testing studies [3], [28], [35], [46], [47]. To be more accurate,
our underlying assumption is that the extensive pool of
tests used in our experiments are a valid representation of
all possible tests that a tester can manually or automatically
generate. This assumption allowed us to identify the minimal
set of mutants (i.e., subsuming mutants) that a tester needs
to kill in order to kill every other killable mutant (i.e.,
subsumed mutants). Also, we assumed that unkilled mutants
are equivalent. Even if this may not be the case, it is likely
that the testers guided by mutation won’t be able to kill
all the killable mutants. Here it must be noted that since
Cerebro is quite precise, its feeding with less noisy data,
i.e., correct labels, will make it perform better, i.e., more
accurate labelling in training will result in better predictions.
Nevertheless, we also investigate the impact of having such
noisy data and found minor discrepancies, please refer to
Section 8.4.

Cerebro’s use may also pose additional threats. In par-
ticular, Cerebro required approximately 5 minutes for pre-
processing of the projects and 5 minutes for classiﬁcation
(decoding results). While this time overhead is low, compared
to the hours of test executions, it may still be important.
Although our implementation is non-optimal and involves
no parallelism, however our encoding and decoding can
easily be parallelized, since mutant instances are independent
of one another.

Construct Validity: Our assessment metrics, subsuming
mutation score, number of equivalent mutants and number
of test executions may not reﬂect the actual testing cost / ef-
fectiveness values. These metrics have been suggested by
literature [5], [35], [48] and are intuitive, i.e., number of
selected and analyzed mutants essentially simulate the
manual effort involved by testers, subsuming mutation score
the level of covering the test requirements [3], [47], and
number of test executions capture the computational effort
involved. Here it should be noted that automated test gener-
ation tools may reduce this cost but they require testers to
check the related test oracles. Similarly, equivalent detection
techniques and related heuristics may also reduce the manual
effort involved [33]. Though, in C we applied TCE (Trivial
Compiler Equivalence) [25], [32] to ﬁlter out equivalent
and duplicated mutants and our approach still provided
signiﬁcant beneﬁts. Similarly, the use of test executions
capture the computational effort involved independently
of the test execution framework and optimizations used [15],
[48], [64], [69], the machines and the level of parallelization
used during test execution. Nevertheless, the differences are
substantial making such threats unlikely to happen. Overall,
we mitigate these threats by following suggestions from
mutation testing literature [5], [35], [48], using state-of-the-
art tools, performing several simulations, forming very large
and diverse test pools, and got consistent and stable results
across our subjects.

10 RELATED WORK

Mutation testing has been established as one of the strongest
test criteria [3], [14]. Despite its potential, mutation is
considered to be expensive since it introduces too many
mutants. To this end, random mutant sampling [19], [50]
and selective mutation [44] (restricting mutant instances
according to their types) have been proposed as potential
solutions. Unfortunately, these approaches fail to capture
relevant program semantics and performing similarly to
random mutant sampling [11], [35], [68].

Other attempts regard the selection of relevant program
locations, which should be mutated. Sun et al. [58] proposed
selecting mutants that reside in diverse static control ﬂow
graph paths. Gong et al. [24] identiﬁed dominator nodes
(using static control ﬂow graph) to select mutants.

More recent attempts regard the identiﬁcation of inter-
esting mutants (pairs of mutant types and related locations).
Petrovic and Ivankovic [52] and Just et al. [30] proposed using
the code AST in order to identify “useful” mutants. Petrovic
and Ivankovic used what they called arid nodes (special
AST nodes), while Just et al. used the AST parent and child
nodes, in order to identify high utility mutants. Mirshokraie
et al. [42] employed complexity metrics together with test
executions to select killable mutants. Similarly, Titcheu et al.
[11] employed static features, including data ﬂow analysis,
complexity and AST information, in order to perform mutant
selection, wrt mutants linked with real faults.

In our analysis we approximate the performance of the
above approaches through the two baselines we adopt and
show that our approach signiﬁcantly outperforms these.
Random mutant sampling is performing comparably to
operator mutant selection [68], while the supervised baseline
we consider simulates the AST-based and complexity-based
approaches.

Perhaps the closest work to ours, is from Marcozzi et al.
[39], which attempts to identify subsumed mutants using
veriﬁcation techniques (such as weakest precondition). While
Marcozzi et al.’s approach is particularly powerful, it targets
weak mutation and not strong as we do. This results in
several false positives in the strong mutation case due to
failed error propagation [14]. Moreover, Marcozzi et al.’s
approach is time consuming, requires complex computations
and infrastructure while Cerebro is fast and simple. Never-
theless, future research should attempt to combine these
methods.

Tufano et al. [61] proposed using Neural Machine Trans-
lation to learn mutations from bug ﬁxes with the aim of
introducing mutations that are syntactically similar to real
bugs. Cerebro relies on the same technology, though it targets
a different problem; the identiﬁcation of high utility mutants,
among those given by regular mutation testing tools, while
Tufano et al. aim at generating mutants regardless of their
potential. This indicates that Cerebro can complement Tufano
et al by selecting relevant mutants. Nevertheless, we focus on
subsuming mutants, that could help measuring test adequacy
and designing test suites, which are unlikely to be supported
by Tufano et al. as there is no notion of subsumption in the
bug-ﬁxing sets they use. Moreover, we make no assumption
on the availability and repetitiveness of historical bugs and
their ﬁxes.

Cerebro: Static Subsuming Mutant Selection

18

Predictive mutation testing (PMT) [67] attempts to predict
whether a given test can kill a given mutant without
performing any mutant execution. The approach relies on a
set of both static and dynamic features (relying on coverage
and code attributes) and achieves relatively good results
(on average with 10% error). Though, PMT mainly targets
intra-project predictions, while Cerebro targets inter-project.
Nevertheless, PMT is incomparable to Cerebro since it aims
at evaluating test execution results, while we do mutant
selection prior to any test execution. In other words, we aim
at identifying the mutants to be used for test design/gen-
eration, while PMT to verify whether mutants are killed by
some tests. Therefore, the two methods target different but
complementary problems.

Evolutionary Mutation Testing (EMT) [21] utilises dy-
namic features (execution traces) in order to identify inter-
esting locations and mutant types. As such, EMT requires
tests and user feedback, which make it different but com-
plementary to ours; Cerebro can set a starting point for EMT
or integrate its predictions within EMT’s ﬁtness function.
Higher-order mutation [28] aims at dynamically optimizing
mutants based on given test suites. This means that Cerebro
can be directly applied to support test generation prior to
any test generation, while higher-order mutation is only
applicable after test generation. Perhaps more importantly,
Cerebro does not introduce any expensive dynamic mutant
execution, while higher-order mutation introduces major
mutant execution overheads.

11 CONCLUSION AND FUTURE WORK

We presented Cerebro, a method that learns to select subsum-
ing mutants (subset of mutants that subsumes the others,
i.e., tests killing them also kill all the mutants of the given
mutant set) from given mutant sets. Experiments with 58
programs showed that Cerebro identiﬁed subsuming mutants
with 0.85 precision and 0.33 recall at an inter-project scenario
(trained on different projects than the ones it was evaluated).
These predictions enable testers designing test cases capable
of killing more than two times the subsuming mutants that
they would kill if they were using either randomly selected
mutants or another previously proposed machine learning-
based mutant selection technique. At the same time Cerebro
entails the analysis of 66% fewer equivalent mutants and
90% less mutant executions, indicating a large reduction on
the practical effort/cost of the approach.

Recently, it has become increasingly common to pre-
train the entire model on a data-rich task, which causes the
model to develop general-purpose abilities and knowledge
that can then be transferred to downstream tasks [53]. In
this practice aka Transfer Learning and its applications to
computer vision [27], [45], pre-training is typically done
via supervised learning on a large labeled data set like
ImageNet [54]. In contrast, modern techniques for transfer
learning in Natural Language Processing (NLP) often pre-
train using unsupervised learning on unlabeled data [20],
[37]. The resulting pre-trained models are further trained
on specialized datasets to accomplish the desired tasks.
Unsupervised pre-training for NLP is attractive and seems a
good ﬁt for neural networks as it have been shown to exhibit
remarkable scalability, i.e., it is often possible to achieve better

performance simply by training a larger model on a larger
data set [26], [29], [38], [55]. It will be worthwhile to explore
such available pre-trained models [22], [40] and if these can
be further reﬁned to address our speciﬁc prediction task.

On the other hand, as we have shown that Cerebro is
proﬁcient in capturing the silent features and patterns of the
code context, it is promising to explore Cerebro in security-
speciﬁc task such as prediction of zero-day vulnerabilities,
which pose a very high risk [66]. Vulnerabilities are fewer
in comparison to defects, limiting the information one can
learn from. Also, their identiﬁcation requires an attacker’s
mindset [43], which developers or code reviewers may
not possess. Lastly, the continuous growth of codebases
makes it difﬁcult to investigate them entirely and track all
code changes. For instance, Linux kernel, which is one of
the projects with the highest number of publicly reported
vulnerabilities, reached 27.8 million LoC (Lines of Codes) at
the beginning of 2020 [36]. Hence, it will also be rewarding
to explore Cerebro in this line of work.

ACKNOWLEDGMENT
This work is supported by the Luxembourg National
Research Funds (FNR) through the INTER project grant,
INTER/ANR/18/12632675/SATOCROSS.

REFERENCES

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, An-
drew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,
Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015. Software
available from tensorﬂow.org.

[2] Allen Troy Acree. On Mutation. Phd thesis, School of Information
and Computer Science, Georgia Institute of Technology, Atlanta,
Georgia, 1980.

[3] P. Ammann, M. E. Delamaro, and J. Offutt. Establishing theoretical
In 2014 IEEE Seventh International
minimal sets of mutants.
Conference on Software Testing, Veriﬁcation and Validation, pages 21–30,
2014.

[4] Paul Ammann and Jeff Offutt.

Introduction to Software Testing.

[5]

Cambridge University Press, USA, 1 edition, 2008.
J. H. Andrews, L. C. Briand, Y. Labiche, and A. S. Namin. Using
mutation analysis for assessing and comparing testing coverage
criteria. IEEE Transactions on Software Engineering, 32(8):608–624,
2006.

[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
machine translation by jointly learning to align and translate. In
Yoshua Bengio and Yann LeCun, editors, 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, 2015.

[7] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philémon
Brakel, and Yoshua Bengio. End-to-end attention-based large
vocabulary speech recognition. In 2016 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 4945–4949,
2016.

[8] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le.
Massive exploration of neural machine translation architectures. In
Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pages 1442–1451, Copenhagen, Denmark,
September 2017. Association for Computational Linguistics.
[9] Timothy Alan Budd and Dana Angluin. Two Notions of Correct-
ness and Their Relation to Testing. Acta Informatica, 18(1):31–45,
March 1982.

Cerebro: Static Subsuming Mutant Selection

19

[10] Cristian Cadar, Daniel Dunbar, and Dawson Engler. Klee: Unas-
sisted and automatic generation of high-coverage tests for complex
systems programs. In Proceedings of the 8th USENIX Conference on
Operating Systems Design and Implementation, OSDI’08, page 209–224,
USA, 2008. USENIX Association.

[11] Thierry Titcheu Chekam, Mike Papadakis, Tegawendé F. Bissyandé,
Yves Le Traon, and Koushik Sen. Selecting fault revealing mutants.
Empirical Software Engineering, 25(1):434–487, 2020.

[12] Thierry Titcheu Chekam, Mike Papadakis, Maxime Cordy, and
Yves Le Traon. Killing stubborn mutants with symbolic execution.
ACM Trans. Softw. Eng. Methodol., 30(2), January 2021.

[13] Thierry Titcheu Chekam, Mike Papadakis, and Yves Le Traon. Mart:
A mutant generation tool for llvm. In Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE
2019, page 1080–1084, New York, NY, USA, 2019. Association for
Computing Machinery.

[14] Thierry Titcheu Chekam, Mike Papadakis, Yves Le Traon, and Mark
Harman. An empirical study on mutation, statement and branch
coverage fault revelation that avoids the unreliable clean program
assumption. In Sebastián Uchitel, Alessandro Orso, and Martin P.
Robillard, editors, Proceedings of the 39th International Conference on
Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28,
2017, pages 597–608. IEEE / ACM, 2017.

[15] Lingchao Chen and Lingming Zhang. Speeding up mutation
testing via regression test selection: An extensive study. In 11th
IEEE International Conference on Software Testing, Veriﬁcation and
Validation, ICST 2018, Västerås, Sweden, April 9-13, 2018, pages 58–69.
IEEE Computer Society, 2018.

[16] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry
Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
Learning phrase representations using RNN encoder–decoder
In Proceedings of the 2014
for statistical machine translation.
Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1724–1734, Doha, Qatar, October 2014. Association
for Computational Linguistics.

[17] Henry Coles, Thomas Laurent, Christopher Henard, Mike Pa-
padakis, and Anthony Ventresque. Pit: A practical mutation
testing tool for java (demo). In Proceedings of the 25th International
Symposium on Software Testing and Analysis, ISSTA 2016, page
449–452, New York, NY, USA, 2016. Association for Computing
Machinery.

[18] M. L. Collard and J. I. Maletic. srcml 1.0: Explore, analyze, and
manipulate source code. In 2016 IEEE International Conference on
Software Maintenance and Evolution (ICSME), pages 649–649, 2016.
[19] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints on test data
IEEE Computer,

selection: Help for the practicing programmer.
11(4):34–41, April 1978.

[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional transformers
for language understanding. Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–
4186, 2019.

[21] Juan José Domínguez-Jiménez, Antonia Estero-Botaro, Antonio
García-Domínguez, and Inmaculada Medina-Bulo. Evolutionary
mutation testing. Inf. Softw. Technol., 53(10):1108–1123, 2011.
[22] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,
Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and
Ming Zhou. Codebert: A pre-trained model for programming and
natural languages. In Trevor Cohn, Yulan He, and Yang Liu, editors,
Findings of the Association for Computational Linguistics: EMNLP 2020,
Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings
of ACL, pages 1536–1547. Association for Computational Linguistics,
2020.

[23] Gordon Fraser and Andreas Zeller. Mutation-driven generation
of unit tests and oracles. In Proceedings of the ACM International
Symposium on Software Testing and Analysis, ISSTA ’10, pages 147–
158, New York, NY, USA, 2010. ACM.

[24] Dunwei Gong, Gongjie Zhang, Xiangjuan Yao, and Fanlin Meng.
Mutant reduction based on dominance relation for weak mutation
testing. Information & Software Technology, 81:82–96, 2017.

[25] Farah Hariri, August Shi, Vimuth Fernando, Suleman Mahmood,
and Darko Marinov. Comparing mutation testing at the levels of
source code and compiler intermediate representation. In 12th IEEE

Conference on Software Testing, Validation and Veriﬁcation, ICST 2019,
Xi’an, China, April 22-27, 2019, pages 114–124. IEEE, 2019.

[26] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F.
Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary,
Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. CoRR, abs/1712.00409, 2017.

[27] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
Jonathan Long, Ross B. Girshick, Sergio Guadarrama, and Trevor
Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. In Kien A. Hua, Yong Rui, Ralf Steinmetz, Alan Hanjalic,
Apostol Natsev, and Wenwu Zhu, editors, Proceedings of the ACM
International Conference on Multimedia, MM ’14, Orlando, FL, USA,
November 03 - 07, 2014, pages 675–678. ACM, 2014.

[28] Yue Jia and Mark Harman. Higher order mutation testing. Inf.

Softw. Technol., 51(10):1379–1393, 2009.

[29] Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and
Yonghui Wu. Exploring the limits of language modeling. CoRR,
abs/1602.02410, 2016.

[30] René Just, Bob Kurtz, and Paul Ammann. Inferring mutant utility
from program context. In Proceedings of the 26th ACM SIGSOFT
International Symposium on Software Testing and Analysis, Santa
Barbara, CA, USA, July 10 - 14, 2017, pages 284–294, 2017.

[31] M. Kintis, M. Papadakis, and N. Malevris. Evaluating mutation
testing alternatives: A collateral experiment. In 2010 Asia Paciﬁc
Software Engineering Conference, pages 300–309, 2010.

[32] Marinos Kintis, Mike Papadakis, Yue Jia, Nicos Malevris, Yves Le
Traon, and Mark Harman. Detecting trivial mutant equivalences
via compiler optimisations. IEEE Trans. Software Eng., 44(4):308–333,
2018.

[33] Marinos Kintis, Mike Papadakis, and Nicos Malevris. Employing
second-order mutation for isolating ﬁrst-order equivalent mutants.
Softw. Test. Veriﬁcation Reliab., 25(5-7):508–535, 2015.

[34] B. Kurtz, P. Ammann, M. E. Delamaro, J. Offutt, and L. Deng.
Mutant subsumption graphs. In 2014 IEEE Seventh International
Conference on Software Testing, Veriﬁcation and Validation Workshops,
pages 176–185, 2014.

[35] Bob Kurtz, Paul Ammann, Jeff Offutt, Márcio E. Delamaro, Mariet
Kurtz, and Nida Gökçe. Analyzing the validity of selective
mutation with dominator mutants. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering, FSE 2016, page 571–582, New York, NY, USA, 2016.
Association for Computing Machinery.

[36] Linux in 2020: 27.8 million lines of code in the kernel, 1.3 million in
systemd. https://www.theregister.com/2020/01/06/linux_2020_
kernel_systemd_code/, (accessed October 12, 2021).

[37] Zhuang Liu, Wayne Lin, Ya Shi, and Jun Zhao. A robustly
optimized bert pre-training approach with post-training. In China
National Conference on Chinese Computational Linguistics, pages 471–
484. Springer, 2021.

[38] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He,
Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van
Der Maaten. Exploring the limits of weakly supervised pretraining.
In Proceedings of the European conference on computer vision (ECCV),
pages 181–196, 2018.

[39] Michaël Marcozzi, Sébastien Bardin, Nikolai Kosmatov, Mike
Papadakis, Virgile Prevosto, and Loïc Correnson. Time to clean
your test objectives. In Michel Chaudron, Ivica Crnkovic, Marsha
Chechik, and Mark Harman, editors, Proceedings of the 40th Inter-
national Conference on Software Engineering, ICSE 2018, Gothenburg,
Sweden, May 27 - June 03, 2018, pages 456–467. ACM, 2018.

[40] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David
Nader-Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele
Bavota. Studying the usage of text-to-text transfer transformer to
support code-related tasks. CoRR, abs/2102.02017, 2021.

[41] B.W. Matthews. Comparison of the predicted and observed
secondary structure of t4 phage lysozyme. Biochimica et Biophysica
Acta (BBA) - Protein Structure, 405(2):442 – 451, 1975.

[42] Shabnam Mirshokraie, Ali Mesbah, and Karthik Pattabiraman.
IEEE
Guided mutation testing for javascript web applications.
Trans. Software Eng., 41(5):429–444, 2015.

[43] Patrick Morrison, Kim Herzig, Brendan Murphy, and Laurie
Williams. Challenges with applying vulnerability prediction
In Proceedings of the 2015 Symposium and Bootcamp on
models.
the Science of Security, pages 1–9, 2015.

[44] A. Jefferson Offutt, Ammei Lee, Gregg Rothermel, Roland H. Untch,
and Christian Zapf. An experimental determination of sufﬁcient

Cerebro: Static Subsuming Mutant Selection

20

mutant operators. ACM Trans. Softw. Eng. Methodol., 5(2):99–118,
1996.

[45] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning
and transferring mid-level image representations using convolu-
In Proceedings of the IEEE conference on
tional neural networks.
computer vision and pattern recognition, pages 1717–1724, 2014.
[46] Mike Papadakis, Thierry Titcheu Chekam, and Yves Le Traon.
Mutant quality indicators. In 2018 IEEE International Conference
on Software Testing, Veriﬁcation and Validation Workshops, ICST
Workshops, Västerås, Sweden, April 9-13, 2018, pages 32–39. IEEE
Computer Society, 2018.

[47] Mike Papadakis, Christopher Henard, Mark Harman, Yue Jia, and
Yves Le Traon. Threats to the validity of mutation-based test
assessment. In Proceedings of the 25th International Symposium on
Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July
18-20, 2016, pages 354–365. ACM, 2016.

[48] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon,
and Mark Harman. Chapter six - mutation testing advances: An
analysis and survey. Adv. Comput., 112:275–378, 2019.

[49] Mike Papadakis and Nicos Malevris. Automatic mutation test
In IEEE 21st
case generation via dynamic symbolic execution.
International Symposium on Software Reliability Engineering, ISSRE
2010, San Jose, CA, USA, 1-4 November 2010, pages 121–130. IEEE
Computer Society, 2010.

[50] Mike Papadakis and Nicos Malevris. An empirical evaluation of
the ﬁrst and second order mutation testing strategies. In Third
International Conference on Software Testing, Veriﬁcation and Validation,
ICST 2010, Paris, France, April 7-9, 2010, Workshops Proceedings, pages
90–99. IEEE Computer Society, 2010.

[51] Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae.
Are mutation scores correlated with real fault detection?: a large
scale empirical study on the relationship between mutants and
real faults. In Michel Chaudron, Ivica Crnkovic, Marsha Chechik,
and Mark Harman, editors, Proceedings of the 40th International
Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden,
May 27 - June 03, 2018, pages 537–548. ACM, 2018.

[52] Goran Petrovic and Marko Ivankovic. State of mutation testing
at google. In 40th IEEE/ACM International Conference on Software
Engineering: Software Engineering in Practice Track, ICSE-SEIP 2018,
May 27 - 3 June 2018, Gothenburg, Sweden, 2018.

[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. CoRR, abs/1910.10683, 2019.

[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya
Imagenet large scale visual
Khosla, Michael Bernstein, et al.
International journal of computer vision,
recognition challenge.
115(3):211–252, 2015.

[55] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,
Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously
large neural networks: The sparsely-gated mixture-of-experts layer.
In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017.

[56] M. Shepperd, D. Bowes, and T. Hall. Researcher bias: The use of
machine learning in software defect prediction. IEEE Transactions
on Software Engineering, 40(6):603–616, 2014.

[57] Apeksha Shewalkar, Deepika Nyavanandi, and Simone Ludwig.
Performance evaluation of deep neural networks applied to speech
recognition: Rnn, lstm and gru. Journal of Artiﬁcial Intelligence and
Soft Computing Research, 9:235–245, 10 2019.

[58] Chang-ai Sun, Feifei Xue, Huai Liu, and Xiangyu Zhang. A
path-aware approach to mutant reduction in mutation testing.
Information & Software Technology, 81:65–81, 2017.

[59] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence

learning with neural networks, 2014.

[60] Gongbo Tang, Mathias Müller, Annette Rios, and Rico Sennrich.
Why self-attention? A targeted evaluation of neural machine
In Ellen Riloff, David Chiang, Julia
translation architectures.
Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018, pages 4263–4272.
Association for Computational Linguistics, 2018.

[61] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
Di Penta, Martin White, and Denys Poshyvanyk. Learning how

to mutate source code from bug-ﬁxes. 2019 IEEE International
Conference on Software Maintenance and Evolution (ICSME), Sep 2019.
[62] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di
Penta, Martin White, and Denys Poshyvanyk. An empirical study
on learning bug-ﬁxing patches in the wild via neural machine
translation. ACM Trans. Softw. Eng. Methodol., 28(4):19:1–19:29,
2019.

[63] András Vargha and Harold D. Delaney. A critique and improve-
ment of the "cl" common language effect size statistics of mcgraw
and wong. Journal of Educational and Behavioral Statistics, 25(2):101–
132, 2000.

[64] Bo Wang, Yingfei Xiong, Yangqingwei Shi, Lu Zhang, and Dan
Hao. Faster mutation analysis via equivalence modulo states. In
Tevﬁk Bultan and Koushik Sen, editors, Proceedings of the 26th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
Santa Barbara, CA, USA, July 10 - 14, 2017, pages 295–306. ACM,
2017.

[65] Xiangjuan Yao, Mark Harman, and Yue Jia. A study of equivalent
and stubborn mutation operators using human analysis of equiva-
lence. In Proceedings of the 36th international conference on software
engineering, pages 919–930, 2014.

[66] Zero-day vulnerability. https://www.trendmicro.com/vinfo/us/
security/deﬁnition/zero-day-vulnerability, (accessed October 12,
2021).

[67] Jie Zhang, Lingming Zhang, Mark Harman, Dan Hao, Yue Jia, and
Lu Zhang. Predictive mutation testing. IEEE Trans. Software Eng.,
45(9):898–918, 2019.

[68] Lingming Zhang, Milos Gligoric, Darko Marinov, and Sarfraz
Khurshid. Operator-based and random mutant selection: Better
together.
In Ewen Denney, Tevﬁk Bultan, and Andreas Zeller,
editors, 2013 28th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2013, Silicon Valley, CA, USA, November
11-15, 2013, pages 92–102. IEEE, 2013.

[69] Lingming Zhang, Darko Marinov, and Sarfraz Khurshid. Faster
mutation testing inspired by test prioritization and reduction. In
Mauro Pezzè and Mark Harman, editors, International Symposium
on Software Testing and Analysis, ISSTA ’13, Lugano, Switzerland, July
15-20, 2013, pages 235–245. ACM, 2013.

Aayush Garg is a doctoral researcher in the
department of Computer Science at the Faculty
of Science, Technology and Medicine (FSTM),
University of Luxembourg. He received his M.S.
degree in Computer Science with a concentration
in Security from Boston University, United States
in 2019. He has several years of industrial expe-
rience as a Software Developer in Fintech orga-
nizations. His research areas comprise computer
security, computational intelligence in software
engineering, and mutation testing.

Milos Ojdanic is a doctoral researcher at the
Interdisciplinary Center for Security, Reliability,
and Trust (SnT) at the University of Luxembourg.
He received his MSc degree from the Faculty
of Innovation, Design, and Technology at the
Mälardalen University (Sweden) in 2019. His
research interests are in software development,
testing, and evolution. In particular, he focuses on
evolving systems, change-aware testing criteria,
mutation testing, and prediction modeling.

Cerebro: Static Subsuming Mutant Selection

21

Renzo Degiovanni is a research associate at
the Interdisciplinary Center for Security, Reliability
and Trust (SnT) at the University of Luxembourg.
He received a Ph.D. diploma in Computer Sci-
ence from the National University of Cordoba,
Argentina. His research interests are in software
engineering, speciﬁcally the validation and veri-
ﬁcation of software. His research has contributed
to the automation of requirements engineering
activities, software testing and formal software
veriﬁcation.

Thierry Titcheu Chekam is a software engineer
at SES, Luxembourg. He received his Ph.D.
diploma in Computer Science from the University
of Luxembourg. He received the B.Sc. degree
in Computer Science and Technology from the
University of Science and Technology of China
in 2013, and the M.Eng. degree in Software En-
gineering from the School of Software, Tsinghua
University in 2015. His research areas comprise
software testing, mutation analysis, symbolic ex-
ecution, and cloud computing/storage.

Mike Papadakis is a senior research scientist at
the Interdisciplinary Center for Security, Reliability
and Trust (SnT) at the University of Luxembourg.
He received a Ph.D. diploma in Computer Sci-
ence from the Athens University of Economics
and Business. He is recognised for his work on
software testing and in particular in the area
of mutation testing. His research interests also
include static analysis, prediction modelling and
search-based software engineering.

Yves Le Traon is professor at the University of
Luxembourg where he leads the SERVAL (SEcu-
rity, Reasoning and VALidation) research team.
His research interests within the group include (1)
innovative testing and debugging techniques, (2)
Android apps security and reliability using static
code analysis, machine learning techniques and,
(3) model-driven engineering with a focus on IoT
and CPS. His reputation in the domain of software
testing is acknowledged by the community. He
has been General Chair of major conferences
in the domain, such as the 2013 IEEE International Conference on
Software Testing, Veriﬁcation and Validation (ICST), and Program Chair
of the 2016 IEEE International Conference on Software Quality, Reliability
and Security (QRS). He serves at the editorial boards of several,
internationally-known journals (STVR, SoSym, IEEE Transactions on
Reliability) and is author of more than 150 publications in international
peer-reviewed conferences and journals.

