©Schweidtmann, Bongartz, Mitsos

Page 1 of 7

Optimization with Trained Machine Learning Models Embedded

Artur M. Schweidtmann1, Dominik Bongartz2,3, Alexander Mitsos2,˚

1 Delft University of Technology, Department of Chemical Engineering, Van der Maasweg 9, Delft 2629 HZ, The
Netherlands
2 Process Systems Engineering (AVT.SVT), RWTH Aachen University, Forckenbeckstr. 51, 52074 Aachen,
Germany.
3 Department of Chemical Engineering, KU Leuven, Celestijnenlaan 200F, 3001 Leuven, Belgium
MSC2000: 90C26 90C30 90C90 68T01 60-04

Keywords: Machine learning; Surrogate modeling; Regression; Supervised learning; Deterministic global
optimization; Artiﬁcial neural networks; Deep learning; Gaussian processes; Reduced-space formulation;
Full-space formulation; McCormick relaxations; Bayesian optimization; Hybrid modeling

1 Introduction

Machine learning (ML) gives computers the ability to learn from data without being explicitly programmed.
Cases where the data includes input vectors (in Rnd ) and corresponding target vectors (in Rnt) are known
as supervised learning. 5 Regression problems learn continuous variables as targets whereas classiﬁcation
problems learn discrete categories as targets. While linear regression goes back to the least-squares method
by Gauss and Legendre in the early 19th century 23, recent advances in computing led to the breakthrough
performance of ML in various domains even surpassing human performance. 13

Across many engineering and science domains, ML models are trained on data and are subsequently
optimized. In chemistry, for instance, ML models are commonly ﬁtted to experimental data. This training
usually constitutes an optimization problem where model parameters are varied to minimize the model error
on the training data. Then, a subsequent optimization problem can be solved to identify the experimental
conditions (i.e., inputs of the ML model) that maximize the experimental performance (i.e., the output of
the ML model). We refer to this subsequent optimization problem as optimization with trained ML models
embedded. Notably, the ML model parameters are optimization variables during the training and ﬁxed
parameters during the optimization with trained ML models embedded. Vice versa, the inputs of the ML
model are ﬁxed parameters (determined by the training data) during the training and optimization variables
during the optimization with trained ML models embedded. While other chapter in this book deal with the
training of ML models (c.f. Section ??), this chapter focuses on the (deterministic global) optimization
with trained ML models embedded. Also, this chapter covers the optimization of hybrid models 27 where ML
models are combined with mechanistic models.

2 Formulations

As illustrated in Figure 1, there exists a broad variety of ML models ranging from linear regression models
to deep artiﬁcial neural networks (ANNs).

Once a ML regression model has been trained (and validated) on input data from D Ă Rnd and target
data from Rnt, the trained model usually represents an explicit function m : D Ñ Rnt. Depending on the
chosen model, this function may have diﬀerent properties, e.g., it may be

• discontinuous (e.g., for ensemble tree models) or continuous (e.g., ANNs),

• nonsmooth (e.g., for ANNs with ReLU activation function) or smooth (e.g., for ANNs with hyperbolic

tangent activation function),

• linear or convex nonlinear or nonconvex.

2
2
0
2

l
u
J

6
2

]

C
O
.
h
t
a
m

[

1
v
2
2
7
2
1
.
7
0
2
2
:
v
i
X
r
a

Corresponding author: ˚A. Mitsos, E-mail: amitsos@alum.mit.edu

 
 
 
 
 
 
Optimization with Trained Machine Learning Models Embedded

27.7.2022

Figure 1: Overview of ML models ordered by increasing complexity. Repreinted from Schweidtmann et al.
(2021) 20 (Figure licensed under Creative Commons CC BY).

2.1 Reduced-Space and Full-Space Model Formulations

For most ML models, there are multiple approaches of embedding the function m in the optimization problem
to be solved. One way to classify these approaches is to distinguish between reduced-space formulations and
full-space formulations. 6

In reduced-space formulations, the ML model is used directly as the function m. For example, if the
problem to be solved consists in minimizing the output of the ML model over a compact input set X Ă D,
the corresponding reduced-space formulation would read

Thus, the only variables are the nd inputs of the model, and there are no constraints beyond those imposed
by the set X . If the ML model is combined with other functions, e.g., in a hybrid model 27 containing both
ML and mechanistic parts, the problem could take the form

min
xPX

mpxq.

min
xPX ,yPY

f px, y, mpxqq

s.t.hpx, y, mpxqq “ 0
gpx, y, mpxqq ď 0,

where y P Y Ă Rny are additional optimization variables, and f , h, and g are some objective function,
equality constraints, and inequality constraints, respectively, all of which may depend on the outputs of
the ML model as well as the variables x and y. Alternatively, the inputs of the ML model may not be
optimization variables themselves, but rather outputs of some other function i : Y Ñ X :

min
yPY

f py, mpipyqqq

s.t.hpy, mpipyqqq “ 0
gpy, mpipyqqq ď 0.

In a full-space formulation, internal variables of the ML models become part of the optimization problem
as optimization variables, and equations deﬁning the ML model become constraints. In this case, the function
m does not occur in the problem as a single function that can be evaluated to obtain outputs for given inputs,
but instead, the evaluation becomes part of solving the optimization problem. Note that typically a range
of full-space formulations exists that diﬀer in how many and which internal variables and equations become
part of the optimization problem. These can also be considered as intermediate versions between full- and
reduced-space formulations. Similar to the reduced-space formulation, more complex problems can of course
be constructed where the ANN is combined with other functions, e.g., in a hybrid mechanistic/data-driven
model. 27

©Schweidtmann, Bongartz, Mitsos

2

Page 2 of 7

Optimization with Trained Machine Learning Models Embedded

27.7.2022

From a modeling perspective, reduced-space formulations are very intuitive and easy to implement in
procedural environments such as e.g., when modeling in Python. In these environments, the ML models are
naturally implemented as functions that can just be used in the optimization problem. In contrast, when
modeling in an equation-oriented environment such as, e.g., GAMS or AMPL, the reduced-space formulation
is often much more challenging to implement. For example, to write an ANN in a reduced-space formulation,
one would have to symbolically substitute the outputs of each layer into the activation function of the next
layer in order to ultimately obtain an expression for the outputs of the ANN (i.e., those of the last layer)
as a function of its inputs (i.e., the inputs of the ﬁrst layer). In such an environment, it is more natural to
write a full-space formulation that merely requires deﬁning the outputs of all layers as variables and writing
the corresponding equations as constraints.

From a computational perspective, the chosen formulation can have a considerable impact on the eﬀort
for solving the problem, because the problem looks very diﬀerent to the optimizer: The reduced-space for-
mulations have much fewer variables and equality constraints than the full-space formulations but contain
potentially very complicated functions. In particular, the number of variables and constraints in the opti-
mization problem is independent of the type of ML model used or the amount of data used for training it.
In the full-space formulation, the number of optimization variables and constraints may increase with the
complexity of the ML model (e.g., for an ANN, it may increase with the number of hidden layers, or with
the number of neurons per layer 21), or even with the amount of data that was used for training the model
(e.g., for Gaussian processes 19). As discussed in the following, it may depend on the optimization algorithm
how eﬃciently either formulation can be solved.

An alternative classiﬁcation of ways of incorporating ML models in optimization problems is given by
Lombardi & Milano 14 who distinguish embedding an ML model by expanding the language as opposed to
encoding the ML model using the native language. This ﬁrst case entails deﬁning a new function representing
the ML model that can then be used when formulating the optimization problem. If this is done for an
entire ML model, this would correspond to a reduced-space formulation as deﬁned above. This approach
may, however, only introduce certain components of an ML model as new functions, e.g., activation functions
of ANNs. The second case entails representing the ML model via functions that are already available in
the chosen optimization environment. This could in fact be done for both full space and reduced space
formulations. In full space formulations, the ML model consists of a number of variables and constraints in
the optimization problems, while in RS formulations, the functions encoding the ML model are a composition
of existing functions.

2.2 Modeling Nonsmooth Functions

For some ML models such as ANNs with ReLU activation function, there is an additional distinction re-
garding the way the nonsmoothness is handled. On the one hand, the ReLU activation function can be
formulated with the max function. This enables a reduced-space formulation of the ANN, meaning that the
network can be expressed as an explicit function mapping the inputs to the outputs of the network. However,
this function is usually nonconvex and nonsmooth and thus challenging to solve to global optimality. On
the other hand, the max function can be reformulated as a piecewise linear function modeled with a binary
variable that determines which of the linear pieces is active (c.f. Section 3.4). This avoids the use of nons-
mooth functions in the model at the expense of adding one binary variable per neuron with ReLU activation
function. Furthermore, this makes a reduced-space formulation as described above impossible, because the
optimizer now needs to control the binary variables, which are part of the internals of the ANN. Typically,
this also entails the use of additional constraints, e.g., in the form of a Big-M formulation. Similarly, the
nonsmoothness of tree models can be reformulated using binary variables (c.f. Section 3.3).

2.3 Validity Domain Modeling

An additional aspect is related to the domain of the function m. Since the ML model has been trained on
a limited number of data points pxi, tiq P Rnd ˆ Rnt, it can not be expected to give a good description of
the underlying, “true” input-output relationship for arbitrary inputs x P Rnd . If x is far from the inputs
of the training data, the model needs to extrapolate, which can give unreliable results.
In this sense, it
may be prudent to add constraints to the optimization problem that restrict the inputs of the ML model

©Schweidtmann, Bongartz, Mitsos

3

Page 3 of 7

Optimization with Trained Machine Learning Models Embedded

27.7.2022

to values that are not too far from its training data. Such constraints could take the form of the convex
hull of the training data points as in the convex region surrogate models 28, or an approximation of the
possibly nonconvex region covered by the training data points, e.g., obtained by one-class classiﬁers. 22 Also,
the distance to training data points can be included as a penalty to the objective function. 17

3 Machine Learning Models

In this section, we discuss a few common ML models and possible optimization formulations with these ML
models embedded.

3.1 Convex region surrogate models

Convex region surrogate models were proposed by Zhang et al. 28 They consist of aﬃne functions deﬁned
over a number of convex polytopes that are constructed to contain the training data. They can approximate
nonlinear (potentially discontinuous) functions over nonconvex regions. Given the regions into which the
domain is divided according to the model, evaluating the model at a point amounts to identifying which
region this point lies in and evaluating the aﬃne function deﬁned on this region.

The models are originally devised in a Generalized Disjunctive Programming (GDP) setting, but they
can be reformulated as MILPs, e.g., via the hull reformulation. This representation is particularly suitable
if the optimization problem in which the model is to be embedded is already a MILP. According to the
above classiﬁcation, this is a full-space formulation of the model, because the optimization solver controls
additional variables and constraints.

To our knowledge, a reduced-space formulation of this model has not been attempted yet. In its purest
form, this would entail expressing the model as a piecewise-deﬁned function of x. In this case, the challenge
lies in the fact that the function may be discontinuous, and that the constraints on the input values need to
be accounted for explicitly.

3.2 Gaussian processes

“

p ypxq ´ mpxq q p ypx1q ´ mpxq qT

Gaussian processes, also known are Kriging, are inﬁnite-dimensional generalizations of multivariate Gaussian
distributions. 18 Their predictions follow Gaussian distributions and thus include an estimate and a variance.
“
‰
˜f pxq
Formally, Gaussian processes are deﬁned by its mean function (mpxq :“ IE
) and covariance function
), where y is an observation from ˜f pxq. The most common
(kpx, x1q :“ IE
covariance function is the squared exponential covariance function, k1
, where r :“
SEprq :“ exp
a
px ´ x1qTΛ px ´ x1q is a Euclidean distance with weighting factors Λ :“ diagpλ2
nxq. The
hyperparameters θ “ rλ1, .., λds are adjusted during training to maximize the probability that the Gaussian
process ﬁts the training data. The Gaussian process posterior distribution is obtained by conditioning the
prior distributions on training data (X , Yq, ˜f pxq „ GPpmpxq, kpx, x1q|X , Yq. The estimate and variance of
the posterior can be calculated by

´ 1
2 r2
1, ¨ ¨ ¨ , λ2

i , ¨ ¨ ¨ λ2

˘

`

‰

mDpxq “ Kx,X pKX ,X q´1 y,
kDpxq “ Kx,x ´ Kx,X pKX ,X q´1 KX ,x,

(1)

(2)
ı

”
kpx, xpDq

q, .., kpx, xpDq
where KX ,X :“ rkpxi, xjqs P RN ˆN is the covariance matrix of the data, Kx,X :“
N q
R1ˆN is the covariance vector between the candidate point x and the training data, and Kx,x :“ kpx, xq.

1

P

Trained Gaussian processes are commonly embedded in optimization problems for iterative planning of
(computational) experiments through Bayesian optimization. Therein, an acquisition function is maximized
to identify an optimal tradeoﬀ between exploration and exploitation. One common example is the maximiza-
tion of the expected improvement acquisition function. 11 Furthermore, Gaussian processes are commonly
used as surrogate models when only limited data is available. 7,8 Optimization problems with trained Gaus-
sian processes embedded are NLPs as the covariance function is nonlinear. Moreover, the optimization of
acquisition functions are known to exhibit a large number of (suboptimal) local optima. 12 In the previous
literature, optimization problems with trained Gaussian processes embedded have mostly been solved using

©Schweidtmann, Bongartz, Mitsos

4

Page 4 of 7

Optimization with Trained Machine Learning Models Embedded

27.7.2022

full space formulations (e.g. 7,8) while recent work indicated that the reduced space formulation can be ad-
vantageous. 19 Notably, tailored convex and concave relaxations of covariance and acquisition functions can
improve the solution times for these problems signiﬁcantly. 19

3.3 Decision tree ensembles

Decision tree ensembles are popular models for representing nonlinear relationships in large-scale, noisy data
sets including both continuous and discrete inputs. They consist of multiple decision trees, where a decision
tree predicts the output based on a sequence of queries involving the inputs (e.g., if x ą c, where c is a
constant parameter, follow branch 1 of the tree, if x ď c follow branch 2 of the tree). For given values of the
inputs, the output is computed by following the tree based on the answers to those queries from the root to a
certain leaf, which then contains the output value. In a tree ensemble, the output of multiple trees is averaged
(in the case of regression) to yield the output of the overall model to improve the prediction accuracy. The
most popular models of this type are gradient boosted trees, where the trees are built sequentially by learning
the residual between the data and the previous tree, and random forests, which consist of trees trained on
random subsets of the data as well as the inputs used in the queries.

By construction, tree ensemble models predict the output as a piecewise constant function of the inputs.
As such, they are inherently discontinuous, which makes the optimization of problems with trained ensemble
models embedded diﬃcult to optimize. Recently, Miˇsi´c 16 and Biggs et al. 4 presented mixed-integer formula-
tions for representing ensemble tree models in optimization problems. They encode the results of the queries
as binary variables and introduce constraints that enforce the selection of the correct leaf node based on
these variables. The approach of Biggs et al. 4 has the advantage that additional (linear) constraints are
easier to incorporate.

Misener and co-workers recently extended these approaches by incorporating an uncertainty measure as
a penalty term in the objective. This term penalizes large deviation of inputs from the training data, either
considering the distance to the nearest (clusters of) data points 24, or the distance to a subspace of the
input space containing the data. 17 The former approach was also used in the opposite sense for application
in Bayesian optimization, i.e., to enhance exploration rather than restrict the search to the vicinity of the
training data. 24

Optimization with tree ensembles models embedded becomes particularly challenging when the ensem-
bles get large (e.g., several thousand trees) or when the trees contain many queries. Therefore, dedicated
approaches for large models have been proposed. These are based on Benders decomposition 4,16, truncation
of the tree depth 16, considering subsets of the trees 4,17, or dedicated B&B procedures. 17,24

3.4 Artiﬁcial neural networks

´

ANNs are one of the most common ML models and they are an essential block of deep learning architec-
tures (see also, e.g., 5,13). Feed-forward ANNs, also known as multilayer perceptrons, consist of N (hid-
den) layers including Dpkq neurons. Generally, ANNs can be viewed as directed acyclic graphs connecting
neurons that are organized in layers. The ﬁrst layer corresponds to the inputs of the ANN and the last
layer corresponds to its outputs. For each layer k ě 2, the output vector zpkq of the layer k is given by
where hkp¨q is an activation function, Wpk´1q is a weight matrix and
zpkq “ hk
bpk´1q P RDpkq a bias vector. Notably, the weights pWpkqq and bias vectors pbpkqq are known constant pa-
rameters in optimization problems with trained ANNs embedded. For regression problem, the hyperbolic
tangent function, hkpxq “ tanhpxq, has been the most common activation function in ANNs. Also, the
rectiﬁed linear unit (ReLU) activation function, hkpxq “ maxpx, 0q, is increasingly common for deep ANNs
as it is more eﬀectively trainable. 2

Wpk´1qzpk´1q ` bpk´1q

¯

Given the nonlinear activation functions, optimization problems with ANNs embedded are generally
nonlinear optimization problems (NLPs). As discussed in Section 2.1, ANNs can be modeled using full space
or reduced space formulations. Previous studies indicate that the reduced space formulation has a favorable
performance compared to the full space formulation for deterministic global optimization with trained ANNs
embedded. 21 This is mostly due to the increased problem size of full space formulations and the diﬃculty to
solve large-scale NLPs to global optimality.

©Schweidtmann, Bongartz, Mitsos

5

Page 5 of 7

Optimization with Trained Machine Learning Models Embedded

27.7.2022

As the ReLU function is a piecewise linear function, it can also be reformulated through binary variables
and linear constraints. This allows formulating optimization problems with trained ReLU ANNs as mixed-
integer linear problems (MILPs). 9,10,25 As intermediate variables are introduced to the optimization problem,
these MILP formulations are essentially full space formulations. While most previous works use big-M
formulations for ReLU ANNs, recent works also proposed sharp or ideal formulations 1 as well as hybrids 26.
Moreover, most previous works employ specialized bound tightening techniques. Recently, open-source
modeling tools for ReLU ANNs as MILP have been proposed bridging established ML and optimization
tools. 3,15

To the best of our knowledge, a comparison between the nonlinear reduced space formulation and the
MILP full space formulation has not yet been published in the literature. Thus, it still remains unclear what
the most eﬃcient problem formation is. While the reduced space formation has signiﬁcantly smaller problem
sizes, the MILP formulation is linear. Irrespective of the formulation, it appears that the problem size of
previous studies has been limited by high computational times.

4 Conclusions

Trained ML models are commonly embedded in optimization problems. In many cases, this leads to large-
scale NLPs that are diﬃcult to solve to global optimality. While ML models frequently lead to large
problems, they also exhibit homogeneous structures and repeating patterns (e.g., layers in ANNs). Thus,
specialized solution strategies can be used for large problem classes. Recently, there have been some promising
works proposing specialized reformulations using mixed-integer programming or reduced space formulations.
However, further work is needed to develop more eﬃcient solution approaches and keep up with the rapid
development of new ML model architectures.

References

[1] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P. Vielma. Strong mixed-integer pro-
gramming formulations for trained neural networks. Mathematical Programming, pages 1–37, 2020.

[2] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with rectiﬁed

linear units. arXiv preprint arXiv:1611.01491, 2016.

[3] D. Bergman, T. Huang, P. Brooks, A. Lodi, and A. U. Raghunathan. Janos: An integrated predictive

and prescriptive modeling framework. INFORMS Journal on Computing, 2021.

[4] M. Biggs, R. Hariss, and G. Perakis. Optimizing objective functions determined from random forests.

Available at SSRN 2986630, 2017.

[5] C. M. Bishop. Pattern recognition and machine learning. Information science and statistics. Springer,

New York, NY, 8 edition, 2009.

[6] D. Bongartz and A. Mitsos. Deterministic global ﬂowsheet optimization: Between equation-oriented

and sequential-modular methods. AIChE Journal, 65(3):1022–1034, 2019.

[7] F. Boukouvala and C. A. Floudas. Argonaut: Algorithms for global optimization of constrained grey-box

computational problems. Optimization Letters, 11(5):895–913, 2017.

[8] J. A. Caballero and I. E. Grossmann. Rigorous ﬂowsheet optimization using process simulators and
surrogate models. In Computer Aided Chemical Engineering, volume 25, pages 551–556. Elsevier, 2008.

[9] C.-H. Cheng, G. N¨uhrenberg, and H. Ruess. Maximum resilience of artiﬁcial neural networks. In Inter-
national Symposium on Automated Technology for Veriﬁcation and Analysis, pages 251–268. Springer,
2017.

[10] M. Fischetti and J. Jo. Deep neural networks and mixed integer linear optimization. Constraints,

23(3):296–309, 2018.

©Schweidtmann, Bongartz, Mitsos

6

Page 6 of 7

Optimization with Trained Machine Learning Models Embedded

27.7.2022

[11] D. R. Jones, M. Schonlau, and W. J. Welch. Eﬃcient global optimization of expensive black-box

functions. Journal of Global optimization, 13(4):455–492, 1998.

[12] J. Kim and S. Choi. On local optimizers of acquisition functions in bayesian optimization. arXiv preprint

arXiv:1901.08350, 2019.

[13] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.

[14] M. Lombardi and M. Milano. Boosting combinatorial problem modeling with machine learning. arXiv

preprint arXiv:1807.05517, 2018.

[15] L. Lueg, B. Grimstad, A. Mitsos, and A. M. Schweidtmann.

relumip: Open source tool for milp

optimization of relu neural networks. Oct 2021.

[16] V. V. Miˇsi´c. Optimization of tree ensembles. Operations Research, 68(5):1605–1624, 2020.

[17] M. Mistry, D. Letsios, G. Krennrich, R. M. Lee, and R. Misener. Mixed-integer convex nonlinear

optimization with gradient-boosted trees embedded. INFORMS Journal on Computing, 2020.

[18] C. E. Rasmussen. Gaussian processes in machine learning. In Advanced lectures on machine learning,

pages 63–71. Springer, 2004.

[19] A. M. Schweidtmann, D. Bongartz, D. Grothe, T. Kerkenhoﬀ, X. Lin, J. Najman, and A. Mitsos. De-
terministic global optimization with gaussian processes embedded. Mathematical Programming Com-
putation, 13(3):553–581, 2021.

[20] A. M. Schweidtmann, E. Esche, A. Fischer, M. Kloft, J.-U. Repke, S. Sager, and A. Mitsos. Machine

learning in chemical engineering: A perspective. Chemie Ingenieur Technik, 93(12):2029–2039, 2021.

[21] A. M. Schweidtmann and A. Mitsos. Deterministic global optimization with artiﬁcial neural networks

embedded. Journal of Optimization Theory and Applications, 180(3):925–948, 2019.

[22] A. M. Schweidtmann, J. M. Weber, C. Wende, L. Netze, and A. Mitsos. Obey validity limits of
data-driven models through topological data analysis and one-class classiﬁcation. Optimization and
Engineering, pages 1–22, 2021.

[23] S. M. Stigler. Gauss and the invention of least squares. the Annals of Statistics, pages 465–474, 1981.

[24] A. Thebelt, J. Kronqvist, M. Mistry, R. M. Lee, N. Sudermann-Merx, and R. Misener. Entmoot: A
framework for optimization over ensemble tree models. Computers & Chemical Engineering, 151:107343,
2021.

[25] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed integer

programming. arXiv preprint arXiv:1711.07356, 2017.

[26] C. Tsay, J. Kronqvist, A. Thebelt, and R. Misener. Partition-based formulations for mixed-integer

optimization of trained relu neural networks. arXiv preprint arXiv:2102.04373, 2021.

[27] M. Von Stosch, R. Oliveira, J. Peres, and S. F. de Azevedo. Hybrid semi-parametric modeling in process

systems engineering: Past, present and future. Computers & Chemical Engineering, 60:86–101, 2014.

[28] Q. Zhang, I. E. Grossmann, A. Sundaramoorthy, and J. M. Pinto. Data-driven construction of convex

region surrogate models. Optimization and Engineering, 17(2):289–332, 2016.

©Schweidtmann, Bongartz, Mitsos

7

Page 7 of 7

