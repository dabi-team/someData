1
2
0
2

c
e
D
1

]
L
M

.
t
a
t
s
[

4
v
9
2
6
0
0
.
0
1
1
2
:
v
i
X
r
a

Factored couplings in multi-marginal optimal
transport via difference of convex programming

Quang Huy Tran
Univ. Bretagne-Sud, CNRS, IRISA
F-56000 Vannes
quang-huy.tran@univ-ubs.fr

Hicham Janati
École Polytechnique, CMAP, UMR 7641
F-91120 Palaiseau
hicham.janati@polytechnique.edu

Ievgen Redko
Univ Lyon, UJM-Saint-Etienne, CNRS, UMR 5516
F-42023 Saint-Etienne
ievgen.redko@univ-st-etienne.fr

Rémi Flamary
École Polytechnique, CMAP, UMR 7641
F-91120 Palaiseau
remi.flamary@polytechnique.edu

Nicolas Courty
Univ. Bretagne-Sud, CNRS, IRISA
F-56000 Vannes
nicolas.courty@irisa.fr

Abstract

Optimal transport (OT) theory underlies many emerging machine learning (ML)
methods nowadays solving a wide range of tasks such as generative modeling,
transfer learning and information retrieval. These latter works, however, usually
build upon a traditional OT setup with two distributions, while leaving a more
general multi-marginal OT formulation somewhat unexplored. In this paper, we
study the multi-marginal OT (MMOT) problem and unify several popular OT
methods under its umbrella by promoting structural information on the coupling.
We show that incorporating such structural information into MMOT results in
an instance of a difference of convex (DC) programming problem allowing us to
solve it numerically. Despite high computational cost of the latter procedure, the
solutions provided by DC optimization are usually as qualitative as those obtained
using currently employed optimization schemes.

1

Introduction

Broadly speaking, the classic OT problem provides a principled approach for transporting one
probability distribution onto another following the principle of the least effort. Such a problem, and
the distance on the space of probability distributions derived from it, arise in many areas of machine
learning (ML) including generative modeling, transfer learning and information retrieval, where OT
has been successfully applied. A natural extension of classic OT, in which the admissible transport
plan (a.k.a coupling) can have more than two prescribed marginal distributions, is called the multi-
marginal optimal transport (MMOT) [Gangbo and Swiech, 1998]. The latter has several attractive
properties: it enjoys a duality theory [Kellerer, 1984] and ﬁnds connections with the probabilistic
graphical models [Haasler et al., 2020] and the Wasserstein barycenter problem [Agueh and Carlier,
2011] used for data averaging. While being less popular than the classic OT with two marginals,
MMOT is a very useful framework on its own with some notable recent applications in generative
adversarial networks [Cao et al., 2019], clustering [Mi and Bento, 2020] and domain adaptation [Hui
et al., 2018, He et al., 2019], to name a few.

Preprint. Under review.

 
 
 
 
 
 
The recent success of OT in ML is often attributed to the entropic regularization [Cuturi, 2013] where
the authors imposed a constraint on the coupling matrix forcing it to be closer to the independent
coupling given by the rank-one product of the marginals. Such a constraint leads to the appearance of
the strongly convex entropy term in the objective function and allows the entropic OT problem to be
solved efﬁciently using simple Sinkhorn-Knopp matrix balancing algorithm. In addition to this, it
was also noticed that structural constraints on the coupling and cost matrices allow to reduce the high
computational cost and sample complexity of the classic OT problem [Genevay et al., 2019, Forrow
et al., 2019, Lin et al., 2021, Scetbon et al., 2021]. However, none of these works considered a much
more challenging case of doing so in a multi-marginal setting. On the other hand, while the work of
[Haasler et al., 2020] considers the MMOT problem in which the cost tensor induced by a graphical
structure, it does not naturally promote the factorizability of transportation plans.

Contributions
In this paper, we deﬁne and study a general MMOT problem with structural penal-
ization on the coupling matrix. We start by showing that a such formulation includes several popular
OT methods as special cases and allows to gain deeper insights into them. We further consider a
relaxed problem where the hard constraint is replaced by a regularization term and show that it leads
to an instance of the difference of convex programming problem. A numerical study of the solutions
obtained when solving the latter in cases of interest highlights their competitive performance when
compared to solutions provided by the optimization strategies used previously.

2 Preliminary knowledge

Notations. For each integer n ≥ 1, we write [n] := {1, ..., n}. For any discrete probability measure
µ with ﬁnite support, its negative entropy is deﬁned as H(µ) = (cid:104)µ, log µ(cid:105), where the logarithm
operator is element-wise, with the convention that 0 log 0 = 0. Here, (cid:104)·, ·(cid:105) denotes the Frobenius
inner product. The Kullback-Leibler divergence between two discrete probability measures µ and ν
with ﬁnite supports is deﬁned as

KL(µ|ν) =

(cid:26)(cid:104)µ, log µ

ν (cid:105), if µ is absolutely continuous with respect to ν

∞, otherwise.

where the division operator in the logarithm is element-wise.
In what follows, given an integer N ≥ 1, for any positive integers a1, ..., aN , we call P ∈ Ra1×...×aN
a N -D tensor. In particular, a 1-D tensor is a vector and 2-D tensor is a matrix. A tensor is a probability
tensor if its entries are nonnegative and the sum of all entries is 1. Given N probability vectors
µ1, ..., µN , we write µ = (µn)N
n=1. We denote Σ the set of N -D probability tensors and U (µ) ⊂ Σ
the set of nonnegative tensors whose N marginal distributions are µ1, ..., µN . In this case, any
coupling in U (µ) is said to be admissible.

Multi-marginal OT problem. Given a collection of N probability vectors µ = (µn ∈ Ran)N
and a N -D cost tensor C ∈ Ra1×...×aN , the MMOT problem reads

n=1

MMOT(µ) = inf

(cid:104)C, P (cid:105).

P ∈U (µ)

In practice, such a formulation is intractable to optimize in a discrete setting as it results in a linear
program where the number of constraints grows exponentially in N . A more tractable strategy for
solving MMOT is to consider the following entropic regularization problem

inf
P ∈U (µ)

(cid:104)C, P (cid:105) + εH(P ).

(1)

which can be solved using Sinkhorn’s algorithm [Benamou et al., 2014]. We refer the interested
reader to Supplementary materials for algorithmic details.

3 Factored Multi-marginal Optimal Transport

In this section, we ﬁrst deﬁne a factored MMOT (F-MMOT) problem where we seek to promote a
structure on the optimal coupling given such as a factorization into a tensor product. Interestingly,
such a formulation can be shown to include several other OT problems as special cases. Then,
we introduce a relaxed version called MMOT-DC where the factorization constraint is smoothly
promoted through a Kullback-Leibler penalty.

2

3.1 Motivation

Before a formal statement of our problem, we ﬁrst give a couple of motivating examples showing
why and when structural constraints on the coupling matrix can be beneﬁcial. To this end, ﬁrst note
that a trivial example of the usefulness of such constraints in OT is the famous entropic regularization.
Indeed, while most of the works deﬁne the latter by adding negative entropy of the coupling to the
classic OT objective function directly, the original idea was to constraint the sought coupling to
remain close (to some extent) to a rank-one product of the two marginal distributions. The appearance
of negative entropy in the ﬁnal objective function is then only a byproduct of such constraint due to
the decomposition of the KL divergence into a sum of three terms with two of them being constant.
Below we give two more examples of real-world applications related to MMOT problem where a
certain decomposition imposed on the coupling tensor can be desirable.

Multi-source multi-target translation. A popular task in computer vision is to match images
across different domains in order to perform the so-called image translation. Such tasks are often
tackled within the GAN framework where one source domain from which the translation is performed,
is matched with multiple target domains modeled using generators. While MMOT was applied in this
context by [Cao et al., 2019] when only one source was considered, its application in a multi-source
setting may beneﬁt from structural constraints on the coupling tensor incorporating the human prior
on what target domains each source domain should be matched to.

Multi-task reinforcement learning.
In this application, the goal is to learn individual policies for
a set of agents while taking into account the similarities between them and hoping that the latter will
improve the individual policies. A common approach is to consider an objective function consisting
of two terms where the ﬁrst term is concerned with learning individual policies, while the second
forces a consensus between them. Similar to the example considered above, MMOT problem was
used to promote the consensus across different agents’ policies in [Cohen et al., 2021], even though
such a consensus could have beneﬁted from a prior regarding the semantic relationships between the
learned tasks.

3.2 Factored MMOT and its relaxation

We start by giving several deﬁnitions used in the following parts of the paper.

Deﬁnition 3.1 (Tuple partition) Given two integers N ≥ M ≥ 2, a sequence of tuples T =
(Tm)M
m=1, is called a tuple partition of the N -tuple (1, ..., N ) if the tuples T1, ..., TM are nonempty
and disjoint, and their concatenation in this order gives (1, ..., N ).

Here, we implicitly take into account the order of the tuple, which is not the case for the partition of
the set [N ]. If there exists a tuple in T which contains only one element, then we say T is degenerate.

Deﬁnition 3.2 (Marginal tensor) Given a tensor P ∈ Ra1×...×aN and a tuple partition T =
(Tm)M
m=1, we call P#Tm its Tm-marginal tensor, by summing P over all dimensions not in Tm.
We write P#T = P#T1 ⊗ ... ⊗ P#TM ∈ Ra1×...×aN the tensor product of its marginal tensors.

For example, for M = N = 2, we have T1 = (1) and T2 = (2). So, given a matrix P ∈ Ra1×a2 ,
its marginal tensors P#T1 and P#T2 are simply vectors in Ra1 and Ra2 , respectively, deﬁned by
(P#T1)i = (cid:80)
j Pij and (P#T2)j = (cid:80)
i Pij for (i, j) ∈ [a1] × [a2]. The tensor product P#T ∈
Ra1×a2 is then deﬁned by (P#T )ij = (P#T1 )i(P#T2)j.
Clearly, if P is a probability tensor, then so are its marginal tensors and tensor product.

Suppose Tm = (p, ..., q) for some m ∈ [M ] and 1 ≤ p ≤ q ≤ N . We denote ΣTm the set of
probability tensors in Rap×...×aq and UTm ⊂ ΣTm the set of probability tensors in Rap×...×aq whose
(r)-marginal vector is µr, for every r = p, ..., q.

Deﬁnition 3.3 (Factored MMOT) Given a collection of histograms µ = (µn)N
partition T = (Tm)M
m=1, we consider the following OT problem

n=1 and a tuple

F-MMOT(T , µ) = inf
P ∈UT

(cid:104)C, P (cid:105),

(2)

3

where UT ⊂ U (µ) is the set of admissible couplings which can be factorized as a tensor product of
M component probability tensors in ΣT1 , ..., ΣTM .

Several remarks are in order here. First, one should note that the partition considered above is in
general not degenerate meaning that the decomposition can involve tensors of an arbitrary order
< N . Second, the decomposition in this setting depicts the prior knowledge regarding the tuples of
measures which should be independent: the couplings for the measures from different tuples will
be degenerate and the optimal coupling tensor will be reconstructed from couplings of each tuple
separately. Third, suppose the partition (Tm)M
m=1 is not degenerate and M = 2, i.e. the tensor is
factorized as product of two tensors, the problem 2 is equivalent to a variation of low nonnegative
rank OT problem (see Appendix for a proof).

As for the existence of the solution to this problem, we have that UT is compact because it is a
close subset of the compact set U (µ), which implies that the problem 2 always admits a solution.
Furthermore, observe that

UT = {P ∈ U (µ) : P = P1 ⊗ ... ⊗ PM , where Pm ∈ ΣTm, ∀m = 1, ..., M }
= {P ∈ Σ : P = P1 ⊗ ... ⊗ PM , where Pm ∈ UTm, ∀m = 1, ..., M }.

Thus, the problem F-MMOT can be rewritten as

F-MMOT(T , µ) =

inf
Pm∈UTm
∀m=1,...,M

(cid:104)C, P1 ⊗ ... ⊗ PM (cid:105).

So, if T1, ..., TM are 2-tuples and two marginal distributions corresponding to each UTm are identical
and uniform, then by Birkhoff’s theorem [Birkhoff, 1946], the problem 2 admits an optimal solution
in which each component tensor Pm is a permutation matrix.

Two special cases. When N = 4 and M = 2 with T1 = (1, 2) and T2 = (3, 4), the problem 2
becomes the CO-Optimal transport (COOT) [Redko et al., 2020], where the two component tensors
are known as sample and feature couplings. If furthermore, a1 = a3, a2 = a4, and µ1 = µ3, µ2 = µ4,
it becomes a lower bound of the discrete Gromov-Wasserstein (GW) distance [Mémoli, 2011]. This
means that our formulation can be seen as a generalization of several OT formulations.

Observe that if a probability tensor P can be factorized as a tensor product of probability tensors,
i.e. P = P1 ⊗ ... ⊗ PM , then each Pm is also the Tm-marginal tensor of P . In this case, we have
P = P#T . This prompts us to consider the following relaxation of factored MMOT, where the hard
constraint UT is replaced by a regularization term.

Deﬁnition 3.4 (Relaxed Factored MMOT) Given ε ≥ 0, a collection of measures µ and a tuple
partition T , we deﬁne the following problem:

MMOT-DCε(T , µ) = inf

(cid:104)C, P (cid:105) + εKL(P |P#T ).

P ∈U (µ)

(3)

From the exposition above, one can guess that this relaxation is reminiscent of the entropic regular-
ization in MMOT and coincides with it when M = N . As such, it also recovers the classical entropic
OT. One should note that the choice of the KL divergence is not arbitrary and its advantage will
become clear when it comes to the algorithm. A special case of the problem 3 is when M = N , we
recover the entropic-regularized MMOT problem, up to a constant.

After having deﬁned the two optimization problems, we now set on exploring their theoretical
properties.

3.3 Theoretical properties

Intuitively, the relaxed problem is expected to allow for solutions with a lower value of the ﬁnal
objective function. We formally prove the validity of this intuition below.

Proposition 3.1 (Preliminary properties) Given a collection of histograms µ and a tuple partition
T ,

1. For every ε ≥ 0, we have MMOT(µ) ≤ MMOT-DCε(T , µ) ≤ F-MMOT(T , µ).

4

2. For every ε > 0, MMOT-DCε(T , µ) = 0 if and only if F-MMOT(T , µ) = 0.

An interesting property of MMOT-DC is that it interpolates between MMOT and F-MMOT. Informally,
for very large ε, the KL divergence term dominates, so the optimal transport plans tend to be
factorizable. On the other hand, for very small ε, the KL divergence term becomes negligible and we
approach MMOT. The result below formalizes this intuition.

Proposition 3.2 (Interpolation between MMOT and F-MMOT) For any tuple partition T and
for ε > 0, let Pε be a minimiser of the problem MMOT-DCε(T , µ).

1. When ε → ∞, one has MMOT-DCε(T , µ) → F-MMOT(T , µ). In this case, any cluster

point of the sequence of minimisers (Pε)ε is a minimiser of F-MMOT(T , µ).

2. When ε → 0, then MMOT-DCε(T , µ) → MMOT(µ). In this case, any cluster point of the

sequence of minimisers (Pε)ε is a minimiser of MMOT(µ).

GW distance revisited. Somewhat surprisingly, the relaxation 3 also allows us to prove the equality
between GW distance and COOT in the discrete setting. Let X be a ﬁnite subset (of size m) of a
certain metric space. Denote Cx ∈ Rm×m its similarity matrix (e.g. distance matrix). We deﬁne
similarly the set Y of size n and the corresponding similarity matrix Cy ∈ Rn×n. We also assign two
discrete probability measures µx ∈ Rm and µy ∈ Rn to X and Y, respectively. The GW distance is
then deﬁned as

GW(Cx, Cy) =

inf
Q∈U (µx,µy)

(cid:104)L(Cx, Cy), Q ⊗ Q(cid:105),

and the COOT reads

COOT(Cx, Cy) =

inf
Qs∈U (µx,µy)
Qf ∈U (µx,µy)

(cid:104)L(Cx, Cy), Qs ⊗ Qf (cid:105),

where L(Cx, Cy) ∈ Rm×n×m×n represents the 4-D cost tensor induced by the matrices Cx and
Cy, and U (µ, ν) is the set of couplings in Rm×n
≥0 whose two marginal distributions are µ and
ν. When Cx and Cy are two squared Euclidean distance matrices, and L(Cx, Cy) is of the form
(cid:0)L(Cx, Cy)(cid:1)
i,j,k,l = |(Cx)i,k −(Cy)j,l|2, it can be shown that the GW distance is equal to the COOT
[Redko et al., 2020]. This is also true when L(Cx, Cy) is a negative deﬁnite kernel [Séjourné et al.,
2020]. Here, we establish a weaker case where this equality still holds.

Corollary 3.3 If L(Cx, Cy) deﬁnes a conditionally negative deﬁnite kernel on (X × Y)2, then we
have the equality between GW distance and COOT. Furthermore, if (Q∗
f ) is a solution of the
COOT problem, then Q∗
f are two solutions of the GW problem. In particular, when L(Cx, Cy)
induces a strictly positive deﬁnite kernel exp (cid:0) − L(Cx,Cy)

(cid:1), for every ε > 0, we have Q∗

s and Q∗

s, Q∗

s = Q∗
f .

ε

The proof relies on the connection between MMOT-DC and COOT shown in the proposition 3.2,
and given a 4-D solution of MMOT-DC, we can construct another 4-D solutions whose T1 and
T2-marginal matrices are identical, under the assumption of the cost tensor. The proof of the second
claim is deferred to the Appendix.

4 Numerical solution

We now turn to the computational aspect of the problem 3. First, note that for any tuple partition
T = (Tm)M
m=1 and probability tensor P , the KL divergence term can be decomposed as

KL(P |P#T ) = H(P ) −

m
(cid:88)

m=1

Hm(P ),

where the function Hm deﬁned by Hm(P ) := H(P#Tm ) is continuous and convex with respect to
P . Now, the problem 3 becomes

MMOT-DCε(T , µ) = inf

(cid:104)C, P (cid:105) + εH(P ) − ε

P ∈U (µ)

M
(cid:88)

m=1

Hm(P ).

(4)

5

Algorithm 1 DC algorithm for the problem 3.
Input. Cost tensor C, tuple partition (Tm)M
m=1, collection of histograms µ = (µn)N
eter ε > 0, initialization P (0), tuple of initial dual vectors for the Sinkhorn step (f (0)
Output. Tensor P ∈ U (µ).
While not converge

n=1, hyperparam-
1 , ..., f (0)
N ).

1. Gradient step: compute the gradient of the convex term G(t) =

M
(cid:80)
m=1

∇P Hm(P (t)).

2. Sinkhorn step: solve

P (t+1) = arg min

(cid:104)C − εG(t), P (cid:105) + εH(P ),

P ∈U (µ)

using the Sinkhorn algorithm 3, with the tuple of initial dual vectors (f (0)

1 , ..., f (0)
N ).

This is nothing but a Difference of Convex (DC) programming problem (which explains the name
MMOT-DC), thanks to the convexity of the set U (µ) and the entropy function H. Thus, it can be
solved by the DC algorithm [Pham and Bernoussi, 1986, Pham and Le, 1997] as follows: at the
iteration t,

1. Calculate G(t) ∈ ∂((cid:80)M

m=1 Hm)(P (t)).

2. Solve P (t+1) ∈ arg minP ∈U (µ)(cid:104)C − εG(t), P (cid:105) + εH(P ).

This algorithm is very easy to implement. Indeed, the second step is an entropic-regularized MMOT
problem, which admits a unique solution, thanks to the strict convexity of the objective function.
Such solution can be found by the Sinkhorn algorithm 3. In the ﬁrst step, the gradient can be
calculated explicitly. For the sake of simplicity, we illustrate the calculation in a simple case, where
M = 2 and N = 4 with T1 and T2 are two 2-tuples. The function H1 + H2 is continuous, so
G(t) = ∇P (H1 + H2)(P (t)). Given a 4-D probability tensor P , we have

H1(P ) + H2(P ) =

(cid:88)

Pi,j,k,l log (cid:0) (cid:88)

Pi,j,k,l

(cid:1) + Pi,j,k,l log (cid:0) (cid:88)

Pi,j,k,l

(cid:1).

i,j,k,l

i,j

k,l

So,

∂(H1 + H2)
∂Pi,j,k,l

= log



Pi,j,k,l

 +



(cid:88)



i,j

Pi,j,k,l
i,j Pi,j,k,l

(cid:80)

+ log



Pi,j,k,l

 +



(cid:88)



k,l

Pi,j,k,l
k,l Pi,j,k,l

(cid:80)

.

The complete DC algorithm for the problem 4 can be found in the algorithm 1. We observed that
initialization is crucial to the convergence of algorithm, which is not surprising for a non-convex
problem. To accelerate the algorithm for large ε, we propose to use the warm-start strategy, which
is similar to the one used in the entropic OT problem with very small regularization parameter
[Schmitzer, 2019]. Its idea is simple: we consider an increasing ﬁnite sequence (εn)N
n=0 approaching
ε such that the solution Pε0 of the problem MMOT-DCε0(T , µ) can be estimated quickly and
accurately using the initialization P (0). Then we solve each successive problem MMOT-DCεn (T , µ)
using the previous solution Pεn−1 as initialization. Finally, the problem MMOT-DCε(T , µ) is solved
using the solution PεN as initialization.

6

Algorithm 2 DC algorithm with warm start for the problem 3.
Input. Cost tensor C, tuple partition T = (Tm)M
n=1,
hyperparameter ε > 0, initialization P (0), initial ε0 > 0, step size s > 1, tuple of initial dual vectors
(f (0)
1 , ..., f (0)
N ).
Output. Tensor P ∈ U (µ).
1. While ε0 < ε:

m=1, collection of histograms µ = (µn)N

(a) Using algorithm 1, solve the problem MMOT-DCε0(T , µ) with initialization P (0)
N ) to ﬁnd the solution Pε0 and its associated tuple of dual vectors

and (f (0)
(f (ε0)
1

1 , ..., f (0)
, ..., f (ε0)
N ).

(b) Set P (0) = Pε0, f (0)
(c) Increase regularization: ε0 := sε0.

i = f (ε0)

i

, for i = 1, ..., N .

2. Using algorithm 1, solve the problem MMOT-DCε(T , µ) using the initialization P (0) and

(f (0)

1 , ..., f (0)
N ).

5 Experimental evaluation

In this section, we illustrate the use of MMOT-DC on simulated data. Rather than performing
experiments in full generality, we choose the setting where N = 4 and M = 2 with T1 = (1, 2)
and T2 = (3, 4), so that we can compare MMOT-DC with other popular solvers of COOT and
GW distance. Given two matrices X and Y , we always consider the 4-D cost tensor C, where
Ci,j,k,l = |Xi,k − Yj,l|2. On the other hand, we are not interested in the 4-D minimiser of MMOT-
DC, but only in its two T1, T2-marginal matrices.

Solving COOT on a toy example. We generate a random matrix X ∈ R30×25, whose entries are
drawn independently from the uniform distribution on the interval [0, 1). We equip the rows and
columns of X with two discrete uniform distributions on [30] and [25]. We ﬁx two permutation
matrices Qs ∈ R30×30 (called sample permutation) and Qf ∈ R25×25 (called feature permutation),
then calculate Y = QsXQf . We also equip the rows and columns of Y with two discrete uniform
distributions on [30] and [25].

It is not difﬁcult to see that COOT(X, Y ) = 0 because (Qs, Qf ) is a solution. As COOT is a special
case of F-MMOT, we see that MMOT-DCε(T , µ) = 0, for every ε > 0, by proposition 3.1. In this
experiment, we will check if marginalizing the minimizer of MMOT-DC allows us to recover the
permutation matrices Qs and Qf . As can be seen from the ﬁgure 1, MMOT-DC can recover the
permutation positions, for various values of ε. On the other hand, it can not recover the true sparse
permutation matrices because the Sinkhorn algorithm applied to the MMOT problem implicitly
results in a dense tensor, thus having dense marginal matrices. For this reason, the loss only remains
very close to zero, but never exactly.

We also plot, with some abuse of notation,
the histograms of the difference between the
(1, 3), (1, 4), (2, 3), (2, 4)-marginal matrices of MMOT-DC and their corresponding counterparts
from F-MMOT. In this example, in theory, as the optimal tensor P of F-MMOT can be factorized
as P = P#T1 ⊗ P#T2 = Qs ⊗ Qf , it is immediate to see that P#(1,3) = P#(1,4) = P#(2,3) =
P#(2,4) ∈ R30×25 are uniform matrices whose entries are 1

750 .

Quality of the MMOT-DC solutions. Now, we consider the situation where the true matching
between two matrices is not known in advance and investigate the quality of the solutions returned by
MMOT-DC to solve the COOT and GW problems. This means that we will look at the COOT loss
(cid:104)C, Qs ⊗ Qf (cid:105), where the smaller the loss, the better when using both exact COOT and GW solvers
and our relaxation.
We generate two random matrices X ∈ R20×3 and Y ∈ R30×2, whose entries are drawn indepen-
dently from the uniform distribution on the interval [0, 1). Then we calculate two corresponding
squared Euclidean distance matrices of size 20 and 30. Their rows and columns are equipped with the
discrete uniform distributions. In this case, [Redko et al., 2020] show that the COOT loss coincides

7

Figure 1: Couplings generated by COOT and MMOT-DC on the matrix recovering task.

Figure 2: Histograms of difference between true independent marginal matrices and their approxima-
tions. We see that the marginal matrices obtained by the algorithm 1 approximate well the theoretical
uniform matrices.

with the GW distance, and the Block Coordinate Descent (BCD) algorithm used to approximate
COOT is equivalent to the Frank-Wolfe algorithm [Frank and Wolfe, 1956] used to solve the GW
distance.

We compare four solvers:

1. The Frank-Wolfe algorithm to solve the GW distance (GW-FW).

al., 2016]

2. The projected gradient algorithm to solve the entropic GW distance [Peyré
et
from
choose
{0.0008, 0.0016, 0.0032, 0.0064, 0.0128, 0.0256} and pick the one which corresponds to
smallest COOT loss.

regularization parameter

(EGW-PGD). We

the

3. The Block Coordinate Descent algorithm to approximate the entropic COOT [Redko
et al., 2020] (EGW-BCD), where two additional KL divergences corresponding
to two couplings are introduced. Both regularization parameters are tuned from
{0, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1}, where 0 means that there is no regular-

8

True permutation on samplesRecovered by MMOT-DC with  =0.0001, loss = 8.7e-06Recovered by MMOT-DC with  =1, loss = 8.6e-06True permutation on features2.50.02.55.07.51e70100200300400(1,3)-marginal0.250.000.250.500.751.001e60100200300400(1,4)-marginal2.01.51.00.50.01e70100200300400500600(1,3)-marginal2.01.51.00.50.01e70100200300400500600(1,4)-marginal2.50.02.55.07.51e70100200300400(2,3)-marginal0.250.000.250.500.751.001e60100200300400(2,4)-marginal2.01.51.00.50.01e70100200300400500600(2,3)-marginal2.01.51.00.50.01e70100200300400500600(2,4)-marginalFor =0.0001For =1GW-FW
0.0829 (± 0.0354)

EGW-PGD
0.0786 (± 0.0347)

EGW-BCD
0.0804 (± 0.0353)

MMOT-DC
0.0822 (± 0.0364)

Table 1: Average and standard deviation of COOT loss of the solvers. MMOT-DC is competitive to
other solvers, except for EGW-PGD and EGW-BCD.

Figure 3: Scatter plots of MMOT-DC versus other solvers. In all three plots, the points tend to
concentrate around the line y = x, which indicates the comparable performance of MMOT-DC. On
the other hand, the top-right plot shows the clear superiority of EGW-PGD.

ization term for the corresponding coupling and we pick the pair whose COOT loss is the
smallest.

4. The algorithm 1 to solve the MMOT-DC. We tune ε ∈ {1, 1.4, 1.8, 2.2, 2.6} and we pick

the one which corresponds to smallest COOT loss.

For GW-FW and EGW-PGD, we use the implementation from the library PythonOT [Flamary et al.,
2021].

Given two random matrices, we record the COOT loss corresponding to the solution generated by
each method. We simulate this process 70 times and compare their overall performance. We can see
in Table 1 the average value and standard deviation and the comparison for the values of the loss
between the different algorithms in Figure 3. The performance is quite similar across methods with a
slight advantage for EGW-PGD. This is in itself a very interesting result that has never been noted, to
the best of our knowledge: the reason that the entropic version of GW can provide better solution than
solving the exact problem, may be due to the "convexiﬁcation" of the problem, thanks to the entropic
regularization. Our approach is also interestingly better than the exact GW-FW, which illustrates that
the relaxation might help in ﬁnding better solutions despite the non-convexity of the problem.

6 Discussion and conclusion

In this paper, we present a novel relaxation of the factorized MMOT problem called MMOT-DC.
More precisely, we replace the hard constraint on factorization constraint by a smooth regularization
term. The resulting problem not only enjoys an interpolation property between MMOT and factorized
MMOT, but also is a DC problem, which can be solved easily by the DC algorithm. We illustrate
the use of MMOT-DC the via some simulated experiments and show that it is competitive with the
existing popular solvers of COOT and GW distance. One limitation of the current DC algorithm is
that, it is not scalable because it requires storing a full-size tensor in the gradient step computation.
Thus, future work may focus on more efﬁciently designed algorithms, in terms of both time and
memory footprint. Moreover, incorporating additional structure on the cost tensor may also be
computationally and practically beneﬁcial. From a theoretical viewpoint, it is also interesting to
study the extension of MMOT-DC to the continuous setting, which can potentially allow us to further
understand the connection between GW distance and COOT.

Acknowledgements. The authors thank to Thibault Séjourné and Titouan Vayer for the fruitful
discussion on the GW distance. The authors thank the anonymous reviewers for their careful

9

0.050.100.150.20GW-FW0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DCLoss of GW-FW vs MMOT-DCLine y=x0.050.100.150.20EGW-PG0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DCLoss of EGW-PGD vs MMOT-DCLine y=x0.050.100.150.20EGW-BCD0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DCLoss of EGW-BCD vs MMOT-DCLine y=xproofreading and invaluable suggestions. This work is partially funded by the projects OATMIL
ANR-17-CE23-0012, OTTOPIA ANR-20-CHIA-0030 and 3IA Côte d’Azur Investments ANR-19-
P3IA-0002 of the French National Research Agency (ANR). This research was produced within
the framework of Energy4Climate Interdisciplinary Center (E4C) of IP Paris and Ecole des Ponts
ParisTech. This research was supported by the 3rd Programme d’Investissements d’Avenir ANR-
18-EUR-0006-02. This action beneﬁted from the support of the Chair "Challenging Technology for
Responsible Energy" led by l’X – Ecole Polytechnique and the Fondation de l’Ecole Polytechnique,
sponsored by TOTAL, and the Chair "Business Analytics for Future Banking" sponsored by NATIXIS.

References

Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein Space. SIAM Journal on

Mathematical Analysis, 43:904–924, 2011.

Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyré. Iterative
Bregman Projections for Regularized Transportation Problems. SIAM Journal on Scientiﬁc
Computing, 37:1111–1138, 2014.

George David Birkhoff. Tres observaciones sobre el algebra lineal. Universidad Nacional de

Tucuman, Revista, 5:147–150, 1946.

Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-Peter Kriegel, Bernhard Schölkopf,
and Alex J. Smola. Integrating structured biological data by Kernel Maximum Mean Discrepancy.
Bioinformatics, 22(14):49–57, 7 2006.

Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan. Multi-marginal
Wasserstein GAN. Advances in Neural Information Processing Systems, pages 1774–1784, 2019.

Joel E. Cohen and Uriel G. Rothblum. Nonnegative ranks, decompositions, and factorizations of

nonnegative matrices. Linear Algebra and its Applications, 190:149–168, 1993.

Samuel Cohen, K. S. Sesh Kumar, and Marc Peter Deisenroth. Sliced multi-marginal optimal

transport. ICML, 2021.

Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. Advances in

Neural Information Processing Systems, pages 2292–2300, 2013.

Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun ichi Amari, Alain Trouvé, and Gabriel
Peyré. Interpolating between Optimal Transport and MMD using Sinkhorn Divergences. Pro-
ceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2019.

Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas
Chambon, Laetitia Chapel, Adrien Corenﬂos, Kilian Fatras, Nemo Fournier, Léo Gautheron,
Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet,
Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and
Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1–8,
2021. URL http://jmlr.org/papers/v22/20-451.html.

Aden Forrow, Jan-Christian Hütter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan
Weed. Statistical Optimal Transport via Factored Couplings. The 22nd International Conference
on Artiﬁcial Intelligence and Statistics, pages 2454–2465, 2019.

Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research

Logistics Quarterly, 3(1-2):95–110, 1956.

Wilfrid Gangbo and Andrzej Swiech. Optimal maps for the multidimensional Monge-Kantorovich

problem. Communications on Pure and Applied Mathematics, 51:23–45, 1998.

Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity
of sinkhorn divergences. Proceedings of the Twenty-Second International Conference on Artiﬁcial
Intelligence and Statistics, 89:1574–1583, 2019.

10

Isabel Haasler, Rahul Singh, Qinsheng Zhang, Johan Karlsson, and Yongxin Chen. Multi-marginal
optimal transport and probabilistic graphical models. arXiv preprint arXiv:2006.14113, 2020.

Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. Attgan: Facial attribute
editing by only changing what you want. IEEE Trans. Image Process., 28(11):5464–5478, 2019.

Le Hui, Xiang Li, Jiaxin Chen, Hongliang He, and Jian Yang. Unsupervised multi-domain image
translation with domain-speciﬁc encoders/decoders. In 24th International Conference on Pattern
Recognition, ICPR 2018, Beijing, China, August 20-24, 2018, pages 2044–2049. IEEE Computer
Society, 2018.

Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Spatio-temporal alignments: Optimal
transport through space and time. In Proceedings of the Twenty Third International Conference on
Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,
pages 1695–1704. PMLR, 26-28 Aug 2020.

Hans G Kellerer. Duality theorems for marginal problems. Zeitschrift für Wahrscheinlichkeitstheorie

und verwandte Gebiete, 67:399–432, 1984.

Chi-Heng Lin, Mehdi Azabou, and Eva Dyer. Making transport more robust and interpretable by
moving data through a small number of anchor points. Proceedings of the 38th International
Conference on Machine Learning, 139:6631–6641, 2021.

Liang Mi and José Bento. Multi-marginal optimal transport deﬁnes a generalized metric. CoRR,

abs/2001.11114, 2020.

Facundo Mémoli. Gromov-Wasserstein distances and the metric approach to object matching.

Foundations of Computational Mathematics, pages 1–71, 2011.

Gabriel Peyré, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein Averaging of Kernel and

Distance Matrices. International Conference on Machine Learning, 48, 2016.

Tao Dinh Pham and Souad El Bernoussi. Algorithms for Solving a Class of Nonconvex Optimization
Problems. Methods of Subgradients. North-Holland Mathematics Studies, 129:249–271, 1986.

Tao Dinh Pham and An Hoai Thi Le. Convex analysis approach to D.C. programming: Theory,

Algorithm and Applications. Acta Mathematica Vietnamica, 22:289–355, 1997.

Aaditya Ramdas, Nicolás García Trillos, and Marco Cuturi. On Wasserstein Two-Sample Testing

and Related Families of Nonparametric Tests. Entropy, 19, 2017.

Ievgen Redko, Titouan Vayer, Rémi Flamary, and Nicolas Courty. CO-Optimal Transport. Advances

in Neural Information Processing Systems, 2020.

Meyer Scetbon, Marco Cuturi, and Gabriel Peyré. Low-Rank Sinkhorn Factorization. Proceedings of

the 38th International Conference on Machine Learning, 139:9344–9354, 2021.

Bernhard Schmitzer. Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport

Problems. SIAM Journal on Scientiﬁc Computing, 41:1443–1481, 2019.

I. J. Schoenberg. Metric Spaces and Positive Deﬁnite Functions. Transactions of the American

Mathematical Society, 44:522–536, 11 1938.

Thibault Séjourné, François-Xavier Vialard, and Gabriel Peyré. The Unbalanced Gromov Wasserstein

Distance: Conic Formulation and Relaxation. arXiv preprint arXiv:2009.04266, 2020.

Thanh Xuan Vo. Learning with sparsity and uncertainty by Difference of Convex functions optimiza-

tion. PhD thesis, Université de Lorraine, 2015.

11

A Appendix

Derivation of the Sinkhorn algorithm in entropic MMOT. The corresponding entropic dual
problem of the primal problem 1 reads

sup
fn∈Ran

N
(cid:88)

(cid:104)fn, µn(cid:105) − ε

(cid:88)

exp

n=1

i1,...,iN

(cid:16) (cid:80)

n(fn)in − Ci1,...,iN
ε

(cid:17)

+ ε.

For each n ∈ [N ] and in ∈ [an], the ﬁrst order optimality condition reads

0 = (µn)in − exp (cid:0) (fn)in

ε

(cid:1) (cid:88)

exp

i−n

(cid:16) (cid:80)

j(cid:54)=n(fj)ij − Ci1,...,iN
ε

(cid:17)

,

where, with some abuse of notation, we write i−n = (i1, ..., in−1, in+1, ..., iN ). Or, equivalently

(fn)in = ε log(µn)in − ε log

(cid:16) (cid:80)

j(cid:54)=n(fj)ij − Ci1,...,iN
ε

(cid:17)

,

exp

(cid:88)

i−n

or even more compact form

fn = ε log µn − ε log

(cid:16) (cid:80)

j(cid:54)=n(fj)ij − C·,i−n
ε

(cid:17)

.

exp

(cid:88)

i−n

Using the primal-dual relation, we obtain the minimiser of the primal problem 1 by

Pi1,...,iN = exp

(cid:16) (cid:80)

n(fn)in − Ci1,...,iN
ε

(cid:17)

,

for in ∈ [an], with n ∈ [N ]. Similar to the entropic OT, the Sinkhorn algorithm 3 is also usually
implemented in log-domain to avoid numerical instability.

Algorithm 3 Sinkhorn algorithm for the entropic MMOT problem 1 from [Benamou et al., 2014].
Input. Histograms µ1, ..., µN , hyperparameter ε > 0, cost tensor C and tuple of initial dual vectors
(f (0)
Output. Optimal transport plan P and tuple of dual vectors (f1, ...fN ) (optional).

1 , ...f (0)
N ).

1. While not converge: for n = 1, ..., N ,

f (t+1)
n

= ε log µn − ε log

(cid:88)

(cid:104)

exp

i−n

(cid:16) (cid:80)

j<n(f (t+1)

j

j>n(f (t)

j )ij − C·,i−n

)ij + (cid:80)
ε

(cid:17)(cid:105)
.

2. Return tensor P , where for in ∈ [an], with n ∈ [N ],

Pi1,...,iN = exp

(cid:16) (cid:80)

n(fn)in − Ci1,...,iN
ε

(cid:17)

.

F-MMOT of two components (i.e. M = 2) is a variation of low nonnegative rank OT. For
the sake of notational ease, we only consider the simplest case, where N = 4 and M = 2 with
T1 = (1, 2) and T2 = (3, 4). However, the same argument still holds in the general case. First, we
deﬁne three reshaping operations.

• vectorization: concatenates rows of a matrix into a vector.

vec : Rm×n → Rmn,
where each element Ai,j of the matrix A ∈ Rm×n is mapped to a unique element b(i−1)n+j
of the vector b ∈ Rmn, with Ai,j = b(i−1)n+j, for i = 1, ..., m and j = 1, ..., n. Conversely,
each element bk is mapped to a unique element Ak//n,n−k%n, for every k = 1, ..., mn.
Here, k//n is the quotient of the division of k by n and k%n is the remainder of this
division, i.e. if k = qn + r, with 0 ≤ r < n, then k//n = q and k%n = r.

12

• Matrization: transforms a 4D tensor to a 2D tensor (matrix) by vectorizing the ﬁrst two and

the last two dimensions of the tensor.

mat : Rn1×n2×n3×n4 → R(n1n2)×(n3n4),
where, similar to the vectorization, each element Pi,j,k,l of the tensor P ∈ Rn1×n2×n3×n4
is mapped to the unique element A(i−1)n2+j,(k−1)n4+l of the matrix A ∈ R(n1n2)×(n3n4),
with Pi,j,k,l = A(i−1)n2+j,(k−1)n4+l.

• Concatenation: stacks vertically two equal-column matrices.

conv :Rm×d × Rn×d → R(m+n)×d

(cid:0)(u1, ..., um), (v1, ..., vn)(cid:1) → (u1, ..., um, v1, ..., vn)T .

Or, stacks horizontally two equal-row matrices

conh :Rn×p × Rn×q → Rn×(p+q)

(cid:0)(u1, ..., up), (v1, ..., vq)(cid:1) → (u1, ..., up, v1, ..., vq).

Lemma A.1 For any 4-D tensor P ∈ Rn1×n2×n3×n4 , denote π its matrisation. We have,

(cid:16) (cid:88)

P·,·,k,l

(cid:17)

=

vec

k,l
where 1n is the vector of ones in Rn.

n3n4(cid:88)

n=1

π·,n = π1n3n4 ,

Proof of lemma A.1. For (i, j) ∈ [n1] × [n2], we have
(cid:16) (cid:88)

(cid:88)

(cid:17)

vec

P·,·,k,l

=

Pi,j,k,l

k,l

(i−1)n2+j

=

=

k,l
(cid:88)

π(i−1)n2+j,(k−1)n4+l

k,l

n3n4(cid:88)

n=1

π(i−1)n2+j,n.

The result then follows.
Now, let (ei)n1n2
denote π its matrisation, then by lemma A.1, we have, for i ∈ [n1],

(cid:3)
i=1 be the standard basis vectors of R(n1n2), i.e. (ei)k = 1{i=k}. For each P ∈ U (µ),

(µ1)i =

(cid:88)

(cid:88)

j

k,l

Pi,j,k,l =

n2(cid:88)

n3n4(cid:88)

j=1

n=1

π(i−1)n2+j,n,

which can be recast in matrix form as

AT

1 π1n3n4 = µ1
where the matrix A1 = conh(v1, ..., vn1) ∈ R(n1n2)×n1, with vi ∈ R(n1n2), where vi =
(cid:80)in2
Similarly, A2π1n3n4 = µ2, where the matrix A2 =
conh(In2, ..., In2) ∈ Rn2×(n1n2), where In ∈ Rn×n is the identity matrix. Both conditions can be
compactly written as

j=(i−1)n2+1 ej, with i ∈ [n1].

AT

12π1n3n4 = µ12,

2 ) ∈ R(n1n2)×(n1+n2) and µ12 = conv(µ1, µ2) ∈ R(n1+n2).
where the matrix A12 = conh(A1, AT
Note that µ12 is not a probability because its mass is 2. The matrix A12 has exactly 2n1n2 ones and
the rest are zeros. Similarly, for A34 and µ34 deﬁned in the same way as A12 and µ12, respectively,
we establish the equality AT
34 are
totally unimodular, i.e. every square submatrix has determinant −1, 0, or 1.

34πT 1n1n2 = µ34. As a side remark, both matrices AT

12 and AT

To handle the factorization constraint, ﬁrst we recall the following concept.

13

Figure 4: An example of the matrix A12 when n1 = 2 and n2 = 3.

Deﬁnition A.1 Given a nonnegative matrix A, we deﬁne its nonnegative rank by

rank+(A) := min (cid:8)r ≥ 1 : A =

r
(cid:88)

i=1

Mi, where rank(Mi) = 1, Mi ≥ 0, ∀i(cid:9).

By convention, zero matrix has zero (thus nonnegative) rank.

So, the constraint P = P1 ⊗ P2 is equivalent to mat(P ) = vec(P1)vec(P2)T . By lemma 2.1 in
[Cohen and Rothblum, 1993], rank+(A) = 1 if and only if there exist two nonnegative vectors u, v
such that A = uvT . Thus, the factorization constraint is equivalent to rank+

(cid:0)mat(P )(cid:1) = 1.

Denote L = mat(C) and M = n1n2, N = n3n4. Now, the problem 2 can be rewritten as

min
Q∈RM ×N
≥0

(cid:104)L, Q(cid:105)

such that AT
AT
rank+(Q) = 1,

12Q1N = µ12
34QT 1M = µ34

which is a variation of the low nonnegative rank OT problem studied in [Scetbon et al., 2021]. (cid:3)

Proof of proposition 3.1. The inequality MMOT(µ) ≤ MMOT-DCε(T , µ) follows from the
positivity of the KL divergence. On the other hand,

F-MMOT(T , µ) = inf
P ∈UT

(cid:104)C, P (cid:105) + εKL(P |P#T ),

because KL(P |P#T ) = 0, for every P ∈ UT . As UT ⊂ U (µ), we have MMOT-DCε(T , µ) ≤
F-MMOT(T , µ).

Now, if F-MMOT(T , µ) = 0, then MMOT-DCε(T , µ) = 0. Conversely, if MMOT-DCε(T , µ) = 0,
for ε > 0, then there exists P ∗ ∈ U (µ) such that (cid:104)C, P ∗(cid:105) = 0 and P ∗ = P ∗
#T (cid:105) = 0,
(cid:3)
which means F-MMOT(T , µ) = 0.

#T . Thus (cid:104)C, P ∗

Proof of proposition 3.2. The function ε → MMOT-DCε(T , µ) is increasing on R≥0 and bounded,
thus admits a ﬁnite limit L ≤ F-MMOT(T , µ), when ε → ∞, and a ﬁnite limit l ≥ MMOT(µ),
when ε → 0.

Let Pε be a solution of the problem MMOT-DCε(T , µ). As U (µ) is compact, when either ε → 0 or
ε → ∞, one can extract a converging subsequence (after reindexing) (Pεk )k → (cid:101)P ∈ U (µ), when
either εk → 0 or εk → ∞. Thus, the convergence of the marginal distributions is also guaranteed, i.e
(Pεk )#Tm → (cid:101)P#Tm ∈ UTm, for every m ∈ [M ], which implies that Pεk − (Pεk )#T → (cid:101)P − (cid:101)P#T .
When ε → 0, let P ∗ be a solution of the problem MMOT(µ). Then,

(cid:104)C, P ∗(cid:105) ≤ (cid:104)C, Pε(cid:105) + εKL(Pε|(Pε)#T ) ≤ (cid:104)C, P ∗(cid:105) + εKL(P ∗|P ∗

#T ).

14

111000001101100100010010001001By the sandwich theorem, when ε → 0, we have MMOT-DCε(T , µ) → (cid:104)C, P ∗(cid:105) = MMOT(µ).
Furthermore, as

0 ≤ (cid:104)C, Pεk (cid:105) − (cid:104)C, P ∗(cid:105) ≤ εkKL(P ∗|P ∗

#T ),

when εk → 0, it follows that (cid:104)C, (cid:101)P (cid:105) = (cid:104)C, P ∗(cid:105). So (cid:101)P is a solution of the problem MMOT(µ). We
conclude that any cluster point of the sequence of minimisers of MMOT-DCε(T , µ) when ε → 0 is a
minimiser of MMOT(µ). As a byproduct, since

KL(P ∗|P ∗

#T ) − KL(Pεk |(Pεk )#T ) ≥

(cid:104)C, Pεk (cid:105) − (cid:104)C, P ∗(cid:105)
εk

≥ 0,

we also deduce that KL( (cid:101)P | (cid:101)P#T ) ≤ KL(P ∗|P ∗
information").
On the other hand, when ε → ∞, for µ⊗N = µ1 ⊗ ... ⊗ µN , one has

#T ) (so the cluster point (cid:101)P has minimal "mutual

(cid:104)C, µ⊗N (cid:105) + ε × 0 ≥ (cid:104)C, Pε(cid:105) + εKL(Pε|(Pε)#T ) ≥ εKL(Pε|(Pε)#T ).

Thus,

0 ≤ KL(Pε|(Pε)#T ) ≤

1
ε

(cid:104)C, µ⊗N (cid:105) → 0, when ε → ∞,

which means KL(Pε|(Pε)#T ) → 0, when ε → ∞.
KL(Pεk |(Pεk )#T ) → 0. We deduce that KL( (cid:101)P | (cid:101)P#T ) = 0, which implies (cid:101)P = (cid:101)P#T .

In particular, when εk → ∞, we have

Now, as MMOT-DCε(T , µ) ≥ (cid:104)C, Pε(cid:105), when ε → ∞, we have L ≥ (cid:104)C, (cid:101)P (cid:105) = (cid:104)C, (cid:101)P#T (cid:105) ≥
F-MMOT(T , µ). Thus L = (cid:104)C, (cid:101)P (cid:105) = F-MMOT(T , µ), i.e. MMOT-DCε(T , µ) → F-MMOT(T , µ)
when ε → ∞. In this case, we also have that any cluster point of the sequence of minimisers of
(cid:3)
MMOT-DCε(T , µ) is a minimiser of F-MMOT(T , µ).

In this proof, we write C := L(Cx, Cy), for notational convenience. In
Proof of corollary 3.3.
the setting of GW distance, we have N = 4 and M = 2 with T1 = (1, 2) and T2 = (3, 4). Given
a solution Pε of the problem 3, we also write Pε,i := (Pε)#Ti, for short. Now, for i = 1, 2, let
Qi ∈ U (Pε,i, Pε,i) ⊂ U (µ). The optimality of Pε implies that

(cid:104)C, Pε(cid:105) + ε(cid:2)H(Pε) − H(Pε,1) − H(Pε,2)(cid:3) ≤ (cid:104)C, Qi(cid:105) + ε(cid:2)H(Qi) − 2H(Pε,i)(cid:3).

Thus,

2(cid:0)(cid:104)C, Pε(cid:105) + εH(Pε)(cid:1) ≤

2
(cid:88)

i=1

(cid:104)C, Qi(cid:105) + εH(Qi).

As this is true for every Qi ∈ U (Pε,i, Pε,i), we have

1
2

2
(cid:88)

i=1

OTε(Pε,i, Pε,i) =

1
2

2
(cid:88)

i=1

inf
Qi∈U (Pε,i,Pε,i)

(cid:104)C, Qi(cid:105) + εH(Qi)

≥ (cid:104)C, Pε(cid:105) + εH(Pε)
inf
≥
P ∈U (Pε,1,Pε,2)
= OTε(Pε,1, Pε,2).

(cid:104)C, P (cid:105) + εH(P )

The second inequality holds because Pε ∈ U (Pε,1, Pε,2). Thus,

OTε(Pε,1, Pε,2) −

1
2

2
(cid:88)

i=1

OTε(Pε,i, Pε,i) ≤ 0.

(5)

The left-hand side of the inequality 5 is nothing but the Sinkhorn divergence between Pε,1 and Pε,2
[Ramdas et al., 2017]. As the kernel C is conditionally negative deﬁnite if and only if for every ε > 0,
the kernel e−C/ε is positive deﬁnite [Schoenberg, 1938], by proposition 5 in [Janati et al., 2020], the
inequality in 5 becomes an equality. As a consequence, for i = 1, 2, if Qε,i ∈ U (Pε,i, Pε,i) is the
(unique) optimal plan of the entropic OT problem OTε(Pε,i, Pε,i), then we must have

(cid:104)C, Pε(cid:105) + ε(cid:2)H(Pε) − H(Pε,1) − H(Pε,2)(cid:3) = (cid:104)C, Qε,i(cid:105) + ε(cid:2)H(Qε,i) − 2H(Pε,i)(cid:3),

15

or equivalently, Qε,1 and Qε,2 are also solutions of the problem 3.
Now, by proposition 3.2, when ε → ∞, a cluster point P ∗ = P ∗
minimisers (Pε)ε induces a solution (P ∗
cluster point of (Pε,i)ε and there exists a cluster point Q∗
But still by proposition 3.2, we also have that Q∗
and the solution (P ∗
P ∗
(cid:105) = (cid:104)C, P ∗
#T1
and P ∗

i = (Q∗
) of the COOT problem satisﬁes: (cid:104)C, P ∗

are two solutions of the GW problem.

i )#T1 ⊗ (Q∗

#T1
⊗ P ∗

, P ∗

, P ∗

#T1

#T2

#T2

#T2

#T1

i of (Qε,i)ε in U (P ∗

#Ti

, P ∗
i )#T2 . Thus, Q∗
#T2

⊗ P ∗

#T1

⊗ P ∗

#T2

#T2
#T1
(cid:105). The equality between GW distance and COOT then follows, and P ∗

of the sequence of
is a
#Ti
), for i = 1, 2.

#Ti
i = P ∗
#Ti
(cid:105) = (cid:104)C, P ∗

⊗ P ∗

#Ti
⊗

) of the COOT problem. In particular, P ∗

#T1

#T2

If furthermore, the kernel C induces a strictly positive deﬁnite kernel, then by proposition 5 in
[Janati et al., 2020], we deduce that Pε,1 = Pε,2. One can also use the following reasoning: in the
ﬁnite setting, a strictly positive deﬁnite kernel is necessarily universal (see for example section 2.3
in [Borgwardt et al., 2006]), and the kernel C deﬁned on (X × Y)2 is necessarily a (symmetric)
Lipschitz function with respect to both inputs. So, the Sinkhorn divergence vanishes if and only if
(cid:3)
Pε,1 = Pε,2 [Feydy et al., 2019]. From either reasoning, we conclude that P ∗

= P ∗

.

#T1

#T2

Intuitively, for sufﬁciently large ε, the minimisation of the KL divergence
An empirical variation.
is prioritised over the linear term in the objective function of the MMOT-DC problem, which implies
that the optimal tensor P ∗ is "close" to its corresponding tensor product P ∗
#T . So, instead of
calculating the gradient at P , one may calculate at P#T . In this case, the gradient reads

M
(cid:88)

m=1

∇P Hm(P#T ) = (cid:2) log P#T1 + P#T1

(cid:3) ⊕ ... ⊕ (cid:2) log P#TM + P#TM

(cid:3),

where ⊕ represents the tensor sum operator between two arbitrary-size tensors: (A⊕B)i,j := Ai+Bj,
where with some abuse of notation, i or j can be understood as a tuple of indices. Thus, we avoid
storing the N -D gradient tensor (as in the algorithm 1) and only need to store M smaller-size tensors.
Not only saving the memory, this variation also seems to be empirically competitive with the original
algorithm 1, if not sometimes better, in terms of COOT loss. The underlying reason might be related
to the approximate DCA scheme [Vo, 2015], where one replaces both steps in each DC iteration by
their approximation. We leave the formal theoretical justiﬁcation of this variation to the future work.
We call this variation MMOT-DC-v1 and use the same setup as in the experiment 5.

Figure 5: Scatter plots of MMOT-DC-v1 versus other solvers. In all three plots, the points tend to
concentrate around the line y = x, which indicates the comparable performance of MMOT-DC-v1.
On the other hand, the top-right plot shows the clear superiority of EGW-PGD.

MMOT-DC
0.0822 (± 0.0364)

MMOT-DC-V1
0.0820 (± 0.0361)

Table 2: Average and standard deviation of COOT loss of MMOT-DC and MMOT-DC-v1. The
performance of the two algorithms is very similar.

16

0.050.100.150.20GW-FW0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DC-v1Loss of GW-FW vs MMOT-DC-v1Line y=x0.050.100.150.20EGW-PG0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DC-v1Loss of EGW-PGD vs MMOT-DC-v1Line y=x0.050.100.150.20EGW-BCD0.0250.0500.0750.1000.1250.1500.1750.2000.225MMOT-DC-v1Loss of EGW-BCD vs MMOT-DC-v1Line y=x