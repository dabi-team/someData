A Scalable AutoML Approach Based on Graph Neural Networks

Mossad Helaliâˆ—, Essam Mansourâˆ—, Ibrahim AbdelazizÂ§, Julian DolbyÂ§, Kavitha SrinivasÂ§

âˆ—Concordia University
Montreal, Canada

Â§IBM Research AI
New York, US

{fname}.{lname}@concordia.ca,ibrahim.abdelaziz1,dolby,Kavitha.Srinivas@ibm.com

2
2
0
2

l
u
J

4
1

]

G
L
.
s
c
[

4
v
3
8
0
0
0
.
1
1
1
2
:
v
i
X
r
a

ABSTRACT
AutoML systems build machine learning models automatically by
performing a search over valid data transformations and learners,
along with hyper-parameter optimization for each learner. Many
AutoML systems use meta-learning to guide search for optimal
pipelines. In this work, we present a novel meta-learning system
called KGpip which (1) builds a database of datasets and correspond-
ing pipelines by mining thousands of scripts with program analysis,
(2) uses dataset embeddings to find similar datasets in the database
based on its content instead of metadata-based features, (3) models
AutoML pipeline creation as a graph generation problem, to suc-
cinctly characterize the diverse pipelines seen for a single dataset.
KGpipâ€™s meta-learning is a sub-component for AutoML systems. We
demonstrate this by integrating KGpip with two AutoML systems.
Our comprehensive evaluation using 121 datasets, including those
used by the state-of-the-art systems, shows that KGpip significantly
outperforms these systems.

PVLDB Reference Format:
Mossad Helaliâˆ—, Essam Mansourâˆ—, Ibrahim AbdelazizÂ§, Julian DolbyÂ§,
Kavitha SrinivasÂ§. A Scalable AutoML Approach Based on Graph Neural
Networks. PVLDB, 14(1): XXX-XXX, 2020.
doi:XX.XX/XXX.XX

PVLDB Artifact Availability:
The source code, data, and/or other artifacts have been made available at
https://github.com/CoDS-GCS/kgpip-public.

1 INTRODUCTION
AutoML is the process by which machine learning models are built
automatically for a new dataset. Given a dataset, AutoML systems
perform a search over valid data transformations and learners,
along with hyper-parameter optimization for each learner [17].
Choosing the transformations and learners over which to search
is our focus. A significant number of systems mine from prior
runs of pipelines over a set of datasets to choose transformers and
learners that are effective with different types of datasets (e.g. [10],
[32], [9]). Thus, they build a database by actually running different
pipelines with a diverse set of datasets to estimate the accuracy of
potential pipelines. Hence, they can be used to effectively reduce
the search space. A new dataset, based on a set of features (meta-
features) is then matched to this database to find the most plausible

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

candidates for both learner selection and hyper-parameter tuning.
This process of choosing starting points in the search space is called
meta-learning for the cold start problem.

Other meta-learning approaches include mining existing data
science code and their associated datasets to learn from human
expertise. The AL [2] system mined existing Kaggle notebooks using
dynamic analysis, i.e., actually running the scripts, and showed that
such a system has promise. However, this meta-learning approach
does not scale because it is onerous to execute a large number of
pipeline scripts on datasets, preprocessing datasets is never trivial,
and older scripts cease to run at all as software evolves. It is not
surprising that AL therefore performed dynamic analysis on just
nine datasets.

Our system, KGpip, provides a scalable meta-learning approach
to leverage human expertise, using static analysis to mine pipelines
from large repositories of scripts. Static analysis has the advantage
of scaling to thousands or millions of scripts [1] easily, but lacks the
performance data gathered by dynamic analysis. The KGpip meta-
learning approach guides the learning process by a scalable dataset
similarity search, based on dataset embeddings, to find the most
similar datasets and the semantics of ML pipelines applied on them.
Many existing systems, such as Auto-Sklearn [9] and AL [2], com-
pute a set of meta-features for each dataset. We developed a deep
neural network model to generate embeddings at the granularity of
a dataset, e.g., a table or CSV file, to capture similarity at the level
of an entire dataset rather than relying on a set of meta-features.
Because we use static analysis to capture the semantics of the
meta-learning process, we have no mechanism to choose the best
pipeline from many seen pipelines, unlike the dynamic execution
case where one can rely on runtime to choose the best performing
pipeline. Observing that pipelines are basically workflow graphs,
we use graph generator neural models to succinctly capture the
statically-observed pipelines for a single dataset. In KGpip, we
formulate learner selection as a graph generation problem to predict
optimized pipelines based on pipelines seen in actual notebooks.
KGpip does learner and transformation selection, and hence is a
component of an AutoML systems. To evaluate this component, we
integrated it into two existing AutoML systems, FLAML [30] and
Auto-Sklearn [9]. We chose FLAML because it does not yet have any
meta-learning component for the cold start problem and instead
allows user selection of learners and transformers. The authors of
FLAML explicitly pointed to the fact that FLAML might benefit
from a meta-learning component and pointed to it as a possibility
for future work. For FLAML, if mining historical pipelines provides
an advantage, we should improve its performance. We also picked
Auto-Sklearn as it does have a learner selection component based
on meta-features, as described earlier [8]. For Auto-Sklearn, we
should at least match performance if our static mining of pipelines

 
 
 
 
 
 
can match their extensive database. For context, we also compared
KGpip with the recent VolcanoML [17], which provides an efficient
decomposition and execution strategy for the AutoML search space.
In contrast, KGpip prunes the search space using our meta-learning
model to perform hyperparameter optimization only for the most
promising candidates.

The contributions of this paper are the following:

â€¢ Section 3 defines a scalable meta-learning approach based
on representation learning of mined ML pipeline semantics
and datasets for over 100 datasets and 11K Python scripts.

â€¢ Sections 4 formulates AutoML pipeline generation as a graph
generation problem. KGpip predicts efficiently an optimized
ML pipeline for an unseen dataset based on our meta-learning
model. To the best of our knowledge, KGpip is the first ap-
proach to formulate AutoML pipeline generation in such a
way.

â€¢ Section 5 presents a comprehensive evaluation using a large
collection of 121 datasets from major AutoML benchmarks
and Kaggle. Our experimental results show that KGpip out-
performs all existing AutoML systems and achieves state-
of-the-art results on the majority of these datasets. KGpip
significantly improves the performance of both FLAML and
Auto-Sklearn in classification and regression tasks. We also
outperformed AL in 75 out of 77 datasets and VolcanoML
in 75 out of 121 datasets, including 44 datasets used only by
VolcanoML [17]. On average, KGpip achieves scores that are
statistically better than the means of all other systems.

2 RELATED WORK
In this section, we summarize the related work and restrict our re-
view to meta-learning approaches for AutoML, dataset embeddings,
and processing tabular structured data.

Learner and preprocessing selection. In most AutoML systems, learner
and pre-processing selection for the cold start problem is driven by
a database of actual executions of pipelines and data; e.g., [2], [9],
[10]. This database often drives both learner selection and hyper
parameter optimization (HPO), so we focus here more on how the
database is collected or applied to either problem, since the actual
application to learner selection or HPO is less relevant. For HPO,
some have cast the application of the database as a multi-task prob-
lem (see [26]), where the hyperparameters for cold start are chosen
based on multiple related datasets. Others, for instance, [9, 25], com-
pute a database of dataset meta-features on a variety of OpenML
[28] datasets, including dataset properties such as the number of
numerical attributes, the number of samples or skewness of the
features in each dataset.

These systems measure similarity between datasets and use
pipelines from the nearest datasets based on the distance between
the datasetsâ€™ feature vectors as we do, but the computation of these
vectors is different, as we describe in detail below. Auto-Sklearn 2.0
[8] defines instead a static portfolio of pipelines that work across
a wide variety of datasets, and use these to cold-start the learner
selection component - that is, every new dataset uses the same
set of pipelines. Others have created large matrices documenting

2

the performance of candidate pipelines for different datasets and
viewed the selection of related pipelines as a collaborative filtering
problem [10].

Dataset embeddings. The most used mechanism to capture dataset
features rely on the use of meta-features for a dataset such as
[9, 25]. These dataset properties vary from simple, such as number
of classes (see, e.g. [6]), to complex and expensive, such as statisti-
cal features (see, e.g. [29]) or landmark features (see, e.g. [23]). As
pointed out in Auto-Sklearn 2.0 [8], these meta-features are not
defined with respect to certain column types such as categorical
columns, and they are also expensive to compute, within limited
budgets. The dataset embedding we adopt is builds individual col-
umn embeddings, and then pools these for a table level embedding.
Similar to our approach, Drori et al. [5] use pretrained language
models to get dataset embeddings based on available dataset tex-
tual information, e.g. title, description and keywords. Given these
embeddings, their approach tries to find the most similar datasets
and their associated baselines. Unlike [5], our approach relies on
embedding the actual data inside the dataset and not just their
overall textual description, which in many cases is not available.
OBOE [33] uses the performance of a few inexpensive, informative
models to compute features of a model.

Pipeline generation. There is a significant amount of work viewing
the selection of learners as well as hyperparameters as a bayesian
optimization problem like [26, 27]. Other systems have used evolu-
tionary algorithms along with user defined templates or grammars
for this purpose such as TPOT [15] or Recipe [4]. Still, others have
viewed the problem of pipeline generation as a probabilistic matrix
factorization [10], an AI planning problem when combined with
a user specified grammar [14, 31], a bayesian optimization prob-
lem combined with Monte Carlo Tree Search [24], or an iterative
alternating direction method of multipliers optimization (ADMM)
problem [19]. Systems like VolcanoML focus on an efficient decom-
position of the search space [17]. To the best of our knowledge,
KGpip is the first system to cast the actual generation of pipelines
as a neural graph generation problem.

Some recent AutoML systems have moved away from the fairly
linear pipelines generated by most earlier systems to use ensembles
or stacking extensively. H2O for instance uses fast random search
in combination with ensembling for the problem of generating
pipelines [16]. Others rely on "stacking a bespoke set of models in
a predefined order", where stacking and training is handled in a
special manner to achieve strong performance [7]. Similarly, PIPER
[20] uses a greedy best-first search algorithm to traverse the space
of partial pipelines guided over a grammar that defines complex
pipelines such as Directed Acyclic Graphs (DAGs). The pipelines
produced by PIPER are more complex than the linear structures used
in the current AutoML systems we use to test our ideas for historical
pipeline modeling, and we do not use ensembling techniques yet
in our approach. Neither is precluded, however, because KGpip
meta-learning model can generate any type of structures, including
complex structures that mined pipelines may have.

Figure 2: An example from a data science notebook.

Figure 1: An overview of KGpipâ€™s meta-learning approach
for mining a database of ML pipelines to train a graph gen-
erator model to predict ML pipeline skeletons in the form
of graphs.

3 THE KGPIP SCALABLE META-LEARNING
Our meta-learning approach is based on mining large databases
of ML pipelines associated with the used datasets, as illustrated in
Figure 1. The mining process uses static program analysis instead
of executing the actual pipeline scripts or preparing the actual raw
data. The KGpip meta-learning component enhances the search
strategy of existing AutoML systems, such as AutoSklearn and
FLAML, and allows these systems to handle ad-hoc datasets, i.e.,
unseen ones. To retain a maximal degree of flexibility, KGpip cap-
tures metadata and semantics in a flexible graph format, and relies
on graph generator models as the database of pipelines.

Unlike existing meta-learning approaches, our approach is de-
signed to learn from a large scale database and achieve high de-
gree of coverage and diversity. Several ML portals, such as Kag-
gle or OpenML [28], provide access to thousands of datasets asso-
ciated with hundreds of thousands of public notebooks, i.e., ML
pipelines/code. KGpip mines these large databases of datasets and
pipelines using static analysis and filters them into ML pipelines
customized for the learner selection problem. The KGpip meta-
learning approach leverages [1] for code understanding via static
analysis of scripts/code of ML pipelines. It extracts the semantics
of these scripts as code and form an initial graph for each script.

KGpip cleans the graphs generated by [1] to keep the semantic
required for the ML meta-learning process. Furthermore, our ap-
proach introduces dataset nodes and interlinks the relevant pipeline
semantic to them. So, our meta-learning approach produces MetaPip,
a highly interconnected graph of seen datasets and pipelines applied
to them. We also developed a deep embedding model to find the
closest datasets to an unseen one, i.e., to effectively prune MetaPip.
We then train a deep graph generator model [18] using MetaPip.
This model is the core of our meta-learning component as illustrated
in Figure 1 and discussed in the next section.

3.1 Graph Representation of Code Semantics
Static and dynamic program analysis techniques could be used to ab-
stract the semantics of programs and extract language-independent
representations of code. A program source code is examined in
the static analysis without running the program. In contrast, dy-
namic analysis examines the source code during runtime to collect
memory traces and more detailed statistics specific to the analysis
technique. Unlike static analysis, dynamic analysis helps in cap-
turing more rich semantics from programs with the high cost of
execution and storing massive memory traces. ML portals, such as

Figure 3: Code graph corresponding to Figure 2 obtained
with GraphGen4Code. The graph shows control flow with
gray edges and data flow with black edges. Numerous other
nodes and edges are not shown for simplicity.

Figure 4: Our MetaPip graph of the graph from Figure 3,
where the abstracted ML pipeline is linked to a dataset node
(highlighted in Orange). MetaPip contains at least 96% less
nodes and edges than the original graph while enhancing
the overall quality of the graph generation process, as ex-
perimented in Section 5.5.

Kaggle, have hundreds of thousands of ML pipelines with no instruc-
tions for running or managing the environments of these pipelines.
KGpip combines dataset embedding with static code analysis tools,
such as GraphGen4Code [1], to enrich the collected semantics of
ML pipelines while avoiding the need to run them.

GraphGen4Code is optimized to efficiently process millions of
Python programs, performing interprocedural data flow and control
flow analysis to examine for instance, what happens to data that
is read from a Pandas dataframe, how it gets manipulated and
transformed, and what transformers or estimators get called on the
dataframe. GraphGen4Codeâ€™s graphs make it explicit what APIs
and functions are invoked on objects without the need to model
the used libraries themselves; hence GraphGen4Code can scale
static analysis to millions of programs. Figures 2 and 3 show a
small code snippet and its corresponding static analysis graph from
GraphGen4Code, respectively. As shown in Figure 3, the graph
captures control flow (gray edges), data flow (black edges), as well
as numerous other nodes and edges that are not shown in the
figure. Examples of these nodes and edges include those capturing
location of calls inside a script file and function call parameters.
For example, GraphGen4Code generates a graph of roughly 1600
nodes and 3700 edges for a Kaggle ML pipeline script of 72 lines of
code. The number of nodes and edges dominate the complexity of
training a graph generator model.

3.2 MetaPip: from Code to Pipeline Semantics
For AutoML systems, a pipeline is a set of data transformations,
learner selection, and hyper-parameter optimization for each model
that is selected. Mined data science notebooks often contain data
analysis, data visualization, and model evaluation. Moreover, each

3

Â df = pd.read_csv('example.csv')Â df_train, df_test = train_test_split(df)Â X = df_train['X'] Â model = svm.SVC()Â model.fit(X, df_train['Y'])read_csvtrain_test_splitdf_train[â€˜Xâ€™]df_train[â€˜Yâ€™]SVCfitread_csvtrain_test_splitdf_train[â€˜Xâ€™]df_train[â€˜Yâ€™]SVCfitread_csvtrain_test_splitSVCfitexample.csvnotebook is associated with one or more datasets. Thus, it is essen-
tial for our meta-learning model to distinguish between different
types of pipelines and realize this association with datasets. Existing
systems for static code analysis extract general semantics of code
and cannot link pipeline scripts to the used datasets. Thus, the gen-
erated graphs by systems, such as GraphGen4Code, are scattered
and unlinked, i.e., a graph per an ML pipeline script. Moreover, each
graph will have nodes and edges that are not relevant for the meta-
learning process. These irrelevant nodes and edges, i.e., triples, will
add noise to the training data. Hence, a meta-learning model will
not be able to learn from the abstracted graph pipelines generated
by such tools, as shown in Table 4. We developed a method to filter
out this kind of triples from GraphGen4Codeâ€™s graph and analyze
ML pipelines to prepare a training set interconnecting repositories
of ML pipeline scripts with their associated datasets. Moreover,
our method cleans the noisy nodes and edges and calls to modules
outside the target ML libraries. For example, our method will ex-
tract triples related to libraries, such as Scikit-learn, XGBoost, and
LGBM. These libraries are the most popular among the top-scoring
ML pipelines in ML portals. The code for the cleaning method is
available at the KGpipâ€™s repository.

Our meta-learning component aims to pick learners and trans-
former for unseen datasets. Thus, KGpip links the filtered ML
pipelines with the used datasets. The result of adding these dataset
nodes is a highly interconnected graph for ML pipelines, we refer to
it as MetaPip. Our MetaPip graph captures both the code and data
aspects of ML pipelines. Hence, we can populate the MetaPip graph
with datasets from different sources, such as OpenML and Kaggle,
and pipelines applied on these datasets. Figure 4 shows the MetaPip
graph corresponding to the code snippet in Figure 2. KGpip utilizes
MetaPip to train a model based on a large set of pipelines associated
with similar datasets. For example, a pandas.read_csv node will be
linked to the used table node, i.e., csv file. In some cases, the code,
which reads a csv file, does not explicitly mention the dataset name.
The pipelines are usually associated with datasets, such as Kaggle
pipelines and datasets, as shown in Figure 1.

3.3 Dataset Representation Learning
Our approach efficiently guides the meta-learning process by link-
ing the extracted semantics of pipelines to dataset nodes represent-
ing the used datasets. There is a sheer amount of datasets of variable
sizes and we need to develop a scalable method for finding the most
similar datasets for an unseen one. The pairwise comparison based
of the actual content of datasets, i.e, tuples in CSV files, does not
scale. Thus, we developed a dataset representation learning method
to generate a fixed-size and dense embedding at the granularity of
a dataset, e.g., a table or CSV file. The embedding of a dataset D is
the average of its column embeddings, i.e.:

â„ğœƒ (D) =

1
|D|

âˆ‘ï¸

ğ‘ âˆˆD

â„ğœƒ (ğ‘)

(1)

where |D| is the number of columns in D. Our work generalizes the
approach outlined in [21] for individual column embeddings, where
column embeddings are obtained by training a neural network on
a binary classification task. The model learns when two columns
represent the same concept, but with different values, as opposed to

Figure 5: An overview of KGpipâ€™s workflows of ML pipeline
generation for a given unseen dataset and certain time
budget. KGpip utilizes systems for hyperparameter opti-
mization, such as FLAML or Auto-Sklearn, to optimize KG-
pipâ€™s top-K predicted pipelines (ğ‘‰ ğº), i.e., pruning the search
space.

columns representing different concepts. Embeddings for an unseen
dataset are produced by the last layer of the neural net.

KGpip reads datasets only once and leverages PySpark DataFrame
to achieve high task and data parallelism. We use the embeddings
of datasets to measure their similarity. With these embeddings,
we build an index of vector embeddings for all the datasets in our
training set. We utilize efficient libraries [13] for similarity search
of dense vectors to retrieve the most similar dataset to a new input
dataset based on its embeddings. Thus, our method scales well and
leads to accurate results in capturing similarities between datasets.

4 THE KGPIP PIPELINE AUTOMATION
The KGpip workflow for pipeline automation is based on our meta-
leaning model, as illustrated in Figure 5. KGpip predicts the top-K
pipeline skeletons, i.e., a specific set {ğ‘ƒ, ğ¸} of Preprocessor (ğ‘ƒ) and
Estimators (ğ¸), for an unseen dataset (ğ·) based on the most similar
seen dataset (ğ‘†ğ·), i.e., the nearest neighbour dataset. KGpip starts
by finding ğ‘†ğ· based on the embedding of the unseen dataset. Then,
KGpip generates the top-K validated ML pipeline graphs ğ‘‰ ğº and
converts them into ML pipeline skeletons {ğ‘ƒ, ğ¸}. Then, it performs
hyperparameter optimization using systems, such as FLAML [30]
and Auto-Sklearn [9], to find the optimum hyperparameters for
each pipeline skeleton within a specific time budget.

4.1 Graph Generation for ML Pipelines
KGpip formulates the generation of ML pipelines as a graph gen-
eration problem. The intuition behind this idea is that a neural
graph generator might capture more succinctly multiple pipelines
seen in practice for a given dataset, and might also capture statis-
tical similarities between different pipelines more effectively. To
effectively use such a network, we add a single dataset node as the
starting point for the filtered pipelines we generate from Python
notebooks. The node is assumed to flow into a read_csv call which
is often the starting point for the pipelines. For generating an ML
pipeline, we simply pass in a dataset node for the nearest neighbour
of the unseen dataset, i.e., the most similar dataset based on content
similarity, as shown in Figure 5.

Our meta-learning model generates ML pipeline graphs in a
sequential node-by-node fashion. Algorithm 1 illustrates the imple-
mentation of the graph generation model. For an empty graph ğº
and the most similar dataset ğ‘†ğ·, the algorithm starts by adding an
edge between ğ‘†ğ· and pandas.read_csv. Then, the graph neural

4

Algorithm 1: Graph Generation Process

Input: Graph ğº: (ğ¸ = ğœ™, ğ‘‰ = ğœ™), Similar Dataset Node: ğ‘†ğ·,

Neural Networks: ğ‘“ğ´ğ‘‘ğ‘‘ğ‘ ğ‘œğ‘‘ğ‘’, ğ‘“ğ´ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’, ğ‘“ğ¶â„ğ‘œğ‘œğ‘ ğ‘’ğ‘ ğ‘œğ‘‘ğ‘’

1 ğ‘‰ â† ğ‘‰ âˆª {ğ‘†ğ·, ğ‘ğ‘ğ‘›ğ‘‘ğ‘ğ‘ .ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘ ğ‘£ }
2 ğ¸ â† ğ¸ âˆª { (ğ‘†ğ·, ğ‘ğ‘ğ‘›ğ‘‘ğ‘ğ‘ .ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘ ğ‘£) }
3 ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ´ğ‘‘ğ‘‘ = ğ‘“ğ´ğ‘‘ğ‘‘ğ‘ ğ‘œğ‘‘ğ‘’ (ğ‘‰ , ğ¸)
4 while ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ´ğ‘‘ğ‘‘ â‰  ğ‘ğ‘¢ğ‘™ğ‘™ do
ğ‘‰ â† ğ‘‰ âˆª {ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ´ğ‘‘ğ‘‘ }
5
ğ‘ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ = ğ‘“ğ´ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ (ğ‘‰ , ğ¸)
while ğ‘ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ do

7

6

ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ¿ğ‘–ğ‘›ğ‘˜ = ğ‘“ğ¶â„ğ‘œğ‘œğ‘ ğ‘’ğ‘ ğ‘œğ‘‘ğ‘’ (ğ‘‰ , ğ¸)
ğ¸ â† ğ¸ âˆª { (ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ´ğ‘‘ğ‘‘, ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ¿ğ‘–ğ‘›ğ‘˜) }
ğ‘ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ â† ğ‘“ğ´ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ (ğ‘‰ , ğ¸)

8

9

10

11

end
ğ‘›ğ‘œğ‘‘ğ‘’ğ‘‡ ğ‘œğ´ğ‘‘ğ‘‘ â† ğ‘“ğ´ğ‘‘ğ‘‘ğ‘ ğ‘œğ‘‘ğ‘’ (ğ‘‰ , ğ¸)

12
13 end
14 ğ‘‰ ğº = ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’_ğ‘ğ‘–ğ‘ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’_ğ‘”ğ‘Ÿğ‘ğ‘â„ (ğº)
15 return VG

network ğ‘“ğ´ğ‘‘ğ‘‘ğ‘ ğ‘œğ‘‘ğ‘’ decides whether to add a new node of a cer-
tain type. The network ğ‘“ğ´ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ decides whether to add an edge
to the newly added node. Then, the network ğ‘“ğ¶â„ğ‘œğ‘œğ‘ ğ‘’ğ‘ ğ‘œğ‘‘ğ‘’ decides
the existing node to which the edge is to be added. The While
loop at line 7 is repeated repeated until no more edges to be added.
The While loop at line 4 is repeated until no more nodes to be
added. The three neural networks, namely ğ‘“ğ´ğ‘‘ğ‘‘ğ‘ ğ‘œğ‘‘ğ‘’ , ğ‘“ğ´ğ‘‘ğ‘‘ğ¸ğ‘‘ğ‘”ğ‘’ , and
ğ‘“ğ¶â„ğ‘œğ‘œğ‘ ğ‘’ğ‘ ğ‘œğ‘‘ğ‘’ , utilize node embeddings that are learned throughout
training via graph propagation rounds. These embeddings capture
the structure of ML pipeline graphs.

The generated graph ğº is not guaranteed to be a valid ML
pipeline. Thus, Algorithm 1 at line 14 checks that ğº is a valid ML
pipeline graph. In KGpip, a graph ğº is valid if 1) it contains at least
one estimator matching the task, i.e., regression or classification,
and 2) the estimator is supported by the hyperparameter optimizer
(AutoSklearn or FLAML in our case). With these modifications, it
is possible to generate ML pipelines for unseen datasets using the
closest seen dataset node â€“ more specifically, its content embedding
obtained from the dataset embedding module. We built Algorithm 1
on top of the system proposed in [18]. This system does not support
conditional graph generation at test time by default, i.e., building a
graph on top of a provided dataset node. We extended this system
to generate valid ML pipeline graphs, as illustrated in Algorithm 1.

4.2 Hyperparameter Optimizion
KGpip maps the valid graphs into ML pipeline skeletons, where
each skeleton is a set of pre-processors and an estimator with place
holders for the optimal parameters. In KGpip, the hyperparam-
eter optimizer is responsible for finding the optimal parameters
for the pre-processors and learners on the target dataset. Then,
KGpip replaces the place holders with these parameters. Finally,
KGpip creates a python script using the pre-processors and estima-
tor achieving the highest scores. KGpip is well designed to support
both numerical and non-numerical datasets. Thus, KGpip applies
different pre-processing techniques on the given dataset (ğ·) and
produces a pre-processed dataset (ğ· â€²). Our pre-processing includes
1) detecting task type (i.e. regression or classification) automatically

5

based on the distribution of the target column 2) automatically infer-
ring accurate data types of columns, 3) vectorizing textual columns
using word embeddings [3], and 4) imputing missing values in the
dataset. In KGpip, the hyperparameter optimizer uses ğ· â€².

Similar to hyperparameter optimizers implemented in AutoML
systems, such as FLAML or Auto-Sklearn, KGpip works within a
provided time budget per dataset. We note here that the majority
of the allotted time budget for ML pipeline generation is spent on
the hyperparameter optimization; that is, if the user desires only
to know what learners would work best for their dataset, KGpip
can do that almost instantaneously. Given a time budget (ğ‘‡ ), KGpip
calculates ğ‘¡, the time consumed in generating and validating the
graphs. KGpip then divides the rest of the time budget between the
ğ¾ graphs. Hence, the hyperparameter optimizer has a time limit of
((ğ‘‡ âˆ’ ğ‘¡)/ğ¾) to optimize each graph independently.

The hyperparameter optimizer repeatedly applies the learners
and pre-processors with different configurations while monitoring
the target score metric throughout. KGpip keeps updating its output
with the best pipeline skeleton, i.e., learners and pre-processors, and
its score. For example, if the predicted learner is LogisticRegression,
it searches for the best combination of regularization type (L1 or
L2) and regularization parameter. The difference between hyper-
parameter optimizers is the search strategy followed to arrive at
the best hyperparameters within the allotted time budget. A naive
approach would be to perform an exhaustive grid search over all
combinations, while a more advanced approach would be to start
with promising configurations first. We integrate KGpip with the
hyperparameter optimizers of both FLAML [30] and Auto-Sklearn
[9] to demonstrate the generality of KGpip. The integration of a
hyperparameter optimizer into KGpip needs a JSON document of
the particular preprocessors and estimators supported by the hyper-
parameter optimizer. Thus, the integration is relatively easy. Finally,
our neural graph generation produces a diverse set of pipelines
across runs, allowing for exploration and exploitation.

5 EXPERIMENTS
5.1 Benchmarks
We evaluate KGpip as well as the other baselines on four benchmark
datasets: 1) Open AutoML Benchmark [11], a collection of 39 binary
and multi-class classification datasets (used by FLAML [30]). The
datasets are selected such that they are representative of the real
world from a diversity of problem domains and of enough difficulty
for the learning algorithms. 2) Penn Machine Learning Benchmark
(PMLB) [22]: Since Open AutoML Benchmark is limited to classifi-
cation datasets, the authors of FLAML [30] evaluated their system
on 14 more regression datasets selected from PMLB, such that the
number of samples is more than 10,000. To demonstrate the gener-
ality of our approach, we include those datasets in our evaluation
as well. 3) ALâ€™s datasets: We also evaluate on the datasets used for
ALâ€™s [2] evaluation which include 6 Kaggle datasets (2 regression
and 4 classification) and another 18 classification datasets (9 from
PMLB and 9 from OpenML). Unlike other benchmarks, the Kag-
gle datasets include datasets with textual features. 4) VolcanoMLâ€™s
datasets: finally, we evaluate KGpip on 44 more datasets used by
VolcanoML [17]. The authors of VolcanoML evaluate their system
on a total of 66 datasets from OpenML and Kaggle, from which

Table 1: Breakdown of all 121 datasets used in our
evaluation, indicating those used by FLAMLâˆ—, ALâ€ , and
VolcanoMLÂ§.

Source

Task

AutoML

PMLB

OpenML

Binary
Multi-class
Regression

22 (18âˆ—+1âˆ—â€ +3âˆ—Â§)
17 (15âˆ—+1âˆ—â€ +1âˆ—Â§)
0

5 (4â€ +1â€ Â§)
4â€ 
14âˆ—

27 (3â€ Â§+3â€ +21Â§)
7 (2â€ Â§+1â€  + 4Â§)
19Â§

Total

39

23

53

Kaggle
2â€ 
2â€ 
2â€ 

6

11 datasets are not specified, 10 datasets overlap with ours, and 1
dataset consists of image samples. Table 1 includes a summary of
all 121 benchmark datasets. The detailed statistics of all datasets
are shown in the appendix. These statistics include names, num-
ber of rows and columns, number of numerical, categorical, and
textual features, number of classes, sizes, sources, and papers that
evaluated on them.

5.2 Baselines
We empirically validate KGpip against three AutoML systems: (1)
Auto-Sklearn (v0.14.0) [9] which is the overall winner of multiple
challenges in the ChaLearn AutoML competition [12], and one of
the top 4 competitors reported in the Open AutoML Benchmark
[11]. (2) FLAML (v0.6.6) [30]: an AutoML library designed with
both accuracy and computational cost in mind. FLAML outperforms
Auto-Sklearn among other systems on two AutoML benchmarks us-
ing a low computational budget, (3) AL [2]: a meta-learning-based
AutoML approach that utilizes dynamic analysis of Kaggle note-
books, an approach that has similarities to ours, and (4) VolcanoML
(v0.5.0) [17], a recent AutoML approach which proposes efficient
decomposition strategies for the large AutoML search spaces. In all
our experiments, we used the latest code provided by the authors
for existing systems, the same exact hardware, time budget, and
the parameters recommended by the authors of these systems.

5.3 Training Setup
Because our approach to mining historical pipelines from scripts
is relatively cheap, we can apply it more easily on a wider variety
of datasets to form a better base as more and more scripts get
generated by domain experts on Kaggle competitions. In this work,
we performed program analysis on 11.7K scripts associated with
142 datasets, and then selected those with estimators from sklearn,
XGBoost and LightGBM since those were the estimators supported
by the most AutoML systems for classification and regression. This
resulted in the selection of 2,046 notebooks for 104 datasets; a vast
portion of the 11.7K programs were about exploratory data analysis,
or involved libraries that were not supported by Auto-Sklearn [9]
or FLAML (e.g., PyTorch and Keras) [30]. We used Macro F1 for
classification tasks to account for data imbalance, if any, and use
ğ‘…2 for regression tasks, as in FLAML [30]. We also varied the time
budget given to each system between 1 hour and 30 minutes, to
measure how fast can KGpip find an efficient pipeline compared to
other approaches. The time budget is end-to-end, from loading the
dataset till producing the best AutoML pipeline. In all experiments,
we report averages over 3 runs.

Table 2: Average scores (mean and standard deviation) of KG-
pip compared to FLAML, Auto-Sklearn, and VolcanoML for
binary classification (F1), multi-class classification (F1) and
regression (ğ‘…2) tasks on 77 benchmark datasets. T-test values
are for KGpip vs. FLAML and KGpip vs. Auto-Sklearn.

Binary

Multi-class Regression T-Test

FLAML
KGpipFLAML
Auto-Sklearn
KGpipAutoSklearn
VolcanoML

0.74 (0.23)
0.81 (0.14)
0.76 (0.20)
0.83 (0.14)
0.55 (0.43)

0.70 (0.29)
0.76 (0.24)
0.65 (0.29)
0.73 (0.28)
0.51 (0.38)

0.65 (0.29)
0.72 (0.24)
0.71 (0.24)
0.72 (0.24)
0.56 (0.32)

0.0129
-
0.0002
-
-

5.4 Comparison with Existing Systems
In this section, we evaluate KGpip against state-of-the-art approaches;
FLAML [30] and Auto-Sklearn [9]. Figure 6 shows a radar graph of
all systems when given a time budget of 1 hour. It shows the perfor-
mance of all systems on the three tasks in all benchmarks, namely,
binary classification, multi-class classification, and regression. For
every dataset, the figure shows the actual performance metric (F1
for classification and ğ‘…2 for regression) obtained from every system
1. Therefore, the out most curve from the center of the radar graph
has the best performance. In Figure 6, both variations of KGpip
achieve the best performance across all tasks, outperforming both
FLAML and Auto-Sklearn. We also performed a two-tailed t-Test
between the performance obtained by KGpip compared to the other
systems. The results show that KGpip achieves significantly bet-
ter performance than both FLAML and Auto-Sklearn with a t-Test
value of 0.01 and 0.0002, respectively (both have ğ‘ < 0.05).

Table 2 also shows the average F1 and ğ‘…2 values for classifica-
tion and regression tasks, respectively. The results show that both
variations of KGpip achieve better performance compared to both
FLAML and Auto-Sklearn over all tasks and datasets.

Scalability of KGpipâ€™s meta-learning against existing systems: The
AL meta-learning approach [2] mines pipelines using dynamic code
analysis, which has high cost as discussed in Section 3.1. Thus, the
authors of AL provided a pre-trained meta-learning model on 500
pipelines and 9 datasets, which does not scale to cover various
cases. In contrast, we trained our meta-learning model using 2000
pipelines and 142 datasets. None of these datasets were included
in the 77 datasets used in testing. AL failed in 22 and timed out in
38 datasets. This shows that the KGpip meta-learning approach,
which is based on pipelines semantics and dataset representation
learning, is more effective. AL failed on many of the datasets during
the fitting process. As the figure shows, KGpip still outperforms all
other approaches, including AL, significantly. On these datasets, AL
achieved the lowest F1 score on binary and multi-class classification
tasks with values of 0.36 and 0.36, respectively. This compares to
0.74 and 0.75 by FLAML, 0.73 and 0.68 by Auto-Sklearn, 0.79 and
0.79 by KGpipFLAML, and 0.79 and 0.74 by KGpipAuto-Sklearn.

VolcanoML Datasets: VolcanoML used a variety of datasets that
are not included in our 77 datasets of Figure 6. Some of these datasets
are quite large which are meant to test the the system scalability.
Therefore, we also collected all 49 the datasets we could find in
their paper and tested the best version of KGpip (KGpipFLAML)

1The detailed scores for every system and dataset as well as the corresponding names
of datasets are shown in tables 6-9 in the appendix.

6

Figure 6: A radar diagram of the performance of KGpip vs. existing systems on multiple tasks (77 datasets) with a time budget
of 1 hour for all systems. The outer numbers indicate different dataset IDs and the ticks inside the figure denote performance
ranges of respective metrics; e.g., 0.2, 0.4, ..., etc. for F1 in binary classification. For any dataset, the system with the out most
curve has the best performance. As an example, KGpipAutoSklearn and KGpipFLAML achieved 100% and 97% F1 on dataset
#23 (multi-class classification) compared to 65% and 26% for AutoSklearn and FLAML, respectively.

Table 4: Different aspects comparing a model trained on a
set of code graphs vs a model trained on a set of MetaPip
graphs. The model based on original code graphs fails in triv-
ial datasets to generate valid pipelines and limits KGpipâ€™s
scalability to a larger set of ML pipelines scripts and KGpipâ€™s
learning by using a fewer number of epochs.

Dataset/Aspect Code Graph MetaPip Graph

kr-vs-kp
nomao
cnae-9
mfeat-factors
segment

0 (0)
0 (0)
0 (0)
0 (0)
0 (0)

Avg. F1
No. Nodes
No. Edges
Training Time

0 (0)
29,139
252,486
175 (min)

1.00 (0)
0.96 (0)
0.95 (0.01)
0.98 (0)
0.98 (0)

0.97 (0.02)
974
1,052
2 (min)

5.5 Ablation Study
5.5.1 The effectiveness of MetaPip. Our MetaPip approach manages
to reduce dramatically the number of nodes and edges in the code
graph. Using the original graph obtained from static analysis, it
produces the MetaPip graph that focuses on the core aspects needed
to train a graph generation model for ML pipelines, such as data
transformations, learner selection, and hyper-parameter selection.
This experiment investigates the scalability of our graph generation
model based on two different training sets, i.e., the sets of MetaPip
graphs described in section 3.2 vs. the original set of code graphs
from static analysis for the same ML pipeline scripts.

For this experiment, we use a small-scale training set of 82
pipeline graphs pertaining to one classification dataset. The orig-
inal code graphs for these 82 pipelines include 29,139 nodes and
252,486 edges. Our MetaPip graph, however, includes 974 nodes
and 1052 edges. This is a graph reduction rate of at least 96.6%,

Figure 7: Score difference between KGpipFLAML and Vol-
canoML on the 44 classification and regression datasets from
VolcanoML with a time budget of 1 hour. For brevity, we re-
moved from this Figure the 22 datasets on which both sys-
tems perform comparably (within a difference of â‰¤ 0.01).

Table 3: Average scores (mean and standard deviation) of
KGpipFLAML compared to VolcanoML on the 44 datasets
from VolcanoML. Overall, KGpipFLAML achieves signifi-
cantly better compared to VolcanoML, according to a statis-
tical significance test of ğ‘ < 0.05 .

Binary

Multi-class Regression T-Test

KGpipFLAML
VolcanoML

0.82 (0.14)
0.69 (0.23)

0.86 (0.16)
0.70 (0.31)

0.83 (0.13)
0.68 (0.25)

-
0.0001

against VolcanoML on these datasets with a time budget of 1 hour.
The performances of KGpipFLAML and VolcanoML are shown in
Figure 7. For brevity, we omitted from the figure all datasets on
which the performance difference between both systems is â‰¤ 0.01
and the datasets overlapping with the ones shown in Figure 6. On
those datasets, KGpipFLAML found a valid pipeline for all of them,
sometimes with a decent absolute difference in F1 or ğ‘…2 scores of
â‰¥ 0.90. Across all the 44 datasets, KGpipFLAML achieved signifi-
cantly better average of scores compared to VolcanoML (statistical
significance test of ğ‘ < 0.05), see Table 3 for details.

7

91968894861028990999287Classification Datasets0.050.000.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.800.850.900.95F1-Score Absolute Difference108115120103117107121118113114109Regression Datasets0.000.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.800.850.900.95RÂ² Absolute DifferenceFigure 8: Top learner and transformers selected by KGpip (A) are with a wide range of coverage and diversity (B).

(a)

(b)

Table 5: Performance of KGpipFLAML (mean and standard
deviation) as we vary the number of predicted pipeline
graphs within 30 minutes time limit. We obtained similar
results for KGpipAutoSklearn, and hence omitted its results.

Binary

Multi-Class Regression

Top-3 graphs
Top-5 graphs
Top-7 graphs

0.80 (0.14)
0.81 (0.14)
0.81 (0.14)

0.70 (0.31)
0.73 (0.26)
0.75 (0.24)

0.71 (0.23)
0.70 (0.23)
0.71 (0.24)

Figure 4 shows these detailed statistics. The main investigation
here is whether this huge reduction ratio will help improving the
accuracy and scalability of our graph generation model. We train
one model on the original code graph and another on the MetaPip
graphs. Both models are trained for 15 epochs with the same set
of hyperparameters. It is worth noting that due to the huge time
required to process the nodes and edges in the code graph, we had
to reduce the number of epochs from 400 to 15.

We test the performance of KGpip when trained on both graphs
on the most trivial binary and multi-class classification datasets in
the AutoML benchmark. These are the datasets where the F1 score
of all the reported systems in section 5.4 is above 0.9. The result is
a total of 5 datasets (1 binary and 4 multi-class). Both models use
Auto-Sklearn as the hyperparameter optimizer with a time budget
of 15 minutes and 3 graphs. We take the average of three runs.
The results are summarized in Table 4. For these trivial datasets,
the model trained using code graphs did not manage to generate
any valid ML pipeline. This means the model failed to capture the
core aspects of ML pipelines, i.e., valid transformation or learners.
Moreover, our MetaPip approach helps KGpip to reduce the training
time by 99%, as shown in Table 4.

5.5.2 The KGpip meta-learning quality. This experiment tests the
quality of our meta-learning component. We test the performance
as we vary the number of graphs selected from the graph genera-
tion phase before feeding it to the hyper-parameter optimization
module. Table 5 shows the KGpipFLAML performance as we vary
the number of predicted graphs between 3, 5 and 7.

With only 3 graphs, KGpip is still outperforming FLAML (second
best system after KGpipFLAML) , although the effect is weaker (t-
Test value = 0.06). Compared to Auto-sklearn (third best system after
KGpipFLAML), all variations have similar or better performance,
but the difference is insignificant. This experiment shows that even

8

with three graphs, KGpip outperforms FLAML and KGpipAuto-
Sklearn, i.e., the correct pipelines often appear in the top 3. As
another assessment of the quality of our predictions, we measure
where in our ranked list of predicted pipelines the best pipeline
turned out to be. Ideally, the top pipeline would always be first, and
we use Mean Reciprocal Rank (MRR) to measure how close to that
our predictions are. Across all runs, the MRR is 0.71, indicating that
the top pipeline is typically very near the top.

5.5.3 The KGpip meta-learning diversity. One question we addressed
is whether KGpip produced different pipelines for the same dataset
across different runs. This gives us a sense of whether KGpip is
deterministic, or whether it produces different pipelines to help
with pruning the AutoML search space. We took different runs for
the exact same dataset, and created a list of learners and transform-
ers produced for each dataset across runs. The list was limited by
the shortest number of learners and transformers produced across
runs. We then computed correlations for datasets across runs 1, 2,
and 3. The correlations ranged from 0.60 - 0.64, suggesting that the
runs did not produce the same transformers and learners across
runs. We also examined the types of learners selected by KGpip for
consideration. Figure 8a shows the learners and transformers found
at least 20 times in the training pipelines. One can see from the
figure that KGpip does not blindly output learners and transformers
by counts. Figure 8b shows more diversity in what was selected
overall. So, a variety of methods are covered by KGpip.

6 CONCLUSION
This paper proposed a novel formulation for the AutoML problem
as a graph generation problem, where we can pose learner and
pre-processing selection as a generation of different graphs rep-
resenting ML pipelines. Hence, we developed the KGpip system
based on mining large repositories of scripts, and leveraging recent
techniques for static code analysis. KGpip utilized embeddings gen-
erated based on dataset contents to predict and optimize a set of ML
pipelines based on the most similar seen datasets. KGpip is designed
to work with AutoML systems, such as Auto-Sklearn and FLAML,
to utilize their hyperparameter optimizers. We conducted the most
comprehensive evaluation of 121 datasets, including the datasets
used by FLAML, VolcanoML, and AL. Our comprehensive evalu-
ation shows that KGpip significantly improves the performance
of FLAML and Auto-Sklearn in classification and regression tasks.
Moreover, KGpip outperformed AL, which is based on a more costly

0255075100125150175200225250275300325350375400425Pipeline CounttrainLinearRegressionRandomForestClassifierLogisticRegressionRandomForestRegressorDecisionTreeClassifierKNeighborsClassifierXGBClassifierGridSearchCVKMeansSVCDecisionTreeRegressorXGBRegressorPipelineGaussianNBMultinomialNBGradientBoostingClassifierSVRGradientBoostingRegressorRidgeAdaBoostClassifierMLPClassifierLassoGaussianMixtureRandomizedSearchCVAffinityPropagationLearner / TransformerSklearn CallXGBoost Callrandom_forest22%extra_tree17%sgd11%gradient_boosting11%xgboost10%k_nearest_neighbors6%decision_tree6%adaboost5%pca3%libsvm_svc2%libsvm_svr2%Other5%[19] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory
Bramble, Horst Samulowitz, Dakuo Wang, Andrew Conn, and Alexander Gray.
2020. An ADMM Based Framework for AutoML Pipeline Configuration. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 4892â€“4899.
https://doi.org/10.1609/aaai.v34i04.5926

[20] Radu Marinescu, Akihiro Kishimoto, Parikshit Ram, Ambrish Rawat, Martin
Wistuba, Paulito P. Palmes, and Adi Botea. 2021. Searching for Machine Learning
Pipelines Using a Context-Free Grammar. In Proceedings of the AAAI Conference
on Artificial Intelligence. 8902â€“8911. https://ojs.aaai.org/index.php/AAAI/article/
view/17077

[21] Jonas Mueller and Alex Smola. 2019. Recognizing Variables from Their Data
via Deep Embeddings of Distributions. International Conference on Data Mining
(ICDM) (2019), 1264â€“1269. https://doi.org/10.1109/ICDM.2019.00158

[22] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz,
and Jason H. Moore. 2017. PMLB: a large benchmark suite for machine learning
evaluation and comparison. BioData Mining 10, 1 (2017), 36. https://doi.org/10.
1186/s13040-017-0154-

[23] Bernhard Pfahringer, Hilan Bensusan, and Christophe Giraud-Carrier. 2000. Meta-
Learning by Landmarking Various Learning Algorithms. In Proceedings of the
International Conference on Machine Learning (ICML). 743â€“750. https://dl.acm.
org/doi/10.5555/645529.658105

[24] Herilalaina Rakotoarison, Marc Schoenauer, and MichÃ¨le Sebag. 2019. Au-
tomated Machine Learning with Monte-Carlo Tree Search. In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI). 3296â€“3303.
https://doi.org/10.24963/ijcai.2019/457

[25] Matthias Reif, Faisal Shafait, and Andreas Dengel. 2012. Meta-learning for
evolutionary parameter optimization of classifiers. Machine Learning 87, 3 (2012),
357â€“380. https://doi.org/10.1007/s10994-012-5286-7

[26] Kevin Swersky, Jasper Snoek, and Ryan P. Adams. 2013. Multi-Task Bayesian
Optimization. In Proceedings of the International Conference on Neural Information
Processing Systems (NeurIPS). 2004â€“2012. https://dl.acm.org/doi/10.5555/2999792.
2999836

[27] Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2013.
Auto-WEKA: Combined election and hyperparameter optimization of classifi-
cation algorithms. In Proceedings of the International Conference on Knowledge
Discovery and Data Mining (SIGKDD). 847â€“855. https://doi.org/10.1145/2487575.
2487629

[28] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2014. OpenML:
Networked Science in Machine Learning. SIGKDD Explorations 15 (2014), 49â€“60.
https://doi.org/10.1145/2641190.2641198

[29] Ricardo Vilalta, Christophe Giraud-carrier, Pavel Brazdil, and Carlos Soares. 2004.
Using Meta-Learning to Support Data Mining. International Journal of Computer
Science & Applications 1 (2004). https://citeseerx.ist.psu.edu/viewdoc/download?
doi=10.1.1.105.1351&rep=rep1&type=pdf

[30] Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. 2021. FLAML: A
Fast and Lightweight AutoML Library. In Proceedings of Machine Learning and
Systems (MLSys), Vol. 3. 434â€“447. https://proceedings.mlsys.org/paper/2021/file/
92cc227532d17e56e07902b254dfad10-Paper.pdf

[31] Marcel Wever, Felix Mohr, and Eyke HÃ¼llermeier. 2018. ML-Plan for Unlimited-
Length Machine Learning Pipelines. In AutoML Workshop at the International
Conference on Machine Learning (ICML). https://ris.uni-paderborn.de/download/
3852/3853/38.pdf

[32] Anatoly Yakovlev, Hesam Fathi Moghadam, Ali Moharrer, Jingxiao Cai, Nikan
Chavoshi, Venkatanathan Varadarajan, Sandeep R. Agrawal, Sam Idicula, Tomas
Karnagel, Sanjay Jinturkar, and Nipun Agarwal. 2020. Oracle AutoML: A Fast and
Predictive AutoML Pipeline. Proceedings of the VLDB Endowment 13, 12 (2020),
3166â€“3180. https://doi.org/10.14778/3415478.3415542

[33] Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. 2019. OBOE:
Collaborative Filtering for AutoML Model Selection. In Proceedings of the In-
ternational Conference on Knowledge Discovery and Data Mining (SIGKDD).
http://doi.org/10.1145/3292500.3330909

meta-learning process, in 97% of the datasets. This outstanding per-
formance shows that the KGpip meta-learning approach is more
effective and efficient. Finally, KGpip outperforms VolcanoML in
62% of the datasets and ties with it in 22%.

REFERENCES
[1] Ibrahim Abdelaziz, Julian Dolby, James P. McCusker, and Kavitha Srinivas. 2020.
A Toolkit for Generating Code Knowledge Graphs. ArXiv (2020). https://arxiv.
org/abs/2002.09440

[2] JosÃ© P. Cambronero and Martin C. Rinard. 2019. AL: Autogenerating Supervised
Learning Programs. In Proceedings of the ACM on Programming Languages, Vol. 3.
https://doi.org/10.1145/3360601

[3] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.
John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-
Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder.
CoRR abs/1803.11175 (2018). http://arxiv.org/abs/1803.11175

[4] Alex G. C. de SÃ¡, Walter JosÃ© G. S. Pinto, Luiz Otavio V. B. Oliveira, and Gisele L.
Pappa. 2017. RECIPE: A Grammar-Based Framework for Automatically Evolv-
ing Classification Pipelines. In Genetic Programming, James McDermott, Mauro
Castelli, Lukas Sekanina, Evert Haasdijk, and Pablo GarcÃ­a-SÃ¡nchez (Eds.). 246â€“
261. https://doi.org/10.1007/978-3-319-55696-3_16

[5] Iddo Drori, Lu Liu, Yi Nian, Sharath C Koorathota, Jung-Shian Li, Antonio Khalil
Moretti, Juliana Freire, and Madeleine Udell. 2019. AutoML using Metadata
Language Embeddings. ArXiv (2019). https://arxiv.org/abs/1910.03698

[6] Robert Engels and Christiane Theusinger. 1998. Using a Data Metric for Prepro-
cessing Advice for Data Mining Applications. In In Proceedings of the European
Conference on Artificial Intelligence (ECAI). 430â€“434. http://citeseerx.ist.psu.edu/
viewdoc/download?doi=10.1.1.56.7414&rep=rep1&type=pdf

[7] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy,
Mu Li, and Alexander Smola. 2020. AutoGluon-Tabular: Robust and Accurate
AutoML for Structured Data. ArXiv (2020). https://arxiv.org/abs/2003.06505
[8] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and
Frank Hutter. 2020. Auto-Sklearn 2.0: The Next Generation. arXiv (2020). https:
//arxiv.org/abs/2007.04074

[9] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
Blum, and Frank Hutter. 2015. Efficient and Robust Automated Machine Learning.
In Proceedings of the International Conference on Neural Information Processing
Systems (NeurIPS). 2962â€“2970. https://dl.acm.org/doi/10.5555/2969442.2969547
[10] Nicolo Fusi, Rishit Sheth, and Melih Elibol. 2018. Probabilistic Matrix Fac-
torization for Automated Machine Learning. In Proceedings of the Interna-
tional Conference on Neural Information Processing Systems (NeurIPS). 3352â€“3361.
https://dl.acm.org/doi/10.5555/3327144.3327254

[11] P. Gijsbers, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. Vanschoren. 2019.
An Open Source AutoML Benchmark. In AutoML Workshop at the International
Conference on Machine Learning (ICML). https://arxiv.org/abs/1907.00909
[12] Isabelle Guyon, Imad Chaabane, Hugo Jair Escalante, Sergio Escalera, Damir
Jajetic, James Robert Lloyd, NÃºria MaciÃ , Bisakha Ray, Lukasz Romaszko, MichÃ¨le
Sebag, Alexander Statnikov, SÃ©bastien Treguer, and Evelyne Viegas. 2016. A brief
Review of the ChaLearn AutoML Challenge: Any-time Any-dataset Learning
without Human Intervention. In Proceedings of Machine Learning Research, Vol. 64.
21â€“30. https://proceedings.mlr.press/v64/guyon_review_2016.html

[13] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2021. Billion-Scale Similarity
Search with GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535â€“547. https:
//doi.org/10.1109/TBDATA.2019.2921572

[14] Michael Katz, Parikshit Ram, Shirin Sohrabi, and Octavian Udrea. 2020. Explor-
ing Context-Free Languages via Planning: The Case for Automating Machine
Learning. In Proceedings of the International Conference on Automated Planning
and Scheduling (ICAPS). 403â€“411. https://ojs.aaai.org//index.php/ICAPS/article/
view/6686

[15] Trang T Le, Weixuan Fu, and Jason H Moore. 2020. Scaling tree-based automated
machine learning to biomedical big data with a feature set selector. Bioinformatics
36, 1 (2020), 250â€“256. https://doi.org/10.1093/bioinformatics/btz470

[16] Erin LeDell and Sebastien Poirier. 2020. H2O AutoML: Scalable Automatic Ma-
chine Learning. In AutoML Workshop at the International Conference on Machine
Learning (ICML). www.automl.org/wp-content/uploads/2020/07/AutoML_2020_
paper_61.pdf

[17] Yang Li, Yu Shen, Wentao Zhang, Jiawei Jiang, Yaliang Li, Bolin Ding, Jingren
Zhou, Zhi Yang, Wentao Wu, Ce Zhang, and Bin Cui. 2021. VolcanoML: Speeding
up End-to-End AutoML via Scalable Search Space Decomposition. Proceedings
of the VLDB Endowment 14, 11 (2021), 2167â€“2176. http://www.vldb.org/pvldb/
vol14/p2167-li.pdf

[18] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. 2018.
Learning Deep Generative Models of Graphs. ArXiv (2018). https://arxiv.org/
abs/1803.03324

9

A STATISTICS OF ALL DATASETS
Tables 6 and 7 show the statistics of all benchmark datasets includ-
ing number of rows and columns, number of numerical, categorical,
and textual features, number of classes, size, source, and papers
that evaluated on these datasets.

B DETAILED SCORES FOR ALL SYSTEMS
Tables 8 and 9 show the detailed macro F1 and ğ‘…2 scores for all
systems on all benchmark datasets.

10

Table 6: Statistics of the 77 benchmark datasets used in FLAML and AL. From left to right: dataset name, number of rows,
number of columns, number of numerical columns, number of categorical columns, number of textual columns, number of
classes (for classification datasets), size in MB, source of the dataset, and papers that evaluated on the dataset.

ID

Dataset

#Rows

#Cols

#Num #Cat

#Text

#Classes

Size (MB)

Task

Source

Paper

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77

adult
airlines
albert
Amazon_employee_access
APSFailure
Australian
bank-marketing
blood-transfusion-service-center
christine
credit-g
guillermo
higgs
jasmine
kc1
KDDCup09_appetency
kr-vs-kp
MiniBooNE
nomao
numerai28.6
phoneme
riccardo
sylvine
car
cnae-9
connect-4
covertype
dilbert
dionis
fabert
Fashion-MNIST
helena
jannis
jungle_chess_2pcs_raw_endgame_complete
mfeat-factors
robert
segment
shuttle
vehicle
volkert
2dplanes
bng_breastTumor
bng_echomonths
bng_lowbwt
bng_pbc
bng_pharynx
bng_pwLinear
fried
house_16H
house_8L
houses
mv
poker
pol
breast_cancer_wisconsin
detecting-insults-in-social-commentary
fri_c1_1000_25
Hill_Valley_with_noise
Hill_Valley_without_noise
ionosphere
MagicTelescope
OVA_Breast
pc4
quake
sick
spambase
titanic
car_evaluation
glass
kropt
mnist_784
sentiment-analysis-on-movie-reviews
splice
spooky-author-identification
wine_quality_red
wine_quality_white
housing-prices
mercedes-benz-greener-manufacturing

48842
539383
425240
32769
76000
690
45211
748
5418
1000
20000
98050
2984
2109
50000
3196
130064
34465
96320
5404
20000
5124
1728
1080
67557
581012
10000
416188
8237
70000
65196
83733
44819
2000
10000
2310
58000
846
58310
40768
116640
17496
31104
1000000
1000000
177147
40768
22784
22784
20640
40768
1025010
15000
569
3947
1000
1212
1212
351
19020
1545
1458
2178
3772
4601
891
1728
205
28056
70000
156060
3190
19579
1599
4898
1460
4209

14
7
78
9
170
14
16
4
1636
20
4296
28
144
21
230
36
50
118
21
5
4296
20
6
856
42
54
2000
60
800
784
27
54
6
216
7200
19
9
18
180
10
9
9
9
18
10
10
10
16
8
8
11
10
48
30
2
25
100
100
34
11
10936
37
3
29
57
11
21
9
6
784
3
61
2
11
11
80
377

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
4
9
3
7
5
355
7
10
100
4
3
10
10
7
7
4
10
-
-
-
-
-
-
-
-
-
-
-
-
-
-
2
2
2
2
2
2
2
2
2
2
2
2
2
4
5
18
10
5
3
3
6
7
-
-

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0
0
0

8
3
0
0
0
0
9
0
0
13
0
0
0
0
38
36
0
0
0
0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
22
0
4
0
0
3
0
0
61
1
0
0
43
8

6
4
78
9
170
14
7
4
1636
7
4296
28
144
21
192
0
50
118
21
5
4296
20
0
856
42
54
2000
60
800
784
27
54
6
216
7200
19
9
18
180
10
9
9
9
18
10
10
10
16
8
8
11
10
48
30
0
25
100
100
34
11
10936
37
3
7
57
6
21
9
3
784
2
0
0
11
11
37
369

11

5.7
18.3
155.4
1.9
74.8
0.0
3.5
0.0
31.4
0.1
424.5
43.3
1.7
0.1
32.8
0.5
69.4
19.3
34.9
0.3
414.0
0.4
0.1
1.8
5.5
71.7
176.0
110.1
13.0
148.0
14.6
36.7
0.6
1.4
268.1
0.3
1.5
0.1
65.1
2.4
6.0
2.3
2.4
220.8
68.6
10.6
8.1
5.8
2.8
1.8
5.9
23.0
3.0
0.1
0.8
0.2
0.8
1.5
0.1
1.5
103.3
0.2
0.0
0.3
1.1
0.1
0.1
0.0
0.5
122.0
8.1
0.4
3.1
0.1
0.3
0.4
3.1

binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
regression
regression

AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
AutoML
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
PMLB
Kaggle
OpenML
PMLB
PMLB
PMLB
OpenML
OpenML
OpenML
OpenML
OpenML
PMLB
Kaggle
PMLB
PMLB
OpenML
OpenML
Kaggle
OpenML
Kaggle
PMLB
PMLB
Kaggle
Kaggle

FLAML, AL
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, AL
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
AL
AL
AL
AL
AL
AL
AL
AL
AL, VolcanoML
AL, VolcanoML
AL, VolcanoML
AL, VolcanoML
AL
AL
AL
AL, VolcanoML
AL, VolcanoML
AL
AL
AL
AL
AL
AL
AL

Table 7: Statistics of the 44 datasets used by VolcanoML. From left to right: dataset name, number of rows, number of columns,
number of numerical columns, number of categorical columns, number of textual columns, number of classes (for classifica-
tion datasets), size in MB, source of the dataset, and papers that evaluated on the dataset.

ID

Dataset

#Rows

#Cols

#Num #Cat

#Text

#Classes

Size (MB)

Task

Source

Paper

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

2.2
0.1
2.1
0.7
0.4
0.3
0.3
1.6
2.9
0.8
0.6
0.8
1.0
0.9
0.2
0.1
2.3
0.6
0.2
1.0
0.4
0.2
0.8
0.7
2.1
2.4
0.6
1.0
0.6
0.2
1.1
0.1
0.2
2.7
0.7
0.4
0.1
0.5
0.1
0.6
1.1
0.1
0.5
0.0

binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression

OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML
OpenML

VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML
VolcanoML

ailerons
analcatdata_supreme
bank32nh_833
cpu_act_761
cpu_small_735
delta_ailerons
delta_elevators
eeg-eye-state
electricity
jm1
kin8nm_807
mammography
mc1
ozone-level-8hr
page-blocks
pollen_871
puma32H_752
puma8NH_816
space_ga_737
waveform-5000
wind_847
abalone
optdigits
pendigits
satimage
bank32nh_558
bank8FM
cpu_act_573
cpu_small_227
debutanizer
kin8nm_189

78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109 Moneyball
pollen_529
110
puma32H_308
111
puma8NH_225
112
rainfall_bangladesh
113
socmob
114
space_ga_507
115
stock
116
sulfur
117
us_crime
118
weather_izmir
119
wind_503
120
witmer_census_1980
121

13750
4052
8192
8192
8192
7129
9517
14980
45312
10885
8192
11183
9466
2534
5473
3848
8192
8192
3107
5000
6574
4177
5620
10992
6430
8192
8192
8192
8192
2394
8192
1232
3848
8192
8192
16755
1156
3107
950
10081
1994
1461
6574
50

40
7
32
21
12
5
6
14
8
21
8
6
38
72
10
5
32
8
6
40
14
8
64
16
36
32
8
21
12
7
8
14
5
32
8
3
5
6
9
6
127
9
14
5

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
28
10
10
6
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

40
7
32
21
12
5
6
14
8
21
8
6
38
72
10
5
32
8
6
40
14
7
64
16
36
32
8
21
12
7
8
12
5
32
8
1
1
6
9
6
126
9
14
4

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
2
0
0
0
2
4
0
0
0
1
0
0
1

12

Table 8: Macro F1 and ğ‘…2 scores for all systems on all 77 benchmark datasets. The reported scores are averages of 3 runs with
1 hour time budget.

ID

Dataset

FLAML

KGpip
FLAML

AutoSklearn

KGpip
AutoSklearn

VolcanoML

Task

Papers

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77

adult
airlines
albert
Amazon_employee_access
APSFailure
Australian
bank-marketing
blood-transfusion-service-center
christine
credit-g
guillermo
higgs
jasmine
kc1
KDDCup09_appetency
kr-vs-kp
MiniBooNE
nomao
numerai28.6
phoneme
riccardo
sylvine
car
cnae-9
connect-4
covertype
dilbert
dionis
fabert
Fashion-MNIST
helena
jannis
jungle_chess_2pcs_raw_endgame_complete
mfeat-factors
robert
segment
shuttle
vehicle
volkert
2dplanes
bng_breastTumor
bng_echomonths
bng_lowbwt
bng_pbc
bng_pharynx
bng_pwLinear
fried
house_16H
house_8L
houses
mv
poker
pol
breast_cancer_wisconsin
car_evaluation
detecting-insults-in-social-commentary
fri_c1_1000_25
glass
Hill_Valley_with_noise
Hill_Valley_without_noise
ionosphere
kropt
MagicTelescope
mnist_784
OVA_Breast
pc4
quake
sentiment-analysis-on-movie-reviews
spambase
sick
splice
spooky-author-identification
titanic
wine_quality_red
wine_quality_white
housing-prices
mercedes-benz-greener-manufacturing

0.81
0.66
0.66
0.74
0.72
0.86
0.76
0.62
0.73
0.72
0.82
0.00
0.80
0.66
0.52
0.99
0.94
0.97
0.52
0.90
1.00
0.95
0.26
0.96
0.74
0.94
0.99
0.88
0.70
0.91
0.23
0.56
0.83
0.97
0.35
0.98
0.99
0.78
0.66
0.95
0.18
0.47
0.62
0.46
0.51
0.62
0.96
0.70
0.71
0.86
0.00
0.92
0.99
0.98
0.99
0.58
0.88
0.58
0.88
0.73
0.94
0.90
0.00
0.98
0.93
0.76
0.51
0.45
0.96
0.62
0.95
0.00
0.80
0.33
0.40
0.90
0.59

0.81
0.66
0.69
0.74
0.92
0.87
0.75
0.67
0.74
0.70
0.82
0.73
0.81
0.69
0.53
1.00
0.94
0.96
0.52
0.91
0.99
0.94
0.97
0.94
0.73
0.94
0.98
0.90
0.71
0.90
0.23
0.57
0.80
0.98
0.40
0.98
0.99
0.79
0.67
0.95
0.19
0.45
0.62
0.45
0.52
0.62
0.95
0.69
0.71
0.86
1.00
0.87
0.99
0.99
1.00
0.76
0.92
0.46
0.65
0.73
0.93
0.90
1.00
0.98
0.96
0.74
0.53
0.49
0.96
0.93
0.95
0.72
0.80
0.35
0.40
0.92
0.65

0.82
0.66
0.69
0.76
0.92
0.85
0.79
0.65
0.74
0.78
0.71
0.73
0.81
0.72
0.57
1.00
0.94
0.96
0.52
0.91
0.99
0.94
1.00
0.95
0.73
0.85
0.98
0.00
0.69
0.86
0.18
0.60
0.87
0.99
0.45
0.99
0.99
0.81
0.64
0.95
0.19
0.46
0.62
0.41
0.52
0.62
0.96
0.71
0.72
0.86
1.00
0.90
0.99
0.99
1.00
0.82
0.93
0.67
1.00
1.00
0.94
0.87
1.00
0.95
0.96
0.83
0.54
0.43
0.97
0.93
0.97
0.72
0.84
0.34
0.41
0.89
0.65

13

0.54
0.66
0.33
0.73
0.88
0.85
0.78
0.64
0.75
0.74
0.83
0.32
0.81
0.70
0.57
0.99
0.94
0.96
0.52
0.89
0.99
0.63
0.65
0.93
0.72
0.30
0.99
0.00
0.70
0.60
0.24
0.60
0.87
0.98
0.49
0.98
0.96
0.82
0.68
0.95
0.18
0.46
0.61
0.45
0.51
0.62
0.96
0.70
0.72
0.85
1.00
0.93
0.99
0.99
0.66
0.43
0.60
0.60
1.00
1.00
0.94
0.85
1.00
0.98
0.97
0.74
0.49
0.43
0.97
0.89
0.96
0.19
0.55
0.30
0.36
0.86
0.59

0.00
0.00
0.00
0.73
0.00
0.86
0.00
0.60
0.75
0.00
0.55
0.00
0.80
0.67
0.00
0.00
0.93
0.96
0.52
0.88
0.99
0.95
0.00
0.94
0.71
0.92
0.98
0.00
0.67
0.88
0.20
0.57
0.90
0.99
0.00
0.98
0.96
0.81
0.61
0.95
0.18
0.45
0.61
0.30
0.34
0.62
0.96
0.69
0.70
0.84
1.00
0.60
0.66
0.98
0.99
0.00
0.92
0.61
1.00
1.00
0.94
0.00
1.00
0.98
0.00
0.72
0.52
0.00
0.63
0.32
0.00
0.00
0.00
0.39
0.39
0.00
0.00

binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
multi-class
regression
regression

FLAML, AL
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, AL
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML, VolcanoML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
FLAML
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL, VolcanoML
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL
AL

Table 9: Macro F1 and ğ‘…2 scores for KGpipFLAML and VolcanoML on the 44 datasets used by VolcanoML. The reported scores
are averages of 3 runs with 1 hour time budget.

ID

Dataset

ailerons
analcatdata_supreme
bank32nh_833
cpu_act_761
cpu_small_735
delta_ailerons
delta_elevators
eeg-eye-state
electricity
jm1
kin8nm_807
mammography
mc1
ozone-level-8hr
page-blocks
pollen_871
puma32H_752
puma8NH_816
space_ga_737
waveform-5000
wind_847
abalone
optdigits
pendigits
satimage
bank32nh_558
bank8FM
cpu_act_573
cpu_small_227
debutanizer
kin8nm_189

78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109 Moneyball
pollen_529
110
puma32H_308
111
puma8NH_225
112
rainfall_bangladesh
113
socmob
114
space_ga_507
115
stock
116
sulfur
117
us_crime
118
weather_izmir
119
wind_503
120
witmer_census_1980
121

KGpip
FLAML

VolcanoML

Task

0.88
0.98
0.79
0.92
0.90
0.93
0.88
0.97
0.92
0.00
0.90
0.82
0.76
0.74
0.62
0.51
0.91
0.83
0.87
0.87
0.86
0.00
0.99
0.99
0.90
0.56
0.96
0.98
0.98
0.67
0.90
0.00
0.81
0.95
0.67
0.00
0.00
0.65
0.99
0.84
0.00
0.99
0.79
0.00

binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
binary
multi-class
multi-class
multi-class
multi-class
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression
regression

0.89
0.99
0.79
0.93
0.91
0.94
0.88
0.96
0.94
0.64
0.87
0.85
0.80
0.68
0.94
0.51
0.89
0.83
0.83
0.88
0.86
0.12
0.99
0.99
0.92
0.60
0.96
0.98
0.98
0.83
0.81
0.94
0.80
0.94
0.68
0.76
0.91
0.67
0.99
0.89
0.68
0.99
0.81
0.50

14

