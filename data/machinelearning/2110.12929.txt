Convergence Rates of Average-Reward Multi-agent
Reinforcement Learning via Randomized Linear Programming

1
2
0
2

t
c
O
2
2

]

C
O
.
h
t
a
m

[

1
v
9
2
9
2
1
.
0
1
1
2
:
v
i
X
r
a

Alec Koppel∗†

Amrit Singh Bedi∗$

Bhargav Ganguly‡

Vaneet Aggarwal‡

†Amazon

$CISD, US Army Research Lab.

‡Purdue University

Abstract

In tabular multi-agent reinforcement learning
with average-cost criterion, a team of agents
sequentially interacts with the environment
and observes local incentives. We focus on
the case that the global reward is a sum of
local rewards, the joint policy factorizes into
agents’ marginals, and full state observabil-
ity. To date, few global optimality guaran-
tees exist even for this simple setting, as most
results yield convergence to stationarity for
parameterized policies in large/possibly con-
tinuous spaces. To solidify the foundations
of MARL, we build upon linear program-
ming (LP) reformulations, for which stochas-
tic primal-dual methods yields a model-free
approach to achieve optimal sample complex-
ity in the centralized case. We develop multi-
agent extensions, whereby agents solve their
local saddle point problems and then per-
form local weighted averaging. We establish
that the sample complexity to obtain near-
globally optimal solutions matches tight de-
pendencies on the cardinality of the state and
action spaces, and exhibits classical scalings
with respect to the network in accordance
with multi-agent optimization. Experiments
corroborate these results in practice.

1

Introduction

In multi-agent reinforcement learning (MARL), a col-
lection of agents repeatedly interact with their en-
vironment and are exposed to localized incentives.
This framework has gained traction in recent years
through successful application to autonomous vehicu-

∗denotes equal contributions.
†Work completed while at the U.S. Army Research Labo-
ratory in Adelphi, MD 20783

lar networks (Wang et al., 2018), games (Vinyals et al.,
2019), and various settings in econometrics (Lussange
et al., 2021; Tesauro & Kephart, 2002). At the core of
MARL is a Markov Decision Process (MDP) (Puter-
man, 2014), which determines the interplay between
agents, states, actions, and rewards. We focus on
the standard objective, whereby the goal of the net-
work agents is to discern policies so as to maximize
the long-term accumulation of instantaneous rewards,
which may be written as a node-separable sum of all
localized rewards (Nedic & Ozdaglar, 2009).

Deﬁning the team reward in this way implies that
agents seek to cooperate towards a common goal,
which may be contrasted with competitive or mixed
settings (Ba¸sar & Olsder, 1998). Due to the surge
of interest in MARL, disparate possible technical set-
tings have been considered, which span how one de-
ﬁnes MDP transition dynamics; the observability of
agents trajectories, the availability of computational
resources at a centralized location, and the protocol
by which agents exchange information. We consider
the case that agents have global knowledge of the
state and action (in contrast to partial observability
(Krishnamurthy, 2016; Mahajan & Mannan, 2016),
which necessitates pooling information as in central-
ized training decentralized execution (CTDE) (Foer-
ster et al., 2016, 2017; Leibo et al., 2017; Rashid et al.,
2018)). Further, we hypothesize that the team’s joint
policy factorizes into the product of marginals, which
is referred to as joint action learners (JAL) (Claus &
Boutilier, 1998; Lee et al., 2020).

Our focus is on decentralized training of JAL, which
means agents’ rewards and policy parameters are lo-
cally held and private. Numerous recent works on
MARL operate in this setting, as in multi-agent exten-
sions of temporal diﬀerence (TD) learning (Doan et al.,
2019; Lee et al., 2018), Q-learning (Kar et al., 2013),
value iteration (Qu et al., 2019; Wai et al., 2018), and
actor-critic (Lowe et al., 2017; Zhang et al., 2018). In
these works, agents may communicate according to the
connectivity of a possibly time-varying graph, which is
intimately connected to multi-agent optimization.

 
 
 
 
 
 
Convergence Rates of Average-Reward MARL

References

Rewards

Setting

(Raveh & Meir, 2019)

Discounted

Centralized

(Wang, 2020b), (Xu et al., 2020)

Average

Centralized, Parallel

Ω

(Raveh & Meir, 2019)

(Qu et al., 2020b)

This work

Average

Average

Average

Parallel

Decentralized

Decentralized

(cid:16)

Ω

(cid:16)

(cid:17)

(cid:101)O
τ 2t2

Sample Complexity
(cid:16) n|S||A|
(1−γ)2
E0|S||A|
(cid:15)2
(cid:16) n|S||A|
(1−γ)2
—-

log 1
δ
(cid:17)

mix

(cid:101)O

√

(cid:17)

τ 2t2

mix

nE0|S||A|D(Γ,ρ)
(cid:15)2

log 1
δ

(cid:17)

Table 1: MARL for average rewards case. The proposed scheme is the ﬁrst decentralized algorithm in the
average reward settings with PAC sample complexity. Here, (cid:15) is the accuracy parameter, δ denotes the high
(cid:3) where Γ=(1 − η/4n2)−2, ρ=(1 − η/(4n2))1/B,
probability parameter, n is the number of agents, D(Γ, ρ) := (cid:2) 1+Γ
B is the network strong connectivity parameter, and η is a lower-bound on the entries of the mixing matrix.

1−ρ

In the aforementioned references (limited to dis-
counted objectives),
convergence guarantees are
mostly asymptotic, apply only to MARL sub-problems
as policy evaluation (estimating the value function as-
suming a ﬁxed policy (Heredia & Mou, 2020; Sha et al.,
2020)), or due to implied non-convexity induced by
policy parameterization, cannot avoid spurious1 poli-
cies (Qu et al., 2020a,b) – see (Zhang et al., 2020) for
further details.

(cid:15)2

For these reasons, we focus on LP reformulation of RL
in the average reward setting (De Farias & Van Roy,
2003; Kallenberg, 1983, 1994), for which stochastic
primal-dual method achieves optimal sample complex-
ity in the centralized tabular case (Wang, 2020b).
2 Our goal
is to understand which settings the
˜O( τ 2t2
mix|S||A|
log(1/δ)) complexity achieved for ﬁnd-
ing an (cid:15)-optimal solution with probability 1 − δ in
the centralized case (Wang, 2020b)[Theorem 4] may
be translatable to the multi-agent setting when agents
may only exchange local information with their one-
hop neighbors. In particular, when agents combine lo-
calized stochastic primal-dual methods with weighted
averaging to diﬀuse information across the network
(Chen & Sayed, 2012; Nedic & Ozdaglar, 2009), we
seek to determine whether the optimal sample com-
plexity of the LP approach generalizes to the average-
reward tabular MARL. Our contributions are to:

(i) propose a novel multi-agent variant of the dual LP
formulation of RL, where agents’ decisions are deﬁned
by estimates of an average state-action occupancy
measure and value vector, and consensus constraints
are imposed on agents’ localized estimates (Sec. 2).

(ii) owing to node-separability of the Lagrangian relax-
ation of the resulting optimization problem, we derive

1Spurious here should be interpreted in the sense of sta-

tionary points that are far from global optimality.

2In particular, we use O(1) to denote an absolute con-
stant, and ˜O(1) to hide polylog factors in |S|, |A|, and (cid:15),
which are the respective cardinalities in the state and ac-
tion spaces, and (cid:15) is some pre-deﬁned optimization error.

a decentralized model-free training mechanism based
on a stochastic variant of primal-dual method that em-
ploys Kullback-Lieber (KL) divergence as its proximal
term in the space of occupancy measures (Sec. 3),
together with local weighted averaging.

(iii) establish that the number of samples required to
attain near-globally optimal solutions matches tight
dependencies on the cardinality of the state and action
spaces (Wang, 2020b), and exhibits classical scalings
with the size of the team in prior theory (Nedic &
Ozdaglar, 2009).

(iv) demonstrate the experimental merits of this ap-
proach in cooperative navigation problems.

Additional Context. Local averaging as a strat-
egy for information mixing in multi-agent optimiza-
tion is outperformed by schemes based upon Lagrange
multiplier exchange, e.g., primal-dual method (Koppel
et al., 2015), alternating direction method of multipli-
ers (ADMM) (Boyd et al., 2011), and dual reformula-
tions (Terelius et al., 2011). In this work, however, we
opt for a primal-only approach to enforcing consen-
sus for simplicity and its compatibility with Perron-
Frobenius theory (Chung & Graham, 1997).

We further focus on the case where the communica-
tions network is a structural component of the problem
setting, as in (Lowe et al., 2017; Zhang et al., 2018).
However, a separate but related body of works esti-
mate the communications architecture when agents’
behavior is ﬁxed using graph neural networks (Ahi-
lan & Dayan, 2020; Bachrach et al., 2020; Eccles
et al., 2019) or statistical tests for correlation be-
tween agents’ local utilities (Lin et al., 2020; Qu et al.,
2020a).

To the best of our knowledge, none of the aforemen-
tioned works deal with the average reward settings in
MARL, with the exception of (Qu et al., 2020b). How-
ever, it provides asymptotic-only analysis. By con-
trast, the probably approximately correct (PAC) sam-
ple complexity results given here are unique to the

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

MARL average-reward setting, and may be seen as a
multi-agent generalization of (Wang, 2020b). Critical
to this generalization is a novel Lyapunov function that
result to jointly tracks the convergence of the primal-
dual iterates and the consensus error. We note that
PAC results have been developed for average-reward
MARL in (Xu et al., 2020). However, it operates un-
der a setting where policy and reward information are
globally shared agents at each step, which in the op-
timization literature is known as parallel (Bertsekas
& Tsitsiklis, 2015), not decentralized, as the updates
cannot be executed with local and neighboring infor-
mation only. For results most similar to this work in
the MARL setting, please see Table 1.

2 Problem Formulation

We consider MARL problems among agents who share
a globally observable state, but take actions and ob-
serve rewards which are distinctly local. In this con-
text, agents seek to coordinate in order to maxi-
mize the team’s cumulative return of rewards, which
locally observed rewards. More
is a sum over all
speciﬁcally, we consider a time-varying network Gt =
(V, Et, W t) of n agents N ={1, 2, . . . , n}, where agent
i ∈ N may communicate with its neighbors j if they
share an edge (i, j) ∈ Et, and no others, at a given
ij] ∈ Rn×n, where
time t. The weight matrix W t:=[wt
wt
ji ∀i, j, t, assigns weights to
each edge (i, j). One canonical example of wt
ij is
the relative degree between agent i and j at time t:
wt
i as the degree, or number
of nodes that are a one-hop neighbor of agent i.

ij ≥ 0 and wt

j), with dt

ij = wt

ij = dt

i + dt

i/(dt

With the network structure clariﬁed, we now detail
how the states, actions, and rewards interconnect.
Precisely, at each time, agent i ∈ V observes the cur-
rent system state s ∈ S and synchronously takes an ac-
tion ai ∈ Ai, which is concatenated as the joint action
a=(a1, ..., an) ∈ A1 × · · · × An. The state space S and
the constituent action spaces Ai are discrete ﬁnite sets
with respectively |S and |A| elements. Trajectories are
Markovian, that is, upon execution of the joint action
a, the state transitions to next state s(cid:48) with proba-
bility ps,s(cid:48)(a) := P(s | s, a). That the joint action a
is observed by all agents after execution is needed to
ensure full observability, i.e., that the MARL problem
can be deﬁned by an MDP. After the joint action a
is executed in state s, each agent i receives a reward
ri
a,s(s) ∈ [0, 1], only known to the agent i. The sys-
tem reward ra,s is deﬁned as the aggregation of local
rewards r(s, a):= 1
a,s. The goal of the coop-
n
erative agents here is the maximization of the global

i=1 ri

(cid:80)n

cumulative return deﬁned as

max
π

Jπ(s) := lim
T →∞

E

1
T

(cid:34)T −1
(cid:88)

t=0

(cid:35)
(cid:12)
(cid:12)
(cid:12)s0 = s

,

ra,s

(1)

where π denotes the joint policy of all agents, that
is, a probability distribution over joint action-space
given system state, π : S × A → [0, 1]. The joint
policy prescribes the probability that a joint action
a = (a1, . . . , an) is taken by the collection of the
agents when in system state s, which we assume fac-
tors into marginals of each individual agent’s policy:
π(a|s)=(cid:81)N
i=1 πi(ai|s). That is, the local policies are
statistically independent, and are further denoted as
πi(ai|s) which deﬁne the probability of taking action
ai by agent i when in state s. Moreover, the expecta-
tion in (1) is over the product measure associated with
state transition dynamics and the policy known as the
ergodic state occupancy measure.

Our speciﬁc goal in this work is the design of pol-
icy optimization schemes to solve (1) such that each
agent, upon the basis of its local action selections and
rewards, together with information exchange amongst
neighbors, in possession of global state-action infor-
mation, learns local policy parameters that result in
the overall team attaining the optimal value (1). We
place speciﬁc emphasis upon the non-asymptotic con-
vergence of such schemes and their scaling with re-
spect to the parameters oﬀ network G. Moreover, we
consider in the model-free setting, i.e., the dynamics
of the environment (the transition probabilities and
transitional rewards) are unknown to the agents, but
a simulation oracle is available to the team to gen-
erate state-action-reward tuples (s, a, r). We require
that the transition dynamics for a ﬁxed policy deﬁne
an irreducible Markov chain: for each state pair (s, s(cid:48))
and any policy π, there exist t such that the probabil-
ity that the system transitions from state s to state s(cid:48)
under policy π in t time-steps is non-zero. This con-
dition is suﬃcient to ensure the limit in (1) exists and
Jπ(s) =: λπ for all states s. Equivalently, the average
cost is independent of the initial state in the system.
Further, the optimal policy is time-invariant.

Towards transforming (1) into a workable form for de-
riving iterative model-free updates, we note that an
optimal policy satisﬁes the average-cost Bellman equa-
tion (Bertsekas et al., 1995) written as

λπ +vs = max
a∈A

(cid:40)

(cid:88)

s(cid:48)

ps,s(cid:48)(a)ra,s +

(cid:41)

ps,s(cid:48)(a)vs(cid:48)

, (2)

(cid:88)

s(cid:48)

for all s ∈ S. Denote solutions to the Bellman’s equa-
tion by pairs (λ∗, v∗), where scalar λ∗ = max Jπ(s) in
(1) is unique and equal to the optimal average cost.
The value vector v ∈ Rs (which aggregates scalars vs

Convergence Rates of Average-Reward MARL

for each s ∈ S) is called a diﬀerential reward function
and is unique up to a constant. Uniqueness is imposed
by (v∗)T ξ∗=0, where ξ∗ is the stationary distribution
under the optimal policy π∗, i.e. P π∗
ξ∗=ξ∗. Note that
each policy π is associated with a transition proba-
bility P π
a π(a|s)ps,s(cid:48)(a) for all s, s(cid:48) ∈ S, and a
stationary state distribution ξπ, which is a probabil-
ity distribution that remains unchanged in the Markov
chain as time progresses, i.e. P πξπ = ξπ. The diﬀeren-
tial reward function characterizes the transient eﬀect
of the initial state under a policy π.

s,s(cid:48):=(cid:80)

Continue then by noting that the optimal joint policy
π∗ may be formulated as the following LP (De Farias
& Van Roy, 2003):

(cid:88)

µ(a)T r(a)

max
µ∈R|S|×|A|
(cid:40)(cid:80)
(cid:80)

s. t.

a∈A
a∈A(I − P T
s∈S,a∈A µ(s, a) = 1µa,s ≥ 0 ∀a, s

a )µa = 0,

∀s

,

(3)

i ∈ N

where I is an identity matrix of the appropriate size
and Pa ∈ R|S|×|S| is the matrix whose (s, s(cid:48))-th entry
equals to ps,s(cid:48)(a). For every feasible point of the above
linear program µ = (µ(a))a∈A, the ξπ=(ξπ
s )s∈S is the
s =(cid:80)
stationary state distribution where ξπ
a µa,s, and
(cid:80)
x,a µ(x, a)ra,s corresponds to the average reward λπ
of policy π where π(a|s)= µa,s
. Moreover, µ(a) ∈ R|S|
ξπ
s
denotes the unnormalized occupancy measure over the
state space S for each action a ∈ A, whose stacking
over the action space A is denoted as µ ∈ R|S|×|A|.
Through normalization, one may recover the associ-
ated policy π for any feasible µ as ξπ
a∈A µ(s, a),
and π(a|s)= µ(s,a)
a∈A µ(s,a) , and from the deﬁnition of ξπ
s
and π(a|s), it holds that µ(s, a) = ξπ
s π(a|s). Then,
an optimal joint policy π∗ can be constructed by nor-
malizing the occupancy measures associated with the
solution to the above linear program. See (Puterman,
2014) and references therein for details.

s =(cid:80)

(cid:80)

π∗(a|s) =

µ∗(s, a)
a µ∗(s, a)

(cid:80)

.

(4)

By substituting the deﬁnition of the global reward
r(s, a) in terms of the local rewards ri(s, a) into(3),
we obtain a multi-agent optimization problem with the
global variables µa,s corresponding to joint policy π:

max
µ∈R|S|×|A|

subject to:

n
(cid:88)

(cid:88)

µ(a)T ri(a)

i=1
(cid:80)
(cid:80)

a∈A
a(I − P T
s,a µ(s, a) = 1






µ(s, a) ≥ 0 ∀s ∈ S, a ∈ A

a )µ(a) = 0 ∀s ∈ S

To solve (5), agents must cooperate in their policy
search. With each agent only exercising control over

their localized policy, the globally optimal joint pol-
icy π∗(a|s) may be obtained via (4). Speciﬁcally, un-
der the previously mentioned independence assump-
tion and knowledge of the state-action information,
each agent may obtain its local policy by marginaliz-
ing the other agents’ policies out of the the optimal
joint policy as follows πi(ai|s) = (cid:80)
µ(s,(ai,a−i)
(cid:80)
a µ(s,a)
where a−i denotes the joint action of all agents ex-
cept the i-th agent; ai denotes the action associated
with the i-th agent, and the joint action of all agents
is denoted by a, i.e. a=(ai, a−i)=(a1, . . . , an).

a−i∈A

Algorithm 1: Randomized Multi-agent Primal-
dual (RMAPD) Algorithm
1 Input:(cid:15) > 0, S, A, t∗
mix, τ
2 Set vi = 0 ∈ R|S|, πi = 1

|Ai|e ∈ R|Ai|, ∀s ∈ S,

mix)2|S||A|, M = 4t∗
(cid:113) log(|S(cid:107)A|)

3 Set T = (τ t∗
4 Set β = 1
t∗
mix
5 for iteration t = 0, 1, 2, ... do

2|S(cid:107)A|T , α = |S|t∗

mix + 1

(cid:113) log(|S(cid:107)A|)
2|A|T

mix

6

7

8

9

10

11

12

13

14

for agent i = 1, 2, ..., N do

Observe the system state s,
Execute action ai ∼ πi(·|s)
Observe local reward ri
Send (µt
i ) to j ∈ ni, receive (µt
Compute local weighted averages [cf. (10)]
i = (cid:80)n
(cid:101)µt
j, (cid:101)vt
ijvt
j ,
Conduct entropic ascent w.r.t. µt+1
in (13):

i = (cid:80)n

j=1 wt

j=1 wt

s,s(cid:48)(a)

ijµt

j,vt

i,vt

j).

i

µt+ 1

2

i

(s, a) =

(cid:80)

s(cid:48)

µt+1
i = argmin

µi∈U

(s, a))

i

i(s, a) exp(∆t+1
(cid:101)µt
(cid:80)
a(cid:48) (cid:101)µt
DKL(µi(cid:107)µt+ 1

i(s, a) exp(∆t+1

),

i

2

i

(s(cid:48), a(cid:48)))

with dual gradient ∆t+1
Update value vector for agent i via (14) as
vt+1
i = ΠV [(cid:101)vt

(s, a) in (11).

] with dt+1

i + dt+1
i

in (12)

i

i

With the setting clariﬁed, we next shift to developing
a decentralized model-free algorithm to solve (1) upon
the basis of Lagrangian relaxation.

3 Randomized Primal-Dual Method

(5)

.

In this section, we reformulate the multi-agent LP of
(5) as a saddle point problem by considering its La-
In particular, we formulate the
grangian relaxation.
following saddle point problem

min
v∈V

max
µ∈U

L(µ, v) :=

n
(cid:88)

(cid:88)

i=1

a∈A

µ(a)T [(Pa − I)v+ri(a)]. (6)

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

Note that we have computed the transpose of the con-
straint to simplify the expression. Under Assumptions
3 and 4 introduced in Sec. 4, we may establish that
the primal-dual optimal pair (v∗, µ∗) of (6) belong to
the following restricted setsfor the value V ⊂ R|S| and
occupancy measures U ⊂ R|S|×|A| deﬁned as
V ={v ∈ R|S|(cid:12)
(cid:12)
(cid:12) (cid:107)v(cid:107)∞ ≤ 2tmix} ,
(cid:12)
(cid:12)eT µ = 1, µ ≥ 0,
(cid:12)

U =(cid:8)µ = (µa)a∈A

µ(a) ≥

e(cid:9),

(cid:88)

(7)

√

1
τ |S|

a∈A

where tmix is the mixing time of the Markov chain
which characterizes how fast the Markov decision pro-
cess reaches its stationary distribution from any state
under any policy (Assumption 4), and τ is a con-
stant greater than one which characterizes how much
the stationary distribution varies as the policy varies
(Assumption 3). The deﬁnitions of these feasible
sets is borne out of the analysis, and mirrors (Wang,
2020b). Next, we note that the Lagrangian of (6) is
node-separable. Speciﬁcally, by deﬁning the local La-
grangian for agent i ∈ V as

Li(µ, v) :=

(cid:88)

a

µ(a)T [(I − Pa)v + ri(a)],

(8)

where v is a column vector with vs as its s-th compo-
nent, then the Lagrangian of the multi-agent problem
L(µ, v) may be decomposed into a sum over local La-
grangian Li(µ, v) as L(µ, v) = (cid:80)n
i=1 Li(µ, v), which
permits us to simplify the saddle point problem as

min
v∈V

max
µ∈U

L(µ, v) =

n
(cid:88)

i=1

Li(µ, v).

(9)

The min-max problem in (9) is convex in v and concave
in µ. We note that the variables v and µ are common
among all the agents in the network and we are inter-
ested in solving the problem in a distributed manner.
This expression in (9) is suggestive of employing a so-
lution methodology upon the basis of a decentralized
stochastic primal-dual method, which is the focus of
the following subsection.

3.1 Stochastic Primal-Dual Method

We propose applying stochastic primal-dual method
to solve (5), which, owing to the node-separability of
the Lagrangian, yields a decentralized scheme for pol-
icy optimization. In particular, in order to solve the
saddle point problem, we note that agents must access
estimate the global reward, but they lack access. In-
stead, agents only observe local rewards. To address
this issue, we allow each agent to track a distinctly
localized estimate vi ∈ S of the value, which are sub-
stituted in place of the global value vector v in (6),

and similarly with respect to the occupancy measure
µt
i, which are in lieu of the global primal-dual pair
(µt, vt). Then, agent i cooperates with other agents
through a weighted averaging of its primal and the
i (resp. (cid:101)vt
dual variables, i.e. a convex combination (cid:101)µt
i )
of its own estimate µt
i (resp. vt
i ) with the estimates
received from those of its neighbors j ∈ ni at time t:

(cid:101)µt
i =

n
(cid:88)

j=1

wt

ijµt
j,

(cid:101)vt
i =

n
(cid:88)

j=1

wt

ijvt
j ,

(10)

Then, each agent takes a gradient descent (respec-
tively, ascent) step to minimize (respectively, maxi-
mize) the local Lagrangian function Li, followed by
a projection onto the constraint set U (respectively,
V). However, since the transition dynamics model is
unavailable to agent i (in the form of transition ma-
trix Pa), it cannot to evaluate the constraint in (3).
This precludes the evaluation of primal and dual gra-
dients of the Lagrangian, which necessitates stochastic
approximations of these quantities, which we present
jointly with respective step-size parameters β and α as

ˆ∇µiLi = ∆t+1

i = β

i(s, s(cid:48), a) − M

i (s(cid:48)) − vt
vt

i (s) + rt
(cid:101)µt
i(s, a)
with probability (cid:101)µt

i(s, a)

ˆ∇vi Li = dt+1

i =

α(es − es(cid:48)),

µt
i(s, a)
(cid:101)µt
i(s, a)
with probability (cid:101)µt

i(s, a)ps,s(cid:48)(a),

where M := 4tmix + 1 is a “shift parameter” which
ensures suﬃcient decrease of a certain martingale pro-
cess deﬁned in terms of the KL divergence that arises
in the analysis (to be made precise later), and the su-
perscript t denotes the value of the variable at time t.
Moreover tmix is the mixing time of the Markov chain
induced by a ﬁxed policy (Assumption 4). Here es,a
is the indicator variable which is 1 for (s, a) and null
otherwise. Further, es denotes the standard basis vec-
tor with 1 in slot s and null otherwise. Note that we
adopt the convention that, at time-step t, the variables
with superscript t are known, and the superscript t + 1
indicates an update direction in terms of random vari-
ables realized at time t. An additional point of note
is that the gradient with respect to the value vector is
−α(es(cid:48) − es), which we swap to cancel out the nega-
tive. Then, using these update directions, stochastic
primal-dual method is such that at every t ≥ 0, each
agent i generates new estimates µt+1

, vi

t+1 as

i

µt+1
i = argmin

µi∈U

DKL(µi(cid:107)µt+ 1

2

i

),

(13)

where µt+ 1

2

i

(s, a) =

(cid:80)

s(cid:48)

vt+1
i = ΠV [(cid:101)vt

i + dt+1
i

],

i(s, a) exp(∆t+1
(cid:101)µt
(cid:80)
a(cid:48) (cid:101)µt

i(s, a) exp(∆t+1

i

i

(s, a))

(s(cid:48), a(cid:48)))

(14)

.es,a,

(11)

(12)

Convergence Rates of Average-Reward MARL

t

is given in (12). Moreover, ∆t+1

where ΠV is a Euclidean projection onto the set V, and
dt+1
is the gradient of
i
the local Lagrangian with respect to µi in (11). Note
that the update on µ is mirror-ascent with a Kullback-
Leibler (KL) divergence over the unnormalized prob-
ability simplex centered at (cid:101)µt
i, whereas the gradient
step on the value vector v is a simple projected gradi-
ent descent centered at (cid:101)vt
i . The descent step on v is
written in terms of an addition due to the cancellation
of a negative, as mentioned after (12). We assume
algorithm initialization as µi = 0 and vi = 0 for all
i ∈ N . The overall MARL policy optimization scheme
based upon randomized primal-dual solutions to the
LP formulation is summarized as Algorithm 1.

4 Convergence Analysis

In this section, we establish the non-asymptotic con-
vergence of the proposed algorithm in the sense that
agents’ local primal-dual variables (a) achieve consen-
sus and (b) converge to the primal-dual optimal pair
of their local Lagrangians (8). As a consequence, upon
the basis of local observations and information ex-
change with neighbors, agents are able to solve (5),
and hence (1). We divide the analysis of the algo-
rithm in two steps. First, we establish that all local
estimates achieve consensus. Second, we show that
the consensus vectors are in fact a pair of primal-dual
optimal solution. To establish these results, we state
some conditions are required on the graph Gt next.

Assumption 1 [Strong Connectivity] There exists a
positive integer B such that graph (N , ∪B−1
l=0 Et+l) is
strongly-connected for any t ≥ 0, i.e., every node is
reachable from another in at most B time-steps.

Assumption 2 For all i ∈ V and t ≥ 0: (a) there
exists a scalar η ∈ (0 < η < 1) such that wt
ij ≥ η
when j ∈ N t
ij =
(cid:80)n
i=1 wt
ij=1; that is, the mixing matrix W t is doubly
stochastic .

ij = 0 otherwise; (b) (cid:80)n

i , and wt

j=1 wt

Assumption 1 ensures that after a union of B time-
slots, the network is connected, which ensures infor-
mation propagates across the network. Assumption 2
ensures that an agent suﬃciently balances the weight-
ing of its own information with that of other agents.
Assumption 2 ensures that the mixing matrices have a
Perron-Frobenius eigenvalue associated with an eigen-
vector whose entries are all 1, i.e., the existence of
a vector satisfying consensus. We also make two as-
sumptions on the MDP stationary distribution and
mixing time of the chain:

Assumption 3 [Ergodic Decision Process] The
Markov decision process is τ -stationary in the sense

that it is ergodic under any stationary policy π and
√
τ
|S| e,
there exists τ > 1 such that
where e is a vector of all 1’s.

1√
τ |S| e ≤ ξπ ≤

Assumption 4 [Fast-Mixing Markov Chains] The
Markov decision process is tmix-mixing in the sense
(cid:12)
that tmix ≥ maxπ min (cid:8)t ≥ 1
(cid:12)
(cid:12) (cid:107)(P π)t(s, .) − ξπ(cid:107)T V ≤
4 , ∀s ∈ S(cid:9), where (cid:107).(cid:107)T V is the total variation norm.

1

The factor τ characterizes the variability of the sta-
tionary distribution with respect to the policy. tmix
deﬁnes how fast the MDP reaches its stationary dis-
tribution from any state under any policy π.

4.1 Primal-dual Optimality

To show that the consensus vector coincides with a pair
of primal-dual optimal solution, we show that, at each
iteration of the algorithm, the local iterates get closer
to the local primal-dual optimal pair in expectation.
It turns out that to do so, we must ﬁrst show that
agents’ estimates reach approximate consensus, and
then construct a Lyapunov function with respect to
quantities deﬁned in terms of network averages. We
proceed to doing so next,

Achieving consensus. We establish that the local it-
erates converge to the global mean at a speciﬁed rate
in terms of the lower bound on the mixing weights, the
diameter of the network, and the strong connectivity
parameter. The consensus error must be character-
ized for both the value vector and occupancy mea-
sure estimates, which motivate the following network-
aggregated averages at time t:

µt =

1
n

n
(cid:88)

i=1

µt
i,

vt =

1
n

n
(cid:88)

i=1

vt
i .

(15)

It also turns out to be convenient to deﬁne the auxil-
iary sequence

i := ΠV [(cid:101)vt
qt

i + dt+1
i

] − (cid:101)vt
i ,

(16)

i (resp. (cid:101)vt

where qt
i represent the error between the weight-
averaged iterates (cid:101)µt
i ) and their previous up-
date following projection/composition with a proxi-
mal operator. We note that a similar technique for
minimization problems is considered in (Chen et al.,
2021), but here we are considering a diﬀerent minimax
setting, necessitating analyzing the consensus error in
both the primal and dual variables. See Lemma 3 in
the appendix for the analysis of consensus error.

Lyapunov Function Construction. Next we de-
ﬁne a decrement process that tracks the evolution of
the averaged primal and dual iterates to the primal-
dual optimal pair, which eventuates in our ability to

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

formalize the overall convergence rate of Algorithm 1.
Consider the Lyapunov function Et and duality gap
quantiﬁer Dt, deﬁned as

lection β = (cid:101)O

(cid:16)(cid:113)

√

E0
n|S||A|˜t2
mixD(Γ,ρ)T

(cid:17)

, it holds that

(cid:34)

(cid:88)

E

(cid:35)
(cid:2)[v∗ − Pav∗ + ra]T ˆµ(a)(cid:3)

+ λ∗

Et :=

1
n

n
(cid:88)

DKL(µ∗(cid:107)µt

i) +

1
2|S|t2

mix

(cid:13)vt − v∗(cid:13)
(cid:13)
2
(cid:13)

a∈A

i=1
Dt := λ∗ +

(cid:88)

(cid:2)µt(a)T [(I − Pa)v∗ + ra](cid:3) .

(17)

(cid:32)

(cid:114) √

≤ (cid:101)O

˜tmix

nE0|S||A|D(Γ, ρ)
T

(cid:33)

,

(19)

a∈A

The ﬁrst term of Et quantiﬁes the sum of KL diver-
gences between the optimal µ∗ and local occupancy
measures µt
i, and the second term quantiﬁes the sub-
optimality of the average sequence vt. In Dt, we track
the constraint violation of (5).

Lemma 1 With Lyapunov function Et and duality
gap quantiﬁer Dt in(17), the iterates of Algorithm 1
exhibits approximate stochastic descent:

E [Et+1 | Ft] ≤ Et − βDt + β2 (cid:101)O (cid:0)|S||A|t2

mix

(cid:1)

(18)

+

β
n

n
(cid:88)

(cid:88)

[(cid:0)vt − vt

i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)]

i=1
a∈A
n
(cid:88)

(cid:88)

i=1

a∈A

+

β
n

[(cid:0)

i(a) − µt(a)(cid:1)T
(cid:101)µt

[(Pa − I)v∗ + ra]],

where β denotes the constant step size.

See Appendix A for proof. This result is a generaliza-
tion of (Wang, 2020b, Proposition 9) to multi-agent
settings. For n = 1, the last two terms are null, which
simpliﬁes to the proposition in the aforementioned ref-
erence. In generalizing it to the multi-agent setting,
we note that existing analyses of multi-agent stochas-
tic optimization methods based on consensus proto-
col rely on ﬁnite variance conditions (Li et al., 2018).
However, the dual gradient to be evaluated at con-
sensus variable (cid:101)µt
i(a) is required for unbiasedness in
the gradient evaluation in (11), may cause unbounded
noises to the stochastic gradient estimates. An addi-
tional complication is the joint treatment of consensus
error in primal and dual variables owing to the struc-
ture of the minimax objective (6), which is not treated
in any of the earlier works (Boyd et al., 2011; Chen
et al., 2021; Kar et al., 2013; Li et al., 2018; Nedic &
Ozdaglar, 2009; Sha et al., 2020).

Next, we present the main result of this subsection
which upper bounds the duality gap as follows.

Theorem 1 For the time-averaged sequence of occu-
(cid:80)T −1
pancy measures ˆµ = 1
i, after T num-
t=0
T
ber of iterations of Algorithm 1, with the step size se-

i=1 µt

(cid:80)n

1
n

(cid:3) such that
where ˜tmix = 1 + 4tmix, D(Γ, ρ) := (cid:2) 1+Γ
Γ=(1 − η/4n2)−2 and ρ=(1 − η/(4n2))1/B, B is the
network strong connectivity parameter.

1−ρ

See Appendix G for proof. Observe that the duality
gap characterization is nonstandard from typical sad-
dle point problems (Nedi´c & Ozdaglar, 2009), that is,
the left-hand side of (19) characterizes how the aver-
aged dual variable ˆµ evaluated at the constraint at the
optimal primal variable (v∗, λ∗), and not an additional
presence of the dual sub-optimality. This is a special
structural consequence of the LP setting that breaks
down for general nonlinear objectives or constraints.
This upper bound characterizes the number of times
the complementary slackness condition is violated on
average, which facilitates deriving the sample complex-
ity required to achieve an (cid:15)-optimal policy with high
probability. Next we shift focus to this result.

4.2 From Duality Gap to Average Reward

We ﬁrst derive the convergence in probability result
for the proposed algorithm in next Lemma 2.

√

(cid:16)

mix

Lemma 2 Suppose Algorithm 1 is run for T =
(cid:17)
nE0|S||A|D(Γ,ρ)
τ 2˜t2
iterations. Then it outputs
Ω
(cid:15)2
a policy ˆπ= 1
t=1 πt such that λ∗−(cid:15) ≤ λˆπ, with prob-
T
ability 2/3, meaning, we output an (cid:15) optimal policy
with probability 2/3.

(cid:80)T

See Appendix H for proof. Lemma 2 establishes that
Algorithm 1 converges to (cid:15)-optimal policy with prob-
ability 2/3. To boost the success probability to near
1, we develop a strategy where one runs Algorithm 1
multiple times and selects the best outcome. This pro-
cedure is formalized in Algorithm 2. With this meta-
strategy in practice, we may establish that one can
indeed achieve achieve comparable sample complexity
to Lemma 2 but with high probability.

Theorem 2 Under Assumption 1-4, if we run Algo-
(cid:1), then we output an approx-
(cid:0) δ
rithm 2 for K = log1/3
2
(cid:101)π ≥ λ∗−(cid:15) with at least proba-
imate policy (cid:101)π such that λ
bility 1−δ. Hence, the total number of samples required
(cid:17)
(cid:16)
τ 2˜t2
are given by T = Ω
We deﬁne D(Γ, ρ) := (cid:2) 1+Γ

nE0|S||A|D(Γ,ρ)
· log 1
(cid:15)2
δ
(cid:3) such that Γ=(1−η/4n2)−2

mix

√

1−ρ

Convergence Rates of Average-Reward MARL

(a) Grid world.

(b) M =3, n=2.

(c) M =2, n=3.

Figure 1: We compare Algorithm 1 with its centralized counterpart (centralized stochastic primal-dual, or
C-SPD), and with independent approximate value iteration (I-AVI). Fig. 1(a) shows the grid world environment
for the experiments. Fig. 1(b) compares the average reward for all the mentioned algorithms. It shows that
RMAPD is able to learn the optimal policy equivalent to centralized technique. We note that I-AVI fails to
learn the optimal policy because agents are not cooperating with each other. Fig. 1(c) shows similar result for
M = 2, n = 3.

Algorithm 2: Meta-Randomized Multi-agent
Primal-dual (M-RMAPD) Algorithm

1 Input:(cid:15) > 0, S, A, t∗
mix, τ
2 Run the Algorithm 1 for K number of iterations

with precision (cid:15)
π(1), · · · , π(K).

3 and denote the output as

3 For each output policy π(k), conduct the

approximate value evaluation for
log (cid:0) 4K
L = ˜O (cid:0) tmix
(cid:15)2
Y
precision level (cid:15)/3 and prob.

(k)

δ

δ
2K .

(cid:1)(cid:1) time steps and obtain

which is approximate value evaluation with

4 Output (cid:101)π = π(k∗) such that k∗ = argmaxk Y

(k)

.

and ρ=(1 − η/(4n2))1/B where B is the network strong
connectivity parameter.

See Appendix I for proof. To the best of our knowl-
edge, the result in Theorem 1 is the ﬁrst to character-
ize the sample complexity of MARL schemes with high
probability to achieve global optimality. We accentu-
ate that we are able to discern explicit dependence
upon the mixing time tmix and network parameters
with tight dependence upon the cardinalities of the
state and action spaces.

5 Experiments

In this section, we evaluate the practical merit of the
proposed algorithm for MARL. In particular, since we
are interested in cooperative multi-agent RL, we con-
sider an experimental setting where the need for aggre-
gating the policy learnt by individual agents via con-

sensus or centralized training is important. Hence, we
consider a cooperative navigation problem in a M ×M
grid world environment shown in Fig. 1(a). Each
agent is equipped with action Ai = {↑, →, ↓, ←} and
observe M × M grid as the local state space Si. In
this environment, each agent receives a reward ri, and
the common goal is to reach a state with maximum
average reward across the agents. For instance, in our
grid world environment as depicted in 1(a) for M = 3
and n = 2, the agent 1 and agent 2 receive a reward
of 8 and 5 in the top left grid, and a reward of 5 and
10 in the lower rightmost grid, respectively, when they
reach there simultaneously, and zero otherwise. This
settings ensures that the cooperative behavior would
result in higher average reward as compared to a non-
cooperative behavior.

We solve the grid world navigation problem using the
proposed RMAPD algorithm and present the average
cumulative reward returns in Fig. 1. We compare the
performance of the proposed decentralized algorithm
with a centralized LP solver and also a variant of ap-
proximate value iteration where each agent operates
independently of all others to maximize its local aver-
age reward. The plot in Fig. 1(b) shows that the pro-
posed algorithm iterates converge to the centralized
optimal solution and is signiﬁcantly better than the
independent learning schemes for a grid of size M = 3
with n = 2 agents, and similarly for M = 2, n = 3 in
Fig. 1(c).

In the experiments, we run 20 independent iterations
of all the algorithms for 107 timesteps and plot the av-
erage rewards. We observe that convergence is reached
sooner in RMAPD as compared to centralized training

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

because of the need to explore a larger state space per
agent before converging as opposed to its decentralized
counterpart. Further, the importance of the consensus
mechanism of the proposed algorithm is highlighted by
the much lower average reward achieved by the inde-
pendent approximate value iteration.

6 Conclusions

In this work, we considered a multi-agent reinforce-
ment learning problem with average-reward criterion.
The problem has been studied in literature in single
agent settings only to date, for which randomized LP
solvers achieve optimal PAC bounds in terms of the
cardinality of the state and action spaces. We general-
ized such approaches to multi-agent settings by com-
bining randomized LP solvers with consensus averag-
ing, and elucidated their PAC bounds for the same.
Interestingly, by the use of a novel Lyapunov function
in the convergence analysis, the dependence upon the
state and action space cardinality is still maintained
similar to centralized counterpart, with an additional
dependence on the way information propagates across
the multi-agent network.

As a future direction, we will develop variants of this
framework that can operate with parameterized oc-
cupancy measures and diﬀerential value vectors, such
that the scaling is with respect to the parameterization
rather than the state and action spaces.

References

Ahilan, S. and Dayan, P. Correcting experience re-
play for multi-agent communication. arXiv preprint
arXiv:2010.01192, 2020.

Bachrach, Y., Everett, R., Hughes, E., Lazaridou, A.,
Leibo, J. Z., Lanctot, M., Johanson, M., Czarnecki,
W. M., and Graepel, T. Negotiating team formation
using deep reinforcement learning. Artiﬁcial Intelli-
gence, 288:103356, 2020.

Ba¸sar, T. and Olsder, G. J. Dynamic noncooperative

game theory. SIAM, 1998.

Bertsekas, D. and Tsitsiklis, J. Parallel and distributed
computation: numerical methods. Athena Scientiﬁc,
2015.

Bertsekas, D. P., Bertsekas, D. P., Bertsekas, D. P.,
and Bertsekas, D. P. Dynamic programming and op-
timal control, volume 1. Athena scientiﬁc Belmont,
MA, 1995.

Boyd, S., Parikh, N., and Chu, E. Distributed opti-
mization and statistical learning via the alternating
direction method of multipliers. Now Publishers Inc,
2011.

Chen, J. and Sayed, A. H. Diﬀusion adaptation strate-
gies for distributed optimization and learning over
networks. IEEE Transactions on Signal Processing,
60(8):4289–4305, 2012.

Chen, S., Garcia, A., and Shahrampour, S. On dis-
tributed non-convex optimization: Projected sub-
gradient method for weakly convex problems in net-
works. IEEE Transactions on Automatic Control,
2021.

Chung, F. R. and Graham, F. C. Spectral graph theory.
Number 92. American Mathematical Soc., 1997.

Claus, C. and Boutilier, C. The dynamics of reinforce-
ment learning in cooperative multiagent systems.
1998.

De Farias, D. P. and Van Roy, B. The linear program-
ming approach to approximate dynamic program-
ming. Operations Research, 2003. ISSN 0030364X.
doi: 10.1287/opre.51.6.850.24925.

De Farias, D. P. and Van Roy, B. The linear program-
ming approach to approximate dynamic program-
ming. Operations research, 51(6):850–865, 2003.

Doan, T., Maguluri, S., and Romberg, J. Finite-time
analysis of distributed td (0) with linear function ap-
proximation on multi-agent reinforcement learning.
In in ICML, pp. 1626–1635, 2019.

Eccles, T., Bachrach, Y., Lever, G., Lazaridou, A., and
Graepel, T. Biases for emergent communication in
multi-agent reinforcement learning. In in NeurIPS,
pp. 13111–13121, 2019.

Foerster, J., Assael, I. A., De Freitas, N., and White-
son, S. Learning to communicate with deep multi-
agent reinforcement learning. in NeurIPS, 29:2137–
2145, 2016.

Foerster, J., Nardelli, N., Farquhar, G., Afouras, T.,
Torr, P. H., Kohli, P., and Whiteson, S. Stabil-
ising experience replay for deep multi-agent rein-
forcement learning.
In Proceedings of the 34th in
ICML-Volume 70, pp. 1146–1155, 2017.

Heredia, P. and Mou, S. Finite-sample analysis of
multi-agent policy evaluation with kernelized gra-
dient temporal diﬀerence. In 2020 59th IEEE Con-
ference on Decision and Control (CDC), pp. 5647–
5652. IEEE, 2020.

Kallenberg, L. C. M. Linear Programming and Finite
Markovian Control Problems. CWI Mathematisch
Centrum, 1983.

Kallenberg, L. C. M. Survey of linear programming for
standard and nonstandard Markovian control prob-
lems. Part I: Theory. Zeitschrift f¨ur Operations Re-
search, 40(1):1–42, 1994.

Kar, S., Moura, J. M., and Poor, H. V. Qd-learning:
A collaborative distributed strategy for multi-agent

Convergence Rates of Average-Reward MARL

reinforcement learning through consensus+ innova-
tions. IEEE Transactions on Signal Processing, 61
(7):1848–1862, 2013.

Puterman, M. L. Markov decision processes: discrete
John Wiley &

stochastic dynamic programming.
Sons, 2014.

Koppel, A., Jakubiec, F. Y., and Ribeiro, A. A saddle
point algorithm for networked online convex opti-
mization. IEEE Transactions on Signal Processing,
63(19):5149–5164, 2015.

Qu, C., Mannor, S., Xu, H., Qi, Y., Song, L., and
Xiong, J. Value propagation for decentralized net-
worked deep multi-agent reinforcement learning. In
in NeurIPS, pp. 1184–1193, 2019.

Koppel, A., Sadler, B. M., and Ribeiro, A. Proxim-
ity without consensus in online multiagent optimiza-
tion. IEEE Transactions on Signal Processing, 65
(12):3062–3077, 2017.

Qu, G., Lin, Y., Wierman, A., and Li, N. Scal-
able multi-agent reinforcement learning for net-
worked systems with average reward. arXiv preprint
arXiv:2006.06626, 2020a.

Krishnamurthy, V. Partially observed Markov decision

processes. Cambridge University Press, 2016.

Lee, D., Yoon, H., Cichella, V., and Hovakimyan,
N. Stochastic primal-dual algorithm for distributed
arXiv
temporal diﬀerence learning.
gradient
preprint arXiv:1805.07918, 2018.

Lee, D., He, N., Kamalaruban, P., and Cevher, V. Op-
timization for reinforcement learning: From a single
agent to cooperative agents. IEEE Signal Processing
Magazine, 37(3):123–135, 2020.

Leibo, J., Zambaldi, V., Lanctot, M., Marecki, J., and
Graepel, T. Multi-agent reinforcement learning in
sequential social dilemmas. In AAMAS, volume 16,
pp. 464–473. ACM, 2017.

Li, J., Li, G., Wu, Z., and Wu, C. Stochastic mir-
ror descent method for distributed multi-agent op-
timization. Optimization Letters, 12(6):1179–1197,
2018.

Lin, Y., Qu, G., Huang, L., and Wierman, A. Dis-
tributed reinforcement learning in multi-agent net-
worked systems. arXiv preprint arXiv:2006.06555,
2020.

Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P.,
and Mordatch, I. Multi-agent actor-critic for mixed
cooperative-competitive environments. Neural In-
formation Processing Systems (NIPS), 2017.

Lussange, J., Lazarevich, I., Bourgeois-Gironde, S.,
Palminteri, S., and Gutkin, B. Modelling stock mar-
kets by multi-agent reinforcement learning. Compu-
tational Economics, 57(1):113–147, 2021.

Mahajan, A. and Mannan, M. Decentralized stochastic
control. Annals of Operations Research, 241(1-2):
109–126, 2016.

Nedic, A. and Ozdaglar, A. Distributed subgradient
methods for multi-agent optimization. IEEE Trans-
actions on Automatic Control, 54(1):48–61, 2009.

Nedi´c, A. and Ozdaglar, A. Subgradient methods
for saddle-point problems. Journal of Optimization
Theory and Applications, 142(1):205–228, 2009.

Qu, G., Lin, Y., Wierman, A., and Li, N. Scal-
able multi-agent reinforcement learning for net-
worked systems with average reward. arXiv preprint
arXiv:2006.06626, 2020b.

Rashid, T., Samvelyan, M., Schroeder, C., Farquhar,
G., Foerster, J., and Whiteson, S. Qmix: Monotonic
value function factorisation for deep multi-agent re-
inforcement learning. In in ICML, pp. 4295–4304,
2018.

Raveh, O. and Meir, R. Pac guarantees for cooperative
multi-agent reinforcement learning with restricted
communication. arXiv preprint arXiv:1905.09951,
2019.

Sha, X., Zhang, J., You, K., Zhang, K., and
Ba¸sar, T. Fully asynchronous policy evaluation in
distributed reinforcement learning over networks.
arXiv preprint arXiv:2003.00433, 2020.

Terelius, H., Topcu, U., and Murray, R. M. Decentral-
ized multi-agent optimization via dual decomposi-
tion. IFAC proceedings volumes, 44(1):11245–11251,
2011.

Tesauro, G. and Kephart, J. O. Pricing in agent
economies using multi-agent q-learning.
Au-
tonomous agents and multi-agent systems, 5(3):289–
304, 2002.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Math-
ieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell,
R., Ewalds, T., Georgiev, P., et al. Grandmaster
level in starcraft ii using multi-agent reinforcement
learning. Nature, 575(7782):350–354, 2019.

Wai, H.-T., Yang, Z., Wang, Z., and Hong, M. Multi-
agent reinforcement learning via double averaging
primal-dual optimization. In in NeurIPS, pp. 9649–
9660, 2018.

Wang, M. Randomized linear programming solves the
Markov decision problem in nearly linear (some-
times sublinear) time, volume 45.
ISBN
2020a.
0000000221. doi: 10.1287/moor.2019.1000.

Wang, M. Randomized linear programming solves
the markov decision problem in nearly linear (some-

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

times sublinear) time. Mathematics of Operations
Research, 45(2):517–546, 2020b.

Wang, P., Chan, C.-Y., and de La Fortelle, A. A re-
inforcement learning based approach for automated
In 2018 IEEE Intelligent
lane change maneuvers.
Vehicles Symposium (IV), pp. 1379–1384. IEEE,
2018.

Xu, Y., Deng, Z., Wang, M., Xu, W., So, A. M.-C.,
and Cui, S. Voting-based multiagent reinforcement
learning for intelligent iot. IEEE Internet of Things
Journal, 8(4):2681–2693, 2020.

Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
Fully decentralized multi-agent reinforcement learn-
ing with networked agents. In in ICML, pp. 5872–
5881, 2018.

Zhang, K., Koppel, A., Zhu, H., and Basar, T. Global
convergence of policy gradient methods to (almost)
locally optimal policies. SIAM Journal on Control
and Optimization, 58(6):3586–3612, 2020.

Convergence Rates of Average-Reward MARL

Supplementary Material for
“Convergence Rates of Average-Reward Multi-agent
Reinforcement Learning via Randomized Linear Programming”

A Proof of Lemma 1

Before starting the proof of Lemma 1, we provide the intermediate lemmas which are useful in the analysis as
follows.

We proceed to deﬁning the transition matrix Φ(t, s), for all (t, s) with t ≥ s ≥ 0 as a product of weight matrices,
given by

Φ(t, s) = W tW t−1 . . . W s,

(20)

as the product of weight matrices associated with the time-varying graph Gt = (V, Et, W t). The following is a
key fact regarding the transition matrices (20).

Proposition 1 (Nedic & Ozdaglar, 2009)[Proposition 1] Under Assumptions 1 and 2, for all i, j ∈ V and all
(t, s) with t ≥ s ≥ 0, we have

(cid:12)
(cid:12)
(cid:12)[Φ(t, s)]ij −

(cid:12)
(cid:12) ≤ Γρt−s
(cid:12)

1
n

(21)

where Γ=(1 − η/4n2)−2 and ρ=(1 − η/(4n2))1/B, with, n the number of nodes in N , B is the strong-connectivity
parameter of Assumption 1, and η the lower-bound on the weights wij in Assumption 2.

Observe that for a static graph, we have Φ(t, s) = W t−s+1 and B = 1, that is, the weighting matrix is constant
over time and the graph is assumed to be strongly connected. Next, we establish that the iterates have bounded
deviation from the mean.

Lemma 3 Let Assumptions 1 and 2 hold. Recall constants ρ and Γ from Lemma 1.

(i) Denote as {vt
constant step size α, for all i ∈ V and t ≥ 0, we have

i }t≥0 the sequence of diﬀerential value vector estimates generated by Algorithm 1. Then, for

E[(cid:107)vt −(

1
n

eeT ⊗ I|S|)vt(cid:107) | Ft] ≤ O(

√

nα) · Zt(Γ, ρ),

where Zt(Γ, ρ):=[1+ Γ(1−ρt−1)
(cid:0) 1
n eeT ⊗ I|S|
the identity matrix.

1−ρ

i , and
(cid:1) vt stacks of n copies of average vector vt [cf. (15)] of dimension n|S|, and I|S| ∈ R|S|×|S| is

stacks value vector estimates vt

n]T ] ∈ Rn|S|

1]T ; · · · ; [vt

], vt=[[vt

(ii) The sequence of occupancy measure estimates {µt
satisfy for all i ∈ V and t ≥ 0

i}t≥0 generated by Algorithm 1 under constant step size α,

(cid:20)
(cid:107)µt −

E

(cid:18) 1
n

eeT ⊗ I|S||A|
≤ (2(4tmix + 1)β(cid:112)|S||A|

µt(cid:107) | Ft
√

(cid:19)

n) · Zt(Γ, ρ),

(cid:21)

(22)

where Zt(Γ, ρ):=[1+ Γ(1−ρt−1)
all i, and (cid:0) 1
I|S||A| ∈ R|S||A|×|S||A| is the identity matrix.

n eeT ⊗ I|S||A|

], µt=[[µt

1−ρ

(cid:1) µt stacks of n copies of average vector µt [cf.

1]T ; · · · ; [µt

n]T ] ∈ Rn|S||A| stacks occupancy measure estimates µt

i across
(15)] of dimension n|S||A|, and

See Appendix B for proof. This result is employed in the analysis of the evolution of the localized primal-dual
iterations (13) - (14) when we decompose the sub-optimality with respect to the global saddle point problem (6)
into a consensus error and an optimization error especially to address the consensus error.

Next, we prove the intermediate results which leads to the convergence of Lemma 1. In particular, we start with
the KL Divergence diﬀerence DKL(µ∗(cid:107)µt+1

i) in the following Lemma 4.

) − DKL(µ∗(cid:107)µt

i

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

Lemma 4 Consider the KL Divergence-based average between the local dual variable iterates deﬁned in (13) and
the optimal occupancy measure µ∗ deﬁned by (8) satisﬁes the following approximate decrement:

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) −

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) ≤

1
n

+

n
(cid:88)

i=1

1
2n

(cid:88)

(cid:88)

(cid:0)

i(s, a) − µ∗(s, a)(cid:1) ∆t+1
(cid:101)µt

i

(s, a)

s∈S
n
(cid:88)

a∈A

(cid:88)

(cid:88)

i=1

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2.

(23)

The proof of Lemma 4 is provided in Appendix C. In comparison tot the analysis for single agent settings in
(Wang, 2020b), the right hand side of (23) now depends upon the consensus variable (cid:101)µt
i. Next, we establish
the conditional expectation of update direction of the local occupancy ∆t+1
i with respect to a diﬀerence of the
averaged occupancy measure with respect to the optimal admits an expression in terms of the constraint violation
of (5).

Lemma 5 For arbitrary s ∈ S and a ∈ A, we have

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

((cid:101)µt

i(s, a) − µ∗(s, a))E[∆t+1

i

(s, a) | Ft] =

β
n

n
(cid:88)

(cid:88)

(µt(a) − µ∗(a))T [(Pa − I)vt

i + ra].

(24)

i=1

a∈A

The proof of Lemma 5 is provided in Appendix D.

Lemma 6 The (cid:101)µt
∆t+1
i

(s, a) satisﬁes the following boundedness condition:

i(s, a) probability-weighted conditional mean-square of the local occupancy measure update

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)E[(∆t+1
(cid:101)µt

i

(s, a))2 | Ft] ≤ 4(4tmix + 1)2β2|S||A| .

(25)

The proof of Lemma 6 is provided in Appendix E.

Lemma 7 Consider the norm diﬀerence between the average diﬀerential value vector update deﬁned by (14) and
the global-optimal v∗ as the minimizer of (cid:80)

i Li(µ, v) in (8). This diﬀerence satisﬁes the following recursion:

E

(cid:104)(cid:13)
(cid:13)vt+1 − v∗(cid:13)
2
(cid:13)

(cid:105)

| Ft

≤ (cid:13)

(cid:13)vt − v∗(cid:13)
2
(cid:13)

+

2α
n

(cid:0)vt − v∗(cid:1)T

(cid:32)

1
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:33)

(I − Pa)T

(cid:101)µt
i(a)

+

1
n

O(α2)

(26)

The proof of Lemma 7 is provided in Appendix F. The statement of Lemma 7 establishes the norm distance of
average dual variable vt+1 from the optimal dual variable v∗.

The proof of Lemma 4-7 is used to proof the statement of Lemma 1. Begin by considering the expectation of (23)
conditional on ﬁltration Ft. Then, employ Lemma 5 and Lemma 6 for the second two terms on the right-hand
side:

1
n

n
(cid:88)

i=1

E (cid:2)DKL(µ∗(cid:107)µt+1

i

) | Ft

(cid:3) ≤

1
n

+

≤

1
n

n
(cid:88)

i=1

1
2n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) +

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

(cid:0)

i(s, a) − µ∗(s, a)(cid:1) E (cid:2)∆t+1
(cid:101)µt

i

(s, a) | Ft

(cid:3)

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

i(s, a)E (cid:2)(∆t+1
(cid:101)µt

i

(s, a))2 | Ft

(cid:3)

DKL(µ∗(cid:107)µt

i) +

β
n

n
(cid:88)

(cid:88)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra)]

i=1

a∈A

+ 4(4tmix + 1)2β2|S||A|.

(27)

Now, consider the expression for the value function decrease in Lemma 7. Multiply both sides of this inequality
(where |S| denotes the cardinality of the state space, and tmix is the mixing time of the MDP deﬁned
by

1
2|S|t2

mix

Convergence Rates of Average-Reward MARL

in Assumption 4) and add to (85) to obtain

(cid:34)

E

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) +

1
2|S|t2

mix

(cid:13)vt+1 − v∗(cid:13)
(cid:13)
2
(cid:13)

(cid:35)

| Ft

≤

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) +

1
2|S|t2

mix

(cid:13)vt − v∗(cid:13)
(cid:13)
2
(cid:13)

+

β
n

+

n
(cid:88)

(cid:88)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra)]

i=1

a∈A

2α
n

1
2|S|t2

mix

(cid:0)vt −v∗(cid:1)T

(cid:32)
1
n

n
(cid:88)

(cid:88)

i=1

a∈A

(I −Pa)T

(cid:33)
(cid:101)µt
i(a)

+

1
2|S|t2

mix

O(α2) + 4(4tmix + 1)2β2|S||A|.

(28)

Notice that the last two terms are respectively of order α2 and β2, with a contrast in the order of the dependence
on problem-dependent constants |S|, |A|, and tmix. These terms may be judiciously balanced via selecting
α = |S|t2

mixβ, which yields

(cid:34)

E

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) +

1
2|S|t2

mix

(cid:13)
(cid:13)vt+1 − v∗(cid:13)
2
(cid:13)

(cid:35)

| Ft

≤

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) +

1
2|S|t2

mix

(cid:13)
(cid:13)vt − v∗(cid:13)
2
(cid:13)

+

β
n

+

n
(cid:88)

(cid:88)

i=1

a∈A

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra)]

(cid:32)

(cid:0)vt − v∗(cid:1)T

β
1
n
n
+ β2 (cid:101)O (cid:0)|S||A|t2

(cid:1) .

mix

n
(cid:88)

(cid:88)

i=1

a∈A

(I − Pa)T

(cid:101)µt
i(a)

(cid:33)

(29)

where as deﬁned earlier, (cid:101)O denotes order-dependence that ignores polylog factors. After grouping the third and
fourth terms on the right-hand side of the previous expression, we can write

E [Et+1 | Ft] ≤Et +

β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:104)
((cid:101)µt

i(a) − µ∗(a))[(Pa − I)vt

i + ra] + (cid:0)vt − v∗(cid:1)T (cid:0)(I − Pa)T

+ β2 (cid:101)O (cid:0)|S||A|t2

mix

(cid:1) .

i(a)(cid:1)(cid:105)
(cid:101)µt

(30)

i=1 DKL(µ∗(cid:107)µt+1
where we have used the deﬁnition Et := 1
n
term on the right-hand side of the previous expression (30) as

i

(cid:80)n

) +

1
2|S|t2

mix

(cid:13)vt − v∗(cid:13)
(cid:13)
2
(cid:13)

. Now, let us study second

β
n

n
(cid:88)

(cid:88)

(cid:104)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra] + (cid:0)vt − v∗(cid:1)T (cid:0)(I − Pa)T

i(a)(cid:1)(cid:105)
(cid:101)µt

a∈A
n
(cid:88)

i=1

=

β
n

i=1

a∈A

(cid:88)

(cid:104)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra] + (cid:0)vt − v∗(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

(31)

where we have used the dual feasibility of µ∗, which means that (cid:80)
a )µ∗(a) = 0. Now, add and subtract
i inside the parenthesis associating the value function diﬀerence between vt and v∗ in the preceding expression
vt

a(I − P T

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

to obtain

β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:104)
((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra] + (cid:0)vt

i − v∗ + vt − vt
i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

=

β
n

=

β
n

n
(cid:88)

(cid:88)

(cid:104)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra] + (cid:0)vt

i − v∗(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

i=1

+

β
n

a∈A
n
(cid:88)

(cid:88)

(cid:104)(cid:0)vt − vt

i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

i=1

a∈A

n
(cid:88)

(cid:88)

(cid:2)((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)v∗ + ra](cid:3)

i=1

+

β
n

a∈A
n
(cid:88)

(cid:88)

(cid:104)(cid:0)vt − vt

i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

i=1

a∈A

(32)

where the ﬁrst equality uses the deﬁnition of the transpose, and the later regroups terms. We proceed to analyze
both terms on the right-hand side of the preceding expression step by step. The former one is analogous to
(Wang, 2020a)[Lemma A.6], but the later term is a novel instantiation of the consensus error due to decentralized
computations. Unsurprisingly, Lemmas 1 is useful to address it.

First, we focus on the ﬁrst term on the right-hand side of (32). To do so, we exploit the linear complementarity
of (v∗, µ∗) in (6), i.e.,

µ∗(s, a)[(Pa − I)v∗ + ra − λ∗e]s = 0 for all s ∈ S ,

where λ∗ is deﬁned as the optimal objective following (2), e is the vector in R|S| whose entries are all 1. This
fact can be substituted in place of µ∗(s, a)(Pa − I)v∗ + ra in ﬁrst term on the right-hand side of (32) as

β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:2)((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)v∗ + ra](cid:3) =

β
n

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

a∈A

(cid:88)

i(a)T [(Pa − I)v∗ + ra](cid:3) −
(cid:2)
(cid:101)µt

λ∗µ∗(a)e

(cid:88)

a∈A

(cid:104)(cid:0)µt(a) − µt(a) + (cid:101)µt

i(a)(cid:1)T

(cid:105)
[(Pa − I)v∗ + ra]

− λ∗

=

β
n

i=1
a∈A
= −β (λ∗ +

(cid:2)µt(a)T [(I − Pa)v∗ + ra](cid:3))

(cid:88)

a∈A

(cid:124)

−

β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:123)(cid:122)
:=Dt

(cid:125)

(cid:104)(cid:0)µt(a) − (cid:101)µt

i(a)(cid:1)T

(cid:105)
[(Pa − I)v∗ + ra]

(33)

Now, let us substitute the deﬁnition of Dt in (33) into the right-hand side of (30) together with the decomposition
of the complimentary slackness in (32):

E [Et+1 | Ft] ≤Et − βDt + β2 (cid:101)O (cid:0)n|S||A|t2

mix

(cid:1) +

n
(cid:88)

(cid:88)

i=1

a∈A

β
n
(cid:124)

(cid:104)(cid:0)vt − vt

i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

(cid:123)(cid:122)
I

(cid:125)

n
(cid:88)

(cid:88)

(cid:104)(cid:0)

i=1

a∈A

+

β
n
(cid:124)

i(a) − µt(a)(cid:1)T
(cid:101)µt

(cid:105)
[(Pa − I)v∗ + ra]

.

(cid:123)(cid:122)
II

(cid:125)

which is as stated in Lemma 1.

(34)

(cid:3).

Convergence Rates of Average-Reward MARL

B Proof of Lemma 3

B.1 Proof of Lemma 3 Statement (i)

Begin by employing the deﬁnition of the the auxiliary sequences {pt
we analyze qt

i}t≥0 and {qt
i and show its magnitude is upper-bounded. In particular, note that

i }t≥0 for all agents i ∈ V. First,

(cid:107)qt

i (cid:107) = (cid:107)ΠV [(cid:101)vt
≤ (cid:107)(cid:101)vt

i + dt+1
] − (cid:101)vt
i (cid:107)
i
i − (cid:101)vt
i (cid:107)

i + dt+1

(35)

by the non-expansive property of the projection. Then, we may cancel out (cid:101)vt
the mean-square magnitude of dt+1

to conclude:

i

i and employ Lemma 6 regarding

Now, ﬁrst we establish the bound on E[(cid:107)dt+1
obtain by direct evaluation of its mean conditional on the ﬁltration Ft:

i

(cid:107)2 | Ft] as follows. For the analysis of the magnitude of dt+1

i

E[(cid:107)qt

i (cid:107) | Ft] ≤ E[(cid:107)dt+1

i

(cid:107) | Ft]≤ O(α).

(36)

, we

E[dt+1
i

| Ft] = α

(I − Pa)T µt

i(a)

(cid:88)

a∈A

from the deﬁnition of the probability of the update direction in (12) and the local Lagrangian in (8). Now,
observe that (cid:101)µt
i may not satisfy the ergodicity constraint of Assumption 3 because weighted averaging [cf. (10)]
does not preserve feasibility, i.e., it may be outside dual feasible set U in (7). However, since the weighting
matrices are doubly stochastic (Assumption 2), (cid:101)µt
i still belongs to the probability simplex, meaning its sum is
unit. Together with the nonnegativity of the probability, means that the the inner product on the right-hand
side of the preceding expression sums up to less than one, i.e.,

Next, in order to obtain the bound on E[(cid:107)dt+1

i

(cid:107)2 | Ft], let us consider the expression as

(cid:88)

α

a∈A

(I − Pa)T µt

i(a) ≤ O(α)

E[(cid:107)dt+1
i

(cid:107)2 | Ft] =E



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

α(es − es(cid:48))



µt
i(s, a)
(cid:101)µt
i(s, a)
≤α2E (cid:2)(cid:107)es − es(cid:48)(cid:107)2 | Ft
≤4α2,

(cid:3)



| Ft



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(37)

(38)

i

(cid:107)2 | Ft] ≤ O(α2). From the Jensen’s inequality, we known that f (E(x)) ≤ Ef ((x))
which establishes that E[(cid:107)dt+1
for a convex function f (x). Now, by selecting convex function f (x) = x2, we can write (cid:0)E[(cid:107)dt+1
≤
E[(cid:107)dt+1
(cid:107) | Ft] ≤ O(α). Hence, we get (36). which we may
i
i ∈ R|S|
stack over all i ∈ V to write E[(cid:107)qt(cid:107) | Ft] ≤ O(
[cf. (16)] over all i. For the moment, consider that the value vector is scalar for each agent. Then, using this
deﬁnition of qt

(cid:107)2 | Ft] ≤ O(α2), which further implies that E[(cid:107)dt+1

i , we may write the the value vector vt = [[vt

n]T ] ∈ Rn|S| stacking vt

n]T ] ∈ Rn|S| stacks qt

nα), where qt = [[qt

i across all i as

1]T ; · · · ; [vt

1]T ; · · · ; [qt

(cid:107) | Ft](cid:1)2

√

i

i

vt+1 = (cid:101)vt + qt

= (W t ⊗ I|S|)vt + qt

(39)

where ⊗Dt denotes the Kronecker product and I|S| ∈ R|S|×|S| is the |S| × |S| identity matrix. With these
observations, shift to deﬁning the respective sequences that track the distance to the mean [cf. (15)]

i = vt
δt

i − vt .

(40)

We ﬁrst analyze the evolution of δt
may be deﬁned in terms of the uniform matrix of weights all set to (1/n), i.e.,
(cid:18) 1
n

i stacked over all i deﬁned as δt = [[δt

(cid:20)
In|S| −

eeT ⊗ I|S|

δt = vt −

(cid:18) 1
n

vt =

eeT ⊗ I|S|

(cid:19)

1]T ; · · · ; [δt

n]T ] ∈ Rn|S|. Observe that δt

(cid:19)(cid:21)

vt

(41)

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

where e is the vector of all 1’s in Rn, meaning that 1
n eeT is an n × n matrix whose entries are all equal to 1/n.
Further, I|S| ∈ R|S|×|S| is the identity matrix, and similarly for In|S| in dimension n|S| × n|S| . Observe that δt
satisﬁes the recursion:
(cid:20)
In|S| −

eeT ⊗ I|S|

δt+1 =

vt+1

(cid:19)(cid:21)

(cid:18) 1
n
(cid:18) 1
n

(cid:20)
In|S| −

=

(cid:19)(cid:21)

eeT ⊗ I|S|

(cid:2)(W t ⊗ I|S|)vt + qt(cid:3)

= (W t ⊗ I|S|)vt −

(cid:20)(cid:18) 1
n

(cid:19)(cid:21)

eeT ⊗ I|S|

(W t ⊗ I|S|)vt +

(cid:20)
In|S| −

(cid:18) 1
n

eeT ⊗ I|S|

(cid:19)(cid:21)

qt

(42)

where we have used the recursion for the stacked value vector in (39), and distributed terms. Now, use the fact
that the matrix W t is doubly stochastic and symmetric to note that (cid:2)I − 1
n eeT (cid:3), which is
unaﬀected by the Kronecker products with identity, to group terms as

n eeT (cid:3) W t = W t (cid:2)I − 1

δt+1 = (W t ⊗ I|S|)

(cid:20)
In|S| −

(cid:18) 1
n

eeT ⊗ I|S|

(cid:19)(cid:21)

vt +

(cid:20)
In|S| −

eeT ⊗ I|S|

(cid:19)(cid:21)

qt

(cid:18) 1
n

= (W t ⊗ I|S|)δt +

(cid:20)
In|S| −

(cid:18) 1
n

eeT ⊗ I|S|

(cid:19)(cid:21)

qt

(43)

Next, recursively apply this logic backwards in time to obtain:

δt+1 = (cid:2)Φ(t, s) ⊗ I|S|

(cid:3) δs +

t−1
(cid:88)

l=s

(cid:0)Φ(t, l + 1) ⊗ I|S|

(cid:1)

(cid:20)
In|S| −

1
n

eeT ⊗ I|S|

(cid:21)

ql +

(cid:20)
In|S| −

eeT ⊗ I|S|

(cid:21)

qt

1
n

(44)

n eeT ) ⊗ I|S|]δt = 0 and [( 1

Now, note that [( 1
n eeT ) ⊗ I|S|][In|S| − ( 1
n eeT ) ⊗ I|S|]qt = 0 , where 0 is the vector of all
0’s in Rn|S| and e ∈ Rn is the vector of all 1’s as deﬁned earlier. These identities may be seen via the deﬁnitions
of δt and qt, respectively, in (41) and (16). Substitute these two identities in the ﬁrst and second terms on the
right-hand side of the previous expression to obtain

(cid:20)(cid:18)

δt+1 =

Φ(t, s) −

(cid:19)

1
n

eeT

(cid:21)

δs +

t−1
(cid:88)

(cid:18)(cid:18)

⊗ I|S|

Φ(t, l + 1) −

l=s

(cid:19)

(eeT )

1
n

(cid:19) (cid:20)

⊗ I|S|

In|S| −

eeT ⊗ I|S|

(cid:21)

ql

1
n

(cid:20)
In|S| −

+

1
n

eeT ⊗ I|S|

(cid:21)

qt

(45)

where Φ(t, s) is deﬁned in (20) as the product of weight matrices. Now, compute the norm of both sides, and
apply the triangle inequality together with Lemma 1 to upper-bound the diﬀerence of product matrices to the
n × n uniform weight matrix 1

n eeT as

(cid:107)δt+1(cid:107) ≤

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Φ(t, 0) −

(cid:19)

1
n

eeT

⊗ I|S|

(cid:18)

(cid:107)δ0(cid:107) +

t−1
(cid:88)

l=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Φ(t, l + 1) −

(cid:19)

(eeT )

1
n

⊗ I|S|

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
In|S| −
(cid:13)
(cid:13)

1
n

(eeT ) ⊗ I|S|

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:107)ql(cid:107)

+

(cid:13)
(cid:13)
In|S| −
(cid:13)
(cid:13)

1
n

eeT ⊗ I|S|

(cid:107)qt(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤ Γρt(cid:107)δ0(cid:107) +

t−1
(cid:88)

l=0

Γρt−(l+1)(cid:107)ql(cid:107) + (cid:107)qt(cid:107)

which, after computing the conditional expectation of both sides and applying (55) yields

E (cid:2)(cid:107)δt+1(cid:107) | Ft

(cid:3) ≤ Γρt(cid:107)δ0(cid:107) + O(

√

(cid:34)

nα)

1 +

(cid:35)

Γρt−(l+1)

t−1
(cid:88)

l=0

(46)

(47)

Now, under initialization vt
fact that 0 < ρ < 1. In particular,(cid:80)t−1
conclude Lemma 3.

i = 0 for all i, we have δ0 = 0. Now, we evaluate the ﬁnite geometric sum via the
u=0 ρu = (1 − ρt−1)/(1 − ρ). Together with (47), we may
(cid:3)

l=0 ρt−(l+1) = (cid:80)t−1

Convergence Rates of Average-Reward MARL

B.2 Proof of Lemma 3 Statement (ii)

Begin by employing the deﬁnition of the the auxiliary sequences {˜pt
we analyze (cid:101)qt

i}t≥0 and {(cid:101)qt
i and show its magnitude is upper-bounded. In particular, note that

i }t≥0 for all agents i ∈ V. First,

i = µt+1
(cid:101)qt

i − (cid:101)µt
i,

(48)

which would then imply that µt+1
i . In a key departure from the standard analysis of the consensus
error in the proof of statement (i) of Lemma 3, we reﬁne the discrepancy to instead be in terms of KL divergence
for the occupancy measure. Speciﬁcally, consider the term (cid:107)µt+1
i(cid:107)1. Via Pinsker’s inequality, we can write

i = (cid:101)µt

i + (cid:101)qt

(cid:107)µt+1

i − (cid:101)µt

i(cid:107)2

1 = (cid:107)(cid:101)µt

i − µt+1
i

).

(49)

i − (cid:101)µt
1 ≤ 2DKL((cid:101)µt
(cid:107)2

i||µt+1
i

Since we have (cid:101)µt
DKL((cid:101)µt

i||µt+ 1

i

2

) where µt+ 1

2

i

i as the weighted average of µt

i ∈ U, which implies that (cid:101)µt

i ∈ U. Therefore, DKL((cid:101)µt

i||µt+1
i

) ≤

/∈ U but it is a valid distribution. Hence, we could further write (49) as

(cid:107)µt+1

i − (cid:101)µt

i(cid:107)2

1 = (cid:107)(cid:101)µt

i − µt+1
i

1 ≤2DKL((cid:101)µt
(cid:107)2
(cid:88)
(cid:88)

=2

s∈S

a∈A

(cid:88)

(cid:88)

s∈S

a∈A

=2

i||µt+ 1

2

i

)

(cid:101)µt(s, a) log

(cid:101)µt(s, a) log

(cid:33)

(cid:32)

(cid:18)

2

i

(cid:101)µt(s, a)
µt+ 1
(s, a)
Zi
exp(∆t+1
(cid:101)µt(s, a) (cid:0)∆t+1

i

i

(cid:19)

(s, a))

Note the bound on log(Zi) from (72) to obtain

=2 log(Zi) − 2

(cid:88)

(cid:88)

s∈S

a∈A

(s, a)(cid:1) .

(50)

(cid:107)µt+1

i − (cid:101)µt

i(cid:107)2

1 ≤ 2

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)∆t+1
(cid:101)µt

i

(s, a) +

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2 − 2

(cid:88)

(cid:88)

s∈S

a∈A

(cid:101)µt(s, a) (cid:0)∆t+1

i

(s, a)(cid:1)

(cid:88)

(cid:88)

=

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2.

From the statement of Lemma 5, we can write

E[(cid:107)µt+1

i − (cid:101)µt

i(cid:107)2

1 | Ft] =

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)E[(∆t+1
(cid:101)µt

i

(s, a))2 | Ft]

From Jensen’s inequality, we note that

≤4(4tmix + 1)2β2|S||A|.

(cid:0)E[(cid:107)µt+1

i − (cid:101)µt

i(cid:107)1 | Ft](cid:1)2

≤E[(cid:107)µt+1
i − (cid:101)µt
≤4(4tmix + 1)2β2|S||A|.

1 | Ft]

i(cid:107)2

Taking square root on both sides, we get

E[(cid:107)µt+1

i − (cid:101)µt

i(cid:107)1 | Ft] ≤2(4tmix + 1)β(cid:112)|S||A|.

(51)

(52)

(53)

(54)

In order to proceed in a similar manner to Lemma 3 Statement (i) for consensus error proof for µ, we need to
bound

E[(cid:107)(cid:101)qt

i (cid:107) | Ft] =E (cid:2)(cid:107)(cid:101)µt
≤O(β),

i − µt+1
i

(cid:107)1 | Ft

(cid:3)

(55)

√

where the second inequality holds from (54). Note that we may stack (cid:101)qt
O(

i over all i ∈ V to write E[(cid:107)(cid:101)qt(cid:107) | Ft] ≤
i ∈ R|S||A(cid:55)| [cf. (16)] over all i. For the moment,

n]T ] ∈ Rn|S||A| stacks (cid:101)qt

nβ), where (cid:101)qt = [[(cid:101)qt

1]T ; · · · ; [(cid:101)qt

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

consider that the value vector is scalar for each agent. Then, using this deﬁnition of (cid:101)qt
distribution vector µt = [[µt

n]T ] ∈ Rn|S||A| stacking µt

i across all i as

1]T ; · · · ; [µt

i , we may write the the

µt+1 = (cid:101)µt + (cid:101)qt

= (W t ⊗ I|S||A|)µt + (cid:101)qt,

(56)

where ⊗ denotes the Kronecker product and I|S||A| ∈ R|S||A|×|S||A| is the |S||A| × |S||A| identity matrix. With
these observations, shift to deﬁning the respective sequences that track the distance to the mean [cf. (15)]

¯δt
i = µt

i − µt .

(57)

We ﬁrst analyze the evolution of ¯δt
the similar steps from (39) to (47), we can write

i stacked over all i deﬁned as ¯δ

t

= [[δt

1]T ; · · · ; [δt

n]T ] ∈ Rn|A|. Next, following

(cid:104)
(cid:107)¯δ

t+1

E

(cid:107) | Ft

(cid:105)

0
≤ Γρt(cid:107)¯δ

(cid:107) + (2(4tmix + 1)β(cid:112)|S||A|

√

(cid:34)

n)

1 +

t−1
(cid:88)

l=0

(cid:35)

Γρt−(l+1)

(58)

Now, under initialization u0
fact that 0 < ρ < 1. In particular,(cid:80)t−1
conclude Lemma 3 Statement (ii).

i = ζ for all i, we have ¯δ

0

l=0 ρt−(l+1) = (cid:80)t−1

= 0. Now, we evaluate the ﬁnite geometric sum via the
u=0 ρu = (1 − ρt−1)/(1 − ρ). Together with (58), we may
(cid:3)

C Proof of Lemma 4

Begin by expanding the ith term on the left-hand side of (23) using the deﬁnition of KL divergence:

DKL(µ∗(cid:107)µt+1

i

) − DKL(µ∗(cid:107)µt

i) ≤ DKL(µ∗(cid:107)µt+ 1

2

i

) − DKL(µ∗(cid:107)µt

i),

(59)

where the inequality holds due to the fact that µt+1
after expanding the deﬁnition of KL divergence, we obtain

i

is the projected version of µt+ 1

2

i

onto the space U. Next,

DKL(µ∗(cid:107)µt+1

i

) − DKL(µ∗(cid:107)µt

i) ≤

(cid:88)

(cid:88)

s∈S

a∈A

µ∗(s, a) log

(cid:88)

(cid:88)

=

µ∗(s, a) log

(cid:32)

(cid:32)

(cid:33)

(cid:88)

(cid:88)

−

µ∗(s, a) log

s∈S

a∈A

(cid:19)

(cid:18) µ∗(s, a)
µt
i(s, a)

(cid:33)

.

(60)

2

(s, a)

µ∗(s, a)
µt+ 1
i
µt
µt+ 1

i(s, a)
(s, a)

2

i

Add the term (cid:80)

s∈S

(cid:80)

a∈A µ∗(s, a) log

s∈S

a∈A

(cid:16)

(cid:17)

(cid:101)µt(s,a)
(cid:101)µt(s,a)

in (60) to obtain

DKL(µ∗(cid:107)µt+1

i

) − DKL(µ∗(cid:107)µt

i) ≤

(cid:32)

µ∗(s, a) log

(cid:88)

(cid:88)

s∈S

a∈A

Take the average over i to obtain

(cid:101)µt(s, a)
µt+ 1
(s, a)

2

i

(cid:33)

(cid:88)

(cid:88)

+

µ∗(s, a) log

s∈S

a∈A

(cid:18) µt

i(s, a)
(cid:101)µt(s, a)

(cid:19)

.

(61)

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) −

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) ≤

1
n

+

n
(cid:88)

(cid:88)

(cid:88)

(cid:32)

µ∗(s, a) log

i=1

a∈A

s∈S
n
(cid:88)

(cid:88)

(cid:88)

µ∗(s, a) log

1
n
(cid:124)

i=1

s∈S

a∈A

(cid:123)(cid:122)
T0

Let us consider the second term on the right hand side of (62) as follows:

T0 =

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log (cid:0)µt

i(s, a)(cid:1) −

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log (cid:0)

(cid:101)µt(s, a)(cid:1) .

2

(cid:101)µt(s, a)
µt+ 1
(s, a)
i
(cid:18) µt

i(s, a)
(cid:101)µt(s, a)

(cid:33)

(cid:19)

.

(cid:125)

(62)

(63)

Convergence Rates of Average-Reward MARL

Substitute the expression for (cid:101)µt(s, a) (cf. (10)) to obtain

T0 =

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log (cid:0)µt

i(s, a)(cid:1) −

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log





n
(cid:88)

j=1

Since − log(·) is convex, via Jensen’s inequality, we obtain



wt

ijµt

j(s, a)

 .

(64)

T0 ≤

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log (cid:0)µt

i(s, a)(cid:1) −

After changing the order of summation, we obtain

T0 ≤

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a) log (cid:0)µt

i(s, a)(cid:1) −

1
n

1
n

After simpliﬁcations, we obtain

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a)

n
(cid:88)

j=1

ij log (cid:0)µt
wt

j(s, a)(cid:1) .

(65)

n
(cid:88)

n
(cid:88)

(cid:88)

(cid:88)

wt
ij

µ∗(s, a) log (cid:0)µt

j(s, a)(cid:1) .

(66)

j=1

i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=1

s∈S

a∈A

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

T0 ≤

1
n

=0.

µ∗(s, a) log (cid:0)µt

i(s, a)(cid:1) −

1
n

n
(cid:88)

(cid:88)

(cid:88)

j=1

s∈S

a∈A

µ∗(s, a) log (cid:0)µt

j(s, a)(cid:1)

Using the upper bound in (67) into (62) to obtain

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) −

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) ≤

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

1
n
(cid:124)

(cid:32)

µ∗(s, a) log

(cid:123)(cid:122)
:=T1

(cid:101)µt(s, a)
µt+ 1
(s, a)

2

i

(67)

(68)

(cid:33)

.

(cid:125)

Let us consider the term T1, which is essentially similar to the quantity that appears in (Wang, 2020a)[Lemma
A.2]. To upper-bound this estimate, make use of the deﬁnition of µt+ 1
in
Algorithm 1 as follows:

i(s,a) exp(∆t+1
(cid:101)µt
(cid:80)
a(cid:48) (cid:101)µt

i(s,a) exp(∆t+1

(s, a) =

(s(cid:48),a(cid:48)))

(s,a))

(cid:80)

s(cid:48)

i

2

i

i

T1 =

=

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:88)

(cid:88)

i=1

s∈S

a∈A

(cid:32)

(cid:18)

µ∗(s, a) log

µ∗(s, a) log

(cid:33)

(cid:101)µt(s, a)
µt+ 1
(s, a)

2

i

(cid:19)

Zi
exp(∆t+1

i

(s, a))

(69)

where we have deﬁned Zi=(cid:80)
(s, a)), and used the fact that a log of a product equals
the sum of logs. Next, use the fact that a log of a ratio is the diﬀerence of the logs in the last term on the
right-hand side of (69) to write:

i(s, a) exp(∆t+1

a∈A (cid:101)µt

(cid:80)

s∈S

i

T1 =

=

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:88)

(cid:88)

s∈S

a∈A

µ∗(s, a) log(Zi) −

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a)∆t+1

i

(s, a)

log(Zi) −

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a)∆t+1

i

(s, a)

(70)

where we have used the fact that µ∗(s, a) is a likelihood whose sum over all s ∈ S, a ∈ A is unit to simplify
the third term on the right-hand side. Next, to analyze Zi, especially its logarithm, it turns out to be useful to
establish that ∆t+1
(s, a) [cf. (11)] is always either negative or zero. To do so, note that vt
mix, 2t∗
mix],
mix−(−2t∗
and rt
mix+1)≤0.

i(s, s(cid:48), a) ∈ [0, 1], and M =4tmix+1 Thus, vt

i (s) ∈ [−2t∗
mix)+1−(4t∗

i(s, s(cid:48), a)−M ≤2t∗

i (s(cid:48))−vt

i (s)+rt

i

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

Then it follows that ∆t+1
deﬁnition of Zi, whose logarithm in (70) is given as:

i

(s, a) ≤ 0 for all s ∈ S and a ∈ A with probability 1. Now, we shift to considering the

log(Zi) = log

≤ log

(cid:32)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:32)

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a) exp(∆t+1
(cid:101)µt

i

(s, a))

(cid:33)

i(s, a)(1 + ∆t+1
(cid:101)µt

i

(s, a) +

(cid:33)

(∆t+1
i

(s, a))2)

1
2

(71)

where we have used the inequality ex ≤ 1 + x + 1
holds for all x ∈ R to the right-hand side as:

2 x2 for x ≤ 0 above. Next, let us apply log(1 + x) ≤ x which

(cid:32)

log Zi ≤ log

1 +

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)∆t+1
(cid:101)µt

i

(s, a) +

1
2

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2

(cid:33)

(cid:88)

(cid:88)

≤

s∈S

a∈A

i(s, a)∆t+1
(cid:101)µt

i

(s, a) +

1
2

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2.

(72)

Via the analysis of log(Zi) in (72), T1 deﬁned in (70), , simpliﬁes to

T1 ≤

=

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:88)

(cid:88)

i=1

s∈S

a∈A

i(s, a)∆t+1
(cid:101)µt

i

(s, a) +

1
2n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2 −

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

µ∗(s, a)∆t+1

i

(s, a)

(cid:0)

i(s, a) − µ∗(s, a)(cid:1) ∆t+1
(cid:101)µt

i

(s, a) +

1
2n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2.

(73)

Let us utilize this bound into the right hand side of (68) to conclude

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt+1

i

) −

1
n

n
(cid:88)

i=1

DKL(µ∗(cid:107)µt

i) ≤

1
n

n
(cid:88)

(cid:88)

(cid:88)

(cid:0)

i(s, a) − µ∗(s, a)(cid:1) ∆t+1
(cid:101)µt

i

(s, a)

i=1

a∈A

s∈S
n
(cid:88)

(cid:88)

i=1

s∈S

a∈A

(cid:88)

i(s, a)(∆t+1
(cid:101)µt

i

(s, a))2.

+

1
2

as stated in Lemma 4.

D Proof of Lemma 5

[cf. (11)] in conditional expectation (where we use

Consider the update direction of the local occupancy ∆t+1
the fact that the update occurs with probability (cid:101)µt
i (s(cid:48)) − vt
vt
(cid:101)µt
i(s, a)

(s, a)|Ft] = β (cid:101)µt

E[∆t+1
i

pss(cid:48)(a)

i(s, a)

(cid:88)

i

i(s, a)), stated as:

i (s)

+ β (cid:101)µt

i(s, a)

(cid:88)

pss(cid:48)(a)

i(s, s(cid:48), a) − M
rt
(cid:101)µt
i(s, a)

s(cid:48)∈S
i − vt
where we have canceled out a factor of (cid:101)µt
exploit the fact that that the sum of the elements of occupancy measures is unit, i.e., (cid:80)
(cid:80)
a∈A(µt

i(s, a). Then compute the quantity in the left-hand side of (24) and
i(s, a) =

i(s, a) = 1 and (cid:80)

i + ra)(s) − M (cid:1)

= β (cid:0)(Pavt

s∈S,a∈A (cid:101)µt

i(a) = (cid:80)

s∈S,a∈A µt

i(a) = 1.

s(cid:48)∈S

a∈A((cid:101)µt

1
n

n
(cid:88)

(cid:88)

(cid:88)

i=1

s∈S

a∈A

((cid:101)µt

i(s, a) − µ∗(s, a))E[∆t+1

i

(s, a) | Ft] =

=

β
n

β
n

n
(cid:88)

(cid:88)

((cid:101)µt

i(a) − µ∗(a))T [(Pa − I)vt

i + ra)]

i=1
n
(cid:88)

a∈A

(cid:88)

(cid:88)

i=1

s∈S

a∈A

((cid:101)µt

i(s, a) − µ∗(s, a))[(Pavt

i − vt

i + ra)(s) − M ].

The result is (24).

(cid:3)

(74)

(cid:3)

(75)

(76)

Convergence Rates of Average-Reward MARL

E Proof of Lemma 6

Consider the expression on the left-hand side of (25). Use the deﬁnition of ∆t+1
fact that it occurs with probability (cid:101)µt
(cid:88)

i(s, a) to write:

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:16)

i

i(s, a)E[(∆t+1
(cid:101)µt

i

(s, a))2 | Ft] =

i(s, a)(cid:101)µt
(cid:101)µt

i(s, a)

pss(cid:48)(a)

β

s∈S

a∈A

s∈S

a∈A

s(cid:48)∈S

(s, a) [cf. (11)] as well as the

i(s(cid:48)) − ht
ht

i(s, s(cid:48), a) − M

i(s) + rt
(cid:101)µt
i(s, a)

(cid:17)2

(77)

Then, we may group the sum together, and cancel out a factor of (cid:101)µt

(cid:88)

(cid:88)

s∈S

a∈A

i(s, a)E[(∆t+1
(cid:101)µt

i

(s, a))2 | Ft] =

≤

(cid:88)

(cid:88)

(cid:88)

s∈S
(cid:88)

a∈A
(cid:88)

s(cid:48)∈S
(cid:88)

pss(cid:48)(a)(2β(4tmix + 1))2

i(s, a)2 to write:
i (s(cid:48)) − vt

i (s) + rt

pss(cid:48)(a)(β(vt

i(s, s(cid:48), a) − M ))2

s(cid:48)∈S
= 4(4tmix + 1)2β2|S||A|
where the inequality uses the fact that vt
i(s, s(cid:48), a) ∈ [0, 1], and M =4tmix + 1, and
mix, 2t∗
the last equality evaluates the sum, using the deﬁnition of the transition probability and the cardinality of the
spaces, and yields Lemma 6.

i (s) ∈ [−2t∗

mix], and rt

(78)

a∈A

s∈S

Next we establish a decrement-like property on the local diﬀerential value vector vt
optimal v∗.

i with respect to the locally

(cid:3)

F Proof of Lemma 7

i

n
(cid:88)

This result is a generalization of the proof of (Wang, 2020a)[Lemma 7] that contains additional consensus-error
terms. First, recall the deﬁnition of vt+1
in (14). Begin by considering the norm-diﬀerence of the global average
of this quantity vt [cf. (15)] to the global optimizer v∗ deﬁned by the one that minimizes (cid:80)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i Li(µ, v) in (8):

i + ΠV [(cid:101)vt
(cid:101)vt

i + qt+1
(cid:101)vt
i

vt+1
i − v∗

i + dt+1
i

i + dt+1
i

ΠV [(cid:101)vt

(cid:1) − v∗

(cid:1) − v∗

] − (cid:101)vt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

] − v∗

n
(cid:88)

n
(cid:88)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(79)

1
n

1
n

1
n

1
n

i=1

i=1

i=1

=

=

=

(cid:0)

(cid:0)

i

i=1

i

:= ΠV [(cid:101)vt

where qt+1
making use of the deﬁnition of the consensus round (cid:101)vt
preceding expression to write

i + dt+1
i

] − (cid:101)vt

i and we make use of the non-expansiveness of the projection. Continue by
j in (10) in the right-hand side of the

i = (cid:80)n

j=1 wt

ijvt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

n
(cid:88)

i=1

j=1

wt

ijvt

j +

1
n

n
(cid:88)

i=1

qt+1
i − v∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

=

n
(cid:88)

j=1

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
vt +
(cid:13)
(cid:13)
(cid:13)

(cid:33)

wt
ij

vt
j +

(cid:32) n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

qt+1
i − v∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

qt+1
i − v∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(80)

where we have applied the fact that the mixing matrix W t is doubly stochastic for all t. Now, let us expand the
square on the right-hand side of the previous expression, and make use of the short-hand vt = (1/n) (cid:80)
j to
write:
(cid:13)
(cid:13)
vt +
(cid:13)
(cid:13)
(cid:13)

(cid:13)vt − v∗(cid:13)
2
(cid:13)

(cid:0)vt − v∗(cid:1)T

qt+1
i − v∗

(I − Pa)T

(cid:13)
(cid:13)qt+1
i

(cid:101)µt
i(a)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2α
n

1
n2

≤ (cid:13)

n
(cid:88)

n
(cid:88)

n
(cid:88)

j vt

(81)

(cid:13)
2
(cid:13)

(cid:88)

1
n

1
n

(cid:33)

(cid:32)

+

+

i=1

a∈A

i=1

i=1

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

Let us consider the term now (cid:13)

(cid:13)qt+1
i

(cid:13)
(cid:13) and try to develop the upper bound

(cid:13)
(cid:13)qt+1
i

(cid:13)
2
(cid:13)

= (cid:13)
≤ (cid:13)

(cid:13)ΠV [(cid:101)vt
(cid:13)[(cid:101)vt

i + dt+1
i

i + dt+1
i

] − (cid:101)vt

i

i

] − (cid:101)vt
(cid:13)
2
(cid:13)

(cid:13)
2
(cid:13)
= (cid:13)

(cid:13)dt+1
i

(cid:13)
2
(cid:13)

(82)

(83)

Now, compute the expectation conditional on ﬁltration Ft of the previous expression and apply (25), we get
(cid:104)(cid:13)
(cid:13)dt+1
i

(cid:104)(cid:13)
(cid:13)qt+1
i

= O(α2).

≤E

| Ft

| Ft

(cid:13)
2
(cid:13)

2(cid:105)

(cid:13)
(cid:13)

E

(cid:105)

(84)

Next, using the deﬁnition of the average vector (15) to write:

E

(cid:104)(cid:13)
(cid:13)vt+1 − v∗(cid:13)
2
(cid:13)

(cid:105)

| Ft

≤ (cid:13)

(cid:13)vt − v∗(cid:13)
2
(cid:13)

+

2α
n

(cid:0)vt − v∗(cid:1)T

(cid:32)

1
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:33)

(I − Pa)T

(cid:101)µt
i(a)

+

1
n

O(α2)

(85)

which is as stated in Lemma 7.

G Proof of Theorem 1

Let us start with the statement of Lemma 1,

E [Et+1 | Ft] ≤Et − βDt + β2 (cid:101)O (cid:0)|S||A|t2

mix

(cid:1) +

n
(cid:88)

(cid:88)

i=1

a∈A

β
n
(cid:124)

(cid:3)

(cid:104)(cid:0)vt − vt

i

(cid:1)T (cid:0)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:1)(cid:105)

(cid:123)(cid:122)
I

(cid:125)

(86)

n
(cid:88)

(cid:88)

i=1

a∈A

+

β
n
(cid:124)

(cid:2)[(Pa − I)v∗ + ra]T (cid:0)

i(a) − µt(a)(cid:1)(cid:3)
(cid:101)µt

.

(cid:123)(cid:122)
II

(cid:125)

Let us consider the terms I and II and try to derive the upper bounds on them separately. Let us write I as

I ≤ |I| ≤

≤

β
n

β
n

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

a∈A

(cid:88)

i=1

a∈A

(cid:2)(cid:107)vt − vt

i (cid:107)∞ · (cid:107)(I − Pa)T ((cid:101)µt

i(a) − µ∗(a))(cid:107)1

(cid:3)

(cid:2)(cid:107)vt − vt

i (cid:107)∞ · (cid:107)I − P T

a (cid:107)1 · (cid:107)((cid:101)µt

i(a) − µ∗(a))(cid:107)1

(cid:3) ,

(87)

which follows from the inequality that (cid:107)(I − Pa)T ((cid:101)µt
(cid:107)I − P T

a (cid:107)1 and (cid:107)I(cid:107)1 = 1 and (cid:107)P T

a (cid:107)1 ≤ (cid:107)I(cid:107)1 + (cid:107)P T

i(a) − µ∗(a))(cid:107)1 ≤ (cid:107)I − P T
(cid:80)|S|

a (cid:107)1 · (cid:107)((cid:101)µt

a (cid:107)1 = maxi

j=1 |Pa(i, j)| = 1, hence we could write

i(a) − µ∗(a))(cid:107)1. Note that

I ≤

2β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:2)(cid:107)vt − vt

i (cid:107)∞ · (cid:107)(cid:101)µt

i(a) − µ∗(a)(cid:107)1

(cid:3) .

(88)

Next note that we have (cid:107)(cid:101)µt
distributions, we have (cid:107)(cid:101)µt

i(a)−µ∗(a)(cid:107)1 ≤ (cid:107)(cid:101)µt
i(a) − µ∗(a)(cid:107)1 ≤ 2. Hence, we could write

i(a)(cid:107)1 +(cid:107)µ∗(a)(cid:107)1 and since µt

i(a) and µ∗(a) are marginal probability

I ≤

2β
n

n
(cid:88)

i=1

(cid:107)vt − vt

i (cid:107)1

(cid:88)

a∈A

(cid:2)(cid:107)(cid:101)µt

i(a)(cid:107)∞ + (cid:107)µ∗(a)(cid:107)∞

(cid:3)

n
(cid:88)

i=1
n
(cid:88)

(cid:107)vt − vt

i (cid:107)1

(cid:107)δt

i (cid:107)1

≤(4β|A|) ·

1
n

1
n

=(4β|A|) ·

i=1
≤(4β|A|) · (cid:107)δt(cid:107)1.

(89)

Convergence Rates of Average-Reward MARL

From the statement of Lemma 3 and the value of α = |S|t2
E [I | Ft] ≤(4β|A|)E (cid:2)(cid:107)δt(cid:107)1 | Ft

mixβ, it holds that
(cid:3)

≤8β2(t2

mix · |S||A|) · (

√

(cid:20)

n)

1 +

Γ(1 − ρt−1)
1 − ρ

(cid:21)

.

Let us consider the term II term in (86) and using Cauchy-Schwartz inequality, we can write

II ≤ |II| ≤

β
n

n
(cid:88)

(cid:88)

i=1

a∈A

(cid:107)(Pa − I)v∗ + ra(cid:107)∞ · (cid:107)µt(a) − (cid:101)µt

i(a)(cid:107)1

(90)

(91)

Next, from the triangle inequality, we can write (cid:107)(Pa −I)v∗ +ra(cid:107)∞ ≤ (cid:107)(Pa −I)v∗(cid:107)∞ +(cid:107)ra(cid:107)∞. From the deﬁnition
of rewards, we note that (cid:107)ra(cid:107)∞ = 1. Also it holds that (cid:107)(Pa − I)v∗(cid:107)∞ ≤ (cid:107)Pa − I(cid:107)∞ · (cid:107)v∗(cid:107)∞ ≤ 2tmix(cid:107)Pa − I(cid:107)∞
from the deﬁnition of V. Further from the triangle inequality, we have (cid:107)Pa − I(cid:107)∞ ≤ (cid:107)Pa(cid:107)∞ + (cid:107)I(cid:107)∞ = (cid:107)Pa(cid:107)∞ + 1.
j=1 |Pa(i, j)| = 1. This
Next, we use the deﬁnition of matrix ∞ norm (maximum row sum) as (cid:107)Pa(cid:107)∞ = maxi
would imply that (cid:107)Pa − I(cid:107)∞ ≤ 2. Combining all these inequalities, we could write

(cid:80)|S|

(cid:107)(Pa − I)v∗ + ra(cid:107)∞ ≤(cid:107)(Pa − I)v∗(cid:107)∞ + (cid:107)ra(cid:107)∞

≤(cid:107)(Pa − I)v∗(cid:107)∞ + 1
≤(cid:107)Pa − I(cid:107)∞ · (cid:107)v∗(cid:107)∞ + 1
≤2tmix(cid:107)Pa − I(cid:107)∞ + 1
≤4tmix + 1.

Using the upper bound in (92) into (91), we get

(cid:107)µt(a) − (cid:101)µt

i(a)(cid:107)1

(cid:107)µt(a) −

n
(cid:88)

j=1

wt

ijµt

i(a)(cid:107)1

(cid:107)µt(a) − µt

i(a)(cid:107)1

II ≤ |II| ≤

β(4tmix + 1)
n

≤

β(4tmix + 1)
n

≤β(4tmix + 1)

≤β(4tmix + 1)

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

a∈A

(cid:88)

i=1

(cid:88)

a∈A
n
(cid:88)

1
n

i=1

a∈A
n
(cid:88)

1
n

i=1

≤β˜tmix

1
n

n
(cid:88)

i=1

(cid:107)µt − µt

i(cid:107)1,

(cid:107)µt − µt

i(cid:107)1

(92)

(93)

where ˜tmix := 4tmix + 1. where µt and µt
the statement (ii) of Lemma 3 for µ to obtain

i concatenated the values for all a ∈ A. To proceed next, we invoke

E [II | Ft] ≤β˜tmix · (

√

n2(˜tmix)β(cid:112)|S||A|) ·
(cid:20)

√

≤2β2 (cid:16)

(cid:17)
mix · (cid:112)|A||S|
˜t2

· (

n) ·

1 +

(cid:20)

1 +

(cid:21)

Γ(1 − ρt−1)
1 − ρ

Γ(1 − ρt−1)
1 − ρ

(cid:21)

.

(94)

Hence, ﬁnally from (91) and (94) combining with the fact that t2
as

mix ≤ ˜t2

mix, we can write the expression in (86)

E [Et+1 | Ft] ≤Et − βDt + β2 (cid:101)O (cid:0)|S||A|t2

mix

(cid:1)

+ β2˜t2

mix

(cid:16)
(cid:17)
8|S||A| + 2(cid:112)|A||S|

· (

√

(cid:20)

n) ·

1 +

(cid:21)

Γ(1 − ρt−1)
1 − ρ

≤Et − βDt + β2

(cid:18)
(cid:101)O (cid:0)|S||A|t2

mix

(cid:1) + (cid:101)O (cid:0)√

n|S||A|˜t2

mix

(cid:1) · (cid:2)1 +

Γ(1 − ρt−1)
1 − ρ

(cid:19)

(cid:3)

.

(95)

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

After rearrangement and taking summation over t, we get

βDt ≤E0 + β2T

(cid:18)

(cid:101)O (cid:0)|S||A|t2

mix

(cid:1) + (cid:101)O (cid:0)√

T −1
(cid:88)

t=0

where we used that (1 − ρt−1) ≤ 1. Let us deﬁne D(Γ, ρ) := (cid:2) 1+Γ

1−ρ

(cid:19)
(cid:3)

mix

n|S||A|˜t2

(cid:1) · (cid:2) 1 + Γ
1 − ρ
(cid:3) and divide both sides by βT to get

,

1
T

T −1
(cid:88)

t=0

(cid:16)(cid:113)

Dt ≤

E0
βT

(cid:16)

(cid:101)O (cid:0)√

+ β

n|S||A|˜t2

mixD(Γ, ρ)(cid:1)(cid:17)

.

(cid:17)

, we obtain

n|S||A|˜t2

E0
mixD(Γ,ρ)T
(cid:32)

(cid:35)

Dt

≤ (cid:101)O

˜tmix

(cid:114) √

nE0|S||A|D(Γ, ρ)
T

(cid:33)

.

By selecting the optimal β = (cid:101)O

(cid:34)

E

1
T

T −1
(cid:88)

t=0

H Proof of Lemma 2

(96)

(97)

(98)

Let us denote the running average of policy as ˆπ= 1
T
write

τ
|S|
where we deﬁne the auxiliary variable χt := (cid:80)

ξ ˆπ ≤

√

(cid:80)T

t=1 πt and note that

1√
τ |S| e ≤ ξ ˆπ ≤

√
τ
|S| e. Hence we can

e = ξ ˆπ = τ ·

√

1
τ |S|

e ≤ τ χt

(99)

a∈A µt(s, a) and the last inequality holds from the fact that µt ∈ U.
We recall the statement of (Wang, 2020b, Lemma A.7) and repeat here for quick reference. From the statement
of Lemma 8, we have

Lemma 8 (Lemma A.7 (Wang, 2020b)) For a given policy π, the induced stationary distribution ξπ, and
the average reward λπ would satisfy

and we have

λπ = (ξπ)T (cid:88)

diag(πa) ((Pa − I)v∗ + ra)

a∈A

λ∗ − λπ = (ξπ)T (cid:88)

diag(πa) (λ∗ · e + (I − Pa)v∗ − ra)

a∈A

(100)

(101)

Let us ﬁrst present the proof of Lemma 8. Note that since ξπ is the stationary distribution, it holds that
(ξπ)T P π = (ξπ)T . To obtain the ﬁrst result, we start with the equality that λπ = (ξπ)T r and then use the fact
that (ξπ)T (P π − I) = 0 to obtain

λπ =(ξπ)T r

=(ξπ)T (cid:0)(ξπ)T (P π − I)v∗ + r(cid:1)
=(ξπ)T (cid:88)

diag(πa) (cid:0)(ξπ)T (Pa − I)v∗ + ra

(cid:1) ,

a∈A

(102)

which holds due to the fact that (ξπ)T (cid:80)
policy ˆπ, we can write

a∈A diag(πa)e = 1 and implies the statement in (101). Hence, for

λ∗ − λˆπ = (cid:0)ξ ˆπ(cid:1)T (cid:88)

diag(ˆπa) (λ∗ · e + (I − Pa)v∗ − ra)

a∈A

=

1
T

T
(cid:88)

t=1

(cid:0)ξ ˆπ(cid:1)T (cid:88)

a∈A

diag(πt

a) (λ∗ · e + (I − Pa)v∗ − ra)

≤τ

1
T

T
(cid:88)

t=1

(cid:0)χt(cid:1)T (cid:88)

a∈A

diag(πt

a) (λ∗ · e + (I − Pa)v∗ − ra)

(103)

Convergence Rates of Average-Reward MARL

where we have used the fact that ξ ˆπ ≤ τ χt and primal feasibility (λ∗ · e + (I − Pa)v∗ − ra) ≥ 0. Next, from the
deﬁnition of πt

a and χt, we can write
(cid:32)

λ∗ − λˆπ ≤τ

Via Markov inequality, we obtain

1
T

T
(cid:88)

(cid:88)

(cid:88)

t=1

s∈S

a∈A

(cid:0)µt(s, a)(cid:1)T

((v∗ − Pav∗) − ra) (s) + λ∗

(cid:33)

λ∗ − λˆπ ≤

(cid:32)

1
T

3
2

τ

(cid:34)

T
(cid:88)

E

(cid:88)

(cid:0)µt(a)(cid:1)T

(cid:35)
((v∗ − Pav∗) − ra)

+ λ∗

(cid:33)

t=1

a∈A

with probability 2/3 at least. And further from the statement of Lemma 1, we can write

λ∗ − λˆπ ≤

(cid:32)

(cid:114)

τ (cid:101)O

˜tmix

3
2

nE0|S||A|D(Γ, ρ)
T

(cid:33)

.

By selecting T = Ω

(cid:16)

τ 2˜t2

mix

nE0|S||A|D(Γ,ρ)
(cid:15)2

(cid:17)

, we can write that

λ∗ − (cid:15) ≤ λˆπ,

with probability 2/3.

I Proof of Theorem 1

(104)

(105)

(106)

(107)

From Algorithm 2, it is clear that each independent K trials generates an (cid:15)/3 optimal policy. Further, for each
, according to (Wang, 2020b, Lemma A.8), we can write that for a given policy π(k), there exists an algorithm

(k)

Y
such that

(k)

Y

− λπ(k) ∈

(cid:104)
−

(cid:105)

,

(cid:15)
3

,

(cid:15)
3

(108)

(cid:1)(cid:1). We note that step 3 in Algorithm
holds with probability 1− δ
2 takes KL number of steps to execute. Next, since we select (cid:101)π = π(k∗), we need to show that (cid:101)π is indeed near
optimal with probability 1−δ. The number of samples required by the above steps is ˜O (cid:2)N(cid:15)/3 log (cid:0) 1
(cid:1)(cid:3)
time steps.

2K in L number of steps where L = ˜O (cid:0) tmix

(cid:1) + L log (cid:0) 1

log (cid:0) 4K

(cid:15)2

δ

δ

δ

Next, the goal is to prove that the output policy ˆpi of the meta Algorithm 2 is indeed (cid:15) optimal with probability
at least 1 − δ. To achieve that, we need to appropriately select K. Let us deﬁne

K := (cid:8)k ∈ [K] | λπ(k) ≥ λ∗ −

(cid:9),

(cid:15)
3

which denotes the set of all possible successful trials of Algorithm 2. Let us consider an event where K (cid:54)= ∅ and
the policy evaluations errors Y

(cid:3) which implies that we have

(k)

− λπ(k) ∈ (cid:2)− (cid:15)

3 , (cid:15)
3
(cid:15)
3

λπ(k) −

(k)

≤ Y

≤ λπ(k) +

(cid:15)
3

,

for all k and we note that λπ(k) ≥ λ∗ − (cid:15)
value would satisﬁes
value Y

(k)

3 if k ∈ K. For K being non-empty, the output policy with the largest

(k)

Y

≥ λ∗ −

2(cid:15)
3

.

(cid:101)π ≤ λ∗ − (cid:15)) as
(cid:16)

Since the policy evaluation error is upper bounded by (cid:15)/3, this would imply that the policy (cid:101)π is (cid:15) optimal. Let
us consider the probability of event (λ

P (λ

(cid:101)π < λ∗ − (cid:15)) ≤P

{K = ∅} ∪ {∃k : Y

≤P (K = ∅) +

(cid:16)

(k)

∃k : Y

(k)

− λπ(k) /∈
(cid:104)

− λπ(k) /∈

−

,

(cid:15)
3
(cid:105)(cid:17)

(cid:104)

−

(cid:15)
3

,

(cid:15)
3
(cid:15)
3

(cid:105)

(cid:17)
}

.

(109)

Alec Koppel∗†, Amrit Singh Bedi∗$, Bhargav Ganguly‡, Vaneet Aggarwal‡

From the deﬁnition of K, we can write

K
(cid:89)

(cid:16)

P

k=1

λπ(k) < λ∗ −

(cid:17)

(cid:15)
3

+

K
(cid:88)

k=1

(cid:16)

Y

P

(k)

− λπ(k) /∈

(cid:105)(cid:17)

(cid:104)

−

(cid:15)
3

,

(cid:15)
3

P (λ

(cid:101)π < λ∗ − (cid:15)) ≤

(cid:19)K

≤

(cid:18) 1
3

+

δ
2

.

By selecting K = log1/3

(cid:0) δ
2

(cid:1), we can write that

P (λ

(cid:101)π < λ∗ − (cid:15)) ≤ δ.

Hence proved.

J Additional Experiments

(110)

(111)

In this section, we plot the consensus error in Fig. 2 for the primal-dual pair ((cid:101)µt
i ) for the multi-agent
experimental setting explained in Sec. 5. As mentioned in (10), each agent i shares its primal-dual variables
with others according to network graph G, and in turn performs weighted averaging to update its local estimates.
In our experiments, we use the Erdos-Reyni model to generate random network graph G among the agents to
perform consensus. Speciﬁcally, with n = 3 agents we utilized binomial model to generate graphs with edge
probability of p = 0.3.

i, (cid:101)vt

(a) Occupancy measure consensus error.

(b) Value function consensus error.

Figure 2: Consensus error plots for primal-dual pairs ((cid:101)µt
measure. Fig. 2(b) plots consensus error for value function.

i, (cid:101)vt

i ). Fig. 2(a) plots consensus error for occupancy

