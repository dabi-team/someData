2
2
0
2

n
u
J

5
1

]
I

A
.
s
c
[

3
v
6
6
7
9
0
.
7
0
1
2
:
v
i
X
r
a

Learning Heuristics for Template-based CEGIS
of Loop Invariants with Reinforcement Learning

Minchao Wu1, Takeshi Tsukada2 ID , Hiroshi Unno3,4 ID , Taro Sekiyama5 ID ,
and Kohei Suenaga6 ID

1 Australian National University & Data61, CSIRO, Australia
Minchao.Wu@anu.edu.au
2 Chiba University, Japan tsukada@math.s.chiba-u.ac.jp
3 University of Tsukuba, Japan uhiro@cs.tsukuba.ac.jp
4 RIKEN AIP, Japan
5 National Institute of Informatics, Japan tsekiyama@acm.org
6 Kyoto University ksuenaga@fos.kuis.kyoto-u.ac.jp

Abstract. Loop-invariant synthesis is the basis of program veriﬁcation.
Due to the undecidability of the problem in general, a tool for invari-
ant synthesis necessarily uses heuristics. Despite the common belief that
the design of heuristics is vital for the performance of a synthesizer,
heuristics are often engineered by their developers based on experience
and intuition, sometimes in an ad-hoc manner. In this work, we pro-
pose an approach to systematically learning heuristics for template-based
CounterExample-Guided Inductive Synthesis (CEGIS) with reinforce-
ment learning. As a concrete example, we implement the approach on
top of PCSat, which is an invariant synthesizer based on template-based
CEGIS. Experiments show that PCSat guided by the heuristics learned
by our framework not only outperforms existing state-of-the-art CEGIS-
based solvers such as HoICE and the neural solver Code2Inv, but also
has slight advantages over non-CEGIS-based solvers such as Eldarica and
Spacer in linear Constrained Horn Clause (CHC) solving.

1

Introduction

Static formal veriﬁcation is gaining more attention owing to the increasing im-
pact of software malfunctions. For its application to real-world software, its per-
formance is of paramount importance.

One of the major properties of interest is partial correctness: given a pro-
gram c and logical formulae ϕpre and ϕpost , deciding whether ϕpre and ϕpost
are the correct precondition and postcondition of c, respectively. A veriﬁcation
procedure needs to either prove or disprove that, if c is executed from an initial
state that satisﬁes ϕpre and terminates, then the ﬁnal state satisﬁes ϕpost .

It is known that a key to solving a partial correctness problem is discovering
an appropriate loop invariant, or simply an invariant [53]. The importance of
an invariant leads to considerable research interest in the methods for solving
an invariant synthesis problem (ISP).

 
 
 
 
 
 
2

M. Wu et al.

Since ISP is undecidable, a common way of solving the ISP is to heuristically
search for an invariant. For example, CounterExample-Guided Inductive Synthe-
sis (CEGIS) [17, 48, 51] is a popular approach to the ISP; it repetitively guesses
an invariant and proves or refutes the correctness of the guess by using an SMT
solver such as Z3 [36]. If a guess is refuted, then CEGIS makes another guess
based on the counterexample.

A CEGIS procedure typically maintains a search space for candidate invari-
ants from which a guess is made. For example, template-based CEGIS [17, 48, 51]
expresses the search space for candidate invariants using a template, which is a
predicate that contains parameters. A guess is obtained by instantiating the pa-
rameters with concrete values so that the instantiated guess does not contradict
the counterexamples obtained so far.

If all the candidates are refuted by the counterexamples, a CEGIS procedure
heuristically expands the search space. For a template-based CEGIS procedure,
if it turns out that any instantiation of the parameters contradicts the obtained
counterexamples, the procedure heuristically updates the template to make it
more expressive. Despite that the choice of heuristics for updating a template
aﬀects the performance of invariant synthesis, heuristics are often engineered by
developers, sometimes in an ad-hoc manner — their design is usually based on
the experience and intuition of experts, and is not systematically explored.

We propose a novel framework that learns heuristics for template-based
CEGIS for ISP using reinforcement learning (RL). RL is a kind of machine
learning techniques that aims at learning a policy of the behavior of an agent
that maximizes the rewards it receives from an environment. RL has been suc-
cessfully applied to the ﬁeld of formal methods recently, including loop invariant
synthesis [45, 46] and theorem proving [25, 54].

To this end, we reformulate template-based CEGIS as an RL problem, in
which an agent issues a sequence of actions following a policy. Each action tells
the underlying synthesizer how to update the template of candidate invariants
given the internal state of that synthesizer. The agent receives rewards depending
on the performance of the synthesizer after executing the actions taken by the
agent. RL algorithms are then implemented to learn a policy that maximizes the
rewards. Such a policy is essentially a heuristic for expanding the search space
that leads to a good performance of the synthesizer.

As a concrete example, we implement our framework on top of PCSat [41, 51],
which is an invariant synthesizer based on template-based CEGIS. We then con-
duct experiments using problems from standard benchmarks of invariant synthe-
sis. The experiments show that PCSat guided by the learned heuristics not only
outperforms state-of-the-art CEGIS-based solvers such as HoICE [9, 10] and the
neural solver Code2Inv [46], but also has slight advantages over non-CEGIS-
based solvers such as Eldarica [20] and Spacer [27] in linear CHC solving.

Contribution. The contribution of the present paper is summarized as follows.

– We reformulate the template-based CEGIS for ISP as an RL problem. Our
reformulation models the behavior of an underlying synthesizer as a Markov

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

3

decision process, in which a state represents the internal state of the synthe-
sizer and an action represents a command that expands the search space for
the candidate invariants.

– We implement our framework on top of PCSat, which is an invariant synthe-
sizer based on template-based CEGIS, and conduct experiments. We observe
that PCSat using the learned heuristics outperforms existing CEGIS-based
solvers. Experiments also show that PCSat using the learned heuristics even
has slight advantages over non-CEGIS-based solvers in linear CHC solving.

Structure of the paper. The rest of this paper is structured as follows. Section 2
reviews the preliminaries; Section 3 presents the template-based CEGIS proce-
dure in detail; Section 4 models template-based CEGIS as a Markov decision
process; Section 5 describes the experimental results and ablation studies; Sec-
tion 6 introduces related work; and Section 7 concludes.

2 Preliminaries

2.1 Notations

We write X for a ﬁnite sequence X1, . . . , Xn. We use symbols ϕ and ψ for ﬁrst-
order formulae over integers. We often write ϕ(x) to express that ϕ may depend
on the variables x. For this ϕ and integers n, we write ϕ(n) for the closed formula
obtained by substituting n for x in ϕ.

Let F (x) be a predicate variable. A constraint over F , denoted by symbols
Φ(F (x)) and E(F (x)), is a ﬁrst-order formula that contains F . For example,
∀x.F (x) =⇒ F (x − 1) is a constraint over F ; this constraint holds if we
set F to x ≤ 0 whereas it does not hold if we set F to x ≥ 0. We write
Φ(ϕ) for the formula that is obtained by substituting ϕ for F in Φ. A ground
constraint is a constraint that does not contain a term variable. For example,
F (0, 0) ∧ (F (1, 2) ⇒ F (2, 3)) ∧ ¬F (−1, 0) is a ground constraint.

We use symbol σ for a mapping from variables to integers; this mapping
represents an assignment of values to variables. We write σ(x) for a value of x
in σ and σ(ϕ) for the formula obtained by replacing every variable in ϕ with
its values in σ. For example, if σ := {x (cid:55)→ 0, y (cid:55)→ 1}, then σ(y) = 1 and
σ(x < 5 ∧ y > 5) = (0 < 5 ∧ 1 > 5), which is false.

2.2 Invariant Synthesis as CHC solving

It is well known that an ISP is equivalent to solving a constraint Φ(F (x)) over
a predicate variable F , where Φ(F (x)) is of the form

(cid:16)

∀x.ϕpre (x) =⇒ F (x)
(cid:16)

(cid:17)

∧

∀xy.ϕtrans (x, y) ∧ F (x) =⇒ F (y)

(cid:16)

∧

∀x.F (x) =⇒ ϕpost (x)

(cid:17)

.

(cid:17)

4

M. Wu et al.

For example, consider the program c1:

while y > 0 do x ← x + 1; y ← y − 1 done .

with precondition x = 0 ∧ y = z ∧ z ≥ 0 and postcondition x = z. Then, the
CHC ∀xyz.Φ1(F (x, y, z)) that expresses F (x, y, z) to be a loop invariant in the
program c1 can be deﬁned using the following Φ1(F (x, y, z))

x = 0 ∧ y = z ∧ z ≥ 0 =⇒ F (x, y, z)
∧
y > 0 ∧ F (x, y, z) =⇒ F (x + 1, y − 1, z) ∧
y ≤ 0 ∧ F (x, y, z) =⇒ x = z.

(1)

The ﬁrst conjunct ensures that F (x, y, z) is implied by the initial condition
x = 0 ∧ y = z ∧ z ≥ 0; the second conjunct ensures that the predicate F (x, y, z)
is preserved by one iteration of the loop if the guard condition (y > 0) of the loop
holds; and the third conjunct ensures that F (x, y, z) implies the postcondition
x = z if the loop terminates. The loop invariant x + y = z ∧ y ≥ 0 satisﬁes
∀xyz.Φ1(F (x, y, z)).

A constraint of this form is called a linear Constrained Horn Clause (linear
CHC) (or, simply CHC in this paper)7. A solution to the above constraint is a
logical formula ϕsol (x) such that Φ(ϕsol ) is true.

2.3 Reinforcement Learning

A Markov decision process, which models an environment in RL problems, is
speciﬁed by the following elements:

– a set S of states,
– an initial state s0 ∈ S,
– a subset R ⊆ R of reals, called rewards,
– a ﬁnite set A of actions, and
– a dynamics function p : S × A × R × (S ∪ {(cid:63)}) −→ [0, 1].

We write p(r, s(cid:48)|s, a) for p(s, a, r, s(cid:48)). The dynamics function speciﬁes a proba-
bility distribution for each (s, a) ∈ S × A, that means,

(cid:88)

(cid:88)

s(cid:48)∈S∪{(cid:63)}

r∈R

p(r, s(cid:48)|s, a) = 1,

for every (s, a) ∈ S × A.

(cid:63) is a special symbol meaning termination; if the current state is s and the action
is a, then (cid:80)

r∈R p(r, (cid:63)|s, a) is the probability of termination at the next step.

7 The invariant synthesis problem in this paper is thus deﬁned in terms of three
constrained Horn clauses for simplicity and Φ denotes the conjunction of the three.
This does not lose generality as linear CHC with multiple predicate variables and
clauses can always be converted to this form by replacing the predicate variables
with a single predicate variable representing their direct sum and then merging the
clauses.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

5

The behaviour of the agent is described by a policy, which is a function
π : S × A −→ [0, 1] such that (cid:80)
a∈A π(s, a) = 1 for every s ∈ S. If the current
state is s, the agent chooses a as the next action with probability π(s, a). We
write π(a|s) for π(s, a).

A pair of a Markov decision process and a policy probabilistically generates

a sequence

s0, a0, r1, s1, a1, r2, s2, . . . ,

where ai is a sample of π(−|si) and si+1, ri+1 is a sample drawn from the distri-
bution speciﬁed by p(−, −|si, ai). We assume that the interaction terminates at
step n, i.e. sn = (cid:63). Then, the return is the sum of the rewards r1 + r2 + · · · + rn
(or r1 + γr2 + · · · + γn−1rn where γ ∈ [0, 1] is a discount factor ).

The goal of RL is to ﬁnd a policy that maximize the expected return. There
are many learning algorithms that can achieve this. For example, our implemen-
tation for the experiments in Section 5 uses the ﬁrst-visit on-policy Monte Carlo
control [47] and the Advantage Actor-Critic [28].

3 Template-Based CEGIS for ISP

3.1 CounterExample Guided Inductive Synthesis (CEGIS) for ISP

CounterExample-Guided Inductive Synthesis (CEGIS) [48] solves a given CHC
Φ(F (x)) via the interaction between a synthesizer (S) and a validator (V). Its
high-level workﬂow is described as follows.

– S maintains a ground constraint E(F (x)); E(F (x)) is called example in-
stances. The constraint E(F (x)) is a necessary condition for Φ(F (x)), i.e.,
every solution of Φ(F (x)) satisﬁes E(F (x)) as well. The task of S is to syn-
thesize a candidate solution ψ(x), which is a solution of E(F (x)). Notice that
S does not access the CHC Φ(F (x)). S then sends ψ(x) to V.

– V checks whether the candidate solution ψ(x) sent from S is a real solution to
Φ(F (x)) by checking whether ϕpre (x)∧¬ψ(x), ϕtrans (x, y)∧ψ(x)∧¬ψ(y) or
ψ(x) ∧ ¬ϕpost (x) is satisﬁable. Most solvers implement this step by querying
the satisﬁability of the above formulas using an oﬀ-the-shelf solver such as
Z3. If all of the formulas are unsatisﬁable, then it follows that Φ(ψ(x)) is true
and thus ψ(x) is a real solution to the CHC Φ(F (x)). Otherwise, one of the
above formulas is satisﬁable, e.g., ϕtrans (c, d) ∧ ψ(c) ∧ ¬ψ(d) is true for some
vectors c, d of constants. Then, V sends F (c) ⇒ F (d) to S as a new example
instance to be satisﬁed. S updates E(F (x)) to E(F (x)) ∧ (F (c) ⇒ F (d)) and
seeks a new candidate solution again.

3.2 Template-Based Approach to CEGIS

In each step of a CEGIS-based CHC solving, a synthesizer needs to ﬁnd a can-
didate solution that satisﬁes all the constraints in the current example instance
E(F (x)). One of the strategies to implement the candidate-solution discovery is
called a template-based approach [3, 17, 51].

6

M. Wu et al.

A template-based synthesizer works as follows. It maintains an example in-
stance E(F (x)) and a template ψ(a, x), which is a predicate over x with parame-
ters a, and constructs a candidate solution by ﬁnding an appropriate assignment
to a. An appropriate assignment to a can be computed by using an SMT solver
such as Z3: since E(F (x)) is a ground constraint, i.e., a formula with no quantiﬁer
nor variable, C(a) := E(ψ(a, x)) is a quantiﬁer-free formula with free variables
a and an SMT solver gives a satisfying assignment σ to a, provided that C(a)
is satisﬁable. Then ψ(σ(a), x) is a candidate solution. If C(a) is not satisﬁable,
there is no candidate solution of the form ψ(c, x), where c is a vector of con-
stants. Then, the synthesizer heuristically updates the template and uses it to
discover a new candidate solution. The strategies for updating the templates is
what we mean by heuristics below.

For Constraint 1, a synthesizer would be able to discover the solution x + y =
z ∧ y ≥ 0 if it designates a template a1x + b1y + c1z ≥ 0 ∧ a2x + b2y + c2z ≥
0 ∧ a3x + b3y + c3z ≥ 0; the above solution is obtained once an SMT solver ﬁnds
a solution (a1, b1, c1, a2, b2, c2, a3, b3, c3) = (1, 1, −1, −1, −1, 1, 0, 0, 1).

Notice that a template ψ(a, x) determines a set of candidate solutions, each
member of which is an instantiation of a in ψ(a, x) to concrete values. There-
fore, a template in the template-based approach determines the search space
for a candidate solution of a given constraint. A template is said to be more
expressive than another if the set of formulae that is obtained by instantiating
the parameters in the latter is a subset of the former.

There is a trade-oﬀ between the expressiveness of a template and the ef-
ﬁciency of a synthesizer [37, 51]. If one uses an expressive template, there is
more chance that there is a true solution that can be obtained by instantiating
the template. However, the constraint C(a) generated by using an expressive
template tends to be complex, which incurs performance degradation of SMT
solving and therefore a synthesizer. In deciding which template to be used, it is
crucial to ﬁnd a sweet spot that addresses this trade-oﬀ.

Algorithm 1 is a typical template-based synthesizer for CHC solving. This

procedure uses the following subprocedures:

– SMT (C(a)): Decides whether the predicate C(a) is satisﬁable or not by an
SMT solver. If it is satisﬁable, then it returns Sat(σ) where σ is a value as-
signment to a in C(a) such that σ(C(a)) is valid. If not, it returns Unsat(I),
where I is a collection of various information on the behavior of the decision
procedure (e.g., consumed time and memory). I also includes explanations
why C(a) is unsatisﬁable such as an unsat core, which is an unsatisﬁable
subconstraint of C(a). We write I.uc for the unsat core contained in I.
– Validator (ϕ(x)): Sends the candidate solution ϕ(x) to the validator and let
it decide whether it is a real solution. If ϕ(x) is indeed a solution, then
the validator returns Valid . Otherwise, the validator returns Cex (E(F (x))),
where E(F (x)) is the new example instance to be satisﬁed by solutions.
– ChangeTempl (ψ(a, x), I): Heuristically updates the template ψ(a, x) to a

new one based on the information I returned by the SMT solver.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

7

C(a) ← E(ψ(a, x))
r1 ← SMT (C(a))
if r1 = Sat(σ) then

Instantiate parameters
c ← σ(a)
Send the candidate solution to the validator
r2 ← Validator (ψ(c, x))
Check whether ψ(c, x) is a real solution
if r2 = Valid then

Algorithm 1 Template-based synthesizer
1: ψ(a, x) ← the initial template
2: E(F (x)) ← ∅
3: while Timeout is not reached do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end while

else if r2 = Cex (E (cid:48)(F (x))) then

ψ(a, x) ← ChangeTempl (ψ(a, x), I)

Return ψ(c, x) as a real solution

else if r = Unsat(I) then

end if

end if

Cex is found; new example instance is received.
E(F (x)) ← E(F (x)) ∧ E (cid:48)(F (x))

3.3 Synthesizer Implemented in PCSat

PCSat [41, 51] is one of the tools for CHC solving based on template-based
CEGIS. It uses a family of template ψN,P,Q(a, x), in which each template is
determined by the values P, Q and N = (N1, . . . , NM ) (of which the length is
denoted by M ). Assume that x := (x1, . . . , xL). Then the parameter a used in
these templates consists of the following parameters.

– Coeﬃcient parameters a(ij)
– Constant parameters c(ij) for each 1 ≤ i ≤ M and 1 ≤ j ≤ Ni.

for each 1 ≤ i ≤ M , 1 ≤ j ≤ Ni and 1 ≤ k ≤ L.

k

The template family ψN,P,Q(a, x) is deﬁned as follows using these parameters:

M
(cid:95)

Ni(cid:94)

L
(cid:88)

i=1

j=1

k=1

a(ij)
k xk ≥ c(ij) ∧

M
(cid:94)

Ni(cid:94)

L
(cid:88)

i=1

j=1

k=1

|a(ij)
k

| ≤ P

∧

M
(cid:94)

Ni(cid:94)

i=1

j=1

|c(ij)| ≤ Q.

We explain the above deﬁnition in the following.

– The subformula

(cid:80)L

M
(cid:87)
i=1

k=1 a(ij)

k xk ≥ c(ij) is a boolean combination of

Ni(cid:86)
j=1
linear inequalities (cid:80)L
k xk ≥ c(ij) expressed in a disjunctive normal
form (DNF). The parameter M (resp. Ni) is the number of disjuncts (resp.
conjuncts in each disjunct) in this template; therefore, the larger they are,
the more expressive the template is.

k=1 a(ij)

8

M. Wu et al.

– The subformula

M
(cid:86)
i=1

Ni(cid:86)
j=1

(cid:80)L

k=1 |a(ij)

k

| ≤ P bounds the sum of the absolute

values of the coeﬃcients in each linear inequality (i.e., the L1 norm of each
(a(ij)
)1≤k≤L). The bound P is a natural number or ∞; if P = ∞, then the
k
coeﬃcients may be any number. The larger P is, the more expressive the
template is.

– The subformula

M
(cid:86)
i=1

Ni(cid:86)
j=1

|c(ij)| ≤ Q bounds the absolute value of the constant

c(ij) in each linear inequality. The bound Q is a natural number or ∞. The
larger Q is, the more expressive the template is.

The strategy of PCSat actually uses a subset of above-deﬁned templates, namely
the subset of templates of the form ([N ] ∗ M, P, Q) where [N ] ∗ M means the
list of length M consisting only of N . In other words, Ni = Ni(cid:48) holds for every
template reachable by the hand-crafted strategy of PCSat.

As mentioned in Section 3.2, an update to a template happens when the
constraint C(a) on parameters a is unsatisﬁable. PCSat decides how to update
a template using the unsat core of C(a).

Concretely, the heuristic implemented by PCSat is as follows. (M, N, P, Q)
are initialized to (1, 1, 1, 0). When PCSat needs to update the current template,
it increments M or N by 1 and it may increment P and/or Q depending on the
unsat core. M and N are incremented in alternation. If P occurs in the unsat
core, then P is incremented by 1. If Q occurs in the unsat core and Q < 3, then
Q is incremented by 1; if Q ≥ 3, then Q is set to ∞.

4 Finding Heuristics with Reinforcement Learning

This section describes how we formulate the problem of learning heuristics as
a reinforcement learning (RL) problem. In our setting, the environment is an
implementation of the template-based CEGIS parameterized by heuristics. We
learn a policy that represents a heuristic, which tells the environment the shape
of the template that should be tried in order to guess the next candidate solution.
The long-term goal is to ﬁnd a solution of a given CHC, preferably in a short
time.

We model the template-based CEGIS procedure as a Markov decision process

(MDP) as follows, and implement RL algorithms to solve such an MDP.

States. A state of the MDP in our formalism is (N, P, Q, f1, f2, z) where (N, P, Q)
are values that determine the current template, f1, f2 are boolean values that
summarize the information of the unsat core for the current template, and z
is the number of candidate solutions that the learner found since the previous
update of template. f1 is true if the unsat core contains P , and f2 is true if the
unsat core contains Q. The information expressed by ﬂags f1 and f2 is also used
by heuristics engineered by the PCSat developers. The parameter z is inspired
by Code2Inv [46], which uses the number of example instances satisﬁed by the
current state of the environment as part of a reward.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

9

Actions. An action is a tuple (n, p, q) ∈ N∗ × (N ∪ {∞}) × (N ∪ {∞}), which
updates the current template (N, P, Q) to (N + n, P + p, Q + q)8. Here N + n is
the coordinate-wise sum; if the lengths of N and n are diﬀerent, we append 0’s
to the tail of the shorter one. For example, if N = (1, 1) and n = (0, 0, 1), then
N + n = (1, 1, 1). Note that the length M of N can be increased by choosing
suﬃciently long n.

Rewards. Since simply checking whether a solution is found or not may have
the problem of sparse rewards [49, Section 17.4] that makes learning harder, we
deﬁne the reward associated with each action to be −T , where T is the time
spent since the last invocation of the agent. The sum of the rewards is then
naturally −Ttotal , where −Ttotal is the total time spent for the run. Intuitively,
the earlier a solution is found, the more reward is given. The smallest reward is
given if the synthesizer eventually times out when guided by the agent.

5 Experiments

Datasets and tasks. We use the problems in the Inv-Track of the SyGuS-Comp [4]
2019 competition9 as the data set of our experiments. A tool for this competition
is supposed to return one of the three answers: Sat, indicating that the tool
successfully synthesized an answer to the given problem; Unsat, indicating that
there is no solution to the given problem; and Timeout, indicating that the
tool fails to solve the given problem within a speciﬁed time limit. An answer of
Sat must be accompanied by a witness, which is an invariant in the case of Inv-
Track. The Inv-Track consists of 858 problems for evaluating the performance of
invariant-synthesis tools. We randomly split the problems in the Inv-Track into
training and test sets in an 80:20 ratio. The learning task is to train an agent
that is capable of guiding PCSat to ﬁnd a solution to each problem as soon as
possible.

Conﬁgurations. We train the agent using two diﬀerent reinforcement learning
algorithms: the ﬁrst-visit on-policy Monte Carlo (MC) control [47] and Advan-
tage Actor-Critic (A2C) [28]. The former is a tabular method that relies on
state-action value tables and the latter leverages deep learning. The state and
action spaces are as described in Section 4.

For MC, in order to control the size of the state-action value table and make
the learning tractable, we set up an upper bound for each parameter in the

8 The actions adopted in the present paper only increase the complexity of templates.
Before settling on the current design, we tried actions that can reduce the complexity
of templates and states that incorporate the timeout feedback from Z3 (as a hint for
the complexity of the current template). We however omitted them in the present
paper because early experiments showed that learning with Monte Carlo control
became unstable and the performance decreased in such a setting.

9 The problems can be found at https://github.com/SyGuS-Org/benchmarks.

10

M. Wu et al.

state representation.10 We train the agent with (cid:15)-greedy11 action selection with
(cid:15) = 0.05 and evaluate it with the greedy policy. The discount factor is set to 1.
The time limit for each problem during training is 120 seconds.

For A2C, given a trajectory τ = (s0, a0, r0, s1, a1, r1, . . . , sT , aT , rT ), our ob-
jective is to maximize the following expected policy rewards with discount factor
γ = 0.99

Eτ ∼πθ(at|st)

(cid:104) T
(cid:88)

t(cid:48)=t

(cid:105)
γt(cid:48)−trt(cid:48) − Vϕ(st)

where θ is the parameters of the actor policy πθ and ϕ is the parameters of the
critic policy Vϕ. Each policy is parameterized by a multi-layer perceptron with
two hidden layers of dimensions 256 and 512. We use the loss

L(ϕ) =

T
(cid:88)
(

T
(cid:88)

t

t(cid:48)=t

γt(cid:48)−trt(cid:48) − Vϕ(st))2

to train the critic. Both policies are trained using RMSProp [19] with a learning
rate of 5 × 10−5. The time limit for each problem during training is 10 seconds.
For each experiment, we train the agent for 200 epochs on the training set.
Among the learned policies, we chose the one that has the best performance
on the training set and evaluate them on the test set. All the experiments are
conducted on PCSat with SMT solver Z3 (version 4.8.9) on a 2.8GHz Intel(R)
Xeon(R) CPU with 64 GB RAM and a Tesla V100 GPU with 16GM RAM.

5.1 Evaluation

In this section, we study the quality of the heuristics learned by our framework
by comparing them with the heuristic engineered by human experts (i.e., the
PCSat developers), existing non-learning-based solvers (CVC4 [6, 7, 39], Loop-
InvGen [37], HoICE [9], Eldarica [20], FreqHorn [15] and Spacer [27]) and the
state-of-the-art neural solver Code2Inv [45, 46]12

10 The action space here is restricted to (n, p, q) ∈ {0, 1}≤4 × {0, 1, ∞} × {0, 1, ∞}. In
particular, the number of conjuncts in templates is unbounded but the number of
disjuncts is at most 4. The agent’s ability to observe a state is thus limited. For
Ni, the agent does not distinguish 4 with greater values; for P and Q it does not
distinguish between 5 with greater values. For example, the state ((2, 1, 4, 0), 7, 4)
looks ((2, 1, ≥4, 0), ≥5, 4) to the agent, where ≥4 and ≥5 are symbols representing
numbers greater than or equal to 4 and 5, respectively.

11 Technical terms of reinforcement learning that are not explained in this paper are
used in this and the next paragraphs, in order to appropriately describe the experi-
mental setting. See a textbook [49] for a reference.

12 We have also tried to use CLN2INV [40], which is a deep-learning-based invariant-
synthesis tool, as a baseline. However, we excluded it from our baseline since the
implementation that is made public is not fully automated. It uses hints that are
given by human to solve certain problems.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

11

If classiﬁed by the underlying approaches to ISP, LoopInvGen, HoICE, Fre-
qHorn and Code2Inv are all CEGIS-based solvers. LoopInvGen is based on
CEGIS with greedy set covering for synthesis. HoICE uses decision-tree based
CEGIS while FreqHorn and Code2Inv use grammar-based CEGIS. Eldarica and
Spacer are non-CEGIS-based solvers. Eldarica uses CounterExample-Guided Ab-
straction Reﬁnement (CEGAR) with predicate abstraction, and Spacer is based
on Property Directed Reachability (PDR).

LoopInvGen and CVC4 are respectively the winners of SyGuS-Comp 2018
and 2019. Eldarica and Spacer are participants of some of the linear CHC tracks
and won the 1st and 2nd places13.

Table 1. Performance of the learned heuristics and that of the baselines on the test
set. PCsat/random refers to PCSat guided by a random policy, and PCSat/expert refers
to PCSat using the heuristic engineered by its developers. PCSat/A2C and PCSat/MC
refer to PCSat using the heuristics learned with the corresponding approaches, as
described in Section 5. PCSat/A2C+PCSat/MC means that the two policies work jointly
in parallel, and a problem is solved if one of them returns a solution. See footnote 14
for the solvers marked with *.

Methods

approach sat unsat

timeout

time(s)

FreqHorn
LoopInvGen*
CVC4*
PCSat/random
Eldarica
PCSat/expert
HoICE

CEGIS
CEGIS
-

70
87
102
CEGIS 116
CEGAR 122
CEGIS 135
CEGIS 141

CEGIS 145
PCSat/A2C
CEGIS 146
PCSat/MC
PCSat/A2C+PCSat/MC CEGIS 149

Spacer

PDR 156

0
5
9
9
9
9
8

9
9
9

9

101
79
60
46
40
27
22

17
16
13

6

6863
5086
3873
3383
3714
2130
1707

1947
1550
1460

380

Comparison with non-learning-based solvers. Table 1 shows the number of prob-
lems14 in the test set solved by each method given a time limit of 60 seconds.

13 According to https://chc-comp.github.io/2021/presentation.pdf. Also note
that our RL formulation itself is general so that it is also applicable to non-linear
CHC, and so is PCSat. The present paper focuses on the eﬀectiveness for linear CHC
as a ﬁrst step because it is a non-trivial class of practical importance.

14 Unfortunately, we were not able to reproduce the same level of performance as de-
scribed in the competition report of LoopInvGen and CVC4, possibly due to the
diﬀerences in versions and conﬁgurations used by the solvers. CVC4 (resp. Loop-
InvGen) solved 592 (resp. 512) instances out of 858 according to the competition

12

M. Wu et al.

Table 2. Performance with a time limit of 600 seconds.

Methods

approach sat unsat

timeout

time(s)

FreqHorn
LoopInvGen*
CVC4*
PCSat/random
PCSat/expert
HoICE

CEGIS
CEGIS
-

91
90
106
CEGIS 121
CEGIS 145
CEGIS 147

CEGIS 150
PCSat/MC
CEGAR 151
Eldarica
CEGIS 151
PCSat/A2C
PCSat/A2C+PCSat/MC CEGIS 154

Spacer

PDR 156

0
5
9
9
9
9

9
9
9
9

9

80
76
56
41
17
15

12
11
11
8

6

52933
46445
34869
25858
12537
10253

8705
12866
8319
6413

3619

It can be seen that the learning is eﬀective — compared to PCSat guided by
a random policy, the policy learned by MC solves signiﬁcantly more problems
(146 as opposed to 116). The learned heuristics are also better than the heuristic
engineered by human experts. PCSat guided by MC solves 146, while it solves
only 135 when using heuristic designed by its developers.

The diﬀerence in the number of solved problems between the two learned
heuristics seems to be minimal. Nonetheless, we observed that PCSat/A2C solves
three problems whose solutions were not found by PCSat/MC and PCSat/MC
solved four problems not solved by PCSat/A2C. In fact, as illustrated by Fig-
ure 2a, while the two policies are similar in terms of time spent on solving most
of the problems, they start to disagree on some of the problems when a larger
time limit is allowed, with one being able to solve them in a short period of
time, and the other taking signiﬁcantly longer. This suggests that the diﬀerence
in learning algorithms may have indeed induced diﬀerent heuristics, and using
learned heuristics jointly may boost the performance of PCSat in practice.

PCSat guided by the learned heuristics outperforms most of the existing
non-learning-based solvers given a time limit of 60 seconds, with the only ex-
ception of Spacer. The diﬀerence in performance between the solvers is reduced
when a larger time limit is allowed. Figure 1 and Table 2 show the diﬀerence
in performance when using a time limit of 600 seconds. It can also be seen that
the learned heuristics achieved the best performance among all CEGIS-based
solvers regardless of the time limit. Spacer is the only one that signiﬁcantly out-
performs PCSat with learned heuristics and every other solver. The performance
of Spacer on the test set seems to be an outlier, for which we do not know the
exact reason. Nonetheless, below we are forced to use a diﬀerent test set in or-

report but only 505 (resp. 451) in our experiment. We are not sure how many in-
stances in the test set was solved by CVC4 and LoopInvGen in the competition,
since the report just tells the total number of solved instances and does not report
which instances are solved by each solver.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

13

Fig. 1. Comparison of the cumulative time of each method.

der to compare the performance of our framework with that of Code2Inv, and
noticed that Spacer is not always the winner on every benchmark.

Comparison with Code2Inv. We now compare the learned heuristics with the
neural solver Code2Inv. Ideally, we would like to evaluate Code2Inv on our test
set. However, the tools accompanied by Code2Inv for generating the graph rep-
resentations of given problems failed to correctly convert the problems in our
benchmark into a form readable by Code2Inv. For fairness, we use all the com-
mon problems that occur in both our data set and the Code2Inv benchmark, in
their original forms readable by each solver respectively. There are 81 such prob-
lems in total. Since our training set may contain problems from the Code2Inv
benchmark, we re-train our agent on a training set containing 127 problems
from the SyGuS-Comp 2018 competition, which does not overlap the set of 81
common problems.

Table 3 shows the performance of our agent and that of Code2Inv as an
out-of-the-box solver given a time limit of 600 seconds. We also add the perfor-
mance of Spacer as an indicator of the state-of-the-art performance on this set
of problems. It can be seen that PCSat with learned heuristics not only out-
performs Code2Inv by solving 50% more problems, but also slightly surpasses
Spacer in terms of the number of solved problems. Figure 3a and Figure 3b illus-
trate the performance on individual problems. Noticeably, every problem solved
by Code2Inv is solved by our agent in a shorter time. Our agent is also able to
ﬁnd four solutions that were not found by Spacer.

0255075100125150175200Testproblemssolved(outof171)0100200300400500600TimeinsecondsSpacerMC/ﬂex+A2C/ﬂexA2C/ﬂexEldaricaMC/ﬂexHoICEPCSat/expertrandomcvc4LoopInvGenFreqHorn14

M. Wu et al.

(a) A2C vs. MC

(b) MC vs. Eldarica

(c) joint vs. expert

Fig. 2. Comparison of the time spent on each problem by diﬀerent methods. The scale
is logarithmic.

The limited performance of Code2Inv is possibly due to the need to learn ev-
ery problem from scratch, because the shapes of the neural networks in Code2Inv’s
algorithm depend on the graph representation of the target problem. Diﬀerent
problems may use diﬀerent neural networks, which makes it diﬃcult to apply
the learned policy for one problem to another which has a completely diﬀerent
graph representation. In contrast, our approach does not have such a restriction
— the learned policies are applicable to all the problems as long as the state and
action spaces remain unchanged.

Table 3. Performance of PCSat guided by the learned heuristics and that of Code2Inv
and Spacer, on the new test set.

Methods

approach sat unsat

timeout

time(s)

Code2Inv CEGIS
PDR
Spacer

PCSat/A2C CEGIS

45
70

72

-
6

6

36
5

3

27289
3009

2373

5.2 Ablation: design of the state space

So far we have been using the state space as described in Section 4. In this
section, we study how a diﬀerent state space may aﬀect the performance of the
learned heuristics. In particular, we are interested in knowing whether embedding
expert knowledge into the state space helps improve the performance. To see if
the expert knowledge helps, we introduce a new design of state space as follows
which reﬂects the human expert knowledge of invariant synthesis.

 1 10 100 1 10 100PCSat/A2CPCSat/MC 1 10 100 1 10 100EldaricaPCSat/MC 1 10 100 1 10 100PCSat/expertPCSat/A2C+PCSat/MCLearning Heuristics for Template-based CEGIS with Reinforcement Learning

15

(a) A2C vs. Code2Inv

(b) Cumulative time

Fig. 3. (a) Comparison of the time spent on each problem by Code2Inv and PCSat
using our learned heuristics. (b) Comparison of the cumulative time of each method.

Recall that the strategy of PCSat only uses templates of the form ([N ] ∗
M, P, Q) where [N ] ∗ M is the list of length M consisting only of N . An expert
state is an abstraction obtained from the original state ([N ] ∗ M, P, Q, f1, f2, z).
Concretely, an expert state is a tuple (b0, b1, b2, b3, b4, b5, f1, f2, z(cid:48)) of the boolean
values, each element of which summarizes a corresponding parameter of the
original state as follows.

b0 (resp. b1) ⇔ M < N (resp. M = N ).
b2 (resp. b3) ⇔ P = ∞ ∨ P ≥ 2 (resp. P = ∞ ∨ P ≥ 5).
b4 (resp. b5) ⇔ Q = ∞ ∨ Q ≥ 2 (resp. Q = ∞ ∨ Q ≥ 5).
f1, f2 are the same as in the original state.
z(cid:48) ⇔ z > 0.

This design of the state space is inspired by the heuristic engineered by
the developers of PCSat (i.e., the heuristic used in the baseline PCSat/expert
in Section 5.1). For example, the boolean value b1 can be used to alternate
incrementing M and N . The baseline heuristic also uses the values of P and
Q to increase P and Q gradually, and the developers of PCSat believe that 2
and 5 are good magic numbers that balance the expressiveness of the templates
well. The developers also believe that whether z = 0 or not should be the most
signiﬁcant information about z.

The expert state space is much smaller than the original state space as it has
eliminated “irrelevant” possible states as believed so by human experts. The size
of the expert state space is only 512. Such a state space is especially useful when
training with tabular methods such as MC, as we do not need to worry about
huge state-action value tables that are impossible to be fully explored. Given
the expert state space, the corresponding expert action is then represented by

 1 10 100 1 10 100Code2InvPCSat/A2C020406080100Testproblemssolved(outof81)0100200300400500600TimeinsecondsPCSat/A2CSpacerCode2Inv16

M. Wu et al.

(n, m, p, q) ∈ {0, 1}×{0, 1}×{0, 1, ∞}×{0, 1, ∞}, which updates ([N ]∗M, P, Q)
to ([N + n] ∗ (M + m), P + p, Q + p).

We train an agent using MC with the expert state space and expert action
space, and compare its performance with that of the agents trained with the
original state space deﬁned in Section 4. We call states in the original state
space raw states below. Table 4 shows the performance of the four agents. It can
be seen that while MC using the expert state space learns well, the extra expert
knowledge does not help solve more problems. Conversely, the slightly worse
performance may be a result of the overly abstracted state space — an agent
using this space may not be able to distinguish states that may have otherwise
led to diﬀerent updates of a template when using a richer representation of states
such as the raw states.

On the other hand, Figure 4a shows that MC using expert states helped ﬁnd
solutions not found by MC using the raw states, and vice versa. Figure 4b and
Figure 4c further shows that the performance of MC/raw working together with
MC/expert is almost identical to that of it working together with A2C/raw, sug-
gesting that the expert insight embedded in the expert states might be learnable
by A2C using the raw states.

Table 4. Performance of the learned heuristics using the expert states, as indicated
by MC/expert, and the learned heuristics using the raw states, as indicated by MC
(resp. A2C)/raw.

Methods

sat unsat

timeout

time(s)

121
random
MC/expert 148
150
MC/raw
151
A2C/raw

9
9
9
9

41
14
12
11

25858
9337
8705
8318

6 Related Work

This paper focused on a template-based approach to CEGIS [3, 17, 51], while
grammar-based synthesis [8, 12, 16, 26, 45, 46] (e.g., Code2Inv) and decision-tree
learning [9, 14, 18, 29, 30, 55] (e.g., HoICE) are also popular approaches to guess-
ing a candidate solution from gathered data in CEGIS. Although the template-
based approach is advantageous in that it adaptively adjusts atomic formulae
to be used in a candidate solution, it requires careful tuning of a heuristic for
deciding the shape of a candidate solution. We have addressed this challenge by
leveraging RL to learn an eﬀective heuristic.

Si et al. [45, 46] proposed a framework called Code2Inv to learn loop in-
variants with deep reinforcement learning. Code2Inv uses graph neural networks
to encode the information of a program and synthesizes loop invariants using

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

17

(a) raw vs. expert

(b) joint vs. joint

(c) Cumulative time

Fig. 4. (a)(b) Comparison of the time spent on each problem by diﬀerent methods. (c)
Cumulative time of each method.

a syntax-directed encoder-decoder structure. In contrast to their approach that
tries to synthesize an invariant directly from a program, we learn the heuristics
that guide the search in existing tools for program veriﬁcation. One restriction
of Code2Inv is that the shape of the neural networks that parameterize its pol-
icy depend on the graph representation of the target problems. It is unclear
how a policy learned for one problem can be easily applied to another that has
a completely diﬀerent graph representation. In contrast, our policy learned for
heuristics is universal — it works with any problem readable by the base solver.
Code2Inv and our approach also diﬀer in the evaluation of the obtained
solver. Si et al. evaluated the performance of their solver in the numbers of queries
to Z3 [36], instead of measuring the actual running time. Although the running

 1 10 100 1 10 100MC/expertMC/raw 1 10 100 1 10 100MC/raw+MC/expertMC/raw+A2C/raw0255075100125150175200Testproblemssolved(outof171)0100200300400500600TimeinsecondsMC/raw+MC/expertMC/raw+A2C/rawA2C/rawMC/rawMC/expertrandom18

M. Wu et al.

cost of Z3 is dominant in most of the program veriﬁcation tasks, Code2Inv con-
ducts online learning of a neural network, which incurs non-trivial additional cost
to the performance. On the contrary, the performance of our solver is evaluated
in the wall-clock time.

RL has also been applied to program synthesis [8, 12] and relational veriﬁca-
tion [11]. These works learn heuristics that select one inference rule or production
rule from ﬁnitely many options, while we apply RL to aid prioritized search for
an inﬁnite set of candidate solutions. In other words, we learn heuristics to syn-
thesize cut-formulas, which correspond to loop invariants in our problem setting.
Approaches such as Concord-StandardPG [12] also need to retrain the policies
for each problem just like Code2Inv [45, 46], while our approach does not have
such limitation.

The beneﬁts from learning heuristics for theorem proving has been demon-
strated in various automated theorem proving techniques including CDCL for
SAT [33, 43, 44], strategies for SMT [5], connection tableau [24], and incremental
determinization for QBF [32, 43]. These previous studies applied machine learn-
ing to enhance the proof search. We cannot directly apply these theorem proving
techniques to the ISP because we need to synthesize an appropriate predicate
as a solution to the ISP, in addition to deciding the validity of a given formula.
Extensive research has been conducted on learning embedding of programs
and logical formulae through neural networks such as LSTMs [21, 22, 23], tree-
based neural networks [13, 34, 35, 42], graph neural networks [1, 31, 38, 52],
and a path-based attention model [2]. The present work does not use learned
embedding of invariants or programs. It is an interesting future direction to
investigate whether using embedding serves for any further improvement.

7 Conclusion

We presented how to apply reinforcement learning to the task of learning eﬀective
heuristics for a template-based CEGIS procedure. To this end, we modeled the
behavior of the procedure as an MDP and a heuristic as an agent that updates
a template. We trained the agents using the ﬁrst-visit on-policy Monte Carlo
control (MC) and Advantage Actor-Critic (A2C); the learned heuristics are the
best in its kind — they outperformed the heuristics engineered by human ex-
perts, and achieved the best performance among the CEGIS-based solvers. The
learned heuristics have also demonstrated comparable (and sometimes superior)
performance to that of the state-of-the-art non-CEGIS-based solvers, validating
the eﬀectiveness of our approach.

We have focused on programs that have only one loop, whose loop-invariant
synthesis can be reduced to solving a linear CHC with a single predicate variable.
Future work includes handling a broader class of constraints such as CHCs with
multiple predicate variables for safety veriﬁcation of a program with multiple
loops and the class pfwCSP of constraints for relational [51] and branching-time
temporal [50] veriﬁcation.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

19

Acknowledgments

This work was supported in part by JSPS KAKENHI Grant Numbers JP19K22842,
JP19H04084, JP20H04162, and JP20H05703, JST CREST Grant Number JP-
MJCR2012, Japan, and ERATO HASUO Metamathematics for Systems Design
Project (No. JPMJER1603), JST.

Bibliography

[1] Allamanis, M., Brockschmidt, M., Khademi, M.: Learning to represent pro-

grams with graphs. In: ICLR ’18. OpenReview.net (2018)

[2] Alon, U., Zilberstein, M., Levy, O., Yahav, E.: Code2vec: Learning dis-
tributed representations of code. Proceedings of the ACM on Programming
Languages 3(POPL) (Jan 2019)

[3] Alur, R., Bod´ık, R., Dallal, E., Fisman, D., Garg, P., Juniwal, G., Kress-
Gazit, H., Madhusudan, P., Martin, M.M.K., Raghothaman, M., Saha, S.,
Seshia, S.A., Singh, R., Solar-Lezama, A., Torlak, E., Udupa, A.: Syntax-
guided synthesis. In: Dependable Software Systems Engineering, pp. 1–25
(2015)

[4] Alur, R., Fisman, D., Padhi, S., Reynolds, A., Singh, R., Udupa, A.: SyGuS,

https://sygus.org, accessed on 15-Oct-2021

[5] Balunovic, M., Bielik, P., Vechev, M.T.: Learning to solve SMT formulas.

In: NeurIPS ’18. pp. 10338–10349 (2018)

[6] Barbosa, H., Reynolds, A., Larraz, D., Tinelli, C.: Extending enumerative
function synthesis via SMT-driven classiﬁcation. In: FMCAD ’19. pp. 212–
220 (Oct 2019)

[7] Barrett, C., Conway, C.L., Deters, M., Hadarean, L., Jovanovi´c, D., King,
T., Reynolds, A., Tinelli, C.: CVC4. In: CAV ’11. p. 171–177. Springer
(2011)

[8] Bunel, R., Hausknecht, M., Devlin, J., Singh, R., Kohli, P.: Leveraging
grammar and reinforcement learning for neural program synthesis. In: ICLR
’18 (2018), https://openreview.net/forum?id=H1Xw62kRZ

[9] Champion, A., Chiba, T., Kobayashi, N., Sato, R.: ICE-based reﬁnement
type discovery for higher-order functional programs. In: TACAS ’18. LNCS,
vol. 10805, pp. 365–384. Springer (2018)

[10] Champion, A., Kobayashi, N., Sato, R.: HoIce: An ice-based non-linear horn

clause solver. In: APLAS ’18. pp. 146–156. Springer (2018)

[11] Chen, J., Wei, J., Feng, Y., Bastani, O., Dillig, I.: Relational veriﬁcation
using reinforcement learning. Proceedings of the ACM on Programming
Languages 3(OOPSLA) (2019)

[12] Chen, Y., Wang, C., Bastani, O., Dillig, I., Feng, Y.: Program synthesis
using deduction-guided reinforcement learning. In: CAV ’20. pp. 587–610.
Springer, Cham (2020)

[13] Evans, R., Saxton, D., Amos, D., Kohli, P., Grefenstette, E.: Can neu-
ral networks understand logical entailment? In: ICLR ’18. OpenReview.net
(2018)

[14] Ezudheen, P., Neider, D., D’Souza, D., Garg, P., Madhusudan, P.: Horn-
ICE learning for synthesizing invariants and contracts. Proceedings of the
ACM on Programming Languages 2(OOPSLA), 131:1–131:25 (Oct 2018)

[15] Fedyukovich, G., Kaufman,

ants

from frequency

S.J., Bod´ık, R.:
In:

invari-
Sampling
2017 Formal Methods

distributions.

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

21

in Computer Aided Design
(FMCAD).
https://doi.org/10.23919/FMCAD.2017.8102247

pp.

100–107

(2017).

[16] Fedyukovich, G., Prabhu, S., Madhukar, K., Gupta, A.: Solving constrained
horn clauses using syntax and data. In: FMCAD ’18. pp. 1–9 (2018).
https://doi.org/10.23919/FMCAD.2018.8603011

[17] Garg, P., L¨oding, C., Madhusudan, P., Neider, D.: ICE: A robust framework

for learning invariants. In: CAV ’14. pp. 69–87. Springer (2014)

[18] Garg, P., Neider, D., Madhusudan, P., Roth, D.: Learning invariants using
decision trees and implication counterexamples. In: POPL ’16. pp. 499–512.
ACM (2016)

[19] Hinton, G., Srivatava, N., Swersky, K.: Lecture 6e: rmsprop: Divide the
gradient by a running average of its recent magnitude (2012),
lecture
notes available from http://www.cs.toronto.edu/~hinton/coursera/
lecture6/lec6.pdf

[20] Hojjat, H., R¨ummer, P.: The Eldarica horn solver. In: FMCAD ’18. pp. 1–7.

IEEE (2018)

[21] Irving, G., Szegedy, C., Alemi, A.A., E´en, N., Chollet, F., Urban, J.: Deep-
Math - deep sequence models for premise selection. In: NIPS ’16. pp. 2235–
2243 (2016)

[22] Iyer, S., Konstas, I., Cheung, A., Zettlemoyer, L.: Summarizing source code
using a neural attention model. In: ACL ’16. The Association for Computer
Linguistics (2016)

[23] Kaliszyk, C., Chollet, F., Szegedy, C.: HolStep: A machine learning dataset
for higher-order logic theorem proving. In: ICLR ’17. OpenReview.net
(2017)

[24] Kaliszyk, C., Urban, J., Michalewski, H., Ols´ak, M.: Reinforcement learning

of theorem proving. In: NeurIPS ’18. pp. 8836–8847 (2018)

[25] Kaliszyk, C., Urban, J., Michalewski, H., Olˇs´ak, M.: Reinforcement
learning of theorem proving. In: Bengio, S., Wallach, H., Larochelle,
H., Grauman, K., Cesa-Bianchi, N., Garnett, R.
(eds.) Advances
Information Processing Systems. vol. 31. Curran As-
in Neural
sociates,
Inc. (2018), https://proceedings.neurips.cc/paper/2018/
file/55acf8539596d25624059980986aaa78-Paper.pdf

[26] Kalyan, A., Mohta, A., Polozov, O., Batra, D., Jain, P., Gulwani, S.: Neural-
guided deductive search for real-time program synthesis from examples. In:
ICLR ’18 (2018), https://openreview.net/forum?id=rywDjg-RW

[27] Komuravelli, A., Gurﬁnkel, A., Chaki, S.: SMT-based model checking for
recursive programs. In: CAV ’14. LNCS, vol. 8559, pp. 17–34. Springer
(2014)

[28] Konda, V., Tsitsiklis, J.: Actor-critic algorithms. In: Solla, S., Leen, T.,
M¨uller, K. (eds.) Advances in Neural Information Processing Systems.
vol. 12. MIT Press (2000)

[29] Krishna, S., Puhrsch, C., Wies, T.: Learning invariants using decision trees.

CoRR abs/1501.04725 (2015)

[30] Kura, S., Unno, H., Hasuo, I.: Decision tree learning in CEGIS-based ter-

mination analysis. In: CAV ’21. pp. 75–98. Springer (2021)

22

M. Wu et al.

[31] Kusumoto, M., Yahata, K., Sakai, M.: Automated theorem proving in
logic by deep reinforcement learning. CoRR

intuitionistic propositional
abs/1811.00796 (2018)

[32] Lederman, G., Rabe, M., Seshia, S., Lee, E.A.: Learning heuristics for quan-
tiﬁed boolean formulas through reinforcement learning. In: ICLR ’20 (2020)
[33] Liang, J.H., Oh, C., Mathew, M., Thomas, C., Li, C., Ganesh, V.: Machine
learning-based restart policy for CDCL SAT solvers. In: SAT ’18. pp. 94–
110. Springer (2018)

[34] Loos, S.M., Irving, G., Szegedy, C., Kaliszyk, C.: Deep network guided
proof search. In: LPAR ’17. EPiC Series in Computing, vol. 46, pp. 85–105.
EasyChair (2017)

[35] Mou, L., Li, G., Zhang, L., Wang, T., Jin, Z.: Convolutional neural networks
over tree structures for programming language processing. In: AAAI ’16. pp.
1287–1293. AAAI Press (2016)

[36] de Moura, L., Bjørner, N.: Z3: An eﬃcient SMT solver. In: TACAS ’08.

LNCS, vol. 4963, pp. 337–340. Springer (2008)

[37] Padhi, S., Millstein, T.D., Nori, A.V., Sharma, R.: Overﬁtting in synthesis:
Theory and practice. In: CAV ’19. LNCS, vol. 11561, pp. 315–334. Springer
(2019)

[38] Paliwal, A., Loos, S.M., Rabe, M.N., Bansal, K., Szegedy, C.: Graph rep-
resentations for higher-order logic and theorem proving. In: AAAI ’20. pp.
2967–2974. AAAI Press (2020)

[39] Reynolds, A., Barbosa, H., N¨otzli, A., Barrett, C., Tinelli, C.: cvc4sy: Smart
and fast term enumeration for syntax-guided synthesis. In: CAV ’19. pp. 74–
83. Springer, Cham (2019)

[40] Ryan, G., Wong, J., Yao, J., Gu, R., Jana, S.: CLN2INV: learning loop
invariants with continuous logic networks. In: ICLR ’20. OpenReview.net
(2020)

[41] Satake, Y., Unno, H., Yanagi, H.: Probabilistic inference for predicate con-

straint satisfaction. AAAI ’20 34(02), 1644–1651 (Apr 2020)

[42] Sekiyama, T., Suenaga, K.: Automated proof synthesis for the minimal
propositional logic with deep neural networks. In: APLAS ’18. LNCS, vol.
11275, pp. 309–328. Springer (2018)

[43] Selsam, D., Bjørner, N.: Guiding high-performance SAT solvers with unsat-
core predictions. In: SAT ’19. LNCS, vol. 11628, pp. 336–353. Springer
(2019)

[44] Selsam, D., Lamm, M., B¨unz, B., Liang, P., de Moura, L., Dill, D.L.: Learn-
ing a SAT solver from single-bit supervision. In: ICLR ’19. OpenReview.net
(2019)

[45] Si, X., Dai, H., Raghothaman, M., Naik, M., Song, L.: Learning loop in-
variants for program veriﬁcation. In: NeurIPS ’18. pp. 7762–7773. Curran
Associates, Inc. (2018)

[46] Si, X., Naik, A., Dai, H., Naik, M., Song, L.: Code2Inv: A deep learning
framework for program veriﬁcation. In: CAV ’20. pp. 151–164. Springer
(2020)

Learning Heuristics for Template-based CEGIS with Reinforcement Learning

23

[47] Singh, S.P., Sutton, R.S.: Reinforcement learning with replacing eligibility

traces. Mach. Learn. 22(1-3), 123–158 (1996)

[48] Solar-Lezama, A., Tancau, L., Bodik, R., Seshia, S., Saraswat, V.: Combi-
natorial sketching for ﬁnite programs. In: ASPLOS XII. pp. 404–415. ACM
(2006)

[49] Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. The

MIT Press, second edn. (2018)

[50] Unno, H., Satake, Y., Terauchi, T., Koskinen, E.: Program veriﬁcation via
predicate constraint satisﬁability modulo theories. CoRR abs/2007.03656
(2020), https://arxiv.org/abs/2007.03656

[51] Unno, H., Terauchi, T., Koskinen, E.: Constraint-based relational veriﬁca-

tion. In: CAV ’21. pp. 742–766. Springer (2021)

[52] Wang, M., Tang, Y., Wang, J., Deng, J.: Premise selection for theorem
proving by deep graph embedding. In: NIPS ’17. pp. 2786–2796 (2017)
[53] Winskel, G.: The Formal Semantics of Programming Languages: An Intro-

duction. MIT Press (1993)

[54] Wu, M., Norrish, M., Walder, C., Dezfouli, A.: Tacticzero: Learning to
prove theorems from scratch with deep reinforcement learning. CoRR
abs/2102.09756 (2021), https://arxiv.org/abs/2102.09756

[55] Zhu, H., Magill, S., Jagannathan, S.: A data-driven CHC solver. In: PLDI

’18. pp. 707–721. ACM (2018)

