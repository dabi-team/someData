Jointly Optimizing Dataset Size and Local Updates
in Heterogeneous Mobile Edge Learning

Umair Mohammad∗, Sameh Sorour†, Mohamed Hefeida‡
∗School of Computing and Information Sciences, Florida International University, Miami, FL, USA
†School of Computing, Queen’s University, Kingston, ON, Canada
‡Department Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA
E-mails: umair.mohammad@ﬁu.edu, sameh.sorour@queensu.ca, mohamed.hefeida@mail.wvu.edu

1
2
0
2

b
e
F
2
2

]
P
S
.
s
s
e
e
[

3
v
2
0
4
7
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—This paper proposes to maximize the accuracy of a
distributed machine learning (ML) model trained on learners
connected via the resource-constrained wireless edge. We jointly
optimize the number of local/global updates and the task size
allocation to minimize the loss while taking into account het-
erogeneous communication and computation capabilities of each
learner. By leveraging existing bounds on the difference between
the training loss at any given iteration and the theoretically
optimal loss, we derive an expression for the objective function
in terms of the number of local updates. The resulting convex
program is solved to obtain the optimal number of local updates
which is used to obtain the total updates and batch sizes
for each learner. The merits of the proposed solution, which
is heterogeneity aware (HA), are exhibited by comparing its
performance to the heterogeneity unaware (HU) approach.

I. INTRODUCTION

Rapid migration towards smart
infrastructure (cities, cars,
grids, etc.) has caused an explosion of Internet-of-Things (IoT)
devices on resource-constrained wireless edge networks. A
recent article reported reports that about every second another
127 devices connect to the internet with 41 billion devices
expected by 2027 and Cisco expects that 800 zettabytes of
data will be generated [1] on wireless edge networks. The
distributed nature of this data will place a heavy ﬁnancial bur-
den on backbone networks and raise security/privacy concerns
[2]. Thus, it is anticipated that edge servers and end devices
(e.g. smart phones, cameras, drones, connected vehicles, etc.)
will perform 90% of the data processing locally [3].

Machine Learning (ML) techniques have shown to perform
better in many data analytics applications such as forecasting,
image classiﬁcation, clustering, etc. Many ML techniques,
including regression, support vector machine (SVM) and
neural networks (NN) are built on gradient-based learning.
This usually involves optimizing the model parameters by
iteratively adding to them the gradient of the loss, itself a
function of the model parameters. In the distributed learning
model considered in this paper, a central server called an
orchestrator initiates the learning process on multiple learners
where each learner performs the ML iterations on the local
dataset, collects the local ML models from each learner, does
the global update/aggregation, and sends back the optimal ML
model for the next cycle until a stopping criteria is reached.

With the advent of Edge Artiﬁcial Intelligence (Edge AI),
deploying ML models over end devices at edge networks will
soon be the norm. Therefore, researchers have turned their
focus to performing machine learning (ML) in a distributed
manner (a.k.a. distributed learning (DL)) at the edge [4]–[9]
in order to support edge analytics. In general, DL at the edge
can be characterized by mobile edge learning (MEL) though
the most commonly studied setup is federated learning (FL).

The works of [5], [6], [8], [9] focus on jointly optimizing
the number of local learning and global update cycles for
FL. However, their approaches do not consider the inherent
heterogeneity in the computing and communication capacities
of different edge learners and links, respectively. Although
the works of [10], [11] have optimized resource allocation
while maintaining accuracy, they do not investigate the impact
of batch allocation. The implications of wireless computa-
tion/communication heterogeneity on optimizing batch allo-
cation to different learners for maximizing accuracy while
satisfying a delay constraint were studied in [7].

To the best of the authors’ knowledge, this work is the ﬁrst
attempt at jointly optimizing batch size allocated to learners
and synchronizing the number of local and total iterations
of the ML algorithm across all
learners while satisfying
time delay constraints for the MEL paradigm. Therefore, our
proposed work differs from existing literature in the following
ways: 1) considers the impact of batch size allocation 2)
models the MEL system on actual channel parameters and
device capabilities as opposed to generic resource consumption
3) optimizes both, the number of local and total updates as
opposed to maximizing the local updates per global update.

The superiority of our proposed heterogeneity aware (HA)
approach is shown by comparing its performance to the
heterogeneity-unaware (HU) approach of [5], [6]. Tests on
classifying the MNIST dataset [12] using a DNN show that
the HA approach is superior in terms of achieving a lower loss
and providing higher validation accuracy. The rest of the paper
is organized as follows: Section 2 introduces the global MEL
model with time constraints. The problem of interest in this
paper is formulated in Section 3 and our proposed solution
is described in Section 4. Section 5 presents the results and
Section 6 concludes the paper.

 
 
 
 
 
 
dataset. This approach has been more commonly studied in
literature as opposed to OL. FL is just a subset of the OL
approach with the component of batch re-transmission from
the orchestrator to each learner k removed. Thus, the MEL
model discussion will focus on the more general ofﬂoaded
learning scenario but all variations for the federated learning
scenario will be clariﬁed whenever needed.

k

We deﬁne Bdata
k = dkFPd as the size of the batch allocated
to learner k in bits and Bmodel
= Pm (dkSd + Sm) as the
size of the model in bits ∀k ∈ K. The variables Pd and Pm
represent precision with which the data and model are stored,
respectively, F represents the feature vector size of xn for
n = 1, . . . , d. Sm represents size of w as deﬁned by the ML
model whereas Sd is the proportion of the model dependent
on the dataset size.

Between any two global update cycles, the orchestrator, which
owns the global model, sends the data and model parameters w
to each learner k in parallel1, waits for all learners to complete
τ local
learning iterations, and then receives the locally
updated model wk∀k ∈ K followed by global aggregation.
The communication of the models (and data) between the
orchestrator and each learner k occurs over a channel having
a bandwidth W , a channel power gain hko and with a noise
power spectral density of N0. We assume that Pko = Pok over
one iteration of the global update and that channel parameters
remain constant during global aggregation.

learner k ∀ k ∈ K has a local processor
Furthermore,
resource fk dedicated to the DL task for an ML model of
complexity Cm that requires Xk clock cycles to perform one
local iteration. Given the above descriptions, the times of each
learner k, ∀ k comprise: orchestrator transmission time tS
k
needed to send w and dk data samples2 to learner k, the
duration tC
k needed by learner k to perform one local update
cycle, and time tR
k is the one needed for learner k to send
its updated local parameter matrix ˜wk to the orchestrator. The
times tS

k ,tC

k , and tR

k , respectively, can be expressed as:
dkFPd + Pm (dkSd + Sm)

tS
k =

(cid:16)

W log2

1 + Pkohko

N0

(cid:17)

tC
k =

Xk
fk

=

dkCm
fk

tR
k =

Pm (dkSd + Sm)
1 + Pkohko

(cid:16)

W log2

N0

(cid:17)

(1)

(2)

(3)

III. PROBLEM FORMULATION

As mentioned in Section I, the objective of this paper is to
optimize the task allocation, i.e. the distributed batch sizes dk
for each learner k and the associated τ updates to be performed

1In the FL scenario, the orchestrator only sends the global model w to

each learner k ∈ K. Learner k selects dk samples from its private dataset.

2For the FL scenario, the only difference in the model is that the ﬁrst

term of the numerator will not exist.

Fig. 1. System model of a MEL setting

II. MEL SYSTEM MODEL

A. Gradient-based Learning Preliminaries

Consider a dataset that consists of d samples that can be
trained using ML where each sample n for n = 1, . . . , d
has a set of F features denoted by xn and a target yn. The
objective is to ﬁnd the relationship between xn and yn using
a set of parameters w such that a loss function, F (xn, yn, w)
(or F (w) for short), is minimized. Because it is generally
difﬁcult to ﬁnd an analytical solution, typically an iterative
gradient descent approach is used to optimize the set of model
parameters such that w[l + 1] = w[l] − η∇F (w[l]) where l
represents the time step or iteration and η is the learning rate
typically set on the interval (0, 1). In deterministic gradient
descent (DGD), the ML model goes over each sample one-
by-one, or more commonly, batch-by-batch using a mini-batch
approach, until it reaches sample # d; completing one epoch.
If data is re-shufﬂed randomly in between epochs, this method
is know as stochastic GD (SGD). A total of L epochs may be
performed depending on the stopping criteria.

B. Transition to MEL

An MEL system consists of an orchestrator and K learners
where dk data samples are allocated to learner k, k ∈ K =
{1, 2, . . . , K} so that it performs τ learning iterations. Each
learner has a computational capacity of fk in Hz and an
associated communication channel hk0 to the orchestrator. An
example of a DL system is illustrated in ﬁg. 1. We assume
that K perfectly orthogonal channels exist.

DL as described in section I in an MEL setting gives rise
to two possibilities: ofﬂoaded learning (OL) and federated
learning (FL). In the former, the orchestrator has the complete
dataset and also re-transmits optimally allocated batches from
the randomly shufﬂed dataset back to the learners. In the
latter, the orchestrator only informs the learner of how many
iterations to do and on what sample size of a locally stored

locally for a total of L updates, such that the global DL loss
is minimized and thus, the accuracy is maximized. To this
end, the problem is formulated as a loss-function minimization
problem over the optimization variables L, τ and dk.

edge. For more details on these bounds, the readers are referred
to [5]. We will use the results and extend the discussion to
our formulation, and then propose a strategy to jointly ﬁnd
the optimal τ , dk, and L.

Consider that after every τ local iterations, a global aggrega-
tion will be performed and a total of G global aggregations
are performed. In any global update cycle, between any learner
k ∈ K the orchestrator O, there will be one communication
round each and τ local updates. For now, to facilitate the
analysis, let us assume that L is an integer multiple of τ such
that L = Gτ and that the communication and computation re-
lated parameters remain unchanged over the complete training
process. In that case, each learner needs time tC
k ∀ k ∈ K for
one local update and tS
k for the gth global aggregation for
k +tR
g = 1, . . . , G. Overall, L local updates and G = L/τ global
updates will be performed. Then,the total time consumed by
learner k denoted by tk ∀k ∈ K can be expressed as:
(cid:18)

(cid:19)

tk = L

tC
k +

k + tR
tS
k
τ

(4)

Later on, we will shoe how the values of τ and dk for
each set of τ local ML iterations in one global cycle g will
be re-calculated according to the latest channel parameters
and computational capabilities. The total training time within
which the process should be completed is given bounded by T .
Because the τ iterations occur in parallel over the K learners,
we need the time for the most time-consuming learner to
be less than T such that max(tk) ≤ T . Alternatively, it is
sufﬁcient for this condition to hold that tk ≤ T ∀k ∈ K.

This point differentiates our work from that of [5] where we
actually capture the time consumed by parallel local update
processes rather than a generic resource consumption model.
Therefore, the optimization problem can be written as:

min
L,τ,dk∀ k

F (w[L])

(5)

s.t.

(cid:18)

L

C 2

kdk +

(cid:19)

C 1

kdk + C 0
k
τ

≤ T,

k ∈ K

The constants C 2

k,C 1

k,and C 0

k can be deﬁned as:

C 2

k =

Cm
fk

C 1

k =

C 0

k =

FPd + 2PmSd

(cid:16)

W log2

1 + Pkohko

N0

2PmSm
(cid:16)

W log2

1 + Pkohko

N0

(6a)

(6b)

(6c)

(cid:17)

(cid:17)

It is generally impossible to ﬁnd an exact expression relating
the optimization variables to the objective for most ML
models. Therefore, the objective will be re-formulated as a
function of the convergence bounds on the DL process over the

A. Convergence Bounds

The convergence bounds have been derived and well-discussed
in [5]. For completeness, we will present some of the important
results here in order to support our analysis. Let us continue
with the assumption that L is an integer multiple of τ . Then,
the global aggregation will only occur at every τ updates.
I.e. the local updates occur at every iteration l = 1, . . . , L
and a global update will occur whenever l = gτ for g =
1, . . . , G. For any interval [g] deﬁned over [(g−1)τ, gτ ], deﬁne
an auxiliary global model denoted by v which would have
been calculated if a global update occurred as follows:

v[g][l] = v[g][l − 1] − η∇F (v[g][l − 1])

(7)

Let the local model parameter set of learner k be denoted by
wk and the local loss by Fk(wk). Then, the optimal model at
iteration l can be obtained by:

w[l] =

1
d

K
(cid:88)

dkwk[l]

(8)

k=1
The optimal w[l] will only be visible when l = gτ and for
that iteration, the global loss can be deﬁned by:

F (w) =

1
d

K
(cid:88)

k=1

dkFk(w)

(9)

The following assumptions are made about the loss function
Fk(w) at learner k: Fk(w) is convex, (cid:107)Fk(w) − Fk( ¯w)| ≤
ρ|w− ¯w|, and (cid:107)∇Fk(w)−∇Fk( ¯w)| ≤ β|w− ¯w| for any w, ¯w.
These assumptions will hold for ML models with convex loss
function such as linear regression and SVM. By simulations,
we will show that the proposed solutions work for non-convex
models such as the neural networks with ReLU activation.

(cid:80)

Let us also assume that the local loss function at Fk(w) does
not diverge by more than δk such that |Fk(w) − F (w)| ≤ δk
k dkδk
and δ =
. Furthermore, |w − v[g][l − 1]| ≤ h(l − (g −
d
β [(ηβ + 1)τ − 1] − ηδτ . Recall that
1)τ ). For any τ , h(τ ) = δ
k dkβk
d

η is the learning rate and β can be estimated by β =
where:

(cid:80)

βk =

(cid:107)∇Fk(wk[l]) − ∇Fk(w[l])|
|wk[l] − w[l]|

(10)

Based on this, the objective can be written as a function of
the difference of the global loss after iteration L and the
optimal global loss. Given the above assumptions about the
loss function, and the constraints on the optimization variables
and the time taken by learner k ∈ K, the optimization problem

can be written as:

Thus, the problem in (11) can be re-formulated as:

min
L,τ,dk∀ k

F (w[L]) − F (w∗)

(11a)

min
L,τ,dk ∀ k

L

(cid:104)
η(1 − βη

1
2 ) − ρ

ω(cid:15)2

(cid:105) (12a)

h(τ )
τ

s.t.

(cid:18)

L

C 2

kdk +

(cid:19)

C 1

kdk + C 0
k
τ

≤ T,

k = 1, . . . , K

K
(cid:88)

dk = d

k=1
τ ∈ Z+
L ∈ Z+
dk ∈ Z+,
βη
2

η(1 −

k = 1, . . . , K

) −

ρ
ω(cid:15)2

h(τ )
τ

≥ 0

ηβ ≤ 1
F (cid:0)v[g][l](cid:1) − F (w∗) ≥ (cid:15)
F (w(L) − F (w∗)) ≥ (cid:15)

(11b)

(11c)

(11d)

(11e)

(11f)

(11g)

(11h)

(11i)

(11j)

Constraint (11b) guarantees that the time consumed by a total
of L updates does not exceed the total training time available
given by T seconds. Constraint (11c) ensures that the total
dataset comprising d samples is utilized. Constraints (11d)
- (11f) are simply non-negativity and integer constraints for
the optimization variables where L, τ and/or all dk’s being
zero represent cases where DL is not possible in the MEL
environment. Constraints (11h) and (11g) represent a bound
on the learning rate, meaning it should be small enough such
that it guarantees convergence. When (11h) holds, (11g) will
always hold. Constraints (11i) and (11j) deﬁne a lower bound
on the gap between the optimal loss and the auxiliary loss
at interval [g] and the global loss, respectively, where (cid:15) > 0.
The parameter ω (cid:44) ming(cid:107)v[g][g − 1]τ − w∗(cid:107)−2 represents the
interval that minimizes the difference between the auxiliary
loss and the global loss. The variables ρ, ω, and (cid:15) appear in a
single term ρ
ω(cid:15)2 in (11g) which represents a control parameter.
Later on, this term will be represented by B0 but for now,
we will continue with the original terms make the analysis
relatable to the original variables.

s.t.

L ≤

K
(cid:88)

T τ
kτ dk + C 1

C 2

kdk + C 0
k

dk = d

k=1
τ ∈ Z+
L ≥ 0
dk ≥ 0,

k = 1, . . . , K

,

k = 1, . . . , K (12b)

(12c)

(12d)

(12e)
(12f)

Note that the integer constraints on dk and L have been
relaxed in (12f) and (12e) which will help in proposing a
solution.

IV. PROPOSED SOLUTION

The idea of the proposed solution is to re-write the objective
as a function of τ by using the constraints on the total time
consumption and the fact that the system must train the model
on at least d training samples.

A. Relating Bounds to τ

The orchestrator can ensure that the bounds in constraints
(11h)-(11j) are satisﬁed by choosing small enough values for
η and (cid:15). In that case, if constraint (11g) holds, the denominator
of the objective function will be positive. Furthermore, if we
relax the integer constraint on L, the optimal value for the
total learning iterations can be given by:

T τ
kτ dk + C 1

L =

,

k = 1, . . . , K

(13)

C 2

kdk + C 0
k
By using the equality constraint in (12c), and re-arranging
(13) to make dk the subject, and deﬁning two new variables
ak = C1
k
C2
k

, we can write L as a function of τ .

and bk = C0
k
C2
k
KT (cid:80)K
k=1 τ (cid:81)K
k=1(τ + bk) + (cid:80)K

d (cid:81)K

l=1
l(cid:54)=k
k=1 ak

(τ +bl)

L(τ ) =

(cid:81)K

l=1
l(cid:54)=k

(τ +bl)

(14)

It is assumed that η > 0 (typically 0 < η < 1) and β > 0.
Furthermore η and (cid:15) can be set to small enough values such
that ηβ ≤ 1, and the constraints in (11g)-(11j) are satisﬁed.
For a β-smooth function, Bernoulli’s inequality will hold
implying that (ηβ + 1)τ ≥ ηβτ + 1. Furthermore, once all the
assumptions about the loss function constraints are satisﬁed,
it can be shown that F (w[L]) − F (w∗) ≤
.

η(1− βη

1
2 )− ρ
ω(cid:15)2

h(τ )
τ

The objective function denoted by O can be re-written as a
function of τ in the following manner:

O(τ ) =

1
L(τ )

(cid:104)

1
2 ) − ρ

ω(cid:15)2

η(1 − βη

(cid:105) =

P (τ )
L(τ )

h(τ )
τ

(15)

Theorem 1: O(τ ) is strictly convex on the domain τ ≥ 0.

Proof: Please refer to Appendix A for the proof.

(cid:4)

Because ∂O
∂τ = 0 does not have a closed form solution, the
optimal τ ∗ can be obtained by solving the following problem:

τ ∗ = arg min

τ

O(τ )

(16)

TABLE I
LIST OF SIMULATION PARAMETERS

Parameter

Value
128 + 37.1 log(R) dB [13]
5 MHz
500m
23 dBm
-174 dBm/Hz

Cell Attenuation Model
Node Bandwidth (W )
Device proximity (R)
Transmission Power (Pk)
Noise Power Density (N0)
Computation Capabilities (fk) ∼ { 2.4, 1.2} GHz
MNIST Dataset size (d)
MNIST Dataset Features (F)

54,000 images
784 ( 28 × 28 ) pixels

Algorithm 1 MEL Process at the Orchestrator
Input: T , B0, τmax, d, K
Output: w[L]

Initialize τ ← 1, dk ← d
Set w ← w[0] as a random vector
Parallel Process
1: while τ > 0 do
2:

K , L = 1, and ˆT ← max(tk)

Send w and dk samples to each learner k
After τ local iterations, collect wk, βk, and ∇Fk(wk)
Estimate w,∇F (w), β, and δ
Receive PkO, hkO and fk from each learner k ∀ k ∈ K

Set T ← T − ˆT
Find the optimal τ by using theorem 1
Calculate L using (14)
Use new value of L to obtain dk ∀ k using (13)
Let ˆT ← ˆT + max(tk)
if ˆT > T then

Reduce τ to maximum value ≥ 0 such that ˆT ≤ T

3:
4:
5:

6:
7:

8:
9:
10:
11:
12:
13:
14: end while
15: return w

end if

The value of τ ∗ can be difﬁcult
to obtain because τ is
unbounded. However, we can limit the search space by τmax
and then use a brute force approach to ﬁnd the optimal τ ∗.
In fact, a binary search procedure has been proposed in [5]
which has a complexity of O(log τmax). Once τ ∗ has been
determined, L∗ can be obtained using (14) and re-set to this
new value. The values of d∗
k ∀ k ∈ K for the next τ updates
can be obtained using (13). Because the integer constraint
on dk ∀ k ∈ K was relaxed in (12), they can be set by
ﬂooring the actual value. This process is repeated for each
global cycle until the total training time is consumed. This
process is summarized in algorithm 1.

V. SIMULATION RESULTS

A. Simulation Environment, Dataset, and Learning Model

The learners are assumed to be located in a cellular type
environment and are assumed to be a combination of smart
phone and Raspberry PI type micro-controllers. The channel
parameters and device capabilities are listed in table 1. To test
our proposed MEL paradigm, the commonly used MNIST [12]
dataset is trained using a DNN with 3 hidden layers consisting
of 300, 124 and 60 neurons, respectively. The details of the
resulting model sizes and complexities are discussed in [7].

For the simulation, we consider a set of K = 20 learners and
test for total training times of T = {300, 400, 500, 600}s. It
was found that a value of η = 0.01 for the learning rate works
very well and setting B0 in the range 0.005 − 0.01 provided
solutions that converge. τmax is set to the case where only 3
global aggregations would be done on dk = d/K ∀k ∈ K.
1) Loss and Validation Accuracy
We plot the ﬁnal loss value after training for time T in ﬁgure
2 and the ﬁnal accuracy in ﬁgure 3 for both approaches,

Fig. 2. Training loss versus total training time for K = 20 learners

the proposed HA approach and the HU approach in [5]. As
expected, as the training time is increased, the loss value
decreases for all approaches. However, for the HA approach,
there is just a slight increase in validation accuracy because it
is able to achieve that in minimum time. The main conclusion
is that optimizing τ and dk jointly inﬂuences the possible
number of global aggregations G and the total iterations L
which helps in converging to a lower loss and a higher ﬁnal
accuracy. For example, the loss of the HA approach is lower
by 0.03-0.05 which represents gains in the range of 27% -
40%. Furthermore, training for 300s using the HA approach
achieves a 97% accuracy within 300s of training, a value not
achieved by the HU approach even in 600s.

VI. CONCLUSION

This paper extends the efforts towards the MEL paradigm
by jointly optimizing the task size allocation for each learner
and the number of local ML iterations in a global cycle for
distributed ML over the wireless edge. The problem uses
existing bounds on the DL paradigm to relate the optimization
variables to the loss function which is shown to be convex.
It is shown that optimal value of the local updates minimizes
the upper bound on the loss difference and the total iterations
and batch sizes after every global step. A heuristic approach
is proposed to carry out the global learning process. Through
simulations, it shown that our HA scheme performed much
better in terms of the possible number of updates and learning
accuracy compared to the HU scheme.

300400500600Total Training Time (s)00.020.040.060.080.10.120.14LossHAHU[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
Learning Applied to Document Recognition,” Proceedings of IEEE,
vol. 86, no. 11, pp. 2278 – 2324, 1998.
[Online]. Available:
https://ieeexplore.ieee.org/document/726791

[13] U. Y. Mohammad and S. Sorour,

for Hierarchical Mobile Edge Computing,”

IEEE Global Communications Conference: Mobile

Optimization
2018
Wireless Networks
dec
Arab
https://ieeexplore.ieee.org/document/8648109

“Multi-Objective Resource
in
and
(Globecom2018 MWN), Abu Dhabi, United
[Online]. Available:

Emirates,

2018,

1–6.

pp.

APPENDIX

A. PROOF OF THEOREM 1

The objective function O is strictly convex under certain
conditions because ∂2O
∂τ 2 > 0. Because τ is a positive integer,
the optimal value τ ∗ is the argument that minimizes O(τ ).
The reciprocal of L(τ ) in (15) can be separated into two terms
M (τ ) and N (τ ) as follows:

M (τ ) =

d
KT

N (τ ) =

1
KT

k=1(τ + bk)

(cid:80)K

(cid:81)K
k=1 τ (cid:81)K
(cid:81)K

k=1 ak
k=1 τ (cid:81)K

l=1
l(cid:54)=k

l=1
l(cid:54)=k

l=1
l(cid:54)=k

(cid:80)K

(cid:80)K

(τ +bl)

(τ +bl)

(τ +bl)

(17a)

(17b)

Therefore, the objective function can be re-written as O(τ ) =
O1(τ ) + O2(τ ) where O1(τ ) = M (τ )P (τ ) and O2(τ ) =
N (τ )P (τ ). Moreover, the term P (τ ) can be written as the
reciprocal of ν(τ ) where

ν(τ ) = A − B

(cid:16)

C τ − 1 − (C − 1)τ
τ
, B = δ
β

(cid:17)

The constants A = η
We can say that B = δ
parameter that can be set empirically.

1 − βη
2
β B0 where B0 = ρ

ρ
ω(cid:15)2 and C = ηβ + 1.
ω(cid:15)2 > 0 is a control

(18)

Fig. 3. Validation accuracy versus total training time for K = 20 learner

REFERENCES

[1] K. Gyarmathy, “Comprehensive Guide to IoT Statistics You Need to
Know in 2020,” 2020. [Online]. Available: https://www.vxchnge.com/
blog/iot-statistics

[2] M. Chiang and T. Zhang, “Fog and IoT: An Overview of Research
Opportunities,” IEEE Internet of Things Journal, vol. 3, no. 6,
pp. 854–864, dec 2016. [Online]. Available: http://ieeexplore.ieee.org/
document/7498684/

[3] Rhea Kelly, “Internet of Things Data To Top 1.6 Zettabytes by 2020,”
2015. [Online]. Available: https://campustechnology.com/articles/2015/
04/15/internet-of-things-data-to-top-1-6-zettabytes-by-2020.aspx

[4] S. Teerapittayanon, B. McDanel, and H. T. Kung, “Distributed Deep
Neural Networks over the Cloud, the Edge and End Devices,” Proceed-
ings - International Conference on Distributed Computing Systems, pp.
328–339, 2017.

[5] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He,
and K. Chan, “When Edge Meets Learning : Adaptive Control for
Resource-Constrained Distributed Machine Learning,” in INFOCOM,
2018. [Online]. Available: https://ieeexplore.ieee.org/document/8486403

[6] ——,

“Adaptive Federated Learning

in Resource Constrained
Edge Computing Systems,” IEEE Journal on Selected Areas in
Communications, no. Early Access, pp. 1–1, 2019. [Online]. Available:
https://ieeexplore.ieee.org/document/8664630/

[7] U. Mohammad and S. Sorour, “Adaptive Task Allocation for
Mobile Edge Learning,” in 2019 IEEE Wireless Communications and
Networking Conference Workshop (WCNCW).
IEEE, apr 2019, pp.
1–6. [Online]. Available: https://ieeexplore.ieee.org/document/8902527/

[8] D. Conway-Jones, T. Tuor, S. Wang,

and K. K. Leung,
“Demonstration of Federated Learning in a Resource-Constrained
Networked Environment,” in 2019 IEEE International Conference
on Smart Computing (SMARTCOMP), 2019.
[Online]. Available:
https://ieeexplore.ieee.org/abstract/document/8784064

[9] C. Wu, L. Zhang, Q. Li, Z. Fu, W. Zhu, and Y. Zhang,
“Enabling Flexible Resource Allocation in Mobile Deep Learning
Systems,” IEEE Transactions on Parallel and Distributed Systems,
vol. 30, no. 2, pp. 346–360,
[Online]. Available:
https://ieeexplore.ieee.org/document/8434315/

feb 2019.

[10] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A Joint
Learning and Communications Framework for Federated Learning over
Wireless Networks,” arXiv e-prints, p. arXiv:1909.07972, sep 2019.
[Online]. Available: https://arxiv.org/abs/1909.07972

[11] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei,
“Energy Efﬁcient Federated Learning Over Wireless Communication
Networks,” arXiv e-prints, p. arXiv:1911.02417, nov 2019. [Online].
Available: https://arxiv.org/abs/1911.02417v1

For brevity, we will represent f (τ ) as f where f may be O,
O1, O2, M , N or P . We will also represent ∂f
∂τ 2 as
f (cid:48) and f (cid:48)(cid:48), respectively. Using this new notation, O(cid:48)(cid:48) can be
given as follows:

∂τ and ∂2f

O(cid:48)(cid:48) = P (M (cid:48)(cid:48) + N (cid:48)(cid:48)) + 2P (cid:48)(M (cid:48) + N (cid:48)) + (M + N )P (cid:48)(cid:48)

(19)

By deﬁnition M > 0 and N > 0 because they are related to
the time consumed by user k, and P > 0 assuming that the
constraint in (11g) holds. Hence, if we can show that each
of the three terms in (19) are strictly positive, then O will be
strictly convex. We need to show that P (cid:48)(cid:48) > 0 and M (cid:48)(cid:48)+N (cid:48)(cid:48) >
0. Furthermore, if we can show that M (cid:48) + N (cid:48) and P (cid:48) follow
the same sign, O(cid:48)(cid:48) > 0.

The ﬁrst derivatives of M , N , and P can be given by:

M (cid:48) = −

d
KT

(cid:80)K

k=1

bk
(τ +bk)2

(cid:16)(cid:80)K

k=1

τ
(τ +bk)

(cid:17)2

(20a)

300400500600Total Training Time (s)92939495969798Accuracy %HAHUN (cid:48) =

M (cid:48)
d

−

1
KT

k=1

2 (cid:80)K
(cid:16)(cid:80)K

k=1

bk
(τ +bk)3
(cid:17)2

τ
(τ +bk)

(20b)

as long as τ is a positive integer and the ML model variables
are selected as deﬁned.

P (cid:48) = −B

C τ [1 − (ln C)τ ] − 1
[Aτ − B(C τ − 1 − (C − 1)τ )]2

(20c)

and bk = C0
The variables ak = C1
, respectively, and both
k
k
C2
C2
k
k
are positive quantities. For M (cid:48), the ﬁrst term outside the square
bracket is always negative whereas the term inside is a sum of
positive quantities whereas N (cid:48) is a sum of negative quantities.
Therefore, M (cid:48) < 0 and N (cid:48) < 0. Hence, we need to show that
P (cid:48) < 0 or ﬁnd the domain on which P (cid:48) < 0.

The complete expressions of M (cid:48)(cid:48) and N (cid:48)(cid:48) can be given by:

M (cid:48)(cid:48) =

d
KT

(cid:16)(cid:80)K

1

(cid:17)2

τ
(τ +bk)

k=1
(cid:34)

2

K
(cid:88)

k=1

bk
(τ + bk)3 +

(cid:35)

K
(cid:88)

k=1

bk
(τ + bk)2

(21a)

N (cid:48)(cid:48) =

1
KT

K
(cid:88)

k=1

ak
(τ + bk)3 (cid:16)(cid:80)K

l=1

(cid:17)4

τ
(τ +bl)

(cid:34) K
(cid:88)

l=1

τ
τ + bl

+ 2(τ + bk)

K
(cid:88)

l=1

1
(τ + bl)2 +

(cid:35)

K
(cid:88)

l=1

2
(τ + bl)3

(21b)
It can be shown that M (cid:48)(cid:48) and N (cid:48)(cid:48) are strictly greater than zero
because they are both a sums of positive terms. Hence, the
left-most term in (19) is strictly positive. Thus we need to
show that P (cid:48)(cid:48) > 0 or ﬁnd the domain for which this is true.

Although the complete expression for P (cid:48)(cid:48)
is omitted for
brevity, it can be shown that the necessary and sufﬁcient
conditions to achieve P (cid:48) < 0 and P (cid:48)(cid:48) > 0 is to satisfy
C τ [1 − (ln C)τ ] > 1. From the Bernoulli
inequality, we
know that C τ ≥ (C − 1)τ + 1. Assuming the worst case
where the equality holds, the expression can be written as
[(C − 1)τ + 1][1 − (ln C)τ ] > 1. By expanding the expression,
we can show that we need to check the following inequality:

τ [(ln C)τ − C(ln C) − 1 + C] > 0

(22)

We know that as long as a feasible τ ∗ > 0 is found, we need
to satisfy the second term enclosed by the square brackets.
Writing τ as a function of C and, we can see that the condition
on τ is the following:

τ >

C(ln C) + 1 − C
ln C

(23)

Recall that C = ηβ + 1 where η is chosen such that ηβ ≤ 1,
and η, β > 0. Hence, it follows that 1 ≤ ηβ + 1 ≤ 2. If we
plot, f (C) = C(ln C)+1−C
against the domain of C, we notice
that for ν(cid:48)(cid:48) < 0 and hence, for P (cid:48) to be strictly negative and
P (cid:48)(cid:48) to be strictly positive, it is sufﬁcient for τ > 0. Hence, we
have now proved that O(τ ) is strictly convex because ∂2O
∂τ 2 > 0

ln C

