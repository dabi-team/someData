1
2
0
2

n
u
J

8
1

]

G
L
.
s
c
[

1
v
2
0
3
0
1
.
6
0
1
2
:
v
i
X
r
a

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

DEPENDENCY STRUCTURE MISSPECIFICATION IN
MULTI-SOURCE WEAK SUPERVISION MODELS

Salva R ¨uhling Cachay
Technical University of Darmstadt
salvaruehling@gmail.com

Benedikt Boecking
Carnegie Mellon University

Artur Dubrawski
Carnegie Mellon University

ABSTRACT

Data programming (DP) has proven to be an attractive alternative to costly hand-
labeling of data. In DP, users encode domain knowledge into labeling functions
(LF), heuristics that label a subset of the data noisily and may have complex
dependencies. A label model is then ﬁt to the LFs to produce an estimate of
the unknown class label. The effects of label model misspeciﬁcation on test set
performance of a downstream classiﬁer are understudied. This presents a serious
awareness gap to practitioners, in particular since the dependency structure among
LFs is frequently ignored in ﬁeld applications of DP. We analyse modeling errors
due to structure over-speciﬁcation. We derive novel theoretical bounds on the
modeling error and empirically show that this error can be substantial, even when
modeling a seemingly sensible structure.

1

INTRODUCTION

Annotating large datasets for machine learning is expensive, time consuming, and a bottleneck for
many practical applications of artiﬁcial intelligence. Recently, data programming, a paradigm that
makes use of multiple weak supervision sources, has emerged as a promising alternative to manual
data annotation (Ratner et al., 2016). In this framework, users encode domain knowledge into weak
supervision sources, such as domain heuristics or knowledge bases, that each noisily label a subset of
the data. A generative model over the sources and the latent true label is then learned. One can use
the learned model to estimate probabilistic labels to train a downstream model, replacing the need to
obtain ground truth labels by manual annotation of individual samples.
In practice, the sources of weak labels that users deﬁne often exhibit statistical dependencies amongst
each other, e.g. sources operating on the same or similar input (some examples can be found in
Table 1). Deﬁning the correct dependency structure is difﬁcult, thus a common approach in popular
libraries (Bach et al., 2019; Ratner et al., 2019a) and related research (Dawid & Skene, 1979;
Anandkumar et al., 2014; Varma & R´e, 2018; Boecking et al., 2021) is to ignore it. However, the
implications of this assumption on downstream performance have not been researched in detail.
Therefore, in this paper we take steps towards gaining a better understanding of the trade-offs
involved.

Contributions We present novel bounds on the label model posterior and the downstream gen-
eralization risk that are explicitly inﬂuenced by misspeciﬁed higher-order dependencies. We also
introduce three new higher-order dependency types, which we name bolstering, negated, and priority
dependencies. Lastly, we empirically show that downstream test performance is highly sensitive to
the user-speciﬁed dependencies, even when they make sense semantically. The ﬁnding suggests that
in practice, it is advisable to only carefully model a few, if any, dependencies.

2 RELATED WORK

Data programming While the original data programming framework (Ratner et al., 2016) is based
on a factor graph that support the modeling of arbitrary dependencies between LFs, more recent
methods for solving for the parameters of the label model only support the modeling of pairwise
correlations (Ratner et al., 2019a;b; Fu et al., 2020) — as such, losing some of the expressive power
of data programming. The former extends data programming to the multi-task setting by exploiting
the graph structure of the inverse covariance matrix among the sources (Ratner et al., 2019b) — in
particular the fact that an entry is zero when there is no edge between the corresponding sources in
the graphical model (Loh & Wainwright, 2012). The latter ﬁnds a closed-form solution for a class of

1

 
 
 
 
 
 
Published at the Workshop on Weakly Supervised Learning, ICLR 2021

binary Ising models by using triplet methods (Fu et al., 2020). Our experimental ﬁndings suggest that
practitioners may indeed beneﬁt from simply ignoring higher-order dependencies.
Structure learning In order to automatically learn the structure between these sources, previous
work optimizes the marginal pseudolikelihood of the noisy labels (Bach et al., 2017), or makes use
of robust PCA to denoise the inverse covariance matrix of the sources labels into a graph structured
term (Varma et al., 2019). A different approach, infers the structure through static analysis of the
weak supervision sources code deﬁnitions and thereby reduces the sample complexity for learning
the structure (Varma et al., 2017). In our experiments, we show that such methods should be carefully
used, and may in fact lead to downstream performance losses.
Model misspeciﬁcation On the side of work on model misspeciﬁcation, (White, 1982) establishes
that the Maximum Likelihood Estimator of a misspeciﬁed model is a consistent estimator of the
learnable parameter that minimizes the KL divergence to the true distribution – if that optimal,
misspeciﬁed parameter is globally identiﬁable. In an interesting result, (Jog & Loh, 2015) show that
the KL divergence between a multivariate Gaussian distribution and a misspeciﬁed (by at least a
single edge) Gaussian graphical model is bounded by a constant from below. It emphasizes the need
to correctly select the model’s edge structure, since otherwise the ﬁtted distribution will differ from
the true one in terms of KL divergence.

3 PROBLEM SETUP

For this work we use the data programming framework as introduced in (Ratner et al., 2016), where
the premise is that experts can model any higher-order dependency between labeling functions. We
extend it by negated, bolstering, priority dependencies (deﬁnitions in the appendix B), e.g. the latter
encoding the notion that one source’s vote should be prioritized over the one from a noisier source.
The additional dependency types are motivated by our selected LF sets that contain dependencies,
such as the ones in Table 1, that we could not express before. Newer weak supervision models
and model ﬁtting approaches often only allow for pairwise correlation dependencies to be modeled
(Ratner et al., 2019b; Fu et al., 2020).
Let (x, y) ∼ D be the true data generating distribution and for simplicity assume that y ∈ {−1, 1}.
As in (Ratner et al., 2016), users provide m labeling functions (LFs) λ = λ(x) ∈ {−1, 0, 1}m, where
0 means that the LF abstained from labeling. Following (Ratner et al., 2016), we model the joint
distribution of y, λ as a factor graph. To study model misspeciﬁcation we compare two label models,
pθ for the conditional independent case and pµ which models M higher-order dependencies:

pµ(λ, y) =

1
Zµ

exp (cid:0)µT φ(λ, y)(cid:1) = Z −1

µ exp (cid:0)µT

1 φ1(λ, y) + µT

2 φ2(λ, y)(cid:1) ,

µ ∈ Rm+M (1)

pθ(λ, y) = Z −1

θ

exp (cid:0)θT φ1(λ, y)(cid:1) ,

θ ∈ Rm,

(2)

, Z −1

where φ1(λ, y) = λy are the accuracy factors, φ2(·) are arbitrary, higher-order dependencies and
Z −1
µ are normalization constants. We assume w.l.o.g. that factors are bounded ≤ 1. Note that
θ
for ease of exposition, the models above do not model the labeling propensity factor (also known as
LF coverage) . Since this factor does not depend on the label, the corresponding terms would cancel
out in the quantities studied below (same goes for dependencies that do not depend on y, e.g. the
similar factor from (Ratner et al., 2016)).

4 THEORETICAL ANALYSIS

Bound on the label model posterior under model misspeciﬁcation We now state our bound on
the probabilistic label difference (which we prove in the appendix A):

|pµ(y |λ) − pθ(y |λ)| ≤

1
2

||µ1 − θ||1 +

1
4

||µ2||1

(3)

This is an important quantity of interest since the probabilistic labels are used to train a downstream
model. Unsurprisingly then, this quantity reappears as a main factor controlling the KL divergence
and generalization risk, see below.
Suppose that the downstream model fw : X → Y is parameterized by w, and that to learn w
we minimize a, w.l.o.g., bounded noise-aware loss function L(fw(x), y) ∈ [0, 1] (that acts on the
probabilistic labels). If we had access to the true labels, we would normally try to ﬁnd the w that
minimizes the risk: w∗ = arg minw R(w) = arg minw E(x,y)∼D [L(fw(x), y)]. Since this is not the
case, we instead will get the parameter ˆw that minimizes the (empirical) noise-aware loss.

2

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

(a) IMDB (0.81)

(b) Bias Bios: professor or teacher (0.91)

Figure 1: Modeling more than a handful dependencies (as the ones in Table 1) signiﬁcantly de-
teriorates ROC-AUC downstream performance as compared to simply ignoring them by up to 4
and 8 points. This effect intensiﬁes as we model more dependencies. In brackets, the baseline
score for the independent model (‘No deps‘). B&N means that we only model the bolstering and
negated dependencies, while F&P means that we only model the ﬁxing and priority dependencies.
Structure learning means that we model the similar, ﬁxing and reinforcing dependencies
returned by the method from (Bach et al., 2017) for different threshold hyperparameters (the lower,
the more dependencies are modeled).

Bound on the generalization risk under model misspeciﬁcation While (Fu et al., 2020) provide
a bound on the generalization error that accounts for model misspeciﬁcation – the label model being
a less expressive Ising model (i.e. no higher-order dependencies may be modeled) – the part of
the bound corresponding to the misspeciﬁcation is a somewhat loose KL-divergence term between
the true and the misspeciﬁed models. If we instead assume that there exists a label model for
some optimal parameterization of factors (not necessarily the ones that are actually modeled) that
is equivalent to the true data generating distribution, we can provide a more meaningful and tight
bound.
As in (Ratner et al., 2016; 2019b), assume that 1) there exists an optimal parameter of either pµ or pθ
(say, µ∗ with label model pµ∗ ) such that sampling (λ, y) from this optimal label model is equivalent
to (λ, y) ∼ D; and 2) the label y is independent of the features used for training fw given the labeling
function outputs λ. Differently than (Ratner et al., 2016; 2019b), the factors that are actually modeled
may differ from the ones of the optimal label model. By adapting the proof of theorem 1 in (Ratner
et al., 2019b) to our case where we incorrectly use a misspeciﬁed model (say, pθ), we can bound the
generalization risk as follows

R( ˆw) − R(w∗) ≤ γ + 2||µ∗

1 − θ||1 + ||µ∗

2||1,

(4)

where γ is a bound on the empirical risk minimization error, which is not speciﬁc to our setting. We
can bound the KL divergence of the two models by a similar quantity, see the appendix A.3.

Interpretation These bounds naturally involve the accuracy parameter estimation error ||µ1 − θ||1
and the learned strength s := ||µ2||1 of the dependencies only modeled in pµ. Note that if we assume
that the model with dependencies pµ is the true one we can interpret s as the magnitude of the
dependencies that the independent model pθ failed to model. If on the other hand we associate the
misspeciﬁed model with pµ, then we can interpret this quantity s as the (dependency) parameter
excess that was incorrectly learned by pµ. The presented bounds are tighter than the ones from (Ratner
et al., 2019b) for the L1 norm, while in addition accounting for model misspeciﬁcation.

5 EXPERIMENTS
5.1 PROXY FOR FINDING TRUE DEPENDENCIES IN REAL DATASETS
The underlying true structure between two real labeling functions λj, λk is, of course, unknown.
However, by using true training labels (solely for this purpose) together with the observed LF
votes, we can compute resulting factor values for each data point i, to observe empirical strength of
i φl(λ(xi)j, λ(xi)k, yi). Sorting dependencies l
dependency factor l over a training set: vl

j,k = (cid:80)

3

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

Table 1: The strongest and weakest dependencies among the IMDB LFs, as measured by their factor
value vl
j,k computed with respect to the true labels. In the experiment from Fig.1, we repeatedly add
weaker dependencies to the set of dependencies used for learning the label model, starting with the
strongest ones below.

LFj

LFk

factor type l

factor value vl

j,k

best
bad
original
recommend
worth
great
worth
special
recommend
bad

great
don’t waste
bad
terrible
not worth
nothing great
not worth
not special
highly recommend
absolutely horrible

bolstering
bolstering
priority
priority
ﬁxing
ﬁxing
negated
negated
reinforcing
reinforcing

801
110
327
53
238
15
219
8
226
7

according to vl
the dependencies for which the true labels provide the most evidence of being correct.

j,k in descending order, we then choose to model the top d dependencies. These are

5.2 PERFORMANCE DETERIORATION DUE TO STRUCTURE OVER-SPECIFICATION

For the following experiment we use the IMDB Movie Review Sentiment dataset consisting of
n = 25k training and test samples each (Maas et al., 2011) and manually select a set of m = 135
sensible LFs that label on the presence of a single word or a pair of words (i.e. uni-/bi-gram LFs).
In addition we use the Bias in Bios dataset (De-Arteaga et al., 2019), and focus on the binary
classiﬁcation task introduced in (Boecking et al., 2021) that aims at distinguishing the biographies of
professors and teachers (n = 12294, m = 85). We deliberately choose unigram and bigram LFs so
as to create dependencies we expect to help with downstream model performance, e.g. by adding
negations (e.g. ”not worth”) of unigrams (e.g. ”worth”) that we expect to ﬁx the less precise votes of
the latter when both do not abstain.
We choose different d ∈ {1, 3, 5, . . . , 40} and then model the strongest ≤ d dependencies of each
factor l according to vl. An example of the strongest and weakest dependencies for the IMDB dataset
is shown in Table 1. For the Bias in Bios experiment, an example of a strong bolstering dependency
is that the term ‘PhD’ appears in addition to the term ‘university’. We report the test set performance
of a simple 3-layer neural network trained on the probabilistic labels, averaged out over 100 runs.
While for IMDB we observe a marginal boost (< 0.005) in performance when modeling the strongest
d = 1, 3 dependencies of each factor (5, 15 in total), the main take-away is the following:
We ﬁnd that modeling more than a handful of dependencies signiﬁcantly deteriorates the downstream
end classiﬁer performance (by up to 8 ROC-AUC points) as compared to simply ignoring them
(Fig. 1). The performance worsens as we increase d, i.e. as we model more, slightly weaker,
dependencies. We reiterate that these additional dependencies still, semantically, make sense (as
depicted in Table 1, where the weakest ones are modeled only for the case where the total number
of dependencies = 122). Using the structure learning method from (Bach et al., 2017) to infer the
dependency structure results in worse test performances too.

6 DISCUSSION AND FUTURE WORK

Discussion Even though this result and insight is highly relevant for practitioners, it has, to the best
of our knowledge, not been explored in detail. It may come as a surprise that modeling seemingly
sensible dependencies can signiﬁcantly deteriorate the targeted downstream model performance. We
hypothesize that this is due to the true model being close to the conditionally independent case and in
our presented bounds, we indeed see that they become looser as more incorrect dependencies are
modeled. In addition, more complex models often suffer of a higher sample complexity, as is also
brieﬂy noted in (Fu et al., 2020). This suggests that practitioners may 1) indeed be best served, at
ﬁrst, by simply ignoring (higher-order) dependencies; and 2) need to be careful when specifying

4

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

dependencies, either by hand or through structure learning algorithms, which emphasizes the need
for a small ground-truth labeled validation set to compare the performance of different label models.

Future work
Future work should give a theoretically precise answer to the reason for why
performance deterioration is observed, conduct more extensive experiments to validate that this holds
for a variety of datasets and labeling function sets, as well as better characterize the settings in which
structure learning actually helps with downstream performance.

5

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

REFERENCES

Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research, 15:
2773–2832, 2014.

Stephen H. Bach, Bryan He, Alexander Ratner, and Christopher R´e. Learning the structure of
generative models without labeled data. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70, ICML’17, pp. 273–282, 2017.

Stephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik
Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Chris R´e, and Rob Malkin.
Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Proceedings
of the 2019 International Conference on Management of Data, SIGMOD ’19, pp. 362–375, New
ISBN 9781450356435. doi:
York, NY, USA, 2019. Association for Computing Machinery.
10.1145/3299869.3314036.

Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski.

Interactive weak su-
pervision: Learning useful heuristics for data labeling. International Conference on Learning
Representations (ICLR), 2021.

A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28,
1979. ISSN 00359254, 14679876.

Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra
Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios:
In Proceedings of the
A case study of semantic representation bias in a high-stakes setting.
Conference on Fairness, Accountability, and Transparency, pp. 120–128, 2019.

Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher R´e.

Fast and three-rious: Speeding up weak supervision with triplet methods. ICML, 2020.

Jean Honorio. Lipschitz parametrization of probabilistic graphical models.

In Proceedings of
the Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence, UAI’11, pp. 347–354,
Arlington, Virginia, USA, 2011. AUAI Press. ISBN 9780974903972.

Varun Jog and Po-Ling Loh. On model misspeciﬁcation and kl separation for gaussian graphical
models. In 2015 IEEE International Symposium on Information Theory (ISIT), pp. 1174–1178.
IEEE, 2015.

Po-ling Loh and Martin J Wainwright. Structure estimation for discrete graphical models: Generalized
covariance matrices and their inverses. In Advances in Neural Information Processing Systems 25,
pp. 2087–2095. Curran Associates, Inc., 2012.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, and Christopher R´e. Data program-
ming: Creating large training sets, quickly. Advances in neural information processing systems,
29, 05 2016.

Alexander Ratner, Stephen Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R´e.
Snorkel: rapid training data creation with weak supervision. The VLDB Journal, 29, 07 2019a.
doi: 10.1007/s00778-019-00552-1.

Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher
R´e. Training complex models with multi-task weak supervision. Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, 33:4763–4771, 07 2019b. doi: 10.1609/aaai.v33i01.33014763.

6

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

Paroma Varma and Christopher R´e. Snuba: Automating weak supervision to label training data.
In Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases,
volume 12, pp. 223. NIH Public Access, 2018.

Paroma Varma, Bryan D He, Payal Bajaj, Nishith Khandwala, Imon Banerjee, Daniel Rubin, and
Christopher R´e. Inferring generative model structure with static analysis. In Advances in neural
information processing systems, pp. 240–250, 2017.

Paroma. Varma, Frederic Sala, Ann He, Alexander Ratner, and Christophe R´e. Learning dependency

structures for weak supervision models. ICML, 2019.

Halbert White. Maximum likelihood estimation of misspeciﬁed models. Econometrica: Journal of

the Econometric Society, pp. 1–25, 1982.

A PROOFS OF THE THEORETICAL ANALYSIS

A.1 PROBLEM SETUP RECAP
Let (x, y) ∼ D be the true data generating distribution and for simplicity assume that y ∈ Y =
{−1, 1}. Users provide m labeling functions (LFs) λ = λ(x) ∈ {−1, 0, 1}m, where 0 means that
the LF abstained from labeling. We compare two label models, pθ for the conditional independent
case, and pµ which models higher-order dependencies:

pµ(λ, y) =

1
Zµ

exp (cid:0)µT φ(λ, y)(cid:1) = Z −1

µ exp (cid:0)µT

1 φ1(λ, y) + µT

2 φ2(λ, y)(cid:1) ,

µ ∈ Rm+M (5)

pθ(λ, y) = Z −1

θ

exp (cid:0)θT φ1(λ, y)(cid:1) ,

θ ∈ Rm,

(6)

, Z −1

where φ1(λ, y) = λy are the accuracy factors, φ2(·) are arbitrary, higher-order dependencies and
Z −1
µ are normalization constants. We assume w.l.o.g. that factors are bounded ≤ 1. Using
θ
an unlabeled dataset X = {xi}n
i=1 of n data points xi ∈ X to which we each apply the m user
provided labeling functions, we attain the label matrix Λ ∈ {−1, 0, 1}n×m. With Λ we then train
the label model to get a set of n probabilistic labels with which we supervise the downstream model
fw : X → Y.

Lemma 1

(Sigmoid posterior). With σ(x) =

1

1+exp(−x) being the sigmoid function, it holds that

pµ(y |λ) = σ (cid:0)2µT
pθ(y |λ) = σ (cid:0)2θT φ1(λ, y)(cid:1) .

1 φ1(λ, y) + µT

2 (φ2(λ, y) − φ2(λ, −y))(cid:1)

(7)

(8)

Proof

pµ(y |λ) =

=

=

=

=

=

pµ(λ, y)
˜y∈Y pµ(λ, ˜y)

pµ(λ, y)
(cid:80)
pµ(λ)
µ exp (cid:0)µT φ(λ, y)(cid:1)
Z −1
˜y∈Y Z −1
exp (cid:0)µT φ(λ, y)(cid:1)
˜y∈Y exp (µT φ(λ, ˜y))

µ exp (µT φ(λ, ˜y))

(cid:80)

(cid:80)

exp (cid:0)µT φ(λ, y)(cid:1)
exp (µT φ(λ, y)) + exp (µT φ(λ, −y))
1
1 + exp (µT (φ(λ, −y) − φ(λ, y)))

= σ (cid:0)µT (φ(λ, y) − φ(λ, −y))(cid:1)
= σ (cid:0)2µT
1 φ1(λ, y) + µT

2 (φ2(λ, y) − φ2(λ, −y))(cid:1) ,

where in the last step we used the fact that the accuracy factors are odd functions, i.e. φ1(λ, −y) =
−λy = −φ1(λ, y). Eq. 8 follows by the same argumentation.

7

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

A.2 PROOF OF THE BOUND ON THE LABEL MODEL POSTERIOR

Bound Our bound on the probabilistic label difference between the two models above is:

|pµ(y |λ) − pθ(y |λ)| ≤

1
2

||µ1 − θ||1 +

1
4

||µ2||1

(9)

Proof Using Lemma 1 we have that

|pµ(y |λ) − pθ(y |λ)| = (cid:12)

(cid:12)σ (cid:0)2µT

1 φ1(λ, y) + µT

2 (φ2(λ, y) − φ2(λ, −y))(cid:1) − σ (cid:0)2θT φ1(λ, y)(cid:1)(cid:12)
(cid:12)

By the mean value theorem it follows that for some c between the arguments of σ above

= σ(cid:48)(c) (cid:12)
(cid:0)2µT
1 φ1(λ, y) + µT
(cid:12)
(cid:12)
(cid:12)2 (µ1 − θ)T φ1(λ, y) + µT
(cid:12)
Using the triangle inequality and the fact that maxx σ(cid:48)(x) = maxx σ(x)(1 − σ(x)) = 1
now bound this expression as follows

2 (φ2(λ, y) − φ2(λ, −y))(cid:1) − 2θT φ1(λ, y)(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 (φ2(λ, y) − φ2(λ, −y))
(cid:12)

= σ(cid:48)(c)

4 , we can

≤

1
2

(cid:12)
(cid:12)
(cid:12)(µ1 − θ)T φ1(λ, y)
(cid:12)
(cid:12)
(cid:12) +

1
4

(cid:12)
(cid:12)µT

2 (φ2(λ, y) − φ2(λ, −y))(cid:12)
(cid:12)

ﬁnally, since the deﬁned higher-order dependencies are indicator functions (cid:54)= 0 for only one y ∈ Y,
and if ||q||∞ ≤ 1 then |xT q| = |(cid:80)

i |xi| = ||x||1, this reduces to

i |xiqi| ≤ (cid:80)

i xiqi| ≤ (cid:80)
1
4

||µ1 − θ||1 +

||µ2||1.

≤

1
2

A.3 PROOF OF THE BOUND ON THE KL DIVERGENCE

Bound

KL (pµ(y |λ) || pθ(y |λ)) ≤ 2||µ1 − θ||1 + ||µ2||1

(10)

Proof We adapt Theorem 7 of (Honorio, 2011) to give a bound on the KL divergence between the
two label model posterior’s. First note that with the same line of argumentation as in A.2 we have that

|log pµ(y |λ) − log pθ(y |λ)| = (log σ)(cid:48)(c)

(cid:12)
(cid:12)2 (µ1 − θ)T φ1(λ, y) + µT
(cid:12)

2 (φ2(λ, y) − φ2(λ, −y))

(cid:12)
(cid:12)
(cid:12)

≤ 2||µ1 − θ||1 + ||µ2||1,

where we use that the derivative of log σ(·) is (1 + exp(x))−1 ∈ (0, 1), and is bounded by 1. Next

KL (pµ(y |λ) || pθ(y |λ)) =

=

=

≤

pµ(y|λ) · log

pµ(λ, y)
pµ(λ)

· log

(cid:19)

(cid:18) pµ(y|λ)
pθ(y|λ)
(cid:18) pµ(y|λ)
pθ(y|λ)

(cid:19)

pµ(λ)

pµ(λ)

(cid:88)

y∈Y

(cid:88)

y∈Y

(cid:88)

λ∈L

(cid:88)

λ∈L
(cid:88)

(cid:88)

pµ(λ, y) · (log pµ(y|λ) − log pθ(y|λ))

λ∈L
(cid:88)

y∈Y
(cid:88)

λ∈L

y∈Y

pµ(λ, y) · |log pµ(y|λ) − log pθ(y|λ)|

≤ (2||µ1 − θ||1 + ||µ2||1)

= 2||µ1 − θ||1 + ||µ2||1

(cid:88)

(cid:88)

λ∈L

y∈Y

pµ(λ, y)

A.4 PROOF OF THE GENERALIZATION RISK BOUND

We now adapt Theorem 1 from (Ratner et al., 2019b) to the setting with model misspeciﬁcation and
assume like in (Ratner et al., 2016; 2019b) that

8

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

1. there exists an optimal parameter of either pµ or pθ such that sampling (λ, y) from this

optimal label model is equivalent to (λ, y) ∼ D.

2. the label y is independent of the features used for training fw given the labeling function

outputs λ, i.e. the LF labels provide sufﬁcient signal to identify the label.

For 1. we now assume without loss of generality, that pµ∗ (λ, y) = pD(λ, y) for an optimal parameter
µ∗ ∈ Rm+M , and that we incorrectly use the misspeciﬁed label model pθ. For the reversed roles (i.e.
pθ is the true model and pµ is misspeciﬁed), the following arguments are symmetric.
Suppose that the downstream model fw : X → Y is parameterized by w, and that to learn w we
minimize a, w.l.o.g., bounded loss function L(fw(x), y) ∈ [0, 1]. If we had access to the true labels,
we would normally try to ﬁnd the w that minimizes the risk:

w∗ = arg min
w

R(w) = arg min

w

E(x,y)∼D [L(fw(x), y)] .

Since this is not the case, we instead minimize the noise-aware loss:

˜w = arg min

w

Rθ(w) = arg min

w

E(x,y)∼D

(cid:2)E˜y∼pθ(·|λ) [L(fw(x), ˜y)](cid:3) .

(11)

(12)

In practice we will get the parameter ˆw that minimizes the empirical noise-aware loss over the
unlabeled dataset X = {x1, ..., xn}:

ˆw = arg min

w

ˆRθ(w) = arg min
w

1
n

n
(cid:88)

i=1

E˜y∼pθ(·|λ(xi)) [L(fw(xi), ˜y)] .

(13)

Since the empirical risk minimization error is not speciﬁc to our setting and can be done with standard
methods, we simply assume that the error |Rθ( ˜w) − Rθ( ˆw)| ≤ γ(n), where γ(n) is a decreasing
function of the unlabeled dataset size n.

Bound By adapting the proof of theorem 1 in (Ratner et al., 2019b) to our case with model
misspeciﬁcation involved, we can bound the generalization risk as follows

R( ˆw) − R(w∗) ≤ γ(n) + 2||µ∗

1 − θ||1 + ||µ∗

2||1.

(14)

Note that the bound from (Ratner et al., 2019b) is mistakenly too tight by a factor of 2 (due to the last
step in the proof below that is partly overseen).

Proof
we have that:

First, using the law of total expectation, followed by our assumptions 2. and 1., in that order,

R(w) = E(x(cid:48),y(cid:48))∼D [R(w)]
= E(x(cid:48),y(cid:48))∼D
= E(x(cid:48),y(cid:48))∼D
= E(x(cid:48),y(cid:48))∼D

(cid:2)E(x,y)∼D [L(fw(x(cid:48)), y)|x = x(cid:48)](cid:3)
(cid:2)E(x,y)∼D [L(fw(x(cid:48)), y)|λ(x) = λ(x(cid:48))](cid:3)
(cid:105)
(cid:104)
E˜y∼pµ∗ (·|λ) [L(fw(x(cid:48)), ˜y)]

= Rµ∗ (w)

Using the result above that R = Rµ∗ and adding and subtracting terms we have:

R( ˆw) − R(w∗) = Rµ∗ ( ˆw) − Rµ∗ (w∗)

= Rµ∗ ( ˆw) + Rθ( ˆw) − Rθ( ˆw) + Rθ( ˜w) − Rθ( ˜w) − Rµ∗ (w∗)

since ˜w minimizes the noise-aware risk, i.e. Rθ( ˜w) ≤ Rθ(w∗), we have that:

≤ Rµ∗ ( ˆw) + Rθ( ˆw) − Rθ( ˆw) + Rθ(w∗) − Rθ( ˜w) − Rµ∗ (w∗)
≤ |Rθ( ˆw) − Rθ( ˜w)| + |Rµ∗ ( ˆw) − Rθ( ˆw)| + |Rθ(w∗) − Rµ∗ (w∗)|
≤ γ(n) + 2 max

|Rµ∗ (w(cid:48)) − Rθ(w(cid:48))|

w(cid:48)

The main term |Rµ∗ (w(cid:48)) − Rθ(w(cid:48))| we now have in the bound, is the difference between the
true/expected noise-aware losses given the true label model parameter µ∗ and the estimated parameter

9

Published at the Workshop on Weakly Supervised Learning, ICLR 2021

|Rµ∗ (w(cid:48)) − Rθ(w(cid:48))| =

θ for the misspeciﬁed model. We now bound this quantity:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:88)

E(x,y)∼D

E(x,y)∼D

(cid:88)

y(cid:48)∈Y

=



(cid:104)
E˜y∼pµ∗ (·|λ) [L(fw(x), ˜y)] − E˜y∼pθ(·|λ) [L(fw(x), ˜y)]


(cid:105)(cid:12)
(cid:12)
(cid:12)

L(fw(x), y(cid:48)) (pµ∗ (y(cid:48) |λ) − pθ(y(cid:48) |λ))

≤

E(x,y)∼D [|L(fw(x), y(cid:48)) (pµ∗ (y(cid:48) |λ) − pθ(y(cid:48) |λ))|]

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

y(cid:48)∈Y
(cid:88)

≤

E(x,y)∼D [|pµ∗ (y(cid:48) |λ) − pθ(y(cid:48) |λ)|]

y(cid:48)∈Y
≤ |Y| max

y(cid:48)

E(x,y)∼D [|pµ∗ (y(cid:48) |λ) − pθ(y(cid:48) |λ)|]

We can now use our bound from (3) and get that:

≤ 2

(cid:18) 1
2

||µ∗

1 − θ||1 +

(cid:19)

1
4

||µ∗

2||1

= ||µ∗

1 − θ||1 +

1
2

||µ∗

2||1

Plugging this back into the term for the generalization risk gives the desired result:

R( ˆw) − R(w∗) ≤ γ(n) + 2 max
w(cid:48)
1 − θ||1 + ||µ∗
≤ γ(n) + 2||µ∗

|Rµ∗ (w(cid:48)) − Rθ(w(cid:48))|

2||1.

B FACTOR DEFINITIONS

We supplement the factor deﬁnitions of higher-order dependencies used in this paper. The ﬁrst two
stem from (Ratner et al., 2016), the rest we deﬁned ourselves for the conducted experiments, and
where motivated by frequently occurring dependency patterns, as the ones in Table 1, that are not
covered by (Ratner et al., 2016). Whenever a factor φj,k(λ, y) is not symmetric (all factors, besides
bolstering), we deﬁne it so that LFk acts on (e.g. negates) LFj.
For the ﬁxing dependency we have:

φF ix

j,k (λ, y) =






+1 if λj = −y ∧ λk = y
−1
0

if λj = 0 ∧ λk (cid:54)= 0
otherwise

φRei

j,k (λ, y) =






+1
−1
0

if λj = λk = y
if λj = 0 ∧ λk (cid:54)= 0
otherwise

φP ri

j,k (λ, y) =






+1
−1
0

if λj = −y ∧ λk = y
if λj = y ∧ λk = −y
otherwise

for the reinforcing one:

for the priority factor:

for the bolstering:

φBol

j,k (λ, y) =




+1
−1

0

if λj = λk = y
if λj = λk (cid:54)= y ∨ λj = −λk (cid:54)= 0
otherwise

and, ﬁnally, for the negated factor:



φN eg
j,k (λ, y) =



+1
−1
0

if λj = −y ∧ λk = y
if (λj = y ∧ λk = −y) ∨ λj = λk (cid:54)= 0
otherwise

10

