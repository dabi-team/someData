Fast Algorithms for Stackelberg Prediction Game with Least Squares Loss

Jiali Wang 1 He Chen 2 Rujun Jiang 1 Xudong Li 1 Zihao Li 2

1
2
0
2

y
a
M
2
1

]

C
O
.
h
t
a
m

[

1
v
1
3
5
5
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

The Stackelberg prediction game (SPG) has been
extensively used to model the interactions be-
tween the learner and data provider in the train-
ing process of various machine learning algo-
rithms.
Particularly, SPGs played prominent
roles in cybersecurity applications, such as in-
trusion detection, banking fraud detection, spam
ﬁltering, and malware detection. Often formu-
lated as NP-hard bi-level optimization problems,
it is generally computationally intractable to ﬁnd
global solutions to SPGs. As an interesting
progress in this area, a special class of SPGs with
the least squares loss (SPG-LS) have recently
been shown polynomially solvable by a bisec-
tion method. However, in each iteration of this
method, a semideﬁnite program (SDP) needs to
be solved. The resulted high computational costs
prevent its applications for large-scale problems.
In contrast, we propose a novel approach that re-
formulates a SPG-LS as a single SDP of a simi-
lar form and the same dimension as those solved
in the bisection method. Our SDP reformulation
is, evidenced by our numerical experiments, or-
ders of magnitude faster than the existing bisec-
tion method. We further show that the obtained
SDP can be reduced to a second order cone pro-
gram (SOCP). This allows us to provide real-time
response to large-scale SPG-LS problems. Nu-
merical results on both synthetic and real world
datasets indicate that the proposed SOCP method
is up to 20,000+ times faster than the state of the
art.

1. Introduction

In the big data era, machine learning (ML) algorithms
have been extensively used to extract useful information

1School of Data Science, Fudan University, China 2School
of Mathematical Sciences, Fudan University, China. Correspon-
dence to: Rujun Jiang <rjjiang@fudan.edu.cn>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

In this case,

from data and found numerous applications in our daily
life.
the na-
In certain areas, such as cybersecurity,
ture of applications requires high robustness of ML al-
gorithms against adversarial attacks. A typical scenario
would be the training data that the ML algorithms or the
learner relied on is deliberately altered by a malicious ad-
versary.
the key assumption for the suc-
cess of ML algorithms, i.e., the stationarity of data or
equivalently the independent and identically distributed
(i.i.d.)
assumption, fails to hold. To alleviate this dif-
ﬁculty, researchers have proposed various game theoretic
approaches (Br¨uckner & Scheffer, 2011; Tong et al., 2018;
Vorobeychik & Kantarcioglu, 2018; Bishop et al., 2020) to
model the strategic interactions between the learner and the
attacker – in our case, the adversarial data provider.

In practice, there are also many applications that the learner
and the data providers are not entirely antagonistic, where
the data providers often manipulate the data only for their
own interests. Introduced by Br¨uckner & Scheffer (2011),
the SPG is used to model the interactions between the
learner and the data provider as a two-players non-zero-
sum sequential game for such cases.
In the SPG, the
learner is regarded as the leader who makes the ﬁrst move
to commit to a predictive model without knowing the strat-
egy of the data provider (or the follower). Then, the
data provider, based on the available information of the
learner’s predictive model, selects his costs-minimizing
strategy to modify the data against the learner. Under the
rationality assumption of both the learner and data provider,
Br¨uckner & Scheffer (2011) introduced the notion of Stack-
elberg equilibrium as the optimal strategy of the SPG and
proposed to ﬁnd it via solving a corresponding bi-level op-
timization problem. Particularly, the bi-level optimization
problem minimizes the prediction loss from the learner’s
perspective under the constraint that the data has been
optimally modiﬁed from the data provider’s perspective.
Since then, SPGs have received a lot of attention in the
literature (Shokri et al., 2012; Zhou & Kantarcioglu, 2016;
Wahab et al., 2016; Papernot et al., 2018; Naveiro & Insua,
2019; Zhou et al., 2019). Unfortunately, bi-level optimiza-
tion problems are generally NP-hard (Jeroslow, 1985) and
their optimal solutions are intrinsically difﬁcult to obtain,
which severely limit the applicability of SPGs in real world
use cases.

 
 
 
 
 
 
Fast algorithms for SPG-LS

Recently, Bishop et al. (2020) made the ﬁrst step to glob-
ally solve a special subclass of SPGs. Speciﬁcally, they
restricted their interests on SPGs with least squares loss
(SPG-LS) (i.e., all the loss functions for the learner and data
providers are the least squares). They further reformulated
the SPG-LS into a quadratically constrained quadratic frac-
tional program that can be solved via a bisection method.
In each iteration of their bisection method, a noncon-
vex quadratically constrained quadratic program (QCQP)
needs to be exactly solved. Fortunately, by using the
celebrated S-lemma (Yakubovich, 1971; P´olik & Terlaky,
2007; Xia et al., 2016), the optimal solutions to the noncon-
vex QCQPs can be obtained via solving their semideﬁnite
programming (SDP) relaxations (Vandenberghe & Boyd,
1996). However, the number of bisection searches is often
of several tens in practice. This, together with heavy com-
putational costs of solving each SDP, makes the bisection
method far less attractive especially for large-scale prob-
lems. Moreover, the requirement for exactly solving each
SDP is too strong for large-scale problems, even armed
with powerful academic and commercial solvers. Theoreti-
cally speaking, given the accumulation of these inaccuracy,
the convergence of the bisection method with inexact SDPs’
solutions remains unknown. More importantly, this accu-
mulated inexactness may ﬁnally result unstable algorithmic
performances, which prevents its applications in the area of
security.

In this paper, we aim to resolve the above mentioned scal-
ability and stability issues of the bisection method for
the SPG-LS. For this purpose, we start by re-examining
the quadratic fractional program (QFP) considered in
Bishop et al. (2020). By using the S-lemma in a slightly
different way, we show that the QFP can be directly refor-
mulated into an SDP of almost the same problem size as the
ones in the bisection method. Furthermore, we prove that
there always exists an optimal solution for our SDP and an
optimal solution to the SPG-LS can be recovered from the
optimal solution of our SDP. It thus implies that the bisec-
tion steps are unnecessary, i.e., to solve the SPG-LS, one
only needs to solve a single SDP. This novel reformulation
outperforms the bisection method by a signiﬁcant margin
as the latter involves solving a series of SDPs with simi-
lar problem sizes. Surprisingly, we can take a step further
in accelerating our method. By carefully investigating the
intrinsic structures of the proposed SDP, we show our sin-
gle SDP reformulation can be further reduced into a second
order cone program (SOCP) (Alizadeh & Goldfarb, 2003),
which can be solved much more efﬁciently than SDPs in
general. More speciﬁcally, we apply two congruence trans-
formation for the linear matrix inequality (LMI) in our SDP.
The second congruence in fact explores a simultaneous di-
agonalizabiliy of submatrices for the three matrices in the
LMI after the ﬁrst congruence transformation. Then by us-

ing a generalized Schur complement, we demonstrate that
our SDP can be further reformulated as an SOCP with a
much smaller size. The main cost in our reformulation is
a spectral decomposition for the data matrix that is cheap
even for large instances. Moreover, solving our SOCP re-
formulation is even cheaper than one spectral decomposi-
tion. Hence our SOCP method is much faster than our sin-
gle SDP method.

We summarise our contributions as follows:

• We derive a single SDP reformulation for the SPG-LS,
while the state of the art needs to solve dozens of SDPs
with similar problems sizes.

• We further derive an SOCP reformulation with a much

smaller dimension than our single SDP.

• We propose two efﬁcient ways to recover an optimal
solution for the SPG-LS either from a rank-1 decom-
position of the dual solution of our single SDP or by
solving a linear system with an additional equation.

• We show that our methods signiﬁcantly improve the
state of the art by numerical experiments on both syn-
thetic and real data sets.

2. Preliminaries

In this section, we formalize the SPG-LS problem by adapt-
ing the same setting as in Bishop et al. (2020). A brief re-
view of Bishop et al.’s bisection method will also be pro-
vided.

Similar as in Bishop et al. (2020), we assume that the
learner has access to a sample S = {(xi, yi, zi)}m
i=1 with
each xi ∈ Rn been the input example, yi and zi been
the output labels of interests to the learner and the data
provider, respectively. The samples are assumed to be re-
alizations of (x, y, z) following some ﬁxed but unknown
distribution D. The learner then aims to train a linear pre-
dictor w ∈ Rn based on S, i.e., to predict correctly label
y when supplied with x. In the SPG-LS, being aware of
the learner’s predictor w, the goal of the adversarial data
provider is to fool the learner to predict the label z by mod-
ifying the input data x to ˆx while maintaining low manipu-
lation costs. Here, we follow Bishop et al. (2020) to model
the modifying costs from x to ˆx as γkx − ˆxk2 with some
positive parameter γ.

To ﬁnd the Stackelberg equilibrium of the above SPG-LS,
we formulate in the following the corresponding bi-level
optimization problem. Given the disclosed predictor w ∈
Rn and the training set S, the data provider described above
aims to solve the following optimization problems:

x∗i = argmin

ˆxi

kwT ˆxi − zik2 + γkxi − ˆxik2
2

i ∈ [m].

Fast algorithms for SPG-LS

Then,
one
can obtain a Stackelberg equilibrium
classic backward induction procedure
through the
(Br¨uckner & Scheffer, 2011). With the modiﬁed data
{x∗i }m
i=1, the learner has to solve the following optimiza-
tion problem:

w∗ ∈ argmin

w

kwT x∗i − yik2.

The predictor w∗ and the optimal modiﬁed data sets
{x∗i }m
i=1 of the data provider are by deﬁnition a Stackel-
berg equilibrium (Br¨uckner & Scheffer, 2011). To obtain
this, we arrive at the following bi-level optimization prob-
lem:

kX ∗w − yk2
min
w
s.t. X ∗ = argmin

ˆX

k ˆXw − zk2 + γk ˆX − Xk2
F ,

(1)
where the i-th row of X ∈ Rm
n is just the input example
xi and the i-th entries of y, z ∈ Rm are labels yi and zi,
respectively.

×

In their work, Bishop et al. (2020) considered the following
reformulation. They started by replacing the lower differ-
entiable and strongly convex optimization problem by its
optimality condition, i.e.,

X ∗ =

zwT + γX

wwT + γI

1

−

.

Then,
mula (Sherman & Morrison, 1950) further implies

Sherman-Morrison

(cid:0)
the

(cid:1) (cid:0)

(cid:1)

for-

following Dinkelbach problem associated with (3), for all
q ∈ R,

α
F (q) := min
γ
w,α
s.t α = wT w.

k

z + Xw − y −

α
γ

yk2 − q(1 +

α
γ

)2

(4)
The correctness of their algorithm is due to the following
well known result for fractional programming.

Lemma 2.1 (Theorem 1 of Dinkelbach (1967)) Assume
that for all q ∈ R, problem (4) has nonempty optimal
solution set. Then, the equation F (q) = 0 has a unique
solution. Furthermore, (w∗, α∗) is a solution to the QFP
T w∗ = α∗ and F (q∗) = 0 where
(3) if and only if w∗
q∗ = k α∗
γ yk2/(1 + α∗/γ)2.

γ z + Xw∗ − y − α∗

As F (q) is a concave monotonically decreasing continu-
ous function (Dinkelbach, 1967), the bisection algorithm
is well-deﬁned. Bishop et al. (2020) further showed that
initial lower and upper bounds q1 and q2 for q∗ satisfying
F (q1) ≥ 0 and F (q2) ≤ 0 are also easy to obtain.

In each iteration of the bisection method, given q, one needs
to compute F (q), i.e., the nonconvex optimization prob-
lem (4) needs to be solved. To this purpose, Bishop et al.
(2020) applied the S-lemma with equality (Xia et al., 2016)
to transform the QCQP (4) into an SDP problem whose op-
timal objective is exactly F (q). More speciﬁcally, deﬁne
matrices

X ∗w =

1

γ zwT w + Xw

1 + 1

γ wT w

.

Substituting the above formula to problem (1), we obtain
the following fractional program:

ˆA =

ˆB =





1
γ (z

−

0n

1

X T X

γ X T (z
−
y)T X 1
z
y
γ2 k
−
1
γ yT (z

−
yT X

y)
2
k
y)

−
−
and ˆC =

1
γ

1
γ2
1
γ 1 !

,

−

In

X T y

−
y)T y
1
γ (z
−
yT y 

.

0
−
1
2 0 (cid:19)

−

1
2

(cid:18)

1

γ zwT w + Xw − y − 1
(1 + 1
γ wT w)2

γ wT wy

min

w (cid:13)
(cid:13)
(cid:13)

2

.

(cid:13)
(cid:13)
(cid:13)

(2)

Given q ∈ R, problem (4) admits the same objective value
with the following SDP

max
τ,λ

τ

s.t. ˆA − q ˆB + λ ˆC − τ E (cid:23) 0,

(5)

2.1. A Bisection Method for Solving (2)

Here, we brieﬂy review the bisection method developed in
Bishop et al. (2020) for solving the fractional program (2).

By introduce an artiﬁcial variable α and an additional con-
straint α = wT w, Bishop et al. (2020) ﬁrst reformulated
(2) as the following QFP:

k α
γ z + Xw − y − α
(1 + α

γ )2

γ yk2

α = wT w.

min
w,α
s.t.

(3)

Then, they adopted a bisection search for q∗ such that
F (q∗) = 0, where F is the optimal value function of the

where E = Diag (0n+1, 1) ∈ R(n+2)
(n+2) is the diago-
nal matrix with ﬁrst n + 1 diagonal entries being zero and
the last entry being one. Then, the SDP (5) is solved by
advanced interior point methods (IPM).

×

Theoretically, Bishop et al. (2020) showed that under the
assumption that each involved SDP is solved exactly, the
bisection method needs log2(2yT y/ε) steps to obtain an
ε-optimal estimation of q∗ with given tolerance ε > 0.
Note that in practice, yT y can be quite large and ε may
the bisection method
be required to be small. Thus,
may need to solve a signiﬁcant numbers of SDPs even
in the moderate-scale setting, e.g., the numbers of sam-
ples m and features n are several thousands. Since the

 
Fast algorithms for SPG-LS

amount of work per iteration of IPM for solving (5) is
O(n3) (Nesterov & Nemirovskii, 1993; Todd, 2001), the
total computational costs of the bisection method can be
prohibitive. Moreover, there in fact exists no optimiza-
tion solver which can return exact solutions to these SDPs.
Hence, the convergence theory of the bisection method may
break down and its stability may be implicitly affected due
to the accumulation of optimization errors in each iteration.
These issues on scalability and stability of the bisection
method motivate our study in this paper.

3. Single SDP Reformulation

In this section, we present a novel result that shows an op-
timal solution to (2), or the Stackelberg equilibrium of the
SPG-LS, can be obtained by just solving a single SDP with
a similar size as the SDP (5). To begin, let us consider the
following equivalent formulation of (2):

min
w, α

s.t. wT w

kαz + Xw − y − αyk2
(1 + α)2

γ = α,
(6)
which is slightly different from (3) in a scaling of α. Now
let us recall the following S-lemma with equality, which is
the main tool in showing the equivalence of (4) and (5) in
Bishop et al. (2020).

1 x + q1 and h(x) = xT Q2x + 2pT

Lemma 3.1 (Theorem 3 in Xia et al. (2016)) Let f (x) =
xT Q1x + 2pT
2 x + q2,
where Q1, Q2 ∈ Rn
n are symmetric matrices, p1, p2 ∈
×
Rn and q1, q2 ∈ R. If function h takes both positive and
negative values and Q2 6= 0, then following two statements
are equivalent:

1. There is no x ∈ Rn such that f (x) < 0, h(x) = 0.

2. There exists a λ ∈ R such that f (x) + λh(x) ≥ 0.

We also need the following result that is well known in
quadratic programming.

Lemma 3.2 (Theorem 2.43 in Beck (2014)) Let Q ∈
Rn
n be a symmetric matrix, p ∈ Rn and q ∈ R. Then the
following two statements are equivalent:

×

1. (xT , 1)

Q p
pT
q

(cid:18)

x
1

(cid:19) (cid:18)

(cid:19)

≥ 0 for all x ∈ Rn.

2.

Q p
pT
q

(cid:18)

(cid:19)

(cid:23) 0.

From now on, let us deﬁne

A =

B =

y)
2

(z

X T X X T (z
−
y
z
k
k
−
yT (z
−
−

y)T X
−
yT X
−
0n

and C =

1 1
1 1

−

y)

−
(z

In
γ

(cid:16)

(cid:17)

X T y

y)T y
−
yT y !

,

(7)

.

1
2
−
0 !

0
1
2

−

With the above facts, we are now ready to present our main
result of this section that (2) can be equivalently reformu-
lated as a single SDP, where our SDP reformulation follows
a similar idea in equations (1.12-1.14) in (Nguyen et al.,
2014).

Theorem 3.3 Problem (2) is equivalent to the following
SDP

supµ,λ µ
s.t.

A − µB + λC (cid:23) 0.

(8)

Proof. Consider the equivalent formulation (6). Let
f (w, α) = kαz + Xw − y − αyk2, p(w, α) = wT w
γ − α
and denote by vfrac the optimal value of (6). Recall the
deﬁnitions of A, B, and C in (7). Then, we conduct the
reformulation in the following manner:

vfrac = inf
w,α

(cid:26)

f (w, α)
(1 + α)2 : p(w, α) = 0

(cid:27)

= sup

µ :

µ (cid:26)

= sup

µ :

µ (cid:26)

{(w, α) | f (w, α) − µ(1 + α)2 < 0,
p(w, α) = 0} = ∅
∃λ ∈ R s.t.f (w, α) − µ(1 + α)2
+λp(w, α) ≥ 0, ∀w ∈ Rn, α ∈ R

(cid:27)

(cid:27)
(9)

= sup

µ,λ (

µ :

( wT α 1 ) (A − µB + λC)
∀w ∈ Rn, α ∈ R

= sup
µ,λ

{µ : A − µB + λC (cid:23) 0} ,

w
α
1

≥ 0,

(cid:16)

(cid:17)

)

(10)

where (9) is due to the S-lemma with equality in Lemma
(cid:3)
3.1 and (10) is due to Lemma 3.2.

We brieﬂy remark that there exists an optimal solution for
the SDP (8) and it can be used to recover an optimal so-
lution to (2). In fact, we can recover an optimal solution
to (2) by either doing a rank-1 decomposition for the dual
solution of SDP (8), thanks to Sturm & Zhang (2003), or
solving a linear system with an additional equation as in
step 8 in Algorithm 11. More details are given in Appendix.

Up to now, we have shown that to obtain a global optimal
solution to the nonconvex fractional program (2), only a
single SDP needs to be solved. A crucial observation is

1See the discussions after Theorem 4.1.

 
 
Fast algorithms for SPG-LS

that our single SDP (8) has a similar form and the same di-
mension of the matrices with (5), the subproblem in each
iteration of the bisection method. We remark the main dif-
ferences: (i) the bisection parameter q is the variable µ in
our formulation; (ii) our formulation does not involve a τ
which is used for generating new half interval in the bisec-
tion method. From the similar forms of two SDPs, we can
expect that solving the SDPs (8) and (5) needs a similar
CPU time. However, the bisection method needs to solve a
series of SDPs. Indeed, for each test instance in our numer-
ical experiments, the bisection method needs to solve about
30 SDPs. In other words, our single SDP method is a more
efﬁcient way to obtain q∗ (or equivalently, µ in (8)) such
that F (q∗) = 0, which closely relates to an optimal solu-
tion of problem (3) (or equivalently, problem (2)) in view
of Lemma 2.12, than the bisection method.

4. SOCP Reformulation

Though our single SDP approach introduced in the previ-
ous section for ﬁnding Stackelberg equilibrium of SPG-LS
has already been much faster than the bisection method, the
fact that solving a large-scale SDP requires extensive com-
putations motivates us to make a step further of seeking
more reductions. For this purpose, in this section, by us-
ing a simultaneous diagonalizability of submatrices in the
linear matrix inequality (LMI) constraint in (8), we can fur-
ther reformulate SDP (8) as an SOCP that can be solved
much more efﬁciently. We brieﬂy describe our main idea
in Algorithm 1.

Algorithm 1 SOCP method for solving (2)

1: Input: matrices A, B, C in (7)
2: set V1 as in (11)
3: set ¯A, ¯B, ¯C as in (14), (12), (13)
4: do spectral decomposition to matrix ¯A11 in (14) with

¯A11 = HDH T
5: set V2 as in (15)
6: obtain the matrices ˜A = V2 ¯AV2, ˜B = V2 ¯BV2, ˜C =
V2 ¯CV2 in forms (16) and (17) with diagonal n + 1th
order leading principal submatrices

7: solve the SOCP problem (20) to obtain optimal µ∗, λ∗
8: obtain w∗ by ﬁnding a solution of the following linear

system

(A − µ∗B + λ∗C)

satisfying 1

γ wT w = α

= 0

w
α
1

(cid:16)

(cid:17)

The motivation of our reformulation comes from simple ob-
servations on matrices A, B and C. The ﬁrst key observa-

2In fact, we use Lemma 2.1 slightly different from its original
statement with a scaling of γ here and in the discussions after
Theorem 4.1.

tion is that B and C can be simultaneously diagonalized by
congruence. Indeed, letting

V1 =

In
0

0

0
0
1
√γ 1
1
√γ 1 !

−

,

we have from (7)

¯B := V T

1 BV1 =

and

¯C := V T

1 CV1 =

0n+1

(cid:18)

1
γ In+1

For convenience, let

(cid:18)

,

4
(cid:19)

.

−1

(cid:19)

¯A := V T

1 AV1 =

¯A11
¯AT
12

(cid:18)

¯A12
¯A22(cid:19)

.

(11)

(12)

(13)

(14)

The second key observation is that the n + 1th order lead-
ing principal submatrices of A, B and C can be simultane-
ously diagonlizable by congruence. To see this, applying
spectral decomposition to the real symmetric matrix ¯A11
yields ¯A11 = HDH T , where H is an (n + 1) × (n + 1)
orthogonal matrix and D = Diag (d) is a diagonal matrix
with di being its ith diagonal entry. Deﬁne

V2 =

H 0
1
0

.

(cid:19)

(cid:18)

Now we have

˜A := V T
2

¯AV2 =

D b
bT
c

(cid:18)

,

(cid:19)

(15)

(16)

where b ∈ Rn+1 and c ∈ R. Since H T H = I, we also
have

˜B := V T
2

¯BV2 = ¯B and ˜C = V T
2

¯CV2 = ¯C.

(17)

As V1 and V2 are both invertible matrices, the LMI con-
straint in (8) is equivalent to

˜A − µ ˜B + λ ˜C (cid:23) 0.

(18)

From the generalized Schur complement (Zhang, 2006),
the LMI (18) is equivalent to

γ In+1 (cid:23) 0,

D + λ
b ∈ Range(D + λ
γ In+1),
c − 4µ − λ − bT (D + λ

γ In+1)†b (cid:23) 0,

(19)

where (M )† denotes the Moore-Penrose pseudoinverse of
matrix M . As D is a diagonal matrix, by deﬁning 0
0 = 0,
(19) is further equivalent to

di + λ
c − 4µ − λ −

γ ≥ 0, and bi = 0 if di + λ
di+λ/γ ≥ 0.

n+1
i=1

b2
i

γ = 0, i ∈ [n + 1],

P

 
Fast algorithms for SPG-LS

These constraints can be further rewritten as

γ ≥ 0, i ∈ [n + 1],

di + λ
c − 4µ − λ −
si(di + λ

n+1
i=1 si ≥ 0,
γ ) ≥ b2
i , i ∈ [n + 1].
P

For i ∈ [n + 1], each constraint si(di + λ
i can
be expressed as a rotated second order cone constraint
(Alizadeh & Goldfarb, 2003), which is equivalent to the
second order cone constraint

γ ) ≥ b2

λ
γ

−

si+di

2

2

≤

!

si + di + λ
γ
2

.

b2
i +

v
u
u
t

Consequently, we have the following theorem.

Theorem 4.1 With the same notation in this section, prob-
lem (8) is equivalent to the following SOCP problem

supµ,λ,s µ
s.t.

γ ≥ 0, i ∈ [n + 1],

di + λ
c − 4µ − λ −
si(di + λ

γ ) ≥ b2
P

n+1
i=1 si ≥ 0,
i , i ∈ [n + 1].

(20)

Note that based on our construction, i.e., the congruence
transformations to the matrices in the LMI constraint in
(8), any optimal solution (µ∗, λ∗) to (20) is still optimal
to (8). We claim that an optimal solution w∗ to (2) can be
recovered by solving the linear system with an additional
equation in step 8 in Algorithm 1. Indeed, Lemma 2.1, the
strong duality theory of SDPs and the S-lemma with equal-
ity guarantee the existence of a rank-1 solution to the SDP
relaxation of

min
w,α

s.t

kαz + Xw − y − αyk2 − µ∗(1 +

α
γ

)2

1
γ

wT w = α,

and the solution solves the following KKT system of the
corresponding SDP relaxation




By setting W =
alent to

hC, W i = 0,
Wn+2,n+2 = 1,
W (cid:23) 0,
hA − µ∗B + λ∗C, W i = 0.

( wT α 1 ), the above facts are equiv-

w
α
1

(cid:16)

(cid:17)

(A − µ∗B + λ∗C)

w
α
1

= 0,

1
γ

wT w = α,

(cid:17)
due to A − µ∗B + λ∗C (cid:23) 0. One may think that the
In fact, the linear
above equations are difﬁcult to solve.

(cid:16)

system usually only has a unique solution and it sufﬁces
to solve the linear system solely. A sufﬁcient condition to
guarantee this is that the matrix (A−µ∗B +λ∗C) is of rank
n + 1, which is exactly the case in all our numerical tests.
More discussions on the solution recovering are given in
Appendix.

In general, SOCPs can be solved much faster than SDPs.
For our problem, it can be seen that IPMs for solving SOCP
(20) takes O(n) costs per iteration (Alizadeh & Goldfarb,
2003; Andersen et al., 2003; T¨ut¨unc¨u et al., 2003) which is
of orders magnitudes faster than the case O(n3) in solving
SDP (8) using interior point methods. The high efﬁciency
of our SOCP approach is also evidenced by our numerical
tests.

5. Experiment Results

In this section, we conduct numerical experiments on both
synthetic and real world datasets to verify the superior per-
formance of our proposed algorithms in terms of both the
computational time and the learning accuracy. We apply
the powerful commercial solver MOSEK (MOSEK, 2021)
to solve all the SDPs and SOCPs in the bisect method and
ours.

All simulations are implemented using MATLAB R2019a
on a PC running Windows 10 Intel(R) Xeon(R) E5-2650
v4 CPU (2.2GHz) and 64GB RAM. We report the results
of two real datasets and three synthetic datasets and defer
other results to the supplementary material.3

5.1. Real World Dataset

We ﬁrst demonstrate the accuracy and efﬁciency of our
proposed methods on two real datasets. We compare the
average mean squared error (MSE) as well as the wall-
clock time of our SDP and SOCP approaches with those
of the bisection method in Bishop et al. (2020), the ridge
regression and a nonlinear programming reformulation of
the SPG-LS in Br¨uckner & Scheffer (2011). Similar as in
Bishop et al. (2020), to evaluate the learning accuracy of
the algorithms, we perform 10-fold cross-validation and
compare their average MSE for 40 different values of the
3, 0.75] in (2). For each γ, a grid
parameter γ ∈ [1 × 10−
5, 1000]
search on 9 logarithmically spaced points [1 × 10−
is used to compute the best regularization parameter for
the ridge regression. We also compare the running time
of all the methods at γ = 0.5, averaged over 10 trials to
further illustrate the efﬁciency of our methods. For the test-
ing purpose, we ﬁrst apply min-max normalization to the
raw data X and scale the labels y, z to y = y/(βkyk
)
∞
), respectively. These labels will be
and z = z/(βkyk

3Our

available
https://github.com/JialiWang12/SPGLS.

is

at

∞
code

 
Fast algorithms for SPG-LS

scaled back to compute the average MSE. It is worth noting
that the constant β can be adjusted with respect to different
datasets.

5.1.1. WINE DATASET

We ﬁrst
test our methods on the red wine dataset
(Cortez et al., 2009), which contains 1599 instances each
with 11 features. The response is a physiochemical mea-
surement ranged from 0 to 10, where higher score means
better quality. We use the same setting as in Bishop et al.
(2020). The wine provider manipulates the data to achieve
a higher score if the original label is smaller than some
threshold t. The wine provider sets his target label z as
follows,

zi = max{yi, t}.

We consider
tmodest = 6 and Asevere with tsevere = 8.

two different providers Amodest with

Our numerical results are reported in Figure 1. From Fig-
ures 1(a) and 1(b), we see that our single SDP method, our
SOCP method and the bisection method achieved the best
performance in average MSE. This is not surprise as the
three methods are guaranteed to solve the SPG-LS glob-
ally. Figures 1(c) and 1(d) indicate that both single SDP
and SOCP are much faster than all the other three meth-
ods. Since the dimension of SDP is rather small, our SDP
method took a similar time with our SOCP method.

5.1.2. BLOG DATASET

We next compare our algorithms on the blogfeedback
dataset4 from the UCI data repository (Dua & Graff, 2019).
It consists of 52397 data processed from raw feedback-
materials collected from the Internet. Each one conveys the
information of a certain session, described by 281 features.
The response is the number of comments. The task for the
learner, in this case, is to predict the the future comment
numbers in a regression manner.

As before, we assume that the label zi = max{yi + δ, 0}
is modiﬁed by the data provider in order to trigger a biased
result. For example, consider an option guider who aims
to manipulate the public expectation of a certain blog news.
He is then motivated to temper the announced comment
number. We assume there are two types of data providers,
Amodest with δ = −5 and Asevere with δ = −10. All the
other hyperparameters are the same with the wine dataset.

For this dataset, we do not compare the method in
Br¨uckner & Scheffer (2011) for time consideration. Hence
we only present the comparisons of the other four methods
in Figure 2. Similarly, Figures 2(a) and 2(b) demonstrate
that our two methods achieved the best average MSE. Fig-

4https://archive.ics.uci.edu/ml/datasets/BlogFeedback

ures 2(c) and 2(d) indicate that both the single SDP and
SOCP methods are much faster than the bisection method,
and the SOCP approach surpasses the single SDP approach.
In fact, our SOCP method takes only about 1/50 time of
our single SDP method, while our single SDP method takes
only about 1/20 time of the bisection method. That is, our
SOCP method is 1000 times more efﬁcient than the bisec-
tion method on this dataset.

5.2. Synthetic Dataset

To further demonstrate the efﬁcacy of our proposed ap-
proaches in large-scale problems, we perform synthetic
experiments with a high feature dimension. The func-
tion make regression in scikit-learn (Pedregosa et al.,
2011) is used to build artiﬁcial datasets of controlled size
and complexity. In particular, we specify the noise as 0.1,
which is the standard deviation of the Gaussian noise ap-
plied to the output y, and all other arguments are set as de-
fault. Our experiment focuses on the comparison of three
different methods including the bisect method, the single
SDP and SOCP methods. Similar as in Bishop et al. (2020),
the fake input label zi is set as

zi = max{yi, y0.25},

where y0.25 represents the lower quartile (25th percentile)
of output y. More speciﬁcally, if the true label is greater
than or equal to the threshold y0.25, then the label would
not be modiﬁed. Otherwise, the label would be set as y0.25.
In all tests, the parameter γ is set as 0.01. More results with
γ = 0.1 can be found in the Appendix.

Table1. Time (seconds) comparison on synthetic data: m = 2n
eig
sSDP

SOCP

ratio2

ratio1

bisect

m

n

200
1000
2000
4000
8000
12000

100
500
1000
2000
4000
6000

4.356
167.732
988.675
7877.041
-
-

0.111
3.997
45.984
438.487
3127.316
-

0.043
0.099
0.178
0.536
1.478
3.079

101
1702
5559
14694
-
-

3
41
259
818
2116
-

0.001
0.020
0.085
0.441
3.349
11.245

Table2. Time (seconds) comparison on synthetic data: m = n
eig
sSDP

SOCP

ratio2

ratio1

bisect

m

n

100
500
1000
2000
4000
6000

100
500
1000
2000
4000
6000

4.342
158.304
990.151
7667.927
-
-

0.107
4.142
21.781
201.411
2142.952
-

0.040
0.072
0.225
0.586
2.485
2.876

108
2197
4408
13094
-
-

3
57
97
344
862
-

0.001
0.018
0.085
0.442
3.264
11.117

Tables 1, 2 and 3 summarise the comparison of wall-clock
time on different scales with m = pn, p ∈ {0.5, 1, 2}.
In these tables, “bisect” represents the bisection method
in Bishop et al. (2020), “sSDP” represents our single SDP
method, “SOCP” represents our SOCP method, “ratio1”
represents the ratio of times of the bisection method and our

Fast algorithms for SPG-LS

)
t
s
e
d
o
m
E
S
M

(

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

)
t
s
e
d
o
m
E
S
M

(

7

6

5

4

3

2

1

0

0

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

 102

 101

 100

10-1

10-2

200

400

600

800

1000

1200

1400

 102

 101

 100

10-1

10-2

200

400

600

800

1000

1200

1400

Figure1. Performance comparison between different algorithms on the red wine dataset. The left two plots correspond to MSE result
generated by Amodest and Asevere, whilst the right two plots correspond to wall-clock time comparison generated by Amodest and
Asevere.

)
t
s
e
d
o
m
E
S
M

(

5

4.5

4

3.5

3

2.5

0

Ridge Regression
Bisection SDP
Single SDP
SOCP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

)
t
s
e
d
o
m
E
S
M

(

13

12.5

12

11.5

11

10.5

10

9.5

9

8.5

0

Ridge Regression
Bisection SDP
Single SDP
SOCP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

 102

 101

 100

10-1

10-2

0.5

Bisection SDP
Single SDP
SOCP

1

1.5

2

2.5

3

3.5

4

4.5

5
104

 102

 101

 100

10-1

10-2

0.5

Bisection SDP
Single SDP
SOCP

1

1.5

2

2.5

3

3.5

4

4.5

5
104

Figure2. Performance comparison between different algorithms on the blog dataset. The left two plots correspond to MSE result
generated by Amodest and Asevere, whilst the right two plots correspond to wall-clock time comparison generated by Amodest and
Asevere.

Table3. Time (seconds) comparison on synthetic data: m =
0.5n

m

50
250
500
1000
2000
3000

n

100
500
1000
2000
4000
6000

bisect

sSDP

SOCP

ratio1

ratio2

eig

4.146
156.018
956.343
7495.735
-
-

0.105
4.471
69.267
177.999
1485.843
8769.430

0.047
0.078
0.189
0.371
1.229
2.616

87
2004
5047
20217
-
-

2
57
366
480
1209
3352

0.001
0.021
0.080
0.405
3.144
10.436

SOCP method, and “ratio2” represents the ratio of times of
our single SDP method and our SOCP method. The last
column “eig” recorded the spectral decomposition time of
matrix ¯A11 in (14). In the test, the algorithm would not be
run in larger dimension case (denoted by “-”), if its wall-
clock time at current dimension exceeds 1800 seconds.

From the three tables, we can ﬁnd that our single SDP
method is consistently faster than the bisection method.
The ratios in the table also demonstrate the high efﬁ-
ciency of our SOCP method, which can be up to 20,000+
times faster than the bisection method for case (m, n) =
(1000, 2000). Our SOCP method is also signiﬁcantly faster
than our single SDP method. For example, our SOCP
method took about 3 seconds for all cases with n = 6000,
while our single SDP method took at least 8,000 seconds
for the case (m, n) = (3000, 6000). We also remark that
the performance gap grows considerably with the problem
size since both the ratios increase as the dimension in-
creases. Finally, we mention that, compared to the time of
our single SDP method, the time of spectral decomposition

in formulating our SOCP is rather small, which is about 11
seconds for n = 6000.

6. Conclusion

In this paper, we study the computation for Stackelberg
equilibrium of SPG-LSs. Hidden convexity in the frac-
tional programming formulation (2) of the SPG-LS is
deeply explored. Then, we are able to reformulate the
SPG-LS as a single SDP, based on the S-lemma with equal-
ity. By using simultaneous diagonalizability of its subma-
trices in the constraint, we further reformulate our SDP into
an SOCP. We also demonstrate the optimal solution to the
SPG-LS can be recovered easily from solving our obtained
SDP or SOCP. Numerical comparisons between our single
SDP and SOCP approaches with the state of the art demon-
strate the high efﬁciency as well as learning accuracy of
our methods for handling large-scale SPG-LSs. We believe
that our work opens up a new way for the applicability of
SPG-LSs in large-scale real scenarios.

References

Alizadeh, F. and Goldfarb, D. Second-order cone program-
ming. Mathematical Programming, 95(1):3–51, 2003.

Andersen, E. D., Terlaky, T., and Roos, C. On imple-
menting a primal-dual interior-point method for conic
quadratic optimization. Mathematical Programming,
95(3):249-277, 2003.

 
 
 
 
Fast algorithms for SPG-LS

Beck, A.

Introduction to nonlinear optimization: The-
ory, algorithms, and applications with MATLAB. SIAM,
2014.

Ben-Tal, A., and Nemirovski, A. Lectures on modern con-

vex optimization. SIAM, 2012.

Bishop, N., Tran-Thanh, L., and Gerding, E. Optimal learn-
ing from veriﬁed training data. In Advances in Neural
Information Processing Systems 33, 2020.

Borwein, J. and Lewis, A. Convex analysis and nonlinear
optimization: theory and examples. Springer Science &
Business Media, 2006.

Br¨uckner, M. and Scheffer, T. Stackelberg games for adver-
sarial prediction problems. In Proceedings of the 17th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 547–555, 2011.

Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J.
Modeling wine preferences by data mining from physic-
ochemical properties. Decision Support Systems, 47(4):
547 – 553, 2009.

Dinkelbach, W. On nonlinear fractional programming.

Management Science, 13(7):492–498, 1967.

Dua, D. and Graff, C. UCI machine learning reposi-
tory http://archive.ics.uci.edu/ml, Irvine,
CA: University of California, School of Information and
Computer Science, 2019.

Hazan, E. and Koren. T. A linear-time algorithm for
trust region problems. Mathematical Programming,
158(1):363-381, 2016.

Jeroslow, R. G. The polynomial hierarchy and a simple
model for competitive analysis. Mathematical Program-
ming, 32(2):146–164, 1985.

Papernot, N., McDaniel, P., Sinha, A., and Wellman, M. P.
Sok: Security and privacy in machine learning. In 2018
IEEE European Symposium on Security and Privacy (Eu-
roS P), pp. 399–414, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., et al. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research, 12:2825–2830, 2011.

P´olik, I. and Terlaky, T. A survey of the S-lemma. SIAM

Review, 49(3):371–418, 2007.

Sherman, J. and Morrison, W. J. Adjustment of an inverse
matrix corresponding to a change in one element of a
given matrix. The Annals of Mathematical Statistics, 21
(1):124–127, 1950.

Shokri, R., Theodorakopoulos, G., Troncoso, C., Hubaux,
J.-P., and Le Boudec, J.-Y. Protecting location privacy:
optimal strategy against localization attacks. In Proceed-
ings of the 2012 ACM conference on Computer and Com-
munications Security, pp. 617–627, 2012.

Sturm, J. F. and Zhang, S. On cones of nonnegative
quadratic functions. Mathematics of Operations Re-
search, 28(2):246–267, 2003.

Todd, M. J. Semideﬁnite optimization. Acta Numerica,

10:515–560, 2001.

Toh, K.-C., Todd, M. J., and T¨ut¨unc¨u, R. H. SDPT3—a
MATLAB software package for semideﬁnite program-
ming, version 1.3. Optimization Methods and Software,
11(1-4):545–581, 1999.

Tong, L., Yu, S., Alfeld, S., et al. Adversarial regression
with multiple learners. In International Conference on
Machine Learning, pp. 4946–4954. PMLR, 2018.

MOSEK Aps.

The MOSEK optimization tool-
9.2.36.

for MATLAB manual. Version

box
http://docs.mosek.com/9.0/toolbox/index.html,
2021.

T¨ut¨unc¨u, R. H., Toh, K.-C., and Todd, M. J., Solving
semideﬁnite-quadratic-linear programs using SDPT3.
Mathematical Programming, 95(2): 189–217, 2003.

Naveiro, R. and Insua, D. R. Gradient methods for solving
stackelberg games. In International Conference on Algo-
rithmic Decision Theory, pp. 126–140. Springer, 2019.

Nesterov, Y. and Nemirovskii, A. Interior-point polynomial
algorithms in convex programming. SIAM Studies in
Applied Mathematics, Philadelphia, 1993.

Nguyen, V.-B., Sheu, R.-L., and Xia, Y. An sdp approach
for solving quadratic fractional programming problems,
2014.

Vandenberghe, L. and Boyd, S. Semideﬁnite programming.

SIAM Review, 38(1):49–95, 1996.

Vorobeychik, Y. and Kantarcioglu, M. Adversarial machine
learning. Synthesis Lectures on Artiﬁcial Intelligence
and Machine Learning, 12(3):1–169, 2018.

Wahab, O. A., Bentahar, J., Otrok, H., and Mourad, A. A
stackelberg game for distributed formation of business-
driven services communities. Expert Systems with Appli-
cations, 45:359–372, 2016.

Fast algorithms for SPG-LS

Xia, Y., Wang, S., and Sheu, R.-L. S-lemma with equality
and its applications. Mathematical Programming, 156
(1-2):513–547, 2016.

Yakubovich, V. A. S-Procedure in Nonlinear Control The-

ory. Vestnik Leningrad University, 1:62–77, 1971.

Zhang, F. The Schur complement and its applications, vol-

ume 4. Springer Science & Business Media, 2006.

Zhou, Y. and Kantarcioglu, M. Modeling adversarial learn-
In Paciﬁc-Asia Con-
ing as nested stackelberg games.
ference on Knowledge Discovery and Data Mining, pp.
350–362. Springer, 2016.

Zhou, Y., Kantarcioglu, M., and Xi, B. A survey of game
theoretic approach for adversarial machine learning. Wi-
ley Interdisciplinary Reviews: Data Mining and Knowl-
edge Discovery, 9(3):e1259, 2019.

Fast algorithms for SPG-LS

Supplementary Material

A. Recovering Solutions to the SPG-LS

1. Existence of an Optimal Solution to (8)

Note that (µ, λ) = (0, 0) is a feasible solution to (8) as

(cid:18)
Hence, the optimal value of (8) is bounded from above. Next, consider the following dual problem of (8)

(cid:19)

A =

( X z

y

−

−

y ) (cid:23) 0.

X T
y
z
−
y
−

min
W
s.t.

hA, W i

hB, W i = 1,
hC, W i = 0,
W (cid:23) 0.

(21)

Note that ˜W =

γ
8n In

, which satisﬁes

3
8
1
8

1
8
3
8 !

˜W ≻ 0, hB, ˜W i = 1 and hC, ˜W i = 0,

is a strictly feasible solution to (21). Then, we know from weak duality that the optimal value of (21) is also bounded
from below, i.e., it is a ﬁnite value. Thus there exists an optimal solution to (21) (see, e.g., Theorem 1.4.2 in
Ben-Tal & Nemirovski (2012) or Corollary 5.3.10 in Borwein & Lewis (2006)).

2. Recovering an Optimal Solution to the SPG-LS

2.1. RECOVERING AN OPTIMAL SOLUTION FROM THE DUAL SOLUTION OF (8)

Now let (µ∗, λ∗) be an optimal solution to (8). From Lemma 2.1, we know that an optimal solution to (2) can be recovered
from an optimal solution of the following QCQP

min g(w, α)

s.t.

1
γ

wT w = α,

(22)

where g(w, α) = f (w, α) − µ∗(1 + α)2. By relaxing



the following standard SDP relaxation of the problem (22),

w
α
1 




wT α 1

(cid:23) 0 to W (cid:23) 0 and Wn+2,n+2 = 1, we have

(cid:0)

(cid:1)

Note that the dual problem of (23) is

min
W
s.t.

hA − µ∗B, W i

hC, W i = 0,
Wn+2,n+2 = 1,
W (cid:23) 0.

τ

sup
λ,τ
s.t. A − µ∗B + λC − τ E (cid:23) 0,

(23)

(24)

where E = Diag (0n+1, 1). From Lemma 2.1, we know that the objective value of (22) is exactly 0 and thus the optimal
value of its SDP relaxation (23) is also non-positive. Then, weak duality implies that the optimal value of (24) is non-
positive. Since (λ, τ ) = (λ∗, 0) is a feasible solution to (24), it holds that 0 is the optimal value of (24). Moreover, we
know that the optimal value of (23) is 0.

 
Fast algorithms for SPG-LS

For problem (23), one may check that ˜W =

γ
2n In

1 1
2
1
2 1 !

is a strictly feasible solution. We assume (24) is also strictly

feasible. From Theorem 1.4.2 or Section 3.1.1.2 in Ben-Tal & Nemirovski (2012), we know that both (23) and (24) have
optimal solutions and any primal dual solution pairs satisfy the KKT optimality condition. Now suppose W ∗ is an optimal
solution to (23). Since (λ, τ ) = (λ∗, 0) is an optimal solution to (24), we have

hC, W ∗i = 0,
W ∗n+2,n+2 = 1,
W ∗ (cid:23) 0,
hA − µ∗B + λ∗C, W ∗i = 0.

(25)

Next we show such a W ∗ can be recovered from a dual solution of SDP (8). Recall

By setting µ = −1 and λ = 1, we have

˜B =

0n+1

(cid:18)

4

(cid:19)

, ˜C =

1
γ In+1

(cid:18)

−1

(cid:19)

˜A − µ ˜B + λ ˜C = ˜A +

1
γ In+1
0

(cid:18)

0
3

(cid:19)

≻ 0,

which is equivalent to A − µB + λC ≻ 0. Thus we see that there exists a strictly feasible solution for (8). Since both (8)
and its dual (21) are strictly feasible and (µ∗, λ∗) solves (8), we know by Ben-Tal & Nemirovski (2012) that there exists
an optimal solution ˆW to (21), which satisﬁes

hB, ˆW i = 1, hC, ˆW i = 0, ˆW (cid:23) 0 and hA − µ∗B + λ∗C, ˆW i = 0.

We assume ˆWn+2,n+2 6= 05. It is easy to verify that ¯W = ˆW / ˆWn+2,n+2 is an optimal solution to (23) as ¯W is feasible
and, together with the dual solution (λ, τ ) = (λ∗, 0), satisﬁes (25).

Now we will introduce the well known rank-1 decomposition for a positive semideﬁnite matrix in Sturm & Zhang (2003)
to obtain an optimal solution for (22).

Lemma A.1 (Proposition 4 in Sturm & Zhang (2003)) Let X be a positive semideﬁnite matrix of rank r in Rm
G be a given matrix. Then hG, Xi = 0 if and only if there exist pi ∈ Rm, i = 1, . . . , r, such that

m. Let

×

X =

r

i=1
X

pipT

i and pT

i Gpi = 0

for all i = 1, 2, . . . , r.

(26)

We next adapt Algorithm 2 for computing a decomposition for X in Lemma A.1, which is a variant of Algorithm 2 in
Hazan & Koren (2016). Similar to Lemma 1 in Hazan & Koren (2016), the while loop ends in at most r steps.

Using Algorithm A.1 and setting G = C, we obtain a decomposition W ∗ =

r
i=1 pipT

i satisfying

Since hA − µ∗B + λ∗C, W ∗i = 0, we have
semideﬁniteness of A − µ∗B + λ∗C, implies

i (A − µ∗B + λ∗C)pT

i = 0. This, together with the positive

for all i = 1, . . . , r.
P

pT

i Cpi = 0
r
i=1 pT

P

pT

i (A − µ∗B + λ∗C)pi = 0 for all i = 1, . . . , r.

Then, we know that pT
(pj )n+2 6= 0. Let ˆw = (pj )1:n+1
(pj)n+2

, then we obtain a rank-1 solution

i (A − µ∗B)pi = 0 for all i = 1, . . . , r. Since W ∗n+2,n+2 = 1, there exists some j ∈ [r] such that

(cid:0)
5One can expect that this is always the case in real applications. Indeed, one may verify from the complementary slackness condition
that ˆW = Diag (0n, 1, 0) if ˆWn+2,n+2 = 0, and consequently we must have X T (y − z) = 0. However, this condition holds with
probability 0 under the assumption that y and z generated from some reasonable distribution.

(cid:1)

ˆW =

ˆwT

1

ˆw
1

(cid:18)

(cid:19)

 
Fast algorithms for SPG-LS

Algorithm 2 Decomposition of X satisfying (26)
1: Input: positive semideﬁnite matrix X with rank r in Rm
2: set ¯A, ¯B, ¯C as in (14), (12), (13)
3: do spectral decomposition to matrix ¯A11 in (14) with ¯A11 = HDH T
4: while there exist pi, pj such that pT
5:

compute a root t of the quadratic equation

i Gpi > 0, pT

j Gpj < 0 do

×

m, a given matrix G such that hG, Xi = 0

i Gpit2 + 2pT
pT

i Gpjt + pT

j Gpj = 0

(tpi + pj), ˜pj = 1

(tpj + pi)

√t2+1

set ˜pi = 1
set pi = ˜pi, pj = ˜pj

√t2+1

6:
7:
8: end while
9: return X =

r
i=1 pipT
i

P

to problem (23). Let w∗ = ˆw1:n ∈ Rn and α∗ = ˆwn+1 ∈ R. It is not difﬁcult to verify that α∗ = w∗
g(w, α) = 0, i.e., (w, α) solves problem (22). Then, we know from Lemma 2.1 that w∗ is an optimal solution to (2).

T w∗/γ and

We remark here that in our recovering phase, no additional SDP needs to be solved. Indeed, in most interior point methods
based solvers for SDPs (e.g., MOSEK (MOSEK, 2021), SDPT3 (Toh et al., 1999)), both primal and dual solutions of SDPs
are computed simultaneously. Hence, to obtain a solution ˆW to (21), we only needs to solve either (8) or (21).

2.2. RECOVERING AN OPTIMAL SOLUTION FROM THE SOLUTION OF SOCP (20)

Next we show the correctness of step 8 in Algorithm 1. The analysis in the previous subsection reveals that there exists a
rank-1 solution ˆW to (23), and (λ, τ ) = (λ∗, 0) is a solution to the dual problem (24). Given the constraint Wn+2,n+2 = 1,
we may assume ˆW = ( ˆw
1 ) ( ˆwT 1 ). Then from (25), we arrive at the following sufﬁcient and necessary condition in terms
of ˆw,

hC, ( ˆw
1 ) ( ˆwT 1 )i = 0,
hA − µ∗B + λ∗C, ( ˆw

1 ) ( ˆwT 1 )i = 0,

Let ˆw = ( w
niteness of A − µ∗B + λ∗C, implies

α ) . Then, the ﬁrst equation implies 1

γ wT w = α, and the second equation, together with the positive semideﬁ-

(A − µ∗B + λ∗C)

w
α
1

= 0.

Since the existence of such a (w, α) is guaranteed from the results in the previous subsection, when the linear system has
a unique solution6, it sufﬁces to solve the linear system solely.

(cid:16)

(cid:17)

6A sufﬁcient condition to guarantee this is that the matrix (A − µ∗B + λ∗C) is of rank n + 1, which is observed in all our numerical

tests.

B. Additional Experiments

Fast algorithms for SPG-LS

In this section, we compare our single SDP and SOCP methods with existing algorithms for additional real and synthetic
datasets. Besides, to validate the robustness of our model, we add experiments with random noises in the target label as
well.

1. Real World Dataset

We illustrate the accuracy and efﬁciency of our methods on two additional real world datasets, the insurance dataset7 and
the residential building dataset8.

1.1. THE INSURANCE DATASET

The insurance dataset consists of 1338 instances with 7 features, each regarding to certain information of an individual
such as age, region and smoking status. For the test purpose, we transform the categorical features into a one-hot vector.
Similar as in Bishop et al. (2020), we consider the scenario with an insurer and multiple individuals. The insurer collects
data-form information from the individuals to predict future insurance quote while the latter provide fake data in order to
make the quote lower. Corresponding to this situation, we deﬁne the individual’s desired outcome as

where δ = −100 in the modest case and δ = −300 in the severe case.

zi = max{yi + δ, 0},

All the hyperparameters are the same as those used in Section 5.1. The MSE and the computational time comparisons are
illustrated in Figure 3. As one can observe, the bisection method, our single SDP and our SOCP methods achieved the
best performance in terms of MSE. On average, our SOCP method, for both the modest and severe cases, is about 40 times
faster than the bisection method, and is slightly better than our single SDP method. These results again verify the accuracy
and efﬁcacy of our methods.

)
t
s
e
d
o
m
E
S
M

(

3.5

3

2.5

2

1.5

1

0.5

0

0

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

)
t
s
e
d
o
m
E
S
M

(

12

11

10

9

8

7

6

5

4

3

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

 102

 101

 100

10-1

10-2

200

400

600

800

1000

1200

 102

 101

 100

10-1

10-2

200

400

600

800

1000

1200

Figure3. Performance comparison between different algorithms on the insurance dataset. The left two plots correspond to MSE result
generated by Amodest and Asevere, whilst the right two plots correspond to wall-clock time comparison generated by Amodest and
Asevere.

1.2. THE RESIDENTIAL BUILDING DATASET

The residential building data set consists of 372 instances each with 107 features. The response variable is chosen to be
the actual sales prices. We consider a scenario in which sellers want to sell the buildings at a higher price and buyers try to
predict fair prices. We deﬁne the seller’s desired outcome

zi = yi + δ,

where δ = 20 for Amodest and δ = 40 for Asevere. All the hyperparameters are the same as those used in Section 5.1.

The resulted MSE of the experiments and the running time comparison are shown in Figure 4. Similar as in the previous
case, our algorithms outperform other approaches in terms of the MSE and running time. On average, our single SDP
method is about 25 times (30 times, respectively) faster than the bisection in the modest case (the severe case, respectively),
while our SOCP method is about 500 times (550 times, respectively) faster than the bisection in the modest case (the

7https://www.kaggle.com/mirichoi0218/insurance/metadata
8https://archive.ics.uci.edu/ml/datasets/Residential+Building+Data+Set

 
 
severe case, respectively). As for the MSE, we observe that for both types of data providers, the predictions made by our
algorithms are much more accurate than the ridge regression, and the same accurate as the bisection method.

Fast algorithms for SPG-LS

)
t
s
e
d
o
m
E
S
M

(

1000

900

800

700

600

500

400

300

200

100

0

0

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

Brucker and Scheffer(2011)
Ridge Regression
Bisection SDP
Single SDP
SOCP

2500

2000

1500

1000

500

)
t
s
e
d
o
m
E
S
M

(

 101

 100

10-1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

10-2

100

150

200

250

300

 101

 100

10-1

10-2

100

150

200

250

300

Figure4. Performance comparison between different algorithms on the building dataset. The left two plots correspond to MSE result
generated by Amodest and Asevere, whilst the right two plots correspond to wall-clock time comparison generated by Amodest and
Asevere.

2. Synthetic Dataset

To further demonstrate the efﬁciency of our approaches, we perform experiments with γ = 0.1 on more synthetic data sets.
All the other settings are the same as in Section 5.2.

From Tables 4-6, the numerical experiments demonstrated the superiority of our SOCP method over others, as observed
in Section 5.2. When the dimension of n is small, both single SDP and SOCP methods are efﬁcient. However, when n
increases, our SOCP method performs much better than the single SDP method. Compared with the bisection method
in the case (m, n) = (1000, 2000), our SOCP can be up to 17,000+ times faster. Moreover, our SOCP method took
less than 3 seconds for all cases with n = 6000, while our single SDP took more than 10,000 seconds for the case
(m, n) = (3000, 6000). We also remark that the performance gap grows considerably with the problem size since both the
ratios increase as the dimension increases (except for ratio2 of the instance (m, n) = (4000, 2000)). Finally, the time of
spectral decomposition in formulating our SOCP is quite small, which is less than 11 seconds for n = 6000.

Table4. Time (seconds) comparison on synthetic data: m = 2n
ratio2

SOCP

ratio1

bisect

sSDP

n

100
500
1000
2000
4000
6000

4.856
175.010
1166.041
9268.016
-
-

0.148
3.854
62.065
183.295
2372.122
-

0.042
0.112
0.154
0.556
1.420
2.783

117
1565
7559
16683
-
-

4
34
409
330
1670
-

Table5. Time (seconds) comparison on synthetic data: m = n
ratio2

SOCP

ratio1

bisect

sSDP

n

100
500
1000
2000
4000
6000

4.885
173.118
1130.008
8547.944
-
-

0.127
4.408
47.321
334.814
2547.903
-

0.024
0.046
0.173
0.476
1.588
2.697

200
3798
6542
17955
-
-

5
97
274
703
1604
-

eig

0.001
0.020
0.084
0.455
3.330
10.943

eig

0.001
0.019
0.083
0.460
3.319
10.880

m

200
1000
2000
4000
8000
12000

m

100
500
1000
2000
4000
6000

 
 
Fast algorithms for SPG-LS

Table6. Time (seconds) comparison on synthetic data: m = 0.5n

m

50
250
500
1000
2000
3000

n

100
500
1000
2000
4000
6000

bisect

sSDP

SOCP

ratio1

ratio2

eig

4.571
167.787
1039.244
8397.725
-
-

0.131
8.960
37.309
296.672
1652.523
10026.490

0.038
0.119
0.135
0.523
1.518
2.550

121
1411
7702
16052
-
-

3
75
277
567
1088
3932

0.001
0.020
0.073
0.378
3.121
10.340

3. Random Noise

To see how the function with random noises can affect the performance, we add the following experiments in wine dataset.
Speciﬁcally, we add Guaussian noises to the target labels, i.e., ti = t + wi for wi ∼ N (0, σ2). We also do experiments
with the truncation threshold randomly changing in some interval, i.e., ti = t + wi for wi uniformly sampled in some
interval centered at the origin. The MSE results of different algorithms in wine dataset generated by Amodest and Asevere
are showed in Figure 5 and 6 respectively. The ﬁgures indicate that the model performance varies when the truncation
threshold changes with respect to a Gaussian noise N (0, 0.52) (truncated back to the interval [yi, 10]) or a uniform noise
in [t − 1, t + 1]. In all cases, the three global methods (i.e., the Bisection, single SDP and SOCP methods) achieve the
best MSEs. These results verify the robustness of our single SDP and SOCP model. The comparisons of runtime are not
presented here as they are similar to those presented in the current paper.

0.8

0.7

0.6

0.5

0.4

0.3

0

7

6

5

4

3

2

1

0

0

0.8

0.7

0.6

0.5

0.4

0.3

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.8

0.7

0.6

0.5

0.4

0.3

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure5. MSE comparison for wine dataset generated by Amodest.

7

6

5

4

3

2

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

7

6

5

4

3

2

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure6. MSE comparison for wine dataset generated by Asevere.

