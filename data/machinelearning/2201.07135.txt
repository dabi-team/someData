Springer Nature 2021 LATEX template

2
2
0
2

t
c
O
2
1

]

G
L
.
s
c
[

2
v
5
3
1
7
0
.
1
0
2
2
:
v
i
X
r
a

Synthesizing explainable counterfactual
policies for algorithmic recourse with
program synthesis

Giovanni De Toni1,2*, Bruno Lepri1 and Andrea Passerini2

1*Fondazione Bruno Kessler, Via Sommarive 18, Trento, 38123,
Italy.
2Department of Information Engineering and Computer Science,
University of Trento, Via Sommarive 9, Trento, 38123, Italy.

*Corresponding author(s). E-mail(s): giovanni.detoni@unitn.it;
Contributing authors: lepri@fbk.eu; andrea.passerini@unitn.it;

Abstract

Being able to provide counterfactual interventions – sequences of actions
we would have had to take for a desirable outcome to happen –
is essential to explain how to change an unfavourable decision by a
black-box machine learning model (e.g., being denied a loan request).
Existing solutions have mainly focused on generating feasible inter-
ventions without providing explanations on their rationale. Moreover,
they need to solve a separate optimization problem for each user. In
this paper, we take a diﬀerent approach and learn a program that
outputs a sequence of explainable counterfactual actions given a user
description and a causal graph. We leverage program synthesis tech-
niques, reinforcement learning coupled with Monte Carlo Tree Search
for eﬃcient exploration, and rule learning to extract explanations for
each recommended action. An experimental evaluation on synthetic
and real-world datasets shows how our approach generates eﬀective
interventions by making orders of magnitude fewer queries to the
black-box classiﬁer with respect to existing solutions, with the addi-
tional beneﬁt of complementing them with interpretable explanations.

Keywords: algorithmic recourse, counterfactuals examples, explainable ai,
marchine learning

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Synthesizing counterfactual policies for recourse with program synthesis

1 Introduction

Counterfactual explanations are very powerful tools to explain the decision
process of machine learning models [1, 2]. They give us the intuition of what
could have happened if the state of the world was diﬀerent (e.g., if you had
taken the umbrella, you would not have gotten soaked). Researchers have devel-
oped many methods that can generate counterfactual explanations given a
trained model [1, 3–7]. However, these methods do not provide any actionable
information about which steps are required to obtain the given counterfactual.
Thus, most of these methods do not enable algorithmic recourse. Algorithmic
recourse describes the ability to provide “explanations and recommendations
to individuals who are unfavourably treated by automated decision-making
systems” [8]. For instance, algorithmic recourse can answer questions such as:
what actions does a user have to perform to be granted a loan? Recently,
providing feasible algorithmic recourse has also become a legal necessity [9].
Some research works address this problem by developing ways to generate
counterfactual interventions [8], i.e., sequences of actions that, if followed, can
overturn a decision made by a machine learning model, thus guaranteeing
recourse. While being quite successful, these methods have several limitations.
First, they are purely optimization methods that must be rerun from scratch
for each new user. As a consequence, this requirement prevents their use for
real-time interventions’ generation. Second, they are expensive in terms of
queries to the black-box classiﬁer and computing time. Last but not least,
they fail to explain their recommendations (e.g., why does the model suggest
getting a better degree rather than changing job?). On the contrary, explain-
ability has been pointed out as a major requirement for methods generating
counterfactual interventions [10].

In this paper, we cast the problem of providing explainable counterfactual
interventions as a program synthesis task [11–14]: we want to generate a “pro-
gram” that provides all the steps needed to overturn a bad decision made by
a machine learning model. We propose a novel reinforcement learning (RL)
method coupled with a discrete search procedure, Monte Carlo Tree Search
[15], to generate counterfactual interventions in an eﬃcient data-driven man-
ner. As done by [16], we assume a causal model encoding relationships between
user features and consequences of potential interventions. We also provide a
solution to distil an explainable deterministic program from the learned policy
in the form of an automaton. Fig. 1 provides an overview of the architecture
and the learning strategy, and an example of an explainable intervention gen-
erated by the extracted automaton. Our approach addresses the three main
limitations that characterize existing solutions:
• It learns a general policy that can be used to generate interventions for
multiple users, rather than running separate user-speciﬁc optimizations.
• By coupling reinforcement learning with Monte Carlo Tree Search, it can
eﬃciently explore the search space, requiring massively fewer queries to
the black-box classiﬁer than the best evolutionary algorithm (EA) model

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

3

Fig. 1: 1. Model architecture. Given the state st representing the features of the
user, the agent generates candidate intervention policies πf and πx for functions and
arguments, respectively (an action is a function-argument pair). MCTS uses these
policies as a prior, and it extracts the best next action (f, x)∗
t+1. Once found, the
reward received upon making the action is used to improve the MCTS estimates, and
correct traces (i.e., those leading to the desired outcome change) are saved in a replay
buﬀer. 2. Training step. The buﬀer is used to sample a subset of correct traces to
be used to train the RL agent to mimic the behaviour of MCTS. 3. Explainable
intervention. Example of an explainable intervention generated by the automaton
extracted from the learned agent. Actions are in black, while explanations for each
action are in red.

available, especially in settings with many features and (relatively) long
interventions.

• By extracting a program from the learned policy, it can complement the
intervention with explanations motivating each action from contextual infor-
mation. Furthermore, the program can be executed in real-time without
accessing the black-box classiﬁer.

Our experimental results on synthetic and real-world datasets conﬁrm the
advantages of the proposed solution over existing alternatives in terms of
generality, scalability and interpretability.

2 Related Work

Counterfactual explanations are versatile techniques to provide post-hoc
interpretability of black-box machine learning models [1, 3–7]. They are model-
agnostic, which means that they can be applied to trained models without
performance loss. Compared to other global methods [17, 18], they provide
instead local explanations. Namely, they underline only the relevant factors
impacting a decision for a given initial target instance. They are also human-
friendly and present many characteristics of what it is considered to be a good
explanation [19]. Therefore, they are suitable candidates to provide explana-
tions to end-users since they are both highly-informative and localized. Recent
research has shown how to generate counterfactual interventions for algo-
rithmic recourse via various techniques [2], such as probabilistic models [20],
integer programming [21], reinforcement learning [22], program synthesis [23]

Springer Nature 2021 LATEX template

4

Synthesizing counterfactual policies for recourse with program synthesis

and genetic algorithms [16]. Methods with (approximated) convergence guar-
antees on the optimal counterfactual policies have also been proposed [24].
However, most of these methods ignore the causal relationships between user
features [21–24]. Without assuming an underlying causal graph, the proposed
interventions become permutation invariant. For example, given an interven-
tion consisting of three actions [A, B, C], any intervention that is a permutation
of the actions will have the same total cost. More importantly, it has been
recently shown that optimal algorithmic recourse is impossible to achieve with-
out a causal model of the interactions between the features [20]. The work by
Karimi et al. [20] does provide algorithmic recourse following a causal model
but optimizes for interventions that can operate on multiple user’s attributes
simultaneously, which is unrealistic. CSCF [16] is the only model-agnostic
method capable of producing consequence-aware sequential interventions by
exploiting causal relationships between features represented by a causal graph.
However, CSCF is still purely an (evolutionary-based) optimization method,
so it has to be run from scratch for each new user. Furthermore, the approach
is opaque with respect to the reasons behind a suggested intervention. In this
work, we show how our approach improves over CSCF in terms of generality,
eﬃciency and interpretability.

3 Methods

3.1 Problem setting

The state of a user is represented as a vector of attributes s ∈ S (e.g., age, sex,
monthly income, job). A black-box classiﬁer h : S → {T rue, F alse} predicts
an outcome given a user state, with T rue being favourable to the user and
F alse being unfavourable. The setting can be easily extended to multiclass
classiﬁcation by either grouping outcomes in favourable and unfavourable ones
or learning separate programs converting from one class to the other. A coun-
terfactual intervention I is a sequence of actions. Each action is represented
as a tuple, (f, x) ∈ A, composed by a function, f , and its argument, x ∈ Xf
(e.g., (change income, 500)). When an action is performed for a certain user,
it modiﬁes their state by altering one of their attributes according to its argu-
ment. A library F contains all the possible functions which can be called. This
library and the corresponding DSL (Domain Speciﬁc Language) are typically
deﬁned as a-priori by experts to prevent changes to protected attributes (e.g.,
age, sex, etc.). Examples of such DSLs can be found in the Appendix B. More-
over, each function possesses pre-conditions in the form of Boolean predicates
over its arguments which describe the conditions that a user state must meet
in order for a function to be called. The end of an intervention I is always
speciﬁed by the STOP action. We also deﬁne a cost function, C : A × S → R
which mimics the eﬀort made by a given user to perform an action given the
current state. The cost is computed by looking at a causal graph G, where
the nodes of the graph are the user’s features. This assumption encodes the
concept of consequences and it ensures a notion of order for the intervention’s

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

5

Fig. 2: Examples of interventions on a causal graph. (A) A causal graph and
a set of candidate actions. (B) Examples of interventions together with their costs.
Note that the green line ((cid:80) C = 15) has a lower cost than the red line ((cid:80) C = 28)
thanks to a better ordering of the actions making up the intervention.

Fig. 3: Agent architecture. Given the user’s state st, it outputs a function policy,
πf , an argument policy πx and an estimate of the expected reward from the state
vt. These outputs are used to select the next best action (f, x)t+1.

actions. For example, it might be easier to get ﬁrst a degree and then a better
salary, rather than doing the opposite. Fig. 2 shows an example of a causal
graph G and of the corresponding costs. Our goal is to train an agent that,
given a user with an unfavourable outcome, generates counterfactual interven-
tions that overturn it. Given a black-box classiﬁer h, a user s0 for whom the
prediction by h is unfavourable (i.e., h(s0) = F alse), a causal graph G and
a set of possible actions A (implicitly represented by the functions in F and
their arguments in X ), we want to generate a sequence I ∗, that, if applied to
s0, produces a new state, s∗ = I(s0), such that h(s∗) = T rue. This sequence
must be actionable, which means that the user has to be able to perform those
actions, and minimize the user’s cost. More formally:

I ∗ = minI

T
(cid:88)

C(at, st)

s.t.

t=0
I = {at}T
t=0 at ∈ A ∀t
st = It−1(st−1) ∀t > 0
h(I(s0)) (cid:54)= h(s0)

(1)

Springer Nature 2021 LATEX template

6

Synthesizing counterfactual policies for recourse with program synthesis

3.2 Model Architecture

Overall structure.

Fig. 1 shows the complete model architecture. It is composed of a binary
encoder and an RL agent coupled with the Monte Carlo Tree Search procedure.
The binary encoder converts the user’s features into a binary representation.
The conversion is done by one-hot-encoding the categorical features and dis-
cretizing the numerical features into ranges. In the following sections, we will
use st to directly indicate the user’s state binary version. Given a state st,
the RL agent generates candidate policies, πf and πx, for the function and
argument generation respectively. MCTS uses these policies as priors for its
exploration of the action space and extracts the best next action (f, x)∗
t+1. The
action is then applied to the environment. The procedure ends when the STOP
action is chosen (i.e., the intervention was successful) or when the maximum
intervention length is reached, in which case the result is marked as a failure.
During training, the reward is used to improve the MCTS estimates of the
policies. Moreover, correct traces (i.e., traces of interventions leading to the
desired outcome change) are stored in a replay buﬀer, and a sample of traces
from the buﬀer is used to reﬁne the RL agent.

RL agent structure.

The agent structure is inspired by previous program synthesis works [11, 12].
It is composed by 5 components: a state encoder, genc, an LSTM controller,
glstm, a function network gf , an argument network gx and a value network gV .
We use simple feedforward networks to implement gf , gx and gV .

genc(st) = et

gf (ht) = πf

gx(ht) = πx

glstm(et, ht−1) = ht
gV (ht) = vt

(2)

(3)

genc encodes the user’s state in a latent representation which is fed to the
controller, glstm. The controller, glstm learns an implicit representation of the
program to generate the interventions. The function and argument networks
are then used to extract the corresponding policies, πf and πx, by taking as
input the hidden state ht from glstm. gV represents the value function V and
it outputs the expected reward from the state st. Here, we omit the state st
when deﬁning the policies and the value function output, since st is already
embedded into the ht representation. In our settings, we try to learn a single
program, which we call INTERVENE.

Policy.

A policy is a distribution over the available actions (i.e., functions and their
arguments) such that (cid:80)N
i=0 π(i) = 1. Our agent produces two policies: πf on
the function space, and πx on the argument space. The next action, (f, x)t+1,

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

7

is chosen by taking the argmax over the policies:

ft+1 = argmax

f ∈F

πf (f ) xt+1 = argmax
x∈Xft+1

πx(x|ft+1)

Each program starts by calling the program INTERVENE, and it ends when the
action STOP is called.

Reward.

Once we have applied the intervention I, given the black-box classiﬁer h, the
reward, r, is computed as:

r = λT R λ ∈ (0, 1), R =

(cid:40)

1 h(I(s)) (cid:54)= h(s)
0 otherwise

(4)

where λ is a regularization coeﬃcient and T is the length of the intervention.
The λT penalizes longer interventions in favour of shorter ones.

3.3 Monte Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a discrete heuristic search procedure that
can successfully solve combinatorial optimization problems with large action
spaces [25, 26]. MCTS explores the most promising nodes by expanding the
search space based on a random sampling of the possible actions. In our setting,
each tree node represents the user’s state at a time t, and each arc represents
a possible action determining a transition to a new state. MCTS searches for
the correct sequence of interventions that minimize the user eﬀort and changes
the prediction of the black-box model. We use the agent policies, πf and πx,
as a prior to explore the program space. Then, the newly found sequence
of interventions is used to train the RL agent. To select the next node, we
maximize the UCT criterion [27]:

(f, x)t+1 = argmax
f ∈F ,x∈Xf

Q(s, (f, x)) + U (s, (f, x)) + L(s, (f, x))

(5)

Here Q(s, (f, x)) returns the expected reward by taking action (f, x).

U (s, (f, x)) is a term which trades-oﬀ exploration and exploitation, and it is
based on how many times we visited node s in the tree. L(s, (f, x)) is a scoring
term which is deﬁned as follows:

L(s, (f, x)) = e−(lcost((f,x),s)+lcount(f ))

(6)

where lcost = C(a, s) ∈ R represents the eﬀort needed to perform the a =
(f, x) ∈ A action, and lcount ∈ R penalizes interventions that call multiple

Springer Nature 2021 LATEX template

8

Synthesizing counterfactual policies for recourse with program synthesis

times the same function f . MCTS uses the simulation results to return an
improved version of the agent policies πmcts

f
From the found intervention, we build an intervention trace, which is a
sequence of tuples that stores, for each time step t: the input state, the output
state, the reward, the hidden state of the controller and the improved policies.
The traces are stored in the replay buﬀer, to be used to train the RL agent.

and πmcts

x

.

3.4 Training the agent

The agent has to learn to replicate the interventions provided by MCTS at
each step t. Given the replay buﬀer, we sample a batch of intervention traces
and we minimize the cross-entropy L between the MCTS policies and the agent
policies for each time step t:

argmin
θ

(cid:88)

(V − r)2 − (πmcts

f

)T log(πf ) − (πmcts

x

)T log(πx)

(7)

batch

where θ represents the agent’s parameters and V is the value function
evaluation computed by the agent.

3.5 Generate Interventions through RL

When training the agent, we learn a general policy that can be used to provide
interventions for many diﬀerent users. The inference procedure is similar to
the one used for training. Given an initial state s, MCTS explores the tree
search space using as “prior” the learnt policies πx and πf coming from the
agent. The policies πx and πf give MCTS a hint of which node to select at
each step. Once MCTS ﬁnds the minimal cost trace that achieves recourse, we
return it to the user. In principle, we can also use only πx and πf to obtain
a viable intervention (e.g., by deterministically taking the action with highest
probability each time). However, keeping the search component (MCTS) with
a small exploration budget outperforms the RL agent alone. See Table 2 in
Section 4 for the comparison between the agent-only model and the agent
augmented with MCTS.

Learning a general policy to provide interventions is a powerful feature.
However, the policy is encoded in the latent states of the agent, thus making
it impossible for us to understand it. We want to be able to extract from
the trained model an explainable version of this policy, which can then be
used to explain why the model suggested a given intervention. Namely, besides
providing to the users a sequence of actions, we want to show also the reason
behind each suggested action. The intuition to achieve this is the following:
given a set of successful interventions generated by the agent, we can distill
a synthetic automaton, or program, which condense the policy in a graph-like
structure which we can traverse.

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

9

Fig. 4: Procedure to generate the explainable program from intervention
traces. 1. For all f ∈ F, we add a new node. 2. Given the samples traces, we add
the transitions, and we store (si, (fi, xi)) in each node. 3. We train a decision tree
for each node to predict the next action (consistently with the sampled traces). 4.
We execute the program on the new instance at prediction time, using the decision
trees to decide the next action at each node. We extract a Boolean rule explaining
it from the corresponding decision tree for each action. On the right, an example of
generated intervention. The actions (f, x) are black, while the explanations are red.

3.6 Explainable Intervention Program

We now show how we can build a deterministic program given the agent. Fig. 4
shows the complete procedure and an example of the produced trace. First, we
sample M intervention traces from the trained agent and extract a sequence
of {(si, (f, x)i)}T
i=0 for each trace. Then, we construct an automaton graph, P,
in the following way:

1. Given the function library F, we create a node for each function f available.
We also add a starting node called INTERVENE and a “sink” node called
STOP;

2. We connect each node by unrolling the sampled traces. Starting from
INTERVENE, we treat each action (f, x)t as a transition. We label the tran-
sition with (f, x) and we connect the current node to the one representing
the function f ;

3. Lastly, for each node f , we store a collection of outgoing state-action pairs
(si, (f, x)i). Namely, we store all the states s and the corresponding outward
transitions which were decided by the model while at the node f ;

4. For each node, f ∈ P, we train a decision tree on the tuples (si, (f, x)i)
stored in the node to predict the transition (f, x)i given a user’s state si.

The decision trees are trained only once by using the collection of traces sam-
pled from the trained agent. The agent is frozen at this step, and it is not
trained further. At this point, we perform Step 1 to 3 of Fig. 4. The pseudocode
of the entire procedure is available in the Appendix A.

Springer Nature 2021 LATEX template

10

Synthesizing counterfactual policies for recourse with program synthesis

3.7 Generate Explainable Interventions

The intervention generation is done by traversing the graph P, starting from
the node INTERVENE, until we reach the STOP node or we reach the maximum
intervention length. In the last case, the program is marked as a failure. Given
the node f ∈ P and given the state st, we use the decision tree of that node to
predict the next transition (f (cid:48), x(cid:48)). Moreover, we can extract from the decision
tree interpretable rules which tell us why the next action was chosen. A rule
is a boolean proposition on the user’s features such as (income > 5000 ∧
education = bachelor). Then, we follow (f (cid:48), x(cid:48)), which is an arc going from
f to the next node f (cid:48), and we apply the action to st to get st+1. Again, the
program is “ﬁxed” at inference time, and it is not trained further. See Step
4 of Fig. 4 for an example of the inference procedure and of the produced
explainable trace.

4 Experiments

Our experimental evaluation aims at answering the following research ques-
tions: (1) Does our method provide better performances than the competitors
in terms of the accuracy of the algorithmic recourse? (2) Does our approach
allow us to complement interventions with action-by-action explanations in
most cases? (3) Does our method minimize the interaction with the black-box
classiﬁer to provide interventions?

The code and the dataset of the experiments are available on Github
to ensure reproducibility1. The software exploit parallelization through
mpi4python [28] to improve inference and training time. We compared the per-
formance of our algorithm with CSCF [16], to the best of our knowledge the
only existing model-agnostic approach that can generate consequence-aware
interventions following a causal graph. However, note that earlier solutions
still perform user-speciﬁc optimization, so that our results in terms of gener-
ality, interpretability and cost (number of queries to the black-box classiﬁer
and computational cost) carry over to these alternatives. For the sake of a fair
comparison, we built our own parallelized version of the CSCF model based on
the original code. We developed the project to make it easily extendable and
reusable by the research community. The experiments were performed using
a Linux distribution on an Intel(R) Xeon(R) CPU E5-2660 2.20GHz with 8
cores and 100 GB of RAM (only 4 cores were used).

4.1 Dataset and black-box classiﬁers

Table 1 shows a brief description of the datasets. They all represent binary
(favourable/unfavourable) classiﬁcation problems. The two real world datasets,
German Credit (german) and Adult Score (adult) [29], are taken from the rel-
evant literature. Given that in these datasets a couple of actions is usually

1https://github.com/unitn-sml/syn-interventions-algorithmic-recourse

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

11

Dataset

|D|

h(s) = 1

h(s) = 0

german
adult
syn
syn long

1002
48845
10004
10004

301
11691
5002
5002

701
37154
5002
5002

|s|

10
15
10
14

|B(s)|

|F|

44
125
40
64

7
6
6
10

Table 1: Description of the datasets. |D| is the size of the dataset. |s| the number
of features for an instance. |B(s)| shows how many binary features the agent sees
after the conversion with the binary converter. |F | is the size of the agent program
library. h(s) indicates the number of favourable (1) and unfavourable (0) samples.

Fig. 5: Experimental results. (Left) Accuracy (fraction of successful interven-
tions); (Middle) Average length of a successful intervention; (Right) Average cost of
a successful intervention. Results are averaged over 100 test examples.

suﬃcient to overturn the outcome of the black-box classiﬁer, we also devel-
oped two synthetic datasets, syn and syn long, where longer interventions are
required, so as to evaluate the models in more challenging scenarios. The
datasets are made by both categorical and numerical features (e.g., monthly
income, job type, etc.). Each dataset was randomly split into 80% train and
20% test. For each dataset, we manually deﬁne a causal graph, G, by looking at
the features available. For the synthetic datasets, we sampled instances directly
from the causal graph. The black-box classiﬁer for german and adult was
obtained by training a 5-layers MLP with ReLu activations. The trained clas-
siﬁers are reasonably accurate (∼ 0.9 test-set accuracy for german, ∼ 0.8 for
adult). The synthetic datasets (syn and syn long) do not require any training
since we directly use our manually deﬁned decision function.

4.2 Models

We evaluate four diﬀerent models: the agent coupled with MCTS (Mmcts),
the explainable deterministic program distilled from the agent (Mprog), and
two versions of CSCF, one (Mcscf ) with a large budget of generation, n, and
population size, p, (n = 50, p = 200) and one (M small
cscf ) with a smaller budget
(n = 25, p = 100).

Springer Nature 2021 LATEX template

12

Synthesizing counterfactual policies for recourse with program synthesis

Dataset Mmcts Magent

german
adult
syn
syn long

1.00
0.93
0.98
0.92

0.00
0.00
0.59
0.00

Table 2: Ablation study. In order to evaluate the contribution of MCTS in ﬁnding
successful interventions, we also developed an additional model which only uses the
RL agent, Magent. The agent just predicts the next action using its own policy
without leveraging MCTS to reﬁne the choice. Results indicate that RL alone is
incapable of ﬁnding successful interventions.

4.3 Evaluation

The left plot in Fig. 5 shows the average accuracy of the diﬀerent models,
namely the fraction of instances for which a model manages to generate a
successful intervention. We can see how Mmcts outperforms or is on-par with
the Mcscf and M small
cscf models on both the real-word and synthetic datasets.
The performance diﬀerence is more evident in the synthetic datasets, because
the evolutionary algorithm struggles in generating interventions that require
more than a couple of actions. The accuracy loss incurred in distilling Mmcts
into a program (Mprog) is rather limited. This implies that we are able to
provide interventions with explanations for 94% (german), 66% (adult), 99%
(syn) and 87% (syn long) of the test users2. Moreover, Mprog generates sim-
ilar interventions to Mmcts. The sequence similarity between their respective
interventions for the same user are 0.89 (german), 0.72 (adult), 0.80 (syn) and
0.71 (syn long), where 1.0 indicates identical interventions.

cscf

The main reason for the accuracy gains of our model is the ability to
generate long interventions, something evolutionary-based algorithms struggle
with. This eﬀect can be clearly seen from the middle plot of Fig. 5. Both Mcscf
and M small
rarely generate interventions with more than two actions, while our
approach can easily generate interventions with up to ﬁve actions. A drawback
of this ability is that intervention costs are, on average, higher (right plot of
Fig. 5). On the one hand, this is due to the fact that our model is capable of
ﬁnding interventions for more complex instances, while Mcscf and M small
fail.
Indeed, if we compute lengths and costs on the subset of instances for which
all models ﬁnd a successful intervention, the diﬀerence between the approaches
is less pronounced. See Fig. 6 for the evaluation. On the other hand, there is
a clear trade-oﬀ between solving a new optimization problem from scratch for
each new user, and learning a general model that, once trained, can generate
interventions for new users in real-time and without accessing the black-box
classiﬁer.

cscf

2Note that the accuracy loss observed on adult is due to the limited sampling budget we
allocated for Mprog (250 traces for all datasets). Adapting this budget to the feature space size
(considerably larger for adult) can help boosting the performance, at the cost of generating longer
explanations.

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

13

Fig. 6: Evaluation considering only the instances for which all the models
provide a successful intervention. If we restrict the comparison to the subset
of instances for which all models manage to generate a successful intervention, the
diﬀerence in costs between methods shrinks substantially (top left vs bottom left).
The same behaviour applies to the intervention length (top right vs bottom right).

Fig. 7: Number of queries. Total number of queries to the black-box classiﬁer
made by the models. Mprog(predict) is not visible, as the automaton does not query
the black-box classiﬁer to generate interventions. Note that the number of queries is
in logscale.

Fig. 7 reports the average number of queries to the black-box classiﬁer.
Our approach requires far fewer queries than Mcscf (note that the plot is in
logscale), and even substantially less than M small
(that is anyhow not compet-
itive in terms of accuracy). Furthermore, most queries are made for training
the agent (Mmcts(train)), which is only done once for all users. Once the model
is trained, generating interventions for a single user requires around two orders

cscf

Springer Nature 2021 LATEX template

14

Synthesizing counterfactual policies for recourse with program synthesis

Fig. 8: Accuracy of Mprog when varying the training budget. We show the
eﬀect on increasing the sampling budget (from 100 to 700 traces) when training the
Mprog model.

of magnitude fewer queries than the competitors. Note that MCTS is crucial
to allow the RL agent to learn a successful policy with a low budget of queries.
Indeed, training an RL agent without the support of MCTS fails to converge in
the given budget, leading to a completely useless policy. By eﬃciently search-
ing the space of interventions, MCTS manages to quickly correct inaccurate
initial policies, allowing the agent to learn high quality policies with a limited
query budget. See Table 2 for the evaluation.

When turning to the program, building the automaton (Mprog(train))
requires a negligible number of queries to extract the intervention traces used
as supervision.

Using the automaton to generate interventions does not require to query the
black-box classiﬁer. This characteristic can substantially increase the usability
of the system, as Mprog can be employed directly by the user even if they have
no access to the classiﬁer. Computationally speaking, the advantage of a two-
step phase is also quite dramatic. Mcscf takes an average of ∼ 693s for each
user to provide a solution (the same order of magnitude of training a model
for all users with Mmcts), while Mmcts inference time is under 1s, allowing
real-time interaction with the user.

Additionally, Fig. 8 shows how it is possible to improve the performances
of Mprog by just sampling more traces from the trained agent (Mmcts). We
can see how the accuracy increases in the adult, syn and syn long datasets.
We also notice that using a larger budget to train Mprog produces longer
explainable rules by keeping the length and cost of the generated interventions
almost constant. The total number of queries to the black-box classiﬁer will
also slightly increase.

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

15

Overall, our experimental evaluation allows us to aﬃrmatively answer the

research questions stated above.

5 Conclusion

This work improves the state-of-the-art on algorithmic recourse by providing a
method that can generate eﬀective and interpretable counterfactual interven-
tions in real-time. Our experimental evaluation conﬁrms the advantages of our
solution with respect to alternative consequence-aware approaches in terms of
accuracy, interpretability and number of queries to the black-box classiﬁer. Our
work unlocks many new research directions, which could be explored to solve
some of its limitations. First, following previous work on causal-aware inter-
vention generation, we use manually-crafted causal graphs and action costs.
Learning them from the available data directly, minimizing the human inter-
vention, would allow applying the approach in settings where this information
is not available or unreliable. Second, we showed how our method learns a
general program by optimizing over multiple users. It would be interesting to
investigate additional RL methods to optimize the interventions globally and
locally to provide more personalized sequences to the users. Such methods
could be coupled with interactive approaches eliciting preferences and con-
straints directly from the user, thus maximizing the chance to generate the
most appropriate intervention for a given user.

Ethical Impact

The research ﬁeld of algorithmic recourse aims at improving fairness, by pro-
viding unfairly treated users with tools to overturn unfavourable outcomes.
By providing real-time, explainable interventions, our work makes a step fur-
ther in making these tools widely accessible. As for other approaches providing
counterfactual interventions, our model could in principle be adapted by mali-
cious users to “hack” a fair system. Research on adversarial training can help
in mitigating this risk.

Acknowledgments. This research was partially supported by TAILOR, a
project funded by EU Horizon 2020 research and innovation programme under
GA No 952215.

Declarations

Funding. This research was partially supported by TAILOR, a project
funded by EU Horizon 2020 research and innovation programme under GA No
952215.

Conﬂict of interest/Competing interests. The authors have no com-
peting interests to declare that are relevant to the content of this article.

Ethics approval. Not applicable

Springer Nature 2021 LATEX template

16

Synthesizing counterfactual policies for recourse with program synthesis

Consent to participate. Not applicable

Consent for publication. Not applicable

Availability of data and materials. The datasets used in the exper-
imental evaluation are freely available at https://github.com/unitn-sml/
syn-interventions-algorithmic-recourse.

Code availability. The code is freely available at https://github.com/
unitn-sml/syn-interventions-algorithmic-recourse.

Authors’ contributions. GDT designed the method, conducted the data
collection process, built the experimental infrastructure and performed the
relevant experiments. BL and AP contributed to the design of the method,
provided supervision and resources. All authors contributed to the writing of
the manuscript.

Appendix A Program Distillation Pseudocode

We present here the pseudocode of two algorithms. Algorithm 1 shows how to
distill the synthetic program from the agent and it refers to Step 3 of Fig. 4.
Algorithm 1 shows how the distilled program is applied at inference time to a
new user and it refers to Step 4 of Fig. 4.

Algorithm 1 Generate the explainable program. Given the automaton
reconstructed from the traces ˆP, we generate the explainable program P. For
each node, we train the decision tree classiﬁer on the traces stored (s, (f, x))i
to predict (f, x) from s.
Input: ˆP, automaton extracted from the traces
Output: P, explainable program

1: Let t = 0.
2: Let P = {}
3: for all node ∈ ˆP do
4:

program = node.program
if program! = ST OP then

if node.arcs.unique() > 1 then

Train a decision tree, D, on node.arcs
node.classif ier = D

else

node.classif ier = {T rue → node.arcs[0]}

end if
P.append(node)

5:

6:

7:

8:

9:

10:

11:

12:

end if

13:
14: end for
15: return P

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

17

Algorithm 2 Predict with the program. Given the program P, we can
infer the complete explainable intervention for a user s0 by traversing P. This
can be done recursively. The recursion ends when we either predict the STOP
action or we reach the maximum intervention length, α ∈ N+.
Input: P, s0, (f, x)0, I, R
Output: s,the ﬁnal state, I,the intervention actions, R, rules for each action

return st+1, I, R

1: if (f, x)t == (ST OP, 0) ∨ t > α then
2:
3: else
4:

(f, x)t+1, rulet+1 = P.get(f ).predict(st)
st+1 = (f, x)t.apply(st)

R.append(rulet+1)
I.append((f, x)t+1)
recursive call (P, st+1(f, x)t+1, I, R)

5:

6:

7:

8:

9:
10: end if

Appendix B Domain Speciﬁc Languages

(DSL)

We now show some Domain Speciﬁc Languages (DSL) used for the german
(Table B1) and synthetic experiments. For each setting, we show the functions
available, the argument type they accept and an exhaustive list of the poten-
tial arguments. Each program operates on a single feature. The name of the
program suggests the feature it operates on (e.g., CHANGE JOB operate on
the feature job). The programs which accept numerical arguments simply add
their argument to the current value of the target feature. The program STOP
does not accept any argument and it signals only the end of the interven-
tion without changing the features. The DSLs for the adult and synthetic long
experiments are similarly deﬁned and are omitted for brevity.

Program

Argument Type Argument

CHANGE SAVINGS
CHANGE JOB

CHANGE CREDIT
CHANGE HOUSING
CHANGE DURATION
CHANGE PURPOSE

Categorical
Categorical

Numerical
Categorical
Numerical
Categorical

STOP

-

unskilled resident,

unknown, little, moderate, rich, quite rich
unskilled non resident,
highly skilled
100, 1000, 2000, 5000
free, rent, own
10, 20, 30
business, car, domestic appliances, education, furni-
ture/equipment, radio/TV, repairs, vacation/others
-

skilled,

Table B1: Example of the DSL for the german experiment.

Springer Nature 2021 LATEX template

18

Synthesizing counterfactual policies for recourse with program synthesis

Program

Argument Type

Argument

CHANGE EDUCATION
CHANGE JOB
CHANGE INCOME
CHANGE HOUSE
CHANGE RELATION
STOP

Categorical
Categorical
Numerical
Categorical
Categorical
-

none,secondary school diploma, bachelor, master, phd
unemployed, worker, oﬃce worker, manager, ceo
5000, 10000, 20000, 30000, 40000, 50000
none, rent, own
single, married, divorced, widow/er
-

Table B2: Example of the DSL for the synthetic experiment.

References

[1] Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations
without opening the black box: Automated decisions and the gdpr. Harv.
JL & Tech. 31, 841 (2017)

[2] Karimi, A., Barthe, G., Sch¨olkopf, B., Valera, I.: A survey of algorith-
mic recourse: deﬁnitions, formulations, solutions, and prospects. arXiv
preprint arXiv:2010.04050 (2020)

[3] Dandl, S., Molnar, C., Binder, M., Bischl, B.: Multi-objective counterfac-

tual explanations. In: PPSN, pp. 448–469 (2020). Springer

[4] Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classi-
ﬁers through diverse counterfactual explanations. In: FAT*, pp. 607–617
(2020)

[5] Karimi, A., Barthe, G., Balle, B., Valera, I.: Model-agnostic counterfac-
tual explanations for consequential decisions. In: AISTATS, pp. 895–905
(2020). PMLR

[6] Guidotti, R., Monreale, A., Ruggieri, S., Pedreschi, D., Turini, F., Gian-
notti, F.: Local rule-based explanations of black box decision systems.
CoRR abs/1805.10820 (2018) https://arxiv.org/abs/1805.10820

[7] Stepin, I., Alonso, J.M., Pereira-Fari˜na, A.C.M.: A survey of contrastive
and counterfactual explanation generation methods for explainable arti-
ﬁcial intelligence. IEEE Access 9, 11974–12001 (2021)

[8] Karimi, A., Sch¨olkopf, B., Valera, I.: Algorithmic recourse: from counter-
factual explanations to interventions. In: FaccT, pp. 353–362 (2021)

[9] Voigt, P., Bussche, A.: The EU General Data Protection Regulation

(GDPR): A Practical Guide, 1st edn. Springer, ??? (2017)

[10] Barocas, S., Selbst, A., Raghavan, M.: The hidden assumptions behind
counterfactual explanations and principal reasons. In: FAT* (2020)

[11] De Toni, G., Erculiani, L., Passerini, A.: Learning compositional programs

Springer Nature 2021 LATEX template

Synthesizing counterfactual policies for recourse with program synthesis

19

with arguments and sampling. In: AIPLANS (2021)

[12] Pierrot, T., Ligner, G., Reed, S.E., Sigaud, O., Perrin, N., Laterre, A.,
Kas, D., Beguir, K., de Freitas, N.: Learning compositional neural pro-
grams with recursive tree search and planning. NeurIPS 32, 14673–14683
(2019)

[13] Bunel, R., Hausknecht, M., Devlin, J., Singh, R., Kohli, P.: Leveraging
grammar and reinforcement learning for neural program synthesis. In:
ICLR (2018). https://openreview.net/forum?id=H1Xw62kRZ

[14] Balog, M., Gaunt, A.L., Brockschmidt, M., Nowozin, S., Tarlow, D.: Deep-
Coder: Learning to Write Programs. In: ICLR (2017). https://openreview.
net/pdf?id=rkE3y85ee

[15] Coulom, R.: Eﬃcient selectivity and backup operators in monte-carlo tree
search. In: In: Proceedings Computers and Games 2006. Springer, ???
(2006)

[16] Naumann, P., Ntoutsi, E.: Consequence-Aware Sequential Counter-
factual Generation. In: ECMLPKDD (2021). https://doi.org/10.1007/
978-3-030-86520-7 42

[17] Greenwell, B.M., Boehmke, B.C., McCarthy, A.J.: A simple and
eﬀective model-based variable importance measure. arXiv preprint
arXiv:1805.04755 (2018)

[18] Apley, D.W., Zhu, J.: Visualizing the eﬀects of predictor variables in black
box supervised learning models. Journal of the Royal Statistical Society:
Series B (Statistical Methodology) 82(4), 1059–1086 (2020)

[19] Miller, T.: Explanation in artiﬁcial intelligence: Insights from the social

sciences. Artiﬁcial intelligence 267, 1–38 (2019)

[20] Karimi, A., von K¨ugelgen, J., Sch¨olkopf, B., Valera,

I.: Algorith-
mic Recourse Under Imperfect Causal Knowledge: a Probabilistic
Approach. In: NeurIPS (2020). https://proceedings.neurips.cc/paper/
2020/ﬁle/02a3c7fb3f489288ae6942498498db20-Paper.pdf

[21] Ustun, B., Spangher, A., Liu, Y.: Actionable recourse in linear classiﬁca-

tion. In: FAT*, pp. 10–19 (2019)

[22] Yonadav, S., Moses, W.S.: Extracting incentives from black-box decisions.
CoRR abs/1910.05664 (2019) https://arxiv.org/abs/1910.05664

[23] Ramakrishnan, G., Lee, Y.C., Albarghouthi, A.: Synthesizing action
sequences for modifying model decisions. In: AAAI, vol. 34, pp. 5462–5469

Springer Nature 2021 LATEX template

20

Synthesizing counterfactual policies for recourse with program synthesis

(2020)

[24] Tsirtsis, S., Rodriguez, M.: Decisions, counterfactual explanations and
strategic behavior. In: NeurIPS (2020). https://proceedings.neurips.cc/
paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html

[25] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan,
K., Hassabis, D.: A general reinforcement learning algorithm that mas-
ters chess, shogi, and go through self-play. Science 362(6419), 1140–1144
(2018) https://arxiv.org/abs/https://www.science.org/doi/pdf/10.1126/
science.aar6404. https://doi.org/10.1126/science.aar6404

[26] Silver, D., A. Huang, C.J.M., Guez, A., Sifre, L., van den Driessche, G.,
Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Diele-
man, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering
the game of go with deep neural networks and tree search. Nature 529,
484–503 (2016)

[27] Kocsis, L., Szepesv´ari, C.: Bandit based monte-carlo planning. In: ECML,
pp. 282–293. Springer, Berlin, Heidelberg (2006). https://doi.org/10.
1007/11871842 29. https://doi.org/10.1007/11871842 29

[28] Dalcin, L., Fang, Y.: mpi4py: Status update after 12 years of development.
Computing in Science Engineering 23(4), 47–54 (2021). https://doi.org/
10.1109/MCSE.2021.3083216

[29] Dua, D., Graﬀ, C.: UCI Machine Learning Repository (2017). http://

archive.ics.uci.edu/ml

