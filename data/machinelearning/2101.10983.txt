UNSUPERVISED CLUSTERING OF SERIES USING DYNAMIC
PROGRAMMING AND NEURAL PROCESSES

1
2
0
2

n
a
J

6
2

]

G
L
.
s
c
[

1
v
3
8
9
0
1
.
1
0
1
2
:
v
i
X
r
a

Karthigan Sinnathamby∗
Visium
Lausanne, Switzerland
karthigan.sinnathamby@visium.ch

Chang-Yu Hou
Schlumberger-Doll Research
Cambridge, MA
CHou2@slb.com

Lalitha Venkataramanan
Schlumberger-Doll Research
Cambridge, MA
LVenkataramanan@slb.com

Vasileios-Marios Gkortsas
Schlumberger-Doll Research
Cambridge, MA
VGkortsas@slb.com

François Fleuret
University of Geneva
Geneva, Switzerland
francois.fleuret@unige.ch

January 2021

ABSTRACT

Following the work of Sinnathamby et al. [2021], we are interested in clustering a given multi-variate
series in an unsupervised manner. We would like to segment and cluster the series such that the
resulting blocks present in each cluster are coherent with respect to a predeﬁned model structure
(e.g. a physics model with a functional form deﬁned by a number of parameters). However, such
approach might have its limitation, partly because there may exist multiple models that describe the
same data, and partly because the exact model behind the data may not immediately known. Hence,
it is useful to establish a general framework that enables the integration of plausible models and
also accommodates data-driven approach into one approximated model to assist the clustering task.
Hence, in this work, we investigate the use of neural processes to build the approximated model while
yielding the same assumptions required by the algorithm presented in Sinnathamby et al. [2021].

Keywords Unsupervised Clustering · Series · Dynamic Programming · Neural Processes

1

Introduction

Clustering groups a set of data-points in such a way that data-points in the same cluster are more similar to each other
than to those in other groups. It can be achieved by various algorithms that differ signiﬁcantly in their understanding
of what constitutes a cluster and how to efﬁciently ﬁnd them. This task can be achieved by various algorithms (the
well-known K-means or spectral clustering but also hierarchical clustering Patel et al. [2015] or density-based clustering
Malzer and Baum [2019]) that differ signiﬁcantly in their understanding of what constitutes a cluster and how to
efﬁciently ﬁnd them.

The work presented in Sinnathamby et al. [2021] has designed an unsupervised clustering algorithm for any series
through dynamic programming by putting constraints on the number of cluster, the number of transition as well as
the minimal size of any block of points. This algorithm assumes that we can model each cluster by a speciﬁc process
(cluster characterization) and that given this characterization, we have a function to measure how well a point belongs to
this cluster (cluster afﬁliation). As a result, it clusters a given multi-variate series such that the resultant blocks present
in each cluster are coherent with respect to these two assumptions. In other words, data points are clustered together if
they can be described using the assumed model with the same parameters.

∗Work done while in Master Data Science at EPFL (Lausanne, Switzerland)

 
 
 
 
 
 
In Sinnathamby et al. [2021], the authors gave an example use-case by clustering the multi-variate series for an array of
petrophysical measurements through a modelisation with a known physics model correlating these data. In practice,
such modelisation process can be non-unique as there may exist multiple plausible models describing the same dataset,
a scenario often occurring for complex systems. In addition, an underlying model may not readily known. Hence, we
are interested in having a more general framework that makes use of deep neural networks to tackle the non-unique
modelisation case as well as the scenario for the absence of known models.

In this paper, we augment the clustering framework presented in Sinnathamby et al. [2021] with a more general
modelisation procedure with neural processes as introduced by Kim et al. [2019]. The neural processes use a
combination of neural networks that approximate the underlying processes with a distribution of stochastic models,
a concept similar to the stochastic processes. Given blocks of coherent datasets, the learning procedure of neural
processes is designed to provide the probabilistic distribution of unknown (targets) points conditioned with a number of
coherent data (context) points. Without an explicit functional form for the underlying model within the framework
of neural processes, we will demonstrate that one can directly use the context points as the signature of the cluster
characterization and use the likelihood of target points as the measure to partition the cluster afﬁliation. As a result,
the approximated models established through neural processes satisfy the two requirements for using the algorithm
introduced in Sinnathamby et al. [2021].

2 Neural Process

2.1 Description

We use neural processes (NPs) as described in Garnelo et al. [2018]. In particular, we make use of the version with
attention mechanism: the attentive neural processes (ANPs) introduced in Kim et al. [2019]. NPs offer an efﬁcient
method to modeling a distribution over regression functions: they can predict the distribution of an arbitrary target
point conditioned on a set of context points of an arbitrary size. This ﬂexibility of NPs enables them to model data that
can be interpreted as being generated from a stochastic process. NPs are trained on samples from multiple realizations
of a stochastic process (i.e. trained on many different functions).

Throughout our discussions, we will use the following notations. The variable y designates the output of the regression
and x is the input. ¯C is the subset of the context points and ¯T of the target points.

A NP consists of a set of three neural networks: a latent encoder, a deterministic encoder and a decoder. Each neural
network has a main multi-layer perceptron (MLP) part: M LPψ for the latent encoder, M LPθ for the deterministic
encoder and M LPφ for the decoder as depicted in ﬁgure 1. The inputs to the encoders are the points (xi,yi). The
deterministic encoder outputs ri for each pair (xi,yi) of context points and yields eventually r ¯C after applying a mean
operation over the ri. The latent encoder outputs si for each pair (xi,yi) of context points and yields eventually s ¯C
which parametrizes a Gaussian multivariate distribution, from which a variable z is sampled. If the points (xi,yi) are
the target points, we note s ¯T instead of s ¯C. Finally, the decoder takes as input r ¯C, z and x∗ (the input of the target
points) and outputs the mean and the standard deviation of y∗ (the output of the target points) assuming a Gaussian
distribution to account for the observation noise. We present the architecture in the ﬁgure 1 which is inspired from
ﬁgure 2 of Kim et al. [2019].

(a) Neural Process

(b) Attentive Neural Process

Figure 1: Model architecture for the NP (left) and ANP (right): in prediction, xi and yi are the context points and x∗
and y∗ are the target points to predict. Inspired from ﬁgure 2 of Kim et al. [2019].

2

The parameters of all the networks are jointly learned by maximising the following Evidence Lower Bound (ELBO) as
in Kim et al. [2019]:

log p(y ¯T |x ¯T , x ¯C, y ¯C) ≥ Eq(z|s ¯T )[log p(y ¯T |x ¯T , r ¯C, z)] − DKL(q(z|s ¯T )(cid:107)q(z|s ¯C))

(1)

where (x ¯T ,y ¯T ) are the target points, (x ¯C,y ¯C) the context points, p is a Gaussian distribution parametrized by the decoder,
q is a Gaussian distribution parametrized by the latent encoder.

2.2

Integration to Dynamic Programming

The algorithm described in Sinnathamby et al. [2021] assume that one provides a cluster characterization and a cluster
afﬁliation functions. The context points represent here the points of a cluster. This is the key link between NPs and
the clustering algorithm. The NP network predicts the output of a target point from its input x∗, given the input x ¯C
and output y ¯C of the context points. As such, r ¯C and z (as deﬁned in the ﬁgure 1) represent the weights of the cluster
(cluster characterization). If using attention mechanism, the encoders compute the weights of the cluster by attending to
the target point (r∗). In this case, we do not have an explicit cluster weights but the encoders do compute implicitly a
representation of the context points. As for the cluster afﬁliation, we can use the NLL part of the loss.

3 Application to petrophysics

Here we revisit the petrophysical application for the unsupervised clustering scheme established in Sinnathamby et al.
[2021]. The difference here is that we ﬁrst apply the modelisation procedure with the neural processes to learn an
approximate model before applying the dynamic programming clustering. The neural processes will be trained with
three plausible models describing the physics relations among data points. This is an effective way to integrate multiple
models into the stochastic regression process. Here, we will demonstrate how we can effectively use the modelisation
of neural processes for the unsupervised clustering based on the dynamic programming.

3.1 Physics Equations

Let us start by reviewing the petrophysical problem studied in Sinnathamby et al. [2021]. We are interested in clustering
the multi-variate series of petrophysical measurements/interpretations consisting of conductivity response of the water-
ﬁlled rock (σo), the porosity (φ), the volume fraction of clay (fclay) and the water saturation fraction (Sw). These
measurements are often collected and presented as consecutive data points along the depths of the well, and are often
refereed as logs. In Sinnathamby et al. [2021], the authors have used a particular relation between these quantities
prescribed by the Waxman-Smits (WS) equation Waxman and Smits [1968]. However, many other physics relations
have been established to correlate these quantities. To exhibit how the neural processes can be used to combine multiple
models, we will adapt three known physics models described by WS, Sen-Goode-Sibbit (SGS) and Archie’s equations,
for training an integrated neural process model Sen et al. [1988].

All three considered physics models aim to correlate the aforementioned quantities with a small number of parameters
that control the underlying physics processes. Both WS and SGS equations are established to account for the clay effect
to the conductivity of water-ﬁlled rocks, while Archie’s equation is for rocks with negligible clay effects. Speciﬁc forms
of these equations are listed below.

The WS equation is:

σo = φm × Sn

w ×

(cid:18)

σw +

B × Qv
Sw

(cid:19)

,

(2)

where m and n are some parameters and B is a function of the temperature T which is considered constant throughout
the studied logs. In addition, σw represents the conductivity of water ﬁlling the pore space, and Qv is the volume
concentration of clay exchange cations with respect to the pore space, which is associated with the cation exchange
capacity (CEC) of the rock:

Qv =

CEC × fclay × (1 − φ)
φ

.

The SGS equation is:



σo = φm × Sn

w ×

σw +

1.93 × m × µT × Qv
1 + 0.7×µT ×S−n

w

σw


 + 1.3 × µT × φm × Qv,

(3)

(4)

3

(a) Training loss

(b) Training loss (starting epoch 50)

Figure 2: Training of ANP. The y-axis is the negative log-likelihood (NLL) and the x-axis is the number of epoch. We
give the training loss curve for the training of the ANP (a) and the same curve but starting at epoch 50 (b). Beware the
scale of y is different between the ﬁgures.

where

µT = 1 + 0.0414 × (T − 22)
(5)
is the effective mobility of double layer cations with T the temperature given in Celsius. Here, m and n are parameters
similar to those in WS equation, σw is the conductivity of water ﬁlling the pore space and Qv is deﬁned in Eq. (3).

When the effect of clay becomes negligible, namely, Qv → 0, both WS and SGS equations effectively reduce to the
Archie’s equation:

σo = φm × Sn

w × σw

(6)

that describe the rock conductivity response of clean formations in the absence of clay. Although one may think that
Archie’s equation is simply a special case for WS and SGS equations, it is important to include this limiting case as an
independent relation for the training of NPs, not only because it represents a huge class of rocks, but also because it
provide a relation directly discount the clay effect.

Now, we have three equations relating the interested logs. These equations with a speciﬁc set of parameters can be seen
as the realization of a stochastic process. A neural process trained on data generated by these three equations would
consist of a more general modelisation or at least an aggregated one. Instead of having explicit parameters to deﬁne a
coherent cluster, one would directly use the context points (the cluster) as the representation of a cluster that deﬁne the
regression function over the distribution approximated by the NP.

3.2 Neural Process Training

Here we provide some details on our training procedure to integrate three physics models into the neural processes.

Let us start by describing how we build/generate the dataset. We will ﬁx all the parameters controlling the underlying
physics processes, m, n, σw and/or CEC, within a physically realistic range. At each epoch, we draw randomly 3000
(size of the training set) sets of parameters and a physics equation among the three models: each of the 3000 draws
have a randomly chosen set of parameters and a randomly selected equation.

For each set, we ﬁrst randomly generate three lists of φ, Sw and fclay within a physically realistic range with 200
elements in each list. For NPs training, these lists are the input of the regression, x. From these three lists and the
selected equation from Eq. (2)– (6), we then generate a list of 200 points of conductivity of rocks with reasonable
noise added into the both input parameters and also output signals. For NPs training, this list of conductivities is
basically the output of regression, y. Among 200 quadruples from these lists, between 50 and 100 quadruples are used
as context points (randomly chosen). The target points consist of all the 200 points. As a result, the training is done
with coherent context points: we know that they come from a same equation using the same parameters due to the
simulation. The target points are also coherent with the context points. In other words, these points should come from
the same stochastic process realization.

Eventually, we have used the same structure of ReLU activated linear layers for the three networks as in Kim et al.
[2019] as well as the same particularization tricks and optimizer. More precisely, we have:

4

050010001500200025003000Epoch−1.0−0.50.00.51.01.5NLL5050010001500200025003000Epoch−1.05−1.00−0.95−0.90−0.85−0.80−0.75−0.70NLLDatasets

Method

Prediction

WS-1

WS-2

WS
ANP

WS
ANP

WS-2-smooth WS
ANP

WS-3

WS-3-smooth

SGS-1

SGS-2

WS
ANP

WS
ANP

WS
ANP

WS
ANP

Most common
Most common
Lowest cost
Most common
Most common
Lowest cost
Most common
Most common
Lowest cost
Most common
Most common
Lowest cost
Lowest cost
Most common
Lowest cost
Most common
Most common
Lowest cost
Most common
Most common
Lowest cost

costpred
0.170
-1.06
-1.12
0.161
-1.07
-1.09
0.164
-1.07
-1.07
0.170
-1.10
-1.13
0.166
-1.08
-1.13
0.163
-1.08
-1.12
0.166
-1.10
-1.15

costtrue ARI
0.76
0.83
0.96
0.91
0.74
0.99
0.82
0.70
0.70
0.81
0.93
0.98
0.84
0.86
0.98
1.0
0.80
0.94
0.82
0.86
1.00

0.170
-1.11
-1.11
0.165
-1.09
-1.09
0.161
-1.09
-1.09
0.166
-1.13
-1.13
0.164
-1.13
-1.13
0.161
-1.10
-1.1
0.165
-1.14
-1.14

Table 1: Results for simulated data. For each dataset, we have a line for the best answer using WS (same results as in
Sinnathamby et al. [2021]) and a line for the most common and the lowest cost answers using ANP. costpred is the
cost of the prediction and costtrue is the cost of the ground truth answer. Beware, the cost for the WS is the MSE
while for ANP it is the NLL. ARI is the Adjusted Rand Index between the prediction and the ground truth as deﬁned in
Sinnathamby et al. [2021]. In bold, we have the prediction with the highest ARI.

• the latent encoder consists of three layers (uniform self-attention) before the layer for the mean and the one for

the sigma of the Gaussian distribution,

• the deterministic encoder has two layers (uniform self-attention) and a cross-attention module with 8 heads

(the keys and queries are ﬁrst passed through two layers),

• the decoder has three layers.

All layers have a hidden output size of 16 (except the last one of the decoder which has two - the mean and sigma of the
one-dimensional regressed value), giving up 3778 parameters in total. We give the training Negative log likelihood
(NLL) curve in Fig. 2.

4 Experiments & Results

We have conducted the same experiments as in Sinnathamby et al. [2021], where multiple examples of synthetic
multi-variate series of σ0, φ, Sw and fclay are generated with multiple blocks belonging to different clusters. All the
data points have noise added to them in the level expected from measurement and interpretation precision. In total,
seven testing examples are generated and used to test the dynamic programming clustering algorithm together with the
NPs models. Five examples consist of blocks satisfying the either WS or Archie’s equation with varying parameters
(WS-1, WS-2, WS-2-smooth, WS-3 and WS-3-smooth). Two examples haves blocks obeying WS, SGS or Archie’s
equation (SGS-1 and SGS-2).

Similar to the strategy discussed in Sinnathamby et al. [2021], we perform the grid search by applying the algorithm
with a range of the number of cluster, Cgrid, and the number of transition, Ngrid. After the grid search, we present
the ﬁnal clustering results of the lowest NLL and of the most common pattern among a selected combinations of
(Cgrid, Ngrid). Table 1 summarizes the cost (NLL) of the clustering pattern and Adjust Rand Index (ARI) between the
clustering results and the ground truth of the generated examples. For comparison, we also list the cost (L2 loss) and
ARI of the clustering results using the same algorithm and WS equation as the underlying physics equation.

5

label

m

0
1
2
3
4
5
6
7

1.85
2.0
2.05
2.3
2.5
2.0
2.0
2.1

n

1.7
2.0
2.0
2.1
2.2
2.5
1.9
2.1

ρw
0.03
0.03
0.029
0.031
0.049
0.05
0.05
0.051

CEC

0
0
30
0
80
0
0
45

Table 2: Parameters for WS-3

Figure 3: Grid-search criterion for WS-3
Most common: Ngrid = 8, Cgrid = 6

From the ARI scores in table 1, the clustering using the method with NP provides very reliable predictions over all
the testing datasets. For WS-1, WS-2, WS-3, WS-3-smooth and SGS-2, the lowest cost answer of the ANP yields a
better ARI than that in the WS case. Moreover, the most common pattern of the ANP also yields a comparable answer
compared to the clustering pattern using WS equation as characterization reference. Even though it may not represent
the most generic case, a pleasant surprise is that the lowest cost pattern of the ANP not only yields a better answer but it
also gives almost a perfect pattern as indicated by their ARI scores.

We will present our clustering results in the following fashion. For each dataset, we give the table of parameters that
were used to generate the data along with the cluster label as shown in Table 2. We then provide a scatter plot of
the cost against N for given numbers of clusters as depicted in Fig. 3. From this plot, we select the combinations of
(Cgrid, Ngrid), indicated by red ticks on the vertical axis, to be used for identifying the most common pattern. Here, we
practice the similar selecting criterion as laid out in Sinnathamby et al. [2021].

(a) Most common

(b) Lowest cost

Figure 4: Predictions for WS-3

The clustering predictions from the algorithm with NPs model are shown in plots similar to those in Fig. 4 (a) and (b).
On those plots, the left ﬁgure is either the most common or the lowest cost prediction and the right one represents the
ground truth. Each color represents a different cluster. However, the color coding for the ground truth pattern and for
the predicted clustering might have nothing in common. The y axis represents the depth and the x axis is simply for
illustration purpose.

6

(a) Most common

(b) Lowest cost

Figure 5: Confusion matrices for WS-3

We also give their confusion matrix with respective to the ground truth as shown in Fig. 5. The confusion matrix give us
a convenient way to inspect the difference between the cluster predicted by the algorithm and the ground truth. By cross
checking the prediction plots and the confusion matrix, we can better judge the quality of the clustering. As shown in
Figs. 4 and 5, both the most common case and the lowest cost cases show promising clustering results. Here we observe
nearly perfect clustering pattern by the lowest cost pattern given in Fig. 4 (b) and Fig. 5 (b). However, beside a mix-up
between the cluster 3 and 5, the most common pattern also provide a quality clustering results as shown in Fig. 5.

The clustering results for all other testing examples are presented in appendix in similar fashions. We observe similar
clustering quality throughout our testing examples, which demonstrate the potential of using a trained NP model as the
reference for clustering characteristic.

5 Discussion

Compared with the clustering results presented in Sinnathamby et al. [2021], we see much more heterogeneity in the
cost scatter plot when using NPs as the underlying characteristic model. Unlike the case directly using WS equation
as the reference, where we can see a more horizontal behavior, i.e. for many (N ,C) tuples we have the same or very
similar cost, we do not have such a clear behavior on the scatter plots of the NPs method.

This heterogeneity in the cost can be explained by several reasons. First, one must note that we use the NLL from NPs
as the cluster afﬁliation metric, which takes into account both the mean and the variance of the predicted value. This in
principle makes the NLL from NPs a more precise measure than the L2 loss from a variance-free physics model such
as WS equation. However, when going for bigger N and C, the random initialized blocks will be smaller, yielding
eventually a fragmented structure which inherently gives a higher variance and hence a higher NLL when predicting the
rest of the points given each of these blocks. This is a clear contrast from using a covariance-free model.

On the other hand, the algorithm might be stuck in a local minimum afterwards providing still low NLL (the NPs are
good at their job) but not the lowest achievable NLL. We expect that as N and C grow, the NLL goes down until a
plateau is reached. We might not see this phenomenon with the NPs method. Taking into account these remarks, we
believe that for pairs (N ,C) with small N and C, the method with WS in Sinnathamby et al. [2021] will have similar
behavior as with NPs. However, for higher N and C, the behavior is different and more disparate in the NPs case.

We have observed that the lowest cost (NLL) answer in the NPs method might be the best prediction that one can
achieve, but we have not been able to prove it. We believe that one must look eventually both at the most common and
the lowest cost answer, and decide according to external information such as the approximate values for N and C we
expect or other logs data.

Overall the NPs method provide equal or better results than with directly the equations and offer a way to represent the
equations in a aggregated way while using the clustering algorithm.

7

6 Conclusion

We have extended the unsupervised clustering algorithm provided in Sinnathamby et al. [2021] by using neural processes
to approximate the underlying model used to characterize the coherence of the clusters. This enables the user to come
up with more expressiveness and compactness using deep neural networks while also providing more reliable answers.

References

Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan, Vasileios-Marios Gkortsas, and François Fleuret.

Unsupervised clustering of series using dynamic programming, 2021.

S. Patel, S. Sihmar, and A. Jatain. A study of hierarchical clustering algorithms. In 2015 2nd International Conference

on Computing for Sustainable Global Development (INDIACom), pages 537–541, 2015.

Claudia Malzer and Marcus Baum. A hybrid approach to hierarchical density-based cluster selection, 2019.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and

Yee Whye Teh. Attentive neural processes, 2019.

Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye

Teh. Neural processes, 2018.

M. H. Waxman and L. J. M. Smits. Electrical conductivities in oil-bearing shaly sands. Society of Petroleum Engineers

Journal, 8(02):107–122, 1968.

Pabitra N. Sen, Peter A. Goode, and Alan Sibbit. Electrical conduction in clay bearing sandstones at low and high

salinities. Journal of Applied Physics, 63(10):4832–4840, 1988.

Appendix A Other Datasets

Here we present the clustering results for all the testing examples, beside WS-3, in the same fashion indicated in the
main text.

8

label

m

0
1
2
3
4
5

2.1
1.9
1.8
2.05
2.2
2.4

n

2.3
1.8
1.75
1.9
2.0
2.1

ρw
0.052
0.05
0.052
0.05
0.048
0.048

CEC

0
0
0
0
20
60

Table 3: Parameters for WS-1

Figure 6: Grid-search criterion for WS-1
Most common: Ngrid = 14, Cgrid = 10

(a) Most common

(b) Lowest cost

Figure 7: Predictions for WS-1

(a) Most common

(b) Lowest cost

Figure 8: Confusion matrices for WS-1

9

label

m

0
1
2
3
4
5

1.85
2.1
2.4
1.9
2.0
2.0

n

1.8
2.0
2.3
2.0
1.95
2.5

ρw
0.052
0.052
0.049
0.051
0.051
0.05

CEC

0
0
80
0
30
0

Table 4: Parameters for WS-2

Figure 9: Grid-search criterion for WS-2
Most common: Ngrid = 14, Cgrid = 8

(a) Most common

(b) Lowest cost

Figure 10: Predictions for WS-2

(a) Most common

(b) Lowest cost

Figure 11: Confusion matrices for WS-2

10

label

m

0
1
2
3
4
5

1.85
2.1
2.4
1.9
2.0
2.0

n

1.8
2.0
2.3
2.0
1.95
2.5

ρw
0.052
0.052
0.049
0.051
0.051
0.05

CEC

0
0
80
0
30
0

Table 5: Parameters for WS-2-smooth

Figure 12: Grid-search criterion for WS-2-smooth
Most common: Ngrid = 13, Cgrid = 6

(a) Most common

(b) Lowest cost

Figure 13: Predictions for WS-2-smooth

(a) Most common

(b) Lowest cost

Figure 14: Confusion matrices for WS-2-smooth

11

label

m

0
1
2
3
4
5
6
7

1.85
2.0
2.05
2.3
2.5
2.0
2.0
2.1

n

1.7
2.0
2.0
2.1
2.2
2.5
1.9
2.1

ρw
0.03
0.03
0.029
0.031
0.049
0.05
0.05
0.051

CEC

0
0
30
0
80
0
0
45

Table 6: Parameters for WS-3

Figure 15: Grid-search criterion
Most common: Ngrid = 16, Cgrid = 7

(a) Most common

(b) Lowest cost

Figure 16: Predictions for WS-3-smooth

(a) Most common

(b) Lowest cost

Figure 17: Confusion matrices for WS-3-smooth

12

label

m

0
1
2
3
4
5
6
7

2.0
1.85
2.1
2.4
2.4
1.9
2.0
2.0

n

1.95
1.8
2.0
2.3
2.3
2.0
1.95
2.5

ρw
0.051
0.052
0.052
0.049
0.049
0.051
0.051
0.05

CEC Equation

30
0
0
80
80
0
30
0

SGS
WS
WS
SGS
WS
WS
WS
WS

Table 7: Parameters for SGS-1

Figure 18: Grid-search criterion for SGS-1
Most common: Ngrid = 13, Cgrid = 9

(a) Most common

(b) Lowest cost

Figure 19: Predictions for SGS-1

(a) Most common

(b) Lowest cost

Figure 20: Confusion matrices for SGS-1

13

label

m

0
1
2
3
4
5
6
7
8
9
10

1.85
2.0
2.05
2.3
2.05
2.5
2.0
2.0
2.5
2.1
2.1

n

1.7
2.0
2.0
2.1
2.0
2.2
2.5
1.9
2.2
2.1
2.1

ρw
0.03
0.03
0.029
0.031
0.029
0.049
0.05
0.05
0.049
0.051
0.051

CEC

0
0
30
0
30
80
0
0
80
45
45

Table 8: Parameters for SGS-2

Figure 21: Grid-search criterion for SGS-2
Most common: Ngrid = 15, Cgrid = 10

(a) Most common

(b) Lowest cost

Figure 22: Predictions for SGS-2

(a) Most common

(b) Lowest cost

Figure 23: Confusion matrices for SGS-2

14

