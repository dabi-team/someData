Convex Programs and Lyapunov Functions for Reinforcement Learning:
A Uniﬁed Perspective on the Analysis of Value-Based Methods

Xingang Guo and Bin Hu

2
2
0
2

b
e
F
4
1

]

C
O
.
h
t
a
m

[

1
v
2
2
9
6
0
.
2
0
2
2
:
v
i
X
r
a

Abstract— Value-based methods play a fundamental role in
Markov decision processes (MDPs) and reinforcement learning
(RL). In this paper, we present a uniﬁed control-theoretic
framework for analyzing valued-based methods such as value
computation (VC), value iteration (VI), and temporal differ-
ence (TD) learning (with linear function approximation). Built
upon an intrinsic connection between value-based methods and
dynamic systems, we can directly use existing convex testing
conditions in control theory to derive various convergence
results for the aforementioned value-based methods. These
testing conditions are convex programs in form of either linear
programming (LP) or semideﬁnite programming (SDP), and
can be solved to construct Lyapunov functions in a straightfor-
ward manner. Our analysis reveals some intriguing connections
between feedback control systems and RL algorithms. It is
our hope that such connections can inspire more work at the
intersection of system/control theory and RL.

I. INTRODUCTION

Over the past 10 years, many research ideas have emerged
from the ﬁelds of control, optimization, and machine learn-
ing. A big research focus is on fundamental connections
between control systems and iterative algorithms. The re-
search on this topic has led to exciting new results on
algorithm analysis and design. For example, iterative opti-
mization methods have been analyzed as feedback control
systems [1]–[15], and control-theoretic tools have been lever-
aged to design new optimization algorithms in various set-
tings [16]–[22]. Recently, there has been an attempt to extend
such control perspectives to reinforcement learning (RL).
In [23], a fundamental connection between temporal differ-
ence learning and Markovian jump linear systems (MJLS)
has been established. In [24], the switching system theory has
been combined with the ODE method [25], [26] to analyze
the asymptotic convergence of Q-learning. More recently,
value iteration has also been connected to PID control [27].
Our paper is inspired by these prior results, and establishes a
new connection between RL and control theory. Speciﬁcally,
we tailor various convex testing conditions in control theory
for unifying the analysis of value-based algorithms.

RL refers to a collection of techniques for solving Markov
decision processes (MDPs), and has shown great promise
in many sequential decision making tasks [28]–[30]. Value-
based methods including value computation (VC), value iter-
ation (VI), and temporal difference (TD) learning [29] have
played a fundamental role in modern RL. The convergence

Xingang Guo and Bin Hu are with the Coordinated Science
Laboratory
and Com-
puter Engineering, University of Illinois at Urbana-Champaign. Email:
{xingang2, binhu7}@illinois.edu

the Department of Electrical

(CSL)

and

proofs for these methods are typically derived in a case-by-
case manner [28]–[37]. Such case-by-case analysis may not
be easily generalized. For example, the convergence proof for
VI is based on applying the contraction mapping theorem,
and requires identifying the right distance metric via deep
expert insights. The same distance metric may not be directly
used in analyzing other algorithms such as TD learning.

In this paper, we present a uniﬁed control-theoretic frame-
work for the convergence analysis of value-based methods. A
key observation is that value-based methods can be viewed as
dynamical control systems whose behaviors can be directly
analyzed using convex programs. In this paper, VC is mod-
eled as a linear time invariant (LTI) positive system, and VI
is viewed as a switched positive afﬁne system. In addition,
we also borrow the Markovian jump linear system (MJLS)
perspective on TD learning from [23]. Notice that there exist
many convex testing conditions for analyzing LTI positive
systems [38], [39], switched positive systems [40]–[45], and
MSLS [46]–[51]. We show that valued-based methods can
be analyzed by directly applying the existing linear program-
ming (LP) or semideﬁnite programming (SDP) conditions
from the positive system or MJLS theory. Importantly, we
can solve these convex conditions analytically to build our
Lyapunov-based proofs in a more transparent manner. It is
our hope that the proposed framework can inspire more work
at the intersection of system/control theory and RL.

Our analysis makes direct use of existing convex programs
in control theory, and complements the work in [23], [24],
[32] which rely on other types of stability analysis tools.
There are many other convex conditions in control theory,
and our work opens the possibility of re-examining these
conditions in the context of RL. Compared with [23], our
SDP approach has led to new stepsize bounds for TD
learning. This result will be given in Section III-C.

II. PRELIMINARIES AND PROBLEM FORMULATION

A. Notation

The set of n-dimensional real vectors is denoted as Rn.
The set of m × n real matrices is denoted as Rm×n. We use
Rn
+ to denote the set of the n-dimensional real vectors whose
entries are all non-negative. For x ∈ Rn, we denote its i-th
element as x(i). The inequality x > 0 (x ≥ 0) means that
x(i) > 0 (x(i) ≥ 0) for all i. For A ∈ Rn×n, the inequality
A > 0 (A ≥ 0) means all the entries of A are positive (non-
negative). We use AT and ρ(A) to denote the transpose and
the spectral radius of A, respectively. A matrix A is said to
be Schur stable if ρ(A) < 1. The inequality G (cid:31) 0 (G (cid:23) 0)
means that the matrix X is positive (semi-)deﬁnite.

 
 
 
 
 
 
B. Markov Decision Process and Reinforcement Learning

D. Value Iteration

First, we present some background materials on MDPs
and RL. Many decision making tasks can be formulated as
MDPs. Consider a MDP deﬁned by the tuple (S, A, P, R, γ),
where S is the set of states, A is the set of actions, P is the
transition kernel, R is the reward function, and γ ∈ (0, 1) is
the discount factor. In this paper, both S and A are assumed
to be ﬁnite. Without loss of generality, we assume S =
{1, 2, . . . , n} and A = {1, 2, . . . , l}. The transition kernel is
speciﬁed by P ((s, a), s(cid:48)) = P(sk+1 = s(cid:48)|sk = s, ak = a).

A policy is a feedback law mapping from states to actions.
A policy can be stochastic and maps each state to a prob-
ability distribution over A. The goal is to ﬁnd an optimal
policy that maximizes the total accumulated rewards:

π∗ = arg max

E

π

(cid:34) ∞
(cid:88)

k=0

γkR(sk, ak)(cid:12)

(cid:12)ak ∼ π(·|sk), s0

(cid:35)

.

To obtain an optimal policy, one can solve the optimal

value function J ∗ from the optimal Bellman equation:
(cid:33)

(cid:32)

J ∗(s) = max
a∈A

R(s, a) + γ

P ((s, a), s(cid:48))J ∗(s(cid:48))

. (1)

(cid:88)

s(cid:48)∈S

(cid:88)

Once J ∗ is found, one can construct the optimal policy as
(cid:33)

(cid:32)

π∗(s) = arg max

R(s, a) + γ

P ((s, a), s(cid:48))J ∗(s(cid:48))

.

a∈A

s(cid:48)∈S

The optimal Bellman equation depends on the transition
kernel. If the transition model is unknown, RL methods (e.g.
TD learning, Q-learning, policy gradient, etc) can be applied.

C. Value Computation

The performance of a given policy π can be evaluated
from the associated value function Jπ, which is deﬁned as

Jπ(i) = E

(cid:34) ∞
(cid:88)

k=0

γkR(sk, ak)(cid:12)

(cid:12)ak ∼ π(·|sk), s0 = i

.

(cid:35)

For given π, denote the probability transition matrix of {sk}
as Pπ. Then Jπ can be solved from the Bellman equation:

Jπ(i) = Rπ(i) + γ

(cid:88)

j

Pπ(i, j)Jπ(j),

(2)

where Pπ(i, j) is the (i, j)-th entry of Pπ, and Rπ(i) is the
immediate reward obtained from state i under the policy π.
The above Bellman equation can be compactly rewritten as

Jπ = Rπ + γPπJπ.
(3)
Obviously, Jπ can be calculated as Jπ = (I − γPπ)−1Rπ
for any 0 < γ < 1. To avoid matrix inversion, a popular
approach for solving Jπ is to apply the following iterative
value computation (VC) scheme:

Jk+1 = γPπJk + Rπ.

(4)

It is known that the above method is guaranteed to converge
to Jπ at a linear rate γ. This is actually obvious from the
linear system theory. For the right stochastic matrix Pπ,
we have ρ(Pπ) = 1. Hence the convergence of (4) can be
guaranteed by the fact that we have ρ(γPπ) = γ ∈ (0, 1).

One can solve the optimal value function J ∗ by recursively
applying the Bellman operator T (·) : Rn → Rn. This leads
to the famous value iteration (VI) method which iterates as

(cid:32)

Jk+1(s) = max
a∈A

R(s, a) + γ

(cid:33)

P ((s, a), s(cid:48))Jk(s(cid:48))

.

(cid:88)

s(cid:48)

A pseudo-code for VI is provided as follows.

Algorithm 1: Value iteration algorithm
Initialization: J(s) ← J0(s), ∀ s ∈ S;
Repeat
For all s ∈ S ;
J(s) ← maxa∈A (R(s, a) + γ (cid:80)
Until J converge

s(cid:48) P ((s, a), s(cid:48))J(s(cid:48)));

The iteration of VI can be compactly rewritten as

Jk+1 = T (Jk),

(5)

where T (·) is the Bellman optimality operator. It is known
that VI converges to J ∗ at the rate γ, and a standard way to
prove this is to apply the contraction mapping theorem [28].

E. TD Learning with Linear Function Approximation

It is very common that the transition kernel of MDP is
unknown. In this case, the VC scheme (4) is not appli-
cable. Instead, TD learning can be used to estimate the
value function from sampled trajectories of the underlying
Markov chain {sk}. Most applications have enormous state
spaces, making policy evaluation difﬁcult. Then one needs
to incorporate function approximation techniques. Suppose
the value function is estimated as Jπ(s) ≈ φ(s)Tθπ where
φ is the feature vector and θπ is the weight to be estimated.
One model-free way to estimate θπ is to apply the following
TD(0) recursion:
θk+1 = θk − αφ(sk) (cid:0)(φ(sk) − γφ(sk+1))Tθk − Rπ(sk)(cid:1) ,
where Rπ is the reward, γ is the discount factor, and α is
the learning rate. The Markov nature of {sk} has caused
trouble for the ﬁnite time analysis of the above method.
Very recently, various specialized tricks [23], [31], [32] have
been developed to address this technical difﬁculty, leading to
several useful ﬁnite time results for TD(0) with sufﬁciently
small α.

F. Main Objective: Uniﬁed Analysis of VC, VI, and TD

The objective of this work is to develop a simple routine
unifying the analysis of VC, VI, and TD(0) with linear
function approximation. Built upon the connections between
value-based methods and dynamic systems, we can directly
use existing convex programs (LP/SDP) in control theory
to analyze the above value-based methods. In addition,
these convex programs lead to different types of Lyapunov
functions, making the Lyapunov-based convergence analysis
transparent. Table I summarizes our main results in this work.
Our analysis sheds new light on how to combine convex
programs and Lyapunov analysis in the context of RL.

TABLE I
CONVEX PROGRAMS FOR VALUE-BASED METHODS

Value-based Algorithms
Value Computation
Value Iteration
TD(0) with Linear Function Approximation

Type of Dynamic Systems
LTI Positive system Eq.(6)
Switched positive system Eq.(11)
MJLS Eq.(22)

Convex Programs
LP & SDP (Theorem 1)
LP (Condition (13))
SDP (Proposition 3)

Lyapunov functions
Eq. (10)
Eq. (16)
Eq. (24)

III. UNIFIED ANALYSIS OF VALUE-BASED METHODS

A. LPs and SDPs for Analyzing VC

To analyze VC, we apply (3) and (4) to rewrite VC as

ζk+1 = Aπζk
(6)
where ζk = Jk −Jπ, and Aπ = γPπ ∈ Rn×n. Notice that (6)
is actually a positive system since Aπ = γPπ ≥ 0. To verify
the Schur stability of Aπ, the following convex conditions
for positive linear systems can be directly applied.

Proposition 1: Suppose Aπ ≥ 0. Then each of the follow-
ing conditions provides a necessary and sufﬁcient condition
for the stability of (6):

1) ∃ ξ ∈ Rn s.t. ξ > 0, and Aπξ − ξ < 0,
2) ∃ ν ∈ Rn s.t. ν > 0, and νTAπ − νT < 0,
3) ∃ G ∈ Rn×n s.t. G (cid:31) 0 AT

πGAπ ≺ G.

Proof: This result is just the discrete-time counterpart

of Proposition 1 in [39] and can be proved similarly.

The above convex programs can be solved to obtain three
types of Lyapunov functions for (6). Let 1n denote the n-
dimensional vector whose entries are all equal to 1. Then
our results can be stated as follows.

Theorem 1: Consider the recursion (6). Set (ξ, ν, G) as

ξ = 1n, ν = ω, G = diag

(cid:18) ν(1)
ξ(1)

, · · · ,

(cid:19)

,

ν(n)
ξ(n)

where ω is the stationary distribution of Pπ. Then we have

ξ > 0 and Aπξ ≤ γξ,
ν > 0 and νTAπ ≤ γνT,
G (cid:31) 0 and AT

πGAπ (cid:22) γ2G.

(7)

(8)

(9)

This leads to the following three types of Lyapunov functions

i

V1(ζ) = max

|ζ(i)|,

V2(ζ) = |νTζ|,

V3(ζ) = ζ TGζ,

(10)
which satisfy V1(ζk) ≤ C1γk, V2(ζk) ≤ C2γk, and V3(ζk) ≤
C3γ2k for some ﬁxed positive constants (C1, C2, C3).

Proof: Since Pπ is always a right stochastic matrix, we

have Pπ1n = 1n and ωTPπ = ωT. Therefore,

Aπ1n − γ1n = γ(Pπ1n − 1n) = 0
ωTAπ − γωT = γ (cid:0)ωTPπ − ωT(cid:1) = 0.

Hence (7) and (8) hold. The third condition can be proved
as discussed by Proposition 2 in [39]. The rest of the results
follow from standard arguments in positive system theory.

Notice that the conditions (7) (8) are LPs, and (9) can be
solved as SDPs. Theorem 1 provides three different types of
Lyapunov functions for the positive system (6):

1) (cid:96)∞-type: V1(ζ) = maxi |ζ(i)|;
2) Linear-type copositive: V2(ζ) = |νTζ|;
3) Quadratic: V3(ζ) = ζ TGζ.

It is trivial to show V1(ζk+1) ≤ γV1(ζk) and V3(ζk+1) ≤
γ2V3(ζk). For i = 2, the Lyapunov function is copositive
and works slightly differently. Here we brieﬂy explain how
it works. For any ζ0 ∈ Rn, ∃ζ +
0 ∈ Rn
0 −ζ −
0 .
Let {ζ +
k } and {ζ −
k } be the state trajectories of (6) initialized
0 and ζ −
from ζ +
0 , respectively. By linearity, we have ζk =
ζ +
k − ζ −
k . Based on the condition (8), we can show V2(ζk) ≤
k ) + V2(ζ −
V2(ζ +
0 ) + V2(ζ −
0 )). This ensures the
convergence of VC.

k ) ≤ γk(V2(ζ +

+ s.t. ζ0 = ζ +

0 , ζ −

A key message from the above analysis is that the Lya-
punov function construction for positive linear systems can
be simpler than general LTI systems due to the use of
LPs. Since the construction of the max-type (or (cid:96)∞-type)
Lyapunov function V1(·) is independent of the underlying
policy, we may construct an (cid:96)∞-type common Lyapunov
function for cases where the policy is changing over time.

B. LPs and Common Lyapunov Functions for VI

Next we establish the connection between VI and switched
positive afﬁne systems. This will lead to LP conditions for
analyzing VI. The VI scheme Jk+1 = T (Jk) can be recast as

Jk+1 = γPσk Jk + Rσk

(11)

where σk ∈ {1, 2, . . . , ln}. Recall
l and n denote the
size of action space and state space, respectively. When
σk = m, we set Pσk = Pm and Rσk = Rm.
For all m ∈ {1, 2, · · · , ln}, Pm is an n × n matrix
whose i-th row (for all i) is a row vector in the form
. . . P ((i, a), n)(cid:3) with some
of (cid:2)P ((i, a), 1) P ((i, a), 2)
a ∈ A. Similarly, for all m, the vector Rm is a vector whose
i-th element (for all i) is equal to R(i, a) for some a ∈ A.
The total number of the (Pm, Rm) pairs is ln, and we denote
the set of all such pairs as Λ. Therefore, we can just view
(11) as a switched positive afﬁne system, and it is not that
surprising that we can analyze VI via switched system theory.
For ease of exposition, we ﬁrst address the case where
Rm = 0 for all m. In this case, we have J ∗ = 0, and (11)
can be rewritten as a switched positive linear system:

Jk+1 = AmJk, m ∈ {1, 2, · · · , ln},
(12)
where Am = γPm ∈ Rn×n. A well-known fact is that
the system state of (12) may diverge for some switching
sequence even when Am is Schur stable for all m [52].
The stability guarantees for (12) are typically obtained by
extending the Lyapunov approach presented in Proposition 1.
One way is to use the common Lyapunov function (CLF).

Proposition 2: Suppose Am ≥ 0 for all m. Then each of
the following conditions provides a sufﬁcient condition for
the stability of the switched positive system (12):

1) ∃ξ ∈ Rn s.t. ξ > 0 and Amξ − ξ < 0 for all Am.
2) ∃ν ∈ Rn s.t. ν > 0 and νTAm − νT < 0 for all Am.
3) ∃ a matrix G (cid:31) 0 s.t. AT
mGAm − G ≺ 0 for all Am.
Proof: The proof is standard. The third condition actu-
ally does not require Am to be positive. If AT
mGAm−G ≺ 0,
then ∃ (cid:15) > 0 such that AT
mGAm − (1 − (cid:15))G (cid:22) 0. Hence we
can deﬁne a Lyapunov function V (Jk) = J T
k GJk satisfying
V (Jk+1) ≤ (1 − (cid:15))V (Jk). This ensures the stability of (12).
The ﬁrst and second conditions do require Am to be positive,
and can be proved similarly. See [44], [45] for details.

Similar to Theorem 1, the testing conditions in Proposi-
tion 2 can be modiﬁed to analyze convergence rates of (12).
One will obtain similar rate bounds as presented in Theo-
rem 1 if any of the following is feasible:

∃ ξ ∈ Rn s.t. ξ > 0 and Amξ ≤ γξ ∀ m.
∃ ν ∈ Rn s.t. ν > 0 and νTAm ≤ γνT ∀ m.
∃ G ∈ Rn×n s.t. G (cid:31) 0 and AT

mGAm (cid:22) γ2G ∀ m.

(13)

(14)

(15)

It is interesting to see that in general, the system (12) does not
have a common linear copositive Lyapunov function since
the stationary distributions for different Pm are typically
not the same. It also seems difﬁcult to construct a common
solution G for the SDP (15). However, since all Pm share
the same right eigenvector 1n, we have γPm1n = γ1n for
all m. Hence we can solve (13) to obtain an (cid:96)∞-type CLF:

V (Jk) = (cid:107)Jk − J ∗(cid:107)∞.

(16)

Condition (13) can be used to guarantee V (Jk) ≤ γkV (J0).
Now, we can extend the above analysis to the general case
where Rm (cid:54)= 0. In this case, we will show that the iterations
of VI can be upper and lower bounded by the trajectories
of two stable positive linear systems. Hence positive system
theory can still be applied. We need the following lemma.

Lemma 1: Consider

the switched positive afﬁne sys-
tem (11) with a switching sequence {σk} completely de-
termined by the Bellman optimality operator1. Then the
following inequality holds for all k

γP ∗(Jk − J ∗) ≤ Jk+1 − J ∗ ≤ γPσk (Jk − J ∗).

(17)

Proof:

Since J ∗ = T (J ∗),

there exists a pair
(P ∗, R∗) ∈ Λ such that J ∗ = γP ∗J ∗ +R∗. By the deﬁnition
of the Bellman optimality operator, one can show that the
following two inequalities holds for all m:

γPmJ ∗ + Rm ≤ γP ∗J ∗ + R∗,
γPmJk + Rm ≤ γPσk Jk + Rσk .

(18)

(19)

Using (18), (19), and the fact that J ∗ = γP ∗J ∗ + R∗, one
can verify γP ∗(Jk − J ∗) ≤ Jk+1 − J ∗ ≤ γPσk (Jk − J ∗).
This leads to the desired conclusion.

Based on Lemma 1, we obtain the following main result.

Theorem 2: Consider the switched positive afﬁne sys-
tem (11) with a switching sequence {σk} completely de-
termined by the Bellman optimality operator T . Suppose the
sequence {J u
k+1 − J ∗ =
k } is generated by the system J u
k − J ∗) with the same switching sequence {σk}. Let
γPσk (J u
the sequence {J o
k+1 − J ∗ =
k } be generated by the system J o
γP ∗(J o

k − J ∗). Suppose J0 = J u

0 . Then we have

0 = J o

k − J ∗, ∀k

k − J ∗ ≤ Jk − J ∗ ≤ J u
J o

(20)
Proof: This theorem can be proved using induction.
When k = 0, it is straightforward to verify that (20) holds
as a consequence of Lemma 1. Suppose (20) holds for k = t.
For k = t + 1, we can apply Lemma 1 to show
Jt+1 − J ∗ ≤ γPσk (Jt − J ∗) ≤ γPσk (J u
t+1 − J ∗
where the second step follows from the fact that Pσk is
right stochastic. Based on Lemma 1, we can use a similar
argument to show Jt+1 − J ∗ ≥ J o
t − J ∗. Hence (20) holds
for k = t + 1. This completes the proof.

t − J ∗) = J u

Therefore, we can directly apply the LP condition (13)
to construct an (cid:96)∞-type CLF for VI and prove the rate
bound (cid:107)Jk − J ∗(cid:107)∞ ≤ max{(cid:107)J u
k − J ∗(cid:107)∞} ≤
γk(cid:107)J0 − J ∗(cid:107)∞. This demonstrates how to apply the simple
LP condition (13) to analyze VI.

k − J ∗(cid:107)∞, (cid:107)J o

C. SDPs for TD(0) with Linear Function Approximation

(cid:3)T

k+1 sT
k

In this section, we provide SDP-based ﬁnite time analysis
for TD(0) with linear function approximation. Since TD(0)
can be viewed as a MJLS, the SDP-based stability conditions
for MJLS can be directly applied. Recall that TD(0) (with lin-
ear function approximation) follows the update rule θk+1 =
θk − αφ(sk) (cid:0)(φ(sk) − γφ(sk+1))Tθk − Rπ(sk)(cid:1), where φ
is the feature vector, and θ is the weight to be estimated.
We can augment (cid:2)sT
∈ S ⊕ S as a new vector zk.
Obviously, there is a one-to-one mapping from S ⊕ S to the
set N = {1, 2, · · · , n2}. Without loss of generality, {zk} can
be set up as a Markov chain sampled from N . Suppose θπ is
the solution to the projected Bellman equation for the ﬁxed
policy π. Due to the one-to-one correspondence between
(cid:2)sT
and zk, the iteration of TD(0) can be recast as

k+1 sT
k
θk+1 − θπ = θk − θπ + α (Azk (θk − θπ) + bzk ) ,

(21)
where Azk = φ(sk)(γφ(sk+1) − φ(sk))T and bzk =
φ(sk) (cid:0)Rπ(sk) + (φ(sk) − γφ(sk+1))Tθπ(cid:1). When zk = i ∈
N , we have Azk = Ai and bzk = bi. We denote ζk = θk−θπ.
Then (21) can be rewritten as the following MJLS:

(cid:3)T

ζk+1 = Hzk ζk + αbzk uk.
where Hzk = I + αAzk , and uk = 1 ∀k. When zk = i ∈ N ,
we have Hzk = Hi. Denote pij = P(zk+1 = j|zk = i), and
N = n2. Then the following mean square stability condition
[46]–[48] can be directly applied to analyze (22).

(22)

Proposition 3: The MJLS (22) is mean square stable
(MSS) if and only if there exist matrices Gi (cid:31) 0 for
i = 1, · · · , N such that the following SDP is feasible:





N
(cid:88)



pijGj

 Hi (cid:31) 0, for i = 1, · · · , N.

(23)

j=1

1In other words, the trajectory of such a switched system now exactly

matches the sequence generated by the VI method.

Gi − H T
i

Proof: This stability condition is well known. For more

details, see discussions in [46] or [47].

There are multiple ways to prove the mean square stability
of (22) from the SDP condition (23). One way is to construct
the following quadratic Lyapunov function from {Gi}:
V (ζk) = E (cid:2)ζ T

(24)

k Gzk ζk

(cid:3) .

Once the MJLS (22) is shown to be MSS, Theorem 3.33 in
[46] can be applied to show that (22) is also asymptotically
wide sense stationary, and then the mean square TD error
can be exactly calculated via Proposition 3.35 in [46]. As a
matter of fact, the convergence bounds in Corollary 2 of [23]
can be directly applied whenever (22) is MSS. Therefore,
the ﬁnite time analysis of TD(0) boils down to checking the
mean square stability of the MJLS (22). Next, we show how
to construct solutions for the SDP condition (23) under the
following standard assumption.

Assumption 1: Suppose {zk} is irreducible and aperiodic.
i = limk→∞ P(zk = i) and ¯A = (cid:80)N
∞Ai.

Denote p∞
We assume ¯A is Hurwitz, and (cid:80)N

i=1 pi

Under Assumption 1, let ¯G be the solution to the Lyapunov
¯G +

equation ¯AT ¯G + ¯G ¯A = −I. We also denote Xi = AT
i
¯GAi + I/(p∞

i N ). Now we can state the following result.

i bi = 0.

i=1 p∞

Lemma 2: Supposed Assumption 1 is given. For sufﬁ-
ciently small α, we can solve the SDP (23) by choosing
Gi = ¯G+α ˜Gi, where ˜GN = 0 and ˜Gi (for i = 1, · · · , N −1)
is solved from the following linear equation:

N −1
(cid:88)

˜Gi −

pij ˜Gj = Xi,

for i = 1, · · · , N − 1.

(25)

j=1

Proof: First, notice that (25) does have a unique solu-
tion. To see this, let ˆP ∈ R(N −1)×(N −1) be a substochastic
to pij. Then ˆP is
matrix whose (i, j)-th entry is equal
a submatrix of the transition matrix of {zk}, and has a
spectral radius which is smaller than 1. Hence (IN −1 − ˆP )
is invertible, and (25) admits a unique well-deﬁned solution.
Since Xi is symmetric for all i, the resultant matrices { ˜Gi}
are also symmetric. Now we brieﬂy explain our choices of
Gi. If we substitute Gi = ¯G + α ˜Gi into (23), we get

˜Gi −

N
(cid:88)

j=1

pij ˜Gj − (AT
i

¯G + ¯GAi) + O(α) (cid:31) 0, ∀i

(26)

For i = 1, · · · , N − 1, we can substitute (25) and ˜GN = 0
into (26) to simplify it as I/(p∞
i N ) + O(α) (cid:31) 0, which
clearly holds for sufﬁciently small α. For i = N , we can
use the fact ¯AT ¯G + ¯G ¯A = −I to show

AT
N

¯G + ¯GAN =

1
p∞
N

(cid:32)

−I −

N −1
(cid:88)

i=1

(cid:33)

i (AT
p∞
i

¯G + ¯GAi)

¯G + ¯GAi = ˜Gi − (cid:80)N −1

We have AT
i N ) for
i
i < N (see (25)). Substituting these into (26) for i = N
leads to I/(p∞

N N ) + O(α) (cid:31) 0, which holds for small α.

j=1 pij ˜Gj − I/(p∞

Next, we provide an explicit upper bound on α. To make
sure that { ¯G + α ˜Gi} solves the SDP condition (23), we need

¯G + α ˜Gi (cid:31) 0, I/(p∞

i N ) + αMi + α2 ˜Mi (cid:31) 0, ∀i

(27)

i

i ((cid:80)N

1/(p∞

j=1 pij ˜Gj) − ((cid:80)N

j=1 pij ˜Gj)Ai, and Mi = −AT

¯GAi −
where ˜Mi = −AT
i ((cid:80)N
j=1 pij ˜Gj)Ai. We know ¯G (cid:31) 0 and
AT
i N ) (cid:31) 0. Notice { ˜Gi}, {Mi}, and { ˜Mi} are symmetric
I/(p∞
matrices which are completely determined by {Ai} and pij.
Hence the SDP condition (23) is feasible with Gi = ¯G+α ˜Gi
if for all i ∈ N , α satisﬁes λmin( ¯G) + αλmin( ˜Gi) > 0 and
i N ) + αλmin(Mi) + α2λmin( ˜Mi) > 0.
where λmin denotes the smallest eigenvalue. Let 1D denote
the indicator function for any set D. We have λmin( ¯G) +
αλmin( ˜Gi) > 0 for any 0 < α <
. When
˜Gi (cid:23) 0, this bound becomes +∞. It is also straightfor-
ward to verify that (28) is true for any 0 < α < ¯αi,
1
where ¯αi
is deﬁned as ¯αi =
if
p∞
i N |λmin(Mi)|(1−1Mi(cid:23)0)
min(Mi)−4λmin( ˜Mi)/(p∞
λ2
˜Mi (cid:23) 0, and ¯αi = −λmin(Mi)−
2λmin( ˜Mi)
otherwise. This leads to the following result.

λmin( ¯G)
|λmin( ˜Gi)|(1−1 ˜Gi(cid:23)0)

(28)

i N )

√

Theorem 3: Given Assumption 1, the TD(0) method (21)
(cid:27)

with step size 0 < α < mini∈N
is MSS, and the mean square estimation error E(cid:107)θk − θπ(cid:107)2
converges exponentially to its stationary value.

λmin( ¯G)
|λmin( ˜Gi)|(1−1 ˜Gi(cid:23)0)

¯αi,

(cid:26)

Proof: From the above discussion, our stepsize bound
can guarantee (27), and hence (21) is MSS. Then the
convergence behavior of E(cid:107)θk − θπ(cid:107)2 can be shown using
Proposition 3.35 of [46] or Corollary 2 of [23].

With the MSS property, we can directly apply Corollary 2
in [23] to obtain explicit formulas for the convergence rate
and the steady state error. We skip those formulas. Clearly,
our result is closely related to [23] which also analyzes TD
learning using MJLS theory. A key difference is that the
analysis in [23] boils down to an LTI system formulation
without exploiting the SDP (23). Our SDP approach brings
a new beneﬁt in providing an explicit stepsize bound guar-
anteeing MSS, as speciﬁed by Theorem 3. In contrast, the
analysis in [23] relies on advanced eigenvalue perturbation
theory and only shows that TD(0) is MSS for sufﬁciently
small α without providing such explicit stepsize bounds.

IV. CONCLUSION AND FUTURE WORK

In this paper, we show that existing convex programs in
control theory can be directly used to analyze value-based
methods such as VC, VI, and TD(0) with linear function
approximation. It is possible that these convex programs can
be extended to address the impacts of computation error and
delay. This will be investigated in the future.

ACKNOWLEDGMENT

This work is generously supported by the NSF award

CAREER-2048168 and the 2020 Amazon research award.

REFERENCES

[1] L. Lessard, B. Recht, and A. Packard, “Analysis and design of opti-
mization algorithms via integral quadratic constraints,” SIAM Journal
on Optimization, vol. 26, no. 1, pp. 57–95, 2016.

[2] R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan, “A
general analysis of the convergence of ADMM,” in International
Conference on Machine Learning, 2015, pp. 343–352.

[3] B. Hu and L. Lessard, “Dissipativity theory for Nesterov’s accelerated
method,” in International Conference on Machine Learning, vol. 70,
2017, pp. 1549–1557.

[4] M. Fazlyab, A. Ribeiro, M. Morari, and V. M. Preciado, “Analysis of
optimization algorithms via integral quadratic constraints: Nonstrongly
convex problems,” SIAM Journal on Optimization, vol. 28, no. 3, pp.
2654–2689, 2018.

[5] A. Sundararajan, B. Hu, and L. Lessard, “Robust convergence analysis
of distributed optimization algorithms,” in Annual Allerton Conference
on Communication, Control, and Computing, 2017, pp. 1206–1212.

[6] B. Hu and L. Lessard, “Control interpretations for ﬁrst-order optimiza-
tion methods,” in American Control Conference, 2017, pp. 3114–3119.
[7] B. Hu, P. Seiler, and A. Rantzer, “A uniﬁed analysis of stochastic opti-
mization methods using jump system theory and quadratic constraints,”
in Conference on Learning Theory, vol. 65, 2017, pp. 1157–1189.
[8] T. Hatanaka, N. Chopra, T. Ishizaki, and N. Li, “Passivity-based dis-
tributed optimization with communication delays using PI consensus
algorithm,” IEEE Transactions on Automatic Control, vol. 63, no. 12,
pp. 4421–4428, 2018.

[9] B. Hu, S. Wright, and L. Lessard, “Dissipativity theory for acceler-
ating stochastic variance reduction: A uniﬁed analysis of SVRG and
Katyusha using semideﬁnite programs,” in International Conference
on Machine Learning, 2018, pp. 2043–2052.

[10] S. Han, “Systematic design of decentralized algorithms for consensus
optimization,” IEEE Control Systems Letters, vol. 3, no. 4, pp. 966–
971, 2019.

[11] N. S. Aybat, A. Fallah, M. Gurbuzbalaban, and A. Ozdaglar, “Robust
accelerated gradient methods for smooth strongly convex functions,”
SIAM Journal on Optimization, vol. 30, no. 1, pp. 717–751, 2020.

[12] H. Xiong, Y. Chi, B. Hu, and W. Zhang, “Analytical convergence
regions of accelerated gradient descent in nonconvex optimization
under regularity condition,” Automatica, vol. 113, 2020.

[13] H. Mohammadi, M. Razaviyayn, and M. R. Jovanovi´c, “Robustness
of accelerated ﬁrst-order algorithms for strongly convex optimization
problems,” IEEE Transactions on Automatic Control, vol. 66, no. 6,
pp. 2480–2495, 2020.

[14] B. Hu, P. Seiler, and L. Lessard, “Analysis of biased stochastic gra-
dient descent using sequential semideﬁnite programs,” Mathematical
Programming, vol. 187, no. 1, pp. 383–408, 2021.

[15] O. Gannot, “A frequency-domain analysis of inexact gradient meth-

ods,” Mathematical Programming, pp. 1–42, 2021.

[16] B. Van Scoy, R. Freeman, and K. Lynch, “The fastest known globally
convergent ﬁrst-order method for minimizing strongly convex func-
tions,” IEEE Control Systems Letters, vol. 2, no. 1, pp. 49–54, 2017.
[17] S. Cyrus, B. Hu, B. Van Scoy, and L. Lessard, “A robust accelerated
optimization algorithm for strongly convex functions,” in American
Control Conference, 2018, pp. 1376–1381.

[18] M. Fazlyab, M. Morari, and V. M. Preciado, “Design of ﬁrst-order
optimization algorithms via sum-of-squares programming,” in IEEE
Conference on Decision and Control, 2018, pp. 4445–4452.

[19] Z. Nelson and E. Mallada, “An integral quadratic constraint frame-
work for real-time steady-state optimization of linear time-invariant
systems,” in American Control Conference, 2018, pp. 597–603.
[20] N. S. Aybat, A. Fallah, M. Gurbuzbalaban, and A. Ozdaglar, “A
universally optimal multistage accelerated stochastic gradient method,”
in Advances in Neural Information Processing Systems, 2019, pp.
8525–8536.

[21] S. Michalowsky, C. Scherer, and C. Ebenbauer, “Robust and structure
exploiting optimisation algorithms: an integral quadratic constraint
approach,” International Journal of Control, pp. 1–24, 2020.

[22] A. Sundararajan, B. Van Scoy, and L. Lessard, “Analysis and design
of ﬁrst-order distributed optimization algorithms over time-varying
graphs,” IEEE Transactions on Control of Network Systems, vol. 7,
no. 4, pp. 1597–1608, 2020.

[23] B. Hu and U. Syed, “Characterizing the exact behaviors of temporal
difference learning algorithms using Markov jump linear system
theory,” in Advances in Neural Information Processing Systems, 2019,
pp. 8479–8490.

[24] D. Lee and N. He, “A uniﬁed switching system perspective and
convergence analysis of Q-learning algorithms,” in Advances in Neural
Information Processing Systems, vol. 33, 2020, pp. 15 556–15 567.

[25] V. Borkar, Stochastic approximation: a dynamical systems viewpoint.

Springer, 2009, vol. 48.

[26] V. Borkar and S. Meyn, “The ODE method for convergence of

stochastic approximation and reinforcement learning,” SIAM Journal
on Control and Optimization, vol. 38, no. 2, pp. 447–469, 2000.
[27] A. Farahmand and M. Ghavamzadeh, “PID accelerated value iteration
algorithm,” in International Conference on Machine Learning, 2021,
pp. 3143–3153.

[28] M. Puterman, Markov decision processes: discrete stochastic dynamic

programming.

John Wiley & Sons, 2014.

[29] R. Sutton and A. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[30] D. Bertsekas and J. Tsitsiklis, Neuro-dynamic programming. Athena

Scientiﬁc Belmont, 1996, vol. 5.

[31] J. Bhandari, D. Russo, and R. Singal, “A ﬁnite time analysis of
temporal difference learning with linear function approximation,” in
Conference on learning theory, 2018, pp. 1691–1692.

[32] R. Srikant and L. Ying, “Finite-time error bounds for linear stochastic
approximation and TD learning,” in Conference on Learning Theory,
2019, pp. 2803–2830.

[33] T. Xu, S. Zou, and Y. Liang, “Two time-scale off-policy TD learning:
Non-asymptotic analysis over Markovian samples,” in Advances in
Neural Information Processing Systems, 2019.

[34] J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang, “Finite-
time analysis of decentralized temporal-difference learning with linear
function approximation,” in AIstats, 2020, pp. 4485–4495.

[35] P. Xu and Q. Gu, “A ﬁnite-time analysis of Q-learning with neural
network function approximation,” in International Conference on
Machine Learning, 2020, pp. 10 555–10 565.

[36] S. Zhang, Z. Zhang, and S. T. Maguluri, “Finite sample analysis
of average-reward TD learning and Q-learning,” Advances in Neural
Information Processing Systems, vol. 34, 2021.

[37] T. T. Doan, “Finite-time analysis and restarting scheme for linear two-
time-scale stochastic approximation,” SIAM Journal on Control and
Optimization, vol. 59, no. 4, pp. 2798–2819, 2021.

[38] L. Farina and S. Rinaldi, Positive linear systems: theory and applica-

tions.

John Wiley & Sons, 2011.

[39] A. Rantzer, “Distributed control of positive systems,” in IEEE Con-
ference on Decision and Control and European Control Conference,
2011, pp. 6608–6611.

[40] F. Blanchini, P. Colaneri, M. E. Valcher et al., “Switched positive
linear systems,” Foundations and Trends® in Systems and Control,
vol. 2, no. 2, pp. 101–273, 2015.

[41] X. Liu, “Stability analysis of switched positive systems: a switched
linear copositive Lyapunov function method,” IEEE Trans. on Circuits
and Systems II: Express Briefs, vol. 56, no. 5, pp. 414–418, 2009.

[42] O. Mason and R. Shorten, “On linear copositive Lyapunov functions
and the stability of switched positive linear systems,” IEEE Transac-
tions on Automatic Control, vol. 52, no. 7, pp. 1346–1349, 2007.
[43] Y. Xu, J. Dong, R. Lu, and L. Xie, “Stability of continuous-time pos-
itive switched linear systems: A weak common copositive Lyapunov
functions approach,” Automatica, vol. 97, pp. 278–285, 2018.
[44] E. Fornasini and M. Valcher, “Stability and stabilizability criteria
for discrete-time positive switched systems,” IEEE Transactions on
Automatic control, vol. 57, no. 5, pp. 1208–1221, 2011.

[45] O. C. Pastravanu and M.-H. Matcovschi, “Max-type copositive Lya-
punov functions for switching positive linear systems,” Automatica,
vol. 50, no. 12, pp. 3323–3327, 2014.

[46] O. Costa, M. Fragoso, and R. Marques, Discrete-time Markov jump

linear systems. Springer Science & Business Media, 2006.

[47] O. Costa and M. Fragoso, “Stability results for discrete-time linear
systems with Markovian jumping parameters,” Journal of Mathemat-
ical Analysis and Applications, vol. 179, no. 1, pp. 154–178, 1993.

[48] L. El Ghaoui and M. Rami, “Robust state-feedback stabilization of
jump linear systems via LMIs,” International Journal of Robust and
Nonlinear Control, vol. 6, no. 9-10, pp. 1015–1022, 1996.

[49] Y. Fang and K. Loparo, “Stochastic stability of jump linear systems,”
IEEE Transactions on Automatic Control, vol. 47, no. 7, pp. 1204–
1208, 2002.

[50] Y. Ji, H. Chizeck, X. Feng, and K. Loparo, “Stability and control
of discrete-time jump linear systems,” Control-Theory and Advanced
Technology, vol. 7, no. 2, pp. 247–270, 1991.

[51] P. Seiler and R. Sengupta, “A bounded real lemma for jump systems,”
IEEE Transactions on Automatic Control, vol. 48, no. 9, pp. 1651–
1654, 2003.

[52] R. DeCarlo, M. Branicky, S. Pettersson, and B. Lennartson, “Perspec-
tives and results on the stability and stabilizability of hybrid systems,”
Proceedings of the IEEE, vol. 88, no. 7, pp. 1069–1082, 2000.

