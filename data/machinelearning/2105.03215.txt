Bring Your Own Codegen to Deep Learning Compiler
Cody Hao Yu∗
hyuz@amazon.com
Amazon Web Services, Inc

Zhi Chen∗
chzhi@amazon.com
Amazon Web Services, Inc

Trevor Morris
trevmorr@amazon.com
Amazon Web Services, Inc

1
2
0
2

y
a
M
3

]

G
L
.
s
c
[

1
v
5
1
2
3
0
.
5
0
1
2
:
v
i
X
r
a

Jorn Tuyls
jornt@xilinx.com
Xilinx

Yi-Hsiang Lai†
yl2666@cornell.edu
Cornell University

Jared Roesch
jroesch@octoml.ai
OctoML, Inc

Elliott Delaye
elliott@xilinx.com
Xilinx

Vin Sharma
vinarm@amazon.com
Amazon Web Services, Inc

Yida Wang
wangyida@amazon.com
Amazon Web Services, Inc

Abstract

Deep neural networks (DNNs) have been ubiquitously ap-
plied in many applications, and accelerators are emerged as
an enabler to support the fast and efﬁcient inference tasks
of these applications. However, to achieve high model cov-
erage with high performance, each accelerator vendor has to
develop a full compiler stack to ingest, optimize, and execute
the DNNs. This poses signiﬁcant challenges in the develop-
ment and maintenance of the software stack. In addition, the
vendors have to contiguously update their hardware and/or
software to cope with the rapid evolution of the DNN model
architectures and operators. To address these issues, this pa-
per proposes an open source framework that enables users
to only concentrate on the development of their proprietary
code generation tools by reusing as many as possible compo-
nents in the existing deep learning compilers. Our framework
provides users ﬂexible and easy-to-use interfaces to parti-
tion their models into segments that can be executed on “the
best” processors to take advantage of the powerful compu-
tation capability of accelerators. Our case study shows that
our framework has been deployed in multiple commercial
vendors’ compiler stacks with only a few thousand lines of
code.

1 Introduction

Deep neural networks (DNNs) have recently achieved tremen-
dous breakthroughs in various domains, such as autonomous
driving [10], speech recognition [16], and machine transla-
tion [11], etc. The achievements come at the expense of high
computation and memory requirements to handle the poten-
tially high dimensional arrays (a.k.a. tensors) that typically re-
quire intense computing resources. However, executing these
applications on the cloud using powerful servers poses strict
requirements on network latency and data privacy when the
data is collected and the results are needed on the user end.

∗Equal contribution
†This work is done when Yi-Hsiang Lai was an intern at Amazon.

To overcome the issues, edge computing has been emerged as
a popular paradigm for processing DNNs, i.e., executing the
model directly at the edge, so that the transmission is omitted
and the data is preserved locally. To cope with the inten-
sive computation, one popular solution is to use the special-
purpose hardware accelerator which aims to achieve peak
performance on DNN model inference tasks with much lower
cost and power budget. For instance, various edge hardware
accelerators, such as Nvidia Jetson family [32, 33], Xilinx
Vitis AI [48], ARM Ethos-N [5], and Google’s Edge Tensor
Processing Unit (TPU) [15], have emerged in the past few
years to support AI applications at the edge.

A large body of deep learning models, particularly image
classiﬁcation ones, are designed with a stack of compute-
intensive blocks that consist of convolution and matrix multi-
plication operators. These operators are essentially the com-
bination of a determined order of multiplication and addition.
The hardware accelerators are customized for these opera-
tors, and thus are promising in handling deep learning models
efﬁciently. In order to support the entire model, however,
only handling the compute-intensive operators in high perfor-
mance is insufﬁcient or even impractical. For example, more
advanced deep learning models may contain complex oper-
ators (e.g., data processing) and network architectures (e.g.,
control ﬂow) that are notoriously difﬁcult for deep learning
accelerators to process due to the presence of non-linearity.
To make these deep learning models still functional, each
vendor has to either develop more sophisticated software ap-
proaches or completely redesign the hardware, leading to the
prohibitively high engineering cost.

In addition, accelerator vendors currently have to develop
a full compiler stack to accept the model representation, per-
form a series of optimizations, generate the hardware com-
patible code, and ﬁnally execute the model inference. Some
of these tasks can be shared by many vendors’ software
toolchains, being orthogonal to the vendor-speciﬁc compute-
intensive kernel optimization. Unfortunately, due to missing
a uniﬁed framework, different vendors have to duplicate the
same effort from time to time, signiﬁcantly distracting them

1

 
 
 
 
 
 
from only focusing on their secret sauce of kernel optimiza-
tion and high performance code generation. To summarize,
existing research and development of the compiler stack for
edge accelerators have been hindered for multiple reasons:
Challenge 1: Failing to execute. The evolution of deep
learning models occurs rapidly which may lead to signif-
icant changes in the model architecture over time, e.g.,
AlexNet [23] to ResNet [19] (with residual connection) to
Inception [42] (with representational bottleneck) to SSD [25]
and Mask R-CNN [18] (with control ﬂow). It is non-trivial for
accelerators to keep the pace of the emergence of new models
without dramatic architectural modiﬁcation. Therefore, exist-
ing accelerators may completely fail to support models with
complicated structures.
Challenge 2: Inefﬁcient to execute. It is also non-trivial to
achieve high performance for a model even if its architec-
ture is a simple dataﬂow, because it may contain a portion
of operators that are not executed in a determined order but
with complex control logic. For example, non-maximum sup-
pression (NMS) is widely adopted in object detection models
to select a bounding box out of the overlapping ones. These
operators are not a good ﬁt to accelerators that are designed
for compute-intensive workloads. Hence, they could be at
most insufﬁciently supported, if not impossible.
Challenge 3: Demanding to execute. Even if all operators in
a model can be well supported, each vendor still has to spend
a considerable amount of engineering efforts to develop a full
compiler stack to ingest, optimize, and compile the model
due to the lack of a uniﬁed framework that allows them to
reuse the platform-agnostic optimization techniques such as
standard compiler optimizations (e.g., dead code elimination,
common sub-expression elimination, constant folding, etc)
and deep learning model speciﬁc optimizations (e.g., layout
transformation [27]).

To address these challenges, in this paper, we present a
uniﬁed framework to facilitate hardware vendors to only fo-
cus on the proprietary codegen while leveraging the existing
standard pipeline to handle the DNNs, which substantially
shortens the software stack development cycles. Speciﬁcally,
our framework decouples a deep learning compiler (DLC)
to two parts – the shared part and the accelerator-exclusive
part. The shared part can be treated as a DLC for general
processors such as CPUs and GPUs. Vendors can contribute
the features required to execute new emerged models to the
shared part and make them available on general devices to
beneﬁt each other, so Challenge 1 is resolved.

While the shared part of the DLC is capable of executing
all new emerged models on general devices, the accelerator-
exclusive part can be maintained by each vendor. Our frame-
work offers an easy-to-use interface for users to annotate and
partition the model strategically and ofﬂoad the supported and
efﬁcient segments to the accelerator while leaving the rest
of it on the host. The partitioning framework is a powerful
enabler for users to adapt new models and/or new operators

without redesigning the hardware. As a result, Challenge 2 is
overcome.

In addition, our framework provides the accessibility to
the widely used DLC optimizations to deal with Challenge 3.
The system paves the way for accelerator vendors to only
concentrate on the hardware-speciﬁc optimizations and code
generation. This allows vendors to fully exploit the available
techniques for quick compiler development with state-of-the-
art performance optimization while not revealing their intel-
lectual property. We have integrated the tool chains of several
hardware vendors with the framework to enjoy all optimiza-
tions provided by the framework using only ∼2k LOC on
average (see Section 4 for details). This notably reduces the
compiler development time.

To the best of our knowledge, this is the ﬁrst practical
solution that provides a uniﬁed, customizable compilation
framework for users to integrate their own accelerator code
generation to a DLC, and it has been used by multiple commer-
cial accelerators. In summary, this paper makes the following
contributions.

• We propose a uniﬁed framework to allow different hard-
ware accelerator vendors to obtain as many as possible
hardware-agnostic optimizations for free by integrating
their codegen tools in a plug-and-play manner.

• We provide ﬂexible interfaces for developers to 1) an-
notate and partition a computational graph with vari-
ous strategies, 2) apply hardware-speciﬁc optimizations
on the partitioned graphs to further improve the perfor-
mance.

• We conduct a number of case studies using multiple
popular edge accelerators to demonstrate the different
ways of codegen integration with on average only ∼2K
LOC, largely saving the engineering efforts and time to
the market.

• The proposed framework has been adopted by several
production edge accelerators’ compilation pipelines to
alleviate the development efforts of the full software
stack.

The rest of the paper is organized as follows. Section 2 de-
tails the challenges of deep learning compilers for embracing
deep learning accelerators to motivate this work. Section 3
presents the design and implementation of our framework.
Section 4 provides the case studies of the integration of mul-
tiple popular accelerators using our framework and demon-
strates the performance gain. Related work is discussed in
Section 5. Section 6 concludes the paper.

2 Motivation

In this section, we introduce an example to motivate the sys-
tem. As we mentioned in Section 1, modern deep learning

2

as a critical component of their compilation toolchain.

3 Framework Design and Implementation

This section describes the framework design with the con-
siderations of challenges in Section 2. Figure 2 presents the
compilation ﬂow of our framework. Note that it is built on
top of a deep learning compiler, the system modules in white
are existing components in the compiler. These common deep
learning compiler modules can be developed in one place and
reused by all accelerators. On the other hand, the components
in grey are the modules related to hardware accelerators, and
they can be customized for each accelerator.

We now use the motivational example from the previous
section to walk through the process at a high level. As can be
seen from the ﬁgure, our framework ﬁrst loads a deep learning
model and constructs the model graph in an intermediate
representation (IR). Then, the graph optimization performs
a series of hardware-independent graph-level optimizations,
such as operator simpliﬁcation, constant folding, etc.

Afterwards, the framework partitions the graph to two
parts – “host” and “accelerator” based on the given
rules(Section 3.1). The accelerator part is composed of one
or more subgraphs. Each subgraph will go through the spe-
cialized compilation ﬂow, including accelerator-speciﬁc op-
timization (Section 3.2) and code generation (Section 3.3),
to generate an executable kernel for runtime ofﬂoading. In
this example, four subgraphs (kernels) are sent to the acceler-
ator for efﬁcient processing: the entire backbone CNN; RoI
(Region of Interest) head modules; most operators in the
RPN module except for the NMS; most operators in the post-
processing module except for the loop that contains control
ﬂow.

On the other hand, the host part is almost the same as the
original graph, with the four accelerator subgraphs replaced by
external function calls to invoke the corresponding kernel at
runtime. The remaining operators will go through the normal
compilation ﬂow that performs scheduling and code gener-
ation for the host, which is usually a general-purpose CPU
or GPU. Finally, both compiled host and accelerator modules
are integrated to a single runtime module. The execution of
the runtime module is detailed in Section 3.4.

In the rest of this section, we introduce each system com-

ponent along with the design options and insights in detail.

3.1 Graph Partitioning

Deep learning accelerators are usually designed to acceler-
ate the compute-intensive operations, which are composed
of massive regular computations, such as multiplication and
accumulations (MACs). While these operators account for
the majority portion of a DNN, failing to execute other oper-
ators (efﬁciently) on the accelerator would cause signiﬁcant
performance degradation or even failures in execution.

Figure 1: A general R-CNN network architecture. The Non-
maximum Suppression (NMS) in Region Proposal Network
(RPN) and the loop (control ﬂow) in post process are non-
trivial for accelerators to support.

models may include new operators that are non-trivial for ac-
celerators to execute. One example is R-CNN (region-based
convolutional neural network) [14], a modern object detection
deep learning model, as shown in Figure 1. R-CNN leverages
a Region Proposal Network (RPN) to extract a set of propos-
als (i.e., image regions which likely contain an object) from
images based on the selective search [44]. Since classifying a
limited number of regions instead of all possible regions in an
image is very efﬁcient, the R-CNN family is widely adopted
in recent years.

The RPN usually involves a non-maximum suppression
(NMS) operation. This operation ﬁlters the proposals (e.g.
bounding boxes) that possibly cover the same object to re-
duce redundancy. It ﬁrst sorts all proposals based on their
scores, then checks each of them against the anchor box, and
ﬁnally discards the ones that have Intersection over Union
(IoU) with the anchor box above a certain threshold. Sorting
may not be supported by most deep learning accelerators that
mainly perform tensor computations. In addition, R-CNN
implementation variants (e.g., torchvision [12]) normally in-
clude loop structures, as shown in the post process module of
Figure 1. The model graph with loop structures implies a con-
trol ﬂow with IF nodes, which may not be easily supported by
existing accelerators leveraging dataﬂow-based architectures.
Although the unsupported operators and structures only oc-
cupy an insigniﬁcant portion of the R-CNN, it is cumbersome
for an accelerator-speciﬁc compiler to partition the model
graph, generate code, and deal with runtime graph execution
and data transfer.

Existing solutions [37,38] are either not ﬂexible enough for
hardware vendors to integrate their compilation toolchain or
lack of details. We propose our solution in the next section that
provides a comprehensive and ﬂexible solution for hardware
vendors. As we will present in Section 4, our framework has
been adopted by a number of hardware vendors in production

3

BackboneCNNRPNRoIHeadsPostProcessNMSLoopFigure 2: The Genesis compilation overview. The rectangles in grey are major components in Genesis. The deep learning model
graph represents the general R-CNN. Each dot in the graph denotes a R-CNN module in Figure 1.

In order to guarantee the successful and performant exe-
cution, we design a partitioning module for users to ﬂexibly
cut their model graphs into various regions/subgraphs. Only
the accelerator friendly regions are ofﬂoaded, and the rest
of the graph is left to the host. Since modern deep learning
compilers [6, 7, 34, 38, 45] generally feature multi-level IRs to
better optimize the model with different analysis techniques
and information, we need to make a design decision on which
IR level we should partition and ofﬂoad the subgraphs.

In the framework, we choose to partition and ofﬂoad the
graph at the high-level IR that includes the operator informa-
tion, such as operator name (e.g., Conv2D) and its attributes
(e.g., stride, padding, and dilation) due to the following rea-
sons. First, some hardware vendors handcraft a kernel library
with limited inter-operator optimizations. In this case, the only
information they need is the name and attributes to map each
operator to the corresponding kernel instead of the hardware
information incorporated by the low-level IR, so high-level
IR is ideal for them to customize the code generation. Second,
even other hardware vendors prefer to generate the code for
accelerator-speciﬁc instructions, high-level IR is also more
ﬂexible for them to connect the desired low-level IR. Vendors
can choose to use their own IR or the builtin low-level IR in
existing deep learning compilers.

In the rest of this subsection, we present the implementation

details of graph partitioning using an example in Figure 3.

3.1.1 Pattern-based Grouping

Many deep learning speciﬁc hardware accelerators have pow-
erful instructions to execute a sequence of operators with
peak performance. For example, the sequence of Conv2D, Add
and ReLU can usually be mapped to a single instruction to
minimize overheads in dealing with intermediate results. As
a result, it is a common requirement from many hardware
vendors to employ a pattern matching algorithm to match a
sequence of IR nodes and replace them with the composite
instruction. This, however, leads to tedious and redundant
work for each vendor.

In the system, we implement the pattern matching mech-
anism and provide a user-friendly programming model for
hardware vendors to easily specify patterns. The program-
ming model allows vendors to share the same infrastructure
to match the sequence of operators that ﬁt the speciﬁc in-

struction in their ISA; hence reducing engineering efforts
in software development. For instance, the following code
snippet speciﬁes a pattern of Conv2D-Add-ReLU,

1 def conv2d_pattern () :
2

data , weight , bias = wildcard () , wildcard () , wildcard ()
conv = is_op (" nn . conv2d ")( data , weight )
bias_optional = conv . optional ( \

lambda x: is_op (" nn . bias_add ")(x , bias ))

3

4

5

return is_op (" nn . relu ")( bias_optional )

6
7 pattern_table = [( conv2d_pattern , " conv2d_pattern ")]

where three wildcards indicate the inputs of this pattern can
be in any type, including input tensors of a model and the
tensors computed by the parent operator. is_op is used to
match the operator type such as Conv2D. optional speciﬁes
that the Add is an optional node in this pattern, meaning that
sequences with and without the Add are both matched. Finally,
line 7 assigns a name “conv2d_pattern” to the pattern.

By taking the above pattern_table, we transform the
model graph from Figure 3(a) to Figure 3(b). As can be seen,
the sequence has been replaced with a single graph node,
which is a function including all three nodes. The pattern
name can be referred to in the code generation stage to map
the operator sequence to the corresponding instruction sup-
ported by the accelerator.

3.1.2 Annotation

After grouping nodes based on the patterns, the next step
is to use a programming model to easily specify a list of
supported operators. For example, the following code snippet
registers a function to indicate that all Conv2D nodes with
ﬂoating point data type should be annotated and ofﬂoaded to
myAccel. Since all attributes and input arguments of the node
are accessible, one is able to specify any rules to determine
whether a node can be ofﬂoaded or not.

1 @register_op_attr (" nn . conv2d " , " codegen . myAccel ")
2 def annotate_conv2d ( expr ):
3

attrs , args = expr . attrs , expr . args
if any ([ x. dtype != " float32 " for x in args ]) :

return False

return True

By taking a set of annotation functions, we generate a num-
ber of regions, which can be potentially ofﬂoaded to the target
accelerator, on the graph, as shown in Figure 3(c). Note that in
order to minimize the data transferring and kernel launching
overheads, we greedily merge consecutive supported opera-
tors to one region and pass them together to the code generator.

4

5

6

4

ModelLoadingCodeGenerationSchedulingCodeGenerationAccelerator-specificProcessingHostSub-modulesAcceleratorSub-modulesRuntimeModuleSec.3.1Sec.3.2Sec.3.3Sec.3.4GraphOptimizationGraphPartitioningFigure 3: An illustrative example of graph partitioning.

In this particular example, we create two regions. Region 1
contains 4N + 1 operators; while Region 2 contains K opera-
tors as well as an operator pattern generated in Section 3.1.1.
While we only focus on one target (i.e., myAccel) in this
example, the proposed mechanism is applicable to multiple
targets. For example, if an operator can be ofﬂoaded to more
than one targets, we allow users to specify the priority and
then ofﬂoad the operator to the one with the highest prior-
ity. How to determine the target of each operator to achieve
the optimal performance is an open research problem that is
beyond the scope of this paper.

3.1.3 Cost-based Partitioning

Although greedily merging supported operators to maximize
the region size is ideal, it might not be practical for some
accelerators due to the resource constraints (e.g., on-chip
memory size, number of compute units, etc.). Therefore, in
addition to the annotation, our framework offers a cost-based
partitioning mechanism to split regions based on user-deﬁned
criteria. In the illustrative example, we set the maximum
number of operators to 3N as the criteria to split regions,
and the result in the Region 1 in Figure 3(c) can be split to
Region 1 and Region 3 in Figure 3(d).

In addition to the hardware resource constraints, ofﬂoading
overhead is another concern. Since ofﬂoading a region from
host to the accelerator usually introduces data transfer and
kernel invocation overheads, we should keep the region on
the host if it has no time-consuming computation, such as
MACs. Accordingly, our framework accepts another layer
of user speciﬁed criteria to fallback regions to the host (see
Section 4.1 for an example of this).

Finally, each ofﬂoad-able region is encapsulated into a sep-
arate function and labeled with a target attribute that indicates
the backend on which it will be executed, as shown in Fig-
ure 3(e). Accelerator-speciﬁc processing will be applied on

these functions for further optimization before code genera-
tion, as we will discuss in the next subsection.

3.2 Accelerator-Speciﬁc Processing

After the partitioning, a graph has been split to multiple re-
gions that are handled by different backends. The regions
remain on the host can effectively leverage the standard op-
timizations from the existing deep learning compilers. How-
ever, the regions ofﬂoaded to accelerators may require some
hardware-dependent optimizations (e.g., fusion, substitution,
layout transformation, quantization, etc.) that are not directly
accessible from the deep learning compilers, as these opti-
mizations are either proprietary or require speciﬁc hardware
information. We illustrate two common accelerator-speciﬁc
processing with Figure 4.

Figure 4: Examples of accelerator-speciﬁc processing. The
diamond nodes in grey are inserted by the accelerator backend
for special handling: (a) Quantization. Q is a sequence of
operators for (de)quantizating data. (b) Layout transformation.
L is an operator for transforming data layout.

Quantization is an important technique to reduce the re-
source utilization and energy consumption of DNN execution
via quantizing the ﬂoating-point weights and activations into
low-precision ﬁxed-point numbers [17]. Some accelerators
can even only perform computations for ﬁxed-point data. Al-
though hardware vendors could support pre-quantized models,
which are quantized by deep learning frameworks with a cal-

5

1NNK1NNK1NNK1NNKRegion 2:𝐾op +1op-patternRegion 1:4𝑁+1opsRegion 1:2𝑁+1opsRegion 3:2𝑁opsRegion 2:𝐾op +1op-patternPattern-basedGroupingAnnotationCost-basedPartitioningOffload Function GenerationUnsupported op running on hostConv2DBias AddReLUA sequence of Nsupported opsNOffload functionMatched patternLegends(a)(b)(c)(d)(e)2N2N2N2N1NNKFun 1Fun32NFun 2F1F2F31. Quantize input𝑇!=𝑇+𝑍𝑆3. Dequantize output𝑇=𝑆×𝑇!−𝑍F1F2F31. Transform input layout from NCHW to NHWC3. Transform output layout from NHWC to NCHW2. Execute the graph with NHWC layout(a)2. Execute the graph with quantized data(b)QQQLLLibration dataset, in their compiler tool chains to ensure that
the aforementioned ﬂow for accelerators is still applicable,
it might not be practical to ask end-users to quantize every
model by themselves before compilation and deployment.
With this framework, hardware vendors are able to enable
“partial quantization”, as shown in Figure 4(a). Speciﬁcally,
quantization and dequantization nodes (denoted as Q) are
inserted in the partitioned function to quantize between ﬂoat-
point tensors (T ) and ﬁxed-points tensors (Tq). The constants
S and Z are the scale and zero-point offset for the tensors,
respectively, and they can be determined either at the compile
time with the given calibration data, or at the run time with
the real data.

Layout transformation is another important operation to
make sure the input/output tensor layouts are computational
friendly. Since a hardware accelerator is usually optimized
under a certain layout (e.g., NCHW1 or NHWC data layout in
Conv2D), it may have poor performance or even fail to per-
form computations on other layouts. In this case, as shown in
Figure 4(b), vendors could insert layout transformation nodes
(denoted as L) at the boundary of the partitioned functions to
guarantee the input layouts are expected.

In summary, by maintaining accelerator-speciﬁc processes
inside the partitioned function, our framework is capable of
bringing the accelerators with certain requirements to the
modern deep learning compilers.

3.3 Code Generation

After partitioning, the ﬁnal step of the compilation ﬂow is the
code generation. Figure 5 depicts an overview of our frame-
work’s code generation ﬂow. Given a deep learning model
graph with partitioned functions, the framework aims to gen-
erate a monolithic executable module for model inference.
The module includes 1) the model graph structure, and 2) the
implementation of each graph node. To achieve this goal, the
framework generates a “host sub-module” by traversing the
graph and invoking the corresponding code generation for
each graph node. When traversing to a node that remains on
the host, we leverage the code generation in existing deep
learning compilers, such as TVM [7] and XLA [49]. Most
of them are capable of generating code for general devices
(e.g., CPU and GPU). When traversing to a node (i.e., a parti-
tioned function) that is annotated with the speciﬁc target, we
simply generate an external function call as a hook for kernel
invocation at runtime. Meanwhile, we invoke the accelerator
speciﬁc code generation, which incorporates the hardware
vendor speciﬁc code generation tools and compilation ﬂow,
to generate an “accelerator sub-module” for the partitioned
function in that node. The reason for separating the accelera-
tor kernel implementations to different modules is to preserve
the high ﬂexibility for hardware vendors to integrate their

1NCHW means Conv2D input data is organized in the order of (N)umber-

(Channel)-(H)eight-(W)eight.

compilation ﬂows, as we will illustrate in the rest of this sub-
section. Finally, one host sub-module and multiple accelerator
sub-modules are then encapsulated together into a monolithic
module as a heterogeneous blob.

Figure 5: An illustrative example of the Genesis code genera-
tion ﬂow. The input is the partitioned graph from Figure 3(e).

The generated code of the partitioned function in accelera-
tor sub-modules has to be represented in a certain format, so it
can be consumed by the accelerators’ execution engine at run
time. Our framework offers the following possible options to
represent the generated code format.
Option 1: Standard graph representation. We choose
JSON as the default graph format because it has gathered
notable popularity in the deep learning community. The code
generation for this option simply translates a partitioned func-
tion into a JSON node with the equivalent information, includ-
ing operator name, attributes, and data ﬂow, etc. This format
is human readable and can be easily interpreted by runtime
execution engines. For instance, NVIDIA TensorRT [30] and
Arm Compute Library [4] leverage our JSON code generator
to bridge the gap between our framework and their runtime
engines.
Option 2: Standard C code. Although Option 1 is easy to
implement and deploy, it requires a graph engine to include the
implementations of all supported operators, which may result
in the large binary size. It is therefore not an ideal solution for
some resource constrained accelerators that prefer to directly
run the executable binary compiled from standard C code.

Hence, our framework offers a standard C code generator
that can support accelerator’s proprietary kernel libraries via
emitting the kernel library function calls and linking them
together with the host sub-module. This solution eases code
packaging because the host code is usually C compatible,
which allows the library calls to be part of the host sub-module.
It implies that vendors do not have to customize their own
runtime but can fully leverage the existing runtime system.
Option 3: Custom graph representation. Option 1 and Op-

6

AcceleratorcodegenGeneral codegenF1External function callsF2F3EncapsulationPartitioned graph1NN2NKF1F2F3Hostsub-moduleAccelsub-module1Accelsub-module2Accelsub-module3Figure 6: An illustrative example of runtime execution. (a) The process of loading a runtime module of the compiled model and
executing the ﬁrst operator on the host. (b) The process of executing an external function call on the accelerator. (c) The detail
execution ﬂow in the accelerator module for F1.

tion 2 trade off simplicity, readability, and ﬂexibility. However,
certain accelerators can only load the customized graph repre-
sentation format which is different from the aforementioned
ones. For instance, ARM Ethos-N [5] and Xilinx Vitis AI [48]
have their own specialized stream formats to represent a neu-
ral network. To accommodate such requirements, we also
allow the customization of serialized code format. In this
scenario, hardware vendors could implement a set of uniﬁed
APIs deﬁned by our framework to specify the behavior of 1)
compiling and serializing the generated code to a bit-stream
so that it can be materialized with other sub-modules; 2) de-
serializing the bit-stream from the sub-module at runtime.

Until this point, the compiled and packed module for the
deep learning model is ready. In the next subsection, we
present a lightweight runtime system to load the module and
perform inferences.

3.4 Runtime

Most deep learning compilers employ runtime systems to
perform model inference. As mentioned in the previous sub-
section, the compiler generates one or more runtime modules
for the given deep learning model. The runtime module is
in charge of executing the model graph and dispatching the
operators and subgraphs to the target platform. The graph
execution engine can be a simple dataﬂow graph visitor to
deal with most convolutional neural networks (CNNs), or a
virtual machine [40] to interpret the bytecode and handle dy-
namism and control ﬂow presented in the modern models.
We design a uniﬁed runtime component to manage the host
and accelerator-speciﬁc kernels in a hierarchical manner. This
component can be integrated into any runtime systems em-
ployed by existing deep learning compiler solutions. We use
an example illustrated in Figure 6 to introduce the runtime
execution ﬂow of our framework in detail.

Initiate the metadata module. DNN models feature a num-
ber of weight parameters which are essentially constants dur-
ing inference and should be included in the runtime modules
with the model graph and generated kernels. As illustrated in
Figure 5, for each deep learning model, our framework gener-
ates a single heterogeneous blob containing multiple runtime
sub-modules for subgraphs on different target platforms. It
implies that the constants (i.e., weights) may be required by
different modules, which leads to cumbersome management.
We propose a uniﬁed module, namely metadata module, to
handle the constants required by different modules. As shown
in Figure 6(a), the metadata module is designed as a hierarchi-
cal module that contains all constants, the host sub-module
2, and the accelerator sub-modules. Upon the initialization,
metadata module loads the constants to the runtime data en-
tries, which are a set of pre-allocated memory buffers on the
host or the accelerator ( 1 ).

In addition to the constants, data entries also maintain the
model inputs and outputs as well as the intermediate results.
Since the partitioned functions are already external function
calls, their intermediate results will not be maintained in the
data entries, meaning that merging ofﬂoaded subgraphs can
not only reduce the number of generated accelerator sub-
modules, but also provide ﬂexibility for hardware vendors
to optimize the memory footprint. For example, the vendor
tools with a runtime engine such as NVIDIA TensorRT [30]
are capable of reducing the memory consumption by fusing
certain operators when building the runtime engine.
Execute graph nodes on the host. When users invoke the
inference process given the model input (e.g., an image), the
host sub-module loads the model graph and initiates the exe-
cution engine on the host ( 2 ). The initiated execution engine

2While CPU is normally used as the host, CUDA can also serve this role

for TensorRT [30].

7

Host sub-moduleAccel sub-module 1Accel sub-module 2Accel sub-module 3Metadata module12Data Entries23 to 2N012N+1Accel Sub-module 1Initialize Engine(One Time)Data Entries(a)(b)(c)1NNLaunch KernelF1F2F33Execution EngineHost sub-moduleAccel sub-module 1Accel sub-module 2Accel sub-module 3Metadata module4Data Entries23 to 2N012N+1F1F2F3Execution Enginethen starts executing the graph nodes sequentially. It is worth
mentioning that since all inputs and outputs of each node
are stored in the data entries with their IDs assigned during
compilation, the runtime module can directly access the cor-
responding data entry to read the inputs or write the outputs.
Taking the ﬁrst node in Figure 6(a) as an example ( 3 ), the
host module reads the inputs from data entry 0 and 1, invokes
the kernel to perform the computation, and writes the output
to data entry 2. The data entry 2 then becomes the input of
the next node.
Execute the graph nodes on the accelerator. Continuing
the illustrative example in Figure 6(b), the host execution
engine now attempts to execute the node F1, which is an ex-
ternal function call to the accelerator ( 4 ). The execution
details are depicted in Figure 6(c). As can be seen, the execu-
tion is composed of two steps. The ﬁrst step initiates the F1
speciﬁc execution engine customized by hardware vendors
on the completion of loading the module. Depending on the
implementation of the accelerator execution engine, the cost
of this step may vary from micro seconds to even minutes
(e.g., the execution engine performs just-in-time compilation).
THe system caches the execution engine after its initialization
to eliminate this overhead for the later inferences. Given the
initiated execution engine, the accelerator sub-module then
reads the input from the assigned data entries, executes F1 on
the accelerator, and writes the output to another data entry.
4 Case Study

We implemented our framework on top of the Apache TVM
(version 0.7) [7], an open source deep learning compiler infras-
tructure for its rich frontend parsers and hardware-agnostic
optimization passes. These beneﬁts relieve the accelerator
vendors from handling the framework speciﬁc details, which
enables our work to focus on the design and implementa-
tion of the uniﬁed interfaces for accelerator integration. Our
technique is also applicable to other compilers.

We have worked with several hardware vendors to integrate
their toolchains. For example, the integration of Arm Ethos-N
processor [5] (2,405 LOC) and Xilinx Vitis-AI [48] with Deep
Learning Processor Unit [46] (1,924 LOC) are enabled by our
framework. In addition, many vendor-speciﬁc software design
kits (SDKs) such as NVIDIA TensorRT [30] (4,403 LOC),
Arm Compute Library [4] (2,188 LOC), Texas Instruments
Deep Learning [21] (3,085 LOC), and CoreML [3] (840 LOC)
are also available.

In this section, we dive into several representative cases
and evaluate them in detail. For each case, we ﬁrst present
their execution ﬂow, starting from graph partitioning to the
custom runtime. Then we use a set of typical deep learn-
ing models from image classiﬁcation and objection detection
applications to evaluate the speedup achieved by the accel-
erators as well as the system overheads. The chosen image
classiﬁcation models are ResNet-18 [19], ResNet50_v1b [19],
Inception V3 [43], DarkNet-53 [35], MobileNet V2 [20], and

VGG19 [25], and the selected object detection models are
SSD [26] with MobileNet and ResNet-34 as the backbone net-
work and Faster R-CNN [36] due to their popularity in edge
applications. Since some accelerators such as Ethos-N are
not publicly available yet for evaluation, we study NVIDIA
JetSon Xavier and Xilinx Vitis AI in detail in this section. For
each case, we seek to explore the following questions.

• What is the obtainable speedup of the models compared

to the selected baseline on the studied platforms?

• How many operators can be ofﬂoaded to the target plat-

form?

• What is the percentage of the MACs the ofﬂoaded graphs

account for?

• How much is the ofﬂoading overhead?

4.1 NVIDIA Jetson AGX Xavier GPUs

We start the case study with the NVIDIA Jestson AGX Xavier
GPU, which is the latest edition to the Jetson platform as
of the writing of this paper. Jetson AGX Xavier contains an
integrated 512-core Volta GPU with Tensor Cores, two Deep
Learning Accelerators (DLAs), and a 8-core NVIDIA Carmel
ARMv8.2 CPU. It is mainly designed for robots, drones, and
other autonomous machines [32].

In this study, we have integrated TensorRT in our frame-
work using the proposed ﬂow. The entire integration was
implemented in C++ and Python with ∼4.4K LOC. Since
the TensorRT supports both 32-bit (full-precision) and 16-
bit (half-precision) ﬂoating point kernels, our integration is
able to compile either of them based on users. When running
half-precision TensorRT kernels with full-precision models,
we leverage the partial quantization mechanism introduced in
Section 3.2 to cast full-precision input data to half-precision
on the ﬂy.

To investigate the performance gain of partitioning the
graph and leveraging TesnorRT, we apply the TVM builtin
CUDA code generation for an entire full-precision model
as the baseline. Note that although TVM has an auto-tuning
framework named AutoTVM [8] to tune a given model on the
target device, we do not use AutoTVM to tune the baselines.
The reason is that AutoTVM leverages on-device measure-
ments to guide the search, and every operator needs at least
4,000 trials to achieve decent performance, which usually
takes more than 3 hours on the device with limited CPU com-
putation resources such as Jetson Xavier. As a result, tuning
an SSD model with ResNet-34 backbone on Jetson Xavier
may take a week.

Table 1 shows the end-to-end latency and speedup on
NVIDIA Jetson Xavier using our framework with TensorRT
over the baseline. As shown in the table, we are able to of-
ﬂoad all operators of image classiﬁcation models to TensorRT.
For object detection models such as SSD and Faster R-CNN,

8

Table 1: End-to-end performance on NVIDIA Jetson Xavier with TensorRT with full/half precision kernels. The baselines are
full-precision models compiled by the TVM builtin CUDA code generation.

Model

ResNet-18
ResNet-50 v1b/v2
Inception V3
DarkNet-53
MobileNet V2 1.0
VGG19
SSD 512 MobileNet 1.0
SSD 300 ResNet-34
Faster R-CNN ResNet50 v1b

Ofﬂoad Ratio (%)
Node
100%
100%
100%
100%
100%
100%
36%
41%
63%

MAC
100%
100%
100%
100%
100%
100%
100%
100%
100%

Full-Precision

Half-Precision

Latency (ms)
4.12
9.66
16.13
12.81
3.32
24.26
18.43
34.31
404.51

Speedup Latency (ms)

1.07
1.53
2.00
3.73
1.19
0.92
2.24
48.44
6.29

2.14
4.12
6.10
5.43
2.22
9.78
11.06
15.29
99.72

Speedup
2.06
3.58
5.30
8.81
1.78
2.29
3.73
108.68
25.53

we can only ofﬂoad a fraction of their operators, because the
control ﬂow operators (e.g., If, For) and some other opera-
tors (e.g., arange) are not supported. However, the ofﬂoad
ratio of MACs3 are still 100% for both models, meaning that
we have already ofﬂoaded all time-consuming computations
to TensorRT. In addition, we can see from Table 1 that the
speedups that TensorRT integration achieved are notable on
some models but moderate on a few other models such as
VGG19. This is because TensorRT leverages hand-crafted
kernels which is not scalable to all possible workloads.

On the other hand, the speedup achieved by ofﬂoading
half-precision kernels is more signiﬁcant across virtually all
studied benchmarks. Compared to the latencies with full-
precision TensorRT kernels, the latencies with half-precision
TensorRT kernels achieve on average 2.36× speedup. Intu-
itively, half-precision computation is expected to achieve up
to 2× speedup over full-precision due to less computation, but
since TensorRT enables the use of Tensor Cores only with half-
precision computation, the speedup of half-precision could
be more than 2× with TensorRT.

We note that the above latency numbers do not include
the one-time TensorRT engine initialization overhead. Since
we choose to generate the subgraphs in the JSON format
(Option 1 in Section 3.3) for TensorRT code generation, a
TensorRT engine has to be built for each subgraph when we
initialize the runtime module. According to our experiments,
the initialization time of the execution engine for each kernel
on NVIDIA Jetson Xavier could range from 8 to 62 seconds.
Since the runtime module will cache the built TensorRT en-
gine, the inference latency will not be affected. With the built
engine, the kernel invocation overhead is negligible during
the inference. Moreover, there is no data transferring cost
either during the inference due to the use of CUDA as the
host. It implies that both the host (CUDA) and the accelerator
(TensorRT) access the same GPU memory; hence eliminating
the need of copying tensors from the host to the accelerator
memory. In summary, the TensorRT integration trades the
inference overhead with the long initialization time.

Finally, we study the impact of subgraph numbers in a

3Usually only Conv2D and Dense have MAC computations in DNNs.

Table 2: Subgraph number and latency comparison with and
without cost-based partitioning of TensorRT integration on
NVIDIA Jetson Xavier.

Model

ResNet-18
ResNet-50 v1b/v2
Inception V3
DarkNet-53
MobileNet V2 1.0
VGG19
SSD 512 MobileNet 1.0
SSD 300 ResNet-34
Faster R-CNN ResNet50 v1b

Without

With

Total
Subgraph #
1
1
1
1
1
1
6
6
21

Latency
(ms)
4.12
9.66
16.13
12.81
3.32
24.26
48.13
79.30
407.15

Total
Subgraph #
1
1
1
1
1
1
1
1
2

Latency
(ms)
4.12
9.66
16.13
12.81
3.32
24.26
18.43
34.31
404.51

model. Table 2 shows the number of subgraphs before and
after the cost-based partitioning and the end-to-end inference
latency. In the TensorRT integration, we simply set the cost
function to CalcMAC(graph) > 0 to fallback all subgraphs
without any MACs to the host to avoid overheads. For classic
CNN models (e.g., ResNet, MobileNet), since we are able to
ofﬂoad the entire model to TensorRT, cost-based partitioning
is not necessary. On the other hand, cost-based partitioning
works quite well for the object detection models, such as SSD
and Faster R-CNN. These models contains many ofﬂoad-
able data processing operators (e.g., transpose, maximum,
reshape) that cannot be grouped with other compute inten-
sive operators such as Conv2D due to the unsupported control
ﬂow in between. As a result, many subgraphs in these mod-
els have no compute intensive operators and cannot achieve
decent speedup by TensorRT, so keeping them on the host
device could reduce the overhead.

We further dive into the overheads of six subgraphs before
cost-based partitioning in SSD-512 with MobileNet backbone.
Except for the subgraph that includes an entire backbone net-
work, other subgraphs do not have any MAC computations so
their kernels may not have speedup over the baseline (rang-
ing from 0.48× to 1.15×). Even worse, we still need to pay
for the initialization time of their execution engines, which
ranges from 0.4 to 19 seconds. This illustrates that cost-based
partitioning is practical to control the overheads and achieve
overall better performance.

9

4.2 Xilinx Edge and Cloud FPGAs

The second study is Xilinx Vitis AI [48], Xilinx’s development
stack for deep learning model inference on both Xilinx edge
devices and Alveo accelerator cards. Xilinx Zynq UltraScale+
FPGA and U250 FPGA are chosen to evaluate the integration
of Vitis AI through our framework. The former is built on
TSMC 16nm FinFET+ process technology and compatible
with the Zynq-7000 SoC. It features a 64-bit 8-core ARM
Cortex-A53 processor, a dual-core ARM Cortex-R5 real-time
processor, and an ARM MaliT M-400MP Graphics Processor.
The latter is a data center accelerator card that built on the
Xilinx 16nm UltraScaleT M architecture. It is able to deliver
up to 90× higher performance than CPUs on workloads, such
as machine learning inference and video transcoding, at much
lower cost [47].

Vitis AI is integrated into the system ﬂow with ∼2K LOC
in C++ and Python. The integration has a custom bit-stream
format to represent the partitioned subgraphs, and the custom
runtime module with Vitis-AI is capable of interpreting the
bit-stream format and dispatching operators to the deployed
Xilinx DPUs. In addition, since Xilinx DPUs only focus on
ﬁxed-point computations to fully utilize the power of FPGAs,
this integration incorporates the customized partial quanti-
zation. In particular, the customized Vitis-AI runtime feeds
the data to the Xilinx’s quantizer to calculate the T and Z in
Figure 4. Consequently, although the rest of evaluations are
based on the models with full-precision ﬂoating points, the
part that ofﬂoaded to the FPGAs still performs ﬁxed point
computation.

Table 3: End-to-end performance of full precision models on
Xilinx U250 cloud FPGA. The baseline is compiled by the
TVM builtin LLVM code generation and runs on Intel(R)
Xeon(R) Gold 6252 CPU.

Model

ResNet-18
ResNet-50 v1b/v2
Inception V3
DarkNet-53
VGG19
SSD 300 ResNet-34.
Faster R-CNN ResNet50 v1b

Latency
(ms)
2.66
5.59
7.51
6.78
15.82
43.21
1,080

Speedup

3.91×
4.48×
4.14×
5.07×
5.52×
2.42×
1.08×

Ofﬂoad Ratio (%)
Node
96%
98%
99%
98%
80%
33%
46%

MAC
99%
99%
99%
100%
99%
100%
20%

Table 3 details the speedup of the studied models on the
Xilinx U250 cloud FPGA4. Since the accelerated comput-
ing instances on public clouds, such as Amazon EC2 F1
instances [1], are usually based on general computing in-
stances with a high-end CPU, the baseline latency of this
study is measured on the Intel Xeon Gold 6252 CPU, which
has 96 logical cores (2 sockets×24 cores×2 threads/core),
with TVM LLVM code generation. As can be seen in Table 3,

4MobileNet and SSD 512 MobileNet are skipped in this evaluation be-
cause depthwise Conv2D is not widely used in the cloud and not supported
by U250 DPU.

the ofﬂoad ratio of operators does not achieve 100% due
to unsupported operators (e.g., strided_slice, split, non
maximum suppression). However, there are still signiﬁcant
performance gains by fully ofﬂoading the MAC computations
to FPGAs for almost all models except for Faster R-CNN.
This is because the current Vitis-AI integration only supports
one kernel in a model, so it only ofﬂoads the backbone ResNet-
50 to the FPGA for Faster R-CNN, but the R-CNN takes a
signiﬁcant amount of time.

Model

Table 4: End-to-end performance of full precision models
on Xilinx Zynq UltraScale+ edge FPGA. The baseline is
compiled by the TVM builtin LLVM code generation and
runs on ARM Cortex-A53 CPU.
Latency
(ms)
5.83
14.7
19.47
18.29
4.72
132.78
352.69
467.91
49,016

ResNet-18
ResNet-50 v1b/v2
Inception V3
DarkNet-53
MobileNet V2 1.0
VGG19
SSD 512 MobileNet 1.0
SSD 300 ResNet-34
Faster R-CNN ResNet50

Ofﬂoad Ratio (%)
Node
96%
98%
98%
98%
99%
80%
28%
34%
46%

34.05×
44.51×
77.34×
75.01×
19.11×
10.22×
5.47×
12.7×
1.29×

MAC
100%
99%
100%
100%
100%
99%
100%
100%
20%.

Speedup

In Table 4, we report the results on Xilinx Zynq UltraScale+
edge FPGA. As can be seen, the ofﬂoad ratio is not exactly
the same compared with Table 3 due to the different hardware
resource constraints. However, the achieved speedup over the
baseline on ARM Cortex-A53 CPU is more signiﬁcant. This
illustrates that the accelerators on edge are more beneﬁcial
than cloud, since the edge CPU usually has limited resources.

Figure 7: The breakdown of kernel execution on Xilinx U250
cloud FPGA. The data transfer and kernel invocation time
take on average 6% and 20%, respectively.

Finally, we evaluate the ofﬂoading overhead by studying
the execution time breakdown of the investigated models on
both Xilinx cloud and edge FPGAs in Figure 7 and Figure 8.
Unlike TensorRT that builds an execution engine for each
subgraph, Xilinx Vitis-AI runtime is capable of executing the
graph directly on its DPU. As a result, it does not have the
engine initialization overhead. Other than that, in Figure 7,
the kernel execution time dominates the overall latency in
most cases. The data transfer time takes only 6% on average,

10

ResNet-18ResNet-50 v1b/v2Inception V3DarkNet-53VGG19SSD-300Faster R-CNNModel20406080100Ratio (%)Launch kernelTransfer dataExecute kernelthe given deep learning model. For example, the accelerator
PUMA (Programmable Ultra-efﬁcient Memristor-based Ac-
celerator) [2] comes up with a C++ runtime that parses the
ONNX graph model and runs it with the proposed ISA. The
authors in [13] proposed an in-memory data parallel proces-
sor, and implement their compiler based on TensorFlow. The
authors of Field Programmable Synapse Array (FPSA) [22]
implemented a compiler system that accepts the deep learning
model graph and maps to the FPSA architecture. All com-
pilers of the above architectures could be potentially built
upon our framework to not only reduce the efforts of imple-
mentation as well as maintenance, but make their accelerators
catch up with the state-of-the-art deep learning models and
frameworks.

On the other hand, another direction maintains a DLC
for a deep learning acceleration friendly ISA. For example,
VTA [29] provides a full compiler stack and supports var-
ious deep learning frameworks. NVIDIA also maintains a
compiler for its deep learning accelerators [31]. As long as
the accelerator developers follow the ISA and I/O ports, their
accelerators are naturally supported by the ofﬁcial DLCs.
However, it restricts the ﬂexibility of the developers when
designing accelerators.

Instead of proposing an architecture for application-speciﬁc
integrated circuit (ASIC), some accelerator developers pro-
posed a parameterized neural network architecture that ﬁts
ﬁeld programmable gate arrays (FPGAs), so that the archi-
tecture can be further customized for each deep learning
model [9, 24, 28, 39, 41]. Speciﬁcally, they proposed an ar-
chitecture “template” with a set of conﬁgurable parameters
such as process unit (PE) number and buffer sizes. Based
on the proposed network architecture, a compiler is imple-
mented to generate an FPGA accelerator for the given neural
network. Since these accelerator-speciﬁc compilers also in-
clude common optimizations (e.g., constant folding, expres-
sion simpliﬁcation, layout transformation, and quantization),
the implementation efforts can be signiﬁcantly reduced if they
are built on our framework.
6 Conclusion

This paper proposed a uniﬁed framework to allow accelerator
vendors to easily integrate their codegen and runtime systems
to state-of-the-art deep learning compilers. This enables ven-
dors to only focus on the development of the in-house code
generator and greatly beneﬁt from the development of stan-
dard compiler and deep learning optimization techniques in
existing deep learning compilers. We demonstrated a series of
design and implementation mechanisms in graph partitioning,
accelerator-speciﬁc processing, code generation, and the run-
time systems to realize the framework. Multiple commercial
accelerator vendors have integrated their compiler stack to our
framework. NVIDIA Jetson Xavier and Xilinx Vitis-AI are
studied in detail to illustrate the simplicity of the integration
and their performance gains over the baselines.

Figure 8: The breakdown of kernel execution on Xilinx Zynq
UltraScale+ edge FPGA. Both data transfer and kernel invo-
cation time take on average 3%.

and the kernel invocation time accounts for roughly 20%. The
relatively high kernel invocation time includes data formatting
and partial quantization as mentioned in Section 3.2.

Finally, in Figure 8, both the data transfer and kernel in-
vocation overheads take only on average 3% except for the
kernel in SSD-512 due to the large number of subgraph out-
puts (i.e., 18) that result in large size of data to be transferred.

5 Related Work

In this section, we survey a subset of representative efforts on
deep learning compilers (DLC) for customized accelerators.
General DLC with custom backend support: Other than
our work, there are a few existing general DLCs [37, 38] for
hardware accelerator vendors to integrate. All of them provide
a mechanism to partition a deep learning model and partially
ofﬂoad the model to the accelerator. In particular, ONNX run-
time [37] is a runtime system that interprets and executes a
deep learning model in ONNX (Open Neural Network eX-
change) format. Vendors could provide a list of supported
operators as well as customize an ONNX runtime module for
their accelerators, and the runtime operator dispatcher will
ofﬂoad the supported operators to the custom runtime run-
ning on the accelerator. However, interpreting a deep learning
model at the runtime loses the opportunity of optimizing the
model graph with the consideration of hardware accelerators.
Accordingly, ONNX runtime executes the graph operator-
by-operator, which may introduce unnecessary data transfer
overheads. Another open source DLC, Glow [38], provides a
compiler backend in addition to the runtime, so that vendors
could customize a graph partitioner and code generator that
partitions the graph and generates the accelerator code for
supported subgraphs. Unfortunately, Glow only provides a
CPU backend as the fallback solution, and it lacks of details
in [38] such as how each subgraph is compiled and serialized.
Accelerator-speciﬁc DLC: Numerous specialized architec-
tures for deep learning acceleration have been proposed in
recently years. To achieve the peak performance for the given
deep learning models on these architectures, the developers
usually present a few specialized intrinsic instructions or a
completely new ISA. In both cases, they require a code gener-
ator to generate the executable code with the instruction from

11

ResNet-18ResNet-50 v1b/v2Inception V3DarkNet-53MobileNet V2VGG19SSD-512SSD-300Faster R-CNNModel20406080100Ratio (%)Launch kernelTransfer dataExecute kernelReferences

[1] Amazon. F1 instances. https://aws.amazon.com/

ec2/instance-types/f1/, 2018.

[2] Aayush Ankit, Izzat El Hajj, Sai Rahul Chalamalasetti,
Geoffrey Ndu, Martin Foltin, R Stanley Williams, Paolo
Faraboschi, Wen-mei W Hwu, John Paul Strachan,
Kaushik Roy, and Dejan S Milojicic. PUMA: A pro-
grammable ultra-efﬁcient memristor-based accelerator
for machine learning inference. In Proceedings of the
Twenty-Fourth International Conference on Architec-
tural Support for Programming Languages and Operat-
ing Systems, pages 715–731, 2019.

[3] Apple. Core ml. https://developer.apple.com/

documentation/coreml, 2020.

[4] Arm. Compute library.

https://www.arm.com/

why-arm/technologies/compute-library, 2017.

[5] ARM.

Arm ethos-n
https://developer.arm.com/ip-products/
processors/machine-learning/arm-ethos-n,
2020.

series

processors.

[6] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane,
Emanuele Del Sozzo, Abdurrahman Akkas, Yunming
Zhang, Patricia Suriana, Shoaib Kamil, and Saman Ama-
rasinghe. Tiramisu: A polyhedral compiler for express-
ing fast and portable code. In 2019 IEEE/ACM Interna-
tional Symposium on Code Generation and Optimiza-
tion (CGO), pages 193–205. IEEE, 2019.

[7] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin
Zheng, Eddie Yan, Haichen Shen, Meghan Cowan,
Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin,
and Arvind Krishnamurthy. TVM: An automated end-
to-end optimizing compiler for deep learning. In 13th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 18), pages 578–594, 2018.

[8] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang,
Thierry Moreau, Luis Ceze, Carlos Guestrin, and Arvind
Krishnamurthy. Learning to optimize tensor programs.
In Advances in Neural Information Processing Systems,
pages 3389–3400, 2018.

[9] Jason Cong and Jie Wang. PolySA: polyhedral-based
systolic array auto-compilation. In 2018 IEEE/ACM
International Conference on Computer-Aided Design
(ICCAD), pages 1–8. IEEE, 2018.

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele. The
cityscapes dataset for semantic urban scene understand-
ing. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 3213–3223, 2016.

12

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: pre-training of deep bidirec-
tional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.

[12] Facebook.

torchvision.

https://github.com/

pytorch/vision, 2016.

[13] Daichi Fujiki, Scott Mahlke, and Reetuparna Das. In-
memory data parallel processor. ACM SIGPLAN No-
tices, 53(2):1–14, 2018.

[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten-
dra Malik. Rich feature hierarchies for accurate object
detection and semantic segmentation. In Proceedings
of the IEEE conference on computer vision and pattern
recognition, pages 580–587, 2014.

[15] Google. Edge tpu. https://cloud.google.com/

edgetpu/, 2019.

[16] Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. Speech recognition with deep recurrent neural
networks. In 2013 IEEE international conference on
acoustics, speech and signal processing, pages 6645–
6649, 2013.

[17] Song Han, Huizi Mao, and William J. Dally. Deep
compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding, 2016.

[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross
In 2017 IEEE International
Girshick. Mask r-cnn.
Conference on Computer Vision (ICCV), pages 2980–
2988, 2017.

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016.

[20] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations. CoRR, abs/1704.04861, 2017.

[21] Texas

Instruments.

Deep

learning

(tidl).

https://downloads.ti.com/mctools/esd/docs/
tidl-api/index.html, 2018.

[22] Yu Ji, Youyang Zhang, Xinfeng Xie, Shuangchen Li,
Peiqi Wang, Xing Hu, Youhui Zhang, and Yuan Xie.
FPSA: A full system stack solution for reconﬁgurable
ReRAM-based NN accelerator architecture. In Proceed-
ings of the Twenty-Fourth International Conference on
Architectural Support for Programming Languages and
Operating Systems, pages 733–747, 2019.

[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural
In Proceedings of the 25th International
networks.
Conference on Neural Information Processing Systems
- Volume 1, NIPS’12, page 1097–1105, Red Hook, NY,
USA, 2012.

[34] Jonathan Ragan-Kelley, Connelly Barnes, Andrew
Adams, Sylvain Paris, Frédo Durand, and Saman Ama-
rasinghe. Halide: a language and compiler for optimiz-
ing parallelism, locality, and recomputation in image
processing pipelines. Acm Sigplan Notices, 48(6):519–
530, 2013.

[24] Yi-Hsiang Lai, Yuze Chi, Yuwei Hu, Jie Wang,
Cody Hao Yu, Yuan Zhou, Jason Cong, and Zhiru Zhang.
HeteroCL: A multi-paradigm programming infrastruc-
ture for software-deﬁned reconﬁgurable computing. In
Proceedings of the 2019 ACM/SIGDA International Sym-
posium on Field-Programmable Gate Arrays, pages 242–
251, 2019.

[25] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European
conference on computer vision, pages 21–37, 2016.

[26] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexan-
der C. Berg. SSD: single shot multibox detector. CoRR,
abs/1512.02325, 2015.

[27] Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma,
and Yida Wang. Optimizing CNN model inference on
cpus. In 2019 USENIX Annual Technical Conference
(USENIXATC 19), pages 1025–1040, 2019.

[28] Yufei Ma, Naveen Suda, Yu Cao, Sarma Vrudhula, and
Jae-sun Seo. ALAMO: FPGA acceleration of deep
learning algorithms with a modularized RTL compiler.
Integration, 62:14–23, 2018.

[29] Thierry Moreau, Tianqi Chen, Ziheng Jiang, Luis Ceze,
Carlos Guestrin, and Arvind Krishnamurthy. Vta: an
open hardware-software stack for deep learning. arXiv
preprint arXiv:1807.04188, 2018.

[30] Nvidia. Tensorrt. https://developer.nvidia.com/

tensorrt, 2017.

[31] NVIDIA. Nvidia deep learning accelerator. http://

nvdla.org/primer.html, 2018.

[32] NVIDIA.

Jetson agx xavier

- deploy ai-
https:
powered autonomous machines at scale.
//www.nvidia.com/en-us/autonomous-machines/
embedded-systems/jetson-agx-xavier/, 2020.

[33] NVIDIA.

Jetson nano - bringing the power
https:
of modern ai
//www.nvidia.com/en-us/autonomous-machines/
embeddedsystems/jetson-nano/, 2020.

to millions of devices.

[35] Joseph Redmon and Ali Farhadi. Yolov3: An incremen-
tal improvement. CoRR, abs/1804.02767, 2018.

[36] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian
Sun. Faster R-CNN: towards real-time object detection
with region proposal networks. CoRR, abs/1506.01497,
2015.

[37] Microsoft Research. Onnx runtime. https://www.

onnxruntime.ai/, 2019.

[38] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret
Catron, Summer Deng, Roman Dzhabarov, Nick Gib-
son, James Hegeman, Meghan Lele, Roman Levenstein,
Jack Montgomery, Bert Maher, Satish Nadathur, Jakob
Olesen, Jongsoo Park, Artem Rakhov, Misha Smelyan-
skiy, and Man Wang. Glow: Graph lowering com-
piler techniques for neural networks. arXiv preprint
arXiv:1805.00907, 2018.

[39] Hardik Sharma, Jongse Park, Emmanuel Amaro,
Bradley Thwaites, Praneetha Kotha, Anmol Gupta,
Joon Kyung Kim, Asit Mishra, and Hadi Esmaeilzadeh.
DnnWeaver: From high-level deep network models to
FPGA acceleration. In the Workshop on Cognitive Ar-
chitectures, 2016.

[40] Haichen Shen, Jared Roesch, Zhi Chen, Wei Chen, Yong
Wu, Mu Li, Vin Sharma, Zachary Tatlock, and Yida
Wang. Nimble: Efﬁciently compiling dynamic neural
networks for model inference, 2020.

[41] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Ja-
son Cong. AutoDSE: Enabling Software Programmers
Design Efﬁcient FPGA Accelerators. arXiv preprint
arXiv:2009.14381, 2020.

[42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going
deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 1–9, 2015.

[43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. CoRR,
abs/1512.00567, 2015.

13

[44] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-
ers, and Arnold WM Smeulders. Selective search for
object recognition. International journal of computer
vision, 104(2):154–171, 2013.

[45] Nicolas Vasilache, Oleksandr Zinenko, Theodoros
Theodoridis, Priya Goyal, Zachary DeVito, William S
Moses, Sven Verdoolaege, Andrew Adams, and Albert
Cohen. Tensor comprehensions: Framework-agnostic
high-performance machine learning abstractions. arXiv
preprint arXiv:1802.04730, 2018.

[46] Xilinx.

Dpu
for convolutional neural net-
https://www.xilinx.com/products/

work.
intellectual-property/dpu.html, 2019.

[47] Xilinx.

Alveo u250 data center accelerator
https://www.xilinx.com/products/

card.
boards-and-kits/alveo/u250.html, 2020.

[48] XILINX. Xilinx vitis ai. https://www.xilinx.com/

products/design-tools/vitis/vitis-ai.html,
2020.

[49] XLA Team.

Xla
https://developers.googleblog.com/2017/
03/xla-tensorflow-compiled.html, March 2017.

tensorﬂow, compiled.

-

14

