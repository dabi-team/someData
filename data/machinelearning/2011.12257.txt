0
2
0
2

v
o
N
4
2

]

C
O
.
h
t
a
m

[

1
v
7
5
2
2
1
.
1
1
0
2
:
v
i
X
r
a

Proceedings of Machine Learning Research vol xxx:1–19, 2021

Safely Learning Dynamical Systems from Short Trajectories

Amir Ali Ahmadi
Abraar Chaudhry
ORFE, Princeton University

Vikas Sindhwani
Stephen Tu
Robotics at Google, New York

AAA@PRINCETON.EDU
AZC@PRINCETON.EDU

SINDHWANI@GOOGLE.COM
STEPHENTU@GOOGLE.COM

Abstract
A fundamental challenge in learning to control an unknown dynamical system is to reduce model
uncertainty by making measurements while maintaining safety. In this work, we formulate a math-
ematical deﬁnition of what it means to safely learn a dynamical system by sequentially deciding
where to initialize the next trajectory. In our framework, the state of the system is required to stay
within a given safety region under the (possibly repeated) action of all dynamical systems that are
consistent with the information gathered so far. For our ﬁrst two results, we consider the setting
of safely learning linear dynamics. We present a linear programming-based algorithm that either
safely recovers the true dynamics from trajectories of length one, or certiﬁes that safe learning is
impossible. We also give an efﬁcient semideﬁnite representation of the set of initial conditions
whose resulting trajectories of length two are guaranteed to stay in the safety region. For our ﬁnal
result, we study the problem of safely learning a nonlinear dynamical system. We give a second-
order cone programming based representation of the set of initial conditions that are guaranteed to
remain in the safety region after one application of the system dynamics.

Keywords: learning dynamical systems, safe learning, uncertainty quantiﬁcation, robust optimiza-
tion, conic programming

1. Introduction and Problem Formulation

The core task in model-based reinforcement learning (Yang et al., 2020; Nagabandi et al., 2018;
Singh et al., 2019; Lowrey et al., 2018; Venkatraman et al., 2016; Kaiser et al., 2019) is to estimate—
from a small set of sampled trajectories—an unknown dynamical system prescribing the evolution
of an agent’s state given the current state and control input. During the initial stages of learning,
deploying even a conservative feedback policy on a real robot is fraught with risk, even if the policy
achieves high task performance and safe behavior in simulation. How should the robot be “set loose”
in the real world so that the dynamics may be precisely estimated by observing state transitions, but
with strong guarantees that the robot will remain safe? This interplay between safety and uncertainty
while learning dynamical systems is the central theme of this paper.

We view the agent armed with a ﬁxed feedback policy in closed loop over a short duration as an

unknown discrete-time dynamical system

xt+1 = f(cid:63)(xt).

(1)

© 2021 A.A. Ahmadi, A. Chaudhry, V. Sindhwani & S. Tu.

 
 
 
 
 
 
SAFE LEARNING FROM SHORT TRAJECTORIES

We consider the problem of safe data acquisition for estimating the unknown map f(cid:63) : Rn → Rn
from a collection of length-T trajectories {φf(cid:63),T (xk)}m
k=1, where φf,T (x) := (x, f (x), . . . , f (T )(x)).
In our setting, we are given as input a set S ⊂ Rn, called the safety region, in which the state should
remain throughout the learning process. We say that a state x ∈ Rn is T -step safe under a map
f : Rn → Rn if x belongs to the set

ST (f ) := {x ∈ S | f (i)(x) ∈ S, i = 1, . . . , T }.

In order to safely learn f(cid:63), we require that measurements are made only at points x ∈ Rn for which
x ∈ ST (f(cid:63)). Obviously, if we make no assumptions about f(cid:63), this task is impossible. We assume,
therefore, that the map f(cid:63) belongs to a set of dynamics U0, which we call the initial uncertainty set.
As experience is gathered, the uncertainty over f(cid:63) decreases. Let us denote the uncertainty set after
the agent has observed k trajectories {φf(cid:63),T (xj)}k

j=1 by,

Uk := {f ∈ U0 | φf,T (xj) = φf(cid:63),T (xj) , j = 1, . . . , k}.

For a nonnegative integer k, deﬁne

ST
k :=

(cid:92)

f ∈Uk

ST (f ) ,

the set of points that are T -step safe under all dynamics consistent with the data after observing k
trajectories. Fix a distance metric d(·, ·) over U0 and a scalar ε > 0. Given a safety region S ⊂ Rn
and an initial uncertainty set U0, we say that T -step safe learning is possible (with respect to the
metric d(·, ·) and up to accuracy ε) if for some nonnegative integer m, we can sequentially choose
vectors x1, . . . , xm such that,

1. (Safety) for each k = 1, . . . , m, xk ∈ ST

k−1,

2. (Learning) supf ∈Um d(f, f(cid:63)) ≤ ε.

k ⊆ ST

Note that for any T (cid:48) > T , we have ST (cid:48)
k for all k. Hence, if T -step safe learning is impossible,
then T (cid:48)-step safe learning is also impossible. Therefore, the highest rate of safe information assim-
ilation during the learning process is achieved when T = 1. One of the main contributions of this
paper is to present an efﬁcient algorithm for the exact one-step safe learning problem (i.e., when
ε = 0 and T = 1) in the case where the dynamics in (1) are linear, U0 is a polyhedron in the space
of n × n matrices that deﬁne the dynamics, and S is a polyhedron (Algorithm 1 and Theorem 6).

Suppose furthermore that initializing the unknown system at a state x ∈ S comes at a cost of
c(x). In such a setting, we are also interested in safely learning at minimum measurement cost. To
do this, one naturally wants to solve an optimization problem of the type

c(x) ,

min
x∈ST
k−1

(2)

whose optimal solution gives us the next cheapest T -step safe query point xk. Another contribution
of this paper is to derive exact reformulations of problem (2), when T ∈ {1, 2}, in terms of tractable
conic optimization problems. More speciﬁcally, under natural assumptions on S and U0, when the
unknown dynamics are linear, we show that problem (2) can be formulated as a linear program
when T = 1 (Theorem 1) and as a semideﬁnite program when T = 2 (Theorem 8). Furthermore,

2

SAFE LEARNING FROM SHORT TRAJECTORIES

when T = 1 and the unknown dynamics are nonlinear (but bounded in a certain sense), we show
that (2) can be formulated as a second-order cone program (Theorem 10). Finally, we note that we
are currently preparing a draft to handle the case when T = ∞ using the set invariance tools of
Ahmadi and G¨unl¨uk (2018).

2. Related Work

Most related to our work is Dean et al. (2019), which uses the system-level synthesis frame-
work (Anderson et al., 2019) to derive inner approximations to the inﬁnite-step safety region of
a linear system subject to polytopic uncertainty in the dynamics and bounded disturbances. Lu et al.
(2017) considers a probabilistic version of one-step safety for linear systems and also presents an
algorithm to conservatively compute the T -step safety regions. Unlike these papers that focus on
inner approximations of safety regions, we are able to exactly characterize one-step and two-step
safety regions under our proposed framework. We also note that we do not require any stability
assumptions on the dynamical systems we want to learn.

We also review other works focused on the general problem of safely learning dynamics in
both the control theory and reinforcement learning literature. Berkenkamp et al. (2017) combines
Lyapunov functions and Gaussian process models to show how to safely explore an uncertain system
and expand an inner estimate of the region of attraction of one of its equilibrium points. Akametalu
et al. (2014) uses reachability analysis to compute maximal safe regions for uncertain dynamics,
and proposes Gaussian processes to iteratively reﬁne the uncertainty. Koller et al. (2019) shows
how to propagate ellipsoidal uncertainty multiple steps into the future, and utilizes this uncertainty
propagation in a model predictive control framework for safely learning to control. Wabersich and
Zeilinger (2018) shows how to minimally perturb a controller designed to learn a linear system in
order for the system to stay within a set of constraints that guarantee reachability to a safe target set.
We also note that our work has some conceptual connections to the literature on experiment
design (see e.g., Pukelsheim, 2006; De Castro et al., 2019). However, this literature typically does
not consider dynamical systems or notions of safety.

3. One-Step Safe Learning of Linear Systems

In this section, we focus on characterizing one-step safe learning for linear systems. Here, the state
evolves according to

xt+1 = A(cid:63)xt,
(3)
where A(cid:63) is an unknown n × n matrix. We assume we know that A(cid:63) belongs to a set U0 ⊂ Rn×n
that represents our prior knowledge of A(cid:63). In this section, we take U0 to be a polyhedron; i.e.,

U0 = (cid:8)A ∈ Rn×n | Tr(V T

j A) ≤ vj

j = 1, . . . , s(cid:9)

(4)

for some matrices V1, . . . , Vs ∈ Rn×n and scalars v1, . . . , vs ∈ R. We also work with a polyhedral
representation of the safety region S; i.e.,

S = (cid:8)x ∈ Rn | hT

i x ≤ bi

i = 1, . . . , r(cid:9)

(5)

3

SAFE LEARNING FROM SHORT TRAJECTORIES

for some vectors h1, . . . , hr ∈ Rn and some scalars b1, . . . , br ∈ R. We assume that making a query
at a point x ∈ Rn comes at a cost cT x, where the vector c ∈ Rn is given1. An extension to more
general semideﬁnite representable cost functions is possible using tools of conic optimization.

We start by ﬁnding the minimum cost point that is one-step safe under all valid dynamics, i.e.,
a point x ∈ S such that Ax ∈ S for all A ∈ U0. Once this is done, we gain further information by
observing the action y = A(cid:63)x of system (3) on our point x, which further constrains the uncertainty
set U0. We then repeat this procedure with the updated uncertainty set to ﬁnd the next minimum
cost one-step safe point. More generally, after collecting k measurements, our uncertainty in the
dynamics reduces to the set

Uk = {A ∈ U0 | Axj = yj

j = 1, . . . , k}.

Hence, the problem of ﬁnding the next cheapest one-step safe query point becomes:

min
x∈Rn

cT x

s.t. x ∈ S

Ax ∈ S ∀A ∈ Uk.

(6)

(7)

In Section 3.1, we show that problem (7) can be efﬁciently solved. We then use (7) as a subrou-

tine in a one-step safe learning algorithm which we present in Section 3.2.

3.1. Reformulation via Duality

In this subsection, we reformulate (7) as a linear program. To do this we introduce auxiliary vari-
ables µ(i)
k ∈ Rn for i = 1, . . . , r, j = 1, . . . , s, and k = 1, . . . , m.

j ∈ R and η(i)

Theorem 1 The feasible set of problem (7) is the projection onto x-space of the feasible set of the
following linear program:

cT x

min
x,µ,η
s.t. hT
i x ≤ bi
m
(cid:88)

i = 1, . . . , r

k η(i)
yT

k +

s
(cid:88)

j=1

µ(i)
j vj ≤ bi

i = 1, . . . , r

(8)

k=1

xhT

i =

m
(cid:88)

k=1

xkη(i)T

k +

s
(cid:88)

j=1

µ(i)
j V T
j

i = 1, . . . , r

µ(i) ≥ 0 i = 1, . . . , r.

In particular, the optimal values of (7) and (8) are the same and the optimal solutions of (7) are the
optimal solutions of (8) projected onto x-space.

1. In practice, measurement costs are typically nonnegative. If S is compact for example, one can always add a constant

term to cT x to ensure this requirement without changing any of our optimization problems.

4

SAFE LEARNING FROM SHORT TRAJECTORIES

Proof Using the deﬁnitions of S and U0, let us ﬁrst rewrite (7) as bilevel program:

cT x

min
x
s.t. hT
i x ≤ bi

maxA
s.t.



i = 1, . . . , r

hT
i Ax
j A) ≤ vj

Tr(V T

j = 1, . . . , s

Axk = yk

k = 1, . . . , m



 ≤ bi

i = 1, . . . , r.

(9)

We proceed by taking the dual of the r inner programs, treating the x variable as ﬁxed. By introduc-
ing dual variables µ(i)
for i = 1, . . . , r, j = 1, . . . , s, and k = 1, . . . , m, and by invoking
strong duality of linear programming, we have


j and η(i)

k



minµ(i),η(i)
s.t.




k=1 yT

(cid:80)m
i = (cid:80)m

k η(i)
k=1 xkη(i)T

k + (cid:80)s

k + (cid:80)s

j=1 µ(i)

j vj
j=1 µ(i)

j V T
j

xhT










maxA
s.t.

hT
i Ax

Tr(V T

j A) ≤ vj j = 1, . . . , s

 =

Axk = yk k = 1, . . . , m

µ(i) ≥ 0

(10)
for i = 1, . . . , r. Thus by replacing the inner problem of (9) with the right-hand side of (10), the
min-max problem (9) becomes a min-min problem. This min-min problem can be combined into a
single minimization problem and be written as problem (8). Indeed, if x is feasible to (9), for that
ﬁxed x and for each i, there exist values of µ(i) and η(i) that attain the optimal value for (10) and
therefore the triple (x, µ, η) will be feasible to (8). Conversely, if some (x, µ, η) is feasible to (8),
it follows that x is feasible to (9). This is because for any ﬁxed x and for each i, the optimal value
of the left-hand side of (10) is bounded from above by the objective value of the right-hand side
evaluated at any feasible µ(i) and η(i).

We remark that (8) can be modiﬁed so that one-step safety is achieved in the presence of distur-
bances. We can ensure, e.g., using linear programming, that Ax + w ∈ S for all A ∈ Um and all w
such that (cid:107)w(cid:107) ≤ W , where (cid:107) · (cid:107) is any norm whose unit ball is a polytope and W is a given scalar.

3.2. An Algorithm for One-Step Safe Learning

We start by giving a mathematical deﬁnition of (exact) safe learning specialized to the case of one-
step safety and linear dynamics. Recall the deﬁnition of the set Uk in (6).

Deﬁnition 2 (One-Step Safe Learning) We say that one-step safe learning is possible if for some
nonnegative integer m, we can sequentially choose vectors xk ∈ S, for k = 1, . . . , m, and observe
measurements yk = A(cid:63)xk such that:

1. (Safety) for k = 1, . . . , m, we have Axk ∈ S ∀A ∈ Uk−1,

2. (Learning) the set of matrices Um is a singleton.

We now present our algorithm for checking the possibility of one-step safe learning (Algo-

rithm 1). The proof of correctness of Algorithm 1 is given in Theorem 6.

Remark 3 As Theorem 6 will demonstrate, the particular choice of the parameter ε ∈ (0, 1] in
the input to Algorithm 1 does not affect the detection of one-step safe learning by this algorithm.
However, a smaller ε leads to a lower cost of learning. Therefore, in practice, ε should be chosen
positive and as small as possible without causing the matrix X in line 25 to be ill conditioned.

5

SAFE LEARNING FROM SHORT TRAJECTORIES

Algorithm 1: One-Step Safe Learning Algorithm

: polyhedra S ⊂ Rn and UA ⊂ Rn×n, cost vector c ∈ Rn, and a constant ε ∈ (0, 1].

Input
Output: A matrix A(cid:63) ∈ Rn×n or a declaration that one-step safe learning is impossible.

1 for k = 0, . . . , n − 1 do
2

Dk ← {(xj, yj) | j = 1, . . . , k}
Uk ← {A ∈ U0 | Axj = yj,
if Uk is a singleton (cf. Lemma 4) then

j = 1, . . . , k}

return the single element in Uk as A(cid:63)

end
Let x(cid:63)
if x(cid:63)

k be the projection onto x-space of an optimal solution to problem (8) with data Dk

k is linearly independent from {x1, . . . , xk} then
xk+1 ← x(cid:63)
k

else

k be the projection onto x-space of the feasible region of problem (8) with data Dk
k) (cf. Theorem 5)

k of span(S1

Let S1
Compute a basis Bk ⊂ S1
for zj ∈ Bk do

if zj is linearly independent from {x1, . . . , xk} then
k + εzj

xk+1 ← (1 − ε)x(cid:63)
break

end

end
if no zj ∈ Bk is linearly independent from {x1, . . . , xk} then

return one-step safe learning is impossible

end

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

end
Observe yk+1 ← A(cid:63)xk+1

23
24 end
25 Deﬁne matrix X = [x1, . . . , xn]
26 Deﬁne matrix Y = [y1, . . . , yn]
27 return A(cid:63) = Y X −1

Algorithm 1 invokes two subroutines which we present next in Lemma 4 and Theorem 5.

Lemma 4 Let A ∈ Rm×n, B ∈ Rm×p, c ∈ Rm, and deﬁne the polyhedron

P := {x ∈ Rn | ∃y ∈ Rp

s.t. Ax + By ≤ c}.

The problem of checking if P is a singleton can be reduced to solving 2n linear programs.

Proof For each i = 1, . . . , n, maximize and minimize the i-th coordinate of x over P .
It is
straightforward to check that P is a singleton if and only if the optimal values of these two linear
programs coincide for every i = 1, . . . , n.

For stating our next theorem, we use the following notation: given a set P ⊆ Rn, let span(P ) denote
the set of all linear combinations of points in P .

6

SAFE LEARNING FROM SHORT TRAJECTORIES

Theorem 5 Let A ∈ Rm×n, B ∈ Rm×p, c ∈ Rm, and deﬁne the polyhedron

P := {x ∈ Rn | ∃y ∈ Rp

s.t. Ax + By ≤ c}.

One can ﬁnd a basis of span(P ) contained within P by solving at most 2n2 linear programs.

Proof We form the desired basis {ei} iteratively and with an inductive argument. Let e1 be any
nonzero vector in P (existence of such a vector can be checked by the argument in the proof of
Lemma 4); if there is no such vector, we return the empty set. Let {e1, . . . , ek} be a linearly
independent set in P . We will either ﬁnd an additional linearly independent vector ek+1 ∈ P , or
show that the dimension of the span of P is k. Let x, x+, and x− be variables in Rn, y+ and y−
be variables in Rp, and λ+ and λ− be variables in R. Consider the following linear programming
feasibility problem:

eT
i x = 0 i = 1, . . . , k
x = x+ − x−
Ax+ + By+ ≤ λ+c
Ax− + By− ≤ λ−c
λ+ ≥ 0
λ− ≥ 0.

(11)

Let F ⊆ Rn be the projection onto x-space of the feasible region of this problem. We claim that
F = {0} if and only if the dimension of span(P ) is k. Moreover, if there is solution to (11) with
x (cid:54)= 0, then there is also a solution (x, x±, y±, λ±) where λ+, λ− (cid:54)= 0. In this case, either x+
λ+ or
x+
λ+ can be taken as ek+1.

Suppose ﬁrst that the dimension span(P ) is at least k +1; then there is a vector ˜x ∈ span(P ) that
is linearly independent from {e1, . . . , ek}. By subtracting the projection of ˜x onto span({e1, . . . , ek}),
we will ﬁnd a nonzero vector x ∈ span(P ) that is orthogonal to the vectors e1, . . . , ek. We claim
this vector x is feasible to (11) for some choice of (x±, y±, λ±). Indeed, since x ∈ span(P ), then

x =

r
(cid:88)

j=1

λjxj,

for some vectors x1, . . . , xr ∈ P and some nonzero scalars λ1, . . . , λr. For each j, as xj ∈ P , there
exists a vector yj such that Axj + Byj ≤ c. Let J denote the set of indices j such that λj > 0. It is
easy to check that the assignment

(x+, y+, λ+) = (

(cid:88)

λjxj,

j∈J
(x−, y−, λ−) = (−

(cid:88)

j /∈J

(cid:88)

j∈J

λjyj,

(cid:88)

j∈J

λj) ,

λjxj, −

λjyj, −

(cid:88)

j /∈J

λj)

(cid:88)

j /∈J

(12)

satisﬁes system (11). Hence, we have shown that if F = {0} then the dimension of span(P ) is k.

To see the converse implication, suppose x (cid:54)= 0, and that the tuple (x, x±, y±, λ±) is feasible to

system (11). Without loss of generality we assume λ± ≥ 1; if not, we replace the tuple with

(x, x± + ˆx, y± + ˆy, λ± + 1),

(13)

7

SAFE LEARNING FROM SHORT TRAJECTORIES

where ˆx and ˆy are any vectors satisfying Aˆx + B ˆy ≤ c. Then, since A x+
x+
λ+ ∈ P . By the same argument, x−
λ+ and x−
at least one of the vectors x+
ek+1, also proving that the dimension of span(P ) is at least k + 1.

λ+ ≤ c, the vector
λ− ∈ P . It follows from the orthogonality constraint of (11) that
λ− is linearly independent from {e1, . . . , ek} and can be taken as

λ+ + B y+

Note that the condition F = {0} can be checked by solving 2n linear programs (cf. the proof
of Lemma 4); if F (cid:54)= {0}, then at least one of these 2n linear programs will return a tuple
(x, x±, y±, λ±) where x (cid:54)= 0. We then transform this tuple via (13) to ensure that both λ+, λ− (cid:54)= 0
(we can take ˆx = e1 and ˆy to be any vector such that Ae1 + B ˆy ≤ c). Since we cannot have more
than n linearly independent vectors in span(P ), this procedure is repeated at most n times.

Our next theorem is the main result of the section.

Theorem 6 Given a safety region S ⊂ Rn and an uncertainty set U0 ⊂ Rn×n, one-step safe
learning is possible if and only if Algorithm 1 (with an arbitrary choice of c ∈ Rn and ε ∈ (0, 1])
returns a matrix.

Proof [“If”] By construction, the sequence of measurements chosen by Algorithm 1 satisﬁes the
ﬁrst condition of Deﬁnition 2, since the vectors x(cid:63)
k and any vector
in S1
k will remain in the safety region under the action of all matrices in Uk; i.e. all matrices in UA
that are consistent with the measurements made so far. If Algorithm 1 terminates early at line 5 for
some iteration k, then clearly the uncertainty set Uk is a singleton. On the other hand, if we reach
line 27, then we must have made n linearly independent measurements {x1, . . . , xn}. From this, it
is clear that the set {A ∈ U0 | Axj = yj, j = 1, . . . , n} = {A(cid:63)}.

k and zj are both contained in S1

[“Only if”] Suppose Algorithm 1 chooses points {x1, . . . , xm} where m < n and terminates
at line 20. Then it is clear from the algorithm that {x1, . . . , xm} must form a basis of span(S1
m)
and that Um is not a singleton. Take ˜m to be any nonnegative integer and {˜x1, . . . , ˜x ˜m} to be any
sequence that satisﬁes the ﬁrst condition of Deﬁnition 2. For k = 1, . . . , ˜m, let

˜Uk = {A ∈ U0 | A˜xj = A(cid:63) ˜xj, j = 1, . . . , k} ,
k = {x ∈ S | Ax ∈ S, ∀A ∈ ˜Uk} .
˜S1

0 ⊆ S1

0 and S1

m. Now we assume ˜x1, . . . , ˜xk ∈ S1

m for k = 1, . . . , ˜m. We show this by induction. It is clear that ˜x1 ∈ S1
First we claim that ˜xk ∈ S1
m
m and show that ˜xk+1 ∈ S1
since ˜x1 ∈ S1
m.
Since {x1, . . . , xm} forms a basis of span(S1
m), it follows that for any matrix A, Axj = A(cid:63)xj for
j = 1, . . . , m implies Ax = A(cid:63)x for all x ∈ S1
m. In particular, for any matrix A, Axj = A(cid:63)xj for
j = 1, . . . , m implies A˜xj = A(cid:63) ˜xj for all j = 1, . . . , k. It follows that Um ⊆ ˜Uk and therefore,
˜S1
k, and thus, ˜xk+1 ∈ S1
k ⊆ S1
m.
This completes the inductive argument and shows that ˜xk ∈ S1
m for k = 1, . . . , ˜m. From this,
it follows that Um ⊆ ˜U ˜m. Recall that Um is not a singleton, thus ˜U ˜m is not a singleton either.
Therefore, the sequence {˜x1, . . . , ˜x ˜m} does not satisfy the second condition of Deﬁnition 2.

m. By the ﬁrst condition of Deﬁnition 2, we must have ˜xk+1 ∈ ˜S1

Corollary 7 Given a safety region S ⊂ Rn and an uncertainty set U0 ⊂ Rn×n, if one-step safe
learning is possible, then it is possible with at most n measurements.

8

SAFE LEARNING FROM SHORT TRAJECTORIES

3.3. The Value of Exploiting Information on the Fly

In addition to detecting the possibility of safe learning, Algorithm 1 attempts to minimize the overall
cost of learning (i.e., (cid:80)m
k=1 cT xk) by exploiting information gathered at every step. In order to
demonstrate the value of using information online, we construct an ofﬂine algorithm which chooses
n measurement vectors x1, . . . , xn ahead of time based solely on U0 and S, and succeeds under the
assumption that S1

0 contains a basis of Rn.

Algorithm 2: Ofﬂine Safe Learning Algorithm

: polyhedra S ⊂ Rn and U0 ⊂ Rn×n, cost vector c ∈ Rn, and a constant ε ∈ (0, 1].

Input
Output: A matrix A(cid:63) ∈ Rn×n or failure.

0 does not contain a basis of Rn (cf. Theorem 5) then
return failure

1 if S1
2
3 end
4 Compute a basis {z1, . . . , zn} ⊂ S1
5 Let x(cid:63)
6 Set xk = (1 − ε)x(cid:63)
7 Observe yk ← A(cid:63)xk for k = 1, . . . , n
8 Deﬁne matrix X = [x1, . . . , xn]
9 Deﬁne matrix Y = [y1, . . . , yn]
10 return A(cid:63) = Y X −1

0 + εzk for k = 1, . . . , n

0 of Rn

0 be the projection onto x-space of an optimal solution to problem (8) with data D0

As ε tends to zero, the cost of Algorithm 2 approaches ncT x(cid:63)
0 . Therefore, ncT x(cid:63)

0, where x(cid:63)
0 is a minimum cost
measurement vector in S1
0 serves as an upper bound on the cost incurred by
Algorithm 1. We note that ncT x(cid:63)
0 is also the minimum cost achievable by any one-step safe ofﬂine
algorithm that takes n measurements, since all measurement vectors {xk} of such an algorithm
must come from S1
0 .

By assuming that we know A(cid:63), we can also compute a lower bound on the cost of one-step
safe learning of any algorithm that takes n measurements. Let S1(A(cid:63)) = {x ∈ S | A(cid:63)x ∈ S}
be the true one-step safety region of A(cid:63). Let x(cid:63) be an optimal solution to the linear program that
minimizes cT x over S1(A(cid:63)). Then, clearly, if we must pick n points that are all one-step safe, we
cannot achieve cost lower than ncT x(cid:63).

3.4. Numerical Example
We present a numerical example with n = 4. Here, we take U0 = {A ∈ R4×4 | |Aij| ≤ 4 ∀i, j},
S = {x ∈ R4 | (cid:107)x(cid:107)∞ ≤ 1}, and c = (−1, −1, 0, 0)T . We choose the matrix A(cid:63) uniformly at
random among integer matrices in U0:

A(cid:63) =







1

4


2
2
2 −3 −1 −2


0

2

1
0 −2

2

−2 −3

.

In this example, Algorithm 1 takes four steps to safely recover A(cid:63). The projection onto the ﬁrst
two dimensions of the four vectors that Algorithm 1 selects are plotted in Figure 1(a) (note that

9

SAFE LEARNING FROM SHORT TRAJECTORIES

(a) S1

k grows with k.

(b) Uk shrinks with k.

Figure 1: One-step safe learning associated with the numerical example in Section 3.4.

two of the points are very close to each other). Because of the cost vector c, points higher and
further to the right in the plot have lower measurement cost. Also plotted in Figure 1(a) are the
k for k ∈ {0, 1, 2, 3} and of the set S1(A(cid:63)),
projections onto the ﬁrst two dimensions of the sets S1
the true one-step safety region of A(cid:63). In Figure 1(b), we plot Uk (the remaining uncertainty after
making k measurements) for k ∈ {0, 1, 2, 3, 4}; we draw a two-dimensional projection of these sets
of matrices by looking at the trace and the sum of the entries of each matrix in the set. Note that U4
is a single point since we have recovered the true dynamics after the fourth measurement.

The cost of learning (i.e., (cid:80)4

i xi) is −1.0000 for the ofﬂine algorithm (Algorithm 2), and
−1.6385 for Algorithm 1. The lower bound on the cost of learning is −2.2264 (cf. Section 3.3). We
can see that the value of exploiting information on the ﬂy is signiﬁcant.

i=1 cT

4. Two-Step Safe Learning of Linear Systems

In this section, we again focus on learning the linear dynamics in (3). However, unlike the previous
section, we are interested in making queries to the system that are two-step safe. The advantage
of this formulation is that we may have fewer system resets and can potentially learn the dynamics
with lower measurement cost.

More formally, in the two-step safe learning problem, we have as input a polyhedral safety
region S ⊂ Rn given in the form of (5), an objective function representing measurement cost which
for simplicity we again take to be a linear function cT x, and an uncertainty set U0 ⊂ Rn×n to which
we assume A(cid:63) belongs. In this section, we take U0 to be an ellipsoid; this means that there is a
strictly convex quadratic function q : Rn×n → R such that:

U0 = (cid:8)A ∈ Rn×n | q(A) ≤ 0(cid:9) .
An example of such an uncertainty set is U0 = {A ∈ Rn×n | (cid:107)A − A0(cid:107)F ≤ γ}, where A0 is a
nominal matrix, γ is a positive scalar, and (cid:107) · (cid:107)F refers to the Frobenius norm. Having collected k
safe length-two trajectories {(xj, A(cid:63)xj, A2

j=1, our uncertainty around A(cid:63) reduces to:

(cid:63)xj)}k

Uk = {A ∈ U0 | Axj = A(cid:63)xj, A2xj = A2

(cid:63)xj, j = 1, . . . , k}.

(14)

10

-0.4-0.200.20.40.6-0.4-0.3-0.2-0.100.10.20.30.40.5-1001020-60-40-20020406080SAFE LEARNING FROM SHORT TRAJECTORIES

The optimization problem we would like to solve to ﬁnd the next best two-step safe query point is
the following:

cT x

min
x
s.t. x ∈ S

Ax ∈ S ∀A ∈ Uk
A2x ∈ S ∀A ∈ Uk.

(15)

4.1. Reformulation via the S-Lemma

In this subsection, we derive a tractable reformulation of problem (15).

Theorem 8 Problem (15) can be reformulated as a semideﬁnite program.

Our proof makes use to the S-lemma (see e.g., P´olik and Terlaky, 2007) which we recall next.

Lemma 9 (S-lemma) For two quadratics functions qa and qb, if there exists a point ¯x such that
qa(¯x) < 0, then the implication

holds if and only if there exists a scalar λ ≥ 0 such that

∀x, [qa(x) ≤ 0 ⇒ qb(x) ≤ 0]

λqa(x) − qb(x) ≥ 0 ∀x.

Proof [of Theorem 8] Note that the set of equations

Axj = A(cid:63)xj, A2xj = A2

(cid:63)xj

j = 1, . . . , k

in the deﬁnition of Uk in (14) is equivalent to the set of linear equations

Axj = A(cid:63)xj, A(A(cid:63)xj) = A2

(cid:63)xj

j = 1, . . . , k.

(16)

If there is only one matrix in U0 that satisﬁes all of the equality constraints in (16) (a condition that
can be checked via a simple modiﬁcation of Lemma 4), then we have found A(cid:63) and (15) becomes
a linear program. Therefore, let us assume that more than one matrix in U0 satisﬁes the constraints
in (16). In order to apply the S-lemma, we need to remove these equality constraints, a task that we
accomplish via variable elimination. Let ˆn be the dimension of the afﬁne subspace of matrices that
satisfy the constraints in (16) and let ˆA ∈ Rn×n be an arbitrary member of this afﬁne subspace. Let
A1, . . . , Aˆn ∈ Rn×n be a basis of the subspace

{A ∈ Rn×n | Axj = 0, A(A(cid:63)xj) = 0 j = 1, . . . , k}.

Consider an afﬁne function g : Rˆn → Rn×n deﬁned as follows:

g(ˆa) := ˆA +

ˆn
(cid:88)

i=1

ˆaiAi.

The function g has the properties that it is injective and that for each A that satisﬁes the equality
constraints, there must be a vector ˆa such that A = g(ˆa). In other words, the function g is simply

11

SAFE LEARNING FROM SHORT TRAJECTORIES

parametrizing the afﬁne subspace of matrices that satisfy the equality constraints. Now we can
reformulate (15) as:

cT x

min
x
s.t. x ∈ S

g(ˆa)x ∈ S ∀ˆa s.t.
g(ˆa)2x ∈ S ∀ˆa s.t.

q(g(ˆa)) ≤ 0

q(g(ˆa)) ≤ 0.

(17)

Let ˆq := q ◦ g. Since q is a strictly convex quadratic function and g is an injective afﬁne map,
ˆq is also a strictly convex quadratic function. Since we are under the assumption that there are
multiple matrices in U0 that satisfy the equality constraints, there must be a vector ¯a ∈ Rˆn such that
ˆq(¯a) < 0. To see this, take ¯a1 (cid:54)= ¯a2 such that ˆq(¯a1), ˆq(¯a2) ≤ 0. It follows from strict convexity of ˆq
that ˆq( 1

2 (¯a1 + ¯a2)) < 0. Using the deﬁnition of S, problem (17) can be rewritten as:

cT x

min
x
s.t. hT

i x ≤ bi
(cid:20)maxˆa
s.t.
(cid:20)maxˆa
s.t.

(cid:21)

i = 1, . . . , r
hT
i g(ˆa)x
ˆq(ˆa) ≤ 0
hT
i g(ˆa)2x
ˆq(ˆa) ≤ 0

(cid:21)

≤ bi

i = 1, . . . , r

(18)

≤ bi

i = 1, . . . , r.

i g(ˆa)2x − bi. We consider these functions
Let q1,i(ˆa; x) = hT
as quadratic functions of ˆa parametrized by x. Note that the coefﬁcients of q1,i and q2,i depend
afﬁnely on x. Using logical implications, problem (18) can be rewritten as:

i g(ˆa)x − bi and q2,i(ˆa; x) = hT

cT x

min
x
s.t. hT

i = 1, . . . , r

i x ≤ bi
∀ˆa, [ˆq(ˆa) ≤ 0 ⇒ q1,i(ˆa; x) ≤ 0]
∀ˆa, [ˆq(ˆa) ≤ 0 ⇒ q2,i(ˆa; x) ≤ 0]

i = 1, . . . , r

i = 1, . . . , r.

(19)

Now we use the S-lemma to reformulate an implication between quadratic inequalities as a con-
straint on the global nonnegativity of a quadratic function. Note that as we have already argued
for the existence of a vector ¯a such that ˆq(¯a) < 0, the condition of the S-lemma is satisﬁed. After
introducing variables λ1,i and λ2,i for i = 1, . . . , r, we apply the S-lemma 2r times to reformulate
(19) as the following program:

cT x

min
x,λ
s.t. hT

i = 1, . . . , r

i x ≤ bi
λ1,i ˆq(ˆa) − q1,i(ˆa; x) ≥ 0 ∀ˆa i = 1, . . . , r
λ2,i ˆq(ˆa) − q2,i(ˆa; x) ≥ 0 ∀ˆa i = 1, . . . , r
λ1,i ≥ 0,

i = 1, . . . , r.

λ2,i ≥ 0

(20)

It is a standard procedure to convert the constraint that a quadratic function is globally nonnegative
into a semideﬁnite constraint. Note that the coefﬁcients of q1,i and q2,i depend afﬁnely on x; this
results in linear matrix inequalities when (20) is converted into a semideﬁnite program.

12

SAFE LEARNING FROM SHORT TRAJECTORIES

(a) S2

k grows with k.

(b) Uk shrinks with k.

Figure 2: Two-step safe learning associated with the numerical example in Section 4.2.

4.2. Numerical Example

We present a numerical example, again with n = 4. Here we choose a nominal matrix

A0 =







0.75

4.25

2.25
1.75
2.25 −3.25 −1.25 −2.25
0.00
2.00

1.25
1.75 −0.25 −2.00

−2.00 −2.75







and let U0 = {A ∈ R4×4 | (cid:107)A − A0(cid:107)F ≤ 1}. We let S = {x ∈ R4 | |xi| ≤ 1, i = 1, . . . , 4} and
c = (−1, 0, 0, 0)T . We choose the true matrix A(cid:63) to be the same matrix used in Section 3.4.

(cid:63)x1, and then choose x2 ∈ R4, and observe A(cid:63)x2 and A2

In this example, by solving two semideﬁnite programs, we learn the true matrix A(cid:63) by making
two measurements that are each two-step safe. In other words, we choose x1 ∈ R4, observe A(cid:63)x1,
A2
(cid:63)x2. We can verify that we have recovered
A(cid:63) if {x1, A(cid:63)x1, x2, A(cid:63)x2} are all linearly independent, which is the case. The projection onto the
ﬁrst two dimensions of the two measurements x1 and x2 that our semideﬁnite programs choose are
plotted in Figure 2(a). Because of the cost vector c, points further to the right in the plot have lower
measurement cost. Also plotted are the projections onto the ﬁrst two dimensions of the sets:

0 = {x ∈ S | Ax ∈ S, A2x ∈ S ∀A ∈ U0},
S2
1 = {x ∈ S | Ax ∈ S, A2x ∈ S ∀A ∈ U1},
S2

S2(A(cid:63)) = {x ∈ S | A(cid:63)x ∈ S, A2

(cid:63)x ∈ S}.

The ﬁrst two sets are the projections onto x-space of the feasible regions of our two semideﬁnite
programs. The third set is the true two-step safety region of A(cid:63). In Figure 2(b), we plot Uk (the
remaining uncertainty after observing k trajectories of length two) for k ∈ {0, 1, 2}; we draw a two-
dimensional projection of these sets of matrices by looking at the trace and the sum of the entries
of each matrix in the set. Note that U2 is a single point since we have recovered the true dynamics
after observing the second trajectory. The cost of learning (i.e., cT x1 + cT x2) is −0.1508.

13

-0.1-0.0500.050.10.15-0.1-0.0500.050.101234-20246SAFE LEARNING FROM SHORT TRAJECTORIES

We can construct an analogue of the ofﬂine Algorithm 2 by only making measurements from
S2
0 (i.e., x1), and then another vector in S2
0 . This approach would ﬁrst pick the optimal point in S2
0
close to x1, but linearly independent from it. The cost of learning for this ofﬂine approach would
be 2cT x1 = −0.1099. Finally, we can again ﬁnd a lower bound on the cost of learning of any
algorithm that makes two measurements (that are each two-step safe) by assuming we know A(cid:63)
ahead of time and optimizing cT x over S2(A(cid:63)); in this example, the lower bound is −0.2097. Here,
again, we see that by using information on the ﬂy, we can succeed at safe learning at a lower cost
than the ofﬂine approach.

5. One-Step Safe Learning of Nonlinear Systems

We consider the problem of safely learning a dynamical system of the form xt+1 = f(cid:63)(xt), where

f(cid:63)(x) = A(cid:63)x + g(cid:63)(x),
for some matrix A(cid:63) ∈ Rn×n and some possibly nonlinear map g(cid:63) : Rn → Rn. We take our safety
region S ⊂ Rn to be the same as (5). Our initial knowledge about A(cid:63), g(cid:63) is membership in the sets

(21)

U0,A := (cid:8)A ∈ Rn×n | Tr(V T
U0,g := {g : Rn → Rn | (cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d

j A) ≤ vj

j = 1, . . . , s(cid:9) ,
p ∀x ∈ S}.

Here, p ≥ 1 is either +∞ or a rational number, γ is a given positive constant, and d is a given
nonnegative integer. The use of the (cid:107) · (cid:107)∞ on g in the deﬁnition of U0,g simpliﬁes some of the
following analysis, though an extension to other semideﬁnite representable norms is possible. Note
that by taking d = 0 e.g., our model of uncertainty captures any map f which is bounded on S.

Again for simplicity, we assume a linear measurement cost cT x for some vector c ∈ Rn. Having
j=1 with yj = f(cid:63)(xj), the optimization problem we are

collected k safe measurements {(xj, yj)}k
interested in solving to ﬁnd the next cheapest one-step safe measurement is:

cT x

min
x
s.t. x ∈ S

Ax + g(x) ∈ S ∀ (A, g) ∈ {A ∈ U0,A, g ∈ U0,g | Axj + g(xj) = yj

j = 1, . . . , k}.

5.1. Reformulation as a Second-Order Cone Program

Our main result of this section is to derive a tractable reformulation of problem (22).

Theorem 10 Problem (22) can be reformulated as a second-order cone program.

Proof We start by rewriting problem (22) using the deﬁnition of S:

cT x

min
x
s.t. hT
i x ≤ bi

maxA,g
s.t.





i = 1, . . . , r

hT
i (Ax + g(x))
Tr(V T
j A) ≤ vj
(cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d

j = 1, . . . , s
p ∀x ∈ S
k = 1, . . . , m

Axk + g(xk) = yk

14







≤ bi

i = 1, . . . , r.

(22)

(23)

SAFE LEARNING FROM SHORT TRAJECTORIES

Note that in the inner maximization problem in (23), the variable x is ﬁxed. We claim that if

x (cid:54)∈ {x1, . . . , xk}, then


maxA,g
s.t.







hT
i (Ax + g(x))
Tr(V T
j A) ≤ vj
(cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d

Axk + g(xk) = yk
hT
i Ax





j = 1, . . . , s
p ∀x ∈ S
k = 1, . . . , m


Tr(V T

j A) ≤ vj ∀j

Axk + g(xk) = yk ∀k

(cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d

p ∀x ∈ S

+









maxA,g
s.t.







=



maxA,g
s.t.

hT
i g(x)

Tr(V T

j A) ≤ vj ∀j

Axk + g(xk) = yk ∀k

(cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d

p ∀x ∈ S

It is clear that the left-hand side is upper bounded by the right-hand side. To show the reverse in-
equality, let (A1, g1) (resp. (A2, g2)) be feasible to the ﬁrst (resp. second) problem on the right-hand
side (if any of these of these problems is infeasible, then the inequality we are after is immediate).
Now let

ˆg2(x) =

(cid:40)

g2(x)
yk − A1xk

if x (cid:54)∈ {x1, . . . , xk}
if x = xk.

It is straightforward to check that the pair (A1, ˆg2) is feasible to the left-hand side of (24), therefore
proving (24).

We now focus on reformulating each term on the right-hand side of (24), again under the as-
sumption that x (cid:54)∈ {x1, . . . , xm}. Using the constraint on g, the ﬁrst term can be rewritten as
follows:

hT
i Ax

max
A
s.t. Tr(V T

j A) ≤ vj

j = 1, . . . , s

Note that (25) is a linear program as it is equivalent to:

(cid:107)Axk − yk(cid:107)∞ ≤ γ(cid:107)xk(cid:107)d
p

k = 1, . . . , m.

hT
i Ax

max
A
s.t. Tr(V T

j A) ≤ vj

j = 1, . . . , s

(Axk − yk)l ≤ γ(cid:107)xk(cid:107)d
p
− (Axk − yk)l ≤ γ(cid:107)xk(cid:107)d
p

k = 1, . . . , m l = 1, . . . , n

k = 1, . . . , m l = 1, . . . , n.

Here, the notation (Axk − yk)l represents the l-th coordinate of the vector (Axk − yk). Following
the same approach as in Section 3, we proceed by taking the dual of this linear program. For
j = 1, . . . , s, k = 1, . . . , m, and l = 1, . . . , n, let µj, η+
kl ∈ R be dual variables. The dual of
problem (26) reads:

kl, η−

min
µ,η+,η−

(cid:88)

µjvj +

(cid:88)

η+
kl(γ(cid:107)xk(cid:107)d

p + (yk)l) +

(cid:88)

η−
kl(γ(cid:107)xk(cid:107)d

p − (yk)l)

j
s.t. xhT

i =

(cid:88)

kl
µjV T

j +

(cid:88)

η+
klxkeT

l −

kl
η+
klxkeT

l

(cid:88)

kl

(27)

j
η+ ≥ 0,

kl
η− ≥ 0,

µ ≥ 0,

15

(24)







.

(25)

(26)

SAFE LEARNING FROM SHORT TRAJECTORIES

where el is the l-th coordinate vector. Now we turn our attention to the second term on the right-hand
side of (24). After eliminating the irrelevant constraints, the problem can be rewritten as:

hT
i g(x)

max
g
s.t. (cid:107)g(x)(cid:107)∞ ≤ γ(cid:107)x(cid:107)d
p.

(28)

Recall that the dual norm of (cid:107)·(cid:107)∞ is (cid:107)·(cid:107)1. Therefore, the optimal value of this optimization problem
is simply γ(cid:107)hi(cid:107)1 · (cid:107)x(cid:107)d
p.
Now consider the optimization problem:

min
x,µ,η+,η−

cT x

s.t. hT
i x ≤ bi
(cid:88)

µjvj +

i = 1, . . . , r
(cid:88)
η+
kl(γ(cid:107)xk(cid:107)d

p + (yk)l)

j

(cid:88)

+

kl
η−
kl(γ(cid:107)xk(cid:107)d

p − (yk)l) + γ(cid:107)hi(cid:107)1 · (cid:107)x(cid:107)d

p ≤ bi

(29)

i = 1, . . . , r

µ ≥ 0,

kl
η+ ≥ 0,

η− ≥ 0.

If d = 0, or if d = 1 and p ∈ {1, +∞}, then (29) is a linear program. Otherwise, the rationality of p
ensures that (cid:107)x(cid:107)d
p is second-order cone representable (see Ben-Tal and Nemirovski, 2001, Sect. 2.3;
Lobo et al., 1998, Sect. 2.5). This means that (29) is indeed a second-order cone program.

Let F ⊂ Rn denote the projection of the feasible set of (29) onto x-space. We claim that the
feasible set of (22) equals F ∪ {x1, . . . , xk}. Indeed, since the vectors xk are one-step safe mea-
surements, we have that xk ∈ S and yk ∈ S. This implies that xk is feasible to (22). Furthermore,
for x ∈ F \ {x1, . . . , xk}, we have shown that x satisﬁes the constraints of (23) if and only if x
satisﬁes the constraints of (29).

Therefore, optimizing an objective function over the feasible set of (22) is equivalent to opti-

mizing the same objective function over F ∪ {x1, . . . , xk}.

5.2. Numerical Example

We present a numerical example, again with n = 4. Here we take:

S = {x ∈ R4 | |xi| ≤ 1, i = 1, . . . , 4},

U0,A = {A ∈ R4×4 | −4 ≤ Aij ≤ 8, i = 1, . . . , 4, j = 1, . . . , 4},
U0,g = {g : R4 → R4 | (cid:107)g(x)(cid:107)∞ ≤ γ ∀x ∈ S}.

In Figure 3(a), we plot S1
0 (the one-step safety region without any measurements) projected onto
the ﬁrst two dimensions of x for γ ∈ {0, 0.4, 0.8}. As expected, larger values of γ result in smaller
one-step safety regions.

For our next experiment, we choose the matrix A(cid:63) in (21) to be the same matrix used in the

example in Section 3.4. We let γ = 0.1, and

g(cid:63)(x) =

(cid:18)

x2
2 − x3x4,

γ
2

(cid:113)

1 + x4
x4
3,

x3 sin2(x1),

(cid:19)T

sin2(x2)

∈ U0,g.

16

SAFE LEARNING FROM SHORT TRAJECTORIES

(a) Dependence of S1

0 on γ.

(b) S1

k grows with k.

Figure 3: One-step safe learning of a nonlinear system associated with the example in Section 5.2.

Since the true system is not linear, we cannot hope to learn the exactly dynamics in n steps as we
did in the linear case. We instead pick 30 one-step safe points x1, . . . , x30 (by sequentially solving
the second-order cone program from Theorem 10) and observe yk = f(cid:63)(xk) for each k = 1, . . . , 30.
In order to encourage exploration of the state space, we optimize in random directions in every
iteration (instead of optimizing the same cost function throughout the process). In Figure 3(b), we
plot S1
k (the one-step safety region after k measurements) projected onto the ﬁrst two dimensions of
x for k = 0, . . . , 30. We also plot the projection of S1
γ(A(cid:63)), which we deﬁne as the set of one-step
safe points if we knew A(cid:63), but not g(cid:63):

S1
γ(A(cid:63)) := {x ∈ S | A(cid:63)x + g(x) ∈ S ∀g ∈ U0,g}.

Finally, we undertake the task of learning the unknown nonlinear dynamics. We only use informa-
tion from our ﬁrst 8 data points in order to make the ﬁtting task more challenging. We ﬁt a function
of the form

ˆf (x) = ˆAx + ˆg(x),
where ˆA ∈ R4×4 and each entry of ˆg : R4 → R4 is a homogeneous quadratic function of x. Our
regression is done by minimizing the least-squares loss function

L( ˆf ) =

8
(cid:88)

k=1

(cid:107) ˆf (xk) − yk(cid:107)2.

We train two models. The ﬁrst model, ˆfls, minimizes the least-squares loss with no constraints.
The second model, ˆfSOS, minimizes the least-squares loss subject to the constraints that ˆA ∈ U0,A,
(cid:107) ˆAxk − yk(cid:107)∞ ≤ γ for k = 1, . . . , 8, and ˆg ∈ U0,g. The constraint that ˆg ∈ U0,g is imposed via
sum of squares constraints (see, e.g., Parrilo, 2000; Ahmadi and Khadir, 2020 for details). More
speciﬁcally, we require that for j = 1, . . . , 4,

γ ± ˆgj(x) = σj,±

0

(x) +

r
(cid:88)

i=1

σj,±
i

(x)(bi − hT

i x) ∀x ∈ R4.

17

-0.15-0.1-0.0500.050.10.15-0.15-0.1-0.0500.050.10.15-0.500.5-0.500.5SAFE LEARNING FROM SHORT TRAJECTORIES

Here, ˆgj(x) is the j-th entry of the vector ˆg(x), and the functions σj,±
, for i = 0, . . . , r and
j = 1, . . . , 4, are sum of squares quadratic functions of x. These constraints can be imposed by
semideﬁnite programming.

i

We sample test points z1, . . . , z1000 uniformly at random in S in order to estimate the general-

ization error. The root-mean-square error (RMSE) is computed as:

RMSE( ˆf ) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
1000

1000
(cid:88)

j=1

(cid:107) ˆf (zi) − f(cid:63)(zi)(cid:107)2.

The RMSE( ˆfSOS) of the constrained model is 0.0851 and the RMSE( ˆfls) of the unconstrained
model is 0.2567. We see that imposing prior knowledge with sum of squares constraints results in a
signiﬁcantly better ﬁt.

Acknowledgments

AAA and AC were partially supported by the MURI award of the AFOSR, the DARPA Young
Faculty Award, the CAREER Award of the NSF, the Google Faculty Award, the Innovation Award of
the School of Engineering and Applied Sciences at Princeton University, and the Sloan Fellowship.

References

Amir Ali Ahmadi and Oktay G¨unl¨uk. Robust-to-dynamics optimization. arXiv:1805.03682, 2018.

Amir Ali Ahmadi and Bachir El Khadir. Learning dynamical systems with side information. In
Proceedings of Machine Learning Research, volume 120, pages 718–727. PMLR, 10–11 Jun
2020.

Anayo K. Akametalu, Jaime F. Fisac, Jeremy H. Gillula, Shahab Kaynama, Melanie N. Zeilinger,
and Claire J. Tomlin. Reachability-based safe learning with gaussian processes. In 53rd IEEE
Conference on Decision and Control, 2014.

James Anderson, John C. Doyle, Steven H. Low, and Nikolai Matni. System level synthesis. Annual

Reviews in Control, 47:364–393, 2019.

Aharon Ben-Tal and Arkadi Nemirovski. Lectures on Modern Convex Optimization. Society for

Industrial and Applied Mathematics, 2001.

Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based
In Neural Information Processing Systems,

reinforcement learning with stability guarantees.
2017.

Yohann De Castro, Fabrice Gamboa, Didier Henrion, Roxana Hess, and Jean-Bernard Lasserre.
Approximate optimal designs for multivariate polynomial regression. Ann. Statist., 47(1):127–
155, 02 2019.

Sarah Dean, Stephen Tu, Nikolai Matni, and Benjamin Recht. Safely learning to control the con-

strained linear quadratic regulator. In 2019 American Control Conference (ACC), 2019.

18

SAFE LEARNING FROM SHORT TRAJECTORIES

Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for Atari. arXiv:1903.00374, 2019.

Torsten Koller, Felix Berkenkamp, Matteo Turchetta, Joschka Boedecker, and Andreas Krause.
learning.

Learning-based model predictive control for safe exploration and reinforcement
arXiv:1906.12189, 2019.

Miguel Sousa Lobo, Lieven Vandenberghe, Stephen Boyd, and Herv´e Lebret. Applications of
second-order cone programming. Linear Algebra and its Applications, 284(1):193–228, 1998.

Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch.
learning and exploration via model-based control.

learn ofﬂine: Efﬁcient

Plan online,
arXiv:1811.01848, 2018.

Tyler Lu, Martin Zinkevich, Craig Boutilier, Binz Roy, and Dale Schuurmans. Safe exploration for

identifying linear systems via robust optimization. arXiv:1711.11165, 2017.

Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dy-
namics for model-based deep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), 2018.

Pablo Parrilo. Structured semideﬁnite programs and semialgebraic geometry methods in robustness

and optimization. PhD thesis, California Institute of Technology, 2000.

Friedrich Pukelsheim. Optimal Design of Experiments. Society for Industrial and Applied Mathe-

matics, 2006.

Imre P´olik and Tam´as Terlaky. A survey of the S-lemma. SIAM Review, 49(3):371–418, 2007.

Sumeet Singh, Spencer M. Richards, Vikas Sindhwani, Jean-Jacques E. Slotine, and Marco
Learning stabilizable nonlinear dynamics with contraction-based regularization.

Pavone.
arXiv:1907.13122, 2019.

Arun Venkatraman, Roberto Capobianco, Lerrel Pinto, Martial Hebert, Daniele Nardi, and J Andrew
In International Symposium on

Improved learning of dynamics models for control.

Bagnell.
Experimental Robotics, pages 703–713. Springer, 2016.

Kim P. Wabersich and Melanie N. Zeilinger. Linear model predictive safety certiﬁcation for

learning-based control. In 2018 IEEE Conference on Decision and Control (CDC), 2018.

Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani. Data
efﬁcient reinforcement learning for legged robots. In Conference on Robot Learning, pages 1–10.
PMLR, 2020.

19

