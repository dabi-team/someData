1
2
0
2

p
e
S
6

]

B
D
.
s
c
[

4
v
4
1
6
3
0
.
2
0
0
2
:
v
i
X
r
a

RDFFrames: Knowledge Graph Access for Machine Learning
Tools

Aisha Mohamed1∗ Ghadeer Abuoda1§ Abdurrahman Ghanem2†
Zoi Kaoudi2‡ Ashraf Aboulnaga∗
∗ Qatar Computing Research Institute, HBKU
§ College of Science and Engineering, HBKU
†Bluescape
‡Technische Universität Berlin
{ahmohamed,gabuoda,aaboulnaga}@hbku.edu.qa
ghanemabdo@gmail.com,zoi.kaoudi@tu-berlin.de

ABSTRACT
Knowledge graphs represented as RDF datasets are integral to many
machine learning applications. RDF is supported by a rich ecosys-
tem of data management systems and tools, most notably RDF data-
base systems that provide a SPARQL query interface. Surprisingly,
machine learning tools for knowledge graphs do not use SPARQL,
despite the obvious advantages of using a database system. This is
due to the mismatch between SPARQL and machine learning tools
in terms of data model and programming style. Machine learning
tools work on data in tabular format and process it using an impera-
tive programming style, while SPARQL is declarative and has as its
basic operation matching graph patterns to RDF triples. We posit
that a good interface to knowledge graphs from a machine learning
software stack should use an imperative, navigational programming
paradigm based on graph traversal rather than the SPARQL query
paradigm based on graph patterns. In this paper, we present RDF-
Frames, a framework that provides such an interface. RDFFrames
provides an imperative Python API that gets internally translated
to SPARQL, and it is integrated with the PyData machine learning
software stack. RDFFrames enables the user to make a sequence of
Python calls to define the data to be extracted from a knowledge
graph stored in an RDF database system, and it translates these
calls into a compact SPQARL query, executes it on the database
system, and returns the results in a standard tabular format. Thus,
RDFFrames is a useful tool for data preparation that combines the
usability of PyData with the flexibility and performance of RDF
database systems.

1 INTRODUCTION
There has recently been a sharp growth in the number of knowledge
graph datasets that are made available in the RDF (Resource Descrip-
tion Framework)1 data model. Examples include knowledge graphs
that cover a broad set of domains such as DBpedia [25], YAGO [41],
Wikidata [42], and BabelNet [29], as well as specialized graphs for
specific domains like product graphs for e-commerce [13], biomed-
ical information networks [6], and bibliographic datasets [16, 27].
The rich information and semantic structure of knowledge graphs
makes them useful in many machine learning applications [10],

1https://www.w3.org/RDF

1 Joint first authors.
2 Work done while at QCRI..

such as recommender systems [21], virtual assistants, and ques-
tion answering systems [45]. Recently, many machine learning
algorithms have been developed specifically for knowledge graphs,
especially in the sub-field of relational learning, which is dedicated
to learning from the relations between entities in a knowledge
graph [30, 31, 44].

RDF is widely used to publish knowledge graphs as it provides a
powerful abstraction for representing heterogeneous, incomplete,
sparse, and potentially noisy knowledge graphs. RDF is supported
by a rich ecosystem of data management systems and tools that has
evolved over the years. This ecosystem includes standard serializa-
tion formats, parsing and processing libraries, and most notably
RDF database management systems (a.k.a. RDF engines or triple
stores) that support SPARQL,2 the W3C standard query language for
RDF data. Examples of these systems include OpenLink Virtuoso,3
Apache Jena,4 and managed services such as Amazon Neptune.5

However, we make the observation that none of the publicly avail-
able machine learning or relational learning tools for knowledge
graphs that we are aware of uses SPARQL to explore and extract
datasets from knowledge graphs stored in RDF database systems. This,
despite the obvious advantage of using a database system such as
data independence, declarative querying, and efficient and scalable
query processing. For example, we investigated all the prominent
recent open source relational learning implementations, and we
found that they all rely on ad-hoc scripts to process very small
knowledge graphs and prepare the necessary datasets for learn-
ing. This observation applies to the implementations of published
state-of-the-art embedding models, e.g., scikit-kge [32, 33],6 and
also holds for the recent Python libraries that are currently used as
standard implementations for training and benchmarking knowl-
edge graph embeddings, e.g., Ampligraph [8], OpenKE [20], and
PyKEEN [3]. These scripts are limited in performance, which slows
down data preparation and leaves the challenges of applying em-
bedding models on the scale of real knowledge graphs unexplored.
We posit that machine learning tools do not use RDF engines due
to an “impedance mismatch.” Specifically, typical machine learn-
ing software stacks are based on data in tabular format and the

2https://www.w3.org/TR/sparql11-query
3https://virtuoso.openlinksw.com
4https://jena.apache.org
5https://aws.amazon.com/neptune
6https://github.com/mnick/scikit-kge

1

 
 
 
 
 
 
split-apply-combine paradigm [46]. An example tabular format is
the highly popular dataframes, supported by libraries in several
languages such as Python and R (e.g., the pandas7 and scikit-learn
libraries in Python), and by systems such as Apache Spark [47].
Thus, the first step in most machine learning pipelines (including
relational learning) is a data preparation step that explores the
knowledge graph, identifies the required data, extracts this data
from the graph, efficiently processes and cleans the extracted data,
and returns it in a table. Identifying and extracting this refined data
from a knowledge graph requires efficient and flexible graph traver-
sal functionality. SPARQL is a declarative pattern matching query
language designed for distributed data integration with unique iden-
tifiers rather than navigation [26]. Hence, while SPARQL has the
expressive power to process and extract data into tables, machine
learning tools do not use it since it lacks the required flexibility and
ease of use of navigational interfaces.

In this paper, we introduce RDFFrames, a framework that bridges
the gap between machine learning tools and RDF engines. RDF-
Frames is designed to support the data preparation step. It defines
a user API consisting of two type of operators: navigational opera-
tors that explore an RDF graph and extract data from it based on a
graph traversal paradigm, and relational operators for processing
this data into refined clean datasets for machine learning applica-
tions. The sequence of operators called by the user represents a
logical description of the required dataset. RDFFrames translates
this description to a corresponding SPARQL query, executes it on
an RDF engine, and returns the results as a table.

In principle, the RDFFrames operators can be implemented in any
programming language and can return data in any tabular format.
However, concretely, our current implementation of RDFFrames
is a Python library that returns data as dataframes of the popular
pandas library so that further processing can leverage the richness
of the PyData ecosystem. RDFFrames is available as open source8
and via the Python pip installer. It is implemented in 6,525 lines of
code, and was demonstrated in [28].

Motivating Example. We illustrate the end-to-end operation of
RDFFrames through an example. Assume the DBpedia knowledge
graph is stored in an RDF engine, and consider a machine learning
practitioner who wants use DBpedia to study prolific American
actors (defined as those who have starred in 50 or more movies).
Let us say that the practitioner wants to see the movies these actors
starred in and the Academy Awards won by any of them. List-
ing 1 shows Python code using the RDFFrames API that prepares
a dataframe with the data required for this task. It is important
to note that this code is a logical description of the dataframe and
does not cause a query to be generated or data to be retrieved from
the RDF engine. At the end of a sequence of calls such as these, the
user calls a special execute function that causes a SPARQL query
to be generated and executed on the engine, and the results to be
returned in a dataframe.

The first statement of the code creates a two-column RDFFrame
with the URIs (Universal Resource Identifiers) of all movies and all
the actors who starred in them. The second statement navigates
from the actor column in this RDFFrame to get the birth place of

7https://pandas.pydata.org
8https://github.com/qcri/rdfframes

Figure 1: RDFFrames architecture.

each actor and uses a filter to keep only American actors. Next,
the code finds all American actors who have starred in 50 or more
movies (prolific actors). This requires grouping and aggregation, as
well as a filter on the aggregate value. The final step is to navigate
from the actor column in the prolific actors RDFFrame to get the
actor’s Academy Awards (if available). The result dataframe will
contain the prolific actors, movies that they starred in, and their
Academy Awards if available. An expert-written SPARQL query
corresponding to Listing 1 is shown in Listing 2. RDFFrames pro-
vides an alternative to writing such a SPARQL query that is simpler
and closer to the navigational paradigm and is better-integrated
with the machine learning environment. The case studies in Sec-
tion 6.1 describe more complex data preparation tasks and present
the RDFFrames code for these tasks and the corresponding SPARQL
queries.

movies = graph . feature_domain_range ( ' dbp : starring ' ,

' movie ' , ' actor ')

american = movies . expand ( ' actor ',

[( ' dbp : birthPlace ', ' country ') ]) \
. filter ({ ' country ': [ '= dbpr : United_States ' ]})

prolific = american . group_by ([ ' actor ']) \

.count ( ' movie ' , ' movie_count ')\
. filter ({ ' movie_count ': [ ' >=50 ' ]})

result = prolific . expand ( ' actor ', [( ' dbpp : starring ' ,
' movie ' , INCOMING ) , ( ' dbpp : academyAward ', ' award ' ,

OPTIONAL ) ])

Listing 1: RDFFrames code - Prolific American actors who
have Academy Awards.

*

SELECT
FROM < http :// dbpedia . org >
WHERE

{ ? movie

dbpp : starring

? actor

{ SELECT DISTINCT

? actor

( COUNT ( DISTINCT ? movie ) AS ? movie_count )

WHERE

{ ? movie
? actor
FILTER ( ? actor_country = dbpr : United_States )

dbpp : starring
dbpp : birthPlace

? actor .
? actor_country

}

GROUP BY ? actor
HAVING ( COUNT ( DISTINCT ? movie ) >= 50 )

}
OPTIONAL

{ ? actor

dbpp : academyAward

? award }

}

Listing 2: Expert-written SPARQL query corresponding to
RDFFrames code shown in Listing 1.

2

RDFFrames in a Nutshell. The architecture of RDFFrames is
shown in Figure 1. At the top of the figure is the user API, which
consists of a set of operators implemented as Python functions.
We make a design decision in RDFFrames to use a lazy evaluation
strategy. Thus, the Recorder records the operators invoked by the
user without executing them, storing the operators in a FIFO queue.
The special execute operator causes the Generator to consume
the operators in the queue and build a query model representing
the user’s code. The query model is an intermediate representation
for SPARQL queries. The goal of the query model is (i) to separate
the API parsing logic from the query building logic for flexible
manipulation and implementation, and (ii) to facilitate optimization
techniques for building the queries, especially in the case of nested
queries. Next, the Translator translates the query model into a
SPARQL query. This process includes validation to ensure that
the generated query has valid SPARQL syntax and is equivalent
to the user’s API calls. Our choice to use lazy evaluation means
that the entire sequence of operators called by the user is captured
in the query model processed by the Translator. We design the
Translator to take advantage of this fact and generate compact
and efficient SPARQL queries. Specifically, each query model is
translated to one SPARQL query and the Translator minimizes the
number of nested subqueries. After the Translator, the Executor
sends the generated SPARQL query to an RDF engine or SPARQL
endpoint, handles all communication issues, and returns the results
to the user in a dataframe.

Contributions: The novelty of RDFFrames lies in:
• First, the API provided to the user is designed to be intuitive
and flexible, in addition to being expressive. The API consists of
navigational operators and data processing operators based on
familiar relational algebra operations such as filtering, grouping,
and joins (Section 3).

• Second, RDFFrames translates the API calls into efficient SPARQL
queries. A key element in this is the query model which ex-
poses query equivalences in a simple way. In generating the
query model from a sequence of API calls and in generating the
SPARQL query from the query model, RDFFrames has the over-
arching goal of generating efficient queries (Section 4). We prove
the correctness of the translation from API calls to SPARQL.
That is, we prove that the dataframe that RDFFrames returns
is semantically equivalent to the results set of the generated
SPARQL query (Section 5).

• Third, RDFFrames handles all the mechanics of processing the
SPARQL query such as the connection to the RDF engine or
SPARQL endpoint, pagination (i.e., retrieving the results in
chunks) to avoid the endpoint timing out, and converting the
result to a dataframe. We present case studies and performance
comparisons that validate our design decisions and show that
RDFFrames outperforms several alternatives (Section 6).

2 RELATED WORK
Data Preparation for Machine Learning. It has been reported
that 80% of data analysis time and effort is spent on the process of
exploring, cleaning, and preparing the data [9], and these activities
have long been a focus of the database community. For example,
the recent Seattle report on database research [1] acknowledges

the importance of these activities and the need to support data
science ecosystems such as PyData and to “devote more efforts on
the end-to-end data-to-insights pipeline.” This paper attempts to
reduce the data preparation effort by defining a powerful API for
accessing knowledge graphs. To underscore the importance of such
an API, note that [40] makes the observation that most of the code
in a machine learning system is devoted to tasks other than learning
and prediction. These tasks include collecting and verifying data
and preparing it for use in machine learning packages. This requires
a massive amount of “glue code”, and [40] observes that this glue
code can be eliminated by using well-defined common APIs for
data access (such as RDFFrames).

Some related work focuses on the end-to-end machine learning
life cycle (e.g., [2, 5, 48]). Some systems, such as MLdp [2], focus
primarily on managing input data, but they do not have special
support for knowledge graphs. RDFFrames provides such support.

Database Support for PyData. Some recent efforts to provide
database support for the PyData ecosystem focus on the scalability
of dataframe operations, while other efforts focus on replacing SQL
as the traditional data access API with pandas-like APIs. Koalas9
implements the pandas dataframe API on top of Apache Spark
for better scalability. Modin [35] is a scalable dataframe system
based on a novel formalism for pandas dataframes. Ibis10 defines
a variant of the dataframe API (not pandas) and translates it to
SQL so that it can execute on a database system for scalability. Ibis
also supports other backends such as Spark. Koalas and Modin do
not support SQL backends, and Ibis does not have a pandas API.
A recent system that addresses these limitations is Magpie [22],
which translates pandas operations to Ibis for scalable execution
on multiple backends, including SQL database systems. Magpie
chooses the best backend for a given program based on the pro-
gram’s complexity and the data size. Grizzly [19] is a framework
that generates SQL queries from a pandas-like API and ships the
SQL to a standard database system for scalable execution. Grizzly
relies on the database system’s support for external tables in order
to load the data. It also creates user UDFs as native UDFs in the
database system. The AIDA framework [14] allows users to write
relational and linear algebra operators in Python and pushes the
execution of these operators into a relational database system.

All of these recent works are similar in spirit to RDFFrames in
that they replace SQL for data access with a pandas-like API and/or
rely on a database backend for scalability. However, all of the works
focus on relational data and not graph data. RDFFrames is the first
to define a pandas-like API for graph data (specifically RDF), and
to support a graph database system as a scalable backend.

Why RDF? Knowledge graphs are typically represented in the RDF
data model. Another popular data model for graphs is the property
graph data model, which has labels on nodes and edges as well as
(property, value) pairs associated with both. Property graphs have
gained wide adoption in many applications and are supported by
popular database systems such as Neo4j11 and Amazon Neptune.
Multiple query languages exist for property graphs, and efforts are
underway to define a common powerful query language [4].

9https://koalas.readthedocs.io/en/latest
10https://ibis-project.org
11https://neo4j.com

3

A popular query language for property graphs is Gremlin.12 Like
RDFFrames, Gremlin adopts a navigational approach to querying
the graph, and some of the RDFFrames operators are similar to
Gremlin operators. The popularity of Gremlin is evidence that a
navigational approach is attractive to users. However, all publicly
available knowledge graphs including DBpedia [25] and YAGO [38]
are represented in RDF format. Converting RDF graphs to property
graphs is not straightforward mainly because the property graph
model does not provide globally unique identifiers and linking ca-
pability as a basic construct. In RDF knowledge graphs, each entity
and relation is uniquely identified by a URI, and links between
graphs are created by using the URIs from one graph in the other.
RDFFrames offers a navigational API similar to Gremlin to data
scientists working with knowledge graphs in RDF format and facil-
itates the integration of this API with the data analysis tools of the
PyData ecosystem.

Why SPARQL? RDFFrames uses SPARQL as the interface for ac-
cessing knowledge graphs. In the early days of RDF, several other
query languages were proposed (see [18] for a survey), but none
of them has seen broad adoption, and SPARQL has emerged as the
standard.

Some work proposes navigational extensions to SPARQL (e.g., [24,
34]), but these proposals add complex navigational constructs such
as path variables and regular path expressions to the language. In
contrast, the navigation used in RDFFrames is simple and well-
supported by standard SPARQL without extensions. The goal of
RDFFrames is not complex navigation, but rather providing a simple
yet common and rich suite of data access and preparation operators
that can be integrated in a machine learning pipeline.

Python Interfaces. A Python interface for accessing RDF knowl-
edge graphs is provided by Google’s Data Commons project.13
However, the goal of that project is not to provide powerful data
access, but rather to synthesize a single graph from multiple knowl-
edge graphs, and to enable browsing for graph exploration. The
provided Python interface has only one data access primitive: fol-
lowing an edge in the graph in either direction, which is but one of
many capabilities provided by RDFFrames.

The Magellan project [17] provides a set of interoperable Python
tools for entity matching pipelines. It is another example of devel-
oping data management solutions by extending the PyData ecosys-
tem [12], albeit in a very different domain from RDFFrames. The
same factors that made Magellan successful in the world of entity
matching can make RDFFrames successful in the world of knowl-
edge graphs.

There are multiple recent Python libraries that provide access
to knowledge graphs through a SPARQL endpoint over HTTP.
Examples include pysparql,14 sparql-client,15 and AllegroGraph
Python client.16 However, all these libraries solve a very different
(and simpler) problem compared to RDFFrames: they take a SPARQL
query written by the user and handle sending this query to the
endpoint and receiving results. On the other hand, the main focus

12https://tinkerpop.apache.org/gremlin.html
13http://datacommons.org
14https://code.google.com/archive/p/pysparql
15https://pypi.org/project/sparql-client
16https://franz.com/agraph/support/documentation/current/python

4

of RDFFrames is generating the SPARQL query from imperative
API calls. Communicating with the endpoint is also handled by
RDFFrames, but it is not the primary contribution.

Internals of RDFFrames. The internal workings of RDFFrames
involve a logical representation of a query. Query optimizers use
some form of logical query representation, and we adopt a represen-
tation similar to the Query Graph Model [36]. Another RDFFrames
task is to generate SPARQL queries from a logical representation.
This task is also performed by systems for federated SPARQL query
processing (e.g., [39]) when they send a query to a remote site.
However, the focus in these systems is on answering SPARQL triple
patterns at different sites, so the queries that they generate are
simple. RDFFrames requires more complex queries so it cannot use
federated SPARQL techniques.

3 RDFFRAMES API
This section presents an overview of the RDFFrames API. RDF-
Frames provides the user with a set of operators, where each op-
erator is implemented as a function in a programming language.
Currently, this API is implemented in Python, but we describe the
RDFFrames operators in generic terms since they can be imple-
mented in any programming language. The goal of RDFFrames is to
build a table (the dataframe) from a subset of information extracted
from a knowledge graph. We start by describing the data model for
a table constructed by RDFFrames, and then present an overview
of the API operators.

3.1 Data Model
The main tabular data structure in RDFFrames is called an RDF-
Frame. This is the data structure constructed by API calls (RDF-
Frames operators). RDFFrames provides initialization operators
that a user calls to initialize an RDFFrame and other operators that
extend or modify it. Thus, an RDFFrame represents the data de-
scribed by a sequence of one or more RDFFrames operators. Since
RDFFrames operators are not executed on relational tables but are
mapped to SPARQL graph patterns, an RDFFrame is not represented
as an actual table in memory but rather as an abstract description of
a table. A formal definition of a knowledge graph and an RDFFrame
is as follows:

Definition 1 (Knowledge Graph). A knowledge graph 𝐺 :
(𝑉 , 𝐸) is a directed labeled RDF graph where the set of nodes 𝑉 ∈
𝐼 ∪ 𝐿 ∪ 𝐵 is a set of RDF URIs 𝐼 , literals 𝐿, and blank nodes 𝐵 existing
in 𝐺, and the set of labeled edges 𝐸 is a set of ordered pairs of elements
of 𝑉 having labels from 𝐼 . Two nodes connected by a labeled edge
form a triple denoting the relationship between the two nodes. The
knowledge graph is represented in RDFFrames by a graph_uri.

Definition 2 (RDFFrame). Let R be the set of real numbers, 𝑁
be an infinite set of strings, and 𝑉 be the set of RDF URIs and literals.
An RDFFrame 𝐷 is a pair (C, R), where C ⊆ 𝑁 is an ordered set of
column names of size 𝑚 and R is a bag of 𝑚-sized tuples with values
from 𝑉 ∪ R denoting the rows. The size of 𝐷 is equal to the size of R.
Intuitively, an RDFFrame is a subset of information extracted
from one or more knowledge graphs. The rows of an RDFFrame
should contain values that are either (a) URIs or literals in a knowl-
edge graph, or (b) aggregated values on data extracted from a graph.

Due to the bag semantics, an RDFFrame may contain duplicate rows,
which is good in machine learning because it preserves the data
distribution and is compatible with the bag semantics of SPARQL.

3.2 API Operators
RDFFrames provides the user with two types of operators: (a) explo-
ration and navigational operators, which operate on a knowledge
graph, and (b) relational operators, which operate on an RDFFrame
(or two in case of joins). The full list of operators, and also other
RDFFrames functions (e.g., for client-server communication), can
be found with the source code.17

The RDFFrames exploration operators are needed to deal with
one of the challenges of real-world knowledge graphs: knowledge
graphs in RDF are typically multi-topic, heterogeneous, incom-
plete, and sparse, and the data distributions can be highly skewed.
Identifying a relatively small, topic-focused dataset from such a
knowledge graph to extract into an RDFFrame is not a simple task,
since it requires knowing the structure and schema of the dataset.
RDFFrames provides data exploration operators to help with this
task. For example, RDFFrames includes operators to identify the
RDF classes representing entity types in a knowledge graph, and
to compute the data distributions of these classes.

Guided by the initial exploration of the graph, the user can
gradually build an RDFFrame representing the information to be
extracted. The first step is always a call to the 𝑠𝑒𝑒𝑑 operator (de-
scribed below) that initializes the RDFFrame with columns from
the knowledge graph. The rest of the RDFFrame is built through a
sequence of calls to the RDFFrames navigational and relational op-
erators. Each of these operators outputs an RDFFrame. The inputs
to an operator can be a knowledge graph, one or more RDFFrames,
and/or other information such as predicates or column names.

The RDFFrames navigational operators are used to extract in-
formation from a knowledge graph into a tabular form using a
navigational, procedural interface. RDFFrames also provides rela-
tional operators that apply operations on an RDFFrame such as
filtering, grouping, aggregation, filtering based on aggregate values,
sorting, and join. These operators do not access the knowledge
graph, and one could argue that they are not necessary in RDF-
Frames since they are already provided by machine learning tools
that work on dataframes such as pandas. However, we opt to pro-
vide these operators in RDFFrames so that they can be pushed into
the RDF engine, which results in substantial performance gains as
we will see in Section 6.

In the following, we describe the syntax and semantics of the
main operators of both types. Without loss of generality, let 𝐺 =
(𝑉 , 𝐸) be the input knowledge graph and 𝐷 = (C, R) be the input
RDFFrame of size 𝑛. Let 𝐷 ′ = (C′, R ′) be the output RDFFrame. In
addition, let (cid:90), ⟕, ⟖, ⟗, 𝜎, 𝜋, 𝜌, and 𝛾 be the inner join, left outer
join, right outer join, full outer join, selection, projection, renaming,
and grouping-with-aggregation relational operators, respectively,
defined using bag semantics as in typical relational databases [15].
Exploration and Navigational Operators. These operators tra-
verse a knowledge graph to extract information from it to either
construct a new RDFFrame or expand an existing one. They bridge
the gap between the RDF data model and the tabular format by

17https://github.com/qcri/rdfframes

5

allowing the user to extract tabular data through graph navigation.
They take as input either a knowledge graph 𝐺, or a knowledge
graph 𝐺 and an RDFFrame 𝐷, and output an RDFFrame 𝐷 ′.
• 𝐺 .𝑠𝑒𝑒𝑑 (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3) where 𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3 are in 𝑁 ∪ 𝑉 : This
operator is the starting point for constructing any RDFFrame.
Let 𝑡 = (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3) be a SPARQL triple pattern, then this op-
erator creates an initial RDFFrame by converting the evaluation
of the triple pattern 𝑡 on graph 𝐺 to an RDFFrame. The returned
RDFFrame has a column for every variable in the pattern 𝑡. For-
mally, let 𝐷𝑡 be the RDFFrame equivalent to the evaluation of
the triple pattern 𝑡 on graph 𝐺. We formally define this notion of
equivalence in Section 5. The returned RDFFrame is defined as
𝐷 ′ = 𝜋𝑁 ∩{𝑐𝑜𝑙1,𝑐𝑜𝑙2,𝑐𝑜𝑙3 } (𝐷𝑡 ). As an example, the 𝑠𝑒𝑒𝑑 operator
can be used to retrieve all instances of class type 𝑇 in graph 𝐺
by calling 𝐺 .𝑠𝑒𝑒𝑑 (𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒, rdf:type,𝑇 ). For convenience, RDF-
Frames provides implementations for the most common vari-
ants of this operator. For example, the feature_domain_range
operator in Listing 1 initializes the RDFFrame with all pairs of
entities in DBpedia connected by the predicate dbpp:starring,
which are movies and the actors starring in them.

• (𝐺, 𝐷).𝑒𝑥𝑝𝑎𝑛𝑑 (𝑐𝑜𝑙, 𝑝𝑟𝑒𝑑, 𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑑𝑖𝑟, 𝑖𝑠_𝑜𝑝𝑡), where 𝑐𝑜𝑙 ∈ C,
𝑝𝑟𝑒𝑑 ∈ 𝑉 , 𝑛𝑒𝑤_𝑐𝑜𝑙 ∈ 𝑁 , 𝑑𝑖𝑟 ∈ {𝑖𝑛, 𝑜𝑢𝑡 }, and 𝑖𝑠_𝑜𝑝𝑡 ∈
{𝑡𝑟𝑢𝑒, 𝑓 𝑎𝑙𝑠𝑒}: This is the main navigational operator in RDF-
Frames. It expands an RDFFrame by navigating from 𝑐𝑜𝑙 fol-
lowing the edge 𝑝𝑟𝑒𝑑 to 𝑛𝑒𝑤_𝑐𝑜𝑙 in direction 𝑑𝑖𝑟 . Depending
on the direction of navigation, either the starting column for
navigation 𝑐𝑜𝑙 is the subject of the triple and the ending col-
umn 𝑛𝑒𝑤_𝑐𝑜𝑙 is the object, or vice versa. 𝑖𝑠_𝑜𝑝𝑡 determines
whether null values are allowed. If 𝑖𝑠_𝑜𝑝𝑡 is false, 𝑒𝑥𝑝𝑎𝑛𝑑
filters out the rows in 𝐷 that have a null value in 𝑛𝑒𝑤_𝑐𝑜𝑙.
Formally, if 𝑡 is a SPARQL pattern representing the naviga-
tion step, then 𝑡 = (𝑐𝑜𝑙, 𝑝𝑟𝑒𝑑, 𝑛𝑒𝑤_𝑐𝑜𝑙) if direction is 𝑜𝑢𝑡 or
𝑡 = (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑝𝑟𝑒𝑑, 𝑐𝑜𝑙) if direction is 𝑖𝑛. Let 𝐷𝑡 be the RDF-
Frame corresponding to the evaluation of the triple pattern 𝑡 on
graph G. 𝐷𝑡 will contain one column 𝑛𝑒𝑤_𝑐𝑜𝑙 and the rows are
the objects of 𝑡 if the direction is 𝑖𝑛 or the subjects if the direc-
tion is 𝑜𝑢𝑡. Then 𝐷 ′ = 𝐷 (cid:90) 𝐷𝑡 if 𝑖𝑠_𝑜𝑝𝑡 is false or 𝐷 ′ = 𝐷 ⟕ 𝐷𝑡
if 𝑖𝑠_𝑜𝑝𝑡 is true. For example, in Listing 1, expand is used twice,
once to add the country attribute of the actor to the RDFFrame
and once to find the movies and (if available) Academy Awards
for prolific American actors.

Relational Operators. These operators are used to clean and fur-
ther process RDFFrames. They have the same semantics as in re-
lational databases. They take as input one or two RDFFrames and
output an RDFFrame.
• 𝐷.𝑓 𝑖𝑙𝑡𝑒𝑟 (𝑐𝑜𝑛𝑑𝑠 = [𝑐𝑜𝑛𝑑1 ∧ 𝑐𝑜𝑛𝑑2 ∧ . . . ∧ 𝑐𝑜𝑛𝑑𝑘 ]), where 𝑐𝑜𝑛𝑑𝑠
is a list of expressions of the form (𝑐𝑜𝑙 {<, >, =, . . .} 𝑣𝑎𝑙) or
one of the pre-defined boolean functions found in SPARQL
like 𝑖𝑠𝑈 𝑅𝐼 (𝑐𝑜𝑙) or 𝑖𝑠𝐿𝑖𝑡𝑒𝑟𝑎𝑙 (𝑐𝑜𝑙): This operator filters out rows
from an RDFFrame that do not conform to 𝑐𝑜𝑛𝑑𝑠. Formally, let
𝜑 = [𝑐𝑜𝑛𝑑1 ∧ 𝑐𝑜𝑛𝑑2 ∧ . . . ∧ 𝑐𝑜𝑛𝑑𝑘 ] be a propositional formula
where 𝑐𝑜𝑛𝑑𝑖 is an expression. Then 𝐷 ′ = 𝜎𝜑 (𝐷). In Listing 1,
filter is used two times, once to restrict the extracted data to
American actors and once to restrict the results of a group by
in order to identify prolific actors (defined as having 50 or more
movies). The latter filter operator is applied after group_by
and the aggregation function count, which corresponds to a

very different SPARQL pattern compared to the first usage. How-
ever, this is handled internally by RDFFrames and is transparent
to the user.

• 𝐷.𝑠𝑒𝑙𝑒𝑐𝑡_𝑐𝑜𝑙𝑠 (𝑐𝑜𝑙𝑠), where 𝑐𝑜𝑙𝑠 ⊆ C: Similar to the relational
projection operation, it keeps only the columns 𝑐𝑜𝑙𝑠 and re-
moves the rest. Formally, 𝐷 ′ = 𝜋𝑐𝑜𝑙𝑠 (𝐷).

• 𝐷.𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, 𝑗𝑡𝑦𝑝𝑒, 𝑛𝑒𝑤_𝑐𝑜𝑙), where 𝐷2 = (C2, R2) is
another RDFFrame, 𝑐𝑜𝑙 ∈ C, 𝑐𝑜𝑙2 ∈ C2, and 𝑗𝑡𝑦𝑝𝑒 ∈ {(cid:90)
, ⟕, ⟖, ⟗}: This operator joins two RDFFrame tables on their
columns 𝑐𝑜𝑙 and 𝑐𝑜𝑙2 using the join type 𝑗𝑡𝑦𝑝𝑒. 𝑛𝑒𝑤_𝑐𝑜𝑙 is
the desired name of the new joined column. Formally, 𝐷 ′ =
𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙 (𝐷) 𝑗𝑡𝑦𝑝𝑒 𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙2 (𝐷2).

• 𝐷.𝑔𝑟𝑜𝑢𝑝_𝑏𝑦(𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠).𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛(𝑓 𝑛, 𝑐𝑜𝑙, 𝑛𝑒𝑤_𝑐𝑜𝑙),

⊆

𝑓 𝑛
∈

C,
𝑐𝑜𝑙

𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠

where
∈
{𝑚𝑎𝑥, 𝑚𝑖𝑛, 𝑎𝑣𝑒𝑟𝑎𝑔𝑒, 𝑠𝑢𝑚, 𝑐𝑜𝑢𝑛𝑡, 𝑠𝑎𝑚𝑝𝑙𝑒},
C and
𝑛𝑒𝑤_𝑐𝑜𝑙 ∈ 𝑁 : This operator groups the rows of 𝐷 ac-
cording to their values in one or more columns 𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠.
As in the relational grouping and aggregation operation, it
partitions the rows of an RDFFrame into groups and then
applies the aggregation function on the values of column 𝑐𝑜𝑙
within each group. It returns a new RDFFrame which contains
the grouping columns and the result of the aggregation on each
group, i.e., C′ = 𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠 ∪ {𝑛𝑒𝑤_𝑐𝑜𝑙 }. The combinations
of values of the grouping columns in 𝐷 ′ are unique. Formally,
𝐷 ′ = 𝛾𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠,𝑓 𝑛 (𝑐𝑜𝑙)↦→𝑛𝑒𝑤_𝑐𝑜𝑙 (𝐷). Note that query gen-
eration has special handling for RDFFrames output by the
𝑔𝑟𝑜𝑢𝑝_𝑏𝑦 operator (termed grouped RDFFrames). This special
handling is internal to RDFFrames and transparent to the user.
In Listing 1, group_by is used with the count function to find
the number of movies in which each actor appears.

• 𝐷.𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑒 (𝑓 𝑛, 𝑐𝑜𝑙, 𝑛𝑒𝑤_𝑐𝑜𝑙), where 𝑐𝑜𝑙 ∈ C and 𝑓 𝑛 ∈
{𝑚𝑎𝑥, 𝑚𝑖𝑛, 𝑎𝑣𝑒𝑟𝑎𝑔𝑒, 𝑠𝑢𝑚, 𝑐𝑜𝑢𝑛𝑡, 𝑑𝑖𝑠𝑡𝑖𝑛𝑐𝑡_𝑐𝑜𝑢𝑛𝑡 }: This opera-
tor aggregates values of the column 𝑐𝑜𝑙 and returns an
RDFFrame that has one column and one row contain-
ing the aggregated value. It has the same formal seman-
tics as the 𝐷.𝑔𝑟𝑜𝑢𝑝_𝑏𝑦 ().𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛() operator except that
𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠 = ∅, so the whole RDFFrame is assumed to be one
group. No further processing can be done on the RDFFrame
after this operator.

• 𝐷.𝑠𝑜𝑟𝑡 (𝑐𝑜𝑙𝑠_𝑜𝑟𝑑𝑒𝑟 ), where 𝑐𝑜𝑙𝑠_𝑜𝑟𝑑𝑒𝑟 is a set of pairs (𝑐𝑜𝑙, 𝑜𝑟𝑑𝑒𝑟 )
with 𝑐𝑜𝑙 ∈ C and 𝑜𝑟𝑑𝑒𝑟 ∈ {𝑎𝑠𝑐, 𝑑𝑒𝑠𝑐}: This operator sorts the
rows of the RDFFrame according to their values in the given
columns and their sorting order and returns a sorted RDFFrame.
• 𝐷.ℎ𝑒𝑎𝑑 (𝑘, 𝑖), where 𝑘 ≤ 𝑛: Returns the first 𝑘 rows of the RDF-
Frame starting from row 𝑖 (by default 𝑖 = 0). No further process-
ing can be done on the RDFFrame after this operator.

4 QUERY GENERATION
One of the key innovations in RDFFrames is the query generation
process. Query generation produces a SPARQL query from an RDF-
Frame representing a sequence of calls to RDFFrames operators.
The guidelines we use in query generation to guarantee efficient
processing are as follows:
• Include all of the computation required for generating an RDF-
Frame in the SPARQL query sent to the RDF engine. Pushing

6

Figure 2: Example of an RDFFrames nested query model.

computation into the engine enables RDFFrames to take ad-
vantage of the benefits of a database system such as query
optimization, bulk data processing, and near-data computing.
• Generate one SPARQL query for each RDFFrame, never more.
RDFFrames combines all graph patterns and operations de-
scribed by an RDFFrame into a single SPARQL query. This
minimizes the number of interactions with the RDF engine
and enables the query optimizer to explore all optimization
opportunities since it can see all operations.

• Ensure that the generated query is as simple as possible. The
query generation algorithm generates graph patterns that mini-
mize the use of nested subqueries and union SPARQL patterns,
since these are known to be expensive. Note that, in principle,
we are doing part of the job of the RDF engine’s query optimizer.
A powerful-enough optimizer would be able to simplify and
unnest queries whenever possible. However, the reality is that
SPARQL is a complex language on which query optimizers do
not always do a good job. As such, any steps to help the query
optimizer are of great use. We show the performance benefit of
this approach in Section 6.

• Adopt a lazy execution model, generating and processing a

query only when required by the user.

• Ensure that the generated SPARQL query is correct, that is,
ensure the query is semantically equivalent to the RDFFrame.
We prove this in Section 5.
Our query model is inspired by the Query Graph Model [36],
and it encapsulates all components required to construct a SPARQL
query. Query models can be nested in cases where nested subqueries
are required. Using the query model as an intermediate representa-
tion between an RDFFrame and the corresponding SPARQL query
allows for (i) flexible implementation by separating the operator
manipulation logic from the query generation logic, and (ii) simpler
optimization. Without a query model, a naive implementation of
RDFFrames would translate each operator to a SPARQL pattern and
encapsulate it in a subquery, with one outer query joining all the
subqueries to produce the result. This is analogous to how some
software-generated SQL queries are produced. Other implemen-
tations are possible such as producing a SPARQL query for each
operator and re-parsing it every time it has to be combined with a
new pattern, or directly manipulating the parse tree of the query.
The query model enables a simpler and more powerful implemen-
tation.

An example query model representing a nested SPARQL query
is shown in Figure 2. The left part of the figure is the outer query

Outer query modelInner query model?pc?yp6p3select varstriple patterns?p?yﬁlter conditionsgroupby varsaggregatessort varslimitoﬀsetsubqueriesall vars?p?yoptional blockunion queriesgraph URIexample.compreﬁxesa: aa.comselect varstriple patterns?p?yﬁlter conditionsgroupby varssort varslimitoﬀsetsubqueriesall vars?p?yoptional blockunion queries?xb?z?yp1p2p3p4ap5?w?xaggregatescount(?y)> 100date(?z) > 2010model, which has a reference to the inner query model (right part
of the figure). The figure shows the components of a SPARQL query
represented in a query model. These are as follows:
• Graph matching patterns including triple patterns, filter condi-
tions, pointers to inner query models for sub-queries, optional
blocks, and union patterns. Graph pattern matching is a basic
operation in SPARQL. A SPARQL query can be formed by com-
bining triple patterns in various ways using different keywords.
The default is that a solution is produced if and only if every
triple pattern that appears in a graph pattern is matched to the
triples in the RDF graph. The OPTIONAL keyword adds triple
patterns that extend the solution if they are matched, but do
not eliminate the solution if they are not matched. That is, OP-
TIONAL creates left outer join semantics. The FILTER keyword
adds a condition and restricts the query results to solutions that
satisfy this condition.

• Aggregation constructs including: group-by columns, aggre-
gation columns, and filters on aggregations (which result in a
HAVING clause in the SPARQL query). These patterns are applied
to the result RDFFrame generated so far. Unlike graph matching
patterns, they are not matched to the RDF graph.Aggregation
constructs in inner query models are not propagated to outer
query models.

• Query modifiers including limit, offset and sorting columns.
These constructs make final modifications to the result of the
query. Any further API calls after adding these modifiers will
result in a nested query as the current query model is wrapped
and added to another query model.

• The graph URIs by the query, the prefixes used, and the variables

in the scope of each query.

4.1 Query Model Generation
The query model is generated lazily, when the special execute
function is called on an RDFFrame. We observe that generating the
query model requires capturing the order of calls to RDFFrames
operators and the parameters of these calls, but nothing more. Thus,
each RDFFrame 𝐷 created by the user is associated with a FIFO
queue of operators. The Recorder component of RDFFrames (recall
Figure 1) records in this queue the sequence of operator calls made
by the user. When execute is called, the Generator component of
RDFFrames creates the query model incrementally by processing
the operators in this queue in FIFO order. RDFFrames starts with an
empty query model 𝑚. For each operator pulled from the queue of
𝐷, its corresponding SPARQL component is inserted into 𝑚. Each
RDFFrames operator edits one or two components of 𝑚. All of the
optimizations to generate efficient SPARQL queries are done during
query model generation.

The first operator to be processed is always a 𝑠𝑒𝑒𝑑 operator
for which RDFFrames adds the corresponding triple pattern to
the query model 𝑚. To process an 𝑒𝑥𝑝𝑎𝑛𝑑 operator, it adds the
corresponding triple pattern(s) to 𝑚. For example, the operator
𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, out, false) will result in the triple pattern (?𝑥, 𝑝𝑟𝑒𝑑, ?𝑦)
being added to the triple patterns of 𝑚. Similarly, processing the
𝑓 𝑖𝑙𝑡𝑒𝑟 operator adds the conditions that are input parameters of
this operator to the filter conditions in 𝑚. To generate succinct
optimized queries, RDFFrames adds all triple and filter patterns to
the same query model 𝑚, as long as the semantics are preserved. As

7

a special case, when 𝑓 𝑖𝑙𝑡𝑒𝑟 is called on an aggregated column, the
Generator adds the filtering condition to the ℎ𝑎𝑣𝑖𝑛𝑔 component of
𝑚.

One of the main challenges in designing RDFFrames was iden-
tifying the cases where a nested SPARQL query is necessary. We
were able to limit this to three cases where a nested query is needed
to maintain the semantics:
• Case 1: when an 𝑒𝑥𝑝𝑎𝑛𝑑 or 𝑓 𝑖𝑙𝑡𝑒𝑟 operator has to be applied on
a grouped RDFFrame. The semantics here can be thought of as
creating an RDFFrame that satisfies the expand or filter pattern
and then joining it with the grouped RDFFrame. For example,
the RDFFrames code in Listing 3 expands the country column
to obtain the continent after the group_by and count. This is
semantically equivalent to building an RDFFrame of countries
and their continents and then performing an inner join with
the grouped RDFFrame.

df = graph . entities ( ': dpo : Actor ' , ' actor ')\

. expand ( ' actor ', [( ' dbp : birthPlace ' , ' country ') ]) \
. group_by ([ ' actor ' ]) \
.count ( ' country ' , ' country_count ')\
. expand ( ' country ', [( ' dbo : continent ' , ' continent '])

Listing 3: RDFFrames code - Expanding a grouped
RDFFrame.
• Case 2: When a grouped RDFFrame has to be joined with an-
other RDFFrame (grouped or non-grouped). For example, List-
ing 4 represents a join between a grouped RDFFrame and an-
other RDFFrame.

df1 = graph . entities ( ' dbo : Actor ' , ' actor ')\

. expand ( ' actor ', [( ' dbp : birthPlace ' , ' count ry ') ]) \
. group_by ([ ' actor ' ]) . count ( ' count ry ' , ' count ry_ count ')

df2 = graph . feature_domain_range ( ' dbp : starring ' ,

' movie ' , ' actor '). join ( df1 , ' actor ' , InnerJoin )

Listing 4: RDFFrames code - Joining a grouped RDFFrame
with another RDFFrame.
• Case 3: When two datasets are joined by a full outer join. For
example, the RDFFrames code in Listing 5 is a full outer join
between two datasests.

df1 = graph . entities ( ' dpo : Actor ' , ' actor ')\

. expand ( ' actor ', [( ' dbp : birthPlace ' , ' count ry ') ])

df2 = graph . feature_domain_range ( ' dbp : starring ' ,

' movie ' , ' actor '). join ( df1 , ' actor ' , OuterJoin )

Listing 5: RDFFrames code - Full outer join.

There is no explicit full outer join between patterns in SPARQL,
only left outer join using the OPTIONAL pattern. Therefore, we
define full outer join using the UNION and OPTIONAL patterns
as the union of the left outer join and the right outer join of 𝐷1
and 𝐷2. A nesting query is required to wrap the query model
for each RDFFrame inside the final query model.
In the first case, when an 𝑒𝑥𝑝𝑎𝑛𝑑 operation is called on a grouped
RDFFrame, RDFFrames has to wrap the grouped RDFFrame in
a nested subquery to ensure the evaluation of the grouping and
aggregation operations before the expansion. RDFFrames uses the
following steps to generate the subquery: (i) create an empty query
model 𝑚′, (ii) transform the query model built so far 𝑚 into a
subquery of 𝑚′, and (iii) add the new triple pattern from the 𝑒𝑥𝑝𝑎𝑛𝑑
operator to the triple patterns of 𝑚′. In this case, 𝑚′ is the outer
query model after the 𝑒𝑥𝑝𝑎𝑛𝑑 operator and the grouped RDFFrame

is represented by the inner query model 𝑚. Similarly, when 𝑓 𝑖𝑙𝑡𝑒𝑟 is
applied on a grouping column in a grouped RDFFrame, RDFFrames
creates a nested query model by transforming 𝑚 into a subquery.
This is necessary since the filter operation was called after the
aggregation and, thus, has to be done after the aggregation to
maintain the correctness of the aggregated values.

The second case in which a nested subquery is required is when
joining a grouped RDFFrame with another RDFFrame. In the fol-
lowing, we describe in full the different cases of processing the 𝑗𝑜𝑖𝑛
operator, including the cases when subqueries are required.

To process the binary 𝑗𝑜𝑖𝑛 operator, RDFFrames needs to join
two query models of two different RDFFrames 𝐷1 and 𝐷2. If the
join type is full outer join, a complex query that is equivalent to
the full outer join is constructed using the SPARQL OPTIONAL
(⟕) and UNION (∪) patterns. Formally, 𝐷1 ⟗ 𝐷2 = (𝐷1 ⟕ 𝐷2) ∪
𝜌𝑟𝑒𝑜𝑟𝑑𝑒𝑟 (𝐷2 ⟕ 𝐷1).

To process a full outer join, two new query models are con-
structed: The first query model 𝑚1 ′ contains the left outer join of
the query models 𝑚1 and 𝑚2, which represent 𝐷1 and 𝐷2, respec-
tively. The second query model 𝑚2 ′ contains the right outer join of
the of the query models 𝑚1 and 𝑚2, which is equivalent to the left
outer join of 𝑚2 and 𝑚1. The columns of 𝑚2 ′ are reordered to make
them union compatible with 𝑚1 ′. Nested queries are necessary to
wrap the two query models 𝑚1 and 𝑚2 inside 𝑚1 ′ and 𝑚2 ′. One
final outer query model unions the two new query models 𝑚1 ′ and
𝑚2 ′.

For other join types, we distinguish three cases:

• 𝐷1 and 𝐷2 are not grouped: RDFFrames merges the two query
models into one by combining their graph patterns (e.g., triple
patterns and filter conditions). If the join type is left outer join,
the patterns of 𝐷2 are added inside a single OPTIONAL block of
𝐷1. Conversely, for a right outer join the 𝐷1 patterns are added
as OPTIONAL in 𝐷2. No nested query is generated here.

• 𝐷1 is grouped and 𝐷2 is not: RDFFrames merges the two query
models via nesting. The query model of 𝐷1 is the inner query
model, while 𝐷2 is set as the outer query model. If the join type
is left outer join, 𝐷2 patterns are wrapped inside a single OP-
TIONAL block of 𝐷1, and if the join type is right outer join, the
subquery model generated for 𝐷1 is wrapped in an OPTIONAL
block in 𝐷2. This is an example of the second case in which
nested queries are necessary. The case when 𝐷2 is grouped and
𝐷1 is not is analogous to this case.

• Both 𝐷1 and 𝐷2 are grouped: RDFFrames creates one query
model containing two nested query models, one for each RDF-
Frame, another example of the second case where nested queries
are necessary.
If 𝐷1 and 𝐷2 are constructed from different graphs, the original
graph URIs are used in the inner query to map each pattern to the
graph it is supposed to match.

To process other operators such as 𝑠𝑒𝑙𝑒𝑐𝑡_𝑐𝑜𝑙𝑠 and 𝑔𝑟𝑜𝑢𝑝_𝑏𝑦,
RDFFrames fills the corresponding component in the query model.
The ℎ𝑒𝑎𝑑 operator maps to the 𝑙𝑖𝑚𝑖𝑡 and offset components of the
query model 𝑚. To finalize the join processing, RDFFrames unions
the selection variables of the two query models, and takes the
minimum of the offsets and the maximum of the limits (in case both
query models have an offset and a limit).

4.2 Translating to SPARQL
The query model is designed to make translation to SPARQL as
direct and simple as possible. RDFFrames traverses a query model
and translates each component of the model directly to the cor-
responding SPARQL construct, following the syntax and style
guidelines of SPARQL. For example, each prefix is translated to
PREFIX name_space:name_space_uri, graph URIs are added to
the FROM clause, and each triple and filter pattern is added to the
WHERE clause. The inner query models are translated recursively to
SPARQL queries and added to the outer query using the subquery
syntax defined by SPARQL. When the query accesses more than
one graph and different subsets of graph patterns are matched to
different graphs, the GRAPH construct is used to wrap each subset
of graph patterns with the matching graph URI.

The generated SPARQL query is sent to the RDF engine or
SPARQL endpoint using the SPARQL protocol18 over HTTP. We
choose communication over HTTP since it is the most general
mechanism to communicate with RDF engines and the only mecha-
nism to communicate with SPARQL endpoints. One issue we need
to address is paginating the results of a query, that is, retrieving
them in chunks. There are several good reasons to paginate results,
for example, avoiding timeouts at SPARQL endpoints and bounding
the amount of memory used for result buffering at the client. When
using HTTP communication, we cannot rely on RDF engine cursors
to do the pagination as they are engine-specific and not supported
by the SPARQL protocol over HTTP. The HTTP response returns
only the first chunk of the result and the size of the chunk is limited
by the SPARQL endpoint configuration. The SPARQL over HTTP
client has to ask for the rest of the result chunk by chunk but this
functionality is not implemented by many existing clients. Since
our goal is generality and flexibility, RDFFrames implements pagi-
nation transparently to the user and returns one dataframe with all
the query results.

5 SEMANTIC CORRECTNESS OF QUERY

GENERATION

In this section, we formally prove that the SPARQL queries gen-
erated by RDFFrames return results that are consistent with the
semantics of the RDFFrames operators. We start with an overview
of RDF and the SPARQL algebra to establish the required notation.
We then summarize the semantics of SPARQL, which is necessary
for our correctness proof. Finally, we formally describe the query
generation algorithm in RDFFrames and prove its correctness.

5.1 SPARQL Algebra
The RDF data model can be defined as follows. Assume there are
countably infinite pairwise disjoint sets 𝐼 , 𝐵, and 𝐿 representing
URIs, blank nodes, and literals, respectively. Let 𝑇 = (𝐼 ∪ 𝐵 ∪ 𝐿)
be the set of RDF terms. The basic component of an RDF graph is
an RDF triple (𝑠, 𝑝, 𝑜) ∈ (𝐼 ∪ 𝐵) × 𝐼 × 𝑇 where 𝑠 is the 𝑠𝑢𝑏 𝑗𝑒𝑐𝑡, 𝑜 is
the 𝑜𝑏 𝑗𝑒𝑐𝑡, and 𝑝 is the 𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒. An RDF graph is a finite set of
RDF triples. Each triple represents a fact describing a relationship
of type 𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒 between the 𝑠𝑢𝑏 𝑗𝑒𝑐𝑡 and the 𝑜𝑏 𝑗𝑒𝑐𝑡 nodes in the
graph.

18https://www.w3.org/TR/sparql11-protocol

8

SPARQL is a graph-matching query language that evaluates
patterns on graphs and returns a result set. Its algebra consists of
two building blocks: expressions and patterns.

Let 𝑋 = {?𝑥1, ?𝑥2, . . . , ?𝑥𝑛 } be a set of variables disjoint from
the RDF terms 𝑇 , the SPARQL syntactic blocks are defined over
𝑇 and 𝑋 . For a pattern 𝑃, 𝑉 𝑎𝑟 (𝑃) is the set of all variables in 𝑃.
Expressions and patterns in are defined recursively as follows:
• A triple 𝑡 ∈ (𝐼 ∪ 𝐿 ∪ 𝑋 ) × (𝐼 ∪ 𝑋 ) × (𝐼 ∪ 𝐿 ∪ 𝑋 ) is a pattern.
• If 𝑃1 and 𝑃2 are patterns, then 𝑃1 𝐽𝑜𝑖𝑛 𝑃2, 𝑃1 𝑈 𝑛𝑖𝑜𝑛 𝑃2, and

𝑃1 𝐿𝑒 𝑓 𝑡 𝐽𝑜𝑖𝑛 𝑃2 are patterns.

• Let all variables in 𝑋 and all terms in 𝐼 ∪ 𝐿 be SPARQL expres-
sions; then (𝐸1 + 𝐸2), (𝐸1 − 𝐸2), (𝐸1 × 𝐸2), (𝐸1/𝐸2), (𝐸1 = 𝐸2),
(𝐸1 < 𝐸2), (¬𝐸1), (𝐸1 ∧ 𝐸2), and (𝐸1 ∨ 𝐸2) are expressions. If 𝑃
is a pattern and 𝐸 is an expression then 𝐹𝑖𝑙𝑡𝑒𝑟 (𝐸, 𝑃) is a pattern.
• If 𝑃 is a pattern and 𝑋 is a set of variables in 𝑉 𝑎𝑟 (𝑃), then
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑋, 𝑃) and 𝐷𝑖𝑠𝑡𝑖𝑛𝑐𝑡 (𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑋, 𝑃)) are patterns. These
two constructs allow nested queries in SPARQL and by adding
them, there is no meaningful distinction between SPARQL pat-
terns and queries.

• If 𝑃 is a pattern, 𝐸 is an expression and ?𝑥 is a variable not
in 𝑉 𝑎𝑟 (𝑃), then 𝐸𝑥𝑡𝑒𝑛𝑑 (?𝑥, 𝐸, 𝑃) is a pattern. This allows as-
signment of expression values to new variables and is used for
variable renaming in RDFFrames.

• If 𝑋 is a set of variables, ?𝑧 is another variable, 𝑓 is an ag-
gregation function, 𝐸 is an expression, and 𝑃 is a pattern,
then 𝐺𝑟𝑜𝑢𝑝𝐴𝑔𝑔(𝑋, ?𝑧, 𝑓 , 𝐸, 𝑃) is a pattern where 𝑋 is the set
of grouping variables, ?𝑧 is a fresh variable to store the ag-
gregation result, 𝐸 is often a variable that we are aggregat-
ing on. This pattern captures the grouping and aggregation
constructs in SPARQL 1.1. It induces a partitioning of a pat-
tern’s solution mappings into equivalence classes based on
the values of the grouping variables and finds one aggregate
value for each class using one of the aggregation functions in
{𝑚𝑎𝑥, 𝑚𝑖𝑛, 𝑎𝑣𝑒𝑟𝑎𝑔𝑒, 𝑠𝑢𝑚, 𝑐𝑜𝑢𝑛𝑡, 𝑠𝑎𝑚𝑝𝑙𝑒}.
SPARQL defines some modifiers for the result set returned by the
evaluation of the patterns. These modifiers include: 𝑂𝑟𝑑𝑒𝑟 (𝑋, 𝑜𝑟𝑑𝑒𝑟 )
where 𝑋 is the set of variables to sort on and 𝑜𝑟𝑑𝑒𝑟 is 𝑎𝑠𝑐𝑒𝑛𝑑𝑖𝑛𝑔 or
𝑑𝑒𝑠𝑐𝑒𝑛𝑑𝑖𝑛𝑔, 𝐿𝑖𝑚𝑖𝑡 (𝑛) which returns the first 𝑛 values of the result
set, and Offset (𝑘) which returns the results starting from the 𝑘-th
value.

Let

𝐸
(cid:74)

𝐺 denote the evaluation of expression 𝐸 on graph 𝐺, 𝜇 (𝑃)
(cid:75)
the pattern obtained from 𝑃 by replacing its variables according
to 𝜇, and 𝑉 𝑎𝑟 (𝑃) the set of all the variables in 𝑃. The semantics of
patterns over graph 𝐺 are defined as:
•

𝐸
(cid:74)

𝜇,𝐺 = 𝑡𝑟𝑢𝑒}}

𝐺 : the solution of a triple pattern 𝑡 is the multiset with 𝑆𝑡 =
(cid:75)
𝐺 (𝜇) = 1
(cid:75)

𝑡
(cid:74)
all 𝜇 such that 𝑑𝑜𝑚(𝜇) = 𝑉 𝑎𝑟 (𝑡) and 𝜇 (𝑡) ∈ 𝐺. 𝑐𝑎𝑟𝑑
𝑡
for all such 𝜇.
(cid:74)
𝑃2
𝑃1
𝐺 = {{𝜇|𝜇1 ∈
𝐺, 𝜇2 ∈
𝐺, 𝜇 = 𝜇1 ∪ 𝜇2}}
𝑃1 𝐽𝑜𝑖𝑛 𝑃2
(cid:75)
(cid:75)
(cid:75)
(cid:74)
(cid:74)
(cid:74)
𝑃1 𝐽𝑜𝑖𝑛 𝑃2
𝐺 = {{𝜇|𝜇 ∈
𝑃1 𝐿𝑒 𝑓 𝑡 𝐽𝑜𝑖𝑛 𝑃2
𝐺 }}⊎
(cid:75)
(cid:74)
(cid:75)
(cid:74)
𝐺, (𝜇 ≁ 𝜇2)}}
𝑃2
𝑃1
𝐺, ∀𝜇2 ∈
{{𝜇|𝜇 ∈
(cid:74)
(cid:75)
(cid:75)
(cid:74)
𝑃2
𝑃1
𝑃1 𝑈 𝑛𝑖𝑜𝑛 𝑃2
𝐺 ⊎
𝐺 =
𝐺
(cid:74)
(cid:74)
(cid:75)
(cid:75)
(cid:75)
(cid:74)
𝑃1
𝐺 = {{𝜇|𝜇 ∈
𝐺,
𝐹𝑖𝑙𝑡𝑒𝑟 (𝐸, 𝑃)
(cid:74)
(cid:74)
(cid:75)
(cid:75)
𝐺 , if 𝜇 is a restriction to 𝑋 then it
𝑃
𝐺 = ∀𝜇 ∈
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑋, 𝑃)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
is in the base set of this pattern and its multiplicity is the sum
of multiplicities of all corresponding 𝜇.
𝐺 ,
𝐷𝑖𝑠𝑡𝑖𝑛𝑐𝑡 (𝑄)
(cid:74)
(cid:75)
but with multiplicity 1 for all mappings. The SPARQL patterns
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑋, 𝑃) and 𝐷𝑖𝑠𝑡𝑖𝑛𝑐𝑡 (𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑋, 𝑃)) define a SPARQL
query. When used in the middle of a query, they define a nested
query.
𝐸𝑥𝑡𝑒𝑛𝑑 (?𝑥, 𝐸, 𝑃)
𝐺 =
(cid:75)
(cid:74)
𝐺, 𝜇 ′ = 𝜇 ∪ {?𝑥 →
{𝜇 ′|𝜇 ∈
𝑃
𝐸
(cid:74)
(cid:75)
(cid:74)
𝜇,𝐺 = 𝐸𝑟𝑟𝑜𝑟 } and 𝑉 𝑎𝑟 (𝐸𝑥𝑡𝑒𝑛𝑑 (?𝑥, 𝐸, 𝑃)) =
𝐸
𝐺,
𝑃
{𝜇|𝜇 ∈
(cid:74)
(cid:75)
(cid:74)
{?𝑥 } ∩ 𝑉 𝑎𝑟 (𝑃)

𝐺 = the multiset with the same base set as
(cid:75)

𝜇,𝐺 ≠ 𝐸𝑟𝑟𝑜𝑟 } ⊎

𝜇,𝐺 },

𝑄
(cid:74)

𝐸
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

•
•

•
•
•

•

•

𝑃
(cid:74)

𝑃
(cid:74)

• Given a graph 𝐺, let 𝑣 |𝑥 be the restriction of 𝑣 to 𝑋 , then
𝐺 is the multiset with the base set:
𝐺𝑟𝑜𝑢𝑝𝐴𝑔𝑔(𝑋, ?𝑧, 𝑓 , 𝐸, 𝑃)
(cid:75)
(cid:74)
{𝜇 ′|𝜇 ′ = 𝜇|𝑋 ∪ {?𝑧 → 𝑣𝜇 }, 𝜇 ∈
𝐺, 𝑣𝜇 ≠ 𝐸𝑟𝑟𝑜𝑟 } ∪
(cid:75)
{𝜇 ′|𝜇 ′ = 𝜇|𝑋, 𝜇 ∈
𝐺, 𝑣𝜇 = 𝐸𝑟𝑟𝑜𝑟 } and multiplicity 1 for each
(cid:75)
mapping in the base set, where for each mapping 𝜇 ∈
𝐺 , the
(cid:75)
value of the aggregation function on the group that the mapping
𝐺, 𝜇 ′|𝑥 = 𝜇|𝑥, 𝑣 =
belongs to is 𝑣𝜇 = 𝑓 ({𝑣 | 𝜇 ′ ∈
𝜇′,𝐺 }).
𝑃
(cid:75)
(cid:74)
5.3 Semantic Correctness
Having defined the semantics of SPARQL patterns, we now prove
the semantic correctness of query generation in RDFFrames as
follows. First, we formally define the SPARQL query generation al-
gorithm. That is, we define the SPARQL query or pattern generated
by any sequence of RDFFrames operators. We then prove that the
solution sets of the generated SPARQL patterns are equivalent to
the RDFFrames tables defined by the semantics of the sequence of
RDFFrames operators.

𝑃
(cid:74)
𝐸
(cid:74)

(cid:75)

5.2 SPARQL Semantics
In this section, we present the semantics defined in [23], which
assumes bag semantics and integrates all SPARQL 1.1 features such
as aggregation and subqueries.

The semantics of SPARQL queries are based on multisets (bags)
of mappings. A mapping is a partial function 𝜇 from 𝑋 to 𝑇 where
𝑋 is a set of variables and 𝑇 is the set of RDF terms. The domain
of a mapping 𝑑𝑜𝑚(𝜇) is the set of variables where 𝜇 is defined.
𝜇1 and 𝜇2 are compatible mappings, written (𝜇1 ∼ 𝜇2), if (∀?𝑥 ∈
𝑑𝑜𝑚(𝜇1) ∩ 𝑑𝑜𝑚(𝜇2), 𝜇1 (?𝑥) = 𝜇2 (?𝑥)). If 𝜇1 ∼ 𝜇2, 𝜇1 ∪ 𝜇2 is also a
mapping and is obtained by extending 𝜇1 by 𝜇2 mappings on all
the variables 𝑑𝑜𝑚(𝜇2) \ 𝑑𝑜𝑚(𝜇1).

A SPARQL pattern solution is a multiset Ω = (𝑆Ω, 𝑐𝑎𝑟𝑑Ω) where
𝑆Ω is the base set of mappings, and the multiplicity function 𝑐𝑎𝑟𝑑Ω
assigns a positive number to each element of 𝑆Ω.

9

5.3.1 Query Generation Algorithm. To formally define the query
generation algorithm, we first define the SPARQL pattern each
RDFFrames operator generates. We then give a recursive definition
of a non-empty RDFFrame and then define a recursive mapping
from any sequence of RDFFrames operators constructed by the user
to a SPARQL pattern using the patterns generated by each operator.
This mapping is based on the query model described in Section 4.
Definition 3 (Non-empty RDFFrame). A non-empty RDFFrame
is either generated by the 𝑠𝑒𝑒𝑑 operator or by applying an RDFFrames
operator on one or two non-empty RDFFrames.

Given a non-empty RDFFrame 𝐷, let 𝑂𝐷 be the sequence of

RDFFrames operators that generated it.

Definition 4 (Operators to Patterns). Let 𝑂 = [𝑜1, . . . , 𝑜𝑘 ]
be a sequence of RDFFrames operators and 𝑃 be a SPARQL pattern.
Also let 𝑔 : (𝑜, 𝑃) → 𝑃 be the mapping from a single RDFFrames
operator 𝑜 to a SPARQL pattern based on the query generation of

Table 1: Mappings of RDFFrames operators on graph 𝐺 and/or RDFFrame 𝐷 to SPARQL patterns on 𝐺. 𝑃 is the SPARQL pattern
equivalent to the sequence of RDFFrames operators called so far on 𝐷 (or null if 𝐷 is new).

RDFFrames Operator 𝑶
𝑠𝑒𝑒𝑑 (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3)
𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑜𝑢𝑡, 𝑓 𝑎𝑙𝑠𝑒)
𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑖𝑛, 𝑓 𝑎𝑙𝑠𝑒)
𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑜𝑢𝑡,𝑇𝑟𝑢𝑒)
𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑖𝑛,𝑇𝑟𝑢𝑒)
𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, (cid:90), 𝑛𝑒𝑤_𝑐𝑜𝑙)
𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, ⟕, 𝑛𝑒𝑤_𝑐𝑜𝑙)
𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, ⟖, 𝑛𝑒𝑤_𝑐𝑜𝑙)
𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, ⟗, 𝑛𝑒𝑤_𝑐𝑜𝑙)

𝑓 𝑖𝑙𝑡𝑒𝑟 (𝑐𝑜𝑛𝑑𝑠 = [𝑐𝑜𝑛𝑑1 ∧ 𝑐𝑜𝑛𝑑2 ∧ · · · ∧ 𝑐𝑜𝑛𝑑𝑘 ])
𝑠𝑒𝑙𝑒𝑐𝑡_𝑐𝑜𝑙𝑠 (𝑐𝑜𝑙𝑠)
𝑔𝑟𝑜𝑢𝑝𝑏𝑦 (𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠).
𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛(𝑓 𝑛, 𝑠𝑟𝑐_𝑐𝑜𝑙, 𝑛𝑒𝑤_𝑐𝑜𝑙)
𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑒 (𝑓 𝑛, 𝑐𝑜𝑙, 𝑛𝑒𝑤_𝑐𝑜𝑙)

SPARQL pattern: 𝒈(𝑶, 𝑷 )
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑉 𝑎𝑟 (𝑡), 𝑡), where 𝑡 = (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3)
𝑃 (cid:90) (?𝑥, 𝑝𝑟𝑒𝑑, ?𝑦)
𝑃 (cid:90) (?𝑦, 𝑝𝑟𝑒𝑑, ?𝑥)
𝑃 ⟕(?𝑥, 𝑝𝑟𝑒𝑑, ?𝑦)
𝑃 ⟕(?𝑦, 𝑝𝑟𝑒𝑑, ?𝑥)
𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙, 𝑃) (cid:90) 𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙2, 𝑃2), 𝑃2 = 𝐹 (𝑂𝐷2 )
𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙, 𝑃) ⟕ 𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙2, 𝑃2), 𝑃2 = 𝐹 (𝑂𝐷2 )
𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙2, 𝑃2) ⟕ 𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙, 𝑃), 𝑃2 = 𝐹 (𝑂𝐷2 )
(𝑃1 ⟕ 𝑃2) ∪ (𝑃2 ⟕ 𝑃1),
𝑃1 = 𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙, 𝑃), 𝑃2 = 𝐸𝑥𝑡𝑒𝑛𝑑 (𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑐𝑜𝑙2, 𝐹 (𝑂𝐷2 ))
𝐹𝑖𝑙𝑡𝑒𝑟 (𝑐𝑜𝑛𝑑𝑠, 𝑃)
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑐𝑜𝑙𝑠, 𝑃)
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠 ∪ {𝑛𝑒𝑤_𝑐𝑜𝑙 },
𝐺𝑟𝑜𝑢𝑝𝐴𝑔𝑔(𝑔𝑟𝑜𝑢𝑝_𝑐𝑜𝑙𝑠, 𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑓 𝑛, 𝑠𝑟𝑐_𝑐𝑜𝑙, 𝑃))
𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 ({𝑛𝑒𝑤_𝑐𝑜𝑙 }, 𝐺𝑟𝑜𝑢𝑝𝐴𝑔𝑔(∅, 𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑓 𝑛, 𝑐𝑜𝑙, 𝑃))

RDFFrames described in Section 4, also illustrated in Table 1. Mapping
𝑔 takes as input an RDFFrames operator 𝑜 and a SPARQL pattern 𝑃
corresponding to the operators done so far on an RDFFrame 𝐷, applies
a SPARQL operator defined by the query model generation algorithm
on the input SPARQL pattern 𝑃, and returns a new SPARQL pattern.
Using 𝑔, we define a recursive mapping 𝐹 on a sequence of RDFFrames
operators 𝑂, 𝐹 : 𝑂 → 𝑃, as:

𝐹 (𝑂) =

𝑔(𝑜1, 𝑁𝑢𝑙𝑙),

𝑔(𝑜𝑘, 𝐹 (𝑂 [1:𝑘−1] ), 𝐹 (𝑂𝐷2 )),

𝑔(𝑜𝑘, 𝐹 (𝑂 [1:𝑘−1] )),


if |𝑂 | ≤ 1.
𝑜𝑘 = 𝑗𝑜𝑖𝑛(𝐷2, . . .).
otherwise.

(1)

𝐹 returns a triple pattern for the seed operator and then builds
the rest of the SPARQL query by iterating over the RDFFrames
operators according to their order in the sequence 𝑂.

5.4 Proof of Correctness
To prove the equivalence between the SPARQL pattern solution
returned by 𝐹 and the RDFFrame generating it, we first define
the meaning of equivalence between a relational table with bag
semantics and the solution sets of SPARQL queries. First, we define a
mapping that converts SPARQL solution sets to relational tables by
letting the domains of the mappings be the columns and their ranges
be the rows. Next, we define the equivalence between solution sets
and relations.

Definition 5 (Solution Sets to Relations). Let Ω =
(𝑆Ω, 𝑐𝑎𝑟𝑑Ω) be a multiset (bag) of mappings returned by the evalua-
tion of a SPARQL pattern and 𝑉 𝑎𝑟 (Ω) = {?𝑥; ?𝑥 ∈ 𝑑𝑜𝑚(𝜇), ∀𝜇 ∈ 𝑆Ω }
be the set of variables in Ω. Let 𝐿 = 𝑜𝑟𝑑𝑒𝑟 (𝑉 𝑎𝑟 (Ω)) be the ordered
set of elements in 𝑉 𝑎𝑟 (Ω). We define a conversion function 𝜆: Ω → 𝑅,
where 𝑅 = (𝐶,𝑇 ) is a relation. R is defined such that its ordered
set of columns (attributes) are the variables in Ω (i.e., 𝐶 = 𝐿), and
𝑇 = (𝑆𝑇 , 𝑐𝑎𝑟𝑑𝑇 ) is a multiset of (tuples) of values such that for ev-
ery 𝜇 in 𝑆Ω, there is a tuple 𝜏 ∈ 𝑆𝑇 of length 𝑛 = |(𝑉 𝑎𝑟 (Ω))| and
𝜏𝑖 = 𝜇 (𝐿𝑖 ). The multiplicity function (𝑐𝑎𝑟𝑑𝑇 ) is defined such that the
multiplicity of 𝜏 is equal to the multiplicity of 𝜇 in 𝑐𝑎𝑟𝑑Ω.

10

Definition 6 (Eqivalence). A SPARQL pattern solution Ω =
(𝑆Ω, 𝑐𝑎𝑟𝑑Ω) is equivalent to a relation 𝑅 = (𝐶,𝑇 ), written(Ω ≡ 𝑅), if
and only if 𝑅 = 𝜆(Ω).

We are now ready to use this definition to present a lemma
that defines the equivalent relational tables for the main SPARQL
patterns used in our proof.

Lemma 1. If 𝑃1 and 𝑃2 are SPARQL patterns, then:
a.
b.
c.
d.
e.
f.
g.

𝐺 ) (cid:90) 𝜆(
(𝑃1 𝐽𝑜𝑖𝑛 𝑃2)
𝐺 ≡ 𝜆(
𝑃1
𝑃1
𝐺 ),
(cid:75)
(cid:75)
(cid:75)
(cid:74)
(cid:74)
(cid:74)
(𝑃1 𝐿𝑒 𝑓 𝑡 𝐽𝑜𝑖𝑛 𝑃2)
𝑃1
𝑃1
𝐺 ≡ 𝜆(
𝐺 ) ⟕ 𝜆(
𝐺 ),
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(𝑃1 𝑈 𝑛𝑖𝑜𝑛 𝑃2)
𝑃1
𝐺 ) ⟗ 𝜆(
𝑃1
𝐺 ≡ 𝜆(
𝐺 )
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(𝐸𝑥𝑡𝑒𝑛𝑑 (?𝑥, 𝐸, 𝑃))
𝑃
𝐺 ≡ 𝜌?𝑥/𝐸 (𝜆(
𝐺 )
(cid:75)
(cid:75)
(cid:74)
(cid:74)
𝐺 ≡ 𝜎𝑐𝑜𝑛𝑑𝑠 (𝜆(
(𝐹𝑖𝑙𝑡𝑒𝑟 (𝑐𝑜𝑛𝑑𝑠, 𝑃))
𝑃
𝐺 ))
(cid:75)
(cid:75)
(cid:74)
(cid:74)
𝐺 ≡ Π𝑐𝑜𝑙𝑠 (𝜆(
(𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑐𝑜𝑙𝑠, 𝑃))
𝑃
𝐺 ))
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(𝐺𝑟𝑜𝑢𝑝𝐴𝑔𝑔(∅, 𝑛𝑒𝑤_𝑐𝑜𝑙, 𝑓 𝑛, 𝑐𝑜𝑙, 𝑃)))
𝐺 ≡
(cid:74)
(cid:75)
𝛾𝑐𝑜𝑙𝑠,𝑓 𝑛 (𝑐𝑜𝑙)↦→𝑛𝑒𝑤_𝑐𝑜𝑙 (𝜆(

𝑃
(cid:74)

𝐺 ))
(cid:75)

Proof. The proof of this lemma follows from (1) the semantics
of SPARQL operators presented in Section 5.2, (2) the well-known
semantics of relational operators, (3) Definition 5 which specifies
the function 𝜆, and (4) Definition 6 which defines the equivalence
between multisets of mappings and relations. For each statement
in the lemma, we use the definition of the function 𝜆, the relational
operator semantics, and the SPARQL operator semantics to define
the relation on the right side. Then we use the definition of SPARQL
operators semantic to define the multiset on the left side. Finally,
□
Definition 6 proves the statement.

Finally, we present the main theorem in this section, which guar-
antees the semantic correctness of the RDFFrames query generation
algorithm.

Theorem 1. Given a graph 𝐺, every RDFFrame 𝐷 that is returned
by a sequence of RDFFrames operators 𝑂𝐷 = [𝑜1, . . . , 𝑜𝑘 ] on 𝐺 is
equivalent to the evaluation of the SPARQL pattern 𝑃 = 𝐹 (𝑂𝐷 ) on G.
In other words, 𝐷 ≡

𝐹 (𝑂𝐷 )
(cid:74)

𝐺 .
(cid:75)

𝐹 (𝑂𝐷 )
(cid:74)

Proof. We prove that 𝐷 ≡ 𝜆(

𝐺 ) via structural induc-
𝐹 (𝑂𝐷 )
(cid:75)
(cid:74)
tion on non-empty RDFFrame 𝐷. For simplicity, we denote the
proposition 𝐷 ≡
Base case: Let 𝐷 be an RDFFrame created by one RDFFrames oper-
ator 𝑂𝐷 = [𝑠𝑒𝑒𝑑 (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3)]. The first operator (and the only
one in this case) has to be the 𝑠𝑒𝑒𝑑 operator since it is the only
operator that takes only a knowledge graph as input and returns
an RDFFrame. From Table 1:

𝐺 as 𝐴(𝐷).
(cid:75)

𝐹 (𝑂𝐷 ) = 𝑔(𝑠𝑒𝑒𝑑 (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3), 𝑁𝑢𝑙𝑙)
= 𝑃𝑟𝑜 𝑗𝑒𝑐𝑡 (𝑉 𝑎𝑟 (𝑡), 𝑡)

(cid:74)

(𝑡)

where 𝑡 = (𝑐𝑜𝑙1, 𝑐𝑜𝑙2, 𝑐𝑜𝑙3). By definition of the RDFFrames op-
erators in Section 3, 𝐷 = Π𝑋 ∩{𝑐𝑜𝑙1,𝑐𝑜𝑙2,𝑐𝑜𝑙3 } (𝜆(
𝐺 )) and by
(cid:75)
Lemma 1(f), 𝐴(𝐷) holds.
Induction hypothesis: Every RDFFrames operator takes as input one
or two RDFFrames 𝐷1, 𝐷2 and outputs an RDFFrame 𝐷. Without
loss of generality, assume that both 𝐷1 and 𝐷2 are non-empty and
𝐺 .
𝐴(𝐷1) and 𝐴(𝐷2) hold, i.e., 𝐷1 ≡
𝐹 (𝑂𝐷1 )
(cid:75)
(cid:74)
Induction step: Let 𝐷 = 𝐷1.𝑂𝑝 (optional 𝐷2), 𝑃1 = 𝐹 (𝑂𝐷1 ), and 𝑃2 =
𝐹 (𝑂𝐷2 ). We use RDFFrames semantics to define 𝐷, the mapping 𝐹
to define the new pattern 𝑃, then Lemma 1 to prove the equivalence
between 𝐹 and 𝐷. We present the different cases next.
• If 𝑂𝑝 is 𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑜𝑢𝑡, 𝑓 𝑎𝑙𝑠𝑒) then: 𝐷 = 𝐷1 (cid:90) 𝜆(

𝐺 )
(cid:75)
according to the definition of the operator in Section 3.2 and
Table 1, where 𝑡 is the triple pattern (?𝑥, 𝑝𝑟𝑒𝑑, ?𝑦). By the
induction hypothesis, it holds that 𝐷1 = 𝜆(
𝐺 ). Thus, it
𝑃1)
(cid:75)
(cid:74)
𝐺 ) and by Lemma 1(a), 𝐴(𝐷)
holds that 𝐷 = 𝜆(
𝑡
(cid:75)
(cid:74)
holds. The same holds when 𝑂𝑝 is 𝑒𝑥𝑝𝑎𝑛𝑑 (𝑥, 𝑝𝑟𝑒𝑑, 𝑦, 𝑖𝑛, 𝑓 𝑎𝑙𝑠𝑒)
except that 𝑡 = (?𝑦, 𝑝𝑟𝑒𝑑, ?𝑥).

𝐺 and 𝐷2 ≡
(cid:75)

𝐺 ) (cid:90) 𝜆(
(cid:75)

𝐹 (𝑂𝐷2 )
(cid:74)

𝑃1
(cid:74)

𝑡
(cid:74)

is

𝜆(

𝜆(

• If 𝑂𝑝

𝑗𝑜𝑖𝑛(𝐷2, 𝑐𝑜𝑙, 𝑐𝑜𝑙2, (cid:90), 𝑛𝑒𝑤_𝑐𝑜𝑙)

𝐺 ) and 𝐷2
(cid:75)
𝑃1
(cid:74)

𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙2 (𝐷2),
𝑃2
=
(cid:74)
𝐺 )) (cid:90) 𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙2 (𝜆(
(cid:75)

then: 𝐷
=
(cid:90)
by
and
𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙 (𝐷1)
𝐺 ). Thus,
𝐴(𝐷1), 𝐷1
𝑃1
=
(cid:75)
(cid:74)
𝐷 = 𝜌𝑛𝑒𝑤_𝑐𝑜𝑙/𝑐𝑜𝑙 𝜆(
𝑃2
𝐺 ))
(cid:75)
(cid:74)
and by Lemma 1(a,c), 𝐴(𝐷) holds. The same argument holds
for other types of join, using the relevant parts of Lemma 1.
• If 𝑂𝑝 is 𝑓 𝑖𝑙𝑡𝑒𝑟 (𝑐𝑜𝑛𝑑𝑠 = [𝑐𝑜𝑛𝑑1 ∧ 𝑐𝑜𝑛𝑑2 ∧ · · · ∧ 𝑐𝑜𝑛𝑑𝑘 ]) then:
𝐺 ). So, 𝐷 =
𝑃1
(cid:75)
(cid:74)
𝐺 )) and by Lemma 1(e), 𝐴(𝐷) holds.
(cid:75)
• If 𝑂𝑝 is 𝑔𝑟𝑜𝑢𝑝𝑏𝑦 (𝑐𝑜𝑙𝑠).𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛(𝑓 , 𝑐𝑜𝑙, 𝑛𝑒𝑤_𝑐𝑜𝑙) then: 𝐷 =
𝐺 ). So,
(cid:75)
𝐺 )) and by Lemma 1(f,g), 𝐴(𝐷)
(cid:75)

𝛾𝑐𝑜𝑙𝑠,𝑓 (𝑐𝑜𝑙)↦→𝑛𝑒𝑤_𝑐𝑜𝑙 (𝐷1), and by 𝐴(𝐷1), 𝐷1 = 𝜆(
𝐷 = 𝛾𝑐𝑜𝑙𝑠,𝑓 (𝑐𝑜𝑙)↦→𝑛𝑒𝑤_𝑐𝑜𝑙 𝜆(
holds.

𝐷 = 𝜎𝑐𝑜𝑛𝑑𝑠 (𝐷1), and by 𝐴(𝐷1), 𝐷1 = 𝜆(
𝜎𝑐𝑜𝑛𝑑𝑠𝜆(

𝑃1
(cid:74)

𝑃1
(cid:74)

𝑃1
(cid:74)

Thus, 𝐴(𝐷) holds in all cases.

□

6 EVALUATION
We present an experimental evaluation of RDFFrames in which
our goal is to answer two questions: (1) How effective are the
design decisions made in RDFFrames? and (2) How does RDFFrames
perform compared to alternative baselines?

We use two workloads for this experimental study. The first is
made up of three case studies consisting of machine learning tasks
on two real-world knowledge graphs. Each task starts with a data
preparation step that extracts a pandas dataframe from the knowl-
edge graph. This step is the focus of the case studies. In the next
section, we present the RDFFrames code for each case study and the
corresponding SPARQL query generated by RDFFrames. As in our

11

motivating example, we will see that the SPARQL queries are longer
and more complex than the RDFFrames code, thereby showing that
RDFFrames can indeed simplify access to knowledge graphs. The
full Python code for the case studies can be found in Appendix A.
The second workload in our experiments is a synthetic workload
consisting of 16 queries. These queries are designed to exercise
different features of RDFFrames for the purpose of benchmarking.
We describe the two workloads next, followed by the experimental
setup and the results.

6.1 Case Studies
6.1.1 Movie Genre Classification. Classification is a basic super-
vised machine learning task. This case study applies a classification
task on movie data extracted from the DBpedia knowledge graph.
Many knowledge graphs, including DBpedia, are heterogeneous,
with information about diverse topics, so extracting a topic-focused
dataframe for classification is challenging.

This task uses RDFFrames to build a dataframe of movies from
DBpedia, along with a set of movie attributes that can be used for
movie genre classification. The task bears some similarity to the
code in Listing 1. Let us say that the classification dataset that we
want includes movies that star American actors (since they are
assumed to have a global reach) or prolific actors (defined as those
who have starred in 100 or more movies). We want the movies
starring these actors, and for each movie, we extract the movie
name (i.e., title), actor name, topic, country of production, and
genre. Genre is not always available so it is an optional predicate.
The full code for this data preparation step is shown in Listing 6, and
the SPARQL query generated by RDFFrames is shown in Listing 7.
The extracted dataframe can be used as a classification dataset
by any popular Python machine learning library. The movies that
have the genre available in the dataframe can be used as labeled
training data to train a classifier. The features for this classifier
are the attributes of the movies and the actors, and the classifier
is trained to predict the genre of a movie based on these features.
The classifier can then be used to predict the genres of all movies
that are missing the genre.

Note that the focus of RDFFrames is the data preparation step of
a machine learning pipeline (i.e., creating the dataframe). That is,
RDFFrames addresses the following problem: Most machine learn-
ing pipelines require as their starting point an input dataframe, and
there is no easy way to get such a dataframe from a knowledge
graph while leveraging an RDF engine. Thus, the focus of RDF-
Frames is enabling the user to obtain a dataframe from an RDF en-
gine, and not how the machine learning pipeline uses this dataframe.
Nevertheless, it is interesting to see this dataframe within an end-
to-end machine learning pipeline. Specifically, for the current case
study, can the dataframe created by RDFFrames be used for movie
genre classification? We emphasize that the accuracy of the clas-
sifier is not our main concern here; our concern is demonstrating
RDFFrames in an end-to-end machine learning pipeline. Issues such
as using a complex classifier, studying feature importance, or ana-
lyzing the distribution of the retrieved data are beyond the scope
of RDFFrames.

To show RDFFrames in an end-to-end machine learning pipeline,
we built a classifier based on the output of Listing 6 to classify
the six most frequent movie genres, specifically, drama, sitcom,

science fiction, legal drama, comedy, and fantasy. The classification
dataset consisted of 7,635 movies that represent the English movies
in these six movie genres. We trained a random forest classifier
using the scikit-learn machine learning library based on movie
features such as actor country, movie country, subject, and actor
name. This classifier achieved 92.4% accuracy on evaluation data
that is 30% of the classification dataset.

We performed a similar experiment on song data from DBpedia.
We extracted 27,956 triples of English songs in DBpedia along with
their features such as album, writer, title, artist, producer, album
title, and studio. We used the same methodology as in the movie
genre classification case study to classify songs into genres such as
alternative rock, hip hop, indie rock, and pop-punk. The accuracy
achieved by a random forest classifier in this case was 70.9%.

movies = graph . feature_domain_range ( ' dbpp : starring ' , ' movie ' , ' actor ')
movies = movies . expand ( ' actor ' ,[( ' dbpp : birthPlace ',

' actor_country '), ( ' rdfs : label ', ' actor_name ' )])\

. expand ( ' movie ', [( ' rdfs : label ' , ' movie_name '),
( ' dcterms : subject ' , ' subject '),
( ' dbpp : country ', ' movie_country '),
( ' dbpo : genre ' , ' genre ', Optional )]). cache ()

american = movies . filter ({ ' actor_country ' :\

[ '= dbpr : UnitedStates ' ]})

prolific = movies . group_by ([ ' actor ' ])\

.count ( ' movie ' , ' movie_count ', unique = True )\
. filter ({ ' movie_count ': [ ' >=100 ' ]})

dataset = american . join ( prolific , ' actor ', OuterJoin )\

. join ( movies , ' actor ' , InnerJoin )

Listing 6: RDFFrames code - Movie genre classification.
SELECT DISTINCT

? actor_name ? movie_name ? actor_country ? genre ?

subject

? actor .
? movie_name .

dbpp : starring
rdfs : label

FROM < http :// dbpedia . org >
WHERE
{ ? movie
? movie
? movie dcterms : subject
? actor dbpp : birthPlace
? actor rdfs : label ? actor_name
OPTIONAL
{ ? movie
{{ SELECT

dbpp : genre
* WHERE

? genre }

? subject .
? actor_country .

{{ SELECT

* WHERE

{ ? movie

dbpp : starring

? actor .

? movie_name .

? movie rdfs : label
? movie dcterms : subject
? actor dbpp : birthPlace
? actor
FILTER regex ( str (? actor_country ) , " USA " )
OPTIONAL

? subject .
? actor_country .

? actor_name

rdfs : label

{ ? movie

dbpp : genre ? genre }

}

}
OPTIONAL

{ SELECT DISTINCT ? actor ( COUNT ( DISTINCT ? movie ) AS ?

movie_count )

WHERE

{ ? movie
? movie
? movie
? actor
? actor
OPTIONAL

dbpp : starring ? actor .

? movie_name .

rdfs : label
dcterms : subject ? subject .
dbpp : birthPlace ? actor_country .
rdfs : label ? actor_name

{ ? movie

dbpp : genre ? genre }

}
GROUP BY ? actor
HAVING ( COUNT ( DISTINCT ? movie ) >= 100 )

}

}

}

UNION
{ SELECT

* WHERE

{{ SELECT DISTINCT ? actor ( COUNT ( DISTINCT ? movie ) AS ?

movie_count ) WHERE

{ ? movie
? movie
? movie
? actor
? actor
OPTIONAL

? movie_name .

dbpp : starring ? actor .
rdfs : label
dcterms : subject ? subject .
dbpp : birthPlace ? actor_country .
rdfs : label ? actor_name

{ ? movie

dbpp : genre

? genre }

}
GROUP BY ? actor
HAVING ( COUNT ( DISTINCT ? movie ) >= 100 )
}
OPTIONAL
{ SELECT

* WHERE

? movie_name .

dbpp : starring ? actor .
rdfs : label
dcterms : subject ? subject .
dbpp : birthPlace ? actor_country .
rdfs : label ? actor_name

{ ? movie
? movie
? movie
? actor
? actor
FILTER regex ( str (? actor_country ) , " USA ")
OPTIONAL

{ ? movie

dbpp : genre ? genre }

}

}

}

}

}

}

Listing 7: SPARQL query generated by RDFFrames for the
code shown in Listing 6.
6.1.2 Topic Modeling. Topic modeling is a statistical technique
commonly used to identify hidden contextual topics in the text. In
this case study, we use topic modeling to identify the active topics of
research in the database community. We define these as the topics
of recent papers published by authors who have published many
SIGMOD and VLDB papers. This is clearly an artificial definition,
but it enables us to study the capabilities and performance of RDF-
Frames. As stated earlier, we are focused on data preparation not
the details of the machine learning task.

papers = graph . entities ( ' swrc : InProceedings ' ,' paper ')
papers = papers . expand ( ' paper ' ,[( ' dc : creator ' ,\
' author ') , ( ' dcterm : issued ' , ' date ') ,\
( ' swrc : series ', ' conference ') ,\

( ' dc : title ' , ' title ') ]) . cache ()

authors = papers . filter ({ ' date ': [ ' >=2000 '],
' conference ': [ 'In ( dblp : vldb ,␣ dblp : sigmod ) ' ]})

. group_by ([ ' author ' ]) .count ( ' paper ' , ' n_papers ')\
. filter ({ ' n_papers ': ' >=20 ', ' date ': [ ' >=2010 ' ]}) \

titles = papers . join ( authors , ' author ', InnerJoin )\

. select_cols ([ ' title ' ])

Listing 8: RDFFrames code - Topic modeling.

? title

SELECT
FROM < http :// dblp . l3s .de >
WHERE

{ ? paper

dc : title
rdf : type
dcterm : issued
dc : creator

? title ;
swrc : InProceedings ;
? date ;
? author

FILTER ( year ( xsd : dateTime (? date )) >= 2005 )
{ SELECT
WHERE

? author

{ ? paper

rdf : type
swrc : series
dc : creator
dcterm : issued

swrc : InProceedings ;
? conference ;
? author ;
? date

FILTER ( ( year ( xsd : dateTime (? date )) >= 2005 )

&& ( ? conference IN ( dblprc : vldb , dblprc : sigmod ) ) )

}

GROUP BY ? author
HAVING ( COUNT (? paper ) >= 20 )

}

12

}

Listing 9: SPARQL query generated by RDFFrames for the
code shown in Listing 8.

The dataframe required for this task is extracted from the DBLP
knowledge graph represented in RDF through the sequence of
RDFFrames operators shown in Listing 8. First, we identify the
authors who have published 20 or more papers in SIGMOD and
VLDB since the year 2000, which requires using the RDFFrames
grouping, aggregation, and filtering capabilities. For the purpose of
this case study, these are considered the thought leaders of the field
of databases. Next, we find the titles of all papers published by these
authors since 2010. The SPARQL query generated by RDFFrames is
shown in Listing 9.

We then run topic modeling on the titles to identify the topics of
the papers, which we consider to be the active topics of database
research. We use off-the-shelf components from the rich ecosystem
of pandas libraries to implement topic modeling (see Appendix A).
Specifically, we use NLP libraries for stop-word removal and scikit-
learn for topic modeling using SVD. This shows the benefit of using
RDFFrames to get data into a pandas dataframe with a few lines
of code, since one can then utilize components from the PyData
ecosystem.
6.1.3 Knowledge Graph Embedding. Knowledge graph embeddings
are widely used relational learning models, and they are state of
the art on benchmark datasets for link prediction and fact classifi-
cation [43, 44]. The input to these models is a dataframe of triples,
i.e., a table of three columns: [𝑠𝑢𝑏 𝑗𝑒𝑐𝑡, 𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒, 𝑜𝑏 𝑗𝑒𝑐𝑡] where the
𝑜𝑏 𝑗𝑒𝑐𝑡 is a URI representing an entity (i.e., not a literal). Currently,
knowledge graph embeddings are typically evaluated only on small
pre-processed subsets of knowledge graphs like FB15K [7] and
WN18 [7] rather than the full knowledge graphs, and thus, the va-
lidity of their performance results has been questioned recently in
multiple papers [11, 37]. Filtering the knowledge graph to contain
only entity-to-entity triples and loading the result in a dataframe is
a necessary first step in constructing knowledge graph embedding
models on full knowledge graphs. RDFFrames can perform this
first step using one line of code as shown in Listing 10 (generated
SPARQL in Listing 11). With this line of code, the filtering can be
performed efficiently in an RDF engine, and RDFFrames handles
issues related to communication with the engine and integrating
with PyData. These issues become important, especially when deal-
ing with large knowledge graphs where the resulting dataframe
has millions of rows.

graph . feature_domain_range (s , p , o)\

. filter ({ o: [ ' isURI ' ]})

Listing 10: RDFFrames code - Knowledge graph embedding.
SELECT *
FROM < http :// dblp .13 s. de />
WHERE {

? sub ? pred ? obj .
FILTER ( isIRI (? obj ) )

}

Listing 11: SPARQL query corresponding to RDFFrames
code shown in Listing 10.

evaluation of the framework. To this end, we created a synthetic
workload consisting of 16 queries written in RDFFrames that exer-
cise different capabilities of the framework. All the queries are on
the DBpedia knowledge graph, and two queries join DBpedia with
the YAGO knowledge graph. One query joins the three knowledge
graphs DBpedia, YAGO, and DBLP. Four of the queries use only
expand and filter (up to 10 expands, including some with optional
predicates). Four of the queries use grouping with expand (includ-
ing one with expand after the grouping). Eight of the queries use
joins, including complex queries that exercise features such as outer
join, multiple joins, joins between different graphs, and joins on
grouped datasets. A description of the queries and the RDFFrames
features and SPARQL capabilities that they exercise can be found
in Appendix B.

6.3 Experimental Setup
6.3.1 Dataset Details. The three knowledge graphs used in the
evaluation have different sizes and statistical features. The first is
the English version of the DBpedia knowledge graph. We extracted
the December 2020 core collection from DBpedia Databus.19 The
collection contains 6 billion triples. The second is the DBLP com-
puter science bibliography dataset (2017 version) containing 88
million triples.20 The third (used in three queries in the synthetic
workload) is YAGO version 3.1, containing 1.6 billion triples. DBLP
is relatively small, structured and dense, while DBpedia and YAGO
are heterogeneous and sparse.

6.3.2 Hardware and Software Configuration. We use an Ubuntu
server with 128GB of memory to run a Virtuoso OpenLink Server
(version 7.2.6-rc1.3230-pthreads as of Jan 9 2019) with its default
configuration. We load the DBpedia, DBLP, and YAGO knowledge
graphs to the Virtuoso server. RDFFrames connects to the server
to process SPARQL queries over HTTP using SPARQLWrapper,21
a Python library that provides a wrapper for SPARQL endpoints.
Recall that the decision to communicate with the server over HTTP
rather than the cursor mechanism of Virtuoso was made to ensure
maximum generality and flexibility. When sending SPARQL queries
directly to the server, we use the curl tool. The client always runs
on a separate core of the same machine as the Virtuoso server so
we do not incur communication overhead. In all experiments, we
report the average running time of three runs.

6.3.3 Alternatives Compared. Our goal is to evaluate the design
decisions of RDFFrames and to compare it against alternative base-
lines. To evaluate the design decisions of RDFFrames, we ask two
questions: (1) How important is it to generate optimized SPARQL
queries rather than using a simple query generation approach?
and (2) How important is it to push the processing of relational
operators into the RDF engine? Both of these design choices are
clearly beneficial and the goal is to quantify the benefit.

To answer the first question, we compare RDFFrames against an
alternative that uses a naive query generation strategy. Specifically,

6.2 Synthetic Workload
While the case studies in the previous section show RDFFrames in
real applications, it is still desirable to have a more comprehensive

19https://databus.dbpedia.org/dbpedia/collections/latest-core
20http://www.rdfhdt.org/datasets
21https://rdflib.github.io/sparqlwrapper

13

for each API call to RDFFrames, we generate a subquery that con-
tains the pattern corresponding to that API call and we finally join
all the subqueries in one level of nesting with one outer query. For
example, each call to an expand creates a new subquery containing
one triple pattern described by the expand operator. We refer to
this alternative as Naive Query Generation. The naive queries
for the first two case studies are shown in Appendices C and D.
The SPARQL query for the third case study is simple enough that
Listing 11 is also the naive query.

To answer the second question, we compare to an alternative that
uses RDFFrames (with optimized query generation) only for graph
navigation using the 𝑠𝑒𝑒𝑑 and 𝑒𝑥𝑝𝑎𝑛𝑑 operators, and performs any
relational-style processing in pandas. We refer to this alternative
as Navigation + pandas.

If we do not use RDFFrames, we can envision three alternatives
for pre-processing the data and loading it into a dataframe, and we
compare against all three:
• Do not use an RDF engine at all, but rather write an ad-hoc
script that runs on the knowledge graph stored in some RDF
serialization format. To implement this solution we write scripts
using the rdflib library22 to load the RDF dataset into pandas,
and use pandas operators for any additional processing. The
rdflib library can process any RDF serialization format, and in
our case the data was stored in the N-Triples format. We refer
to this alternative as rdflib + pandas.

• Use an RDF engine, and use a simple SPARQL query to load
the RDF dataset into a dataframe. Use pandas for additional
processing. This is a variant of the first alternative but it uses
SPARQL instead of rdflib. The advantage is that the required
SPARQL is very simple, but still benefits from the processing
capabilities of the RDF engine. We refer to this alternative as
SPARQL + pandas.

• Use a SPARQL query written by an expert (in this case, the
authors of the paper) to do all the pre-processing inside the RDF
engine and output the result to a dataframe. This alternative
takes full advantage of the capabilities of the RDF engine, but
suffers from the “impedance mismatch” described in the intro-
duction: SPARQL uses a different programming style compared
to machine learning tools and requires expertise to write, and
additional code is required to export the data into a dataframe.
We refer to this alternative as Expert SPARQL.
We verify that the results of all alternatives are identical. Note
that RDFFrames, Naive Query Generation, and Expert SPARQL
generate semantically equivalent SPARQL queries. The query opti-
mizer of an RDF engine should be able to produce query execution
plans for all three queries that are identical or at least have sim-
ilar execution cost. We will see that Virtuoso, being an industry-
strength RDF engine, does indeed deliver the same performance
for all three queries in many cases. However, we will also see that
there are cases where this is not true, which is expected due to the
complexity of optimizing SPARQL queries.

6.4 Results on Case Studies
6.4.1 Evaluating the design decisions of RDFFrames. Figure 3 shows
the running time of Naive Query Generation, Navigation +
pandas, and RDFFrames on the three case studies.

22https://github.com/RDFLib/rdflib

14

Movie Genre Classification. This task requires heavy processing
on the DBpedia dataset and returns a dataframe of 19,633 movies.
The results are presented in Figure 3(a).

The running time of RDFFrames is 687.96 seconds. Of this time,
less than 5 milliseconds is spent on preparing the SPARQL query
(i.e., recording the RDFFrames operations, generating the query
model, and producing the query). The remaining time is spent on
issuing the query to the engine and retrieving the results. This is
typical in all our experiments: RDFFrames needs a few millisec-
onds to generate the SPARQL query and the remaining time is
spent on query processing. The query produced by naive query
generation did not finish in one hour and we terminated it after
this time, which demonstrates the need for RDFFrames to generate
optimized SPARQL and not rely exclusively on the query optimizer.
The Navigation + pandas alternative also timed out after one
hour, which demonstrates the need for pushing computation into
the engine.

Topic Modeling. This task requires heavy processing on the DBLP
dataset and returns a dataframe of 4,209 titles. The results are de-
picted in Figure 3(b). Naive query generation did well here, with
the query optimizer generating a good plan for the query Nonethe-
less, naive query generation is 2x slower than RDFFrames. This
further demonstrates the need for generating optimized SPARQL.
The Navigation + pandas alternative here was particularly bad,
reinforcing the need to push computation into the engine.

Knowledge Graph Embedding. This task keeps only triples where
the object is an entity (i.e., not a literal). It does not require heavy
processing but requires handling the scalability issues of returning
a huge final dataframe with all triples of interest. The results on
DBLP are shown in Figure 3(c). All the alternatives have similar per-
formance for this task, since the required SPARQL query is simple
and processed well by Virtuoso, and since there is no processing
required in pandas.
6.4.2 Comparing RDFFrames With Alternative Baselines. Figure 4
compares the running time of RDFFrames on the three case studies
to the three alternative baselines: rdflib + pandas, SPARQL +
pandas, and Expert SPARQL.

Movie Genre Classification. Both the rdflib + pandas and
SPARQL + pandas baselines crashed after more than one hour due
to scalability issues, showing that they are not viable alternatives.
On the other hand, RDFFrames and Expert SPARQL have similar
performance. This shows that RDFFrames does not add overhead
and is able to match the performance of an expert-written SPARQL
query, which is the best case for an automatic query generator.
Thus, the flexibility and usability of RDFFrames does not come at
the cost of reduced performance.

Topic Modeling. The baselines that perform computation in pan-
das did not crash as before, but are orders of magnitude slower than
RDFFrames and Expert SPARQL. In this case as well, the running
time of RDFFrames matches the expert-written SPARQL query.

Knowledge Graph Embedding. In this experiment, rdflib +
pandas is 3x slower than RDFFrames and SPARQL + pandas is 2x
slower than RDFFrames, while RDFFrames has the same perfor-
mance as Expert SPARQL. These results reinforce the conclusions
drawn earlier.

(a)

(b)

(c)

Figure 3: Evaluating the design of RDFFrames.

(a)

(b)

(c)

Figure 4: Comparing RDFFrames to alternative baselines.

quality of the queries generated by RDFFrames and whether this
broad set of queries shows that naive query generation would work
well. Figure 5 compares naive query generation and RDFFrames to
expert-written SPARQL.

The y-axis shows the ratio between the running time of naive
query generation and expert-written SPARQL, and between the
running time of RDFFrames and expert-written SPARQL. Thus,
expert-written SPARQL is considered the gold standard and the
figure shows how well the two query generation alternatives match
this standard. To improve the comparison, the absolute running
time of expert-written SPARQL in seconds is shown under each
query on the x-axis. The queries are sorted in ascending order by
the ratio of naive query generation to expert-written SPARQL (the
blue bars in the figure). The dashed horizontal line represents a
ratio of 1.

The ratios for RDFFrames range between 0.99 and 1.04, which
shows that RDFFrames is good at generating queries that match
the performance of expert-written queries. On the other hand, the
ratios for naive query generation vary widely. The first six queries
have ratios between 1.01 and 1.14. For these queries, the Virtuoso
optimizer does a good job of generating a close-to-optimal plan
for the naive query. The next six queries have ratios between 1.24
and 4.56. Here, we are seeing the weakness of naive query genera-
tion and the need for the optimizations performed by RDFFrames
during query generation. The situation is worse for the last four
queries: naive query generation is an order of magnitude slower

Figure 5: Results on the synthetic workload.

6.5 Results on Synthetic Workload
In this experiment, we use the synthetic workload of 16 queries to
do a more comprehensive evaluation of RDFFrames. The previous
section showed that Navigation + pandas, rdflib + pandas,
and SPARQL + pandas are not competitive with RDFFrames. Thus,
we exclude them from this experiment. Instead, we focus on the

15

Movie Genre Classification on DBpedia  05001000150020002500Time (secs)>3600>3600    687.96Naive Query GenerationNavigation + pandasRDFFramesTopic Modeling on DBLP            0123456Time (secs)      0.24    359.70      0.10Naive Query GenerationNavigation + pandasRDFFrames   KG Embedding on DBLP0200040006000800010000Time (secs)   3219.38   3240.79   3246.14Naive Query GenerationNavigation + pandasRDFFrames   Movie Genre Classification on DBpedia025050075010001250150017502000Time (secs)>3600(crashed)>3600(crashed)    685.25    687.96rdflib + pandasSPARQL + pandasExpert SPARQLRDFFrames   Topic Modeling on DBLP01234Time (secs)   9418.94   4397.36      0.10      0.10rdflib + pandasSPARQL + pandasExpert SPARQLRDFFrames            KG Embedding on DBLP0200040006000800010000Time (secs)  11126.65   6244.43   3219.38   3246.14rdflib + pandasSPARQL + pandasExpert SPARQLRDFFramesQ150.9|Q24.44|Q31.63|Q44.35|Q50.76|Q63.25|Q73.22|Q80.24|Q91.82|Q100.78|Q110.96|Q122.29|Q130.53|Q140.63|Q1545.7|Q1639.2|02468101214Time - Ratio to Expert SPARQLTimeoutTimeoutNaive SPARQLRDFFramesfor two queries and the last two queries time out after one hour.
Thus, the results on this more comprehensive workload validate
the quality of the queries generated by RDFFrames and the need
for its sophisticated query generation algorithm.

6.6 Effect of Operator Complexity
In our final experiment, we study how the complexity of various
RDFFrames operators affects performance. Unlike the previous
experiment, in which we varied the complexity of large queries
as indicated in Appendix B, this experiment studies the issue of
complexity at the granularity of an operator. The cost of operators
is highly dependent on the RDF engine being used (Virtuoso in
our case). Nevertheless, we want to see if there are any patterns in
performance.

To study the effect of operator complexity, we create four RDF-
Frames queries of increasing complexity that operate on movies in
the DBpedia knowledge graph. The performance of these queries is
shown in Table 2. The first query counts the number of movies. We
find movies by finding entities that are the subject of a ‘starring’
predicate. We then use the 𝑒𝑥𝑝𝑎𝑛𝑑 RDFFrames operator to get the
movie titles and apply the 𝑐𝑜𝑢𝑛𝑡 aggregation function on these
titles. The second query uses the 𝑠𝑒𝑙𝑒𝑐𝑡 RDFFrames operator to re-
trieve all movie titles. The third query uses the 𝑔𝑟𝑜𝑢𝑝_𝑏𝑦 operator
to group movies by genre and counts the number of movies in each
genre. The fourth query is a join query that finds actors who are
also movie directors. This query creates a dataset of actors and a
dataset of directors, and then joins the two datasets through an
inner 𝑗𝑜𝑖𝑛 operator on actor/director name.

We ran the four queries with 𝑓 𝑖𝑙𝑡𝑒𝑟 operators of varying selec-
tivity. In one case, we had no filter (i.e., we ran the query on all
movies). In the second case, we had a filter specifying that movie
genre has to be one of sitcom, drama, or comedy. In the third case,
the filter specified that movie genre has to be sitcom (the most
selective filter).

Each row in Table 2 represents a filter, and the number of movies
retrieved by this filter is presented in the second column. The rows
of the table are sorted by this column. The next four columns in
each row show the running time of the four queries for this filter. In
all cases, the time that RDFFrames spends to generate the SPARQL
query is less than one millisecond, so we do not report it separately.
Looking at each running time column from top to bottom, we see
that the bigger the input data, the more time is required. Looking
at each row from left to right, we see that the more complex the
query, the more time is required. Both of these results are expected.
Another observation about Table 2 is that the variation in running
times is not excessive. Even the most expensive query, the join
query with no filter, which joins a dataset of 31221 actors with a
dataset of 1784 directors, runs in a reasonable 77.896 seconds.

Thus, the experiment shows that operator complexity and dataset
size do have an effect on performance. The observed effect is in-line
with expectations and does not affect the usability of RDFFrames.
The robust performance we observe is partly due to the robust-
ness of Virtuoso and partly due to our process for SPARQL query
generation.

7 CONCLUSION
We presented RDFFrames, a framework for seamlessly integrating
knowledge graphs into machine learning applications. RDFFrames
is based on a number of powerful operators for graph navigation
and relational processing that enable users to generate tabular
data sets from knowledge graphs using procedural programming
idioms that are familiar in machine learning environments such
as PyData. RDFFrames automatically converts these procedural
calls to optimized SPARQL queries and manages the execution of
these queries on a local RDF engine or a remote SPARQL endpoint,
shielding the user from all details of SPARQL query execution. We
provide a Python implementation of RDFFrames that is tightly
integrated with the pandas library and experimentally demonstrate
its efficiency.

Directions for future work include conducting a comprehensive
user study to identify and resolve any usability-related issues that
could be faced by RDFFrames users. A big problem in RDF is that
users need to know the knowledge graph vocabulary and struc-
ture in order to effectively query it. To address this problem, one
direction for future work is expanding the exploration operators
of RDFFrames to include keyword search. Testing and evaluating
RDFFrames on multiple RDF engines is another possible future
direction.

REFERENCES
[1] Daniel Abadi et al. 2019. The Seattle Report on Database Research. SIGMOD

Record 48, 4 (2019), 44–53.

[2] Pulkit Agrawal et al. 2019. Data Platform for Machine Learning. In SIGMOD.
[3] Mehdi Ali et al. 2020. PyKEEN 1.0: A Python Library for Training and Evaluating

Knowledge Graph Emebddings. arXiv preprint arXiv:2007.14175 (2020).

[4] Renzo Angles et al. 2018. G-CORE: A Core for Future Graph Query Languages.

In SIGMOD.

[5] Denis Baylor et al. 2017. TFX: A TensorFlow-Based Production-Scale Machine

Learning Platform. In SIGKDD.

[6] François Belleau, Marc-Alexandre Nolin, Nicole Tourigny, Philippe Rigault, and
Jean Morissette. 2008. Bio2RDF: Towards a Mashup to Build Bioinformatics
Knowledge Systems. Journal of Biomedical Informatics 41 (2008).

[7] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational
Data. In NIPS.

[8] Luca Costabello et al. 2019. AmpliGraph: A Library for Representation Learning

on Knowledge Graphs. https://doi.org/10.5281/zenodo.2595043

[9] Tamraparni Dasu and Theodore Johnson. 2003. Exploratory Data Mining and

Data Cleaning. Vol. 479. John Wiley & Sons.

[10] Randall Davis, Howard Shrobe, and Peter Szolovits. 1993. What is a knowledge

representation? AI Magazine 14 (1993), 17–17.

[11] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.

Convolutional 2d Knowledge Graph Embeddings. In AAAI.

[12] AnHai Doan. 2018. Human-in-the-Loop Data Analysis: A Personal Perspective.

In Proc. Workshop on Human-In-the-Loop Data Analytics (HILDA).

[13] Xin Luna Dong. 2018. Challenges and Innovations in Building a Product Knowl-

edge Graph. In SIGKDD.

[14] Joseph Vinish D’silva, Florestan De Moor, and Bettina Kemme. 2018. AIDA
- Abstraction for Advanced In-Database Analytics. Proceedings of the VLDB
Endowment (PVLDB) 11, 11 (2018), 1400–1413.

[15] Hector Garcia-Molina, Jeffrey D. Ullman, and Jennifer Widom. 2008. Database

Systems: The Complete Book (second ed.). Pearson.

[16] C. ˜Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998. CiteSeer: An Automatic

Citation Indexing System. In ACM DL.

[17] Yash Govind et al. 2019. Entity Matching Meets Data Science: A Progress Report

from the Magellan Project. In SIGMOD.

[18] Peter Haase, Jeen Broekstra, Andreas Eberhart, and Raphael Volz. 2004. A Com-

parison of RDF Query Languages. In ISWC.

[19] Stefan Hagedorn, Steffen Kläbe, and Kai-Uwe Sattler. 2021. Putting Pandas in a

Box. In CIDR.

[20] Xu Han et al. 2018. OpenKE: An Open Toolkit for Knowledge Embedding. In

EMNLP.

16

Table 2: Running time with varying operator complexity.

filter (on genre)
Sitcom
Sitcom, Drama, Comedy
No filter (all genres)

No. of Movies
5015
12115
87811

count
0.088
0.574
3.26

select
0.114
0.635
3.163

group_by
0.115
0.704
6.286

join
0.342
1.838
77.896

Running Time (seconds)

[21] Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes, and Guillaume R. Obozinski.

[48] Matei Zaharia et al. 2018. Accelerating the Machine Learning Lifecycle with

2012. A Latent Factor Model for Highly Multi-relational Data. In NIPS.

[22] Alekh Jindal et al. 2021. Magpie: Python at Speed and Scale using Cloud Backends.

MLflow. IEEE Data Eng. Bull. 41 (2018), 39–45.

In CIDR.

[23] Mark Kaminski, Egor V Kostylev, and Bernardo Cuenca Grau. 2016. Semantics
and expressive power of subqueries and aggregates in SPARQL 1.1. In Proceedings
of the 25th International Conference on World Wide Web. International World Wide
Web Conferences Steering Committee, 227–238.

[24] Krys Kochut and Maciej Janik. 2007. SPARQLeR: Extended SPARQL for Semantic
Association Discovery. In Proc. European Semantic Web Conference (ESWC).
[25] Jens Lehmann et al. 2015. DBpedia - A large-scale, multilingual knowledge base

extracted from Wikipedia. Semantic Web 6 (2015).

[26] Shota Matsumoto, Ryota Yamanaka, and Hirokazu Chiba. 2018. Mapping RDF
Graphs to Property Graphs. In Proc. Joint Int. Semantic Technology Conf. (JIST).
[27] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
2000. Automating the construction of Internet portals with machine learning.
Information Retrieval 3, 2 (2000), 127–163.

[28] Aisha Mohamed, Ghadeer Abuoda, Abdurrahman Ghanem, Zoi Kaoudi, and
Ashraf Aboulnaga. 2020. RDFFrames: Knowledge Graph Access for Machine
Learning Tools. Proceedings of the VLDB Endowment (PVLDB) 13, 12 (2020),
2889–2892. (Demonstration).

[29] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The Automatic Con-
struction, Evaluation and Application of a Wide-coverage Multilingual Semantic
Network. 193 (2012), 217–250.

[30] Dat Quoc Nguyen. 2017. An Overview of Embedding Models of Entities and
Relationships for Knowledge Base Completion. arXiv preprint arXiv::1703.08098
(2017).

[31] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2015.
A Review of Relational Machine Learning for Knowledge Graphs. Proc. IEEE 104
(2015).

[32] Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic

Embeddings of Knowledge Graphs. In AAAI.

[33] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A Three-way

Model for Collective Learning on Multi-relational Data. In ICML.

[34] Jorge Pérez, Marcelo Arenas, and Claudio Gutiérrez. 2010. nSPARQL: A Naviga-

tional Language for RDF. J. Web Semantics 8 (2010).

[35] Devin Petersohn et al. 2020. Towards Scalable Dataframe Systems. PVLDB 13

(2020).

[36] Hamid Pirahesh, Joseph M. Hellerstein, and Waqar Hasan. 1992. Extensible/Rule

Based Query Rewrite Optimization in Starburst. In SIGMOD.

[37] Jay Pujara, Eriq Augustine, and Lise Getoor. 2017. Sparsity and Noise: Where

Knowledge Graph Embeddings Fall Short. In EMNLP.

[38] Thomas Rebele, Fabian M. Suchanek, Johannes Hoffart, Joanna Biega, Erdal
Kuzey, and Gerhard Weikum. 2016. YAGO: A Multilingual Knowledge Base from
Wikipedia, Wordnet, and Geonames. In Proceedings International Semantic Web
Conference (ISWC).

[39] Andreas Schwarte, Peter Haase, Katja Hose, Ralf Schenkel, and Michael Schmidt.
2011. FedX: Optimization Techniques for Federated Query Processing on Linked
Data. In ISWC.

[40] D. Sculley et al. 2015. Hidden Technical Debt in Machine Learning Systems. In

NIPS.

[41] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A Core

of Semantic Knowledge. In WWW.

[42] Denny Vrandecic. 2012. Wikidata: A New Platform for Collaborative Data Col-

lection. In WWW.

[43] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge Graph
Embedding: A Survey of Approaches and Applications. IEEE Trans. Knowl. Data
Eng. 29 (2017).

[44] Yanjie Wang, Rainer Gemulla, and Hui Li. 2018. On Multi-relational Link Predic-

tion with Bilinear Models. In AAAI.

[45] Robert West et al. 2014. Knowledge Base Completion via Search-based Question

Answering. In WWW.

[46] Hadley Wickham. 2011. The Split-Apply-Combine Strategy for Data Analysis.

Journal of Statistical Software 40 (2011).

[47] Matei Zaharia et al. 2016. Apache Spark: A Unified Engine for Big Data Processing.

Commun. ACM 59 (2016).

17

A FULL PYTHON CODE FOR CASE STUDIES
A.1 Movie Genre Classification

# RDFFrames imports , graph , and prefixes
from rdfframes . knowledge_graph import KnowledgeGraph
from rdfframes . dataset . rdfpredicate import RDFPredicate
from rdfframes . utils . constants import JoinType
from rdfframes . client . http_client import HttpClientDataFormat , HttpClient
graph = KnowledgeGraph ( graph_uri = ' http :// dbpedia . org ' ,

prefixes = { ' dcterms ': ' http :// purl . org / dc / terms / ',

' rdfs ': ' http :// www . w3 . org /2000/01/ rdf - schema # ',
' dbpprop ': ' http :// dbpedia . org / property / ' ,
' dbpr ': ' http :// dbpedia . org / resource / '})

# RDFFrames code for creating the dataframe
dataset = graph . feature_domain_range ( ' dbpp : starring ' ,' movie ', ' actor ')
dataset = dataset . expand ( ' actor ' ,[( ' dbpp : birthPlace ' , ' actor_country ') ,( ' rdfs : label ', ' actor_name ' )])

. expand ( ' movie ', [( ' rdfs : label ' , ' movie_name ') ,( ' dcterms : subject ' , ' subject '),
( ' dbpp : country ' , ' movie_country ') ,( ' dbpo : genre ' , ' genre ' , Optional )]). cache ()

american = dataset . filter ({ ' actor_country ' :[ ' regex ( str (? actor_country ) ," USA ") ' ]})
prolific = dataset . group_by ([ ' actor ']) .count ( ' movie ' , ' movie_count ', unique = True ). filter ({ ' movie_count ': [ ' >=100 ' ]})
movies = american . join ( prolific , ' actor ' , OuterJoin ). join ( dataset , ' actor ' , InnerJoin )

# Client and execution
output_format = HttpClientDataFormat . PANDAS_DF
client = HttpClient ( endpoint_url = endpoint , return_format = output_format )
df = movies . execute ( client , return_format = output_format )

# Preprocessing and preparation
import re
import nltk

def clean ( dataframe ):

for i , row in df . iterrows ():

if df . loc [ i ][ ' genre '] != None :
value = df . at [i , ' genre ']
if re . match ( regex , str ( value )) is not None :

df . at [i , ' genre '] = value . split ( '/ ' )[ -1]

return dataframe

# Remove URL from the ' genre ' and convert to label keys
df = clean ( df )

# Find the most most frequent genres
all_genres = nltk . FreqDist ( df [ ' genre ' ]. values )
all_genres_df = pd . DataFrame ({ ' genre ': list ( all_genres . keys ()) , ' Count ': list ( all_genres . values ())})
all_genres_df . sort_values ( by =[ ' Count '], ascending = False )

# In this example , use 900 movies as a cut off for the frequent movies
most_frequent_genres = all_genres_df [ all_genres_df [ ' Count ']> 900]
df = df [ df [ ' genre ' ]. isin ( list ( most_frequent_genres [ ' genre ' ]))]

# Features and factorization
from sklearn . model_selection import train_test_split
from sklearn . preprocessing import StandardScaler

df = df . apply ( lambda col : pd . factorize ( col , sort = True )[0])
features = [" movie_name " , " actor_name " , " actor_country " ," subject " ," movie_country " , " subject "]
df = df . dropna ( subset =[ ' genre ' ])
x = df [ features ]
y = df [ ' genre ']
x_train , x_test , y_train , y_test = train_test_split (x , y , random_state =20)
sc = StandardScaler ()
x_train = sc . fit_transform ( x_train )
x_test = sc . fit_transform ( x_test )

# Random Forest classifier
from sklearn . ensemble import RandomForestClassifier

model = RandomForestClassifier ( n_estimators =100)
model . fit ( x_train , y_train )
model . fit ( x_train , y_train )
y_pred = clf . predict ( x_test )
print ( " Accuracy : " , metrics . accuracy_score ( y_test , y_pred ))

Listing 12: Full code for movie genre classification.

18

A.2 Topic Modeling

# RDFFrames imports , graph , prefixes , and client
import pandas as pd
from rdfframes . client . http_client import HttpClientDataFormat , HttpClient
from rdfframes . knowledge_graph import KnowledgeGraph
graph = KnowledgeGraph (

graph_uri = ' http :// dblp . l3s . de ',
prefixes = { " xsd ": " http :// www . w3 . org /2001/ XMLSchema #" ,
" swrc ": " http :// swrc . ontoware . org / ontology #" ,
" rdf ": " http :// www . w3 . org /1999/02/22 - rdf - syntax - ns #" ,
" dc ": " http :// purl . org / dc / elements /1.1/ " ,
" dcterm ": " http :// purl . org / dc / terms /" ,
" dblprc ": " http :// dblp . l3s . de / d2r / resource / conferences /"

})

output_format = HttpClientDataFormat . PANDAS_DF
client = HttpClient ( endpoint_url = endpoint , port = port , return_format = output_format )

# RDFFrames code for creating the dataframe
papers = graph . entities ( ' swrc : InProceedings ' , paper )
papers = papers . expand ( ' paper ' ,[( ' dc : creator ', ' author ') ,( ' dcterm : issued ' , ' date '), ( ' swrc : series ', ' conference '),

authors = papers . filter ({ ' date ': [ ' >=2005 '], ' conference ': [ 'In ( dblp : vldb ,␣ dblp : sigmod ) ' ]}). group_by ([ ' author ' ])
. count ( ' paper ' , ' n_papers ' ). filter ({ ' n_papers ': ' >=20 ', ' date ': [ ' >=2005 ' ]})

( ' dc : title ', ' title ' )]). cache ()

titles = papers . join ( authors , ' author ', InnerJoin ). select_cols ([ ' title ' ])
df = titles . execute ( client , return_format = output_format )

# Preprocessing and cleaning
from nltk . corpus import stopwords
df [ ' clean_title '] = df [ ' title ' ]. str . replace (" [^a -zA - Z #] " , "␣")
df [ ' clean_title '] = df [ ' clean_title ']. apply ( lambda x: x. lower ())
df [ ' clean_title '] = df [ ' clean_title ']. apply ( lambda x: '␣ '. join ([ w for w in str (x ). split () if len (w ) >3]))
stop_words
tokenized_doc
df [ ' clean_title '] = tokenized_doc . apply ( lambda x :[ item for item in x if item not in stop_words ])

= stopwords . words ( ' english ')
= df [ ' clean_title ']. apply ( lambda x: x. split ())

# Vectorization and SVD model using the scikit - learn library
from sklearn . feature_extraction . text import TfidfVectorizer
from sklearn . decomposition import TruncatedSVD
vectorizer
Tfidf_title = vectorizer . fit _transform ( df [ ' clean_title ' ])
svd_model
svd_model . fit ( Tfidf_titles )

= TfidfVectorizer ( stop_words = ' english ' , max_features = 1000 , max_df = 0.5 , smooth_idf = True )

= TruncatedSVD ( n_components =20 , algorithm = ' randomized ' , n_iter =100 , random_state =122)

# Extracting the learned topics and their keyterms
terms = vectorizer . get_feature_names ()
for i , comp in enumerate ( svd_model . components_ ):

= zip ( terms , comp )

terms_comp
sorted_terms = sorted ( terms_comp , key = lambda x :x [1] , reverse = True )[:7]
print_string = " Topic "+ str (i )+ " :␣ "
for t in sorted_terms :

print_string += t [0] + "␣"

Listing 13: Full code for topic modeling.

19

A.3 Knowledge Graph Embedding
# Get all triples where the object is a URI

from rdfframes . knowledge_graph import KnowledgeGraph
from rdfframes . dataset . rdfpredicate import RDFPredicate
from rdfframes . client . http_client import HttpClientDataFormat , HttpClient
output_format = HttpClientDataFormat . PANDAS_DF
client = HttpClient ( endpoint_url = endpoint ,

port = port ,
return_format = output_format ,
timeout = timeout ,
default_graph_uri = default_graph_url ,
max_rows = max_rows
)

dataset = graph . feature_domain_range (s , p , o ). filter ({ o: [ ' isURI ' ]})
df = dataset . execute ( client , return_format = output_format )

# Train / test split and create ComplEx model from ampligraph library
from ampligraph . evaluation import train_test_split_no_unseen
triples = df . to_numpy ()
X_train , X_test = train_test_split_no_unseen ( triples , test_size =10000)
# complEx model from ampligraph library
from ampligraph . latent_features import ComplEx
from ampligraph . evaluation import evaluate_performance , mrr_score , hits_at_n_score
model = ComplEx ( batches_ count =50 , epochs =300 , k =100 , eta =20 , optimizer = ' adam ', optimizer_params ={ 'lr ' :1e -4} ,

loss = ' multiclass_nll ' , regularizer = 'LP ' , regularizer_params ={ 'p ':3 , ' lambda ' :1e -5} , seed =0 , verbose = True )

model . fit ( X_train )
# Evaluate embedding model
filter _triples = np . concatenate (( X_train , X_test ))
ranks = evaluate_performance ( X_test , model = model , filter _triples = filter _triples ,

use_default_protocol = True , verbose = True )

= mr_score ( ranks )

mr
mrr = mrr_score ( ranks )

Listing 14: Full code for knowledge graph embedding.

20

B DESCRIPTION OF QUERIES IN THE SYNTHETIC WORKLOAD

Table 3: Description of the queries in the sytnthetic workload.

Query English Description

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Q8

Q9

Q10

Q11

Q12

Q13

Q14

Q15

Q16

Get a list of films in DBpedia. For each film, return the actor, language, coun-
try, genre, story, and studio, in addition to the director, producer, and title (if
available).
Get a list of actors available in the DBpedia or YAGO graphs.

Get a list of American actors available in both the DBpedia and YAGO graphs.

Get the nationality, place of birth, and date of birth of each basketball player
in DBpedia, in addition to the sponsor, name, and president of his team (if
available).
Get the players (athletes) in DBpedia and their teams, group by teams, count
players, and expand the team’s name.
For films in DBpedia that are produced by any studio in India or the United
States excluding ’Eskay Movies’, and that have one of the following genres (film
score, soundtrack, rock music, house music, or dubstep), get the actor, director,
producer, time and language.
For the films in DBpedia, get actors, director, country, producer, language, title,
genre, story, and studio. Filter on country, studio, genre, and runtime.
Get the nationality, place of birth, and date of birth of each basketball player in
DBpedia, in addition to the sponsor, name, and president of his team.
Get the list of basketball players in DBpedia, their teams, and the number of
players on each team.
For films in DBpedia that are produced by any studio in India or the United
States excluding ’Eskay Movies’, and that have one of the following genres
(film score, soundtrack, rock music, house music, or dubstep), get the actor and
language, in addition to the producer, director, and title (if available).
Get the list of athletes in DBpedia. For each athlete, return his birthplace and
the number of athletes who were born in that place.
Get the pairs of films in DBpedia that belong to the same genre and are produced
in the same country. For each film in each pair, return the actor, country, story,
language, genre, and studio, in addition to the director, producer, and title (if
available).
Get the sponsor, name, president, and the number of basketball players of each
basketball team in DBpedia.

Get the sponsor, name, president, and the number of basketball players (if
available) of each basketball team in DBpedia.

Get a list of the books in DBpedia that were written by American authors who
wrote more than two books. For each author, return the birth place, country,
and education, and for each book return the title, subject, country (if available),
and publisher (if available).
Get a list of people in the DBpedia graph who were born in the United States.
Get a list of authors from the DBLP graph who have publications dated after
2015. Get a list of people in the YAGO graph who are citizens of the United
States. Join the three lists retrieved from the three graphs on name.

21

RDFFrames Operators
expand (including optional
predicates)

SPARQL Features
OPTIONAL, DISTINCT

between two

join (outer) between two graphs,
filter
join (inner)
graphs, expand, filter
join (left outer) between two ex-
pandable datasets, expand (in-
cluding optional predicates)
group_by, count, expand

expand, filter

FILTER,

OPTIONAL,
UNION
FILTER

OPTIONAL

GROUP BY, COUNT,
DISTINCT
FILTER

expand, filter

FILTER

join (inner) between two ex-
pandable datasets, expand
group_by, count, expand

expand (including optional
predicates), filter

conjunctive

Multiple
graph patterns
GROUP BY, COUNT,
DISTINCT
OPTIONAL, FILTER

group_by, count, expand

group_by on multiple columns,
count, expand (including op-
tional predicates)

GROUP BY, COUNT,
DISTINCT
GROUP BY, COUNT,
OPTIONAL, DISTINCT

outer)

between two
(expandable,

join (inner)
datasests
group_by)
between
(left
join
two
(expandable,
datasets
group_by), expand (including
optional predicates)
join (outer), group_by, count,
expand (including optional
predicates), filter

join (full outer) between three
graphs, expand (including op-
tional predicates), filter

GROUP BY, COUNT,
DISTINCT

GROUP BY, COUNT,
OPTIONAL, DISTINCT

GROUP BY, COUNT,
HAVING, OPTIONAL,
FILTER, UNION

OPTIONAL,
DISTINCT, UNION

FILTER,

C NAIVE SPARQL QUERY FOR MOVIE GENRE CLASSIFICATION
PREFIX dbpp: <http://dbpedia.org/property/>
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX dbpo: <http://dbpedia.org/ontology/>
PREFIX dbpr: <http://dbpedia.org/resource/>
SELECT DISTINCT ?actor_name ?movie_name ?actor_country ?subject ?genre
FROM <http://dbpedia.org> WHERE

{{{ SELECT * WHERE

{{ SELECT * WHERE

{ { SELECT ?movie ?actor WHERE

{ ?movie dbpp:starring ?actor } }
{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country } }

{ SELECT ?actor ?actor_name WHERE

{ ?actor rdfs:label ?actor_name } }

{ SELECT ?movie ?movie_name WHERE

{ ?movie rdfs:label ?movie_name } }

{ SELECT ?movie ?subject WHERE

{ ?movie dcterms:subject ?subject } }

{ SELECT ?movie ?movie_country WHERE

{ ?movie dbpp:country ?movie_country } }

{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country

FILTER regex(str(?actor_country), "USA") } }

{ SELECT ?movie ?genre WHERE

{ OPTIONAL

{ ?movie dbpo:genre ?genre } } } } }

OPTIONAL

{ SELECT DISTINCT ?actor (COUNT(DISTINCT ?movie) AS ?movie_count) WHERE

{ { SELECT ?movie ?actor WHERE

{ ?movie dbpp:starring ?actor } }
{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country } }

{ SELECT ?actor ?actor_name WHERE

{ ?actor rdfs:label ?actor_name } }

{ SELECT ?movie ?movie_name WHERE

{ ?movie rdfs:label ?movie_name } }

{ SELECT ?movie ?subject WHERE

{ ?movie dcterms:subject ?subject } }

{ SELECT ?movie ?movie_country WHERE

{ ?movie dbpp:country ?movie_country } }

{ SELECT ?movie ?genre WHERE

{ OPTIONAL

{ ?movie dbpo:genre ?genre } } } }

GROUP BY ?actor
HAVING ( COUNT(DISTINCT ?movie) >= 100 ) } } }

UNION

{ SELECT * WHERE

{ { SELECT DISTINCT ?actor (COUNT(DISTINCT ?movie) AS ?movie_count) WHERE

{ { SELECT ?movie ?actor WHERE

{ ?movie dbpp:starring ?actor } }
{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country } }

{ SELECT ?actor ?actor_name WHERE

{ ?actor rdfs:label ?actor_name } }

{ SELECT ?movie ?movie_name WHERE

{ ?movie rdfs:label ?movie_name } }

{ SELECT ?movie ?subject WHERE

{ ?movie dcterms:subject ?subject } }

{ SELECT ?movie ?movie_country WHERE

{ ?movie dbpp:country ?movie_country } }

{ SELECT ?movie ?genre WHERE

{ OPTIONAL { ?movie dbpo:genre ?genre } } } }

GROUP BY ?actor
HAVING ( COUNT(DISTINCT ?movie) >= 100 ) }

OPTIONAL
{ SELECT * WHERE

{ { SELECT ?movie ?actor WHERE

{ ?movie dbpp:starring ?actor } }
{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country } }

{ SELECT ?actor ?actor_name WHERE

{ ?actor rdfs:label ?actor_name } }

{ SELECT ?movie ?movie_name WHERE

{ ?movie rdfs:label ?movie_name } }

{ SELECT ?movie ?subject WHERE

22

{ ?movie dcterms:subject ?subject } }

{ SELECT ?movie ?movie_country WHERE

{ ?movie dbpp:country ?movie_country } }

{ SELECT ?actor ?actor_country WHERE

{ ?actor dbpp:birthPlace ?actor_country

FILTER regex(str(?actor_country), "USA") } }

{ SELECT ?movie ?genre WHERE

{ OPTIONAL { ?movie dbpo:genre ?genre } } } } } } } } }

Listing 15: Naive SPARQL query corresponding to the SPARQL query shown in Listing 7.

23

D NAIVE SPARQL QUERY FOR TOPIC MODELING
PREFIX swrc: <http://swrc.ontoware.org/ontology#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX dcterm: <http://purl.org/dc/terms/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX dc: <http://purl.org/dc/elements/1.1/>
PREFIX dblprc: <http://dblp.l3s.de/d2r/resource/conferences/>

SELECT ?title
FROM <http://dblp.l3s.de>
WHERE
{

{SELECT ?paper WHERE {?paper rdf:type swrc:InProceedings}}.
{SELECT ?paper ?author WHERE {?paper dc:creator ?author}}.
{SELECT ?paper ?date WHERE {?paper dcterm:issued ?date }}.
{SELECT ?paper ?conference WHERE {?paper swrc:series ?conference}} .
{SELECT ?paper ?title WHERE {?paper dc:title ?title}}.
{SELECT ?paper ?date WHERE {?paper dcterm:issued ?date FILTER ( year(xsd:dateTime(?date)) >= 2005 ) }}

{ SELECT ?author WHERE

{
{ SELECT ?author COUNT(?paper) as ?count_paper

WHERE
{

{SELECT ?paper WHERE {?paper rdf:type swrc:InProceedings}}.
{SELECT ?paper ?author WHERE {?paper dc:creator ?author}}.
{SELECT ?paper ?date WHERE {?paper dcterm:issued ?date }}.
{SELECT ?paper ?conference WHERE {?paper swrc:series ?conference}} .
{SELECT ?paper ?title WHERE {?paper dc:title ?title}}.
{SELECT ?paper ?date WHERE {?paper dcterm:issued ?date FILTER ( year(xsd:dateTime(?date)) >= 2000 ) }} .
{SELECT ?paper ?conference WHERE {?paper swrc:series ?conference FILTER( ?conference IN (dblprc:vldb, dblprc:sigmod) )}}

}

GROUP BY ?author

}
FILTER ( ?count_paper >= 20 )

}

}

}

Listing 16: Naive SPARQL query corresponding to the SPARQL query shown in Listing 9.

24

