Report of the Workshop on Program
Synthesis for Scientiﬁc Computing

August 4–5, 2020

Organizers:
Hal Finkel, Argonne National Laboratory
Ignacio Laguna, Lawrence Livermore National Laboratory

1
2
0
2

b
e
F
2

]

G
L
.
s
c
[

1
v
7
8
6
1
0
.
2
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
Report of Workshop on Program Synthesis for SC

Participants:

Zhengchun Liu (Argonne National Laboratory)
Alexey Loginov (GrammaTech, Inc.)
Nuno Lopes (Microsoft Corporation)
Abid Malik (Brookhaven National Laboratory)
Ruben Martins ( Carnegie Mellon University)
James McDonald (Kestrel Institute)

Abdullah Muzahid (Texas A&M University)
Mayur Naik (University of Pennsylvania)
Sri Hari Krishna Narayanan (Argonne National Laboratory)
Brandon Neth ( University of Arizona)
Matthew Norman (Oak Ridge National Laboratory)
Boyana Norris (University of Oregon)
Cathie Olschanowsky (Boise State Univeristy)
Cyrus Omar (University of Michigan)
Peter-Michael Osera (Grinnell College)
Pavel Panchekha (University of Utah)
Sheena Panthaplackel (University of Texas at Austin)
Eun Jung Park (EJ) (Los Alamos National Laboratory)
Gilchan Park (Brookhaven National Laboratory)
Madhusudan Parthasarathy (UIUC)
Tharindu Patabandi (University of Utah)
Hila Peleg (University of California, San Diego)
Ruzica Piskac (Yale University)
Nadia Polikarpova (University of California, San Diego)
Tobi Popoola (Boise State Univeristy)
Xiaokang Qiu (Purdue University)
Arjun Radhakrishna (Microsoft Corporation)
Jonathan Ragan-Kelley (Massachusetts Institute of Technology)
Krishnan Raghavan (Argonne National Laboratory)
Yihui Ren (Brookhaven National Laboratory)
Thomas Reps (University of Wisconsin - Madison)
Kamil Rocki (Cerebras Systems)
Jose Rodriguez (Intel Corporation)
Tiark Rompf (Purdue University)
Randi Rost (Intel Corporation)
Roopsha Samanta (Purdue University)

Maaz Ahmad (University of Washington)
Alex Aiken (Stanford University)
Aws Albarghouthi (University of Wisconsin - Madison)
Farhana Aleen (NVIDIA Corporation)
Francis Alexander (Brookhaven National Laboratory)
Rajeev Alur (University of Pennsylvania)
Saman Amarasinghe (Massachusetts Institute of Technology) Thirimadura Mendis (Massachusetts Institute of Technology)
Todd Anderson (Intel Corporation)
Pavan Balaji (Argonne National Laboratory)
Prasanna Balaprakash (Argonne National Laboratory)
Ras Bodik (University of Washington)
James Bornholt (University of Texas at Austin)
Bill Carlson (Center for Computing Sciences)
Barbara Chapman (BNL & Stony Brook University)
François Charton (Facebook)
Swarat Chaudhuri (University of Texas at Austin)
Estee Chen (University of Pennsylvania)
Alvin Cheung (University of California, Berkeley)
Amazon & University of Chicago (spertus.com)
Taylor Childers (Argonne National Laboratory)
Ravi Chugh (University of Chicago)
Valentin Clement (Oak Ridge National Laboratory)
Loris D’Antoni (University of Wisconsin - Madison)
Arnab Das (University of Utah)
Isil Dillig (University of Texas at Austin)
Johannes Doerfert (Argonne National Laboratory)
Krzysztof Drewniak (University of Washington)
Anshu Dubey (Argonne National Laboratory)
Souradeep Dutta (University of Pennsylvania)
Markus Eisenbach (Oak Ridge National Laboratory)
Nur Aiman Fadel (Swiss National Supercomputing Centre)
Kyle Felker (Argonne National Laboratory)
Ian Foster (Argonne National Laboratory)
Franz Franchetti (Carnegie Mellon University)
Milos Gligoric (University of Texas at Austin)
Cindy Rubio Gonzalez (University of California, Davis)
Ganesh Gopalakrishnan (University of Utah)
Justin Gottschlich (Intel Labs & University of Pennsylvania) Mark Santolucito (Barnard College)
Vinod Grover (NVIDIA Corporation)
Mary Hall (University of Utah)
Niranjan Hasabnis (Intel Corporation)
Amaury Hayat (Rutgers University)
Thomas Helmuth (Hamilton College)
Michael Heroux (Sandia National Laboratories)
Paul Hovland (Argonne National Laboratory)
Justin Hsu (University of Wisconsin - Madison)
Jan Hueckelheim (Argonne National Laboratory)
Roshni Iyer (University of California, Los Angeles)
John Jacobson (University of Utah)
Xiao-Yong Jin (Argonne National Laboratory)
Beau Johnston (Oak Ridge National Laboratory)
Vinu Joseph (University of Utah)
Ian Karlin (Lawrence Livermore National Laboratory)
Vineeth Kashyap (GrammaTech, Inc.)
Samuel Kaufman (University of Washington)
Sarfraz Khurshid (University of Texas at Austin)
Jungwon Kim (Oak Ridge National Laboratory)
Martin Kong (University of Oklahoma)
Jaehoon Koo (Northwestern University)
Siddharth Krishna (Microsoft Corporation)
Michael Kruse (Argonne National Laboratory)
Ignacio Laguna (Lawrence Livermore National Laboratory) Wei Wang (University of California, Los Angeles)
Jacob Lambert (University of Oregon)
Edward Lee (University of California, Berkeley)
Seyong Lee (Oak Ridge National Laboratory)
Richard Lethin (Reservoir Labs)
Dmitry Liakh (Oak Ridge National Laboratory)
Nevin Liber (Argonne National Laboratory)

Vivek Sarkar (Georgia Institute of Technology)
Markus Schordan (Lawrence Livermore National Laboratory)
Eric Schulte (GrammaTech, Inc.)
Koushik Sen (University of California, Berkeley)
Srinivasan Sengamedu (Amazon)
Dolores Shaffer (Science and Technology Associates, Inc.)
Min Si (Argonne National Laboratory)
Douglas Smith (Kestrel Institute)
Armando Solar-Lezama (Massachusetts Institute of Technology)
George Stelle (Los Alamos National Laboratory)
Michelle Strout (University of Arizona)
Yizhou Sun (University of California, Los Angeles)
Joseph Tarango (Intel Corporation)
Zachary Tatlock (University of Washington)
Valerie Taylor (Argonne National Laboratory)
Supun Tennakoon (Purdue University)
Aditya Thakur (University of California, Davis)
Rajeev Thakur (Argonne National Laboratory)
Jesmin Jahan Tithi (Intel Corporation)
Jeffrey Vetter (Oak Ridge National Laboratory)
Brice Videau (Argonne National Laboratory)
Ashwin Vijayakumar (Intel Corporation)
Fei Wang (Purdue University)

Xinyu Wang (University of Michigan)
Fangke Ye (Georgia Institute of Technology)
Jisheng Zhao (Georgia Institute of Technology)
Shengtian Zhou (Intel Corporation)
Tong Zhou (Georgia Institute of Technology)

2

Report of Workshop on Program Synthesis for SC

This report was prepared as an account of a workshop sponsored by the U.S. Department of Energy. Neither the United
States Government nor any agency thereof, nor any of their employees or ofﬁcers, makes any warranty, express or implied,
or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus,
product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any
speciﬁc commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily
constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof.
The views and opinions of document authors expressed herein do not necessarily state or reﬂect those of the United States
Government or any agency thereof. Copyrights to portions of this report (including graphics) are reserved by original
copyright holders or their assignees and are used by the Government’s license and by permission. Requests to use any
images must be made to the provider identiﬁed in the image credits (if any) or the ﬁrst author.

The Workshop on Program Synthesis for Scientiﬁc Computing was held virtually on August 4–5 2020.

3

INTRODUCTION

Computational methods have become a corner-
stone of scientiﬁc progress, with nearly every sci-
entiﬁc discipline relying on computers for data
collection, data analysis, and simulation. As a re-
sult, signiﬁcant opportunities exist to accelerate
scientiﬁc discovery by accelerating the develop-
ment and execution of scientiﬁc software.

The ﬁrst priority research direction of the U.S.
Department of Energy’s (DOE’s) report on ex-
treme heterogeneity [101] and the ninth section of
the DOE community’s AI for Science report [93],
among other sources, highlight the need for re-
search in AI-driven methods for creating scientiﬁc
software. This workshop expands on these re-
ports by exploring how program synthesis, an ap-
proach to automatically generating software pro-
grams based on some user intent [35, 36]—along
with other high-level, AI-integrated programming
methods—can be applied to scientiﬁc applications
in order to accelerate scientiﬁc discovery.

We believe that the promise of program synthesis

(also referred to as machine programming [35]) for
the scientiﬁc programming domain is at least the
following:

• Signiﬁcantly reducing the temporal overhead
associated with software development. We
anticipate increases in productivity of scien-
tiﬁc programming by orders of magnitude
by making all parts of the software life cy-
cle more efﬁcient, including reducing the time
spent tracking down software quality defects,
such as those concerned with correctness, per-
formance, security, and portability [3, 4, 39]

• Enabling scientists, engineers,

technicians,
and students to produce and maintain high-
quality software as part of their problem-
solving process without requiring specialized
software development skills [77]

Program synthesis is an active research ﬁeld in
academia, national labs, and industry. Yet, work
directly applicable to scientiﬁc computing, while
having some impressive successes, has been lim-
ited. This report reviews the relevant areas of pro-

1

Report of Workshop on Program Synthesis for SC

free to adapt the higher-order invented software
representation to a lower-order representation that
is speciﬁc to the user’s unique software and hard-
ware environment. This process tends to be neces-
sary to ensure robust software quality characteris-
tics such as performance and security.

When the implementation—the how of
the
program—is unambiguously speciﬁed, the pro-
cess of translating that speciﬁed implementation
into an executable program is called compilation.

Good compiler technology is essential to scientiﬁc
programming; and as discussed later in this re-
port, opportunities exist for enhancing compiler
technology to better enable program synthesis
technology. These two areas, program synthesis
and compilers, inform each other; and as we look
toward the future, state-of-the-art programming
environments are likely to contain both synthesis
and compilation capabilities.

Program synthesis is expected to reduce
software development overhead and in-
crease conﬁdence in the correctness of pro-
grams. Compiler technology will be essen-
tial to enable this technology.

The ﬁrst program-synthesis systems focused on
the automated translation of mathematically pre-
cise program speciﬁcations into executable code.
These systems performed what
is sometimes
called deductive synthesis and often functioned
by trying to match parts of the speciﬁcation to a
library of relevant implementation techniques.

With the advent of strong satisﬁability modulo
theories (SMT) solvers, program synthesis had an
important new technology on which to build. An
SMT solver can naturally produce counterexam-
ples to an inconsistent set of mathematical asser-
tions or produce an assertion that no counterex-
amples exist, which is useful for several kinds of
veriﬁcation and synthesis tasks. Writing mathe-
matical speciﬁcation is often subtle, however, and
considerable attention in the synthesis community
has focused on inductive synthesis: the synthesis
of programs based on behavioral examples [37].

Of course, systems can be both deductive and

Figure 1: The Three Pillars of Machine Program-
ming (credit: Gottschlich et al. [35]).

gram synthesis work, discusses successes to date,
and outlines opportunities for future work.

Background on Program Synthesis

Program synthesis represents a wide array of
machine programming [35] techniques that can
greatly enhance programmer productivity and
software quality characteristics, such as program
correctness, performance, and security. Speciﬁ-
cally, program synthesis incorporates techniques
whereby the following may occur:

• The desired program behavior is speciﬁed,
but the (complete) implementation is not. The
synthesis tool determines how to produce an
executable implementation.

• The desired program behavior or any par-
tial implementation is ambiguously speciﬁed.
Iteration with the programmer, data-driven
techniques, or both are used to construct a
likely correct solution.

In "The Three Pillars of Machine Programming"
nomenclature, program synthesis is classiﬁed in
the space of intention [35] (see Figure 1). The goal
of intention is to provide programmers (and non-
programmers) with ways to communicate their
ideas to the machine. Once these goals have
been expressed, the machine constructs (invents)
a higher-order representation of the user’s inten-
tion. Next, the machine programming system is

2

Report of Workshop on Program Synthesis for SC

inductive, which is useful because sometimes a
programmer knows some of the desired prop-
erties but wishes to ﬁll in the remaining infor-
mation necessary to construct the program us-
ing examples. For example, techniques for type-
and-example-directed synthesis deductively use
types provided by the programmer to guide an
inductive search for missing program fragments
that satisfy the given examples [71, 60]. Com-
bining inductive synthesis with SMT solver iter-
ation, using solver-generated counterexamples to
guide the synthesis process, has also been a fruit-
ful area: counterexample-guided inductive syn-
thesis (CEGIS) has produced exciting results over
the past decade.

CEGIS and related techniques are good at reﬁn-
ing potential solutions that are structurally close
to a correct answer, but the techniques have more
difﬁculty in searching the unbounded space of
potential program structures. Evolutionary algo-
rithms have made important contributions to this
problem, especially those that encourage the selec-
tion of specialists (potential solutions that work for
some, but not necessarily all, solution objectives).

The deep learning revolution has led to signiﬁcant
advancements in program synthesis as well. The
space of potential program structures can be ex-
plored by using differentiable programming, re-
inforcement learning, and other machine learning
techniques. Deep learning has also made practi-
cal the incorporation of natural language process-
ing into the synthesis process and the generation
of natural language comments as part of the syn-
thesis output. Machine learning techniques now
power advanced autocomplete features in various
development environments, and looking toward
the future, advanced tools can explore more than
single-statement completions.

As program-synthesis technology, driven by ad-
vanced deep learning, evolutionary, and veriﬁca-
tion techniques, moves toward tackling real-world
programming problems, imparting the resulting
productivity gains to scientiﬁc programming will
require techniques and capabilities that might not
be required for other domains.
In this report,
we explore the state of and challenges in scien-
tiﬁc programming and how research in program
synthesis technology and synergistic research in

compiler technology might be directed to apply to
scientiﬁc-programming tasks.

CURRENT STATE OF SCIENTIFIC APPLICATION DE-
VELOPMENT

Scientiﬁc application development is essential to
scientiﬁc progress; and yet, while advances in both
scientiﬁc techniques and programming tools con-
tinue to improve programmer productivity, cre-
ating state-of-the-art scientiﬁc programs remains
challenging.

Software complexity has increased over the past
decades at an astounding rate, with many pop-
ular applications and libraries containing tens of
millions of lines of code. As shown in Figure 2,
scientiﬁc development has not been immune from
this increase in complexity.

General software infrastructures, along with al-
gorithmic and mathematical techniques, have be-
come increasingly sophisticated. Hardware archi-
tectures and the techniques necessary to exploit
them have also become increasingly sophisticated,
as has the science itself. Managing the result-
ing complexity is difﬁcult even for the most ex-
perienced scientiﬁc programmers. Moreover, a lot
of scientiﬁc programming is not done by experi-
enced programmers but by scientiﬁc-domain stu-
dents and recent graduates with only a few years
of experience [63]. As a result, new programmers
on a project have difﬁculty reaching high levels of
productivity.

The Largest Challenges

Code development itself is labor intensive; and
in order to develop scientiﬁc applications, signif-
icant portions of the development require direct
input from domain experts. These applications
often contain critical components that are math-
ematically complex; and hence developers require
advanced mathematical abilities, strong program-
ming skills, and a good understanding of the sci-
ence problem being solved.

Separations of concerns are common, and not ev-
ery scientiﬁc-software developer implements ev-
ery mathematical technique from scratch. Never-
theless, developers need sufﬁcient knowledge of

3

Report of Workshop on Program Synthesis for SC

Figure 2: Number of source lines of code in various packages: general packages on the left, scientiﬁc
packages on the right. Nearly all data from openhub.net (Sept. 2020).

the relevant mathematical techniques to select and
use applicable libraries.

Scientiﬁc software has become increas-
ingly complex over the years. Tools and
techniques to improve productivity in this
space are desperately needed.

Moreover, often the complete design for the ap-
plication cannot be speciﬁed up front. One may
not know ahead of time what grid resolutions,
discretization techniques, solvers, and so on will
work best for the target problems.
Instead, de-
velopment is an iterative process that is part of
the scientiﬁc investigatory process. As the science
evolves, the target problem might change as well,
necessitating signiﬁcant changes to the application
design.

While application development for dynamic con-
sumer markets also faces challenges with evolv-
ing requirements, scientiﬁc software must of-
ten meet tight performance and mathematical re-
quirements, where large changes in design must

be implemented quickly by small development
teams, imparting unique needs for productivity-
enhancing tools in this space.

Adapting to New Hardware: Scientiﬁc comput-
ing applications tend to require high computa-
tional performance and, as a result, are structured
and tuned to maximize achieved performance on
platforms of interest. However, computational
performance is maximized on cutting-edge hard-
ware, and cutting-edge hardware has been evolv-
ing rapidly. As a result, applications have had to
adapt to new CPU features, such as SIMD vector
registers, GPU accelerators, and distribution over
tens of thousands of independent nodes. These ar-
chitectures continue to evolve, with corresponding
changes to their programming models, and appli-
cations must adapt in turn. If signiﬁcant work is
invested in turning for a particular architecture, as
is often the case [7], repeating that level of work
for many different kinds of systems is likely infea-
sible.

To maintain developer productivity in the face of
a variety of target architectures, the community
has placed signiﬁcant focus on programming envi-

4

ronments that provide some level of performance
portability. The idea is that reasonable perfor-
mance, relative to the underlying system capabil-
ities for each system, can be obtained with no, or
minimal, source code changes between systems.

What qualiﬁes as reasonable performance and
what qualiﬁes as minimal changes to the source
code are hotly debated and, in the end, depend
on many factors speciﬁc to individual develop-
ment teams. Nevertheless, overall goals are clear,
and these motivate the creation of compiler ex-
tensions (e.g., OpenMP), C++ abstraction libraries
(e.g., Kokkos [32]), and domain-speciﬁc languages
(e.g., Halide [76]).

Furthermore, the design of these portable abstrac-
tions is largely reactionary; and while one can an-
ticipate and incorporate some new hardware fea-
tures before the hardware is widely available, of-
ten the best practices for using a particular hard-
ware architecture are developed only after exten-
sive experimentation on the hardware itself. This
situation leads to a natural tension between the
productivity gain from using portable program-
ming models and the time-to-solution gain poten-
tially available from application- and hardware-
speciﬁc optimizations.

The ability to perform autotuning on top of these
technologies has been demonstrated to enhance
performance signiﬁcantly [11].
Some systems
(e.g., Halide) were speciﬁcally designed with this
integration in mind. However, autotuning brings
with it a separate set of challenges that make de-
If the autotuning process is
ployment difﬁcult.
part of the software build process, then the build
process becomes slow and potentially nondeter-
ministic.

On the other hand, if the autotuning process pro-
duces artifacts that are separately stored in the
source repository, then the artifacts need to be kept
up to date with the primary source code, a require-
ment made more difﬁcult by the fact that not all
developers have access to all of the hardware on
which the artifacts were generated. A potential
solution to these challenges is to perform the au-
totuning while the application is running, but then
the time spent autotuning must be traded against
the potential performance beneﬁts. Nevertheless,

Report of Workshop on Program Synthesis for SC

the optimal tuning results sometimes depend on
the state of the application’s data structures (e.g.,
matrix sizes), and these can change during the
course of a long-running process, providing an ad-
ditional advantage to during-execution autotun-
ing and autotuning procedures that can make use
of detailed proﬁling data.

High performance can often be obtained by us-
ing different implementations of the same algo-
rithm, and the aforementioned frameworks natu-
rally apply to this case. Sometimes, however, es-
pecially when accelerators are involved, different
algorithms are needed in order to obtain accept-
able performance on different kinds of hardware.
In recognition of this reality, a number of capabili-
ties have been explored for supporting algorithmic
variants that can be substituted in a modular fash-
ion as part of the porting process (e.g., OpenMP
metadirectives, PetaBricks [9]). Having multiple
available algorithms for tasks within an applica-
tion, however, makes testing and veriﬁcation of
the application more difﬁcult. Development and
maintenance are also more expensive because each
algorithmic variant must be updated as the base-
line set of required features expands over time and
as defects are ﬁxed.

The ability of scientiﬁc codes to quickly
adapt to new hardware is increasingly chal-
lenging. While performance-portable pro-
gramming models and autotuning help,
more advanced end-to-end capabilities are
needed to adapt algorithms and data struc-
tures to new environments.

Data Movement Cost: The performance of many
scientiﬁc applications is dominated by data move-
ment. Customizing temporary storage require-
ments to architectures and application needs has
demonstrated promising performance improve-
ments [67]. Previous work combined schedul-
ing transformations within a sparse polyhedral
framework and dataﬂow graphs to enable human-
in-the-loop architecture-speciﬁc optimization [29].
Because of large variations in the design of mem-
ory subsystems, optimizing the actual layout of
data in memory can reduce data movement as

5

well as reduce its cost. As one example, bricks—
mini-subdomains in contiguous memory orga-
nized with adjacency lists—are used to represent
stencil grids [110, 111]. Bricks reduce the cost
of data movement because they are contiguous
and therefore decrease the number of pages and
cache lines accessed during a stencil application.
Through a layout of bricks optimized for commu-
nication, we also eliminate the data movement cost
of packing/unpacking data to send/receive mes-
sages.

Scheduling: A large fraction of scientiﬁc applica-
tions are still written in legacy code such C/C++
and require mapping to multithreaded code ei-
ther manually or by autoparalellizing compilers.
Performant execution of such legacy codes is de-
pendent on good task scheduling. While most
scheduling techniques perform statically, Aleen
et al. [6] showed that orchestrating dynamically
(by running a lightweight emulator on the input-
characterization graph extracted from the pro-
gram) can better balance workloads and provide
further improvement over static scheduling.

Polyhedral Multiobjective Scheduling: As a step
toward achieving portable performance, Kong and
Pouchet [48] recently proposed a extensible kernel
set of integer linear program (ILP) objectives that
can be combined and reordered to produce dif-
ferent performance properties. Each ILP objective
aims to maximize or minimize some property on
the generated code, for instance, minimizing the
stride penalty of the innermost loop. However,
this work did not address how to select the ob-
jectives to embed into the ILP. More recently, Che-
lini et al. [20] proposed a systematic approach to
traverse the space of ILP objectives previously de-
ﬁned by ﬁrst creating an ofﬂine database of trans-
formations. Such a database is constructed from a
number of input cases exhibiting different depen-
dence patterns, which are then transformed via
all the possible 3-permutations of ILP objectives
from the kernel set of transformations. The result-
ing transformed codes are then analyzed to extract
speciﬁc code features that are stored together with
the input dependencies and the transformations
used (see Fig. 3). Later, during the compilation
phase, the database is queried to adaptively select
ILP objectives to embed. This process is illustrated

Report of Workshop on Program Synthesis for SC

in Fig. 4.

Figure 3: Ofﬂine Database Construction

Figure 4: Adaptive Scheduling and Objective Se-
lection (Database Querying).

Testing and Veriﬁcation:
The correctness of
scientiﬁc-computing applications is critical be-
cause the scientiﬁc enterprise depends on predic-
tive computational techniques. Especially in ar-
eas where the results produced by scientiﬁc ap-
plications are used to ensure safety or otherwise
inform public policy, incorrect results can cause
In general, science and engi-
serious problems.
neering are competitive ﬁelds, and erroneous re-
sults put those depending on them at a relative
disadvantage. Across the board, productivity can
be severely hampered by application crashes and
the time spent diagnosing and ﬁxing misbehaving
code.

Writing tests for scientiﬁc applications is often dif-
ﬁcult and time consuming. As in any other soft-
ware project,
individual components should be
tested with unit tests, and in addition, application-
level tests are essential. If tests represent a ﬁxed set
of known input-output pairs for each component
or for the application, covering all of the various
combinations of allowed features and behaviors
generally requires an exponential number of test
cases. For tests that, individually, automatically
explore more of the allowed state space, verifying
the correct behavior is difﬁcult. Programmers of-

6

Arch.featuresExampleProg.(e.g., MM)Nano Kernel(NK) Generator(IV-B)ILP Objective Space Generator(IV-B)Nano KernelTransformedVariants (IV-B)Feature Extractor(IV-C)Algorithm 1LexiconSODFGSIS....Nano-KernelDatabase ofTransformations NK1NK1[...][...][SO, DGF, SIS][...][SIS, DGF, SO][...]KeyILP Objs.Out. features................Raising Tool(C to polyhedralmodel)Code Generator Scheduler Dependence Query (IV-D)Cost Function SufﬁxLegal SpaceBuilder (III-A)Statementpartitioner (IV-F)Dependenceranker (IV-E)ILP selector (IV-G)Algorithm 2LexiconSODFGSIS....Nano-KernelDatabase ofTransformations ten fall back on verifying only invariants, not the
complete output itself. This approach is especially
true for physical-simulation results where exact
answers are not known. Invariants (where they ex-
ist) such as conservation of energy and the appli-
cation not crashing might be the only things that
the tests actually end up checking. Tests providing
higher-conﬁdence veriﬁcation, including compar-
isons with competing applications, and numerical-
convergence analysis are often not automated and
hence are performed manually only on irregular
occasions (e.g., just before a major release). Prob-
lems discovered during these manual tests, as with
other problems discovered late in the development
cycle, are often difﬁcult to diagnose and correct.
The following are some notable challenges.

• Numerics: Many physical systems exhibit
nontrivial sensitivity to their initial condi-
tions, and thus small differences from trun-
cation error, round-off error, and other small
perturbations legitimately cause large differ-
ences in the ﬁnal results. Veriﬁcation in these
cases is subtle, sometimes relying on statisti-
cal properties to compare with known solu-
tions and sometimes relying on physical in-
variants; but even so, picking good thresholds
for numerical comparisons is often done by
manual experimentation and rules of thumb.

• Asynchronous Execution: Modern hard-
ware demands the use of concurrent, and of-
ten parallel, execution in order to take advan-
tage of the available computational capabili-
ties. This makes testing and veriﬁcation dif-
ﬁcult because concurrent execution is nearly
always nondeterministic.
It is difﬁcult to be
sure that a particular algorithmic implemen-
tation is free from race conditions and other
constraint violations under all possible execu-
tion scenarios: only a ﬁnite number, often a
small part of the overall space of possibilities,
are exhibited during testing. Pairing that test-
ing with special programs designed to detect
race conditions (e.g., valgrind, Thread Sani-
tizer) can help, but these programs add over-
heads and thus additional trade-offs into the
overall testing process. Asynchronous execu-
tion frameworks (e.g., OpenMP tasks) gen-
erally depend on programmer-declared de-

7

Report of Workshop on Program Synthesis for SC

pendencies to ensure valid task scheduling,
and mistakes in these dependency declara-
tions are sometimes difﬁcult to detect.

• Large Scale: Scientiﬁc simulations often need
to run at large scale in order to exhibit behav-
iors relevant for testing. Should something
go wrong, debugging applications running in
parallel on tens of thousands of nodes is dif-
ﬁcult. Runs of an application, which are of-
ten dispatched from batch queues at unpre-
dictable times, are expensive to recreate; and
while interactive debugging sessions can be
scheduled, turnaround times for large-scale
reservations are not fast, and even state-of-
the-art debugging tools for parallel applica-
tions have difﬁculty scaling to large systems.
In some cases, state capture and mocking can
be used to replicate and test relevant behav-
iors at smaller scales, but these are time con-
suming to implement.

• System Defects: At leadership scale, systems
and their software push the state of the art
and, as a result, may themselves have de-
fects that lead to incorrect application behav-
iors. For the largest machines, vendors are
often unable to test their software (e.g., their
MPI implementation) at the full scale of the
machine until that machine is assembled in
the customer’s data center. Early users of
these machines frequently spend a lot of time
helping track down bugs in hardware, system
software, and compilers. For these users, the
beneﬁt from being able to run unprecedented
calculations provides the motivation to con-
tinue even in the face of these kinds of issues,
but getting things working at large scale is of-
ten more difﬁcult than one might imagine.

• Performance: It is often desirable for testing
to cover not only correctness but also perfor-
mance. Application performance in scientiﬁc
computing is often a critical requirement be-
cause only a limited number of core-hours are
generally available in order to carry out par-
ticular calculations of interest. Testing of per-
formance is difﬁcult, however, both because
many testing systems run many processes
in parallel, making small-scale performance
measurements noisy, and because many per-

formance properties can be observed only
with large problem sizes at larger scales. Spe-
cial tests that extrapolate small-scale perfor-
mance tests can sometimes be constructed,
but these are often done manually because the
automation would involve even more work.

• Resource Management: As scientiﬁc soft-
ware becomes more complex, problems of-
ten appear due to unfortunate interactions be-
tween different components. A common is-
sue is resource exhaustion, where resources
such as processor cores, memory, accelera-
tors, ﬁle handles, and disk space, which are
adequately managed by the different com-
ponents in isolation, are not managed well
by the composition of the components. For
example, different components often allocate
system memory with no regard for the needs
of other components. Dedicated tests for re-
source usage can be written, but in practice
this kind of testing is also performed manu-
ally. Many of these issues surface only when
problems are being run at large scale.

Requirements for Automation

Increasing the amount of automation in the scien-
tiﬁc application development process can increase
programmer productivity, and program synthesis
can play a key role. In order to be truly helpful,
automated tools need to be integrated into the it-
erative and uncertain scientiﬁc discovery process.
Experience has taught us that a number of impor-
tant factors should be considered.

• Separation of Concerns:

Tools must be
constructed and their input formats designed
recognizing that relevant expertise is spread
across different users and different organiza-
tions. A domain scientist may understand
very well the kinds of mathematical equations
that need to be solved, an applied mathemati-
cian may understand very well what kinds
of numerical algorithms best solve different
kinds of equations, and a performance engi-
neer may understand very well how to tune
different kinds of algorithms for a particu-
lar kind of hardware; but these people might
it
not work together directly. As a result,

8

Report of Workshop on Program Synthesis for SC

must be possible to compartmentalize the rel-
evant knowledge provided to the tool, to en-
able both reuse and independent develop-
ment progress. To the extent that tools sup-
port providing mathematical proofs of cor-
the information needed for these
rectness,
proofs should be providable on component
interfaces, so the modularity can improve the
efﬁciency, understandability, and stability of
the proof process.

• Testing, Veriﬁcation, Debugging: Tools re-
quire speciﬁc features in order to assist with
veriﬁcation and debugging. Any human-
generated input can, and likely will, contain
mistakes. Mistakes can take the form of a mis-
match between the programmer’s intent [35]
and the provided input and can also stem
from the programmer’s overlooking some im-
portant aspect of the overall problem. More-
over, tools themselves can have defects. Thus,
tools must be constructed to maximize the
ability of programmers to ﬁnd their own mis-
takes and isolate tool defects [56]. To this
end, tools must generally produce informa-
tion on what they did, and why, and provide
options to embed runtime diagnostics help-
ful for tracking down problems during execu-
tion. FPDetect [28] is an example of a tool pro-
viding sophisticated, low-overhead diagnos-
tics that might be integrated with an automa-
tion workﬂow. Another example of synthesiz-
able error detectors that can trap soft errors as
well as bugs associated with incorrect index-
ing transformations is FailAmp [19], which, in
effect, makes errors more manifest for easier
detection.

• Community and Support:

State-of-the-art
science applications are complex pieces of
software, often actively used for decades, and
some are developed by large communities.
Tools automating this development process
need to integrate with existing code bases and
support the complexity of production soft-
ware. Actively used tools need to be sup-
ported by an active team. The productiv-
ity loss from issues with an undersupported
tool, whether defects or missing features, can
easily overwhelm the gains from the tool it-
self. Moreover, tools that generate hardware-

speciﬁc code need continual development to
support the latest hardware platforms. All
tools need to evolve over time to incorpo-
rate updated scientiﬁc and mathematical tech-
niques.
Support and adoption are helped
by using tools that integrate with popular
languages and programming environments,
such as C++ and C++ libraries (e.g., Kokkos).
Tools making use of production-quality com-
piler components (e.g., Clang as a library)
tend to be best positioned for success. Some-
times, however, adoption of such tools is ham-
pered by existing code bases in older or less
widely used languages (e.g., C, Fortran), and
an opportunity exists for automation tools to
assist with translating that code to newer lan-
guages.

PROGRAM SYNTHESIS

Existing Work in Scientiﬁc Programming

Claw [24]/GridTools [72]

In scientiﬁc computing, domain-speciﬁc languages
have a long history, and scientists and engineers
have increased the productivity of programming
by creating specialized translators from mathe-
matical expressions to code in C, C++, or For-
tran. While many of these tools have an im-
plicit implementation speciﬁcation, they are in-
structive in the synthesis context as demon-
strations of the kinds of high-level descriptions
of mathematical calculations that programmers
ﬁnd useful.
(sten-
cil generation for climate modeling), Kranc [43]
(stencil generation for numerical relativity), Fire-
Drake [78]/FEniCS [57] (for the generation of
ﬁnite-element solvers for partial-differential equa-
tions),
lbmpy [16] (for the generation of lattice
Boltzmann simulations), TCE [41] (for the genera-
tion of tensor-contraction expressions), and many
others have demonstrated the utility of code gen-
eration from physical equations. Likewise, for
traditional mathematical kernels, high-level code
generators have been produced (e.g., SPIRAL [33],
LGen [91], Linnea [14], TACO [47], Devito [54]),
and the codelet generator in FFTW [34]). Some of
these tools, such as SPIRAL, perform synthesis as
well as code generation because they discover new
algorithms from the exploration of relevant math-
ematical properties.

9

Report of Workshop on Program Synthesis for SC

Autotuning is a common technique used to pro-
duce high-performance code, and machine learn-
ing can be used to dynamically construct surro-
gate performance models to speed up the search
process. Autotuning can be used simply to choose
predetermined parameter values or to explore
complex implementation spaces and anything be-
tween. Stencil code generators such as PATUS [22]
and LIFT [92] use autotuning to ﬁnd the best tile
sizes. ATLAS [104] uses this technique to select
implementation parameters for linear algebra ker-
nels. Recent work, making use of the ytopt [12] au-
totuning framework, has explored directly search-
ing complex hierarchical spaces of loop nest trans-
formations, as shown in Figure 5, using a tree
search algorithm [49]. Some domain-speciﬁc lan-
guages, such as Halide, have been designed with a
separate scheduling language that can be used by
this kind of autotuning search technique.

To synthesize code that examines runtime data to
perform optimizations, inspector/executor strate-
gies [64, 84] employ inspector code that at run-
time determines data reorderings, communication
schedules for parallelism, and computation re-
orderings that are then used by the transformed
executor code [83, 94, 31, 65, 62, 38, 105, 15].
Strout and others have developed inspector/ex-
ecutor strategies such as full sparse tiling that en-
able better scaling on the node because of reduced
memory bandwidth demands [96, 97, 30, 80]. Rav-
ishankar et al. [81] composed their distributed-
memory parallelization inspector/executor trans-
formation with afﬁne transformations that enable
vector optimization. Compiler support for such
techniques consists of program analyses to deter-
mine where such parallelization is possible and
the compile-time insertion of calls to runtime li-
brary routines for performing aspects of the in-
spector [79].

Some early work applying general-purpose syn-
thesis techniques to scientiﬁc computing problems
has appeared. For example, AccSynt [25] applied
enumerative synthesis to the problem of gener-
ating efﬁcient code for GPUs. AutoPandas [17]
uses machine-learning-based search to automate
the programming of data-table-manipulating pro-
grams. Lifting, the process of extracting a high-
level model from the source code of an existing

Report of Workshop on Program Synthesis for SC

Figure 5: Autotuning search space for composed loop transformations

implementation, is important for handling legacy
code and extracting knowledge from existing code
bases [46, 21, 1]. Dexter [2], for example, can
translate C++ kernel implementations into Halide,
and OpenARC [51] can automatically translate ex-
isting OpenACC/OpenMP programs to OpenCL
specialized for FPGAs.

Existing Work in Other Areas

A lot of existing work on program synthesis has
focused on general programming tasks, such as
writing functions that operate on common data
structures like strings, lists, and trees [71, 60]. Re-
cent work has tended to focus on tools that use
natural language, behavioral examples, or both
as input. The most widely deployed synthesis
system is perhaps the Flash Fill feature in Mi-
crosoft Excel 2013+. Flash Fill can synthesize a
small program to generate some column of output
data given some columns of input data and some
ﬁlled-in examples. The combination of deep learn-
ing with the large amount of publicly available
source code (e.g., on GitHub) has provided oppor-
tunities for machine learning methods to capture
knowledge from existing code bases at a massive

scale. The resulting models can then be used for
relatively simple tasks, such as autocomplete ca-
pabilities in editors, but can also drive complex
search techniques in sophisticated synthesis sys-
tems. Past academic work in restricted domains,
such as SQLizer [106], which translates natural
languages to SQL queries, has begun inspiring
commercial implementations. One interesting as-
pect of mining data from version-control reposi-
tories is that not only is the code itself available
but its development history can also provide crit-
ical data. Systems that learn from past commits
have been able to learn likely defects and can sug-
gest ﬁxes. Sketch [90], Rosette [98], Bayou [66],
Trinity [61], HPAT [99], and many others have ex-
plored different aspects of data-driven synthesis.

The autocomplete features in modern develop-
ment environments are starting to incorporate ad-
vanced machine learning technology. Microsoft,
for example, has built complex programming-
language models for use with its technologies,
and both Codota and Tabnine provide intelligent
completion for several programming languages.
Commercial tools have started to offer more ad-
vanced programming assistance, such as suggest-

10

Experiment 0Exec �me 0:00:00.663699Func�on kernel_gemm:#pragma clang loop id(loop1)for (...) /* C:Usersmeinersbursrcmctreegemmgemm.c:82:3 */    #pragma clang loop id(loop2)    for (...) /* C:Usersmeinersbursrcmctreegemmgemm.c:83:5 */        #pragma clang loop id(loop3)        for (...) /* C:Usersmeinersbursrcmctreegemmgemm.c:84:8 */            code;Experiment 1Exec �me 0:00:00.663598Func�on kernel_gemm: #pragma clang loop(loop1) �le sizes(16) ﬂoor_ids(loop4) �le_ids(loop5)Experiment 2Exec �me 0:00:00.288233Func�on kernel_gemm:#pragma clang loop(loop1,loop2) �le sizes(16,16) ﬂoor_ids(loop8,loop6) �le_ids(loop9,loop7)Experiment 9Exec �me 0:00:00.426723#pragma clang loop(loop1,loop2,loop3) �le sizes(16,16,16) ﬂoor_ids(loop14,loop12,loop10) �le_ids(loop15,loop13,loop11)Experiment 10Exec �me 0:00:00.161591Func�on kernel_gemm:#pragma clang loop(loop1) parallelize_threadExperiment 6Exec �me 0:00:00.860978Func�on kernel_gemm: #pragma clang loop(loop1,loop2) interchange permuta�on(loop2,loop1) permuted_ids(loop17,loop18)Experiment 3Exec �me 0:00:14.372996 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)Experiment 13Exec �me 0:00:02.521868 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop3,loop1,loop2) permuted_ids(loop22,loop23,loop24)Experiment 14Exec �me 0:00:13.664162 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop3,loop2,loop1) permuted_ids(loop25,loop26,loop27)Experiment 12Exec �me 0:00:00.651948Func�on kernel_gemm:#pragma clang loop(loop2) �le sizes(16) ﬂoor_ids(loop28) �le_ids(loop29)Experiment 11Exec �me 0:00:00.616741Func�on kernel_gemm:#pragma clang loop(loop2,loop3) �le sizes(16,16) ﬂoor_ids(loop32,loop30) �le_ids(loop33,loop31)Experiment 8Func�on kernel_gemm:#pragma clang loop(loop2) parallelize_threadExperiment 7Exec �me 0:00:02.901187Func�on kernel_gemm: #pragma clang loop(loop2,loop3) interchange permuta�on(loop3,loop2) permuted_ids(loop35,loop36)Experiment 5Exec �me 0:00:00.794667Func�on kernel_gemm:#pragma clang loop(loop3) �le sizes(16) ﬂoor_ids(loop37) �le_ids(loop38)Experiment 4Exec �me 0:00:03.654259Func�on kernel_gemm:#pragma clang loop(loop3) parallelize_threadExperiment 15Exec �me 0:00:13.532768 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19) �le sizes(16) ﬂoor_ids(loop40) �le_ids(loop41)Experiment 21Exec �me 0:00:13.631089 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) �le sizes(16,16) ﬂoor_ids(loop44,loop42) �le_ids(loop45,loop43)Experiment 26Exec �me 0:00:00.968410#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20,loop21) �le sizes(16,16,16) ﬂoor_ids(loop50,loop48,loop46) �le_ids(loop51,loop49,loop47)Experiment 27 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19) parallelize_threadExperiment 28Exec �me 0:00:13.672639 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54)Experiment 22Exec �me 0:00:02.036552#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20,loop21) interchange permuta�on(loop20,loop21,loop19) permuted_ids(loop55,loop56,loop57)Experiment 25Exec �me 0:00:00.520139#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20,loop21) interchange permuta�on(loop21,loop19,loop20) permuted_ids(loop58,loop59,loop60)Experiment 23Exec �me 0:00:02.558158#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20,loop21) interchange permuta�on(loop21,loop20,loop19) permuted_ids(loop61,loop62,loop63)Experiment 24Exec �me 0:00:13.535839 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop20) �le sizes(16) ﬂoor_ids(loop64) �le_ids(loop65)Experiment 16Exec �me 0:00:02.128314 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop20,loop21) �le sizes(16,16) ﬂoor_ids(loop68,loop66) �le_ids(loop69,loop67)Experiment 18Exec �me 0:00:04.000300 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop20) parallelize_threadExperiment 19Exec �me 0:00:00.638294 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop20,loop21) interchange permuta�on(loop21,loop20) permuted_ids(loop71,loop72)Experiment 20Exec �me 0:00:13.579889 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop21) �le sizes(16) ﬂoor_ids(loop73) �le_ids(loop74)Experiment 17Exec �me 0:00:02.951613 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop21) parallelize_threadExperiment 29Exec �me 0:00:13.963746 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop53) �le sizes(16) ﬂoor_ids(loop76) �le_ids(loop77)Experiment 30Exec �me 0:00:14.352183 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop53,loop54) �le sizes(16,16) ﬂoor_ids(loop80,loop78) �le_ids(loop81,loop79)Experiment 31Exec �me 0:00:01.136404#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54)#pragma clang loop(loop53,loop54,loop21) �le sizes(16,16,16) ﬂoor_ids(loop86,loop84,loop82) �le_ids(loop87,loop85,loop83)Experiment 32Exec �me 0:00:04.184413 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop53) parallelize_threadExperiment 33Exec �me 0:00:14.304038 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop53,loop54) interchange permuta�on(loop54,loop53) permuted_ids(loop89,loop90)Experiment 34Exec �me 0:00:01.394184#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54)#pragma clang loop(loop53,loop54,loop21) interchange permuta�on(loop54,loop21,loop53) permuted_ids(loop91,loop92,loop93)Experiment 36Exec �me 0:00:03.087133#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54)#pragma clang loop(loop53,loop54,loop21) interchange permuta�on(loop21,loop53,loop54) permuted_ids(loop94,loop95,loop96)Experiment 41Exec �me 0:00:00.843855#pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21)#pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54)#pragma clang loop(loop53,loop54,loop21) interchange permuta�on(loop21,loop54,loop53) permuted_ids(loop97,loop98,loop99)Experiment 42Exec �me 0:00:14.071857 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop54) �le sizes(16) ﬂoor_ids(loop100) �le_ids(loop101)Experiment 37Exec �me 0:00:02.367299 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop54,loop21) �le sizes(16,16) ﬂoor_ids(loop104,loop102) �le_ids(loop105,loop103)Experiment 40 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop54) parallelize_threadExperiment 38Exec �me 0:00:02.425979 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop54,loop21) interchange permuta�on(loop21,loop54) permuted_ids(loop107,loop108)Experiment 39Exec �me 0:00:14.105705 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop21) �le sizes(16) ﬂoor_ids(loop109) �le_ids(loop110)Experiment 35Exec �me 0:00:05.165870 #pragma clang loop(loop1,loop2,loop3) interchange permuta�on(loop2,loop3,loop1) permuted_ids(loop19,loop20,loop21) #pragma clang loop(loop19,loop20) interchange permuta�on(loop20,loop19) permuted_ids(loop53,loop54) #pragma clang loop(loop21) parallelize_threading code refactorings to increase the productivity
of maintenance tasks. Expanding on these tech-
niques, researchers have demonstrated unsuper-
vised translation between different programming
languages [50]. In addition, considerable effort has
been put into developing techniques for program
repair. In the future, these kinds of technologies
can be made available for scientiﬁc programmers
as well, with models customized for relevant pro-
gramming languages and libraries.

Integrating sophisticated example- and
type-driven program synthesis and au-
tomated program-repair
techniques into
modern development environments re-
mains an ongoing challenge.

IDE Integration.
Integrating sophisticated
example- and type-driven program synthesis and
automated program-repair techniques into mod-
ern development environments remains an ongo-
ing challenge. The Mnemosyne project1 provides
a framework for the integration of program syn-
thesis techniques for code generation (e.g., Trin-
ity [61]), as well as type inference [75, 103] and test
generation.2 In this system independent synthesis
modules communicate with each other and with
a user’s IDE using Microsoft’s Language Server
Protocol.3 Programmatic communication between
modules enables workﬂows in which multiple
modules collaborate in multiphase synthesis pro-
cesses. For example, the results of an automated
test generation process may trigger and serve as
input to subsequent automated code synthesis or
program repair processes.

Another signiﬁcant challenge is that program syn-
thesis is often requested when the program is in-
complete, thst is, when there are missing pieces
or errors that the programmer hopes the synthe-
In other situations, how-
sizer can help with.
ever, standard techniques for parsing, typecheck-
ing, and evaluation fail. Some or all of these may
be necessary for the synthesizer to proceed. Re-
cent work on formal reasoning about incomplete

1https://grammatech.gitlab.io/Mnemosyne/docs/
2https://hypothesis.readthedocs.io/en/latest/
3https://microsoft.github.io/

language-server-protocol/

Report of Workshop on Program Synthesis for SC

programs by using holes has started to address
this issue [68, 70], and the Hazel programming en-
vironment is being designed speciﬁcally around
this hole-driven development methodology [69].
Programs with holes are also known as program
sketches in the program synthesis community [89].
Recent work on type-and-example directed pro-
gram sketching that takes advantage of these mod-
ern advances in reasoning about incomplete pro-
grams represents a promising future direction for
human-in-the-loop program synthesis [60].

Reversible Computation. The reversible compu-
tation paradigm extends the traditional forward-
only mode of computation with the ability to com-
pute deterministically in both directions, forward
It allows the program to reverse
and backward.
the effects of the forward execution and go back-
wards to a previous execution state. Reverse exe-
cution is based on the idea that for many programs
there exists an inverse program that can uncom-
pute all results of the (forward) computed pro-
gram. The inverse program can be obtained au-
tomatically either by generating reverse code from
a given forward code or by implementing the pro-
gram in a reversible programming language; and
its compiler offers the capability to automatically
generate both the forward and the inverse pro-
gram. Alternatively, an interpreter for a reversible
language can execute a program in both direc-
tions, forward and backward. For example, the
reverse C compiler presented in [74] generates re-
verse C code for a given C (forward) code. The
imperative reversible language Janus [109] allows
both the interpretation of a Janus program, where
every language construct has standard and inverse
semantics, and the generation of forward and re-
verse C code for a given Janus program.4

Over the years, a number of theoretical aspects
of reversible computing have been studied, deal-
ing with categorical foundations of reversibility,
foundations of programming languages, and term
rewriting, considering various models of sequen-
tial computations (automata, Turing machines)
as well as concurrent computations (cellular au-
tomata, process calculi, Petri nets, and membrane

4Online Janus interpreter at https://topps.diku.dk/

pirc/?id=janus\protect\leavevmode@ifvmode\kern+
.1667em\relax.

11

computing). An overview of the state of the
art and use cases can be found in [100], titled
“Reversible Computation – Extending Horizons
of Computing.” Reversible computation has at-
tracted interest for multiple applications, covering
areas as different as low-power computing [53],
high-performance computing with optimistic par-
allel discrete event simulation [85, 23, 86], robotics
In [87], the
[55], and reversible debugging [88].
generation of forward and reverse C++ code from
Janus code, as well as automatically generated
code based on incremental state saving, is system-
atically evaluated. The example discussed in de-
tail is a reversible variant of matrix multiplication
and its use in a benchmark for optimistic parallel
discrete event simulation.

Typically,

Automated Machine Learning.
the
ML pipeline comprises several components: pre-
processing, data balancing, feature engineering,
model development, hyperparameter search, and
model selection. Each of these components can
have multiple algorithmic choices, and each of
these algorithms can have different hyperparame-
ter conﬁgurations. Conﬁguring the whole pipeline
is beyond human experts, who therefore often re-
sort to trial-and-error methods, which are nonro-
bust and computationally expensive. Automated
machine learning (AutoML) [44] is a technique for
automating the design and development of an ML
pipeline. Several approaches developed for Au-
toML can be leveraged for program synthesis and
autotuning. A related ﬁeld is programming by op-
timization [42], where the algorithm designer de-
velops templates and an optimization algorithm
is used to compose these templates to ﬁnd the
right algorithm to solve a given problem. These
methods have been used to develop stochastic lo-
cal search methods for solving difﬁcult combina-
torial optimization problems.

Expanding on Existing Work

Existing work on program synthesis, while contin-
uing to evolve to address programming challenges
in a variety of domains, can be expanded to specif-
ically address the needs of the scientiﬁc program-
ming community.

• Semantic Code Equivalence/Similarity: The

12

Report of Workshop on Program Synthesis for SC

domain of semantic code similarity [73] aims
to identify whether two or more code snip-
pets (e.g., functions, classes, whole programs)
are attempting to achieve the same goal, even
if the way they go about achieving that goal
(i.e., the implementation) is largely divergent.
We believe this is one of the most critical ar-
eas of advancement in the space of machine
programming. The reason is that once seman-
tic code similarity systems demonstrate a re-
liable level of efﬁcacy, they will likely become
the backbone as well as enabling a deeper ex-
ploration of many auxiliary MP systems (e.g.,
automatic bug defection, automatic optimiza-
tions, repair) [18, 59, 108, 102, 26, 73].

Program synthesis might address chal-
lenges inherent in targeting heterogeneous
hardware architectures and generating
performance-portable code.

• Hardware Heterogeneity:

Program syn-
thesis might address challenges inherent
in targeting heterogeneous hardware archi-
in cases where specialized code
tectures,
is needed (perhaps “superoptimizations”),
where specialized algorithms are needed, and
where specialized “scheduling” functions are
needed to dynamically direct work and data
to the most appropriate hardware. Different
data structures, in addition to different loop
structures, may be required to achieve accept-
able performance on different kinds of hard-
ware.

• Performance Portability:

Program synthe-
sis might address the challenge of generating
code that performs well across a wide variety
of relevant hardware architectures [82]. This
synthesis procedure might account for later
abilities to autotune the implementation for
different target architectures.

• Data Representation Synthesis: Optimiz-
ing data movement for performance porta-
bility demands the ability to synthesize the
representation of data to take advantage of

hardware features and input data charac-
teristics such as sparsity. Data representa-
tion synthesis includes data layout consider-
ations and potentially should be coordinated
with storage mappings that specify storage
reuse. Complex interactions among algo-
rithms, memory subsystems, available paral-
lelism, and data motivate delaying data struc-
ture selection until adequate information is
available to make beneﬁcial decisions. Exist-
ing data structure synthesis research [40, 58,
107] has focused on more general relational
or map-based data structures and must be ex-
tended to scientiﬁc computing domains.

• Numerical Properties:

Scientiﬁc programs
are often characterized by numerical require-
ments for accuracy and sensitivity. Synthe-
sis techniques might address the challenges
in ﬁnding concrete implementations of math-
ematical algorithms meeting these require-
ments. This problem is made more difﬁ-
cult because these requirements are often not
Instead, requirements
explicitly speciﬁed.
might be only indirectly speciﬁed by the need
for some postprocessing step to meet its re-
quirements.
In addition, the properties be-
ing extracted from the output of the algorithm
might be fundamentally statistical in nature
(e.g., a two-point correlation function), mak-
ing veriﬁcation of the program properties it-
self a fundamentally statistical process.

• Workﬂows:

Program synthesis might ad-
dress the challenges in correctly and efﬁ-
ciently composing different analysis and sim-
ulation procedures into an overall scientiﬁc
workﬂow. This work might include the gen-
eration of specialized interfaces to enable cou-
pling otherwise-modular components with
low overhead. It might also include helping
use existing APIs in order to combine existing
components to accomplish new tasks.

• Translation: The existing code base of scien-
tiﬁc software contains large amounts of code
in C, C++, Fortran, Python, and other lan-
guages. Much of this code lacks high-level de-
scriptions; moreover, reusing codes together
that are written in different programming lan-
guages is often difﬁcult. Worse, even within

Report of Workshop on Program Synthesis for SC

the same language, different parallel or con-
current programming models do not com-
pose, for example, Kokkos vs. OpenMP vs.
SYCL in C++, OpenMP vs. OpenACC in C/-
Fortran [52], Dask vs. Ray vs. Parsl [10] in
Python. Program synthesis and semantic rep-
resentations, such as Iyer et al.’s program-
derived semantics graph [45], can help by
performing lifting and translation of existing
code between different languages and repre-
sentations and by providing better documen-
tation.

Program synthesis can act as an automated facil-
ity; but given the often ambiguous nature of ex-
pressed programming goals, synthesis tools are
expected to interact with programmers in a more
iterative manner. A program synthesis tool can
act as an intelligent advisor, offering feedback in
addition to some automated assistance. Synthesis
tools can prompt programmers for additional in-
formation. However, how to best interact with sci-
entists regarding different kinds of scientiﬁc pro-
gramming tasks is an open question in need of
further exploration.

An important relationship exists between
program synthesis and explainable AI.

An important relationship exists between program
synthesis and explainable AI. State-of-the-art pro-
gram synthesis techniques depend on deep learn-
ing and other data-driven approaches. As a re-
sult, the extent to which the program synthesis
process itself is explainable depends on explain-
able AI, including machine learning techniques.
Program synthesis itself, on the other hand, can
distill a data-driven learning and exploration pro-
cess into a set of symbolic, understandable rules.
These rules can form an explainable result from
the machine learning process and thus serve as a
technique for producing explainable AI processes.

13

Tools useful for scientiﬁc programming
may need to leverage transfer learning tech-
niques in order to apply knowledge from
larger programming domains to scientiﬁc
programming.

What are the challenges in applying program synthesis
to scientiﬁc computing and HPC problems?

• Small Sample Sizes:

Scientiﬁc program-
ming comprises a small part of the overall
programming market. This situation implies
that tools useful for scientiﬁc programming,
regardless of how the tool development is
funded, may need to leverage transfer learn-
ing techniques in order to apply knowledge
from larger programming domains to scien-
tiﬁc programming. The challenges associated
with the small sample sizes of scientiﬁc pro-
grams are exacerbated by the fact that scien-
tiﬁc programs are effectively optimizing a va-
riety of different objectives (e.g., performance,
portability, numerical accuracy), and how the
relevant trade-offs were being made by the
application developers often is unknown. The
challenge of small sample size applies not
only to the code itself but also to the set of
programmers; and since many synthesis tools
are interactive, training data on how humans
most effectively interact with the tools is re-
quired but challenging to obtain.

• Lack of Relevant Requirements Languages:
There does not currently exist, either as an
ofﬁcial or a de facto standard, an input lan-
guage capable of expressing the requirements
of a scientiﬁc application. While work has
been done on expressing correctness condi-
tions in general-purpose software, this is of-
ten focused on correctness. Covering the per-
formance requirements, numerical require-
ments, and so on needed for scientiﬁc applica-
tions has not been captured in a requirements
language. Work also has been done on ex-
tracting requirements from natural language,
unit tests, and other informal artifacts associ-
ated with existing code, and these techniques
should be applied to scientiﬁc applications as
well; but in cases where a user might wish to

14

Report of Workshop on Program Synthesis for SC

directly supply requirements, no broadly ap-
plicable way to do so currently exists. Sig-
niﬁcant domain knowledge is involved in
constructing scientiﬁc applications, but this
knowledge is often not explicitly stated, such
as how boundary conditions should be han-
dled and how accurate the results need to be.
This lack also is manifest in challenges getting
different synthesis tools to interoperate effec-
tively.

• Veriﬁcation and Validation: Verifying that
a scientiﬁc program satisﬁes its requirements
is a complicated process, and validating the
application is likewise a complicated but es-
sential part of the scientiﬁc method. The lack
of a suitable requirements language, the am-
biguities inherent in natural language speciﬁ-
cations, and other implicit parts of the code
requirements make even deﬁning what “cor-
rect” means a challenge. For many conﬁg-
urations of interest, no analytic solution ex-
ists to which one can compare with certainty.
Moreover, the use of randomized and nonde-
terministic algorithms, limited numerical sta-
bility, and the use of low-precision and ap-
proximate computing make even the process
of comparing with a reference solution difﬁ-
cult. Scale also makes veriﬁcation difﬁcult:
the computational resources required to test
an application might not be readily or regu-
larly acquired. Veriﬁcation can take multiple
forms, often all of which are important:

– A mathematical veriﬁcation (proof) of

correctness

– The results of randomized testing show-

ing no problems

– A human-understandable explanation of
what the code does and why it is correct

the base
Veriﬁcation often assumes that
system functions correctly; but especially
on leading-edge hardware, defects in the
hardware and system software can be ob-
served, and narrowing down problems actu-
ally caused by system defects is an important
goal. How to measure comprehensibility, suc-
cinctness, and naturalness and otherwise en-
sure that code can be, and is, explained is an

open question. For applications that simu-
late physical processes, the process of validat-
ing that the result matches reality sufﬁciently
well can involve expensive and sometimes
difﬁcult-to-automate physical experiments.

• Legacy Code: Existing scientiﬁc code bases
have large amounts of code in Fortran and
C, generally considered legacy programming
languages. Because of a lack of robust, mod-
ular components for processing code in these
legacy languages and because of the smaller
number of samples for training in these legacy
languages, it can be challenging to apply pro-
gram synthesis tools to code that must inter-
act with these code bases. Moreover, even if
modern languages such as C++ or Python are
used, the libraries used for scientiﬁc devel-
opment are often distinct from the libraries
used more generally across domains. Some of
these libraries use legacy interface styles (e.g.,
BLAS) that present some of the same chal-
lenges as do legacy programming languages.

• Integration and Maintenance:

The use of
tools that generate source code has long been
problematic from an integration perspective,
especially if the code generation process is
nondeterministic or depends on hardware-
speciﬁc measurements or on human interac-
tion. With all such tools, difﬁcult questions
must be addressed by the development team:

– Should the tool be run as part of the

build process?

– If run as part of the build process, builds
become nondeterministic and perhaps
slower, both because of the time con-
sumed by the tool and because the re-
quirements associated with making suf-
ﬁciently reliable performance measure-
ments may limit build parallelism.

– If not run as part of the build process,
how is the output cached? Is it part of
the source repository? How is the cache
kept synchronized with the tool’s input?
Do all developers have access to all of the
relevant hardware to update the cached
output? Alternatively, can the tool gen-
erate performance-portable code? If the

15

Report of Workshop on Program Synthesis for SC

tool is interactive, how are invocation-to-
invocation changes minimized?

Synthesis tools face challenges, not only in
interfacing with existing code, but also in in-
terfacing with each other.

• Composability: As with other software com-
ponents and tools, synthesis tools face chal-
lenges, not only in interfacing with existing
code, but also in interfacing with each other.
The properties of code generated by one tool
may need to be understood by another syn-
thesis tool, and the tools may need to iterate
together in order to ﬁnd an overall solution to
the programming challenge at hand [5]. The
Sparse Polyhedral Framework [95] provides a
possible foundation for composing data and
schedule transformations with compile-time
and runtime components in the context of
program synthesis.

• Stability and Support: Like other parts of the
development environment, program synthe-
sis tools require a plan for stability and sup-
port in order to mitigate the risk that defects
or missing features in the tools do not block
In order to transition
future science work.
from research prototypes to tools useful by
the larger scientiﬁc community, the tools must
be built on robust, well-maintained infrastruc-
tures and must be robust and well maintained
themselves.

• Search Space Modeling for Program Syn-
thesis: The search space of program syn-
thesis for scientiﬁc workloads is complex,
rendering many search algorithms ineffec-
tive. For example, in autotuning, the search
can be formulated as a noisy, mixed-integer,
nonlinear mathematical optimization prob-
lem over a decision space comprising continu-
ous, discrete, and categorical parameters. For
LLVM loop optimization, as an example, we
are faced with a dynamic search space that
changes based on the decisions at the previ-
ous step. Modeling the search space of these

problems will signiﬁcantly reduce the com-
plexity of the problem and will allow us to de-
velop effective problem-speciﬁc search meth-
ods. Application- and architecture-speciﬁc
knowledge should be incorporated as mod-
els and constraints. Moreover, by considering
metrics such as power and energy as objec-
tives rather than constraints, we can obtain a
hierarchy of solutions and quantify the sen-
sitivities associated with changing constraint
bounds. These can signiﬁcantly simplify (or
even trivialize) online optimization at run-
time, when quantities such as resource avail-
ability and system state or health are known.

What are aggressive short-term, medium-term, and
long-term opportunities and goals?

A number of opportunities exist to apply program
synthesis techniques to scientiﬁc-computing prob-
lems, and these opportunities will expand with
time.

3.5.1 Short Term (1–2 Years)

• Deﬁning Challenge and Benchmark Prob-
lems:
The community working to de-
velop new program synthesis technologies
can use challenge problems to direct their
long-term aims and enable conversations
with the scientiﬁc programming community.
In order to measure progress toward ad-
dressing those challenge problems, establish
concrete examples, and enable comparison
between systems, collections of benchmark
problems should also be developed.
Sep-
arate challenge and benchmark collections
can be developed for different classes of
problems (e.g., programming-language trans-
lation,
speciﬁcation-driven synthesis, and
example-driven synthesis).

• Interactive Synthesis, Repair, and Debug-
ging:
Constrained techniques, such as
proposing simple hole ﬁllings [60, 8] and lo-
cal source-code edits to correct user-identiﬁed
mismatches between expected and observed
application behaviors, can be intcorporated
into integrated development environment

16

Report of Workshop on Program Synthesis for SC

software that is being used to develop scien-
tiﬁc software.

• Smarter Code Templates:

Synthesis tools
can provide assistance with generating boiler-
plate code, applying design patterns, and per-
forming other largely repetitive programming
tasks. Some of these patterns occur more fre-
quently in scientiﬁc computing than in other
domains, such as halo exchange in physical
simulations.

• Numerical Precision, Accuracy, and Stabil-
ity: Synthesis tools can start addressing sit-
uations in which different options exist be-
tween algorithmic variants and the precision
used to represent numerical quantities. Auto-
matic selection between these options, based
on user-supplied metrics,
including perfor-
mance, accuracy, and stability, should be pos-
sible. A fundamental requirement is to guard
synthesis via rigorous and scalable roundoff
error estimation methods [27].

• Superoptimization and High-Quality Com-
pilation: Synthesis systems can use exhaus-
tive search techniques effectively in restricted
domains to ﬁnd the best algorithmic com-
positions and code generation for particular
systems. Superoptimization in restricted do-
mains can apply to library call sequences and
interface generation tasks.
other high-level
Compilation improvements will include more
helpful and more accurate user feedback re-
garding where and what code annotations
will be helpful.

• Knowledge Extraction: Analysis and veri-
ﬁcation tools will extract ever more precise
and relevant high-level speciﬁcations from ex-
isting implementations, often referred to as
lifting, and use these speciﬁcations to enable
interface generation, and other
veriﬁcation,
tasks [45].

• Intelligent Searching:

Intelligent code
search engines can be constructed that ac-
count for code structure, naming, comments,
associated documents (e.g., academic papers),
and other metadata. Finding code examples
and other existing solutions to related prob-

lems can signiﬁcantly increase programmer
productivity.

• Synthesis via Component Assembly: The
large-scale application frameworks in the past
two decades have used componentization and
assembly for a modest level of high-level pro-
gram construction. The key idea is to have
an infrastructural framework that becomes a
backbone that model and algorithmic compo-
nents of the solution system can plug into
as needed. While this methodology by it-
self would not sufﬁce for the multilevel paral-
lelism that we are facing now, the concept of
assembly from components can still provide
an easy way of attacking some of the perfor-
mance portability challenges by incorporating
variable granularities in how componentiza-
tion occurs. Past frameworks assumed com-
ponents at the level of separate standalone ca-
pabilities in a quest to shield the science and
core numerical algorithms from the details of
infrastructure and assembly. Allowing com-
ponentization within such standalone capa-
bilities (in a sense letting some of the infras-
tructural aspects intrude into the science sec-
tions of the code) can be helpful in reducing
replicated code for different platforms. For
example, one can view any function as hav-
ing a declaration block, blocks of arithmetic
expressions, blocks of logic and control, and
blocks of explicit data movements that can
each become a component. A subset of these
components can have multiple implementa-
tions if needed or, better still, be synthesized
from higher-level expressions. A tool that
does not care whether the implementations
exist as alternatives or are synthesized will
be relatively simple and general purpose and
can have a huge impact on productivity and
maintainability, while at the same time reduc-
ing the complexity burden on other synthesis
tools in the toolchain.

• Benchmarks: Developing a set of easy-to-
use benchmarks and well-deﬁned metrics for
comparison will be critical for advancing the
algorithms for program generation. These
benchmarks should reduce or hide the over-
head required to let researchers from other
areas to develop algorithms. For example,

Report of Workshop on Program Synthesis for SC

autotuning can beneﬁt greatly from applied
math and optimization researchers, but cur-
rently no easy-to-use framework allows these
researchers to test algorithms.

3.5.2 Medium Term (3–5 Years)

• Synthesis of Test Cases:

Based on
user-speciﬁed priorities, background domain
knowledge, and source analysis, synthesis
tools can generate test cases for an application
and its subcomponents. These span the gran-
ularity space from unit tests through whole-
application integration tests and, moreover,
can include various kinds of randomized test-
ing (e.g., “fuzz testing”) and the generation of
speciﬁc, ﬁxed tests.

• Parallelization and Programming Models:
Synthesis tools can assist with converting se-
rial code to parallel code and converting be-
tween different parallel-programming mod-
els. Parallelism exists at multiple levels, in-
cluding distributed-memory parallelism (e.g.,
expressed using MPI) and intranode paral-
lelism (e.g., expressed using OpenMP, SYCL,
or Kokkos).

• Optimized Coupling Code: Complex appli-
cations often require different subcomponents
to communicate efﬁciently, and the “glue
code” needed between different components
is often tedious to write. Synthesis tools can
create this kind of code automatically and
over the medium term can create customized
bindings that limit unnecessary copying and
format conversions. Over the longer term,
these customized data structure choices can
permeate and be optimized over the entire ap-
plication.

• Performance Portability: Generating code
that works well on a particular target archi-
tecture is challenging, but generating code
that performs well across many different tar-
get architectures adds an additional layer of
complexity. Synthesis should be able to ad-
dress this combined challenge of generat-
ing performance-portable code (using, e.g.,
OpenMP, SYCL, or Kokkos) that has good

17

performance across a wide array of different
platforms.

• Autotuning: Autotuning is becoming a
proven technology to achieve high perfor-
mance and performance portability. Despite
several promising results, however, a number
of challenges remain that we need to over-
come for a wider adoption [13]. Autotun-
ing should be made seamless and easy to
use from the application developer’s perspec-
tive. This task involves a wide range of ad-
vancements from automated kernel extraction
and large-scale search algorithms, to reduc-
ing the computationally expensive nature of
the autotuning process. Modeling objectives
such as runtime, power, and energy as func-
tions of application and platform characteris-
tics will play a central role. These models will
be used to quantify meaningful differences
across the decision space and to offer a con-
venient mechanism for exposing near-optimal
spots in the decision space.

3.5.3 Long Term

• Solving Challenge Problems:

Challenge
problems should be solvable by using com-
posable, widely available tools. These tools
incorporating back-
should be capable of
ground knowledge from a wide variety of
domains and of producing efﬁcient, veriﬁ-
able solutions in a reasonable amount of time.
Where appropriate, the code will use sensi-
ble identiﬁer names and otherwise be read-
able and maintainable.

• Intentional Programming:

Synthesis tools
can operate using high-level mathematical
and natural language speciﬁcations, largely
automatic but eliciting key feedback from hu-
man scientists, working within a common
framework that supports the tooling ecosys-
tem.

• Lifelong Learning: Synthesis tools will use
an iterative reﬁnement scheme, learning from
user feedback and automatically improving
themselves as time goes on. Synthesis tools

Report of Workshop on Program Synthesis for SC

will be able to create new abstractions for dif-
ferent domains and evolve them over time.
Recent developments in reinforcement learn-
ing can be an effective vessel for lifelong
learning.

• Understanding Legacy Code: Lifting, or the
extraction of high-level features from concrete
implementations, can work over large bodies
of code. Synthesis tools will use lifting pro-
cesses to understand legacy code, including
any necessary use of background knowledge,
and can interface with it or translate it to other
forms.

• Full-Application Veriﬁcation:

Full appli-
cations, including library dependencies, can
be veriﬁed, symbolically, statistically, or oth-
erwise, with high conﬁdence. The veriﬁca-
tion procedures can be driven automatically
based on user-provided priorities and intelli-
gent source-code analysis.

• Proxy Generation: Proxy applications, repre-
senting user-speciﬁed aspects of a full appli-
cation, can be automatically generated by syn-
thesis tools. These proxies will include appro-
priate source-code comments and documen-
tation.

• End-to-End Automation for Autotuning: Au-
totuning needs to be part of the compiler tool
chain. The process of autotuning should not
involve any manual intervention. We need
to develop autonomous, intelligent autotun-
ing technology that can surpass human capa-
bility to accelerate computational science on
extreme-scale computing platforms.

COMPILER TECHNOLOGY

Synthesis technology and compiler technology are
intimately tied.
In the end, compilers are an es-
sential consumer of the output of synthesis tools.
Moreover, synthesis tools often reuse program-
ming language processing and analysis infrastruc-
ture from compiler infrastructures in order to pro-
cess inputs and mine existing code bases for pat-
terns and other background knowledge. Improve-
ments in synthesis technology are expected to

18

go hand in hand with improvements in compiler
technology.

Improvements in synthesis technology are
expected to go hand in hand with improve-
ments in compiler technology.

What opportunities exist for improving compilers and
program analysis tools to better support scientiﬁc ap-
plications and HPC?

Compilers can better leverage machine learning
techniques to drive heuristics and other algorith-
mic tuning parameters. Examples include the fol-
lowing:

• Optimizing the order in which transforma-

tions are applied

• Optimizing the thresholds and other parame-
ters used by transformation and analysis rou-
tines, including choices between algorithms,
to balance cost vs. beneﬁt tradeoffs.

• Optimizing the sets of features used to drive

the aforementioned tuning decisions.

• Building cheap surrogate models for perfor-
mance modeling in autotuning, which can be
used to prune the large search space and iden-
tify promising regions in a short time.

Compilers and other analysis tools can produce
more information, both about the code being com-
piled and about the tool’s modeling results and
decisions. This information can be used by hu-
man engineers and by synthesis tools as part of an
iterative development process.

As hardware architectures become more complex,
with multiple levels in memory and storage hier-
archies and different computational accelerators,
compilers can offer additional support for map-
ping applications into the underlying hardware.
This may require more information regarding data
sizes, task dependencies, and so on than what
is traditionally provided to a compiler for a pro-
gramming language such as C++ or Fortran. Data
sizes, for example, affect how code is optimized:

Report of Workshop on Program Synthesis for SC

the optimal code structure for data that ﬁts into
the L1 cache may be different from that for data
that ﬁts into the L2 cache. These code structure
differences include loop structures, data structures
and layouts, and accelerator targeting.

How might higher-level information be leveraged to cap-
ture those opportunities?

A signiﬁcant challenge in implementing compilers
for Fortran, C++, and other high-performance lan-
guages used in scientiﬁc computing is the lack of
higher-level information in the source code. The
compiler generally does not know the size of data
arrays, the likely location of data within the mem-
ory subsystem, and the number of threads being
used. This lack of information forces compiler de-
velopers to fall back on heuristics that are likely to
work across a larger number of use cases instead
of creating models that can optimize speciﬁc use
cases.

When DSLs are used, some of this information is
contained in the input, and where it is not, the se-
mantics of the DSL can constrain the number of
specialized versions that the compiler might gen-
erate to a practical number. Similarly, known li-
brary interfaces can effectively form a DSL and
have similar properties that can be extracted by
an intelligent compiler to leverage in driving the
compilation process.
Separate compilation, the
general case where an application’s source code
is split across multiple separately compiled source
ﬁles,
in enabling parallelism in the
build process and other separations of concerns
but limits the higher-level information that can be
usefully extracted. Tools that build application-
scoped databases from source-analysis processes
can potentially mitigate that loss of information.
These tools, along with associated source anno-
tations and known library interfaces, can enable
high-level compiler optimizations while remain-
ing transparent to the programmer.

is helpful

How might compiler technology be improved to better
integrate with program synthesis systems?

Synthesis systems are generally iterative, combin-
ing techniques to search the space of potential
solutions with techniques for evaluating partic-
ular candidate solutions. Compiler technology

19

can be enhanced for better participation in both
parts of this process. Information extracted from
code compilation or attempted compilation of par-
tial solutions can be used to constrain the search.
In addition, information from compilation can be
used to evaluate potential solutions in terms of
both correctness and performance; however, per-
formance models from the compiler can be used
to inform a wider set of metrics.

Recent advances in compiler frameworks, such as
LLVM and its multilevel IR (MLIR), can be lever-
aged to extract suitable information from appli-
cations. Examples of features that can be easily
extracted are properties of loop-based computa-
tions that exhibit regularity, such as loop depth, ar-
ray access patterns, shape and size of multidimen-
sional arrays, and dependence structures (e.g., de-
pendence polyhedra in afﬁne programs). As these
features evolve and change within and among in-
termediate representations of a compiler, meta-
data must be collected describing the sequence of
transformations applied. Similarly, for more irreg-
ular application such as those in arising in sparse
linear systems, the sparsity pattern and structure
can be used as synthesis features and can be either
collected during the system assembly process or
provided by the end user via compiler directives.

Integrating with an iterative synthesis process may
also require new schemes to be implemented in
compilers that allow for the caching of partial
compilation and analysis results so that each vari-
ant evaluated during the synthesis process does
not incur the full overhead of performing the nec-
essary analysis and transformations each time.

When both a synthesis tool and the under-
lying compiler are responsible for code op-
timization, it is an open question how this
responsibility is best divided and how the
cooperation will be arranged.

Report of Workshop on Program Synthesis for SC

correct in the absence of higher-level information,
for example, or for the use of hand-tuned code
fragments that a machine-programming system is
unlikely to discover automatically. As work con-
tinues, open interfaces that are adopted by mul-
tiple compilers and synthesis tools will likely en-
able a diverse, vibrant ecosystem of programming-
environment capabilities.

REFERENCES

[1] Maaz Bin Safeer Ahmad and Alvin Cheung. Auto-
matically leveraging MapReduce frameworks for data-
intensive applications. In Proceedings of the 2018 Inter-
national Conference on Management of Data, SIGMOD
Conference 2018, Houston, TX, USA, June 10-15, 2018,
pages 1205–1220. ACM, 2018.

[2] Maaz Bin Safeer Ahmad, Jonathan Ragan-Kelley, Alvin
Cheung, and Shoaib Kamil. Automatically translating
image processing libraries to Halide. ACM Transac-
tions on Graphics (TOG), 38(6):1–13, 2019.

[3] Mejbah Alam,

Justin Gottschlich, Nesime Tatbul,
Javier S Turek, Tim Mattson, and Abdullah Muza-
hid. A Zero-Positive Learning Approach for Diagnos-
ing Software Performance Regressions. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dAlchBuc, E. Fox, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 32, NeurIPS 2019, pages 11623–
11635. Curran Associates, Inc., 2019.

[4] Mohammad Mejbah ul Alam and Abdullah Muzahid.
Production-run software failure diagnosis via adaptive
communication tracking.
In Proceedings of the 43rd
International Symposium on Computer Architecture,
ISBN 9781467389471.
ISCA ’16, page 354–366, 2016.
doi: 10.1109/ISCA.2016.39.

[5] Farhana Aleen and Nathan Clark. Commutativity
analysis for software parallelization: Letting program
transformations see the big picture. SIGARCH Com-
put. Archit. News, 44(1):241–252, March 2009.
ISSN
0163-5964. doi: 10.1145/2528521.1508273. URL https:
//doi.org/10.1145/2528521.1508273.

[6] Farhana Aleen, Monirul Sharif, and Santosh Pande.
Input-driven dynamic execution prediction of stream-
ing applications. In Proceedings of the 15th ACM SIG-
PLAN Symposium on Principles and Practice of Paral-
lel Programming, PPoPP ’10, pages 315–324, 2010. URL
https://doi.org/10.1145/1693453.1693494.

When both a synthesis tool and the underlying
compiler are responsible for code optimization, it
is an open question how this responsibility is best
divided and how the cooperation will be arranged.
Synthesis systems might be responsible for higher-
level transformations that are difﬁcult to prove

[7] Farhana Aleen, Vyacheslav P. Zakharin, Rakesh Kris-
haniyer, Garima Gupta, David Kreitzer, and Chang-
Sun Lin. Automated compiler optimization of multiple
vector loads/stores. In Proceedings of the ACM Inter-
national Conference on Computing Frontiers, CF ’16,
pages 82––91, New York, NY, USA, 2016. Association
for Computing Machinery. ISBN 9781450341288. doi:

20

Report of Workshop on Program Synthesis for SC

10.1145/2903150.2903169. URL https://doi.org/10.
1145/2903150.2903169.

Information Processing Systems 31, pages 3585–3597.
Curran Associates, Inc., 2018.

[8] S. An, R. Singh, S. Misailovic, and R. Samanta. Aug-
mented Example-Based Synthesis using Relational Per-
turbation Properties. Proceedings of the ACM on Pro-
gramming Languages (PACMPL), 4(POPL):56:1–56:24,
2020. doi: 10.1145/3371124.

[9] Jason Ansel, Cy Chan, Yee Lok Wong, Marek Ol-
szewski, Qin Zhao, Alan Edelman, and Saman Ama-
rasinghe. PetaBricks: A language and compiler for al-
gorithmic choice. ACM Sigplan Notices, 44(6):38–49,
2009.

[10] Yadu Babuji, Anna Woodard, Zhuozhao Li, Daniel S
Katz, Ben Clifford, Rohan Kumar, Lukasz Lacinski,
Ryan Chard, Justin M Wozniak, Ian Foster, et al. Parsl:
Pervasive parallel programming in Python. In 28th In-
ternational Symposium on High-Performance Parallel
and Distributed Computing, pages 25–36, 2019.

[11] P. Balaprakash, J. Dongarra, T. Gamblin, M. Hall, J. K.
Hollingsworth, B. Norris, and R. Vuduc. Autotun-
ing in high-performance computing applications. Pro-
ceedings of the IEEE, 106(11):2068–2083, 2018. doi:
10.1109/JPROC.2018.2841200.

[12] Prasanna Balaprakash, Romain Egele, and Paul Hov-
land. ytopt. GitHub repository. URL https://github.
com/ytopt-team/ytopt.

[13] Prasanna Balaprakash, Jack Dongarra, Todd Gamblin,
Mary Hall, Jeffrey K Hollingsworth, Boyana Norris,
and Richard Vuduc. Autotuning in high-performance
computing applications. Proceedings of the IEEE, 106
(11):2068–2083, 2018.

[14] Henrik Barthels, Christos Psarras, and Paolo Bientinesi.
Automatic generation of efﬁcient linear algebra pro-
In Proceedings of the Platform for Advanced
grams.
Scientiﬁc Computing Conference, pages 1–11, 2020.

[19] Ian Briggs, Arnab Das, Mark Baranowski, Vishal Chan-
dra Sharma, Sriram Krishnamoorthy, Zvonimir Raka-
maric, and Ganesh Gopalakrishnan. FailAmp: Rel-
ativization transformation for soft error detection in
structured address generation. ACM Trans. Archit.
Code Optim., 16(4):50:1–50:21, 2020. doi: 10.1145/
3369381. URL https://doi.org/10.1145/3369381.

[20] Lorenzo Chelini, Tobias Gysi, Tobias Grosser, Martin
Kong, and Henk Corporaal. Automatic generation of
multi-objective polyhedral compiler transformations.
In Proceedings of the ACM International Conference
on Parallel Architectures and Compilation Techniques,
pages 83–96, 2020.

[21] Alvin Cheung, Armando Solar-Lezama, and Samuel
Madden. Optimizing database-backed applications
with query synthesis. In ACM SIGPLAN Conference
on Programming Language Design and Implementa-
tion, PLDI ’13, Seattle, WA, USA, June 16-19, 2013,
pages 3–14. ACM, 2013.

[22] Matthias Christen, Olaf Schenk, and Helmar Burkhart.
PATUS: A code generation and autotuning framework
iterative stencil computations on mod-
for parallel
ern microarchitectures.
In International Parallel Dis-
tributed Processing Symposium (IPDPS’11), pages 676–
687. IEEE, 2011. doi: 10.1109/IPDPS.2011.70.

[23] Davide Cingolani, Alessandro Pellegrini, Markus
Schordan, Francesco Quaglia, and David R. Jeffer-
son. Dealing with reversibility of shared libraries in
PDES. In Proceedings of the 2017 ACM SIGSIM Con-
ference on Principles of Advanced Discrete Simulation,
SIGSIM-PADS ’17, page 41–52, New York, NY, USA,
2017. Association for Computing Machinery.
ISBN
9781450344890. doi: 10.1145/3064911.3064927. URL
https://doi.org/10.1145/3064911.3064927.

[15] Ayon Basumallik and Rudolf Eigenmann. Optimizing
irregular shared-memory applications for distributed-
memory systems. In Proceedings of the Eleventh ACM
SIGPLAN Symposium on Principles and Practice of
Parallel Programming, pages 119–128, New York, NY,
USA, 2006. ACM Press.

[24] Valentin Clement, Sylvaine Ferrachat, Oliver Fuhrer,
Xavier Lapillonne, Carlos E Osuna, Robert Pincus, Jon
Rood, and William Sawyer. The CLAW DSL: Abstrac-
tions for performance portable weather and climate
models. In Proceedings of the Platform for Advanced
Scientiﬁc Computing Conference, pages 1–10, 2018.

[16] Martin Bauer, Harald Köstler, and Ulrich Rüde. lbmpy:
for highly efﬁ-
arXiv preprint

A ﬂexible code generation toolkit
cient lattice boltzmann simulations.
arXiv:2001.11806, 2020.

[17] Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik
Sen, and Ion Stoica. AutoPandas: Neural-backed gen-
erators for program synthesis. Proceedings of the ACM
on Programming Languages, 3(OOPSLA):1–27, 2019.

[18] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
Hoeﬂer. Neural Code Comprehension: A Learn-
able Representation of Code Semantics.
In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural

21

[25] Bruce Collie.

for heteroge-
nous accelerators. URL https://baltoli.github.io/
static/msc.pdf.

Program synthesis

[26] L. D’Antoni, R. Samanta, and R. Singh. Qlose: Pro-
In Com-
gram Repair with Quantitative Objectives.
puter Aided Veriﬁcation (CAV), pages 383–401, 2016.
doi: 10.1007/978-3-319-41540-6_21.

[27] Arnab Das, Ian Briggs, Ganesh Gopalakrishnan, Sri-
ram Krishnamoorthy, and Pavel Panchekha. Scalable
yet rigorous ﬂoating-point error analysis. In Proceed-
ings of the International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis,
SC ’20. IEEE Press, 2020. ISBN 9781728199986.

[28] Arnab Das, Sriram Krishnamoorthy,

Ian Briggs,
Ganesh Gopalakrishnan, and Ramakrishna Tipireddy.
FPDetect: Efﬁcient reasoning about stencil programs
using selective direct evaluation. ACM Trans. Ar-
chit. Code Optim., 17(3):19:1–19:27, 2020. URL https:
//dl.acm.org/doi/10.1145/3402451.

[29] Eddie C Davis, Michelle Mills Strout, and Catherine
Olschanowsky. Transforming loop chains via macro
dataﬂow graphs.
In Proceedings of the 2018 Interna-
tional Symposium on Code Generation and Optimiza-
tion, pages 265–277, 2018.

[30] James Demmel, Mark Hoemmen, Marghoob Mohiyud-
din, and Katherine Yelick. Avoiding communication in
sparse matrix computations. In Proceedings of Interna-
tional Parallel and Distributed Processing Symposium
(IPDPS), Los Alamitos, CA, USA, 2008. IEEE Computer
Society.

[31] Chen Ding and Ken Kennedy. Improving cache perfor-
mance in dynamic applications through data and com-
putation reorganization at run time.
In Proceedings
of the ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 229–241,
New York, NY, USA, May 1999. ACM.

[32] H Carter Edwards, Christian R Trott, and Daniel Sun-
derland. Kokkos: Enabling manycore performance
portability through polymorphic memory access pat-
terns. Journal of Parallel and Distributed Computing,
74(12):3202–3216, 2014.

[33] Franz Franchetti, Tze Meng Low, Doru Thom Popovici,
Richard M Veras, Daniele G Spampinato, Jeremy R
Johnson, Markus Püschel, James C Hoe, and José MF
Moura. SPIRAL: Extreme performance portability. Pro-
ceedings of the IEEE, 106(11):1935–1968, 2018.

[34] Matteo Frigo and Steven G Johnson. FFTW: An adap-
tive software architecture for the FFT. In Proceedings
of the 1998 IEEE International Conference on Acous-
tics, Speech and Signal Processing, ICASSP’98 (Cat. No.
98CH36181), volume 3, pages 1381–1384. IEEE, 1998.

[35] Justin Gottschlich, Armando Solar-Lezama, Nesime
Tatbul, Michael Carbin, Martin Rinard, Regina Barzi-
lay, Saman Amarasinghe, Joshua B. Tenenbaum, and
Tim Mattson. The Three Pillars of Machine Program-
ming. In Proceedings of the 2Nd ACM SIGPLAN In-
ternational Workshop on Machine Learning and Pro-
gramming Languages, MAPL 2018, pages 69–80, New
York, NY, USA, 2018. ACM.
ISBN 978-1-4503-5834-7.
doi: 10.1145/3211346.3211355. URL http://doi.acm.
org/10.1145/3211346.3211355.

[36] S. Gulwani, O. Polozov, and R. Singh.

Program
Foundations and Trends(r) in Program-
ISBN
URL https://books.google.com/

Synthesis.
ming Languages Series. Now Publishers, 2017.
9781680832921.
books?id=mK5ctAEACAAJ.

22

Report of Workshop on Program Synthesis for SC

[37] Sumit Gulwani. Automating string processing in
spreadsheets using input-output examples. In PoPL’11,
January 26-28, 2011, Austin, Texas, USA, January 2011.

[38] Hwansoo Han and Chau-Wen Tseng. Exploiting local-
ity for irregular scientiﬁc codes. IEEE Transactions on
Parallel and Distributed Systems, 17(7):606–618, 2006.

[39] Niranjan Hasabnis and Justin Gottschlich. ControlFlag:
A self-supervised idiosyncratic pattern detection sys-
tem for software control structures. In 34th Conference
on Neural Information Processing Systems (NeurIPS
2020), ML for Systems Workshop. 2020.

[40] Peter Hawkins, Alex Aiken, Kathleen Fisher, Martin Ri-
nard, and Mooly Sagiv. Data representation synthesis.
In Proceedings of Programming Languages Design and
Implementation (PLDI), 2011.

[41] So Hirata. Tensor contraction engine: Abstraction and
automated parallel implementation of conﬁguration-
interaction, coupled-cluster, and many-body perturba-
tion theories. The Journal of Physical Chemistry A, 107
(46):9887–9897, 2003.

[42] Holger H Hoos. Programming by optimization. Com-

munications of the ACM, 55(2):70–80, 2012.

[43] Sascha Husa,

Ian Hinder, and Christiane Lechner.
Kranc: A Mathematica package to generate numeri-
cal codes for tensorial evolution equations. Computer
Physics Communications, 174(12):983–1004, 2006.

[44] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren,
editors. Automated Machine Learning: Methods, Sys-
tems, Challenges. Springer, 2018. In press, available at
http://automl.org/book.

[45] Roshni G. Iyer, Yizhou Sun, Wei Wang, and Justin
Gottschlich. Software language comprehension using a
program-derived semantics graph. In 34th Conference
on Neural Information Processing Systems (NeurIPS
2020), Computer-Assisted Programming Workshop.
2020.

[46] Shoaib Kamil, Alvin Cheung, Shachar Itzhaky, and Ar-
mando Solar-Lezama. Veriﬁed lifting of stencil com-
putations. In Proceedings of the 37th ACM SIGPLAN
Conference on Programming Language Design and
Implementation, PLDI 2016, Santa Barbara, CA, USA,
June 13-17, 2016, pages 711–726.

[47] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David
Lugato, and Saman Amarasinghe. The tensor algebra
compiler. Proceedings of the ACM on Programming
Languages, 1(OOPSLA):1–29, 2017.

[48] Martin Kong and Louis-Noël Pouchet. Model-driven
transformations for multi- and many-core CPUs.
In
Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementa-
tion, PLDI 2019, page 469–484, New York, NY, USA,
2019. Association for Computing Machinery.
ISBN
9781450367127. doi: 10.1145/3314221.3314653. URL
https://doi.org/10.1145/3314221.3314653.

[49] Michael Kruse, Xingfu Wu, and Hal Finkel. Autotun-
ing Search Space for Loop Transformations, September
2020.

structural code search. Proc. ACM Program. Lang., 3
(OOPSLA), October 2019. doi: 10.1145/3360578. URL
https://doi.org/10.1145/3360578.

Report of Workshop on Program Synthesis for SC

[50] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanus-
sot, and Guillaume Lample. Unsupervised trans-
arXiv preprint
lation of programming languages.
arXiv:2006.03511, 2020.

[51] Jacob Lambert, Seyong Lee,

Jungwon Kim,

Jef-
frey S. Vetter, and Allen D. Malony. Directive-based,
high-level programming and optimizations for high-
performance computing with FPGAs.
In ACM Inter-
national Conference on Supercomputing (ICS18), June
2018.

[52] Jacob Lambert, Seyong Lee, Jeffrey Vetter, and Allen
Malony. CCAMP: An integrated translation and opti-
mization framework for OpenACC and OpenMP.
In
2020 SC20: International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis
(SC), pages 1387–1400, Los Alamitos, CA, USA, nov
2020. IEEE Computer Society. doi: 10.1109/SC41405.
2020.00102. URL https://doi.ieeecomputersociety.
org/10.1109/SC41405.2020.00102.

[53] R. Landauer. Irreversibility and heat generation in the
computing process. IBM Journal of Research and De-
velopment, 5(3):183–191, 1961.

[54] Michael Lange, Navjot Kukreja, Mathias Louboutin,
Fabio Luporini, Felippe Vieira, Vincenzo Pandolfo,
Paulius Velesko, Paulius Kazakas, and Gerard Gorman.
Devito: Towards a generic ﬁnite difference DSL using
symbolic python. In 2016 6th Workshop on Python for
High-Performance and Scientiﬁc Computing (PyHPC),
pages 67–75. IEEE, 2016.

[55] J. S. Laursen, U. P. Schultz, and L. Ellekilde. Auto-
matic error recovery in robot assembly operations us-
ing reverse execution. In 2015 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 1785–1792, 2015.

[56] Seyong Lee, Dong Li, and Jeffrey S. Vetter. Interactive
program debugging and optimization for directive-
based GPU computing. In IEEE International Parallel
and Distributed Processing Symposium (IPDPS), May
2014.

[57] Anders Logg, Kristian B Ølgaard, Marie E Rognes, and
Garth N Wells. FFC: The FEniCS form compiler.
In
Automated Solution of Differential Equations by the
Finite Element Method, pages 227–238. Springer, 2012.

[58] Calvin Loncaric, Michael D. Ernst, and Emina Torlak.
Generalized data structure synthesis.
In Proceedings
of the 40th International Conference on Software Engi-
neering, ICSE ’18, pages 958–968, New York, NY, USA,
2018. Association for Computing Machinery.

[59] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and
Satish Chandra. Aroma: Code recommendation via

23

[60] Justin Lubin, Nick Collins, Cyrus Omar, and Ravi
Chugh. Program sketching with live bidirectional eval-
uation. Proc. ACM Program. Lang., 4(ICFP):109:1–
doi: 10.1145/3408991. URL https:
109:29, 2020.
//doi.org/10.1145/3408991.

[61] Ruben Martins, Jia Chen, Yanju Chen, Yu Feng, and Isil
Dillig. Trinity: An extensible synthesis framework for
data science. Proceedings of the VLDB Endowment, 12
(12):1914–1917, 2019. Publisher: VLDB Endowment.

[62] John Mellor-Crummey, David Whalley, and Ken
Kennedy.
Improving memory hierarchy performance
for irregular applications using data and computation
reorderings. International Journal of Parallel Program-
ming, 29(3):217–247, 2001.

[63] Reed Milewicz, Gustavo Pinto, and Paige Rodeghero.
Characterizing the roles of contributors in open-source
scientiﬁc software projects.
In 2019 IEEE/ACM 16th
International Conference on Mining Software Reposi-
tories (MSR), pages 421–432. IEEE, 2019.

[64] R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nico,
and K. Crowley. Principles of runtime support for par-
allel processors.
In Proceedings of the 2nd Interna-
tional Conference on Supercomputing, pages 140–152,
1988.

[65] Nicholas Mitchell, Larry Carter, and Jeanne Ferrante.
Localizing non-afﬁne array references. In Proceedings
of the International Conference on Parallel Architec-
tures and Compilation Techniques (PACT), pages 192–
202, Los Alamitos, CA, USA, October 1999. IEEE Com-
puter Society.

[66] Vijayaraghavan Murali, Letao Qi, Swarat Chaud-
huri, and Chris Jermaine. Neural sketch learning
arXiv preprint
for conditional program generation.
arXiv:1703.05698, 2017.

[67] Catherine Olschanowsky, Stephen Guzik, John Loffeld,
Jeffrey Hittinger, and Michelle M. Strout. A study on
balancing parallelism, data locality, and recomputation
in existing PDE solvers.
In Proceedings of the Inter-
national Conference for High Performance Comput-
ing, Networking, Storage and Analysis, pages 793–804.
IEEE Press, 2014.

[68] Cyrus Omar, Ian Voysey, Michael Hilton, Jonathan
Aldrich, and Matthew A. Hammer. Hazelnut: a bidi-
rectionally typed structure editor calculus. In Giuseppe
Castagna and Andrew D. Gordon, editors, Proceedings
of the 44th ACM SIGPLAN Symposium on Principles
of Programming Languages, POPL 2017, Paris, France,
January 18-20, 2017, pages 86–99. ACM, 2017. URL
http://dl.acm.org/citation.cfm?id=3009900.

[69] Cyrus Omar,

Ian Voysey, Michael Hilton,

Joshua
Jonathan Aldrich, and
Sunshine, Claire Le Goues,
Toward semantic founda-
Matthew A. Hammer.
tions for program editors.
In Benjamin S. Lerner,
Rastislav Bodík, and Shriram Krishnamurthi, editors,
2nd Summit on Advances in Programming Languages,
SNAPL 2017, May 7-10, 2017, Asilomar, CA, USA, vol-
ume 71 of LIPIcs, pages 11:1–11:12. Schloss Dagstuhl
- Leibniz-Zentrum für Informatik, 2017.
10.
4230/LIPIcs.SNAPL.2017.11. URL https://doi.org/
10.4230/LIPIcs.SNAPL.2017.11.

doi:

[70] Cyrus Omar, Ian Voysey, Ravi Chugh, and Matthew A.
Hammer. Live functional programming with typed
holes. Proc. ACM Program. Lang., 3(POPL):14:1–14:32,
2019. doi: 10.1145/3290327. URL https://doi.org/
10.1145/3290327.

[71] Peter-Michael Osera and Steve Zdancewic. Type-and-
example-directed program synthesis. In David Grove
and Steve Blackburn, editors, Proceedings of the 36th
ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation, Portland, OR, USA,
June 15-17, 2015, pages 619–630. ACM, 2015. doi:
10.1145/2737924.2738007. URL https://doi.org/10.
1145/2737924.2738007.

[72] Carlos Osuna. Report on the performance portabil-
ity demonstrated for the relevant weather & climate
dwarfs. arXiv preprint arXiv:1908.06094, 2019.

[73] D. M. Perry, D. Kim, R. Samanta, and X. Zhang. Sem-
Cluster: Clustering of Imperative Programming As-
signments Based on Quantitative Semantic Features.
In Programming Language Design and Implementa-
tion (PLDI), pages 860–873, 2019. doi: 10.1145/3314221.
3314629.

[74] Kalyan S. Perumalla. Introduction to Reversible Com-

puting. CRC Press Book, 2013.

[75] Michael Pradel, Georgios Gousios,

Satish Chandra.
tion with search-based validation.
arXiv:1912.03768, 2019.

Jason Liu, and
TypeWriter: Neural type predic-
arXiv preprint

[76] Jonathan Ragan-Kelley, Connelly Barnes, Andrew
Adams, Sylvain Paris, Frédo Durand, and Saman Ama-
rasinghe. Halide: A language and compiler for opti-
mizing parallelism, locality, and recomputation in im-
age processing pipelines. Acm Sigplan Notices, 48(6):
519–530, 2013.

[77] Jonathan Ragan-Kelley, Connelly Barnes, Andrew
Adams, Sylvain Paris, Frédo Durand, and Saman P.
Amarasinghe. Halide: A language and compiler
locality, and recomputa-
for optimizing parallelism,
In ACM SIG-
tion in image processing pipelines.
PLAN Conference on Programming Language Design
and Implementation, PLDI ’13, Seattle, WA, USA,
June 16-19, 2013, pages 519–530, 2013. doi: 10.1145/
2462156.2462176. URL http://doi.acm.org/10.1145/
2462156.2462176.

24

Report of Workshop on Program Synthesis for SC

[78] Florian Rathgeber, David A Ham, Lawrence Mitchell,
Michael Lange, Fabio Luporini, Andrew TT McRae,
Gheorghe-Teodor Bercea, Graham R Markall, and
Paul HJ Kelly. Firedrake: Automating the ﬁnite ele-
ment method by composing abstractions. ACM Trans-
actions on Mathematical Software (TOMS), 43(3):1–27,
2016.

[79] Lawrence Rauchwerger, Nancy M. Amato,

and
David A. Padua. A scalable method for run-time loop
International Journal of Parallel Pro-
parallelization.
gramming, 23(6):537–576, 1995.

[80] Mahesh Ravishankar,

John Eisenlohr, Louis-Noel
Pouchet, J. Ramanujam, Atanas Rountev, and P. Sa-
dayappan. Code generation for parallel execution of
a class of irregular loops on distributed memory sys-
tems. In The International Conference for High Perfor-
mance Computing, Networking, Storage, and Analysis
(SC), November 2012.

[81] Mahesh Ravishankar, Roshan Dathathri, Venmugil
Elango, Louis-Noël Pouchet, J. Ramanujam, Atanas
Rountev, and P. Sadayappan. Distributed memory code
generation for mixed irregular/regular computations.
In Proceedings of the 20th ACM SIGPLAN Symposium
on Principles and Practice of Parallel Programming,
PPoPP 2015, pages 65–75, New York, NY, USA, 2015.
ACM.

[82] Amit Sabne, Putt Sakdhnagool, Seyong Lee, and Jef-
frey S. Vetter. Understanding portability of a high-level
programming model on contemporary heterogeneous
In IEEE Micro, volume 35. IEEE, July
architectures.
2015. doi: http://dx.doi.org/10.1109/MM.2015.73.

[83] Joel Saltz, Chialin Chang, Guy Edjlali, Yuan-Shin
Hwang, Bongki Moon, Ravi Ponnusamy, Shamik
Sharma, Alan Sussman, Mustafa Uysal, Gagan
Agrawal, Raja Das, and Paul Havlak. Programming
irregular applications: Runtime support, compilation
and tools. Advances in Computers, 45:105–153, 1997.

[84] Joel H. Salz, Ravi Mirchandaney, and Kay Crowley.
Run-time parallelization and scheduling of loops. IEEE
Transactions on Computers, 40(5):603–612, 1991.

[85] Markus Schordan, David Jefferson, Peter Barnes,
Tomas Oppelstrup, and Daniel Quinlan.
Reverse
code generation for parallel discrete event simula-
tion.
In Jean Krivine and Jean-Bernard Stefani, ed-
itors, Reversible Computation, pages 95–110, Cham,
2015. Springer International Publishing.
ISBN 978-3-
319-20860-2.

[86] Markus Schordan, Tomas Oppelstrup, David R. Jeffer-
son, and Peter D. Barnes Jr. Generation of reversible
C++ code for optimistic parallel discrete event simula-
tion. New Gener. Comput., 36(3):257–280, 2018. doi:
10.1007/s00354-018-0038-2. URL https://doi.org/
10.1007/s00354-018-0038-2.

[87] Markus

Tomas

Schordan,

Oppelstrup,
and Robert Glück.
Michael Kirkedal Thomsen,
Reversible languages and incremental state saving
in optimistic parallel discrete event simulation.
In
Irek Ulidowski,
Ivan Lanese, Ulrik Pagh Schultz,
and Carla Ferreira, editors, Reversible Computation:
Extending Horizons of Computing: Selected Results
of the COST Action IC1405, pages 187–207. Springer
International Publishing, Cham, 2020.
ISBN 978-3-
030-47361-7. doi: 10.1007/978-3-030-47361-7_9. URL
https://doi.org/10.1007/978-3-030-47361-7_9.

[88] Shyh-Kwei Chen, W. K. Fuchs, and Jen-Yao Chung.
Reversible debugging using program instrumentation.
IEEE Transactions on Software Engineering, 27(8):715–
727, 2001.

[89] Armando Solar-Lezama. The sketching approach to
program synthesis. In Zhenjiang Hu, editor, Program-
ming Languages and Systems, 7th Asian Symposium,
APLAS 2009, Seoul, Korea, December 14-16, 2009.
Proceedings, volume 5904 of Lecture Notes in Com-
puter Science, pages 4–13. Springer, 2009. doi: 10.
1007/978-3-642-10672-9\_3. URL https://doi.org/
10.1007/978-3-642-10672-9_3.

[90] Armando Solar-Lezama, Gilad Arnold, Liviu Tancau,
Rastislav Bodik, Vijay Saraswat, and Sanjit Seshia.
Sketching stencils.
In Proceedings of the 28th ACM
SIGPLAN Conference on Programming Language De-
sign and Implementation, pages 167–178, 2007.

[91] Daniele G Spampinato and Markus Püschel. A ba-
sic linear algebra compiler. In Proceedings of Annual
IEEE/ACM International Symposium on Code Gener-
ation and Optimization, pages 23–32, 2014.

[92] Michel Steuwer, Toomas Remmelg, and Christophe
Dubach. Matrix multiplication beyond auto-tuning:
In Proceedings
Rewrite-based GPU code generation.
of the International Conference on Compilers, Archi-
tectures and Synthesis for Embedded Systems, CASES
’16. ACM, 2016. doi: 10.1145/2968455.2968521.

[93] Rick Stevens, Valerie Taylor, Jeff Nichols, Arthur Bar-
ney Maccabe, Katherine Yelick, and David Brown.
AI for science. Technical report, Argonne National
Lab.(ANL), Argonne, IL (United States), 2020.

[94] James M. Stichnoth and Thomas Gross. Code compo-
sition as an implementation language for compilers.
In Proceedings of the Conference on Domain-Speciﬁc
Languages (DSL-97), pages 119–132, Berkeley, Octo-
ber 15–17 1997. USENIX Association.

[95] M. M. Strout, M. Hall, and C. Olschanowsky. The
sparse polyhedral framework: Composing compiler-
generated inspector-executor code. Proceedings of the
IEEE, 106(11):1921–=1934, Nov 2018.

[96] Michelle Mills Strout, Larry Carter, Jeanne Ferrante,
Jonathan Freeman, and Barbara Kreaseck. Combin-
ing performance aspects of irregular Gauss–Seidel via

Report of Workshop on Program Synthesis for SC

sparse tiling.
In Proceedings of the 15th Workshop
on Languages and Compilers for Parallel Computing
(LCPC), Berlin / Heidelberg, July 2002. Springer.

[97] Michelle Mills Strout, Fabio Luporini, Christopher D.
Krieger, Carlo Bertolli, Gheorghe-Teodor Bercea,
Catherine Olschanowsky, J . Ramanujam, and Paul H.J.
Kelly. Generalizing run-time tiling with the loop chain
abstraction. In 28th IEEE International Parallel and Dis-
tributed Processing Symposium (IPDPS), May 2014.

[98] Emina Torlak and Rastislav Bodik. Growing solver-
aided languages with rosette.
In Proceedings of the
2013 ACM international symposium on New ideas,
new paradigms, and reﬂections on programming &
software, pages 135–152, 2013.

[99] Ehsan Totoni, Todd A Anderson, and Tatiana Shpeis-
man. HPAT: high performance analytics with scripting
ease-of-use.
In Proceedings of the International Con-
ference on Supercomputing, pages 1–10, 2017.

[100] I. Ulidowski, I. Lanese, U. Schultz, Carla Ferreira,
E. Bertino, and Carla Ferreira Eds. Reversible computa-
tion: Extending horizons of computing. LNCS, 12070,
2020. doi: 10.1007/978-3-030-47361-7.

[101] Jeffrey S Vetter, Ron Brightwell, Maya Gokhale, Pat Mc-
Cormick, Rob Ross, John Shalf, Katie Antypas, David
Donofrio, Travis Humble, Catherine Schuman, et al.
Extreme heterogeneity 2018-productive computational
science in the era of extreme heterogeneity: Report for
doe ascr workshop on extreme heterogeneity. Technical
report, USDOE Ofﬁce of Science (SC), Washington, DC
(United States), 2018.

[102] Yanjun Wang, Jinwei Liu, Dalin Zhang, and Xiaokang
Qiu. Reasoning about recursive tree traversals.
In
Proceedings of the 25th ACM SIGPLAN Symposium
on Principles and Practice of Parallel Programming,
PPoPP ’21. ACM, 2021.

[103] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig.
LambdaNet: Probabilistic type inference using graph
neural networks. arXiv preprint arXiv:2005.02161, 2020.

[104] R Clint Whaley, Antoine Petitet, and Jack J Dongarra.
Automated empirical optimizations of software and
the ATLAS project. Parallel computing, 27(1-2):3–35,
2001.

[105] Bo Wu, Zhijia Zhao, Eddy Zheng Zhang, Yunlian
Jiang, and Xipeng Shen. Complexity analysis and al-
gorithm design for reorganizing data to minimize non-
coalesced memory accesses on GPU. In Proceedings of
the 18th ACM SIGPLAN symposium on Principles and
practice of parallel programming, PPoPP ’13, pages 57–
68, New York, NY, USA, 2013. ACM.

[106] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. SQLizer: Query synthesis from natural
language. Proceedings of the ACM on Programming
Languages, 1(OOPSLA):1–26, 2017.

25

Report of Workshop on Program Synthesis for SC

[107] Cong Yan and Alvin Cheung. Generating application-
speciﬁc data layouts for in-memory databases. Proc.
VLDB Endow., 12(11):1513–1525, 2019.

[108] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan
Marcus, Nesime Tatbul, Jesmin Jahan Tithi, Niran-
jan Hasabnis, Paul Petersen, Timothy Mattson, Tim
Kraska, Pradeep Dubey, Vivek Sarkar, and Justin
Gottschlich. MISIM: A novel code similarity system,
2020.

[109] Tetsuo Yokoyama and Robert Glück. A reversible pro-
gramming language and its invertible self-interpreter.
In G. Ramalingam and Eelco Visser, editors, Proceed-
ings of the 2007 ACM SIGPLAN Workshop on Par-
tial Evaluation and Semantics-based Program Manip-
ulation, 2007, Nice, France, January 15-16, 2007, pages
144–153. ACM, 2007. URL https://doi.org/10.1145/
1244381.1244404.

[110] Tuowen Zhao, Protonu Basu, Samuel Williams, Mary
Hall, and Hans Johansen. Exploiting reuse and vector-
ization in blocked stencil computations on cpus and
gpus.
In Proceedings of the International Confer-
ence for High Performance Computing, Networking,
Storage and Analysis, SC ’19, New York, NY, USA,
2019. Association for Computing Machinery.
ISBN
9781450362290. doi: 10.1145/3295500.3356210. URL
https://doi.org/10.1145/3295500.3356210.

[111] Tuowen Zhao, Protonu Basu, Samuel Williams, Mary
Hall, and Hans Johansen.
Improving communication
by optimizing on-node data movement with data lay-
out.
In Proceedings of the ACM SIGPLAN/SIGHPC
Conference on Principles and Practice of Parallel Pro-
gramming, PPoPP’21, 2021.

26

