1
2
0
2

n
a
J

2
2

]

G
L
.
s
c
[

1
v
6
0
3
9
0
.
1
0
1
2
:
v
i
X
r
a

Partition-Based Convex Relaxations for Certifying the
Robustness of ReLU Neural Networks

Brendon G. Anderson∗
Ziye Ma†
Jingqi Li†
Somayeh Sojoudi∗†
∗Department of Mechanical Engineering, University of California, Berkeley
†Department of Electrical Engineering and Computer Sciences, University of California, Berkeley

bganderson@berkeley.edu
ziyema@berkeley.edu
jingqili@berkeley.edu
sojoudi@berkeley.edu

Abstract
In this paper, we study certifying the robustness of ReLU neural networks against adver-
sarial input perturbations. To diminish the relaxation error suﬀered by the popular linear
programming (LP) and semideﬁnite programming (SDP) certiﬁcation methods, we propose
partitioning the input uncertainty set and solving the relaxations on each part separately.
We show that this approach reduces relaxation error, and that the error is eliminated
entirely upon performing an LP relaxation with an intelligently designed partition. To
scale this approach to large networks, we consider courser partitions that take the same
form as this motivating partition. We prove that computing such a partition that directly
minimizes the LP relaxation error is NP-hard. By instead minimizing the worst-case LP
relaxation error, we develop a computationally tractable scheme with a closed-form optimal
two-part partition. We extend the analysis to the SDP, where the feasible set geometry
is exploited to design a two-part partition that minimizes the worst-case SDP relaxation
error. Experiments on IRIS classiﬁers demonstrate signiﬁcant reduction in relaxation er-
ror, oﬀering certiﬁcates that are otherwise void without partitioning. By independently
increasing the input size and the number of layers, we empirically illustrate under which
regimes the partitioned LP and SDP are best applied.
Keywords: ReLU Neural Networks, Robustness Certiﬁcation, Adversarial Attacks, Con-
vex Optimization, Partitioning

1. Introduction

It is evident that the data used in real-world engineering systems has uncertainty. This
uncertainty takes many forms, including corruptions, random measurement noise, and ad-
versarial attacks (Franceschi et al., 2018; Balunovic et al., 2019; Jin et al., 2020). Recently,
researchers have shown that the performance of neural networks can be highly sensitive to
these uncertainties in the input data (Szegedy et al., 2014; Fawzi et al., 2016; Su et al.,
2019). Clearly, safety-critical systems, such as autonomous vehicles (Bojarski et al., 2016;
Wu et al., 2017) and the power grid (Kong et al., 2017; Muralitharan et al., 2018; Pan et al.,
2019), must be robust against ﬂuctuations and uncertainty in the inputs to their decision-
making algorithms. This fact has motivated a massive inﬂux of research studying methods
to certify the robustness of neural networks against uncertainty in their input data (Wong
and Kolter, 2018; Raghunathan et al., 2018; Xiang and Johnson, 2018; Weng et al., 2018;
Zhang et al., 2018; Royo et al., 2019; Fazlyab et al., 2020; Anderson et al., 2020; Anderson
and Sojoudi, 2020a,b; Ma and Sojoudi, 2020; Jin et al., 2021).

©2021 Brendon G. Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Anderson, Ma, Li, and Sojoudi

The two primary settings taken in the robustness certiﬁcation literature consider either
random input uncertainty or adversarial uncertainty.
In the former, the neural network
input is assumed to be random and follow a known probability distribution. For instance,
the works Weng et al. (2019); Anderson and Sojoudi (2020a,b) derive high-probability guar-
antees that this randomness causes no misclassiﬁcation or unsafe output. In the adversarial
setting, the input is assumed to be unknown but contained in a prescribed input uncer-
tainty set. The goal here is to certify that all possible inputs from the uncertainty set are
mapped to outputs that the network operator deems as safe (Wong and Kolter, 2018; Royo
et al., 2019). In this paper, we take the latter, worst-case perspective. We remark that
this approach is more generally applicable than the techniques for random inputs. Indeed,
certifying that a network is robust against adversarial inputs immediately also certiﬁes it is
robust against random inputs distributed over the same uncertainty set; if the worst-case
input cannot cause a failure, then neither will randomly selected ones.

The problem of adversarial robustness certiﬁcation amounts to proving that all possible
outputs in the output set, i.e., the image of the input uncertainty set under the mapping of
the network, are contained in the safe set. However, this output set is generally nonconvex,
even when the input set is convex. Consequently, the certiﬁcation decision problem has
been shown to be NP-complete, and the optimization-based formulation for solving it is an
NP-hard, nonconvex optimization problem (Katz et al., 2017; Weng et al., 2018). To make
the problem more tractable, researchers have proposed various ways to over-approximate
the nonconvex output set with a convex one. Performing the certiﬁcation over the convex
surrogate reduces the problem to an easy-to-solve convex relaxation, and if the relaxation
issues a robustness certiﬁcate for the outer approximation, it also issues a certiﬁcate for the
true set of outputs.

Perhaps the simplest and most popular outer approximation technique is based on a lin-
ear programming (LP) relaxation of the ReLU activation function (Wong and Kolter, 2018).
However, this method has been shown to yield relatively loose outer approximations, making
it possible for the approximating set to contain unsafe outputs, even if the true output set
is entirely safe. If this occurs, the convex relaxation fails to issue a certiﬁcate of robustness,
despite the fact the network is indeed robust. A semideﬁnite programming (SDP) relaxation
was proposed in Raghunathan et al. (2018) and shown to yield tighter outer approximations
when compared to the LP method. Other methods, such as the quadratically-constrained
semideﬁnite program (Fazlyab et al., 2020), use sophisticated relaxations to tighten the
approximation, but these SDP-based methods inevitably gain accuracy at the expense of
enlarging the computational cost, and are still susceptible to relaxation error.

1.1 Related Works

Instead of resorting to exorbitantly costly relaxation techniques to lower the approximation
error, the approach we take in this paper is based on the eﬃcient method of partitioning the
input uncertainty set and solving an LP or SDP relaxation on each input uncertainty subset.
Partitioning methods are powerful and have been used to tighten bounds on optimization
problems in many areas, including robust optimization (Bertsimas and Dunning, 2016)
and deep learning (Montufar et al., 2014). A second beneﬁt of partitioning is the ability

2

Partition-Based Robustness Certification

to parallelize the optimization, reducing the computational overhead compared to more
elegant centralized methods.

We focus our attention in this paper on networks with ReLU activation functions. ReLU
networks are extremely popular for their non-vanishing gradient property and their quick
training times (Xiang and Johnson, 2018). There exist a handful of works in the literature
that utilize partitioning in the robustness certiﬁcation problem of these networks. For ex-
ample, Xiang and Johnson (2018) uses partitioning to certify the safety of neural network
controllers from the lens of reachability analysis. Their partitioning method cuts the input
uncertainty set and the outer approximation of the reachable set into hyperrectangles. Al-
though this approach extends to networks with other activations (e.g., tanh, logistic, etc.),
it fails to take into account the special piecewise linear structure of the ReLU function,
making the outer approximations unnecessarily loose for the popular ReLU networks. On
the other hand, Royo et al. (2019) uses Lagrangian duality to propose a partitioning method
specialized to ReLU activations. However, that method is restricted to splitting box-shaped
uncertainty sets in half along coordinate axes. Finally, Everett et al. (2020) provides theo-
retical guarantees for the amount of volume reduction in the outer approximation induced
by partitioning, but it only considers axis-aligned gridding of the input space. These recent
works demonstrate the increased interest in partition-based certiﬁcation, and their positive
results are evidence that partitioning yields tightened approximations with only a modest
increase in computational overhead. However, these works do not fully exploit the special
ReLU structure at hand when designing the partition, inevitably leading to conservative
bounds.

1.2 Contributions

In this paper, we fully exploit the piecewise linear structure of ReLU networks in order to
tighten convex relaxations for robustness certiﬁcation. A condensed summary of our main
contributions is as follows:

1. We prove that partitioning is theoretically guaranteed to tighten both the LP and the

SDP relaxation.

2. For the LP relaxation, we show that a methodically designed ﬁnite partition attains
zero relaxation error. We then use the structure of this motivating partition to derive
a computationally eﬃcient two-part partitioning scheme that minimizes worst-case
relaxation error. Unlike prior works, this optimal partition is not axis-aligned in
general.

3. We prove that computing the two-part partition that minimizes the actual LP relax-
ation error is NP-hard, in turn theoretically justifying our approach of minimizing the
worst-case relaxation error.

4. For the SDP relaxation, we develop a geometrically interpretable measure for how
far the solution is from being rank-1. We then show that the partition along a given
direction that minimizes this rank-1 gap is indeed given by a uniform grid. Finally,
for the specialized case of a two-part partition, we derive the partitioning axis that
minimizes the worst-case relaxation error.

3

Anderson, Ma, Li, and Sojoudi

This paper is a major extension of the conference paper Anderson et al. (2020). Notably,
all of the SDP results of this paper, as well as the third and fourth major theoretical
contributions from above, are new additions. Furthermore, two new experiments have been
added. The contributions of this paper culminate into two theoretically justiﬁed partition-
based convex relaxations for ReLU robustness certiﬁcation. Our experiments demonstrate
that the proposed approaches are eﬀective and eﬃcient, and from these results we develop
general guidance for which network size and structure regimes the LP, partitioned LP, SDP,
and partitioned SDP are best applied.

1.3 Outline

This paper is organized as follows. Section 2 introduces the ReLU robustness certiﬁcation
problem as well as its basic LP and SDP relaxations.
In Section 3, we study the eﬀect
of partitioning the input uncertainty set for the LP relaxation. After developing formal
guarantees for its eﬀectiveness, we develop a two-part partitioning strategy that optimally
reduces the worst-case relaxation error. Similarly, in Section 4 we study the partitioned SDP
relaxation and propose an optimal two-part partitioning scheme. Section 5 demonstrates
the developed methods on numerical examples and studies the eﬀectiveness of partitioning
the LP and SDP as the network grows in width and depth. Finally, we conclude in Section
6.

1.4 Notations

The set of n-vectors with real-valued elements is written as Rn. The symbol ei is reserved
to denote the ith standard basis vector of Rn for i ∈ {1, 2, . . . , n}, i.e., ei is a vector in Rn
with ith element equal to one and all other elements equal to zero. We denote the n-vector
of all ones by 1n, and for an index set I ⊆ {1, 2, . . . , n}, the symbol 1I is used to denote the
n-vector with (1I)i = 1 for i ∈ I and (1I)i = 0 for i ∈ I c = {1, 2, . . . , n} \ I. For x ∈ Rn,
we denote its ith element by xi, or, when necessary for clarity, by (x)i. We write maxi∈I xi
to mean max{xi : i ∈ I} (that is, the maximum element of x), and we deﬁne arg maxi∈I xi
to be the set {i∗ ∈ I : xi∗ = maxi∈I xi}. For a function f : Rn → Rm and the point x ∈ Rn,
we denote the ith element of the m-vector f (x) by fi(x).

The set of m × n matrices with real-valued elements is written as Rm×n, and we denote
the set of symmetric n × n matrices with real elements by Sn. For a matrix X ∈ Rm×n, we
use two indices to denote an element of X and one index to denote a column of X, unless
In particular, the (i, j) element and the ith column of X are denoted
otherwise stated.
by Xij and Xi respectively, or, when necessary for clarity, by (X)ij and (X)i respectively.
For X, Y ∈ Rm×n, we write X ≤ Y to mean Xij ≤ Yij for all i ∈ {1, 2, . . . , m} and all
j ∈ {1, 2, . . . , n}. The Hadamard (element-wise) product between X and Y is written
as X (cid:12) Y and the Hadamard division of X by Y as X (cid:11) Y . We write the rank of the
matrix X as rank(X). Furthermore, for a function f : R → R, we deﬁne f (X) to be
an m × n matrix whose (i, j) element is equal to f (Xij) for all i ∈ {1, 2, . . . , m} and all
j ∈ {1, 2, . . . , n}. If Z is a square n × n matrix, we use the notation diag(Z) to mean the
n-vector (Z11, Z22, . . . , Znn) and tr(Z) to mean the trace of Z. When Z ∈ Sn, we write
Z (cid:23) 0 to mean that Z is positive semideﬁnite.

4

Partition-Based Robustness Certification

The ReLU function is deﬁned as ReLU(·) = max{0, ·}. We use I to denote the indicator
function, i.e., I(A) = 1 if event A holds and I(A) = 0 if event A does not hold. For a
set S, we denote its cardinality by |S|. Recall that the inﬁmum and supremum of a set
T ⊆ R are the set’s greatest lower bound and least upper bound, respectively. For the set
T we respectively denote its inﬁmum and supremum by inf T and sup T . For a function
f : Rn → R and a set X ⊆ Rn, we write inf x∈X f (x) to mean inf{f (x) : x ∈ X } and
similarly for suprema. Finally, we assume that all inﬁma and suprema throughout the
paper are attained.

2. Problem Statement

2.1 Description of the Network and Uncertainty

In this paper, we consider a pre-trained K-layer ReLU neural network deﬁned by

x[0] = x,
ˆz[k] = W [k−1]x[k−1] + b[k−1],
x[k] = ReLU(ˆz[k]),

z = x[K],

(1)

for all k ∈ {1, 2, . . . , K}. Here, the neural network input is x ∈ Rnx, the output is z ∈ Rnz ,
and the kth layer’s preactivation is ˆz[k] ∈ Rnk . The parameters W [k] ∈ Rnk+1×nk and
b[k] ∈ Rnk+1 are the weight matrix and bias vector applied to the kth layer’s activation
x[k] ∈ Rnk , respectively. Without loss of generality,1 we assume that the bias terms are
accounted for in the activations x[k], thereby setting b[k] = 0 for all layers k. We let the
function f : Rnx → Rnz denote the map x (cid:55)→ z deﬁned by (1).
In the case that f is a
classiﬁcation network, the output dimension nz equals the number of classes. The problem
at hand is to certify the robustness in the neural network output z when the input x is
subject to uncertainty.

To model the input uncertainty, we assume that the network inputs are unknown but
contained in a compact set X ⊆ Rnx, called the input uncertainty set. We assume that
the set X is a convex polytope, so that the condition x ∈ X can be written as a ﬁnite
number of aﬃne inequalities and equalities. In the adversarial robustness literature, the
input uncertainty set is commonly modeled as X = {x ∈ Rnx : (cid:107)x − ¯x(cid:107)∞ ≤ (cid:15)}, where
¯x ∈ Rnx is a nominal input to the network and (cid:15) > 0 is an uncertainty radius (Wong
and Kolter, 2018; Raghunathan et al., 2018). We remark that our generalized polytopic
model for X includes this common case. The primary theme of this paper revolves around
partitioning the input uncertainty set in order to strengthen convex robustness certiﬁcation
methods. Let us brieﬂy recall the deﬁnition of a partition.

1. Note that W x + b = (cid:2)W b(cid:3)

(cid:21)
(cid:20)x
1

=: ˜W ˜x, so the bias term b can be eliminated by appending a ﬁxed

value of 1 at the end of the activation x. This parameterization can be used throughout this paper by
using matching lower and upper activation bounds of 1 in the last coordinate of each layer.

5

Anderson, Ma, Li, and Sojoudi

Deﬁnition 1 (Partition) The collection {X (j) ⊆ X : j ∈ {1, 2, . . . , p}} is said to be a
partition of the input uncertainty set X if X = ∪p
j=1X (j) and X (j) ∩ X (k) = ∅ for all j (cid:54)= k.
The set X (j) is called the jth input part.

Now, in order to describe the robustness of the network (1), we need a notion of permis-
sible outputs. For this, we consider a safe set, denoted S ⊆ Rnz . If an output z ∈ Rnz is an
element of S, we say that z is safe. It is common in the robustness certiﬁcation literature
to consider (possibly unbounded) polyhedral safe sets. We take this same perspective, and
assume that S is deﬁned by

S = {z ∈ Rnz : Cz ≤ d},
where C ∈ RnS ×nz and d ∈ RnS are given. Note that, again, our model naturally includes
the classiﬁcation setting. In particular, suppose that i∗ ∈ arg maxi∈{1,2,...,nz} fi(¯x) is the
true class of the nominal input ¯x. Then, deﬁne the ith row of C ∈ Rnz×nz to be

i = e(cid:62)
c(cid:62)

i − e(cid:62)
i∗

and d to be the zero vector. Then it is immediately clear that an input x (which can be
thought of as a perturbed version of ¯x) is safe if and only if fi(x) ≤ fi∗(x) for all i, i.e., the
network classiﬁes x into class i∗. From this classiﬁcation perspective, the safe set represents
the set of outputs without any misclassiﬁcation (with respect to the nominal input ¯x). From
here on, we consider f , X , and S in their general forms—we do not restrict ourselves to the
classiﬁcation setting.

2.2 The Robustness Certiﬁcation Problem

The fundamental goal of the robustness certiﬁcation problem is to prove that all inputs in
the input uncertainty set map to safe outputs, i.e., that f (x) ∈ S for all x ∈ X . If this
certiﬁcate holds, the network is said to be certiﬁably robust, which of course is a property
that holds with respect to a particular input uncertainty set. The robustness condition can
also be written as f (X ) ⊆ X , or equivalently

(cid:16)

sup
x∈X

c(cid:62)
i f (x) − di

(cid:17)

≤ 0 for all i ∈ {1, 2, . . . , nS},

is the ith row of C. Under this formulation, the certiﬁcation procedure amounts
where c(cid:62)
i
to solving nS optimization problems. The methods we develop in this paper can be applied
to each of these optimizations individually, and therefore in the sequel we focus on a single
optimization problem by assuming that nS = 1, namely supx∈X c(cid:62)f (x). We also assume
without loss of generality that d = 0. If d were nonzero, one may absorb d into the cost
vector c and modify the network model by appending a ﬁxed value of 1 at the end of the
output vector f (x). Under these formulations, we write the robustness certiﬁcation problem
as

f ∗(X ) = sup{c(cid:62)z : z = f (x), x ∈ X },

(2)

and recall that we seek to certify that f ∗(X ) ≤ 0.

Since the function f is in general nonconvex, the nonlinear equality constraint z = f (x)
makes the optimization (2) a nonconvex problem and the set f (X ) a nonconvex set. Fur-
thermore, since the intermediate activations and preactivations of the network generally

6

Partition-Based Robustness Certification

have a large dimension in practice, the problem (2) is typically a high-dimensional problem.
Therefore, computing an exact robustness certiﬁcate, as formulated in (2), is computa-
Instead of directly maximizing c(cid:62)z over the nonconvex output set
tionally intractable.
f (X ), one can overcome these hurdles by optimizing over a convex outer approximation
ˆf (X ) ⊇ f (X ). Indeed, this new problem is a convex relaxation of the original problem,
so it is generally easier and more eﬃcient to solve. If the convex outer approximation is
shown to be safe, i.e., ˆf (X ) ⊆ S, then the true nonconvex set f (X ) is also known to be
safe, implying that the robustness of the network is certiﬁed. Figure 1 illustrates this idea.

f

X

f (X )

ˆf (X )

Figure 1: The set ˆf (X ) is a convex outer approximation of the nonconvex set f (X ). If the

outer approximation is safe, i.e., ˆf (X ) ⊆ S, then so is f (X ).

A fundamental drawback to the convex relaxation approach to robustness certiﬁcation
is as follows: if the outer approximation ˆf (X ) is too loose, then it may intersect with the
unsafe region of the output space, meaning ˆf (X ) (cid:42) S, even in the case where the true output
set is safe. In this scenario, the convex relaxation fails to issue a certiﬁcate of robustness,
since the optimal value to the convex relaxation is positive, which incorrectly suggests the
presence of unsafe network inputs within X . This situation is illustrated in Figure 2.

safe

f (X )

ˆf (X )

unsafe

Figure 2: This scenario shows that if the convex outer approximation ˆf (X ) is too large,
meaning the relaxation is too loose, then the convex approach fails to issue a
certiﬁcate of robustness.

7

Anderson, Ma, Li, and Sojoudi

The fundamental goal of this paper is to develop convex relaxation methods for robust-
ness certiﬁcation such that the outer approximation tightly ﬁts f (X ), in eﬀect granting
strong certiﬁcates while maintaining the computational and theoretical advantages of con-
vex optimization. We focus on two popular types of convex relaxations, namely, LP (Wong
and Kolter, 2018) and SDP (Raghunathan et al., 2018) relaxations. It has been shown that
the SDP relaxation for ReLU networks is tighter than the LP relaxation, with the tradeoﬀ
of being computationally more demanding. Our general approach for both the LP and the
SDP relaxations is based on partitioning the input uncertainty set and solving a convex
relaxation on each input part separately. Throughout our theoretical development and ex-
periments, we will show that this approach presents a valid, eﬃcient, and eﬀective way to
tighten the relaxations of both LP and SDP certiﬁcations. We now turn to mathematically
formulating these relaxations.

2.3 LP Relaxation of the Network Constraints

We now introduce the LP relaxation. First, we remark that since X is bounded, the
preactivations at each layer are bounded as well. That is, for every k ∈ {1, 2, . . . , K},
there exist preactivation bounds l[k], u[k] ∈ Rnk such that l[k] ≤ ˆz[k] ≤ u[k], where ˆz[k] is
the kth layer’s preactivation in (1), for all x ∈ X . We assume without loss of generality
that l[k] ≤ 0 ≤ u[k] for all k. Although there exist various methods in the literature for
eﬃciently approximating these preactivation bounds, we consider the bounds to be tight,
i.e., ˆz[k] = l[k] for some x ∈ X , and similarly for the upper bound u[k]. Now, following
the approach initially introduced in Wong and Kolter (2018), we can relax the kth ReLU
constraint in (1) to its convex upper envelope between the preactivation bounds. This is
graphically shown in Figure 3.

x[k]
i

ReLU(ˆz[k]
i )

l[k]
i

0

ˆz[k]
i

u[k]
i

Figure 3: Relaxed ReLU constraint set N [k] at a single neuron i in layer k of the network.

We call the convex upper envelope associated with layer k the relaxed ReLU constraint

set, and its mathematical deﬁnition is given by three linear inequalities:

N [k] = {(x[k−1], x[k]) ∈ Rnk−1 × Rnk : x[k] ≤ u[k] (cid:12) (ˆz[k] − l[k]) (cid:11) (u[k] − l[k]),

x[k] ≥ 0, x[k] ≥ ˆz[k], ˆz[k] = W [k−1]x[k−1]}.

(3)

8

Partition-Based Robustness Certification

Next, we deﬁne the relaxed network constraint set to be

N = {(x, z) ∈ Rnx × Rnz : (x, x[1]) ∈ N [1], (x[1], x[2]) ∈ N [2], . . . , (x[K−1], z) ∈ N [K]}.

(4)

In essence, N is the set of all input-output pairs of the network that satisfy the relaxed
ReLU constraints at every layer. Note that, since the bounds l[k] and u[k] are determined
by the input uncertainty set X , the set N [k] is also determined by X for all layers k.

Remark 2 For networks with one hidden layer (i.e., K = 1), the single relaxed ReLU
constraint set coincides with the relaxed network constraint set: N [1] = N . Therefore, for
K = 1 we drop the k-notation and simply write z, ˆz, x, W , l, u, and N .

Finally, we introduce the LP relaxation of (2):

ˆf ∗(X ) = sup{c(cid:62)z : (x, z) ∈ N , x ∈ X }.

(5)

Notice that, if x ∈ X and z = f (x), then (x, z) ∈ N by the deﬁnition of N . Furthermore,
since X is a bounded convex polytope and N is deﬁned by a system of linear constraints,
we conﬁrm that (5) is a linear program. Therefore, (5) is indeed an LP relaxation of (2),
so it holds that

f ∗(X ) ≤ ˆf ∗(X ).

(6)

This analytically shows what Figures 1 and 2 illustrate: the condition that ˆf ∗(X ) ≤ 0 is
suﬃcient to conclude that the network is certiﬁably robust, but if ˆf ∗(X ) > 0, the relaxation
fails to certify whether or not the network is robust, since it may still hold that f ∗(X ) ≤ 0.

2.4 SDP Relaxation of the Network Constraints

An alternative convex relaxation of the robustness certiﬁcation problem can be formulated
as an SDP. This method was ﬁrst introduced in Raghunathan et al. (2018). Here, we
will introduce the SDP relaxation for a network with a single hidden layer for notational
convenience. The extension to multi-layer networks is straightforward. In this formulation,
the optimization variable (x, z) ∈ Rnx+nz is lifted to a symmetric matrix

P =









1
x
z

(cid:2)1 x(cid:62) z(cid:62)(cid:3) ∈ Snx+nz+1.

We use the following block-indexing for P :

P =





P (cid:62)
P1 P (cid:62)
z
x
Px Pxx Pxz
Pz Pzx Pzz



 ,

9

Anderson, Ma, Li, and Sojoudi

where P1 ∈ R, Px ∈ Rnx, Pz ∈ Rnz , Pxx ∈ Snx, Pzz ∈ Snz , Pxz ∈ Rnx×nz , and Pzx = P (cid:62)
xz.
This lifting procedure results in the optimization problem

c(cid:62)Pz

maximize
P ∈Snx+nz +1
subject to Pz ≥ 0,

Pz ≥ W Px,
diag(Pxx) ≤ (l + u) (cid:12) Px − l (cid:12) u,
diag(Pzz) = diag(W Pxz),
P1 = 1,
P (cid:23) 0,

rank(P ) = 1.

Here, there are no preactivation bounds, unlike the LP relaxation. The vectors l, u ∈ Rnx
are lower and upper bounds on the input, which are determined by the input uncertainty
set. For example, if X = {x ∈ Rnx : (cid:107)x − ¯x(cid:107)∞ ≤ (cid:15)}, then l = ¯x − (cid:15)1nx and u = ¯x + (cid:15)1nx.

We remark that the above problem is equivalent to the original robustness certiﬁcation
problem; no relaxation has been made yet. The only nonconvex constraint in this formu-
lation is the rank-1 constraint on P . Dropping this rank constraint, we obtain the SDP
relaxed network constraint set:

NSDP = {P ∈ Snx+nz+1 : Pz ≥ 0, Pz ≥ W Px, diag(Pzz) = diag(W Pxz),

diag(Pxx) ≤ (l + u) (cid:12) Px − l (cid:12) u, P1 = 1, P (cid:23) 0}.

(7)

Using this relaxed network constraint set as the feasible set for the optimization, we arrive
at the SDP relaxation

ˆf ∗
SDP(X ) = sup{c(cid:62)Pz : P ∈ NSDP, Px ∈ X }.
It is clear that by dropping the rank constraint, we have enlarged the feasible set, so
SDP(X ). In the

again we obtain a viable relaxation of the original problem (2): f ∗(X ) ≤ ˆf ∗
case the solution P ∗ to (8) is rank-1, we can factorize it as

(8)

P ∗ =





1
x∗
z∗





(cid:2)1 x∗(cid:62) z∗(cid:62)(cid:3)

and conclude that (x∗, z∗) solves the original nonconvex problem. However, it is generally
the case that the SDP solution will be of higher rank, leading to relaxation error and the
possibility of a void robustness certiﬁcate, similar to the LP relaxation. We now turn to
building upon the LP and SDP convex relaxations via input partitioning in order to tighten
their relaxations.

3. Partitioned LP Relaxation

3.1 Properties of Partitioned Relaxation

In this section, we investigate the properties and eﬀectiveness of partitioning the input
uncertainty set when solving the LP relaxation for robustness certiﬁcation. We start by

10

Partition-Based Robustness Certification

validating the approach, namely, by showing that solving the LP relaxation separately on
each input part maintains a theoretically guaranteed upper bound on the optimal value of
the unrelaxed problem (2). Afterwards, the approach is proven to yield a tighter upper
bound than solving the LP relaxation without partitioning.

3.1.1 Partitioning Gives Valid Relaxation

Proposition 3 (Partitioned relaxation bound) Let {X (j) ⊆ X : j ∈ {1, 2, . . . , p}} be
a partition of X . Then, it holds that

f ∗(X ) ≤ max

j∈{1,2,...,p}

ˆf ∗(X (j)).

(9)

Proof See Appendix A.

Despite the fact that Proposition 3 asserts an intuitively expected bound, we remark the
importance for its formal statement and proof. In particular, the inequality (9) serves as
the fundamental reason for why the partitioned LP relaxation can be used to certify that all
inputs in X map to safe outputs in the safe set S. Knowing that the partitioning approach
is valid for robustness certiﬁcation, we move on to studying the eﬀectiveness of partitioning.

3.1.2 Tightening of the Relaxation

We now show that the bound (6) can always be tightened by partitioning the input uncer-
tainty set. The result is given for networks with one hidden layer for simplicity. However,
the conclusion naturally generalizes to multi-layer ReLU networks.

Proposition 4 (Improving the LP relaxation bound) Consider a feedforward ReLU
neural network with one hidden layer. Let {X (j) ⊆ X : j ∈ {1, 2, . . . , p}} be a partition of
X . For the jth input part X (j), denote the corresponding preactivation bounds by l(j) and
u(j), where l ≤ l(j) ≤ W x ≤ u(j) ≤ u for all x ∈ X (j). Then, it holds that

max
j∈{1,2,...,p}

ˆf ∗(X (j)) ≤ ˆf ∗(X ).

(10)

Proof See Appendix B.

Combining Propositions 3 and 4 shows that f ∗(X ) ≤ maxj∈{1,2,...,p}

ˆf ∗(X (j)) ≤ ˆf ∗(X ),
i.e., that the partitioned LP relaxation is theoretically guaranteed to perform at least as
good as the unpartitioned LP when solving the robustness certiﬁcation problem. The
improvement in the partitioned LP relaxation is captured by the diﬀerence

ˆf ∗(X ) − max

j∈{1,2,...,p}

ˆf ∗(X (j)),

which is always nonnegative. We remark that it is possible for the improvement to be null
ˆf ∗(X (j)) = ˆf ∗(X ). This may occur when the partition used
in the sense that maxj∈{1,2,...,p}
is poorly chosen. An example of such a poor choice may be if one were to partition along
a direction in the input space that, informally speaking, corresponds to directions near-
orthogonal to the cost vector c in the output space. In this case, one would expect all im-
provements to be nulliﬁed, and for the partitioned relaxation to give the same optimal value

11

Anderson, Ma, Li, and Sojoudi

as the unpartitioned relaxation. Consequently, the following important question arises:
what constitutes a good partition so that the improvement ˆf ∗(X ) − maxj∈{1,2,...,p}
ˆf ∗(X (j))
is strictly greater than zero and maximal? We address this question in Sections 3.2 and 3.3.

3.2 Motivating Partition

In this section, we begin to answer our earlier inquiry, namely, how to choose a partition
in order to maximize the improvement ˆf ∗(X ) − maxj∈{1,2,...,p}
ˆf ∗(X (j)) in the partitioned
LP relaxation. Recall that this is equivalent to minimizing the relaxation error relative
ˆf ∗(X (j)) ≤ ˆf ∗(X ). To
to the original unrelaxed problem, since f ∗(X ) ≤ maxj∈{1,2,...,p}
this end, we construct a partition with ﬁnitely many parts, based on the parameters of
the network, which is shown to exactly recover the optimal value of the original unrelaxed
problem (2). For simplicity, we present the result for a single hidden layer, but the basic
idea of partitioning at the “kinks” of the ReLUs in order to collapse the ReLU upper
envelope onto the ReLU curve and eliminate relaxation error can be generalized to multi-
layer settings. At this point, let us remark that in Proposition 5 below, we use a slight
diﬀerence in notation for the partition. Namely, we use the set of all nz-vectors with binary
elements, J := {0, 1}nz = {0, 1} × {0, 1} × · · · × {0, 1}, to index the parts of the partition.
Under this setting, the partition is composed of p = 2nz parts, so that X (j) is the part of
the partition corresponding to the binary vector j, which is an element of the index set J .
This temporary change in notation is chosen to simplify the proof of Proposition 5.

Proposition 5 (Motivating partition) Consider a feedforward ReLU neural network
i ∈ R1×nx for all i ∈ {1, 2, . . . , nz}.
with one hidden layer and denote the ith row of W by w(cid:62)
Deﬁne J = {0, 1}nz and take the partition of X to be indexed by J , meaning that {X (j) ⊆
X : j ∈ J }, where for a given j ∈ J we deﬁne

X (j) = {x ∈ X : w(cid:62)

i x ≥ 0 for all i such that ji = 1, w(cid:62)

i x < 0 for all i such that ji = 0}.

Then, the partitioned relaxation is exact, i.e.,

f ∗(X ) = max
j∈J

ˆf ∗(X (j)).

(11)

(12)

Proof We ﬁrst show that {X (j) ⊆ X : j ∈ J } is a valid partition. Since X (j) ⊆ X for
all j ∈ J , the relation ∪j∈J X (j) ⊆ X is satisﬁed. Now, suppose that x ∈ X . Then, for all
i ∈ {1, 2, . . . , nz}, either w(cid:62)

i x < 0 holds. Deﬁne j ∈ {0, 1}nz as follows:
i x ≥ 0 or w(cid:62)
(cid:40)
1
0

if w(cid:62)
if w(cid:62)

i x ≥ 0,
i x < 0,

ji =

for all i ∈ {1, 2, . . . , nz}. Then, by the deﬁnition of X (j) in (11), it holds that x ∈ X (j).
Therefore, the relation x ∈ X implies that x ∈ X (j) for some j ∈ {0, 1}nz = J . Hence,
X ⊆ ∪j∈J X (j), and therefore ∪j∈J X (j) = X .

We now show that X (j) ∩ X (k) = ∅ for all j (cid:54)= k. Let j, k ∈ J with the property that
j (cid:54)= k. Then, there exists i ∈ {1, 2, . . . , nz} such that ji (cid:54)= ki. Let x ∈ X (j). In the case
i x ≥ 0, it holds that ji = 1 and therefore ki = 0. Hence, for all y ∈ X (k), it holds
that w(cid:62)

12

Partition-Based Robustness Certification

i y < 0, and therefore x /∈ X (k). An analogous reasoning shows that x /∈ X (k) when
i x < 0. Therefore, one concludes that x ∈ X (j) and j (cid:54)= k implies that x /∈ X (k), i.e.,

that w(cid:62)
w(cid:62)
that X (j) ∩ X (k) = ∅. Hence, {X (j) ⊆ X : j ∈ J } is a valid partition.

We now prove (12). Let j ∈ J . Since w(cid:62)

lower bound becomes l(j)
such that ji = 0, the preactivation upper bound becomes u(j)
the relaxed network constraint set (4) for the jth input part reduces to

i = 0 for all such i. On the other hand, since w(cid:62)

i x ≥ 0 for all i such that ji = 1, the preactivation
i x < 0 for all i
i = 0 for all such i. Therefore,

N (j) = {(x, z) ∈ Rnx × Rnz : zi = 0 for all i such that ji = 0,

zi = w(cid:62)

i x = (W x)i for all i such that ji = 1}.

That is, the relaxed ReLU constraint envelope collapses to the exact ReLU constraint
through the prior knowledge of each preactivation coordinate’s sign. Therefore, we ﬁnd
that for all x ∈ X (j) it holds that (x, z) ∈ N (j) if and only if z = ReLU(W x). Hence, the
LP over the jth input part yields that

ˆf ∗(X (j)) = sup{c(cid:62)z : (x, z) ∈ N (j), x ∈ X (j)} = sup{c(cid:62)z : z = ReLU(W x), x ∈ X (j)}
≤ sup{c(cid:62)z : z = ReLU(W x), x ∈ X } = f ∗(X ).

Since j was chosen arbitrarily, it holds that

ˆf ∗(X (j)) ≤ f ∗(X ).

max
j∈J

Since f ∗(X ) ≤ maxj∈J
desired.

ˆf ∗(X (j)) by the relaxation bound (9), the equality (12) holds, as

Although the partition proposed in Proposition 5 completely eliminates relaxation error
of the LP, using it in practice may be computationally intractable, as it requires solving 2nz
separate linear programs. Despite this limitation, the result provides two major theoretical
implications. First, our input partitioning approach is fundamentally shown to be a simple,
yet very powerful method, as the robustness certiﬁcation problem can be solved exactly via a
ﬁnite number of linear program subproblems. Second, the partition proposed in Proposition
5 shows us the structure of an optimal partition, namely that the parts of the partition are
deﬁned by the half-spaces generated by the rows of W (see Figure 4). This result paves the
way to develop a tractable two-part partition that incorporates the optimal reduction in
relaxation error endowed by the structure of this motivating partition. In the next section,
we explore this idea further, and answer the following question: if we only partition along
a single row of the weight matrix, which one is the best to choose?

3.3 Partitioning Scheme

In this section, we propose an explicit, computationally tractable, and eﬀective LP parti-
tioning scheme. The partitioning scheme is developed based on analyses for a single hidden
layer. However, the resulting partitioning scheme is still applicable to multi-layer networks,
and indeed will be shown to remain eﬀective on two-layer networks in the experiments of

13

Anderson, Ma, Li, and Sojoudi

wi

zi

ReLU(ˆzi)

X (1)
i

X (2)
i

li

0

ˆzi

ui

Figure 4: Partitioning based on row w(cid:62)

exact ReLU constraint in coordinate i over the two resulting input parts X (1)
{x ∈ X : w(cid:62)
.

i of the weight matrix. This partition results in an
i =

i x ≥ 0} and X (2)

i = X \ X (1)

i

i = {x ∈ X : w(cid:62)

Section 5. The development of the partition boils down to two main ideas. First, we restrict
our attention to two-part partitions deﬁned by rows of the weight matrix W , speciﬁcally,
i = X \ X (1)
X (1)
, as motivated in the previous section.
Second, we seek which index i ∈ {1, 2, . . . , nz} gives the best partition, in the sense that the
relaxation error of the resulting partitioned LP is minimized. As will be shown in Section
3.3.3, this second aspect is NP-hard to discern in general. Therefore, to ﬁnd the optimal
row to partition along, we instead seek to minimize the worst-case relaxation error.

i x ≥ 0} and X (2)

i

3.3.1 Worst-Case Relaxation Bound

We begin by bounding the relaxation error below.

Theorem 6 (Worst-case relaxation bound) Consider a feedforward ReLU neural net-
work with one hidden layer, with the input uncertainty set X and preactivation bounds
l, u ∈ Rnz . Consider also the relaxation error ∆f ∗(X ) := ˆf ∗(X ) − f ∗(X ). Let (˜x∗, ˜z∗) and
(x∗, z∗) be optimal solutions for the relaxation ˆf ∗(X ) and the unrelaxed problem f ∗(X ), re-
spectively. Given an arbitrary norm (cid:107) · (cid:107) on Rnx, there exists (cid:15) ∈ R such that (cid:107)˜x∗ − x∗(cid:107) ≤ (cid:15)
and

∆f ∗(X ) ≤

nz(cid:88)

i=1

(cid:18)

ReLU(ci)

ui
ui − li

(min{(cid:15)(cid:107)wi(cid:107)∗, ui} − li) + ReLU(−ci) min{(cid:15)(cid:107)wi(cid:107)∗, ui}

,

(cid:19)

(13)

where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107).

Proof First, since X is bounded, there exists (cid:15) ≥ 0 such that (cid:107)˜x∗ −x∗(cid:107) ≤ (cid:15). The deﬁnitions
of (˜x∗, ˜z∗) and (x∗, z∗) give that

∆f ∗(X ) =

nz(cid:88)

i=1

ci(˜z∗

i − z∗

i ) ≤

nz(cid:88)

i=1

∆f ∗
i ,

(14)

14

Partition-Based Robustness Certification

where

(cid:26)

∆f ∗

i = sup

ci(˜zi − zi) : zi = ReLU(w(cid:62)

i x), ˜zi ≥ 0, ˜zi ≥ w(cid:62)

i ˜x, ˜zi ≤

ui
ui − li

(w(cid:62)

i ˜x − li),

(cid:27)

(cid:107)˜x∗ − x∗(cid:107) ≤ (cid:15), x, ˜x ∈ X

for all i ∈ {1, 2, . . . , nz}. Note that

(cid:26)

∆f ∗

i = sup

ci(˜zi − zi) : zi = ReLU(ˆzi), ˜zi ≥ 0, ˜zi ≥ ˆ˜zi, ˜zi ≤

ui
ui − li

(ˆ˜zi − li),

(cid:107)˜x∗ − x∗(cid:107) ≤ (cid:15), ˆz = W x, ˆ˜z = W ˜x, x, ˜x ∈ X

(cid:27)

.

If x, ˜x ∈ X and ˆz, ˆ˜z satisfy ˆz = W x, ˆ˜z = W ˜x, and (cid:107)˜x − x(cid:107) ≤ (cid:15), then they satisfy l ≤ ˆz, ˆ˜z ≤ u
and |ˆ˜zi − ˆzi| = |w(cid:62)
i (˜x − x)| ≤ (cid:107)wi(cid:107)∗(cid:107)˜x − x(cid:107) ≤ (cid:15)(cid:107)wi(cid:107)∗ for all i ∈ {1, 2, . . . , nz} by the
Cauchy-Schwarz inequality for dual norms. Therefore,

(cid:26)

∆f ∗

i ≤ sup

ci(˜zi − zi) : zi = ReLU(ˆzi), ˜zi ≥ 0, ˜zi ≥ ˆ˜zi, ˜zi ≤

ui
ui − li

(ˆ˜zi − li),
(cid:27)

l ≤ ˆz, ˆ˜z ≤ u, |ˆ˜zk − ˆzk| ≤ (cid:15)(cid:107)wk(cid:107)∗ for all k ∈ {1, 2, . . . , nz}, ˆz, ˆ˜z ∈ Rnz

(cid:26)

= sup

ci(˜zi − zi) : zi = ReLU(ˆzi), ˜zi ≥ 0, ˜zi ≥ ˆ˜zi, ˜zi ≤

li ≤ ˆzi, ˆ˜zi ≤ ui, |ˆ˜zi − ˆzi| ≤ (cid:15)(cid:107)wi(cid:107)∗, ˆzi, ˆ˜zi ∈ R

(cid:27)

.

ui
ui − li

(ˆ˜zi − li),

For ci ≥ 0, the above inequality yields that

(cid:26)

∆f ∗

i ≤ ci sup

˜zi − zi : zi = ReLU(ˆzi), ˜zi ≥ 0, ˜zi ≥ ˆ˜zi, ˜zi ≤

ui
ui − li

(ˆ˜zi − li),

li ≤ ˆzi, ˆ˜zi ≤ ui, |ˆ˜zi − ˆzi| ≤ (cid:15)(cid:107)wi(cid:107)∗, ˆzi, ˆ˜zi ∈ R

(cid:27)

.

The optimal solution to the above supremum is readily found by comparing the line ˜zi =
(ˆ˜zi − li) to the function zi = ReLU(ˆzi) over ˆ˜zi, ˆzi ∈ [li, ui]. In particular, the maximum
ui
ui−li
distance between ˜zi and zi on the above feasible set occurs when zi = ˆzi = 0, ˆ˜zi = (cid:15)(cid:107)wi(cid:107)∗,
and ˜zi = ui
ui−li

((cid:15)(cid:107)wi(cid:107)∗ − li). Therefore, we ﬁnd that

∆f ∗

i ≤ ci

ui
ui − li

((cid:15)(cid:107)wi(cid:107)∗ − li),

(15)

for all i ∈ {1, 2, . . . , nz} such that ci ≥ 0. We also note the trivial bound that ˜zi − zi ≤ ui
on the feasible set of the above supremum, so that

∆f ∗

i ≤ ciui = ci

ui
ui − li

(ui − li).

15

(16)

Anderson, Ma, Li, and Sojoudi

The inequalities (15) and (16) together imply that

∆f ∗

i ≤ ci

ui
ui − li

(min{(cid:15)(cid:107)wi(cid:107)∗, ui} − li)

(17)

for all i ∈ {1, 2, . . . , nz} such that ci ≥ 0.

On the other hand, for all i ∈ {1, 2, . . . , nz} such that ci < 0, we have that

(cid:26)

∆f ∗

i ≤ ci inf

˜zi − zi : zi = ReLU(ˆzi), ˜zi ≥ 0, ˜zi ≥ ˆ˜zi, ˜zi ≤

ui
ui − li

(ˆ˜zi − li),

li ≤ ˆzi, ˆ˜zi ≤ ui, |ˆ˜zi − ˆzi| ≤ (cid:15)(cid:107)wi(cid:107)∗, ˆzi, ˆ˜zi ∈ R

(cid:27)

.

The optimal solution to the above inﬁmum is readily found by comparing the line ˜zi = 0 to
the function zi = ReLU(ˆzi) over ˆ˜zi, ˆzi ∈ [li, ui]. In particular, the minimum value of ˜zi − zi
on the above feasible set occurs when ˜zi = ˆ˜zi = 0 and zi = ˆzi = (cid:15)(cid:107)wi(cid:107)∗. Therefore, we ﬁnd
that

∆f ∗

i ≤ −ci(cid:15)(cid:107)wi(cid:107)∗,

(18)

for all i ∈ {1, 2, . . . , nz} such that ci < 0. We also note the trivial bound that ˜zi − zi ≥ −ui
on the feasible set of the above inﬁmum, so that

The inequalities (18) and (19) together imply that

∆f ∗

i ≤ −ciui.

∆f ∗

i ≤ −ci min{(cid:15)(cid:107)wi(cid:107)∗, ui}

(19)

(20)

for all i ∈ {1, 2, . . . , nz} such that ci < 0. Substituting (17) and (20) into (14) gives the
desired bound (13).

The value ∆f ∗

i in the proof of Theorem 6 can be interpreted as the worst-case relaxation
error in coordinate i. From this perspective, Theorem 6 gives an upper bound on the worst-
case relaxation error of the overall network. In the case that ˜x∗ (cid:54)= x∗ and (cid:15)(cid:107)wi(cid:107)∗ ≥ ui for
all (cid:15) ∈ R such that (cid:107)˜x∗ − x∗(cid:107) ≤ (cid:15), for all i ∈ {1, 2, . . . , nz}, the bound (13) becomes

∆f ∗(X ) ≤

nz(cid:88)

i=1

|ci|ui.

This is the loosest the bound can ever be. On the contrary, if ˜x∗ = x∗, i.e., the relaxation
and the true certiﬁcation problem share an optimal input (meaning that it is the most
adversarial), then Theorem 6 holds for (cid:15) = 0. Substituting this into (13) gives

∆f ∗(X ) ≤ −

nz(cid:88)

i=1

ReLU(ci)

uili
ui − li

.

(21)

Note that in practice we expect the condition (cid:15) ≈ 0 to hold, since a worst-case input to a
neural network is likely to also be a worst-case input to the relaxed network. Therefore, for

16

Partition-Based Robustness Certification

the remainder of this paper, we take the worst-case LP relaxation bound to be that given
by (21) to simplify the analysis.

To continue our development of a two-part partitioning scheme that is optimal with
respect to the worst-case relaxation error, we use (21) to bound the relaxation error of the
partitioned LP in terms of the row w(cid:62)
i that is used to deﬁne the partition. This bound is
given in the following lemma.

Lemma 7 (Two-part bound) Let i ∈ {1, 2, . . . , nz} and consider a two-part partition of
X given by {X (1)
. Consider
i
also the partitioned relaxation error ∆f ∗({X (1)
) − f ∗(X ). It
holds that

i x ≥ 0} and X (2)
i }) := maxj∈{1,2}

i = {x ∈ X : w(cid:62)
, X (2)

i = X \ X (1)
ˆf ∗(X (j)
i

i }, where X (1)

, X (2)

i

i

∆f ∗({X (1)

i

, X (2)

i }) ≤ −

ReLU(ck)

uklk
uk − lk

.

(22)

nz(cid:88)

k=1
k(cid:54)=i

Proof Consider the relaxation solved over the ﬁrst input part, X (1)
l(1), u(1) ∈ Rnz the corresponding preactivation bounds. Since w(cid:62)
part, the preactivation bounds for the ﬁrst subproblem ˆf ∗(X (1)

i

, and denote by
i x ≥ 0 on this input

) can be taken as

i

l(1) = (l1, l2, . . . , li−1, 0, li+1, . . . , lnz )

and u(1) = u. It follows from (21) that

ˆf ∗(X (1)
i

) − f ∗(X (1)

i

) ≤ −

nz(cid:88)

k=1

ReLU(ck)

u(1)
k l(1)
k − l(1)
u(1)

k

k

= −

nz(cid:88)

k=1
k(cid:54)=i

ReLU(ck)

uklk
uk − lk

.

(23)

Similarly, over the second input part, X (2)
bounds for the second subproblem ˆf ∗(X (2)

i

, we have that w(cid:62)
) can be taken as l(2) = l and

i x < 0, and so the preactivation

i

u(2) = (u1, u2, . . . , ui−1, 0, ui+1, . . . , unz ),

resulting in the same bound as in (23):

ˆf ∗(X (2)
i

) − f ∗(X (2)

i

) ≤ −

ReLU(ck)

uklk
uk − lk

.

nz(cid:88)

k=1
k(cid:54)=i

(24)

Putting the two bounds (23) and (24) together and using the fact that f ∗(X (j)
for all j ∈ {1, 2}, we ﬁnd that

i

) ≤ f ∗(X )

∆f ∗({X (1)

i

, X (2)

i }) = max
j∈{1,2}

(cid:16) ˆf ∗(X (j)

i

(cid:17)
) − f ∗(X )

≤ max
j∈{1,2}

(cid:16) ˆf ∗(X (j)

i

) − f ∗(X (j)

i

(cid:17)

)

as desired.

≤ −

nz(cid:88)

k=1
k(cid:54)=i

ReLU(ck)

uklk
uk − lk

,

17

Anderson, Ma, Li, and Sojoudi

3.3.2 Proposed Partition

Lemma 7 bounds the worst-case relaxation error for each possible row-based partition.
Therefore, our ﬁnal step in the development of our two-part partition is to ﬁnd which row
minimizes the upper bound (22). This optimal two-part partition is now presented.

Theorem 8 (Optimal partition) Consider the two-part partitions deﬁned by the rows
of W : {X (1)
for all
i ∈ {1, 2, . . . , nz} =: I. The optimal partition that minimizes the worst-case relaxation
error bound in (22) is given by

i x ≥ 0} and X (2)

i = {x ∈ X : w(cid:62)

i }, where X (1)

i = X \ X (1)

, X (2)

i

i

i∗ ∈ arg min
i∈I

ReLU(ci)

uili
ui − li

.

(25)

Proof Minimizing the bound in (22) of Lemma 7 over the partition i gives rise to



min
i∈I

−





nz(cid:88)

k=1
k(cid:54)=i

as desired.



ReLU(ck)

uklk
uk − lk





= −

nz(cid:88)

k=1

ReLU(ck)

uklk
uk − lk

+ min
i∈I

ReLU(ci)

uili
ui − li

,

Theorem 8 provides the two-part partition that optimally reduces the worst-case relax-
ation error that we seek. We remark its simplicity: to decide which row to partition along,
it suﬃces to enumerate the values ReLU(ci) uili
for i ∈ {1, 2, . . . , nz}, then choose the
ui−li
row corresponding to the minimum amongst these values. Note that this optimization over
i scales linearly with the dimension nz, and the resulting LP subproblems on each input
part only require the addition of one extra linear constraint, meaning that this partitioning
scheme is highly eﬃcient.

We also note that Theorem 8 can be immediately extended to design multi-part par-
titions in two interesting ways. First, by ordering the values ReLU(ci) uili
, we are or-
ui−li
dering the optimality of the rows w(cid:62)
to partition along. Therefore, by partitioning along
i
the np > 1 rows corresponding to the smallest np of these values, Theorem 8 provides
a strategy to design an eﬀective 2np-part partition, in the case one prefers to perform
more than just a two-part partition. Second, Theorem 8 can be used in a recursive way,
similar to branch-and-bound. In particular, by solving the two-part partitioned LP using
Theorem 8, we ﬁnd ˆf ∗(X (1)
i∗ ), then we can further
partition X (1)
, again according to (25) but this
i∗
time using the tighter preactivation bounds on X (1)
i∗ . Then our relaxation bound becomes
f ∗(X ) ≤ max{ ˆf ∗(X (1,1)
i∗ )}. Choosing the
part amongst X (1,1)
i∗ with the largest LP relaxation value, we can again perform
a partition and repeat the process in order to continue reducing the relaxation error.

i∗ ). If ˆf ∗(X (1)
i∗ ) > ˆf ∗(X (2)
and X (1,2)
i∗

i∗ ) and ˆf ∗(X (2)
i∗

into two more parts, say X (1,1)

i∗ )} ≤ max{ ˆf ∗(X (1)

), ˆf ∗(X (1,2)
i∗
, X (2)

i∗ ), ˆf ∗(X (2)

i∗
, X (1,2)
i∗

), ˆf ∗(X (2)

i∗

3.3.3 Optimal Partitioning is NP-Hard

In this section, we show that ﬁnding a row-based partition that minimizes the actual LP
relaxation error is an NP-hard problem. Recall that this approach is in contrast to our

18

Partition-Based Robustness Certification

previous approach in the sense that our optimal partition in Theorem 8 minimizes the
worst-case relaxation error. Consequently, the results of this section show that the partition
given by Theorem 8 is in essence the best tractable LP partitioning scheme.

To start, recall the robustness certiﬁcation problem for a K-layer ReLU neural network:

c(cid:62)x[K]

maximize
subject to x[0] ∈ X ,

(26)

x[k+1] = ReLU(W [k]x[k]), k ∈ {0, 1, . . . , K − 1},

where the optimal value of (26) is denoted by f ∗(X ). Moreover, recall the LP relaxation of
(26):

c(cid:62)x[K]

maximize
subject to x[0] ∈ X ,

k ∈ {0, 1, . . . , K − 1},

x[k+1] ≥ W [k]x[k],
x[k+1] ≥ 0,
k ∈ {0, 1, . . . , K − 1},
x[k+1] ≤ u[k+1] (cid:12) (W [k]x[k] − l[k+1]) (cid:11) (u[k+1] − l[k+1]), k ∈ {0, 1, . . . , K − 1}.
(27)
As suggested by the motivating partition of Proposition 5, consider partitioning the
input uncertainty set into 2np parts based on np preactivation decision boundaries cor-
In particular, for each j ∈ Jp :=
responding to activation functions in the ﬁrst layer.
{j1, j2, . . . , jnp} ⊆ {1, 2, . . . , n1} we partition the input uncertainty set along the hyper-
plane w[0](cid:62)
x[0] = 0. Note that, for all j ∈ Jp, the partition implies that the jth coordinate
of the ﬁrst layer’s ReLU equality constraint becomes linear and exact on each part of the
partition. Therefore, we may write the partitioned LP relaxation as

j

c(cid:62)x[K]

maximize
subject to x[0] ∈ X ,

k ∈ {0, 1, . . . , K − 1},

x[k+1] ≥ W [k]x[k],
x[k+1] ≥ 0,
k ∈ {0, 1, . . . , K − 1},
x[k+1] ≤ u[k+1] (cid:12) (W [k]x[k] − l[k+1]) (cid:11) (u[k+1] − l[k+1]), k ∈ {0, 1, . . . , K − 1},
j = ReLU(w[0](cid:62)
x[1]

j ∈ Jp.
(28)
We denote the optimal objective of this problem as f ∗
(X ). To reiterate, the ﬁnal equal-
Jp
ity constraint is linear over each part of the partition, which makes the problem (28) a
partitioned linear program. For notational convenience, the restriction of the input to a
particular part of the partition, as well as the outer maximization over the parts of the
partition, is implicit in the expression (28).

x[0]),

j

If we now allow the indices used to deﬁne the partition, namely Jp, to act as a variable,
we can search for the optimal np rows of the ﬁrst layer that result in the tightest parti-
tioned LP relaxation. To this end, the problem of optimal partitioning in the ﬁrst layer is
formulated as

19

Anderson, Ma, Li, and Sojoudi

minimize
Jp⊆{1,2,...,n1}

f ∗
Jp(X )

subject to

|Jp| = np.

(29)

In what follows, we prove the NP-hardness of the optimal partitioning problem (29),
thereby supporting the use of the worst-case sense optimal partition developed in Theorem
8. To show the hardness of (29), we reduce an arbitrary instance of an NP-hard problem,
the Min-K-Union problem, to an instance of (29). The reduction will show that the Min-
K-Union problem can be solved by solving an optimal partitioning problem. Before we
proceed, we ﬁrst recall the deﬁnition of the Min-K-Union problem.

Deﬁnition 9 (Min-K-Union problem (Hochbaum, 1996)) Consider a collection of n
sets {S1, S2, . . . , Sn}, where Sj is ﬁnite for all j ∈ {1, 2, . . . , n}, and a positive integer K ≤ n.
Find K sets in the collection whose union has minimum cardinality, i.e., ﬁnd a solution J ∗
of the following optimization problem:

minimize
J ⊆{1,2,...,n}

subject to

Sj

(cid:91)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j∈J
|J | = K.

(30)

Remark the similarities between the optimal partitioning problem and the Min-K-Union
problem. In particular, if we think of the convex upper envelopes of the relaxed ReLU con-
straints as a collection of sets, then the goal of ﬁnding the optimal np input coordinates to
partition along is intuitively equivalent to searching for the K = n1 − np convex upper en-
velopes with minimum size, i.e., those with the least amount of relaxation. This perspective
shows that the optimal partitioning problem is essentially a Min-K-Union problem over the
collection of relaxed ReLU constraint sets. Since the Min-K-Union problem is NP-hard in
general, it is not surprising that the optimal partitioning problem is also NP-hard. Indeed,
this result is formalized in the following proposition.

Proposition 10 (NP-hardness of optimal partition) Consider the partitioned LP re-
laxation (28) of the K-layer ReLU neural network certiﬁcation problem. The optimal par-
titioning problem in the ﬁrst-layer, as formulated in (29), is NP-hard.

Proof See Appendix C.

This concludes our development and analysis for partitioning the LP relaxation. In the
next section, we follow a similar line of reasoning to develop a partitioning scheme for the
other popular convex robustness certiﬁcation technique, i.e., the SDP relaxation. Despite
approaching this relaxation from the same partitioning perspective as the LP, the vastly
diﬀerent geometries of the LP and SDP feasible sets make the partitioning procedures quite
distinct.

20

Partition-Based Robustness Certification

4. Partitioned SDP Relaxation

4.1 Tightening of the Relaxation

As with the LP relaxation, we begin by showing that the SDP relaxation error is decreased
when the input uncertainty set is partitioned. This proposition is formalized below.

Proposition 11 (Improving the SDP relaxation bound) Consider a neural network
with one hidden ReLU layer. Let {X (j) : j ∈ {1, 2, . . . , p}} be a partition of X . For the jth
input part X (j), denote the corresponding input bounds by l ≤ l(j) ≤ x ≤ u(j) ≤ u, where
x ∈ X (j). Then, it holds that

max
j∈{1,2,...,p}

SDP(X (j)) ≤ ˆf ∗
ˆf ∗

SDP(X ).

(31)

Proof See Appendix D.

Proposition 11 guarantees that partitioning yields a tighter SDP relaxation. However,
it is not immediately clear how to design the partition in order to maximally reduce the
relaxation error.
Indeed, a poorly designed partition may even yield an equality in the
bound (31). One notable challenge in designing the SDP partition relates to an inherent
diﬀerence between the SDP relaxation and the LP relaxation. With the LP relaxation, the
eﬀect of partitioning can be visualized by how the geometry of the feasible set changes; see
Figure 4. However, with the SDP, the relaxation comes from dropping the nonconvex rank
constraint, the geometry of which is more abstract and harder to exploit.

In the next section, we develop a bound measuring how far the SDP solution is from
being rank-1, which corresponds to an exact relaxation, where the improvement in (31)
is as good as possible. By studying the geometry of the SDP feasible set through this
more tractable bound, we ﬁnd that the partition design for the SDP naturally reduces to a
uniform partition along the coordinate axes of the input set.

4.2 Motivating Partition

In this section, we seek the form of a partition that best reduces the SDP relaxation error.
By restricting our focus to ReLU networks with one hidden layer, we develop a simple
necessary condition for the SDP relaxation to be exact, i.e., for the matrix P to be rank-1.
We then work on the violation of this condition to deﬁne a measure of how close P is to
being rank-1 in the case it has higher rank. Next, we develop a tractable upper bound on
this rank-1 gap. Finally, we formulate an optimization problem in which we search for a
partition of the input uncertainty set that minimizes our upper bound. We show that an
optimal partition takes the form of a uniform division of the input set. The result motivates
the use of uniform partitions of the input uncertainty set, and in Section 4.3, we answer
the question of which coordinate is best to uniformly partition along. Note that, despite
the motivating partition being derived for networks with one hidden layer, the relaxation
tightening in Proposition 11 still holds for multi-layer networks. Indeed, the experiments in
Section 5 will show that the resulting SDP partition design maintains a relatively constant
eﬃcacy as the number of layers increases.

21

Anderson, Ma, Li, and Sojoudi

Proposition 12 (Necessary condition for exact SDP) Let P ∗ ∈ Snx+nz+1 denote a
solution to the semideﬁnite programming relaxation (8). If the relaxation is exact, meaning
that rank(P ∗) = 1, then the following conditions hold:

tr(P ∗

xx) = (cid:107)P ∗

x (cid:107)2
2,

tr(P ∗

zz) = (cid:107)P ∗

z (cid:107)2
2.

(32)

Proof Since the SDP relaxation is exact, it holds that rank(P ∗) = 1. Therefore, P ∗ can
be expressed as

P ∗ =

(cid:2)1 v(cid:62) w(cid:62)(cid:3) =









1
v
w





v(cid:62)
w(cid:62)
1
vv(cid:62) vw(cid:62)
v
w wv(cid:62) ww(cid:62)





for some vectors v ∈ Rnx and w ∈ Rnz . Recall the block decomposition of P ∗:

P ∗ =





P ∗
1 P ∗(cid:62)
P ∗(cid:62)
x
z
xx P ∗
x P ∗
P ∗
xz
zx P ∗
z P ∗
P ∗
zz



 .

Equating coeﬃcients, we ﬁnd that P ∗
fore,

xx = vv(cid:62) = P ∗

x P ∗(cid:62)
x

and P ∗

zz = ww(cid:62) = P ∗

z P ∗(cid:62)
z

. There-

x P ∗(cid:62)
proving the ﬁrst condition in (32). The second condition follows in the same way.

x ) = tr(P ∗(cid:62)

xx) = tr(P ∗

x ) = (cid:107)P ∗

x P ∗

tr(P ∗

x (cid:107)2
2,

Enforcing the conditions (32) as constraints in the SDP relaxation may assist in pushing
the optimization variable P towards a rank-1 solution. However, because the conditions in
(32) are nonlinear equality constraints in the variable P , we cannot impose them directly
on the SDP without making the problem nonconvex.
Instead, we will develop a convex
method based on the rank-1 conditions (32) that can be used to motivate the SDP solution
to have a lower rank.

In the general case that rank(P ) = r ≥ 1, P may be written as P = V V (cid:62), where

V =





e(cid:62)
X
Z


 , e ∈ Rr, X ∈ Rnx×r, Z ∈ Rnz×r,

2 = 1. The ith row of X (respectively,
and where the vector e satisﬁes the equation e(cid:62)e = (cid:107)e(cid:107)2
i ∈ R1×r). Under this expansion, we ﬁnd that
Z) is denoted by X (cid:62)
Px = Xe, Pz = Ze, Pxx = XX (cid:62), and Pzz = ZZ(cid:62). Therefore, the conditions (32) can be
written as

i ∈ R1×r (respectively, Z(cid:62)

tr(XX (cid:62)) = (cid:107)Xe(cid:107)2
2,

tr(ZZ(cid:62)) = (cid:107)Ze(cid:107)2
2.

To simplify the subsequent analysis, we will restrict our attention to the ﬁrst of these
two necessary conditions for P to be rank-1. As the simulation results in Section 5 show,
this restriction still yields signiﬁcant reduction in relaxation error. Now, note that

tr(XX (cid:62)) =

nx(cid:88)

i=1

(XX (cid:62))ii =

nx(cid:88)

i=1

(cid:107)Xi(cid:107)2
2,

22

Partition-Based Robustness Certification

where (XX (cid:62))ii is the (i, i) element of the matrix XX (cid:62), and also that

(cid:107)Xe(cid:107)2

2 =

nx(cid:88)

(Xe)2

i =

i=1

nx(cid:88)

i=1

(X (cid:62)

i e)2,

where (Xe)i is the ith element of the vector Xe. Therefore, the rank-1 necessary condition
is equivalently written as

g(P ) :=

nx(cid:88)

((cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2) = 0,

i=1

where g(P ) serves as a measure of the rank-1 gap. Note that g(P ) is solely determined by
P = V V (cid:62), even though it is written in terms of X and e, which are blocks of V . In general,
g(P ) ≥ 0 when rank(P ) ≥ 1.

Lemma 13 (Rank-1 gap) Let P ∈ Snx+nz+1 be an arbitrary feasible point for the SDP
relaxation (8). The rank-1 gap g(P ) is nonnegative, and is zero if P is rank-1.

Proof By the Cauchy-Schwarz inequality, we have that |X (cid:62)
i e| ≤ (cid:107)Xi(cid:107)2(cid:107)e(cid:107)2 for all i ∈
{1, 2, . . . , nx}. Since P is feasible for (8) we also have that (cid:107)e(cid:107)2 = 1, so squaring both sides
of the inequality gives that (X (cid:62)

2. Summing these inequalities over i gives

i e)2 ≤ (cid:107)Xi(cid:107)2
nx(cid:88)

g(P ) =

((cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2) ≥ 0.

i=1

If P is rank-1, then the dimension r of the vectors e and Xi is equal to 1. That is, e, Xi ∈ R.
i e2 = 0.
Hence, (cid:107)Xi(cid:107)2 = |Xi| and |e| = (cid:107)e(cid:107)2 = 1, yielding (cid:107)Xi(cid:107)2
Therefore, g(P ) = 0 in the case that rank(P ) = 1.

i e)2 = X 2

2 − (X (cid:62)

i − X 2

Since g(P ) = 0 is necessary for P to be rank-1 and g(P ) ≥ 0, it is desirable to make
g(P ∗) as small as possible at the optimal solution P ∗ of the partitioned SDP relaxation.
Indeed, this is our partitioning motivation: we seek to partition the input uncertainty set
to minimize g(P ∗), in order to inﬂuence P ∗ to be of low rank. However, there is a major
hurdle with this approach. In particular, the optimal solution P ∗ depends on the partition
we choose, and ﬁnding a partition to minimize g(P ∗) in turn depends on P ∗ itself. To
overcome this cyclic dependence, we propose ﬁrst bounding g(P ∗) by a worst-case upper
bound, and then choosing an optimal partition to minimize the upper bound. This will
make the partition design tractable, resulting in a closed-form solution.

To derive the upper bound on the rank-1 gap at optimality, let {X (j) : j ∈ {1, 2, . . . , p}}
denote the partition of X . For the jth input part X (j), denote the corresponding input
bounds by l(j), u(j). The upper bound is derived below.

Lemma 14 (Rank-1 gap upper bound) The rank-1 gap at the solution P ∗ of the par-
titioned SDP satisﬁes

0 ≤ g(P ∗) ≤

1
4

nx(cid:88)

i=1

max
j∈{1,2,...,p}

(u(j)

i − l(j)

i )2.

(33)

23

Anderson, Ma, Li, and Sojoudi

Proof The left inequality is a direct result of Lemma 13. For the right inequality, note
that

g(P ∗) ≤ max

j∈{1,2,...,p}

sup

g(P ) ≤

P ∈N (j)

SDP, Px∈X (j)

nx(cid:88)

i=1

max
j∈{1,2,...,p}

sup

P ∈N (j)

SDP, Px∈X (j)

((cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2).

Let us focus on the optimization over the jth part of the partition, namely,

(34)

sup

P ∈N (j)

SDP, Px∈X (j)

((cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2).

To bound this quantity, we analyze the geometry of the SDP relaxation over part j, following
the methodology of Raghunathan et al. (2018); see Figure 5.

u(j)
i e

(X (cid:62)

i e)e

a

Xi

b

e

r(j)
i

1

2 (u(j)

i + l(j)

i )e

(cid:107)Xi(cid:107)2

0

l(j)
i e

Figure 5: Geometry of the SDP relaxation in coordinate i over part j of the partition. The
shaded region shows the feasible Xi satisfying the input constraint (Raghunathan
et al., 2018).

The shaded circle represents the set of feasible Xi over part j of the partition, namely,
those satisfying the ith coordinate of the constraint diag(Pxx) ≤ (l(j) +u(j))(cid:12)Px −l(j) (cid:12)u(j).
To understand this, note that the constraint is equivalent to (cid:107)Xi(cid:107)2
i e −

2 ≤ (l(j)

i + u(j)

i )X (cid:62)

24

Partition-Based Robustness Certification

i

, or, more geometrically written, that (cid:107)Xi − 1

l(j)
i + l(j)
i u(j)
This shows that Xi is constrained to a 2-norm ball of radius r(j)
at 1

2 (u(j)
The geometry of Figure 5 immediately shows that (cid:107)Xi(cid:107)2

i )e, as shown in Figure 5.

i + l(j)

2 (u(j)

(a + b)2 + (X (cid:62)

i e − 1

2 (u(j)

i + l(j)

i ))2, and therefore

i )e(cid:107)2
i = 1

(cid:17)2

(cid:16) 1
2 (u(j)
i − l(j)

i − l(j)
i )
i ) centered

2 ≤
2 (u(j)

.

2 = a2 + (X (cid:62)

i e)2 and r(j)2

i

=

(cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2 = a2 = r(j)2

i − (X (cid:62)

i e − 1

2 (u(j)

i + l(j)

i ))2 − 2ab − b2.

Since a and b are nonnegative,

sup

(cid:107)Xi(cid:107)2

2 − (X (cid:62)

i e)2

P ∈N (j)

SDP, Px∈X (j)
(r(j)2
i − (X (cid:62)

i e − 1

2 (u(j)

i + l(j)

i ))2 − 2ab − b2)

i − (X (cid:62)

i e − 1

2 (u(j)

i + l(j)

i ))2) ≤ r(j)2

i =

1
4

(u(j)

i − l(j)

i )2.

=

sup

P ∈N (j)

SDP, Px∈X (j)
(r(j)2

≤

sup

P ∈N (j)

SDP, Px∈X (j)

Thus, (34) gives

as desired.

g(P ∗) ≤

1
4

nx(cid:88)

i=1

max
j∈{1,2,...,p}

(u(j)

i − l(j)

i )2,

With Lemma 14 in place, we now have an upper bound on the rank-1 gap at optimality,
in terms of the input bounds {l(j), u(j)}p
j=1 associated with the partition. At this point,
we turn to minimizing the upper bound over all valid choices of p-part partitions of the
input uncertainty set along a given coordinate. Note that, in order for {l(j), u(j)}p
j=1 to
deﬁne valid input bounds for a p-part partition, it must be that the union of the input parts
cover the input uncertainty set. In terms of the input bounds, this leads to the constraint
that [l, u] = ∪p
j=1[l(j), u(j)], where [l, u] := [l1, u1] × [l2, u2] × · · · × [lnx, unx], and similarly
for [l(j), u(j)]. Since we consider the partition along a single coordinate k, this constraint
becomes equivalent to ∪p
k ] = [lk, uk], because all other coordinates i (cid:54)= k satisfy
i = li and u(j)
l(j)
i = ui for all j by assumption. We now give the optimal partitioning scheme
for the SDP that minimizes the upper bound in Lemma 14.

k , u(j)

j=1[l(j)

Theorem 15 (Optimal SDP partition via rank-1 gap) Let Ik = {1, 2, . . . , nx} \ {k}.
Consider the optimization problem of ﬁnding the partition to minimize the upper bound

25

Anderson, Ma, Li, and Sojoudi

(33), namely

minimize

P={l(j),u(j)}p

j=1⊆Rnx

h(P) =

nx(cid:88)

i=1

max
j∈{1,2,...,p}

(u(j)

i − l(j)

i )2

subject to

p
(cid:91)

k , u(j)
[l(j)

k ] = [lk, uk],

i ∈ Ik, j ∈ {1, 2, . . . , p},

(35)

j=1
l(j)
i = li,
u(j)
i = ui,

i ∈ Ik, j ∈ {1, 2, . . . , p},

i ∈ Ik, j ∈ {1, 2, . . . , p},

Consider also the uniform partition deﬁned by ¯P = {¯l(j), ¯u(j)}p

j=1 ⊆ Rnx, where

¯l(j)
i =

¯u(j)
i =

(cid:40) j−1

p (ui − li) + li

li
(cid:40) j

p (ui − li) + li
ui

if i = k,

otherwise,

if i = k,

otherwise,

for all j ∈ {1, 2, . . . , p}. It holds that ¯P is a solution to (35).

Proof To prove the result, we show that the proposed ¯P is feasible for the optimization,
and that h( ¯P) ≤ h(P) for all feasible P. First, note that it is obvious by the deﬁnition of ¯P
that ¯l(j)
i = ui for all i ∈ {1, 2, . . . , nx} \ {k} and all j ∈ {1, 2, . . . , p}. Therefore,
j=1[¯l(j)
to prove that ¯P is feasible, it suﬃces to show that ∪p

k ] = [lk, uk]. Indeed, since

i = li and ¯u(j)

k , ¯u(j)

¯u(j)
k =

j
p

(uk − lk) + lk =

(j + 1) − 1
p

(uk − lk) + lk = ¯l(j+1)

k

for all j ∈ {1, 2, . . . , p − 1},

¯l(1)
k =

1 − 1
p

(uk − lk) + lk = lk,

¯u(p)
k =

p
p

(uk − lk) + lk = uk,

and

we have that

p
(cid:91)

k , ¯u(j)
[¯l(j)

k ] = [¯l(1)

k , ¯u(1)

k ] ∪ [¯l(2)

k , ¯u(2)

k ] ∪ · · · ∪ [¯l(p)

k , ¯u(p)

k ] = [¯l(1)

k , ¯u(p)

k ] = [lk, uk].

j=1

Hence, ¯P = {¯l(j), ¯u(j)}p

j=1 is feasible.

26

Partition-Based Robustness Certification

The objective at the proposed feasible point can be computed as

h( ¯P) =

nx(cid:88)

i=1

max
j∈{1,2,...,p}

(¯u(j)

i − ¯l(j)

i )2 =

nx(cid:88)

i=1
i(cid:54)=k

max
j∈{1,2,...,p}

(¯u(j)

i − ¯l(j)

i )2 + max

(¯u(j)

k − ¯l(j)

k )2

j∈{1,2,...,p}

max
j∈{1,2,...,p}

(ui − li)2 + max

j∈{1,2,...,p}

(cid:18) j
p

(uk − lk) + lk −

(cid:19)2

(uk − lk) − lk

j − 1
p

=

nx(cid:88)

i=1
i(cid:54)=k

=

(ui − li)2 + max

j∈{1,2,...,p}

nx(cid:88)

i=1
i(cid:54)=k

(cid:19)2

(uk − lk)

= C +

(cid:18) 1
p

1
p2 (uk − lk)2,

where C := (cid:80)nx
i=1
i(cid:54)=k

(ui − li)2. Now, let P = {l(j), u(j)}p

j=1 be an arbitrary feasible point for

the optimization (35). Then by a similar analysis as above, the objective value at P satisﬁes

h(P) =

nx(cid:88)

i=1

max
j∈{1,2,...,p}

(u(j)

i − l(j)

i )2 =

nx(cid:88)

i=1
i(cid:54)=k

max
j∈{1,2,...,p}

(u(j)

i − l(j)

i )2 + max

(u(j)

k − l(j)

k )2

j∈{1,2,...,p}

k )2 = C +

(u(j)

k − l(j)

2

= C + max

j∈{1,2,...,p}


1
p

p
(cid:88)

j=1

≥ C +



(u(j)

k − l(j)
k )



= C +

(cid:18)

max
j∈{1,2,...,p}





p
(cid:88)

j=1

1
p2

(cid:19)2

(u(j)

k − l(j)
k )

2

(u(j)

k − l(j)
k )



.

Since P is feasible, it holds that [lk, uk] = (cid:83)p
Lebesgue measure µ on the Borel σ-algebra of R, we have that

k , u(j)

j=1[l(j)

k ]. Therefore, by subadditivity of

uk − lk = µ([lk, uk]) = µ





p
(cid:91)



k , u(j)
[l(j)
k ]

 ≤

j=1

p
(cid:88)

j=1

µ([l(j)

k , u(j)

k ]) =

p
(cid:88)

j=1

(u(j)

k − l(j)
k ).

Substituting this into our above expressions, we conclude that

h( ¯P) = C +

1
p2 (uk − lk)2 ≤ C +

1
p2





p
(cid:88)


2

(u(j)

k − l(j)
k )



≤ h(P).

j=1

Since P was an arbitrary feasible point for the optimization, this implies that ¯P is a solution
to the optimization.

Theorem 15 shows that by choosing the partition of the input set to be uniformly divided
amongst the p parts, we obtain an optimal partition that minimizes the worst-case bound
on the gap of the rank-1 necessary condition (32). This gives a well-motivated, yet simple
way to design a partition of the input uncertainty set in order to push the SDP relaxation
towards being rank-1, thereby reducing relaxation error.

27

Anderson, Ma, Li, and Sojoudi

4.3 Partitioning Scheme

With the motivating partition of Section 4.2 now established, we turn our attention from the
form of an optimal SDP partition to the coordinate of an optimal partition. In particular,
we seek to ﬁnd the best two-part partition to minimize relaxation error of the SDP. The
results of Section 4.2 suggest using a uniform partition, and in this section we seek to ﬁnd
which coordinate to apply the partitioning to. Similar to the LP relaxation, we derive an
optimal partitioning scheme by ﬁrst bounding the relaxation error in the worst-case sense.

4.3.1 Worst-Case Relaxation Bound

In the worst-case relaxation bound of Theorem 18 below, and the subsequent optimal SDP
partitioning scheme proposed in Theorem 19, we restrict our attention to a single hidden
ReLU layer and make the following assumption on the weight matrix.

Assumption 16 (Normalized rows) The rows of the weight matrix are assumed to be
normalized with respect to the (cid:96)1-norm, i.e., that (cid:107)wi(cid:107)1 = 1 for all i ∈ {1, 2, . . . , nz}.

We brieﬂy remark that Assumption 16 imposes no loss of generality, as it can be made
to hold for any network by a simple rescaling. In particular, if the assumption does not
hold, the network architecture can be rescaled as follows:

z = ReLU(W x) = ReLU










x




















w(cid:62)
1
w(cid:62)
2
...
w(cid:62)
nz













= ReLU

diag((cid:107)w1(cid:107)1, (cid:107)w2(cid:107)1, . . . , (cid:107)wnz (cid:107)1)










w(cid:62)
1
(cid:107)w1(cid:107)1
w(cid:62)
2
(cid:107)w2(cid:107)1

...

w(cid:62)
nz
(cid:107)wnz (cid:107)1








x








= Wscale ReLU(Wnormx),

where Wscale = diag((cid:107)w1(cid:107)1, (cid:107)w2(cid:107)1, . . . , (cid:107)wnz (cid:107)1) ∈ Rnz×nz and

Wnorm =



















w(cid:62)
1
(cid:107)w1(cid:107)1
w(cid:62)
2
(cid:107)w2(cid:107)1

...

w(cid:62)
nz
(cid:107)wnz (cid:107)1

∈ Rnz×nx

are the scaling and normalized factors of the weight matrix W , respectively. The scaling
factor can therefore be absorbed into the optimization cost vector c, yielding a problem
with normalized rows as desired.

Before introducing the worst-case relaxation bound of Theorem 18, we state a short

lemma that will be used in proving the relaxation bound.

28

Partition-Based Robustness Certification

Lemma 17 (Bound on elements of PSD matrices) Let P ∈ Sn be a positive semidef-
inite matrix. Then |Pij| ≤ 1
2 (Pii + Pjj) for all i, j ∈ {1, 2, . . . , n}.

Proof See Appendix E.

Theorem 18 (Worst-case relaxation bound for SDP) Consider a feedforward ReLU
neural network with one hidden layer, and with the input uncertainty set X . Let the network
have input bounds l, u ∈ Rnx and preactivation bounds ˆl, ˆu ∈ Rnz . Consider also the relax-
SDP(X ) := ˆf ∗
ation error ∆f ∗
SDP(X ) − f ∗(X ). Let P ∗ and (x∗, z∗) be optimal solutions for
the relaxation ˆf ∗
SDP(X ) and the unrelaxed problem f ∗(X ), respectively. Given an arbitrary
norm (cid:107) · (cid:107) on Rnx, there exists (cid:15) ∈ R such that (cid:107)P ∗
nz(cid:88)

x − x∗(cid:107) ≤ (cid:15) and

(cid:19)

(cid:18)

ReLU(ci)q(l, u) + ReLU(−ci) min{ˆui, (cid:15)(cid:107)wi(cid:107)∗}

,

∆f ∗

SDP(X ) ≤

(36)

where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107), and where

i=1

q(l, u) =

max
k∈{1,2,...,nx}

max{|lk|, |uk|}.

Proof First, since X is bounded, there exists (cid:15) ≥ 0 such that (cid:107)P ∗
of P ∗

x and (x∗, z∗) give that

x −x∗(cid:107) ≤ (cid:15). The deﬁnitions

∆f ∗

SDP(X ) =

nz(cid:88)

i=1

ci((P ∗

z )i − z∗

i ) ≤

nz(cid:88)

i=1

∆f ∗
i ,

where

(cid:26)

∆f ∗

i = sup

ci((Pz)i − zi) : zi = ReLU(w(cid:62)

i x), Pz ≥ 0, Pz ≥ W Px,

(37)

(cid:27)

diag(Pzz) = diag(W Pxz), P1 = 1, P (cid:23) 0, (cid:107)Px − x(cid:107) ≤ (cid:15), x, Px ∈ X

for all i ∈ {1, 2, . . . , nz}. Deﬁning the auxiliary variables Pˆz = W Px and ˆz = W x, this is
equivalent to

(cid:26)

∆f ∗

i = sup

ci((Pz)i − zi) : zi = ReLU(ˆzi), Pz ≥ 0, Pz ≥ Pˆz, diag(Pzz) = diag(W Pxz),

P1 = 1, P (cid:23) 0, (cid:107)Px − x(cid:107) ≤ (cid:15), Pˆz = W Px, ˆzi = w(cid:62)

i x, x, Px ∈ X

(cid:27)

.

If x, Px ∈ X and ˆz, Pˆz satisfy ˆz = W x, Pˆz = W Px, and (cid:107)Px − x(cid:107) ≤ (cid:15), then |(Pˆz)i − ˆzi| =
|w(cid:62)
i (Px − x)| ≤ (cid:107)wi(cid:107)∗(cid:107)Px − x(cid:107) ≤ (cid:15)(cid:107)wi(cid:107)∗ for all i ∈ {1, 2, . . . , nz} by the Cauchy-Schwarz
inequality for dual norms. Therefore,

(cid:26)

∆f ∗

i ≤ sup

ci((Pz)i − zi) : zi = ReLU(ˆzi), Pz ≥ 0, Pz ≥ Pˆz,

diag(Pzz) = diag(W Pxz), P1 = 1, P (cid:23) 0, ˆl ≤ ˆz, Pˆz ≤ ˆu,

|(Pˆz)k − ˆzk| ≤ (cid:15)(cid:107)wk(cid:107)∗ for all k ∈ {1, 2, . . . , nz}, ˆz, Pˆz ∈ Rnz

(cid:27)

.

29

Anderson, Ma, Li, and Sojoudi

We now translate the optimization variables in the above problem from ˆz ∈ Rnz and P ∈
S1+nx+nz to the scalars ˆzi, (Pˆz)i ∈ R. To this end, we note that if P is feasible for the above
supremum, then

diag(Pzz)i = diag(W Pxz)i = w(cid:62)

i (Pxz)i ≤ (cid:107)(Pxz)i(cid:107)∞(cid:107)wi(cid:107)1,

where (Pxz)i is the ith column of the matrix Pxz, and the inequality again comes from
Cauchy-Schwarz. By the weight matrix scaling assumption, this yields

Now, since P is positive semideﬁnite, Lemma 17 gives that

diag(Pzz)i ≤ (cid:107)(Pxz)i(cid:107)∞.

(cid:107)(Pxz)i(cid:107)∞ =

max
k∈{1,2,...,nx}

|(Pxz)i|k =

max
k∈{1,2,...,nx}

|(Pxz)ki|

≤

max
k∈{1,2,...,nz}

1
2

(cid:18)

(cid:19)

(Pxx)kk + (Pzz)ii

=

1
2

(Pzz)ii +

1
2

max
k∈{1,2,...,nx}

(Pxx)kk.

Noting that (Pzz)ii = diag(Pzz)i, the bound of interest becomes

diag(Pzz)i ≤

max
k∈{1,2,...,nz}

(Pxx)kk.

We now seek to bound (Pxx)kk. Recall that (Pxx)kk = diag(Pxx)k ≤ (lk + uk)(Px)k − lkuk.
If (lk + uk) ≥ 0, then (Px)k ≤ uk implies that (lk + uk)(Px)k ≤ (lk + uk)uk, and therefore
(Pxx)kk ≤ (lk + uk)uk − lkuk = u2
k. On the other hand, if (lk + uk) < 0, then (Px)k ≥ lk
implies that (lk + uk)(Px)k ≤ (lk + uk)lk, and therefore (Pxx)kk ≤ (lk + uk)lk − lkuk = l2
k.
Hence, in all cases, it holds that

(Pxx)kk ≤ I(lk + uk ≥ 0)u2

k + I(lk + uk < 0)l2
k.

We can further simplify this bound as follows. If lk + uk ≥ 0, then uk ≥ −lk and uk ≥ lk,
implying |lk| ≤ uk, so l2
k ≤ u2
k}. On the other hand, if
k, u2
lk + uk < 0, then an analogous argument shows that l2
k}. Hence, we conclude
that the above bound on (Pxx)kk can be rewritten as

k, u2
k = max{l2

k and therefore u2

k = max{l2

(Pxx)kk ≤ max{l2

k, u2

k}.

Therefore, returning to the bound on (Pzz)i, we ﬁnd that

diag(Pzz)i ≤

max
k∈{1,2,...,nx}

max{l2

k, u2

k},

for all i ∈ {1, 2, . . . , nz}. Now, note that since P (cid:23) 0, the Schur complement gives that

which implies that

(cid:20)Pxx − PxP (cid:62)
x Pxz − PxP (cid:62)
z
Pzz − PzP (cid:62)
xz − PzP (cid:62)
P (cid:62)
z
x

(cid:21)

(cid:23) 0,

diag(Pzz) ≥ diag(PzP (cid:62)

z ) = Pz (cid:12) Pz.

30

Partition-Based Robustness Certification

Therefore, our upper bound on the diagonal elements of Pzz yields that

(Pz)i ≤

max
k∈{1,2,...,nx}

max{|lk|, |uk|} = q(l, u).

Hence, we have derived a condition on the component (Pz)i that all feasible P must satisfy.
The supremum of interest may now be further upper bounded giving rise to

(cid:26)

∆f ∗

i ≤ sup

ci((Pz)i − zi) : zi = ReLU(ˆzi), (Pz)i ≥ 0, (Pz)i ≥ (Pˆz)i, (Pz)i ≤ q(l, u),

ˆli ≤ ˆzi, (Pˆz)i ≤ ˆui, |(Pˆz)i − ˆzi| ≤ (cid:15)(cid:107)wi(cid:107)∗, ˆzi, (Pˆz)i ∈ R

(cid:27)

,

(38)

which is now in terms of the scalar optimization variables ˆzi and (Pˆz)i, as we desired. This
reformulation makes it tractable to compute the supremum in (38) in closed-form, which
we now turn to do.

First, consider the case that ci ≥ 0. Then we seek to maximize the diﬀerence (Pz)i − zi
subject to the given constraints. Noting that (Pz)i ≤ q(l, u) and zi ≥ 0 on the above feasible
set, we remark that the objective is upper bounded as ci((Pz)i − zi) ≤ ciq(l, u). Indeed,
this upper bound is attained at the feasible point deﬁned by zi = ˆzi = (Pˆz)i = 0 and
(Pz)i = q(l, u). Hence, we conclude that for all i ∈ {1, 2, . . . , nz} such that ci ≥ 0, it holds
that

∆f ∗

i ≤ ciq(l, u).

(39)

Now consider the case that ci < 0. Then we seek to minimize the diﬀerence (Pz)i −zi subject
to the given constraints. In this case, the optimal objective value depends on the relative
sizes of ˆui and (cid:15)(cid:107)wi(cid:107)∗. In particular, when ˆui ≤ (cid:15)(cid:107)wi(cid:107)∗, the constraint ˆzi ≤ ˆui becomes
active at optimum, yielding a supremum value of −ciui. Alternatively, when (cid:15)(cid:107)wi(cid:107)∗ ≤ ˆui,
the constraint |(Pˆz)i − ˆzi| ≤ (cid:15)(cid:107)wi(cid:107)∗ becomes active at optimum, yielding the supremum
value of −ci(cid:15)(cid:107)wi(cid:107)∗. Therefore, we conclude that for all i ∈ {1, 2, . . . , nz} such that ci < 0,
it holds that

∆f ∗

i ≤ −ci min{ˆui, (cid:15)(cid:107)wi(cid:107)∗}.

(40)

Substituting (39) and (40) into (37) gives the desired bound.

When the x-block P ∗

x of the SDP relaxation stays close to the true solution x∗, the
bound (36) shows that the worst-case relaxation error scales with the loosest input bound,
i.e., the maximum value amongst the limits |lk| and |uk|. This fact allows us to choose
which coordinate to partition along in order to maximally reduce the relaxation bound on
the individual parts of the partition. We state our proposed SDP partition next.

4.3.2 Proposed Partition

We now focus on developing an optimal two-part partitioning scheme based on the worst-
case relaxation bound of Theorem 18. Similar to the LP relaxation approach, we take (cid:15) ≈ 0
to simplify the analysis (refer to the LP analysis for justiﬁcations of this simpliﬁcation).
The bound therefore takes the form

∆f ∗

SDP(X ) ≤ q(l, u)

nz(cid:88)

i=1

ReLU(ci),

31

(41)

Anderson, Ma, Li, and Sojoudi

where

q(l, u) =

max
k∈{1,2,...,nx}

max{|lk|, |uk|}.

Since the design of the partition amounts to choosing input bounds l and u for the in-
put parts, the input bounds serve as our optimization variables in minimizing the above
worst-case relaxation bound. By restricting the form of our partition to the uniform
division motivated in Theorem 15, it follows from the form of q that the best coordi-
nate to partition along is that with the loosest input bound, i.e., along coordinate i∗ ∈
arg maxk∈{1,2,...,nx} max{|lk|, |uk|}. This observation is formalized below.

Theorem 19 (Optimal SDP partition) Consider the two-part partitions deﬁned by di-
viding X uniformly along the coordinate axes: {X (1)
i ≤ x ≤
i } and X (2)
u(1)
2 (li +
ui), ui+1, . . . , unx), l(2)
i = u, for all
i ∈ {1, 2, . . . , nx} =: I. Let

, X (2)
i }, with X (1)
i = l, u(1)
2 (li + ui), li+1, . . . , lnx), and u(2)

i = {x ∈ X : l(1)
i = (u1, u2, . . . , ui−1, 1

i ≤ x ≤ u(2)
i = (l1, l2, . . . , li−1, 1

i = {x ∈ X : l(2)

i }, where l(1)

i

i∗ ∈ arg max

k∈{1,2,...,nx}

max{|lk|, |uk|},

(42)

and assume that |li∗| (cid:54)= |ui∗|. Then the partition {X (1)
i∗ , u(j)
upper bound factor q(l(j)
part j of the partition, is strictly less than q(l, u) on the other part, and q(l(j)
for both j ∈ {1, 2} for all other i /∈ arg maxk∈{1,2,...,nx} max{|lk|, |uk|}.

i∗ } is optimal in the sense that the
i∗ ) in (41) equals the unpartitioned upper bound q(l, u) on one
i ) = q(u, l)

i∗ , X (2)

, u(j)

i

Proof First, consider partitioning along coordinate i /∈ arg maxk∈{1,2,...,nx} max{|lk|, |uk|}.
Then

q(l(1)
i

, u(1)

i ) =

max{|(l(1)

i

i )k|}

)k|, |(u(1)
(cid:27)

max
k∈{1,2,...,nx}
li + ui
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|l1|, . . . , |lnx|, |u1|, . . . ,

, . . . , |unx|

= max{|li∗|, |ui∗|} = q(l, u),

(cid:26)

= max

since

(cid:12)
(cid:12)
(cid:12)

li+ui
2

(cid:12)
(cid:12) ≤ |li|+|ui|
(cid:12)

2

< max{|li∗|, |ui∗|} and i (cid:54)= i∗ implies that

max{|li∗|, |ui∗|} ∈

|l1|, . . . , |lnx|, |u1|, . . . ,

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

li + ui
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

, . . . , |unx|

.

In an analogous fashion, it follows that

q(l(2)
i

, u(2)

i ) = q(l, u).

Now, consider partitioning along coordinate i∗. Note that either

max
k∈{1,2,...,nx}

max{|lk|, |uk|} = |li∗|, or

max
k∈{1,2,...,nx}

max{|lk|, |uk|} = |ui∗|.

32

Partition-Based Robustness Certification

Suppose that the ﬁrst case holds true. Then

q(l(1)

i∗ , u(1)

i∗ ) =

max
k∈{1,2,...,nx}

max{|(l(1)

i∗ )k|, |(u(1)
i∗ )k|}
(cid:27)

(cid:26)

= max

|l1|, . . . , |lnx|, |u1|, . . . ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

li∗ + ui∗
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, . . . , |unx|

= |li∗| = q(l, u),

since |(li∗ + ui∗)/2| ≤ (|l∗

i | + |u∗

i |)/2 < |l∗

i | and

(cid:26)

|li∗| ∈ max

|l1|, . . . , |lnx|, |u1|, . . . ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

li∗ + ui∗
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

, . . . , |unx|

.

Over the second part of the partition,

(cid:26)

i∗ ) =

max
k∈{1,2,...,nx}

q(l(2)

i∗ , u(2)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

li∗ + ui∗
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max{|(l(2)

i∗ )k|, |(u(2)
i∗ )k|}
(cid:27)

= max

|l1|, . . . ,

, . . . , |lnx|, |u1|, . . . , |unx|

< |li∗| = q(l, u),

since |(li∗ + ui∗)/2| < |li∗| and

(cid:26)

|l1|, . . . ,

|l∗
i | /∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)

li∗ + ui∗
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, . . . , |lnx|, |u1|, . . . , |unx|

(cid:27)

since |li∗| (cid:54)= |ui∗|. In the other case that maxk∈{1,2,...,nx} max{|lk|, |uk|} = |ui∗|, it follows
via the same argument that q(l(1)
i∗ ) = q(l, u). Since partition-
ing along any other coordinate i /∈ arg maxk∈{1,2,...,nx} max{|lk|, |uk|} was shown to yield
q(l(1)
, u(1)
i ) = q(u, l), we conclude that the coordinate i∗ is optimal in the
i
sense proposed.

i∗ ) < q(l, u) and q(l(2)

i ) = q(l(2)

i∗ , u(1)

i∗ , u(2)

, u(2)

i

Intuitively, the partitioning scheme deﬁned in Theorem 19 is optimal because any other
uniform partition along a coordinate axis cannot tighten the worst-case relaxation error
bound (41). On the other hand, Theorem 19 guarantees that using the partition coordinate
in (42) results in a strict tightening of the worst-case relaxation error on at least one part
of the partition.

5. Simulation Results

In this section, we experimentally corroborate the eﬀectiveness of our proposed certiﬁcation
methods. We ﬁrst perform the partitioned LP relaxation on an IRIS classiﬁcation network
and show that our proposed partitioning scheme is able to generate a robustness certiﬁcate
which was previously unattainable. We then compute the SDP and partitioned SDP on
the same network and compare to the LP results. Finally, we explore the eﬀectiveness
of partitioning the LP and SDP as the network grows in size, namely, as the number of
inputs and the number of layers independently increase. All experiments are performed
using Matlab and CVX on a standard laptop with a 2.9 GHz quad-core i7 processor.

33

Anderson, Ma, Li, and Sojoudi

5.1 Partitioned LP Results

In this experiment, we consider a classiﬁcation network trained on the Iris data set (Fisher,
1936) with a test accuracy of 97%. The network has a single hidden layer, four inputs, and
three outputs. We consider 10 diﬀerent nominal inputs ¯x and corresponding uncertainty
sets X = {x ∈ Rnx : (cid:107)x − ¯x(cid:107)∞ ≤ (cid:15)}, where (cid:15) = 0.1. In this setting, a negative optimal
objective value of the robustness certiﬁcation problem proves that no perturbation of ¯x
within X results in misclassiﬁcation.

For each nominal input being tested, we ﬁrst solve the original, nonconvex certiﬁcation
problem to local optimality using Matlab’s fmincon local search function with 5 multi-starts.
Upon using more than 5 starts, we found no change in the nonconvex objective values. Recall
that since the unrelaxed certiﬁcation problem is nonconvex, the solutions found using the
fmincon local search are not known to be globally optimal. Consequently, even though
the objective values of these local solutions may suggest that the network is robust, they
do not provide a formal certiﬁcate of robustness, since in general a gap between local and
global optimality may exist. Next, we solve the unpartitioned LP relaxation. Finally, we
solve three two-part partitioned LP relaxations, one per row of the weight matrix W , to
explore the eﬀectiveness of partitioning along diﬀerent rows, and how the results compare to
the optimality given by Theorem 8. The average times (taken over the 10 nominal inputs)
to solve the nonconvex approximation, unpartitioned LP relaxation, and partitioned LP
relaxations are 0.21, 0.26, and 0.48 seconds, respectively. The computational cost of the
two-part partitioned LP is twice that of the unpartitioned LP, both of which are very fast
for this network. This factor of 2 is expected, since the two LPs in the partitioned LP are
solved sequentially. However, we remark that the two LPs can easily be parallelized in order
to speed up the computation.

Figure 6a displays the resulting optimal objective values. The optimally partitioned LP,
i.e., that partitioned along row w(cid:62)
i∗ from Theorem 8, is clearly shown to give the tightest
upper bound for every nominal input tested. Two other partitioned LPs are displayed. In
particular, the second-best partition (suboptimally partitioned LP 1) deﬁned by

i1 ∈ arg min

i∈I\{i∗}

ReLU(ci)

uili
ui − li

is seen to provide the second-best upper bound, and similarly the third-best (suboptimally
partitioned LP 2) deﬁned by i2 ∈ arg mini∈I\{i∗,i1} ReLU(ci) uili
gives the worst bound out
ui−li
of the three. Hence, the order of optimality indicated by Theorem 8 holds true, despite the
result being derived in a worst-case setting.

This example shows that the suboptimally partitioned LP 2 (partitioned along the worst
row w(cid:62)
) coincides with the unpartitioned LP at every nominal input. This suggests that
i2
none of the relaxation error is attributed to the ith
2 coordinate of the ReLU layer, and
demonstrates the importance of using an intelligent and theoretically justiﬁed partitioning
scheme, as developed in Theorem 8, instead of partitioning heuristically. It can be observed
that the optimally partitioned LP is the only convex certiﬁcation scheme in this experiment
that is able to certify the robustness of this network, and it is able to do so at every input
tested.

Finally, we add a 5-neuron layer to the network and re-run the experiment on the result-
ing two-layer network. The results are displayed in Figure 6b. The optimally partitioned

34

Partition-Based Robustness Certification

LP is again seen to yield the best convex upper bound, substantially reducing the relax-
ation error. However, we remark that the worst-case upper bound from (22) was derived
for networks with one hidden layer, so it is violated by a handful of nominal inputs in the
two-layer case, unlike the one-layer case where the bound is guaranteed to hold. Despite
this, the eﬃcacy of partitioning, and in particular the partitioning scheme of Theorem 8,
is still seen to hold empirically for this multi-layer example. Lastly, we note that the com-
putation times for the nonconvex, unpartitioned LP, and partitioned LP rise to 0.94, 0.68,
and 1.48 seconds, respectively. The two-part partitioned LP maintains the polynomial-time
complexity (with respect to the number of neurons) of linear programming, since it requires
solving two instances of the same LP structure (Karmarkar, 1984). This time complexity
is further explored in Section 5.3 below.

e
u
l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

1

0

−1

2

4

8
Nominal Input Number

6

2

1

0

−1

e
u
l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

10

2

4

8
Nominal Input Number

6

10

(a) One-layer network.

(b) Two-layer network.

Nonconvex problem via multistart
Unpartitioned LP
Upper bound for optimally partitioned LP
Optimally partitioned LP
Suboptimally partitioned LP 1
Suboptimally partitioned LP 2

Figure 6: Optimal values of robustness certiﬁcation for ReLU Iris classiﬁer. For the two-
layer network, the optimal partitioning scheme is applied to only the ReLU con-
straints of the ﬁnal layer.

5.2 Partitioned SDP Results

In this experiment, we compare the performance of the LP-based certiﬁcates against the
SDP-based certiﬁcates on two classiﬁcation networks with one hidden layer each. In each
case, we test 10 nominal inputs ¯x using an input uncertainty set X = {x ∈ Rnx : (cid:107)x− ¯x(cid:107)∞ ≤
(cid:15)} with (cid:15) = 0.1, similar to the previous experiment. The ﬁrst network is the same Iris
classiﬁer used in Section 5.1, and the second network is a 5 × 5 network with randomly
generated weights, each element being a standard normal random variable. For each network
and nominal input, we approximate the nonconvex certiﬁcation problem using fmincon

35

Anderson, Ma, Li, and Sojoudi

with 5 multi-starts. We then solve the unpartitioned LP and partitioned LP according to
Theorem 8. Finally, we solve the unpartitioned SDP and the partitioned SDP according
to Theorem 19. The optimal objective values are displayed in Figure 7. As seen, the SDP
consistently outperforms the LP-based approaches, and the partitioning does not have a
noticeable eﬀect on this small network.

2

1

0

e
u
l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

−1

2

5

0

−5

e
u

l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

10

−10

2

4

8
Nominal Input Number

6

4

8
Nominal Input Number

6

10

(a) One-layer Iris network.

(b) One-layer random network.

Nonconvex problem via multistart
Unpartitioned LP
Partitioned LP
Unpartitioned SDP
Partitioned SDP

Figure 7: SDP relaxations compared to LP relaxations on one-layer networks with diﬀer-

ently generated weights.

The observed SDP and partitioned SDP performance matches the assertion recently
made in Zhang (2020), namely, that the SDP provides an exact relaxation (i.e., no re-
laxation error) of the one-layer ReLU robustness certiﬁcation problem under only mild
assumptions. In this case, if the unpartitioned SDP is exact, then certainly the partitioned
SDP will be exact, meaning no further improvement may be made. This is a ﬁrst indica-
tion that for relatively small networks with a single hidden layer, the unpartitioned SDP
is suﬃcient for computing certiﬁcates with little to no relaxation error, without needing to
resort to partitioning. However, as we will see in the next experiment, partitioning yields
a substantial improvement in the certiﬁcate once the network becomes two or more layers,
which is the case in many practical settings. We now move to this experiment, and propose
a general rule of thumb for when LP, SDP, and their partitioned variants are best applied
based on the depth and width of the network.

5.3 Eﬀectiveness as Network Grows

In this section, we perform two experiments to test the eﬀectiveness of partitioning as the
size and structure of the network changes. First, we consider two-layer networks of structure
nx×100×5, where nx is the input dimension. For each input size nx ∈ {5, 10, 20, 40, 80, 100},

36

Partition-Based Robustness Certification

we generate one network with standard normal random weights, and another network with
uniformly distributed weights (where each element is distributed uniformly on the interval
[0, 1]). The weights are normalized according to Assumption 16. For each network being
tested, we compute the LP, partitioned LP, SDP, and partitioned SDP relaxations at a ﬁxed
nominal input ¯x using the input uncertainty set X = {x ∈ Rnx : (cid:107)x − ¯x(cid:107)∞ ≤ (cid:15)} with (cid:15) =
0.5. The optimal values, corresponding computation times, and percentage improvements
induced by partitioning are reported in Table 1. The eﬀectiveness of partitioning for the LP
remains relatively constant between 5 and 10 percent improvement, whereas the partitioning
appears to lose its eﬃcacy on the SDP as the input size grows. As expected, the partitioned
convex relaxations take twice as long to solve as their unpartitioned counterparts. Note
that, despite the fact that partitioning works better for the LP with wide networks, the
actual optimal value of the SDP-based certiﬁcates are always lower (tighter) than the LP-
based ones. This matches what is known in the literature: the SDP is a tighter relaxation
technique than the LP (Raghunathan et al., 2018). However, the computation times of the
SDP and partitioned SDP quickly increase as the network size increases, whereas the LP
and partitioned LP computation times are seen to slowly increase. All of this suggests the
following: in the regime of shallow (i.e., one or two hidden layers) but very wide networks,
the partitioned LP should be used, since the partitioning remains eﬀective in tightening the
relaxation, yet the method is scalable to large networks where the SDP cannot be feasibly
applied.

In the second simulation of this section, we analyze the eﬀectiveness of partitioning
In particular, we consider networks with normal
as the depth of the network increases.
random weights having 5 inputs and 5 outputs, and each intermediate layer having 10
neurons. We run the experiment on networks having 1 through 6 such intermediate layers.
Note that when the network has more than one hidden layer, an extra step is needed in
order to apply the optimal LP partition from Theorem 8 since the number of rows n1 of
the ﬁrst layer’s weight matrix W [0] (i.e., the rows being partitioned along) may not equal
the dimension of the output space nz. The extra step is to generate a surrogate “c”-vector
of size n1 × 1 so that Theorem 8 can be applied using this surrogate cost vector. There are
a few ways of doing this. One such method is to treat the activation at the ﬁrst hidden
layer, x[1], as the output and determine which coordinate i ∈ Rn1 of the nominal activation
¯x[1] = ReLU(W [0] ¯x) is maximal. This means that i would be the class assigned to ¯x if the
output were after the ﬁrst hidden layer. Then, we ﬁnd the second-best coordinate j (cid:54)= i
so that ¯x[1]
for all other k. Afterwards, the surrogate vector c can be taken
as c = ej − ei, meaning that it serves as a measure of whether the classiﬁcation after the
ﬁrst hidden layer changes from i to j. In the case of our experiment, we choose the above
j (cid:54)= i randomly for simplicity. Of course, the surrogate vector c is only used to compute the
partitioning coordinate i∗ in Theorem 8, and the full network and original cost vector c are
used in the resultant partitioned LP.

j ≥ ¯x[1]

i ≥ ¯x[1]

k

Unlike the partitioned LP, the SDP partitioning scheme given in Theorem 19 can directly
be applied to deep networks, without the need to use the intermediate steps to compute the
partition. We compute the LP, partitioned LP, SDP, and partitioned SDP on the networks at
hand and report the objective values and computation times in Table 2. In this simulation,
we see a stark contrast to the results in Table 1. Speciﬁcally, the percentage improvement
induced by partitioning the LP reduces quickly to nearly zero percent for networks with 3

37

Anderson, Ma, Li, and Sojoudi

Table 1: Varying input size nx for nx × 100 × 5 ReLU network. Optimal values and corre-
sponding computation times reported. P-LP and P-SDP correspond to partitioned
LP and partitioned SDP, respectively. %-LP and %-SDP represent the percentage
tightening of the optimal values obtained from partitioning.

(a) Normally distributed network weights.

Input size

LP

P-LP

%-LP

SDP P-SDP %-SDP

5

10

20

40

80

100

126.93
0.71 s
187.57
0.77 s
386.49
0.71 s
874.70
1.27 s
1591.41
1.76 s
2184.94
0.78 s

117.92
1.46 s
176.19
1.36 s
364.53
1.42 s
864.56
2.68 s
1496.23
2.97 s
2175.87
1.84 s

7.10% 16.82
104.18% 1.66 s
6.07% 33.62
76.13% 1.54 s
5.68% 54.02
100.49% 1.85 s
1.16% 104.90
110.93% 4.79 s
5.98% 310.37
69.00% 9.81 s
0.42% 383.63
136.93% 5.02 s

14.83
3.33 s
32.96
3.16 s
54.01
4.31 s
104.38
9.33 s
310.31
17.87 s
383.50
10.52 s

11.85%
101.33%
1.98%
105.57%
0.02%
132.94%
0.49%
95.01%
0.02%
82.11%
0.03%
109.46%

(b) Uniformly distributed network weights.

Input size

LP

P-LP

%-LP

SDP P-SDP %-SDP

5

10

20

40

80

100

11.65
0.65 s
34.13
0.68 s
83.74
0.67 s
141.37
0.69 s
260.80
0.71 s
400.73
0.74 s

10.69
1.36 s
34.13
1.36 s
83.02
1.40 s
133.30
1.43 s
242.19
1.42 s
387.24
1.56 s

8.31%

5.95
109.54% 1.39 s
0.00% 12.61
101.35% 1.32 s
0.86% 19.20
106.88% 1.31 s
5.71% 25.89
106.67% 1.63 s
7.14% 21.86
99.25% 2.82 s
3.37% 102.87
111.10% 3.33 s

5.74
2.20 s
11.92
2.48 s
19.00
2.85 s
25.69
3.23 s
21.68
5.44 s
102.35
6.89 s

3.44%
58.32%
5.47%
87.45%
1.06%
118.39%
0.74%
97.62%
0.84%
92.97%
0.51%
106.64%

or more intermediate 10-neuron hidden layers. Indeed, this is one fundamental drawback
behind the LP relaxation: the convex upper envelope is used independently at every neuron,
so the relaxation error quickly compounds as the network becomes deeper. On the other
hand, the SDP relaxation takes into account the coupling between the layers of the network.
This theoretical advantage is demonstrated empirically, as the percentage improvement
gained by the partitioned SDP hovers around 10% even for the deep networks tested here.

38

Partition-Based Robustness Certification

Moreover, note how the SDP computation time remains relatively close to that of the LP,
unlike the rapid increase in computation time seen when increasing the input size. This
in the regime of deep but relatively narrow networks,
behavior suggests the following:
the partitioned SDP should be used, since the partitioning is eﬀective in tightening the
relaxation, yet the computational cost grows relatively slowly as more layers are added
(compared to the case where more inputs are added).

Table 2: Varying number of hidden layers for a 5 × 10 × 10 × · · · × 10 × 5 ReLU network with
normal random weights. Optimal values and corresponding computation times
reported. P-LP and P-SDP correspond to partitioned LP and partitioned SDP,
respectively. %-LP and %-SDP represent the percentage tightening of the optimal
values obtained from partitioning.

Layers

LP

P-LP

%-LP

SDP P-SDP %-SDP

1

2

3

4

5

6

10.16
0.59 s
46.29
0.62 s
626.96
0.61 s
5229.32
0.65 s
37625.91
0.69 s
326743.55
0.75 s

7.03
1.21 s
44.89
1.21 s
626.96
1.29 s
5229.32
1.29 s
37625.86
1.34 s
326743.34
1.35 s

30.79%
105.06%
3.03%
93.07%
0.00%
110.86%

4.70
0.68 s
2.42
0.71 s
36.29
0.72 s
0.00% 179.79
97.47%
0.99 s
0.00% 628.78
1.13 s
94.59%
0.00% 3245.41
1.19 s
79.44%

4.65
1.29 s
1.94
1.49 s
34.36
1.47 s
167.34
1.88 s
561.60
2.04 s
3050.69
2.35 s

1.12%
91.17%
19.94%
108.81%
5.31%
103.32%
6.93%
89.81%
10.68%
80.35%
6.00%
98.01%

6. Conclusions

In this paper, we propose intelligently designed partitioning schemes for linear program-
ming (LP) and semideﬁnite programming (SDP) robustness certiﬁcation methods of ReLU
neural networks. The partitions are derived by minimizing the worst-case error induced
by the corresponding convex relaxations, which is theoretically justiﬁed by showing that
minimizing the true relaxation error is NP-hard. The proposed techniques are experimen-
tally substantiated by demonstrating signiﬁcant reduction in relaxation error on real and
synthetic networks, with only doubling the computation time. Our experiments show that
the LP and SDP partitioning schemes exhibit tradeoﬀs between diﬀerent regimes, namely,
as the input size and the number of layers are varied. The results conclude that both LP
and SDP partitioning schemes yield a reduction in relaxation error on the order of 10%,
with LP best applying to shallow but wide networks, and SDP best applying to deep but
narrow networks. Consequently, partitioning proves to be a simple yet eﬀective method for
obtaining tighter robustness certiﬁcates for ReLU neural networks.

39

Anderson, Ma, Li, and Sojoudi

Acknowledgments

This work was supported by grants from AFOSR, ONR, and NSF.

Appendix A. Proof of Proposition 3

Proof Assume that f ∗(X ) > maxj∈{1,2,...,p}

ˆf ∗(X (j)). Then,

f ∗(X ) > ˆf ∗(X (j)) for all j ∈ {1, 2, . . . , p}.

(43)

Let (x∗, z∗) denote an optimal solution to the unrelaxed problem (2), i.e., x∗ ∈ X , z∗ =
f (x∗), and

(44)
Since ∪p
j=1X (j) = X , there exists j∗ ∈ {1, 2, . . . , p} such that x∗ ∈ X (j∗). Since x∗ ∈ X (j∗)
and z∗ = f (x∗), it holds that (x∗, z∗) ∈ N (j∗), where N (j∗) is the relaxed network constraint
set deﬁned by X (j∗). Therefore,

c(cid:62)z∗ = f ∗(X ).

c(cid:62)z∗ ≤ sup{c(cid:62)z : x ∈ X (j∗), (x, z) ∈ N (j∗)} = ˆf ∗(X (j∗)) < f ∗(X ),

where the ﬁrst inequality comes from the feasibility of (x∗, z∗) over the j∗th subproblem and
the ﬁnal inequality is due to (43). This contradicts the optimality of (x∗, z∗) given in (44).
Hence, (9) must hold.

Appendix B. Proof of Proposition 4

Proof Let j ∈ {1, 2, . . . , p}. It will be shown that N (j) ⊆ N . Let (x, z) ∈ N (j). Deﬁne
u(cid:48) = u(j), l(cid:48) = l(j), and

g(x) = u (cid:12) (W x − l) (cid:11) (u − l),
g(cid:48)(x) = u(cid:48) (cid:12) (W x − l(cid:48)) (cid:11) (u(cid:48) − l(cid:48)).

Then, by letting ∆g(x) = g(x) − g(cid:48)(x) = a (cid:12) (W x) + b, where

a = u (cid:11) (u − l) − u(cid:48) (cid:11) (u(cid:48) − l(cid:48)),
b = u(cid:48) (cid:12) l(cid:48) (cid:11) (u(cid:48) − l(cid:48)) − u (cid:12) l (cid:11) (u − l),

the following relations are derived for all i ∈ {1, 2, . . . , nz}:

g∗
i :=

inf
{x:l(cid:48)≤W x≤u(cid:48)}

(∆g(x))i ≥

=

inf
i≤ˆzi≤u(cid:48)
i}

{ˆzi:l(cid:48)

(ai ˆzi + bi) =

(cid:40)

inf
{ˆz:l(cid:48)≤ˆz≤u(cid:48)}
ail(cid:48)
aiu(cid:48)

i + bi
i + bi

(a (cid:12) ˆz + b)i

if ai ≥ 0,
if ai < 0.

In the case that ai ≥ 0, we have that

i ≥ ail(cid:48)
g∗

i + bi =

(cid:18) ui

ui − li

−

u(cid:48)
i
i − l(cid:48)
u(cid:48)
i

(cid:19)

l(cid:48)
i +

(cid:18) u(cid:48)
il(cid:48)
i
i − l(cid:48)
u(cid:48)
i

−

uili
ui − li

(cid:19)

=

ui
ui − li

(l(cid:48)

i − li) ≥ 0,

40

Partition-Based Robustness Certification

where the ﬁnal inequality comes from the fact that u ≥ 0, l(cid:48) ≥ l, and u > l. On the other
hand, if ai < 0, it holds that

i ≥ aiu(cid:48)
g∗

i + bi =

(cid:18) ui

ui − li

−

u(cid:48)
i
i − l(cid:48)
u(cid:48)
i
i − li) − u(cid:48)

i =

=

ui
ui − li

(u(cid:48)

(cid:19)

u(cid:48)
i +

(cid:18) u(cid:48)
il(cid:48)
i
i − l(cid:48)
u(cid:48)
i

−

uili
ui − li

(cid:19)

u(cid:48)
i − ui
ui − li

li ≥ 0,

where the ﬁnal inequality comes from the fact that u(cid:48) ≤ u, l ≤ 0, and u > l. Therefore,

g∗ = (g∗

1, g∗

2, . . . , g∗

nz ) ≥ 0,

which implies that ∆g(x) = g(x) − g(cid:48)(x) ≥ 0 for all x such that l(j) = l(cid:48) ≤ W x ≤ u(cid:48) = u(j).
Hence, since (x, z) ∈ N (j), it holds that z ≥ 0, z ≥ W x, and

z ≤ g(cid:48)(x) ≤ g(x) = u (cid:12) (W x − l) (cid:11) (u − l).

Therefore, we have that (x, z) ∈ N .

Since X (j) ⊆ X (by deﬁnition) and N (j) ⊆ N , it holds that the solution to the problem
over the smaller feasible set gives a lower bound to the original solution: ˆf ∗(X (j)) ≤ ˆf ∗(X ).
Finally, since j was chosen arbitrarily, this implies the desired inequality (10).

Appendix C. Proof of Proposition 10

Proof We prove the result by reducing an arbitrary instance of the Min-K-Union problem
to an instance of the optimal partitioning problem (29). The proof is broken down into
steps.
In Step 1, we introduce the Min-K-Union problem. We then construct a speciﬁc
neural network based on the parameters of the Min-K-Union problem in Step 2. In Step
3, we construct the solution to the partitioned LP relaxation for our neural network in the
case that the partition is performed along all input coordinates. In Step 4, we construct
the solution to the partitioned LP relaxation in the case that only a subset of the input
coordinates are partitioned. Finally, in Step 5, we show that the solution to the Min-K-
Union problem can be constructed from the solution to the optimal partitioning problem,
i.e., by ﬁnding the best subset of coordinates to partition along in the fourth step. As a
consequence, we show that optimal partitioning is NP-hard.

i=j Sj

(cid:12)
(cid:83)n
(cid:12)
(cid:12)

(cid:12)
(cid:12) ∈ N. Therefore, there exists a bijection between the elements of (cid:83)n
(cid:12)

Step 1: Arbitrary Min-K-Union Problem. Suppose that we are given an arbitrary
instance of the Min-K-Union problem, i.e., a ﬁnite number of ﬁnite sets S1, S2, . . . , Sn and a
positive integer K ≤ n. Since each set Sj is ﬁnite, the set (cid:83)n
j=1 Sj is ﬁnite with cardinality
m :=
j=1 Sj
and the set {1, 2, . . . , m}. Hence, without loss of generality, we assume Sj ⊆ N for all
j ∈ {1, 2, . . . , n} such that (cid:83)n
In this Min-K-Union problem, the
objective is to ﬁnd K sets Sj1, Sj2, . . . , SjK among the collection of n given sets such that
(cid:12)
(cid:12)
(cid:83)K
(cid:12)
(cid:12)
(cid:12) is minimized over all choices of K sets. In what follows, we show that the solution
(cid:12)

j=1 Sj = {1, 2, . . . , m}.

i=1 Sji

41

Anderson, Ma, Li, and Sojoudi

to this problem can be computed by solving a particular instance of the optimal partitioning
problem (29).

Step 2: Neural Network Construction. Consider a 3-layer ReLU network, where
x[0], x[1] ∈ Rn and x[2], x[3] ∈ Rm. Let the weight vector on the output be c = 1m. Take
the input uncertainty set to be X = [−1, 1]n. Let W [0] = In and W [2] = Im. In addition,
construct the weight matrix on the ﬁrst layer to be W [1] ∈ Rm×n such that

W [1]

ij =

(cid:40)
1
0

if i ∈ Sj,
otherwise.

We remark that, since all entries of c = 1m, W [0] = In, W [1], and W [2] = Im are nonnegative,
the optimal value of the unrelaxed certiﬁcation problem (26) is f ∗(X ) = 1(cid:62)

mW [1]1n.

To ﬁnish deﬁning the network and its associated LP relaxations, we must specify the
preactivation bounds at each layer. Since all weights of the neural network are nonnegative,
the largest preactivation at each layer is attained when the input is x[0] = 1n, the element-
wise maximum vector in X . The preactivations corresponding to this input are ˆz[1] = 1n,
ˆz[2] = W [1]1n, and ˆz[3] = W [1]1n. Therefore, setting

u[1] = 21n,

u[2] =

u[3] =

3
2
5
4

W [1]1n,

W [1]1n +

1
8

1m,

we obtain valid preactivation upper bounds. Similarly, taking

l[k] = −u[k]

for all k ∈ {1, 2, 3} deﬁnes valid preactivation lower bounds.

Step 3: Densely Partitioned LP Relaxation. With the network parameters
deﬁned, we now consider the ﬁrst variant of our partitioned LP relaxation. In particular,
we consider the relaxation where all coordinates of the ﬁrst layer are partitioned. We denote
by ¯f (X ) the optimal objective value of the problem

c(cid:62)x[3]

maximize
subject to x[0] ∈ X ,

x[k+1] ≥ W [k]x[k],
x[k+1] ≥ 0,
k ∈ {0, 1, 2},
x[k+1] ≤ u[k+1] (cid:12) (W [k]x[k] − l[k+1]) (cid:11) (u[k+1] − l[k+1]), k ∈ {0, 1, 2},
x[1] = ReLU(W [0]x[0]).

k ∈ {0, 1, 2},

(45)

This problem serves as a baseline; this is the tightest LP relaxation of the certiﬁcation
problem among all those with partitioning along the input coordinates.

We denote by ¯x = (¯x[0], ¯x[1], ¯x[2], ¯x[3]) an optimal solution of (45). We will now show

that

¯x[3] =

5
4

W [1]1n +

1
16

1m.

42

Partition-Based Robustness Certification

To see this, note that since all weights of the network and optimization (45) are nonnegative,
the optimal activations ¯x will be as large as possible in all coordinates and at all layers.
Therefore, since the input is constrained to X = [−1, 1]n, the optimal input for (45) is
¯x[0] = 1n. Since the ﬁrst ReLU constraint in (45) is exact, this implies that the optimal
activation at the ﬁrst layer is

¯x[1] = ReLU(W [0] ¯x[0]) = ReLU(1n) = 1n.

Now, for the second layer, the activation attains its upper bound. Since u[2] = −l[2] =
3
2 W [1]1n, this implies that

¯x[2] = u[2] (cid:12) (W [1] ¯x[1] − l[2]) (cid:11) (u[2] − l[2])
= u[2] (cid:12) (W [1] ¯x[1] + u[2]) (cid:11) (2u[2])

=

=

=

1
2
1
2
5
4

(W [1] ¯x[1] + u[2])
(cid:18)

W [1]1n +

W [1]1n

3
2

(cid:19)

W [1]1n.

Similarly, for the third layer, we ﬁnd that the optimal activation attains its upper bound
as well. Since u[3] = −l[3] = 5

8 1m and W [2] = Im this gives that

4 W [1]1n + 1

¯x[3] = u[3] (cid:12) (W [2] ¯x[2] − l[3]) (cid:11) (u[3] − l[3])

= u[3] (cid:12) (¯x[2] + u[3]) (cid:11) (2u[3])

=

=

=

1
2
1
2
5
4

(¯x[2] + u[3])
(cid:18) 5
4

W [1]1n +

5
4

W [1]1n +

(cid:19)

1
8

1m

W [1]1n +

1
16

1m,

as claimed in (3). It is easily veriﬁed that ¯x as computed above satisﬁes all constraints of
the problem (45).

Step 4: Sparsely Partitioned LP Relaxation. We now introduce the second
variant of the partitioned LP relaxation. In particular, let Jp ⊆ {1, 2, . . . , n} be an index
set such that |Jp| = np = n − K. Denote the complement of Jp by J c
p = {1, 2, . . . , n} \ Jp.
We consider the partitioned LP deﬁned in (28), which partitions along each coordinate in
the index set Jp. The optimal value of this problem is denoted by f ∗
(X ), and we denote
Jp
an optimal solution by ˆx = (ˆx[0], ˆx[1], ˆx[2], ˆx[3]). We will compute ˆx in three steps.

Step 4.1: Upper Bounding the Solution. We start by upper bounding the ﬁnal
layer activation of the solution. In particular, we claim that the optimal solution ˆx satisﬁes

ˆx[3] ≤ t := u[3] −

1
16

1Ic.

43

(46)

Anderson, Ma, Li, and Sojoudi

where I = (cid:83)
j∈J c
p
the bound (46) is equivalent to

Sj ⊆ {1, 2, . . . , m} and I c = {1, 2, . . . , m} \ I. Since ¯x[3] = u[3] − 1

16 1m,

ˆx[3]
i ≤ ti =

(cid:40)

u[3]
i
¯x[3]
i

if i ∈ I,
if i ∈ I c,

(47)

for all i ∈ {1, 2, . . . , m}. We now prove the element-wise representation of the bound, (47).
First, by the feasibility of ˆx and the deﬁnitions of u[3], l[3], it must hold for all i ∈

{1, 2, . . . , m} that

ˆx[3]
i ≤

u[3]
i
i − l[3]
u[3]

i

(w[2](cid:62)
i

ˆx[2] − l[3]

i ) =

1
2

(w[2](cid:62)
i

ˆx[2] + u[3]

i ),

and also that

i ≥ w[2](cid:62)
ˆx[3]
i
Combining these inequalities, we ﬁnd that ˆx[3]
i ≤ 1
i ≤ u[3]
ˆx[3]
This bound holds for all i ∈ {1, 2, . . . , m}, and therefore it also holds for i ∈ I. This proves
the ﬁrst case in the bound (47).

i ), or, equivalently, that

ˆx[2].
2 (ˆx[3]

i + u[3]

.

i

We now prove the second case of the claimed upper bound. For this case, suppose i /∈ I.

Then i /∈ Sj for all j ∈ J c

p , which implies that

by the deﬁnition of W [1]. Therefore,

W [1]

ij = 0 for all j ∈ J c
p ,

w[1](cid:62)
i

ˆx[1] =

n
(cid:88)

j=1

W [1]

ij ˆx[1]

j =

(cid:88)

j∈J c
p

W [1]

ij ˆx[1]

j +

(cid:88)

j∈Jp

W [1]

ij ˆx[1]

j =

W [1]

ij ˆx[1]
j .

(cid:88)

j∈Jp

Now, note that for j ∈ Jp, the jth coordinate of the input is being partitioned, and therefore
the optimal solution must satisfy
j = ReLU(w[0](cid:62)
ˆx[1]

j ˆx[0]) = ReLU(ˆx[0]

ˆx[0]) = ReLU(e(cid:62)

j ) ≤ 1,

j

since ˆx[0]

j ∈ [−1, 1]. Therefore,

w[1](cid:62)
i

ˆx[1] ≤

W [1]

ij ≤

(cid:88)

j∈J c
p

n
(cid:88)

j=1

W [1]

ij = w[1](cid:62)

i

1n.

It follows from the feasibility of ˆx and the deﬁnitions of u[2], l[2] that

ˆx[2]
i ≤

u[2]
i
u[2]
i − l[2]
i
(cid:18)
1
w[1](cid:62)
i
2

≤

1n +

(w[1](cid:62)
i

ˆx[1] − l[2]

i ) =

1
2

(w[1](cid:62)
i

ˆx[1] + u[2]
i )

3
2

w[1](cid:62)
i

1n

(cid:19)

=

5
4

w[1](cid:62)
i

1n = ¯x[2]
i

,

44

Partition-Based Robustness Certification

where ¯x is the solution computed for the densely partitioned LP relaxation in Step 3.
Therefore, we conclude that for all i /∈ I, it holds that

ˆx[3]
i ≤

u[3]
i
i − l[3]
u[3]

i

(w[2](cid:62)
i

ˆx[2] − l[3]

i ) ≤

u[3]
i
i − l[3]
u[3]

i

(w[2](cid:62)
i

¯x[2] − l[3]

i ) = ¯x[3]

i

,

by our previous construction of ¯x[3]. Thus, we have proven the second case in (47) holds.
Hence, the claimed bound (46) holds.

Step 4.2: Feasibility of Upper Bound.

Let us deﬁne x = (x[0], x[1], x[2], x[3]), a

point in Rn × Rn × Rm × Rm, by

x[0] = 1n,

x[1] = 1Jp +

5
4

p ,
1J c

x[2] = u[3] −

1
8

1Ic,

x[3] = u[3] −

1
16

1Ic.

Note that x[3] equals the upper bound t = (t1, t2, . . . , tm). We now show that x is feasible
for (28).

First, the input uncertainty constraint is satisﬁed, since x[0] = 1n ∈ X . Next, the

relaxed ReLU constraints at the ﬁrst layer are satisﬁed, since

x[1] = 1Jp +

5
4
x[1] − W [0]x[0] =

1J c

p ≥ 0,

1
4

1J c

p ≥ 0,

(Layer 1 lower bound.)

(Layer 1 lower bound.)

x[1] − u[1] (cid:12) (W [0]x[0] − l[1]) (cid:11) (u[1] − l[1]) = −

1
2

1Jp −

1
4

1J c

p ≤ 0. (Layer 1 upper bound.)

The relaxed ReLU constraints are also satisﬁed in the second layer, since

x[2] =

5
4

1I ≥ 0,

1
8
W [1]1Jp +

W [1]1n +
1
4

1
8

1I ≥ 0,

x[2] − W [1]x[1] =

x[2] − u[2] (cid:12) (W [1]x[1] − l[2]) (cid:11) (u[2] − l[2]) =

(Layer 2 lower bound.)

(Layer 2 lower bound.)

1
8

(1I − W [1]1J c

p ) ≤ 0.

(Layer 2 upper bound.)

The ﬁnal inequality above follows from the fact that either (1I)i = 0 or (1I)i = 1. For
coordinates i such that (1I)i = 0, the inequality obviously holds. For coordinates i such
that (1I)i = 1, we know that i ∈ I, implying that i ∈ Sj for some j ∈ J c
p . This in turn
W [1]
implies that W [1]
p , and therefore w[1](cid:62)
ij ≥ 1 = (1I)i.
Continuing to check feasibility of x, the relaxed ReLU constraints in the ﬁnal layer are

ij = 1 for some j ∈ J c

p = (cid:80)

j∈J c
p

1J c

i

also satisﬁed:

x[3] =

5
4

W [1]1n +

1
16
x[3] − W [2]x[2] =

1m +

1
16

1I ≥ 0,

1
16

1Ic ≥ 0,

(Layer 3 lower bound.)

(Layer 3 lower bound.)

x[3] − u[3] (cid:12) (W [2]x[2] − l[3]) (cid:11) (u[3] − l[3]) = 0 ≤ 0.

(Layer 3 upper bound.)

45

Anderson, Ma, Li, and Sojoudi

Hence, the relaxed ReLU constraints are satisﬁed at all layers. The only remaining con-
straints to verify are the exact ReLU constraints for the partitioned input indices Jp. Indeed,
for all j ∈ Jp, we have that

x[1]
j − ReLU(W [0]x[0])j = (1Jp)j +

5
4

(1J c

p )j − ReLU(1n)j = 1 + 0 − 1 = 0.

Hence, the ReLU equality constraint is satisﬁed for all input coordinates in Jp. Therefore,
our proposed point x is feasible for (28).

Step 4.3: Solution to Sparsely Partitioned LP. As shown in the previous step,
the proposed point x = (x[0], x[1], x[2], x[3]) is feasible for (28). Recall from the upper bound
(46) that our solution ˆx = (ˆx[0], ˆx[1], ˆx[2], ˆx[3]) satisﬁes ˆx[3] ≤ t. The objective value of the
feasible point x gives that

c(cid:62)x[3] =

m
(cid:88)

i=1

x[3] =

m
(cid:88)

i=1

ti ≥

m
(cid:88)

i=1

ˆx[3]
i = f ∗

Jp(X ).

Since f ∗
Jp
c(cid:62)x[3] = f ∗
Jp
the ﬁnal activation of our optimal solution ˆx to (28) as

(X ) is the maximum value of the objective for all feasible points, it must be that
(X ). Hence, the point x is an optimal solution to (28). Therefore, we can write

ˆx[3] = x[3] = t = u[3] −

1
16

1Ic.

(48)

Step 5: Min-K-Union from Optimal Partition. With the solutions constructed in
Steps 3 and 4, we compute the diﬀerence in the objective values between the two partitioned
LP relaxations:

Jp(X ) − ¯f (X ) = c(cid:62) ˆx[3] − c(cid:62) ¯x[3] = c(cid:62)
f ∗

(cid:18)

u[3] −

= c(cid:62)

(cid:18) 5
4

W [1]1n +

1
8

1m −

1
16

1Ic −

5
4

W [1]1n −

1
16
1
16

1Ic −

(cid:19)

1m

=

Therefore,

=

1
16

c(cid:62)1I =

1
16

(cid:88)

i∈I

1 =

1
16

|I| =

1
16

(cid:91)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j∈J c
p

(cid:91)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j∈J c
p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Sj

(cid:16)

= 16

Jp(X ) − ¯f (X )
f ∗

(cid:17)

,

5
4

W [1]1n −

(cid:19)

1
16

1m

1
16
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Sj

c(cid:62)(1m − 1Ic)

(49)

which holds for all partition index sets Jp ⊆ {1, 2, . . . , n} such that |Jp| = np = n − K.

Now, let J ∗

(cid:16)

p be an optimal partition, i.e., a solution to (29) with our speciﬁed neu-
(cid:12)
(cid:83)
(cid:12)
≤
(cid:12)

(cid:17)
(X ) − ¯f (X )
p )c Sj
(cid:12)
(cid:12) for all Jp with |Jp| = np. Since this holds for all J c
(cid:12)

p with
16
p )c is an optimal solution to the Min-K-Union
|J c
problem speciﬁed at the beginning of the proof. Now, suppose the optimal partitioning

ral network parameters. Then, by (49), we have
(cid:12)
(cid:83)
(cid:12)
(cid:12)
p | = n − np = K, this shows that the set (J ∗

(cid:17)
(X ) − ¯f (X )

(cid:12)
(cid:12)
(cid:12) = 16

f ∗
J ∗
p

f ∗
Jp

j∈(J ∗

j∈J c
p

Sj

=

(cid:16)

46

Partition-Based Robustness Certification

problem in (29) could be solved for J ∗
p in polynomial time. Then the optimal solution
(J ∗
p )c to the Min-K-Union problem is also computable in polynomial time. Since this holds
for an arbitrary instance of the Min-K-Union problem, this implies that the Min-K-Union
problem is polynomially solvable in general, which is a contradiction. Therefore, the prob-
lem (29) is NP-hard in general.

Appendix D. Proof of Proposition 11

Proof Let j ∈ {1, 2, . . . , p}. From the deﬁnition of the partition, it holds that X (j) ⊆ X .
What remains to be shown is that N (j)

SDP ⊆ NSDP.

Let P ∈ N (j)

SDP. Deﬁne u(cid:48) = u(j) and l(cid:48) = l(j). Since P ∈ N (j)
Pz ≥ 0,
Pz ≥ W Px,
diag(Pzz) = diag(W Pxz),
diag(Pxx) ≤ (l(cid:48) + u(cid:48)) (cid:12) Px − l(cid:48) (cid:12) u(cid:48),

SDP, it follows that

P1 = 1,
P (cid:23) 0.

To show that P ∈ NSDP, we should show that the above expressions imply that diag(Pxx) ≤
(l + u) (cid:12) Px − l (cid:12) u. To do so, deﬁne ∆li ≥ 0 and ∆ui ≥ 0 such that l(cid:48)
i = li + ∆li and
u(cid:48)
i = ui − ∆ui for all i ∈ {1, 2, . . . , nx}. Then we ﬁnd that
iu(cid:48)
i
= (li + ui)(Px)i − liui + (∆li − ∆ui)(Px)i

((l(cid:48) + u(cid:48)) (cid:12) Px − l(cid:48) (cid:12) u(cid:48))i = (l(cid:48)

i)(Px)i − l(cid:48)

i + u(cid:48)

− (∆liui − ∆uili − ∆ui∆li)

= ((l + u) (cid:12) Px − l (cid:12) u)i + (∆li − ∆ui)(Px)i

− (∆liui − ∆uili − ∆ui∆li)
= ((l + u) (cid:12) Px − l (cid:12) u)i + ∆i,

where ∆i := (∆li − ∆ui)(Px)i − (∆liui − ∆uili − ∆ui∆li). Therefore, it suﬃces to prove that
∆i ≤ 0 for all i. Since −liui ≥ −l(cid:48)
i by deﬁnition, it holds that ∆liui − ∆uili − ∆ui∆li ≥ 0.
Thus, when (∆li − ∆ui)(Px)i ≤ 0, it holds that ∆i ≤ 0, as desired. On the other hand,
suppose that (∆li − ∆ui)(Px)i ≥ 0. Then we ﬁnd two cases:

iu(cid:48)

1. (∆li − ∆ui) ≥ 0 and (Px)i ≥ 0. In this case, the maximum value of (∆li − ∆ui)(Px)i

is (∆li − ∆ui)u(cid:48)

i. Therefore, the maximum value of ∆i is

∆i = (∆li − ∆ui)u(cid:48)

i − (∆liui − ∆uili − ∆ui∆li)

= ∆li(u(cid:48)
i − ui) − ∆uiu(cid:48)
= ∆li(−∆ui) + ∆ui∆li − ∆uiu(cid:48)
= − ∆uiu(cid:48)
i + ∆uili.

i + ∆uili + ∆ui∆li
i + ∆uili

Both of the two ﬁnal terms are nonpositive, and therefore ∆i ≤ 0.

47

Anderson, Ma, Li, and Sojoudi

2. (∆li − ∆ui) ≤ 0 and (Px)i ≤ 0. In this case, the maximum value of (∆li − ∆ui)(Px)i

is (∆li − ∆ui)l(cid:48)

i. Therefore, the maximum value of ∆i is

∆i = (∆li − ∆ui)l(cid:48)

i − (∆liui − ∆uili − ∆ui∆li)

= − ∆ui∆li + ∆ui∆li + ∆lil(cid:48)
= ∆lil(cid:48)

i − ∆liui.

i − ∆liui

Both of the two ﬁnal terms are nonpositive, and therefore ∆i ≤ 0.

Hence, we ﬁnd that (l(cid:48) + u(cid:48)) (cid:12) Px − l(cid:48) (cid:12) u(cid:48) ≤ (l + u) (cid:12) Px − l (cid:12) u for all P ∈ N (j)
that P ∈ NSDP, and therefore N (j)
SDP ⊆ NSDP.
Since X (j) ⊆ X and N (j)

smaller feasible set lower bounds the original solution: ˆf ∗
since j was chosen arbitrarily, this implies the desired inequality (31).

SDP ⊆ NSDP, it holds that the solution to the problem over the
SDP(X ). Finally,

SDP(X (j)) ≤ ˆf ∗

SDP, proving

Appendix E. Proof of Lemma 17

Proof Let i, j ∈ {1, 2, . . . , n}. Since P is positive semideﬁnite, the 2nd-order principal
minor PiiPjj − P 2

ij is nonnegative, and therefore

|Pij| ≤ (cid:112)PiiPjj.

(50)

Furthermore, by the basic inequality that 2ab ≤ a2 + b2 for all a, b ∈ R, we have that
(cid:112)PiiPjj ≤ 1

2 (Pii + Pjj). Substituting this inequality into (50) gives the desired bound.

References

Brendon G. Anderson and Somayeh Sojoudi. Certifying neural network robustness to ran-

dom input noise from samples. arXiv preprint arXiv:2010.07532, 2020a.

Brendon G. Anderson and Somayeh Sojoudi. Data-driven assessment of deep neural net-

works with random input uncertainty. arXiv preprint arXiv:2010.01171, 2020b.

Brendon G. Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi. Tightened convex re-
laxations for neural network robustness certiﬁcation. In Proceedings of the 59th IEEE
Conference on Decision and Control, 2020.

Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin Vechev.
Certifying geometric robustness of neural networks. In Advances in Neural Information
Processing Systems, pages 15287–15297, 2019.

Dimitris Bertsimas and Iain Dunning. Multistage robust mixed-integer optimization with

adaptive partitions. Operations Research, 64(4):980–998, 2016.

48

Partition-Based Robustness Certification

Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin
Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv
preprint arXiv:1604.07316, 2016.

Michael Everett, Golnaz Habibi, and Jonathan P. How. Robustness analysis of neural
networks via eﬃcient partitioning: Theory and applications in control systems. arXiv
preprint arXiv:2010.00540, 2020.

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of clas-
siﬁers: from adversarial to random noise. In Advances in Neural Information Processing
Systems, pages 1632–1640, 2016.

Mahyar Fazlyab, Manfred Morari, and George J. Pappas. Safety veriﬁcation and robust-
ness analysis of neural networks via quadratic constraints and semideﬁnite programming.
IEEE Transactions on Automatic Control, 2020. doi: 10.1109/TAC.2020.3046193.

Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals of

eugenics, 7(2):179–188, 1936.

Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi. Robustness of classiﬁers to uni-
form (cid:96)p and Gaussian noise. In Amos Storkey and Fernando Perez-Cruz, editors, Proceed-
ings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics,
volume 84 of Proceedings of Machine Learning Research, pages 1280–1288. PMLR, April
2018.

Dorit S. Hochbaum. Approximating Covering and Packing Problems: Set Cover, Vertex
Cover, Independent Set, and Related Problems, pages 94–143. PWS Publishing Co.,
USA, 1996. ISBN 0534949681.

Ming Jin, Javad Lavaei, Somayeh Sojoudi, and Ross Baldick. Boundary defense against cy-
ber threat for power system state estimation. IEEE Transactions on Information Foren-
sics and Security, 16:1752–1767, 2020. doi: 10.1109/TIFS.2020.3043065.

Ming Jin, Heng Chang, Wenwu Zhu, and Somayeh Sojoudi. Power up! Robust graph con-
volutional network via graph powering. 35th AAAI Conference on Artiﬁcial Intelligence,
2021. to appear.

Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Pro-
ceedings of the sixteenth annual ACM symposium on Theory of computing, pages 302–311,
1984.

Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex:
An eﬃcient smt solver for verifying deep neural networks. In International Conference
on Computer Aided Veriﬁcation, pages 97–117. Springer, 2017.

Weicong Kong, Zhao Yang Dong, Youwei Jia, David J. Hill, Yan Xu, and Yuan Zhang.
Short-term residential load forecasting based on LSTM recurrent neural network. IEEE
Transactions on Smart Grid, 10(1):841–851, 2017.

49

Anderson, Ma, Li, and Sojoudi

Ziye Ma and Somayeh Sojoudi. Strengthened SDP veriﬁcation of neural network robustness

via non-convex cuts. arXiv preprint arXiv:2010.08603, 2020.

Guido F. Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number
of linear regions of deep neural networks. In Advances in neural information processing
systems, pages 2924–2932, 2014.

K. Muralitharan, Rathinasamy Sakthivel, and R. Vishnuvarthan. Neural network based
optimization approach for energy demand prediction in smart grid. Neurocomputing,
273:199–208, 2018.

Xiang Pan, Tianyu Zhao, and Minghua Chen. DeepOPF: Deep neural network for DC
optimal power ﬂow. In 2019 IEEE International Conference on Communications, Control,
and Computing Technologies for Smart Grids (SmartGridComm), pages 1–6. IEEE, 2019.

Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semideﬁnite relaxations for cer-
tifying robustness to adversarial examples. In Advances in Neural Information Processing
Systems, pages 10877–10887, 2018.

Vicenc Rubies Royo, Roberto Calandra, Dusan M. Stipanovic, and Claire Tomlin. Fast
neural network veriﬁcation via shadow prices. arXiv preprint arXiv:1902.07247, 2019.

Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling
deep neural networks. IEEE Transactions on Evolutionary Computation, 23(5):828–841,
2019.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International
Conference on Learning Representations, 2014.

Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane
Boning, and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for ReLU
networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pages 5276–5285, Stockholmsm¨assan, Stockholm Sweden, July 2018. PMLR.

Lily Weng, Pin-Yu Chen, Lam Nguyen, Mark Squillante, Akhilan Boopathy, Ivan Oseledets,
and Luca Daniel. Proven: Verifying robustness of neural networks with a probabilistic
approach. In International Conference on Machine Learning, pages 6727–6736. PMLR,
2019.

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope.
In Jennifer Dy and Andreas Krause, editors, Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of
Machine Learning Research, pages 5286–5295, Stockholmsm¨assan, Stockholm Sweden,
July 2018. PMLR.

Bichen Wu, Forrest Iandola, Peter H. Jin, and Kurt Keutzer. Squeezedet: Uniﬁed, small, low
power fully convolutional neural networks for real-time object detection for autonomous

50

Partition-Based Robustness Certification

driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition Workshops, pages 129–137, 2017.

Weiming Xiang and Taylor T. Johnson. Reachability analysis and safety veriﬁcation for

neural network control systems. arXiv preprint arXiv:1805.09944, 2018.

Huan Zhang, Lily Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Eﬃcient neural
network robustness certiﬁcation with general activation functions. In Advances in neural
information processing systems, pages 4939–4948, 2018.

Richard Zhang. On the tightness of semideﬁnite relaxations for certifying robustness to
adversarial examples. Advances in Neural Information Processing Systems, 33, 2020.

51

