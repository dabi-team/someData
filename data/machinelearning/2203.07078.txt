2
2
0
2

r
a

M
1
1

]

G
L
.
s
c
[

1
v
8
7
0
7
0
.
3
0
2
2
:
v
i
X
r
a

A Mixed Integer Programming Approach for Verifying Properties of
Binarized Neural Networks

Christopher Lazarus∗ and Mykel J. Kochenderfer
Stanford University
{clazarus, mykel}@stanford.edu

Abstract

Many approaches for verifying input-output prop-
erties of neural networks have been proposed re-
cently. However, existing algorithms do not scale
well to large networks. Recent work in the ﬁeld
of model compression studied binarized neural net-
works (BNNs), whose parameters and activations
are binary. BNNs tend to exhibit a slight decrease
in performance compared to their full-precision
counterparts, but they can be easier to verify. This
paper proposes a simple mixed integer program-
ming formulation for BNN veriﬁcation that lever-
ages network structure. We demonstrate our ap-
proach by verifying properties of BNNs trained on
the MNIST dataset and an aircraft collision avoid-
ance controller. We compare the runtime of our
approach against state-of-the-art veriﬁcation algo-
rithms for full-precision neural networks. The re-
sults suggest that the difﬁculty of training BNNs
might be worth the reduction in runtime achieved
by our veriﬁcation algorithm.

1 Introduction

Neural networks have been shown to be susceptible to ad-
versarial attacks [Papernot et al., 2016] often leading to dras-
tically different outputs when slightly perturbing their input
which can be costly or dangerous. Multiple approaches to
evaluate the robustness of networks to adversarial attacks
have been developed. Many of these only provide statistical
assessments and focus on evaluating the network on a large
but ﬁnite collection of points in the input space. However,
given that the input space is, in principle, inﬁnite in cardinal-
ity, it is not viable to assess the output for all the points in the
input space.

Recently, new approaches have emerged as an alternative
to formally certify the input-output properties of neural net-
works [Liu et al., 2021]. Properties are often speciﬁed in the

∗Contact Author
Copyright © 2021 for this paper by its authors. Use permitted
under Creative Commons License Attribution 4.0 International (CC
BY 4.0).

form of a statement that if the input belongs to a set X , then
the output is in a set Y. In the context of control, this formu-
lation can be used to verify that the network satisﬁes safety
constraints; in the classiﬁcation setting, this formulation can
be used to certify that points near a training sample are as-
signed the same label as that sample.

Veriﬁcation algorithms are typically designed to be sound,
which means that they will only conclude a property holds if
the property truly holds [Katz et al., 2017; Katz et al., 2019].
With the aim to improve efﬁciency, some approaches sac-
riﬁce completeness, meaning that even if a property holds,
the algorithm might fail to identify it. Incomplete algorithms
often rely on over-approximation, allowing them to scale to
problems involving larger networks, high dimensional input
spaces, or high dimensional output spaces.

Even when restricting the class of networks to those with
Rectiﬁed Linear Unit (ReLU) activation functions (or even
a more general piecewise-linear function) the problem has
been shown to be NP-hard [Katz et al., 2017]. The hard-
ness of the veriﬁcation problem has motivated many differ-
ent approaches [Liu et al., 2021], including reachability, opti-
mization and search algorithms. Even incomplete algorithms
struggle to verify properties of networks with sizes commonly
encountered in contemporary applications. In principle, sim-
pler models should be easier to verify and binarized neural
networks (BNNs) [Hubara et al., 2016] are simpler than tra-
ditional full precision neural networks. Their parameters are
binary and their activations are binary.

Binarization is an extreme quantization scheme that can
signiﬁcantly reduce memory and computation requirements.
However, binarization introduces non-differentiable and even
non-continuous blocks to the computational graph of a net-
work, which complicates the optimization used to train the
network. However, recent work motivated by their applicabil-
ity in highly constrained environments such as edge devices
has enabled them to achieve performance comparable to tra-
ditional full precision networks [Hubara et al., 2016].

The reduced memory requirement and simpliﬁed com-
putation resulting from this representation has a draw-
back: binarized neural networks are harder to train. A
major challenge is back propagating the gradient of the
weights through sign functions. There are workarounds
[Simons and Lee, 2019] such as using straight-through esti-
mators (STE) [Hubara et al., 2016].

 
 
 
 
 
 
This paper presents an approach to verify properties of
BNNs that leverages their structure. The veriﬁcation problem
is formulated as a mixed integer programming (MIP) problem
that encodes the input set X by constraining variables asso-
ciated with the input layer of the network and the output set
Y by constraining variables associated to the output layer of
the network. In our approach, we leverage the binary nature
of both the parameters and the activations of the network.

Experimentally, we show that our approach can ver-
ify properties of BNNs.
Section 4 demonstrates the
capabilities of our approach by formally assessing the
adversarial robustness of BNNs trained on the MNIST
[LeCun and Cortes, 2010] dataset and properties of a colli-
sion avoidance system [Katz et al., 2017]. We compare the
runtime of our approach to that of the state-of-the-art imple-
mentation of a full precision network veriﬁcation algorithm
for the equivalent full precision networks. Our proposed ap-
proach reduces signiﬁcantly veriﬁcation runtime.

2 Background
2.1 Neural Networks
Consider F a feedforward neural network with n layers with
input x ∈ Dx ⊆ Rk0 and output y ∈ Dy ⊆ Rkn . Here,
y = F (x) corresponds to evaluating the network on input
x and obtaining output y, k0 is the dimensionality of x, and
kn is the dimensionality of y. We assume that all inputs and
outputs are ﬂattened to vectors. Each layer in F is a function
: Rki−1 → Rki , where ki is the dimensionality of the
fi
hidden variable zi in layer i. Accordingly, z0 = x and zn =
y. The network can be represented by

F = fn ◦ fn ◦ fn−1 ◦ · · · ◦ f1

(1)

and

(2)
zi = fi(zi−1) = σi (Wizi−1 + bi)
where Wi ∈ Rki×ki−1 is the weight matrix, bi ∈ Rki is the
bias vector, and σi : Rki → Rki is the activation function.

Let zi,j be the value of the jth node in the ith layer, wi,j ∈
R1×ki−1 be the jth row of Wi, and wi,j,k be the kth entry in
wi,j . Given that activation functions are usually component-
wise vector functions we have that

zi,j = σi,j (wi,j zi−1) = σ

wi,j,kzi−1,k

k
X

= σi,j (ˆzi,j)

!

(3)

(4)

where ˆzi := Wizi−1. We dropped the bias terms in this anal-
ysis for compactness and without loss of generality.

2.2 Binarized Neural Networks
A binarized neural network is a network involving binary
weights and activations [Hubara et al., 2016]. The goal of
network binarizaion is to represent the ﬂoating-point weights
W and the activations zi,j for a given layer using 1 bit. The
parameters are represented by:

Q(W ) = αBW

Q(z) = βBz

(5)

where BW and Bz are binarized weights and binarized acti-
vations, with scaling factors α and β used for batch normal-
ization. The sign function is often used to compute QW and
Qz

sign(x) =

(cid:26)

if x ≥ 0

+1,
−1, otherwise

(6)

The above representation enables an easy implementation
of batch normalization while keeping most parameters and
operations binary.

In this context, the arithmetic operations needed for a for-
ward pass of a layer zb in a binarized network F b can be
computed as:

zb
i = σ

Q(W )zb
i

Q(W )izb
= fi(zb
= σ (αβBW ⊚ Bz) = αβ sign (BW ⊚ Bz)

i−1) = σi

i−1

(cid:0)

(cid:1)

(cid:0)

(7)

(8)

(cid:1)

where ⊚ denotes the inner product for binary vectors with
bitwise operation XNOR-Bitcount.
Linear.

ˆzi = Qizi−1

(9)

where Qi ∈ {−1, 1}ki+1×k.
Batch Normalization.

ˆzi = αki

yi − µki

(cid:18)

σki (cid:19)

+ γk

(10)

where y = (y1, . . . , ynk+1) and αk, γk, σki ∈ R.
Activation Function.

zi = sign(ˆzi)

(11)

where ˆzi ∈ Rki+1 and zi ∈ {−1, 1}ki+1.

for binarized inputs as
Some BNN architectures call
well. Our veriﬁcation approach does not require binarized
inputs, but this requirement is easy to incorporate by rep-
resenting ﬂoating or ﬁxed point inputs as a combination of
binary inputs, either by quantizing or directly using their
binary representation.

Section 3 derives how each type of block can be encoded
with linear constraints, enabling the formulation of a mixed-
integer programming problem for veriﬁcation purposes. The
last layer of a network can be used in different ways, often
using a softmax or an argmax operator. In either case, we can
encode desired properties at the output of the layer before
such functions, at the so called logits.

2.3 Neural Network Veriﬁcation
The veriﬁcation problem consists of checking whether input-
output relationships of a function hold [Liu et al., 2021]. A
subset of the input space X ⊆ Dx and a subset of the output
space Y ⊆ Dy are deﬁned. In its most general form, solv-
ing the veriﬁcation problem requires certifying whether the
following holds:

x ∈ X =⇒ y = F (x) ∈ Y.

(12)

In general, the input and output sets could have any geometry
but are often constrained by the veriﬁcation algorithm. Our

 
approach restricts the class of X and Y to closed polytopes,
corresponding to the intersection of half-spaces. These re-
gions can be encoded as a conjunction of linear constraints,
which is required for our mixed integer programming formu-
lation.

Applications Given that neural networks provide the state-
of-the-art performance for many tasks, such as perception and
control [Katz et al., 2017] tasks, studying their robustness has
attracted signiﬁcant attention [Papernot et al., 2016]. In the
context of image classiﬁcation, a network F assigns a value
to each of the possible labels in its training set and the max-
imum value arg maxi yi is often used to impute the label of
an input x. Consider an input x0 with label i∗ ∈ {1, . . . , ln}.
It would be desirable that yi∗ > yk for all j 6= i∗, which can
be encoded with the following sets:

x ∈ Dx : kx − x0kp ≤ ǫ

X =
,
Y = {y ∈ Dy : yi∗ > yj∀j 6= i∗} ,
o

n

(13)

(14)

where ǫ is the radius of the allowable perturbation in the in-
put. If p = 1 or p = ∞, we have linear constraints. En-
coding the output set Y is not possible with a single linear
program given that the maximum operator requires a disjunc-
tion of half-spaces. With our MIP formulation, the set can be
encoded directly.

Full Precision Neural Network Veriﬁcation
There are plenty of approaches to verifying properties of neu-
ral networks [Liu et al., 2021]. Some methods approach ver-
iﬁcation as an optimization problem, which is an idea that
we will use here. Many methods that only work for piece-
wise linear functions implement a search strategy over the
activation state of the nodes in the network. Some apply Sat-
isﬁability Modulo Theory (SMT) by iteratively searching for
assignments that satisfy all given constraints while treating
the non-linear constraints lazily. One such method is the Re-
luplex algorithm [Katz et al., 2017; Katz et al., 2019], which
can be used to verify properties of binarized neural networks.
However, without signiﬁcant modiﬁcation, it would not ex-
ploit the speciﬁc characteristics of BNNs.

Binarized Neural Network Veriﬁcation
There are a few approaches designed speciﬁcally for
Some approaches rely
verifying properties of BNNs.
on SAT solvers by reducing the veriﬁcation problem to
a Boolean satisﬁability problem [Narodytska et al., 2018;
Narodytska et al., 2020] which limits their applicability ex-
clusively to fully binarized networks (networks with exclu-
sively binary parameters and activations). Recently, a SAT
solver based approach that is able to handle BNN con-
straints to speedup the veriﬁcation process was introduced
[Jia and Rinard, 2020a].

Another approach that can be applied to networks
with both binary and full precision parameters and piece-
wise linear activations functions was recently introduced
[Amir et al., 2021] and is an SMT-based technique that ex-
tends the Reluplex algorithm [Katz et al., 2019] and in-
cludes various optimizations speciﬁcally designed to leverage
BNNs.

Our approach relies on a simple mixed integer linear pro-
gramming formulation, allowing us to handle both fully
and partially binarized neural networks. A similar ap-
proach has been demonstrated to work well for DNNs
[Tjeng et al., 2019] and MIP has been also used for reachabil-
ity analysis [Lomuscio and Maganti, 2017]. Our main contri-
bution is to apply MIP to the veriﬁcation of BNNs.

3 Veriﬁcation of BNNs using Mixed Integer

Programing

Binarized neural networks are composed of piecewise-linear
layers that may be fully connected, convolutional, or average-
pooling that may have piecewise-linear activation functions,
such as ReLU, max and sign. Other commonly used ele-
ments such as batch normalization, shortcut connections, and
dropout can be also characterized as afﬁne transformations at
evaluation time.

In order to address the veriﬁcation problem in Section 2.3,
we encode each of the components of the network as a set of
linear constraints and then apply existing MIP technology to
solve it. We encode the input and the linear equations that de-
scribe the forward-pass of the network. We then encode Y C ,
the complement of the output set Y, and search for a feasible
assignment. Any feasible assignment corresponds to a coun-
terexample of the desired property. If no feasible assignment
is found, then we can conclude that the network maps all the
points in X to Y, which certiﬁes the property.

Below we present the formulation of each block as a set of

linear constraints.

Input and Output Sets. The veriﬁcation problem consists
of determining whether all the elements in an input set X
map to an output set Y. In order to formulate that problem
as a MIP, then X must be expressible as a conjunction of lin-
ear constraints. Additionally, given that the MIP solver will
search for a feasible assignment, we need to formulate Y C as
another polyhedron given that

x ∈ X =⇒ y = F (x) ∈ Y

⇐⇒

x ∈ X =⇒ y = F (x) /∈ Y c.

(15)
(16)
(17)

Therefore, the input set and the complement of the output set
have to be encoded as linear constraints on their correspond-
ing variables.

Linear.

ˆzi = Qizi−1 where Qi ∈ {−1, 1}ki+1×ki

ˆzi,j = qT

j zi−1

j = 1, . . . , ki+1

(18)

ReLU.
ˆzi ≤ u and β ∈ {0, 1}, we can encode the block as:

zi = ReLU(ˆzi) = max(0, ˆzi) and given that l ≤

zi ≤ ˆzi − l(1 − β)
ˆzi ≤ zi
zi ≤ β · u
0 ≤ zi
β ∈ {0, 1}

(19)
(20)
(21)
(22)
(23)

Sign.

zi = sign(ˆzi)

ˆzi ≥ 0 =⇒ zi = 1
ˆzi < 0 =⇒ zi = 0

but given bounds l ≤ zi ≤ u, this can be formulated as

−1 ≤ zi
zi ≤ 1
l · β ≤ ˆzi

ˆzi ≤ u(1 − β)
zi = 1 − 2 · β
β ∈ {0, 1}

(24)
(25)

(26)
(27)
(28)
(29)
(30)
(31)

Additionally, given the structure of the linear block that pre-
cedes the sign block and the fact that the variables only take
values in {−1, 1}, we can always easily compute l ≥ −ki
and u ≤ ki.
Batch Normalization. The batch normalization blocks
have the output of linear blocks as their input.

zi = αki

yi − µki

(cid:18)

σki (cid:19)

+ γk

(32)

where y = (y1, . . . , ynk+1) and αk, γk, µkσki ∈ R, assuming
σki > 0. which can be rewritten as

σki zi = αkiyi − αki µki + σki γki

i = 1, . . . , nk+1

(33)

which is a linear constraint.

Max. y = max(x1, x2, . . . , xm) and li ≤ xi ≤ ui can be
formulated as a set of linear constraints in the following way:

y ≤ xi + (1 − βi)(umax,−i − li)
y ≥ xi

i = 1, . . . , m (34)
i = 1, . . . , m (35)

m

βi = 1

i=1
X
βi ∈ {0, 1}

(36)

(37)

In practice, BNNs are implemented by repeated composi-
tions of the blocks described above. In the case of BNNs,
the usual order of the linear layer and batch normalization are
exchanged as suggested by [Hubara et al., 2016] but the MIP
formulation is equivalent.

Using the above formulations, we proceded to encode the
veriﬁcation problem by parsing a BNN to construct the con-
straints that represent its forward pass and the corresponding
input and output set constraints. We then call a mixed integer
program solver and search for a feasible solution. If a feasi-
ble solution is found, then it certiﬁes that the property does
not hold and it serves as a counter example. If the search for
a feasible solution terminates, given the completeness of the
procedure, we certify that the property holds.

We implemented a tool that parses networks and produces
their corresponding encoding for the solver. In the follow-
ing section, we present the experiments we used to evaluate
our approach and compare it to traditional DNN veriﬁcation
tools.

4 Experiments
To demonstrate our approach, we performed a series of ex-
periments. We then compared the performance of our ap-
proach to that of other publicly available veriﬁcation algo-
rithms. However, given the fact that our algorithm is specif-
ically designed for BNNs, we decided to compare its per-
formance to the performance of veriﬁcation algorithms for
DNNs. Even though the networks are not identical, we opted
for this comparison to motivate the use of BNNs for tasks
where DNNs are normally favored given that we expect BNN
veriﬁcation to be signiﬁcantly faster.

4.1 MNIST
We trained DNNs and BNNs on the MNIST dataset
[LeCun and Cortes, 2010].
tune hyper-
parameters, and we used a very simple training setup. The
test set accuracy of the DNN was 98.2% and the BNN 95.6%.
We thresholded all the grayscale values to black and white,
which is equivalent to adding a binarization block at the input
of the network.

We did not

We veriﬁed robustness properties using the input and out-
put sets deﬁned in equations 13 and 14 by allowing a maxi-
mum perturbation of ǫ around known input points using the
p = ∞ norm. We set a time limit of 120 seconds for each
property and report our results in Table 1.

ǫ

Time (s)

Accuracy (%)

Mean Max

timeout Veriﬁed Data

BNN 0.1
DNN

BNN 0.3
DNN

0.223
5.47

0.194
7.12

3.21
28.12

4.54
41.33

0.00
0.05%

0.00
1.02%

88.24
94.33

61.78
80.68

95.6
98.22

95.6
98.22

Table 1: MNIST results. The ǫ column indicates the maximum al-
lowed perturbation that deﬁnes X , The Mean column corresponds to
the average time needed for the properties that did not timeout, the
Max column shows the maximum time taken to verify a property
that did not time out, the timeout column shows the proportion of
properties that exceeded the time limit. The veriﬁed accuracy cor-
responds to the proportion of samples that for which the input set
X deﬁned a property that was veriﬁed, whereas the data accuracy
columns shows the accuracy of the network evaluated on the test
set.

4.2 ACAS
We used a networks and property introduced by Katz et al.
[2017]. We trained a BNN version of the ACAS controller
and tested a subset of Property 10 that only requires running
one query. We sliced the state space by setting the time until
loss of vertical separation τ = 5 and the previous advisory as
clear-of-conﬂict. We ﬁxed the speed of ownship and speed
of intruder values to simplify the property. For the BNN,
we used 8-bits to represent each input and added a layer at
the input to decrease the dimensionality to that of the orig-
inal ACAS network. Quantizing the input can signiﬁcantly
alter the behavior of the controller, our goal was simply to as-
sess the speedup of veriﬁcation. We also trained versions of

Loss

time (s)

result

full precision

8 bit

BNN 2174.43
DNN 1203.44

BNN 1634.25

2.37
41.44

5.73

violated
holds

holds

Table 2: ACAS results.

the network with full precision inputs but neither satisﬁed the
property. We report our results in Table 2.

5 Conclusion

Our results indicate that our simple MIP approach for veri-
fying properties of BNNs performs signiﬁcantly faster than
other methods for DNNs. Training BNNs is challenging but
the reduction in veriﬁcation cost should be considered and in-
centivize their use for safety-critical applications that require
veriﬁcation of certain properties.

In our MNIST experiments we were able to verify the ro-
bustness of networks with about a 10× reduction in veriﬁca-
tion time.

Our ACAS experiments show that our approach is able
to verify BNNs that implement controllers in about 20× re-
duced time. BNNs appear to be particularly well suited as
controllers for safety critical systems.

Our proposed approach encodes BNN as mixed integer lin-
ear programs and is able to verify properties of binarized neu-
ral networks and partially binarized neural networks. Our
experiments indicate that this approach is about an order of
magnitude faster than verifying properties of comparable full
precision neural networks.

The ease with which we can verify BNNs should increase
their use for safety critical applications. BNNs are harder
to train, but the difﬁculty might be worth the cost given
how much faster veriﬁcation becomes along with the efﬁ-
cient hardware implementations that they enable. The use of
BNNs has some drawbacks and requires considerations such
as how to handle non-binary inputs. Quantizing the inputs al-
lows us to preserve the binary architecture but decreases the
applicability of BNNs because some applications might have
continuous input domains that would be better modeled with
ﬂoating point numbers. However, it appears that even veri-
ﬁed networks with ﬂoating point parameters are potentially
unsafe [Jia and Rinard, 2020b].

We used a general purpose mixed integer programming
solver. A potential area of future research would be to design
a MIP solver that exploits some of the BNN speciﬁc charac-
teristics of the problems to further decrease veriﬁcation time.
Another potential direction would be to train ternary BNNs
in order to explore the impact of sparsiﬁcation on runtime,
given that he equations that encode ternary BNNs require
fewer variables

References

[Amir et al., 2021] Guy Amir, Haoze Wu, Clark Barrett, and
Guy Katz. An SMT-based approach for verifying bina-

rized neural networks. Tools and Algorithms for the Con-
struction and Analysis of Systems, TACAS, 2021.

[Hubara et al., 2016] Itay Hubara, Matthieu Courbariaux,
Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina-
rized neural networks. In Advances in Neural Information
Processing Systems, 2016.

[Jia and Rinard, 2020a] Kai Jia and Martin Rinard. Efﬁcient
In Ad-

exact veriﬁcation of binarized neural networks.
vances in Neural Information Processing Systems, 2020.
[Jia and Rinard, 2020b] Kai Jia and Martin Rinard. Exploit-
ing veriﬁed neural networks via ﬂoating point numerical
error. CoRR, abs/2003.03021, 2020.

[Katz et al., 2017] Guy Katz, Clark Barrett, David L. Dill,
Kyle Julian, and Mykel J Kochenderfer. Reluplex: An ef-
ﬁcient SMT solver for verifying deep neural networks. In
International Conference on Computer Aided Veriﬁcation,
2017.

[Katz et al., 2019] Guy Katz, Derek A Huang, Duligur Ibel-
ing, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth
Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji´c,
et al. The Marabou framework for veriﬁcation and analy-
sis of deep neural networks. In International Conference
on Computer Aided Veriﬁcation, 2019.

[LeCun and Cortes, 2010] Yann LeCun and Corinna Cortes.

MNIST handwritten digit database. 2010.

[Liu et al., 2021] Changliu Liu, Tomer Arnon, Christopher
Lazarus, Christopher Strong, Clark Barrett, and Mykel J.
Kochenderfer. Algorithms for verifying deep neural net-
works. Foundations and Trends® in Optimization, 4(3-
4):244–404, 2021.

[Lomuscio and Maganti, 2017] Alessio Lomuscio and Lalit
Maganti. An approach to reachability analysis for feed-
forward relu neural networks. CoRR, abs/1706.07351,
2017.

[Narodytska et al., 2018] Nina Narodytska,

Shiva Ka-
siviswanathan, Leonid Ryzhyk, Mooly Sagiv, and Toby
Walsh. Verifying properties of binarized deep neural
In AAAI Conference on Artiﬁcial Intelligence
networks.
(AAAI), 2018.

[Narodytska et al., 2020] Nina Narodytska, Hongce Zhang,
Aarti Gupta, and Toby Walsh. In search for a SAT-friendly
In International
binarized neural network architecture.
Conference on Learning Representations, 2020.

[Papernot et al., 2016] Nicolas Papernot, Patrick McDaniel,
Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Anan-
thram Swami. The limitations of deep learning in adver-
sarial settings. In IEEE European Symposium on Security
and Privacy (EuroS&P), pages 372–387. IEEE, 2016.
[Simons and Lee, 2019] Taylor Simons and Dah-Jye Lee. A
review of binarized neural networks. Electronics, 8(6):4–
5, 2019.

[Tjeng et al., 2019] Vincent Tjeng, Kai Y. Xiao, and Russ
Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference
on Learning Representations, 2019.

