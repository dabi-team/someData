Transformation-Interaction-Rational Representation for
Symbolic Regression

Fabr√≠cio Olivetti de Fran√ßa
Universidade Federal do ABC
Center for Mathematics, Computing and Cognition
Santo Andr√©, SP, Brazil
folivetti@ufabc.edu.br

2
2
0
2

r
p
A
5
2

]
E
N
.
s
c
[

1
v
7
0
8
6
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Symbolic Regression searches for a function form that approxi-
mates a dataset often using Genetic Programming. Since there is
usually no restriction to what form the function can have, Ge-
netic Programming may return a hard to understand model due to
non-linear function chaining or long expressions. A novel repre-
sentation called Interaction-Transformation was recently proposed
to alleviate this problem. In this representation, the function form
is restricted to an affine combination of terms generated as the
application of a single univariate function to the interaction of
selected variables. This representation obtained competing solu-
tions on standard benchmarks. Despite the initial success, a broader
set of benchmarking functions revealed the limitations of the con-
strained representation. In this paper we propose an extension to
this representation, called Transformation-Interaction-Rational rep-
resentation that defines a new function form as the rational of two
Interaction-Transformation functions. Additionally, the target vari-
able can also be transformed with an univariate function. The main
goal is to improve the approximation power while still constraining
the overall complexity of the expression. We tested this represen-
tation with a standard Genetic Programming with crossover and
mutation. The results show a great improvement when compared
to its predecessor and a state-of-the-art performance for a large
benchmark.

CCS CONCEPTS
‚Ä¢ Computing methodologies ‚Üí Genetic programming.

KEYWORDS
symbolic regression, regression, genetic programming

ACM Reference Format:
Fabr√≠cio Olivetti de Fran√ßa. 2022. Transformation-Interaction-Rational Rep-
resentation for Symbolic Regression. In Genetic and Evolutionary Computa-
tion Conference (GECCO ‚Äô22), July 9‚Äì13, 2022, Boston, MA, USA. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3512290.3528695

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9237-2/22/07.
https://doi.org/10.1145/3512290.3528695

1 INTRODUCTION

Regression analysis is an important field that studies the rela-
tionship of measured quantities [11, 15]. This has applications in
a broad set of fields like economics, public health, engineering,
etc [9].

Most frequently, such analysis is performed by means of para-
metric approaches where we fix a certain function form following
some assumptions and adjust its parameters to a given set of data
points [9, 11, 15]. Distinct from the parametric approach, Symbolic
Regression [18, 22, 25] searches for the optimal function form al-
together with the adjusted parameters corresponding to the best
fit of a certain studied phenomena. This has the advantages that
no prior assumptions are required (e.g., linearity, homoscedastic-
ity, etc.). The evolutionary approach called Genetic Programming
(GP) [18, 25] is often employed to solve this particular problem.

In the standard approach, the representation of a solution in GP
is an expression tree data structure that is only constrained by the
set of symbols allowed in the function nodes. The main advantage
is that the there is no requirement of a prior study step, the search
algorithm is in charge of finding the adequate function form. In
practice, though, this not always works as expected since the search
space can be hard to navigate as a small change in the expression
can make up for a very different prediction behavior.

A new representation was recently proposed in [2, 24] that con-
straints the search space to a specific pattern composed of an affine
combination of non-linear transformed features. These features
are a composition of a polynomial, representing the interactions
between the original variables, and the application of an unary func-
tion, called transformation. This representation, called Interaction-
Transformation, rendered competitive results when compared to
modern symbolic regression approaches [20]. Its main advantage
is the guarantee that some complicated function forms are not rep-
resentable, such as those models containing non-linear function
chaining. Also, the model parameters can be determined by using
an ordinary least squares solver, removing this burden from the
evolutionary search.

However, the imposed restrictions may keep the search algo-
rithm from finding the correct solution, if the ground truth is not
representable. In this paper we propose an extension to this repre-
sentation, called Transformation-Interaction-Rational, that allows
to represent a broader class of function forms while still limiting
the search space to avoid overcomplicated function forms.

The remainder of this paper is organized as follows. In Section 2
we explain the Symbolic Regression problem with a brief detail
of Genetic Programming and its common issues. Section 3 gives a

 
 
 
 
 
 
GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Fabr√≠cio Olivetti de Fran√ßa

brief review of the current approaches proposed to alleviate such
problems. In Section 4, we start with a short explanation of the
Interaction-Transformation followed by our proposed representa-
tion, Transformation-Interaction-Rational. Section 5 describes the
implementation details of the evolutionary algorithm used to search
for symbolic expressions. The experimentation method is described
in Section 6 followed by the results and discussions in Section 7.
Finally, in Section 8, we conclude this paper with a summary of the
results and discussing some future steps.

2 GENETIC PROGRAMMING
Genetic Programming is an evolutionary search specialized in evolv-
ing computer programs [18, 22, 25]. Specifically for Symbolic Re-
gression, the computer program is an abstract syntax tree represent-
ing a mathematical expression that describes the observed relation-
ship. This algorithm follows the traditional framework of Genetic
Algorithm that repeats until convergence the cycle of selection,
crossover, mutation, and reproduction.

A common crossover operator is the sub-tree crossover in which
the algorithm chooses random subtrees from distinct parents and
recombines them into new solutions. The mutation operator can be
a choice of inserting, removing or replacing a random subtree, or a
special case of replacing a node with another one of the same arity.
One issue with this procedure is that the tree can grow indefi-
nitely without a proper control mechanism. This can lead to unnec-
essarily long expressions, that are harder to understand or contain
unused subtrees (e.g., part of expression that evaluates to +0) also
known as bloat [7]. This problem can be alleviated by penalizing
larger solutions or stipulating a maximum tree depth and prohibit-
ing children solutions larger than this depth.

Another issue is the creation of nodes representing fixed model
parameters or coefficients. A natural procedure to create this type
of nodes is to draw a random value within a reasonable range. This
creates a sub-problem within Symbolic Regression since, even if
the algorithm finds the correct function form, it still has to find the
correct coefficients. For example, let us suppose that the correct
function form is sin (ùëê ¬∑ ùë•) with ùëê = 0.5. If the algorithm finds the
correct form but with ùëê = 5, it will likely discard this expression
due to a large approximation error.

Many authors proposed different solutions to this problem. Most
noteworthy is the Linear Scaling [16] that, for a given symbolic
model ùëì (ùë•), it fits a simple regression ùëé ¬∑ ùëì (ùë•) + ùëè to adjust the
scale and offset of the model. This simple procedure can help to
maintain correct function forms throughout the generation with the
expectation of eventually finding a better value for their coefficients.
More recently, there has been some studies on applying non-linear
optimization to adjust the scaling and offsets of inner nodes [17],
this particular approach has been proved to be successful in a
thorough benchmark framework [4].

In this same line, some authors propose the combination of
smaller models using an affine combination, this is called Multiple
Regression GP [3] and is best represented by FEAT [19, 21] and
ITEA [1, 5, 6], briefly explained in the next section.

3 RELATED RESEARCH
As mentioned in the previous section, the recent literature focuses
on creating mechanisms that help to improve the exploration power
of the genetic programming algorithm while reducing the influence
or the need to search for model parameters.

Feature Engineering Automation Tool (FEAT) [19, 21] rep-
resents the regression model as the affine combination of smaller
expression trees (sub-trees). The main idea is that the search prob-
lem is decomposed into smaller segments where the affine coeffi-
cients act as a sub-tree selection and scaling. The algorithm fits the
coefficients using a gradient descent algorithm to find the nearest
local optima. In this algorithm, the initial population is formed of a
trivial linear model and a set of small solutions created at random.
These random solutions are created as an affine combination of
expression trees with a limited depth. The main operators for this
algorithm is the sub-tree crossover, that swaps sub-trees of two
or more expressions; dimension crossover, that swaps two vari-
able nodes between parents, and the mutations: node replacement,
replacement of a sub-tree by a terminal node, sub-tree removal,
insertion of a variable node, removal of a variable node. Besides
the functions and operators commonly used in the literature, the
authors also included functions often used as activation functions
of Neural Network as well as boolean functions.

Interaction-Transformation Evolutionary Algorithm (IT‚Äì
EA) [1, 5, 6] also describes an affine combination of smaller expres-
sions (denoted as terms) but, differently from FEAT, it constrains
the form of these expressions as the application of a single unary
function, called transformation, to the product of powers of the
original features, called interaction (more details in Sec. 4). The
algorithm is a mutation-based evolutionary search that applies a
mutation to every solution in the population and selects the next
generation using tournament selection. The mutation operators
can either add or remove a new term, replace the transformation
function, replace one of the exponents, combine the interactions
between two terms. The results reported for a commonly used set of
regression datasets [5] showed a competitive performance of ITEA
when compared to FEAT and other SR algorithms. More recently,
a more thoroughly benchmark [20] showed that in some situa-
tions this representation may limit the goodness-of-fit obtained
by this algorithm. In this paper we propose an extension to this
representation that alleviates this problem.

Genetic programming with nonlinear least squares (GP
NLS) [17] is a recent modification of the original GP that when
evaluating a solution it adds a scaling parameter to every node rep-
resenting a variable and envelopes the expression tree with a linear
scaling. The parameters are then determined by a nonlinear least
squares method using the Jacobian of the expression. This is possible
since the set of non-terminal have a well determined derivative that
allows the use of automatic differentiation for the whole expression.
The authors reported an outstanding result in a set of benchmark
datasets when compared to other common approaches [17]. Its
current optimized implementation, called Operon [4] was used in a
large benchmark of Symbolic Regression approaches and shown
to outperform other SR and nonlinear regression approaches, on
average, in both goodness-of-fit and runtime [20].

Transformation-Interaction-Rational Representation for Symbolic Regression

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Table 1: Examples of mathematical functions and their corresponding representation as IT and TIR expressions, if any.

Equation

ùë•0 (ùë•1 + ùë•2ùë•3ùë†ùëñùëõ(ùë•4))

‚àöÔ∏Ç

ùë•0

1‚àí

ùë• 2
1
ùë• 2
2

+

1
1+ùë• ‚àí4
0
(cid:16) ùë• 2
0
ùë•1

sin

(cid:17)

1
1+ùë• ‚àí4
1

+ 1
2

ùëí

ùë•1
ùë•0

IT

‚Äì

‚Äì

‚Äì

TIR

‚Äì
‚àöÔ∏Ç ùë• 2
0
ùë• ‚àí2
1‚àíùë• 2
2
1

sin (ùë• 2
0

1 ) + 0.5ùëíùë•ùëù (ùë•0ùë• ‚àí1
ùë• ‚àí1
1 )

sin (ùë• 2
0

1 +2ùë• 4
0 +ùë• 4
ùë• 4
1 +ùë• 4
ùë• 4
1+ùë• 4
0

0

ùë• 4
1
0 +ùë• 4
1 ) + 0.5ùëíùë•ùëù (ùë•0ùë• ‚àí1
ùë• ‚àí1
1 )

1

4 TRANSFORMATION-INTERACTION-

RATIONAL

The Interaction-Transformation representation (IT) [24] constrains
the function form of the mathematical model to those forms that can
be represented as an affine combination of non-linear transforma-
tions applied to polynomial functions (interactions). The main idea
comes from the observation that many engineering and physics
equations can be described in this form.

Considering a tabular data set where each sample point has ùëë
variables x = (ùë•1, ùë•2, ¬∑ ¬∑ ¬∑ , ùë•ùëë ). The regression model following the
Interaction-Transformation (IT) representation has the form:

ùëìùêºùëá (x, w) = ùë§0 +

ùëö
‚àëÔ∏Å

ùë§ ùëó ¬∑ (ùëìùëó ‚ó¶ ùëü ùëó )(x),

(1)

ùëó=1
representing a model with ùëö terms where w ‚àà Rùëö+1 are the coeffi-
cients of the affine combination, ùëìùëó : R ‚Üí R is the ùëó-th transforma-
tion function and ùëü ùëó : Rùëë ‚Üí R is the interaction function:

polynomial regression model [23, 26, 27] and apply an invertible
function to the resulting value:

ùëìùëá ùêºùëÖ (x, wp, wq) = ùëî

(cid:18) ùëù (x, wp)
1 + ùëû(x, wq)

(cid:19)

,

where ùëî : R ‚Üí R is an invertible function, ùëù, ùëû : Rùëë ‚Üí R are IT
expressions exactly as defined in Eq. 1 with ùëöùëù > 0 and ùëöùëû ‚â• 0
terms. With this function form we can still use an ordinary least
squares method to adjust the coefficients wp, wq to the training
data. Assuming ytr = ùëìùëá ùêºùëÖ (xtr, wp, wq) a vector containing the
values of the application of the TIR model to the training data ùë•ùë°ùëü ,
we can derive the following expression:

ùë¶ùë°ùëü = ùëî

(cid:19)

(cid:18) ùëù (x, wp)
1 + ùëû(x, wq)
ùëù (x, wp)
1 + ùëû(x, wq)

ùëî‚àí1 (ùë¶ùë°ùëü ) =

ùëü ùëó (x) =

ùëë
(cid:214)

ùëñ=1

ùë•

ùëòùëñ ùëó
ùëñ

,

(2)

ùëî‚àí1 (ùë¶ùë°ùëü ) = ùëù (x, wp) ‚àí ùëî‚àí1 (ùë¶ùë°ùëü ) ¬∑ ùëû(x, wq).

where ùëòùëñ ùëó ‚àà Z represents the exponents for each variable. Whenever
we fix the values of ùëìùëó and ùëü ùëó , the expressions becomes a linear
model with w as the coefficients, as such we can find their optimal
values with an ordinary least squares method. This is convenient
for the main search algorithm that only needs to find the optimal
value of ùëö, the values for the exponents and the functions of each
term. Computationally, we can represent such expression as a list
of tuples in the form (kj, ùë§ ùëó , ùëìùëó ) with the the 0-th term represented
as (0, ùë§0, ùëñùëë).

To illustrate the possibilities and limitations of this representa-
tion, in Table 1 we show four simple equations in which only one
of them can be represented as an IT expression. The second column
shows the same equation rewritten with a similar notation to an
IT expression. Notice that, even though not every equation can
be represented, this representation is still capable of finding good
approximations [2, 5, 6, 14].

We now propose the Transformation-Interaction-Rational repre-
sentation (TIR) 1 as an extension of the IT expressions. The main
idea is that we combine two IT expressions similarly to a rational

1Fun fact: Tir was the god of wisdom in ancient Armenia.

We can adjust the coefficients using the same fitting algorithm
(OLS) using a transformed training target by applying the inverse
of the transformation function ùëî. For that to work, it is required that
the function ùëî is invertible and that ùë¶ùë°ùëü is within the domain of the
inverted function when ùëî‚àí1 is partial. Additionally, it is important
that ùëù contains at least one term to avoid the trivial solution where
ùëû(x, wq) = ‚àí1.

To avoid the effect of the collinearity induced by the term ùëî‚àí1 (ùë¶ùë°ùëü )¬∑
ùëû(x, wq), some authors suggest to apply a regularization factor [26,
27] or to fit using only a smaller subset of the data points [8]. In
some initial experiments, we did not observed any benefit in any
of these approaches, thus this will be left as a future investigation.
Finally, the adjusted model should be evaluated with a validation
set containing at least some distinct samples from the training data
to avoid keeping overfitted models in the population.

In Table 1 we illustrate the equivalent form for three out of
the four expressions used as an example. We can see that this
new representation expands the search space represented by IT
allowing us to represent more function forms while still keeping
some simplicity. There are, of course, still some expressions that
cannot be exactly represented by TIR but, as it was the case with
IT, they still can be approximated.

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Fabr√≠cio Olivetti de Fran√ßa

5 TIR FRAMEWORK
To evaluate the potential of this representation we have imple-
mented an evolutionary algorithm that searches for the best fitting
TIR expression. This framework is written in Haskell and it is avail-
able at https://github.com/folivetti/tir, we also provide a scikit-learn
compatible Python wrapper.

The framework uses three helper libraries for different steps of

the evolutionary process: modal-interval, srtree, evolution.

The modal-interval library 2 is an implementation of the Modal
Interval Arithmetic [10], an extension of the Interval Arithmetic [12],
capable of giving an estimate of the image of a function given the
input domain ranges such as the true image is inside the estimated
range. In TIR framework this is used to evaluate what functions ùëî‚àí1
can be evaluated within the range of the training target and to re-
move any invalid terms of the IT expressions ùëù, ùëû. Notice that since
any term of the IT expression contains at most a single occurrence
of each variable, interval arithmetic returns an exact interval of the
image of the function. To estimate the domains, we calculate the
minimum and maximum values for each variable in the training
data. The framework also allows the user to manually insert the
corresponding domains if they are known a priori.

The srtree library 3 is responsible for managing and evaluating
the expression trees. It supports optimized evaluation of vectorized
data and the calculation of the derivative of any order for a given
expression. This library is used to evaluate the validation data using
the fitted expression, to generate a report of the properties of the
generated model (i.e., monotonicity, etc.), convert the expressions
into different formats (e.g., Python Numpy compatibility), and to
apply a non-linear fitting algorithm (implemented but not used in
this work).

Finally, the evolution library 4 is a generic framework developed
to help with the implementation of evolutionary algorithms. This
library provides support for a high-level domain specific language
and it automatically handles the search main loop. In short, one just
needs to provide the implementation of the specific search operators
(i.e., initialization, crossover and mutation) and a description of the
desired search process so that the library handle the search.

In this paper we are using the generational replacement with
the application of a one-point crossover with probability ùëùùëê and
using two parents selected by tournaments between two individuals,
followed by a multi-mutation with probability ùëùùëö. Having defined
the main loop of the evolutionary search, we need to determine the
three specific operators: initialization, crossover, and mutation.

5.1 Initial Population
Each solution is represented as a triple (ùëî, ùëù, ùëû) where ùëî is the in-
vertible function, and ùëù, ùëû are IT-expressions represented as a list of
triples [(ùë§ùëñ, ùëìùëñ, [ùëòùëñ ùëó ])]. For any purpose, we can translate this rep-
resentation into an expression tree and vice-versa. The initial pop-
ulation is composed of random solutions limited by a user defined
budget for the sum of the number of terms of the IT expressions ùëù
and ùëû. The procedure starts by first choosing a random invertible
function ùëî from a set ùê∫ of invertible functions, then generating

2https://github.com/folivetti/modal-interval
3https://github.com/folivetti/srtree
4https://github.com/folivetti/evolution

a random non-null IT expression ùëù and, finally, an IT expression
ùëû. To generate an IT expression we keep creating random terms
(see Eq.2) until either this procedure returns an empty term or we
do not have enough budget. Each term is generated by repeatedly
drawing a random variable without replacement together with a
corresponding random exponent. A dummy variable is inserted into
the set to signal when to stop drawing new variables, so initially
the process has a probability of 1/(ùëë + 1) of choosing one of the
variables or returning a null term. If it draws a variable in the first
turn, the second turn will have a probability of 1/ùëë and so on.

5.2 Crossover
The crossover procedure will choose two parents through tourna-
ment to take part of the process. In the first step, the procedure
draws a random point of the first parent to make the recombina-
tion. If this point is located at the transformation function ùëî, it will
generate a child with ùëî, ùëù taken from the first parent and ùëû from
the second. If the point is located at the IT expression ùëù, it will
create a child with ùëî, ùëû from the first parent and a new ùëù as the
recombination of both parents. Likewise, if the point is at ùëû the
child will have ùëî, ùëù from the first parent and a recombined ùëû.

5.3 Mutation
The mutation procedure is an uniformly random choice between the
set of mutations insertNode, removeNode, changeVar, changeExpo-
nent, changeFunction. The insertNode mutation will either randomly
insert a new variable in one of the terms of either ùëù or ùëû, or insert a
new term in one of those expressions. In the same way, ùëüùëíùëöùëúùë£ùëíùëÅùëúùëëùëí
will either remove a variable or a term. The operators changeVar,
changeExponent, changeFunction will change one of such elements
chosen at random. Notice that in the special case where the number
of nodes in the expression exceeds the budget, it will remove the
operator insertNode from the set.

6 EXPERIMENTS
To evaluate the proposed algorithm, we have used the symbolic
5. This framework con-
regression benchmark framework srbench
tains 122 regression datasets with no knowledge of the generating
function. This benchmark executes each algorithm 10 times with a
fixed set of random seeds for every dataset. In each run, the frame-
work first splits the dataset with a ratio of 0.75/0.25 for training
and testing, respectivelly, subsampling the training data in case
it contains more than 10, 000 samples. After that, it performs a
halving grid search [13] with 5-fold cross-validation to choose the
optimal hyperparameters among 6 user defined choices. Finally,
given the chosen hyperparameters, it executes the algorithm with
the whole training data to determine the optimal regression model.
It evaluates this model in the test set and stores the approximation
error information, running time, number of nodes, and the symbolic
model.

SRBench calculates and store the mean squared error (mse),
mean absolute error (mae), and coefficent of determination (ùëÖ2)
measurements. The results of each dataset are summarized using
the median to minimize the impact of outliers and they are reported
with error plots sorted by the median of this summarization. In this

5https://cavalab.org/srbench/

Transformation-Interaction-Rational Representation for Symbolic Regression

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Table 2: Hyperparameters for the TIR algorithm used during
the regression benchmark experiments.

intersection between the fitting and validation data because some
of the data sets contains very few samples, and this was causing
overfitted models in some pre-experiments.

Parameter

Pop. size
Gens.
Cross. prob.
Mut. prob
Transf. functions
Invertible functions
Error measure

budget
ùëòùëñ ùëó range (Eq. 2)

value

1000
500
30%
70%
[ùëñùëë, ùë°ùëéùëõ‚Ñé, ùë†ùëñùëõ, ùëêùëúùë†, ùëôùëúùëî, ùëíùë•ùëù, ùë†ùëûùëüùë°]
[ùëñùëë, ùëéùë°ùëéùëõ, ùë°ùëéùëõ, ùë°ùëéùëõ‚Ñé, ùëôùëúùëî, ùëíùë•ùëù, ùë†ùëûùëüùë°]
ùëÖ2
ùëöùëéùë• (5, ùëöùëñùëõ(15,

(cid:106) ùëõ_ùë†ùëéùëöùëùùëôùëíùë†
10

(cid:107)

)

{(‚àí5, 5), (0, 5), (‚àí2, 2), (0, 2), (‚àí1, 1), (0, 1)}

work we will focus on the ùëÖ2 metric following the same analysis
in [20].

Currently the srbench repository contains results for 21 regres-
sion algorithms, 14 of which are Symbolic Regression algorithms.
From these SR algorithms, 10 use an evolutionary algorithm as a
search procedure. In the next section, we will first report an overall
result of the performance of TIR compared to these approaches fol-
lowed by a more detailed analysis of different aspects of the results.
It is important to notice that 62 of the 122 datasets are variations
of the Friedman benchmark with different numbers of variables,
samples, and noise levels. As such, we will also report a separate
analysis for the Friedman and non-Friedman sets.

As all the results reported in [20] are publicly available at their
repository, we only executed the experiments for TIR with the same
random seeds so that the results remain comparable. We executed
these experiments using Intel DevCloud instance with an Intel(R)
Xeon(R) Gold 6128 CPU @ 3.40GHz using only a single thread.
The original experiment was executed on an Intel(R) Xeon(R) CPU
E5-2690 v4 @ 2.60GHz, so this may create a bias when comparing
the execution time.

6.1 Hyperparameters
As the rules for this benchmark framework allows only 6 hyperpa-
rameters combinations for the grid search procedure, some of the
TIR hyperparameters were fixed after some smaller experiments
performed on a subset of the datasets used in [5]. Table 2 shows
the choice of hyperparameters used during the srbench experiment.
The budget formula is based on a common rule-of-thumb [11]
that we should have a number of samples of at least 10 times the
number of variables for linear regression. We clip this value in
the range [5, 15] so that we do not limit the size of the model
to a very small number of terms and neither creates overly long
expressions. Regarding the choice of functions set for ùëî, note that
before the search main loop, the algorithm eliminates from this set
any function whose inverse cannot be applied to the training target
value. Also notice that we did not use a pre-scaling of the training
data and target values unlike some of the algorithms in srbench.
Finally, during fitness evaluation, the first 90% training samples
are used for the ordinary least squares procedure and the last 90%
for calculating the fitness of the final model. We chose to have this

7 RESULTS
In this section we will summarize the obtained results using error
bars and critical difference diagrams. For the error bars, we estimate
the error using a thousand bootstrap iterations with a confidence
interval of 0.95. The point in this plot represents the median of
medians of the ùëÖ2 calculated over the test sets and the bar represents
the range of values that contains the true median with 95% of
confidence. The critical difference diagrams uses the Nemenyi test
with ùõº = 0.05 as a post-hoc test to find the groups of algorithms that
presents a significant difference to each other. This test is calculated
using the average rank of each algorithm on each dataset using ùëÖ2
of the test set as the ranking criteria.

7.1 Initial results
In Fig. 1 we can see the overall rank using the median of the medians
of the ùëÖ2 calculated over the test sets of the entire benchmark. In this
plot, TIR is ranked 6th, a five position increase when compared to
its predecessor, ITEA. As it turns out, this algorithm now belongs
to the set of algorithms with a median in the right end of the
ùëÖ2 spectrum. The error bar indicates that its median is not far
from what is reported, ranging from 0.82 to 0.85. Comparing with
the better ranked approaches, TIR median value is 0.1 less than
Operon, which is ranked first. Regarding the black-box approaches,
TIR presents a very close result to XGB and LGBM (5th and 7th,
respectivelly) and better results than other black-box models.

Because of the ùëèùë¢ùëëùëîùëíùë° parameter, TIR model sizes are comparable
to the top GP approaches, maintaining a median close to 100 similar
to Operon, FEAT and EPLEX, and significantly smaller than SBP-GP
and the black-box approaches. Regarding the training time, TIR was
a bit faster than ITEA, EPLEX and FEAT, but those were executed
in a different machine, thus these values may vary. Even though it
used a slower processor, Operon runtime was still faster than TIR.
Operon is a heavly optimized GP framework built with computing
performance in mind. Besides, TIR also incorporates the ordinary
least squares step when evaluating the fitness which dominates the
computational costs of this implementation.

As previously mentioned, more than half of the benchmarks
are variations of the Friedman benchmark, used to test the innate
feature selection capabilities of GP algorithms. As such, the results
can be biased toward the algorithm that is more competent in this
class of benchmark. In Figs. 2 and 3, we show rank of the median
of medians of the ùëÖ2 when considering only the Friedman and non-
Friedman datasets, respectively. We can see that, when considering
only the Friedman datasets, TIR is ranked 5th, evidencing that the
evolutionary algorithms are more capable of handling the feature
selection task in parallel with the model search. When looking at
the non-Friedman results, TIR is ranked 9th. This is due to the
increase in rank observed for the black-box approaches. In this
specific rank, Kernel Ridge and XGB is ranked first and second,
respectivelly. The only GP approaches in the top-5 are GP-GOMEA,
FEAT and SBP-GP.

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Fabr√≠cio Olivetti de Fran√ßa

Figure 1: Median of the median results of the srbench. Notice that the TIR algorithm was executed on a different machine, so
the comparison of the training time may be imprecise.

Figure 2: Top 15 algorithms when considering only the Fried-
man datasets.

Figure 3: Top 15 algorithms when considering only the non-
Friedman datasets.

7.2 Penalized fitness for small datasets
When inspecting the obtained results 6, we noticed that TIR per-
formed worse on smaller datasets due to overfitting when applying
ordinary least squares with just a few data points. In this situation,
TIR was capable of finding a perfect score for every hyperparame-
ter combination during the grid search and a perfect score on the
training data with the optimal parameters, even though it obtained

6https://github.com/folivetti/tir/tree/main/srbench/results

a much worse score in the test set (often a negative ùëÖ2 value). To al-
leviate this problem, we introduce a penalization term in the fitness
function:

fit‚Ä≤(ùë•) = fit(ùë•) ‚àí ùëê ¬∑ size(ùë•),
so that the original fitness value is reduced proportional to the size
of the expression. As already mentioned, this penalization could
not be a part of the grid search since when the issue appears the
training ùëÖ2 score is exactly 1.0. So, the penalized fitness must be

‚àí0.250.000.250.500.751.00*Operon*SBP-GP*FEAT*EPLEXXGB*TIRLGBM*GP-GOMEAAdaBoostRandomForest*ITEA*AFP_FE*AFP*FFXKernelRidge*DSR*MRGP*gplearnMLPLinear*BSR*AIFeynmanR2 Test102104Model Size100102104Training Time (s)0.60.70.80.9*Operon*SBP-GP*FEAT*EPLEX*TIRLGBMXGB*GP-GOMEA*FFXAdaBoostRandomForest*ITEA*AFP_FE*AFP*MRGPfriedman R2 Test0.60.70.8KernelRidgeXGB*GP-GOMEA*FEAT*SBP-GP*EPLEXAdaBoostRandomForest*TIR*OperonMLPLinear*ITEA*AFP_FE*AFPnon-friedman R2 TestTransformation-Interaction-Rational Representation for Symbolic Regression

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Figure 4: Top 15 median of medians when considering the
three penalization strategies with all of the datasets.

Figure 5: Top 15 median of medians when considering the
three penalization strategies for the Friedman datasets.

applied according to some rules based on the dataset size. As such,
we propose three ad hoc rules to verify how much it improves the
results, the rules will apply the penalization whenever we have
i) a number of samples smaller than 100 (TIR-samples); ii) the
dimension smaller than 6 (TIR-dim); iii) the product of the number
of samples with the dimension smaller than 1, 000 (TIR-points).

For this next set of experiments, we used a penalization constant
of ùëê = 0.01. We illustrate the benefits of a penalty function in one
of these smaller data sets in Table 3. In this table we can see the
solutions generated by Operon and TIR, both overly long and with
a small ùëÖ2 while a much better solution is obtained with a simple
and small expression. We argue that such penalization may benefit
other SR algorithms as well.

Figs. 4, 5, and 6 show the algorithms ranked by the median of
medians of the ùëÖ2 metric for the overall results, Friedman datasets,
and non-Friedman datasets, respectively. Since we are using the
median of medians to calculate the rank, the overall rank is barely
affected since all of the penalization criteria affect less than 20
datasets. In this first plot we can see that this criteria does not
affect the overall performance negatively. When looking only at the
Friedman datasets (Fig. 5), we can see that all of the penalization
strategies increase the rank in one position, making TIR the 4th in
rank.

These strategies seem to work particularly well when applying
them to the non-Friedman datasets. As we can see in Fig. 6, all
of the strategies presented an increase in rank but, particularly
the strategy TIR-points, is now ranked 2nd, in between two black-
box approaches, Kernel Ridge and XGB. The confidence interval
indicates that the top-3 algorithms present similar results.

7.3 Average rank and critical difference
Finally, in Figs 7, 8, and 9 we show the critical difference diagram
using the Nemenyi test with ùõº = 0.05 calculated over the average
rank. Different from the previous plots, this diagram ranks the

Figure 6: Top 15 median of medians when considering the
three penalization strategies for the non-Friedman datasets.

algorithms by averaging the rank for each individual problem, thus
it depicts a different view from the previous plots. Considering the
entire benchmark we can see that, when looking at the average
rank, TIR-points is ranked third with no significant difference with
the second in rank and the next five in rank (three of which are
TIR variations). This particular result not only indicates that the
median of medians rank only show a partial view of the results, as
the rank from the third place forward is changed, but it also shows
that there is still insufficient data to draw a conclusive observation
of the difference between the third, fourth and fifth approaches
(excluding TIR variations).

0.60.70.80.91.0*Operon*SBP-GP*FEAT*EPLEXXGB*TIR-dim*TIR-samples*TIR-points*TIRLGBM*GP-GOMEAAdaBoostRandomForest*ITEA*AFP_FER2 Test0.70.80.9*Operon*SBP-GP*FEAT*TIR-points*TIR-dim*TIR-samples*EPLEX*TIRLGBMXGB*GP-GOMEA*FFXAdaBoostRandomForest*ITEAfriedman R2 Test0.60.70.8KernelRidge*TIR-pointsXGB*GP-GOMEA*TIR-dim*TIR-samples*FEAT*SBP-GP*EPLEXAdaBoostRandomForest*TIR*OperonMLPLinearnon-friedman R2 TestGECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

Fabr√≠cio Olivetti de Fran√ßa

Table 3: Expressions generated by Operon, TIR and penalized TIR for the 192 vineyard dataset with the random seed 11284.
This is an example chosen to illustrate the benefits, not to be taken as a representative of the average simplicity of either
methods.

Algorithm

Expression

Operon

TIR

TIR + penalty

‚àö

‚àí28643.10‚àí30479.64¬∑ùë•0
(1+(4205.37¬∑ùë•1 ) 2
‚àö

‚àö

+ 21305.05¬∑ùë•0‚àí36.03¬∑ùë•0 ¬∑227.11¬∑ùë•1
1+(‚àí56.40‚àí170.41¬∑ùë•0‚àí3.58¬∑ùë•1 ) 2

+259.47+61.52¬∑ùë•0

1+(‚àí33.91¬∑ùë•1‚àí339.93‚àí161.49¬∑ùë•0) 2

‚àí2.8+1.17¬∑ùëôùëúùëî (ùë• 5

1 )‚àí2.42¬∑ùë†ùëñùëõ (ùë• 2

ùë°ùëéùëõ(1.52 ‚àí 0.20 ¬∑ ùë°ùëéùëõ‚Ñé(

1 ¬∑ùë• 5
1.0‚àí0.09¬∑ùë†ùëñùëõ (ùë• 3
))

1
ùë•1 ¬∑ùë•0

0 )‚àí1.10¬∑ùë†ùëñùëõ (ùë• 5
1 ¬∑ùë• 4

1 ¬∑ùë• 5
0 )‚àí0.05¬∑ùë†ùëñùëõ (ùë• 2

1 ¬∑ùë•0)

0 )+9.20¬∑ùë°ùëéùëõ‚Ñé (ùë•0)‚àí1.10¬∑ùëõùëù.ùë†ùëñùëõ (ùë•0)

ùëÖ2

0.3466

0.2621
0.6233

Figure 7: Critical difference diagram of the top 15 algorithms
for the overall results. This plot is computed using the Ne-
menyi test with ùõº = 0.05 calculated over the average rank.

Figure 9: Critical difference diagram of the top 15 algorithms
for the non-Friedman datasets results. This plot is computed
using the Nemenyi test with ùõº = 0.05 calculated over the
average rank.

extension of Interaction-Transformation (IT) representation. This
representation extends the IT representation as a rational of two
expressions composed with a unary function. With this new rep-
resentation, many more function forms can be represented while
prohibiting some complicated forms such as those with function
chaining (except for the target transformation). Together with the
new representation, we presented a framework for the evolutionary
search of TIR expressions composed of supporting libraries aimed
at improving the search process. Another benefit of this represen-
tation, when coupled with modal arithmetic, is the guarantee of
generating only valid expressions without the need of protected
operators.

We tested this new representation following a simple evolu-
tionary search procedure on a thorough benchmark specifically
crafted to evaluate symbolic regression approaches. Overall the re-
sults showed a significant improvement over its predecessor while
maintaining a good rank position in different scenarios.

Particularly when testing penalization strategies for small data‚Äì
sets, we observed a performance rivaling black-box approaches
(top-3) for a certain class of datasets. For the Friedman datasets, the
proposed approach still mantained a good position in rank, having
comparable results against other Symbolic Regression approaches.
For the next steps, we will investigate other strategies for dealing
with overfitting and the use of adaptive hyperparameters to avoid
the burden of having to apply a grid search strategy. Additionally,
we will consider the inclusion of inner adjustable parameters in this
representation similar to what is done in GP NLS/Operon. While

Figure 8: Critical difference diagram of the top 15 algorithms
for the Friedman datasets results. This plot is computed us-
ing the Nemenyi test with ùõº = 0.05 calculated over the aver-
age rank.

Looking at the Friedman datasets, TIR-dim and TIR-points are
ranked fourth and fifth, respectively. They both have no signifi-
cant difference with the third place, FEAT, but there is a signifi-
cant difference to the first and second place. When looking at the
non-Friedman datasets, TIR-points is ranked third without any sig-
nificant difference with the first and second place, both black-box
models. Again, the horizontal bar indicates that further experiments
are necessary, with more datasets, to draw conclusive observations.

8 CONCLUSION
In this paper we propose a novel representation for Symbolic Re-
gression, called Transformation-Interaction-Rational (TIR) as an

123456789101112131415*Operon*SBP-GP*TIR-pointsXGB*FEAT*TIR-dim*TIR-samples*TIR*EPLEX*GP-GOMEALGBMKernelRidgeRandomForest*ITEAAdaBoostCD123456789101112131415*Operon*SBP-GP*FEAT*TIR-dim*TIR-points*TIR*EPLEX*TIR-samplesXGB*GP-GOMEALGBM*FFXAdaBoostRandomForest*ITEACD123456789101112131415XGBKernelRidge*TIR-points*Operon*TIR-samples*SBP-GPMLP*TIR-dim*TIR*GP-GOMEARandomForest*ITEA*FEATLGBMAdaBoostCDTransformation-Interaction-Rational Representation for Symbolic Regression

GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA

this may add an extra cost to the search process, it can help to
decrease the approximation error in some situations.

REFERENCES
[1] Guilherme Seidyo Imai Aldeia and Fabricio Olivetti de Franca. 2020. A Paramet-
ric Study of Interaction-Transformation Evolutionary Algorithm for Symbolic
Regression. In 2020 IEEE Congress on Evolutionary Computation (CEC). IEEE, New
York, 8 pages. https://doi.org/10.1109/cec48606.2020.9185521

[2] Guilherme Seidyo Imai Aldeia and Fabr√≠cio Olivetti de Fran√ßa. 2018. Lightweight
Symbolic Regression with the Interaction - Transformation Representation. In
2018 IEEE Congress on Evolutionary Computation (CEC). IEEE, New York, 8 pages.
https://doi.org/10.1109/cec.2018.8477951

[3] Ignacio Arnaldo, Krzysztof Krawiec, and Una-May O‚ÄôReilly. 2014. Multiple
regression genetic programming. In Proceedings of the 2014 Annual Conference on
Genetic and Evolutionary Computation. ACM, 879‚Äì886.

[4] Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. 2020. Operon C++
an efficient genetic programming framework for symbolic regression. In Proceed-
ings of the 2020 Genetic and Evolutionary Computation Conference Companion.
1562‚Äì1570.

[5] F. O. de Franca and G. S. I. Aldeia. 2020. Interaction-Transformation Evolutionary
Algorithm for Symbolic Regression. Evolutionary Computation (12 2020), 1‚Äì25.
https://doi.org/10.1162/evco_a_00285 arXiv:https://direct.mit.edu/evco/article-
pdf/doi/10.1162/evco_a_00285/1888497/evco_a_00285.pdf

[6] Fabricio Olivetti de Franca and Maira Zabuscha de Lima. 2021.

Interaction-
transformation symbolic regression with extreme learning machine. Neurocom-
puting 423 (2021), 609‚Äì619.

[7] Grant Dick. 2014. Bloat and generalisation in symbolic regression. In Asia-Pacific

Conference on Simulated Evolution and Learning. Springer, 491‚Äì502.

[8] Norbert Gaffke and Berthold Heiligers. 1996. 30 Approximate designs for polyno-
mial regression: Invariance, admissibility, and optimality. Handbook of Statistics
13 (1996), 1149‚Äì1199.

[9] Andrew Gelman, Jennifer Hill, and Aki Vehtari. 2020. Regression and other stories.

Cambridge University Press.

[10] Alexandre Goldsztejn. 2008. Modal intervals revisited part 1: A generalized

interval natural extension. (2008).

[11] Frank E Harrell. 2017. Regression modeling strategies. Bios 330, 2018 (2017), 14.
[12] Timothy Hickey, Qun Ju, and Maarten H Van Emden. 2001. Interval arithmetic:
From principles to implementation. Journal of the ACM (JACM) 48, 5 (2001),
1038‚Äì1068.

[13] Kevin Jamieson and Ameet Talwalkar. 2016. Non-stochastic best arm identifi-
cation and hyperparameter optimization. In Artificial Intelligence and Statistics.

PMLR, 240‚Äì248.

[14] Daniel Kantor, Fernando J Von Zuben, and Fabricio Olivetti de Franca. 2021.
Simulated annealing for symbolic regression. In Proceedings of the Genetic and
Evolutionary Computation Conference. 592‚Äì599.

[15] Robert E Kass. 1990. Nonlinear regression analysis and its applications. J. Amer.

[16] Maarten Keijzer. 2004. Scaled symbolic regression. Genetic Programming and

Statist. Assoc. 85, 410 (1990), 594‚Äì596.

Evolvable Machines 5, 3 (2004), 259‚Äì269.

[17] Michael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Affen-
zeller. 2020. Parameter identification for symbolic regression using nonlinear
least squares. Genetic Programming and Evolvable Machines 21, 3 (2020), 471‚Äì501.
[18] John R Koza et al. 1994. Genetic programming II. Vol. 17. MIT press Cambridge,

MA.

[19] William La Cava and Jason H. Moore. 2019. Semantic Variation Operators
for Multidimensional Genetic Programming. In Proceedings of the Genetic and
Evolutionary Computation Conference (Prague, Czech Republic) (GECCO ‚Äô19).
ACM, New York, NY, USA, 1056‚Äì1064. https://doi.org/10.1145/3321707.3321776
[20] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabricio Olivetti de
Fran√ßa, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H. Moore. 2021.
Contemporary Symbolic Regression Methods and their Relative Performance. In
Proceedings of the Neural Information Processing Systems Track on Datasets and
Benchmarks. https://openreview.net/pdf?id=xVQMrDLyGst

[21] William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason Moore.
2019. Learning concise representations for regression by evolving networks of
trees. In International Conference on Learning Representations. https://openreview.
net/forum?id=Hke-JhA9Y7

[22] William B Langdon. 1999. Size fair and homologous tree genetic programming
crossovers. In Proceedings of the 1st Annual Conference on Genetic and Evolutionary
Computation-Volume 2. Morgan Kaufmann Publishers Inc., 1092‚Äì1097.

[23] SH Alizadeh Moghaddam, M Mokhtarzade, A Alizadeh Naeini, and SA Alizadeh
Moghaddama. 2017. Statistical method to overcome overfitting issue in rational
function models. International Archives of the Photogrammetry, Remote Sensing
and Spatial Information Sciences 42, 4/W4 (2017).

[24] Fabr√≠cio Olivetti de Fran√ßa. 2018. A greedy search tree heuristic for symbolic
regression. Information Sciences 442-443 (2018), 18 ‚Äì 32. https://doi.org/10.1016/
j.ins.2018.02.040

[25] Riccardo Poli, William B Langdon, Nicholas F McPhee, and John R Koza. 2008. A

field guide to genetic programming. Lulu. com.

[26] Veli-Matti Taavitsainen. 2010. Ridge and PLS based rational function regression.

Journal of chemometrics 24, 11-12 (2010), 665‚Äì673.

[27] Veli-Matti Taavitsainen. 2013. Rational function ridge regression in kinetic
modeling: A case study. Chemometrics and Intelligent Laboratory Systems 120
(2013), 136‚Äì141.

