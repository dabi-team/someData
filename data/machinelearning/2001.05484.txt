1
2
0
2

b
e
F
8
2

]
L
M

.
t
a
t
s
[

2
v
4
8
4
5
0
.
1
0
0
2
:
v
i
X
r
a

Bridging Convex and Nonconvex Optimization in Robust PCA:
Noise, Outliers, and Missing Data

Yuxin Chen∗

Jianqing Fan†

Cong Ma‡

Yuling Yan†

March 2, 2021

Abstract

This paper delivers improved theoretical guarantees for the convex programming approach in low-
rank matrix estimation, in the presence of (1) random noise, (2) gross sparse outliers, and (3) missing
data. This problem, often dubbed as robust principal component analysis (robust PCA), ﬁnds applica-
tions in various domains. Despite the wide applicability of convex relaxation, the available statistical
support (particularly the stability analysis vis-à-vis random noise) remains highly suboptimal, which we
strengthen in this paper. When the unknown matrix is well-conditioned, incoherent, and of constant
rank, we demonstrate that a principled convex program achieves near-optimal statistical accuracy, in
terms of both the Euclidean loss and the (cid:96)∞ loss. All of this happens even when nearly a constant
fraction of observations are corrupted by outliers with arbitrary magnitudes. The key analysis idea lies
in bridging the convex program in use and an auxiliary nonconvex optimization algorithm, and hence
the title of this paper.

Keywords: robust principal component analysis, nonconvex optimization, convex relaxation, (cid:96)∞ guarantees,
leave-one-out analysis

Contents

1 Introduction

1.1 A principled convex programming approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Theory-practice gaps under random noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Models, assumptions and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 A peek at our technical approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 Random signs of outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Prior art

3 Architecture of the proof

3.1 Crude estimation error bounds for convex relaxation . . . . . . . . . . . . . . . . . . . . . . .
3.2 Approximate stationary points of the nonconvex formulation . . . . . . . . . . . . . . . . . .
3.3 Constructing an approximate stationary point via nonconvex algorithms . . . . . . . . . . . .
3.4 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Discussion

2
3
3
5
6
10
12

13

14
14
15
16
17

19

Author names are sorted alphabetically. Corresponding author: Yuxin Chen (Email: yuxin.chen@princeton.edu).
∗Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA; Email:

yuxin.chen@princeton.edu.

†Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA; Email:

{jqfan, yulingy}@princeton.edu.

‡Department of Electrical Engineering and Computer Sciences, UC Berkeley, Berkeley, CA 94720, USA; Email:

congm@berkeley.edu.

1

 
 
 
 
 
 
A An equivalent probabilistic model of Ω(cid:63) used throughout the proof

B Preliminaries

B.1 A few preliminary facts
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Proof of Lemma 2

D Crude error bounds (Proof of Theorem 3)

D.1 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Equivalence between convex and nonconvex solutions (Proof of Theorem 4)

E.1 Preliminary facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.2 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.2.1 Step 1: showing that PΩobs(∆L + ∆S) ≈ 0 . . . . . . . . . . . . . . . . . . . . . . . .
E.2.2 Step 2: showing that PT ⊥(∆L) ≈ 0 and P(Ω(cid:63))c (∆S) ≈ 0 . . . . . . . . . . . . . . . .
E.2.3 Step 3: controlling the size of ∆S (and hence that of ∆L) . . . . . . . . . . . . . . . .
E.2.4 Proof of Claim 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F Analysis of the nonconvex procedure (Proof of Theorem 5)

F.1 Leave-one-out analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
F.2 Key lemmas for establishing the induction hypotheses
F.3 Proof of Lemma 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.4 Proof of Lemma 17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.5 Proof of Lemma 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.6 Proof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.7 Proof of Lemma 20

19

20
20
21

22

23
26
27

28
28
29
30
31
31
32

34
34
36
39
40
42
46
47

1

Introduction

A diverse array of science and engineering applications (e.g. video surveillance, joint shape matching,
graph clustering, covariance modeling, graphical models) involves estimation of low-rank matrices [CLC19,
CLMW11, CGH14, JCSX11, CPW12, FLM13, DR16]. The imperfectness of data acquisition processes, how-
ever, presents several common yet critical challenges: (1) random noise: which reﬂects the uncertainty of
the environment and/or the measurement processes; (2) outliers: which represent a sort of corruption that
exhibits abnormal behavior; and (3) incomplete data, namely, we might only get to observe a fraction of the
matrix entries. Low-rank matrix estimation algorithms aimed at addressing these challenges have been exten-
sively studied under the umbrella of robust principal component analysis (Robust PCA) [CSPW11,CLMW11],
a terminology popularized by the seminal work [CLMW11].

To formulate the above-mentioned problem in a more precise manner, imagine that we seek to estimate
an unknown low-rank matrix L(cid:63) ∈ Rn1×n2. What we can obtain is a collection of partially observed and
corrupted entries as follows

Mij = L(cid:63)

ij + S(cid:63)

ij + Eij,

(i, j) ∈ Ωobs,

(1.1)

where S(cid:63) = [S(cid:63)
ij] is a matrix consisting of outliers, E = [Eij] represents the random noise, and we only observe
entries over an index subset Ωobs ⊆ [n1] × [n2] with [n] := {1, 2, · · · , n}. The current paper assumes that S(cid:63)
is a relatively sparse matrix whose non-zero entries might have arbitrary magnitudes. This assumption has
been commonly adopted in prior work to model gross outliers, while enabling reliable disentanglement of the
outlier component and the low-rank component [CSPW11,CLMW11,CJSC13,Li13]. In addition, we suppose
that the entries {Eij} are independent zero-mean sub-Gaussian random variables, as commonly assumed in
the statistics literature to model a large family of random noise. The aim is to reliably estimate L(cid:63) given
the grossly corrupted and possibly incomplete data (1.1). Ideally, this task should be accomplished without
knowing the locations and magnitudes of the outliers S(cid:63).

2

1.1 A principled convex programming approach

Focusing on the noiseless case with E = 0, the papers by [CSPW11, CLMW11] delivered a positive and
somewhat surprising message: both the low-rank component L(cid:63) and the sparse component S(cid:63) can be
eﬃciently recovered with absolutely no error by means of a principled convex program

minimize
L,S∈Rn1×n2

(cid:107)L(cid:107)∗ + τ (cid:107)S(cid:107)1

subject to PΩobs(L + S − M ) = 0,

(1.2)

provided that certain “separation” and “incoherence” conditions on (L(cid:63), S(cid:63), Ωobs) hold1 and that the regu-
larization parameter τ is properly chosen. Here, (cid:107)L(cid:107)∗ denotes the nuclear norm (i.e. the sum of the singular
values) of L, (cid:107)S(cid:107)1 = (cid:80)
i,j |Sij| denotes the usual entrywise (cid:96)1 norm, and PΩobs (M ) denotes the Euclidean
projection of a matrix M onto the subspace of matrices supported on Ωobs. Given that the nuclear norm
(cid:107) · (cid:107)∗ (resp. the (cid:96)1 norm (cid:107) · (cid:107)1) is the convex relaxation of the rank function rank(·) (resp. the (cid:96)0 counting
norm (cid:107) · (cid:107)0), the rationale behind (1.2) is rather clear: it seeks a decomposition (L, S) of M by promoting
the low-rank structure of L as well as the sparsity structure of S.

Moving on to the more realistic noisy setting, a natural strategy is to solve the following regularized

least-squares problem

minimize
L,S∈Rn1×n2

1
2

(cid:107)PΩobs (L + S − M )(cid:107)2

F + λ (cid:107)L(cid:107)∗ + τ (cid:107)S(cid:107)1 .

(1.3)

With the regularization parameters λ, τ > 0 properly chosen, one hopes to strike a balance between enhancing
the goodness of ﬁt (by enforcing L+S −M to be small) and promoting the desired low-complexity structures
(by regularizing both the nuclear norm of L and the (cid:96)1 norm of S). A natural and important question comes
into our mind:

Where does the algorithm (1.3) stand in terms of its statistical performance vis-à-vis random noise,
sparse outliers and missing data?

Unfortunately, however simple this program (1.3) might seem, the existing theoretical support remains far
from satisfactory, as we shall discuss momentarily.

1.2 Theory-practice gaps under random noise

To assess the tightness of prior statistical guarantees for (1.3), we ﬁnd it convenient to ﬁrst look at a simple
setting where (i) n1 = n2 = n, (ii) E consists of independent Gaussian components, namely, Eij ∼ N (0, σ2),
and (iii) there is no missing data. This simple scenario is suﬃcient to illustrate the sub-optimality of prior
theory.

Prior statistical guarantees The paper [ZLW+10] was the ﬁrst to derive a sort of statistical perfor-
mance guarantees for the above convex program. Under mild conditions, [ZLW+10] demonstrated that any
minimizer ( (cid:98)L, (cid:98)S) of (1.3) achieves2

(cid:13) (cid:98)L − L(cid:63)(cid:13)
(cid:13)

(cid:13)F = O (cid:0)n(cid:13)

(cid:13)E(cid:13)
(cid:13)F

(cid:1) = O(σn2)

(1.4)

with high probability, where we have substituted in the well-known high-probability bound (cid:107)E(cid:107)F = O (σn)
under i.i.d. Gaussian noise. While this theory corroborates the potential stability of convex relaxation against
both additive noise and sparse outliers, it remains unclear whether the estimation error bound (1.4) reﬂects
the true performance of the convex program in use. In what follows, we shall compare it with an oracle error
bound and collect some numerical evidence.

1Clearly, if the low-rank matrix L(cid:63) is also sparse, one cannot possibly separate S(cid:63) from L(cid:63). The same holds true if the

matrix S(cid:63) is simultaneously sparse and low-rank.

2Mathematically, [ZLW+10] investigated an equivalent constrained form of (1.3) and developed an upper bound on the

corresponding estimation error.

3

(a)

(b)

Figure 1: (a) Euclidean estimation errors of (1.3) and (1.5) vs. the problem size
n, where we ﬁx r = 5, σ =
10−3; (b) Euclidean estimation errors of (1.3) and (1.5) vs. the noise level σ in a log-log plot, where we ﬁx
log n. The results are averaged over 50
n = 1000, r = 5. For both plots, we take λ = 5σ
independent trials.

n and τ = 2σ

√

√

√

Comparisons with an oracle bound Consider an idealistic scenario where an oracle informs us of the
outlier matrix S(cid:63). With the assistance of this oracle, the task of estimating L(cid:63) reduces to a low-rank matrix
denoising problem [DG14]. By ﬁxing S to be S(cid:63) in (1.3), we arrive at a simpliﬁed convex program

minimize
L∈Rn×n

1
2

(cid:107)L − (L(cid:63) + E)(cid:107)2

F + λ (cid:107)L(cid:107)∗ .

(1.5)

It is known that (e.g. [DG14,CCF+20]): under mild conditions and with a properly chosen λ, the estimation
error of (1.5) satisﬁes

(cid:13)F = O (cid:0)σ
where we abuse the notation and denote by (cid:98)L the minimizer of (1.5). The large gap between the above two
bounds (1.4) and (1.6) is self-evident; in particular, if r = O(1), the gap between these two bounds can be
as large as an order of n1.5.

(cid:13) (cid:98)L − L(cid:63)(cid:13)
(cid:13)

nr(cid:1) ,

(1.6)

√

√

A numerical example without oracles One might naturally wonder whether the discrepancy between
the two bounds (1.4) and (1.6) stems from the magical oracle information (i.e. S(cid:63)) which (1.3) does not
have the luxury to know. To demonstrate that this is not the case, we conduct some numerical experiments
to assess the importance of such oracle information. Generate L(cid:63) = X (cid:63)Y (cid:63)(cid:62), where X (cid:63), Y (cid:63) ∈ Rn×r are
random orthonormal matrices. Each entry of S(cid:63) is generated independently from a mixed distribution:
with probability 1/10, the entry is drawn from N (0, 10); otherwise, it is set to be zero. In other words,
approximately 10% of the entries in L(cid:63) are corrupted by large outliers. Throughout the experiments, we
set λ = 5σ
log n with σ the standard deviation of each noise entry {Eij}. Figure 1(a) ﬁxes
n. Similarly,
r = 5, σ = 10−3 and examines the dependency of the Euclidean error (cid:107) (cid:98)L − L(cid:63)(cid:107)F on the size
Figure 1(b) ﬁxes r = 5, n = 1000 and displays the estimation error (cid:107) (cid:98)L − L(cid:63)(cid:107)F as the noise size σ varies in a
log-log plot. As can be seen from Figure 1, the performance of the oracle-aided estimator (1.5) matches the
√
theoretical prediction (1.6), namely, the numerical estimation error (cid:107) (cid:98)L − L(cid:63)(cid:107)F is proportional to both
n
and σ. Perhaps more intriguingly, even without the help of the oracle, the original estimator (1.3) performs
quite well and behaves qualitatively similarly.
In comparison with the bound (1.4) derived in the prior
work [ZLW+10], our numerical experiments suggest that the convex estimator (1.3) might perform much
better than previously predicted.

n and τ = 2σ

√

√

All in all, there seems to be a large gap between the practical performance of (1.3) and the existing theoretical
support. This calls for a new theory that better explains practice, which we pursue in the current paper.
We remark in passing that statistical guarantees have been developed in [ANW12, KLT17] for other convex

4

203040500.20.40.60.8without oracle informationwith oracle information10-610-510-410-310-410-2100without oracle informationwith oracle informationestimators (i.e. the ones that are diﬀerent from the convex estimator (1.3) considered herein). We shall
compare our results with theirs later in Section 1.4.

1.3 Models, assumptions and notation

As it turns out, the appealing empirical performance of the convex program (1.3) in the presence of both
sparse outliers and zero-mean random noise can be justiﬁed in theory. Towards this end, we need to introduce
several notations and model assumptions that will be used throughout. Let U (cid:63)Σ(cid:63)V (cid:63)(cid:62) be the singular value
decomposition (SVD) of the unknown rank-r matrix L(cid:63) ∈ Rn1×n2 , where U (cid:63) ∈ Rn1×r and V (cid:63) ∈ Rn2×r
consist of orthonormal columns and Σ(cid:63) = diag{σ(cid:63)
1, . . . , σ(cid:63)

r } is a diagonal matrix. Here, we let

σmax := σ(cid:63)

1 ≥ σ(cid:63)

2 ≥ · · · ≥ σ(cid:63)

r =: σmin

and

κ := σmax/σmin

represent the singular values and the condition number of L(cid:63), respectively. We denote by Ω(cid:63) the support
set of S(cid:63), that is,

Ω(cid:63) := {(i, j) ∈ Ωobs : S(cid:63)

ij (cid:54)= 0}.

(1.7)

With this set of notation in place, we list below our key model assumptions.

Assumption 1 (Incoherence). The low-rank matrix L(cid:63) with SVD L(cid:63) = U (cid:63)Σ(cid:63)V (cid:63)(cid:62) is assumed to be
µ-incoherent in the sense that

(cid:107)U (cid:63)(cid:107)2,∞ ≤

(cid:114) µ
n1

(cid:107)U (cid:63)(cid:107)F =

(cid:114) µr
n1

and

(cid:107)V (cid:63)(cid:107)2,∞ ≤

(cid:114) µ
n2

(cid:107)V (cid:63)(cid:107)F =

(cid:114) µr
n2

.

(1.8)

Here, (cid:107)U (cid:107)2,∞ denotes the largest (cid:96)2 norm of all rows of a matrix U .
Assumption 2 (Random sampling). Each entry is observed independently with probability p, namely,

P {(i, j) ∈ Ωobs} = p.

(1.9)

Assumption 3 (Random locations of outliers). Each observed entry is independently corrupted by an
outlier with probability ρs, namely,

P {(i, j) ∈ Ω(cid:63) | (i, j) ∈ Ωobs} = ρs,

(1.10)

where Ω(cid:63) ⊆ Ωobs is the support of the outlier matrix S(cid:63).

Assumption 4 (Random signs of outliers). The signs of the nonzero entries of S(cid:63) are i.i.d. symmetric
Bernoulli random variables (independent from the locations), namely,

sign(S(cid:63)

ij) ind.=

(cid:40)

1,
−1,

with probability 1/2,
else,

for all (i, j) ∈ Ω(cid:63).

(1.11)

Assumption 5 (Random noise). The noise matrix E = [Eij] is composed of independent symmetric3
zero-mean sub-Gaussian random variables with sub-Gaussian norm at most σ > 0, i.e. (cid:107)Eij(cid:107)ψ2 ≤ σ (see
[Ver12, Deﬁnition 5.7] for precise deﬁnitions).

We take a moment to expand on our model assumptions. Assumption 1 is standard in the low-rank matrix
recovery literature [CR09, CLMW11, Che15, CLC19]. If µ is small, then this assumption speciﬁes that the
singular spaces of L(cid:63) is not sparse in the standard basis, thus ensuring that L(cid:63) is not simultaneously low-rank
and sparse. Assumption 3 requires the sparsity pattern of the outliers S(cid:63) to be random, which precludes it
from being simultaneously sparse and low-rank. In essence, Assumptions 1 and 3 are identiﬁability conditions,
taken together as a sort of separation condition on (L(cid:63), S(cid:63)), which plays a crucial role in guaranteeing
exact recovery in the noiseless case (i.e. E = 0); see [CLMW11] for more discussions on these conditions.

3In fact, we only require Eij to be symmetric for all (i, j) ∈ Ω(cid:63).

5

Assumption 4 requires the signs of the outliers to be random, which has also been made in [ZLW+10,WL17].4
We shall discuss in detail the crucial role of this random sign assumption (as opposed to deterministic sign
patterns) in Section 1.6.

Finally, we introduce several notation to be used throughout. Denote by f (n) (cid:46) g(n) or f (n) = O(g(n))
the condition |f (n)| ≤ Cg(n) for some constant C > 0 when n is suﬃciently large; we use f (n) (cid:38) g(n) to
denote f (n) ≥ C|g(n)| for some constant C > 0 when n is suﬃciently large; we also use f (n) (cid:16) g(n) to
indicate that f (n) (cid:46) g(n) and f (n) (cid:38) g(n) hold simultaneously. The notation f (n) (cid:29) g(n) (resp. f (n) (cid:28)
g(n)) means that there exists a suﬃciently large (resp. small) constant c1 > 0 (resp. c2 > 0) such that
f (n) ≥ c1g(n) (resp. f (n) ≤ c2g(n)). For any subspace T , we denote by PT (M ) the Euclidean projection
of a matrix M onto the subspace T , and let PT ⊥ (M ) := M − PT (M ). For any index set Ω, we denote by
PΩ(M ) the Euclidean projection of a matrix M onto the subspace of matrices supported on Ω, and deﬁne
PΩc(M ) := M − PΩ(M ). For any matrix M , we let (cid:107)M (cid:107), (cid:107)M (cid:107)F, (cid:107)M (cid:107)∗, (cid:107)M (cid:107)1 and (cid:107)M (cid:107)∞ denote its
spectral norm, Frobenius norm, nuclear norm, entrywise (cid:96)1 norm, and entrywise (cid:96)∞ norm, respectively.

1.4 Main results

Armed with the above model assumptions, we are positioned to present our improved statistical guarantees
for convex relaxation (1.3) in the random noise setting. Without loss of generality, assume that

n1 ≥ n2.

As we shall elucidate in Section 1.5 and Section 3, our theory is established by exploiting an intriguing
and intimate connection between convex relaxation and nonconvex optimization, and hence the title of this
paper.

For the sake of simplicity, we shall start by presenting our statistical guarantees when the rank r, the
condition number κ and the incoherence parameter µ of L(cid:63) are all bounded by some constants. Despite
its simplicity, this setting subsumes as special cases a wide array of fundamentally important applications,
including angular and phase synchronization [Sin11] in computational biology, joint shape mapping prob-
lem [HG13, CGH14] in computer vision, and so on. All of these problems involve estimating a very well-
conditioned matrix L(cid:63) with a small rank.

Theorem 1. Suppose that Assumptions 1-5 hold, and that r, κ, µ = O(1). Take λ = Cλσ
Cτ σ

log n2 in (1.3) for some large enough constants Cλ, Cτ > 0. Assume that

√

n1n2p ≥ Csamplen1 log6 n1,

σ
σmin

(cid:114) n1
p

≤

cnoise√
log n1

and ρs ≤

coutlier
log n1

√

n1p and τ =

(1.12)

for some suﬃciently large constant Csample > 0 and some suﬃciently small constants cnoise, coutlier > 0. Then
with probability exceeding 1 − O(n−3

2 ), the following holds:

1. Any minimizer (Lcvx, Scvx) of the convex program (1.3) obeys
(cid:114) n1
p

(cid:107)Lcvx − L(cid:63)(cid:107)F ≤ Cerr

σ
σmin

(cid:107)L(cid:63)(cid:107)F

(cid:107)Lcvx − L(cid:63)(cid:107)∞ ≤ Cerr

(cid:107)Lcvx − L(cid:63)(cid:107) ≤ Cerr

(cid:115)

n1 log n1
p

(cid:107)L(cid:63)(cid:107)∞

(cid:114) n1
p

(cid:107)L(cid:63)(cid:107)

σ
σmin

σ
σmin

(1.13a)

(1.13b)

(1.13c)

for some constant Cerr > 0.

2. Letting Lcvx,r := arg minL:rank(L)≤r (cid:107)L − Lcvx(cid:107)F be the best rank-r approximation of Lcvx, we have

(cid:107)Lcvx,r − Lcvx(cid:107)F ≤

1
n5
2

·

σ
σmin

(cid:114) n1
p

(cid:107)L(cid:63)(cid:107)F ,

(1.14)

4Note that while the theorems in [ZLW+10, WL17] do not make explicit this random sign assumption, the proofs therein do

rely on this assumption to guarantee the existence of certain approximate dual certiﬁcates.

6

and the statistical guarantees (1.13) hold unchanged if Lcvx is replaced by Lcvx,r.

Before we embark on interpreting our statistical guarantees, let us ﬁrst parse the required conditions (1.12)

in Theorem 1. For simplicity we assume that n1 = n2 = n.

• Missing data. Theorem 1 accommodates the case where a dominant fraction of entries are unobserved
(more precisely, the sample size can be as low as an order of n poly log n). This is an appealing result since,
even when there is no noise and no outlier (i.e. E = 0 and ρs = 0), the minimal sample size required
for exact matrix completion is at least on the order of n log n [CT10].
In comparison, prior theory
on robust PCA with both sparse outliers and dense additive noise is either based on full observations
[ZLW+10, ANW12], or assumes the sampling rate p exceeds some universal constant [WL17]. In other
words, these prior results require the number of observed entries to exceed the order of n2. The only
exception is [KLT17], which also allows a signiﬁcant amount of missing data (i.e. p (cid:38) (poly log n)/n).
• Noise levels. The noise condition, namely σ(cid:112)n log n/p (cid:46) σmin, accommodates a wide range of noise

levels. To see this, it is straightforward to check that this noise condition is equivalent to

σ (cid:46)

(cid:114) np
log n

(cid:107)L(cid:63)(cid:107)∞

as long as r, µ, κ (cid:16) 1. In other words, the entrywise noise level σ is allowed to be signiﬁcantly larger than
the maximum magnitude of the entries in the low-rank matrix L(cid:63), as long as p (cid:29) (log n)/n.

• Tolerable fraction of outliers. The above theorem assumes that no more than a fraction ρs (cid:46) 1/ log n of
observations are corrupted by outliers. In words, our theory allows nearly a constant proportion (up to
a logarithmic order) of the entries of L(cid:63) to be corrupted with arbitrary magnitudes.

Next, we move on to the interpretation of our statistical guarantees. Note that we still assume that

n1 = n2 = n for ease of presentation.

• Near-optimal statistical guarantees. Our ﬁrst result (1.13a) gives an Euclidean estimation error bound

of (1.3)

(cid:107)Lcvx − L(cid:63)(cid:107)F

(cid:46) σ

(cid:114) n
p

.

(1.15)

This cannot be improved even when an oracle has informed us of the outliers S(cid:63) and the tangent space
of L(cid:63); see [CP10, Section III.B]. We remark that under similar model assumptions, the paper [WL17]
derived an estimation error bound for a constrained version of the convex program (1.3), which asserts
that this convex estimator (cid:101)Lcvx satisﬁes 5
(cid:13) (cid:101)Lcvx − L(cid:63)(cid:13)
(cid:13)
(cid:13)F

(cid:46) σn1.5,

(1.16)

with the proviso that p is at least on the constant order. The restriction on p arises from the dual
certiﬁcate constructed in [CLMW11], which is also used in the Proof of Theorem 4 in [WL17]. While
this is sub-optimal compared to our results in the setting considered herein, it is worth pointing out
that the bound therein accommodates arbitrary noise matrix E (e.g. deterministic, adversary), and here
in (1.16) we specialize their result to the random noise setting, namely the noise E obeys Assumption 5.
In addition, under the full observation (i.e. p = 1) setting, the paper [ANW12] derived an estimation error
bound for a convex program similar to (1.3), but with an additional constraint regularizing the spikiness
of the low-rank component. Note that instead of imposing the incoherence condition as in Assumption 1,
the prior work [ANW12] assumes a milder spikiness condition on L(cid:63), which only constrains the maximum
entry in the matrix L(cid:63) is not too large. When {Eij} are i.i.d. drawn from N (0, σ2) and when there is

5More speciﬁcally, [WL17, Theorem 4] studies the following convex program minimizeL,S∈Rn×n (cid:107)L(cid:107)∗ +λ(cid:107)S(cid:107)1 s.t. (cid:107)PΩobs (L+
S − M )(cid:107)F ≤ δ. Here, the quantity δ needs to be larger than (cid:107)PΩobs (L + S − M )(cid:107)F. Under our setting, the minimum level of δ
p. With this choice of δ, [WL17, Theorem
should be a high-probability upper bound on (cid:107)PΩobs (E)(cid:107)F, which is on the order of σn
4] yields (cid:107) (cid:101)Lcvx − L(cid:63)(cid:107)F ≤ [2 + 8

n(1 + (cid:112)8/p)]δ (cid:46) σn1.5.

√

√

7

Table 1: Comparison of our statistical guarantee and prior theory.

Euclidean estimation error

Accounting for missing data

[ZLW+10]

[ANW12]

√

σ

√

n max{

√

r,

σn2

nρs log n} + (cid:107)L(cid:63)(cid:107)∞n

√

ρs

no

no

[WL17]
[KLT17] max{σ, (cid:107)L(cid:63)(cid:107)∞, (cid:107)S(cid:63)(cid:107)∞}(cid:112)(n log n)/p max{1,
This paper

σn1.5

σ(cid:112)nr/p

√

npρs}

yes (p (cid:38) 1)
yes (p (cid:38) (poly log n)/n)
yes (p (cid:38) κ4µ2r2(poly log n)/n)

no missing data (i.e. p = 1), the Euclidean estimation error bound achievable by their estimator LANW
reads

cvx

(cid:13)
(cid:13)LANW

cvx − L(cid:63)(cid:13)
(cid:13)F

√

(cid:46) σ

n max

(cid:110)
1, (cid:112)nρs log n

(cid:111)

+ (cid:107)L(cid:63)(cid:107)∞n

ρs,

(1.17)

√

√

which is sub-optimal compared to our results. In particular, (i) the bound (1.17) does not vanish even
as the noise level decreases to zero, and (ii) it becomes looser as ρs grows (e.g. if ρs (cid:16) 1/ log n, the bound
(1.17) is O(
n) larger than our bound). Moreover, the work [ANW12] did not account for missing data.
Similar to [ANW12] (but with an additional spikiness condition on S(cid:63)), the paper [KLT17] derived an
estimation error bound for a constrained convex program, with a new constraint regularizing the spikiness
of the sparse outliers. Their Euclidean estimation error bound reads

(cid:13)
(cid:13)LKLT

cvx − L(cid:63)(cid:13)
(cid:13)F

(cid:46) max {σ, (cid:107)L(cid:63)(cid:107)∞ , (cid:107)S(cid:63)(cid:107)∞}

(cid:115)

n log n
p

max {1,

√

npρs} ,

(1.18)

which is also sub-optimal compared to our results. In particular, (1) their error bound degrades as the
magnitude (cid:107)S(cid:63)(cid:107)∞ of the outlier increases; (2) when there is no missing data (i.e. p = 1), their bound
might be oﬀ by a factor as large as O(
n). It is worth emphasizing that the theory developed in these
prior works is developed to accommodate a broader range of matrices. For example, both [ANW12]
and [KLT17] study the set of entrywise bounded low-rank matrices (without assuming the incoherence
condition); [ANW12] even allows L(cid:63) to be approximately low rank. To ease comparison, Table 1 displays
a summary of our results vs. prior statistical guarantees when specialized to the settings considered
herein.

√

• Entrywise and spectral norm error control. Moving beyond Euclidean estimation errors, our theory
also provides statistical guarantees measured by two other important metrics: the entrywise (cid:96)∞ norm
(cf. (1.13b)) and the spectral norm (cf. (1.13c)). In particular, our entrywise error bound (1.13b) in reads

(cid:107)Lcvx − L(cid:63)(cid:107)∞

(cid:46) σ

(cid:115)

log n
np

(1.19)

as long as r, κ, µ (cid:16) 1, which is about O(n) times small than the Euclidean loss (1.15) modulo some
logarithmic factor. This uncovers an appealing “delocalization” behavior of the estimation errors, namely,
the estimation errors of L(cid:63) are fairly spread out across all entries. This can also be viewed as an “implicit
regularization” phenomenon: the convex program automatically controls the spikiness of the low-rank
solution, without the need of explicitly regularizing it (e.g. adding a constraint (cid:107)L(cid:107)∞ ≤ α as adopted in
the prior work [ANW12, KLT17]). See Figure 2 for the numerical evidence for the relative entrywise and
spectral norm error of Lcvx.

• Approximate low-rank structure of the convex estimator Lcvx. Last but not least, Theorem 1 ensures that
the convex estimate Lcvx is nearly rank-r, so that a rank-r approximation of Lcvx is extremely accurate.
In other words, the convex program automatically adapts to the true rank of L(cid:63) without having any
prior knowledge about r. As we shall see shortly, this is a crucial observation underlying the intimate
connection between convex relaxation and a certain nonconvex approach.

8

Figure 2: The relative estimation error of Lcvx measured by both (cid:107) · (cid:107)∞ (i.e. (cid:107)Lcvx − L(cid:63)(cid:107)∞/(cid:107)L(cid:63)(cid:107)∞) and (cid:107) · (cid:107)
(i.e. (cid:107)Lcvx − L(cid:63)(cid:107)/(cid:107)L(cid:63)(cid:107)) vs. the standard deviation σ of the noise in a log-log plot. The results are reported
log n, and are averaged over 50 independent
for n = 1000, r = 5, p = 0.2, ρs = 0.1, λ = 5σ
trials. In addition, the data generating process is similar to that in Figure 1.

np, τ = 2σ

√

√

Moving beyond the setting with r, κ, µ (cid:16) 1, we have developed theoretical guarantees that allow r, κ, µ

to grow with the problem dimension n1, n2. The result is this.

Theorem 2. Suppose that Assumptions 1-5 hold and that n1 ≥ n2. Take λ = Cλσ
in (1.3) for some large enough constants Cλ, Cτ > 0. Assume that

√

n1p and τ = Cτ σ

√

log n2

n1n2p ≥ Csampleκ4µ2r2n1 log6 n1,

σ
σmin

(cid:114) n1
p

≤

cnoise
(cid:112)κ4µr log n1

, and ρs ≤

coutlier
κ3µr log n1

(1.20)

for some suﬃciently large constant Csample > 0 and some suﬃciently small constants cnoise, coutlier > 0. Then
with probability exceeding 1 − O(n−3

2 ), the following holds:

1. Any minimizer (Lcvx, Scvx) of the convex program (1.3) obeys

(cid:107)Lcvx − L(cid:63)(cid:107)F ≤ Cerrκ

σ
σmin

(cid:114) n1
p

(cid:107)L(cid:63)(cid:107)F
(cid:115)

(cid:107)Lcvx − L(cid:63)(cid:107)∞ ≤ Cerr

(cid:112)

κ3µr ·

σ
σmin

n1 log n1
p

(cid:107)Lcvx − L(cid:63)(cid:107) ≤ Cerr

σ
σmin

(cid:114) n1
p

(cid:107)L(cid:63)(cid:107)

(cid:107)L(cid:63)(cid:107)∞

(1.21a)

(1.21b)

(1.21c)

for some constant Cerr > 0.

2. Letting Lcvx,r := arg minL:rank(L)≤r (cid:107)L − Lcvx(cid:107)F be the best rank-r approximation of Lcvx, we have

(cid:107)Lcvx,r − Lcvx(cid:107)F ≤

1
n5
2

·

σ
σmin

(cid:114) n1
p

(cid:107)L(cid:63)(cid:107)F ,

(1.22)

and the statistical guarantees (1.21) hold unchanged if Lcvx is replaced by Lcvx,r.

Similar to Theorem 1, our general theory (i.e. Theorem 2) provides the estimation error of the convex
estimator Lcvx in three diﬀerent norms (i.e. the Euclidean, entrywise and operator norms), and reveals the
near low-rankness of the convex estimator (cf. (1.22)) as well as the implicit regularization phenomenon
(cf. (1.21b)).

Finally, we make note of several aspects of our general theory that call for further improvement. For
instance, when there is no missing data and n1 = n2 = n, the rank r of the unknown matrix L(cid:63) needs to
n. On the positive side, our result allows r to grow with the problem dimension n. However,
satisfy r (cid:46)

√

9

10-610-510-410-310-410-2100Entrywise estimation errorSpectral norm estimation errorFigure 3: Euclidean estimation errors of (1.3) vs. ρs under four diﬀerent ranks r = 5, 10, 15, 20. The results
are reported for n = 1000, p = 0.04r, σ = 10−3, λ = 5σ
log n, and are averaged over 50
independent trials. In addition, the data generating process is similar to that in Figure 1.

np, τ = 2σ

√

√

prior results in the noiseless case [CLMW11,Li13] allow r to grow almost linearly with n. This unsatisfactory
aspect arises from the suboptimal analysis (in terms of the dependency on r) of a tightly related nonconvex
estimation algorithm (to be elaborated on later), which, to the best of our knowledge, has not been resolved
in the nonconvex low-rank matrix recovery literature [MWCC20, CLL20]. See Section 2 for more discussions
about this point. Moreover, when E = 0, it is known that ρs can be as large as a constant even when the
rank r is allowed to grow with the dimension n [Li13, CJSC13]. Our current theory, however, fails to cover
the case with ρs (cid:16) 1 in the presence of noise. We demonstrate through numerical experiments that the
dependence of ρs on r might indeed by suboptimal in our current theory. More speciﬁcally, Figure 3 depicts
the numerical Euclidean estimation errors w.r.t. the corruption probability ρs as we vary the rank while
ﬁxing the sampling ratio. It can be seen that the estimation error curves corresponding to diﬀerent ranks
align very well with each other, thus suggesting the capability of convex relaxation in tolerating a constant
fraction ρs of outliers.

1.5 A peek at our technical approach

Before delving into the proof details, we immediately highlight our key technical ideas and novelties. For
simplicity we assume n1 = n2 = n throughout this section.

Connections between convex and nonconvex optimization.
program (1.3), we turn attention to a seemingly diﬀerent, but in fact closely related, nonconvex program

Instead of directly analyzing the convex

minimize
X,Y ∈Rn×r,S∈Rn×n

1
2

(cid:13)
(cid:13)PΩobs

(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
F +
(cid:13)

λ
2

(cid:0) (cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

F

(cid:1) + τ (cid:107)S(cid:107)1 .

(1.23)

This idea is inspired by an interesting numerical ﬁnding (cf. Figure 4) that the solution to the convex
program (1.3), and an estimate obtained by attempting to solve the nonconvex formulation (1.23), are
If such an intimate connection can be formalized, then it suﬃces
exceedingly close in our experiments.
to analyze the statistical performance of the nonconvex approach instead.6 Fortunately, recent advances
in nonconvex low-rank factorization (see [CLC19] for an overview) provide powerful tools for analyzing
nonconvex low-rank estimation, allowing us to derive the desired statistical guarantees that can then be
transferred to the convex approach. Of course, this is merely a high-level picture of our proof strategy, and
we defer the details to Section 3.

It is worth emphasizing that our key idea — that is, bridging convex and nonconvex optimization — is
drastically diﬀerent from previous technical approaches for analyzing convex estimators (e.g. (1.3)). As it

6On the surface, the convex program (1.3) and the nonconvex one (1.23) are closely related: the convex solution (Lcvx, Scvx)
coincides with that of the nonconvex program (1.23) if Lcvx is rank-r. This is an immediate consequence of the algebraic identity
(cid:107)Z(cid:107)∗ = infX,Y ∈Rn×r :XY (cid:62)=Z ((cid:107)X(cid:107)2
F) [SS05, MHT10]. However, it is diﬃcult to know a priori the rank of the convex
solution. Hence such a connection does not prove useful in establishing the statistical properties of the convex estimator.

F + (cid:107)Y (cid:107)2

10

00.050.10.150.20.250.811.21.41.6(a)

(b)

Figure 4: (a) The relative estimation errors of both Lcvx (the convex estimator (1.3)) and Lncvx (the estimate
returned by the nonconvex approach tailored to (1.23)) and the relative distance between them vs. the
standard deviation σ of the noise. (b) The relative estimation errors of both Scvx (the convex estimator
in (1.3)) and Sncvx (the estimate returned by the nonconvex approach tailored to (1.23)) and the relative
distance between them vs. the standard deviation σ of the noise. The results are reported for n = 1000,
r = 5, p = 0.2, ρs = 0.1, λ = 5σ

log n and are averaged over 50 independent trials.

np, τ = 2σ

√

√

turns out, these prior approaches, which include constructing dual certiﬁcates and/or exploiting restricted
strong convexity, have their own deﬁciencies in analyzing (1.3) and fall short of explaining the eﬀectiveness of
(1.3) in the random noise setting. For instance, constructing dual certiﬁcates in the noisy case is notoriously
challenging given that we do not have closed-form expressions for the primal solutions (so that it is diﬃcult
to invoke the powerful dual construction strategies like the golﬁng scheme [Gro11] developed for the noiseless
case). If we directly utilize the dual certiﬁcates constructed for the noiseless case, we would end up with an
overly conservative bound like (1.4), which is exactly why the results in [ZLW+10, WL17] are sub-optimal.
On the other hand, while it is viable to show certain strong convexity of (1.3) when restricted to some highly
local sets and directions, it is unclear how (1.3) forces its solution to stay within the desired set and follow
the desired directions, without adding further (and often unnecessary) constraints to (1.3).

It is worth noting that a similar
Nonconvex low-rank estimation with nonsmooth loss functions.
connection between convex and nonconvex optimization has been pointed out by [CCF+20] towards under-
standing the power of convex relaxation for noisy matrix completion. Due to the absence of sparse outliers
in the noisy matrix completion problem, the nonconvex loss function considered therein is smooth in nature,
which greatly simpliﬁes both the algorithmic and theoretical development. By contrast, the nonsmoothness
inherent in (1.23) makes it particularly challenging to achieve the two desiderata mentioned above, namely,
connecting the convex and nonconvex solutions and establishing the optimality of the nonconvex solution.
In fact, to establish the connection between convex and nonconvex solutions, we put forward a novel two-
step analysis strategy. Speciﬁcally, we ﬁrst develop a crude upper bound on the Euclidean estimation error
leveraging the idea of approximate dual certiﬁcates; see Theorem 3. While this crude upper bound is far
from optimal, it serves as an important starting point towards formalizing the intimate relation between
the convex solution (Lcvx, Scvx) and the nonconvex solution (X, Y , S), since it is challenging to establish
XY ≈ Lcvx and S ≈ Scvx simultaneously without the aid of a crude bound. Second, in establishing the
optimality of the nonconvex solution, the nonsmoothness nature of the nonconvex loss prevents us from
applying the vanilla gradient descent scheme (as has been done in [CCF+20]). To address this issue, we
develop an alternating minimization scheme — which alternates between gradient updates on (X, Y ) and
minimization of S — aimed at minimizing the nonsmooth nonconvex loss function (1.23); see Algorithm 1
for details. As it turns out, such a simple algorithm allows us to track the proximity of the convex and
nonconvex solutions and establish the optimality of the nonconvex solution all at once.

11

10-610-510-410-310-1010-5100Estimation error: convexEstimation error: nonconvexDistance between solutions10-610-510-410-310-1010-5100Estimation error: convexEstimation error: nonconvexDistance between solutionsn under ﬁxed
Figure 5: The red (resp. blue) line displays the Euclidean estimation error of (1.3) vs.
(resp. random) sign patterns of S(cid:63). The green line displays the Euclidean distance between Lcvx and (cid:101)L(cid:63)
under ﬁxed sign patterns of S(cid:63). The results are reported for r = 5, p = 1, σ = 10−3, and ρs = 1/ log n, with
log n and are averaged over 50 independent trials. For the random sign setting,
λ = 5σ
the nonzero entries of S(cid:63) are independently generated as z · 5σ, where z follows a Rademacher distribution.
For the ﬁxed sign setting, each nonzero entry of S(cid:63) equals to 5σ.

np and τ = 2σ

√

√

√

1.6 Random signs of outliers

The careful reader might wonder whether it is possible to remove the random sign assumption on S(cid:63) (namely,
Assumption 4) without compromising our statistical guarantees. After all, the results of [CSPW11,CLMW11,
Li13] derived for the noise-free case do not rely on such a random sign assumption at all.7 Unfortunately,
removal of such a condition might be problematic in general, as illustrated by the following example.

An example with non-random signs Suppose that (i) n1 = n2 = n, (ii) each non-zero entry of S(cid:63)
obeys S(cid:63)
ij = c0σ, (iii) ρs = c1/ log n for some suﬃciently small constant c1 > 0, and (iv) there is no missing
data (i.e. p = 1). In such a scenario, the data matrix can be decomposed as

M = L(cid:63) + S(cid:63) + E = L(cid:63) + E[S(cid:63)]
(cid:125)

(cid:124)

(cid:123)(cid:122)
=: (cid:101)L(cid:63)

+ S(cid:63) − E[S(cid:63)] + E
.
(cid:125)
(cid:123)(cid:122)
=: (cid:101)E

(cid:124)

Two observations are worth noting: (1) given that E[S(cid:63)] = c0ρsσ11(cid:62) with 1 the all-one vector, the rank of
the matrix (cid:101)L(cid:63) = L(cid:63) + E[S(cid:63)] is at most r + 1; (2) (cid:101)E is a zero-mean random matrix consisting of independent
entries with sub-Gaussian norm O(σ). In other words, the decomposition M = (cid:101)L(cid:63) + (cid:101)E corresponds to a
case with random noise but no outliers. Consequently, we can invoke Theorem 1 to conclude that (assuming
r = O(1) and (cid:101)L(cid:63) is incoherent with condition number O(1)): any minimizer (Lcvx, Scvx) of (1.3) obeys

(cid:107)Lcvx − L(cid:63) − ρsσ11(cid:62)(cid:107)F = (cid:107)Lcvx − (cid:101)L(cid:63)(cid:107)F (cid:46)

√

nr

(cid:46) σ

σmax( (cid:101)L(cid:63))
σmin( (cid:101)L(cid:63))

√

n(cid:107) (cid:101)L(cid:63)(cid:107)F

σ
σmin( (cid:101)L(cid:63))
√

(cid:46) σ

n

with high probability. Here the last step follows since (cid:101)L(cid:63) is of constant rank and condition number. This,
however, leads to a lower bound on the estimation error

(cid:107)Lcvx − L(cid:63)(cid:107)F ≥ (cid:107)c0ρsσ11(cid:62)(cid:107)F − (cid:107)Lcvx − L(cid:63) − ρsσ11(cid:62)(cid:107)F = σ(cid:0)c0ρsn − O(

√

n)(cid:1)

= (1 − o(1))

c0c1σn
log n

,

7Notably, in the noisy setting, prior theory [ZLW+10, WL17] also implicitly assumes this random sign condition, while

[ANW12, KLT17] do not require this condition.

12

2030405000.511.52√

n/ log n) times larger than the desired estimation error O(σ

n). Numerical experiments
which can be O(
under the above setting (with c0 = 5 and c1 = 1) also suggest that (i) the estimation error under the ﬁxed
sign setting might be orderwise larger than that under the random sign setting; and (ii) under the ﬁxed sign
setting, the estimator (1.3) approximately recovers (cid:101)L(cid:63) instead of L(cid:63); see Figure 5.

√

The take-away message is this: when the entries of S(cid:63) are of non-random signs, it might sometimes be
possible to decompose S(cid:63) into (1) a low-rank bias component with a large Euclidean norm, and (2) a
random ﬂuctuation component whose typical size does not exceed that of E. If this is the case, then the
convex program (1.3) might mistakenly treat the bias component as a part of the low-rank matrix L(cid:63), thus
dramatically hampering its estimation accuracy.

2 Prior art

Principal component analysis (PCA) [Pea01,Jol11,FSZZ18] is one of the most widely used statistical methods
for dimension reduction in data analysis. However, PCA is known to be quite sensitive to adversarial outliers
— even a single corrupted data point can make PCA completely oﬀ. This motivated the investigation of
robust PCA, which aims at making PCA robust to gross adversarial outliers. As formulated in [CLMW11,
CSPW11], this is closely related to the problem of disentangling a low-rank matrix L(cid:63) and a sparse outlier
matrix S(cid:63) (with unknown locations and magnitudes) from a superposition of them. Consequently, robust
PCA can be viewed as an outlier-robust extension of the low-rank matrix estimation/completion tasks
[CR09, KMO10, CLC19]. In a similar vein, robust PCA has also been extensively studied in the context of
structured covariance estimation under approximate factor models [FFL08, FLM13, FWZ18, FWZ19], where
the population covariance of certain random sample vectors is a mixture of a low-rank matrix and a sparse
matrix, corresponding to the factor component and the idiosyncratic component, respectively.

Focusing on the convex relaxation approach, [CSPW11, CLMW11] started by considering the noiseless
case with no missing data (i.e. E = 0 and p = 1) and demonstrated that, under mild conditions, convex
relaxation succeeds in exactly decomposing both L(cid:63) and S(cid:63) from the data matrix L(cid:63) +S(cid:63). More speciﬁcally,
[CSPW11] adopted a deterministic model without assuming any probabilistic structure on the outlier matrix
S(cid:63). As shown in [CSPW11] and several subsequent work [CJSC13, HKZ11], convex relaxation is guaranteed
to work as long as the fraction of outliers in each row/column does not exceed O(1/r). In contrast, [CLMW11]
proposed a random model by assuming that S(cid:63) has random support (cf. Assumption 3); under this model,
exact recovery is guaranteed even if a constant fraction of the entries of S(cid:63) are nonzero with arbitrary
magnitudes. Following the random location model proposed in [CLMW11], the paper [GWL+10] showed
that, in the absence of noise, convex programming can provably tolerate a dominant fraction of outliers,
provided that the signs of the nonzero entries of S(cid:63) are randomly generated (cf. Assumption 4). Later, the
papers [CJSC13, Li13] extended these results to the case when most entries of the matrix are unseen; even
in the presence of highly incomplete data, convex relaxation still succeeds when a constant proportion of the
observed entries are arbitrarily corrupted. It is worth noting that the results of [CJSC13] accommodated
both models proposed in [CSPW11] and [CLMW11], while the results of [Li13] focused on the latter model.
The literature on robust PCA with not only sparse outliers but also dense noise — namely, when the
measurements take the form M = PΩobs(L(cid:63) + S(cid:63) + E) — is relatively scarce. [ZLW+10, ANW12] were
among the ﬁrst to present a general theory for robust PCA with dense noise, which was further extended in
[WL17, KLT17]. As we mentioned before, the ﬁrst three [ZLW+10, ANW12, WL17] accommodated arbitrary
noise with the last one [KLT17] focusing on the random noise. As we have discussed in Section 1.4, the
statistical guarantees provided in these papers are highly suboptimal when it comes to the random noise
setting considered herein. The paper [CC14] extended the robust PCA results to the case where the truth
is not only low-rank but also of Hankel structure. The results therein, however, suﬀered from the same
sub-optimality issue.

Moving beyond convex relaxation methods, another line of work proposed nonconvex approaches for
robust PCA [NNS+14,GWL16,YPCC16,CGJ17,ZWG18,CCD+19,LMCC19,CCW19], largely motivated by
the recent success of nonconvex methods in low-rank matrix factorization [CLC19, KMO10, CLS15, SL16,
CC17, CW15, ZCL16, CC18, JNS13, NJS13, MWCC20, CCFM19, WGE17, WCCL16, CW18, ZL16, CDDD19].
Following the deterministic model of [CSPW11], the paper [NNS+14] proposed an alternating projec-
tion / minimization scheme to seek a low-rank and sparse decomposition of the observed data matrix. In the

13

noiseless setting, i.e. E = 0, this alternating minimization scheme provably disentangles the low-rank and
sparse matrix from their superposition under mild conditions. In addition, [NNS+14] extended their result to
the arbitrary noise case where the size of the noise is extremely small, namely, (cid:107)E(cid:107)∞ (cid:28) σmin/n. When the
log n). Comparing this with our
noise {Eij} ∼ N (0, σ2), this is equivalent to the condition σ (cid:28) σmin/(n
noise condition σ (cid:28) σmin/(
n log n) (cf. (1.12)) when r, µ, κ (cid:16) 1, one sees that our theoretical guarantees
cover a wider range of noise levels. Similarly, [YPCC16] applied regularized gradient descent on a smooth
nonconvex loss function which enjoys provable convergence guarantees to (L(cid:63), S(cid:63)) under the noiseless and
partial observation setting. A recent paper [CCD+19] considered the nonsmooth nonconvex formulation for
robust PCA and established rigorously the convergence of subgradient-type methods in the rank-1 setting,
i.e. r = 1. However, the extension to more general rank remains out of reach.

√

√

It is worth noting that noisy matrix completion problem [CP10, CCF+20] is subsumed as a special case
by the model studied in this paper (namely, it is a special case with S(cid:63) = 0). Statistical optimality under
the random noise setting (cf. Assumption 5) — including the convex relaxation approach [CCF+20, NW12,
KLT11, Klo14] and the nonconvex approach [MWCC20, CLL20] — has been extensively studied. Focusing
on arbitrary deterministic noise, [CP10] established the stability of the convex approach, whose resulting
estimation error bound is similar to the one established for robust PCA with noise in [ZLW+10]) (see (1.4)).
The paper [KS20] later conﬁrmed that the estimation error bound established in [CP10] is the best one can
hope for in the arbitrary noise setting for matrix completion, although it might be highly suboptimal if we
restrict attention to random noise.

Finally, there is also a large literature considering robust PCA under diﬀerent settings and/or from
diﬀerent perspectives. For instance, the computational eﬃciency in solving the convex optimization prob-
lem (1.3) and its variants has been studied in the optimization literature (e.g. [TY11,GMS13,SWZ14,MA18]).
The problem has also been investigated under a streaming / online setting [GQV14, QV10, FXY13, ZLGV16,
QVLH14, VN18]. These are beyond the scope of the current paper.

3 Architecture of the proof

In this section, we give an outline for proving Theorem 2. The proof of Theorem 1 follows immediately as it is
a special case of Theorem 2. For simplicity of presentation, our proof sets n1 = n2 = n. It is straightforward
to obtain the proof for the general rectangular case via minor modiﬁcation.

The main ingredient of the proof lies in establishing an intimate link between convex and nonconvex

optimization. Unless otherwise noted, we shall set the regularization parameters as

throughout. In addition, the soft thresholding operator at level τ is deﬁned such that

λ = Cλσ

√

np

and

τ = Cτ σ(cid:112)log n

Sτ (x) := sign (x) max (|x| − τ, 0)

(3.1)

(3.2)

For any matrix X, the matrix Sτ (X) is obtained by applying the soft thresholding operator Sτ (·) to each
entry of X separately. Additionally, we deﬁne the true low-rank factors as follows

where U (cid:63)Σ(cid:63)V (cid:63)(cid:62) is the SVD of the true low-rank matrix L(cid:63).

X (cid:63) := U (cid:63) (Σ(cid:63))1/2

and

Y (cid:63) := V (cid:63) (Σ(cid:63))1/2 ,

(3.3)

3.1 Crude estimation error bounds for convex relaxation

We start by delivering a crude upper bound on the Euclidean estimation error, built upon the (approximate)
duality certiﬁcate previously constructed in [CJSC13]. The proof is postponed to Appendix D.

Theorem 3. Consider any given λ > 0 and set τ (cid:16) λ(cid:112)(log n)/np. Suppose that Assumptions 1-4 hold, and
that

n2p ≥ Cµ2r2n log6 n

and

ρs ≤ c

14

hold for some suﬃciently large (resp. small) constant C > 0 (resp. c > 0). Then with probability at least
1 − O(n−10), any minimizer (Lcvx, Scvx) of the convex program (1.3) satisﬁes

(cid:107)Lcvx − L(cid:63)(cid:107)2

F + (cid:107)Scvx − S(cid:63)(cid:107)2

F

(cid:46) λ2n5 log3 n +

n
λ2

(cid:13)PΩobs (E) (cid:13)
(cid:13)
4
F.
(cid:13)

(3.4)

It is worth noting that the above theorem holds true for an arbitrary noise matrix E. When specialized
to the case with independent sub-Gaussian noise, this crude bound admits a simpler expression as follows.

Corollary 1. Take λ = Cλσ
assumptions of Theorem 3 and Assumption 5, we have — with probability exceeding 1 − O(n−10) — that

log n for some universal constant Cλ, Cτ > 0. Under the

np and τ = Cτ σ

√

√

(cid:107)Lcvx − L(cid:63)(cid:107)F

(cid:46) σn3 log3/2 n and

(cid:107)Scvx − S(cid:63)(cid:107)F

(cid:46) σn3 log3/2 n.

(3.5)

Proof. This corollary follows immediately by combining Theorem 3 and Lemma 1 below.

Lemma 1. Suppose that Assumption 5 holds and that n2p > C1n log2 n for some suﬃciently large constant
C1 > 0. Then with probability exceeding 1 − O(n−10), one has
(cid:13)PΩobs (E) (cid:13)
(cid:13)
(cid:13)PΩobs (E) (cid:13)
(cid:13)
(cid:13)F

(cid:13) (cid:46) σ

(cid:46) σn

and

np

√

√

p.

While the above results often lose a polynomial factor in n vis-à-vis the optimal error bound, it serves as

an important starting point that paves the way for subsequent analytical reﬁnement.

3.2 Approximate stationary points of the nonconvex formulation

Instead of analyzing the convex estimator directly, we take a detour by considering the following nonconvex
optimization problem

minimize
X,Y ∈Rn×r, S∈Rn×n

F (X, Y , S) :=

(cid:13)
(cid:13)PΩobs

1
2p
(cid:124)

(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
F +
(cid:13)
(cid:123)(cid:122)
=: f (X,Y ;S)

λ
2p

(cid:107)X(cid:107)2

F +

λ
2p

(cid:107)Y (cid:107)2
F
(cid:125)

+

τ
p

(cid:107)S(cid:107)1 . (3.6)

Here, f (X, Y ; S) is a function of X and Y with S frozen, which contains the smooth component of the
loss function F (X, Y , S). As it turns out, the solution to convex relaxation (1.3) is exceedingly close to an
estimate (X, Y , S) obtained by a nonconvex algorithm aimed at solving (3.6) — to be detailed in Section 3.3.
This fundamental connection between the two algorithmic paradigms provides a powerful framework that
allows us to understand convex relaxation by studying nonconvex optimization.

In what follows, we set out to develop the aﬀorementioned intimate connection. Before proceeding,
we ﬁrst state the following conditions concerned with the interplay between the noise size, the estimation
accuracy of the nonconvex estimate (X, Y , S), and the regularization parameters.
Condition 1. The regularization parameters λ and τ (cid:16) λ(cid:112)(log n)/np satisfy
• (cid:107)PΩobs (E)(cid:107) < λ/16 and (cid:107)PΩobs(E)(cid:107)∞ ≤ τ /4;
• (cid:107)S − S(cid:63)(cid:107) < λ/16 and (cid:107)XY (cid:62) − L(cid:63)(cid:107)∞ ≤ τ /4;
• (cid:107)PΩobs (XY (cid:62) − L(cid:63)) − p(XY (cid:62) − L(cid:63))(cid:107) < λ/8.

As an interpretation, the above condition says that: (1) the regularization parameters are not too small
compared to the size of the noise, so as to ensure that we enforce a suﬃciently large degree of regularization;
(2) the estimate represented by the point (XY (cid:62), S) is suﬃciently close to the truth. At this point, whether
this condition is meaningful or not remains far from clear; we shall return to justify its feasibility shortly.

In addition, we need another condition concerning the injectivity of PΩ(cid:63) w.r.t. a certain tangent space.
For a rank-r matrix L with singular value decomposition U ΣV (cid:62) where U , V ∈ Rn×r, the tangent space of
the set of rank-r matrices at the point L is given by

Again, the validity of this condition will be discussed momentarily.

(cid:8)U A(cid:62) + BV (cid:62) | A, B ∈ Rn×r(cid:9) .

15

Condition 2 (Injectivity). Let T be the tangent space of the set of rank-r matrices at the point XY (cid:62).
Assume that there exist a constants cinj > 0 such that for all H ∈ T , one has
cinj
κ

p−1 (cid:107)PΩobs (H)(cid:107)2

p−1 (cid:107)PΩ(cid:63) (H)(cid:107)2

(cid:107)H(cid:107)2
F .

(cid:107)H(cid:107)2
F

cinj
4κ

F ≥

F ≤

and

With the above conditions in place, we are ready to make precise the intimate link between convex

relaxation and a candidate nonconvex solution. The proof is deferred to Appendix E.

Theorem 4. Suppose that n ≥ κ and ρs ≤ c/κ for some suﬃciently small constant c > 0. Assume that
there exists a triple (X, Y , S) such that

(cid:107)∇f (X, Y ; S)(cid:107)F ≤

√

1
n20

λ
p

σmin,

and S = PΩobs

(cid:0)Sτ

(cid:0)M − XY (cid:62)(cid:1)(cid:1) .

(3.7)

Further, assume that any singular value of X and Y lies in [(cid:112)σmin/2,
to the convex program (1.3) admits the following crude error bound

√

2σmax]. If the solution (Lcvx, Scvx)

(cid:107)Lcvx − L(cid:63)(cid:107)F

(cid:46) σn4,

(3.8)

then under Conditions 1-2 we have

(cid:13)
(cid:13)XY (cid:62) − Lcvx

(cid:13)
(cid:13)F

(cid:46) σ
n5

and

(cid:107)S − Scvx(cid:107)F

(cid:46) σ
n5 .

This theorem is a deterministic result, focusing on some sort of “approximate stationary points” of
F (X, Y , S). To interpret this, observe that in view of (3.7), one has ∇f (X, Y ; S) ≈ 0, and S minimizes
F (X, Y , ·) for any ﬁxed X and Y . If one can identify such an approximate stationary point that is suﬃciently
close to the truth (so that it satisﬁes Condition 1), then under mild conditions our theory asserts that

XY (cid:62) ≈ Lcvx

and

S ≈ Scvx.

This would in turn formalize the intimate relation between the solution to convex relaxation and an approx-
imate stationary point of the nonconvex formulation. The existence of such approximate stationary points
will be veriﬁed shortly in Section 3.3.

The careful reader might immediately remark that this theorem does not say anything explicit about the
minimizer of the nonconvex optimization problem (3.6); rather, it only pays attention to a special class of
approximate stationary points of the nonconvex formulation. This arises mainly due to a technical consider-
ation: it seems more diﬃcult to analyze the nonconvex optimizer directly than to study certain approximate
stationary points. Fortunately, our theorem indicates that any approximate stationary point obeying the
above conditions serves as an extremely tight approximation of the convex estimate, and, therefore, it suﬃces
to identify and analyze any such points.

3.3 Constructing an approximate stationary point via nonconvex algorithms

By virtue of Theorem 4, the key to understanding convex relaxation is to construct an approximate stationary
point of the nonconvex problem (3.6) that enjoys desired statistical properties. For this purpose, we resort
to the following iterative algorithm (Algorithm 1) to solve the nonconvex program (3.6).

In a nutshell, Algorithm 1 alternates between one iteration of gradient updates (w.r.t. the decision
matrices X and Y ) and optimization of the non-smooth problem w.r.t. S (with X and Y frozen).8 For
the sake of simplicity, we initialize this algorithm from the ground truth (X (cid:63), Y (cid:63), S(cid:63)), but our analysis
framework might be extended to accommodate other more practical initialization (e.g. the one obtained by
a spectral method [CCFM20]).

The following theorem makes precise the statistical guarantees of the above nonconvex optimization

algorithm; the proof is deferred to Appendix F. Here and throughout, we deﬁne

H t := arg min
R∈Or×r

(cid:0)(cid:107)X tR − X (cid:63)(cid:107)2

F + (cid:107)Y tR − Y (cid:63)(cid:107)2
F

(cid:1)1/2

,

(3.10)

where Or×r denotes the set of r × r orthonormal matrices.

8Note that for any given X and Y , the solution to minimizeS F (X, Y , S) is given precisely by Sτ (PΩobs (M − XY (cid:62))).

16

Algorithm 1 Alternating minimization method for solving the nonconvex problem (3.6).

Suitable initialization: X 0 = X (cid:63), Y 0 = Y (cid:63), S0 = S(cid:63).
Gradient updates: for t = 0, 1, . . . , t0 − 1 do

X t+1 =X t − η∇X f (cid:0)X t, Y t; St(cid:1) = X t −

Y t+1 =Y t − η∇Y f (cid:0)X t, Y t; St(cid:1) = Y t −

St+1 =Sτ

(cid:2)PΩobs

(cid:0)M − X t+1Y t+1(cid:62)(cid:1)(cid:3) .

η
p
η
p

(cid:2)PΩobs
(cid:110)(cid:2)PΩobs

(cid:0)X tY t(cid:62) + St − M (cid:1) Y t + λX t(cid:3) ;

(cid:0)X tY t(cid:62) + St − M (cid:1)(cid:3)(cid:62)

X t + λY t(cid:111)

;

(3.9a)

(3.9b)

(3.9c)

Theorem 5. Instate the assumptions of Theorem 2 and deﬁne

δn :=

σ
σmin

(cid:114) n
p

.

Take t0 = n47 and η (cid:16) 1/(nκ3σmax) in Algorithm 1. With probability at least 1 − O(n−3), the iterates
{(X t, Y t, St)}0≤t≤t0 of Algorithm 1 satisfy

max (cid:8)(cid:13)

(cid:13)X tH t − X (cid:63)(cid:13)

(cid:13)Y tH t − Y (cid:63)(cid:13)
(cid:13)F , (cid:13)
(cid:9) (cid:46) δn (cid:107)X (cid:63)(cid:107)F ,
(cid:13)F
(cid:13)Y tH t − Y (cid:63)(cid:13)
(cid:13) , (cid:13)
(cid:13)X tH t − X (cid:63)(cid:13)
(cid:9) (cid:46) δn (cid:107)X (cid:63)(cid:107) ,
(cid:13)
(cid:111)
(cid:13)Y tH t − Y (cid:63)(cid:13)
(cid:13)2,∞ , (cid:13)

max (cid:8)(cid:13)
(cid:110)(cid:13)
(cid:13)X tH t − X (cid:63)(cid:13)

(cid:46) κ(cid:112)log n δn max

(cid:13)2,∞
(cid:13)St − S(cid:63)(cid:13)
(cid:13)

(cid:13) (cid:46) σ

√

np.

max

(cid:110)
(cid:107)X (cid:63)(cid:107)2,∞ , (cid:107)Y (cid:63)(cid:107)2,∞

(cid:111)

,

In addition, with probability at least 1 − O(n−3), one has

min
0≤t<t0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)

(cid:13)F ≤

√

1
n20

λ
p

σmin.

(3.11a)

(3.11b)

(3.11c)

(3.11d)

(3.12)

In short, the bounds (3.11a)-(3.11c) reveal that the entire sequence {X t, Y t}t0

stays suﬃciently close to
the truth (measured by (cid:107) · (cid:107)F, (cid:107) · (cid:107), and more importantly, (cid:107) · (cid:107)2,∞), the inequality (3.11d) demonstrates the
goodness of ﬁt of {St}0≤t≤t0
in terms of the spectral norm accuracy, whereas the last bound (3.12) indicates
that there is at least one point in the sequence {X t, Y t, St}0≤t≤t0
that can serve as an approximate stationary
point of the nonconvex formulation.

t=0

We shall also gather a few immediate consequences of Theorem 5 as follows, which contain basic properties

that will be useful throughout.

Corollary 2. Instate the assumptions of Theorem 5. Suppose that the sample size obeys n2p (cid:29) κ4µ2r2n log4 n,
the noise satisﬁes δn (cid:28) 1/(cid:112)κ4µr log n, the outlier fraction satisﬁes ρs (cid:28) 1/(κ3µr log n). With probability
at least 1 − O(n−3), the iterates of Algorithm 1 satisfy

(cid:13)
(cid:13)X tY t(cid:62) − L(cid:63)(cid:13)
(cid:13)F
(cid:13)
(cid:13)X tY t(cid:62) − L(cid:63)(cid:13)
(cid:13)∞
(cid:13)
(cid:13)X tY t(cid:62) − L(cid:63)(cid:13)

(cid:46) κδn (cid:107)L(cid:63)(cid:107)F ,
(cid:46) (cid:112)

κ3µr log n δn (cid:107)L(cid:63)(cid:107)∞ ,

(cid:13) (cid:46) δn (cid:107)L(cid:63)(cid:107)

(3.13a)

(3.13b)

(3.13c)

simultaneously for all t ≤ t0.

Proof. See [CCF+20, Appendix D.12].

3.4 Proof of Theorem 2

Deﬁne

t∗ := arg min
0≤t<t0

(cid:107)∇f (X t, Y t; St)(cid:107)F;

(3.14)

17

(cid:0)Xncvx, Yncvx, Sncvx

(cid:1) := (cid:0)X t∗ H t∗ , Y t∗ H t∗ , St∗ (cid:1).

(3.15)

Theorem 5 and Corollary 2 have established appealing statistical performance of the nonconvex solution
(Xncvx, Yncvx, Sncvx). To transfer this desired statistical property to that of (Lcvx, Scvx), it remains to show
that the nonconvex estimator (cid:0)XncvxY (cid:62)
(cid:1) is extremely close to the convex estimator (Lcvx, Scvx).
Towards this end, we intend to invoke Theorem 4; therefore, it boils down to verifying the conditions
therein.

ncvx, Sncvx

1. The small gradient condition (cf. (3.7)) holds automatically under (3.12).

2. By virtue of the spectral norm bound (3.11b), one has

(cid:107)Xncvx − X (cid:63)(cid:107) = (cid:13)

(cid:13)X t∗ H t∗ − X (cid:63)(cid:13)

(cid:13) (cid:46) σ
σmin

(cid:114) n
p

(cid:107)L(cid:63)(cid:107) ≤

√

σmin
10

,

as long as σ(cid:112)κn/p (cid:28) σmin. This together with the Weyl inequality veriﬁes the constraints on the
singular values of (Xncvx, Yncvx).

3. The crude error bounds are valid in view of Theorem 3.

4. Regarding Condition 1 and Condition 2, Lemma 1 and standard inequalities about sub-Gaussian random
variables imply that (cid:107)PΩobs(E)(cid:107) < λ/16 and (cid:107)PΩobs(E)(cid:107)∞ ≤ τ /4.
In addition, the bounds (3.11d)
and (3.13b) ensure the second assumption (cid:107)Sncvx −S(cid:63)(cid:107) ≤ λ/16 and (cid:107)XY (cid:62) −L(cid:63)(cid:107)∞ ≤ τ /4 in Condition 1.
We are left with the last assumption in Condition 1 and Condition 2, which are guaranteed to hold in
view of the following lemma (see Appendix C for the proof).

Lemma 2. Instate the notations and assumptions of Theorem 2. Then with probability exceeding 1 −
O(n−10), we have

(cid:13)
(cid:13)PΩ

(cid:0)XY (cid:62) − M (cid:63)(cid:1) − p(cid:0)XY (cid:62) − M (cid:63)(cid:1)(cid:13)
1
p
p−1 (cid:107)PΩ(cid:63) (H)(cid:107)2

(cid:107)PΩobs (H)(cid:107)2

(cid:107)H(cid:107)2
F ,

(cid:107)H(cid:107)2
F ,

1
32κ
1
128κ

F ≥

F ≤

(cid:13) < λ/8,

∀H ∈ T,

∀H ∈ T

simultaneously for all (X, Y ) obeying

(cid:107)X − X (cid:63)(cid:107)2,∞ ≤ C∞κ

(cid:107)Y − Y (cid:63)(cid:107)2,∞ ≤ C∞κ

(cid:115)

(cid:115)

σ
σmin

σ
σmin

n log n
p

n log n
p

max

(cid:110)
(cid:107)X (cid:63)(cid:107)2,∞ , (cid:107)Y (cid:63)(cid:107)2,∞

(cid:111)

max

(cid:110)
(cid:107)X (cid:63)(cid:107)2,∞ , (cid:107)Y (cid:63)(cid:107)2,∞

(cid:111)

;

.

(3.16a)

(3.16b)

(3.16c)

(3.17a)

(3.17b)

Here, T denotes the tangent space of the set of rank-r matrices at the point XY (cid:62), and C∞ > 0 is an
absolute constant.

Armed with the above conditions, we can readily invoke Theorem 4 to reach

(cid:13)
(cid:13)XncvxY (cid:62)

ncvx − Lcvx

(cid:13)
(cid:13)F

(cid:46) σ
n5

and

(cid:107)Sncvx − Scvx(cid:107)F

(cid:46) σ
n5

with high probability. This taken collectively with Corollary 2 gives

(cid:107)Lcvx − L(cid:63)(cid:107)F ≤ (cid:13)
(cid:46) σ

(cid:13)F + (cid:13)
(cid:13)
(cid:107)L(cid:63)(cid:107)F

(cid:13)XncvxY (cid:62)
ncvx − Lcvx
(cid:114) n
σ
σmin
p
(cid:114) n
p

n5 + κ
σ
σmin

(cid:107)L(cid:63)(cid:107)F .

(cid:16) κ

(cid:13)XncvxY (cid:62)

ncvx − L(cid:63)(cid:13)
(cid:13)F

18

Similar arguments lead to the advertised high-probability bounds

(cid:107)Lcvx − L(cid:63)(cid:107)∞

(cid:46) (cid:112)

κ3µr

(cid:115)

σ
σmin

n log n
p

(cid:107)L(cid:63)(cid:107)∞ ,

(cid:107)Lcvx − L(cid:63)(cid:107) (cid:46) σ
σmin

(cid:114) n
p

(cid:107)L(cid:63)(cid:107) .

Finally, given that XncvxY (cid:62)
ncvx
Lcvx(cid:107)F of Lcvx necessarily satisﬁes

is a rank-r matrix, the rank-r approximation Lcvx,r := arg minZ:rank(Z)≤r (cid:107)Z−

(cid:107)Lcvx,r − Lcvx(cid:107)F ≤ (cid:13)

(cid:13)XncvxY (cid:62)

ncvx − Lcvx

(cid:13)
(cid:13)F

(cid:46) σ

n5 ≤

1
n5 ·

σ
σmin

(cid:114) n
p

(cid:107)L(cid:63)(cid:107) ,

which establishes (1.22). In view of the triangle inequality, the properties (1.21) hold unchanged if Lcvx is
replaced by Lcvx,r.

4 Discussion

This paper investigates the unreasonable eﬀectiveness of convex programming in estimating an unknown
low-rank matrix from grossly corrupted data. We develop an improved theory that conﬁrms the optimality
of convex relaxation in the presence of random noise, gross sparse outliers, and missing data. In particular,
our results signiﬁcantly improve upon the prior statistical guarantees [ZLW+10] under random noise, while
further allowing for missing data. Our theoretical analysis is built upon an appealing connection between
convex and nonconvex optimization, which has not been established previously.

Having said this, our current work leaves open several important issues that call for further investigation.
To begin with, the conditions (1.20) stated in the main theorem are likely suboptimal in terms of the
dependency on both the rank r and the condition number κ. For example, we shall keep in mind that in the
noise-free setting, the sample size can be as low as O(nrpoly log n) and the tolerable outlier fraction can be as
large as a constant [Li13, CJSC13], both of which exhibit more favorable scalings w.r.t. r and κ compared to
our current condition (1.20). Moving forward, our analysis ideas suggest a possible route for analyzing convex
relaxation for other structured estimation problems under both random noise and outliers, including but not
limited to sparse PCA (the case with a simultaneously low-rank and sparse matrix) [CMW13], low-rank
Hankel matrix estimation (the case involving a low-rank Hankel matrix) [CC14], and blind deconvolution9
(the case that aims to recover a low-rank matrix from structured Fourier measurements) [ARR14]. Last but
not least, we would like to point out that it is possible to design a similar debiasing procedure as in [CFMY19]
for correcting the bias in the convex estimator, which further allows uncertainty quantiﬁcation and statistical
inference on the unknown low-rank matrix of interest.

Acknowledgements

Y. Chen is supported in part by the AFOSR YIP award FA9550-19-1-0030, by the ONR grant N00014-19-
1-2120, by the ARO grants W911NF-20-1-0097 and W911NF-18-1-0303, by the NSF grants CCF-1907661,
IIS-1900140 and DMS-2014279, and by the Princeton SEAS innovation award. J. Fan is supported in part
by the NSF grants DMS-1662139 and DMS-1712591, the ONR grant N00014-19-1-2120, and the NIH grant
2R01-GM072611-14.

A An equivalent probabilistic model of Ω(cid:63) used throughout the

proof

Recall that Ω(cid:63) is the support of the sparse component S∗.
In this section, we introduce an equivalent
probabilistic model of Ω(cid:63), which is more amenable to analysis and shall be assumed throughout the proof.

9Our ongoing work [CFWY20] is pursuing this direction.

19

• The original model. Recall from Assumption 3 the way we generate Ω(cid:63) : (1) sample Ωobs from the
i.i.d. Bernoulli model with parameter p; (2) for each (i, j) ∈ Ωobs, let (i, j) ∈ Ω(cid:63) independently with
probability ρs.

• An equivalent model.

The model involves over-sampling and rejection method: (1) sample Ωobs
from the i.i.d. Bernoulli model with parameter p; (2) generate an augmented index set Ωaug ⊆ Ωobs such
that: for each (i, j) ∈ Ωobs, we generate (i, j) ∈ Ωaug independently with probability ρaug; (3) for any
(i, j) ∈ Ωaug, include (i, j) in Ω(cid:63) independently with probability ρs/ρaug.

It is straightforward to verify that the two models for Ω(cid:63) are equivalent as long as ρs ≤ ρaug ≤ 1. Two
important remarks are in order. First, by construction, we have Ω(cid:63) ⊆ Ωaug. Second, the choice of ρaug can
vary as needed as long as ρs ≤ ρaug ≤ 1.

The introduction of this augmented index set Ωaug comes in handy when we would like to control the size
(cid:107)PΩ(cid:63) (A)(cid:107)F for some matrix A ∈ Rn×n. The ﬁrst inclusion property Ω(cid:63) ⊆ Ωaug allows us to upper bound
(cid:107)PΩ(cid:63) (A)(cid:107)F by (cid:107)PΩaug (A)(cid:107)F, and the freedom to choose ρaug allows us to leverage stronger concentration
results, which might not hold for the smaller ρs. See Corollary 3 in the next section for an example.

B Preliminaries

B.1 A few preliminary facts

This subsection collects several results that are useful throughout the proof. To begin with, the incoherence
assumption (cf. Assumption 1) asserts that

(cid:107)X (cid:63)(cid:107)2,∞ ≤ (cid:112)µr/n (cid:107)X (cid:63)(cid:107)

and

(cid:107)Y (cid:63)(cid:107)2,∞ ≤ (cid:112)µr/n (cid:107)Y (cid:63)(cid:107) .

(B.1)

This is because

(cid:107)X (cid:63)(cid:107)2,∞ = (cid:13)

(cid:13)U (cid:63)(Σ(cid:63))1/2(cid:13)

(cid:13)2,∞ ≤ (cid:107)U (cid:63)(cid:107)2,∞

(cid:13)(Σ(cid:63))1/2(cid:13)
(cid:13)

(cid:13) ≤ (cid:112)µr/n (cid:107)X (cid:63)(cid:107) ,

where the ﬁrst inequality comes from the elementary inequality (cid:107)AB(cid:107)2,∞ ≤ (cid:107)A(cid:107)2,∞(cid:107)B(cid:107), and the last
inequality is a consequence of the incoherence assumption as well as the fact that (cid:107)(Σ(cid:63))1/2(cid:107) = (cid:107)X (cid:63)(cid:107).

The next lemma is extensively used in the low-rank matrix completion literature.

Lemma 3. Suppose that each (i, j) is included in Ω0 ⊆ [n] × [n] independently with probability ρ0. Then
with probability exceeding 1 − O(n−10), one has

(cid:13)
(cid:13)PT (cid:63) − ρ−1

0 PT (cid:63) PΩ0PT (cid:63)

(cid:13)
(cid:13) ≤

1
2

,

(B.2)

provided that n2ρ0 (cid:29) µrn log n. Here, T (cid:63) denotes the tangent space of the set of rank-r matrices at the point
L(cid:63) = X (cid:63)Y (cid:63)(cid:62).

Proof. See [CR09, Theorem 4.1]

In fact, the bound (B.2) uncovers certain near-isometry of the operator ρ−1

0 PΩ0(·) when restricted to the

tangent space T (cid:63). This property is formalized in the following fact.

Fact 1. Suppose that (cid:107)PT (cid:63) − ρ−1

0 PT (cid:63) PΩ0 PT (cid:63) (cid:107) ≤ 1/2. Then one has

1
2

(cid:107)H(cid:107)2

F ≤

1
ρ0

(cid:107)PΩ0 (H)(cid:107)2

F ≤

3
2

(cid:107)H(cid:107)2
F ,

for all H ∈ T (cid:63).

Proof. The proof has actually been documented in the literature. For completeness, we present the proof for
the lower bound here; the upper bound follows from a very similar argument. For any H ∈ Rn×n, one has

(cid:107)PΩ0 PT (cid:63) (H)(cid:107)2

F = (cid:104)PΩ0PT (cid:63) (H) , PΩ0PT (cid:63) (H)(cid:105) = (cid:104)PT (cid:63) (H) , PT (cid:63) PΩ0PT (cid:63) (H)(cid:105)

= ρ0 (cid:107)PT (cid:63) (H)(cid:107)2

F − ρ0

(cid:10)PT (cid:63) (H) , (cid:0)PT (cid:63) − ρ−1

0 PT (cid:63) PΩ0PT (cid:63)

(cid:1) (PT (cid:63) (H))(cid:11)

20

≥ ρ0 (cid:107)PT (cid:63) (H)(cid:107)2
(cid:107)PT (cid:63) (H)(cid:107)2
F .

≥

ρ0
2

F − ρ0

(cid:13)
(cid:13)PT (cid:63) − ρ−1

0 PT (cid:63) PΩ0PT (cid:63)

(cid:13)
(cid:13) (cid:107)PT (cid:63) (H)(cid:107)2

F

Here, the penultimate inequality relies on the elementary fact that (cid:104)A, B(cid:105) ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F, and the last step
follows from the assumption (cid:107)PT (cid:63) − ρ−1

0 PT (cid:63) PΩ0 PT (cid:63) (cid:107) ≤ 1/2.

The following corollary is an immediate consequence of Lemma 3 and Fact 1.

Corollary 3. Suppose that ρs ≤ ρaug ≤ 1/12 and that n2pρaug (cid:29) µrn log n. Then with probability at least
1 − O(n−10), we have

(cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2 ≤ p/8.
Proof. Recall the auxiliary index set Ωaug introduced in Appendix A. Since Ω(cid:63) ⊆ Ωaug, we have for any
H ∈ Rn×n

(cid:107)PΩ(cid:63) PT (cid:63) (H)(cid:107)2

(cid:13)PΩaug PT (cid:63) (H)(cid:13)
2
F ≤
(cid:13)
Here, the second inequality arises from Lemma 3 and Fact 1 (by taking Ω0 = Ωaug and ρ0 = pρaug). The
proof is complete by recognizing the assumption ρaug ≤ 1/12.

(cid:107)PT (cid:63) (H)(cid:107)2

(cid:107)H(cid:107)2
F .

F ≤ (cid:13)

F ≤

3pρaug
2

3pρaug
2

As it turns out, the near-isometry property of ρ−1

0 PΩ0 (·) can be strengthened to a uniform version

(uniform over a large collection of tangent spaces), as shown in the lemma below.

Lemma 4. Suppose that each (i, j) is included in Ω0 ⊆ [n] × [n] independently with probability ρ0, and that
n2ρ0 (cid:29) µrn log n. Then with probability at least 1 − O(n−10),

1
32κ

(cid:107)H(cid:107)2

F ≤

1
ρ0

(cid:107)PΩ0 (H)(cid:107)2

F ≤ 40κ (cid:107)H(cid:107)2
F ,

for all H ∈ T

holds simultaneously for all (X, Y ) obeying

max

(cid:110)
(cid:107)X − X (cid:63)(cid:107)2,∞ , (cid:107)Y − Y (cid:63)(cid:107)2,∞

(cid:111)

≤

c
√

κ

n

(cid:107)X (cid:63)(cid:107) .

Here, c > 0 is some suﬃciently small constant, and T denotes the tangent space of the set of rank-r matrices
at the point XY (cid:62).

Proof. See Appendix B.2.

In the end, we recall a useful lemma which relates the operator norm to the (cid:96)2,∞ norm of a matrix.

Lemma 5. Suppose that each (i, j) is included in Ω0 ⊆ [n] × [n] independently with probability ρ0, and
that n2ρ0 (cid:29) n log n. Then there exists some absolute constant C > 0 such that with probability at least
1 − O(n−10),

(cid:13)
(cid:13)PΩ0

(cid:0)AB(cid:62)(cid:1) − ρ0AB(cid:62)(cid:13)

(cid:13) ≤ C

√

nρ0 (cid:107)A(cid:107)2,∞ (cid:107)B(cid:107)2,∞

holds simultaneously for all A and B.

Proof. See [CLL20, Lemmas 4.2 and 4.3].

B.2 Proof of Lemma 4

The lower bound has been established in [CCF+20, Lemma 7], and hence we focus on the upper bound. We
start by expressing H ∈ T as H = XA(cid:62) + BY (cid:62), where A, B ∈ Rn×r are chosen to be

(A, B) :=

arg min
( ˜A, ˜B): H=X ˜A(cid:62)+ ˜BY (cid:62)

The optimality condition of (A, B) requires

(cid:110)

(cid:107) ˜A(cid:107)2

F/2 + (cid:107) ˜B(cid:107)2

F/2

(cid:111)
.

see [CCF+20, Section C.3.1] for the justiﬁcation of this identity. The proof then consists of two steps:

X (cid:62)B = A(cid:62)Y ;

(B.3)

21

1. Showing that (cid:107)H(cid:107)2
F

is bounded from below, namely,

(cid:107)H(cid:107)2

F ≥

(cid:16)

σmin

49
100

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

.

To see this, we can invoke the bound on α2 stated in [CCF+20, Appendix C.3.1] to yield

(cid:107)H(cid:107)2

(cid:13)XA(cid:62) + BY (cid:62)(cid:13)
F = (cid:13)
2
F ≥
(cid:13)
(cid:16)
1
2

F + (cid:107)B(cid:107)2

(cid:107)A(cid:107)2

σmin

≥

F

1
2
(cid:17)

(cid:16)(cid:13)
(cid:13)X (cid:63)A(cid:62)(cid:13)
2
F + (cid:107)BY (cid:63)(cid:107)2
(cid:13)
(cid:16)
1
100

(cid:107)A(cid:107)2

σmin

−

F

F + (cid:107)B(cid:107)2

F

(cid:17)

−

1
100

(cid:16)

σmin

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

(cid:17)

≥

49
100

(cid:16)

σmin

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

.

2. Showing that (cid:107)PΩ(cid:63) (H)(cid:107)2
F

is bounded from above, namely,

1
2ρ0

(cid:107)PΩ(cid:63) (H)(cid:107)2

F ≤ 9σmax

(cid:16)

To this end, one starts with the following decomposition

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

.

1
2ρ0

(cid:107)PΩ0 (H)(cid:107)2

F =

1
2

(cid:107)H(cid:107)2

F +

1
2ρ0

(cid:107)PΩ0 (H)(cid:107)2

F −

1
2

(cid:107)H(cid:107)2
F .

(B.4)

Apply [CCF+20, Equation (83)] to obtain

(cid:13)XA(cid:62) + BY (cid:62)(cid:13)
(cid:13)
2
F ≤ 8σmax
(cid:13)

(cid:16)

1
2

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

.

In addition, the bound on α1 stated in [CCF+20, Appendix C.3.1] tells us that

1
2ρ0

(cid:107)PΩ0 (H)(cid:107)2

F −

1
2

(cid:107)H(cid:107)2

F =

≤

1
2

1
2ρ0
1
32
(cid:18) 1
32

(cid:13)
(cid:13)PΩ0

(cid:0)XA(cid:62) + BY (cid:62)(cid:1)(cid:13)
2
F −
(cid:13)
(cid:17)

(cid:16)(cid:13)
(cid:13)X (cid:63)A(cid:62)(cid:13)
2
F + (cid:107)BY (cid:63)(cid:107)2
(cid:13)
(cid:19) (cid:16)
1
25

σmax +

σmin

(cid:107)A(cid:107)2

(cid:13)XA(cid:62) + BY (cid:62)(cid:13)
(cid:13)
2
(cid:13)
(cid:16)
1
(cid:107)A(cid:107)2
25
F + (cid:107)B(cid:107)2

σmin

+

(cid:17)

F

.

F

F

≤

F + (cid:107)B(cid:107)2

F

(cid:17)

Substitution into (B.4) gives

1
2ρ0

(cid:107)PΩ0 (H)(cid:107)2

F ≤ 8σmax

≤ 9σmax

(cid:16)

(cid:16)

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

σmax +

(cid:19) (cid:16)

1
25

σmin

(cid:18) 1
32

(cid:17)

(cid:17)

+

.

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

Putting the above two bounds together, we conclude that

(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

≤

900
49

κ ·

49
100

σmin

1
2ρ0

(cid:107)PΩ0 (H)(cid:107)2

F ≤ 9σmax

(cid:16)

as claimed.

C Proof of Lemma 2

(cid:16)
(cid:107)A(cid:107)2

F + (cid:107)B(cid:107)2

F

(cid:17)

≤ 20κ (cid:107)H(cid:107)2
F

With Lemma 4 in place, we can immediately justify Lemma 2.

To begin with, the ﬁrst two parts (3.16a) and (3.16b) are the same as [CCF+20, Lemma 4]. Hence, it
suﬃces to verify the last one (3.16c). Recall from Appendix A that Ω(cid:63) ⊆ Ωaug, where Ωaug is randomly
sampled such that each (i, j) is included in Ωaug independently with probability pρaug. Applying Lemma 4
on Ωaug ﬁnishes the proof, with the proviso that ρaug (cid:16) 1/κ2 and ρs ≤ ρaug.

22

D Crude error bounds (Proof of Theorem 3)

This section is devoted to establishing our crude statistical error bounds on (cid:107)Lcvx − L(cid:63)(cid:107)F and (cid:107)Scvx − S(cid:63)(cid:107)F.
. The proof works for general choices
Without loss of generality, we only consider the case when τ = λ

(cid:113) log n
np

τ (cid:16) λ

(cid:113) log n
np

with slight modiﬁcation. To simplify the notation hereafter, we denote

ΛL := Lcvx − L(cid:63),
Λ+ := (PΩobs (ΛL) + ΛS)/2,

and

and

ΛS := Scvx − S(cid:63),
Λ− := (PΩobs (ΛL) − ΛS)/2,

which immediately imply

ΛL = Λ+ + Λ− + PΩc

obs

(ΛL) ,

and

ΛS = Λ+ − Λ−.

These in turn allow us to decompose (cid:107)ΛL(cid:107)2
F = (cid:13)
(cid:13)Λ− + PΩc

(cid:13)Λ+ + Λ− + PΩc

(cid:107)ΛL(cid:107)2
= (cid:13)
= 2 (cid:13)

F + (cid:107)ΛS(cid:107)2
F + (cid:13)
(cid:13)Λ+(cid:13)
2
(cid:13)
F + (cid:13)
(cid:13)Λ+(cid:13)
2
(cid:13)

(ΛL)(cid:13)
2
(cid:13)
F
(ΛL)(cid:13)
2
(cid:13)
F

(cid:13)Λ− + PΩc

+ (cid:13)

F + (cid:107)ΛS(cid:107)2
F
(ΛL)(cid:13)
2
(cid:13)
F
+ 2 (cid:10)Λ+, Λ− + PΩc
+ (cid:13)

obs

obs

obs

(cid:13)Λ−(cid:13)
F + 2 (cid:10)Λ+, PΩc
2
(cid:13)

obs

(cid:13)Λ+ − Λ−(cid:13)
2
(cid:13)
F
(ΛL)(cid:11) + (cid:13)

obs

as follows

F + (cid:13)
(cid:13)Λ+(cid:13)
2
(cid:13)

(cid:13)Λ−(cid:13)
F − 2 (cid:10)Λ+, Λ−(cid:11)
2
(cid:13)

(ΛL)(cid:11) .

(D.1)

Since (Lcvx, Scvx) is the minimizer of (1.3), it is self-evident that Scvx must be supported on Ωobs. Then by
construction, ΛS, Λ+ and Λ− are all necessarily supported on Ωobs, thus indicating that

Making use of this relation, we can continue the derivation (D.1) above to obtain

(cid:104)Λ+, PΩc

obs

(ΛL)(cid:105) = 0.

(cid:107)ΛL(cid:107)2
= 2 (cid:13)
(cid:124)

F = 2 (cid:13)
F + (cid:107)ΛS(cid:107)2
(cid:13)Λ+(cid:13)
+ (cid:13)
2
(cid:13)PT (cid:63)
(cid:13)
F
(cid:123)(cid:122)
(cid:125)
(cid:124)
=:α1

F + (cid:13)
(cid:13)Λ+(cid:13)
2
(cid:13)
(cid:0)Λ− + PΩc

(cid:13)Λ− + PΩc
obs
(ΛL)(cid:1)(cid:13)
2
(cid:13)
F
(cid:123)(cid:122)
=:α2

obs

(ΛL)(cid:13)
2
(cid:13)
F
+ (cid:13)
(cid:13)PΩ(cid:63)

(cid:13)Λ−(cid:13)
+ (cid:13)
2
(cid:13)
F
+ (cid:13)
(cid:0)Λ−(cid:1)(cid:13)
2
(cid:13)PT (cid:63)⊥
(cid:13)
F
(cid:125)
(cid:124)

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1)(cid:13)
2
(cid:13)
F
(cid:123)(cid:122)
=:α3

+ (cid:13)

(cid:13)PΩ(cid:63)c

(cid:0)Λ−(cid:1)(cid:13)
2
(cid:13)
F
(cid:125)

.

In the sequel, we shall control the three terms α1, α2 and α3 separately.

Step 1: bounding α1. By deﬁnition, we have

α1 = 2 (cid:13)
1
2

=

(cid:13)Λ+(cid:13)
2
F =
(cid:13)

1
2

(cid:107)PΩobs (ΛL) + ΛS(cid:107)2

F =

1
2

(cid:107)PΩobs (ΛL + ΛS)(cid:107)2

F

(cid:107)PΩobs (Lcvx + Scvx − M + M − L(cid:63) − S(cid:63))(cid:107)2

F

≤ (cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2
= (cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2

F + (cid:107)PΩobs (L(cid:63) + S(cid:63) − M )(cid:107)2
F + (cid:107)PΩobs (E)(cid:107)2
F ,

F

(D.2)

where the third identity holds true since ΛS = PΩobs(ΛS), the penultimate relation is due to the elementary
, and the last line follows since PΩobs(L(cid:63) + S(cid:63) − M ) = PΩobs(E).
inequality (cid:107)A + B(cid:107)2
To upper bound (cid:107)PΩobs(Lcvx + Scvx − M )(cid:107)2
, we leverage the optimality of (Lcvx, Scvx) w.r.t. the convex
F
program (1.3) to obtain

F ≤ 2(cid:107)A(cid:107)2

F + 2(cid:107)B(cid:107)2
F

1
2

(cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2

F + λ (cid:107)Lcvx(cid:107)∗ + τ (cid:107)Scvx(cid:107)1

≤

1
2

(cid:107)PΩobs (L(cid:63) + S(cid:63) − M )(cid:107)2

F + λ (cid:107)L(cid:63)(cid:107)∗ + τ (cid:107)S(cid:63)(cid:107)1 .

(D.3)

Recognizing again that PΩobs (L(cid:63) + S(cid:63) − M ) = PΩobs(E), we can rearrange terms in (D.3) to derive
(cid:13)
(cid:13)1

F + 2λ (cid:107)L(cid:63)(cid:107)∗ + 2τ (cid:107)S(cid:63)(cid:107)1 − 2λ(cid:13)

(cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2

F ≤ (cid:107)PΩobs (E) (cid:107)2

(cid:13)∗ − 2τ (cid:13)
(cid:13)

(cid:13)Lcvx

(cid:13)Scvx

23

(i)
≤ (cid:107)PΩobs (E) (cid:107)2
(ii)
≤ (cid:107)PΩobs (E) (cid:107)2
(iii)
≤ (cid:107)PΩobs (E) (cid:107)2

F + 2λ (cid:107)ΛL(cid:107)∗ + 2τ (cid:107)ΛS(cid:107)1

√

F + 2λ
√

F + 2

n (cid:107)ΛL(cid:107)F + 2τ (cid:112)|Ωobs| (cid:107)ΛS(cid:107)F
2λ(cid:112)n log n ((cid:107)ΛL(cid:107)F + (cid:107)ΛS(cid:107)F) ,

(D.4)

where |Ωobs| denotes the cardinality of Ωobs. Here, the relation (i) results from the triangle inequality,
the inequality (ii) holds true since (cid:107)A(cid:107)∗ ≤
n(cid:107)A(cid:107)F for any A ∈ Rn×n and (cid:107)ΛS(cid:107)1 = (cid:107)PΩobs (ΛS)(cid:107)1 ≤
(cid:112)|Ωobs|(cid:107)ΛS(cid:107)F, and the last line (iii) arises from the fact that |Ωobs| ≤ 2n2p with high probability as well as
the choice τ = λ

. Combine (D.2) and (D.4) to reach

√

(cid:113) log n
np

α1 ≤ 2(cid:107)PΩobs (E) (cid:107)2
≤ 2 (cid:107)PΩobs (E)(cid:107)2

√

2λ(cid:112)n log n ((cid:107)ΛL(cid:107)F + (cid:107)ΛS(cid:107)F)
(cid:107)ΛL(cid:107)2

F + (cid:107)ΛS(cid:107)2
F,

(cid:113)

F + 2
F + 4λ(cid:112)n log n
√

√

where we use the elementary inequality a + b ≤

2 ·

a2 + b2.

(D.5)

Step 2: bounding α2 via α3. To relate α2 to α3, the following lemma plays a crucial role, whose proof
is deferred to Appendix D.1.

Lemma 6. Suppose that (cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2 ≤ p/8 and that (cid:107)PT (cid:63) − p−1PT (cid:63) PΩobs PT (cid:63) (cid:107) ≤ 1/2. Then for any pair
(A, B) of matrices, we have

(cid:107)PT (cid:63) (A)(cid:107)2

F + (cid:107)PΩ(cid:63) (B)(cid:107)2

F ≤

4
p

(cid:107)PΩobs [PT (cid:63) (A) + PΩ(cid:63) (B)](cid:107)2
F .

(D.6)

(ΛL), −Λ−)

obs

Suppose for the moment that the assumptions of Lemma 6 hold. Taking (A, B) as (Λ−+PΩc

in Lemma 6 yields

α2 = (cid:13)

(cid:13)PT (cid:63)

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1)(cid:13)
2
(cid:13)
F

+ (cid:13)

(cid:13)PΩ(cid:63)

(cid:0)Λ−(cid:1)(cid:13)
2
F ≤
(cid:13)

4
p

(cid:13)
(cid:13)PΩobs

(cid:2)PT (cid:63)

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) − PΩ(cid:63)

(cid:0)Λ−(cid:1)(cid:3)(cid:13)
2
(cid:13)
F

.

By virtue of the identity

PT (cid:63)

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) − PΩ(cid:63)

we further obtain

(cid:0)Λ−(cid:1) = Λ− + PΩc
= PΩc

obs

obs

(ΛL) − PT (cid:63)⊥

(cid:0)Λ− + PΩc

(ΛL)(cid:1) − Λ− + P(Ω(cid:63))c(Λ−)

obs

(ΛL) − PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) + P(Ω(cid:63))c (Λ−),

α2 ≤

=

≤

≤

4
p
4
p
4
p
8
p

(cid:13)
(cid:13)PΩobs

(cid:2)PΩc

obs

(ΛL) − PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) + P(Ω(cid:63))c(Λ−)(cid:3)(cid:13)
2
(cid:13)
F

(cid:13)
(cid:13)PΩobs

(cid:2)PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) − P(Ω(cid:63))c(Λ−)(cid:3)(cid:13)
2
(cid:13)
F

(cid:13)
(cid:13)PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1) − P(Ω(cid:63))c

(cid:0)Λ−(cid:1)(cid:13)
2
(cid:13)
F

(cid:13)
(cid:13)PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1)(cid:13)
2
(cid:13)
F

+

8
p

(cid:13)
(cid:13)P(Ω(cid:63))c

(cid:0)Λ−(cid:1)(cid:13)
2
F =
(cid:13)

8
p

α3.

Once again, the derivation has made use of the elementary inequality (cid:107)A + B(cid:107)2

F ≤ 2(cid:107)A(cid:107)2

F + 2(cid:107)B(cid:107)2
F

(D.7)

.

Step 3: bounding α3 via α1 and (cid:107)PΩobs(E)(cid:107)F. The following lemma proves useful in linking α3 with
α1, and we postpone the proof to Appendix D.2.

Lemma 7. Assume that n2p (cid:29) n log n, ρs (cid:28) 1 and (cid:107)PT (cid:63) − p−1(1 − ρs)−1PT (cid:63) PΩobs\Ω(cid:63) PT (cid:63) (cid:107) ≤ 1/2. Further
assume that there exists a dual certiﬁcate W ∈ Rn×n such that

(cid:13)
(cid:13)PT (cid:63)

(cid:2)λW + τ sgn (S(cid:63)) − λU (cid:63)V (cid:63)(cid:62)(cid:3)(cid:13)

(cid:13)F ≤ τ /

√

n,

(D.8a)

24

(cid:107)PT (cid:63)⊥ [λW + τ sign (S(cid:63))](cid:107) < λ/2,

P(Ωobs\Ω(cid:63))c (W ) = 0,

(cid:107)λW (cid:107)∞ < τ /2,

(D.8b)

(D.8c)

(D.8d)

where sign (S(cid:63)) := [sign(S(cid:63)
has

ij)]1≤i,j≤n. Then for any HL, HS ∈ Rn×n satisfying PΩobs(HL) + HS = 0, one

λ (cid:107)L(cid:63) + HL(cid:107)∗ + τ (cid:107)S(cid:63) + HS(cid:107)1 ≥ λ (cid:107)L(cid:63)(cid:107)∗ + τ (cid:107)S(cid:63)(cid:107)1 +

λ
4

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

τ
4

(cid:13)PΩobs\Ω(cid:63) (HS)(cid:13)
(cid:13)

(cid:13)1 .

Again, we assume for the moment that the assumptions in Lemma 7 hold. Setting HL = Λ− + PΩc

(ΛL)

obs

and HS = −Λ− in Lemma 7 gives

λ (cid:13)

(cid:13)L(cid:63) + Λ− + PΩc

+ τ (cid:13)
(ΛL)(cid:13)
(cid:13)∗
λ
≥ λ (cid:107)L(cid:63)(cid:107)∗ + τ (cid:107)S(cid:63)(cid:107)1 +
4

(cid:13)
(cid:13)PT (cid:63)⊥

obs

(cid:13)S(cid:63) − Λ−(cid:13)
(cid:13)1

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1)(cid:13)
(cid:13)∗

+

τ
4

(cid:13)
(cid:13)PΩobs\Ω(cid:63)

(cid:0)Λ−(cid:1)(cid:13)

(cid:13)1 .

In addition, recalling the identities Lcvx = L(cid:63) + Λ+ + Λ− + PΩc
invoke the triangle inequality to obtain

obs

(ΛL) and Scvx = S(cid:63) + Λ+ − Λ−, we can

λ (cid:107)Lcvx(cid:107)∗ + τ (cid:107)Scvx(cid:107)1 = λ (cid:13)
≥ λ (cid:13)

(cid:13)S(cid:63) − Λ− + Λ+(cid:13)
(cid:13)1
(cid:13)∗ − τ (cid:13)
(cid:13)Λ+(cid:13)
(cid:13)1 − λ (cid:13)
Adding the above two inequalities and using the fact support(Λ−) ⊆ Ωobs lead to

(ΛL)(cid:13)
+ τ (cid:13)
(cid:13)∗
(cid:13)S(cid:63) − Λ−(cid:13)
+ τ (cid:13)
(ΛL)(cid:13)
(cid:13)∗

(cid:13)L(cid:63) + Λ− + Λ+ + PΩc
(cid:13)L(cid:63) + Λ− + PΩc

obs

obs

(cid:13)Λ+(cid:13)

(cid:13)1 .

λ
4

obs

(ΛL)(cid:1)(cid:13)
(cid:13)∗

(cid:0)Λ− + PΩc

τ
(cid:13)
(cid:13)
(cid:13)PT (cid:63)⊥
(cid:13)P(Ω(cid:63))c
+
4
(cid:13)∗ + τ (cid:13)
(cid:13)Λ+(cid:13)
≤ λ (cid:107)Lcvx(cid:107)∗ + τ (cid:107)Scvx(cid:107)1 + λ (cid:13)
1
(cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2
(cid:107)PΩobs (L(cid:63) + S(cid:63) − M )(cid:107)2
F −
2
F + 4λ(cid:112)n log n (cid:13)
(cid:13)Λ+(cid:13)

(cid:13)
(cid:13)1 =
(cid:13)PT (cid:63)⊥
(cid:13)1 − λ (cid:107)L(cid:63)(cid:107)∗ − τ (cid:107)S(cid:63)(cid:107)1
F + λ (cid:13)

(cid:0)Λ−(cid:1)(cid:13)
(cid:13)Λ+(cid:13)

(cid:107)PΩobs (E)(cid:107)2

λ
4

≤

≤

(cid:13)F .

1
2
1
2

(cid:0)Λ− + PΩc

obs

(ΛL)(cid:1)(cid:13)
(cid:13)∗

+

τ
4

(cid:13)
(cid:13)PΩobs\Ω(cid:63)

(cid:0)Λ−(cid:1)(cid:13)
(cid:13)1

(cid:13)Λ+(cid:13)

(cid:13)∗ + τ (cid:13)

(cid:13)Λ+(cid:13)
(cid:13)1

(D.9)

Here, the penultimate line results from the inequality (D.3) and last line follows from the same argument in
obtaining (D.4).

We are now ready to establish the upper bound on α3. Invoke the elementary inequalities (cid:107)A(cid:107)F ≤ (cid:107)A(cid:107)∗

and (cid:107)A(cid:107)F ≤ (cid:107)A(cid:107)1 for any A ∈ Rn×n to show that

α3 = (cid:13)

(cid:13)PT (cid:63)⊥
(cid:18) 16

λ2 +

(cid:0)Λ− + PΩc
(cid:19) (cid:18) λ
16
τ 2
4

≤

(ΛL)(cid:1)(cid:13)
2
(cid:13)
F

+ (cid:13)

(cid:13)P(Ω(cid:63))c

obs

(cid:13)
(cid:13)PT (cid:63)⊥

(cid:0)Λ− + PΩc

obs

(cid:13)PT (cid:63)⊥

F ≤ (cid:13)
(cid:0)Λ−(cid:1)(cid:13)
2
(cid:13)
τ
(cid:13)
(cid:13)P(Ω(cid:63))c
4

+

(ΛL)(cid:1)(cid:13)
(cid:13)∗

(ΛL)(cid:1)(cid:13)
2
(cid:13)
∗

+ (cid:13)

(cid:13)P(Ω(cid:63))c

(cid:0)Λ−(cid:1)(cid:13)
2
(cid:13)
1

(cid:0)Λ− + PΩc
(cid:19)2

(cid:0)Λ−(cid:1)(cid:13)
(cid:13)1

obs

.

This combined with (D.9) allows us to obtain

α3 ≤

(cid:18) 16

λ2 +

16
τ 2

(cid:19) (cid:18) 1
2

(cid:107)PΩobs (E)(cid:107)2

F + 4λ

√

n (cid:13)
(cid:13)Λ+(cid:13)
(cid:13)F

(cid:19)2

≤

(cid:18) 32

λ2 +

32
τ 2

(cid:19) (cid:18) 1
4

(cid:107)PΩobs (E)(cid:107)4

F + 16λ2n log n (cid:13)

(cid:13)Λ+(cid:13)
2
(cid:13)
F

(cid:19)

,

where we have used the elementary inequality (a + b)2 ≤ 2a2 + 2b2. Recalling that τ = λ/(cid:112)np/ log n and
that np ≥ 1, we arrive at

α3 ≤

64np
λ2

(cid:18) 1
4

(cid:107)PΩobs (E)(cid:107)4

F + 16λ2n log n (cid:13)

(cid:13)Λ+(cid:13)
2
(cid:13)
F

(cid:19)

=

16np
λ2 (cid:107)PΩobs (E)(cid:107)4

F + 29n2pα1 log n,

(D.10)

where we have identiﬁed 2(cid:107)Λ+(cid:107)2
F

with α1.

25

Step 4: putting the above bounds on α1, α2, α3 together. Taking the preceding bounds on α1, α2
and α3 collectively yields

(cid:107)ΛL(cid:107)2

F + (cid:107)ΛS(cid:107)2

F = α1 + α2 + α3

(i)
≤ α1 +

(cid:18)

1 +

(cid:19)

8
p

(ii)
≤ α1 +

α3

16
p

α3

(iii)

≤ (cid:0)213n2 log n + 1(cid:1) α1 +

28n
λ2 (cid:107)PΩobs (E)(cid:107)4

F

(cid:20)

(iv)

≤ (cid:0)213n2 log n + 1(cid:1)

28n
λ2 (cid:107)PΩobs (E)(cid:107)4
F .
Here, the ﬁrst inequality (i) comes from (D.7), the second inequality (ii) follows from the fact 1 ≤ 8/p, the
third relation (iii) is a consequence of (D.10), and the last line (iv) results from (D.5). Note that this forms
a quadratic inequality in (cid:112)(cid:107)ΛL(cid:107)2

. Solving the inequality yields the claimed bound

F + 4λ(cid:112)n log n

2 (cid:107)PΩobs (E)(cid:107)2

F + (cid:107)ΛS(cid:107)2

(cid:107)ΛL(cid:107)2

+

F

F + (cid:107)ΛS(cid:107)2
F

(cid:113)

(cid:21)

(cid:107)ΛL(cid:107)2

F + (cid:107)ΛS(cid:107)2

F

(cid:46) λ2n5 log3 n + n2 log n (cid:107)PΩobs (E)(cid:107)2

F +

n
λ2 (cid:107)PΩobs (E)(cid:107)4
F .

Further, the elementary inequality a2 + b2 ≥ 2ab yields
n
λ2 (cid:107)PΩobs (E)(cid:107)4

λ2n5 log3 n +

F ≥ 2n3 log3/2 n (cid:107)PΩobs (E)(cid:107)2

F ≥ n2 log n (cid:107)PΩobs (E)(cid:107)2
F ,

leading to the simpliﬁed bound

(cid:107)ΛL(cid:107)2

F + (cid:107)ΛS(cid:107)2

F

(cid:46) λ2n5 log3 n +

n
λ2 (cid:107)PΩobs (E)(cid:107)4
F .

Step 5: checking the conditions in Lemmas 6 and 7. We are left with proving that the conditions
in Lemmas 6 and 7 hold with high probability.
In view of Lemma 3 and Corollary 3, the conditions
(cid:107)PT (cid:63) − p−1PT (cid:63) PΩobsPT (cid:63) (cid:107) ≤ 1/2 and (cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2 ≤ p/8 hold with high probability, provided that n2p (cid:29)
µrn log n and ρs ≤ 1/12. In addition, Lemma 3 ensures that (cid:107)PT (cid:63) − p−1(1 − ρs)−1PT (cid:63) PΩobs\Ω(cid:63) PT (cid:63) (cid:107) ≤ 1/2
holds with high probability, with the proviso that n2p(1 − ρs) (cid:29) µrn log n, which holds true under the
assumptions ρs ≤ 1/12 and n2p (cid:29) µrn log n. Last but not least, the existence of the dual certiﬁcate W
obeying (D.8) is guaranteed with high probability according to [CJSC13, Section III.D], under the conditions
ρs (cid:28) 1 and n2p (cid:29) µ2r2n log6 n.10

D.1 Proof of Lemma 6
Expand (cid:107)PΩobs(PT (cid:63) (A) + PΩ(cid:63) (B))(cid:107)2
F
(cid:107)PΩobs [PT (cid:63) (A) + PΩ(cid:63) (B)](cid:107)2

to obtain

F = (cid:107)PΩobsPT (cid:63) (A)(cid:107)2

F + (cid:107)PΩ(cid:63) (B)(cid:107)2

F + 2 (cid:104)PΩobsPT (cid:63) (A) , PΩ(cid:63) (B)(cid:105)

(cid:107)PT (cid:63) (A)(cid:107)2

F + (cid:107)PΩ(cid:63) (B)(cid:107)2

F + 2 (cid:104)PΩobsPT (cid:63) (A) , PΩ(cid:63) (B)(cid:105) .

≥

p
2

Here, the equality uses the fact Ω(cid:63) ⊆ Ωobs, and the inequality holds because of the assumption (cid:107)PT (cid:63) −
p−1PT (cid:63) PΩobsPT (cid:63) (cid:107) ≤ 1/2 and Fact 1. Use Ω(cid:63) ⊆ Ωobs once again to obtain

2 (cid:104)PΩobsPT (cid:63) (A) , PΩ(cid:63) (B)(cid:105) = 2 (cid:104)PΩ(cid:63) PT (cid:63) PT (cid:63) (A) , PΩ(cid:63) (B)(cid:105)

≥ −2 (cid:107)PΩ(cid:63) PT (cid:63) (cid:107) (cid:107)PT (cid:63) (A)(cid:107)F (cid:107)PΩ(cid:63) (B)(cid:107)F
≥ −2 (cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2 (cid:107)PT (cid:63) (A)(cid:107)2

F −

(cid:107)PΩ(cid:63) (B)(cid:107)2
F .

1
2

Here, the last relation arises from the elementary inequality ab ≤ (a2 +b2)/2 and the fact that (cid:107)PΩ(cid:63) PT (cid:63) (cid:107) ≤ 1.
Combine the above two inequalities to obtain

(cid:107)PΩobs [PT (cid:63) (A) + PΩ(cid:63) (B)](cid:107)2

F ≥

(cid:16) p
2

− 2 (cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2(cid:17)

(cid:107)PT (cid:63) (A)(cid:107)2

F +

1
2

(cid:107)PΩ(cid:63) (B)(cid:107)2
F

10Note that [CJSC13, Section III.D] requires n2p (cid:29) max{µ, µ2}rn log6 n under an additional

tion (cid:107)U (cid:63)V (cid:63)(cid:62)(cid:107)∞ ≤ (cid:112)µ2r/n2. While we do not impose this extra condition,
(cid:107)U (cid:63)(cid:107)2,∞(cid:107)V (cid:63)(cid:107)2,∞ ≤ µr/n and hence µ2 ≤ µ2r.

incoherence condi-
it is easily seen that (cid:107)U (cid:63)V (cid:63)(cid:62)(cid:107)∞ ≤

26

≥

≥

p
4
p
4

1
2

(cid:107)PT (cid:63) (A)(cid:107)2
(cid:16)

(cid:107)PT (cid:63) (A)(cid:107)2

(cid:107)PΩ(cid:63) (B)(cid:107)2
F
(cid:17)

F +
F + (cid:107)PΩ(cid:63) (B)(cid:107)2

F

as claimed, where we have used the assumption (cid:107)PΩ(cid:63) PT (cid:63) (cid:107)2 ≤ p/8 in the middle line and the fact 1/2 ≥ p/4
in the last inequality.

D.2 Proof of Lemma 7
In view of the convexity of the nuclear norm (cid:107) · (cid:107)∗, one has
(cid:107)L(cid:63) + HL(cid:107)∗ ≥ (cid:107)L(cid:63)(cid:107)∗ + (cid:10)U (cid:63)V (cid:63)(cid:62) + G1, HL

(cid:11) = (cid:107)L(cid:63)(cid:107)∗ + (cid:10)U (cid:63)V (cid:63)(cid:62), HL

(cid:11) + (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ .

Here, U (cid:63)V (cid:63)(cid:62) + G1 is a sub-gradient of (cid:107) · (cid:107)∗ at L(cid:63). The last identity holds by choosing G1 such that
(cid:104)G1, HL(cid:105) = (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗. Similarly, using the assumption PΩobs(HL) + HS = 0 and the convexity of the
(cid:96)1 norm (cid:107) · (cid:107)1, we can obtain

(cid:107)S(cid:63) + HS(cid:107)1 = (cid:107)S(cid:63) − PΩobs (HL)(cid:107)1

(i)

≥ (cid:107)S(cid:63)(cid:107)1 − (cid:104)sign (S(cid:63)) + G2, PΩobs (HL)(cid:105)
= (cid:107)S(cid:63)(cid:107)1 − (cid:104)sign (S(cid:63)) , PΩobs (HL)(cid:105) + (cid:13)
= (cid:107)S(cid:63)(cid:107)1 − (cid:104)sign (S(cid:63)) , HL(cid:105) + (cid:13)

(ii)

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)

(cid:13)1 ,

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)1

where sign(S(cid:63)) := [sign(S(cid:63)
ij)]1≤i,j≤n, and sign(S(cid:63)) + G2 is a sub-gradient of (cid:107) · (cid:107)1 at S(cid:63). The ﬁrst equality
(i) holds by choosing G2 such that −(cid:104)G2, PΩobs (HL)(cid:105) = (cid:107)PΩobs\Ω(cid:63) (HL)(cid:107)1, and the last relation (ii) arises
since sign(S(cid:63)) is supported on Ω(cid:63) ⊆ Ωobs. Combine the above two bounds to deduce that

∆ := λ (cid:107)L(cid:63) + HL(cid:107)∗ + τ (cid:107)S(cid:63) + HS(cid:107)1 − λ (cid:107)L(cid:63)(cid:107)∗ − τ (cid:107)S(cid:63)(cid:107)1

(cid:11) + λ (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ − τ (cid:104)sgn (S(cid:63)) , HL(cid:105) + τ (cid:13)

≥ λ (cid:10)U (cid:63)V (cid:63)(cid:62), HL
= (cid:10)λU (cid:63)V (cid:63)(cid:62) − τ sgn (S(cid:63)) , HL
= (cid:10)λU (cid:63)V (cid:63)(cid:62) − τ sgn (S(cid:63)) − λW , HL

(cid:124)

(cid:123)(cid:122)
=:θ1

(cid:11) + λ (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ + τ (cid:13)
+ (cid:104)λW , HL(cid:105)
(cid:123)(cid:122)
(cid:125)
=:θ2

(cid:11)

(cid:125)

(cid:124)

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)1
(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)1
+λ (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ + τ (cid:13)

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)

(cid:13)1 ,

(D.11)

where W ∈ Rn×n is the dual certiﬁcate stated in Lemma 7.

In what follows, we shall lower bound the right-hand side of (D.11). To begin with, for θ1 we have

(cid:2)λU (cid:63)V (cid:63)(cid:62) − τ sign (S(cid:63)) − λW (cid:3) , PT (cid:63) (HL)(cid:11) + (cid:10)PT (cid:63)⊥
(cid:2)λU (cid:63)V (cid:63)(cid:62) − τ sign (S(cid:63)) − λW (cid:3) , PT (cid:63) (HL)(cid:11) − (cid:104)PT (cid:63)⊥ [τ sign (S(cid:63)) + λW ] , PT (cid:63)⊥ (HL)(cid:105)

(cid:2)λU (cid:63)V (cid:63)(cid:62) − τ sign (S(cid:63)) − λW (cid:3) , PT (cid:63)⊥ (HL)(cid:11)

(cid:13)F (cid:107)PT (cid:63) (HL)(cid:107)F − (cid:107)PT (cid:63)⊥ [τ sign (S(cid:63)) + λW ](cid:107) (cid:107)PT (cid:63)⊥ (HL)(cid:107)∗

θ1 = (cid:10)PT (cid:63)
= (cid:10)PT (cid:63)
≥ − (cid:13)
(cid:13)PT (cid:63)
τ
√
n

≥ −

(cid:2)λU (cid:63)V (cid:63)(cid:62) − τ sign (S(cid:63)) − λW (cid:3)(cid:13)
λ
2

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ .

(cid:107)PT (cid:63) (HL)(cid:107)F −

Here, the penultimate line uses the fact U (cid:63)V (cid:63)(cid:62) ∈ T (cid:63) and the elementary inequalities |(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F
and |(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)(cid:107)B(cid:107)∗, whereas the last inequality relies on the properties of the dual certiﬁcate W ,
namely, (D.8a) and (D.8b). Moving on to θ2, one has

θ2 = (cid:10)λPΩobs\Ω(cid:63) (W ) , HL

(cid:11) + (cid:10)λP(Ωobs\Ω(cid:63))c (W ) , HL

(cid:11)

= (cid:10)λW , PΩobs\Ω(cid:63) (HL)(cid:11) (i)

≥ − (cid:107)λW (cid:107)∞

(cid:13)
(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)1

(ii)
≥ −

τ
2

(cid:13)
(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)

(cid:13)1 .

Here, the second identity uses the assumption (D.8c), the ﬁrst inequality (i) uses the elementary inequality
|(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)∞(cid:107)B(cid:107)1, and the last relation (ii) holds because of the assumption (D.8d). Substituting the
above two bounds back into (D.11) gives

∆ ≥ −

τ
√
n

(cid:107)PT (cid:63) (HL)(cid:107)F +

λ
2

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

τ
2

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)

(cid:13)1 .

(D.12)

27

Continuing the lower bound, we have

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)
(cid:13)1

(i)

≥ (cid:13)

(cid:13)F = (cid:13)
(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:13)
(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:13)

(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL) + PΩobs\Ω(cid:63) PT (cid:63)⊥ (HL)(cid:13)
(cid:13)F
(cid:13)F − (cid:13)
(cid:13)F − (cid:107)PT (cid:63)⊥ (HL)(cid:107)F ,

(cid:13)PΩobs\Ω(cid:63) PT (cid:63)⊥ (HL)(cid:13)
(cid:13)F

(ii)

≥ (cid:13)
≥ (cid:13)

where (i) holds because (cid:107)A(cid:107)1 ≥ (cid:107)A(cid:107)F for any matrix A, and (ii) arises from the triangle inequality. Putting
the above relation and (D.12) together results in

(cid:107)PT (cid:63) (HL)(cid:107)F +

τ
√
n
(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:13)
(cid:13)

(cid:18) λ
2

(cid:13)F −

∆ ≥ −

≥

τ
4

−

τ
4
τ
√
n

(cid:19)

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

(cid:107)PT (cid:63) (HL)(cid:107)F +

λ
4

τ
4

(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:13)
(cid:13)

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

τ
4

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)
(cid:13)1

(cid:13)F +

τ
4
(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)

(cid:13)1 ,

(D.13)

where the last line holds since λ = τ (cid:112)np/ log n ≥ τ (as long as np ≥ log n). Everything then boils
down to lower bounding (cid:107)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:107)F. To this end, one can use the assumption (cid:107)PT (cid:63) − p−1(1 −
ρs)−1PT (cid:63) PΩobs\Ω(cid:63) PT (cid:63) (cid:107) ≤ 1/2 and Fact 1 to obtain

(cid:13)PΩobs\Ω(cid:63) PT (cid:63) (HL)(cid:13)
(cid:13)
2
F ≥
(cid:13)

1
2

p (1 − ρs) (cid:107)PT (cid:63) (HL)(cid:107)2
F .

(D.14)

Take (D.13) and (D.14) collectively to yield

∆ ≥

(cid:32)

(cid:114) 1
2

τ
4

p (1 − ρs) −

(cid:33)

τ
√
n

(cid:107)PT (cid:63) (HL)(cid:107)F +

λ
4

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

τ
4

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)
(cid:13)1

≥

λ
4

(cid:107)PT (cid:63)⊥ (HL)(cid:107)∗ +

τ
4

(cid:13)PΩobs\Ω(cid:63) (HL)(cid:13)
(cid:13)

(cid:13)1 ,

where the last relation is guaranteed by np (cid:29) 1 and ρs (cid:28) 1. Recognizing that PΩobs\Ω(cid:63) (HL) = −PΩobs\Ω(cid:63) (HS)
ﬁnishes the proof.

E Equivalence between convex and nonconvex solutions (Proof of

Theorem 4)

The goal of this section is to establish the intimate connection between the convex and nonconvex solu-
tions (cf. Theorem 4). Before continuing, we remind the readers of the following notations:

• XY (cid:62) = U ΣV (cid:62): the rank-r singular value decomposition of XY (cid:62);

• T : the tangent space of the set of rank-r matrices at the estimate XY (cid:62).

In addition, we deﬁne

and denote the support of S by

E.1 Preliminary facts

∆L := Lcvx − XY (cid:62),

∆S := Scvx − S,

Ω := {(i, j) | Sij (cid:54)= 0}.

(E.1)

(E.2)

We begin with two useful lemmas which demonstrate that the point (XY (cid:62), S) described in Theorem 4
satisﬁes approximate optimality conditions w.r.t. the convex program (1.3).

28

Lemma 8. Instate the assumptions in Theorem 4. The triple (X, Y , S) as stated in Theorem 4 satisﬁes

PΩobs

(cid:0)XY (cid:62) + S − M (cid:1) = −U V (cid:62) + R1

1
λ

for some matrix R1 ∈ Rn×n obeying

(cid:107)PT (R1)(cid:107)F

(cid:46) κp (cid:107)∇f (X, Y ; S)(cid:107)F
√

λ

σmin

(cid:46) 1
n19

and

(cid:107)PT ⊥ (R1)(cid:107) ≤

1
2

.

(E.3)

(E.4)

Proof. The proof can be straightforwardly adapted from [CCF+20, Claim 2] by replacing E therein with
E + S(cid:63) − S. We omit it for the sake of brevity.

Lemma 9. The point (XY (cid:62), S) as stated in Theorem 4 obeys

PΩobs

(cid:0)XY (cid:62) + S − M (cid:1) = −sign (S) + R2

1
τ

for some matrix R2 ∈ Rn×n, where R2 satisﬁes

PΩ (R2) = 0

and

(cid:107)PΩc(R2)(cid:107)∞ ≤ 1

with Ω deﬁned in (E.2).

(E.5)

(E.6)

Proof. By deﬁnition, one has S = PΩobs[Sτ (M − XY (cid:62))]. Clearly, this is equivalent to saying that S is the
unique minimizer of the following convex program

S = arg min
(cid:98)S∈Rn×n

(cid:13)
(cid:13)
(cid:13)PΩobs

1
2

(cid:0)XY (cid:62) + (cid:98)S − M (cid:1)(cid:13)
2
(cid:13)
(cid:13)

F

+

λ
2

(cid:107)X(cid:107)2

F +

λ
2

(cid:107)Y (cid:107)2

F + τ (cid:13)

(cid:13) (cid:98)S(cid:13)
(cid:13)1.

(E.7)

The claim of this lemma then follows from the optimality condition of this convex program (E.7).

Additionally, in view of the crude error bound (3.8) and Condition 1, the matrix ∆L (cf. (E.1)) obeys
(cid:107)∆L(cid:107)F = (cid:13)

(cid:13)F ≤ (cid:107)Lcvx − L(cid:63)(cid:107)F + n (cid:13)

(cid:13)F ≤ (cid:107)Lcvx − L(cid:63)(cid:107)F + (cid:13)

(cid:13)XY (cid:62) − L(cid:63)(cid:13)
(cid:13)∞

(cid:13)XY (cid:62) − L(cid:63)(cid:13)

(cid:13)Lcvx − XY (cid:62)(cid:13)
(cid:46) σn4 + nτ (cid:16) σn4,

(E.8)

where we use the the elementary inequality (cid:107)A(cid:107)F ≤ n(cid:107)A(cid:107)∞ and the fact that τ (cid:16) σ

√

log n.

E.2 Proof of Theorem 4

We now present the proof of Theorem 4, which consists of three main steps:

1. Showing that (XY (cid:62), S) is not far from (Lcvx, Scvx) over Ωobs, in the sense that PΩobs(∆L + ∆S) ≈ 0;
2. Showing that ∆L (resp. ∆S) is extremely small outside the tangent space T (resp. the support Ω(cid:63)), and
hence most of the energy of ∆L (resp. ∆S) — if it is not vanishingly small — has to reside within T
(resp. Ω(cid:63));

3. Showing that ∆S ≈ 0 and ∆L ≈ 0, with the assistance of the preceding two steps.

In what follows, we shall detail each of these steps.

29

E.2.1 Step 1: showing that PΩobs(∆L + ∆S) ≈ 0
Since (Lcvx, Scvx) is the minimizer of the convex program (1.3), we have

1

2 (cid:107)PΩobs (Lcvx + Scvx − M )(cid:107)2

= 1
2
≤ 1
2

(cid:13)
(cid:13)PΩobs
(cid:13)
(cid:13)PΩobs

F + λ (cid:107)Lcvx(cid:107)∗ + τ (cid:107)Scvx(cid:107)1
F + λ (cid:13)
(cid:0)XY (cid:62) + ∆L + S + ∆S − M (cid:1)(cid:13)
2
(cid:13)
(cid:13)XY (cid:62)(cid:13)
F + λ (cid:13)
(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
(cid:13)∗ + τ (cid:107)S(cid:107)1 .
(cid:13)

(cid:13)XY (cid:62) + ∆L

(cid:13)
(cid:13)∗ + τ (cid:107)S + ∆S(cid:107)1

Here, the equality arises from the relations Lcvx = XY (cid:62) + ∆L and Scvx = S + ∆S. Expanding the squares
and rearranging terms, we arrive at

1

2 (cid:107)PΩobs (∆L + ∆S)(cid:107)2

F ≤ − (cid:10)PΩobs

(cid:0)XY (cid:62) + S − M (cid:1) , ∆L + ∆S

(cid:11) + λ (cid:13)

(cid:13)XY (cid:62)(cid:13)

(cid:13)∗ − λ (cid:13)

(cid:13)XY (cid:62) + ∆L

In view of the convexity of the nuclear norm (cid:107) · (cid:107)∗ and the (cid:96)1 norm (cid:107) · (cid:107)1, one has

+ τ (cid:107)S(cid:107)1 − τ (cid:107)S + ∆S(cid:107)1 .

(cid:13)
(cid:13)∗
(E.9)

(cid:13)
(cid:13)XY (cid:62) + ∆L

(cid:13)∗ ≥ (cid:13)
(cid:13)

(cid:13)XY (cid:62)(cid:13)
(cid:107)S + ∆S(cid:107)1 ≥ (cid:107)S(cid:107)1 + (cid:10)sign (S) + G, ∆S

(cid:13)∗ + (cid:10)U V (cid:62) + W , ∆L
(cid:11) (ii)

(cid:11) (i)

= (cid:13)

(cid:13)XY (cid:62)(cid:13)

(cid:13)∗ + (cid:10)U V (cid:62), ∆L

(cid:11) + (cid:107)PT ⊥(∆L)(cid:107)∗ ;

= (cid:107)S(cid:107)1 + (cid:10)sign (S) , ∆S

(cid:11) + (cid:107)PΩc (∆S)(cid:107)1 .

(E.10a)

(E.10b)

Here, U V (cid:62) + W is a sub-gradient of (cid:107) · (cid:107)∗ at XY (cid:62). The identity (i) holds by choosing W such that
(cid:104)W , ∆L(cid:105) = (cid:107)PT ⊥(∆L)(cid:107)∗. Similarly, sign(S) + G is a sub-gradient of (cid:107) · (cid:107)1 at S and one can choose G
obeying (cid:104)G, ∆S(cid:105) = (cid:107)PΩc(∆S)(cid:107)1 to make (ii) valid. These taken together with (E.9) lead to

1

2 (cid:107)PΩobs (∆L + ∆S)(cid:107)2

F ≤ −(cid:10)PΩobs

(cid:0)XY (cid:62) + S − M (cid:1) , ∆L + ∆S
(cid:11) − τ (cid:107)PΩc (∆S)(cid:107)1 .

− τ (cid:10)sign (S) , ∆S

(cid:11) − λ(cid:10)U V (cid:62), ∆L

(cid:11) − λ (cid:107)PT ⊥ (∆L)(cid:107)∗

Recall the deﬁnitions of R1 and R2 from Lemmas 8 and 9. We can then simplify the above inequality as

1

2 (cid:107)PΩobs (∆L + ∆S)(cid:107)2

F ≤ −λ (cid:104)R1, ∆L(cid:105) − λ (cid:107)PT ⊥ (∆L)(cid:107)∗
(cid:125)

(cid:124)

(cid:123)(cid:122)
=:θ1

−τ (cid:104)R2, ∆S(cid:105) − τ (cid:107)PΩc(∆S)(cid:107)1
(cid:124)
(cid:123)(cid:122)
(cid:125)
=:θ2

.

(E.11)

In the sequel, we develop bounds on θ1 and θ2.

1. With regards to θ1, one can further decompose it into

θ1 = −λ (cid:104)R1, PT (∆L)(cid:105) − λ (cid:104)R1, PT ⊥ (∆L)(cid:105) − λ (cid:107)PT ⊥ (∆L)(cid:107)∗

≤ λ (cid:107)PT (R1)(cid:107)F (cid:107)PT (∆L)(cid:107)F − λ (1 − (cid:107)PT ⊥ (R1)(cid:107)) (cid:107)PT ⊥ (∆L)(cid:107)∗
λ
2
where the middle line arises from the elementary inequalities |(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F and |(cid:104)A, B(cid:105)| ≤
(cid:107)A(cid:107)(cid:107)B(cid:107)∗, and the last inequality holds since (cid:107)PT ⊥ (R1)(cid:107) ≤ 1/2 (see Lemma 8).

≤ λ (cid:107)PT (R1)(cid:107)F (cid:107)PT (∆L)(cid:107)F −

(cid:107)PT ⊥ (∆L)(cid:107)∗ ,

(E.12)

2. Similarly, one can decompose θ2 into

θ2 = −τ (cid:104)R2, PΩ (∆S)(cid:105) − τ (cid:104)R2, PΩc (∆S)(cid:105) − τ (cid:107)PΩc (∆S)(cid:107)1

≤ τ (cid:104)PΩ (R2) , PΩ (∆S)(cid:105) − τ (1 − (cid:107)PΩc (R2)(cid:107)∞) (cid:107)PΩc (∆S)(cid:107)1 ≤ 0.

(E.13)

Here, the ﬁrst inequality comes from the facts that |(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)∞(cid:107)B(cid:107)1 and |(cid:104)A, B(cid:105)| ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F,
and the second one utilizes the facts that PΩ(R2) = 0 and (cid:107)PΩc(R2)(cid:107)∞ ≤ 1 (cf. Lemma 9).

Combining (E.11), (E.12) and (E.13) yields

1

2 (cid:107)PΩobs (∆L + ∆S)(cid:107)2

F ≤ λ (cid:107)PT (R1)(cid:107)F (cid:107)PT (∆L)(cid:107)F −

λ
2

(cid:107)PT ⊥ (∆L)(cid:107)∗

(E.14)

30

(cid:46) λ

n19 (cid:107)∆L(cid:107)F

(cid:46)

√

σ

np

n19 σn4 (cid:46) σ2
n14.5 ,

where we make use of the upper bound (cid:107)PT (R1)(cid:107)F (cid:46) n−19 (cf. Lemma 8), the choice λ (cid:16) σ
np as well as the
crude error bound (cid:107)∆L(cid:107)F (cid:46) σn4 (cf. (E.8)). Consequently, we have demonstrated that PΩobs(∆L + ∆S) ≈ 0
in the sense that

√

(cid:107)PΩobs (∆L + ∆S)(cid:107)F

(cid:46) σ

n7.25 ≤

σ
n7 .

(E.15)

E.2.2 Step 2: showing that PT ⊥ (∆L) ≈ 0 and P(Ω(cid:63))c(∆S) ≈ 0
We begin by demonstrating that PT ⊥(∆L) ≈ 0. From the inequality (E.14), we have

1
2

(cid:107)PT ⊥ (∆L)(cid:107)∗ ≤ (cid:107)PT (R1)(cid:107)F (cid:107)PT (∆L)(cid:107)F −

(cid:107)PΩobs (∆L + ∆S)(cid:107)2

F

1
2λ
(cid:46) 1
n19 (cid:107)∆L(cid:107)F ,

≤ (cid:107)PT (R1)(cid:107)F (cid:107)PT (∆L)(cid:107)F

where the last inequality again results from the estimate (cid:107)PT (R1)(cid:107)F (cid:46) n−19 given in Lemma 8. Invoking
the condition (cid:107)∆L(cid:107)F (cid:46) σn4 (cf. (E.8)) yields

n19 σn4 (cid:46) σ
(cid:46) 1
which demonstrates that the energy of ∆L outside T is extremely small.

(cid:107)PT ⊥ (∆L)(cid:107)F ≤ (cid:107)PT ⊥ (∆L)(cid:107)∗

n15 ≤

We now move on to P(Ω(cid:63))c (∆S). This term obeys

σ
n14 ,

(E.16)

P(Ω(cid:63))c(∆S) = PΩobs\Ω(cid:63) (∆S),

where the relation holds since ∆S is supported on Ωobs. To facilitate the analysis of Ωobs\Ω(cid:63), we introduce
another index subset

Ω1 := {(i, j) ∈ Ωobs : |(∆S)ij| ≤ (cid:107)PΩobs(∆L + ∆S)(cid:107)∞} .
The usefulness of Ω1 can be seen through the following claim, whose claim is postponed to the end of this
section.

(E.17)

Claim 1. Under Condition 1, we have

Ωobs\Ω(cid:63) ⊆ Ω1.

An immediate consequence of Claim 1 is that

(cid:13)P(Ω(cid:63))c (∆S)(cid:13)
(cid:13)

(cid:13)F = (cid:13)

(cid:13)PΩobs\Ω(cid:63) (∆S)(cid:13)

(cid:13)F ≤ (cid:107)PΩ1 (∆S)(cid:107)F

≤ n (cid:107)PΩ1 (∆S)(cid:107)∞ ≤ n (cid:107)PΩobs (∆L + ∆S)(cid:107)∞
≤ n (cid:107)PΩobs (∆L + ∆S)(cid:107)F ≤

σ
n6 ,

(E.18)

which justiﬁes our assertion that the energy of ∆S outside Ω(cid:63) is extremely small. Here, the last inequality
arises from (E.15).

E.2.3 Step 3: controlling the size of ∆S (and hence that of ∆L)

In view of (E.15) and the triangle inequality, we have

σ
n7 ≥ (cid:107)PΩobs (∆L + ∆S)(cid:107)F

≥ (cid:107)PΩobsPT (∆L)(cid:107)F − (cid:107)PΩobs PT ⊥ (∆L)(cid:107)F − (cid:107)PΩ(cid:63) (∆S)(cid:107)F − (cid:13)
≥ (cid:107)PΩobsPT (∆L)(cid:107)F − (cid:107)PΩ(cid:63) (∆S)(cid:107)F − (cid:107)PT ⊥ (∆L)(cid:107)F − (cid:13)
≥ (cid:107)PΩobsPT (∆L)(cid:107)F − (cid:107)PΩ(cid:63) (∆S)(cid:107)F −

σ
n6 −

σ
n14 ,

(cid:13)PΩobs\Ω(cid:63) (∆S)(cid:13)
(cid:13)F
(cid:13)P(Ω(cid:63))c (∆S)(cid:13)
(cid:13)F

(E.19)

31

where the last step follows from (E.16) and (E.18). By Condition 2, we have

(cid:107)PΩobs PT (∆L)(cid:107)F ≥

(cid:114) cinj
κ

p (cid:107)PT (∆L)(cid:107)F

and

(cid:107)PΩ(cid:63) PT (∆L)(cid:107)F ≤

(cid:114) cinj
κ

1
2

p (cid:107)PT (∆L)(cid:107)F ,

given that PT (∆L) ∈ T . The latter inequality combined with (E.15) and (E.16) further gives

(cid:107)PΩ(cid:63) (∆S)(cid:107)F ≤ (cid:107)PΩ(cid:63) (∆S + ∆L)(cid:107)F + (cid:107)PΩ(cid:63) (∆L)(cid:107)F

≤ (cid:107)PΩobs (∆S + ∆L)(cid:107)F + (cid:107)PΩ(cid:63) PT (∆L)(cid:107)F + (cid:107)PΩ(cid:63) PT ⊥ (∆L)(cid:107)F
σ
n14 .

p (cid:107)PT ∆L(cid:107)F +

(cid:114) cinj
κ

σ
n7 +

1
2

≤

Substituting the above bounds into (E.19) gives

σ
n7 ≥

≥

which further yields

(cid:114) cinj
κ
(cid:114) cinj
κ

1
2

p (cid:107)PT (∆L)(cid:107)F −

(cid:114) cinj
κ

1
2

p (cid:107)PT (∆L)(cid:107)F −

σ
n7 −

σ
n6 −

2σ
n14

p (cid:107)PT (∆L)(cid:107)F −

2σ
n6 ,

(cid:107)PT (∆L)(cid:107)F

(cid:46) σ
n6

(cid:114) κ
p

≤

σ
n5 ,

provided that n2p (cid:29) κ. This combined with (E.16) allows one to control the size of ∆L:

(cid:13)
(cid:13)F ≤ (cid:107)PT (∆L)(cid:107)F + (cid:107)PT ⊥ (∆L)(cid:107)F
In view of (E.15) and the fact that ∆S is supported on Ωobs, we have

(cid:13)
(cid:13)∆L

(cid:46) σ
n5 .

(cid:107)∆S(cid:107)F = (cid:107)PΩobs (∆S)(cid:107)F ≤ (cid:107)PΩobs (∆L + ∆S)(cid:107)F + (cid:107)PΩobs (∆L)(cid:107)F

≤ (cid:107)PΩobs (∆L + ∆S)(cid:107)F + (cid:107)∆L(cid:107)F

thus concluding the proof.

E.2.4 Proof of Claim 1

We ﬁrst recall the facts that

(cid:46) σ
n5 ,

S = PΩobs

(cid:2)Sτ

(cid:0)M − XY (cid:62)(cid:1)(cid:3)

and

S + ∆S = PΩobs

(cid:2)Sτ

(cid:0)M − XY (cid:62) − ∆L

(cid:1)(cid:3) ,

where the second identity follows since (Lcvx, Scvx) = (XY (cid:62) + ∆L, S + ∆S) is the optimizer of the convex
program (1.3). These allow us to write

(cid:0)M − XY (cid:62) − ∆L
(cid:0)M − XY (cid:62)(cid:1)(cid:3)
(cid:2)M − XY (cid:62) + ∆S − (∆L + ∆S)(cid:3) − Sτ
This characterization of ∆S turns out to be crucial when establishing the inclusion Ωobs\Ω(cid:63) ⊆ Ω1. Towards
this end, we need to introduce another index subset

∆S = PΩobs
= PΩobs

(cid:0)M − XY (cid:62)(cid:1)(cid:3) .

(cid:2)Sτ
(cid:2)Sτ

(cid:1) − Sτ

(E.20)

Ω2 :=

(cid:110)
(i, j) ∈ Ωobs : τ − (cid:107)PΩobs(∆L + ∆S)(cid:107)∞ ≤ (cid:12)
(cid:12)

(cid:0)M − XY (cid:62)(cid:1)

(cid:12)
(cid:12) ≤ τ

(cid:111)

.

ij

As it turns out, the sets Ω, Ω1 and Ω2 obey the following three conditions

Ω2 ∩ Ω = ∅,

Ωobs\Ω ⊆ Ω1 ∪ Ω2,

and

Ω ∪ Ω2 ⊆ Ω(cid:63),

which immediately lead to

(ii)
= (Ωobs\Ω)\Ω2
Here, (i) follows since Ω ∪ Ω2 ⊆ Ω(cid:63), (ii) holds true since Ω2 ∩ Ω = ∅, and (iii) results from the condition
Ωobs\Ω ⊆ Ω1 ∪ Ω2. It then boils down to proving each of the above three conditions.

(iii)
⊆ (Ω1 ∪ Ω2)\Ω2 ⊆ Ω1.

(i)
⊆ Ωobs\(Ω ∪ Ω2)

Ωobs\Ω(cid:63)

32

1. The ﬁrst one Ω2 ∩ Ω = ∅ is straightforward to establish. Note that for any (i, j) ∈ Ω2, one must have
ij = 0, which means that (i, j) /∈ Ω. This proves the

(cid:12)
(cid:12) ≤ τ and hence (cid:2)Sτ (M − XY (cid:62))(cid:3)

(cid:12)
(cid:0)M − XY (cid:62)(cid:1)
(cid:12)
relation Ω2 ∩ Ω = ∅.

ij

2. Moving on to the second one Ωobs\Ω ⊆ Ω1 ∪ Ω2, we prove this via contradiction. Suppose that this

inclusion is false, i.e. there exits an index (i, j) ∈ Ωobs\Ω such that

|(∆S)ij| > (cid:107)PΩobs(∆L + ∆S)(cid:107)∞ and |(M − XY (cid:62))ij| < τ − (cid:107)PΩobs(∆L + ∆S)(cid:107)∞.

(E.21)

Here, we have taken into account the fact that

To reach contradiction, we ﬁnd it convenient to state the following simple fact.

|(M − XY (cid:62))ij| ≤ τ,

for any (i, j) ∈ Ωobs\Ω.

Fact 2. Suppose that |a| ≤ τ and that Sτ (a + b) (cid:54)= 0. Then

(cid:12)Sτ (a + b)(cid:12)
(cid:12)

(cid:12) ≤ |b| + |a| − τ.

Proof. Given that Sτ (a + b) (cid:54)= 0, one necessarily has |a + b| > τ . Without loss of generality, assume that
a + b > 0, which gives

This together with the fact τ ≥ |a| yields |Sτ (a + b)| = a + b − τ ≤ |b| + |a| − τ .

Sτ (a + b) = a + b − τ > 0.

With this fact in mind, we can deduce that

(cid:12)
(cid:12)

(cid:0)∆S

(cid:1)

ij

(cid:12)
(cid:12) =

(cid:2)M − XY (cid:62) + ∆S − (∆L + ∆S)(cid:3) − Sτ
(cid:12)
(cid:12)
(cid:12)

(cid:2)M − XY (cid:62) + ∆S − (∆L + ∆S)(cid:3)(cid:9)

ij

(cid:0)M − XY (cid:62)(cid:1)(cid:9)

(cid:12)
(cid:12)
(cid:12)

ij

(cid:12)
(cid:12) + (cid:12)
(cid:12)
(cid:12)

(cid:0)M − XY (cid:62)(cid:1)

(cid:12)
(cid:12) − τ

ij

(i)
=

(cid:8)Sτ

(cid:12)
(cid:8)Sτ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)[∆S − (∆L + ∆S)]ij
< (cid:12)
(cid:0)∆S
(cid:12)
= (cid:12)
(cid:1)
(cid:0)∆S
(cid:12)

(ii)
≤

(iii)

(cid:1)

ij

(cid:12)
(cid:12) + (cid:107)PΩobs(∆L + ∆S)(cid:107)∞ − (cid:107)PΩobs(∆L + ∆S)(cid:107)∞
ij
(cid:12)
(cid:12),

(E.22)

where (i) holds true since (cid:2)Sτ (M − XY (cid:62))(cid:3)
ij = 0 for any (i, j) ∈ Ωobs\Ω, (ii) follows from Fact 2 (by
taking a = (M − XY (cid:62))ij and b = [∆S − (∆L + ∆S)]ij
), and (iii) is a consequence of (E.21) as well
as the triangle inequality. The inequality (E.22), however, is clearly impossible. This establishes that
Ωobs\Ω ⊆ Ω1 ∪ Ω2.

3. We are left with the last one Ω ∪ Ω2 ⊆ Ω(cid:63), which is equivalent to saying Ω ⊆ Ω(cid:63) and Ω2 ⊆ Ω(cid:63). First, for

any (i, j) ∈ Ω, one has

(cid:12)
(cid:12)Sij

(cid:12)
(cid:12) > 0 =⇒ (i, j) ∈ Ωobs

=⇒ (i, j) ∈ Ωobs

and (cid:12)
(cid:0)L(cid:63) + S(cid:63) + E − XY (cid:62)(cid:1)
(cid:12)
and (cid:12)
(cid:13)L(cid:63) − XY (cid:62)(cid:13)
(cid:12)S(cid:63)
ij

(cid:12)
(cid:12) > τ − (cid:13)

(cid:12)
(cid:12) > τ

ij
(cid:13)∞ − (cid:107)E(cid:107)∞ > 0.

Here, the last step comes from the triangle inequality and Condition 1. This reveals that Ω ⊆ Ω(cid:63).
Similarly, for any (i, j) ∈ Ω2 we have

τ − (cid:107)PΩobs (∆L + ∆S)(cid:107)∞ ≤ (cid:12)
(cid:12)
⇐⇒ τ − (cid:107)PΩobs (∆L + ∆S)(cid:107)∞ ≤ (cid:12)
(cid:12)
(cid:12) ≥ τ − (cid:13)
(cid:12)
=⇒ (cid:12)

(cid:13)L(cid:63) − XY (cid:62)(cid:13)

(cid:12)S(cid:63)
ij

(cid:0)M − XY (cid:62)(cid:1)
(cid:0)S(cid:63) + L(cid:63) − XY (cid:62) + E(cid:1)

ij

(cid:12)
(cid:12)

(cid:12)
(cid:12)

ij

(cid:13)∞ − (cid:107)E(cid:107)∞ − (cid:107)PΩobs(∆L + ∆S)(cid:107)F ≥

τ
2

−

σ
n7 > 0,

where we have used Condition 1, the bound (E.15), and the fact that τ (cid:29) σ. This demonstrates that
Ω2 ⊆ Ω(cid:63). We have therefore justiﬁed that Ω ∪ Ω2 ⊆ Ω(cid:63).

33

F Analysis of the nonconvex procedure (Proof of Theorem 5)

This section is devoted to establishing Theorem 5. For notational convenience, we introduce

F t := (cid:2)X t(cid:62), Y t(cid:62)(cid:3)(cid:62)

∈ R2n×r

and

F (cid:63) := (cid:2)X (cid:63)(cid:62), Y (cid:63)(cid:62)(cid:3)(cid:62)

∈ R2n×r.

These allow us to express succinctly the rotation matrix H t deﬁned in (3.10) as

H t = arg min

R∈Or×r

(cid:13)F tR − F (cid:63)(cid:13)
(cid:13)

(cid:13)F .

(F.1)

(F.2)

With the deﬁnitions of F t and H t in mind, it suﬃces to justify that: for all 0 ≤ t ≤ t0 = n47, the following
hypotheses

(cid:13)F tH t − F (cid:63)(cid:13)
(cid:13)

(cid:13)F ≤ CF

(cid:13)F tH t − F (cid:63)(cid:13)
(cid:13)

(cid:13) ≤ Cop

(cid:13)F tH t − F (cid:63)(cid:13)
(cid:13)

(cid:13)2,∞ ≤ C∞κ

(cid:13)X t(cid:62)X t − Y t(cid:62)Y t(cid:13)
(cid:13)
(cid:13)St − S(cid:63)(cid:13)
(cid:13)

(cid:13)F ≤ CBκη
√

(cid:13) ≤ CSσ

(cid:19)

(cid:107)X (cid:63)(cid:107)F ,
(cid:19)

(cid:107)X (cid:63)(cid:107) ,

(cid:18) σ
σmin
(cid:18) σ
σmin
(cid:32)

(cid:114) n
p
(cid:114) n
p
(cid:115)

+

+

λ
pσmin
λ
pσmin

n log n
p

+

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ ,

λ
pσmin
(cid:19) √

(cid:114) n
p

+

λ
pσmin

rσ2

max,

σ
σmin
(cid:18) σ
σmin

np

hold for some universal constants CF, Cop, C∞, CB, CS > 0, and, in addition,

F (cid:0)X t, Y t; St(cid:1) ≤ F (cid:0)X t−1, Y t−1; St−1(cid:1) −

η
2

(cid:13)∇f (cid:0)X t−1, Y t−1; St−1(cid:1)(cid:13)
(cid:13)
2
(cid:13)
F

(F.3a)

(F.3b)

(F.3c)

(F.3d)

(F.3e)

(F.4)

holds for all 1 ≤ t ≤ t0 = n47.

Clearly, the bounds (3.11a), (3.11b), (3.11c), and (3.11d) in Theorem 5 follow immediately from (F.3a),
(F.3b), (F.3c), and (F.3e), respectively. It remains to justify the small gradient bound (3.12) on the basis of
(F.3) and (F.4), which is exactly the content of the following lemma.

Lemma 10 (Small gradient). Set λ = Cλσ
n2p (cid:29) κ3µrn log2 n and that the noise satisﬁes

√

np log n for some large constant Cλ > 0. Suppose that
If the

. Take η (cid:16) 1/(nκ3σmax).

1√

(cid:113) n

p (cid:28)

σ
σmin

κ4µr log n

iterates satisfy (F.3) for all 0 ≤ t ≤ t0 and (F.4) for all 1 ≤ t ≤ t0, then with probability at least 1 − O(n−50),
one has

Proof. See Appendix F.3.

min
0≤t<t0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)

(cid:13)F ≤

√

1
n20

λ
p

σmin.

,
The remainder of this section is thus dedicated to showing that (F.3) and (F.4) hold for {(F t, St)}0≤t≤t0
which we accomplish via mathematical induction. Throughout this section, we let Xl,· denote the lth row
of a matrix X.

F.1 Leave-one-out analysis
The above hypotheses (F.3) require, among other things, sharp control of the (cid:96)2,∞ estimation errors, which
calls for ﬁne-grained statistical analyses. In order to decouple complicated statistical dependency, we resort to
the following leave-one-out analysis framework that has been successfully applied to analyze other nonconvex
algorithms [ZB18, MWCC20, CLL20, CCFM19, CCF+20, CLPC20, LWC+20, DC20].

34

Leave-one-out loss functions. For each 1 ≤ l ≤ n, we deﬁne the following auxiliary loss functions

F (l) (X, Y , S)
1
2p
(cid:124)

:=

(cid:13)
(cid:13)P(Ωobs)−l,·

(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
(cid:13)

F

(cid:0)XY (cid:62) − L(cid:63)(cid:1)(cid:13)
2
F +
(cid:13)

+

1
2

(cid:13)
(cid:13)Pl,·
(cid:123)(cid:122)
=:f (l)(X,Y ;S)

λ
2p

(cid:107)X(cid:107)2

F +

λ
2p

(cid:107)Y (cid:107)2
F
(cid:125)

+

τ
p

(cid:107)S(cid:107)1 .

Here, P(Ωobs)−l,· (·) (resp. Pl,·(·)) denotes orthogonal projection onto the space of matrices supported on the
index set {(i, j) ∈ Ωobs | i (cid:54)= l} (resp. {(i, j) | i = l}), namely, for any matrix B ∈ Rn×n one has

(cid:2)P(Ωobs)−l,· (B)(cid:3)

ij

=

(cid:40)

Bij,
0,

if (i, j) ∈ Ωobs and i (cid:54)= l,
otherwise

and

[Pl,· (B)]ij =

(cid:40)

Bij,
0,

if i = l,
otherwise.

The above auxiliary loss function is obtained by dropping the randomness coming from the lth row of M ,
which, as we shall see shortly, facilitates analysis in establishing the incoherence properties (F.3c). Similarly,
we deﬁne for each n + 1 ≤ l ≤ 2n that

F (l) (X, Y , S)
1
2p
(cid:124)

:=

(cid:13)
(cid:13)
(cid:13)P(Ωobs)·,−(l−n)

(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
(cid:13)
(cid:13)

F

+

1
2

(cid:13)
(cid:13)P·,(l−n)

(cid:0)XY (cid:62) − L(cid:63)(cid:1)(cid:13)
2
F +
(cid:13)

(cid:123)(cid:122)
=:f (l)(X,Y ;S)

λ
2p

(cid:107)X(cid:107)2

F +

λ
2p

(cid:107)Y (cid:107)2
F
(cid:125)

+τ (cid:107)S(cid:107)1 ,

where the projection operators P(Ωobs)·,−(l−n)(·) and P·,(l−n)(·) are deﬁned such that for any matrix B ∈ Rn×n,

(cid:104)

P(Ωobs)·,−(l−n) (B)

(cid:105)

ij

=

(cid:40)

Bij,
0,

if (i, j) ∈ Ωobs and j (cid:54)= l − n,
otherwise

and

(cid:2)P·,(l−n) (B)(cid:3)

ij =

(cid:40)

Bij,
0,

if j = l − n,
otherwise.

Again, this auxiliary loss function is produced in a way that is independent from the (l − n)-th column of
M . In the above notation, f (l) (X, Y ; S) is a function of X and Y with S frozen.

Leave-one-out auxiliary sequences. For each 1 ≤ l ≤ 2n, we construct a sequence of leave-one-out
iterates {F t,(l), St,(l)}t≥0 via Algorithm 2.

Algorithm 2 Construction of the lth leave-one-out sequences.

Initialization: X 0,(l) = X (cid:63), Y 0,(l) = Y (cid:63), S0,(l) = S(cid:63), F 0,(l) :=
Gradient updates: for t = 0, 1, . . . , t0 − 1 do

(cid:20) X 0,(l)
Y 0,(l)

(cid:21)
, and the step size η > 0.

F t+1,(l) :=

(cid:20) X t+1,(l)
Y t+1,(l)

(cid:21)

=

St+1,(l) :=

(cid:40)Sτ
Sτ

(cid:2)P(Ωobs)−l,·
(cid:104)
P(Ωobs)·,−(l−n)

(cid:20) X t,(l) − η∇X f (l)(X t,(l), Y t,(l); St,(l))
Y t,(l) − η∇Y f (l)(X t,(l), Y t,(l); St,(l))
(cid:0)M − X t+1,(l)Y t+1,(l)(cid:62)(cid:1)(cid:3) + Pl,· (S(cid:63)) ,

(cid:21)

;

(cid:0)M − X t+1,(l)Y t+1,(l)(cid:62)(cid:1)(cid:105)

+ P·,(l−n) (S(cid:63)) ,

(F.5a)

(F.5b)

if 1 ≤ l ≤ n,
if n + 1 ≤ l ≤ 2n.

Properties of leave-one-out sequences. There are several features of the leave-one-out sequences that
prove useful for our statistical analysis: (1) for the lth leave-one-out sequence, one can exploit the statistical
independence to control the estimation error of F t,(l) in the lth row; (2) the leave-one-out sequences and the
original sequence (F t, St) are exceedingly close (since we have only discarded a small amount of information).
These properties taken collectively allow us to control the estimation error of F t in each row. To formalize
these features, we make an additional set of induction hypotheses

(cid:13)F tH t − F t,(l)Rt,(l)(cid:13)
(cid:13)

(cid:13)F ≤ C1

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ ,

(F.6a)

35

max
1≤l≤2n

(cid:13)
(cid:0)F t,(l)H t,(l) − F (cid:63)(cid:1)
(cid:13)

(cid:13)
(cid:13)2 ≤ C2κ

l,·

(cid:32)

(cid:13)
(cid:13)P−l,·

max
1≤l≤n
(cid:13)
(cid:13)P·,−(l−n)

max
n<l≤2n

(cid:0)St − St,(l)(cid:1)(cid:13)

(cid:13)F ≤ C3

(cid:0)St − St,(l)(cid:1)(cid:13)

(cid:13)F ≤ C3

σ
σmin
σ
σmin

(cid:115)

(cid:33)

+

λ
pσmin

n log n
p

σ
σmin
(cid:112)n log n (cid:107)F (cid:63)(cid:107) (cid:107)F (cid:63)(cid:107)2,∞ ,
(cid:112)n log n (cid:107)F (cid:63)(cid:107) (cid:107)F (cid:63)(cid:107)2,∞ .

(cid:107)F (cid:63)(cid:107)2,∞ ,

(F.6b)

(F.6c)

(F.6d)

Here, the rotation matrices H t,(l) and Rt,(l) are deﬁned respectively by

H t,(l) := arg min

R∈Or×r

(cid:13)F t,(l)R − F (cid:63)(cid:13)
(cid:13)

(cid:13)F,

and

Rt,(l) := arg min

R∈Or×r

(cid:13)F t,(l)R − F t(cid:13)
(cid:13)

(cid:13)F.

F.2 Key lemmas for establishing the induction hypotheses

This subsection establishes the induction hypotheses made in Appendix F.1, namely (F.3), (F.4) and (F.6).
Before continuing, we ﬁnd it convenient to introduce another function of X and Y (with S frozen) as follows

faug (X, Y ; S) :=

1
2p

(cid:13)
(cid:13)PΩobs

(cid:0)XY (cid:62) + S − M (cid:1)(cid:13)
2
F +
(cid:13)

λ
2p

(cid:107)X(cid:107)2

F +

λ
2p

(cid:107)Y (cid:107)2

F +

1
8

(cid:13)X (cid:62)X − Y (cid:62)Y (cid:13)
(cid:13)
2
F .
(cid:13)

(F.7)

The diﬀerence between faug and f lies in the following balancing term

fdiﬀ (X, Y ) := −

1
8

(cid:13)X (cid:62)X − Y (cid:62)Y (cid:13)
(cid:13)
2
F ,
(cid:13)

that is, f = faug + fdiﬀ .

The following four lemmas, which are inherited from [CCF+20] with little modiﬁcation, are concerned

with local strong convexity as well as the hypotheses (F.3a), (F.3b), (F.3d), (F.6b) and (F.3c).

Lemma 11 (Restricted strong convexity). Set λ = Cλσ
Suppose that the sample size obeys n2p (cid:29) κµrn log n and that the noise satisﬁes
function faug be deﬁned in (F.7). Then with probability at least 1 − O(n−100),

np for some large enough constant Cλ > 0.
p (cid:28) 1. Let the

(cid:113) n log n

σ
σmin

√

vec (∆)(cid:62) ∇2faug (X, Y ; S) vec (∆) ≥ 1
(cid:13) , (cid:13)

(cid:13)∇2f (X, Y )(cid:13)
(cid:9) ≤ 10σmax
(cid:13)

(cid:13)∇2faug (X, Y ; S)(cid:13)

10 σmin (cid:107)∆(cid:107)2

F

max (cid:8)(cid:13)

hold uniformly over all X, Y ∈ Rn×r, S ∈ Rn×n obeying

(cid:13)
(cid:20) X − X (cid:63)
(cid:13)
(cid:13)
Y − Y (cid:63)
(cid:13)

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)2,∞

≤

1
1000κ

√

n

(cid:107)X (cid:63)(cid:107) ,

(cid:107)S − S(cid:63)(cid:107) ≤ CSσ

√

np

and all ∆ =

(cid:21)

(cid:20) ∆X
∆Y

∈ R2n×r lying in the set

(cid:21)

(cid:26) (cid:20) X1
Y1

ˆH −

(cid:20) X2
Y2

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:20) X2 − X (cid:63)
(cid:13)
(cid:13)
Y2 − Y (cid:63)
(cid:13)

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

1
500κ

(cid:107)X (cid:63)(cid:107) , ˆH := arg min

R∈Or×r

(cid:21)

(cid:13)
(cid:20) X1
(cid:13)
(cid:13)
Y1
(cid:13)

R −

(cid:20) X2
Y2

(cid:27)

.

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

Lemma 12 (Frobenius norm error w.r.t. F ). Set λ = Cλσ
Suppose that the sample size obeys n2p (cid:29) κµrn log2 n and that the noise satisﬁes

np for some large enough constant Cλ > 0.
. If

1√

(cid:113) n

σ
σmin

p (cid:28)

κ4µr log n

√

the iterates satisfy (F.3) in the tth iteration, then with probability at least 1 − O(n−100) ,

(cid:13)F t+1H t+1 − F (cid:63)(cid:13)
(cid:13)

(cid:13)F ≤ CF

(cid:18) σ
σmin

(cid:114) n
p

+

λ
pσmin

(cid:19)

(cid:107)X (cid:63)(cid:107)F

holds as long as 0 < η (cid:28) 1/(κ5/2σmax).

36

Lemma 13 (Spectral norm error w.r.t. F ). Set λ = Cλσ
Suppose that the sample size obeys n2p (cid:29) κ4µ2r2n log2 n and that the noise satisﬁes

np for some large enough constant Cλ > 0.
.

1√

(cid:113) n

σ
σmin

p (cid:28)

κ4 log n

√

If the iterates satisfy (F.3) in the tth iteration, then with probability at least 1 − O(n−100), one has

(cid:13)F t+1H t+1 − F (cid:63)(cid:13)
(cid:13)

(cid:13) ≤ Cop

(cid:18) σ
σmin

(cid:114) n
p

+

λ
pσmin

(cid:19)

(cid:107)X (cid:63)(cid:107)

holds as long as 0 < η (cid:28) 1/(κ3σmax

√

r) and Cop (cid:29) 1.

Lemma 14 (Approximate balancedness). Set λ = Cλσ
Suppose that the sample size obeys n2p (cid:29) κ2µ2r2n log n and that the noise satisﬁes

np for some large enough constant Cλ > 0.
σ
. If
σmin
the iterates satisfy (F.3) in the tth iteration, then with probability at least 1 − O(n−100),

p (cid:28) 1√

κ2 log n

(cid:113) n

√

(cid:13)X t+1(cid:62)X t+1 − Y t+1(cid:62)Y t+1(cid:13)
(cid:13)

(cid:13)F ≤ CBκη

max
1≤l≤2n

(cid:13)X t+1,(l)(cid:62)X t+1,(l) − Y t+1,(l)(cid:62)Y t+1,(l)(cid:13)
(cid:13)

(cid:13)F ≤ CBκη

(cid:18) σ
σmin
(cid:18) σ
σmin

(cid:114) n
p
(cid:114) n
p

+

+

λ
pσmin
λ
pσmin

(cid:19) √

(cid:19) √

rσ2

max

rσ2

max

hold for some suﬃciently large constant CB (cid:29) C 2

op, provided that 0 < η < 1/σmin.
√

Lemma 15 ((cid:96)2,∞ norm error of leave-one-out sequences). Set λ = Cλσ
np for some large enough
constant Cλ > 0. Suppose that the sample size obeys n2p (cid:29) κ4µ2r2n log3 n and that the noise satisﬁes
If the iterates satisfy (F.3) in the tth iteration, then with probability at least 1 −

1√

(cid:113) n

.

p (cid:28)

σ
σmin
O(n−100),

κ2 log n

max
1≤l≤2n

(cid:13)
(cid:0)F t+1,(l)H t+1,(l) − F (cid:63)(cid:1)
(cid:13)

(cid:13)
(cid:13)2 ≤ C2κ

l,·

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞

holds, provided that 0 < η (cid:28) 1/(κ2√
Lemma 16 ((cid:96)2,∞ norm error of the true sequence). Set λ = Cλσ
Cλ > 0. Suppose that n ≥ µr and that the noise satisﬁes
and (F.6) in the tth iteration, then with probability at least 1 − O(n−99), one has

rσmax), Cop (cid:29) 1 and C2 (cid:29) Cop.

p (cid:28) 1√

σ
σmin

κ2 log n

(cid:113) n

√

np for some large enough constant

. If the iterates satisfy (F.3)

(cid:13)F t+1H t+1 − F (cid:63)(cid:13)
(cid:13)

(cid:13)2,∞ ≤ C∞κ

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ ,

provided that C∞ ≥ 5C1 + C2.

Proof of Lemmas 11, 12, 13, 14, 15 and 16. As it turns out, Lemmas 11, 12, 13 and 14 follow immediately
from the proofs of [CCF+20, Lemmas 17, 10, 11, 15] respectively. More speciﬁcally, the proofs can be
accomplished by replacing E in the proofs therein with ˜E := E + S(cid:63) − St. To see this, we remark that the
np with
only property of the perturbation matrix E utilized in the proofs therein is that (cid:107)PΩobs(E)(cid:107) (cid:46) σ
probability at least 1 − O(n−10); under our hypotheses, the new matrix ˜E clearly satisﬁes this property since

√

(cid:107)PΩobs

(cid:0) ˜E(cid:1)(cid:107) ≤ (cid:107)PΩobs (E)(cid:107) + (cid:13)

(cid:13)S(cid:63) − St(cid:13)

(cid:13) (cid:46) σ

√

np.

Regarding Lemma 15, we note that St,(l)
l,· }t≥0 and {Y t,(l)

by construction. Therefore, the update rule regarding the
l,· }t≥0 is exactly the same as that in the leave-one-out sequence introduced

lth row of {X t,(l)
in [CCF+20]. Thus, Lemma 15 follows immediately from the proof of [CCF+20, Lemma 13].
Finally, the proof of Lemma 16 is exactly the same as the proof of [CCF+20, Lemma 14].

l,· ≡ S(cid:63)
l,·

37

Next, we justify the hypotheses (F.3e), (F.6a) and (F.6c) in the following three lemmas, which require

more careful analysis of the properties about {St}.

Lemma 17 (Spectral norm error w.r.t. S). Set τ = Cτ σ
Suppose that the sample size obeys n2p (cid:29) κ4µrn log n, the noise satisﬁes

log n for some large enough constant Cτ > 0.
p (cid:28) 1/(cid:112)κ2 log n, the outlier
κ5µr log2 n and n2pρaug (cid:29) µnr log2 n. If the iterates satisfy (F.3b) and

fraction satisﬁes ρs ≤ ρaug (cid:28) 1/
(F.3c) in the (t + 1)-th iteration, then with probability at least 1 − O(n−100),

σ
σmin

(cid:113) n

(cid:112)

√

(cid:13)St+1 − S(cid:63)(cid:13)
(cid:13)

(cid:13) ≤ CSσ

√

np

holds for some constant CS > 0 that does not rely on the choice of other constants.

Proof. See Appendix F.4.

Lemma 18 (Leave-one-out perturbation w.r.t. F ). Set λ = Cλσ
np for some large enough con-
stant Cλ > 0. Suppose that the sample size obeys n2p (cid:29) κ4µ2r2n log4 n, the noise satisﬁes
p (cid:28)
1/(cid:112)κ4µr log n, the outlier fraction satisﬁes ρs ≤ ρaug (cid:28) 1/(κ3µr log n) and n2pρaug (cid:29) µrn log n. If the
iterates satisfy (F.3) and (F.6) in the tth iteration, then with probability at least 1 − O(n−100),

σ
σmin

(cid:113) n

√

max
1≤l≤2n

(cid:13)F t+1H t+1 − F t+1,(l)Rt+1,(l)(cid:13)
(cid:13)

(cid:13)F ≤ C1

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞

holds for some constant C1 > 0, provided that η (cid:28) 1/(nκ2σmax) and C1 (cid:29) C3.

Proof. See Appendix F.5.

Lemma 19 (Leave-one-out perturbation w.r.t. S). Set τ = Cτ σ
log n for some large enough constant
(cid:113) n
p (cid:28) 1/(cid:112)κ2 log n
Cτ > 0. Suppose that the sample size satisﬁes n2p (cid:29) κ4µ2r2n log n, the noise obeys
and the outlier fraction satisﬁes ρs ≤ ρaug (cid:28) 1/κ. If the iterates satisfy (F.3b), (F.3c) and (F.6a) in the
(t + 1)-th iteration, then with probability at least 1 − O(n−100),

σ
σmin

√

(cid:13)
(cid:13)P−l,·

max
1≤l≤n
(cid:13)
(cid:13)P·,−(l−n)

max
n<l≤2n

(cid:0)St+1 − St+1,(l)(cid:1)(cid:13)

(cid:13)F ≤ C3

(cid:0)St+1 − St+1,(l)(cid:1)(cid:13)

(cid:13)F ≤ C3

σ
σmin
σ
σmin

(cid:112)n log n (cid:107)F (cid:63)(cid:107) (cid:107)F (cid:63)(cid:107)2,∞ ,
(cid:112)n log n (cid:107)F (cid:63)(cid:107) (cid:107)F (cid:63)(cid:107)2,∞

hold for some constant C3 that does not rely on the choice of other constants.

Proof. See Appendix F.6.

Finally, it remains to justify (F.4), which is a straightforward consequence from standard gradient descent

theory and implies the existence of a point with nearly zero gradient.

Lemma 20 (Monotonicity of the function values). Set λ = Cλσ
Cλ > 0. Suppose that the noise satisﬁes
with probability at least 1 − O(n−100),

np for some large enough constant
p (cid:28) 1. If the iterates satisfy (F.3) in the tth iteration, then

σ
σmin

(cid:113) n

√

F (cid:0)X t+1, Y t+1; St+1(cid:1) ≤ F (cid:0)X t, Y t; St(cid:1) −

η
2

(cid:13)
(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
2
(cid:13)
F

holds as long as η (cid:28) 1/(κnσmax).

Proof. See Appendix F.7.

38

F.3 Proof of Lemma 10
Summing (F.4) over t = 1, . . . , t0 gives

F (cid:0)X t0 , Y t0, St0(cid:1) ≤ F (cid:0)X 0, Y 0, S0(cid:1) −

η
2

t0−1
(cid:88)

t=0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
F ,
(cid:13)

which further implies

min
0≤t<t0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
F ≤
(cid:13)

≤

t0−1
(cid:88)

1
t0
2
ηt0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
(cid:13)
F

t=0

(cid:2)F (X (cid:63), Y (cid:63), S(cid:63)) − F (cid:0)X t0, Y t0, St0(cid:1)(cid:3) .

(F.8)

Here, the last inequality results from our choice (X 0, Y 0, S0) = (X (cid:63), Y (cid:63), S(cid:63)). Therefore, it suﬃces to
control F (X (cid:63), Y (cid:63), S(cid:63)) − F (X t0 , Y t0, St0).
We ﬁrst decompose the diﬀerence into

F (X (cid:63), Y (cid:63), S(cid:63)) − F (cid:0)X t0, Y t0 , St0(cid:1) = f (X (cid:63), Y (cid:63); S(cid:63)) +

τ
p

(cid:107)S(cid:63)(cid:107)1 − f (cid:0)X t0 , Y t0; St0(cid:1) −

= f (X (cid:63), Y (cid:63); S(cid:63)) − f (cid:0)X t0, Y t0 ; S(cid:63)(cid:1)

+ f (cid:0)X t0, Y t0; S(cid:63)(cid:1) − f (cid:0)X t0, Y t0 ; St0 (cid:1)

+

(cid:124)

(cid:123)(cid:122)
=:∆1

(cid:125)

(cid:124)

(cid:123)(cid:122)
=:∆2

(cid:125)

τ
p
(cid:124)

In what follows, we shall bound ∆1, ∆2 and ∆3 separately.

1. In view of the proof of [CCF+20, Lemma 9], we have

|∆1| (cid:46) rκ2

(cid:19)2

,

(cid:18) λ
p

provided that

σ
σmin

(cid:113) n

p (cid:28)

1√

κ4µr log n

.

2. When it comes to ∆2, we deduce that

τ
p
(cid:107)S(cid:63)(cid:107)1 −
(cid:123)(cid:122)
=:∆3

(cid:13)St0(cid:13)
(cid:13)
(cid:13)1
τ
(cid:13)St0(cid:13)
(cid:13)
(cid:13)1
p
(cid:125)

.

|∆2| =

(cid:13)
(cid:13)PΩobs

1
2
(cid:10)PΩobs

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:13)

=

(cid:0)X t0 Y t0(cid:62) + S(cid:63) − M (cid:1)(cid:13)
2
F +
(cid:13)

λ
2p

(cid:107)F t0(cid:107)2

(cid:0)X t0 Y t0(cid:62) + St0 − M (cid:1) , S(cid:63) − St0(cid:11) +

1
2

(cid:13)PΩobs

(cid:0)X t0Y t0(cid:62) + St0 − M (cid:1)(cid:13)
(cid:13)F

(cid:13)S(cid:63) − St0(cid:13)
(cid:13)

(cid:13)F +

1
2

F −

(cid:13)
(cid:13)PΩobs

1
2
(cid:13)S(cid:63) − St0(cid:13)
(cid:13)
2
(cid:13)
F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)S(cid:63) − St0 (cid:13)
(cid:13)
2
F ,
(cid:13)

(cid:0)X t0 Y t0(cid:62) + St0 − M (cid:1)(cid:13)
2
F −
(cid:13)

λ
2p

(cid:107)F t0(cid:107)2
F

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(F.9)

where the last step arises from the elementary inequality (cid:104)A, B(cid:105) ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F and the triangle inequality.
It is straightforward to derive from (F.3e) that

(cid:107)S(cid:63) − St0(cid:107)F ≤

√

n(cid:107)S(cid:63) − St0(cid:107) (cid:46) σn

√

p.

(cid:13)
(cid:13)PΩobs

Moving on to the ﬁrst term in (F.9), one has by the triangle inequality
(cid:13)F + (cid:13)
(cid:13)F + σn

(cid:0)X t0Y t0(cid:62) − X (cid:63)Y (cid:63)(cid:62)(cid:1)(cid:13)
(cid:0)X t0Y t0(cid:62) − X (cid:63)Y (cid:63)(cid:62)(cid:1)(cid:13)

(cid:0)X t0Y t0(cid:62) + St0 − M (cid:1)(cid:13)

(cid:13)F ≤ (cid:13)
(cid:46) (cid:13)

(cid:13)PΩobs
(cid:13)PΩobs

(cid:13)PΩobs
√
p,

(cid:0)St0 − S(cid:63)(cid:1)(cid:13)

(cid:13)F + (cid:107)PΩobs (E)(cid:107)F

where the last bound follows from Lemma 1 and the bound above (cid:107)PΩobs(St0 − S(cid:63))(cid:107)F ≤ (cid:107)St0 − S(cid:63)(cid:107)F (cid:46)
σn

√

p. We can further decompose (cid:107)PΩobs (X t0 Y t0(cid:62) − X (cid:63)Y (cid:63)(cid:62))(cid:107)F into
(cid:13)
(cid:13)
(cid:13)PΩobs

(cid:2)(cid:0)X t0H t0 − X (cid:63)(cid:1) Y (cid:63)(cid:62)(cid:3)(cid:13)

(cid:0)X t0Y t0(cid:62) − X (cid:63)Y (cid:63)(cid:62)(cid:1)(cid:13)

(cid:13)F ≤ (cid:13)

(cid:13)PΩobs

(cid:13)F +

(cid:13)
(cid:13)PΩobs

(cid:104)

X t0H t0 (cid:0)Y t0H t0 − Y (cid:63)(cid:1)(cid:62)(cid:105)(cid:13)
(cid:13)
(cid:13)F

39

(i)

(cid:46) √
(cid:46) √

(ii)

(cid:46) √

(cid:0)X t0 H t0 − X (cid:63)(cid:1) Y (cid:63)(cid:62)(cid:13)
κp (cid:13)
(cid:13)F +
(cid:13)
√
(cid:13)X t0H t0 − X (cid:63)(cid:13)
κp (cid:13)
(cid:13)F (cid:107)Y (cid:63)(cid:107) +
(cid:19)
(cid:18) σ
(cid:114) n
λ
pσmin
p
σmin

κp

+

√

(cid:13)X t0H t0 (cid:0)Y t0H t0 − Y (cid:63)(cid:1)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)
κp
(cid:13)F
(cid:13)Y t0H t0 − Y (cid:63)(cid:13)
(cid:13)
(cid:13)X t0 H t0 (cid:13)
κp (cid:13)
(cid:13)F
(cid:13)

(cid:107)X (cid:63)(cid:107)F (cid:107)X (cid:63)(cid:107) (cid:46) κ3/2 λ
√
p

√

r.

(F.10)

Here, the relation (i) utilizes Lemma 4, and the facts that (X t0H t0−X (cid:63))Y (cid:63)(cid:62) ∈ T (cid:63) and that X t0H t0(Y t0H t0−
Y (cid:63))(cid:62) ∈ T t0, where T t0 denotes the tangent space at X t0Y t0(cid:62). In addition, the last line (ii) holds because
of the hypothesis (F.3a) and the simple fact (cid:107)X t0 H t0 (cid:107) ≤ 2(cid:107)X (cid:63)(cid:107), which is an immediate consequence of
the hypothesis (F.3b) provided that

p (cid:28) 1. Collecting the bounds together, we arrive at

(cid:113) n

σ
σmin

|∆2| (cid:46)

(cid:18)
κ3/2 λ
√
p

√

√

r + σn

(cid:19)

p

· σn

√

p + σ2n2p +

λ
p

· κ

λ
p

r (cid:46) σ2n2p,

with the proviso that np (cid:29) κ3r.

3. In the end, we have the following upper bound on ∆3:

|∆3| ≤

τ
p

(cid:13)St0 − S(cid:63)(cid:13)
(cid:13)

(cid:13)1 ≤

τ
p

(cid:13)St0 − S(cid:63)(cid:13)
n (cid:13)
(cid:13)F

(cid:46) 1
p

λ
(cid:112)np/ log n

σn2√

p (cid:16)

λ
p

σn3/2(cid:112)log n,

where we have made use of the elementary fact that (cid:107)A(cid:107)1 ≤ n(cid:107)A(cid:107)F for all A ∈ Rn×n.

Putting the above bounds together, one can reach

(cid:12)
(cid:12)F (X (cid:63), Y (cid:63), S(cid:63)) − F (cid:0)X t0, Y t0, St0 (cid:1)(cid:12)

(cid:12) (cid:46) rκ2

(cid:19)2

(cid:18) λ
p

+ σ2n2p +

λ
p

σn3/2(cid:112)log n (cid:46) n

(cid:19)2

(cid:18) λ
p

(cid:112)log n

as long as n (cid:29) κ2r and λ (cid:16) σ

√

np. Substitution into (F.8) allows us to conclude that

min
0≤t≤t0

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
(cid:13)F

(cid:46)

(cid:115)

(cid:19)2

1
ηt0

n

(cid:18) λ
p

(cid:112)log n ≤

√

1
n20

λ
p

σmin,

provided that η (cid:16) 1/(nκ3σmax), t0 ≥ n47 and n ≥ κ.

F.4 Proof of Lemma 17

In view of the deﬁnitions Ω(cid:63) = {(i, j) : S(cid:63)
X t+1Y t+1(cid:62))], we have the decomposition

ij (cid:54)= 0} ⊆ Ωaug ⊆ Ωobs and St+1 = Sτ [PΩobs(L(cid:63) + S(cid:63) + E −

St+1 − S(cid:63) = PΩaug

(cid:0)St+1(cid:1) − PΩaug (S(cid:63)) + PΩc

aug

(cid:0)St+1(cid:1)

(cid:2)PΩaug

= Sτ
(cid:124)

(cid:0)X (cid:63)Y (cid:63)(cid:62) + S(cid:63) + E − X t+1Y t+1(cid:62)(cid:1)(cid:3) − PΩaug (S(cid:63))
(cid:125)

(cid:123)(cid:122)
=:At+1

(cid:2)PΩobs\Ωaug

+ Sτ
(cid:124)

(cid:0)X (cid:63)Y (cid:63)(cid:62) + E − X t+1Y t+1(cid:62)(cid:1)(cid:3)
(cid:125)

.

(cid:123)(cid:122)
=:Bt+1

(F.11)

We shall control (cid:107)At+1(cid:107) and (cid:107)Bt+1(cid:107) separately.

1. We begin by controlling the size of At+1, which can be further decomposed into

(cid:2)PΩaug (S(cid:63) + E)(cid:3) − PΩaug (S(cid:63) + E)
At+1 = PΩaug (E) + Sτ
(cid:124)
(cid:125)
(cid:123)(cid:122)
=:A1
(cid:0)X (cid:63)Y (cid:63)(cid:62) − X t+1Y t+1(cid:62) + S(cid:63) + E(cid:1)(cid:3) − Sτ

(cid:2)PΩaug

+ Sτ
(cid:124)

(cid:123)(cid:122)
=:At+1
2

40

(cid:2)PΩaug (S(cid:63) + E)(cid:3)
(cid:125)

.

(F.12)

np, as long as n2pρaug (cid:29) n log2 n. This arises
First of all, we know that (cid:107)PΩaug (E)(cid:107) (cid:46) σ
from standard concentration results for the spectral norm of sub-Gaussian random matrices (cf. Lemma 1).
Regarding A1, we know from the deﬁnition of Sτ (·) that (cid:107)A1(cid:107)∞ ≤ τ . More precisely, we have

npρaug ≤ σ

√

√

(A1)ij =




−τ
−S(cid:63)

τ

ij − Eij

ij + Eij ≥ τ,

if S(cid:63)
if − τ < S(cid:63)
if S(cid:63)

ij + Eij ≤ −τ.

ij + Eij < τ,

Recall from Assumption 4 that S(cid:63) has random signs on its support Ω(cid:63) ⊆ Ωaug and Eij is symmetric
around zero. It then follows from standard concentration results for the spectral norm of matrices with
i.i.d. entries that

(cid:107)A1(cid:107) (cid:46) τ

npρaug = Cτ σ(cid:112)npρaug log n,

√

provided that n2pρaug (cid:29) n log2 n. Moving on to At+1
decompose (cid:107)At+1

2

, since it is supported on Ωaug, we can further

(cid:13)PΩaug

(cid:0)At+1

2

(cid:1)(cid:13)
(cid:13) ≤ pρaug

(cid:13)
(cid:13)At+1
2

(cid:13) + (cid:13)
(cid:13)

(cid:13)PΩaug

(cid:0)At+1

2

(cid:1) − pρaugAt+1

2

(cid:13)
(cid:13) .

(F.13)

2 (cid:107) into
(cid:13) = (cid:13)
(cid:13)
(cid:13)
(cid:13)At+1
2

Invoking Lemma 5 with A = At+1

2

, B = In and ρ0 = pρaug, we have
(cid:13)
(cid:13)
(cid:13)At+1
(cid:13) ≤ C
2

(cid:1) − pρaugAt+1

npρaug

√

2

2

(cid:0)At+1

(cid:13)
(cid:13)PΩaug

(cid:13)
(cid:13)2,∞ ,

(F.14)

with the proviso that n2pρaug (cid:29) n log n. Combine the above bounds to reach

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13) ≤ pρaug

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13) + C

√

npρaug

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13)2,∞ ≤

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13) + C

√

npρaug

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13)2,∞ ,

1
2

as soon as ρs ≤ ρaug ≤ 1/2. We are then in need of an upper bound on (cid:107)At+1
the following fact.

2 (cid:107)2,∞, which is supplied in

Fact 3. Suppose that n2pρaug (cid:29) µrn log n and
1 − O(n−100), one has

σ
σmin

(cid:113) n log n

p (cid:28) 1/κ. Then with probability exceeding

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13)2,∞ ≤ (cid:112)40κpρaug (C∞κ + 2Cop)

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ (cid:107)X (cid:63)(cid:107) .

With the help of Fact 3, we can continue the upper bound as follows

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13) ≤ 2C

√

npρaug

(cid:13)
(cid:13)At+1
2

(cid:13)
(cid:13)2,∞

(cid:46) (C∞ + Cop)

√

npκ3/2ρaug

σ
σmin

(cid:112)n log n (cid:107)F (cid:63)(cid:107)2,∞ (cid:107)X (cid:63)(cid:107) .

All in all, we obtain the following bound on At+1:

(cid:107)At+1(cid:107) ≤ (cid:107)PΩaug (E)(cid:107) + (cid:107)A1(cid:107) + (cid:13)

(cid:13)At+1
2

(cid:13)
(cid:13)

√

(cid:46) σ

np + Cτ σ(cid:112)npρaug log n + (C∞ + Cop)
√

≤ CSσ

np,

√

npκ3/2ρaug

σ
σmin

(cid:112)n log n (cid:107)F (cid:63)(cid:107)2,∞ (cid:107)X (cid:63)(cid:107)

with the proviso that ρs ≤ ρaug (cid:28) 1/
(cid:107)F (cid:63)(cid:107)2,∞ ≤ (cid:112)µr/n(cid:107)X (cid:63)(cid:107) (cf. (B.1)).
2. When it comes to Bt+1, we ﬁrst note that

(cid:112)

κ5µr log2 n. Here, the last line uses the incoherence assumption

(cid:13)X (cid:63)Y (cid:63)(cid:62) − X t+1Y t+1(cid:62)(cid:13)
(cid:13)
(cid:13)X (cid:63) − X t+1H t+1(cid:13)

≤ (cid:13)

(cid:0)X (cid:63) − X t+1H t+1(cid:1) Y (cid:63)(cid:62) + X t+1H t+1 (cid:0)Y (cid:63) − Y t+1H t+1(cid:1)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞ =
(cid:13)∞
(cid:13)
(cid:13)2,∞ (cid:107)Y (cid:63)(cid:107)2,∞ + (cid:13)

(cid:13)Y (cid:63) − Y t+1H t+1(cid:13)
(cid:13)

(cid:13)X t+1H t+1(cid:13)

(cid:13)2,∞

(cid:13)2,∞

41

≤ 3C∞κ

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2

2,∞ ≤ 3C∞κ

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

µr
n

σmax.

(F.15)

Here, we have plugged in (F.3c) for the (t+1)-th iteration and its immediate consequence (cid:107)X t+1H t+1(cid:107)2,∞ ≤
(cid:107)F t+1(cid:107)2,∞ ≤ 2(cid:107)F (cid:63)(cid:107)2,∞, as long as

p (cid:28) 1/κ. As a result, for all (i, j) we have

(cid:113) n log n

σ
σmin

(cid:12)
(cid:12)
(cid:12)

(cid:0)M − X t+1Y t+1(cid:62)(cid:1)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)

ij

(cid:0)X (cid:63)Y (cid:63)(cid:62) + E − X t+1Y t+1(cid:62)(cid:1)
(cid:32)

(cid:115)

ij

(i)

≤ (cid:12)

(cid:12)Eij

(cid:12)
(cid:12) + 3C∞κ

σ
σmin

n log n
p

+

(cid:12) + (cid:13)
(cid:12)
(cid:12)Eij
(cid:33)

(cid:12)
(cid:12) ≤ (cid:12)
(cid:12)
λ
pσmin

µr
n

σmax

(cid:13)X (cid:63)Y (cid:63)(cid:62) − X t+1Y t+1(cid:62)(cid:13)
(cid:13)∞

(ii)

≤ Cλσ(cid:112)log n = τ.

Here, the inequality (i) comes from (F.15), and the last line (ii) relies on the property of sub-Gaussian
random variables (namely, |Eij| ≤ τ /2 with probability exceeding 1 − O(n−102)) and the sample size
condition n2p (cid:29) κ4µ2r2n log n. An immediate consequence is that with probability at least 1−O(n−100),

Bt+1 = Sτ

(cid:2)PΩobs\Ωaug

(cid:0)X (cid:63)Y (cid:63)(cid:62) + E − X t+1Y t+1(cid:62)(cid:1)(cid:3) = 0.

(F.16)

Substituting the above two bounds into (F.11), we conclude that (cid:13)

(cid:13)St+1 − S(cid:63)(cid:13)

(cid:13) ≤ CSσ

√

np as claimed.

Proof of Fact 3. In view of the deﬁnition of At+1 in (F.12), we have

(cid:13)
(cid:13)At+1
2

(cid:13)2,∞ ≤ (cid:13)
(cid:13)

(cid:13)PΩaug

(cid:0)X t+1Y t+1(cid:62) − X (cid:63)Y (cid:63)(cid:62)(cid:1)(cid:13)

(cid:13)2,∞ ,

where we use the non-expansiveness of the proximal operator Sτ (·). Apply a similar argument as in bounding
(F.10) to obtain

(cid:0)X t+1Y t+1(cid:62) − X (cid:63)Y (cid:63)(cid:62)(cid:1)(cid:13)

(cid:13)2,∞

(cid:13)
(cid:13)PΩaug
≤ (cid:13)

(cid:13)PΩaug

(cid:2)(cid:0)X t+1H t+1 − X (cid:63)(cid:1) Y (cid:63)(cid:62)(cid:3)(cid:13)

(cid:13)2,∞ +
(cid:0)X t+1H t+1 − X (cid:63)(cid:1) Y (cid:63)(cid:62)(cid:13)
(cid:13)
(cid:13)

≤ (cid:112)40κpρaug

(cid:13)
(cid:13)
(cid:13)PΩaug

(cid:104)

X t+1H t+1 (cid:0)Y t+1H t+1 − Y (cid:63)(cid:1)(cid:62)(cid:105)(cid:13)
(cid:13)
(cid:13)2,∞

(cid:13)2,∞ + (cid:112)40κpρaug

(cid:13)
(cid:13)

(cid:13)X t+1H t+1 (cid:0)Y t+1H t+1 − Y (cid:63)(cid:1)(cid:62)(cid:13)

(cid:13)
(cid:13)2,∞

≤ (cid:112)40κpρaug (C∞κ + 2Cop)

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ (cid:107)X (cid:63)(cid:107) ,

as long as n2pρaug (cid:29) µrn log n. Here, the last line uses the induction hypotheses (F.3b) and (F.3c) for the
(t+1)-th iteration and their immediate consequence (cid:107)X t+1H t+1(cid:107)2,∞ ≤ 2(cid:107)F (cid:63)(cid:107)2,∞, as long as
p (cid:28)
1/κ. Taking the preceding two bounds together concludes the proof.

(cid:113) n log n

σ
σmin

F.5 Proof of Lemma 18

Without loss of generality, we only consider the case when 1 ≤ l ≤ n. The case with n + 1 ≤ l ≤ 2n can be
derived similarly with very minor modiﬁcation, and hence we omit it for the sake of brevity.

To begin with, since (H t+1, Rt+1,(l)) is the choice of the rotation matrix that best aligns F t+1 and

F t+1,(l), we have

(cid:13)
(cid:13)F t+1H t+1 − F t+1,(l)Rt+1,(l)(cid:13)

(cid:13)F ≤ (cid:13)

(cid:13)F t+1H t − F t+1,(l)Rt,(l)(cid:13)

(cid:13)F.

In view of the gradient update rule, one has

F t+1H t − F t+1,(l)Rt,(l)

= (cid:2)F t − η∇f (cid:0)F t; St(cid:1)(cid:3) H t −

(cid:104)

F t,(l) − η∇f (l)(cid:0)F t,(l); St,(l)(cid:1)(cid:105)

Rt,(l)

42

= F tH t − η∇f (cid:0)F tH t; St(cid:1) −

(cid:104)

= F tH t − F t,(l)Rt,(l) − η

(cid:104)

(cid:124)

∇faug

(cid:105)
F t,(l)Rt,(l) − η∇f (l)(F t,(l)Rt,(l); St,(l))
(cid:0)F t,(l)Rt,(l); St(cid:1)(cid:105)
(cid:125)

(cid:0)F tH t; St(cid:1) − ∇faug

(cid:123)(cid:122)
=:C1

− η
(cid:124)

(cid:104)

∇fdiﬀ

(cid:0)F tH t(cid:1) − ∇fdiﬀ

(cid:123)(cid:122)
=:C2

(cid:0)F t,(l)Rt,(l)(cid:1)(cid:105)
(cid:125)

+ η
(cid:124)

(cid:104)

∇f (l)(cid:0)F t,(l)Rt,(l); St,(l)(cid:1) − ∇f (cid:0)F t,(l)Rt,(l); St,(l)(cid:1)(cid:105)
(cid:123)(cid:122)
(cid:125)
=:C3

+ η
(cid:124)

(cid:104)

∇f (cid:0)F t,(l)Rt,(l); St,(l)(cid:1) − ∇f (cid:0)F t,(l)Rt,(l); St(cid:1)(cid:105)
(cid:123)(cid:122)
(cid:125)
=:C4

.

Here, the second identity relies on the facts that ∇f (F ; S)R = ∇f (F R; S) and ∇f (l)(F ; S)R = ∇f (l)(F R; S)
for any orthonormal matrix R ∈ Or×r. We shall then control C1, C2, C3 and C4 separately.

Employing the same strategy used to bound A1 and A2 in the proof of [CCF+20, Lemma 12], we can

demonstrate that

(cid:107)C1(cid:107)F ≤

(cid:16)

1 −

σmin
20

(cid:17) (cid:13)
(cid:13)F tH t − F t,(l)Rt,(l)(cid:13)
(cid:13)F

η

and

(cid:107)C2(cid:107)F ≤ η

(cid:18)

(cid:114) n
p

σ

+

(cid:19)

λ
p

(cid:107)F (cid:63)(cid:107)2,∞ ,

provided that

σ
σmin
deﬁnitions of ∇f and ∇f (l) that

(cid:113) n

p (cid:28) 1/(cid:112)κ4µr log n and η (cid:28) 1/(nκ2σmax). With regards to C3, it is seen from the

(cid:34)

C3 = η

(cid:2)Pl,·

(cid:0)X t,(l)Y t,(l) − L(cid:63)(cid:1) − p−1P(Ωobs)l,·
(cid:0)X t,(l)Y t,(l) − L(cid:63)(cid:1) − p−1P(Ωobs)l,·

(cid:0)X t,(l)Y t,(l) − L(cid:63)(cid:1)(cid:3) Y t,(l)Rt,(l) + p−1P(Ωobs)l,· (E) Y t,(l)Rt,(l)
(cid:0)X t,(l)Y t,(l) − L(cid:63)(cid:1)(cid:3)(cid:62)
X t,(l)Rt,(l) + p−1P(Ωobs)l,· (E)(cid:62) X t,(l)Rt,(l)

(cid:2)Pl,·

(cid:35)

,

which has the same form as A3 in the proof of [CCF+20, Lemma 12]. It thus follows from [CCF+20, Claim
5, 6 and 7] that

(cid:107)C3(cid:107)F

(cid:46) ησ

(cid:115)

n log n
p

(cid:107)F (cid:63)(cid:107)2,∞ + η

(cid:115)

µ2r2 log n
np

(cid:13)
(cid:13)

(cid:13)F t,(l)Rt,(l) − F (cid:63)(cid:13)

(cid:13)
(cid:13)2,∞

σmax,

provided that

σ
σmin

(cid:113) n

p (cid:28) 1√

κ2 log n

and that n2p (cid:29) n log3 n.

We are then left with controlling the term C4. Towards this, we invoke the deﬁnition of f to decompose

(cid:34)

(cid:34)

C4 = η

=

η
p
(cid:124)

p−1PΩobs
p−1PΩobs

P−l,·
(cid:2)P−l,·

(cid:0)St,(l) − St(cid:1) Y t,(l)Rt,(l)
(cid:0)St,(l) − St(cid:1)(cid:62)
X t,(l)Rt,(l)
(cid:35)

(cid:0)St,(l) − St(cid:1)Y t,(l)Rt,(l)
(cid:0)St,(l) − St(cid:1)(cid:3)(cid:62)

X t,(l)Rt,(l)

(cid:123)(cid:122)
=:D1

(cid:125)

(cid:35)

(cid:34)

+

η
p
(cid:124)

Pl,·
(cid:2)Pl,·

(cid:0)St,(l) − St(cid:1) Y t,(l)Rt,(l)
(cid:0)St,(l) − St(cid:1)(cid:3)(cid:62)
X t,(l)Rt,(l)

(cid:123)(cid:122)
=:D2

(cid:35)

.

(cid:125)

Here, we have used the fact that both St,(l) and St are supported on Ω(cid:63) ⊆ Ωobs. Regarding the ﬁrst matrix
D1, we have the following fact.

Fact 4. Suppose that the sample size obeys n2p (cid:29) κ4µ2r2n log n, the noise satisﬁes
p (cid:28) 1/κ,
the outlier fraction satisﬁes ρs ≤ ρaug (cid:28) 1/κ3 and n2pρaug (cid:29) µrn log n hold. Then with probability at least
1 − O(n−100), we have

σ
σmin

(cid:113) n log n

(cid:115)

(cid:13)
(cid:13)D1

(cid:13)
(cid:13)F

(cid:46) ησ

n log n
p

(cid:13)
(cid:13)F (cid:63)(cid:13)

(cid:13)2,∞.

With regards to D2, recall that St,(l)

l,· = S(cid:63)
l,·

. Using the decomposition (F.11) in the proof of Lemma 17,

and recalling that Bt+1 = 0 from the proof of Lemma 17, we obtain

Pl,·

(cid:0)St,(l) − St(cid:1)Y t,(l)Rt,(l) = Pl,·

(cid:0)A1 + E(cid:1)Y t,(l)Rt,(l) + Pl,·

(cid:0)At

2

(cid:1)Y t,(l)Rt,(l),

(F.17)

43

where

A1 = Sτ
At
2 := Sτ

(cid:2)PΩaug (S(cid:63) + E)(cid:3) − PΩaug (S(cid:63) + E) ;
(cid:2)PΩaug

(cid:0)X (cid:63)Y (cid:63)(cid:62) − X tY t(cid:62) + S(cid:63) + E(cid:1)(cid:3) − Sτ

(cid:2)PΩaug (S(cid:63) + E)(cid:3) .

For the ﬁrst term Pl,·(A1 + E)Y t,(l)Rt,(l), the independence between Y t,(l)Rt,(l) and the l-th row of A1 + E
allows us to obtain the following bound.

Fact 5. Suppose that ρs ≤ ρaug (cid:28) 1/ log n and that n2p (cid:29) n log4 n. Then with probability at least 1 −
O(n−100), we have

(cid:13)
(cid:13)

(cid:13)Pl,· (A1) Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

(cid:46) σ(cid:112)np log n (cid:107)Y (cid:63)(cid:107)2,∞ .
is controlled in the following claim, which relies heavily on the small scale of the

The term involving At
2

entries in At
2

.

Fact 6. Suppose that n (cid:29) κµr,
with probability at least 1 − O(n−100), we have

σ
σmin

(cid:113) n log n

p (cid:28) 1/κ, ρs ≤ ρaug (cid:28) 1/(κµr) and that n2pρaug (cid:29) n log n. Then

(cid:1) Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F
Combining the two bounds in Facts 5 and 6 gives

(cid:13)
(cid:13)
(cid:13)Pl,·

(cid:0)At

2

(cid:46) σ(cid:112)np log n (cid:107)Y (cid:63)(cid:107)2,∞ .

(cid:13)
(cid:13)
(cid:13)Pl,·

(cid:0)St,(l) − St(cid:1)Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

(cid:46) σ(cid:112)np log n (cid:107)Y (cid:63)(cid:107)2,∞ .

The same bound applies to (cid:107)Pl,·(St,(l) − St)(cid:62)X t,(l)Rt,(l)(cid:107)F via the same technique. As a result, we have
(cid:115)

(cid:107)D2(cid:107)F (cid:46) ησ

Putting the above bounds together yields

n log n
p

(cid:107)F (cid:63)(cid:107)2,∞.

(cid:13)F t+1H t+1 − F t+1,(l)Rt+1,(l)(cid:13)
(cid:13)

(cid:13)F ≤ (cid:107)C1(cid:107)F + (cid:107)C2(cid:107)F + (cid:107)C3(cid:107)F + (cid:107)D1(cid:107)F + (cid:107)D2(cid:107)F

(cid:16)

≤

1 −

σmin
20

η

(cid:17) (cid:13)
(cid:13)

(cid:13)F tH t − F t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

(cid:18)

+ η

σ

(cid:19)

(cid:114) n
p

+

λ
p

(cid:107)F (cid:63)(cid:107)2,∞

(cid:32)

(cid:115)

+ ˜C

ησ

n log n
p

(cid:107)F (cid:63)(cid:107)2,∞ + η

(cid:115)

µ2r2 log n
np

(cid:13)
(cid:13)

(cid:13)F t,(l)Rt,(l) − F (cid:63)(cid:13)

(cid:13)
(cid:13)2,∞

(cid:33)

(cid:115)

σmax

+ ˜Cησ

n log n
p

(cid:13)F (cid:63)(cid:13)
(cid:13)

(cid:13)2,∞

(i)
≤

(cid:16)

1 −

σmin
20

(cid:17)

η

C1

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

(cid:33)

(cid:107)F (cid:63)(cid:107)2,∞ + η

(cid:18)

(cid:114) n
p

σ

+

(cid:19)

λ
p

(cid:107)F (cid:63)(cid:107)2,∞ + ˜C

λ
p

(cid:13)F (cid:63)(cid:13)
(cid:13)

(cid:13)2,∞

(cid:32)

(C∞κ + C1)

σ
σmin

n log n
p

(cid:33)

+

λ
pσmin

(cid:107)F (cid:63)(cid:107)2,∞ σmax + ˜Cησ

(cid:115)

n log n
p

(cid:13)
(cid:13)F (cid:63)(cid:13)

(cid:13)2,∞

λ
pσmin
(cid:115)

(cid:115)

+ ˜Cη

µ2r2 log n
np
(cid:115)

(cid:32)

(ii)
≤ C1

σ
σmin

n log n
p

(cid:33)

+

λ
pσmin

(cid:107)F (cid:63)(cid:107)2,∞ ,

where (i) invokes (F.6a) and its immediate consequence that

(cid:13)
(cid:13)

(cid:13)F t,(l)Rt,(l) − F (cid:63)(cid:13)

(cid:13)
(cid:13)2,∞

≤

(cid:13)
(cid:13)

(cid:13)F tH t − F t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

+ (cid:13)

(cid:13)F tH t − F (cid:63)(cid:13)
(cid:33)

(cid:13)2,∞

≤ (C∞κ + C1)

(cid:115)

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:107)F (cid:63)(cid:107)2,∞ .

(F.18)

(F.19)

The last line (ii) holds as long as n2p (cid:29) κ4µ2r2n log n and C1 is large enough.

44

σ
σmin

(cid:113) n log n

Proof of Fact 4. First notice that St is supported on Ωaug, which is a consequence of (F.11) and (F.16)
p (cid:28) 1/κ and n2p (cid:29) κ4µ2r2n log n. By replacing X t+1 (resp. Y t+1) with X t+1,(l)
as long as
(resp. Y t+1,(l))) and invoking (F.19) instead of (F.3c), the same arguments yield the fact that St,(l) is also
supported on Ωaug. Deﬁne ωij := 1(i,j)∈Ωaug
. The Frobenius norm of the upper block of D1 can be bounded
by

(cid:13)
(cid:13)
(cid:13)P−l,·

(cid:0)St,(l) − St(cid:1)Y t,(l)Rt,(l)(cid:13)
2
(cid:13)
(cid:13)

F

=

r
(cid:88)

(cid:34) n
(cid:88)

(cid:88)

i:i(cid:54)=l

j=1

k=1

(cid:16)

St,(l) − St(cid:17)

Y t,(l)
kj

ik

(cid:35)2

r
(cid:88)

(cid:34) n
(cid:88)

(cid:88)

=

i:i(cid:54)=l

j=1

k=1

(cid:16)

St,(l) − St(cid:17)

ωik

Y t,(l)
kj

ik

(cid:35)2

≤

r
(cid:88)

(cid:34) n
(cid:88)

(cid:88)

i:i(cid:54)=l

j=1

k=1

(cid:16)

ωik

St,(l) − St(cid:17)2

ik

(cid:35) (cid:34) n
(cid:88)

k=1

ωik

(cid:0)Y t,(l)

kj

(cid:1)2

(cid:35)

,

where we use the Cauchy-Schwarz inequality in the last step. Converting to the matrix notation, we obtain

n
(cid:88)

k=1

ωik

(cid:0)Y t,(l)

kj

(cid:1)2

=

(cid:16)

(cid:13)
(cid:13)
(cid:13)PΩaug

eie(cid:62)

j Y t,(l)(cid:62)(cid:17)(cid:13)
2
(cid:13)
(cid:13)

F

.

Applying a similar argument as in bounding (F.10), one can obtain from Lemma 4 that

r
(cid:88)

j=1

(cid:16)

(cid:13)
(cid:13)
(cid:13)PΩaug

eie(cid:62)

j Y t,(l)(cid:62)(cid:17)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:46) κpρaug

r
(cid:88)

(cid:13)
(cid:13)Y t,(l)

·,j

j=1

(cid:13)
2
F = κpρaug(cid:107)Y t,(l)(cid:107)2
F,
(cid:13)

provided that n2pρaug (cid:29) µrn log n. This allows us to reach

(cid:13)
(cid:13)
(cid:13)P−l,·

(cid:0)St,(l) − St(cid:1)Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

(cid:46)

(cid:34) n
(cid:88)

(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

i:i(cid:54)=l

k=1

ωik

(cid:0)St,(l) − St(cid:1)2

ik

(cid:35)

√

·

κpρaug

(cid:13)Y t,(l)(cid:13)
(cid:13)
(cid:13)F

√

=

κpρaug
√

κpρaug

≤ C3
(cid:46) σ(cid:112)np log n(cid:13)

(cid:16)

(cid:13)
(cid:13)P−l,·
σ
σmin
(cid:13)F (cid:63)(cid:13)

(cid:13)2,∞.

St,(l) − St(cid:17)(cid:13)
(cid:13)F
(cid:112)n log n(cid:13)
(cid:13)F (cid:63)(cid:13)
(cid:13)

(cid:13)Y t,(l)(cid:13)
(cid:13)
(cid:13)F
(cid:13)F (cid:63)(cid:13)
2(cid:13)

(cid:13)2,∞

Here, the penultimate step comes from the hypothesis (F.6c), whereas the last step holds as long as ρs ≤
ρaug (cid:28) 1/κ3. The Frobenius norm of the lower block of D1 admits the same bound. As a result, we obtain
(cid:13)
(cid:13)D1

as claimed.

(cid:13)F (cid:63)(cid:13)
(cid:13)

(cid:46) ησ

(cid:113) n log n
p

(cid:13)2,∞

(cid:13)
(cid:13)F

Proof of Fact 5. Regarding the ﬁrst term on the right-hand side of (F.17), we can write

(cid:13)
(cid:13)Pl,·

(cid:0)A1 + E(cid:1)Y t,(l)Rt,(l)(cid:13)

(cid:13)F =

(cid:13)
(cid:13)
(cid:13)

(cid:88)n

j=1

(cid:0)A1 + E(cid:1)

ljY t,(l)

j,·

(cid:13)
(cid:13)
(cid:13)2

=

(cid:13)
(cid:13)
(cid:13)

(cid:88)n

j=1

ωlj
(cid:124)

(cid:2)Sτ (S(cid:63)

lj + Eij) − S(cid:63)
lj
(cid:123)(cid:122)
=:uj

(cid:3) Y t,(l)
(cid:125)

j,·

(cid:13)
(cid:13)
(cid:13)2

,

where ωlj := 1{(l, j) ∈ Ωaug} is a Bernoulli random variable with mean pρaug. Since Y t,(l) is independent
of {ωlj}1≤j≤n and S(cid:63)
are statistically independent conditional on Y t,(l). We can thus
l,·
apply the matrix Bernstein inequality to control this term. Speciﬁcally, conditional on Y t,(l), we have

, the vectors {uj}n

j=1

(cid:107)(cid:107)uj(cid:107)2(cid:107)ψ1

≤ (cid:13)

(cid:13)Y t,(l)(cid:13)

(cid:13)2,∞

(cid:13)
(cid:13)ωlj

(cid:2)Sτ (S(cid:63)

lj + Eij) − S(cid:63)
lj

(i)
(cid:46) τ

(cid:3)(cid:13)
(cid:13)ψ1

(cid:13)
(cid:13)

(cid:13)Y t,(l)(cid:13)

(cid:13)
(cid:13)2,∞

,

(cid:104)(cid:88)n

(cid:13)
(cid:13)
(cid:13)

E

V :=

(cid:13)Y t,(l)(cid:13)
2
F,
(cid:13)
denotes the sub-exponential norm [Ver17]. Here, the relation (i) holds since

lj + Eij) − S(cid:63)
lj

j,· Y t,(l)(cid:62)
Y t,(l)

(cid:46) pρaugτ 2(cid:13)

(cid:0)Sτ (S(cid:63)

ω2
lj

j=1

(cid:1)2

j,·

(ii)

(cid:105)(cid:13)
(cid:13)
(cid:13)

where (cid:107) · (cid:107)ψ1

(cid:13)
(cid:13)ωlj

(cid:2)Sτ (S(cid:63)

lj + Eij) − S(cid:63)
lj

(cid:3)(cid:13)
(cid:13)ψ1

≤ (cid:13)

(cid:13)Sτ (S(cid:63)

lj + Eij) − S(cid:63)
lj

(cid:13)
(cid:13)ψ1

45

≤ (cid:13)

(cid:13)Sτ (S(cid:63)

lj + Eij) − (cid:0)S(cid:63)

lj + Eij

(cid:1)(cid:13)
(cid:13)ψ1

+ (cid:107)Eij(cid:107)ψ1

≤ (cid:12)

(cid:12)Sτ (S(cid:63)

lj + Eij) − (cid:0)S(cid:63)

lj + Eij

(cid:1)(cid:12)
(cid:12) + (cid:107)Eij(cid:107)ψ2

≤ 2τ,

where we have used the fact that |Sτ (x) − x| ≤ τ and (cid:107)Eij(cid:107)ψ1 ≤ (cid:107)Eij(cid:107)ψ2 ≤ σ ≤ τ . In addition, the second
inequality (ii) comes from the identity E[ω2

lj] = pρaug and the fact that

E

(cid:104)(cid:0)Sτ (S(cid:63)

lj + Eij) − S(cid:63)
lj

(cid:1)2(cid:105)

≤ 2E

(cid:104)(cid:0)Sτ (S(cid:63)

lj + Eij) − S(cid:63)

lj − Eij

(cid:1)2(cid:105)

+ 2E (cid:2)E2

ij

(cid:3) (cid:46) τ 2.

With the aid of the above bounds, we can invoke the matrix Bernstein inequality [KLT11, Proposition 2] to
reach

(cid:88)n

(cid:13)
(cid:13)
(cid:13)

j=1

uj

(cid:13)
(cid:13)
(cid:13)2

(cid:46) (cid:112)V log n + (cid:107)(cid:107)uj(cid:107)2(cid:107)ψ1

log2 n

(cid:113)

(cid:46)

(cid:46)

(cid:13)Y t,(l)(cid:13)
F log n + τ (cid:13)
(cid:13)Y t,(l)(cid:13)
pρaugτ 2 (cid:13)
2
(cid:13)
(cid:17) (cid:13)
τ (cid:112)npρaug log n + τ log2 n
(cid:13)Y t,(l)(cid:13)

(cid:16)

(cid:13)2,∞

(cid:13)2,∞ log2 n

with probability at least 1 − O(n−10). Here, the last inequality arises from (cid:107)Y t,(l)(cid:107)2
Consequently, we conclude that, with high probability,

F ≤ n(cid:107)Y t,(l)(cid:107)2

2,∞

.

(cid:13)
(cid:13)

(cid:13)Pl,· (A1 + E) Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

(cid:46)

(cid:16)
τ (cid:112)npρaug log n + τ log2 n

(cid:17) (cid:13)
(cid:13)Y t,(l)(cid:13)

(cid:13)2,∞

(cid:46) σ(cid:112)np log n (cid:107)Y (cid:63)(cid:107)2,∞ ,

with the proviso that ρs ≤ ρaug (cid:28) 1/ log n and n2p (cid:29) n log4 n.

Proof of Fact 6. Regarding the second term on the right-hand side of (F.17), we have

(cid:13)
(cid:13)
(cid:13)Pl,·

(cid:0)At

2

(cid:1) Y t,(l)Rt,(l)(cid:13)
(cid:13)
(cid:13)F

=

(cid:13)
(cid:13)
(cid:13)

(cid:88)n

j=1

(cid:1)

(cid:0)At

2

lj Y t,(l)

j,·

(cid:13)
(cid:13)
(cid:13)2
(cid:13)Y t,(l)(cid:13)
(cid:13)
(cid:115)

(cid:13)
(cid:13)∞
(cid:32)

(cid:13)2,∞

(i)
≤ 2npρaug

(cid:13)
(cid:13)At
2

(ii)
≤ 12npρaugC∞

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

µr
n

σmax (cid:107)Y (cid:63)(cid:107)2,∞

(iii)

(cid:46) σ(cid:112)np log n (cid:107)Y (cid:63)(cid:107)2,∞ .

2)lj (cid:54)= 0} ⊆ {j | (l, j) ∈ Ωaug}, whose
Here, the ﬁrst upper bound (i) arises from the fact that {j | (At
cardinality is upper bounded by 2npρaug with high probability as long as npρaug (cid:29) log n. The second
inequality (ii) comes from the simple fact that (cid:107)Y t,(l)(cid:107)2,∞ ≤ 2(cid:107)Y (cid:63)(cid:107)2,∞ as well as the bound
(cid:2)PΩaug (S(cid:63) + E)(cid:3)(cid:13)
(cid:13)∞
(cid:0)X (cid:63)Y (cid:63)(cid:62) − X tY t(cid:62) + S(cid:63) + E(cid:1) − PΩaug (S(cid:63) + E)(cid:13)
(cid:13)∞

(cid:0)X (cid:63)Y (cid:63)(cid:62) − X tY t(cid:62) + S(cid:63) + E(cid:1)(cid:3) − Sτ

(cid:2)PΩaug

(cid:13)
(cid:13)At
2

(cid:13)∞ = (cid:13)
(cid:13)
≤ (cid:13)
≤ (cid:13)

(cid:13)Sτ
(cid:13)PΩaug
(cid:13)X (cid:63)Y (cid:63)(cid:62) − X tY t(cid:62)(cid:13)
(cid:13)∞

(cid:115)

≤ 3C∞

(cid:32)

σ
σmin

n log n
p

+

λ
pσmin

(cid:33)

µr
n

σmax,

where we use the non-expansiveness of Sτ (·) and the established bound (F.15), which holds as long as
p (cid:28) 1/κ. Last but not least, the relation (iii) holds as long as ρs ≤ ρaug (cid:28) 1/(κµr) and

(cid:113) n log n

σ
σmin
n (cid:29) κµr.

F.6 Proof of Lemma 19

Without loss of generality, we assume 1 ≤ l ≤ n. Following the deﬁnitions of St+1,(l) and St+1, we have

(cid:13)
(cid:13)P−l,·

(cid:0)St+1,(l) − St+1(cid:1)(cid:13)

(cid:13)F =

(cid:13)
(cid:13)
(cid:13)P−l,·

(cid:104)
Sτ

(cid:0)M − X t+1,(l)Y t+1,(l)(cid:62)(cid:1) − Sτ

(cid:0)M − X t+1Y t+1(cid:62)(cid:1)(cid:105)(cid:13)
(cid:13)
(cid:13)F

46

≤ (cid:13)

(cid:13)PΩaug (∆)(cid:13)

(cid:13)F +

(cid:13)
(cid:13)
(cid:13)PΩc

aug

(∆)

(cid:13)
(cid:13)
(cid:13)F

,

(F.20)

where we denote ∆ := Sτ (M − X t+1,(l)Y t+1,(l)(cid:62)) − Sτ (M − X t+1Y t+1(cid:62)). Recall from Appendix A that
each (i, j) is included in Ωaug independently with probability pρaug, where 1 ≥ ρaug ≥ ρs.
1. For the ﬁrst term (cid:107)PΩaug (∆) (cid:107)F, the non-expansiveness of the proximal operator Sτ (·) yields

(cid:13)PΩaug (∆)(cid:13)
(cid:13)

(cid:13)F ≤

(cid:13)
(cid:13)
(cid:13)PΩaug

(cid:0)X t+1,(l)Y t+1,(l)(cid:62) − X t+1Y t+1(cid:62)(cid:1)(cid:13)
(cid:13)
(cid:13)F

.

Apply Lemma 4 and a similar argument in bounding (F.10) to obtain

(cid:13)PΩaug (∆)(cid:13)
(cid:13)

(cid:13)F ≤

(cid:13)
(cid:13)
PΩaug
(cid:13)
(cid:13)

(cid:20)
X t+1H t+1 (cid:16)
(cid:104)(cid:16)

Y t+1,(l)Rt+1,(l) − Y t+1H t+1(cid:17)(cid:62)(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F
Rt+1,(l)(cid:62)Y t+1,(l)(cid:62)(cid:105)(cid:13)
(cid:13)
(cid:13)F

X t+1,(l)Rt+1,(l) − X t+1H t+1(cid:17)

(cid:13)
(cid:13)
(cid:13)PΩaug

+
(cid:46) √

+

κpρaug
√

κpρaug

(cid:13)X t+1H t+1(cid:13)
(cid:13)
(cid:13)
(cid:13)Y t+1,(l)H t+1,(l)(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)Y t+1,(l)Rt+1,(l) − Y t+1H t+1(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)

(cid:13)X t+1,(l)Rt+1,(l) − X t+1H t+1(cid:13)
(cid:13)
(cid:13)F
In view of (F.6a) and the simple facts (cid:107)X t+1H t+1(cid:107) ≤

,

with the proviso that n2pρaug (cid:29) µrn log n.
2(cid:107)X (cid:63)(cid:107), (cid:107)Y t+1,(l)H t+1,(l)(cid:107) ≤ 2(cid:107)X (cid:63)(cid:107), one has

(cid:13)PΩaug (∆)(cid:13)
(cid:13)
(cid:13)F

(cid:46) √

κpρaug (cid:107)X (cid:63)(cid:107)

(cid:32)

(cid:115)

σ
σmin
(cid:112)n log n (cid:107)F (cid:63)(cid:107)2,∞ (cid:107)F (cid:63)(cid:107) ,

n log n
p

+

(cid:33)

λ
pσmin

(cid:107)F (cid:63)(cid:107)2,∞

≤ C3

σ
σmin

provided that ρaug (cid:28) 1/κ.

2. Regarding the second term (cid:107)PΩc

aug

(∆) (cid:107)F, we ﬁrst recall from (F.16) that
(cid:0)M − X t+1Y t+1(cid:62)(cid:1)(cid:105)

(cid:104)
PΩc

= 0.

Sτ

aug

By replacing X t+1 (resp. Y t+1) with X t+1,(l) (resp. Y t+1,(l))) and invoking (F.19) instead of (F.3c), the
same arguments that we used to prove (F.16) also allow us to demonstrate

(cid:104)

Sτ

PΩc

aug

(cid:16)

M − X t+1,(l)Y t+1,(l)(cid:62)(cid:17)(cid:105)

= 0

provided that n2p (cid:29) κ4µ2r2n log n and σ
σmin

(cid:113) n log n

p (cid:28) 1/κ. Consequently, we have PΩc

aug

(∆) = 0.

Substituting the above two bounds into (F.20), we conclude that

(cid:13)
(cid:13)
(cid:13)P−l,·

(cid:0)St+1,(l) − St+1(cid:1)(cid:13)
(cid:13)
(cid:13)F

≤ (cid:13)

(cid:13)PΩaug (∆)(cid:13)

(cid:13)F ≤ C3

σ
σmin

(cid:112)n log n (cid:107)F (cid:63)(cid:107)2,∞ (cid:107)F (cid:63)(cid:107) .

F.7 Proof of Lemma 20

Following [CCF+20, Lemma 18], we already know that

f (cid:0)X t+1, Y t+1; St(cid:1) ≤ f (cid:0)X t, Y t; St(cid:1) −

η
2

(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
F .
(cid:13)

(F.21)

As a result, one has

F (cid:0)X t+1, Y t+1, St+1(cid:1) (i)

≤ F (cid:0)X t+1, Y t+1, St(cid:1) = f (cid:0)X t+1, Y t+1; St(cid:1) + τ (cid:13)

(cid:13)St(cid:13)
(cid:13)1

47

(ii)

≤ f (cid:0)X t, Y t; St(cid:1) −

= F (cid:0)X t, Y t, St(cid:1) −

η
2
η
2

F + τ (cid:13)
(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)∇f (cid:0)X t, Y t; St(cid:1)(cid:13)
(cid:13)
2
F ,
(cid:13)

(cid:13)St(cid:13)
(cid:13)1

where (i) follows since, by construction, St+1 is the minimizer of F (X t+1, Y t+1, S) for any given (X t+1, Y t+1),
and (ii) arises from (F.21).

References

[ANW12] Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Noisy matrix decomposition via
convex relaxation: Optimal rates in high dimensions. The Annals of Statistics, 40(2):1171–1197,
2012.

[ARR14]

Ali Ahmed, Benjamin Recht, and Justin Romberg. Blind deconvolution using convex program-
ming. IEEE Transactions on Information Theory, 60(3):1711–1732, 2014.

[CC14]

[CC17]

[CC18]

Yuxin Chen and Yuejie Chi. Robust spectral compressed sensing via structured matrix comple-
tion. IEEE Transactions on Information Theory, 60(10):6576 – 6601, 2014.

Yuxin Chen and Emmanuel J. Candès. Solving random quadratic systems of equations is nearly
as easy as solving linear systems. Comm. Pure Appl. Math., 70(5):822–883, 2017.

Yuxin Chen and Emmanuel Candès. The projected power method: An eﬃcient algorithm for
joint alignment from pairwise diﬀerences. Communications on Pure and Applied Mathematics,
71(8):1648–1714, 2018.

[CCD+19] Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Díaz, Lijun Ding, and Dmitriy
Drusvyatskiy. Low-rank matrix recovery with composite optimization: good conditioning and
rapid convergence. arXiv preprint arXiv:1904.10020, 2019.

[CCF+20] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion:
Understanding statistical guarantees for convex relaxation via nonconvex optimization. SIAM
Journal on Optimization, 30(4):3098–3121, 2020.

[CCFM19] Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random initial-
ization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
176(1-2):5–37, July 2019.

[CCFM20] Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Spectral methods for data science: A

statistical perspective. arXiv preprint arXiv:2012.08496, 2020.

[CCW19] HanQin Cai, Jian-Feng Cai, and Ke Wei. Accelerated alternating projections for robust principal

component analysis. The Journal of Machine Learning Research, 20(1):685–717, 2019.

[CDDD19] Vasileios Charisopoulos, Damek Davis, Mateo Díaz, and Dmitriy Drusvyatskiy. Composite
optimization for robust blind deconvolution. arXiv preprint arXiv:1901.01624, 2019.

[CFMY19] Yuxin Chen, Jianqing Fan, Cong Ma, and Yuling Yan. Inference and uncertainty quantiﬁcation
for noisy matrix completion. Proceedings of the National Academy of Sciences, 116(46):22931–
22937, 2019.

[CFWY20] Yuxin Chen, Jianqing Fan, Bingyan Wang, and Yuling Yan. Convex and nonconvex optimization
are both minimax-optimal for noisy blind deconvolution. arXiv preprint arXiv:2008.01724, 2020.

[CGH14]

Y. Chen, L. J. Guibas, and Q. Huang. Near-optimal joint optimal matching via convex relax-
ation. International Conference on Machine Learning (ICML), pages 100 – 108, June 2014.

48

[CGJ17]

Yeshwanth Cherapanamjeri, Kartik Gupta, and Prateek Jain. Nearly optimal robust matrix
completion. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 797–805. JMLR. org, 2017.

[Che15]

Yudong Chen.
Theory, 61(5):2909–2923, 2015.

Incoherence-optimal matrix completion.

IEEE Transactions on Information

[CJSC13] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Constantine Caramanis. Low-rank matrix recovery

from errors and erasures. IEEE Transactions on Information Theory, 59(7):4324–4337, 2013.

[CLC19]

Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix fac-
torization: An overview. IEEE Transactions on Signal Processing, 67(20):5239 – 5269, October
2019.

[CLL20]

Ji Chen, Dekai Liu, and Xiaodong Li. Nonconvex rectangular matrix completion via gradient
descent without (cid:96)2,∞ regularization. IEEE Transactions on Information Theory, 2020.

[CLMW11] Emmanuel Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analy-

sis? Journal of ACM, 58(3):11:1–11:37, Jun 2011.

[CLPC20] Changxiao Cai, Gen Li, H Vincent Poor, and Yuxin Chen. Nonconvex low-rank tensor comple-

tion from noisy data. Accepted to Operations Research, 2020.

[CLS15]

E. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger ﬂow: Theory and
algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, April 2015.

[CMW13] T Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: Optimal rates and adaptive estima-

tion. The Annals of Statistics, 41(6):3074–3110, 2013.

[CP10]

Emmanuel Candès and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE,
98(6):925 –936, June 2010.

[CPW12] Venkat Chandrasekaran, Pablo A Parrilo, and Alan S Willsky. Latent variable graphical model

selection via convex optimization. Annals of Statistics, 40(4):1935–1967, 2012.

[CR09]

Emmanuel Candès and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational Mathematics, 9(6):717–772, April 2009.

[CSPW11] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-sparsity

incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596, 2011.

[CT10]

[CW15]

[CW18]

[DC20]

[DG14]

[DR16]

Emmanuel Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053 –2080, May 2010.

Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:
General statistical and algorithmic guarantees. arXiv:1509.03025, 2015.

Jian-Feng Cai and Ke Wei. Solving systems of phaseless equations via riemannian optimization
with optimal sampling complexity. arXiv preprint arXiv:1809.02773, 2018.

Lijun Ding and Yudong Chen. Leave-one-out approach for matrix completion: Primal and dual
analysis. IEEE Transactions on Information Theory, 2020.

David Donoho and Matan Gavish. Minimax risk of matrix denoising by singular value thresh-
olding. The Annals of Statistics, 42(6):2413–2440, 2014.

Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from in-
complete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622,
2016.

49

[FFL08]

[FLM13]

Jianqing Fan, Yingying Fan, and Jinchi Lv. High dimensional covariance matrix estimation
using a factor model. Journal of Econometrics, 147(1):186–197, 2008.

J. Fan, Y. Liao, and M. Mincheva. Large covariance estimation by thresholding principal or-
thogonal complements. Journal of the Royal Stat. Society: Series B, 75(4):603–680, 2013.

[FSZZ18]

Jianqing Fan, Qiang Sun, Wen-Xin Zhou, and Ziwei Zhu. Principal component analysis for big
data. arXiv preprint arXiv:1801.01602, 2018.

[FWZ18]

[FWZ19]

[FXY13]

[GMS13]

[GQV14]

Jianqing Fan, Weichen Wang, and Yiqiao Zhong. An (cid:96)∞ eigenvector perturbation bound and
its application. Journal of Machine Learning Research, 18(207):1–42, 2018.

Jianqing Fan, Weichen Wang, and Yiqiao Zhong. Robust covariance estimation for approximate
factor models. Journal of econometrics, 208(1):5–22, 2019.

Jiashi Feng, Huan Xu, and Shuicheng Yan. Online robust PCA via stochastic optimization. In
Advances in Neural Information Processing Systems, pages 404–412, 2013.

Donald Goldfarb, Shiqian Ma, and Katya Scheinberg. Fast alternating linearization methods
for minimizing the sum of two convex functions. Mathematical Programming, 141(1-2):349–382,
2013.

Han Guo, Chenlu Qiu, and Namrata Vaswani. An online algorithm for separating sparse and
low-dimensional signal sequences from their sum.
IEEE Transactions on Signal Processing,
62(16):4284–4297, 2014.

[Gro11]

David Gross. Recovering low-rank matrices from few coeﬃcients in any basis. IEEE Transactions
on Information Theory, 57(3):1548–1566, March 2011.

[GWL+10] Arvind Ganesh, John Wright, Xiaodong Li, Emmanuel J Candes, and Yi Ma. Dense error
correction for low-rank matrices via principal component pursuit. In 2010 IEEE international
symposium on information theory, pages 1513–1517. IEEE, 2010.

[GWL16] Quanquan Gu, Zhaoran Wang Wang, and Han Liu. Low-rank and sparse structure pursuit via

alternating minimization. In Artiﬁcial Intelligence and Statistics, pages 600–609, 2016.

[HG13]

[HKZ11]

Q. Huang and L. Guibas. Consistent shape maps via semideﬁnite programming. Computer
Graphics Forum, 32(5):177–186, 2013.

Daniel Hsu, Sham M Kakade, and Tong Zhang. Robust matrix decomposition with sparse
corruptions. IEEE Transactions on Information Theory, 57(11):7221–7234, 2011.

[JCSX11] Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. In International Conference on Machine Learning, volume 11, pages
1001–1008, 2011.

[JNS13]

[Jol11]

[Klo14]

[KLT11]

[KLT17]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating mini-
mization. In ACM symposium on Theory of computing, pages 665–674, 2013.

Ian Jolliﬀe. Principal component analysis. Springer, 2011.

Olga Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli,
20(1):282–303, 2014.

Vladimir Koltchinskii, Karim Lounici, and Alexandre B. Tsybakov. Nuclear-norm penalization
and optimal rates for noisy low-rank matrix completion. Ann. Statist., 39(5):2302–2329, 2011.

Olga Klopp, Karim Lounici, and Alexandre B Tsybakov. Robust matrix completion. Probability
Theory and Related Fields, 169(1-2):523–564, 2017.

50

[KMO10] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.

IEEE

Transactions on Information Theory, 56(6):2980 –2998, June 2010.

[KS20]

[Li13]

Felix Krahmer and Dominik Stöger. On the convex geometry of blind deconvolution and matrix
completion. Communications on Pure and Applied Mathematics, 2020.

Xiaodong Li. Compressed sensing and matrix completion with constant proportion of corrup-
tions. Constructive Approximation, 37:73–99, 2013.

[LMCC19] Yuanxin Li, Cong Ma, Yuxin Chen, and Yuejie Chi. Nonconvex matrix factorization from
rank-one measurements. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pages 1496–1505, 2019.

[LWC+20] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier
in model-based reinforcement learning with a generative model. Neural Information Processing
Systems, 2020.

[MA18]

Shiqian Ma and Necdet Serhat Aybat. Eﬃcient optimization algorithms for robust principal
component analysis and its variants. Proceedings of the IEEE, 106(8):1411–1426, 2018.

[MHT10]

R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning
large incomplete matrices. Journal of machine learning research, 11(Aug):2287–2322, 2010.

[MWCC20] Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion,
and blind deconvolution. Foundations of Computational Mathematics, 20(3):451–632, 2020.

[NJS13]

P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. Advances
in Neural Information Processing Systems (NIPS), 2013.

[NNS+14] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Non-convex robust PCA.

In Advances in Neural Information Processing Systems, pages 1107–1115, 2014.

[NW12]

[Pea01]

[QV10]

S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion:
Optimal bounds with noise. Journal of Machine Learning Research, pages 1665–1697, May 2012.

Karl Pearson. Liii. on lines and planes of closest ﬁt to systems of points in space. The London,
Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559–572, 1901.

Chenlu Qiu and Namrata Vaswani. Real-time robust principal components’ pursuit. In 2010
48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages
591–598. IEEE, 2010.

[QVLH14] Chenlu Qiu, Namrata Vaswani, Brian Lois, and Leslie Hogben. Recursive robust pca or recur-
sive sparse recovery in large but structured noise. IEEE Transactions on Information Theory,
60(8):5007–5039, 2014.

[Sin11]

[SL16]

[SS05]

[SWZ14]

Amit Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Applied
and computational harmonic analysis, 30(1):20–36, 2011.

Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.
IEEE Transactions on Information Theory, 62(11):6535–6579, 2016.

Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Confer-
ence on Computational Learning Theory, pages 545–560. Springer, 2005.

Yuan Shen, Zaiwen Wen, and Yin Zhang. Augmented lagrangian alternating direction method
for matrix separation based on low-rank factorization. Optimization Methods and Software,
29(2):239–263, 2014.

51

[TY11]

[Ver12]

Min Tao and Xiaoming Yuan. Recovering low-rank and sparse components of matrices from
incomplete and noisy observations. SIAM Journal on Optimization, 21(1):57–81, 2011.

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed
Sensing, Theory and Applications, pages 210 – 268, 2012.

[Ver17]

Roman Vershynin. High dimensional probability, 2017.

[VN18]

Namrata Vaswani and Praneeth Narayanamurthy. Static and dynamic robust pca and matrix
completion: A review. Proceedings of the IEEE, 106(8):1359–1379, 2018.

[WCCL16] K. Wei, J.F. Cai, T. Chan, and S. Leung. Guarantees of Riemannian optimization for low rank

matrix recovery. SIAM Journal on Matrix Analysis and Applications, 37(3):1198–1222, 2016.

[WGE17] Gang Wang, Georgios B Giannakis, and Yonina C Eldar. Solving systems of random quadratic

equations via truncated amplitude ﬂow. IEEE Transactions on Information Theory, 2017.

[WL17]

Raymond KW Wong and Thomas Lee. Matrix completion with noisy entries and outliers. The
Journal of Machine Learning Research, 18(1):5404–5428, 2017.

[YPCC16] Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis. Fast algorithms for

robust PCA via gradient descent. In NIPS, pages 4152–4160, 2016.

[ZB18]

[ZCL16]

Yiqiao Zhong and Nicolas Boumal. Near-optimal bound for phase synchronization. SIAM
Journal on Optimization, 2018.

Huishuai Zhang, Yuejie Chi, and Yingbin Liang. Provable non-convex phase retrieval with
outliers: Median truncated Wirtinger ﬂow. In International conference on machine learning,
pages 1022–1031, 2016.

[ZL16]

Qinqing Zheng and John Laﬀerty. Convergence analysis for rectangular matrix completion using
Burer-Monteiro factorization and gradient descent. arXiv:1605.07051, 2016.

[ZLGV16]

Jinchun Zhan, Brian Lois, Han Guo, and Namrata Vaswani. Online (and oﬄine) robust PCA:
Novel algorithms and performance guarantees.
In Artiﬁcial intelligence and statistics, pages
1488–1496, 2016.

[ZLW+10] Z. Zhou, X. Li, J. Wright, E. Candès, and Y. Ma. Stable principal component pursuit.

In

International Symposium on Information Theory, pages 1518–1522, 2010.

[ZWG18] Xiao Zhang, Lingxiao Wang Wang, and Quanquan Gu. A uniﬁed framework for nonconvex
low-rank plus sparse matrix recovery. In International Conference on Artiﬁcial Intelligence and
Statistics, 2018.

52

