2
2
0
2

l
u
J

2

]
I

A
.
s
c
[

1
v
9
1
9
0
0
.
7
0
2
2
:
v
i
X
r
a

An AlphaZero-Inspired Approach to Solving
Search Problems

Evgeny Dantsin1, Vladik Kreinovich2, and Alexander Wolpert1

1Roosevelt University, Chicago, U.S.
2The University of Texas at El Paso, U.S.

Abstract

AlphaZero and its extension MuZero are computer programs that use
machine-learning techniques to play at a superhuman level in chess, go,
and a few other games. They achieved this level of play solely with rein-
forcement learning from self-play, without any domain knowledge except
the game rules. It is a natural idea to adapt the methods and techniques
used in AlphaZero for solving search problems such as the Boolean sat-
isﬁability problem (in its search version). Given a search problem, how
to represent it for an AlphaZero-inspired solver? What are the “rules of
solving” for this search problem? We describe possible representations in
terms of easy-instance solvers and self-reductions, and we give examples
of such representations for the satisﬁability problem. We also describe a
version of Monte Carlo tree search adapted for search problems.

1 Introduction

AlphaZero [10] and its extension MuZero [8] are computer programs developed
by Google’s subsidiary DeepMind. They use machine-learning techniques to
play at a superhuman level in chess, go, and a few other games. AlphaZero
achieved this level of play solely with reinforcement learning from self-play, with
no human data, no handcrafted evaluation functions, and no domain knowledge
except the game rules. In comments on playing chess, the play style of Alp-
haZero is called “alien”: it sometimes wins by making moves that would seem
unthinkable to a human chess player.

The purpose of this paper is to adapt the methods and techniques used
in AlphaZero for solving search problems such as, for example, the Boolean
satisﬁability problem (in its search version). Reinforcement learning has been
applied to combinatorial optimization [6, 12], but with using expert knowledge
and handcrafted heuristics, which diﬀers these applications from AlphaZero’s
approach.

To design an AlphaZero-inspired solver for a search problem Π, we ﬁrst need
to represent Π as a one-player combinatorial game, where the player starts from

1

 
 
 
 
 
 
an initial position and tries to reach a winning position by making moves from
one position to another. That is, we need to deﬁne what we mean by positions,
winning positions, and possible moves.

We think of any instance of Π as a possible position in the game. For
example, if Π is SAT, then any formula in CNF (a set of clauses) is viewed as a
position. Certain instances of Π are thought as “easy” instances, assuming that
we already have an eﬃcient solver for such instances. An easy instance plays
the role of a winning position. In the case of SAT, a set of easy instances could
contain formulas with the empty clause, formulas where each clause contains a
pure literal, formulas in 2-CNF, etc. By a possible move we mean a transition
from an instance x to an instance x′ such that the following holds:

• x has a solution if and only if x′ has a solution;
• a solution to x can be computed from a solution to x′.

The resolution rule gives examples of possible moves: x′ is obtained from x by
choosing two clauses and adding their resolvent to x. Another example is the
pure literal elimination rule: x′ is obtained from x by removing all clauses that
contain pure literals.

Thus, we deﬁne the “game rules” for Π by specifying two components:
• a set of easy instances and a solver for this restriction of Π;

• for each non-easy instance of Π, a set of possible moves from this instance.

We call such a speciﬁcation a setup for solving Π. Section 2 gives a formal deﬁ-
nition of setups in terms of easy-instance solvers and self-reductions. Section 3
gives examples of setups for SAT.

Suppose we have chosen a setup for solving Π. A solver for Π based on
a given setup is described in Section 4. This solver uses adapted versions of
two key algorithms of AlphaZero: a reinforcement-learning algorithm and a
parameter-adjustment algorithm. The former one uses Monte Carlo tree search
to ﬁnd a sequence of moves from an input instance of Π to an easy instance.
This algorithm has many parameters that are adjusted with help of the latter
algorithm. The parameter-adjustment algorithm trains a deep neural network
to ﬁnd better values of the parameters; the choice of architecture of this network
depends on how we represent instances of Π.

The solver described in Section 4 can also be applied to another task called
per-instance algorithm selection [4, 5], In this task, we wish to design a “meta-
solver” that solves a search problem Π by automatically choosing (on a per-
instance basis) a solver from a “portfolio” of solvers for Π, see Section 5 for
details.

2 Setups for Solving Search Problems

Search problems. A search problem is one of the standard types of com-
putational problems. It is common to represent a search problem by a binary

2

relation R ⊆ X × Y where X is a set of instances and Y is a set of solutions. If
(x, y) ∈ R then y is called a solution to x.

The satisﬁability problem (in its search version) is an example of a search
problem, see Section 3 for details. The corresponding set X of instances consists
of Boolean formulas in CNF. The set Y of solutions consists of assignments of
truth values to variables. An instance x ∈ X has a solution y ∈ Y if y is
a satisfying assignment of x. There are many ways to encode instances and
solutions of the satisﬁability problem; no particular encoding is speciﬁed in our
example.

Solvers. Let Π be a search problem. A solver for Π is an algorithm S that
either ﬁnds a solution, or reports that there is no solution, or may give up saying
“don’t know”. That is, on every instance x ∈ X,

• if x has a solution, then S returns some solution to x or says “don’t know”;

• if x has no solution, then S says “no solution” or says “don’t know”.

Solvers may have parameters, additional input, and additional output. For
example, in Section 4, we describe a solver that takes as input not only an
instance x but also additional data θ with information about previous traces;
the solver outputs an answer for x and updates θ.

Easy instances. We assume that the set of instances of Π has a designated
subset E ⊆ X whose elements are called easy instances. The assumption behind
E is that it is “easy”to determine whether an instance x ∈ E has a solution
and, moreover, if a solution exists, it is “easy” to ﬁnd it. To formalize this
assumption, we equip Π with an algorithm denoted by E and called an easy-
instance solver. On every instance x ∈ X, this algorithm determines whether x
is an easy instance and, if so, the algorithm ﬁnds a solution to x or reports that
x has no solution:

“not easy”
“no solution”
some solution to x if x ∈ E and x has a solution

if x /∈ E
if x ∈ E and x has no solution

E(x) = 




Section 3 gives examples of the set E for the satisﬁability problem. For example,
E can be the set of formulas φ such that φ is the empty set (this formula is
satisﬁable) or φ contains the empty clause (this formula is unsatisﬁable).

Self-reductions and moves. We deﬁne a self-reduction of Π to be a pair
r = (fr, gr), where fr and gr are computable functions such that for every
instance x ∈ X,

• fr(x) is a ﬁnite set of instances;

• if x has a solution, then each instance in fr(x) has a solution;

3

• for every instance x′ ∈ fr(x) and for every solution y ∈ Y , if y is a solution

to x′, then gr(x, x′, y) is a solution to x.

If x′ ∈ fr(x), then we say that the self-reduction r oﬀers a move from x to x′.
Thus, for each instance, fr deﬁnes the set of all moves from this instance. We
call fr the move function of the self-reduction r. For a move from x to x′, the
function gr computes a solution to x from a solution to x′ (if any). We call gr
the solution function of r.

Examples of self-reductions of the satisﬁability problem are given in Sec-
tion 3. Here we just mention two of them. The ﬁrst example is a self-reduction
r = (fr, gr) where the move function fr is in fact the pure literal elimination
rule. This move function maps a CNF formula φ to a one-element set {φ′}
where φ′ is a CNF formula obtained from φ by successively removing all clauses
containing pure literals. Another example is a self-reduction r = (fr, gr) that
uses the resolution rule. For every CNF formula φ, the set fr(φ) consists of
all CNF formulas obtained from φ by choosing two clauses and adding their
resolvent to φ. In both examples, the solution functions gr are deﬁned in the
obvious way, see Section 3 for details.

Paths. Let R be a ﬁnite set of self-reductions of Π. Let x and x′ be instances
of Π. By a path from x to x′ we mean a sequence

x0, r1, x1, r2, x2, . . . , xn−1, rn, xn

where x0 = x, xn = x′, and ri is a self-reduction from R that oﬀers a move from
xi−1 to xi for all i = 1, . . . , n. Clearly, given such a path, we have the following:

• if x has a solution, then x′ also has a solution;

• if y is a solution to x′, then x has a solution that can be computed from

y by successively computing solutions to xn−1, . . . , x1, x0.

Setups for solving. An easy-instance solver E and a ﬁnite set R of self-
reductions of Π suggest the following approach to solving Π:

1. Try to ﬁnd a path from an input instance x to an easy instance x′.

2. If such a path is found, either return a solution to x (computed from
a solution to x′) or return “no solution” (in the event that x′ has no
solution). Otherwise, return “don’t know”.

The key step here is a search for a path and its success depends on the choice
of E and R. We call the pair (E, R) a setup for solving Π. Such a setup allows
us to think of Π as a one-player combinatorial game, where the player tries to
ﬁnd a sequence of moves from an initial position to a winning position. From
this point of view, a setup for solving Π deﬁnes the rules of this game.

Note that, in general, a setup (E, R) is not required to be “complete” in
the following sense: for every instance x, there must be a path from x to an

4

easy instance. Section 3 shows examples of diﬀerent setups for solving the
satisﬁability problem, including a setup where only satisﬁable formulas have
paths to easy instances. Solvers based on “incomplete” setups output “don’t
know” on instances that do not have paths to easy instances.

3 Examples for the Satisﬁability Problem

There are many possible setups for solving the satisﬁability problem that make
sense in our context. In this section, we give three setups for the purpose of
illustration.

Satisﬁability. Although the satisﬁability problem is very well known and de-
scribed in numerous books and articles [1], we give the basic deﬁnitions here to
avoid ambiguity (notation and terminology slightly vary in the literature).

Let V = {v1, v2, . . .} be a set of variables. A literal is a variable from V or
its negation; each of them is the complement of the other. The complement of
a literal a is denoted by ¬a. A clause is a ﬁnite set of literals that contains no
pair of complements (a clause is thought of as the disjunction of its literals). A
formula is a ﬁnite set of clauses (a formula is thought of as the conjunction of
its clauses). An assignment is usually deﬁned as a function from a ﬁnite set of
variables to {0, 1}, but it is convenient for us to use an equivalent deﬁnition: an
assignment is a ﬁnite set of literals without any pair of complements (this set is
thought of as the conjunction of its literals). An assignment α satisﬁes a clause
C if the intersection α ∩ C is not empty. An assignment α satisﬁes a formula φ
if α satisﬁes every clause of φ; we also call α a satisfying assignment for φ.

The deﬁnitions above allow the empty clause and the empty formula. No
assignment satisﬁes the empty clause and, thus, every formula with the empty
clause is unsatisﬁable. The formula consisting of only the empty clause is de-
noted by ⊥. The empty formula (“no constraints at all”) is denoted by ⊤. By
deﬁnition, ⊤ is satisﬁed by the empty assignment.

It is common to denote the following decision problem by SAT: given a for-
mula φ, does it have a satisfying assignment? Slightly abusing this notation,
we write SAT to refer to the satisﬁability problem in its search version: given a
formula φ, ﬁnd a satisfying assignment or return “no solution”. In terms of Sec-
tion 2, this search version is deﬁned as follows. The set X of instances consists of
all formulas over V . The set Y of solutions consists of all possible assignments,
i.e., all ﬁnite subsets of literals over V without any pair of complements. An
assignment α ∈ Y is a solution to an instance φ ∈ X if and only if α satisﬁes φ.

Example 1: Setup based on resolutions. There are only two easy in-
stances: ⊤ and ⊥. Thus, an empty-instance solver E is trivial. A set R of
self-reductions consists of the following three self-reductions commonly used in
SAT solving:

5

• Resolution rule. Let φ be a formula with clauses C1 and C2 such that
C1 contains a literal a and C2 contains its complement ¬a.
If the set
C1 ∪ C2 − {a, ¬a} contains no pair of complements, then we call this set
the resolvent of C1 and C2. If φ′ is the formula obtained from φ by adding
this resolvent, we say that φ′ is obtained from φ by the resolution rule.
The resolution self-reduction is a pair r = (fr, gr) where the move function
is deﬁned by

′
fr(φ) = {φ

| φ′ is obtained from φ by the resolution rule}

and the solution function gr is deﬁned as follows: for all formulas φ, if α
is a solution to a formula φ′ ∈ fr(φ) then gr(φ, φ′, α) is α.

• Subsumption rule. If a clause C1 is a proper subset of a clause C2, we say
that C1 is subsumed by C2 and we call the clause C2 unnecessary. The
subsumption self-reduction is the following self-reduction r = (fr, gr). The
move function fr maps a formula φ to a one-element set {φ′} where the
formula φ′ is obtained from φ by removing all unnecessary clauses. The
solution function gr is obvious: gr(φ, φ′, α) = α for all φ, φ′, and α.

• Pure literal elimination. A literal a in φ is called a pure literal if no clause
of φ contains ¬a. The pure literal self-reduction r = (fr, gr) is deﬁned as
follows. The move function fr maps a formula φ to a one-element set {φ′}
where φ′ is obtained from φ by successively removing all clauses containing
pure literals until φ′ has no pure literals. If α is a satisfying assignment
for φ′, then gr(φ, φ′, α) is the extension of α that assigns “true” to all pure
literals in φ.

This setup (E, R) is “complete”: for every formula φ, there is a path from φ to
either ⊤ or ⊥, see for example [2].

Example 2: Setup based on resolutions and the extension rule. The
setup described above can be extended by adding a self-reduction based on the
extension rule [11]. Let φ be a formula and let v be a variable not appearing
in φ: no clause of φ contains v or ¬v. Let a and b be literals such that their
underlying variables appear in φ. The extension rule adds clauses

{a, ¬v}, {b, ¬v}, {¬a, ¬b, v}

to φ. In the corresponding self-reduction r = (fr, gr), the move function fr is
deﬁned by

′
fr(φ) = {φ

| φ′ is obtained from φ by the extension rule}

and the solution function gr is the same as in the resolution self-reduction:
gr(φ, φ′, α) is α.

The extension rule makes resolution proof systems much stronger, but there
are no good heuristics for choosing extension literals a and b. This problem

6

of using the extension rule in practical SAT solvers is discussed in [2, section
7.8], where the authors note that “if this could be done well, the gains would
be enormous” and “the main bottleneck appears to be that we have no good
heuristics for how to choose extension formulas”.

Example 3: Setup based on ﬂipping. A variable is called a positive lit-
eral; its negation is called a negative literal. We deﬁne an easy instance to be
a formula in which every clause has at least one positive literal. Obviously,
such a formula is satisﬁed by the set of these positive literals. An algorithm
E determines whether an instance φ is an easy instance and if so, E(φ) is the
corresponding set of positive literals. The ﬂipping rule transforms a formula φ
taking the following two steps:

1. Choose a clause C ∈ φ in which all literals are negative (if φ is not an easy

instance, such a clause exists).

2. Choose a literal ¬vi ∈ C and “ﬂip” all of its occurrences in φ, i.e., replace

¬vi with vi everywhere in φ.

We can deﬁne R to be a set of one or more self-reductions based on the ﬂipping
rule. The move function in a such a self-reduction maps φ into a set of formulas
obtained from φ by applying the ﬂipping rule. Note that the setup (E, R) is not
“complete”. If φ is satisﬁable, then there is a path from φ to an easy instance.
Otherwise, φ has no path to any easy instance (all easy instances are satisﬁable).

4 Solvers Based on Setups

Let (E, R) be a setup for solving a search problem Π. We describe a solver for
Π based on this setup. This solver, denoted by S, tries to ﬁnd a path from an
input instance x to an easy instance and, if such a path is found, S outputs
an answer for x. The solver has parameters whose values change from run to
run, and S updates these values itself. The key point is that S uses machine-
learning techniques for both tasks, namely, for a path search and for updating
values of the parameters. Roughly, S uses a reinforcement-learning algorithm
A1 to search for a path and it uses a parameter-adjustment algorithm A2 to
search for “better” values of the parameters. We ﬁrst describe a bird’s eye view
of S and then give more details.

Input and output. The input to S has two parts: and instance x ∈ X and
a binary string θ ∈ {0, 1}∗ called a parameter string. This string encodes values
of the parameters of S and information about the solver’s previous traces. We
assume that θ is stored in a data store outside S. We also assume that θ is
initialized before the ﬁrst run of S and it is updated after each next run. Thus,
the output of S on x and θ is an answer for x (either a solution to x, or “no
solution”, or “don’t know”) and the updated parameter string θ′.

7

Solver S. On input x and θ, the solver S works as follows:

1. Run A1. This algorithm produces a path

x0, r1, x1, r2, x2, . . . , xn−1, rn, xn

(1)

where x0 = x. Note that xn is not necessarily an easy instance. In the
course of producing this path, A1 generates other paths and measures the
“quality” of the moves occurring in these paths: one move is better than
another if it is expected to have a better chance of leading to an easy
instance. Information about the quality of the moves is stored as quality
data δ. The output of A1 is path (1) and δ.

2. Return an answer for x:

(a) If E(xn) is “not easy”, then return “don’t know”.

(b) If E(xn) is “no solution”, then return “no solution”.

(c) If E(xn) is a solution to xn, work backwards from xn and use the solu-
tion functions gr from R to ﬁnd successively solutions to xn−1, . . . , x0.
Finally, return the solution to x0.

3. Run A2. This algorithm takes δ and merges it with similar quality data
collected in the previous runs of S. The result of merge is used for training
and updating parameters θ to new parameters θ′.

4. Return the updated parameter string θ′ for storing.

Reinforcement-learning algorithm A1. This algorithm is a Monte Carlo
tree search algorithm adapted for search problems. More exactly, A1 is a version
of the Adaptive Multistage Sampling algorithm (AMS) described in [3]. The
algorithm A1 cannot apply AMS as a black-box algorithm because the input to
AMS is not given explicitly. Instead, A1 supplies the input data in a “just-in-
time” manner as follows.

• Initialization of rewards. In each recursive call, AMS initializes rewards
of moves. Given an instance x, the reward of a move is a measure for
the belief that this move is on a bounded-length path from x to an easy
instance. The reward is maximum if the move is on such a path to an
easy instance. The algorithm A1 needs a belief estimation algorithm that
computes initial reward values for moves. This estimation is implemented
by a deep neural network N that uses parameters given in θ. The initial
rewards are improved by training this network.

• Sampling algorithm. The algorithm A1 provides a sampling algorithm
for AMS. On an instance x, this algorithm uses the parameters in θ to
sample the moves from fr(x) for each self-reduction r ∈ R. The sampling
algorithm can be implemented using the same deep neural network N ,
or it can be a diﬀerent neural network that shares weights with N . The

8

distributions for self-reductions are improved by training N , which means
that the improved distributions assign higher probabilities to moves with
higher accumulated rewards.

• Output. According to the description of AMS in [3], this algorithm returns
a path that has the maximum accumulated reward. In addition to this
optimal path, A1 collect the following quality data δ and returns it for
training:

– for every instance x and every self-reduction r explored in the run,

the accumulated probability distribution on fr(x);

– for every instance x explored in the run, the accumulated quality of

x (“value” of x in the AMS terminology).

Parameter-adjustment algorithm A2. After taking the quality data δ and
merging it with similar datasets, A2 trains the deep neural network N to adjust
the parameters θ. Note that the choice of architecture of N is dictated by
instance representation. For example, convolutional neural networks can be
used in the case of ﬁnite dimensional tensor representation.
If instances are
represented by binary strings of variable length, recurrent neural networks can
be used. In the case of SAT, it is natural to represent instances by graphs and,
therefore, N can be implemented as a graph neural network. In particular an
extension of the network constructed in [9] could be used. Also note that the
architecture of N determines what instance features can be discovered from
training.

What datasets can be used for the initial training of N ? It is more or less
common to train a neural network using randomly shuﬄed data. Certain sets of
instances (for example, industrial instances of SAT) expose self-similarity: large
instances have the same properties as smaller ones. In such cases, it makes sense
to train N using curriculum learning [7] where the training starts from samples
of small size and moves to larger ones.

5 Concluding Remark

In this paper, we described how to adapt AlphaZero’s techniques for designing
a solver for a search problem. This adaptation can also be used for another task
called per-instance algorithm selection [4, 5]. In this task, we are given a search
problem Π and a “portfolio” of solvers for Π. We wish to design a “meta-solver”
that automatically chooses a solver from the portfolio on a per-instance basis
and, thereby, it achieves better performance than any single solver from the
portfolio.

Suppose all solvers in the portfolio are of the following type. Such a solver
takes as input an instance x of Π and produces another instance x′ such that
(1) x′ has a solution if and only if x has a solution and (2) a solution to x can
If x′ is an easy instance then the solver
be computed from a solution to x′.

9

returns an answer, otherwise the solver returns x′ and says“don’t know”. Many
SAT solvers are of this type, for example, iterative solvers like resolution-based
solvers with a limited number of iterations. The portfolio with such solvers can
be viewed as a self-reduction where the move function maps an input instance
x to the set of all instances x′ produced by the solvers. Thus, we can use the
solver described in Section 4 as a meta-solver for Π.

References

[1] Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh, editors.

Handbook of Satisﬁability. IOS Press, 2nd edition, 2021.

[2] Sam Buss and Jacob Nordstr¨om. Proof complexity and SAT solving. In
Handbook of Satisﬁability, volume 336, chapter 7, pages 233–350. IOS Press,
2nd edition, 2021.

[3] Hyeong Soo Chang, Michael C. Fu, Jiaqiao Hu, and Steven I. Marcus. An
adaptive sampling algorithm for solving Markov decision processes. Oper-
ations Research, 53(1):126–139, 2005.

[4] Holger H. Hoos, Frank Hutter, and Kevin Leyton-Brown. Automated con-
ﬁguration and selection of SAT solvers. In Handbook of Satisﬁability, vol-
ume 336 of Frontiers in Artiﬁcial Intelligence and Applications, chapter 12,
pages 481–507. IOS Press, 2nd edition, 2021.

[5] Pascal Kerschke, Holger H. Hoos, Frank Neumann, and Heike Trautmann.
Automated algorithm selection: Survey and perspectives. Evolutionary
Computation, 27(1):3–45, 2019.

[6] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev.
Reinforcement learning for combinatorial optimization: A survey. Com-
puters and Operations Research, 134:105400, 2021.

[7] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E.
Taylor, and Peter Stone. Curriculum learning for reinforcement learning
domains: A framework and survey. Journal of Machine Learning Research,
21:181:1–181:50, 2020.

[8] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Si-
monyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver.
Mastering Atari, Go, chess and shogi by planning with a learned model.
Nature, 588:604–609, 2020.

[9] Daniel Selsam, Matthew Lamm, Benedikt B¨unz, Percy Liang, Leonardo
de Moura, and David L. Dill. Learning a SAT solver from single-bit su-
pervision. In Proceedings of the 7th International Conference on Learning
Representations, ICLR 2019, 2019.

10

[10] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Ku-
maran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis
Hassabis. A general reinforcement learning algorithm that masters chess,
shogi, and Go through self-play. Science, 362(6419):1140–1144, 2018.

[11] Gregory S. Tseitin. On the complexity of derivation in propositional cal-
culus. Zapiski Nauchnykh Seminarov LOMI, 8:234–259, 1968. In Russian.
Reprinted in: J. Siekmann and G. Wrightson (eds.). Automation of Reason-
ing 2: Classical Papers on Computational Logic 1967–1970, pp. 466—483.
Springer (1983).

[12] Natalia Vesselinova, Rebecca Steinert, Daniel F. Perez-Ramirez, and Mag-
nus Boman. Learning combinatorial optimization on graphs: A survey with
applications to networking. IEEE Access, 8:120388–120416, 2020.

11

