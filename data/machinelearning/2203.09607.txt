2
2
0
2

r
a

M
7
1

]

G
L
.
s
c
[

1
v
7
0
6
9
0
.
3
0
2
2
:
v
i
X
r
a

Learning Distributionally Robust Models at Scale via
Composite Optimization

Farzin Haddadpour† Mohammad Mahdi Kamani‡ Mehrdad Mahdavi§ Amin Karbasi†

†Yale University
{farzin.haddadpour,amin.karbasi}@yale.edu

‡Wyze Labs Inc.
mmkamani@alumni.psu.edu

§The Pennsylvania State University
mzm616@psu.edu

Abstract

To train machine learning models that are robust to distribution shifts in the data, distributionally
robust optimization (DRO) has been proven very eﬀective. However, the existing approaches to learning
a distributionally robust model either require solving complex optimization problems such as semideﬁnite
programming or a ﬁrst-order method whose convergence scales linearly with the number of data samples–
which hinders their scalability to large datasets. In this paper, we show how diﬀerent variants of DRO
are simply instances of a ﬁnite-sum composite optimization for which we provide scalable methods. We
also provide empirical results that demonstrate the eﬀectiveness of our proposed algorithm with respect
to the prior art in order to learn robust models from very large datasets.

1

Introduction

Conventional machine learning problem aims at learning a model based on the assumption that training
data and test data come from same data distribution. However, this assumption may not hold in various
practical learning problems where there is label shift [61], distribution shift [47], fairness constraints [23], and
adversarial examples [50], to name a few. Distributionally robust optimization (DRO), which has recently
attracted remarkable attention from the machine learning community, is a common approach to deal with the
aforementioned uncertainties [5, 13, 46]. Deﬁning the empirical distribution of the training data of size m by
ˆPm (cid:44) 1
where δ is the Dirac delta function, the goal of DRO is to solve the following optimization
m
problem

i=1 δ ˆξi

(cid:80)m

(cid:2)Ψ(x) (cid:44) supξ∈Q

EQ [(cid:96)(x; ξ)] (cid:3),

inf
x

(1)

where ξ is a data sample randomly drawn from distribution Q, (cid:96)(x; ξ) is the corresponding loss function and
EQ [(cid:96)(x, ξ)] is the expected loss over distribution Q which belongs to uncertainty set
m. The uncertainty set
indicates the ball of a distribution with center ˆPm and also d(P, Q)
m is deﬁned as
(cid:15)
U
is a distance measure between probability distribution P and Q. We note this uncertainty set captures the
distribution shift hence Eq. (1) minimizes the worse data distribution. Prior studies [2, 3, 4, 15, 42] considered
diﬀerent uncertainty sets (see Deﬁnition 3.1 in [15]) for which they proposed equivalent reformulations of
Eq. (1) based on the speciﬁc choice of

Q : d(Q, ˆPm)
{

m (cid:44)

≤

U

U

}

To solve the above min-max optimization problems, majority of prior studies heavily rely on either
semideﬁnite programming [15] or stochastic primal-dual methods both for convex [10, 39, 26, 57, 58, 35] and
non-convex (deep learning) objectives [58]. While primal-dual methods can be used as an approach to solve
min-max optimization problems, it suﬀers from a few downsides. First and foremost, they need to store a
probability distribution of constrained violation of dimension m corresponding to dual variables. Additionally,

m.

U

1

 
 
 
 
 
 
available primal-dual methods often demand data sampling that corresponds to the probability distribution
over m data samples which introduces additional cost over uniform sampling. Finally, while majority of prior
studies are limited to DRO problems with convex objectives, establishing tight convergence rate for DRO
problems with penalty with non-convex objectives is still lacking.

To overcome these issues, we consider three diﬀerent reformulations of Eq. (1), corresponding to three
m namely, (1) DRO with Wasserstein metrics, (2) DRO with χ2
diﬀerent choices of uncertainty sets
divergence metrics, and (3) DRO with regularized entropy metrics (also known as KL) and show in Section 2
that all aforementioned DRO notions are indeed diﬀerent instances of a deterministic composite optimization
and can be solved by reducing to an instances of the following problem:

U

(cid:20)
Ψ(x) (cid:44) r(x) +

1
m

(cid:88)m

i=1

hi(x) + f

(cid:18) 1
m

(cid:88)m

i=1

(cid:19)(cid:21)

gi(x)

,

min
x

(2)

i

≤

≤

m are scalar-valued functions, and gi(x) : Rd

R for
R and hi(x) : Rd
where we suppose r(x) is convex and a relatively simple function, f (x) : Rp
m are vector-valued functions. On
1
the road to solve problem (2) at scale, we also develop a novel algorithm for heavily constrained optimization
problems [37, 51, 52] that rather surprisingly invokes a single projection through the course of optimization.
This algorithm is of independent interest and addresses the scalability issues raised in applications such as
fairness [11, 60].
We summarize the main contributions of our paper below:

Rp for 1

→

→

→

≤

≤

i

• We provide a large-scale analysis of DRO with Wasserstein distance and heavily constrained reformulation
when the objective function is strongly-convex. Our result relies on a novel mini-batch constraint
sampling for handling heavily-constrained optimization problems. As summarized in Table 1, our
convergence analysis improves the state-of-the-art both in terms of the dependence on the convergence
error (cid:15) as well as the number of constraints m.

• We represent a large-scale analysis of DRO with non-convex objectives and χ2 or KL divergences and

propose a distributed varaint to further improve scalability of DRO problems.

• We verify our theoretical results through various extensive experiments on diﬀerent datasets.

In
particular, we show that our proposed method outperforms recent methods in DRO for heavily
constrained problems with a great reduction in time complexity over them.

The proofs of all the theorems are provided in the appendix.

1.1 Related Work

DRO and connections to heavily constrained optimization. As mentioned earlier, DRO has many
diﬀerent formulations, depending on the divergence metrics used (e.g., Wasserstein , χ2 or KL). While
[14, 35, 49] consider constrained or penalized DRO formulation, [30, 50] formulate the underlying optimization
problem as unconstrained. One of the contributions of our paper is to provide a unifying framework through
the language of composite optimization and treat all these variants similarly.

In particular, when the objective function is convex, [30] recently proposed scalable algorithms for diﬀerent
variants of the DRO problems with , e.g., χ2 or KL divergence metrics. Our unifying approach readily
extends those results to the more challenging non-convex setting for which we are unaware of any prior
work with convergence guarantees (for instance,
[23] studied DRO with χ2-divergence but did not provide
any convergence guarantee). Similarly, [15, 29] formulated DRO with Wasserstein distance as an instance
of constrained optimization. Notably, they require ti impose one constraint per training data point and to
solve such a constrained problem they proposed a semi-deﬁnite program. Even though the formulation is
very novel, it cannot scale. We, in contrast, consider such a heavily constrained optimization as an instance
of a composite optimization for which we provide a scalable solution. What is rather surprising about our
method is that it only checks a batch of constraints per iteration, inspired by [6], and performs a single
projection at the ﬁnal stage of the algorithm in order to provide an (cid:15)-optimal solution in the case of strongly

2

convex objectives. Moreover, in contrast to [6], we do not keep a probability distribution over the set of
constraints. We should also remark that our convergence guarantees achieve the known lower bounds in terms
of accuracy (cid:15) and the number of constraints m. Finally, we should highlight the diﬀerence of our algorithm
and Frank-Wolfe (FW) [17, 25, 64]. While FW does not require a projection oracle, it performs a linear
program over the set of constraints at each iteration. In contrast, our heavily- constrained optimization
solution performs a single projection without the overhead of running a linear program at each iteration.
(cid:2)Ψ(x) (cid:44)
Stochastic composite optimization. The general stochastic composite optimization minx
r(x) + f (Eξ [gξ(x))] (cid:3) has recently received a lot of attentions [44, 43, 54, 27]. Our reformulation of DRO
variants is a ﬁnite-sum instance of this general problem. More concretely,
[24, 31, 62] aimed to solve the
following ﬁnite-sum problem minx
, using SVRG or SAGA [8]. In
contrast, our proposed algorithm is inspired by [62] and generalizes their method to the case where the extra
terms hi(x) in Eq. (2) are non-zero. We should also note that [43] proposed a similar idea in the context of
online learning for DRO problems with KL divergence. Our work in contrast provides guarantees for DRO
with both constraints or penalty terms.

Ψ(x) (cid:44) r(x) + 1
n

i=1 gi(x)(cid:1)(cid:105)

j=1 fj

(cid:0) 1
m

(cid:80)m

(cid:80)n

(cid:104)

2 DRO via Finite-Sum Composite Optimization

In this section, we discuss in detail how a ﬁnite-sum composite optimization (2) can unify various notions of
distributionally robust learning, where some of which rely on heavily constrained optimization subroutines.
While much research eﬀort has been devoted to develop a specialized algorithm for each notion, our reduction
paves the way to developing a scalable algorithm, discussed in Section 3.

DRO with Wasserstein distance. An equivalent and tractable reformulation of Eq. (1) is provided
in [15, 29], which can be regarded as a heavily constrained optimization problem as follows:

min
x

r(x) (cid:44) 1
m

m
(cid:88)

i=1

fi(x)

subject to

˜gi(x)

0,

i
∀

∈

≤

[m].

(3)

where ˜gi(x) are functions related to loss function as well as slack variables (please see Appendix A for
more details). Naively solving optimization problem (3) suﬀers from the computational complexity due
to the large number of constraints m. To eﬃciently solve the optimization problem (3), inspired by [32]
and [6], we pursue the smoothed constrained reduction approach and introduce the augmented optimization
and
problem (see Appendix B) of the form min
x
i=1 gi(x)]. We can see that this optimization problem is a special case of the optimization
g(x) = 1
problem Eq. (2) where r(x) = f (x), f ( 1
i=1 gi(x)) = γ ln g(x), and h(x) = 0. In contrast to [6] that
m
requires an extra storage cost of probability distribution of dimension m, and relatively poor convergence
rate in terms of m and accuracy (cid:15), we propose an algorithm that simply checks a batch of constraints and
achieves the optimum dependency in terms of m and (cid:15).

Ψ(x) (cid:44) [r(x) + γ ln (g(x))] where gi(x) (cid:44) exp

m+1 [1 + (cid:80)m

(cid:16) α˜gi(x)
γ

(cid:80)m

(cid:17)

DRO with χ2-divergence. The second type of DRO problem we consider utilizes the χ2-divergence metric
as follows:

min
x

max
0≤pi≤1, (cid:80)m

i=1 pi=1

−

pifi(xi)

γDχ2(p).

(4)

i=1
where the χ2 divergence is deﬁned as the distance between the uniform distribution and an arbitrary probability
(cid:1)2. Levy et al. [30] studied this problem only for the case of
(cid:80)m
distribution p, i.e., Dχ2(p) (cid:44) m
i=1
2
convex objectives. In this paper, we allow objective functions fi for 1
m to be both non-convex or
strongly-convex. The following claim derives the equivalent ﬁnite-sum composite optimization.

(cid:0)pi

1
m

−

≤

≤

i

m
(cid:88)

3

SEVR [59]

General Linearized Wasserstein O

Reference

DRO type

Midtouch [6]

Wasserstein

Theorem 4.1 Wasserstein

Theorem 4.3

χ2 or KL

Theorem D.1 χ2 or KL

# Constraint/Sample Checks to achieve (cid:15) error

objective

O

(cid:16) ln m
(cid:16)

(cid:15) + m1.5(ln m)1.5
m ln (cid:0) 1
(cid:1) + DL
√
O (cid:0)(cid:0)m + κ

3
4

(cid:15)

(cid:15)

(cid:15) + ((cid:96)+κ+1)2D4
(cid:1)

(cid:15)2

u

+ m(ln m)2/3
(cid:15)
(cid:17)

2
3

√

(cid:16)

˜O
min{
O (cid:0)(cid:0)m + κ

m(cid:1) ln 1
(cid:15)
(cid:17)
(cid:15)1.5 }
m(cid:1) ln 1

m

(cid:15) , 1
√

(cid:1)

(cid:15)

+ m2 log m√

(cid:15)

(cid:17)

Strongly convex

Strongly convex-concave min-max

Optimally strongly convex

Non-convex

Optimally strongly convex

Table 1: Comparison of our results with prior approached. All three approaches are using variance reduction
techniques. Du and DL respectively denotes the upper bound on the distance of initial model from optimal
model and initial optimality gap. Please see [59] for more details. Finally, we note that while Midtouch
approach in [6] requires additional storage of probability distribution of dimension m, our approach does not.

Claim 2.1. The optimization problem (4) is equivalent to the following composite problem:


Ψ(x) (cid:44) 1

min
x

1
2γm

−

m
(cid:88)

i=1

[fi(x)]2 +

1
2γ

(cid:34)

1
m

m
(cid:88)

i=1

(cid:35)2


fi(x)

(5)

We note that the optimization problem (5) ﬁts into the formulation of ﬁnite-sum composite optimization (2)
f 2
i (x)
,
2γ

i=1 fi(x)(cid:3)2 with hi(x) =

and f (g(x)) = 1
2γ

(fi(x))2
2γ

(cid:2) 1
m

(cid:80)m

(cid:80)m

−

by choosing r(x) = 1, h(x) = 1
m
gi(x) = fi(x) and f (x) = x2
2γ

.

i=1 −

DRO with KL divergence. Finally, for DRO with KL-divergence, usually considered in online settings [43],
we consider solving the following optimization problem:

min
x

max
0≤pi≤1, (cid:80)m

i=1 pi=1

pifi(xi) + γH(p1, . . . , pm)

,

(cid:35)

(cid:34) m
(cid:88)

i=1

(6)

where H(p1, . . . , pm) =
convert it to the following equivalent stochastic composite optimization problem:

i=1 pi log pi is the entropy function. To solve problem (6), it is straightforward to

−

(cid:80)m

(cid:34)

min
x

Ψ(x) (cid:44) ln

(cid:32)

1
m

m
(cid:88)

i=1

exp

(cid:18) fi(x)
γ

(cid:19)(cid:33)(cid:35)

.

(7)

As it can be seen, the optimization problem (7) ﬁts into the composite optimization (2) by choosing
r(x) = h(x) = 0 and f (g(x)) = ln

(cid:80)m

(cid:17)(cid:17)

.

i=1 exp

(cid:16) fi(x)
γ

(cid:16) 1
m

3 Our Proposed Algorithm

(cid:80)m

i=1 gi(x)). We note that the compositional structure in Φ(
·

Having reduced the diﬀerent notions of DRO to an instance of the composite optimization, in this section we
describe our scalable approach for minimizing the objective Ψ(x) = r(x)+Φ(x) where Φ(x) = 1
i=1 hi(x)+
m
) leads to more challenges in optimization
f ( 1
m
compared with the non-compositional ﬁnite-sum problem, since the stochastic gradient of the loss function
is not an unbiased estimation of the full gradient. To overcome this issue, and by building on incremental
variance reduction [63], we propose a more general algorithm with a new ingredient in which we also employ
variance reduction on the extra term h(x) = (1/m) (cid:80)m
), similar to [63], we assume r
is convex and a relatively simple function. We follow the proximal gradient iterates [1, 40] as follows:

i=1 hi(x). To handle r(
·

(cid:80)m

x(t+1) = Πη
r

(cid:16)

x(t)

(cid:17)

Φ(x(t))

η

∇

−

4

(8)

Algorithm 1: Generalized Composite Incremental Variance Reduction (GCIVR (x(0)))

Inputs: Number of iterations t = 1, . . . , T , learning rate η, initial global model x(0), the size of epoch
length τt, and mini-batch sizes of Bt and St at time t.
for t = 1, . . . , T do

Sample a mini-batch

(t) with size Bt uniformly over [m] and compute

y(t)
0 =

(cid:88)

1
Bt

g(x(t)
τt

ξ∈B(t)

; ξ), z(t)

(cid:88)

ξ∈B(t) ∇

g(x(t)
τt

; ξ), w(t)

0 =

(cid:88)

1
Bt

ξ∈B(t) ∇

h(x(t)
τt

; ξ)

B

0 =

1
Bt
(cid:17)
f (cid:48)(y(t)
0 )
1 = Πη
r

+ w(t)
0
(cid:16)
x(t)

0 −

(cid:17)

Φ(x(t)

0 ))

˜
∇

Compute ˜
∇

Φ(x(t)

0 ) =

(cid:16)

z(t)
0

(cid:17)(cid:62) (cid:16)

Update the model as follows: x(t)
for j = 1, . . . , τt

Sample a mini-batch

−

1 do in parallel
(t)
j
S
j = y(t)
y(t)

j−1 +

with size St uniformly over [m], and form the estimates

1
St
1
St
1
St

(cid:88)

ξ∈S (t)
j

(cid:104)

g(x(t)

j ; ξ)

−

g(x(t)

j−1; ξ)

(cid:105)

(cid:88)

ξ∈S (t)
j

(cid:88)

(cid:104)
∇
(cid:104)

ξ∈S (t)
j

∇

g(x(t)

j ; ξ)

− ∇

g(x(t)

(cid:105)
j−1; ξ)

h(x(t)

j ; ξ)

− ∇

h(x(t)

(cid:105)
j−1; ξ)

(9)

(10)

(11)

j = z(t)
z(t)

j−1 +

w(t)

j = w(t)

j−1 +

Compute ˜
∇

Φ(x(t)

j ) =

(cid:16)

z(t)
j

(cid:17)(cid:62) (cid:16)

Update the model as follows: x(t)

(cid:17)
f (cid:48)(y(t)
j )
j+1 = Πη
r

+ w(t)
j
(cid:16)
x(t)

j −

(cid:17)

Φ(x(t)

j ))

˜
∇

end

end
Output: Return a randomly selected solution from

t=1,...,T
j=0,...,τt

xt
j}

{

η(y)

2 (cid:3)
(cid:107)G
(cid:107)
Focusing on Φ(

y

x

−

(cid:2)r(y) +
where we apply the proximal operator of r(x) with the learning rate η as Πη
2 (cid:3). By deﬁning the proximal gradient mapping of Ψ as
Φ(x))], the
1
η [x
2η (cid:107)
−
(cid:107)
updating rule in Eq. (8) can be equivalently written as x(t+1) = x(t)
η(x(t)). Given any y as an output
G
of randomized algorithm, we say y achieves the stationary point of problem in Eq. (2) in expectation if
E(cid:2)
(cid:15) holds. Our goal is to achieve an (cid:15) stationary point with the least number of calls to a
≤
(mini-batch) stochastic oracle.

η(x) (cid:44) 1
G
η
−

r (x) (cid:44) arg miny

r (x

Πη

∇

−

η

∇

(cid:80)m

(cid:80)m

τt ), z(t)

0 = 1
m

0 = 1
m

i=1 gi(x(t)

), as detailed in Algorithm 1, we apply three time-scale variance-reduced estimators
·
for gi(x) and its gradient
gi(x), as well as hi(x). For DRO with Wasserstein divergence metric with
optimally strongly objective, at the beginning of each epoch t we compute a full-batch gradient over the entire
data samples Bt = m, i.e., y(t)
τt ). In
contrast, for the case of DRO with χ2 or KL divergence metrics and non-convex objectives we incrementally
increase the size of the mini-batch Bt
m at the beginning of each epoch until it reaches the point where we
need to compute the full-batch of samples. We should also highlight that a mini-batch in case of Wasserstein
DRO indicates a batch of constraints and in DRO with χ2 or KL divergence metrics represents the number
of data sample accessed. We denote the length of each epoch and the mini-batch size within each epoch with
τt and St, respectively. In each epoch t and iteration j, we estimate mini-batch gradients from Eqs. (9), (10),
and (11) in Algorithm 1, with some corrections term applied from the previous iteration. We note that the
variance-reduced term corresponding to the correction is inspired by SARAH [41] and SPIDER [16]. Finally,
the algorithm returns a randomly selected solution from the iterates.
Distributed variant of Algorithm 1. As mentioned before, eﬃcient training of the stochastic composite
optimization problem has attracted increasing attention in recent years. Despite much progress, all of existing

τt ), w(t)

0 = 1
m

hi(x(t)

gi(x(t)

i=1 ∇

i=1 ∇

(cid:80)m

≤

5

methods including Algorithm 1 only focus on the single-machine setting. To employ Algorithm 1 in a
distributed setting with p machines, in Appendix E we propose a distributed variant of proposed algorithm
and establish its convergence rate for convex and non-convex objectives which enjoys a speedup in terms of
number of machines.

4 Convergence Analysis

In this section we establish the convergence of proposed algorithm for diﬀerent DRO notions discussed in
Section 2. We start by stating the general assumptions and then discuss the obtained rates. Due to lack of
space we only include the rates on the convergence of DRO with Wasserstein metric for strongly convex, and
χ2 and KL divergence metrics for non-convex objectives. We defer the analysis of χ2 and KL divergence
metrics with strongly objectives to Appendix D. To establish the convergence rates, we ﬁrst introduce some
standard assumptions.

Assumption 1. We make the following assumptions on the components of objective Ψ(x) = r(x) +
1
m

i=1 hi(x) + f ( 1
m

i=1 gi(x)):

(cid:80)m

(cid:80)m

h(x2)

Lh

x1

(cid:107)

−

x2

(cid:107)

(cid:107) ≤

where x1, x2

(cid:107) ≤

Lf

x1
(cid:107)

−

x2

(cid:107)

where x1, x2

Rd. We also assume that

h(x1)
(cid:107)

−

h(x2)

(cid:107) ≤

R. We also assume that

f (x1)
(cid:107)

−

f (x2)

(cid:107) ≤

∈

∈

1)

2)

3)

h(x1)
x1

− ∇
.
x2
(cid:107)
f (cid:48)(x2)
−
.
x2
(cid:107)

(cid:107)∇
(cid:96)h
(cid:107)
−
f (cid:48)(x1)
(cid:107)
x1
(cid:96)f
−
(cid:107)
g(x1; ξ)

(cid:107)∇

g(x2; ξ)

Lg

x1
(cid:107)

−

x2

(cid:107)

(cid:107) ≤

and

g(x1; ξ)
(cid:107)

−

− ∇

g(x2; ξ)

(cid:107) ≤

(cid:96)g

x1
(cid:107)

−

x2

(cid:107)

for

x1, x2

∀

∈

Rd.

4) We suppose that Ψ(x) in Eq. (2) is bounded from below that Ψ∗ = inf x Ψ(x) >

.
−∞

5) We assume r(x)

R

∈

∪ {∞}

is a convex and lower-semicontinues function.

An immediate implication of above assumption is that f (g(x)) (cid:44) f ( 1
m
(cid:1), hence Ψ(x) is smooth with module LΦ (cid:44) (cid:2)(cid:0)(cid:96)2
gLf + (cid:96)f Lg

gLf + (cid:96)f Lg

(cid:0)(cid:96)2

(cid:80)m
(cid:1) + Lh

(cid:3) [53, 63].

i=1 gi(x)) is smooth with module

Assumption 2 (Unbiased estimation). We assume that the mini-batch of samples ξ over functions gi for
i = 1, 2, . . . , m is unbiased, that is E [g(x; ξ)] = g(x) and E [

g(x; ξ)] =

g(x).

∇

∇

4.1 Optimally Strongly Convex Objectives

We here establish the convergence for optimally strongly convex objectives under Wasserstein metric.

Deﬁnition 1 (Optimally strongly convex). We say that the objective function Ψ(x) is optimally strongly
convex objective with module µ if Ψ(x)

Ψ(

x

µ

x∗ (x))

x∗ (x)

−

P

≥

(cid:107)

− P

2.
(cid:107)

According to Deﬁnition 4 of [38] optimally strong objectives or quadratic functional growth property is the
generalization of strongly convex condition. According to [28] below, the PL condition implies an optimally
strongly convex condition but not vice versa. Therefore, optimally strongly convex generalizes PL condition
as well.

Following [6] and [32], we make the following assumption.

Assumption 3. There exists a constant ρ such that if we deﬁne g(x) = max1≤i≤m ˜gi(x) we have min
ρ.

g(x)=0 (cid:107)∇

g(x)

(cid:107)2 ≥

Assumption 4. Function r(x) is smooth with module G.

6

Theorem 4.1. Assume Ψ is µ-optimally strongly convex and set τt = St = √Bt = √m, T = 5√
mµη ,
ρ , and γ = exp(−K)
2
ln(m+1) . Let us denote x(k+1) =
η <
LΦ+√L2
Φ+36G0
GCIVR (cid:0)x(k)(cid:1) for k = 0, . . . , K
1 (using Algorithm 1). Under Assumptions 1-4 and by letting K = ln (1/(cid:15)),
the solution of DRO with Wasserstein divergence is obtained by projecting x(K) onto the constraint set
(cid:15), we

, i.e., ¯x(K) = ΠK(x(K)). In order to achieve r(¯x(K))

where G0 (cid:44) 3((cid:96)4

0, i = 1, 2, . . . , m

h), α > G

f + (cid:96)2

g + (cid:96)2

r(x(∗))

f L2

gL2

=

−

x
{

gi(x)
|

K
≤
require an O (cid:0)(m + κ√m) ln 1

(cid:1) calls to the stochastic oracle.

}

(cid:15)

−

≤

Comparison with previous results. Compared to the MidTouch approach by [6], our obtained rate
improves both on the dependency on the number of constraints m, as well as convergence error (cid:15). To better
understand the intuition behind achieving such a double folded improvement in terms of m and (cid:15), we note that
unlike [6] which utilizes a primal-dual approach in order to obtain an approximate to the optimal distribution
over the constraints, we directly ﬁnd the optimal distribution exactly. Second, while [6] does not apply any
variance reduction technique to primal variable x, our algorithm beneﬁts from variance reduction over x too.
Furthermore, Theorem 4.1 entails tighter rate in terms of ﬁnal accuracy compared to [59]. These results are
summarized in Table. 1.

In Algorithm 1 for optimally strongly convex objectives, we need to do a single projection at the end.

The following corollary bounds the error between the solution before and after the projection.

Corollary 4.2. Under the assumptions made in Theorem 4.1, the error between the solution with and without
projection is bounded by

r(¯x(K))

r(x(K))

−

G

≤

(cid:20) γ ln(m + 1)

1

+

αρ

G

αρ

G

O (exp(

−

(cid:21)

K))

+ γ ln(m + 1).

(12)

−
Therefore, with a proper choice of γ we can establish convergence rate obtained in Theorem 4.1.

−

Remark 1. It is worthy to highlight that through reduction of heavily-constrained optimization to composite
optimization, GCIVR algorithm can be considered as an alternative method to solving constrained optimization
problems via mini-batch sampling of constraints.
In particular, Theorem 4.1 shows that under certain
conditions, we can solve any heavily-constrained optimization problem with sampling a mini-batch of constraints
and achieve similar guarantees compared to projection-based counterparts while avoiding the heavy dependency
on the number of constraints. Another implication of Theorem 4.1 is that we can solve heavily constrained
optimization with the sample complexity similar to that of an unconstrained optimization problem.

(cid:105)
Remark 2. To understand the tightness of our obtained rate, consider minx
i=1 gi(x)
which is an instance of our general optimization problem (2). Clearly, any lower bound to solve this instance
also holds for the original optimization problem (2). [56] provides a lower-bound of O ((m + √κm) ln (1/(cid:15)))
for above instance, matching our upper bound in terms of the dependency on the number of data samples,
while the dependency on κ can still be improved. Furthermore, for the DRO problem with Wasserstein
divergence metric, rather than solving constrained optimization problem we solve unconstrained compositional
optimization problem. Thus, since solving constrained optimization is more complex than unconstrained
optimization problem, we do not expect to obtain any better bound regarding m even for DRO with Wasserstein
metric.

Ψ(x) (cid:44) r(x) + 1
m

(cid:80)m

(cid:104)

Remark 3. In a distributed setting, we are able to improve the sample complexity to O
with p devices. The proof can be found in Appendix F.3.

(cid:16)(cid:16)

m + m

p + κ√m

(cid:17)

(cid:17)

ln 1
(cid:15)

4.2 Non-Convex Objectives

We now turn to establishing the convergence of DRO problems with χ2 and KL divergence metrics for
non-convex objectives by making an additional assumption on the variance of stochastic mini-batches.

7

Assumption 5 (Bounded variance). The mini-batch sampling has bounded variance that is E(cid:2)
σ2
B , where B indicates the batch size.
Theorem 4.3. Under Assumptions 1,2, and 5 using Algorithm 1 for some positive constants β and 0
ζ < √m, denote T0 (cid:44)
t > T0 set τt = St = √m. Then, if η <
˜O (cid:0)min
√m/(cid:15), 1/(cid:15)1.5
{
used ˜O(.) notation to hide terms with logarithmic dependency.

4
LΦ+√L2
Φ+12G0
η(x(T ))(cid:13)
(cid:1) number of calls to the stochastic oracle it holds that E(cid:2) (cid:13)
(cid:13)
(cid:13)
G

≤
T0, let us set τt = St = √Bt = βt + ζ and for
, after T = ˜O (min(1/√(cid:15), 1/√m(cid:15))) iterations, with

= O (√m). For t

(cid:15), where we

g(x; ξ)

m−ζ
β

− ∇

(cid:107)∇

2 (cid:3)

≤

≤

}

√

2 (cid:3)
g(x)
(cid:107)

≤

√

We also remark that Assumption 5 is only needed for convergence until T0 =

and after t > T0 we
will not need this assumption. We also note that this assumption is not required in optimally strongly convex
setting as we utilize a full batch at the beginning of each epoch.
Discussion on lower bound. As we discussed in Remark 2, any impossibility result for optimizing
also holds for general compositional optimization problem in Eq. (2).
minx
For former problem, [16] shows that to achieve the stationary point with error (cid:15) we require at least
˜O
(almost optimal) gradient oracle calls for strongly optimal objectives. As a consequence,
for the DRO with χ2 or KL divergence metrics we do not expect less number of constraint checks or sample
complexity, respectively.

(cid:104)
Ψ(x) (cid:44) r(x) + 1
m

(cid:17)
(cid:15)1.5 }

i=1 gi(x)

(cid:15) , 1

(cid:80)m

min

m−ζ
β

(cid:16)

{

m

(cid:105)

√

Remark 4. In distributed setting with p devices, we can improve the convergence rate of DRO with χ2 and
KL metrics to ˜O

. For details, please refer to Appendix F.3.

min

(cid:16)

1

√
m
p(cid:15) +

√
m
(cid:15) ,

p(cid:15)1.5 + 1

(cid:17)
(cid:15)1.5 }

{

5 Experiments

In this section, we empirically examine the eﬃcacy of the proposed algorithm in diﬀerent use cases. The main
algorithm that we compare against is Heavily-Constrained algorithm that uses a parametric multiplier model
proposed in [37]. We call this algorithm Heavily-constrained, where they learn the Lagrange multiplier values
using a parametric model such as a neural network. The tasks we apply the algorithms are mainly focused on
fairness constraints since they ﬁt greatly to the problem deﬁnition in this paper due to the large number of
constraints they add to the main learning problem. In addition, similar to distribution robust optimization
approaches, the main goal of fairness constraint is to ﬁnd a solution that is agnostic to the distribution of
protected groups. The code for the experiments is available at this repository.

Distributionally robust optimization for fairness constraints. The ﬁrst experiment is based on [37,
55], where we want to enforce equality of opportunity [22] constraints for diﬀerent groups while the group
membership is noisy and changing during the training. Hence, the problem is to make the solution distri-
butionally robust among diﬀerent protected groups in the problem. Based on the setup in [37], we assume
we have access to the marginal probability of the true groups (P (gi = j
ˆgi = k), where gi is the true group
|
membership and ˆgi is the noisy group membership). Hence, to enforce fairness constraints, we consider all
possible proxy groups using this marginal probability, which can increase the number of constraints greatly.
The goal in this case for equal opportunity is to have the true positive rate of each group in the vicinity of the
(cid:15) for every proxy group we deﬁne.
true positive rate (tpr) of the overall data, that is: tpr(g = j)
In this experiment, we use the Adult dataset [12], and consider race groups of “white”, “black”, and “other”
as protected groups. We train a linear classiﬁer with logistic regression, and report the overall error rate of
the classiﬁer, as well as the maximum violation of the fairness constraints (equal opportunity) over true group
memberships. We set (cid:15) = 0.05 and the noise level to 0.3. Again, we compare with unconstrained optimization
and heavily-constrained algorithm with a linear model as its multiplier model. First row of Figure 1 shows
the results for this experiment, where the proposed GCIVR algorithm can achieve the same level of constraint
violation on true group memberships as the heavily constraint while outperforms it in terms of overall error

tpr(ALL)

≥

−

8

s
s
e
n
r
i
a
F
O
R
D

s
s
e
n
r
i
a
F

l
a
n
o
i
t
c
e
s
r
e
t
n
I

s
s
e
n
r
i
a
F

g
n
i
k
n
a
R

(a) Error Rate

(b) Max Constraint Violation

(c) Error Rate (Runtime)

Figure 1: Comparison of the proposed GCIVR algorithm with unconstrained optimization and heavily-
constrained algorithm [37] in three diﬀerent tasks of fairness and DRO. Each row shows the result for one
task based on the error rate, max constraint violations and the runtime of the codes. In all the cases solutions
learned by GCIVR dominates the heavily-constrained solution and it converges way faster.

rate. Hence, the solution found by the GCIVR dominates heavily-constrained solution. In terms of runtime
speed, the ﬁrst row of Figure 1(c) clearly shows the advantage of GCIVR over heavily-constrained with much
lower overhead to the unconstrained optimization.

In this task we ought to learn a linear classiﬁer with
Fairness Constraints on intersectional groups.
logistic regression to predict the crime rate for a community on Communities and Crime dataset [12]. This
dataset contains 1994 instances of communities each with 128 features to predict the per capita crime rate for
each community in the US. The labels are high and low crime rates to represent if a community is above the
70th percentile of the data or not. To add fairness constraints, we ﬁrst determine diﬀerent communities based
on the percentages of the Black, Hispanic and Asian population in each community, as discussed in [7, 37].
[0, 1]3 to deﬁne each group. Consider the
Similar to [37], we generate 1000 thresholds of the form (τ1, τ2, τ3)
population of each race as (p1, p2, p3), then a community belongs to group gτ1,τ2,τ3
[3]. Then
the fairness constraints enforce that the error rate of each group should be in the neighborhood of the overall
error by margin of (cid:15). That is err (gτ1,τ2,τ3)
err (ALL) + (cid:15). Similar to [37], we set (cid:15) = 0.01 and not consider
groups with less than 1% of data.

if pi

i
∀

τi

≤

≥

∈

∈

We compare with the unconstrained optimization and Heavily-constrained algorithm with a 2-layer
neural network, each with 100 nodes as their Lagrange model, as described in their paper. We compare
the trade-oﬀ between the error rate and maximum violation of fairness constraints among groups. The

9

01000020000300004000050000Iterations0.150.200.250.300.350.400.450.50ErrorRateUnconstrainedHeavilyConstrainedGCIVR01000020000300004000050000Iterations−0.10−0.050.000.050.100.15MaxGroupViolationUnconstrainedHeavilyConstrainedGCIVR0100200300400500Wall-clockTime(min)0.150.200.250.300.350.400.450.50ErrorRateUnconstrainedHeavilyConstrainedGCIVR0240.1500.1750.2000.2250.250020000400006000080000100000Iterations0.080.100.120.140.160.180.20ErrorRateUnconstrainedHeavilyConstrainedGCIVR020000400006000080000100000Iterations0.0000.0250.0500.0750.1000.1250.1500.1750.200MaxGroupViolationUnconstrainedHeavilyConstrainedGCIVR020406080100Wall-clockTime(min)0.080.100.120.140.160.180.20ErrorRateUnconstrainedHeavilyConstrainedGCIVR020000400006000080000100000Iterations0.380.400.420.440.46ErrorRateUnconstrainedHeavilyConstrainedGCIVR020000400006000080000100000Iterations−0.25−0.20−0.15−0.10−0.050.00MaxGroupViolationUnconstrainedHeavilyConstrainedGCIVR050100150200250300Wall-clockTime(min)0.380.400.420.440.46ErrorRateUnconstrainedHeavilyConstrainedGCIVRsecond row of Figure 1 portrays this comparison for the test dataset. As it can be inferred, the proposed
GCIVR algorithm achieves the same error rate as Heavily-constrained, but both higher than the unconstrained
optimization. However, comparing the maximum violation of constraints, it is clear that GCIVR outperforms
both optimization methods. Also, the third ﬁgure shows the runtime of diﬀerent algorithms. It is clear that
GCIVR adding a minimal overhead to the unconstrained optimization, while heavily-constrained increases the
runtime more than 100 times.

In the third problem we evaluate our algorithm on the
Per-query fairness constraints in ranking.
fairness in the ranking problem, where we intend to impose per-query constraints on the learning to rank
problem as deﬁned in [36, 37]. We divide document-query pairs into two groups based on the 40th percentile
of the their QualityScore features. Hence, in this problem, we want to learn a ranking function f :
R,
which maps a pair of document-query features to a real number as the score. Consider the groups of g0 and g1
as mentioned before, we want the error rate for these groups to be close to each other. In other terms, if the
gj], where y and y(cid:48) are
pairwise error rate is deﬁned as: erri,j(q) = E [1
.
the respective binary labels. Then, the fairness constraints can be satisﬁed as
err1,0(q)
∈ Q
For this experiment, we use Microsoft Learning to Rank Dataset (MSLR-WEB10K) [45], which contains
10K queries and 136 features. For this experiment we use a non-convex objective, where the model is a
two-layer neural network each with 128 nodes and cross-entropy as the loss function. We compare against the
unconstrained optimization and the heavily-constrained algorithm with an one-layer neural network with
64 nodes as its multiplier model. We use 1000 queries in the training and 100 queries in the test datasets.
The third row in Figure 1 shows the error rate and maximum violation of groups constraints. As it is clear
GCIVR outperforms heavily-constrained in both error rate and maximum violations of groups by a large
margin. In terms of runtime, GCIVR is very close to the unconstrained optimization, and about 2
faster
than heavily-constrained.

gi, d(cid:48)
∈
err0,1(q)
|

f (d, q) < f (d(cid:48), q)

y > y(cid:48), d

D × Q →

∈
−

q
∀

| ≤

} |

×

{

(cid:15)

6 Conclusion

In this paper, we showed that many DRO problems or heavily constrained optimization problems can be cast
into a general framework based on ﬁnite-sum composite optimization. To solve this composite ﬁnite-sum
optimization, we introduced centralized and distributed algorithms. We theoretically illustrated that our
algorithm converges with an almost optimal number of constraint checks (for Wasserstein distance) or gradient
calls (for χ2 or KL divergence metrics). Finally, we validated our theory with a number of experiments.

Acknowledgements

Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032) and ONR
(N00014-19-1-2406). The work of Mehrdad Mahdavi was supported in part by NSF (CNS-1956276).

References

[1] Amir Beck. First-order methods in optimization. SIAM, 2017.

[2] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust
solutions of optimization problems aﬀected by uncertain probabilities. Management Science, 59(2):
341–357, 2013.

[3] Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical

Programming, 167(2):235–292, 2018.

[4] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein proﬁle inference and applications

to machine learning. Journal of Applied Probability, 56(3):830–857, 2019.

10

[5] Robert Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-convex

objectives. arXiv preprint arXiv:1707.01047, 2017.

[6] Andrew Cotter, Maya Gupta, and Jan Pfeifer. A light touch for heavily constrained sgd. In Conference

on Learning Theory, pages 729–771. PMLR, 2016.

[7] Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for eﬃcient non-convex

constrained optimization. In Algorithmic Learning Theory, pages 300–332. PMLR, 2019.

[8] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information processing
systems, pages 1646–1654, 2014.

[9] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated

learning. arXiv preprint arXiv:2003.13461, 2020.

[10] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated

averaging. arXiv preprint arXiv:2102.12660, 2021.

[11] Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Empirical

risk minimization under fairness constraints. arXiv preprint arXiv:1802.08626, 2018.

[12] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017. URL http://archive.ics.uci.

edu/ml.

[13] John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. arXiv

preprint arXiv:1610.02581, 2016.

[14] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally

robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021.

[15] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming,
171(1):115–166, 2018.

[16] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path integrated diﬀerential estimator. arXiv preprint arXiv:1807.01695, 2018.

[17] Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research logistics

quarterly, 3(1-2):95–110, 1956.

[18] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated

learning. arXiv preprint arXiv:1910.14425, 2019.

[19] Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local sgd
with periodic averaging: Tighter analysis and adaptive synchronization. Advances in Neural Information
Processing Systems, 2019.

[20] Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading
redundancy for communication: Speeding up distributed sgd for non-convex optimization. In ICML,
pages 2545–2554, 2019.

[21] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated
learning with compression: Uniﬁed analysis and sharp guarantees. arXiv preprint arXiv:2007.01154,
2020.

[22] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in

neural information processing systems, 29:3315–3323, 2016.

11

[23] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In International Conference on Machine Learning, pages
1929–1938. PMLR, 2018.

[24] Zhouyuan Huo, Bin Gu, Ji Liu, and Heng Huang. Accelerated method for stochastic composition
optimization with nonsmooth regularization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018.

[25] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International

Conference on Machine Learning, pages 427–435. PMLR, 2013.

[26] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic

mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.

[27] Dionysios S Kalogerias and Warren B Powell. Zeroth-order stochastic compositional algorithms for

risk-aware learning. arXiv preprint arXiv:1912.09484, 2019.

[28] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pages 795–811. Springer, 2016.

[29] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shaﬁeezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pages 130–166. INFORMS, 2019.

[30] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally

robust optimization. arXiv preprint arXiv:2010.05893, 2020.

[31] Xiangru Lian, Mengdi Wang, and Ji Liu. Finite-sum composition optimization via variance reduced

gradient descent. In Artiﬁcial Intelligence and Statistics, pages 1159–1167. PMLR, 2017.

[32] Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, and Jinfeng Yi. Stochastic gradient descent
with only one projection. In Advances in Neural Information Processing Systems 25 (NIPS), 2012.

[33] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-eﬃcient

learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.

[34] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. arXiv preprint

arXiv:1902.00146, 2019.

[35] Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust

optimization with f-divergences. In NIPS, volume 29, pages 2208–2216, 2016.

[36] Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Serena Wang. Pairwise fairness for ranking
and regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages
5248–5255, 2020.

[37] Harikrishna Narasimhan, Andrew Cotter, Yichen Zhou, Serena Wang, and Wenshuo Guo. Approximate
heavily-constrained learning with lagrange multiplier models. Advances in Neural Information Processing
Systems, 33, 2020.

[38] Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of ﬁrst order methods for non-

strongly convex optimization. Mathematical Programming, 175(1):69–107, 2019.

[39] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609,
2009.

12

[40] Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):

125–161, 2013.

[41] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine Learning,
pages 2613–2621. PMLR, 2017.

[42] Farzad Pourbabaee. Robust experimentation in the continuous time bandit problem. Economic Theory,

pages 1–31, 2020.

[43] Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. A practical online method for distributionally

deep robust optimization. arXiv preprint arXiv:2006.10138, 2020.

[44] Qi Qi, Yan Yan, Zixuan Wu, Xiaoyu Wang, and Tianbao Yang. A simple and eﬀective framework for
pairwise deep metric learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XXVII 16, pages 375–391. Springer, 2020.

[45] Tao Qin and Tie-Yan Liu. Introducing letor 4.0 datasets. arXiv preprint arXiv:1306.2597, 2013.

[46] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint

arXiv:1908.05659, 2019.

[47] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networks for group shifts: On the importance of regularization for worst-case generalization. arXiv
preprint arXiv:1911.08731, 2019.

[48] Soroosh Shaﬁeezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust

logistic regression. arXiv preprint arXiv:1509.09259, 2015.

[49] Alexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization, 27

(4):2258–2275, 2017.

[50] Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional

robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.

[51] Mengdi Wang and Dimitri P Bertsekas. Incremental constraint projection methods for variational

inequalities. Mathematical Programming, 150(2):321–363, 2015.

[52] Mengdi Wang and Dimitri P Bertsekas. Stochastic ﬁrst-order methods with random constraint projection.

SIAM Journal on Optimization, 26(1):681–717, 2016.

[53] Mengdi Wang, Ji Liu, and Ethan X Fang. Accelerating stochastic composition optimization. arXiv

preprint arXiv:1607.07329, 2016.

[54] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for
minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):419–449,
2017.

[55] Serena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Michael I
Jordan. Robust optimization for fairness with noisy protected groups. arXiv preprint arXiv:2002.09343,
2020.

[56] Guangzeng Xie, Luo Luo, and Zhihua Zhang. A general analysis framework of lower complexity bounds

for ﬁnite-sum optimization. arXiv preprint arXiv:1908.08394, 2019.

[57] Yan Yan, Yi Xu, Qihang Lin, Lijun Zhang, and Tianbao Yang. Stochastic primal-dual algorithms
arXiv preprint

with faster convergence than o(1/√T ) for problems without bilinear structure.
arXiv:1904.10112, 2019.

13

[58] Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Sharp analysis of epoch stochastic gradient

descent ascent methods for min-max optimization. 2020.

[59] Yaodong Yu, Tianyi Lin, Eric Mazumdar, and Michael I Jordan. Fast distributionally robust learning

with variance reduced min-max optimization. arXiv preprint arXiv:2104.13326, 2021.

[60] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P Gummadi. Fairness
constraints: A ﬂexible approach for fair classiﬁcation. The Journal of Machine Learning Research, 20(1):
2737–2778, 2019.

[61] Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra.
Coping with label shift via distributionally robust optimisation. arXiv preprint arXiv:2010.12230, 2020.

[62] Junyu Zhang and Lin Xiao. A composite randomized incremental gradient method. In International

Conference on Machine Learning, pages 7454–7462. PMLR, 2019.

[63] Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance reduction.

arXiv preprint arXiv:1906.10186, 2019.

[64] Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. One sample stochastic
frank-wolfe. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4012–4023. PMLR,
2020.

14

In the appendix, we provide the missing proofs and derivations from the main manuscript, as well as

proposing the distributed version of Algorithm 1 to further improve the convergence speed.

Appendix

Table of Contents

A Example of Equivalent Wessterstein Reformulation

B Obtaining Augmented Optimization Problem

C Proof of Claim 2.1

D Strongly-convex convergence for DRO with KL and χ2 metrics

E Distributed Stochastic Composite Algorithm

F Proof of Theorems 4.1 and F.2

. . . . . . . . . . . . . . . . .
F.1 Computational complexity of Theorem 4.1 and Theorem D.1
F.2 Computational Complexity of strongly-convex objectives Corresponding to Theorems 4.3
. .
F.3 Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

G Proof of the Distributed Algorithm

G.1 General Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

16

16

17

17

18
21
21
21

23
23

H Approximate Approach For DRO with Wesserstein Metric with Non-convex Objectives 27

15

A Example of Equivalent Wessterstein Reformulation

A tractable reformulation of distributionally robust logistic regression problem by reduction to a constrained
optimization problem is provided in [48] as follows:

min
x

r(x) (cid:44) λ(cid:15) +

1
m

subject to gi(x) = ((cid:96)β(z, y)

gj(x) = ((cid:96)β(z,

g(cid:96)(x) =

(cid:16)

β

(cid:107)

(cid:107)

−

2
∗ −

m
(cid:88)

si

i=1
si)

−
y)
−
λ2(cid:17)

0,

i
∀
sj)

≤
λκ

−
0

≤

[1 : m]

0,

j
∀

∈

∈

≤

[1 : m]

(13)

where (cid:96)β(z, y) = log (1 + exp (
β, z
(cid:105)
(cid:104)
and we deﬁne x = (z, s1, . . . , sm, λ, β).

y
−

)) is the associate loss with parameter β and the data sample (z, y)

B Obtaining Augmented Optimization Problem

To derive the smoothed constrained variant of original optimization problem we follow the reduction method
originally introduced in [32] to get:

(cid:34)

min
x

¯Ψ(x) = min
x

r(x) +

max

0≤pi≤α, (cid:80)m+1

i=1 pi=α

(cid:34) m
(cid:88)

i=1

pi˜gi(x) + γH(

p1
α

,

p2
α

, . . . ,

pm
α

,

pm+1
α

,

1
m + 1

(cid:35)(cid:35)
)

(cid:34)

(cid:32)

r(x) + γ ln

1 +

m
(cid:88)

exp

(cid:19)(cid:33)(cid:35)

(cid:18) α˜gi(x)
γ

= min

x

(cid:34)

(cid:32)

= min

x

r(x) + γ ln



i=1
(cid:34)

1
m + 1

1 +



(cid:19)(cid:35)(cid:33)

m
(cid:88)

i=1

exp

(cid:18) α˜gi(x)
γ

(cid:35)

+ γ ln (m + 1)

= min

x

 ,
Ψ(x) + γ ln (m + 1)
(cid:125)
(cid:124)

(cid:123)(cid:122)
Constant

(14)

where in Eq. (14) we used the deﬁnitions Ψ(x) = r(x) + γ ln (g(x)), gi(x) (cid:44) exp
m+1 [1 + (cid:80)m
and consequently we can solve the optimization problem for Ψ via ﬁnite-sum composite optimization.

and g(x) =
i=1 gi(x)]. Since γ ln (m + 1) is a constant, the minimizer of both ¯Ψ(x) and Ψ(x) is the same,

1

(cid:16) α˜gi(x)
γ

(cid:17)

C Proof of Claim 2.1

We use Lagrangian multiplier for the purpose of the proof. The Lagrangian of the optimization problem in
Eq. (4) is

L(x, γ; λ) =

max
0≤pi≤1, (cid:80)m

i=1 pi=1

(cid:34) m
(cid:88)

i=1

pifi(xi)

γ

1
2m

−

m
(cid:88)

i=1

(mpi

1)2

λ

−

−

(cid:32) m
(cid:88)

i=1

pi

−

(cid:33)(cid:35)
1

By setting

piL = 0 we obtain

∇

pi =

1
m

(cid:18) fi(x)
γ

λ

−

(cid:19)

+ 1

.

16

(15)

(16)

Then from the condition (cid:80)

i pi = 1, it follows that:

λ =

1
m

m
(cid:88)

i=1

fi(x)

Plugging the obtained values for λ and pi into Eq. (4) yields

Ψ(x; γ) = 1 +

m
(cid:88)

i=1

1
γm



(cid:32)



fi(x)

(cid:33)

fi(x)

1
m

m
(cid:88)

i=1

−

= 1

−

2γ

1

×

m

m
(cid:88)

i=1

(cid:32)

fi(x)

−

1
m

m
(cid:88)

i=1

fi(x)

= 1

1
2γm

−

m
(cid:88)

i=1

[fi(x)]2 +

which completes the proof.

1
2γ

(cid:34)

1
m

m
(cid:88)

i=1

(cid:35)2

fi(x)

(cid:32)

fi

−

1
m

m
(cid:88)

i=1

(cid:33)2


fi(x)

1
2

−

(cid:33)2

(17)

(18)

D Strongly-convex convergence for DRO with KL and χ2 metrics

Theorem D.1. Suppose Assumptions 1 and 2 hold, and also assume Ψ is µ-optimally strongly convex. If
, by letring x(k+1) = GCIVR (cid:0)x(k)(cid:1) for
we set τt = St = Bt = √m and T = 5√
1 using Algorithm 1 after K = ln (1/(cid:15)) repetition solves the optimization problem in Eq. (2)
k = 0, . . . , K
(cid:1) number of gradient calls. Therefore,
with convergence error Ψ(¯x(T ))
depending on the objective function Ψ, this sample complexity corresponds to both DRO problem with χ2 and
regularized KL metrics.

(cid:15) with O (cid:0)(m + κ√m) ln 1

2
LΦ+√L2
Φ+36G0

mµη , letting η <

Ψ(x(∗))

−

−

≤

(cid:15)

Based on Theorem D.1, for DRO problem with χ2 divergence metric, we can achieve Ψ(¯x(K))

(cid:15) with O (cid:0)(m + κ√m) ln 1

(cid:15)

−
(cid:1) number of gradient oracle

γ=1
|

Ψ(x(∗))
calls.

|

γ=1

≤

γ(cid:15) and Ψ(¯x(K))

γ>1

|

−

Ψ(x(∗))

γ>1
|

≤

E Distributed Stochastic Composite Algorithm

In the main body, we mainly focused on studying the sample complexity of solving DRO problems in a
centralized setting. The question we are interested is that “Can we further improve the sample complexity?"
In this section, we give an aﬃrmative answer to this question via studying distributed version of DRO
problem. We suppose data samples are distributed among p clients 1. For distributionally robust optimization
problem via Wasserstein , χ2 and KL divergence metrics in the distributed fashion like federated learning
setups [33, 34, 20, 18, 19, 9, 21].

1We suppose that each device has access to at least m/p number of data points.

17

We ﬁrst introduce our distributed ﬁnite-sum compositional algorithm as detailed in Algorithm 2.

Algorithm 2: Distributed Generalized Composite
(DistGCIVR(x(0)))

Incremental Variance Reduction

Inputs: Number of iterations t = 1, . . . , T , learning rate η, initial global model x(0), and a set of
triples
τt, Bt, St
}
{
time t.
for t = 1, . . . , T do

where τt indicating the size of epoch length and mini-batch sizes of Bt and St at

for i = 1, . . . , p do in parallel

Sample minibatches

y(t,i)
0 =

1
Bt

(cid:88)

ξ∈B(t,i)

B
gi(x(t)
τt

(t,i) with size Bt uniformly over [m] and compute
(cid:88)
ξ∈B(t,i) ∇
(cid:80)p

; ξ), w(t,i)

z(t,i)
0 =

gi(x(t)
τt

1
Bt

; ξ),

(cid:80)p

0 =

and w(t)

0 = 1
p

(cid:80)p

i=1 w(t,i)

0

1
Bt

(cid:88)
ξ∈B(t,i) ∇

hi(x(t)
τt

; ξ)

Server computes y(t)

0 = 1
p
Φ(x(t)

i=1 y(t,i)
0
(cid:16)
z(t)
0

, z(t)

0 = 1
p
f (cid:48)(y(t)
0 )

(cid:17)T (cid:16)

(cid:17)

i=1 z(t,i)
0
+ w(t)
0

Server computes ˜
∇
Update the model as follows and broadcasts it to devices

0 ) =

x(t)
1 = Πη
r

(cid:16)

x(t)

0 −

˜
∇

Φ(x(t)

0 ))

(cid:17)

for j = 1, . . . , τt
Sample a set

1 do in parallel
−
(t,i)
j
S

with size St over [m], and form the estimates

(cid:88)

(cid:104)

gi(x(t)

j ; ξ)

−

gi(x(t)

(cid:105)
j−1; ξ)

j = y(t,i)
y(t,i)

j−1 +

j = z(t,i)
z(t,i)

j−1 +

w(t,i)

j = w(t,i)

j−1 +

1
St

1
St

1
St

ξ∈S (t,i)
j
(cid:88)

ξ∈S (t,i)
j
(cid:88)

ξ∈S (t,i)
j

(cid:104)
∇

gi(x(t)

j ; ξ)

− ∇

gi(x(t)

(cid:105)
j−1; ξ)

(cid:104)

∇

hi(x(t)

j ; ξ)

− ∇

hi(x(t)

(cid:105)
j−1; ξ)

(19)

(20)

and send y(t,i) and z(t,i) back to server
Server computes y(t)

(cid:80)p

Compute ˜
∇

Φ(x(t)

j ) =

Update the model as follows: x(t)

j = 1
p
(cid:16)
z(t)
j

, z(t)
i=1 y(t,i)
j = 1
j
p
(cid:17)
(cid:17)T (cid:16)
+ w(t)
f (cid:48)(y(t)
j )
j
(cid:16)
x(t)
j+1 = Πη
r

(cid:17)

Φ(x(t)

j ))

˜
∇

j −

(cid:80)p

i=1 z(t,i)

j

and w(t)

j = 1
p

(cid:80)p

i=1 w(t,i)

j

end

end

end
Output: Return a randomly selected solution from
We emphasize that our algorithm is developed based on Algorithm 1, so we only describe the main
diﬀerences. In the the distributed setting, in epoch t, i-th device has local version of global model, i.e.,
y(t,i)
and at the beginning and during the epochs all devices send back their local models back
j
to the server to be averaged and global model x to be updated. Then, server broadcast global model to the
devices.

t=1,...,T
j=0,...,τt

, w(t,i)
j

, z(t,i)
j

xt
j}

{

F Proof of Theorems 4.1 and F.2

Before proceeding to the proof of this theorem, we need to mention that for Wasserstein DRO problem the
main objective r(x) is an aﬃne function of input data. This property naturally satisﬁes condition (5) in
Assumption 1 and will be useful in distributed setting. For an illustrative example please see Appendix A.

18

Proof of Theorem 4.1: Suppose that x(∗) is the solution for the following optimization problem:

minimize
x

subject to

m
(cid:88)

fi(xi)

r(x) (cid:44) 1
m

˜gi(x)

i=1
i
0,
≤
∀
(cid:16) α˜gi(x)
i=1 exp
γ

1,
∈ {
(cid:17)(cid:105)(cid:17)

· · ·

.
, m
}
, we have

Deﬁning Ψ(x) (cid:44) r(x) + γ ln

(cid:16) 1

(cid:104)

m+1

1 + (cid:80)m

(21)

Noting that ¯x(K) and x(K) are the output of algorithm with and without projection to the constraints set

¯Ψ(x) (cid:44) Ψ(x) + γ ln(m + 1).

≤
respectively. Our proof is based on following two steps:

K

=

x : ˜gi(x)
{

0,

i
∀

[1 : m]
}

∈

(22)

(1) We ﬁrst show that E (cid:2)Ψ(x(K))(cid:3)

Ψ(x(∗)) + O (exp(

aK)), which indicates the convergence rate of

stochastic compositional optimization problem depends on used algorithm without any projection.

−

≤

(2) Second step involves showing that E (cid:2)r(¯x(K))(cid:3)

aK)); in other words the ﬁnal
projected solution converges to optimal solution of augmented objective function and a is some positive
constant depends on condition number κ.

r(x(∗)) + O (exp(

≤

−

We note step (1) follows directly from Theorem 8 in [63]. In the following, we prove step (2). First note that
as ˜gi(x∗)

m, we have:

0 for all 1

i

≤

≤

≤

Ψ(x(∗)) = r(x(∗)) + γ ln

(cid:32)

(cid:34)

m
(cid:88)

exp

1 +

1
m + 1
(cid:32)

r(x(∗)) + γ ln

(cid:19)(cid:35)(cid:33)

(cid:18) α˜gi(x(∗))
γ
(cid:35)(cid:33)

i=1
(cid:34)

1
m + 1

1 +

(1)

m
(cid:88)

i=1

˜gi(x(∗))≤0

≤

= r(x(∗))

This leads to

Therefore, using item (1) we have:

Ψ(x(∗))

≤

r(x(∗)) + γ ln(m + 1)

(cid:104) ¯Ψ(x(K))

(cid:105)

E

≤

r(x(∗)) + O (exp(

−

aK))

On the other hand, due to the deﬁnition of Ψ(x) and smoothed max or log-sum property, we have:

¯Ψ(x(K))

r(x(K)) + max
1≤i≤m

≥

(cid:16)

0, α˜gi(x(K))

(cid:17)

.

For the purpose of lower bounding the second term in Eq. (26) we need the following Lemma:

Lemma F.1. If ¯xT

= xT , by deﬁning g(x(K)) (cid:44) max
1≤i≤m

˜gi(x(K)) for all i

∈

[1 : m] we have:

g(x(K))

(cid:13)
(cid:13)x(K)
(cid:13)

ρ

¯x(K)(cid:13)
(cid:13)
(cid:13)2

−

≥

(23)

(24)

(25)

(26)

(27)

The proof of this lemma can be found in [32] but for the sake of completeness we also include the proof.

19

(cid:54)
Proof. As ¯x(K) is the projection of x(K) into

K

; i.e., ¯x(K) = arg min
g(x(K))≤0

optimality condition, there exists a positive constant k > 0 such that

(cid:13)
(cid:13)x

−

x(K)(cid:13)
2, then due to ﬁrst order
(cid:13)

g(¯x(K)) = 0 s.t.,

¯x(K)

As a result we have:

g(x(K)) = g(x(K))

−

g(¯x(K))

≥
¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13)

((cid:48))

(cid:16)

(cid:13)
(cid:13)
(cid:13)

ρ

x(K)

≥
where ((cid:48)) follows from Assumption 3.

−

(cid:16)

x(K)

¯x(K)(cid:17)

−

x(K) = k

g(¯x(K))

∇

g(¯x(K)) =

(cid:16)

(cid:13)
(cid:13)
(cid:13)

x(K)

¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)∇

(cid:13)
g(¯x(K))
(cid:13)
(cid:13)

−

−

∇

Hence, we have:

Moreover, note that we have:

¯Ψ(x(K))

≥

r(x(K)) + αρ

(cid:16)

(cid:13)
(cid:13)
(cid:13)

x(K)

¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13)

−

r(x(K))

≤

≤

r(x(∗)) + γ ln (m + 1) + O (exp(

r(x(∗)) + γ ln (m + 1) + O (exp(

aK))

max
1≤i≤m

−

aK))

−

−

(cid:16)

(cid:17)
0, α˜gi(x(K))

Next, we can write:

r(x(∗))

−

≤
Eqs. (24), (30) and (31) lead to the bound:

r(¯x(K)) = r(x(∗))
r(x(K))
(cid:13)
(cid:16)
(cid:13)
(cid:13)

G

≤

−
x(K)

−

r(x(K)) + r(x(K))
r(¯x(K))
¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13)

−

r(¯x(K))

−

αρ

(cid:16)

(cid:13)
(cid:13)
(cid:13)

x(K)

¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13) ≤

−

≤

which allows us to the following:

r(x(∗))
(cid:13)
(cid:16)
(cid:13)
(cid:13)

G

−
x(K)

r(x(K)) + γ ln (m + 1) + O (exp(

−
¯x(K)(cid:17)(cid:13)
(cid:13)
(cid:13) + γ ln (m + 1) + O (exp(

−

aK))

aK))

−

(cid:13)
(cid:13)x(K)
(cid:13)

¯x(K)(cid:13)
(cid:13)
(cid:13) ≤

−

γ ln m
G
αρ

−

+

αρ

1

−

G

O (exp(

aK))

−

Finally, we have:

(28)

(29)

(30)

(31)

(32)

(33)

r(x(K)) + r(x(K))
r(¯x(K)) = r(¯x(K))
(cid:13)
¯x(K)(cid:13)
(cid:13)x(K)
(cid:13) + r(x(K))
(cid:13)
(cid:13)
−
(cid:20) γ ln(m + 1)
1

G

−

≤

≤

G

(cid:124)

+

O (exp(

αρ

G

−

αρ

G

−

(cid:123)(cid:122)
Cost of violating constraints

(cid:125)

(cid:21)

aK))

+ γ ln(m + 1)

+r(x(∗)) + O (exp(

−

aK))

(34)

−

Therefore, by setting γ = exp(−aK)
ln(m+1)

we achieve the desired result.

Proof of Corollary 4.2: The proof follows directly from Eq. (34).

20

Proof of Theorem D.1: The proof follows directly from Theorem 8 in [63].

F.1 Computational complexity of Theorem 4.1 and Theorem D.1
Given the GCIVR algorithm, the sample complexity for Theorem 4.1 (where St, Bt and τt are ﬁxed) is

O (max

T B, 2T τ S
{

)
}

×

K) = O

max

m,
{

(cid:18)(cid:18)

(cid:18)

m

+ max

2
{

}

5
√mµη
5
√mµη

= O

max

m,
m
{
}
= O (cid:0)(cid:2)m + κ√m(cid:3) ln(1/(cid:15))(cid:1)

+ max

√m√m, √m√m

(cid:19)

(cid:19)

(cid:19)

ln(1/(cid:15))

}

5
√mµη
10
µη

{

√m, m
}

ln(1/(cid:15))

(35)

Proof of Theorem 4.3: The proof follows directly from Theorem 4 in [63].

F.2 Computational Complexity of strongly-convex objectives Corresponding to

Theorems 4.3

From Theorem 4 in [63] for non-convex objectives we have:

E

(cid:20)(cid:13)
(cid:13)
(cid:13)G

η(¯x(T ))

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤

(cid:1)

(cid:40) O (cid:0) ln T
T 2
(cid:16)
O
√

ln m
m(T −T0+1)

(cid:17)

T0,

if T
≤
else T > T0,

(36)

Considering the GCIVR algorithm and the bound in Eq. (36), the sample complexity for Theorem 4.3

(where St and τt are ﬁxed) can be expressed as follows:

=

(cid:26) (cid:80)T
T
×
(cid:40) (cid:80)T
1√

t=0 (Bt + Stτt)
(B + Sτ )
t=0 2 (γt + β)2
m(cid:15) ×
(cid:40) O (cid:0)γ2T 3(cid:1)
(cid:16) √
(cid:17)
m
(cid:15)

O

=

=

(cid:40) O (cid:0)(cid:15)−3/2(cid:1)
(cid:16) √
(cid:17)
m
(cid:15)

O

if T = O (cid:0)(cid:15)−1/2(cid:1)
T0 = O (√m) ,
T = O (1/√m(cid:15)) > T0 = O (√m) ,
if T = O (cid:0)(cid:15)−1/2(cid:1)
T0 = O (√m) ,
(m + √m√m) T = O (1/√m(cid:15)) > T0 = O (√m) ,

≤

≤

≤

if T = O (cid:0)(cid:15)−1/2(cid:1)
T0 = O (√m) ,
T = O (1/√m(cid:15)) > T0 = O (√m) ,
if T = O (cid:0)(cid:15)−1/2(cid:1)
T0 = O (√m) ,
T = O (1/√m(cid:15)) > T0 = O (√m) ,

≤

Therefore, depending on how large √m is and also desired accuracy level (cid:15), we can decide to choose an
adaptive or a non-adaptive approach.

F.3 Convergence analysis

In this section, we extend assumptions used for centralized setting to distributed counterpart as follows:

Assumption 6. We have the following assumptions:

(37)

1)

2)

f (cid:48)(x1)
(cid:107)
x1
(cid:96)h
(cid:107)
−
hj(x1)
(cid:107)∇
x1
(cid:96)h
(cid:107)

−

f (cid:48)(x2)
−
x2

(cid:107)

− ∇
x2
(cid:107)

Lh

x1
(cid:107)

−

x2

(cid:107)

where x1, x2

∈

R. We also assume that

f (x1)

(cid:107)

−

f (x2)

(cid:107) ≤

(cid:107) ≤

hj(x2)

(cid:107) ≤

Lh

x1
(cid:107)

−

x2

(cid:107)

where x1, x2

∈

Rd. We also assume that

hj(x1)
(cid:107)

−

hj(x2)

(cid:107) ≤

21

3)

gj(x1; ξ)

(cid:107)∇
and 1

j

≤

≤

gj(x2; ξ)

(cid:107) ≤

Lg

x1
(cid:107)

−

x2

(cid:107)

and

gj(x1; ξ)
(cid:107)

−

gj(x2; ξ)

(cid:107) ≤

(cid:96)g

x1
(cid:107)

−

x2

− ∇
p.

4) We suppose that Ψ(x) in Eq. (2) is bounded from below that Ψ∗ = inf x Ψ(x) >

for

x1, x2

∀

∈

Rd

(cid:107)

.

−∞

5) We assume r(x)

R

is a convex and lower-semicontinues function.

∈

∪ {∞}

Assumption 7 (Unbiased estimation). The mini-batch sampling is unbiased that is E [gj(x; ξ)] = gj(x) and
E [

gj(x) for 1

gj(x; ξ)] =

p.

j

∇

∇

≤

≤

Convergence Analysis for Strongly Convex Objectives:

Assumption 8. We additionally for the DRO problem with Wessterstein metric suppose that the function
r(x) is smooth with module G.

Theorem F.2. Under Assumptions 3 and 6 to 8, when Ψ is µ-optimally strongly convex, if we set τt =
St = √Bt = √m and T = 5√
ρ , if we let
x(k+1) = DistBCO (cid:0)x(k)(cid:1) for k = 0, . . . , K
1 using Algorithm 2 after K = ln (1/(cid:15)) stages and returning the
, i.e., ¯x(K) = ΠK(x(K)),
0, i = 1, 2, . . . , m
ﬁnal solution after projecting onto the constraint set
}
r(x(∗))
solves the optimization problem in Eq. (3) with convergence error r(¯x(T ))
≤

mµη , and γ = exp(−K)

2
LΦ+√L2
Φ+36G0

and α > G

letting η <

gi(x)
|

(cid:15) with

ln(m+1)

x
{

−

≤

−

=

K

O

m +

+ κ√m

ln

(cid:18)(cid:18)

m
p

(cid:19)

(cid:19)

1
(cid:15)

(38)

per device number of constraint checks.
Remark 5 (Computational Complexity). The sample complexity for Theorem F.2 (where St, Bt and τt are
ﬁxed) is

O (max

{

T B, 2T τ S

)
}

×

K) = O

max

(cid:18)

m
p

,

{

5
√mµη
5
√mµη
(cid:21)

m
}

+ max

2
{

m

+ max

{

}

(cid:19)

+ κ√m

ln(1/(cid:15))

5
√mµη
10
µη

√m, m
}

√m√m, √m√m
}

ln(1/(cid:15))

(cid:19)

(cid:19)

(cid:19)

ln(1/(cid:15))

(39)

(cid:18)(cid:18)

(cid:18)(cid:20)

= O

= O

m,

max
{
m
p

m +

Convergence Analysis for Non-Convex Objectives: For the non-convex case we also need the following
extra assumption.

Assumption 9 (Bounded variance). The mini-batch sampling has bounded variance that is

E

(cid:104)
(cid:107)∇

gj(x;

)

Z

gj(x)
(cid:107)

− ∇

2(cid:105)

σ2

≤

m.

for 1

j

≤

≤

Theorem F.3. Under Assumptions 6 to 9 using Algorithm 2, for some positive constants β and 0
denoting T0 (cid:44)
≤
t > T0, set τt = St = √m. Then, if η <

ζ < √m;
T0, parameters are chosen to be τt = St = √Bt = βt + ζ; and when

m−ζ
β = O (√m), if t

it holds that

≤

√

4
LΦ+√L2
Φ+12G0
(cid:40) O (cid:0) ln T
T 2
(cid:16)
O
√

2(cid:21)

≤

(cid:1)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)G

(cid:13)
η(¯x(T ))
(cid:13)
(cid:13)

(cid:17)

T0,

if T
T > T0,

≤

(40)

ln p
m(T −T0+1)

This leads to the per device sample complexity of ˜O
where ˜O(.) notation hides
min
{
logarithmic factors. Therefore, depending on the objective function Ψ this sample complexity corresponds to
both DRO problem with χ2 and regularized KL metrics.

p(cid:15)1.5 + 1

1

(cid:16)

√
m
p(cid:15) +

√
m
(cid:15) ,

(cid:17)
(cid:15)1.5 }

22

We highlight that while in distributed setting we can not reduce computational complexity in terms of

order of magnitude, we can reduce computational complexity partially linearly with number of devices.

G Proof of the Distributed Algorithm

For the convince, we deﬁne the following notations

ξ(t) =

ξ(t)
1 , . . . , ξ(t)
,
p }

{

to denote the set of local solutions and sampled mini-batches at iteration t at diﬀerent machines, respectively.

We use notation E[

] to denote the conditional expectation E
·

ξ(t) [
·

].

G.1 General Lemmas

Before proceeding to the proof of main theorems, we ﬁrst review a few properties of our algorithm that will
be useful in our convergence proof:

E

E

(cid:104)

y(t,i)
j
(cid:104)
z(t,i)
j

(cid:104)

E

w(t,i)
j

(cid:105)

(cid:105)

(cid:105)

x(t)
j
|
x(t)
j
|
x(t)
j
|

= y(t,i)

j−1 + gi(x(t)
j )

= z(t,i)

j−1 +

∇

−
gi(x(t)
j )

gi(x(t)

j−1)

gi(x(t)

j−1)

− ∇

= w(t,i)

j−1 +

hi(x(t)
j )

∇

− ∇

hi(x(t)

j−1)

which due to linearity and taking average over the models of all devices, leads to

E

E

(cid:104)

E

(cid:104)

y(t)
j |
(cid:104)
z(t)
j |
w(t)
j |

(cid:105)

(cid:105)

(cid:105)

x(t)
j

x(t)
j

x(t)
j

= y(t)

j−1 + g(x(t)
j )

= z(t)

j−1 +

∇

−
g(x(t)
j )

g(x(t)

j−1)

g(x(t)

j−1)

− ∇

= w(t)

j−1 +

h(x(t)
j )

∇

− ∇

h(x(t)

j−1)

Additionally, we have equivalent update rule as follows:

j = y(t)
y(t)

j−1 +

j = z(t)
z(t)

j−1 +

w(t)

j = w(t)

j−1 +

1
S(t)

1
S(t)

1
S(t)

ξ∈S (t)
j
(cid:88)

ξ∈S (t)
j
(cid:88)

ξinS (t)

j

(cid:88)

(cid:104)
g(x(t)

j ; ξ)

g(x(t)

j−1; ξ)

(cid:105)

−

(cid:104)
∇

g(x(t)

j ; ξ)

− ∇

g(x(t)

(cid:105)
j−1; ξ)

(cid:104)

∇

h(x(t)

j ; ξ)

− ∇

h(x(t)

(cid:105)
j−1; ξ)

Lemma G.1. For any 1

E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

j −

j

(cid:16)

≤

g

τt we have:

≤

x(t)
j

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

E

(cid:20)(cid:13)
(cid:13)z(t)
(cid:13)

j −

g(cid:48) (cid:16)

x(t)
j

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

j −

h(cid:48) (cid:16)

x(t)
j

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

0 −

(cid:16)

g

x(t)
0

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

+

j
(cid:88)

r=1

(cid:96)2
g
S(t)

E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

r −

x(t)
r−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

E

(cid:20)(cid:13)
(cid:13)z(t)
(cid:13)

0 −

g(cid:48) (cid:16)

x(t)
0

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

+

j
(cid:88)

r=1

L2
g
S(t)

E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

r −

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

x(t)
r

E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

0 −

h(cid:48) (cid:16)

x(t)
0

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

+

j
(cid:88)

r=1

L2
h
S(t)

E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

r −

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

x(t)
r

≤

≤

≤

23

(41)

(42)

(43)

(44)

(45)

(46)

(47)

We note that Lemma G.1 is the generalization of Lemma 1 in [63], and we provide the proof for the sake

of completeness.

Proof. Our proof can be considered as a generalization of the proof in [63] to distributed setting. For this
end, we use the following equation for every ﬁx vector u:

Var(x) = E

(cid:104)

x
(cid:107)

−

u

2(cid:105)
(cid:107)

E [x]

u

2
(cid:107)

−

− (cid:107)

Therefore, letting u = g(x(t)

j ) we obtain:
(cid:21)

(cid:13)
g(x(t)
(cid:13)
j )
(cid:13)

2 (cid:12)
(cid:12)x(t)
(cid:12)

j

E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

j −

The ﬁnal step of proof involves bounding the term Var

=

(cid:104)

(cid:13)
(cid:13)
(cid:13)

E

y(t)
j

(42)
=

(cid:104)

(cid:13)
(cid:13)
(cid:13)

E

y(t)
j−1

j

(cid:105)

(cid:12)
(cid:12)x(t)
(cid:12)
−
(cid:12)
(cid:12)x(t)
(cid:12)
(cid:16)

j−1

y(t)
j

(cid:105)

g(x(t)
j )

(cid:13)
2
(cid:13)
(cid:13)

(cid:16)

+ Var

y(t)
j

g(x(t)

(cid:13)
2
(cid:13)
j−1)
(cid:13)

+ Var

(cid:17)

as follows:

−
(cid:12)
(cid:12)x(t)
(cid:12)

j

(cid:17)

j

(cid:12)
(cid:12)x(t)
(cid:12)
(cid:16)

y(t)
j

(cid:17)

(cid:12)
(cid:12)x(t)
(cid:12)

j

Var

(cid:16)

y(t)
j

(cid:17)

(cid:12)
(cid:12)x(t)
(cid:12)

j

= Var


y(t)


j−1 +

1
S(t)

(cid:88)

(cid:104)
g(x(t)

j ; ξ)

ξ∈S (t)
j

g(x(t)

j−1; ξ)

(cid:105) (cid:12)
(cid:12)x(t)
(cid:12)

j

−






Var

(cid:16)

g(x(t)

j ; ξ)

g(x(t)

j

(cid:17)

(cid:12)
(cid:12)x(t)
(cid:12)
j−1; ξ)
2 (cid:12)
(cid:13)
(cid:12)x(t)
(cid:12)
(cid:13)
j−1; ξ)
(cid:13)

j

(cid:21)

−

−

j ; ξ)

g(x(t)

1
S(t)
1
S(t)
(cid:96)2
g
S(t)

=

((cid:47))

≤
((cid:48))

≤

E

E

(cid:20)(cid:13)
(cid:13)g(x(t)
(cid:13)
(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

j −

x(t)
j−1

(cid:13)
(cid:13)
(cid:13)

2 (cid:12)
(cid:12)x(t)
(cid:12)

j

(cid:21)

(48)

(49)

(50)

where ((cid:47)) and ((cid:48)) follows from the deﬁnition of Var(.) and Assumption 6.

The proof for Eq. (47) follows similarly.
The rest of the proof is similar to [63] but for the sake of completeness we add the rest of the proof. For

this purpose, we use the notation Φ(x) (cid:44) h(x) + f (g(x)) and ˜
∇

Lemma G.2. Under Assumption 6 we have:

Φ(x(t)

j ) =

(cid:17)T

(cid:16)

z(t)
j

f (cid:48)(y(t)

j ) + w(t)

j

E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇

Φ(x(t)
j )

˜
∇

−

Φ(x(t)
j )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

G0
S(t)

≤

(cid:88) E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

2(cid:21)

x(t)(cid:13)
(cid:13)
(cid:13)

+

σ2
0
B(t)

−

(51)

with deﬁnitions G0 (cid:44) 3

(cid:16)

gL2
g4

f + g2

f L2
g

(cid:17)

and σ2
0

(cid:44) 3

(cid:16)

Proof. Using Assumption 6, we obtain:

E

(cid:20)(cid:13)
(cid:13)
(cid:13)

˜
∇

Φ(x(t)
j )

− ∇

2(cid:21)

(cid:13)
Φ(x(t)
(cid:13)
j )
(cid:13)

gL2
g2

f σ2

g + g2

f σ2

g(cid:48) + σ2
h

(cid:17)

z(t)
j

z(t)
j

(cid:34)(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)

= E

= E

3E

≤

(cid:17)T

(cid:17)T

f (cid:48)(y(t)

j ) + w(t)

j −

g(cid:48)(x(t)

j )f (cid:48)(g(x(t)

j ))

2(cid:35)

(cid:13)
(cid:13)
h(cid:48)(x(t)
j )
(cid:13)
(cid:13)

−

f (cid:48)(y(t)
j )

−

g(cid:48)(x(t)

j )f (cid:48)(y(t)

j ) + g(cid:48)(x(t)

j )f (cid:48)(y(t)
j )

−

g(cid:48)(x(t)

j )f (cid:48)(g(x(t)

j )) + w(t)

j −

h(cid:48)(x(t)
j )

2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)T

z(t)
j

f (cid:48)(y(t)
j )

g(cid:48)(x(t)

j )f (cid:48)(y(t)
j )

−

2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 3E

(cid:20)(cid:13)
(cid:13)g(cid:48)(x(t)
(cid:13)

j )f (cid:48)(y(t)
j )

−

24

g(cid:48)(x(t)

j )f (cid:48)(g(x(t)

j ))

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 3E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

2(cid:21)

(cid:13)
h(cid:48)(x(t)
(cid:13)
j )
(cid:13)

Assumption 6

≤

j −
(cid:20)(cid:13)
(cid:13)z(t)
(cid:13)

E

3g2
f

j −

g(x(t)
j )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 3g2

gL2
f

E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

j −

x(t)
j

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 3E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

j −

2(cid:21)

(cid:13)
h(cid:48)(x(t)
(cid:13)
j )
(cid:13)

(52)

Next step of proof is to utilize Lemma G.1 in Eq. (52), which leads to

E

(cid:20)(cid:13)
(cid:13)
(cid:13)

˜
∇

Φ(x(t)
j )

Φ(x(t)
j )

− ∇

(cid:16)

3

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤

gL2
(cid:96)4

f L2
g

f + (cid:96)2
S(t)

(cid:17)

(cid:88) E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

2(cid:21)

x(t)(cid:13)
(cid:13)
(cid:13)

−

+ 3 (cid:0)(cid:96)2

gL2
f

(cid:1) E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

0 −

x(t)
0

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 3 (cid:0)(cid:96)2

f

(cid:1) E

(cid:20)(cid:13)
(cid:13)z(t)
(cid:13)

0 −

x(k)
0

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+

j
(cid:88)

r=1

3L2
h
S(t)

E

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

r −

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

x(t)
r

+ 3E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

0 −

h(cid:48) (cid:16)

x(t)
0

2(cid:21)

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:16)

3

(a)
=

f L2

g + L2
h

gL2
(cid:96)4

f + (cid:96)2
S(t)

(cid:17)

j
(cid:88)

E

r=1

(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

r −

x(t)
r−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+

σ2
0
B(t)

(53)

where (a) comes from algorithm and computing initial full batch in the beginning of each epoch, and using
Assumption 9 as well as following bounds:

E

(cid:20)(cid:13)
(cid:13)y(t)
(cid:13)

0 −

2(cid:21)

(cid:13)
g(x(t)
(cid:13)
0 )
(cid:13)

≤

σ2
g
B(t)

, E

(cid:20)(cid:13)
(cid:13)z(t)
(cid:13)

0 −

2(cid:21)

(cid:13)
g(cid:48)(x(k)
(cid:13)
0 )
(cid:13)

≤

σ2
g(cid:48)
B(t)

and E

(cid:20)(cid:13)
(cid:13)w(t)
(cid:13)

0 −

h(cid:48)(x(k)
0 )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

σ2
h(cid:48)
B(t)

≤

(54)

The rest of the proof is to show that E

(cid:104)(cid:13)
(cid:13)
G

2(cid:105)

η(x2

j )(cid:13)
(cid:13)

(cid:15) where

G

≤

η(x(t)

j ) (cid:44) 1

η

(cid:16)
x(t)

j −

(cid:17)

ˆx(t)
j+1

and

However, Algorithm 2 produces approximate proximal gradient mapping:

ˆx(t)
j+1 = Πη

f

(cid:16)

x(t)

j −

η

(cid:104)

∇

(cid:105)(cid:17)

Φ(x(t)
j )

η(x(t)
˜
G

j ) (cid:44) 1
η

(cid:16)

x(t)

j −

(cid:17)

x(t)
j+1

(cid:17)
Φ(x(t)
j )

η

∇

. So the following Lemmas connect two gradient approximations:

(cid:16)

where ˆx(t)

j+1 = Πη

j −
Lemma G.3. We have:

f

x(t)

E

(cid:104)(cid:13)
η(x2
(cid:13)
G

j )(cid:13)
(cid:13)

2(cid:105)

2(cid:21)

(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G

2E

≤

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)

˜
∇

Φ(x(t)
j )

− ∇

2(cid:21)

(cid:13)
Φ(x(t)
(cid:13)
j )
(cid:13)

(55)

(56)

Proof. Using triangle inequality
η(x(t)
˜
G

j ), we have

η(x2
G

j ) and

(cid:13)
(cid:13)x(t)
(cid:13)

j −

ˆx(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)x(t)
(cid:13)

j −

x(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

≤

+ 2

(cid:13)
(cid:13)x(t)
(cid:13)

j+1 −

ˆx(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

and the deﬁnition of

E

(cid:104)(cid:13)
(cid:13)
G

2(cid:105)

η(x2

j )(cid:13)
(cid:13)

2E

≤

= 2E

= 2E

2(cid:21)

2(cid:21)

2(cid:21)

(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G
(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G
(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G

+

+

(cid:13)
2
(cid:13)x(t)
(cid:13)
j −
η2
(cid:13)
(cid:16)
2
(cid:13)Πη
(cid:13)
η2
(cid:13)
(cid:104) ˜
(cid:13)
(cid:13)
∇

f

+ 2

ˆx(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

x(t)

j −

η

(cid:104) ˜
∇

Φ(x(t)
j )

(cid:105)(cid:17)

(cid:16)

Πη
f

x(t)

j −

η

(cid:104)

∇

−

Φ(x(t)
j )

(cid:105)(cid:17)(cid:13)
2
(cid:13)
(cid:13)

Φ(x(t)
j )

Φ(x(t)
j )

(cid:105)(cid:13)
2
(cid:13)
(cid:13)

− ∇

(57)

25

Based on the deﬁnition Ψ(x) (cid:44) F (g(x)) + h(x) + r(x) we have the following lemma:

Lemma G.4. With deﬁnition LΦ = LF + Lh, we have:

E

(cid:104)
Ψ(x(t)

(cid:105)
j+1)

E

(cid:104)
Ψ(x(t)
j )

(cid:105)

(cid:0)η

1
2

−

−

≤

LΨη2(cid:1) E

2(cid:21)

(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G

+

η
2

(cid:13)
(cid:13)
(cid:13)∇

Φ(x(t)
j )

(cid:13)
2
Φ(x(t)
(cid:13)
j )
(cid:13)

˜
∇

−

(58)

and

E

(cid:104)

Ψ(x(t)

(cid:105)
j+1)

E

(cid:104)

(cid:105)
Ψ(x(t)
j )

≤

(cid:18) 1

−

4η −

E

η
8
−
(cid:19)
LΦ
2

(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G
(cid:20)(cid:13)
(cid:13)x(t)
(cid:13)

j −

E

2(cid:21)

+

x(t)
j+1

(cid:20)(cid:13)
(cid:13)
(cid:13)∇

Φ(x(t)
j )

˜
∇

−

2(cid:21)

(cid:13)
Φ(x(t)
(cid:13)
j )
(cid:13)

E

3η
4
2(cid:21)
(cid:13)
(cid:13)
(cid:13)

(59)

Proof. Using LΦ-Lipschitz continuity of Ψ and the optimality of the 1
we obtain

η −

strongly convex of subproblem (r(x)),

Ψ(x(t)

j+1) = F (g(x(t)
F (g(x(t)

j+1)) + h(x(t)
j )) + h(x(t)

j ) +

j+1) + r(x(t)
(cid:68)(cid:104)

j+1)

F (g(x(t)

≤
= F (g(x(t)

j )) + h(x(t)

j ) +

F (g(x(t)

∇
(cid:68)(cid:104) ˜
∇
h(x(t)
j )

(cid:105)

j )) +

(cid:105)
h(x(t)
j )
(cid:105)
h(x(t)
j )

∇
j )) + ˜
∇
(cid:104) ˜
F (g(x(t)
j )) + ˜
∇
∇
(cid:13)
2
(cid:13)
(cid:13)

(cid:68)(cid:104)

∇
(cid:18) 1

+

+

F (g(x(t)

j )) +
LF + Lh
2

∇
(cid:19) (cid:13)
(cid:13)x(t)
(cid:13)

2η −

j+1 −

−
x(t)
j

, x(t)

j+1 −

, x(t)

j+1 −
(cid:105)
h(x(t)
j )

(cid:69)

(cid:69)

x(t)
j

x(t)
j

+

+

, x(t)

j+1 −

LF + Lh
2
(cid:13)
(cid:13)x(t)
(cid:13)
(cid:69)

1
2η
x(t)
j

j+1 −

(cid:13)
(cid:13)x(t)
(cid:13)
j+1 −
(cid:13)
2
x(t)
(cid:13)
(cid:13)
j

x(t)
j

(cid:13)
2
(cid:13)
(cid:13)

+ r(x(t)

j+1)

+ r(x(t)

j+1)

F (g(x(t)

j )) + h(x(t)

j ) + r(x(t)
j )

≤

−

+

(cid:13)
(cid:104)
(cid:13)
(cid:13)

η
2

∇

F (g(x(t)

j )) +

h(x(t)
j )

∇

= Ψ(x(t)
j )

1
2η

(cid:13)
(cid:13)x(t)
(cid:13)

j −

x(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

−

−

2η −

1
2η
(cid:104) ˜
−
∇
(cid:18) 1

(cid:13)
(cid:13)x(t)
(cid:13)

j −

x(t)
j+1

(cid:13)
2
(cid:13)
(cid:13)

(cid:18) 1

−

2η −

LF + Lh
2

(cid:19) (cid:13)
(cid:13)x(t)
(cid:13)

j+1 −

x(t)
j

(cid:13)
2
(cid:13)
(cid:13)

h(x(t)
j )

F (g(x(t)

j )) + ˜
∇
(cid:19) (cid:13)
(cid:13)x(t)
(cid:13)

LF + Lh
2

j+1 −

(cid:105)(cid:105)(cid:13)
2
(cid:13)
(cid:13)

+

1
2η

x(t)
j

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)x(t)
(cid:13)
(cid:13)
η
(cid:13)
(cid:13)
2

˜
∇

x(t)
j

(cid:13)
2
(cid:13)
(cid:13)

j+1 −

Φ(x(t)
j )

− ∇

Φ(x(t)

j ))

(cid:13)
2
(cid:13)
(cid:13)
(60)

Next, we complete the proof by taking expectation from both sides of Eq. (60) concludes the proof of Eq. (58).

Next, using Lemma G.2, we have

E

η
4

−

2(cid:21)

(cid:20)(cid:13)
(cid:13)
˜
η(x2
(cid:13)
(cid:13)
j )
(cid:13)
(cid:13)
G

E

η
8

≤ −

(cid:104)(cid:13)
η(x2
(cid:13)
G

j )(cid:13)
(cid:13)

2(cid:105)

+

E

η
4

(cid:20)(cid:13)
(cid:13)
(cid:13)∇

Φ(x(t)
j )

˜
∇

−

Φ(x(t)
j )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

(61)

We can prove Eq. (59), by simply adding Eq. (61) to both sides of Eq. (58).

We note that Lemmas G.3 and G.4 are an extension of Lemma 3 and 4 in Appendix Section of [63] with
diﬀerence that here the function Ψ(x) includes extra function h(x), which leads to LΦ = LF + Lh which is
bigger than LF in [63].

The rest of the proof is same as the proof of Theorems 4 and 8 in [63] which is based on Lemmas G.3

and G.4, and for more details we refer the reader to the Appendix section of the reference [63].

26

H Approximate Approach For DRO with Wesserstein Metric with

Non-convex Objectives

For the non-convex objectives, the optimization problem in Eq. (3) can be upper-bounded with the optimization
problem as follows:

inf
x∈X

sup
Q∈ ˆDN

EQ [h(x; ξ)]

minimize
x

≤

r(x) (cid:44) 1
m

m
(cid:88)

fi(xi)

(62)

subject to

˜gi(x)

0,

≤

i=1
i
∀

∈ {

1,

, m
}

· · ·

We note that for the case of convex objective in optimum solution inequality holds with equality; however, in
case of non-convex cost functions we do not have equality necessarily. We note that Eq. (62) with non-convex
constraint can not be easily solved via augmented function approach in previous section. In order to solve
the upper bound optimization problem in Eq. (62) eﬃciently via our suggested composite approach. For this
end, we suggest to solve the following optimization problem where in constrained are modiﬁed to be convex:

minimize
x

f (x) (cid:44) 1
m

m
(cid:88)

i=1

fi(xi)

(63)

(cid:107)
where we choose ˜gi such that ˆgi are strongly convex. Therefore, we have the following relationship between
optimization problem:

∈ {

· · ·

≤

subject to

ˆgi (cid:44) ˜gi(x) + µi

2

x
(cid:107)

0,

i
∀

1,

, m
}

(64)

(65)

(66)

(67)

inf
x∈X

sup
Q∈ ˆDN

EQ [h(x; ξ)]

minimize
x

subject to

minimize
x

≤

≤

r(x) (cid:44) 1
m

m
(cid:88)

fi(xi)

˜gi(x)
0,
≤
r(x) (cid:44) 1
m

1,

, m
}

· · ·

fi(xi)

i=1
i
∀
∈ {
m
(cid:88)

i=1
2
(cid:107)

x
(cid:107)

0,

i
∀

≤

1,

∈ {

, m
}

· · ·

subject to

˜gi(x) + µi

27

