0
2
0
2

l
u
J

2
2

]

M
E
.
n
o
c
e
[

1
v
6
0
6
1
1
.
7
0
0
2
:
v
i
X
r
a

Mode Treatment Eﬀect

Neng-Chieh Chang∗

Abstract

Mean, median, and mode are three essential measures of the centrality of probability
distributions.
In program evaluation, the average treatment eﬀect (mean) and the
quantile treatment eﬀect (median) have been intensively studied in the past decades.
The mode treatment eﬀect, however, has long been neglected in program evaluation.
This paper ﬁlls the gap by discussing both the estimation and inference of the mode
treatment eﬀect. I propose both traditional kernel and machine learning methods to
I also derive the asymptotic properties of the
estimate the mode treatment eﬀect.
proposed estimators and ﬁnd that both estimators follow the asymptotic normality but
with the rate of convergence slower than the regular rate √N , which is diﬀerent from
the rates of the classical average and quantile treatment eﬀect estimators.

1 Introduction

The eﬀects of policies on the distribution of outcomes have long been of central interest
in many areas of empirical economics. A policy maker might be interested in the diﬀer-
ence of the distribution of outcome under treatment and the distribution of outcome in
the absence of treatment. The empirical studies of distributional eﬀects include but not
are not limited to Freeman (1980), Card (1996), DiNardo, Fortin, & Lemieux (1995), and
Bitler, Gelbach, & Hoynes (2006). Most researches use the diﬀerence of the averages or
quantiles of the treated and untreated distribution, known as average treatment eﬀect and
quantile treatment eﬀect, as a summary for the eﬀect of treatment on distribution. The
mode of a distribution, which is also an important summary statistics of data, has long been
ignored in the literature. This paper ﬁlls up the gap by studying the mode treatment eﬀect:
the diﬀerence of the modes of the treated and untreated distribution. Compared to the
average and the quantile treatment eﬀect, the mode treatment eﬀect has two advantages:
(1) mode captures the most probable value of the distribution under treatment and in the
absence of treatment. It provides a better summary of centrality than average and quantile
when the distributions are highly skewed; (2) mode is robust to heavy-tailed distributions
where outliers don’t follow the same behavior as the majority of a sample.
In economic
studies, it is especially often to confront a skewed and heavy-tailed distribution when the
outcome of interest is income or wage.

∗Department of Economics, University of California Los Angeles, 315 Portola Plaza, Los Angeles, CA

90095, USA. email: nengchiehchang@g.ucla.edu

1

 
 
 
 
 
 
This paper discusses the estimation and inference of the mode treatment eﬀect under the
Strong Ignorability assumption (Rosenbaum & Rubin, 1983), which states that conditional
on a vector of control variables the treatment is randomly assigned. The ﬁrst estimator I
propose is the kernel estimator. I estimate the density function of the outcome distribution
using the kernel method and deﬁne the maximum of the estimated density function as
the estimator of the mode. While the kernel estimator is a straightforward estimator, it
requires to estimate the conditional density function in the process, and the estimation of
the conditional density function may be diﬃcult in practice when there exist more than two
or three control variables, due to the curse of dimension. The kernel estimator is appropriate
if there are less than three control variables. In practice, however, researchers may want to
include as many control variables as possible in order to make their identiﬁcation robust. In
this circumstance, the curse of dimension may lead to inaccurate estimation and misleading
inference.

To address this problem, I propose the ML estimator. The key feature of the proposed
ML estimator is that it translates the estimation of the conditional density function into
the estimation of conditional expectation, which we can apply a rich set of ML methods,
such as Lasso, random forests, neural nets, and etc, to estimate. This feature provides
researchers with the ﬂexibility to apply ML methods to estimate the density function of
the outcome distribution. By the virtue of ML methods, the proposed ML estimator can
handle the situation when there exist many control variables, even the number of control
variables is comparable to or more than the sample size. However, it is well-known that the
regularization bias embedded in ML methods may lead to the bias of the ﬁnal estimator and
misleading inference (Chernozhukov et al., 2018). To solve this problem, I further derive the
Neyman-orthogonal scores (Chernozhukov et al., 2018) for each estimation which requires
the ﬁrst-step estimation of the conditional expectation. These Neyman-orthogonal scores,
to my best knowledge, are new results. The proposed ML estimator is built on the newly
derived Neyman-orthogonal score, and hence, it is robust to the regularization bias of the
ﬁrst-step ML estimation.

I derive the asymptotic properties for both the proposed kernel and ML estimators. I show
that both estimators are consistent and asymptotically normal with the rate of convergence
√Nh3, where N is the sample size and h is the bandwidth of the chosen kernel, which is
slower than the traditional rate of convergence √N in the estimation of mean and quantile.
In fact, this rate of convergence complies the intuition. While the estimators of mean and
quantile are the weighted average of all the available observations, only a small portion of
observations near the mode provides the information to the estimator of the mode. This
explains the slower rate of convergence for the proposed estimators.

This paper contributes to the program evaluation literature which includes the studies of

average treatment eﬀect: Rosenbaum & Rubin (1983), Heckman & Robb (1985), Heckman, Ichimura, & Todd
(1997), Hahn (1998), and Hirano, Imbens, & Ridder (2003); the studies of quantile treat-
ment eﬀect: Abadie, Angrist, & Imbens (2002), Chernozhukov & Hansen (2005), and Firpo
(2007); the studies of mode estimation and mode regression: Parzen (1962), Eddy et al.
(1980), Lee (1989), Yao & Li (2014), and Chen, Genovese, Tibshirani, Wasserman, et al.
(2016); as well as the causal inference of ML methods: Belloni et al. (2012), Belloni et al.
(2014), Chernozhukov et al. (2015), Belloni et al. (2017), Chernozhukov et al. (2018), and
Athey et al. (2019). This paper is also closely related to the robustness of average treat-

2

ment eﬀect estimation discussed in (Robins & Rotnitzky, 1995) and the general discussion
in (Chernozhukov, Escanciano, Ichimura, & Newey, 2016). The asymptotic properties of the
robust estimators discussed in these papers remain unaﬀected if only one of the ﬁrst-step
estimation with classical nonparametric method is inconsistent.

Plan of the paper. Section 2 sets up the notation and framework for the discussion of
the mode treatment eﬀect. Section 3 discusses the kernel method and derives the asymptotic
properties. Section 4 presents the ML estimator for density estimation and the corresponding
Neyman-orthogonal score. I combine the Neyman-orthogonal score with the cross-ﬁtting al-
gorithm to propose the ML estimator of the mode treatment eﬀect, and derive its asymptotic
properties. Section 5 concludes this paper.

2 Notation and Framework

×

Let Y be a continuous outcome variable of interest, D the binary treatment indicator, and
X d
1 vector of control variables. Denote by Y1 an individual’s potential outcome when
D = 1 and Y0 if D = 0. Let fY1 (y) and fY0 (y) be the marginal probability density function
(p.d.f.) of Y1 and Y0, respectively. The modes of Y1 and Y0 are the values that appear with
the highest probability. That is,

θ∗
1 ≡

arg max
y∈Y1

fY1 (y) and θ∗

0 ≡

arg max
y∈Y0

fY0 (y) ,

Y1 and

Y0 are the supports of Y1 and Y0. Here I assume that θ∗

0 are unique,
where
meaning that both Y1 and Y0 are unimodal. I also assume that the modes θ∗
0 are in
the interior of the common supports of Y1 and Y0. These conditions are formally stated in
the following assumption:

1 and θ∗

1 and θ∗

Assumption 1 (Uni-mode)

For all ǫ > 0,

•

and

1, θ∗
θ∗

0 ∈

•

Int (

Y1 ∩ Y0).

sup
y:|y−θ∗

1 |>ε

sup
y:|y−θ∗

0 |>ε

fY1 (y) < fY1 (θ∗

1) for y

fY0 (y) < fY0 (θ∗

0) for y

∈ Y1,

∈ Y0.

Assumption 1 has been widely adopted in many studies (Parzen, 1962; Eddy et al., 1980; Lee,
1989; Yao & Li, 2014). Under Assumption 1, the mode treatment eﬀect is uniquely deﬁned as
∆∗
θ∗
0. The following states the strong ignorability assumption (Rosenbaum & Rubin,
≡
1983):

θ∗
1 −

Assumption 2 (strong ignorability)

(Y0, Y1)

D

X

|

⊥

•

3

0 < P

D = 1

X

< 1

|

•

(cid:0)

(cid:1)

The ﬁrst part of Assumption 2 assumes that potential outcomes are independent of treatment
after conditioning on the observable covariates X. The second part states that for all values
of X, both treatment status occur with a positive probability. Under the strong ignorability
condition, both fY1 and fY0 can be identiﬁed from the observable variables (Y, D, X) since

fY |D=1,X

y

|

x

= fY1|D=1,X

y

and thus

Similarly, we have

(cid:0)
fY1(y) = E

(cid:1)
fY1|X
h

y

|

(cid:0)

fY0 (y) = E

x

= fY1|X

y

|

(cid:1)
fY |D=1,X
h

(cid:0)

y

|

(cid:0)

x

,

|

(cid:1)

X

.

(cid:1)i

y

X

.

|

(cid:0)

X

= E

(cid:1)i
fY |D=0,X
h

(cid:1)i
(cid:0)
Equation (2.1) and (2.2) shows the identiﬁcation result of the density function fY1 and fY0.
1 and θ∗
Then it is straightforward to identify their modes θ∗
0:

(2.1)

(2.2)

fY |D=1,X
h
X

θ∗
1 = arg max

E

y∈Y1

y

X

|

and θ∗

0 = arg max

E

y∈Y0

y

X

.

(2.3)

fY |D=0,X
h

|

(cid:1)i
If both fY |D=1,X
y
further identify the modes using the ﬁrst-order conditions under Assumption 1:
(cid:0)

(cid:0)
and fY |D=0,X

(cid:1)i

X

y

(cid:0)

(cid:1)

(cid:0)

|

|

are diﬀerentiable with respect to y, we can

(cid:1)
f (1)
f (1)
Y |D=0,X
Y |D=1,X
h
h
∂sm (y, x) /∂ys denotes the partial derivatives with respect to y.

= 0 and E

θ∗
1 |

θ∗
0 |

= 0,

(cid:1)i

(cid:1)i

X

X

(cid:0)

(cid:0)

(2.4)

where m(s) (y, x)

Equation (2.1)-(2.4) provide us a direct way to estimate the modes θ∗

0. Intuitively,
we estimate the density functions fY1(y) and fY0(y) in the ﬁrst step and use the maximizers
of the estimated density functions as the estimators of the modes. Section 3 and 4 presents
the kernel and ML estimation method, respectively.

1 and θ∗

E

≡

3 The Kernel Estimation

In this section, I propose kernel estimators for θ∗
θ∗
1 −
·
functions fY1 (y) and fY0 (y) as,

0, and the mode treatment eﬀect ∆∗ =
) be a kernel function with bandwidth h. Deﬁne the estimators of the density

θ∗
0. Let K(

1, θ∗

ˆfY1 (y) =

ˆfY0 (y) =

1
n

1
n

Xi

,

y

|

(cid:0)

(cid:1)

Xi

y

|

(cid:0)

(cid:1)

n

i=1
X
n

ˆfY |D=1,X

ˆfY |D=0,X

i=1
X

4

with the kernel estimators

ˆfY |D=1,X

ˆfY |D=0,X

y

(cid:0)

|

x

=

y

|

(cid:1)

x

=

P

n
j=1

1

P

(cid:0)

n
j=1 DjKh

Yj

y
−
n
j=1 DjKh
(cid:0)

Kh

x

x
(cid:1)

−

Xj
(cid:0)

Xj

−

,

(cid:1)

Kh

P
−
n
j=1

Dj
1
(cid:1)
−

Dj
(cid:0)

y

Yj

(cid:0)
−
Kh

(cid:1)

(cid:0)

(cid:1)
Kh

x

x
(cid:1)

−

Xj
(cid:0)

Xj

−

(cid:1)

where Kh

y

Yj

−

(cid:0)
= h−1K

(cid:1)
y−Yj
h

and

P

(cid:0)

(cid:0)

(cid:1)
Kh

(cid:16)

Xj

x

−

(cid:17)
= h−dK

(cid:0)

(cid:1)

Xj1

x1 −
h

K

...

×

×

(cid:19)

xd −
h

(cid:18)

(cid:18)

Xjd

Then it is straightforward to deﬁne the estimators of the modes θ∗

(cid:1)

.

(cid:19)
1 and θ∗
0:

ˆθ1 ≡
ˆθ0 ≡

arg max
y

arg max
y

ˆfY1 (y) ,

ˆfY0 (y) .

The estimator of the mode treatment eﬀect ∆∗ is ˆ∆
impose the following conditions on the kernel K (

):

·

ˆθ1 −

≡

ˆθ0. Through out the paper, I

Assumption 3:

K (u)

¯K <

.

∞

≤
K (u) du = 1,

(cid:12)
(cid:12)
R
K (u) is diﬀerentiable.

(cid:12)
(cid:12)

R

uK (u) du = 0,

•

•

•

u2K (u) du <

.

∞

R

The ﬁrst part of Assumption 3 requires that K (u) is bounded. Although the second part
implies that K (u) is a ﬁrst-order kernel, the arguments in this paper can be easily extended
to higher-order kernels. We assume the ﬁrst-order kernel here just for simplicity. The third
part imposes enough smoothness on K (u).

y

x

and fY |D=0,X

Theorem 1 (Consistency) Suppose Assumption 1-3 hold. Assume that the density
functions fY |D=1,X
some function d (x) with E
compact
and
(cid:2)
bounded away from zero. If n
and ˆθ0

(cid:0)
. We also assume that the density functions fX|D=1 (x) and fX|D=0 (x) are
θ∗
1

are (i) continuous in y, (ii) bounded by
with

∈ Y
0, then we have ˆθ1

x
y
for all y
(cid:1)

(cid:3)
→ ∞

, and (iii) y

0, and ln n

nhd+1

and x

d (X)

p
→

∈ X

∈ Y

, h

(cid:0)
∞

→

→

<

X

Y

−1

(cid:1)

|

|

(cid:0)

(cid:1)

p
→

θ∗
0.

Theorem 2 (Asymptotic Normality) Suppose that the assumptions of Theorem 1 hold.
Assume that f (2)

are continuous at y = θ∗

and f (2)

1 and y = θ∗

0 for

x

x

y

y

Y |X,D=1

Y |X,D=0

|

|

(cid:0)

(cid:1)

(cid:0)

(cid:1)

5

all x, respectively. If n
and √nh3h2
0, then

→

→ ∞

, h

→

0, √nh3 (ln n)

nhd+3

−1

0, (ln n)

nhd+5

→

−1

0,

→

(cid:0)

(cid:1)

(cid:0)

(cid:1)

where

√nh3

√nh3

ˆθ1 −
(cid:16)
ˆθ0 −
(cid:16)
M1 ≡

E

M0 ≡

E

V1 = κ(1)

0 E

V0 = κ(1)

0 E

"

"

(cid:1)

(cid:1)

θ∗
1

θ∗
0

(cid:17)

d
→

d
→

N

0, M −1

1 V1M −1
1

(cid:0)

N

0, M −1

0 V0M −1
0

(cid:0)

(cid:17)
f (2)
Y |X,D=1
h
f (2)
Y |X,D=0
h

θ∗
1 |

X

(cid:0)

θ∗
0 |

X

,

,

(cid:1)i

(cid:1)i
X

(cid:0)
fY |X,D=1
P

θ∗
1 |
X
D = 1
(cid:0)
|
θ∗
0 |
X
D = 0
(cid:0)
|

(cid:1)
X

(cid:0)
fY |X,D=0
P

,

,

#

(cid:1)

#

(cid:1)

(cid:1)

and κ(1)

0 =

K (1) (u)2 du. Further, we have

(cid:0)

R

√nh3

ˆ∆

∆∗

−

d
→

N (0, M1V1M1 + M0V0M0) .

(cid:16)

(cid:17)

Theorem 1 and 2 show that the asymptotic properties of the estimator of the mode
treatment eﬀect. We can see that the proposed estimators follows the asymptotic normality
but with the rate of convergence slower than the regular rate √N . The intuition is that,
unlike the estimation of the average and the quantile treatment eﬀect, the estimation of
modes only uses a small portion of total observations which are around the modes. The
usage rate of observations determines that the rate of convergence is slower than the regular
rate √N .

To estimate the asymptotic variances, we deﬁne π0 (X)

propensity score. The consistent variance estimators are

P

D = 1

|

≡

X

to be the

(cid:0)

(cid:1)

ˆM1 =

ˆM0 =

1
n

1
n

n

i=1
X
n

ˆf (2)
Y |X,D=1

ˆθ1 |

Xi

(cid:16)

ˆf (2)
Y |X,D=0

ˆθ0 |

Xi

(cid:17)

,

,

i=1
X
n

1
n

(cid:16)
ˆfY |X,D=1

ˆθ1 |
(cid:16)
ˆπ (Xi)

(cid:17)
Xi

ˆV1 = κ(1)
0

ˆV0 = κ(1)
0

1
n

i=1
X
n

i=1
X

,

(cid:17)

.

(cid:17)

ˆfY |X,D=0

ˆθ0 |
(cid:16)
ˆπ (Xi)

Xi

6

Theorem 3 (Variance Estimation) Suppose that the assumptions in Theorem 2 hold.
Let ˆπ (x) be an uniformly consistent estimator for π0 (x). If n
nhd+5
ln n
ˆV1 ˆM −1
ˆM −1
(cid:0)
(cid:1)
1
1

0, and
→ ∞
V0. Thus we have

p
V1, ˆV0
→
0 V0M −1
M −1
0 .

0, then ˆM1
1 V1M −1
M −1
1

p
M1, ˆM0
→
and ˆM −1

p
→
ˆV0 ˆM −1
0

p
→

, h

→

M0, ˆV1
p
→

→
p
→

0

4 The Machine Learning Estimation

In this section, I propose the ML estimator of the mode treatment eﬀect. The ML estimator
can accommodate a large number of control variables, potentially more than the sample
size. This ﬂexibility will enable researcher to include as many control variables they consider
important to make their identiﬁcation assumptions more plausible. The key to implement
ML methods is to replace the estimation of the conditional density function with the estima-
tion of the conditional expectation. To begin with, the estimation of the conditional density
function in the traditional kernel estimation is

ˆfY |D=1,X

x

=

y

|

P

(cid:0)

(cid:1)

n
j=1 DjKh

Yj

y
−
n
j=1 DjKh
(cid:0)

Kh

x

x
(cid:1)

−

Xj
(cid:0)

Xj

−

.

(cid:1)

Notice that we can divide both the numerator and the denominator by
to obtain

P

(cid:1)

(cid:0)

n
j=1 Kh

x

Xj

−

P
x

−

(cid:0)

(cid:1)

Xj

.

(cid:1)

ˆfY |D=1,X

x

=

y

|

(cid:0)

(cid:1)

n
j=1 DjKh

Yj

y
−
n
j=1 DjKh
(cid:0)

Kh

x

x
(cid:1)

−

Xj
(cid:0)

−
/

/
Xj
n
j=1 Kh
(cid:1)
P

n
j=1 Kh
x

−

Xj
(cid:0)

P

The numerator is an kernel estimator of E
kernel estimator of the propensity score E
estimator of E

DKh (y

Y )

X

−

|

P

(cid:1)
(cid:0)
−
= π (X). Hence, ˆfY |D=1,X
/π (X). Then the marginal density estimator
(cid:0)

and the denominator is an
is an

P
|

Y )

X

x

y

(cid:1)

(cid:1)

(cid:3)

|

(cid:0)
DKh (y
X
D
(cid:2)
(cid:2)
n

(cid:3)

|

(cid:2)

(cid:3)
ˆfY1 (y) =

1
n

ˆfY |D=1,X

Xi

y

|

(cid:0)

(cid:1)

i=1
X

deﬁned in the previous section can be interpreted as an estimator of

E

E

"

(cid:2)

DKh (y

−
π (X)

Y )

X

|

= E

#

(cid:3)

(cid:20)

DKh (y

−
π (X)

Y )

.

(cid:21)

Therefore, we can use the machine learning estimator of E
as an estimator for
fY1 (y). We have successfully translate the estimation of the conditional density function
into the estimation of the conditional expectation, which is the propensity score π(X).

i

h

DKh(y−Y )
π(X)

Here we pursue a little bit further to construct the Neyman-orthogonal score (Chernozhukov et al.,

2018) for the robustness of the ﬁrst-step estimation:

m1 (Z, y, η10) =

DKh (y

−
π0 (X)

Y )

D

−

π0 (X)

−
π0 (X)

E

Kh (y

Y )

|

−

X, D = 1

,

(4.1)

(cid:2)

(cid:3)

7

where Z = (Y, D, X) and η0 = (π0, g10) with g10 (X)
the Neyman-orthogonal score for fY0 (y) is

≡

E

Kh (y

Y )

−

(cid:2)

E

Kh (y

Y )

−

|

|

X, D = 1

. Similary,

(cid:3)

X, D = 0

,

(4.2)

m2 (Z, y, η20) =

(1

D) Kh (y
−
1

−
π0 (X)

−

Y )

−

π0 −
1
−

D (X)
π0 (X)

where η20 = (π0, g20) with g20 (X)
. Equation (4.1) and (4.2), to
−
my best knowledge, should be the new results for density estimation. The Neyman orthogo-
nality will make the estimation of the density functions more robust to the ﬁrst-step estima-
tion. Now I combine (4.1) and (4.2) with the cross-ﬁtting algorithm (Chernozhukov et al.,
2018) to propose the new estimator:

Kh (y

Y )

≡

E

(cid:3)

(cid:2)

|

(cid:2)
X, D = 0

(cid:3)

Deﬁnition.

1. Take a K-fold random partition (Ik)K

k=1 of [N] =

is n = N/K. For each k

[K] =

{

∈

1, ..., K

2. For each k

∈

[K], use the auxiliary sample I c

{

1, ..., N

}
, deﬁne the auxiliary sample I c

such that the size of each Ik
.

}
k to construct machine learning estimators

1, ..., N

k ≡ {

}

ˆπk (x) , ˆg1k (x) , and ˆg2k (x)

of π0 (x), g10 (x), and g20 (x).

3. Construct the estimator of fY1 (y) and fY0 (y):

K

En,k

m1 (Z, y, ˆη1k)

and ˆfY0 (y) =

ˆfY1 (y) =

1
K

where En,k

m (Z)

Xk=1

(cid:2)
= n−1

4. Construct the estimator for θ∗

P

(cid:3)

(cid:2)

(cid:3)
m (Zi).

i∈Ik
1 and θ∗
0

1
K

K

Xk=1

En,k

m2 (Z, y, ˆη2k)

(cid:2)

(cid:3)

ˆθ1 = arg max

y

ˆfY1 (y) and ˆθ0 = arg max

y

ˆfY0 (y) .

5. Construct the estimator for the mode treatment eﬀect ˆ∆ = ˆθ1 −
η10 kP,2≤
ˆg1k −

Theorem 4.1. Suppose that with probability 1
2
ˆπk −
π0 k
ˆπk −
P,2 +
kP,∞≤
k
k
ǫN = o((Nh3)−1/4) and Nh7
0, then we have

ˆη1k −
k
π0 kP,2 × k

o (1),
ˆπk −

κ, and

−
k

1/2

1/2

−

ˆθ0.

εN ,
g10 kP,2≤

(εN )2. If

→

θ∗
1

θ∗
0

(cid:17)

(cid:17)

√nh3

(cid:16)

√nh3

ˆθ1 −
ˆθ0 −

(cid:16)
ˆf (2)
Y |D=1,X(ˆθ1 |

P

8

d
→
d
→

N

0, M −1

1 V1M −1
1

(cid:0)

N

0, M −1

0 V0M −1
0

,

.

(cid:1)

(cid:0)
As for the variance estimation, recall that the kernel estimator of M1 in the previous
x), where

section is ˆM1 = N −1

(cid:1)

N
i=1

ˆf (2)
Y |D=1,X

x

=

y

|

(cid:0)

(cid:1)

n
j=1 DjKh
y
Yj
n
j=1 DjKh

−

(cid:0)

P

(2) Kh
Xj
x
(cid:1)
(cid:0)

−

Xj

x

−

.

(cid:1)

Notice that we can divide both the numerator and the denominator by
to obtain

P

(cid:1)

(cid:0)

n
j=1 Kh

x

Xj

−

ˆf (2)
Y |D=1,X

x

=

y

|

(cid:0)

(cid:1)

n
j=1 DjKh
Yj
y
n
j=1 DjKh

−

(cid:0)

P

x

(2) Kh
Xj
x
(cid:1)
(cid:0)

−

−
/

P
Observe that the numerator is an kernel estimator of E
nominator is an kernel estimator of the propensity score E
ˆfY |D=1,X

is an estimator of E

Y )

P

X

x

y

h (y

(cid:1)

(cid:0)

|

DK (2)
h
DK (2)
h (y−Y )
π(X)

−

|

i

(cid:1)
machine learning estimator of E

(cid:0)

(cid:20)
a DML estimator using the Neyman-orthogonal functional form

(cid:21)

P
x

(cid:1)

Xj

−

(cid:0)

.

(cid:1)

n
j=1 Kh
Xj
x
(cid:0)

−

Y )

(cid:1)
and the de-
X
|
= π (X). Hence,

−
X

Xj
/
n
j=1 Kh
(cid:1)
P
(cid:0)
DKh (y
D

(cid:2)
/π (X). Hence, we can use the

(cid:3)

|

(cid:2)

(cid:3)

as an estimator for M1. We can also construct

DK (2)

h (y
−
π (X)

Y )

D

−

π0 (X)

−
π0 (X)

E

h (y

K (2)
h

Y )

X, D = 1

−

|

In step 1, we use machine learning methods to estimate π0(X) and E[K (2)
h
1] using auxiliary sample I c

i
ˆθ1 −
Y
k. In Step 2, we construct the DML estimator of M1:
(cid:16)

X, D =

|

(cid:17)

ˆM1 =

1
K

K

i∈Ik
Xk=1 X

DiK (2)
ˆθ1 −
h
(cid:16)
ˆπ (Xi)

Yi

(cid:17)

−

Di −

ˆπ0 (Xi)

ˆπ0 (Xi)

ˆE

K (2)
h
(cid:20)

(cid:16)

ˆθ1 −

Y

|

(cid:17)

Xi, D = 1

.

(cid:21)

By the general DML theory (Chernozhukov et al., 2018), ˆM1 is a consistent estimator of M1.
Similarly, we can construct the DML estimators for V1, M0, and V0 using the following table:

M1 E

M0 E

V1

V0

Original Form
f (2)
Y |X,D=1
h
κ(1)
0 E

θ∗
1 |
fY |X,D=1(θ∗
(cid:0)
P (D=1|X)

(cid:20)
f (2)
Y |X,D=0
h
κ(1)
0 E

θ∗
1 |
fY |X,D=0(θ∗
(cid:0)
P (D=0|X)

X

1|X)

(cid:1)i

X

1|X)

(cid:1)i

(cid:21)

(cid:21)

(cid:20)

E

(cid:20)
E

DK (2)

h (θ∗
π(X)

1−Y )

−

DKh(θ∗

1−Y )
π(X)2 −
1 −Y )
h (θ∗
1−π(X)

(cid:20)
(1−D)K (2)

(1−D)Kh(θ∗

1−Y )

(1−π(X))2 −

E

E

(cid:20)

(cid:20)

Equivalent Form

h (y

D−π0(X)
π0(X) E

K (2)
h
2 D−π0(X)
0(X) E
π2

Kh (y

Y )

−

Y )

−

|

|

(cid:2)
π0(X)−D
1−π0(X) E

K (2)
−
h
2 π0(X)−D
(1−π0(X))2 E

h (y

Kh (y

Y )

Y )

|

|

−

−

(cid:2)

X, D = 1

i(cid:21)

X, D = 1

(cid:21)
(cid:3)
X, D = 0

i(cid:21)

X, D = 0

(cid:21)
(cid:3)

5 Conclusion

This paper studies the estimation and inference of the mode treatment eﬀect, which has been
ignored in the treatment eﬀect literature compared to the estimation of the average and the

9

quantile treatment eﬀect estimation. I propose both kernel and ML estimators to accom-
modate a variety of data sets faced by researchers. I also derive the asymptotic properties
of the proposed estimators. I show that both estimators are consistent and asymptotically
normal with the rate of convergence √Nh3.

References

Abadie, A., Angrist, J., & Imbens, G. (2002). Instrumental variables estimates of the eﬀect
of subsidized training on the quantiles of trainee earnings. Econometrica, 70 (1), 91–117.

Athey, S., Tibshirani, J., Wager, S., et al. (2019). Generalized random forests. The Annals

of Statistics, 47 (2), 1148–1178.

Belloni, A., Chen, D., Chernozhukov, V., & Hansen, C. (2012). Sparse models and methods
for optimal instruments with an application to eminent domain. Econometrica, 80 (6),
2369–2429.

Belloni, A., Chernozhukov, V., Fernández-Val, I., & Hansen, C. (2017). Program evaluation

and causal inference with high-dimensional data. Econometrica, 85 (1), 233–298.

Belloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment eﬀects after
. The Review of Economic Studies, 81 (2),
selection among high-dimensional controls
†
608-650.

Bitler, M. P., Gelbach, J. B., & Hoynes, H. W. (2006). What mean impacts miss: Dis-
tributional eﬀects of welfare reform experiments. American Economic Review , 96 (4),
988–1012.

Card, D. (1996). The eﬀect of unions on the structure of wages: A longitudinal analysis.

Econometrica: Journal of the Econometric Society, 957–979.

Chen, Y.-C., Genovese, C. R., Tibshirani, R. J., Wasserman, L., et al. (2016). Nonparametric

modal regression. The Annals of Statistics, 44 (2), 489–514.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W., &
Robins, J. (2018). Double/debiased machine learning for treatment and structural pa-
rameters. The Econometrics Journal , 21 (1), C1–C68.

Chernozhukov, V., Escanciano, J. C., Ichimura, H., & Newey, W. K. (2016). Locally robust

semiparametric estimation. arXiv preprint arXiv:1608.00033 .

Chernozhukov, V., & Hansen, C. (2005). An iv model of quantile treatment eﬀects. Econo-

metrica, 73 (1), 245–261.

Chernozhukov, V., Hansen, C., & Spindler, M.

(2015). Valid post-selection and post-
regularization inference: An elementary, general approach. Annu. Rev. Econ., 7 (1), 649–
688.

10

DiNardo, J., Fortin, N. M., & Lemieux, T.

(1995). Labor market institutions and the
distribution of wages, 1973-1992: A semiparametric approach (Tech. Rep.). National
bureau of economic research.

Eddy, W. F., et al. (1980). Optimum kernel estimators of the mode. The Annals of Statistics,

8 (4), 870–882.

Firpo, S. (2007). Eﬃcient semiparametric estimation of quantile treatment eﬀects. Econo-

metrica, 75 (1), 259–276.

Freeman, R. B. (1980). Unionism and the dispersion of wages. ILR Review , 34 (1), 3–23.

Hahn, J. (1998). On the role of the propensity score in eﬃcient semiparametric estimation

of average treatment eﬀects. Econometrica, 315–331.

Hansen, B. E. (2008). Uniform convergence rates for kernel estimation with dependent data.

Econometric Theory, 24 (3), 726–748.

Heckman, J., Ichimura, H., & Todd, P. E. (1997). Matching as an econometric evaluation
estimator: Evidence from evaluating a job training programme. The review of economic
studies, 64 (4), 605–654.

Heckman, J., & Robb, R. (1985). Alternative methods for evaluating the impact of inter-

ventions: An overview. Journal of econometrics, 30 (1-2), 239–267.

Hirano, K., Imbens, G. W., & Ridder, G. (2003). Eﬃcient estimation of average treatment

eﬀects using the estimated propensity score. Econometrica, 71 (4), 1161–1189.

Lee, M.-J. (1989). Mode regression. Journal of Econometrics, 42 (3), 337–349.

Newey, W. K., & McFadden, D. (1994). Large sample estimation and hypothesis testing.

Handbook of econometrics, 4 , 2111–2245.

Parzen, E. (1962). On estimation of a probability density function and mode. The annals

of mathematical statistics, 33 (3), 1065–1076.

Robins, J. M., & Rotnitzky, A. (1995). Semiparametric eﬃciency in multivariate regression
models with missing data. Journal of the American Statistical Association, 90 (429), 122–
129.

Rosenbaum, P. R., & Rubin, D. B.

(1983). The central role of the propensity score in

observational studies for causal eﬀects. Biometrika, 70 (1), 41–55.

Tauchen, G.

(1985). Diagnostic testing and evaluation of maximum likelihood models.

Journal of Econometrics, 30 (1-2), 415–443.

Van der Vaart, A. W. (2000). Asymptotic statistics (Vol. 3). Cambridge university press.

Yao, W., & Li, L. (2014). A new regression model: modal linear regression. Scandinavian

Journal of Statistics, 41 (3), 656–671.

11

6 Appendix

Proof of Theorem 1: We only present the proof of the ﬁrst claim, ˆθ1
θ∗
1, since the second
claim follows from the same arguments. The proof proceeds in two steps. In Step 1, we show
the uniform law of large number holds

p
→

ˆfY1 (y)

sup
y

|

In Step 2, we establish the consistency ˆθ1
Van der Vaart (2000).

−

p
→

fY1 (y)

|

= op (1) .

θ∗
1 using the same argument of Theorem 5.7 in

Step 1. Notice that we have the decomposition

ˆfY1 (y)

−

fY1 (y) =

=

1
n

1
n

n

i=1
X
n

ˆfY |D=1,X

y

(cid:0)
ˆfY |D=1,X

i=1 (cid:16)
X

|

y

Xi

(cid:1)
Xi

|

(cid:0)

(cid:1)
A(y)

E

fY |D=1,X

y

−

h

fY |D=1,X

−

X

|

(cid:1)i

Xi

(cid:1)(cid:17)

(cid:0)
y

|

(cid:0)

+

1
|
n

n

i=1
X

fY |D=1,X

y

Xi

{z

−

E

|

(cid:0)

(cid:1)
B(y)

fY |D=1,X
h

y

|

(cid:0)

Hence,

|
ˆfY1 (y)

sup
y

|

fY1 (y)

sup
y

|

|≤

−

{z
A (y)

+ sup
y

|

|

B (y)

|

}

X

.

(cid:1)i

}

By Theorem 6 in Hansen (2008) (uniform rates of convergence of kernel estimators), the ﬁrst
term supy |

is bounded by

A (y)

|

A (y)

sup
y

(cid:12)
(cid:12)

(cid:12)
(cid:12)

sup
y

≤

1
n

n

ˆfY |D=1,X

≤

≤

sup
x,y

sup
x

sup
y

i=1 (cid:12)
X
(cid:12)
ˆfY |D=1,X
(cid:12)
(cid:12)
(cid:12)
ˆfY |D=1,X
(cid:12)
(cid:12)
(cid:12)
(cid:12)
 r
= op (1) .

(cid:0)
ln n
nhd+1 + h2

= Op

y

|

(cid:1)

!

Xi

y

|

−

fY |D=1,X

y

Xi

|

(cid:0)
y

|

x

−

(cid:1)
fY |D=1,X

y

(cid:0)
x

−

(cid:1)
fY |D=1,X

(cid:1)(cid:12)
(cid:12)
(cid:12)

|

(cid:0)
x
(cid:1)(cid:12)
(cid:12)
(cid:12)

y

|

(cid:0)

(cid:0)
x
(cid:1)(cid:12)
(cid:12)
(cid:12)

On the other hand, by Lemma 1 of Tauchen (1985) (uniform law of large numbers), we have

1
n

n

i=1
X

B (y)

sup
y∈Y

(cid:12)
(cid:12)

(cid:12)
(cid:12)

= sup
y∈Y (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

fY |D=1,X

y

Xi

|

E

−

(cid:0)

(cid:1)

12

fY |D=1,X
h

X

y

|

(cid:0)

p
→

0.

(cid:12)
(cid:12)
(cid:1)i
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Combining the results of supy |

A (y)

and supy |

B (y)

gives

|

|
ˆfY1 (y)

sup
y

|

fY1 (y)

|

−

= op (1) .

Step 2. The deﬁnition of ˆθ1 implies that ˆfY1

ˆθ1

ˆfY1 (θ∗

1). Therefore, we have

fY1 (θ∗
1)

fY1

−

ˆθ1
(cid:16)

(cid:17)

≥

(cid:16)
(cid:17)
1) + ˆfY1 (θ∗
ˆfY1 (θ∗
1)
ˆθ1
1) + ˆfY1

ˆfY1 (θ∗

fY1 (y)

−

(cid:17)

(cid:16)
.
|

= fY1 (θ∗
1)

−

fY1 (θ∗
1)
−
ˆfY1 (y)

2 sup
y

|

≤

≤

−

−

fY1

ˆθ1

(cid:16)
fY1

(cid:17)
ˆθ1

(cid:16)

(cid:17)

By Step 1, we have that for any δ > 0,

P

fY1 (θ∗
1)

(cid:18)

−

fY1

ˆθ1

> δ

(cid:16)

(cid:17)

≤

(cid:19)

P

sup
y

|

ˆfY1 (y)

fY1 (y)

> δ/2

|

−

0.

! →

Further, Assumption 1 implies that for any ε > 0, there exists δ > 0 such that

sup
y:|y−θ∗

1 |>ε

fY1 (y) < fY1 (θ∗
1)

δ.

−

Then the following inequality holds

ˆθ1 −

θ∗
1 |

> ε

P

|

(cid:16)

(cid:17)

P

P

≤

≤

(cid:18)

(cid:18)

Thus, we prove the consistency ˆθ1

p
→

θ∗
1.

fY1

ˆθ1
(cid:16)
(cid:17)
fY1 (θ∗
1)

< fY1 (θ∗
1)

−

fY1

ˆθ1
(cid:16)

(cid:17)

δ

−

> δ

(cid:19)

(cid:19)

0.

→

Proof of Theorem 2: Here we focus on the result for ˆθ1 only. Notice that the ﬁrst-order

condition for ˆθ1 gives

0 = ˆf (1)
Y1

ˆθ1
(cid:16)
n

=

1
n

i=1
X

=

1
n

n

ˆf (1)
Y |X,D=1

(cid:17)
ˆf (1)
Y |X,D=1

i=1
X
θ∗
1 |

Xi

+

(cid:0)

(cid:1)

Xi

(cid:17)
ˆf (2)
Y |X,D=1

ˆθ1 |

n

i=1
X

(cid:16)
1
n

˜θ1 |

Xi

(cid:16)

ˆθ1 −

(cid:17) (cid:16)

θ∗
1

,

(cid:17)

where ˜θ1 ∈

√nh3

(cid:16)

ˆθ1, θ∗
1

. Then we have

(cid:16)
ˆθ1 −

(cid:17)

θ∗
1

=

(cid:17)

1
n

n

i=1
X

− 



ˆf (2)
Y |X,D=1

Xi

˜θ1 |
(cid:16)

13

−1

(cid:17)









√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

Xi

(cid:0)

.





(cid:1)

 
The proof proceeds in six steps. In Step 1, we show that the ﬁrst term of r.h.s converges to
M1 = E
in probability. In Step 2-5, we show the asymptotic normality
of the second term. Then, by Slutsky’s theorem, we can show the asymptotic normality for
(cid:1)i
ˆθ1. In Step 6, we show the asymptotic normality for ˆ∆.

f (2)
Y |X,D=1
h

θ∗
1 |

X

(cid:0)

For convenience, we deﬁne γ10 (x)

f (1)
Y,X|D=1 (θ∗

1, x), γ20 (x)

fX|D=1 (x), and

≡

n

j=1
X
n

ˆγ1 (x)

1
n

≡

ˆγ2 (x)

1
n

≡

≡
DjK (1)
h

DjKh

(cid:0)
x
P (D = 1)
(cid:0)

−

Xj

(cid:1)

(cid:1)

.

θ∗
Yj
Kh
1 −
P (D = 1)

Xj

x

−

(cid:0)

(cid:1)

j=1
X
In these notations, we can express ˆf (1)
x
γ10 (x) /γ20 (x), respectively. Also, let γ0 = (γ10, γ20)′ and ˆγ = (ˆγ1, ˆγ2)′.
(cid:1)
E

Step 1. In this step, we show that n−1

and f (1)

θ∗
1 |

θ∗
1 |

Y |X,D=1

Y |X,D=1

(cid:1)
ˆf (2)
Y |X,D=1

(cid:0)
Xi

n
i=1

x

(cid:0)

Notice that

where

and

1
n

n

i=1
X

ˆf (2)
Y |X,D=1

P
1
n

i=1
X

n

f (2)
Y |X,D=1

˜θ1 |

Xi

=

(cid:17)

(cid:16)

n

A1 =

1
n

A2 =

1
n

ˆf (2)
Y |X,D=1

Xi

˜θ1 |
(cid:16)

−

(cid:17)

f (2)
Y |X,D=1

˜θ1 |

Xi

(cid:16)

(cid:17)

i=1
X

n

f (2)
Y |X,D=1

˜θ1 |

Xi

f (2)
Y |X,D=1

θ∗
1 |

Xi

.

as ˆγ1 (x) /ˆγ2 (x) and

f (2)
Y |X,D=1
h

(cid:0)

X

θ∗
1 |

.
(cid:1)i

˜θ1 |

(cid:16)

p
→

(cid:17)

θ∗
1 |

(cid:0)

Xi

+ A1 + A2

(cid:1)

n

i=1 f (2)

Since 1
n
only have to show that A1 = op (1) and A2 = op (1). Note that
(cid:1)

θ∗
1 |

Y |X,D=1

(cid:1)i

X

E

(cid:0)

(cid:0)

p
→

P

i=1
X
Xi

−

(cid:17)
θ∗
1 |

(cid:16)
f (2)
Y |X,D=1
h

(cid:0)

(cid:1)

by the law of large numbers, we

A1| ≤

|

1
n

n

ˆf (2)
Y |X,D=1

≤

sup
y,x

i=1 (cid:12)
X
(cid:12)
(cid:12)
ˆf (2)
(cid:12)
Y |X,D=1
(cid:12)
(cid:12)
(cid:12)
 r
= op (1) ,

= Op

(cid:0)
ln n
nhd+5 + h2

(cid:1)

!

f (2)
Y |X,D=1

˜θ1 |

˜θ1 |

Xi

(cid:16)
y

|

x

−

−

(cid:17)
f (2)
Y |X,D=1

Xi

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

y

|

(cid:0)

(cid:16)
x
(cid:1)(cid:12)
(cid:12)
(cid:12)

where the ﬁrst equality follows from the uniform rates of convergence of kernel estimators
(Hansen, 2008). For A2, we use the argument in Lemma 4.3 of Newey & McFadden (1994).
By consistency of ˆθ1, and thus ˜θ1, there is δn →
δn with probability
approaching to one. Deﬁne

0 such that

˜θ1 −

θ∗
1

≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

14

∆n (Xi) = sup
y−θ∗
≤δn(cid:13)
1k
(cid:13)
(cid:13)
Xi
y
∆n (Xi)

Y |X,D=1

k

|

By the continuity of f (2)
gence theorem, we have E

(cid:0)

(cid:1)
→

f (2)
Y |X,D=1

Xi

y

|

−

f (2)
Y |X,D=1

θ∗
1 |

Xi

.

(cid:0)
1, ∆n (Xi)

at θ∗

(cid:1)

p
→

(cid:1)(cid:13)
(cid:13)
(cid:13)
0. Hence, by the dominated conver-

(cid:0)

0. Then, by Markov’s inequality,

(cid:2)
1
n

P





n

(cid:3)

∆n (Xi) > ǫ



≤

E

∆n (Xi)

/ǫ

0.

→

(cid:2)

(cid:3)



i=1
X

Therefore, we have

A2| ≤
Step 2. In this step, we show

|

1
n

n

i=1
X

∆n (Xi) + op (1) = op (1) .

√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

Xi

=

(cid:0)

√nh3
n

n

i=1
X

f (1)
Y |X,D=1

θ∗
1 |

Xi

+

(cid:0)

(cid:1)

√nh3
n

n

i=1
X

G (Zi, ˆγ

−

γ0)+op (1) ,

where G (z, γ) = γ20 (x)−1
do this, it suﬃces to show

γ (x) and z = (y, x, d) denotes data observation. To

γ10(x)
γ20(x)

−

i

(cid:1)

1,

h

√nh3
n

n

ˆf (1)
Y |X,D=1

i=1 h
X

(cid:1)
Using the notation of γ, we have

(cid:0)

θ∗
1 |

Xi

−

f (1)
Y |X,D=1

θ∗
1 |

Xi

G (Zi, ˆγ

−

−

(cid:1)

γ0)

= op (1) .

i

(cid:0)

ˆf (1)
Y |X,D=1

θ∗
1 |

x

−

f (1)
Y |X,D=1

θ∗
1 |

x

=

ˆγ1 (x)
ˆγ2 (x) −

γ10 (x)
γ20 (x)

.

−

(cid:0)
The following argument follows from Newey & McFadden (1994). Consider the algebra re-
lation ˜a/˜b

a/b = b−1

˜b−1

a

1

(cid:1)

(cid:1)

(cid:0)

˜b
(cid:16)

−

a/b

b
˜a
(cid:17)(cid:21) (cid:20)

b
(cid:17)(cid:21)
, and the remaining term is of higher order. By letting

. The linear part of the

−

−

−

(cid:0)

˜b
(cid:1) (cid:16)

a

˜a

r.h.s is b−1

b
(cid:17)(cid:21)
γ0). The remaining higher-order term will satisfy

a = γ10, ˜a = ˆγ1, b = γ20, and ˜b = ˆγ2, this linear term corresponds to the linear functional
G (Zi, ˆγ

−

−

−

(cid:20)

(cid:0)

(cid:20)
a/b

−
˜b
(cid:1) (cid:16)

−

γ1 (x)
γ2 (x) −

γ10 (x)
γ20 (x) −

|

G (z, γ

γ2 (x)

|

≤|

−1 γ20 (x)−1

1 +

C sup
x∈X

≤

γ (x)

−

(cid:13)
(cid:13)

(cid:20)
γ0 (x)

2

(cid:13)
(cid:13)

γ0)

−
|
γ10 (x)
γ20 (x)

γ1 (x)

−

(cid:21) h(cid:0)

γ10 (x)

2 +

γ2 (x)

−

(cid:1)

(cid:0)

γ20 (x)

2

i

(cid:1)

15

for some constant C if γ2 and γ20 are bounded away from zero. Hence Lemma 1 holds if
√nh3 supx∈X
0. By the uniform rates of convergence of kernel estimators
(Hansen, 2008), we have

2 p
→

γ0 (x)

ˆγ (x)

−

(cid:13)
(cid:13)
ˆγ (x)

sup
x∈X

γ0 (x)

−

(cid:13)
(cid:13)
2 = sup
x∈X

ˆγ1 (x)

γ10 (x)

2 +

ˆγ1 (x)

−

−

γ10 (x)

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

sup
x∈X

≤

(cid:16)(cid:0)
ˆγ1 (x)

γ10 (x)

−

(cid:1)
(cid:0)
2 + sup
x∈X

ˆγ2 (x)

−

(cid:17)
γ20 (x)

(cid:1)

2

(cid:0)
(ln n)

= Op

nhd+3

(cid:1)
−1

+ h4

(cid:0)
+ Op

(ln n)

−1

(cid:1)
nhd

+ h4

(cid:16)

nhd+3

(cid:17)

−1

(ln n)

(cid:21)

(cid:20)

+ h4

.

(cid:16)

(cid:17)

(cid:21)

= Op

(cid:20)

(cid:20)

(cid:16)

(cid:17)
ˆγ (x)

(cid:21)

γ0 (x)

−

2 p
→

0.

The rates of h and n imply that √nh3 supx∈X

Step 3. In this step, we show

√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

Xi

=

√nh3
n

(cid:0)

(cid:1)

+ op (1) ,

(cid:13)
(cid:13)

f (1)
Y |X,D=1

n

i=1
X

θ∗
1 |

Xi

(cid:0)

(cid:1)

(cid:13)
(cid:13)
+ √nh3

Z

G (z, ˆγ

−

γ0) dF0 (z)

where F0 is the c.d.f. of z. To do this, it suﬃces to show that

√nh3

1
n




n

i=1
X

G (Zi, ˆγ

γ0)

−

−

Z

G (z, ˆγ

−

γ0) dF0 (z)




= op (1) .

Let ¯γ

≡

E [ˆγ] and by the linearity of G (z, γ), we have the decomposition





G (z, ˆγ

−

γ0) = G (z, ˆγ

−

¯γ) + G (z, ¯γ

γ0) .

−

Therefore we just need to show that

√nh3

1
n




n

i=1
X

G (Zi, ˆγ

¯γ)

−

−

Z

G (z, ˆγ

−

¯γ) dF0 (z)

and

√nh3


1
n




n

i=1
X

G (Zi, ¯γ

γ0)

−

−

Z

G (z, ¯γ

−

γ0) dF0 (z)

The second condition holds by the central limit theorem since



= op (1)






= op (1) .






√nh3

1
n

n

i=1
X






G (Zi, ¯γ

γ0)

−

−

Z

G (z, ¯γ

−

γ0) dF0 (z)

= √nh3Op

n−1/2

= op (1) .

(cid:16)

(cid:17)






16

It remains to show the ﬁrst condition. We follow the arguments in Newey & McFadden
(1994). Deﬁne qj ≡

, we can rewrite

, Dj
P (D=1)

h (θ∗
P (D=1)

Dj K (1)

1 −Yj)

′

(cid:18)

(cid:19)

ˆγ (x) =

ˆγ1 (x)
ˆγ2 (x)#
"

=

1
n

n

j=1
X

qjKh

x

Xj

.

−

(cid:0)

(cid:1)

We also deﬁne

m

Zi, Zj

= G

(cid:0)

(cid:1)

m1 (z) =

m2 (z) =

Z

Z

Xj

Zi, qjKh
h
m (z, ˜z) dF0 (˜z) = G (z, ¯γ)

· −
(cid:0)

(cid:1)i

m (˜z, z) dF0 (˜z) =

G

˜z, qKh (

X)

dF0 (˜z) .

· −

Z

(cid:2)

Then the l.h.s. of the ﬁrst condition equals

(cid:3)

Z

G (z, ¯γ) dF0 (z)

√nh3

1
n




n

i=1
X

G (Zi, ˆγ

¯γ)

−

−

Z

G (z, ˆγ

−

¯γ) dF0 (z)





= √nh3

= √nh3

= √nh3









×

1
n

1
n2

i=1
X
n

Op

E

(

n

G (z, ˆγ)

1
n

−

n

i=1
X

G (z, ¯γ)

−

Z



G (z, ˆγ) dF0 (z) +

n

m

Zi, Zj

1
n

−

n

i=1
X

(cid:1)

m1 (Zi)

1
n

−

n

i=1
X

i=1
X

j=1
X

(cid:0)

m (Z1, Z1)

/n +

E

m (Z1, Z2)

2

m2 (Zi) + E

m1 (z)

(cid:2)

,

1/2

/n

)

i(cid:19)






(cid:3)






h(cid:12)
(cid:12)
where the last equality follows from Lemma 8.4 of Newey & McFadden (1994). The last term
converges to zero in probability if we can control the convergence rates of E

m (Z1, Z1)

i
(cid:12)
(cid:12)

h(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:18)

and E

m (Z1, Z2)

2

. Notice that we have

G (z, γ)

b (z)

γ
k

k2 with

≤

h(cid:12)
(cid:12)

i
(cid:12)
(cid:12)

h(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

b (z) =

(cid:12)
(cid:12)
f (1)
Y |X,D=1 (θ∗

1, x)

(cid:12)
(cid:12)
fX|D=1 (x)−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1,

h

−

where

k·k2 denotes the ℓ2 norm. Then E

G

z, qKh (

x)

· −

boundedness of K (u). By that fX|D=1 (x) is bounded away from zero and fY |X,D=1 (θ∗

(cid:0)

b (z) h−d

q

k

k2 by the
1, x) is

2
i(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:21)
(cid:1)(cid:12)
(cid:12)
(cid:12)

≤

(cid:20)(cid:12)
(cid:12)
(cid:12)

17

bounded from above, we have that E

b (z)2
h

i

≤ ∞

. Therefore, we have

m (Z1, Z1)

/n +

E

m (Z1, Z2)

1/2

2

/n

)

(cid:18)

/n +

h(cid:12)
(cid:12)
E

(cid:18)

(cid:3)

q

k

k
h

i(cid:19)

(cid:12)
(cid:12)

2

2 b (Z1)2

1/2

−1

nhd

i(cid:19)

(cid:16)

(cid:17)

)

√nh3

Op

E

(

×

= √nh3

= √nh3

= op (1)

Op

Op

×

×

i
(cid:12)
(cid:12)
k2 b (Z1)

q

h(cid:12)
(cid:12)
E

(

k
(cid:2)

n−1h−d−2

(cid:16)

(cid:17)

by the assumptions on n and h. The additional h−2 in the rates of convergence follows from
that q contains K (1)

with bounded K (1) (u).

h (u) = h−2K (1)

u/h

Step 4. In this step, we show that

(cid:0)
√nh3
n

(cid:1)
n

i=1
X

√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

Xi

=

f (1)
Y |X,D=1

θ∗
1 |

Xi

+

where v (Xi) = P (D=1)
P (D=1|Xi)

suﬃces to show that

γ10(Xi)
γ20(Xi)

−

i

and qi =

(cid:0)

(cid:1)

1,

h

√nh3
n

n

i=1
X

v (Xi) qi + op (1) ,

, Di
P (D=1)

′

(cid:19)

. To do this, it

DiK (1)

1 −Yi)

(cid:1)
h (θ∗
P (D=1)

(cid:0)

(cid:18)

Notice that

Z

√nh3

G (z, ˆγ

Z

γ0) dF0 (z)

−

√nh3
n

−

n

i=1
X

v (Xi) qi = op (1)

G (z, γ) dF0 (z) =

γ20 (x)−1

1,

γ (x) fX (x) dx

γ10 (x)
γ20 (x)

−

(cid:21)
γ10 (x)
γ20 (x)

−

fX|D=1 (x)−1

1,

(cid:20)
P (D = 1)
D = 1

X = x

P

(cid:20)

|

v (x) γ (x) dx,

(cid:0)

(cid:1)

=

=

=

Z

Z

Z

Z

γ (x) fX (x) dx

(cid:21)
γ10 (x)
γ20 (x)

1,

(cid:20)

−

γ (x) dx

(cid:21)

where fX (x) is the density function of X and v (x) = P (D=1)

P (D=1|X=x)

1,

γ10(x)
γ20(x)

. Also, we have

i

−

h
γ10 (x)
γ20 (x)#

v (x) γ0 (x) =

=

P

P

P (D = 1)
D = 1

X = x

|
P (D = 1)
(cid:0)
D = 1

(cid:1)
X = x

|

γ10 (x)
γ20 (x)

−

1,

(cid:20)

(cid:21) "

γ10 (x)

γ10 (x)

−

= 0.

(cid:0)

(cid:0)

(cid:1)

18

(cid:1)

Therefore, we have

G (z, ˆγ

−

Z

γ0) dF0 (z) =

v (x) ˆγ (x) dx

−

Z

v (x) γ0 (x) dx =

v (x) ˆγ (x) dx

Z

n

i=1 Z
X
n

i=1
X
n

Z
1
n

1
n

1
n

=

=

=

v (x) qiKh (x

v (Xi) qi +



1
n

v (Xi) qi +


1
n

i=1 Z
X
n

Xi) dx

−

n

v (x) qiKh (x

Xi) dx

−

1
n

−

n

i=1
X

v (Xi) qi


i=1 (cid:20)Z
X
By Chebyshev’s inequality, suﬃcient conditions for √nh3 times the second term in the last
line converging to zero in probability are that

i=1
X

(cid:21)

v (x) Kh (x

Xi) dx

v (Xi)

qi.

−

−

√nh3E

"(cid:18)Z

v (x) Kh (x

Xi) dx

−

−

v (Xi)

qi

0

# →

and

E

qik

"k

2

v (x) Kh (x

Xi) dx

−

−

(cid:13)
(cid:13)
(cid:13)
The expectation in the ﬁrst condition is the diﬀerence of E
(cid:13)

Z

E

v (Xi) qi

. We begin with the second term E

v (Xi) qi

(cid:2)

(cid:3)
v (Xi) qi

E

= E

(cid:2)

(cid:3)

v (Xi) E
h

qi |
(cid:2)

(cid:2)

(cid:3)

Xi

(cid:19)

2

v (Xi)

0.

# →

(cid:13)
(cid:13)
(cid:13)
v (x) Kh (x
(cid:13)
. Notice that
h(cid:0)R

−

Xi) dx

qi

and

i

(cid:1)

1−Yi)

(cid:3)i
DiK (1)
h (θ∗
P (D=1)
Di
P (D=1)





= E 

v (Xi) E 











= E 

v (Xi)

P

Xi

D = 1
P (D = 1)
(cid:0)

|

E

(cid:1)








|

Xi






K (1)
h (θ∗
1 −
1

Yi)

Xi, D = 1





! |






by the law of iterated expectations. The inner conditional expectation in the last line satisﬁes

19

 
E

h (θ∗

K (1)
h

1 −

Yi)

|

Xi, D = 1

=

i

=

=

=

=

+

+

Z

Z

Z

Yi

θ∗
1 −
h

1
h2 E
1
h2
1
h

Z

K (1)

"

K (1)

(cid:18)

θ∗
1 −
h
y

(cid:18)
θ∗
1 −
h

(cid:19)

Y |X,D=1

K

(cid:18)
Z
K (u) f (1)

Xi, D = 1

#

|

(cid:19)

y

fY |X,D=1

y

(cid:19)
f (1)
Y |X,D=1

y

(cid:0)

|

Xi

(cid:1)
dy

Xi

dy

|

θ∗
1 + hu

(cid:0)
|

Xi

(cid:1)
du

(cid:0)

θ∗
1 |

(cid:1)

Xi

du

du

(cid:1)
Xi

θ∗
1 |
˜θ1 |
(cid:16)
h2
κ2f (3)
2

(cid:1)
Xi

du

(cid:17)

Y |X,D=1

K (u) f (1)

Y |X,D=1

huK (u) f (1)

(cid:0)
Y |X,D=1

h2u2
2

K (u) f (3)

(cid:0)
Y |X,D=1

Z
= f (1)

Y |X,D=1

θ∗
1 |

Xi

+

(cid:0)

(cid:1)

˜θ1 |

Xi

(cid:16)

(cid:17)

with ˜θ1 ∈
by parts and the forth from change of variables. Hence,

1 + hu) and κ2 =

1, θ∗

(θ∗

u2K (u) du. The third equality follows from integration

R

E

v (Xi) qi

= E

(cid:2)

(cid:3)

+

h2
2

P

v (Xi)





κ2E 

v (Xi)

Xi

D = 1
|
P (D = 1)  
(cid:0)

(cid:1)

f (1)
Y |X,D=1
1
(cid:0)

θ∗
1 |

Xi

P

Xi

D = 1
|
P (D = 1) 
(cid:0)

(cid:1)

f (3)
Y |X,D=1

˜θ1 |
0
(cid:16)



v (Xi)

P

= E



Xi

D = 1
|
P (D = 1)  
(cid:0)

(cid:1)


f (1)
Y |X,D=1
1
(cid:0)

θ∗
1 |

Xi

!
(cid:1)

Xi

!
(cid:1)

Xi = x





(cid:17)





+ O

h2


v (x)

P

v (x)

v (x)

D = 1

Xi = x

|
P (D = 1)

(cid:0)
f (1)
Y |X,D=1

(cid:1)

Xi = x

f (1)
Y |X,D=1

θ∗
1 |
1

(cid:0)

θ∗
1 |
1

(cid:0)
θ∗
1 |
fX|D=1 (x)

(cid:0)

f (1)
Y,X|D=1

Xi = x

fX|D=1 (x) dx + O

h2

(cid:0)

(cid:1)

dx + O

h2

(cid:0)

(cid:1)

!

(cid:1)

!

(cid:1)

=

=

=

=

Z

Z

Z

Z

v (x) γ0 (x) dx + O

h2

.

(cid:0)

(cid:1)

20

(cid:0)

(cid:1)

fX (x) dx + O

h2

(cid:0)

(cid:1)

!

(cid:1)

 
 
 
Using the same arguments, we can also show that

E

"(cid:18)Z

v (x) Kh (x

−

Xi) dx

qi

(cid:19)

= E

#

"(cid:18)Z

v (Xi + hu) K (u) du

qi

(cid:19)

#

Then the ﬁrst condition equals

=

v (x + hu) K (u) du

γ0 (x) dx + O

Z (cid:18)Z

(cid:19)

h2

(cid:0)

(cid:1)

v (x) Kh (x

Xi) dx

v (Xi)

−

−

(cid:19)

"(cid:18)Z

qi

v (x + hu) K (u) du

γ0 (x) dx

Z (cid:18)Z

(cid:19)

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
Z

√nh3

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
= √nh3
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

v (x) γ0 (x) dx + O

h2

.

(cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Following the argument in Theorem 8.11 of Newey & McFadden (1994), the last line satisﬁes

√nh3

v (x) K (u) γ0 (x

v (x) γ0 (x) dx + O

h2

v (x)

γ0 (x

γ0 (x)

du

dx + O

h2

hu) dudx

−

Z

−

−

hu)

−

(cid:3)
γ0 (x)

(cid:27)

du

dx + O

γ0 (x

hu)

−

−

(cid:2)
dx + O

√nh3h2

(cid:3)

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
√nh3h2
(cid:13)

(cid:0)

(cid:16)

Z Z

(cid:13)
(cid:13)
(cid:13)
= √nh3
(cid:13)

Z

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Z

√nh3

(cid:13)
√nh3Ch2
(cid:13)

≤

≤

Z
√nh3h2

(cid:13)
.
(cid:13)

= O

(cid:26)Z

(cid:2)

v (x)

(cid:13)
Z
(cid:13)
(cid:13)
(cid:13)
(cid:13)
v (x)
(cid:13)

(cid:13)
(cid:13)

Therefore the ﬁrst condition holds if √nh3h2

(cid:16)

(cid:17)

0.

→

Recall that the second condition we would like to show is

qik
By Cauchy Schwartz inequality, it suﬃces to show that

v (x) Kh (x

Xi) dx

"k

−

E

Z

−

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

0.

# →

v (Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

4

By the continuity of v (x), v (x + hu)
convergence theorem,
v (x) for all x. Therefore we have

v (x) Kh (x

−

E

v (x) Kh (x

−

Z

Xi) dx

v (Xi)

0.

v (x) for all x and u as h

−

# →

(cid:13)
(cid:13)
(cid:13)
(cid:13)
→
v (x + hu) K (u) du

→
xi) dx =

"(cid:13)
(cid:13)
(cid:13)
(cid:13)

R

0. By the dominated
v (x) K (u) du =

→

E

Z

"(cid:13)
(cid:13)
(cid:13)
(cid:13)

v (x) Kh (x

Xi) dx

−

−

v (Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

4

#

R

= E

Z

"(cid:13)
(cid:13)
(cid:13)
(cid:13)

21

v (Xi + hu) K (u) du

R

−

v (Xi)

0.

# →

4

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Step 5. By Step 4 and the deﬁnition of v (Xi) and qi, we have

√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

Xi

=

(cid:0)

(cid:1)

=

+

=

+

√nh3
n

√nh3
n

√nh3
n

√nh3
n

√nh3
n

n

i=1
X
n

i=1
X
n

i=1
X
n

i=1
X
n

f (1)
Y |X,D=1

f (1)
Y |X,D=1

(cid:0)

θ∗
1 |

θ∗
1 |

√nh3
n

n

i=1
X

Xi

+

(cid:1)

Xi

+ op (1)

v (Xi) qi + op (1)

(cid:0)

Di
D = 1

P

(cid:0)
f (1)
Y |X,D=1

(cid:0)

Di
D = 1

Xi

|

θ∗
1 |

Xi

|

(cid:1)
K (1)
h

h (θ∗

1 −

Yi)

−

f (1)
Y |X,D=1

θ∗
1 |

Xi

(cid:1)i

(cid:0)

Xi

#

|

(cid:1)
Xi

Di
D = 1

−

P

1
"

(cid:1)
K (1)
h (θ∗

1 −

(cid:0)
(cid:1)
Yi) + op (1) .

Since we have E

f (1)
Y |X,D=1
"

θ∗
1 |

Xi

Di
P (D=1|Xi)

−

1
(cid:20)

(cid:21)#

(cid:1)
tions, the central limit theorem holds for the ﬁrst term of r.h.s. Hence,

(cid:0)

(cid:1)
= 0 by the law of iterated expecta-

P

i=1
X

(cid:0)

√nh3
n

n

i=1
X

ˆf (1)
Y |X,D=1

θ∗
1 |

(cid:0)

=

In this step, we show that

Xi

= Op

√nh3n−1/2

+

(cid:1)

(cid:16)
+ op (1)

= Op

√h3
(cid:16)
1
√nh

n

(cid:17)

P

i=1
X

√nh3
nh2

n

P

i=1
X

(cid:0)

Di
D = 1

Xi

|

K (1)

Di
D = 1

(cid:0)
K (1)

(cid:18)

Xi

|
θ∗
1 −
h

(cid:1)
θ∗
1 −
h

(cid:18)

K (1)

Yi
(cid:1)

(cid:19)

+ op (1) .

(cid:17)

+

1
√nh

Di
D = 1

(cid:0)

n

P

i=1
X

Xi

|

(cid:1)

Yi

θ∗
1 −
h

(cid:19)

+ op (1)

(cid:18)

Yi

(cid:19)

n

1
√nh

P

i=1
X
fY |X,D=1(θ∗
P (D=1|X)

(cid:0)
1 |X)

(cid:20)

(cid:21)

Di
D = 1

Xi

|

K (1)

Yi

θ∗
1 −
h

d
→

(cid:19)

(cid:18)

and κ(1)

(cid:1)
0 =

K (1) (u)2 du.

N (0, V ) ,

where V = κ(1)

0 E

For convenience, we deﬁne ˆg (θ∗
1)

−1

R
nh2

n
i=1

Di
P (D=1|Xi)

K (1)

θ∗
1 −Yi
h

. Then it is

≡

equivalent to show that

P

d
→

N (0, V ) .

(cid:16)

(cid:17)

√nh3

(cid:0)
ˆg (θ∗
1)

−

(cid:1)
0

(cid:1)

(cid:0)

22

To use central limit theorem, we have to calculate E

ˆg (θ∗
1)

and V ar

ˆg (θ∗
1)

.

E

ˆg (θ∗
1)

=

(cid:2)

(cid:3)

1
h2 E

P

"

Di
D = 1

=

1
h2 E



P

(cid:0)

1
D = 1

Xi

(cid:1)

Xi

|

|

K (1)

(cid:2)

(cid:18)

θ∗
1 −
h

DiK (1)

E

"

(cid:18)

(cid:19)#
θ∗
1 −
h

Yi

(cid:3)
Yi

(cid:0)

(cid:1)

Xi

|

(cid:19)

#


=

1
h2 E







(cid:0)

E

K (1)
"

(cid:18)

(cid:1)
θ∗
Yi
1 −
h

|

(cid:19)

Xi, D = 1

.

#

2 κ2f (3)

+ h2

Since h−2E

K (1)
(cid:20)

θ∗
1 −Yi
h

|

the calculation in Step 4, then

(cid:17)

(cid:16)

Xi, D = 1

= f (1)

Y |X,D=1

(cid:21)

θ∗
1 |

Xi

(cid:0)

(cid:1)

Y |X,D=1

˜θ1 |
(cid:16)

Xi

from

(cid:17)

E

ˆg (θ∗
1)

= E

For the variance,

(cid:2)

(cid:3)

f (1)
Y |X,D=1
h

θ∗
1 |

Xi

(cid:0)

+ O

h2

= 0 + O

h2

.

(cid:1)i

(cid:0)

(cid:1)

(cid:0)

(cid:1)

V ar

ˆg (θ∗
1)

=

(cid:0)

(cid:1)

1
nh4 V ar

Di
D = 1

P

K (1)

Xi

|

(cid:0)

Di
D = 1

P

(cid:0)

Di
D = 1

P

"

(cid:0)

Di
D = 1

P

(cid:1)
K (1)

Xi

(cid:1)

Xi

K (1)

(cid:1)

K (1)

Xi

|

|

|

Yi

θ∗
1 −
h

(cid:18)
θ∗
1 −
h

(cid:18)

Yi

Yi

θ∗
1 −
h

Yi

θ∗
1 −
h

(cid:18)

(cid:18)

(cid:19)!
2

(cid:19)!




2

(cid:19)#

2

+



(cid:19)!


Yi

2

(cid:0)

1



P

D = 1



(cid:0)



P

1
D = 1

Xi

Xi

|

|

(cid:1)
2 E

D2

i K (1)

"

θ∗
1 −
h

(cid:18)

(cid:1)

E

K (1)

"

(cid:18)

θ∗
1 −
h

2

Yi

|

(cid:19)





E





=

1
nh4 E

1
nh4 


1
nh4 E

−

=

=

1
nh4 E

=

1
nh4 E

The inner expectation in the last line equals



(cid:0)

(cid:1)

23

1
nh4 O

h4

(cid:0)

X

+

(cid:19)

|

#

X, D = 1

(cid:1)
1
nh4 O

h4

(cid:0)

(cid:1)

+

1
nh4 O

h4

.

(cid:0)

(cid:1)

#


 
 
 
E

K (1)

"

(cid:18)

θ∗
1 −
h

2

Yi

|

(cid:19)

X, D = 1

=

K (1)

y

θ∗
1 −
h

2

(cid:19)

(cid:18)

K (1) (u)2 fY |X,D=1

#

Z

= h

fY |X,D=1

y

X

dy

|

(cid:1)
X

(cid:0)
θ∗
1 + hu

|
(cid:0)
K (1) (u)2 du

du

(cid:1)

Z

(cid:1)
X

(cid:17) Z

uK (1) (u)2 du,

Z
= hfY |X,D=1

+ h2f (1)

(cid:0)
Y |X,D=1

X

θ∗
1 |
˜θ1 |
(cid:16)

where ˜θ1 ∈
equals

(θ∗

1, θ∗

1 + hu). Deﬁne κ(1)

0 =

K (1) (u)2 du and κ(1)

1 =

uK (1) (u)2 du, the variance

V ar

ˆg (θ∗
1)

=

(cid:0)

(cid:1)

1
nh4 E

h

P

"

D = 1

+

1
nh4 E

P

"

h2
(cid:0)
D = 1

R

Xi

Xi

|

|

1

nh3 

(cid:0)
κ(1)
0 E

P

"

(cid:1)
1
D = 1

=

=

1
nh3



(cid:16)

V + O (h) + O

(cid:0)

h3

(cid:1)

.

(cid:0)

(cid:1)(cid:17)

R

#

(cid:1)

κ(1)
0 fY |X,D=1

θ∗
1 |

X

(cid:1)
1 f (1)
κ(1)

Y |X,D=1

(cid:0)
θ∗
1 |

(cid:0)

fY |X,D=1

Xi

|

θ∗
1 |

(cid:0)

X

+

#

(cid:1)

X

#

(cid:1)

1
nh4 O

h4

(cid:0)

(cid:1)

+ O (h) + O

h3

(cid:0)





(cid:1)

Then we are ready to apply the central limit theorem.

Let

Zn,i ≡

(nh)−1/2



P

Di
D = 1

then E

Zn,i



(cid:0)
= 0 and V ar

Xi

|

(cid:1)

Zn,i

K (1)

Yi

θ∗
1 −
h

(cid:18)

E

P

"

−

(cid:19)

Di
D = 1

K (1)

Yi

θ∗
1 −
h

(cid:18)

Xi

|

= h3V ar

ˆg (θ∗
1)

(cid:0)

= n−1V + o

(cid:1)
. Then

n−1

,

(cid:19)#


(cid:2)

(cid:3)
√nh3

ˆg (θ∗
1)

−

(cid:0)

(cid:0)
(cid:1)
= √nh3

0

(cid:1)

= √nh3

(cid:0)
ˆg (θ∗
1)
−

ˆg (θ∗
1)

(cid:1)
ˆg (θ∗
1)

E

(cid:0)
+ √nh3

(cid:1)
E

ˆg (θ∗
1)

E

(cid:2)

ˆg (θ∗
1)

(cid:3)(cid:17)

(cid:16)
+ √nh3O

(cid:2)
h2

0

−

(cid:17)

(cid:3)

d
→
by Liapunov CLT and √nh3h2

(cid:3)(cid:17)

(cid:2)

h2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:16)

(cid:16)

−

n

=

Zn,i + √nh3O

i=1
X
N (0, V )

0.

→

24

Step 6. In this step, we show that

√nh3

ˆθ1 −
ˆθ0 −
"

θ∗
1
θ∗
0#

d
→

N

0
0#
"



,

M1V1M1
0

"

and thus, by the delta method, we have



0

M0V0M0#


√nh3

ˆ∆

∆∗

−

d
→

N (0, M1V1M1 + M0V0M0) .

(cid:16)

(cid:17)

To show the joint distribution we adopt vector notations. The ﬁrst-order conditions of

ˆθ1 and ˆθ give

0
0#
"

= 

ˆfY1
ˆfY0




where

ˆθ1
ˆθ0
(cid:16)
(cid:16)



(cid:17)

(cid:17)




=

1
n

n

i=1
X

ˆf (1)
Y |X,D=1
ˆf (1)
Y |X,D=0



ˆθ1 |
ˆθ0 |

Xi

Xi

(cid:16)

(cid:16)



(cid:17)

(cid:17)




=

1
n

ˆf (1)
Y |X,D=1
ˆf (1)
Y |X,D=0

n

i=1 "
X

θ∗
1 |
θ∗
0 |

Xi
Xi

(cid:0)
(cid:0)

#
(cid:1)
(cid:1)

+ Jn

ˆθ1 −
ˆθ0 −
"

θ∗
1
θ∗
0#




n



i=1
X

n

i=1
X









Jn =

=

1
n

1
n

Hence we have

∂ ˆf (1)

Y |X,D=1(˜θ1|Xi)
∂θ1
Y |X,D=0(˜θ0|Xi)
∂θ1

∂ ˆf (1)

∂ ˆf (1)

Y |X,D=1(˜θ1|Xi)
∂θ0
Y |X,D=0(˜θ0|Xi)
∂θ0

∂ ˆf (1)






ˆf (2)
Y |X,D=1

˜θ1 |

Xi

(cid:16)
0

(cid:17)

ˆf (2)
Y |X,D=0

0

(cid:16)

˜θ0 |

Xi

.



(cid:17)




√nh3

ˆθ1 −
ˆθ0 −
"

θ∗
1
θ∗
0#

=

J −1
n

(cid:0)

(cid:1)

√nh3
n

n

i=1
X

n

ˆf (1)
Y |X,D=1
ˆf (1)
Y |X,D=0





Di
P (D=1|Xi)
1−Di
P (D=0|Xi)

(cid:16)

ˆθ1 |
ˆθ0 |
(cid:16)
K (1)

Xi

Xi



(cid:17)


(cid:17)

θ∗
1−Yi
h

K (1)

(cid:16)

θ∗
0−Yi
h

1
√nh

=

J −1
n

(cid:0)

(cid:1)

i=1
X






+



(cid:17)

op (1)
op (1)#
"

(cid:16)

(cid:17)




where the last equality follows from the Step 5 in the proof of Theorem 1. Since

and

Jn

p
→ "

M1
0
0 M0#

1
√nh

n

i=1
X






Di
P (D=1|Xi)
1−Di
P (D=0|Xi)

K (1)

θ∗
1 −Yi
h

K (1)

(cid:16)

θ∗
0 −Yi
h

(cid:16)

25

d
→

N

0
0#
"



,



,

"

V1
0
0 V0#




(cid:17)

(cid:17)




then by Slutsky’s theorem we have

√nh3

ˆθ1 −
ˆθ0 −
"

θ∗
1
θ∗
0#

d
→

N

0
0#
"



,

M1V1M1
0

"



.

0

M0V0M0#


Proof of Theorem 3. It is enough to show the results of ˆM1 and ˆV1. We ﬁrst show that
p
→

M1. By adding and subtracting additional terms, we have

ˆM1

ˆM1 =

1
n

n

i=1
X

ˆf (2)
Y |X,D=1

ˆθ1 |
(cid:16)

Xi

=

(cid:17)

1
n

n

i=1
X

f (2)
Y |X,D=1

θ∗
1 |

(cid:0)

Xi

+ A1 + A2

(cid:1)

where

and

A1 =

1
n

A2 =

1
n

ˆf (2)
Y |X,D=1

Xi

ˆθ1 |
(cid:16)

−

(cid:17)

f (2)
Y |X,D=1

ˆθ1 |

Xi

(cid:16)

(cid:17)

n

i=1
X

n

f (2)
Y |X,D=1

ˆθ1 |

Xi

−

f (2)
Y |X,D=1

θ∗
1 |

Xi

.

i=1
X
If we can show that A1 = op (1) and A2 = op (1), then ˆM
Note that

(cid:16)

(cid:17)

(cid:1)

M by law of large numbers.

(cid:0)

p
→

A1| ≤

|

1
n

n

ˆf (2)
Y |X,D=1

≤

sup
y,x

i=1 (cid:12)
X
(cid:12)
(cid:12)
ˆf (2)
(cid:12)
Y |X,D=1
(cid:12)
(cid:12)
(cid:12)
 r
= op (1) ,

= Op

(cid:0)
ln n
nhd+5 + h2

(cid:1)

!

f (2)
Y |X,D=1

ˆθ1 |

ˆθ1 |

Xi

(cid:16)
y

|

x

−

−

(cid:17)
f (2)
Y |X,D=1

Xi

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

y

|

(cid:0)

(cid:16)
x
(cid:1)(cid:12)
(cid:12)
(cid:12)

where the ﬁrst equality follows from the uniform rates of convergence of kernel estimators
(Hansen, 2008). For A2, we use the argument in Lemma 4.3 of Newey & McFadden (1994).
By consistency of ˆθ1 there is δn →
δn with probability approaching to
one. Deﬁne ∆n (Zi) = sup
k
ity of f (2)
at θ∗
E
∆n (Zi)
Therefore, we have

0. By the dominated convergence theorem, we have
(cid:1)
(cid:0)
n
0.
i=1 ∆n (Zi) > ǫ

0 such that
f (2)
y−θ∗
Y |X,D=1
1k
p
(cid:13)
1, ∆n (Zi)
(cid:13)
→
(cid:13)

y
0. By Markov inequality, P

ˆθ1 −
(cid:13)
Xi
y
(cid:13)
|
(cid:13)
(cid:0)
n−1

≤
f (2)
Y |X,D=1

. By the continu-

∆n (Zi)

(cid:1)(cid:13)
(cid:13)
(cid:13)
E

θ∗
1 |

Y |X,D=1

(cid:13)
(cid:13)
−
(cid:13)

(cid:0)
→

Xi

Xi

θ∗
1

→

/ǫ

≤δn

≤

(cid:1)

|

(cid:1)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

A2| ≤

|

1
n

n

(cid:0)
P
∆n (Zi) = op (1) .

i=1
X

26

V1. We can rewrite

Next we show that ˆV1

ˆV1/κ(1)

0 =

1
n

n

i=1
X

p
→
ˆfY |X,D=1

ˆθ1 |
(cid:16)
ˆπ (Xi)

Xi

=

(cid:17)

1
n

n

i=1
X

fY |X,D=1

θ∗
1 |
π (Xi)
(cid:0)

Xi

(cid:1)

+ B1 + B2

with

and

B1 =

B2 =

1
n

1
n

ˆfY |X,D=1

ˆθ1 |
(cid:16)
ˆπ (Xi)

Xi

(cid:17)

−

fY |X,D=1

ˆθ1 |
(cid:16)
π (Xi)

Xi

(cid:17)

fY |X,D=1

ˆθ1 |
(cid:16)
π (Xi)

Xi

(cid:17)

−

fY |X,D=1

θ∗
1 |
π (Xi)
(cid:0)

Xi

.

(cid:1)

n

i=1
X

n

i=1
X

It remains to show that B1 = op (1) and B2 = op (1). The result of B2 follows from the same
arguments as in the proof of A2 if fY |X,D=1
y
Xi
1. Thus, we only focus
|
on B1. For conenience, deﬁne f
= fY |X,D=1
. For π bounded away from zero,
(cid:1)
we have

is continuous at θ∗
y

x

x

y

(cid:0)

|

|

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆf

|

x
y
ˆπ (x) −
(cid:0)

(cid:1)

f

x
y
|
π (x)
(cid:0)

(cid:1)

=

=

=

π (x) ˆf

y

π (x) ˆf

(cid:0)

y

x

(cid:1)
x

ˆπ (x) f

−
|
ˆπ (x) π (x)

π (x) f

−

|

y

x

|

(cid:0)
y

(cid:1)
x

ˆf

y

|

(cid:0)

(cid:0)
x

f
−
ˆπ (x)
(cid:1)

(cid:1)

x

y

|

+

(cid:0)

(cid:1)

(cid:1)
x

(cid:0)
f
y
|
ˆπ (x) π (x)
(cid:1)

(cid:0)

+ π (x) f

|
ˆπ (x) π (x)

y

x

|

−

ˆπ (x) f

y

x

|

(cid:0)

(cid:1)

(cid:1)

π (x)

(cid:0)

−

ˆπ (x)

C

≤

ˆf

y

x

|

−

f

y

|

(cid:18)(cid:16)

x
(cid:1)(cid:17)

(cid:0)
ˆπ (x)

+

(cid:1)

π (x)

−

(cid:19)

(cid:0)
for some C > 0. By the uniform rates of convergence of kernel estimators (Hansen, 2008),
we have

(cid:0)

(cid:0)

(cid:1)

(cid:1)

n

ˆfY |X,D=1

ˆθ1 |
(cid:16)
ˆπ (Xi)

Xi

fY |X,D=1

ˆθ1 |
(cid:16)
π (Xi)

(cid:17)

−

Xi

1
n

B1| ≤

|

≤

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆf
C sup
(cid:12)
y,x (cid:18)(cid:12)
(cid:0)
(cid:1)
(cid:12)
ln n
(cid:12)
nhd+1 + h2

x

y

|

−

 r

= Op

= op (1)

ˆπ (x)

π (x)

−

+

x
(cid:1)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆπ (x)

f

y

|

(cid:0)
+ sup
x

!

(cid:12)
(cid:12)

π (x)

−

(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)
(cid:12)
(cid:12)

by the rates of n and h and the uniform convergence of ˆπ (x).

27

Proof of Theorem 4: Suppose that

ˆfY1(y) =

1
K

K

Xk=1

En,k[m1(Z, y, ˆη1k)]

is diﬀerentiable with respect to y. Deﬁne

ˆf (1)
Y1 (y)

1
K

≡

K

Xk=1

En,k[m(1)

1 (Z, y, ˆη1k)]

where m(1)

1 (Z, y, ˆη1k)

By the deﬁnition of ˆθ1, we have

≡

∂m1(Z, y, ˆη1k)/∂y.

0 = ˆf (1)

Y1 (ˆθ1) =

1
K

K

En,k[m(1)

1 (Z, ˆθ1, ˆη1k)]

Xk=1
En,k[m(1)
1 (Z, θ∗

1, ˆη1k)] +

=

1
K

K

Xk=1

1
K

K

Xk=1

En,k[m(2)

1 (Z, ˜θ1, ˆη1k)](ˆθ1 −

θ∗
1)

and

√Nh3(ˆθ1 −

θ∗
1) =

− 

1
K

K

Xk=1

En,k[m(2)

1 (Z, ˜θ1, ˆη1k)]



−1









√Nh3
K

K

Xk=1

En,k[m(1)

1 (Z, θ∗

1, ˆη1k)]

.





In Step 1 and 2 below, we will show that

1
K

En,k[m(2)

1 (Z, ˜θ1, ˆη1k)]

p
→

M1

K

Xk=1

K

En,k[m(1)

1 (Z, θ∗

1, ˆη1k)] d
→

N(0, V1),

Xk=1

and

√Nh3
K

respectively. Hence, we can obtain the ﬁnal result

Step 1. Since K is a ﬁxed integer, which is independent of N, it suﬃces to show that for

√Nh3(ˆθ1 −

1) d
θ∗
→

N(0, M −1

1 V1M −1

1 ).

each k

[K],

∈

1 (Z, ˜θ1, ˆη1k)]
Then we can show this convergence using the same argument in Step 1 in the proof of
Theorem 2.

En,k[m(2)

M1.

p
→

Step 2. Since K is a ﬁxed integer, which is independent of N, it is enough to consider

the convergence of En,k[m(1)

1 (Z, θ∗

1, ˆη1k)]. Notice that

28

En,k[m(1)

1 (Z, θ∗

1, ˆη1k)] =

m(1)

1 (Z, θ∗

1, η10) + R2k

1
n

Xi∈Ik

where

R2,k = En,k[m(1)

1 (Z, θ∗

1, ˆη1k)]

1
n

−

i∈Ik
X

m(1)

1 (Z, θ∗

1, η10).

Then by triangular inequality,

where

I1,k + I2,k
√n

,

≤

R2,k

(cid:13)
(cid:13)

(cid:13)
(cid:13)

I1,k ≡

Gn,k

1 (Z, θ∗

1, ˆη1k)

Gn,k

1 (Z, θ∗

1, η10)

,

m(1)
h
m(1)
1 (Z, θ∗

1, ˆη1k)

−

i
(Wi)i∈I c

m(1)
h

EP

I2,k ≡

√n

−
h
Two auxiliary results will be used to bound I1,k and I2,k:

i

|

k

i(cid:13)
(cid:13)
(cid:13)
1, η10)
(cid:13)

1 (Z, θ∗

m(1)
h

.

i(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
EP
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

k
h

sup
η1∈TN (cid:18)
sup
r∈(0,1),η1∈TN k

m(1)
1

(Z, θ∗

1, η1)

−

m(1)
1

(Z, θ∗

1, η10)

1/2

2

k

i(cid:19)

∂2
r E

m(1)
1
h

(cid:0)

Z, θ∗

1, η10 + r (η1 −

η10)

k≤

(cid:1)i

εN ,

≤

(εN )2 ,

(A.1)

(A.2)

k
π

where
such that

TN is the set of all η1 = (π0, g10) consisting of square-integrable functions π0 and g10
η10 kP,2≤
η1 −
1/2
1/2
kP,∞≤
−
−
g1 −
π0 kP,2 × k
π
Then by assumption, we have ˆη1k ∈ TN with probability 1
the event that ˆη1k ∈ TN , we have
=EP

To bound I1,k, note that conditional on (Wi)i∈I c

κ,
g10 kP,2≤
o (1).
−

the estimator ˆη1k is nonstochastic. Under

k
2
P,2 +

(εN )2 .

π0 k

m(1)

m(1)

εN ,

EP

1 (Z, θ∗

1 (Z, θ∗

1, ˆη1k)

1, η10)

(Wi)i∈I c

(Wi)i∈I c

−

−

π

k

k

2

k

I 2
1,k |

h

k

k
h
sup
η1∈TN

i

≤

= sup
η1∈TN
= (εN )2

EP

EP

k
h

k
h

−
1 (Z, θ∗
1, η1)

m(1)

m(1)

1 (Z, θ∗

k
|
1, η10)

m(1)

1 (Z, θ∗

1, η1)

m(1)

1 (Z, θ∗

1, η10)

−

−

k

i
(Wi)i∈I c

k

i

2

k

|

2

k

i

by (A.1). Hence, I1,k = OP (εN ). To bound I2,k, deﬁne the following function

fk (r) = EP

1 (Z, θ∗

1, η10 + r (ˆη1k −

m(1)
h

η10))

(Wi)i∈I c

k

|

E

−

i

1 (Z, θ∗

1, η10)

m(1)
h

i

29

for r

∈

[0, 1). By Taylor series expansion, we have

fk (1) = fk (0) + f ′

k (0) + f ′′

k (˜r) /2, for some ˜r

(0, 1) .

Note that fk (0) = E
|
calculation in Step 4 in the proof of Theorem 2. Further, on the event ˆη1k ∈ TN ,
∂η1E[m(1)

(Wi)i∈I c

1, η10)

1, η10)

1 (Z, θ∗

f ′
k (0)

= E

η10]

= 0

=

i

i

1 (Z, θ∗

m(1)
h

m(1)
h

k

1, η10)] [ˆη1k −

k

k

k

k

∈
1 (Z, θ∗

= O(h2) by the

by the orthogonality. Also, on the event ˆη1k ∈ TN ,
f ′′
k (r)

f ′′
k (˜r)

k

sup
r∈(0,1) k

k≤

(εN )2

k≤

by (A.2). Thus,

k
Together with the result on I1,k, we have

k

I2,k = √n

fk (1)

= OP

√n (εN )2 + √nh2
(cid:16)

(cid:17)

.

R2,k

I1,k + I2,k
√n

≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

=OP

n−1/2εN + (εN )2 + h2
(cid:16)

(cid:17)

√Nh3

R2,k

= OP (√h3ǫN + √Nh3ǫ2

N + √Nh3h2) = oP (1)

Hence,

by the assumptions on the rate of convergence that ǫN = o((Nh3)−1/4) and Nh7
Therefore,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0.

→

En,k[m(1)

1 (Z, θ∗

1, ˆη1k)] =

1
n

i∈Ik
X

m(1)

1 (Z, θ∗

1, η10) + oP (1) d
→

N(0, V1).

30

