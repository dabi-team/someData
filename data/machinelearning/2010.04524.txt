Multi-objective Optimisation of Multi-output Neural
Trees

Varun Ojha
University of Reading
Reading, United Kingdom
0000-0002-9256-1192

Giuseppe Nicosia
University of Cambridge
Cambridge, United Kingdom
0000-0002-0650-3157

2
2
0
2

b
e
F
8
1

]
E
N
.
s
c
[

2
v
4
2
5
4
0
.
0
1
0
2
:
v
i
X
r
a

Abstract—We propose an algorithm and a new method to
tackle the classiﬁcation problems. We propose a multi-output
neural tree (MONT) algorithm1, which is an evolutionary learning
algorithm trained by the non-dominated sorting genetic algo-
rithm (NSGA)-III. Since evolutionary learning is stochastic, a
hypothesis found in the form of MONT is unique for each run
of evolutionary learning, i.e., each hypothesis (tree) generated
bears distinct properties compared to any other hypothesis
both in topological space and parameter-space. This leads to
a challenging optimisation problem where the aim is to minimise
the tree-size and maximise the classiﬁcation accuracy. Therefore,
the Pareto-optimality concerns were met by hypervolume indi-
cator analysis. We used nine benchmark classiﬁcation learning
problems to evaluate the performance of the MONT. As a result
of our experiments, we obtained MONTs which are able to tackle
the classiﬁcation problems with high accuracy. The performance
of MONT emerged better over a set of problems tackled
in this study compared with a set of well-known classiﬁers:
multilayer perceptron, reduced-error pruning tree, na¨ıve Bayes
classiﬁer, decision tree, and support vector machine. Moreover,
the performances of three versions of MONT’s training using
genetic programming, NSGA-II, and NSGA-III suggests that the
NSGA-III gives the best Pareto-optimal solution.

Index Terms—Neural tree, Multi-class classiﬁcation, multi-
objective optimisation, non-dominated sorting genetic algorithm;
NSGA-III

I. INTRODUCTION

Learning from data is essentially a search process by
which we search for a hypothesis (a trained model) from
a hypothesis space that maps (ﬁts) the given input data to
its target output as good as possible (the high accuracy on
test data). A learning algorithm like multilayer perceptron’s
architecture and parameter tuning are the efforts to ﬁnd a
hypothesis that ﬁts the data well [1], [2]. Similarly, there is
a variety of hypothesis selection possible. Such examples are
decision tree [3], reduced error pruning tree [4], na¨ıve Bayes
classiﬁer [5], and support vector machine [6].

In this study, our effort is to take advantages of the evolu-
tionary processes for designing a new method for searching
a hypothesis (an evolutionary learning algorithm) that ﬁts
well on a variety of datasets. Therefore, we propose a multi-
output neural tree (MONT) algorithm, which resembles a tree
data structure whose nodes are neural nodes similar to the
nodes of a multilayer perceptron. The tree exploits the genetic

1Source code: https://github.com/vojha-code/Multi-Output-Neural-Tree

programming (GP) [7] and non-dominated sorting genetic
algorithm (NSGA) frameworks II [8] and III [9] to evolve
from data, at independent instances.

The proposed algorithm MONT is an innovation from
the early tree-based learning algorithms such as a ﬂexible
neural tree where a tree-like-structure was optimised by using
probabilistic incremental program evolution [10] and het-
erogeneous ﬂexible neural tree (HFNT) [11] where a tree-
like-structure was optimised by NSGA-II. Similar to these
two approaches, in [12], a fuzzy inference system enabled
hierarchical tree-based predictors was illustrated. In [13], a
tree-based algorithm was evaluated on beta-basis function as
a neural node. Among these algorithms, the proposed MONT
algorithm closely linked to HFNT. Hence, a comparison of
MONT with HFNT is presented in this research.

These early versions of the tree-based algorithms are limited
to binary class classiﬁcation since the root node reports the
output. Hence,
these algorithms worked in a multi-input-
single-output fashion. For the multi-class classiﬁcation, these
algorithms need to be repeated for each class separately, which
results in as many as trees as the number of classes.

Our proposed algorithm eliminates this limitation by using
a single tree to learn for multiple classes. In MONT algorithm,
each child of the tree’s root-node is formulated as the class
output. Hence, the proposed MONT works in multi-input-
multi-output fashion and treats binary classiﬁcation as two-
class classiﬁcation. The results show that the competitive na-
ture of the evolutionary process improve performance. More-
over, our proposed method applies NSGA-III combined with
hypervolume inductor analysis [14] to obtain the best neural
trees serving Pareto-optimality for an evolutionary learning
process. The contributions of this study are as follows:

• A new algorithm called MONT is designed for classiﬁ-
cation tasks, speciﬁcally aims at adapting multi-class.
• A new method is proposed for neural trees generations.
• A Pareto-optimality of evolutionary learning processes
was investigated using hypervolume indicator analysis.
• A comprehensive analysis of the MONT’s (trained with
NSGA-III) performance compared with other algorithms
and with MONT’s other two training version GP, NSGA-
II is presented.

The rest of the paper is organised as follows: Section II
describes the multi-class classiﬁcation problem and the basic

 
 
 
 
 
 
architecture, principles, and properties of MONT algorithm.
Section II-C describes the evolutionary learning processes and
framework for constructing optimal neural trees. Section II-E
describes the experimental setup designed for the induction
of a varied range of hypothesis, including MONT and well-
known ﬁve algorithms over a diverse range of datasets. The
results of the experiments are summarised in Section III and
discussed in Section IV, followed by conclusions in Section V.

II. MULTI-OBJECTIVE-MULTI-OUTPUT NEURAL TREES

A. Problem Statement

Let X ∈ Rd be an instance-space and let Y = {c1, . . . , cr}
be a set of r labels such that label y ∈ Y can be assigned
to an instance x ∈ X . Hence, for a training set of instance-
label pairs S = (xi, yi)N
i=1, we face a multi-class learning
problem. And, a hypothesis h from a set of hypothesis class
H is induced that aims at reducing a cost function f (·). A
typical cost function is the classiﬁcation error-rate:

f =

1
N

N
(cid:88)

(h(xi) (cid:54)= yi)

i=1

(1)

where h(xi) is the predicted output for an input instance xi =
(cid:11) and yi ∈ {c1, . . . , cr} being its target class.
(cid:10)xi

2, . . . , xi
d

1, xi

B. Multi-Output Neural Trees

Multi-output neural tree (MONT) takes a tree-like structure
where the root node takes as many as child nodes as the
number of classes it is to be induced on. Each child node of
the tree’s root predicts a class, and each of them is a subtree
of the MONT classiﬁer.

Mathematically, MONT, G, is an m–ary rooted tree with
one node designated as the root node and each node takes at
least m ≥ 2 child nodes except for the leaf node that has no
child node. Hence, for a tree depth p, MONT takes a maximum
of (2p+1 −1) ≤ n ≤ (mp+1 −1)/(m−1) nodes (including the
number of internal nodes K = |V | and the leaf node L = |T |).
A MONT, G, is denoted as:

G = V ∪ T =

(cid:110)

1, vj
vj

2, . . . , vj

K

(cid:111)

∪ {t1, t2, . . . , tL}

(2)

where k-th node vj
k ∈ V is an internal node and receives 2 ≤
j ≤ m inputs from its child nodes. The k-th leaf node tk ∈ T
has no child and it contains an input xi ∈ {x1, x2, . . . , xd}.
An example of a MONT is shown in Fig. 1.

1, v2

2, and v3

0 takes three child nodes v2

Fig. 1 is an example of three class problem, where the root
node v3
3 representing
three classes c1, c2, and c3. Each child node of the root is a full
m-ary subtree. For example, node v2
1 takes two child nodes
and the node v3
1 takes three child nodes. The number of child
nodes and the size of subtrees is governed by an evolutionary
learning process. The leaf nodes of the MONT are the input
nodes that takes an input feature x ∈ x.

The internal nodes of the MONT are neural nodes. Each
internal node has an activation function (e.g., Gaussian, sig-
moid, tangent hyperbolic) and behave similarly to a node
in multilayer perceptron. Fig. 2 is an example of the i-th

Fig. 1. Representation of a multi-output neural tree for a three-class problem
with classes c1, c2, and c3. The immediate child nodes v1, v3, and v4 of the
root node v0 are the output class nodes. The other internal nodes or leaf nodes
construct subtrees for each root child (the respective output class). This tree
vj
takes its input from the set {x1, x2, . . . , x5}. The link w
between nodes
i
are neural weights.

Illustration of a computational (neural) node. The variable dvi
Fig. 2.
indicates the number of inputs zi
j received at the i-th node
vi, the variable bi is the bias at the i-th node and the variable zk is the output
of the i-th node squashed by an activation function ϕ(yi).

j and weights wi

MONT’s neural node that receives the inputs from its child
nodes (higher tree-depth) and produces an output for its parent
node (e.g., k-th node in lower tree-depth).

A MONT resembles an expression tree, and the computation
of MONT takes place as per depth-ﬁrst-search in pre-order
fashion (Fig. 1). Hence, the time complexity of MONT for
computing its output is O(n), where n is the number of nodes
in the tree.

C. Multi-Objective Optimisation of Neural Trees

n

In our designed evolutionary process, an evolutionary algo-
rithm like NSGA-III searches through all possible combina-
tion of MONTs, which is roughly close to Catalan number
Cn = 1/[(m − 1)(n + 1)].(cid:0)m.n
(cid:1), and for MONT, n is ≥ 7 and
only 1 tree-structure (shape) possible for n = 7 since MONT
takes at least two classes and each class takes at least two
inputs. Hence, 1 root node, 2 child nodes and 4 leaf node, total
7 nodes can only be arranged in one unique structure as per
MONT’s deﬁnition (Section II-B). However, the parameters of
the tree’s edges and the node’s function further increase the
search-space size.

Felsenstein presented a theory for all possible combina-
tions of tree-structure for an m–ary tree that has a total

x1x3x3x4x2x2v30v21v34v35x1x3x3v32c1c3rootnodeneuralnodeinputnodebv1bv4bv2bv5biasinputoutputclassv23c2x5x0bv3wv32wv31wv12wv11wv21wv23wv22wv41wv43wv42wv51wv53wv52d(i)(cid:80)j=1wijzij+bizi1zijzid(i)zi2......biyiϕzkvi1.0wi1wi2wijwid(i)L = mp labelled leaf nodes [15], and, for L labelled leaf
nodes, a total number of possible tree-structures arrange-
ments (combinations) shown for an evolutionary process is
(2L − 3)!/(2L−2(L − 2))! [15]. Therefore, an evolutionary
processes search through such a vast hypothesis search-space
to obtain an optimal tree.

In our method, we used three training versions for MONT:
NSGA-III [9], NSGA-II [8], and GP [7]. MONT takes genetic
operators such as crossover (of subtrees) and mutation deﬁned
in [11]. In [11], the following forms of mutation is deﬁned
for the tree’s mutation: 1) deletion of a randomly selected
leaf node, 2) replacement a randomly selected leaf node, 3)
replacement of a randomly selected function node by a leaf
node or a new subtree. Using the mentioned genetic operators,
the MONT follows the typical evolutionary computation steps
as per Algorithm 1 for its training [16]. For MONT’s multi-
objective training, we supplied two objectives to be minimised:
classiﬁcation error-rate f1 and tree-size f2. The error-rate f1
is expressed as per (1). The objective tree-size f2 is |G|, i.e.,
the total number of nodes in a tree G.

Algorithm 1 Evolutionary Learning of MONT
Require: Initial population P0 of randomly generated neural
trees, objectives F = [f1, f2] , data S, maximum evolu-
tionary generations (termination criteria) gmax.
Ensure: Final population Pgmax of Pareto-optimal trees

while number of generation g reached gmax do

1: function TREE EVOLUTION(P0, F, S, gmax.)
2:
3:
4:
5:
6:

selection: parent trees for crossover and mutation
generation: a new population Q
combined population: R = Pg + Q
evaluation: NSGA-II/III non-dominated sorting(R)
survive: elitism/niching (Pg+1, size(P0), R)

7:
8:
9:
10: end function

end while
return Pgmax

For MONT training,

the algorithms GP, NSGA-II, and
NSGA-III follow Algorithm 1 with some differences in a
few steps. Especially, they differ in line numbers 6 and 7 of
Algorithm 1. In GP, no non-dominated sorting is performed.
Instead, the recombined population R in line no. 5 is sorted
according to single objective f1 (line no. 6 of Algorithm 1).
Whereas, NSGA-II and NSGA-III both follows non-dominated
sorting as per [8]. However, NSGA-II and NSGA-III differ in
line no. 7 of Algorithm 1. NSGA-II performs elitism based on
crowding distance of the individuals computed based on each
objective [8]. On the other hand, NSGA-III performs niching.
The niching operation takes advantage of a predeﬁned set of
reference points placed on a normalised hyperplane of an M -
dimensional objective-space [17], where each individual in the
population is associated to a reference point [9]. Moreover, the
total number of reference points depends on the division of
each objective axis. Both, the crowding distance and niching

in NSGA-II and NSGA-III respectively aims at preserving
diversity in the population.

D. Hypervolume Analysis for Pareto-Optimality

An evolutionary process (e.g. NSGA-II or NSGA-III) for
two objectives gives a non-dominated set of solutions. A non-
dominated solution is the one for which no one objective
function can be improved without a simultaneous detriment
to at least one of the other objectives [8]. The non-dominated
solution is also known as the Pareto-optimal solution. More-
over, a set of such solution creates a Pareto-optimal front.

In this study, we choose to compute the hypervolume
indicator Hi, which measures the dominance of Paret-front
solutions on a geometric space (area for a 2D objective space)
framed by the M -dimensional objective-space with respect of
a positive semi-axle. Hence, Hi measures the quality Pareto-
optimal solutions set [14], and it is an indicator of the quality
of the solutions obtained by two algorithms with respect to the
same reference frame. We want hypervolume indicator index
Hi to be maximised. A greater value indicates that the overall
performance of the algorithm is better with respect to another
algorithm associated with a smaller hypervolume value. More-
over, the greatest contributing point in a hypervolume indicator
analysis is the point that covers the largest area and that can
be considered as the best solution [18].

E. Experiment Set-Up

We designed our experiments to evaluate the performance of
MONT algorithm on a set of different datasets each pertaining
to distinct feature-space and a varied number of classes. The
datasets used for the experiments were retrieved from the UCI
machine learning repository[19]. The details of the dataset
are described in Table I. These datasets were chosen because
of their diversity in feature-space. For example, the dataset
Australia and Heat have a mix of nominal and categorical
attributes (features), the dataset Iris and Glass have real-values
attributes, and the dataset Ionosphere has it every attribute
within the range of -1.0 to 1.0. These make a single algorithm
to perform equally well on each dataset difﬁcult [20].

The performance of the proposed MONT (NSGA-III ver-
sion) trained over these datasets was compared with the
performance of ﬁve well-known algorithms bearing a differing

TABLE I
DESCRIPTIONS OF THE DATASETS USED IN THE EXPERIMENTS.

Index
aus
hrt
ion
pma
wis
irs
win
vhl
gls

Name
Australia
Heart
Ionosphere
Pima
Wisconsin
Iris
Wine
Vehicle
Glass

Features
14
13
33
8
30
4
13
18
9

Samples
691
270
351
768
569
150
178
846
214

Classes
2
2
2
2
2
3
3
4
7

characteristic and a tree-based algorithm heterogeneous ﬂexi-
ble neural tree (HFNT) [11]. We used decision tree (DT) [3];
multilayer perception (MLP) [1]; reduced error pruning tree
(REP-T) [4]; na¨ıve Bayes classiﬁer (NBC) [5]; and support
vector machine (SVM) [6]. We chose the state-of-the-art
implementations of these algorithms from the Weka tool [21].
Each of these chosen algorithms differs in their nature.
Therefore, we expect
them to perform differently over a
diverse range of the dataset. Hence, we look for the average
performance of MONT over a set of datasets in comparisons to
the average performance of the chosen algorithms, as well as;
we look for the comparisons between three different versions
of neural tree (MONT) training processes and settings. These
versions are: single objective GP based training, MONT1,
NSGA-II enabled multi-objective optimisation, MONT2, and
NSGA-III enabled multi-objective optimisation, MONT3. Ad-
ditionally, each of these training versions MONT1, MONT2,
and MONT3 also run for three different activation nodes:
Gaussian, sigmoid, and tangent hyperbolic (tanh) functions.
Hence, six training versions of MONT were evaluated.

The three training versions MONT1, MONT2, and MONT3
respectively assume GP, NSGA-II, and NSGA-III and follows
the evolutionary process outlined in Algorithm 1. For the
comparison of solutions obtained by these three versions of
MONT training, a reference point 1.0 and 100 respectively
indicating the worst test error-rate and worst tree-size was
chosen for computing hypervolume indicator index Hi.

For the performance comparisons, we set-up a hold-out
method of validation where datasets were randomly partitioned
into 80% training and 20% test sets. For each dataset, 30 runs
of algorithm training and testing results were collected. The
parameter settings of the chosen algorithms were set to their
default settings prescribed in Weka tool. A summary of the
training parameters used for each algorithm are as follows:

• MONT: iterations - 100; population - 50; max child nodes
- 5; max tree height - 10; optimisers - GP, NSGA-II, and
NSGA-III; crossover probability 0.5; mutation probability
0.5; NSGA-III’s reference point division - 10.

• HFNT: iterations - 100; population - 50; max child nodes
- 5; max tree height - 10; optimisers - NSGA-II; crossover
probability 0.5; mutation probability 0.5.

• MLP: iterations - 500; number of hidden layer nodes -
(features + classes)/2; learning rate - 0.3; momentum rate
- 0.2; optimiser- backpropagation.

• REP-T: max tree height - unlimited; tree punning - no

pruning; max feature at a node - 2.

• NBC: distribution function - normal distribution.
• DT: attributes information quality measure - Gini index;

tree punning - no pruning.

• SVM: kernel type at the nodes - redial basis function.

III. RESULTS

A. Hypervolume Indicator Analysis and Tree Selection

Each solution in the MONT’s population has an error-rate
and a tree-size associated with it which are conﬂicting objec-
tives. For MONT’s population, the Pareto-optimal solutions

TABLE II
QUALITY OF TRADE-OFF OBTAINED BY HYPERVOLUME INDICATOR Hi
ON THREE VERSIONS OF MONT.

Data
aus
gls
hrt
ion
irs
pma
vhl
win
wis
Avg.

MONT1
84.10
44.14
78.74
85.65
85.60
73.38
49.21
86.47
90.68
75.33

MONT2
83.57
56.16
77.46
83.83
90.00
76.01
52.72
87.44
90.55
77.53

MONT3
83.57
52.35
77.46
88.97
89.97
76.01
51.99
87.28
91.34
77.66

set can be evaluated by hypervolume indicator index Hi as
discussed in Section II-D. The hypervolume indicator analysis
allow us to select the best solution from the MONT3’s pop-
ulation for each dataset. Fig. 3 shows hypervolume indicator
analysis of the dataset over two objectives: neural tree-size
against training error-rate. In Fig. 3, the blue (upside) triangle
indicates the greatest contributing point with respect to the
reference point (maximum tree-size and maximum error-rate
of the population with an offset 0.1) indicated with symbol
“*.” The Pareto-optimal solutions are indicated in red dots
and other feasible solution are in gray.

The best neural tree (MONT3) for each dataset was ob-
tained using the hypervolume indicated analysis. The greatest
contributing point was considered as the best MONT satisfying
both objectives: tree-size and error-rate. The average test error-
rates of the best trees of the 30 runs are compared with other
algorithms (Section III-B).

Fig. 4 shows the best trees obtained by using hypervolume
analysis for Iris (marked T1 and T2 in Fig. 3) and Wine
(marked Ta and Tb in Fig. 3). Fig. 5 illustrate the example
of MONT3 learning ability over the 100 generations of the
evolutionary optimisation process for datasets Iris and Wine.
In Fig. 5, the MONT3 performance is also indicated through
training and test receiver operating characteristic (ROC) curve
plot where solutions lying top-left corner indicate good per-
formance of the classiﬁer.

Moreover, since hypervolume inductor analysis is the per-
formance quantiﬁer of the multi-objective algorithm’s solu-
tions quality, we compared MONT3 solutions with MONT1
and MONT2 with a reference point bearing test error-rate 1.0
and tree-size 100. The results of the analysis are shown in
Table II.

B. Neural Tree Performance Against Other Algorithms

The collected results of 30 runs of MONT3 and 30 runs of
the mentioned algorithms HFNT, MLP, REP-T, NBC, DT, and
SVM are shown in Table III. Table III shows the average test
error and variance over the mentioned nine datasets. Since
not a single algorithm’s performance (measured as per the
average test error-rate) outperform all other algorithms, the
three lowest average test error obtained by the respective
algorithms for each dataset are marked in bold (Table III).

Fig. 4. Best performing trees (as per Fig. 3) for datasets Iris (T1 and T2 )
and Wine (Ta and Tb). The shaded nodes are function nodes. Trees T1, T2,
Ta, and Tb of respective datasets gives test error-rate 0.00, 0.013, 0.00, 0.167.

Fig. 3. Hypervolume analysis of MONT3 over datasets (Iris top) and on
aus, hrt, ion, pim, wis, win, vhl, and gls, respectively from top left to bottom
right. The reference point marked in “*”, blue (upside) triangle is the greatest
contributing point. Pareto-front solutions are indicated by the red dots on the
left bottom corner and other feasible solutions are indicated in gray dots.
Select trees are marked Ti (see iris and wine plots) and are shown in Fig 4.

Fig. 5. Performance of an evolutionary generation for the Iris-T1 (top) and
Wine-Ta (bottom) results shown in Fig. 4. The error-rate reduction is shown
by the line, the ROC plot for train (top) and test (bottom) in each plot.

We performed two-sided t-test statistics with setting the
alpha value to 0.05 to compare the average error-rate of
MONT3 algorithm against other algorithms for each dataset.
Hence, as a null hypothesis, we test “whether the average
test error-rate of the MONT3 is signiﬁcantly lower than the
average test error-rate of other algorithms?” The results of the
statistical t-test are shown in Table IV.

Table V shows the best test error of 30 runs of experiments
over the mentioned nine datasets. Similar to the performance
of algorithms shown in Table III, the three lowest test error-

0.00.10.20.30.4error rate101520253035404550tree sizeT1T2iris0.150.200.250.02.55.07.510.012.515.017.520.0 W U H H  V L ] Haustralian0.200.250.300.02.55.07.510.012.515.017.520.0heart0.100.150.200.250.02.55.07.510.012.515.017.520.0 W U H H  V L ] Hionosphere0.200.250.300.350.02.55.07.510.012.515.017.520.0pima0.050.100.150.02.55.07.510.012.515.017.520.0 W U H H  V L ] Hwisconsin0.050.100.150.200.250.300.02.55.07.510.012.515.017.520.0TbTawine0.400.450.500.550.600.65 H U U R U  U D W H0510152025303540 W U H H  V L ] Hvehicle0.350.400.450.500.550.60 H U U R U  U D W H0510152025303540glassC0C1C2T1X2X0X3X3X3X0X2X0C0C1C2TaX7X12X6X19X12X7C0C1C2X2X1X0X3X3X0X3T2C0C1C2TbX0X9X8X10X6X4X11X12X4X5X6020406080100 W U D L Q L Q J  J H Q H U D W L R Q V0.050.100.150.200.250.30 H U U R U  U D W H0.00.20.40.60.81.0     V S H F L I L F L W \   W U D L Q 0.00.51.0 V H Q V L W L Y L W \   W U D L Q SetosaVersicolourVirginica0.00.20.40.60.81.0     V S H F L I L F L W \   W H V W 0.00.51.0 V H Q V L W L Y L W \   W H V W 020406080100 W U D L Q L Q J  J H Q H U D W L R Q V0.050.100.150.200.250.300.35 H U U R U  U D W H0.00.20.40.60.81.0     V S H F L I L F L W \   W U D L Q 0.00.51.0 V H Q V L W L Y L W \   W U D L Q Class 0Class 1Class 20.00.20.40.60.81.0     V S H F L I L F L W \   W H V W 0.00.51.0 V H Q V L W L Y L W \   W H V W TABLE III
AVERAGE TEST ERROR-RATE Fµ AND VARIANCE Fσ OF 30 RUNS OF EXPERIMENTS ON MONT3 AND OTHER ALGORITHMS

Algorithm
MONT3

HFNT

MLP

REP-T

NBC

DT

SVM

f1
fµ
fσ
fµ
fσ
fµ
fσ
fµ
fσ
fµ
fσ
fµ
fσ
fµ
fσ

aus
0.111
0.002
0.174
0.006
0.175
0.001
0.150
0.001
0.231
0.001
0.146
0.001
0.455
0.002

hrt
0.191
0.000
0.230
0.004
0.213
0.004
0.247
0.004
0.176
0.003
0.312
0.012
0.461
0.004

ion
0.102
0.000
0.178
0.003
0.094
0.001
0.107
0.002
0.166
0.002
0.126
0.001
0.073
0.001

pma
0.201
0.000
0.284
0.003
0.249
0.001
0.255
0.001
0.244
0.001
0.337
0.001
0.353
0.001

wis
0.038
0.000
0.065
0.001
0.024
0.001
0.096
0.003
0.026
0.001
0.514
0.004
0.532
0.006

data

irs
0.011
0.000
0.189
0.019
0.040
0.002
0.064
0.001
0.047
0.001
0.070
0.001
0.029
0.001

win
0.048
0.000
0.176
0.014
0.037
0.000
0.071
0.000
0.070
0.001
0.370
0.002
0.369
0.002

vhl
0.450
0.003
0.591
0.005
0.183
0.001
0.291
0.001
0.544
0.001
0.463
0.002
0.754
0.000

gls
0.371
0.001
0.601
0.015
0.367
0.004
0.348
0.005
0.525
0.008
0.337
0.006
0.353
0.004

Avg.

0.169
0.021
0.276
0.039
0.154
0.013
0.181
0.012
0.225
0.035
0.297
0.024
0.376
0.046

Note: for all datasets three lowest average test error rates are marked in Bold.

TABLE IV
TWO-SIDED T-TEST: MONT3 AGAINST OTHER ALGORITHMS

Data
aus

hrt

ion

pma

wis

irs

win

vhl

gls

T-test
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value
t-stat
p-Value

HFNT MLP REP-T
-13.52
-4.59
0.00
0.00
-2.98
-4.31
0.00
0.00
0.91∗
-7.68
0.36∗
0.00
-9.04
-8.55
0.00
0.00
0.00
0.00
0.00
0.00
-3.79
-7.06
0.00
0.00
3.91†
-5.98
0.00†
0.00
39.23
-11.17
0.00
0.00
-0.18
-10.45
0.86
0.00

-7.02
0.00
-6.69
0.00
-3.14
0.00
-22.68
0.00
0.00
0.00
-8.24
0.00
-39.64
0.00
-3.01
0.00
1.85∗
0.07∗

NBC
-19.20
0.00
0.00∗
1.00∗
-7.73
0.00
-7.57
0.00
0.00
0.00
-4.92
0.00
2.93†
0.00†
-16.33
0.00
-9.51
0.00

DT
-6.42
0.00
-5.94
0.00
-0.78
0.44
-8.28
0.00
0.00
0.00
-7.25
0.00
-4.85
0.00
21.61
0.22
1.18∗
0.24∗

SVM
-44.81
0.00
-22.71
0.00
3.67
0.00
-25.65
0.00
0.00
0.00
-2.97
0.00
-33.49
0.00
-56.72
0.00
0.84∗
0.40∗

Note: For all datasets NOT marked in Bold, the MONT’s average
test error-rate was statistically signiﬁcant than listed algorithms. For
results marked ∗, other algorithm’s mean was better than MONT, but
statistically NOT signiﬁcant. Only for dataset “win” indicated with
symbol †, REP-Tree and NBC are statistically signiﬁcant.

rate by respective algorithms for each dataset are marked in
bold (Table V). Section IV presents a detailed discussion of
the performance of the algorithms.

C. Neural Tree Training Versions Performances

Apart from comparing the MONT3’s performance against
other algorithms, the performances of the MONT’s training
versions were also evaluated and compared. Fig. 6 shows the
performance of three training versions of MONT: MONT1,
MONT2, and MONT3. Fig. 6(a) is a box plot of the error-
rates collected for 30 runs of each of these training versions,
i.e. optimisation of MONT using GP, NSGA-II, and NSGA-III
respectively. Additionally, Fig. 6(b) shows the performance of
MONT for three versions of nodes used: Gaussian, sigmoid,
and tangent hyperbolic labelled 1, 2, and 3, respectively.

TABLE V
BEST TEST ERROR-RATE OF 30 RUNS OF EXPERIMENTS ON MON3
COMPARED WITH OTHER ALGORITHMS

algorithm

Data MONT3 HFNT MLP
0.123
aus
0.074
hrt
0.028
ion
0.201
pma
0.000
irs
0.000
wis
0.009
win
0.147
vhl
0.256
gls

0.101
0.093
0.042
0.182
0.000
0.018
0.000
0.388
0.302

0.072
0.111
0.070
0.195
0.000
0.009
0.000
0.447
0.372

REP-T NBC
0.087
0.181
0.074
0.148
0.085
0.056
0.182
0.195
0.000
0.000
0.000
0.028
0.026
0.027
0.218
0.482
0.209
0.395

DT
0.094
0.167
0.028
0.293
0.000
0.345
0.254
0.400
0.209

SVM
0.326
0.315
0.028
0.292
0.000
0.361
0.281
0.712
0.209

Note: the best three are marked in Bold

TABLE VI
AVERAGE TRAINING ERROR AND TREE-SIZE OBTAINED BY THREE
VERSIONS OF MONT TRAINING AND HFNT

Avg. training error-rate

Avg. tree-size

data MONT1 MONT2 MONT3 HFNT MONT1 MONT2 MONT3 HFNT
8.53
0.072
aus
8.13
9.20
0.111
hrt
8.73
7.63
0.070
ion
9.83
10.28
0.195
pma
14.00
16.13 156.73
0.000
irs
8.97
9.91
0.009
wis
26.20
13.94
0.000
win
31.23
18.13
0.447
vhl
60.20
25.83
0.372
gls
35.85
13.42
0.276
Avg.

20.03
27.20
32.16
49.28
218.51
30.49
44.37
51.36
85.53
62.37

8.43
8.59
9.6
10.61
17.07
9.2
13.78
17.74
25.87
13.43

0.15
0.26
0.18
0.26
0.08
0.08
0.16
0.53
0.54
0.25

0.15
0.25
0.18
0.26
0.07
0.09
0.15
0.53
0.5
0.24

0.15
0.26
0.16
0.27
0.18
0.09
0.16
0.54
0.52
0.26

Moreover, the average training error-rates and average tree-
size produced by MONT1, MONT2, and MONT3 representing
training by GP, NSGA-II, and NSGA-III respectively are
shown in Table VI.

(a)

(b)

Fig. 6. Performance of neural tree training versions: (a) Performance of optimisers GP, NSGA-II, and NSGA-III respectively marked 1, 2, and 3 as the
subscript of the dataset names. (b) Performance of for activation functions at neural nodes: Gaussian, sigmoid, and tanh respectively marked 1, 2, and 3 as
the subscript of dataset names. The median of error-rates is marked in red; the average error-rate is marked in a green triangle.

IV. DISCUSSION

The MONT optimisation is an evolutionary process where
a population of MONTs competes to yield the ﬁttest solution.
The computational properties of a MONT are similar to an
MLP except for that the MONT has a tree-like structure com-
pared to MLP that has fully connected network-like structure
and MONT does a simultaneous feature selection ignoring
insigniﬁcant features during its induction.

The MONT uses an evolutionary learning process for its
tree structure evolution. The optimisation of tree-structure
gives MONT the ability to ﬁnd a hypothesis from a large
hypothesis search-space as mentioned in Section II-C. More-
over, two multi-objective optimisers NSGA-II and NSGA-III
both bearing differing mechanism for maintaining diversity in
the population of an evolutionary process ensures exploration
of this large hypothesis search space. This gives MONT the
ability to induce a hypothesis from topological-space, feature-
space, and parameter-space speciﬁc to each class of each
problem.

Table III shows that the performance of the solution ob-
tained by hypervolume inductor analysis from MONT3’s popu-
lation dominates other listed algorithms, which is evident from
the MONT3 being in the top three performing algorithms for
the most selected datasets. The average test error-rate of the
MONT3 outperformed all algorithms for three datasets: Aus-
tralia, Iris, and Pima (Table III). The performance of MONT3
was closely second to other algorithms for the datasets Wine,
Wisconsin, and Vehicle. For dataset Glass, the performance
of all algorithms marginally differed from each other, and the
best performing algorithms was REP-T.

The statistical signiﬁcance test using two-sided t-test shown
in Table IV indicates that MONT3 has statistically signiﬁ-
cant performance for dataset Australia, Iris, and Pima, and
Wisconsin. For datasets, Heart, Ionosphere, Vehicle, can be
said competitive since some other algorithm’s average test
error rates were better but not statistically signiﬁcant. For the
dataset, wine, however, REP-Tree and NBC appeared to be
statistically signiﬁcant.

Similarly, the best test error-rates of the algorithms over
datasets shown in Table V indicate that MONT3 performed
well on six datasets out of nine datasets. And, MONT3
performed competitively over two datasets. This is evident
from the statistical test where the average error-rate of MONT
was found statistically signiﬁcant and equivalent to 8 datasets.
This performance of the MONT3 is in the view that MONT3
induce a hypothesis by simultaneous minimisation of the
hypostasis complexity (model’s parameter reduction) and the
error rate minimisation. Whereas, other algorithms had a single
objective (error rate) to minimise. However, one advantage
with MONT3 its nature of being population-based hypothesis
induction compared with other mentioned algorithm. Consider
this fact the MONT3 was trained with small-scale training
set with a minimum of 100 iterations and a population of 50
individuals. Hence, the performance of MONT3 may improve
for a higher number of iteration and population size.

Apart from comparing MONT3 (trained by NSGA-III)
performance against other algorithms, its performance was
compared with two other training versions: MONT2 (trained
by NSGA-II) and single objective (optimisation of error-rates)
version of MONT trained with GP. As per the obtained

aus1aus2aus3gls1gls2gls3hrt1hrt2hrt3ion1ion2ion3irs1irs2irs3pma1pma2pma3vhl1vhl2vhl3win1win2win3wis1wis2wis30.00.20.40.60.81.0Error rateaus1aus2aus3gls1gls2gls3hrt1hrt2hrt3ion1ion2ion3irs1irs2irs3pma1pma2pma3vhl1vhl2vhl3win1win2win3wis1wis2wis30.00.20.40.60.81.0Error rateresults of these three training versions shown in Fig. 6(a),
the NSGA-III based optimisation of MONT,
i.e., version
MONT3 performed well in the cases of six datasets. As well
as, the Pareto-optimality analysis shown in Table II shows
that on a hypervolume indicator analysis, the Pareto-optimal
solutions set generated by NSGA-III is competitively better
than NSGA-II and GP, which is attributed to its exploitation
of non-dominated sorting and niching operator for population
diversity maintenance. This is evident from average Hi values
77.66 obtained for NSGA-III is higher than the NSGA-II
(Hi = 77.53) and GP (Hi = 75.33). That is a solution in
the NSGA-III tends to offer a better trade-off between tree-
size and test error-rates.

The MONT3 performance was also compared for three
different types of activation functions: Gaussian, Sigmoid, and
tangent hyperbolic that can be used as activation nodes. The
performance of Gaussian function was found better in the
cases of seven datasets [Fig. 6(b)]. However, the performance
of the Sigmoid function was competitive and very close to the
Gaussian function. The Gaussian function may have advan-
tages of its ability to possess varies shapes during training,
whereas the sigmoid function the advantages of the bias at
the neural nodes. Moreover, this is evident from MONT’s
better performance compared to HFNT that uses a set of
heterogeneous nodes (a variety of activation function set)
without a bias input (Table VI). As well as, MONT3’s NSGA-
III based training results lower tree-size than HFNT.

In MONT algorithm, each class is represented by a subtree,
and each subtree competes with another subtree to maximise
its classiﬁcation accuracy (Fig. 5). This comparative nature of
subtrees of MONT results in maximisation of the MONT’s
classiﬁcation accuracy. This is evident from the differences
between the subtrees of each class obtained by the MONT
algorithm, as shown in Fig. 4 for Iris and Wine. Moreover, the
learning process of each class during an evolutionary process
shown in Fig. 5.

V. CONCLUSION

This paper proposes a new algorithm so-called multi-output
neural tree (MONT) that is trained by using non-dominated
sorting algorithm frameworks (NSGA) III. The evolutionary
process yields a population of MONTs as the solutions to
a problem. The population of MONTs was analysed using
hypervolume indicator analysis for selection of a Pareto-
optimal set. The performance of the MONT was compared
with ﬁve well-known algorithms: decision tree, multilayer per-
ceptron, reduced error-pruning tree, na¨ıve Bayes, and support
vector machine and heterogeneous ﬂexible neural tree. The
performance of MONT outperforms the mentioned algorithms
on three datasets, and for another three datasets,
it was
competitive with other algorithms. In general, results show
that the MONT performed competitively over a larger set
of chosen data compared with the other algorithms. This
performance of MONT attributed to its property where each
class is represented as a subtree that competes against the
other subtrees for improving the classiﬁcation accuracy. In an

additional set of experiments MONT’s three training version
GP, NSGA-II, and NSGA-III were compared, where NSGA-III
emerged better in terms of minimising the trade-off between
two objective tree-size and training accuracy. The use of
activation function Guassun and sigmoid were found better
choice compared to tangent-hyperbolic. Thus, simultaneously
maximising overall classiﬁcation accuracy and minimising
tree-size (reducing parameter). These properties of MONT
is crucial to achieving generalisation ability in searching a
hypothesis from hypothesis-space.

REFERENCES

[1] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-

tations by back-propagating errors,” Nature, vol. 323, Oct 1986.

[2] V. K. Ojha, A. Abraham, and V. Sn´aˇsel, “Metaheuristic design of
feedforward neural networks: A review of two decades of research,”
Engineering Applications of Artiﬁcial Intelligence, vol. 60, pp. 97–116,
2017.

[3] J. R. Quinlan, C4.5: Programs for Machine Learning. Elsevier, 2014.
[4] R. Kohavi and J. R. Quinlan, “Data mining tasks and methods: Clas-
siﬁcation: decision-tree discovery,” in Handbook of Data Mining and
Knowledge Discovery. Oxford University Press, 2002, pp. 267–276.

[5] G. H. John and P. Langley, “Estimating continuous distributions in
bayesian classiﬁers,” in Proceedings of the 11th Conference on Uncer-
tainty in Artiﬁcial Intelligence. Morgan Kaufmann Publishers, 1995,
pp. 338–345.

[6] C.-C. Chang and C.-J. Lin, “Libsvm: A library for support vector
machines,” ACM Transactions on Intelligent Systems and Technology,
vol. 2, no. 3, p. 27, 2011.

[7] M. D. Schmidt and H. Lipson, “Solving iterated functions using ge-
netic programming,” in Proceedings of the 11th Annual Conference
Companion on Genetic and Evolutionary Computation Conference: Late
Breaking Papers. ACM, 2009, pp. 2149–2154.

[8] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast elitist non-
dominated sorting genetic algorithm for multi-objective optimization:
NSGA-II,” in Parallel Problem Solving from Nature PPSN VI, ser.
Lecture Notes in Computer Science.
Springer, 2000, vol. 1917, pp.
849–858.

[9] K. Deb and H. Jain, “An evolutionary many-objective optimization
algorithm using reference-point-based nondominated sorting approach,
part i: solving problems with box constraints,” IEEE Transactions on
Evolutionary Computation, vol. 18, no. 4, pp. 577–601, 2013.

[10] Y. Chen, B. Yang, J. Dong, and A. Abraham, “Time-series forecasting
using ﬂexible neural tree model,” Information Sciences, vol. 174, no. 3,
pp. 219–235, 2005.

[11] V. K. Ojha, A. Abraham, and V. Sn´aˇsel, “Ensemble of heterogeneous
ﬂexible neural trees using multiobjective genetic programming,” Applied
Soft Computing, vol. 52, pp. 909–924, 2017.

[12] V. K. Ojha, V. Sn´aˇsel, and A. Abraham, “Multiobjective programming
for type-2 hierarchical fuzzy inference trees,” IEEE Transactions on
Fuzzy Systems, vol. 26, no. 2, pp. 915–936, 2017.

[13] S. Bouaziz, H. Dhahri, A. M. Alimi, and A. Abraham, “A hybrid learning
algorithm for evolving ﬂexible beta basis function neural tree model,”
Neurocomputing, vol. 117, pp. 107–117, 2013.

[14] C. M. Fonseca, L. Paquete, and M. L´opez-Ib´anez, “An improved
dimension-sweep algorithm for the hypervolume indicator,” in IEEE
International Conference on Evolutionary Computation.
IEEE, 2006,
pp. 1157–1163.

[15] J. Felsenstein, “The number of evolutionary trees,” Systematic Zoology,

vol. 27, no. 1, pp. 27–33, 1978.

[16] D. E. Goldberg, Genetic Algorithms in Search. Optimization and Ma-

chine Learning. Addison-Wesley, 1989.

[17] I. Das and J. E. Dennis, “Normal-boundary intersection: A new method
for generating the pareto surface in nonlinear multicriteria optimization
problems,” SIAM Journal on Optimization, vol. 8, no. 3, pp. 631–657,
1998.

[18] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V. G. Da Fon-
seca, “Performance assessment of multiobjective optimizers: An analysis
and review,” IEEE Transactions on Evolutionary Computation, vol. 7,
no. 2, pp. 117–132, 2003.

[19] K. Bache and M. Lichman, “Uci machine learning repository,” 2013.
[20] D. H. Wolpert, “The lack of a priori distinctions between learning
algorithms,” Neural Computation, vol. 8, no. 7, pp. 1341–1390, 1996.
[21] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten, “The weka data mining software: an update,” ACM SIGKDD
Explorations Newsletter, vol. 11, no. 1, pp. 10–18, 2009.

