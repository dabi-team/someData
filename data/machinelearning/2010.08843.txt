1
2
0
2

p
e
S
3

]

G
L
.
s
c
[

2
v
3
4
8
8
0
.
0
1
0
2
:
v
i
X
r
a

Approximate information state

Approximate information state for approximate planning
and reinforcement learning in partially observed systems

Jayakumar Subramanian∗
Media and Data Science Research Lab, Digital Experience Cloud,
Adobe Systems India Private Limited, Noida, Uttar Pradesh, India

jasubram@adobe.com

Amit Sinha
Raihan Seraj
Aditya Mahajan
Department of Electrical and Computer Engineering
McGill University, Montreal, QC, Canada

Editor:

amit.sinha@mail.mcgill.ca
raihan.seraj@mail.mcgill.ca
aditya.mahajan@mcgill.ca

Abstract
We propose a theoretical framework for approximate planning and learning in partially
observed systems. Our framework is based on the fundamental notion of information state.
We provide two equivalent deﬁnitions of information state—i) a function of history which
is suﬃcient to compute the expected reward and predict its next value; ii) equivalently, a
function of the history which can be recursively updated and is suﬃcient to compute the
expected reward and predict the next observation. An information state always leads to a
dynamic programming decomposition. Our key result is to show that if a function of the
history (called approximate information state (AIS)) approximately satisﬁes the properties
of the information state, then there is a corresponding approximate dynamic program. We
show that the policy computed using this is approximately optimal with bounded loss of
optimality. We show that several approximations in state, observation and action spaces
in literature can be viewed as instances of AIS. In some of these cases, we obtain tighter
bounds. A salient feature of AIS is that it can be learnt from data. We present AIS based
multi-time scale policy gradient algorithms. and detailed numerical experiments with low,
moderate and high dimensional environments.
Keywords: Partially observed reinforcement learning, partially observable Markov decision
processes, approximate dynamic programming, information state, approximate information
state.

1. Introduction

Reinforcement learning (RL) provides a conceptual framework for designing agents which
learn to act optimally in an unknown environment. RL has been successfully used in
various applications ranging from robotics, industrial automation, ﬁnance, healthcare, and
natural language processing. The success of RL is based on a solid foundation of combining
the theory of exact and approximate Markov decision processes (MDPs) with iterative
algorithms that are guaranteed to learn an exact or approximate action-value function

∗. This work was done when Jayakumar Subramanian was at McGill University.

1

 
 
 
 
 
 
Subramanian, Sinha, Seraj, and Mahajan

and/or an approximately optimal policy (Sutton and Barto, 2018; Bertsekas and Tsitsiklis,
1996). However, for the most part, the research on RL theory is focused primarily on systems
with full state observations.

In various applications including robotics, ﬁnance, and healthcare, the agent only gets
a partial observation of the state of the environment. Such partially observed systems are
mathematically modeled as partially observable Markov decision processes (POMDPs) and
there is a fairly good understanding of how to identify optimal or approximately optimal
policies for POMDPs when the system model is known to the agent.

Since the initial work on POMDPs (Astr¨om, 1965), it is known that POMDPs can be
modeled as fully observed MDPs by considering the belief state (i.e., the posterior belief of
the unobserved state given all the observations made by the agent) as an information state.
Therefore, the theory and algorithms for exact and approximate planning for MDPs are also
applicable to POMDPs. One computational challenge is that the belief state is continuous
valued. However, the value function based on the belief state has a nice property—it is
piecewise linear and a convex function of the belief state—which can be exploited to develop
eﬃcient algorithms to identify the optimal policy. Building on the one-pass algorithm of
(Smallwood and Sondik, 1973), various such algorithms have been proposed in the literature
including the linear support algorithm (Cheng, 1988), the witness algorithm (Cassandra et al.,
1994), incremental pruning (Zhang and Liu, 1996; Cassandra et al., 1997), the duality based
approach (Zhang, 2009), and others. Since POMDPs are PSPACE-complete (Papadimitriou
and Tsitsiklis, 1999), the worst case complexity of such algorithms is exponential in the size
of the unobserved state space. To overcome the worst case complexity of ﬁnding an optimal
policy, various point-based methods have been proposed in the literature which obtain an
approximate solution by sampling from the belief space (Pineau et al., 2003; Smith and
Simmons, 2004; Spaan and Vlassis, 2005; Shani et al., 2007; Kurniawati et al., 2008; Poupart
et al., 2011); see Shani et al. (2013) for an overview and comparison.

However, the exact and approximate planning results are of limited value for partially
observed reinforcement learning (PORL) because they are based on the belief state, con-
structing which requires the knowledge of the system model. So, when an agent is operating
in an unknown environment, it cannot construct a belief state based on its observations. An
attempt to circumvent this diﬃculty was to use memoryless policies (i.e., choose the action
based only on the current observation) (Littman, 1994; Loch and Singh, 1998; Jaakkola
et al., 1995; Williams and Singh, 1999; Li et al., 2011; Azizzadenesheli et al., 2016). A
related idea is to choose the action based on k recent observations (Littman, 1994; Loch
and Singh, 1998) or choose the action based on a memory which is updated using a ﬁnite
state machine (Whitehead and Lin, 1995; McCallum, 1993; Hansen, 1997; Meuleau et al.,
1999; Amato et al., 2010). Such ﬁnite memory policies are also amenable to policy search
methods (Hansen, 1998; Baxter and Bartlett, 2001; Poupart and Boutilier, 2004). However,
there are no approximation guarantees available for such methods.

Another approach taken in the literature is to use a Bayesian RL framework (Ross
et al., 2008; Poupart and Vlassis, 2008; Ross et al., 2011; Katt et al., 2019) where a
posterior distribution over the models of the environment is maintained; at each step, a
model is sampled from the posterior and the corresponding optimal policy is executed.
Appproximation error bounds in using such methods are derived in Ross et al. (2011).

2

Approximate information state

A completely diﬀerent class of model-based RL algorithms are methods using predictive
state representations (PSRs) (Littman et al., 2002; Singh et al., 2003). PSRs are constructed
only based on observational data so they can easily be adapted to the RL setup. There have
been a number of papers which use PSRs to propose model based RL algorithms (James
et al., 2004; Rosencrantz et al., 2004; Boots et al., 2011; Hamilton et al., 2014; Kulesza et al.,
2015b,a; Jiang et al., 2016).

Inspired by the recent successes of deep reinforcement learning, there are many recent
results which suggest using RNNs (Recurrent Neural Networks (Rumelhart et al., 1986))
or LSTMs (Long Short-Term Memories (Hochreiter and Schmidhuber, 1997)) for modeling
the action-value function and/or the policy function (Bakker, 2002; Wierstra et al., 2007,
2010; Hausknecht and Stone, 2015; Heess et al., 2015; Zhu et al., 2017; Ha and Schmidhuber,
2018; Baisero and Amato, 2018; Igl et al., 2018; Zhang et al., 2019).
It is shown that
these approaches perform well on empirical benchmarks, but there are no approximation
guarantees available for such methods.

Our main contribution is to present a rigorous approach for PORL which is based on a

principled theory of approximate planning for POMDPs that we develop. In particular:

1. In Sec. 2, we formalize the notion of information state for partially observed systems

and provide equivalent methods of identifying information states.

2. In Secs. 3 and 4, we present the notion of an approximate information state (AIS) as a
compression of history which approximately satisﬁes the properties of an information
state. The two equivalent formulations of information state lead to two equivalent
formulations of AIS. We present bounds on the loss in performance (compared to the
optimal history dependent policy) when planning using an AIS. We generalize these
results to cover approximation in action spaces as well. We show that various existing
approximation results for MDPs and POMDPs in the literature may be viewed as
special cases of AIS (and in some cases, our bounds are tighter than those in the
literature).

3. In Sec. 5, we present a theory for approximate planning for decentralized (i.e., multi-

agent) partially observed systems using a common-information based AIS.

4. In Secs. 6 and 7, we then present policy gradient based online RL algorithms for
PORL which learn an AIS representation using multi-timescale stochastic gradient
descent. We provide detailed numerical experiments on several classes of partially
observed environments ranging from classical low-dimensional toy environments, to
moderate-dimensional environments, and high-dimensional grid-world environments.

2. Preliminaries: Information state and dynamic programming

decomposition for partially observed systems

2.1 General model for a partially observed system

Traditionally, partially observed systems are modeled as partially observable Markov de-
cision processes (POMDPs) (Astr¨om, 1965; Smallwood and Sondik, 1973), where there
is a controlled state and an agent which makes noise corrupted observations of the state.

3

Subramanian, Sinha, Seraj, and Mahajan

However, for the purpose of understanding approximation for partially observed systems,
it is conceptually cleaner to start with an input-output model of the system as described
below.

(Y1, R1)

(Y2, R2)

(Yt, Rt)

Stochastic input Wt

Controlled input At

System

Observation Yt

Reward Rt

Figure 1: A stochastic input-output system

A1 W1

A2 W2

At Wt

Figure 2: The timing diagram of the input-
output system.

We view a partially observed system as a black-box input-output system shown in Fig. 1.
At each time t, the system has two inputs and generates two outputs. The inputs to the
system are a control input (also called an action) At ∈ A and a disturbance Wt ∈ W. The
outputs of the system are an observation Yt ∈ Y and a reward Rt ∈ R. For the ease of
exposition, we assume that A, W, and Y are ﬁnite sets. The analysis extends to general
spaces under appropriate technical conditions. The order in which the input and output
variables are generated is shown in Fig. 2.

As stated before, we do not impose a state space model on the system. Therefore, all we
can say is that the outputs (Yt, Rt) at time t are some function of all the inputs (A1:t, W1:t)
up to time t, i.e.,

Yt = ft(A1:t, W1:t)

and Rt = rt(A1:t, W1:t),

where {ft : At ×Wt → Y}T
are called the system reward functions.

t=1 are called the system output functions and {rt : At ×Wt → R}T

t=1

There is an agent which observes the output Yt and generates a control input or the
action At as a (possibly stochastic) function of the history Ht = (Y1:t−1, A1:t−1) of the past
observations and actions, i.e.,

At ∼ πt(Ht),

where π := (πt)t≥1 is a (history-dependent and possibly stochastic) policy. We use Ht to
denote the space of all histories up to time t. Then the policy πt is a mapping from Ht
to ∆(A) (which denotes the space of probability measures on A). We will use πt(at|ht) to
denote the probability of choosing action at at time t given history ht and use Supp(πt(ht))
to denote the support of πt (i.e., the set of actions chosen with positive probability).

We assume that the disturbance {Wt}t≥1 is a sequence of independent random variables
deﬁned on a common probability space (Ω, F, P). Thus, if the control input process {At}t≥1 is
speciﬁed, then the output processes {Yt, Rt}t≥1 are random variables on (Ω, F, P). Specifying
a policy π for the agent induces a probability measure on the output processes {Yt, Rt}t≥1,
which we denote by Pπ.

We start our discussion by looking at the planning problem faced by the agent when
the system runs for a ﬁnite horizon T . We will generalize our results to the inﬁnite horizon
discounted reward setup later. In the ﬁnite horizon setup, the performance of any policy π

4

Approximate information state

is given by

J(π) := Eπ

(cid:20) T

(cid:88)

(cid:21)

Rt

,

t=1

(1)

where Eπ denotes the expectation with respect to the probability measure Pπ.

We assume that the agent knows the system dynamics {ft}t≥1, the reward functions
{rt}t≥1, and the probability measure P on the primitive random variables {Wt}t≥1. The
objective of the agent is to choose a policy π which maximizes the expected total reward
J(π).

Since all system variables are assumed to be ﬁnite valued and the system runs for a ﬁnite
horizon, there are only a ﬁnite number of policies π. So, an optimal policy always exists and
the important question is to determine an eﬃcient algorithm to compute the optimal policy.
In Sec. 2.2, we start by presenting a trivial dynamic programming decomposition which
uses the entire history of observations as a state. Such a history-dependent dynamic program
is not an eﬃcient method to compute the optimal policy; rather it serve as a reference with
which we compare the more eﬃcient exact and approximate dynamic programs that we
derive later.

In Sec. 2.3, we present suﬃcient conditions to identify an information state for dynamic
programming. Our main result, presented in Secs. 3 and 4, is to identify a notion of
approximate information state and derive approximation bounds when an approximate
policy is computed using an approximate information state.

2.2 A dynamic programming decomposition

To obtain a dynamic program to identify an optimal policy for (1), we can view the history
Ht as a “state” of a Markov decision process (MDP) with transition probability

P(Ht+1 = (h(cid:48)

t, a(cid:48)

t, yt) | Ht = ht, At = at) =

(cid:40)P(Yt = yt|Ht = ht, At = at),
0,

t = ht & a(cid:48)

if h(cid:48)
otherwise

t = at

and per-step reward E[Rt|Ht, At]. Therefore, from standard results from Markov decision
processes Bellman (1957), we can recursively compute the performance of a given policy as
well as the best possible performance using “standard” dynamic program.

Proposition 1 (Policy evaluation) For any given (history dependent) policy π, deﬁne
the reward-to-go function for any time t and realization ht of history Ht as

t (ht) := Eπ
V π

(cid:20) T

(cid:88)

s=t

Rs

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

Ht = ht

.

(2)

The reward-to-go functions deﬁned above satisfy the following recursion. Deﬁne V π
0 and for any t ∈ {T, . . . , 1},

T +1(hT +1) =

t (ht) = Eπ(cid:2)Rt + V π
V π

t+1(Ht+1) (cid:12)

(cid:12) Ht = ht

(cid:3).

(3)

The reward-to-go function V π
future when starting from history ht at time t and following policy π. Note that V π

t (ht) denotes the expected cumulative rewards obtained in the
t (ht)

5

Subramanian, Sinha, Seraj, and Mahajan

only depends on the policy π only through the choice of the future policy (πt, . . . , πT ) and
therefore can be computed without the knowledge of the past policy (π1, . . . , πt−1).

Note that h1 = ∅ and the performance J(π) deﬁned in (1) equals V π(h1). Thus,
Proposition 1 gives a recursive method to evaluate the performance of any history dependent
policy π. Following the standard argument for Markov decision processes, we can modify
the recursion (3) to obtain a dynamic program to identify an optimal policy as follows.

Proposition 2 (Dynamic programming) Recursively deﬁne value functions {Vt : Ht →
R}T +1

t=1 as follows. VT +1(Ht+1) := 0 and for t ∈ {T, . . . 1},

Vt(ht) := max
at∈A

E(cid:2)Rt + Vt+1(Ht+1) (cid:12)

(cid:12) Ht = ht, At = at

(cid:3).

(4)

Then, a stochastic policy π = (π1, . . . , πT ) is optimal if and only if for all t ∈ {1, . . . T } it
satisﬁes

Supp(πt(ht)) ⊆ arg max
at∈A

E(cid:2)Rt + Vt+1(Ht+1) (cid:12)

(cid:12) Ht = ht, At = at

(cid:3).

(5)

Note that the expectation in (4) is with respect to the probability measure P on (Ω, F) and
can be computed without the knowledge of the policy π.

2.3 Information state and simpliﬁed dynamic programs

The dynamic program of Proposition 2 uses the entire history as state and may not be
eﬃcient for identifying an optimal policy. In this section, we present a general class of
dynamic programming decompositions which may be more eﬃcient. This class of dynamic
programs is based on the notion of information state, which we describe next.

Deﬁnition 3 Let {Zt}T
{σt : Ht → Zt}T
if the process {Zt}T

t=1 be a pre-speciﬁed collection of Banach spaces. A collection
t=1 of history compression functions is called an information state generator

t=1, where Zt = σt(Ht), satisﬁes the following properties:

(P1) Suﬃcient for performance evaluation, i.e., for any time t, any realization ht of

Ht and any choice at of At, we have

E[Rt | Ht = ht, At = at] = E[Rt | Zt = σt(ht), At = at].

(P2) Suﬃcient to predict itself, i.e., for any time t, any realization ht of Ht and any

choice at of At, we have that for any Borel subset B of Zt+1,

P(Zt+1 ∈ B | Ht = ht, At = at) = P(Zt+1 ∈ B | Zt = σt(ht), At = at).

In the sequel, we will sometimes use the phrase “let {Zt}T

t=1 be an information state” to
specify an information state and will implicitly assume that the corresponding information
state spaces are {Zt}T

t=1 and the corresponding compression functions are {σt}T

t=1.

Note that both the probabilities in Property (P2) can be computed without the knowledge
of the policy π. Furthermore, there are no restrictions on the spaces {Zt}T
t=1 although in
practice an information state is useful only when these spaces are “small” in an appropriate
sense.

Condition (P1) is easy to verify but condition (P2) can be a bit abstract. For some

models, instead of (P2), it is easier to verify the following stronger conditions:

6

Approximate information state

(P2a) Evolves in a state-like manner, i.e., there exist measurable functions {ϕt}T

t=1

such that for any time t and any realization ht+1 of Ht+1, we have

σt+1(ht+1) = ϕt(σt(ht), yt, at).

Informally, the above condition may be written as Zt+1 = ϕt(Zt, Yt, At).

(P2b) Is suﬃcient for predicting future observations, i.e., for any time t, any real-

ization ht of Ht and any choice at of At, we have that for any subset D of Y,

P(Yt ∈ D | Ht = ht, At = at) = P(Yt ∈ D | Zt = σt(ht), At = at).

Proposition 4 (P2a) and (P2b) imply (P2).

Proof For any Borel subset D of Zt+1, we have

P(Zt+1 ∈ D | Ht = ht, At = at)

(cid:88)

(a)
=

(b)
=

(c)
=

yt∈Y
(cid:88)

yt∈Y
(cid:88)

yt∈Y

P(Yt = yt, Zt+1 ∈ D | Ht = ht, At = at)

1{ϕt(σt(ht), yt, at) ∈ D}P(Yt = yt | Ht = ht, At = at)

1{ϕt(σt(ht), yt, at) ∈ D}P(Yt = yt | Zt = σt(ht), At = at)

(d)
= P(Zt+1 ∈ D | Zt = σt(ht), At = at)

where (a) follows from the law of total probability, (b) follows from (P2a), (c) follows from
(P2b) and (d) from the law of total probability.

The following example illustrates how (P2a) and (P2b) are stronger conditions than (P2).
Consider a Markov decision process (MDP) with state (S1
t ) ∈ S 1 × S 2 and action At ∈ A,
where the dynamics of the two components of the state are conditionally independent given
the action, i.e.,

t , S2

P(S1

t+1 = s1

+, S2

t+1 = s2

+|S1

t = s1, S2
= P(S1

t = s2, At = a)
+|S1

t+1 = s1

t = s1, At = a)P(S2

t+1 = s2

+|S2

t = s2, At = a).

t }t≥1 of the state satisﬁes properties (P1) and (P2). Therefore, {S1

Furthermore, suppose the reward Rt at any time t is given by Rt = rt(S1
t , At). Since the
model is an MDP, the observation at time t is the same as the state. For this model, the
component {S1
t }t≥1 is an
information state process. However, {S1
t }t≥1 is not suﬃcient to predict the next observation
(S1
t }t≥1 does not satisfy property (P2b). This shows that properties
(P2a) and (P2b) are stronger than property (P2). The above example may be considered as
an instance of what is called the Noisy-TV problem (Burda et al., 2018).

t+1). Therefore, {S1

t+1, S2

Next, we show that an information state is useful because it is always possible to write a
dynamic program based on the information state. To explain this dynamic programming

7

Subramanian, Sinha, Seraj, and Mahajan

decomposition, we ﬁrst write the history-based dynamic programs of Proposition 1 and 2 in
a more compact manner as follows: Let VT +1(hT +1) := 0 and for t ∈ {T, . . . , 1}, deﬁne

Qt(ht, at) := E(cid:2)Rt + Vt+1(Ht+1) (cid:12)

(cid:12) Ht = ht, At = at

(cid:3),

Vt(ht) := max
at∈A

Qt(ht, at).

(6a)

(6b)

The function Qt(ht, at) is called the action-value function. Moreover, for a given stochastic
policy π = (π1, . . . , πT ), where πt : Ht → ∆(At), let V π
T +1(hT +1) = 0 and for t ∈ {T, . . . , 1},
deﬁne

t (ht, at) := E(cid:2)Rt + V π
Qπ
V π
t (ht) :=

(cid:88)

πt(at | ht).Qπ

t (ht, at).

t+1(Ht+1) (cid:12)

(cid:12) Ht = ht, At = at

(cid:3),

(7a)

(7b)

at∈A

Theorem 5 Let {Zt}T
{ ¯Vt : Zt → R}T +1

t=1 be an information state. Recursively deﬁne value functions

t=1 , as follows: ¯VT +1(zT +1) := 0 and for t ∈ {T, . . . , 1}:
¯Qt(zt, at) := E[Rt + ¯Vt+1(Zt+1) | Zt = zt, At = at],

(8a)

(8b)

¯Vt(zt) := max
at∈A

¯Qt(zt, at).

Then, we have the following:

1. For any time t, history ht, and action at, we have that

Qt(ht, at) = ¯Qt(σt(ht), at) and Vt(ht) = ¯Vt(σt(ht)).

(9)

2. Let ¯π = (¯π1, . . . ¯πT ), where ¯πt : Zt → ∆(A), be a stochastic policy. Then, the policy
π = (π1, . . . , πT ) given by πt = ¯πt ◦ σt is optimal if and only if for all t and all
realizations zt of information states Zt, Supp(¯πt(zt)) ⊆ arg maxat∈A ¯Qt(zt, at).

Proof We prove the result by backward induction. By construction, (9) is true at time
T + 1. This forms the basis of induction. Assume that (9) is true at time t + 1 and consider
the system at time t. Then,

Qt(ht, at) = E[Rt + Vt+1(Ht+1) | Ht = ht, At = at]

(a)
= E[Rt + ¯Vt+1(σt+1(Ht+1)) | Ht = ht, At = at]
(b)
= E[Rt + ¯Vt+1(Zt+1) | Zt = σt(ht), At = at]
(c)
= ¯Qt(σt(ht), at),

where (a) follows from the induction hypothesis, (b) follows from the properties (P1) and
(P2) of information state, and (c) follows from the deﬁnition of ¯Q. This shows that the
action-value functions are equal. By maximizing over the actions, we get that the value
functions are also equal. The optimality of the policy follows immediately from (9).

8

Approximate information state

2.4 Examples of information state

For a general model, it is not immediately evident that a non-trivial information state exists.
The question of existence will depend on the speciﬁcs of the observation and reward functions
{ft, rt}t≥1 as well as the properties of the probability measure on the primitive random
variables {Wt}t≥1. We do not pursue the question of existence in this paper, but present
various speciﬁc models where information state exists and show that the corresponding
results for these models in the literature may be viewed as a special case of Theorem 5.

1. For any partially observed model, the history Ht is always a trivial information state.
Therefore, the dynamic program of Proposition 2 may be viewed as a special case of
Theorem 5.

2. Markov decision process (MDP): Consider a Markov decision process (MDP)
with state St ∈ S and action At ∈ A (Bellman, 1957). At each time, the state evolves
in a controlled Markovian manner with

P(St+1 = st+1 | S1:t = S1:t, A1:t = A1:t) = P(St+1 = st+1 | St = St, At = At).

The observation of the agent is Yt = St+1 and the reward output is Rt = r(St, At).
An information state for an MDP is given by the current state St (the corresponding
compression function is σt(S1:t, A1:t−1) = St). The standard dynamic program for
MDPs may be viewed as a special case of Theorem 5.

3. Even MDPs: Consider an MDP where the state space S is either R or a symmetric
subset of R of the form [−B, B], the controlled transition matrix is even, i.e., for every
a ∈ A and s, s(cid:48) ∈ S,

P(St+1 = s(cid:48) | St = s, At = a) = P(St+1 = −s(cid:48) | St = −s, At = a),

and for every a ∈ A, the per-step reward function r(s, a) is even in s. Such MDPs
are called even MDPs (Chakravorty and Mahajan, 2018) and an information state for
such MDPs is given by the absolute value state |St| (the corresponding compression
function is σt(S1:t, A1:t−1) = |St|). The dynamic program for even MDPs derived in
Chakravorty and Mahajan (2018) may be viewed as a special case of Theorem 5.

+|s1, s2, a) = P 1(s1

4. MDP with irrelevant components: Consider an MDP with state space S = S1×S2,
action space A, transition matrix P (s1
+|s1, s2, a),
+, s2
and per-step reward r(s1, a), which does not depend on the second component of
the state. As explained in Feinberg (2005), such models arise in control of queues
and transformation of continuous time Markov decision processes to discrete time
MDPs using uniformization. An information state for such MDPs is given by the ﬁrst
1:t, A1:t) = S1
component S1
t ).
The qualitative properties of optimal policies for such models derived in Feinberg
(2005) may be viewed as a special case of Theorem 5.

t (the corresponding compression function is σt(S1

+|s1, a)P 2(s2

1:t, S2

5. MDP with delayed state observation: Consider an MDP where the observation
Yt of the agent is the δ-step delayed state St−δ+1 of the system (Altman and Nain,

9

Subramanian, Sinha, Seraj, and Mahajan

1992). An information state for such MDPs is given by the vector (St−δ+1, Ut−δ+1:t−1).
The dynamic program for such models derived in Altman and Nain (1992) may be
viewed as a special case of Theorem 5.

6. Partially observable Markov decision processes (POMDPs): Consider a
partially observable Markov decision process (POMDP) where there is a state space
model as for an MDP but the observation Yt is some function of the state and the
disturbance, i.e., Yt = f y
t (St, Wt) (Astr¨om, 1965; Smallwood and Sondik, 1973). An
information state for the POMDP is given by the belief state Bt ∈ ∆(S) which is given
by Bt(s) = P(St = s | Ht = ht). The corresponding compression function may be
identiﬁed via the update functions {ϕt}T
t=1 of Property (P2a), which are the standard
belief update functions for non-linear ﬁltering. The standard belief state dynamic
program for POMDPs (Astr¨om, 1965; Smallwood and Sondik, 1973) may be viewed as
a special case of Theorem 5.

7. Linear quadratic and Gaussian (LQG) models: Consider a POMDP where the
state and action spaces are Euclidean spaces, the system dynamics P(St+1 | St, At)
and the observation f y
t (St, Wt) are linear, the disturbance Wt is Gaussian, and the
per-step cost is a quadratic function of the state and action (Astr¨om, 1970). For
such a linear-quadratic-and-Gaussian POMDP, an information state is given by the
state estimate ˆSt = E[St | Ht = ht]. The corresponding compression function may be
identiﬁed via the update functions {ϕt}T
t=1 of Property (P2a), which in this case are
Kalman ﬁltering update equations. The standard conditional estimate based dynamic
program for LQG models (Astr¨om, 1970) may be viewed as a special case of Theorem 5.

8. POMDPs with delayed observations: Consider a POMDP where the observation
is delayed by δ time steps (Bander and White, 1999). For such a system the belief
on δ step delayed state based on the δ-step delayed observations and control, as well
as the vector of last δ control actions is an information state. The structure of the
optimal policy and the dynamic program derived in Bander and White (1999) may be
viewed as a special case of Theorem 5.

9. Machine maintenance: Consider the following model for machine maintenance
(Eckles, 1968). A machine can be in one of n ordered states where the ﬁrst state is the
best and the last state is the worst. The production cost increases with the state of
the machine. The state evolves in a Markovian manner. At each time, an agent has
the option to either run the machine or stop and inspect it for a cost. After inspection,
the agent may either repair it (at a cost that depends on the state) or replace it (at a
ﬁxed cost). The objective is to identify a maintenance policy to minimize the cost of
production, inspection, repair, and replacement.

Let τ denote the time of last inspection and Sτ denote the state of the machine
after inspection, repair, or replacement. Then, it can be shown that (Sτ , t − τ ) is an
information state for the system. This is an instance of an incrementally expanding
representation for a POMDP described in Arabneydi and Mahajan (2015).

The above examples show that there are generic information states for certain class of
models (e.g., MDPs, MDPs with delays, POMDPs, POMDPs with delays) as well as speciﬁc

10

Approximate information state

information states tuned to the model (e.g., even MDPs, MDPs with irrelevant components,
LQG models, machine repair).

2.5 Discussion and related work

Although we are not aware of a previous result which formally deﬁnes an information state
and shows that an information state always implies a dynamic programming decomposition
(Theorem 5), yet the notion of information state is not new and has always existed in the
stochastic control literature. Information state may be viewed as a generalization of the
traditional notion of state (Nerode, 1958), which is deﬁned as a statistic (i.e., a function of the
observations) suﬃcient for input-output mapping. In contrast, we deﬁne an information state
as a statistic suﬃcient for performance evaluation (and, therefore, for dynamic programming).
Such a deﬁnition is hinted in Witsenhausen (1976). The notion of information state is also
related to suﬃcient statistics for optimal control deﬁned in Striebel (1965) for systems with
state space models.

As far as we are aware, the informal deﬁnition of information state was ﬁrst proposed
by Kwakernaak (1965) for adaptive control systems. Formal deﬁnitions for linear control
systems were given by Bohlin (1970) for discrete time systems and by Davis and Varaiya
(1972) for continuous time systems. Kumar and Varaiya (1986) deﬁne an information state
as a compression of past history which satisﬁes property (P2a) but do not formally show
that such an information state always leads to a dynamic programming decomposition. A
formal deﬁnition of information state appears in our previous work (Mahajan and Mannan,
2016) where the result of Theorem 5 is asserted without proof. Properties of information
states for multi-agent teams were asserted in Mahajan (2008). Adlakha et al. (2012) provide
a deﬁnition which is stronger than our deﬁnition. They require that in a POMDP with
unobserved state St ∈ S, σt(ht) should satisfy (P1) and (P2) as well be suﬃcient to predict
St, i.e., for any Borel subset B of S and any realization ht of Ht,

P(St ∈ B | Ht = ht) = P(St ∈ B | ˆZt = σt(ht)).

A similar deﬁnition is also used in Francois-Lavet et al. (2019). We had presented a deﬁnition
similar to Deﬁnition 3 in the preliminary version of this paper (Subramanian and Mahajan,
2019).

The notion of information state is also related to Γ-trace equivalence for MDPs and
POMDPs deﬁned by Castro et al. (2009). For MDPs. Γ-trace equivalence takes a partition
of the state space and returns a ﬁner partition such that for any choice of future actions any
two states in the same cell of the ﬁner partition have the same distribution on future states
and rewards. Castro et al. (2009) show that recursive applications of Γ-trace equivalence has
a ﬁxed point, which is equivalent to bisimulation based partition (Givan et al., 2003) of the
state space of the MDP. Similar results were shown for MDPs in Ferns et al. (2004, 2011).
Castro et al. (2009) extend the notion of trace equivalence for MDPs to belief trajectory
equivalence for POMDPs. In particular, two belief states are said to be belief trajectory
equivalent if for any choice of future actions, they generate the same distribution on future
observations and rewards. Such belief trajectory equivalence is related to predictive state
representation (PSR) (Littman et al., 2002; Singh et al., 2003; Izadi and Precup, 2003; James
et al., 2004; Rosencrantz et al., 2004; Wolfe et al., 2005) and observable operator models

11

Subramanian, Sinha, Seraj, and Mahajan

(OOM) (Jaeger, 2000; Jaeger et al., 2006), which are a compression of the past history which
is suﬃcient to predict the future observations (but not necessarily rewards). Information
state may be viewed as a “Markovianized” version of belief trajectory equivalence and
PSRs, which has the advantage that both (P1) and (P2) are deﬁned in terms of “one-step”
equivalence while belief trajectory equivalence and PSR are deﬁned in terms of “entire future
trajectory” equivalence. It should be noted that PSR and bisimulation based equivalences
are deﬁned for inﬁnite horizon models, while the information state is deﬁned for both ﬁnite
and inﬁnite horizon models (see Sec. 4).

Another related notion is the notion of causal states (or ε-machines) used in computational
mechanics (Crutchﬁeld and Young, 1989; Shalizi and Crutchﬁeld, 2001). and forecasting
in dynamical systems (Grassberger, 1986, 1988). These deﬁnitions are for uncontrolled
Markov chains and the emphasis is on the minimal state representation for time-invariant
inﬁnite-horizon systems.

3. Approximate planning in partially observed systems

Our key insight is that information states provide a principled approach to approximate
planning and learning in partially observed systems. To illustrate this, reconsider the
machine maintenance example presented earlier in Sec. 2.4. Theorem 5 implies that we can
write a dynamic program for that model using the information state (Sτ , t − τ ), which takes
values in a countable set. This countable state dynamic program is considerably simpler than
the standard belief state dynamic program typically used for that model. Moreover, it is
possible to approximate the countable state model by a ﬁnite-state model by truncating the
state space, which provides an approximate planning solution to the problem. Furthermore,
the information state (Sτ , t − τ ) does not depend on the transition probability of the state
of the machine or the cost of inspection or repair. Thus, if these model parameters were
unknown, we can use a standard reinforcement learning algorithm to ﬁnd an optimal policy
which maps (Sτ , t − τ ) to current action.

Given these beneﬁts of a good information state, it is natural to consider a data-driven
approach to identify an information state. An information state identiﬁed from data will
not be exact and it is important to understand what is the loss in performance when using
an approximate information state. Theorem 5 shows that a compression of the history
which satisﬁes properties (P1) and (P2) is suﬃcient to identify a dynamic programming
decomposition. Would a compression of history that approximately satisﬁed properties (P1)
and (P2) lead to an approximate dynamic program? In this section, we show that the answer
to this question is yes. First, we need to precisely deﬁne what we mean by “approximately
satisfy properties (P1) and (P2)”. For that matter, we need to ﬁx a distance metric on
probability spaces. There are various metrics on probability space and it turns out that
the appropriate distance metric for our purposes is the integral probability metric (IPM)
(M¨uller, 1997).

3.1 Integral probability metrics (IPM)

Deﬁnition 6 Let (X, G) be a measurable space and F denote a class of uniformly bounded
measurable functions on (X, G). The integral probability metric (IPM) between two probability

12

Approximate information state

distributions µ, ν ∈ ∆(X) with respect to the function class F is deﬁned as

(cid:12)
(cid:90)
(cid:12)
dF(µ, ν) := sup
(cid:12)
(cid:12)
f ∈F

X

f dµ −

(cid:90)

X

f dν

(cid:12)
(cid:12)
.
(cid:12)
(cid:12)

In the literature, IPMs are also known as probability metrics with a ζ-structure; see e.g.,
Zolotarev (1983); Rachev (1991). They are useful to establish weak convergence of probability
measures. Methods for estimating IPM from samples are discussed in Sriperumbudur et al.
(2012).

Examples of integral probability metrics (IPMs)

When (X, G) is a metric space, then various commonly used distance metrics on (X, G) lead
to speciﬁc instances of IPM for a particular choice of function space F. We provide some
examples below:

1. Total variation distance: If F is chosen as {f : (cid:107)f (cid:107)∞ ≤ 1}, then dF is the total

variation distance.1

2. Kolmogorov distance: If X = Rm and F is chosen as {1(−∞,t] : t ∈ Rm}, then dF is

the Kolmogorov distance.

3. Kantorovich metric or Wasserstein distance: Let (cid:107)f (cid:107)Lip denote the Lipschitz
semi-norm of a function. If F is chosen as {f : (cid:107)f (cid:107)Lip ≤ 1}, then dF is the Kantorovich
metric. When X is separable, the Kantorovich metric is the dual representation of the
Wasserstein distance via the Kantorovich-Rubinstein duality (Villani, 2008).

4. Bounded-Lipschitz metric: If F is chosen as {f : (cid:107)f (cid:107)∞ + (cid:107)f (cid:107)Lip ≤ 1}, then dF is

the bounded-Lipschitz (or Dudley) metric.

5. Maximum mean discrepancy (MMD): Let H be a reproducing kernel Hilbert space
(RKHS) of real valued functions on X and let F = {f ∈ H : (cid:107)f (cid:107)H ≤ 1}, then dF is
the maximum mean discrepancy2 (Sriperumbudur et al., 2008). The energy distance

1. In particular, if µ and ν are absolutely continuous with respect to some measure λ and let p = dµ/dλ

and q = dν/dλ, then

X

(cid:90)

(cid:90)

(cid:90)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f dν

f dµ −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
In this paper, we are deﬁning total variation distance as (cid:82)
X |p(x) − q(x)|λ(dx). Typically, it is deﬁned as
half of that quantity. Note that it is possible to get a tighter bound than above where (cid:107)f (cid:107)∞ is replaced
by 1

(cid:12)
(cid:12)
f (x)q(x)λ(dx)
(cid:12)
(cid:12)

(cid:12)p(x) − q(x)(cid:12)
(cid:12)

f (x)p(x)λ(dx) −

≤ (cid:107)f (cid:107)∞

(cid:12)λ(dx).

(cid:90)

(cid:90)

X

X

X

X

2 span(f ) = 1

2 (max(f ) − min(f )).

2. One of features of MMD is that the optimizing f can be identiﬁed in closed form. In particular, if k is

the kernel of the RKHS, then (see Gretton et al. (2006); Sriperumbudur et al. (2012) for details)

(cid:90)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:20)(cid:90)

X

(cid:90)

dF(µ, ν) =

=

k(·, x)dµ(x) −

(cid:90)

X

k(·, x)dν(x)

(cid:13)
(cid:13)
(cid:13)
(cid:13)H

k(x, y)µ(dx)µ(dy) +

(cid:90)

(cid:90)

X

X

k(x, y)ν(dx)ν(dy) − 2

k(x, y)µ(dx)ν(dy)

.

(cid:21)1/2

(cid:90)

(cid:90)

X

X

X

X

We use an MMD as a IPM in the PORL algorithms proposed in Sec. 6, where we exploit this property.

13

Subramanian, Sinha, Seraj, and Mahajan

studied in statistics (Sz´ekely and Rizzo, 2004) is a special case of maximum mean
discrepancy; see Sejdinovic et al. (2013) for a discussion.

We say that F is a closed set if it is closed under the topology of pointwise convergence.
We say that F is a convex set if f1, f2 ∈ F implies that for any λ ∈ (0, 1), λf1 + (1 − λ)f2 ∈ F.
Note that all the above function classes are convex and all except Kolmogorov distance are
closed.

We now list some useful properties of IPMs, which immediately follow from deﬁnition.

1. Given a function class F and a function f (not necessarily in F),

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

X

f dµ −

(cid:90)

X

f dν

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ρF(f ) · dF(µ, ν),

where ρF(f ) is the Minkowski functional with respect to F given by

ρF(f ) := inf{ρ ∈ R>0 : ρ−1f ∈ F}.

(10)

(11)

For the total variation distance, (cid:12)
X f dµ − (cid:82)
(cid:82)
2 span(f )dF(µ, ν). Thus, for
(cid:12)
X f dµ − (cid:82)
total variation, ρF(f ) = 1
(cid:12) ≤
(cid:107)f (cid:107)LipdF(µ, ν). Thus, for Kantorovich metric, ρF(f ) = (cid:107)f (cid:107)Lip. For the maximum mean
discrepancy, (cid:12)
(cid:82)
(cid:12) ≤ (cid:107)f (cid:107)HdF(µ, ν). Thus, for maximum mean discrepancy,
(cid:12)
ρF(f ) = (cid:107)f (cid:107)H.

2 span(f ). For the Kantorovich metric, (cid:12)
(cid:82)
(cid:12)
X f dν(cid:12)

X f dµ − (cid:82)

X f dν(cid:12)

X f dν(cid:12)

(cid:12) ≤ 1

2. Let X and Y be Banach spaces and let FX and FY denote the function class for dF
with domain X and Y, respectively. Then, for any (cid:96) : X → Y, any real-valued function
f ∈ FY and any measures µ and ν on ∆(X), we have

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

X

f ((cid:96)(x))µ(dx) −

(cid:90)

X

f ((cid:96)(x))ν(dx)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ρFX(f ◦ (cid:96))dFX(µ, ν).

We deﬁne the contraction factor of the function (cid:96) as

κFX,FY ((cid:96)) = sup
f ∈FY

ρFX(f ◦ (cid:96)).

(12)

Therefore, we can say that for any f ∈ FY,

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

X

f ((cid:96)(x))µ(dx) −

(cid:90)

X

(cid:12)
(cid:12)
f ((cid:96)(x))ν(dx)
(cid:12)
(cid:12)

≤ κFX,FY ((cid:96))dFX(µ, ν).

(13)

For the total variation distance, 1
For the Kantorovich metric, (cid:107)f ◦ (cid:96)(cid:107)Lip ≤ (cid:107)f (cid:107)Lip(cid:107)(cid:96)(cid:107)Lip Thus, κF((cid:96)) ≤ (cid:107)(cid:96)(cid:107)Lip.

2 span(f ◦ (cid:96)) ≤ (cid:107)f ◦ (cid:96)(cid:107)∞ ≤ (cid:107)f (cid:107)∞ ≤ 1. Thus, κF((cid:96)) ≤ 1.

3.2 Approximate information state (AIS) and approximate dynamic

programming

Now we deﬁne a notion of AIS as a compression of the history of observations and actions
which approximately satisﬁes properties (P1) and (P2).

14

Approximate information state

t=1 be a pre-speciﬁed collection of Banach spaces, F be a function class
t=1 be pre-speciﬁed positive real numbers. A collection {ˆσt : Ht →
t=1 of history compression functions, along with approximate update kernels { ˆPt : ˆZt ×
t=1, is called an
t=1, where ˆZt = ˆσt(Ht), satisﬁes the following

t=1 and reward approximation functions {ˆrt : ˆZt × A → R}T

t=1-AIS generator if the process { ˆZt}T

Deﬁnition 7 Let {ˆZt}T
for IPMs, and {(εt, δt)}T
ˆZt}T
A → ∆(ˆZt+1)}T
{(εt, δt)}T
properties:

(AP1) Suﬃcient for approximate performance evaluation, i.e., for any time t, any

realization ht of Ht and any choice at of At, we have

(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), at)(cid:12)
(cid:12)

(cid:12) ≤ εt.

(AP2) Suﬃcient to predict itself approximately. i.e., for any time t, any realization
ht of Ht, any choice at of At, and for any Borel subset B of ˆZt+1, deﬁne µt(B) :=
P( ˆZt+1 ∈ B | Ht = ht, At = at) and νt(B) := ˆPt(B | ˆσt(ht), at); then,

dF(µt, νt) ≤ δt.

We use the phrase “(ε, δ)-AIS” when εt and δt do not depend on time.

Similar to Proposition 4, we can provide an alternative characterization of an AIS where

we replace (AP2) with the following approximations of (P2a) and (P2b).

(AP2a) Evolves in a state-like manner, i.e., there exist measurable update functions

{ ˆϕt : ˆZt × Y × A}T

t=1 such that for any realization ht+1 of Ht+1, we have

ˆσt+1(ht+1) = ˆϕ(ˆσt(ht), yt, at).

(AP2b) Is suﬃcient for predicting future observations approximately, i.e., there
exist measurable observation prediction kernels { ˆP y
t=1 such that for
any time t, any realization ht of Ht, any choice at of At, and for any Borel subset B of
Y deﬁne, µy

t (B) := P(Yt ∈ B | Ht = ht, At = at) and νy

t : ˆZt × A → ∆(Y)}T

t (B|ˆσt(ht), at); then,

t (B) = ˆP y

dF(µy

t , νy

t ) ≤ δ/κF( ˆϕt),

where κF( ˆϕt) is deﬁned as supht∈Ht,at∈At κF( ˆϕt(ˆσt(ht), ·, at)). Note that for the total
variation distance κF( ˆϕt) = 1; for the Kantorovich distance κF( ˆϕt) is equal to the
Lipschitz uniform bound on the Lipschitz constant of ˆϕt with respect to yt.

Proposition 8 (AP2a) and (AP2b) imply (AP2) holds with transition kernels { ˆP y
deﬁned as follows: for any Borel subset B of ˆZ,

t }T

t=1

ˆPt(B | ˆσt(ht), at) =

(cid:90)

Y

1B( ˆϕt(ˆσt(ht), yt, at)) ˆP y

t (dyt|ˆσt(ht), at).

Therefore, we can alternatively deﬁne an {(εt, δt)}T
which satisﬁes (AP1), (AP2a), and (AP2b).

t=1-AIS generator as a tuple {(ˆσt, ˆrt, ˆϕt, ˆP y

t )}T

t=1

15

Subramanian, Sinha, Seraj, and Mahajan

Proof Note that by the law of total probability, µt and νt deﬁned in (AP2) are

(cid:90)

Y

(cid:90)

µt(B) =

νt(B) =

Y
Thus, for any function f : ˆZt+1 → R,

1B( ˆϕt(ˆσt(ht), yt, at))µy

t (dyt),

1B( ˆϕt(ˆσt(ht), yt, at))νy

t (dyt).

(cid:90)

ˆZt+1
(cid:90)

ˆZt+1

f dµt =

f dνt =

(cid:90)

Yt

(cid:90)

Yt

f ( ˆϕt(ˆσt(ht), yt, at))µy

t (dyt),

f ( ˆϕt(ˆσt(ht), yt, at))νy

t (dyt).

The result then follows from (13).

Our main result is to establish that any AIS gives rise to an approximate dynamic

program.
Theorem 9 Suppose {ˆσt, ˆPt, ˆrt}T
t=1 is an {(εt, δt)}T
approximate action-value functions { ˆQt : ˆZt×A → R}T
as follows: ˆVT +1(ˆzT +1) := 0 and for t ∈ {T, . . . , 1}:

t=1-AIS generator. Recursively deﬁne
t=1 and value functions { ˆVt : ˆZt → R}T
t=1

ˆQt(ˆzt, at) := ˆrt(ˆzt, at) +

(cid:90)

ˆZt+1

ˆVt(ˆzt) := max
at∈A

ˆQt(ˆzt, at).

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆzt, at),

(14a)

(14b)

Then, we have the following:

1. Value function approximation: For any time t, realization ht of Ht, and choice

at of At, we have

|Qt(ht, at) − ˆQt(ˆσt(ht), at)| ≤ αt

and

|Vt(ht) − ˆVt(ˆσt(ht))| ≤ αt,

(15)

where αt satisﬁes the following recursion: αT +1 = 0 and for t ∈ {T, . . . , 1},

Therefore,

αt = εt + ρF( ˆVt+1)δt + αt+1.

αt = εt +

T
(cid:88)

τ =t+1

(cid:2)ρF( ˆVτ )δτ −1 + ετ

(cid:3).

2. Approximately optimal policy: Let ˆπ = (ˆπ1, . . . , ˆπT ), where ˆπt : ˆZt → ∆(A), be a

stochastic policy that satisﬁes

Supp(ˆπ(ˆzt)) ⊆ arg max
at∈A

ˆQt(ˆzt, at).

(16)

Deﬁne policy π = (π1, . . . , πT ), where πt : Ht → ∆(A) by πt := ˆπt ◦ ˆσt. Then, for any
time t, realization ht of Ht, and choice at of At, we have

|Qt(ht, at) − Qπ

t (ht, at)| ≤ 2αt

and

|Vt(ht) − V π

t (ht)| ≤ 2αt.

(17)

16

Approximate information state

Proof We prove both parts by backward induction. We start with value function approxi-
mation. Eq. (15) holds at T + 1 by deﬁnition. This forms the basis of induction. Assume
that (15) holds at time t + 1 and consider the system at time t. We have that
(cid:12)Qt(ht, at) − ˆQt(ˆσt(ht), at)(cid:12)
(cid:12)
(cid:12)
(a)
≤ (cid:12)

(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), at)(cid:12)
(cid:12)
+ E(cid:2)(cid:12)
(cid:12)Vt+1(Ht+1) − ˆVt+1(ˆσt+1(Ht+1))(cid:12)
(cid:12)
(cid:12)
(cid:12)
E[ ˆVt+1(ˆσt+1(Ht+1)) | Ht = ht, At = at] −
(cid:12)
(cid:12)

+

(cid:12)
(cid:12) Ht = ht, At = at

(cid:3)

(cid:90)

ˆZt+1

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆσt(ht), at)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(b)
≤ εt + αt+1 + ρF( ˆVt+1)δt = αt

where (a) follows from triangle inequality and (b) follows from (AP1), the induction hy-
pothesis, (AP2) and (10). This proves the ﬁrst part of (15). The second part follows
from

(a)
≤ max
at∈A
where (a) follows from the inequality max f (x) ≤ max |f (x) − g(x)| + max g(x).

(cid:12)Qt(ht, at) − ˆQt(ˆσt(ht), at)(cid:12)
(cid:12)

(cid:12)Vt(ht) − ˆVt(ˆσt(ht))(cid:12)
(cid:12)
(cid:12)

(cid:12) ≤ αt,

To prove the policy approximation, we ﬁrst prove an intermediate result. For policy ˆπ
T +1(ˆzT +1) := 0

t=1 as follows: ˆV ˆπ

t : ˆZ × A → R}T

t=1 and { ˆV ˆπ

: ˆZ → R}T +1

t

recursively deﬁne { ˆQˆπ
and for t ∈ {T, . . . , 1}:

ˆQˆπ

t (ˆzt, at) := ˆrt(ˆzt, at) +

t (ˆzt+1) ˆPt(dˆzt+1 | ˆzt, at)
ˆV ˆπ

(cid:90)

ˆZt+1
ˆπt(at | ˆzt). ˆQˆπ

t (ˆzt, at).

ˆV ˆπ
t (ˆzt) :=

(cid:88)

at∈A

Note that (16) implies that

ˆQˆπ

t (ˆzt, at) = ˆQt(ˆzt, at)

and

t (ˆzt) = ˆVt(ˆzt).
ˆV ˆπ

Now, we prove that

|Qπ

t (ht, at) − ˆQˆπ

t (ˆσt(ht), at)| ≤ αt

and |V π

t (ht) − ˆV ˆπ

t (ˆσt(ht))| ≤ αt.

(18a)

(18b)

(19)

(20)

(cid:12)
(cid:12)Qπ

t (ht, at) − ˆQˆπ
(a)
≤ (cid:12)

We prove the result by backward induction. By construction, Eq. (20) holds at time T + 1.
This forms the basis of induction. Assume that (20) holds at time t + 1 and consider the
system at time t. We have
t (ˆσt(ht), at)(cid:12)
(cid:12)
(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), at)(cid:12)
(cid:12)
+ E(cid:2)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
t+1(ˆzt+1) ˆPt(dˆzt+1 | ˆσt(ht), at)
ˆV ˆπ
(cid:12)
(cid:12)

t+1(ˆσt+1(Ht+1)) | Ht = ht, At = at] −

(cid:12)
(cid:12) Ht = ht, At = at

t+1(ˆσt+1(Ht+1))(cid:12)
(cid:12)

t+1(Ht+1) − ˆV ˆπ

E[ ˆV ˆπ

(cid:12)V π

+

(cid:90)

(cid:3)

ˆZt+1

(b)
≤ εt + αt+1 + ρF( ˆVt+1)δt = αt

17

Subramanian, Sinha, Seraj, and Mahajan

where (a) follows from triangle inequality and (b) follows from (AP1), the induction hypoth-
esis, (AP2) and (10). This proves the ﬁrst part of (20). The second part follows from the
triangle inequality:

(cid:12)
(cid:12)V π

t (ht) − ˆV ˆπ

t (ˆσt(ht))(cid:12)

(cid:12) ≤

(cid:88)

at∈A

ˆπt(at|ˆσt(ht))(cid:12)

(cid:12)Qπ(ht, at) − ˆQˆπ

t (ˆσt(ht), at)(cid:12)

(cid:12) ≤ αt.

t (ht, at)(cid:12)

(cid:12)
(cid:12)Qt(ht, at)−Qπ

Now, to prove the policy approximation, we note that
(cid:12)Qπ

t (ˆσt(ht), at)(cid:12)
where the ﬁrst inequality follows from the triangle inequality, the ﬁrst part of the second
inequality follows from (15) and (19) and the second part follows from (20). This proves the
ﬁrst part of (17). The second part of (17) follows from the same argument.

(cid:12)Qt(ht, at)− ˆQˆπ

t (ˆσt(ht), at)(cid:12)

t (ht, at)− ˆQˆπ

(cid:12) ≤ αt+αt,

(cid:12) ≤ (cid:12)

(cid:12)+(cid:12)

An immediate implication of Theorems 5 and 9 is the following.

Corollary 10 Let {σt}T
t=1 be an AIS
generator. Then, for any time t, realization ht of history Ht, and choice at of action At, we
have

t=1 be an information state generator and {(ˆσt, ˆPt, ˆrt)}T

(cid:12) ¯Qt(σt(ht), at) − ˆQt(ˆσt(ht), at)(cid:12)
(cid:12)
where ¯Qt and ¯Vt are deﬁned as in Theorem 5.

(cid:12) ≤ αt

and

(cid:12) ¯Vt(σt(ht)) − ˆVt(ˆσt(ht))(cid:12)
(cid:12)

(cid:12) ≤ αt,

Remark 11 It is possible to derive a tighter bound in Theorem 9 and show that

αt = εt + ∆∗

t ( ˆVt+1) + αt+1

where

∆∗

t ( ˆVt+1) = sup
ht,at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)
E[ ˆVt+1(ˆσt+1(Ht+1)) | Ht = ht, At = at]−

ˆZt+1

(cid:12)
(cid:12)
ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆσt(ht), at)
(cid:12)
(cid:12)

The bound presented in Theorem 9 can be then thought of as an upper bound on ∆∗
ρF( ˆVt+1)δ using (10).

t ( ˆVt+1) ≤

Remark 12 In part 1 of Theorem 9, it is possible to derive an alternative bound

|Qt(ht, at) − ˆQt(ˆσt(ht), at)| ≤ α(cid:48)
t

and

|Vt(ht) − ˆVt(ˆσt(ht))| ≤ α(cid:48)
t

where α(cid:48)

t satisﬁes the recursion: α(cid:48)
T +1 = 0 and for t ∈ {T, . . . , 1},
t = εt + ρF(Vt+1)δt + α(cid:48)
α(cid:48)
This is because while using the triangle inequality in step (a) in the proof of Theorem 9,
we could have alternatively added and subtracted the term E[V π
t+1(Ht+1) | Ht = ht, At = at]
instead of E[ ˆV ˆπ
t+1(ˆσt+1(Ht+1)) | Ht = ht, At = at]. Using this bound, we can also derive an
alternative bound for part 2 of the Theorem and show that

t+1.

|Qt(ht, at) − Qπ

t (ht, at)| ≤ αt + α(cid:48)
t

and

|Vt(ht) − V π

t (ht)| ≤ αt + α(cid:48)
t.

18

Approximate information state

3.3 Examples of approximate information states

We now present various examples of information state and show that many existing results
in the literature may be viewed as a special case of Theorem 9. Some of these examples are
for inﬁnite horizon discounted reward version of Theorem 9 (with discount factor γ ∈ (0, 1)),
which we prove later in Theorem 27.

1. Model approximation in MDPs: Consider an MDP with state space S, action space
A, transition kernel Pt : S × A → ∆(S), and per-step reward rt : S × A → R. Consider an
approximate model deﬁned on the same state and action spaces with transition kernel
ˆPt : S × A → ∆(S) and per-step reward ˆrt : S × A → R. Deﬁne ˆσt(S1:t, A1:t−1) = St.
Then {(ˆσt, ˆPt, ˆrt)}T
t=1 is an AIS with
(cid:12)rt(s, a) − ˆrt(s, a)(cid:12)
(cid:12)

(cid:0)Pt(·|s, a), ˆPt(·|s, a)(cid:1).

(cid:12) and δt = sup

εt := sup

dF

s∈S,a∈A

s∈S,a∈A

A result similar in spirit to Theorem 9 for this setup for general dF is given in
Theorem 4.2 of M¨uller (1997). When dF is the Kantorovich metric, a bound for model
approximation for inﬁnite horizon setup is provided in Theorem 2 of Asadi et al. (2018).
This is similar to our result generalization of Theorem 9 to inﬁnite horizon, which is
given in Theorem 27; a bound on ρF( ˆV ) in this case can be obtained using results of
Hinderer (2005); Rachelson and Lagoudakis (2010).

2. State abstraction in MDPs: Consider an MDP with state space S, action space
A, transition kernel Pt : S × A → ∆(S), and per-step reward rt : S × A → R. Consider
an abstract model deﬁned over a state space ˆS (which is “smaller” than S) and the
same action space with transition kernel ˆPt : ˆS × A → ∆(ˆS) and per-step reward
ˆrt : ˆS × A → R. Suppose there is an abstraction function q : S → ˆS and, in state S ∈ S,
we choose an action based on q(S). For such a model, deﬁne ˆσt(S1:t, A1:t−1) = q(St).
Then {(ˆσt, ˆPt, ˆrt)}T

t=1 is an AIS with
(cid:12)rt(s, a) − ˆrt(q(s), a)(cid:12)
(cid:12)

εt := sup

s∈S,a∈A

(cid:12) and δt := sup

s∈S,a∈A

(cid:0)µt(·|s, a), ˆPt(·|q(s), a)(cid:1),

dF

where for any Borel subset B of ˆS, µt(B|s, a) := Pt(q−1(B)|s, a).
There is a rich literature on state abstraction starting with Bertsekas (1975) and Whitt
(1978), but the error bounds in those papers are of a diﬀerent nature. There are some
recent papers which derive error bounds similar to Theorem 9 for the inﬁnite horizon
setup with state abstraction. We generalize Theorem 9 to inﬁnite horizon later in
Theorem 27.
When dF is the Kantorovich metric, a bound on ρF( ˆV ) = (cid:107) ˆV (cid:107)Lip can be obtained
using results of Hinderer (2005); Rachelson and Lagoudakis (2010). Substituting this
bound in Theorem 27 gives us the following bound on the policy approximation error
by using AIS.

(cid:12)V (s) − V π(s)(cid:12)
(cid:12)

(cid:12) ≤

2ε
(1 − γ)

+

2γδ(cid:107) ˆV (cid:107)Lip
(1 − γ)

.

Similar bound has been obtained in Theorem 5 of Gelada et al. (2019). A detailed
comparison with this model is presented in Appendix B.

19

Subramanian, Sinha, Seraj, and Mahajan

When dF is the total variation distance, a bound on dF( ˆV ) is given by span(r)/(1 − γ).
Substituting this in Theorem 27, we get that

|V (s) − V π(s)| ≤

2ε
(1 − γ)

+

γδ span(r)
(1 − γ)2 .

A O(1/(1 − γ)3) bound on the policy approximation error in this setup was obtained
in Lemma 2 and Theorem 2 of Abel et al. (2016). Directly using the AIS bound
of Theorems 9 and 27 gives a factor of 1/(1 − γ) improvement in the error
bound of Abel et al. (2016). See Appendix A for a detailed comparison.

3. Belief approximation in POMDPs: Consider a POMDP with state space S,
action space A, observation space Y, and a per-step reward function rt : S × A → R.
Let bt(·|Ht) ∈ ∆(S) denote the belief of the current state given the history, i.e.,
bt(s|Ht) = P(St = s | Ht). Suppose there are history compression functions {φt : Ht →
Φt}T
t=1 (where Φt is some arbitrary space) along with belief approximation functions
{ˆbt : Φt → ∆(S)}T

t=1, such that for any time t and any realization ht of Ht, we have

(cid:107)ˆbt(· | φt(ht)) − bt(· | ht)(cid:107)1 ≤ ε.

Such a {(φt, ˆbt)}T
t=1 was called an ε-suﬃcient statistic in Francois-Lavet et al. (2019).
An example of ε-suﬃcient statistic is belief quantization, where the belief is quantized
to the nearest point in the type lattice (here m = |S|)

Qn := (cid:8)(p1, . . . , pm) ∈ ∆(S) : npi ∈ Z≥0

(cid:9).

An eﬃcient algorithm to ﬁnd the nearest point in Qn for any given belief bt ∈ ∆(S)
is presented in Reznik (2011). Under such a quantization, the maximum (cid:96)1 distance
between a belief vector and its quantized value is given by 2(cid:98)m/2(cid:99)(cid:100)m/2(cid:101)/mn ≈ m/2n
(see Proposition 2 of Reznik (2011)). Thus, by taking n > m/2ε, we get an ε-suﬃcient
statistic.

Francois-Lavet et al. (2019) showed that the bias of using the optimal policy based
on ˆbt(ht) in the original model is 2ε(cid:107)r(cid:107)∞/(1 − γ)3. This result uses the same proof
argument as Abel et al. (2016) discussed in the previous bullet point, which is not
tight. By metricizing the belief space using total variation distance and using the
bounded-Lipschitz metric on the space of probability measures on beliefs, we can show
that an ε-suﬃcient statistic induces a (ε span(r), 3ε)-AIS. When dF is the bounded-
Lipschitz metric, a bound on ρF( ˆV ) is given by 2(cid:107)r(cid:107)∞/(1 − γ). Substituting this in
Theorem 27, we get that

|V (s) − V π(s)| ≤

2ε(cid:107)r(cid:107)∞
(1 − γ)

+

6γε(cid:107)r(cid:107)∞
(1 − γ)2 .

Thus, directly using the AIS bound of Theorems 9 and 27 gives a factor of
1/(1 − γ) improvement in the error bound of Francois-Lavet et al. (2019).
See Appendix C for details.

In a slightly diﬀerent vein, belief quantization in POMDPs with ﬁnite or Borel
valued unobserved state was investigated in Saldi et al. (2018), who showed that

20

Approximate information state

under appropriate technical conditions the value function and optimal policies for
the quantized model converge to the value function and optimal policy of the true
model. However Saldi et al. (2018) did not provide approximation error for a ﬁxed
quantization level.

3.4 Approximate policy evaluation

In some settings, we are interested in comparing the performance of an arbitrary policy in
an approximate model with its performance in the real model. The bounds of Theorem 9
can be adapted to such a setting as well.

1 , . . . , ˆπ#
T ),
t : ˆZt → ∆(A), be an arbitrary stochastic policy. Recursively deﬁne approximate

t=1-AIS generator. Let ˆπ# = (ˆπ#

t=1 is an {(εt, δt)}T

Theorem 13 Suppose {ˆσt, ˆPt, ˆrt}T
where ˆπ#
policy action-value functions { ˆQˆπ#
as follows: ˆV ˆπ#

: ˆZt × A → R}T
t
T +1(ˆzT +1) := 0 and for t ∈ {T, . . . , 1}:

t=1 and value functions { ˆV ˆπ#

: ˆZt → R}T

t=1

t

ˆQˆπ#
t

(ˆzt, at) := ˆrt(ˆzt, at) +

ˆV ˆπ#
t

(ˆzt+1) ˆPt(dˆzt+1 | ˆzt, at)

(cid:90)

ˆV ˆπ#
t

(ˆzt) :=

(cid:88)

at∈A

ˆZt+1
t (at | ˆzt). ˆQˆπ#
ˆπ#

t

(ˆzt, at).

(21a)

(21b)

1 , . . . , π#
Deﬁne policy π# = (π#
time t, realization ht of Ht, and choice at of At, we have:

T ), where π#

t : Ht → ∆(A) by π#

t

:= ˆπ#

t ◦ ˆσt. Then, for any

|Qπ#
t

(ht, at) − ˆQˆπ#

t

(ˆσt(ht), at)| ≤ α#
t

and

|V π#
t

(ht) − ˆV ˆπ#

t

(ˆσt(ht))| ≤ α#
t ,

(22)

where α#

t satisﬁes the following recursion: α#

T +1 = 0 and for t ∈ {T, . . . , 1},

t = εt + ρF( ˆV ˆπ#
α#

t+1)δt + α#

t+1.

Therefore,

α#

t = εt +

T
(cid:88)

τ =t+1

(cid:2)ρF( ˆV ˆπ#

τ

)δτ −1 + ετ

(cid:3).

Proof The proof proceeds by backward induction along the same lines as the proof of
Theorem 9. By construction, Eq. (22) holds at time T + 1. This forms the basis of induction.
Assume that (22) holds at time t + 1 and consider the system at time t. We have

(cid:12)
(cid:12)Qπ#

t

(ht, at) − ˆQˆπ#

t

(ˆσt(ht), at)(cid:12)
(cid:12)

(a)
≤ (cid:12)

(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), at)(cid:12)
(cid:12)
+ E(cid:2)(cid:12)
(cid:12)V π#
(cid:12)
(cid:12)
E[ ˆV ˆπ#
(cid:12)
(cid:12)

t+1(ˆσt+1(Ht+1)) | Ht = ht, At = at] −

t+1(ˆσt+1(Ht+1))(cid:12)
(cid:12)

t+1(Ht+1) − ˆV ˆπ#

+

(cid:12)
(cid:12) Ht = ht, At = at

(cid:3)

(cid:90)

ˆZt+1

(cid:12)
(cid:12)
ˆV ˆπ#
t+1(ˆzt+1) ˆPt(dˆzt+1 | ˆσt(ht), at)
(cid:12)
(cid:12)

(b)
≤ εt + α#

t+1 + ρF( ˆV ˆπ#

t+1)δt = α#

t

21

Subramanian, Sinha, Seraj, and Mahajan

where (a) follows from triangle inequality and (b) follows from (AP1), the induction hypoth-
esis, (AP2) and (10). This proves the ﬁrst part of (22). The second part follows from the
the fact that π#(at|ht) = ˆπ#(at|ˆσt(ht)) and the triangle inequality:
(cid:88)
(ht, at) − ˆQˆπ#

(ˆσt(ht), at)(cid:12)

(ht) − ˆV ˆπ#

(ˆσt(ht))(cid:12)

t (at|ˆσt(ht))(cid:12)
ˆπ#

(cid:12) ≤ α#
t .

(cid:12)
(cid:12)V π#

(cid:12)Qπ#

(cid:12) ≤

t

t

t

at∈A

3.5 Stochastic AIS
We have so far assumed that the history compression functions ˆσt : Ht → ˆZt are deterministic
functions. When learning a discrete-valued AIS from data, it is helpful to consider stochastic
mappings of history, so that quality of the mapping may be improved via stochastic gradient
descent. In general, the deﬁnition of deterministic AIS also covers the case of stochastic AIS
because a stochastic function from Ht to ˆZt may be viewed as a deterministic function from
Ht to ∆(ˆZt). However, a more explicit characterization is also possible, which we present
next.

Deﬁnition 14 Let {ˆZt}T
class for IPMs, and {(εt, δt)}T
∆(ˆZt)}T
{ ˆPt : ˆZt × A → ∆(ˆZt+1)}T
called an {(εt, δt)}T
satisﬁes the following properties:

t=1 be a pre-speciﬁed collection of Banach spaces, F be a function
t : Ht →
t=1 of stochastic history compression functions, along with approximate update kernels
t=1, is
t=1, where ˆZt = ˆσt(Ht),

t=1 and reward approximation functions {ˆrt : ˆZt × A → R}T

t=1 be pre-speciﬁed positive real numbers. A collection {ˆσs

t=1-stochastic AIS generator if the process { ˆZt}T

(AP1) Suﬃcient for approximate performance evaluation, i.e., for any time t, any

realization ht of Ht and any choice at of At, we have

(cid:12)
(cid:12)E[Rt | Ht = ht, At = at] − E ˆZt∼ˆσs

t (ht)[ˆrt( ˆZt, at)](cid:12)

(cid:12) ≤ εt.

(AP2) Suﬃcient to predict itself approximately. i.e., for any time t, any realization
ht of Ht, any choice at of At, and for any Borel subset B of ˆZt+1, deﬁne µt(B) :=
P( ˆZt+1 ∈ B | Ht = ht, At = at) and νt(B) := E ˆZt∼ˆσs
dF(µt, νt) ≤ δt.

t (ht)[ ˆPt(B| ˆZt, at)]; then,

Similar to Theorem 9, we then have the following result.
t , ˆPt, ˆrt}T
t=1 as in Theorem 9. Then, we have the

Theorem 15 Given a stochastic AIS generator {ˆσs
t=1 and action-value functions { ˆQt : ˆZt × A → R}T
R}T
following:

t=1, deﬁne value functions { ˆVt : ˆZt →

1. Value function approximation: For any time t, realization ht of Ht, and choice

at of At we have

|Qt(ht, at) − E ˆZt∼ˆσs

t (ht)[ ˆQt( ˆZt, at)]| ≤ αt

and

|Vt(ht) − E ˆZt∼ˆσs

t (ht)[ ˆVt( ˆZt)]| ≤ αt,
(23)

where αt is deﬁned as in Theorem 9.

22

Approximate information state

2. Approximately optimal policy: Let ˆπ = (ˆπ1, . . . , ˆπT ), where ˆπt : ˆZt → ∆(A), be a

stochastic policy that satisﬁes

Supp(ˆπ(ˆzt)) ⊆ arg max
at∈A

ˆQt(ˆzt, at).

(24)

Deﬁne policy π = (π1, . . . , πT ), where πt : Ht → ∆(A) by πt(ht) = E ˆZt∼ˆσs
Then, for any time t, realization ht of Ht, and choice at of At, we have

t (ht)[ˆπt( ˆZt)].

|Qt(ht, at) − Qπ

t (ht, at)| ≤ 2αt

and

|Vt(ht) − V π

t (ht)| ≤ 2αt.

(25)

Proof The proof is almost the same as the proof of Theorem 9. The main diﬀerence is that
for the value and action-value functions of the stochastic approximation state, we take an
additional expectation over the realization of the stochastic AIS. We only show the details
of the proof of the ﬁrst part of the result (value approximation). The second part (policy
approximation) follows along similar lines.

Eq. (23) holds at T + 1 by deﬁnition. This forms the basis of induction. Assume that (23)

holds at time t + 1 and consider the system at time t. We have that

(cid:12)
(cid:12)Qt(ht, at) − E ˆZt∼ˆσs

t (ht)[ ˆQt( ˆZt, at)](cid:12)
(cid:12)

(a)
≤ (cid:12)

(cid:12)E[Rt | Ht = ht, At = at] − E ˆZt∼ˆσs
+ E(cid:2)(cid:12)
(cid:12)Vt+1(Ht+1) − E ˆZt+1∼ˆσs
(cid:12)
(cid:12)
E[ ˆVt+1(ˆσt+1(Ht+1)) | Ht = ht, At = at] − E ˆZt∼ˆσs
(cid:12)
(cid:12)

t (ht)[ˆrt( ˆZt, at)](cid:12)
(cid:12)
(cid:12)
t+1(ht+1)[ ˆVt+1( ˆZt+1)](cid:12)
(cid:12) Ht = ht, At = at
(cid:12)

t (ht)

(cid:20)(cid:90)

+

(cid:3)

ˆZt

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆZt, at)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(b)
≤ εt + αt+1 + ρF( ˆVt+1)δt = αt

where (a) follows from triangle inequality and (b) follows from (AP1), the induction hy-
pothesis, (AP2) and (10). This proves the ﬁrst part of (23). The second part follows
from

(cid:12)
(cid:12)Vt(ht) − ˆVt(ˆσs

t (ht))(cid:12)
(cid:12)

(a)
≤ max
at∈A

(cid:12)
(cid:12)Qt(ht, at) − ˆQt(ˆσs

t (ht), at)(cid:12)

(cid:12) ≤ αt,

where (a) follows from the inequality max f (x) ≤ max |f (x) − g(x)| + max g(x). This com-
pletes the proof of value approximation. The proof of policy approximation is similar to
that of Theorem 9 adapted in the same manner as above.

3.6 AIS with action compression

So far we have assumed that the action space for the AIS is the same as the action space for
the original model. In some instances, for example, for continuous or large action spaces, it
may be desirable to quantize or compress the actions as well. In this section, we generalize
the notion of AIS to account for action compression.

23

Subramanian, Sinha, Seraj, and Mahajan

Deﬁnition 16 As in the deﬁnition of AIS, suppose {ˆZt}T
t=1 are pre-speciﬁed collection of
Banach spaces, F be a function class for IPMs, and {(εt, δt)}T
t=1 be pre-speciﬁed positive
real numbers. In addition, suppose we have a subset ˆA ⊂ A of quantized actions. Then, a
collection {ˆσt : Ht → ˆZt}T
t=1 of history compression functions, along with action quantization
function ψ : A → ˆA, approximate update kernels { ˆPt : ˆZt × ˆA → ∆(ˆZt+1)}T
t=1 and reward
approximation functions {ˆrt : ˆZt × ˆA → R}T
t=1-action-quantized AIS
generator if the process { ˆZt}T

t=1, where ˆZt = ˆσt(Ht), satisﬁes the following properties:

t=1, is called an {(εt, δt)}T

(AQ1) Suﬃcient for approximate performance evaluation, i.e., for any time t, any

realization ht of Ht and any choice at of At, we have

(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), ψ(at))(cid:12)
(cid:12)

(cid:12) ≤ εt.

(AQ2) Suﬃcient to predict itself approximately. i.e., for any time t, any realization
ht of Ht, any choice at of At, and for any Borel subset B of ˆZt+1, deﬁne µt(B) :=
P( ˆZt+1 ∈ B | Ht = ht, At = at) and νt(B) := ˆPt(B | ˆσt(ht), ψ(at)); then,

dF(µt, νt) ≤ δt.

Similar to Theorem 9, we show that an action-quantized AIS can be used to determine

an approximately optimal policy.

Theorem 17 Suppose {ˆσt, ψ, ˆPt, ˆrt}T
deﬁne approximate action-value functions { ˆQt : ˆZt×ˆA → R}T
t=1 as follows: ˆVT +1(ˆzT +1) := 0 and for t ∈ {T, . . . , 1}:
R}T

t=1 is an action-quantized AIS generator. Recursively
t=1 and value functions { ˆVt : ˆZt →

ˆQt(ˆzt, ˆat) := ˆrt(ˆzt, ˆat) +

(cid:90)

ˆZt
ˆQt(ˆzt, ˆat).

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆzt, ˆat),

(26a)

(26b)

ˆVt(ˆzt) := max
ˆat∈ˆA

Then, we have the following:

1. Value function approximation: For any time t, realization ht of Ht, and choice

at of At, we have

|Qt(ht, at) − ˆQt(ˆσt(ht), ψ(at))| ≤ αt

and

|Vt(ht) − ˆVt(ˆσt(ht))| ≤ αt,

(27)

where αt is deﬁned as in Theorem 9.

2. Approximately optimal policy: Let ˆπ = (ˆπ1, . . . , ˆπT ), where ˆπt : ˆZt → ∆(ˆA), be a

stochastic policy that satisﬁes

Supp(ˆπt(ˆzt)) ⊆ arg max
ˆat∈ˆA

ˆQt(ˆzt, ˆat).

(28)

Deﬁne policy π = (π1, . . . , πT ), where πt : Ht → ∆(A) by πt := ˆπt ◦ ˆσt. Then, for any
time t, realization ht of Ht, and choice at of At, we have

|Qt(ht, at) − Qπ

t (ht, ψ(at))| ≤ 2αt

and

|Vt(ht) − V π

t (ht)| ≤ 2αt.

(29)

24

Approximate information state

Proof The proof is similar to the proof of Theorem 9. We only show the details of the ﬁrst
part (value approximation). The second part (policy approximation) follows along similar
lines.

As before, we prove the result by backward induction. Eq. (27) holds at T + 1 by
deﬁnition. This forms the basis of induction. Assume that (27) holds at time t + 1 and
consider the system at time t. We have that
(cid:12)Qt(ht, at) − ˆQt(ˆσt(ht), ψ(at))(cid:12)
(cid:12)
(cid:12)

(a)
≤ (cid:12)

(cid:12)E[Rt | Ht = ht, At = at] − ˆrt(ˆσt(ht), ψ(at))(cid:12)
(cid:12)
(cid:12)Vt+1(Ht+1) − ˆVt+1(ˆσt+1(Ht+1))(cid:12)
+ E(cid:2)(cid:12)
(cid:12)
(cid:12)
(cid:12)
E[ ˆVt+1(ˆσt+1(Ht+1)) | Ht = ht, At = at] −
(cid:12)
(cid:12)

+

(cid:90)

ˆZt

(cid:12)
(cid:12) Ht = ht, At = at

(cid:3)

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆσt(ht), ˆat)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(b)
≤ εt + αt+1 + ρF( ˆVt+1)δt = αt

where (a) follows from triangle inequality and (b) follows from (AQ1), the induction hy-
pothesis, (AQ2) and (10). This proves the ﬁrst part of (27). The second part follows
from

(cid:12)Vt(ht) − ˆVt(ˆσt(ht))(cid:12)
(cid:12)
(cid:12)

(cid:12)Qt(ht, at) − ˆQt(ˆσt(ht), ψ(at))(cid:12)
(cid:12)

(cid:12) ≤ αt,

(a)
≤ max
at∈A

where (a) follows from the inequality max f (x) ≤ max |f (x) − g(x)| + max g(x). We have also
ˆQt(ˆzt, ˆat) = maxa∈A ˆQt(ˆzt, ψ(at)).
used the fact that if ψ is an onto function, then maxˆa∈ˆA
This completes the proof of value approximation. The proof of policy approximation is
similar to that of Theorem 9 adapted in the same manner as above.

Action quantization in POMDPs with ﬁnite or Borel valued unobserved state was
investigated in Saldi et al. (2018), who showed that under appropriate technical conditions
the value function and optimal policies for the quantized model converge to the value
function and optimal policy of the true model. However Saldi et al. (2018) did not provide
approximation error for a ﬁxed quantization level.

Simpliﬁcation for perfectly observed case: The approximation bounds for action
compression derived in Theorem 17 can be simpliﬁed when the system is perfectly observed.
In particular, consider an MDP with state space S, action space A, transition probability
P : S × A → ∆(S), per-step reward function r : S × A → R, and discount factor γ.

For MDPs, we can simplify the deﬁnition of action quantized AIS-generator as follows.

Deﬁnition 18 Given an MDP as deﬁned above, let F be a function class for IPMs, and
(ε, δ) be pre-speciﬁed positive real numbers. In addition, suppose we have a subset ˆA ⊂ A of
quantized actions. Then, an action quantization function ψ : A → ˆA, where ˆA ⊂ A, is called
an (ε, δ)-action-quantizer if the following properties are satisﬁed:

(AQM1) Suﬃcient for approximate performance evaluation, i.e., for any s ∈ S

and a ∈ A, we have

(cid:12)
(cid:12)r(s, a) − r(s, ψ(a))(cid:12)

(cid:12) ≤ ε.

25

Subramanian, Sinha, Seraj, and Mahajan

(AQM2) Suﬃcient to predict the next state approximately.

i.e., for any s ∈ S

and a ∈ A,

dF(P (·|s, a), P (·|s, ψ(a)) ≤ δ.

Then, the approximation in Theorem 17 simpliﬁes for an MDP as follows.

Corollary 19 Suppose ψ is an (ε, δ)-action-quantizer. Recursively deﬁne approximate
action-value functions { ˆQt : S × ˆA → R} and value functions { ˆVt : St → R} as follows:
ˆVT +1(sT +1) := 0 and for t ∈ {T, . . . , 1}:

ˆQt(st, ˆat) := r(st, ˆat) +

(cid:90)

S

ˆVt+1(st+1)P (dst+1 | st, ˆat),

ˆVt(st) := max
ˆat∈ˆA

ˆQt(st, ˆat).

(30a)

(30b)

Then, we have the following:

1. Value function approximation: For any time t, s ∈ S and a ∈ A, we have

|Qt(s, a) − ˆQt(s, ψ(a))| ≤ αt

and

|Vt(s) − ˆVt(s)| ≤ αt,

(31)

where αt is deﬁned as in Theorem 9.

2. Approximately optimal policy: Let ˆπ = (ˆπ1, . . . , ˆπT ), where ˆπt : S → ∆(ˆA), be a

stochastic policy that satisﬁes

Supp(ˆπt(st)) ⊆ arg max
ˆat∈ˆA

ˆQt(st, ˆat).

(32)

Since V ˆπ

t (st) = ˆVt(st) and Qˆπ

t (s, ˆat) = ˆQt(s, ˆat), we have

|Qt(st, at) − Qˆπ(st, ψ(at))| ≤ αt

and

|Vt(st) − V ˆπ(st)| ≤ αt.

(33)

Proof The proof follows in a straightforward manner from the proof of Theorem 17.

Note that in contrast to Theorem 17, the ﬁnal approximation bounds (33) in Corollary 19
do not have an additional factor of 2. This is because the approximate policy ˆπ can be
directly executed in the original MDP because ˆA ⊂ A.

Approximation bounds similar to Corollary 19 are used to derive bounds for lifelong
learning in Chandak et al. (2020). We show that similar bounds may be obtained using
Corollary 19 in Appendix D.

3.7 AIS with observation compression

In applications with high-dimensional observations such as video input, it is desirable to
pre-process the video frames into a low-dimensional representation before passing them on
to a planning or learning algorithm. In this section, we generalize the notion of AIS to
account for such observation compression.

26

Approximate information state

Deﬁnition 20 As in the deﬁnition of AIS, suppose {ˆZt}T
t=1 are a pre-speciﬁed collection
of Banach spaces, F be a function class for IPMs, and {(εt, δt)}T
t=1 be pre-speciﬁed positive
In addition, suppose we have a set ˆY of compressed observations and a
real numbers.
compression function q : Y → ˆY. Let ˆHt denote the history ( ˆY1:t−1, A1:t−1) of compressed
observations and actions and ˆHt denote the space of realizations of such compressed histories.
Then, a collection {ˆσt : ˆHt → ˆZt}T
t=1 of history compression functions, along with observation
compression function q : Y → ˆY, approximate update kernels { ˆPt : ˆZt × A → ∆(ˆZt+1)}T
t=1 and
reward approximation functions {ˆrt : ˆZt × A → R}T
t=1-observation-
t=1, where ˆZt = ˆσt( ˆHt), satisﬁes properties
compressed AIS generator if the process { ˆZt}T
(AP1) and (AP2).

t=1, is called an {(εt, δt)}T

Stochastic input Wt

Controlled input At

System

Obs.
Comp.

Compressed Obs. Yt

Reward Rt

Figure 3: A stochastic input-output system with observation compression

Modiﬁed input-output system

In essence, we can view observation compression as a new input-output system whose
outputs are ( ˆYt, Rt) instead of (Yt, Rt) as shown in Fig. 3. A construction similar to
observation-compressed AIS is proposed in Ha and Schmidhuber (2018), where it is shown that
such a construction performs well empirically, but there was no analysis of the approximation
guarantees of such a construction.

An immediate implication of the above deﬁnition is the following:

Corollary 21 Let {ˆσt, q, ˆPt, ˆrt}t≥1 be an {(εt, δt)}T
the bounds of Theorem 9 hold.

t=1-observation-compression AIS. Then,

3.8 Discussion and related work

AIS may be viewed as a generalization of state discretization (Bertsekas, 1975) or state
aggregation (Whitt, 1978) in MDPs. As illustrated by the examples in Sec. 3.3, many of the
recent results on approximation bounds for state aggregation and latent state embedding in
MDPs are speciﬁc instances of AIS and, in some instances, using the approximation bounds
of Theorem 9 or its generalization to inﬁnite horizon (Theorem 27) provide tighter bounds
than those in the literature. A detailed comparison with these results is presented in the
Appendices. We had presented a simpler deﬁnition of AIS and the approximation bounds in
the preliminary version of this paper (Subramanian and Mahajan, 2019).

As mentioned in Sec. 2.5 while discussing the related literature on information states,
there are two other methods for identifying “states” for POMDPs: bisimulation-based
methods and predictive state representations (PSRs). Approximation techniques for both
these methods have been proposed in the literature.

State aggregation techniques based on bisimulation metrics have been proposed in Ferns
et al. (2004, 2011) for MDPs and Castro et al. (2009) for POMDPs. The key insight of these

27

Subramanian, Sinha, Seraj, and Mahajan

papers is to deﬁne a semi-metric called bisimulation metric on the state space of an MDP
or the belief space of a POMDP as the unique ﬁxed point of an operator on the space of
semi-metrics on the state space of the MDP or the belief space of the POMDP. It is then
shown that the value function is Lipschitz with respect to this metric. Then, they propose
state aggregation based on the bisimulation metric. Although the basic building blocks of
bisimulation metrics are the same as those of an AIS, the approximation philosophies are
diﬀerent. The bisimulation-metric based approximations are a form of state aggregation,
while AIS need not be a state aggregation.

Various methods for learning low dimensional approximations of PSRs have been proposed
in the literature, including approaches which use spectral learning algorithms (Rosencrantz
et al., 2004; Boots et al., 2011; Hamilton et al., 2014; Kulesza et al., 2015b,a; Jiang et al.,
2016), and stochastic gradient descent (Jiang et al., 2016). Error bounds for using an
approximate PSR were derived in Wolfe et al. (2008); Hamilton et al. (2014). These
approximation methods for PSRs rely on the speciﬁc structure of PSRs and are conceptually
diﬀerent from the approximation methods used in AIS.

4. Inﬁnite-horizon discounted reward setup

So far, we have restricted attention to the ﬁnite horizon setup. In this section, we show
how to generalize the notions of information state and approximate information state to the
inﬁnite horizon discounted reward setup.

4.1 System model and problem formulation

We consider the same model as described in Sec. 2.1 but assume that the system runs for an
inﬁnite horizon. The performance of any (history dependent and possibly stochastic) policy
π := (π1, π2, . . . ), where πt : Ht → ∆(A), is given by

J(π) := lim inf
T →∞

(cid:20) T

(cid:88)

Eπ

γt−1Rt

(cid:21)
,

t=1

where γ ∈ (0, 1) is the discount factor. As before, we assume that the agent knows the
system dynamics {ft}t≥1, the reward functions {rt}t≥1, and the probability measure P on
the primitive random variables {Wt}t≥1. The objective of the agent is to choose a policy π
that maximizes the expected discounted total reward J(π).

Note that we use lim inf rather than lim in the above deﬁnition because in general
the limit might not exist. We later assume that the rewards are uniformly bounded (see
Assumption 1) which, together with the ﬁniteness of the action space, implies that the limit is
well deﬁned. When the action space is uncountable, we need to impose appropriate technical
conditions on the model to ensure that an appropriate measurable selection condition holds
(Hern´andez-Lerma and Lasserre, 2012).

4.2 A dynamic programming decomposition

In the ﬁnite-horizon setup, we started with a dynamic program to evaluate the performance
{V π

t=1 for any history dependent policy π. We then identiﬁed an upper-bound {Vt}T

t }T

t=1

28

Approximate information state

t }T

on {V π
t=1 and showed that this upper bound is tight and achieved by any optimal policy.
The subsequent analysis of the information state and the approximate information state
based dynamic programs was based on comparison with {Vt}T

t=1.

One conceptual diﬃculty with the inﬁnite horizon setup is that we cannot write a general
dynamic program to evaluate the performance {V π
t }t≥1 of an arbitrary history dependent
policy π and therefore identify a tight upper-bound {Vt}t≥1. In traditional MDP models,
this conceptual diﬃculty is resolved by restricting attention to Markov strategies and then
establishing that the performance of a Markov strategy can be evaluated by solving a ﬁxed
point equation. For partially observed MDPs, a similar resolution works because one can
view the belief state as an information state. However, for general partially observed models
as considered in this paper, there is no general methodology to identify a time-homogeneous
information state. So, we follow a diﬀerent approach and identify a dynamic program which
bounds the performance of a general history dependent policy. We impose the following
mild assumption on the model.

Assumption 1 The reward process {Rt}t≥1 is uniformly bounded and takes values inside a
ﬁnite interval [Rmin, Rmax].

Given any (history dependent) policy π, we deﬁne the reward-to-go function for any

time t and any realization ht of Ht as

t (ht) := Eπ
V π

(cid:20) ∞
(cid:88)

s=t

γs−tRs

(cid:21)

Ht = ht

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Deﬁne the corresponding action value function as:

Qπ

t (ht, at) := Eπ[Rt + γV π

t+1(Ht+1) | Ht = ht, At = at].

(34)

(35)

As stated above, we cannot identify a dynamic program to recursively compute {V π
t }t≥1.
Nonetheless, we show that under Assumption 1 we can identify arbitrarily precise upper
and lower bounds for {V π
t }t≥1 which can be recursively computed.

Proposition 22 Arbitrarily pick a horizon T and deﬁne {J π
J π
T,T (hT ) = 0 and for t ∈ {T − 2, . . . , 1},

t,T : Ht → R}T

t=1 as follows:

t,T (ht) := Eπ[Rt + γJ π
J π

t+1,T (Ht+1) | Ht = ht].

Then, for any time t ∈ {1, . . . , T } and realization ht of Ht, we have

J π
t,T (ht) +

γT −t
1 − γ

Rmin ≤ V π

t (ht) ≤ J π

t,T (ht) +

γT −t
1 − γ

Rmax.

(36)

(37)

Proof The proof follows from backward induction. Note that for t = T , Rt ∈ [Rmin, Rmax]
implies that

Rmin
1 − γ

≤ V π

T (hT ) ≤

Rmax
1 − γ

.

29

Subramanian, Sinha, Seraj, and Mahajan

This forms the basis of induction. Now asusme that (37) holds for time t + 1 and consider
the model for time t:

t (ht) = Eπ
V π

(cid:20) ∞
(cid:88)

γs−tRs

Ht = ht

(cid:21)

s=t
(cid:20)
Rt + γEπ

(a)
= Eπ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:20) ∞
(cid:88)

s=t+1

γs−(t+1)Rs

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ht+1

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

Ht = ht

(cid:20)

(b)
≤ Eπ

Rt + γEπ

(cid:20)

J π
t+1,T (Ht+1) +

γT −(t+1)
1 − γ

Rmax

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ht+1

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

Ht = ht

(c)
= J π

t,T (ht) +

γT −t
1 − γ

Rmax,

where (a) follows from the smoothing property of conditional expectation, (b) follows from
the induction hypothesis, and (c) follows from the deﬁnition of J π
t,T (·). This establishes one
side of (37). The other side can be established in a similar manner. Therefore, the result
holds by the principle of induction.

Note that Proposition 22 gives a recursive method to approximately evaluate the perfor-
mance of any history dependent policy π. We can modify the recursion in (36) to obtain
policy independent upper bound on performance of an arbitrary policy. For that matter,
deﬁne value functions {Vt : Ht → R}t≥1as follows:

Vt(ht) = sup
π

V π
t (ht),

(38)

where the supremum is over all history dependent policies. Furthermore, deﬁne action-value
functions {Qt : Ht × A → R}t≥1 as follows:

Qt(ht, at) = E[Rt + γVt+1(Ht+1) | Ht = ht, At = at].

(39)

Then, we have the following.

Proposition 23 Arbitrarily pick a horizon T and deﬁne {Jt,T : Ht → R} as follows:
JT,T (hT ) = 0 and for t ∈ {T − 2, . . . , 1},

Jt,T (ht) := max
at∈A

E[Rt + γJt+1(Ht+1) | Ht = ht, At = at].

(40)

Then, for any time t ∈ {1, . . . , T } and realization ht of Ht,

V π
t (ht) ≤ Jt,T (ht) +

γT −t
1 − γ

Rmax.

Therefore,

Jt,T (ht) +

γT −t
1 − γ

Rmin ≤ Vt(ht) ≤ Jt,T (ht) +

γT −t
1 − γ

Rmax.

30

(41)

(42)

Approximate information state

Note that Jt,T (ht) is the optimal value function for a ﬁnite horizon system with the discounted
reward criterion that runs for horizon T − 1.
Proof By following almost the same argument as Proposition 2, we can establish that for
any history dependent policy π, J π

t,T (ht) ≤ Jt,T (ht), which immediately implies (41).

Maximizing the left hand side of (41) gives us the upper bound in (42). For the lower

bound in (42), observe that

Vt(ht) = sup
π

(cid:20) ∞
(cid:88)

Eπ

γs−tRs

(cid:21)

Ht = ht

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s=t
(cid:20)T −1
(cid:88)

(a)
≥ sup

Eπ

π

= sup

Eπ

π

s=t
(cid:20)T −1
(cid:88)

s=t

γs−tRs +

∞
(cid:88)

s=T

γs−tRmin

(cid:21)

Ht = ht

(cid:12)
(cid:12)
(cid:12)
(cid:12)

γs−tRs

(cid:21)

Ht = ht

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

γT −t
1 − γ

Rmin

(b)
= Jt,T (ht) +

γT −t
1 − γ

Rmin.

where (a) follows from the fact that Rs ≥ Rmin and (b) follows from the deﬁnition of Jt,T (ht).
This complete the proof of (42).

4.3 Time-homogeneous information state and simpliﬁed dynamic program

Deﬁnition 24 Given a Banach space Z, an information state generator {σt : Ht → Z} is
said to be time-homogeneous if, in addition to (P1) and (P2), it satisﬁes the following:

(S) The expectation E[Rt|Zt = σt(Ht), At = at] and the transition kernel P(Zt+1 ∈ B|Zt =

σt(Ht), At = at) are time-homogeneous.

Note that all except the ﬁrst example of information state presented in Sec. 2.4 are
time-homogeneous. However, in general, a time-homogeneous information state may not
exist for all partially observed models and it is important to understand conditions under
which such an information state exists. However, we do not pursue that direction in this
paper.

For any time-homogeneous information state, deﬁne the Bellman operator B : [Z → R] →

[Z → R] as follows: for any uniformly bounded function ¯V : Z → R

[B ¯V ](z) = max
a∈A

E[Rt + γ ¯V (Zt+1) | Zt = z, At = a],

(43)

where γ ∈ (0, 1) is the discount factor. Because of (S), the expectation on the right hand
side does not depend on time. Due to discounting, the operator B is a contraction and
therefore, under Assumption 1, the ﬁxed point equation

has a unique bounded solution (due to the Banach ﬁxed point theorem). Let ¯V ∗ be the
ﬁxed point and π∗ be any policy such that π∗(z) achieves the arg max in the right hand side

¯V = B ¯V

(44)

31

Subramanian, Sinha, Seraj, and Mahajan

of (43) for [B ¯V ∗](z). It is easy to see that ¯V ∗ is the performance of the time homogeneous
policy (π∗, π∗, . . . ). However, it is not obvious that ¯V ∗ equals to the optimal performance
V1 deﬁned in (38), because the proof of Theorem 5 relies on backward induction and is not
applicable to inﬁnite horizon models. So, we present an alternative proof below which uses
the performance bounds of Proposition 23.

Theorem 25 Let {Zt}t≥1 be a time-homogeneous information state process with generator
{σt : Ht → Z}t≥1. Suppose Assumption 1 holds and let ¯V ∗ be the unique bounded ﬁxed point
of (43). Then, for any time t and realization ht of Ht, we have

Vt(ht) = ¯V ∗(σt(ht)).

Furthermore, let π∗ : Z → ∆(A) be a time-homogeneous (stochastic) policy such that
Supp(π∗(z)) is a subset of the arg max of the right hand side of (43). Then, the time-
homogeneous policy π∗ := (π∗, π∗, . . . ) is optimal.

Proof Consider the following sequence of value functions: ¯V (0)(z) = 0 and for n ≥ 0, deﬁne
¯V (n+1) = B ¯V (n). Now ﬁx a horizon T and consider the ﬁnite-horizon discounted reward
problem of horizon T − 1. As argued earlier, Jt,T (ht) is the optimal value-function for this
ﬁnite horizon discounted problem. Moreover, note that {Zt}T
t=1 is an information state for
this ﬁnite horizon discounted problem. Therefore, from using the result of Theorem 5, we
get that for any time t ∈ {1, . . . , T }, and realization ht of Ht,

Jt,T (ht) = ¯V (T −t)(σt(ht)).

Substituting (42) from Proposition 23 in the above, we get

¯V (T −t)(σt(ht)) +

γT −t
1 − γ

Rmin ≤ Vt(ht) ≤ ¯V (T −t)(σt(ht)) +

γT −t
1 − γ

Rmax.

The result follows from taking limit T → ∞ and observing that ¯V (T −t)(z) converges to
¯V ∗(z).

4.4 Time-homogeneous AIS and approximate dynamic programming
Deﬁnition 26 Given a Banach space ˆZ, a function class F for IPMs, and positive real
numbers (ε, δ), we say that a collection {ˆσt : Ht → ˆZ}t≥1 along with a time-homogeneous
update kernel ˆP : ˆZ × A → ∆(ˆZ) and a time-homogeneous reward approximation function
ˆr : ˆZ × A → R is a (ε, δ) time homogeneous AIS generator if the process { ˆZt}t≥1, where
ˆZt = ˆσt(Ht), satisﬁes (AP1) and (AP2) where ˆrt, ˆPt, εt and δt in the deﬁnition of (AP1)
and (AP2) are replaced by their time-homogeneous counterparts.

For any time-homogeneous AIS, deﬁne the approximate Bellman operator ˆB : [ˆZ → R] →

[ˆZ → R] as follows: for any uniformly bounded function ˆV : ˆZ → R,

(cid:26)

[ ˆB ˆV ](ˆz) = max
a∈A

ˆr(ˆz, a) + γ

(cid:90)

ˆZ

ˆV (ˆz(cid:48)) ˆP (dˆz(cid:48)|ˆz, a)

(cid:27)

.

(45)

32

Approximate information state

Note that the expectation on the right hand side does not depend on time. Due to discounting,
the operator ˆB is a contraction, and therefore, under Assumption 1, the ﬁxed point equation

ˆV = ˆB ˆV

(46)

has a unique bounded solution (due to the Banach ﬁxed point theorem). Let ˆV ∗ be the
ﬁxed point and ˆπ∗ be any policy such that ˆπ∗(ˆz) achieves the arg max in the right hand side
of (45) for [ ˆB ˆV ∗](ˆz). It is not immediately clear if ˆV ∗ is close to the performance of policy
π = (π1, π2, . . . ), where πt = π∗ ◦ ˆσt, or if ˆV ∗ is close to the optimal performance. The proof
of Theorem 9 relies on backward induction and is not immediately applicable to the inﬁnite
horizon setup. Nonetheless, we establish results similar to Theorem 9 by following the proof
idea of Theorem 25.

Theorem 27 Suppose ({ˆσt}t≥1, ˆP , ˆr) is a time-homogeneous (ε, δ)-AIS generator. Consider
the ﬁxed point equation (46), which we rewrite as follows:

ˆQ(ˆz, a) := ˆr(ˆz, a) + γ

(cid:90)

ˆV (ˆz) := max
a∈A

ˆZ
ˆQ(ˆz, a).

ˆV (ˆz(cid:48)) ˆP (dˆz(cid:48)|ˆz, a),

(47a)

(47b)

Let ˆV ∗ denote the ﬁxed point of (47) and ˆQ∗ denote the corresponding action-value function.
Then, we have the following:

1. Value function approximation: For any time t, realization ht of Ht, and choice

at of At, we have

|Qt(ht, at) − ˆQ∗(ˆσt(ht), at)| ≤ α and

|Vt(ht) − ˆV ∗(ˆσt(ht))| ≤ α,

(48)

where

α =

ε + γρF( ˆV ∗)δ
1 − γ

2. Approximately optimal policy: Let ˆπ∗ : ˆZ → ∆(A) be a stochastic policy that

satisﬁes

Supp(ˆπ∗(ˆz)) ⊆ arg max
a∈A

ˆQ∗(ˆz, a).

(49)

Deﬁne policy π = (π1, π2, . . . ), where πt : Ht → ∆(A) is deﬁned by πt := ˆπ∗ ◦ ˆσt. Then,
for any time t, realization ht of Ht, and choice at of At, we have

|Qt(ht, at) − Qπ

t (ht, at)| ≤ 2α and

|Vt(ht) − V π

t (ht)| ≤ 2α.

(50)

Proof The proof follows by combining ideas from Theorem 9 and 25. We provide a detailed
proof of the value approximation. The proof argument for policy approximation is similar.
Consider the following sequence of value functions: ˆV (0)(ˆz) = 0 and for n ≥ 0, deﬁne
ˆV (n+1) = ˆB ˆV (n). Now ﬁx a horizon T and consider the ﬁnite-horizon discounted reward
problem of horizon T − 1. As argued earlier, Jt,T (ht) is the optimal value-function for this
ﬁnite horizon discounted problem. Moreover, note that { ˆZt}T
t=1 is an (ε, δ)-AIS for this ﬁnite

33

Subramanian, Sinha, Seraj, and Mahajan

horizon discounted problem. Therefore, from using the result of Theorem 9, we get that for
any time t ∈ {1, . . . , T }, and realization ht of Ht,

where

|Jt,T (ht) − ˆV (T −t)(ˆσt(ht))| ≤ αt,

αt = ε +

T −1
(cid:88)

τ =t+1

γτ −t(cid:2)ρF( ˆV (T −τ ))δ + ε(cid:3).

Substituting (42) from Proposition 23 in the above, we get that

ˆV (T −t)(ˆσt(ht)) − αt +

γT −t
1 − γ

Rmin ≤ Vt(ht) ≤ ˆV (T −t)(ˆσt(ht)) + αt +

γT −t
1 − γ

Rmax.

Since ˆB is a contraction, from the Banach ﬁxed point theorem we know that limT →∞ ˆV (T −t) =
ˆV ∗. Therefore, by continuity of ρF(·), we have limT →∞ ρF( ˆV T −t) = ρF( ˆV ∗). Consequently,
limT →∞ αt = α. Therefore, taking the limit T → ∞ in the above equation, we get

ˆV ∗(ˆσt(ht)) − α ≤ Vt(ht) ≤ ˆV ∗(ˆσt(ht)) + α,

which establishes the bound on the value function in (48). The bound on the action-value
function in (48) follows from a similar argument.

Theorem 27 shows how the result of Theorem 9 generalizes to inﬁnite horizon. We can
similarly extend the results for approximate policy evaluation (as in Sec. 3.4), the stochastic
AIS case (as in Sec. 3.5), the action compression case (as in Sec. 3.6), and the observation
compression case (as in Sec. 3.7).

5. An AIS-based approximate dynamic programming for Dec-POMDPs

The theory of approximation for partially observed systems presented in the previous section
is fairly general and is applicable to other models of decision making as well. As an example,
in this section we show how to use the same ideas to obtain approximation results for
decentralized (i.e., multi-agent) partially observed models.

There is a rich history of research on these models in multiple research disciplines.
Decentralized multi-agent systems have been studied in Economics and Organizational
Behavior since the mid 1950s (Marschak, 1954; Radner, 1962; Marschak and Radner, 1972)
under the heading of team theory. Such models have been studied in systems and control
since the mid 1960s under the heading of decentralized stochastic control (Witsenhausen,
1968, 1971; Sandell et al., 1978). Such models have also been studied in Artiﬁcial Intelligence
since the 2000s (Bernstein et al., 2005; Szer et al., 2005; Seuken and Zilberstein, 2007; Carlin
and Zilberstein, 2008) under the heading of Dec-POMDPs. In the interest of space, we do not
provide a detailed overview of this rich area; instead we refer the reader to the comprehensive
survey articles of Mahajan et al. (2012); Liu et al. (2016) for a detailed overview from the
perspective of Systems and Control and Artiﬁcial Intelligence, respectively.

We brieﬂy state the facts about this literature which are pertinent to the discussion
below. The general Dec-POMDP problem is NEXP complete (Bernstein et al., 2002),

34

Approximate information state

so it is not possible to derive an eﬃcient algorithm to compute the optimal solution.
Nonetheless, considerable progress has been made in identifying special cases where a
dynamic programming decomposition is possible (Walrand and Varaiya, 1983; Aicardi et al.,
1987; Ooi et al., 1997; Mahajan and Teneketzis, 2009a,b; Mahajan et al., 2008; Nayyar, 2011;
Nayyar et al., 2013; Mahajan, 2013; Arabneydi and Mahajan, 2014; Oliehoek and Amato,
2015; Dibangoye et al., 2016; Boularias and Chaib-Draa, 2008; Kumar and Zilberstein,
2009). A high level approach which encapsulates many of these special cases is the common
information approach of Nayyar et al. (2013) which shows that the Dec-POMDP problem
with a speciﬁc but relatively general information structure can be converted into a single
agent, partially observed problem from the point of view of a virtual agent which knows the
information commonly known to all agents and chooses prescriptions (or partially evaluated
policies) which map the local information at each agent to their respective actions. We
summarize these results in the next subsection and then show how we can identify an AIS
for such models.

5.1 Model of a Dec-POMDP

A Dec-POMDP is a tuple (cid:104)K, S, (Yk)k∈K, (Ak

t )k∈K, P1, P, P y, r(cid:105) where

• K = {1, . . . , K} is the set of agents.

• S is the state space. Yk, Ak, k ∈ K, are the observation and action spaces of agent k.
t )k∈K ∈ Y, and

k∈K Ak. We use St ∈ S, Yt := (Y k

k∈K Yk and A = (cid:81)

t )k∈K ∈ A, to denote the system state, observations, and actions at time t.

Let Y = (cid:81)
At := (Ak

• P1 ∈ ∆(S) is the initial distribution of the initial state S1.

• P : S × A → ∆(S) denotes the transition probability of the system, i.e.,

P(St+1 = st+1 | S1:t = s1:t, A1:t = a1:t) = P(St+1 = st+1 | St = st, At = at)

= P (st+1|st, at).

• P y : S × A → ∆(Y) denotes the observation probability of the system, i..e,

P(Yt = yt | S1:t = s1:t, A1:t−1 = a1:t−1) = P(Yt = yt | St = st, At−1 = at−1)

= P y(yt|st, at−1).

• r : S × A × S → R denotes the per-step reward function. The team receives a reward

Rt = r(St, At, St+1) at time t.

Information structure: A critical feature of a Dec-POMDP is the information structure
which captures the knowledge of who knows what about the system and when. We use I k
t
to denote the information known to agent k at time t. In general, I k
is a subset of the total
t
information (Y1:t, A1:t−1, R1:t−1) known to all agents in the system. We use Ik
t to denote the
space of the information available to agent k at time t. Note that, in general, the information
available to agent k increases with time. So, Ik
t are sets that are increasing with time. Some
examples of information structures are:

35

Subramanian, Sinha, Seraj, and Mahajan

• Delayed sharing: I k

t = {Y1:t−d, A1:t−d, Y k

t−d+1:t−1}. This models systems
where agents broadcast their information and communication has delay of d. Planning
for models where d = 1 has been considered in Sandell and Athans (1974); Yoshikawa
(1975) and for general d has been considered in Nayyar et al. (2011).

t−d+1:t, Ak

• Periodic sharing: I k

(cid:5). This
models systems where agents periodically broadcast their information every p steps.
Planning for this model has been considered in Ooi et al. (1997).

t−τ +1:t−1}, where τ = p(cid:4) t

t = {Y1:t−τ , A1:t−τ , Y k

t−τ +1:t, Ak

p

t = {Y k

• Control sharing: I k

1:t, A1:t−1}. This models systems where control actions
are observed by everyone (which is the case for certain communication and economic
applications). Planning for variations of this model has been considered in Bismut
(1972); Sandell and Athans (1974); Mahajan (2013).

• Mean-ﬁeld sharing: I k

t , . . . , SK
1:t, Ak
t = {Sk
t ),
(cid:1)/K denotes the empirical
t , and Mt = (cid:0)(cid:80)
the observation of agent k is Sk
distribution of the states. This models systems where mean-ﬁeld is observed by all
agents (which is the case for smart grid and other large-scale systems). Planning for
variations of this model has been considered in Arabneydi and Mahajan (2014).

1:t−1, M1:t}, where the state St is (S1

k∈K δSk

t

Policy: The policy of agent k is a collection πk = (πk
t → ∆(Ai). We
use π = (πk)k∈K to denote the policy for all agents. The performance of a policy π is given
by

2 , . . . ), where πk

1 , πk

t : Ik

J(π) = Eπ

(cid:20) T

(cid:88)

(cid:21)
.

Rt

t=1

(51)

The objective is to ﬁnd a (possibly time-varying) policy π that maximizes the performance
J(π) deﬁned in (51).

5.2 Common information based planning for Dec-POMDPs

As mentioned earlier, in general, ﬁnding the optimal plan for multi-agent teams is NEXP-
complete (Bernstein et al., 2002). However, it is shown in Nayyar et al. (2013) that when the
information structure is of a particular form (known as partial history sharing), it is possible
to reduce the multi-agent planning problem to a single agent planning problem from the
point of view of a virtual agent called the coordinator. We summarize this approach below.

Common and local information: Deﬁne

Ct =

(cid:92)

(cid:92)

I k
s

s≥t

k∈K

and

t = I k
Lk

t \ Ct,

k ∈ K.

Ct denotes the common information, i.e., the information that is common to all agents all
the time in the future and Lk
t denotes the local information at agent k. By construction,
t = {Ct, Lk
I k
t and let
Lt = (Lk
t+1 = Ct+1 \ Ct denote
the new common information at time t. Then, Ct may be written as Cnew
1:t

t denote the space of realizations of Ct and Lk
t . By construction, Ct ⊆ Ct+1. Let Cnew

t }. Let Ct and Lk
k∈K Lk

t )k∈K and Lt = (cid:81)

.

36

Approximate information state

Deﬁnition 28 The information structure is called partial history sharing if for any Borel
subset B of Lk
t+1, we have

t+1 and any realization ct of Ct, (cid:96)k

t+1 of Y k

t and yk

t of Ak

t of Lk

t , ak

P(Lk

t+1 ∈ B | Ct = ct, Lk

t = (cid:96)k

t , Ak

t = ak

t , Y k

t+1 = yk
= P(Lk

t+1)
t+1 ∈ B | Lk

t = (cid:96)k

t , Ak

t = ak

t , Y k

t+1 = yk

t+1).

The main intuition behind this deﬁnition is as follows. For any system, the information
available to the agents can always be split into common and local information such that
I k
t = {Ct, Lk
t }. A partial history sharing information structure satisﬁes the property that at
any time t and for any agent k, the updated value Lk
t+1 of the local information is a function
of only the current local information Lk
t and the next local
observation Y k
t+1. Consequently, the common information Ct is not needed to keep track of
the update of the local information. This ensures that compressing the common information
into an information state or an approximate information state does not impact the update
of the local information.

t , the current local action Ak

Prescriptions: Given a policy π = (πk)k∈K and a realized trajectory (c1, c2, . . . ) of
the common information, the prescription ˆξk
t , i.e.,
t
ˆξk
is a function from Lk
t = πk
t )k∈K
and let X denote the space of all such prescriptions for time t.

is the partial application of ct to πk
t ). Let ˆξt denote ( ˆξk

t (ct, ·), k ∈ K. Note that ˆξk
t

t to ∆(Ak

The reason for constructing prescriptions is as follows. Prescriptions encode the informa-
tion about the policies of all agents needed to evaluate the conditional expected per-step
reward given the common information, i.e., E[Rt|Ct, (πk)k∈K] can be written as a function
of Ct and ( ˆξk
t )k∈K). This allows us to construct a virtual single-agent
optimization problem where a decision maker (which we call the virtual coordinator) observes
the common information Ct and chooses the prescriptions ( ˆξk
t )k∈K to maximize the sum of
rewards ˆrt(Ct, ( ˆξk
t )k∈K). The details of this virtual coordinated system are presented next.

t )k∈K, say ˆrt(Ct, ( ˆξk

A virtual coordinated system: The key idea of Nayyar et al. (2013) is to construct a
virtual single agent planning problem which they call a coordinated system. The environment
of the virtual coordinated system consists of two components: the ﬁrst component is the
same as the environment of the original multi-agent system which evolves according to
dynamics P ; the second component consists of K passive agents, whose operation we will
describe later. There is a virtual coordinator who observes the common information Ct and
chooses prescriptions ˆΞt = (ˆΞk
t : Lk → ∆(Ak) using a coordination rule ψt,
i.e., ˆΞt ∼ ψt(Ct). In general, the coordination rule can be stochastic. Let ˆξt denote the
realization of ˆΞt. Each agent in the virtual coordinated system is a passive agent and agent k
uses the prescription ˆΞk

t )k∈K, where ˆΞk

t to sample an action Ak

t ∼ ˆΞk

t (Lk

t ).

A key insight of Nayyar et al. (2013) is that the virtual coordinated system is equivalent

to the original multi-agent system in the following sense.

Theorem 29 (Nayyar et al. (2013)) Consider a Dec-POMDP with a partial history
sharing information structure. Then, for any policy π = (πk)k∈K, where πk = (πk
1 , . . . , πk
T )
for the Dec-POMDP, deﬁne a coordination policy ψ = (ψ1, . . . , ψT ) for the virtual coordinated
system given by ψt(ct) = (cid:0)πk
k∈K. Then, the performance of the virtual coordinated
system with policy ψ is the same as the performance of the Dec-POMDP with policy π.

t (ct, ·)(cid:1)

37

Subramanian, Sinha, Seraj, and Mahajan

Conversely, for any coordination policy ψ = (ψ1, . . . , ψT ) for the virtual coordinated
1 , . . . , πk
T ) for the Dec-POMDP given by
t ). Then, the performance of the Dec-POMDP with policy π is the same

system, deﬁne a policy π = (πk)k∈K with πk = (πk
t (ct, (cid:96)k
πk
as that of the virtual coordinated system with policy ψ.

t ) = ψk

t (ct)((cid:96)k

Dynamic program: Theorem 29 implies that the problem of ﬁnding optimal decentralized
policies in a Dec-POMDP is equivalent to a centralized (single-agent) problem of ﬁnding the
optimal coordination policy for the virtual coordinated system. The virtual coordinated
system is a POMDP with unobserved state (St, L1
t , . . . , LK
, and actions
ˆΞt. The corresponding history of observations is (Cnew
, ˆΞ1:t−1) and therefore we can write a
1:t
history dependent dynamic program similar to the one presented in Proposition 2. Nayyar
et al. (2013) presented a simpliﬁed dynamic program which used the belief state as an
information state; however, it is clear from the above discussion that any other choice of
information state will also lead to a dynamic programming decomposition.

t ), observation Cnew

t

5.3 Common-information based AIS and approximate dynamic programming

Since the coordinated system is a POMDP, we can simply adapt the deﬁnition of AIS
Dec-POMDPs and obtain an approximate dynamic program with approximation guarantees.
Let Xt denote the space of realization of ˆΞt. Then, we have the following.

Deﬁnition 30 Let {ˆZt}T
tion class for IPMs, and {(εt, δt)}T
{ˆσt : (Ct, ˆΞ1:t−1) (cid:55)→ ˆZt}T
date kernels { ˆPt : ˆZt × Xt → ∆(ˆZt+1)}T
Xt → R}T
t=1, is called an {(εt, δt)}T
ˆσt(Ct, ˆΞ1:t−1), satisﬁes the following properties:

t=1 be a pre-speciﬁed collection of Banach spaces, F be a func-
t=1 be pre-speciﬁed positive real numbers. A collection
t=1 of history compression functions, along with approximate up-
t=1 and reward approximation functions {ˆrt : ˆZt ×
t=1, where ˆZt =

t=1-AIS generator if the process { ˆZt}T

(DP1) Suﬃcient for approximate performance evaluation, i.e., for any time t, any

realization ct of Ct and any choice ˆξ1:t of ˆΞ1:t, we have

(cid:12)E[Rt | Ct = ct, ˆΞ1:t = ˆξ1:t] − ˆrt(ˆσt(ct, ˆξ1:t−1), ˆξt)(cid:12)
(cid:12)

(cid:12) ≤ εt.

(DP2) Suﬃcient to predict itself approximately. i.e., for any time t, any realization
ct of Ct, any choice ˆξ1:t of ˆΞ1:t, and for any Borel subset B of ˆZt+1, deﬁne µt(B) :=
P( ˆZt+1 ∈ B | Ct = ct, ˆΞ1:t = ˆξ1:t) and νt(B) := ˆPt(B | ˆσt(ct, ˆξ1:t−1), ˆξt); then,

dF(µt, νt) ≤ δt.

Similar to Proposition 4, we can provide an alternative characterization of an AIS where
we replace (DP2) with approximations of (P2a) and (P2b) and we can prove a proposition
similar to Proposition 8 for the virtual coordinated system.

We can now establish a result similar to Theorem 9 that any AIS gives rise to an
approximate dynamic program. In this discussion, ht denotes (ct, ˆξ1:t−1) and Ht denotes the
space of realization of ht.

38

Approximate information state

Theorem 31 Suppose {ˆσt, ˆPt, ˆrt}T
approximate action-value functions { ˆQt : ˆZt × Xt → R}T
t=1 as follows: ˆVT +1(ˆzT +1) := 0 and for t ∈ {T, . . . , 1}:
R}T

t=1 is an {(εt, δt)}T

t=1-AIS generator. Recursively deﬁne
t=1 and value functions { ˆVt : ˆZt →

ˆQt(ˆzt, ˆξt) := ˆrt(ˆzt, ˆξt) +

(cid:90)

ˆZt+1
ˆQt(ˆzt, ˆξt).

ˆVt+1(ˆzt+1) ˆPt(dˆzt+1 | ˆzt, ˆξt),

(52a)

(52b)

ˆVt(ˆzt) := max
ˆξt∈Xt

Then, we have the following:

1. Value function approximation: For any time t, realization ht of Ht, and choice

ˆξt of ˆΞt, we have

|Qt(ht, ˆξt) − ˆQt(ˆσt(ht), ˆξt)| ≤ αt

and

|Vt(ht) − ˆVt(ˆσt(ht))| ≤ αt,

(53)

where

αt = εt +

T
(cid:88)

τ =t+1

(cid:2)ρF( ˆVτ )δτ −1 + ετ

(cid:3).

2. Approximately optimal policy: Let ˆψ = ( ˆψ1, . . . , ˆψT ), where ˆψt : ˆZt → ∆(Xt), be

a coordination rule that satisﬁes

Supp( ˆψ(ˆzt)) ⊆ arg max
ˆξt∈Xt

ˆQt(ˆzt, ˆξt).

(54)

Deﬁne coordination rule ψ = (ψ1, . . . , ψT ), where ψt := ˆψt ◦ ˆσt. Then, for any time t,
realization ht of Ht, and choice ˆξt of ˆΞt, we have

|Qt(ht, ˆξt) − Qψ

t (ht, ˆξt)| ≤ 2αt

and

|Vt(ht) − V ψ

t (ht)| ≤ 2αt.

(55)

Proof The proof is similar to the proof of Theorem 9.

We can extend the approximation results for the virtual coordinated system to the
approximate policy evaluation case (as in Sec. 3.4), inﬁnite horizon case (as in Sec. 4), the
stochastic AIS case (as in Sec. 3.5), the action compression case (as in Sec. 3.6), and the
observation compression case (as in Sec. 3.7) in a straightforward manner.

6. Reinforcement learning for partially observed systems using AIS

In this section, we present a policy gradient based reinforcement learning (RL) algorithm for
inﬁnite horizon partially observed systems. The algorithm learns a time-homogeneous AIS
generator (ˆσt, ˆr, ˆP ) which satisﬁes (AP1) and (AP2) or a time-homogeneous AIS generator
(ˆσt, ˆr, ˆϕ, ˆP y) which satisﬁes (AP1), (AP2a), and (AP2b). The key idea is to represent each
component of the AIS generator using a parametric family of functions/distributions and
use a multi time-scale stochastic gradient descent algorithm (Borkar, 1997) which learns
AIS generator at a faster time-scale than the policy and/or the action-value function.

39

Subramanian, Sinha, Seraj, and Mahajan

Then, for the ease of exposition, we ﬁrst assume that the policy is ﬁxed and describe
how to learn the AIS generator using stochastic gradient descent. To specify an AIS, we
must pick an IPM F as well. Although, in principle, we can choose any IPM, in practice,
we want to choose an IPM such that the distance dF(µt, νt) in (AP2) or (AP2b) can be
computed eﬃciently. We discuss the choice of IPMs in Sec. 6.1 and then discuss the stochastic
gradient descent algorithm to learn the AIS-generator for a ﬁxed policy in Sec. 6.2. Then we
describe how to simultaneously learn the AIS generator and the policy using a multi-time
scale algorithm, ﬁrst for an actor only framework and then for an actor-critic framework in
Sec. 6.3.

6.1 The choice of an IPM

As we will explain in the next section in detail, our general modus operandi is to assume
that the stochastic kernel ˆP or ˆP y that we are trying to learn belongs to a parametric family
and then update the parameters of the distribution to either minimize dF(µ, ν) deﬁned in
(AP2) or minimize dF(µy, νy) deﬁned in (AP2b). Just to keep the discussion concrete, we
focus on (AP2). Similar arguments apply to (AP2b) as well. First note that for a particular
choice of parameters, we know the distribution ν in closed form, but we do not know the
distribution µ in closed form and only have samples from that distribution. One way to
estimate the IPM between a distribution and samples from another distribution is to use
duality and minimize (cid:12)
(cid:82)
(cid:12) over the choice of function f such that f ∈ F. When
(cid:12)
dF is equal to the total variation distance or the Wasserstein distance, this optimization
problem may be solved using a linear program (Sriperumbudur et al., 2012). However,
solving a linear program at each step of the stochastic gradient descent algorithm can become
a computational bottleneck. We propose two alternatives here. The ﬁrst is to use the total
variation distance or the Wasserstein distance but instead of directly working with them, we
use a KL divergence based upper bound as a surrogate loss. The other alternative is to work
with RKHS-based MMD (maximum mean discrepancy) distance, which can be computed
from samples without solving an optimization problem (Sriperumbudur et al., 2012). It
turns out that for the AIS-setup, a speciﬁc form of MMD known as distance-based MMD is
particularly convenient as we explain below.

ˆZ f dµ − (cid:82)

ˆZ f dν(cid:12)

KL-divergence based upper bound for total variation or Wasserstein distance.
Recall that the KL-divergence between two densities µ and ν on ∆(X) is deﬁned as

DKL(µ(cid:107)ν) =

(cid:90)

X

log µ(x)µ(dx) −

(cid:90)

X

log ν(x)µ(dx).

The total variation distance can be upper bounded by the KL-divergence using Pinsker’s
inequality (Csiszar and K¨orner, 2011) (see footnote 1 for the diﬀerence in constant factor
from the standard Pinsker’s inequality):

dTV(µ, ν) ≤ (cid:112)2DKL(µ(cid:107)ν).

(56)

As we will explain in the next section, we consider the setup where we know the
distribution ν but only obtain samples from the distribution µ. Since there are two losses—the
reward prediction loss ε and the AIS/observation prediction loss δ, we work with minimizing

40

Approximate information state

the weighted square average λε2 + (1 − λ)δ2, where λ ∈ [0, 1] is a hyper-parameter. Pinsker’s
inequality (56) suggests that instead of dTV(µ, ν)2, we can use the surrogate loss function

(cid:90)

X

log ν(x)µ(dx)

where we have dropped the term that does not depend on ν. Note that the above expression
is the same as the cross-entropy between µ and ν which can be eﬃciently computed from
samples. In particular, if we get T i.i.d. samples X1, . . . , XT from µ, then

1
T

T
(cid:88)

t=1

log ν(Xt)

(57)

is an unbiased estimator of (cid:82)

X log ν(x)µ(dx).

Finally, if X is a bounded space with diameter D, then

dWass(µ, ν) ≤ DdTV(µ, ν).

So, using cross-entropy as a surrogate loss also works for Wasserstein distance.

Distance-based MMD. The key idea behind using a distance-based MMD is the following
results.

Proposition 32 (Theorem 22 of Sejdinovic et al. (2013)) Let X ⊆ Rm and dX,p : X×
X → R≥0 be a metric given by dX,p(x, x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)p
2, for p ∈ (0, 2]. Let kp : X × X → R be
any kernel given

kp(x, x(cid:48)) = 1
2

(cid:2)dX,p(x, x0) + dX,p(x(cid:48), x0) − dX,p(x, x(cid:48))(cid:3),

where x0 ∈ X is arbitrary, and let Hp be a RKHS with kernel kp and Fp = {f ∈ Hp : (cid:107)f (cid:107)Hp ≤
1}. Then, for any distributions µ, ν ∈ ∆(X), the IPM dFp(µ, ν) can be expressed as follows:

dFp(µ, ν) =

(cid:113)

E[dX,p(X, W )] − 1
2

E[dX,p(X, X (cid:48))] − 1
2

E[dX,p(W, W (cid:48))],

(58)

where X, X (cid:48) ∼ µ, W, W (cid:48) ∼ ν and (X, X (cid:48), W, W (cid:48)) are all independent.

We call dp deﬁned above as a distance-based MMD. For p = 1 (for which dX corresponds to
the L2 distance), the expression inside the square root in (58) is called the Energy distance
in the statistics literature (Sz´ekely and Rizzo, 2004). In Sejdinovic et al. (2013), the above
result is stated for a general semimetric of a negative type. Our statement of the above
result is specialized to the semimetric dX,p. See Proposition 3 and Example 15 of Sejdinovic
et al. (2013) for details.

As explained in the previous section, we work with minimizing the weighted square
average λε2 + (1 − λ)δ2, where λ is a hyper-parameter. Proposition 32 suggests that instead
of dFp(µ, ν)2, we can use a surrogate loss function

(cid:90)

(cid:90)

X

X

(cid:107)x − w(cid:107)p

2 µ(dx)ν(dw) −

1
2

(cid:90)

(cid:90)

X

X

(cid:107)w − w(cid:48)(cid:107)p

2 ν(dw)ν(dw(cid:48))

(59)

41

Subramanian, Sinha, Seraj, and Mahajan

for p ∈ (0, 2], where we have dropped the term that does not depend on ν. It is possible
to compute the surrogate loss eﬃciently from samples as described in Sriperumbudur et al.
(2012). In particular, if we get T i.i.d. samples X1, . . . , XT from µ, then

1
T

T
(cid:88)

(cid:90)

t=1

X

(cid:107)Xt − w(cid:107)p

2 ν(dw) −

1
2

(cid:90)

(cid:90)

X

X

(cid:107)w − w(cid:48)(cid:107)p

2 ν(dw)ν(dw(cid:48))

(60)

is an unbiased estimator of (59).

In our numerical experiments, we use the surrogate loss (60) for p = 2, which simpliﬁes

as follows.

Proposition 33 Consider the setup of Proposition 32 for p = 2. Suppose νξ is a known
parameterized distribution with mean Mξ and X is a sample from µ. Then, the gradient of
(Mξ − 2X)(cid:124)Mξ

(61)

with respect to ξ in an unbiased estimator of ∇ξdF2(µ, νξ)2.

Proof For p = 2, we have that

dF2(µ, νξ)2 = E[(cid:107)X − W (cid:107)2

2] − 1
2

E[(cid:107)X − X (cid:48)(cid:107)2

2] − 1
2

E[(cid:107)W − W (cid:48)(cid:107)2

2],

where X, X (cid:48) ∼ µ and W, W (cid:48) ∼ νξ. Simplifying the right hand side, we get that

dF2(µ, νξ)2 = (cid:107)E[X](cid:107)2

2 − 2E[X](cid:124)E[W ] + (cid:107)E[W ](cid:107)2
2.

Note that the term (cid:107)E[X](cid:107)2
captures all the terms which depend on ξ.

2 does not depend on the distribution νξ. Thus, the expression (61)

The implication of Proposition 33 is if we use MMD with the RKHS H2 deﬁned in
Proposition 32, then we can can use the expression in (61) as a surrogate loss function for
dF2(µ, νξ)2.

Now we show how to compute the surrogate loss (61) for two types of parameterized

distributions νξ.

1. Surrogate loss for predicting discrete variables: When predicting a discrete-
valued random variable, say a discrete-valued AIS ˆZt+1 in (AP2) or a discrete-valued
observation Yt in (AP2b), we view the discrete random variable as a continuous-valued
random vector by representing it as a one-hot encoded vector. In particular, if the
discrete random variable, which we denote by V , takes m values, then its one-hot
encoded representation, which we denote by X, takes values in the corner points
of the simplex on Rm. Now, suppose νξ is any parameterized distribution on the
discrete set {1, . . . , m} (e.g., the softmax distribution). Then, in the one-hot encoded
representation, the mean Mξ is given by




 ,






νξ(1)
...
νξ(m)

Mξ =

m
(cid:88)

i=1

νξ(i)ei =

42

Approximate information state

where ei denotes the m-dimensional unit vector with 1 in the i-th location. Thus,
when we one-hot encode discrete AIS or discrete observations, the “mean” Mξ is same
as the probability mass function (PMF) νξ. Thus, eﬀectively, dF2(µ, ν)2 is equivalent
to (cid:107)µ − ν(cid:107)2
2 and (61) is an unbiased estimator where we have removed the terms that
do not depend on ν.

2. Surrogate loss for predicting continuous variables: When predicting a
continuous-valued random variable, say a continuous-valued AIS ˆZt+1 in (AP2) or a
continuous-valued observation Yt in (AP2b), we can immediately use the surrogate
loss (61) as long as the parameterized distribution νξ is such that its mean Mξ is given
in closed form. Note that the surrogate loss (61) only depends on the mean of the
distribution and not one any other moment. So, any two distributions ν and ν(cid:48) that
have the same mean, the surrogate loss between any distribution µ and ν is same as
the surrogate loss between µ and ν(cid:48). Thus, using the surrogate loss (61) for predicting
continuous variables only makes sense when we expect the true distribution to be close
to a deterministic function.

6.2 Learning an AIS for a ﬁxed policy

The deﬁnition of AIS suggests that there are two ways to construct an information state
from data: we either learn a time-homogeneous AIS-generator (ˆσ, ˆr, ˆP ) that satisﬁes (AP1)
and (AP2) or we learn a time-homogeneous AIS-generator (ˆσ, ˆr, ˆϕ, ˆP y) that satisﬁes (AP1),
(AP2a), and (AP2b). In either case, there are three types of components of AIS-generators:
(i) regular functions such as ˆr and ˆϕ; (ii) history compression functions {ˆσt}t≥1; and
(iii) stochastic kernels ˆP and ˆP y. To learn these components from data, we must choose
parametric class of functions for all of these. In this section, we do not make any assumption
about how these components are chosen. In particular, ˆr and ˆϕ could be represented by any
class of function approximators (such as a multi-layer preceptron); ˆσ could be represented by
any class of time-series approximators (such as a RNN or its reﬁnements such as LSTM or
GRU); and ˆP and ˆP y could be represented by any class of stochastic kernel approximators
(such as softmax distribution or mixture of Gaussians). We use ξt to denote the corresponding
parameters.

There are two losses in the deﬁnition of an AIS: the reward loss |Rt − ˆr(ˆzt, at)| and
t ). We combine these into a single criterion and

t , νy

the prediction loss dF(µt, νt) or dF(µy
minimize the combined loss function

1
T

T
(cid:104)
(cid:88)

t=1

λ|Rt − ˆr( ˆZt, At)|2 + (1 − λ)dF(µt, νt)2(cid:105)

where T is the length of the episode or the rollout horizon and λ ∈ [0, 1] may be viewed as a
hyper-parameter.

As described in Section 6.1, there are two possibilities to eﬃciently compute dF(µt, νt)2:
use total-variation distance or Wasserstein distance as the IPM and use surrogate loss (57);
or use distance-based MMD as the IPM and use the surrogate loss (61).

43

Subramanian, Sinha, Seraj, and Mahajan

In particular, to choose an AIS that satisﬁes (AP1) and (AP2), we either minimize the

surrogate loss

1
T

T
(cid:88)

t=1

(cid:2)λ|Rt − ˆr( ˆZt, At)|2 + (1 − λ) log(νt( ˆZt+1))(cid:3)

or we minimize the surrogate loss (specialized for p = 2)

1
T

T
(cid:104)
(cid:88)

t=1

λ|Rt − ˆr( ˆZt, At)|2 + (1 − λ)(Mt − 2 ˆZt+1)(cid:124)Mt

(cid:105)

(62)

(63)

where Mt is the mean of the distribution νt.

Similarly, in order to choose an AIS that satisﬁes (AP1), (AP2a) and (AP2b), we

minimize the surrogate loss

1
T

T
(cid:88)

t=1

(cid:2)λ|Rt − ˆr( ˆZt, At)|2 + (1 − λ) log(νy

t (Yt))(cid:3)

or we minimize the surrogate loss (specialized for p = 2)

1
T

T
(cid:104)
(cid:88)

t=1

λ|Rt − ˆr( ˆZt, At)|2 + (1 − λ)(M y

t − 2Yt)(cid:124)M y

t

(64)

(65)

(cid:105)

where M y
t

is the mean of the distribution νy
t .

We use ¯ξ to denote the parameters of the AIS-generator, i.e., the parameters of (ˆσ, ˆP , ˆr)
when using (AP1) and (AP2) or the parameters of (ˆσ, ˆϕ, ˆP y, ˆr) when using (AP1), (AP2a),
(AP2b). We then use L( ¯ξ) to the denote the corresponding loss (62), (63), (64), or (65).
Then, we can learn the parameters ¯ξ using stochastic gradient descent:

¯ξk+1 = ¯ξk − ak∇ ¯ξL( ¯ξk),

(66)

where the learning rates {ak}k≥0 satisfy the standard conditions (cid:80) ak = ∞ and (cid:80) a2

k < ∞.

6.3 AIS-based PORL

Given the stochastic gradient descent algorithm to learn an AIS-generator for a ﬁxed policy,
we can simultaneously learn a policy and AIS-generator by following a multi time-scale
stochastic gradient descent (Borkar, 1997), where we learn the AIS-generator at a faster
learning rate than the policy.

In particular, let πθ : ˆZ → ∆(A) be a parameterized stochastic policy with parameters θ.
Let J( ¯ξ, θ) denote the performance of policy πθ. From the policy gradient theorem (Sutton
et al., 2000; Baxter and Bartlett, 2001), we know that

∇θJ( ¯ξ, θ) = E

(cid:20) ∞
(cid:88)

(cid:18) t

(cid:88)

t=1

τ =1

∇θ log πθ(At | ˆZt)

γt−1Rt

(cid:19)

(cid:21)

(67)

44

Approximate information state

which can be estimated from a sampled trajectory with a rollout horizon of T using the
G(PO)MDP gradient (Baxter and Bartlett, 2001)

(cid:98)∇θJ( ¯ξ, θ) =

T
(cid:88)

(cid:18) t

(cid:88)

∇θ log πθ(At | ˆZt)

(cid:19)

γt−1Rt.

(68)

t=1

τ =1

We can iteratively update the parameters {( ¯ξk, θk)}k≥1 of both the AIS-generator and
policy as follows. We start with an initial choice ( ¯ξ1, θ1), update both parameters after a
rollout of T as follows

¯ξk+1 = ¯ξk − ak∇ ¯ξL( ¯ξk)

and θk+1 = θk + bk (cid:98)∇θJ( ¯ξk, θk)

(69)

where the learning rates {ak}k≥1 and {bk}k≥1 satisfy the standard conditions on multi time-
scale learning: (cid:80)
k bk = ∞, (cid:80)
k a2
k < ∞, and limk→∞ bk/ak = 0,
which ensures that AIS-generator learns at a faster rate than the policy.

k ak = ∞, (cid:80)

k < ∞, (cid:80)

k b2

A similar idea can be used for an actor-critic algorithm. Suppose we have a parameterized
policy πθ : ˆZ → ∆(A) and a parameterized critic ˆQζ : ˆZ × A → R, where θ denotes the
parameters of the policy and ζ denotes the parameters of the critic. Let J( ¯ξ, θ, ζ) denote
the performance of the policy. From the policy gradient theorem (Sutton et al., 2000; Konda
and Tsitsiklis, 2003), we know that

∇θJ( ¯ξ, θ, ζ) =

1
1 − γ

E(cid:2)∇θ log πθ(At | ˆZt)Qζ( ˆZt, At)(cid:3)

which can be estimated from a sampled trajectory with a rollout horizon of T by

(cid:98)∇θJ( ¯ξ, θ, ζ) =

1
(1 − γ)T

T
(cid:88)

t=1

∇θ log πθ(At | ˆZt) ˆQζ( ˆZt, At).

For the critic, we use the temporal diﬀerence loss

(70)

(71)

LTD( ¯ξ, θ, ζ) =

1
T

T
(cid:88)

t=1

smoothL1(cid:0) ˆQζ( ˆZt, At) − Rt − γ ˆQζ( ˆZt+1, At+1)(cid:1)

(72)

where smoothL1 is the smooth L1 distance given by

smoothL1(x) =

(cid:40) 1

2 x2
|x| − 1
2

if |x| < 1
otherwise.

We can iteratively update the parameters {( ¯ξk, θk, ζk)}k≥1 of the AIS-generator, policy,
and critic as follows. We start with an initial choice ( ¯ξ1, θ1, ζ1), and update all the parameters
after a rollout of T steps as follows

¯ξk+1 = ¯ξk−ak∇ ¯ξL( ¯ξk),

θk+1 = θk+bk (cid:98)∇θJ( ¯ξk, θk, ζk)

and ζk+1 = ζk−ck∇ζLTD( ¯ξk, θk, ζk)
(73)
where the learning rates {ak}k≥1, {bk}k≥1, {ck}k≥1 satisfy the standard conditions on multi
k bk = ∞, (cid:80)
time-scale learning: (cid:80)
k < ∞,

k ak = ∞, (cid:80)

k ck = ∞, (cid:80)

k < ∞, (cid:80)

k a2

k b2

45

Subramanian, Sinha, Seraj, and Mahajan

Algorithm 1 AIS-based PORL algorithm
Input: Initial AIS-Generator: (ˆσ, ˆP , ˆr) ¯ξ0

Reward weight: λ, Number of episodes: K, AIS-LR: aK

Output: Learned policy: πθK , Learned AIS-generator: (ˆσ, ˆP , ˆr) ¯ξK
1: procedure AIS-based PORL
2:

for all k ∈ {1, . . . , K} do

, Initial Policy: πθ0, Discount factor: γ,
k=1, Policy-LR: bK

k=1.

3:

4:

5:

6:

7:

Reset environment and perform an episode using πθk−1, (ˆσ, ˆP , ˆr) ¯ξk−1
.
A1:T , Y1:T , R1:T ← Actions, observations, and rewards for episode k.
Compute AIS loss using A1:T , Y1:T , R1:T , λ, (ˆσ, ˆP , ˆr) ¯ξk−1
using Eq. (64) or (65)
using Eq. (68)
Compute policy loss using A1:T , Y1:T , R1:T , γ, πθk−1, (ˆσ) ¯ξk−1
Update AIS parameters ¯ξk−1 and policy parameters πθk−1 using Eq. (69)

(cid:80)

k c2

k < ∞, limk→∞ ck/ak = 0, and limk→∞ bk/ck = 0, which ensures that AIS-generator
learns at a faster rate than the critic, and the critic learns at a faster rate than the policy.
The complete algorithm is shown in Algorithm 1.

Under standard technical conditions (see Theorem 23 of Borkar (1997) or Page 35 of
Leslie (2004)), we can show that iterations (69) and (73) will converge to a stationary point
of the corresponding ODE limits. At convergence, depending on ε and δ for the quality of
AIS approximation, we can obtain approximation guarantees corresponding to Theorem 27.
For a more detailed discussion on convergence, please refer to Appendix E.

We conclude this discussion by mentioning that algorithms similar to the AIS-based PORL
have been proposed in the literature including Bakker (2002); Wierstra et al. (2007, 2010);
Hausknecht and Stone (2015); Heess et al. (2015); Zhu et al. (2017); Ha and Schmidhuber
(2018); Baisero and Amato (2018); Igl et al. (2018); Zhang et al. (2019). However, these
papers only discuss the empirical performance of the proposed algorithms but do not derive
performance bounds.

7. Experiments

We perform numerical experiments to check the eﬀectiveness of AIS-based PORL algorithms
proposed in the previous section. The code for all AIS experiments is available in Subramanian
et al. (2020). We consider three classes of POMDP environments, which have increasing
diﬃculty in terms of the dimension of their state and observation spaces:

1. Low-dimensional environments (Tiger, Voicemail, and Cheese Maze)

2. Moderate-dimensional environments (Rock Sampling and Drone Surveillance)

3. High-dimensional environments (diﬀerent variations of MiniGrid)

For each environment, we use the actor only framework and learn an AIS based on (AP1),
(AP2a), and (AP2b). There are four components of the corresponding AIS-generator: the
history compression function ˆσ, the AIS update function ˆϕ, the reward prediction function ˆr,
and the observation prediction kernel ˆP y. We model the ˆσ as an LSTM, where the memory
update unit of LSTM acts as ˆϕ. We model ˆr, ˆP y, and the policy ˆπ as feed-forward neural

46

Approximate information state

Figure 4: Network architecture for PORL using AIS.

networks. A block diagram of the network architecture is shown in Fig. 4 and the details of
the networks and the hyperparameters are presented in Appendix F. To avoid over-ﬁtting,
we use the same network architecture and hyperparameters for all environments in the same
diﬃculty class.

We repeat each experiment for multiple random seeds and plot the median value along
with the uncertainty band from the ﬁrst to the third quartile. For all environments, we
compare our performance with a baseline which uses an actor-critic algorithm where both
the actor and critic are modeled using LSTM and the policy parameters are updated
using PPO. This architecture was proposed as a baseline for the Minigrid environments in
Chevalier-Boisvert et al. (2018a). The details of the baseline architecture are presented in
Appendix F.

To evaluate the performance of the policy while training for AIS-based PORL, a separate
set of rollouts is carried out at ﬁxed intervals of time steps and the mean of these rollouts
is considered. For the PPO baseline a number of parallel actors are used during training,
and once the episodes are completed, their returns are stored in a list. A ﬁxed number
(based on the number of parallel actors) of past episodes are considered to evaluate the mean
performance of the current policy during training. See Appendix F for details.

For the low and moderate dimensional environments, we compare the performance
with the best performing planning solution obtained from the JuliaPOMDP repository
(Egorov et al., 2017). For the high-dimensional environments, ﬁnding a planning solution is
intractable, so we only compare with the PPO baseline mentioned previously.

7.1 Low-dimensional environments

In these POMDP environments, the size of the unobserved state space is less than about 10
and the planning solution can be easily obtained using standard POMDP solvers.

1. Tiger: The Tiger environment is a sequential hypothesis testing task proposed in
Kaelbling et al. (1998). The environment consists of two doors, with a tiger behind
one door and a treasure behind the other. The agent can either perform a listen
action, which has a small negative reward of −1 and gives a noisy observation about
the location of the tiger, or the agent can open one of the doors. Opening the door
with the treasure gives a reward of +10 while opening the door with a tiger gives a
large negative reward of −100. After opening a door, the environment is reset.

47

RNN:ˆ𝜎NN:𝜋𝜃NN:(ˆ𝑟,ˆ𝑃𝑦)State:ˆ𝑍𝑡−1𝑌𝑡−1𝐴𝑡−1ˆ𝑍𝑡𝐴𝑡Toenvironmentˆ𝑅𝑡𝜈𝑡+1HistorycompressorPolicynetworkRewardandobservationpredictorSubramanian, Sinha, Seraj, and Mahajan

(a) Tiger

(b) Voicemail

Figure 5: Comparison of AIS-based actor only PORL algorithm with LSTM+PPO baseline
for low-dimensional environments (for 10 random seeds).

(c) Cheese Maze

2. Voicemail: The Voicemail enviroment is also a sequential hypothesis testing task
propsed in Williams and Young (2007). This environment models a dialog system for
managing voicemails. The agent can either perform an ask action, wich has a small
negative reward of −1 and gives a noisy observation about the intent of the user, or the
agent can execute save or delete. Choosing a save/delete action which matches
the intent of the user gives a reward of +5. The agent receives a negative reward of
−20 for action delete when the user intent is save, while choosing action save when
the user intent is delete gives a smaller but still signiﬁcant negative reward of −10.
Since the user prefers save more than delete, the initial belief is given by [0.65, 0.35]
for save and delete respectively. After taking a save/delete action, the agent
moves on to the next voicemail message.

3.

CheeseMaze: The CheeseMaze environment is a POMDP
with masked states proposed in McCallum (1993). The envi-
ronment consists of 11 states and 7 observations as shown on
the right. The objective is to reach the goal state, which is
indicated by observation 7. The agent only receives a reward
of +1, when the goal state is reached.

2

1

5

6

3

5

7

2

4

5

6

For all three environments, we compare the performance of AIS-based PORL with the
LSTM+PPO baseline, described earlier. We also compare with the best performing planning
solution from the JuliaPOMDP repository (Egorov et al., 2017). The results are presented in

48

Approximate information state

Fig. 5, which shows both AIS-based PORL and LSTM+PPO converge close to the planning
solutions relatively quickly.3

7.2 Moderate-dimensional environments

(a) Rock Sampling

(b) Drone Surveillance

Figure 6: Comparison of AIS-based actor only PORL algorithm with LSTM+PPO baseline
for moderate-dimensional environments (for 10 random seeds).

In these environments, the size of the unobserved state is moderately large (of the order
of 102 to 103 unobserved states) and the optimal planning solution cannot be easily obtained
using standard POMDP solvers. However, an approximate planning solution can be easily
obtained using standard approximation algorithms for POMDPs.

1. RockSample: RockSample is a scalable POMDP environ-
ment introduced in Smith and Simmons (2004) which models
the rover science exploration. The RockSample(n, k) envi-
ronment consists of a n × n grid with k rocks. The rocks
are at known positions. Some of the rocks which are labeled
as good rocks have scientiﬁc values; other rocks which are
labeled as bad rocks do not. Sampling a rock is expensive
and the agent has a noisy long-range sensor to help determine
if a rock is good before choosing to approach and sample it.
At each stage, the agent can choose from k + 5 actions: north, south, east, west,
sample, check1, . . . , checkk. The ﬁrst four are deterministic single-step motion
actions. The sample action samples the rock at the current location; if the rock is
good, there is a reward of +20 and the rock becomes bad (so that no further reward
can be gained from sampling it); if the rock is bad, there is a negative reward of −10.
The right edge of the map is a terminal state and reaching it gives a reward of +10.
In our experiments, we use a RockSample(5, 3) environment.

3. The performance of all learning algorithms for CheeseMaze are better than the best planning solution.
We solved the CheeseMaze model with other solvers available in the JuliaPOMDP Egorov et al. (2017),
and all these solution performed worse than the solution obtained by incremental pruning presented here.

49

Subramanian, Sinha, Seraj, and Mahajan

2. DroneSurveillance: DroneSurveillance is a POMDP
model of deploying an autonomous aerial vehicle in a par-
tially observed, dynamic, indoor environment introduced in
Svoreˇnov´a et al. (2015). The environment is a 5 × 5 grid
with two agents: a ground agent which moves randomly and
an aerial agent, whose motion has to be controlled. The
aerial agent starts at the bottom-left cell and has to reach the
upper-right cell (the goal state) without being in the same
location as the ground agent. The ground agent cannot enter
the start or goal states. The aerial agent has a downward facing camera which can
view a 3 × 3 grid centered at its current location and it can perfectly see the location
of the ground agent if it is in this view. At each stage, the aerial agent may choose
from 5 actions: north, south, east, west, hover. The ﬁrst four are deterministic
single-step motion actions and the hover action keeps the aerial vehicle at its current
position. Reaching the goal gives a reward of +1 and ends the episode. If both agents
are in the same cell, there is a negative reward of −1 and the episode ends.

The visualizations above are taken from the JuliaPOMDP environments (Egorov et al.,
2017). For both environments, we compare the performance of AIS-based PORL with the
LSTM+PPO baseline described earlier. We also compare with the best performing planning
solution from the JuliaPOMDP repository (Egorov et al., 2017). The results are shown
in Fig. 6 which shows that both AIS-based PORL algorithms converge close to the best
planning solution in both environments. The performance of LSTM+PPO is similar in
DroneSurveillance but LSTM+PPO gets stuck in a local minima in RockSample.

7.3 High-dimensional environments

We use the MiniGrid environments from the BabyAI platform (Chevalier-Boisvert et al.,
2018b), which are partially observable 2D grid environments which has tasks of increasing
complexity level. The environment has multiple entities (agent, walls, lava, boxes, doors,
and keys); objects can be picked up, dropped, and moved around by the agent; doors can be
unlocked via keys of the same color (which might be hidden inside boxes). The agents can
see a 7 × 7 view in front of it but it cannot see past walls and closed doors. At each time,
it can choose from the following actions: {Move Forward, Turn Left, Turn Right,
Open Door/Box, Pick up item, Drop Item, Done}. The agent can only hold one item
at a time. The objective is to reach a goal state in the quickest amount of time (which is
captured by assigning to the goal state a reward which decays over time).

Most of the environments have a certain theme, and we cluster the environments
accordingly. The visualizations below are taken from the Gym Minigrid environments
(Chevalier-Boisvert et al., 2018b).

50

Approximate information state

1. Simple Crossing: A simple crossing environment is a 2D grid
with columns of walls with an opening (or a crossing). The agent
can traverse the wall only through the openings and needs to
ﬁnd a path from the start to the goal state. There are four
such environments (MGSCS9N1, MGSCS9N2, MGSCS9N3, and
MGSCS11N5) where the label SnNm means that the size of the
environment is n × n and there are m columns of walls.

2. Lava Crossing: The lava crossing environments are similar to the
simple crossing environments, but the walls are replaced by lava.
If the agent steps on to the lava block then it dies and the episode
ends. Therefore, exploration is more diﬃcult in lava crossing as
compared to simple crossing. There are two such environments
(MGLCS9N1 and MGLCS9N2) where the label SnNm has the
same interpretation as simple crossing.

3. Key Corridor: The key corridor environments consist of a
central corridor which has rooms on the left and right sides which
can be accessed through doors. When the door is locked it can
be opened using a key of the same color. The agent has to move
to the location of the key, pick it up, move to the location of
the correct door, open the door, drop the key, and pick up the
colored ball. There are three such environments (MGKCS3R1,
MGKCS3R2, and MGKCS3R3), where the label SnRm means that the size of the grid
is proportional to n and the number of rooms present is 2m.

4. Obstructed Maze: The obstructed maze environments
are similar to key corridor environments but the key is
inside a box and the box has to be opened to ﬁnd the
key. We consider two such environments (MGOM1Dl and
MGOM1Dlh). In MGOM1Dl box is already open while
in MGOM1Dlh the box is closed. There is an additional
such environment in the BabyAI platform (MGOM1Dlhb), which is more suitable for
continual learning algorithms so we exclude it here.

The number of observations in a given Minigrid environment is discrete but is too large
to model it as a one-hot encoded discrete observation as done in the previous environments.
Instead we compress the observations as described in Section 3.7 by using an autoencoder
to convert a large discrete space to a continuous space with a tractable size. A separate
autoencoder is trained for each environment using a dataset that is created by performing
random rollouts. Once the autoencoder is trained over the ﬁxed dataset for several epochs,
it is ﬁxed and used to generate the observations for learning the AIS. This is very similar
to Ha and Schmidhuber (2018), where they learn the autoencoder in a similar fashion and
then ﬁx it, following which their training procedure for the next observation distribution
prediction and policy takes place.

Note that the output of the autoencoder is a continuous variable and we are using
MMD with p = 2 as an IPM. As explained in Section 6.1, dF2(µ, ν)2 only depends on the

51

Subramanian, Sinha, Seraj, and Mahajan

(a) MGSCS9N1

(b) MGSCS9N2

(c) MGSCS9N3

(d) MGSCS11N5

(e) MGLCS9N1

(f) MGLCS9N2

(g) MGKCS3R1

(h) MGKCS3R2

Figure 7: Comparison of AIS-based actor only PORL algorithm with LSTM+PPO baseline
for high-dimensional environments (for 5 random seeds).

52

Approximate information state

(i) MGKCS3R3

(j) MGOM1Dl

Figure 7 (continued): Comparison of AIS-based actor only PORL algorithm with
LSTM+PPO baseline for high-dimensional environments (for 5 random seeds).

(k) MGOM1Dlh

mean of µ and ν. So, to simplify the computations, we assume that ν is a Dirac delta
distribution centered at its mean. Thus, eﬀectively, we are predicting the mean of the next
observation. In general, simply predicting the mean of the observations may not lead to
a good representation, but in the Minigrid environments, the transitions are deterministic
and the only source of stochasticity in the observations is due to the initial conﬁguration
of the environment. So, in practice, simply predicting the mean of the next observation
works reasonably well. We emphasize that for other more general environments with truly
stochastic observations, such a choice of IPM may not work well and it may be better to
choose the MMD dFp deﬁned in Proposition 32 for a diﬀerent value of p, say p = 1 (which
corresponds to the energy distance (Sz´ekely and Rizzo, 2004)).

For all minigrid environments, we compare the performance of AIS-based PORL with
the LSTM+PPO baseline proposed in Chevalier-Boisvert et al. (2018a). The results are
shown in Fig. 7 which shows that for most environments AIS-based PORL converges to
better performance values. Note that AIS-based PORL fails to learn in the Lava Crossing
environments (MGLCS9N1 and MGLCS9N2) while LSTM+PPO fails to learn in the larger
Key Crossing environments (MGKCS3R2 and MGKCS3R3) and in the Obstructed
Maze environments (MGOM1Dl and MGOM1Dlh).

The results indicate that one IPM does not necessarily lead to better performance than
others in all cases. The performance of a particular IPM depends on whether the observation
and AIS spaces are discrete or continuous, on the size of these spaces, and the stochasticity
of the environment. The fact that we are approximating the policy using non-linear neural

53

Subramanian, Sinha, Seraj, and Mahajan

networks makes it diﬃcult to quantify the impact of the choice of IPM on the accuracy
of learning. It will be important to understand this impact in more detail and develop
guidelines on how to choose an IPM based on the features of the environment.

8. Conclusion

In this paper, we present a theoretical framework for approximate planning and learning in
partially observed system. Our framework is based on the fundamental notion of information
state. We provide two equivalent deﬁnitions of information state. An information state is a
function of history which is suﬃcient to compute the expected reward and predict its next
value. Equivalently, an information state is a function of the history which can be recursively
updated and is suﬃcient to compute the expected reward and predict the next observation.
We show that an information state always leads to a dynamic programming decomposition
and provide several examples of simpliﬁed dynamic programming decompositions proposed
in the literature which may be viewed as speciﬁc instances of information states.

We then relax the deﬁnition of an information state to describe an approximate informa-
tion state (AIS), which is a function of the history that approximately satisﬁes the properties
of the information state. We show that an AIS can be used to identify an approximately
optimal policy with the approximation error speciﬁed in terms of the “one-step” approx-
imation errors in the deﬁnition of the AIS. We present generalizations of AIS to setups
with observation and action compression as well as to multi-agent systems. We show that
various approximation approaches for both fully and partially observed setups proposed in
the literature may be viewed as special cases of AIS.

One of the salient features of the AIS is that it is deﬁned in terms of properties that can
be estimated from data, and hence the corresponding AIS generators can be learnt from
data. These can then be used as history representations in partially observed reinforcement
learning (PORL) algorithms. We build up on this idea to present policy gradient algorithms
which learn an AIS representation and an optimal policy and/or action-value function using
multi time-scale stochastic gradient descent.

We present detailed numerical experiments which compare the performance of AIS-based
PORL algorithms with a state-of-the-art PORL algorithm for three classes of partially
observed problems—small, medium and large scale problems—and ﬁnd out that AIS-based
PORL outperforms the state-of-the-art baseline in most cases.

We conclude by observing that in this paper we restricted attention to the simplest classes
of algorithms but the same idea can be extended to develop AIS-based PORL algorithms
which uses value-based approaches such as Q-learning and its improved variants such as
DQN, DDQN, distributional RL, etc. Finally, we note that the AIS representation includes
a model of the system, so it can be used as a component of model-based reinforcement
learning algorithms such as Dyna (Sutton and Barto, 2018, Sec 8.2, page 161). Such an
approach will provide anytime guarantees on the approximation error which will depend on
the “one-step” approximation error of the current AIS-representation. Therefore, we believe
that AIS presents a systematic framework to reason about learning in partially observed
environments.

54

Approximate information state

Acknowledgment

The authors are grateful to Demosthenis Teneketzis, Peter Caines, and Dileep Kalathil for
useful discussions and feedback. The work of JS and AM was supported in part by the
Natural Science and Engineering Research Council of Canada through Discovery Grant
RGPIN-2016-05165. The work of AS, RS, and AM was supported in part by the Innovation
for Defence Excellence and Security (IDEaS) Program of the Canadian Department of
National Defence through grant CFPMN2-037. AS was also supported by an FRQNT
scholarship. The numerical experiments were enabled in part by support provided by Calcul
Qu´ebec and Compute Canada.

References

D. Abel, D. Hershkowitz, and M. Littman. Near optimal behavior via approximate state
abstraction. In M. F. Balcan and K. Q. Weinberger, editors, International Conference
on Machine Learning (ICML), volume 48, pages 2915–2923, New York, New York, USA,
June 2016.

S. Adlakha, S. Lall, and A. Goldsmith. Networked Markov decision processes with delays.

IEEE Transactions on Automatic Control (TAC), 57(4):1013–1018, Apr. 2012.

M. Aicardi, F. Davoli, and R. Minciardi. Decentralized optimal control of Markov chains
with a common past information set. IEEE Trans. Autom. Control, 32(11):1028–1031,
1987.

E. Altman and P. Nain. Closed-loop control with delayed information. ACM SIGMETRICS

Performance Evaluation Review, 20(1):193–204, June 1992.

C. Amato, D. S. Bernstein, and S. Zilberstein. Optimizing ﬁxed-size stochastic controllers
for POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent Systems
(AAMAS), 21(3):293–320, 2010.

J. Arabneydi and A. Mahajan. Team optimal control of coupled subsystems with mean-ﬁeld
sharing. In IEEE Conference on Decision and Control (CDC), pages 1669–1674. IEEE,
2014.

J. Arabneydi and A. Mahajan. Reinforcement learning in decentralized stochastic control
systems with partial history sharing. In American Control Conference (ACC), pages
5449–5456. IEEE, July 2015.

K. Asadi, D. Misra, and M. Littman. Lipschitz continuity in model-based reinforcement
In International Conference on Machine Learning (ICML), pages 264–273,

learning.
Stockholm, Sweden, 10–15 Jul 2018.

K. J. Astr¨om. Optimal control of Markov processes with incomplete state information.

Journal of mathematical analysis and applications, 10(1):174–205, 1965.

K. J. Astr¨om. Introduction to Stochastic Control Theory. Dover, 1970.

55

Subramanian, Sinha, Seraj, and Mahajan

K. Azizzadenesheli, A. Lazaric, and A. Anandkumar. Reinforcement learning of POMDPs
using spectral methods. In Annual Conference on Learning Theory (COLT), volume 49,
pages 193–256, 2016.

A. Baisero and C. Amato. Learning internal state models in partially observable envi-
ronments;. Reinforcement Learning under Partial Observability, NeurIPS Workshop,
2018.

B. Bakker. Reinforcement learning with long short-term memory. In Neural Information

Processing Systems (NIPS), 2002.

J. L. Bander and C. C. White. Markov decision processes with noise-corrupted and delayed
state observations. Journal of the Operational Research Society, 50(6):660–668, June 1999.

J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial

Intelligence Research, 15:319–350, 2001.

R. Bellman. Dynamic Programming. Princeton University Press, 1957.

D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The complexity of decentralized
control of markov decision processes. Mathematics of operations research, 27(4):819–840,
2002.

D. S. Bernstein, E. A. Hansen, and S. Zilberstein. Bounded policy iteration for decentralized
pomdps. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 52–57,
2005.

D. Bertsekas. Convergence of discretization procedures in dynamic programming. IEEE

Transactions on Automatic Control (TAC), 20:415–419, 1975.

D. Bertsekas and J. Tsitsiklis. Neuro-dynamic Programming. Anthropological Field Studies.

Athena Scientiﬁc, 1996. ISBN 9781886529106.

J.-M. Bismut. An example of interaction between information and control: The transparency

of a game. IEEE Trans. Autom. Control, 18(5):518–522, Oct. 1972.

T. Bohlin. Information pattern for linear discrete-time models with stochastic coeﬃcients.

IEEE Transactions on Automatic Control (TAC), 15(1):104–106, Feb. 1970.

B. Boots, S. M. Siddiqi, and G. J. Gordon. Closing the learning-planning loop with predictive
state representations. The International Journal of Robotics Research, 30(7):954–966,
2011.

V. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge Univer-

sity Press, 2008.

V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29

(5):291–294, 1997.

56

Approximate information state

A. Boularias and B. Chaib-Draa. Exact dynamic programming for decentralized POMDPs
with lossless policy compression. In International Conference on Automated Planning and
Scheduling (ICAPS), pages 20–27. AAAI Press, 2008.

Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network

distillation. arXiv preprint arXiv:1810.12894, 2018.

A. Carlin and S. Zilberstein. Observation compression in Dec-POMDP policy trees. In
International Conference on Autonomous Agents and Multi-agent Systems (AAMAS),
pages 31–45, 2008.

A. Cassandra, M. L. Littman, and N. L. Zhang.

Incremental pruning: A simple, fast,
exact method for partially observable Markov decision processes. In Proceedings of the
Thirteenth Conference on Uncertainty in Artiﬁcial Intelligence, 1997.

A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Acting optimally in partially observable

stochastic domains. In AAAI Conference on Artiﬁcial Intelligence, 1994.

P. S. Castro, P. Panangaden, and D. Precup. Equivalence relations in fully and partially
observable Markov decision processes. In International Joint Conference on Artiﬁcial
Intelligence (IJCAI), pages 1653–1658, 2009.

J. Chakravorty and A. Mahajan. Suﬃcient conditions for the value function and optimal
strategy to be even and quasi-convex. IEEE Transactions on Automatic Control (TAC),
63(11):3858–3864, Nov. 2018.

Y. Chandak, G. Theocharous, C. Nota, and P. S. Thomas. Lifelong learning with a changing

action set. In AAAI Conference on Artiﬁcial Intelligence, pages 3373–3380, 2020.

H.-T. Cheng. Algorithms for Partially Observable Markov Decision Processes. PhD thesis,

University of British Columbia, Vancouver, BC, 1988.

M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. H. Nguyen,
and Y. Bengio. Babyai: A platform to study the sample eﬃciency of grounded language
learning. In International Conference on Learning Representations (ICLR), 2018a.

M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for

openai gym. https://github.com/maximecb/gym-minigrid, 2018b.

J. P. Crutchﬁeld and K. Young. Inferring statistical complexity. Physical Review Letters, 63:

105–108, July 1989.

I. Csiszar and J. K¨orner. Information theory: coding theorems for discrete memoryless

systems. Cambridge University Press, 2011.

M. Davis and P. Varaiya. Information states for linear stochastic systems. Journal of

Mathematical Analysis and Applications, 37(2):384–402, Feb. 1972.

J. S. Dibangoye, C. Amato, O. Buﬀet, and F. Charpillet. Optimally solving dec-pomdps as
continuous-state mdps. Journal of Artiﬁcial Intelligence Research, 55:443–497, 2016.

57

Subramanian, Sinha, Seraj, and Mahajan

J. E. Eckles. Optimum maintenance with incomplete information. Operations Research, 16

(5):1058–1067, 1968.

M. Egorov, Z. N. Sunberg, E. Balaban, T. A. Wheeler, J. K. Gupta, and M. J. Kochenderfer.
POMDPs.jl: A framework for sequential decision making under uncertainty. Journal of
Machine Learning Research (JMLR), 18(26):1–5, 2017. URL http://jmlr.org/papers/
v18/16-300.html.

E. A. Feinberg. On essential information in sequential decision processes. Mathematical

Methods of Operations Research, 62(3):399–410, Nov. 2005.

N. Ferns, P. Panangaden, and D. Precup. Metrics for ﬁnite Markov decision processes. In
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 162–169. AUAI Press,
2004.

N. Ferns, P. Panangaden, and D. Precup. Bisimulation metrics for continuous Markov

decision processes. SIAM Journal on Computing, 40(6):1662–1714, 2011.

V. Francois-Lavet, G. Rabusseau, J. Pineau, D. Ernst, and R. Fonteneau. On overﬁtting
and asymptotic bias in batch reinforcement learning with partial observability. Journal of
Artiﬁcial Intelligence Research (JAIR), 65:1–30, 2019.

C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning
continuous latent space models for representation learning. In International Conference
on Machine Learning (ICML), 2019.

R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in Markov

decision processes. Artiﬁcial Intelligence, 147(1-2):163–223, July 2003.

P. Grassberger. Toward a quantitative theory of self-generated complexity. International

Journal of Theoretical Physics, 25(9):907–938, Sept. 1986.

P. Grassberger. Complexity and forecasting in dynamical systems. Springer Berlin Heidelberg,

1988. ISBN 978-3-540-45968-2.

A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method
for the two-sample-problem. In Neural Information Processing Systems (NIPS), page
513–520, 2006.

D. Ha and J. Schmidhuber. World models. arXiv:1803.10122, 2018.

W. Hamilton, M. M. Fard, and J. Pineau. Eﬃcient learning and planning with compressed
predictive states. The Journal of Machine Learning Research (JMLR), 15(1):3395–3439,
2014.

E. A. Hansen. An improved policy iteratioll algorithm for partially observable MDPs. In

Neural Information Processing Systems (NIPS), page 1015–1021. MIT Press, 1997.

E. A. Hansen. Solving POMDPs by searching in policy space. In Uncertainty in Artiﬁcial

Intelligence (UAI), pages 211–219, 1998.

58

Approximate information state

M. Hausknecht and P. Stone. Deep recurrent Q-learning for partially observable MDPs. In

AAAI Fall Symposium Series, 2015.

N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver. Memory-based control with recurrent

neural networks, 2015. arXiv:1512.04455.

O. Hern´andez-Lerma and J. B. Lasserre. Discrete-time Markov control processes: basic

optimality criteria. Springer, 2012.

K. Hinderer. Lipschitz continuity of value functions in Markovian decision processes.
Mathematical Methods of Operations Research, 62(1):3–22, Sep 2005. ISSN 1432-5217.

S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

M. Igl, L. Zintgraf, T. A. Le, F. Wood, and S. Whiteson. Deep variational reinforcement
learning for POMDPs. In International Conference on Machine Learning (ICML), pages
2117–2126, 2018.

M. T. Izadi and D. Precup. A planning algorithm for predictive state representations. In
G. Gottlob and T. Walsh, editors, International Joint Conference on Artiﬁcial Intelligence
(IJCAI), pages 1520–1521. Morgan Kaufmann, 2003.

T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement learning algorithm for partially
observable markov decision problems. In Neural Information Processing Systems (NIPS),
pages 345–352. 1995.

H. Jaeger. Observable operator models for discrete stochastic time series. Neural computation,

12(6):1371–1398, 2000.

H. Jaeger, M. Zhao, and A. Kolling. Eﬃcient estimation of OOMs. In Advances in Neural

Information Processing Systems, pages 555–562, 2006.

M. James, S. Singh, and M. Littman. Planning with predictive state representations. In

International Conference on Machine Learning and Applications (ICMLA), 2004.

N. Jiang, A. Kulesza, and S. P. Singh. Improving predictive state representations via gradient

descent. In AAAI Conference on Artiﬁcial Intelligence, pages 1709–1715, 2016.

L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially

observable stochastic domains. Artiﬁcial intelligence, 101(1-2):99–134, 1998.

S. Katt, F. A. Oliehoek, and C. Amato. Bayesian reinforcement learning in factored POMDPs.

In Autonomous Agents and MultiAgent Systems (AAMAS), page 7–15, 2019.

V. R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, 2003.

A. Kulesza, N. Jiang, and S. Singh. Low-rank spectral learning with weighted loss functions.

In Artiﬁcial Intelligence and Statistics, pages 517–525, 2015a.

59

Subramanian, Sinha, Seraj, and Mahajan

A. Kulesza, N. Jiang, and S. P. Singh. Spectral learning of predictive state representations
with insuﬃcient statistics. In AAAI Conference on Artiﬁcial Intelligence, pages 2715–2721,
2015b.

A. Kumar and S. Zilberstein. Constraint-based dynamic programming for decentralized
POMDPs with structured interactions. In C. Sierra, C. Castelfranchi, K. S. Decker, and
J. S. Sichman, editors, International Conference on Autonomous Agents and Multiagent
Systems (AAMAS), pages 561–568. IFAAMAS, 2009.

P. R. Kumar and P. Varaiya. Stochastic Systems: Estimation, Identiﬁcation and Adaptive
Control. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1986. ISBN 0-13-846684-X.

H. Kurniawati, D. Hsu, and W. S. Lee. Sarsop: Eﬃcient point-based pomdp planning by
approximating optimally reachable belief spaces. In Robotics: Science and systems, 2008.

H. Kwakernaak. Theory of Self-Adaptive Control Systems, chapter Admissible Adaptive

Control, pages 14–18. Springer, 1965.

W. S. Lee, N. Rong, and D. Hsu. What makes some POMDP problems easy to approximate?

In Neural Information Processing Systems (NIPS), pages 689–696, 2008.

D. S. Leslie. Reinforcement learning in games. PhD thesis, The University of Bristol, 2004.

Y. Li, B. Yin, and H. Xi. Finding optimal memoryless policies of POMDPs under the
expected average reward criterion. European Journal of Operational Research, 211(3):
556–567, 2011.

M. L. Littman. Memoryless policies: Theoretical limitations and practical results.

In

International conference on simulation of adaptive behavior, volume 3, page 238, 1994.

M. L. Littman, R. S. Sutton, and S. P. Singh. Predictive representations of state. In Neural

Information Processing Systems (NIPS), 2002.

M. Liu, C. Amato, E. P. Anesta, J. D. Griﬃth, and J. P. How. Learning for decentralized
control of multiagent systems in large, partially-observable stochastic environments. In
AAAI Conference on Artiﬁcial Intelligence, pages 2523–2529, 2016.

J. Loch and S. P. Singh. Using eligibility traces to ﬁnd the best memoryless policy in

partially observable markov decision processes. In ICML, pages 323–331, 1998.

A. Mahajan. Sequential decomposition of sequential dynamic teams: applications to real-time
communication and networked control systems. PhD thesis, Univ. Michigan, Ann Arbor,
MI, Sept. 2008.

A. Mahajan. Optimal decentralized control of coupled subsystems with control sharing.

IEEE Transactions on Automatic Control (TAC), 58(9):2377–2382, Sept. 2013.

A. Mahajan and M. Mannan. Decentralized stochastic control. Annals of Operations

Research, 241:109–126, June 2016.

60

Approximate information state

A. Mahajan and D. Teneketzis. Optimal performance of networked control systems with
non-classical information structures. SIAM Journal of Control and Optimization, 48(3):
1377–1404, May 2009a.

A. Mahajan and D. Teneketzis. Optimal design of sequential real-time communication

systems. IEEE Trans. Inf. Theory, 55(11):5317–5338, Nov. 2009b.

A. Mahajan, A. Nayyar, and D. Teneketzis. Identifying tractable decentralized control
problems on the basis of information structure. In Proc. 46th Annual Allerton Conf.
Communication, Control, and Computing, pages 1440–1449, Monticello, IL, Sept. 2008.

A. Mahajan, N. C. Martins, M. C. Rotkowitz, and S. Y¨uksel. Information structures in
optimal decentralized control. In IEEE Conference on Decision and Control (CDC), pages
1291–1306. IEEE, 2012.

J. Marschak. Decision Processes, chapter Towards an Economic Theory of Organization

and Information. Wiley, New York, 1954.

J. Marschak and R. Radner. Economic theory of teams. Yale University Press, 1972.

R. A. McCallum. Overcoming incomplete perception with utile distinction memory. In

International Conference on Machine Learning (ICML), 1993.

N. Meuleau, L. Peshkin, K.-E. Kim, and L. P. Kaelbling. Learning ﬁnite-state controllers for
partially observable environments. In Uncertainty in Artiﬁcial Intelligence (UAI), page
427–436, 1999.

A. M¨uller. How does the value function of a Markov decision process depend on the transition

probabilities? Mathematics of Operations Research, 22(4):872–885, 1997.

A. M¨uller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability, 29(2):429–443, 1997.

A. Nayyar. Sequential Decision Making in Decentralized Systems. PhD thesis, University of

Michigan, 2011.

A. Nayyar, A. Mahajan, and D. Teneketzis. Optimal control strategies in delayed sharing

information structures. IEEE Trans. Autom. Control, 56(7):1606–1620, July 2011.

A. Nayyar, A. Mahajan, and D. Teneketzis. Decentralized stochastic control with partial
history sharing: A common information approach. IEEE Transactions on Automatic
Control (TAC), 58(7):1644–1658, 2013.

A. Nerode. Linear automaton transformations. Proceedings of American Mathematical

Society, 9:541–544, 1958.

F. A. Oliehoek and C. Amato. A concise introduction to decentralized POMDPs, volume 1.

Springer, 2015.

61

Subramanian, Sinha, Seraj, and Mahajan

J. M. Ooi, S. M. Verbout, J. T. Ludwig, and G. W. Wornell. A separation theorem for
IEEE Trans. Autom.

periodic sharing information patterns in decentralized control.
Control, 42(11):1546–1550, Nov. 1997. ISSN 0018-9286. doi: 10.1109/9.649699.

C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of optimal queuing network control.

Mathematics of Operations Research, 24(2):293–305, 1999.

J. Pineau, G. Gordon, S. Thrun, et al. Point-based value iteration: An anytime algorithm

for POMDPs. In IJCAI, volume 3, pages 1025–1032, 2003.

P. Poupart and C. Boutilier. Bounded ﬁnite state controllers.

In Neural Information

Processing Systems (NIPS), pages 823–830, 2004.

P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observ-

able domains. In Symp. on Artiﬁcial Intelligence and Mathematics, 2008.

P. Poupart, K.-E. Kim, and D. Kim. Closing the gap: Improved bounds on optimal
POMDP solutions. In Twenty-First International Conference on Automated Planning
and Scheduling, 2011.

E. Rachelson and M. G. Lagoudakis. On the locality of action domination in sequential
decision making. In International Symposium on Artiﬁcial Intelligence and Mathematics,
Jan. 2010.

S. T. Rachev. Probablity Metrics and the Stability of Stochastic Models. Wiley, 1991.

R. Radner. Team decision problems. The Annals of Mathematical Statistics, 33(3):857–881,

1962.

Y. A. Reznik. An algorithm for quantization of discrete probability distributions. In 2011

Data Compression Conference. IEEE, mar 2011.

M. Rosencrantz, G. Gordon, and S. Thrun. Learning low dimensional predictive representa-

tions. In International Conference on Machine Learning (ICML), 2004.

S. Ross, B. Chaib-draa, and J. Pineau. Bayes-adaptive POMDPs. In Neural Information

Processing Systems (NIPS), pages 1225–1232, 2008.

S. Ross, J. Pineau, B. Chaib-draa, and P. Kreitmann. A bayesian approach for learning and
planning in partially observable markov decision processes. Journal of Machine Learning
Research (JMLR), 12(5), 2011.

D. E. Rumelhart, G. E. Hinton, R. J. Williams, et al. Learning representations by back-

propagating errors. Nature, 323(9):533–536, 1986.

N. Saldi, T. Linder, and S. Y¨uksel. Finite approximations in discrete-time stochastic control.

Springer, 2018.

N. Sandell and M. Athans. Solution of some nonclassical lqg stochastic decision problems.

IEEE Trans. Autom. Control, 19:108–116, 1974.

62

Approximate information state

N. Sandell, P. Varaiya, M. Athans, and M. Safonov. Survey of decentralized control methods
for large scale systems. IEEE Transactions on automatic Control (TAC), 23(2):108–128,
1978.

I. Sason. On data-processing and majorization inequalities for f-divergences with applications.

Entropy, 21(10):1022, oct 2019.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-
based and RKHS-based statistics in hypothesis testing. The Annals of Statistics, 41(5):
2263–2291, Oct. 2013.

S. Seuken and S. Zilberstein. Memory-bounded dynamic programming for Dec-POMDPs. In
International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 2009–2015, 2007.

C. R. Shalizi and J. P. Crutchﬁeld. Computational Mechanics: Pattern and prediction,

structure and simplicity. Journal of Statistical Physics, 104(3):817–879, Aug. 2001.

G. Shani, R. I. Brafman, and S. E. Shimony. Forward search value iteration for POMDPs.

In IJCAI, pages 2619–2624, 2007.

G. Shani, J. Pineau, and R. Kaplow. A survey of point-based POMDP solvers. International
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 27:1–51, 2013.

S. P. Singh, M. L. Littman, N. K. Jong, D. Pardoe, and P. Stone. Learning predictive state

representations. In International Conference on Machine Learning (ICML), 2003.

R. D. Smallwood and E. J. Sondik. The optimal control of partially observable Markov

processes over a ﬁnite horizon. Operations research, 21(5):1071–1088, 1973.

T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In UA, pages

520–527, 2004.

M. T. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for POMDPs.

Journal of artiﬁcial intelligence research, 24:195–220, 2005.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. G. Lanckriet, and B. Sch¨olkopf.
Injective Hilbert space embeddings of probability measures. In Conference on Learning
Theory, 2008.

B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Sch¨olkopf, and G. R. G. Lanckriet. On
integral probability metrics, φ-divergences and binary classiﬁcation, 2009. arXiv:0901.2698.

B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Sch¨olkopf, and G. R. G. Lanckriet. On
the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6
(0):1550–1599, 2012.

C. Striebel. Suﬃcient statistics in the optimal control of stochastic systems. Journal of

Mathematical Analysis and Applications, 12:576–592, 1965.

J. Subramanian and A. Mahajan. Approximate information state for partially observed

system. In IEEE Conference on Decision and Control (CDC), Dec. 2019.

63

Subramanian, Sinha, Seraj, and Mahajan

J. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. Approximate information
state for reinforcement learning in partially observed systems. https://github.com/
info-structures/ais, 2020.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 2018.

R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Neural Information Processing
Systems (NIPS), pages 1057–1063, Nov. 2000.

M. Svoreˇnov´a, M. Chmel´ık, K. Leahy, H. F. Eniser, K. Chatterjee, I. ˇCern´a, and C. Belta.
Temporal logic motion planning using POMDPs with parity objectives: Case study paper.
In International Conference on Hybrid Systems: Computation and Control, page 233–238,
2015.

G. J. Sz´ekely and M. L. Rizzo. Testing for equal distributions in high dimensions. InterStat,

(5), 2004.

D. Szer, F. Charpillet, and S. Zilberstein. MAA*: A heuristic search algorithm for solving
decentralized POMDPs. In Conference in Uncertainty in Artiﬁcial Intelligence (UAI),
pages 576–590. AUAI Press, 2005.

C. Villani. Optimal transport: Old and New. Springer, 2008.

J. C. Walrand and P. Varaiya. Optimal causal coding-decoding problems. IEEE Trans. Inf.

Theory, 29(6):814–820, Nov. 1983.

S. D. Whitehead and L.-J. Lin. Reinforcement learning of non-markov decision processes.

Artiﬁcial Intelligence, 73(1-2):271–306, 1995.

W. Whitt. Approximations of dynamic programs, I. Mathematics of Operations Research, 3

(3):231–243, 1978.

D. Wierstra, A. Foerster, J. Peters, and J. Schmidhuber. Solving deep memory POMDPs
with recurrent policy gradients. In International Conference on Artiﬁcial Neural Networks
(ICANN), 2007.

D. Wierstra, A. F¨orster, J. Peters, and J. Schmidhuber. Recurrent policy gradients. Logic

Journal of the IGPL, 18(5):620–634, 2010.

J. D. Williams and S. Young. Partially observable Markov decision processes for spoken

dialog systems. Computer Speech & Language, 21(2):393–422, 2007.

J. K. Williams and S. P. Singh. Experimental results on learning stochastic memoryless poli-
cies for partially observable markov decision processes. In Neural Information Processing
Systems (NIPS), pages 1073–1080, 1999.

H. S. Witsenhausen. A counterexample in stochastic optimum control. SIAM Journal on

Control, 6(1):131–147, 1968.

64

Approximate information state

H. S. Witsenhausen. Separation of estimation and control for discrete time systems. Pro-

ceedings of the IEEE, 59(11):1557–1566, 1971.

H. S. Witsenhausen. Some remarks on the concept of state. In Y. C. Ho and S. K. Mitter,

editors, Directions in Large-Scale Systems, pages 69–75. Plenum, 1976.

B. Wolfe, M. R. James, and S. Singh. Learning predictive state representations in dynamical
systems without reset. In International Conference on Machine Learning (ICML), 2005.

B. Wolfe, M. R. James, and S. Singh. Approximate predictive state representations. In
International Conference on Autonomous Agents and Multiagent Systems (AAMAS),
pages 363–370, 2008.

T. Yoshikawa. Dynamic programming approach to decentralized stochastic control problems.

IEEE Trans. Autom. Control, 20(6):796 – 797, Dec. 1975.

A. Zhang, Z. C. Lipton, L. Pineda, K. Azizzadenesheli, A. Anandkumar, L. Itti, J. Pineau, and
T. Furlanello. Learning causal state representations of partially observable environments,
2019. arXiv:1906.10437.

H. Zhang. Partially observable Markov decision processes: A geometric technique and

analysis. Operations Research, 2009.

N. Zhang and W. Liu. Planning in stochastic domains: Problem characteristics and
approximation. Technical Report HKUST-CS96-31, Hong Kong Univeristy of Science and
Technology, 1996.

P. Zhu, X. Li, P. Poupart, and G. Miao. On improving deep reinforcement learning for

POMDPs, 2017. arXiv:1704.07978.

V. M. Zolotarev. Probability metrics. Theory of Probability & Its Applications, 28(2):

278–302, Jan. 1983.

65

Subramanian, Sinha, Seraj, and Mahajan

A. Comparison with the results of Abel et al. (2016) for state

aggregation in MDPs

Abel et al. (2016) introduce four models of state aggregation and derive approximation
bounds for all four. In this section, we show that one of these models, which they call
approximate model similarity may be viewed as an AIS. We also show that the approximation
bounds of Theorem 27 for this model are stronger than those derived in Abel et al. (2016)
by a factor of O(1/(1 − γ)).

Since we follow a slightly diﬀerent notation than Abel et al. (2016) and for the sake of
completeness, we start by describing the notion of approximate model similarity deﬁned in
Abel et al. (2016).

Consider an inﬁnite horizon ﬁnite-state ﬁnite-action MDP with state space S, action space
A, transition probability matrix P : S × A → ∆(S), per-step reward function r : S × A → R,
and discount factor γ.

Let ˆS be an aggregated state space and it is assumed that the following two functions
are available: a compression function q : S → ˆS and a weight function w : S → [0, 1] such
that for all ˆs ∈ ˆS, (cid:80)
s∈q−1(ˆs) w(s) = 1. Given these functions, deﬁne an aggregated MDP
with state space ˆS, action space A, transition probability function ˆP : ˆS × A → ˆS given by

ˆP (ˆs(cid:48)|ˆs, a) =

(cid:88)

(cid:88)

P (s(cid:48)|s, a)w(s),

∀ˆs, ˆs(cid:48) ∈ ˆS, a ∈ A,

s∈q−1(ˆs)

s(cid:48)∈q−1(ˆs(cid:48))

and a per-step reward ˆr : ˆS × A → R given by

ˆr(ˆs, a) =

(cid:88)

r(s, a)w(s),

∀ˆs ∈ ˆS, a ∈ A.

s∈q−1(ˆs)

Deﬁnition 34 (ε-approximate model similarity (Abel et al., 2016)) The aggregated
MDP is said to be ε-approximate model similar to the original MDP if it satisﬁes the following
two properties:

1. For all ˆs ∈ ˆS, s1, s2 ∈ q−1(ˆs), and a ∈ A, we have
(cid:12)r(s1, a) − r(s2, a)(cid:12)
(cid:12)

(cid:12) ≤ ε.

2. For all ˆs, ˆs(cid:48) ∈ ˆS, s1, s2 ∈ q−1(ˆs), and a ∈ A, we have

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
s(cid:48)∈q−1(ˆs(cid:48))

P (s(cid:48)|s1, a) −

(cid:88)

s(cid:48)∈q−1(ˆs(cid:48))

(cid:12)
(cid:12)
P (s(cid:48)|s2, a)
(cid:12)
(cid:12)

≤ ε.

Proposition 35 (Lemma 2 of Abel et al. (2016)) Let ˆπ : ˆS → A be the (deterministic)
optimal policy for the aggregated MDP. Deﬁne π : S → A by π = ˆπ ◦ q. Let V : S → R denote
the optimal value function and let V π : S → R denote the value function for policy π. Then,
for all s ∈ S

(cid:12)
(cid:12)V (s) − V π(s)(cid:12)

(cid:12) ≤

2ε
(1 − γ)2 +

2γε|S|(cid:107)r(cid:107)∞
(1 − γ)3

.

66

Approximate information state

Note that the result is presented slightly diﬀerently in Abel et al. (2016). They assume that
(cid:107)r(cid:107)∞ = 1 and simplify the above expression.

We now show an approximate model similarity is also an AIS and directly using the

result of Theorem 27 for this model gives a stronger bound than Proposition 35.

Proposition 36 Let (q, w) be such that the aggregated model is ε-approximate model similar
to the true model. Then, (q, ˆP , ˆr) is an (ε, ε|ˆS|)-AIS with respect to the total variation
distance.

Proof We ﬁrst establish (AP1). For any s ∈ S and a ∈ A,

(cid:12)r(s, a) − ˆr(q(s), a)(cid:12)
(cid:12)
(cid:12)

(a)
≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(b)
≤

(cid:88)

w(˜s)r(s, a) −

(cid:88)

w(˜s)r(˜s, a)

˜s∈q−1(q(s))

˜s∈q−1(q(s))

(cid:88)

w(˜s)(cid:12)

(cid:12)r(s, a) − r(˜s, a)(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜s∈q−1(q(s))

(c)
≤ ε

where (a) follows from the basic property of the weight function w and the deﬁnition of
the aggregated reward ˆr; (b) follows form the triangle inequality; and (c) follows from the
deﬁnition of approximate model similarity and the basic property of the weight function w.
This establishes property (AP1).

Now, we establish (AP2). Let dF denote the total variation distance. Deﬁne probability

measures µ, ν on ∆(ˆS) in the deﬁnition of (AP2), i.e., for any s ∈ S, ˆs(cid:48) ∈ ˆS, and a ∈ A,

µ(ˆs(cid:48)) :=

(cid:88)

P (s(cid:48)|s, a)

s(cid:48)∈q−1(ˆs(cid:48))
ν(ˆs(cid:48)) := ˆP (ˆs(cid:48)|q(s), a) =

(cid:88)

(cid:88)

P (s(cid:48)|˜s, a)w(˜s)

˜s∈q−1(q(s))

s(cid:48)∈q−1(ˆs(cid:48))

Now consider (see footnote 1 on page 13)

dF(µ, ν) =

=

(a)
≤

(b)
≤

(cid:88)

ˆs(cid:48)∈ˆS
(cid:88)

ˆs(cid:48)∈ˆS

|µ(ˆs(cid:48)) − ν(ˆs(cid:48))|

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
s(cid:48)∈q−1(ˆs(cid:48))

P (s(cid:48)|s, a) −

(cid:88)

(cid:88)

P (s(cid:48)|˜s, a)w(˜s)

˜s∈q−1(q(s))

s(cid:48)∈q−1(ˆs(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

ˆs(cid:48)∈ˆS

˜s∈q−1(q(s))

w(˜s)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
s(cid:48)∈q−1(ˆs(cid:48))

P (s(cid:48)|s, a) −

(cid:88)

s(cid:48)∈q−1(ˆs(cid:48))

(cid:12)
(cid:12)
P (s(cid:48)|˜s, a)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

w(˜s)ε

(cid:88)

(c)
=

ε = |ˆS|ε,

ˆs(cid:48)∈ˆS

˜s∈q−1(q(s))

ˆs(cid:48)∈ˆS

where (a) follows from triangle inequality, (b) follows from deﬁnition of approximate model
similarity and (c) follows from the basic property of the weight function. This proves (AP2).

67

Subramanian, Sinha, Seraj, and Mahajan

Lemma 37 For any MDP

span(V ) ≤

span(r)
1 − γ

.

Therefore, when dF is the total variation distance, ρF(V ) ≤ 1

2 span(r)/(1 − γ).

Proof This result follows immediately by observing that the per-step cost r(St, At) ∈
[min(r), max(r)]. Therefore, max(V ) ≤ max(r)/(1 − γ) and min(V ) ≥ min(r)/(1 − γ).

Proposition 38 Let ˆπ, π, V , and V π be deﬁned as in Proposition 36. Then, for all s ∈ S,

(cid:12)V (s) − V π(s)(cid:12)
(cid:12)

(cid:12) ≤

2ε
(1 − γ)

+

γε|ˆS| span(r)
(1 − γ)2

.

Proof This follows immediately from Theorem 27, Proposition 36 and Lemma 37.

Note that the error bounds of Propositions 36 and 38 have similar structure but the
key diﬀerence is that the bound of Proposition 38 is tighter than a factor of 1/(1 − γ) as
compared to Proposition 36. There are other minor improvements as well (|ˆS| instead of |S|
and 1

2 span(r) instead of (cid:107)r(cid:107)∞).

B. Comparison with the results of Gelada et al. (2019) for latent space

models for MDPs

Gelada et al. (2019) propose a latent space model for an MDP and show that minimizing the
losses in predicting the per-step reward and repredicting the distribution over next latent
space provides a bound on the quality of the representation. In this section, we show that
latent space representation deﬁned in Gelada et al. (2019) may be viewed as an instance of
an AIS and show that the approximation bounds of Theorem 27 are similar to those derived
in Gelada et al. (2019).

Since we follow a slightly diﬀerent notation than Gelada et al. (2019) and for the sake
of completeness, we start by describing the notion of latent space representation used in
Gelada et al. (2019).

Consider an MDP with inﬁnite horizon, ﬁnite-state and ﬁnite-action having state space
S, action space A, transition probability matrix P : S × A → ∆(S), per-step reward function
r : S × A → R and discount factor γ.

Let (ˆS, d) be a Banach space and it is assumed that we are given an embedding function
φ : S → ˆS, along with transition dynamics ˆP : ˆS×A → ∆(ˆS) and reward function ˆr : ˆS×A → R.
The MDP (cid:99)M = (ˆS, A, ˆP , ˆr, γ) along with the embedding function φ is called the latent space
model of the original MDP.

Deﬁnition 39 The MDP (cid:99)M is said to be (Lr, Lp)-Lipschitz if for any ˆs1, ˆs2 ∈ ˆS and a ∈ A,

(cid:12)
(cid:12)ˆr(ˆs1, a) − ˆr(ˆs2, a)(cid:12)

(cid:12) ≤ Lrd(ˆs1, ˆs2)

and dF( ˆP (·|ˆs1, a), ˆP (·|ˆs2, a)) ≤ Lpd(ˆs1, ˆs2),

where dF denotes the Kantorovich distance.

68

Approximate information state

Given a latent space embedding, deﬁne

ε = sup

s∈S,a∈A

(cid:12)r(s, a) − ˆr(φ(s), a)(cid:12)
(cid:12)

(cid:12) and δ = sup

dF(µ, ˆP (·|φ(s), a)),

s∈S,a∈A

where µ ∈ ∆(ˆS) given by µ(B) = P (φ−1(B)|s, a) for any Borel subset B of ˆS.

Proposition 40 (Theorem 5 of Gelada et al. (2019)) Let ˆπ : ˆS → A be the (determin-
istic) optimal policy of the latent space MDP. Deﬁne π : S → A by π = ˆπ ◦ φ. Let V : S → R
denote the optimal value function and let V π : S → R denote the value function for policy π.

If the latent space MDP (cid:99)M is (Lr, Lp)-Lipschitz, then,

(cid:12)V (s) − V π(s)(cid:12)
(cid:12)

(cid:12) ≤

2ε
1 − γ

+

2γδLr
(1 − γ)(1 − γLp)

.

We show that a latent space model is an AIS and directly using the result of Theorem 27

gives the same approximation bound.

Proposition 41 Let (cid:99)M = (ˆS, A, ˆP , ˆr, γ) be a latent space model with embedding function φ.
Then, (φ, ˆP , ˆr) is an (ε, δ)-AIS with respect to the Kantorovich distance.

Proof The result is an immediate consequence of the deﬁnition of ε and δ for latent space
model.

Lemma 42 For any (Lr, Lp)-Lipschitz MDP, if γLp < 1, then

(cid:107)V (cid:107)Lip ≤

Lr
1 − γLp

.

Therefore, when dF is the Kantorovich distance, ρF(V ) = (cid:107)V (cid:107)Lip ≤ Lr/(1 − γLp).

Proof This result follows immediately from Theorem 4.2 of Hinderer (2005).

Proposition 43 Let ˆπ, π, V , and V π be deﬁned in Proposition 40. The, for all s ∈ S,

(cid:12)V (s) − V π(s)(cid:12)
(cid:12)

(cid:12) ≤

2ε
1 − γ

+

2γδLr
(1 − γ)(1 − γLp)

.

Proof This follows immediately from Theorem 27, Proposition 40, and Lemma 42.

Note that the error bounds in Propositions 40 and 43 are exactly the same.

69

Subramanian, Sinha, Seraj, and Mahajan

C. Comparison with the results of Francois-Lavet et al. (2019) for belief

approximation in POMDPs

Francois-Lavet et al. (2019) analyze the trade oﬀ between asymptotic bias and overﬁtting
in reinforcement learning with partial observations. As part of their analysis, they express
the quality of state representation in terms of the bounds on the L1 error of the associated
belief states. We show that these approximation bounds may be viewed as an instance of
AIS-based bounds of Theorems 9 and 27. We also show that the bounds of Theorem 27 for
this model are stronger than those derived in Francois-Lavet et al. (2019) by a factor of
O(1/(1 − γ)).

Since we follow a slightly diﬀerent notation than Francois-Lavet et al. (2019) and for the
sake of completeness, we start by describing the notion of ε-suﬃcient statistics deﬁned in
Francois-Lavet et al. (2019).

Consider an inﬁnite-horizon ﬁnite-state ﬁnite-action POMDP with state space S, action
space A, observation space Y, transition probability matrix P : S × A → ∆(S), observation
matrix P y : S → ∆(Y), per-step reward r : S × A → R, and discount factor γ.

Deﬁnition 44 (ε-suﬃcient statistic (Francois-Lavet et al., 2019)) Given a family of
Banach spaces {Φt}L
t=1, an ε-suﬃcient statistic is a collection of history compression function
{φt : Ht → Φt}T
t=1 such that for any
time t and any realization ht of Ht, we have

t=1 and belief approximation functions {ˆbt : Φt → ∆(S)}T

(cid:107)ˆbt(·|φt(ht)) − bt(·|ht)(cid:107)1 ≤ ε.

Given an ε-suﬃcient statistic, Francois-Lavet et al. (2019) deﬁne an MDP with state
space ∆(S), action space A, transition probability kernel P(ˆbt+1(·|φ(ht+1)) | ˆbt(·|φ(ht)), at)
computed from the underlying POMDP, and per-step reward given by

ˆr(ˆbt(ht), at) =

(cid:88)

s∈S

r(s, at)ˆbt(s|φ(ht)).

Proposition 45 (Theorem 1 of Francois-Lavet et al. (2019)) Let {(ˆbt, φt)}T
t=1 be an
ε-suﬃcient statistic and ˆπ = (ˆπ1, ˆπ2, . . . ) be an optimal policy for the MDP described above.
Deﬁne a policy π = (π1, π2, . . . ) given by πt = ˆπt ◦ φt. Let Vt : Ht → R denote the optimal
value functions and ˆV π
: Ht → R denote the value function for policy π. Then for any initial
t
history h1 ∈ H1,

(cid:12)
(cid:12)V1(h1) − V π

1 (h1)(cid:12)

(cid:12) ≤

2ε(cid:107)r(cid:107)∞
(1 − γ)3 .

We now show that an ε-suﬃcient statistic gives rise to an AIS and directly using the

results of Theorem 27 for this model gives a stronger bound than Proposition 45.

Proposition 46 Let {(ˆbt, φt)}T
diﬀerent components of an AIS as follows:

t=1 be an ε-suﬃcient statistic. Let ˆZt = ∆(S) and deﬁne the

• history compression functions ˆσt = ˆbt ◦ φt,

70

Approximate information state

• AIS prediction kernels ˆPt(·|ˆzt, at) is given by

ˆPt(B|ˆzt, at) =

(cid:88)

yt+1∈Y

ψ(yt+1|ˆzt, at)1B{ ˆϕ(ˆzt, yt+1, at)},

where

and

ψ(yt+1|ˆzt, at) =

(cid:88)

(cid:88)

st+1∈S

st∈S

P y(yt+1|st+1)P (st+1|st, at)ˆzt(st)

ˆϕ(ˆzt, yt+1, at)(st+1) =

(cid:80)

st∈S P y(yt+1|st+1)P (st+1|st, at)ˆzt(st)
ψ(yt+1|ˆzt, at)

,

where ˆϕ is the same as the Bayes’-rule based update of the belief state,

• reward approximation functions ˆr(ˆzt, at) = (cid:80)

s∈S ˆzt(s)r(s, at).

Then, {(ˆσt, ˆPt, ˆrt)}T

t=1 is an (ε(cid:107)r(cid:107)∞, 3ε)-AIS with respect to the bounded-Lipschitz metric.

Proof We need to equip ˆZ = ∆(S) with a metric in order to deﬁne a bounded-Lipschitz
metric over ∆(ˆZ). We use the total variation as the metric and denote it by dTV. We use F to
denote {f : ∆(ˆZ) → R : (cid:107)f (cid:107)∞ +(cid:107)f (cid:107)Lip ≤ 1} and denote the corresponding bounded-Lipschitz
metric over ∆(ˆZ) by dF.

We ﬁrst establish (AP1). For any time t, realization ht of history Ht, and action at ∈ A,

we have

(cid:12)E[r(St, at) | Ht = ht, At = at] − ˆrt(ˆσt(ht), at)(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

s∈S

r(s, at)bt(s|ht) −

(cid:88)

s∈S

(cid:12)
(cid:12)
r(s, at)ˆbt(s|φ(ht))
(cid:12)
(cid:12)

(a)
≤ (cid:107)r(cid:107)∞dTV(bt, ˆbt)
(b)
≤ ε(cid:107)r(cid:107)∞

where (a) follows from (10) and the fact that for total variation distance ρTV(r) ≤ (cid:107)r(cid:107)∞;
and (b) follows from deﬁnition of ε-suﬃcient statistic.

Before establishing (AP2), we note that ˆϕ is the Bayes’-rule based update of the true

belief; therefore,

bt+1(·|ht+1) = ˆϕ(bt(·|ht), yt+1, at).
For ease of notation, we use bt(·) and ˆbt(·) instead of bt(·|ht) and ˆbt(·|φt(ht)), when the
conditioning is clear from context.

Now consider µt and νt as deﬁned in the deﬁnition of (AP2). In particular, for any Borel

set B,

µt(B) =

(cid:88)

ψ(yt+1|bt, at)1B{ˆbt+1(·|φ(ht, yt+1, at))}

yt+1∈Y
νt(B) = ˆPt(B|ˆzt, at).

71

Subramanian, Sinha, Seraj, and Mahajan

We also deﬁne an additional measure ξt given by

ξt(B) =

(cid:88)

yt+1∈Y

ψ(yt+1|bt, at)1B{ ˆϕ(bt, yt+1, at)},

Now, by the triangle inequality

dF(µt, νt) ≤ dF(µt, ξt) + dF(ξt, νt).

(74)

Now consider the ﬁrst term of (74):

(cid:12)
(cid:12)
dF(µt, ξt) = sup
(cid:12)
(cid:12)
f ∈F
(cid:12)
(cid:12)
= sup
(cid:12)
(cid:12)
f ∈F

(cid:90)

ˆZ

f dµt −

(cid:90)

ˆZ

f dξt

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

yt+1∈Y

f (ˆbt+1(·|φ(ht, yt+1, at)))ψ(yt+1|bt, at)

(cid:88)

−

yt+1∈Y

(cid:12)
(cid:12)
f (bt+1(·|ht, yt+1, at))ψ(yt+1|bt, at)
(cid:12)
(cid:12)

dTV(ˆbt+1(·|φ(ht+1)), bt+1(·|ht+1))ψ(yt+1|ht, at)

(a)
≤

(cid:88)

yt+1∈Y

(b)
≤ ε

(75)

where (a) follows from triangle inequality and the fact that slope of f is bounded by 1;
and (b) follows from the deﬁnition of ε-suﬃcient statistic (see footnote 1 on page 13). Now
consider the second term of (74) (for ease of notation, we use bt(·) instead of bt(·|ht)):

f ( ˆϕ(ˆzt, yt+1, at))ψ(yt+1|ˆzt, at)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ( ˆϕ(ˆzt, yt+1, at))ψ(yt+1|bt, at)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

yt+1∈Y

(cid:88)

yt+1∈Y

(cid:12)
(cid:12)
dF(ξt, νt) = sup
(cid:12)
(cid:12)
f ∈F
(cid:12)
(cid:12)
= sup
(cid:12)
(cid:12)
f ∈F

(cid:90)

ˆZ

f dξt −

(cid:90)

ˆZ

f dνt

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

f ( ˆϕ(bt, yt+1, at))ψ(yt+1|bt, at) −

f ( ˆϕ(bt, yt+1, at))ψ(yt+1|bt, at) −

(cid:88)

yt+1∈Y
(cid:12)
(c)
(cid:12)
≤ sup
(cid:12)
(cid:12)
f ∈F
yt+1∈Y
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup
f ∈F

(cid:88)

yt+1∈Y

f ( ˆϕ(ˆzt, yt+1, at))ψ(yt+1|bt, at) −

(cid:12)
(cid:12)
f ( ˆϕ(ˆzt, yt+1, at))ψ(yt+1|ˆzt, at)
(cid:12)
(cid:12)

(cid:88)

yt+1∈Y

(d)
≤

(cid:88)

dTV( ˆϕ(bt, yt+1, at), ˆϕ(ˆzt, yt+1, at))ψ(yt+1|bt, at)

yt+1∈Y
+ κF,TV( ˆϕ(ˆzt, ·, at)) dTV(ψ(·|bt, at), ψ(·|ˆzt, at)),

(76)

where (c) follows from the triangle inequality; the ﬁrst step of (d) follows from an argument
similar to step (a) of (75); and the second part of (d) follows from (13).

72

Approximate information state

Now, we obtain bounds for both terms of (76). For yt+1 ∈ Y, deﬁne

ξy
t (yt+1) := ψ(yt+1|bt, at) =

νy
t (yt+1) := ψ(yt+1|ˆzt, at) =

(cid:88)

(cid:88)

st+1∈S
(cid:88)

st∈S
(cid:88)

st+1∈S

st∈S

P y(yt+1|st+1)P (st+1|st, at)bt(st|ht),

P y(yt+1|st+1)P (st+1|st, at)ˆzt(st),

Total variation is also an f -divergence4 therefore, it satisﬁes the strong data processing
t may be viewed as outputs of a
t , the channel input is distributed according to bt(·|ht)
t , the channel input is distributed according to ˆzt. Therefore, from the data

inequality.5 Note that the deﬁnition of both ξy
“channel” from st to yt+1. In case of ξy
and in case of νy
processing inequality,

t and νy

t , νy
where the last inequality follows from the deﬁnition of ε-suﬃcient statistic.

t ) ≤ dTV(bt(·|ht), ˆzt) ≤ ε

dTV(ξy

(77)

A similar argument can be used to bound dTV( ˆϕ(bt, yt+1, at), ˆϕ(ˆzt, yt+1, at)). In particular,
we can think of ˆϕ(·, yt+1, at) as a channel from st to st+1. Then, by the data processing
inequality,

dTV( ˆϕ(bt, yt+1, at), ˆϕ(ˆzt, yt+1, at)) ≤ dTV(bt, ˆzt) ≤ ε

(78)

where the last inequality follows from the deﬁnition of ε-suﬃcient statistic.

The ﬁnal part of (76) that needs to be characterized is κF,TV( ˆϕ(ˆzt, ·, at)). From (12)

κF,TV( ˆϕ(ˆzt, ·, at)) = sup
f ∈F

ρTV(f ◦ ˆϕ(ˆzt, ·, at)) ≤ sup
f ∈F

(cid:107)f ◦ ˆϕ(ˆzt, ·, at)(cid:107)∞ ≤ 1.

Substituting this bound along with (77) and (78) in (76), we get dF(ξt, νt) ≤ 2ε. Substituting
this along with (75) in (74), we get that dF(µt, νt) ≤ 3ε. Hence (AP2) is satisﬁed.

Lemma 47 For any POMDP,

ρF(V ) = (cid:107)V (cid:107)∞ + (cid:107)V (cid:107)Lip ≤

2(cid:107)r(cid:107)∞
1 − γ

.

Proof The result follows immediately from the sup norm on the value function (Lemma 37
and the bounds on the Lipschitz constant of the value function (Lemma 1 of Lee et al.
(2008)).

4. Let f : R≥0 → R be a convex function such that f (1) = 0. Then the f -divergence between two measures

µ and ν deﬁned on a measurable space X is given by
(cid:90)

Df (µ(cid:107)ν) =

X

(cid:17)

f

(cid:16) dµ
dν

dν.

Total variation is a f -divergence with f (x) = |x − 1| (also see footnote 1 on page 1). Sriperumbudur et al.
(2009) showed that total variation is the only non-trivial IPM which is also an f -divergence.

5. Let X and Y be measurable spaces, µ and ν be measures on X and P : X → ∆(Y) be a stochastic kernel
from X to Y. We use µP to denote the measure µY on Y given by µY(dy) = (cid:82)
X P (dy|x)µ(dx). Similar
interpretation holds for νP . Then, the strong data processing inequality (Sason, 2019) states that for any
f -divergence, Df (µP (cid:107)νP ) ≤ Df (µ(cid:107)ν).

73

Subramanian, Sinha, Seraj, and Mahajan

Proposition 48 Let ˆπ, π, V , and V π be as deﬁned in Proposition 45. Then, for any initial
history h1 ∈ H1,

(cid:12)V (h1) − V π(h1)(cid:12)
(cid:12)

(cid:12) ≤

2ε(cid:107)r(cid:107)∞
(1 − γ)

+

6γε(cid:107)r(cid:107)∞
(1 − γ)2 .

Proof This follows immediately from Theorem 27, Proposition 46, and Lemma 47.

Note that the error bounds of Propositions 45 and 48 have similar structure but the key

diﬀerence is that the bound of Proposition 48 is tighter by a factor of 1/(1 − γ).

D. Comparison with the results of Chandak et al. (2020) on lifelong

learning for time-varying action spaces

Lifelong learning refers to settings where a reinforcement learning agent adapts to a time-
varying environment. There are various models for lifelong learning and Chandak et al.
(2020) recently proposed a model where the action spaces change over time. The environment
has an underlying ﬁnite state space S, ﬁnite action space A, and reward r : S → R. Note
that the reward depends only on the current state and not the current action.

It is assumed that there is an underlying ﬁnite dimensional representation space E and
for any feasible action a ∈ A, there is an underlying representation e ∈ E. This relationship is
captured via an invertible map φ, i.e., a = φ(e). There is a transition kernel P : S × E → ∆(S)
with respect to this representation space. This induces a transition kernel P a : S × A → ∆(S)
with respect to the action, where P a(s(cid:48)|s, a) = P (s(cid:48)|s, φ−1(a)).
It is assumed that the
transition kernel P is ρ-Lipschitz, i.e., for all s, s(cid:48) ∈ S and ei, ej ∈ E,

(cid:13)P (s(cid:48)|s, ei) − P (s(cid:48)|s, ej)(cid:13)
(cid:13)

(cid:13)1 ≤ ρ(cid:107)ei − ej(cid:107)1.

Chandak et al. (2020) consider inﬁnite horizon discounted setup with discount factor γ.

Initially, the RL agent is not aware of the action space and learns about the actions in
discrete stages indexed by k ∈ Z≥0. At stage k, the agent becomes aware of a subset Uk
of E, where Uk ⊇ Uk−1. Thus, the environment at stage k may be modelled as an MDP
Mk = {S, Ak, P a

k , r}, where Ak = {φ(e) : e ∈ Uk} and P a

k (s(cid:48)|s, a) = P (s(cid:48)|s, φ−1(a)).

Two main results are established in Chandak et al. (2020). The ﬁrst one is the following.

Proposition 49 (Theorem 1 of Chandak et al. (2020)) Let πk and V πk denote the
optimal policy for MDP Mk and its performance. Let V denote the value function for the
hypothetical model when the agent has access to all actions. Let

Then, for any s ∈ S,

ηk = sup

ai,aj ∈Ak

(cid:107)φ−1(ai) − φ−1(aj)(cid:107)1.

V (s) − V πk (s) ≤

γρηk(cid:107)r(cid:107)∞
(1 − γ)2 .

We now show that this result may be viewed as a corollary of Corollary 19. In particular,

we have the following.

74

Approximate information state

Lemma 50 The action set Ak may be viewed as a “quantization” of A using a function
ψ : A → Ak, which maps any action a = φ(e) ∈ A to action a(cid:48) = φ(e(cid:48)) ∈ Ak such that
e(cid:48) = arg mine(cid:48)(cid:48)∈Uk (cid:107)e − e(cid:48)(cid:48)(cid:107)1. Then, ψ is (0, ρηk)-action-quantizer with respect to the total
variation distance.

Proof Since the per-step reward does not depend on the action, there is no approximation
error in the reward and, therefore, ε = 0. Now note that for any s ∈ S and a ∈ A, we have

dTV(P a(·|s, a), P a(·|s, ψ(a))) ≤ sup

ei,ej ∈Uk

(cid:107)P (·|s, ei) − P (·|s, ej)(cid:107)1 ≤ ρηk

where the last equality follows from the ρ-Lipschitz continuity of P and the deﬁnition of ηk.
Thus, δ = ρηk.

Proposition 51 Let πk, V πk and V be as deﬁned in Proposition 49. Then, for any s ∈ S,

V (s) − V πk (s) ≤

γρηk span(r)
2(1 − γ)2

Proof The result can be established from the following observations: (i) The result of
Corollary 19 continues to hold in the inﬁnite horizon discount reward setup with αt replaced
by (ε + γρF( ˆV ∗)δ)/(1 − γ). This can be established in a manner similar to Theorem 27.
(ii) From Lemma 37, we know that for total variation distance ρF( ˆV ∗) ≤ 1
2 span(r)/(1 − γ).
The result follows from substituting the values of (ε, δ) from Lemma 50 and the value of
ρF( ˆV ∗) from (ii) in (i).

Note that if the rewards r(s) belongs in a symmetric interval, say [−Rmax, Rmax], as is
assumed in Chandak et al. (2020), the result of Proposition 51 matches that of Proposition 49.
The second result of Chandak et al. (2020) is for the setting when the mapping φ is not
known. They assume that the agent selects some ﬁnite dimensional representation ˆE and, for
every k, parameterizes the policy using two components: (i) a map β : S → ∆(ˆE) and (ii) an
estimator ˆφk : ˆE → ∆(Ak). Then the action at any state St ∈ S is chosen by ﬁrst sampling
ˆe ∼ β(s) and then choosing the action a ∼ ˆφk(ˆe). The second main result in Chandak et al.
(2020) is the following.6

Proposition 52 (Theorem 2 of Chandak et al. (2020)) Let ˆπk denote the best overall
policy that can be represented using the above structure, V ˆπk denotes its performance, and
V denote the value function when the agent has access to the complete model. Suppose there
exists a ζ ∈ R≥0, β : S → ∆(ˆE) and ˆφk : ˆE → ∆(Ak), such that for

sup
s∈S,ak∈Ak

KL(P a(·|s, ak)(cid:107)P a(·|s, ˆA)) ≤ ζ 2

k /2,

6. This result is stated slightly diﬀerent in Chandak et al. (2020) using an inverse dynamics function
ϕ : S × S → ∆(ˆE), where e ∼ ϕ(s, s(cid:48)) is a prediction of a latent action e which might have caused the
transition from s to s(cid:48). However, the bounds hold for the simpler form presented here as well.

75

Subramanian, Sinha, Seraj, and Mahajan

where ˆA ∼ ˆφk( ˆE) and ˆE ∼ β(s). Then, for any s ∈ S,

V (s) − V πk (s) ≤

γ(ρηk + ζk)(cid:107)r(cid:107)∞
(1 − γ)2

.

We now show that this result may be viewed as a corollary of Corollary 19. In particular,

we have the following.

Lemma 53 The action set ˆE may be viewed as a “compression” of the “quantized” action
set Ak. In particular, let ψ : A → Ak be as deﬁned in Lemma 50. Then, the function ˆφ−1
k ◦ ψ
is a (0, ρηk + ζk)-action- quantizer with respect to the total variation distance.

Proof As argued in the proof of Lemma 50, since the reward function does not depend
on action, ε = 0. Now, recall that from Pinsker’s inequality, for any distribution µ and ν,
dTV(µ, ν) ≤ (cid:112)2DKL(µ(cid:107)ν). Thus,

sup
s∈S,ak∈Ak

dTV(P a(·|s, ak), P a(·|s, ˆA)) ≤ ζk

where ˆA ∼ ˆφk( ˆE) and ˆE ∼ β(s). Now, by the triangle inequality, for any s ∈ S and a ∈ A

dTV(P a(·|s, a), P a(·|s, ˆA)) ≤ dTV(P a(·|s, a), P a(·|s, ψ(a))) + dTV(P a(·|s, ψ(a)), P a(·|s, ˆA))

≤ ρηk + ζk,

Thus, δ = ρηk + ζk.

Proposition 54 Let ˆπk, V ˆπk and V be as deﬁned in Proposition 49. Then, for any s ∈ S,

V (s) − V ˆπk (s) ≤

γ(ρηk + ζk) span(r)
2(1 − γ)2

Proof The proof is similar to the proof of Proposition 51, where we replace the values of
Lemma 50 with those of Lemma 53.

As before, if the rewards r(s) belongs in a symmetric interval, say [−Rmax, Rmax], as is
assumed in Chandak et al. (2020), the result of Proposition 54 matches that of Proposition 52.

E. Convergence of the PORL algorithm

In this section, we discuss the convergence of the PORL algorithm presented in Sec. 6.2
and 6.3. The proof of convergence relies on multi-timescale stochastic approximation Borkar
(1997) under conditions similar to the standard conditions for convergence of policy gradient
algorithms with function approximation stated below:

Assumption 2 The following conditions are satisﬁed:
1. All network parameters ( ¯ξk, ζk, θk) lie in convex and bounded subsets of Euclidean spaces.

76

Approximate information state

2. The gradient of the loss function ∇ ¯ξL( ¯ξk) of the state approximator is Lipschitz in ¯ξk,
the gradient of the TD loss ∇ζLTD( ¯ξk, θk, ζk) and the policy gradient (cid:98)∇θk J( ¯ξk, θk, ζk) is
Lipschitz in ( ¯ξk, θk, ζk) with respect to the sup norm.

3. All the gradients—∇ ¯ξL( ¯ξk) at the state approximator; ∇ζLTD( ¯ξk, θk, ζk) at the critic; and
(cid:98)∇θk J( ¯ξk, θk, ζk) at the actor—are unbiased with bounded variance. Furthermore, the critic
and the actor function approximators are compatible as given in Sutton et al. (2000), i.e.,

∂Qζk ( ˆZt, At)
∂ζ

=

1
πθk ( ˆZt, At)

∂πθk ( ˆZt, At)
∂θ

.

4. The learning rates are sequences of positive numbers {ak}k≥0, {bk}k≥0, {ck}k≥0 that satisfy:
k < ∞, limk→∞ ck/ak = 0,

k < ∞, (cid:80) c2

k < ∞, (cid:80) b2

(cid:80) ak = ∞, (cid:80) bk = ∞, (cid:80) ck = ∞, (cid:80) a2
and limk→∞ bk/ck = 0.

Assumption 3 The following regularity conditions hold:

1. The ODE corresponding to θ in (73) is locally asymptotically stable.

2. The ODEs corresponding to ¯ξ and ζ in (73) are globally asymptotically stable. In
addition, the ODE corresponding to ζ has a ﬁxed point which is Lipschitz continuous
in θ.

The proposed RL framework has the following convergence guarantees.

Theorem 55 Under Assumptions 2 and 3, along any sample path, almost surely we have
the following:

(a) The iteration for ¯ξ in (73) converges to a state estimator that minimizes the loss

function L( ¯ξ);

(b) The iteration for ζ in (73) converges to a critic that minimizes the error with respect

to the true Q-function;

(c) The iteration for θ in (73) converges to a local maximum of the performance J( ¯ξ∗, ζ ∗, θ),

where ¯ξ∗ and ζ ∗ are the converged values of ¯ξ and ζ.

Proof The assumptions satisfy all the four conditions stated in (Leslie, 2004, page 35),
(Borkar, 1997, Theorem 23). The proof follows from combining this two-time scale algorithm
proof with the fastest third time-scale of learning the state representation. Due to the
speciﬁc choice of learning rates, the state representation algorithm sees a stationary actor
and critic, while the actor and critic in turn see a converged state approximator ietration due
to its faster learning rate. The convergence of the state approximator follows from (Borkar,
2008, Theorem 2.2) and the fact that the model satisﬁes conditions (A1)–(A4) of (Borkar,
2008, pg 10–11). The Martingale diﬀerence condition (A3) of Borkar (2008) is satisﬁed due
to the unbiasedness assumption of the state approximator. The result then follows from
by combining the theorem given in (Leslie, 2004, page 35), (Borkar, 1997, Theorem 23)
along with (Borkar, 2008, Theorem 2.2) and using a third fastest time scale for the state
apparoximator.

77

Subramanian, Sinha, Seraj, and Mahajan

F. Details about the network architecture, training, and hyperparameters

As explained in Sec. 7, the AIS-generator consists of four components: the history compression
function ˆσ, the AIS update function ˆϕ, the reward prediction function ˆr, and the observation
prediction kernel ˆP y. We model the ﬁrst as an LSTM, where the memory update unit of
LSTM acts as ˆϕ. We model ˆr, ˆP y, and the policy ˆπ as feed-forward neural networks. We
describe the details for each diﬃculty class of environment separately. In the description
below, we use Linear(n, m) to denote a linear layer Tanh(n, m) to denote a tanh layer,
ReLU(n, m) to denote a ReLU layer, and LSTM(n, m) to denote an LSTM layer, where n
denotes the number of inputs and m denotes the number of outputs of each layer. The size
of the input of the outputs depend on the size of the observation and action spaces, which
we denote by nO and nA, respectively as well as on the dimension of AIS and for the case of
minigrid environments, the dimension of the latent space for observations, we denote by d ˆZ
and dO. We also use Conv2d(IC, OC, (F Sx, F Sy)) to denote a 2D convolutional layer with
IC, OC, (F Sx, F Sy) represent the number of input channels, output channels and kernel
size (along x and y) respectively. Note that the strides are the same as the kernel size in
this case. ELU represents Exponential Linear Unit and is used to model the prediction of
variance. Finally, GMM(ncomp) represents a Gaussian Mixture Model with ncomp Gaussian
components. Most of the details are common for both the AIS+KL and the AIS+MMD
cases, we make a distinction whenever they are diﬀerent by indicating KL or MMD.

F.1 Details for low dimensional environments:

• Environment Details:

Environment Discount No. of actions No. of obs.

Voicemail
Tiger
CheeseMaze

γ

0.95
0.95
0.7

nA

3
3
4

nO

2
2
7

The discount factor for CheeseMaze is chosen to match with standard value used in
that environment (McCallum, 1993).

• AIS and Network details:
• Dimensions of AIS (d ˆZ)
• Weight in AIS loss (λ) (KL)

Weight in AIS loss (λ) (MMD)

ˆσ

40
0.0001
0.001

:
:
:

ˆr

ˆP y

ˆπ

Linear(nO + nA + 1, d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(d ˆZ, d ˆZ)

⇒
Tanh(d ˆZ, d ˆZ)
⇒
LSTM(d ˆZ, d ˆZ)

Tanh( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, 1)

2 d ˆZ)

2 d ˆZ)

Tanh( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, nO)
⇒
Softmax

⇒
Tanh(d ˆZ, d ˆZ)
⇒
Linear(d ˆZ, nA)
⇒
Softmax

78

Approximate information state

• Training details: As explained in Section 6.3, we update the parameters after a
rollout of T , which we call a training batch. The choice of parameters for the training
batch are as follows:
• Samples per training batch
• Number of training batches

200
105

:
:

In addition, we use the following learning rates:

• AIS learning rate
• Policy learning rate (KL)

Policy learning rate (MMD)

: ADAM(0.003)
: ADAM(0.0006)
: ADAM(0.0008)

In the above description, we use ADAM(α) to denote the choice of α parameter of
ADAM. All other parameters have their default value.

• Evaluation details:

• No. of batches after which evaluation is done
• Number of rollouts per evaluation

:
:

500
50

F.2 Details for moderate dimensional environments:

• Environment Details:

Environment

Discount No. of actions No. of obs.
nA

nO

γ

Drone Surveillance
Rock Sampling

0.99
0.99

5
8

10
3

• AIS and Network details:

• Dimensions of AIS (d ˆZ)
• Weight in AIS loss (λ) (KL)

Weight in AIS loss (λ) (MMD)

ˆσ

128
0.0001
0.001

:
:
:

ˆr

ˆP y

ˆπ

LSTM(nO + nA + 1, d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(d ˆZ, nA)

ReLU( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, 1)

2 d ˆZ)

⇒
Softmax

2 d ˆZ)

ReLU( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, nO)
⇒
Softmax

• Training details: As explained in Section 6.3, we update the parameters after a
rollout of T , which we call a training batch. The choice of parameters for the training
batch are as follows:
• Samples per training batch
• Number of training batches

200
105

:
:

79

Subramanian, Sinha, Seraj, and Mahajan

In addition, we use the following learning rates:

• AIS learning rate
• Policy learning rate

: ADAM(0.003)
: ADAM(0.0007)

In the above description, we use ADAM(α) to denote the choice of α parameter of
ADAM. All other parameters have their default value.

• Evaluation details:

• No. of batches after which evaluation is done
• Number of rollouts per evaluation

:
:

500
100

F.3 Details for high dimensional environments:

• Environment Details:

Note that here nO represents the number of possible observations that a general
minigrid environment can have. With the actual rules of the environment plugged
in, this number is smaller since some combinations of the encoded observation are
not possible. The actual input that we get from the environment is a vector of size
147 (dO) which is basically an observation grid of 7 × 7 with 3 channels containing
characteristic information about the observation.

Environment

Discount No. of actions

Minigrid Envs

γ

0.99

nA

7

No. of obs.
nO
(6 × 11 × 3)7×7

Obs. dimen.
dO

7 × 7 × 3

• Autoencoder (q) details:

:
• Latent space dimensions (dL)
• Type of autoencoder used
: Basic autoencoder
• Reconstruction Loss Criterion Used : Mean Square Error

64

q

Linear(dO, 3

2 dL)

ReLU( 3

⇒
2 dL, 3
⇒
Linear( 3
2 dL, dL)

2 dL)

• AIS and Network details:

• Dimensions of AIS (d ˆZ)
• Weight in AIS loss (λ)
• Number of GMM components used (ncomp) (only for KL)

:
:
:

128
0.1
5

80

Approximate information state

ˆσ

ˆr

ˆP y

ˆπ

LSTM(dL + nA + 1, d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(nA + d ˆZ, 1

2 d ˆZ) Linear(d ˆZ, d ˆZ)

ReLU( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, 1)

2 d ˆZ)

ReLU( 1

⇒
2 d ˆZ, 1
⇒
Linear( 1
2 d ˆZ, dL)

2 d ˆZ)

⇒
ReLU(d ˆZ, d ˆZ)
⇒
Linear(d ˆZ, nA)
⇒
Softmax

For KL, ˆP y is replaced by the following while other networks remain the same:

ˆP y

Linear(nA + d ˆZ, 1

2 d ˆZ)

Linear( 1

2 d ˆZ, dLncomp) ELU(Linear( 1

2 d ˆZ, dLncomp)) + 1 + 10−6

Softmax(Linear( 1

2 d ˆZ, ncomp))

ReLU( 1

⇒
2 d ˆZ, 1
⇒

2 d ˆZ)

⇒
GMM(ncomp)

Note that the third layer generates the mean vector of each component, the diagonal
vector for variance of each component and the mixture weights of each component of
the GMM model in the last layer.

• Training details: As explained in Section 6.3, we update the parameters after a
rollout of T , which we call a training batch. The choice of parameters for the training
batch are as follows:
• Samples per training batch
• Number of training batches

:
:

(MGKCS3R3, MGOM1Dl, MGOM1Dlh)
(others)

200
2 × 105
105

In addition, we use the following learning rates:

• AIS learning rate
• Policy learning rate

: ADAM(0.001)
: ADAM(0.0007)

In the above description, we use ADAM(α) to denote the choice of α parameter of
ADAM. All other parameters have their default value.

• Evaluation details:

• No. of batches after which evaluation :

is done

• Number of rollouts per evaluation

:

5000
1000
20

(MGKCS3R3, MGOM1Dl, MGOM1Dlh)
(others)

F.4 Details for PPO with LSTM and Critic:

• Environment Details:

The environment details are the same as mentioned previously.

81

⇒
⇒
⇒
⇒
Subramanian, Sinha, Seraj, and Mahajan

• Network details:

– Low and moderate dimensionality environments:

Feature Extractor

Actor Head

Critic Head

LSTM(nO, nO)

Linear(nO, 64) Linear(nO, 64)

⇒
Tanh(64, 64)
⇒
Linear(64, nA)
⇒
Softmax

⇒
Tanh(64, 64)
⇒
Linear(64, 1)

– High dimensionality environments:

• Observation tensor
• Embedding size (dE)

:
:

7 × 7 × 3
64

Conv. Feature Extractor

Actor Head

Critic Head

Linear(dE, dE) Linear(dE, dE)

⇒
Tanh(dE, dE)
⇒
Linear(dE, nA)
⇒
Softmax

⇒
Tanh(dE, dE)
⇒
Linear(dE, 1)

Conv2d(3, 1
4 dE, (2, 2))
⇒
ReLU
⇒
MaxPool2d
⇒
4 dE, 1
⇒
ReLU
⇒

2 dE, (2, 2))

Conv2d( 1

Conv2d( 1

2 dE, dE, (2, 2))

⇒
ReLU
⇒
LSTM(dE, dE)

• Training details:

• Number of parallel actors
• Number of training batches

:
:

:
• Epochs per training batch
:
• Samples per training batch
:
• Frames per parallel actor
• GAE (λGAE)
:
• Trajectory recurrence length :

64
4 × 107
2 × 107
4
1280
40
0.99
20

(MGKCS3R3, MGOM1Dl, MGOM1Dlh)
(others)

In addition, we use ADAM with the following details:

• Learning rate α
• ADAM parameter (cid:15)

:
:

0.0001
0.00001

82

Approximate information state

• Evaluation details:

• No. of batches after which evaluation

is done

• Rollouts used for evaluation

200

:
: All recent episodes completed by all actors

F.5 Details about hyperparameter tuning

Hyperparameter tuning was carried out by searching a grid of values, but exhaustive grid
search was not carried out due to the prohibitive computational cost. Instead, coarse values
were used initially as starting points and ﬁner tuning was done around promising values,
which was essentially an iterative process of performing experiments, observing results and
trying similar parameters to the ones generating good results. Hyperparameters observed
in each previous environment class (low, moderate, high dimensionality) were used as a
starting point for the search in the new environment class.

Performance was quite sensitive to diﬀerent learning rates used for the AIS and policy in
most environments. Performance generally improved or remained the same when a larger
AIS State Size was used (values considered were 128, 256, 512 for moderate/high-dimensional
environments and 5, 10, 20, 40 for low-dimensional environments), although in some cases, it
was more unstable during training. λ values considered were between 0 and 1 and generally
only made a diﬀerence (in terms of performance results) when the rewards were very large.
The choice of activation function between ReLU and Tanh did not seem to make a signiﬁcant
diﬀerence for the considered environments.

83

