StructFormer: Learning Spatial Structure
for Language-Guided Semantic Rearrangement of Novel Objects

Weiyu Liu1, Chris Paxton2, Tucker Hermans2,3, and Dieter Fox2,4

1
2
0
2

t
c
O
9
1

]

O
R
.
s
c
[

1
v
9
8
1
0
1
.
0
1
1
2
:
v
i
X
r
a

Fig. 1: StructFormer rearranges unknown objects into semantically meaningful spatial structures based on high-level language instructions and partial-view
point cloud observations of the scene. The model use multi-modal transformers to predict both which objects to manipulate and where to place them.

Abstract— Geometric organization of objects into semanti-
cally meaningful arrangements pervades the built world. As
such, assistive robots operating in warehouses, ofﬁces, and
homes would greatly beneﬁt from the ability to recognize and
rearrange objects into these semantically meaningful structures.
To be useful, these robots must contend with previously unseen
objects and receive instructions without signiﬁcant program-
ming. While previous works have examined recognizing pair-
wise semantic relations and sequential manipulation to change
these simple relations none have shown the ability to arrange
objects into complex structures such as circles or table settings.
To address this problem we propose a novel transformer-
based neural network, StructFormer, which takes as input a
partial-view point cloud of the current object arrangement and
a structured language command encoding the desired object
conﬁguration. We show through rigorous experiments that
StructFormer enables a physical robot to rearrange novel ob-
jects into semantically meaningful structures with multi-object
relational constraints inferred from the language command.

I. INTRODUCTION

Organizing objects into complex, semantically meaningful
arrangements has many real-world applications, such as set-
ting the table, organizing books, and loading the dishwasher.
Indeed, the organizing task received the second most internet

1Georgia Tech. 2NVIDIA. 3Univ. of Utah. 4Univ. of Washington

searches on the comprehensive chore list from [1] sug-
gesting its practical importance for future domestic robots.
More generally rearrangement was recently proposed as a
benchmark for embodied AI [2]. However, to be broadly
deployed these robots should ideally receive instructions in
a way that doesn’t overly burden the operator assigning the
task. In this work, we focus on the problem of semantic
rearrangement, where a robot must move a set of novel
objects to form a spatial structure that satisﬁes a high-level
language instruction such as put the mugs in a row, build a
circle, and set the table as shown in Fig. 1.

Successful rearrangement manipulation in unstructured
environments requires a robot to jointly reason about ge-
ometric and semantic properties of these novel objects and
the objects’ physical interactions [2], [3]. Realistic settings
additionally present objects not required to accomplish the
task requiring the robot to reason about additional relations
between structures of interest and distractor objects. Finally,
the robot needs an encoding of its goal compatible with
efﬁcient planning, while also being easily provided by the
operator. Language provides an obvious input modality for
untrained users to specify goals [4]; however, it brings with it
challenges in inferring the implied object conﬁguration and
generating representations interpretable by the robot.

Rearrange objects that are smaller than the green glass pantower, top, left, westline, top, left, largecircle, top, right, large, northRearrange objects that have the same color as the glass staplertower, top, right, westline, bottom, left, largecircle, top, left, small, westRearrange yellow objectscircle, bottom, right, mediumcircle, top, middle, mediumcircle, top, middle, largeRearrange yellow objectsRearrange objects that have the same material as the blue objectcircle, bottom, right, largecircle, bottom, middle, mediumcircle, bottom, middle, largeSet the tableRearrange objects that are smaller than the green glass pantower, top, left, westline, top, left, largecircle, top, right, large, northRearrange objects that have the same color as the glass staplertower, top, right, westline, bottom, left, largecircle, top, left, small, westRearrange yellow objectscircle, bottom, right, mediumcircle, top, middle, mediumcircle, top, middle, largeRearrange yellow objectsRearrange objects that have the same material as the blue objectcircle, bottom, right, largecircle, bottom, middle, mediumcircle, bottom, middle, largeSet the tablePut objects in a lineSet the tablePut objects in a lineMake a circle 
 
 
 
 
 
Previously, researchers have worked on modeling, inter-
preting, and grounding relations between scene elements [5],
[6], [7], [8]. Such reasoning can enable robots to move a
query object with respect to an anchor object to satisfy a
desired spatial relations [9], [10], [3], [11], [12], [13]. More
complex spatial structures from language instructions have
be generated in a blocks world environment by chaining
these binary relation changing manipulations [14]. Others
have leveraged spatio-semantic relations between pairs of
objects as an abstraction for planning [15], [16], [17], [18].
Alternatively, robots can perform multi-object manipulation
by directly learning to transform object groups without
associated semantic reasoning [19].

In contrast to the pairwise reasoning of prior work, our
model explicitly reasons about multi-object semantic rela-
tions given a structured language instruction to specify the
goal. We treat the semantic rearrangement planning problem
as a sequential prediction task using a novel transformer-
based architecture named StructFormer. We use transformer
encoders to build a contextualized representation of abstract
concepts expressed in language instructions as well as se-
mantic and geometric multi-object relations. This enables
StructFormer to reason over a variety of objects of varying
number without access to object models. The encoder can
directly predict what objects to move and also provide a
context for an autoregressive transformer decoder to predict
where the objects should go.

To train and validate our approach, we procedurally gen-
erate a dataset of four different structures—circles, lines,
towers, and table settings—using models from 335 real-
world objects. We tested our method both on novel objects
and arrangements in simulation, and on a physical robot.
Our experiments show that StructFormer’s ability to directly
reason over multiple objects enables higher manipulation
success compared to a model that only reasons about pair-
wise relations between objects. For further videos, gifs,
and supplemental material please visit our website https:
//sites.google.com/view/structformer.

Visual question answering and understanding rea-
sons about spatial and compositional structures of visual
elements–an active research area in the vision commu-
nity [27], [28], [29], [30]. The CLEVR visual question
answering (VQA) dataset [27] and subsequent extensions
[31], [32], [33] have provided useful benchmarks for devel-
oping systems that perform object-centric reasoning. More
recent efforts leverage RAVEN’s Progressive Matrices to
test models’ abilities to discover structure among objects
[34], [35]. The NLVR dataset also helps develop methods
to explicitly reason about sets of objects [36]. In contrast to
passively parsing objects in the scene, our method actively
manipulates objects to achieve desired structures.
3D structure synthesis methods can be categorized into
holistic generative models [37], [38], which directly generate
the whole 3D structure, and structure-aware models [39],
[40], [41], [42], [43], [44], which leverages substructures
(e.g., object parts or furniture in a room). Our work is
most closely related to the latter. These models decompose
generations into synthesizing object parts and creating spatial
arrangements of parts. Instead of leveraging structure prior
of object parts (e.g., a table has four legs on each corner), we
condition spatial arrangements of objects based on high-level
language instruction.

III. TRANSFORMER PRELIMINARY
Transformers were proposed in [45] for modeling sequen-
tial data. At the heart of the Transformer architecture is the
scaled dot-product attention function, which allows elements
in a sequence to attend to other elements. Speciﬁcally, an
attention function takes in an input sequence {x1, ..., xn}
and outputs a sequence of the same length {y1, ..., yn}. Each
input xi is linearly projected to a query qi, key ki, and value
vi. The output yi is computed as a weighted sum of the
values, where the weight assigned to each value is based on
the compatibility of the query with the corresponding key.
The function is computed on a set of queries simultaneous
with matrix multiplication.

QK T
√
dk

)V

(1)

II. RELATED WORK

Attention(Q, K, V ) = softmax(

Grounding spatial relations deﬁnes prior work on spatial
understanding focusing on modeling relations between pairs
of objects, such as in, on, and left of [20], [21], [22], [23].
These spatial relations have been used to specify action
goals (e.g., moving the cup to the right of the bowl) [9],
[10]. More complex structures such as towers or letters can
also be created by chaining these action goals, as shown
in [14], while [24] grounds such spatial abstractions. Other
works examine classifying spatial structures [25]. Besides
facilitating communicating language goals, spatial relations
have also been used as an abstraction for planning [15]. A
recent work segments video demonstrations of object manip-
ulations into sequences of spatial relations, demonstrating the
beneﬁt of this abstract representation for imitation learning
[26]. Different from existing methods, we model spatial
relations between multiple objects, extending from binary
spatial relations to multi-object relations.

where dk is the dimension of queries and keys. The queries,
keys, and values are stacked together into matrix Q ∈
Rn×dmodel, K ∈ Rn×dmodel , and V ∈ Rn×dmodel . The original
transformer architecture has a stack of encoder layers and
a stack of decoder layers. Each encoder layer includes an
attention layer and a position-wise fully connected feed
forward network. Each decoder layer has one additional
attention layer, which performs attention over the output
of the encoder stack. With the causal attention mask, the
decoder layer can prevent the predictions from relying on
future inputs, therefore suitable for autoregressive generation.
We refer the readers to the original paper for details [45].

IV. StructFormer FOR OBJECT REARRANGEMENT
Given a single view of a scene s containing objects
{o1, ..., oN } and a structured language instruction l con-
taining word tokens {w1, ..., wM }, our goal is to rearrange

Fig. 2: Visualization of the components of StructFormer. StructFormer takes as input a sequence of object point clouds and language
instruction, passes this through an object selection network and pose generator to output a sequence of object rearrangement actions to
achieve the conﬁguration implied by the language command. We describe each component in detail in Sec. IV

a subset of the objects {o1, ..., oNq }, which we call query
objects, to reach a goal scene s∗. The rearranged scene
should satisfy the spatial and semantic constraints encoded
in the instruction l while being physically valid. We assume
we are given a partial-view point cloud of the scene Z with
segment labels for points to identify the different objects.
Given this point cloud Z and the language instruction l,
the robot must ﬁnd pose offsets {δ1, ..., δNq }, which can
transform the query objects {o1, ..., oNq } in the initial scene
to new poses that satisfy the goal arrangement implied by l.
Our model consists of an object selection network and a
pose generator network, as shown in Fig. 2. Both networks
jointly reason about the object point clouds and language
instructions using transformers [45]. For a given scene, latent
representations of words and objects are used to construct the
input sequence {w1, .., wN , o1, ..., oN } for the two networks.
The object selection network uses a transformer encoder
to predict output sequence {κ1, ...κN } in a single forward
inference, where κi is a binary variable indicating if the
robot should rearrange object oi. The pose generator net-
work uses a transformer decoder to autoregressively gen-
erate a sequence of pose offsets {δ1, ..., δNq }, which we
achieve by feeding in representations of the selected objects
{o1, ..., oNq } = {oi|κi = 1}N
i=1 as targets for decoding.
Below, we discuss how we build our latent representations
of objects and language instructions. Then we describe the
two transformer networks in detail. Finally, we discuss how
to train our system and use it for rearrangement planning.

A. Object and Sentence Encoders

To jointly model objects and language instructions, we
convert point clouds and word tokens to hidden representa-
tions. Given the segmented point cloud xi of an object oi, we
learn a mapping ho(xi) → ˜ei, in order to obtain the latent
representation of the object. We base our object encoder
around the point cloud transformer (PCT) model [46], which
leverages the transformer as a permutation-invariant function
for learning from unordered points. We also map each unique
word token from the language instructions to an embedding

with a learned mapping hw(wi) → ˜ci. We use a learned
position embedding hpos(i) → pi to indicate the position
of the words and objects in input sequences and a learned
type embedding htype(τi) → ri to differentiate object point
clouds (τ = 1) and word tokens (τ = 0). We concatenate our
latent codes together to obtain the ﬁnal object ei = [˜ei; pi; ri]
and word ci = [˜ci; pi; ri] embeddings.

B. Object Selection Network

The object selection network kΦ({ei}, {ci}) → {κi}
predicts objects that need to be rearranged based on the
language instruction. We use a transformer encoder to per-
form relational inference over all objects in a scene and
the given language instruction (e.g., identifying “objects that
are smaller than a pan”). The object and sentence encoders
encode the words {wi} and objects {oi} to create the input
sequence {c1, .., cM , e1, ...eN } to the transformer encoder.
We feed encoder’s output at each object’s position into
a linear layer to predict {κ1, ...κN }. Formally, the object
selection transformer models the distribution

p({κi}N

i=1|{ei}N

i=1, {ci}N

i=1) ≈

N
(cid:89)

i=1

p(κi|{ei}N

i=1, {ci}N

i=1)

C. Language Conditioned Pose Generator

We learn a generative distribution πΩ({ei}, {ci}) → {δi}
over possible pose offsets for objects that might satisfy the
language instruction and are physically valid. We use a trans-
former encoder-decoder model. The encoder has the same
architecture as the object selection network and encodes
the sequence {c1, .., cM , e1, ...eN } to build a contextualized
representation of the language instruction and objects in
the scene,
including objects that need to be moved and
objects that will remain stationary. The decoder autore-
gressively predicts each object’s pose offset, conditioning
on the global context and the pose offsets of previously
predicted objects. Formally, the decoder takes as input the
sequence {e0, [δ0; e1], [δ1; e2], ..., [δNq−1; eNq ]} and predicts
{δ0, δ1, ..., δNq }. We ensure the input object poses are not

MatMulScaleSoftmaxMatMulQKVMatMulScaleSoftmaxMatMulQKVStartObject Selection Network          / Pose Generator Encoder             Pos EmbType EmbLatent RepRearrangement Sequence123456Selection6-DoF PosePose Generator Decodersameclassyellowmugcirclebottomrightlarge123456780000000012345678910111111111110100110110Pos EmbType EmbLatent Rep1234561111111<latexit sha1_base64="88FPJSCOiUp7xtnlUktG8avw+YE=">AAACEXicbVDLSgNBEJyNrxhfUY9eBoPgKewGUU8S8OIxonlAsoTZyWwyZGZnmekVwpJP8CTot3gTr36Bn+LN2WQPJrGhoajqproriAU34LrfTmFtfWNzq7hd2tnd2z8oHx61jEo0ZU2qhNKdgBgmeMSawEGwTqwZkYFg7WB8m+ntJ6YNV9EjTGLmSzKMeMgpAUs9xH3eL1fcqjsrvAq8HFRQXo1++ac3UDSRLAIqiDFdz43BT4kGTgWblnqJYTGhYzJkXQsjIpnx09mpU3xmmQEOlbYdAZ6xfzdSIo2ZyMBOSgIjs6xl5H9aN4Hw2k95FCfAIjo3ChOBQeHsbzzgmlEQEwsI1dzeiumIaELBprPgEsiFH9LMC5QSZlqyWXnLyayCVq3qXVZr9xeV+k2eWhGdoFN0jjx0heroDjVQE1E0RM/oFb05L8678+F8zkcLTr5zjBbK+foFkA6eVQ==</latexit>pi<latexit sha1_base64="AY1ayiyXJ4QtndsBm/Psr9dC50I=">AAACEXicbVDLSgNBEJyNrxhfUY9eBoPgKewGUU8S8OIxonlAsoTZyWwyZGZnmekVwpJP8CTot3gTr36Bn+LN2WQPJrGhoajqproriAU34LrfTmFtfWNzq7hd2tnd2z8oHx61jEo0ZU2qhNKdgBgmeMSawEGwTqwZkYFg7WB8m+ntJ6YNV9EjTGLmSzKMeMgpAUs96D7vlytu1Z0VXgVeDioor0a//NMbKJpIFgEVxJiu58bgp0QDp4JNS73EsJjQMRmyroURkcz46ezUKT6zzACHStuOAM/YvxspkcZMZGAnJYGRWdYy8j+tm0B47ac8ihNgEZ0bhYnAoHD2Nx5wzSiIiQWEam5vxXRENKFg01lwCeTCD2nmBUoJMy3ZrLzlZFZBq1b1Lqu1+4tK/SZPrYhO0Ck6Rx66QnV0hxqoiSgaomf0it6cF+fd+XA+56MFJ985RgvlfP0Ck2ieVw==</latexit>ri<latexit sha1_base64="syvhC730blCTGJVAfC3BriLXkIk=">AAACFnicbVBNS8NAEN34WetX1aOXxSJ4KkkR9SQFLx4r2A9oQ9lsNu3S3WzYnQgl9E94EvS3eBOvXv0p3ty0OdjWgYHHezO8mRckghtw3W9nbX1jc2u7tFPe3ds/OKwcHbeNSjVlLaqE0t2AGCZ4zFrAQbBuohmRgWCdYHyX650npg1X8SNMEuZLMox5xCkBS3X7IRNABnxQqbo1d1Z4FXgFqKKimoPKTz9UNJUsBiqIMT3PTcDPiAZOBZuW+6lhCaFjMmQ9C2MimfGz2b1TfG6ZEEdK244Bz9i/GxmRxkxkYCclgZFZ1nLyP62XQnTjZzxOUmAxnRtFqcCgcP48DrlmFMTEAkI1t7diOiKaULARLbgEcuGHLPcCpYSZlm1W3nIyq6Bdr3lXtfrDZbVxW6RWQqfoDF0gD12jBrpHTdRCFAn0jF7Rm/PivDsfzud8dM0pdk7QQjlfv5i+oH0=</latexit> i<latexit sha1_base64="ze+HWQH+alT0htHmjSRb+BCWl2g=">AAACGXicbVBNS8NAEN3Ur1q/qh69LBbBU0mKqCcpePFYoV+QhrLZTNqlm2zY3Qgl9Gd4EvS3eBOvnvwp3ty0OdjWgYHHezO8mecnnClt299WaWNza3unvFvZ2z84PKoen3SVSCWFDhVcyL5PFHAWQ0czzaGfSCCRz6HnT+5zvfcEUjERt/U0AS8io5iFjBJtKHfQZjyADGZDNqzW7Lo9L7wOnALUUFGtYfVnEAiaRhBryolSrmMn2suI1IxymFUGqYKE0AkZgWtgTCJQXjY/eYYvDBPgUEjTscZz9u9GRiKlppFvJiOix2pVy8n/NDfV4a2XsThJNcR0YRSmHGuB8/9xwCRQzacGECqZuRXTMZGEapPSkosfLf2Q5V5aCK5mFZOVs5rMOug26s51vfF4VWveFamV0Rk6R5fIQTeoiR5QC3UQRQI9o1f0Zr1Y79aH9bkYLVnFzilaKuvrFzDeoeA=</latexit>˜ei<latexit sha1_base64="wKB0evn94rGE6F9WDwHUsnFoiWg=">AAACGXicbVDLSsNAFJ3UV62vqks3g0VwVZIi6koKblxW6AvSUCaTSTt0HmFmIpTQz3Al6Le4E7eu/BR3TtosbOuFC4dz7uXce8KEUW1c99spbWxube+Udyt7+weHR9Xjk66WqcKkgyWTqh8iTRgVpGOoYaSfKIJ4yEgvnNzneu+JKE2laJtpQgKORoLGFCNjKX/QpiwiGZ4N6bBac+vuvOA68ApQA0W1htWfQSRxyokwmCGtfc9NTJAhZShmZFYZpJokCE/QiPgWCsSJDrL5yTN4YZkIxlLZFgbO2b8bGeJaT3loJzkyY72q5eR/mp+a+DbIqEhSQwReGMUpg0bC/H8YUUWwYVMLEFbU3grxGCmEjU1pySXkSz9kuZeRkulZxWblrSazDrqNunddbzxe1Zp3RWplcAbOwSXwwA1oggfQAh2AgQTP4BW8OS/Ou/PhfC5GS06xcwqWyvn6BS2Cod4=</latexit>˜ci<latexit sha1_base64="8AUfkGOrpYE1SvdfPH7e2hd7Hsk=">AAACFnicbVBNSwMxEM36WetX1aOXYBE8ld0i6kkKXjxWsB/QLiWbpm1osgnJrFCW/glPgv4Wb+LVqz/Fm9l2D7Z1YODx3gxv5kVacAu+/+2trW9sbm0Xdoq7e/sHh6Wj46ZViaGsQZVQph0RywSPWQM4CNbWhhEZCdaKxneZ3npixnIVP8JEs1CSYcwHnBJwVLs7JlqTHu+Vyn7FnxVeBUEOyiiveq/00+0rmkgWAxXE2k7gawhTYoBTwabFbmKZJnRMhqzjYEwks2E6u3eKzx3TxwNlXMeAZ+zfjZRIaycycpOSwMguaxn5n9ZJYHATpjzWCbCYzo0GicCgcPY87nPDKIiJA4Qa7m7FdEQMoeAiWnCJ5MIPaeYFSgk7LbqsguVkVkGzWgmuKtWHy3LtNk+tgE7RGbpAAbpGNXSP6qiBKBLoGb2iN+/Fe/c+vM/56JqX75yghfK+fgGd2aCA</latexit>i<latexit sha1_base64="ze+HWQH+alT0htHmjSRb+BCWl2g=">AAACGXicbVBNS8NAEN3Ur1q/qh69LBbBU0mKqCcpePFYoV+QhrLZTNqlm2zY3Qgl9Gd4EvS3eBOvnvwp3ty0OdjWgYHHezO8mecnnClt299WaWNza3unvFvZ2z84PKoen3SVSCWFDhVcyL5PFHAWQ0czzaGfSCCRz6HnT+5zvfcEUjERt/U0AS8io5iFjBJtKHfQZjyADGZDNqzW7Lo9L7wOnALUUFGtYfVnEAiaRhBryolSrmMn2suI1IxymFUGqYKE0AkZgWtgTCJQXjY/eYYvDBPgUEjTscZz9u9GRiKlppFvJiOix2pVy8n/NDfV4a2XsThJNcR0YRSmHGuB8/9xwCRQzacGECqZuRXTMZGEapPSkosfLf2Q5V5aCK5mFZOVs5rMOug26s51vfF4VWveFamV0Rk6R5fIQTeoiR5QC3UQRQI9o1f0Zr1Y79aH9bkYLVnFzilaKuvrFzDeoeA=</latexit>˜ei<latexit sha1_base64="88FPJSCOiUp7xtnlUktG8avw+YE=">AAACEXicbVDLSgNBEJyNrxhfUY9eBoPgKewGUU8S8OIxonlAsoTZyWwyZGZnmekVwpJP8CTot3gTr36Bn+LN2WQPJrGhoajqproriAU34LrfTmFtfWNzq7hd2tnd2z8oHx61jEo0ZU2qhNKdgBgmeMSawEGwTqwZkYFg7WB8m+ntJ6YNV9EjTGLmSzKMeMgpAUs9xH3eL1fcqjsrvAq8HFRQXo1++ac3UDSRLAIqiDFdz43BT4kGTgWblnqJYTGhYzJkXQsjIpnx09mpU3xmmQEOlbYdAZ6xfzdSIo2ZyMBOSgIjs6xl5H9aN4Hw2k95FCfAIjo3ChOBQeHsbzzgmlEQEwsI1dzeiumIaELBprPgEsiFH9LMC5QSZlqyWXnLyayCVq3qXVZr9xeV+k2eWhGdoFN0jjx0heroDjVQE1E0RM/oFb05L8678+F8zkcLTr5zjBbK+foFkA6eVQ==</latexit>pi<latexit sha1_base64="AY1ayiyXJ4QtndsBm/Psr9dC50I=">AAACEXicbVDLSgNBEJyNrxhfUY9eBoPgKewGUU8S8OIxonlAsoTZyWwyZGZnmekVwpJP8CTot3gTr36Bn+LN2WQPJrGhoajqproriAU34LrfTmFtfWNzq7hd2tnd2z8oHx61jEo0ZU2qhNKdgBgmeMSawEGwTqwZkYFg7WB8m+ntJ6YNV9EjTGLmSzKMeMgpAUs96D7vlytu1Z0VXgVeDioor0a//NMbKJpIFgEVxJiu58bgp0QDp4JNS73EsJjQMRmyroURkcz46ezUKT6zzACHStuOAM/YvxspkcZMZGAnJYGRWdYy8j+tm0B47ac8ihNgEZ0bhYnAoHD2Nx5wzSiIiQWEam5vxXRENKFg01lwCeTCD2nmBUoJMy3ZrLzlZFZBq1b1Lqu1+4tK/SZPrYhO0Ck6Rx66QnV0hxqoiSgaomf0it6cF+fd+XA+56MFJ985RgvlfP0Ck2ieVw==</latexit>ri<latexit sha1_base64="wgCPyhud8rfsn9o/rwGB7Z79k5o=">AAACFnicbVBNS8NAEN34WetX1aOXxSJ4KkkR9VjQg8cK9gPaUDbbTbt0Nxt2J0IJ+ROeBP0t3sSrV3+KNzdtDrZ1YODx3gxv5gWx4AZc99tZW9/Y3Nou7ZR39/YPDitHx22jEk1ZiyqhdDcghgkesRZwEKwba0ZkIFgnmNzmeueJacNV9AjTmPmSjCIeckrAUt3JIO03xzwbVKpuzZ0VXgVeAaqoqOag8tMfKppIFgEVxJie58bgp0QDp4Jl5X5iWEzohIxYz8KISGb8dHZvhs8tM8Sh0rYjwDP270ZKpDFTGdhJSWBslrWc/E/rJRDe+CmP4gRYROdGYSIwKJw/j4dcMwpiagGhmttbMR0TTSjYiBZcArnwQ5p7gVLCZGWblbeczCpo12veVa3+cFlt3BWpldApOkMXyEPXqIHuURO1EEUCPaNX9Oa8OO/Oh/M5H11zip0TtFDO1y+3DKCU</latexit>k <latexit sha1_base64="SVtLYj8OOuSvEwn1Iy9wjDVPDE0=">AAACGnicbVDLSgMxFM3UV62vqks3wSK4KjNF1GVBF+6sYB/QGUomzbShySQkGaEM8xuuBP0Wd+LWjZ/izkw7C9t6IHA4517OzQklo9q47rdTWlvf2Nwqb1d2dvf2D6qHRx0tEoVJGwsmVC9EmjAak7ahhpGeVATxkJFuOLnJ/e4TUZqK+NFMJQk4GsU0ohgZK/m+pIPUv+dkhLJBtebW3RngKvEKUgMFWoPqjz8UOOEkNpghrfueK02QImUoZiSr+IkmEuEJGpG+pTHiRAfp7OYMnlllCCOh7IsNnKl/N1LEtZ7y0E5yZMZ62cvF/7x+YqLrIKWxTAyJ8TwoShg0AuYFwCFVBBs2tQRhRe2tEI+RQtjYmhZSQr7whzTPMkIwnVVsV95yM6uk06h7l/XGw0WteVu0VgYn4BScAw9cgSa4Ay3QBhhI8AxewZvz4rw7H87nfLTkFDvHYAHO1y/5J6JO</latexit>⇡⌦<latexit sha1_base64="SVtLYj8OOuSvEwn1Iy9wjDVPDE0=">AAACGnicbVDLSgMxFM3UV62vqks3wSK4KjNF1GVBF+6sYB/QGUomzbShySQkGaEM8xuuBP0Wd+LWjZ/izkw7C9t6IHA4517OzQklo9q47rdTWlvf2Nwqb1d2dvf2D6qHRx0tEoVJGwsmVC9EmjAak7ahhpGeVATxkJFuOLnJ/e4TUZqK+NFMJQk4GsU0ohgZK/m+pIPUv+dkhLJBtebW3RngKvEKUgMFWoPqjz8UOOEkNpghrfueK02QImUoZiSr+IkmEuEJGpG+pTHiRAfp7OYMnlllCCOh7IsNnKl/N1LEtZ7y0E5yZMZ62cvF/7x+YqLrIKWxTAyJ8TwoShg0AuYFwCFVBBs2tQRhRe2tEI+RQtjYmhZSQr7whzTPMkIwnVVsV95yM6uk06h7l/XGw0WteVu0VgYn4BScAw9cgSa4Ay3QBhhI8AxewZvz4rw7H87nfLTkFDvHYAHO1y/5J6JO</latexit>⇡⌦“Rearrange objects that have the same class as the yellow object into a circle”Contextused by the decoder by shifting the input poses by one
position and using a causal attention mask. We model the
following distribution with the encoder-decoder model

p({δi}Nq

i=0|{ei}N

i=1, {ci}M

i=1) =

N
(cid:89)

n=0

p(δi|δ<i, {ei}N

i=1, {ci}M

i=1)

The network obtains its stochasticity by using a dropout layer
with probability p ∈ [0, 1] during training and inference.

We parameterize 6-DoF pose offset δ as (t, R) ∈ SE(3).
We directly predict t ∈ R3 and predict two vectors a, b ∈
R3, which are used to construct the rotation matrix R ∈
SO(3) using a Gram–Schmidt-like process proposed in [47].
In contrast to quaternion and eular angle representation, this
representation has no discontinuities and facilitates learning
accurate 6-DoF placement poses. δ0 is the pose of a virtual
structure frame in the world frame. δi, ∀i > 0 deﬁnes the 3D
position of object oi in the structure frame and the relative
rotational offset between its target and initial pose. e0 is a
learned embedding for the structure frame.

The order of the query objects in the input sequence
is predeﬁned for each spatial structure. For example, the
rearranged objects will build a circle structure clockwise.
We ﬁnd empirically that imposing an order on objects and
using a virtual frame help create precise spatial structures.

D. Inference and Training

During inference, we select objects to rearrange based on
prediction from the object selection network kΦ. We sample
a batch of B rearrangements from our pose generator πΩ
for the query objects. For each sample, we autoregressively
predict the target pose of the structure frame and each object,
conditioned on the previous predictions in the sequence.

We train the object selection network kΦ and pose gen-
erator πΩ with data from rearrangement sequences. The
object selection network is trained on initial scenes and
groundtruth query objects using a binary cross entropy loss.
The generator is trained with an L2-loss minimizing the
distance between groundtruth and predicted placement poses.

V. DATA GENERATION

We introduce a dataset containing more than 100,000
rearrangement sequences. We pair each rearrangement with
a high level language instruction specifying the target spatial
rearrangement for a set of objects. The language instructions
involve many different semantic and geometric properties for
both grounding objects and specifying the spatial structures,
as shown in Table I. We procedurally generate stable and
collision-free object arrangements in the PyBullet physics
simulator [48] and render with the photo-realistic image
render NVISII [49]. With the goal of generalization in mind,
we adopt 335 everyday household objects from the acronym
dataset [50]. Figure 3 shows the diversity of objects used
from 35 distinct classes.

We generate a language-conditioned rearrangement se-
quence in three steps: (1) sampling a referring expression for
query objects, (2) arranging query objects into a physically

TABLE I: Semantic and spatial concepts in our dataset.
Values

Type (# Value)

Entity

obj

class (35)

material (3)
color (6)
relate (3)

basket, beer bottle, book, bowl, calculator,
candle, controller, cup, donut, ...
glass, metal, plastic
blue, cyan, green, magenta, red, yellow
less, equal, more

struct

shape (4)
size (3)
vertical position (3)
horizontal position (3)
rotation (4)

circle, line, tower, table setting
small, medium, large
top, middle, bottom
left, center, right
north, east, south, west

Fig. 3: Objects used in generating our arrangement dataset orga-
nized by the 35 object classes.
realistic spatial structure, and (3) creating an action sequence
with time reversal. We discuss each step in detail below.

We functionally generate referring expressions for sets of
objects that need to be rearranged. A referring expression can
indicate the query objects explicitly with a discrete feature
(e.g., metal objects) or by relating to an anchor object, which
in turn can be described by one to three discrete features.
Using anchor objects allows us to create referring expres-
sions that require relational reasoning of abstract semantic
properties of objects (e.g., objects that have the same material
as the blue bottle) and continuous geometric properties (e.g.,
objects that are shorter than the glass cup). After sampling
a referring expression, we add query objects, an optional
anchor object, and additional distractor objects, which do
not match the referring expression, to the scene. Since table
settings involve speciﬁc objects, we do no create referring
expressions for this structure type.

We rearrange query objects into physically correct in-
stances of one of the four deﬁned spatial structures according
to different geometric parameters (e.g., radius, size, position,
rotation) in PyBullet. By discretizing the parameters of
the structure according to a pre-speciﬁed vocabulary, we
generate a sentence describing the structure (e.g., place query
objects into a large circle on the top right of the table).

Finally, we move objects out of the structure to random,
collision-free poses in the scene. We obtain an action se-
quence that rearranges objects from random poses into the
goal conﬁguration described by the language instruction by
reversing the random action and associated image sequences.
We use NVISII to render color and depth images and instance
segmentation masks of all objects in the sequence.

VI. EXPERIMENTS

In this section, we provide rigorous experimental valida-
tion of our approach. We ﬁrst evaluate the individual com-

TABLE II: Our pose generator produces smaller translational and rotational errors of predicted object poses for four structures.

Model

Circle

Line

Tower

Table Setting

Obj

Struct

Obj

Struct

Obj

Struct

Obj

Struct

Binary
No Structure
No Encoder
Ours

t

R

11.40
8.32
5.43
3.54

76.36
53.58
54.52
42.08

t

/
/
8.85
8.60

R

/
/
22.47
22.55

t

5.64
4.24
4.19
2.95

R

58.32
40.26
45.39
39.66

t

/
/
5.97
5.86

R

/
/
0.00
0.11

t

6.35
6.12
2.86
2.81

R

t

R

65.62
74.02
41.14
37.17

/
/
10.58
10.61

/
/
22.63
22.55

t

7.87
4.79
3.35
3.09

R

46.91
46.22
46.57
42.43

t

/
/
6.08
5.33

R

/
/
0.01
0.05

previously predicted objects. This baseline is similar to the
previous transformer models used in ﬂoor plan generation
and clip-art generation work [51], [52], where the place-
ments of objects are less spatially constrained.

• No Structure: This variant of our pose generator directly
predicts 6-DoF pose offset of each object in the world frame
without predicting and using the virtual structure frame.
We

between
groundtruth and predicted positions t ∈ R3 in centimeters
and geodesic distances between groundtruth and predicted
rotation matrices R ∈ SO(3) in degrees.

average Euclidean

distances

report

Fig. 4: Visualizations of the predicted circular arrangements from
our pose generator and baselines. Our model creates precise struc-
tures for different numbers of objects with various geometries.

ponents of StructFormer on the collected dataset. Following
this we show planning performance for our entire system.
We then provide results for using our system to generate
and execute rearrangement plans on a physical robot with
real-world objects and sensing.

A. Model Component Testing

We split the dataset into 80% training, 10% validation,
and 10% testing. The test data consist of new target spatial
structures and novel object combinations.

1) Language Conditioned Pose Generator: We compare

our pose generator to the following baselines.
• Binary: This baseline uses a transformer encoder to encode
input {w1, ..., wM , xj, Z∼oj , xj+1} and predict δj+1, where
xj+1 is the point cloud of the current object to move and
xj is that of the previously moved object oj, which serves
as a spatial anchor. Z∼oj
is the scene point cloud 0.5m
around oj. To rearrange objects, we iteratively decode each
object’s placement. For the ﬁrst object, xj is omitted and
the entire scene point cloud Z is used in place of Z∼oj . This
baseline allows us to investigate whether modeling pairwise
relations between objects alone is enough for generating
complex spatial structures.

• No Encoder: This variant of our pose generator does not
use the transformer encoder to extract global context for
decoding. When predicting the pose offset for an object,
it only has information about the language instruction and

As shown in Table II, our model outperforms all baselines
at predicting precise placements of objects for four different
structures. Comparison with No Encoder validates that the
transformer encoder in our pose generator is crucial for
precise rearrangements. The larger errors produced by the
No Struct baseline indicate that using a virtual structure
frame helps anchor placements of objects for better structure
generation. This is analogous to leveraging one object as
the spatial anchor for placing another object when ma-
nipulating pairwise spatial relations [9], [3]. Additionally,
we hypothesize that separately predicting the placements
of the structures and objects also helps deal with spatial
ambiguities embedded in language instructions (e.g., arrange
a circle in the middle of the table). Finally, we highlight
the performance difference between Binary and our model,
which conﬁrms that modeling multi-object spatial relations
is beneﬁcial not only for creating complex spatial structures
but also for generating simpler structures such as lines and
towers, which can be described by binary relations. The
”Struct” results in Table II show that we have comparable
accuracy in locating the target structure relative to anchor
objects in the scene with or with out the use of the encoder.
Besides better quantitative performance achieved by our
method, we also see qualitative improvement. In Fig. 4,
we visualize predicted rearrangements by transforming point
clouds of objects. We highlight that the No Encoder and Bi-
nary baselines are inadequate at producing circular structures
because these two methods are not able to condition place-
ment of an object based on future objects (e.g., how many
more objects to be placed and what are their dimensions). We
also note that No Struct fails to predict precise placement for
a large number of objects (O) and is inconsistent at producing
accurate alignments of objects (P).

2) Object Selection Network: We test our object selection
network on novel combinations of objects. Fig. 5 shows
that our model can retrieve objects based on both direct
references and relational references and can also reason about

No EncoderNo StructureBinaryOursABCDEFGHIJKLMOPQFig. 7: Success rate comparison to the Binary baseline for generat-
ing semantically correct, physically valid structures in simulation.

and objects (e.g., arranging large pans into a small circle).
We also compared our pose generator to the Binary
baseline using groundtruth object selections. As shown in
Fig. 7, our model consistently outperformed Binary at
building all four structures. Our model successfully built
58/112 (52%) more structures that requires modeling com-
plex spatial relations (i.e., circles and table settings) and
14/95 (15%) structures that can be described by pairwise
spatial relations (i.e., lines and towers). In scenes where
both methods were successful, our method moved on average
0.17 ± 0.38 distractor objects in the scenes while Binary
moved 0.21 ± 0.41. This result suggests that our method can
more effectively reason about the relevance of the objects in
the scene, a necessary ability for rearrangements in clutter.
Figure 1 show our model can generate rearrangements of
different structures and size at different parts of the table
conditioned on the language instructions.

C. Physical Robot Experiments

We deploy our system on a Franka Panda Robot with
an arm-mounted RGB-D camera to evaluate real-world ob-
ject manipulation. We generate grasps using the method
from [53] and use RRT-connect [54] for motion planning.
We visualize successful rearrangements in Fig. 1.

Failures of the system are driven by oddly perceived
objects. When a large portion of an object is occluded, the
system is prone to place the object such that it intersects
with other objects. Since our work focus on generating
3D structures, we do not use any sophisticated planning
method. As a result, motion planning fails sometimes due
to unreachable objects. However, the candidate goal scenes
generated by our method could be combined with the vision-
based planners in [55] to ﬁnd feasible motion plans.

VII. CONCLUSIONS

We presented an learning-based approach for robot plan-
ning and manipulation of multi-objects semantic arrange-
ments. Our method leverages a transformer architecture to
generate plans as sequence output from an input scene point
cloud and language command. Our results show the beneﬁt
of our speciﬁc architecture over alternative networks.

While our method outperforms the baselines, we leave
open the problem of operating directly from natural lan-
guage, instead using structured language to specify parame-
ters of each structure. We also don’t currently address place-
ment in clutter or ﬁnding the optimal order of rearranging
actions. Instead, we always build in a predeﬁned order. In the
future, we will incorporate StructFormer into a full-ﬂedged
task-and-motion planner to examine solving rearrangement
problems in a variety of environments, not just on tables.

Fig. 5: F1 scores for query objects predicted by our object selection
network. The top chart organizes performance based on associated
property types in referring expressions. The bottom orders results
by the number of objects identiﬁed in referring expressions.

Fig. 6: Examples of objects predicted to move based on referring
expressions. Green dots indicate objects selected by our network.

continuous-valued properties including height and volume.
Fig. 5 further demonstrates that our model maintains high
accuracy when given an increasing number of target objects.
We visualize identifying objects to move in Fig. 6.

B. Evaluating Full System in Simulation

We evaluated our entire system in the simulation environ-
ment using 138 novel object models from 23 known object
classes. We preserve physical interactions of objects while
using ground-truth instance segmentation and omitting low-
level control of the robot. We simulate object placement by
dropping any moved object from 3 cm above the predicted
target z value. For each scene, we use the same procedure
as our data generation to sample a referring expression and
corresponding query, anchor, and distractor objects. After
putting objects into the scene, we randomly select a structure
and its parameters to create a high-level language instruction.
We ﬁrst tested the combined system. In our experiments,
the object selection network was able to identify all query
objects given a referring expression in 95/156 (61%) of the
tested scenes and identify 70% of the speciﬁed objects in
129/156 (83%) of the scenes. With the selected objects, the
pose generator successfully rearranged objects into circles,
lines, and towers in 40/61 (66%), 40/61 (65%) and 10/34
(29%) of the scenes. The table setting structure was not
included because it does not use the same object referring
expressions as the other three structures. The overall success
rate of the whole pipeline on building spatial structures based
on high-level language instructions was 58/156 (37%). Con-
structing tower structures was especially challenging because
many tested objects have irregular shapes and inherently can-
not be stacked (e.g., apples, teapots, candle stands). Another
major failure mode was incompatible structure parameters

F1 Score0255075100Relevant Property Typeclasscolormaterialclasscolormaterialheightvolume80.6698.2295.39Direct ReferenceF1 Score0255075100Number of Objects In the Scene456789101195.593.5890.0588.0490.0591.2892.5891.2391.6992.2880.3291.6791.56Relational Reference2forkssame color as the metal bowlsmaller than the red mugshorter than the plastic, red teapotTable 1OursBinaryOursBinaryCircle0.6557377049180330.14754098360655765.573770491803314.7540983606557Line0.6721311475409840.50819672131147567.213114754098450.8196721311475Tower0.2941176470588240.17647058823529429.411764705882417.6470588235294Table Setting0.6346153846153850.11538461538461563.461538461538511.53846153846150255075100CircleLineTowerTable SettingOursBinary0255075100CircleLineTowerTable Setting11.5417.6550.8214.7563.4629.4167.2165.57OursBinary1REFERENCES

[1] M. Cakmak and L. Takayama, “Towards a comprehensive chore list
for domestic robots,” IEEE International Conference on Human-Robot
Interaction (HRI), pp. 93–94, 2013.

[2] D. Batra, A. Chang, S. Chernova, A. Davison, J. Deng, V. Koltun,
S. Levine, J. Malik, I. Mordatch, R. Mottaghi, M. Savva, and
H. Su, “Rearrangement: A challenge for embodied ai,” ArXiv, vol.
abs/2011.01975, 2020.

[3] C. Paxton, C. Xie, T. Hermans, and D. Fox, “Predicting stable
conﬁgurations for semantic placement of novel objects,” in Conference
on Robot Learning (CoRL), 2021.

[4] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek, “Robots that
use language,” Annual review of control, robotics, and autonomous
systems, no. 3, 2020.

[5] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., “Visual genome:
Connecting language and vision using crowdsourced dense image
annotations,” International journal of computer vision, vol. 123, no. 1,
pp. 32–73, 2017.

[6] M. Shridhar, D. Mittal, and D. Hsu, “Ingress: Interactive visual
grounding of referring expressions,” The International Journal of
Robotics Research, vol. 39, no. 2-3, pp. 217–232, 2020.

[7] L. Kunze, C. Burbridge, M. Alberti, A. Thippur, J. Folkesson,
P. Jensfelt, and N. Hawes, “Combining top-down spatial reasoning
and bottom-up object class recognition for scene understanding,” in
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, 2014, pp. 2910–2915.

[8] M. G¨unther, J. Ruiz-Sarmiento, C. Galindo, J. Gonz´alez-Jim´enez, and
J. Hertzberg, “Context-aware 3d object anchoring for mobile robots,”
Robotics and Autonomous Systems, vol. 110, pp. 12–32, 2018.
[9] O. Mees, A. Emek, J. Vertens, and W. Burgard, “Learning object
placements for relational instructions by hallucinating scene repre-
sentations,” in 2020 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2020, pp. 94–100.

[10] M. Janner, K. Narasimhan, and R. Barzilay, “Representation learning
for grounded spatial reasoning,” Transactions of the Association for
Computational Linguistics, vol. 6, pp. 49–61, 2018.

[11] S. G. Venkatesh, A. Biswas, R. Upadrashta, V. Srinivasan, P. Talukdar,
and B. Amrutur, “Spatial reasoning from natural language instructions
for robot manipulation,” in IEEE International Conference on Robotics
and Automation (ICRA), 2021.

[12] R. Kartmann, Y. Zhou, D. Liu, F. Paus, and T. Asfour, “Representing
spatial object relations as parametric polar distribution for scene
manipulation based on verbal commands,” in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2020,
pp. 8373–8380.

[13] F. Yan, D. Wang, and H. He, “Robotic understanding of spatial
relationships using neural-logic learning,” in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2020,
pp. 8358–8365.

[14] Y. Bisk, K. J. Shih, Y. Choi, and D. Marcu, “Learning interpretable
spatial operations in a rich 3d blocks world,” in Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, 2018.

[15] Y. Zhu, J. Tremblay, S. Birchﬁeld, and Y. Zhu, “Hierarchical planning
for long-horizon manipulation with geometric and symbolic scene
graphs,” in IEEE International Conference on Robotics and Automa-
tion (ICRA), 2021.

[16] K. Kase, C. Paxton, H. Mazhar, T. Ogata, and D. Fox, “Transferable
task execution from pixels through deep planning domain learning,” in
IEEE International Conference on Robotics and Automation (ICRA),
2020, pp. 10 459–10 465.

[17] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Foxl, “Prospec-
tion: Interpretable plans from language by predicting the future,” in
2019 International Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp. 6942–6948.

[18] Z. Zeng, Z. Zhou, Z. Sui, and O. C. Jenkins, “Semantic robot
programming for goal-directed manipulation in cluttered scenes,” in
IEEE International Conference on Robotics and Automation (ICRA).
IEEE, 2018, pp. 7462–7469.

[19] M. Wilson and T. Hermans, “Learning to Manipulate Object
Collections Using Grounded State Representations,” in Conference
on Robot Learning (CoRL), 2019.
[Online]. Available: https:
//arxiv.org/abs/1909.07876

[20] B. Rosman and S. Ramamoorthy, “Learning spatial relationships
between objects,” The International Journal of Robotics Research,
vol. 30, no. 11, pp. 1328–1342, 2011.

[21] S. Fichtl, A. McManus, W. Mustafa, D. Kraft, N. Kr¨uger, and
F. Guerin, “Learning spatial relationships from 3d vision using his-
tograms,” in IEEE International Conference on Robotics and Automa-
tion (ICRA).

IEEE, 2014, pp. 501–508.

[22] O. Mees, N. Abdo, M. Mazuran, and W. Burgard, “Metric learning
for generalizing spatial relations to new objects,” in IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS).
IEEE,
2017, pp. 3175–3182.

[23] W. Yuan, C. Paxton, K. Desingh, and D. Fox, “Sornet: Spatial object-

centric representations for sequential manipulation,” 2021.

[24] R. Paul, J. Arkin, N. Roy, and T. Howard, “Efﬁcient grounding of
abstract spatial concepts for natural language interaction with robot
manipulators,” in Robotics: Science and Systems, 2016.

[25] L. Teodorescu, K. Hofmann, and P.-Y. Oudeyer, “Spatialsim: Recog-
nizing spatial conﬁgurations of objects with graph neural networks,”
arXiv preprint arXiv:2004.04546, 2020.

[26] Y. Hristov, D. Angelov, M. Burke, A. Lascarides, and S. Ramamoorthy,
“Disentangled relational representations for explaining and learning
from demonstration,” in Conference on Robot Learning. PMLR, 2020,
pp. 870–884.

[27] J.

Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei,
C. Lawrence Zitnick, and R. Girshick, “Clevr: A diagnostic dataset
for compositional
language and elementary visual reasoning,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 2901–2910.

[28] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum,
“Neural-symbolic vqa: Disentangling reasoning from vision and lan-
guage understanding,” in Advances in Neural Information Processing
Systems, 2018, pp. 1039–1050.

[29] D. Ding, F. Hill, A. Santoro, and M. Botvinick, “Object-based at-
tention for spatio-temporal reasoning: Outperforming neuro-symbolic
models with ﬂexible distributed architectures,” arXiv preprint
arXiv:2012.08508, 2020.

[30] M. Nazarczuk and K. Mikolajczyk, “Shop-vrb: A visual reasoning
benchmark for object perception,” in 2020 IEEE International Con-
ference on Robotics and Automation (ICRA).
IEEE, 2020, pp. 6898–
6904.

[31] R. Girdhar and D. Ramanan, “CATER: A diagnostic dataset for

Compositional Actions and TEmporal Reasoning,” in ICLR, 2020.

[32] X. Hong, Y. Lan, L. Pang, J. Guo, and X. Cheng, “Transformation
driven visual reasoning,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021, pp. 6903–6912.
[33] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum,
“CLEVRER: collision events for video representation and reasoning,”
in ICLR, 2020.

[34] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu, “Raven: A dataset
for relational and analogical visual reasoning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 5317–5327.

[35] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap, “Measuring
abstract reasoning in neural networks,” in International conference on
machine learning. PMLR, 2018, pp. 511–520.

[36] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi, “A corpus of natural language
for visual reasoning,” in Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers),
2017, pp. 217–223.

[37] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning
representations and generative models for 3d point clouds,” in Inter-
national conference on machine learning. PMLR, 2018, pp. 40–49.
[38] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove,
“Deepsdf: Learning continuous signed distance functions for shape
representation,” in Proceedings of
the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2019, pp. 165–174.
[39] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. Mitra, and L. Guibas,
“Structurenet: Hierarchical graph networks for 3d shape generation,”
ACM Transactions on Graphics (TOG), Siggraph Asia 2019, vol. 38,
no. 6, p. Article 242, 2019.

[40] R. Wu, Y. Zhuang, K. Xu, H. Zhang, and B. Chen, “Pq-net: A
generative part seq2seq network for 3d shapes,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 829–838.

[41] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas,
“Grass: Generative recursive autoencoders for shape structures,” ACM
Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1–14, 2017.
[42] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu,
B. Chen, D. Cohen-Or, and H. Zhang, “Grains: Generative recursive
autoencoders for indoor scenes,” ACM Transactions on Graphics
(TOG), vol. 38, no. 2, pp. 1–16, 2019.

[43] S. Chaeibakhsh, R. S. Novin, T. Hermans, A. Merryweather, and
A. Kuntz, “Optimizing Hospital Room Layout to Reduce the Risk of
Patient Falls,” in International Conference on Operations Research
and Enterprise Systems
[Online]. Available:
https://arxiv.org/abs/2101.03210

(ICORES), 2021.

[44] Y. Jiang, M. Lim, and A. Saxena, “Learning object arrangements in
3d scenes using human context,” in Proceedings of the 29th Interna-
tional Coference on International Conference on Machine Learning.
Omnipress, 2012, p. 907–914.

[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
Advances in neural information processing systems, 2017, pp. 5998–
6008.

[46] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,
“PCT: Point cloud transformer,” Computational Visual Media, vol. 7,
no. 2, pp. 187–199, 2021.

[47] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, “On the continuity
of rotation representations in neural networks,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,

2019, pp. 5745–5753.

[48] E. Coumans and Y. Bai, “Pybullet, a python module for physics

simulation in robotics, games and machine learning,” 2017.

[49] N. Morrical, J. Tremblay, Y. Lin, S. Tyree, S. Birchﬁeld, V. Pascucci,
and I. Wald, “Nvisii: A scriptable tool for photorealistic image
generation,” arXiv preprint arXiv:2105.13962, 2021.

[50] C. Eppner, A. Mousavian, and D. Fox, “ACRONYM: A large-scale
grasp dataset based on simulation,” in IEEE Int. Conf. on Robotics
and Automation, ICRA, 2020.

[51] X. Wang, C. Yeshwanth, and M. Nießner, “SceneFormer: Indoor scene
generation with transformers,” arXiv preprint arXiv:2012.09793, 2020.
[52] G. Radevski, G. Collell, M. F. Moens, and T. Tuytelaars, “Decoding
language spatial relations to 2d spatial arrangements,” in Proceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, 2020, pp. 4549–4560.

[53] A. Mousavian, C. Eppner, and D. Fox, “6-DOF graspnet: Variational
grasp generation for object manipulation,” International Conference
on Computer Vision, pp. 2901–2910, 2019.

[54] J. J. Kuffner and S. M. LaValle, “RRT-connect: An efﬁcient approach
to single-query path planning,” in Proceedings 2000 ICRA. Millennium
Conference. IEEE International Conference on Robotics and Automa-
tion. Symposia Proceedings (Cat. No. 00CH37065), vol. 2.
IEEE,
2000, pp. 995–1001.

[55] A. Qureshi, A. Mousavian, C. Paxton, M. Yip, and D. Fox, “Nerp:
Neural rearrangement planning for unknown objects,” in Proceedings
of Robotics: Science and Systems, 2021.

