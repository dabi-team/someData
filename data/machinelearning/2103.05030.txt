Program Synthesis Over Noisy Data with Guarantees

Shivam Handa
EECS
Massachusetts Institute of Technology
USA
shivam@mit.edu

Martin Rinard
EECS
Massachusetts Institute of Technology
USA
rinard@csail.mit.edu

1
2
0
2

r
p
A
7
2

]
L
P
.
s
c
[

4
v
0
3
0
5
0
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT
We explore and formalize the task of synthesizing programs over
noisy data, i.e., data that may contain corrupted input-output
examples. By formalizing the concept of a Noise Source, an Input
Source, and a prior distribution over programs, we formalize
the probabilistic process which constructs a noisy dataset. This
formalism allows us to define the correctness of a synthesis
algorithm, in terms of its ability to synthesize the hidden underlying
program. The probability of a synthesis algorithm being correct
depends upon the match between the Noise Source and the Loss
Function used in the synthesis algorithm’s optimization process.
We formalize the concept of an optimal Loss Function given prior
information about the Noise Source. We provide a technique
to design optimal Loss Functions given perfect and imperfect
information about the Noise Sources. We also formalize the concept
and conditions required for convergence, i.e., conditions under
which the probability that the synthesis algorithm produces a
correct program increases as the size of the noisy data set increases.
This paper presents the first formalization of the concept of optimal
Loss Functions, the first closed form definition of optimal Loss
Functions, and the first conditions that ensure that a noisy synthesis
algorithm will have convergence guarantees.

such as

for domains

from examples,

1 INTRODUCTION
Program synthesis has been successfully used to synthesize
programs
string
transformations [12, 21], data wrangling [7], data completion [23],
and data structure manipulation [8, 16, 25]. In recent years, there
has been interest in synthesizing programs from input-output
examples in presence of noise/corruptions
[13, 17, 19]. The
motivation behind this line of work is to extract information left
intact in noisy/corrupted data to synthesize the correct program.
These techniques have empirically shown that synthesizing the
correct program, even in presence of substantial noise, is possible.

1.1 Noisy Program Synthesis Framework
Previous work [13, 17] formulates the noisy program synthesis
problem as an optimization problem over the program space and
the noisy dataset. Given a Loss Function, these techniques return
the program which best fits the noisy dataset. A Loss Function,
within this context, measures the cost of the input-output examples
on which the program 𝑝 produces a different output than the output
in the noisy dataset.

However, no previous research explores the connection between
the best-fit program and the hidden underlying program which

, ,
2021.

produced (via the Noise Source) the noisy data set. No previous
research characterizes when the noisy program synthesis algorithm,
working with a given Loss Function, will synthesize a program
equivalent to the hidden underlying program. Nor does it specify
how to pick an appropriate Loss Function to maximize the synthesis
algorithm’s probability of synthesizing the correct program.

We formulate the correctness of Loss Function based noisy
program synthesis techniques in terms of their ability to find the
underlying hidden program. We achieve this by formalizing and
specifying the underlying hidden probabilistic process which selects
the hidden program, generates inputs, and corrupts the program’s
outputs to produce the noisy data set provided to the synthesis
algorithm. Our formalism uses the following concepts:
• Program Space: A set of programs 𝑝 defined by a grammar G,
• Prior Distribution over Programs: 𝜌𝑝 from which a hidden

underlying program 𝑝ℎ is sampled from,

• Input Source: A probability distribution 𝜌𝑖 which generates 𝑛

input examples (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩,

• Hidden Outputs: The correct outputs computed by the hidden

program 𝑝ℎ over the input examples (cid:174)𝑥.

• Noise Source: A probability distribution 𝜌𝑁 , which corrupts the
hidden outputs to construct a set of noisy outputs (cid:174)𝑦 = ⟨𝑦1, . . . 𝑦𝑛⟩,
• Noisy Dataset: The collection of inputs and noisy outputs D =
( (cid:174)𝑥, (cid:174)𝑦) which are visible to the synthesis algorithm to synthesize
the hidden underlying program 𝑝ℎ.

Working with this formalism, we present the following two results:
• Optimal Loss Functions: Given information about the Noise
Source, we formally define the optimal Loss Function and provide
a technique to design the optimal Loss Function given perfect
and imperfect information about the Noise Source.

• Convergence: We formalize the concept of convergence and
the conditions required for the program synthesis technique to
converge to the correct program as the size of the noisy data set
increases.

1.2 Optimal Loss Functions
Different Loss Functions can be appropriate for different Noise
Sources and synthesis domains. For example, the 0/1 Loss Function,
which counts the number of input-output examples where the
noisy dataset and synthesized program 𝑝 disagree, is a general
Loss Function that assumes that the Noise Source can corrupt the
output arbitrarily. The Damerau-Levenshtein (DL) Loss Function [5]
measures the edit difference under character insertions, deletions,
substitutions, and/or transpositions. It can extract information
present in partially corrupted outputs and thus can be appropriate
for measuring loss when text strings are corrupted by a discrete
Noise Source.

1

 
 
 
 
 
 
, ,

If the Noise Source and a Loss Function are a good match, the
noisy synthesis is often able to extract enough information left
intact in corrupted outputs to synthesize the correct program. The
better the match between the Noise Source and Loss Function,
the higher the probablity that the noisy synthesis algorithm
will produce a correct program (i.e, a program equivalent to the
underlying hidden program). Given a noisy dataset and a Noise
Source, an optimal Loss Function has the highest probability of
returning a correct program. We formally define the concept of
an optimal Loss Function and derive a closed-form expression for
optimal Loss Functions (Section 3).

We also derive optimal Loss Functions for Noise Sources
previously studied in the noisy synthesis literature [13] (Section 5).
These case studies provide a theoretical explanation for the
empirical results provided in [13].

1.3 Convergence
Convergence is an important property which is well studied in the
statistics and machine learning literature [10, 15]. Convergence
properties connect the size of the noisy dataset to the probability
of synthesizing a correct underlying program. Given a synthesis
setting (i.e. a set of programs, an Input Source, a Noise Source, a
prior distribution, and a Loss Function), we define a convergence
property that allows us to guarantee that the probability of the
synthesis algorithm producing the correct underlying program
increases as the size of the noisy dataset increases.

This is the first paper to formulate the conditions required for
the synthesis algorithm to have convergence guarantees. Given
an Input Source and a Noise Source, these conditions allow us to
select an appropriate Loss Function, which will allow our synthesis
algorithm to guarantee convergence.

1.4 Contributions
This paper makes the following contributions:

• Formalism: It presents and formalizes a framework for noisy
program synthesis. The framework includes the concepts of a
prior probability distribution over programs, an Input Source,
and a Noise Source. It then formalizes how these structured
distributions interact to form a probabilistic process which
creates the Noisy Dataset. Given a noisy data set, this formalism
allows us to define a correct solution for the noisy synthesis
problem over that data set.

• Optimal Loss Function: It presents a framework to calculate
the expected reward associated with predicting a program, given
a noisy dataset. Based on this framework, it presents a technique
to design Loss Functions which are optimal , i.e., have the highest
probability of returning the correct solution to the synthesis
problem.

• Convergence: It formalizes the concept of convergence, i.e., for
any probability tolerance 𝛿, there exists a threshold dataset size 𝑘,
such that, given a random noisy dataset of size 𝑛 ≥ 𝑘 generated
by a hidden program 𝑝, the synthesis algorithm will synthesize a
program equivalent to 𝑝 with probability greater than 𝛿. Based
on this definition, this paper formulates conditions on the Input
Source, the Loss Function, and the Noise Source which ensure
convergence.

Shivam Handa and Martin Rinard

(Variable)

𝑥 ⇒ 𝑥 (𝑡)
(cid:75)

𝑛𝑘
(cid:74)

𝑥 ⇒ 𝑣𝑘
(cid:75)

(Function)

𝑡
(cid:74)
. . .

(Constant)

𝑐
𝑥 ⇒ 𝑐
(cid:74)
(cid:75)
𝑥 ⇒ 𝑣1
(cid:75)
𝑓 (𝑛1, 𝑛2, . . . 𝑛𝑘 )

𝑛2
(cid:74)

𝑛1
(cid:74)

𝑥 ⇒ 𝑣2
(cid:75)

𝑥 ⇒ 𝑓 (𝑣1, 𝑣2, . . . 𝑣𝑘 )
(cid:75)

(cid:74)
Figure 1: Execution semantics for program 𝑝

• Case Studies: It presents multiple case studies highlighting Input
Sources, Noise Sources, and Loss Functions which break these
conditions and thus make convergence impossible. It also proves
that these conditions hold for some Noise Sources and Loss
Functions studied in prior work, thus providing a theoretical
explanation for the reported empirical results.

2 SYNTHESIS OVER NOISY DATA
Within this section, we formalize a conceptual framework to
view Noisy program synthesis. We also introduce a modified
noisy synthesis algorithm to accommodate the concept of prior
distribution of programs.

2.1 Noisy Program Synthesis Framework
We first define the programs we consider, how inputs to the program
are specified, and the program semantics. Without loss of generality,
we assume programs 𝑝 are specified as parse trees in a domain-
specific language (DSL) grammar G. Internal nodes represent
function invocations; leaves are constants/0-arity symbols in the
DSL. A program 𝑝 executes in an input 𝑥.
𝑥 denotes the output
𝑝
(cid:75)
(cid:74)
is defined in Figure 1).
of 𝑝 on input 𝑥 (

.

(cid:75)

All valid programs (which can be executed) are defined by a DSL

(cid:74)
grammar G = (𝑇 , 𝑁 , 𝑃, 𝑠0) where:
• 𝑇 is a set of terminal symbols. These may include constants and
symbols which may change value depending on the input 𝑥.
• 𝑁 is the set of nonterminals that represent subexpressions in our

DSL.

• 𝑃 is the set of production rules of the form 𝑠 → 𝑓 (𝑠1, . . . , 𝑠𝑛),
where 𝑓 is a built-in function in the DSL and 𝑠, 𝑠1, . . . , 𝑠𝑛 are
non-terminals in the grammar.

• 𝑠0 ∈ 𝑁 is the start non-terminal in the grammar.

We assume that we are given a black box implementation of each
built-in function 𝑓 in the DSL. In general, all techniques explored
within this paper can be generalized to any DSL which can be
specified within the above framework. This is a standard way of
specifying DSLs in program synthesis literature [13, 22]

Example 1. The following DSL defines expressions over input x,

constants 2 and 3, and addition and multiplication:

𝑛
𝑡

:= 𝑥 | 𝑛 + 𝑡
:= 2 | 3;

| 𝑛 × 𝑡;

Notation: We will use the notation 𝑝 (𝑥) to denote
𝑥 within the
(cid:75)
rest of the paper. Given a vector of input values (cid:174)𝑥 = ⟨𝑥1, 𝑥2, . . . 𝑥𝑛⟩,
we use the notation 𝑝 [ (cid:174)𝑥] to denote vector ⟨𝑝 (𝑥1), 𝑝 (𝑥2), . . . 𝑝 (𝑥𝑛)⟩.
We use the notation 𝐺 ⊆ G to denote a finite subset of programs
accepted by G. Given an input vector (cid:174)𝑥, we use the notation 𝐺 [ (cid:174)𝑥]

𝑝
(cid:74)

2

to denote the set of outputs produced by programs in 𝐺. Formally,

𝐺 [ (cid:174)𝑥] = {𝑝 [ (cid:174)𝑥] | 𝑝 ∈ 𝐺 }
Given a set of programs 𝐺, an input vector (cid:174)𝑥, and an output vector (cid:174)𝑧,
we use the notation 𝐺 (cid:174)𝑥,(cid:174)𝑧 to denote the set of programs in 𝐺, which
given input vector (cid:174)𝑥 produce the output (cid:174)𝑧. Formally,

𝐺 (cid:174)𝑥,(cid:174)𝑧 = {𝑝 ∈ 𝐺 |𝑝 [ (cid:174)𝑥] = (cid:174)𝑧}
Given two programs 𝑝1, 𝑝2 ∈ 𝐺, we use the notion 𝑝1 ≈ 𝑝2 to imply
program 𝑝1 is equivalent to 𝑝2, i.e., for all 𝑥 ∈ 𝑋 , 𝑝1 (𝑥) = 𝑝2 (𝑥).
Given an input vector (cid:174)𝑥, we use the notion 𝑝1 ≈ (cid:174)𝑥 𝑝2 to imply that
program 𝑝1 and 𝑝2 have the same outputs on input vector (cid:174)𝑥, i.e.,
𝑝1 [ (cid:174)𝑥] = 𝑝2 [ (cid:174)𝑥].
Given a set of programs 𝐺, we use the notation 𝐺𝑝ℎ to denote the
set of programs in 𝐺 which are equivalent to program 𝑝ℎ. Formally,
𝐺𝑝ℎ = {𝑝 ∈ 𝐺 | 𝑝 ≈ 𝑝ℎ }

to denote the set of programs in 𝐺 which

We use the notation 𝐺𝐶
𝑝ℎ
are not equivalent to program 𝑝ℎ. Formally,
𝐺𝐶
𝑝ℎ = {𝑝 ∈ 𝐺 | 𝑝 (cid:48) 𝑝ℎ }
Prior Distribution over Programs: Given a set of programs 𝐺, let
𝜌𝑝 be a prior distribution over programs in 𝐺. We assume the hidden
underlying program 𝑝ℎ is sampled from the prior distribution
with probability 𝜌𝑝 (𝑝ℎ). Prior distributions over models are a
standard way to introduce prior knowledge withing systems which
learn from data [10]. The prior distribution allows us to direct
the synthesis procedure by introducing information about the
underlying process. The synthesis algorithm use this distribution
to trade off performance over the noisy data set and synthesizing
the most likely program.

Within this paper, we assume that 𝜌𝑝 is expressed via a set
𝐺 ⊆ G and a weight function 𝑤 G over the DSL G. We assume, we
are given a weight function 𝑤 G, which assigns a weight to each
terminal 𝑡 ∈ 𝑇 , and each production 𝑠 ← 𝑓 (𝑠1, . . . 𝑠𝑘 ) is 𝑃.

Given a program 𝑝 ∈ 𝐺, 𝜌𝑝 (𝑝) is defined as

𝜌𝑝 (𝑝) =

𝑤 (𝑠0, 𝑝)
(cid:205)
𝑝 ∈𝐺

𝑤 (𝑠0, 𝑝)

where 𝑤 (𝑝) is computed via the following recursive definition:
:= 𝑤𝐺 (𝑡)

𝑤 (𝑡, 𝑡)
𝑤 (𝑠, 𝑓 (𝑒1, . . . 𝑒𝑛))

:=

(cid:205)
𝑝𝑑 ∈𝑃

𝑤𝐺 (𝑝𝑑) ×

𝑛
(cid:206)
𝑖=1

𝑤 (𝑠𝑖, 𝑒𝑖 )

where 𝑝𝑑 are productions of the form 𝑠 ← 𝑓 (𝑠1, . . . 𝑠𝑛).
Input Source: An Input Source is a probabilistic process which
generates the inputs provided to the hidden underlying program.
Formally, an Input Source is a probability distribution 𝜌𝑖 , from
which 𝑛 ∈ N inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ are sampled with probability
𝜌𝑖 (⟨𝑥1, . . . 𝑥𝑛⟩ | 𝑝ℎ, 𝑛). Where 𝑝ℎ is hidden underlying program.
Note: Within this paper, we assume that inputs are independent
of the hidden program 𝑝ℎ. We leave the exploration of idea that
the inputs maybe selected to provide information about the hidden
program for future work.
Noise Source: A Noise Source 𝑁 is a probabilistic process which
corrupts the correct outputs returned by the hidden program to
create the noisy outputs. Formally, a Noise Source 𝑁 is attached

, ,

with a probability distribution 𝜌𝑁 . Given a hidden program 𝑝ℎ
and a set of outputs ⟨𝑧1, . . . 𝑧𝑛⟩, the noisy outputs ⟨𝑦1, . . . 𝑦𝑛⟩ are
sampled from the probability distribution 𝜌𝑁 , with probability

𝜌𝑁 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩)

Note: Within this paper, we will use the notation 𝑁 and 𝜌𝑁 for a
Noise Source interchangeably.
Noisy Dataset: A noisy dataset D is a set of input values, denoted
by (cid:174)𝑥 and noisy output values, denoted by (cid:174)𝑦. Within this paper,
we assume the dataset D = ( (cid:174)𝑥, (cid:174)𝑦) of size 𝑛 is constructed by the
following process:

• A hidden program 𝑝ℎ is sampled from 𝐺 with probability 𝜌𝑝 (𝑝ℎ).
• 𝑛 inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ are sampled from probability

distribution 𝜌𝑖 (⟨𝑥1, . . . 𝑥𝑛⟩ | 𝑛).

• We compute outputs (cid:174)𝑧 = ⟨𝑧1, . . . 𝑧𝑛⟩, where 𝑧𝑖 = 𝑝ℎ (𝑥𝑖 ).
• The Noise Source introduces noise by corrupting outputs

𝑧1, . . . 𝑧𝑛 to (cid:174)𝑦 = ⟨𝑦1, . . . 𝑦𝑛⟩ with probability

𝜌𝑁 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩)

Correctness: The goal of the synthesis procedure is to find a
program 𝑝 such that 𝑝 is equivalent to 𝑝ℎ, i.e., 𝑝 ≈ 𝑝ℎ. But even in
absence of noise, it maybe be impossible to synthesize a program
equivalent to 𝑝ℎ.
Therefore, similar to Noise-Free programming-by-example systems
[18, 22], we relax the synthesis requirements to find a program 𝑝
such that 𝑝 and 𝑝ℎ have the same outputs on the input vector (cid:174)𝑥
(𝑝 ≈ (cid:174)𝑥 𝑝ℎ), given input vector (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩. Note that, even in
this relaxed setting, the noise introduced by the Noise Source may
make it impossible to acquire enough information to synthesize
the correct program.

In Section 4, we will tackle the harder problem of convergence,
i.e., probability of synthesizing a program 𝑝, equivalent to the
hidden program 𝑝ℎ (𝑝 ≈ 𝑝ℎ) will improve as we increase the size
of the noisy dataset.

2.2 Loss Function
Given a dataset D = ( (cid:174)𝑥, (cid:174)𝑦) (inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ and outputs (cid:174)𝑦 =
⟨𝑦1, . . . 𝑦𝑛⟩), and a program 𝑝, a Loss Function L (𝑝, D) calculates
how incorrect the program is with respect to the given dataset. A
Loss Function, only depends on outputs (cid:174)𝑦, and the outputs of the
program 𝑝 over inputs (cid:174)𝑥 (i.e., (cid:174)𝑧 = 𝑝 [ (cid:174)𝑥]). Given programs 𝑝1, 𝑝2,
such that for all 𝑥 ∈ (cid:174)𝑥, 𝑝1 (𝑥) = 𝑝2 (𝑥), then L (𝑝1, D) = L (𝑝2, D).
We can rewrite the Loss Function as

L (𝑝, D) = L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦)

Definition 1. Piecewise Loss Function: A Piecewise Loss

Function L (𝑝, D) can be expressed in the following form

𝑛
∑︁

L (𝑝, D) =

𝐿(𝑝 (𝑥𝑖 ), 𝑦𝑖 )

𝑖=1
where D = ( (cid:174)𝑥, (cid:174)𝑦), (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩, (cid:174)𝑦 = ⟨𝑦1, . . . 𝑦𝑛⟩, and 𝐿(𝑧, 𝑦) is
the per-example Loss Function.

Definition 2. 0/1 Loss Function: The 0/1 Loss Function
L0/1 (𝑝, D) counts the number of input-output examples where 𝑝

3

, ,

Shivam Handa and Martin Rinard

𝑡 ∈ 𝑇 , (cid:174)𝑐 = ⟨
𝑥1, . . .
𝑡
(cid:74)
(cid:75)
𝑞 (cid:174)𝑐
𝑡 ∈ 𝑄

𝑡
(cid:74)

𝑥𝑛⟩
(cid:75)

(Term)

𝑞 (cid:174)𝑐
𝑠0
𝑞 (cid:174)𝑐
𝑠0

∈ 𝑄

∈ 𝑄 𝑓

(Final)

12

×3

𝑠 ← 𝑓 (𝑠1, . . . 𝑠𝑘 ) ∈ 𝑃, {𝑞 (cid:174)𝑐1𝑠1
(cid:174)𝑐 = ⟨𝑐𝑜,1, . . . 𝑐𝑜,𝑛⟩, 𝑐𝑜,𝑖 =
, . . . 𝑞 (cid:174)𝑐𝑘
𝑠 ∈ 𝑄, 𝑓 (𝑞 (cid:174)𝑐1𝑠1
𝑞 (cid:174)𝑐

, . . . 𝑞 (cid:174)𝑐𝑘
𝑠𝑘 } ⊆ 𝑄,
𝑓 (𝑐1,𝑖, . . . 𝑐𝑙,𝑖 )

(cid:74)
𝑠𝑘 ) → 𝑞 (cid:174)𝑐

𝑠 ∈ Δ

(cid:75)

(Prod)

𝑥

1

8

4

2

3

7

×2

+3

+2

6

× 3
×2,+3

+3

+ 2

×3

5

9

+3

2
×

,

2
+

×2

+

2,

×

3

Figure 2: Rules for constructing a FTA A = (𝑄, 𝐹, 𝑄 𝑓 , Δ) given
inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ and DSL G = (𝑇 , 𝑁 , 𝑃, 𝑠0).

does not agree with the data set D:

L0/1 (𝑝, D) =

𝑛
∑︁

𝑖=1

1 if (𝑦𝑖 ≠ 𝑝 (𝑥𝑖 )) else 0

2.3 Complexity Measure
As previously defined in literature [13], given a program 𝑝, a
Complexity Measure 𝐶 (𝑝) ranks programs independent of the
input-output examples in the dataset D. This measure is used to
synthesize a simple program out of the current correct candidate
programs. Formally, a complexity measure is a function 𝐶 (𝑝) that
maps each program 𝑝 expressible in 𝐺 to a real number. The
following Cost(𝑝) complexity measure computes the complexity of
given program 𝑝 represented as a parse tree recursively as follows:

Cost(𝑡) = cost(𝑡)

Cost(𝑓 (𝑒1, 𝑒2, . . . 𝑒𝑘 )) = cost(𝑓 ) +

𝑘
(cid:205)
𝑖=1

Cost(𝑒𝑖 )

where 𝑡 and 𝑓 are terminals and built-in functions in our DSL G
respectively. Setting cost(𝑡) = cost(𝑓 ) = 1 delivers a complexity
measure Size(𝑝) that computes the size of 𝑝.

2.4 Program Synthesis over Noisy Data
We modify the synthesis algorithm presented in [13] to include
the concept of prior distributions over programs. The synthesis
algorithm builds upon Finite Tree Automata.

Definition 3 (FTA). A finite tree automaton (FTA) over alphabet
𝐹 is a tuple A = (𝑄, 𝐹, 𝑄 𝑓 , Δ) where 𝑄 is a set of states, 𝑄 𝑓 ⊆ 𝑄 is
the set of accepting states, and Δ is a set of transitions of the form
𝑓 (𝑞1, . . . , 𝑞𝑘 ) → 𝑞 where 𝑞, 𝑞1, . . . 𝑞𝑘 are states, 𝑓 ∈ 𝐹 .

Given an input vector (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩, Figure 2 presents rules for
constructing a FTA that accepts all programs in grammar G. The
alphabet of the FTA contains built-in functions within the DSL. The
states in the FTA are of the form 𝑞 (cid:174)𝑐
𝑠 , where 𝑠 is a symbol (terminal or
non-terminal) in G and (cid:174)𝑐 is a vector of values. The existence of state
𝑞 (cid:174)𝑐
𝑠 implies that there exists a partial program which can map (cid:174)𝑥 to
values (cid:174)𝑐. Similarly, the existence of transition 𝑓 (𝑞 (cid:174)𝑐1𝑠1
𝑠𝑘 ) →
𝑞 (cid:174)𝑐
𝑠 implies ∀𝑗 ∈ [1, 𝑛].𝑓 ((cid:174)𝑐1,𝑗 , (cid:174)𝑐2,𝑗 . . . (cid:174)𝑐𝑘,𝑗 ) = (cid:174)𝑐 𝑗 .

. . . 𝑞 (cid:174)𝑐𝑘

, 𝑞 (cid:174)𝑐2𝑠2

Figure 3: The FTA constructed for Example 2

𝑡 (where (cid:174)𝑐𝑖 =

The Term rule states that if we have a terminal 𝑡 (either a
constant in our language or input symbol 𝑥), execute it with the
input 𝑥𝑖 and construct a state 𝑞 (cid:174)𝑐
𝑥𝑖 ). The Prod rule
(cid:75)
states that, if we have a production rule 𝑓 (𝑠1, 𝑠2, . . . 𝑠𝑘 ) → 𝑠 ∈ Δ,
. . . 𝑞 (cid:174)𝑐𝑘
and there exists states 𝑞 (cid:174)𝑐1𝑠1
𝑠𝑘 ∈ 𝑄, then we also have state
𝑠 in the FTA and a transition 𝑓 (𝑞 (cid:174)𝑐1𝑠1
, 𝑞 (cid:174)𝑐2𝑠2
𝑠𝑘 ) → 𝑞 (cid:174)𝑐
𝑞 (cid:174)𝑐
𝑠 .
The FTA Final rule (Figure 2) marks all states 𝑞 (cid:174)𝑐
𝑠0

with start
symbol 𝑠0 as accepting states regardless of the values (cid:174)𝑐 attached to
the state.

, . . . 𝑞 (cid:174)𝑐𝑘

, 𝑞 (cid:174)𝑐2𝑠2

𝑡
(cid:74)

The FTA divides the set of programs in the DSL into subsets.
Given an input set (cid:174)𝑥, all programs in a subset produce the same
outputs (based on the accepting state), i.e., if a program 𝑝 is accepted
by the accepting state 𝑞 (cid:174)𝑐
, then 𝑝 [ (cid:174)𝑥] = (cid:174)𝑐.
𝑠0
In general, the rules in Figure 2 may result in a FTA which has
infinitely many states. To control the size of the resulting FTA, we
do not add a new state within the constructed FTA if the smallest
tree it will accept is larger than a given threshold 𝑑. This results
in a FTA which accepts all programs which are consistent with
the input-output example but are smaller than the given threshold
(it may accept some programs which are larger than the given
threshold but it will never accept a program which is inconsistent
with the input-output example). This is a standard practice in the
synthesis literature [18, 22].

We denote the FTA constructed from DSL G, given input vector
G [ (cid:174)𝑥]. We omit the subscript grammar G

(cid:174)𝑥 and threshold 𝑏 as A𝑑
and threshold 𝑏 wherever it is obvious from context.

Example 2. Consider the DSL presented in Example 1. Given input
𝑥 = 1, Figure 3 presents the FTA which represents all programs of
height less than 3.
For readability, we omit the states for terminals 2 and 3.

Algorithm 1 presents a modified version of the synthesis
algorithm presented by [13] to synthesize programs within the
noisy synthesis settings. The algorithm first constructs a FTA

4

which optimizes
, G, A, 𝑑) is defined in

over input vector (cid:174)𝑥. It then finds the state 𝑞 (cid:174)𝑐
𝑠0
L ((cid:174)𝑐, (cid:174)𝑦) − log 𝜋 (𝑞 (cid:174)𝑐
𝑠0
figure 4.
𝑤 (𝑞 (cid:174)𝑐

, G, A, 𝑑), where 𝜋 (𝑞 (cid:174)𝑐
𝑠0

𝑠 , 𝑚) denotes the sum of weights of partial programs of size
𝑠 . 𝜋 (𝑞 (cid:174)𝑐
, G, A, 𝑑) denotes the sum of
𝑠0
of size ≤ 𝑑. Note
, G, A, 𝑑) = 𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑐 ), where 𝐺 is the subset of programs

≤ 𝑚, accepted by the state 𝑞 (cid:174)𝑐
probabilities of all programs accepted by state 𝑞 (cid:174)𝑐
𝑠0
that, 𝜋 (𝑞 (cid:174)𝑐
𝑠0
in G of height ≤ 𝑑.

Note that (cid:174)𝑐 minimizes L ((cid:174)𝑐, (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑐 ), where 𝐺 (cid:174)𝑥,(cid:174)𝑐 is the

set of programs in 𝐺 which map input (cid:174)𝑥 to output (cid:174)𝑐.

Given the optimal state 𝑞∗, we find the program accepted by
𝑞∗ which minimizes our complexity metric 𝐶 (𝑝). The following
equations hold true:

(cid:174)𝑐 = arg min
(cid:174)𝑐 ∈𝐺 [ (cid:174)𝑥 ]

L ((cid:174)𝑐, (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑐 ), 𝑝∗ = arg min
𝑝 ∈𝐺 (cid:174)𝑥,(cid:174)𝑐

𝐶 (𝑝)

𝑝∗ ∈ 𝐺 (cid:174)𝑥,(cid:174)𝑐 ⇐⇒ (cid:174)𝑐 = 𝑝∗ [ (cid:174)𝑥] and 𝑝∗ ∈ 𝐺

Algorithm 1: Synthesis Algorithm
Input : DSL G, prior distribution 𝜋, threshold 𝑑, data set

D = ( (cid:174)𝑥, (cid:174)𝑦), Loss function L, complexity measure 𝐶

𝑑 [ (cid:174)𝑥] = (𝑄, 𝐹, 𝑄 𝑓 , Δ)

Result: Synthesized program 𝑝∗
A G
𝑞∗ ← argmin𝑞 (cid:174)𝑐
𝑠0
𝑝∗ ← argmin𝑝 ∈(𝑄,𝐹,{𝑞∗ },Δ)𝐶 (𝑝)

∈𝑄 𝑓

L ((cid:174)𝑐, (cid:174)𝑦) − log 𝜋 (𝑞 (cid:174)𝑐
𝑠0

Given a FTA A, we can use dynamic programming to find the
minimum complexity parse tree (under the certain Cost(𝑝) like
measures) accepted by A [9]. In general, given a FTA A, we assume
we are provided with a method to find the program 𝑝 accepted by
A which minimizes the complexity measure.

3 OPTIMAL LOSS FUNCTION
Within this section, we will formalize the connection between the
Noise Source and the Loss Function. We then derive a closed form
expression for the optimal Loss Function in case of perfect and
imperfect information about the Noise Source.

3.1 Optimal Loss Function given a Noise Source
Given dataset D = ( (cid:174)𝑥, (cid:174)𝑦), let us assume that a synthesis procedure
predicts 𝑝 to be the underlying hidden program. The Expected
Reward is the probability that 𝑝 generated dataset D. Formally,
given D = ( (cid:174)𝑥, (cid:174)𝑦),

, ,

Since the above reward only depends on the output of program 𝑝
on input set (cid:174)𝑥, we can rewrite the above reward as

E ((cid:174)𝑐 | (cid:174)𝑥, (cid:174)𝑦) =

1
𝜌 ( (cid:174)𝑦 | (cid:174)𝑥)

𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑐 )𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑐)

where (cid:174)𝑐 = 𝑝 [ (cid:174)𝑥].

Therefore, given dataset D, the probability D was generated by

the synthesized program 𝑝 is E (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦).

Given a Loss Function L and prior distribution 𝜌𝑝 , our synthesis
algorithm is correct with probability E (𝑝𝑙 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦), where 𝑝𝑙 is:

(cid:174)𝑐 = arg min
(cid:174)𝑐 ∈𝐺 [ (cid:174)𝑥 ]

L ((cid:174)𝑐, (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑐 ), 𝑝𝑙 = arg min
𝑝 ∈𝐺 (cid:174)𝑥,(cid:174)𝑐

𝐶 (𝑝)

𝑝𝑙 will be the ideal prediction if it maximizes the expected reward.
Let E𝐿 (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦) = − log E (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦).

E𝐿 (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦) = − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] ) + (− log 𝜌𝑁 ( (cid:174)𝑦 | 𝑝 [ (cid:174)𝑥])) + 𝐶
Therefore, given a set of programs 𝐺, dataset D, prior
distribution 𝜌𝑝 , and Noise Source 𝑁 , and no other information
about the hidden program 𝑝ℎ, the synthesis algorithm will always
return the program which maximizes the expected reward if the
Loss Function L ((cid:174)𝑐, (cid:174)𝑦) is equal to − log 𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑐)+𝐶, for any constant
𝐶. Hence, within this setting, − log 𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑐) +𝐶 is the optimal Loss
Function.

Note that, in expectation, no other Loss Function will out perform

3.2 Optimal Loss Function given imperfect

information

Let us now consider a scenario where we are presented with some
imperfect information about the Noise Source, i.e., the Noise Source
corrupting the correct output belongs to the set N and we are
presented with a prior probability distribution 𝜌 N over possible
Noise Sources in N . The probability that 𝑁 ∈ N is the underlying
Noise Source corrupting the correct dataset is 𝜌 N (𝑁 ).

Given dataset D = ( (cid:174)𝑥, (cid:174)𝑦), we assume it was constructed by the

following underlying process:
• A Noise Source 𝑁 ∈ N is sampled from the prior distribution

𝜌 N, with probability 𝜌 N (𝑁 ).

• A hidden program 𝑝ℎ is sampled from the set 𝐺 with probability

distribution 𝜌𝑝 .

• 𝑛 inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ are sampled from probability

distribution 𝜌𝑖 with probability 𝜌𝑖 (⟨𝑥1, . . . 𝑥𝑛⟩ |𝑛).

• The process then computes outputs (cid:174)𝑧 = ⟨𝑧1, . . . 𝑧𝑛⟩, where 𝑧𝑖 =

𝑝ℎ (𝑥𝑖 ).

• The sampled Noise Source 𝑁 , introduces noise by corrupting

, G, A, 𝑑)

the above optimal version.

E (𝑝 | (cid:174)𝑥, (cid:174)𝑦) =

∑︁

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌 (𝑝ℎ | (cid:174)𝑥, (cid:174)𝑦)

=

1
𝜌 ( (cid:174)𝑦 | (cid:174)𝑥)

∑︁

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌𝑝 (𝑝ℎ | (cid:174)𝑥)𝜌𝑁 ( (cid:174)𝑦|𝑝ℎ [ (cid:174)𝑥])

=

1
𝜌 ( (cid:174)𝑦 | (cid:174)𝑥)

∑︁

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌𝑝 (𝑝ℎ)𝜌𝑁 ( (cid:174)𝑦|𝑝ℎ [ (cid:174)𝑥])

E (𝑝 | (cid:174)𝑥, (cid:174)𝑦) =

1
𝜌 ( (cid:174)𝑦 | (cid:174)𝑥)

𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] )𝜌𝑁 ( (cid:174)𝑦 | 𝑝 [ (cid:174)𝑥])

outputs 𝑧1, . . . 𝑧𝑛 to (cid:174)𝑦 = ⟨𝑦1, . . . 𝑦𝑛⟩ with probability

𝜌𝐶 (⟨𝑦1, . . . 𝑦𝑛⟩|⟨𝑧1, . . . 𝑧𝑛⟩, 𝑁 )

which is equal to

𝜌𝑁 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩)
Expected Reward: Given dataset D, let us assume that a synthesis
procedure predicts 𝑝 to be the underlying hidden program. The
Expected Reward is the probability dataset D was generated by
𝑝 as the hidden underlying program. Formally, given D = ( (cid:174)𝑥, (cid:174)𝑦),

5

, ,

Shivam Handa and Martin Rinard

𝑤 (𝑞 (cid:174)𝑐
𝑤 (𝑞 (cid:174)𝑐

𝑡 , 𝑚)
𝑠 , 𝑚)

𝜋 (𝑞 (cid:174)𝑐
𝑠0

, G, A, 𝑑)

:= 𝑤 G (𝑡)
:=

𝑓 (𝑞 (cid:174)𝑐1
𝑠1
:= 𝑤 (𝑞 (cid:174)𝑐
𝑠0

𝑤 G (𝑓 )( (cid:206)

𝑖 ∈ [1,𝑘 ]

𝑤 (𝑞 (cid:174)𝑐𝑖
𝑠𝑖

, 𝑚 − 1))

(cid:205)
,...𝑞 (cid:174)𝑐𝑘
𝑠𝑘 )→𝑞 (cid:174)𝑐
, 𝑑)/ (cid:205)
𝑞 (cid:174)𝑧
𝑠0

∈𝑄 𝑓

𝑠 ∈Δ

𝑤 (𝑞 (cid:174)𝑧
𝑠0

, 𝑑)

Figure 4: Rules for calculating 𝜋 (𝑞 (cid:174)𝑐
𝑠0

, G, A, 𝑑)

E (𝑝 | (cid:174)𝑥, (cid:174)𝑦) =

∑︁

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌 (𝑝ℎ | (cid:174)𝑥, (cid:174)𝑦)

∑︁

∑︁

∝

𝑁 ∈N

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌𝑝 (𝑝ℎ | (cid:174)𝑥)𝜌𝐶 ( (cid:174)𝑦|𝑝ℎ [ (cid:174)𝑥], 𝑁 )𝜌 N (𝑁 )

∑︁

∑︁

=

𝑁 ∈N

𝑝ℎ ∈𝐺

1(𝑝 ≈ (cid:174)𝑥 𝑝ℎ)𝜌𝑝 (𝑝ℎ)𝜌𝑁 ( (cid:174)𝑦|𝑝ℎ [ (cid:174)𝑥], 𝑁 )𝜌 N (𝑁 )

= 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] )(

∑︁

𝑁 ∈N

𝜌𝐶 ( (cid:174)𝑦 | 𝑝 [ (cid:174)𝑥], 𝑁 )𝜌 N (𝑁 ))

Let E𝐿 (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦) = − log E (𝑝 [ (cid:174)𝑥] | (cid:174)𝑥, (cid:174)𝑦)

= − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] ) + (− log(

∑︁

𝑁 ∈N

𝜌 N (𝑁 )𝜌𝐶 ( (cid:174)𝑦 | 𝑝 [ (cid:174)𝑥])))

Therefore, given a set of programs 𝐺, dataset D, prior
distribution 𝜌𝑝 , and no other information about the hidden program
𝑝ℎ or the hidden Noise Source, the synthesis algorithm will always
return the program which maximizes the expected reward if the
Loss Function L ((cid:174)𝑐, (cid:174)𝑦) is equal to

−[log

∑︁

𝑁 ∈N

𝜌𝐶 ( (cid:174)𝑦 | (cid:174)𝑐, 𝑁 )𝜌 N (𝑁 )] + 𝐶 = − log 𝐸 [𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑐)] + 𝐶

for any constant 𝐶. Hence, the optimal Loss function, in presence
of imperfect information of the Noise Source, is the negative log
of the expected probability of output (cid:174)𝑐 being corrupted to noisy
output (cid:174)𝑦.

4 CONVERGENCE
Within this section, we explore the conditions under which the
synthesis algorithm will have convergence guarantees.

Given a synthesis setting (i.e., a finite set of programs 𝐺, an
Input Source 𝜌𝑖 , a Noise Source 𝜌𝑁 , a prior probability 𝜌𝑝 , and a
Loss Function L), convergence property allows us to guarantee
synthesis algorithm’s output will be the correct underlying program
with high probability if we are providing the algorithm with a large
enough dataset.

Given a Noise Source 𝑁 , a Loss Function L, prior probability
𝜌𝑝 , a positive probability hidden program 𝑝ℎ (i.e., 𝜌𝑝 (𝑝ℎ) > 0), and
a dataset size 𝑛, let 𝑃𝑟 [𝑝𝑛
𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] denote the probability on
synthesizing program equivalent 𝑝ℎ on a random data set ( (cid:174)𝑥, (cid:174)𝑦) of
size 𝑛, constructed assuming 𝑝ℎ as the hidden program. Formally,
𝑃𝑟 [𝑝𝑛
𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] is the probability of the following process
returning true.

• Sample 𝑛 inputs (cid:174)𝑥 with probability 𝜌𝑖 ( (cid:174)𝑥 | 𝑛).
• (cid:174)𝑧ℎ = 𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦 is sampled from the distribution 𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑧ℎ).

• Return true, if for all programs 𝑝 ∈ 𝐺𝐶
𝑝ℎ

:

L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝ℎ [ (cid:174)𝑥 ] ) < L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] )
𝑠 ∈ 𝐺𝑝ℎ , such that for all 𝑝 ∈ 𝐺𝐶
and there exists a program 𝑝𝑛
𝑝ℎ
where 𝑝 ≈ (cid:174)𝑥 𝑝ℎ, 𝐶 (𝑝𝑛
𝑠 ) < 𝐶 (𝑝).

,

Note that if the procedure returns true then the following is true:
L ((cid:174)𝑧, (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,(cid:174)𝑧), 𝑝𝑛

𝐶 (𝑝)

𝑝ℎ [ (cid:174)𝑥] = arg min
(cid:174)𝑧 ∈𝐺 [ (cid:174)𝑥 ]

𝑠 = arg min
𝑝 ∈𝐺 (cid:174)𝑥,𝑝ℎ [ (cid:174)𝑥 ]

i.e., 𝑝𝑛

𝑠 ≈ 𝑝ℎ is synthesized.

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] measures the probability that a program
equivalent to the hidden program is synthesized, given a random
noisy dataset using 𝑝ℎ as the underlying program.

Definition 4. Convergence: Given a finite set of programs 𝐺,
a Loss Function L, a Noise Source 𝑁 , an Input Source 𝜌𝑖 , and a
probability distribution over programs 𝜌𝑝 , the synthesis will converge,
if for all 𝛿 > 0, there exists a natural number 𝑘, such that for all
positive probability hidden program 𝑝ℎ and 𝑛 ≥ 𝑘:

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] ≥ (1 − 𝛿)

i.e., for all probability tolerance 𝛿 > 0, we can find a minimum dataset
size 𝑘, such that for all hidden programs and dataset sizes ≥ 𝑘, the
probability the synthesized program will be equivalent to the hidden
program is greater that (1 − 𝛿).

If the convergence property is true for a finite set of programs
𝐺, a Loss Function L, a Noise Source 𝑁 , an Input Source 𝜌𝑖 , and a
probability distribution over programs 𝜌𝑝 , then for all 𝛿 > 0, there
exists a natural number 𝑘, such that for all positive probability
programs 𝑝ℎ (i.e., there is positive probability that 𝑝ℎ will be
sampled), for any dataset of size greater than 𝑘, with probability
(1 − 𝛿), the algorithm will synthesize a program equivalent to 𝑝ℎ.

4.1 Differentiating Input Distributions
Even in absence of noise, the Input Source may hinder the ability
of the synthesis algorithm to converge to the hidden program. For
example, consider an Input Source which only generates vectors
(cid:174)𝑥, such that, there exist two programs 𝑝1 and 𝑝2 which have
the same outputs on input (cid:174)𝑥 (i.e., 𝑝1 [ (cid:174)𝑥] = 𝑝2 [ (cid:174)𝑥]). For such an
Input Source, the synthesis algorithm, even in the absence of noise,
cannot differentiate between datasets produced assuming 𝑝1 is
the underlying program and datasets produced assuming 𝑝2 is the
underlying program. This issue comes up in traditional Noise-Free
programming-by-example synthesis as well. Noise-Free synthesis
frameworks assume that the Input Source will eventually provide
input-output examples to differentiate the underlying program from
all other possible candidate programs to guarantee convergence. We

6

take a similar approach and constrain the Input Source to provide
convergence guarantees.

Let 𝑑 be some distance metric which measures distance between
two equally sized output vectors. For any underlying program
and a probability tolerance, increasing the dataset size should
eventually allow us to differentiate between this program and any
other program in 𝐺.

Definition 5. Differentiating Input Source: Given a set of

programs 𝐺 and a distance metric 𝑑, an Input Source 𝜌𝑖 is
differentiating, if for a large enough dataset size, the distribution
will return an input dataset which will differentiate any two programs
within 𝐺, with a high probability.

An Input Source is differentiating if for all 𝛿 > 0 and 𝜖 > 0,
there exists a minimum dataset size 𝑘, such that for dataset sizes
𝑛 ≥ 𝑘 and all programs 𝑝ℎ ∈ 𝐺, the following process returns true
with probability greater than (1 − 𝛿):
• Sample (cid:174)𝑥 of size 𝑛 from the distribution 𝜌𝑖 ( (cid:174)𝑥).
• Return true if ∀𝑝 ∈ ˜𝐺𝑝ℎ , 𝑑 (𝑝 [ (cid:174)𝑥], 𝑝ℎ [ (cid:174)𝑥]) ≥ 𝜖.

Formally, given a set of programs 𝐺 and a distance metric 𝑑, an
Input Distribution 𝜌𝑖 is differentiating, if for all 𝛿 > 0, for all
distance 𝜖 > 0, there exists a natural number 𝑘, such that for all
natural numbers 𝑛 ≥ 𝑘, and for all programs 𝑝ℎ ∈ 𝐺, the following
statement is true:

∫

(cid:174)𝑥 ∈𝑋 𝑛

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

. 𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿)

i.e., When sampling input vectors (cid:174)𝑥 of length 𝑛, with probability
greater than (1−𝛿), the distance between 𝑝ℎ [ (cid:174)𝑥] (output of program
𝑝ℎ on input (cid:174)𝑥) and 𝑝 [ (cid:174)𝑥], for all programs 𝑝 not equivalent to 𝑝ℎ, is
greater than 𝜖.

Having a differentiating Input Source ensures that as we increase
the size of the dataset, a random dataset will contain inputs which
will allow us to differentiate a program with other programs with
high probability.

4.2 Differentiating Noise Sources
Even if we are given an Input Source which allows us to differentiate
between the hidden underlying program and other programs in 𝐺
in the absence of noise, the Noise Source can, in theory, make
convergence impossible. For example, consider a Noise Source
which corrupts all outputs 𝑧 to a single noisy output value 𝑦∗.
A dataset corrupted by this Noise Source contains no information
about the underlying correct outputs. A synthesis process cannot
extract any information about the hidden underlying program from
such a dataset. Therefore, no synthesis algorithm will be able to
differentiate between different programs in the input program space.
Restrictions have to placed on the types of Noise Sources a synthesis
algorithm can handle in order to provide convergence guarantees.

, ,

vectors (cid:174)𝑧ℎ of length 𝑛, the following is true:

𝜌𝑁 [∀(cid:174)𝑧 ∈ 𝑍 𝑛. L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) ≤ 𝛾
=⇒ 𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) < 𝜖 | 𝑧ℎ] ≥ (1 − 𝛿)

If we are using the optimal Loss Function for the given Noise

Source (Subsection 3.1), the above condition reduces to:

(cid:104)
∀(cid:174)𝑧 ∈ 𝑍 𝑛.

𝜌𝑁

𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑧ℎ)
𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑧)

≤ 𝛾 =⇒ 𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) < 𝜖

(cid:105)

(cid:174)𝑧ℎ

(cid:12)
(cid:12)
(cid:12)

≥ (1 − 𝛿)

Convergence:
Convergence is guaranteed in presence of a differentiating Input
Source and a differentiating Noise Source.

Theorem 1. Given a finite set of programs 𝐺, distribution 𝜌𝑝
from which the programs are sampled, a Loss Function L, a
differentiating Input Source 𝜌𝑖 , and a differentiating Noise
Source 𝜌𝑁 , then our synthesis algorithm will guarantee convergence.

We present the proof of this theorem in the Appendix A.1.

5 CASE STUDIES
In the previous section, we proved how having a differentiating
Input Source and a differentiating Noise Source are sufficient for the
synthesis algorithm to have convergence guarantees. Within this
section, we will prove that the Noise Sources and Loss Functions
studied in [13] fulfill these requirements, and therefore, allow the
synthesis algorithm to provide convergence guarantees. We will
then show how breaking these requirements makes convergence
impossible. We will also show the importance of picking an
appropriate distance metric 𝑑 which connects the Input Source
to the Noise Source.

5.1 Differentiating Input Distributions
In the special case, where each element of the input vector (cid:174)𝑥 are i.i.d.,
the Differentiating Input Distribution condition can be simplified.

For any vector (cid:174)𝑥 = ⟨𝑥1, 𝑥2, . . . 𝑥𝑛⟩,

𝜌𝑖 ( (cid:174)𝑥) =

𝑛
(cid:214)

𝑗=1

𝜌𝐼 (𝑥 𝑗 )

Given any two equal length vectors (cid:174)𝑧 = ⟨𝑧1, . . . 𝑧𝑛⟩ and (cid:174)𝑧ℎ =
⟨𝑧 ′
1

𝑛⟩, let 𝑑𝑖 be a distance metric such that

, . . . 𝑧 ′

𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) =

𝑛
∑︁

𝑗=1

𝑑𝑖 (𝑧 𝑗 , 𝑧 ′
𝑗 )

We say the input distribution 𝜌𝐼
program 𝑝ℎ ∈ 𝐺, and 𝑝 ∈ 𝐺𝐶
𝑝ℎ
𝜌𝐼 (𝑥𝑝 ) > 0, and

is differentiating, if for all
there exists an input 𝑥𝑝 , such that

𝑑𝑖 (𝑝 (𝑥𝑝 ), 𝑝ℎ (𝑥𝑝 )) > 0

Definition 6. Differentiating Noise Source: Given a finite
set of programs 𝐺, a distance metric 𝑑, and a Loss function L, a Noise
Source 𝜌𝑁 is differentiating, if for all 𝛿 > 0 and 𝛾 > 0, there exists
a natural number 𝑘, and 𝜖 ∈ R+, such that for all 𝑛 ≥ 𝑘, for all

Theorem 2. If 𝜌𝐼 is differentiating then 𝜌𝑖 is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.2).

7

, ,

Shivam Handa and Martin Rinard

5.2 Differentiating Noise Distributions and

optimal Loss Function

Case 1: We first consider the case where the noise distribution
never introduces any corruptions in the correct output. Formally,
for all 𝑛 ∈ N, for all (cid:174)𝑧 ∈ 𝑍 𝑛, 𝜌𝑁 ((cid:174)𝑧 | (cid:174)𝑧) = 1. Consider the Loss
Function L0/∞ and the counting distance metric 𝑑𝑐 :

Definition 7. 0/∞ Loss Function: The 0/∞ Loss Function

L0/∞ ((cid:174)𝑧, (cid:174)𝑦) is 0 if (cid:174)𝑧 = (cid:174)𝑦 and ∞ otherwise.

Definition 8. Counting Distance The counting distance metric
𝑑𝑐 counts the number of positions two equal length vectors disagree
on, i.e.,

𝑑𝑐 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑧 ′
1

, . . . 𝑧 ′

𝑛⟩) =

𝑛
∑︁

𝑖=1

1(𝑧𝑖 ≠ 𝑧 ′
𝑖 )

Note that for all 𝛾 > 0, for all 𝑛 ≥ 1, for all 𝜖 ≥ 1, and for all

(cid:174)𝑧ℎ ∈ 𝑍 𝑛, the following is true:

1(∀ 𝑧 ∈ 𝑍 𝑛.L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) ≤ 𝛾 =⇒

∫

(cid:174)𝑦 ∈𝑌 𝑛

𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) < 𝜖)𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑧ℎ)
= 1(∀ 𝑧 ∈ 𝑍 𝑛.L ((cid:174)𝑧, (cid:174)𝑧ℎ) − L ((cid:174)𝑧ℎ, (cid:174)𝑧ℎ) ≤ 𝛾 =⇒ 𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) < 𝜖) = 1

Therefore, in this case 𝜌𝑁 is differentiating.
Case 2: Consider the following 𝑛-Substitution Noise Source and
𝑛-Substitution Loss Function studied in previous work [13].

Definition 9. 𝑛-Substitution Noise Source:

The 𝑛-Substitution Noise Source 𝑁𝑛𝑆 , given an output vector
⟨𝑧1, . . . 𝑧𝑛⟩ corrupts each string 𝑧𝑖 independently. For each string
𝑧 = 𝑐1 · · · 𝑐𝑘 , it replaces character 𝑐𝑖 with a random character not
equal to 𝑐𝑖 with probability 𝛿𝑛𝑆 .

Definition 10. 𝑛-Substitution Loss Function:

The 𝑛-Substitution Loss Function L𝑛𝑆 ((cid:174)𝑧, (cid:174)𝑦) uses per-example Loss
Function 𝐿𝑛𝑆 that captures a weighted sum of positions where the
noisy output string agrees and disagrees with the output from the
synthesized program. If the synthesized program produces an output
that is longer or shorter than the output in the noisy data set, the Loss
Function is ∞:

L𝑛𝑆 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑦1, . . . 𝑦𝑛⟩) =

𝑛
∑︁

𝑖=1

𝐿𝑛𝑆 (𝑧𝑖, 𝑦𝑖 ), where

𝐿𝑛𝑆 (𝑧, 𝑦) =

− log 𝛿𝑖 if 𝑧 [𝑖] ≠ 𝑦 [𝑖] else − log(1 − 𝛿𝑖 )

|𝑧| = |𝑦|

|𝑧| ≠ |𝑦|

∞
|𝑧 |
(cid:205)
𝑖=1





Note that this Loss Function is a linear transformation of the

𝑛-Substitution Loss Function proposed in [13].

Definition 11. Length Distance Metric Given two equal
length vectors of strings, the length distance metric 𝑑𝑙 counts the
number of positions, which have unequal length strings in the two
vectors, i.e.,

𝑑𝑙 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑧 ′
1

, . . . 𝑧 ′

𝑛⟩) =

𝑛
∑︁

𝑖=1

1(|𝑧𝑖 | ≠ |𝑧 ′

𝑖 |)

8

Theorem 3. 𝑛-Substitution Loss Function L𝑛𝑆 is the optimal Loss
Function for the 𝑛-Substitution Noise Source 𝑁𝑛𝑆 . Also, given Length
Distance Metric 𝑑𝑙 and the 𝑛-Substitution Loss Function L𝑛𝑆 , 𝑛-
Substitution Noise Source 𝑁𝑛𝑆 is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.3).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 𝑛-Substitution
Noise Source and 𝑛-Substitution Loss Function.
Case 3: Consider the 1-Delete Loss Function L1𝐷 and the 1-Delete
Noise Source 𝑁1𝐷 studied in previous work [13].

Definition 12. 1-Delete Noise Source: The 1-Delete Noise
source 𝑁1𝐷 given an output vector ⟨𝑧1, 𝑧2, . . . 𝑧𝑛⟩ independently
corrupts each output 𝑧𝑖 by deleting a random character, with
probability 0 < 𝛿1𝐷 < 1. Formally,

𝜌𝑁1𝐷 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩) =

𝑛
(cid:214)

𝑖=1

𝜌𝑛1𝐷 (𝑦𝑖 | 𝑧𝑖 )

where

𝜌𝑛1𝐷 (𝑧 | 𝑧) = 1 − 𝛿1𝐷

𝜌𝑛1𝐷 (𝑎 · 𝑏 |𝑎 · 𝑥 · 𝑐) =

1
len(𝑎 · 𝑐 · 𝑏)

𝛿1𝐷

where 𝑐 is a character.

Definition 13. 1-Delete Loss Function: The 1-Delete Loss
Function L1𝐷 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑦1, . . . 𝑦𝑛⟩) assigns loss − log(1 − 𝛿𝑖 )
if the output 𝑧𝑖 from the synthesized program and the data set 𝑦𝑖
match exactly, − log 𝛿𝑖 if a single deletion enables the output from
the synthesized program to match the output from the data set, and
∞ otherwise (for 0 < 𝛿𝑖 < 1):

L1𝐷 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑦1, . . . 𝑦𝑛⟩) =

𝑛
∑︁

𝑖=1

𝐿1𝐷 (𝑧𝑖, 𝑦𝑖 ), where

𝐿1𝐷 (𝑧, 𝑦) =

− log(1 − 𝛿𝑖 )
− log 𝛿𝑖
∞





𝑧 = 𝑦
𝑎 · 𝑥 · 𝑏 = 𝑧 ∧ 𝑎 · 𝑏 = 𝑦 ∧ |𝑥 | = 1
otherwise

Note that this Loss Function is a linear transformation of the

1-Delete Loss Function proposed in [13].

Definition 14. DL-k Distance The DL-k Distance metric 𝑑𝐷𝐿𝑘 ,
given two vectors of strings, returns the count of string pairs with
Damerau-Levenshtein distance greater than equal to 𝑘 between them.
Formally,

𝑑𝐷𝐿 (⟨𝑧1, . . . 𝑧𝑛⟩, ⟨𝑧 ′
1

, . . . 𝑧 ′

𝑛⟩) =

𝑛
∑︁

𝑖=1

1(𝐿𝑧𝑖,𝑧′

𝑖

(|𝑧𝑖 |, |𝑧 ′

𝑖 |) ≥ 𝑘)

where, 𝐿𝑎,𝑏 (𝑖, 𝑗) is the Damerau-Levenshtein metric [5].

Theorem 4. 1-Delete Loss Function L1𝐷 is the optimal Loss
Function for the 1-Delete Noise Source 𝑁1𝐷 . Also, given DL-2 Distance
Metric 𝑑𝐷𝐿2 [5] and the 1-Delete Loss Function L1𝐷 , 1-Delete Noise
Source 𝑁1𝐷 is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.4)

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 1-Delete Noise
Source and 1-Delete Loss Function.
Case 4: Consider the Damerau-Levenshtein Loss Function L𝐷𝐿
and the 1-Delete Noise Source 𝑁1𝐷 studied in previous work [13].
Definition 15. Damerau-Levenshtein (DL) Loss Function:
The DL Loss Function L𝐷𝐿 (𝑝, D) uses the Damerau-Levenshtein
metric [5], to measure the distance between the output from the
synthesized program and the corresponding output in the noisy data
set:

L𝐷𝐿 (𝑝, D) =

∑︁

𝐿𝑝 (𝑥𝑖 ),𝑦𝑖

(cid:0) |𝑝 (𝑥𝑖 )| , |𝑦𝑖 | (cid:1)

(𝑥𝑖,𝑦𝑖 ) ∈D

where, 𝐿𝑎,𝑏 (𝑖, 𝑗) is the Damerau-Levenshtein metric [5].

This metric counts the number of single character deletions,
insertions, substitutions, or transpositions required to convert one
text string into another. Because more than 80% of all human
misspellings are reported to be captured by a single one of these
four operations [5], the DL Loss Function may be appropriate for
computations that work with human-provided text input-output
examples.

Theorem 5. Given DL-2 Distance Metric 𝑑𝐷𝐿2 and the Damerau-
Levenshtein Loss Function L𝐷𝐿, the 1-Delete Noise Source 𝑁1𝐷 is
differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.5).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 1-Delete Noise
Source and Damerau-Levenshtein Loss Function.
Case 5: Consider the Damerau-Levenshtein Loss Function L𝐷𝐿 and
the 𝑛-Substitution Noise Source 𝑁𝑛𝑆 studied in previous work [13].

Theorem 6. Given Length Distance Metric 𝑑𝑙 and the Damerau-
Levenshtein Loss Function L𝐷𝐿, the 𝑛-Substitution Noise Source 𝑁𝑛𝑆
is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.6).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 𝑛-Substitution
Noise Source and Damerau-Levenshtein Loss Function.
Connections to Empirical Results: Handa et al. within their
paper [13], empirically evaluated Damerau-Levenshtein and 1-
Delete Loss Functions in presence of 1-Delete style noise. Both
1-Delete Loss Function and Damerau-Levenshtein Loss Function
are able to synthesize the correct program in presence of some
noise. This is in line with our convergence results. 1-Delete Loss
Function is also able to tolerate datasets with more noise, compared
to Damerau-Levenshtein Loss Function. Even when all input-output
examples were corrupted, 1-Delete Loss Function was able to
synthesize the correct program. Note that 1-Delete Loss Function
is the optimal Loss Function in presence of 1-Delete Noise Source,
therefore it has a higher probability of synthesizing the correct
program.

, ,

They also evaluated 𝑛-Substitution Loss Function in presence of
the 𝑛-Substitution Noise Source. Inline with our theoretical results,
their technique was able to synthesize the correct answer over
datasets corrupted by 𝑛-Substitution Noise Source.

5.3 Non-Differentiating Input Distributions
Let 𝐺 be a set of two programs 𝑝1 and 𝑝2 and let 𝑥 ∗ be the only
input on which 𝑝1 and 𝑝2 disagree on, i.e., 𝑝1 (𝑥 ∗) ≠ 𝑝2 (𝑥 ∗) and
∀𝑥 ∈ 𝑋, 𝑥 ≠ 𝑥 ∗ =⇒ 𝑝1 [𝑥] = 𝑝2 [𝑥]. Let 𝜌𝑝 be a probability
distribution over programs in 𝐺. Without loss of generality, we
assume 𝐶 (𝑝2) ≥ 𝐶 (𝑝1). Let 𝜌𝑖 be the probability distribution over
input vectors.
Case 1: The input process never returns a differentiating input, i.e.,
for all 𝑛 ∈ N and for all (cid:174)𝑥 ∈ 𝑋 𝑛, 𝑥 ∗ ∈ (cid:174)𝑥 =⇒ 𝜌𝑖 ( (cid:174)𝑥) = 0.

For all 𝑛 ∈ N and for all 𝜖 > 0, the following statement is true

for both 𝑝ℎ = 𝑝1 and 𝑝2:
∫

1(∀𝑝 ∈ ˜𝐺𝑝ℎ . 𝑑 (𝑝 [ (cid:174)𝑥], 𝑝ℎ [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥)

(cid:174)𝑥 ∈𝑋 𝑛

∫

≤

(cid:174)𝑥 ∈𝑋 𝑛

1(𝑝1 [ (cid:174)𝑥] ≠ 𝑝2 [ (cid:174)𝑥])𝜌𝑖 (𝑑 (cid:174)𝑥) = 0

Therefore, 𝜌𝑖 is non-differentiating.

Theorem 7. In this case, 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] = 0

We present the proof of this theorem in the Appendix (
Theorem A.7).

Since 𝑝2 [ (cid:174)𝑥] = 𝑝1 [ (cid:174)𝑥] for all (cid:174)𝑥, the best move for any algorithm
is to always predict the simplest program (which in this case is 𝑝2).
Hence, if the hidden program is not the simplest program, even in
the absence of noise, the synthesis algorithm will not converge to
the correct hidden program.
Case 2: The input process never returns a differentiating input
with sufficient probability, i.e., there exists a 𝛿𝑖 , such that for all
𝑛 ∈ N,

1(𝑥 ∗ ∈ (cid:174)𝑥)𝜌𝑖 (𝑑 (cid:174)𝑥) < 𝛿𝑖

∫

(cid:174)𝑥 ∈𝑋 𝑛

For all 𝑛 ∈ N and for all 𝜖 > 0, the following statement is true for
both 𝑝ℎ = 𝑝1 or 𝑝2:

∫

(cid:174)𝑥 ∈𝑋 𝑛

1(∀𝑝 ∈ ˜𝐺𝑝ℎ . 𝑑 (𝑝 [ (cid:174)𝑥], 𝑝ℎ [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥)

∫

≤

(cid:174)𝑥 ∈𝑋 𝑛

1(𝑝1 [ (cid:174)𝑥] ≠ 𝑝2 [ (cid:174)𝑥])𝜌𝑖 (𝑑 (cid:174)𝑥) < 𝛿𝑖

Therefore, 𝜌𝑖 is non-differentiating.

Theorem 8. In this case, 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] < 𝛿𝑖 .

We present the proof of this theorem in the Appendix (
Theorem A.8).

Hence, no matter how much we increase 𝑛 (i.e., the size of the
dataset sampled), the probability that 𝑝1 will be synthesized (in
presence of any Loss Function) is less than 𝛿𝑖 .

9

, ,

Shivam Handa and Martin Rinard

5.4 Non-Differentiating Noise Distributions
Case 1: We first consider the case where the Noise Source corrupts
all information identifying the hidden program. Let 𝐺 be a set
containing two programs, 𝑝𝑎 and 𝑝𝑏 . 𝑝𝑎 takes an input string 𝑥 and
appends character “𝑎” in front of it. 𝑝𝑏 , similarly, takes an input
string 𝑥 and appends character “𝑏” in front of it.

𝑝𝑎
𝑝𝑏

:= append(“𝑎”, 𝑥)
:= append(“𝑏”, 𝑥)

Let 𝑁 be a Noise Source which deletes the first character of
the output string with probability 1. Note that in presence of this
Noise Source, no synthesis algorithm can infer which of the given
programs 𝑝𝑎 or 𝑝𝑏 is the hidden program.

Theorem 9. 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎, 𝑁 ] + 𝑃𝑟 [𝑝𝑛
We present the proof of this theorem in the Appendix (
Theorem A.9).

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏, 𝑁 ] ≤ 1

Therefore, any gains in improving convergence of the synthesis
algorithm by increasing 𝑛, assuming 𝑝𝑎 is the hidden program,
will be on the cost of convergence when 𝑝𝑏 is the hidden program.
Formally,
𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎, 𝑁 ] ≥ 1 − 𝛿 =⇒ 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏, 𝑁 ] ≤ 𝛿

Case 2: The choice of Loss Function affects whether the Noise
Source is differentiating or not. For example, consider a Noise
Source which reveals information identifying the hidden program
with high probability, but the Loss Function does not capture this
information. 1-Delete Noise Source 𝑁1𝐷 , given the 1-Delete Loss
function L1𝐷 (and DL-2 distance metric 𝑑𝐷𝐿2), is differentiating.
Since, 𝑛-Substitution Loss Function L𝑛𝑆 penalizes a deletion

with infinite loss, the following is true:

Theorem 10. Given 𝑛-Substitution Loss Function L𝑛𝑆 , 1-Delete
Noise Source 𝑁1𝐷 is non-differentiating. In this case, the synthesis
algorithm will never converge.

We present the proof of this theorem in the Appendix (
Theorem A.10).

Handa et al. empirically showed within their paper [13] that the
𝑛-Substitution Noise Source requires all input-output examples to
be correct (i.e., it cannot tolerate any noise) when the specific noise
is introduced by the 1-Delete Noise Source. This theorem is in line
with their experimental results.

5.5 Necessity of the distance metric 𝑑
The distance metric serves as an important link between the Input
Source and the Noise Source. A simple distance metric (like the
counting distance) makes it easy to construct a differentiating Input
Source but a simple distance metric lack the restrictions required to
prove a Noise Source differentiating. Similarly, a restricted distance
metric may enable us to easily prove the Noise Source differentiating
but make it hard to construct a differentiating Input Source.

For example, consider the following case. Let 𝐺 be a set
containing two programs 𝑝𝑎 and 𝑝𝑏 . 𝑝𝑎 takes an input tuple (string
and a boolean) (𝑥, 𝑏) and appends character “𝑎” in front of 𝑥 if 𝑏 is
true, else it appends “𝑎𝑎” in front of 𝑥. 𝑝𝑏 , similarly, takes an input
tuple (string and a boolean) (𝑥, 𝑏) and appends character “𝑏” in

10

front of 𝑥 if 𝑏 is true, else it appends “𝑏𝑏” in front of 𝑥. Formally,

𝑝𝑎
𝑝𝑏

:= append(if 𝑏 then “𝑎” else “𝑎𝑎”, 𝑥)
:= append(if 𝑏 then “𝑏” else “𝑏𝑏”, 𝑥)

Consider a Noise Source which deletes the first character with
probability 1. For counting distance metric 𝑑𝑐 , an Input Source 𝜌𝑖
which only returns inputs with 𝑏 set to true, is differentiating, i.e.,
the outputs produced by 𝑝𝑎 and 𝑝𝑏 will be of the from 𝑐 · 𝑥, where
𝑐 is “𝑎” and “𝑏” respectively.

Theorem 11. There exists no Loss Functions for which the above
Noise Source is differentiating. Note that for 𝜌𝑖 described above, the
synthesis algorithm will not converge as well.

We present the proof of this theorem in the Appendix (
Theorem A.11).

But for DL-2 Distance metric 𝑑𝐷𝐿2 (which only counts the
number of disagreements have atleast 2 Damerau-Levenshtein
between them), then an Input Source 𝜌𝑖 has to return inputs with
𝑏 set to true, with high probability, to be differentiating.

Theorem 12. Consider a Loss Function L𝑎𝑏 which checks if the
first character appended to a string is either “𝑎” or “𝑏”. Given DL-2
Distance metric 𝑑𝐷𝐿2 and the Loss Function L𝑎𝑏 , the Noise Source
described above is differentiating.

In this case, if we pick a differentiating Input Source, our synthesis

algorithm will have convergence guarantees.

We present the proof of this theorem in the Appendix (
Theorem A.12).

But note that for space of programs considered in Subsection 5.4
(case 1), even through the Noise Source is differentiating, the
complex 𝑑𝐷𝐿2 distance metric will not work as a differentiating
Input Source is impossible to construct for 𝑑𝐷𝐿2 distance.

6 RELATED WORK
Noise-Free Programming-by-examples: The problem of
learning programs from a set of correct input-output examples has
been studied extensively [12, 20, 21]. Even though these techniques
provide correctness guarantees (and convergence guarantees if all
inputs which allow us to differentiate it from other programs are
provided), these techniques cannot handle noisy datasets.
Noisy Program Synthesis: There has been recent interest in
synthesizing programs over noisy datasets [13, 17]. These systems
do not formalize the requirements for their algorithms to converge
to the correct underlying program. Handa el al’s work uses Loss
Functions as a parameter in their synthesis procedure to tailor it
to different noise sources. It does not provide any process to either
pick or design loss functions, given information about the noise
source.
Neural Program Synthesis: There is extensive work that uses
machine learning/deep neural networks to synthesize programs [3,
6, 19]. These techniques require a training phase, a differentable
loss function, and provide no formal correctness or convergence
guarantees. There also has been a lack of work on designing an
appropriate loss function, given information about the noise source.
Learning Theory: Learning theory captures the formal aspects
of learning models over noise data [4, 14, 15]. Our work takes
concepts from learning theory and applies them to the specific

, ,

[12] Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-

output examples. In ACM Sigplan Notices, Vol. 46. ACM, 317–330.

[13] Shivam Handa and Martin C Rinard. 2020. Inductive program synthesis over
noisy data. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
87–98.

[14] Knud Illeris. 2018. An overview of the history of learning theory. European

Journal of Education 53, 1 (2018), 86–101.

[15] Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. 1994. An

introduction to computational learning theory. MIT press.

[16] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed

program synthesis. ACM SIGPLAN Notices 50, 6 (2015), 619–630.

[17] Hila Peleg and Nadia Polikarpova. 2020. Perfect is the Enemy of Good: Best-Effort
Program Synthesis. In 34th European Conference on Object-Oriented Programming
(ECOOP 2020). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.

[18] Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: a framework for
inductive program synthesis. In ACM SIGPLAN Notices, Vol. 50. ACM, 107–126.
[19] Veselin Raychev, Pavol Bielik, Martin Vechev, and Andreas Krause. 2016. Learning
programs from noisy data. In ACM SIGPLAN Notices, Vol. 51. ACM, 761–774.

[20] D Shaw. 1975. Inferring LISP Programs From Examples.
[21] Rishabh Singh and Sumit Gulwani. 2016. Transforming spreadsheet data types

using examples. In Acm Sigplan Notices, Vol. 51. ACM, 343–356.

[22] Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017. Program synthesis using
abstraction refinement. Proceedings of the ACM on Programming Languages 2,
POPL (2017), 63.

[23] Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017. Synthesis of data completion
scripts using finite tree automata. Proceedings of the ACM on Programming
Languages 1, OOPSLA (2017), 62.

[24] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. 2019.

L_dmi: An
information-theoretic noise-robust loss function. arXiv preprint arXiv:1909.03388
(2019).

[25] Navid Yaghmazadeh, Christian Klinger, Isil Dillig, and Swarat Chaudhuri. 2016.
Synthesizing transformations on hierarchically structured data. In ACM SIGPLAN
Notices, Vol. 51. ACM, 508–521.

context of synthesizing programs over noisy data. To the best of
our knowledge, the special case of noisy program synthesis has
never been explored in learning theory.

There has considerable work done on designing loss functions
for training neural networks [11, 24]. To the best of our knowledge,
these works do not theoretically prove the optimality of their loss
functions with respect to a given noise source.

7 CONCLUSION
Learning models from noisy data with guarantees is an important
problem. There has been recent work on synthesizing programs
over noisy datasets using loss functions. Even though these systems
have delivered effective program synthesis over noisy datasets, they
do not provide any guidance for constructing loss functions given
prior information about the noise source, nor do they comment
on the convergence of their algorithms to the correct underlying
program.

We are the first paper to formalize the hidden process which
samples a hidden program and constructs a noisy dataset, thus
formally specifying the correct solution to a given noisy synthesis
problem. We are the first paper to formalize the concept of an
optimal loss function for noisy synthesis and provide a closed form
expression for such a loss function, in presence of perfect and
imperfect information about the noise source. We are the first
paper of formalize the constraints on the input source, the noise
source, and the loss function which allow our synthesis algorithm
to eventually converge to the correct underlying program. The
case studies highlight why these constraints are necessary. We also
provide proofs of convergence for the noisy program synthesis
problems studied in literature.

REFERENCES
[1] 2018. SyGuS 2018 String Benchmark Suite. https://github.com/SyGuS-Org/
benchmarks/tree/master/comp/2019/PBE_SLIA_Track/from_2018. Accessed:
2020-07-18.

[2] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund
Raghothaman, Sanjit A Seshia, Rishabh Singh, Armando Solar-Lezama, Emina
Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In 2013 Formal
Methods in Computer-Aided Design. IEEE, 1–8.

[3] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint
arXiv:1611.01989 (2016).

[4] Robert C Bolles. 1975. Learning theory. (1975).
[5] Fred J. Damerau. 1964. A Technique for Computer Detection and Correction of
Spelling Errors. Commun. ACM 7, 3 (March 1964), 171–176. https://doi.org/10.
1145/363958.363994

[6] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman
Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program learning under
noisy I/O. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org, 990–998.

[7] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri.
2017. Component-based synthesis of table consolidation and transformation
tasks from examples. In ACM SIGPLAN Notices, Vol. 52. ACM, 422–436.

[8] John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure
transformations from input-output examples. In ACM SIGPLAN Notices, Vol. 50.
ACM, 229–239.

[9] Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993.
Directed hypergraphs and applications. Discrete applied mathematics 42, 2-3
(1993), 177–201.

[10] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and

Donald B Rubin. 2013. Bayesian data analysis. CRC press.

[11] Aritra Ghosh, Himanshu Kumar, and PS Sastry. 2017. Robust loss functions under
label noise for deep neural networks. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 31.

11

, ,

A APPENDIX
A.1 Convergence

Shivam Handa and Martin Rinard

Theorem A.1. Given a finite set of programs 𝐺, a prior distribution 𝜌𝑝 , a Loss Function L, a differentiating Input Source 𝜌𝑖 , and a

differentiating Noise Source 𝜌𝑁 , the synthesis algorithm 1 will have convergence guarantees.

Proof. Given a 𝛿 > 0, let 𝛿𝑖 > 0 and 𝛿𝑁 > 0 be two real numbers such that 𝛿 = 𝛿𝑖 + 𝛿𝑁 .
For all positive probability programs 𝑝ℎ ∈ 𝐺:

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] ≥

∫

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

1(cid:0)∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

.L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜌𝑃 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] ) > L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜋 (𝐺 (cid:174)𝑥,𝑝ℎ [ (cid:174)𝑥 ] )(cid:1)

𝜌𝑖 (𝑑 (cid:174)𝑥)𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝ℎ [ (cid:174)𝑥])

Let 𝛾 = min𝑝 ∈𝐺 𝜌𝑝 (𝐺𝑝 ). Note that log 𝜋 (𝐺 (cid:174)𝑥,𝑝 [ (cid:174)𝑥 ] ) − log 𝜋 (𝐺 (cid:174)𝑥,𝑝ℎ [ (cid:174)𝑥 ] ) > 𝛾

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] ≥

∫

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

1(cid:0)∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

.L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) > 𝛾 (cid:1)𝜌𝑖 (𝑑 (cid:174)𝑥)𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝ℎ [ (cid:174)𝑥])

∫

≥

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

1(cid:0)∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

.L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) > 𝛾 (cid:1)𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝ℎ [ (cid:174)𝑥])

.𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) ≥ 𝜖) 𝜌𝑖 (𝑑 (cid:174)𝑥)
.𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) ≥ 𝜖, (cid:0)∀(cid:174)𝑧 ∈ 𝑍 𝑛.𝑑 (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑧) ≥ 𝜖 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) > 𝛾 (cid:1) =⇒ (cid:0)∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

.L (𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) > 𝛾 (cid:1).

If ∀𝑝 ∈ 𝐺𝐶
𝑝ℎ
Therefore,

∫

≥

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

1(cid:0)∀(cid:174)𝑧 ∈ 𝑍 𝑛.𝑑 (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑧) ≥ 𝜖 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L (𝑝ℎ [ (cid:174)𝑥], (cid:174)𝑦) > 𝛾 (cid:1)

𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝ℎ [ (cid:174)𝑥])1(∀𝑝 ∈ 𝐺 .𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) ≥ 𝜖) 𝜌𝑖 (𝑑 (cid:174)𝑥)

Since 𝜌𝑁 is differentiating, we can assume that that for 𝛿𝑁 > 0, If 𝑛 ≥ 𝑛𝑁 , 𝜖 ≥ 𝜖𝑁 , 𝛿𝑁 > 0, the following statement is true, for any 𝑧ℎ and
𝛾 = log 𝜋 (𝐺𝐶

𝑝ℎ ) − log 𝜋 (𝐺𝑝ℎ ):

Since 𝜌𝑖 is differentiating, we can assume that for 𝛿𝑖 > 0 there exists an 𝑛𝑖 , for 𝑛 ≥ 𝑛𝑖 , the following is true:

(cid:104)
∀(cid:174)𝑧 ∈ 𝑍 𝑛. 𝑑 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 𝜖 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) > 𝛾

𝜌𝑁

(cid:105)

(cid:174)𝑧ℎ

(cid:12)
(cid:12)
(cid:12)

≥ (1 − 𝛿𝑁 )

∫

(cid:174)𝑥 ∈𝑋 𝑛

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

. 𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿𝑖 )

Then for 𝑛 ≥ max(𝑛𝑁 , 𝑛𝑖 ), the following is true:

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝ℎ | 𝑝ℎ, 𝑁 ] ≥ (1 − 𝛿𝑁 )

∫

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

. 𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿𝑁 )(1 − 𝛿𝑖 ) ≥ (1 − 𝛿)

(cid:174)𝑥 ∈𝑋 𝑛

□

A.2 Supplementary Material: Case Studies
Differentiating Input Distributions

Theorem A.2. If 𝜌𝐼 is differentiating then 𝜌𝑖 is differentiating.
Proof. Let for all 𝑝 ∈ 𝐺𝐶
𝑝ℎ

, 𝑥𝑝 be an input such that 𝜌𝐼 (𝑥𝑝 ) > 0

Let 𝛿𝑖 and 𝜖𝑖 be the largest rational numbers such that, for all 𝑝, 𝜌𝐼 (𝑥𝑝 ) ≥ 𝛿𝑖 and 𝑑𝑖 (𝑝 (𝑥𝑝 ), 𝑝ℎ (𝑥𝑝 )) ≥ 𝜖𝑖 .
Let 𝜖 and 𝛿 be any rational number greater than 0. Let 𝑚 and 𝑛0 be natural numbers such that 𝑚 ≥ 𝜖
𝜖𝑖

, for 𝑛 ≥ |𝐺 |𝑛0,

𝑑𝑖 (𝑝 (𝑥𝑝 ), 𝑝ℎ (𝑥𝑝 )) > 0

𝑚
∑︁

𝑗=0

𝑗 𝛿 𝑗
𝐶𝑛0

𝑖 (1 − 𝛿𝑖 )𝑛0−𝑗 ≤

𝛿
|𝐺 |

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

. 𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) > 𝜖)𝜌𝑖 (𝑑 (cid:174)𝑥)

∫

(cid:174)𝑥 ∈𝑋 𝑛

12

∫

≥

(cid:174)𝑥 ∈𝑋 𝑛
∫

≥

(cid:174)𝑥 ∈𝑋 𝑛
𝑚
∑︁

= (1 −

𝑗=0

1(∀𝑝 ∈ 𝐺𝐶
𝑝ℎ

. 𝑑 (𝑝ℎ [ (cid:174)𝑥], 𝑝 [ (cid:174)𝑥]) > 𝑚𝜖𝑖 )𝜌𝑖 (𝑑 (cid:174)𝑥)

1(∀ 𝑝 ∈ 𝐺𝐶
𝑝ℎ

.atleast 𝑚 𝑥𝑝 occur in (cid:174)𝑥)𝜌𝑖 (𝑑 (cid:174)𝑥)

𝑗 𝛿 𝑗
𝐶𝑛0

𝑖 (1 − 𝛿𝑖 )𝑛0−𝑗 ) |𝐺 | ≥ (1 −

𝛿
|𝐺 |

) |𝐺 | ≥ 1 − 𝛿

, ,

□

Differentiating Noise Distributions and optimal Loss Function
Case 2:

Theorem A.3. 𝑛-Substitution Loss Function L𝑛𝑆 is the optimal Loss Function for the 𝑛-Substitution Noise Source 𝑁𝑛𝑆 . Also, given Length

Distance Metric 𝑑𝑙 and the 𝑛-Substitution Loss Function L𝑛𝑆 , 𝑛-Substitution Noise Source 𝑁𝑛𝑆 is differentiating.

Proof.

where:

− log 𝜌𝑁𝑛𝑆 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩) =

𝑛
∑︁

𝑖=1

− log 𝜌𝑛𝑛𝑆 (𝑦𝑖 | 𝑧𝑖 )

− log 𝜌𝑛𝑛𝑆 (𝑠 ′

1 · . . . · 𝑠 ′

𝑘 | 𝑠1 · . . . · 𝑠𝑘 ) =

𝑛
∑︁

𝑖=1

(−1(𝑠 ′

𝑖 = 𝑠𝑖 ) log(1 − 𝛿𝑖 )) + (−1(𝑠 ′

𝑖 ≠ 𝑠𝑖 ) log 𝛿𝑖 )

Note that L𝑛𝑆 = − log 𝜌𝑁𝑛𝑆

. Hence 𝑛-Substitution Loss Function L𝑛𝑆 is the optimal Loss Function for 𝑛-Substitution Noise Source.

if 𝑑𝑙 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 1, then for all samples (cid:174)𝑦, L𝑛𝑆 ((cid:174)𝑧, (cid:174)𝑦) = ∞. Therefore for all 𝛾 > 0, 𝛿 > 0,

𝜌𝑁 [∀ (cid:174)𝑧 ∈ 𝑍 𝑛.𝑑𝑙 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 1 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) = ∞ > 𝛾 | (cid:174)𝑧ℎ] = 1 ≥ (1 − 𝛿)

□

Case 3:

Theorem A.4. 1-Delete Loss Function L1𝐷 is the optimal Loss Function for the 1-Delete Noise Source 𝑁1𝐷 . Also, given DL-2 Distance Metric

𝑑𝐷𝐿2 and the 1-Delete Loss Function L1𝐷 , 1-Delete Noise Source 𝑁1𝐷 is differentiating.

Proof.

− log 𝜌𝑁1𝐷 (⟨𝑦1, . . . 𝑦𝑛⟩ | ⟨𝑧1, . . . 𝑧𝑛⟩) =

𝑛
∑︁

𝑖=1

− log 𝜌𝑛1𝐷 (𝑦𝑖 | 𝑧𝑖 )

where 𝜌𝑛1𝐷 (𝑧 | 𝑧) = (1 − 𝛿𝑖 ) and 𝜌𝑛1𝐷 (𝑦 | 𝑧) = 𝛿𝑖 if 𝑦 has exactly one character deleted with respect to 𝑧.

Note that L1𝐷 = − log 𝜌𝑁1𝐷
If 𝑑𝐷𝐿2 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 1 then L1𝐷 ((cid:174)𝑧, (cid:174)𝑦) = ∞. Therefore for all 𝛾 > 0, 𝛿 > 0,

. Hence 1-Delete Loss Function L1𝐷 is the optimal Loss Function for 1-Delete Noise Source.

𝜌𝑁 [∀ (cid:174)𝑧 ∈ 𝑍 𝑛.𝑑𝐷𝐿2 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 1 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) = ∞ > 𝛾 | (cid:174)𝑧ℎ] = 1 ≥ (1 − 𝛿)

□

Case 4:

Theorem A.5. Given DL-2 Distance Metric 𝑑𝐷𝐿2 and the Damerau-Levenshtein Loss Function L𝐷𝐿, the 1-Delete Noise Source 𝑁1𝐷 is

differentiating.

Proof. If 𝑑𝐷𝐿2 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 𝑚, then L𝐷𝐿 ((cid:174)𝑧, (cid:174)𝑦) − L𝐷𝐿 ((cid:174)𝑧, (cid:174)𝑦) ≥ 𝑚 . Therefore for all 𝛾 > 0, 𝛿 > 0,

𝜌𝑁 [∀ (cid:174)𝑧 ∈ 𝑍 𝑛.𝑑𝐷𝐿2 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 𝛾 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) > 𝛾 | (cid:174)𝑧ℎ] = 1 ≥ (1 − 𝛿)

□

Case 5:

Theorem A.6. Given Length Distance Metric 𝑑𝑙 and the Damerau-Levenshtein Loss Function L𝐷𝐿, the 𝑛-Substitution Noise Source 𝑁𝑛𝑆 is

differentiating.

13

, ,

Shivam Handa and Martin Rinard

Proof. If 𝑑𝑙 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 𝑚, then L𝐷𝐿 ((cid:174)𝑧, (cid:174)𝑦) − L𝐷𝐿 ((cid:174)𝑧, (cid:174)𝑦) ≥ 𝑚 (Assuming replacements take place from a very large set). Therefore for all

𝛾 > 0, 𝛿 > 0,

𝜌𝑁 [∀ (cid:174)𝑧 ∈ 𝑍 𝑛.𝑑𝑙 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 𝛾 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) > 𝛾 | (cid:174)𝑧ℎ] = 1 ≥ (1 − 𝛿)

Non-Differentiating Input Distributions
Case 1:

Theorem A.7. In this case, 𝑃𝑟 [𝑝𝑛
Proof.

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] = 0

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] ≤

∫

1(cid:0)L (𝑝2 [ (cid:174)𝑥], (cid:174)𝑦)−

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝2 [ (cid:174)𝑥 ] ) > L (𝑝1 [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝1 [ (cid:174)𝑥 ] )(cid:1)𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝1 [ (cid:174)𝑥])𝜌𝑖 (𝑑 (cid:174)𝑥)
1(cid:0)L (𝑝2 [ (cid:174)𝑥], 𝑝1 [ (cid:174)𝑥]) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝2 [ (cid:174)𝑥 ] ) > L (𝑝1 [ (cid:174)𝑥], 𝑝1 [ (cid:174)𝑥]) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝1 [ (cid:174)𝑥 ] )(cid:1)𝜌𝑖 (𝑑 (cid:174)𝑥)

∫

=

(cid:174)𝑥 ∈𝑋 𝑛

= 0

Case 2:

Theorem A.8. In this case, 𝑃𝑟 [𝑝𝑛
Proof.

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] < 𝛿𝑖 .

𝑠 ≈ 𝑝1 | 𝑝1, 𝑁 ] ≤
1(cid:0)L (𝑝2 [ (cid:174)𝑥], (cid:174)𝑦) − (log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝2 [ (cid:174)𝑥 ] )) > L (𝑝1 [ (cid:174)𝑥], (cid:174)𝑦) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝1 [ (cid:174)𝑥 ] )(cid:1)𝜌𝑁 (𝑑 (cid:174)𝑦 | 𝑝1 [ (cid:174)𝑥])𝜌𝑖 (𝑑 (cid:174)𝑥)

𝑃𝑟 [𝑝𝑛

∫

(cid:174)𝑥 ∈𝑋 𝑛, (cid:174)𝑦 ∈𝑌 𝑛

∫

=

(cid:174)𝑥 ∈𝑋 𝑛

1(cid:0)L (𝑝2 [ (cid:174)𝑥], 𝑝1 [ (cid:174)𝑥]) − (log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝2 [ (cid:174)𝑥 ] )) > L (𝑝1 [ (cid:174)𝑥], 𝑝1 [ (cid:174)𝑥]) − log 𝜌𝑝 (𝐺 (cid:174)𝑥,𝑝1 [ (cid:174)𝑥 ] )(cid:1)𝜌𝑖 (𝑑 (cid:174)𝑥)

∫

≤

(cid:174)𝑥 ∈𝑋 𝑛

1(𝑝1 [ (cid:174)𝑥] ≠ 𝑝2 [ (cid:174)𝑥])𝜌𝑖 (𝑑 (cid:174)𝑥) < 𝛿𝑖

Non-Differentiating Noise Distributions
Case 1:

□

□

□

Theorem A.9. 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎, 𝑁 ] + 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏, 𝑁 ] ≤ 1

Proof. For distance metric 𝑑𝑐 , an Input Source 𝜌𝑖 which only returns ⟨𝑥, true⟩ is differentiating, as for all 𝑥, 𝑝𝑎 (⟨𝑥, true⟩) ≠ 𝑝𝑏 (⟨𝑥, true⟩).

Note that for all Loss Functions L,

𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩]
+𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩]
= 𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩]
+𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩]
≤ 1

Case 2:

Theorem A.10. Given 𝑛-Substitution Loss Function L𝑛𝑆 , 1-Delete Noise Source 𝑁1𝐷 is non-differentiating. In this case, the synthesis algorithm

will never converge.

Proof. If even a single deletions happen in (cid:174)𝑦, then L𝑛𝑆 ((cid:174)𝑧ℎ, (cid:174)𝑦) = ∞. The prob of no deletions is equal to (1 − 𝛿𝑖 )𝑛 which decreases with 𝑛,
□

therefore in this case 1-Delete Noise Source is non-differentiating.

Necessity of the distance metric 𝑑

14

□

Theorem A.11. There exists no Loss Functions for which the above Noise Source is differentiating. Note that for 𝜌𝑖 described above, the

synthesis algorithm will not converge as well.

Proof. For distance metric 𝑑𝑐 , a 𝜌𝑖 which only returns ⟨𝑥, true⟩ is differentiating, as for all 𝑥, 𝑝𝑎 (⟨𝑥, true⟩) ≠ 𝑝𝑏 (⟨𝑥, true⟩). Note that for

all Loss Functions L,

𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩]
+𝜌𝑁 [L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑎 · 𝑥1, . . . 𝑎 · 𝑥𝑛⟩) − L (⟨𝑥1, . . . 𝑥𝑛⟩, ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩) > 𝛾 | ⟨𝑏 · 𝑥1, . . . 𝑏 · 𝑥𝑛⟩]
≤ 1

Therefore 𝜌𝑁 is not differentiating.

Similarly,

, ,

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎] + 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏 ] ≤ 1

Therefore, if for any 𝑛, 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎] ≥ (1 − 𝛿) then 𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏 ] ≤ 𝛿

□

Theorem A.12. Consider a Loss Function which checks if the first character appended to a string is either “𝑎” or “𝑏”. Given the above Loss

Function and DL-2 Distance metric 𝑑𝐷𝐿2, the Noise Source described above is differentiating.

In this case, if we pick a differentiating Input Source, our synthesis algorithm will have convergence guarantees.

Proof. If L (⟨𝑦1, . . . 𝑦𝑛⟩, ⟨“𝑎𝑎” · 𝑥1, . . . “𝑎𝑎” · 𝑥1⟩) = ∞ if for some 𝑖, 𝑦𝑖 = “𝑏” · 𝑥𝑖 . Similarly, we can define this in the case of Therefore if

for two vectors (cid:174)𝑧1 (cid:174)𝑧2, 𝑑𝐷𝐿2 ((cid:174)𝑧1, (cid:174)𝑧2) ≥ 1, then L ((cid:174)𝑧2, (cid:174)𝑦) − L ((cid:174)𝑧1, (cid:174)𝑦) = ∞ if 𝜌𝑁 ( (cid:174)𝑦 | (cid:174)𝑧) > 0.

Note that if the Input Source is differentiating then the following is true.

𝜌𝑁 [∀(cid:174)𝑧 ∈ 𝑍 𝑛. 𝑑𝐷𝐿2 ((cid:174)𝑧, (cid:174)𝑧ℎ) ≥ 1 =⇒ L ((cid:174)𝑧, (cid:174)𝑦) − L ((cid:174)𝑧ℎ, (cid:174)𝑦) = ∞ | (cid:174)𝑧ℎ] = 1] = 1

Note that

Similarly,

1(⟨?, true⟩ ∈ (cid:174)𝑥)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿)

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑎 | 𝑝𝑎, 𝑁 ] = 1(⟨?, true⟩ ∈ (cid:174)𝑥)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿)

𝑃𝑟 [𝑝𝑛

𝑠 ≈ 𝑝𝑏 | 𝑝𝑏, 𝑁 ] = 1(⟨?, true⟩ ∈ (cid:174)𝑥)𝜌𝑖 (𝑑 (cid:174)𝑥) ≥ (1 − 𝛿)

□

15

