Program Synthesis Over Noisy Data with Guarantees

Shivam Handa
EECS
Massachusetts Institute of Technology
USA
shivam@mit.edu

Martin Rinard
EECS
Massachusetts Institute of Technology
USA
rinard@csail.mit.edu

1
2
0
2

r
p
A
7
2

]
L
P
.
s
c
[

4
v
0
3
0
5
0
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT
We explore and formalize the task of synthesizing programs over
noisy data, i.e., data that may contain corrupted input-output
examples. By formalizing the concept of a Noise Source, an Input
Source, and a prior distribution over programs, we formalize
the probabilistic process which constructs a noisy dataset. This
formalism allows us to define the correctness of a synthesis
algorithm, in terms of its ability to synthesize the hidden underlying
program. The probability of a synthesis algorithm being correct
depends upon the match between the Noise Source and the Loss
Function used in the synthesis algorithmâ€™s optimization process.
We formalize the concept of an optimal Loss Function given prior
information about the Noise Source. We provide a technique
to design optimal Loss Functions given perfect and imperfect
information about the Noise Sources. We also formalize the concept
and conditions required for convergence, i.e., conditions under
which the probability that the synthesis algorithm produces a
correct program increases as the size of the noisy data set increases.
This paper presents the first formalization of the concept of optimal
Loss Functions, the first closed form definition of optimal Loss
Functions, and the first conditions that ensure that a noisy synthesis
algorithm will have convergence guarantees.

such as

for domains

from examples,

1 INTRODUCTION
Program synthesis has been successfully used to synthesize
programs
string
transformations [12, 21], data wrangling [7], data completion [23],
and data structure manipulation [8, 16, 25]. In recent years, there
has been interest in synthesizing programs from input-output
examples in presence of noise/corruptions
[13, 17, 19]. The
motivation behind this line of work is to extract information left
intact in noisy/corrupted data to synthesize the correct program.
These techniques have empirically shown that synthesizing the
correct program, even in presence of substantial noise, is possible.

1.1 Noisy Program Synthesis Framework
Previous work [13, 17] formulates the noisy program synthesis
problem as an optimization problem over the program space and
the noisy dataset. Given a Loss Function, these techniques return
the program which best fits the noisy dataset. A Loss Function,
within this context, measures the cost of the input-output examples
on which the program ğ‘ produces a different output than the output
in the noisy dataset.

However, no previous research explores the connection between
the best-fit program and the hidden underlying program which

, ,
2021.

produced (via the Noise Source) the noisy data set. No previous
research characterizes when the noisy program synthesis algorithm,
working with a given Loss Function, will synthesize a program
equivalent to the hidden underlying program. Nor does it specify
how to pick an appropriate Loss Function to maximize the synthesis
algorithmâ€™s probability of synthesizing the correct program.

We formulate the correctness of Loss Function based noisy
program synthesis techniques in terms of their ability to find the
underlying hidden program. We achieve this by formalizing and
specifying the underlying hidden probabilistic process which selects
the hidden program, generates inputs, and corrupts the programâ€™s
outputs to produce the noisy data set provided to the synthesis
algorithm. Our formalism uses the following concepts:
â€¢ Program Space: A set of programs ğ‘ defined by a grammar G,
â€¢ Prior Distribution over Programs: ğœŒğ‘ from which a hidden

underlying program ğ‘â„ is sampled from,

â€¢ Input Source: A probability distribution ğœŒğ‘– which generates ğ‘›

input examples (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©,

â€¢ Hidden Outputs: The correct outputs computed by the hidden

program ğ‘â„ over the input examples (cid:174)ğ‘¥.

â€¢ Noise Source: A probability distribution ğœŒğ‘ , which corrupts the
hidden outputs to construct a set of noisy outputs (cid:174)ğ‘¦ = âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©,
â€¢ Noisy Dataset: The collection of inputs and noisy outputs D =
( (cid:174)ğ‘¥, (cid:174)ğ‘¦) which are visible to the synthesis algorithm to synthesize
the hidden underlying program ğ‘â„.

Working with this formalism, we present the following two results:
â€¢ Optimal Loss Functions: Given information about the Noise
Source, we formally define the optimal Loss Function and provide
a technique to design the optimal Loss Function given perfect
and imperfect information about the Noise Source.

â€¢ Convergence: We formalize the concept of convergence and
the conditions required for the program synthesis technique to
converge to the correct program as the size of the noisy data set
increases.

1.2 Optimal Loss Functions
Different Loss Functions can be appropriate for different Noise
Sources and synthesis domains. For example, the 0/1 Loss Function,
which counts the number of input-output examples where the
noisy dataset and synthesized program ğ‘ disagree, is a general
Loss Function that assumes that the Noise Source can corrupt the
output arbitrarily. The Damerau-Levenshtein (DL) Loss Function [5]
measures the edit difference under character insertions, deletions,
substitutions, and/or transpositions. It can extract information
present in partially corrupted outputs and thus can be appropriate
for measuring loss when text strings are corrupted by a discrete
Noise Source.

1

 
 
 
 
 
 
, ,

If the Noise Source and a Loss Function are a good match, the
noisy synthesis is often able to extract enough information left
intact in corrupted outputs to synthesize the correct program. The
better the match between the Noise Source and Loss Function,
the higher the probablity that the noisy synthesis algorithm
will produce a correct program (i.e, a program equivalent to the
underlying hidden program). Given a noisy dataset and a Noise
Source, an optimal Loss Function has the highest probability of
returning a correct program. We formally define the concept of
an optimal Loss Function and derive a closed-form expression for
optimal Loss Functions (Section 3).

We also derive optimal Loss Functions for Noise Sources
previously studied in the noisy synthesis literature [13] (Section 5).
These case studies provide a theoretical explanation for the
empirical results provided in [13].

1.3 Convergence
Convergence is an important property which is well studied in the
statistics and machine learning literature [10, 15]. Convergence
properties connect the size of the noisy dataset to the probability
of synthesizing a correct underlying program. Given a synthesis
setting (i.e. a set of programs, an Input Source, a Noise Source, a
prior distribution, and a Loss Function), we define a convergence
property that allows us to guarantee that the probability of the
synthesis algorithm producing the correct underlying program
increases as the size of the noisy dataset increases.

This is the first paper to formulate the conditions required for
the synthesis algorithm to have convergence guarantees. Given
an Input Source and a Noise Source, these conditions allow us to
select an appropriate Loss Function, which will allow our synthesis
algorithm to guarantee convergence.

1.4 Contributions
This paper makes the following contributions:

â€¢ Formalism: It presents and formalizes a framework for noisy
program synthesis. The framework includes the concepts of a
prior probability distribution over programs, an Input Source,
and a Noise Source. It then formalizes how these structured
distributions interact to form a probabilistic process which
creates the Noisy Dataset. Given a noisy data set, this formalism
allows us to define a correct solution for the noisy synthesis
problem over that data set.

â€¢ Optimal Loss Function: It presents a framework to calculate
the expected reward associated with predicting a program, given
a noisy dataset. Based on this framework, it presents a technique
to design Loss Functions which are optimal , i.e., have the highest
probability of returning the correct solution to the synthesis
problem.

â€¢ Convergence: It formalizes the concept of convergence, i.e., for
any probability tolerance ğ›¿, there exists a threshold dataset size ğ‘˜,
such that, given a random noisy dataset of size ğ‘› â‰¥ ğ‘˜ generated
by a hidden program ğ‘, the synthesis algorithm will synthesize a
program equivalent to ğ‘ with probability greater than ğ›¿. Based
on this definition, this paper formulates conditions on the Input
Source, the Loss Function, and the Noise Source which ensure
convergence.

Shivam Handa and Martin Rinard

(Variable)

ğ‘¥ â‡’ ğ‘¥ (ğ‘¡)
(cid:75)

ğ‘›ğ‘˜
(cid:74)

ğ‘¥ â‡’ ğ‘£ğ‘˜
(cid:75)

(Function)

ğ‘¡
(cid:74)
. . .

(Constant)

ğ‘
ğ‘¥ â‡’ ğ‘
(cid:74)
(cid:75)
ğ‘¥ â‡’ ğ‘£1
(cid:75)
ğ‘“ (ğ‘›1, ğ‘›2, . . . ğ‘›ğ‘˜ )

ğ‘›2
(cid:74)

ğ‘›1
(cid:74)

ğ‘¥ â‡’ ğ‘£2
(cid:75)

ğ‘¥ â‡’ ğ‘“ (ğ‘£1, ğ‘£2, . . . ğ‘£ğ‘˜ )
(cid:75)

(cid:74)
Figure 1: Execution semantics for program ğ‘

â€¢ Case Studies: It presents multiple case studies highlighting Input
Sources, Noise Sources, and Loss Functions which break these
conditions and thus make convergence impossible. It also proves
that these conditions hold for some Noise Sources and Loss
Functions studied in prior work, thus providing a theoretical
explanation for the reported empirical results.

2 SYNTHESIS OVER NOISY DATA
Within this section, we formalize a conceptual framework to
view Noisy program synthesis. We also introduce a modified
noisy synthesis algorithm to accommodate the concept of prior
distribution of programs.

2.1 Noisy Program Synthesis Framework
We first define the programs we consider, how inputs to the program
are specified, and the program semantics. Without loss of generality,
we assume programs ğ‘ are specified as parse trees in a domain-
specific language (DSL) grammar G. Internal nodes represent
function invocations; leaves are constants/0-arity symbols in the
DSL. A program ğ‘ executes in an input ğ‘¥.
ğ‘¥ denotes the output
ğ‘
(cid:75)
(cid:74)
is defined in Figure 1).
of ğ‘ on input ğ‘¥ (

.

(cid:75)

All valid programs (which can be executed) are defined by a DSL

(cid:74)
grammar G = (ğ‘‡ , ğ‘ , ğ‘ƒ, ğ‘ 0) where:
â€¢ ğ‘‡ is a set of terminal symbols. These may include constants and
symbols which may change value depending on the input ğ‘¥.
â€¢ ğ‘ is the set of nonterminals that represent subexpressions in our

DSL.

â€¢ ğ‘ƒ is the set of production rules of the form ğ‘  â†’ ğ‘“ (ğ‘ 1, . . . , ğ‘ ğ‘›),
where ğ‘“ is a built-in function in the DSL and ğ‘ , ğ‘ 1, . . . , ğ‘ ğ‘› are
non-terminals in the grammar.

â€¢ ğ‘ 0 âˆˆ ğ‘ is the start non-terminal in the grammar.

We assume that we are given a black box implementation of each
built-in function ğ‘“ in the DSL. In general, all techniques explored
within this paper can be generalized to any DSL which can be
specified within the above framework. This is a standard way of
specifying DSLs in program synthesis literature [13, 22]

Example 1. The following DSL defines expressions over input x,

constants 2 and 3, and addition and multiplication:

ğ‘›
ğ‘¡

:= ğ‘¥ | ğ‘› + ğ‘¡
:= 2 | 3;

| ğ‘› Ã— ğ‘¡;

Notation: We will use the notation ğ‘ (ğ‘¥) to denote
ğ‘¥ within the
(cid:75)
rest of the paper. Given a vector of input values (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, ğ‘¥2, . . . ğ‘¥ğ‘›âŸ©,
we use the notation ğ‘ [ (cid:174)ğ‘¥] to denote vector âŸ¨ğ‘ (ğ‘¥1), ğ‘ (ğ‘¥2), . . . ğ‘ (ğ‘¥ğ‘›)âŸ©.
We use the notation ğº âŠ† G to denote a finite subset of programs
accepted by G. Given an input vector (cid:174)ğ‘¥, we use the notation ğº [ (cid:174)ğ‘¥]

ğ‘
(cid:74)

2

to denote the set of outputs produced by programs in ğº. Formally,

ğº [ (cid:174)ğ‘¥] = {ğ‘ [ (cid:174)ğ‘¥] | ğ‘ âˆˆ ğº }
Given a set of programs ğº, an input vector (cid:174)ğ‘¥, and an output vector (cid:174)ğ‘§,
we use the notation ğº (cid:174)ğ‘¥,(cid:174)ğ‘§ to denote the set of programs in ğº, which
given input vector (cid:174)ğ‘¥ produce the output (cid:174)ğ‘§. Formally,

ğº (cid:174)ğ‘¥,(cid:174)ğ‘§ = {ğ‘ âˆˆ ğº |ğ‘ [ (cid:174)ğ‘¥] = (cid:174)ğ‘§}
Given two programs ğ‘1, ğ‘2 âˆˆ ğº, we use the notion ğ‘1 â‰ˆ ğ‘2 to imply
program ğ‘1 is equivalent to ğ‘2, i.e., for all ğ‘¥ âˆˆ ğ‘‹ , ğ‘1 (ğ‘¥) = ğ‘2 (ğ‘¥).
Given an input vector (cid:174)ğ‘¥, we use the notion ğ‘1 â‰ˆ (cid:174)ğ‘¥ ğ‘2 to imply that
program ğ‘1 and ğ‘2 have the same outputs on input vector (cid:174)ğ‘¥, i.e.,
ğ‘1 [ (cid:174)ğ‘¥] = ğ‘2 [ (cid:174)ğ‘¥].
Given a set of programs ğº, we use the notation ğºğ‘â„ to denote the
set of programs in ğº which are equivalent to program ğ‘â„. Formally,
ğºğ‘â„ = {ğ‘ âˆˆ ğº | ğ‘ â‰ˆ ğ‘â„ }

to denote the set of programs in ğº which

We use the notation ğºğ¶
ğ‘â„
are not equivalent to program ğ‘â„. Formally,
ğºğ¶
ğ‘â„ = {ğ‘ âˆˆ ğº | ğ‘ (cid:48) ğ‘â„ }
Prior Distribution over Programs: Given a set of programs ğº, let
ğœŒğ‘ be a prior distribution over programs in ğº. We assume the hidden
underlying program ğ‘â„ is sampled from the prior distribution
with probability ğœŒğ‘ (ğ‘â„). Prior distributions over models are a
standard way to introduce prior knowledge withing systems which
learn from data [10]. The prior distribution allows us to direct
the synthesis procedure by introducing information about the
underlying process. The synthesis algorithm use this distribution
to trade off performance over the noisy data set and synthesizing
the most likely program.

Within this paper, we assume that ğœŒğ‘ is expressed via a set
ğº âŠ† G and a weight function ğ‘¤ G over the DSL G. We assume, we
are given a weight function ğ‘¤ G, which assigns a weight to each
terminal ğ‘¡ âˆˆ ğ‘‡ , and each production ğ‘  â† ğ‘“ (ğ‘ 1, . . . ğ‘ ğ‘˜ ) is ğ‘ƒ.

Given a program ğ‘ âˆˆ ğº, ğœŒğ‘ (ğ‘) is defined as

ğœŒğ‘ (ğ‘) =

ğ‘¤ (ğ‘ 0, ğ‘)
(cid:205)
ğ‘ âˆˆğº

ğ‘¤ (ğ‘ 0, ğ‘)

where ğ‘¤ (ğ‘) is computed via the following recursive definition:
:= ğ‘¤ğº (ğ‘¡)

ğ‘¤ (ğ‘¡, ğ‘¡)
ğ‘¤ (ğ‘ , ğ‘“ (ğ‘’1, . . . ğ‘’ğ‘›))

:=

(cid:205)
ğ‘ğ‘‘ âˆˆğ‘ƒ

ğ‘¤ğº (ğ‘ğ‘‘) Ã—

ğ‘›
(cid:206)
ğ‘–=1

ğ‘¤ (ğ‘ ğ‘–, ğ‘’ğ‘– )

where ğ‘ğ‘‘ are productions of the form ğ‘  â† ğ‘“ (ğ‘ 1, . . . ğ‘ ğ‘›).
Input Source: An Input Source is a probabilistic process which
generates the inputs provided to the hidden underlying program.
Formally, an Input Source is a probability distribution ğœŒğ‘– , from
which ğ‘› âˆˆ N inputs (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© are sampled with probability
ğœŒğ‘– (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© | ğ‘â„, ğ‘›). Where ğ‘â„ is hidden underlying program.
Note: Within this paper, we assume that inputs are independent
of the hidden program ğ‘â„. We leave the exploration of idea that
the inputs maybe selected to provide information about the hidden
program for future work.
Noise Source: A Noise Source ğ‘ is a probabilistic process which
corrupts the correct outputs returned by the hidden program to
create the noisy outputs. Formally, a Noise Source ğ‘ is attached

, ,

with a probability distribution ğœŒğ‘ . Given a hidden program ğ‘â„
and a set of outputs âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, the noisy outputs âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© are
sampled from the probability distribution ğœŒğ‘ , with probability

ğœŒğ‘ (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©)

Note: Within this paper, we will use the notation ğ‘ and ğœŒğ‘ for a
Noise Source interchangeably.
Noisy Dataset: A noisy dataset D is a set of input values, denoted
by (cid:174)ğ‘¥ and noisy output values, denoted by (cid:174)ğ‘¦. Within this paper,
we assume the dataset D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦) of size ğ‘› is constructed by the
following process:

â€¢ A hidden program ğ‘â„ is sampled from ğº with probability ğœŒğ‘ (ğ‘â„).
â€¢ ğ‘› inputs (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© are sampled from probability

distribution ğœŒğ‘– (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© | ğ‘›).

â€¢ We compute outputs (cid:174)ğ‘§ = âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, where ğ‘§ğ‘– = ğ‘â„ (ğ‘¥ğ‘– ).
â€¢ The Noise Source introduces noise by corrupting outputs

ğ‘§1, . . . ğ‘§ğ‘› to (cid:174)ğ‘¦ = âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© with probability

ğœŒğ‘ (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©)

Correctness: The goal of the synthesis procedure is to find a
program ğ‘ such that ğ‘ is equivalent to ğ‘â„, i.e., ğ‘ â‰ˆ ğ‘â„. But even in
absence of noise, it maybe be impossible to synthesize a program
equivalent to ğ‘â„.
Therefore, similar to Noise-Free programming-by-example systems
[18, 22], we relax the synthesis requirements to find a program ğ‘
such that ğ‘ and ğ‘â„ have the same outputs on the input vector (cid:174)ğ‘¥
(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„), given input vector (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©. Note that, even in
this relaxed setting, the noise introduced by the Noise Source may
make it impossible to acquire enough information to synthesize
the correct program.

In Section 4, we will tackle the harder problem of convergence,
i.e., probability of synthesizing a program ğ‘, equivalent to the
hidden program ğ‘â„ (ğ‘ â‰ˆ ğ‘â„) will improve as we increase the size
of the noisy dataset.

2.2 Loss Function
Given a dataset D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦) (inputs (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© and outputs (cid:174)ğ‘¦ =
âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©), and a program ğ‘, a Loss Function L (ğ‘, D) calculates
how incorrect the program is with respect to the given dataset. A
Loss Function, only depends on outputs (cid:174)ğ‘¦, and the outputs of the
program ğ‘ over inputs (cid:174)ğ‘¥ (i.e., (cid:174)ğ‘§ = ğ‘ [ (cid:174)ğ‘¥]). Given programs ğ‘1, ğ‘2,
such that for all ğ‘¥ âˆˆ (cid:174)ğ‘¥, ğ‘1 (ğ‘¥) = ğ‘2 (ğ‘¥), then L (ğ‘1, D) = L (ğ‘2, D).
We can rewrite the Loss Function as

L (ğ‘, D) = L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦)

Definition 1. Piecewise Loss Function: A Piecewise Loss

Function L (ğ‘, D) can be expressed in the following form

ğ‘›
âˆ‘ï¸

L (ğ‘, D) =

ğ¿(ğ‘ (ğ‘¥ğ‘– ), ğ‘¦ğ‘– )

ğ‘–=1
where D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦), (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, (cid:174)ğ‘¦ = âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©, and ğ¿(ğ‘§, ğ‘¦) is
the per-example Loss Function.

Definition 2. 0/1 Loss Function: The 0/1 Loss Function
L0/1 (ğ‘, D) counts the number of input-output examples where ğ‘

3

, ,

Shivam Handa and Martin Rinard

ğ‘¡ âˆˆ ğ‘‡ , (cid:174)ğ‘ = âŸ¨
ğ‘¥1, . . .
ğ‘¡
(cid:74)
(cid:75)
ğ‘ (cid:174)ğ‘
ğ‘¡ âˆˆ ğ‘„

ğ‘¡
(cid:74)

ğ‘¥ğ‘›âŸ©
(cid:75)

(Term)

ğ‘ (cid:174)ğ‘
ğ‘ 0
ğ‘ (cid:174)ğ‘
ğ‘ 0

âˆˆ ğ‘„

âˆˆ ğ‘„ ğ‘“

(Final)

12

Ã—3

ğ‘  â† ğ‘“ (ğ‘ 1, . . . ğ‘ ğ‘˜ ) âˆˆ ğ‘ƒ, {ğ‘ (cid:174)ğ‘1ğ‘ 1
(cid:174)ğ‘ = âŸ¨ğ‘ğ‘œ,1, . . . ğ‘ğ‘œ,ğ‘›âŸ©, ğ‘ğ‘œ,ğ‘– =
, . . . ğ‘ (cid:174)ğ‘ğ‘˜
ğ‘  âˆˆ ğ‘„, ğ‘“ (ğ‘ (cid:174)ğ‘1ğ‘ 1
ğ‘ (cid:174)ğ‘

, . . . ğ‘ (cid:174)ğ‘ğ‘˜
ğ‘ ğ‘˜ } âŠ† ğ‘„,
ğ‘“ (ğ‘1,ğ‘–, . . . ğ‘ğ‘™,ğ‘– )

(cid:74)
ğ‘ ğ‘˜ ) â†’ ğ‘ (cid:174)ğ‘

ğ‘  âˆˆ Î”

(cid:75)

(Prod)

ğ‘¥

1

8

4

2

3

7

Ã—2

+3

+2

6

Ã— 3
Ã—2,+3

+3

+ 2

Ã—3

5

9

+3

2
Ã—

,

2
+

Ã—2

+

2,

Ã—

3

Figure 2: Rules for constructing a FTA A = (ğ‘„, ğ¹, ğ‘„ ğ‘“ , Î”) given
inputs (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© and DSL G = (ğ‘‡ , ğ‘ , ğ‘ƒ, ğ‘ 0).

does not agree with the data set D:

L0/1 (ğ‘, D) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

1 if (ğ‘¦ğ‘– â‰  ğ‘ (ğ‘¥ğ‘– )) else 0

2.3 Complexity Measure
As previously defined in literature [13], given a program ğ‘, a
Complexity Measure ğ¶ (ğ‘) ranks programs independent of the
input-output examples in the dataset D. This measure is used to
synthesize a simple program out of the current correct candidate
programs. Formally, a complexity measure is a function ğ¶ (ğ‘) that
maps each program ğ‘ expressible in ğº to a real number. The
following Cost(ğ‘) complexity measure computes the complexity of
given program ğ‘ represented as a parse tree recursively as follows:

Cost(ğ‘¡) = cost(ğ‘¡)

Cost(ğ‘“ (ğ‘’1, ğ‘’2, . . . ğ‘’ğ‘˜ )) = cost(ğ‘“ ) +

ğ‘˜
(cid:205)
ğ‘–=1

Cost(ğ‘’ğ‘– )

where ğ‘¡ and ğ‘“ are terminals and built-in functions in our DSL G
respectively. Setting cost(ğ‘¡) = cost(ğ‘“ ) = 1 delivers a complexity
measure Size(ğ‘) that computes the size of ğ‘.

2.4 Program Synthesis over Noisy Data
We modify the synthesis algorithm presented in [13] to include
the concept of prior distributions over programs. The synthesis
algorithm builds upon Finite Tree Automata.

Definition 3 (FTA). A finite tree automaton (FTA) over alphabet
ğ¹ is a tuple A = (ğ‘„, ğ¹, ğ‘„ ğ‘“ , Î”) where ğ‘„ is a set of states, ğ‘„ ğ‘“ âŠ† ğ‘„ is
the set of accepting states, and Î” is a set of transitions of the form
ğ‘“ (ğ‘1, . . . , ğ‘ğ‘˜ ) â†’ ğ‘ where ğ‘, ğ‘1, . . . ğ‘ğ‘˜ are states, ğ‘“ âˆˆ ğ¹ .

Given an input vector (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, Figure 2 presents rules for
constructing a FTA that accepts all programs in grammar G. The
alphabet of the FTA contains built-in functions within the DSL. The
states in the FTA are of the form ğ‘ (cid:174)ğ‘
ğ‘  , where ğ‘  is a symbol (terminal or
non-terminal) in G and (cid:174)ğ‘ is a vector of values. The existence of state
ğ‘ (cid:174)ğ‘
ğ‘  implies that there exists a partial program which can map (cid:174)ğ‘¥ to
values (cid:174)ğ‘. Similarly, the existence of transition ğ‘“ (ğ‘ (cid:174)ğ‘1ğ‘ 1
ğ‘ ğ‘˜ ) â†’
ğ‘ (cid:174)ğ‘
ğ‘  implies âˆ€ğ‘— âˆˆ [1, ğ‘›].ğ‘“ ((cid:174)ğ‘1,ğ‘— , (cid:174)ğ‘2,ğ‘— . . . (cid:174)ğ‘ğ‘˜,ğ‘— ) = (cid:174)ğ‘ ğ‘— .

. . . ğ‘ (cid:174)ğ‘ğ‘˜

, ğ‘ (cid:174)ğ‘2ğ‘ 2

Figure 3: The FTA constructed for Example 2

ğ‘¡ (where (cid:174)ğ‘ğ‘– =

The Term rule states that if we have a terminal ğ‘¡ (either a
constant in our language or input symbol ğ‘¥), execute it with the
input ğ‘¥ğ‘– and construct a state ğ‘ (cid:174)ğ‘
ğ‘¥ğ‘– ). The Prod rule
(cid:75)
states that, if we have a production rule ğ‘“ (ğ‘ 1, ğ‘ 2, . . . ğ‘ ğ‘˜ ) â†’ ğ‘  âˆˆ Î”,
. . . ğ‘ (cid:174)ğ‘ğ‘˜
and there exists states ğ‘ (cid:174)ğ‘1ğ‘ 1
ğ‘ ğ‘˜ âˆˆ ğ‘„, then we also have state
ğ‘  in the FTA and a transition ğ‘“ (ğ‘ (cid:174)ğ‘1ğ‘ 1
, ğ‘ (cid:174)ğ‘2ğ‘ 2
ğ‘ ğ‘˜ ) â†’ ğ‘ (cid:174)ğ‘
ğ‘ (cid:174)ğ‘
ğ‘  .
The FTA Final rule (Figure 2) marks all states ğ‘ (cid:174)ğ‘
ğ‘ 0

with start
symbol ğ‘ 0 as accepting states regardless of the values (cid:174)ğ‘ attached to
the state.

, . . . ğ‘ (cid:174)ğ‘ğ‘˜

, ğ‘ (cid:174)ğ‘2ğ‘ 2

ğ‘¡
(cid:74)

The FTA divides the set of programs in the DSL into subsets.
Given an input set (cid:174)ğ‘¥, all programs in a subset produce the same
outputs (based on the accepting state), i.e., if a program ğ‘ is accepted
by the accepting state ğ‘ (cid:174)ğ‘
, then ğ‘ [ (cid:174)ğ‘¥] = (cid:174)ğ‘.
ğ‘ 0
In general, the rules in Figure 2 may result in a FTA which has
infinitely many states. To control the size of the resulting FTA, we
do not add a new state within the constructed FTA if the smallest
tree it will accept is larger than a given threshold ğ‘‘. This results
in a FTA which accepts all programs which are consistent with
the input-output example but are smaller than the given threshold
(it may accept some programs which are larger than the given
threshold but it will never accept a program which is inconsistent
with the input-output example). This is a standard practice in the
synthesis literature [18, 22].

We denote the FTA constructed from DSL G, given input vector
G [ (cid:174)ğ‘¥]. We omit the subscript grammar G

(cid:174)ğ‘¥ and threshold ğ‘ as Ağ‘‘
and threshold ğ‘ wherever it is obvious from context.

Example 2. Consider the DSL presented in Example 1. Given input
ğ‘¥ = 1, Figure 3 presents the FTA which represents all programs of
height less than 3.
For readability, we omit the states for terminals 2 and 3.

Algorithm 1 presents a modified version of the synthesis
algorithm presented by [13] to synthesize programs within the
noisy synthesis settings. The algorithm first constructs a FTA

4

which optimizes
, G, A, ğ‘‘) is defined in

over input vector (cid:174)ğ‘¥. It then finds the state ğ‘ (cid:174)ğ‘
ğ‘ 0
L ((cid:174)ğ‘, (cid:174)ğ‘¦) âˆ’ log ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0
figure 4.
ğ‘¤ (ğ‘ (cid:174)ğ‘

, G, A, ğ‘‘), where ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0

ğ‘  , ğ‘š) denotes the sum of weights of partial programs of size
ğ‘  . ğœ‹ (ğ‘ (cid:174)ğ‘
, G, A, ğ‘‘) denotes the sum of
ğ‘ 0
of size â‰¤ ğ‘‘. Note
, G, A, ğ‘‘) = ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘ ), where ğº is the subset of programs

â‰¤ ğ‘š, accepted by the state ğ‘ (cid:174)ğ‘
probabilities of all programs accepted by state ğ‘ (cid:174)ğ‘
ğ‘ 0
that, ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0
in G of height â‰¤ ğ‘‘.

Note that (cid:174)ğ‘ minimizes L ((cid:174)ğ‘, (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘ ), where ğº (cid:174)ğ‘¥,(cid:174)ğ‘ is the

set of programs in ğº which map input (cid:174)ğ‘¥ to output (cid:174)ğ‘.

Given the optimal state ğ‘âˆ—, we find the program accepted by
ğ‘âˆ— which minimizes our complexity metric ğ¶ (ğ‘). The following
equations hold true:

(cid:174)ğ‘ = arg min
(cid:174)ğ‘ âˆˆğº [ (cid:174)ğ‘¥ ]

L ((cid:174)ğ‘, (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘ ), ğ‘âˆ— = arg min
ğ‘ âˆˆğº (cid:174)ğ‘¥,(cid:174)ğ‘

ğ¶ (ğ‘)

ğ‘âˆ— âˆˆ ğº (cid:174)ğ‘¥,(cid:174)ğ‘ â‡â‡’ (cid:174)ğ‘ = ğ‘âˆ— [ (cid:174)ğ‘¥] and ğ‘âˆ— âˆˆ ğº

Algorithm 1: Synthesis Algorithm
Input : DSL G, prior distribution ğœ‹, threshold ğ‘‘, data set

D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦), Loss function L, complexity measure ğ¶

ğ‘‘ [ (cid:174)ğ‘¥] = (ğ‘„, ğ¹, ğ‘„ ğ‘“ , Î”)

Result: Synthesized program ğ‘âˆ—
A G
ğ‘âˆ— â† argminğ‘ (cid:174)ğ‘
ğ‘ 0
ğ‘âˆ— â† argminğ‘ âˆˆ(ğ‘„,ğ¹,{ğ‘âˆ— },Î”)ğ¶ (ğ‘)

âˆˆğ‘„ ğ‘“

L ((cid:174)ğ‘, (cid:174)ğ‘¦) âˆ’ log ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0

Given a FTA A, we can use dynamic programming to find the
minimum complexity parse tree (under the certain Cost(ğ‘) like
measures) accepted by A [9]. In general, given a FTA A, we assume
we are provided with a method to find the program ğ‘ accepted by
A which minimizes the complexity measure.

3 OPTIMAL LOSS FUNCTION
Within this section, we will formalize the connection between the
Noise Source and the Loss Function. We then derive a closed form
expression for the optimal Loss Function in case of perfect and
imperfect information about the Noise Source.

3.1 Optimal Loss Function given a Noise Source
Given dataset D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦), let us assume that a synthesis procedure
predicts ğ‘ to be the underlying hidden program. The Expected
Reward is the probability that ğ‘ generated dataset D. Formally,
given D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦),

, ,

Since the above reward only depends on the output of program ğ‘
on input set (cid:174)ğ‘¥, we can rewrite the above reward as

E ((cid:174)ğ‘ | (cid:174)ğ‘¥, (cid:174)ğ‘¦) =

1
ğœŒ ( (cid:174)ğ‘¦ | (cid:174)ğ‘¥)

ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘ )ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘)

where (cid:174)ğ‘ = ğ‘ [ (cid:174)ğ‘¥].

Therefore, given dataset D, the probability D was generated by

the synthesized program ğ‘ is E (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦).

Given a Loss Function L and prior distribution ğœŒğ‘ , our synthesis
algorithm is correct with probability E (ğ‘ğ‘™ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦), where ğ‘ğ‘™ is:

(cid:174)ğ‘ = arg min
(cid:174)ğ‘ âˆˆğº [ (cid:174)ğ‘¥ ]

L ((cid:174)ğ‘, (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘ ), ğ‘ğ‘™ = arg min
ğ‘ âˆˆğº (cid:174)ğ‘¥,(cid:174)ğ‘

ğ¶ (ğ‘)

ğ‘ğ‘™ will be the ideal prediction if it maximizes the expected reward.
Let Eğ¿ (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦) = âˆ’ log E (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦).

Eğ¿ (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦) = âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] ) + (âˆ’ log ğœŒğ‘ ( (cid:174)ğ‘¦ | ğ‘ [ (cid:174)ğ‘¥])) + ğ¶
Therefore, given a set of programs ğº, dataset D, prior
distribution ğœŒğ‘ , and Noise Source ğ‘ , and no other information
about the hidden program ğ‘â„, the synthesis algorithm will always
return the program which maximizes the expected reward if the
Loss Function L ((cid:174)ğ‘, (cid:174)ğ‘¦) is equal to âˆ’ log ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘)+ğ¶, for any constant
ğ¶. Hence, within this setting, âˆ’ log ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘) +ğ¶ is the optimal Loss
Function.

Note that, in expectation, no other Loss Function will out perform

3.2 Optimal Loss Function given imperfect

information

Let us now consider a scenario where we are presented with some
imperfect information about the Noise Source, i.e., the Noise Source
corrupting the correct output belongs to the set N and we are
presented with a prior probability distribution ğœŒ N over possible
Noise Sources in N . The probability that ğ‘ âˆˆ N is the underlying
Noise Source corrupting the correct dataset is ğœŒ N (ğ‘ ).

Given dataset D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦), we assume it was constructed by the

following underlying process:
â€¢ A Noise Source ğ‘ âˆˆ N is sampled from the prior distribution

ğœŒ N, with probability ğœŒ N (ğ‘ ).

â€¢ A hidden program ğ‘â„ is sampled from the set ğº with probability

distribution ğœŒğ‘ .

â€¢ ğ‘› inputs (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© are sampled from probability

distribution ğœŒğ‘– with probability ğœŒğ‘– (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ© |ğ‘›).

â€¢ The process then computes outputs (cid:174)ğ‘§ = âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, where ğ‘§ğ‘– =

ğ‘â„ (ğ‘¥ğ‘– ).

â€¢ The sampled Noise Source ğ‘ , introduces noise by corrupting

, G, A, ğ‘‘)

the above optimal version.

E (ğ‘ | (cid:174)ğ‘¥, (cid:174)ğ‘¦) =

âˆ‘ï¸

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒ (ğ‘â„ | (cid:174)ğ‘¥, (cid:174)ğ‘¦)

=

1
ğœŒ ( (cid:174)ğ‘¦ | (cid:174)ğ‘¥)

âˆ‘ï¸

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒğ‘ (ğ‘â„ | (cid:174)ğ‘¥)ğœŒğ‘ ( (cid:174)ğ‘¦|ğ‘â„ [ (cid:174)ğ‘¥])

=

1
ğœŒ ( (cid:174)ğ‘¦ | (cid:174)ğ‘¥)

âˆ‘ï¸

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒğ‘ (ğ‘â„)ğœŒğ‘ ( (cid:174)ğ‘¦|ğ‘â„ [ (cid:174)ğ‘¥])

E (ğ‘ | (cid:174)ğ‘¥, (cid:174)ğ‘¦) =

1
ğœŒ ( (cid:174)ğ‘¦ | (cid:174)ğ‘¥)

ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] )ğœŒğ‘ ( (cid:174)ğ‘¦ | ğ‘ [ (cid:174)ğ‘¥])

outputs ğ‘§1, . . . ğ‘§ğ‘› to (cid:174)ğ‘¦ = âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© with probability

ğœŒğ¶ (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©|âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, ğ‘ )

which is equal to

ğœŒğ‘ (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©)
Expected Reward: Given dataset D, let us assume that a synthesis
procedure predicts ğ‘ to be the underlying hidden program. The
Expected Reward is the probability dataset D was generated by
ğ‘ as the hidden underlying program. Formally, given D = ( (cid:174)ğ‘¥, (cid:174)ğ‘¦),

5

, ,

Shivam Handa and Martin Rinard

ğ‘¤ (ğ‘ (cid:174)ğ‘
ğ‘¤ (ğ‘ (cid:174)ğ‘

ğ‘¡ , ğ‘š)
ğ‘  , ğ‘š)

ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0

, G, A, ğ‘‘)

:= ğ‘¤ G (ğ‘¡)
:=

ğ‘“ (ğ‘ (cid:174)ğ‘1
ğ‘ 1
:= ğ‘¤ (ğ‘ (cid:174)ğ‘
ğ‘ 0

ğ‘¤ G (ğ‘“ )( (cid:206)

ğ‘– âˆˆ [1,ğ‘˜ ]

ğ‘¤ (ğ‘ (cid:174)ğ‘ğ‘–
ğ‘ ğ‘–

, ğ‘š âˆ’ 1))

(cid:205)
,...ğ‘ (cid:174)ğ‘ğ‘˜
ğ‘ ğ‘˜ )â†’ğ‘ (cid:174)ğ‘
, ğ‘‘)/ (cid:205)
ğ‘ (cid:174)ğ‘§
ğ‘ 0

âˆˆğ‘„ ğ‘“

ğ‘  âˆˆÎ”

ğ‘¤ (ğ‘ (cid:174)ğ‘§
ğ‘ 0

, ğ‘‘)

Figure 4: Rules for calculating ğœ‹ (ğ‘ (cid:174)ğ‘
ğ‘ 0

, G, A, ğ‘‘)

E (ğ‘ | (cid:174)ğ‘¥, (cid:174)ğ‘¦) =

âˆ‘ï¸

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒ (ğ‘â„ | (cid:174)ğ‘¥, (cid:174)ğ‘¦)

âˆ‘ï¸

âˆ‘ï¸

âˆ

ğ‘ âˆˆN

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒğ‘ (ğ‘â„ | (cid:174)ğ‘¥)ğœŒğ¶ ( (cid:174)ğ‘¦|ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ )ğœŒ N (ğ‘ )

âˆ‘ï¸

âˆ‘ï¸

=

ğ‘ âˆˆN

ğ‘â„ âˆˆğº

1(ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„)ğœŒğ‘ (ğ‘â„)ğœŒğ‘ ( (cid:174)ğ‘¦|ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ )ğœŒ N (ğ‘ )

= ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] )(

âˆ‘ï¸

ğ‘ âˆˆN

ğœŒğ¶ ( (cid:174)ğ‘¦ | ğ‘ [ (cid:174)ğ‘¥], ğ‘ )ğœŒ N (ğ‘ ))

Let Eğ¿ (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦) = âˆ’ log E (ğ‘ [ (cid:174)ğ‘¥] | (cid:174)ğ‘¥, (cid:174)ğ‘¦)

= âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] ) + (âˆ’ log(

âˆ‘ï¸

ğ‘ âˆˆN

ğœŒ N (ğ‘ )ğœŒğ¶ ( (cid:174)ğ‘¦ | ğ‘ [ (cid:174)ğ‘¥])))

Therefore, given a set of programs ğº, dataset D, prior
distribution ğœŒğ‘ , and no other information about the hidden program
ğ‘â„ or the hidden Noise Source, the synthesis algorithm will always
return the program which maximizes the expected reward if the
Loss Function L ((cid:174)ğ‘, (cid:174)ğ‘¦) is equal to

âˆ’[log

âˆ‘ï¸

ğ‘ âˆˆN

ğœŒğ¶ ( (cid:174)ğ‘¦ | (cid:174)ğ‘, ğ‘ )ğœŒ N (ğ‘ )] + ğ¶ = âˆ’ log ğ¸ [ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘)] + ğ¶

for any constant ğ¶. Hence, the optimal Loss function, in presence
of imperfect information of the Noise Source, is the negative log
of the expected probability of output (cid:174)ğ‘ being corrupted to noisy
output (cid:174)ğ‘¦.

4 CONVERGENCE
Within this section, we explore the conditions under which the
synthesis algorithm will have convergence guarantees.

Given a synthesis setting (i.e., a finite set of programs ğº, an
Input Source ğœŒğ‘– , a Noise Source ğœŒğ‘ , a prior probability ğœŒğ‘ , and a
Loss Function L), convergence property allows us to guarantee
synthesis algorithmâ€™s output will be the correct underlying program
with high probability if we are providing the algorithm with a large
enough dataset.

Given a Noise Source ğ‘ , a Loss Function L, prior probability
ğœŒğ‘ , a positive probability hidden program ğ‘â„ (i.e., ğœŒğ‘ (ğ‘â„) > 0), and
a dataset size ğ‘›, let ğ‘ƒğ‘Ÿ [ğ‘ğ‘›
ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] denote the probability on
synthesizing program equivalent ğ‘â„ on a random data set ( (cid:174)ğ‘¥, (cid:174)ğ‘¦) of
size ğ‘›, constructed assuming ğ‘â„ as the hidden program. Formally,
ğ‘ƒğ‘Ÿ [ğ‘ğ‘›
ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] is the probability of the following process
returning true.

â€¢ Sample ğ‘› inputs (cid:174)ğ‘¥ with probability ğœŒğ‘– ( (cid:174)ğ‘¥ | ğ‘›).
â€¢ (cid:174)ğ‘§â„ = ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦ is sampled from the distribution ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘§â„).

â€¢ Return true, if for all programs ğ‘ âˆˆ ğºğ¶
ğ‘â„

:

L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘â„ [ (cid:174)ğ‘¥ ] ) < L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] )
ğ‘  âˆˆ ğºğ‘â„ , such that for all ğ‘ âˆˆ ğºğ¶
and there exists a program ğ‘ğ‘›
ğ‘â„
where ğ‘ â‰ˆ (cid:174)ğ‘¥ ğ‘â„, ğ¶ (ğ‘ğ‘›
ğ‘  ) < ğ¶ (ğ‘).

,

Note that if the procedure returns true then the following is true:
L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,(cid:174)ğ‘§), ğ‘ğ‘›

ğ¶ (ğ‘)

ğ‘â„ [ (cid:174)ğ‘¥] = arg min
(cid:174)ğ‘§ âˆˆğº [ (cid:174)ğ‘¥ ]

ğ‘  = arg min
ğ‘ âˆˆğº (cid:174)ğ‘¥,ğ‘â„ [ (cid:174)ğ‘¥ ]

i.e., ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ is synthesized.

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] measures the probability that a program
equivalent to the hidden program is synthesized, given a random
noisy dataset using ğ‘â„ as the underlying program.

Definition 4. Convergence: Given a finite set of programs ğº,
a Loss Function L, a Noise Source ğ‘ , an Input Source ğœŒğ‘– , and a
probability distribution over programs ğœŒğ‘ , the synthesis will converge,
if for all ğ›¿ > 0, there exists a natural number ğ‘˜, such that for all
positive probability hidden program ğ‘â„ and ğ‘› â‰¥ ğ‘˜:

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] â‰¥ (1 âˆ’ ğ›¿)

i.e., for all probability tolerance ğ›¿ > 0, we can find a minimum dataset
size ğ‘˜, such that for all hidden programs and dataset sizes â‰¥ ğ‘˜, the
probability the synthesized program will be equivalent to the hidden
program is greater that (1 âˆ’ ğ›¿).

If the convergence property is true for a finite set of programs
ğº, a Loss Function L, a Noise Source ğ‘ , an Input Source ğœŒğ‘– , and a
probability distribution over programs ğœŒğ‘ , then for all ğ›¿ > 0, there
exists a natural number ğ‘˜, such that for all positive probability
programs ğ‘â„ (i.e., there is positive probability that ğ‘â„ will be
sampled), for any dataset of size greater than ğ‘˜, with probability
(1 âˆ’ ğ›¿), the algorithm will synthesize a program equivalent to ğ‘â„.

4.1 Differentiating Input Distributions
Even in absence of noise, the Input Source may hinder the ability
of the synthesis algorithm to converge to the hidden program. For
example, consider an Input Source which only generates vectors
(cid:174)ğ‘¥, such that, there exist two programs ğ‘1 and ğ‘2 which have
the same outputs on input (cid:174)ğ‘¥ (i.e., ğ‘1 [ (cid:174)ğ‘¥] = ğ‘2 [ (cid:174)ğ‘¥]). For such an
Input Source, the synthesis algorithm, even in the absence of noise,
cannot differentiate between datasets produced assuming ğ‘1 is
the underlying program and datasets produced assuming ğ‘2 is the
underlying program. This issue comes up in traditional Noise-Free
programming-by-example synthesis as well. Noise-Free synthesis
frameworks assume that the Input Source will eventually provide
input-output examples to differentiate the underlying program from
all other possible candidate programs to guarantee convergence. We

6

take a similar approach and constrain the Input Source to provide
convergence guarantees.

Let ğ‘‘ be some distance metric which measures distance between
two equally sized output vectors. For any underlying program
and a probability tolerance, increasing the dataset size should
eventually allow us to differentiate between this program and any
other program in ğº.

Definition 5. Differentiating Input Source: Given a set of

programs ğº and a distance metric ğ‘‘, an Input Source ğœŒğ‘– is
differentiating, if for a large enough dataset size, the distribution
will return an input dataset which will differentiate any two programs
within ğº, with a high probability.

An Input Source is differentiating if for all ğ›¿ > 0 and ğœ– > 0,
there exists a minimum dataset size ğ‘˜, such that for dataset sizes
ğ‘› â‰¥ ğ‘˜ and all programs ğ‘â„ âˆˆ ğº, the following process returns true
with probability greater than (1 âˆ’ ğ›¿):
â€¢ Sample (cid:174)ğ‘¥ of size ğ‘› from the distribution ğœŒğ‘– ( (cid:174)ğ‘¥).
â€¢ Return true if âˆ€ğ‘ âˆˆ Ëœğºğ‘â„ , ğ‘‘ (ğ‘ [ (cid:174)ğ‘¥], ğ‘â„ [ (cid:174)ğ‘¥]) â‰¥ ğœ–.

Formally, given a set of programs ğº and a distance metric ğ‘‘, an
Input Distribution ğœŒğ‘– is differentiating, if for all ğ›¿ > 0, for all
distance ğœ– > 0, there exists a natural number ğ‘˜, such that for all
natural numbers ğ‘› â‰¥ ğ‘˜, and for all programs ğ‘â„ âˆˆ ğº, the following
statement is true:

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

. ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿)

i.e., When sampling input vectors (cid:174)ğ‘¥ of length ğ‘›, with probability
greater than (1âˆ’ğ›¿), the distance between ğ‘â„ [ (cid:174)ğ‘¥] (output of program
ğ‘â„ on input (cid:174)ğ‘¥) and ğ‘ [ (cid:174)ğ‘¥], for all programs ğ‘ not equivalent to ğ‘â„, is
greater than ğœ–.

Having a differentiating Input Source ensures that as we increase
the size of the dataset, a random dataset will contain inputs which
will allow us to differentiate a program with other programs with
high probability.

4.2 Differentiating Noise Sources
Even if we are given an Input Source which allows us to differentiate
between the hidden underlying program and other programs in ğº
in the absence of noise, the Noise Source can, in theory, make
convergence impossible. For example, consider a Noise Source
which corrupts all outputs ğ‘§ to a single noisy output value ğ‘¦âˆ—.
A dataset corrupted by this Noise Source contains no information
about the underlying correct outputs. A synthesis process cannot
extract any information about the hidden underlying program from
such a dataset. Therefore, no synthesis algorithm will be able to
differentiate between different programs in the input program space.
Restrictions have to placed on the types of Noise Sources a synthesis
algorithm can handle in order to provide convergence guarantees.

, ,

vectors (cid:174)ğ‘§â„ of length ğ‘›, the following is true:

ğœŒğ‘ [âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›. L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) â‰¤ ğ›¾
=â‡’ ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) < ğœ– | ğ‘§â„] â‰¥ (1 âˆ’ ğ›¿)

If we are using the optimal Loss Function for the given Noise

Source (Subsection 3.1), the above condition reduces to:

(cid:104)
âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.

ğœŒğ‘

ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘§â„)
ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘§)

â‰¤ ğ›¾ =â‡’ ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) < ğœ–

(cid:105)

(cid:174)ğ‘§â„

(cid:12)
(cid:12)
(cid:12)

â‰¥ (1 âˆ’ ğ›¿)

Convergence:
Convergence is guaranteed in presence of a differentiating Input
Source and a differentiating Noise Source.

Theorem 1. Given a finite set of programs ğº, distribution ğœŒğ‘
from which the programs are sampled, a Loss Function L, a
differentiating Input Source ğœŒğ‘– , and a differentiating Noise
Source ğœŒğ‘ , then our synthesis algorithm will guarantee convergence.

We present the proof of this theorem in the Appendix A.1.

5 CASE STUDIES
In the previous section, we proved how having a differentiating
Input Source and a differentiating Noise Source are sufficient for the
synthesis algorithm to have convergence guarantees. Within this
section, we will prove that the Noise Sources and Loss Functions
studied in [13] fulfill these requirements, and therefore, allow the
synthesis algorithm to provide convergence guarantees. We will
then show how breaking these requirements makes convergence
impossible. We will also show the importance of picking an
appropriate distance metric ğ‘‘ which connects the Input Source
to the Noise Source.

5.1 Differentiating Input Distributions
In the special case, where each element of the input vector (cid:174)ğ‘¥ are i.i.d.,
the Differentiating Input Distribution condition can be simplified.

For any vector (cid:174)ğ‘¥ = âŸ¨ğ‘¥1, ğ‘¥2, . . . ğ‘¥ğ‘›âŸ©,

ğœŒğ‘– ( (cid:174)ğ‘¥) =

ğ‘›
(cid:214)

ğ‘—=1

ğœŒğ¼ (ğ‘¥ ğ‘— )

Given any two equal length vectors (cid:174)ğ‘§ = âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ© and (cid:174)ğ‘§â„ =
âŸ¨ğ‘§ â€²
1

ğ‘›âŸ©, let ğ‘‘ğ‘– be a distance metric such that

, . . . ğ‘§ â€²

ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) =

ğ‘›
âˆ‘ï¸

ğ‘—=1

ğ‘‘ğ‘– (ğ‘§ ğ‘— , ğ‘§ â€²
ğ‘— )

We say the input distribution ğœŒğ¼
program ğ‘â„ âˆˆ ğº, and ğ‘ âˆˆ ğºğ¶
ğ‘â„
ğœŒğ¼ (ğ‘¥ğ‘ ) > 0, and

is differentiating, if for all
there exists an input ğ‘¥ğ‘ , such that

ğ‘‘ğ‘– (ğ‘ (ğ‘¥ğ‘ ), ğ‘â„ (ğ‘¥ğ‘ )) > 0

Definition 6. Differentiating Noise Source: Given a finite
set of programs ğº, a distance metric ğ‘‘, and a Loss function L, a Noise
Source ğœŒğ‘ is differentiating, if for all ğ›¿ > 0 and ğ›¾ > 0, there exists
a natural number ğ‘˜, and ğœ– âˆˆ R+, such that for all ğ‘› â‰¥ ğ‘˜, for all

Theorem 2. If ğœŒğ¼ is differentiating then ğœŒğ‘– is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.2).

7

, ,

Shivam Handa and Martin Rinard

5.2 Differentiating Noise Distributions and

optimal Loss Function

Case 1: We first consider the case where the noise distribution
never introduces any corruptions in the correct output. Formally,
for all ğ‘› âˆˆ N, for all (cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›, ğœŒğ‘ ((cid:174)ğ‘§ | (cid:174)ğ‘§) = 1. Consider the Loss
Function L0/âˆ and the counting distance metric ğ‘‘ğ‘ :

Definition 7. 0/âˆ Loss Function: The 0/âˆ Loss Function

L0/âˆ ((cid:174)ğ‘§, (cid:174)ğ‘¦) is 0 if (cid:174)ğ‘§ = (cid:174)ğ‘¦ and âˆ otherwise.

Definition 8. Counting Distance The counting distance metric
ğ‘‘ğ‘ counts the number of positions two equal length vectors disagree
on, i.e.,

ğ‘‘ğ‘ (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘§ â€²
1

, . . . ğ‘§ â€²

ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

1(ğ‘§ğ‘– â‰  ğ‘§ â€²
ğ‘– )

Note that for all ğ›¾ > 0, for all ğ‘› â‰¥ 1, for all ğœ– â‰¥ 1, and for all

(cid:174)ğ‘§â„ âˆˆ ğ‘ ğ‘›, the following is true:

1(âˆ€ ğ‘§ âˆˆ ğ‘ ğ‘›.L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) â‰¤ ğ›¾ =â‡’

âˆ«

(cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) < ğœ–)ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘§â„)
= 1(âˆ€ ğ‘§ âˆˆ ğ‘ ğ‘›.L ((cid:174)ğ‘§, (cid:174)ğ‘§â„) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘§â„) â‰¤ ğ›¾ =â‡’ ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) < ğœ–) = 1

Therefore, in this case ğœŒğ‘ is differentiating.
Case 2: Consider the following ğ‘›-Substitution Noise Source and
ğ‘›-Substitution Loss Function studied in previous work [13].

Definition 9. ğ‘›-Substitution Noise Source:

The ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† , given an output vector
âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ© corrupts each string ğ‘§ğ‘– independently. For each string
ğ‘§ = ğ‘1 Â· Â· Â· ğ‘ğ‘˜ , it replaces character ğ‘ğ‘– with a random character not
equal to ğ‘ğ‘– with probability ğ›¿ğ‘›ğ‘† .

Definition 10. ğ‘›-Substitution Loss Function:

The ğ‘›-Substitution Loss Function Lğ‘›ğ‘† ((cid:174)ğ‘§, (cid:174)ğ‘¦) uses per-example Loss
Function ğ¿ğ‘›ğ‘† that captures a weighted sum of positions where the
noisy output string agrees and disagrees with the output from the
synthesized program. If the synthesized program produces an output
that is longer or shorter than the output in the noisy data set, the Loss
Function is âˆ:

Lğ‘›ğ‘† (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ¿ğ‘›ğ‘† (ğ‘§ğ‘–, ğ‘¦ğ‘– ), where

ğ¿ğ‘›ğ‘† (ğ‘§, ğ‘¦) =

âˆ’ log ğ›¿ğ‘– if ğ‘§ [ğ‘–] â‰  ğ‘¦ [ğ‘–] else âˆ’ log(1 âˆ’ ğ›¿ğ‘– )

|ğ‘§| = |ğ‘¦|

|ğ‘§| â‰  |ğ‘¦|

âˆ
|ğ‘§ |
(cid:205)
ğ‘–=1

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

Note that this Loss Function is a linear transformation of the

ğ‘›-Substitution Loss Function proposed in [13].

Definition 11. Length Distance Metric Given two equal
length vectors of strings, the length distance metric ğ‘‘ğ‘™ counts the
number of positions, which have unequal length strings in the two
vectors, i.e.,

ğ‘‘ğ‘™ (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘§ â€²
1

, . . . ğ‘§ â€²

ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

1(|ğ‘§ğ‘– | â‰  |ğ‘§ â€²

ğ‘– |)

8

Theorem 3. ğ‘›-Substitution Loss Function Lğ‘›ğ‘† is the optimal Loss
Function for the ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† . Also, given Length
Distance Metric ğ‘‘ğ‘™ and the ğ‘›-Substitution Loss Function Lğ‘›ğ‘† , ğ‘›-
Substitution Noise Source ğ‘ğ‘›ğ‘† is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.3).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of ğ‘›-Substitution
Noise Source and ğ‘›-Substitution Loss Function.
Case 3: Consider the 1-Delete Loss Function L1ğ· and the 1-Delete
Noise Source ğ‘1ğ· studied in previous work [13].

Definition 12. 1-Delete Noise Source: The 1-Delete Noise
source ğ‘1ğ· given an output vector âŸ¨ğ‘§1, ğ‘§2, . . . ğ‘§ğ‘›âŸ© independently
corrupts each output ğ‘§ğ‘– by deleting a random character, with
probability 0 < ğ›¿1ğ· < 1. Formally,

ğœŒğ‘1ğ· (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©) =

ğ‘›
(cid:214)

ğ‘–=1

ğœŒğ‘›1ğ· (ğ‘¦ğ‘– | ğ‘§ğ‘– )

where

ğœŒğ‘›1ğ· (ğ‘§ | ğ‘§) = 1 âˆ’ ğ›¿1ğ·

ğœŒğ‘›1ğ· (ğ‘ Â· ğ‘ |ğ‘ Â· ğ‘¥ Â· ğ‘) =

1
len(ğ‘ Â· ğ‘ Â· ğ‘)

ğ›¿1ğ·

where ğ‘ is a character.

Definition 13. 1-Delete Loss Function: The 1-Delete Loss
Function L1ğ· (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©) assigns loss âˆ’ log(1 âˆ’ ğ›¿ğ‘– )
if the output ğ‘§ğ‘– from the synthesized program and the data set ğ‘¦ğ‘–
match exactly, âˆ’ log ğ›¿ğ‘– if a single deletion enables the output from
the synthesized program to match the output from the data set, and
âˆ otherwise (for 0 < ğ›¿ğ‘– < 1):

L1ğ· (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ¿1ğ· (ğ‘§ğ‘–, ğ‘¦ğ‘– ), where

ğ¿1ğ· (ğ‘§, ğ‘¦) =

âˆ’ log(1 âˆ’ ğ›¿ğ‘– )
âˆ’ log ğ›¿ğ‘–
âˆ

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

ğ‘§ = ğ‘¦
ğ‘ Â· ğ‘¥ Â· ğ‘ = ğ‘§ âˆ§ ğ‘ Â· ğ‘ = ğ‘¦ âˆ§ |ğ‘¥ | = 1
otherwise

Note that this Loss Function is a linear transformation of the

1-Delete Loss Function proposed in [13].

Definition 14. DL-k Distance The DL-k Distance metric ğ‘‘ğ·ğ¿ğ‘˜ ,
given two vectors of strings, returns the count of string pairs with
Damerau-Levenshtein distance greater than equal to ğ‘˜ between them.
Formally,

ğ‘‘ğ·ğ¿ (âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©, âŸ¨ğ‘§ â€²
1

, . . . ğ‘§ â€²

ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

1(ğ¿ğ‘§ğ‘–,ğ‘§â€²

ğ‘–

(|ğ‘§ğ‘– |, |ğ‘§ â€²

ğ‘– |) â‰¥ ğ‘˜)

where, ğ¿ğ‘,ğ‘ (ğ‘–, ğ‘—) is the Damerau-Levenshtein metric [5].

Theorem 4. 1-Delete Loss Function L1ğ· is the optimal Loss
Function for the 1-Delete Noise Source ğ‘1ğ· . Also, given DL-2 Distance
Metric ğ‘‘ğ·ğ¿2 [5] and the 1-Delete Loss Function L1ğ· , 1-Delete Noise
Source ğ‘1ğ· is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.4)

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 1-Delete Noise
Source and 1-Delete Loss Function.
Case 4: Consider the Damerau-Levenshtein Loss Function Lğ·ğ¿
and the 1-Delete Noise Source ğ‘1ğ· studied in previous work [13].
Definition 15. Damerau-Levenshtein (DL) Loss Function:
The DL Loss Function Lğ·ğ¿ (ğ‘, D) uses the Damerau-Levenshtein
metric [5], to measure the distance between the output from the
synthesized program and the corresponding output in the noisy data
set:

Lğ·ğ¿ (ğ‘, D) =

âˆ‘ï¸

ğ¿ğ‘ (ğ‘¥ğ‘– ),ğ‘¦ğ‘–

(cid:0) |ğ‘ (ğ‘¥ğ‘– )| , |ğ‘¦ğ‘– | (cid:1)

(ğ‘¥ğ‘–,ğ‘¦ğ‘– ) âˆˆD

where, ğ¿ğ‘,ğ‘ (ğ‘–, ğ‘—) is the Damerau-Levenshtein metric [5].

This metric counts the number of single character deletions,
insertions, substitutions, or transpositions required to convert one
text string into another. Because more than 80% of all human
misspellings are reported to be captured by a single one of these
four operations [5], the DL Loss Function may be appropriate for
computations that work with human-provided text input-output
examples.

Theorem 5. Given DL-2 Distance Metric ğ‘‘ğ·ğ¿2 and the Damerau-
Levenshtein Loss Function Lğ·ğ¿, the 1-Delete Noise Source ğ‘1ğ· is
differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.5).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of 1-Delete Noise
Source and Damerau-Levenshtein Loss Function.
Case 5: Consider the Damerau-Levenshtein Loss Function Lğ·ğ¿ and
the ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† studied in previous work [13].

Theorem 6. Given Length Distance Metric ğ‘‘ğ‘™ and the Damerau-
Levenshtein Loss Function Lğ·ğ¿, the ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘†
is differentiating.

We present the proof of this theorem in the Appendix (
Theorem A.6).

Therefore, given a differentiating Input Source, the synthesis
algorithm will guarantee convergence in presence of ğ‘›-Substitution
Noise Source and Damerau-Levenshtein Loss Function.
Connections to Empirical Results: Handa et al. within their
paper [13], empirically evaluated Damerau-Levenshtein and 1-
Delete Loss Functions in presence of 1-Delete style noise. Both
1-Delete Loss Function and Damerau-Levenshtein Loss Function
are able to synthesize the correct program in presence of some
noise. This is in line with our convergence results. 1-Delete Loss
Function is also able to tolerate datasets with more noise, compared
to Damerau-Levenshtein Loss Function. Even when all input-output
examples were corrupted, 1-Delete Loss Function was able to
synthesize the correct program. Note that 1-Delete Loss Function
is the optimal Loss Function in presence of 1-Delete Noise Source,
therefore it has a higher probability of synthesizing the correct
program.

, ,

They also evaluated ğ‘›-Substitution Loss Function in presence of
the ğ‘›-Substitution Noise Source. Inline with our theoretical results,
their technique was able to synthesize the correct answer over
datasets corrupted by ğ‘›-Substitution Noise Source.

5.3 Non-Differentiating Input Distributions
Let ğº be a set of two programs ğ‘1 and ğ‘2 and let ğ‘¥ âˆ— be the only
input on which ğ‘1 and ğ‘2 disagree on, i.e., ğ‘1 (ğ‘¥ âˆ—) â‰  ğ‘2 (ğ‘¥ âˆ—) and
âˆ€ğ‘¥ âˆˆ ğ‘‹, ğ‘¥ â‰  ğ‘¥ âˆ— =â‡’ ğ‘1 [ğ‘¥] = ğ‘2 [ğ‘¥]. Let ğœŒğ‘ be a probability
distribution over programs in ğº. Without loss of generality, we
assume ğ¶ (ğ‘2) â‰¥ ğ¶ (ğ‘1). Let ğœŒğ‘– be the probability distribution over
input vectors.
Case 1: The input process never returns a differentiating input, i.e.,
for all ğ‘› âˆˆ N and for all (cid:174)ğ‘¥ âˆˆ ğ‘‹ ğ‘›, ğ‘¥ âˆ— âˆˆ (cid:174)ğ‘¥ =â‡’ ğœŒğ‘– ( (cid:174)ğ‘¥) = 0.

For all ğ‘› âˆˆ N and for all ğœ– > 0, the following statement is true

for both ğ‘â„ = ğ‘1 and ğ‘2:
âˆ«

1(âˆ€ğ‘ âˆˆ Ëœğºğ‘â„ . ğ‘‘ (ğ‘ [ (cid:174)ğ‘¥], ğ‘â„ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

âˆ«

â‰¤

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(ğ‘1 [ (cid:174)ğ‘¥] â‰  ğ‘2 [ (cid:174)ğ‘¥])ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) = 0

Therefore, ğœŒğ‘– is non-differentiating.

Theorem 7. In this case, ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] = 0

We present the proof of this theorem in the Appendix (
Theorem A.7).

Since ğ‘2 [ (cid:174)ğ‘¥] = ğ‘1 [ (cid:174)ğ‘¥] for all (cid:174)ğ‘¥, the best move for any algorithm
is to always predict the simplest program (which in this case is ğ‘2).
Hence, if the hidden program is not the simplest program, even in
the absence of noise, the synthesis algorithm will not converge to
the correct hidden program.
Case 2: The input process never returns a differentiating input
with sufficient probability, i.e., there exists a ğ›¿ğ‘– , such that for all
ğ‘› âˆˆ N,

1(ğ‘¥ âˆ— âˆˆ (cid:174)ğ‘¥)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) < ğ›¿ğ‘–

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

For all ğ‘› âˆˆ N and for all ğœ– > 0, the following statement is true for
both ğ‘â„ = ğ‘1 or ğ‘2:

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(âˆ€ğ‘ âˆˆ Ëœğºğ‘â„ . ğ‘‘ (ğ‘ [ (cid:174)ğ‘¥], ğ‘â„ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

âˆ«

â‰¤

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(ğ‘1 [ (cid:174)ğ‘¥] â‰  ğ‘2 [ (cid:174)ğ‘¥])ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) < ğ›¿ğ‘–

Therefore, ğœŒğ‘– is non-differentiating.

Theorem 8. In this case, ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] < ğ›¿ğ‘– .

We present the proof of this theorem in the Appendix (
Theorem A.8).

Hence, no matter how much we increase ğ‘› (i.e., the size of the
dataset sampled), the probability that ğ‘1 will be synthesized (in
presence of any Loss Function) is less than ğ›¿ğ‘– .

9

, ,

Shivam Handa and Martin Rinard

5.4 Non-Differentiating Noise Distributions
Case 1: We first consider the case where the Noise Source corrupts
all information identifying the hidden program. Let ğº be a set
containing two programs, ğ‘ğ‘ and ğ‘ğ‘ . ğ‘ğ‘ takes an input string ğ‘¥ and
appends character â€œğ‘â€ in front of it. ğ‘ğ‘ , similarly, takes an input
string ğ‘¥ and appends character â€œğ‘â€ in front of it.

ğ‘ğ‘
ğ‘ğ‘

:= append(â€œğ‘â€, ğ‘¥)
:= append(â€œğ‘â€, ğ‘¥)

Let ğ‘ be a Noise Source which deletes the first character of
the output string with probability 1. Note that in presence of this
Noise Source, no synthesis algorithm can infer which of the given
programs ğ‘ğ‘ or ğ‘ğ‘ is the hidden program.

Theorem 9. ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] + ğ‘ƒğ‘Ÿ [ğ‘ğ‘›
We present the proof of this theorem in the Appendix (
Theorem A.9).

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] â‰¤ 1

Therefore, any gains in improving convergence of the synthesis
algorithm by increasing ğ‘›, assuming ğ‘ğ‘ is the hidden program,
will be on the cost of convergence when ğ‘ğ‘ is the hidden program.
Formally,
ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] â‰¥ 1 âˆ’ ğ›¿ =â‡’ ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] â‰¤ ğ›¿

Case 2: The choice of Loss Function affects whether the Noise
Source is differentiating or not. For example, consider a Noise
Source which reveals information identifying the hidden program
with high probability, but the Loss Function does not capture this
information. 1-Delete Noise Source ğ‘1ğ· , given the 1-Delete Loss
function L1ğ· (and DL-2 distance metric ğ‘‘ğ·ğ¿2), is differentiating.
Since, ğ‘›-Substitution Loss Function Lğ‘›ğ‘† penalizes a deletion

with infinite loss, the following is true:

Theorem 10. Given ğ‘›-Substitution Loss Function Lğ‘›ğ‘† , 1-Delete
Noise Source ğ‘1ğ· is non-differentiating. In this case, the synthesis
algorithm will never converge.

We present the proof of this theorem in the Appendix (
Theorem A.10).

Handa et al. empirically showed within their paper [13] that the
ğ‘›-Substitution Noise Source requires all input-output examples to
be correct (i.e., it cannot tolerate any noise) when the specific noise
is introduced by the 1-Delete Noise Source. This theorem is in line
with their experimental results.

5.5 Necessity of the distance metric ğ‘‘
The distance metric serves as an important link between the Input
Source and the Noise Source. A simple distance metric (like the
counting distance) makes it easy to construct a differentiating Input
Source but a simple distance metric lack the restrictions required to
prove a Noise Source differentiating. Similarly, a restricted distance
metric may enable us to easily prove the Noise Source differentiating
but make it hard to construct a differentiating Input Source.

For example, consider the following case. Let ğº be a set
containing two programs ğ‘ğ‘ and ğ‘ğ‘ . ğ‘ğ‘ takes an input tuple (string
and a boolean) (ğ‘¥, ğ‘) and appends character â€œğ‘â€ in front of ğ‘¥ if ğ‘ is
true, else it appends â€œğ‘ğ‘â€ in front of ğ‘¥. ğ‘ğ‘ , similarly, takes an input
tuple (string and a boolean) (ğ‘¥, ğ‘) and appends character â€œğ‘â€ in

10

front of ğ‘¥ if ğ‘ is true, else it appends â€œğ‘ğ‘â€ in front of ğ‘¥. Formally,

ğ‘ğ‘
ğ‘ğ‘

:= append(if ğ‘ then â€œğ‘â€ else â€œğ‘ğ‘â€, ğ‘¥)
:= append(if ğ‘ then â€œğ‘â€ else â€œğ‘ğ‘â€, ğ‘¥)

Consider a Noise Source which deletes the first character with
probability 1. For counting distance metric ğ‘‘ğ‘ , an Input Source ğœŒğ‘–
which only returns inputs with ğ‘ set to true, is differentiating, i.e.,
the outputs produced by ğ‘ğ‘ and ğ‘ğ‘ will be of the from ğ‘ Â· ğ‘¥, where
ğ‘ is â€œğ‘â€ and â€œğ‘â€ respectively.

Theorem 11. There exists no Loss Functions for which the above
Noise Source is differentiating. Note that for ğœŒğ‘– described above, the
synthesis algorithm will not converge as well.

We present the proof of this theorem in the Appendix (
Theorem A.11).

But for DL-2 Distance metric ğ‘‘ğ·ğ¿2 (which only counts the
number of disagreements have atleast 2 Damerau-Levenshtein
between them), then an Input Source ğœŒğ‘– has to return inputs with
ğ‘ set to true, with high probability, to be differentiating.

Theorem 12. Consider a Loss Function Lğ‘ğ‘ which checks if the
first character appended to a string is either â€œğ‘â€ or â€œğ‘â€. Given DL-2
Distance metric ğ‘‘ğ·ğ¿2 and the Loss Function Lğ‘ğ‘ , the Noise Source
described above is differentiating.

In this case, if we pick a differentiating Input Source, our synthesis

algorithm will have convergence guarantees.

We present the proof of this theorem in the Appendix (
Theorem A.12).

But note that for space of programs considered in Subsection 5.4
(case 1), even through the Noise Source is differentiating, the
complex ğ‘‘ğ·ğ¿2 distance metric will not work as a differentiating
Input Source is impossible to construct for ğ‘‘ğ·ğ¿2 distance.

6 RELATED WORK
Noise-Free Programming-by-examples: The problem of
learning programs from a set of correct input-output examples has
been studied extensively [12, 20, 21]. Even though these techniques
provide correctness guarantees (and convergence guarantees if all
inputs which allow us to differentiate it from other programs are
provided), these techniques cannot handle noisy datasets.
Noisy Program Synthesis: There has been recent interest in
synthesizing programs over noisy datasets [13, 17]. These systems
do not formalize the requirements for their algorithms to converge
to the correct underlying program. Handa el alâ€™s work uses Loss
Functions as a parameter in their synthesis procedure to tailor it
to different noise sources. It does not provide any process to either
pick or design loss functions, given information about the noise
source.
Neural Program Synthesis: There is extensive work that uses
machine learning/deep neural networks to synthesize programs [3,
6, 19]. These techniques require a training phase, a differentable
loss function, and provide no formal correctness or convergence
guarantees. There also has been a lack of work on designing an
appropriate loss function, given information about the noise source.
Learning Theory: Learning theory captures the formal aspects
of learning models over noise data [4, 14, 15]. Our work takes
concepts from learning theory and applies them to the specific

, ,

[12] Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-

output examples. In ACM Sigplan Notices, Vol. 46. ACM, 317â€“330.

[13] Shivam Handa and Martin C Rinard. 2020. Inductive program synthesis over
noisy data. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
87â€“98.

[14] Knud Illeris. 2018. An overview of the history of learning theory. European

Journal of Education 53, 1 (2018), 86â€“101.

[15] Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. 1994. An

introduction to computational learning theory. MIT press.

[16] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed

program synthesis. ACM SIGPLAN Notices 50, 6 (2015), 619â€“630.

[17] Hila Peleg and Nadia Polikarpova. 2020. Perfect is the Enemy of Good: Best-Effort
Program Synthesis. In 34th European Conference on Object-Oriented Programming
(ECOOP 2020). Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik.

[18] Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: a framework for
inductive program synthesis. In ACM SIGPLAN Notices, Vol. 50. ACM, 107â€“126.
[19] Veselin Raychev, Pavol Bielik, Martin Vechev, and Andreas Krause. 2016. Learning
programs from noisy data. In ACM SIGPLAN Notices, Vol. 51. ACM, 761â€“774.

[20] D Shaw. 1975. Inferring LISP Programs From Examples.
[21] Rishabh Singh and Sumit Gulwani. 2016. Transforming spreadsheet data types

using examples. In Acm Sigplan Notices, Vol. 51. ACM, 343â€“356.

[22] Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017. Program synthesis using
abstraction refinement. Proceedings of the ACM on Programming Languages 2,
POPL (2017), 63.

[23] Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017. Synthesis of data completion
scripts using finite tree automata. Proceedings of the ACM on Programming
Languages 1, OOPSLA (2017), 62.

[24] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. 2019.

L_dmi: An
information-theoretic noise-robust loss function. arXiv preprint arXiv:1909.03388
(2019).

[25] Navid Yaghmazadeh, Christian Klinger, Isil Dillig, and Swarat Chaudhuri. 2016.
Synthesizing transformations on hierarchically structured data. In ACM SIGPLAN
Notices, Vol. 51. ACM, 508â€“521.

context of synthesizing programs over noisy data. To the best of
our knowledge, the special case of noisy program synthesis has
never been explored in learning theory.

There has considerable work done on designing loss functions
for training neural networks [11, 24]. To the best of our knowledge,
these works do not theoretically prove the optimality of their loss
functions with respect to a given noise source.

7 CONCLUSION
Learning models from noisy data with guarantees is an important
problem. There has been recent work on synthesizing programs
over noisy datasets using loss functions. Even though these systems
have delivered effective program synthesis over noisy datasets, they
do not provide any guidance for constructing loss functions given
prior information about the noise source, nor do they comment
on the convergence of their algorithms to the correct underlying
program.

We are the first paper to formalize the hidden process which
samples a hidden program and constructs a noisy dataset, thus
formally specifying the correct solution to a given noisy synthesis
problem. We are the first paper to formalize the concept of an
optimal loss function for noisy synthesis and provide a closed form
expression for such a loss function, in presence of perfect and
imperfect information about the noise source. We are the first
paper of formalize the constraints on the input source, the noise
source, and the loss function which allow our synthesis algorithm
to eventually converge to the correct underlying program. The
case studies highlight why these constraints are necessary. We also
provide proofs of convergence for the noisy program synthesis
problems studied in literature.

REFERENCES
[1] 2018. SyGuS 2018 String Benchmark Suite. https://github.com/SyGuS-Org/
benchmarks/tree/master/comp/2019/PBE_SLIA_Track/from_2018. Accessed:
2020-07-18.

[2] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund
Raghothaman, Sanjit A Seshia, Rishabh Singh, Armando Solar-Lezama, Emina
Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In 2013 Formal
Methods in Computer-Aided Design. IEEE, 1â€“8.

[3] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint
arXiv:1611.01989 (2016).

[4] Robert C Bolles. 1975. Learning theory. (1975).
[5] Fred J. Damerau. 1964. A Technique for Computer Detection and Correction of
Spelling Errors. Commun. ACM 7, 3 (March 1964), 171â€“176. https://doi.org/10.
1145/363958.363994

[6] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman
Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program learning under
noisy I/O. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org, 990â€“998.

[7] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri.
2017. Component-based synthesis of table consolidation and transformation
tasks from examples. In ACM SIGPLAN Notices, Vol. 52. ACM, 422â€“436.

[8] John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure
transformations from input-output examples. In ACM SIGPLAN Notices, Vol. 50.
ACM, 229â€“239.

[9] Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993.
Directed hypergraphs and applications. Discrete applied mathematics 42, 2-3
(1993), 177â€“201.

[10] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and

Donald B Rubin. 2013. Bayesian data analysis. CRC press.

[11] Aritra Ghosh, Himanshu Kumar, and PS Sastry. 2017. Robust loss functions under
label noise for deep neural networks. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 31.

11

, ,

A APPENDIX
A.1 Convergence

Shivam Handa and Martin Rinard

Theorem A.1. Given a finite set of programs ğº, a prior distribution ğœŒğ‘ , a Loss Function L, a differentiating Input Source ğœŒğ‘– , and a

differentiating Noise Source ğœŒğ‘ , the synthesis algorithm 1 will have convergence guarantees.

Proof. Given a ğ›¿ > 0, let ğ›¿ğ‘– > 0 and ğ›¿ğ‘ > 0 be two real numbers such that ğ›¿ = ğ›¿ğ‘– + ğ›¿ğ‘ .
For all positive probability programs ğ‘â„ âˆˆ ğº:

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] â‰¥

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

1(cid:0)âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

.L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ƒ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] ) > L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœ‹ (ğº (cid:174)ğ‘¥,ğ‘â„ [ (cid:174)ğ‘¥ ] )(cid:1)

ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘â„ [ (cid:174)ğ‘¥])

Let ğ›¾ = minğ‘ âˆˆğº ğœŒğ‘ (ğºğ‘ ). Note that log ğœ‹ (ğº (cid:174)ğ‘¥,ğ‘ [ (cid:174)ğ‘¥ ] ) âˆ’ log ğœ‹ (ğº (cid:174)ğ‘¥,ğ‘â„ [ (cid:174)ğ‘¥ ] ) > ğ›¾

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] â‰¥

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

1(cid:0)âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

.L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) > ğ›¾ (cid:1)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘â„ [ (cid:174)ğ‘¥])

âˆ«

â‰¥

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

1(cid:0)âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

.L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) > ğ›¾ (cid:1)ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘â„ [ (cid:174)ğ‘¥])

.ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) â‰¥ ğœ–) ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)
.ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) â‰¥ ğœ–, (cid:0)âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘§) â‰¥ ğœ– =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) > ğ›¾ (cid:1) =â‡’ (cid:0)âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

.L (ğ‘ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) > ğ›¾ (cid:1).

If âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„
Therefore,

âˆ«

â‰¥

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

1(cid:0)âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘§) â‰¥ ğœ– =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L (ğ‘â„ [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) > ğ›¾ (cid:1)

ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘â„ [ (cid:174)ğ‘¥])1(âˆ€ğ‘ âˆˆ ğº .ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) â‰¥ ğœ–) ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

Since ğœŒğ‘ is differentiating, we can assume that that for ğ›¿ğ‘ > 0, If ğ‘› â‰¥ ğ‘›ğ‘ , ğœ– â‰¥ ğœ–ğ‘ , ğ›¿ğ‘ > 0, the following statement is true, for any ğ‘§â„ and
ğ›¾ = log ğœ‹ (ğºğ¶

ğ‘â„ ) âˆ’ log ğœ‹ (ğºğ‘â„ ):

Since ğœŒğ‘– is differentiating, we can assume that for ğ›¿ğ‘– > 0 there exists an ğ‘›ğ‘– , for ğ‘› â‰¥ ğ‘›ğ‘– , the following is true:

(cid:104)
âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›. ğ‘‘ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ ğœ– =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) > ğ›¾

ğœŒğ‘

(cid:105)

(cid:174)ğ‘§â„

(cid:12)
(cid:12)
(cid:12)

â‰¥ (1 âˆ’ ğ›¿ğ‘ )

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

. ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿ğ‘– )

Then for ğ‘› â‰¥ max(ğ‘›ğ‘ , ğ‘›ğ‘– ), the following is true:

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘â„ | ğ‘â„, ğ‘ ] â‰¥ (1 âˆ’ ğ›¿ğ‘ )

âˆ«

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

. ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿ğ‘ )(1 âˆ’ ğ›¿ğ‘– ) â‰¥ (1 âˆ’ ğ›¿)

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

â–¡

A.2 Supplementary Material: Case Studies
Differentiating Input Distributions

Theorem A.2. If ğœŒğ¼ is differentiating then ğœŒğ‘– is differentiating.
Proof. Let for all ğ‘ âˆˆ ğºğ¶
ğ‘â„

, ğ‘¥ğ‘ be an input such that ğœŒğ¼ (ğ‘¥ğ‘ ) > 0

Let ğ›¿ğ‘– and ğœ–ğ‘– be the largest rational numbers such that, for all ğ‘, ğœŒğ¼ (ğ‘¥ğ‘ ) â‰¥ ğ›¿ğ‘– and ğ‘‘ğ‘– (ğ‘ (ğ‘¥ğ‘ ), ğ‘â„ (ğ‘¥ğ‘ )) â‰¥ ğœ–ğ‘– .
Let ğœ– and ğ›¿ be any rational number greater than 0. Let ğ‘š and ğ‘›0 be natural numbers such that ğ‘š â‰¥ ğœ–
ğœ–ğ‘–

, for ğ‘› â‰¥ |ğº |ğ‘›0,

ğ‘‘ğ‘– (ğ‘ (ğ‘¥ğ‘ ), ğ‘â„ (ğ‘¥ğ‘ )) > 0

ğ‘š
âˆ‘ï¸

ğ‘—=0

ğ‘— ğ›¿ ğ‘—
ğ¶ğ‘›0

ğ‘– (1 âˆ’ ğ›¿ğ‘– )ğ‘›0âˆ’ğ‘— â‰¤

ğ›¿
|ğº |

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

. ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) > ğœ–)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

12

âˆ«

â‰¥

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›
âˆ«

â‰¥

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›
ğ‘š
âˆ‘ï¸

= (1 âˆ’

ğ‘—=0

1(âˆ€ğ‘ âˆˆ ğºğ¶
ğ‘â„

. ğ‘‘ (ğ‘â„ [ (cid:174)ğ‘¥], ğ‘ [ (cid:174)ğ‘¥]) > ğ‘šğœ–ğ‘– )ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

1(âˆ€ ğ‘ âˆˆ ğºğ¶
ğ‘â„

.atleast ğ‘š ğ‘¥ğ‘ occur in (cid:174)ğ‘¥)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

ğ‘— ğ›¿ ğ‘—
ğ¶ğ‘›0

ğ‘– (1 âˆ’ ğ›¿ğ‘– )ğ‘›0âˆ’ğ‘— ) |ğº | â‰¥ (1 âˆ’

ğ›¿
|ğº |

) |ğº | â‰¥ 1 âˆ’ ğ›¿

, ,

â–¡

Differentiating Noise Distributions and optimal Loss Function
Case 2:

Theorem A.3. ğ‘›-Substitution Loss Function Lğ‘›ğ‘† is the optimal Loss Function for the ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† . Also, given Length

Distance Metric ğ‘‘ğ‘™ and the ğ‘›-Substitution Loss Function Lğ‘›ğ‘† , ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† is differentiating.

Proof.

where:

âˆ’ log ğœŒğ‘ğ‘›ğ‘† (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

âˆ’ log ğœŒğ‘›ğ‘›ğ‘† (ğ‘¦ğ‘– | ğ‘§ğ‘– )

âˆ’ log ğœŒğ‘›ğ‘›ğ‘† (ğ‘  â€²

1 Â· . . . Â· ğ‘  â€²

ğ‘˜ | ğ‘ 1 Â· . . . Â· ğ‘ ğ‘˜ ) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

(âˆ’1(ğ‘  â€²

ğ‘– = ğ‘ ğ‘– ) log(1 âˆ’ ğ›¿ğ‘– )) + (âˆ’1(ğ‘  â€²

ğ‘– â‰  ğ‘ ğ‘– ) log ğ›¿ğ‘– )

Note that Lğ‘›ğ‘† = âˆ’ log ğœŒğ‘ğ‘›ğ‘†

. Hence ğ‘›-Substitution Loss Function Lğ‘›ğ‘† is the optimal Loss Function for ğ‘›-Substitution Noise Source.

if ğ‘‘ğ‘™ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ 1, then for all samples (cid:174)ğ‘¦, Lğ‘›ğ‘† ((cid:174)ğ‘§, (cid:174)ğ‘¦) = âˆ. Therefore for all ğ›¾ > 0, ğ›¿ > 0,

ğœŒğ‘ [âˆ€ (cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ğ‘™ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ 1 =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) = âˆ > ğ›¾ | (cid:174)ğ‘§â„] = 1 â‰¥ (1 âˆ’ ğ›¿)

â–¡

Case 3:

Theorem A.4. 1-Delete Loss Function L1ğ· is the optimal Loss Function for the 1-Delete Noise Source ğ‘1ğ· . Also, given DL-2 Distance Metric

ğ‘‘ğ·ğ¿2 and the 1-Delete Loss Function L1ğ· , 1-Delete Noise Source ğ‘1ğ· is differentiating.

Proof.

âˆ’ log ğœŒğ‘1ğ· (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ© | âŸ¨ğ‘§1, . . . ğ‘§ğ‘›âŸ©) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

âˆ’ log ğœŒğ‘›1ğ· (ğ‘¦ğ‘– | ğ‘§ğ‘– )

where ğœŒğ‘›1ğ· (ğ‘§ | ğ‘§) = (1 âˆ’ ğ›¿ğ‘– ) and ğœŒğ‘›1ğ· (ğ‘¦ | ğ‘§) = ğ›¿ğ‘– if ğ‘¦ has exactly one character deleted with respect to ğ‘§.

Note that L1ğ· = âˆ’ log ğœŒğ‘1ğ·
If ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ 1 then L1ğ· ((cid:174)ğ‘§, (cid:174)ğ‘¦) = âˆ. Therefore for all ğ›¾ > 0, ğ›¿ > 0,

. Hence 1-Delete Loss Function L1ğ· is the optimal Loss Function for 1-Delete Noise Source.

ğœŒğ‘ [âˆ€ (cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ 1 =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) = âˆ > ğ›¾ | (cid:174)ğ‘§â„] = 1 â‰¥ (1 âˆ’ ğ›¿)

â–¡

Case 4:

Theorem A.5. Given DL-2 Distance Metric ğ‘‘ğ·ğ¿2 and the Damerau-Levenshtein Loss Function Lğ·ğ¿, the 1-Delete Noise Source ğ‘1ğ· is

differentiating.

Proof. If ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ ğ‘š, then Lğ·ğ¿ ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ Lğ·ğ¿ ((cid:174)ğ‘§, (cid:174)ğ‘¦) â‰¥ ğ‘š . Therefore for all ğ›¾ > 0, ğ›¿ > 0,

ğœŒğ‘ [âˆ€ (cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ ğ›¾ =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) > ğ›¾ | (cid:174)ğ‘§â„] = 1 â‰¥ (1 âˆ’ ğ›¿)

â–¡

Case 5:

Theorem A.6. Given Length Distance Metric ğ‘‘ğ‘™ and the Damerau-Levenshtein Loss Function Lğ·ğ¿, the ğ‘›-Substitution Noise Source ğ‘ğ‘›ğ‘† is

differentiating.

13

, ,

Shivam Handa and Martin Rinard

Proof. If ğ‘‘ğ‘™ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ ğ‘š, then Lğ·ğ¿ ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ Lğ·ğ¿ ((cid:174)ğ‘§, (cid:174)ğ‘¦) â‰¥ ğ‘š (Assuming replacements take place from a very large set). Therefore for all

ğ›¾ > 0, ğ›¿ > 0,

ğœŒğ‘ [âˆ€ (cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›.ğ‘‘ğ‘™ ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ ğ›¾ =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) > ğ›¾ | (cid:174)ğ‘§â„] = 1 â‰¥ (1 âˆ’ ğ›¿)

Non-Differentiating Input Distributions
Case 1:

Theorem A.7. In this case, ğ‘ƒğ‘Ÿ [ğ‘ğ‘›
Proof.

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] = 0

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] â‰¤

âˆ«

1(cid:0)L (ğ‘2 [ (cid:174)ğ‘¥], (cid:174)ğ‘¦)âˆ’

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘2 [ (cid:174)ğ‘¥ ] ) > L (ğ‘1 [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘1 [ (cid:174)ğ‘¥ ] )(cid:1)ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘1 [ (cid:174)ğ‘¥])ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)
1(cid:0)L (ğ‘2 [ (cid:174)ğ‘¥], ğ‘1 [ (cid:174)ğ‘¥]) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘2 [ (cid:174)ğ‘¥ ] ) > L (ğ‘1 [ (cid:174)ğ‘¥], ğ‘1 [ (cid:174)ğ‘¥]) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘1 [ (cid:174)ğ‘¥ ] )(cid:1)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

âˆ«

=

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

= 0

Case 2:

Theorem A.8. In this case, ğ‘ƒğ‘Ÿ [ğ‘ğ‘›
Proof.

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] < ğ›¿ğ‘– .

ğ‘  â‰ˆ ğ‘1 | ğ‘1, ğ‘ ] â‰¤
1(cid:0)L (ğ‘2 [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ (log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘2 [ (cid:174)ğ‘¥ ] )) > L (ğ‘1 [ (cid:174)ğ‘¥], (cid:174)ğ‘¦) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘1 [ (cid:174)ğ‘¥ ] )(cid:1)ğœŒğ‘ (ğ‘‘ (cid:174)ğ‘¦ | ğ‘1 [ (cid:174)ğ‘¥])ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

âˆ«

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›, (cid:174)ğ‘¦ âˆˆğ‘Œ ğ‘›

âˆ«

=

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(cid:0)L (ğ‘2 [ (cid:174)ğ‘¥], ğ‘1 [ (cid:174)ğ‘¥]) âˆ’ (log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘2 [ (cid:174)ğ‘¥ ] )) > L (ğ‘1 [ (cid:174)ğ‘¥], ğ‘1 [ (cid:174)ğ‘¥]) âˆ’ log ğœŒğ‘ (ğº (cid:174)ğ‘¥,ğ‘1 [ (cid:174)ğ‘¥ ] )(cid:1)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥)

âˆ«

â‰¤

(cid:174)ğ‘¥ âˆˆğ‘‹ ğ‘›

1(ğ‘1 [ (cid:174)ğ‘¥] â‰  ğ‘2 [ (cid:174)ğ‘¥])ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) < ğ›¿ğ‘–

Non-Differentiating Noise Distributions
Case 1:

â–¡

â–¡

â–¡

Theorem A.9. ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] + ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] â‰¤ 1

Proof. For distance metric ğ‘‘ğ‘ , an Input Source ğœŒğ‘– which only returns âŸ¨ğ‘¥, trueâŸ© is differentiating, as for all ğ‘¥, ğ‘ğ‘ (âŸ¨ğ‘¥, trueâŸ©) â‰  ğ‘ğ‘ (âŸ¨ğ‘¥, trueâŸ©).

Note that for all Loss Functions L,

ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
+ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
= ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
+ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
â‰¤ 1

Case 2:

Theorem A.10. Given ğ‘›-Substitution Loss Function Lğ‘›ğ‘† , 1-Delete Noise Source ğ‘1ğ· is non-differentiating. In this case, the synthesis algorithm

will never converge.

Proof. If even a single deletions happen in (cid:174)ğ‘¦, then Lğ‘›ğ‘† ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) = âˆ. The prob of no deletions is equal to (1 âˆ’ ğ›¿ğ‘– )ğ‘› which decreases with ğ‘›,
â–¡

therefore in this case 1-Delete Noise Source is non-differentiating.

Necessity of the distance metric ğ‘‘

14

â–¡

Theorem A.11. There exists no Loss Functions for which the above Noise Source is differentiating. Note that for ğœŒğ‘– described above, the

synthesis algorithm will not converge as well.

Proof. For distance metric ğ‘‘ğ‘ , a ğœŒğ‘– which only returns âŸ¨ğ‘¥, trueâŸ© is differentiating, as for all ğ‘¥, ğ‘ğ‘ (âŸ¨ğ‘¥, trueâŸ©) â‰  ğ‘ğ‘ (âŸ¨ğ‘¥, trueâŸ©). Note that for

all Loss Functions L,

ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
+ğœŒğ‘ [L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) âˆ’ L (âŸ¨ğ‘¥1, . . . ğ‘¥ğ‘›âŸ©, âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©) > ğ›¾ | âŸ¨ğ‘ Â· ğ‘¥1, . . . ğ‘ Â· ğ‘¥ğ‘›âŸ©]
â‰¤ 1

Therefore ğœŒğ‘ is not differentiating.

Similarly,

, ,

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘] + ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘ ] â‰¤ 1

Therefore, if for any ğ‘›, ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘] â‰¥ (1 âˆ’ ğ›¿) then ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘ ] â‰¤ ğ›¿

â–¡

Theorem A.12. Consider a Loss Function which checks if the first character appended to a string is either â€œğ‘â€ or â€œğ‘â€. Given the above Loss

Function and DL-2 Distance metric ğ‘‘ğ·ğ¿2, the Noise Source described above is differentiating.

In this case, if we pick a differentiating Input Source, our synthesis algorithm will have convergence guarantees.

Proof. If L (âŸ¨ğ‘¦1, . . . ğ‘¦ğ‘›âŸ©, âŸ¨â€œğ‘ğ‘â€ Â· ğ‘¥1, . . . â€œğ‘ğ‘â€ Â· ğ‘¥1âŸ©) = âˆ if for some ğ‘–, ğ‘¦ğ‘– = â€œğ‘â€ Â· ğ‘¥ğ‘– . Similarly, we can define this in the case of Therefore if

for two vectors (cid:174)ğ‘§1 (cid:174)ğ‘§2, ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§1, (cid:174)ğ‘§2) â‰¥ 1, then L ((cid:174)ğ‘§2, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§1, (cid:174)ğ‘¦) = âˆ if ğœŒğ‘ ( (cid:174)ğ‘¦ | (cid:174)ğ‘§) > 0.

Note that if the Input Source is differentiating then the following is true.

ğœŒğ‘ [âˆ€(cid:174)ğ‘§ âˆˆ ğ‘ ğ‘›. ğ‘‘ğ·ğ¿2 ((cid:174)ğ‘§, (cid:174)ğ‘§â„) â‰¥ 1 =â‡’ L ((cid:174)ğ‘§, (cid:174)ğ‘¦) âˆ’ L ((cid:174)ğ‘§â„, (cid:174)ğ‘¦) = âˆ | (cid:174)ğ‘§â„] = 1] = 1

Note that

Similarly,

1(âŸ¨?, trueâŸ© âˆˆ (cid:174)ğ‘¥)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿)

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] = 1(âŸ¨?, trueâŸ© âˆˆ (cid:174)ğ‘¥)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿)

ğ‘ƒğ‘Ÿ [ğ‘ğ‘›

ğ‘  â‰ˆ ğ‘ğ‘ | ğ‘ğ‘, ğ‘ ] = 1(âŸ¨?, trueâŸ© âˆˆ (cid:174)ğ‘¥)ğœŒğ‘– (ğ‘‘ (cid:174)ğ‘¥) â‰¥ (1 âˆ’ ğ›¿)

â–¡

15

