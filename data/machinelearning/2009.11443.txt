0
2
0
2

p
e
S
4
2

]

G
L
.
s
c
[

1
v
3
4
4
1
1
.
9
0
0
2
:
v
i
X
r
a

Neurocoder: Learning General-Purpose Computation Using
Stored Neural Programs

Hung Le and Svetha Venkatesh
Applied AI Institute, Deakin University, Geelong, Australia
{thai.le,svetha.venkatesh}@deakin.edu.au

Abstract

Artiﬁcial Neural Networks are uniquely adroit at machine learning by processing data through
a network of artiﬁcial neurons. The inter-neuronal connection weights represent the learnt Neural
Program that instructs the network on how to compute the data. However, without an external
memory to store Neural Programs, they are restricted to only one, overwriting learnt programs when
trained on new data. This is functionally equivalent to a special-purpose computer. Here we design
Neurocoder, an entirely new class of general-purpose conditional computational machines in which
the neural network “codes” itself in a data-responsive way by composing relevant programs from a
set of shareable, modular programs. This can be considered analogous to building Lego structures
from simple Lego bricks. Notably, our bricks change their shape through learning. External memory
is used to create, store and retrieve modular programs. Like today’s stored-program computers,
Neurocoder can now access diverse programs to process diﬀerent data. Unlike manually crafted
computer programs, Neurocoder creates programs through training. Integrating Neurocoder into
current neural architectures, we demonstrate new capacity to learn modular programs, handle
severe pattern shifts and remember old programs as new ones are learnt, and show substantial
performance improvement in solving object recognition, playing video games and continual learning
tasks. Such integration with Neurocoder increases the computation capability of any current neural
network and endows it with entirely new capacity to reuse simple programs to build complex ones.
For the ﬁrst time a Neural Program is treated as a datum in memory, paving the ways for modular,
recursive and procedural neural programming.

1 Introduction

From its inception in 1943 until recently, the fundamental architectures of Artiﬁcial Neural Networks
remained largely unchanged - a program is executed by passing data through a network of artiﬁcial
neurons whose inter-neuronal connection weights are learnt through training with data. These inter-
neuronal connection weights, or Neural Programs, correspond to a program in modern computers [30].
Memory Augmented Neural Networks (MANN) are an innovative solution allowing networks to access
external memory for manipulating data [10, 11]. But they were still unable to store Neural Programs
in such external memory, and this severely limits machine learning. Storing inter-neuronal connection
weights only in their network does not permit modular separation of Neural programs and is analogous
to a computer with one ﬁxed program. Recent works introduce conditional computation, adjusting
or activating parts of a network in an input-dependent manner [35, 31, 3, 12], but networks remain

1

 
 
 
 
 
 
Figure 1: Overview structure of Neurocoder. Main Network processes inputs to produce outputs.
Program Memory stores modular units. Program Controller reads modular units from the Program
Memory, composing Neural Programs for the Main Network in a data-driven manner. Each Neural Pro-
gram is designed for speciﬁc input. Intuitively, it is analogous to building Lego structures corresponding
to inputs from basic Lego bricks.

monolithic. Current networks forget when retrained, old inter-neuronal connection weights are merged
with new ones or erased.

The brain is modular, not a monolithic system [7, 5]. Neuroscience research indicates that the
brain is divided into functional modules [18, 6, 8]. If the neural program for each module is kept in
separate networks, networks proliferate. Modular neural networks combine the output of multiple
expert networks, but as the experts grow, the networks grow drastically [19, 13, 32, 27]. This requires
huge computational storage and introduces redundancy as these experts do not share common basic
programs.

A pathway out of this bind is to keep such basic programs in memory and combine them as required.
This brings neural networks towards modern general-purpose computers that use the stored-program
principle [33, 36] to eﬃciently access reusable programs in external memory. Here we show how
Neurocoder, a new neural framework, introduces an entirely new class of general-purpose conditional
computation machines in which an entire main neural network can be “coded” in an input-dependent
manner. Eﬃcient decomposition of Neural Programs creates shareable modular components that can
reconstruct the whole program space. These components change their “shapes” based on training and
are stored in an external Program Memory. Then, in a data-responsive way, a Program Controller
retrieves relevant modular components to reconstruct the Neural Program. The process is analogous to
shape-shifting Lego bricks that can be reused to build unlimited shapes and structures (See Fig. 1).

Using modular components vastly increases the learning capacity of the neural network by allowing
re-utilisation of parameters, eﬀectively curbing network growth as programs increase. The construction
of modular components and the input-speciﬁc reconstruction of Neural Programs from the components
is learnt through training via traditional backpropagation [28] as the architecture is end-to-end

2

ProgramControllerReconstruct	and	load	Neural	Programs8Main	Network1ProgramMemoryRetrieve	modularunits	based	on	inputdiﬀerentiable.

2 System

A Neurocoder is a neural network (Main Network) coupled to an external Program Memory through a
Program Controller. The working program of the Main Network processes the input data to produce the
output. This working program is “coded” by the Program Controller by creating an input-dependent
active program from the Program Memory (Fig. 2).

Neurocoder stores Singular Value Decomposition of Neural Programs in Program Mem-
ory

The Neural Program needs to be stored eﬃciently in Program Memory. This is challenging as there may
be millions of inter-neuronal connection weights, thus storing them directly ([23]) is grossly ineﬃcient.
Instead, the Neurocoder forms the basis of a subspace spanned by Neural Programs and stores the
singular values and vectors of this subspace in memory slots of the Program Memory (hereafter referred
to as singular programs). Based on the input, relevant singular programs are retrieved, a new program
is reconstructed and then loaded in the Main Network to process the input. This representational choice
signiﬁcantly reduces the number of stored elements and allows each singular program to eﬀectively
represent a unitary function of the active program.

The active program matrix P can be composed by standard low-rank approximation as

P = USVT

(1)

where U and V are matrices of the left and right singular vectors, and S the matrix of singular values.
The Program Memory is crafted as three singular program memories {MU , MV , MS} to store and
retrieve these components. The process “codes” the active program using singular programs from
{MU , MV , MS}.

The Program Memory also maintains the status for each singular program in terms of access and
usage. To access a singular program, program keys (k) are used. These keys are low-dimensional
vectors that represent the singular program function and computed by a neural network that eﬀectively
compresses the content of memory slots. The program usage (m) measures memory utilisation, recording
how much a memory slot is used in constructing a program. The components of the Program Memory
are summarised in Fig. 2 (c).

Recurrent multi-head program attention mechanisms for program storage and retrieval
Neural networks use the concept of diﬀerentiable attention to access memory [10, 1, 24]. This deﬁnes a
weighting distribution over the memory slots essentially weighting the degree to which each memory
slot participates in a read or write operation. This is unlike conventional computers that use a unique
address to access a single memory slot.

Here we use two kinds of attention. First is content-based attention [10, 11] to ensure that the
singular program is selected based on its functionality and the data input. This is achieved by producing
a query vector based on the input and comparing it to the program keys (k) using cosine similarity.
Higher cosine similarity scores indicate higher attention weights to the singular programs associated
with those program keys. Second, to encourage better memory utilisation, higher attention weights are
assigned to slots with lower program usage (m) through usage-based attention [11, 29]. The attention

3

Figure 2: Neurocoder (a) The Main Network uses a working program to compute the output for the
input. Here only the ﬁnal layer of the Main Network is adaptively loaded with the working program
(1 ). Other layers use traditional Neural Programs as connection weights (ﬁxed-after-training). (b) The
Program Controller’s composition network controls access to the Program Memory, emitting queries and
interpolating gate control signals in response to the input (2 ). It then performs recurrent multi-head
program attention to the Program Status (3), triggering attention weights to the Singular Programs
(4). The attended Singular Programs form an active program using low-rank approximation (5). This
active program is then used to derive the working program from a residual program produced by the
Program Controller’s integration network ( 6). (c) The Program Memory stores the representations
(singular programs) required to reconstruct the active program to be used by the Program Controller.
Access is controlled through the Program Status including keys (k), and slot usage (m) that are updated
during the training and computation (7).

4

AttentionWeightStatusUpdateOutputWorking	ProgramSingularProgramsProgram	AttentionActive	ProgramInputIntegrationNetworkReccurentAccess(a)	MAIN	NETWORK(b)	PROGRAM	CONTROLLER(c)	PROGRAM	MEMORYLow-rank	approximationProgram	IntegrationProgramStatus(1)(2)(3)(4)(5)(6)(7)CompostionNetworkResidualProgramFinal	Layerweights from the two schemas are then combined using interpolating gates to compose the ﬁnal attention
weights to the Program Memory.

We adapt multi-head attention [10, 34] that applies multiple attentions in parallel to retrieve H
singular components. Besides, we introduce a recurrent attention mechanism, in which multi-head
access is performed recurrently in J steps. The j-th set of H retrieved components is conditioned on
the previous ones. This recurrent, multi-head attention allows the composition network to attend to
multiple memory slots recurrently, incrementally searching for optimal components for building relevant
active programs.

Neurocoder learns to “code” a relevant working program via training

The structure of the Program Memory and the role of the Program Controller facilitates the automatic
construction of working programs via training. The Program Controller controls memory access
through its composition network that creates the attention weight deﬁning how to weight the singular
programs. Applying the recurrent multi-head attention described earlier, multiple singular programs
are retrieved to construct an active program (Eq. 1). Then the Program Controller generates a residual
program using its integration network to transform the active program into the working program of the
Main Network. This transformation enables creation of ﬂexible higher-rank working programs, which
compensates for the low-rank coding process. The structure of the Program Controller is illustrated in
Fig. 2 (b).

The singular programs are trained to represent unitary functions necessary for any computation
whilst the composition and integration networks are trained to compose the relevant programs from
the singular programs to compute the current input. The parameters of the networks, and the stored
singular programs are adjusted during end-to-end training. Initially, all the parameters will be random,
leading to creation of representations and working programs that produce huge training loss. However,
as training proceeds, these parameters get adjusted and the network gets trained. Training is traditional
and minimises the total loss using gradient descent as

where Ltask represents the supervised training loss and Lo represents a term weighted by a hyper-
parameter a to enforce orthogonality of the singular vectors.

L = Ltask + aLo

(2)

Neurocoder can be integrated with any neural network

The Main Network of the Neurocoder can be any current neural network. One or more layers of the
neural network can be replaced in this way and re-coded with working programs from the Neurocoder
in a data-responsive way (Fig. 2 (a)). Here we show how Neurocoders re-code programs for single
or all layers of diverse networks like Multi-layer Perceptron, Convolutional Neural Networks and
Reinforcement Learning architectures. It is signiﬁcant that this can be done by plugging Neurocoder
into current architectures without modiﬁcation of training paradigms or major addition to the number
of parameters. Thus any state-of-the-art neural architecture plus Neurocoder can re-code itself from a
suite of programs.

5

3 Methods

Program Coding as Low-rank Approximation

The Program Memory stores singular programs in three singular program memories {MU , MV , MS}.
At some time t, we compose the active program Pt by low-rank approximation as follows,

Pt = USVT
rm(cid:88)

=

σtnutnv(cid:62)
tn

n

(3)

(4)

where rm is the total number of components we want to retrieve, {σtn}rm
and {vtn}rm
n=1 the singular vectors of S, U, and V, respectively.

n=1 the singular values, {utn}rm

n=1

By limiting rm, we put a constrain on the rank of the active program. To enforce orthogonality of

the singular vectors, we minimise the orthogonal loss

Lo = MU M(cid:62)

U − I + MV M(cid:62)

V − I

(5)

Since the active program is dynamically composed at time t for the computation of input xt, it
resembles fast-weights in neural networks [35]. Unlike fast-weights, the working program consists of
singular programs stored in Program Memory representing the stored-program principle [33, 36]. It
diﬀers from an earlier attempt to implement a memory for programs, the Neural Stored-program
Memory [23] which stores each item as a working program itself. It extends the concept of slot-based
neural memory [10, 11, 21, 22] to storing neural programs as data.

We now use attention weights (wu

tin, wv

tin, wσ

tin jointly denoted as wu,v,σ

tin ) to each slot of the singular

program memories MU , MV and MS to read each singular vector as

utn =

vtn =

Pu(cid:88)

i=1

Pv(cid:88)

i=1

wu

tinMU (i)

wv

tinMV (i)

For the singular values, we enforce σt1 > σt2 > ... > σtrm > 0 by using




softplus

(cid:16)(cid:80)Ps

i=1 wσ

σtn =

(cid:17)
tinMS (i)
(cid:16)(cid:80)Ps
i=1 wσ



σtn+1 + softplus

tinMS (i)

(cid:17)

n = rm

n < rm

(6)

(7)

(8)

Here, Pu, Pv and Ps are the number of memory slots of MU , MV and MS, respectively. For simplicity,
in this paper, we set P = Pu = Pv = Ps as the number of memory slots of the Program Memory.
The attention weights wu,v,σ
tijh , are determined by program memory attention
mechanisms, which will be discussed in the upcoming sections.

tin , shorten form for wu,v,σ

Recurrent Access to the Program Memory via the composition network

To perform program attention, the Program Controller employs a composition network (denoted as
fθu,v,σ ), which takes the current input xt and produce program composition control signals (ξp
t ). If

6

Figure 3: Active program coding. The Program Controller uses the composition network (a recurrent
neural network) to process the input xt and generate composition signal ξp
t , which is composed of the
queries (q) and the interpolating gates (g). The similarity of the query to program memory keys (k) is
then computed together with the memory usage (m) from which attention weights for the Program
Memory are derived. The active program Pt is then “coded” through low-rank approximation using the
j-th component accessed by recurrent attentions. For simplicity, one attention head is shown (H = 1).

7

CompositionNetworkOuterproductProgramAttentionMultiplySumProgramMemory	ProgramControllerfθu,v,σ performs all attentions concurrently via multi-head attention (as in [10, 34]), it may lead to
program collapse [23]. To have a better control of the component formation and alleviate program
collapse, we propose to recurrently attend to the program memory. To this end, we implement fθu,v,σ
as a recurrent neural network (LSTM [15]) and let it access the program memory J times, resulting in
t = (cid:8)ξp
ξp
. At access step j, the recurrent network updates its hidden states and generates ξp
tj
using recurrent dynamics as

(cid:9)J

j=1

tj

tj, hu,v,σ
ξp

= fθu,v,σ

(cid:0)xt, hu,v,σ

(cid:1)

0

j
is initialized as zeros and ξp
where hu,v,σ
tj is the program composition control signal at step j that
depends on both on the input data xt and the the previous state hu,v,σ
j−1 . Particularly, the control
signal contains the queries and the interpolation gates used to compute the program attention (wu,v,σ
tin ):
ξp
. Here, at each attention step, we perform multi-head attention with H as the
tj =
number of attention heads or retrieved components and thus, each ξp
tj consists of H pairs of queries
and gates. Hence, the total number of retrieved components rm = J × H.

tjh , gu,v,σ
qu,v,σ

(cid:111)H

h=1

tijh

j−1

(cid:110)

(9)

Attending to Programs by “Name”

Inspired by the content-based attention mechanism for data memory [10], we use the query to look
In computer programming, to ﬁnd the appropriate program for some
for the singular programs.
computation, we often refer to the program description or at least the name of the program. Here, we
create the “name” for our neural programs by compressing the program content to a low-dimensional
key vector. As such, we employ a neural network (fϕ) to compute the program memory keys as

ku,v,σ
i
i ∈ RK(cid:9)Pv

= fϕu,v,σ (MU,V,S (i))
(cid:111)
i=1 , (cid:8)kσ

i ∈ RK(cid:9)Pσ

(cid:110)(cid:8)ku

i ∈ RK(cid:9)Pu

i=1 , (cid:8)kv

where ku,v,σ =
. Here, fϕu,v,σ learns to compress each
memory slot of the singular program memories into a K-dimensional vector. As the singular programs
evolve, their keys get updated. In this paper, we calculate the program keys after each learning iteration
during training.

i=1

(10)

Finally the content-based program memory attention cu,v,σ

(cid:110)

(cid:111)

tijh =
and the queries qu,v,σ

tijh, cv
tijh, cσ
cu
as

tijh

tjh

(cid:32) qu,v,σ
tjh
||qu,v,σ

· ku,v,σ
i
tjh || · ||ku,v,σ

i

(cid:33)

||

is computed using

(11)

cosine distance between the program keys ku,v,σ

i

cu,v,σ
tijh = softmax(i)

Making Every Program Count

Similarly to [11, 29], in addition to the content-based attention, we employ a least-used reading strategy
to encourage the Program Controller to assign diﬀerent singular programs to diﬀerent components. In
particular, we calculate the memory usage for each program slot across attentions as

mu,v,σ

tijh = max
˜j≤j

(cid:17)

(cid:16)

wu,v,σ
ti˜jh

(12)

8

where mu,v,σ = (cid:8)mu ∈ RPu×1, mv ∈ RPv×1, mσ ∈ RPσ×1(cid:9). Since we want to consider only lI amongst
P memory slots that have smallest usages, let ˆmu,v,σ(lI )
denote the value of the lI -th smallest usage,
then the least-used attention is computed as

tjh

lu,v,σ
tijh =




max
i



0

(cid:16)

mu,v,σ
tijh

(cid:17)

− mu,v,σ
tijh

; mu,v,σ
; mu,v,σ

tijh ≤ ˆmu,v,σ(lI )
tijh > ˆmu,v,σ(lI )

tjh

tjh

The ﬁnal program memory attention is computed as

wu,v,σ

tijh = sigmoid

(cid:16)

gu,v,σ
tijh

(cid:17)

cu,v,σ
tijh +

(cid:16)

1 − sigmoid

(cid:16)

gu,v,σ
tijh

(cid:17)(cid:17)

lu,v,σ
tijh

(13)

(14)

Since the usage record are computed along the memory accesses, the multi-step Neurocoder utilises this
attention mechanism better than the single-step Neurocoder, leading to diﬀerent attention behaviors
(see Sec. 4). The whole process of composing the active program Pt is illustrated in Fig. 3.

Program Integration via the integration network

Since the working program Pt only contains top rm principal components, it may be not ﬂexible enough
for sophisticated computation. We propose to enhance Pt with a residual program R– a traditional
connection weight trained as the integration network’s parameters. The residual program represents
the sum of the remaining less important components. To this end, we suppress R with a multiplier that
is smaller than σtrm – the smallest singular value of the main components - resulting in the integration
formula

Wt = Pt + wr

t σtrmR

(15)

where wr
t = sigmoid (fφ (xt)) is an adaptive gating value that controls the contribution of the residual
program. fφ is the integration network in the Program Controller and hence, in our implementation,
the integration control signal sent by the Program Controller is λp
t , σtrm} . We note that in our
experiments, the program integration is sometimes disabled (Wt is directly set to Pt) to eliminate the
eﬀect of R or reduce the number of parameters.

t = {wr

The working program Wt is then used by the Main Network to execute the input data xt. For
example, with linear classiﬁer Main Network, the execution is yt = xtWt. Table 1 summarises the
notations used for important parameters of Neurocoder.

The Main Network can be any neural network in which one or more layers of this network can be
replaced by the Neurocoder. In our experiments, we always apply Neurocoder to all layers of multi-layer
perceptrons (MLP) or just the ﬁnal output layer of CNNs (LeNet, DenseNet, ResNet), RNNs (GRU),
and the policy/value networks of A3C. Other competitors such as MOE, NSM and HyperNet are
applied to the Main Networks in the same manner.

4 Results

To demonstrate the ﬂexibility of this framework, we consider diﬀerent learning paradigms: instance-
based, sequential and continual learning. We do not focus on breaking performance records by
augmenting state-of-the-art models with Neurocoder. Rather our inquiry is on re-coding layers of
diverse neural networks with the Neurocoder’s programs and testing on varied data types to demonstrate
its intrinsic properties (details of the following section are in Appendix).

9

Figure 4: (a) MNIST test set classiﬁcation error vs the number of steps (J) in Neurocoder (blue),
compared with a linear classiﬁer (red). (b) 1st column: Digit images; Middle column: Single-step
attention weights for 30 slots in MU (vertical axis) for ﬁrst 3 singular vectors (horizontal axis) for each
digit; Last column: Multi-step attention weights for 10 slots in MU (vertical axis) for ﬁrst 3 singular
vectors (horizontal axis). Multi-step attention is able to produce far more diverse patterns with fewer
slots - 10 slots compared to single-step 30 slots. (c) Two attention patterns of single-step Neurocoder.
(d) The binary decision tree derived from single-step Neurocoder’s attention patterns. The two patterns
across components represent the decisions going up and down across the binary tree.

10

4.1

Instance-based learning - Object Recognition

We tested Neurocoder on instance-based learning through classical image classiﬁcation tasks using
MNIST [25] and CIFAR [20] datasets. The ﬁrst experiment interpreted Neurocoder’s behaviour in
classifying digits into 10 classes (0 − 9) using linear classiﬁer Main Network. With equivalent model
size, Neurocoder using the novel recurrent attention surpasses the performance of the linear classiﬁer
[25] by up to 5% (Fig. 4 (a)).

To diﬀerentiate the input, Neurocoder attends to diﬀerent components of the active program to
guide the decision-making process. Fig. 4 (b) shows single-step and multi-step attention to the ﬁrst
3 singular vectors for each digit across memory slots. Multi-step attention produces richer patterns
compared to single-step Neurocoder that manages only 2 attention weight patterns (Fig. 4 (c)).

Fig. 4 (d) illustrates how Neurocoder performs modular learning by showing the attention assignment
for top 3 singular vectors as a binary decision tree. Digits under the same parental node share similar
attention paths, and thereby similar active programs. Some digits look unique (e.g. 7) resulting in
active programs composed of unique attention paths, discriminating themselves early in the decision
tree. Some digits (e.g. 0 and 9) share the same attention pattern for the ﬁrst 3 components and are
thus unclassiﬁable in the binary tree. They can only be distinguished by considering more singular
vectors.

We integrated Neurocoder with deep networks - 5-layer LeNet and 100-layer DenseNet - and tested
on complex CIFAR datasets. Neurocoder signiﬁcantly outperformed the original Main Networks with
performance gain around 2 − 5%. Compared with recent conditional computing models such as Mixture
of Experts (MOE [32]) and Neural Stored-program Memory (NSM [23]), Neurocoder required a tenth
of the number of parameters and performed better by up to 7% (see Table 2).

4.2 Sequential learning - Adaption to sequence changes and game playing

using reinforcement learning

Recurrent neural networks (RNN) can learn from sequential data by updating the hidden states of
the networks. However, this does not suﬃce when local patterns shift, as is often the case. We now
demonstrate that Neurocoder helps RNNs overcome this limitation by composing diverse programs to
handle sequence changes.

Synthetic polynomial auto-regression We created a simple auto-regression task in which data
points are sampled from polynomial function chunks that change over time. The Main Network is a
strong RNN–Gated Recurrent Unit (GRU [4]). We found that GRU integrated with a single-step or
multi-step Neurocoder learned much faster than its original version and HyperNet counterparts [12] (
Fig. 6).

Visualising the ﬁrst singular vector attention weights in MU , we ﬁnd that the multi-step attention
Neurocoder changes its attention following polynomial changes - it attends to the same singular
program when processing data from the same polynomial and alters attention for data from a diﬀerent
polynomial (Fig. 5(a)). In contrast, the single-step Neurocoder only changes its attention when there
is a remarkable change in y-coordinate values (Fig. 5(b)). We hypothesise that when recurrence
is employed, usage-based attention takes eﬀect, stipulating better memory utilisation and diverse
attentions over timesteps. Although single-step Neurocoder converges well, it did not discover the
underlying structure of the data, and thus underperformed the multi-step Neurocoder.

11

Figure 5: Visualisation for (a) multi-step (J = 5, 20 memory slots) and (b) single-step (J = 1, 10
memory slots) cases showing while processing a sequence of the polynomial auto-regression task. The
Neurocoder’s attentions to MU that form the ﬁrst component of the active program are shown over
sequence timesteps (upper ) with Neurocoder’s yt prediction (orange) and ground truth (blue) (lower ).
The vertical dash green lines separate polynomial chunks. Each chuck represents a local pattern, and
thus ideally requires a speciﬁc active program to compute the input xt. Although both predict well,
only the multi-step Neurocoder discovers the chunk boundaries, assigning program attention to the
ﬁrst component in accordance with sequence changes.

Atari game reinforcement learning We used reinforcement learning as a further testbed to show
the ability to adapt to environmental changes. We performed experiments on several Atari 2600 games
[2] wherein the agent (or Main Network) was implemented as the Asynchronous Advantage Actor-Critic
(A3C [26]). In the Atari platform, agents are allowed to observe the screen snapshot of the games and
act to earn the highest score. We augmented the A3C by employing Neurocoder’s working programs
for the actor and critic networks, aiming to decompose the policy and value function into singular
programs that were selected depending on the game state.

Frostbite and Montezuma’s Revenge. These games are known to be challenging for A3C and other
algorithms [26]. We trained A3C and HyperNet-based A3C for over 300 million steps, yet these models
did not show any sign of learning, performing equivalently to random agents. For such complicated
environments with sparse rewards, both the monolithic neural networks and the unstored fast-weights
fail to learn (almost zero scores). In contrast, Neurocoder enabled A3C to achieve from 1, 500 to 3, 000
scores on these environments (Fig. 7), conﬁrming the importance of decomposing a complex solution
to smaller, simple stored programs.

4.3 Continual learning

In continual learning, standard neural networks often suﬀer from “catastrophic forgetting” in which
they cannot retain knowledge acquired from old tasks upon learning new ones [9]. Our Neurocoder
oﬀers natural mitigation of such catastrophic forgetting in neural networks by storing task-dependent
singular programs in separated program memory slots.

12

Split MNIST We ﬁrst considered the split MNIST dataset–a standard continual learning benchmark
wherein the original MNIST was split into a 5 2-way classiﬁcation tasks, consecutively presented to
a Multi-layer Perceptron Main Network (MLP). We followed the benchmarking as in [16] in which
various optimisers and state-of-the-art continual learning methods were examined under incremental
task and domain scenarios. We measured the performance of the MLP versus Neurocoder and NSM
under each continual learning method.

In both scenarios, Neurocoder was compatible with all continual leaning methods, demonstrating
superior performance over MLP and NSM with performance gain between 1 to 16% (see Table 3 and 4).

Split CIFAR We veriﬁed the scalability of Neurocoder to more challenging datasets. We split
CIFAR datasets as in the split MNIST, wherein 5-task 2-way split CIFAR10 and a 20-task 5-way split
CIFAR100 were created. We integrate Neurocoder with ResNet [14]–a very deep CNN architecture as
the Main Network.

When we stressed the orthogonal loss (a = 10) and used bigger program memory (100 slots),
Neurocoder improved ResNet classiﬁcation by 15% and 10% on CIFAR10 and CIFAR100, respectively.
When we integrated Neurocoder with Synaptic Intelligence (SI [37]), the performance was further
improved, maintaining a stable performance above 80% accuracy for CIFAR10 and outperforming using
SI alone by 10% for CIFAR100 (see Fig. 8).

5 Discussion

Our experiments demonstrate that Neurocoder is capable of re-coding Neural Programs in distinctive
neural networks, amplifying their capabilities in diverse learning scenarios: instance-based, sequential
and continual learning. This consistently results in signiﬁcant performance increase, and further creates
novel robustness to pattern shift and catastrophic forgetting. This unprecedented ability for each
architecture to re-code itself is made possible without changing the way it is trained, or majorly
increasing the number of parameters it needs to learn.

The MNIST problem illustrates the reasoning process of Neurocoder when classifying digit images
wherein its singular program assignment resembles a binary tree decision-making process - it shows how
some singular programs are shared, others are not. The polynomial auto-regression problem highlights
the importance of eﬃcient memory utilisation in re-constructing the working program enabling discovery
of hidden structures in sequential data. Training our framework with reinforcement learning, we enable
neural agents to solve complex games wherein traditional methods fail or learn slowly. Finally, continual
learning problems show that Neurocoder mitigates catastrophic forgetting eﬃciently under diﬀerent
learning settings/algorithms.

Our solution oﬀers a single framework that is scalable and adaptable to various problems and
learning paradigms. Unlike previous attempts to employ a bank of separate big programs [19, 32, 23],
Neurocoder maintains only shareable, smaller components that can reconstruct the whole program
space, thereby heavily utilising the parameters and preventing the model from proliferating. We can
further extend Neurocoder’s ability by allowing a growing Program Memory, in which the model decides
to add or erase memory slots as the number of data patterns grows or shrinks beyond the current
program space’s capacity. Such a system represents a more ﬂexible general-purpose computer that can
dynamically allocate computing resources by itself without human pre-speciﬁcation.

13

References

[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.

[2] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,
47:253–279, 2013.

[3] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

[4] Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1724–1734. Association for Computational Linguistics, October 2014.

[5] JC Eccles. The modular operation of the cerebral neocortex considered as the material basis of

mental events. Neuroscience, 6(10):1839–1855, 1981.

[6] Gerald M Edelman. Neural darwinism: selection and reentrant signaling in higher brain function.

Neuron, 10(2):115–125, 1993.

[7] Gerald M Edelman and Vernon B Mountcastle. The mindful brain: cortical organization and the

group-selective theory of higher brain function. Massachusetts Inst of Technology Pr, 1978.

[8] Richard SJ Frackowiak. Human brain function. Elsevier, 2004.

[9] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences,

3(4):128–135, 1999.

[10] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines.

arXiv preprint

arXiv:1410.5401, 2014.

[11] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471–
476, 2016.

[12] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on

Learning Representations, 2017.

[13] Bart LM Happel and Jacob MJ Murre. Design and evolution of modular neural network architec-

tures. Neural networks, 7(6-7):985–1004, 1994.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[15] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1780, 1997.

14

[16] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual
learning scenarios: A categorization and case for strong baselines. In NeurIPS Continual learning
Workshop, 2018.

[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4700–4708, 2017.

[18] D.H. Hubel. Eye, Brain, and Vision. Scientiﬁc American Library series. Scientiﬁc American

Library, 1988.

[19] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoﬀrey E Hinton. Adaptive mixtures

of local experts. Neural computation, 3(1):79–87, 1991.

[20] Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images.

2009.

[21] Hung Le, Truyen Tran, Thin Nguyen, and Svetha Venkatesh. Variational memory encoder-decoder.

In Advances in Neural Information Processing Systems, pages 1508–1518, 2018.

[22] Hung Le, Truyen Tran, and Svetha Venkatesh. Learning to remember more with less memorization.

In International Conference on Learning Representations, 2018.

[23] Hung Le, Truyen Tran, and Svetha Venkatesh. Neural stored-program memory. In International

Conference on Learning Representations, 2020.

[24] Hung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In Proceedings

of Machine Learning and Systems 2020, pages 2363–2372. 2020.

[25] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning applied

to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[26] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pages 1928–1937, 2016.

[27] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selec-
tion of non-linear functions for multi-task learning. In International Conference on Learning
Representations, 2018.

[28] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by

back-propagating errors. Nature, 323(6088):533–536, 1986.

[29] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.
Meta-learning with memory-augmented neural networks. In International conference on machine
learning, pages 1842–1850, 2016.

[30] Jiirgen Schmidhuber. Making the world diﬀerentiable: On using self-supervised fully recurrent n
eu al networks for dynamic reinforcement learning and planning in non-stationary environm nts.
1990.

15

[31] J¨urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic

recurrent networks. Neural Computation, 4(1):131–139, 1992.

[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoﬀrey E. Hinton,
and Jeﬀ Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
In International Conference on Learning Representations, 2017.

[33] A.M Turing. On computable numbers, with an application to the entscheidungsproblem. In

Proceedings of the London Mathematical Society, 1936.

[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
(cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[35] Christoph von der Malsburg. The correlation theory of brain function, 1981.

[36] John Von Neumann. First draft of a report on the edvac. IEEE Annals of the History of Computing,

15(4):27–75, 1993.

[37] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.

Proceedings of machine learning research, 70:3987, 2017.

16

Appendix

Instance-based learning experiments

Image classiﬁcation-linear Main Network We used the standard training and testing set of
MNIST dataset. To train the models, we used the standard SGD with a batch size of 32. Each
MNIST image was ﬂattened to a 768-dimensional vector, which requires a linear classiﬁer of 7, 680
parameters to categorise the inputs into 10 classes. For Neurocoder, we used Program Memory with
P = 6 and K = 2. The Program Controller’s composition network was an LSTM with a hidden
size of 8. We controlled the number of parameters of Neurocoder, which included parameters for
the Program Memory and the Program Controller by reducing the input dimension using random
projection zt = xtU with U ∈ R768×200 initialised randomly and ﬁxed during the training. We also
excluded the program integration to eliminate the eﬀect of the residual program R. Given the ﬂattened
image input xt, Neurocoder generated the active program Pt, predicting the class of the input as
yt = argmax (xtPt). The performance of the linear classiﬁer was imported from [25] and conﬁrmed by
our own implementation.

Image classiﬁcation-deep Main Network We used the standard training and testing sets of
CIFAR datasets. We use Adam optimiser with a batch size of 128. The deep Main Networks were
adopted from the original papers, resulting in 3-layer MLP, 5-layer LeNet [25] and 100-layer DenseNet
[17]. The other baselines for this task included a recent Mixture of Experts (MOE [32]) and the
Neural Stored-program Memory (NSM [23]). For this case, we employed the program integration with
the residual program R. The Main Network’s hyper-parameters were ﬁxed and we only tuned the
hyper-parameters of Neurocoder, MOE and NSM. We report details of hyper-parameters in Table 5
and 6.

Sequential learning experiments

Synthetic polynomial auto-regression A sequence was divided into npa chunks, each of which
associated with a randomly generated polynomial. The degree and coeﬃcients of each polynomial were
sampled from U ∼ [2, 10] and U ∼ [−1, 1], respectively. Each sequence started from x1 = −5 and ended
with xT = 5, equally divided into npa chunks. Each chunk contained several consecutive points (xt, yt)
from the corresponding polynomial, representing a local transformation from the input to the output.
Given previous points (x<t, y<t) and the current x-coordinate xt, the task was to predict the current
y-coordinate yt. To be speciﬁc, at each timestep, the Main Network GRU was fed with (xt, yt−1) and
trained to predict yt by minimizing the mean square error 1/T (cid:80)T
t=1 ( ˆyt − yt)2 where y0 = 0, ˆyt is the
prediction of the network and yt the ground truth.

We augmented GRU by applying Neurocoder and HyperNet [12] to the output layer of the GRU.
Here, the HyperNet baseline generated adaptive scales for the output weight of the GRU. We trained
the networks with Adam optimiser with a batch size of 128. To balance the model size, we used GRU’s
hidden size of 32, 28, 16 and 8 for the original Main Network, HyperNet, single-step and multi-step
Neurocoder, respectively. We also excluded program integration phase in Neurocoders to keep the model
size equivalent to or smaller than that of the Main Network. We report details of hyper-parameters for
GRU, HyperNet and Neurocoder in Table 5 and 6.

We compared two conﬁgurations of Neurocoder - single-step, multi-head (J = 1, H = 15) and
multi-step, single-head (J = 5, H = 1)- against the original GRU with output layer made by MLP and
HyperNet–a weight-adaptive neural network [12]. We found that MLP failed to learn and converge

17

within 10, 000 learning iterations. In contrast, both Neurocoders learn and converge, in as little as only
2, 000 iterations with the multi-step Neurocoder. HyperNet converged much slower than Neurocoders
and could not minimize the predictive error as well as Neurocoders when Gaussian noise is added or
the number of polynomials (npa) is doubled (see Fig. 6).

Atari 2600 games We used OpenAI’s Gym environments to simulate Atari games. We used the
standard environment settings, employing no-frame-skip versions of the games. The picture of the
game snapshot was preprocessed by CNNs and the A3C agent was adopted from the original paper
with default hyper-parameters as in [26]. The actor/critic network of A3C was LSTM whose output
layer’s working program was provided by Neurocoder or HyperNet. The hidden size of the LSTM was
512 for all baselines. We list details of models and hyper-parameters in Table 5 and 6.

Seaquest and MsPacman. The original A3C agent was able to learn and obtain a moderate score
of around 2, 500 after 32 million environment steps. We also equipped A3C with HyperNet-based
actors/critics, however, the performance remained unchanged, with scores of about 2/3 of Neurocoder-
based agent’s.

Continual learning experiments

Split MNIST We used the same 2-layer MLP and continual learning baselines as in [16]. Here, we
again excluded program integration to avoid catastrophic forgetting happening on the residual program
R. We only tuned the hyper-parameters of NSM and Neurocoder for this task. Remarkably, the NSM
with much more parameters could not improve MLP’s performance, illustrating that simple conditional
computation is not enough for continual learning (see Table 3).

Split CIFAR The 18-layer ResNet implementation was adopted from Pytorch’s oﬃcial release whose
weights was pretrained with ImageNet dataset. When performing continual learning with CIFAR
images, we froze all except for the output layers of ResNet, which was a 3-layer MLP. We only
tuned the hyper-parameters of SI and Neurocoder for this task. Details of model architectures and
hyper-parameters are reported in Table 5 and 6.

In the CIFAR10 task, compared to the monolithic ResNet, the Neurocoder-augmented ResNet
could achieve much higher accuracy when we ﬁnished the learning for all 5 tasks (55% versus 70%,
respectively). Also, we realised that stressing the orthogonal loss further improved the performance.
When we employed Synaptic Intelligence (SI [37]), the performance of ResNet improved, yet it still
dropped gradually to just above 70%. In contrast, the Neurocoder-augmented ResNet with SI maintained
a stable performance above 80% accuracy (see Fig. 8 (left)).

In the CIFAR100 task, Neurocoder alone with a bigger program memory slightly exceeded the per-
formance of SI, which was about 10% better than ResNet. Moreover, Neurocoder plus SI outperformed
using only SI by another 10% of accuracy as the number of seen tasks grew to 20 (see Fig. 8 (right)).

18

Notation

Meaning

θu,v,σ
φ
ϕu,v,σ
R
MU
MV
MS

ξp
t
λp
t
ku,v,σ
mu,v,σ

P
K
lI
J
H
a

Trainable parameters

Composition network
Integration network
Key generator network
Residual program (optional)
Memory of left singular vectors
Memory of right singular vectors
Memory of singular values

Control variables

Composition control signal
Integration control signal
Program keys
Program usages

Hyper-parameters

Number of memory slots
Key dimension
Number of considered least-used slots
Number of recurrent attention steps
Number of attention heads
Orthogonal loss weight

Location
Program Program
Controller Memory

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

Table 1: Important parameters of Neurocoder.

Architecture

MLP

LeNet

DenseNet

Task
CIFAR10
CIFAR100
CIFAR10
CIFAR100
CIFAR10
CIFAR100

Original MOE NSM Neurocoder

52.06
23.31
75.71
42.73
93.61
68.37

50.76
22.79
75.88
42.47
80.61
38.20

52.76
25.65
75.45
43.14
94.24
64.53

54.86
26.24
78.92
47.21
95.61
72.20

Table 2: Best test accuracy over 5 runs on image classiﬁcation tasks comparing original architecture,
Mixture of Experts (MOE), Neural Stored-program Memory (NSM) and our architecture (Neurocoder).
Three architectures of the Main Network of Neurocoder were considered: 3-layer perceptron (MLP),
5-layer CNN (LeNet [25]) and very deep Densely Connected Convolutional Networks (DenseNet [17]).
We employed two classical image classiﬁcation datasets: CIFAR10 and CIFAR100.

19

Figure 6: Polynomial auto-regression: mean square error (MSE) over training iterations with a batch
size of 128 comparing HyperNet (red), MLP (green), multi-step (blue) and single-step (yellow) attention
Neurocoders. All baselines use GRU as the Main Network. The learning curves are taken average over
5 runs.

Figure 7: Learning curves (mean and std. over 5 runs) on representative Atari 2600 games. All baselines
are applied to the actor/critic networks in the A3C agent.

Method MLP [16] MLP (ours)
93.75±3.28
Adam
98.02±0.89
Adagrad
98.14±0.43
L2
98.69±0.20
SI
97.00±1.10
EWC
98.23±1.17

93.46±2.01
98.06±0.53
98.18±0.96
98.56±0.49
97.70±0.81
O-EWC 98.04±1.10

NSM
87.55± 4.38
96.63±1.49
91.44± 3.80
98.87±0.20
93.94±2.36
96.11±1.27

Neurocoder
96.54±1.39
99.01±0.19
98.35±0.74
99.14±0.24
97.88±0.22
98.30±1.48

Table 3: Incremental task continual learning with Split MNIST. Final test accuracy (mean and std.)
over 10 runs.

20

Method MLP [16] MLP (ours)
53.55±1.27
Adam
57.83±2.74
Adagrad
64.37±2.40
L2
64.41±3.36
SI
58.41±2.37
EWC
57.78±1.84

55.16±1.38
58.08±1.06
66.00±3.73
64.76±3.09
58.85±2.59
O-EWC 57.33±1.44

NSM
54.85±2.81
58.42±1.87
62.83±7.21
64.36±2.99
58.12±3.24
58.55±3.40

Neurocoder
58.46±0.46
62.28±4.03
69.89±1.72
67.96±3.22
65.66±1.25
73.97±1.50

Table 4: Incremental domain continual learning with Split MNIST. Final test accuracy (mean and std.)
over 10 runs.

Figure 8: Incremental task continual learning with Split CIFAR10 (left) and CIFAR100 (right). Average
classiﬁcation accuracy with error bar over all learned tasks as a function of number of tasks.

Task

MNIST

CIFARs

Neurocoder
P = 5, J = 5, H = 1
K = 2, lI = 2, a = 0.1
P = 30, J = 5, H = 3
K = 5, lI = 5, a = 0.1
P = 10, J = 1, H = 15 P = 20, J = 5, H = 1
auto-regression K = 3, lI = 2, a = 0.1 K = 3, lI = 2, a = 0.1
P = 80, J = 1, H = 15, K = 3, lI = 5, a = 0.1
P = 50, J = 1, H = 10, K = 5, lI = 5, a = 10
P = 100, J = 1, H = 10, K = 5, lI = 5, a = 10

Atari games
Split MNIST
Split CIFARs

Polynomial

Table 5: Best hyper-parameters of Neurocoder in all experiments. – denotes not available. For
polynomial auto-regression task, two Neurocoder conﬁgurations are tested, corresponding to single-step
and multi-step Neurocoder. Across experiments, MOE and NSM employ 10 experts and 10 memory
banks, respectively. HyperNet does not have any special hyper-parameters.

21

Task

MNIST

CIFARs

Polynomial
auto-regression
Atari games
Split MNIST
Split CIFARs

Main Network

Original MOE

NSM HyperNet

Neurocoder

Linear classiﬁer
3-layer MLP
LeNet
DenseNet

GRU

LSTM
2-layer MLP
ResNet

7.8K
1.7M
2.1M
7.0M

3.4K

3.2M
328K
12.6M

–

–

15.4M 21.2M
12.3M 27.1M
20.5M 16.7M

–
–
–
–

7.3K
1.9M
2.3M
7.3M

–

–

–

–

3.5K

3.6K

2.1K

–
2.3M
–

3.6M

–

3.3M
348K
12.6M

Table 6: Number of parameters of machine learning models in all experiments. The parameter count
includes the parameter of the Main Network and the conditional computing model. – denotes not
available. For tasks that contain diﬀerent datasets, leading to slightly diﬀerent model size, the numbers
of parameters are averaged. For polynomial auto-regression task, two Neurocoder conﬁgurations are
tested, corresponding to single-step and multi-step Neurocoder.

22

