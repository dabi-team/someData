1
2
0
2
c
e
D
4
2

]

B
D
.
s
c
[

1
v
9
9
0
3
1
.
2
1
1
2
:
v
i
X
r
a

Fine-Tuning Data Structures for Analytical Query Processing

Amir Shaikhha
University of Edinburgh
United Kingdom
amir.shaikhha@ed.ac.uk

Marios Kelepeshis
University of Oxford
United Kingdom
marios.kelepeshis@stx.ox.ac.uk

Mahdi Ghorbani
University of Edinburgh
United Kingdom
mahdi.ghorbani@ed.ac.uk

ABSTRACT
We introduce a framework for automatically choosing data struc-
tures to support efficient computation of analytical workloads. Our
contributions are twofold. First, we introduce a novel low-level in-
termediate language that can express the algorithms behind various
query processing paradigms such as classical joins, groupjoin, and
in-database machine learning engines. This language is designed
around the notion of dictionaries, and allows for a more fine-grained
choice of its low-level implementation. Second, the cost model for
alternative implementations is automatically inferred by combin-
ing machine learning and program reasoning. The dictionary cost
model is learned using a regression model trained over the profiling
dataset of dictionary operations on a given hardware architecture.
The program cost model is inferred using static program analysis.
Our experimental results show the effectiveness of the trained
cost model on micro benchmarks. Furthermore, we show that the
performance of the code generated by our framework either outper-
forms or is on par with the state-of-the-art analytical query engines
and a recent in-database machine learning framework.

1 INTRODUCTION
Query processing engines have undergone a massive progress over
the previous decade. Traditionally, the volcano iterator model [33]
was considered the de-facto standard for building pipelined query
engines. This model streams the data along the query operators,
and works well for out-of-core scenarios. For in-memory databases,
the runtime overhead caused by this iterator model is mitigated by
a mixture of techniques such as query compilation [11, 23, 40, 45,
48, 57, 58, 74, 83] and vectorization [64, 90, 91].

To accommodate specialized query operators in the design of
modern query processing engines for in-memory databases, the
following considerations are common.
Hash-Based and Sort-Based Query Operators. The efficient
evaluation of query operators can benefit from hash-based and
sort-based data-structures [13, 32, 67]. The trade-offs between hash-
ing and sorting has been investigated in depth in the literature [10,
14, 43, 52, 56]. Most database systems tend to implement various
types of physical query operators using these two approaches (e.g.,
sort-merge-join and hash-join), and delegate to the optimizer the
task of picking the best choice based on the workload features.
Specialized Compound Query Operators. Such operators may
be beneficial for OLAP workloads and are implemented in state-
of-the-art in-memory database systems [51, 59]. For example, the
groupjoin operator [53] merges aggregate and hash-join operators.
The above considerations typically lead to two main challenges
for the query optimizer. First, for every specialized query operator,
the database developer needs to extend the set of supported query
operators. Second, one has to provide a cost model for the new

query operator. This can be daunting especially if the difference is
only in the low-level implementation details for the data structures.
This paper addresses these issues by proposing DBFlex, a query
processing engine with two main design decisions. First, DBFlex
uses a dictionary-based intermediate language. This language
is expressive enough to capture query processing algorithms for,
e.g., classical query operators, compound operators, and in-database
machine learning engines, and allows a cost-based choice for its
dictionary implementation.

Second, DBFlex automatically infers the cost models for al-
ternative implementations of a query and may uncover the right
trade-off between hashing and sorting for operators based on the
given workload and its underlying hardware architecture. Our de-
sign thus frees the database developers from the difficult and error-
prone task of defining a cost model for different query operators.
Motivating Example. To better understand the differences from
previous approaches, consider the following simplified TPCH query
Q3, where we removed one join and simplified the group-by clause:

select L.K, sum(L.P * L.D)
from L join O on L.K = O.K
where O.T < %DATE%
group by L.K

The join on K is key/foreign-key. Following TPCH specifications,
relation O cannot be indexed by T. As K is a part of compound key
for relation L, this relation can be ordered on it. Most traditional
query engines process the query using two hash-tables. The first
hash table is built and probed to construct the intermediate join
result. The second hash table is used for the group-by aggregate
result. The pseudocode for the corresponding data-centric compiled
engine [58] is as follows:

init Γ𝐻𝑇 , ⋈𝐻𝑇 as HashTable
for each tuple l in L
⋈𝐻𝑇 .insert(l.K, l)
for each tuple o in O

if o.T < %DATE%

if(⋈𝐻𝑇 .contains(o.K))

for each tuple l in ⋈𝐻𝑇 [o.K]

Γ𝐻𝑇 [l.K] += l.P * l.D

return Γ𝐻𝑇

This evaluation strategy can be improved as follows:
1) Compound Groupjoin. In this query, the join key and the group-by
attribute are the same. Thus, the intermediate hash tables Γ𝐻𝑇 and
⋈𝐻𝑇 can be merged. The resulting compound operator is referred
to as groupjoin [53]. As a result, the previous query can be rewritten
as follows:

init ⋈ 𝐻𝑇 as HashTable
for each tuple o in O

if o.T < %DATE%
⋈ 𝐻𝑇 [o.K] = 0

 
 
 
 
 
 
Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

Figure 1: Performance of the query in the running example
with different dictionaries as function of the selectivity of
the predicate on O.T. Relation L is already ordered on K.

for each tuple l in L

if(⋈ 𝐻𝑇 .contains(l.K))

⋈ 𝐻𝑇 [l.K] += l.P * l.D

return ⋈ 𝐻𝑇

2) Specialized Hash-Tables. The intermediate hash table for join,
group-by aggregate, and groupjoin can be implemented in various
ways. As an example, to resolve hash collisions there are different
approaches such as Hopscotch [35] and Robin hood [20] hashing.
Each implementation can be beneficial for different selectivities (cf.
Figure 1).
3) Sort-Based Dictionaries. Apart from using hash tables, one can use
sort-based dictionaries in order to maintain the intermediate joins
and aggregates. Examples are tree-base dictionaries (e.g., 𝐵+-trees,
Red-Black trees, etc.) and sorted arrays. These dictionaries can be
especially useful when one of the input relations is already ordered
based on the join/group-by key, which is the case for relation L
in our running example. Similarly, one can also have a sort-based
variant of groupjoin operator, which is used in engines such as
LMFAO [70] and FDB [60].

In order to support these specialized operators we need to over-
come the following challenges. First, the database developer needs
to extend the set of query operators with hash-based and sort-
based groupjoin, each variant with possibly different specialized
implementations. Second, designing the cost model for each vari-
ant is very tedious and is not easily portable to different hardware
architectures.

DBFlex solves these issues by introducing an intermediate lan-
guage around dictionaries. First, the program in this language does
not specify the data structure for dictionary ⋈ 𝐷𝑖𝑐𝑡 . Nevertheless,
it encodes the join order as well as the basic algorithm behind
groupjoin. Hence, there is no need to extend the set of query opera-
tors. More specifically, the query is expressed as follows:

init ⋈ 𝐷𝑖𝑐𝑡 as Dictionary
for each tuple o in O

if o.T < %DATE%

⋈ 𝐷𝑖𝑐𝑡 [o.K] = 0
for each tuple l in L

if(⋈ 𝐷𝑖𝑐𝑡 .contains(l.K))

⋈ 𝐷𝑖𝑐𝑡 [l.K] += l.P * l.D

return ⋈ 𝐷𝑖𝑐𝑡

Then, by using program synthesis, the implementation of the un-
derlying dictionary becomes explicit. To estimate the run time of

Figure 2: High-level architecture of DBFlex.

the program for each dictionary implementation, DBFlex uses ma-
chine learning and program reasoning. First, to model the cost of
dictionaries, a regression model is trained whose input features
are the size of dictionary, the orderedness of input data, and the
number of accessed tuples. This way, there is no need to use any
hardware parameters as features, as DBFlex profiles the dictionary
operations on each machine. Second, DBFlex statically analyzes the
statements to estimate the execution time of the whole program.
Figure 1 shows that the best dictionary implementation depends
on the selectivity: 1) this is hopscotch hashing for selectivities under
0.1%, 2) it is robin hood hashing for selectivities between 0.1% and
5%, and 3) becomes sorted table for selectivities larger than 5%, as
its amortized lookup cost starts paying off. DBFlex uses dictionary
size and number of access tuples, which define the selectivity.

The contributions of this paper are as follows:

• We propose a new architecture for building database systems
using our proposed intermediate language (Section 2). The high-
level view of the architecture of DBFlex is shown in Figure 2.
DBFlex allows for defining specialized dictionary implementa-
tions, the cost models of which can be automatically learned.
• We introduce a novel low-level intermediate language, called
LLQL. Our intermediate language is designed around the no-
tion of nested dictionaries [73] that generalize flat relations,
nested relations [69], tree-based indices, and trie-based represen-
tations [60] (Section 3). LLQL can express various basic query
operators (selection, projection, aggregation, and nested loop
join). Furthermore, it can express physical operators (hash-based
and sort-based groupby, hash join, sort-merge join, and index-
nested loop join), compound operators (hash-based and sort-
based groupjoin), and efficient in-DB machine learning engines.
• LLQL is designed with two goals in mind. First, it has to be low-
level enough to capture the underlying hardware architecture
behavior (e.g., execution time of dictionary operations). Second,
it should be high-level enough to allow for statically reasoning
about the programs (e.g., run-time execution cost of programs).
We show how the low-level nature of LLQL allows us to use
regression models learned over dictionary-related features such
as dictionary size, number of operator invocations, and ordered-
ness of data to capture the dictionary cost model (Section 4.1).

105104103102101100Selectivity0100200300Million Tuples / SecondRobin Hood Hash TableHopscotch Hash TableSorted TableSQLLinear AlgebraCollection ProgrammingLLQLRuntimeSynthesizerC / C++Cost EngineDictionary InterfaceCardinality EstimationRegression Model(Section 3)(Section 4)(Section 5)Fine-Tuning Data Structures for Analytical Query Processing

Furthermore, thanks to the domain-specific nature of LLQL, we
show how use program reasoning to infer the cost of LLQL ex-
pressions by using the dictionary cost model and a cardinality
model (Section 4.2).

• The derived cost model (Section 4) allows DBFlex to automatically
synthesize the LLQL program with the best cost. We present a
greedy algorithm for choosing the dictionary implementations
that lead to the LLQL with the lowest cost estimate (Section 5).
• Finally, we show experimentally the effectiveness of the learned
dictionary cost model and the inferred LLQL cost model. Also, we
show the advantage of using several dictionary implementations
for a query over using a single dictionary. Overall, our engine
outperforms the state-of-the-art engines Typer and Tectorwise
by 1.5x and respectively 2x in average, while also recovering
the runtime performance of the LMFAO in-database machine
learning engine, which is tuned for specific workloads (Section 6).

2 ARCHITECTURE AND SYSTEM DESIGN
In this section, we present the architecture of DBFlex. First, we de-
scribe the high-level architecture of our system (Section 2.1). Then,
we describe the workflow of our cost-based program synthesis (Sec-
tion 2.2). Finally, we show how database developers can extend
DBFlex with alternative dictionary implementations (Section 2.3).

2.1 Overall System Architecture
The architecture of DBFlex is presented in Figure 2. The input
program can be in a variety of languages including SQL, linear
algebra, and functional collection programming languages. This
means that DBFlex can serve as the backend engine of existing
DBMSes by getting their produced query plan and generating the
optimized C++ code for it. Futhermore, DBFlex can not only run
a wide range of analytical workloads, but also hybrid workloads
such as in-database machine learning.

As opposed to optimizing queries at the level of physical query
plan, DBFlex goes deeper [24]. Given a dataflow of query oper-
ators (i.e., the join order is specified using state-of-the-art tech-
niques [81, 88]), DBFlex synthesizes a LLQL program with the
dictionary implementations that lead to lowest cost estimates.

Similar to query optimization, there are two main components

for the synthesis:
• Cost Model: DBFlex defines cost model at the level of dictio-
nary operations and LLQL program. The dictionary cost model
is learned using a regression model over the profiling data (Sec-
tion 4.1). The LLQL cost model is statically inferred using infer-
ence rules over LLQL constructs (Section 4.2).

• Search: LLQL synthesis can be implemented using the same
search techniques employed in query optimizers such as dynamic-
programming, randomized algorithms, or other optimization
techniques such as integer linear programming [81, 88]). Section 5
gives a greedy algorithm for this component.

2.2 Workflow
Figure 3 shows the workflow of DBFlex in three stages.1

1In practice, the query optimization and query execution are the same stages. We make
this distinction for the sake of presentation.

Figure 3: The workflow of query processing in DBFlex.

1) Installation Stage: At this stage, the database system is be-
ing deployed to a particular machine. By generating a synthetic
profiling data and running the operations of various dictionary
implementations we generate a training set. Then, we can train a
regression model to capture the dictionary cost model.
2) Query Optimization: The input query is provided at this stage,
and is translated to a logical execution plan with the choice for
join orders (e.g., by Postgres or Apache Calcite). The logical plan
is translated into LLQL without the implementation choice for
dictionaries. The program synthesis generates the search space of
different LLQL programs with different dictionary implementations.
By using the trained regression models and inferring the execution
cost of alternative LLQL programs (Section 4.2), the LLQL with the
best dictionary implementations is chosen. The best LLQL program
is translated to low-level specialized engine code in C++.
3) Query Execution: Finally, the input data is passed to the gen-
erated specialized engine and output result is produced.

2.3 Extensions
DB developers can extend DBFlex in three dimensions:
• Dictionary Implementation: The runtime of DBFlex includes
a dictionary interface serving as an extension point (cf. Figure 3).
DBFlex exposes the API shown in Figure 4, inspired by the API
of standard library of C++ for dictionaries. After providing an
appropriate hash-based or sort-based dictionary implementation,
the developer needs to register the dictionary to be used during
the installation.

• Regression for Dictionary Cost Model: DBFlex uses out-of-
the-box regression models provided by machine learning frame-
works (e.g., sklearn and TensorFlow). The developers can provide
additional regression models.

• Cardinality Model: Cardinality and selectivity estimates are
essential components for cost-based optimizers. DBFlex relies
on state-of-the-art cardinality estimation models [31, 54, 89], as
studying the impact of different cardinality estimation models is
beyond the scope of this paper. However, the developer can use
alternative cardinality models.

Installation timeQuery OptimizationRuntimeRegressionModelProﬁling DataTraining SetML FrameworkCost EngineQuery ExecutionLogical Query PlanSynthesizerSearch SpaceC/C++ Specialized EngineInput DataQuery ResultRuntimetemplate<class Key, class Value>
class dict_impl {

class iterator; /* Iterator interface */
iterator begin();
iterator end();
iterator find(const Key& k);
iterator emplace(const Key& k, const Value& v);
/* Only for sorted dictionaries */
iterator find_hint(iterator position, const Key& k);
iterator emplace_hint(iterator position,

const Key& k, const Value& v);

};

Figure 4: The dictionary interface exposed by the runtime
of DBFlex.

3 LLQL
LLQL is a domain-specific language inspired by bag-based and
dictionary-based query languages (e.g., AGCA [45] and FAQ [8]).
There are two major design decision behind this language. First,
LLQL is not a purely functional language; in order to have full con-
trol on performance, the dictionary data-structure is not immutable.
This means that one can destructively update the value associated
with a key without the need to recreate another dictionary. Second,
LLQL can be data-structure-aware and encode the implementa-
tion choice for the underlying dictionaries. More specifically, the
dictionary accesses in this language are either based on hash ta-
bles or sorted data-structures. LLQL can express physical operators
(hash-based and sort-based groupby, hash join, sort-merge join,
and index-nested loop join), compound operators (hash-based and
sort-based groupjoin), and efficient in-DB ML engines.

3.1 Syntax
Figure 5 shows the grammar of LLQL for both expressions (e) and
types (T). The core data type supported by LLQL is a dictionary,
represented as {{ T -> T }}. LLQL represents input relations as
dictionaries from tuples to their multiplicity, because of their bag
semantics in database systems, as opposed to their set semantics in
relational algebra.

The expression for (r <- R) e specifies iteration over the ele-
ments of dictionary R, and performing e at each iteration. Records
can be constructed using { a_1 = e_1 , ... , a_n = e_n } and
the field a_i of record rec can be accessed using rec . a_i.

3.2 Dictionaries
The {{ k -> v }} constructs a singleton dictionary that has k
and v as its key and value, respectively. The dict (k) operator
performs a lookup for key k in the dictionary dict. The elements
of a dictionary are key-value pairs, which can be seen as records
with field names key and val. Thus, in the body of the loop for (r
<- R), one has to use r. key and r. val to access the key and value
of r, respectively. The addition for dictionaries is defined in terms
of elementwise addition; the values with the same key are added.
The choice of data structure is specified using @ ds, which is used
for constructing a dictionary: @ ds {{ e -> e }}. We can use
every specialized dictionary data structure instead of @ ds. However,

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

e

T

@ ds

Grammar
e ; e
()
let x = e in e
if (e) then e else e
{ a = e }
e.a
e b_op e | u_op e
n | r | false | true
" some_text "
ref (T)
e += e
@ ds {{ e -> e }}
for (x <- e) e
e(e) += e
e(e)
e. iter
e <e >( e) += e
e <e >( e)
@ ds {{ T -> T }}
int | double | bool
string
@ ht
@ st

::=
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
::=
|
|
::=
|

Description

Sequence of Statements
No-op
Variable Binding
Conditional
Record Construction
Field Access
Binary & Unary Operations
Numeric & Boolean Values
String Literal
Mutable Reference Init
Mutable Reference Update
Dictionary Construction
Dictionary Iteration
Dictionary Update
Dictionary Lookup
Dictionary Iterator
Dictionary Hinted Update
Dictionary Hinted Lookup
Dictionary Type
Numeric & Boolean Types
String Type
Hash-Table Dictionary
Sort-Based Dictionary

Figure 5: Grammar of LLQL.

for the sake of brevity, we only use these two annotations: 1) @ ht
for hash-based dictionaries, and 2) @ st for sort-based dictionaries.
Next, we present these two types of dictionaries.

3.2.1 Hash-based Dictionaries. One of the most obvious ways of
implementing a dictionary is by using a hash-table data-structure.
The keys are first mapped through a hash function to a particu-
lar bucket and the associated values can be accessed, inserted, or
removed with a constant-time complexity.

One of the key challenges for hash-based dictionaries is handling
collisions. Database systems have developed different approaches
such as robin hood hashing and chained hashing [68]. LLQL can
make such choices explicit by using an appropriate hash table
implementation.

The operators provided for a hash-based dictionary are: 1) iterat-
ing over a dictionary dict is specified using for (x <- dict ), 2)
inserting or updating the value v associated with the key k in a dic-
tionary dict is specified using dict += k -> v, and 3) looking
up the value associated with key k in a dictionary dict is specified
using dict (k).2

Sort-Based Dictionaries. In databases, there has been always
3.2.2
a dual approach to hashing, which is based on sorted data struc-
tures [67]. Such data structures can be either implemented using
sorted collections, or through tree-based data structures such as
B-Trees, B+-Trees, Red-Black Trees, etc.

2LLQL can be extended to support deletions as well. This makes it appropriate for
transactional workloads, which we leave for the future.

Fine-Tuning Data Structures for Analytical Query Processing

let Sh = @ht {{ }} in
for(s <- S) {

let Ss = @st {{ }} in
for(s <- S) {

Sh( part(s.key) ) += @ht {{ s.key -> s.val }}

Ss( part(s.key) ) += @st {{ s.key -> s.val }}

} ;
let RS = @ht {{ }} in

for(r <- R) {

let rkey = part(r.key) in
for(s <- Sh(rkey)) {

} ;
let RS = @st {{ }} in
let it = Ss.iter in
for(r <- R) {

let rkey = part(r.key) in
for(s <- Ss<it>(rkey)) {

RS( concat(r.key, s.key) ) += r.val * s.val

RS( concat(r.key, s.key) ) += r.val * s.val

} }

} }

(a) Hash join of R and S, join key given by part.

(b) Sort-merge join of R and S, join key given by part.

let Ragg = @ht {{ }} in

for(r <- R) {

let Ragg = @st {{ }} in
let it = Ragg.iter in
for(r <- R) {

Ragg( grp(r.key) ) += agg(r.key) * r.val

Ragg<it>( grp(r.key) ) += agg(r.key) * r.val

}

}

(c) Hash-based aggregation agg grouped by grp.

(d) Sort-based aggregation agg grouped by grp.

let Aggh = @ht {{}} in
let Sh = @ht {{}} in
for(s <- S) {

Sh( s.key.A ) += g(s)

} ;

for(r <- R) {

for(gs <- Sh(r.key.A)) {

Aggh( r.key.A ) += f(r) * gs.val

} }

let Aggs = @st {{}} in
let Ss = @st {{}} in
for(s <- S) {

Ss( s.key.A ) += g(s)

} ;
let it1 = Ss.iter
let it2 = Aggs.iter
for(r <- R) {

for(gs <- Ss<it1>(r.key.A)) {

Aggs<it2>( r.key.A ) += f(r) * gs.val

} }

(e) Hash-based groupjoin on A with partial aggregates f and g.

(f) Sort-based groupjoin on A with partial aggregates f and g.

Figure 6: Different query operators as LLQL expressions.

Dictionaries that are implemented using such sort-based data
structures can achieve logarithmic time for access, insertion, and up-
dates. Similar to hash-based dictionaries, the sort-based dictionaries
have the following three operations: 1) iteration using for (x <-
dict ), 2) insertion or update using dict += k -> v, and 3) look
up using dict (k). Each of these operators can leverage the fact
that the dictionary is already ordered. For example, the lookup can
use various specialized algorithms in order to further benefit from
the underlying architecture [82].

Furthermore, if the accessed keys are ordered, one can lower
the logarithmic run times to constant time. In order to access the
values associated with a key, if one knows that the accessed keys are
ordered, there is no need to lookup the whole range at each iteration.
Instead, one can only consider the keys not already covered by the
previous iterations. The same technique can be applied to insertion.
In order to support these optimized operators, LLQL provides
facilities in order to retrieve the iterator of a collection. There
are three constructs related to this feature: 1) the iterator of the
dictionary dict is retrieved using dict . iter, 2) the iterator it is
used as a hint for lookup of the key k in the dictionary dict using
dict <it >( k), and 3) the iterator it is used as a hint for updating
the value associated with the key k in the dictionary dict by v
using dict <it > += k -> v. Note that both lookup and insert
operators update the passed iterator.

3.2.3 Mapping to Runtime. The iteration over a dictionary corre-
sponds to the begin() and end() functions of the Runtime API in
Figure 4. The lookup corresponds to the find() function. The up-
date construct is implemented by first invoking find(). In the case
that a match is found, the corresponding value is incremented by
the input value. Otherwise, the corresponding key-value pair is in-
serted by using the emplace() function. Finally, the hinted versions
correspond similarly to find_hint() and emplace_hint().

Next, we show how both these two data structures can be used

for implementing various physical query operators.

3.3 Basic Query Operators
Selection. Consider a relation R, for which we are interested
3.3.1
in selecting the elements that satisfy a predicate p. For each element
r of this relation, if the predicate is satisfied, we increment the
multiplicity of the associated value with the key r. key (which
specifies the row of the relation) by r. val (which specifies the
multiplicity of that row). Otherwise, we do nothing.

let sel = {{}} in
for(r <- R) {

if(p(r.key)) then sel(r.key) += r.val
else ()

}

3.3.2 Projection. Similar to selection, we iterate over the elements
of the relation R. This time, we update the element of the dictionary
specified the application of the projection function f on each row
of relation (f(r. key )) as its key and unchanged value (r. val).
let proj = {{}} in
for(r <- R) {

proj(f(r.key)) += r.val

}

3.3.3 Nested-Loop Join. For this operation, we have to use nested
loops iterating over the elements r and s of relations R and S,
respectively. For each combination of tuples, we check if joinCond
is satisfied. If this is the case, we update the dictionary with an
element that has the combination of the tuples of these two relations
using function concat as its key, and r. val * s. val as its value.
let join = {{}} in
for(r <- R) {

for(s <- S) {

if(joinCond(r.key, s.key)) then

join( concat(r.key, s.key) ) += r.val * s.val

else
()

} }

This expression is inefficient because one has to consider all combi-
nations of r and s. This situation can be improved by leveraging
data locality as will be shown in Section 3.4.

Scalar Aggregation. These operators can be implemented
3.3.4
by iterating over the elements of the relation and computing the
appropriate aggregate function aggFun. For example, in the case of
summing the attributes A, aggFun (r. key ) is replaced by r. key .A,
and in the case of counting, is replaced by 1. As there could be
duplicates of an element in the input relation (the multiplicity of
which is shown by r. val), the aggregate result for each element
needs to be multiplied by r. val.
let agg = ref(double) in
for(r <- R) {

agg += aggFun(r.key) * r.val

}

3.3.5 Group-by Aggregate. The key difference between this opera-
tor and its non-grouped variant is that at each iteration, a group-by
aggregate returns a single dictionary with the key specified by the
grouping function grp, and the value specified using the aggregate
function aggFun:
let Agg = {{}} in
for(r <- R) {

Agg( grp(r.key) ) -> aggFun(r.key) * r.val }}

}

Both scalar and group-by aggregate operators can be generalized to
compute other forms of aggregates such as minimum and maximum
by supplying appropriate addition and multiplication operators. For
example, in the case of maximum, the maximum and numerical
addition need to be supplied as the addition and multiplication
operators, respectively [55]. To compute aggregates such as average,
one has to compute both summation and count.

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

3.4 Partitioned Joins
In the case of equality joins, one can partition the elements of the
relations based on their join key, and then only combining the
elements of the two relations that fall into the same partition.

3.4.1 Hash Join. Using a hash-table data-structure for a partitioned
join results in a hash-join operator. For example, the partitioned
join between relations R and S is lowered to hash join in Figure 6a.

Sort-Merge Join. In a partitioned join, if one uses a sort-
3.4.2
based dictionary rather than a hash table. In this case, if one of the
relations is ordered on the join key, one can avoid searching the
entire range from scratch for the next matching partition, by using
the hinted lookup operator.

As shown in Figure 6b, the iterator it is first set to the beginning
of the dictionary Ss. At the first iteration over relation R, the hinted
lookup Ss <it >( rkey ) searches the entire range to find the next
matching element. After returning the matching element (or an
empty dictionary if it does not find any match), the iterator it
is updated to the least upper bound position. This limits the next
lookups to the ranges that have been explored before. Thus, these
hinted lookups have an amortized constant time.

This algorithm has the same behavior as a sort-merge join oper-
ator. More generally, in the case where the relation R is not ordered
based on the join key, one needs to use the partitioned join operator
that partitions (by sorting) relation R based on the join key.

Furthermore, when relation S is already sorted based on the
join key, one can use a hinted insert operator. This brings the
computational complexity of the build phase from 𝑂 (𝑛.𝑙𝑜𝑔(𝑛)) to
𝑂 (𝑛). This algorithm is essentially the same as merge join, when
the input relations are already sorted:

let Ss = @st {{ }} in
let it1 = Ss.iter in
for(s <- S) {

Ss<it1>( part(s.key) ) += (@ht {{ s.key -> s.val }})

} ;
... // same as Figure 6b

3.4.3 Tree-Based Join. If rather than using a sorted dictionary, one
uses a tree-based dictionary (e.g., a dictionary implemented using
B+-Tree), DBFlex synthesizes a tree-based join algorithm. The tree-
based data structures have a significantly better insertion time in
the case where the input data is not already ordered.

3.5 Index-Nested Loop Join
Index-Nested Loop Join can be thought of as a specific case of
partitioned join; when one of the relations is already partitioned (i.e.,
indexed) on the join key, there is no need to perform the partitioning
in the query processing time. In this case, the partitioned join
operator can be seen as an index-nested loop join operator.

As an example, consider the case where there is a hash-based in-
dex, named as Sind, for relation S using the function part (s. key ).
The index-nested loop join for S and R is expressed as follows:
let RS = @ht {{ }} in
for(r <- R) {

for(s <- Sind(part(r.key))) {

RS( join(r.key, s.key) ) += r.val * s.val

} }

Fine-Tuning Data Structures for Analytical Query Processing

let Rp = {{}} in
for(r <- R) {

Rp( r.key.s ) += {{ r.key -> r.val }}

let Ragg = {{}} in
for(r <- R) {

Ragg( r.key.s ) +=

} ;
let Q = {{}} in
for(s <- S) {

for(r <- Rp(s.key.s)) {

Q( {i=s.key.i,c=r.key.c} ) += r.val*s.val

} } in

let Covar = ref({i_i:double, i_c:double, c_c:double})
for(x <- Q) {
Covar += { i_i=x.key.i*x.key.i*x.val,

i_c=x.key.i*x.key.c*x.val, c_c=x.key.c*x.key.c*x.val }

}

{ m = r.val, c = r.key.c * r.val,

c_c = r.key.c * r.key.c * r.val }

} ;
let Covar = ref({i_i:double, i_c:double, c_c:double}) in
for(s <- S) {

let r = Ragg(s.key.s) in

Covar += { i_i=s.key.i*s.key.i*s.val*r.m,
i_c=s.key.i*s.val*r.c, c_c=s.val*r.c_c }

}

(a) Initial unoptimized LLQL expression.

(b) After interleaving join and aggregations.

let Ragg = {{}}
for(r <- R) {

let Ragg = {{}}
for(r <- R) {

Ragg( r.key.s ) += { m = r.val, c = r.key.c * r.val,

Ragg( r.key.s ) += { m = r.val, c = r.key.c * r.val,

c_c = r.key.c * r.key.c * r.val }

c_c = r.key.c * r.key.c * r.val }

} in
let Covar = ref({i_i:double, i_c:double, c_c:double}) in
for(st <- Strie) {
let r = Ragg(st.key) in

for(s <- st.val) {

Covar += { i_i=s.key.i*s.key.i*s.val*r.m,
i_c=s.key.i*s.val*r.c, c_c=s.val*r.c_c }

}
}

} in

let Covar = ref({i_i:double, i_c:double, c_c:double}) in
for(st <- Strie) {
let r = Ragg(st.key) in
let sagg = ref({i_i:double, i:double, m:double}) in
for(s <- st.val) {

sagg += { i_i=s.key.i*s.key.i*s.val,

i=s.key.i*s.val, m=s.val}

} ;
Covar += { i_i=sagg.i_i*r.m,

i_c=sagg.i*r.c, c_c=sagg.m*r.c_c }

}

(c) After introducing trie indices.

(d) After factorization and loop-invariant code motion.

Figure 7: Different LLQL expressions representing covariance matrix computation over join of two relations.

Similarly, one can use a sort-based index (e.g., tree-based or sorted
dictionary). In this case, one can again benefit from hinted lookups
and inserts by utilizing an iterator.

or if the next operator can benefit from sorted input), using sort-
based aggregates can be more beneficial over their hash-based
counterparts.

3.6 Group-By Aggregation
The dictionaries used for computing group-by aggregates can also
be hash-based or sort-based.

3.6.1 Hash-Based Aggregation. Using a hash table as the underly-
ing data structure for dictionaries, results in a hash-based group
aggregate. In our previous group-by aggregate example, the final
result is stored in the variable Ragg, which is instantiated with an
empty hash-based dictionary. For each element r of relation R, we
update the value associated with key grp (r. key ) with the new
value g(r. key ) * r. val (cf. Figure 6c).

Sort-Based Aggregation. We can use a sort-based dictionary
3.6.2
for group-by aggregates as well. Furthermore, if the elements of
relation R are already sorted based on their group-by key, one can
use hinted inserts, as shown in Figure 6d.

As a result, the group-by aggregate can be computed in linear
time rather than 𝑂 (𝑛.𝑙𝑜𝑔(𝑛)). Furthermore, in the case where a
sorted result is more preferable (e.g., the presence of ORDER BY

3.7 Groupjoin Operators
Consider an aggregation over the result of join between two rela-
tions. The aggregate can be interleaved by the join computation.
This is achieved by decomposing the aggregate function into func-
tions that are only dependent on one of the relations. Then, one can
push the decomposed functions into their corresponding relations.
Finally, the result of these partial aggregates are joined together.

3.7.1 Hash-Based Groupjoin. In essence, this optimization has
the effect of fusing a partitioned join operator with an aggregate
operator. In fact, in the specific case of using a hash-table data-
structure for dictionaries, this optimization produces a groupjoin
operator [53].

Sort-Based Groupjoin. In the case of using a sort-based dictio-
3.7.2
nary, LLQL synthesizes the dual form of groupjoin operator. In the
previous example, if the elements of R are sorted based on A, then
one can make the aggregation even faster by having an amortized
constant time access for the elements of Ss (cf. Figure 6f).

3.8 In-DB Learning Engines
Recently, there has been an increasing interest in performing ma-
chine learning tasks inside a database system. One of the main
techniques for in-DB machine learning is to express the machine
learning task as an aggregation query. This way, one can globally
optimize both the feature extraction part of the ML task and its train-
ing, achieving orders of magnitude performance improvement [70].
As an example, let us consider a database with two relations:
S(s, i, u), R(s, c). The goal is to train a linear regression model that
predicts 𝑢 given features F = {𝑖, 𝑐}, where the training dataset is
the join of two relations Q = S ⋈ R.

Covariance matrix computation is an essential technique for
efficiently training machine learning models such as linear regres-
sion [7]. In our example, we consider the part of covariance ma-
trix that considers the interactions of 𝑖 and 𝑐, which is defined by
Σ𝑥 ∈Q𝑥ℓ ∗ 𝑥 𝑗 , where ℓ, 𝑗 ∈ F. The naïve approach for computing the
covariance matrix consists of two stages: 1) computing the join of
the input relations using a feature extraction query, followed by
2) aggregations computing the elements of this matrix. Thus, this
computation can be seen as a multi-aggregate query, the code of
which is shown in Figure 7a.

By interleaving the aggregate and join computations [4, 70],
there is no more need to compute the intermediate query Q, as
can be seen in Figure 7b. Instead the partial aggregates that are
dependent only on relation R, bound to variable Ragg, are computed
while scanning this relation.

Next, we introduce a trie index for relation S, represented as the
nested dictionary Strie. Instead of an iteration over the relation
S, this program performs a nested iteration over the trie Strie (cf.
Figure 7c); the outer iteration is over the first level of trie Strie,
and the second iteration is over the second level st . val.

Finally, we further improve the performance by factorizing the
independent factors from the inner loop and performing loop-
invariant code motion to hoist them outside. The expression sagg
computes the partial aggregates dependent only on relation S. The
last expression computes the final aggregate by multiplying the
corresponding factors from the two partial aggregates of relations
R and S (cf. Figure 7d).

4 COST ENGINE
In this section, we present how DBFlex automatically infers the
execution cost of LLQL programs by combining machine learning
and program reasoning. First, we present the dictionary cost model
trained by regression models in Section 4.1. Then, we show the cost
inference rules required for estimating the run-time cost of LLQL
programs using the dictionary cost model and cardinality model in
Section 4.2.

4.1 Dictionary Cost Model: Regression

Learning

The training set is generated by profiling the run time of the insert
and lookup operations for different dictionary implementations. For
insert, the profiling is generated for different dictionary sizes. For
lookup, the profiling is generated for different dictionary sizes and
number of accessed tuples. Note that the lookup operator shows
a different runtime behavior depending on whether the lookup is

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

successful or not. Hence, the training set contains profiling for both
successful and unsuccessful lookups.

Another important feature is the orderedness of the input data.
As observed in Section 3, the sort-based dictionaries can use hinted
lookups and inserts when the input keys is ordered. We profile the
situations where the sequence of key-value pairs to insert or to
look up are ordered or unordered. Note that the performance of
hash tables is independent of orderedness of keys.

In summary, our training dataset has the following features:
dictionary size, number of accessed tuples, and orderedness of data.
We also enhanced the features by adding the logarithm of dictionary
size and number of accessed tuples as explained in Section 6.2. The
labels are the run time performance for various operations in milli-
seconds. As future work, one can investigate further features, such
as data distribution parameters, tuple arity, and data type.

The next step involves training a regression model over the
profiling training set to predict the run time cost of dictionary
operations. This defines our dictionary cost model. We have used
a wide range of regression models, the behavior of which can be
observed in Section 6.2.

Next, we use program reasoning to derive the cost of LLQL

programs.

4.2 LLQL Cost Model: Program Reasoning
The trained regression models give a cost estimate for individual
dictionary operations. However, the program synthesizer requires
the cost estimate for LLQL expressions. Figure 8 shows how the
dictionary cost model (Δ
) are
combined with the runtime context of LLQL expressions to derive
their execution cost.

) and cardinality model (Σ

In Figure 8, the top and bottom parts of the inference rules spec-
ify the premises and the conclusions, respectively. As an example,
the premises for the second inference rule specifies that for the ex-
pression for (x <- e1 ) e2, the runtime context of e2 (Γ′) should
extend the runtime context of e1 (Γ) by recording that the number
of invocations is multiplied by the cardinality of e1 (Σ𝑐𝑎𝑟𝑑 (e1)).
The conclusion of this rule specifies that the execution cost of the
mentioned expression is 𝑐1 + 𝑐2 given that the execution cost of e1
and e2 is 𝑐1 and 𝑐2, respectively.
Example for Cost Inference. To give a more concrete example,
consider the hash-based group-by aggregate from Figure 6c. The ini-
tial runtime context is set as Γ = (Γ𝑐𝑎𝑙𝑙𝑠 = 1, Γ𝑐𝑜𝑛𝑑 = 1), specifying
that the number of invocations and the probability of taking the exe-
cution path are both 1. The runtime context of the update statement,
which is the body of the loop, is modified with Γ′
𝑐𝑎𝑙𝑙𝑠 = Σ𝑐𝑎𝑟𝑑 (R).
In the case of having a filter, Γ′
is modified according to the
inference rule of if statements.

𝑐𝑜𝑛𝑑

The execution cost of the update statement is computed follow-
ing the corresponding inference rule. First, assume that the cost
for the group-by key (grp (r. key )) and value (agg (r. key ) *
r. val) are 𝑐𝑘 and 𝑐𝑣, respectively. Second, the number of invoca-
tions of the update statement and the size of dictionary are com-
puted as 𝐶 = Γ′
𝑐𝑜𝑛𝑑 = Σ𝑐𝑎𝑟𝑑 (R) and 𝑁 = Σ𝑐𝑎𝑟𝑑 (Ragg).
Then, the number of lookup hits is computed as 𝐻 = 𝐶 − 𝑁 =
Σ𝑐𝑎𝑟𝑑 (R) − Σ𝑐𝑎𝑟𝑑 (Ragg). Finally, total execution time is computed
as 𝑐𝑘 +𝑐𝑣 +Δ𝑙𝑢𝑠 (𝐻, 𝑁 ) +Δ𝑙𝑢 𝑓 (𝑁 , 𝑁 ) +Δ𝑖𝑛𝑠 (𝑁 ), where Δ𝑙𝑢𝑠 (𝐻, 𝑁 ) is

𝑐𝑎𝑙𝑙𝑠 ∗ Γ′

Fine-Tuning Data Structures for Analytical Query Processing

Σ, Δ, Γ ⊢ e1: 𝑐1

Σ, Δ, Γ ⊢ e2: 𝑐2

Σ, Δ, Γ ⊢ e1: 𝑐1

Σ, Δ, Γ ⊢ e1 ; e2: 𝑐1 + 𝑐2

Σ, Δ, Γ′ ⊢ e2: 𝑐2
Σ, Δ, Γ ⊢ for (x <- e1 ) e2: 𝑐1 + 𝑐2

Γ′ = Γ [Γ′

𝑐𝑎𝑙𝑙𝑠 = Γ𝑐𝑎𝑙𝑙𝑠 ∗ Σ𝑐𝑎𝑟𝑑 (e1)]

Σ, Δ, Γ ⊢ e1: 𝑐1

Σ, Δ, Γ′ ⊢ e2: 𝑐2

Σ, Δ, Γ ⊢ e1: 𝑐1

Σ, Δ, Γ ⊢ e2: 𝑐2

Σ, Δ, Γ′′ ⊢ e3: 𝑐3

𝑐𝑜𝑛𝑑 = Γ𝑐𝑜𝑛𝑑 ∗ Σ𝑠𝑒𝑙 (e1)]
Σ, Δ, Γ ⊢ if ( e1 ) then e2 else e3: 𝑐1 + 𝑐2 + 𝑐3

Γ′ = Γ [Γ′

Γ′′ = Γ [Γ′′

𝑐𝑜𝑛𝑑 = Γ𝑐𝑜𝑛𝑑 ∗ (1 − Σ𝑠𝑒𝑙 (e1))]

𝐶 = Γ𝑐𝑎𝑙𝑙𝑠 ∗ Γ𝑐𝑜𝑛𝑑
Σ, Δ, Γ ⊢ e1 ( e2 ): 𝑐1 + 𝑐2 + Δ𝑙𝑢𝑠 (𝑑𝑠, 𝐻, 𝑁 ) + Δ𝑙𝑢 𝑓 (𝑑𝑠, 𝑀, 𝑁 )

𝑁 = Σ𝑐𝑎𝑟𝑑 (e1)

𝜎 = Σ𝑑𝑖𝑠𝑡 (e2)/𝑁 𝐻 = 𝜎 ∗ 𝐶 𝑀 = 𝐶 − 𝐻 𝑑𝑠 = Γ𝑑𝑖𝑐𝑡 (e1)

Σ, Δ, Γ ⊢ e1: 𝑐1

Σ, Δ, Γ ⊢ e2: 𝑐2
𝐻 = 𝐶 − 𝑁
Σ, Δ, Γ ⊢ e1 ( e2 ) += e3: 𝑐1 + 𝑐2 + 𝑐3 + Δ𝑙𝑢𝑠 (𝑑𝑠, 𝐻, 𝑁 ) + Δ𝑙𝑢 𝑓 (𝑑𝑠, 𝑁 , 𝑁 ) + Δ𝑖𝑛𝑠 (𝑑𝑠, 𝑁 )

𝐶 = Γ𝑐𝑎𝑙𝑙𝑠 ∗ Γ𝑐𝑜𝑛𝑑

𝑁 = Σ𝑐𝑎𝑟𝑑 (e1)

Σ, Δ, Γ ⊢ e3: 𝑐3

𝑑𝑠 = Γ𝑑𝑖𝑐𝑡 (e1)

Σ

Δ
Γ

Σ𝑐𝑎𝑟𝑑 Cardinality of the given dictionary
Δ𝑙𝑢𝑠
Γ𝑐𝑎𝑙𝑙𝑠

Cost of successful lookup
Total number of invocations

Σ𝑑𝑖𝑠𝑡 Number of distinct elements
Δ𝑙𝑢 𝑓
Γ𝑐𝑜𝑛𝑑

Cost of failed lookup
Accumulative probability

Selectivity of condition

Σ𝑠𝑒𝑙
Δ𝑖𝑛𝑠
Γ𝑑𝑖𝑐𝑡 Dictionary implementation

Cost of insertion

Figure 8: Cost inference of a subset of LLQL expressions. The contexts used in the inference rules are as follows: Σ corresponds
to cardinality model, Δ corresponds to the dictionary cost model, and Γ corresponds to the runtime context.

the estimation cost returned by the regression model for successful
lookups of 𝐻 elements in a dictionary of size 𝑁 , Δ𝑙𝑢 𝑓 (𝑁 , 𝑁 ) speci-
fies the cost model for failed lookups of 𝑁 elements in a dictionary
of size 𝑁 , and Δ𝑖𝑛𝑠 (𝑁 ) specifies the cost for inserting 𝑁 elements
to a dictionary.

In the next section, we show how this cost model can be used

for program synthesis.

5 PROGRAM SYNTHESIS
This section presents fine-tuning of the dictionary implementations
by using program synthesis. The input to the program synthesis
is an LLQL expression for which the join order is already speci-
fied. Then, the search space for using different combinations of
dictionary implementations is generated. By using the cost infer-
ence engine shown in the previous section, we find the LLQL with
dictionary implementations that lead to the lowest execution time.
Algorithm 1 shows a greedy algorithm for the data-structure
selection process. First, the distinct dictionaries that exist in the
input LLQL expression are extracted (Line 2). Then, a dependency
graph [28] among these dictionaries is created, so that we traverse
them in dependency order (Line 3).

For each dictionary symbol (sym), we select the data structure
with the minimum total run-time cost estimate (Line 6). The func-
tion Cost uses the inference rules presented in Figure 8, and its
runtime context is updated to use 𝑑𝑠 for the dictionary symbol sym.
Accordingly we update the runtime context with best dictionary
implementation for sym (Line 7).

Finally, we replace the dictionary symbols in exp by the imple-
mentation choices collected in the runtime context (Line 9). This is
achieved by changing the annotations @ ds in the statements where
the dictionary symbols are introduced with the corresponding dic-
tionary implementation.

We observe that for many analytical queries, where one uses
pipelining or the intermediate results only used for probing, there
is no dependency between dictionary symbols. In such cases, the
greedy algorithm finds the optimal program (assuming that the

Inputs:
exp: Input expression
Σ: Cardinality model

Δ: Dictionary cost model
𝑫𝑺: Dictionary implementations
1: function ProgramSynthesis(exp, Σ, Δ, 𝑫𝑺)
𝑫 𝒊𝒄𝒕 ← ExtractDictSymbols(exp)
2:
𝑫𝑨𝑮 ← DependencyGraph(exp, 𝑫 𝒊𝒄𝒕)
Γ ← (Γ𝑐𝑎𝑙𝑙𝑠 = 1, Γ𝑐𝑜𝑛𝑑 = 1)
for sym ← 𝑫𝑨𝑮 do
𝑑𝑠𝑏𝑒𝑠𝑡 ← argmin
𝑑𝑠 ∈𝑫𝑺
Γ ← Γ [Γ𝑑𝑖𝑐𝑡 (sym) = 𝑑𝑠𝑏𝑒𝑠𝑡 ]

3:

5:

6:

4:

7:

Cost(exp, Σ, Δ, Γ [Γ𝑑𝑖𝑐𝑡 (sym) = 𝑑𝑠])

8:

9:

end for
final ← ChooseDictDS(exp, Γ𝑑𝑖𝑐𝑡 )
return final

10:
11: end function

Algorithm 1: A greedy algorithm for program synthesis.

cost/cardinality models are precise). However, in cases where one
needs to iterate over the intermediate dictionaries (e.g., in-DB ML
and TPCH query 18), the greedy algorithm can fall into local opti-
mum. We leave the usage of further search algorithms [38, 81, 88]
for future work.

6 EXPERIMENTAL RESULTS
In this section, we investigate the experimental results of DBFlex.
Our findings are summarized as follows:
• The predicted cost by the dictionary cost model is proportional to
the actual time spent for each operation using different dictionary
implementations (Section 6.2.1).

• Utilizing the LLQL cost model to find the best query operator
can prevent a slowdown compared to the best plan in most cases
(Section 6.2.2).

• If we resort to a single dictionary implementation for the entire
query, we observe our engine performs on par with state-of-the-
art analytical engines (Section 6.3).

• By using several dictionary implementations for one query, we
observe an average of 70% performance improvement over the
version that uses a single dictionary implementation. This is in
particular the case for queries that can benefit from sort-based
group-by and join (Section 6.3).

• Overall, DBFlex outperforms the state-of-the-art query engines
Typer and Tectorwise by 1.5x and respectively 2x on average
(Section 6.3), while also recovering the runtime performance of
the LMFAO specialized in in-DB machine learning (Section 6.4).

6.1 Experimental Setup
All experiments are performed on two machines:
• Machine 1 is an iMac equipped with an Intel Core i5 CPU run-
ning at 2.7GHz, 32GB of DDR3 RAM with OS X 10.13.6. We use
CLang 900.0.39.2 for compiling the generated C++ code using
the O3 optimization flag.

• Machine 2 is equipped with Intel(R) Core(TM) i7-4770 at 3.40GHz,
32GB of DDR3 RAM, running Ubuntu 18.04. We use g++ 6.4.0 for
compiling the generated C++ code using the O3 flag.

We use the following dictionary implementations:
• unordered_map: C++ STL hashing.
• robinhood_dict: robin-hood hashing [3].
• tsl_dict: hopscotch hashing [2].
• boost_unordered_map: Boost hashing [5].
• map: C++ STL Red-Black tree dictionary.
• boost_flat_map: Boost sorted flat array [5].
• tlx_dict: TLX B+-tree dictionary [6].
• absl_dict: Abseil B-tree dictionary [1].
Competitors. We benchmark our engine against the following
in-memory engines3: (1) Typer and (2) Tectorwise [23], the open
source implementation4 of HyPer [58] and Vectorwise [91], and (3)
the in-DB ML engine LMFAO [70].5 Typer and LMFAO use query
compilation and Tectorwise uses vectorization. The code for queries
in all systems is in C++.6

For all the experiments, we compute the average of ten subse-
quent runs. We perform all experiments using a single core. We
leave the experimentation for multi-core environments for the fu-
ture since it requires dealing with parallelization concerns (e.g.,
lock-based vs. lock-free data structures). The loading time of the
database into main memory is not considered. For in-DB machine
learning experiments, all relations are sorted by their join attributes
for both DBFlex and LMFAO.

6.2 Cost Engine Performance
6.2.1 Dictionary Cost Model. In this section, we report the per-
formance of our dictionary cost model using various regression
models for prediction. Several regression models were trained over

3Comparing against disk-based engines requires our cost model to consider hardware
characteristics of HDDs and SDDs (e.g., erase time), which we leave for future.
4https://github.com/TimoKersten/db-engine-paradigms
5https://github.com/fdbresearch/LMFAO
6Generating LLVM or machine code can improve query compilation time [46]. How-
ever, improving compilation time is beyond the scope of this paper; instead we only
focus on query execution time.

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

Figure 9: Comparison of the prediction of different regres-
sion models trained under various methods with operation
running times.

the profiling training set to predict the run time cost of dictionary
operations using three different methods:
• All in One Model: A single regression model is used for cost pre-
diction. The model uses the dictionary size, number of accessed
tuples, orderedness of data, dictionary type, and the operation
as features. The last two mentioned features are passed to the
model in the one-hot encoded format.

• Individual Models Without Feature Engineering: 32 different
regression models are constructed based on the combinations
of data order, dictionary implementation, and operations. Each
model takes the dictionary size and the number of accessed tuples
as features.

• Individual Models With Feature Engineering: It is constructed
in the same way to the previous method. However, its features
are enriched with the logarithm values of the dictionary size and
the number of accessed tuples.

Figure 9 shows a comparison between our cost model with the
actual run-time spent on basic operations (lookup and insert). We
observe that in most cases, our cost model is proportional to the
actual time on a logarithmic scale. Since the logarithm of dictionary
size accurately captures the relationship between dictionary size
and actual operation cost, the models that have been trained with
feature engineering outperform other methods. Overall, KNN with
𝐾 = 4 and trained with logarithmic features performs the best
among all these models.

6.2.2 LLQL Cost Model. To evaluate the LLQL cost model, we
benchmark the LLQL program for the group-by operation by vary-
ing the selectivity of the input relation. Overall, we generate 70

1e-61e-41e-211e21e41e6All in One ModelW/o Feature EngineeringPolynomialWith Feature Engineering1e-61e-41e-211e21e41e6KNN (K=4)1e-41e-211e21e41e-61e-41e-211e21e41e61e-41e-211e21e41e-41e-211e21e4AdaBoostTime [ms] (Log10 Scale)Predicted Cost (Log10 Scale)Fine-Tuning Data Structures for Analytical Query Processing

1
e
n
i
h
c
a
M

2
e
n
i
h
c
a
M

Figure 10: Slowdowns compared to the best option for the
group-by operation.

Figure 11: Run time results for TPCH queries comparing dif-
ferent strategies employed for choosing data-structures in
DBFlex and state-of-the-art query processing engines.

different experiments by logarithmically increasing the selectivity.
Figure 10 demonstrates that our cost model mostly suggests the
best dictionary choice based on given features. Selecting the best
dictionary based on our cost model’s prediction outperforms each
of the implementations individually.

6.3 Analytical Query Engines
In this section, we investigate the performance of DBFlex for OLAP
workloads. For this purpose, we use a representative subset of
TPCH queries involving joins and aggregations with a wide range
of intermediate cardinalities [16, 41]. We compare the performance
of generated optimized code for two best hash-based dictionaries,
the best sort-based dictionary, the fine-tuned versions (M1 Tuned
and M2 Tuned), Typer, and Tectorwise.

As Figure 11 shows, we observe that overall the fine-tuned opti-
mized queries perform better or the same as the Typer and Tector-
wise engines. Furthermore, we observe that in most cases, the tuned
versions for the two machines produce identical query plans. In Q1,
the only involved dictionary favors a robinhood_dict dictionary
in machine 1, instead of tsl_dict in machine 2.

Q3, Q5, and Q9 all involve multiple joins, and for all of them the
hash-based robinhood_dict dictionaries show promising perfor-
mance. However, the fine-tuned optimized query for all of them
involves a mixture of boost_flat_map and robinhood_dict. Fur-
thermore, all these queries show good performance for vectorized
engines such as Tectorwise. Especially, Q9 involves a large interme-
diate dictionary, for which a vectorized engine is better at hiding
memory stalls for a large intermediate hash join [41].

Finally, Q18 involves a high-cardinality aggregation operator.
For this query, we observe that sort-based dictionaries such as

boost_flat_map outperform hash-based ones. A particular inter-
esting characteristic of this query is that two instances of the sort-
based dictionaries cannot use the hinted version of lookup. Never-
theless, due to the low cardinality of the corresponding interme-
diate dictionaries, makes the overall non-hinted and logarithmic
lookup computation time of sort-based dictionaries is comparable
to the constant lookup time of hash-based ones. Thus, the overall
performance of sort-based dictionaries is better than hash tables.

6.4 In-DB Machine Learning
As the final set of experiments, we show the performance of DBFlex
for in-DB ML workloads. We use two real-world datasets: 1) Fa-
vorita [25], which is a publicly available Kaggle dataset, and 2)
Retailer is a dataset from a US retailer [71]. Both datasets are used
in retail forecasting scenarios and have a snowflake schema with 4
dimension tables for both and with fact tables of 87 million and 125
million tuples, respectively. We only use the continuous attributes
of these datasets, which are 6 and 35 attributes, respectively.

Figure 12 shows the run-time comparison of different configu-
rations of DBFlex for computing the covariance matrix on these
two datasets. As the input relations are already indexed as ordered
tries, sort-based dictionaries show better performance. Thus, we
compare the generated code using two best sort-based dictionaries
and the best hash table.

For the Favorita dataset, we observe that boost_flat_map out-
performs robinhood_dict and tlx_dict. The sort-based dictio-
naries use hinted lookups and inserts in all cases by default thanks
the ordered nature of their input data. However, the fine-tuned
version for both machines prefer a non-hinted lookup in the case
where the size of the intermediate dictionary is too small and there
are too many failed lookups in deeply nested loops. This kind of
knowledge is not possible to be captured by the competitor systems
such as LMFAO.

01020304050Predicted Dictionary (KNN)robinhood1(1, 2](2, 5](5, 10](10, 30]01020304050tsl1(1, 2](2, 5](5, 10](10, 30]boost_flatmapQ1Q3Q5Q9Q182050100100200500100010002000Run Time (ms)Q1Q3Q5Q9Q182050100100200500100010002000Run Time (ms)DBFlex (tsl_dict)DBFlex (robinhood_dict)DBFlex (boost_flat_map)DBFlex (M1 Tuned)DBFlex (M2 Tuned)TyperTectorwise1
e
n
i
h
c
a
M

2
e
n
i
h
c
a
M

Figure 12: Run time results for in-DB ML of the covari-
ance matrix computation comparing different strategies em-
ployed for choosing data-structures in DBFlex and LMFAO.

The Retailer dataset shows better performance for hash-based
dictionaries. This is because of the failed lookups in deeply nested
loops, which makes hinted lookups perform worse. The interme-
diate dictionaries in the fine-tuned generated code are a mixture
of robinhood_dict and boost_flat_map, and one of the lookups
of boost_flat_map is non-hinted. This makes the performance of
fine-tuned DBFlex better than LMFAO in machine 1, and compara-
ble to it in machine 2.

7 RELATED WORK
Query Languages. Nested relational model [69], monad calculus,
and monoid comprehension [17–19, 26, 34, 80, 84, 87] are query
languages for nested collections, whereas AGCA [45], FAQ [8], and
HoTTSQL [22] represent relations as bags. The dictionary-oriented
nature of LLQL combines these two lines of work; both relations and
group-by aggregates are represented as dictionaries. Furthermore,
LLQL allows hash-based and sort-based data structures to be used
for dictionaries, with the capability of encoding hinted lookup and
insertions for sort-based ones.
Query Compilation. Just-in-time compilation of queries allows
for generating specialized engines and has been heavily used for
analytical query processing [11, 23, 40, 45, 48, 57, 58, 61, 74, 75, 83].
In parallel, the compilers of functional languages heavily investi-
gated the specialized low-level code generation with focus on fusion
of intermediate collections [44, 50, 72]. Recently, there has been
several efforts on low-level query plan languages, mainly inspired
by functional collections [15, 27, 47, 63]. None of these systems
have focused on fine-tuning data structures and do not support
automatic inference of cost models.

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

Data-structure specialization in LegoBase [74] and LB2 [79] fo-
cuses on more aggressive partial evaluation for the provided hash-
table implementations, without tuning based on cardinalities or
using sort-based dictionaries.

Chestnut [88] uses integer linear programming to specify data
layouts used for database-backed applications, using manually spec-
ified cost models. Micro adaptivity [66] is a technique for choosing
the best function implementation in runtime for Vectorwise. Simi-
lar to DBFlex, it frees database developers from manually writing
cost models. DBFlex generalizes this idea to higher-level decisions
such as the choice of data structures, while combining it with query
compilation.
In-DB Machine Learning. Training ML models inside the data-
base system by avoiding the materialization of join has recently
gained great interest in the community. The current solutions are
currently divided into two categories. First, systems such as Mor-
pheus [21, 49] cast the in-DB ML task as a linear algebra problem.
For example, such tasks are expressed on top of linear algebra li-
braries of R [21] and NumPy [49]. The second category casts the
in-DB ML task as a multi-aggregate analytical query. Systems such
as F [60, 71], AC/DC [42], LMFAO [70], IFAQ [76, 77], SDQL [73],
as well as DBFlex fall into this category. None of the mentioned sys-
tems support fine-tuning hash-based and sort-based data structures
as well as automatic inferring of cost models.
Cost Inference. The run-time cost estimation of programs has
been heavily investigated for databases [67] and programming
languages [9, 36, 39, 85]. In addition to improving the performance,
cost estimation can be used for verification purposes (e.g., ensuring
resource usage is bounded for embedded devices) [37]. The cost
model used in DBFlex estimates the run-time cost by relying on the
trained models over actual profile data, as opposed to the alternative
approaches which mostly rely on asymptotic reasoning [85].
Auto-Tuning. Automatic tuning of performance-critical kernels
is a well-investigated topic in the high-performance computing
and compilers communities [12]. Examples include FFTW [30] and
Spiral [65] for Fourier transforms, and LGen [78] and ATLAS [86]
for linear algebra. None of the mentioned systems support query
processing workloads, and do not fine-tune data structures.

8 CONCLUSION
In this paper, we present DBFlex, a framework that automatically
synthesizes analytical engines with fine-tuned data structures. This
is facilitated by LLQL, a low-level intermediate language based on
dictionaries. The execution cost of LLQL is automatically inferred
by 1) training regression models for the cost model of dictionary im-
plementations, and 2) using cost inference rules on LLQL statements
for the cost model of the entire LLQL program. Our experimental
results show the effectiveness of our cost model and its performance
in comparison with state-of-the-art in-memory engines for query
processing and in-database machine learning. We plan to explore
including more features for the dictionary cost model. Another
promising future direction is multi-core and parallel architectures
that can impose further challenges.

REFERENCES
[1] Abseil b-tree dictionary. https://github.com/abseil/abseil-cpp.

FavoritaRetailer1125101020Run Time (s)DBFlex (robinhood_dict)DBFlex (boost_flat_map)DBFlex (tlx_dict)DBFlex (M1 Tuned)DBFlex (M2 Tuned)LMFAOFavoritaRetailer1125101020Run Time (s)Fine-Tuning Data Structures for Analytical Query Processing

[2] C++ implementation of a fast hash map and hash set using hopscotch hashing.

https://github.com/Tessil/hopscotch-map.

[3] Fast & memory efficient hashtable based on robin hood hashing for c++. https:

//github.com/martinus/robin-hood-hashing.

[4] GLib: Library package for low-level data structures in C. https://developer.gnome.

org/glib/2.38/.

[5] Stl-like containers from boost. https://github.com/boostorg/container.
[6] Tlx - a collection of sophisticated c++ data structures, algorithms, and miscella-

neous helpers. https://github.com/tlx/tlx.

[7] M. Abo Khamis, H. Q. Ngo, X. Nguyen, D. Olteanu, and M. Schleich. In-database
learning with sparse tensors. In Proceedings of the 37th ACM SIGMOD-SIGACT-
SIGAI Symposium on Principles of Database Systems, SIGMOD/PODS ’18, page
325–340, New York, NY, USA, 2018. Association for Computing Machinery.
[8] M. Abo Khamis, H. Q. Ngo, and A. Rudra. Faq: Questions asked frequently. In
Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of
Database Systems, PODS ’16, pages 13–28, New York, NY, USA, 2016. Association
for Computing Machinery.

[9] E. Albert, P. Arenas, S. Genaim, M. Gómez-Zamalloa, G. Puebla, D. Ramírez,
G. Román, and D. Zanardini. Termination and cost analysis with costa and its
user interfaces. Electronic Notes in Theoretical Computer Science, 258(1):109–121,
2009.

[10] M.-C. Albutiu, A. Kemper, and T. Neumann. Massively parallel sort-merge joins in
main memory multi-core database systems. Proceedings of the VLDB Endowment,
5(10), 2012.

[11] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng, T. Kaftan,
M. J. Franklin, A. Ghodsi, and M. Zaharia. Spark SQL: Relational Data Processing
in Spark. In Proceedings of the 2015 ACM SIGMOD International Conference on
Management of Data, SIGMOD ’15, pages 1383–1394, New York, NY, USA, 2015.
ACM.

[12] A. H. Ashouri, W. Killian, J. Cavazos, G. Palermo, and C. Silvano. A survey on
compiler autotuning using machine learning. ACM Computing Surveys (CSUR),
51(5):1–42, 2018.

[13] M. Athanassoulis and S. Idreos. Design tradeoffs of data access methods. In
Proceedings of the 2016 International Conference on Management of Data, SIGMOD
’16, page 2195–2200, New York, NY, USA, 2016. Association for Computing
Machinery.

[14] C. Balkesen, G. Alonso, J. Teubner, and M. T. Özsu. Multi-core, main-memory
joins: Sort vs. hash revisited. Proceedings of the VLDB Endowment, 7(1):85–96,
2013.

[15] M. Bandle and J. Giceva. Database technology for the masses: Sub-operators as

first-class entities. Proc. VLDB Endow., 14(11):2483–2490, 2021.

[16] P. Boncz, T. Neumann, and O. Erling. TPC-H Analyzed: Hidden Messages and
Lessons Learned from an Influential Benchmark, pages 61–76. Springer Interna-
tional Publishing, Cham, 2014.

[17] V. Breazu-Tannen, P. Buneman, and L. Wong. Naturally embedded query languages.

Springer, 1992.

[18] V. Breazu-Tannen and R. Subrahmanyam. Logical and computational aspects of

programming with sets/bags/lists. Springer, 1991.

[20] P. Celis, P.-A. Larson, and J. I. Munro. Robin hood hashing.

[19] P. Buneman, S. Naqvi, V. Tannen, and L. Wong. Principles of programming with
complex objects and collection types. Theor. Comput. Sci., 149(1):3–48, Sept. 1995.
In 26th Annual
Symposium on Foundations of Computer Science (sfcs 1985), pages 281–288. IEEE,
1985.

[21] L. Chen, A. Kumar, J. Naughton, and J. M. Patel. Towards linear algebra over
normalized data. Proceedings of the VLDB Endowment, 10(11):1214–1225, 2017.
[22] S. Chu, K. Weitz, A. Cheung, and D. Suciu. Hottsql: Proving query rewrites with

univalent sql semantics. ACM SIGPLAN Notices, 52(6):510–524, 2017.

[23] A. Crotty, A. Galakatos, K. Dursun, T. Kraska, U. Çetintemel, and S. B. Zdonik.

Tupleware:" big" data, big analytics, small clusters. In CIDR, 2015.

[24] J. Dittrich and J. Nix. The case for deep query optimisation. arXiv preprint

arXiv:1908.08341, 2019.

[25] C. Favorita. Corp. Favorita Grocery Sales Forecasting: Can you accurately predict

sales for a large grocery chain?, October 2017.

[26] L. Fegaras and D. Maier. Optimizing object queries using an effective calculus.

ACM Trans. Database Syst., 25(4):457–516, Dec. 2000.

[27] P. Fent and T. Neumann. A practical approach to groupjoin and nested aggregates.

Proc. VLDB Endow., 14(11):2383–2396, 2021.

[28] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph

and its use in optimization. TOPLAS, 9(3):319–349, July 1987.

[29] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line
learning and an application to boosting. Journal of computer and system sciences,
55(1):119–139, 1997.

[30] M. Frigo and S. G. Johnson. Fftw: An adaptive software architecture for the fft.
In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and
Signal Processing, ICASSP’98 (Cat. No. 98CH36181), volume 3, pages 1381–1384.
IEEE, 1998.

[31] L. Getoor, B. Taskar, and D. Koller. Selectivity estimation using probabilistic
In Proceedings of the 2001 ACM SIGMOD international conference on

models.

Management of data, pages 461–472, 2001.

[32] G. Graefe. Query evaluation techniques for large databases. CSUR, 25(2):73–169,

June 1993.

[33] G. Graefe. Volcano-an extensible and parallel query evaluation system. IEEE

Transactions on Knowledge and Data Engineering, 6(1):120–135, 1994.

[34] T. Grust and M. Scholl. How to comprehend queries functionally. Journal of

Intelligent Information Systems, 12(2-3):191–218, 1999.
[35] M. Herlihy, N. Shavit, and M. Tzafrir. Hopscotch hashing.

In International

Symposium on Distributed Computing, pages 350–364. Springer, 2008.

[36] J. Hoffmann, K. Aehlig, and M. Hofmann. Multivariate amortized resource
analysis. In Proceedings of the 38th annual ACM SIGPLAN-SIGACT symposium on
Principles of programming languages, pages 357–370, 2011.

[37] J. Hoffmann, A. Das, and S.-C. Weng. Towards automatic resource bound analysis
for ocaml. In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of
Programming Languages, pages 359–373, 2017.

[38] Y. E. Ioannidis and Y. Kang. Randomized algorithms for optimizing large join
queries. In Proceedings of the 1990 ACM SIGMOD International Conference on
Management of Data, SIGMOD ’90, pages 312–321, New York, NY, USA, 1990.
ACM.

[39] S. Jost, K. Hammond, H.-W. Loidl, and M. Hofmann. Static determination of
quantitative resource usage for higher-order programs. In Proceedings of the
37th annual ACM SIGPLAN-SIGACT symposium on Principles of programming
languages, pages 223–236, 2010.

[40] M. Karpathiotakis, I. Alagiannis, T. Heinis, M. Branco, and A. Ailamaki. Just-
in-time data virtualization: Lightweight data management with vida. In CIDR,
2015.

[41] T. Kersten, V. Leis, A. Kemper, T. Neumann, A. Pavlo, and P. Boncz. Everything
you always wanted to know about compiled and vectorized queries but were
afraid to ask. Proceedings of the VLDB Endowment, 11(13):2209–2222, 2018.
[42] M. A. Khamis, H. Q. Ngo, X. Nguyen, D. Olteanu, and M. Schleich. Ac/dc: In-
database learning thunderstruck. In Proceedings of the Second Workshop on Data
Management for End-To-End Machine Learning, DEEM’18, pages 8:1–8:10, New
York, NY, USA, 2018. ACM.

[43] C. Kim, T. Kaldewey, V. W. Lee, E. Sedlar, A. D. Nguyen, N. Satish, J. Chhugani,
A. Di Blas, and P. Dubey. Sort vs. hash revisited: Fast join implementation on
modern multi-core cpus. Proceedings of the VLDB Endowment, 2(2):1378–1389,
2009.

[44] O. Kiselyov, A. Biboudis, N. Palladinos, and Y. Smaragdakis. Stream fusion, to
completeness. In Proceedings of the 44th ACM SIGPLAN Symposium on Principles
of Programming Languages, POPL 2017, pages 285–299, New York, NY, USA, 2017.
ACM.

[45] C. Koch, Y. Ahmad, O. Kennedy, M. Nikolic, A. Nötzli, D. Lupei, and A. Shaikhha.
DBToaster: higher-order delta processing for dynamic, frequently fresh views.
VLDBJ, 23(2):253–278, 2014.

[46] A. Kohn, V. Leis, and T. Neumann. Adaptive execution of compiled queries.
In 2018 IEEE 34th International Conference on Data Engineering (ICDE), pages
197–208. IEEE, 2018.

[47] A. Kohn, V. Leis, and T. Neumann. Building Advanced SQL Analytics From Low-
Level Plan Operators, page 1001–1013. Association for Computing Machinery,
New York, NY, USA, 2021.

[48] K. Krikellas, S. Viglas, and M. Cintra. Generating code for holistic query evalua-

tion. In ICDE, pages 613–624, 2010.

[49] S. Li, L. Chen, and A. Kumar. Enabling and optimizing non-linear feature in-
teractions in factorized linear algebra. In Proceedings of the 2019 International
Conference on Management of Data, pages 1571–1588. ACM, 2019.

[50] G. Mainland, R. Leshchinskiy, and S. Peyton Jones. Exploiting Vector Instruc-
tions with Generalized Stream Fusion. In Proceedings of the 18th ACM SIGPLAN
International Conference on Functional Programming, ICFP’13, pages 37–48, New
York, NY, USA, 2013. ACM.

[51] P. Menon, T. C. Mowry, and A. Pavlo. Relaxed operator fusion for in-memory
databases: Making compilation, vectorization, and prefetching work together at
last. Proc. VLDB Endow., 11(1):1–13, Sept. 2017.

[52] N. Mirzadeh, Y. O. Koçberber, B. Falsafi, and B. Grot. Sort vs. hash join revisited
for near-memory execution. In 5th Workshop on Architectures and Systems for
Big Data (ASBD 2015), 2015.

[53] G. Moerkotte and T. Neumann. Accelerating queries with group-by and join by

groupjoin. Proceedings of the VLDB Endowment, 4(11), 2011.

[54] G. Moerkotte, T. Neumann, and G. Steidl. Preventing bad plans by bounding
the impact of cardinality estimation errors. Proceedings of the VLDB Endowment,
2(1):982–993, 2009.

[55] M. Mohri. Semiring frameworks and algorithms for shortest-distance problems.

Journal of Automata, Languages and Combinatorics, 7(3):321–350, 2002.

[56] I. Müller, P. Sanders, A. Lacurie, W. Lehner, and F. Färber. Cache-efficient aggre-
gation: Hashing is sorting. In Proceedings of the 2015 ACM SIGMOD International
Conference on Management of Data, pages 1123–1136, 2015.

[57] F. Nagel, G. Bierman, and S. D. Viglas. Code generation for efficient query

processing in managed runtimes. PVLDB, 7(12):1095–1106, Aug. 2014.

[58] T. Neumann. Efficiently Compiling Efficient Query Plans for Modern Hardware.

PVLDB, 4(9):539–550, 2011.

[59] T. Neumann, V. Leis, and A. Kemper. The complete story of joins (inhyper).

Datenbanksysteme für Business, Technologie und Web (BTW 2017), 2017.

[60] D. Olteanu and M. Schleich. Factorized databases. SIGMOD Rec., 45(2):5–16, Sept.

2016.

[61] S. Palkar, J. J. Thomas, A. Shanbhag, D. Narayanan, H. Pirk, M. Schwarzkopf,
S. Amarasinghe, M. Zaharia, and S. InfoLab. Weld: A common runtime for high
performance data analytics. In Conference on Innovative Data Systems Research
(CIDR), 2017.

[62] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in
python. Journal of machine learning research, 12(Oct):2825–2830, 2011.

[63] H. Pirk, O. Moll, M. Zaharia, and S. Madden. Voodoo - a vector algebra for portable
database performance on modern hardware. Proc. VLDB Endow., 9(14):1707–1718,
oct 2016.

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

[85] P. Wang, D. Wang, and A. Chlipala. Timl: a functional language for practical
complexity analysis with invariants. Proceedings of the ACM on Programming
Languages, 1(OOPSLA):1–26, 2017.

[86] R. C. Whaley, A. Petitet, and J. J. Dongarra. Automated empirical optimizations
of software and the atlas project. Parallel computing, 27(1-2):3–35, 2001.
[87] L. Wong. Kleisli, a functional query system. Journal of Functional Programming,

10(1):19–56, 2000.

[88] C. Yan and A. Cheung. Generating application-specific data layouts for in-
memory databases. Proceedings of the VLDB Endowment, 12(11):1513–1525, 2019.
[89] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel, J. M. Heller-
stein, S. Krishnan, and I. Stoica. Deep unsupervised cardinality estimation. Proc.
VLDB Endow., 13(3):279–292, Nov. 2019.

[90] J. Zhou and K. A. Ross.

Implementing Database Operations Using SIMD In-
structions. In Proceedings of the 2002 ACM SIGMOD International Conference
on Management of Data, SIGMOD’02, pages 145–156, New York, NY, USA, 2002.
ACM.

[64] O. Polychroniou, A. Raghavan, and K. A. Ross. Rethinking SIMD Vectorization
for In-Memory Databases. In Proceedings of the 2015 ACM SIGMOD International
Conference on Management of Data, SIGMOD’15, pages 1493–1508, New York,
NY, USA, 2015. ACM.

[91] M. Zukowski, P. A. Boncz, N. Nes, and S. Héman. MonetDB/X100 - A DBMS In

The CPU Cache. IEEE Data Eng. Bull., 28(2):17–22, 2005.

[65] M. Puschel, J. M. Moura, J. R. Johnson, D. Padua, M. M. Veloso, B. W. Singer,
J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, et al. SPIRAL: code generation
for DSP transforms. Proceedings of the IEEE, 93(2):232–275, 2005.

[66] B. Răducanu, P. Boncz, and M. Zukowski. Micro adaptivity in vectorwise. In
Proceedings of the 2013 ACM SIGMOD International Conference on Management of
Data, pages 1231–1242, 2013.

[67] R. Ramakrishnan and J. Gehrke.

Osborne/McGraw-Hill, 2nd edition, 2000.

Database Management Systems.

[68] S. Richter, V. Alvarez, and J. Dittrich. A seven-dimensional analysis of hashing

methods and its implications on query processing. PVLDB, 9(3):96–107, 2015.

[69] M. A. Roth, H. F. Korth, and A. Silberschatz. Extended algebra and calculus
for nested relational databases. ACM Transactions on Database Systems (TODS),
13(4):389–417, 1988.

[70] M. Schleich, D. Olteanu, M. Abo Khamis, H. Q. Ngo, and X. Nguyen. A layered
aggregate engine for analytics workloads. In Proceedings of the 2019 International
Conference on Management of Data, SIGMOD ’19, pages 1642–1659, New York,
NY, USA, 2019. ACM.

[71] M. Schleich, D. Olteanu, and R. Ciucanu. Learning linear regression models
over factorized joins.
In Proceedings of the 2016 International Conference on
Management of Data, SIGMOD ’16, pages 3–18, New York, NY, USA, 2016. ACM.
[72] A. Shaikhha, M. Dashti, and C. Koch. Push versus Pull-Based Loop Fusion in

Query Engines. Journal of Functional Programming, 28:e10, 2018.

[73] A. Shaikhha, M. Huot, J. Smith, and D. Olteanu. Functional collection program-
ming with semi-ring dictionaries. arXiv preprint arXiv:2103.06376, 2021.
[74] A. Shaikhha, Y. Klonatos, and C. Koch. Building efficient query engines in a
high-level language. ACM Transactions on Database Systems, 43(1):4:1–4:45, Apr.
2018.

[75] A. Shaikhha, Y. Klonatos, L. Parreaux, L. Brown, M. Dashti, and C. Koch. How to
architect a query compiler. In Proceedings of the 2016 International Conference on
Management of Data, SIGMOD’16, pages 1907–1922, New York, NY, USA, 2016.
ACM.

[76] A. Shaikhha, M. Schleich, A. Ghita, and D. Olteanu. Multi-layer optimizations

for end-to-end data analytics. In CGO, page 145–157, 2020.

[77] A. Shaikhha, M. Schleich, and D. Olteanu. An intermediate representation for
hybrid database and machine learning workloads. Proc. VLDB Endow., 14(12):2831–
2834, 2021.

[78] D. G. Spampinato and M. Püschel. A basic linear algebra compiler.

In Pro-
ceedings of Annual IEEE/ACM International Symposium on Code Generation and
Optimization, CGO ’14, pages 23:23–23:32. ACM, 2014.

[79] R. Y. Tahboub, G. M. Essertel, and T. Rompf. How to architect a query compiler,
revisited. In Proceedings of the 2018 International Conference on Management of
Data, pages 307–322, 2018.

[80] P. Trinder. Comprehensions, a Query Notation for DBPLs. In Proc. of the 3rd
DBPL workshop, DBPL3, pages 55–68, San Francisco, CA, USA, 1992. Morgan
Kaufmann Publishers Inc.

[81] I. Trummer and C. Koch. Solving the join ordering problem via mixed integer
linear programming. In Proceedings of the 2017 ACM International Conference on
Management of Data, pages 1025–1040, 2017.

[82] P. Van Sandt, Y. Chronis, and J. M. Patel. Efficiently searching in-memory sorted
arrays: Revenge of the interpolation search? In Proceedings of the 2019 Interna-
tional Conference on Management of Data, SIGMOD ’19, page 36–53, New York,
NY, USA, 2019. Association for Computing Machinery.

[83] S. Viglas, G. M. Bierman, and F. Nagel. Processing Declarative Queries Through
Generating Imperative Code in Managed Runtimes. IEEE Data Eng. Bull., 37(1):12–
21, 2014.

[84] P. Wadler. Comprehending monads. In Proceedings of the 1990 ACM Conference
on LISP and Functional Programming, LFP ’90, pages 61–78, New York, NY, USA,
1990. ACM.

Fine-Tuning Data Structures for Analytical Query Processing

A MICRO BENCHMARKS
We next report on micro-benchmarks for the eight dictionary im-
plementations in the following disciplines: (1) inserts of a varying
number of data points into a dictionary; (2) successful and (3) un-
successful lookups for a varying number of keys into dictionaries
of varying size. The key-value pairs to be inserted, or the keys
to be looked up are integer values generated following a uniform
distribution. The keys can be either ordered or unordered.

Figure 13 shows the results for the case of insertion. For the
case of unordered keys, we observe the superiority of hash-based
dictionaries over sort-based ones. However, for ordered keys, the
sort-based dictionaries perform better than most hash-based ones.
An interesting case is boost_flat_map, which behaves poorly for
the case of unordered keys, due to the linear insertion needed for
bigger keys. Nevertheless, this data structure outperforms others
for ordered keys.

Unordered

Ordered

1
e
n
i
h
c
a
M

2
e
n
i
h
c
a
M

Figure 13: Micro benchmark results for insert in different
dictionary data structures. The key/value pairs to be in-
serted are either ordered based on their keys, or unordered.

As previously mentioned in Section 5, the performance of lookup
operation is different in the case of a successful lookup and un-
successful one. Figure 15 shows the performance of successful
lookup operations. Apart from unordered and ordered data, to
better demonstrate the behavior of different dictionaries, we vary
both the dictionary size and the number of lookup invocations.
Similarly to the insert operation, for unordered data one observes
the superiority of the hash-based dictionaries. For ordered data,
the sort-based dictionaries outperform the hash-based ones, and
the performance gap widens as the number of lookup invocations
increases. One exception is map, for which the inefficient tree traver-
sal leads to worse performance. Furthermore, for a fixed dictionary
size, decreasing the number of lookup invocations may make the
amortized hinted lookup cost more than the non-hinted lookup.
We later observe in Section 6.4 the cases where using non-hinted
lookups is preferred over hinted ones.

Figure 14 shows the results for unsuccessful lookups. Although
for insertion and successful lookup operations both tsl_dict and
robinhood_dict behave similarly, for unsuccessful lookups the
latter dictionary clearly shows a better behavior. For ordered keys,

the performance are similar to successful lookups, thus, omitted
from the experiments.

Unordered / Tuples=1M

Unordered / Size=16K

1
e
n
i
h
c
a
M

2
e
n
i
h
c
a
M

Figure 14: Micro benchmark results for unsuccessful
lookups in different dictionary data structures.

Note that all figures use a logarithmic scale for both axes. For both
insertion and lookup we observe up to three orders of magnitude
performance difference among the different dictionary implementa-
tions. This is interesting because all the implementations are stable
and used massively in software artifacts written using C++.

Finally, we observe that for all operations, the micro-architecture
of the underlying hardware has an impact on the relative perfor-
mance of different dictionary implementations. Here we give three
important examples. First, boost_flat_map performs better than
hash-based dictionaries on insertions for dictionary size of smaller
than 100 elements in machine 2, but not in machine 1. Second, the
successful lookup operator of map for ordered data is faster than all
other dictionaries for dictionary size of smaller than 8000 elements
in machine 2, but not in machine 1. Third, the unsuccessful lookup
operator of robinhood_dict is consistently better than other dic-
tionaries for every dictionary size in machine 1, but not in machine
2.

B REGRESSION MODELS
In this section, we investigate the space of possible regression mod-
els trained using scikit-learn. All of the models are trained based
on all three methods explained in section 6.2: (1) Linear: linear re-
gression. (2) Polynomial: degree-2 polynomial regression. (3) SVM:
linear support-vector machine. (4) KNN: K-nearest neighbor (K =
4). (5) Decision Tree: regression decision tree of depth 5. (6) Ad-
aBoost: AdaBoost [29] with 200 estimators. (7) Gradient Boost:
gradient boosting with 200 estimators. (8) Random Forest: random
forest with 200 estimators.
Figure 16 shows the relation between the predicted and actual
running times for operations such as lookup and insert. Utilizing
32 models (with and without feature engineering) outperforms the
All in One Model training method. Splitting the cost prediction
task among several models and providing them simpler integral
feature values (not one-hot encoded) makes the estimation easier.
Therefore, such performance is expected.

222527210212215217220Number of Tuples102102101101100100101101102102Million Tuples / Secondunordered_maprobinhood_dicttsl_dictboost_unordered_mapmapboost_flat_maptlx_dictabsl_dict222527210212215217220Number of Tuples102102101101100100101101102102Million Tuples / Second222527210212215217220Number of Tuples102102101101100100101101102102Million Tuples / Second222527210212215217220Number of Tuples102102101101100100101101102102Million Tuples / Second222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Secondunordered_maprobinhood_dicttsl_dictboost_unordered_mapmapboost_flat_maptlx_dictabsl_dict25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / Second222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Second25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / SecondUnordered / Tuples=1M

Ordered / Tuples=1M

Unordered / Size=16K

Ordered / Size=16K

Amir Shaikhha, Marios Kelepeshis, and Mahdi Ghorbani

1
e
n
i
h
c
a
M

2
e
n
i
h
c
a
M

Figure 15: Micro benchmark results for successful lookups in different dictionary data structures.

Figure 16: Comparison of the prediction of 8 different regression models trained under various methods with operation run-
ning times.

The KNN model training with logarithmic features performs the
best among all these models. It uses as feature the logarithm of
dictionary size which captures accurately the relationship between
dictionary size and actual operation cost. For the same reason, Poly-
nomial model which trained with similar method behaves better
than Linear and Polynomial models W/o feature engineering.

This may be unsurprising, as the expected cost is logarithmic in
dictionary size for sort-based dictionaries. Yet the constant factors
in the complexity are now captured more accurately by the model.
Among the tree-based models, Gradient Boost outperforms oth-
ers. and trained them using scikit-learn [62]

222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Secondunordered_maprobinhood_dicttsl_dictboost_unordered_mapmapboost_flat_maptlx_dictabsl_dict222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Second25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / Second25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / Second222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Second222527210212215217220Size of Dictionary100100101101102102103103Million Tuples / Second25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / Second25210215220225Number of Tuples103103102102101101100100101101102102103103Million Tuples / Second1e-61e-41e-211e21e41e6All in One ModelW/o Feature EngineeringPolynomialWith Feature EngineeringAll in One ModelW/o Feature EngineeringAdaBoostWith Feature Engineering1e-61e-41e-211e21e41e6SVMLinear1e-61e-41e-211e21e41e6Decision TreeRandom Forest1e-41e-211e21e41e-61e-41e-211e21e41e61e-41e-211e21e41e-41e-211e21e4Gradient Boost1e-41e-211e21e41e-41e-211e21e41e-41e-211e21e4KNN (K=4)Time [ms] (Log10 Scale)Predicted Cost (Log10 Scale)