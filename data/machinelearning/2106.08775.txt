Momentum-inspired Low-Rank Coordinate Descent for
Diagonally Constrained SDPs

Junhyung Lyle Kim, Jose Antonio Lara Benitez, Mohammad Taha Toghani, Cameron Wolfe,
Zhiwei Zhang, Anastasios Kyrillidis
Rice University
Houston, TX, USA
jlylekim,jx24,mt72,crw13,zhiwei,anastasios@rice.edu

1
2
0
2

l
u
J

3

]

C
O
.
h
t
a
m

[

2
v
5
7
7
8
0
.
6
0
1
2
:
v
i
X
r
a

ABSTRACT
We present a novel, practical, and provable approach for solving
diagonally constrained semi-definite programming (SDP) problems
at scale using accelerated non-convex programming. Our algorithm
non-trivially combines acceleration motions from convex optimiza-
tion with coordinate power iteration and matrix factorization tech-
niques. The algorithm is extremely simple to implement, and adds
only a single extra hyperparameter â€“ momentum. We prove that
our method admits local linear convergence in the neighborhood
of the optimum and always converges to a first-order critical point.
Experimentally, we showcase the merits of our method on three
major application domains: MaxCut, MaxSAT, and MIMO signal de-
tection. In all cases, our methodology provides significant speedups
over non-convex and convex SDP solvers â€“ 5Ã— faster than state-of-
the-art non-convex solvers, and 9 to 103Ã— faster than convex SDP
solvers â€“ with comparable or improved solution quality.

CCS CONCEPTS
â€¢ Theory of Computation â†’ Design and Analysis of Algo-
rithms; Theory and Algorithms for Application Domains; â€¢ Mathe-
matics of Computing â†’ Mathematical Software.

KEYWORDS
Semi-Definite Programming, Non-Convex Optimization

1 INTRODUCTION
Background. This work focuses on efficient ways to solve semi-
definite programming instances (SDPs) with diagonal constraints:

min
ğ‘‹ âª°0

âŸ¨ğ¶, ğ‘‹ âŸ©

s.t. ğ‘‹ğ‘–ğ‘– = 1, ğ‘– = 1, . . . , ğ‘›.

(1)

Here, ğ‘‹ âˆˆ Sğ‘› is the ğ‘› Ã— ğ‘› symmetric optimization variable and
ğ¶ âˆˆ Sğ‘› is a problem-dependent cost matrix. The above formulation
usually appears in practice as the convex relaxation of quadratic
form optimization over discrete variables.

min
ğ‘¥ âˆˆDğ‘›

âŸ¨ğ‘¥, ğ¶ğ‘¥âŸ©,

(2)

D is a discrete set on the unit imaginary circle:

D =

(cid:110)
ğ‘’

ğš¤2ğœ‹ğ‘š
ğ‘€

: ğ‘š = 0, 1, . . . , ğ‘€ âˆ’ 1(cid:111)

.

âˆš

where ğš¤ :=
âˆ’1 denotes the imaginary unit. Different values for
ğ‘€ define different realizations of the discrete set D. For example,
ğ‘€ = 2 reduces D to the binary set {Â±1}, while D becomes {Â±1, Â±ğš¤}

Authors before AK are listed in alphabetical order.

for ğ‘€ = 4.1 The formulation in (2) appears in many applications,
including MaxCut or Max-ğ‘˜-Cut [5, 14, 16, 20, 21, 26, 29, 39, 49, 51],
MaxSAT [20], maximal margin classification [19], semi-supervised
learning [58], correlation clustering on dot-product graphs [55],
community detection [1, 25], and (quantized) phase synchroniza-
tion in communication networks [10, 64]. Other problems that
can be cast as (quantized) quadratic form optimization over the
unit complex torus include phase recovery [56], angular synchro-
nization [50], and optimization problems in communication sys-
tems [27, 32, 33, 37, 42].

Typically, solving (2) is computationally expensive due to the
presence of discrete structures. Discrete algorithmic solutions have
been developed to solve (2) with near-optimal performance and
rigorous theoretical guarantees by exploiting the problemâ€™s struc-
ture [24, 30, 38, 41]. However, the go-to techniques for solving (2)
are oftentimes continuous. For example, methods exist that relax
combinatorial entities in (2) and find a solution with non-linear,
continuous optimization. Such techniques can be roughly catego-
rized into convex [17, 23, 44, 45, 53, 54] and non-convex, continuous
approaches [8, 9, 11, 31, 59].

Recently, numerous algorithms have been proposed for solving
large-scale SDP instances of the form (1). [59] present a low-rank
coordinate descent approach to solving MaxCut and MaxSat SDP
instances that is 10-100Ã— faster than state-of-the-art solvers. Simi-
larly, [62] solve MaxCut instances on a laptop with almost 8 million
vertices and 1013 matrix entries, while [22] propose a simulated
bifurcation algorithm that can solve all-to-all, 100, 000-node Max-
Cut problems with continuous weights. Despite such developments,
however, SDPs are still hard problems to solve in practice. This work
aims to contribute along the path of efficient algorithms for solving
large-scale SDPs.

This Paper. We present an algorithmic prototype for solving diago-
nally constrained SDPs at scale. Based on the low ğ‘˜-rank property of
SDPs [2, 6, 47] (see Lemma 2.1), we focus on solving a non-convex,
but equivalent, formulation of (1):
ğ‘“ (ğ‘‰ ) â‰œ (cid:10)ğ¶, ğ‘‰ âŠ¤ğ‘‰ (cid:11)

âˆ¥ğ‘£ğ‘– âˆ¥2 = 1, âˆ€ğ‘–.

s.t.

(3)

min
ğ‘‰ âˆˆRğ‘˜Ã—ğ‘›

Here, ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘› is the optimization variable and the linear con-
straints on ğ‘‹ in (1) translate to ğ‘‰ âˆˆ (ğ‘†ğ‘˜âˆ’1)ğ‘› (i.e., each column vector
ğ‘£ğ‘– satisfies âˆ¥ğ‘£ğ‘– âˆ¥2 = 1). Our algorithm, named Mixing Method++,
adds momentum to a coordinate power iteration-style technique
for solving SDPs by incorporating ideas from accelerated convex

1Many other
ğš¤2ğœ‹
D = {1, ğ‘’
3 , ğ‘’

realizations of
ğš¤4ğœ‹
3 } for Max-3-Cut [21, 51].

the set D exist. For example, we define

 
 
 
 
 
 
optimization and matrix factorization. As its predecessor [59], the
algorithm is simple to implement, requiring only one additional
hyperparameter â€“ momentum. From an empirical perspective, we
test the algorithm on MaxCut, MaxSAT, and MIMO signal detection
applications, where we demonstrate significant speedups in com-
parison to other state-of-the-art solvers (e.g., 9Ã— to 102Ã— speedup on
MaxSAT instances) with comparable or improved solution quality.
Additionally, we prove that our accelerated method admits local
linear convergence in the neighborhood of the optimum and always
converges to a first order stationary point. To the best of our knowl-
edge, this is the first theoretical result that incorporates momentum
into a coordinate-descent approach for solving SDPs.

2 BACKGROUND
Notation. We use Sğ‘› to denote the cone of positive semi-definite
(PSD) matrices. We denote the ğ‘˜-dimensional sphere by ğ‘†ğ‘˜âˆ’1, where
ğ‘˜ âˆ’ 1 indicates the dimension of the manifold. The product of ğ‘›,
ğ‘˜-dimensional spheres is denoted by (ğ‘†ğ‘˜âˆ’1)ğ‘›. For any ğ‘£ âˆˆ Rğ‘› and
ğ‘€ âˆˆ Rğ‘›Ã—ğ‘š, we define âˆ¥ğ‘£ âˆ¥2 and âˆ¥ğ‘€ âˆ¥ğ¹ as the â„“2 and Frobenius norms.
For a matrix ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘›, we denote its ğ‘–-th column interchangeably
as ğ‘‰:,ğ‘– or ğ‘£ğ‘– . For ğ‘ âˆˆ Rğ‘›, we define ğ·ğ‘ as the ğ‘› Ã— ğ‘› diagonal matrix
with entries (ğ·ğ‘)ğ‘–ğ‘– = ğ‘ğ‘– . We denote the vector of all 1â€™s as 1 and the
Hadamard product as âŠ™. Singular values are denoted as ğœ (Â·), while
ğœnnz (Â·) represents minimum non-zero singular value(s). The matrix
ğ¶ corresponds to the cost matrix of the optimization problem (1).

Low-rank Property of SDPs. Focusing on (1), the number of lin-
ear constraints on ğ‘‹ is far less than the number of variables within
ğ‘‹ . This constitutes such SDPs as weakly constrained, which yields
the following result [6, 47].

Lemma 2.1. The SDP in (1) has a solution with rank ğ‘˜ = (cid:6)âˆš
One can enforce low-rank solutions to weakly constrained SDPs
by defining ğ‘‹ = ğ‘‰ âŠ¤ğ‘‰ , where ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘› and ğ‘˜ (ğ‘˜ + 1)/2 > ğ‘› [10, 11].
This low-rank parameterization, shown in (3), yields a non-convex
formulation of (1) with the same global solution. This formula-
tion can be solved efficiently because the conic constraint in (1) is
automatically satisfied.2

2ğ‘› (cid:7).

Mixing Method. Consider again the low-rank SDP parameteriza-
tion given in (3). To connect this formulation with its discrete form
in (2), assume ğ‘˜ = 1, which yields the following expression:

min
ğ‘£ âˆˆR1Ã—ğ‘›

ğ‘£ğ¶ğ‘£ âŠ¤

s.t.

|ğ‘£ğ‘– | = 1, âˆ€ğ‘–,

(4)

Notice that ğ‘£ is normalized entry-wise during each optimization
step. To solve (4), one can consider using related algorithms, such
as a power iteration-style methods [40], as shown below.

ğ‘¢ â† ğ¶ğ‘£,

ğ‘£ + â† normalize(ğ‘¢),

Here, normalize(ğ‘¢) projects ğ‘¢ such that |ğ‘£ +
ğ‘– | = 1 for each entry
of ğ‘£ +. This is the crux of the Mixing Method [59], which applies
a coordinate power iteration routine to sequentially update each
column of ğ‘‰ and solve (3); see Algorithm 1. Variants of Algorithm 1
that utilize random coordinate selection instead of cyclic updates
for the columns of ğ‘‰ have also been explored [15].

2Handling a PSD constraint requires an eigenvalue decomposition of ğ‘› Ã— ğ‘› matrices,
leading to a ğ‘‚ (ğ‘›3) overhead per iteration.

Kim, et al.

(cid:17)

Algorithm 1 Mixing Method [59]
1: Input: ğ¶ âˆˆ Sğ‘›, ğ‘‰ âˆˆ (ğ‘†ğ‘˜âˆ’1)ğ‘›.
2: while not yet converged do
3:

for ğ‘– = 1 to ğ‘› do

4:

ğ‘£ğ‘– â† normalize

5:
end for
6: end while

(cid:16)

âˆ’ (cid:205)ğ‘—â‰ ğ‘– ğ‘ğ‘– ğ‘— ğ‘£ ğ‘—

In (3), diagonal elements of ğ¶ do not contribute to the optimiza-
tion due to the constraint âˆ¥ğ‘£ğ‘– âˆ¥2 = 1, and thus can be set to zero.
When considering only the ğ‘–-th column of ğ‘‰ , the objective (cid:10)ğ¶, ğ‘‰ âŠ¤ğ‘‰ (cid:11)
takes the form 2ğ‘£ âŠ¤
ğ‘—=1 ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— . The full gradient of the objective
ğ‘–
with respect to ğ‘£ğ‘– can be computed in closed form at each iteration,
yielding the coordinate power iteration routine outlined in Algo-
rithm 1. Interestingly, this procedure also comes with theoretical
guarantees [59], which we include below for completeness.3

(cid:205)ğ‘›

Theorem 2.1. The Mixing Method converges linearly to the global
optimum of (3), under the assumption that the initial point is close
enough to the optimal solution.

3 MIXING METHOD++
Towards a Momentum-inspired Update Rule. In this work,
we introduce Mixing Method++, which uses acceleration tech-
niques [43, 48] to significantly improve upon the performance of
Mixing Method. For gradient descent, a classical acceleration tech-
nique is the Heavy-Ball method [48], which iterates as follows:
ğ‘¤ğ‘¡ +1 = ğ‘¤ğ‘¡ âˆ’ ğœ‚âˆ‡ğ‘“ (ğ‘¤ğ‘¡ ) + ğ›½ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡ âˆ’1).
Here, ğ‘¤ is the optimization variable, ğ‘“ (Â·) is a differentiable loss
function, ğœ‚ is the step size, and ğ›½ is the momentum parameter.
Intuitively, the heavy-ball method exploits the history of previous
updates by moving towards the direction ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡ âˆ’1, weighted by
the momentum parameter ğ›½. A similar momentum term can be
naively incorporated into power iteration:

ğ‘¤ğ‘¡ +1 = normalize (cid:0)ğ¶ğ‘¤ğ‘¡ + ğ›½ (ğ¶ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡ )(cid:1) ,
where there is no notion of a step size ğœ‚. By adapting this accelerated
power iteration scheme to the Mixing Method update rule, we arrive
at the following recursion:

Ë†ğ‘£ğ‘– = normalize(ğ‘”ğ‘– + ğ›½ (ğ‘”ğ‘– âˆ’ ğ‘£ğ‘– )),
where ğ‘”ğ‘– = âˆ’ (cid:205)ğ‘—â‰ ğ‘– ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— . Because ğ‘”ğ‘– is not normalized before the
addition of the momentum term, ğ‘”ğ‘– and ğ‘£ğ‘– can be of significantly
different magnitude. As such, we normalize ğ‘”ğ‘– as an intermediate
step and adopt a double-projection approach per iteration; see Al-
gorithm 2. Such double projection ensures that ğ‘¢ğ‘– and ğ‘£ğ‘– are of
comparable magnitude when the momentum term is added, result-
ing in significantly-improved momentum inertia.

Properties of Mixing Method++. The proposed algorithm is com-
prised of a two-step update procedure. The last normalization
projects ğ‘¢ğ‘– + ğ›½ (ğ‘¢ğ‘– âˆ’ ğ‘£ğ‘– ) onto the unit sphere, which ensures the up-
dated matrix is feasible. In contrast, the first normalization ensures

3Most of global guarantees in [59] hold for a variant of the Mixing Method that
resembles coordinate gradient descent with step size, rather than the coordinate power
iteration method that is used and tested in practice.

Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs

Algorithm 2 Mixing Method++
1: Input: ğ¶ âˆˆ Sğ‘›, ğ‘‰ âˆˆ (ğ‘†ğ‘˜âˆ’1)ğ‘›, 0 â‰¤ ğ›½ < 1.
2: while not yet converged do
3:

for ğ‘– = 1 to ğ‘› do

(cid:16)

(cid:17)

ğ‘¢ğ‘– â† normalize
ğ‘£ğ‘– â† normalize (ğ‘¢ğ‘– + ğ›½ (ğ‘¢ğ‘– âˆ’ ğ‘£ğ‘– ))

âˆ’ (cid:205)ğ‘—â‰ ğ‘– ğ‘ğ‘– ğ‘— ğ‘£ ğ‘—

4:

5:

6:
end for
7: end while

that âˆ’ (cid:205)ğ‘›
ğ‘—=1 ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— and ğ‘£ğ‘– are comparable in magnitude, as the former
may have a norm greater than 1. The overall computational cost
of the added momentum step is negligible, making the step-wise
computational complexity of Mixing Method++ roughly equal to
that of the Mixing Method.

We show that Mixing Method++ converges linearly when ğ‘‰ lies
in a neighborhood of the global optimum and always reaches a first-
order stationary point. Despite the similarity of our convergence
rates to previous work [59], achieving acceleration in theory is not
always feasible, even in convex, non-stochastic cases; see [4, 13,
18, 34, 36, 48]. Nonetheless, we observe acceleration in practice,
and Mixing Method++ with ğ›½ = 0.8 yields a robust algorithm with
significant speed-ups in comparison to Mixing Method.

4 CONVERGENCE ANALYSIS
We begin by presenting some notation for brevity and readability.
ğ‘“ â˜… is used to denote the optimal value of the objective function
ğ‘“ (ğ‘‰ ) = âŸ¨ğ¶, ğ‘‰ âŠ¤ğ‘‰ âŸ©. The matrices ğ‘‰ and Ë†ğ‘‰ refer to the current and
next iterates from the inner iteration of Algorithm 2. For each
ğ‘– âˆˆ [ğ‘›], we define the family of maps ğ‘ğ‘– : Rğ‘˜Ã—ğ‘› â†’ Rğ‘˜Ã—ğ‘› as:

Figure 1: The minimum norm of the columns of ğ‘‰ through-
out iterations of Mixing Method and Mixing Method++ on the
MaxCut and MIMO objectives.

trick from [15] in practice. Given this assumption, no further as-
sumptions are required for the second normalization step provided
that 0 â‰¤ ğ›½ < 1, as summarized below.

Observation 4.1. ğ‘¤ğ‘– = âˆ¥ğ‘¢ğ‘– âˆ’ ğ›½ğ‘£ğ‘– âˆ¥2 â‰  0, whenever 0 â‰¤ ğ›½ < 1.

Proof. Based on upper and lower triangle inequality bounds
and the fact that ğ‘¢ğ‘– and ğ‘£ğ‘– are both in ğ‘†ğ‘˜âˆ’1, we get the inequality
â–¡
1 â‰¤ âˆ¥(1 + ğ›½) ğ‘¢ğ‘– âˆ’ ğ›½ğ‘£ğ‘– âˆ¥2 â‰¤ 1 + 2ğ›½.

Our first technical lemma lower bounds the decrease in the

objective over inner iterations of Algorithm 2.

|
Ë†ğ‘£1
ğ‘‰ â†¦âˆ’â†’ (cid:169)
(cid:173)
|
(cid:171)

. . .

|
Ë†ğ‘£ğ‘–âˆ’1
|

|
ğ‘£ğ‘–
|

|
ğ‘£ğ‘–+1
|

. . .

|
ğ‘£ğ‘›
|

.

(cid:170)
(cid:174)
(cid:172)

In words, ğ‘ğ‘– (ğ‘‰ ) is the partially updated ğ‘‰ matrix (i.e., within the
inner loop of Algorithm 2) before updating ğ‘£ğ‘– to Ë†ğ‘£ğ‘– . Thus, only the
vector ğ‘£ğ‘– changes from ğ‘ğ‘– (ğ‘‰ ) to ğ‘ğ‘–+1 (ğ‘‰ ). Notice that ğ‘ğ‘›+1 (ğ‘‰ ) = Ë†ğ‘‰ .
We now present our main theoretical results. All proofs are

provided in Section 6.4 We adopt Assumption 3.1 from [59].

Assumption 4.1. For ğ‘– âˆˆ [ğ‘›], assume âˆ¥ (cid:205)ğ‘›

ğ‘—=1 ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— âˆ¥2 do not de-
generate in the procedure. That is, all norms are always greater than
or equal to a constant value ğ›¿ > 0.

Put simply, Assumption 4.1 states that the update rule will never
yield ğ‘¢ğ‘– with zero norm. We find that this assumption consistently
holds in practice for both Mixing Method and Mixing Method++
applied over several objectives. In particular, we track the smallest
value of âˆ¥ (cid:205)ğ‘—â‰ ğ‘– ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— âˆ¥2 observed during the inner loop of Mixing
Method and Mixing Method++, then plot these minimum norms
throughout all iterations in Figure 1. As can be seen, Assumption 4.1
consistently holds for both MaxCut and MIMO objectives. Further-
more, if Assumption 4.1 does not hold, we can exploit the simple

(5)

Lemma 4.1. Let Ë†ğ‘‰ , ğ‘‰ be the next and previous iteration of Algo-
rithm (2), respectively. We define ğ‘¦ğ‘– = âˆ¥ğ‘”ğ‘– âˆ¥2, where ğ‘”ğ‘– = âˆ’ (cid:205)ğ‘—â‰ ğ‘– ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— .
We then derive the following.

ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ ( Ë†ğ‘‰ ) â‰¥

1 âˆ’ ğ›½
1 + ğ›½

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¦ğ‘– âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2.

Furthermore, ğ‘“ is non-increasing whenever 0 â‰¤ ğ›½ < 1.

Next, we lower bound the magnitude of the update from ğ‘‰ to Ë†ğ‘‰
based on the objective residual, showing that the magnitude of our
update will be large until we approach an optimum.

Lemma 4.2. Under assumption 4.1, there exists positive constants

ğ›¾ and ğœ such that

âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2

ğ¹ â‰¥ (cid:0)ğœ âˆ’ ğ›¾ âˆ¥Î”âˆ¥ğ¹ (cid:1) (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—),

where Î” := (ğ·ğ‘¤ğ·ğ‘¦ + ğ›½ğ·ğ‘¦) âˆ’ (ğ·ğ‘¤âˆ— ğ·ğ‘¦âˆ— + ğ›½ğ·ğ‘¦âˆ— ).
Finally, we upper bound the value of âˆ¥Î”âˆ¥2

2 with respect to the
objective residual, which later enables the definition of a neighbor-
hood for local linear convergence within Theorem 4.1.

Lemma 4.3. Under Assumption 4.1, âˆƒ ğœ > 0 such that

âˆ¥(ğ‘¤ + ğ›½1) âŠ™ ğ‘¦ âˆ’ (ğ‘¤ âˆ— + ğ›½1) âŠ™ ğ‘¦âˆ— âˆ¥2

2 â‰¤ ğœ (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—).

4The proof that Mixing Method++ always converges to a first-order critical point is
provided in Section 7.

Drawing upon these technical lemmas, we show Mixing Method++

converges linearly within a neighborhood of the optimum.

4567010203040505.05.56.06.5IterationsMaxCutMIMOminikvik2MixingMethod++MixingMethodSolver

Sedumi
MoSeK
SDPNAL+
CGAL
SDPLR
Mixing Method
Mixing Method++

# SI

60
74
52
92
95
106
111

MLOR

-4.30
-3.24
-4.33
0.93
-2.89
-2.80
-3.87

Acc. (Ã—)
360.20
348.37
1316.35
49.19
9.14
5.26
1

Table 1: Results on 203 MaxCut instances. SI stands for solved in-
stances; MLOR stands for median(log(objective residual)). Mixing
Method++ solved most instances efficiently with good precision.

Theorem 4.1 (Local linear convergence). Let ğ‘“ â˜… represent
the optimal value of the objective function, and let ğ›¿ be the non-
degenerative lower-bound of Assumption 4.1. Define the neighborhood,

Nâ˜… :=

(cid:26)
ğ‘‰ âˆˆ (ğ‘†ğ‘˜âˆ’1)ğ‘› : ğœ (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜…) â‰¤

(cid:16) ğœ âˆ’ğœ…
ğ›¾

(cid:17)2(cid:27)

,

for some positive constant ğœ… and ğœ , ğœ, and ğ›¾ as defined in Lemmas 4.2
and 4.3. If ğ‘‰ âˆˆ Nâ˜…, we then have:

ğ‘“ ( Ë†ğ‘‰ ) âˆ’ ğ‘“ â˜… â‰¤ (1 âˆ’ ğœŒ) (cid:0)ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜…(cid:1) ,
where ğœŒ âˆˆ (0, 1]. This result shows that Algorithm (2) converges
linearly when ğ‘‰ âˆˆ Nâ˜….

5 EXPERIMENTAL RESULTS
We test Mixing Method++ on three well-known applications of (1):
MaxCut, MaxSAT, and MIMO signal detection. We use the same
formulation in [59] to solve MaxCut and MaxSAT, while for MIMO
we follow the experimental setup in [35]. We implement Mixing
Method++ in C with ğ›½ = 0.8.5 Each experiment is run on a single
core in a homogeneous Linux cluster with 2.63 GHz CPU and 16
GB of RAM. We compare Mixing Method++ with wide variety of
solvers depending on the application. For MaxCut, we compare
with CGAL [63], Sedumi [52], SDPNAL+ [61], SDPLR [11], MoSeK
[3], and Mixing Method [59]. For MaxSAT, we compare with Mixing
Method and Loandra [7]. Lastly, for MIMO signal detection, we
compare with Mixing Method.

We often report performance in terms of the objective residual,
which is defined as the difference between objective values given by
each solver and the lowest objective value achieved by any solver.
More formally, let Objğ‘– ğ‘— be the value of the objective function given
by optimizer ğ‘– on SDP instance ğ‘—. Then, the objective residual of
optimizer ğ‘– on instance ğ‘— is defined as |Objğ‘– ğ‘— âˆ’ min
Objğ‘–â€² ğ‘— |, where
ğ‘–â€² âˆˆğ‘‚
ğ‘‚ is the set of all solvers used on problem instance ğ‘—.

5.1 MaxCut
The SDP relaxation of the MaxCut problem is given by

max
âˆ¥ğ‘£ğ‘– âˆ¥=1,

1
2

âˆ‘ï¸

ğ‘ğ‘– ğ‘—

(cid:16) 1âˆ’ğ‘£âŠ¤
ğ‘– ğ‘£ğ‘—
2

(cid:17)

,

ğ‘– ğ‘—

Kim, et al.

Figure 2: G40: ğ‘› = 2000, number of edges = 11766. Mixing
Method++ converges within 1s while Mixing Method uses
about 4s. MoSeK and Sedumi need 102 and 103 seconds re-
spectively to converge.

Figure 3: uk: ğ‘› = 4824, number of edges= 6837. Consider the
CPU time when each solver reaches objective residual 10âˆ’2.
Mixing Method++ uses only 20s while Mixing Method needs
200s. MoSeK and Sedumi consume more than 103 and 104 sec-
onds respectively.

where ğ¶ is the adjacency matrix. Thus, we aim to minimize the
following objective function:

ğ‘“MaxCut =

(cid:68)
ğ¶, 1ğ‘›Ã—ğ‘›âˆ’ğ‘‰ âŠ¤ğ‘‰

4

(cid:69) s.t. âˆ¥ğ‘£ğ‘– âˆ¥2 = 1 âˆ€ğ‘–,

where ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘› and 1ğ‘›Ã—ğ‘› is the ğ‘› Ã— ğ‘› matrix of all 1s. We consider
the following benchmarks within our MaxCut experiments.
â€¢ Gset: 67 binary-valued matrices, produced by an autonomous,
random graph generator. The dimension ğ‘› varies from 800 to 104.

â€¢ Dimacs10: 152 symmetric matrices with ğ‘› varying from 39 to
50, 912, 018 , chosen for the 10-th Dimacs Implementation Chal-
lenge. We consider the 136 instances with dimension ğ‘› â‰¤ 8 Â· 106.

Table 1 lists the number of solved instances, median logarithm of
objective residual, and the pairwise acceleration ratio for different
SDP solvers tested on MaxCut.6 In comparison to the other solvers,
Mixing Method++ solves the most MaxCut instances (i.e., 111 of 203

5We extensively tested our theoretical bound on ğ›½ in experiments. After testing all
values in the set [0, 1) with an interval of 0.01, we observe that ğ›½ = 0.8 indeed yields
the best performance on over 90% of instances.

6When an instance is solved by both Mixing Method++ and solver ğ‘  within the 24
hour time limit, the pairwise acceleration ratio is defined as the CPU time taken by
solver ğ‘  divided by that of Mixing Method++.

101100101102103CPU Time (sec)106104102100102104Objective ResidualMixing MethodMixing Method++MoSeKSedumi101100101102103104CPU Time (sec)106104102100102104Objective ResidualMixing MethodMixing Method++MoSeKSedumiMomentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs

Methods

Avg. Approx. Ratio

Loandra
Mixing Method
Mixing Method++

0.945
0.975
0.977

Table 2: Results on MaxSAT. Mixing Method++ achieves the
best approximation ratio with a time limit of 300s. 850 prob-
lems are tested in total. Problems are derived from crafted
(331), random (454), and industrial domains (65).

total instances) within the imposed 24 hour time limit and provides a
5Ã— to 1316Ã— speedup. Moreover, the efficiency of Mixing Method++
does not harm its precision, which is evident in its median logarithm
objective residual of âˆ’3.87. Such performance is preceded only by
Sedumi (âˆ’4.30) and SDPNAL+ (âˆ’4.33), both of which are more
than 300Ã— slower than Mixing Method++.

We also study two specific cases of MaxCut: G40 from Gset and
uk from Dimacs10. In Figures 2 and 3, we plot objective residual
with respect to CPU time of Mixing Method++ and other SDP
solvers on the G40 and uk instances, respectively.7 In both MaxCut
instances, Mixing Method++ converges significantly faster than
other solvers that were considered.

5.2 MaxSAT
Let ğ‘ ğ‘– ğ‘— âˆˆ {Â±1, 0} be the sign of variable ğ‘– in clause ğ‘—. The following
problem provides an upper bound to the exact MaxSAT solution:

max
âˆ¥ğ‘£ğ‘– âˆ¥=1,

ğ‘š
âˆ‘ï¸

ğ‘—=1

(cid:16)1 âˆ’

âˆ¥ğ‘‰ ğ‘  ğ‘— âˆ¥2âˆ’( |ğ‘  ğ‘— |âˆ’1) 2
4 |ğ‘  ğ‘— |

(cid:17)

,

where ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘› and ğ¶ = (cid:205)ğ‘š
ğ‘—=1

where ğ‘  ğ‘— = [ğ‘ 1ğ‘— , ..., ğ‘ ğ‘› ğ‘— ]âŠ¤. Therefore, the MaxSAT SDP relaxation
can then be solved by minimizing the following, related objective:
ğ‘“MaxSAT = (cid:10)ğ¶, ğ‘‰ âŠ¤ğ‘‰ (cid:11) s.t. âˆ¥ğ‘£ğ‘– âˆ¥2 = 1 âˆ€ğ‘–,
ğ‘  ğ‘— ğ‘ âŠ¤
ğ‘—
4 |ğ‘  ğ‘— |
We consider 850 problem instances from the 2016-2019 MaxSAT
competition. Each instance is categorized as random, crafted or
industrial. We evaluate Mixing Method++ as a partial MaxSAT
solver8 and compare its performance to that of the Mixing Method
and Loandra; the latter was the best partial solver in the 2019
MaxSAT competition. The best solution given by all solvers is used
as ground truth when the optimal solution is not known.

.

The average approximation ratio of each solver across all MaxSAT
instances is shown in Table 2. Mixing Method++ achieves the best
average approximation ratio. We also consider two specific MaxSat
instances: s3v90c800-6 from the random track and from the indus-
trial track wb.4m8s4.dimacs.filtered. In Figures 4 and 5, we plot
both the objective residual and the cost (i.e., the number of un-
satisfied clauses) with respect to CPU time for s3v90c800-6 and
wb.4m8s4.dimacs.filtered instances, respectively. Mixing Method++

7We do not include CGAL and SDPLR in the plots, because the intermediate solution
given by those methods is infeasible, which makes intermediate objective values
difficult to quantify.
8A partial solver is only required to provide the assignment it finds with the least
number of violated clauses, while a complete solver must also provide proof that such
an assignment is optimal.

Figure 4: s3v90c800-6.cnf (ğ‘› = 90, ğ‘š = 800). Mixing Method++
not only converges faster than Mixing Method, but provides
better solution (30) compared to Mixing Method (31) and
Loandra (33). The optimal solution has cost 26.

Figure 5: wb_4m8s4_dimacs_filtered.cnf (ğ‘› = 463, 080, ğ‘š =
1, 759, 150). Loandra reaches optimal (230) in 1 sec, while Mix-
ing Method and Mixing Method++ return a solution with cost
28, 306 and 25, 210 respectively in 1 hour.

converges faster than Mixing Method in both cases and returns the
best solution on the s3v90c800-6 instance. Although Loandra pro-
vides the best solution to wb.4m8s4.dimacs.filtered quickly, Mixing
Method++ still outperforms Mixing Method significantly.

5.3 MIMO
The MIMO signal detection setting is defined as follows:

arg min
ğ‘¥ âˆˆS

âˆ¥ğ‘¦ âˆ’ ğ»ğ‘¥ âˆ¥2
2,

where ğ‘¦ âˆˆ Rğ‘š is the signal vector, ğ» âˆˆ Rğ‘šÃ—ğ‘› is the channel
matrix, and ğ‘¥ âˆˆ S is the transmitted signal. It is assumed that ğ‘¦ =
ğ»ğ‘¥ + ğ‘£, where ğ‘£ is a noise vector. For our experiments, we consider
the constellation {Â±1}, which yields S = {Â±1}ğ‘›. The MIMO SDP
relaxation can then be solved by minimizing the following objective.
ğ‘“MIMO = âŸ¨ğ¶, ğ‘‰ ğ‘‡ ğ‘‰ âŸ© s.t. âˆ¥ğ‘£ğ‘– âˆ¥2 = 1 âˆ€ğ‘–,

102100105104103102101100101Objective Residual10210034567Cost of Solution1e1CPU Time (sec)Mixing Method++Mixing MethodLoandraOptimal102103101102103104105Objective Residual1001020.00.51.01.5Cost of Solution1e5CPU Time (sec)Mixing Method++Mixing MethodLoandraOptimalKim, et al.

Figure 6: Convergence comparison of Mixing Method++ and
Mixing Method on MIMO signal detection problems with
different SNRs. The size of the channel matrix is listed above
each subplot.

Figure 7: Convergence comparison of Mixing Method++ (blue
line) with ğ›½ = 0.8 and Mixing Method (orange line) on 500-
node MaxCut problem instances with different sparsity lev-
els.

.

(cid:21)

âˆ’ğ»ğ‘‡ ğ‘¦
ğ‘¦ğ‘‡ ğ‘¦

where ğ‘‰ âˆˆ Rğ‘˜Ã—(ğ‘›+1) and ğ¶ =

(cid:20) ğ»ğ‘‡ ğ»
âˆ’ğ‘¦ğ‘‡ ğ»
We compare Mixing Method++ to Mixing Method on several
simulated instances of the MIMO SDP objective. ğ» is sampled
from a standard normal distribution, ğ‘¥ is sampled from {Â±1}, and
ğ‘£ is sampled from a centered normal distribution with variance
ğ‘£ . Experiments are performed for numerous settings of ğœ2
ğœ2
ğ‘£ de-
termined by the signal-to-noise ratio (SNR) (i.e., ğœ2
SNR ). We
perform tests with SNR âˆˆ {8, 16} and problem sizes (ğ‘š, ğ‘›) âˆˆ
{(16, 16), (32, 32), (64, 32)}. We plot the objective residual achieved
by both Mixing Method and Mixing Method++ on MIMO instances
with respect to CPU time in Figure 6. As can be seen, Mixing
Method++ matches or exceeds the performance of the Mixing Method
in all experimental settings.

ğ‘£ = ğ‘šğ‘›

5.4 The Effect of Sparsity
We empirically observe that Mixing Method++ achieves different
levels of acceleration based on the sparsity of ğ¶.9 To further un-
derstand the impact of sparsity on the performance of Mixing
Method++, we compare Mixing Method++ to Mixing Method for
objectives with different sparsity levels. In particular, we construct
a MaxCut objective for a graph with 500 nodes and consider adja-
cency matrices with three different sparsity levels: 0.8, 0.5, and 0.2.
In Figure 7, we plot the objective residual of both Mixing Method
and Mixing Method++ on these problem instances. As can be seen,
the acceleration achieved by Mixing Method++ is more significant
when ğ¶ is sparse. Furthermore, decreasing the value of ğ›½ improves
the performance of Mixing Method++ on MaxCut instances when
ğ¶ is dense.

5.5 Adaptive Momentum
We observe that Mixing Method sometimes decreases the objective
residual faster than Mixing Method++ during early stage iterations.
Inspired by this observation and previous work on adaptive mo-
mentum schedules [12, 46, 57], we explore whether dynamically
adjusting the value of ğ›½ between iterations can further improve
the performance of Mixing Method++. We propose the following

9We define sparsity as the number of zero-valued elements divided by the total number
of elements.

momentum schedule:

ğ›½ğ‘¡ = ğ›½ (1 âˆ’ exp(âˆ’ğ›¼ğ‘¡/ğ‘‡ )) ,
where ğ›½ = 0.8 is the default momentum, ğ‘¡ is the current iteration,
ğ‘‡ is the total number of iterations, and ğ›¼ is a constant such that
exp(âˆ’ğ›¼) â‰ˆ 0. Intuitively, this schedule begins with a low momen-
tum value, which quickly increases to ğ›½ = 0.8, thus forming a
warm-up schedule for the momentum. As shown in Figure 8, this
schedule improves the convergence speed of Mixing Method++.

Figure 8: Comparison of Mixing Method (orange line),
Mixing Method++ with fixed momentum ğ›½ = 0.8 (blue line),
Mixing Method++ with an increasing Scheduled Momentum
from 0 to 0.8 (green line) in terms of the objective cost per
iteration.

6 LOCAL LINEAR CONVERGENCE
We now give detailed proof of Theorem 4.1. Let ğ‘”ğ‘– be the linear
combination in step 4 of Algorithm 1, before projecting onto the
unit sphere:

ğ‘”ğ‘– =

âˆ‘ï¸

ğ‘— <ğ‘–

ğ‘ğ‘– ğ‘— Ë†ğ‘£ ğ‘— +

ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— .

âˆ‘ï¸

ğ‘— >ğ‘–

We also define ğ‘¦ğ‘– = âˆ¥ğ‘”ğ‘– âˆ¥2 and ğ‘“ (ğ‘‰ ) â‰œ âŸ¨ğ¶, ğ‘‰ ğ‘‡ ğ‘‰ âŸ© as the objective
value for some viable solution ğ‘‰ . Therefore, step 4 of Algorithm 1
can be expressed as: ğ‘¦ğ‘–ğ‘¢ğ‘– = âˆ’ğ‘”ğ‘– . Thus, the changes presented in
the Algorithm 2 can be written as the combination of 4 and the
momentum, in step 5:
ğ‘”ğ‘–
ğ‘¦ğ‘–

(1 + ğ›½) ğ‘¢ğ‘– âˆ’ ğ›½ğ‘£ğ‘–
ğ‘¤ğ‘–

ğ‘¢ğ‘– = âˆ’

Ë†ğ‘£ğ‘– =

and

,

,

0.000.020.040.061041021001020.000.050.100.151041021001020.000.050.10103100103CPU Time (sec)Objective Residual16 x 1632 x 3264 x 32MM++, SNR = 16MM++, SNR = 32MM, SNR = 16MM, SNR = 3205010015020010310210110010110210380%05010015020010310210110010110210310450%05010015020010310210110010110210310420%IterationsObjective ResidualMixing Method++Mixing Method01020304050607080Iteration103101101103Objective ResidualMixing MethodMixing Method++Mixing Method++ W/ Adaptive MomentumMomentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs

where ğ‘¤ğ‘– = âˆ¥(1 + ğ›½) ğ‘¢ğ‘– âˆ’ ğ›½ğ‘£ğ‘– âˆ¥2. This leads to the following one line
relation,

ğ‘¤ğ‘–ğ‘¦ğ‘– Ë†ğ‘£ğ‘– = âˆ’ (1 + ğ›½) ğ‘”ğ‘– âˆ’ ğ›½ğ‘¦ğ‘–ğ‘£ğ‘–, âˆ€ğ‘– âˆˆ [ğ‘›].
(6)
Herein, the matrices ğ‘‰ and Ë†ğ‘‰ refer to the current and the end
state of the inner iteration in algorithm 2. For each ğ‘– âˆˆ [ğ‘›], we
define the family of maps ğ‘ğ‘– : (ğ‘†ğ‘˜âˆ’1)ğ‘› â†’ (ğ‘†ğ‘˜âˆ’1)ğ‘› as follows
(cid:40) Ë†ğ‘£ ğ‘— ,
ğ‘£ ğ‘— ,
where {ğ‘’ ğ‘— } is the canonical basis in Rğ‘›. It is trivial to see that ğ‘ğ‘– (ğ‘‰ )
corresponds to the matrix in the inner cycle before updating ğ‘£ğ‘– to Ë†ğ‘£ğ‘– .
Thus, only the vector ğ‘£ğ‘– has been changed from ğ‘ğ‘– (ğ‘‰ ) to ğ‘ğ‘–+1 (ğ‘‰ ).
Also notice that ğ‘ğ‘›+1 (ğ‘‰ ) corresponds to the end of the inner cycle,
i.e., ğ‘ğ‘›+1 (ğ‘‰ ) = Ë†ğ‘‰ .

if ğ‘— < ğ‘–
if ğ‘— â‰¥ ğ‘–,

ğ‘ğ‘– (ğ‘‰ ) ğ‘’ ğ‘— =

(7)

Matrix Representation
The transition from the state ğ‘£ğ‘– to Ë†ğ‘£ğ‘– is expressed in the relation (6),
then for any ğ‘– âˆˆ [ğ‘›] in each inner cycle we have,

(1 + ğ›½)

âˆ‘ï¸

ğ‘— <ğ‘–

ğ‘ğ‘– ğ‘— Ë†ğ‘£ ğ‘— + ğ‘¤ğ‘–ğ‘¦ğ‘– Ë†ğ‘£ğ‘– = âˆ’ (1 + ğ›½)

âˆ‘ï¸

ğ‘— >ğ‘–

ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— âˆ’ ğ›½ğ‘¦ğ‘–ğ‘£ğ‘– .

(8)

Let ğ¿ be the strictly lower triangular matrix of ğ¶ so (8) leads us to
the representation:

(cid:0)(1 + ğ›½) ğ¿ + ğ·ğ‘¤ğ·ğ‘¦ (cid:1) Ë†ğ‘‰ âŠ¤ = âˆ’ (cid:0)(1 + ğ›½) ğ¿âŠ¤ + ğ›½ğ·ğ‘¦ (cid:1) ğ‘‰ âŠ¤.

(9)

Proof of Lemma 4.1
Let ğ›½ be in [0, 1]. When fixing all the others variables, ğ‘“ with respect
to ğ‘£ğ‘– is given by

âŸ¨ğ¶, ğ‘‰ âŠ¤ğ‘‰ âŸ© = 2ğ‘£ âŠ¤
ğ‘–

(cid:0) âˆ‘ï¸
ğ‘—

ğ‘ğ‘– ğ‘— ğ‘£ ğ‘— (cid:1) + constant.

Since only ğ‘£ğ‘– is changed, during the ğ‘–th coordinate-wise updating,
the only part of the objective that will change is 2ğ‘£ âŠ¤

ğ‘– ğ‘”ğ‘– . That is,

Proof. Since ğ‘£ âˆ—
ğ‘– + ğ›½)ğ‘£ âˆ—

(ğ‘¤ âˆ—
are in the unit sphere.

ğ‘– . Taking the norm, ğ‘¤ âˆ—

ğ‘– is a fixed point, using this fact in Eq. (6) yields
ğ‘– and ğ‘£ âˆ—
ğ‘–
â–¡

ğ‘– + ğ›½ = 1 + ğ›½, since ğ‘¢âˆ—

ğ‘– = ğ‘¢âˆ—

Let ğ‘† be defined as (1 + ğ›½) ğ¶+ğ·ğ‘¤ğ·ğ‘¦+ğ›½ğ·ğ‘¦ and let ğ‘†âˆ— be (1 + ğ›½) ğ¶+
ğ·ğ‘¤âˆ— ğ·ğ‘¦âˆ— + ğ›½ğ·ğ‘¦âˆ— ; namely, the corresponding value of ğ‘† when ğ‘‰ is
optimal in the optimization (3). We have the following observation.

Observation 6.2. ğ‘†âˆ— is PSD.

Proof. By observation 6.1 we have ğ‘¤ âˆ—

ğ‘– = 1, so
ğ‘†âˆ— = (1 + ğ›½) ğ¶ + ğ·ğ‘¤âˆ— ğ·ğ‘¦âˆ— + ğ›½ğ·ğ‘¦âˆ— = (1 + ğ›½)

(cid:16)
ğ¶ + ğ·âˆ—
ğ‘¦

(cid:17)

,

where according to lemma 3.12, and since ğ‘‰ is optimal, it follows
that ğ‘† âˆ—
â–¡

1+ğ›½ âª° 0, hence ğ‘†âˆ— âª° 0.

Proof of Lemma 4.2
By (9), we have

Ë†ğ‘‰ = âˆ’ğ‘‰ (cid:0)(1 + ğ›½) ğ¿ + ğ›½ğ·ğ‘¦ (cid:1) (cid:0)(1 + ğ›½) ğ¿âŠ¤ + ğ·ğ‘¤ğ·ğ‘¦ (cid:1)âˆ’1 .

This leads to

ğ‘‰ âˆ’ Ë†ğ‘‰ =ğ‘‰ ((1 + ğ›½) ğ¶ + ğ·ğ‘¤ğ·ğ‘¦ + ğ›½ğ·ğ‘¦)

((1 + ğ›½) ğ¿âŠ¤ + ğ·ğ‘¤ğ·ğ‘¦)âˆ’1
=ğ‘‰ ğ‘† ((1 + ğ›½) ğ¿âŠ¤ + ğ·ğ‘¤ğ·ğ‘¦)âˆ’1
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

,

(cid:123)(cid:122)
ğ‘…

Using the definition Î” â‰œ (ğ·ğ‘¤ğ·ğ‘¦ + ğ›½ğ·ğ‘¦) âˆ’ (ğ·ğ‘¤âˆ— ğ·ğ‘¦âˆ— + ğ›½ğ·ğ‘¦âˆ— ),

and the fact that ğ‘† = ğ‘†âˆ— + Î” in the previous equation, we get

âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2

ğ¹ â‰¥ âˆ¥ğ‘‰ ğ‘†âˆ—ğ‘…âˆ¥2

ğ¹ + 2tr(cid:0)ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—ğ‘…ğ‘…âŠ¤Î”(cid:1).

(10)

We claim that

âˆ¥(ğ‘¤ + ğ›½1) âŠ™ ğ‘¦ âˆ’ (ğ‘¤ âˆ— + ğ›½1) âŠ™ ğ‘¦âˆ— âˆ¥2ğ¼ğ‘› + Î” âª° 0.

To see this, first notice ğ·ğ‘¤ğ·ğ‘¦ + ğ›½ğ·ğ‘¦ = ğ· (ğ‘¤+ğ›½1) âŠ™ğ‘¦. Thus,

Î” = ğ· (ğ‘¤+ğ›½1) âŠ™ğ‘¦ âˆ’ ğ· (ğ‘¤âˆ—+ğ›½1) âŠ™ğ‘¦âˆ— .

ğ‘“ (ğ‘ğ‘– (ğ‘‰ )) âˆ’ ğ‘“ (ğ‘ğ‘–+1 (ğ‘‰ )) = 2ğ‘”âŠ¤

ğ‘– (ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– ) .

Besides,

The updating rule of algorithm 2 is given by the recursion (6),

so using this into the above equation yields

ğ‘“ (ğ‘ğ‘– (ğ‘‰ )) âˆ’ ğ‘“ (ğ‘ğ‘–+1 (ğ‘‰ )) =

=

2ğ‘¦ğ‘– (ğ‘¤ğ‘– âˆ’ ğ›½)
1 + ğ›½
ğ‘¦ğ‘– (ğ‘¤ğ‘– âˆ’ ğ›½)
1 + ğ›½

(1 âˆ’ ğ‘£ âŠ¤

ğ‘– Ë†ğ‘£ğ‘– ),

âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2.

From observation 4.1, we have ğ‘¤ğ‘– âˆ’ ğ›½ â‰¥ 1 âˆ’ ğ›½. Thus,

ğ‘“ (ğ‘ğ‘– (ğ‘‰ )) âˆ’ ğ‘“ (ğ‘ğ‘–+1 (ğ‘‰ )) â‰¥ ğ‘¦ğ‘–

1 âˆ’ ğ›½
1 + ğ›½

âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2.

Adding over all the ğ‘–, leads us to the desired conclusion

ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ ( Ë†ğ‘‰ ) =

ğ‘›
âˆ‘ï¸

ğ‘“ (ğ‘ğ‘– (ğ‘‰ )) âˆ’ ğ‘“ (ğ‘ğ‘–+1 (ğ‘‰ ))

ğ‘–=1
1 âˆ’ ğ›½
1 + ğ›½

â‰¥

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¦ğ‘– âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2.

The following observation will be useful later on.

Observation 6.1. For all ğ‘– âˆˆ [ğ‘›], ğ‘¤ âˆ—

ğ‘– = 1

âˆ¥(ğ‘¤ + ğ›½1) âŠ™ ğ‘¦ âˆ’ (ğ‘¤ âˆ— + ğ›½1) âŠ™ ğ‘¦âˆ— âˆ¥2 = âˆ¥Î”âˆ¥ğ¹ .

So the claim follows since ğ›½ â‰¥ 0 and termwise
âˆ¥Î”âˆ¥ğ¹ + Î”ğ‘– â‰¥ |Î”ğ‘– | + Î”ğ‘– â‰¥ 0.

Now, for the first part in (10), we get
âˆ¥ğ‘‰ ğ‘†âˆ—ğ‘…âˆ¥2

ğ¹ = tr( ğ‘†âˆ—âŠ¤
(cid:124)(cid:123)(cid:122)(cid:125)
âª°0

ğ‘‰ âŠ¤ğ‘‰
(cid:124)(cid:123)(cid:122)(cid:125)
âª°0

ğ‘†âˆ—
(cid:124)(cid:123)(cid:122)(cid:125)
âª°0

ğ‘…ğ‘…âŠ¤
)
(cid:124)(cid:123)(cid:122)(cid:125)
âª°0

â‰¥ ğœ2

min (ğ‘…)ğœnnz (ğ‘†âˆ—)tr(ğ‘‰ ğ‘‡ ğ‘‰ ğ‘†âˆ—).

For the second in (10), we get

tr(cid:0)ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—ğ‘…ğ‘…âŠ¤Î”(cid:1) â‰¥ âˆ’âˆ¥Î”âˆ¥ğ¹ Tr (cid:0)ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—ğ‘…ğ‘…âŠ¤(cid:1)

â‰¥ âˆ’âˆ¥Î”âˆ¥ğ¹ ğœ2
this last inequality is given by our previous claim, the fact that
ğ‘†âˆ— âª° 0 (observation 6.2) and

max (ğ‘…)tr(cid:0)ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—(cid:1),

) = Tr(ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—ğ·2)
Tr(ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—ğ‘…ğ‘…âŠ¤) = Tr(ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ— ğ‘ˆ ğ·2ğ‘ˆ âŠ¤
(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ğ‘…ğ‘…âŠ¤

â‰¤ ğœ2

max (ğ‘…) Tr(ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—).

Using the lower bounds in (10) gives us

âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2

ğ¹ â‰¥ (cid:0)ğœ2

min (ğ‘…)ğœnnz (ğ‘†âˆ—) âˆ’ 2âˆ¥Î”âˆ¥ğ¹ ğœ2

max (ğ‘…)(cid:1)tr(cid:0)ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—(cid:1).

To conclude, the term
Tr(ğ‘‰ âŠ¤ğ‘‰ ğ‘†âˆ—) = Tr(ğ‘‰ âŠ¤ğ‘‰ğ¶) âˆ’ Tr(âˆ’ğ‘‰ âŠ¤ğ‘‰ ğ·ğ‘¦âˆ— ) = ğ‘“ (ğ‘‰ ) âˆ’ Tr(âˆ’ğ‘‰ âŠ¤ğ‘‰ ğ·ğ‘¦âˆ— )

ğ‘– = ğ‘“ (ğ‘‰ ) + âŸ¨1, ğ‘¦âˆ—âŸ©
Â·ğ‘¦âˆ—

ğ‘–)
= ğ‘“ (ğ‘‰ ) +

âˆ‘ï¸

ğ‘–

âŸ¨ğ‘£ğ‘–, ğ‘£ğ‘– âŸ©
(cid:32)
(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
=1

ğ‘–ğ‘–)
â‰¥ ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—.

where ğ‘–) follows from the fact that ğ·ğ‘¦âˆ— is diagonal. To understand
ğ‘–ğ‘–), consider the dual to the diagonally constrained SDP problem (1)
(i.e., see Lemma 3.12 of [59] for more details):

max
ğ‘¦

âˆ’âŸ¨1, ğ‘¦âŸ©, such that ğ¶ + ğ·ğ‘¦ âª° 0

(11)

We know that ğ¶ + ğ·ğ‘¦âˆ— = ğ‘† âˆ—
1+ğ›½ âª° 0 from Observation 6.2. Therefore,
ğ‘¦âˆ— is a feasible solution to the dual problem (11), revealing that
ğ‘“ âˆ— â‰¥ âˆ’âŸ¨1, ğ‘¦âˆ—âŸ©. This fact allows us to derive the inequality given by
ğ‘–ğ‘–). Then, if we define ğœ = ğœ2
max (ğ‘…) with
ğœmin (ğ‘…) = 1/(ğ‘¤ âŠ™ ğ‘¦)max and ğœmax (ğ‘…) = 1/(ğ‘¤ âŠ™ ğ‘¦)min, we arrive at
the final result:

min (ğ‘…)ğœğ‘›ğ‘›ğ‘§ (ğ‘†âˆ—) and ğ›¾ = 2ğœ2

âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2

ğ¹ â‰¥ (ğœ âˆ’ ğ›¾ âˆ¥Î”âˆ¥ğ¹ ) (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜…)

It should be noted that both ğœ and ğ›¾ are strictly positive constants
because ğ‘… is defined as an invertible matrix and, therefore, has all
non-zero eigenvalues.

Proof of Lemma 4.3
Recall that Î”ğ‘– = (ğ‘¤ğ‘– + ğ›½)ğ‘¦ğ‘– âˆ’ (ğ‘¤â˜…
of Î”ğ‘– Ë†ğ‘£ğ‘– into the following expression:

ğ‘– + ğ›½)ğ‘¦â˜…

ğ‘– . We can unroll the value

ğ‘– )(cid:1) Ë†ğ‘£ğ‘–
Then, we derive the following equality from (6):

ğ‘– ) + ğ›½ (ğ‘¦ğ‘– âˆ’ ğ‘¦âˆ—

(cid:0)(ğ‘¤ğ‘–ğ‘¦ğ‘– âˆ’ ğ‘¤ âˆ—

ğ‘– ğ‘¦âˆ—

ğ‘¤ğ‘–ğ‘¦ğ‘– Ë†ğ‘£ğ‘– = âˆ’ (1 + ğ›½) ğ‘”ğ‘– âˆ’ ğ›½ğ‘¦ğ‘–ğ‘£ğ‘–

= âˆ’ (1 + ğ›½) ğ‘ğ‘–ğ‘ğ‘– âˆ’ ğ›½ğ‘¦ğ‘–ğ‘£ğ‘–

(12)

(13)

ğ‘– = (1 + ğ›½) ğ‘ğ‘– + ğ‘¤ âˆ—

where we denote ğ‘ğ‘– = ğ‘ğ‘– (ğ‘‰ ) for brevity and ğ‘ğ‘– as the ğ‘–th column
of ğ¶. We denote ğ‘ âˆ—
ğ‘– ğ‘’ğ‘– . By adding (12) to
both sides of (13), we arrive at the following, expanded version of
the update rule in (6).
ğ‘– ğ‘¦âˆ—

ğ‘– ğ‘’ğ‘– + ğ›½ğ‘¦âˆ—

ğ‘– ğ‘¦âˆ—

ğ‘¤ğ‘–ğ‘¦ğ‘– Ë†ğ‘£ğ‘– âˆ’ ğ‘¤ âˆ—
ğ‘– Ë†ğ‘£ğ‘– + ğ›½ğ‘¦ğ‘– Ë†ğ‘£ğ‘– âˆ’ ğ›½ğ‘¦âˆ—
ğ‘– Ë†ğ‘£ğ‘–
ğ‘– ğ‘¦âˆ—
= âˆ’ (1 + ğ›½) ğ‘ğ‘–ğ‘ğ‘– âˆ’ ğ›½ğ‘¦ğ‘–ğ‘£ğ‘– âˆ’ ğ‘¤ âˆ—
ğ‘–)
ğ‘– )(cid:1) (ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– ).
= âˆ’ğ‘ğ‘–ğ‘ âˆ—

ğ‘– âˆ’ ğ›½ (ğ‘¦ğ‘– âˆ’ ğ‘¦âˆ—

ğ‘– ğ‘¦âˆ—

ğ‘– + (cid:0)ğ‘¤ âˆ—
where ğ‘–) follows from the definition of ğ‘ â˜…
By combining Assumption 4.1 and Lemma 4.1, it is known that

ğ‘– and the fact that ğ‘ğ‘–ğ‘’ğ‘– = ğ‘£ğ‘– .

ğ‘– Ë†ğ‘£ğ‘– + ğ›½ğ‘¦ğ‘– Ë†ğ‘£ğ‘– âˆ’ ğ›½ğ‘¦âˆ—
ğ‘– Ë†ğ‘£ğ‘–

(14)

Kim, et al.

Therefore, based on (15) and (16), it is known that âˆ¥ğ‘ğ‘–ğ‘ â˜…

2 and
2 are both upper bounded by a positive factor of ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜….

ğ‘– âˆ¥2

âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
The inner product of (12) with itself to yields:
ğ‘– )(cid:1)2

ğ‘– ) + ğ›½ (ğ‘¦ğ‘– âˆ’ ğ‘¦âˆ—

ğ‘– ğ‘¦âˆ—

Î”2
ğ‘–

ğ‘–)
= (cid:0)(ğ‘¤ğ‘–ğ‘¦ğ‘– âˆ’ ğ‘¤ âˆ—
ğ‘–ğ‘–)
â‰¤ 2âˆ¥ğ‘ğ‘–ğ‘ âˆ—
= 2âˆ¥ğ‘ğ‘–ğ‘ âˆ—

ğ‘– âˆ¥2 + 2(ğ‘¦âˆ—
ğ‘– âˆ’ ğ›½ğ‘¦ğ‘– )2 âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2
ğ‘– âˆ¥2 + 2(ğ‘¦âˆ—
ğ‘–

ğ‘– ğ‘¦ğ‘– + ğ‘¦ğ‘–

âˆ’ 2ğ›½ğ‘¦âˆ—

2) âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2

where ğ‘–) follows from the fact that Ë†ğ‘£ğ‘– is a unit vector and ğ‘–ğ‘–) follows
from combining observation 6.1 with (14). Next, we have:

âˆ¥(ğ‘¤ + ğ›½1) âŠ™ ğ‘¦ âˆ’ (ğ‘¤ âˆ— + ğ›½1) âŠ™ ğ‘¦âˆ— âˆ¥2

2 = âˆ¥Î”âˆ¥2

ğ¹ =

ğ‘›
âˆ‘ï¸

ğ‘–=1

Î”2
ğ‘– .

We combine this expression with the upper bounds for âˆ¥ğ‘ğ‘–ğ‘ â˜…
and âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
where ğœ is defined as some positive constant:

ğ‘– âˆ¥2
2
2 derived in (15) and (16) to arrive at the final result,

âˆ¥(ğ‘¤ + ğ›½1) âŠ™ ğ‘¦ âˆ’ (ğ‘¤ âˆ— + ğ›½1) âŠ™ ğ‘¦âˆ— âˆ¥2

2 â‰¤ ğœ (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—)

Proof of Theorem 4.1
Based on Lemma 4.3, a neighborhood can be selected around the
optimum such that the value of ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜… is sufficiently small.
We define this neighborhood through the selection of a positive
constant ğœ… such that the following inequality holds, where ğœ is
defined in Lemma 4.3:

(cid:19)2

(cid:18) ğœ âˆ’ ğœ…
ğ›¾

â‰¥ ğœ (cid:0)ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ â˜…(cid:1)

(17)

Within this expression, ğœ and ğ›¾ are both defined in Lemma 4.2. Be-
cause ğœ and ğ›¾ are both strictly positive (i.e., see Lemma 4.2), (cid:16) ğœ âˆ’ğœ…
(cid:17) is
ğ›¾
known to be strictly positive so long as ğœ… âˆˆ [0, ğœ ). Therefore, there
always exists a value of ğœ… such that (17) will be true within a suffi-
ciently small neighborhood around the optimum. We combine (17)
with the inequality from Lemma 4.3 to yield the following:

(cid:19)2

(cid:18) ğœ âˆ’ ğœ…
ğ›¾
Then, because (cid:16) ğœ âˆ’ğœ…
ğ›¾
above expression to yield the following:

â‰¥ ğœ (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—) â‰¥ âˆ¥Î”âˆ¥2
ğ¹ ,

(cid:17) is strictly positive, we can manipulate the

ğœ âˆ’ ğ›¾ âˆ¥Î”âˆ¥ğ¹ â‰¥ ğœ….

(18)

The above inequality holds for all successive iterations of Algo-
rithm 2. Then, based on the combination of Lemma 4.1 and assump-
tion 4.1, ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ ( Ë†ğ‘‰ ) â‰¥ ğ›¿ 1âˆ’ğ›½
1+ğ›½ âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2
1 âˆ’ ğ›½
1 + ğ›½
1 âˆ’ ğ›½
1 + ğ›½

ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ ( Ë†ğ‘‰ ) â‰¥ ğ›¿

ğœ… (ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—)

ğ¹ , and thus

âˆ¥ğ‘‰ âˆ’ Ë†ğ‘‰ âˆ¥2
ğ¹

ğ‘–)
â‰¥ ğ›¿

ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ— â‰¥

1 âˆ’ ğ›½
1 + ğ›½

ğ›¿ âˆ¥ğ‘£ğ‘– âˆ’ Ë†ğ‘£ğ‘– âˆ¥2
2.

Besides,

ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ— â‰¥ ğ‘“ (ğ‘ğ‘– ) âˆ’ ğ‘“ âˆ— = Tr(ğ‘ âŠ¤

â‰¥ ğœâˆ’1

max (ğ‘†âˆ—)âˆ¥ğ‘ğ‘–ğ‘†âˆ— âˆ¥2

max (ğ‘†âˆ—)âˆ¥ğ‘ğ‘–ğ‘ âˆ—

ğ‘– âˆ¥2
2

ğ‘– ğ‘ğ‘–ğ‘†âˆ—)
ğ¹ â‰¥ ğœâˆ’1

(15)

(16)

where ğ‘–) is derived by combining (18) and Lemma 4.2. This expres-
sion in turn implies the following.

(cid:18)

1 âˆ’ ğ›¿ğœ…

(cid:19)

1 âˆ’ ğ›½
1 + ğ›½

(ğ‘“ (ğ‘‰ ) âˆ’ ğ‘“ âˆ—) â‰¥ ğ‘“ ( Ë†ğ‘‰ ) âˆ’ ğ‘“ âˆ—.

thus giving us the desired linear convergence. By substituting ğœŒ =
ğ›¿ğœ… 1âˆ’ğ›½
1+ğ›½

, we arrive at the final result.

Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs

7 CONVERGENCE TO FIRST-ORDER

CRITICAL POINT

Mixing method++ not only has local linear convergence, but it
always converges to a first-order critical point, per following Theo-
rem:

Theorem 7.1. Let ğ‘‰ â„“ = ğ‘ â„“

ğ‘›+1 (ğ‘‰ ), see definition (7), for all â„“ âˆˆ N;
i.e., the collection of points generated by Mixing Method++ after
finishing the inner loop. Then, under the assumption 4.1, {ğ‘‰ â„“ }â„“ con-
verges to a limit point Â¯ğ‘‰ and the Riemannian gradient at Â¯ğ‘‰ is zero
(Mixing Method++ converges to a first-order critical point).

Proof. For ğ›½ in [0, 1) we know by lemma 4.1 that the objective
function is decreasing. Besides, (ğ‘†ğ‘˜âˆ’1)ğ‘› is compact, indeed limit
point compact, so there exists a limit point Â¯ğ‘‰ such that ğ‘“ ( Â¯ğ‘‰ ) =
limğ‘˜ ğ‘“ (ğ‘‰ ğ‘˜ ) by continuity of ğ‘“ . It is clear that Â¯ğ‘‰ is a fixed point in
relation (9), so

Â¯ğ‘‰ğ¶ = âˆ’

Â¯ğ‘‰ ğ· ( Â¯ğ‘¤+ğ›½1) âŠ™ Â¯ğ‘¦
1 + ğ›½

.

(19)

Since the constraint set is a product of spheres, its corresponding
tangent space (henceforth denoted by the letter T) at the point Â¯ğ‘‰ is
given by the product of tangent spaces of ğ‘†ğ‘˜âˆ’1; namely

ğ‘†ğ‘˜âˆ’1(cid:17)ğ‘›
(cid:16)

=

T Â¯ğ‘‰

(cid:16)
ğ‘†ğ‘˜âˆ’1(cid:17)

,

T Â¯ğ‘£ğ‘—

ğ‘›
(cid:214)

ğ‘—=1

(20)

where Â¯ğ‘£ ğ‘— is the ğ‘—th column of Â¯ğ‘‰ .

The above result gives us a way to characterize the tangent space.
To this end, it is actually enough to work with the tangent space of
ğ‘†ğ‘˜âˆ’1.

By definition, for any ğ‘£ ğ‘— , the vector (cid:164)ğ‘£ ğ‘— âˆˆ Rğ‘˜ is in the tangent
space of ğ‘†ğ‘˜âˆ’1 if and only if there exists a curve ğœ‘ : ğ¼ â†’ ğ‘†ğ‘˜âˆ’1 such
that ğœ‘ (0) = ğ‘£ ğ‘— and (cid:164)ğœ‘ (0) = (cid:164)ğ‘£ ğ‘— , where (cid:164)ğœ‘ is the derivative of ğœ‘. Then
ğœ‘ (ğ‘¡) âˆˆ ğ‘†ğ‘˜âˆ’1 if satisfies âŸ¨ğœ‘ (ğ‘¡), ğœ‘ (ğ‘¡)âŸ© = 1 for all ğ‘¡ âˆˆ ğ¼ . Differentiating
on both sides leads us to 0 = âŸ¨ (cid:164)ğœ‘ (ğ‘¡), ğœ‘ (ğ‘¡)âŸ© + âŸ¨ğœ‘ (ğ‘¡), (cid:164)ğœ‘ (ğ‘¡)âŸ©. Evaluating
at ğ‘¡ = 0, we get

2âŸ¨(cid:164)ğ‘£ ğ‘— , ğ‘£ ğ‘— âŸ© = 0,
thus { (cid:164)ğ‘£ ğ‘— âˆˆ Rğ‘˜ : âŸ¨(cid:164)ğ‘£ ğ‘— , ğ‘£ ğ‘— âŸ© = 0} âŠ† Tğ‘£ğ‘— ğ‘†ğ‘˜âˆ’1, since both subspaces are
of the same dimension, { (cid:164)ğ‘£ ğ‘— âˆˆ Rğ‘˜ : âŸ¨(cid:164)ğ‘£ ğ‘— , ğ‘£ ğ‘— âŸ© = 0} = Tğ‘£ğ‘— ğ‘†ğ‘˜âˆ’1.

Now by (20) and the previous analysis, we finally arrive to the

characterization of the tangent space; namely

ğ‘†ğ‘˜âˆ’1(cid:17)ğ‘›
(cid:16)

T Â¯ğ‘‰

(cid:110) (cid:164)Â¯ğ‘‰ âˆˆ Rğ‘˜Ã—ğ‘› : âŸ¨(cid:164)Â¯ğ‘£ ğ‘— , Â¯ğ‘£ ğ‘— âŸ© = 0 âˆ€ğ‘— âˆˆ [ğ‘›]

(cid:111)

,

=

and (cid:164)Â¯ğ‘£ ğ‘— (ğ‘£ ğ‘— ) corresponds to the ğ‘—th column of (cid:164)Â¯ğ‘‰ ( Â¯ğ‘‰ ), respectively.

ğ‘†ğ‘˜âˆ’1(cid:17)ğ‘›
(cid:16)

Let PâŠ¥ : Rğ‘˜Ã—ğ‘› â†’ T Â¯ğ‘‰

be the projector operator from the
euclidean space to the tangent space at Â¯ğ‘‰ , defined for anyğ‘Š âˆˆ Rğ‘˜Ã—ğ‘›
as follows,

ğ‘Š

PâŠ¥
âˆ’âˆ’â†’ (cid:0)ğ‘¤1 âˆ’ âŸ¨Â¯ğ‘£1, ğ‘¤1âŸ© Â¯ğ‘£1

. . . ğ‘¤ğ‘› âˆ’ âŸ¨Â¯ğ‘£ğ‘›, ğ‘¤ğ‘›âŸ© Â¯ğ‘£ğ‘› (cid:1) .

Let the Riemannian gradient of ğ‘“ denoted by gradğ‘“ (Â·), and the
gradient of ğ‘“ defined in the entire euclidean domain as âˆ‡ğ‘“ (Â·). Using
tools from matrix manifold the Riemannian gradient is given by

gradğ‘“ ( Â¯ğ‘‰ ) = PâŠ¥ (âˆ‡ğ‘“ ( Â¯ğ‘‰ )).

It is easy to see that âˆ‡ğ‘“ ( Â¯ğ‘‰ ) = 2 Â¯ğ‘‰ğ¶, and by relation (19), this yields
us to the equivalent version âˆ‡ğ‘“ ( Â¯ğ‘‰ ) = âˆ’ 2
1+ğ›½

Â¯ğ‘‰ ğ· ( Â¯ğ‘¤+ğ›½1) âŠ™ Â¯ğ‘¦.

Finally, (cid:0)âˆ‡ğ‘“ ( Â¯ğ‘‰ )(cid:1)

ğ‘— = âˆ’ 2

1+ğ›½ (ğ‘¤ ğ‘—ğ‘¦ ğ‘— + ğ›½ğ‘¦ ğ‘— ) Â¯ğ‘£ ğ‘— , âˆ€ğ‘— âˆˆ [ğ‘›] and by defi-

nition of the projection map we get

(cid:0)PâŠ¥ (âˆ‡ğ‘“ ( Â¯ğ‘‰ ))(cid:1)

ğ‘— =

2
1 + ğ›½

(ğ‘¤ ğ‘—ğ‘¦ ğ‘— + ğ›½ğ‘¦ ğ‘— )(âˆ’Â¯ğ‘£ ğ‘— + Â¯ğ‘£ ğ‘— ) = 0.

So, we have (cid:0)PâŠ¥ (âˆ‡ğ‘“ ( Â¯ğ‘‰ )(cid:1)

ğ‘— = 0, for all coordinates ğ‘— âˆˆ [ğ‘›], and thus

gradğ‘“ ( Â¯ğ‘‰ ) = PâŠ¥ (âˆ‡ğ‘“ ( Â¯ğ‘‰ )) = 0.

â–¡

8 CONCLUSION AND FUTURE DIRECTIONS
We present a novel approach, Mixing Method++, to solve diagonally
constrained SDPs. Mixing Method++ inherits the simplicityâ€”and
non-trivially preserves theoretical guarantees fromâ€”its predeces-
sor. In practice, it yields not only faster convergence on nearly all
tested instances, but also improvements in solution quality. Mixing
Method++ adds one addition hyperparameter ğ›½ for which we pro-
vide a theoretical upper bound (i.e., ğ›½ < 1). Using ğ›½ = 0.8 experi-
mentally leads to a robust algorithm, which outperforms Mixing
Method and numerous other state-of-the-art SDP solvers. Theoreti-
cally validating the experimental acceleration provided by Mixing
Method++ is still an open problem, which could potentially be han-
dled with the help of Lyapunov Analysis [28, 60].

ACKNOWLEDGMENTS
AK acknowledges funding by the NSF (CCF-1907936). This work
was partially done as MTTâ€™s and CWâ€™s class project for â€œCOMP545:
Advanced Topics in Optimization,â€ Rice University, Spring 2021. AK
thanks Danny Carey for his percussion performance at the song
â€œPneuma.â€

REFERENCES
[1] E. Abbe. 2018. Community Detection and Stochastic Block Models. Foundations

and TrendsÂ® in Communications and Information Theory 14, 1-2 (2018), 1â€“162.
[2] Farid Alizadeh, Jean-Pierre A Haeberly, and Michael L Overton. 1997. Com-
plementarity and nondegeneracy in semidefinite programming. Mathematical
programming 77, 1 (1997), 111â€“128.

[3] Mosek ApS. 2019. Mosek optimization toolbox for MATLAB. Userâ€™s Guide and

Reference Manual, version 4 (2019).

[4] M. Assran and M. Rabbat. 2020. On the Convergence of Nesterovâ€™s Accelerated
Gradient Method in Stochastic Settings. arXiv preprint arXiv:2002.12414 (2020).
[5] F. Barahona, M. GrÃ¶tschel, M. JÃ¼nger, and G. Reinelt. 1988. An application
of combinatorial optimization to statistical physics and circuit layout design.
Operations Research 36, 3 (1988), 493â€“513.

[6] Alexander I. Barvinok. 1995. Problems of distance geometry and convex proper-
ties of quadratic maps. Discrete & Computational Geometry 13, 2 (1995), 189â€“202.
[7] Jeremias Berg, Emir DemiroviÄ‡, and Peter J. Stuckey. 2019. Core-Boosted Linear
Search for Incomplete MaxSAT. In Integration of Constraint Programming, Arti-
ficial Intelligence, and Operations Research, Louis-Martin Rousseau and Kostas
Stergiou (Eds.). Springer International Publishing, Cham, 39â€“56.

[8] S. Bhojanapalli, N. Boumal, P. Jain, and P. Netrapalli. 2018. Smoothed analysis
for low-rank solutions to semidefinite programs in quadratic penalty form. arXiv
preprint arXiv:1803.00186 (2018).

[9] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. 2016. Dropping
convexity for faster semi-definite optimization. In Conference on Learning Theory.
530â€“582.

[10] N. Boumal. 2016. Nonconvex phase synchronization. SIAM Journal on Optimiza-

tion 26, 4 (2016), 2355â€“2377.

[11] S. Burer and R. Monteiro. 2003. A nonlinear programming algorithm for solving
semidefinite programs via low-rank factorization. Mathematical Programming
95, 2 (2003), 329â€“357.

Kim, et al.

[12] John Chen, Cameron Wolfe, Zhao Li, and Anastasios Kyrillidis. 2019. Demon:
Momentum Decay for Improved Neural Network Training. arXiv preprint
arXiv:1910.04952 (2019).

[13] O. Devolder, F. Glineur, and Y. Nesterov. 2014. First-order methods of smooth
convex optimization with inexact oracle. Mathematical Programming 146, 1-2
(2014), 37â€“75.

[14] M. Deza and M. Laurent. 1994. Applications of cut polyhedra - II. J. Comput.

Appl. Math. 55, 2 (1994), 217â€“247.

[15] Murat A. Erdogdu, Asuman Ozdaglar, Pablo A. Parrilo, and Nuri Denizcan Vanli.
2018. Convergence Rate of Block-Coordinate Maximization Burer-Monteiro
Method for Solving Large SDPs. arXiv e-prints, Article arXiv:1807.04428 (July
2018), arXiv:1807.04428 pages. arXiv:1807.04428 [math.OC]

[16] Alan Frieze and Mark Jerrum. 1995. Improved approximation algorithms for MAX
ğ‘˜-CUT and MAX BISECTION. In International Conference on Integer Programming
and Combinatorial Optimization. Springer, 1â€“13.

[39] S. Mei, T. Misiakiewicz, A. Montanari, and R. Oliveira. 2017. Solving SDPs
for synchronization and MaxCut problems via the Grothendieck inequality. In
Conference on Learning Theory. 1476â€“1515.
[40] RV Mises and Hilda Pollaczek-Geiringer. 1929.

Praktische Verfahren der
GleichungsauflÃ¶sung. ZAMM-Journal of Applied Mathematics and Mechan-
ics/Zeitschrift fÃ¼r Angewandte Mathematik und Mechanik 9, 2 (1929), 152â€“164.

[41] ApS Mosek. 2015. The MOSEK optimization toolbox for Python manual.
[42] Idin Motedayen-Aval, Arvind Krishnamoorthy, and Achilleas Anastasopoulos.
2006. Optimal joint detection/estimation in fading channels with polynomial
complexity. IEEE transactions on information theory 53, 1 (2006), 209â€“223.
[43] Yurii Nesterov. 2013. Introductory lectures on convex optimization: A basic course.

Vol. 87. Springer Science & Business Media.

[44] Y. Nesterov and A. Nemirovskii. 1989. Self-concordant functions and polynomial-
time methods in convex programming. USSR Academy of Sciences, Central Eco-
nomic & Mathematic Institute.

[17] B. GÃ¤rtner and J. Matousek. 2012. Approximation algorithms and semidefinite

[45] Y. Nesterov and A. Nemirovskii. 1994. Interior-point polynomial algorithms in

programming. Springer Science & Business Media.

convex programming. SIAM.

[18] E. Ghadimi, H. Feyzmahdavian, and M. Johansson. 2015. Global convergence
of the heavy-ball method for convex optimization. In 2015 European control
conference (ECC). IEEE, 310â€“315.

[19] F. Gieseke, T. Pahikkala, and C. Igel. 2013. Polynomial runtime bounds for fixed-
rank unsupervised least-squares classification. In Asian Conference on Machine
Learning. 62â€“71.

[20] M. Goemans and D. Williamson. 1995.

Improved approximation algorithms
for maximum cut and satisfiability problems using semidefinite programming.
Journal of the ACM (JACM) 42, 6 (1995), 1115â€“1145.

[21] Michel X Goemans and David P Williamson. 2004. Approximation algorithms
for MAX-3-CUT and other problems via complex semidefinite programming. J.
Comput. System Sci. 68, 2 (2004), 442â€“470.

[22] Hayato Goto, Kosuke Tatsumura, and Alexander R Dixon. 2019. Combinatorial
optimization by simulating adiabatic bifurcatâ€˜ions in nonlinear Hamiltonian
systems. Science advances 5, 4 (2019), eaav2372.

[46] Brendan Oâ€™donoghue and Emmanuel Candes. 2015. Adaptive restart for acceler-
ated gradient schemes. Foundations of computational mathematics 15, 3 (2015),
715â€“732.

[47] GÃ¡bor Pataki. 1998. On the rank of extreme matrices in semidefinite programs
and the multiplicity of optimal eigenvalues. Mathematics of operations research
23, 2 (1998), 339â€“358.

[48] Boris T Polyak. 1987. Introduction to optimization. optimization software. Inc.,

Publications Division, New York 1 (1987).

[49] J. Shi and J. Malik. 2000. Normalized cuts and image segmentation. IEEE Trans-
actions on pattern analysis and machine intelligence 22, 8 (2000), 888â€“905.
[50] A. Singer. 2011. Angular synchronization by eigenvectors and semidefinite
programming. Applied and computational harmonic analysis 30, 1 (2011), 20â€“36.
[51] Anthony Man-Cho So, Jiawei Zhang, and Yinyu Ye. 2007. On approximating com-
plex quadratic optimization problems via semidefinite programming relaxations.
Mathematical Programming 110, 1 (2007), 93â€“110.

[23] M. GrÃ¶tschel, L. LovÃ¡sz, and A. Schrijver. 2012. Geometric algorithms and combi-

[52] Jos F Sturm. 1999. Using SeDuMi 1.02, a MATLAB toolbox for optimization over

natorial optimization. Vol. 2. Springer Science & Business Media.

[24] Gurobi. 2014. Inc.,â€œGurobi optimizer reference manual,â€ 2015.
[25] B. Hajek, Y. Wu, and J. Xu. 2016. Achieving exact cluster recovery threshold via
semidefinite programming. IEEE Transactions on Information Theory 62, 5 (2016),
2788â€“2797.

[26] A. Hartmann. 1996. Cluster-exact approximation of spin glass groundstates.
Physica A: Statistical Mechanics and its Applications 224, 3-4 (1996), 480â€“488.
[27] Robert W Heath and Arogyaswami Paulraj. 1998. A simple scheme for transmit
diversity using partial channel feedback. In Conference Record of Thirty-Second
Asilomar Conference on Signals, Systems and Computers (Cat. No. 98CH36284),
Vol. 2. IEEE, 1073â€“1078.

[28] Chi Jin, Praneeth Netrapalli, and Michael I Jordan. 2018. Accelerated gradient
descent escapes saddle points faster than gradient descent. In Conference On
Learning Theory. 1042â€“1085.

[29] R. Karp. 1972. Reducibility among combinatorial problems. In Complexity of

computer computations. Springer, 85â€“103.

[30] N. Krislock, J. Malick, and F. Roupin. 2017. BiqCrunch: a semidefinite branch-
and-bound method for solving binary quadratic problems. ACM Transactions on
Mathematical Software (TOMS) 43, 4 (2017), 32.

[31] Anastasios Kyrillidis, Amir Kalev, Dohyung Park, Srinadh Bhojanapalli, Con-
stantine Caramanis, and Sujay Sanghavi. 2018. Provable compressed sensing
quantum state tomography via non-convex methods. npj Quantum Information
4, 1 (2018), 1â€“7.

[32] Anastasios Kyrillidis and George N Karystinos. 2014. Fixed-rank Rayleigh quo-
tient maximization by an MPSK sequence. IEEE transactions on communications
62, 3 (2014), 961â€“975.

[33] Anastasios T Kyrillidis and George N Karystinos. 2011. Rank-deficient quadratic-
form maximization over M-phase alphabet: Polynomial-complexity solvability
and algorithmic developments. In 2011 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 3856â€“3859.

[34] L. Lessard, B. Recht, and A. Packard. 2016. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization 26,
1 (2016), 57â€“95.

[35] Huikang Liu, Man-Chung Yue, Anthony Man-Cho So, and Wing-Kin Ma. 2017.
A discrete first-order method for large-scale MIMO detection with provable guar-
antees. In 2017 IEEE 18th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC). IEEE, 1â€“5.

[36] N. Loizou and P. RichtÃ¡rik. 2017. Momentum and stochastic momentum for
stochastic gradient, Newton, proximal point and subspace descent methods.
arXiv preprint arXiv:1712.09677 (2017).

[37] David J Love, Robert W Heath, and Thomas Strohmer. 2003. Grassmannian beam-
forming for multiple-input multiple-output wireless systems. IEEE transactions
on information theory 49, 10 (2003), 2735â€“2747.

[38] R. MartÃ­, A. Duarte, and M. Laguna. 2009. Advanced scatter search for the max-cut

problem. INFORMS Journal on Computing 21, 1 (2009), 26â€“38.

symmetric cones. Optimization methods and software 11, 1-4 (1999), 625â€“653.

[53] Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. 2014. An inexact proximal path-
following algorithm for constrained convex minimization. SIAM Journal on
Optimization 24, 4 (2014), 1718â€“1745.

[54] Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. 2016. A single-phase, proximal path-

following framework. arXiv preprint arXiv:1603.01681 (2016).

[55] N. Veldt, A. Wirth, and D. Gleich. 2017. Correlation Clustering with Low-Rank
Matrices. In Proceedings of the 26th International Conference on World Wide Web.
International World Wide Web Conferences Steering Committee, 1025â€“1034.

[56] I. Waldspurger, A. dâ€™Aspremont, and S. Mallat. 2015. Phase recovery, MAXCUT
and complex semidefinite programming. Mathematical Programming 149, 1-2
(2015), 47â€“81.

[57] Bao Wang, Tan M Nguyen, Andrea L Bertozzi, Richard G Baraniuk, and Stanley J
Osher. 2020. Scheduled restart momentum for accelerated stochastic gradient
descent. arXiv preprint arXiv:2002.10583 (2020).

[58] J. Wang, T. Jebara, and S.-F. Chang. 2013. Semi-supervised learning using greedy
MaxCut. Journal of Machine Learning Research 14, Mar (2013), 771â€“800.
[59] P.-W. Wang, W.-C. Chang, and Z. Kolter. 2017. The Mixing method: coordinate
descent for low-rank semidefinite programming. arXiv preprint arXiv:1706.00476
(2017).

[60] Ashia C Wilson, Benjamin Recht, and Michael I Jordan. 2016. A Lyapunov
analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635
(2016).

[61] Liuqin Yang, Defeng Sun, and Kim-Chuan Toh. 2015. SDPNAL: a majorized semis-
mooth Newton-CG augmented Lagrangian method for semidefinite program-
ming with nonnegative constraints. Mathematical Programming Computation 7,
3 (2015), 331â€“366.

[62] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher.
2019. Scalable Semidefinite Programming. arXiv preprint arXiv:1912.02949 (2019).
[63] Alp Yurtsever, Joel A. Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher.
2019. Scalable Semidefinite Programming. arXiv e-prints, Article arXiv:1912.02949
(Dec 2019), arXiv:1912.02949 pages. arXiv:1912.02949 [math.OC]

[64] Y. Zhong and N. Boumal. 2018. Near-optimal bounds for phase synchronization.

SIAM Journal on Optimization 28, 2 (2018), 989â€“1016.

