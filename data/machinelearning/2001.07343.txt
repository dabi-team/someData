0
2
0
2

n
a
J

1
2

]

O
R
.
s
c
[

1
v
3
4
3
7
0
.
1
0
0
2
:
v
i
X
r
a

Lyceum: An efﬁcient and scalable ecosystem for robot learning

Colin Summers1, Kendall Lowrey1, Aravind Rajeswaran1
Siddhartha Srinivasa1, Emanuel Todorov1,2

{colinxs, klowrey, aravraj, siddh, todorov}@cs.uw.edu
1 University of Washington Seattle, 2 Roboti LLC

Abstract
We introduce Lyceum, a high-performance computational ecosystem for robot learning. Lyceum is
built on top of the Julia programming language and the MuJoCo physics simulator, combining the
ease-of-use of a high-level programming language with the performance of native C. In addition,
Lyceum has a straightforward API to support parallel computation across multiple cores and
machines. Overall, depending on the complexity of the environment, Lyceum is 5-30X faster
compared to other popular abstractions like OpenAI’s Gym and DeepMind’s dm-control. This
substantially reduces training time for various reinforcement learning algorithms; and is also fast
enough to support real-time model predictive control through MuJoCo. The code, tutorials, and
demonstration videos can be found at: www.lyceum.ml.

1. Introduction

Progress in artiﬁcial intelligence has exploded in recent years, due in large part to advances computa-
tional infrastructure. The advent of massively parallel GPU computing, combined with powerful
automatic-differentiation tools like TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2017),
have lead to new classes of deep learning algorithms by enabling what was once computationally
intractable. These tools, alongside fast and accurate physics simulators like MuJoCo (Todorov et al.,
2012), and associated frameworks like OpenAI’s Gym (Brockman et al., 2016) and DeepMind’s
dm_control (Tassa et al., 2018), have similarly transformed various aspects of robotic control like
Reinforcement Learning (RL), Model-Predictive Control (MPC), and motion planning. These plat-
forms enable researchers to give their ideas computational form, share results with collaborators, and
deploy their successes on real systems.

From these advances, simulation to real-world (sim2real) transfer has emerged as a promis-
ing paradigm for robotic control. A growing body of recent work suggests that robust control policies
trained in simulation can successfully transfer to the real world (OpenAI, 2018; Rajeswaran et al.,
2016; Sadeghi and Levine, 2016; Lowrey et al., 2018a; Tobin et al., 2017; Mordatch et al., 2015).
However, many algorithms used in these works for controller synthesis are computationally intensive.
Training control policies with state-of-the-art RL algorithms often takes many hours to days of
compute time. For example, OpenAI’s landmark Dactyl work (OpenAI, 2018) required 50 hours
of training time across 6144 CPU cores and 8 NVIDIA V100 GPUs. Such computational budgets
are available only to a select few labs. Furthermore, such experiments are seldom run only once
in deep learning and especially in deep RL. Indeed, RL algorithms are notoriously sensitive to
choices of hyper-parameters (Rajeswaran et al., 2017; Henderson et al., 2017; Mania et al., 2018).
Thus, many iterations of the learning process may be required, with humans in the loop, to improve
hyperparameter choices and reward functions, before ﬁnally deploying solutions in the real world.

© C. Summers, K. Lowrey, A. Rajeswaran, S. Srinivasa & E. Todorov.

 
 
 
 
 
 
LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

This computational bottleneck often leads to a scarcity of hardware results, relative to the number of
papers that propose new algorithms on highly simpliﬁed and well tuned benchmark tasks. Exploring
avenues to reduce experiment turn around time is thus crucial for scaling up to harder tasks and
making resource-intensive algorithms and environments accessible to research labs without massive
cloud computing budgets.

In a similar vein, computational considerations have also limited progress in model-based control
algorithms. For real-time model predictive control (MPC), the computational restrictions manifest
as the requirement to compute actions in bounded time with limited local resources. As we will
show, existing frameworks such as Gym and dm_control, while providing a convenient abstraction
in Python, are too slow to meet this real-time computation requirement. As a result, most planning
algorithms are run ofﬂine and deployed in open-loop mode on hardware. This is unfortunate, since it
does not take feedback into account which is well known to be critical for stochastic control.

Our Contributions: Our goal in this work is to overcome the aforementioned computational
restrictions to enable faster training of policies with RL algorithms, facilitate real-time MPC with
a detailed physics simulator, and ultimately enable researchers to engage with complex robotic
tasks. To this end, we develop Lyceum, a computational ecosystem that uses the Julia programming
language and the MuJoCo physics engine. Lyceum ships with the main OpenAI gym continuous
control tasks, along with other environments representative of challenges in robotics. Julia’s unique
features allow us to wrap MuJoCo with zero-cost abstractions, providing the ﬂexibility of a high-level
programming language to enable easy creation of environments, tasks, and algorithms, while retaining
the performance of native C. This allows RL and MPC algorithms implemented in Lyceum to be
5-30X faster compared to Gym and dm_control. We hope that this speedup will enable RL researchers
to scale up to harder problems with reduced computational costs, and also enable real-time MPC.

2. Related Works

Recently, various physics simulators and the computational ecosystems surrounding them have
transformed robot learning research. They allow for exercising creativity to quickly generate new
and interesting robotic scenes, as well as quickly prototype various learning and control solutions.
We summarize the main threads of related work below.

Physics simulators: MuJoCo (Todorov et al., 2012) has quickly emerged as a leading physics
simulator for robot learning research. It is fast and efﬁcient, and particularly well suited for contact-
rich tasks. Numerous recent works have also demonstrated simulation to reality transfer with
MuJoCo through physically consistent system identiﬁcation (Lowrey et al., 2018a) or domain
randomization (OpenAI, 2018; Mordatch et al., 2015; Nachum et al., 2019). Our framework wraps
MuJoCo in Julia and enables programming and research with a high level language, while retaining
the speed of native C. While we primarily focus on MuJoCo, we believe that similar design principles
can be extended to other simulators like Bullet (Coumans, 2013) and DART (Lee et al., 2018).

Computational ecosystems: OpenAI’s gym (Brockman et al., 2016) and DeepMind’s dm_control
(Tassa et al., 2018) sparked a wave of interest by providing Python bindings for MuJoCo with
a high-level API, as well as easy-to-use environments and algorithms. This has enabled the RL
community to quickly access physics-based environments and prototype algorithms. Unfortunately,
this ﬂexibility comes at the price of computational efﬁciency. Existing ecosystems are slow due
to inefﬁciencies and poor parallelization capabilities of Python. Prior works have tried to address

2

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

some of the shortcomings of Python-based frameworks by attempting to add JIT compilation to the
language (Lam et al., 2015; Paszke et al., 2017; Agrawal et al., 2019) but only support a subset of the
language, and do not achieve the same performance as Julia. Fan et al. (2018) developed a framework
similar to Gym that supports distributed computing, but it still suffers the same performance issues
of Python and multi-processing. Perhaps closest to our motivation is the work of Koolen and Deits
(2019), which demonstrates the usefulness of Julia as a language for robotics. However, it uses a
custom and minimalist rigid body simulator with limited contact support. In contrast, our work
addresses the inefﬁciencies of existing computational ecosystems through use the of Julia, and
directly wraps a more capable simulator, MuJoCo, with zero overhead.

Algorithmic toolkits and environments: A number of algorithmic toolkits like OpenAI Base-
lines (Dhariwal et al., 2017), mjRL (Rajeswaran et al., 2017), Soft-Learning (Haarnoja et al., 2018),
and RL-lab (Duan et al., 2016); as well as environments like Hand Manipulation Suite (Rajeswaran
et al., 2018), Robel (Ahn et al., 2019), Door Gym (Urakami et al., 2019), and Surreal Robosuite (Fan
et al., 2018) have been developed around existing computational ecosystems. Our framework sup-
ports all the underlying functionality needed to transfer these advances into our ecosystem (e.g.
simulator wrappers and automatic differentiation through Flux). Lyceum comes with a few popular
algorithms out of the box like NPG (Kakade, 2002; Rajeswaran et al., 2017) for RL and variants of
MPPI (Lowrey et al., 2018b; Williams et al., 2016) for MPC. In the future, we plan to port further
algorithms and advances into our ecosystem and look forward to community contributions as well.

3. The Lyceum ecosystem

The computational considerations are unique for designing infrastructure and ecosystems for robotic
control with RL and MPC. We desire a computational ecosystem that is high-level and easy to use
for research, can efﬁciently handle parallel operations, while ideally also matching serial operation
speed of native C to be usable on robots. We found Julia to be well suited for these requirements,
and we summarize some of these main advantages that prompted us to use Julia. Subsequently, we
outline some salient features of Lyceum.

3.1. Julia for robotics and RL

Julia is a general-purpose programming language developed in 2012 at MIT with a focus on technical
computing (Bezanson et al., 2017). While a full description of Julia is beyond the scope of this paper,
we highlight a few key aspects that we leverage in Lyceum and believe make Julia an excellent tool
for robotics and RL researchers.

Just-in-time compilation Julia feels like a dynamic, interpreted scripting language, enabling an
interactive programming experience. Under the hood, however, Julia leverages the LLVM backend
to "just-in-time" (JIT) compile native machine code that is as fast as C for a variety of hardware
platforms (Bezanson et al., 2017). This enables researchers to quickly prototype ideas and optimize
for performance with the same language.

Julia can easily call functions in Python and C In addition to the current ecosystem of Julia
packages, users can interact with Python and C as illustrated below. This allows researchers to beneﬁt
from existing body of deep learning research (in Python), and also interact easily with low-level
robot hardware drivers.

3

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

using PyCall
so = pyimport("scipy.optimize")
so.newton(x -> cos(x) - x, 1)
ccall((:mjr_getError,libmujoco),Cint,())

Easy paralellization Julia comes with extensive support for distributed and shared-memory multi-
threading that allows users to trivially parallelize their code. The following example splits the indices
of X across all the available cores and performs in-place multiplication in parallel:

@threads for i in eachindex(X)

X[i] *= 2

end

Julia can also transpile to alternative hardware backends, allowing use of parallel processors like
GPUs by writing high level Julia code.

Simple package management To handle the 3000+ packages available, Julia comes with a built-in
package manager, avoiding "dependency hell", and facilitating collaboration and replication. This
means less time is spent getting things to run and more time for focusing on the task at hand.

3.2. Salient Features of Lyceum

Lyceum consists of the following packages

1. LyceumBase.jl, a "base" package which deﬁnes a set of abstract environment and con-

troller interfaces, along with several utilities.

2. MuJoCo.jl, a low-level Julia wrapper for the MuJoCo physics simulator.
3. LyceumMuJoCo.jl, a high-level "environment" abstraction similar to Gym and dm_control.
4. LyceumMuJoCoViz.jl, a ﬂexible policy and trajectory visualizer with interaction.
5. LyceumAI.jl, a collection of various algorithms for robotic control.

LyceumBase.jl At the highest level we provide LyceumBase.jl, which contains several
convenience utilities used throughout the Lyceum ecosystem for data logging, multi-threading, and
controller benchmarking (i.e. measuring throughput, jitter, etc.). LyceumBase.jl also contains
interface deﬁnitions, such as AbstractEnvironment which LyceumMuJoCo.jl implements.
See the appendix for the full AbstractEnvironment interface.

This interface is similar to the popular Python frameworks Gym and dm_control, where an agent’s
observations are deﬁned, actions are chosen, and the simulator can step. A few key differences are as
follows:

1. The ability to arbitrarily get/set the state of the simulator, a necessary feature for model-based
methods like MPC or motion planning. An important component of this is deﬁning a proper
notion of a state, which is often missing from existing frameworks, as it can include more than
just the position and velocities of the dynamics.

2. Optional, in-place versions for all functions (e.g. getstate!(·)) which store the return value in
a pre-allocated data structure. This eliminates unnecessary memory allocations and garbage
collection, enabling environments to be used in tight, real-time control loops.

4

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

3. An optional "evaluation" metric. Often times reward functions are heavily "shaped" and hard
to interpret. For example, the reward function for bipedal walking may include root pose, ZMP
terms, control costs, etc., while success can instead be simply evaluated by distance of the root
along an axis.

We expect most users will be interested in implementing their own environments, which forms a
crucial part of robotics research. Indeed, different researchers may be interested in different robots
performing different tasks, ranging from whole arm manipulators to legged locomotion to dexterous
anthropomorphic hands. To aid this process, we provide sensible defaults for most of the API, making
it easy to get started and experiment with different environments. The separation of interface and
implementation also allows for other simulators and back-ends (e.g. RigidBodySim.jl or DART) to
be used in lieu of the MuJoCo-based environments we provide, should the user desire.

MuJoCo.jl, LyceumMuJoCo.jl, and LyceumMuJoCoViz.jl MuJoCo.jl is a low-level
Julia wrapper for MuJoCo that has a one-to-one correspondence to MuJoCo 2.0’s C interface and
includes soft body dynamics. All data is memory mapped with no overhead, and named ﬁelds in a Mu-
JoCo.xml ﬁle are exposed to the data structures, enabling ﬁeld access as d.qpos[:, :arm]. We
then build LyceumMuJoCo.jl, the MuJoCo implementation of our AbstractEnvironment
API, on top of MuJoCo.jl. This is the construction of an environment based in MuJoCo, and allows
the user to conﬁgure tasks rewards and programatically modify dynamics before passing the structure
to algorithms for processing. Finally the LyceumMuJoCoViz.jl package visualizes the results
of MuJoCo based models. Data is passed in as a list of trajectories for viewing or control function; a
trained policy or MPC controller, for example, can be passed to the visualizer, which has hooks for
keyboard and mouse interaction. Robots in the real world encounter perturbations and disturbances,
and with LyceumMuJoCoViz.jl the user can interact with the simulated environment to test the
robustness of a controller.

LyceumAI.jl Coupled with these environments is LyceumAI.jl, a collection of algorithms
for robotic control that similarly leverage Julia’s performance and multi-threading abilities. Currently
we provide implementations of "Model Predictive Path Integral Control" (MPPI), a stochastic
shooting method for model-predictive control and Natural Policy Gradient. We compare these
methods with a Python implementation in the next sections. Both of these methods beneﬁt from
multi-threaded rollouts, either with respect to controls or a policy, which can be performed in parallel.
Neural networks and automatic differentiation for objects like control policies or ﬁtted value functions
are handled by Flux.jl and Zygote.jl (Innes et al., 2018; Innes, 2018), which are also Julia based. The
combination of efﬁcient compute utilization, ﬂexible high level programming, and an ecosystem of
tools should allow both robotics and RL researchers to experiment with different robotic systems,
algorithm design, and hopefully deploy to real systems.

4. Benchmark Experiments and Results

We designed our experiments and timing benchmarks to answer the following questions: (a) Do
the implementations of Gym RL environments and algorithms in Lyceum produce comparable
results? (b) Does Lyceum lead to faster environment sampling and experiment turn-around time
when compared to Gym and dm_control?

5

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Experiment Setup All experiments are performed on a 16-core Intel i9-7960X with the CPU
governor pinned at 1.2GHz so as to prevent dynamic scaling or thermal throttling from affecting
results. As Gym and dm_control do not come with built-in support for parallel computing, the
authors implement this functionality using Python’s multiprocessing library as recommended
in several GitHub Issues by the respective library authors. Below we describe the various benchmarks
we considered and their results.

Sampling Throughput
In the ﬁrst benchmark, we study the sampling throughput and parallel
scaling performance of LyceumMuJoCo.jl against Gym, dm_control, and a native C implementa-
tion using an OpenMP thread pool. To do so, we consider various models of increasing complexity:
CartPole, Ant, HERB, and Humanoid. In the ﬁrst experiment, we use all 16 of the avail-
able cores to measure the number of samples we can collect per second. Figure 1 (left) shows the
results, which are presented in two forms: as a fraction of native C’s throughput, and as samples
per second. We see that Lyceum and native C signiﬁcantly outperform Gym and dm_control in all
cases. In particular, for CartPole, Lyceum is more than 200x faster compared to Gym.

In the second experiment, we study how the sampling performance scales with the number of
cores for the various implementations. To do so, we consider the Humanoid model and measure the
number of samples that can be generated per second with varying number of cores. The results are
presented in Figure 1 (right), where we see substantial gains for Lyceum. In particular, the scaling
is near linear with the number of cores for C and Lyceum, while there are diminishing returns for
Gym and dm_control. This is due to inherent multi-threading limitations of (pure) Python. When
using more cores (e.g. on a cluster), the performance difference is likely to be even larger.

Reinforcement Learning with Policy Gradients
In the second benchmark, we compare the
learning curves and wall clock time of Natural Policy Gradient (Kakade, 2002; Rajeswaran et al.,

Figure 1: (Left) Comparison of sampling throughput for models of varying complexity. In the top
left plot, we provide the sampling throughput relative to native C, while in the bottom left
plot, we provide samples per second. This experiment uses all 16 available cores (see main
text for details). We ﬁnd that Lyceum can match the performance of native C, while still
retaining the beneﬁts of writing research code in a high-level language. (Right) Sampling
throughput as a function of threads for the Humanoid model.

6

0.000.250.500.751.00ThroughputRelativetoCSamplingThroughputAcrossEnvironmentsCDMCGymLyceumAntCartpoleHERBHumanoid101103105Samples/Second12481605.0×1041.0×1051.5×105NumberofThreadsSamples/SecondParallelScalingofSamplingThroughputCDMCGymLyceumLYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Figure 2: Reinforcement learning with NPG in the Gym and Lyceum frameworks, training for one
million time-steps. Top row presents environment reward vs experiment (i.e. wall clock
time), and bottom row presents environment reward vs number of simulated timesteps.
Performance of the underlying deterministic policy is reported.

2017), which is closely related to TRPO (Schulman et al., 2015), between Gym and Lyceum. Our
implementation of NPG, closely based on the algorithm as described in Rajeswaran et al. (2017) and
consistent with majority practice in the community, considers 2 layer neural network policies. Details
about hyperparameters are provided in the appendix. We compare based on three representative tasks
(Swimmer, Hopper, and Walker) and ﬁnd that the learning curves match across the two frameworks.
The results are summarized in Figure 2, and we ﬁnd that the performance curves match well. We
note that RL algorithms are known to be sensitive to many implementation details (Henderson et al.,
2017; Ilyas et al., 2018), and thus even approximately matching results is a promising sign for
both the original code base and Lyceum. We expect that with further code-level optimization, the
performance of RL algorithms in Lyceum can match their counterparts in Gym.

Model Predictive Control
In the ﬁnal benchmark, we compare the performance of a model-based
trajectory optimizer on Gym and Lyceum. For this purpose, we consider the Model Predictive
Path Integral (MPPI) algorithm (Williams et al., 2016), which in conjunction with learning based
techniques have demonstrated impressive results in tasks like aggressive driving and dexterous hand
manipulation (Lowrey et al., 2018a; Nagabandi et al., 2019). MPPI is a sampling based algorithm
where different candidate action sequences are considered to generate many potential trajectories

7

0780100200300400Swimmer-v2LyceumGym02970100020003000ExperimentTime(s)Hopper-v2022105001000150020002500Walker-v201×106010020030040001×1060100020003000Samples01×10605001000150020002500LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Task
Hand (gym)
Hand (ours)
Reacher (gym)
Reacher (ours)

Time (s)
40.4
14.3
3.67
0.119

Success (%)
92 ± 5.31 %
88 ± 6.36 %
100%
100%

Figure 3: (Left) Illustration of the 30-DOF in-hand manipulation task with a Shadow Hand (Adroit).
The goal is to manipulate the (blue) pen to match the (green) desired pose. (Middle)
Illustration of the reaching task with a 7-DOF Sawyer arm. Goal is to make the end-
effector (blue) reach the (green) target. (Right) comparison of time taken and success
percentage in gym and Lyceum. Time refers to the time taken to execute a single episode
with the MPPI controller (in MPC mode). Success % measures the number of successful
episodes when the robot is controlled using the MPPI algorithm. 95% conﬁdence intervals
are also reported. See appendix for additional details and hyperparameters.

starting from the current state. Rewards are calculated for each of these trajectories, and the candidate
action sequences are combined with exponentially weighted trajectory rewards.

We consider two tasks for the MPPI comparison: a 7-DOF sawyer arm that reaches various
spatial goals with the end effector, and a 30-DOF in-hand manipulation task where a Shadow
hand (Adroit) (Kumar, 2016) has to perform in-hand manipulation of a pen to match a desired
conﬁguration. We compare the times taken by MPPI to optimize a trajectory and also the fraction
of times MPPI optimized a successful trajectory. The results are provided in Figure 3. In summary,
we ﬁnd that the MPPI success percentage are comparable in both Gym and Lyceum, and the
Lyceum implementation is approx. 30x faster for the Sawyer arm task and 3x faster for the Shadow
Hand task. This trend is consistent with the earlier trend, where the relative differences are larger
for lower dimensional systems with fewer contacts. This is because for complex models with many
contacts like the Shadow Hand, most of the computational work is performed by MuJoCo, thereby
diminishing the impact of overheads in Gym. We also note, however, that we found the performance
scaling with cores to be better in Lyceum compared to Gym, and thus the difference between the
frameworks is likely larger when using more cores (e.g. on a cluster).

5. Conclusion and Future Work

We introduced Lyceum, a new computational ecosystem for robot learning in Julia that provides
the rapid prototyping and ease-of-use beneﬁts of a high-level programming language, yet retaining
the performance of a low-level language like C. We demonstrated that this ecosystem can obtain
substantial speedups compared to existing ecosystems like OpenAI gym and dm_control. We also
demonstrated that this speed up enables faster experimental times for RL and MPC algorithms. In
the future, we hope to port over algorithmic infrastructures like OpenAI’s baselines (Dhariwal et al.,
2017). We also hope to include and support models and environments involving real robots like
Cassie, HERB (Srinivasa et al., 2009), ROBEL (Ahn et al., 2019), and Shadow Hand (Kumar, 2016;
Rajeswaran et al., 2018; Jain et al., 2019), and also support for fast rendering to enable research
integrating perception and control.

8

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Acknowledgements

The authors thank Ben Evans for continued valuable feedback over the course of the project. The
authors also thank Vikash Kumar and Svet Kolev for valuable comments about the paper draft, and
Motoya Ohnishi for early adoption of Lyceum and feedback. This work was (partially) funded by the
National Institute of Health R01 (#R01EB019335), National Science Foundation CPS (#1544797),
National Science Foundation NRI (#1637748), the Ofﬁce of Naval Research, the RCTA, Amazon,
and Honda Research Institute USA.

References

Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. In OSDI, 2016.

Akshay Agrawal, Akshay Naresh Modi, Alexandre Passos, Allen Lavoie, Ashish Agarwal, Asim
Shankar, Igor Ganichev, Josh Levenberg, Mingsheng Hong, Rajat Monga, and Shanqing Cai. Ten-
sorﬂow eager: A multi-stage, python-embedded dsl for machine learning. ArXiv, abs/1903.01855,
2019.

Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and
Vikash Kumar. ROBEL: RObotics BEnchmarks for Learning with low-cost robots. In Conference
on Robot Learning (CoRL), 2019.

Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to
numerical computing. SIAM review, 59(1):65–98, 2017. URL https://doi.org/10.1137/
141000671.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. Openai gym, 2016.

Erwin Coumans. Bullet physics library, 2013.

Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep

reinforcement learning for continuous control. ArXiv, abs/1604.06778, 2016.

Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio
Savarese, and Li Fei-Fei. Surreal: Open-source reinforcement learning framework and robot
manipulation benchmark. In Conference on Robot Learning, 2018.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms
and applications. Technical report, 2018.

9

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.

Deep reinforcement learning that matters. ArXiv, abs/1709.06560, 2017.

Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph,
and Aleksander Madry. Are deep policy gradient algorithms truly policy gradient algorithms?
ArXiv, abs/1811.02553, 2018.

Michael Innes. Don’t unroll adjoint: Differentiating ssa-form programs. CoRR, abs/1810.07951,

2018. URL http://arxiv.org/abs/1810.07951.

Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso,
Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. Fashionable modelling with
ﬂux. CoRR, abs/1811.01457, 2018. URL http://arxiv.org/abs/1811.01457.

Divye Jain, Andrew Li, Shivam Singhal, Aravind Rajeswaran, Vikash Kumar, and Emanuel Todorov.
Learning Deep Visuomotor Policies for Dexterous Hand Manipulation. In International Conference
on Robotics and Automation (ICRA), 2019.

Sham M Kakade. A natural policy gradient. In NIPS, 2002.

Twan Koolen and Robin Deits. Julia for robotics: simulation and real-time control in a high-level
programming language. 2019 International Conference on Robotics and Automation (ICRA),
pages 604–611, 2019.

Vikash Kumar. Manipulators and Manipulation in high dimensional spaces. PhD thesis, Uni-
versity of Washington, Seattle, 2016. URL https://digital.lib.washington.edu/
researchworks/handle/1773/38104.

Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. Numba: A LLVM-based Python JIT Compiler.
In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, LLVM
’15, pages 7:1–7:6, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-4005-2. doi: 10.1145/
2833157.2833162.

Jeongseok Lee, Michael X. Grey, Sehoon Ha, Tobias Kunz, Sumit Jain, Yuting Ye, Siddhartha S.
Srinivasa, Mike Stilman, and Chuanjian Liu. Dart: Dynamic animation and robotics toolkit. J.
Open Source Software, 3:500, 2018.

Kendall Lowrey, Svetoslav Kolev, Jeremy Dao, Aravind Rajeswaran, and Emanuel Todorov. Rein-
forcement learning for non-prehensile manipulation: Transfer from simulation to physical system.
CoRR, abs/1803.10371, 2018a.

Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan
ICLR,

Online, Learn Ofﬂine: Efﬁcient Learning and Exploration via Model-Based Control.
abs/1811.01848, 2018b.

Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is

competitive for reinforcement learning. In NeurIPS, 2018.

Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion
planning that transfers to physical humanoids. 2015 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pages 5307–5314, 2015.

10

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Oﬁr Nachum, Michael J. Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent manipula-

tion via locomotion using hierarchical sim2real. ArXiv, abs/1908.05224, 2019.

Anusha Nagabandi, Kurt Konoglie, Sergey Levine, and Vikash Kumar. Deep Dynamics Models for

Learning Dexterous Manipulation. In Conference on Robot Learning (CoRL), 2019.

OpenAI. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning

robust neural network policies using model ensembles. In ICLR, 2016.

Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards Generalization

and Simplicity in Continuous Control. In NIPS, 2017.

Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforcement
Learning and Demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018.

Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image ﬂight without a single real image.

ArXiv, abs/1611.04201, 2016.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region

policy optimization. In ICML, 2015.

Siddhartha S. Srinivasa, Dave Ferguson, Casey Helfrich, Dmitry Berenson, Alvaro Collet, Rosen
Diankov, Garratt Gallagher, Geoffrey A. Hollinger, James J. Kuffner, and Michael Vande Weghe.
Herb: a home exploring robotic butler. Autonomous Robots, 28:5–20, 2009.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin
Riedmiller. DeepMind control suite. Technical report, DeepMind, January 2018. URL https:
//arxiv.org/abs/1801.00690.

Josh Tobin, Lukas Biewald, Rocky Duan, Marcin Andrychowicz, Ankur Handa, Vikash Kumar,
Bob McGrew, Alex Ray, Jonas Schneider, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.
Domain randomization and generative models for robotic grasping. 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 3482–3489, 2017.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.

In IROS, 2012.

Yusuke Urakami, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca Rigazio, and Pieter Abbeel.
Doorgym: A scalable door opening environment and baseline agent. ArXiv, abs/1908.01887, 2019.

Grady Williams, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. Aggres-
sive driving with model predictive path integral control. In Robotics and Automation (ICRA), 2016
IEEE International Conference on, pages 1433–1440. IEEE, 2016.

11

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Appendix A. AbstractEnvironment Interface

For a more thorough description of the AbstractEnvironment interface and more, see the documenta-
tion at docs.lyceum.ml/dev.

statespace(env)
getstate(env)
getstate!(s, env)
setstate!(env, s)

obsspace(env)
getobs(env)
getobs!(o, env)

actionspace(env)
getaction(env)
getaction!(a, env)
setaction!(env, a)

rewardspace(env)
getreward(s, a, o, env)

evalspace(env)
geteval(s, a, o, env)

# Task evaluation metric
# that can differ from reward

reset!(env)
randreset!(env)

# Reset to a fixed initial state.
# Reset to a random initial state.

step!(env)
isdone(env)

# Step the environment.
# return `true` if `env` terminated early

Appendix B. Example MuJoCo Environment

The following is an example of a the OpenAI Gym hopper environment ported to Lyceum. Functions
that are not deﬁned from the previous section use the default behaviors of the AbstractEnvironment
type.

# We begin by importing functions from LyceumMuJoCo we wish
# to extend with our environments specifications. We define
# the functions to work with our environment struct.
import LyceumMujoco: getobs!, randreset!, geteval, step!
struct HopperV2 <: AbstractEnvironment

# This thread safe data structure
# stores the simulator and other
# user-desired values

sim::MJSim

function HopperV2()

new(MJSim("hopper.xml"))

end

end

function getobs!(o, env::HopperV2) # writes data to pre-allocated o

nq = env.sim.m.nq
qpos = env.sim.d.qpos[2:end]
qvel = copy(env.sim.d.qvel)
clamp!(qvel, -10, 10)

# 'env' will be the struct above,
# and accessing its fields provides the
# desired observations

12

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

copyto!(o, vcat(qpos, qvel))
return o

end

function randreset!(env::HopperV2) # Reset environment to a random state

reset!(env)
dist = Uniform(-0.005, 0.005)
env.sim.d.qpos .+= rand.(dist) # to fill the qpos, qvel vectors
env.sim.d.qvel .+= rand.(dist) # '.' notation vectorizes call to `rand`
forward!(env.sim)
return env

# MuJoCo's forward dynamics
# to propagate changes

# Uniform sampler, called multiple times

end

function geteval(s, a, o, env::HopperV2) # We evaluate distance along x axis

statespace(env)(s).qpos[1]

end

function getreward(s, a, o, env::HopperV2)

shapedstate = statespace(env)(s)
qpos = shapedstate.qpos
qvel = shapedstate.qvel

x0 = qpos[1]
step!(env.sim, a)
x1, height, ang = qpos[1:3]

alive_bonus = 1.0
reward = (x0 - x1) / dt(env)
reward += alive_bonus
reward -= 1e-3 * sum(x->x^2, a) # lambda function squares `a`
return reward

end

# # # # # # # # # # # # # # # # # # # # # #
# After creation of the environment we can use it with LyceumAI.
# we first include LyceumAI and other packages.
using LyceumAI, Flux

hop = HopperV2()
dobs, dact = length(obsspace(hop)), length(actionspace(hop))

# A policy and value function are created using helper
# functions built on Flux.jl
policy = DiagGaussianPolicy(

multilayer_perceptron(dobs, 64, 64, dact),
ones(dact) .*= -0.5

)

value = multilayer_perceptron(dobs, 128, 128, 1)
valueloss(bl, X, Y) = mse(vec(bl(X)), vec(Y))

# FluxTrainer is thing you can iterate on. The result at each
# loop is passed to stopcb below, so you can quit after
# a number of epochs, convergence, both, or never
valuetrainer = FluxTrainer(

13

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

optimiser = ADAM(1e-3),
szbatch = 64,
lossfn = valueloss,
stopcb = s->s.nepochs > 2

)

# The LyceumAI NaturalPolicyGradient is an iterator, where each loop
# the data is returned to the for-loop below. We construct nthreads
# number of Hopper Environments to be parallel evaluated.
npg = NaturalPolicyGradient(

(HopperV2() for _=1:Threads.nthreads()),
policy,
value,
gamma = 0.995,
gaelambda = 0.97,
valuetrainer,
Hmax=1000,
norm_step_size=0.1,
N=10000

)

# We iterate on NPG for 100 iterations, printing useful information
# every 25 iterations.
for (i, state) in enumerate(npg)

if i > 100

break

end
if mod(i, 25) == 0

println("stocreward = ", state.stoctraj_reward)

end

end

14

LYCEUM: AN EFFICIENT AND SCALABLE ECOSYSTEM FOR ROBOT LEARNING

Appendix C. Details on MPC experiments

For the MPC comparison with MPPI, we considered two environments. A 7-DOF Sawyer robot
reaching various spatial goals with the end effector, and a 30-DOF in-hand manipulation task where
a Shadow Hand has to manipulate a pen to match a desired orientation. Both tasks are episodic,
where at the start of an episode, a random initial conﬁguration and a random target conﬁguration
are generated. Each episode is 75 time-steps. The speciﬁc MPPI algorithm we used was based
on Lowrey et al. (2018a), where the authors ﬁrst observed that correlated noise sequences were
important for hand manipulation tasks. Our observations are consistent with this ﬁnding. The main
hyper-parameters used are summarized in the below table.

Parameter
Planning horizon
# trajectories sampled
Temperature
Smoothing parameters

Table 1: MPC experiment parameters
Shadow Hand experiment
32
160
1.0
β0 = 0.25, β1 = 0.8

Sawyer experiment
16
30
5.0
β0 = 0.25, β1 = 0.8

Appendix D. Details on RL experiments

As a representative RL experiment, we use the NPG algorithm and compared Lyceum with Gym.
We study the learning curve as a function of both the number of environment interactions as well
as wall clock time. As a function of environment interactions, we found Gym and Lyceum to
be comparable (as expected), however Lyceum was substantially faster in wall clock time. The
hyper-parameters for the RL experiment are mentioned in Table 2.

Table 2: Parameters for the RL (NPG) experiment

Parameter
# NPG steps
Samples per NPG step
NPG step size (normalized)
Policy size
Value function size
Discount (γ)
GAE (λ)

Value
100
10,000
0.1
(64, 64)
(128, 128)
0.995
0.97

15

