1
2
0
2

t
c
O
7
2

]

G
L
.
s
c
[

2
v
6
7
7
2
0
.
7
0
1
2
:
v
i
X
r
a

Counterfactual Explanations in
Sequential Decision Making Under Uncertainty

Stratis Tsirtsis§, Abir De∗, and Manuel Gomez Rodriguez§

§Max Planck Institute for Software Systems, {stsirtsis, manuelgr}@mpi-sws.org
∗IIT Bombay, abir@cse.iitb.ac.in

Abstract

Methods to ﬁnd counterfactual explanations have predominantly focused on one-step decision making
processes. In this work, we initiate the development of methods to ﬁnd counterfactual explanations for
decision making processes in which multiple, dependent actions are taken sequentially over time. We
start by formally characterizing a sequence of actions and states using ﬁnite horizon Markov decision
processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally
state the problem of ﬁnding counterfactual explanations for sequential decision making processes. In our
problem formulation, the counterfactual explanation speciﬁes an alternative sequence of actions diﬀering
in at most k actions from the observed sequence that could have led the observed process realization to a
better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to
build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation
on every possible realization of the counterfactual environment dynamics. We validate our algorithm
using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual
explanations our algorithm ﬁnds can provide valuable insights to enhance sequential decision making
under uncertainty.

1

Introduction

In recent years, there has been a rising interest on the potential of machine learning models to assist and
enhance decision making in high-stakes applications such as justice, education and healthcare [1–3]. In this
context, machine learning models and algorithms have been used not only to predict the outcome of a decision
making process from a set of observable features but also to ﬁnd what would have to change in (some of)
the features for the outcome of a speciﬁc process realization to change. For example, in loan decisions, a
bank may use machine learning both to estimate the probability that a customer repays a loan and to ﬁnd
by how much a customer may need to reduce her credit card debt to increase the probability of repayment
over a given threshold. Here, our focus is on the latter, which has been often referred to as counterfactual
explanations.

The rapidly growing literature on counterfactual explanations has predominantly focused on one-step
decision making processes as the one described above [4–6]. In such settings, the probability that an outcome
occurs is typically estimated using a supervised learning model and ﬁnding counterfactual explanations
reduces to a search problem across the space of features and model predictions [7–11]. Moreover, it has been
argued that, to obtain counterfactual explanations that are actionable and have the predicted eﬀect on the
outcome, one should favor causal models [12–14]. In this work, our goal is instead to ﬁnd counterfactual
explanations for decision making processes in which multiple, dependent actions are taken sequentially over
time. In this setting, the (ﬁnal) outcome of the process depends on the overall sequence of actions and the
counterfactual explanation speciﬁes an alternative sequence of actions diﬀering in at most k actions from the
observed sequence that could have led the process realization to a better outcome. For example, in medical

1

 
 
 
 
 
 
treatment, assume a physician takes a sequence of actions to treat a patient but the patient’s prognosis does
not improve, then a counterfactual explanation would help the physician understand how a small number
of actions taken diﬀerently could have improved the patient’s prognosis. However, since there is (typically)
uncertainty on the counterfactual dynamics of the environment, there may be a diﬀerent counterfactual
explanation that is optimal under each possible realization of the counterfactual dynamics. Moreover, since,
in many realistic scenarios, a decision maker needs to decide among a small number of actions on the basis of
a few observable covariates, which are often discretized into (percentile) ranges, in this work, we focus on
decision making processes where the state and action spaces are discrete and low-dimensional.

The work most closely related to ours, which lies within the area of machine learning for healthcare,
has achieved early success at using machine learning to enhance sequential (clinical) decisions [15–18].
However, rather than ﬁnding counterfactual explanations, this line of work has predominantly focused on
using reinforcement learning to design better treatment policies. A notable exception is by Oberst and
Sontag [18], which introduces an oﬀ-policy evaluation procedure for highlighting speciﬁc realizations of a
sequential decision making process where a policy trained using reinforcement learning would have led to a
substantially diﬀerent outcome. While our work builds upon their modeling framework, we do not focus on
oﬀ-policy evaluation but, instead, on the generation of (better) alternative action sequences as a means of
explanation for the process’s outcome. Therefore, we see their contributions as complementary to ours.

More broadly, our work is not the ﬁrst to use counterfactual reasoning in the context of sequential decision
making in the machine learning literature [19, 20]. However, previous work has used counterfactual reasoning
either to learn optimal policies using limited observational data [19] or to explain the action choices of a
reinforcement learning agent [20]. In contrast, our work uses a causal model of the environment to compute
alternative action sequences, close to the observed one, which, in retrospect, could have improved the outcome
of the decision making process. Refer to Appendix A for a discussion of further related work.
Our contributions. We start by formally characterizing a sequence of (discrete) actions and (discrete) states
using ﬁnite horizon Markov decision processes (MDPs). Here, we model the transition probabilities between a
pair of states, given an action, using the Gumbel-Max structural causal model [18]. This model has been shown
to have a desirable counterfactual stability property and, given a sequence of actions and states, it allows us
to reliably estimate the counterfactual outcome under an alternative sequence of actions. Building upon this
characterization, we formally state the problem of ﬁnding the counterfactual explanation for an observed
realization of a sequential decision making process as a constrained search problem over the set of alternative
sequences of actions diﬀering in at most k actions from the observed sequence. Then, we present a polynomial
time algorithm based on dynamic programming that ﬁnds a counterfactual policy that is guaranteed to
always provide the optimal counterfactual explanation on every possible realization of the counterfactual
transition probability induced by the observed process realization. Finally, we validate our algorithm using
both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations
can provide valuable insights to enhance sequential decision making under uncertainty1.

2 Characterizing Sequential Decision Making using Causal Markov

Decision Processes

Our starting point is the following stylized setting, which resembles a variety of real-world sequential decision
making processes. At each time step t ∈ {0, . . . , T − 1}, the decision making process is characterized by a
state st ∈ S, where S is a space of n states, an action at ∈ A, where A is a space of m actions, and a reward
R(st, at) ∈ R. Moreover, given a realization of a decision making process τ = {(st, at)}T −1
t=0 , the outcome of
the decision making process o(τ ) = (cid:80)

t R(st, at) is given by the sum of the rewards.

Given the above setting, we characterize the relationship between actions, states and outcomes using ﬁnite
horizon Markov decision processes (MDPs). More speciﬁcally, we consider an MDP M = (S, A, P, R, T ),
where S is the state space, A is the set of actions, P denotes the transition probability P (St+1 = st+1 | St =
st, At = at), R denotes the immediate reward R(st, at), and T is the time horizon. While this characterization

1Our code is accessible at https://github.com/Networks-Learning/counterfactual-explanations-mdp.

2

Figure 1: Structural causal model C for a Markov decision process M. Green boxes represent endogenous
random variables and pink boxes represent exogenous noise variables. The value of each endogenous variable
is given by a function of the values of its ancestors in the structural causal model, as deﬁned by Eq. 1. The
value of each exogenous noise variable is sampled independently from a given distribution. An intervention
do(At = a) breaks the dependence of the variable At from its ancestors (highlighted by dotted lines) and
sets its value to a. After observing an event St+1 = st+1, St = st, At = at, a counterfactual prediction can be
thought of as the result of an intervention do(At = a) in a modiﬁed SCM where Ut takes values ut from a
posterior distribution with support such that st+1 = gS(st, at, ut).

is helpful to make predictions about future states and design action policies [21], it is not suﬃcient to make
counterfactual predictions, e.g., given a realization of a decision making process τ = {(st, at)}T −1
t=0 , we cannot
know what would have happened if, instead of taking action at at time t, we had taken action a(cid:48) (cid:54)= at. To be
able to overcome this limitation, we will now augment the above characterization using a particular class of
structural causal model (SCM) [22, 23] with desirable properties.
Let C be a structural causal model deﬁned by the assignments

St+1 = gS(St, At, Ut)

and At = gA(St, Vt),

(1)

where Ut and Vt are n- and m-dimensional independent noise variables, respectively, and gS and gA are two
given functions, and let P C be the distribution entailed by C. Then, as argued by Buesing et al. [19], we can
always ﬁnd a distribution for the noise variables and a function gS so that the transition probability of the
MDP of interest is given by the following interventional distribution over the SCM C:

P (St+1 = st+1 | St = st, At = at) = P C ; do(At=at)(St+1 = st+1 | St = st)

(2)

where do(At = at) denotes a (hard) intervention in which the second assignment in Eq. 1 is replaced by the
value at.

Under this view, given an observed realization of a decision making process τ = {(st, at)}T −1

t=0 , we can
compute the posterior distribution P C | St=st,St+1=st+1,At=at(Ut) of each noise variable Ut. Building on the
conditional density function of this posterior distribution, which we denote as f C | St=st,St+1=st+1,At=at
(u),
we can deﬁne a (non-stationary) counterfactual transition probability

Ut

Pτ,t(St+1 = s(cid:48) | St = s, At = a) = P C | St=st,St+1=st+1,At=at ; do(At=a)(St+1 = s(cid:48) | St = s)

=

(cid:90)

Rn

P C | St=st,St+1=st+1,At=at ; do(At=a)(St+1 = s(cid:48) | St = s, Ut = u)

× f C | St=st,St+1=st+1,At=at ; do(At=a)

Ut

(u)du

1[s(cid:48) = gS(s, a, u)] · f C | St=st,St+1=st+1,At=at
Ut

(u)du

(a)
=

(cid:90)

Rn

= EUt | St=st,St+1=st+1,At=at[1[s(cid:48) = gS(s, a, Ut)]],

(3)

where, in (a), we drop the do(·) because Ut and At are independent in the modiﬁed SCM. Importantly, the
above counterfactual transition probability allows us to make counterfactual predictions, e.g., given that, at
time t the state was st and, at time t + 1, the process transitioned to state st+1 after taking action at, what

3

would have been the probability of transitioning to state s(cid:48) after taking action a (cid:54)= at if the state had been s
at time t. Refer to Figure 1 for a visual description of our model and the notion of counterfactual predictions.
However, for state variables taking discrete values, the posterior distribution of the noise may be non-
identiﬁable without further assumptions, as argued by Oberst and Sontag [18]. This is because there may
be several noise distributions and functions gS which give interventional distributions consistent with the
MDP’s transition probabilities but result in diﬀerent counterfactual transition distributions. To avoid these
non-identiﬁability issues, we follow Oberst and Sontag and restrict our attention to the class of Gumbel-Max
SCMs, i.e.,

St+1 = gS(St, At, Ut) := argmax

s∈S

{log P (St+1 = s | St, At) + Ut,s},

(4)

where Ut,s ∼ Gumbel(0, 1) and P (· | St, At) is the transition probability of the MDP. More speciﬁcally, this
class of SCMs has been shown to satisfy a desirable counterfactual stability property, which goes intuitively
as follows. Assume that, at time t, the process transitioned from state st to state st+1 after taking action at.
Then, in a counterfactual scenario, it is unlikely that, at time t, the process would transition from a state s to
a state s(cid:48) other than st+1—the factual one—unless choosing an action a that decreases the relative chances
of st+1 compared to the other states. More formally, for a model satisfying counterfactual stability, given
τ = {(st, at)}T −1

t=0 , for all s, s(cid:48) with s(cid:48) (cid:54)= st+1 such that

P (St+1 = st+1 | St = s, At = a)
P (St+1 = st+1 | St = st, At = at)

≥

P (St+1 = s(cid:48) | St = s, At = a)
P (St+1 = s(cid:48) | St = st, At = at)

,

it holds that Pτ,t(St+1 = s(cid:48) | St = s, At = a) = 0. In practice, in addition to solving the non-identiﬁability
issues, the use of Gumbel-Max SCMs allows for an eﬃcient procedure to sample from the corresponding noise
posterior distribution P C | St=st,St+1=st+1,At=at(Ut), described elsewhere [18, 24], and given a set of d samples
from the noise posterior distribution, we can compute an unbiased ﬁnite sample Monte-Carlo estimator for
the counterfactual transition probability, as deﬁned in Eq. 3, as follows:

Pτ,t(St+1 = s(cid:48) | St = s, At = a) ≈

1
d

(cid:88)

j∈[d]

1[s(cid:48) = gS(s, a, uj)]

(5)

3 Counterfactual Explanations in Markov Decision Processes

Inspired by previous work on counterfactual explanations in supervised learning [7, 8], we focus on counter-
factual outcomes that could have occurred if the alternative action sequence was “close” to the observed one.
However, since in our setting, there is uncertainty on the counterfactual dynamics of the environment, we will
look for a non-stationary counterfactual policy π that, under every possible realization of the counterfactual
transition probability deﬁned in Eq. 3, is guaranteed to provide the optimal alternative sequence of actions
diﬀering in at most k actions from the observed one.

More speciﬁcally, let τ = {(st, at)}T −1

t=0 be an observed realization of a decision making process characterized
by a Markov decision process (MDP) M = (S, A, P, R, T ) with a transition probability deﬁned via a Gumbel-
Max structural causal model (SCM), as described in Section 2. Then, to characterize the eﬀect that any
alternative action sequence would have had on the outcome of the above process realization, we start by
building a non-stationary counterfactual MDP Mτ = (S +, A, P +
τ , R+, T ). Here, S + = S ×{0, . . . , T −1} is an
enhanced state space such that each s+ ∈ S + corresponds to a pair (s, l) indicating that the original decision
making process would have been at state s ∈ S had already taken l actions diﬀerently from the observed
sequence. Following this deﬁnition, R+ denotes the reward function which we deﬁne as r+((s, l), a) = R(s, a)
for any (s, l) ∈ S + and a ∈ A, i.e., the counterfactual rewards remain independent of the number of
modiﬁcations in the action sequence. Lastly, let Pτ be the counterfactual transition probability, as deﬁned by
Eq. 3. Then, the transition probability P +
τ

for the enhanced state space is deﬁned as:

P +
τ,t

(cid:0)S+

t+1 = (s(cid:48), l(cid:48)) | S+

t = (s, l) , At = a(cid:1) =

Pτ,t (St+1 = s(cid:48) | St = s, At = a)

if (a = at ∧ l(cid:48) = l) ∨ (a (cid:54)= at ∧ l(cid:48) = l + 1)
otherwise,





0

4

ALGORITHM 1: It samples a counterfactual explanation from the counterfactual policy π

Input: counterfactual policy π, horizon T , counterfactual transition probability Pτ , reward function
R, initial state s0.
s(cid:48)
0 ← s0
l0 ← 0
reward ← 0
for t = 0, . . . , T − 1 do

a(cid:48)
t ← π((s(cid:48)
t, lt), t)
reward ← reward + R(s(cid:48)
if t (cid:54)= T − 1 then

t, a(cid:48)
t)

s(cid:48)
t+1 ∼ Pτ,t(St+1 | St = s(cid:48)
if a(cid:48)

t (cid:54)= at then
lt+1 ← lt + 1

t, At = a(cid:48)
t)

else

lt+1 ← lt

end

end

end
t, lt), a(cid:48)
τ (cid:48) ← {((s(cid:48)
o(τ (cid:48)) ← reward
Return τ (cid:48), o(τ (cid:48))

t)}T −1
t=0

where note that the dynamics of the original states s are equivalent both under P +
τ,t and Pτ,t, however,
under P +
τ,t, we also keep track of the number of actions diﬀering from the observed actions. Now, let
π : S + × {0, . . . , T − 1} → A be a policy that deterministically decides about the counterfactual action a(cid:48)
t
that should have been taken if the process’s enhanced state had been s+
t, lt), i.e., the counterfactual
state at time t was s(cid:48)
t after performing lt action changes. Then, given such a counterfactual policy π, we can
compute the corresponding average counterfactual outcome as follows:

t = (s(cid:48)

¯oπ(τ ) := E

τ (cid:48)∼P +

τ | s+

0 =(s0,0)

(cid:35)

r+((s(cid:48)

t, lt), a(cid:48)
t)

(cid:34)T −1
(cid:88)

t=0

(6)

t)}T −1

t, lt), a(cid:48)

where τ (cid:48) = {((s(cid:48)
t =
π((s(cid:48)
t, lt), t) and the expectation is taken over all the realizations induced by the transition probability P +
τ
and the policy π. Here, note that, if π((st, 0), t) = at for all t ∈ {0, . . . , T − 1}, then ¯oπ(τ ) = o(τ ) matches
the outcome of the observed realization.

t=0 is a realization of the non-stationary counterfactual MDP Mτ with a(cid:48)

Then, our goal is to ﬁnd the optimal counterfactual policy π∗

τ that maximizes the counterfactual outcome
subject to a constraint on the number of counterfactual actions that can diﬀer from the observed ones, i.e.,

maximize
π

¯oπ(τ )

subject to

T −1
(cid:88)

t=0

1[at (cid:54)= a(cid:48)

t] ≤ k ∀τ (cid:48) ∼ P +
τ

(7)

1, . . . , a(cid:48)

where a(cid:48)
T −1 is one realization of counterfactual actions and a1, . . . , aT −1 are the observed actions.
The constraint guarantees that any counterfactual action sequence induced by the counterfactual transition
probability P +
τ and the counterfactual policy π can diﬀer in at most k actions from the observed sequence.
Finally, once we have found the optimal policy π∗
τ , we can sample a counterfactual realization of the process
and the optimal counterfactual explanation using Algorithm 1.

5

ALGORITHM 2: It returns the optimal counterfactual policy and its average counterfactual
outcome

Input: States S, actions A, realization τ , horizon T , counterfactual transition probability Pτ , reward
function R, constraint k.
Initialize: h(s, r, c) ← 0,
for r = 1, . . . , T do
for s ∈ S do

s ∈ S, r = 0, . . . , T, c = 0, . . . , k.

/* Base cases:

No action changes left (Eq. 9) */

h(s, r, 0) ← h(s, r, 0) + Pτ,T −r(s(cid:48) | s, aT −r)h(s(cid:48), r − 1, 0)

/* Take the observed action */

/* Compute the first part of Eq. 8 */

reward ← reward + Pτ,T −r(s(cid:48) | s, aT −r)h(s(cid:48), r − 1, c)

h(s, r, 0) ← R(s, aT −r) ;
for s(cid:48) ∈ S do

end
π∗
τ ((s, k), T − r) ← aT −r ;

end

end
for r = 1, . . . , T do

for c = 1, . . . , k do
for s ∈ S do

reward ← R(s, aT −r) ;
for s(cid:48) ∈ S do

end
best reward ← reward
best action ← aT −r
for a ∈ A \ {aT −r} do

reward alt ← R(s, a) ;
for s(cid:48) ∈ S do

/* Compute the second part of Eq. 8 */

reward alt ← reward alt + Pτ,T −r(s(cid:48) | s, a)h(s(cid:48), r − 1, c − 1)

end
if reward alt > best reward then
best reward ← reward alt
best action ← a

end

end
h(s, r, c) ← best reward
π∗
τ ((s, k − c), T − r) ← best action ;

end

end

end
Return π∗

τ , h(s0, T, k)

/* Take the action maximizing Eq. 8 */

4 Finding Optimal Counterfactual Explanations via Dynamic Pro-

gramming

To solve the problem deﬁned by Eq. 7, we break the problem into several smaller sub-problems. Here, the
key idea is to compute the counterfactual policy values that lead to the optimal counterfactual outcome
recursively by expanding the expectation in Eq. 6 for one time step.

We start by computing the highest average cumulative reward h(s, r, c) that one could have achieved in
the last r steps of the decision making process, starting from state ST −r = s, if at most c actions had been

6

diﬀerent to the observed ones in those last steps. For c > 0, we proceed recursively:

(cid:32)

h(s, r, c) = max

R(s, aT −r) +

(cid:88)

s(cid:48)∈S

Pτ,T −r(s(cid:48) | s, aT −r)h(s(cid:48), r − 1, c),

(cid:34)

max
a∈A : a(cid:54)=aT −r

R(s, a) +

(cid:88)

s(cid:48)∈S

Pτ,T −r(s(cid:48) | s, a)h(s(cid:48), r − 1, c − 1)

,

(8)

(cid:35)(cid:33)

and, for c = 0, we trivially have that:

h(s, r, 0) = R(s, aT −r) +

(cid:88)

s(cid:48)∈S

Pτ,T −r(s(cid:48) | s, aT −r)h(s(cid:48), r − 1, 0),

(9)

with s ∈ S, r ∈ {1, . . . , T }, c ∈ {1, . . . , k}, and h(s, 0, c) = 0 for all s and c. In Eq. 8, the ﬁrst parameter of
the outer maximization corresponds to the case where, at time T − r, the observed action aT −r is taken and
the second parameter corresponds to the case where, instead of the observed action, the best alternative
action is taken.

By deﬁnition, we can easily conclude that h(s0, T, k) is the average counterfactual outcome of the optimal
counterfactual policy π∗
τ , i.e., the objective value of the solution to the optimization problem deﬁned by Eq. 7,
and we can recover the optimal counterfactual policy π∗
τ by keeping track of the action chosen at each recursive
step in Eq. 8 and 9. The overall procedure, summarized by Algorithm 2, uses dynamic programming—it ﬁrst
computes the values h(s, 1, c) for all s and c and then proceeds with the rest of computations in a bottom-up
fashion—and has complexity O(n2mT k). Finally, we have the following proposition (proven by induction in
Appendix B):

Proposition 1 The counterfactual policy π∗
problem deﬁned by Eq. 7.

τ returned by Algorithm 2 is the solution to the optimization

5 Experiments on Synthetic Data

In this section, we evaluate Algorithm 2 on realizations of a synthetic decision making process2. To this end,
we ﬁrst look into the average outcome improvement that could have been achieved if at most k actions had
been diﬀerent to the observed ones in every realization, as dictated by the optimal counterfactual policy.
Then, we investigate to what extent the level of uncertainty of the decision making process inﬂuences the
average counterfactual outcome achieved by the optimal counterfactual policy as well as the number of
distinct counterfactual explanations it provides.
Experimental setup. We characterize the synthetic decision making process using an MDP with states
S = {0, . . . , n − 1} and actions A = {0, . . . , m − 1}, where n = 20 and m = 10, and time horizon T = 20. For
each state s and action a, we set the immediate reward equal to R(s, a) = s, i.e., the higher the state, the
higher the reward. To set the values of the transition probabilities P (St+1 | St = st, At = at), we proceed
as follows. First we pick one s(cid:63) ∈ S uniformly at random and we set a weight ws(cid:63) = 1 and then, for the
remaining states s ∈ S \ s(cid:63), we sample weights ws ∼ U [0, α], where α ≤ 1. Next, for all s ∈ S, we set
P (St+1 = s | St = st, At = at) = ws/ (cid:80)
s(cid:48)∈S ws(cid:48). It is easy to check that, for each state-action pair (st, at) at
time t, st+1 = s(cid:63) is most likely to be observed in the next timestep t + 1. The parameter α controls the level
of uncertainty.

Then, we compute the optimal policy that maximizes the average outcome of the decision making process
by solving Bellman’s equation using dynamic programming [21] and use this policy to sample the (observed)
realizations as follows. For each realization, we start from a random initial state s0 ∈ S and, at each time step
t, we pick the action indicated by the optimal policy with probability 0.95 and a diﬀerent action uniformly at
random with probability 0.05. This leads to action sequences that are slightly suboptimal in terms of average

2 All experiments were performed on a machine equipped with 48 Intel(R) Xeon(R) 3.00GHz CPU cores and 1.5TB memory.

7

(a) k = 2

(b) k = 5

(c) k = 10

τ (τ )
Figure 2: Empirical distribution of the relative diﬀerence between the average counterfactual outcome ¯oπ∗
achieved by the optimal counterfactual policy π∗
τ (τ ) − o(τ ))/o(τ )],
in a synthetic decision making process. In all panels, we set n = 20, m = 10, α = 0.4, d = 1,000 and
estimate the distributions using 500 realizations from 10 diﬀerent instances of the decision making process
(50 realizations per instance), each with diﬀerent ws.

τ and the observed outcome o(τ ), i.e., P[(¯oπ∗

(a) Eτ [¯oπ∗

τ

(τ )] vs. k

(b) Eτ [(¯oπ∗

τ

(τ ) − o(τ ))/o(τ )] vs. α

(c) # of uniq. explanations vs. k

Figure 3: Inﬂuence that the level of uncertainty of a synthetic decision making process has on the average
τ (τ ) achieved by the optimal counterfactual policy π∗
counterfactual outcome ¯oπ∗
τ as well as on the number
of distinct counterfactual explanations π∗
τ provides. In all panels, we set n = 20, m = 10 and d = 1,000
and, in each experiment, use 500 realizations from 10 diﬀerent instances of the decision making process
(50 realizations per instance), each with diﬀerent ws. In panel (c), for each realization, we sample 100
counterfactual realizations and compute the average number of unique explanations across realizations.
Shaded regions correspond to 95% conﬁdence intervals.

outcome3. Finally, to compute the counterfactual transition probability Pτ,t for each observed realization τ ,
we follow the procedure described in Section 2 with d = 1,000 samples for each noise posterior distribution4.
Results. We ﬁrst measure to what extent the counterfactual explanations provided by the optimal coun-
terfactual policy π∗
τ would have improved the outcome of the decision making process. To this end, for
each observed realization τ , we compute the relative diﬀerence between the average optimal counterfactual
outcome and the observed outcome, i.e., (¯oπ∗
τ (τ ) − o(τ ))/o(τ ). Figure 2 summarizes the results for diﬀerent
values of k. We ﬁnd that the relative diﬀerence between the average counterfactual outcome and the observed
outcome is always positive, i.e., the sequence of actions speciﬁed by the counterfactual explanations would
have led the process realization to a better outcome in expectation. However, this may not come as a surprise
given that the counterfactual policy π∗
τ is optimal, as shown in Proposition 1. In this context, note that, in
the worst case, the counterfactual policy π∗
τ would trivially repeat the observed action sequence, leading to
an average counterfactual outcome equal to the observed outcome with probability 1. Moreover, we observe

3We introduce this randomization to emulate slightly suboptimal (human) policies. However, note that an observed action

sequence may be suboptimal in retrospect even if it was picked using an optimal policy.

4For the estimation of the counterfactual transition probabilities Pτ of a single realization τ , we report an average execution

time of 62 seconds.

8

0%5%10%15%20%25%Averagecounterfactualimprovement020406080#ofrealizations0%5%10%15%20%25%Averagecounterfactualimprovement020406080#ofrealizations0%5%10%15%20%25%Averagecounterfactualimprovement020406080#ofrealizations0246810k250300350Averagecounterfactualoutcomeα=0.00α=0.01α=0.05α=0.10α=0.50α=1.000.00.20.40.60.81.0α2%5%8%10%Averagecounterfactualimprovementk=2k=5k=100246810k0255075100#ofuniq.explanationsα=0.00α=0.01α=0.05α=0.10α=0.50α=1.00that, as the sequences of actions speciﬁed by the counterfactual explanations diﬀer more from the observed
actions (i.e., k increases), the improvement in terms of expected outcome increases.

Next, we investigate how the level of uncertainty α of the decision making process inﬂuences the average
counterfactual outcome achieved by the optimal counterfactual policy π∗
τ as well as the number of distinct
counterfactual explanations π∗
τ provides. Figure 3 summarizes the results, which reveal several interesting
insights. As the level of uncertainty α increases, the average counterfactual outcome decreases, as shown
in panel (a), however, the relative diﬀerence with respect to the observed outcome increases, as shown in
panel (b). This suggest that, under high level of uncertainty, the counterfactual explanations may be more
valuable to a decision maker who aims to improve her actions over time. However, in this context, we also ﬁnd
that, under high levels of uncertainty, the number of distinct counterfactual explanations increases rapidly
with k. As a result, it may be preferable to use relatively low values of k to be able to eﬀectively show the
counterfactual explanations to a decision maker in practice.

6 Experiments on Real Data

In this section, we evaluate Algorithm 2 using real patient data from a series of cognitive behavioral therapy
sessions. Similarly as in Section 5, we ﬁrst measure the average outcome improvement that could have been
achieved if at most k actions had been diﬀerent to the observed ones in every therapy session, as dictated
by the optimal counterfactual policy. Then, we look into individual therapy sessions and showcase how
Algorithm 2, together with Algorithm 1, can be used to highlight speciﬁc patients and actions of interest for
closer inspection5. Appendix D contains additional experiments benchmarking the optimal counterfactual
policy against several baselines6.
Experimental setup. We use anonymized data from a clinical trial comparing the eﬃcacy of hypnotherapy
and cognitive behavioral therapy [25] for the treatment of patients with mild to moderate symptoms of
major depression7. In our experiments, we use data from the 77 patients who received manualized cognitive
behavioral therapy, which is one of the gold standards in depression treatment. Among these patients, we
discard four of them because they attended less than 10 sessions. Each patient attended up to 20 weekly
therapy sessions and, for each session, the dataset contains the theme of discussion, chosen by the therapist
from a pre-deﬁned set of themes (e.g., psychoeducation, behavioural activation, cognitive restructuring
techniques). Additionally, a severity score is included, based on a standardized questionnaire [27], ﬁlled by
the patient at each session, which assesses the severity of depressive symptoms. For more details about the
severity score and the pre-deﬁned set of discussion themes refer to Appendix C.

To derive the counterfactual transition probability for each patient, we start by creating an MDP with
n = 5 states and m = 11 actions. Each state s ∈ S = {0, . . . , 4} corresponds to a severity score, where small
numbers represent lower severity, and each action a ∈ A corresponds to a theme from the pre-deﬁned list of
themes that the therapists discussed during the sessions. Moreover, each realization of the MDP corresponds
to the therapy sessions of a single patient ordered in chronological order and time horizon T ∈ {10, . . . , 20} is
the number of therapy sessions per patient. Here, we denote the set of realizations for all patients as T .

In addition, to estimate the values of the transition probabilities, we proceed as follows. For every
state-action pair (si, a), we assume a n-dimensional Dirichlet(αi1, . . . , αi1) prior on the probabilities pj | i,a =
P (St+1 = sj | St = si, At = a), where αij = 1 if j ∈ {i − 1, i, i + 1} and αij = 0.01 otherwise. Then, if we
observe cj transitions from state si to each state sj after action a in the patients’ therapy sessions T , we have
that the posterior of the probabilities pj | i,a is a Dirichlet(αi1 + c1, . . . , αin + cn). Finally, to estimate the
value of the transition probabilities P (St+1 | St = si, At = a), we take the average of 100,000 samples from
the posterior probability pj | i,a. This procedure sets the value of the transition probabilities proportionally to
the number of times they appeared in the data, however, it ensures that all transition probability values are

5Our results should be interpreted in the context of our modeling assumptions and they do not suggest the existence of

medical malpractice.

6We do not evaluate our algorithm against prior work on counterfactual explanations for one-step decision making processes

since the settings are not directly comparable.

7All participants gave written informed consent and the study protocol was peer-reviewed [26].

9

(a) (¯oπ∗
τ

(τ ) − o(τ ))/o(τ )

(b)

(cid:80)

1
|T |

τ ∈T ¯oπ∗

τ

(τ ) vs. k

(c) # of uniq. explanations vs. k

Figure 4: Performance achieved by the optimal counterfactual policy π∗
τ in a series of real manualized cognitive
behavioral therapy sessions T , where each realization τ ∈ T includes all the sessions of an individual patient
sorted in chronological order. Panel (a) shows the distribution of the relative diﬀerence between the average
τ (τ ) − o(τ ))/o(τ ), for
counterfactual outcome ¯oπ∗
k = 3 (refer to Appendix E for additional results under other k values). Panels (b) and (c) show the average
counterfactual outcome ¯oπ∗
τ and the average number of unique counterfactual explanations
provided by each π∗
τ , averaged across patients, against the number of actions k diﬀering from the observed
ones. In panel (c), for each realization, the average number of unique counterfactual explanations provided
by π∗
τ is estimated using 1,000 counterfactual realizations. In all panels, we set d = 1,000 and use data from
73 patients. Shaded regions correspond to 95% conﬁdence intervals.

τ and the observed outcome o(τ ), i.e., (¯oπ∗

τ (τ ) achieved by π∗

τ (τ ) achieved by π∗

non zero and transitions between adjacent severity levels are much more likely to happen. Moreover, we set
the immediate reward for a pair of state and action (s, a) equal to R(s, a) = 5 − s ∈ {1, . . . , 5}, i.e., the lower
the patient’s severity level, the higher the reward. Here, if some state-action pair (s, a) is never observed in
the data, we set its immediate reward to R(s, a) = −∞. This ensures that those state-action pairs never
appear in a realization induced by the optimal counterfactual policy. Finally, to compute the counterfactual
transition probability Pτ,t for each realization τ ∈ T , we follow the procedure described in Section 2 with
d = 1,000 samples for each noise posterior distribution.
Results. We ﬁrst measure to what extent the counterfactual explanations provided by the optimal counter-
factual policy π∗
τ would have improved each patient’s severity of depressive symptoms over time. To this end,
for each observed realization τ ∈ T corresponding to each patient, we compute the same quality metrics as in
experiments on synthetic data in Section 5. Figure 4 summarizes the results. Panel (a) reveals that, for most
patients, the improvement in terms of relative diﬀerence between the average optimal counterfactual outcome
τ (τ ) and the observed outcome o(τ ) is rather modest. Moreover, panel (b) also shows that the absolute
¯oπ∗
average optimal counterfactual outcome ¯oπ∗
τ (τ ), averaged across patients, does not increase signiﬁcantly
even if one allows for more changes k in the sequence of observed actions. These ﬁndings suggest that, in
retrospect, the choice of themes by most therapists in the sessions was almost optimal. That being said,
for 20% of the patients, the average counterfactual outcome improves a ≥3 % over the observed outcome
and, as we will later discuss, there exist individual counterfactual realizations in which the counterfactual
outcome improves much more than 3%. In that context, it is also important to note that, as shown in panel
(c), the growth in the number of unique counterfactual explanations with respect to k is weaker than the
growth found in the experiments with synthetic data and, for k ≤ 4, the number of unique counterfactual
explanations is smaller than 10. This latter ﬁnding suggests that, in practice, it may be possible to eﬀectively
show, or summarize, the optimal counterfactual explanations, a possibility that we investigate next.

We focus on a patient for whom the average counterfactual outcome ¯oπ∗

τ (τ ) achieved by the optimal policy
τ with k = 3 improves 9.5% over the observed outcome o(τ ). Then, using the policy π∗
π∗
τ , also with k = 3,
and the counterfactual transition probability Pτ , we sample multiple counterfactual explanations τ (cid:48) using
Algorithm 1 and look at each counterfactual outcome o(τ (cid:48)). Figure 5(a) summarizes the results, which show
that, in most of these counterfactual realizations, the counterfactual outcome is greater than the observed
outcome—if at most k actions had been diﬀerent to the observed ones, as dictated by the optimal policy,

10

0%5%10%15%20%25%Averagecounterfactualimprovement0102030#ofrealizations0246810k5055606570Averagecounterfactualoutcome0246810k0204060#ofuniq.explanations(a) P[o(τ (cid:48))]

(b) Action changes, severity vs. t

(c) Unique explanations

Figure 5: Insights on the counterfactual explanations provided by the optimal counterfactual policy π∗
τ for
one real patient who received manualized cognitive behavioral therapy. Panel (a) shows the distribution
of the counterfactual outcomes o(τ (cid:48)) for the counterfactual realizations τ (cid:48) induced by π∗
τ and Pτ . Panel
(b) shows, for each time step, how frequently a counterfactual explanation changes the observed action as
well as the observed severity level and the severity level in the counterfactual realization with the highest
counterfactual outcome. Here, darker colors correspond to higher frequencies and higher severities. Panel (c)
shows the action changes in the unique counterfactual explanations (green) provided by π∗
τ along with the
mean of counterfactual outcomes (r) that each one achieves and how frequently (f) they appear across the
counterfactual realizations. Here, the bottom row shows the observed actions that were changed by at least
one of the counterfactual explanations. Refer to Appendix C for a deﬁnition of the actions (i.e., themes). In
all panels, we set d = 1,000 and the results are estimated using 1,000 counterfactual realizations.

there is a high probability that the outcome would have improved. Next, we investigate to what extent
there are speciﬁc time steps within the counterfactual realizations τ (cid:48) where π∗
τ is more likely to suggest an
action change. Figure 5(b) shows that, for the patient under study, there are indeed time steps that are
overrepresented in the optimal counterfactual explanations, namely t ∈ {10, 13, 16}. Moreover, the ﬁrst of
these time steps (t = 10) is when the patient had started worsening their depression after an earlier period
in which they showed signs of recovery. Remarkably, we ﬁnd that, in the counterfactual realization τ (cid:48) with
the best counterfactual outcome, the worsening is mostly avoided. Finally, we look closer into the actual
action changes suggested by the optimal counterfactual policy π∗
τ . Figure 5(c) summarizes the results, which
reveal that π∗
τ recommends replacing some of the sessions on cognitive restructuring techniques (CRT) by
behavioral activation (BHA) consistently across counterfactual realizations τ (cid:48), particularly at the start of the
worsening period. We discussed this recommendation with one of the researchers on clinical psychology who
co-authored Fuhr et al. [25] and she told us that, from a clinical perspective, such recommendation is sensible
since, whenever the severity of depressive symptoms is high, it is very challenging to apply CRT and instead
it is quite common to use BHA. Appendix F contains additional insights about other patients in the dataset.

7 Conclusions

We have initiated the study of counterfactual explanations in decision making processes in which multiple,
dependent actions are taken sequentially over time. Building on a characterization of sequential decision
making processes using MDPs and the Gumbel-Max SCM, we have developed a polynomial time algorithm
to ﬁnd optimal counterfactual explanations. Using synthetic and real data from cognitive behavioral therapy,
we have shown that the counterfactual explanations our algorithm ﬁnds can provide valuable insights to
enhance sequential decision making under uncertainty.

Our work opens up many interesting avenues for future work, which may solve some of its limitations. For
example, we have considered sequential decision making processes with discrete states and actions satisfying the
Markov property. Although this setting may ﬁt a plethora of real-world applications, it would be interesting to
extend our work to continuous states and actions and/or semi-Markov processes. Moreover, we experimentally
validated our method using a single real dataset. It would be valuable to evaluate counterfactual explanations

11

404550Counterfactualoutcome0.00.10.20.3ProbabilityObservedoutcomeAvg.counterfactual01234567891011121314151617Frequencyofactionchangespertimestep01234567891011121314151617Observedseveritylevelpertimestep01234567891011121314151617Bestcounterfactualseveritylevelpertimestepgenerated by our algorithm using additional datasets from other (medical) applications. In this context,
it would be worth to consider applications in which the true transition probabilities are due to a machine
learning algorithm. Finally, the usefulness of the counterfactual explanations given by our algorithm crucially
depends on the particular structural causal model we have focused on. To make such explanations more
practical, it would be important to consider alternative structural causal models and carry out a user study
in which the counterfactual explanations are shared with the human experts (e.g., therapists) who took the
observed actions.

Acknowledgements. We would like to thank Kristina Fuhr and Anil Batra for giving us access to the
cognitive behavioral therapy data that made this work possible. Tsirtsis and Gomez-Rodriguez acknowledge
support from the European Research Council (ERC) under the European Union’s Horizon 2020 research and
innovation programme (grant agreement No. 945719). De has been partially supported by a DST Inspire
Faculty Award.

References

[1] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human

decisions and machine predictions. The quarterly journal of economics, 133(1):237–293, 2018.

[2] Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward optimal

education. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2015.

[3] Muhammad Aurangzeb Ahmad, Carly Eckert, and Ankur Teredesai. Interpretable machine learning in
healthcare. In Proceedings of the 2018 ACM international conference on bioinformatics, computational
biology, and health informatics, pages 559–560, 2018.

[4] Sahil Verma, John Dickerson, and Keegan Hines. Counterfactual explanations for machine learning: A

review. arXiv preprint arXiv:2010.10596, 2020.

[5] Amir-Hossein Karimi, Gilles Barthe, Bernhard Sch¨olkopf, and Isabel Valera. A survey of algorithmic
recourse: deﬁnitions, formulations, solutions, and prospects. arXiv preprint arXiv:2010.04050, 2020.

[6] Ilia Stepin, Jose M Alonso, Alejandro Catala, and Mart´ın Pereira-Fari˜na. A survey of contrastive
and counterfactual explanation generation methods for explainable artiﬁcial intelligence. IEEE Access,
9:11974–12001, 2021.

[7] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the

black box: Automated decisions and the gdpr. Harv. JL & Tech., 31:841, 2017.

[8] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classiﬁcation.
Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 10–19, 2019.

In

[9] Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual
explanations for consequential decisions. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 895–905. PMLR, 2020.

[10] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classiﬁers through
diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability,
and Transparency, pages 607–617, 2020.

[11] Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive
summaries of actionable recourses. Advances in Neural Information Processing Systems, 33, 2020.

[12] Amir-Hossein Karimi, Julius von K¨ugelgen, Bernhard Sch¨olkopf, and Isabel Valera. Algorithmic recourse
In Advances in Neural Information

under imperfect causal knowledge: a probabilistic approach.
Processing Systems, volume 33, pages 265–277, 2020.

12

[13] Amir-Hossein Karimi, Bernhard Sch¨olkopf, and Isabel Valera. Algorithmic recourse: from counterfactual
explanations to interventions. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,
and Transparency, pages 353–362, 2021.

[14] Stratis Tsirtsis and Manuel Gomez Rodriguez. Decisions, counterfactual explanations and strategic
behavior. In Advances in Neural Information Processing Systems, volume 33, pages 16749–16760, 2020.

[15] Sonali Parbhoo, Jasmina Bogojeska, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Combining
kernel and model based learning for hiv therapy selection. AMIA Summits on Translational Science
Proceedings, 2017:239, 2017.

[16] Arthur Guez, Robert D Vincent, Massimo Avoli, and Joelle Pineau. Adaptive treatment of epilepsy via

batch-mode reinforcement learning. In AAAI, pages 1671–1678, 2008.

[17] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The artiﬁcial
intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine,
24(11):1716–1720, 2018.

[18] Michael Oberst and David Sontag. Counterfactual oﬀ-policy evaluation with gumbel-max structural
causal models. In International Conference on Machine Learning, pages 4881–4890. PMLR, 2019.

[19] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau,
and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. arXiv preprint
arXiv:1811.06272, 2018.

[20] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning

through a causal lens. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020.

[21] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

[22] Judea Pearl. Causality. Cambridge university press, 2009.

[23] Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal inference: foundations and

learning algorithms. The MIT Press, 2017.

[24] Chris J. Maddison and Danny Tarlow. Gumbel machinery, Jan 2017.

[25] Kristina Fuhr, Cornelie Schweizer, Christoph Meisner, and Anil Batra. Eﬃcacy of hypnotherapy compared
to cognitive-behavioural therapy for mild-to-moderate depression: study protocol of a randomised-
controlled rater-blind trial (wiki-d). BMJ open, 7(11):e016978, 2017.

[26] Kristina Fuhr, Christoph Meisner, Angela Broch, Barbara Cyrny, Juliane Hinkel, Joana Jaberg, Monika
Petrasch, Cornelie Schweizer, Anette Stiegler, Christina Zeep, et al. Eﬃcacy of hypnotherapy compared
to cognitive behavioral therapy for mild to moderate depression-results of a randomized controlled
rater-blind clinical trial. Journal of Aﬀective Disorders, 286:166–173, 2021.

[27] Kurt Kroenke, Robert L Spitzer, and Janet BW Williams. The phq-9: validity of a brief depression

severity measure. Journal of general internal medicine, 16(9):606–613, 2001.

[28] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining, pages 1135–1144, 2016.

[29] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions.

In

International Conference on Machine Learning, pages 1885–1894. PMLR, 2017.

[30] Scott Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. arXiv preprint

arXiv:1705.07874, 2017.

13

[31] Samuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari

agents. In International Conference on Machine Learning, pages 1792–1801. PMLR, 2018.

[32] Akanksha Atrey, Kaleigh Clary, and David D. Jensen. Exploratory not explanatory: Counterfactual
analysis of saliency maps for deep reinforcement learning. In 8th International Conference on Learning
Representations, ICLR, 2020.

[33] Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy,
and Sameer Singh. Explain your move: Understanding agent actions using speciﬁc and relevant feature
attribution. In 8th International Conference on Learning Representations, ICLR, 2020.

[34] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences.

Cambridge University Press, 2015.

[35] Daniel Kasenberg, Ravenna Thielstrom, and Matthias Scheutz. Generating explanations for temporal
logic planner decisions. In Proceedings of the International Conference on Automated Planning and
Scheduling, volume 30, pages 449–458, 2020.

[36] Sergio Jim´enez, Tom´as De La Rosa, Susana Fern´andez, Fernando Fern´andez, and Daniel Borrajo. A
review of machine learning for automated planning. The Knowledge Engineering Review, 27(4):433–467,
2012.

[37] Doina Precup. Eligibility traces for oﬀ-policy policy evaluation. Computer Science Department Faculty

Publication Series, page 80, 2000.

[38] Carles Gelada and Marc G Bellemare. Oﬀ-policy deep reinforcement learning by bootstrapping the
covariate shift. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages
3647–3655, 2019.

[39] Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an estimated
behavior policy. In International Conference on Machine Learning, pages 2605–2613. PMLR, 2019.

14

A Further related work

Our work also builds upon further related work on interpretable machine learning and counterfactual inference.
In terms of interpretable machine learning, in addition to the work on counterfactual explanations for one-step
decision making processes discussed in Section 1 [4–6], there is also a popular line of work focused on
feature-based explanations [28–30]. Feature-based explanations highlight the importance each feature has on
a particular prediction by a model, typically through local approximation. In this context, one particular
type of feature-based explanations that have been relatively popular in explaining the action choices of
reinforcement learning agents in simple gaming environments (e.g., Atari games) are those in the form of
saliency maps [31–33]. In terms of counterfactual inference, the literature has a long history [34], however, it
has primarily focused on estimating quantities related to the counterfactual distribution of interest such as,
e.g., the conditional average treatment eﬀect (CATE). More broadly, one may also draw connections between
our work and automated planning [35, 36] and oﬀ-policy evaluation in reinforcement learning [37–39].

B Proof of Proposition 1

Using induction, we will prove that the policy value πτ ((s, l), t(cid:48)) set by Algorithm 2 is optimal for every s ∈ S,
l ∈ {0, . . . , k}, t(cid:48) ∈ {0, . . . , T − 1} in the sense that following this policy maximizes the average cumulative
reward h(s, r, c) that one could have achieved in the last r = T − t(cid:48) steps of the decision making process,
starting from state ST −r = s, if at most c = k − l actions had been diﬀerent to the observed ones in those
last steps. Formally:

h(s, r, c) = max

π

E

{((s(cid:48)

t,lt),a(cid:48)

t)}T −1

t=t(cid:48) ∼P +

τ | S+

t(cid:48) =(s,l)

(cid:35)

r+ ((s(cid:48)

t, lt) , a(cid:48)
t)

(cid:34)T −1
(cid:88)

t=t(cid:48)

subject to

T −1
(cid:88)

t=t(cid:48)

1[at (cid:54)= a(cid:48)

t] ≤ c ∀{((s(cid:48)

t, lt), a(cid:48)

t)}T −1

t=t(cid:48) ∼ P +

τ

(10)

(11)

Recall that, a1, . . . , aT −1 are the observed actions and the counterfactual realizations a(cid:48)

1, . . . , a(cid:48)

T −1 are

induced by the counterfactual transition probability P +

τ and the policy π.

We start by proving the induction basis. Assume that a realization has reached a state s+

T −1 = (s, l)
at time T − 1, one time step before the end of the process. If c = 0 (i.e., l = k), following Equation 9,
the algorithm will choose the observed action πτ ((s, l), t(cid:48)) = aT −1 and return an average cumulative reward
h(s, 1, 0) = R(s, aT −1) + (cid:80)
s(cid:48)∈S Pτ,T −1(s(cid:48) | s, aT −1) h(s(cid:48), 0, 0) = R(s, aT −1), where h(s(cid:48), 0, 0) = 0 for all s(cid:48) ∈ S.
Since no more action changes can be performed at this stage, this is the only feasible solution and therefore it
is optimal.

If c > 0, since h(s(cid:48), 0, c) = h(s(cid:48), 0, c − 1) = 0 for all s(cid:48) ∈ S it is easy to verify that Equation 8 reduces to
h(s, 1, c) = maxa∈A R(s, a) and πτ ((s, l), t(cid:48)) = argmaxa∈A R(s, a) is obviously the optimal choice for the last
time step.

Now, we will prove that, for a counterfactual realization being at state s+

t(cid:48) = (s, l) at a time step t(cid:48) < T − 1
(i.e., r = T − t(cid:48), c = k − l), the maximum average cumulative reward h(s, r, c) given by Algorithm 2 is optimal,
under the inductive hypothesis that the values of h(s(cid:48), r(cid:48), c(cid:48)) already computed for r(cid:48) < r, c(cid:48) < c and all
s(cid:48) ∈ S are optimal. Assume that the algorithm returns an average cumulative reward h(s, r, c) by choosing
action πτ ((s, l), t(cid:48)) = a while the optimal solution gives an average cumulative reward OP Ts,r,c > h(s, r, c) by
choosing an action a∗ (cid:54)= a. Here, by τ (cid:48)
t=t(cid:48) we will denote realizations starting from time
t(cid:48) with a(cid:48)
t, lt) , t) where πτ is the policy given by Algorithm 2 and we will use τ ∗
t(cid:48) if the policy is
optimal. Also, we will denote a possible next state at time t(cid:48) + 1, after performing action a, as (s(cid:48), l(cid:48)) where
l(cid:48) = l + 1 if a (cid:54)= at, l(cid:48) = l otherwise and, c(cid:48) = k − l(cid:48). Similarly, after performing action a∗, we will denote a

t = πτ ((s(cid:48)

t(cid:48) = {((s(cid:48)

t, lt) , a(cid:48)

t)}T −1

15

possible next state as (s(cid:48), l∗) where l∗ = l + 1 if a∗ (cid:54)= at, l∗ = l otherwise and, c∗ = k − l∗. Then, we get:

h(s, r, c) < OP Ts,r,c

=⇒ E

t(cid:48) ∼P +
τ (cid:48)

τ | S+

t(cid:48) =(s,l)

(cid:34)T −1
(cid:88)

(cid:35)
r+ ((st, lt) , at)

t=t(cid:48)

< E

t(cid:48) ∼P +
τ ∗

τ | S+

t(cid:48) =(s,l)

(cid:88)

(a)
=⇒

Pτ,T −r(s(cid:48) | s, a)E

∼P +

τ | S+

t(cid:48)+1

=(s(cid:48),l(cid:48))

(cid:34)T −1
(cid:88)

t=t(cid:48)

τ (cid:48)
t(cid:48)+1

r+ ((st, lt) , at)

(cid:34)T −1
(cid:88)

(cid:35)
r+ ((st, lt) , at)

t=t(cid:48)
(cid:35)

s(cid:48)

<

(cid:88)

s(cid:48)

=⇒

(cid:88)

s(cid:48)

Pτ,T −r(s(cid:48) | s, a∗)E

τ ∗
t(cid:48)+1

(cid:34)

∼P +

τ | S+

t(cid:48)+1

Pτ,T −r(s(cid:48) | s, a)

r+ ((s, l) , a) + E

τ (cid:48)
t(cid:48)+1

∼P +

τ | S+

t(cid:48)+1

(cid:34)T −1
(cid:88)

(cid:35)
r+ ((st, lt) , at)

=(s(cid:48),l∗)

t=t(cid:48)

=(s(cid:48),l(cid:48))

(cid:34) T −1
(cid:88)

t=t(cid:48)+1

(cid:35)(cid:35)

r+ ((st, lt) , at)

(cid:88)

<

s(cid:48)

Pτ,T −r(s(cid:48) | s, a∗)

(cid:34)
r+ ((s, l) , a∗) + E

τ ∗
t(cid:48)+1

∼P +

τ | S+

t(cid:48)+1

=(s(cid:48),l∗)

(cid:34) T −1
(cid:88)

t=t(cid:48)+1

(cid:35)(cid:35)

r+ ((st, lt) , at)

(cid:88)

(b)
=⇒

s(cid:48)

Pτ,T −r(s(cid:48) | s, a)R(s, a) +

(cid:88)

s(cid:48)

Pτ,T −r(s(cid:48) | s, a)h(s(cid:48), r − 1, c(cid:48))

(cid:88)

<

s(cid:48)

Pτ,T −r(s(cid:48) | s, a∗)R(s, a∗) +

(cid:88)

s(cid:48)

Pτ,T −r(s(cid:48) | s, a∗)OP Ts(cid:48),r−1,c∗

(c)
=⇒ R(s, a) +

(cid:88)

s(cid:48)

Pτ,T −r(s(cid:48) | s, a)h(s(cid:48), r − 1, c(cid:48)) < R(s, a∗) +

Pτ,T −r(s(cid:48) | s, a∗)h(s(cid:48), r − 1, c∗),

(cid:88)

s(cid:48)

where, in (a), we expand the expectation for one time step, in (b), we replace the average cumulative
reward starting from time step t(cid:48) + 1 with h(s(cid:48), r − 1, c(cid:48)) and OP Ts(cid:48),r−1,c∗ for the policy of Algorithm 2 and
the optimal one respectively and, in (c), we replace OP Ts(cid:48),r−1,c∗ with h(s(cid:48), r − 1, c∗) due to the inductive
hypothesis.

It is easy to see that, it can either be a∗ = at with c∗ = c or a∗ ∈ A \ at with c∗ = c − 1. If c = 0, following
Equation 9, the algorithm will choose the observed action (i.e., a = at). This is the only feasible solution,
since a∗ (cid:54)= at would give c∗ = −1 and l∗ = k − c∗ = k + 1, which is not a valid state. Therefore, we get
a = a∗ = at, which is a a contradiction. If c > 0, because of the max operator in Equation 8, for the action a
chosen by Algorithm 2, it necessarily holds that:

R(s, a) +

(cid:88)

s(cid:48)

Pτ,T −r(s(cid:48) | s, a)h(s(cid:48), r − 1, c(cid:48)) ≥ R(s, a∗) +

Pτ,T −r(s(cid:48) | s, a∗)h(s(cid:48), r − 1, c∗),

(cid:88)

s(cid:48)

which is clearly a contradiction.

Therefore, the average cumulative reward h(s, r, c) computed by Algorithm 2 and its associated policy
value πτ ((s, l), t(cid:48)) are optimal for every s ∈ S, l ∈ {0, . . . , k}, t(cid:48) ∈ {0, . . . , T − 1} and h(s0, T, k) is the solution
to the optimization problem deﬁned by Equation 7.

C Additional Details about the Cognitive Behavioral Therapy

Dataset

Each patient’s severity of depression is measured using the standardized questionnaire PHQ-9 [27], which
consists of 9 questions regarding the frequency of depressive symptoms (e.g., “Feeling tired or having little
energy?”) manifested over a period of two weeks. The patient has to answer each question by placing
themselves on a scale ranging from 0 (“Not at all”) to 3 (“Nearly every day”). The sum of those answers,
ranging from 0 to 27, reﬂects the overall depression severity and it is usually discretized into ﬁve categories,

16

corresponding to no depression (0 − 4), mild depression (5 − 9), moderate depression (10 − 14), moderately
severe depression (15 − 19), severe depression (20 − 27)8. In our experiments, the states S = {0, . . . , 4}
correspond to these ﬁve categories.

Each session of cognitive behavioral therapy contains information about the topic of discussion between
the patient and the therapist, among 24 pre-deﬁned topics [25], with some of the topics having similar content.
For example, there were 4 topics about “cognitive restructuring techniques” which, we observed that, some
therapists merged and covered in 2 sessions. Here, we grouped the above topics into the following eleven
broader themes:

• STR – First session: Introduction, discussing expectations, getting to know each other, discussing the

current symptoms / problems, current life situation.

• BIO – Biography: A look at biography, family and social frame of reference, school and professional

development, emotional development, partnerships, important turning points or crises.

• PSE – Psychoeducation: Discuss symptoms of depression, recognize and understand connections between
feelings, thoughts and behavior (depression triangle) based on a situation analysis from the current /
last episode, causes of depression, develop a disease model, explain the treatment approach in relation
to the model.

• BHA – Behavioural activation: Focus on behaviour, discuss the vicious circle (depression spiral), discuss
list of pleasant activities, attention to life balance, if necessary improve the daily structure, recognizing
and eliminating obstacles and problems.

• REV – Review: Review of the last sessions, collection of strategies learned so far, ﬁnd suitable strategies

for typical situations, draft a personal strategy plan, plan further steps.

• CRT – Cognitive restructuring techniques: Discuss inﬂuence of thoughts on feelings and actions, identify
thought patterns, discuss inﬂuence of automatic thoughts / basic assumptions, check the validity of
automatic thoughts.

• INR – Interactional competence: Self-assessment of your own self-conﬁdence, discuss current interpersonal

issues and derive goals, carry out role plays, transfer into everyday life.

• THP – Re-evaluation of thought patterns: Review, evaluate and rename basic assumptions, schemes

and general plans.

• RLP – Relapse prevention: Explain the risk of relapse, discuss early warning symptoms, recognize risk

situations, develop suitable strategies.

• END – Closing session: Finding a good end to the therapy, looking back on the last 5-6 months, parting

ritual.

• EXT – Extra material: Sleep disorders, problem-solving skills, brooding module ”When thinking doesn’t
help”, discuss the inﬂuence of rumination on mood and impairments in everyday life, progressive muscle
relaxation.

In our experiments, the actions A correspond to these broader themes. However, since the themes STR and
END appeared only in the ﬁrst (t = 0) and last (t = T − 1) time steps of each realization, we kept them ﬁxed
and we did not allow these themes to be used as action changes during the time steps t = {1, . . . , T − 2}.

8The full version of the questionnaire can be found at https://patient.info/doctor/patient-health-questionnaire-phq-9.

17

Figure 6: Performance achieved by the optimal counterfactual policy π∗
τ given by Algorithm 2 and the baseline
policies in the same series of cognitive behavioral therapy sessions T introduced in Section 6. The plot shows
the average counterfactual outcome 1
τ ∈T ¯oπτ (τ ) achieved by π∗
τ and the baseline policies, averaged over
T
the set of observed realizations T , against the number of actions k diﬀering from the observed ones. For each
observed realization, the average counterfactual outcome is estimated using 1,000 counterfactual realizations.
Here, we set d = 1,000 and use data from |T | = 73 patients. Shaded regions correspond to 95% conﬁdence
intervals.

(cid:80)

D Performance Comparison with Baseline Policies

Experimental setup. In this section, we compare the average counterfactual outcome achieved by the
optimal counterfactual policy, given by Algorithm 2, with that achieved by several baseline policies. To
this end, we use the same experimental setup as in Section 6, however, instead of setting R(s, a) = −∞ for
every unobserved pair (s, a), we set R(s, a) = 5 − s ∈ {1, . . . , 5}, similarly as for the observed pairs. This is
because, otherwise, we observed that there were always realizations under the baselines policies for which the
counterfactual outcome was −∞. In our experiments, we consider with the following baselines policies:

• Random: At each time step t, the policy chooses the next action a∗ uniformly at random if lt < k and

it chooses a∗ = at otherwise.

• Greedy: At each time step t, being at state (s(cid:48)

t, lt), the policy chooses the next action a∗ greedily, i.e.,

if lt < k, then

a∗ = argmax

R(s, a) +

a∈A

(cid:88)

s(cid:48)∈S

Pτ,t(St+1 = s(cid:48) | St = s(cid:48)

t, At = a)R(s(cid:48), a(cid:48)),

(12)

and, if lt = k, a∗ = at.

• Noisy greedy: At each time step t, being at state (s(cid:48)

t, lt), it chooses the next action a∗ as follows. If

lt < k, a∗ is given by Eq. 12 with probability 0.5 and a∗ = at otherwise. If lt = k, a∗ = at.

Results. Figure 6 shows the average counterfactual outcomes achieved by the optimal policy, as given
by Algorithm 2, and the above baselines for diﬀerent k values. The results show that, as expected, the
optimal policy outperforms all the baselines across the entire range of k values and, moreover, the competitive
advantage is greater for smaller k values. In addition, we also ﬁnd that the performance of the random
baseline policy drops signiﬁcantly as k increases, since, as discussed in Section 6, the observed trajectories are
close to optimal in retrospect and, diﬀering from them causes the random policy to worsen the counterfactual
outcome.

18

0246810k5055606570AveragecounterfactualoutcomeRandomGreedyNoisygreedyAlgorithm2(a) k = 1

(b) k = 2

(c) k = 5

τ (τ )
Figure 7: Empirical distribution of the relative diﬀerence between the average counterfactual outcome ¯oπ∗
achieved by the optimal counterfactual policy π∗
τ (τ ) − o(τ ))/o(τ )]
for several values of k. Here, we consider the same series of cognitive behavioral therapy sessions T introduced
in Section 6. In all panels, we set d = 1,000 and use data from 73 patients.

τ and the observed outcome o(τ ), i.e., P[(¯oπ∗

E Average counterfactual improvement for additional values of k

In this section, we use the same experimental setup as in Section 6 to measure to what extent the counterfactual
explanations provided by the optimal counterfactual policy π∗
τ would have improved each patient’s severity of
depressive symptoms, under other values of k. To this end, for each observed realization τ ∈ T corresponding
to each patient, we compute the relative diﬀerence between the average optimal counterfactual outcome and
the observed outcome, i.e., (¯oπ∗

τ (τ ) − o(τ ))/o(τ ).

Figure 7 summarizes the results, which show that, similarly as in Section 6, for most patients, the
improvement in terms of relative diﬀerence between the average optimal counterfactual outcome ¯oπ∗
τ (τ ) and
the observed outcome o(τ ) is modest. That being said, we observe that, as the sequences of actions speciﬁed
by the counterfactual explanations diﬀer more from the observed actions (i.e., k increases), the improvement
in terms of expected outcome presents a slight increase.

F Insights About Individual Patients

In this section, we provide insights about additional patients in the dataset. For each of these additional
patients, we follow the same procedure in Section 6, i.e., we use Algorithm 1 with the policy π∗
τ , with k = 3, to
sample multiple counterfactual explanations τ (cid:48) and look at the corresponding counterfactual outcomes o(τ (cid:48)).
Figure 8 summarizes the results, where each row corresponds to a diﬀerent patient. The results reveal several
interesting insights. For most of the patients, all of the counterfactual realizations lead to counterfactual
outcomes greater or equal than the observed outcome (left column), however, the diﬀerence between the
average counterfactual outcome and the observed outcome is relatively small. Notable exceptions are a few
patients for whom there is a small probability that the counterfactual outcome is worse than the observed one
(top row) as well as patients for whom the diﬀerence between the average counterfactual outcome and the
observed outcome is high (bottom row). Additionally, we also ﬁnd that the actual action changes suggested
by the optimal counterfactual policies π∗
τ are typically concentrated in a few time steps across counterfactual
realizations (right column), usually at the beginning or the end of the realizations.

19

0%5%10%15%20%25%Averagecounterfactualimprovement010203040#ofrealizations0%5%10%15%20%25%Averagecounterfactualimprovement010203040#ofrealizations0%5%10%15%20%25%Averagecounterfactualimprovement010203040#ofrealizationsP[o(τ (cid:48))]

Action changes, severity vs. t

Figure 8: Insights on the counterfactual explanations provided by the optimal counterfactual policy π∗
τ
for ﬁve real patients who received manualized cognitive behavioral therapy. Each row corresponds to a
diﬀerent patient with an observed realization τ . The panels in the left column show the distribution of the
counterfactual outcomes o(τ (cid:48)) for the counterfactual realizations τ (cid:48) induced by π∗
τ and Pτ . The panels in the
right column show, for each time step, how frequently a counterfactual explanation changes the observed
action as well as the observed severity level and the severity level in the counterfactual realization with the
highest counterfactual outcome. Here, darker colors correspond to higher frequencies and higher severities.
In all panels, we set d = 1,000, k = 3, and the results are estimated using 1,000 counterfactual realizations.

20

4244464850Counterfactualoutcome0.00.10.2ProbabilityObservedoutcomeAvg.counterfactual024681012141618Frequencyofactionchangespertimestep024681012141618Observedseveritylevelpertimestep024681012141618Bestcounterfactualseveritylevelpertimestep40455055Counterfactualoutcome0.000.050.100.150.20ProbabilityObservedoutcomeAvg.counterfactual01234567891011121314Frequencyofactionchangespertimestep01234567891011121314Observedseveritylevelpertimestep01234567891011121314Bestcounterfactualseveritylevelpertimestep42.545.047.550.052.5Counterfactualoutcome0.00.20.4ProbabilityObservedoutcomeAvg.counterfactual0123456789101112131415Frequencyofactionchangespertimestep0123456789101112131415Observedseveritylevelpertimestep0123456789101112131415Bestcounterfactualseveritylevelpertimestep60.062.565.067.570.0Counterfactualoutcome0.00.10.20.3ProbabilityObservedoutcomeAvg.counterfactual024681012141618Frequencyofactionchangespertimestep024681012141618Observedseveritylevelpertimestep024681012141618Bestcounterfactualseveritylevelpertimestep7274767880Counterfactualoutcome0.00.20.40.6ProbabilityObservedoutcomeAvg.counterfactual024681012141618Frequencyofactionchangespertimestep024681012141618Observedseveritylevelpertimestep024681012141618Bestcounterfactualseveritylevelpertimestep