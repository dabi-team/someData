0
2
0
2

t
c
O
5

]
h
p
-
t
n
a
u
q
[

1
v
8
8
0
2
0
.
0
1
0
2
:
v
i
X
r
a

Quantum Bayesian decision-making (cid:63)

Michael de Oliveira1 and Luis Soares Barbosa1

INL - Quantum Software Engineering & Universidade do Minho, Braga, Portugal
michaeldeoliveira@live.com.pt, lsb@di.uminho.pt

Abstract. As a compact representation of joint probability distribu-
tions over a dependence graph of random variables, and a tool for mod-
elling and reasoning in the presence of uncertainty, Bayesian networks are
of great importance for artiﬁcial intelligence to combine domain knowl-
edge, capture causal relationships, or learn from incomplete datasets.
Known as a NP-hard problem in a classical setting, Bayesian inference
pops up as a class of algorithms worth to explore in a quantum frame-
work. This paper explores such a research direction and improves on pre-
vious proposals by a judicious use of the utility function in an entangled
conﬁguration. It proposes a completely quantum mechanical decision-
making process with a proven computational advantage. A prototype
implementation in Qiskit (a Python-based program development kit for
the IBM Q machine) is discussed as a proof-of-concept.

Keywords: Bayesian inference · Quantum algorithms · Quantum deci-
sion making

1 Motivation

Bayesian reasoning is widely used in machine learning and data science, as a
powerful framework for probabilistic analysis, applications ranging from learn-
ing processes [17] to pragmatic representations [12]. Broadly speaking, machine
learning algorithms are able to learn from data, with the purpose of performing
some tasks, without requiring explicit programming; in a sense outcomes are
directly built by the sampled data. However, the current rate of data creation
is almost exponential [1] (going, for example, from 3.5 million text messages
per minute in 2016, to over 15 million in 2017), a fact that calls for radically
new approaches and, most probably, new computational models and hardware
to eﬀectively deal with such numbers.

(cid:63) This work is ﬁnanced by the ERDF – European Regional Development Fund through
the Operational Programme for Competitiveness and Internationalisation - COM-
PETE 2020 Programme and by National Funds through the Portuguese funding
agency, FCT, within project POCI-01-0145-FEDER-030947. The ﬁrst author was fur-
ther supported by project NORTE-01-0145-FEDER-000037, funded by Norte Portu-
gal Regional Operational Programme (NORTE 2020), under the PORTUGAL 2020
Partnership Agreement.

 
 
 
 
 
 
2

Michael Oliveira et al.

Can quantum computing bring some useful contribution to this state of
aﬀairs? On the one hand it is well known that building very large quantum-
addressable classical memories is technologically very demanding, and will not
be available soon. On the other, at least from a theoretical point of view, the
question seems worth to discuss. Actually, even at its present, quite preliminar
stage of development, quantum computing allows for a variety of speed ups with
respect to classical algorithmic counterparts in e.g. information storage [6], pat-
tern recognition [3], and matrix inversion, the latter being a basic ingredient
of several machine learning algorithms [8]. As a matter of fact, quantum algo-
rithms, as the ones discussed in this paper, suggest radically diﬀerent ways to
approach old problems and to explore complexity boundaries. For example, to
know whether for a concrete problem, as the size of the input parameter grows,
one may asymptotically go faster with the use of a quantum memory than with
purely classical states, is a question underlying many interesting problems from
big-data to optimisation, or molecular synthesis.

The synergies between research lines in quantum technologies and Bayesian
inference, in particular, seem promising. In one direction, a quantum proces-
sor can be expressed and studied as a Bayesian Network [26]. In the reverse
one, quantum mechanics can describe naturally probabilistic systems in physi-
cal terms. Reference [14], for example, describes very promising improvements
on the implementation of approximate Bayesian inference routines resorting to
physical stochastic logic gates building up hardware implementations of sampling
algorithms. Quantum processors are, in a sense, part of such a family.

This paper is a step in this direction. Our starting point is a quantum version
of a Bayesian inference algorithm introduced by Low et al [13] based on a square-
root quantum speedup to rejection sampling on a Bayesian network, which avoids
the use of an oracle. Note that an oracle-based version appeared previously in
[21]. This approach, revisited in section 3, was implemented by us on Qiskit —
the IBM open-source platform for quantum computing. Our main contribution,
presented in section 4, extends Low et al algorithm to a decision-making setting:
this incorporates an utility function which is applied before any observation of
the quantum state which encodes the Bayesian network. The computational
eﬀort for the proposed solution and a simpler quantum solution are determined
in Section 5. A proof-of-concept implementation Qiskit is discussed in section 6.
Finally, section 7 concludes and points out a number of issues for future work. A
background section — section 2 — recalls the Bayesian inference problem and
provides a brief overview of the basic intuitions underlying decision making.

2 Background

2.1 Bayesian inference

Bayesian inference is used to update the posterior probability distribution of
some query variables given the value of the observed variables, also known as

Quantum Bayesian decision-making

3

evidence variables [25]. The conditional probability is given by

P (A|B) =

P (A, B)
P (B)

(1)

These joint probabilities can be stored in a distribution table, but note that
the dimension of the latter grows exponentially with the number of variables.
This means that for most applications the table would be too large to be stored
computationally. Alternatively, Bayesian networks, as in Fig. 1, allow for a com-
pact representation of joint probability distributions [5] as a directed acyclic
graph structure. The advantage is that the space complexity of the representa-
tion can be made much smaller than in the general case, by exploiting conditional
dependencies in the distribution, through the association to each graph node of
a conditional probability table for each random variable, with directed edges
representing conditional dependencies. For this reason, they are largely used in
industrial applications. However, inference via a Bayesian Networks is still a NP-
problem. Fig. 1 depicts a toy Bayesian network relating a few variables encoding
diﬀerent sorts of activities and the possibility of a lung cancer diagnosis.

Fig. 1: Bayesian Network over 5 variables.

Inference. Algorithms that infer over a Bayesian network compute joint prob-
abilities using the following equation:

P (x1, ..., xn) =

n
(cid:89)

i=1

P (xi|P arents(Xi))

(2)

The computational eﬀort to determine this value is low because the number of
values selected is linearly bounded by the number of variables. The envisaged
joint probability, however, may not be deﬁned for all variables. If such is the
case, it is necessary to sum out over all undeﬁned variables as in

P (X1|x2, x3) =

(cid:88)

(cid:88)

x4

x5

P (X1, x2, x3, x4, x5)

(3)

4

Michael Oliveira et al.

Consequently, the number of values to sum out grows exponentially with the
number of undeﬁned variables. A well known algorithm for variable elimination
algorithm works exactly in this way. Approximate algorithms, treading oﬀ con-
sumption of computational resources for precision, are typically used to tackle
this problem. Solutions are found faster but may not be precise. The literature
documents a bunch of approximate algorithms. In this paper we will focus on
rejection sampling because the ﬁrst part of the quantum inference algorithm
discussed in the next section is a quantum analog to it. Rejection sampling is a
popular method ﬁrst systematised by von Neumann [18], who curiously enough
also developed the Hilbert space formalization of quantum mechanics and its
logic.

Rejection sampling generates samples resorting to the probability distribu-
tions deﬁned by the conditional probability tables as depicted in Fig. 1. The
consequence of this generation process is that a certain conﬁguration of values
for the variables is only sampled with the associated probability:

P (Sample < X1 = true, X2 = f alse >) = P (X1 = true, X2 = f alse)

(4)

Under those circumstances, a conditional probability can be determined by:

P (X1 = true|X2 = f alse) ≈

#Samples(X1 = true, X2 = f alse)
#Samples(X2 = f alse)

(5)

Clearly, the precision of the query grows with the number of useful samples
(#Samples). It is important to notice that not all samples are useful since
samples with diﬀerent values for the evidence variables are not used.

Bayesian networks for decision making. Bayesian networks are equipped
with an utility function in order to support decision-making processes. Its pur-
pose is to quantify the utility of possible outcomes. The expected utility (EU )
of an outcome is the product of its probability and the associated utility value.
Formally, to ﬁnd the expected utility of some action a, one computes

EU (a|e) =

(cid:88)

r

P (Result = r|a, e) ∗ U (r)

(6)

If the EU values of all feasible actions is known, it is possible to choose the
‘best’, or more proﬁtable, one:

action = argmaxaEU (a|e)

(7)

This is indeed the maximum expected utility principle; a rational entity is ex-
pected to choose the action with the greatest expected utility with respect to
her set of beliefs [25].

The previous principle describes many algorithms and solutions used in artiﬁ-
cial intelligence. For example, in reinforcement learning a great number of agents

Quantum Bayesian decision-making

5

and robots are built on a process that attempts to ﬁnd the optimal policy. This
works with an instance of a Bayesian network (Markov decision process) and a
more complex utility function (the so-called discounted reward function), where
the agent also accounts for future rewards (R(St)), which are reachable from the
starting state. Further, it values present rewards over future rewards with the
use of a discount factor γt.

Uπ(s) = E

(cid:35)

γtR(St)

(cid:34) ∞
(cid:88)

t=0

(8)

Therefore, any algorithm that computes the best decision by Equations (6) and
(7) has the potential to be applied to all other instances derived from the prin-
cipal one, as a consequence of having the same computational pattern.

3 Quantum Bayesian inference

In brief, a quantum algorithm can be regarded as a targeted manipulation of a
quantum state (realised as an assembly of qubits) with a subsequent measure-
ment to retrieve relevant information. States are represented by column vectors
of complex numbers whose sum of moduli squared is 1, often represented in
the so-called Dirac notation [19] as |Ψ (cid:105). Typically, they correspond to linear
combinations of basis states aﬀected by complex coeﬁcients, as in, for example,
equation (9). The dynamics of a quantum system is represented by (the multi-
plication of the quantum state by) unitary matrices and is therefore reversible
in time, as long as no measurement is involved. The reversal corresponds simply
to a composition with the adjoint of the unitary matrix that represents forward
evolution. Such an evolution can be expressed as a sequence of only a few ele-
mentary transformations represented as quantum gates, which only act on one
or two qubits at a time. Therefore, quantum algorithms are widely formulated
as circuits built of these elementary gates, as depicted, for example, in Fig 2.

A quantum algorithm for inference on Bayesian networks was introduced by
Low et al [13], which, as mentioned above, is based on an improved quantum
version of the Rejection Sampling algorithm. This algorithm is able to generate
samples quadratically faster than the classical version, provided that the network
is not too densely connected. The algorithm is divided into 3 stages, detailed in
the sequel.

In the ﬁrst stage, the Bayesian network is encoded into a quantum state. For
this, a binary variable can be represented by a single qubit and the probabilities
are mapped to the coeﬃcients of the quantum state:

|Ψ (cid:105) = α |V ar1 = true(cid:105) + β |V ar1 = f alse(cid:105) ⇔ |Ψ (cid:105) =

(cid:19)

(cid:18)α
β

with the corresponding density matrix [2],

ρΨ = |Ψ (cid:105) (cid:104)Ψ | =

(cid:33)

(cid:32) α2 α ∗ β
α ∗ β β2

(9)

(10)

6

Michael Oliveira et al.

Whenever two variables share an edge in the network they are related, and
therefore not independent from each other, such a relationship is expressed
through state entanglement. Entanglement represents a strong correlation be-
tween quantum states, therefore expressing shared information between diﬀerent
elements. The envisaged state is achieved though the application of a speciﬁc sort
of gates — controlled rotations — to the state qubits. The fact that a rotation is
controlled by another qubit permits the creation of entanglement between them.
The amplitude of the rotation deﬁnes the value of the coeﬃcients. For instance,
a circuit that encodes the Bayesian network in Fig. 1 is represented in Fig 2.

Fig. 2: Encoding circuit.

The whole quantum state is equivalent to a superposition of all on entries of the
original joint probability distribution table:

1 |V ar1 = true, V ar2 = true, ...(cid:105) + γ2
|Ψ (cid:48)(cid:105) = γ2
3 |V ar1 = f alse, V ar2 = true, ..(cid:105) + γ2
γ2
...

2 |V ar1 = true, V ar2 = f alse, ...(cid:105) +
4 |V ar1 = f alse, V ar2 = f alse, ...(cid:105) +

(11)

Afterward, a measurement1 to this state produces a sample, as in the Rejection
Sampling algorithm, as the probability of each sample is the same as the one in
the distribution:

1 Notice, that the use of the density matrix notation with projectors is equivalent to

the description of the measurement in the Dirac notation:

(cid:10)V ar1 = true, V ar2 = true, ...(cid:12)

(cid:12)Ψ (cid:48)(cid:11) ≡ T r(P0 ∗ ρΨ (cid:48) )

(12)

The matrix density notation was not required, as we are not dealing with mixed
quantum states. However, it helped, later on, to expose the main ideas more clearly.

Quantum Bayesian decision-making

7

P (V ar1 = true, V ar2 = true, ...) = T r(P0 ∗ ρΨ (cid:48)) =

∗










...

γ2
1 ... ...
... γ2
2 ...
...
. . . ...
... ...
... ... ... γ2
n










= γ2

1 ∗ 1 + γ2

2 ∗ 0 + ... + γ2

n ∗ 0 = γ2
1



1 0 ... 0










0 0 ... 0
...
...
. . . ...
0 0 ... 0








At this point, a quantum analog to Rejection Sampling is created. However, it
is not an eﬃcient way to do inference because every time we measure the state
it collapses, and it is necessary to reconstruct the state entailing the need for a
subsequent reconstruction.

In a second stage, the Amplitude Ampliﬁcation algorithm [4] is applied to
amplify the states that have the right values for the evidence variable. It allows
for a square root speed up in search problems, a fact that explains its relevance
and ubiquity to many quantum programs. In our case, the quantum state that
encodes the Bayesian network is divided into two orthogonal states, one where
the evidence variables have the right value and another state where they lack it:

|Ψinit(cid:105) =(cid:112)P (e) |V ar1, V ar2, ..., evidences(cid:105)

+ (cid:112)1 − P (e) |V ar1, V ar2, ..., ¬evidences(cid:105)

(13)

Next, the amplitude ampliﬁcation algorithm is applied to search for the state
that has the right values for the evidence variables [4].

Qk ∗ |Ψinit(cid:105) = cos

(cid:19)

|V ar1, V ar2, ..., evidences(cid:105)

∗ θ

(cid:19)

∗ θ

|V ar1, V ar2, ..., ¬evidences(cid:105)

(cid:18) 2k + 1
2
(cid:18) 2k + 1
2

+ sin

where Q represent the operator that ampliﬁes the selected elements, k the num-
ber of iterations, and θ the initial probabilities,

θ = 2sin−1((cid:112)P (e)) ∧ θ = 2cos−1((cid:112)1 − P (e))
then for the right number of iteration (k(cid:48)), the ﬁnal quantum state approximates
with great probability to the pretended state,

(14)

Qk(cid:48)

∗ |Ψinit(cid:105) = |Ψf inal(cid:105) ≈ |V ar1, V ar2, ..., evidences(cid:105)

(15)

The last stage amounts simply to observe this state and use the result as a sam-
ple. In Table 1 we can see the comparison between the classical and the quantum

8

Michael Oliveira et al.

versions. The latter exhibits a quadratic speed up but only if the Bayesian net-
work is not too densely connected, meaning that m the number of edges between
the nodes (n) is not to large. Otherwise the price of encoding it to a quantum
state will be too high, as the corresponding term grows exponentially.

Table 1: Classical vs quantum complexity.

Process type Complexity
Classical

O(n ∗ m ∗ P (e)−1)
−1
O(n ∗ 2m ∗ P (e)
2 )

Quantum

4 Quantum decision-making

Clearly, a quantum computer could be used to work out the conditional proba-
bilities with a quadratic speed up for decision problems, according to equation
(16).

EU (a|e) =

(cid:88)

r

P (Result = r|a, e)
(cid:123)(cid:122)
(cid:125)
(cid:124)
Quantum

∗ U (r)
(cid:124) (cid:123)(cid:122) (cid:125)
Classical

(16)

In this section, however, we would like to propose a diﬀerent approach which,
in principle, will increase the advantage of having the quantum resources. The
idea is quite simple: Instead of sampling the conditional probabilities, the quan-
tum state remains unobserved until the utility function is applied. The intention
is to apply a transformation to the outcome variable and look to what happens to
the action variable. As both the outcome and the action variables are entangled,
a transformation applied to the former will produce an eﬀect on the latter.

The new algorithm modiﬁes the process described in the previous section to
infer a conditional probability by preventing the action variable to be used as
an evidence variable. Thus, after an application of the amplitude ampliﬁcation
algorithm and tracing out the non-evidence variables (N E), as nothing happens
to them during the proposed process, we have,

trN E(QSearch1 |Ψinit(cid:105)) = |ΨA,R(cid:105) = γa,r |a, r, evidences(cid:105) + γa,¬r |a, ¬r, evidences(cid:105) +

+ γ¬a,r |¬a, r, evidences(cid:105) + γ¬a,¬r |¬a, ¬r, evidences(cid:105)

(17)

or, equivalently,

|ΨA,R(cid:105) =







γa,r
γa,¬r
γ¬a,r
γ¬a,¬r







(18)

Quantum Bayesian decision-making

9

describing only the states of the featured variables. The utility function U (r) is
then applied to this state (|ΨA,R(cid:105)). Therefore, a quantum state |ΨU (cid:105) isomorphic
to the utility function will be created:

U (R) =

(cid:26) U (r)
U (¬r)

−→ |ΨU (cid:105) =













(cid:112)U (r)
n
(cid:112)U (¬r)
n

where n is a normalization term such that (cid:104)ΨA,R|ΨA,R(cid:105) sums up to 1. Also, by
creating both states in memory, the whole product state |ΨA,R(cid:105) ⊗ |ΨU (cid:105) becomes,

|ΨSyt(cid:105) = |ΨA,R(cid:105) ⊗ |ΨU (cid:105) =







γa,r
γa,¬r
γ¬a,r
γ¬a,¬r







⊗













(cid:112)U (r)
n
(cid:112)U (¬r)
n

=



































γa,r ∗

γa,r ∗

γa,¬r ∗

γa,¬r ∗

γ¬a,r ∗

γ¬a,r ∗

γ¬a,¬r ∗

γ¬a,¬r ∗

(cid:112)U (r)
n
(cid:112)U (¬r)
n
(cid:112)U (r)
n
(cid:112)U (¬r)
n
(cid:112)U (r)
n
(cid:112)U (¬r)
n
(cid:112)U (r)
n
(cid:112)U (¬r)
n



































(19)

Then, this state |ΨSyt(cid:105) already contains the relevant terms where the Utility
function is applied to the correct bases. All one has to do is to amplify them
resorting again to amplitude ampliﬁcation algorithm. For the example at hands,
such is the case when r ∧ r and ¬r ∧ ¬r hold, yielding,

10

Michael Oliveira et al.

QSearch2 |ΨSyt(cid:105) ≈






























γa,r ∗ (cid:112)U (r)
n(cid:48)
0

0
γa,¬r ∗ (cid:112)U (¬r)
n(cid:48)
γ¬a,r ∗ (cid:112)U (r)
n(cid:48)
0

0
γ¬a,¬r ∗ (cid:112)U (¬r)
n(cid:48)






























(20)

At this moment the amplitudes of the action variable hold the solution to the de-
cision problem. To understand how the Results variable and the Utility function
are traced out as follows,

ρA = trR,U (ρSyt) =







γa,r ∗ (cid:112)U (r) + γa,¬r ∗ (cid:112)U (¬r)
n(cid:48)

(

)2

...

A measurement yields,

...
γ¬a,r ∗ (cid:112)U (r) + γ¬a,¬r ∗ (cid:112)U (¬r)
n(cid:48)

(

(21)






)2

P (a(cid:48)) = tr(P0 ∗ρSyt) =

(cid:19)

(cid:18)1 0
0 0

∗ρSyt = (

γa,r ∗ (cid:112)U (r) + γa,¬r ∗ (cid:112)U (¬r)
n(cid:48)

)2 (22)

and,

P (¬a(cid:48)) = tr(P1 ∗ ρSyt) =

(cid:19)

(cid:18)0 0
0 1

∗ ρSyt = (

γ¬a,r ∗ (cid:112)U (r) + γ¬a,¬r ∗ (cid:112)U (¬r)
n(cid:48)

)2

Combining the deduced probability of the action variable with,

and

γ2
a,r = P (a, r, evidences)

P (r|a, evidences) =

P (a, r, evidences)
P (a, evidences)

(23)

(24)

(25)

Quantum Bayesian decision-making

11

we conclude that

γ2
a,r ∝ P (r|a, evidences)

(26)

and the transformation yields a state where

P (a(cid:48)

i) = tr(Pi ∗ ρSyt) =

γ2
ai,r1

∗ U (r1) + γ2

ai,r2

∗ U (r2) + ... + γ2

ai,rn

∗ U (rn)

n(cid:48)2

(27)

So, after applying there transformations the probability of some action (ai) is
proportional to its expected utility (Equation 6),

P (a(cid:48)

i) ∝ EU (ai|e)

P (a(cid:48)

i) = (cid:37)ai ∗ EU (ai|e)

Finally, if initially the Bayesian networks respects,

then

and

P (a|evidences) = P (a)

P (r|a, evidences) =

P (r, a|evidences)
P (a)

P (ai(cid:48)) = P (ai), i (cid:54)= i(cid:48)

The constant of proportionality takes the following value for all actions:

(cid:37)i(cid:48) = (cid:37)i =

1
n(cid:48)2 ∗ P (a)

, i (cid:54)= i(cid:48)

(28)

(29)

(30)

(31)

(32)

(33)

Thus, the values of the proportionality constants (cid:37)i between all the Expected
Utilities remain the same. This means that an action with a greater probability
has a greater expected utility. Consequently, to choose the action with the great-
est probability/utility (Figure 3), it is enough to resort to a limited collection of
samples, rather than obtaining ﬁrst all the conditional probabilities. Moreover,
this provides a more precise way of choosing an action because the sampling
method always yields an approximation and the error aﬀecting the computed
values grows for every conditional probability determined.

12

Michael Oliveira et al.

Fig. 3: Probability distribution of an action variable.

Remember, that this decision process requires that Equation (32) has to be
initially true. This expresses the rational choice which considers all actions as
equal at the beginning. In other words, the intelligent agent is not biased before-
hand. Additionally, the action variable should be independent of the evidence
variables as in Equation (30), which means that the topology of the network
has to be as in Figure 4. This requirement ensures that the intelligent agent is
not biased by the current state of his environment and performs his decisions in
order to achieve the best outcome in the future state.

Fig. 4: Bayesian network with an independent action node.

Quantum Bayesian decision-making

13

5 Complexity

The purpose of this Section is to characterize computational complexity of the
proposed algorithm and compare it to the solution that computes the conditional
probabilities with use of the quantum inference algorithm (Equation (16)). So, to
simplify let us denote by Process A the new quantum algorithm and by Process
B the second one.

Both algorithms generate samples to determine which is the best action. The
number of operations (It) in each algorithm is deﬁned by the number of iterations
per sample (Is) and the number of samples (S) necessary, as in Equation (34).

It = S ∗ Is

(34)

Number of iterations The number of iterations per sample of the two pro-
cesses are deﬁned by the number of Search iterations that are necessary to apply
in each case. Also, the number of iterations necessary to ﬁnd the goal state in a
quantum search is deﬁned by the probability of this state:

(cid:115)

Is =

1
P (state)

(35)

For Process A, this is the probability of the state which has already the utility

function applied to it,

knowing that,

P (state) =

(cid:88)

r

U (r) ∗ P (r, e)

(cid:88)

1 =

U (r)

r

(36)

(37)

Assuming that any distribution is possible for U(r) and P(r,e), we conclude that
the probability can take any value between 0 and P(e). We also know that the
mean value for U (r) is:

1
Nr
where Nr represents the dimension of the outcome variable. Thus, P(e,r) can be
described as:

(38)

So the mean value for the product of the two values U (r) ∗ P (e, r) will be,

P (e)
Nr

P (e)
Nr

∗

1
Nr

=

P (e)
N 2
r

(39)

(40)

14

Michael Oliveira et al.

if they are independent, which is the case because the utility function is inde-
pendent of the information present in the Bayesian Network. The mean value
for the sum can be computed by the sum over the mean terms

M ean(P (state)) =

(cid:88)

r

P (e)
N 2
r

=

P (e) ∗ Nr
N 2
r

=

P (e)
Nr

(41)

This mean value for the probability will be used to deﬁne the number of steps:

(cid:115)

Nr
P (e)

Is =

(42)

deﬁning in this way the number of iterations necessary to obtain a sample with
Process A.

The same has to be done for Process B, where the probabilty of the goal

state is

P (state) = P (e, a)

(43)

In this case, we have to apply the requirements determined by Process A de-
scribed in (30) and (32) in order to make a correct comparison at a later stage,

P (e, a) = P (e) ∗ P (a) =

P (e)
Na

(44)

where Na is the dimension of the action variable. Finally, we estimate the number
of iterations as

(cid:115)

Na
P (e)

Is =

(45)

Number of samples The next step is to obtain the number of samples neces-
sary for each process. Recall that the simultaneous error terms for a Multinomial
Distribution are:

(pi − πi)2 =

A ∗ πi(1 − πi)
N

, (i = 1, 2, ..., k)

(46)

The value A represents the upper α ∗ 100 − th percentile of a Chi-Square
Distribution (ﬁgure 5) with k-1 degrees of freedom, πi represent the probability
of category i, N is the number of samples, and the diﬀerence on the left side of
the equation represents the error term [7].

Quantum Bayesian decision-making

15

Fig. 5: Chi-square distributions with diﬀerent degrees of freedom (k).

Writing the same equation as a function of N , yields

N =

A ∗ πi(1 − πi)
δ2

, (i = 1, 2, ..., k)

(47)

Equation (47), deﬁnes the number of samples necessary for Process A and
Process B, since, both processes are sampling from a quantum state with multiple
bases.

Total number of operations The total number of operations is characterized
by the product of the terms deduced in the previous sections and the number
of operations necessary to encode the network as a quantum state. Additionally,
Process B requires at least 2(Na + Nr) operations to apply the Utility function
and sum the respective terms for the expected utilities. Finally, the total number
of operations for each process to solve the decision problem is shown in Table 2.

Table 2: Mean number of operations for each process.
Process A
(cid:113) Nr

Process B
P (e) ∗ A∗πi(1−πi)

P (e) ∗ A∗πi(1−πi)

n ∗ 2m ∗

(cid:113) Na

δ2
a

δ2
c

∗ Na + 2(Na + Nr)

n ∗ 2m ∗

When the decision problem is totally deﬁned all that it requires is to plug
the number in the equation and look which process performs better. However,
a comparison was performed (as described in Appendix A) and the relation
between the computational eﬀort is asymptotically over the term,

P rocessB
P rocessA

≥

(cid:114) Nr
Na

(48)

16

Michael Oliveira et al.

This result shows that Process A is faster when the outcome variable has
a greater dimension than the action variable. This is a quite normal scenario
in real applications because the number of states in which an agent can tran-
sit is tremendously smaller than the possible states that his environment can
evolve. Also, for a ﬁxed number of action this process allows the agent to ex-
plore quadratically more outcomes with the same computational eﬀort, making
him a wiser decision maker.

Finally, this process can also be compared with a quantum version of deci-
sion networks [25], as discussed in [20]. The results show that again the best
process depends on the characteristics of the problem. Although, it is important
to mention that process A solves a decision process that wants to sample an
action from a distribution based on the expected utilities in a extremely eﬃcient
way with only one sample, this kind of decision processes could be used and
studied for applications in reinforcement learning.

6 Proof-of-concept implementation

The algorithm presented in Section 4 was implemented on the IBM Q quantum
simulator as a proof-of-concept. At our disposal was the IBM 20-qubit machine,
which is based on superconducting circuits [28]. This machine speciﬁes an error
term associated with each gate used in a quantum circuit and a life-time for each
qubit. So, as the number of gates grows the error of the outcome grows as well.
The output of a circuit with a considerable number of gates would be majorly
noise. Thus, the decision processes presented before, which is based on a search
problem, would be impossible to compute with a manageable error term.

IBM’s best quantum computer is not the only that fails to solve such prob-
lems. The best quantum devices, in the world, are not even near to solve prob-
lems related to search problems with a higher dimension. However, that does
not mean that the current devices are completely useless. There are problems
where a Noisy Intermediate-Scale Quantum (NISQ) devices may have an impact,
in the near future [23]. The applications of these NISQ devices are related to
simulations in chemistry and many-body quantum physics. 2

Over the last years, quantum devices have had a lot of progress. For example,
the number of qubits are smoothly increasing, the gate errors are reducing [27]
and entanglement between them is becoming stronger [11,22]. This progress has
been giving hope to construct a powerful universal quantum computer, which
one day may have a great impact on our everyday life. But to validate results
as pretended, in this section a classical simulator has to be used. However, the
same simulator struggles to compute the outcomes, if the number of qubits used
increases. As mentioned before the complexity to simulate a quantum computer

2 It is interesting to mention that the major companies investing in quantum comput-
ing are constructing devices based on diﬀerent technologies. Microsoft devices are
based on topological quantum computing [16], while Intel is exploring spin qubits
[30].

Quantum Bayesian decision-making

17

on a classical computer is too high. Given that, a very simple Bayesian network
(Figure 6) was selected for the decision process.

Fig. 6: Bayesian network over 3 variables. Node L represents the evidence vari-
able, A the action variable and R the outcome variable.

The network was encoded to a quantum state with use of the technique presented
in [13], producing the circuit shown in Figure 7.

Fig. 7: Quantum circuit composed by rotations and controlled-rotations.

A value for the evidence variable L was selected (L = F alse) and the follow-

ing utility function,

U (R) =

(cid:26) 7
3

, R = r
, R = ¬r

(49)

was applied with use of the proposed algorithm. However, to compute the algo-
rithm on IBM’s quantum simulator, each part has be to converted in a concrete
quantum circuit (Figure 8). Every state has to be encoded, which in theory is
simple with the use of rotations and controlled-rotations. The major diﬃculty
exists when the rotation is controlled by more than one qubit. In such cases,
this operation has to be decomposed to simpler and available operations in the
working framework. The existence of an equivalent circuit is guaranteed by the
fact that the simulator is a universal quantum computer, meaning that it may

18

Michael Oliveira et al.

perform any possible computation. In practice, there are tools to decompose
complex operations into simpler ones [31,15].

...

Fig. 8: Circuit representation of the amplitude ampliﬁcation algorithm for the
decision-making process.

Finally, these circuits have to be built on ”Qiskit” which runs on a ”jupyter
notebook” (Github link ”Quantum Bayesian Decisions”). The circuits created
are sent as a job to IBM’s servers and the results are sent back to the client.
For this example, a signiﬁcant number of samples was generated. The number of
samples for each state of the action variable must be similar to the theoretical
probability. Table 3 shows that this is indeed the case: the experimental result
is quite similar to the one foreseen by theory.

The small discrepancies pointed out in Table 3 can be explained by de-
ﬁciencies of the implementation. First, note that the amplitude ampliﬁcation
algorithm is probabilistic, i.e. the result is never entirely precise. On the other
hand, the number of iterations in the amplitude ampliﬁcation algorithm was an
integer number; thus, if n.m non-integer iterations are required the usage of the
value bellow n or the one above n + 1, generates a small variation. Actually, it is
possible to perform a a quantum search with a non-integer number of iterations
[32], but this would not add relevance to this results.

Table 3: Comparison between theoretical and experimental results after sampling
from the constructed state.

States Theoretically expected probability Percentage of Samples
Action0
Action1

0.544
0.456

0,58
0,42

7 Conclusions and Future work

A quantum algorithm which solves the generic decision-making problem was pre-
sented. It is a very curious solution for couple of reasons. First, it has a proven
computational advantage over the classical and the semi-classical solutions when

Quantum Bayesian decision-making

19

the parameters are in the correct relation. Secondly, it samples from a very par-
ticular probability distribution, which classically would require an tremendous
amount of computational work to recreate. Moreover, it beneﬁts from the struc-
ture of the data, which no classical algorithm could beneﬁt from, making it
a very interesting example to illustrating the diﬀerences between classical and
quantum computations.

To support the theoretical work described the algorithm was implemented in
IBM’s quantum simulator as a proof-of-concept. The results were as in correspon-
dence with what theory anticipated for the example chosen, further conﬁrming
the ideas presented.

As an extension to this work we propose a search for decision problems that
take advantage by sampling from the probability distribution created by this
solution. Also, the decision-making process discussed here was related to a static
model, in which, neither the utility function nor the Bayesian network change in
time. It would be of interest to verify if the decision-making process could beneﬁt
from an additional learning process [10,24,29]. Enabling an agent to adapt its
behaviour to a changing environment, would probably result in better outcomes,
raising the number of possible applications.

References

1. Al-Jarrah, O.Y., Yoo, P.D., Muhaidat, S., Karagiannidis, G.K., Taha, K.: Eﬃ-
cient Machine Learning for Big Data: Review. CoRR abs/1503.0 (2015), http:
//arxiv.org/abs/1503.05296

2. Barnett, S.: Quantum Information. Oxford University Press, Inc., USA (2009)
3. Biamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., Lloyd, S.: Quan-
tum Machine Learning. Nature 549 (2016). https://doi.org/10.1038/nature23474
4. Brassard, G., Hoyer, P., Mosca, M., Tapp, A.: Quantum Amplitude Ampliﬁcation

and Estimation. arXiv e-prints pp. quant–ph/0005055 (2000)

5. Darwiche, A.: Chapter 11 Bayesian Networks. Foundations of Artiﬁcial Intelligence

3(07), 467–509 (2008). https://doi.org/10.1016/S1574-6526(07)03011-8

6. Giovannetti,

V.,
Access Memory.
https://doi.org/10.1103/PhysRevLett.100.160501

Lloyd,
Physical

S., Maccone,

review

letters

L.:

Quantum

100,

160501

Random
(2008).

7. Goodman,

L.A.:

for
Multinomial
(1965).
Technometrics
https://doi.org/10.1080/00401706.1965.10490252, https://amstat.tandfonline.
com/doi/abs/10.1080/00401706.1965.10490252

Conﬁdence
7(2),

Simultaneous

Proportions.

Intervals

247–254

On

8. Harrow, A.W., Hassidim, A., Lloyd, S.: Quantum Algorithm for Lin-
(2009).
of Equations. Phys. Rev. Lett. 103(15),
150502
https://link.aps.org/

ear Systems
https://doi.org/10.1103/PhysRevLett.103.150502,
doi/10.1103/PhysRevLett.103.150502

9. Inglot, T.: Inequalities for quantiles of the chi-square distribution. Probability and

Mathematical Statistics 30 (2010)

10. Jonsson, A., Barto, A.: Active Learning of Dynamic Bayesian Networks in
Markov Decision Processes. In: Proceedings of the 7th International Confer-
ence on Abstraction, Reformulation, and Approximation. pp. 273–284. SARA’07,

20

Michael Oliveira et al.

Springer-Verlag, Berlin, Heidelberg (2007), http://dl.acm.org/citation.cfm?
id=1770681.1770705

11. Kues, M., Reimer, C., Roztocki, P., Cort´es, L.R., Sciara, S., Wetzel, B., Zhang,
Y., Cino, A., Chu, S.T., Little, B.E., Moss, D.J., Caspani, L., Aza˜na, J., Moran-
dotti, R.: On-chip generation of high-dimensional entangled quantum states and
their coherent control. Nature 546, 622 (jun 2017), https://doi.org/10.1038/
nature22986http://10.0.4.14/nature22986

12. Li, C., Welling, M., Zhu, J., Zhang, B.: Graphical Generative Adversarial Networks.

CoRR abs/1804.0 (2018), http://arxiv.org/abs/1804.03429

13. Low, G.H., Yoder,

networks.

Bayesian
https://doi.org/10.1103/PhysRevA.89.062315,
10.1103/PhysRevA.89.062315

A

T.J.,
Phys.

Chuang,
Rev.

I.L.: Quantum inference
62315

on
2014).
https://link.aps.org/doi/

89(6),

(jun

14. Mansinghka, V.K.: Natively probabilistic computation. Ph.D.

thesis, Mas-

sachusetts Institute of Technology (2009)

15. M¨ott¨onen, M., Vartiainen, J.J., Bergholm, V., Salomaa, M.M.: Quantum
Circuits
for General Multiqubit Gates. Phys. Rev. Lett. 93(13), 130502
(2004). https://doi.org/10.1103/PhysRevLett.93.130502, https://link.aps.org/
doi/10.1103/PhysRevLett.93.130502

16. Nayak, C., Simon, S.H., Stern, A., Freedman, M., Das Sarma, S.: Non-Abelian
anyons and topological quantum computation. Rev. Mod. Phys. 80(3), 1083–
1159 (2008). https://doi.org/10.1103/RevModPhys.80.1083, https://link.aps.
org/doi/10.1103/RevModPhys.80.1083

17. Neal, R.M.: Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Hei-

delberg (1996)

18. von Neumann, J.: Various techniques used in connection with random digits. Monte

Carlo Method, Appl. Math. Series pp. 36–38 (1951)

19. Nielsen, M.A., Chuang, I.L.: Quantum Computation and Quantum Information

(10th Anniversary Edition). Cambridge University Press (2010)

20. Oliveira, M.: On Quantum Bayesian Networks. Physical engineering, University of

Minho (2019)

21. Ozols, M., Roetteler, M., Roland, J.: Quantum rejection sampling. In: Innovations
in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012.
pp. 290–308. ACM (2012)

22. Pirandola, S., Vitali, D., Tombesi, P., Lloyd, S.: Macroscopic Entan-
glement by Entanglement Swapping. Phys. Rev. Lett. 97(15), 150403
(2006). https://doi.org/10.1103/PhysRevLett.97.150403, https://link.aps.org/
doi/10.1103/PhysRevLett.97.150403

23. Preskill, J.: Quantum {C}omputing in the {NISQ} era and beyond. Quantum
79 (2018). https://doi.org/10.22331/q-2018-08-06-79, https://doi.org/10.

2,
22331/q-2018-08-06-79

24. Robinson, J.W., Hartemink, A.J.: Learning Non-Stationary Dynamic Bayesian
Networks. J. Mach. Learn. Res. 11, 3647–3680 (2010), http://dl.acm.org/
citation.cfm?id=1756006.1953047

25. Russel, S., Norvig, P.: Artﬁcial Intelligence : a modern approach. Upper Saddle

River, NJ : Prentice Hall, ed, 3rd edn. (2010)

26. Sakkaris, P.: QuDot Nets: Quantum Computers and Bayesian Networks. arXiv

e-prints (2016)

27. Sch¨afer, V.M., Ballance, C.J., Thirumalai, K., Stephenson, L.J., Ballance, T.G.,
Steane, A.M., Lucas, D.M.: Fast quantum logic gates with trapped-ion qubits.

Quantum Bayesian decision-making

21

Nature 555, 75 (feb 2018), https://doi.org/10.1038/nature25737http://10.
0.4.14/nature25737

28. Steﬀen, M., DiVincenzo, D.P., Chow, J.M., Theis, T.N., Ketchen, M.B.: Quantum
computing: An IBM perspective. IBM Journal of Research and Development 55(5),
13:1–13:11 (2011). https://doi.org/10.1147/JRD.2011.2165678

29. Tong, S., Koller, D.: Active Learning for Parameter Estimation in Bayesian Net-

works. Proc 13th Conf Neural Information Processing (2001)

30. Vandersypen, L.M.K., Bluhm, H., Clarke, J.S., Dzurak, A., Ishihara, R., Morello,
A., Reilly, D.J., Schreiber, L.R., Veldhorst, M.: Interfacing spin qubits in quantum
dots and donors—hot, dense, and coherent. npj Quantum Information 3, 1–10
(2017)
31. Vartiainen,

J.J., M¨ott¨onen, M.,

sition of Quantum Gates. Phys. Rev. Lett. 92(17),
https://doi.org/10.1103/PhysRevLett.92.177902,
10.1103/PhysRevLett.92.177902

Salomaa, M.M.: Eﬃcient Decompo-
(2004).
https://link.aps.org/doi/

177902

32. Zekrifa, D.M.S., Hoyer, P., Mosca, M., Tapp, A.: Quantum Amplitude Ampli-
ﬁcation and Estimation. AMS Contemporary Mathematics Series 305 (2000).
https://doi.org/10.1090/conm/305/05215

A Complexity comparison

The decision-making processes we aim at comparing require inequality (50) to
be satisﬁed. It assures that the decision maker chooses with certainty the best
action.

∀n\{max}EU (actionmax) − EU (actionn) > δactionmax + δactionn

(50)

Thus, to compare Process A and Process B it is necessary to consider all terms
that are diﬀerent. Therefore, the error term δa for Process A is related to directly
sampling values for the expected utilities, while in Process B the expected utility
is determined indirectly. For this reason, in Process B it is necessary to apply
error propagation rules:

EU (a|e) + δEU (a|e) =

(cid:88)

R

(P (Result = r|a, e) + δb) ∗ U (r)

(51)

Before applying error propagation to this equation, we need to normalize it so
that EU (a|e)/k is equal to P (a).

P (a) + δa =

(cid:88)

(P (Result = r|a, e) + δb) ∗ F (r)

R

where the normalization function (F (r)) is expressed as,

F (r) =

U (r)

(cid:80)
a

(cid:80)

r U (r) ∗ P (r|a, e)

Here, again, the mean value of U (r) is used:

(52)

(53)

22

Michael Oliveira et al.

F (r) =

U (r)

(cid:80)
a

(cid:80)

r P (r|a, e) ∗ U (r)

=

U (r)
(cid:80)
1
Nr

a

=

U (r)
Na
Nr

=

Nr ∗ U (r)
Na

F (r) =

1
Na

(54)

(55)

Expressing the equation that determines the error term δa as a function of the
error term δb yields

δa =

(cid:115)(cid:88)

R

δ2
b ∗ F (r)2

Using equation (55) we obtain:

δa =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:88)

R

δ2
b ∗ (

2
)

1
Na

(56)

(57)

Then, assuming that δb is similar, which is in favor of Process B because it
minimizes the δa term:

(cid:115)

δa =

Nr ∗ δ2

b ∗ (

2
)

1
Na

(58)

yielding,

√

Nr
Na
With the relation between the error terms determined, it is possible to compare
the diﬀerence of the computational eﬀort involved in both processes, assuming
again the mean terms for the probabilities:

δa = (

) ∗ δb

(59)

(cid:114) Na
Nr

∗

Ar,α ∗ 1
Nr
Aa,α ∗ 1
Na

∗ (1 − 1
Nr
∗ (1 − 1
Na

) ∗ δ2

a ∗ Na
) ∗ δ2
b

+

2 ∗ Na ∗ Nr

n ∗ 2m ∗

(cid:114) Nr
P (e)

∗

Aa,α ∗ 1
Na

)

∗ (1 − 1
Na
δ2
a

(60)

Let us call the term on the right,

Using 59,

t1 =

2 ∗ Na ∗ Nr

n ∗ 2m ∗

(cid:114) Nr
P (e)

∗

Aa,α ∗ 1
Na

∗ (1 − 1
Na
δ2
a

)

(cid:114) Na
Nr

∗

Nr
Na

∗

Ar,α ∗ 1
Nr
Aa,α ∗ 1
Na

∗ (1 − 1
Nr
∗ (1 − 1
Na

)

)

+ t1

(61)

(62)

also,

and,

Quantum Bayesian decision-making

23

(cid:114) Nr
Na

∗

Ar,α ∗ 1
Nr
Aa,α ∗ 1
Na

∗ (1 − 1
Nr
∗ (1 − 1
Na

)

)

+ t1

(cid:114) Nr
Na

∗

Ar,α ∗ ( 1
Nr
Aa,α ∗ ( 1
Na

− 1
N 2
r
− 1
N 2
a

)

)

+ t1

(63)

(64)

From [9] we obtain a lower bound for Aα,k. Althouhg these terms are diﬀerent
for distinct values of α, we consider the one where α is not leaning to zero too
fast. Thus,

Aα,k ≥ k + 2 ∗ log

1
α

−

5
2

(65)

With this equation it is possible to deﬁne a better value for the diﬀerence between
the computational eﬀorts,

(cid:114) Nr
Na

∗

(Nr + 2 ∗ log 1
(Na + 2 ∗ log 1

α − 7
α − 7

2 ) ∗ ( 1
Nr
2 ) ∗ ( 1
Na

− 1
N 2
r
− 1
N 2
a

)

)

+ t1

As

and,

lim
Nr→∞

(Nr + 2 ∗ log

lim
Na→∞

(Na + 2 ∗ log

1
α

−

1
α

−

7
2

7
2

) ∗ (

1
Nr

−

1
N 2
r

) = 1

) ∗ (

1
Na

−

1
N 2
a

) = 1

it is possible to approximate the expression to

(66)

(67)

(68)

(cid:114) Nr
Na

∗

(Nr + 2 ∗ log 1
(Na + 2 ∗ log 1

α − 7
α − 7

2 ) ∗ ( 1
Nr
2 ) ∗ ( 1
Na

− 1
N 2
r
− 1
N 2
a

)

)

+t1 ≈

(cid:114) Nr
Na

2 ∗ Na ∗ (cid:112)Nr ∗ P (e) ∗ δ2
n ∗ 2m

a

+

(69)
Writing the term of δa as a function of its dimension and a factor that adjusts
the precision,

we obtain,

δa =

1
c ∗ Na

(cid:114) Nr
Na

+

2 ∗ (cid:112)Nr ∗ P (e)
Na ∗ c2 ∗ n ∗ 2m

(70)

(71)

Rewriting this the expression as,

24

Michael Oliveira et al.

(cid:114) Nr
Na

∗ (1 +

√

2 ∗ (cid:112)P (e)
Na ∗ c2 ∗ n ∗ 2m

)

Because,

2 ∗ (cid:112)P (e)
Na ∗ c2 ∗ n ∗ 2m
for any value of the composing variables, then,

√

≥ 0

(cid:114) Nr
Na

∗ (1 +

√

2 ∗ (cid:112)P (e)
Na ∗ c2 ∗ n ∗ 2m

) ≥

(cid:114) Nr
Na

we prove that the relation between Process A and B is under bounded by,

(cid:114) Nr
Na

(72)

(73)

(74)

(75)

