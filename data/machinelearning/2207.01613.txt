2
2
0
2

l
u
J

4

]

G
L
.
s
c
[

1
v
3
1
6
1
0
.
7
0
2
2
:
v
i
X
r
a

Doubly-Asynchronous Value Iteration:
Making Value Iteration Asynchronous in Actions

Tian Tian
Department of Computer Science
University of Alberta
Edmonton, AB
ttian@ualberta.ca

Kenny Young
Department of Computer Science
University of Alberta
Edmonton, AB
kjyoung@ualberta.ca

Richard S. Sutton
Department of Computer Science
University of Alberta
Edmonton, AB
rsutton@ualberta.ca

Abstract

Value iteration (VI) is a foundational dynamic programming method, important for
learning and planning in optimal control and reinforcement learning. VI proceeds
in batches, where the update to the value of each state must be completed before
the next batch of updates can begin. Completing a single batch is prohibitively
expensive if the state space is large, rendering VI impractical for many applications.
Asynchronous VI helps to address the large state space problem by updating one
state at a time, in-place and in an arbitrary order. However, Asynchronous VI
still requires a maximization over the entire action space, making it impractical
for domains with large action space. To address this issue, we propose doubly-
asynchronous value iteration (DAVI), a new algorithm that generalizes the idea of
asynchrony from states to states and actions. More concretely, DAVI maximizes
over a sampled subset of actions that can be of any user-deﬁned size. This simple
approach of using sampling to reduce computation maintains similarly appealing
theoretical properties to VI without the need to wait for a full sweep through the
entire action space in each update. In this paper, we show DAVI converges to
the optimal value function with probability one, converges at a near-geometric
rate with probability 1 − δ, and returns a near-optimal policy in computation time
that nearly matches a previously established bound for VI. We also empirically
demonstrate DAVI’s effectiveness in several experiments.

1

Introduction

Dynamic programming has been utilized to solve many important real-world problems, including but
not limited to wireless networks (Levorato et al., 2012; Liu et al., 2017, 2019) resource allocation
(Powell et al., 2002), and inventory problems (Bensoussan, 2011). Value iteration (VI) is a foun-
dational dynamic programming algorithm central to learning and planning in optimal control and
reinforcement learning.

VI is one of the most widely studied dynamic programming algorithm (Williams and Baird III, 1993;
Bertsekas and Tsitsiklis, 1996; Puterman, 1994). VI starts from an arbitrary value function and
proceeds by updating the value of all states in batches. The value estimate for each state in the state

Preprint. Under review.

 
 
 
 
 
 
space S must be computed before the next batch of updates will even begin:

(cid:40)

v(s) ← max
a∈A

r(s, a) + γ

(cid:88)

s(cid:48)∈S

(cid:41)

p(s(cid:48)|s, a)¯v(s(cid:48))

for all s ∈ S,

(1)

where s(cid:48) ∈ S is the next state, p(s(cid:48)|s, a) is the transition probability, and r(s, a) is the reward. VI
.
maintains two arrays of real values v, ¯v, both the size of state space S
= |S|, with ¯v being used to
make the update, and v being used to keep track of the updated values. In domains with large state
space, for example wireless networks where the state-space of the network scales exponentially in the
number of nodes, computing a single batch of state value updates using VI is prohibitively expensive,
rendering VI impractical.

An alternative to updating the state values in batches is to update the state values one at a time, using
the most recent state value estimates in the computation. Asynchronous value iteration (Williams
and Baird III, 1993; Bertsekas and Tsitsiklis, 1996; Puterman, 1994) starts from an arbitrary value
function and proceeds with in-place updates:

(cid:40)

v(sn) ← max
a∈A

r(sn, a) + γ

(cid:41)

p(s(cid:48)|sn, a)v(s(cid:48))

,

(cid:88)

s(cid:48)∈S

(2)

where sn ∈ S is the sampled state for update in iteration n with all other states s (cid:54)= sn value remain
the same.

.
= r(s, a) + γ (cid:80)

Although Asynchronous VI helps to address domains with large state space, it still requires a sweep
over the actions for every update due to the maximization operation over the look-ahead values:
Lv(s, a)
s(cid:48)∈S p(s(cid:48)|s, a)v(s(cid:48)) for a given v ∈ RS. Evaluating the look-ahead values,
as large as the action space, can be prohibitively expensive in domains with large action space. For
example, running Asynchronous VI would be impractical in ﬂeet management, where the action set
is exponential in the size of the ﬂeet.

There have been numerous works that address the large action space problem. Szepesvári and Littman
(1996) proposed an algorithm called sampled-max, which performs a max operation over a smaller
subset of look-ahead values. Their algorithm resembles Q-learning, requiring a step-size parameter
that needs additional tuning. However, their algorithm does not converge to the optimal value
function v∗. Williams and Baird III (1993) presented convergence analysis on a class of asynchronous
algorithms, including some that may help address the large action space problem. Hubert et al. (2021)
and Danihelka et al. (2022) applied the idea of sampling a subset of actions and using the subset
in policy improvement and evaluation to improve planning in Monte Carlo Tree Search. Ahmad
et al. (2020) focused on the generation of a smaller candidate action set to be used in planning with
continuous action space.

We consider the setting where we have access to the underlying model of the environment and propose
a variant of Asynchronous VI called doubly-asynchronous value iteration (DAVI) that generalizes
the idea of asynchrony from states to states and actions. Like Asynchronous VI, DAVI samples a
state for the update, eliminating the need to wait for all states to complete their update in batches.
Unlike Asynchronous VI, DAVI samples a subset of actions of any user-deﬁned size via a predeﬁned
sampling strategy. It then computes only the look-ahead values of the corresponding subset and a
best-so-far action, and updates the state value to be the maximum over the computed values. The
intuition behind DAVI is the idea of incremental maximization, where maximizing over a few actions
could improve the value estimate for a certain state, which helps to evaluate other state-action pairs
in subsequent back-ups. This simple approach of using sampling to reduce computation maintains
similarly appealing theoretical properties to VI. In particular, we show DAVI converges to v∗ with
probability 1 and at a near-geometric rate with probability 1 − δ. Additionally, DAVI returns an
(cid:15)-optimal policy with probability 1 − δ using

(cid:18)

(cid:18)

O

mSHγ,(cid:15)

ln

(cid:18) SHγ,(cid:15)
δ

(cid:19) (cid:46)

(cid:18)

ln

1
1 − qmin

(cid:19)(cid:19)(cid:19)

(3)

elementary operations, where m is the size of the action subset, Hγ,(cid:15) is a horizon term. and qmin
is the minimum probability that any state-action pair is sampled. We also provide a computational
complexity bound for Asynchronous VI, which to the best of our knowledge, has not been previously
reported. Our computational complexity bounds for both DAVI and Asynchronous VI nearly match

2

a previously established bound for VI (Littman et al., 1995). Finally, we demonstrate DAVI’s
effectiveness in several experiments.

Related work by Zeng et al. (2020) uses an incremental maximization mechanism that is similar to
ours. However, their work focuses on a different setting from ours, an asynchronous parallel setting,
where an algorithm is hosted on several different machines running concurrently without waiting
for synchronization of the computations. Aside from this difference, their work considers the case
where the agent has access to only a generative model (Kearns et al., 2002) of the environment,
whereas we assume full access to the transition dynamics and reward function. They also provided a
computational complexity bound, which differs signiﬁcantly from ours due to differences in settings.

2 Background

We consider a ﬁnite discounted Markov Decision Process (S, A, r, p, γ) consisting of a ﬁnite set
of states S, a ﬁnite set of actions A, a stationary reward function r : S × A → [0, 1], a stationary
transition function p : S × A × S → [0, 1], and a discount parameter γ ∈ [0, 1). The stochastic
transition between state s ∈ S and the next state s(cid:48) ∈ S is the result of choosing an action a ∈ A in s.
For a particular s, a ∈ S × A, the probability of landing in various s(cid:48) is characterized by a transition
probability, which we denote as p(s(cid:48)|s, a).

For this paper, we use Π to denote a set of deterministic Markov polices {π : S → A}. The value
.
= E [(cid:80)∞
n=0 γnr(sn, an)|s0 = s] for
function of a state evaluated according to π is deﬁned as vπ(s)
.
a s ∈ S. The optimal value function for a state s ∈ S is then v∗(s)
= maxπ∈Π vπ(s), and there
exists a deterministic optimal policy π∗ for which the value function is v∗. For the rest of the paper,
we will consider v as a vector of values of RS. For a ﬁxed (cid:15) > 0, a policy π is said to be (cid:15)-optimal
if vπ ≥ v∗ − (cid:15)1. Finally, we will use (cid:107) · (cid:107) to denote inﬁnity norm (i.e, (cid:107)v(cid:107) = maxi |vi|) and A to
denote the size of the action space.

3 Making value iteration asynchronous in actions

We assume access to the reward and transition probabilities of the environment. In each iteration n,
DAVI samples a state sn for the update and m actions from the action set A. It then computes the
corresponding look-ahead values of the sampled actions and the look-ahead value of a best-so-far
action πn(sn). Finally, DAVI updates the state value to the maximum over the computed look-ahead
values. The size of the action subset can be any user-deﬁned value between 1 and A. To maintain a
best-so-far action, one for every state amounts to maintaining a deterministic policy in every iteration.
.
= r(s, a) + γ (cid:80)
Recall Lv(s, a)
s(cid:48)∈S p(s(cid:48)|s, a)v(s(cid:48)) for a given s, a ∈ S × A and v ∈ RS, the
pseudo-code for DAVI is shown in Algorithm 1.

Note that the policy will only change if there is another action in the newly sampled subset whose
look-ahead value is strictly better than the current best-so-far action’s look-ahead value.

4 Convergence

We show in Theorem 1 that DAVI converges to the optimal value function despite only maximizing
over a subset of actions in each update. Before showing the proof, we establish some necessary
deﬁnitions, lemmas and assumptions.

Deﬁnition 1 (qmin and pmin) Recall p is a distribution over states and q is a potentially state
conditional distribution over the sets of actions of size m. Then, we will use ˜q(s, a) to denote the
joint probability that a single state s ∈ S is sampled for update with a particular action a included
in the set sampled by q. Furthermore, let qmin

.
= mins,a ˜q(s, a) and pmin

.
= mins p(s).

Deﬁnition 2 (Bellman optimality operator and policy evaluation) Let T : RS → RS. For all
.
= maxa∈A Lv(s, a). Let Tπ : RS → RS. For a given π, for all s ∈ S, deﬁne
s ∈ S, deﬁne T v(s)
.
= Lv(s, π(s)).
Tπv(s)

3

Algorithm 1: DAVI(m, p, q, τ )
Input: State sampling distribution p ∈ ∆(S)
Input: A potentially state conditional distribution over the sets of actions of size m denoted by q
Input: Number of iterations τ , see Corollary 1 for how to choose τ to obtain an (cid:15)-optimal policy

with high probability
1 Initialize the value function v0 ∈ RS
2 Initialize the policy π0 to an arbitrary deterministic policy
3 for n = 0, · · · , τ do
4

Sample a state from p
Sample m actions from q(·|sn) and let it be An
Let a∗
Value back-up step:

n = arg maxa∈An Lvn (s, a) with ties broken randomly

(cid:26)max {Lvn(s, a∗

n), Lvn (s, πn(s))}

vn+1(s) =

vn(s)

if s = sn
otherwise

Policy improvement step:
(cid:26)a∗
n
πn(s) otherwise

πn+1(s) =

if s = sn and Lvn (s, a∗

n) > Lvn (s, πn(s))

5

6

7

8

9

10

11 end
12 return vn, πn

Deﬁnition 3 (DAVI back-up operator Tn) Let Tn : RS → RS. For a given An ∼ ˜q, πn ∈ Π,
sn ∈ S, and for all s ∈ S and v ∈ RS, deﬁne

Tnv(s)

.
=

(cid:26)maxa∈An∪πn(s) Lv(s, a)
v(s)

for s = sn
otherwise.

(4)

We show in Appendix A that Tn is a monotone operator.

Assumption 1 (Initialization) We consider the following initialisations, (i) v0 = 01, or (ii) v0 =
−c1 for c > 0, or (iii) v0(s) ≤ Lv0 (s, π0(s)) for all s ∈ S.

Lemma 1 (Monotonicity) If DAVI is initialized according to (i),(ii), or (iii) of Assumption 1, the
value iterates of DAVI, (vn)n≥0 is a monotonically increasing sequence: vn ≤ vn+1 for all n ∈ N0,
if r(s, a) ∈ [0, 1] for any s, a ∈ S × A.

Proof: See Appendix A.

(cid:4)

Lemma 2 (Boundedness(Williams and Baird III, 1993)) Let vmax = maxs v0(s) and vmin =
mins v0(s) and recall that any reward ∈ [0, 1]. If we start with any (v0, π0), then applying DAVI’s
operation on the (v0, π0) thereafter, will satisfy: min {0, vmin} ≤ Vn(s) ≤ max
, for
all s ∈ S and for all n ∈ N0.

1−γ , vmax

(cid:110) 1

(cid:111)

Lemma 3 (Fixed-point iteration (Szepesvári, 2010)) Given any v ∈ RS, and T, Tπ deﬁned in
Deﬁnition 2

1. vπ = limn→∞ T n

π v for a given policy π. In particular for any n ≥ 0, (cid:107)T n

π v − vπ(cid:107) ≤

γn(cid:107)v − vπ(cid:107) where vπ is the unique function that satisﬁes Tπvπ = vπ

2. v∗ = limn→∞ T nv and in particular for any n ≥ 0, (cid:107)v∗ − T nv(cid:107) ≤ γn(cid:107)v∗ − v(cid:107), where v∗

is the unique function that satisﬁes T v∗ = v∗.

Theorem 1 (Convergence of DAVI) Assume that ˜q(s, a) > 0 and r(s, a) ∈ [0, 1] for any s, a ∈
S × A, then DAVI converges to the optimal value function with probability 1, if DAVI initializes
according to (i),(ii), or (iii) of Assumption 1

Proof:

4

By the Monotonicity Lemma 1 and Boundedness Lemma 2, DAVI’s value iterates (vn)n≥0 are
a bounded and monotonically increasing sequence. By the monotone convergence theorem,
.
= ¯v. It remains to show that ¯v = v∗. We ﬁrst show ¯v ≤ v∗ and then
limn→∞ vn = supn vn
show ¯v ≥ v∗ to conclude that ¯v = v∗. We note that vn = Tn−1vn−1 ≤ T vn−1, where T is the
Bellman optimality operator that satisﬁes the Fixed-point Lemma 3 (2). By the monotonicity of T
and the monontonicity of vn’s, for any n ≥ 0, vn ≤ T nv0. By taking the limit of n → ∞ on both
sides, we get ¯v ≤ v∗.
Now, we show ¯v ≥ v∗. Let (nk)∞
k=0 be a sequence of increasing indices, where n0 = 0, such that
between nk-th and nk+1-th iteration, all state s ∈ S have been updated at least once with an action
set containing π∗(s). We note that the number of iterations between any nk and nk+1 is ﬁnite with
probability 1 since there is a ﬁnite number of states and actions, and all state-action pairs are sampled
with non-zero probability. Then, for any state s, let t(s, k) be an iteration index such that st(s,k) = s,
nk ≤ t(s, k) ≤ nk+1 and π∗(s) ∈ At(s,k), then

vnk+1(s) ≥ vt(s,k)+1(s)

by monotonicity of vn

(cid:26)

= max

max
a∈{At(s,k)∪πt(s,k)}\π∗(s)

Lvt(s,k) (s, a), Lvt(s,k)(s, π∗(s))

(cid:27)

≥ Lvt(s,k) (s, π∗(s))

= Tπ∗ vt(s,k)(s) ≥ Tπ∗ vnk (s) by monotonicity of the operator.

(5)

(6)

(7)

By the nk+1-th iteration, vnk+1 ≥ Tπ∗ vnk , where Tπ∗ is the policy evaluation operator that satisﬁes
the Fixed-point Lemma 3 (1). Continuing with the same reasoning, and for any k ≥ 0, vnk ≥ T k
π∗ v0.
(cid:4)
By taking limit of k → ∞ on both sides, we get ¯v ≥ v∗. Altogether, ¯v = v∗.

Remark 1: The initialization requirement in the Convergence of DAVI Theorem 1 can be relaxed to
be any initialization, and DAVI will still converge to v∗ with probability 1. A more general proof can
be found in Appendix A, which follows a similar argument to that of the proof for Theorem 4.3.2 in
Williams and Baird III (1993) paper. Intuitively, there exists a ﬁnite sequence of value back-up and
policy improvement operations that will lead to one contraction, and if there are l ∈ N copies of such
sequence, this will lead to l contractions. Once the value iterates contract into an “optimality-capture”
region, where all the policies πn are optimal thereafter, DAVI is performing policy evaluations of an
optimal policy. As long as all states are sampled inﬁnitely often, the value iterates must converge
to v∗. Finally, we show that such a ﬁnite sequence as a contiguous subsequence exists in an inﬁnite
sequence of operators generated by a stochastic process.

Remark 2: DAVI could be considered an Asynchronous Policy Iteration (Bertsekas and Tsitsiklis,
1996) in that DAVI consists of a policy improvement step and a policy evaluation step. However, the
algorithmic construct discussed in Bertsekas and Tsitsiklis (1996) does not exactly match that of
DAVI with sampled action subsets. Consequently, we could not directly apply Proposition 2.5 of
Bertsekas and Tsitsiklis (1996) to show DAVI’s convergence. A more useful analysis can be found
in Williams and Baird III (1993), and we could have applied their Theorem 4.2.6 to show DAVI’s
convergence after having shown that DAVI’s value iterates are monotonically increasing in Lemma 1.
However, Williams and Baird III (1993) analysis provides no convergence rate or computational
complexity. Therefore, we chose to present a different convergence proof, more closely related to the
convergence rate proof in the next section.

5 Convergence rate

DAVI relies on sampling to reduce computational complexity, which introduces additional errors.
Despite this, we show in Theorem 2 that DAVI converges at a near-geometric rate and nearly matches
the computational complexity of VI.

Theorem 2 (Convergence rate of DAVI) Assume ˜q(s, a) > 0 and r(s, a) ∈ [0, 1] for any s, a ∈
S × A, and also assume DAVI initialises according to (i), (ii), (iii) of Assumption 1. With γ ∈ [0, 1)
and probability 1 − δ, the iterates of DAVI, (vn)n≥0 converges to v∗ at a near-geometric rate. In
particular, with probability 1 − δ, for a given l ∈ N,

(cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107)

(8)

5

for any n satisfying

where qmin = mins,a ˜q(s, a).

(cid:24)

n ≥ l

ln

(cid:19) (cid:46)

(cid:18)

ln

(cid:18) Sl
δ

1
1 − qmin

(cid:19)(cid:25)

,

(9)

Proof: Recall from Lemma 1, we have shown vn → v∗ monotonically from below. From Theorem 1,
we have also deﬁned (nk)∞
k=0 to be a sequence of increasing indices, where n0 = 0, such that
between the nk-th and nk+1-th iteration, all state s ∈ S have been updated at least once with an
action set containing π∗(s). At the nk+1-th iteration, vnk+1 ≥ Tπ∗ vnk . This implies that at the
nk+1-th iteration, DAVI would have γ-contracted at least once:

0 ≤ v∗ − vnk+1 ≤ v∗ − Tπ∗ vnk , =⇒ (cid:107)v∗ − vnk+1 (cid:107) ≤ (cid:107)v∗ − Tπ∗ vnk (cid:107),
(cid:107)v∗ − Tπ∗ vnk (cid:107) = (cid:107)Tπ∗ v∗ − Tπ∗ vnk )(cid:107) ≤ γ(cid:107)v∗ − vnk (cid:107)

=⇒ (cid:107)v∗ − vnk+1(cid:107) ≤ γ(cid:107)v∗ − vnk (cid:107).

(10)

(11)
(12)

Consider dividing n ∈ N iterations into uniform intervals of length N such that the i-th interval is
(iN, (i + 1)N − 1). Let Ei(s) denote the event that at some iteration in the i-th interval, state s has
.
been updated with an action set containing π∗(s). Therefore, an occurrence of event Ei
= ∩s∈S Ei(s)
would mean that at (i + 1)N -th iteration, v(i+1)N would have contracted at least once. Then, on the
.
event E
= ∩iEi = ∩i ∩s∈S Ei(s), there have been at least l γ-contraction after n iterations.
We would like P(E) ≥ 1 − δ or alternatively the probability of failure event P(E c) ≤ δ, for some
δ > 0. However, just how large should N be in order to maintain a failure probability of δ? To
answer this question, we ﬁrst bound P(E c) using union bound:

P(E c) = P(∪i ∪s∈S E c

i (s)) ≤

l
(cid:88)

(cid:88)

i=1

s∈S

P(E c

i ).

(13)

From Deﬁnition 1, ˜q(s, π∗(s)) is the joint probability that a single state s is sampled for update with
π∗(s) included in the action subset sampled by q. Then, the probability that state s is not updated
with an action set containing π∗(s) in N iterations is (1 − ˜q(s, π∗(s)))N . Continuing from (13),

where qmin

l
(cid:88)

(cid:88)

P(E c) ≤

(1 − ˜q(s, π∗(s)))N ≤ Sl(1 − qmin)N ,

i=1

s∈S

.
= mins,a ˜q(s, a). Now set Sl(1 − qmin)N ≤ δ and solve for N ,
(cid:18) δ
Sl

ln (1 − qmin) .

N ≥ ln

(cid:19) (cid:46)

Thus, with probability at least 1 − δ, within

(cid:24)

n = l

ln

(cid:19) (cid:46)

(cid:18)

ln

(cid:18) Sl
δ

1
1 − qmin

(cid:19)(cid:25)

iterations DAVI will have γ-contracted at least l times.

(14)

(15)

(16)

(cid:4)

Remark: We note that p, q could be non-stationary and potentially chosen adaptively based on
current value estimates, which is an interesting direction for future work.

Corollary 1 (Computational complexity of obtaining an (cid:15)-optimal policy) Fix an (cid:15) ∈ (0, (cid:107)v∗ −
v0(cid:107)), and assume DAVI initialises according to (i), (ii), or (iii) of Assumption 1. Deﬁne

Hγ,(cid:15)

.
= ln

(cid:18) (cid:107)v∗ − v0(cid:107)
(cid:15)

(cid:19)

/1 − γ

as a horizon term. Then, DAVI run for at least

(cid:18)

τ = Hγ,(cid:15)

ln

(cid:19)

(cid:18) SHγ,(cid:15)
δ

(cid:18)

/ ln

1
1 − qmin

(cid:19)(cid:19)

(17)

(18)

iterations, returns an (cid:15)-optimal policy πn : vπn ≥ v∗ − (cid:15)1 with probability at least 1 − δ using
O (mSτ ) elementary arithmetic and logical operations, where m is the size of the action subset and
S is the size of the state space. Note that (cid:107)v∗ − v0(cid:107) is unknown but it can be upper bounded by
1
1−γ + (cid:107)v0(cid:107) given rewards are in [0, 1].

6

Table 1: Computational complexity of VI, Asynchronous VI, DAVI

Algorithms

VI

Computational complexity References
(cid:17)

(cid:16)

Littman et al. (1995)

O

S2AHγ, (cid:15)(1−γ)
(cid:18)

Asynchronous VI O

ASHγ,(cid:15)

DAVI

(cid:18)

O

mSHγ,(cid:15)

(cid:17)

(cid:19)

(cid:17)

(cid:19)

2γ
(cid:16) SHγ,(cid:15)
ln
δ
(cid:16)

1
1−pmin
(cid:16) SHγ,(cid:15)
δ

ln

ln
(cid:16)

ln

1
1−qmin

(cid:17)

This paper

(cid:17)

This paper

Proof: See Appendix A.

(cid:4)

Remark: As a straightforward consequence of Theorem 1 and Corollary 1, we show in Appendix A,
Corollary 2, that DAVI returns an optimal policy π∗ with probability 1 − δ within a number of
computations that depends on the minimal value gap between the optimal action and the second-best
action with respect to v∗.

We can compare the computational complexity bound for DAVI (the result of Corollary 1) to
similar bounds for Asynchronous VI and VI. As far as we know, computational complexity bounds
for Asynchronous VI have not been reported in the literature. We followed similar argument to
Theorem 2 and Corollary 1 to obtain the computational complexity bound for Asynchronous VI in
Appendix B.

Recall that qmin
form sampling of the states results in a probability of 1
sampling of m actions without replacement results in a probability of m
tion in the subset. Altogether qmin = m
pling strategy for the bound O
strategy would result in ˜qmin < m
SA. Therefore, DAVI’s computational complexity O

.
= mins,a ˜q(s, a). Consider the case of uniform sampling of states and actions. Uni-
S of sampling a particular state, while uniform
A of including a particular ac-
SA . Uniform sampling of state and action subset is the best sam-
/ ln
mSHγ,(cid:15)
ln
because any non-uniform
(cid:1) =
SA )(cid:1) ≈ m/ (cid:0) m
SA , then −m/ (cid:0)ln(1 − m
SA . Suppose ˜qmin = m
SA
(cid:16)
(cid:17)(cid:17)(cid:17)
(cid:16)
mSHγ,(cid:15)
≈
/ ln
ln

(cid:16) SHγ,(cid:15)
δ

1
1−qmin

(cid:17)(cid:17)(cid:17)

(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:16) SHγ,(cid:15)
δ

1
1−qmin

(cid:16)

(cid:16) SHγ,(cid:15)
δ

(cid:17)(cid:17)

S2AHγ,(cid:15) ln

O
so it follows that −1/ (cid:0)ln (cid:0)1 − 1
O

. Likewise, uniform sampling of state will result in pmin = 1

S , and
(cid:1)(cid:1) ≈ S. Then, Asynchronous VI’s computational complexity
(cid:16) SHγ,(cid:15)
δ

S2AHγ,(cid:15) ln

(cid:16) SHγ,(cid:15)
δ

ASHγ,(cid:15)

≈ O

(cid:17)(cid:17)(cid:17)

1
1−pmin

/ ln

(cid:17)(cid:17)

ln

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

S

.

Chen and Wang (2017) have established a lower bound on the computational complexity of planning
in ﬁnite discounted MDP to be Ω(S2A). The importance of their result shows that no algorithm
can escape this S2A computational complexity. Both DAVI and Asynchronous VI computational
complexity matches that of the lower-bound Ω(S2A) up to log terms in S2A but have additional
dependence on Hγ,(cid:15) and ln(1/δ).

VI, Asynchronous VI, and DAVI all include a horizon term. The horizon term Hγ,(cid:15) improves
(cid:16) 2γ(cid:107)v∗−v0(cid:107)
upon the horizon term of Hγ, (cid:15)(1−γ)
/(1 − γ) that appears in VI (Littman et al.,
(cid:15)(1−γ)
1995) when γ > 0.5. As VI does not require sampling, it has no failure probability. Thus, DAVI
and Asynchronous VI both have an additional ln (cid:0) 1
(cid:1). We leave open the question of whether the
additional log term ln(SHγ,(cid:15)) in DAVI and Asynchronous VI is necessary.

= ln

(cid:17)

2γ

δ

The computational complexity of DAVI nearly matches that of VI, but DAVI does not need to sweep
through the action space in every state update. Similar to Asynchronous VI, DAVI also does not need
to wait for all states to complete their update in batches, as is the case of VI, making DAVI a more
practical algorithm.

6 Experiments

DAVI relies on sampling to reduce the computation of each update, and the performance of DAVI
can be affected by the sparsity of rewards. If a problem is like a needle in a haystack, where only

7

one speciﬁc sequence of actions leads to a reward, then we do not anticipate uniform sampling to be
beneﬁcial in terms of total computation. An algorithm would still have to consider most states and
actions to make progress in this case. On the other hand, we hypothesize that DAVI would converge
faster than Asynchronous VI in domains with multiple optimal or near-optimal policies. To isolate the
effect of reward sparsity from the MDP structure, we ﬁrst test our hypothesis on several single-state
MDP domains. However, solving a multi-state MDP is generally more challenging than solving a
single-state MDP. In our second experiment, we examine the performance of DAVI on two sets of
MDPs: an MDP with a tree structure and a random MPD.

The algorithms that will be compared in the experiments are VI, Asynchronous VI, and DAVI. We
implement Asynchronous VI and DAVI using uniform sampling to obtain the states. DAVI samples a
new set of actions via uniform sampling without replacement in each iteration.

6.1 Single-state experiment

This experiment consists of a single-state MDP with 10000 actions, all terminate immediately. We
experiment on two domains: needle-in-the-haystack and multi-reward. Needle-in-the-haystack has
one random action selected to have a reward of 1, with all other rewards set to 0. Multi-reward
has 10 random actions with a reward of 1. The problems in this single-state experiment amount to
brute-force search for the actions with the largest reward.

6.2 Multi-state experiment

This experiment consists of two sets of MDPs. The ﬁrst set consists of a tree with a depth of 2. Each
state has 50 actions, where each action leads to 2 other distinct next states. All actions terminate at
the leaf states. Rewards are 0 everywhere except at a random leaf state-action pair, where reward is
set to 1. With this construct, there are around 10000 states. The second set consists of a random MDP
with 100 states, and each state has 1000 actions. Each action leads to 10 next states randomly selected
from the 100 states with equal probability. All transitions have a 0.1 probability of terminating. A
single state-action pair is randomly chosen to have a reward of 1. The γ in all of the MDPs are 1.

6.3 Discussion

Figure 1 and Figure 2 show the performance of the algorithms. All graphs included error bars showing
the standard error of the mean. Notice that all graphs started at 0 and eventually reached an asymptote
unique to each problem setting. All graphs smoothly increased towards the asymptote except for
Asynchronous VI in Figure 1 and VI in Figure 2, whose performances were step-functions 1. The
y-axis of each graph showed a state value averaged over 200 runs. The x-axes showed run-times,
which have been adjusted for computations.

In Figure 1(a,b), DAVI with m = 1 was signiﬁcantly different from that of DAVI with m =
10, 100, 1000, and DAVI with m = 10, 100, 1000 converged at a similar rate. While in Figure 2(a,b),
all algorithms were signiﬁcantly different. DAVI m = 10 in random MDP Figure 2(b) converged
faster than any other algorithms. These results suggest that an ideal m exists for each domain.

In the needle-in-the-haystack setting Figure 1(a), all four DAVI algorithms made some progress by
the 10000 computation mark while Asynchronous VI stayed ﬂat. Once Asynchronous VI ﬁnished
computing the look-ahead value for all of the actions, it reached the asymptote immediately. On
the other hand, DAVI might have been lucky in some of the runs, found the optimal action amongst
the subset early on, logged it as a best-so-far action, and had a state value sustained at 1 thereafter.
However, there could also be runs where sampling had the opposite effect.

Changing the reward structure by introducing a few redundant optimal actions into the action space
increased the probability that an optimal action was included in the subset. In the multi-reward
setting Fig. 1(b), DAVI with all settings of m have essentially reached the asymptote by the 10000
mark. DAVI with all four action subset sizes reached the asymptote faster than Asynchronous VI. As
expected, DAVI converged faster than Asynchronous VI in the case of multiple rewarding actions.

In the Figure 2(a), we saw a similar performance to that of the needle-in-the-haystack in the single-
state experiment Figure 1(a). When we changed the MDP structure to allow for multiple possible

1Asychronous VI in the single-state experiment is equivalent to VI since there is only one state.

8

paths that led to the special state with the hidden reward, as evident in Figure 2(b), DAVI with all
settings of m all reached the asymptote faster than Asynchronous VI and VI. As expected, DAVI
converged faster than Asynchronous VI in the case of multiple near-optimal policies.

We note that perhaps many other real-world problems resemble a setting like random-MDP more
than needle-in-the-haystack, and hence the result on random-MDP may be more important. See
Appendix C for additions experiments with rewards drawn from Normal and Pareto distributions.

Figure 1: Single-state experiment with 10, 000 actions: (a) has only one action with a reward of 1 (b)
has 10 actions with a reward of 1. The Asynchronous VI in this experiment is equivalent to VI since
there is only one state. We run each instance 200 times with a new MDP generated each time. In
each run, each algorithm is initialized to 0.

Figure 2: Multi-state-action needle-in-the-haystack experiment: (a) MDP with a tree structure (b)
random MDP. We run each instance 200 times with a new MDP generated each time. In each run, all
algorithms are initialized to 0.

7 Conclusion

The advantage of running asynchronous algorithms on domains with large state and action space have
been made apparent in our studies. Asynchronous VI helps to address the large state space problem
by making in-place updates, but it is still intractable in domains with large action space. DAVI is
asynchronous in the state updates as in Asynchronous VI, but also asynchronous in the maximization
over the actions. We show DAVI converges to the optimal value function with probability 1 and
at a near-geometric rate with a probability of at least 1 − δ. Asynchronous VI also achieves a
computational complexity closely matching that of VI. We give empirical evidence for DAVI’s
computational efﬁciency in several experiments with multiple reward settings.

9

10state value averaged over 200 runs10,0000100,000run-time (# of operations x iterations)(a) needle-in-the-haystack10,000020,000run-time (# of operations x iterations)(b) multi-reward10Asynchronous VIDAVI m=1DAVI m =10DAVI m=100DAVI m=1000root state value averaged over 200 runs00.125260100run-time (# of operations x iterations)in 1 billions(a) Tree15(b) Random MDPstate 0 value averaged over 200 runs00.15run-time (# of operations x iterations)in 10 millions0250VIAsynchronous VIDAVI m =1DAVI m =10DAVI m =1007VIAsynchronous VIDAVI m=1DAVI m =10Note that DAVI does not address the summation over the states: (cid:80)
s(cid:48)∈S p(s(cid:48)|s, a)v(s(cid:48)) in the com-
putation of the look-ahead values. If the state space is large, computing such a sum can also be
prohibitively expensive. Prior works by Van Seijen and Sutton (2013) use “small back-ups” to address
this problem. Instead of a summation of all successor states, they update each state’s value with
respect to one successor state in each update. Another possibility is to sample a subset of successor
states to compute the look-ahead values at the cost of additional failure probability. Combining these
techniques with DAVI is a potential direction for future work.

Acknowledgments and Disclosure of Funding

We would like to thank Tadashi Kozuno, Csaba Szepesvári and Roshan Shariff for their valuable
comments and feedback.

References

Ahmad, Z., Lelis, L., and Bowling, M. (2020). Marginal utility for planning in continuous or large
discrete action spaces. In Advances in Neural Information Processing Systems, volume 33, pages
1937–1946.

Bensoussan, A. (2011). Dynamic programming and inventory control, volume 3. IOS Press.

Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc, 1st

edition.

Chen, Y. and Wang, M. (2017). Lower bound on the computational complexity of discounted markov

decision problems. arXiv preprint arXiv:1705.07312.

Danihelka, I., Guez, A., Schrittwieser, J., and Silver, D. (2022). Policy improvement by planning

with gumbel. In International Conference on Learning Representations.

Hubert, T., Schrittwieser, J., Antonoglou, I., Barekatain, M., Schmitt, S., and Silver, D. (2021).
Learning and planning in complex action spaces. In International Conference on Machine Learning,
pages 4476–4486. PMLR.

Kearns, M., Mansour, Y., and Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal

planning in large markov decision processes. Machine learning, 49(2):193–208.

Levorato, M., Narang, S., Mitra, U., and Ortega, A. (2012). Reduced dimension policy iteration
for wireless network control via multiscale analysis. In 2012 IEEE Global Communications
Conference (GLOBECOM), pages 3886–3892. IEEE.

Littman, M. L., Dean, T. L., and Kaelbling, L. P. (1995). On the complexity of solving markov decision
problems. In Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence,
UAI’95, page 394–402.

Liu, L., Chattopadhyay, A., and Mitra, U. (2017). On exploiting spectral properties for solving mdp
with large state space. In 2017 55th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), pages 1213–1219. IEEE.

Liu, L., Chattopadhyay, A., and Mitra, U. (2019). On solving mdps with large state space: Exploitation
of policy structures and spectral properties. IEEE Transactions on Communications, 67(6):4151–
4165.

Powell, W. B., Shapiro, J. A., and Simão, H. P. (2002). An adaptive dynamic programming algorithm

for the heterogeneous resource allocation problem. Transportation Science, 36(2):231–249.

Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons, Inc., USA, 1st edition.

Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artiﬁcial

intelligence and machine learning, 4(1):1–103.

10

Szepesvári, C. and Littman, M. L. (1996). Generalized markov decision processes: Dynamic-
programming and reinforcement-learning algorithms. In Proceedings of International Conference
of Machine Learning, volume 96.

Van Seijen, H. and Sutton, R. (2013). Planning by prioritized sweeping with small backups. In

International Conference on Machine Learning, pages 361–369. PMLR.

Williams, R. J. and Baird III, L. C. (1993). Analysis of some incremental variants of policy iteration:
First steps toward understanding actor-critic learning systems. Technical report, NU-CCS-93-11,
Northeastern University, College of Computer Science, Boston, MA.

Zeng, Y., Feng, F., and Yin, W. (2020). AsyncQVI: Asynchronous-parallel Q-value iteration for
discounted markov decision processes with near-optimal sample complexity. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 713–723. PMLR.

Supplementary material

A Auxilary proofs for DAVI’s theoretical results

This section shows the proof of the supporting lemmas required in the proof of DAVI’s convergence
and convergence rate. We also include here a more general proof of the convergence of DAVI and
each of the corollaries. The numbering of each lemma, corollary, and theorem corresponds to the
main paper’s numbering.

Deﬁnition 4 Recall Tn : RS → RS. For a given An ∼ ˜q, πn ∈ Π, sn ∈ S, and for all s ∈ S and
v ∈ RS,

Tnv(s)

.
=

(cid:26)maxa∈An∪πn(s) Lv(s, a)
v(s)

if s = sn
otherwise.

Deﬁne Tπ,sn : RS → RS. For a given π ∈ Π, sn ∈ S, and for all s ∈ S and v ∈ RS,

Tπ,sn v(s)

.
=

(cid:26)Lv(s, π(s))

v(s)

if s = sn
otherwise.

(19)

(20)

Then, the value iterates of DAVI evolves according to vn+1 = Tnvn for all n ∈ N0. Alternatively,
vn+1 = Tπn+1,sn vn with πn+1(s) being the the action that satisﬁes maxa∈An∪πn(s) Lvn (s, a) for
s = sn and with πn+1(s) = πn(s) for all other state s (cid:54)= sn. (i.e., Tπn+1,snvn = Tnvn).

Deﬁnition 5 (Optimality capture region (Williams and Baird III, 1993)) Deﬁne

∆v(s) = min

(cid:20)(cid:26)

max
a(cid:48)∈A

(cid:12)
Lv(s, a(cid:48)) − Lv(s, a)
(cid:12)
(cid:12)a ∈ A

(cid:27)

(cid:21)

− {0}

(21)

as the difference between the look-ahead value with respect to v of the greedy action and a second-
best action for state s. Let ∆v∗ .
(s). Then, the optimality capture region is deﬁned to
be

= mins∈S ∆v∗

(cid:26)

v : (cid:107)v∗ − v(cid:107) <

, v ∈ RS

(cid:27)

.

∆v∗
2γ

(22)

Lemma 4 DAVI operators Tn and Tπ,s(cid:48) are monotone operators. That is given v, u ∈ RS if v ≤ u,
then Tnv ≤ Tnu and Tπ,s(cid:48)v ≤ Tπ,s(cid:48)u.

Proof: Given any v, u ∈ RS s.t. v ≤ u, then

Tnv(s) =

≤

(cid:26)maxa∈An∪πn(s) r(s, a) + γ (cid:80)
v(s)
(cid:26)maxa∈An∪πn(s) r(s, a) + γ (cid:80)
u(s)

s(cid:48) p(s(cid:48)|s, a)v(s(cid:48))

s(cid:48) p(s(cid:48)|s, a)u(s(cid:48))

= Tnu(s)

11

if s = sn
otherwise

if s = sn
otherwise

(23)

(24)

(25)

Given any v, u ∈ RS s.t. v ≤ u, then

Tπ,sn v(s)

.
=

(cid:26)r(s, π(s)) + γ (cid:80)

s(cid:48) p(s(cid:48)|s, π(s))v(s(cid:48))

v(s)

(cid:26)r(s, π(s)) + γ (cid:80)

s(cid:48) p(s(cid:48)|s, π(s))u(s(cid:48))

≤

u(s)
= Tπ,sn u(s)

for s = sn
otherwise

for s = sn
otherwise

(26)

(27)

(28)

(cid:4)

Lemma 1 (Monotonicity) The iterates of DAVI, (vn)n≥0 is a monotonically increasing sequence:
vn ≤ vn+1 for all n ∈ N0, if r(s, a) ∈ [0, 1] for any s, a ∈ S × A and if DAVI is initialized according
to (i),(ii), or (iii) of Assumption 1.

Proof: We show (vn)n≥0 is a monotonically increasing sequence by induction. All inequalities
between vectors henceforth are element-wise. Let (s0, s1, ..., sn, sn+1) be the sequence of states
sampled for update from iteration 1 to n + 1. By straight-forward calculation, we show v1 ≥ v0. For
all rewards in [0, 1] and for any s ∈ S,

case i : v1(s) = max

a∈A0∪π0(s)

(cid:40)

r(s, a) + γ

(cid:41)

(cid:88)

p(s(cid:48)|s, a)0

≥ r(s, π0(s)) + γ

s(cid:48)
p(s(cid:48)|s, π0(s))0

(cid:88)

s(cid:48)

= Lv0(s, π0(s)) ≥ 0 = v0(s)
(cid:40)

case ii : v1(s) = max

a∈A0∪π0(s)

r(s, a) + γ

(cid:41)

p(s(cid:48)|s, a)(−c)

(cid:88)

s(cid:48)

≥ r(s, π0(s)) + γ

(cid:88)

s(cid:48)

p(s(cid:48)|s, π0(s)))(−c) = Lv0(s, π0(s))

= −γc + r(s, π0(s)) ≥ −c = v0(s)

case iii : v1(s) = max

a∈A0∪π0(s)

r(s, a) + γ

(cid:40)

(cid:41)

p(s(cid:48)|s, a)v0(s(cid:48))

(cid:88)

s(cid:48)

≥ r(s, π0(s)) + γ

(cid:88)

s(cid:48)

p(s(cid:48)|s, π0(s))v0(s(cid:48)) = Lv0 (s, π0(s))

≥ v0(s) by assumption

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

Thus, v1(s0) ≥ v1(s0). For all other states s (cid:54)= s0, v0(s) = v1(s). Therefore, v1 ≥ v0. Now, assume
vn ≥ · · · ≥ v0 with n ≥ 1, then for any s ∈ S,

vn+1(s) = Tnvn(s)

=

≥

≥

(cid:26)maxa∈An∪πn(s) Lvn (s, a)
vn(s)

if s = sn
otherwise

(cid:26)Lvn (s, πn(s))

vn(s)

(cid:26)Lvn−1 (s, πn(s))

vn−1(s)

if s = sn
otherwise

if s = sn
otherwise

by assumption vn ≥ vn−1

(38)

(39)

(40)

(41)

If sn = sn−1, then (41) is Tπn,sn−1vn−1. By Deﬁnition 4, Tπn,sn−1 vn−1 = vn. Hence, vn+1 ≥ vn.
However, if sn (cid:54)= sn−1, we have to do more work. There are two possible cases. The ﬁrst case is that
sn has been sampled for update before. That is, let 1 < j ≤ n s.t. sn−j is the last time that sn is
sampled for update. Then sn = sn−j, and vn(sn) = vn−j+1(sn) and πn(sn) = πn−j+1(sn). By

12

assumption, vn ≥ ... ≥ vn−j ≥ ... ≥ v0, then

vn+1(sn) =

max
a∈An∪πn(sn)

Lvn(sn, a) ≥ Lvn (sn, πn(sn))

≥ Lvn−j (sn, πn(sn)) by assumption vn ≥ vn−j
= Lvn−j (sn, πn−j+1(sn))
= Tπn−j+1,sn−j vn−j(sn) by (20)
= vn−j+1(sn) by Deﬁnition 4
= vn(sn).

(42)

(43)
(44)
(45)

(46)
(47)

We have just showed that vn+1(sn) ≥ vn(sn), and for all other state s (cid:54)= sn, vn+1(s) = vn(s).
For the second case, sn has not been sampled for updated before n, then vn(sn) = v0(sn) and
πn(sn) = π0(sn). By assumption, vn ≥ ... ≥ v0, then

vn+1(sn) =

Lvn(sn, a) ≥ Lvn (sn, πn(sn))

max
a∈An∪πn(sn)
≥ Lv0 (sn, πn(sn))
= Lv0 (sn, π0(sn)) ≥ v0(sn)
= vn(sn).

shown in base case

For all other state s (cid:54)= sn, vn+1(s) = vn(s). Altogether, vn+1 ≥ vn for all n ∈ N0.

(48)

(49)
(50)
(51)

(cid:4)

Corollary 1 (Computational complexity of obtaining an (cid:15)-optimal policy) Fix an (cid:15) ∈ (0, (cid:107)v∗ −
v0(cid:107)), and assume DAVI initializes according to (i), (ii), or (iii) of Assumption 1. Deﬁne

Hγ,(cid:15)

.
= ln

(cid:18) (cid:107)v∗ − v0(cid:107)
(cid:15)

(cid:19)

/1 − γ

as a horizon term. Then, DAVI run for at least

(cid:18)

τ = Hγ,(cid:15)

ln

(cid:19)

(cid:18) SHγ,(cid:15)
δ

(cid:18)

/ ln

1
1 − qmin

(cid:19)(cid:19)

(52)

(53)

iterations, returns an (cid:15)-optimal policy πn : vπn ≥ v∗ − (cid:15)1 with probability at least 1 − δ using
O (mSτ ) elementary arithmetic and logical operations. Note that (cid:107)v∗ − v0(cid:107) is unknown but it can
be upper bounded by

1
1−γ + (cid:107)v0(cid:107) given rewards are in [0, 1].

Proof: Recall from Lemma 1, DAVI’s value iterates, vn → v∗ monotonically from below (i.e.,
vn ≥ vn−1 ≥ · · · ≥ v0 ). Using this result, one can show Lvn (s, πn(s)) ≥ vn(s) for all s ∈ S
and n ∈ N0 following an induction process. We have already shown in the proof Lemma 1 that
Lv0 (s, π0(s)) ≥ v0(s) for any s ∈ S in the base case. Assume that Lvn (s, πn(s)) ≥ vn(s) for
any s ∈ S, we will show that Lvn+1(s, πn+1(s)) ≥ vn+1(s). For any n ∈ N0 and sn ∈ S, let
πn+1(sn) = arg maxa∈An∪πn(sn) Lvn (sn, a) with πn+1(¯s) = πn(¯s) for all other ¯s (cid:54)= sn.
For the case when s = sn,

vn+1(s) = Tπn+1,sn vn(s) = Lvn (s, πn+1(s))

≤ Lvn+1(s, πn+1(s)) by vn ≤ vn+1

For the case when s (cid:54)= sn, then vn+1(s) = vn(s) and πn+1(s) = πn(s) and thus

vn+1(s) = Tπn+1,sn vn(s) = vn(s) ≤ Lvn(s, πn(s)) by assumption

= Lvn+1(s, πn+1(s)).

(54)
(55)

(56)
(57)

Altogether, we get Lvn+1 (s, πn+1(s)) ≥ vn+1(s) for any s ∈ S, which concludes the induction.
Now, we show that vπn ≥ vn for any n ∈ N0 using the result Lvn (s, πn(s)) ≥ vn(s) for any s ∈ S
and n ∈ N0. Fix n and if we are to apply the policy evaluation operator Tπn that satisfy Lemma 3(1)
to every state s ∈ S, then we obtain

Tπn vn(s) = Lvn(s, πn(s)) ≥ vn(s).

(58)

13

Therefore, Tπnvn ≥ vn. By applying the Tπn operator to Tπn vn ≥ vn repeatedly, and using the
monotonicity of Tπn , we have for any k ≥ 0,

T k
vn ≥ T k−1
πn
By taking limits of both sides of T k
vn ≥ vn as k → ∞, we get vπn ≥ vn. Therefore,
πn

vn ≥ · · · ≥ vn.

πn

0 ≤ v∗ − vπn ≤ v∗ − vn =⇒ (cid:107)v∗ − vπn (cid:107) ≤ (cid:107)v∗ − vn(cid:107).

(59)

(60)

Next, recall from the proof of Theorem 2 that for a given l ∈ N, and with probability 1 − δ,
vn of DAVI would have γ-contracted at least l times: (cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107), with n ≥
(cid:108)
ln (cid:0) Sl
. Following from (60), with probability 1 − δ,
l

(cid:1) (cid:46)

(cid:17)(cid:109)

ln

(cid:16)

δ

1
1−qmin

(61)

(62)

(cid:107)v∗ − vπn (cid:107) ≤ (cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107).

By setting γl(cid:107)v∗ − v0(cid:107) = (cid:15) and solve for l, we get:
(cid:107)v∗ − v0(cid:107)
(cid:15)
(cid:16) (cid:107)v∗−v0(cid:107)
(cid:15)

(cid:16) (cid:107)v∗−v0(cid:107)
(cid:15)

l = ln

(cid:16) 1
γ

≤ ln

/ ln

(cid:17)

(cid:17)

/ ln

(cid:17)

(cid:19)

.

(cid:18) 1
γ

.
We observe that ln
= Hγ,(cid:15). To compute vn, DAVI
takes O(mS) elementary arithmetic operations. With probability 1 − δ, DAVI obtains an (cid:15)-optimal
policy with

/(1 − γ)

(cid:18)

O(mSn) = O

mSHγ,(cid:15) ln

(cid:19)

(cid:18) SHγ,(cid:15)
δ

(cid:18)

/ ln

1
1 − qmin

(cid:19)(cid:19)

arithmetic and logical operations.

(63)

(cid:4)

Corollary 2 (Computational complexity of obtaining an optimal policy) Assume DAVI initial-
izes according to (i), (ii), or (iii) of Assumption 1. Deﬁne the horizon term

Hγ,∆v∗

.
= ln

(cid:18) (cid:107)v∗ − v0(cid:107)
∆v∗

(cid:19)

/(1 − γ),

(64)

where ∆v∗
policy π∗ ∈ Π∗ with probability 1 − δ, requiring

is the optimality capture region deﬁned in Deﬁnition 5. Then, DAVI returns an optimal

(cid:18)

(cid:18) SHγ,∆v∗
δ
elementary arithmetic operations. Note that (cid:107)v∗ − v0(cid:107) is unknown but it can be upper bounded by
1
1−γ + (cid:107)v0(cid:107) given rewards are in [0, 1].

1
1 − qmin

mSHγ,∆v∗ ln

(65)

/ ln

(cid:19)(cid:19)

O

(cid:19)

(cid:18)

Proof: We ﬁrst show that any πn such that vπn > v∗ − ∆v∗
contradiction. Assume πn is not optimal but satisﬁes vπn > v∗ − ∆v∗

1 is an optimal policy. We prove this by
1, then for any s ∈ S

Lv∗

(s, πn(s)) < Lv∗

(s, π∗(s))

(s, π∗(s)) − Lv∗
(s, πn(s)) > 0
(s, π∗(s)) − Lv∗
(s, πn(s)) ≥ ∆v∗
(s, π∗(s)) − Lvπn (s, πn(s)) ≥ ∆v∗

=⇒ Lv∗
=⇒ Lv∗
=⇒ Lv∗
=⇒ v∗(s) − vπn(s) ≥ ∆v∗
=⇒ vπn (s) ≤ v∗(s) − ∆v∗

.

by Deﬁnition 5

(66)

(67)

(68)

(69)

(70)

(71)
This contradicts the assumption and thus πn must be optimal. It is straight-forward to show that the
result of Corollary 1 still holds if we require πn : vπn > v∗ − (cid:15)1 instead of πn : vπn ≥ v∗ − (cid:15)1. We
can then apply this result to show that DAVI returns policy πn such that πn : vπn > v∗ − ∆v∗
1, and
thus an optimal policy, with probability 1 − δ within
(cid:18) SHγ,∆v∗
δ

1
1 − qmin

mSHγ,∆v∗

(cid:19)(cid:19)(cid:19)

(72)

/ ln

ln

O

(cid:18)

(cid:18)

(cid:18)

(cid:19)

arithmetic and logical operations.

(cid:4)

Now we show an alternative proof to the convergence of DAVI with any initialization. Before we
prove the main result, we deﬁne the following supporting lemmas.

14

Lemma 5 (Williams and Baird III (1993)) Let v, u
arg maxa∈A Lv(s, a) and an a ∈ A satisﬁes Lu(s, a) ≥ Lu(s, π(s)). Then

∈

∈

RS, s

implies that Lv(s, π(s)) = Lv(s, a).

(cid:107)v − u(cid:107) <

∆v
2γ

S.

Let π(s) =

(73)

Lemma 6 Given v ∈ RS which satisﬁes (cid:107)v∗ − v(cid:107) < ∆v∗
region), if an action a satisﬁes Lv(s, a) = maxa(cid:48)∈A Lv(s, a(cid:48)), then a is an optimal action at s.

2γ (i.e., v is inside the optimality capture

Proof: For any s ∈ S, let the optimal policy at s be π∗(s) = arg maxa∈A Lv∗(s, a) and π(s) =
arg maxa∈A Lv(s, a), then

Since (cid:107)v∗ − v(cid:107) < ∆v∗

2γ and by Lemma 5, Lv∗

(s, π∗(s)) = Lv∗

(s, π(s)).

Lv(s, π(s)) ≥ Lv(s, π∗(s)).

(74)

(cid:4)

Lemma 7 (Stochastically always (Williams and Baird III, 1993)) Let X be a set of ﬁnite opera-
tors on AS × RS. We say a stochastic process is stochastic always if every operator in X has a
non-zero probability of being drawn. Let Σ be an inﬁnite sequence operator from X generated by a
stochastic always stochastic process. Let Σ(cid:48) be a given ﬁnite sequence of operators from X, then

1. Σ(cid:48) appears as a contiguous subsequence of Σ with probability 1, and

2. Σ(cid:48) appears inﬁnitely often as a contiguous subsequence of Σ with probability 1.

Theorem 3 (Convergence of DAVI with any initialisation) Let ˜A be some arbitrary action subset
of A, and let X = {I ˜A,s, Ts|s ∈ S} be a set of DAVI operators that operate on AS × RS that is the
joint space of policy and value function, where

πn+1(s) = I ˜A,sn

πn(s) =

(cid:40)

arg maxa∈ ˜A∪πn(s) Lvn (s, a)
πn(s)

if s = sn
otherwise,

(75)

and

vn+1(s) = Tsn vn(s) =

(cid:26)Lvn (s, πn+1(s))

if s = sn
otherwise.

vn(s)
Recall Π is a set of deterministic policies deﬁned in Section 2 and π∗ ∈ Π. Without loss of generality,
we write S = 1, ..., S. If DAVI performs the following sequence of operations in some ﬁxed order,
I ˜A1,1T1I ˜A2,2T2...I ˜AS ,STS
(77)
where ˜Ai contains the optimal action π∗(i) for state i, then vn would have γ-contracted at least
once by the same argument as in the proof of Theorem 2. Let Σ(cid:48) be a concatenation of l copies of a
sequence (77). Then, after having performed all the operations in Σ(cid:48), vn would have γ-contracted l
times. If l satisﬁes:

(76)

γl(cid:107)v∗ − vn(cid:107) <

∆v∗
2γ

,

(78)

then vn is inside the optimality capture region deﬁned in Deﬁnition 5. Once inside the optimality
capture region, by Lemma 6, all policies πn are optimal thereafer. We know from Lemma 3 (1),
limn→∞ Tπ∗ v = v∗ and by Lemma 2 (Boundedness), all vn’s are bounded. Then, the convergence
of DAVI with any initialization is ensured as long as all of the states are sampled for update inﬁnitely
often.
The only question is whether if Σ(cid:48) would ever exist in an inﬁnite sequence Σ that is generated by
running DAVI forever. To show that such event happens with probability 1, we apply Lemma 7. To
apply Lemma 7 (Stochastically always), X must be ﬁnite, which indeed it is since the state and action
space are ﬁnite. Ensuring that the ˜q(s, a) > 0 guarantees every operator in X is drawn with a
non-zero probability. Therefore, the stochastic process generated by running DAVI would satisfy
all the properties of Lemma 7. By Lemma 7, running DAVI forever will generate any contiguous
subsequence Σ(cid:48) inﬁnitely often with probability 1.

15

B Theoretical analysis of Asynchronous VI

Bertsekas and Tsitsiklis (1996) and Williams and Baird III (1993) have shown Asynchronous VI
converges. We can view Asynchronous VI as a special case of DAVI if the subset of actions
sampled in each iteration is the entire action space. That is for any s ∈ S, v ∈ RS and π ∈ Π,
maxa∈A∪π(s) Lv(s, a) = maxa∈A Lv(s, a). We can follow similar reasoning to the proof of the
convergence rate of DAVI (Theorem 2 )and show the convergence rate of Asynchronous VI with the T
operator deﬁned in Deﬁnition 6. However, the sequence of increasing indices (nk)∞
k=0, where n0 = 0
in Theorem 2 takes on a slightly different meaning. In particular, between the nk-th and nk+1-th
iteration, all s ∈ S have been updated at least once. Finally, the computational complexity bound of
Asynchronous VI is similar to the computational complexity bound of DAVI with pmin = mins p(s)
instead of qmin. The computational complexity result is proven similarly to the proof of Corollary 1
found in Appendix A.

Deﬁnition 6 (Asynchronous VI operator) Recall Tsn : RS → RS. For a given sn ∈ S, and for all
s ∈ S and v ∈ RS,

Tsn v(s)

.
=

(cid:26)maxa∈A Lv(s, a)

v(s)

if s = sn
otherwise.

(79)

Then the iterates of Asynchronous VI evolves according to vn+1 = Tsnvn for all n ∈ N0.

Lemma 8 (Asynchronous VI Monotonicity) The iterates of Asynchronous VI, (vn)n≥0 is a mono-
tonically increasing sequence: vn ≤ vn+1 for all n ∈ N0, if r(s, a) ∈ [0, 1] for any s, a ∈ S × A
and if Asynchronous VI is initialized according to (i) or (ii) of Assumption 1

Proof: We show (vn)n≥0 is a monotonically increasing sequence by induction. All inequalities
between vectors henceforth are element-wise. Let (s0, s1, ..., sn, sn+1) be the sequence of states
sampled for update from iteration 1 to n + 1. By straight-forward calculation, we show v1 ≥ v0. For
all rewards in [0, 1] and any s ∈ S,

(cid:40)

case i : v1(s) = max
a∈A

r(s, a) + γ

(cid:40)

case ii : v1(s) = max
a∈A

r(s, a) + γ

(cid:41)

p(s(cid:48)|s, a)0

≥ v0(s)

(cid:41)

p(s(cid:48)|s, a)(−c)

(cid:88)

s(cid:48)

(cid:88)

s(cid:48)

= max
a∈A

{−γc + r(s, a)} ≥ v0(s)

(80)

(81)

(82)

Thus, v1(s0) ≥ v0(s0). For all other states s (cid:54)= s0, v0(s) = v1(s). Therefore, v1 ≥ v0. Now, assume
vn ≥ · · · ≥ v0 with n ≥ 1, then for any s ∈ S,

vn+1(s) = Tsn vn(s)

=

≥

(cid:26)maxa∈A Lvn(s, a)

vn(s)

(cid:26)maxa∈A Lvn−1(s, a)

vn−1(s)

if s = sn
otherwise

if s = sn
otherwise

by assumption vn ≥ vn−1

(83)

(84)

(85)

If sn = sn−1, then (85) is Tsn−1 vn−1. By Deﬁnition 6, Tsn−1vn−1 = vn. Hence, vn+1 ≥ vn.
However, if sn (cid:54)= sn−1, we have to do more work. There are two possible cases. The ﬁrst case is that
sn has been sampled before. That is, let 1 < j ≤ n s.t. sn−j is the last time that sn is sampled for
update. Then sn = sn−j, and vn(sn) = vn−j+1(sn). By assumption, vn ≥ ... ≥ vn−j ≥ ... ≥ v0,
then

vn+1(sn) = max
a∈A
≥ max
a∈A

Lvn (sn, a)

Lvn−j (sn−j, a) by assumption vn ≥ vn−j

= Tsn−j vn−j(sn−j) = vn−j+1(sn−j) = vn(sn).

(86)

(87)

(88)

16

We have just showed that vn+1(sn) ≥ vn(sn), and for all other state s (cid:54)= sn, vn+1(s) = vn(s).
For the second case, sn has not been sampled before n, then vn(sn) = v0(sn). By assumption,
vn ≥ ... ≥ v0, then

Lvn(sn, a)

vn+1(sn) = max
a∈A
≥ max
a∈A
≥ v0(sn)

Lv0(sn, a)

by assumption vn ≥ v0

shown in base case.

For all other state s (cid:54)= sn, vn+1(s) = vn(s). Altogether, vn+1 ≥ vn for all n ∈ N0.

(89)

(90)

(91)

(cid:4)

Theorem 4 (Convergence rate of Asynchronous VI) Assume p(s) > 0 and r(s, a) ∈ [0, 1] for
any s, a ∈ S × A, and also assume Asynchronous VI initialises according to (i), (ii) of Assumption 1.
With γ ∈ [0, 1) and probability 1 − δ, the iterates of Asynchronous VI, (vn)n≥0 converges to v∗ at a
near-geometric rate. In particular, with probability 1 − δ, for a given l ∈ N,

for any n satisfying

where pmin = mins p(s).

(cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107)

(cid:24)

n ≥ l

ln

(cid:19) (cid:46)

(cid:18)

ln

(cid:18) Sl
δ

1
1 − pmin

(cid:19)(cid:25)

,

(92)

(93)

Proof: Recall from Lemma 8, we have shown the iterates of Asynchronous VI, vn → v∗ monotoni-
cally from below. We deﬁne (nk)∞
k=0 to be a sequence of increasing indices, where n0 = 0, such
that between the nk-th and nk+1-th iteration, all state s ∈ S have been updated at least once. At
the nk+1-th iteration, vnk+1 ≥ Tπ∗ vnk . This implies that at the nk+1-th iteration, Asynchronous VI
would have γ-contracted at least once:

0 ≤ v∗ − vnk+1 ≤ v∗ − Tπ∗ vnk , =⇒ (cid:107)v∗ − vnk+1 (cid:107) ≤ (cid:107)v∗ − Tπ∗ vnk (cid:107),
(cid:107)v∗ − Tπ∗ vnk (cid:107) = (cid:107)Tπ∗ v∗ − Tπ∗ vnk )(cid:107) ≤ γ(cid:107)v∗ − vnk (cid:107)

=⇒ (cid:107)v∗ − vnk+1(cid:107) ≤ γ(cid:107)v∗ − vnk (cid:107).

The probability of the failure event

P(E c) ≤

l
(cid:88)

(cid:88)

P(E c
i )

i=1

s∈S
≤ Sl(1 − pmin)N ,

(94)

(95)
(96)

(97)

(98)

with pmin = mins∈S p(s) instead of qmin. The rest follows similar reasoning to the proof of
(cid:4)
Theorem 2 and obtain the result.

Corollary 3 (Computational complexity of Asynchronous VI) Fix an (cid:15) ∈ (0, (cid:107)v∗ − v0(cid:107)), and
assume Asynchronous VI initialises according to (i) or (ii) of Assumption 1. Deﬁne

Hγ,(cid:15)

.
= ln

(cid:18) (cid:107)v∗ − v0(cid:107)
(cid:15)

(cid:19)

/1 − γ

(99)

as a horizon term. Then, Asynchronous VI returns an (cid:15)-optimal policy πn : vπn ≥ v∗ − (cid:15)1 with
probability at least 1 − δ using

(cid:18)

(cid:18)

O

ASHγ,(cid:15)

ln

(cid:19)

(cid:18) SHγ,(cid:15)
δ

(cid:18)

/ ln

1
1 − pmin

(cid:19)(cid:19)(cid:19)

(100)

elementary arithmetic and logical operations. Note that (cid:107)v∗ − v0(cid:107) is unknown but it can be upper
bounded by

1
1−γ + (cid:107)v0(cid:107) given rewards are in [0, 1].

Proof: Recall from Lemma 8, the iterates of Asynchronous VI, vn → v∗ monotonically from below
(i.e., vn ≥ vn−1 ≥ · · · ≥ v0 ). For any n ∈ N0 and sn ∈ S, let πn+1(sn) = arg maxa∈A Lvn (sn, a)

17

with πn+1(¯s) = πn(¯s) for all other ¯s (cid:54)= sn. One can show Lvn (s, πn(s)) ≥ vn(s) for any s ∈ S and
n ∈ N0 following similar argument as in the proof of Corollary 1. Now, we show vπn ≥ vn for any
n ∈ N0. Fix n and if we are to apply the policy evaluation operator Tπn that satisfy Lemma 3(1) to
every state s ∈ S, then we obtain

Tπn vn(s) = Lvn(s, πn(s)) ≥ vn(s).

(101)

Therefore, Tπnvn ≥ vn. By applying the Tπn operator to Tπn vn ≥ vn repeatedly, and using the
monotonicity of Tπn , we have for any k ≥ 0,

T k
πn

vn ≥ T k−1

πn

vn ≥ · · · ≥ vn.

By taking limits of both sides of T k
πn

vn ≥ vn as k → ∞, we get vπn ≥ vn. Therefore,

0 ≤ v∗ − vπn ≤ v∗ − vn =⇒ (cid:107)v∗ − vπn (cid:107) ≤ (cid:107)v∗ − vn(cid:107).

(102)

(103)

Next, recall from the proof of Theorem 4 that for a given l ∈ N, and with probability 1 − δ, vn
of Asynchronous VI would have γ-contracted at least l times: (cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107), with
n ≥ l

. Following from (103), with probability 1 − δ,

(cid:108)
ln (cid:0) Sl

(cid:1) (cid:46)

(cid:17)(cid:109)

ln

(cid:16)

δ

1
1−pmin

(cid:107)v∗ − vπn (cid:107) ≤ (cid:107)v∗ − vn(cid:107) ≤ γl(cid:107)v∗ − v0(cid:107).

By setting γl(cid:107)v∗ − v0(cid:107) = (cid:15) and solve for l, we get:

l = ln

(cid:107)v∗ − v0(cid:107)
(cid:15)

/ ln

(cid:19)

.

(cid:18) 1
γ

(104)

(105)

.
= Hγ,(cid:15). To compute vn, Asyn-
We observe that ln
chronous VI takes O(AS) elementary arithmetic operations. With probability 1 − δ, Asynchronous
VI obtains an (cid:15)-optimal policy within

/(1 − γ)

(cid:16) (cid:107)v∗−v0(cid:107)
(cid:15)

(cid:16) (cid:107)v∗−v0(cid:107)
(cid:15)

(cid:16) 1
γ

≤ ln

/ ln

(cid:17)

(cid:17)

(cid:17)

(cid:18)

(cid:18)

O(ASn) = O

ASHγ,(cid:15)

ln

(cid:19)

(cid:18) SHγ,(cid:15)
δ

(cid:18)

/ ln

1
1 − pmin

(cid:19)(cid:19)(cid:19)

arithmetic and logical operations.

C More experiments

(106)

(cid:4)

In this section, we show additional experiments with the MDPs described in Section 6 with rewards
generated via a standard normal and a Pareto distribution.

Recall that the experiments were set up to see how DAVI’s performance is affected by the sparsity
of rewards. Pareto distribution with a shape of 2.5 is a “heavy-tail" distribution, and the rewards
sampled from this distribution could result in a few large values. On the other hand, the rewards
sampled via the standard Normal distribution could result in many similar values. We hypothesize that
DAVI would converge faster than Asynchronous VI in domains with multiple optimal or near-optimal
policies, which could be the case in the normal-distributed reward setting.

The algorithms that will be compared in the experiments are VI, Asynchronous VI and DAVI. We
implement Asynchronous VI and DAVI using uniform sampling to obtain the states. DAVI samples a
new set of actions via uniform sampling without replacement in each iteration.

C.1 Single-state experiment

This experiment consists of a single-state MDP with 10000 actions, and all terminate immediately. We
experiment with two reward distributions: Pareto-reward and Normal-reward. For Pareto-reward, all
actions have rewards generated according to a Pareto distribution with shape 2.5. For Normal-reward,
all actions have rewards generated according to the standard normal distribution.

18

C.2 Multi-reward experiment

This experiment consists of two MDPs. The ﬁrst set consists of a tree with a depth of 2. Each state
has 50 actions, where each action leads to 2 other distinct next states. All actions terminate at the leaf
states. In one setting, the rewards are distributed according to the Pareto distribution with a shape of
2.5. In the other setting, the rewards are distributed according to the normal distribution.

The second set of MDPs consists of a random MDP with 100 states, and each state has 1000 actions.
Each action leads to 10 next states randomly selected from the 100 states with equal probability. All
transitions have a 0.1 probability of terminating. In one setting, the rewards are distributed according
to the Pareto distribution with a shape of 2.5. In the other setting, the rewards are distributed according
to the standard normal distribution. The γ in all of the MDPs are 1.

C.3 Discussion

Figure 3 and Figure 4 show the performance of the algorithms. All graphs included error bars showing
the standard error of the mean. All graphs smoothly increased towards the asymptote except for
Asynchronous VI in Figure 3 and VI in Figure 4, whose performances were step-functions 2. The
y-axis of each graph showed a state value averaged over 200 runs. The x-axes showed run-times,
which have been adjusted for computations.

In Figure 3, DAVI with m = 1 was signiﬁcantly different from that of DAVI with m = 10, 100, 1000.
However, in the Normal-reward setting, the performance of DAVI with m = 1 was much closer to
the performance of DAVI with m = 10, 100, 1000. In the Pareto-reward setting, where there could
only be a few large rewards, the results were similar to that of the needle-in-the-haystack setting of
Figure 1. In the Normal-reward setting, where most of the rewards were similar and concentrated
around 0, the results were similar to that of the multi-reward setting of Figure 1.

In Figure 4 in both tree Pareto-reward and Normal-reward settings (top row), DAVI with m = 1 was
signiﬁcantly different from that of DAVI m = 10. In the tree setting, with normal-distributed rewards,
where there may be multiple actions with similarly large rewards, DAVI m = 10 converged faster
than VI and Asynchronous VI.

In the random-MDP setting, DAVI, for all values of m, converged faster than VI and Asynchronous
VI in both the Pareto-reward and Normal-reward settings, as evident in the bottom row of Figure 4. As
expected, DAVI converged faster than Asynchronous VI and VI in the case of multiple near-optimal
policies. Note, DAVI m = 100 was the slowest to converge, a case where the action subset size is
large. This result makes sense as Asynchronous VI with the full action space did not converge as fast
as DAVI with smaller action subsets.

Figure 3: Single-state experiment with 10000 actions: (a) rewards are Pareto distributed with shape
2.5 (b) rewards are standard normal distributed. The Asynchronous VI in this experiment is equivalent
to VI since there is only one state. We run each instance 200 times with a new MDP generated each
time. In each run, each algorithm is initialized to 0.

2Asynchronous VI is equivalent to VI in the single-state experiment since there is only one state.

19

state value averaged over 200 runs10,0000100,000run-time (# of operations x iterations)600Pareto4010,000050,000run-time (# of operations x iterations)NormalAsynchronous VIDAVI m=1DAVI m =10DAVI m=100DAVI m=1000Figure 4: Multi-state top row: MDP with a tree structure with Pareto and normal distributed rewards.
Multi-state bottom row: random MDP with Pareto and normal distributed rewards. We run each
instance 200 times with a new MDP generated each time. In each run, all algorithms are initialized to
0.

20

TreeRandom MDPNormalPareto00502.50015run-time (# of operations x iterations)in 1 billions50run-time (# of operations x iterations)in 1 billions155090007500750100run-time (# of operations x iterations)in 10 millionsrun-time (# of operations x iterations)in 10 millionsVIAsynchronous VIDAVI m=1DAVI m =10VIAsynchronous VIDAVI m =1DAVI m =10DAVI m =100root state value averaged over 200 runsstate 0 value averaged over 200 runs