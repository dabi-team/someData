2
2
0
2

n
a
J

7
1

]

C
O
.
h
t
a
m

[

2
v
5
3
1
0
1
.
8
0
0
2
:
v
i
X
r
a

LEARNING DYNAMICAL SYSTEMS WITH SIDE INFORMATION†∗

AMIR ALI AHMADI AND BACHIR EL KHADIR
{AAA, BKHADIR}@PRINCETON.EDU

Abstract. We present a mathematical and computational framework for the problem of learning a dynami-
cal system from noisy observations of a few trajectories and subject to side information. Side information is any
knowledge we might have about the dynamical system we would like to learn besides trajectory data. It is typically
inferred from domain-speciﬁc knowledge or basic principles of a scientiﬁc discipline. We are interested in explicitly
integrating side information into the learning process in order to compensate for scarcity of trajectory observations.
We identify six types of side information that arise naturally in many applications and lead to convex constraints in
the learning problem. First, we show that when our model for the unknown dynamical system is parameterized as
a polynomial, one can impose our side information constraints computationally via semideﬁnite programming. We
then demonstrate the added value of side information for learning the dynamics of basic models in physics and cell
biology, as well as for learning and controlling the dynamics of a model in epidemiology. Finally, we study how well
polynomial dynamical systems can approximate continuously-differentiable ones while satisfying side information
(either exactly or approximately). Our overall learning methodology combines ideas from convex optimization, real
algebra, dynamical systems, and functional approximation theory, and can potentially lead to new synergies between
these areas.

Key words. Learning, Dynamical Systems, Sum of Squares Optimization, Convex Optimization

1. Motivation and problem formulation. In several safety-critical applications, one
has to learn the behavior of an unknown dynamical system from noisy observations of a very
limited number of trajectories. For example, to autonomously land an airplane that has just
gone through engine failure, limited time is available to learn the modiﬁed dynamics of the
plane before appropriate control action can be taken. Similarly, when a new infectious disease
breaks out, few observations are initially available to understand the dynamics of contagion.
In situations of this type where data is limited, it is essential to exploit “side information”—
e.g. physical laws or contextual knowledge—to assist the task of learning.

More formally, our interest in this paper is to learn a continuous-time dynamical system

of the form

˙x(t) = f (x(t)),
(1.1)
over a given compact set Ω ⊂ Rn from noisy observations of a limited number of its trajecto-
ries. Here, ˙x(t) denotes the time derivative of the state x(t) ∈ Rn at time t. We assume that
the unknown vector ﬁeld f that is to be learned is continuously differentiable over an open set
containing Ω, an assumption that is often met in applications. In our setting, we have access
to a training set of the form

D := {(xi, yi),

(1.2)
where xi ∈ Ω (resp. yi ∈ Rn) is a possibly noisy measurement of the state of the dynamical
system (resp. of f (xi)). Typically, this training set is obtained from observation of a few
trajectories of (1.1). The vectors yi could be either directly accessible (e.g., from sensor
measurements) or approximated from the state variables using a ﬁnite-difference scheme.

i = 1, . . . , N },

Finding a vector ﬁeld fF that best agrees with the training set D among a particular class

∗ The authors are with the department of Operations Research and Financial Engineering at Princeton University.
This work was partially supported by the MURI award of the AFOSR, the DARPA Young Faculty Award, the
CAREER Award of the NSF, the Google Faculty Award, the Innovation Award of the School of Engineering and
Applied Sciences at Princeton University, and the Sloan Fellowship.

† An 8-page version of this paper [2] has appeared in the proceedings of the conference on Learning for Dynamics

& Control (L4DC), 2020.

1

 
 
 
 
 
 
F of continuously-differentiable functions amounts to solving the optimization problem

(1.3)

fF ∈ arg min

p∈F

(cid:88)

(cid:96)(p(xi), yi),

(xi,yi)∈D
where (cid:96)(·, ·) is some loss function that penalizes deviation of p(xi) from yi. For instance,
(cid:96)(·, ·) could simply be the (cid:96)2 loss function

(cid:96)2(u, v) := (cid:107)u − v(cid:107)2 ∀u, v ∈ Rn,
though the computational machinery that we propose can readily handle various other convex
loss functions (see Section 3).

In addition to ﬁtting the training set D, we desire for our learned vector ﬁeld fF to
generalize well, i.e., to be consistent as much as possible with the behavior of the unknown
vector ﬁeld f on all of Ω. Indeed, the optimization problem in (1.3) only dictates how the
candidate vector ﬁeld should behave on the training data. This could easily lead to overﬁtting,
especially if the function class F is large and the observations are limited. Let us demonstrate
this phenomenon by a simple example.

EXAMPLE 1. Consider the two-dimensional vector ﬁeld
f (x1, x2) := (−x2, x1)T .
(1.4)
The trajectories of the system ˙x(t) = f (x(t)) from any initial condition are given by circular
orbits. In particular, if started from the initial condition xinit = (1, 0)T , the trajectory is given
by x(t, xinit) = (cos(t), sin(t))T . Hence, for any function g : R2 → R2, the vector ﬁeld
(1.5)
agrees with f on the sample trajectory x(t, xinit). However, the behavior of the trajectories of
h depends on the arbitrary choice of the function g. If g(x) = x for instance, the trajectories
of h starting outside of the unit disk diverge to inﬁnity. See Figure 1 for an illustration.

h(x) := f (x) + (x2

2 − 1)g(x)

1 + x2

Fig. 1: Streamplot of the vector ﬁeld f in (1.4) (in
blue), together with two sample trajectories of the
vector ﬁeld h in (1.5) with g(x) = x when started
from (1, 0)T (drawn in black) and from (1.01, 0)T
(drawn in red). The trajectories of f and h match ex-
actly when started from (1, 0)T , but get arbitrarily far
from each other when started from (1.01, 0)T .

To address the issue of insufﬁciency of data and to avoid overﬁtting, we would like
to exploit the fact that in many applications, one may have contextual information about
the vector ﬁeld f without knowing f precisely. We call such contextual information side
information. Formally, every side information is a subset S of the set of all continuously-
differentiable functions that the vector ﬁeld f is known to belong to. Equipped with a list of
side information S1, . . . , Sk, our goal is to replace the optimization problem in (1.3) with

(1.6)

fF ∩S1∩···∩Sk ∈

arg min
p∈F ∩S1∩···∩Sk

(cid:88)

(cid:96)(p(xi), yi),

(xi,yi)∈D

i.e., to ﬁnd a vector ﬁeld fF ∩S1∩···∩Sk ∈ F that is closest to f on the training set D and also
satisﬁes the side information S1, . . . , Sk that f is known to satisfy.

1.1. Outline and contributions of the paper. In the remainder of this paper, we
build on the mathematical formalism we have introduced thus far and make problem (1.6)

2

more concrete and amenable to computation. In Section 2, we identify six notions of side
information that are commonly encountered in practice and that have attractive convexity
properties, therefore leading to a convex optimization formulation of problem (1.6). In Sec-
tion 3, we show that when the function class F is chosen as the set of polynomial functions of
a given degree, then any combination of our six notions of side information can be enforced
by semideﬁnite programming. The derivation of these semideﬁnite programs leverages ideas
from sum of squares optimization, a concept that we brieﬂy review in the same section for
the convenience of the reader. In Section 4, we demonstrate the applicability of our approach
on three examples from epidemiology, classical mechanics, and cell biology. In each exam-
ple, we show how common sense and contextual knowledge translate to the notions of side
information we present in this paper. Furthermore, in each case, we show that by imposing
side information via semideﬁnite programming, we can learn the behavior of the unknown
dynamics from a very limited set of observations. In our epidemiology example, we also
show the beneﬁts of our approach for a downstream task of optimal control (Subsection 4.5).
In Section 5, we study the question of how well trajectories of a continuously differentiable
vector ﬁeld that satisﬁes some side information can be approximated by trajectories of a poly-
nomial vector ﬁeld that satisﬁes the same side information either exactly or approximately.
We end the paper with a discussion of future research directions in Section 6.

We emphasize that our aim in this paper is not to propose our framework as an alternative
to other learning algorithms, but rather to present a road map for incorporating side informa-
tion in the problem of learning dynamical systems from data in general. We make this road
map more explicit by focusing on the common problem of ﬁtting a polynomial vector ﬁeld
to data. However, our hope is that this work stimulates future research on incorporating side
information in many other approaches to learning dynamical systems.

1.2. Related work. The idea of using sum of squares and semideﬁnite optimization
for verifying various properties of a known dynamical system has been the focus of much
research in the control and optimization communities [38, 9, 31, 11]. Our work borrows some
of these techniques to instead impose a desired set of properties on a candidate dynamical
system that is to be learned from data.

Learning dynamical systems from data is an important problem in the ﬁeld of system
identiﬁcation; see e.g. [12, 26, 5] and references therein. Various classes of vector ﬁelds have
been proposed throughout the years as candidates for the function class F in (1.3); e.g., repro-
ducing kernel Hilbert spaces [45, 47, 14], Guassian mixture models [28], and neural networks
[55, 19]. Some recent approaches to learning dynamical systems from data impose additional
properties on the candidate vector ﬁeld. These properties include contraction [45, 18], sta-
bilizability [46], and stability [29], and can be thought of as side information. In contrast
to our work, imposing these properties requires formulation of nonconvex optimization prob-
lems, which can be hard to solve to global optimality. Furthermore, these references impose
the desired properties only on sample trajectories (as opposed to the entire space where the
properties are known to hold), or introduce an additional layer of nonconvexity to impose the
constraints globally.

We also note that the problem of ﬁtting a polynomial vector ﬁeld to data has appeared
e.g. in [44], though the focus there is on imposing sparsity of the coefﬁcients of the vector
ﬁeld as opposed to side information. More generally, the problem of ﬁnding sparse repre-
sentations of dynamical systems form data has been studied in [13], where an algorithm for
sparse identiﬁcation of nonlinear dynamics is introduced. The closest work in the literature
to our work is that of Hall on shape-constrained regression [22, Chapter 8], where similar
algebraic techniques are used to impose constraints such as convexity and monotonicity on
a polynomial regressor. See also [16] for some statistical properties of these regressors and

3

several applications. Our work can be seen as an extension of this approach to a dynamical
system setting.

2. Side information. In this section, we identify six types of side information which
we believe are useful in practice (see, e.g., Section 4) and that lead to a convex formulation
of problem (1.6). For example, we will see in Section 3 that semideﬁnite programming
can be used to impose any list of side information constraints of the six types below on a
candidate vector ﬁeld that is parameterized as a polynomial function. The set Ω that appears
in these deﬁnitions is a compact subset of Rn over which we would like to learn an unknown
vector ﬁeld f . Throughout this paper, the notation f ∈ C ◦
1 (Ω) denotes that f is continuously
differentiable over an open set containing Ω.

• Interpolation at a ﬁnite set of points. For a set of points {(xi, yi) ∈ Ω × Rn}m

i=1,
we denote by Interp({(xi, yi)}m
1 (Ω) that satisfy
f (xi) = yi for i = 1, . . . , m. An important special case is the setting where the vectors
yi are equal to 0. In this case, the side information is the knowledge of certain equilibrium
points of the vector ﬁeld f .

i=1)1 the set of vector ﬁelds f ∈ C ◦

• Group symmetry. For two given linear representations2 σ, ρ : G → Rn×n of a ﬁnite
group G, with σ(g)x ∈ Ω ∀(x, g) ∈ Ω × G, we deﬁne Sym(G, σ, ρ) to be the set of
vector ﬁelds f ∈ C ◦

1 (Ω) that satisfy the symmetry condition

f (σ(g)x) = ρ(g)f (x) ∀x ∈ Ω, ∀g ∈ G.
For example, let Ω be the unit ball in Rn and consider the group F2 = {1, −1}, with
scalar multiplication as the group operation. If we take σF2 to be the linear representation
of F2 deﬁned by σF2(1) = −σF2(−1) = I, where I denotes the n × n identity matrix,
then a vector ﬁeld f ∈ C ◦

1 (Ω) is in Sym(F2, σF2, σF2) if and only if

f (σF2(1)x) = σF2 (1)f (x) and f (σF2(−1)x) = σF2(−1)f (x) ∀x ∈ Ω,

that is

f (x) = −f (x) ∀x ∈ Ω.
In other words, the set Sym(F2, σF2 , σF2) is exactly the set of even vector ﬁelds in
C ◦
1 (Ω). Similarly, if we take ρF2 to be the linear representation of F2 deﬁned by ρF2(1) =
ρF2 (−1) = I, then Sym(F2, σF2, ρF2) is the set of odd vector ﬁelds in C ◦
1 (Ω). As an-
other example, consider the group Sn of all permutations of the set {1, . . . , n}, with
composition as the group operation. If we take σSn to be the map that assigns to an ele-
ment p ∈ Sn the permutation matrix P obtained by shufﬂing the columns of the identity
matrix according to p, and ρSn to be the constant map that assigns the identity matrix
to every p ∈ Sn, then the set Sym(Sn, σSn , ρSn ) is the set of symmetric functions
in C ◦
1 (Ω) that are invariant under permutations of their argu-
ments. We remark that a ﬁnite combination of side information of type Sym can often
be written equivalently as a single side information of type Sym. For example, the set
Sym(F2, σF2 , ρF2 )∩Sym(Sn, σSn , ρSn ) of even symmetric functions in C ◦
1 (Ω) is equal
to Sym(F2 × Sn, σ, ρ), where F2 × Sn is the direct product of F2 and Sn, σ is given by
σ(g, g(cid:48)) = σF2(g)σSn (g(cid:48)) ∀(g, g(cid:48)) ∈ F2 × Sn, and ρ is the constant map that assigns the
identity matrix to every element in F2 × Sn.

1 (Ω), i.e., functions in C ◦

1To simplify notation, we drop the dependence of the side information on the set Ω.
2Recall that a linear representation of a group G on the vector space Rn is any group homomorphism from G
to the group GL(Rn) of invertible n × n matrices. That is, a linear representation is a map µ : G → GL(Rn) that
satisﬁes µ(gg(cid:48)) = µ(g)µ(g(cid:48)) ∀g, g(cid:48) ∈ G. See [20] for more details about linear representations of groups.

4

• Coordinate nonnegativity. For given sets Pi, Ni ⊆ Ω, i = 1, . . . , n, we denote by

Pos({(Pi, Ni)}n

i=1) the set of vector ﬁelds f ∈ C ◦

1 (Ω) that satisfy

fi(x) ≥ 0 ∀x ∈ Pi, and fi(x) ≤ 0 ∀x ∈ Ni, ∀i ∈ {1, . . . , n}.
These constraints are useful when we know that certain components of the state vector
are increasing or decreasing functions of time in some regions of the state space.3

• Coordinate directional monotonicity. For given sets Pij, Nij ⊆ Ω, i, j = 1, . . . , n, we

denote by Mon({(Pij, Nij)}n

i,j=1) the set of vector ﬁelds f ∈ C ◦

1 (Ω) that satisfy

∂fi
∂xj

(x) ≥ 0 ∀x ∈ Pij, and

∂fi
∂xj

(x) ≤ 0 ∀x ∈ Nij, ∀i, j ∈ {1, . . . , n}.

See Figure 2 for an illustration of a simple example.

x2

f (x1, x2)

x1

Fig. 2: An example of the behavior of a vector ﬁeld
f : R2 → R2 satisfying Mon({(Pij, Nij)}2
i,j=1)
with P21 = N11 = [0, 1] × {0} (i.e.,
∂f2
(x1, 0) ≤ 0 ∀x1 ∈ [0, 1]),
∂x1
and with the rest of the sets Pij and Nij equal to the
empty set.

(x1, 0) ≥ 0 and ∂f1
∂x1

In the special case where Pij = Rn and Nij = ∅ for all i, j ∈ {1, . . . , n} with i (cid:54)= j,
and Pii = Nii = ∅ for all i ∈ {1, . . . , n}, the side information is the knowledge of
the following property of the vector ﬁeld f which appears frequently in the literature on
monotone systems [49]:

∀xinit, ˜xinit ∈ Rn,

xinit ≤ ˜xinit =⇒ x(t, xinit) ≤ x(t, ˜xinit) ∀t ≥ 0.

Here, the inequalities are interpreted elementwise, and the notation x(t, xinit) is used as
before to denote the trajectory of the vector ﬁeld f starting from the initial condition xinit.

• Invariance of a set. A set B ⊆ Ω is invariant under a vector ﬁeld f if any trajectory of
the dynamical system ˙x(t) = f (x(t)) which starts in B stays in B forever.4 In particular,
if B = {x ∈ Rn | hj(x) ≥ 0, j = 1, , . . . , m} for some differentiable functions
hj : Rn → R, then invariance of the set B under the vector ﬁeld f implies the following
constraints (see Figure 3 for an illustration):

∀j ∈ {1, . . . , m}, ∀x ∈ B,

[hj(x) = 0 =⇒ (cid:104)f (x), ∇hj(x)(cid:105) ≥ 0] .

(2.1)
Indeed, suppose for some ˜x ∈ B and for some j ∈ {1, . . . , m}, we had hj(˜x) = 0
but (cid:104)f (˜x), ∇hj(˜x)(cid:105) = ˙h(˜x) < 0, then h(x(t, ˜x)) < 0 for t small enough, implying
that x(t, ˜x) (cid:54)∈ B for t small enough. It is also straightforward to verify that if the “≥”
in (2.1) were replaced with a “>”, then the resulting condition would be sufﬁcient for
invariance of the set B under f . In fact, it follows from a theorem of Nagumo [37, 8]
that condition (2.1) is necessary and sufﬁcient for invariance of the set B under f if B is
convex, the functions h1, . . . , hm are continuously-differentiable, and for every point x
on the boundary of B, the vectors {∇hj(x) | j ∈ {1, . . . , m}, hj(x) = 0} are linearly
independent.5 Given sets Bi = {x ∈ Rn | hij(x) ≥ 0, j = 1, , . . . , mi}, i = 1, . . . , r,

3There is no loss of generality in assuming that each coordinate of the vector ﬁeld is nonnegative or nonpositive
on a single set since one can always reduce multiple sets to one by taking unions. The same comment applies to the
side information of coordinate directional monotonicity that is deﬁned next.

4In some other texts, this property is referred to as forward invariance (to be contrasted with forward-and-

backward invariance).

5In an earlier draft of this paper [2], we had incorrectly claimed that condition (2.1) is equivalent to invariance

5

deﬁned by differentiable functions hij : Rn → R, we denote by Inv({Bi}r
all vector ﬁelds f ∈ C ◦

1 (Ω) that satisfy (2.1) for B ∈ {B1, . . . , Br}.

i=1) the set of

f (ˆx)

∇hi(¯x)

∇hj(ˆx)

f (¯x)

B

Fig. 3: An example of the behavior of a vector
ﬁeld f : R2 → R2 satisfying Inv({B}), where
B := {x ∈ R2 | h1(x) ≥ 0, . . . , hm(x) ≥ 0}
is the set shaded in gray.

• Gradient and Hamiltonian systems. A vector ﬁeld f ∈ C ◦

1 (Ω) is said to be a gradient
vector ﬁeld if there exists a differentiable, scalar-valued function V : Rn → R such that
f (x) = −∇V (x) ∀x ∈ Ω.
(2.2)
Typically, the function V is interpreted as a notion of potential or energy associated with
the dynamical system ˙x(t) = f (x(t)). Note that the value of the function V decreases
along the trajectories of this dynamical system. We denote by Grad the subset of C ◦
1 (Ω)
consisting of gradient vector ﬁelds.
A vector ﬁeld f ∈ C ◦
1 (Ω) over n state variables (x1, . . . , xn) is said to be Hamiltonian if
n is even and there exists a differentiable scalar-valued function H : Rn → R such that
∂H
∂pi
2 +1, . . . , xn)T . The states p and q are usually
where p := (x1, . . . , x n
referred to as generalized momentum and generalized position respectively, following
terminology from physics. Note that a Hamiltonian system conserves the quantity H
along its trajectories. We denote by Ham the subset of C ◦
1 (Ω) consisting of Hamiltonian
vector ﬁelds. For related work on learning Hamiltonian systems, see [3, 21].

(p, q), ∀(p, q) ∈ Ω, ∀i ∈

)T and q := (x n

2 +i(p, q) =

fi(p, q) = −

(p, q), f n

∂H
∂qi

1, . . . ,

n
2

(cid:110)

(cid:111)

,

2

3. Learning Polynomial Vector Fields Subject to Side Information. In this paper,
we take the function class F in (1.6) to be the set of polynomial vector ﬁelds of a given
degree d, i.e., polynomial vector ﬁelds whose monomials have degree at most d. We denote
this function class by

Pd := {p : Rn → Rn | pi is a (scalar-valued) polynomial of degree d for i = 1, . . . , n}.
Furthermore, we assume that the set Ω over which we would like to learn the unknown dy-
namical system, the sets Pi, Ni in the deﬁnition of Pos({(Pi, Ni)}n
i=1), the sets Pij, Nij in
the deﬁnition of Mon({Pij, Nij}n
i=1)
are all closed semialgebraic. We recall that a closed basic semialgebraic set is a set of the
form

i,j=1), and the sets Bi in the deﬁnition of Inv({Bi}r

Λ := {x ∈ Rn| gi(x) ≥ 0, i = 1, . . . , m},
(3.1)
where g1, . . . , gm are (scalar-valued) polynomial functions, and that a closed semialgebraic
set is a ﬁnite union of closed basic semialgebraic sets.

Our choice of working with polynomial functions to describe the vector ﬁeld and the
sets that appear in the side information deﬁnitions are motivated by two reasons. The ﬁrst is
that polynomial functions are expressive enough to represent or approximate a large family
of functions and sets that appear in applications. The second reason, which shall be made
clear shortly, is that because of some connections between real algebra and semideﬁnite opti-
mization, several side information constraints that are commonly available in practice can be
imposed on polynomial vector ﬁelds in a numerically tractable fashion.

of the set B, while in fact additional assumptions are needed for its sufﬁciency.

6

With our aforementioned choices, the optimization problem in (1.6) has as decision vari-
ables the coefﬁcients of a candidate polynomial vector ﬁeld p : Rn → Rn. When the notion
of side information is restricted to the six types presented in Section 2, and under the mild
assumptions that Ω is full dimensional (i.e, that it contains an open set), the constraints of
(1.6) are of the following two types:

(i) Afﬁne constraints in the coefﬁcients of p.
(ii) Constraints of the type

q(x) ≥ 0 ∀x ∈ Λ,
(3.2)
where Λ is a given closed basic semialgebraic set of the form (3.1), and q is a (scalar-
valued) polynomial whose coefﬁcients depend afﬁnely on the coefﬁcients of the vector
ﬁeld p.

For example, membership to Interp({(xi, yi)}m
i=1), Sym(G, σ, ρ), Grad, or Ham can be
enforced by afﬁne constraints,6 while membership to Pos({(Pi, Ni)}n
i=1), or
Mon({(Pij, Nij)}n
i,j=1) can be cast as constraints of the type (3.2). Unfortunately, imposing
the latter type of constraints is NP-hard already when q is a quartic polynomial and Λ = Rn,
or when q is quadratic and Λ is a polytope (see, e.g., [36]).

i=1), Inv({Bi}r

An idea pioneered to a large extent by Lasserre [30] and Parrilo [39] has been to write
algebraic sufﬁcient conditions for (3.2) based on the concept of sum of squares polynomials.
We say that a polynomial h is a sum of squares (sos) if it can be written as h = (cid:80)
i for
some polynomials qi. Observe that if we succeed in ﬁnding sos polynomials σ0, σ1, . . . , σm
such that the polynomial identity

i q2

q(x) = σ0(x) + σ1(x)g1(x) + . . . + σm(x)gm(x)
(3.3)
holds (for all x ∈ Rn), then, clearly, the constraint in (3.2) must be satisﬁed. When the
degrees of the sos polynomials σi are bounded above by an integer r, we refer to the identity
in (3.3) as the degree-r sos certiﬁcate of the constraint in (3.2). Conversely, the following
celebrated result in algebraic geometry [42] states that if g1, . . . , gm satisfy the so-called
“Archimedean property” (a condition slightly stronger than compactness of the set Λ), then
positivity of q on Λ guarantees existence of a degree-r sos certiﬁcate for some integer r large
enough.

THEOREM 3.1 (Putinar’s Positivstellensatz [42]). Let

Λ = {x ∈ Rn | g1(x) ≥ 0, . . . , gm(x) ≥ 0}
and assume that the collection of polynomials {g1, . . . , gm} satisﬁes the Archimedean prop-
erty, i.e., there exists a positive scalar R such that
n
(cid:88)

x2
i = s0(x) + s1(x)g1(x) + . . . + sm(x)gm(x),

R2 −

i=1

where s0, . . . , sm are sos polynomials.7 For any polynomial q, if q(x) > 0 ∀ x ∈ Λ, then
q(x) = σ0(x) + σ1(x)g1(x) + . . . + σm(x)gm(x),

for some sos polynomials σ0, . . . , σm.

The computational appeal of the sum of squares approach stems from its connection to

6To see why membership of a polynomial vector ﬁeld p to Grad can be enforced by afﬁne constraints (a
similar argument works for membership to Ham), observe that if there exists a continuously-differentiable function
V : Rn → R such that p(x) = −∇V (x) for all x in a full-dimensional set Ω, then the function V is necessarily
a polynomial of degree equal to the degree of p plus one. Furthermore, equality between two polynomial functions
over a full-dimensional set can be enforced by equating their coefﬁcients.

7If Λ is known to be contained in a ball of radius R, one can add the redundant constraint R2 − (cid:80)n

i=1 x2

i ≥ 0

to the description of Λ, and then the Archimedean property will be automatically satisﬁed.

7

semideﬁnite programming (SDP). We recall that semideﬁnite programming is the problem
of minimizing a linear function of a symmetric matrix over the intersection of the cone of
positive semideﬁnite matrices with an afﬁne subspace. Semideﬁnite programs can be solved
to arbitrary accuracy in time that scales polynomially with their input size; see [53] for a
survey of the theory and applications of this subject.

To make the connection between Theorem 3.1 and SDP more clear, we remark that the
search for sos polynomials σ0, σ1, . . . , σm of a given degree that verify the polynomial iden-
tity in (3.3) can be automated via SDP. This is true even when some coefﬁcients of the poly-
nomial q are left as decision variables. This claim is a straightforward consequence of the
following well-known fact (see, e.g., [38]): A polynomial h of degree 2d is a sum of squares
if and only if there exists a symmetric matrix Q which is positive semideﬁnite and veriﬁes
the identity

d

h(x) = z(x)T Qz(x),
(3.4)
where z(x) denotes the vector of all monomials in x of degree less than or equal to d. The size
(cid:1), which is polynomial in n (resp. d) if d (resp. n) is ﬁxed. Identity
of the matrix Q is (cid:0)n+d
(3.4) can be written in an equivalent manner as a system of (cid:0)n+2d
(cid:1) linear equations involving
the entries of the matrix Q and the coefﬁcients of the polynomial h. These equations come
from equating the coefﬁcients of the polynomials appearing on the left and right hand sides
of (3.4). The problem of ﬁnding a positive semideﬁntie matrix Q whose entries satisfy these
linear equations is a semideﬁnite program. For implementation purposes, there exist model-
ing languages, such as YALMIP [33], SOSTOOLS [40], or SumOfSquares.jl [54], that accept
sos constraints on polynomials directly and do the conversion to a semideﬁnite program in
the background. See e.g. [32, 9, 23] for more background on sum of squares techniques.

2d

To end up with a semideﬁnite programming formulation of problem (1.6), we also need to
take the loss function (cid:96) that appears in the objective function to be semideﬁnite representable
(i.e., we need its epigraph to be the projection of the feasible set of a semideﬁnite program;
see [7, Chapter 3.2] for a discussion on semideﬁnite representability and several examples).
Luckily, many common loss functions in machine learning are semideﬁnite representable.
Examples of such loss functions include (i) any (cid:96)p norm for a rational number p ≥ 1, or
for p = ∞, (ii) any convex piece-wise linear function, (iii) any sos-convex polynomial (see
e.g. [25] for a deﬁnition), and (iv) any positive integer power of the previous three function
classes.

4. Illustrative Experiments. In this section, we present numerical experiments from
four application domains to illustrate our methodology. The ﬁrst three applications are learn-
ing experiments and the last one involves an optimal control component. In all of our exper-
iments, we use the SDP-based approach explained in Section 3 to tackle problem (1.6) and
2 ∀u, v ∈ Rn. The added value
take our loss function (cid:96)(·, ·) in (1.6) to be (cid:96)(u, v) = (cid:107)u − v(cid:107)2
of side information for learning dynamical systems from data will be demonstrated in these
experiments.

4.1. Diffusion of a contagious disease. The following dynamical system has ap-
peared in the epidemiology literature (see, e.g., [4]) as a model for the spread of a sexually
transmitted disease in a heterosexual population:

(4.1)

˙x(t) = f (x(t)), where x(t) ∈ R2 and f (x) =

(cid:18) −a1x1 + b1(1 − x1)x2
−a2x2 + b2(1 − x2)x1

(cid:19)

.

Here, the quantity x1(t) (resp. x2(t)) represents the fraction of infected males (resp. females)
in the population. The parameter ai (resp. bi) denotes the recovery rate (resp. the infection
8

Fig. 4: Streamplot of the vector ﬁeld in (4.1).
We consider this vector ﬁeld to be the ground
truth and unknown to us. We would like to
learn it over [0, 1]2 from noisy snapshots of
a single trajectory starting from (0.7, 0.3)T
(plotted inred).

rate) in the male population when i = 1, and in the female population when i = 2. We take
(a1, b1, a2, b2) = (0.05, 0.1, 0.05, 0.1)
(4.2)
and plot the resulting vector ﬁeld f in Figure 4. We suppose that this vector ﬁeld is unknown
to us, and our goal is to learn it over Ω := [0, 1]2 from a few noisy snapshots of a single
trajectory. More speciﬁcally, we have access to the training data set

(4.3)

(cid:26)(cid:18)

D :=

x(ti, xinit), f (x(ti, xinit)) + 10−4

(cid:18)εi,1
εi,2

(cid:19)(cid:19)(cid:27)20

,

i=1

where x(t, xinit) is the trajectory of the system (4.1) starting from the initial condition

xinit = (0.7, 0.3)T ,
the scalars ti := i represent a uniform subdivision of the time interval [0, 20], and the scalars
ε1,1, ε1,2, . . . , ε20,1, ε20,2 are independently sampled from the standard normal distribution.
Following our approach in Section 3, we parameterize our candidate vector ﬁeld

p : R2 → R2
as a polynomial function. We choose the degree of this polynomial to be d = 3. The degree
d is taken to be larger than 2 because we do not want to assume knowledge of the degree of
the true vector ﬁeld in (4.1). This makes the task of learning more difﬁcult; see the end of
this subsection where we also learn a vector ﬁeld of degree 2 for comparison.

In absence of any side information, one could solve the least-squares problem

(4.4)

min
p∈Pd

(cid:88)

(cid:107)p(xi) − yi(cid:107)2
2

(xi,yi)∈D

to ﬁnd a polynomial of degree d that best agrees with the training data. For this experiment
only, and for educational purposes, we include a template code using the library SumOf-
Squares.jl [54] of the Julia programming language and demonstrate how the code changes as
we impose side information constraints. We initiate our template with the following code that
solves optimization problem (4.4).

# Input: vectors X1, X2, Y1, Y2 ∈ R20 representing the training set in (4.3),
#
#

where X1 = {x1(ti, xinit)}20
Y1 = {f1(x(ti, xinit)) + 10−4εi,1}20

i=1, X2 = {x2(ti, xinit)}20

i=1,

i=1, and Y2 = {f2(x(ti, xinit)) + 10−4εi,2}20
i=1

model = SOSModel ( s o l v e r )

@polyvar x1 x2
d = 3
z = monomials ( [ x1 , x2 ] , 0 : d )
@ v a r i a b l e ( model , p1 , P o l y ( z ) )

9

# solver could be any SDP solver,
# e.g., Mosek [1], SDPT3 [52], CSDP [10]
# Deﬁne state variables x1, x2
# Construct vector of monomials
# in (x1, x2) up to degree d
# Declare a polynomial vector ﬁeld

(a) No side information

(b) Interp

(c) Interp ∩ Inv

(d) Interp ∩ Inv ∩ Mon

Fig. 5: Streamplots of polynomial vector ﬁelds of degree 3 that are optimal to (4.4) with
different side information constraints appended to it.

@ v a r i a b l e ( model , p2 , P o l y ( z ) )
e r r o r v e c = [ p [ 1 ] . ( X1 , X2 ) − Y1 ;
p [ 2 ] . ( X1 , X2 ) − Y2 ]

@ o b j e c t i v e model Min e r r o r v e c (cid:48)
# S i d e i n f o r m a t i o n c o n s t r a i n t s go h e r e
# . . .
o p t i m i z e ! ( model )

* e r r o r v e c

# whose coefﬁcients are decision variables
# Vector of individual terms appearing
# in the objective of (4.4)

# Solve the optimization problem

Julia template code for learning dynamical systems with side information.

The solution to problem (4.4) returned by the solver MOSEK [1] is plotted in Figure 5a.
Observe that while the learned vector ﬁeld replicates the behavior of the true vector ﬁeld f
on the observed trajectory, it differs signiﬁcantly from f on the rest of the unit square. To
remedy this problem, we leverage the following list of side information that is available from
the context without knowing the exact structure of f .

• Equilibrium point at the origin (Interp). Naturally, if no male or female is infected,
there would be no contagion and the number of infected individuals will remain at
zero. This side information corresponds to our vector ﬁeld p having an equilibrium
point at the origin, i.e., p(0, 0) = 0. Note from Figures 4 and 5a that the true vector
ﬁeld f in (4.1) satisﬁes this constraint, but the vector ﬁeld learned by solving the least-
squares problem in (4.4) does not. We can impose this linear constraint by simply
adding the following lines of code to our template:

@ c o n s t r a i n t model p1(0, 0) == 0
@ c o n s t r a i n t model p2(0, 0) == 0

The vector ﬁeld resulting from solving this new problem is plotted in Figure 5b.

• Invariance of the box [0, 1]2 (Inv). The state variables (x1, x2) of the dynamics in
(4.1) represent fractions of infected individuals and as such, the vector x(t) should be
contained in the box [0, 1]2 at all times t ≥ 0. Note that this property is violated by
the vector ﬁelds learned in Figures 5a and 5b. Mathematically, the invariance of the
unit box is equivalent to the four (univariate) polynomial nonnegativity constraints

p2(x1, 0) ≥ 0, p2(x1, 1) ≤ 0 ∀x1 ∈ [0, 1],

p1(0, x2) ≥ 0, p1(1, x2) ≤ 0 ∀x2 ∈ [0, 1].
These constraints imply that the vector ﬁeld points inwards on the four edges of
the unit box. We replace each one of these four constraints with the corresponding

10

degree-2 sos certiﬁcate of the type in (3.3). For instance, we replace the constraint

p2(x1, 0) ≥ 0 ∀x1 ∈ [0, 1]
with linear and semideﬁnite constraints obtained from equating the coefﬁcients of the
two sides of the polynomial identity

p2(x1, 0) = x1s0(x1) + (1 − x1)s1(x1),
(4.5)
and requiring that the newly-introduced (univariate) polynomials s0 and s1 be qua-
dratic and sos. Obviously, the algebraic identity (4.5) is sufﬁcient for nonnegativity
of p2(x1, 0) over [0, 1]; in this case, it also happens to be necessary [34]. The code in
Julia for imposing the degree-2 sos certiﬁcate in (4.5) is as follows:

@ v a r i a b l e ( model , s1 , P o l y ( [ 1, x1, x2
@ v a r i a b l e ( model , s2 , P o l y ( [ 1, x1, x2
@ c o n s t r a i n t ( model , s1 ,
@ c o n s t r a i n t ( model , s2 ,

1 ] ) )
1 ] ) )
i n SOSCone ( ) )
i n SOSCone ( ) )

# Declare decision polynomial s1
# Declare decision polynomial s2
# Enforce s1 to be sos
# Enforce s2 to be sos

p o l y n o m i a l

i d e n t i t y =

p2((x1, x2) ⇒ (0, x2)) − x1 *s1 −(1 −x1 ) * s2

# Enfroce polynomial
# identity in (4.5)

@ c o n s t r a i n t ( model , c o e f f i c i e n t s ( p o l y n o m i a l

i d e n t i t y ) . = = 0 )

The output of the semideﬁnite program which imposes the invariance of the unit box
and the equilibrium point at the origin is plotted in Figure 5c.

• Coordinate directional monotonicity (Mon). Naturally, one would expect that if
the fraction of infected males rises in the population, the rate of infection of females
should increase. Mathematically, this observation is equivalent to the constraint that

Similarly, swapping the roles played by males and females leads to the constraint

∂p2
∂x1

(x) ≥ 0 ∀x ∈ [0, 1]2.

∂p1
∂x2

(x) ≥ 0 ∀x ∈ [0, 1]2.

Note that this property is violated by the vector ﬁelds learned in Figures 5a to 5c.
Just as in the previous bullet point, we replace each of the above two nonnegativity
constraints with its corresponding degree-2 sos certiﬁcate. To do this, we represent
the closed basic semialgebraic set [0, 1]2 with the polynomial inequalities

x1 ≥ 0, x2 ≥ 0, 1 − x1 ≥ 0, 1 − x2 ≥ 0.
The Julia code for imposing this side information is similar to the one of the previous
bullet point and therefore omitted. Figure 5d demonstrates the vector ﬁeld learned by
our semideﬁnite program when all side information constraints discussed thus far are
imposed.

Note from Figures 5a to 5d that as we add more side information, the learned vector ﬁeld
respects more and more properties of the true vector ﬁeld f . In particular, the learned vector
ﬁeld in Figure 5d is quite similar qualitatively to the true vector ﬁeld in Figure 4 even though
only noisy snapshots of a single trajectory are used for learning.

It is interesting to observe what would happen if we try to learn a degree-2 vector ﬁeld
from the same training set using the list of side information discussed in this subsection. The
outcome of this experiment is plotted in Figure 6. Note that with the equilibrium-at-the-origin
side information, the behavior of the learned vector ﬁeld of degree 2 is already quite close
to that of the true vector ﬁeld. Figure 6d shows that when we impose all side information,

11

(a) No side information

(b) Interp

(c) Interp ∩ Inv

(d) Interp ∩ Inv ∩ Mon

Fig. 6: Streamplots of polynomial vector ﬁelds of degree 2 that are optimal to (4.4) with
different side information constraints appended to it.

θ

(cid:96)

gravity

Fig. 7: The simple pendulum and the streamplot of its vector ﬁeld. We would like to learn
this vector ﬁeld over [−π, π]2 from 10 noisy snapshots coming from two trajectories.

the learned vector ﬁeld is almost indistinguishable from the true vector ﬁeld (even though,
once again, only noisy snapshots of a single trajectory are used for learning). The vector ﬁeld
plotted in Figure 6d is given by

pInterp ∩ Inv ∩ Mon,deg 2(x1, x2) =

(cid:18)0.038x2

1 − 0.100x1x2 − 0.009x2

2 − 0.084x1 + 0.119x2

−0.101x1x2 + 0.003x2

2 + 0.101x1 − 0.052x2

(cid:19)

,

which is indeed very close to the vector ﬁeld in (4.1). For experiments with other degrees and
noise levels, see the appendix.

4.2. Dynamics of the simple pendulum. In this subsection, we consider the dy-
namics of the simple pendulum, i.e., a mass m hanging from a massless rod of length (cid:96) (see
Figure 7). The state variables of this system are given by x = (θ, ˙θ), where θ is the angle that
the rod makes with the vertical axis and ˙θ is the time derivative of this angle. By convention,
the angle θ ∈ [−π, π] is positive when the mass is to the right of the vertical axis, and negative
otherwise. By applying Newton’s second law of motion, the equation

g
(cid:96)
for the dynamics of the pendulum can be derived, where g here is the acceleration due to
gravity. This is a one-dimensional second-order system that we convert to a ﬁrst-order system
as follows:

¨θ(t) = −

sin(θ(t))

(4.6)

˙x(t) = f (x(t)) where

x(t) :=

(cid:19)

(cid:18)θ(t)
˙θ(t)

and f (θ, ˙θ) :=

(cid:18)

˙θ
(cid:96) sin θ

− g

(cid:19)

.

We take the vector ﬁeld in (4.6) with g = (cid:96) = 1 to be the ground truth. We observe

12

(a) No side information

(b) Sym

(c) Sym ∩ Pos

(d) Sym ∩ Pos ∩ Ham

Fig. 8: Streamplots of polynomial vector ﬁelds of degree 5 that best agree with the data
(in the least-squares sense) and obey an increasing number of side information constraints.
In each case, the trajectories of the learned vector ﬁeld starting from the same two initial
conditions as the trajectories observed in the training set are plotted in black.

from this vector ﬁeld a noisy version of two trajectories x(t, x1
init) sampled at
init = ( π
times ti = 3i
10 , 0)T (see Figure 7).
4 , 0)T and x2
More precisely, we assume that our training set (with a slightly different representation of its
elements) is given by

5 , where i ∈ {0, . . . , 4}, with x1

init) and x(t, x2
init = ( 9π

4











2
(cid:91)

(4.7)

D :=


 + 10−2εi,k

θ(ti, xk
˙θ(ti, xk
¨θ(ti, xk

init)
init)
init)
where the εi,k (for k = 1, 2 and i = 0, . . . , 4) are independent 3 × 1 standard normal vectors.
We are interested in learning the vector ﬁeld f over the set Ω := [−π, π]2 from the
training data in (4.7). We parameterize our candidate vector ﬁeld p : R2 → R2 as a degree-5
polynomial. Note that p1(θ, ˙θ) = ˙θ, just from the meaning of our state variables. The only
unknown is therefore the polynomial p2(θ, ˙θ).

k=1

i=0





,

In absence of side information, one can solve a least-squares problem that ﬁnds a poly-
nomial of degree 5 that best agrees with the training data. As it can be seen in Figure 8a,
the resulting vector ﬁeld is very far from the true vector ﬁeld and is unable to even repli-
cate the observed trajectories when started from the same two initial conditions. To learn a
better model, we describe a list of side information which could be derived from contextual
knowledge without knowing the true vector ﬁeld f .

• Sign symmetry (Sym). The pendulum obviously behaves symmetrically with respect
to the vertical axis (plotted with a dotted line in Figure 7). We therefore require our
candidate vector ﬁeld p to satisfy the same symmetry condition

p(−θ, − ˙θ) = −p(θ, ˙θ) ∀(θ, ˙θ) ∈ Ω.
Note that this is an afﬁne constraint in the coefﬁcients of the polynomial p, and that

13

π
4

0

)
t
(
θ

− π
4

0

t

10

0

t

10

Training Data
Ground truth
Learned vector ﬁeld

Fig. 9: Comparison of the trajectory of the simple pendulum in (4.6) starting from ( π
4 , 0)T
(dotted) with the trajectory from the same initial condition of the polynomial vector ﬁeld of
degree 5 that best agrees with the data (in the least-squares solution) in the absence of side
information (left), and subject to side information constraints Sym ∩ Pos ∩ Ham (right).

the true vector ﬁeld f in (4.6) satisﬁes this constraint.

• Coordinate nonnegativity (Pos). We know that the force of gravity pulls the pen-
dulum’s mass down and pushes the angle θ towards 0. This means that the angular
velocity ˙θ decreases when θ is positive and increases when θ is negative. Mathemati-
cally, we must have
p2(θ, ˙θ) ≤ 0 ∀(θ, ˙θ) ∈ [0, π] × [−π, π] and p2(θ, ˙θ) ≥ 0 ∀(θ, ˙θ) ∈ [−π, 0] × [−π, π].
We replace each one of these constraints with their corresponding degree-4 sos certiﬁ-
cate (see the deﬁnition following equation (3.3)). (Note that, because of the previous
symmetry side information, we actually only need to impose one of these two con-
straints.)

• Hamiltonian (Ham). In the simple pendulum model, there is no dissipation of energy

(through friction for example), so the total energy

(4.8)

E(θ, ˙θ) =

1
2

ml2 ˙θ2 + mgl(1 − cos(θ))

is conserved. The two terms appearing in this equation can be interpreted physically
as the kinetic and the potential energy of the system. Furthermore, the total energy E
satisﬁes

˙θ(t) =

1
m(cid:96)2

∂E
∂ ˙θ

(θ(t), ˙θ(t)), and ¨θ(t) = −

1
m(cid:96)2

∂E
∂θ

(θ(t), ˙θ(t)).

The simple pendulum system is therefore a Hamiltonian system, with the associated
Hamiltonian function E
m(cid:96)2 . Note that neither the vector ﬁeld in (4.6) describing the dy-
namics of the simple pendulum nor the associated Hamiltonian are polynomial func-
tions. In our learning procedure, we use only the fact that the system is Hamiltonian,
i.e., that there exists a function H such that
p1(θ, ˙θ) =

(θ, ˙θ), and p2(θ, ˙θ) = −

(θ, ˙θ),

(4.9)

∂H
∂θ

∂H
∂ ˙θ

but not the exact form of this Hamiltonian. Since we are parameterizing the candidate
vector ﬁeld p as a degree-5 polynomial, the function H must be a (scalar-valued)
polynomial of degree 6. The Hamiltonian structure can thus be imposed by adding
afﬁne constraints on the coefﬁcients of p, or by directly learning H and obtaining p
from (4.9).

Observe from Figure 8 that as more side information is added, the behavior of the learned
vector ﬁeld gets closer to the truth. In particular, the solution returned by our semideﬁnite
program in Figure 8d is almost identical to the true dynamics in Figure 7 even though it is
obtained only from 10 noisy samples on two trajectories. Figure 9 shows that even if we start
from an initial condition from which we have made partial trajectory observations, using side

14

information can lead to better predictions for the future of the trajectory.

Fig. 10: Streamplot of the vector ﬁeld in (4.11)
describing the time evolution of the volume N
of a cancerous tumor and the host’s carrying
capacity K (in cubic centimeters). We con-
sider this vector ﬁeld to be the ground truth and
unkown to us. We try to learn it over [0, 2]2
from noisy measurements of three trajectories
(plotted in red).

4.3. Growth of cancerous tumor cells. In this subsection, we consider a model
governing the time evolution of the volume N of a cancerous tumor inside a human body [43].
Cancerous tumors depend for their development on availability of the so-called Endothelial
cells, the supply of which is characterized by a quantity called the carrying capacity K.
Intuitively, K, which has the same unit as N , is proportional to the physical and energetic
resources available for cell growth.

Two common modeling assumptions in cancer cell biology are that (i) the growth rate
of the tumor decreases as the tumor grows, and (ii) that the volume of the tumor increases
(resp. decreases) if it is below (resp. above) the carrying capacity. We follow the dynamics
proposed in [43], which in contrast to prior works in that literature, also models the time
evolution of the carrying capacity. The dynamics reads

(4.10)

(cid:19)

(cid:18) ˙N (t)
˙K(t)

= f (N (t), K(t)),

where f : R2 → R2 is given by
µN
ν

f1(N, K) :=

(4.11)

(cid:19)ν(cid:21)

(cid:20)

1 −

(cid:18) N
K

,

f2(N, K) := ωN − γN

2
3 K.

Here, µ, ν, γ and ω are positive parameters. The dynamics of N is motivated by the so-called
generalized logistic differential equation; the term ωN models the inﬂuence of the tumor on
the Endothelial cells via short-range stimulators, and the term −γN 2
3 K models this same
inﬂuence via long-range inhibitors. See [43] for more details.

Having an accurate model of the growth dyanmics of cancerous tumors is crucial for
the follow-up task of designing treatment plans, e.g., via radiation therapy. We hope that
in the future leveraging side information can lead to learning more accurate models directly
from patient data (as opposed to postulating an exact functional form such as (4.11)). For the
moment however, we take (4.11) with the following parameters to be the ground truth:

See Figure 10 for a streamplot of the corresponding vector ﬁeld.

(µ, ν, γ, ω) =

1
10

(1, 5, 1, 2).

We consider the task of learning the vector ﬁeld f over the compact set Ω := [0, 2]2
from noisy snapshots of three trajectories. Each trajectory was started from a random initial
conditions xk
init) (with k = 1, 2, 3) inside Ω and sampled at times ti = i/20,
with i = 0, . . . , 19 (see Figure 10). More precisely, we have access to the following training
data:
(4.12)

init := (N k

init, K k

(cid:110)(cid:16)

D :=

(N (ti, xk

init), K(ti, xk

init)), ( ˙N (ti, xk

init) + 10−4εi,k,1, ˙K(ti, xk

init) + 10−4εi,k,2)

(cid:17)(cid:111)

0≤i<20,1≤k≤3

,

where the εi,k,l (for i = 0, . . . , 19, k = 1, . . . , 3, and l = 1, 2) are independent standard
15

(a) No side information

(b) Pos - Mon

(c) Pos - Mon ∩ Inv

(d) Pos - Mon ∩ Inv ∩ Pos

(e) Pos - Mon ∩ Inv ∩ Pos
∩ Interp

Fig. 11: Streamplots of polynomial vector ﬁelds of degree 5 that best agree with the data (in
the least-squares sense) and obey an increasing number of side information constraints.

normal variables. We parameterize our candidate vector ﬁeld p as a degree-5 polynomial.
Without any side infromation, ﬁtting this candidate vector ﬁeld to the data in (4.12) via a
least-squares problem leads to the vector ﬁeld plotted in Figure 11a. Once again, the vector
ﬁeld obtained in this way is very far from the true vector ﬁeld.

To do a better job at learning, we impose the side information constraints listed below

that come from expert knowledge in the tumor growth literature (see, e.g., [43, 48, 24]):

• A mix between coordinate nonnegativity and coordinate directional monotonicity
(Pos - Mon). As stated in [43], “one of the few near-universal observations about
˙N
solid tumors is that almost all decelerate, i.e., reduce their speciﬁc growth rate
N , as
they grow larger.”
Based on this contextual knowledge, our candidate vector ﬁeld p should satisfy

1
N

∂p1
∂N

(N, K) −

1
N 2 p1(N, K) ≤ 0 ∀N ∈ (0, 2], ∀K ∈ [0, 2].

Since the state variable N is nonnegative at all times, we can clear denominators to
obtain the constraint

N

∂p1
∂N

(N, K) − p1(N, K) ≤ 0 ∀(N, K) ∈ [0, 2]2.

This is a polynomial nonnegativity constraint over a closed basic semialgebraic set.

• Invariance of the nonnegative orthant (Inv). The state variables N and K quantify
volumes, and as such, should be nonnegative at all times. This corresponds to the
nonnegativity constraints

p2(N, 0) ≥ 0 ∀N ∈ [0, 2],

p1(0, K) ≥ 0 ∀K ∈ [0, 2].

• Coordinate nonnegativity (Pos). As mentioned before, the rate of change ˙N in the

16

Sample trajectory of
Fig. 12:
the system in (4.13) starting from
x(0) = (1, 1, 1)T .

tumor volume is nonnegative when the carrying capacity K exceeds N , and nonposi-
tive otherwise [43]. Mathematically, we must have

p1(N, K) ≥ 0 ∀N ∈ [0, 2], ∀K ∈ [N, 2],

p1(N, K) ≤ 0 ∀N ∈ [0, 2], ∀K ∈ [0, N ].

• Equilibrium point at the origin (Interp). The tumor does not grow if the volume
of cancerous cells and the carrying capacity are both zero. This corresponds to the
constraint p(0, 0) = 0.

We observe from Figure 11 that as more side information is added, the behavior of the
learned vector ﬁeld gets closer and closer to the ground truth.
In particular, the solution
returned by our semideﬁnite program in Figure 11e is very close to the true dynamics in
Figure 10.

4.4. Learning the Lorenz system. In this subsection, we consider the classical

Lorenz system (see, e.g., [50]) that is known for the chaotic properties of its solutions:

(4.13)

˙x(t) = f (x(t)), where x(t) ∈ R3 and f (x) =

Here, we work with the commonly used parameter values

ρ = 2, σ = 10, and β =

8
3

.





σ(x2 − x1)
x1(ρ − x3) − x2
x1x2 − βx3



 .

We consider the task of learning the Lorenz system in (4.13) from 20 noisy snapshots of Nt
trajectories. More precisely, our data is D1 ∪ . . . ∪ DNt, where for k = 1, . . . , Nt,

(4.14)

Dk :=







x(ti, xk

init), f (x(ti, xk

init)) + 10−1





εk
i,1
εk
i,2
εk
i,3














20

.

i=1

init is generated uniformly at random from the box [0, 10]3, and the scalars εk
Here, xk
i,l (for
i = 1, . . . , 20, and l = 1, . . . , 3) are independent standard normal variables. We parameterize
our candidate vector ﬁeld p : R3 → R3 as a polynomial of degree 3. The degree is taken to
be larger than 2 because we do not want to assume knowledge of the degree of the true vector
ﬁeld in (4.13). We assume access to the following list of side information:

• Equilibrium point at the origin (Interp). This corresponds to the constraint p(0) = 0.
• Coordinate nonnegativity (Pos). The state variable x1 should increase when x2 ≥ x1

and decrease otherwise. In other words,
p1(x) ≥ 0 ∀x ∈ S,

p1(x) ≤ 0 ∀x ∈ S(cid:48),

where S := {x ∈ R3 | x2 − x1 ≥ 0} and S(cid:48) := {x ∈ R3 | x2 − x1 ≤ 0}.

17

Side information:

∅

Interp Interp ∩ Pos

Interp ∩ Pos ∩ Mon

Nt = 1
Nt = 2
Nt = 3

1.47e+03
4.61
1.38

516
2.51
0.943

664
2.25
0.787

54.9
0.963
0.352

Table 1: Test error of cubic vector ﬁelds learned from side information and noisy snapshots
of Nt trajectories of the Lorenz system.

• Monotonicity (Mon). The rate of change of any state variable decreases as the vari-

able gets larger. In other words,

∂pi
∂xi

(x) ≤ 0 ∀x ∈ R3,

i = 1, 2, 3.

Since a 3-dimensional streamplot is too clutered to be insightful, we instead report in Table 1
the test error of the polynomial vector ﬁeld of degree d = 3 learned from the side information
and the data described above. Here, the test error of the vector ﬁeld p is measured as
(cid:88)

1
1000

x∈D

(cid:107)p(x) − f (x)(cid:107),

where D is a uniform discretization of the box [−10, 10]3 with 1000 samples. As one can
observe, the test error is reduced most of the time as more side information constraints are
incorporated. For experiments with other degrees, noise levels, and a different notion of test
error, see the appendix.

4.5. Following learning with optimal control. In this subsection, we revisit the
contagion dynamics (4.1) and study the effect of side information on policy decisions to
contain an outbreak. Suppose that by an initial screening of a random subset of the population,
it is estimated that a fraction 0.5 (resp. 0.4) of males (resp. females) are infected with the
disease. We would like to contain the outbreak by performing daily widespread testing of the
population. We introduce two control decision variables u1 and u2, representing respectively
the fraction of the population of males and females that are tested per unit of time. We
suppose that testing slows down the spread of the disease (due e.g. to appropriate action that
can be taken on the positive cases) as follows:

(4.15)

˙x(t) = f (x(t)) −

(cid:18)u1x1
u2x2

(cid:19)

,

where f (x(t)) is the unknown dynamics of the spread of the disease in the absence of any
control.

We suppose that the monetary cost of testing a fraction u1 of males and u2 of females is

given by α(u1 + u2) for some known positive scalar α. Our goal is to minimize the sum
c(u1, u2) := x1(T, ˆxinit) + x2(T, ˆxinit) + α(u1 + u2),
(4.16)
of the total number x1(T, ˆxinit) + x2(T, ˆxinit) of infected individuals at the end of a desired
time period T , and the monetary cost α(u1 + u2) of our control law. Here, x1(t, ˆxinit) and
x2(t, ˆxinit) evolve according to (4.15) when started from the point ˆxinit = (0.5, 0.4)T . In our
experiments, we take T = 20, α = 0.4, and f to be the vector ﬁeld in (4.1) with parameters
in (4.2).

Given access to the vector ﬁeld f , one could simply design an optimal control law
2) that minimizes the cost function c(u1, u2) in (4.16) by gridding the control space

1, u∗

(u∗

18

Fig. 13: The graph of the function
c(u1, u2) in (4.16) with T = 20,
α = 0.4, and f as in (4.1) with pa-
rameters in (4.2). The minimizer of
the function c(u1, u2), which corre-
sponds to the optimal control law,
is indicated with a blue arrow. The
control laws that are optimal for dy-
namics learned from a single tra-
jectory of f with different side in-
formation constraints are indicated
with black arrows.

[0, 1]2, and computing c(u1, u2) for every point of the grid. Indeed, for a given (u1, u2),
c(u1, u2) can be computed by simulating the dynamics in (4.15) from the initial condition
ˆxinit. The optimal control law obtained by following this strategy is depicted in Figure 13
together with the graph of the function c(u1, u2).

We now consider the same setup as Subsection 4.1, where we do not know the vector ﬁeld
f , but have observed 20 noisy samples of a single trajectory of it starting from (0.7, 0.3)T
(c.f.
(4.3)). We design our control laws instead for the four polynomial vector ﬁelds of
degree 3 that were learned in Subsection 4.1 with no side information, with Interp, with
Interp ∩ Inv, and with Interp ∩ Inv ∩ Mon. This is done by following the procedure
described in the previous paragraph, but using the learned vector ﬁeld instead of f . The
corresponding four control laws are depicted in Figure 13 with black arrows. We emphasize
that while the control laws are computed from the learned vector ﬁelds, their associated costs
in Figure 13 are computed by applying them to the true vector ﬁeld. It is interesting to observe
that adding side information constraints during the learning phase leads to the design of better
control laws.

From Table 2, we see that if we had access to the true vector ﬁeld, an optimal control
law would lead to the eradication of the disease by time T . The ﬁrst four rows of this table
demonstrate the fraction of infected males and females at time T when control laws that are
optimal for dynamics learned with different side information constraints are applied to the
true vector ﬁeld. It is interesting to note that with no side information, a large fraction of
the population remains infected, whereas control laws computed with three side information
constraints are able to eradicate the disease almost completely.

5. Approximation Results. In this section, we present some density results for poly-
nomial vector ﬁelds that obey side information. This provides some theoretical justiﬁcation
for our choice of parameterizing our candidate vector ﬁelds as polynomial functions.

More precisely, we are interested in the following question: Given a continuously-
differentiable vector ﬁeld f satisfying a list of side information constraints from Section 2, is
there a polynomial vector ﬁeld that is “close” to f and satisﬁes the same list of side informa-
tion constraints? For the purpose of learning dynamical systems, arguably the most relevant
notion of “closeness” between two vector ﬁelds is one that measures how differently their
corresponding trajectories can behave when started from the same initial condition. More
formally, we ﬁx a compact set Ω ⊂ Rn and a time horizon T , and we deﬁne the following
19

Side information
None
Interp
Interp ∩ Inv
Interp ∩ Inv ∩ Mon
True vector ﬁeld

x1(T, ˆxinit)
0.45
0.41
0.31
0.01
0.00

x2(T, ˆxinit)
0.40
0.29
0.12
0.01
0.00

Table 2: The ﬁrst four rows indicate the fraction of infected males and females at the end
of the period T when a control law, optimal for dynamics learned from a single trajectory
with different side information constraints, is applied to the true vector ﬁeld. The last row
indicates the fraction of infected males and females at time T resulting from applying the
optimal control law computed with access to the true dynamics.

notion of distance between any two vector ﬁelds f, g ∈ C ◦
(5.1)

1 (Ω):

dΩ,T (f, g) := sup

max {(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2, (cid:107) ˙xf (t, xinit) − ˙xg(t, xinit)(cid:107)2} ,

(t,xinit)∈S

where xf (t, xinit) (resp. xg(t, xinit)) is the trajectory starting from xinit ∈ Ω and following the
dynamics of f (resp. g), and
S := {(t, xinit) ∈ [0, T ] × Ω | xf (s, xinit), xg(s, xinit) ∈ Ω ∀s ∈ [0, t]}.
(5.2)
The reason why in the deﬁnition of dΩ,T , we take the supremum over S instead of over
[0, T ] × Ω is to ensure that the trajectories that appear in (5.1) are well deﬁned.

In Subsection 5.1, we show that under some assumptions that are often met in prac-
tice, polynomial vector ﬁelds can be made arbitrarily close to any continuously-differentiable
vector ﬁeld f (in the sense of (5.1)), even if they are required to satisfy one side information
constraint that f is known to satisfy. In Subsection 5.2, we drop our assumptions and general-
ize this approximation result to any list of side information constraints at the price of allowing
an arbitrarily small error in the satisfaction of these constraints. Furthermore, we show that
the approximate satisfaction of side information can be certiﬁed by a sum of squares proof.

5.1. Approximating a vector ﬁeld while (exactly) satisfying one side information
constraint. The following theorem is the main result of this section. We will need the fol-
lowing deﬁnition for a subcase of this theorem: Given a collection of sets A1, . . . , Ar, we
deﬁne G(A1, . . . , Ar) to be the graph on r vertices labeled by the sets A1, . . . , Ar, where two
vertices Ai and Aj are connected if Ai ∩ Aj (cid:54)= ∅.

THEOREM 5.1. For any compact set Ω ⊂ Rn, time horizon T > 0, desired accuracy
1 (Ω) which satisﬁes one of the following side information

ε > 0, and vector ﬁeld f ∈ C ◦
constraints (see Section 2):
(i) Interp({(xi, yi)}m
(ii) Sym(G, σ, ρ),
(iii) Pos({(Pi, Ni)}n
(iv) Mon({(Pij, Nij)}n

i=1), where x1, . . . , xm ∈ Ω,

i=1), where for each i ∈ {1, . . . , n}, Pi ∩ Ni = ∅,

(v) Inv({Bi}r

i,j=1), where for each i, j ∈ {1, . . . , n}, the sets Pij and Nij
belong to different connected components of the graph G(Pi1, Ni1, . . . , Pin, Nin),
i=1), where the sets Bi are pairwise nonintersecting, and deﬁned as
Bi := {x ∈ Rn | hij(x) ≥ 0, j = 1, . . . , mi} for some concave continuously-
differentiable functions hij that satisfy

∀i ∈ {1, . . . , r}, ∃xi ∈ Bi such that hij(xi) > 0 for j = 1, . . . , mi,

(vi) Grad,
(vi’) Ham,

20

there exists a polynomial vector ﬁeld p : Rn → Rn that satisﬁes the same side information
constraint as f and has dΩ,T (f, p) ≤ ε.

Before we present the proof, we recall the classical Stone-Weierstrass approximation theo-
rem. Note that while the theorem is stated here for scalar-valued functions, it readily extends
to vector-valued ones.

THEOREM 5.2. (see, e.g., [51]) For any compact set Ω ⊂ Rn, scalar ε > 0, and contin-

uous function f : Ω → R, there exists a polynomial p : Rn → R such that
max
x∈Ω

|f (x) − p(x)| ≤ ε.

For two vector ﬁelds f, g : Rn → Rn and a set Ω ⊆ Rn, let us deﬁne

(cid:107)f − g(cid:107)Ω := max
x∈Ω

(cid:107)f (x) − g(x)(cid:107)2.

The following proposition relates this quantity to the notion of distance dΩ,T (f, g) deﬁned in
(5.1). We recall that for a scalar L ≥ 0, a vector ﬁeld f is said to be L-Lipschitz over Ω if

(cid:107)f (x) − f (y)(cid:107)2 ≤ L(cid:107)x − y(cid:107)2 ∀x, y ∈ Ω.

Note that any vector ﬁeld f ∈ C ◦
ative scalar L.

1 (Ω) is L-Lipschitz over a compact set Ω for some nonneg-

PROPOSITION 5.3. For any compact set Ω ⊂ Rn, any ﬁnite time horizon T > 0, and

any two vector ﬁelds f, g ∈ C ◦

1 (Ω), we have

(cid:107)f − g(cid:107)Ω ≤ dΩ,T (f, g) ≤ max{T eLT , 1 + LT eLT }(cid:107)f − g(cid:107)Ω,

where L ≥ 0 is any scalar for which either f or g is L-Lipschitz over Ω.

We note that the dependence of the right-most term on T cannot be avoided. (For example,
for n = 1, Ω = [0, 1], f (x) = 0, gε(x) = εx, we have (cid:107)f − gε(cid:107)Ω = ε, but dΩ,T (f, gε) ≥ 1
2
for all T ≥ log 2
ε .) To present the proof of this proposition, we need to recall the Gr¨onwall-
Bellman inequality.

LEMMA 5.4 (Gr¨onwall-Bellman inequality [6, 27]). Let I = [a, b] denote a nonempty

interval on the real line. Let u, α, β : I → R be continuous functions satisfying

u(t) ≤ α(t) +

(cid:90) t

a

β(s)u(s) ds ∀t ∈ I.

If α is nondecreasing and β is nonnegative, then

u(t) ≤ α(t)e

(cid:82) t
a β(s) ds ∀t ∈ I.

Proof of Proposition 5.3. We ﬁx a compact set Ω ⊂ Rn, a time horizon T > 0, and
1 (Ω), with f being L-Lipschitz over Ω for some scalar L ≥ 0. To
˙xf (0, xinit) = f (xinit) and

vector ﬁelds f, g ∈ C ◦
see that the ﬁrst inequality holds, note that for any xinit ∈ Ω,
˙xg(0, xinit) = g(xinit). Therefore, (cid:107)f − g(cid:107)Ω ≤ dΩ,T (f, g).

For the second inequality, ﬁx (t, xinit) ∈ S, where S is deﬁned in (5.2). Let us ﬁrst bound

(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2. By deﬁnition of xf and xg, we have

xf (t, xinit) − xg(t, xinit) =

=

+

(cid:90) t

0
(cid:90) t

0
(cid:90) t

0

f (xf (s, xinit)) − g(xg(s, xinit))ds

f (xf (s, xinit)) − f (xg(s, xinit)) ds

f (xg(s, xinit)) − g(xg(s, xinit)) ds.

21

Using the triangular inequality, we get

(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2 ≤

(5.3)

+

(cid:90) t

0
(cid:90) t

0

(cid:107)f (xf (s, xinit)) − f (xg(s, xinit))(cid:107)2 ds

(cid:107)f (xg(s, xinit)) − g(xg(s, xinit))(cid:107)2 ds.

Because the function f is L-Lipschitz over Ω, we have

(cid:107)f (xf (s, xinit)) − f (xg(s, xinit))(cid:107)2 ≤ L(cid:107)xf (s, xinit) − xg(s, xinit)(cid:107)2 ∀s ∈ [0, t].

Furthermore, we know that for all s ∈ [0, t], xg(s, xinit) ∈ Ω, and therefore

(cid:107)f (xg(s, xinit)) − g(xg(s, xinit))(cid:107)2 ≤ (cid:107)f − g(cid:107)Ω.

We can hence further bound the left hand side of (5.3) as

(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2 ≤ L

By Lemma 5.4, we get

(cid:90) t

0

(cid:107)xf (s, xinit) − xg(s, xinit)(cid:107)2 ds + t(cid:107)f − g(cid:107)Ω.

(5.4)

(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2 ≤ teLt(cid:107)f − g(cid:107)Ω.

Next, we bound the quantity(cid:107) ˙xf (t, xinit)− ˙xg(t, xinit)(cid:107)2, which can be expressed in terms

of the vector ﬁelds f and g as (cid:107)f (xf (t, xinit)) − g(xg(t, xinit))(cid:107)2. We have

(5.5)

(cid:107)f (xf (t, xinit)) − g(xg(t, xinit))(cid:107)2 ≤ (cid:107)f (xf (t, xinit)) − f (xg(t, xinit))(cid:107)2
+ (cid:107)f (xg(t, xinit)) − g(xg(t, xinit))(cid:107)2
≤ L(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2 + (cid:107)f − g(cid:107)Ω
≤ (1 + LteLt)(cid:107)f − g(cid:107)Ω,

where the ﬁrst inequality follows from the triangular inequality, the second from the deﬁnition
of (cid:107) · (cid:107)Ω and the fact that f is L-Lipschitz over Ω, and the third one from (5.4).
Putting (5.4) and (5.5) together, and using the fact that t ≤ T , we have

max {(cid:107)xf (t, xinit) − xg(t, xinit)(cid:107)2, (cid:107)f (xf (t, xinit)) − g(xg(t, xinit))(cid:107)2}

≤ max{teLt, 1 + LteLt}(cid:107)f − g(cid:107)Ω
≤ max{T eLT , 1 + LT eLT }(cid:107)f − g(cid:107)Ω.

Taking the supremum over (t, xinit) ∈ S, we get

dΩ,T (f, g) ≤ max{T eLT , 1 + LT eLT }(cid:107)f − g(cid:107)Ω.

Proof of Theorem 5.1. Let us ﬁx a compact set Ω ⊂ Rn, a time horizon T > 0, and a
desired accuracy ε > 0. Let f ∈ C ◦
1 (Ω) be a vector ﬁeld that satisﬁes any one of the side
information constraints stated in the theorem. Note that f is L-Lipschitz over Ω for some
L ≥ 0. We claim that for any δ > 0, there exists a polynomial vector ﬁeld p : Rn → Rn that
satisﬁes the same side information constraint as f and the inequality

By Proposition 5.3, if we take

(cid:107)f − p(cid:107)Ω ≤ δ.

δ = ε/ max{T eLT , 1 + LT eLT },
(5.6)
this shows that there exists a polynomial vector ﬁeld p : Rn → Rn that satisﬁes the same side
information as f and the inequality

dΩ,T (f, p) ≤ ε.

We now give a case-by-case proof of our claim above depending on which side informa-

tion f satisﬁes. Throughout the rest of the proof, the constant δ is ﬁxed as in (5.6).

• Case (i): Suppose f ∈ Interp({(xi, yi)}m

i=1), where x1, . . . , xm ∈ Ω. Without

22

loss of generality, we assume that the points xi are all different, or else we can discard the
redundant constraints. Let δ(cid:48) be a positive constant that will be ﬁxed later. By Theorem 5.2,
there exists a polynomial vector ﬁeld q that satisﬁes (cid:107)f − q(cid:107)Ω ≤ δ(cid:48). We claim that there
exists a polynomial ˜q of degree m − 1 such that
(5.7)
and (cid:107)˜q(cid:107)Ω ≤ Cδ(cid:48), where C is a constant depending only on the points xi and the set Ω. Indeed,
(5.7) can be viewed as a linear system of equations where the unknowns are the coefﬁcients
of ˜q in some basis. For example, if we let N = (cid:0)n+m−1
(cid:1) and ˜qcoeff ∈ RN ×n be the matrix
whose j-th column is the vector of coefﬁcients of ˜qj in the standard monomial basis, then
(5.7) can be written as

(q + ˜q)(xi) = yi

i = 1, . . . , m,

n

A ˜qcoeff = ∆,

(5.8)
where ∆ ∈ Rm×n is the matrix whose i-th row is given by yT
i − q(xi)T , and A ∈ Rm×N
is the matrix whose i-th row is the vector of all standard monomials in n variables and of
degree up to m − 1 evaluated at the point xi. One can verify that the rows of the matrix A
are linearly independent (see, e.g., [15, Corollary 4.4]), and so the matrix AAT is invertible.
If we let A+ = AT (AAT )−1, then ˜qcoeff = A+∆ is a solution to (5.8). Since the matrix
A+ only depends on the points xi, and since all the entries of the matrix ∆ are bounded
in absolute value by δ(cid:48), there exists a constant c such that all the entries of the matrix ˜qcoeff
are bounded in absolute value by cδ(cid:48). Since the set Ω is compact, there exists a constant C
depending only on the points xi and the set Ω such that (cid:107)˜q(cid:107)Ω ≤ Cδ(cid:48).
Finally, by taking p := q + ˜q, we get p ∈ Interp({(xi, yi)}m

i=1), and

(cid:107)f − p(cid:107)Ω ≤ δ(cid:48)(1 + C).

We take δ(cid:48) = δ

1+C to conclude the proof for this case.

• Case (ii): Suppose f ∈ Sym(G, σ, ρ), where G is a ﬁnite group. Let δ(cid:48) be a positive
constant that will be ﬁxed later. By Theorem 5.2, there exists a polynomial vector ﬁeld p that
satisﬁes (cid:107)f − p(cid:107)Ω ≤ δ(cid:48). Let pG : Rn → Rn be the polynomial deﬁned as

pG(x) :=

1
|G|

(cid:88)

g∈G

ρ(g−1)p(σ(g)x) ∀x ∈ Rn,

where |G| is the size of the group G. We claim that pG ∈ Sym(G, σ, ρ). Indeed, for any
h ∈ G and x ∈ Ω, using the fact that σ is a group homomorphism, we get

pG(σ(h)x) =

(cid:88)

ρ(g−1)p(σ(gh)x).

1
|G|

g∈G
By doing the change of variables g(cid:48) = gh in the sum above, and using the fact that ρ is a
group homomorphism, we get

pG(σ(h)x) =

=

1
|G|

1
|G|

(cid:88)

g(cid:48)∈G
(cid:88)

g(cid:48)∈G

ρ(hg(cid:48)−1)p(σ(g(cid:48))x)

ρ(h)ρ(g(cid:48)−1)p(σ(g(cid:48))x).

We now claim that by by taking δ(cid:48) = δ

= ρ(h)pG(x).
(cid:16) 1
|G|

23

(cid:80)

g∈G (cid:107)ρ(g−1)(cid:107)

(cid:17)−1

, where (cid:107) · (cid:107) denotes

the operator norm of its matrix argument, we get (cid:107)f − pG(cid:107)Ω ≤ δ. Indeed,
1
|G|

(cid:0)f (x) − ρ(g−1)p(σ(g)x)(cid:1)

f (x) − pG(x) =

(cid:88)

=

=

1
|G|

1
|G|

g∈G
(cid:88)

g∈G
(cid:88)

g∈G

ρ(g−1) (ρ(g)f (x) − p(σ(g)x))

ρ(g−1) (f (σ(g)x) − p(σ(g)x)) ,

where in the last equation, we used the fact that f ∈ Sym(G, σ, ρ). Therefore,
(cid:88)

(cid:107)ρ(g−1)(cid:107)(cid:107)f (σ(g)x) − p(σ(g)x)(cid:107)2

(cid:107)f (x) − pG(x)(cid:107) ≤

1
|G|

g∈G



≤



1
|G|

(cid:88)

(cid:107)ρ(g−1)(cid:107)


 δ(cid:48) = δ.

• Case (iii): If f ∈ Pos({(Pi, Ni)}n
Ni are subsets of Ω and satisfy Pi ∩ Ni = ∅.

g∈G
i=1), where for each i ∈ {1, . . . , n}, the sets Pi and

For i = 1, . . . , n, let di denote the distance between the sets Pi and Ni:
di := min

(cid:107)x − x(cid:48)(cid:107)2.

x∈Pi,x(cid:48)∈Ni

Since Pi and Ni are compact sets with empty intersection, the scalar di is positive. Fix γ to
be any positive scalar smaller than mini=1,...,n di. For i = 1, . . . , n, let
γ
z | x ∈ Pi, z ∈ Rn, and (cid:107)z(cid:107)2 ≤ 1},
2
γ
2

z | x ∈ Ni, z ∈ Rn, and (cid:107)z(cid:107)2 ≤ 1}.

:= {x +

N γ
i

P γ
i

i = ∅ for i = 1, . . . , n. Let ψ : Rn → Rn be the piecewise-

With our choice of γ, P γ
constant function deﬁned as

:= {x +
i ∩ N γ




1
−1
0
and φγ : Rn → R be the “bump-like” function that is equal to e− 1
0 elsewhere. Let ψconv : Rn → Rn be the normalized convolution of ψ with φγ , i.e.,

for i = 1, . . . , n,

if x ∈ P γ
i
if x ∈ N γ
i
otherwise

ψi(x) =

1−(cid:107)z(cid:107)2 when (cid:107)z(cid:107)2 ≤ γ



2 and

ψconv(x) :=

1
z∈Rn φγ(z)dz

(cid:82)

(cid:90)

z∈Rn

ψ(x + z)φγ(z)dz.

Note that ψconv is a continuous function as it is the convolution of a piecewise-constant func-
tion ψ with a continuous function φγ. Moreover, for each i ∈ {1, . . . , n}, ψconv

satisﬁes

ψconv
i

(x) = 1 ∀x ∈ Pi, ψconv

i

(x) = −1 ∀x ∈ Ni, and |ψconv

i

i
(x)| ≤ 1 ∀x ∈ Ω.

Now let f δ ∈ C ◦

1 (Ω) be the vector ﬁeld deﬁned component-wise by

δ
√
2

f δ
i (x) = fi(x) +

ψconv
i

(x)

i = 1, . . . , n.

n
Note that for i = 1, . . . , n, the function f δ
i
and bounded above by −δ
√
the existence of a polynomial vector ﬁeld p such that (cid:107)f δ − p(cid:107)Ω ≤ δ
√
2
p ∈ Pos({(Pi, Ni)}n
i=1) and satisﬁes (cid:107)f − p(cid:107)Ω ≤ δ.

n on Ni. Moreover (cid:107)f − f δ(cid:107)Ω ≤ δ

is continuous, bounded below by

n on Pi,
2 . Theorem 5.2 guarantees
In particular,

n .

δ
√

2

2

24

• Case (iv): If f ∈ Mon({(Pij, Nij)}n

i,j=1), where for each i, j ∈ {1, . . . , n}, the
sets Pij and Nij are subsets of Ω and belong to different connected components of the graph
G(Pi1, Ni1, . . . , Pin, Nin). (Recall the deﬁnition of this graph from the ﬁrst paragraph of
Subsection 5.1.)

Consider an index i ∈ {1, . . . , n}, and let Ci1, . . . , Ciri be the connected components
of the graph G(Pi1, Ni1, . . . , Pin, Nin). Let Uil be the union of the sets in component Cil.
Since the sets Ui1, . . . , Uiri are compact and pairwise non-intersecting, the minimum distance
di := minx∈Uil,x(cid:48)∈Uil(cid:48) ,l(cid:54)=l(cid:48) (cid:107)x − x(cid:48)(cid:107)2 between any two of them is positive. Fix γi to be a
positive scalar smaller than di, and for each l ∈ {1, . . . , ri}, let

U γi
il

:= {x +

γi
2
Deﬁne ψi : Rn → R to be the piecewise-linear function deﬁned as
xj − (cid:80)
0

if x ∈ U γi

ψi(x) =

z | x ∈ Uil, z ∈ Rn, and (cid:107)z(cid:107)2 ≤ 1}.

otherwise.

j:Nij ∈Cl

j:Pij ∈Cl

(cid:26) (cid:80)

xj

il for some l ∈ {1, . . . , ri}

: Rn → R be the normalized convolution of ψi with the “bump-like” function

Let ψconv
φγi : Rn → R that is equal to e− 1

i

1−(cid:107)z(cid:107)2 when (cid:107)z(cid:107)2 ≤ γi

2 and 0 elsewhere; that is

ψconv
i

(x) :=

1
z∈Rn φγi(z)dz

(cid:82)

(cid:90)

z∈Rn

ψi(x + z)φγi (z)dz.

The function ψconv
and satisﬁes

i

is continuously differentiable (because φγi is continuously differentiable)

∂ψconv
i
∂xj

(x) = 1 ∀x ∈ Pij,

∂ψconv
i
∂xj

(x) = −1 ∀x ∈ Nij,

|ψconv
i

(x)| ≤ sup
x∈Ω

|ψi(x)| ∀x ∈ Ω.

1

n )T and f δ(cid:48)

, . . . , ψconv

Now, let ψconv := (ψconv

(x) := f (x) + δ(cid:48)ψconv(x) for a constant
− f (cid:107)Ω ≤ δ(cid:48)(cid:107)ψconv(cid:107)Ω, and for each pair of

δ(cid:48) > 0 that will be ﬁxed later. Note that (cid:107)f δ(cid:48)
indices i, j ∈ {1, . . . , n},
∂f δ(cid:48)
i
∂xj

(x) ≥ δ(cid:48) ∀x ∈ Pij,

∂f δ(cid:48)
i
∂xj
A generalization of the Stone-Weierstrass approximation result stated in Theorem 5.2 to
continuously-differentiable functions (see, e.g., [41]) guarantees the existence of a polyno-
mial p : Rn → Rn such that

(x) ≤ −δ(cid:48) ∀x ∈ Nij.

(cid:107)f δ(cid:48)

− p(cid:107)Ω ≤ δ(cid:48),

sup
x∈Ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂f δ(cid:48)
i
∂xj

(x) −

∂pi
∂xj

(cid:12)
(cid:12)
(cid:12)
(x)
(cid:12)
(cid:12)

≤ δ(cid:48)/2 ∀i, j ∈ {1, . . . , n}.

In particular, p ∈ Mon({Pij, Nij}n

i,j=1) and satisﬁes (cid:107)f − p(cid:107)Ω ≤ δ(cid:48)(1 + (cid:107)ψconv(cid:107)Ω).

We conclude the proof by taking δ(cid:48) =
• Case (v): If f ∈ Inv({Bi}r

δ
1+(cid:107)ψconv(cid:107)Ω

.

intersecting, and deﬁned as Bi
continuously-differentiable concave functions hij : Rn → R that satisfy
(5.9)

∀i ∈ {1, . . . , r}, ∃xi ∈ Bi such that hij(xi) > 0 for j = 1, . . . , mi.

i=1), where the sets Bi are subsets of Ω, pairwise non-
:= {x ∈ Rn | hij(x) ≥ 0, j = 1, . . . , mi} for some

By the same argument as that for Case (iii), for each i ∈ {1, . . . , r}, there exists a

continuous function ψconv

i

: Rn → R that satisﬁes

ψconv
i
Let δ(cid:48) :=

(x) = 1 ∀x ∈ Bi, ψconv

(x) = 0 ∀x ∈ ∪i(cid:48)(cid:54)=iBi(cid:48), |ψconv
2r(1+maxx,x(cid:48) ∈Ω (cid:107)x−x(cid:48)(cid:107)2) , and for i = 1 . . . , r, let xi ∈ Bi be any point satisfying

(x)| ≤ 1 ∀x ∈ Ω.

δ

i

i

25

hij(xi) > 0 for j = 1, . . . , mi. Consider the continuous vector ﬁeld

f δ(cid:48)

(x) := f (x) − δ(cid:48)

r
(cid:88)

i=1

ψconv
i

(x)(x − xi).

For every x ∈ Ω, the triangular inequality gives

(cid:107)f (x) − f δ(cid:48)

(x)(cid:107)2 ≤ δ(cid:48)

r
(cid:88)

i=1

(cid:107)x − xi(cid:107)2

≤ rδ(cid:48) max
x(cid:48)∈Ω

(cid:107)x − x(cid:48)(cid:107)2 =

δ
2

,

and so (cid:107)f − f δ(cid:48)
and for each x ∈ Bi satisfying hij(x) = 0,

(cid:107)Ω ≤ δ/2. Furthermore, for each i ∈ {1, . . . , r}, for each j ∈ {1, . . . , mi},

(cid:104)f δ(cid:48)

(x), ∇hij(x)(cid:105) = (cid:104)f (x), ∇hij(x)(cid:105) − δ(cid:48)

r
(cid:88)

k=1

φconv
k

(x)(cid:104)x − xk, ∇hij(x)(cid:105)

(5.10)

= (cid:104)f (x), ∇hij(x)(cid:105) − δ(cid:48)(cid:104)x − xi, ∇hij(x)(cid:105)
≥ −δ(cid:48)(cid:104)x − xi, ∇hij(x)(cid:105)
≥ δ(cid:48)(hij(xi) − hij(x))
= δ(cid:48)hij(xi),

where the second equality follows from the deﬁnition of ψconv
ﬁrst inequality from the fact thatf ∈ Inv({Bi}r
the function hij, and the last equality from the fact that hij(x) = 0.

and the fact that x ∈ Bi, the
i=1), the second inequality from concavity of

For a constant δ(cid:48)(cid:48) > 0 that will be ﬁxed later, Theorem 5.2 guarantees the existence of
a polynomial vector ﬁeld p such that (cid:107)f δ(cid:48)
− p(cid:107)Ω ≤ δ(cid:48)(cid:48). By triangular inequality we have
(cid:107)f − p(cid:107)Ω ≤ δ/2 + δ(cid:48)(cid:48). Furthermore, for each i ∈ {1, . . . , r}, for each j ∈ {1, . . . , mi}, and
for each x ∈ Bi satisfying hij(x) = 0, we have

i

(cid:104)p(x), ∇hij(x)(cid:105) = (cid:104)p(x) − f δ(cid:48)

(x), ∇hij(x)(cid:105) + (cid:104)f δ(cid:48)

(x), ∇hij(x)(cid:105)

≥ −δ(cid:48)(cid:48)(cid:107)∇hij(x)(cid:107)2 + δ(cid:48)hij(xi)

due to (5.10) and the Cauchy-Schwarz inequality. Let

δ(cid:48)(cid:48) := min

(cid:26) δ
2

, min
i∈{1,...,r}

min
j∈{1,...,mi},x∈Bi

δ(cid:48) hij(xi)
(cid:107)∇hij(x)(cid:107)2

(cid:27)

,

• Case (vi): If f ∈ Grad.

and note that δ(cid:48)(cid:48) > 0 as we needed before because hij(xi) > 0 for each i ∈ {1, . . . , r} and
j ∈ {1, . . . , mi}. With this choice of δ(cid:48)(cid:48), we get that p ∈ Inv({Bi}r
i=1) and (cid:107)f − p(cid:107)Ω ≤ δ.
In this case, there exists a continuously-differentiable
function V : Rn → R such that f (x) = −∇V (x). A generalization of the Stone-Weierstrass
theorem to continuously-differentiable functions (see, e.g., [41]) guarantees the existence of
a polynomial W : Rn → R such that

max
x∈Ω

(cid:107)∇V (x) − ∇W (x)(cid:107)2 ≤ δ.

Letting p(x) = −∇W (x), we get that p ∈ Grad and (cid:107)f − p(cid:107)Ω ≤ δ.

• Case (vi’): If f ∈ Ham. The proof for this case is analogous to Case (vi).

5.2. Approximating a vector ﬁeld while approximately satisfying multiple side
information constraints. It is natural to ask whether Theorem 5.1 can be generalized to
allow for polynomial approximation of vector ﬁelds satisfying multiple side information con-
straints. It turns out that our proof idea of “smoothing by convolution” can be used to show
that the answer is positive if the following three conditions hold: (i) the side information con-

26

Side information S
Interp({(xi, yi)})m
with xi ∈ Ω
for i = 1 . . . , m
Sym(G, σ, ρ)

i=1)

i=1)

Pos({(Pi, Ni)}n
with Pi, Ni ⊆ Ω
for i = 1, . . . , n
Mon({(Pij, Nij)}n
with Pij, Nij ⊆ Ω
for i, j = 1, . . . , n

i,j=1)

Functional LS,Ω(f )

max
i=1,...,m

(cid:107)f (xi) − yi(cid:107)2

max
g∈G

max
i=1,...,n
x∈Ω

|fi(σ(g)x) − (ρ(g)f (x))i|

(cid:26)

(cid:27)

max
i=1,...,n

max

0, max
x∈Pi

−fi(x), max
x∈Ni

fi(x)

(cid:26)

max
i,j=1,...,n

max

0, max
x∈Pij

−

∂fi
∂xj

(x), max
x∈Nij

(cid:27)

(x)

∂fi
∂xj

Inv({Bi}r
i=1) where
Bi := {x | hij(x) ≥ 0
∀j ∈ {1, . . . , mi}} ⊆ Ω
for i = 1, . . . , r

max
i=1...,r

max
x∈Bi
j∈{1,...,mi}
hij (x)=0

max {0, −(cid:104)f (x), ∇hij(x)(cid:105)}

inf
V :Rn→R

max
i=1,...,n
x∈Ω

fi(x) +

∂V
∂xi

(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

inf
H:Rn→R

max
(p,q)∈Ω,
i=1...,n/2

max

fi(p, q) +

∂H
∂qi

(p, q)

,

(cid:12)
(cid:12)
(cid:12)fi+n/2(p, q) − ∂H

∂pi

(p, q)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:111)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Grad

Ham

Table 3: For each side information S, the functional LS,Ω : C ◦
a vector ﬁeld f ∈ C ◦
1 (Ω) is to satisfying S.

1 (Ω) → R quantiﬁes how close

straints are of type Interp, Pos, Mon, Inv, or Sym, (ii) each side information constraint
satisﬁes the assumptions of Theorem 5.1, and (iii) the regions of the space where the ﬁrst four
types of side information constraints are imposed are pairwise nonintersecting. In absence of
condition (iii), the answer is no longer positive as the next example shows.

EXAMPLE 2. Consider the univariate vector ﬁeld f : R → R given by

0
−e− 1
This vector ﬁeld is continuously differentiable over R and satisﬁes the following combination
of side information constraints:

x ≥ 0
x < 0.

f (x) :=

x2

(cid:40)

Interp({(0, 0), (1, 0)}) and Mon({([−1, 1], ∅)}).
(5.11)
In other words, f is nondecreasing on the interval [−1, 1] and satisﬁes f (0) = f (1) = 0.
Yet, the only polynomial vector ﬁeld that satisﬁes the constraints in (5.11) is the identically
zero polynomial. As a result, the vector ﬁeld f cannot be approximated arbitrarily well over
[−1, 1] by polynomial vector ﬁelds that satisfy the side information constraints in (5.11).

To overcome difﬁculties associated with such examples, we introduce the notion of ap-
proximate satisﬁability of side information over a compact set Ω ⊂ Rn. Before we give a
formal deﬁnition of this notion, for each side information constraint S, we present in Table 3
a functional LS,Ω : C ◦
1 (Ω) is to satis-
fying the side information S. One can verify that the functional LS,Ω has the following two
properties: (i) for any vector ﬁeld f ∈ C ◦
(5.12)

1 (Ω) → R that measures how close a vector ﬁeld in C ◦

1 (Ω),

LS,Ω(f ) = 0 if and only if f satisﬁes S,
27

and (ii) for any δ > 0, there exists γ > 0, such that for any two vector ﬁelds f, ˆf ∈ C ◦
(5.13)

(cid:107)f − ˆf (cid:107)Ω ≤ γ and max
x∈Ω,
i,j=1,...,n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Indeed, take e.g. S = Inv({Bi}r
It is clear from condition (2.1) that LS,Ω(f ) = 0 if and only if f ∈ Inv({Bi}r
the second property, let δ > 0 be given. If we take

i=1), where Bi := {x ∈ Rn | hij(x) ≥ 0, j = 1, . . . , mi}.
i=1). To verify

≤ γ =⇒ |LS,Ω(f ) − LS,Ω( ˆf )| ≤ δ.

∂ ˆfi
∂xj

∂fi
∂xj

1 (Ω),

(x) −

(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γ = δ






max
x∈Ω,
i=1...,r
j=1...,mi

(cid:107)∇hij(x)(cid:107)






−1

,

it is easy to see that for any two vector ﬁelds f, ˆf ∈ C ◦
1 (Ω) satisfying (cid:107)f − ˆf (cid:107)Ω ≤ γ, we must
have |LS(f ) − LS( ˆf )| ≤ δ. Indeed, let i ∈ {1, . . . , r} and x ∈ Bi be such that hij(x) = 0
for some j ∈ {1, . . . , mi}. Then, the Cauchy-Schwarz inequality and our choice of γ give
|(cid:104)f (x), ∇hij(x)(cid:105) − (cid:104) ˆf (x), ∇hij(x)(cid:105)| ≤ (cid:107)f − ˆf (cid:107)Ω(cid:107)∇hij(x)(cid:107) ≤ δ.

The desired result follows by taking the maximum over i, j, and x.

DEFINITION 1 (δ-satisﬁability). Let Ω ⊂ Rn be a compact set and consider any side
information S presented in Table 3 together with its corresponding functional LS,Ω. For a
scalar δ > 0, we say that a vector ﬁeld f ∈ C ◦

1 (Ω) δ-satisﬁes S if LS,Ω(f ) ≤ δ.

From a practical standpoint, for small values of δ, it is reasonable to substitute the re-
quirement of exact satisﬁability of side information for δ-satisﬁability. This is especially
true since most optimization solvers return an approximate numerical solution anyway. The
following theorem shows that polynomial vector ﬁelds can approximate any continuously-
differentiable vector ﬁeld f and satisfy the same side information as f up to an arbitrarily
small error tolerance δ. It also shows that in the context of learning a vector ﬁeld from trajec-
tory data, one can always impose δ-satisﬁability on a candidate polynomial vector ﬁeld via
semideﬁnite programming.

THEOREM 5.5. For any compact set Ω ⊂ Rn, time horizon T > 0, desired approxi-
mation accuracy ε > 0, desired side information satisﬁability accuracy δ > 0, and for any
vector ﬁeld f ∈ C ◦
1 (Ω) that satisﬁes any combination of the side information constraints from
the ﬁrst column of Table 3, there exists a polynomial vector ﬁeld p : Rn → Rn that δ-satisﬁes
the same combination of side information as f and has dΩ,T (f, p) ≤ ε.

Moreover, if the set Ω, the sets Pi, Ni in the deﬁnition of Pos({(Pi, Ni)}n

i=1), the
sets Pij, Nij in the deﬁnition of Mon({Pij, Nij}n
i,j=1), and the sets Bi in the deﬁnition
of Inv({Bi}r
i=1)) are all closed basic semialgebraic and their deﬁning polynomials satisfy
the Archimedian property, then δ-satisﬁability of all side information constraints by the poly-
nomial vector ﬁeld p has a sum of squares certiﬁcate.

Proof. Let f ∈ C ◦

1 (Ω) satisfy a list S1, . . . , Sk of side information constraints from the
ﬁrst column of Table 3, and let the scalars T , ε, δ > 0 be ﬁxed. A generalization of the
Stone-Weierstrass approximation theorem to continuously-differentiable functions (see, e.g.,
[41]) guarantees that for any γ > 0, there exists a polynomial pγ : Rn → Rn such that

(5.14)

(cid:107)f − pγ(cid:107)Ω ≤ γ and max
x∈Ω
i,j=1,...,n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂fi
∂xj

(x) −

∂pγ
i
∂xj

(cid:12)
(cid:12)
(x)
(cid:12)
(cid:12)

≤ γ.

For the rest of this paragraph, for any γ > 0, we ﬁx an (arbitrary) choice for the polyno-
mial pγ. Since for each i ∈ {1, . . . , k}, the functional LSi,Ω satisﬁes (5.13), there exists a
28

scalar γi > 0 for which LSi,Ω(pγ) ≤ δ/2 for any γ ∈ (0, γi]. If we let

¯γ := min{ε/ max{T eLT , 1 + LT eLT }, γ1, . . . , γk},
where L > 0 is any scalar for which f is L-Lipschitz over Ω, then the polynomial p := p¯γ
δ/2-satisﬁes S1, . . . , Sk (and hence δ-satisﬁes S1, . . . , Sk), and because of Proposition 5.3,
also satisﬁes dΩ,T (f, p) ≤ ε.

To prove the second claim of the theorem, observe that for each (cid:96) ∈ {1, . . . , k}, the fact

that p δ/2-satisﬁes S(cid:96) implies the following inequalities:8

• If S(cid:96) = Sym(G, σ, ρ),

pi(σ(g)x) − (ρ(g)p(x))i + δ > 0 and (ρ(g)p(x))i − pi(σ(g)x) + δ > 0 ∀x ∈ Ω,

for g ∈ G and i = 1, . . . , n;
i=1),

• If S(cid:96) = Pos({(Pi, Ni)}n

pi(x) + δ > 0 ∀x ∈ Pi and − pi(x) + δ > 0 ∀x ∈ Ni,

for i = 1, . . . , n;

• If S(cid:96) = Mon({(Pij, Nij)}n
∂pi
∂xj

i,j=1),

(x) + δ > 0 ∀x ∈ Pij, and −

∂pi
∂xj

(x) + δ > 0 ∀x ∈ Nij,

for i, j = 1, . . . , n;
• If S(cid:96) = Inv({Bi}r

i=1),

(cid:104)p(x), ∇hij(x)(cid:105) + δ > 0 ∀x ∈ Bi ∩ {x ∈ Rn | hij(x) = 0},

for i = 1 . . . , r, j = 1, . . . , mi;

• If S(cid:96) = Grad,

pi(x) +

(x) + δ > 0 and − pi(x) −

(x) + δ > 0 ∀x ∈ Ω,

∂V
∂xi

∂V
∂xi

for i = 1 . . . , n, where V : Rn → R is a polynomial function. (The fact that V can
be taken to be a polynomial function follows from an other application of the general-
ization of Stone-Weierstrass approximation theorem that was used at the beginning of
this proof.)

Observe that each of the above inequalities states that a certain polynomial is positive
over a certain closed basic semialgebraic set whose deﬁning polynomials satisfy the Archi-
median property by assumption. Therefore, by Putinar’s Positivstellesatz (Theorem 3.1),
there exists a nonnegative integer d such that each one of these inequalities has a degree-d
sos-certiﬁcate (see (3.3)). Therefore, δ-satisﬁability of each side information S1, . . . , Sk by
the vector ﬁeld p can be proven by a sum of squares certiﬁcate.

An interesting corollary of Theorem 5.5 is that given a list of side information that an
unknown vector ﬁeld f is known to satisfy, and a training dataset that is large enough, one
can ﬁnd a polynomial vector ﬁeld p, which approximately satisﬁes the same combination of
side information and is close to f , by solving a ﬁnite sequence of semideﬁnite programs.
More formally, consider a compact set Ω ⊂ Rn, time horizon T > 0, desired approximation
accuracy ε > 0, desired side information satisﬁability accuracy δ > 0, and a vector ﬁeld
f ∈ C ◦
1 (Ω) that satisﬁes any combination of the side information constraints from the ﬁrst
column of Table 3. Let

L := sup
x∈Ω

(cid:107)Jf (x)(cid:107),

8We exclude the case S(cid:96) = Interp({(xi, yi)}m

i=1) because verifying δ-satisﬁability is trivial there, and the

case S(cid:96) = Ham because the argument for it is similar to that of S(cid:96) = Grad.

29

where Jf is the Jacobian of the vector ﬁeld f . Let D be any ﬁnite subset of Ω that satisﬁes

sup
x∈Ω

min
xi∈D

(cid:107)x − xi(cid:107) ≤

ε
4(L + 1) max{T eLT , 1 + LT eLT }

.

Furthermore, assume that the set Ω, the sets Pi, Ni in the deﬁnition of Pos({(Pi, Ni)}n
i=1),
the sets Pij, Nij in the deﬁnition of Mon({Pij, Nij}n
i,j=1), and the sets Bi in the deﬁnition
of Inv({Bi}r
i=1)) are all closed basic semialgebraic and their deﬁning polynomials satisfy
the Archimedian property. Consider the sequence of semideﬁnite programs SDPd indexed by
a nonnegative integer d:9

min
p∈Pd

max
xi∈D

(cid:107)p(xi) − f (xi)(cid:107)

s.t. p has a degree-d sos-certiﬁcate

of the inequality (cid:107)Jp(x)(cid:107) ≤ L + 1 ∀x ∈ Ω
as well as of δ-satisﬁability of the side information of f .
We claim that for d large enough, SDPd is feasible with optimal value at most

ε(cid:48) :=

ε
4 max{T eLT , 1 + LT eLT }

,

and that any of its feasible solutions ˜p with objective value at most 2ε(cid:48) would δ-satisfy the
same side information as f , and have dΩ,T (f, ˜p) ≤ ε. We know from Theorem 5.5 (and
its proof) that for d large enough, there exists a polynomial vector ﬁeld ˆp of degree d that
δ-satisﬁes the same side informationas f with degree-d sos-certiﬁcates of δ-satisfaction and
has

(5.15)

max
x∈Ω
i,j=1,...,n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂fi
∂xj

(x) −

∂ ˆpi
∂xj

(cid:12)
(cid:12)
(x)
(cid:12)
(cid:12)

≤

1
2n

and dΩ,T (f, ˆp) ≤ ε(cid:48).

In particular, ˆp is feasible to SDPd (because of the ﬁrst inequality) and has objective value no
larger than ε(cid:48) since (cid:107)ˆp(x) − f (x)(cid:107) ≤ dΩ,T (f, ˆp) ∀x ∈ Ω.

Let ˜p be a feasible solution to SDPd (for some d ∈ N) with objective value at most 2ε(cid:48).
Then, ˜p δ-satisﬁes the same side information as f by construction. Moreover, for any x ∈ Ω
and xi ∈ D, the triangular inequality gives

(cid:107)˜p(x) − f (x)(cid:107) ≤ (cid:107)˜p(x) − ˜p(xi)(cid:107) + (cid:107)˜p(xi) − f (xi)(cid:107) + (cid:107)f (xi) − f (x)(cid:107).
Observe that (cid:107)˜p(xi) − f (xi)(cid:107) ≤ 2ε(cid:48). Moreover, since ˜p and f are L + 1 Lipschitz, we have
(cid:107)˜p(x) − ˜p(xi)(cid:107) ≤ (L + 1)(cid:107)x − xi(cid:107) and (cid:107)f (x) − f (xi)(cid:107) ≤ (L + 1)(cid:107)x − xi(cid:107).

Therefore,

It follows that

(cid:107)˜p(x) − f (x)(cid:107) ≤ 2(L + 1)(cid:107)x − xi(cid:107) + 2ε(cid:48) ∀xi ∈ D, ∀x ∈ Ω.

(cid:107)˜p(x) − f (x)(cid:107) ≤

ε
max{T eLT , 1 + LT eLT }

∀x ∈ Ω,

Proposition 5.3 therefore gives dΩ,T (f, ˜p) ≤ ε.

We remark that the SDP construction depends on the possibly unknown Lipschitz con-
stant L of the vector ﬁeld f . However, any upper bound on L would sufﬁce for the SDP
construction and the convergence guarantee established above. Note that without an upper
bound on L to be imposed on the Lipschitz constant of our candidate vector ﬁelds, no algo-
rithm could learn the vector ﬁeld f based on the values that it takes on a ﬁnite dataset D alone.
Indeed, for any ﬁnite training dataset D, there always exists another vector ﬁeld ˜f ∈ C ◦
1 (Ω)

9Recall that Pd denotes the set of polynomial vector ﬁelds of degree d, and see (3.3) for the notion of a degree-d

sos-certiﬁcate.

30

that is equal to f on D, and arbitrarily far from f outside of D.

6. Discussion and future research directions. From a computational perspective,
our approach to learning dynamical systems from trajectory data while leveraging side infor-
mation relies on convex optimization. If the side information of interest is Interp, Sym,
Grad, or Ham, then our approach leads to a least-squares problem, and thus can be imple-
mented at large scale. For side information constraints of Pos, Mon, or Inv, our approach
requires solutions to semideﬁnite programs. Classical interior-point methods for SDP come
with polynomial-time solvability guarantees (see e.g. [53]), and in practice scale to problems
of moderate sizes. In the ﬁeld of dynamical systems, many applications of interest involve a
limited number of state variables, and therefore our approach to learning such systems leads
to semideﬁnite programs that off-the-shelf interior-point method solvers can readily handle.
For instance, each semideﬁnite program that was considered in the numerical applications
of Section 4 was solved in under a second on a standard personal machine by the solver
MOSEK [1]. An active and exciting area of research is focused on developing algorithms
for large-scale semideﬁnite programs (see e.g. [35, 17]), and we believe that this effort can
extend our learning approach to large-scale dynamical systems.

The size of our semideﬁnite programs is also affected by the degree of our candidate
polynomial vector ﬁeld and the degrees of the sos multipliers in (3.3) that result from the
application of Putinar’s Positivstellesatz. In practice, these degrees can be chosen using a
statistical model validation technique, such as cross validation. For example, one can split
the available data into training and testing, use the training data for learning a vector ﬁeld of
degree d ∈ {1, 2, . . .}, and choose the degree that achieves the lowest generalization error on
the test data.

These techniques take into account the fact that lower degrees can sometimes have a
model regularization effect and lead to better generalization on unobserved parts of the state
space.

We end by mentioning some questions that are left for future research.

• While the framework presented in this paper deals with continuous-time dynamical sys-
tems, we believe that most of the ideas could be extended to the discrete-time setting.
It would be interesting to see how the deﬁnitions of side information, the approximation
results, and the computational aspects contrast with the continuous-time case. Extending
our framework to the problems of learning partial differential equations and stochastic
differential equations with side information would also be interesting research directions.
• We have shown that for any δ > 0, polynomial vector ﬁelds can approximate to arbitrary
accuracy any vector ﬁeld f ∈ C ◦
1 (Ω) while δ-satisfying any list of side information that f
is known to satisfy. Even though from a practical standpoint, δ-satisﬁability is sufﬁcient
(when δ is small), it is an interesting mathematical question in approximation theory to
see which combinations of side information can be imposed exactly on polynomial vector
ﬁelds while preserving an arbitrarily tight approximation guarantee to functions in C ◦
1 (Ω).
• We have presented a list of six types of side information that arise naturally in many appli-
cations and that lead to a convex formulation (meaning that a convex combination of two
vector ﬁelds that satisfy any one of the six side information constraints will also satisfy the
same side information constraint). There are of course other interesting side information
constraints that do not lead to a convex formulation. Examples include the knowledge
that an equilibirum point is locally or globally stable/stabilizable, and the knowledge that
trajectories of the system starting in a set A ⊆ Rn avoid/reach another set B ⊆ Rn. It
is an interesting research direction to extend our approximation results and our sos-based
approach to handle some of these nonconvex side information constraints.

• Finally, from a statistical and information-theoretic point of view, it is an interesting ques-

31

tion to quantify the beneﬁt of a particular side information constraint in reducing the num-
ber of trajectory observations needed to learn a good approximation of the unknown vector
ﬁeld.

Aknowledgments: The authors are grateful to two anonymous referees, Charles Feffer-
man, Georgina Hall, Frederick Leve, Clancey Rowley, Vikas Sindhwani, and Ufuk Topcu for
insightful questions and comments.

32

Appendix A. Additional numerical experiments.

Description of the experiments. For the ﬁrst and last learning experiments of this pa-
per (Subsections 4.1 and 4.4), we conduct additional numerical tests here to show the effects
of varying the degree of the learned polynomial vector ﬁeld, the noise level, and the number
of trajectories used in learning.

For the ﬁrst (resp. last) learning application, we simulate Nt trajectories starting from ini-
tial conditions picked uniformly at random from the box B = [0, 1]2 (resp. B = [−10, 10]3)
up to time T = 30, and collect 20 noisy samples from each trajectory at times evenly spaced
on the interval [0, T ]. Each sample has the form (x, f (x) + ε), where ε is a 2 × 1 (resp.
3 × 1) Gaussian variable with mean 0 and covariance σ2I, with σ = 10−3, 10−2, 10−1. The
noise terms added to samples are independent from each other. We experiment with learning
polynomial vector ﬁelds p(x) with degree d = 2, 3, 4 either with no side information, or with
the three side information constraints presented in Subsections 4.1 and 4.4.

For each combination of Nt, σ, d, we report the following metrics related to the learned

vector ﬁeld p:
• The training error: The average value over all training sample points (x, f (x) + ε) of

the quantity (cid:107)p(x) − (f (x) + ε)(cid:107).

• The vector ﬁeld test error: The average value of (cid:107)p(x) − f (x)(cid:107), where x runs over a
regular discretization of the box B, where each dimension has size 10. Therefore, the
test set includes 100 points in our two-dimensional example and 1000 points in our three-
dimensional one.

• The trajectory test error: The average value of (cid:107)xf (ti; x0)−xp(ti; x0)(cid:107), where xf (t; x0)
(resp. xp(t; x0)) is the trajectory of f (resp. p) starting from x0. Here, the average is over
10 random choices of x0 from the box B, and 100 scalars ti that form a regular subdivision
of the interval [0, 30].

The value ∞ in our table entries indicates that the trajectory of the learned vector ﬁeld (as
simulated by our ODE solver) is diverging.

Observations. We make some observations on the patterns that arise in the tables below.
As expected, the training error is always higher in presence of the side information because
the underlying optimization problems involve more constraints. However, the test error (both
in the vector ﬁeld and the trajectory sense) is lower in more than 90% of the experiments.
This indicates that side information helps with generalization and overﬁtting to the noise. On
average, the improvement in test error is more pronounced when model complexity is high
(i.e., for higher values of d), the noise level is high, and the number of trajectories is low.
For example, the vector-ﬁeld test error of the polynomial vector ﬁeld of degree 2 learned
from 3 trajectories with noise level σ = 10−3 does not improve when one considers the side
information constraints. However, the vector ﬁeld test error of the polynomial vector ﬁeld
of degree 4 learned from 1 trajectory with noise level σ = 10−1 improves by 3 orders of
magnitudes when the side information constraints are added.

Diffusion of a contagious disease, degree = 2.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes

no

1
2
3
1
2
3
1
2
3

0.001
0.001
0.001
0.008
0.008
0.009
0.08
0.085
0.086

0.001
0.002
0.002
0.009
0.009
0.009
0.087
0.088
0.088

189
0.044
0.018
1.89e+03
0.444
0.179
1.89e+04
4.44
1.78

33

0.068
0.081
0.083
0.071
0.061
0.074
0.075
0.068
0.11

∞
0.004
0.002
∞
0.031
∞
∞
∞
∞

0.008
0.005
0.005
0.012
0.004
0.006
0.02
0.019
0.016

Diffusion of a contagious disease, degree = 3.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes
no

1
2
3
1
2
3
1
2
3

0.001
0.001
0.001
0.008
0.008
0.008
0.079
0.082
0.084

0.001
0.001
0.001
0.008
0.009
0.009
0.084
0.087
0.087

229
21.8
1.65
2.29e+03
218
16.5
2.29e+04
2.18e+03
165

4.22 ∞
0.248 ∞
0.151 ∞
43.4 ∞
0.47 ∞
0.292 ∞
435 ∞
2.64 ∞
0.982 ∞

0.065
0.005
0.004
0.581
0.013
0.007
5.8
0.079
0.023

Diffusion of a contagious disease, degree = 4.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes
no

1
2
3
1
2
3
1
2
3

0.001
0.001
0.001
0.008
0.008
0.008
0.076
0.079
0.081

0.001
0.001
0.001
0.008
0.009
0.009
0.083
0.087
0.087

1.63e+04
2.58e+03
421
1.62e+05
2.58e+04
4.21e+03
1.57e+06
2.58e+05
4.21e+04

21.7 ∞
0.726 ∞
0.575 ∞
191 ∞
1.83 ∞
1.53 ∞
1.92e+03 ∞
8.01 ∞
7.99 ∞

0.092
0.004
0.002
0.822
0.046
0.031
8.22
0.018
0.017

Lorenz, degree = 2.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes

no

1
2
3
1
2
3
1
2
3

0.001
0.001
0.001
0.007
0.008
0.008
0.068
0.079
0.083

0.001
0.001
0.001
0.008
0.009
0.009
0.083
0.086
0.088

0.049
0.002
0.002
0.491
0.02
0.018
4.91
0.202
0.185

0.001
0.001
0.001
0.009
0.007
0.007
0.09
0.074
0.071

0.028
0.002
0.003
0.285
0.018
0.025
3.01
0.185
0.251

0.002
0.001
0.001
0.018
0.008
0.005
0.17
0.064
0.052

Lorenz, degree = 3.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes

no

1
2
3
1
2
3
1
2
3

0
0.001
0.001
0.003
0.006
0.008
0.032
0.064
0.075

0
0.001
0.001
0.005
0.007
0.008
0.057
0.071
0.078

110
0.046
0.014
192
0.461
0.138
1.47e+03
4.61
1.38

10.9
0.008
0.044
10.9
0.063
0.035
54.9
0.963
0.352

∞
0.016
0.006
57.9
0.157
0.058
237
1.59
0.576

13.7
0.005
0.034
14.2
0.05
0.019
23.6
0.461
0.279

34

Lorenz, degree = 4.

noise

0.001

0.010

0.100

side information used? →
# trajectories ↓

training error

no

yes

test error (vector ﬁeld)
yes

no

test error (trajectories)
yes

no

1
2
3
1
2
3
1
2
3

0
0
0.001
0
0.003
0.006
0
0.028
0.061

0
0
0
0.001
0.004
0.005
0.031
0.057
0.069

167
46.1
0.283
858
461
2.83
8.92e+03
4.61e+03
28.3

532
0.479
0.205
6.22e+03
1.71
0.407
2.55e+03
6.21
2.47

82.7
51.8
0.047
∞
∞
0.468
∞
∞
4.84

94.4
0.832
0.421
286
1.97
0.179
120
5.15
0.939

REFERENCES

[1] Introducing the MOSEK optimization suite. 2018. URL https://docs.mosek.com/8.1/intro/index.html.
[2] A. A. Ahmadi and B. El Khadir. Learning dynamical systems with side information (short version).

In
Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120, pages 718–727.
Proceedings of Machine Learning Research, 2020.

[3] M. Ahmadi, U. Topcu, and C. Rowley. Control-oriented learning of Lagrangian and Hamiltonian systems. In

Annual American Control Conference, pages 520–525, 2018.

[4] R. M. Anderson, B. Anderson, and R. M. May.

Infectious Diseases of Humans: Dynamics and Control.

Oxford University Press, 1992.

[5] K. J. ˚Astr¨om and P. Eykhoff. System identiﬁcation—a survey. Automatica, 7(2):123–162, 1971.
[6] R. Bellman. The stability of solutions of linear differential equations. Duke Mathematical Journal, 10(4):643–

647, 1943.

[7] A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engi-

neering Applications, volume 2. Siam, 2001.

[8] F. Blanchini. Set invariance in control. Automatica, 35(11):1747–1767, 1999.
[9] G. Blekherman, P. A. Parrilo, and R. Thomas. Semideﬁnite Optimization and Convex Algebraic Geometry.

SIAM Series on Optimization, 2013.

[10] B. Borchers. CSDP, a C library for semideﬁnite programming. Optimization Methods and Software, 11(1-

4):613–623, 1999.

[11] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities In System And Control

Theory. SIAM, 1994.

[12] S. L. Brunton and J. N. Kutz. Data-Driven Science and Engineering: Machine learning, Dynamical Systems,

and Control. Cambridge University Press, 2019.

[13] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identiﬁcation
of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932–3937,
2016.

[14] C.-A. Cheng and H.-P. Huang. Learn the Lagrangian: A vector-valued RKHS approach to identifying La-

grangian systems. IEEE Transactions on Cybernetics, 46(12):3247–3258, 2015.

[15] P. Comon, G. Golub, L.-H. Lim, and B. Mourrain. Symmetric tensors and symmetric tensor rank. SIAM

Journal on Matrix Analysis and Applications, 30(3):1254–1279, 2008.

[16] M. Curmei and G. Hall. Shape-constrained regression using sum of squares polynomials. Preprint available

at arXiv:2004.03853, 2020.

[17] L. Ding, A. Yurtsever, V. Cevher, J. A. Tropp, and M. Udell. An optimal-storage approach to semideﬁnite

programming using approximate complementarity. Preprint available at arXiv:1902.03373, 2019.

[18] B. El Khadir, J. Varley, and V. Sindhwani. Teleoperator imitation with continuous-time safety. Robotics

Science and Systems, 2019.

[19] D. J. Foster, A. Rakhlin, and T. Sarkar. Learning nonlinear dynamical systems from a single trajectory. Preprint

available at arXiv:2004.14681, 2020.

[20] W. Fulton and J. Harris. Representation Theory: A First Course, volume 129. Springer Science & Business

Media, 2013.

[21] S. Greydanus, M. Dzamba, and J. Yosinski. Hamiltonian neural networks. In Advances in Neural Information

Processing Systems, pages 3240–3249, 2019.

[22] G. Hall. Optimization over nonnegative and convex polynomials with and without semideﬁnite programming.

PhD thesis, Princeton University, 2018.

[23] G. Hall. Engineering and business applications of sum of squares polynomials. Preprint available at

arXiv:1906.07961, 2019.

[24] D. Hart, E. Shochat, and Z. Agur. The growth law of primary breast cancer as inferred from mammography

35

screening trials data. British Journal of Cancer, 78(3):382–387, 1998.

[25] J. W. Helton and J. Nie. Semideﬁnite representation of convex sets. Mathematical Programming, 122(1):21–

64, 2010.

[26] K. J. Keesman and K. J. Keesman. System Identiﬁcation: An Introduction, volume 2. Springer, 2011.
[27] H. K. Khalil. Nonlinear Systems. Prentice-Hall, 2002.
[28] S. M. Khansari-Zadeh and A. Billard. Learning stable nonlinear dynamical systems with Gaussian mixture

models. IEEE Transactions on Robotics, 27(5):943–957, 2011.

[29] J. Z. Kolter and G. Manek. Learning stable deep dynamics models.

In Advances in Neural Information

Processing Systems, pages 11126–11134, 2019.

[30] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Opti-

mization, 11(3):796–817, 2001.

[31] J. B. Lasserre. Moments, Positive Polynomials And Their Applications, volume 1. World Scientiﬁc, 2010.
[32] M. Laurent. Sums of squares, moment matrices and optimization over polynomials. In Emerging applications

of algebraic geometry, pages 157–270. Springer, 2009.

[33] J. L¨ofberg. Yalmip: a toolbox for modeling and optimization in matlab.

In Proceedings of the CACSD

Conference, Taipei, Taiwan, 2004.

[34] F. Luk´acs. Versch¨arfung des ersten Mittelwertsatzes der Integralrechnung f¨ur rationale Polynome. Mathema-

tische Zeitschrift, 2(3):295–305, 1918.

[35] A. Majumdar, G. Hall, and A. A. Ahmadi. A survey of recent scalability improvements for semideﬁ-
nite programming with applications in machine learning, control, and robotics. Preprint available at
arXiv:1908.05209, 2019.

[36] K. G. Murty and S. N. Kabadi. Some NP-complete problems in quadratic and nonlinear programming. Math-

ematical Programming, 39:117–129, 1985.

[37] M. Nagumo.

¨Uber die Lage der Integralkurven gew¨ohnlicher Differentialgleichungen. Proceedings of the

Physico-Mathematical Society of Japan., 24:551–559, 1942.

[38] P. A. Parrilo. Structured semideﬁnite programs and semialgebraic geometry methods in robustness and opti-

mization. PhD thesis, California Institute of Technology, May 2000.

[39] P. A. Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Mathematical Programming,

96(2, Ser. B):293–320, 2003.

[40] S. Prajna, A. Papachristodoulou, and P. A. Parrilo. SOSTOOLS: Sum of squares optimization toolbox for
MATLAB, 2002. Available from http://www.cds.caltech.edu/sostools and http://www.mit.edu/∼parrilo/
sostools.

[41] J. B. Prolla and C. S. Guerreiro. An extension of Nachbin’s theorem to differentiable functions on Banach

spaces with the approximation property. Arkiv f¨or Matematik, 14(1-2):251, 1976.

[42] M. Putinar. Positive polynomials on compact semi-algebraic sets. Indiana University Mathematics Journal,

42(3):969–984, 1993.

[43] R. Sachs, L. Hlatky, and P. Hahnfeldt. Simple ODE models of tumor growth and anti-angiogenic or radiation

treatment. Mathematical and Computer Modelling, 33(12-13):1297–1305, 2001.

[44] H. Schaeffer, G. Tran, R. Ward, and L. Zhang. Extracting structured dynamical systems using sparse opti-
mization with very few samples. SIAM Journal on Multiscale Modeling & Simulation, 18(4):1435–1461,
2020.

[45] V. Sindhwani, S. Tu, and M. Khansari. Learning contracting vector ﬁelds for stable imitation learning. Preprint

available at arXiv:1804.04878, 2018.

[46] S. Singh, S. M. Richards, V. Sindhwani, J.-J. E. Slotine, and M. Pavone. Learning stabilizable nonlinear
dynamics with contraction-based regularization. Preprint available at arXiv:1907.13122, 2019.
[47] S. Singh, V. Sindhwani, J.-J. Slotine, and M. Pavone. Learning stabilizable dynamical systems via control

contraction metrics. In Workshop on Algorithmic Foundations of Robotics, 2018.

[48] P. Skehan. On the normality of growth dynamics of neoplasms in vivo: a data base analysis. Growth,

50(4):496—515, 1986.

[49] H. L. Smith. Monotone Dynamical Systems: An Introduction to the Theory of Competitive and Cooperative

Systems. Number 41. American Mathematical Soc., 2008.

[50] C. Sparrow. The Lorenz Equations: Bifurcations, Chaos, and Strange Attractors, volume 41. Springer Science

& Business Media, 2012.

[51] M. H. Stone. The generalized Weierstrass approximation theorem. Mathematics Magazine, 21(5):237–254,

1948.

[52] K. C. Toh, R. H. T¨ut¨unc¨u, and M. J. Todd. SDPT3 - a MATLAB software package for semideﬁnite-quadratic-

linear programming. URL http://www.math.cmu.edu/∼reha/sdpt3.html.

[53] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, 1996.
[54] T. Weisser, B. Legat, C. Coey, L. Kapelevich, and J. P. Vielma. Polynomial and moment optimization in Julia

and JuMP. In JuliaCon, 2019.

[55] Y. Yang, K. Caluwaerts, A. Iscen, T. Zhang, J. Tan, and V. Sindhwani. Data efﬁcient reinforcement learning

for legged robots. In Conference on Robot Learning, pages 1–10, 2020.

36

