1
2
0
2

t
c
O
4
2

]

G
L
.
s
c
[

1
v
5
8
4
2
1
.
0
1
1
2
:
v
i
X
r
a

Scaling Neural Program Synthesis with
Distribution-based Search

Nathana¨el Fijalkow1,2, Guillaume Lagarde1, Th´eo Matricon1,
Kevin Ellis3, Pierre Ohlmann4, and Akarsh Potta5

1CNRS, LaBRI and Universit´e de Bordeaux, France
2The Alan Turing Institute of data science, United Kingdom
3Cornell University, United States
4University of Paris, France
5Indian Institute of Technology Bombay, India

Abstract

We consider the problem of automatically constructing computer pro-
grams from input-output examples. We investigate how to augment prob-
abilistic and neural program synthesis methods with new search algo-
rithms, proposing a framework called distribution-based search. Within
this framework, we introduce two new search algorithms: Heap Search,
an enumerative method, and SQRT Sampling, a probabilistic method.
We prove certain optimality guarantees for both methods, show how they
integrate with probabilistic and neural techniques, and demonstrate how
they can operate at scale across parallel compute environments. Collec-
tively these ﬁndings oﬀer theoretical and applied studies of search algo-
rithms for program synthesis that integrate with recent developments in
machine-learned program synthesizers.

1

Introduction

Writing software is tedious, error-prone, and accessible only to a small share of
the population – yet coding grows increasingly important as the digital world
plays larger and larger roles in peoples’ lives. Program synthesis seeks to make
coding more reliable and accessible by developing methods for automatically
constructing programs Gulwani et al. (2017). For example, the FlashFill sys-
tem Gulwani et al. (2017) in Microsoft Excel makes coding more accessible by
allowing nontechnical users to synthesize spreadsheet programs by giving input-
output examples, while the TF-coder system Shi et al. (2020a) seeks to make
coding neural networks more reliable by synthesizing TensorFlow code from
input-outputs. Where these systems have been most successful is when they
pair a specialized domain-speciﬁc language (DSL) to a domain-speciﬁc search
algorithm for synthesizing programs in that DSL. A recent trend – both in
industry Kalyan et al. (2018) and academia Devlin et al. (2017) – is to em-
ploy machine learning methods to learn to quickly search for a program in the
DSL Balog et al. (2017); Devlin et al. (2017); Lee et al. (2018); Zhang et al.

1

 
 
 
 
 
 
(2018); Polosukhin and Skidanov (2018); Kalyan et al. (2018); Zohar and Wolf
(2018); Chen et al. (2018). Many such recent works have explored engineering
better neural networks for guiding program search, eﬀectively by training the
network to act as a language model over source code that conditions on input-
outputs Polosukhin and Skidanov (2018). Here, we ‘pop up’ a level and instead
ask: given a neural net that probabilistically generates source code, how can we
most eﬃciently deploy that model in order to ﬁnd a program consistent with
some input-outputs? This concern arises because program synthesis requires
solving a hard combinatorial search problem (exploring a possibly inﬁnite space
of programs), so taming this combinatorial explosion makes the diﬀerence be-
tween a practically useful system, and a system which cannot scale to anything
but the most trivial of programs.

At a high-level the approaches we develop in this work follow a 2-stage
pipeline: in the ﬁrst stage a learned model predicts probabilistic weights, and in
the second stage a symbolic search algorithm uses those weights to explore the
space of source code. Our contributions target the second stage of this pipeline,
and we focus on theoretical analysis of sampling-based search algorithms, new
search algorithms based on neurally-informed enumeration, and empirical eval-
uations showing that recent neural program synthesizers can compose well with
our methods.

This 2-stage pipelined approach has several beneﬁts. The ﬁrst is that the
cost of querying the neural network is usually very small compared to the cost of
combinatorial search, yet in practice the neural model learns to provide rough-
and-ready probabilistic predictions to guide the search. A second beneﬁt is
that even if the probabilistic predictions are inaccurate, our methods maintain
soundness and completeness (but may take longer to run). Another appeal is
that it can be naturally combined with other classical approaches for program
synthesis.

Our contributions:

• A theoretical framework called distribution-based search for evaluating
and comparing search algorithms in the context of machine-learned pre-
dictions.

• Two new search algorithms: Heap Search, an enumerative method, and
SQRT Sampling, a probabilistic method. We prove a number of theo-
retical results about them, in particular that they are both loss optimal.

• A method for running any search algorithm across parallel compute envi-

ronments.

We perform an empirical evaluation of existing and new search algorithms, show-
ing how the new methods integrate with probabilistic and neural techniques.

2 Distribution-based search

We work within the syntax guided program synthesis (SyGuS) framework in-
troduced by Alur et al. (2013), see also Alur et al. (2018). In this setting, the
DSL is given by a set of primitives together with their (possibly polymorphic)
types and semantics.

2

Figure 1: Pipeline for neural predictions for syntax guided program synthesis.

We describe the machine learning pipeline for program synthesis, illustrated

in Figure 1 on a toy DSL describing integer list manipulating programs.

The compilation phase constructs a context-free grammar (CFG) from the
DSL together with a set of syntactic constraints. The CFG may incorporate
important information about the program being generated, such as the n last
primitives (encompassing n-gram models) or semantic information (e.g. non-
zero integer, sorted list).

A prediction model (typically a neural network) takes as inputs a set of
I/O and outputs a probabilistic labelling of the CFG, inducing a probabilistic
context-free grammar (PCFG). The network is trained so that most likely pro-
grams (with respect to the PCFG) are the most likely to be solutions, meaning
map the inputs to corresponding outputs.

We refer to Appendix A for an in-depth technical discussion on program
representations and on the compilation phase.
In this work we focus on the
search phase and start with deﬁning a theoretical framework for analysing search
algorithms.

The PCFG obtained through the predictions of the neural network deﬁnes a
probabilistic distribution D over programs. We make the theoretical assumption
that the program we are looking for is actually sampled from D, and construct
algorithms searching through programs which ﬁnd programs sampled from D
as quickly as possible. Formally, the goal is to minimise the expected number
of programs the algorithm outputs before ﬁnding the right program.

We write A(n) for the nth program chosen by the algorithm A; since A
may be a randomised algorithm A(n) is a random variable. The performance
L(A, D) of the algorithm A, which we call its loss, is the expected number of
tries it makes before ﬁnding x:

L(A, D) = Ex∼D [inf {n ∈ N : A(n) = x}] .

An algorithm A∗ is ‘loss optimal’ if L(A∗, D) = inf A L(A, D). Let us state a
simple fact: an algorithm is loss optimal if it generates each program once and in

3

non-increasing order of probabilities. Depending on D constructing an eﬃcient
loss optimal algorithm may be challenging, pointing to a trade oﬀ between
quantity and quality: is it worth outputting a lot of possibly unlikely programs
quickly, or rather invest more resources into outputting fewer but more likely
programs?

An example. To illustrate the deﬁnitions let us consider the distribution D
over the natural numbers such that D(n) = 1
2n+1 ; it is generated by the following
PCFG:

S →.5 f (S)

; S →.5 x,

when identifying n with the program f n(x). Let us analyse a few algorithms.
• The algorithm A1 enumerates in a deterministic fashion the natural num-
n+1
2n+1 = 2. This

bers starting from 0: A1(n) = n. Then L(A1, D) = (cid:80)
enumeration algorithm A1 is loss optimal.

n≥0

• The algorithm A2 samples the natural numbers using the distribution D.
this is the
1
2n+1 . Then
2n+1
2n+1 = ∞. Hence the naive sampling algorithm us-

For n ≥ 0, the value of E [inf {n(cid:48) : A2(n(cid:48)) = n}] is 2n+1:
expectation of the geometric distribution with parameter
L(A2, D) = (cid:80)
ing D has inﬁnite loss.

n≥0

• The algorithm A3 samples the natural numbers using a distribution that

√

√

we call

D deﬁned1 by

D(n) = 1
√
1+

1
n+1
2

.

2

2

For n ≥ 0, the value of E [inf {n(cid:48) : A3(n(cid:48)) = n}] is (1 +
2)2
the expectation of the geometric distribution with parameter

√

n+1
2 : this is
.

1
√
1+

1
n+1
2

2

2

Then

L(A3, D) = (cid:80)

√
n≥0

√

= (1 +

= (1 +

√

n+1
2

2)2
2n+1

(1+
2) (cid:80)
n≥0
2
2)2 ≈ 5.83.

1
n+1
2

As we will prove in a more general statement (Theorem 2), the algorithm
A3 is loss optimal among sampling algorithms. Suprisingly it is not much
worse than the loss optimal algorithm, yet oﬀers many advantages: it is
much easier to implement, and requires no memory at all. Last but not
least in the case of PCFG it can be implemented using a new probabilistic
labelling of the PCFG inducing D.

The next two sections are devoted to the two natural classes of algorithms
for distribution-based search: enumeration and sampling. We then study how
they can run at scale accross parallel compute environments.

3 Enumerative methods and
the Heap Search algorithm

A number of enumerative methods have been investigated in previous works Menon
et al. (2013); Balog et al. (2017); Feng et al. (2018); Zohar and Wolf (2018). They

1For the normalisation factor, note that (cid:80)

n≥0

1
n+1
2

2

= 1 +

√

2.

4

Figure 2: Illustration of the tree of leftmost derivations.

proceed in a top-down fashion, and can be understood as ways of exploring the
tree of leftmost derivations of the grammar as illustrated in Figure 2.

We present a new eﬃcient and loss optimal algorithm called Heap Search
and following a bottom-up approach. It uses a data structure based on heaps
to eﬃciently enumerate all programs in non-increasing order of the probabilities
in the PCFG.

Let us write D for the distribution induced by a PCFG. For a program x, we
say that x(cid:48) is the ‘successor of x’ if it is the most likely program after x, meaning
D(x) > D(x(cid:48)) and there are no programs x(cid:48)(cid:48) such that D(x) > D(x(cid:48)(cid:48)) > D(x(cid:48)).
For a non-terminal T in the grammar, the ‘successor of x from T ’ is the most
likely program after x among those generated from T . We deﬁne ‘predecessor
of x’ and ‘predecessor of x from T ’ in a similar way.

We create a procedure Query(T, x) which for any program x generated
from a non-terminal T outputs the successor of x from T . Note that once this
is deﬁned, the algorithm performs successive calls to Query(S, x) with S the
initial non-terminal and x the latest generated program, yielding all programs
in non-increasing order of the probabilities.

To explain how Query(T, x) works, we ﬁrst deﬁne the data structure. For
each non-terminal T we have a hash table SuccT which stores the successors
of all previously seen programs generated from T , and a heap HeapT which
contains candidate programs, ordered by non-increasing probability. The key
invariant is the following: the successor of T from x has either already been

5

computed, hence is in SuccT , or is the maximum program in HeapT . This
means that implementing Query(T, x) is very simple:
it checks whether the
successor has already been computed and returns it in that case, and if not
it pops the heap. The diﬃculty is in maintaining the invariant; for this we
need to add a number of candidate programs to the heaps. They are obtained
by substituting one argument from the returned successor, and propagate this
recursively to the corresponding non-terminals.

Theorem 1. The Heap Search algorithm is loss optimal: it enumerates every
program exactly once and in non-increasing order of probabilities.

We refer to Appendix C for a complete description of the algorithm with a

pseudocode, a proof of Theorem 1, and a computational complexity analysis.

Heap Search is related to A∗ from Feng et al. (2018): they are both loss op-
timal algorithms, meaning that they both enumerate programs in non-increasing
order of probabilities. As we will see in the experiments Heap Search is better
in two aspects: it is faster, and it is bottom-up, implying that program evalua-
tions can be computed along with the programs and avoiding evaluating twice
the same (sub)program.

4 Sampling methods and

the SQRT Sampling algorithm

A sampling algorithm takes random samples from a distribution D(cid:48); what is
both a strength and a weakness is that a sampling algorithm is memoryless: a
weakness because the algorithm does not remember the previous draws, which
means that it may draw them again, but also a strength because it uses very
little space and can be very easily implemented.

In the case of sampling algorithms, we identify algorithms with distributions.
The following theorem shows a dichotomy: either there exists a loss optimal
sampling algorithm among sampling algorithms, and then it is characterised as
the ‘square root’ of the distribution, or all sampling algorithms have inﬁnite
loss.
Theorem 2. Let D a distribution over a set X. If (cid:80)
distribution

(cid:112)D(x) < ∞, the

D deﬁned by

x∈X

√

√

D(x) =

(cid:112)D(x)

(cid:80)

y∈X

(cid:112)D(y)

is loss optimal among all sampling algorithms. If (cid:80)
sampling algorithms D(cid:48) we have L(D(cid:48), D) = ∞.

x∈X

(cid:112)D(x) = ∞, for all

Proof. Let D(cid:48) be a distribution. For an element x, the expectation of the number
of tries for D(cid:48) to draw x is
D(cid:48)(x) : this is the expectation of success for the
geometric distribution with parameter D(cid:48)(x). It follows that

1

L(D(cid:48), D) = Ex∼D

(cid:20)

1
D(cid:48)(x)

(cid:21)

=

(cid:88)

x∈X

D(x)
D(cid:48)(x)

.

6

Let us assume that (cid:80)

x∈X

ity we have:

(cid:112)D(x) < ∞. Thanks to Cauchy-Schwarz inequal-

(cid:16)(cid:80)

x∈X

(cid:112)D(x)

(cid:17)2

=

≤

(cid:16)(cid:80)

x∈X

(cid:16)(cid:80)

x∈X

(cid:113) D(x)
D(cid:48)(x)
(cid:17)

D(x)
D(cid:48)(x)

·

(cid:112)D(cid:48)(x)
(cid:32)

(cid:17)2

(cid:33)

(cid:88)

D(cid:48)(x)

x∈X

(cid:124)

(cid:123)(cid:122)
=1

(cid:125)

= (cid:80)

x∈X

D(x)
D(cid:48)(x) .

√

We note that L(
√
L(D(cid:48), D) ≥ L(
and if it is not deﬁned, then for any D(cid:48) we have L(D(cid:48), D) = ∞.

(cid:16)(cid:80)
x∈X
√
D is loss optimal among sampling algorithms,

, so the previous inequality reads

D, D). Thus

(cid:112)D(x)

D, D) =

(cid:17)2

Theorem 2 characterises the loss optimal sampling algorithm, but does not

explain how to implement it. The following result answers that question.

Theorem 3. If D is deﬁned by a PCFG and
eﬀectively construct a PCFG deﬁning

D.

√

√

D is well deﬁned, then we can

√

The PCFG for

D is obtained from the PCFG for D by taking the square
root of each transition probability, and then globally renormalising. Details of
this procedure can be found in Appendix D.

5 Parallel implementations

Harnessing parallel compute environments is necessary for scalable, future-proof
search algorithms, because combinatorial search bottlenecks on compute, and
both the present and likely future of massive compute is a parallel one. Accord-
ingly, we have taken care to design and evaluate extensions of our algorithms
which can metabolize these compute resources through multiprocessing.

We introduce a new algorithm called the grammar splitter, which partitions a
PCFG into a balanced family of k sub-PCFGs. Each of the k threads is assigned
a sub-PCFG and simply runs a search algorithm on it. Two key advantages of
our approach are that any search algorithm can be used in this very simple
parallel architecture, and that the theoretical gain of using k threads is linear
in k. The output of the grammar splitter is illustrated in Figure 3: the white
PCFG is split into 4 sub-PCFGs.

The two crucial properties of the grammar splitter are:

• the space of programs is partitioned into k subspaces. This implies that
the threads do not carry out redundant work and that all programs are
generated,

• the k program subspaces are balanced, meaning that their mass probabil-
ities are (approximately) equal. This implies that all threads contribute
equally to the search eﬀort.

A split is a collection of partial programs, for instance map (* 2) HOLE and
fold + HOLE HOLE, it induces a PCFG. A set of k incomparable splits yields a

7

Figure 3: The grammar splitter: a balanced partition with quality α = .3
1.2.

.25 =

partition of the PCFG. Let us write α for the quality of a partition, deﬁned as
the ratio between the maximum and the minimum probability mass of a split.
We are looking for a balanced partition, i.e. one for which the quality α is close
to 1.

Our algorithm ﬁnds a balanced partition through a hill climbing process: at
each point the algorithm either looks for an improving swap or a reﬁnement. In
the ﬁrst case, the action of an improving swap is to transfer a partial program
In
from one split to another, and its goal is to lower the quality coeﬃcient.
the second case, we consider the partial program with maximal probability
in a split and reﬁne it:
for example map (* 2) HOLE could be replaced by
map (* 2) var0 and map (* 2) (filter HOLE HOLE).

6 Experiments

We study a range of search algorithms – both our new ones and prior work –
across list processing and string manipulation domains, with the goal of answer-
ing the following questions:

• Heap Search and A∗ are both loss optimal enumerative algorithms; be-
yond these theoretical guarantees, how do the two algorithms compare in
practice?

• How eﬀective are our search algorithms for solving complex program syn-

thesis benchmarks using neural guidance?

• How do our algorithms scale with parallel compute?

We use a generic program synthesizer written from scratch in Python (Ap-
pendix A), studying random PCFGs (more controlled) and machine-learned

8

(a) Cumulative probability against time in log-scale

(b) Cumulative probability against number of programs output in log-scale

Figure 4: Comparing all search algorithms on random PCFGs

9

PCFGs (more naturalistic).

We report results on DSLs from DeepCoder Balog et al. (2017) and Dream-
Coder Ellis et al. (2021). Both target the classic program synthesis chal-
lenge of integer list processing programs, but with diﬀerent properties. Deep-
Coder’s DSL is larger and more specialized, with around 40 high-level primitives,
and does not use polymorphic types, while DreamCoder’s is smaller and more
generic, with basic functional programming primitives such as map, fold, un-
fold, car, cons, and cdr, etc., for a total of around 20 primitives. Both DSLs are
compiled into a CFG with minimal syntactic constraints generating programs
of depth 6.

The search algorithms under consideration are:

• Threshold from Menon et al. (2013): iterative-deepening-search, where
the threshold that is iteratively deepened is a bound on program descrip-
tion length (i.e. negative log probability),

• Sort and add from Balog et al. (2017): an inner loop of depth-ﬁrst-
search, with an outer loop that sorts productions by probability and runs
depth-ﬁrst-search with the top k productions for increasing values of k,

• A∗ from Feng et al. (2018): best-ﬁrst-search on the graph of (log proba-

bilities of) tree derivations,

• Beam search from Zhang et al. (2018): breadth-ﬁrst-search with bounded

width that is iteratively increased.

As well as our new algorithms: Heap Search and SQRT Sampling. We refer
to Appendix B for a description of the algorithms and their implementations.
Our implementation of SQRT Sampling uses the Alias method Walker
(1977), which is an eﬃcient data structure for sampling from a categorical dis-
tribution. We associate to each non-terminal an Alias table, reducing the task
of sampling a derivation rule with n choices to sampling uniformly in [1, n] and
in [0, 1].

All algorithms have been reimplemented and optimised in the codebase to
provide a fair and uniform comparison. We also report on parallel implementa-
tions using our grammar splitter.

6.1 Random PCFGs

In this ﬁrst set of experiments we run all search algorithms on random PCFGs
until a timeout, and compare the number of programs they output and the
cumulative probability of all programs output.

To obtain random PCFGs from the CFGs we sample a probabilistic labeling
with an exponential decrease (this is justiﬁed by the fact that machine-learned
PCFGs feature exponential decrease in transition probabilities). In this experi-
ment the initialization cost of each algorithm is ignored. The results presented
here are averaged over 50 samples of random PCFGs, the solid lines represent
the average and a lighter color indicates the standard deviation. Details on the
sampling procedure can be found in Appendix F.

Figure 4 shows the results for all algorithms in a non-parallel implementa-
tion. On the lhs we see that Heap Search (almost always) has the highest
cumulative probability against both time and number of distinct programs. Note

10

Figure 5: Comparing Heap Search and A∗

that since A∗ and Heap Search enumerate the same programs in the same or-
der they produce the same curve in the rhs of Figure 4 so we did not include
A∗.

To compare A∗ and Heap Search we refer to Figure 5, showing that Heap
Search generates 2.35 times more programs than A∗, consistently over time.
The larger variations for A∗ are due to the manipulation of a single heap of
growing size, requiring frequent memory reallocations. The diﬀerence in perfor-
mance can be explained by the fact that A∗ uses a single heap for storing past
computations, while Heap Search distributes this information in a family of
connected heaps and hash tables.

We then turn to parallel implementation and perform the same experiments
using a variable number of CPUs for Heap Search and SQRT Sampling using
the grammar splitter. We do not report on a baseline parallel implementation of
SQRT Sampling which would simply sample using the same PCFG on multiple
CPUs. Indeed this naive approach performs poorly in comparison, since thanks
to the grammar splitter two CPUs cannot generate the same program.

The results are shown in Figure 6, where we count programs with repetitions.
We see that for SQRT Sampling the scale-up is linear, and it is mostly linear
for Heap Search with an acceleration from the 0.2s mark. This acceleration
can be explained in two ways: ﬁrst, each sub-PCFG is shallower since it is split
thus it is faster to enumerate program from it, second, once all the successors
have been computed Heap Search is a simple lookup table. At the end of the
experiment, SQRT Sampling has generated 2.8 times more programs with 6
CPUs than with 2 CPUs, whereas Heap Search has generated 7.6 times more
programs with 6 CPUs than with 2 CPUs.

11

Figure 6: Parallel implementations of Heap Search and SQRT Sampling
using the grammar splitter

This experiment suggests that the grammar splitter enables us to scale our

search on multiple CPUs with a linear speed up in the number of CPUs.

6.2 Machine-learned PCFGs

In this second set of experiments we consider the benchmark suites of hand-
picked problems and sets of I/O. We extracted 218 problems from DreamCoder’s
dataset (Ellis et al., 2021). (The experiments can be easily replicated on Deep-
Coder’s dataset (Balog et al., 2017) but we do not report on the results here.)
We train a neural network to make predictions from a set of I/O. Our neural
network is composed of a one layer GRU Cho et al. (2014) and a 3-layer MLP
with sigmoid activation functions, and trained on synthetic data generated from
the DSL. The details of the architecture and the training can be found in Ap-
pendix F. Our network architecture induces some restrictions, for instance the
types of the programs must be int list -> int list; we removed tasks that
did not ﬁt our restrictions and obtained a ﬁltered dataset of 137 tasks. For
each task we run every search algorithm on the PCFG induced by the neural
predictions with a timeout of 100s and a maximum of 1M programs. Unlike in
the previous experiments the initialization costs of algorithms are not ignored.
Figure 7 shows the number of tasks solved within a time budget. Heap
Search solves the largest number of tasks for any time budget, and in particular
97 tasks out of 137 before timeout. The comparison between Threshold and
A∗ is insightful: A∗ solves a bit more tasks than Threshold (85 vs 83) but in
twice the time. SQRT Sampling performs just a bit worse than A∗ despite

12

Figure 7: Comparing all search algorithms on the DreamCoder reduced dataset
with machine-learned PCFGs

Table 1: Number of programs generated

Algorithm

Number of programs generated

Heap Search
Threshold
DFS
SQRT Sampling
A∗

38735 prog/s
25381 prog/s
20281 prog/s
14020 prog/s
6071 prog/s

being a sampling algorithm.

Table 1 shows for each algorithm how many programs were generated per
second. Recall that Heap Search and A∗ generate the same programs in
the same order. Overall in these experiments Heap Search is 6 times faster
than A∗.

Since Heap Search follows a bottom-up approach we save on program
evaluations in two ways: partial programs are evaluated along the search, and
the results are cached. On the other hand A∗ is a top-down enumeration method
so every new program has to be evaluated from scratch.

It is interesting to compare the rates of SQRT Sampling and A∗: although
SQRT Sampling generates over two times more programs, their overall per-
formances are similar. This can be explained in two ways: SQRT Sampling
may sample the same programs many times, while A∗ enumerates each program
once and starts with the most promising ones according to the predictions.

13

Finally, we want to see how Heap Search improves with the quality of the
predictions. According to the properties of Heap Search, we expect that the
better the predictions the faster tasks are solved since Heap Search is loss
optimal. In order to show this, we ran Heap Search on the reduced Dream-
Coder dataset with a timeout of 5 minutes and kept the tasks where a solution
was found since some tasks cannot be solved with our DSL. Then we trained a
neural network on this new reduced set with the solutions found. At diﬀerent
epochs of the training we checked how fast and how many tasks Heap Search
could solve with the predictions of the network with a timeout of 30 seconds and
a limit of 1M programs. We plot on Figure 8 the number of tasks solved with
respect to total time used by Heap Search after diﬀerent number of training
epochs with the uniform PCFG as a baseline. We clearly observe the expected
outcome, as the number of training epochs grows and the neural networks learn
to better predict the solutions, Heap Search dramatically decreases the time
required to solve the tasks. While this may be true for every algorithm, loss
optimal algorithms like Heap Search beneﬁt the most from it.

Figure 8: Evolution of time and tasks solved by Heap Search at diﬀerent
epochs of the training of a machine-learning PCFG predictor on a reduced
DreamCoder dataset

14

7 Discussion

7.1 Related work

The idea of guiding program search via probabilities is an old one Solomonoﬀ
(1989) but which recently has fast become a standard practice in the AI and pro-
gram synthesis communities. To the best of our knowledge, practical program
synthesizers that learn to use probabilistic predictions originated in (Menon
et al., 2013), which was ﬁrst extended with deep learning in (Balog et al., 2017),
and such methods are now winning program synthesis competitions Lee et al.
(2018). To ﬁrst approximation, such recent progress has drawn on advances in
neural network architecture: e.g., early learned FlashFill-like systems Parisotto
et al. (2017) beneﬁted from sophisticated attention mechanisms Devlin et al.
(2017), and procedural planning programs Bunel et al. (2018) beneﬁted from
feeding execution traces into the neural net Chen et al. (2018). While prior works
have explored novel test-time strategies, from Sequential Monte Carlo Ellis et al.
(2019) to ensembling methods Chen et al. (2018), here we sought a systematic
empirical/theoretical study of two diﬀerent families of inference strategies, which
we intend to mesh well with the larger body of work on neural program syn-
thesis. While the algorithms introduced here do not straightforwardly extend
to neural autoregressive models (e.g. RobustFill Devlin et al. (2017)), methods
such as SQRT Sampling in principle apply to this setting too. We hope that
our work here spurs the development of the right tricks to get the theoretical
beneﬁts of SQRT Sampling for these more ﬂexible model classes, just as Deep-
Coder paved the way for RobustFill. Many extensions: Lee et al. (2018); Zhang
et al. (2018); Polosukhin and Skidanov (2018); Kalyan et al. (2018); Zohar and
Wolf (2018).

Shi et al. (2020b) introduced Unique Randomizer in order to sample without
replacement: it is a general technique eﬀectively turning a sampling method into
an enumerative one by updating the probabilistic weights along the search. It
is further improved through batching via Stochastic Beam Search Kool et al.
It is possible to combine the SQRT Sampling algorithm with the
(2019).
Unique Randomizer and Stochastic Beam Search. Our experiments did not
yield interesting results in that direction, possibly because of memory allocation
issues. We leave for future work to optimise this approach.

7.2 Contributions and Outlook

Learning to synthesize programs is a canonical neural-symbolic learning prob-
lem: training high capacity statistical models to guide the construction of richly
structured combinatorial objects, such as programs. Yet while the neural side of
this problem has received much deserved attention, the symbolic component is
sometimes taken for granted–after all, symbolic program synthesis has received
decades of attention. But the entrance of powerful neural networks for synthe-
sizing programs forces us to reconsider how we deploy symbolic methods for
program synthesis. We have considered both systematic and stochastic meth-
ods, from both theoretical angles (obtaining guarantees) and also engineering
perspectives (such as how to parallelize our new techniques). We hope this work
helps contribute to thinking through the symbolic search back-end in this more
modern context.

15

References

R. Alur, R. Bod´ık, G. Juniwal, M. M. K. Martin, M. Raghothaman, S. A.
Seshia, R. Singh, A. Solar-Lezama, E. Torlak, and A. Udupa. Syntax-guided
In Formal Methods in Computer-Aided Design, FMCAD, 2013.
synthesis.
URL http://ieeexplore.ieee.org/document/6679385/.

R. Alur, R. Singh, D. Fisman, and A. Solar-Lezama. Search-based program
synthesis. Communications of the ACM, 61(12), 2018. URL https://doi.
org/10.1145/3208071.

M. Balog, A. L. Gaunt, M. Brockschmidt, S. Nowozin, and D. Tarlow. Deep-
coder: Learning to write programs. In International Conference on Learning
Representations, ICLR, 2017. URL https://openreview.net/forum?id=
ByldLrqlx.

R. Bunel, M. J. Hausknecht, J. Devlin, R. Singh, and P. Kohli. Leverag-
ing grammar and reinforcement learning for neural program synthesis.
In
International Conference on Learning Representations, ICLR, 2018. URL
https://openreview.net/forum?id=H1Xw62kRZ.

X. Chen, C. Liu, and D. Song. Execution-guided neural program synthesis. In

International Conference on Learning Representations, ICLR, 2018.

Z. Chi. Statistical properties of probabilistic context-free grammars. Computa-
tional Linguistics, 25(1), 1999. URL https://www.aclweb.org/anthology/
J99-1004.

K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Conference on Em-
pirical Methods in Natural Language Processing, EMNLP, 2014. URL https:
//aclanthology.org/D14-1179.

J. Clymo, A. Gasc´on, B. Paige, N. Fijalkow, and H. Manukian. Data generation
for neural programming by example. In International Conference on Artiﬁcial
Intelligence and Statistics, AI&STATS, 2020. URL http://proceedings.
mlr.press/v108/clymo20a.html.

J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A. Mohamed, and P. Kohli.
Robustﬁll: Neural program learning under noisy I/O.
In International
Conference on Machine Learning, ICML, volume 70 of Proceedings of Ma-
chine Learning Research, 2017. URL http://proceedings.mlr.press/v70/
devlin17a.html.

K. Ellis, M. Nye, Y. Pu, F. Sosa, J. Tenenbaum, and A. Solar-Lezama. Write,
execute, assess: Program synthesis with a REPL. In Neural Information Pro-
cessing Systems, NeurIPS, 2019. URL https://proceedings.neurips.cc/
paper/2019/hash/50d2d2262762648589b1943078712aa6-Abstract.html.

K. Ellis, C. Wong, M. I. Nye, M. Sabl´e-Meyer, L. Morales, L. B. Hewitt, L. Cary,
A. Solar-Lezama, and J. B. Tenenbaum. Dreamcoder: bootstrapping induc-
tive program synthesis with wake-sleep library learning. In International Con-
ference on Programming Language Design and Implementation, PLDI, 2021.
URL https://doi.org/10.1145/3453483.3454080.

16

Y. Feng, R. Martins, O. Bastani, and I. Dillig. Program synthesis using conﬂict-
driven learning. In ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI, 2018. URL https://doi.org/10.1145/
3192366.3192382.

S. Gulwani, O. Polozov, and R. Singh. Program synthesis. Foundations and
Trends in Programming Languages, 4(1-2), 2017. URL https://doi.org/
10.1561/2500000010.

A. Kalyan, A. Mohta, O. Polozov, D. Batra, P. Jain, and S. Gulwani. Neural-
guided deductive search for real-time program synthesis from examples. In
International Conference on Learning Representations, ICLR, 2018. URL
https://openreview.net/forum?id=rywDjg-RW.

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

In
International Conference on Learning Representations, ICLR, 2015. URL
http://arxiv.org/abs/1412.6980.

W. Kool, H. Van Hoof, and M. Welling. Stochastic beams and where to ﬁnd
them: The gumbel-top-k trick for sampling sequences without replacement.
In International Conference on Machine Learning, ICML, 2019. URL https:
//proceedings.mlr.press/v97/kool19a.html.

W. Lee, K. Heo, R. Alur, and M. Naik. Accelerating search-based program
synthesis using learned probabilistic models. In ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI, 2018. URL
https://doi.org/10.1145/3211992.

A. K. Menon, O. Tamuz, S. Gulwani, B. W. Lampson, and A. Kalai. A machine
learning framework for programming by example. In International Conference
on Machine Learning, ICML, 2013. URL http://proceedings.mlr.press/
v28/menon13.html.

E. Parisotto, A. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-
symbolic program synthesis.
In International Conference on Learning
Representations, ICLR, 2017. URL https://openreview.net/forum?id=
rJ0JwFcex.

I. Polosukhin and A. Skidanov. Neural program search: Solving programming
tasks from description and examples. In International Conference on Learning
Representations, ICLR, 2018. URL https://openreview.net/forum?id=
BJTtWDyPM.

K. Shi, D. Bieber, and R. Singh. TF-coder: Program synthesis for tensor ma-
nipulations. In Workshop on Computer-Assisted Programming, CAP, 2020a.
URL https://openreview.net/forum?id=nJ5Ij53umw2.

K. Shi, D. Bieber, and C. Sutton. Incremental sampling without replacement for
sequence models. In International Conference on Machine Learning, ICML,
2020b. URL https://proceedings.mlr.press/v119/shi20a.html.

R. Solomonoﬀ. A system for incremental learning based on algorithmic probabil-
ity. In Israeli Conference on Artiﬁcial Intelligence, Computer Vision and Pat-
tern Recognition, 1989. URL https://theworld.com/~rjs/publications/
IncLrn89.pdf.

17

A. J. Walker. An eﬃcient method for generating discrete random variables with
general distributions. ACM Transactions on Mathematical Software, 3(3),
1977. URL https://doi.org/10.1145/355744.355749.

L. Zhang, G. Rosenblatt, E. Fetaya, R. Liao, W. E. Byrd, R. Urtasun, and R. S.
Zemel. Leveraging constraint logic programming for neural guided program
synthesis. In International Conference on Learning Representations, ICLR,
2018. URL https://openreview.net/forum?id=HJIHtIJvz.

A. Zohar and L. Wolf. Automatic program synthesis of

long programs
with a learned garbage collector.
In Neural Information Processing Sys-
tems, NeurIPS, 2018. URL https://proceedings.neurips.cc/paper/
2018/hash/390e982518a50e280d8e2b535462ec1f-Abstract.html.

18

A Machine-learned program synthesis

A.1 Program representation as trees

Programs are internally represented as trees as illustrated on the lhs of Figure 9.
The program described by this tree is (take (head (rev l1)) (rev l2)).
We enforce a simple type system for programs consisting of basic types such
as int, bool, string, float and two constructors: list and arrow. Each
primitive in the DSL comes with a possibly polymorphic type, for instance take
has the type take: int -> t0 list -> t0 list, where t0 can be instanti-
ated to any type. We only construct correctly typed programs using classical
techniques from programming languages such as type inference and de Bruijn’s
indices for variables.

Figure 9: Tree and straight line representation of the same program.

A.2 Compilation phase: from domain speciﬁc language to

grammar

A domain speciﬁc language (DSL) is a set of primitives together with their type
and semantics. The compilation phase turns a DSL and a set of syntactic con-
straints into a CFG which generates the set of all correctly typed programs by
following typing rules. Syntactic constraints can be very diverse, they include
maximum program depth, type request for the program, and other invariants
to be satisﬁed. The type request induces the initial non-terminal of the CFG:
for instance if the type request is int -> int list -> int list then the ini-
tial non-terminal carries the information that the program should output an
int list, and that we can use one variable of type int and another one of
type int list.

Polymorphic types can be accommodated in two diﬀerent ways: at compi-
lation time or at generation time. DreamCoder’s implementation Ellis et al.
(2021) uses polymorphic types in the CFGs, and performs type inference when
generating programs. In our tool we instantiate types during the compilation
into CFGs, implying that they do not contain polymorphic types. So when
adding the primitive take: int -> t0 list -> t0 list we create a number
of primitives for each type of bounded size, for instance

take[int]: int -> int list -> int list.
This makes the CFGs larger, but considerably reduces the computation time

for generating programs by avoiding type inference algorithms.

19

The compilation phase consists in expanding typing rules as long as syntactic
constraints are satisﬁed. The CFGs are trimmed to keep their sizes manageable,
removing non-productive and non-reachable non-terminals.

B Review of enumerative methods

We describe the diﬀerent search algorithms constructed in Menon et al. (2013);
Balog et al. (2017); Feng et al. (2018); Zhang et al. (2018), which can be under-
stood as ways of exploring the tree of leftmost derivations of the grammar.

BFS and DFS for CFGs. The ﬁrst algorithms we mention do not make use
of the weights in the PCFG, they are enumeration algorithms for CFGs. We
consider the tree of all leftmost partial derivations starting from the initial non-
terminal; each node of this tree is a partial program. There are two classical
algorithms for searching this tree. Both maintain a tree of partial derivations.
To resolve ambiguity, for each non-terminal we ﬁx an (arbitrary) order on the
derivation rules from this non-terminal. The breadth-ﬁrst search (BFS) expands
the highest node containing a non-terminal by applying the next derivation rule
to the leftmost non-terminal in this node (next with respect to the order on
derivation rules from this non-terminal). The depth-ﬁrst search (DFS) instead
expands the lowest node containing a non-terminal. The DFS may not termi-
nate, we need to introduce a stopping criterion; the most natural is the size
of the program: we ﬁx a target size n and before applying a derivation rule
we check whether the partial program has size at most n. With this stopping
criterion the DFS enumerates all programs of size at most n.

The remaining algorithms take into account the probabilities attached to the

PCFG.

The sort and add search Balog et al. (2017). The algorithm is based on
two ideas: the ﬁrst idea is called ‘biassing’; we specify the order on the derivation
rules for each non-terminal, naturally by non-increasing order of probabilities,
and the second idea is an incremental search: we ﬁx a number k, restrict for
each non-terminal to the k most likely derivation rules, runs the search on
this restricted grammar, and iterates with a larger k in case the program was
not found. The choice of the sequence of values for k is both crucial for the
performances and very hard to make.
In our experiments we use arithmetic
progressions.

The maximal probability from a non-terminal. The following subrou-
tine was introduced in Menon et al. (2013) and will be used by all subsequent
algorithms. For each non-terminal T , we write mT for the most likely program
generated from T , and pT for its probability. Computing mT and pT is done
using dynamic programming: pT is the maximum of p · (cid:81)
i∈[1,k] pTi over all
derivation rules T →p f (T1, . . . , Tk).

The threshold algorithm Menon et al. (2013). The algorithm ﬁxes a
threshold, enumerates all programs with probability beating the threshold, and
iterates with a lower threshold. Enumerating all programs beating a ﬁxed

20

threshold is achieved using a DFS with the following stopping criterion: be-
fore expanding a partial program we check whether it may be completed into a
program with probability beating the threshold by replacing each non-terminal
T by the probability of the most likely program pT and checking whether that
In our experiments we use geometric
program has high enough probability.
progressions for the sequence of thresholds.

The beam search Zhang et al. (2018). Beam search is a BFS algorithm
with bounded memory: the size of the queue of partial program, called the
beam, is restricted to a constant, the beam width.

C Heap Search

The pseudocode is given in Algorithm 1.

for all non-terminal symbols T do

Algorithm 1 Heap search
1: procedure Initialization( )
2:
3:
4:
5:
6:

Create an empty max heap HeapT
Create an empty hash table SuccT
Create an empty set SeenT
for all derivation rules T → f (T1, . . . , Tk) do

Add f (m1, . . . , mk) to HeapT with priority D(f (m1, . . . , mk))
Add f (m1, . . . , mk) to SeenT

end for

7:
8:
9:
end for
10:
11: end procedure
12:
13: procedure Query(T ,x)
14:
15:
16:
17:

Return SuccT [x]

else

if x is a key of SuccT then

18:
19:
20:
21:
22:
23:

end if

24:
25:
26:
27:
28:
end if
29:
30: end procedure

end for
Return x(cid:48)

21

x(cid:48) ← pop(HeapT )
SuccT [x] ← x(cid:48)
(Assumes x(cid:48) = f (x1, . . . , xk) is generated by T → f (T1, . . . , Tk))
for all i ∈ [1, k] do

(cid:46) x(cid:48) is the successor of x
(cid:46) updating the data structure

(cid:46) add all potential successors

yi = Query(Ti, xi)
x(cid:48)
i = f (x1, . . . , xi−1, yi, xi+1, . . . , xk)
if x(cid:48)

i is not in SeenT then
Add x(cid:48)
Add x(cid:48)

i to HeapT with priority D(x(cid:48)
i)
i to SeenT

C.1 Correctness proof

The following lemma gives the correctness of the heap search algorithm.

Lemma 1. For any non-terminal T and program x generated from T , if we have
already run Query(T,y) for any program y preceding x (among those generated
from T) then Query(T,x) outputs the successor of x from T

Proof. Suppose the lemma is not true and consider the ﬁrst time where it fails,
on say, input (T, x). At step 14, (T, x) was not queried before and therefore the
algorithm branches out to line 16. Saying that the algorithm fails is equivalent
to say that line 17 fails, meaning that x(cid:48) = pop(HeapT ) is not the successor of
x from T . Let us denote by y the correct successor. There are two cases which
can lead the algorithm the failure:

• First case: x(cid:48) is a program with higher probability than y. In this case, it
means that x(cid:48) was already the output of a previous query (because (T, x)
is the ﬁrst query for which the algorithm fails, so programs with higher
probability than y have already been output before). Thus the program
x(cid:48) was pop at least two times from HeapT , one time when x(cid:48) was the
correct output of a query, and another time when the program failed: this
is not possible since we push programs only once into the heaps thanks to
the condition at line 22.

• Second case: x(cid:48) has a smaller probability than the probability of y. In
this case, it means that y was not in HeapT when the algorithm performed
a pop at line 17 (otherwise y would have pop since y has higher probabil-
ity than x(cid:48)). Suppose y = f (x1, . . . , xk), generated by the derivation rule
T → f (T1, . . . , Tk). If all xi are maximal (meaning that they don’t have
a predecessor from Ti) then f (x1, . . . , xk) is pushed in HeapT during the
initialization procedure (line 6,7) so the algorithm cannot fail because of
this case. Therefore there is at least one xi, say w.l.o.g x1, which has a pre-
decessor x(cid:48)
1, x2, . . . , xk); this program
has higher probability than y and therefore has been seen before. Thus
1), x2, . . . , xk) was added to HeapT because of line 23. To
f (Query(T1, x(cid:48)
conclude, observe that f (Query(T1, x(cid:48)
1), x2, . . . , xk) = f (x1, x2, . . . , xk))
so y has previously been added to HeapT .

1 from T1. Consider the program f (x(cid:48)

C.2 Complexity analysis

Lemma 2. Fix any non-terminal T . Suppose that we have already generated
the ﬁrst i programs generated from T (meaning that if x is the j-th program for
j < i, then x is a key of the hash table SuccT and SuccT [x] is the (j + 1)-th
program generated from T ). Then querying the successor of the i-th program
has a running time of O(log i).

Proof. First observe that a query can call recursively several others queries.
However, for any non-terminal symbol T there is at most one query of the form
Query(T, x) which leads the algorithm to branch out to line 16 and thus to
possibly rise other queries. Indeed, this case happens only when the successor
of x has not been computed yet (otherwise the query stops at line 15); this can

22

happen for at most one program for any ﬁxed symbol T : the last program from
T already seen in any execution of the algorithm. Forgetting about recursive
queries, the running time of a query going through line 16 is given by the pop
and push operations (line 17 and 23). The number of pops and pushs is at most
m + 1 where m is the maximal arity of a function in the grammar. Moreover,
each such operation costs a running time of O(log |HeapT |), so the total time
for the query is O(log |HeapT |).

Overall, the total running time is bounded by

O(log |HeapT |).

(cid:88)

T

To conclude, observe that when we query the successor of the i-th program
generated from T , the size of HeapT is bounded by m · i since we push at most
m programs during any query not already computed before.

C.3 Program evaluation

A key advantage of Heap Search is that it is bottom-up: partial programs
are composed from the leaves to the root. This implies that partial programs
can be evaluated and their evaluations cached. Although memory hungry, this
optimisation leads to major gains when taking evaluation into account.

D SQRT Sampling

D.1 An example

The simplest instantiation of this theorem is X = {Head, Tail} and the Bernoulli
distribution D with parameter p: it draws Head with probability p and Tail with
probability 1 − p.

A sampling algorithm D(cid:48) is a Bernoulli distribution with parameter p(cid:48), and

its loss is

L(D(cid:48), D) = Ex∼D

(cid:20)

1
D(cid:48)(x)

(cid:21)

=

p
p(cid:48) +

1 − p
1 − p(cid:48) .

Minimising for p(cid:48) yields p(cid:48) =

√

algorithm

D.

√
p
1−p , inducing the loss optimal sampling
√

p+

√

D.2

Implementation details

√

The construction follows two steps: ﬁrst construct a weighted CFG that recog-
nises
D, and then normalise it into a PCFG using Chi (1999). The normali-
sation requires computing the partition function Z deﬁned by

Z(S) =

(cid:88)

D(P ).

P generated from S

In general the partition function can be computed by solving a system of poly-
nomial equations. This is easier in our case since we restrict ourselves to acyclic
PCFGs.

23

E Parallel implementation

E.1 Description and pseudocode

The pseudocode of the grammar splitter is given in Algorithm 2 using two
procedures: split and ﬁnd improving swap. The split procedure describes at a
high level how our grammar splitter works. The ﬁnd improving swap procedure
is here to provide a clear method of ﬁnding an improving swap or reﬁnement.

In our experiments, we initialized the splitting as follows: we split the node
with highest probability until the total number of nodes is greater than the
number of splits required, then assign one node to each split, and the remaining
nodes to the last split. We also limited the search of an improving swap or
reﬁnement to the most probable split and the least probable split unlike in the
ﬁnd improving swap procedure where all splits are considered. Finally, in all of
our experiments αdesired = 1.05.

F Details on the experiments

F.1 Random PCFG search

To turn the CFGs into PCFGs we sample a probabilistic labelling, meaning a
weight for each derivation rule such that the sum over all derivation rules from
each non-terminal is one. The distribution depends on a parameter α ∈ (0, 1]:
the ith weight is sampled uniformly at random in [0, αi], and eventually renor-
malised. The smaller α, the more biassed the distribution, implying that the
search for programs will be faster since we have better hints on the target pro-
gram. For α = 1 the weights are sampled uniformly, resulting in a very unbiased
PCFG, making the search more diﬃcult. In the random PCFGs experiment we
use α = 0.7.

F.2 Machine-learned PCFG

We only work with tasks of type int list -> int list. We remove examples
where lists have length greater than Lmax = 10 or where one of the elements of
the input or output list are not in Lin = [−30; 30].

We say that a task is solved if a program is found which satisﬁes all examples.
The timeout of 100s only takes into account the search time and the evaluation
times and not the time to query the neural network for predictions.

The neural network

The neural network takes as input (the encoding of) a list of examples and
outputs a probabilistic labelling for the CFG.

Input encoding Each list in the examples is encoded in a naive way by
mapping each element of Lin to [0, 60]. We additionally use two special symbols
PAD and NOPAD, leading to a ﬁxed size encoding of a list into a vector of size
2Lmax. Hence an example is encoded as a tensor of shape (ninputs, 2Lmax)
where ninputs is the number of inputs in the example.

24

3:

4:
5:

6:

7:

8:
9:
10:

17:
18:

19:
20:
21:
22:
23:
24:

25:
26:
27:
28:
29:

30:
31:
32:
33:
34:
35:

36:
37:
38:
39:
40:
41:

42:
43:
44:
45:
46:
47:

48:
49:
50:
51:
52:

Algorithm 2 Grammar splitter
1: procedure split(G, nsplits, αdesired)
2:

Create an initial splitting Splits
α ← maxsG∈Splits probability mass(sG)
minsG∈Splits probability mass(sG)
while α > αdesired do

if an improving swap exists then

Update Splits with the improving swap
α ← maxsG∈Splits probability mass(sG)
minsG∈Splits probability mass(sG)

else

Find the partial program with largest probability P
Replace P in its split with its children

end if
end while
Return Splits

11:
12:
13:
14: end procedure
15: procedure Find improving swap(G, Splits)
16:

α ← maxsG∈Splits probability mass(sG)
minsG∈Splits probability mass(sG)
α∗ ← α
s ← None
L ← argmaxsG∈Splits probability mass(sG)
Sort Splits by increasing probability mass
for all sG ∈ Splits \ {L} do

(cid:46) best improving swap α
(cid:46) best improving swap

for all P (cid:48) ∈ L do

for all P ∈ G do

β ← Compute new α with Swap(L, sG, P , P (cid:48))
if β < α∗ then
α∗ ← β
s ← Swap(L, sG, P , P (cid:48))

end if

end for
β ← Compute new α with Gift(L, sG, P (cid:48))
if β < α∗ then
α∗ ← β
s ← Gift(L, sG, P (cid:48))

end if

end for

end for
l ← argminsG∈Splits probability mass(sG)
Sort Splits by decreasing probability mass
for all sG ∈ Splits \ {L, l} do

for all P ∈ sG do

for all P (cid:48) ∈ l do

β ← Compute new α with Swap(l, sG, P , P (cid:48))
if β < α∗ then
α∗ ← β
s ← Swap(l, sG, P , P (cid:48))

end if

end for
β ← Compute new α with Gift(sG, l, P )
if β < α∗ then
α∗ ← β
s ← Gift(sG, l, P )

25

end if

end for

53:
54:
55:
56: end procedure

end for
Return s

Embedding The encoded examples are fed into an embedding consisting of a
single layer GRU Cho et al. (2014) outputting a tensor of shape (sGRU ×2Lmax).
In our experiments sGRU = 10.

Main layers The embedded examples are fed into an MLP with 3 layers.
The ﬁrst two have output size sM LP = 64 and the last layer outputs a tensor
of dimension k the number of derivation rules in the CFG C. Assigning these
(normalised) weights to the rules of the CFG yields a PCFG.

Training

We optimised end-to-end the neural network with Adam Kingma and Ba (2015)
with default parameters and learning rate of lr = 0.001. We trained for one
epoch with a batch size of 128 on a generated dataset of 10, 000 problems. To
generate the dataset, programs are sampled from the uniform PCFG and inputs
by choosing a length at most Lmax and elements uniformly at random in Lin.
If the output of such a generated input does not fall in the lexicon Lin then the
program is discarded.

We use the binary cross entropy loss between the output of the neural net-
work and the encoding of a solution program: a rule in the CFG has probability
1 if it is used to derive the solution program, and 0 otherwise.

26

