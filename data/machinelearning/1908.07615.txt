9
1
0
2

g
u
A
0
2

]

C
O
.
h
t
a
m

[

1
v
5
1
6
7
0
.
8
0
9
1
:
v
i
X
r
a

Iterative Linearized Control:
Stable Algorithms and Complexity Guarantees

Vincent Roulet1

Siddhartha Srinivasa2

Dmitriy Drusvyatskiy3

Zaid Harchaoui1

1 Department of Statistics, University of Washington
2 Paul G. Allen School of Computer Science and Engineering, University of Washington
3 Department of Mathematics, University of Washington
{vroulet, zaid, ddrusv}@uw.edu, {siddh}@cs.uw.edu

June 10th, 2019

Abstract

We examine popular gradient-based algorithms for nonlinear control in the light of the modern complexity anal-
ysis of ﬁrst-order optimization algorithms. The examination reveals that the complexity bounds can be clearly stated
in terms of calls to a computational oracle related to dynamic programming and implementable by gradient back-
propagation using machine learning software libraries such as PyTorch or TensorFlow. Finally, we propose a reg-
ularized Gauss-Newton algorithm enjoying worst-case complexity bounds and improved convergence behavior in
practice. The software library based on PyTorch is publicly available.

Introduction

Finite horizon discrete time nonlinear control has been studied for decades, with applications ranging from spacecraft
dynamics to robot learning (Bellman, 1971; Whittle, 1982; Bertsekas, 2005). Popular nonlinear control algorithms,
such as differential dynamic programming or iterative linear quadratic Gaussian algorithms, are commonly derived
using a linearization argument relating the nonlinear control problem to a linear control problem (Todorov & Li, 2003;
Li & Todorov, 2007).

We examine nonlinear control algorithms based on iterative linearization techniques through the lens of the modern
complexity analysis of ﬁrst-order optimization algorithms. We ﬁrst reformulate the problem as the minimization of
an objective that is written as a composition of functions. Owing to this reformulation, we can frame several popular
nonlinear control algorithms as ﬁrst-order optimization algorithms applied to this objective.

We highlight the equivalence of dynamic programming and gradient back-propagation in this framework and
underline the central role of the corresponding automatic differentiation oracle in the complexity analysis in terms of
convergence to a stationary point of the objective. We show that the number of calls to this automatic differentiation
oracle is the relevant complexity measure given the outreach of machine learning software libraries such as PyTorch
or TensorFlow (Paszke et al., 2017; Abadi et al., 2015).

Along the way we propose several improvements to the iterative linear quadratic regulator (ILQR) algorithm, re-
sulting in an accelerated regularized Gauss-Newton algorithm enjoying a complexity bound in terms of convergence
to a stationary point and displaying stable convergence behavior in practice. Regularized Gauss-Newton algorithms
give a template for the design of algorithms based on partial linearization with guaranteed convergence (Bjorck, 1996;
Burke, 1985; Nesterov, 2007; Lewis & Wright, 2016; Drusvyatskiy & Paquette, 2018). The proposed accelerated
regularized Gauss-Newton algorithm is based on a Gauss-Newton linearization step stabilized by a proximal regu-
larization and boosted by a Catalyst extrapolation scheme, potentially accelerating convergence while preserving the
worst-case guarantee.

1

 
 
 
 
 
 
Related work. Differential dynamic programming (DDP) and iterative linearization algorithms are popular algo-
rithms for ﬁnite horizon discrete time nonlinear control (Tassa et al., 2014). DDP is based on approximating the
Bellman equation at the current trajectory in order to use standard dynamic programming. Up to our knowledge, the
complexity analysis of DDP has been limited; see (Mayne, 1966; Jacobson & Mayne, 1970; Todorov & Li, 2003) for
classical analyses of DDP.

Iterative linearization algorithms such as the iterative linear quadratic regulator (ILQR) or the iterative linearized
Gaussian algorithm (ILQG) linearize the trajectory in order to use standard dynamic programming (Li & Todorov,
2004; Todorov & Li, 2005; Li & Todorov, 2007). Again, the complexity analysis of ILQR for instance has been
limited. In this paper, we refer to the deﬁnitions of ILQR and ILQG as given in the original papers (Li & Todorov,
2004; Todorov & Li, 2005; Li & Todorov, 2007), the same names have been then used for variants of those algorithms
that use a roll-out phase on the true trajectory as in, e.g., (Tassa et al., 2012) where line-searches were proposed.
Line-searches akin to the Levenberg-Marquardt method were proposed but without convergence rates (Todorov & Li,
2005). It is worthwhile to mention related approaches in the nonlinear model predictive control area (Gr¨une & Pannek,
2017; Richter et al., 2012; Dontchev et al., 2018).

We adopt the point of view of the complexity theory of ﬁrst-order optimization algorithms. The computation of
a Gauss-Newton step (or a Newton step) through dynamic programming for nonlinear control problems is classical;
see (Whittle, 1982; Dunn & Bertsekas, 1989; Sideris & Bobrow, 2005). However, while the importance of the addition
of a proximal term in Gauss-Newton algorithms is now well-understood (Nesterov, 2007), several popular nonlinear
control algorithms involving such steps, such as ILQR, have not been revisited yet (Li & Todorov, 2004). Our work
shows how to make these improvements.

We also show how gradient back-propagation, i.e., automatic differentiation (Griewank & Walther, 2008), a popu-
lar technique usually derived using either a chain rule argument or a Lagrangian framework (Bertsekas, 2005; LeCun
et al., 1988), allows one to solve the dynamic programming problems arising in linear quadratic control. Consequently,
the subproblems that arise when using iterative linearization for nonlinear control can be solved with calls to an auto-
matic differentiation oracle implementable in PyTorch or TensorFlow (Abadi et al., 2015; Paszke et al., 2017; Kakade
& Lee, 2018).

The regularized Gauss-Newton method was extensively studied to minimize the nonlinear least squares objectives
arising in inverse problems (Bjorck, 1996; Nocedal & Wright, 2006; Kaltenbacher et al., 2008; Hansen et al., 2013).
The complexity-based viewpoint used in (Nesterov, 2007; Cartis et al., 2011; Drusvyatskiy & Paquette, 2018) informs
our analysis and offers generalizations to locally Lipschitz objectives. We build upon these results in particular when
equipping the proposed regularized Gauss-Newton algorithm with an extrapolation scheme in the spirit of (Paquette
et al., 2018).

All notations are presented in Appendix A. The code for this project is available at https://github.com/

vroulet/ilqc.

1 Discrete time control

We ﬁrst present the framework of ﬁnite horizon discrete time nonlinear control.

Exact dynamics. Given state variables x
trajectories ¯x = (x1; . . . ; xτ )
Rτ p, through

∈

∈

Rd and control variables u

Rp, we consider the control of ﬁnite

Rτ d of horizon τ whose dynamics are controlled by a command ¯u = (u0; . . . ; uτ −1)

∈

xt+1 = φt(xt, ut),

for t = 0, . . . , τ

1,

(1)

starting from a given ˆx0

Rd, where the functions φt : Rd

Rp

Rd are assumed to be differentiable.

×
Optimality is measured through convex costs ht, gt, on the state and control variables xt, ut respectively, deﬁning

→

∈

∈

−

2

the discrete time nonlinear control problem

min
x0,...,xτ ∈Rd
u0,...,uτ −1∈Rp

τ
(cid:88)

t=1

ht(xt) +

τ −1
(cid:88)

t=0

gt(ut)

subject to xt+1 = φt(xt, ut),

x0 = ˆx0,

(2)

where, here and thereafter, the dynamics must be satisﬁed for t = 0, . . . , τ

1.

−

Noisy dynamics. The discrepancy between the model and the dynamics can be taken into account by considering
noisy dynamics as

xt+1 = φt(xt, ut, wt),

(3)

(0, Iq) for t = 0, . . . , τ
where wt
average cost under the noise ¯w = (w0; . . . ; wτ −1) as

∼ N

−

1. The resulting discrete time control problem consists of optimizing the

min
x0,...,xτ ∈Rd
u0,...,uτ −1∈Rp

E ¯w

(cid:34) τ

(cid:88)

t=1

(cid:35)

ht(xt)

+

τ −1
(cid:88)

t=0

gt(ut)

subject to xt+1 = φt(xt, ut, wt),

x0 = ˆx0.

(4)

Costs and penalties. The costs on the trajectory can be used to force the states to follow a given orbit ˆx1, . . . , ˆxτ as

ht(xt) =

1
2

(xt

−

ˆxt)(cid:62)Qt(xt

−

ˆxt), with Qt

0,

(cid:23)

(5)

which gives a quadratic tracking problem, while the regularization penalties on the control variables are typically
quadratic functions

gt(ut) =

u(cid:62)
t Rtut, with Rt

0.

(cid:31)

(6)

1
2

The regularization penalties can also encode constraints on the control variable such as the indicator function of a box

gt(ut) = ι{u:c−

t ≤u≤c+

t }(ut), with c−

t , c+

t ∈

Rp,

(7)

where ιS denotes the indicator function of a set S.

Iterative Linear Control algorithms. We are interested in the complexity analysis of algorithms such as the iterative
linear quadratic regulator (ILQR) algorithm as deﬁned in (Li & Todorov, 2004; Todorov & Li, 2005; Li & Todorov,
2007), used for exact dynamics, which iteratively computes the solution of

τ
(cid:88)

qht(x(k)

min
y0,...yτ ∈Rd
v0,...,vτ −1∈Rp
subject to yt+1 = (cid:96)φt(yt, vt),

t + yt) +

t=1

τ −1
(cid:88)

t=0

qgt(u(k)

t + vt)

y0 = 0,

(8)

where ¯u(k) is the current command, ¯x(k) is the corresponding trajectory given by (1), qht , qgt are quadratic approxima-
tions of the costs ht, gt around respectively x(k)
). The next
iterate is then given by ¯u(k+1) = ¯u(k) + α¯v∗ where ¯v∗ is the solution of (8) and α is a step-size given by a line-search
method. To understand this approach, we frame the problem as the minimization of a composition of functions.

and (cid:96)φt is the linearization of φt around (x(k)

, u(k)
t

, u(k)
t

t

t

Note that the term ILQR or ILQG has then been used to refer to a variant of the above algorithm that uses the
feedback gains computed in the resolution of the linear control problem to control to move along the true trajectory,
see (Tassa et al., 2012).

3

(10)

(11)

Formulation as a composite optimization problem. We call an optimization problem a composite optimization
Rτ p, denote by
problem if it consists in the minimization of a composition of functions. For a ﬁxed command ¯u
˜x(¯u) = (˜x1(¯u); . . . ; ˜xτ (¯u))

Rτ d the trajectory given by the exact dynamics, which reads

∈

∈

˜x1(¯u) = φ0(ˆx0, u0),

˜xt+1(¯u) = φt(˜xt(¯u), ut).

(9)

Similarly denote by ˜x(¯u, ¯w)
the total penalty by g(¯u) = (cid:80)τ −1

Rτ d the trajectory in the noisy case. Denoting the total cost by h(¯x) = (cid:80)τ
∈
t=0 gt(ut), the control problem (2) with exact dynamics reads
f (¯u) (cid:44) h(˜x(¯u)) + g(¯u),

t=1 ht(xt),

min
¯u∈Rτ p

and with noisy dynamics,

min
¯u∈Rτ p

f (¯u) (cid:44) E ¯w [h(˜x(¯u, ¯w))] + g(¯u),

i.e., we obtain a composite optimization problem whose structure can be exploited to derive oracles on the objective.

2 Oracles in discrete time control

We adopt here the viewpoint of the complexity theory of ﬁrst-order optimization. Given the composite problem (10),
what are the relevant oracles and what are the complexities of calls to these oracles? We ﬁrst consider exact dynamics
φt of the form (1) and unconstrained cost penalties such as (6).

2.1 Exact and unconstrained setting

Model minimization. Each step of the optimization algorithm is deﬁned by the minimization of a regularized model
of the objective. For example, a gradient step on a point ¯u with step-size γ corresponds to linearizing both h and ˜x
and deﬁning the linear model

(cid:96)f (¯u + ¯v; ¯u) = (cid:96)h

(cid:0)˜x(¯u) +

˜x(¯u)(cid:62)¯v; ˜x(¯u)(cid:1) + (cid:96)g(¯u + ¯v; ¯u)

∇

of the objective f , where (cid:96)h(¯x + ¯y; ¯x) = h(¯x) +
with a proximal regularization is minimized in order to get the next iterate

∇

h(¯x)(cid:62) ¯y and (cid:96)g(¯u + ¯v; ¯u) is deﬁned similarly. Then, this model

¯u+ = ¯u + arg min
¯v∈Rτ p

(cid:26)

(cid:96)f (¯u + ¯v; ¯u) +

(cid:27)

.

1
2γ (cid:107)

2
¯v
2
(cid:107)

(12)

Different models can be deﬁned to better approximate the objective. For example, if only the mapping ˜x is

linearized, this corresponds to deﬁning the convex model at a point ¯u

We get then a regularized Gauss-Newton step on a point ¯u

cf (¯u + ¯v; ¯u) = h (cid:0)˜x(¯u) +

˜x(¯u)(cid:62)¯v(cid:1) + g(¯u + ¯v).

∇
Rτ p with step size γ > 0 as

(cid:26)

¯u+ = ¯u + arg min
¯v∈Rτ p

∈

cf (¯u + ¯v; ¯u) +

(cid:27)

.

1
2γ (cid:107)

2
2

¯v
(cid:107)

(13)

(14)

Although this model better approximates the objective, its minimization may be computationally expensive for
general functions h and g. We can use a quadratic approximation of h around the current mapping ˜x(¯u) and linearize
the trajectory around ¯u which deﬁnes the quadratic model

qf (¯u + ¯v; ¯u) = qh

(cid:0)˜x(¯u) +

∇

˜x(¯u)(cid:62)¯v; ˜x(¯u)(cid:1) + qg(¯u + ¯v; ¯u),

(15)

where qh(¯x+ ¯y; ¯x) (cid:44) h(¯x)+
∇
step with step-size γ consists in minimizing the model (15) with a proximal regularization

2h(¯x)¯y/2 and qg(¯u+ ¯v; ¯u) is deﬁned similarly. A Levenberg-Marquardt

h(¯x)(cid:62) ¯y + ¯y(cid:62)

∇

¯u+ = ¯u + arg min
¯v∈Rτ p

(cid:26)

qf (¯u + ¯v; ¯u) +

(cid:27)

.

1
2γ (cid:107)

2
2

¯v
(cid:107)

(16)

4

Model-minimization steps by linear optimal control. Though the chain rule gives an analytic form of the gradient,
we can use the deﬁnition of a gradient step as an optimization sub-problem to understand its implementation. Formally,
the above steps (12), (14), (16), deﬁne a model mf of the objective f in (10) on a point ¯u, as

f (¯u + ¯v)

≈

mf (¯u + ¯v; ¯u) = mh

(cid:0)˜x(¯u)+

∇

˜x(¯u)(cid:62)¯v; ˜x(¯u)(cid:1) + mg(¯u + ¯v; ¯u),

where mh = (cid:80)τ
t=1 mht, mg = (cid:80)τ −1
variables. The model-minimization step with step-size γ,

t=0 mgt are models of h and g respectively, composed of models on the individual

¯u+ = ¯u + arg min
¯v∈Rτ p

(cid:26)

mf (¯u + ¯v; ¯u) +

(cid:27)

,

1
2γ (cid:107)

2
2

¯v
(cid:107)

(17)

amounts then to a linear control problem as shown in the following proposition.

Proposition 2.1. The model-minimization step (17) for control problem (2) written as (10) is given by ¯u+ = ¯u + ¯v∗
where ¯v∗ = (v∗

τ −1) is the solution of

0; . . . , v∗

τ
(cid:88)

min
y0,...yτ ∈Rd
v0,...,vτ −1∈Rp
subject to yt+1 = Φ(cid:62)

t=1

mht(xt+yt; xt) +

τ −1
(cid:88)

t=0

mgt(ut+vt; ut) +

1
2γ (cid:107)

vt

2
2
(cid:107)

t,xyt + Φ(cid:62)

t,uvt,

y0 = 0,

(18)

where Φt,x=

∇

xφt(xt, ut), Φt,u=

∇

uφt(xt, ut) and xt = ˜xt(¯u).

Proof. Recall that the trajectory deﬁned by ¯u reads

˜x1(¯u) = φ0(ˆx0, F (cid:62)
Rτ p×p, et

0 ¯u),
Rτ is the tth canonical vector in Rτ , such that F (cid:62)

˜xt+1(¯u) = φt(˜xt(¯u), F (cid:62)

Ip

t ¯u),

t ¯u = ut. The gradient reads

where Ft = et+1
˜x1(¯u) = F0

∇

∇

⊗

∈

uφ0(x0, u0) followed by

∈

where xt = ˜xt(¯u) and x0 = ˆx0. For a given ¯v = (v0; . . . ; vτ −1), the product ¯y = (y1; . . . ; yτ ) =
y1 =

uφ0(x0, u0)(cid:62)v0 followed by

˜x(¯u)(cid:62)¯v reads

∇

˜xt+1(¯u) =

˜xt(¯u)

∇

∇

∇

xφt(xt, ut) + Ft

uφt(xt, ut),

∇

∇

where we used that yt =

∇

yt+1 =

xφt(xt, ut)(cid:62)yt +

uφt(xt, ut)(cid:62)vt,

∇

∇

˜xt(¯u)(cid:62)¯v. Plugging this into (17) gives the result.

Dynamic programming.
If the models used in (17) are linear or quadratic, the resulting linear control problems (18)
can be solved efﬁciently using dynamic programming, i.e., with a linear cost in τ , as presented in the following
proposition. The cost is

(τ p3d3). Details on the implementation for quadratic costs are provided in Appendix B.

Since the leading dimension of the discrete time control problem is the length of the trajectory τ , all of the above
optimization steps have roughly the same cost. This means that, in discrete time control problems, second order steps
such as (16) are roughly as expensive as gradient steps.

O

Proposition 2.2. Model-minimization steps of the form (17) for discrete time control problem (2) written as (10) with
linear or quadratic convex models mh and mg can be solved in linear time with respect to the length of the trajectory
τ by dynamic programming.

The proof of the proposition relies on the dynamic programming approach explained below. The linear optimal
control problem (18) can be divided into smaller subproblems and then solved recursively. Consider the linear optimal
control problem (18) as

τ
(cid:88)

qht(yt) +

τ −1
(cid:88)

qgt(vt)

(19)

min
y1,...,yτ
v0,...,vτ −1

t=1
t=0
subject to yt+1 = (cid:96)t(yt, vt),

y0 = 0,

5

where (cid:96)t is a linear dynamic in state and control variables, qgt are strongly convex quadratics and qht are convex
quadratic or linear functions. For 0

τ , given ˆyt, deﬁne the cost-to-go from ˆyt, as the solution of

t

≤

≤

τ
(cid:88)

τ −1
(cid:88)

qgt(cid:48) (vt(cid:48))

qht(cid:48) (yt(cid:48)) +

ct(ˆyt) = min
yt,...,yτ
vt,...,vτ −1
t(cid:48)=t
subject to yt(cid:48)+1 = (cid:96)t(cid:48)(yt(cid:48), vt(cid:48)),
yt = ˆyt.

t(cid:48)=t

for t(cid:48) = t . . . , τ

1

−

The cost-to-go functions can be computed recursively by the Bellman equation for t

τ
∈ {

−

1, . . . , 0

,
}

ct(ˆyt) = qht(ˆyt) + min

vt {

qgt(vt) + ct+1((cid:96)t(ˆyt, vt))
}

(20)

(21)

t (ˆyt)= arg minvt {

solved for v∗
. The ﬁnal cost initializing the recursion is deﬁned as cτ (ˆyτ ) =
qhτ (ˆyτ ). For quadratic costs and linear dynamics, the problems deﬁned in (21) are themselves quadratic problems that
can be solved analytically to get an expression for ct.

qgt(vt)+ct+1((cid:96)t(ˆyt, vt))
}

The solution of (19) is given by computing c0(0), which amounts to iteratively solving the Bellman equations
starting from ˆy0 = 0. Formally, starting form t = 0 and ˆy0 = 0, it iteratively gets the optimal control v∗
t at time t
deﬁned by the analytic form of the cost-to-go function and moves along the dynamics to get the corresponding optimal
next state,

t = v∗
v∗

t (yt),

yt+1 = (cid:96)t(yt, v∗

t ).

(22)

The cost of the overall dynamic procedure that involves a backward pass to compute the cost-to-go functions and a
roll-out pass to compute the optimal controls is therefore linear in the length of the trajectory τ . The main costs lie in
solving quadratic problems in the Bellman equation (21) which only depend on the state and control dimensions d and
p.

Gradient back-propagation as dynamic programming. We illustrate the derivations for a gradient step in the
(τ (pd + d2)). We recover the well-known gradient back-propagation
following proposition that shows a cost of
algorithm used to compute the gradient of the objective. The dynamic programming viewpoint provides here a natural
derivation.

O

Proposition 2.3. A gradient step (12) for discrete time control problem (2) written as (10) and solved by dynamic
programming amounts to

1. a forward pass that computes the derivatives

xφt(xt, ut),

along the trajectory given by xt+1 = φt(xt, ut) for t = 0, , . . . , τ

∇

uφt(xt, ut),
1,

∇

−

ht(xt),

∇

∇

gt(ut) for t = 0, . . . , τ

2. a backward pass that computes linear cost-to-go functions as ct(yt) = λ(cid:62)

t yt + µt where λτ =

λt =

∇

ht(xt) +

∇

xφt(xt, ut)λt+1, for t = τ

1, . . . 0,

−

3. a roll-out pass that outputs v∗

t =

γ(

∇

−

uφt(xt, ut)λt+1 +

∇

gt(ut)), for t = 0, . . . τ

1.

−

Proof. Recall that a gradient step is given as ¯u+ = ¯u + ¯v∗ where ¯v∗ is the solution of

hτ (xτ ),

∇

min
¯v∈Rτ p

(cid:96)h

(cid:0)˜x(¯u) +

∇

˜x(¯u)(cid:62)¯v; ˜x(¯u)(cid:1) + (cid:96)g(¯u + ¯v; ¯u) +

1
2
2.
¯v
2γ (cid:107)
(cid:107)

where (cid:96)h(¯x + ¯y; ¯x) = h(¯x) +
to a linear optimal control problem of the form

∇

h(¯x)(cid:62) ¯y and (cid:96)g(¯u + ¯v; ¯u) is deﬁned similarly. From Prop. 2.1, we get that it amounts

τ
(cid:88)

y(cid:62)
t at +

min
y0,...,yτ
v0,...,vτ −1
subject to yt+1 = Φ(cid:62)

t=1

τ −1
(cid:88)

v(cid:62)
t bt +

t=0
t,xyt + Φ(cid:62)

t,uvt,

1
2γ

vt

2
(cid:107)

τ −1
(cid:88)
t=0 (cid:107)
y0 = 0,

(23)

6

where at =
the linear problem is the forward pass.

ht(xt), bt =

∇

∇

gt(ut), Φt,x =

xφt(xt, ut), Φt,u =

∇

∇

uφt(xt, ut) and xt = ˜xt(¯u). The deﬁnition of

When solving (23) with dynamic programming, cost-to-go functions are linear, ct(y) = λ(cid:62)

t y + µt. Recursion

starts with λτ = aτ , µτ = 0. Then, assuming ct+1(y) = λ(cid:62)

t+1y + µt+1 for t

ct(y) = a(cid:62)

t y + min
v∈Rp

(cid:26)

t v + λ(cid:62)
b(cid:62)

t+1(Φ(cid:62)

= (at + Φt,xλt+1)(cid:62)y + µt+1 +

t,uv) +

t,xy + Φ(cid:62)
γ
2 (cid:107)
bt + Φt,uλt+1

bt + Φt,uλt+1

+ µt+1

−
2
2
(cid:107)

, we get
}

1, . . . , 0
(cid:27)

τ
∈ {
1
v
2γ (cid:107)
2
2,
(cid:107)
2
2 that deﬁne the cost-to-go function at
(cid:107)

(24)

and so we identify λt = at + Φt,xλt+1 and µt = µt+1 + γ
time t. This deﬁnes the backward pass.

2 (cid:107)

The optimal control variable at time t is then independent of the starting state and reads from (24),

This deﬁnes the roll-out pass.

2.2 Noisy or constrained settings

v∗
t =

−

γ(Φt,uλt+1 + bt).

Noisy dynamics. For inexact dynamics deﬁning the problem (11), we consider a Gaussian approximation of the lin-
earized trajectory around the exact current trajectory. Formally, the Gaussian approximation of the random linearized
trajectory (cid:96)˜x(¯u + ¯v; ¯u, ¯w) = ˜x(¯u, ¯w) +

¯u ˜x(¯u, ¯w)(cid:62)¯v around the exact linearized trajectory given for ¯w = 0 reads

ˆ(cid:96)˜x(¯u + ¯v; ¯u, ¯w) =˜x(¯u, 0) +

¯u ˜x(¯u, 0)(cid:62)¯v +

¯w ˜x(¯u, 0)(cid:62) ¯w +

∇
which satisﬁes E ¯w[ˆ(cid:96)˜x(¯u + ¯v; ¯u, ¯w)] = ˜x(¯u, 0) +

∇
¯u ˜x(¯u, 0)(cid:62)¯v, see Appendix A for gradient and tensor notations.

∇

2
¯u ¯w ˜x(¯u, 0)[¯v, ¯w,

],
·

The model we consider for the state cost is then of the form

mf (¯u + ¯v; ¯u) = E ¯w

(cid:104)
mh

∇
(cid:16)ˆ(cid:96)˜x(¯u + ¯v; ¯u, ¯w); ˜x(¯u, 0)

(cid:17)(cid:105)

+ mg(¯u + ¯v; ¯u).

(25)

∇

For simple dynamics φt, their minimization with an additional proximal term amounts to a linear quadratic Gaussian
control problem as stated in the following proposition.

2
uxφt to be zero. The model minimization step (17) for model (25) is
Proposition 2.4. Assume
∇
given by ¯u+ = ¯u + ¯v∗ where ¯v∗ is the solution of

2
xwφt and

2
xxφt,

∇

∇

min
¯y,¯v

s.t.

τ
(cid:88)

t=1

E ¯w [mht(xt + yt; xt)] +

τ −1
(cid:88)

t=0

mgt(ut + vt; ut) +

1
2γ (cid:107)

vt

2
2
(cid:107)

t,xyt + Φ(cid:62)

t,uvt + Φ(cid:62)

t,wwt + φt,u,w[vt, wt,

yt+1 = Φ(cid:62)
y0 = 0,

],

·

(26)

∇

xφt(xt, ut, 0), Φt,u =

where Φt,x =
˜xt(¯u, 0).
Proof. The Gaussian approximation ˆ(cid:96)˜x(¯u + ¯v; ¯u, ¯w) can be decomposed as in Prop. 2.1. Recall that the trajectory
reads

wφt(xt, ut, 0), φt,u,w =

uφt(xt, ut, 0), Φt,w =

2
uwφt(xt, ut, 0), xt =

∇

∇

∇

where Ft = et+1
We have then

⊗

Ip, Gt = et+1

⊗

˜x1(¯u, ¯w) = φ0(ˆx0, F0 ¯u, G0 ¯w),

˜xt+1 = φt(˜xt(¯u, ¯w), Ft ¯u, Gt ¯w),
Iq and et is the tth canonical vector in Rτ , such that F (cid:62)

t ¯u = ut and G(cid:62)

t ¯w = wt.

¯u ˜x1(¯u, ¯w) =F0

∇
¯u ˜xt+1(¯u, ¯w) =

∇
¯w ˜x1(¯u, ¯w) =G0

∇
¯w ˜xt+1(¯u, ¯w) =

uφ0(ˆx0, F (cid:62)

∇
¯u ˜xt(¯u, ¯w)

wφ0(ˆx0, F (cid:62)

∇
¯w ˜xt(¯u, ¯w)

0 ¯u, G(cid:62)
0 ¯w)
xφt(˜xt(¯u, ¯w), F (cid:62)
0 ¯u, G(cid:62)
0 ¯w)
xφt(˜xt(¯u, ¯w), F (cid:62)

∇

∇

∇

∇

∇

t ¯u, G(cid:62)

t ¯w) + Ft

uφt(˜xt(¯u, ¯w), F (cid:62)

t ¯u, G(cid:62)

t ¯w)

∇

(27)

t ¯u, G(cid:62)

t ¯w) + Gt

wφt(˜xt(¯u, ¯w), F (cid:62)

t ¯u, G(cid:62)

t ¯w)

∇

7

Finally denoting for clarity ˜x = ˜x(¯u, ¯w) and φt = φt(˜xt, ut, wt),
uwφ0[F (cid:62)
0 , G(cid:62)
2
0 ,
·
2
xφt] +
,
uw ˜xt[
,
∇
·
·
uxφt[F (cid:62)
2
t ,

2
¯u ¯w ˜x1 =
∇
2
¯u ¯w ˜xt+1 =

∇
+

∇

∇

]

∇
¯w ˜x(cid:62)
t ,
∇
¯w ˜x(¯u, 0)(cid:62) ¯w and c =

2
xxφt[
] +
·

] +

¯w ˜x(cid:62)
¯u ˜x(cid:62)
t ,
t ,
∇
∇
·
t , G(cid:62)
uwφt[F (cid:62)
2
t ,
2
¯u ¯w ˜x(¯u, 0)[¯v, ¯w,

]
·

∇
¯u ˜x(¯u, 0)(cid:62)¯v, b =
Denote a =
decomposed as, e.g., a = (a1; . . . ; aτ ) with at =
ct =
we get as in Prop. 2.1,

2
¯u ¯w ˜xt(¯u, 0)[¯v, ¯w,

∇

∇

∇

∇

∇

] the decomposition of b and c in τ slices. Assuming
·

∇

2
xxφt

2
xwφt and

∇

∇

·
¯u ˜xt(¯u, 0)(cid:62)¯v and we denote similarly bt =

∇

∈

], with a, b, c

Rτ d. Those can be
¯w ˜xt(¯u, 0)(cid:62) ¯w,
∇
2
uxφt to be zero,

2
xwφt[

∇

¯u ˜x(cid:62)

t , G(cid:62)
t ,

]
·

∇

a1 = Φ(cid:62)
at+1 = Φ(cid:62)

0,uv0
t,xat + Φ(cid:62)

t,uvt

b1 = Φ(cid:62)
bt+1 = Φ(cid:62)

0,ww0
t,xbt + Φ(cid:62)

t,wwt

c1 = φ0,u,w[v0, w0,
ct+1 = Φ(cid:62)

t,xct + φt,u,w[vt, wt,

]

·

]

·

xφt(xt, ut, 0), Φt,u =

where Φt,x =
xt = ˜xt(¯u, 0). Therefore the variable y = a + b + c =
decomposed as y = (y1; . . . ; yτ ) satisﬁes

uφt(xt, ut, 0), Φt,w =

∇

∇

∇

wφt(xt, ut, 0), φt,u,w =

∇

¯u ˜x(¯u, 0)(cid:62)¯v +

∇
¯w ˜x(¯u, 0)(cid:62) ¯w +

∇

2
uwφt(xt, ut, 0) and
2
]
¯u ¯w ˜x(¯u, 0)[¯v, ¯w,
·

∇

0,ww0 + φ0,u,w[v0, w0,
t,uvt + Φ(cid:62)
Plugging this in the model-minimization step gives the result.

0,uv0 + Φ(cid:62)
t,xyt + Φ(cid:62)

y1 = Φ(cid:62)
yt+1 = Φ(cid:62)

]
·

t,wwt + φt,u,w[vt, wt,

].
·

The linear control problem (26) can again be solved by dynamic programming by modifying the Bellman equa-

tion (21) in the backward pass, i.e., by solving analytically for white noise wt,

ct(ˆyt) = mht(ˆyt) + min
vt

(cid:8)mgt(vt) + Ewt

(cid:2)ct+1(Φ(cid:62)

t,x ˆyt + Φ(cid:62)

t,uvt + Φ(cid:62)

t,wwt + φt,u,w[vt, wt,

])(cid:3)(cid:9) .

·

The complete resolution for quadratics is provided in Appendix C.

Dealing with constraints. For constrained control problems with exact dynamics, the model-minimization steps
will amount to linear control problems under constraints, which cannot be solved directly by dynamic programming.
However their resolution by an interior point method boils down to solving linear quadratic control problems each of
which has a low computational cost as shown before.

Formally, the resulting subproblems we are interested in are linear quadratic control problems under constraints of

the form

min
y0,...,yτ ∈Rd
v0,...,vτ −1∈Rp

τ
(cid:88)

t=1

qht(yt) +

τ −1
(cid:88)

t=0

qgt(vt)

subject to yt+1 = (cid:96)t(yt, vt),

y0 = 0,

vt

t,

∈ U

(28)

t =

, qht are convex quadratics, qgt are strongly convex quadratics and (cid:96)t are linear dynamics.
where
}
Interior point methods introduce a log-barrier function

Ctu) and minimize

t(u) = log(dt

u : Ctu

dt

≤

U

{

B

−

τ
(cid:88)

qht(yt) +

min
y0,...,yτ ∈Rd
v0,...,vτ −1∈Rp
subject to yt+1 = (cid:96)t(yt, vt),

t=1

τ −1
(cid:88)

t=0

qgt(vt) + µk

t(vt)

B

y0 = 0,

where µk increases along the iterates k of the interior point method. We leave the exploration of constrained problems
to future work.

8

3 Automatic-differentiation oracle

The iterative composition structure we studied so far appears not only in control but more generally in optimization
problems that involve successive transformations of a given input as for example in

min
u0,...,uτ −1

h(φτ −1(φτ −2(. . . φ0(ˆx0, u0) . . . , uτ −2), uτ −1)).

(29)

The identiﬁcation of such structures led to the development of efﬁcient automatic-differentiation software libraries
able to compute gradients in any graph of computations both in CPUs and GPUs. We present then implementations
and complexities of the optimization methods presented before where automatic-differentiation is the computational
bottleneck.

Functions and problem deﬁnition. We ﬁrst recall the deﬁnition of decomposable functions along the trajectories.

Deﬁnition 3.1. A function f : Rτ d
ft : Rd

such that for ¯x = (x1; . . . ; xτ )

→

Rτ d(cid:48)

is a multivariate τ -decomposable function if it is composed of τ functions

.
R is a real τ -decomposable function if it is composed of τ functions ft : Rd

Rτ d, we have f (¯x) = (f1(x1); . . . ; fτ (xτ ))

∈

∈

Rτ d(cid:48)

R such that

→

→
Rτ d, we have f (¯x) = (cid:80)τ
τ (Rτ d, Rτ d(cid:48)

) and

∈

t=1 ft(xt).

τ (Rτ d) the sets of multivariate and real, respectively, τ -decomposable functions

→

Rd(cid:48)
A function f : Rτ d
for ¯x = (x1; . . . ; xτ )
We denote by

D

whose components ft are differentiable.

D

For a given decomposable function f
f1(x1)z1; . . . ;

fτ (xτ )zτ )

f (¯x)¯z = (

∇
f . Similarly, the convex conjugate of a real decomposable function f
conjugate of its components.

∇

∇

∈ D

∈ D
∈

) and a point ¯z

τ (Rτ d, Rτ d(cid:48)
, the gradient-vector product reads
Rτ d, i.e., it can be computed directly from the components deﬁning
τ (Rτ d) is directly given by the convex

Rτ d(cid:48)

∈

We formalize now the class of trajectory functions.

Deﬁnition 3.2 (Trajectory function). A function ˜x : Rτ p
an input ˆx0
have ˜x(¯u) = (˜x1(¯u); . . . ; ˜xτ (¯u)) deﬁned by

→
Rd and τ compositions of functions φt : Rd

∈

Rτ d is a trajectory function of horizon τ if it is deﬁned by
Rτ p, we
Rd such that for ¯u = (u0; . . . ; uτ −1)

Rp

×

→

∈

˜x1(¯u) = φ0(ˆx0, u0),
τ (Rτ p, Rτ d) the set of trajectory functions of horizon τ whose dynamics φt are differentiable.

˜xt+1(¯u) = φt(˜xt(¯u), ut),

for t = 1, . . . , τ

−

1.

We denote by

T

As presented in Section 2, the gradient back-propagation is divided in two main phases: (i) the forward pass that
computes and store the gradients of the dynamics along the trajectory given by a command, (ii) the backward and roll-
out passes that compute the gradient of the objective given the gradients of the costs and penalties along the trajectory.
We can decouple the two phases by computing and storing once and for all the gradients of the dynamics along the
trajectory, then making calls to the backward and roll-out passes for any dual inputs, i.e., not restricting ourselves to
the gradients of the costs and penalties along the trajectories.

∈ T

Formally, given ˜x

τ (Rτ p, Rτ d) and ¯u

Rτ p, we use that, once ˜x(¯u) is computed and the successive gradi-
Rτ d can be computed in linear time with
ents are stored, any gradient vector product of the form
∇
respect to τ by a dynamic programming procedure (speciﬁcally an automatic-differentiation software) that solves
2
2. The main difference with classical optimization oracles is that we do not compute
min¯v∈Rτ p
¯v
2 (cid:107)
−
(cid:107)
Rτ p×τ d but yet have access to gradient-vector products ¯z
or store the gradient
˜x(¯u)¯z. This lead us to
∈
deﬁne oracles for trajectory functions as calls to an automatic-differentiation procedure as follows.

˜x(¯u)(cid:62)¯v + 1
˜x(¯u)

˜x(¯u)¯z for ¯z

→ ∇

¯z(cid:62)

∇

∇

∈

∈

Deﬁnition 3.3 (Automatic-differentiation oracle). An automatic-differentiation oracle is any procedure that, given
˜x

τ (Rτ p, Rτ d) and ¯u

Rτ p, computes

∈ T

∈

¯z

˜x(¯u)¯z

→ ∇

for any ¯z

Rτ d.

∈

Derivatives of the gradient vector product can then be computed themselves by back-propagation as recalled in the

following lemma.

9

Lemma 3.4. Given a trajectory function ˜x
f

τ (Rτ p), the derivative of ¯z

f (

∈ T

∈ D

→

∇

τ (Rτ p, Rτ d), a command ¯u

Rτ p and a real decomposable function

˜x(¯u)¯z) requires two calls to an automatic-differentiation procedure.

∈

Proof. We describe the backward pass of Prop. 2.3 as a function of ¯z, the computations are the same except that
˜x(¯u)¯z deﬁnes a linear
¯a is replaced by
˜θ(¯λ) =
trajectory function ˜λ : ¯z
(˜θ0(λ1); . . . ˜θτ −1(λτ ))

Rτ d and a linear decomposable function ˜θ : ¯λ

Rτ d, the backward pass that computes

¯z. Given ¯z = (z1; . . . ; zτ )

∈
(˜λ1(¯z); . . . ; ˜λτ (¯z))

→

∇

−

∈

→
Rτ p as
∈
˜λτ (¯z) =

zτ ,
−
˜θt(λt+1) =

˜λt(¯z) = Φt,x

˜λt+1(¯z)

zt

for t = τ

1, . . . , 1,

Φt,uλt+1

for t = 0, . . . , τ

−

−
1,

∇

xφt(xt, ut), Φt,u=

−
uφt(xt, ut) and xt = ˜xt(¯u). The function we are interested in reads then
where Φt,x=
˜x(¯u)¯z) = f (˜θ(˜λ(¯z))). Its derivative amounts then to compute the linear trajectory function ˜λ(¯z) by one call to an
f (
automatic differentiation procedure, then to back-propagate through this linear trajectory function by another call to
an automatic-differentiation procedure. The derivatives of the decomposable functions can be directly computed from
their individual components.

∇

∇

−

We focus on problems that involve only a ﬁnal state cost as in (29) or in the experiments presented in Section 5.

Formally those problems read

min
¯u∈Rτ p

h(˜xτ (¯u)) + g(¯u),

(30)

where ˜x is trajectory function of horizon τ , h is a cost function and g is a real τ -decomposable penalty. Denote by
P (˜x, h, g), the problem (30) for a given choice of ˜x, h, g. We present complexities of the oracles deﬁned before for
, h
deﬁned by a class of trajectory functions
classes of problems
∈ T
∈ G}
f : Rd
and a class of decomposable
{
τ (Rτ d). Inclusions of classes of functions deﬁne inclusions of the problems. The class of

(
T
τ (Rτ p, Rτ d), a class of state cost

T ⊂ T
penalty function
problems for which we can provide iteration complexity is deﬁned by

R, f differentiable
}

P (˜x, h, g) : ˜x

(Rd) =

G ⊂ D

H ⊂ F

∈ H

) =

→

, g

H

P

G

{

,

,

=

=

=

• T

• H

• G

α (Rτ p, Rτ d) the class of trajectory functions of horizon τ with α-continuously differentiable dynamics,
τ

T

L(Rd) the class of quadratic convex functions L-smooth,

Q
L(Rτ p) =
τ
α (Rτ p, Rτ d) is α-continuously differentiable. In the rest of this section we provide the oracle
τ

τ (Rτ p) the class of quadratic τ -decomposable functions L-smooth.

Note that any ˜x
complexity of oracles for this problem general classes of problems detailed each time.

L(Rτ p)

∩ D

∈ T

Q

Q

Model-minimization steps with automatic-differentiation oracles. Now we precise the feasibility and the com-
plexity of the inner-steps of the steps deﬁned in Section 2 in terms of the class of problems and the automatic-
differentiation oracle deﬁned above. The total complexity of the algorithms, when available, are presented in Sec-
tion 4.

Gradient step. For any problem belonging to
pute

h(˜xτ (¯u)) and

˜xτ (¯u)

(

P

T

∇

∇

∇

g(¯u) by a single call to an automatic-differentiation oracle.

F

D

τ (Rτ p, Rτ d),

(Rd),

τ (Rτ p)), a gradient step amounts to com-

Regularized Gauss-Newton step.

In the setting (30), the regularized Gauss-Newton step (14) amounts to solve

h (cid:0)˜xτ (¯u) +

min
¯v∈Rτ p

∇

˜xτ (¯u)(cid:62)¯v(cid:1) + g(¯u + ¯v) +

1
2γ (cid:107)

2
2.

¯v
(cid:107)

(31)

For smooth objectives h and g, this is a smooth strongly convex problem that can be solved approximately by a linearly
convergent ﬁrst order method, leading to the inexact regularized Gauss-Newton procedures described in (Drusvyatskiy
& Paquette, 2018). The overall cost of an approximated regularized Gauss-Newton step is then given by the following
(Rτ p, Rτ d) the class of trajectory functions of horizon τ whose dynamics φt
proposition. We deﬁne (i)

τ
α,L0,...,Lα

T

10

α,β,Lβ (Rd) the class of convex functions α-differentiable whose
are Lβ-Lipschitz-continuous for all β
C
τ (Rτ p) the class of τ -
α,β,Lβ (Rτ p)
β-derivative, for β
(Rτ p, Rτ d)
decomposable convex functions with corresponding smoothness properties. Note that any ˜x
has a Lipschitz continuous β-derivative for β

≤
α, is Lβ-Lipschitz continuous and (iii)

τ
α,L0,...,Lα

∩ D
∈ T

(Rτ p) =

τ
α,β,Lβ

α, (ii)

α.

≤

C

C

(Rτ p)) deﬁned in (30), an
Proposition 3.5. For problems belonging to
approximate regularized Gauss-Newton step given by (31) is solved up to ε accuracy by a fast gradient method with
at most

1,L0,L1(Rτ p, Rτ d),

C1,1,Lh

τ
1,1,Lg
1

(Rd),

(
T

P

C

1

≤

(cid:18)(cid:113)

O

Lh

1 M 2

0 γ + Lg

1γ + 1 log(ε)

(cid:19)

,

calls to an automatic differentiation oracle, where M0 is the Lipschitz-continuity of ˜xτ .

P

(
T

1,L0,L1(Rτ p, Rτ d),

Proof. For problems
0 +Lg
1 +γ−1 smooth and γ−1 strongly convex. Therefore to achieve ε accuracy, a fast gradient method requires
1 M 2
is Lh
˜xτ (¯u)(cid:62)¯v) + g(¯u +
at most
¯v) + 1
Rd, which
costs two calls to an automatic differentiation procedure according to Lem. 3.4. An additional call to an automatic
differentiation oracle is then needed to compute

((cid:112)(Lh
O
2
¯v
2. Each call requires to compute
2γ (cid:107)
(cid:107)

1 + γ−1)/γ−1 log(ε)) calls to ﬁrst order oracles of ¯v

→
˜xτ (¯u)(cid:62)¯v, that is the derivative of z

(Rτ p)), the regularized Gauss-Newton subproblem (30)

∇
˜xτ (¯u)z for z

h(˜xτ (¯u) +

0 + Lg

C1,1,Lh

τ
1,1,Lg
1

1 M 2

(Rd),

˜xτ (¯u)(cid:62)¯v).

h(˜xτ (¯u) +

˜xτ (¯u)

v(cid:62)

→

∇

∇

∈

C

1

∇

∇

∇

Levenberg-Marquardt step.

In the setting (30), the Levenberg-Marquardt step (16) amounts to solve

(cid:0)˜xτ (¯u) +

qh

min
¯v∈Rτ p

∇

˜xτ (¯u)(cid:62)¯v; ˜xτ (¯u)(cid:1) + qg(¯u + ¯v; ¯u) +

1
2γ (cid:107)

2
2,

¯v
(cid:107)

(32)

where qh and qg are quadratic approximations of h and g respectively, both being assumed to be twice differentiable.
Here, duality offers a fast resolution of the step as shown in the following proposition. It shows that its cost is only 2d+
1 times more than one of a gradient step. Recall also that for h, g quadratics the Levenberg-Marquardt step amounts to
α(Rd) the class of convex functions α-continuously differentiable and
a regularized Gauss-Newton step. We deﬁne (i)
τ (Rτ p) the class of τ -decomposable convex functions with corresponding differentiation
(ii)
C
properties.

α(Rτ p) =
τ

α(Rτ p)

∩ D

C

C

Proposition 3.6. For problems belonging to
P
step (32) is solved exactly with at most 2d + 1 calls to an automatic differentiation oracle.

(
T

τ (Rτ p, Rτ d),

2(Rd),

C

C

τ
2 (Rτ p)) deﬁned in (30), a Levenberg-Marquardt

Proof. The dual problem of the Levenberg-Marquardt step (32) reads

min
z∈Rd

r∗(z) + s∗(

−∇

˜xτ (¯u)z),

(33)

where r(x) = qh (˜xτ (¯u) + x; ˜xτ (¯u)), s(¯v) = qg(¯u+ ¯v; ¯u)+ 1
2 and r∗, s∗ are their respective conjugate functions
2
that can be computed in closed form. Note that, as s is τ decomposable, so is s∗. The dual problem can then be solved
in d iterations of a conjugate gradient method, each iteration requires to compute the gradient of s∗(
˜xτ (¯u)z).
According to Lem. 3.4 this amounts to two calls to an automatic differentiation oracle. A primal solution is then given
˜xτ (¯u)z∗) which is given by an additional call to an automatic differentiation oracle.
by ¯v∗ =

¯v
2γ (cid:107)
(cid:107)

−∇

s∗(

∇

−∇

4 Composite optimization

Before analyzing the methods of choice for composite optimization, we review classical algorithms for nonlinear
control and highlight improvements for better convergence behavior. All algorithms are completely detailed in Ap-
pendix C.

4.1 Optimal control methods

Differential Dynamic Programming. Differential Dynamic Programming (DDP) is presented as a dynamic pro-
gramming procedure applied to a second order approximation of the Bellman equation (Jacobson & Mayne, 1970).

11

Formally at a given command ¯u with associated trajectory ¯x = ˜x(¯u), it consists in approximating the cost-to-go
functions as

ct(y) = qht(xt + y; xt) + min
v {

qgt(ut + v; ut) + qct+1◦φt(xt + y, ut + v; xt, ut)
}

where for a function f (y), qf (y; x) denotes its second order approximation around x. The roll-out pass is then
performed on the true trajectory as normally done in a dynamic programming procedure. We present an interpretation
of DDP as an optimization on the state variables in Appendix D.

ILQR, ILQG (Li & Todorov, 2004; Todorov & Li, 2005; Li & Todorov, 2007). DDP was superseded by the
Iterative Linearized Quadratic Regulator (ILQR) method, presented in Section 1 (Li & Todorov, 2004). In the case of
noisy dynamics, the Linear Quadratic Regulator problem (8) was replaced by a Linear Quadratic Gaussian problem
where the objectives are averaged with respect to the noise, the iterative procedure was then called ILQG as presented
in (Todorov & Li, 2005; Li & Todorov, 2007).

Prop. 2.1 clariﬁes that these procedures, as deﬁned in (Li & Todorov, 2004; Todorov & Li, 2005; Li & Todorov,

2007), amount to compute

¯v∗ = arg min
¯v∈Rτ p

qf (¯u + ¯v; ¯u)

(34)

to perform a line-search along its direction such that f (¯u + α¯v∗)
f (¯u). For ILQR the model qf is deﬁned as in
(15), while for ILQG this corresponds to the model deﬁned in (25) with quadratic models qf and qg. Compared to a
Levenberg-Marquardt step (16), that reads

≤

¯u+ = ¯u + arg min
¯v∈Rτ p

(cid:26)

qf (¯u + ¯v; ¯u) +

(cid:27)

,

1
2γ (cid:107)

2
2

¯v
(cid:107)

(35)

we see that those procedures do not take into account the inaccuracy of the model far from the current point. Although
a line-search can help ensuring convergence, no rate of convergence is known. For quadratics ht, gt, the Levenberg-
Marquardt steps become regularized Gauss-Newton steps whose analysis shows the beneﬁts of the regularization term
in (35) to ensure convergence to a stationary point.

ILQG (Tassa et al., 2012). The term ILQG has often been used to refer to an algorithm combining ideas from DDP
and ILQR resp. (Tassa et al., 2012). The general structure proposed then is akin to DDP in the sense that it uses a
dynamic programming approach where the cost-to-go functions are approximated. However, as in ILQR, only the ﬁrst
order derivatives of the dynamics are taken into account to approximate the cost-to-go functions. Formally, at a given
command ¯u with associated trajectory ¯x = ˜x(¯u), ILQG consists in approximating the cost-to-go functions as

ct(y) = qht(xt + y; xt) + min
v {

qgt(ut + v; ut) + qct+1(φt(xt) +

φt(xt, ut)(cid:62)v; xt)
}

∇

While the cost-to-go functions are the same as in (Li & Todorov, 2004), the roll-out pass is then performed on the true
trajectory and not the linearized one. The analysis is therefore similar to the one of DDP. We leave it for future work
and focus on the original deﬁnition of ILQR given in (Li & Todorov, 2004).

4.2 Regularized ILQR via regularized Gauss-Newton

We present convergence guarantees of the regularized Gauss-Newton method for composite optimization problems of
the form

min
¯u∈Rτ d

f (¯u) = h(˜x(¯u)) + g(¯u),

(36)

R and g : Rτ p

where h : Rτ d
Rτ d is differentiable with continuous
gradients. The regularized Gauss-Newton method then naturally leads to a regularized ILQR. In the following, we
denote by Lh and Lg the smoothness constants of respectively h and g and by (cid:96)˜x,S the Lipschitz constant of ˜x on the
initial sub-level set S =

R are convex quadratic, and ˜x : Rτ p

→

→

→

.

¯u : f (¯u)
{

f (¯u0)
}

≤

12

The regularized Gauss-Newton method consists in iterating, starting from a given ¯u0,

¯uk+1 = ¯uk + arg min
¯v∈Rτ p

cf (¯uk+¯v; ¯uk) +

(cid:26)

1
2
¯v
2
2γk (cid:107)
(cid:107)

(cid:27)

,

(37)

We use ¯uk+1 = GN(uk; γk) to denote (37) hereafter. The convergence is stated in terms of the difference of iterates
that, in this case, can directly be linked to the norm of the gradient, denoting H =

2h(¯x) and G =

2g(¯u),

¯uk+1 = ¯uk

(

∇

−

˜x(¯uk)H

∇

˜x(¯uk)(cid:62) + G + γ−1

k

Iτ p)−1

∇

∇
f (¯uk).

∇

(38)

The convergence to a stationary point is guaranteed as long as we are able to get a sufﬁcient decrease condition

when minimizing this model as stated in the following proposition.

Proposition 4.1. Consider a composite objective f as in (36) with convex models cf (
the step sizes γk of the regularized Gauss-Newton method (37) are chosen such that

; ¯u) deﬁned in (13). Assume that
·

f (¯uk+1)

≤

cf (¯uk+1; ¯uk) +

1
2γk (cid:107)

¯uk+1

¯uk

2
2
(cid:107)

−

(39)

and γmin

γk

γmax.

≤

≤

Then the objective value decreases over the iterations and the sequence of iterates satisﬁes

where L = maxγ∈[γmin,γmax] γ((cid:96)2

min

k=0,...,N (cid:107)∇

2
(cid:107)
˜x,SLh + Lg + γ−1)2 and f ∗ = limk→+∞ f (¯uk).

−
N + 1

f (¯uk)

≤

,

2L(f (¯u0)

f ∗)

To ensure the sufﬁcient decrease condition, one needs the model to approximate the objective up to a quadratic

error which is ensured on any compact set as stated in the following proposition.

Lemma 4.2. Consider a composite objective f as in (36) with convex models cf (
·
set C

Rτ p there exists MC > 0 such that for any ¯u, ¯v

C,

; ¯u) deﬁned in (13). For any compact

⊂

∈
cf (¯v; ¯u)

f (¯v)

|

−

MC

¯v
2 (cid:107)

2
2.

¯u
(cid:107)

−

(40)

| ≤

Finally one needs to ensure that the iterates stay in a bounded set which is the case for sufﬁciently small step-sizes

such that the sufﬁcient decrease condition is satisﬁed along the sequence of iterates generated by the algorithm.

Lemma 4.3. Consider a composite objective f as in (36). For any k such that ¯uk
is the initial sub-level set, any step-size

∈

S, where S =

¯u : f (¯u)
{

≤

f (¯u0)

}

(41)
ensures that the sufﬁcient decrease condition (39) is satisﬁed, where (cid:96)f,S is the Lipschitz constant of f on S, C =
S + B1 with B1 the unit Euclidean ball centered at 0 and MC ensures (40).

f,S, M −1
(cid:96)−1
ˆγ = min
C }
{

γk

≤

Combining Prop. 4.1 and Lem. 4.2, we can guarantee that the iterates stay in the initial sub-level set and satisfy
the sufﬁcient decrease condition for sufﬁciently small step-sizes γk. At each iteration the step-size can be found by a
line-search guaranteeing sufﬁcient decrease; see Appendix E for details. The ﬁnal complexity of the algorithm with
line-search then follows.

Corollary 4.4. For a composite objective f as in (36), the regularized Gauss-Newton method (37) with a decreasing
line-search starting from γ0

ˆγ with decreasing factor ρ ﬁnds an ε-stationary point after at most

≥

2L(f (¯u0)
ε2

−

f ∗)

+ log(γ0/ˆγ)/ log(ρ−1)

calls to the regularized Gauss-Newton oracle, with ˆγ deﬁned in (41), f ∗ = limk→+∞ f (¯uk) and

L = max

γ∈[ˆγ,γ0]

γ((cid:96)2

˜x,SLh + Lg + γ−1)2.

13

Global convergence guarantee
Number of calls to auto-differentiation oracle
Cost per call to auto-differentiation oracle

GD
Yes
τ (pd + d2)
1

ILQR
No
τ p3d3
2d + 1

RegILQR
Yes
τ p3d3
2d + 1

Table 1: Convergence properties and oracle costs of Gradient Descent (GD), ILQR, and regularized ILQR (RegILQR)
for problem (2) with quadratic h, g . The automatic-differentiation oracle cost is stated for problems of the form (30) .

Algorithm 1 Accelerated Regularized Gauss-Newton

Input: Composite objective f in (10) with convex models cf as in (13). Initial ¯u0
Initialize: α1 := 1, ¯z0 := ¯u0
Repeat: for k = 1, 2, . . .
1: Compute regularized step

∈

Get ¯vk = GN(¯uk−1; γk) by line-search on γk s.t.

Rτ p, desired accuracy ε.

f (¯vk)

cf (¯vk; ¯uk−1) +

≤

2: Compute extrapolated step
- Set ¯yk = αk ¯zk−1 + (1
- Get ¯wk = GN(¯yk; δk) by line-search on δk s.t.

αk)¯uk−1.

−

1
2γk (cid:107)

¯vk

¯uk−1

2
2.
(cid:107)

−

- Set ¯zk = ¯uk−1 + ( ¯wk
- Pick αk+1

−
(0, 1) s.t. (1

∈

3: Pick best of two steps
Choose ¯uk such that
until ε-near stationarity

−

f ( ¯wk)

cf ( ¯wk; ¯yk) +

≤

1
2δk (cid:107)

¯wk

¯yk

2
2.

(cid:107)

−

¯uk−1)/αk.

αk+1)/α2

k+1 = 1/α2
k.

f (¯uk)

min

f (¯vk), f ( ¯wk)
}
{

≤

f (¯uk)
(cid:107)

(cid:107)∇

< ε

(43)

(44)

4.3 Accelerated ILQR via accelerated Gauss-Newton

In Algo. 1 we present an accelerated variant of the regularized Gauss-Newton algorithm that blends a regularized
Gauss-Newton step and an extrapolated step to potentially capture convexity in the objective. See Appendix F for the
proof.

Proposition 4.5. Consider Algo. 1 applied to a composite objective f as in (36) with decreasing step-sizes (γk)k≥0
and (δk)k≥0. Then Algo. 1 satisﬁes the convergence of the regularized Gauss-Newton method (37) with line-search as
presented in Cor. 4.4. Moreover, if the convex models cf (¯v; ¯u) deﬁned in (13) lower bound the objective as

for any ¯u, ¯v

∈

Rτ p, then after N iterations of Algo. 1,

f (¯uN )

f ∗

−

≤

4δ−1

¯u0

¯u∗
(cid:107)
(N + 1)2

−

2

(cid:107)

,

cf (¯v; ¯u)

f (¯v)

≤

(42)

where δ = mink∈{1,...N } δk, f ∗ = min¯u f (¯u) and ¯u∗

arg min¯u f (¯u).

∈

4.4 Total complexity with automatic-differentiation oracles

Previous results allow us to state the total complexity of the regularized ILQR algorithm in terms of calls to automatic
differentiation oracles as done in the following corollary that combines Cor. 4.4 and Prop. 4.5 with Prop. 3.6. A similar
result can be obtained for the accelerated variant. Table 1 summarizes then convergence properties and computational
costs of classical methods for discrete time non-linear control.

14

τ
Corollary 4.6. Consider problems
Lg
Q
Newton method (37) with a decreasing line-search starting from γ0
point after at most

1(Rτ p, Rτ d),

Lh (Rd),

Q

P

T

(

≥

(Rτ p)) deﬁned in (30). The regularized Gauss-
ˆγ with decreasing factor ρ ﬁnds an ε-stationary

(2d + 1)

(cid:18) 2L(f (¯u0)
ε2

f ∗)

−

+ log(γ0/ˆγ)/ log(ρ−1)

(cid:19)

calls to an automatic differentiation oracle, with ˆγ deﬁned in (41), L = maxγ∈[ˆγ,γ0] γ((cid:96)2
the Lipschitz constant of ˜x on the initial sub-level set S =

f (¯u0)

and f ∗ = limk→+∞ f (¯uk)

˜x,SLh + Lg + γ−1)2, (cid:96)˜x,S is

¯u : f (¯u)
{

≤

}

5 Experiments

We illustrate the performance of the algorithms considered in Sec. 4 including the proposed accelerated regularized
Gauss-Newton algorithm on two classical problems drawn from (Li & Todorov, 2004): swing-up a pendulum, and
move a two-link robot arm.

5.1 Control settings

The physical systems we consider below are described by continuous dynamics of the form

¨z(t) = f (z(t), ˙z(t), u(t))

where z(t), ˙z(t), ¨z(t) denote respectively the position, the speed and the acceleration of the system and u(t) is a force
applied on the system. The state x(t) = (x1(t), x2(t)) of the system is deﬁned by the position x1(t) = z(t) and the
speed x2(t) = ˙z(t) and the continuous cost is deﬁned as

J(x, u) =

(cid:90) T

0

h(x(t))dt +

(cid:90) T

0

g(u(t))dt

or J(x, u) = h(x(T )) +

(cid:90) T

0

g(u(t))dt,

where T is the time of the movement and h, g are given convex costs. The discretization of the dynamics with a time
step δ starting from a given state ˆx0 = (z0, 0) reads then

x1,t+1 = x1,t + δx2,t
x2,t+1 = x2,t + δf (x1,t, x2,t, ut)

for t = 0, . . . , τ

1

−

(45)

where τ =

T /δ
(cid:100)

(cid:101)

and the discretized cost reads

J(¯x, ¯u) =

τ
(cid:88)

t=1

h(xt) +

τ −1
(cid:88)

t=0

g(ut) or J(¯x, ¯u) = h(xτ ) +

τ −1
(cid:88)

t=0

g(ut).

Pendulum. We consider a simple pendulum illustrated in Fig. 1, where m = 1 denotes the mass of the bob, l = 1
denotes the length of the rod, θ describes the angle subtended by the vertical axis and the rod, and µ = 0.01 is the
friction coefﬁcient. The dynamics are described by

¨θ(t) =

g
l

−

sin θ(t)

µ
ml2

−

˙θ(t) +

1
ml2 u(t)

The goal is to make the pendulum swing up (i.e. make an angle of π radians) and stop at a given time T . The cost
writes as

(cid:90) T

0

u2(t)dt,

(46)

J(x, u) = (π

−

x1(T ))2 + λ1x2(T )2 + λ2

where x(t) = (θ(t), ˙θ(t)), λ1 = 0.1, λ2 = 0.01, T = 5.

15

Figure 1: Control settings considered. From left to right: pendulum, two-link arm robot.

Two-link arm. We consider the arm model with two joints (shoulder and elbow), moving in the horizontal plane
presented in (Li & Todorov, 2004) and illustrated in 1. The dynamics are described by

M (θ(t))¨θ(t) + C(θ(t), ˙θ(t)) + B ˙θ(t) = u(t),

(47)

where θ = (θ1, θ2) is the joint angle vector, M (θ)
is a vector centripetal and Coriolis forces, B
we control. We drop the dependence on t for readability. The dynamics are then

R2×2 is the joint friction matrix, and u(t)

∈

∈

∈

R2×2 is a positive deﬁnite symmetric inertia matrix, C(θ, ˙θ)

R2
R2 is the joint torque that

∈

−
The expressions of the different variables and parameters are given by

−

¨θ = M (θ)−1(u

C(θ, ˙θ)

B ˙θ).

M (θ) =

(cid:18)a1 + 2a2 cos θ2 a3 + a2 cos θ2

(cid:19)

a3 + a2 cos θ2

a3

B =

(cid:18)b11
b21

(cid:19)

b12
b22

C(θ, ˙θ) =

a2 sin θ2

(cid:18)

(cid:19)

−

˙θ2(2 ˙θ1 + ˙θ2)
˙θ2
1
a1 = k1 + k2 + m2l2
1
a2 = m2l1d2
a3 = k2,

(48)

(49)

(50)

where b11 = b22 = 0.05, b12 = b21 = 0.025, li and ki are respectively the length (30cm, 33cm) and the moment of
inertia (0.025kgm2 , 0.045kgm2) of link i , m2 and d2 are respectively the mass (1kg) and the distance (16cm) from
the joint center to the center of the mass for the second link.

The goal is to make the arm reach a feasible target θ∗ and stop at that point. The objective reads

J(x, u) =

θ(T )
(cid:107)

−

θ∗

(cid:107)

2 + λ1

(cid:107)

˙θ(T )

2 + λ2
(cid:107)

(cid:90) T

0 (cid:107)

2dt,

u(t)
(cid:107)

(51)

where x(t) = (θ(t), ˙θ(t)), λ1 = 0.1, λ2 = 0.01, T = 5.

5.2 Results

We use the automatic differentiation capabilities of PyTorch (Paszke et al., 2017) to implement the automatic differ-
entiation oracles introduced in Sec. 3. The Gauss-Newton-type steps in Algo. 1 are computed by solving the dual
problem associated as presented in Sec. 3.

In Figure 2, we compare the convergence, in terms of function value and gradient norm, of ILQR (based on
Gauss-Newton), regularized ILQR (based on regularized Gauss-Newton), and accelerated regularized ILQR (based on
accelerated regularized Gauss-Newton). These algorithms were presented in Sec. 4.

For ILQR, we use an Armijo line-search to compute the next step. For both the regularized ILQR and the acceler-
ated regularized ILQR, we use a constant step-size sequence tuned after a burn-in phase of 5 iterations. We leave the
exploration of more sophisticated line-search strategies for future work.

16

12z*z()Figure 2: Convergence of ILQR, regularized ILQR and accelerated regularized ILQR on the inverted pendulum (top)
and two-link arm (bottom) control problems for an horizon τ = 100.

The plots show stable convergence of the regularized ILQR on these problems. The proposed accelerated regu-
larized Gauss-Newton algorithm displays stable and fast convergence. Applications of accelerated regularized Gauss-
Newton algorithms to reinforcement learning problems would be interesting to explore (Recht, 2018; Fazel et al.,
2018; Dean et al., 2018).

17

0102030Iterations10−910−610−3100FunctionvaluesILQRRegILQRAccRegILQR05101520Iterations10−1010−710−410−1GradientnormILQRRegILQRAccRegILQR0102030Iterations10−1010−710−410−1FunctionvaluesILQRRegILQRAccRegILQR0102030Iterations10−1110−810−510−2GradientnormILQRRegILQRAccRegILQRAcknowledgements

We would like to thank Aravind Rajeswaran for pointing out additional references. This work was funded by NIH R01
(#R01EB019335), NSF CCF (#1740551), CPS (#1544797), DMS (#1651851), DMS (#1839371), NRI (#1637748),
ONR, RCTA, Amazon, Google, and Honda.

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin,
M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur,
M., Levenberg, J., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,
Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Wattenberg,
M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.
URL https://www.tensorflow.org/.

Bellman, R. Introduction to the mathematical theory of control processes, volume 2. Academic press, 1971.

Bertsekas, D. P. Dynamic programming and optimal control. Athena Scientiﬁc, 3rd edition, 2005.

Bjorck, A. Numerical methods for least squares problems. SIAM, 1996.

Burke, J. V. Descent methods for composite nondifferentiable optimization problems. Mathematical Programming,

33(3):260–279, 1985.

Cartis, C., Gould, N. I. M., and Toint, P. L. On the evaluation complexity of composite function minimization with

applications to nonconvex nonlinear programming. SIAM Journal on Optimization, 21(4):1721–1739, 2011.

De O. Pantoja, J. Differential dynamic programming and Newton’s method. International Journal of Control, 47(5):

1539–1553, 1988.

Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S. Regret bounds for robust adaptive control of the linear quadratic

regulator. In Advances in Neural Information Processing Systems, pp. 4188–4197, 2018.

Dontchev, A. L., Huang, M., Kolmanovsky, I. V., and Nicotra, M. M.

Inexact Newton-Kantorovich methods for

constrained nonlinear model predictive control. IEEE Transactions on Automatic Control, 2018.

Drusvyatskiy, D. and Paquette, C. Efﬁciency of minimizing compositions of convex functions and smooth maps.

Mathematical Programming, pp. 1–56, 2018.

Dunn, J. C. and Bertsekas, D. P. Efﬁcient dynamic programming implementations of Newton’s method for uncon-

strained optimal control problems. Journal of Optimization Theory and Applications, 63(1):23–38, 1989.

Fazel, M., Ge, R., Kakade, S., and Mesbahi, M. Global convergence of policy gradient methods for the linear quadratic

regulator. In Proceedings of the 35th International Conference on Machine Learning, volume 80, 2018.

Griewank, A. and Walther, A. Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM,

2008.

Gr¨une, L. and Pannek, J. Nonlinear model predictive control. Springer, 2017.

Hansen, P. C., Pereyra, V., and Scherer, G. Least squares data ﬁtting with applications. JHU Press, 2013.

Jacobson, D. H. and Mayne, D. Q. Differential Dynamic Programming. Elsevier, 1970.

Kakade, S. M. and Lee, J. D. Provably correct automatic sub-differentiation for qualiﬁed programs. In Advances in

Neural Information Processing Systems, pp. 7125–7135, 2018.

18

Kaltenbacher, B., Neubauer, A., and Scherzer, O. Iterative regularization methods for nonlinear ill-posed problems,

volume 6. Walter de Gruyter, 2008.

LeCun, Y., Touresky, D., Hinton, G., and Sejnowski, T. A theoretical framework for back-propagation. In Proceedings

of the 1988 connectionist models summer school, volume 1, pp. 21–28, 1988.

Lewis, A. S. and Wright, S. J. A proximal method for composite minimization. Mathematical Programming, 158:

501–546, 2016.

Li, W. and Todorov, E. Iterative linear quadratic regulator design for nonlinear biological movement systems. In 1st

International Conference on Informatics in Control, Automation and Robotics, volume 1, pp. 222–229, 2004.

Li, W. and Todorov, E. Iterative linearization methods for approximately optimal control and estimation of non-linear

stochastic system. International Journal of Control, 80(9):1439–1453, 2007.

Liao, L.-Z. and Shoemaker, C. A. Convergence in unconstrained discrete-time differential dynamic programming.

IEEE Transactions on Automatic Control, 36(6):692–706, 1991.

Liao, L.-Z. and Shoemaker, C. A. Advantages of differential dynamic programming over Newton’s method for

discrete-time optimal control problems. Technical report, Cornell University, 1992.

Mayne, D. A second-order gradient method for determining optimal trajectories of non-linear discrete-time systems.

International Journal of Control, 3(1):85–95, 1966.

Nesterov, Y. Modiﬁed Gauss-Newton scheme with worst case guarantees for global performance. Optimization

Methods & Software, 22(3):469–483, 2007.

Nocedal, J. and Wright, S. J. Numerical Optimization. Springer, 2nd edition, 2006.

Paquette, C., Lin, H., Drusvyatskiy, D., Mairal, J., and Harchaoui, Z. Catalyst for gradient-based nonconvex optimiza-

tion. In 21st International Conference on Artiﬁcial Intelligence and Statistics, pp. 1–10, 2018.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,

A. Automatic differentiation in PyTorch, 2017. URL https://pytorch.org/.

Recht, B. A tour of reinforcement learning: The view from continuous control. Annual Review of Control, Robotics,

and Autonomous Systems, 2018.

Richter, S., Jones, C. N., and Morari, M. Computational complexity certiﬁcation for real-time MPC with input
constraints based on the fast gradient method. IEEE Transactions on Automatic Control, 57(6):1391–1403, 2012.

Sideris, A. and Bobrow, J. E. An efﬁcient sequential linear quadratic algorithm for solving nonlinear optimal control

problems. In Proceedings of the American Control Conference, pp. 2275–2280, 2005.

Tassa, Y., Erez, T., and Todorov, E. Synthesis and stabilization of complex behaviors through online trajectory opti-
mization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4906–4913. IEEE,
2012.

Tassa, Y., Mansard, N., and Todorov, E. Control-limited differential dynamic programming. In IEEE International

Conference on Robotics and Automation, pp. 1168–1175, 2014.

Todorov, E. and Li, W. Optimal control methods suitable for biomechanical systems.

In Proceedings of the 25th

Annual International Conference of the IEEE, volume 2, pp. 1758–1761, 2003.

Todorov, E. and Li, W. A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear

stochastic systems. In Proceedings of the American Control Conference, pp. 300–306, 2005.

Whittle, P. Optimization over Time. John Wiley & Sons, Inc., New York, NY, USA, 1982.

19

A Notations

We use semicolons to denote concatenation of vectors, namely for τ d-dimensional vectors a1, . . . , aτ
(a1; . . . ; aτ )

Rτ d. The Kronecker product is denoted

.

Rd, we have

∈

∈

⊗

A.1 Tensors

A

A tensor
where Ak = (ai,j,k)i∈{1,...,d},j∈{1,...,n} ∈
Rp×p(cid:48)

= (ai,j,k)i∈{1,...,d},j∈{1,...,n},k∈{1,...,p} ∈
Rd×n for k

, we denote

Rd×n×p is represented as list of matrices
Rd×d(cid:48)

. Given matrices P
1, . . . p
}

∈ {

∈

= (A1, . . . , Ap)
Rn×n(cid:48)

, R

A
, Q

∈

∈

[P, Q, R] =

A

(cid:32) p

(cid:88)

k=1

Rk,1P (cid:62)AkQ, . . . ,

(cid:33)

Rk,p(cid:48)P (cid:62)AkQ

p
(cid:88)

k=1

Rd(cid:48)×n(cid:48)×p(cid:48)

∈

If P, Q or R are identity matrices, we use the symbol ”

[P, Q, Ip] =
A
particular, for x

[P, Q,
Rd, y

A
∈

” in place of the identity matrix. For example, we denote
] = (cid:0)P (cid:62)A1Q, . . . , P (cid:62)ApQ(cid:1). If P, Q or R are vectors we consider the ﬂatten object. In
·
∈

Rn, we denote

·

[x, y,

A

] =

·






x(cid:62)A1y
...
x(cid:62)Apy



Rp


 ∈

rather than having

[x, y,

A

]
·

∈

R1×1×p. Similarly, for z

Rp, we have

∈
p
(cid:88)

k=1

, z] =

,
[
·
A

·

zkAk

∈

Rd×n.

Finally note that we have for x

(x, y, R) =

A

∈
(cid:32) p

Rd, y

Rn and R

∈

Rp×p(cid:48)

,

∈

x(cid:62)AkyRk,1, . . . ,

(cid:33)(cid:62)

x(cid:62)AkyRk,p(cid:48)

= R(cid:62)

p
(cid:88)

k=1

[x, y,

A

]
·

∈

Rp(cid:48)

.

(cid:88)

k=1

For a tensor

= (ai,j,k)i∈{1,...,d},j∈{1,...,n},k∈{1,...,p} ∈

A

Rd×n×p, we denote

π = (ak,i,j)k∈{1,...,p},i∈{1,...,d},j∈{1,...,n} ∈
A

Rp×d×n.

the tensor whose indexes have been shifted onward. We then have for matrices P

Rd×d(cid:48)

, Q

Rn×n(cid:48)

, R

Rp×p(cid:48)

,

∈

∈

∈

For matrices, we have Aπ = A(cid:62) and for one dimensional vectors xπ = x.

(

[P, Q, R])π =

A

π[R, P, Q].

A

A.2 Gradients

Given a state space of dimension d, and control space of dimension p, for a real function of state and control f :
Rd+p on (x, u) as a part depending
Rd+p
R, whose value is denoted f (x, u), we decompose its gradient
on the state variables and a part depending on the control variables as follows

f (x, u)

(cid:55)→

∇

∈

(cid:18)

f (x, u) =

∇

(cid:19)

xf (x, u)
uf (x, u)

∇
∇

with

xf (x, u)

∇

Rd,

∈

uf (x, u)

∇

Rp.

∈

20

Similarly we decompose its Hessian
variables as follows

∇

f (x, u)

∈

R(d+p)×(d+p) on blocks that correspond to the state and control

with

xxf (x, u)

∇

∇

∈

(cid:18)

2f (x, u) =

xxf (x, u)
uxf (x, u)

∇
∇

∇
∇
Rp×p,

(cid:19)

xuf (x, u)
uuf (x, u)

Rd×d,

uuf (x, u)

∇

∈
Rn, composed of f (j) real functions with j

∇

xuf (x, u) =

uxf (x, u)(cid:62)

Rd×p.

∈

∇

For a multivariate function f : Rd
f (1)(x), . . . ,

f (n)(x))

(cid:55)→

(
∇
∈
Rd×n. We represent its second order information by a tensor

∇

Rd×n, that is the transpose of its Jacobian on x,

2f (x) = (

∇
2f (1)(x), . . . ,

1, . . . n
, we denote
}
∈ {
f (x) = ( ∂f (j)
∂xi

(x))1≤i≤d,1≤j≤n

∇

f (x) =

∈

2f (n)(x))

Rd×d×n

∈
We combine previous deﬁnitions to describe the dynamic functions. Given a state space of dimension d, a control
Rd+ and a pair
(cid:55)→
xφ(d+)(x, u)) and we deﬁne sim-
∇
xxφ(d+)(x, u)),
xxφ(1)(x, u), . . . ,

space of dimension p and an output space of dimension d+, for a dynamic function φ : Rd+p
of control and state variable (x, u), we denote
ilarly
∇
similarly for

xφ(x, u) = (
uφ(x, u). For its second order information we deﬁne

xxφ(x, u). Dimension of these deﬁnitions are

xφ(1)(x, u), . . . ,
xxφ(x, u) = (

∇
∇

∇

∇

∇

∇

∇

∇

∇

xφ(x, u)

∇

∈

Rd×d+,

uφ(x, u)

∇
xuφ(x, u)

∇

Rp×d+,
∇
Rd×p×d+,

∈

∈

xxφ(x, u)

Rd×d×d+,

∈

uxφ(x, u)

∇
Rp×d×d+.

∇

∈

uuφ(x, u)

∈

Rp×p×d+

B Dynamic programming for linear quadratic optimal control problems

We present the dynamic programming resolution of the Linear Quadratic control problem

min
x0,...,xτ ∈Rd
u0,...,uτ −1∈Rp

τ
(cid:88)

t=1

ht(xt) +

τ −1
(cid:88)

t=0

gt(ut)

subject to xt+1 = φt(xt, ut)

for t = 0, . . . , τ

x0 = ˆx0,

1

−

(52)

with

φ(x, u) = Φ(cid:62)

t,xx + Φ(cid:62)

t,uu,

ht(x) = h(cid:62)

t,xx +

1
2

x(cid:62)Ht,xx,

and

gt(u) = g(cid:62)

t,uu +

1
2

u(cid:62)Gt,uuu,

where Φt,x

0.
Dynamic programming applied to this problem is presented in Algo. 2 it is based on the following proposition that

0 and Gt,uu

Rp×p with Ht,xx

Rd×d, Gt,uu

Rd×d, Φt,u

Rp×d, ht,x

Rp, Ht,xx

Rd, gt,u

(cid:31)

(cid:23)

∈

∈

∈

∈

∈

∈

computes cost-to-go functions as quadratics.

Proposition B.1. Cost-to-go functions (20) to minimize are quadratics of the form, for t
1
2

t,xx, with Ct,xx

x(cid:62)Ct,xxx + c(cid:62)

ct(x) =

0.

(cid:23)

0, . . . , τ ,

∈

t)

(
H

The optimal control at time t from a state xt reads for t

0, . . . , τ

∈ {

−
u∗
t (xt) = Ktxt + kt

,
1
}

where Ct,xx, ct,x, Kt, kt are deﬁned recursively in lines 3, 5, 6, 7 of Algo. 2.

Proof. We prove recursively (

t). By deﬁnition of the cost-to-function, we have

H

cτ (x) = hτ (x),

so Cτ,xx = Hτ,xx,

cτ,x = hτ,x,

and by assumption on the costs Cτ,xx = Hτ,xx
τ

1, assume (

Then for 0

t

(cid:23)

≤

≤

−

H

0, which ensures (

τ ).

H

t+1), we search to compute

ct(x) = ht(x) + min
u {

gt(u) + ct+1(φt(x, u))

}

21

To follow the computations, we drop the dependency on time and denote by superscript (cid:48) the quantities at time t + 1,
e.g. c(cid:48) = ct+1. We therefore search an expression for

u {
The function W is a quadratic in (x, u) of the form

c(x) = h(x) + min

g(u) + c(cid:48)(φ(x, u))

= min

u

}

W (x, u).

(53)

W (x, u) = w(cid:62)

x x + w(cid:62)

u u +

1
2

x(cid:62)Wxxx +

1
2

u(cid:62)Wuuu + u(cid:62)Wuxx.

Developing the terms in (53) we have

W (x, u) = h(cid:62)

x x + g(cid:62)

u u +

x(cid:62)Hxxx +

u(cid:62)Guuu

1
2
u u)(cid:62)C (cid:48)

1
2
x x + Φ(cid:62)

+

(Φ(cid:62)

x x + Φ(cid:62)

1
2
xx is symmetric, we get

xx(Φ(cid:62)

u u) + (Φ(cid:62)

x x + Φ(cid:62)

u u)(cid:62)c(cid:48)
x.

By identiﬁcation and using that C (cid:48)

wx = gx + Φxc(cid:48)
x
wu = gu + Φuc(cid:48)
x

Wxx = Hxx + ΦxC (cid:48)
Wuu = Guu + ΦuC (cid:48)
Wux = ΦuC (cid:48)

xxΦ(cid:62)
x .

xxΦ(cid:62)
x
xxΦ(cid:62)
u

By assumption Guu

(cid:31)

0 and by (Ht+1), C (cid:48)

0, therefore Wuu

0, minimization in u in (53) is possible and reads

c(x) = w(cid:62)

x x +

1
2

xx (cid:23)
x(cid:62)Wxxx

(cid:31)
(Wuxx + wu)(cid:62)W −1

uu (Wuxx + wu),

with optimal control variable

u∗(x) =

W −1

uu (Wuxx + wu).

Cost-to-go c is then deﬁned by (ignoring the constant terms for the minimization)

1
2

−

−

Denote then C
last equation gives

(cid:48)1/2
xx

a squared root matrix of C (cid:48)

xx (cid:23)

(cid:48)1/2
xx = C (cid:48)

xx. Developing the terms in the

cx = wx

−
Cxx = Wxx

W (cid:62)

uxW −1
W (cid:62)

uu wu
uxW −1

−

0 such that (C

uu Wux.
(cid:48)1/2
xx )(cid:62)C

Cxx = Wxx

W (cid:62)
−
= Hxx + ΦxC (cid:48)

= Hxx + ΦxC

uu Wux

uxW −1
xxΦ(cid:62)
(cid:48)1/2(cid:62)
xx

x −
(cid:16)
I

ΦxC (cid:48)

u (Guu + ΦuC (cid:48)

xxΦ(cid:62)
(cid:48)1/2
xx Φ(cid:62)

C

−

u (Guu + ΦuC

= Hxx + ΦxC

(cid:48)1/2(cid:62)
xx

(cid:16)

I + C

(cid:48)1/2
xx Φ(cid:62)

u G−1

uu ΦuC

(cid:48)1/2(cid:62)
xx

u )−1ΦuC (cid:48)
u )−1ΦuC

xxΦ(cid:62)
x
(cid:48)1/2(cid:62)
xx

(cid:48)

xxΦ(cid:62)
xxΦ(cid:62)
(cid:17)−1

C

(cid:48)1/2
xx Φ(cid:62)

x (cid:23)

0,

(cid:17)

C

(cid:48)1/2
xx Φ(cid:62)
x

where we used Sherman-Morrison-Woodbury’s formula in the last equality. This therefore proves (
rence.

H

t) and the recur-

22

Algorithm 2 Dynamic Programming for Linear Dynamics, Quadratic Convex Costs (52)
Input: Initial state ˆx0,

1: Quadratic costs deﬁned by ht,x, Ht,xx with Ht,xx

0 for t = 1, . . . , τ and gt,u, Gt,uu with Gt,uu

0, for

(cid:31)

(cid:23)

t = 0, . . . , τ

1

−

2: Linear dynamics deﬁned by Φt,x, Φt,u for t = 0, . . . , τ

1

−

Backward pass

3: Initialize Cτ,xx = Hτ,xx, cτ,x = hτ,x
4: for t = τ
1, . . . , 0 do
5:

−
Deﬁne

wt,x = ht,x + Φt,xct+1,x

wt,u = gt,u + Φt,uct+1,u

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
t,x
Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)
t,u
Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x

6:

Compute

7:

Store

Ct,xx = Wt,xx

−

W (cid:62)

t,uxW −1

t,uuWt,ux

ct,x = wt,x

−

W (cid:62)

t,uxW −1

t,uuwt,u

Kt =

−

W −1

t,uuWt,ux

kt =

−

W −1

t,uuwt,u

8: end for

Roll-out pass
9: Initialize x0 = ˆx0
10: for t = 0, . . . , τ

−

1 do

u∗
t = Ktxt + kt

xt+1 = Φ(cid:62)

t,xxt + Φ(cid:62)

t,uu∗
t

11: end for
Output: Optimal u∗

0, . . . , u∗

τ −1 for (52)

23

C Control algorithms detailed

We detail the complete implementation of the control algorithms presented in Section 4. We detail the simple im-
plementations of ILQR (Li & Todorov, 2004), ILQG (Todorov & Li, 2005; Li & Todorov, 2007), the variant of
ILQG (Tassa et al., 2012) and DDP (Mayne, 1966). Various line-searches have been proposed as in e.g. (Tassa et al.,
2012). We leave their analysis for future work.

C.1

ILQR (Li & Todorov, 2004) and regularized ILQR

ILQR and regularized ILQR amount to solve

τ
(cid:88)

min
y0,...yτ ∈Rd
v0,...,vτ −1∈Rp
subject to yt+1 = Φ(cid:62)

t=1

t,xyt + Φ(cid:62)

t,uvt

qht(xt + yt; xt) +

τ −1
(cid:88)

t=0

qgt(ut + vt; ut) +

1
2γ (cid:107)

vt

2
2

(cid:107)

(54)

where Φt,x =

∇

xφt(xt, ut), Φt,u =

∇

uφt(xt, ut), xt = ˜xt(¯u) and

y0 = 0,

qht(xt + yt; xt) = ht(xt) + h(cid:62)

t,xyt +

qgt(ut + vt; ut) = gt(ut) + g(cid:62)

t,uvt +

y(cid:62)
t Ht,xxyt

1
2
1
v(cid:62)
t Gt,uuvt,
2

∇

0.

(cid:23)

where ht,x =

ht(xt), Ht,xx =

2ht(xt)

0, gt,u =

gt(ut), Gt,uu =

2gt(ut)

∇
For γ = +

∇

(cid:23)

∇

, this amounts to the ILQR method. If h, g are quadratics this is a Gauss-Newton step, otherwise
, if h, g are quadratics this amounts to a regularized

it amounts to a generalized Gauss-Newton step. For γ < +
Gauss-Newton step (14), otherwise it amounts to a Levenberg-Marquardt step (16).

∞

∞

The steps (54) are detailed in Algo. 3 based on the derivations made in Section B. The complete methods are
detailed in Algo. 7 for the ILQR algorithm and in Algo. 8 for the regularized step. For the classical ILQR method, an
Armijo line-search can be used to ensure decrease of the objective as we present in Algorithm 7. Different line-searches
were proposed, like the one used in Differential dynamic Programming (see below). A line-search for the regularized
ILQR is proposed in Algo. 14 based on the regularized Gauss-Newton method’s analysis. Note that theoretically a
constant step-size can also be used a shown in Sec. 4. More sophisticated line-searches with proven convergence rates
are left for future work. Note that they were experimented in (Li & Todorov, 2007) for the ILQG method.

C.2

ILQG (Todorov & Li, 2005; Li & Todorov, 2007) and regularized ILQG

The ILQG method as presented in (Todorov & Li, 2005; Li & Todorov, 2007) consists in approximating the linearized
trajectory by a Gaussian as presented in Sec. 2 and solve the corresponding dynamic programming. As for ILQR we
add a proximal term to account for the inaccuracy of the model. Formally it amounts to solve

min
y0,...yτ ∈Rd
v0,...,vτ −1∈Rp

τ
(cid:88)

t=1

E ¯w [qht(xt + yt; xt)] +

τ −1
(cid:88)

t=0

qgt(ut + vt; ut) +

1
2γ (cid:107)

vt

2
2

(cid:107)

(55)

s.t.

t,xyt + Φ(cid:62)

t,uvt + Φ(cid:62)

t,wwt + φt,u,w[vt, wt,

yt+1 = Φ(cid:62)
y0 = 0,

],
·

where Φt,x =
˜xt(¯u, 0), and

∇

xφt(xt, ut, 0), Φt,u =

uφt(xt, ut, 0), Φt,w =

∇

∇

wφt(xt, ut, 0), φt,u,w =

2
uwφt(xt, ut, 0), xt =

∇

qht(xt + yt; xt) = ht(xt) + h(cid:62)

t,xyt +

qgt(ut + vt; ut) = gt(ut) + g(cid:62)

t,uvt +

y(cid:62)
t Ht,xxyt

1
2
1
v(cid:62)
t Gt,uuvt,
2

24

where ht,x =

ht(xt), Ht,xx =

2ht(xt)

0, gt,u =

gt(ut), Gt,uu =

2gt(ut)

∇

∇

(cid:23)

∇

The classical ILQG algorithm did not take into account a regularization as presented in Algo. 9 where we present
an Armijo line-search though other line-searches akin to the ones made for DDP are possible. Its regularized version
is presented in Algo. 10. They are based on solving the above problem at each step as presented in Algo. 4. It is based
on the following resolution of the dynamic programming problem.

∇

0.

(cid:23)

Proposition C.1. Cost-to-go functions for problem (55) are quadratics of the form, for t
1
2

t,xx, with Ct,xx

x(cid:62)Ct,xxx + c(cid:62)

ct(x) =

0.

(cid:23)

0, . . . , τ ,

∈

(56)

The optimal control at time t from a state xt reads for t

0, . . . , τ

∈ {

−
u∗
t (xt) = Ktxt + kt

,
1
}

where Ct,xx, ct,x, Kt, kt are deﬁned recursively in Algo. 4.

Proof. The classical Bellman equation is replaced for problem (55) by

ct(ˆyt) = qht(ˆyt) + min
vt

qgt(vt) +

(cid:26)

1
2γ (cid:107)

vt

2
2 + Ewt
(cid:107)

(cid:2)ct+1(Φ(cid:62)

t,xyt + Φ(cid:62)

t,uvt + Φ(cid:62)

t,wwt + φt,u,w[vt, wt,

(cid:27)

])(cid:3)
·

.

(57)

where we denoted shortly qht(ˆyt) = qht(xt + yt; xt) and qgt(vt) = qgt(ut + vt; ut).

The dynamic programming procedure is initialized by cτ,x = hτ,x, Cτ,xx = Hτ,xx

0. At iteration t, we seek to
solve analytically the equation (57). For given y and v. denoting with (cid:48) the quantities at time t + 1 and omitting the
index t otherwise, the expectation in (57) reads (recall that we supposed wt

(0, Iq))

(cid:23)

∼ N

E = Ew

= Ew

(cid:2)c(cid:48)(Φ(cid:62)
(cid:104)
c(cid:48)
x

(cid:62)(Φ(cid:62)

x y + Φ(cid:62)

u v + Φ(cid:62)

x y + Φ(cid:62)

u v + Φ(cid:62)

ww + φu,w[v, w,

])(cid:3)
·
ww + φu,w[v, w,

])
·

x y + Φ(cid:62)

ww + φu,w[v, w,

])(cid:62)C (cid:48)

xx(Φ(cid:62)

x y + Φ(cid:62)

u v + Φ(cid:62)

ww + φu,w[v, w,

(cid:105)
])
·

=c(cid:48)
x

+

(Φ(cid:62)

1
2
(cid:62)(Φ(cid:62)
x y + Φ(cid:62)
(cid:104) 1
2

(Φ(cid:62)

+ Ew

·
xx(Φ(cid:62)

u v) +

x y + Φ(cid:62)

u v)(cid:62)C (cid:48)

x y + Φ(cid:62)

u v)

ww + φu,w[v, w,

])(cid:62)C (cid:48)
·

xx(Φ(cid:62)

ww + φu,w[v, w,

(cid:105)
])
·

Now we have

u v + Φ(cid:62)
1
2

(Φ(cid:62)

(cid:104) 1
2
(cid:104) 1
2
(cid:104) 1
2
Tr((Φ(cid:62)

=

1
2

∈

E(cid:48) = Ew

(Φ(cid:62)

ww + φu,w[v, w,

xx(Φ(cid:62)

ww + φu,w[v, w,

= Ew

(Φ(cid:62)

ww + φπ

u,w[

xx(Φ(cid:62)

ww + φπ

(cid:105)
, v, w])
u,w[
·

= Ew

w(cid:62)(Φ(cid:62)

w + φπ

u,w[

])(cid:62)C (cid:48)

xx(Φ(cid:62)

w + φπ

u,w[
·

, v,

])w
·

(cid:105)
])
·

(cid:105)

])(cid:62)C (cid:48)
·
, v, w])(cid:62)C (cid:48)
·

, v,
·
·
])(cid:62)C (cid:48)
·

w + φπ

, v,
u,w[
·

xx(Φ(cid:62)

w + φπ

u,w[

]))

, v,
·

·

u,w = (Ψ1, . . . , Ψq)

Rd×p×q denotes the shufﬂed tensor φu,w as deﬁned in Appendix A. Therefore we

] = (Ψ1v, . . . , Ψqv)
·

∈

Rd×q. Now denoting Φ(cid:62)
(cid:33)(cid:33)

w = (ψ1, . . . , ψq)

Rd×q, we have

∈

Tr

C (cid:48)

xx

(ψi + Ψiv)(ψi + Ψiv)(cid:62)

=

(ψi + Ψiv)(cid:62)C (cid:48)

xx(ψi + Ψiv) = θ0 + v(cid:62)θ +

(cid:32) q

(cid:88)

1
2

v(cid:62)Θv

q
(cid:88)

i=1

1
2

where φπ
have φπ
u,w[
·

, v,
(cid:32)

E(cid:48) =

1
2

where

i=1

θ0 =

1
2

Tr(ΦwC (cid:48)

xxΦ(cid:62)

w),

θ =

q
(cid:88)

i=1

Ψ(cid:62)

i C (cid:48)

xxψi,

Θ =

q
(cid:88)

i=1

Ψ(cid:62)

i C (cid:48)

xxΨi

0.

(cid:23)

25

The computation of the cost-to-go function (57) reads then (ignoring the constant terms such as θ0),

c(ˆy) = h(cid:62)

x y +

1
2

y(cid:62)Hxxy + min

v

(cid:26)

˜g(cid:62)
u v +

1
2

v(cid:62) ˜Guuv + c(cid:48)(Φ(cid:62)

x y + Φ(cid:62)

u v)

(cid:27)

.

(58)

where

The rest of the computations follow form Prop. B.1. The positive semi-deﬁniteness of Cxx is ensured since Θ

0.

(cid:23)

˜gu = gu + θ

˜Guu = Guu + Θ + γ−1 Ip

C.3

ILQG as in (Tassa et al., 2012)

For completeness, we detail the implementation of ILQG steps as presented in (Tassa et al., 2012) in Algo. 5. The
overall method consists in simply iterating these steps, i.e., starting from ¯u0,

¯uk+1 = iLQG step(¯uk).

This algorithm is the same as the Differential Dynamic Programming algorithm (see below) except that second order
information of the trajectory is not taken into account. We present a simple line-search as the one used for DDP,
although more reﬁned line-searches were proposed in (Tassa et al., 2012).

C.4 Differential Dynamic Programming (Tassa et al., 2014)

For completeness, we present a detailed implementation of Differential Dynamic Programing (DDP) as presented
in (Tassa et al., 2014). Note that several variants of the algorithms exist, especially in the implementation of line-
searches, see e.g. (De O. Pantoja, 1988). A single step of the differential dynamic programming approach is described
in Algo. 6. The overall algorithm simply consists in iterating that step as, starting from ¯u0,

¯uk+1 = DDP step(¯uk).

We present the rationale behind the computations as explained in (Tassa et al., 2014) and precise the discrepancy
between the motivation and the implementation. Formally, at a given command ¯u with associated trajectory ¯x = ˜x(¯u),
the approach consists in approximating the cost-to-go functions as

ct(y) =qht(xt + y; xt) + min

v

(cid:8)qgt (ut + v; ut) + qct+1◦φt(xt + y, ut + v; xt, ut)(cid:9) ,

(59)

where for a function f (y), qf (x + y; x) = f (x) +
around x. They will take the following form, ignoring constant terms for the minimization,

f (x)(cid:62)y + y(cid:62)

∇

∇

2f (x)y/2 denotes its second order approximation

ct(y) =

1
2

y(cid:62)Ct,xxy + c(cid:62)

t,xy.

The initial value function is an approximation of the last cost around the current point, i.e.

cτ (y) =

1
2

y(cid:62)

∇

2h(xτ )y +

h(xτ )(cid:62)y,

∇

where we identify Cτ,xx =
step (59) involves computing a second order approximation around points (xt, ut) of

2h(xτ ), cτ,x =

∇

∇

h(xτ ). At time t + 1 given an approximate value function ct+1,

Mt+1(x, u) = ht(x) + gt(u) + ct+1(φt(x, u)).

Denote Wt(y, v) = qMt+1(xt + y, ut + v; xt, ut). We have, denoting z = (y; v)

Wt(y, v) = qMt+1(xt + y, ut + v; xt, ut)

Rd+p,

∈

(60)

= Mt+1(xt, ut) +

∇

ht(xt)(cid:62)y +

2ht(xt)y +

gt(ut)(cid:62)v +

∇

1
2

v(cid:62)

∇

2gt(ut)v

1
2

z(cid:62)

∇

φt(xt, ut)

∇

2ct+1(φ(xt, ut))

∇

φt(xt, ut)(cid:62)z

y(cid:62)

1
2
φt(xt, ut)(cid:62)z +

∇

ct+1(φt(xt, ut))(cid:62)

∇

+

+

∇
1
2 ∇

2φ(xt, ut)[z, z,

ct+1(φ(xt, ut))],

∇

26

where

∇

φt(xt, ut)(cid:62)z = (

xφt(xt, ut)(cid:62)y +

uφt(xt, ut)(cid:62)v), and

∇
2φ(xt, ut)[z, z,

∇
ct+1(φ(xt, ut))] =

∇

∇

2
xxφ(xt, ut)[y, y,

∇
+

∇
+ 2

∇

∇
2
uuφ(xt, ut)[v, v,
∇
2
uxφ(xt, ut)[v, y,

∇

ct+1(φ(xt, ut))]

ct+1(φ(xt, ut))]
ct+1(φ(xt, ut))].

By parameterizing Wt(y, v) as

Wt(y, v) = wt,0 + w(cid:62)

t,xy + w(cid:62)

t,uv +

1
2

y(cid:62)Wt,xxy +

1
2

v(cid:62)Wt,uuv + v(cid:62)Wt,uxy,

we get after identiﬁcation

wt,x = ht,x + Φt,x˜ct+1,x
wt,u = gt,u + Φt,u˜ct+1,x

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x + φt,ux[
Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)

t,x + φt,xx[
,
·
, ˜ct+1,x]
·
t,u + φt,uu[
,
·

,
·

, ˜ct+1,x]
·

, ˜ct+1,x],
·

where

ht,x =

∇

ht(xt),

Ht,xx =

2ht(xt),

gt,u =

Φt,x =
2
xxφt(xt, ut),

∇

∇
xφt(xt, ut),
φt,uu =

∇

Φt,u =
∇
2
uuφt(xt, ut),

gt(ut),
∇
uφt(xt, ut),
φt,ux =

Gt,uu =

2gt(ut),

∇

2
uxφt(xt, ut),

∇

˜ct+1,x = ct+1,x + Ct+1φt(xt, ut).

φt,xx =

∇

Here rather than using ˜ct+1,x as advocated by the idea of approximating the Bellman equation around the current
iterate, the implementation uses ct+1. To minimize the resulting function in v, one must ensure that Wt,uu is invert-
0 as presented in e.g.
ible. This is done by adding a small regularization λ such that Wt,uu := Wt,uu + λ Ip
(De O. Pantoja, 1988) and further explored in (Tassa et al., 2014).

(cid:31)

After minimizing in v, we get the new approximate value function to minimize

ct(y) =

1
2

y(cid:62)Ct,xxy + c(cid:62)

t,xy,

with

ct,x = wt,x

−

W (cid:62)

t,uxW −1

t,uuwt,u,

Ct,xx = Wt,xx

−

W (cid:62)

t,uxW −1

t,uuWt,ux.

Once the cost-to-go functions are computed, the next command is given by the solution of these approximated Bellman
equations around the trajectory. Precisely, denote

v∗(y) = arg min

v

(cid:8)qgt(ut + v; ut) + qct+1◦φt(xt + y, ut + v; xt, ut)(cid:9) = Kty + kt,

where kt =
command as

W −1

t,uuwt,u and Kt =

−

W −1

t,uuWt,uxy. The roll-out phase starts with x+

0 = ˆx0 and outputs the next

−

t = ut + v∗(x+
u+

t −

xt) = ut + Kt(x+

t −

xt) + kt

t+1 = φt(x+
x+

t , u+

t ).

(61)

A line-search advocated in e.g. (Tassa et al., 2014) is to move along the direction given by the ﬁxed gain kt, i.e., the
roll-out phase reads

t = ut + Kt(x+
u+

t −

xt) + αkt

t+1 = φt(x+
x+

t , u+

t ),

where α is chosen such that the next iterate has a lower cost than the previous one, by a decreasing line-search
initialized at α = 1. More sophisticated line-searches were also proposed (Mayne, 1966; Liao & Shoemaker, 1992).

27

Algorithm 3 ILQR step(¯u, γ). ILQR step (Li & Todorov, 2004) (γ =
γ [Sec. 4] on a command ¯u.

∞

) or regularized ILQR step with step-size

Inputs: Command ¯u = (u0; . . . , uτ −1), step-size γ, initial state ˆx0 convex twice differentiable costs ht for
1 differentiable dynamics φt
t = 0, . . . τ with h0 = 0, convex twice differentiable penalties gt for t = 0, . . . , τ
for t = 0, . . . , τ
Forward pass:

−

−

1

1: Set x0 = ˆx0
2: for t = 0, . . . , τ
3:

1 do
Compute and store

−

ht,x =

∇

ht(xt), Ht,xx =

2ht(xt),

gt,u =
xφt(xt, ut), Φt,u =

∇

gt(ut), Gt,uu =
uφt(xt, ut).

∇

∇

2gt(ut),

∇

Φt,x =

∇

Go to next state xt+1 = φt(xt, ut)

4:
5: end for

Backward pass:
6: Initialize Cτ,xx =
7: for t = τ
8:

−
Deﬁne

∇

1, . . . , 0 do

2hτ (xτ ), cτ,x =

hτ (xτ )

∇

wt,x = ht,x + Φt,xct+1,x

wt,u = gt,u + Φt,uct+1,x

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
t,x
Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)
t,u + γ−1 Ip
Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x

9:

Compute

10:

Store

Ct,xx = Wt,xx

−

W (cid:62)

t,uxW −1

t,uuWt,ux

ct,x = wt,x

−

W (cid:62)

t,uxW −1

t,uuwt,u

Kt =

−

W −1

t,uuWt,ux

kt =

−

W −1

t,uuwt,u

11: end for

Roll-out pass:
12: Initialize y0 = 0
13: for t = 0, . . . , τ

−

1 do

v∗
t = Ktyt + kt

yt+1 = Φ(cid:62)

t,xyt + Φ(cid:62)

t,uv∗
t

14: end for
Output: ILQR step(¯u, γ) = ¯v∗ where ¯v∗ = (v∗

0; . . . ; v∗

τ −1) is optimal for (54)

28

Algorithm 4 ILQG step(¯u, γ). ILQG step (Todorov & Li, 2005; Li & Todorov, 2007) (γ =
step with stepsize γ [Sec. 4] on a command ¯u.

) or regularized ILQG

∞

Inputs: Command ¯u = (u0; . . . , uτ −1), step-size γ, initial state ˆx0 convex twice differentiable costs ht for
t = 0, . . . τ with h0 = 0, convex twice differentiable penalties gt for t = 0, . . . , τ
1 twice differentiable noisy
dynamics φt for t = 0, . . . , τ
Forward pass:

1 as in (3).

−

−

1: Set x0 = ˆx0
2: for t = 0, . . . , τ
3:

1 do
Compute and store

−

ht,x =

ht(xt), Ht,xx =

∇
xφt(xt, ut, 0), Φt,u =

2ht(xt),
gt,u =
uφt(xt, ut, 0), Φt,w =

∇

gt(ut), Gt,uu =
∇
wφt(xt, ut, 0), φt,u,w =

2gt(ut),
2
uwφt(xt, ut, 0)

∇

∇

∇

∇

Φt,x =

∇

Go to the exact next state xt+1 = φt(xt, ut, 0)

4:
5: end for

Backward pass:
6: Initialize Cτ,xx =
7: for t = τ
8:

−

∇
Denoting (ψt,1, . . . , ψt,q) = Φ(cid:62)

1, . . . , 0 do

2hτ (xτ ), cτ,x =

hτ (xτ )

∇

t,w and (Ψt,1, . . . , Ψt,q) = φπ

t,u,w, deﬁne

wt,x = ht,x + Φt,xct+1,x

wt,u = gt,u + Φt,uct+1,x +

q
(cid:88)

i=1

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
t,x

Ψ(cid:62)

t,iCt+1,xxψt,i Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)

t,u +

q
(cid:88)

i=1

Ψ(cid:62)

t,iCt+1,xxΨt,i + γ−1 Ip

Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x

9:

Compute

10:

Store

Ct,xx = Wt,xx

−

W (cid:62)

t,uxW −1

t,uuWt,ux

ct,x = wt,x

−

W (cid:62)

t,uxW −1

t,uuwt,u

Kt =

−

W −1

t,uuWt,ux

kt =

W −1

t,uuwt,u

−

11: end for

Roll-out pass:
12: Initialize y0 = 0
13: for t = 0, . . . , τ

−

1 do

v∗
t = Ktyt + kt

yt+1 = Φ(cid:62)

t,xyt + Φ(cid:62)

t,uv∗
t

14: end for
Output: ILQG step(¯u, γ) = ¯v∗ where ¯v∗ = (v∗

0; . . . ; v∗

τ −1) is optimal for (55)

29

Algorithm 5 iLQG step(¯u) iLQG step as presented in (Tassa et al., 2012) on a command ¯u

Input: Command ¯u = (u0; . . . , uτ −1), step-size γ, initial state ˆx0 convex twice differentiable costs ht for t =
1 differentiable dynamics φt for

0, . . . τ with h0 = 0, convex twice differentiable penalties gt for t = 0, . . . , τ
t = 0, . . . , τ
1, decreasing factor ρ− < 1, control objective f
−
Forward pass:

−

1: Set x0 = ˆx0
2: for t = 0, . . . , τ
3:

1 do
Compute and store

−

ht,x =

∇

ht(xt), Ht,xx =

2ht(xt),

gt,u =
xφt(xt, ut), Φt,u =

∇

gt(ut), Gt,uu =
uφt(xt, ut).

∇

∇

2gt(ut),

∇

Φt,x =

∇

Go to next state xt+1 = φt(xt, ut)

4:
5: end for

Backward pass:
6: Initialize Cτ,xx =
7: for t = τ
8:

−
Deﬁne

∇

1, . . . , 0 do

2hτ (xτ ), cτ,x =

hτ (xτ )

∇

wt,x = ht,x + Φt,xct+1,x

wt,u = gt,u + Φt,uct+1,x

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
t,x
Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)
t,u
Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x

Ct,xx = Wt,xx

−

W (cid:62)

t,uxW −1

t,uuWt,ux

ct,x = wt,x

−

W (cid:62)

t,uxW −1

t,uuwt,u

Kt =

−

W −1

t,uuWt,ux

kt =

−

W −1

t,uuwt,u

9:

Compute

10:

Store

11: end for

Roll-out pass:

12: Initialize x+
13: repeat
14:

for t = 0, . . . , τ

0 = ˆx0, α = 1

1 do

−

t = ut + Kt(x+
u+

t −

xt) + αkt

t+1 = φt(x+
x+

t , u+
t )

end for
Update α = ρ−α
f (¯u)

15:
16:
17: until f (¯u+)
Output: iLQG step(¯u) = ¯u+ where ¯u+ = (u+

≤

0 ; . . . ; u+

τ −1)

30

Algorithm 6 DDP step(¯u). Differential dynamic programming step on a command ¯u (Tassa et al., 2014)

Hyper-parameters: regularization λ0, increasing regularization factor ρ+ > 1, decreasing factor ρ− < 1
Inputs: Command ¯u = (u0; . . . ; uτ −1), initial state ˆx0 convex twice differentiable costs ht for t = 0, . . . τ
with h0 = 0, convex twice differentiable penalties gt for t = 0, . . . , τ
1, twice differentiable dynamics φt for
t = 0, . . . , τ
−
Forward pass:

1, control objective f

−

1: Set x0 = ˆx0
2: for t = 0, . . . , τ do
3:

Compute and store

ht,x =

∇

ht(xt),

Ht,xx =

2ht(xt),

gt,u =

Φt,x =
2
xxφt(xt, ut),

∇
xφt(xt, ut),
φt,uu =

∇

Φt,u =
∇
2
uuφt(xt, ut),

∇

gt(ut),
∇
uφt(xt, ut),
φt,ux =

Gt,uu =

2gt(ut),

∇

2
uxφt(xt, ut).

∇

h(xτ ), Hτ,xx =

∇

2h(xτ )

∇

φt,xx =

∇
Go to next state xt+1 = φt(xt, ut)

4:
5: end for
6: Compute and store hτ,x =

Backward pass:

7: Initialize Cτ,xx = Hτ,xx, cτ,x = hτ,x
8: for t = τ
1, . . . , 0 do
9:

Compute

−

wt,x = ht,x + Φt,xct+1,x
wt,u = gt,u + Φt,uct+1,x

Set λ = λ0, W 0
while Wt,uu (cid:7) 0 do
Wt,uu := W 0

t,uu = Wt,uu

t,uu + λ Ip,

λ = ρ+λ

10:

11:
12:
13:
14:

end while
Compute

Wt,xx = Ht,xx + Φt,xCt+1,xxΦ(cid:62)
Wt,ux = Φt,uCt+1,xxΦ(cid:62)
t,x + φt,ux[
,
·
Wt,uu = Gt,uu + Φt,uCt+1,xxΦ(cid:62)

t,x + φt,xx[

,
·
, ct+1,x]
,

·
t,u + φt,uu[
·

, ct+1,x]
·

, ct+1,x]

·

Store Kt =

15:
16: end for

−

ct,x = wt,x

W −1

t,uuWt,ux

t,uuwt,u, Ct,xx = Wt,xx

W (cid:62)

t,uxW −1
W −1

−
kt =

t,uuwt,u.

−

W (cid:62)

t,uxW −1

t,uuWt,ux.

−

Roll-out pass:

17: Initialize x+
18: repeat
19:

for t = 0, . . . , τ

0 = ˆx0, α = 1

1 do

−

t = ut + Kt(x+
u+

t −

xt) + αkt

t+1 = φt(x+
x+

t , u+
t )

end for
Update α = ρ−α
f (¯u)

20:
21:
22: until f (¯u+)
Output: DDP step(¯u) = ¯u+ where ¯u+ = (u+

≤

0 ; . . . ; u+

τ −1)

31

Algorithm 7 ILQR (Li & Todorov, 2004)
Input: Initial state ˆx0, differentiable dynamics φt for t = 0, . . . , τ

t = 1, . . . , τ , convex twice differentiable penalties gt for t = 0, . . . , τ
deﬁned in (10), initial command ¯u0, number of iterations K

−

−

1, convex twice differentiable costs ht for
1, , total cost f on the trajectory as

Using Algo. 3, compute ¯vk = ILQR step(¯uk, +
Find γk s.t. f (¯uk + γk ¯vk) < f (¯uk)
Set ¯uk+1 = ¯uk + γk ¯vk

1: for k = 0, . . . , K do
2:
3:
4:
5: end for
Output: ¯u∗ = ¯uK

)
∞

Algorithm 8 Regularized ILQR as presented in Sec. 4
Input: Initial state ˆx0, differentiable dynamics φt for t = 0, . . . , τ

−

1, convex twice differentiable costs ht for
1, , total cost f on the trajectory as

t = 1, . . . , τ , convex twice differentiable penalties gt for t = 0, . . . , τ
deﬁned in (10), initial command ¯u0, number of iterations K

−

1: for k = 0, . . . , K do
2:

Find γk, such that ¯uk+1 = ¯uk + ILQR step(¯uk, γk) computed by Algo. 3 satisﬁes

f (¯uk+1)

f (¯uk) +

≤

1
2γk (cid:107)

¯uk

¯uk+1

2
2
(cid:107)

−

3: end for
Output: ¯u∗ = ¯uK

Algorithm 9 ILQG (Todorov & Li, 2005; Li & Todorov, 2007)
Input: Initial state ˆx0, noisy twice differentiable dynamics φt for t = 0, . . . , τ
ht for t = 1, . . . , τ , convex twice differentiable penalties gt for t = 0, . . . , τ
deﬁned in (10), initial command ¯u0, number of iterations K

−
−

1, convex twice differentiable costs
1, , total cost f on the trajectory as

Using Algo. 4, compute ¯vk = ILQG step(¯uk, +
Find γk s.t. f (¯uk + γk ¯vk) < f (¯uk)
Set ¯uk+1 = ¯uk + γk ¯vk

1: for k = 0, . . . , K do
2:
3:
4:
5: end for
Output: ¯u∗ = ¯uK

)
∞

Algorithm 10 Regularized ILQG as presented in Sec. 4
Input: Initial state ˆx0, differentiable dynamics φt for t = 0, . . . , τ

t = 1, . . . , τ , convex twice differentiable penalties gt for t = 0, . . . , τ
deﬁned in (10), initial command ¯u0, number of iterations K

−

1: for k = 0, . . . , K do
2:

Find γk, such that ¯uk+1 = ¯uk + ILQG step(¯uk, γk) computed by Algo. 4 satisﬁes

f (¯uk+1)

f (¯uk) +

≤

1
2γk (cid:107)

¯uk

¯uk+1

2
2
(cid:107)

−

3: end for
Output: ¯u∗ = ¯uK

32

−

1, convex twice differentiable costs ht for
1, , total cost f on the trajectory as

D Differential Dynamic Programming interpretation

A characteristic of Differential Dynamic Programming is that the update pass follows the original trajectory. This little
difference makes it very different to the classical optimization schemes we presented so far. Though its convergence is
often derived as a Newton’s method, it was shown that in practice it outperforms Newton’s method (Liao & Shoemaker,
1991, 1992). We analyze it as an optimization procedure on the state variables using recursive model-minimization
schemes.

D.1 Approximate dynamic programming

We consider problems in the last control variable since any optimal control problem can be written in the form (30) by
adding a dimension in the states. We write then Problem (30) as a constrained problem of the form

min
xτ ∈Dτ

hτ (xτ )

(62)

where the constraint sets Dt are deﬁned recursively as

D0 =
Dt+1 =

}

x0
{
xt+1 : xt+1 = φt(xt, ut), xt
{

∈

Dt , ut

Rp

,
}

∈

for t = 0, . . . , τ

1.

−

The approximate dynamic approach consists then as a nested sequence of subproblems that attempt to make an
approximate step in the space of the last state. Formally, at a given iterate ˆxτ deﬁned by (ˆu0, . . . , ˆuτ −1), it considers
a model-minimization step i.e.

min
z∈Dτ

Vτ (z) := mhτ (z; ˆxτ ) + γ−1d(z, ˆxτ ),

(63)

where mhτ (
·
of the procedure.

; ˆxt) is a given model that approximates hτ around ˆxτ , d(
·

, ˆxτ ) is a proximal term and γ is the step-size

Then the procedure consists in considering recursively model-minimizations steps of functions Vt, for t = τ, . . . , 1,
where each model-minimization step introduces the minimization of a new value function Vt on a simpler constraint
space.

Formally, assume that at time t the problem considered is

min
z∈Dt

Vt(z)

(64)

for a given function Vt and that one is given an initial point ˆzt
deﬁnes states ˆz0, . . . , ˆzt as ˆzs+1 = φs(ˆzs, ˆvs) for 0
problem reads

≤

≤

−

s

t

Dt with associated sub-command ˆv0, . . . , ˆvt−1 that
∈
1 with ˆz0 = ˆx0. Then developing the constraint set, the

a minimization step on this problem around the given initial point is

min
z∈Dt−1,v∈Rp

Mt(z, v) := Vt(φt−1(z, v))

min
z∈Dt−1,v∈Rp

mMt(z, v; ˆzt−1, ˆvt−1) + γ−1d((z, v), (ˆzt−1, ˆvt−1)).

Then this problem simpliﬁes as

min
z∈Dt−1

Vt−1(z) := min
v∈Rp

mMt(z, v; ˆzt−1, ˆvt−1) + γ−1d((z, v), (ˆzt−1, ˆvt−1)),

(65)

(66)

(67)

which deﬁnes the next problem. The initial point of this subproblem is chosen as ˆzt−1 with associated subcommand
ˆv0, . . . , ˆvt−2.

The recursive algorithm is deﬁned in Algo. 11. These use sub-trajectories deﬁned by the dynamics and sub-
commands. The way the stopping criterion and the step-sizes are chosen depend on the implementation just as the
choices of the model m and the proximal term d. The optimal command is tracked along the recursion to be output at
the end.

The whole procedure instantiates iteratively Algo. 11 on (63) as presented in Algo. 12. Note that it is of potential

interest to have a different model-minimization scheme for the outer loop and the inner recursive loop.

33

Algorithm 11 Approximate Dynamic Programming Recursion

1: Inputs:
2: -Approximate model m, proximal term d, initial point ˆx0
3: -Time t, value function Vt
4: -Dynamics φ0, . . . , φt, initial point z0
s , v0

s+1 = φs(z0

1 with z0

as z0

s ) for 0

s
≤
t )k∈N, stopping criterion δt : N

0 = ˆx0
Rd

t ∈
−

≤

t

×

0, 1

→ {

}

5: -Step sizes (γk
6: if t = 0 then
7:
8: else
9:

Return z∗ = ˆx0

repeat for k = 1, . . .

Dt with associated subcommand v0

0, . . . , v0

t−1 deﬁning states z0

0, . . . , z0
t

Denoting Mt(z, v) = Vt(φt−1(z, v)) and ˆz = zk−1

t−1 , ˆv = vk−1

t−1 , deﬁne

Vt−1(z) = min
v∈Rp

mMt(z, v; ˆz, ˆv) + (γk

t )−1d((z, v), (ˆz, ˆv))

(68)

Find zk

t−1 and its associated subcommand vk

0 , . . . , vk

t−2 and subtrajectory zk

0 , . . . , zk

t−2 using Algo. 11 s.t.

fed with

zk
t−1 ≈

arg min
y∈Dt−1

Vt−1(y)

- same m, d, ˆx0
- time t
1, value function Vt−1
- dynamics φ0, . . . , φt−1, initial point zk−1
- a strategy of step sizes (γk

−

Compute

t−1 with associated subcommand vk−1

0

, . . . , vk−1
t−2

t−1)k∈N and a stopping criterion δt−1 : N

Rd

0, 1
}

→ {

×

vk
t−1 = arg min

v∈Rp

mMt(zk, v; ˆz, ˆv) + (γk

t )−1d((zk, v), (ˆz, ˆv))

(69)

10:

11:

12:

13:
14:

Compute zk

t = φt−1(zk

t−1, vk
until stopping criterion δt(k, zk
Return zk

t−1)
t ) is met

t with its associated subcommand vk

0 , . . . , vk

t−1

15:
16: end if

D.2 Differential dynamic programming

Differential dynamic programing is an approximate instance of the above algorithm where (i) one considers a second
order approximation of the function to deﬁne the model m, (ii) one does not use a proximal term d, (iii) the stopping
criterion is simply to stop after one iteration.

−

∇

ˆw)(cid:62)

2 (w

2f ( ˆw)(w

Precisely, for a twice differentiable function f , on a point ˆw, we use mf (w; ˆw) = qf (w; ˆw) = f ( ˆw)+

ˆw). Notice that without additional assumption on the Hessian

−
ˆw) + 1
; ˆw) may
·
be unbounded below, such that the model-minimization steps may be not well deﬁned. The deﬁnition of the models
qMt in Eq. (68) correspond to the computations in Eq. (60) that lead to the formulation of the cost-to-go functions ct.
The solutions output by the recursion in Eq. (69) correspond to the roll-out presented in Eq. (61). Crucially, as in the
classical DDP formulation, the output at the tth time step in the roll-out phase (here when the recursion is unrolled line
13 in Algo. 11) is given by the true trajectory.

2f ( ˆw), qf (

∇

∇

−

f ( ˆw)(cid:62)(w

Recall that the implementation differs from the motivation. The choice of using the un-shifted cost-to-go functions,

i.e., choosing ct+1 instead of ˜ct+1 as presented in Sec. C.4, is not explained by our theoretical approach.

The iLQG method as presented in Tassa et al. (2012) follows the same approach except that they use the quadratic

models deﬁned in a Levenberg-Marquardt steps for each model-minimization of the recursion.

34

Algorithm 12 Approximate Dynamic Programming

Inputs:
-Cost function hτ , outer approximate model m, outer proximal term d, inner approximate model ˜m and inner
proximal term ˜d
-Dynamics φ0, . . . , φτ −1, initial point x0
τ ∈
as x0
1 with x0
t ) for 0
-Step sizes (γk)k∈N, stopping criterion δ : N
repeat for k = 1, . . .

Dτ with associated command u0

τ −1 deﬁning states x0

t+1 = φt(x0

0 = ˆx0
Rd

0, . . . , u0

0, . . . , x0
τ

0, 1
}

t , u0

→ {

≤

≤

−

τ

t

Find xk

τ with associated command uk

τ −1 using Algo. 11 to solve

×
0, . . . , uk

xk
τ ≈

arg min
z∈Dτ

Vτ (z) := mhτ (z; xk−1

τ

) + (γk)−1d(z, xk−1

τ

)

(70)

fed with
- Inner approximate model ˜m and proximal term ˜d, initial point ˆx0
- time τ , value function Vτ
- initial point xk−1
- a strategy of step sizes (γk
until Stopping criterion δ(k, xk

τ )k∈N and a stopping criterion δτ : N
τ ) is met
τ with associated command uk

τ with associated command uk−1

, . . . , uk−1
τ −1

0, . . . , uk

τ −1

0

×

Output: xk

Rd

0, 1
}

→ {

E Regularized Gauss-Newton analysis

For completeness we recall how equality (38) is obtained. As h, g are quadratics, we have h(¯x + ¯y) = qh(¯x +
¯y; ¯x), g(¯u + ¯v) = qg(¯u + ¯v; ¯u). Therefore cf (¯u + ¯v; ¯u) = qf (¯u + ¯v; ¯u) with cf deﬁned in (13) and qf deﬁned in (15).
The regularized Gauss-Newton step reads then, denoting ¯xk = ˜x(¯uk), H =

2h(¯xk) and G =

2g(¯uk)

∇

∇

¯uk+1 = ¯uk + arg min

¯v

qf (¯uk + ¯v; ¯uk) +

1
2
¯v
2
2γk (cid:107)
(cid:107)

= ¯uk + arg min

¯v

(cid:110)

∇

+

= ¯uk

= ¯uk

−

−

(
∇

(

∇

˜x(¯uk)H

˜x(¯uk)H

∇

∇

h(¯xk)(cid:62)(

∇

˜x(¯uk)(cid:62)¯v) +

1
2

1
2

g(¯uk)(cid:62)¯v +

¯v(cid:62)G¯v +

∇
˜x(¯uk)(cid:62) + G + γ−1
˜x(¯uk)(cid:62) + G + γ−1

k

k

(
∇
1
2
¯v
2
2γk (cid:107)
(cid:107)
˜x(¯uk)

(cid:111)

∇
f (¯uk)

∇

Iτ p)−1(
Iτ p)−1

˜x(¯uk)(cid:62)¯v)(cid:62)H(

˜x(¯uk)(cid:62)¯v)

∇

h(¯xk) +

g(¯uk))

∇

∇

We prove the overall convergence of the regularized Gauss-Newton method under a sufﬁcient decrease condition.

Proposition 4.1. Consider a composite objective f as in (36) with convex models cf (
the step sizes γk of the regularized Gauss-Newton method (37) are chosen such that

; ¯u) deﬁned in (13). Assume that
·

f (¯uk+1)

≤

cf (¯uk+1; ¯uk) +

1
2γk (cid:107)

¯uk+1

¯uk

2
2
(cid:107)

−

(39)

and γmin

γk

γmax.

≤

≤

Then the objective value decreases over the iterations and the sequence of iterates satisﬁes

where L = maxγ∈[γmin,γmax] γ((cid:96)2

min

k=0,...,N (cid:107)∇

2
(cid:107)
˜x,SLh + Lg + γ−1)2 and f ∗ = limk→+∞ f (¯uk).

−
N + 1

f (¯uk)

≤

,

2L(f (¯u0)

f ∗)

35

Proof. For k

0,

≥

((cid:63))

f (¯uk) = cf (¯uk; ¯uk)

cf (¯uk+1; ¯uk) +

¯uk+1

≥
(cid:107)
where we used in ((cid:63)) the deﬁnition of ¯uk+1 and strong convexity of ¯u
ﬁrst that the iterates stay in the initial level set. Then, summing the inequality and taking the minimum gives

−
¯uk

→

−

≥

−

¯u

(cid:107)

(39)

¯uk

2
2

f (¯uk+1) +

1
2γk (cid:107)
c(¯u; ¯uk) + (2γk)−1

¯uk+1

¯uk

2,
(cid:107)
2
2. This ensures
(cid:107)

1
γk (cid:107)

Finally using (38), we get

min
k=0,...,N

γ−1
k (cid:107)

¯uk+1

¯uk

−

2

(cid:107)

≤

2(f (¯u0)

−
N + 1

f ∗)

.

f (¯uk)
¯uk+1
(cid:107)
(cid:107)
Plugging this in previous inequality and rearranging the terms give the result.

˜x,SLh + Lg + γ−1
k )

((cid:96)2

(cid:107)∇

≤

2

−

¯uk

2.
(cid:107)

Now we show how the model approximates the objective up to a quadratic error for exact dynamics

Lemma 4.2. Consider a composite objective f as in (36) with convex models cf (
·
set C

Rτ p there exists MC > 0 such that for any ¯u, ¯v

C,

; ¯u) deﬁned in (13). For any compact

⊂

∈
cf (¯v; ¯u)

f (¯v)

|

−

MC

¯v
2 (cid:107)

2
2.

¯u
(cid:107)

−

(40)

| ≤

Proof. As ˜x has continuous gradients, it is (cid:96)˜x,C-Lipschitz continuous and has L˜x,C- Lipschitz gradients on C
Similarly h is Lh-smooth and (cid:96)h,C(cid:48) on any compact set C (cid:48)
at the origin that contains C and ρ its radius. Deﬁne B(cid:48)
C (cid:48) = ˜x(C) + B(cid:48) such that for any ¯u, ¯v

Rτ p.
Rτ p a ball centered
Rτ p, denote B
⊂
Rτ d a ball centered at the origin of radius 2ρ(cid:96)˜x,C and ﬁnally
⊂
˜x(¯u)(cid:62)(¯v

Rτ d. Now on C

C, ˜x(¯u) +

C,

⊂

⊂

⊂

f (¯v)

|

−

∈
cf (¯v; ¯u)
|

¯u)
−
∈
h (cid:0)˜x(¯u) +
˜x(¯u)

C (cid:48). Then for any ¯u, ¯v
˜x(¯u)(cid:62)(¯v
∇
˜x(¯u)(cid:62)(¯v

¯u)(cid:1)
|
¯u)
2
(cid:107)

−

−

∈

∇
h(˜x(¯v))
|
−
˜x(¯v)
(cid:96)h,C(cid:48)
(cid:107)
(cid:96)h,C(cid:48)L˜x,C
2

=

≤

≤

− ∇
2
2

−
¯v
(cid:107)
−
¯u))(cid:62)(¯v

¯u
(cid:107)

where the last line uses ˜x(¯v) = ˜x(¯u) + (cid:82) 1

0 ∇

˜x(¯u + s(¯v

−

¯u)ds and the smoothness of ˜x on C.

−

Finally we precise a minimal step-size for which the sufﬁcient decrease condition is ensured.

Lemma 4.3. Consider a composite objective f as in (36). For any k such that ¯uk
is the initial sub-level set, any step-size

∈

S, where S =

¯u : f (¯u)
{

≤

f (¯u0)

}

(41)
ensures that the sufﬁcient decrease condition (39) is satisﬁed, where (cid:96)f,S is the Lipschitz constant of f on S, C =
S + B1 with B1 the unit Euclidean ball centered at 0 and MC ensures (40).

f,S, M −1
(cid:96)−1
ˆγ = min
C }
{

γk

≤

Proof. Using (38),

so for γk

(cid:96)−1
f,S,

≤

¯uk+1
(cid:107)

−

¯uk

2
(cid:107)

≤

¯uk+1
(cid:107)

¯uk

2
−
(cid:107)∇
≤
(cid:107)
C. As ¯uk, ¯uk+1

γk

f (¯uk)

2
(cid:107)

1 and ¯uk+1

C we have by (40),

∈

∈
MC

¯uk

¯uk+1

2
2
(cid:107)

−

2 (cid:107)

which is the sufﬁcient decrease condition (39) for γk

M −1
C .

≤

f (¯uk+1)

≤

c(¯uk+1; ¯uk) +

We rigorously deﬁne the back-tracking line-search that supports Cor. 4.4 in Algo. 13 and 14.

F Accelerated Gauss-Newton

We detail the proof of convergence of the accelerated Gauss-Newton algorithm.

36

Algorithm 13 Line-search for regularized Gauss-Newton method
Input: Objective f as in (10), convex models cf as in (13), point ¯u, step size γ > 0, regularized Gauss-Newton oracle

(¯u, γ) = (¯u+, γ+)

L

GN(¯u; γ) (cid:44) arg min¯v∈Rτ p cf (¯v; ¯u) + γ−1
¯v
2 (cid:107)
while f (GN(¯u; γ)) > cf (GN(¯u; γ); ¯u) + γ

¯u
−
(cid:107)
GN(¯u; γ)

2
2, decreasing factor ρ < 1
2
2 do
(cid:107)

−

¯u

2 (cid:107)

γ := ργ

end while

Output: ¯u+ := GN(¯u; γ), γ+ = γ

Algorithm 14 Regularized Gauss-Newton method with line-search
Input: Objective f as in (10), convex models cf as in (13), initial point ¯u0, initial step size γ−1, accuracy (cid:15)

repeat for k = 0, . . .

Compute ¯uk+1, γk =

L

(¯uk, γk−1) using Algo. 13 such that

f (¯uk+1)

≤

cf (¯uk+1; ¯uk) +

γk
2 (cid:107)

¯uk+1

2
2

¯u
(cid:107)

−

until (cid:15)-near stationarity, i.e.,

Output: ¯uk+1

f (¯uk+1)

(cid:15)

(cid:107) ≤

(cid:107)∇

Proposition 4.5. Consider Algo. 1 applied to a composite objective f as in (36) with decreasing step-sizes (γk)k≥0
and (δk)k≥0. Then Algo. 1 satisﬁes the convergence of the regularized Gauss-Newton method (37) with line-search as
presented in Cor. 4.4. Moreover, if the convex models cf (¯v; ¯u) deﬁned in (13) lower bound the objective as

for any ¯u, ¯v

∈

Rτ p, then after N iterations of Algo. 1,

f (¯uN )

f ∗

−

≤

4δ−1

¯u0

¯u∗
(cid:107)
(N + 1)2

−

2

(cid:107)

,

cf (¯v; ¯u)

f (¯v)

≤

(42)

where δ = mink∈{1,...N } δk, f ∗ = min¯u f (¯u) and ¯u∗

arg min¯u f (¯u).

∈

Proof. First part of the statement is ensured by taking the best of both steps. For the second part, note ﬁrst that
assumption (42) implies that the objective f is convex as shown in Lemma 8.3 in (Drusvyatskiy & Paquette, 2018).
Now, at iteration k

1, for any ¯u,

≥

f ( ¯wk)

cf ( ¯wk; ¯yk) +

δ−1
k
2 (cid:107)

¯wk

¯yk

2
2

(cid:107)

−

cf (¯u; ¯yk) +

δ−1
k
2

(

¯u
(cid:107)

−

¯yk

2
2 − (cid:107)
(cid:107)

¯u

−

¯wk

2
2)

(cid:107)

f (¯uk)

(44)

≤
(43)

≤
((cid:63))

≤
(42)

≤

where ((cid:63)) comes from strong convexity of ¯u
choosing ¯u = αk ¯u∗ + (1
convexity of f ,

−

αk)¯uk−1, such that ¯u

→

−

f (¯u) +

δ−1
k
2

¯u

−

¯yk

2
¯u
(
2 − (cid:107)
(cid:107)
(cid:107)
cf (¯u; ¯yk) + δ−1
¯u
k (cid:107)
¯yk = αk(¯u∗

−
−

¯wk

2
2),
(cid:107)

−
2
2/2 and the fact that ¯wk minimizes it. Now
(cid:107)
¯zk), we get by

¯yk
¯zk−1) and ¯u

¯wk = αk(¯u∗

−

−

f (¯uk)

≤

αkf (¯u∗) + (1

−

αk)f (¯uk−1) +

kδ−1
α2
k
2

¯u∗
(
(cid:107)

−

¯zk−1

2
2 − (cid:107)
(cid:107)

¯u∗

¯zk

2
2).
(cid:107)

−

Subtracting f ∗ on both sides and rearranging the terms, we get
kδ−1
α2
k
2

αk)f (¯uk−1

f ∗) +

f (¯uk)

f ∗

(1

≤

−

−

−

¯u∗
(
(cid:107)

−

¯zk−1

2
2 − (cid:107)
(cid:107)

¯u∗

¯zk

2
2).
(cid:107)

−

37

For k = 1, using that α1 = 1, we get

δ1
α2
1

(f (¯u1)

f ∗)

−

For k

≥

2, using the deﬁnition of αk, i.e., that (1

(cid:0)

1
2

≤
(cid:107)
αk)/α2

¯u∗

¯z0

¯u∗

2
2 − (cid:107)
−
(cid:107)
k = 1/α2
k−1, we get

−

¯z1

(cid:1) .

2
2
(cid:107)

δk
α2
k

(f (¯uk)

f ∗)

−

δk
α2
k−1
δk−1
α2

k−1

≤

≤

−

(f (¯uk−1)

(f (¯uk−1)

f ∗) +

f ∗) +

−

−

(cid:0)

(cid:0)

1
2
1
2

¯u∗
(cid:107)

−

¯zk−1

2
2 − (cid:107)
(cid:107)

¯u∗

−

¯zk

¯u∗
(cid:107)

−

¯zk−1

2
2 − (cid:107)
(cid:107)

¯u∗

−

¯zk

(cid:1)

(cid:1) .

2
2
(cid:107)

2
2
(cid:107)

Developing the recursion, we obtain

f (¯uk)

f ∗

−

≤

kδ−1
α2
k
2

¯u∗
(cid:107)

¯z0

2
2 ≤

(cid:107)

−

4δ−1
(k + 1)2 (cid:107)

¯u∗

¯u0

2
2,

(cid:107)

−

where δ = mink∈{1,...N } δk and we used the estimate on αk provided in Lemma B.1 in (Paquette et al., 2018).

38

