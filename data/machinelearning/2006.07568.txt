Journal of XXXX manuscript No.
(will be inserted by the editor)

Primal-dual path-following methods and the trust-region
updating strategy for linear programming with noisy data

Xin-long Luo ∗ · Yi-yan Yao

1
2
0
2

b
e
F
2
2

]

C
O
.
h
t
a
m

[

4
v
8
6
5
7
0
.
6
0
0
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract In this article, we consider the primal-dual path-following method and the
trust-region updating strategy for the standard linear programming problem. For the
rank-deﬁcient problem with the small noisy data, we also give the preprocessing
method based on the QR decomposition with column pivoting. Then, we prove the
global convergence of the new method when the initial point is strictly primal-dual
feasible. Finally, for some rank-deﬁcient problems with or without the small noisy
data from the NETLIB collection, we compare it with other two popular interior-
point methods, i.e. the subroutine pathfollow.m and the built-in subroutine linprog.m
of the MATLAB environment. Numerical results show that the new method is more
robust than the other two methods for the rank-deﬁcient problem with the small noise
data.

Keywords Continuation Newton method · trust-region method · linear program-
ming · rank deﬁciency · path-following method · noisy data

Mathematics Subject Classiﬁcation (2010) 65K05 · 65L05 · 65L20

1 Introduction

In this article, we are mainly concerned with the linear programming problem with
the small noisy data as follows:

cT x, subject to Ax = b, x ≥ 0,

min
x∈ℜn

(1)

Xin-long Luo, Corresponding author
School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, P. O. Box 101,
Xitucheng Road No. 10, Haidian District, 100876, Beijing China, E-mail: luoxinlong@bupt.edu.cn

Yi-yan Yao
School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, P. O. Box 101,
Xitucheng Road No. 10, Haidian District, 100876, Beijing China, E-mail: yaoyiyan@bupt.edu.cn

 
 
 
 
 
 
2

Luo, Yao

where c and x are vectors in ℜn, b is a vector in ℜm, and A is an m × n matrix. For
the problem (1), there are many efﬁcient methods to solve it such as the simplex
methods [38,49], the interior-point methods [18,21,36,43,45,48] and the continuous
methods [1,11,26,33]. Those methods are all assumed that the constraints of problem
(1) are consistent, i.e. rank(A, b) = rank(A). For the consistent system of redundant
constraints, references [3, 4, 34] provided a few preprocessing strategies which are
widely used in both academic and commercial linear programming solvers.

However, for a real-world problem, since it may include the redundant constraints
and the measurement errors, the rank of matrix A may be deﬁcient and the right-hand-
side vector b has small noise. Consequently, they may lead to the inconsistent system
of constraints [6, 12, 29]. On the other hand, the constraints of the original real-world
problem are intrinsically consistent. Therefore, we consider the least-squares approx-
imation of the inconsistent constraints in the linear programming problem based
on the QR decomposition with column pivoting. Then, according to the ﬁrst-order
KKT conditions of the linear programming problem, we convert the processed prob-
lems into the equivalent problem of nonlinear equations with nonnegative constraints.
Based on the system of nonlinear equations with nonnegative constraints, we consider
a special continuous Newton ﬂow with nonnegative constraints, which has the non-
negative steady-state solution for any nonnegative initial point. Finally, we consider
a primal-dual path-following method and the adaptive trust-region updating strategy
to follow the trajectory of the continuous Newton ﬂow. Thus, we obtain an optimal
solution of the original linear programming problem.

The rest of this article is organized as follows. In the next section, we consider the
primal-dual path-following method and the adaptive trust-region updating strategy
for the linear programming problem. In section 3, we analyze the global convergence
of the new method when the initial point is strictly primal-dual feasible. In section
4, for the rank-deﬁcient problems with or without the small noise, we compare the
new method with two other popular interior-point methods, i.e. the traditional path-
following method (pathfollow.m in p. 210, [18]) and the predictor-corrector algorithm
(the built-in subroutine linprog.m of the MATLAB environment [31,32,48]). Numer-
ical results show that the new method is more robust than the other two methods for
the rank-deﬁcient problem with the small noisy data. Finally, some discussions are
given in section 5. (cid:107) · (cid:107) denotes the Euclidean vector norm or its induced matrix norm
throughout the paper.

2 Primal-dual path-following methods and the trust-region updating strategy

2.1 The continuous Newton ﬂow

For the linear programming problem (1), it is well known that its optimal solution
x∗ if and only if it satisﬁes the following Karush-Kuhn-Tucker conditions (pp. 396-
397, [36]):

Ax − b = 0, AT y + s − c = 0, XSe = 0, (x, s) ≥ 0,

(2)

Primal-dual path-following methods and the trust-region updating strategy

where

X = diag(x), S = diag(s), and e = (1, . . . , 1)T .

3

(3)

For convenience, we rewrite the optimality condition (2) as the following nonlinear
system of equations with nonnegative constraints:

F(z) =

AT y + s − c

= 0, (x, s) ≥ 0, and z = (x, y, s).

(4)








Ax − b

XSe








It is not difﬁcult to know that the Jacobian matrix J(z) of F(z) has the following

form:

J(z) =








A 0 0

0 AT I

S 0 X








.

(5)

From the third block XSe = 0 of equation (4), we know that xi = 0 or si = 0 (i = 1 :
n). Thus, the Jacobian matrix J(z) of equation (5) may be singular, which leads to
numerical difﬁculties near the solution of the nonlinear system (4) for the Newton’s
method or its variants. In order to overcome this difﬁculty, we consider its perturbed
system [2, 42] as follows:

Fµ (z) = F(z) −















0

0

µe

= 0, (x, s) > 0, µ > 0 and z = (x, y, s).

(6)

The solution z(µ) of the perturbed system (6) deﬁnes the primal-dual central path,
and z(µ) approximates the solution z∗ of the nonlinear system (4) when µ tends to
zero [18, 36, 43, 44].

We deﬁne the strictly feasible region F0 of the problem (1) as
F0 = (cid:8)(x, y, s)|Ax = b, AT y + s = c, (x, s) > 0(cid:9) .

(7)

Then, when there is a strictly feasible interior point ( ¯x, ¯y, ¯s) ∈ F0 and the rank of
matrix A is full, the perturbed system (6) has a unique solution (Theorem 2.8, p.
39, [43]). The existence of its solution can be derived by the implicit theorem [17]
and the uniqueness of its solution can be proved via considering the strict convexity
of the following penalty problem and the KKT conditions of its optimal solution [19]:

min cT x − µ

n
∑
i=1

log(xi) subject to Ax = b,

(8)

4

Luo, Yao

where µ is a positive parameter.

According to the duality theorem of the linear programming (Theorem 13.1, pp.

368-369, [36]), for any primal-dual feasible solution (x, y, s), we have

cT x ≥ bT y∗ = cT x∗ ≥ bT y,

(9)

where the triple (x∗, y∗, s∗) is a primal-dual optimal solution. Moreover, when the
positive number µ is small, the solution z∗(µ) of perturbed system (6) is an approxi-
mation solution of nonlinear system (4). Consequently, from the duality theorem (9),
we know that x∗(µ) is an approximation of the optimal solution of the original linear
programming problem (1). It can be proved as follows. Since z∗(µ) is the primal-dual
feasible, from inequality (9), we have

cT x∗(µ) ≥ bT y∗ = cT x∗ ≥ bT y∗(µ)

and

0 ≤ (x∗(µ))T s∗(µ) = cT x∗(µ) − bT y∗(µ).

From equations (10)-(11), we obtain

(10)

(11)

|cT x∗(µ) − cT x∗| ≤ cT x∗(µ) − bT y∗(µ) = (x∗(µ))T s∗(µ) = nµ.

(12)

Therefore, x∗(µ) is an approximation of the optimal solution of the original linear
programming problem (1).

If the damped Newton method is applied to the perturbed system (6) [16, 36], we

have

zk+1 = zk − αkJ(zk)−1Fµ (zk),

(13)

where J(zk) is the Jacobian matrix of Fµ (z). We regard zk+1 = z(tk + αk), zk = z(tk)
and let αk → 0, then we obtain the continuous Newton ﬂow with the constraints
[7, 8, 13, 30, 41] of the perturbed system (6) as follows :

dz(t)
dt

= −J(z)−1Fµ (z), z = (x, y, s) and (x, s) > 0.

(14)

Actually, if we apply an iteration with the explicit Euler method [40, 47] for the con-
tinuous Newton ﬂow (14), we obtain the damped Newton method (13).

Since the Jacobian matrix J(z) = F (cid:48)

µ (z) may be singular, we reformulate the con-

tinuous Newton ﬂow (14) as the following general formula [8, 41]:

−J(z)

dz(t)
dt

= Fµ (z), z = (x, y, s) and (x, s) > 0.

(15)

The continuous Newton ﬂow (15) has some nice properties. We state one of them as
the following property 2.1 [8, 30, 41].

Primal-dual path-following methods and the trust-region updating strategy

5

Property 2.1 (Branin [8] and Tanabe [41]) Assume that z(t) is the solution of the
continuous Newton ﬂow (15), then f (z(t)) = (cid:107)Fµ (z)(cid:107)2 converges to zero when t → ∞.
That is to say, for every limit point z∗ of z(t), it is also a solution of the perturbed
system (6). Furthermore, every element F i
µ (z) of Fµ (z) has the same convergence
rate exp(−t) and z(t) can not converge to the solution z∗ of the perturbed system (6)
on the ﬁnite interval when the initial point z0 is not a solution of the perturbed system
(6).

Proof. Assume that z(t) is the solution of the continuous Newton ﬂow (15), then
we have

d
dt

(cid:0)exp(t)Fµ (z)(cid:1) = exp(t)J(z)

dz(t)
dt

+ exp(t)Fµ (z) = 0.

Consequently, we obtain

Fµ (z(t)) = Fµ (z0)exp(−t).

(16)

From equation (16), it is not difﬁcult to know that every element F i
µ (z) of Fµ (z)
converges to zero with the linear convergence rate exp(−t) when t → ∞. Thus, if the
solution z(t) of the continuous Newton ﬂow (15) belongs to a compact set, it has a
limit point z∗ when t → ∞, and this limit point z∗ is also a solution of the perturbed
system (6).

If we assume that the solution z(t) of the continuous Newton ﬂow (15) converges
to the solution z∗ of the perturbed system (6) on the ﬁnite interval (0, T ], from equa-
tion (16), we have

Fµ (z∗) = Fµ (z0)exp(−T ).

(17)

Since z∗ is a solution of the perturbed system (6), we have Fµ (z∗) = 0. By substituting
it into equation (17), we obtain

Fµ (z0) = 0.

Thus, it contradicts the assumption that z0 is not a solution of the perturbed system
(6). Consequently, the solution z(t) of the continuous Newton ﬂow (15) can not con-
verge to the solution z∗ of the perturbed system (6) on the ﬁnite interval.
(cid:117)(cid:116)

Remark 2.1 The inverse J(x)−1 of the Jacobian matrix J(x) can be regarded as the
preconditioner of Fµ (x) such that the every element xi(t) of x(t) has roughly the same
convergence rate and it mitigates the stiffness of the ODE (15) [30]. This property is
very useful since it makes us adopt the explicit ODE method to follow the trajectory
of the Newton ﬂow (15) efﬁciently.

6

Luo, Yao

2.2 The primal-dual path-following method

From property 2.1, we know that the continuous Newton ﬂow (15) has the nice global
convergence property. However, when the Jacobian matrix J(x) is singular or nearly
singular, the ODE (15) is the system of differential-algebraic equations [5, 9, 23] and
its trajectory can not be efﬁciently followed by the general ODE method such as the
backward differentiation formulas (the built-in subroutine ode15s.m of the MATLAB
environment [31, 40]). Thus, we need to construct the special method to solve this
problem. Furthermore, we expect that the new method has the global convergence
as the homotopy continuation methods [2, 37] and the fast convergence rate as the
traditional optimization methods. In order to achieve these two aims, we consider the
continuation Newton method and the trust-region updating strategy for problem (15).

We apply the implicit Euler method to the continuous Newton ﬂow (15) [5, 9],

then we obtain

J(zk+1)

zk+1 − zk
∆tk

= −Fµk+1 (zk+1).

(18)

Since the system (18) is nonlinear which is not directly solved, we seek for its ex-
plicit approximation formula. To avoid solving the nonlinear system of equations,
we replace J(zk+1) with J(zk) and substitute Fµk+1(zk+1) with its linear approxima-
tion Fµk+1(zk) + J(zk)(zk+1 − zk) into equation (18). Then, we obtain a variant of the
damped Newton method:

zk+1 = zk −

∆tk
1 + ∆tk

J(zk)−1Fµk+1(zk).

(19)

Remark 2.2 If we let αk = ∆tk/(1 + ∆tk) in equation (13), we obtain the method
(19). However, from the view of the ODE method, they are different. The damped
Newton method (13) is derived from the explicit Euler method applied to the con-
tinuous Newton ﬂow (15). Its time-stepping size αk is restricted by the numerical
stability [23, 40, 47]. That is to say, for the linear test equation dx/dt = −λ x, its
time-stepping size αk is restricted by the stable region |1 − λ αk| ≤ 1. Therefore, the
large time-stepping size αk can not be adopted in the steady-state phase. The method
(19) is derived from the implicit Euler method applied to the continuous Newton ﬂow
(15) and the linear approximation of Fµk+1(zk+1), and its time-stepping size ∆tk is not
restricted by the numerical stability for the linear test equation. Therefore, the large
time-stepping size ∆tk can be adopted in the steady-state phase, and the method (19)
mimics the Newton method. Consequently, it has the fast convergence rate near the
solution z∗ of the nonlinear system (4). The most of all, the new time-stepping size
αk = ∆tk/(∆tk + 1) is favourable to adopt the trust-region updating strategy for adap-
tively adjusting the time-stepping size ∆tk such that the continuation method (19)
accurately tracks the trajectory of the continuation Newton ﬂow (15) in the transient-
state phase and achieves the fast convergence rate in the steady-state phase.

Primal-dual path-following methods and the trust-region updating strategy

7

We set the parameter µk as the average of the residual sum:

µk =

(cid:107)Axk − b(cid:107)1 + (cid:107)AT yk + sk − c(cid:107)1 + xT
n

k sk

.

(20)

This selection of µk is slightly different to the traditional selection µk = xT
k sk/n [18,
36,43]. According to our numerical experiments, this selection of µk can improve the
robustness of the path-following method. In equation (19), µk+1 is approximated by
σkµk, where the penalty coefﬁcient σk is simply selected as follows:

σk =

(cid:40)

0.05, when µk > 0.05,
µk, when µk ≤ 0.05.

(21)

Thus, from equations (19)-(21), we obtain the following iteration scheme:








A 0 0

0 AT I

Sk 0 Xk















∆ xk

∆ yk

∆ sk








= −








Axk − b
AT yk + sk − c

XkSke − σkµke








= −Fσk µk (zk)

(22)

and

(xk+1, yk+1, sk+1) = (xk, yk, sk) +

∆tk
1 + ∆tk

(∆ xk, ∆ yk, ∆ sk) ,

(23)

where Fµ (z) is deﬁned by equation (6).

When matrix A has full row rank, the linear system (22) can be solved by the

following three subsystems:

AXkS−1

k AT ∆ yk = −

(cid:16)
Xkrk

d − rk
c

(cid:17)(cid:17)

,

(cid:16)
rk
p + AS−1
k
d − AT ∆ yk,

(XkSke + Xk∆ sk − σkµke) ,

∆ sk = −rk
∆ xk = −S−1
k

(24)

(25)

(26)

where the primal residual rk
are respectively deﬁned by

p, the dual residual rk

d and the complementary residual rk
c

rk
p = Axk − b,
d = AT yk + sk − c,
rk
rd
c = SkXke − σkµke.

(27)

(28)

(29)

The matrix AXkS−1

k AT becomes very ill-conditioned when (xk, yk, sk) is close to
the solution (x∗, y∗, s∗) of the nonlinear system (4). Thus, the Cholesky factorization

8

Luo, Yao

method may fail to solve the linear system (24) for the large-scale problem. There-
fore, we use the QR decomposition (pp. 247-248, [22]) to solve it as follows:

DkAT = QkRk, Dk = diag(sqrt(xk./sk)),
(cid:16)
k ∆ ym
rk
RT
p + AS−1
k = −
k
Rk∆ yk = ∆ ym
k .

(cid:16)
Xkrk

d − rk
c

(cid:17)(cid:17)

,

(30)

where Qk ∈ ℜn×m satisﬁes QT
full rank.

k Qk = I and Rk ∈ ℜm×m is an upper triangle matrix with

2.3 The trust-region updating strategy

Another issue is how to adaptively adjust the time-stepping size ∆tk at every iteration.
There is a popular way to control the time-stepping size based on the trust-region
updating strategy [10, 14, 24, 27, 28, 30, 46]. Its main idea is that the time-stepping
size ∆tk+1 will be enlarged when the linear model Fσk µk (zk) + J(zk)∆ zk approximates
Fσk µk (zk + ∆ zk) well, and ∆tk+1 will be reduced when Fσk µk (zk) + J(zk)∆ zk approxi-
mates Fσk µk (zk + J(zk)∆ zk) badly. We enlarge or reduce the time-stepping size ∆tk at
every iteration according to the following ratio:

ρk =

(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk + ∆ zk)(cid:107)
(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk) + J(zk)∆ zk(cid:107)

.

A particular adjustment strategy is given as follows:

∆tk+1 =






2∆tk,
∆tk,
1
2 ∆tk, others,

if 0 ≤ |1 − ρk| ≤ η1 and (xk+1, sk+1) > 0,
else if η1 < |1 − ρk| < η2 and (xk+1, sk+1) > 0,

(31)

(32)

where the constants are selected as η1 = 0.25, η2 = 0.75, according to our numerical
experiments. When ρk ≥ ηa and (xk+1, sk+1) > 0, we accept the trial step and set

(xk+1, yk+1, sk+1) = (xk, yk, sk) +

∆tk
1 + ∆tk

(∆ xk, ∆ yk, ∆ sk),

(33)

where ηa is a small positive number such as ηa = 1.0 × 10−6. Otherwise, we discard
the trial step and set

(xk+1, yk+1, sk+1) = (xk, yk, sk).

(34)

Remark 2.3 This new time-stepping size selection based on the trust-region updat-
ing strategy has some advantages compared to the traditional line search strategy.
If we use the line search strategy and the damped Newton method (13) to track the
trajectory z(t) of the continuous Newton ﬂow (15), in order to achieve the fast conver-
gence rate in the steady-state phase, the time-stepping size αk of the damped Newton

Primal-dual path-following methods and the trust-region updating strategy

9

method is tried from 1 and reduced by the half with many times at every iteration.
Since the linear model Fσk µk (zk) + J(zk)∆ zk may not approximate Fσk µk (zk + ∆ zk)
well in the transient-state phase, the time-stepping size αk will be small. Conse-
quently, the line search strategy consumes the unnecessary trial steps in the transient-
state phase. However, the selection of the time-stepping size ∆tk based on the trust-
region updating strategy (31)-(32) can overcome this shortcoming.

2.4 The treatment of rank-deﬁcient problems

For a real-world problem, the rank of matrix A may be deﬁcient and the constraints
are even inconsistent when the right-hand-side vector b has small noise [29]. How-
ever, the constraints of the original problem are intrinsically consistent. For the rank-
deﬁcient problem with the consistent constraints, there are some efﬁciently pre-solving
methods to eliminate the redundant constraints in references [3, 4, 34]. Here, in order
to handle the inconsistent system of constraints, we consider the following least-
squares approximation problem:

(cid:107)Ax − b(cid:107)2.

min
x∈ℜn

(35)

Then, by solving problem (35), we obtain the consistent system of constraints.

Firstly, we use the QR factorization with column pivoting (pp. 276-278, [22]) to
factor A into a product of an orthogonal matrix Q ∈ ℜm×m and an upper triangular
matrix R ∈ ℜm×n:

AP = QR =

(cid:104)
Q1|Q2

(cid:105)



 = Q1R1,

(36)





R1

0

where r = rank(A), Q1 = Q(1 : m, 1 : r), R1 = R(1 : r, 1 : n), and P ∈ ℜn×n is a
permutation matrix. Then, from equation (36), we know that problem (35) equals the
following problem

(cid:13)
(cid:13)R1PT x − QT

1 b(cid:13)
2
(cid:13)

.

min
x∈ℜn

By solving problem (37), we obtain its solution x as follows:

R1 ˜x = br, x = P ˜x,

where br = QT

1 b.

(37)

(38)

Therefore, when the constraints of problem (1) are consistent, problem (1) equals

the following linear programming problem:

cT
r ˜x subject to R1 ˜x = br, ˜x ≥ 0,

min
x∈ℜn

(39)

where br = QT
1 b and cr = PT c. When the constraints of problem (1) are inconsistent,
the constraints of problem (39) are the least-squares approximation of constraints

10

Luo, Yao

of problem (1). Consequently, in subsection 2.2, we replace matrix A, vector b and
vector c with matrix R1, vector br and vector cr, respectively, then the primal-dual
path-following method can handle the rank-deﬁcient problem.

From the reduced linear programming problem (39), we obtain its KKT condi-

tions as follows:

R1 ˜x − br = 0,
1 ˜y + ˜s − cr = 0,

RT

˜X ˜Se = 0, i = 1, 2, . . . , n,
( ˜x, ˜s) ≥ 0,

(40)

(41)

(42)

(43)

where ˜X = diag( ˜x), ˜S = diag( ˜s), br = QT
1 b and cr = PT c. Thus, from the QR decom-
position (36) of matrix A and equation (2), we can recover the solution (x, y, s) of
equation (2) as follows:

x = P ˜x, y = Q1 ˜y, s = P ˜s,

(44)

where ( ˜x, ˜y, ˜s) is a solution of equations (40)-(43).

Remark 2.4 The preprocessing strategies of the redundant elimination in references
[3, 4, 34] are for empty rows and columns, the row or column singletons, duplicate
rows or columns, forcing and dominated constraints, and ﬁnding the linear depen-
dency based on the Gaussian elimination. Some of those techniques such as for empty
rows and columns, the row or column singletons, duplicate rows or columns can be
also applied to the inconsistent system. However, those preprocessing strategies in
references [3, 4, 34] can not transform an inconsistent system to a consistent system.
Thus, in order to handle the inconsistent system, we can replace the QR decomposi-
tion with the Gaussian elimination method after the preprocessing strategies such as
for empty rows or columns, the row or column singletons, duplicate rows or colums.
Here, for simplicity, we directly use the QR decomposition as the preprocessing strat-
egy and do not use the preprocessing strategies in references [3, 4, 34].

According to the above discussions, we give the detailed descriptions of the
primal-dual path-following method and the trust-region updating strategy for linear
the programming problem (1) in Algorithm 1.

3 Algorithm analysis

We deﬁne the one-sided neighborhood N−∞(γ) as

N−∞(γ) = {(x, y, s) ∈ F0|XSe ≥ γ µe},
where X = diag(x), S = diag(s), e = ones(n, 1), µ = xT s/n and γ is a small positive
constant such as γ = 10−3. In order to simplify the convergence analysis of Algorithm
1, we assume that (i) the initial point (x0, s0) is strictly primal-dual feasible, and (ii)
the time-stepping size ∆tk is selected such that (xk+1, yk+1, sk+1) ∈ N−∞(γ). Without
the loss of generality, we assume that the row rank of matrix A ∈ ℜm×n is full.

(45)

Primal-dual path-following methods and the trust-region updating strategy

11

Algorithm 1 Primal-dual path-following methods and the trust-region updating strat-
egy for linear programming (The PFMTRLP method)
Input:

matrix A ∈ ℜm×n, vectors b ∈ ℜm and c ∈ ℜn for the problem: minx∈ℜn cT x subject to Ax = b, x ≥ 0.

Output:

the primal-dual optimal solution: (solx, soly, sols).
the maximum error of KKT conditions: KKT Error = max{(cid:107)A ∗ solx − b(cid:107)∞, (cid:107)AT ∗ soly + sols −
c(cid:107)∞, (cid:107)solx. ∗ soly(cid:107)∞}.

1 b and cr = PT c.

if (ﬂag success trialstep == 1) then

1: Initialize parameters: ηa = 10−6, η1 = 0.25, η2 = 0.75, ε = 10−6, ∆t0 = 0.9, maxit = 100.
2: Factor matrix A by using the QR decomposition (36). Set br = QT
3: Set bigM = max(max(abs(R1))); bigM = max((cid:107)br(cid:107)∞, (cid:107)cr(cid:107)∞, bigM).
4: Initialize x0 = bigMfac*bigM*ones(n, 1), s0 = x0, y0 = zeros(r, 1).
5: ﬂag success trialstep = 1, itc = 0, k = 0.
6: while (itc < maxit) do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

end if
Set σk = min(0.05, µk).
Compute F 3
By using the QR decomposition to solve (30), we obtain ∆ yk. Compute ∆ sk = −F 2
and ∆ xk = −S−1
k

Set itc = itc + 1.
Compute F 1
Compute Resk = (cid:107)[F 1
if (Resk < ε) then

k = R1xk − br, F 2
k ; F 3

k − σk µk ∗ ones(n, 1). Set Fk = [F 1

k = Xksk, µk = ((cid:107)F 1

1 yk + sk − cr, F 3

k = RT
k ](cid:107)∞.

k (cid:107)1 + (cid:107)F 2

k + Xk∆ sk

k = F 3

k ; F 3
k ].

k ; F 2

k ; F 2

break;

(cid:0)F 3

(cid:1) .

k (cid:107)1 + (cid:107)F 3

k (cid:107)1)/n.

k − Q1∆ yk,

k+1 = Xk+1sk+1 − σk µk ∗ ones(n, 1),

k + Xk(sk+1 − sk) + Sk(xk+1 − xk))].

k+1; F 2

k+1; F 3

k+1; (F 3

17:
18:

19:

Set ∆tk+1 = ∆tk;

Set ∆tk+1 = 2∆tk;

1 xk+1 − br, F 2
k+1].

(∆ xk, ∆ yk, ∆ sk).
1 yk+1 + sk+1 − cr, F 3

else if ((η1 < |ρk − 1| ≤ η2)&&(xk+1, sk+1) > 0)) then

end if
Set (xk+1, yk+1, sk+1) = (xk, yk, sk) + ∆tk
1+∆tk
k+1 = RT
k+1 = RT
Compute F 1
Fk+1 = [F 1
k+1; F 2
Compute LinAppFk+1 = [F 1
Compute the ratio ρk = ((cid:107)Fk(cid:107) − (cid:107)Fk+1(cid:107))/((cid:107)Fk(cid:107) − (cid:107)LinAppFk+1(cid:107)).
if ((|ρk − 1| ≤ η1)&&(xk+1, sk+1) > 0)) then

20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35: end while
36: Return (solx, soly, sols) = (Pxk, Q1yk, Psk), KKT Error = (cid:107)Fk(cid:107)∞.

end if
if ((ρk ≥ ηa)&&(xk+1, sk+1) > 0)) then

Set (xk+1, yk+1, sk+1) = (xk, yk, sk), ﬂag success trialstep = 0.

end if
Set k ← k + 1.

Set ∆tk+1 = 0.5∆tk;

else

else

Accept the trial point (xk+1, yk+1, sk+1). Set ﬂag success trialstep = 1.

Lemma 3.1 Assume (xk, yk, sk) ∈ N−∞(γ), then there exists a sufﬁciently small pos-
itive number δk such that (xk(α), yk(α), sk(α)) ∈ N−∞(γ) holds when 0 < α ≤ δk,
where (xk(α), yk(α), sk(α)) is deﬁned by

(xk(α), yk(α), sk(α)) = (xk, yk, sk) + α(∆ xk, ∆ yk, ∆ sk),

(46)

and (∆ xk, ∆ yk, ∆ sk) is the solution of the linear system (22).

12

Proof.
obtain

Since (xk, yk, sk) is a primal-dual feasible point, from equation (22), we

Luo, Yao

A∆ xk = 0, AT ∆ yk + ∆ sk = 0.

Consequently, from equations (46)-(47), we have

∆ xT

k ∆ sk = −∆ xT

k AT ∆ yk = (A∆ xk)T ∆ yk = 0,
Axk(α) = b, AT yk(α) + sk(α) = c.

From equation (46), we have

Xk(α)Sk(α) = (Xk + α∆ Xk)(Sk + α∆ Sk)

= XkSk + α(Xk∆ Sk + Sk∆ Xk) + α 2∆ Xk∆ Sk.

By replacing Xk∆ Sk + Sk∆ Xk with equation (26) into equation (50), we obtain

Xk(α)Sk(α)e = XkSke + α(σkµke − XkSke) + α 2∆ Xk∆ Ske

= (1 − α)XkSke + ασkµke + α 2∆ Xk∆ Ske.

From equations (48)-(51), we have

µk(α) =

1
n

eT Xk(α)Sk(α)e = (1 − (1 − σk)α)µk.

We denote

β k
max = max
1≤i≤n

{|∆ xi

k|, |∆ si

k|}.

Then, from equation (51), we have

Xk(α)Sk(α)e ≥

(cid:16)
(1 − α)γ µk + ασkµk − α 2(β k

max)2(cid:17)

e.

From equations (52) and (54), we know that the proximity condition

holds, provided that

Xk(α)Sk(α)e ≥ γ µk(α)

(1 − α)γ µk + ασkµk − α 2(β k

max)2 ≥ γ(1 − α + ασk)µk.

By reformulating the above expression, we obtain

α(1 − γ)σkµk ≥ α 2(β k

max)2.

We choose

δk =

(1 − γ)σkµk
max)2

(β k

.

Then, inequality (55) is true when 0 < α ≤ δk.

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

(55)

(56)

(cid:117)(cid:116)

In the following Lemma 3.2, we give the lower bounded estimation of (xk, sk).

Primal-dual path-following methods and the trust-region updating strategy

13

Lemma 3.2 Assume that (x0, y0, s0) > 0 is a primal-dual feasible point and (xk, yk, sk)
(k = 0, 1, . . .) generated by Algorithm 1 satisfy the proximity condition (45). Further-
more, if there exists a constant Cµ such that

holds for all k = 0, 1, 2, . . ., there exist two positive constants Cmin and Cmax such that

µk ≥ Cµ > 0

(57)

0 < Cmin ≤ min
1≤i≤n

{xi

k, si

k} ≤ max
1≤i≤n

{xi

k, si

k} ≤ Cmax

(58)

holds for all k = 0, 1, 2, . . ..

Proof.
dual feasible point, from equation (52), we have

Since (xk, yk, sk) is generated by Algorithm 1 and (xk, yk, sk) is a primal-

µk+1 =

xT
k+1sk+1
n

Consequently, we obtain

= (1 − (1 − σk)αk)µk ≤ µk, k = 0, 1, . . . .

µk+1 =

k
∏
i=0

(1 − (1 − σi)αi)µi ≤ µ0, k = 0, 1, . . . .

From equation (49), we have

A(xk − x0) = 0, AT (yk − y0) + (sk − s0) = 0.

Consequently, we obtain

(xk − x0)T (sk − s0) = −(xk − x0)T AT (yk − y0) = 0.

By rearranging this expression and using the property (59), we obtain

k s0 + sT
xT

k x0 = xT

k sk + xT

0 s0 ≤ n(µk + µ0) ≤ 2nµ0.

Consequently, we obtain

xi
k ≤

2nµ0
min1≤ j≤n{s j
0}

and si

k ≤

2nµ0
min1≤ j≤n{x j
0}

, 1 ≤ i ≤ n, k = 0, 1, . . . .

Therefore, if we select

(cid:40)

Cmax = max

2nµ0
min1≤ j≤n{s j
0}

,

2nµ0
min1≤ j≤n{x j
0}

(cid:41)

,

we obtain

max
1≤i≤n

{xi

k, si

k} ≤ Cmax, k = 0, 1, . . . .

(59)

(60)

(61)

14

Luo, Yao

On the other hand, from the assumption (57) and the proximity condition (45),

we have

ksi
xi

k ≥ γ µk ≥ γCµ , 1 ≤ i ≤ n, k = 0, 1, . . . .

By combining it with the estimation (61) of (xk, sk), we obtain

xi
k ≥

γCµ
max1≤ j≤n{s j
k}

≥

γCµ
Cmax

, and si

k ≥

γCµ
max1≤ j≤n{x j
k}

≥

γCµ
Cmax

, k = 0, 1, . . . . (62)

We select Cmin = γCµ /Cmax. Then, from equation (62), we obtain
k, si

k} ≥ Cmin, k = 0, 1, 2, . . . .

{xi

min
1≤i≤n

(cid:117)(cid:116)

Lemma 3.3 Assume that (x0, y0, s0) > 0 is a primal-dual feasible point and (xk, yk, sk)
(k = 0, 1, . . .) generated by Algorithm 1 satisfy the proximity condition (45). Further-
more, if the assumption (57) holds for all k = 0, 1, . . ., there exit two positive constants
C∆ x and C∆ s such that

(cid:107)∆ sk(cid:107) ≤ C∆ s and (cid:107)∆ xk(cid:107) ≤ C∆ x

(63)

hold for all k = 0, 1, . . ..

Proof.

Factorize matrix A with the singular value decomposition (pp. 76-80, [22]):

A = UΣV T , Σ =







Σr 0

0 0

 , Σr = diag(λ1, λ2, . . . , λr) (cid:31) 0,

(64)

where U ∈ ℜm×m and V ∈ ℜn×n are orthogonal matrices, and the rank of matrix A
equals r. Then, from the bounded estimation (58) of (xk, sk), we have

and

zAXkS−1

k AT z ≥

zAXkS−1

k AT z ≤

Cmin
Cmax

Cmax
Cmin

(cid:107)Az(cid:107)2 ≥

Cminλ 2
min
Cmax

(cid:107)z(cid:107)2, k = 0, 1, . . . , ∀z ∈ ℜn,

(65)

(cid:107)Az(cid:107)2 ≤

Cmaxλ 2
Cmin

max

(cid:107)z(cid:107)2, k = 0, 1, . . . , ∀z ∈ ℜn,

(66)

where λmin and λmax are the smallest and largest singular values of matrix A, respec-
tively.

From equations (24), (58) and (65)-(66), we obtain

Cminλ 2
min
Cmax
≤ (cid:107)∆ yk(cid:107)(cid:107)A(cid:107) (cid:13)

(cid:107)∆ yk(cid:107)2 ≤ ∆ yT
k

(cid:0)AXkS−1

k AT (cid:1) ∆ yk = ∆ yT

k AS−1

k (XkSke − σkµke)

(cid:13)S−1
k

(cid:13)
(cid:13) (cid:107)XkSke − σkµke(cid:107) ≤ (cid:107)∆ yk(cid:107)

λmax
Cmin

((cid:107)XkSke(cid:107) + nσkµk)

≤ (cid:107)∆ yk(cid:107)

λmax
Cmin

((cid:107)XkSke(cid:107)1 + nσkµk) = (cid:107)∆ yk(cid:107)

λmax
Cmin

(1 + σk)nµk.

≤

≤

≤

1
Cmin
1
Cmin
1
Cmin

Primal-dual path-following methods and the trust-region updating strategy

15

That is to say, we obtain

(cid:107)∆ yk(cid:107) ≤

Cmaxλmax
minλ 2
C2
min

(1 + σk)nµk ≤

Cmaxλmax
minλ 2
C2
min

2nµk ≤

Cmaxλmax
minλ 2
C2
min

2nµ0.

(67)

The second inequality of equation (67) can be inferred by σk ≤ 1 from equation (21).
The last inequality of equation (67) can be inferred by µk ≤ µ0 from equation (59).
Therefore, from equation (25) and equation (67), we have

(cid:107)∆ sk(cid:107) = (cid:107) − AT ∆ yk(cid:107) ≤ (cid:107)AT (cid:107)(cid:107)∆ yk(cid:107) ≤

Cmaxλ 2
minλ 2
C2
min
min). Thus, from equation (68), we prove the

2nµ0.

(68)

max

We set C∆ s = (Cmaxλ 2
ﬁrst part of equation (63).

max2nµ0)/(C2

minλ 2

From equations (26), (58) and the ﬁrst part of equation (63), we have
(cid:107)∆ xk(cid:107) = (cid:13)

(cid:13)
(cid:13) (cid:107)XkSke + Xk∆ sk − σkµke(cid:107)

(cid:13)−S−1

(cid:13) ≤ (cid:13)

(cid:13)S−1
k

k (XkSke + Xk∆ sk − σ µke)(cid:13)
((cid:107)XkSke(cid:107) + (cid:107)Xk∆ sk(cid:107) + (cid:107)σkµke(cid:107))

((cid:107)XkSke(cid:107)1 + (cid:107)Xk(cid:107)(cid:107)∆ sk(cid:107) + nσkµk(cid:107))

(nµk +CmaxC∆ s + nσkµk) ≤

1
Cmin

(2nµ0 +CmaxC∆ s) .

(69)

The last inequality of equation (69) can be inferred by σk ≤ 1 from equation (21)
and µk ≤ µ0 from equation (59). We set C∆ x = (2nµ0 +CmaxC∆ s) /Cmin. Thus, from
(cid:117)(cid:116)
equation (69), we also prove the second part of equation (63).

Lemma 3.4 Assume that (x0, y0, s0) > 0 is a primal-dual feasible point and (xk, yk, sk)
(k = 0, 1, . . .) generated by Algorithm 1 satisfy the proximity condition (45). Further-
more, if the assumption (57) holds for all k = 0, 1, . . ., there exits a positive constant
C∆t such that

∆tk ≥ C∆t > 0

(70)

holds for all k = 0, 1, 2, . . ..

Proof.

From equations (31), (22)-(23), we have

|ρk − 1| =

=

=

≤

=

(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk + ∆ zk)(cid:107)
(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk) + J(zk)∆ zk(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk + ∆ zk)(cid:107)
(cid:12)
(cid:12)
(cid:107)Fσk µk (zk)(cid:107) − (cid:107)Fσk µk (zk) − (∆tk)/(1 + ∆tk)Fσk µk (zk)(cid:107)
(cid:12)
(cid:12)(cid:107)Fσk µk (zk)(cid:107) − (1 + ∆tk)(cid:107)Fσk µk (zk + ∆ zk)(cid:107)(cid:12)
(cid:12)
(cid:12)
∆tk(cid:107)Fσk µk (zk)(cid:107)

(cid:13)
(cid:13)(1 + ∆tk) (cid:0)Fσk µk (zk + ∆ zk) − Fσk µk (zk)(cid:1) + ∆tkFσk µk (zk)(cid:13)
(cid:13)
∆tk(cid:107)Fσk µk (zk)(cid:107)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

∆tk
1 + ∆tk

(cid:107)∆ Xk∆ Ske(cid:107)
(cid:107)Xksk − σkµke(cid:107)

.

(71)

16

Luo, Yao

The last equality of equation (71) can be inferred from

Fσk µk (zk + ∆ zk) − Fσk µk (zk) = J(zk)∆ zk +

(cid:18) ∆tk

(cid:19)2

1 + ∆tk

∆ Xk∆ Ske

= −

∆tk
1 + ∆tk

Fσk µk (zk) +

(cid:18) ∆tk

(cid:19)2

1 + ∆tk

∆ Xk∆ Ske.

On the other hand, from the property (cid:107)a(cid:107) ≥ ai (i = 1, 2, . . . , n), we have

(cid:107)Xksk − σkµke(cid:107) ≥ xi

ksi

k − σkµk, i = 1, 2, . . . , n.

By summing the n components of the above two sides and µk = xT

k sk/n, we obtain

(cid:107)Xksk − σkµke(cid:107) ≥ (1 − σk)µk ≥ (1 − 0.05)Cµ = 0.95Cµ .

(72)

The second inequality of equation (72) can be inferred by σk ≤ 0.05 from equation
(21) and the assumption µk ≥ Cµ from equation (57).

Thus, from the bounded estimation (63) of (∆ xk, ∆ sk) and equations (71)-(72),

we obtain

|ρk − 1| ≤

≤

∆tk
1 + ∆tk
∆tk
1 + ∆tk

(cid:107)∆ Xk∆ Ske(cid:107)
0.95Cµ
(cid:107)∆ xk(cid:107)(cid:107)∆ sk(cid:107)
0.95Cµ

≤

≤

∆tk
1 + ∆tk
∆tk
1 + ∆tk

(cid:107)∆ Xk(cid:107)(cid:107)∆ sk(cid:107)
0.95Cµ

C∆ xC∆ s
0.95Cµ

≤ η1,

provided that

∆tk ≤

0.95Cµ η1
C∆ xC∆ s

.

(73)

Thus, if we assume that K is the ﬁrst index such that ∆tK satisﬁes equation (73), ac-
cording to the trust-region updating formula (32), ∆tK+1 will be enlarged. Therefore,
we prove the result (70) if we set

C∆t =

0.95Cµ η1
2C∆ xC∆ s

.

According to the above discussions, we give the global convergence analysis of

Algorithm 1.

Theorem 3.1 Assume that (x0, y0, s0) > 0 is a primal-dual feasible point and (xk, yk, sk)
(k = 0, 1, . . .) generated by Algorithm 1 satisfy the proximity condition (45). Then,
we have

(cid:117)(cid:116)

lim
k→∞

µk = 0,

(74)

where µk = xT

k sk/n.

Primal-dual path-following methods and the trust-region updating strategy

Proof. Assume that there exists a positive constant Cµ such that

µk ≥ Cµ

17

(75)

holds for all k = 0, 1, . . .. Then, according to the result of Lemma 3.4, we know that
there exists a positive constant C∆t such that ∆tk ≥ C∆t holds for all k = 0, 1, . . ..
Therefore, from equation (52), we have

µk+1 = µk(αk) = (1 − (1 − σk)αk)µk ≤

1 − (1 − σk)

(cid:18)

(cid:19)

µk

C∆t
1 +C∆t

(cid:18)

≤

1 − 0.95

C∆t
1 +C∆t

(cid:19)

(cid:18)

µk ≤

1 − 0.95

(cid:19)k+1

µ0.

C∆t
1 +C∆t

(76)

The second inequality of equation (76) can be inferred by σk ≤ 0.05 from equation
(21). Thus, we have µk → 0, which contradicts the assumption (75). Consequently,
we obtain limk→∞ inf µk = 0. Since µk is monotonically decreasing, it is not difﬁcult
to know limk→∞ µk = 0. Furthermore, we obtain limk→∞ (cid:107)Xksk(cid:107) = 0 from (cid:107)Xksk(cid:107) ≤
(cid:107)Xksk(cid:107)1 = nµk and (xk, sk) > 0.
(cid:117)(cid:116)

Remark 3.1 Our analysis framework of Algorithm 1 is the same as that of the clas-
sical primal-dual interior method (pp. 411-413, [36]). However, the result of Lemma
3.4 is new.

4 Numerical experiments

In this section, we test Algorithm 1 (the PFMTRLP method) for some linear pro-
gramming problems with full rank matrices or rank-deﬁcient matrices, and compare
it with the traditional path-following method (pathfollow.m, p. 210, [18]) and the
state-of-the-art predictor-corrector algorithm (the built-in subroutine linprog.m of the
MATLAB environment [31, 32, 48]).

The tolerable errors of three methods are all set by ε = 10−6. We use the max-
imum absolute error (KKTError) of the KKT condition (2) and the primal-dual gap
xT s to measure the error between the numerical optimal solution and the theoretical
optimal solution.

4.1 The problem with full rank

For the standard linear programming problem with full rank, the sparse matrix A of
given density 0.2 is randomly generated and we choose feasible x, y, s at random,
with x and s each about half-full. The dimension of matrix A varies from 10 × 100
to 300 × 3000. One of its implementation is given by Algorithm 2 (p. 210, [18]).
According to Algorithm 2, we randomly generate 30 standard linear programming
problems with full rank matrices.

18

Luo, Yao

Algorithm 2 The standard linear programming problem with full rank
Input:

the number of equality constraints: m;
the number of unknown variables: n.

Output:

matrix A and vectors b ∈ ℜm and c ∈ ℜn.

1: density=0.2;
2: A = sprandn(m,n,density); % Generate a sparse matrix of give density.
3: xfeas = [rand(n/2,1); zeros(n-(n/2),1)];
4: sfeas = [zeros(n/2,1); rand(n-(n/2),1)];
5: xfeas = xfeas(randperm(n));
6: sfeas = sfeas(randperm(n));
7: yfeas = (rand(m,1)-0.5)*4;

% Choose b and c to make this (x,y,s) feasible.

8: b = A*xfeas;
9: c = A’*yfeas + sfeas;

For those 30 test problems, we compare Algorithm 1 (the PFMTRLP method),
Mehrotra’s predictor-corrector algorithm (the subroutine linprog.m of the MATLAB
environment), and the path-following method (the subroutine pathfollow.m). The nu-
merical results are arranged in Table 1 and illustrated in Figure 1. The left sub-
ﬁgure of Figure 1 represents the number of iterations and the right sub-ﬁgure rep-
resents the consumed CPU time. From Table 1, we ﬁnd that PFMTRLP and lin-
prog.m can solve all test problems, and their KKTErrors are small. However, path-
follow.m cannot perform well for some higher-dimensional problems, such as exam
7, 11, 13, 23, 24, 25, 26, 28, since their solutions do not satisfy the KKT conditions.
From Figure 1, we also ﬁnd that linprog.m performs the best, and the number of its
iterations is less than 20. The number of iterations of PFMTRLP is around 20, and
the number of iterations of pathfollow.m often reaches the maximum number (i.e.
200 iterations). Therefore, PFMTRLP is also an efﬁcient and robust path-following
method for the linear programming problem with full rank.

(a) The number of iterations

(b) The computational time

Fig. 1: The number of iterations and the computational time.

051015202530Exam020406080100120140160180200IterationsPFMTRLPLinprogpath-following051015202530Exam012345678910Solution timePFMTRLPLinprogpath-followingPrimal-dual path-following methods and the trust-region updating strategy

19

Table 1: Numerical results of problems with full rank matrices.

Problem (m × n, r)

Exam. 1 (10 × 100, 10)

Exam. 2 (20 × 200, 20)

Exam. 3 (30 × 300, 30)

Exam. 4 (40 × 400, 40)

Exam. 5 (50 × 500, 50)

Exam. 6 (60 × 600, 60)

Exam. 7 (70 × 3700, 70)

Exam. 8 (80 × 800, 80)

Exam. 9 (90 × 900, 90)

Exam. 10 (100 × 1000, 100)

PFMTRLP

linprog

pathfollow

KKTError

Gap

KKTError

Gap

KKTError

Gap

3.77E-06

2.07E-06

9.68E-06

3.16E-06

1.14E-05

1.42E-06

1.05E-04

7.67E-06

7.67E-06

1.93E-05

3.61E-05

1.98E-07

1.18E-08

1.03E-07

1.46E-04

3.20E-10

8.83E-14

6.62E-08

1.28E-03

1.07E-11

3.10E-10

1.83E-09

5.67E-04

6.55E-08

1.15E-06

1.34E-07

2.47E-03

5.12E-07

3.79E-05

1.25E-08

2.26E-04

6.83E-09

2.90E-07

3.95E-09

6.27E-06

1.04E-05

4.48E-07

4.42E-05

4.55E-06

1.79E-06

1.27E-02

4.21E-07

3.71E-05

4.61E+04

4.48E-04

2.36E-03

4.40E-09

5.09E-07

4.03E-07

2.43E-03

1.14E-09

1.08E-07

2.62E-08

7.86E-03

2.34E-12

3.67E-11

9.09E-09

2.64E-04

1.64E-05

6.65E-06

Exam. 11 (1100 × 1100, 110)

9.00E-05

3.54E-02

6.36E-08

1.13E-05

8.52E+04

9.33E-04

Exam. 12 (120 × 1200, 120)

Exam. 13 (130 × 1300, 130)

Exam. 14 (140 × 1400, 140)

Exam. 15 (150 × 1500, 150)

Exam. 16 (160 × 1600, 160)

Exam. 17 (170 × 1700, 170)

Exam. 18 (180 × 1800, 180)

Exam. 19 (190 × 1900, 190)

Exam. 20 (200 × 2000, 200)

Exam. 21 (210 × 2100, 210)

Exam. 22 (220 × 2200, 220)

Exam. 23 (230 × 2300, 230)

1.46E-05

8.44E-06

3.33E-06

7.23E-05

1.19E-05

4.45E-05

1.22E-04

6.51E-05

2.49E-05

1.42E-04

1.59E-05

3.64E-04

5.08E-03

3.84E-08

1.18E-05

1.08E-09

4.78E-07

3.78E-03

1.37E-10

2.67E-08

1.23E+05

2.17E-03

1.55E-03

3.53E-07

3.78E-05

8.73E-07

2.71E-02

3.59E-07

4.16E-05

1.25E-06

6.79E-03

3.58E-08

1.22E-05

3.07E-07

2.25E-02

6.33E-11

1.37E-08

3.53E-09

6.32E-02

8.85E-07

3.28E-04

1.88E-08

5.81E-02

8.86E-08

6.25E-06

2.06E-07

1.61E-02

7.56E-07

4.67E-04

1.23E-06

7.20E-02

3.50E-13

7.61E-11

1.33E-06

1.04E-02

1.64E-07

7.08E-05

2.72E-08

8.96E-04

1.40E-03

4.06E-04

4.74E-06

2.37E-05

3.24E-04

1.91E-03

1.92E-03

4.33E-05

2.28E-01

8.65E-14

2.62E-11

2.75E+05

1.48E-02

Exam. 24 (240 × 24000, 240)

9.80E-05

8.36E-02

6.76E-07

2.19E-04

2.39E+05

2.19E-02

Exam. 25 (250 × 2500, 250)

Exam. 26 (260 × 2600, 260)

3.06E-04

1.21E-05

1.80E-01

8.40E-08

3.59E-05

2.13E+05

8.76E-02

8.87E-03

7.98E-09

2.15E-07

5.59E+05

3.12E-01

Exam. 27 (270 × 2700, 2700)

1.15E-04

1.44E-01

4.79E-07

1.33E-04

2.06E-08

4.20E-05

Exam. 28 (280 × 2800, 280)

Exam. 29 (290 × 2900, 290)

Exam. 30 (300 × 3000, 300)

3.33E-05

4.42E-05

7.84E-05

4.27E-02

2.58E-13

1.13E-10

4.94E+05

1.87E-02

4.38E-02

3.31E-07

1.04E-04

3.81E-08

4.21E-02

1.82E-08

1.24E-05

5.38E-08

8.19E-05

1.30E-04

4.2 The rank-deﬁcient problem with noisy data

For a real-world problem, the rank of matrix A in problem (1) may be deﬁcient and the
constraints are even inconsistent when the right-hand-side vector b has small noise.
However, the constraints of the original problem are intrinsically consistent. In order
to evaluate the effect of PFMTRLP handling those problems, we select some rank-
deﬁcient problems from the NETLIB collection [35] as test problems and compare it
with linprog.m for those problems with or without the small noisy data.

The numerical results of the problems without the noisy data are arranged in Table
2. Then, we set b = b + rand(m, 1) ∗ ε for those test problems, where ε = 10−5. The
numerical results of the problems with the small noise are arranged in Table 3. From
Tables 2 and 3, we can ﬁnd that PFMTRLP can solve all those problems with or

20

Luo, Yao

without the small noise. However, from Table 3, we ﬁnd that linprog.m can not solve
some problems with the small noise since inprog.m outputs NaN for those problems.
Furthermore, although linprog.m can solve some problems, the KKT errors or the
primal-dual gaps are large. For those problems, we conclude that linprog.m also fails
to solve them. Therefore, from Table 3, we ﬁnd that PFMTRLP is more robust than
linprog.m for the rank-deﬁcient problem with the small noisy data.

Table 2: Numerical results of some rank-deﬁcient problems from NETLIB.

Problem (m × n, r)

lp brandy
(220 × 303, 193)

lp bore3d
(233 × 334, 231)

lp wood1p
(244 × 2595, 243)

lp scorpion
(388 × 466, 358)

lp ship04s
(402 × 1506, 360)

lp ship04l
(402 × 2166, 360)

lp degen2
(444 × 757, 442)

lp bnl1
(643 × 1586, 642)

lp ship08s
(778 × 2467, 712)

lp qap8
(912 × 1632, 742)

lp 25fv47
(821 × 1876, 820)

lp ship08l
(778 × 4363, 712)

lp ship12l
(1151 × 5533, 1041)

lp ship12s
(1151 × 2869, 1042)

lp degen3
(1503 × 2604, 1501)

lp qap12
(3192 × 8856, 2794)

lp cre c
(3068 × 6411, 2981)

lp cre a
(3516 × 7248, 3423)

PFMTRLP

linprog

KKTError

Gap

Iter

CPU

KKTError

Gap

Iter

CPU

2.37E-03

3.57E-02

38

0.19

2.44E-08

1.14E-08

16

0.09

3.00E-08

3.32E-06

43

0.22

5.67E-10

1.37E+03

17

0.24

5.09E-05

3.09E-05

100

5.28

1.80E-12

3.16E-11

20

0.26

6.16E-07

8.65E-05

37

0.64

2.12E-13

5.08E-08

14

0.05

7.06E-07

2.71E-03

36

1.83

2.39E-07

2.24E-04

13

0.03

1.79E-07

1.00E-03

36

2.48

1.04E-11

1.02E-04

12

0.08

8.90E-07

4.45E-05

35

1.23

2.79E-13

9.43E-10

14

0.40

7.40E-07

6.50E-04

97

11.33

3.36E-09

3.16E-06

26

0.08

4.34E-07

3.29E-03

41

12.72

2.12E-11

1.98E-07

14

0.04

6.84E-07

1.82E-04

22

4.78

2.67E-11

6.48E-07

9

0.49

5.74E-07

9.20E-04

80

14.67

3.35E-10

6.10E-11

25

0.24

9.95E-07

9.87E-03

40

18.84

1.07E-09

4.28E-03

15

0.15

9.22E-07

4.21E-03

47

37.06

4.08E-10

1.15E-02

15

0.06

3.01E-07

9.36E-04

47

26.11

1.75E-11

1.50E-05

16

0.05

4.79E-07

1.01E-05

48

27.61

5.90E-10

3.55E-08

21

0.52

7.00E-07

6.86E-04

25

228.75

1.01E-05

2.95E-04

85

218.06

7.97E-07

4.79E-01

90

516.67

5.15E-10

4.50E-02

27

0.21

7.86E-07

7.02E-01

83

707.98

7.26E-09

7.10E-02

28

0.24

Primal-dual path-following methods and the trust-region updating strategy

21

Table 3: Numerical results of rank-deﬁcient problems with noise ε = 10−5.

PFMTRLP

linprog

KKTError

Gap

Iter

CPU

KKTError

Gap

Iter

CPU

4.21E-02

2.63E+00

28

0.06

Failed

Failed

1.84E-01

1.69E+01

42

0.19

Failed

Failed

5.03E-02

1.05E+01

18

0.36

Failed

Failed

0

0

0

0.01

0.01

0.01

1.41E-03

9.63E-02

28

1.22

2.92E-04

2.12E+10

52

0.37

3.91E-02

2.93E+02

23

1.56

Failed

Failed

3.49E-03

2.68E+00

61

7.70

Failed

Failed

8.08E-07

2.19E-04

22

4.58

6.24E-04

6.93E+02

6.34E-07

1.01E-03

80

14.41

Failed

Failed

9.44E-03

7.35E+01

25

2.03

Failed

Failed

1.69E-02

1.72E+02

24

8.31

Failed

Failed

0

0

6

0

0

0

0.00

0.01

0.35

0.01

0.00

0.00

8.91E-04

6.99E-04

60

3.16

2.41E-04

4.64E+07

33

0.62

9.56E-04

2.92E-02

37

24.06

6.84E-04

1.89E+05

77

2.53

1.36E-02

6.26E+01

31

18.39

Failed

Failed

1.36E-02

1.55E+02

24

12.80

Failed

Failed

1.18E-02

5.88E+01

31

29.44

Failed

Failed

5.81E-02

3.84E+04

43

277.59

Failed

Failed

7.37E-02

6.73E+04

37

350.22

Failed

Failed

0

0

0

0

0

0.03

0.00

0.00

0.01

0.00

7.01E-07

6.86E-04

25

240.84

2.07E-02

1.40E+03

10

24.80

Problem (m × n, r)

lp brandy
(220 × 303, 193)

lp bore3d
(233 × 334, 231)

lp scorpion
(388 × 466, 358)

lp degen2
(444 × 757, 442)

lp ship04s
(402 × 1506, 360)

lp bnl1
(643 × 1586, 642)

lp qap8
(912 × 1632, 742)

lp 25fv47
(821 × 1876, 820)

lp ship04l
(402 × 2166, 360)

lp ship08s
(778 × 2467, 712)

lp wood1p
(244 × 2595, 243)

lp degen3
(1503 × 2604, 1501)

lp ship12s
(1151 × 2869, 1042)

lp ship08l
(778 × 4363, 712)

lp ship12l
(1151 × 5533, 1041)

lp cre c
(3068 × 6411, 2981)

lp cre a
(3516 × 7248, 3423)

lp qap12
(3192 × 8856, 2794)

5 Conclusions

For the rank-deﬁcient linear programming problem, we give a preprocessing method
based on the QR decomposition with column pivoting. Then, we consider the primal-
dual path-following and the trust-region updating strategy for the postprocessing
problem. Finally, we prove that the global convergence of the new method when the
initial point is strictly prima-dual feasible. According to our numerical experiments,
the new method (PFMTRLP) is more robust than the path-following methods such

22

Luo, Yao

as pathfollow.m (p. 210, [18]) and linprog.m [31, 32, 48] for the rank-deﬁcient prob-
lem with the small noisy data. Therefore, PFMTRLP is worth exploring further as a
primal-dual path-following method with the new adaptive step size selection based
on the trust-region updating stratgy. The computational efﬁciency of PFMTRLP has
a room to improve.

Acknowledgments

This work was supported in part by Grant 61876199 from National Natural Science
Foundation of China, Grant YBWL2011085 from Huawei Technologies Co., Ltd.,
and Grant YJCB2011003HI from the Innovation Research Program of Huawei Tech-
nologies Co., Ltd.. The ﬁrst author is grateful to professor Li-zhi Liao for introducing
him the interior-point methods when he visited Hongkong Baptist University in July,
2012. The authors are grateful to two anonymous referees for their comments and
suggestions which greatly improve the presentation of this paper.

References

1. I. Adler and R.D.C. Monteiro, Limiting behavior of the afﬁne scaling continuous trajectories for linear

programming problems, Math. Program. 50 (1991), pp. 29–51.

2. E.L. Allgower and K. Georg, Introduction to Numerical Continuation Methods, SIAM, Philadelphia,

2003.

3. E.D. Andersen and K.D. Andersen, Presolving in linear programming, Math. Program. 71 (1995), pp.

221–245.

4. E.D. Andersen, Finding all linearly dependent rows in large-scale linear programming, Optim. Meth-

ods Softw. 6 (1995), pp. 219–227.

5. U.M. Ascher and L.R. Petzold, Computer Methods for Ordinary Differential Equations and

Differential-Algebraic Equations, SIAM, Philadelphia, 1998.

6. I. Averbakh and Y.B. Zhao, Explicit reformulations for robust optimization problems with general

uncertainty sets, SIAM J. Optim. 18 (2008), pp. 1436–1466.

7. O. Axelsson and S. Sysala, Continuation Newton methods, Computer and Mathematics with Applica-

tions 70 (2015), pp. 2621–2637.

8. F.H. Branin, Widely convergent method for ﬁnding multiple solutions of simultaneous nonlinear equa-

tions, IBM J. Res. Dev. 16 (1972), pp. 504–521.

9. K.E. Brenan, S.L. Campbell, and L.R. Petzold, Numerical solution of initial-value problems in

differential-algebraic equations, SIAM, Philadelphia, 1996.

10. A. R. Conn, N. Gould, and Ph. L. Toint, Trust-Region Methods, SIAM, 2000.
11. M. T. Chu and M. M. Lin, Dynamical system characterization of the central path and its variants- a

vevisit, SIAM J. Appl. Dyn. Syst. 10 (2011), pp. 887-905.

12. B.D. Craven and S.M.N. Islam, Linear programming with uncertain data: extensions to robust opti-

mization, J. Optim. Theory Appl. 155 (2012), pp. 673–679.

13. D.F. Davidenko, On a new method of numerical solution of systems of nonlinear equations (in Rus-

sian), Dokl. Akad. Nauk SSSR 88 (1953), pp. 601–602.

14. P. Deuﬂhard, Newton Methods for Nonlinear Problems: Afﬁne Invariance and Adaptive Algorithms,

Springer, Berlin, 2004.

15. P. Deuﬂhard, H.J. Pesch, and P. Rentrop, A modiﬁed continuation method for the numerical solution
of nonlinear two-point boundary value problems by shooting techniques, Numer. Math. 26 (1975), pp.
327–343.

16. J.E. Dennis and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear

Equations, SIAM, Philadelphia, 1996.

Primal-dual path-following methods and the trust-region updating strategy

23

17. E.J. Doedel, Lecture notes in numerical analysis of nonlinear equations, in Numerical Continuation
Methods for Dynamical Systems, B. Krauskopf, H.M. Osinga, and J. Gal´an-Vioque, eds., Springer,
Berlin, 2007, pp. 1–50.

18. M.C. Ferris, O.L. Mangasarian, and S.J. Wright, Linear Programming with MATLAB, SIAM,
Philadelphia, 2007. Software available at http://pages.cs.wisc.edu/~ferris/ferris.
books.html.

19. A.V. Fiacco and G.P. McCormick, Nonlinear programming: Sequential Unconstrained Minimization

Techniques, SIAM, Philadelphia, 1990.

20. O. G¨uler and Y. Ye, Convergence behavior of interior-point algorithms, Math. Program. 60 (1993),

pp. 215–228.

21. C.C. Gonzaga, Path-following methods for linear programming, SIAM Rev. 34 (1992), pp. 167–224.
22. G.H. Golub and C.F. Van Loan, Matrix Computation, 4th ed., The John Hopkins University Press,

Baltimore, 2013.

23. E. Hairer and G. Wanner, Solving Ordinary Differential Equations II, Stiff and Differential-Algebraic

Problems, 2nd ed., Springer, Berlin, 1996.

24. D.J. Higham, Trust region algorithms and timestep selection, SIAM J. Numer. Anal. 37 (1999), pp.

194–210.

25. N. Karmarkar, A new polynomial-time algorithm for linear programming, Combinatorica 4 (1984),

pp. 373–395.

26. L.-Z. Liao, A study of the dual afﬁne scaling continuous trajectories for linear programming, J. Optim.

Theory Appl. 163 (2014), pp. 548–568.

27. X.-L. Luo, A second-order pseudo-transient method for steady-state problems, Appl. Math. Comput.

216 (2010), pp. 1752–1762.

28. X.-L. Luo, A dynamical method of DAEs for the smallest eigenvalue problem, J. Comput. Sci. 3

(2012), pp. 113–119.

29. X.-L. Luo, J.-H. Lv and G. Sun, Continuation method with the trusty time-stepping scheme for lin-
early constrained optimization with noisy data, Optim. Eng., Accepted, available at http://doi.
org/10.1007/s11081-020-09590-z or arXiv preprint http://arxiv.org/abs/2005.05965,
December 20, 2020.

30. X.-L. Luo, H. Xiao and J.-H. Lv, Continuation Newton methods with the residual trust-region
time-stepping scheme for nonlinear equations, arXiv preprint no. 2006.02634 available at https:
//arxiv.org/abs/2006.02634, June 2020.

31. MATLAB 9.6.0 (R2019a), The MathWorks Inc., available at http://www.mathworks.com, 2019.
32. S. Mehrotra, On the implementation of a primal-dual interior point method, SIAM J. Optim. 2 (1992),

pp. 575–601.

33. R.D.C. Monteiro, Convergence and boundary behavior of the projective scaling trajectories for linear

programming, Math. Oper. Res. 16 (1991), pp. 842–858.

34. Cs. M´esz´aros and U.H. Suhl, Advanced preprocessing techniques for linear and quadratic program-

ming, OR Spectrum 25 (2003), pp. 575-595.

35. The NETLIB collection of linear programming problems, http://www.netlib.org.
36. J. Nocedal and S.J. Wright, Numerical Optimization, Springer, Berlin, 1999.
37. J.M. Ortega and W.C. Rheinboldt, Iteration Solution of Nonlinear Equations in Several Variables,

SIAM, Philadelphia, 2000.

38. P.Q. Pan, A fast simplex algorithm for linear programming, J. Comput. Math. 28 (2010), pp. 837–847.
39. M.J.D. Powell, Convergence properties of a class of minimization algorithms, in Nonlinear Program-
ming 2, O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., Academic Press, New York, 1975,
pp. 1-27.

40. L.F. Shampine, I. Gladwell, and S. Thompson, Solving ODEs with MATLAB, Cambridge University

Press, Cambridge, 2003.

41. K. Tanabe, Continuous Newton-Raphson method for solving an underdetermined system of nonlinear

equations, Nonlinear Anal. 3 (1979), pp. 495–503.

42. K. Tanabe, Centered Newton method for mathematical programming, in System Modeling and Opti-
mization: Proceedings of the 13th IFIP conference, vol. 113 of Lecture Notes in Control and Infor-
mation Systems, Springer, Berlin, 1988, pp. 197–206.

43. S.J. Wright, Primal-dual Interior Point Methods, SIAM, Philadelphia, 1997.
44. Y.Y. Ye, Interior Point Algorithms-Theory and Analysis, Wiley, New York, 1997.
45. Y. Ye, M. J. Todd and S. Mizuno, An O(

√

nL)-iteration homogeneous and self-dual linear program-

ming algorithm, Math. Oper. Res. 19 (1994), pp. 53–67.

24

Luo, Yao

46. Y. Yuan, Recent advances in trust region algorithms, Math. Program. 151: 249-281, 2015.
47. Z.-D. Yuan, J.-G. Fei, and D.-G. Liu, The Numerical Solutions for Initial Value Problems of Stiff

Ordinary Differential Equations (in Chinese), Science Press, Beijing, 1987.

48. Y. Zhang, Solving large-Scale linear programs by interior-point methods under the MATLAB environ-

ment, Optim. Methods Softw. 10 (1998), pp. 1–31.

49. L.H. Zhang, W.H. Yang, and L.-Z. Liao, On an efﬁcient implemetation of the face algorithm for linear

programming, J. Comput. Math. 31 (2013), pp. 335–354.

