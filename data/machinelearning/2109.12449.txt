AbstractDifferentiation.jl:Backend-AgnosticDifferentiableProgramminginJuliaFrankSchäferDepartmentofPhysicsUniversityofBasel,Switzerlandfrank.schaefer@unibas.chMohamedTarekPumas-AIInc.,USAUNSWCanberra,Australiamohamed@pumas.aiLyndonWhiteInveniaLabsCambridge,UKlyndon.white@invenialabs.co.ukChrisRackauckasMassachusettsInstituteofTechnology,USAJuliaComputingInc.,USAPumas-AIInc.,USAcrackauc@mit.eduAbstractNosingleAutomaticDifferentiation(AD)systemistheoptimalchoiceforallproblems.ThismeansinformedselectionofanADsystemandcombinationscanbeaproblem-speciﬁcvariablethatcangreatlyimpactperformance.IntheJuliaprogramminglanguage,themajorADsystemstargetthesameinputandthusintheorycancompose.Hitherto,switchingbetweenADpackagesintheJuliaLanguagerequiredend-userstofamiliarizethemselveswiththeuser-facingAPIoftherespectivepackages.Furthermore,implementinganew,usableADpackagerequiredADpackagedeveloperstowriteboilerplatecodetodeﬁneconvenienceAPIfunctionsforend-users.Asaresponsetotheseissues,wepresentAbstractDif-ferentiation.jlfortheautomatizedgenerationofanextensive,uniﬁed,user-facingAPIforanyADpackage.BysplittingthecomplexitybetweenADusersandADdevelopers,ADpackagedevelopersonlyneedtoimplementoneortwoprimitivedeﬁnitionstosupportvariousutilitiesforADuserslikeJacobians,Hessiansandlazyproductoperatorsfromnativeprimitivessuchaspullbacksorpushforwards,thusremovingtedious–butsofarinevitable–boilerplatecode,andenablingtheeasyswitchingandcomposingbetweenADimplementationsforend-users.1IntroductionDifferentiableprogramming(∂P),i.e.,theabilitytodifferentiategeneralcomputerprogramstructures,hasenabledtheefﬁcientcombinationofexistingpackagesforscientiﬁccomputationandmachinelearning[Raissietal.,2019,Rackauckasetal.,2020a,deAvilaBelbute-Peresetal.,2018].Black-boxmachinelearningapproachesareﬂexiblebutrequirealargeamountofdata.Incorporatingscientiﬁcknowledgeaboutthestructureofaproblemvia∂Preducestheamountofdataneeded.Itallowsthelearningtasktobesimpliﬁed,forexample,byfocusingonlearningonlythepartsofthemodelthataremissing[Rackauckasetal.,2020b,Dandekaretal.,2020].Therearealreadymanyexampleswheresuchdifferentiableframeworkshaveprovidedperformanceandaccuracyadvantagesoverblack-boxapproachestomachinelearning,includingbutnotlimitedtoprotein-folding[AlQuraishi,2018,Ingrahametal.,2018],ﬂuiddynamics[SchenckandFox,2018],robotics[SchenckandFox,2018],andquantumcontrol[Schäferetal.,2020,2021].∂Pis(commonly)realizedbyautomaticdifferentiation(AD),afamilyoftechniquestoefﬁcientlyandaccuratelydifferentiatenumericfunctionsexpressedascomputerprograms.Generally,besidesPreprint.Underreview.arXiv:2109.12449v2  [cs.MS]  4 Feb 2022forward-andreverse-modeAD,thetwomainADbranches,manysoftwareimplementationswithdifferentprosandconsexist.SomeADsoftwareimplementationsworkatalowerlevelcoderepre-sentation,possiblymixinginLLVM-levelcompilerpasses,tofullyoptimizescalaroperations[Revelsetal.,2016,MosesandChuravy,2020]whileothersperformtransformationsatahigherleveltokeeplinearalgebraoperationsintactforoptimalusageofBLASprimitives[Innesetal.,2019,Paszkeetal.,2017].ThegoalistomakethebestchoiceofADsystemineverypartoftheprogramwithoutrequiringuserstoextensivelycontorttheircodetothedifferingAPIs.TheADlandscapeoftheJuliaprogramminglanguageisdevelopedinamannerinwhichcompos-abilitybetweentheADsystemsispossible.Whilemanyautomaticdifferentiationsystemsrequirespeciﬁcformulationsofthecode,forexamplePyTorchusinganalternativeimplementationoftheNumPyAPIknownastorch.numpy[Paszkeetal.,2017]withtorch.tensorandsimilarlyforJaxwithjax.numpy[Bradburyetal.,2018]eachdifferingfromtheoriginalNumPy[Oliphant,2006]APIinsubtlewayswithdifferentnumericalproperties,theJuliaADsystemsgenerallyactdirectlyonthestandardJuliasyntax,withitsstandardlibrary,arrayimplementation,itsstandardGPUaccelerationtools[Besardetal.,2018],andmore.ThishaspreviouslybeenshowntoallowpackagesinJuliawhichweredevelopedwithoutknowledgeofADsystemstobefullydifferentiablewithoutmodiﬁcationbymultipledifferenttools[Rackauckasetal.,2020a].Furthermore,Juliahasacommongroundonwhichdifferentiationrulesaredeﬁned,ChainRules.jl[Whiteetal.,2021],whichissharedamongsttheADpackages.Thisempowerstheideaofa“glueAD”system[Rackauckas]wheresoftwarelibraryauthorsdeﬁneChainRulesoverloadstoadddomaininsightintotheautomaticdifferentiationprocesswithouttyingtooneparticularADsystem.However,switchingfromonebackendtoanotherontheusersidecanstillbetediousbecausetheuserhastolearnandadaptthecodetowardstheuser-facingAPIofthenewADpackage.Similarly,fortheauthoroftheADpackagedeﬁninganextensiveAPIsupportingeverypossibledifferentiationusecaserequiresalotofboilerplatecode,e.g.todeﬁnetheJacobianfunction,Jacobian-vectorproduct,Hessian,Hessian-vectorproduct,etc.DeﬁningallofthesefunctionsforeachADimplementationistediousandunnecessarysincetherelationshipbetweenthesefunctionsisabstractandnotimplementation-speciﬁc.Therefore,whileintheoryswitchingbetweenADsystemscanbetriviallydone,inpracticethecompetingAPIsofthevariousADmechanismshaslimiteditsusethroughoutthelanguage’secosystem.TheJuliaLanguage[Bezansonetal.,2012]hasoveradozenautomaticdifferentiationpack-ages[White].Differentpackageshavedifferentuserinterfacesandofferdifferenttradeoffs.Popularsystemsinclude:1.ForwardDiff.jl[Revelsetal.,2016],anoperator-overloading-based,forward-modeADimplementation,withmanyyearsofextensiveuseandthusveryhighreliability2.ReverseDiff.jl[Revels,2018],anoperator-overloading-based,reverse-modeADimplemen-tation,featuringseveraltape-basedoptimizations3.Zygote.jl[Innesetal.,2019],areverse-modeADimplementationthatdoessourcecodetransformationtogeneratethederivative’scodefromthefunction’scode,operatingatthelevelofJulia’sintermediaterepresentation.ZygoteisthereforeabletohandlearbitraryJuliacodebutisunabletohandlemutation.4.Enzyme.jl[MosesandChuravy,2020],areverse-modeADimplementationthatrunsbysourcecodetransformationattheLLVMlevel,withexcellentperformanceonscalaroperations,butatpresentlesserperformanceonlargematrixoperations.5.Diffractor.jl[Fischer],anewsource-to-sourceADpackagepromisinghighperformanceonbothscalarandvector/tensorcodeAmoredetailedsummaryofthestrengthsandlimitationsofdifferentADpackagesisgiveninAppendixA.EachoftheseADsystems(andeachofthemanyothers)hasitsownuniquesetofadvantagesanddisadvantages.Additionally,allofthemonlydeﬁneAPIfunctionsforasubsetofallthepossibledifferentiationusecases,oftenrequiringuserstodopackage-speciﬁcimplementationsofquantitieslikeJacobian-vectorproductorHessian-vectorproductwhenneeded.BesidetheexistingstableADimplementations,anynewimplementationmayormaynotbematureenoughtohandleperturbationconfusionproperly[SiskindandPearlmutter,2005,Manzyuketal.,2019]whichpreventsonefrom2doinggeneral,higher-orderADcorrectly.AsimpleworkaroundistocomposevariousADpackagesforeachlevelofdifferentiation,furthergivingrisetoapplicationswherechangingbetweenADmechanismsisincreasinglycommon.AsADsystemshavedifferentprosandcons,asoftwareauthorwillwanttochangeADsystemsdependingontheproblemandavailablehardwareresources,seeAppendixB.However,thisismorechallengingthanitmightseem.ChangingADsystemsresultsinforkingthecode,eventhoughthenominalvalueofthesoftwareusingtheADremainsthesame.Togivesomeexamples:Flux.jl1changedfromusingTracker.jl2toZygote.jl[Innesetal.,2019].Thisresultedinaforkbeingcreated,viz.TrackerFlux.jl3,forthosewhowanttousetheoldADsystem–eventhoughconceptuallyFluxisaNeuralNetworklibrarythatshouldbeabstractedawayfromtheAD.PyMC4wascreatedasanattempttomovefromTheano[Al-Rfouetal.,2016],asusedinPyMC3[Salvatieretal.,2016],tousingTensorFlow[Abadietal.,2015].Thisattemptwaseventuallyabandoned,infavorofkeepingTheanobutaddingaJax[Bradburyetal.,2018]backend[ThePyMCDevelopmentTeam].Notonlydidthecodeneedtobeforked,buttheoverallattemptwasnotsuccessful.Admittedly,thiswasaparticularlycomplexcasebeyondjustAD,withTensorFlowandTheanobeingmoregeneralcomputationalframeworkswithADasjustonefeature.TheworkwepresenthereaimstoensurethatchangingtheADsystemisaccessiblebyprovidingconsistentabstractionsthattheauthorofthe∂Palgorithmimplementationcanuse.AsimilarbutmorecomplexproblemwassolvedbytheMathOptInterface.jl[Legatetal.,2020].MathOptInterface.jlprovidescommonabstractionsacrossconstrainedmathematicaloptimizerssuchasIPOPT[Wachter,2002],Cbc[Forrestetal.,2018],andGurobi[GurobiOptimization,LLC,2021].ItinturnisusedbymathematicaloptimizationframeworksincludingJuMP[Dunningetal.,2017]andConvex.jl[Udelletal.,2014].Eachofthedifferentmathematicaloptimizershastheirownveryuniqueinternalsetofabstractions,butMathOptInterface.jlexposesthemallinthesameway.Anadditionalcomplicationisthateachsupportsdifferentkindsofproblemsandsothistoomustbeexposed.Furtherstill,forsomeclassesofproblemstheycanbere-expressedasadifferentkindthroughamathematicaltransformation,MathOptInterfaceexposesthisthroughanextensiblesystemofso-called"bridges",thatwillautomaticallyperformthesereformulations.ThissystemisconsiderablymorecomplicatedthanoursettingaseveryADsystemcanperformalltheoperations,tovaryingdegreesofefﬁciency.TheMathOptInterfacesystemhasprovenverysuccessful,whichsupportstheideathatthiskindofabstractionisvaluableandcanbepracticallyrealized.Inlightoftheabove,theauthorsbelieveitisnecessarytohaveabackend-agnosticinterfacetoprovideobjectslikethefunctionvalue,itsgradient,Hessian,etc.aswellascombiningADimplementationstogetherforhigher-orderAD.SuchaninterfacecanhelpusavoidacombinatorialexplosionofcodewhensupportingeverydifferentiationpackageinJuliaineverypieceofsoftwarerequiringgradientsand/orHessians.Thisisespeciallyimportantforhigher-orderderivativesbecauseonecancombineanytwodifferentiationbackendstocreateanewhigher-orderbackend.Moregenerallyforakthorderderivative,theamountofcoderequiredtosupportndifferentiationpackagesinm∂PalgorithmimplementationsisO(m×nk).Inthispaper,wepresentAbstractDifferentiation.jl[Tareketal.],apackagethat:•Deﬁnesanabstract,extensiveAPIfordifferentiationinJuliaenablingthedevelopmentofalgorithmsrequiringﬁrstandhigher-orderderivativesinanAD-implementation-agnosticwayusingasingle,uniﬁedinterfacereducingthecodecomplexityfromO(m×nk)toO(m+n).•Automaticallydeﬁnesmostoftheextensiveuser-facingAPIforanynewADpackagefromjustoneortwoprimitiveAPIfunctiondeﬁnitions,thusmakingiteasierfortheADpackagedevelopertosupporteverypossibleusecasewithoutagreatdealofboilerplatecode.2LevelsofabstractioninJulia’sADecosystemInFigure1,anoverviewofthelevelsofabstractioninJulia’sADecosystemwithAbstractDifferenti-ation.jlispresented.Atthebottomlevel,wehavelibrariesofdifferentiationrules(DiffRules.jland1https://github.com/FluxML/Flux.jl2https://github.com/FluxML/Tracker.jl3https://github.com/AStupidBear/TrackerFlux.jl3ChainRules.jl)forspeciﬁcfunctions.TheserulesareeitherdeﬁnedbyADdevelopersforbasicJuliaconstructs,orbyADusersforspeciﬁcuser-deﬁnedfunctionswithknownanalyticderivatives.SittingontopofthelibraryofrulesarealltheADpackageimplementations.Atthislevel,numerousdesigndecisionsandoptimizationscanbemadegivingavarietyofdifferentADpackageimple-mentationswithdifferenttradeoffs.EachADpackagedeveloperwillthendeﬁneaminimalsetofprimitivesandabackendtypeextendingAbstractDifferentiation.jl.TheseminimaldeﬁnitionsthenenableAbstractDifferentiation.jltoautomaticallydeﬁneanextensivesetofuser-facingAPIfunctionsforADuserstouse,e.g.derivative,Jacobian,Hessian,Jacobian-vectorproduct,Hessian-vectorproduct,etc.Atthetoplevel,ADuserscanthenusetherelevantpartoftheAbstractDifferentiation.jlAPItoimplementalgorithmsrequiring∂P.Withthisabstractiondesign,theamountofcodeneededtosupportallofnADpackagesinmalgorithmsrequiringkthorderderivativesisonlyO(m+n),asigniﬁcantreductionfromtheO(m×nk)withoutAbstractDifferentiation.jl.Additionally,theADusersanddevelopersdonotneedtoaddunnecessaryboilerplatecodetoextendanADpackage’sAPIanymore,sinceAbstractDifferentiation.jlautomaticallydoesthisforthem.Figure1:ThelevelsofabstractioninJulia’sADecosystem.3APIdescriptionInstallationandloadingAbstractDifferentiation.jlisaregisteredJuliapackageandcanbeinstalledbytheJuliapackagemanager.Thepackagecanbeloadedby#alternatively:importAbstractDifferentiationasADusingAbstractDifferentiationNotethatAbstractDifferentiation.jlexports“AD"asanaliasfortheAbstractDifferentiationmodule.ThisaliasallowsustoconvenientlyaccessnameswithinAbstractDifferentiation.jlviaADinsteadoftypingthefullpackagename.43.1BackendsandprimitivesForward-mode,reverse-mode,andﬁnite-differencebackendsAllfunctionalitiesinAbstractDif-ferentiation.jlareimplementedbasedonanab::AbstractBackendtype.AnADpackagedeveloper(ortheADuserifnecessary)ﬁrstconstructsabackendinstancethatsubtypesab::AbstractForwardMode,ab::AbstractReverseMode,orab::AbstractFiniteDifference,whicharethemselvessubtypesofab::AbstractBackend.Forexample,backendsthatsupportForwardDiff.jlorZygote.jlaredeﬁnedasfollows:##ForwardDiffstructForwardDiffBackend<:AD.AbstractForwardModeendconstforwarddiff_backend=ForwardDiffBackend()##ZygotestructZygoteBackend<:AD.AbstractReverseModeendconstzygote_backend=ZygoteBackend()Byaddingﬁeldstothebackendstruct,wecancontrolconﬁgurationsofthedifferentiationpackagesuchaschunksizes,compilationﬂags,ormethodchoices.Touseaﬁnitedifferencingmethodatacentralgridof5pointsasimplementedintheFiniteDifferences.jlpackage,wewrite:##FiniteDifferencesstructFDMBackend{A}<:AD.AbstractFiniteDifferencealg::Aend#1denotestheorderofthederivativetoestimate.FDMBackend()=FDMBackend(central_fdm(5,1))Higher-orderbackendsTocomputehigher-orderderivatives,itmaybedesirabletocombinedif-ferentbackends.WeprovideAD.HigherOrderBackendtoimplementhigher-orderbackends.Letab_fbeaforward-modeautomaticdifferentiationbackendandletab_rbeareverse-modeautomaticdifferentiationbackend.Toconstructahigher-orderbackendfordoingforward-over-reverse-modeautomaticdifferentiation,onedeﬁnesAD.HigherOrderBackend((ab_f,ab_r)).Analogously,higher-orderbackendfordoingreverse-over-forward-modeautomaticdifferentiationisconstructedviaAD.HigherOrderBackend((ab_r,ab_f)).Jacobian,pushforward,andpullbackasprimitiveoperationInadditiontothedeﬁnitionofabackend,theADpackagedeveloperneedstodeﬁneoneofthefollowingprimitiveoperations:AD.@primitivefunctionjacobian(ab::backend,f,xs...)return..endAD.@primitivefunctionpushforward_function(ab::backend,f,xs...)return..endAD.@primitivefunctionpullback_function(ab::backend,f,xs...)return..endAbstractDifferentiation.jlthengeneratestheothertwoprimitivefunctions.Forinstance,asource-to-sourcereverse-modeADpackagedevelopercanspecifyonlyAD.pullback_functionasthenativeprimitiveoperation.##Zygoteissource-to-sourcereverse-modeAD.@primitivefunctionpullback_function(ab::ZygoteBackend,f,xs...)returnfunction(vs)#Supportsonlysingleoutput_,back=Zygote.pullback(f,xs...)ifvsisaAbstractVectorreturnback(vs)else5#vsisaTuple@assertlength(vs)==1returnback(vs[1])endendendInthecaseofoperatoroverloadingADimplementations,werequireadditionallythedeﬁnitionofAD.primal_valuereturningtheprimalvalueoftheforwardpass.3.2AutomaticallyprovidedfunctionsAfterthesepreparatorysteps,AbstractDifferentiation.jlautomaticallydeﬁnesvariousfunctionsforADusersmakinguseoftheprimitivesdeﬁned.SomeofthemostimportantAPIfunctionsprovidedarepresentedinthefollowing.Wereferthereadertothepackagedocumentationforfurtherdetails[Tareketal.].Derivative,gradient,jacobian,hessiands=AD.derivative(ab::AD.AbstractBackend,f,xs::Number...)gs=AD.gradient(ab::AD.AbstractBackend,f,xs...)js=AD.jacobian(ab::AD.AbstractBackend,f,xs...)h=AD.hessian(ab::AD.AbstractBackend,f,x)Valueandderivative,gradient,jacobian,hessianv,ds=AD.value_and_derivative(ab::AD.AbstractBackend,f,xs::Number...)v,gs=AD.value_and_gradient(ab::AD.AbstractBackend,f,xs...)v,js=AD.value_and_jacobian(ab::AD.AbstractBackend,f,xs...)v,h=AD.value_and_hessian(ab::AD.AbstractBackend,f,x)v,g,h=AD.value_gradient_and_hessian(ab::AD.AbstractBackend,f,x)LazyoperatorsFinally,weimplementedlazyversionsofthederivative,gradient,Jacobian,andHessian,ld=lazy_derivative(ab::AbstractBackend,f,xs::Number...)lg=lazy_gradient(ab::AbstractBackend,f,xs...)lj=lazy_jacobian(ab::AbstractBackend,f,xs...)lh=lazy_hessian(ab::AbstractBackend,f,x)whichareofinterestiniterativesolvers.Forexample,wecomputeavector-Jacobianproductbymultiplyingasingle(transposed)vector,oratupleofanappropriateshape,fromthelefttothelazyJacobianoperator.4∂PusecasesandanexampleManynumericalalgorithmsrequirethecomputationofconstructssuchastheonesdescribedinSection3.2.Table1presentsaroughsummarylinkingsomeofthemostwidelyadoptedroutinesacrossdifferentdomainstothequantitiesusedintherespectiveiterativeupdatesteps.Asanexample,wepresenta(simple,non-optimized)backend-agnosticimplementationoftheGauss-Newtonalgorithmtosolvenon-linearleastsquaresproblemsinAppendixC.WealsoexpectAbstractDifferentiation.jltobespeciﬁcallyhandyfor(future)ADpackagelikeDiffractor.jlorEnzyme.jlwherecomputingconstructslikeJacobiansorHessiansistechnicallypossiblebutnotyetpartofthepublicAPIduetoabstractionsornamingconventionsmadeinthepackage.6algorithmsrequiredquantitiesrootﬁndingNewton–RaphsonJacobianJacobian-FreeNewtonKrylovJacobian-vectorproductsoptimizationADAMgradientNewtongradient,HessianLevenberg–MarquardtJacobianGauss-NewtonJacobiandifferentialequationsstiffODEsolversJacobianstiffODEJacobian-freesolversJacobian-vectorproductsforwardsensitivitymethodsJacobian-vectorproductsadjointsensitivitymethodsvector-JacobianproductTable1:Domain-speciﬁcalgorithmsrequiringderivatives,gradients,Jacobians,Hessians,vector-Jacobianproducts,Jacobian-vectorproductscommonlycomputedbyADpackages.5Summary&FutureworkTheabilitytostraightforwardlycombinedifferentpackagesinoneworkﬂowisoneofthemostversatileandkeyfeaturesofJulia.SwitchingbetweendifferentADpackagesandcombiningthemforhigher-orderderivativesisausefulfeaturetohavewhenselectingthebestADimplementationforaspeciﬁcapplication.WehavepresentedtheAbstractDifferentiation.jlpackagewhichmakesthisswitchingandcombiningofADimplementationsaspainlessaspossibleforend-userswhilealsoreducingtheamountofnecessaryboilerplatecodeperADpackagetosupportalldifferentiationusecases.Inthefuture,weaimtosupportAbstractDifferentiation.jlinalloftheADpackagesinJuliaandremovelotsofboilerplatecodefrompopularJuliapackages(e.g.intheSciMLandTuringLangorganizations)thatheavilyemployAD.6AcknowledgmentsThismaterialisbaseduponworksupportedbytheNationalScienceFoundationundergrantno.OAC-1835443,grantno.SII-2029670,grantno.ECCS-2029670,grantno.OAC-2103804,andgrantno.PHY-2021825.WealsogratefullyacknowledgetheU.S.AgencyforInternationalDevelopmentthroughPennStateforgrantno.S002283-USAID.Theinformation,data,orworkpresentedhereinwasfundedinpartbytheAdvancedResearchProjectsAgency-Energy(ARPA-E),U.S.DepartmentofEnergy,underAwardNumberDE-AR0001211andDE-AR0001222.WealsogratefullyacknowledgetheU.S.AgencyforInternationalDevelopmentthroughPennStateforgrantno.S002283-USAID.TheviewsandopinionsofauthorsexpressedhereindonotnecessarilystateorreﬂectthoseoftheUnitedStatesGovernmentoranyagencythereof.ThismaterialwassupportedbyTheResearchCouncilofNorwayandEquinorASAthroughResearchCouncilproject"308817-Digitalwellsforoptimalproductionanddrainage".ResearchwassponsoredbytheUnitedStatesAirForceResearchLaboratoryandtheUnitedStatesAirForceArtiﬁcialIntelligenceAcceleratorandwasaccomplishedunderCooperativeAgreementNumberFA8750-19-2-1000.Theviewsandconclusionscontainedinthisdocumentarethoseoftheauthorsandshouldnotbeinterpretedasrepresentingtheofﬁcialpolicies,eitherexpressedorimplied,oftheUnitedStatesAirForceortheU.S.Government.TheU.S.GovernmentisauthorizedtoreproduceanddistributereprintsforGovernmentpurposesnotwithstandinganycopyrightnotationherein.ReferencesM.Abadi,A.Agarwal,P.Barham,E.Brevdo,Z.Chen,C.Citro,G.S.Corrado,A.Davis,J.Dean,M.Devin,S.Ghemawat,I.Goodfellow,A.Harp,G.Irving,M.Isard,Y.Jia,R.Jozefowicz,7L.Kaiser,M.Kudlur,J.Levenberg,D.Mané,R.Monga,S.Moore,D.Murray,C.Olah,M.Schuster,J.Shlens,B.Steiner,I.Sutskever,K.Talwar,P.Tucker,V.Vanhoucke,V.Vasudevan,F.Viégas,O.Vinyals,P.Warden,M.Wattenberg,M.Wicke,Y.Yu,andX.Zheng.TensorFlow:Large-scalemachinelearningonheterogeneoussystems,2015.URLhttps://www.tensorflow.org/.Softwareavailablefromtensorﬂow.org.R.Al-Rfou,G.Alain,A.Almahairi,C.Angermueller,D.Bahdanau,N.Ballas,F.Bastien,J.Bayer,A.Belikov,A.Belopolsky,Y.Bengio,A.Bergeron,J.Bergstra,V.Bisson,J.BleecherSnyder,N.Bouchard,N.Boulanger-Lewandowski,X.Bouthillier,A.deBrébisson,O.Breuleux,P.-L.Carrier,K.Cho,J.Chorowski,P.Christiano,T.Cooijmans,M.-A.Côté,M.Côté,A.Courville,Y.N.Dauphin,O.Delalleau,J.Demouth,G.Desjardins,S.Dieleman,L.Dinh,M.Ducoffe,V.Dumoulin,S.EbrahimiKahou,D.Erhan,Z.Fan,O.Firat,M.Germain,X.Glorot,I.Goodfellow,M.Graham,C.Gulcehre,P.Hamel,I.Harlouchet,J.-P.Heng,B.Hidasi,S.Honari,A.Jain,S.Jean,K.Jia,M.Korobov,V.Kulkarni,A.Lamb,P.Lamblin,E.Larsen,C.Laurent,S.Lee,S.Lefrancois,S.Lemieux,N.Léonard,Z.Lin,J.A.Livezey,C.Lorenz,J.Lowin,Q.Ma,P.-A.Manzagol,O.Mastropietro,R.T.McGibbon,R.Memisevic,B.vanMerriënboer,V.Michalski,M.Mirza,A.Orlandi,C.Pal,R.Pascanu,M.Pezeshki,C.Raffel,D.Renshaw,M.Rocklin,A.Romero,M.Roth,P.Sadowski,J.Salvatier,F.Savard,J.Schlüter,J.Schulman,G.Schwartz,I.V.Serban,D.Serdyuk,S.Shabanian,E.Simon,S.Spieckermann,S.R.Subramanyam,J.Sygnowski,J.Tanguay,G.vanTulder,J.Turian,S.Urban,P.Vincent,F.Visin,H.deVries,D.Warde-Farley,D.J.Webb,M.Willson,K.Xu,L.Xue,L.Yao,S.Zhang,andY.Zhang.Theano:APythonframeworkforfastcomputationofmathematicalexpressions.arXive-prints,abs/1605.02688,May2016.URLhttp://arxiv.org/abs/1605.02688.M.AlQuraishi.End-to-enddifferentiablelearningofproteinstructure.bioRxiv,2018.doi:10.1101/265231.URLhttps://www.biorxiv.org/content/early/2018/02/14/265231.T.Besard,C.Foket,andB.DeSutter.Effectiveextensibleprogramming:UnleashingJuliaonGPUs.IEEETransactionsonParallelandDistributedSystems,2018.ISSN1045-9219.doi:10.1109/TPDS.2018.2872064.J.Bezanson,S.Karpinski,V.B.Shah,andA.Edelman.Julia:Afastdynamiclanguagefortechnicalcomputing.arXivpreprintarXiv:1209.5145,2012.J.Bradbury,R.Frostig,P.Hawkins,M.J.Johnson,C.Leary,D.Maclaurin,G.Necula,A.Paszke,J.VanderPlas,S.Wanderman-Milne,andQ.Zhang.JAX:composabletransformationsofPython+NumPyprograms,2018.URLhttp://github.com/google/jax.R.T.Chen,Y.Rubanova,J.Bettencourt,andD.Duvenaud.Neuralordinarydifferentialequations.arXivpreprintarXiv:1806.07366,2018.R.Dandekar,C.Rackauckas,andG.Barbastathis.Amachinelearning-aidedglobaldiagnosticandcomparativetooltoassesseffectofquarantinecontrolincovid-19spread.Patterns,1(9):100145,2020.F.deAvilaBelbute-Peres,K.Smith,K.Allen,J.Tenenbaum,andJ.Z.Kolter.End-to-enddifferen-tiablephysicsforlearningandcontrol.Advancesinneuralinformationprocessingsystems,31:7178–7189,2018.I.Dunning,J.Huchette,andM.Lubin.Jump:Amodelinglanguageformathematicaloptimization.SIAMReview,59(2):295–320,2017.K.Fischer.Non-localcompilertransformationsinthepresenceofdynamicdispatch.URLhttps://www.youtube.com/watch?v=mQnSRfseu0c.J.Forrest,T.Ralphs,S.Vigerske,LouHafer,B.Kristjansson,jpfasano,EdwinStraver,M.Lubin,H.G.Santos,rlougee,andM.Saltzman.coin-or/cbc:Version2.9.9,July2018.URLhttps://doi.org/10.5281/zenodo.1317566.GurobiOptimization,LLC.GurobiOptimizerReferenceManual,2021.URLhttps://www.gurobi.com.8J.Ingraham,A.Riesselman,C.Sander,andD.Marks.Learningproteinstructurewithadifferentiablesimulator.InInternationalConferenceonLearningRepresentations,2018.M.Innes,A.Edelman,K.Fischer,C.Rackauckus,E.Saba,V.B.Shah,andW.Tebbutt.Zygote:Adifferentiableprogrammingsystemtobridgemachinelearningandscientiﬁccomputing.arXivpreprintarXiv:1907.07587,2019.URLhttps://arxiv.org/abs/1907.07587.B.Legat,O.Dowson,J.D.Garcia,andM.Lubin.Mathoptinterface:adatastructureformathematicaloptimizationproblems,2020.URLhttps://arxiv.org/abs/2002.03447.O.Manzyuk,B.A.Pearlmutter,A.A.Radul,D.R.Rush,andJ.M.Siskind.Perturbationconfusioninforwardautomaticdifferentiationofhigher-orderfunctions.JournalofFunctionalProgramming,29,2019.W.MosesandV.Churavy.Insteadofrewritingforeigncodeformachinelearning,automaticallysynthesizefastgradients.InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages12472–12485.CurranAssociates,Inc.,2020.URLhttps://proceedings.neurips.cc/paper/2020/file/9332c513ef44b682e9347822c2e457ac-Paper.pdf.T.E.Oliphant.AguidetoNumPy,volume1.TrelgolPublishingUSA,2006.A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin,A.Desmaison,L.Antiga,andA.Lerer.Automaticdifferentiationinpytorch.2017.C.Rackauckas.GlueADforfulllanguagedifferentiablepro-gramming.URLhttp://www.stochasticlifestyle.com/glue-ad-for-full-language-differentiable-programming/.C.Rackauckas,Y.Ma,V.Dixit,X.Guo,M.Innes,J.Revels,J.Nyberg,andV.Ivaturi.Acomparisonofautomaticdifferentiationandcontinuoussensitivityanalysisforderivativesofdifferentialequationsolutions.arXivpreprintarXiv:1812.01892,2018.C.Rackauckas,A.Edelman,K.Fischer,M.Innes,E.Saba,V.B.Shah,andW.Tebbutt.Generalizedphysics-informedlearningthroughlanguage-widedifferentiableprogramming.InAAAISpringSymposium:MLPS,2020a.C.Rackauckas,Y.Ma,J.Martensen,C.Warner,K.Zubov,R.Supekar,D.Skinner,andA.Ramadhan.Universaldifferentialequationsforscientiﬁcmachinelearning.arXivpreprintarXiv:2001.04385,2020b.M.Raissi,P.Perdikaris,andG.E.Karniadakis.Physics-informedneuralnetworks:Adeeplearningframeworkforsolvingforwardandinverseproblemsinvolvingnonlinearpartialdifferentialequations.JournalofComputationalPhysics,378:686–707,2019.J.Revels.ReverseDiff.jl,2018.URLhttp://github.com/JuliaDiff/ReverseDiff.jl.J.Revels,M.Lubin,andT.Papamarkou.Forward-modeautomaticdifferentiationinjulia.arXivpreprintarXiv:1607.07892,2016.J.Salvatier,T.V.Wiecki,andC.Fonnesbeck.ProbabilisticprogramminginpythonusingPyMC3.PeerJComputerScience,2:e55,apr2016.doi:10.7717/peerj-cs.55.URLhttps://doi.org/10.7717/peerj-cs.55.F.Schäfer.Abstractdifferentiation.jlforAD-backendagnosticcode.URLhttps://frankschae.github.io/post/abstract_differentiation/.F.Schäfer,M.Kloc,C.Bruder,andN.Lörch.Adifferentiableprogrammingmethodforquantumcontrol.MachineLearning:ScienceandTechnology,1(3):035009,2020.F.Schäfer,P.Sekatski,M.Koppenhöfer,C.Bruder,andM.Kloc.Controlofstochasticquantumdynamicsbydifferentiableprogramming.MachineLearning:ScienceandTechnology,2(3):035004,2021.9C.SchenckandD.Fox.Spnets:Differentiableﬂuiddynamicsfordeepneuralnetworks.InConferenceonRobotLearning,pages317–335.PMLR,2018.J.M.SiskindandB.A.Pearlmutter.Perturbationconfusionandreferentialtransparency:Correctfunctionalimplementationofforward-modeAD.2005.F.Srajer,Z.Kukelova,andA.Fitzgibbon.Abenchmarkofselectedalgorithmicdifferentiationtoolsonsomeproblemsincomputervisionandmachinelearning.OptimizationMethodsandSoftware,33(4-6):889–906,2018.M.Tarek,F.Schäfer,andcontributers.AbstractDifferentiation.jl.URLhttps://github.com/JuliaDiff/AbstractDifferentiation.jl.ThePyMCDevelopmentTeam.Thefutureofpymc3,or:Theanoisdead,longlivetheano.URLhttps://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b.M.Udell,K.Mohan,D.Zeng,J.Hong,S.Diamond,andS.Boyd.ConvexoptimizationinJulia.SC14WorkshoponHighPerformanceTechnicalComputinginDynamicLanguages,2014.A.Wachter.Aninteriorpointalgorithmforlarge-scalenonlinearoptimizationwithapplicationsinprocessengineering.PhDthesis,CarnegieMellonUniversity,2002.L.White.Juliadiffwebsite.URLhttps://juliadiff.org/.L.White,M.Zgubic,M.Abbott,J.Revels,A.Arslan,S.Axen,S.Schaub,N.Robinson,Y.Ma,G.Dhingra,W.Tebbutt,N.Heim,A.D.W.Rosemberg,N.Schmitz,C.Rackauckas,D.Widmann,R.Heintzmann,F.Schäfer,K.Fischer,A.Robson,M.Brzezinski,A.Zhabinski,M.Besançon,P.Vertechi,S.Gowda,A.Fitzgibbon,C.Lucibello,C.Vogt,D.Gandhi,andF.Chorney.Juliad-iff/chainrules.jl:v1.11.5,Sept.2021.URLhttps://doi.org/10.5281/zenodo.5467874.10ASummaryofthecurrentstateofADpackagesinJuliaasofSeptember2021Table2:ThistablesummarizesthecurrentstateofpopularJuliaADpackagesinSeptember2021.“Scalar"referstoscalaroperationssupportincludingdeﬁningcustomrulesforscalar-valuedfunc-tionsofscalars."Vector/tensor"referstonativevector/tensorsupportasaconstructincludingtheabilitytodeﬁnecustomdifferentiationrulesforvector/tensor-valuedfunctionsand/orfunctionsofvectors/tensors.Similarly,“Firstclassstructsupport"referstothenativesupportofJuliastructsasaconstructincludingtheabilitytodeﬁnecustomdifferentiationrulesforstruct-valuedfunctionsorfunctionsofstructs.“GPU"referstotheabilitytodifferentiatefunctionsoforreturningGPUarrays."GC"referstosupportingfunctionsthatcalltheJuliagarbagecollector.“Mutation"referstotheabilitytosupportmutatingarraysandstructs.“Runtimebranches"referstotheabilitytosupport“piece-wise"functionswithcontrolﬂowsuchthatthepaththatthefunctiontakesandultimatelythestructureofthemathematicalfunctiondifferentiateddependsonthevaluesoftheinputstothefunction.“Maturity"referstoasubjectivemeasureofhowmatureeachpackageisintheeyesofthecommunityaswellasthefeaturematurityofthepackage.PackageScalarVector/tensorFirstclassstructsupportGPUGCMutationRuntimebranchesMaturityForwardDiff3773333veryhighReverseDiffslow3773limited3highReverseDiffwithcompiledtape33773limited7highTrackerslow3733limited3highZygoteslow333373highEnzyme3limitedwipwipwip33lowDiffractor3333373lowTable2summarizesthecurrentstateofthemostpopularADpackagesintheJuliaecosystemasofthetimeofthewritingofthispaper.11BADperformancecanbeproblem-speciﬁcItiswellknowthatforafunctionf:Rn→Rmwithnindependentinputvariablesandmdependentoutputvariables,forward-modeADispreferredtobuildtheJacobianwhenm(cid:29)nwhilereverse-modeADispreferredwhenn(cid:29)m,i.e.asoneincreasesthenumberofinputswithinthesameproblem,reverse-modeADmodewilleventuallyovertakeforward-modeADinperformance.Thishasbeeninvestigatedindepthfordifferentialequationswhenappliedtomodelsrelevanttobiopharmacology,alongsidevariousadjointoptions[Rackauckasetal.,2018].ThisworkshowsthatonsufﬁcientlysmallODEs(<100ODEs+parameters),forward-modeADviaForwardDiff.jlisthemostefﬁcientmethodcomparingagainstanalyticaltechniquesandadjointtechniquesusingTracker.jl,Enzyme.jl,andReverseDiff.jl.WhenthesizeoftheODEs+parametersisincreasedinastiffpartialdifferentialequation,itwasshownthatEnzyme.jlvector-Jacobianproductsmixedwithaspeciﬁcadjointmethodwasthemostefﬁcient,outperformingtheForwardDiff.jltechniqueslongwithReverseDiff.jlandTracker.jl.Inwhatfollows,wedemonstrateontwoadditionalexamplesthatthechoiceofthespeciﬁcreverse-modeADpackagemayalsosigniﬁcantlyimpacttheperformance[Srajeretal.,2018].TheseexamplesshowReverseDiff.jlincompiledmodeoutperformingEnzyme.jlundercertaincircumstances.How-ever,asReverseDiff.jlisnotcompatiblewithGPUsandwasshowntonotbeperformancecompetitiveonotherpotentialequationsinscientiﬁccomputingapplications,whichallowsZygote.jlandTracker.jltobethemostefﬁcient.Togetherthisshowsinoneapplicationthat5ADsystemscouldpotentiallybetheoptimalchoicedependingonuserinputsintothepackagecode.ThisestablishesthattheoptimalchoiceofADmechanismcanberathercomplexforusersandpackagedevelopers,andthusdecreasingthecostofperformingsuchbenchmarksisofvaluetomanyscientists.Example1:Lotka–VolterramodelFigure2:Benchmark1:Lotka–Volterramodel.Inallcases,weuseacheckpointedinterpolatingadjointmethod[Rackauckasetal.,2020b]tocomputethelocalsensitivities.‘false’and‘true’indicateifthetapeinReverseDiff.jlisprecompiled.Supposethatwehaveaninstantaneousobjectivel(x(t),y(t))=x(t)+y(t)(1)12fortheLotka–Volterramodel˙x=αx−βxy,(2)˙y=γxy−δy,(3)withinitialconditionsx(t=0)=1andy(t=0)=1.Letξdenoteanyoftheparametersα,β,γ,δ.Weareinterestedinthesensitivities∂∂ξPil(x(ti),y(ti))withrespecttoanequallyspacedtimegridbetween0and10withagridspacingof0.1.Figure2showsaviolinplotfortheruntimesforfourchoicesoftheinternallyusedADsystem.Thisdemonstratesthatthevector-JacobianproductswhichusestaticcompilationoftheODEfunction,ReverseDiff.jlwithcompilationenabledandEnzyme.jl,vastlyoutperformtheotherchoicesforsmallODEswithalotofscalarindexing,whichisacommonfeatureinmanynonlinearphysicalandbiochemicalmodels.NotethatalladjointtechniqueswereshowntobeoutperformedbyForwardDiff.jlonthisexampleelsewhere[Rackauckasetal.,2018],butthisexamplestillconﬁrmsthatinmanyscalarindexingcasestheZygote.jlsystemcanperformratherpoorly.Example2:NeuralODEThisexampleistheSpiralNeuralODEchosenfromtheNeuralOrdinaryDifferentialEquationsmanuscript[Chenetal.,2018].ItisanODEdeﬁnedasaneuralnetworkappliedtothecubedstatesofthesystem:˙u=NN(u3)(4)whereNN(u)isamultilayerperceptronwithonehiddenlayerofsize50andatanhactivationfunction,andu∈R2.Figure3showsaviolinplotfortheruntimesforfourchoicesoftheinternallyusedADsystem.TheresultsshowthatfordirectdifferentiationonCPUs,ReverseDiffVJPwithacompiledtapeisthemostefﬁcientmethod.However,thishasmanycaveats.OnecaveatisthatReverseDiff.jl’stape-compiledformisonlyapplicableifthecodehasnobranching,andthuswouldbeincompatiblewithactivationfunctionslikerelu.Additionally,bytestingovervarioussizesofhiddenlayers,weestablishedthataRTX2080SuperGPUoutperformedaRyzen95950xCPUwhenthehiddenlayersizereachedapproximately7,500(notethecrossoverpointcouldpotentiallybealotsmallerinmanyscenariosiftheneuralnetworkisdeepersincetheﬁrstandlastlayersizesare2,matchingthedimensionalityofu).Ataroundthissizeofneuralnetworks,theZygote.jlandTracker.jlstrategiesonGPUsbecomemoreefﬁcientthantheoneofReverseDiff.jlwhichisrestrictedtoCPUs.Thesetwoexamples,inadditiontothepriorresearch,clearlydemonstratethattheinternalADsystemmustbecarefullychosenbasedontheproblem(andhardwareresources)athand.13Figure3:Benchmark2:SpiralNeuralODEmodel.Inallcases,weuseacheckpointedinterpolatingadjointmethod[Rackauckasetal.,2020b]tocomputethelocalsensitivities.‘false’and‘true’indicateifthetapeinReverseDiff.jlisprecompiled.14CImplementationoftheGauss-NewtonalgorithmInthisappendix,weuseAbstractDifferentiation.jlfortheimplementationoftheGauss–Newtonalgorithmforsolvingnonlinearleastsquaresproblems[Schäfer].TheGauss–NewtonalgorithmiterativelyﬁndsthevalueoftheNvariablesx=(x1,...,xN)minimizingthesumofsquaresofMresiduals(f1,...,fM)S(x)=12MXi=1fi(x;p)2.(5)Startingfromaninitialguessx0fortheminimum,themethodrunsthroughtheiterationsxk+1=xk−αk(cid:0)JTJ(cid:1)−1JTf(xk;p),(6)wheretheresidualsf(xk;p)dependonthecurrentstepxkandparametersp.JistheJacobianmatrixatxk,andαkisthesteplengthdeterminedviaalinesearchsubroutine.##Gauss-NewtonschemefunctionGaussNewton!(xs,x,pbackend;maxiter=100)fori=1:maxiterx=step(x,p,backend)push!(xs,x)endreturnxs,xendfunctionstep(x,p,backend,a=1//1)x2=deepcopy(x)while!done(x,x2,p)#line-searchcondition#firstreturnvalueofAD.jacobianisdfdx#secondreturnvalueofAD.jacobianisdfdpJ=AD.jacobian(backend,f,x,p)[1]d=-inv(J'*J)*J'*f(x,p)copyto!(x2,x+a*d)a=a//2endreturnx2endSwitchingbetweendifferentADsystemsistheneasilyaccomplishedbypassingdifferentbackendsasinputtotheGaussNewtonfunction.15AbstractDifferentiation.jl:Backend-AgnosticDifferentiableProgramminginJuliaFrankSchäfer,MohamedTarek,LyndonWhite,ChristopherRackauckas,arxiv:2109.12449AutomaticDifferentiationinJuliaCurrentstateofADpackages(asofSeptember2021)PackageScalarVector/tensorFirstclassstructsupportGPUMutationRuntimebranchesMaturityForwardDiff377333veryhighReverseDiffslow377limited3highReverseDiffwithcompiledtape3377limited7highTrackerslow373limited3highZygoteslow33373highEnzyme3limitedwipwip33lowDiffractor333373lowSome∂PusecasesalgorithmsrequiredquantitiesrootﬁndingNewton–RaphsonJacobianJacobian-FreeNewtonKrylovJacobian-vectorproductsoptimizationADAMgradientNewtongradient,HessianLevenberg–MarquardtandGauss-NewtonJacobiandifferentialequationsstiffODEsolversJacobianstiffODEJacobian-freesolversJacobian-vectorproductsforwardsensitivitymethodsJacobian-vectorproductsadjointsensitivitymethodsvector-JacobianproductComparedtootherlanguages,JuliaADsystems:•GenerallyactdirectlyonthestandardJuliasyntax,withitsstandardlibrary,arrayimplementation,andGPUaccelerationtools.•Haveacommongroundonwhichdifferentiationrulesaredeﬁned,ChainRules.jl.•Caninteropandbeusedtogetherforhigher-orderAD.Thisempowerstheideaofa“glueAD”system,orasystemwhichhelpsusersoptimallycombineseparateADs.LevelsofabstractionIDEA:DEFINEABSTRACTAPIFORDIFFERENTIATIONREDUCINGTHECODECOMPLEXITYFROMO(m×nk)TOO(m+n).APIdescriptionAPI:twoprimitivedeﬁnitionstosupportvariousutilitiesforADusers.ADbackend,primitiveoperation,(primalvalue) derivative,gradient,Jacobian,Hessian,push-forward_function,pullback_function,lazyoperators.Optionsforwithandwithoutreturningtheprimalvalue.Example:ZygoteADperformanceLotka-VolterramodelSpiralNeuralODESummary&outlook•ADperformancecanbeproblemspeciﬁc.NoADpackageistheoptimalchoiceforallproblems.•SwitchingbetweenADpackagesandcombiningthemforhigher-orderderivativesisessentialforperformance.•AbstractDifferentiation.jlreducestheamountofnecessaryboilerplatecodetomixADsystemsandcompletetheirfeature-sets:–bydeﬁningcommonfunctionalitylikeJacobiansandHessiansforADpackages.–bygivingacommoninterfaceacrossADpackagesforsoftwaresuitsthatheavilyemployAD(eg.SciML,Turing).arXiv:2109.12449v2  [cs.MS]  4 Feb 2022