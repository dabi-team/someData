2
2
0
2

g
u
A
1
1

]

G
L
.
s
c
[

1
v
6
9
5
5
0
.
8
0
2
2
:
v
i
X
r
a

Finding Reusable Machine Learning
Components to Build Programming Language
Processing Pipelines

Patrick Flynn2,1, Tristan Vanderbruggen1, Chunhua Liao1, Pei-Hung Lin1,
Murali Emani3, and Xipeng Shen4

1 Lawrence Livermore National Laboratory, Livermore, CA 94550, USA
2 University of North Carolina at Charlotte, NC 28223, USA
3 Argonne National Laboratory, Lemont, IL 60439, USA
4 North Carolina State University, Raleigh, NC 27695, USA

Abstract. Programming Language Processing (PLP) using machine learn-
ing has made vast improvements in the past few years. Increasingly more
people are interested in exploring this promising ﬁeld. However, it is chal-
lenging for new researchers and developers to ﬁnd the right components
to construct their own machine learning pipelines, given the diverse PLP
tasks to be solved, the large number of datasets and models being re-
leased, and the set of complex compilers or tools involved. To improve
the ﬁndability, accessibility, interoperability and reusability (FAIRness)
of machine learning components, we collect and analyze a set of rep-
resentative papers in the domain of machine learning-based PLP. We
then identify and characterize key concepts including PLP tasks, model
architectures and supportive tools. Finally, we show some example use
cases of leveraging the reusable components to construct machine learn-
ing pipelines to solve a set of PLP tasks.

Keywords: reusable datasets, resuable machine learning, programming lan-
guage processing, interoperable pipelines

Introduction

1
In the past decade, machine learning (ML) has made tremendous progress in
solving natural language processing (NLP) tasks. This was due to a variety of
factors, from the advent of Transformer models to the availability of high quality
datasets. In particular, large-scale pre-trained models, such as BERT [9,18], have
been a strong driver of innovation in the domain of NLP.

Similarly, programming language processing (PLP) tasks are beneﬁting from
the availability of pre-trained models and quality datasets. The past few years
saw a vast improvement in the ability of ML to perform a large number of tasks,
such as code generation[21], clone detection[21], source-to-source translation[19],
defect correction[4] or code documentation[12]. There is an increasing interest
in the science community to either directly reuse or expand the ML models for
PLP, given numerous challenges in programming or software engineering ﬁelds.

This work was performed in part under the auspices of U.S. Department of Energy by Lawrence Livermore Na-
tional Laboratory under Contract DE-AC52-07NA27344. It is based upon work supported by the U.S. Department
of Energy, Office of Science, Advanced Scientific Computing Program (ASCR SC-21) under Award Number DE-
SC0021293. This work used resources of the Argonne Leadership Computing Facility (ALCF), which is a DOE
Office of Science User Facility supported under Contract DE-AC02-06CH1135. LLNL-CONF-837414

 
 
 
 
 
 
2

Flynn et al.

However, the rapid development of ML for PLP also brings challenges for
researchers and developers who are interested in this promising ﬁeld. First of all,
a large number of diﬀerent ML models and datasets are published each year. It is
diﬃcult for people, especially newcomers, to identify representative ones to get
started. Secondly, diﬀerent model architectures are used to solve diﬀerent types
of PLP tasks, ranging from program understanding to code generation. It is a
daunting job for people to pick the right architectures for a given task. Third,
machine learning workﬂows are typically split into a set of independent, reusable,
modular components that can be pipelined together to create ML models. Such
pipelines make model building more eﬃcient by reusing expensive pre-trained
models and other components. Finally, in many cases, the trend is to add com-
piler analysis into the inputs of traditional NLP models in order to improve the
quality of ML models for PLP. The use of compiler tools make the entire ML
pipelines more complex, resulting in more constraints to its applicability to a
given task.

To improve the ﬁndability, accessibility, interoperability and reusability (FAIR-

ness) of machine learning components, we search through the literature to ﬁnd
representative papers in the domain of machine learning-based programming
language processing. The goal of this paper is twofold: 1) we want to ﬁnd repre-
sentative tasks, datasets, AI model architectures, and supportive compilers and
tools in the domain of programming language processing using machine learning.
2) By studying the capabilities, connections, and constraints of machine learning
models and associated tools, we aim to facilitate the reuse of components of ML
pipelines so researchers or developers can easily create customized ML pipelines
to solve a given task.

We highlight the contributions of this paper as follows.

– We identify and characterize key components of ML pipeline for PLP, in-

cluding tasks, models, datasets, and tools.

– We extend the taxonomy for PLP tasks, adding code-to-analysis and semantic-

matching categories of tasks.

– We propose a taxonomy of tokenization tools used in PLP based on compiler

engineering terminology.

– We demonstrate how the identiﬁed components can be composed to form

diﬀerent pipelines to solve given tasks.

2 Selected Publications
Our search for relevant papers used the following method. We ﬁrst selected
a few recent high-impact publications (ie, BERT) as our initial seed publica-
tions. These were either well-known in the research community or discovered
by searching for speciﬁc keywords (mostly cross-product of ”ML”, ”AI” with
”source code”, ”code analysis”, ”code understanding”) using Google Scholar.
From these seed publications, we followed their referenced publications (recur-
sively), especially looking for (1) model architecture that inspired the publi-
cation, (2) their experiments (downstream tasks and datasets) and (3) which

Reusable PLP Components

3

models they compared against. To narrow down the scope further, only rep-
resentative publications with compelling artifacts, including published models,
supportive tools and suﬃcient documentation, are selected. We still evaluated
publications associated with some models that are only available through an
API, such as AlphaCode [20] and OpenAI’s CodeX [7]. We stopped the search
when enough papers were collected to build a cohesive picture of PLP domain
presented in this paper.

Out of the surveyed publications, CodeXGLUE [23] is the most signiﬁcant.
It is a benchmark dataset for code understanding and generation. It aggregates
both datasets and baseline models from the NLP side of the community. These
models are based on CodeBERT [11] and CodeGPT [23], variations of BERT [9]
and GPT [26] trained on code.

We selected SynCoBERT[33] and ProGraML[8] to be our reference models as
together they cover most of the features that we captured in our taxonomies. Syn-
CoBERT is an extension to CodeBERT and CuBERT [16]. It considers source
code, natural language text, and the corresponding AST, the latter for semantic
understanding. It presents an evaluation with common code-to-code, code-to-
text, and text-to-code tests. ProGraML is an evolution of NCC which branched
away from NLP methods. It applies graph neural networks to graph representa-
tions of LLVM IR. Most importantly, ProGraML is used to solve ﬁve traditional
compiler analysis tasks, including control- and data-ﬂow, function boundaries,
instruction types, and the type and order of operands over complex programs.

3 Taxonomies
One contribution of this paper is a set of taxonomies for things in the domain of
Programming Language Processing (PLP) using machine learning (ML). We ex-
tend CodeXGLUE’s classiﬁcation of downstream tasks, organize relevant model
architectures, and propose a taxonomy of tokenization tools/methods.

3.1 Downstream Tasks
The surveyed publications used a variety of downstream tasks to evaluate the
capability of their models. Most of these tasks fall clearly under one of the
four categories (Code-to-Code, Code-to-Text, Text-to-Code, and Text-to-Text)
introduced by CodeXGLUE.

CodeXGLUE categorizes the tasks in terms of the input and output modali-
ties. However, the authors only considered code and text, which are sequences of
programming language and natural language tokens. It means that tasks where
the output is a scalar value (classiﬁcation and regression tasks) cannot be easily
categorized. There are three such tasks: defect detection, clone detection, and
code search. Defect detection aims at classifying code samples between correct
and defective, it is a binary classiﬁcation. The other two aim at predicting se-
mantic matching between two sequences. ProGraML and its predecessor NCC
use tasks that target scalar values: OpenCL device mapping, OpenCL thread
coarsening factor, and algorithm classiﬁcation.

Given the limitation of CodeXGLUE’s task categorization, we propose to
add two new categories of PLP tasks: code-to-analysis and semantic-matching.
This is illustrated in Figure 1.

4

Flynn et al.

Fig. 1. Our taxonomy of tasks based on CodeXGLUE’s categorization

The Code-to-Analysis category groups tasks taking code as input and gen-
erating results that are neither text nor code. We create four subcategories in
this category: 1) Performance modeling either predicts a runtime metric of the
code or a conﬁguration that optimizes the said metric. 2) Compiler analysis
includes traditional static compiler analysis tasks, particularly the reachability,
dominator trees, and liveness analysis from ProGraML. 3) Algorithm detection
is similar to clone detection, but we narrowly deﬁne it as detecting the use of a
ﬁnite set of algorithms in a code. 4) Defect detection aims to use commits with
information about resolved vulnerabilities to predict detects in new code. While
these tasks have traditional solutions, the most advanced analysis are long and
expensive (if even possible). We will see later that these analysis are a good
source of self-supervised learning tasks.

Semantic-matching includes both clone detection [29] and code search [12]
tasks. With the current state-of-the-art in deep-learning, all search problems are
formulated by evaluating the semantic similarity between a “query” and each
element of a set of “candidates”. The result of this formulation are models that
take a pair of code-code (used by clone detection) or text-code (used by code
search) sequences as inputs and predict a similarity score for the pair.

The Code-to-Code category includes code completion, code repair, code trans-
lation, and cloze testing (occlusion test or “ﬁlling the blank” colloquially). Code
completion aims to predict the next code token or the next section of code. Code
repairs aims at reproducing a code without an existing defect. This defects are
found by mining git repositories for very simple commits with descriptive mes-
sage of the the issue being ﬁxed. Code translation aims to convert a body of text
from one programming language to another.

The Code-to-Text, Text-to-Code, and Text-to-Text categories are the same
as described in CodeXGlue. Code-to-text includes code summarization, which
aims to produce documentation for a given piece of code. Text-to-code includes
generating code from natural language requirements. Text-to-text includes doc-
ument translation accross natural languages.

Finally, we must mention self-supervised tasks that are used to pretrain mod-
els. We describe a few of these in more detail when illustrating BERT and Syn-
CoBert. We particularly look into Masked Language Modeling (MLM) which is a

Code to AnalysisCode to CodeCode CompletionCode to TextCode SummarizationText to CodeNL description to CodePerformance ModelingText to TextCompiler AnalysisAlgorithm ClassificationDefect DetectionCode RepairCode TranslationCloze TestingDocumentation TranslationSemantic MatchingClone DetectionCode SearchReusable PLP Components

5

form of Cloze Testing, Identiﬁer Prediction (IP), and Edge Prediction (EP). The
interesting fact about these tasks is how much they are tied to the tokenization
tools that we introduce in Section 3.3.

In Table 1, tasks are associated with pretrained models that are used to
support downstream tasks and the datasets from which they can be derived.
This table can also be used to guide users to easily ﬁnd the right models and
datasets for a given task.

Tasks

Models

Datasets

C
C
N

L
M
a
r
g
o
r
P

T
R
E
B
o
C
n
y
S

T
R
A
B
A
L
P

5
T
e
d
o
C

P
V
M

-
e
d
o
C

T
R
E
B
e
e
r
T

e
d
o
C
a
r
t
n
o
C

T
R
E
B
e
d
o
C
h
p
a
r
G

T
x
e
T
o
C

T
R
E
B
e
d
o
C

e
u
l
G
X
e
d
o
C

]
2
1
[
t
e
N
h
c
r
a
e
S
e
d
o
C

L
C
n
e
p
O
e
n
u
T
p
e
e
D

T
C
A
P

]
5
2
[
4
0
1
-
J
O
P

]
9
2
[
h
c
n
e
B
e
n
o
l
C
g
i
B

]
4
1
[
J
4
s
t
c
e
f
e
D

n
g
i
v
e
D

s
t
e
s
a
t
a
D
a
v
a
J

g
n
a
W

R

I

M
V
L
L

r
e
p
y
T
p
e
e
D

]
7
2
[
0
5
1
Y
P

]
2
[
s
u
p
r
o
C
a
v
a
J

b
u
h
t
i

G

]
0
3
[
t
e
s
a
t
a
d

s
’
o
n
a
f
u
T

t
e
s
a
t
a
d

s
’
n
e
y
u
g
N

s
n
a
r
T
e
d
o
C

t
s
e
T
v
d
A

]
3
1
[
E
D
O
C
N
O
C

A
Q
s
o
C

a
L
a
N
o
C

m
o
C
p
e
e
D

0
0
0
8
n
o
h
t
y
P

(cid:51)

(cid:51) (cid:51)
Performance Modeling
Algorithm Classiﬁcation (cid:51) (cid:51)
Defect Detection
Compiler Analyses
Code Completion
Code Repair
Code Translation
Cloze Testing
Text-to-Code Generation
Code Summarization
Document Translation
Code Search
Clone Detection

(cid:51) (cid:51)

(cid:51)

(cid:51)
(cid:51) (cid:51) (cid:51) (cid:51)

(cid:51) (cid:51) (cid:51) (cid:51)

(cid:51)
(cid:51) (cid:51) (cid:51)

(cid:51) (cid:51)

(cid:51) (cid:51)
(cid:51)

(cid:51)

(cid:51)

(cid:51) (cid:51) (cid:51)

(cid:51) (cid:51) (cid:51)

(cid:51) (cid:51)

(cid:51) (cid:51)
(cid:51)
(cid:51) (cid:51) (cid:51)

(cid:51)
(cid:51) (cid:51)

(cid:51) (cid:51)

(cid:51) (cid:51)

(cid:51)
(cid:51) (cid:51) (cid:51)

(cid:51) (cid:51) (cid:51) (cid:51)

(cid:51)

(cid:51) (cid:51) (cid:51) (cid:51)

(cid:51)

(cid:51)

Table 1. PLP tasks and suitable models and datasets

3.2 Model Architectures
We have seen that models can be trained to solve a number of diﬀerent tasks.
In this section, we categorize the various neural architectures used to solve PLP
tasks. Being aware of the various architectures and how they can be composed
is essential to leveraging the pretrained models. Indeed, often the input-output
of the pretraining does not match the task that we wish to specialize it for. In
this case, parts of the pretrained model are discarded and new components are
trained from scratch. This leads to some confusion, as a given model is often
used to refer to either its neural architecture or a trained instance of the said
architecture.

We present a coarse taxonomy of the diﬀerent architecture we have encoun-
tered, as shown in Figure 2. It is based on the modality on which the architecture
operates. This categorization is common for a general purpose presentation of
Neural Networks. We redirect the reader toward [15,28] and [6] for taxonomies
of NLP models and graph models, respectively.

Feed Forward Neural Networks (FFNN) operate on tensors. Their neurons’
connections form a DAG. Common FFNNs are the multilayer perceptron (MLP)
or a simple embedding layer. FFNNs are the building blocks of deep-learning.

6

Flynn et al.

Fig. 2. Taxonomy of architectures based on the modality they operate on.
Recursive Neural Networks (RNN) operate on sequences (of tensors). RNNs have
an initial state (tensor) that is updated for each element of the input sequence.
After each update, an output can be produced while the ﬁnal state can be used
for sequence understanding. Long Short Term Memory (LSTM) is an advanced
RNN architecture.

Transformers have changed the landscape of deep-learning quite radically.
Initially the attention mechanism, which is the base of the transformer archi-
tecture, was used as part of RNN architectures. However, since its introduction
in the aptly named “Attention is All You Need” paper in 2017 [31], Transform-
ers have replaced RNNs for language modeling tasks. Transformers have also
shown the ability to outperform convolutional neural networks (CNN) for image
processing [10,5] and text-to-image [34].

The Transformer architecture uses attention, a deep-learning mechanism,
whereas the dot-product of keys and queries measures the attention that should
be given to a value. The nature of the attention mechanism makes transformers
a set-to-set architecture. However, by simply adding a positional embedding to
each token’s embedding, Transformers act as sequence-to-sequence architectures.

Fig. 3. Architectural building blocks and drawing conventions.

In our taxonomy, we highlighted two sorts of transformers: Encoder and Gen-
erator. In the original Transformers [32], both an encoder stack and a decoder

Feed Forward Neural Network (FFNN)Recursive Neural Network (RNN)Long Short Term Memory (LSTM)TransformerEncoderGeneratorGraph Neural Network (GNN)Message Passing Neural Network (MPNN)Multi Layer Perceptron (MLP)Embedding LayerEmbEmb-1Encoder Stack31204Adjacency Matrix2(0)1(0)0(0)2(T)1(T)0(T)FFNNFFNNFFNNFFNNFFNNEmbEnc1Dec1MEnc0Dec000N0MN00M0N0MM0N0N00NDecoder StackM0FFNN0120121320FFNNFFNN(a) Unrolled RNN(b) Transformer (d) Transformer Details(c) Message Passing Neural Network0N0NFFNNFFNNEmb & PositionFFNNFFNNFFNN & Emb-1FFNNFFNNEmb & PositionFFNNFFNNEmb & PositionFFNNFFNNFFNN & Emb-1tensorprocessinputoutputinternalFFNNNN moduleNLPLConventions for Shapes and ColoringtokenrawReusable PLP Components

7

stack are used to produce the output as depicted in Figure 3d. However, the
architecture can be split as shown in Figure 3b. Following this realization, both
encoder-only and decoder-only Transformers have been devised. Bidirectional
Encoder Representations from Transformers (BERT) [9], and Generative Pre-
trained Transformer (GPT) [26] are respective examples of the Transformer’s
Encoder and the Transformer’s Generator. This separation between encoder
and decoder components is common in ML. We illustrate in Figure 4 how this
paradigm applies to the processing of sequence of tokens.

Fig. 4. Left-to-right: a) an encoder produces a tensor from a sequence of tokens, b) a
generator sees an input sequence and autoregressively generates the output sequence,
and c) an encoder-decoder uses the tensor from the encoder to parameterize an autore-
gressive decoder. (See color legend in ﬁgure 3)

Graph Neural Networks (GNN) generalize convolutional neural networks
(CNN) from the grid of an image to the unstructured mesh deﬁned by a graph.
We focused on Message Passing Neural Networks (MPNN), a recent implemen-
tation of the paradigm which oﬀers a lot of ﬂexibility while having good compu-
tational performance (by exploiting the sparsity of the adjacency matrix).

Figure 3 illustrates some common building blocks of various architectures. It
depicts a full Transformer (b) which we use to illustrate BERT and SynCoBERT
in Section 4.1. The Transformer is modeled with four blocks: token and positional
embeddings, encoder-stack, decoder-stack, and ﬁnally embedding reversal.

The ﬁgure also includes the most basic RNN (a) and the coarse-grained
details of the Transformer (d) to illustrate the fundamental diﬀerence between
these architectures. While RNN must propagate information from the ﬁrst token
to the last, Transformers see all the input tokens and the decoder sees all the
previous output tokens (and the encoded inputs).

Finally, Mixture-of-Expert (MoE) models are increasingly being used to scale
large language models eﬃciently. The modern approach to MoE consists of
sparsely activated models which minimizes resource utilization. For example
[17] implemented the MoE-based model to scale a 700 million-parameter dense
model from 1.8 billion parameters with eight experts to 10 billion parameters
using 64 experts, with no impact on the model convergence. The building blocks
within the core transformer model are still the same and it has little inﬂuence
on reusability.
3.3 Tokenization Tools
One of the signiﬁcant changes in the past few years is the integration of compiler-
based analysis results in PLP to enrich the representation of code. In this section,

STARTSTOPEncoder0N0N0M0MSTARTSTOP0M0M0NDecoderGenerator0NEncoder8

Flynn et al.

we categorize the tokenization tools used in PLP as they are diﬀerent from
the traditional NL Tokenization tools (such as NLTK, spaCy, TextBlob, and
WordPiece).

Fig. 5. Our taxonomy of tools.

Fig. 6. Tokenization building block for the pipeline (See color legend in ﬁgure 3)

We organize Programming Language Tokenization tools based on three tra-
ditional stages in compiler frontends: lexical analysis, syntactic analysis, and
semantic analysis (or elaboration). The names scanner and parser are typically
used for the ﬁrst two, while the more general term compiler is used for the last
one. Although compiler generally refers to all three of these tools together, we
use it here because it includes both semantic analysis and other advanced analy-
ses that are compiler-speciﬁc, while diﬀerentiating from the scanner and parser.
This categorization diﬀerentiates the tools based on the output modalities: a
sequence of tokens for scanners, a tree of tokens for parsers, and a graph of
tokens for compilers. We illustrate the diﬀerent scenarios in Figure 6. We found
that in many cases the right most construct is used to leverage compiler tools.
In this case, the tree or graph is traversed using a predetermined traversal. The
resulting sequence of tokens is then joined and tokenized using a NL tokenizer.
We discuss in Section 4.1 how self-supervised pretraining tasks are constructed
using additional information from the PL tokenizers.

In Table 2, we summarize the tokenizers used by several of the publications

that we surveyed.

4 PLP Pipelines
In this section, we demonstrate how to assemble a PLP pipeline. It is build by
composing model architectures to produce the targets speciﬁed by tasks given
the tokens generated by tokenization tools. The pipeline models how raw rep-
resentations ﬂow through diﬀerent components. Given the prohibitive cost of

Natural LanguagePrograming LanguageScannerNLTKspaCyTextBlobWordPieceParserCompilerSource CodeParserSource CodeCompilerSource CodeScannerNL TokenizerTextSource CodeNL Tokenizer0N0N0N3120431204TraversalSource CodeCompilerNL Tokenizer0NReusable PLP Components

9

NL Scanner Parser Compiler

Model
NCC
PrograML
CodeBERT
GraphCodeBERT
SynCoBERT
PLABART
TreeBERT
ContraCode
CoTexT
CodeT5
Code-MVP

Tools
LLVM
LLVM
WordPiece
tree-sitter
tree-sitter
Custom
tree-sitter
BabelJS (compiler)
Custom
tree-sitter

(cid:51)
(cid:51)
tree-sitter, Scalpel, Custom (cid:51)
Table 2. Models & Tools with input types

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

training large scale models, pretrained models can be the only way to solve com-
plex downstream tasks. We look at two representative pretraining pipelines, and
then model how it can be used to solve our downstream tasks.

4.1 Pretraining Pipelines
Figure 7 illustrates the pretraining pipeline of BERT [9]. On the left, we show
a high-level view of the pipeline. It takes two sentences as inputs which are to-
kenized using WordPiece, preﬁxed with the classiﬁcation token, and separated
using a separator token. These tokens are encoded, and positional and segment
embeddings are added. A transformer encoder stack is used to produce bidi-
rectional embeddings of each token. The embedding of the classiﬁcation token,
referred to as “classiﬁcation head”, is used to predict whether sentence A di-
rectly precedes sentence B. All other embeddings are trained using the Masked
Language Model (MLM) self-supervised tasks which we illustrate in the right
part of Figure 7.

Fig. 7. Self-supervised pretraining pipeline of BERT. (See color legend in ﬁgure 3)

CodeBERT [11] uses the same pipeline with pairs of text and code instead of
sentences. The classiﬁcation head is made to predict whether the text and code
are related. They used CodeSearchNet [12] to provide matching pairs of text and
code.

SynCoBERT [33] is a BERT-like PLP model which demonstrates a complex
PLP pipeline, as shown in Figure 8. During pretraining, SynCoBERT inputs can
be text and/or code. The code is processed into scanned tokens and AST to-
kens (resulting from a pre-order depth-ﬁrst traversal of the tree). The ﬁnal text

Masked Language Modelling (MLM)Transformer EncoderSentence ASentence BFFNNFFNNEmb & Position & Segment A/BFFNNWordPieceMLMLossA->BMASKCLSXijModelFFNNFFNNFFNN & Emb-1TokenizerkkSelect 15% of tokensReplace:●80% with <MASK>●10% with random tokens●10% left unchangedLoss on selected tokensWordPieceSEP10

Flynn et al.

Fig. 8. SynCoBERT pretraining pipeline (See color legend in ﬁgure 3)

tokens, code tokens, and AST tokens are produced using a NL tokenizer. The
diﬀerent sequences are separated by a special token but there is no segment em-
bedding. SynCoBERT is pretrained using multiple self-supervised tasks: Masked
Language Model (MLM), Identiﬁer Prediction (IP), and Edge Prediction (EP).
IP uses information from the scanner to determine which of the tokens are part
of an identiﬁer (NL tokenizer can split words). Then, it trains a FFNN to predict
from the encoding of a token whether or not it is part of an identiﬁer. EP uses the
adjacency matrix of the AST as a target. It trains a FFNN to predict whether
there is an edge between two tokens from their embeddings. Both IP and EP
aims at making the encoder understand the syntax of code and AST, and add
that information into the embedding. Finally, SynCoBERT uses multi-modal
contrastive learning (MCL) to train a multi-layer perceptron (MLP) on the clas-
siﬁcation head. This approach helps the encoder to produce a representation
that distinguishes and correlates the diﬀerent inputs.

4.2 Pipeline Specialization
We will now devise how we could leverage the pretrained CodeBERT or Syn-
CoBert to solve our downstream tasks. The two example downstream tasks are:
(1) code similarity analysis on a dataset of OpenMP codes, and (2) listing the
fully qualiﬁed names of all declarations given a piece of C++ code.

The ﬁrst task is to analyze the codes from DataRaceBench [22], a benchmark
suite of both correct and defective OpenMP codes. This benchmark is designed
to systematically and quantitatively evaluate the eﬀectiveness of data race de-
tection tools. We want to use ML to identify redundant source code and identify
potential gaps in coverage of typical code patterns. This is a specialized code
similarity analysis task.

For this task, we will use zero-shot or few-shot learning pipeline which has
become practical with the advent of large scale pretrained models. In Figure 9a,
the pipeline applies pretrained BERT-like models to each of the samples and
collects the embedding on the classiﬁcation head. It then uses the same models to

  NLT  NLTTranformer EncoderFFNNFFNNEmb & PositionMLMIPCodeParserScannerTextNL TokenizerEPCLSSynCoBERT(NL+PL+AST)MLMIPCode AParserScannerText AEPCLSSynCoBERT(NL+AST+PL)MLMIPParserScannerEPCLSSynCoBERT(NL)MLMNLTCLSSynCoBERT (PL+AST)MLMIPParserScannerEPCLSCode BText BIdentifiers Prediction (IP)Edge Prediction (EP)Contrastive LossModelScanner0/1FFNNFFNNFFNN0/1Is identifier?ModelParser/CompilerCross ProductFFNNFFNNFFNN(a)Default SynCoBERT configuration(b) Multi-modal Contrastive Learning (MCL) applied to SynCoBERTAdjacency MatrixMLPMLPMLPMLPReusable PLP Components

11

Fig. 9. This ﬁgure illustrates the zero-shot approach to information discovery in a
dataset of codes. To the left, we illustrate how a pretrained model can be used to
produce embedding of codes and relevant sentences. These embeddings are then com-
pared with pairwise cosine and clustered using conventional techniques. To the right,
we illustrate how we could ﬁne-tune SynCoBERT in the absence of text-code pairs.
(See color legend in ﬁgure 3)

embed relevant text (we consider mining the publication for relevant sentences).
After that, the pipeline applies clustering techniques based on cosine distance to
analyze the dataset. The expectation here is to ﬁnd sentences clustered alongside
the code providing descriptions of the clusters. In the few-shot variation, we
would ﬁrst ﬁne-tune the encoder on this dataset. This will not be as applicable
for CodeBERT pretraining pipeline as the classiﬁcation head requires paired
inputs of text and code to be trained. For SynCoBERT, we could ﬁne tune with
paired code-code by only using (PL-AST) vs (AST-PL) in the MCL task. We
are not sure how this code-only ﬁne tuning would aﬀect the quality of the text
embeddings used for the reference sentences.

The second task is to list all declarations in C++ code (fully-qualiﬁed). This
is the simplest task we could devise for a lexical analyzer (parser). For this task,
we are using automatically generated C++ source codes. Targets are produced
by the code generator.

Fig. 10. On the left, we train an encoder to predict whether a qualiﬁed name refers to
a declaration in a given code. Then this encoder is attached to a decoder to form a full
transformer which we train to generate the full list of declarations. On the right, we
illustrate how a batch of inputs (code and list of qualiﬁed names) can easily be used
to form contrastive pairs to train the encoder. (See color legend in ﬁgure 3)

The ﬁrst step of the pipeline for this task looks at ﬁne-tuning a pretrained
encoder to predict whether a qualiﬁed-name has a matching declaration in a
code. There is a large number of paired inputs for this task. So we can easily
create negative pairs to train the classiﬁcation head of any classiﬁer contrastively.
The second step uses this ﬁne-tuned encoder and trains a decoder to produce
the full list of qualiﬁed names from C++ code.

CLSCLS(a) Clustering PipelineSentenceSource CodeEncoderEncoderCosineClusteringSentenceEncoderSentenceEncoderSentenceEncoderSentenceEncoderSentenceEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSource CodeEncoderSynCoBERT(PL+AST)MLMIPParserScannerEPContrastive LossSynCoBERT(AST+PL) MLMIPScannerParserEPCode ACode BA == B(b) Code-only fine-tuning of SynCoBERTCLSMLPCLSMLPQualified NamePretrained EncoderSourceCodeMLMExists?Fine-Tuned EncoderSourceCodeDecoderList of Qual. NamesN1N5N6N7N8N3N9N5N1N2N3N4N1N2N3N4N5N6N7N8N9✔✔✘✔✘✘C1C2C3✔✘✔✘✔✘✔✘✔✘✔✘✘✔✘✘✔✘✘✔✘C1C2C312

Flynn et al.

5 Related Work

Allamanis et al. discuss how the similarities and diﬀerences between natural lan-
guages and program languages drive the design of probabilistic models. A tax-
onomy is designed based on the underlying design principles of each model and
used to survey the latest published literature. A survey reviews how researchers
adapted these probabilistic models to an application areas and discusses cross-
cutting and application-speciﬁc challenges and opportunities [1].

Maas introduced a taxonomy of ML for systems that aims to provide guidance
if machine learning should be applied to a particular system problem [24]. A
decision diagram is designed to provide recommendations to practitioners and
researchers to choose the most suitable machine learning strategies. Ashouri et
al. survey and classify recent research activities in compiler optimization with
machine learning approaches [3]. The survey focuses on two major compiler
optimization problems: optimization selection and phase ordering. Kalyan et
al. summarize core concepts of transformer-based pretrained language models
(T-PTLMs) and present taxonomy of T-PTLMs with brief overview of various
benchmarks. Various T-PTLMs libraries and highlight of the future research
directions to improve the T-PTLMs are presented [15].

Sarker presents a deep learning taxonomy to provide a comprehensive overview
of deep learning techniques.[28] The taxonomy covers techniques from major
categories including supervised or discriminative learning, unsupervised or gen-
erative learning and hybrid learning. Potential research directions in deep learn-
ing and real-world applications with deep learning are also presented. Chami
et al. also present a taxonomy but with focus in graph representation learn-
ing (GRL) [6]. The taxonomy includes GRL from network embedding, graph
regularization and graph neural networks. GRAPHEDM, a general framework
for GRL, that can be used to succinctly describe over thirty GRL methods is
proposed.

6 Conclusion

In this paper, we have selected a set of representative papers in the domain
of programming language processing (PLP) using machine learning. We have
identiﬁed and categorized common PLP tasks and the associated reusable com-
ponents so newcomers can easily ﬁnd the right models and datasets for a given
task. Using two example tasks, we have shown that the discovered components
can be easily reused to construct customized machine learning pipelines to solve
the given tasks.

We started to implement the pipelines we have describe above to learn more
about DataRaceBench and construct challenging PLP problems. We will also
encode the information into a formal knowledge representation such as ontology
to enable automated pipeline adaptation using workﬂow synthesis techniques.

Reusable PLP Components

13

References

1. Allamanis, M., Barr, E.T., Devanbu, P., Sutton, C.: A survey of machine learning
for big code and naturalness. ACM Computing Surveys (CSUR) 51(4), 81 (2018)
2. Allamanis, M., Sutton, C.: Mining Source Code Repositories at Massive Scale
using Language Modeling. In: The 10th Working Conference on Mining Software
Repositories. pp. 207–216. IEEE (2013)

3. Ashouri, A.H., Killian, W., Cavazos, J., Palermo, G., Silvano, C.: A survey on
compiler autotuning using machine learning. ACM Comput. Surv. 51(5) (Sep 2018)
4. Boˇziˇc, J., Tabernik, D., Skoˇcaj, D.: Mixed supervision for surface-defect detection:
From weakly to fully supervised learning. Computers in Industry 129, 103459 (aug
2021)

5. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-

to-end object detection with transformers (2020)

6. Chami, I., Abu-El-Haija, S., Perozzi, B., R´e, C., Murphy, K.: Machine learning on

graphs: A model and comprehensive taxonomy (2020)

7. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards,
H., Burda, Y., Joseph, N., Brockman, G., et al.: Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374 (2021)

8. Cummins, C., Fisches, Z., Ben-Nun, T., Hoeﬂer, T., O’Boyle, M., Leather, H.:
ProGraML: A Graph-based Program Representation for Data Flow Analysis and
Compiler Optimizations. In: Thirty-eighth International Conference on Machine
Learning (ICML) (2021)

9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018)

10. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale (2020)
11. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu,
T., Jiang, D., et al.: Codebert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155 (2020)

12. Husain, H., Wu, H.H., Gazit, T., Allamanis, M., Brockschmidt, M.: Codesearchnet

challenge: Evaluating the state of semantic code search (2019)

13. Iyer, S., Konstas, I., Cheung, A., Zettlemoyer, L.: Mapping language to code in

programmatic context (2018)

14. Just, R., Jalali, D., Ernst, M.D.: Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In: Proceedings of the 2014 Inter-
national Symposium on Software Testing and Analysis. p. 437–440. ISSTA 2014,
Association for Computing Machinery, New York, NY, USA (2014)

15. Kalyan, K.S., Rajasekharan, A., Sangeetha, S.: Ammus : A survey of transformer-

based pretrained models in natural language processing (2021)

16. Kanade, A., Maniatis, P., Balakrishnan, G., Shi, K.: Learning and evaluating con-

textual embedding of source code (2020)

17. Kim, Y.J., Awan, A.A., Muzio, A., Cruz Salinas, F., Lu, L., Hendy, A., Rajb-
handari, S., He, Y., Hassan Awadalla, H.: Scalable and eﬃcient moe training for
multitask multilingual models (September 2021)

18. Koroteev, M.: Bert: A review of applications in natural language processing and

understanding. arXiv preprint arXiv:2103.11943 (2021)

14

Flynn et al.

19. Lachaux, M.A., Roziere, B., Chanussot, L., Lample, G.: Unsupervised translation

of programming languages (2020)

20. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles,
T., Keeling, J., Gimeno, F., Lago, A.D., et al.: Competition-level code generation
with alphacode. arXiv preprint arXiv:2203.07814 (2022)

21. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles,
T., Keeling, J., Gimeno, F., Lago, A.D., Hubert, T., Choy, P., d’Autume, C.d.M.,
Babuschkin, I., Chen, X., Huang, P.S., Welbl, J., Gowal, S., Cherepanov, A., Mol-
loy, J., Mankowitz, D.J., Robson, E.S., Kohli, P., de Freitas, N., Kavukcuoglu, K.,
Vinyals, O.: Competition-level code generation with alphacode (2022)

22. Liao, C., Lin, P.H., Asplund, J., Schordan, M., Karlin, I.: Dataracebench: a bench-
mark suite for systematic evaluation of data race detection tools. In: Proceedings
of the International Conference for High Performance Computing, Networking,
Storage and Analysis. pp. 1–14 (2017)

23. Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C.B.,
Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano,
M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S.K., Fu, S., Liu, S.:
Codexglue: A machine learning benchmark dataset for code understanding and
generation. CoRR abs/2102.04664 (2021)

24. Maas, M.: A taxonomy of ml for systems problems. IEEE Micro 40(5), 8–16 (2020)
25. Mou, L., Li, G., Zhang, L., Wang, T., Jin, Z.: Convolutional neural networks over
tree structures for programming language processing. In: Proceedings of the Thir-
tieth AAAI Conference on Artiﬁcial Intelligence. pp. 1287–1293 (2016)

26. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-

guage understanding by generative pre-training (2018)

27. Raychev, V., Bielik, P., Vechev, M.: Probabilistic model for code with decision

trees. SIGPLAN Not. 51(10), 731–747 (oct 2016)

28. Sarker, I.H.: Deep learning: a comprehensive overview on techniques, taxonomy,
applications and research directions. SN Computer Science 2(6), 1–20 (2021)
29. Svajlenko, J., Islam, J.F., Keivanloo, I., Roy, C.K., Mia, M.M.: Towards a big
data curated benchmark of inter-project code clones. In: 2014 IEEE International
Conference on Software Maintenance and Evolution. pp. 476–480 (2014)

30. Tufano, M., Watson, C., Bavota, G., Penta, M.D., White, M., Poshyvanyk, D.:
An empirical study on learning bug-ﬁxing patches in the wild via neural machine
translation. ACM Trans. Softw. Eng. Methodol. 28(4) (sep 2019)

31. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,

L., Polosukhin, I.: Attention is all you need (2017)

32. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
(cid:32)L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)

33. Wang, X., Wang, Y., Mi, F., Zhou, P., Wan, Y., Liu, X., Li, L., Wu, H., Liu, J.,
Jiang, X.: Syncobert: Syntax-guided multi-modal contrastive pre-training for code
representation (2021)

34. Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A.,
Yang, Y., Ayan, B.K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H.,
Baldridge, J., Wu, Y.: Scaling autoregressive models for content-rich text-to-image
generation (2022)

