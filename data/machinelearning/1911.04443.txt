1
2
0
2

b
e
F
1

]

C
O
.
h
t
a
m

[

3
v
3
4
4
4
0
.
1
1
9
1
:
v
i
X
r
a

Bundle Method Sketching for Low Rank
Semideﬁnite Programming

Lijun Ding
Cornell University
Ithaca, NY 14850, USA
ld446@cornell.edu

Benjamin Grimmer
Cornell University
Ithaca, NY 14850, USA
bdg79@cornell.edu

Abstract

In this paper, we show that the bundle method can be applied to solve
semideﬁnite programming problems with a low rank solution without ever
constructing a full matrix. To accomplish this, we use recent results from
randomly sketching matrix optimization problems and from the analysis
of bundle methods. Under strong duality and strict complementarity of
SDP, our algorithm produces primal and the dual sequences converging
in feasibility at a rate of ˜O(1/(cid:15)) and in optimality at a rate of ˜O(1/(cid:15)2).
Moreover, our algorithm outputs a low rank representation of its approximate
(cid:15)) within ˜O(1/(cid:15)2)
solution with distance to the optimal solution at most O(
iterations.

√

1

Introduction

We consider solving semideﬁnite problems of the following form:

h−C, Xi

maximize
X∈Sn⊂Rn×n
subject to AX = b
X (cid:23) 0,

(P)

where C ∈ Sn ⊂ Rn×n, A : Sn → Rm being linear, and b ∈ Rn. Denote the solution set as
X?. To accomplish the task of solving (P), we consider the dual problem:

minimize
y∈Rm

h−b, yi

subject to A∗y (cid:22) C,

(D)

whose solution set is denoted as Y?. Then for all suﬃciently large α, e.g., larger than the
trace of any solution X? ∈ X? [5, Lemma 6.1], we can reformulate this as

minimize
y∈Rm

F (y) = h−b, yi − α min{λmin(C − A∗y), 0}.

(1)

We propose applying the bundle method to solve this problem, which generates a sequence
of dual solutions yt. While the bundle method runs on the dual problem, a primal solution
Xt can be constructed through a series of rank one updates corresponding to subgradients of
F . However maintaining such a primal solution greatly increases memory costs. Fortunately,
the primal problem enjoys having a low rank solution in many applications, e.g., matrix
completion [11] and phase retrieval [4]. Also, without specifying the detailed structure the
problem, there always exists a solution X? to (P) with rank r? satisfying r?(r?+1)
≤ m [9]
To utilize the existence of such a low rank solution, we employ the matrix sketching methods
introduced in [13] and speciﬁcally for optimization algorithm in [15]. The main idea is the

2

 
 
 
 
 
 
following: the skecthing method forms a linear sketch of the column and row spaces of the
primal decision variable X, and then uses the sketched column and row spaces to recover the
primal decision variable. The recovered decision variable approximates the original X well if
X is (approximately) low rank. Notably, we need not store the entire decision variable X at
each iteration, but only the sketch. Hence the memory requirements of the algorithm are
substantially reduced.

Our Contributions. Our proposed sketching bundle method produces a sequence of dual
solutions yk and a sequence of primal solutions Xt (which are sketched by low rank matrices
ˆXt). This is done without ever needing to write down the solutions Xt, which can be a
substantial boon to computational eﬃciency.

In particular, we consider problems satisfying the following pair of standard assumptions:
(i) Strong duality holds, meaning that there is a solution pair (X?, y?) ∈ X? × Y? satisfying

p? := h−C, X?i = h−b, y?i =: d?

and (ii) Strict Complementarity holds, meaning there is a solution pair (X?, y?) satisfying
rank(X?) + rank(C − A∗(y?)) = n.

Under these condtions, we show Xt and yt converge in terms of primal and dual feasibility
at a rate of eO(1/t) and the optimality gap converges at a rate of eO(1/
t). In particular, all
three of these quantities converge .
Theorem 1.1 (Primal-Dual Convergence). Suppose the sets X?, Y? are both compact, and
that strong duality and a strict complementary condition holds. For any (cid:15) > 0, Algorithm 1
with properly chosen parameters produces a solution pair Xt and yt with

√

that satisﬁes

t ≤ eO(1/(cid:15))

approximate primal feasibility:
approximate dual feasibility:

approximate primal-dual optimality:

kb − AXtk2 ≤ (cid:15), Xt (cid:23) 0,
λmin(C − A∗yt) ≥ −(cid:15),
√
(cid:15)
|hb, yti − hC, Xti| ≤

Moreover, we show that assuming all of the minimizers in X? are low rank, the sketched
primal solutions ˆXt converge to the set of minimizers at the following rate.
Theorem 1.2 (Sketched Solution Convergence). Suppose the sets X?, Y? are both compact,
strong duality and a strict complementary condition holds, and all solutions X? ∈ X? have
rank at most r. For any (cid:15) > 0, Algorithm 1 with properly chosen parameters produces a
sketched primal solution ˆXt with

that satisﬁes E distF( ˆXt, X?) ≤

√

(cid:15).

t ≤ eO(1/(cid:15)2)

2 Deﬁning The Sketching Bundle Method

Our proposed proximal bundle method relies on an approximation of F given by eF t(y) =
max{F (zt) + hgt, y − zti, ¯F t(y)} where ¯F t(y) is a convex combination of the lower bounds
from previous subgradients. Notice that a subgradient gk of F at zt can be computed as

vt =

(cid:26)MinEigenvector(C − A∗zt)

0

if λmin(C − A∗zt) ≤ 0
otherwise

gk = −b + αAvtv∗

t ∈ ∂F (zt).

Each iteration then computes a proximal step on this piecewise linear function given by

zt+1 ∈ argmin

y∈Rd

eF t(y) +

ρ
2

ky − ytk2

2

(2)

(3)

(4)

for some ρ > 0. The optimality condition of this subproblem ensures that for some θt ∈ [0, 1]
0 = θtgt + (1 − θt)(cid:0)∇ ¯F t(zt) + ρ(zt+1 − yt)(cid:1). Our aggregate lower bound ¯F t(y) is updated
to match this certifying subgradient ¯F t+1(y) = ¯F t(zt+1) + hθtgt + (1 − θt)∇ ¯F t(zt), y − zti.
Then the following is an exact solution for the subproblem (4):

(cid:26)

θt = min

1,

ρ(F (zt) − ¯F t(zt))
kgt − ∇ ¯F t(zt)k2

(cid:27)

zt+1 = yt −

1
ρ

(cid:0)θtgt + (1 − θt)∇ ¯F t(zt)(cid:1).

(5)

(6)

If the decrease in value of F from yt to zt+1 is at least β fraction of the decrease in value
of eF t from yt to zt+1, then the bundle method sets yt+1 = zt+1 (called a descent step).
Otherwise the method sets yt+1 = yt (called a null step).

Algorithm 1 Bundle Method with Cut Aggregation

Input: A : Rn×n → Rd, b ∈ Rd, C ∈ Rn×n , y1 ∈ Rd, ρ > 0, β ∈ (0, 1), T ≥ 0, α > 0
Deﬁne ¯F 1(·) = −∞, z1 = y1, X1 = 0
for t = 1, 2, . . . T do

Compute vt and gt as in (2) and (3)
Compute θt and zt+1 as in (5) and (6)
if F (zt+1) ≤ F (yt) − β
else yt+1 = yt end if
Set ¯F t+1(y) = ¯F t(zt+1) + hθtgt + (1 − θt)∇ ¯F t(zt), y − zti

F (yt) − eF k(zt+1)

(cid:17)

(cid:16)

then yt+1 = zt+1

Compute One Subgradient
Compute One Step

Take Descent Step
Take Null Step
Update Model of F

end for

Extracting Primal Solutions Directly. A solution to the primal problem (P) can be
extracted from the sequence of subgradients gk (as these describe the dual of the dual
problem). Set our initial primal solution to be X0 = 0. When each iteration updates the
model ¯F t, we could update our primal variable similarly:

Xt+1 = θtαvtv∗

(7)
As stated in Theorem 1.1, these primal solutions Xt converge to optimality and feasibility at
a rate of O(1/t). Alas this approach requires us to compute and store the full n × n matrix
Xt at each iteration. Assuming every solution is low rank, an ideal method would only
require O(nr) memory. The following section shows matrix sketching can accomplish this.

t + (1 − θt)Xt.

Extracting Primal Solutions Via Matrix Sketching. Here we describe how the matrix
sketching method of [13] can be used to store an approximation of our primal solution Xt ∈ Sn
+.
First, we draw two matrices with independent standard normal entries

Ψ ∈ Rn×k with k = 2r + 1;
Φ ∈ Rl×n with l = 4r + 3;
Here r is chosen by the user. It either represents the estimate of the true rank of the primal
solution or the user’s computational budget in dealing with larges matrices.
We use Y C

to capture the column space and the row space of Xt:

t and Y R
t

t = XtΨ ∈ Rn×k,
Y C
Hence we initially have Y C
0 = 0 and Y R
matrix Xt directly. Rather, it observes a stream of rank one updates

t = ΦXt ∈ Rl×n.
Y R

0 = 0. Notice Algorithm 1 does not observe the

(8)

Xt+1 = θtαvtv∗

t + (1 − θt)Xt.

In this setting, Y C

t+1 and Y R
t+1 can be directly computed as
t Ψ) + (1 − θt)Y C
t+1 = θtαvt(v∗
Y C
t + (1 − θt)Y R
t+1 = θtα(Ψvt)v∗
Y R

t ∈ Rn×k,
t ∈ Rl×n.

(9)

(10)

3

This observation allows us to form the sketch Y C
We then reconstruct Xt and get the reconstructed matrix ˆXt by

t and Y R
t

from the stream of updates.

t = QtRt, Bt = (ΦQt)†Y R
Y C
t ,

ˆXt = Qt[Bt]r,

(11)

where QtRt is the QR factorization of Y C
t and [·]r returns the best rank r approximation in
Frobenius norm. Speciﬁcally, the best rank r approximation of a matrix Z is U ΣV ∗, where
U and V are right and left singular vectors corresponding to the r largest singular values of
Z and Σ is a diagonal matrix with r largest singular values of Z. In actual implementation,
we may only produce the factors (QU, Σ, V ) deﬁning ˆXT in the end instead reconstructing
ˆXt in every iteration.
Hence the expensive primal update (7) can be replaced by much more eﬃcient operations (9)
and (10). Then a low rank approximation of Xt can be computed by (11).
We remark that the reconstructed matrix ˆXt is not necessarily positive deﬁnite. However,
this suﬃces for the purpose of ﬁnding a matrices close to Xt. More sophisticated procedure
is available for producing a positive semideﬁnite approximation of Xt [13, Section 7.3].

3 Numerical Experiments

In this section, we demonstrate Algorithm 1 equipped with the sketching procedure does
solve problem instances in four important problem classes: (1) Generalized eigenvalue [3,
Section 5.1], (2) Z2 synchronization [2], (3) Max-Cut [7], and (4) matrix completion[11]. For
all experiments, we set β = 1
4 , ρ = 1, and α = 2hI, X?i where X? is computed via MOSEK
[8] or prior knowledge of the problem. We present the results of convergences in Figure
1 for the following problem instances (results for other problems instances are similar for
each problem class): Let D be the distribution of symmetric matrices in S800 with upper
triangular part (including the diagonal) being independent standard Gaussians.

1. Generalized eigenvalue (GE): C ∼ D, A(X) = hB, Xi for any X, where B =

I + 1

n W W ∗ and W ∼ D and is independent of C, and b = 1 ∈ R.

2. Z2 synchronization (Z2): C = J + 1
√
2

5

W where J ∈ S800 is the all one matrix and

W ∼ D, A = diag, and b ∈ R800 is the all one vector.

3. Max-Cut(MCut): C = −L where L is the Laplacian matrix of the G1 graph [1]

with 800 vertices, A and b are the same as Z2 synchronization

4. Matrix Completion(MComp): A random rank 3 matrix X \ ∈ S400 is generated.
The index set Ω ⊂ {1, . . . , 400} × {1, . . . , 400} =: [400]2 is generated in a way that
each (i, j) ∈ [400]2 is in Ω with probability 0.2 independently from anything else.
Set C = I ∈ S800. The linear constraint is Xn+i,n+j = X \
i,j for each (i, j) ∈ Ω. So
[A(X)]i,j = Xi+n,j+n and bi,j = X \

i,j for each (i, j) ∈ Ω.

As can be seen from the experiments, with 2000 iterations or less, the dual and primal
objective converges fairly fast except the matrix completion problem. The infeasibility
measured by kAX − bk and distance to solution is about 10−2 for most of the problems
except GE. In general, we note the convergence is quicker when the problems have rank 1
optimal solutions (GE and Z2) comparing to problems with higher rank optimal solutions
(MCut and MComp).

References
[1] The university of ﬂorida sparse matrix collection: Gset group.

[2] Afonso S Bandeira. Random laplacian matrices and convex relaxations. Foundations of

Computational Mathematics, 18(2):345–379, 2018.

[3] Nicolas Boumal, Vladislav Voroninski, and Afonso S Bandeira. Deterministic guarantees for
burer-monteiro factorizations of smooth semideﬁnite programs. Communications on Pure and
Applied Mathematics, 2018.

4

Figure 1: The upper, middle, and lower plots give the diﬀerent convergence measures (as
indicated on the title) for the dual iterates zt and yt, the primal iterates Xk, and the sketching
iterate ˆXt respectively. The color corresponds to diﬀerent problem instances as indicated in
the legend of the upper plots. Due to numerical error in updating Xt and non-asymmetry of
ˆXt, we compute the minimum eigenvalue of their symmetrized version. λmin( 1
t))
appears to be negative but very small due to numerical error.

2 (Xt + X 0

[4] Emmanuel J Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics, 66(8):1241–1274, 2013.

[5] Lijun Ding, Alp Yurtsever, Volkan Cevher, Joel A Tropp, and Madeleine Udell. An optimal-
storage approach to semideﬁnite programming using approximate complementarity. arXiv
preprint arXiv:1902.03373, 2019.

[6] Yu Du and Andrzej Ruszczyński. Rate of convergence of the bundle method. J. Optim. Theory

Appl., 173(3):908–922, June 2017.

[7] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM),
42(6):1115–1145, 1995.

[8] APS Mosek. The mosek optimization software. Online at http://www. mosek. com, 54(2-1):5,

2010.

[9] Gábor Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity

of optimal eigenvalues. Mathematics of operations research, 23(2):339–358, 1998.

[10] Andrzej P Ruszczyński and Andrzej Ruszczynski. Nonlinear optimization, volume 13. Princeton

university press, 2006.

[11] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm.

In International

Conference on Computational Learning Theory, pages 545–560. Springer, 2005.

[12] Jos F Sturm. Error bounds for linear matrix inequalities. SIAM Journal on Optimization,

10(4):1228–1248, 2000.

[13] Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Practical sketching algo-
rithms for low-rank matrix approximation. SIAM Journal on Matrix Analysis and Applications,
38(4):1454–1485, 2017.

[14] Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Randomized single-view

algorithms for low-rank matrix approximation. 2017.

5

0500100015002000iteration number10-410-2100GEZ2MCutMComp0500100015002000iteration number-1000-800-600-400-2000GEZ2MCutMComp0500100015002000iteration number10-410-2100GEZ2MCutMComp0500100015002000iteration number-1000-800-600-400-2000GEZ2MCutMComp0500100015002000iteration number10-410-21000500100015002000iteration number10-51000500100015002000iteration number-1-0.8-0.6-0.4-0.2010-120500100015002000iteration number10-210-11000500100015002000iteration number10-410-21000500100015002000iteration number10-410-21000500100015002000iteration number-0.5-0.4-0.3-0.2-0.100500100015002000iteration number10-210-1100[15] Alp Yurtsever, Madeleine Udell, Joel Tropp, and Volkan Cevher. Sketchy decisions: Convex
low-rank matrix optimization with optimal storage. In Artiﬁcial intelligence and statistics,
pages 1188–1196. PMLR, 2017.

A Proofs of Convergence Guarantees

A.1 Auxiliary lemmas

Lemma A.1. (Sketching Guarantee)[14, Theorem 5.1] Fix a target rank r. Let X be a
matrix, and let (Y, W ) be a sketch of X of the form (8). The procedure (11) yields a rank-r
matrix ˆX with

(cid:13)
(cid:13)X − ˆX
E
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

√

≤ 3

2kX − [X]rkF.

Here k·kF is the Frobenius norm. Similar operator 2-norm bounds hold with high probability.
Lemma A.2 (Compact sublevel set). If a convex lower semicontinuous function f (x) :
Rn → R ∪ {∞} has a compact nonempty solution set, then all of its sublevel set is compact.

Proof. Suppose for some L ∈ R, then the closed sublevel set SL = {x | f (x) ≤ L} is
unbounded. Then there is a unit direction vector γ ∈ R such that for all x ∈ SL, α ≥ 0,
x + αγ ∈ SL. This in particular violates the fact the solution set is bounded and the proof is
completed.

Lemma A.3 (Quadratic Growth). [12, Section 4] If the solution sets X? and Y? are compact
and strict complementarity holds, then for any ﬁxed (cid:15) > 0, there are some γ1, γ2 such that
for all y with F (y) ≤ h−b, y?i + (cid:15) with α = 2 supX?∈X? trace(X?) , and all X (cid:23) 0 with
|hC, Xi − hC, X?i| ≤ (cid:15) and kAX − bk2 ≤ (cid:15):
2(y, Y?) ≤ γ1(F (y) − F (y?)),

2(X, X?) ≤ γ2(|hC, Xi − hC, X?i| + kAX − bk2).

distF

distF

(cid:15)
Proof. Using the proof of Lemma A.5, we ﬁnd that λmin(C − A∗(y)) ≥ −
.
supX? ∈X? trace X?
Thus h−b, yi ≤ h−b, y?i + 3(cid:15). The result in [12, Section 4] requires the set S1 = {y | F (y) ≤
h−b, y?i + (cid:15)}, and the set S2 = {X | X (cid:23) 0, |hC, Xi − hC, X?i| ≤ (cid:15), and kAX − bk2 ≤ (cid:15)}
being compact. Using [10, Theorem 7.21], the optimization problem minX(cid:23)0 g(X) :=
hC, Xi + γkAX − bk2 has the same solution set as the primal SDP (P) for some large γ > 0
. Thus the compactness of the set S1 and S2 is ensured by Lemma A.2, and the proof is
completed.

A.2 Proof of Theorem 1.1

Let DX? ≥ supX?∈X? trace(X?) and Dy ≥ supF (y)≤F (y0) kyk2. Then we set the bundle
method’s parameters to be α = 2DX? , β = 1/2, and ρ = 1/D2
y. We recall the inner product
for matrices is the trace inner product and is the dot product for the vectors.

In the following three lemmas, we prove bounds on primal feasibility, dual feasibility, and
optimality in terms of F (yt) − F (y?). From this, we can conclude these quantities converge
at the claimed rate since the Du and Ruszcynskii [6] recently showed the bundle method has
F (yt) − F (y?) converge at a eO(1/(cid:15)) rate.
Lemma A.4 (Primal Feasibility). At every descent step t, we have approximate primal
feasibility

Xt+1 (cid:23) 0,

kb − AXt+1k2 ≤

2(F (yt) − F (y?))
βD2
y

.

Proof. Noting that Xt+1 is built out of a convex combination of the rank matrices vtvT
immediate that it is always a positive semideﬁnite matrix.

t , its

6

The deﬁnition of Xt immediately gives the following alternative characterization of ¯F t+1,
¯F t+1(y) = −hC, Xt+1i − hb − AXt+1, yi.
Since we constructed ¯F t+1 to correspond to the ﬁrst-order optimality condition of the
subproblem (4), we have

0 = ∇ ¯F t+1(yt+1) + ρ(zt+1 − yt).
Hence kb − AXt+1k2 = ρ2kyt+1 − ytk2. The distance traveled during any descent step can
be bounded by the objective value gap as

ρ
2

kyt+1 − ytk2 ≤ F (yt) − eF t(yt+1) ≤

F (yt) − F (yt+1)
β

≤

F (yt) − F (y?)
β

where the ﬁrst inequality uses the fact that zt+1 minimizes eF t(·) + ρ
2 k · −ytk and the second
inequality uses the deﬁnition of a descent step. Combining this with our feasibility bound
shows

kb − AXt+1k2 ≤

Then our choice of ρ completes the proof.

2ρ(F (yt) − F (y?))
β

.

Lemma A.5 (Dual Feasibility). At every descent step t, we have approximate dual feasibility

λmin(C − A∗yt+1) ≥

−(F (yt) − F (y?))
DX?

.

Proof. Standard strong duality and exact penalization arguments show for any X? ∈ X?,

hb, yt+1 − y?i = hAX?, yt+1i − hC, X?i

≤ hX?, A∗(yt+1 − C)i
≤ − trace(X?) min{λmin(C − A∗yt+1), 0}.

Recalling our assumption that α ≥ 2D ≥ 2 trace(X?) yields the claimed feasibility bound
F (yt) − F (y?) ≥ F (yt+1) − F (y?) = h−b, yt+1 − y?i − α min{λmin(C − A∗yt+1), 0}

≥ − trace(X?) min{λmin(C − A∗yt+1), 0}.

Lemma A.6 (Primal-Dual Optimality). At every descent step t, we have approximate
primal-dual optimality bounded above by

hb, yt+1i − hC, Xt+1i ≤

α
DX?

(F (yt) − F (y?)) +

s

2(F (yt) − F (y?))
β

.

and below by

hb, yt+1i − hC, Xt+1i ≥ −

1 − β
β

(F (yt) − F (y?)) −

s

2(F (yt) − F (y?))
β

.

Proof. The standard duality analysis shows the primal-dual objective gap equals

hb, yt+1i − hC, Xt+1i = hAXt+1, yt+1i − hC, Xt+1i + hb − AXt+1, yt+1i

= hXt+1, A∗yt+1 − Ci + hb − AXt+1, yt+1i.

Notice that the second term here is bounded above and below as
s
s

|hb − AXt+1, yt+1i| ≤

2(F (yt) − F (y?))
βD2
y

kyt+1k2 ≤

2(F (yt) − F (y?))
β

by Lemma A.4. Hence we only need to show that the ﬁrst term also approaches zero (that
is, we approach holding complementary slackness).

7

An upper bound on this inner product follows from Lemma A.5 as

hXt+1, A∗yt+1−Ci ≤ − trace(Xt+1) min{λmin(C−A∗yt+1), 0} ≤

trace(Xt+1)(F (yt) − F (y?))
DX?

.

Hence

hb, yt+1i − hC, Xt+1i ≤

α
DX?

(F (yt) − F (y?)) +

s

2(F (yt) − F (y?))
β

.

A lower bound on this inner product follows as

1 − β
β

(F (yt) − F (yt+1)) ≥ F (yt+1) − eF t(yt+1)

= F (yt+1) − ¯F t+1(yt+1)
= −α min{λmin(C − A∗yt+1), 0} + hC, Xt+1i − hAXt+1, yt+1i
≥ hXt+1, C − A∗yt+1i,

where the ﬁrst inequality follows from the deﬁnition of a descent step. Hence

hb, yt+1i − hC, Xt+1i ≥ −

1 − β
β

(F (yt) − F (y?)) −

s

2(F (yt) − F (y?))
β

.

A.3 Proof of Theorem 1.2

Using triangle inequality, we see that

(cid:13)
ˆXt − Xt
E distF( ˆXt, X?) ≤ E
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

+ distF(Xt, X?).

(12)

The ﬁrst term of (12) is bounded by

E

(cid:13)
ˆXt − Xt
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

√

(a)
≤ 3

2kXt − [Xt]rkF

√

(b)
≤ 3

2 distF(Xt, X?),

where step (a) is due to Lemma A.1, and step (b) is because X? ∈ X? all has rank less than
or equal to r and [Xt]r is the best rank r approximation of Xt in terms of Frobenius norm.

Combining the above inequalities, we see that

E distF( ˆXt, X?) ≤ (1 + 3

√

2) distF(Xt, X?).

(13)

Our task now is to have an estimate the rates convergence of distF(Xt, X?). Denote the
shorthand g(X) = |hC, Xi − hC, X?i| + γkAX − bk2. Using Lemma A.3, there are some γ
such that for all X with g(X) ≤ g(X?) + 1 satisfy

distF

2(X, X?) ≤ γ(g(X) − g(X?)).

(14)

Then the theorem follows from Theorem 1.1 as g(Xt) − g(X?) converges at a rate of eO(1/(cid:15)2).

8

