Convex Reconstruction of Structured Matrix Signals   

from Linear Measurements (I): Theoretical Results     

Yuan Tian     

Software School, Dalian University of Technology, Dalian, P.R.China, 116620   

12 October, 2019   

Abstract    We investigate the problem of reconstructing n-by-n structured matrix signal X=(x1,â€¦,xn) 
via  convex  programming,  where  each  column  xj is  a  vector  of  s-sparsity  and  all  columns  have  the  same 
l1-norm. The regularizer is matrix norm |||X|||1:=maxj|xj|1. The contribution in this paper has two parts. The 
first  part  is  about  conditions  for  stability  and  robustness  in  signal  reconstruction  via  solving  the  inf-|||.|||1 
convex  programming  from  noise-free  or  noisy  measurements.  We  establish  uniform  sufficient  conditions 

which are very close to necessary conditions and non-uniform conditions are also discussed. Similar as the 

inf-l1  compressive  sensing  theory  for  reconstructing  vector  signals,  a  |||.|||1-version  RIP  condition  is 
established. In addition, stronger conditions are investigated to guarantee the reconstructed signalâ€™s support 

stability, sign stability and approximation-error robustness(e.g., with linear convergence rate relative to any 

matrix norm). The second part is about conditions on number of measurements for robust reconstruction in 

noise.  We  take  the  convex  geometric  approach  in  random  measurement  setting  and  one  of  the  critical 

ingredients in this approach is to estimate the related widthsâ€™ bounds in case of Gaussian and non-Gaussian 

distributions.  These  bounds  are  explicitly  controlled  by  signalâ€™s  structural  parameters  r  and  s  which 

determine matrix signalâ€™s column-wise sparsity and l1-column-flatness respectively.   
Keywords    Compressive Sensing, Structured Matrix Signal, Convex Optimization, Column-wise Sparsity, 

Flatness, Sign-Stability, Support-Stability, Robustness, Random Measurement.   

1    Introduction 

Compressive  sensing  develops  effective  methods  to  reconstruct  signals  accurately  or  approximately  from 

accurate  or  noisy  measurements  by  exploiting  a  priori  knowledge  about  the  signal,  e.g.,  the  signalâ€™s 
structural  features[1-2].  So  far  in  most  investigations  the  signals  are  modeled  as  vectors  of  high  ambient 
dimension.  However,  there  are  lots  of  applications  in  which  the  signals  are  naturally  matrices  or  even 
tensors of high orders. For example, in modern radar  systems, e.g., MIMO radar [3], the measurements can 
be  naturally  represented  as  ykl =  âˆ‘ijÎ¦kl,ijXij +  ekl  where  each  ykl  is  the  echo  sampled  at  specific  time  k  and 
specific receiver element l in a linear or planar array; Î¦kl,ij is the coefficient of a linear processor determined 
by  system  transmitting/receiving  and  waveform  features;  ekl  is  the  intensity  of  noise  and  clutter;  Xij,  if 
nonzero,  is  the  scattering  intensity  of  a  target  detected  in  specific  state  cell  (i,j),  e,g,  a  target  at  specific 

distance and radial speed, or at specific distance and direction, etc. In another class of applications related to 

signal  sampling/  reconstruction,  multivariable  functions  (waveforms)  in  a  linear  space  spanned  by  given 

basis,  e.g.,  {ÏˆÎ¼(u)Ï†Î½(v)}Î¼,Î½,  are  sampled  as  s(ui,vj)=âˆ‘Î¼Î½  ÏˆÎ¼(ui)Ï†Î½(vj)Ï‡Î¼,Î½  where  Ï‡Î¼,Î½  are  the  signalâ€™s  Fourier 
coefficients  to  be  recovered  from  the  samples  {s(ui,vj)}.  These  are  typical  examples  to reconstruct  matrix 
signals and many of them can be naturally extended to even more general tensor signal models.   
          So  far  typical  works  on  matrix  signal  compressive  sensing  include  low-rank  matrix  recovery[1,4], 

1 

 
 
 
 
 
matrix  completion,  Kronecker  compressive  sensing[5-6],  etc.  Low-rank  matrix  recovery  deals  with  how  to 
reconstruct the matrix signal with sparse singular values from linear measurements using nuclear norm (sum 

of  singular  values)  as  the  regularizer,  Kronecker  compressive  sensing  deals  with  how  to  reconstruct  the 

matrix  signal  from  matrix  measurements  via  matrix  L1-norm  âˆ‘ij|Xij|  as  the  regularizer,  dealing  with  the 
measurement operator in tensor-product form.   

Matrix signals can have richer and more complicated structures than vector signals. When solving the 

reconstruction  problem  via  convex  programming,  it  is  important  to  select  the  appropriate  matrix  norm  or 

regularizer for specific signal structure. For example, L1-norm is suitable for general sparsity, nuclear norm 
is  suitable  for  singular-value-sparsity,  and  other  regularizers  are  needed  for  more  special  or  more 

fine-grained structures, e.g., column-wise sparsity, row-wise sparsity or some hybrid structure. Appropriate 

regularizer determines the reconstructionâ€™s performance.   

Contributions and Paper Organization    In this paper we investigate the problem of reconstructing 

n-by-n  matrix  signal  X=(x1,â€¦,xn)  by  convex  programming.  Signalâ€™s  structural  features  in  concern  are 
sparsity  and  flatness,  i.e.,  each  column  xj is  a  vector  of  s-sparsity  and  all  columns have  the  same  l1-norm. 
Such  signals  naturally  appear  in  some  important  applications,  e.g.,  radar  waveform  space-time  analysis,   

which will be investigated as an application in subsequent papers. The regularizer to be used is matrix norm 

|||X|||1:=maxj|xj|1 where |.|1 is the l1-norm on column vector space.   

The  contribution  in  this  paper has  two  parts. The  first  part (sec.3  and  sec.4)  is about  conditions  for 

stability  and  robustness  in  signal  reconstruction  via  solving  the  inf-|||.|||1  convex  programming  from 
noise-free or noisy measurements. In sec. 3 we establish uniform sufficient conditions which are very close 

to  necessary  conditions  and  non-uniform  conditions  are  also  discussed.  Similar  as  the  inf-l1  compressive 
sensing theory for reconstructing vector signals, a |||.|||1-version RIP condition is investigated (theorem 3.5). 
In  sec.4  stronger  conditions  are  established  to  guarantee  the  reconstructed  signalâ€™s  support  stability,  sign 

stability  and  approximation-error  robustness.  For  example,  linear  convergence  rate  relative  to  any  matrix 

norm metric is established under a general condition in Theorem 4.1.             

The  second  part  in  our  work  (sec.5  and  sec.6)  is  to  establish  conditions  on  number  of  linear 
measurements  for  robust  reconstruction  in  noise.  We  take  the  convex  geometric  approach [7-10]  in  random 
measurement  setting  and  one  of  the  critical  ingredients  in  this  approach  is  to  estimate  the  related  widthsâ€™ 

bounds  incase  of  Gaussian  and  non-Gaussian  distributions.  These  bounds  are  explicitly  controlled  by 

signalâ€™s  structural  parameters  r  and  s  which  determine  matrix  signalâ€™s  column-wise  sparsity  and 

l1-column-flatness respectively(e.g., lemma 5.1, 6.1 and 6.3).   

Foundations  for  works  in  the  first  part  is  mainly  general  theory  on  convex  optimization(e.g., 

first-order  optimization  conditions)  in  combination  with  the  information  on  subdifferential  âˆ‚|||X|||1,  while 
foundations for the second part is mainly general theory on high-dimensional probability with some recent 

deep  extensions.  This  paper  is  only  focused  on  theoretical  analysis.  Algorithms,  numerical  investigations 

and applications will be the subjects in subsequent papers.   

2    Basic Problems, Related Concepts and Fundamental Facts         

Conventions and Notations: In this paper we only deal with vectors and matrices in real number field and 

only  deal  with  square  matrix  signals  for  notation  simplification,  but  all  results  are  also  true  for  rectangle 
matrix  signals in  complex  field.  Any  vector  x is regarded  as  column  vector,  xT  denotes  its transpose  (row 
vector).  For  a pair  of  vectors  x  and  y,  <x,y>  denotes their  scalar  product.  For  a  pair  of  matrices  X  and  Y, 
<X,Y> denotes the scalar product tr(XTY). In particular, the Frobenius norm <X,X>1/2 is denoted as |X|F.   

For  a  positive  integer  s,  âˆ‘ğ‘›Ã—ğ‘›

ğ‘ 

denotes  the  set  of  n-by-n  matrices  which  column  vectors  are  all  of 

2 

 
sparsity s, i.e., the number of non-zero components of each column vector is at most s. Let S â‰¡ S1âˆª â€¦ âˆªSn 
be  a  subset  of  {(i,j):  i,j=1,â€¦,n}  where  each  Sj  is  a  subset  of  {(i,j):  i=1,â€¦,n}  and  its  cardinality  |Sj|  â‰¤  s, 
ğ‘›Ã—ğ‘›
âˆ‘ (ğ‘†)
  denotes  the  set  of  n-by-n  matrices  {M:  Mij=0  if  (i,j)  not  in  S}.  S  is  called  the  matrix  signalâ€™s 
ğ‘ 
s-sparsity pattern. Obviously  âˆ‘ (ğ‘†)

  is a linear space for given S and  âˆ‘

ğ‘›Ã—ğ‘›
    âˆªğ‘† âˆ‘ (ğ‘†)
ğ‘ 

=ğ‘›Ã—ğ‘›
ğ‘ 

ğ‘›Ã—ğ‘›
ğ‘ 

.   

A matrix M=(m1,â€¦,mn) is called l1-column-flat if all its columnsâ€™ l1-norms |mj|1 have the same value.     
If Xk is a group of random variables and p(x) is some given probability distribution, then Xk ~iid p(x) 

denotes that all these Xkâ€™s are identically and independently sampled under this distribution.   

2.1    Basic Problems   

In this paper we investigate the problem of reconstructing n-by-n matrix signal X=(x1,â€¦,xn) with s-sparse 
and l1-flat column vectors x1,â€¦,xn (i.e., |||X|||1 = |xj|1 for all j) by solving the following convex programming 
problems. The regularizer is matrix norm |||X|||1:=maxj|xj|1.     

y, Î¦, Î·:                  inf |||Z|||1    s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦(Z)|Î± â‰¤ Î·                                        (2.1a) 

Problem MP(Î±)
In this setting y is a measurement vector in Rm with some vector norm |.|Î± defined on it, e.g., |.|Î± being 
the  l2-norm.  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š  is  a  linear  operator  and  there  is  a  matrix  X  (the  real  signal)  satisfying 
y=Î¦(X)+e where |e|Î±â‰¤Î·. In an equivalent component-wise formulation, yi=<Î¦i,X>+ei where Î¦i  âˆˆ ğ‘…ğ‘›Ã—ğ‘›
  for 
each i=1,â€¦,m.     

y, A,B, Î·:                inf |||Z|||1    s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |Yâ€“AZBT|Î± â‰¤ Î·                                      (2.1b) 

Problem MP(Î±)
In this setting Y is a matrix in space  ğ‘…ğ‘šÃ—ğ‘š  with some matrix norm |.|Î± defined on it, e.g., |.|Î± being the 
Frobenius-norm.  Î¦A,B:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘šÃ—ğ‘š:  Zâ†’AZBT  is  a  linear  operator  and  there  is  a  matrix  signal  X 
satisfying  Y=  AXBT+E  and  |E|Î±â‰¤Î·.  In  an  equivalent  component-wise  formulation,  ykl=<Î¦kl,X>+ekl= 
âˆ‘ijAkiXijBlj+ekl where for each 1â‰¤k, lâ‰¤m Î¦kl is a n-by-n matrix with its (i,j)-entry as AkiBlj.   

Remark 2.1    Throughout this paper we only consider the case 0 â‰¤ Î· < |y|Î± for problem MP(Î±)
< |Y|Î± for problem MP(Î±)

y, A,B, Î· since otherwise the minimizers X* of these problems are trivially O.   

y, Î¦, Î· and 0 â‰¤ Î· 

For  the  above  problems,  we  will  investigate  the  real  matrix  signal  Xâ€™s  reconstructability  and 

approximation error where the measurement operator Î¦ and Î¦A,B (actually matrix A and B) are deterministic 
or at random. In some cases problem MP(Î±)
y, A,B, Î· are equivalent each other but in other cases 
some specific hypothesis is only suitable to one of them, so itâ€™s appropriate to deal with them respectively.   

y, Î¦, Î· and MP(Î±)

2.2    Related Concepts   

Some related concepts are presented in this subsection which are necessary and important to our work. For 

brevity all definitions are only presented in the form of vectors, however the generalization to the form of 

matrices is straightforward. 

A cone C is a subset in Rn such that tC is a subset of C for any t>0. For a subset K in Rn, its polar dual 

K*:={y: <x,y>â‰¤0 for all x in K}. K* is always a convex cone.   

For a proper convex function F(x), there are two important and related sets:   

D(F,x):={v:F(x+tv) â‰¤ F(x) for some t>0}   

âˆ‚F(x)={u: F(y) â‰¥ F(x) + <yâ€“x, u> for all y}   

and an important relation is D(F,x)* =  âˆªğ‘¡>0tâˆ‚F(x).   

Let |.| be some vector norm and |.|* be its conjugate norm, i.e., |u|*:=max{<x,u>: |x| â‰¤ 1}(e.g., |||X|||1
* = 
âˆ‘j |xj|âˆ) then                                      âˆ‚|x| = {u: |x| = <x,u> and |u|* â‰¤ 1}                                                    (2.2) 
Let K be a cone in a vector space L on which Î¦ is a linear operator, the minimum singular value of Î¦ 

with respect to K, norm |.|Î² on L and norm |.|Î± on Î¦â€™s image space is defined as 

3 

 
                                        Î»min,Î±,Î²(Î¦;K):=inf{|Î¦u|Î±: u in K and |u|Î²=1}                                                (2.3) 

When both |.|Î² and |.|Î± are l2(or Frobenius) norms, Î»min,Î±,Î²(Î¦;K) is simply denoted as Î»min(Î¦;K).   

Let K be a cone (not necessarily convex) in normed space (L, |.|Î²), its conic Gaussian width is defined 
as                                            wÎ²(K):=Eg[sup{<g,u>: u in K and |u|Î²=1]                                                    (2.4) 
where  g  is  the  random  vector  on  L  sampled  under  standard  Gaussian  distribution.  When  |.|Î²  is  l2  or 
Frobenius norm on L, wÎ²(K) is simply denoted as w(K).   

2.3    Fundamental Facts 

Our  research  in  the  second  part  (sec.5  and  sec.6)  follows  the  convex  geometric  approach  built  upon  a 

sequence  of  important  results,  which  are  summarized  in  this  section  as  the  fundamental  facts.  Originally 
these facts were presented for vector rather than matrix signals[7-10]. We re-present them for matrix signals in 
consistency with the form of our problems. For brevity, all facts are only presented with respect to problem 
MP(Î±)

y, Î¦, Î· except for FACT 2.6. 

y, Î¦, Î· where Î·=0, then X* = X    iff    kerÎ¦âˆ©D(|||.|||1,X)={O}. 

FACT 2.1 (1)Let Xâˆˆ ğ‘…ğ‘›Ã—ğ‘›  be any matrix signal and y=Î¦(X), X* is the solution (minimizer) to the problem 
MP(Î±)
(2)  Let  Xâˆˆ ğ‘…ğ‘›Ã—ğ‘›  be  any  matrix  signal and y=Î¦(X)+e  where  |e|Î±â‰¤Î·,  X*  be  the  solution  (minimizer) to  the 
problem MP(Î±)

y, Î¦, Î· where Î· > 0, |.|Î² be a norm on signal space to measure the reconstruction error, then   
|X*â€“ X|Î² â‰¤2Î·/Î»min,Î±,Î²(Î¦;D(|||.|||1,X)) 

FACT 2.2    K is a cone in  ğ‘…ğ‘›Ã—ğ‘›(not necessarily convex), Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š  is a linear operator with entries 
Î¦kij ~iid N(0,1), then for any t > 0: 

        Combining these two facts, the following quite useful corollary can be obtained. 

        P[Î»min(Î¦;K) â‰¥ (mâ€“1)1/2 â€“ w(K) â€“ t] â‰¥ 1â€“ exp(â€“t2/2)   

FACT  2.3    Let  X  and  X*  be  respectively  the  matrix  signal  and  the  solution  to  MP(2)
FACT 2.1(2), Î¦kij ~iid N(0,1), then for any t>0:   

y, Î¦, Î·  as  specified  in 

P[|X*â€“X|F    â‰¤ 2Î·/((mâ€“1)1/2 â€“ w(D(|||.|||1,X)) â€“t)+] â‰¥ 1 â€“ exp(â€“t2/2) 
where  (u)+:=max(u,0).  In  particular,  when  the  measurement  vectorâ€™s  dimension  m  â‰¥  w2(D(|||.|||1,X)  + 
Cw(D(|||.|||1, X) where C is some absolute constant, X can be reconstructed robustly with respect to the error 
norm |X*â€“X|F with high probability by solving MP(2)

y, Î¦, Î·.   

FACT  2.4    Let  F  be  any  proper  convex  function  and  zero  matrix  is  not  in  âˆ‚F(X),  then  wÎ²
EG[inf{|Gâ€“tV|Î²*
entries Gij~iid N(0,1). In particular, when |.|Î² is |.|F then w2(D(F, X)) â‰¤ EG[inf{|Gâ€“tV|F
          This fact is useful to estimate the squared Gaussian width wÎ²

2(D(F,  X))  â‰¤ 
2:  t>0,  V  in  âˆ‚F(X)}]  where  |.|Î²*  is  the  norm  dual  to  |.|Î² and  G  is  the  random  matrix  with 
2: t>0, V in âˆ‚F(X)}].     

2(D(F, X))â€™s upper bound. 

FACT  2.5    Let  X  and  X*  be  respectively  the  matrix  signal  and  the  solution  to  MP(2)
y, Î¦, Î·  as  specified  in 
FACT 2.1(2), with the equivalent component-wise formulation yk=<Î¦k,X>+ek, each Î¦k ~iid Î¦ where Î¦ is a 
random matrix which satisfies the following conditions: (1)E[Î¦]=0; (2)There exists a constant Î±>0 such that 
Î± â‰¤ E[<Î¦,U>] for all U: |U|F=1; (3) There exists a constant Ïƒ>0 such that P[|<Î¦,U>| â‰¥ t] â‰¤ 2exp(â€“t2/2Ïƒ2). Let 
Ï:= Ïƒ/Î±, then for any t>0: 

P[ |X*â€“X|F    â‰¤ 2Î·/(c1Î±Ï-2m1/2 â€“ c2Ïƒw(D(|||.|||1,X)) â€“ Î±t)+ ] â‰¥ 1 â€“ exp(â€“c3t2) 

where c1, c2, c3 are absolute constants.   

FACT 2.6    Î“ is a subset in n-by-n matrix space. Define the linear operator Î¦A,B:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘šÃ—ğ‘š: Y=AXBT. 
In the equivalent component-wise formulation, ykl=<Î¦kl,X>=âˆ‘ijAkiXijBlj for each 1â‰¤k,lâ‰¤m, Î¦kl ~iid Î¦ which is 

4 

 
sampled under some given distribution. For any parameter Î¾>0, define 

QÎ¾(Î“; Î¦):= inf {P[|<Î¦,U>| â‰¥ Î¾]: U in Î“ and |U|F=1}   
Furthermore,  for  each  1â‰¤k,lâ‰¤m,  let  Îµkl  ~iid Rademacher  random  variable  Îµ  (P[Îµ=Â±1]=1/2)  which  are  also 
independent of Î¦, and define 

W(Î“; Î¦):=EH[sup{<H,U>: U in Î“ and |U|F=1}]    where H:=m-1âˆ‘klÎµklÎ¦kl=m-1ATEB, E=[Îµkl]; 
Î»min(Î¦; Î“) := inf{(âˆ‘kl |UklÎ¦kl|2 )1/2: U in Î“ and |U|F=1 } 

Then for any Î¾>0 and t>0: 

P[ Î»min(Î¦; Î“) â‰¥ Î¾mQ2Î¾(Î“; Î¦) â€“ 2W(Î“; Î¦) â€“ Î¾t ] â‰¥ 1â€“ exp(â€“t2/2) 
Remark: In Fact 2.6 the definition of Î»min(Î¦; Î“) is the matrix version of that in subsection 2.2 with respect to 
Frobenius  norm.  The  proof  of  FACT  2.5  and  2.6  (with  respect  to  vector  signals)  can  be  found  in  [8]â€™s 

Theorem 6.3 and Proposition 5.1.   

3    Basic Conditions on Matrix Signal Reconstruction   

In this and next section we investigate sufficient and necessary conditions on the measurement operator for 
accurate  and  approximate  signal  reconstruction  via  solving  problems  MP(Î±)
notation  simplicity,  we  only  deal  with  problem  MP(Î±)
transformed into problem MP(Î±)
        We present conditions for accurate, stable and robust reconstruction respectively. As will be seen, these 

y,  A,B,  Î·.  For 
y,  Î¦,  Î·  and  the  formulation  can  be  straightforwardly 

y,  Î¦,  Î·  and  MP(Î±)

y, A,B, Î·. 

conditions are similar as those related to the regularizers with so-called decomposable subdifferentials. The 

vectorâ€™s  l1-norm  and  matrixâ€™s  nuclear  norm  are  such  examples.  However,  âˆ‚|||X|||1  is  not  even 
weakly-decomposable (i.e., there is no W0 in âˆ‚|||X|||1 such that <W0, Wâ€“W0> = 0 for all W in âˆ‚|||X|||1). At first 
we prove a technical lemma 3.1 which describes âˆ‚|||X|||1â€™s structure.     

Lemma 3.1    For n-by-n matrix X=(x1,â€¦,xn) and matrix norm |||X|||1:=maxj|xj|1, the subdifferential   

âˆ‚|||X|||1 = {(Î»1Î¾1,â€¦, Î»nÎ¾n): Î¾j in âˆ‚|xj|1 and Î»j â‰¥ 0 for all j, Î»1+â€¦+Î»n =1 and Î»j=0 for j: |xj|1<maxk|xk|1}   

Proof    Itâ€™s easy to verify the set {(Î»1Î¾1,â€¦, Î»nÎ¾n): Î¾j in âˆ‚|xj|1 and Î»jâ‰¥0 for all j, Î»1+â€¦+Î»n=1 and Î»j=0 for j: 
|xj|1<maxk|xk|1} is contained in âˆ‚|||X|||1: since for any M â‰¡ (Î»1Î¾1,â€¦, Î»nÎ¾n) in this set, we have   

<M,X> = âˆ‘j Î»j<Î¾j,xj> = âˆ‘j Î»j|xj|1 = |||X|||1âˆ‘j Î»j = |||X|||1   

and |||.|||1â€™s conjugate norm |||M|||1
          Now  prove  that  any  M  in  âˆ‚|||X|||1  has  the  form  specified  as  a  member  in  the  above  set.  Let  M  â‰¡ 
(Î·1,â€¦,Î·n), |||Y|||1 â‰¥ |||X|||1 + <Yâ€“X,M> for all Y â‰¡ (y1,â€¦, yn) implies: 

* = âˆ‘j Î»j|Î¾j|âˆ â‰¤ âˆ‘j Î»j = 1, as a result M is in âˆ‚|||X|||1.         

maxj|yj|1 â‰¥ maxj|xj|1 + âˆ‘j <yjâ€“xj, Î·j>                                          (3.1)   

j, j=1,â€¦,n, (3.1) implies 

j be such a vector with component e*

Let Î·j=|Î·j|âˆÎ¾j ( so |Î¾j|âˆ=1 if Î·jâ‰ 0 ), then maxj|yj|1 â‰¥ maxj|xj|1 + âˆ‘j |Î·j|âˆ<yjâ€“xj, Î¾j>. For each j: Î·jâ‰ 0 we can select 
a ij such that |Î¾j(ij)| = 1 and let e*
j(i) = 0 for all iâ‰ ij, 
then for yj = xj + e*
          1+ maxj|xj|1 â‰¥ maxj|yj|1 â‰¥ maxj|xj|1 + âˆ‘j |Î·j|âˆ<e*
As a result 1â‰¥ âˆ‘j |Î·j|âˆ . 
          Furthermore  for  any  given  i,  let  yj =  xj  for  all j  â‰   i  and  yi  be  any  vector  satisfying  |yi|1  â‰¤  |xi|1,  then 
substitute these y1,â€¦,yn into (3.1) we obtain   

j, Î¾j> = maxj|xj|1 + âˆ‘j |Î·j|âˆ|Î¾j|âˆ = maxj|xj|1 + âˆ‘j |Î·j|âˆ   

j(ij) = sgn Î¾ j(ij) and e*

maxj|xj|1 â‰¥ maxj|yj|1 â‰¥ maxj|xj|1 + âˆ‘j <yjâ€“xj, Î·j> = maxj|xj|1 + |Î·j|âˆ<yiâ€“xi, Î¾i>     

i.e., <yiâ€“xi,  Î¾i>  â‰¤  0.  As  a result, <xi,  Î¾i>  â‰¥  <yi, Î¾i>  for  all  yi:  |yi|1  â‰¤  |xi|1  so  <xi,  Î¾i>  â‰¥  |xi|1|Î¾i|âˆ =  |xi|1, hence 
finally we get <xi, Î¾i> = |xi|1. This (together with |Î¾i|âˆ=1) implies Î¾i in âˆ‚|xi|1 if Î·i â‰  0, for any i=1,â€¦,n.   
          In summary, we have so far proved that for any M in  âˆ‚|||X|||1, M always has the form (Î»1Î¾1,â€¦, Î»nÎ¾n) 
where Î¾j in âˆ‚|xj|1, Î»j â‰¥ 0 for all j and Î»1+â€¦+Î»n â‰¤ 1. Since |||X|||1 = <M,X> = âˆ‘j Î»j<Î¾j,xj> = âˆ‘j Î»j|xj|1 â‰¤ maxj|xj|1 

5 

 
âˆ‘j Î»j â‰¤ |||X|||1, as a result Î»1+â€¦+Î»n =1 and Î»j=0 for j: |xj|1<maxk|xk|1.                                                              â–¡   

Remark  3.1:  More  explicitly,  âˆ‚|||X|||1 =  {(Î»1Î¾1,â€¦,  Î»nÎ¾n):  Î»j â‰¥  0  for  all  j,  Î»1+â€¦+Î»n=1  and  Î»j=0  for  j:  |xj|1 < 
maxk|xk|1; |Î¾j|âˆ â‰¤ 1 for all j and Î¾j(i) = sgn(Xij) for Xijâ‰ 0 }.       

3.1    Conditions on Î¦ For Accurate Reconstruction From Noise-free Measurements   

At first we investigate the conditions for matrix signal reconstruction via solving the following problem:   

Problem MPy, Î¦, 0:                          inf |||Z|||1    s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, y = Î¦(Z)                                              (3.2)   

Theorem 3.1    Given positive integer s and linear operator Î¦, the signal Xâˆˆ  âˆ‘ğ‘›Ã—ğ‘›
minimizer of problem MPy, Î¦, 0 where y=Î¦(X) if and only if   

ğ‘ 

is always the unique 

|||HS|||1 < |||H~S|||1                                                              (3.3) 

for any H=(h1,â€¦,hn)  âˆˆ  kerÎ¦\{O} and any s-sparsity pattern S.     

Proof    To prove the necessity, let S be a  s-sparsity pattern and H  âˆˆ  kerÎ¦\{O}. Set y â‰¡ Î¦(HS) = Î¦(â€“H~S) 
and HS  âˆˆ  âˆ‘ğ‘›Ã—ğ‘›
, HS should be the unique minimizer of MPy, Î¦, 0 with â€“H~S as its feasible solution, hence 
|||HS|||1 < |||H~S|||1.   

ğ‘ 

Now  prove  the  sufficiency.  Let  X=(x1,â€¦,xn)  be  a  matrix  signal  with  its  support  S  =  S1âˆª â€¦ âˆªSn  as  a 
s-sparsity  pattern  (where  Sj  =  supp(xj))  and  let  y  =  Î¦(X)  .  For  any  feasible  solution  Z  (â‰ X)  of  MPy, Î¦, 0, 
obviously there exists H=(h1,â€¦, hn) in kerÎ¦\{O} such that Z=X+H. Since âˆ‚|||Z|||1 â‰¥ âˆ‚|||X|||1 + <H,M> for any 
M in âˆ‚|||X|||1, we have     
              âˆ‚|||Z|||1 â€“ âˆ‚|||X|||1 â‰¥ sup{<H,M> : for any M in âˆ‚|||X|||1 }   
          = sup{<H,M> : M=E+V where E=(Î»1sgn(x1)â€¦, Î»nsgn(xn)) and V=(Î»1Î¾1,â€¦, Î»nÎ¾n), |Î¾j|âˆ â‰¤ 1,   

Î»j â‰¥ 0 for all j, Î»1+â€¦+Î»n =1 } ( by lemma 3.1 and notice supp(sgn(xj)) = Sj = ~ supp(Î¾j) ) 

          â‰¥ sup{ â€“ | <H,E> | + <H,V>: E and V specified as the above }       
Î»j<hj|Sj, sgn(xj)> | +  âˆ‘ğ‘›
          = sup{ â€“ |  âˆ‘ğ‘›
          â‰¥ â€“ sup |  âˆ‘ğ‘›
Î»j<hj|Sj, sgn(xj)> |: Î»j â‰¥ 0 for all j, Î»1+â€¦+Î»n =1} + sup {<H~S,V>: |||V|||1
* is |||.|||1â€™s conjugate norm)   

*=âˆ‘j |Î»jÎ¾j|âˆ â‰¤ âˆ‘j |Î¾j|âˆ = 1 where |||.|||1

ğ‘—=1

ğ‘—=1

ğ‘—=1

Î»j<hj|~Sj, Î¾j> :    Î»j and Î¾j specified as the above }   

* â‰¤ 1 } 

Î»j<hj|Sj, sgn(xj)> |: Î»j â‰¥ 0 for all j, Î»1+â€¦+Î»n =1} + |||H~S|||1                                  (3.4)   

                      (note that |||V|||1
          = â€“ sup |  âˆ‘ğ‘›

ğ‘—=1

          = â€“ max j | <hj|Sj, sgn(xj)> | + |||H~S|||1   
          â‰¥  â€“  max  j |  hj|Sj  |1 +  |||H~S|||1  =  â€“  |||HS|||1 +  |||H~S|||1 >  0 under  the  condition  (3.3).  As a result,  X  is  the 
unique minimizer of MPy, Î¦, 0.                                                                                                                          â–¡ 

Remark 3.2:    (3.3) provides the uniform condition for signal reconstruction which applies to all unknown 

column-wise sparse signals. By a similar proof, we can also obtain a (non-uniform) sufficient condition for 

individual signal reconstruction, namely, for given operator Î¦ and unknown signal X with unknown support 

S=S1âˆª â€¦ âˆªSn, if  there  exist  Î»1,  â€¦,  Î»n â‰¥  0:  Î»1+â€¦+Î»n =1  such  that for any  H=(h1,â€¦,  hn)  in kerÎ¦\{O}  there 
holds:                                              |  âˆ‘ğ‘›

Î»j<hj|Sj, sgn(xj)> | < |||H~S|||1                                                                          (3.5) 

ğ‘—=1

then X will be the unique minimizer of MPy, Î¦, 0 where y = Î¦(X).   
          On the  other hand,  from  MPy, Î¦, 0â€™s  first-order  optimization  condition,  i.e.,  for  its minimizer  X  there 
exist M in âˆ‚|||X|||1 and a multiplier vector u such that     
                                                                                          M + Î¦T(u) = O                                                                  (3.6)   
then for any H=(h1,â€¦, hn) in ker Î¦ we have   

0 = <Î¦(H),u > = <H, Î¦T(u)> = â€“ <H, M> = â€“ <H,E+V>     
where E=(Î»1sgn(x1)â€¦, Î»nsgn(xn)) and V=(Î»1Î¾1,â€¦, Î»nÎ¾n), |Î¾j|âˆ â‰¤ 1, so â€“ <H,E> = <H,V>. Note that for the left 
hand side |<H,E>| = |  âˆ‘ğ‘›
|Î»j<hj|~Sj, Î¾j>| 

Î»j<hj|Sj, sgn(xj)> | and for the right hand side |<H,V>| â‰¤  âˆ‘ğ‘›

ğ‘—=1

ğ‘—=1

6 

 
ğ‘—=1

â‰¤  âˆ‘ğ‘›

Î»j|hj|~Sj|1|Î¾j|âˆ  â‰¤  max j |hj|~Sj|1âˆ‘ğ‘›

Î»j |Î¾j|âˆ â‰¤  max j |hj|~Sj|1  =  |||H~S|||1,  we  obtain  a  (relatively  weak) 
non-uniform  necessary  condition,  namely,  for  given  operator  Î¦  and  unknown  signal  X  with  unknown 

ğ‘—=1

support S=S1âˆª â€¦ âˆªSn, if X is the minimizer of MPy, Î¦, 0 where y = Î¦(X) then there exist Î»j â‰¥ 0 for j=1,â€¦,n: 
Î»1+â€¦+Î»n =1 such that for any H=(h1,â€¦, hn) in ker Î¦ there holds the inequality     

|âˆ‘ğ‘›

ğ‘—=1

Î»j<hj|Sj, sgn(xj)> | â‰¤ |||H~S|||1                                                                          (3.7) 

3.2    Conditions on Î¦ For Stable Reconstruction From Noise-free Measurements   

Now investigate the sufficient condition for reconstructing matrix signal via solving MPy, Î¦, 0 where y=Î¦(X) 
for some signal X which is unnecessarily sparse but l1-column-flat. The established condition guarantees the 
minimizer X* to be a good approximation to the real signal X.   

Theorem 3.2    Given positive integer s and linear operator Î¦ with the s-|||.|||1 Stable Null Space Property, 
i.e., there exists a constant 0 < Ï < 1 such that   

|||HS|||1 â‰¤ Ï |||H~S|||1                                                                                      (3.8)   
for  any  H  in  ker  Î¦  and  sparsity  pattern  S  =  S1âˆª â€¦ âˆªSn  where  |Sj|  â‰¤  s.  Let  Z=(z1,â€¦,zn)  be  any  feasible 
solution to problem MPy, Î¦, 0 where y = Î¦(X) for some signal X = (x1,â€¦,xn), then       

|||Z â€“ X|||1 â‰¤ (1â€“Ï) â€“1(1+Ï)(2max j Ïƒs(xj)1 + maxj (|zj|1 â€“ |xj|1))                      (3.9)   
where Ïƒs(v)1 := |v|1 â€“ (|v(i1)|+â€¦+|v(is)|), v(i1),â€¦,v(is) are vâ€™s s components with the largest absolute values. 

In  particular,  for  the  minimizer  X*  of  MPy, Î¦, 0  where  the  real  signal  X  is  l1-column-flat,  there  is  the 

reconstruction-error bound:              |||X* â€“ X|||1 â‰¤ 2(1â€“Ï)â€“1(1+Ï) maxj Ïƒs(xj)1                                        (3.10) 

The proof follows almost the same logic of proving l1-min reconstructionâ€™s stability for vector signals 
under  the  l1  Null  Space  Property  assumption  (e.g.,  see  sec.  4.2  in  [1]).  For  presentation  completeness  we 
provide the simple proof here. The basic tool is an auxiliary inequality (which unfortunately does not hold 
for matrix norm |||.|||1): given index subset Î” and any vector x, z, then[1]   

                                            |(x â€“ z)~Î”|1 â‰¤ | z |1 â€“ | x |1 + | (x â€“z)Î” |1 + 2| x ~Î” |1                                                    (3.11) 

Proof of Theorem 3.2:    For any feasible solution Z=(z1,â€¦,zn) to problem MPy, Î¦, 0 where y=Î¦(X), there is H 
= (h1,â€¦,hn) in ker Î¦ such that Z = H + X. Apply (3.11) to each column vector zj and xj we get     

| hj|~Sj |1 â‰¤ | zj |1 â€“ | xj |1 + | hj|Sj |1 + 2| xj|~Sj |1                                     

Hence |||H~S|||1 â‰¡ maxj |hj|~Sj |1 â‰¤ maxj (|zj|1 â€“ |xj|1) + |||HS|||1 + 2maxj |xj|~Sj| â‰¤ maxj (|zj|1 â€“ |xj|1) + Ï|||H~S|||1+2maxj 
| xj|~Sj |1 ( by (3.8)), namely : 
                                            |||H~S|||1 â‰¤ (1â€“Ï) â€“1(2max j | xj|~Sj |1 + maxj (|zj|1 â€“ |xj|1))   
As  a result  |||H|||1 =  |||HS|||1 +  |||H~S|||1 â‰¤  (1+Ï)|||H~S|||1 â‰¤  (1â€“Ï) â€“1(1+Ï)(2max  j |  xj|~Sj |1  +  maxj  (|zj|1  â€“  |xj|1))  for 
any s-sparsity pattern S, which implies (3.9) since minS max j | xj|~Sj |1 = max j Ïƒs(xj)1.   

In particular, if Z is minimizer X* and X is l1-column-flat then |xj|1=|||X|||1 for any j so maxj (|x*
j|1 â€“ |xj|1) 
= |||X*|||1 â€“ |||X|||1 â‰¤ 0 for minimizer X*, which implies (3.10).                                                                      â–¡   

Remark 3.3: For any flat and sparse signal X, condition (3.8) guarantees X can be uniquely reconstructed by 
solving MPy, Î¦, 0 due to Theorem 3.1, while in this case the right hand side of (3.10) is zero, i.e., this theorem 
is  consisted  with  the  former  one.  In  addition,  (3.10)  indicates  that  the  error  for  the  minimizer  X*  to 
approximate the flat but non-sparse signal X is controlled column-wisely by Xâ€™s non-sparsity (measured by 

maxj Ïƒs(xj)1).       

Remark 3.4: Given positive integer s, sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s and linear operator Î¦, 
Let Î¦T denote the adjoint operator and Î¦S denote the operator restricted on  âˆ‘ (ğ‘†)
(so Î¦S(X) = Î¦(XS)). If 

ğ‘›Ã—ğ‘›
ğ‘ 

7 

 
Î¦S

N((Î¦S

TÎ¦S)â€“1Î¦S

TÎ¦S is a bijection and there exists a constant 0 < Ï < 1 such that the operator norm satisfies the inequality 
TÎ¦~S: |||.|||1 â†’|||.|||1 ) â‰¤ Ï                                            (3.12a)   
TÎ¦S)â€“1 
then  for  any  H  in  ker  Î¦,  from  Î¦S(HS) =  Î¦(HS)  =  Î¦(â€“H~S) =  Î¦~S(â€“H~S) we  can  obtain  HS  =  â€“  (Î¦S
TÎ¦~S: |||.|||1 â†’|||.|||1 )|||H~S|||1 â‰¤ Ï|||H~S|||1, therefore (3.12a) is a 
Î¦S
uniformly  sufficient  condition  stronger  than  s-|||.|||1  Stable  Null  Space  Property  (3.8),  similar  as  Troppâ€™s 
exact recovery condition for vector signalâ€™s l1-min reconstruction. By operator normâ€™s duality N(M: |.|Î± â†’|.|Î² ) 
= N(MT: |.|*

TÎ¦~S(H~S) and then |||H~S|||1 â‰¤ N((Î¦S

TÎ¦S)â€“1Î¦S

Î²â†’|.|*

Î±) an equivalent sufficient condition is:   
TÎ¦S)â€“1: |||.|||1
N(Î¦~S

TÎ¦S(Î¦S

*
 ) â‰¤ Ï                                            (3.12b)   
The  condition  (3.12)  can  be  enhanced  to  provide  more  powerful  results  for  signal  reconstruction 

* â†’|||.|||1

(discussed in next section). Now we conclude this subsection with a simple condition  for problem MPy, Î¦, 0 
and MPy, Î¦, Î·    to guarantee their minimizersâ€™ l1-column-flatness.   

Theorem 3.3(Condition for Minimizerâ€™s l1-Column-Flatness) Given positive integer s, sparsity pattern S = 
S1âˆª â€¦ âˆªSn  where  |Sj|  â‰¤  s  and  linear  operator  Î¦,  let  Î¦T  denote  the  adjoint  operator  and  Î¦S  denote  the 
T(z) doesnâ€™t have any zero-column for any z â‰  0, 
operator restricted on  âˆ‘ (ğ‘†)
then any minimizer X* of MPy, Î¦, 0 or MPy, Î¦, Î· with supp(X*) contained in S is l1-column-flat.     

  (so Î¦S(X) = Î¦(XS)). If Î¦S

ğ‘›Ã—ğ‘›
ğ‘ 

Proof    Consider the problem MPy, Î¦, Î· : inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦S(Z)|2 â‰¤ Î· at first where Î· > 0. For any 
minimizer  X*  of  this  problem  with  both  its  objective  |||.|||1  and  constraint  function  |yâ€“Î¦S(.)|2  convex, 
according  to  the  general  convex  optimization  theory,  there  exist  a  positive  multiplier  Î³*  >  0  and  M*  in 
âˆ‚|||X*|||1 such that   
                                                                            M* + Î³*Î¦S
then    M*  =  Î³*Î¦S
maxk|xk

T(Î¦S(X*) â€“ y) = O and |y â€“ Î¦S(X*)|2 = Î·                                  (3.13) 
j|1 = 

*|1 for every j according to lemma 3.1.   
Now consider the problem MPy, Î¦, 0 : inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, y = Î¦S(Z). For its minimizer X* there is a 
multiplier  vector  u  such  that    M* +  Î¦S
T(u)  =  O.  If  u  â‰   0  then  M*  doesnâ€™t  have  any  zero  column  which 
implies |xj|1=maxk|xk|1 for every j according to lemma 3.1. On the other hand, u = 0 implies M* = O which 
cannot happen according to lemma 3.1 unless X* = O.                                                                                  â–¡ 

T(y  â€“  Î¦S(X*))  can  not  have  any  zero  column  since  yâ€“Î¦S(X*)  â‰   0,  which  implies  |x*

ğ‘š
ğ‘˜=1 Î¦k
ğ‘§ğ‘˜

T(z)) is always a subset in S.   

TZ),  the  adjoint  operator  Î¦T(z)  =  âˆ‘

 :  ğ‘…ğ‘š â†’ ğ‘…ğ‘›Ã—ğ‘›.    For 
Note:  For  MPy,  Î¦,  Î·  where  Î¦(Z)k  =  tr(Î¦k
problem MPY, A,B, Î· where Î¦A,B(Z) = AZBT, the adjoint operator Î¦T(Y) = ATYB:  ğ‘…ğ‘šÃ—ğ‘š â†’ ğ‘…ğ‘›Ã—ğ‘›. In addition, 
supp(Î¦S
Remark 3.5: For the unconstrained convex optimization problem   
2                         
                                                        X* = Arg inf |||Z|||1 + (1/2)Î³ |y â€“ Î¦S(Z)|2
The  sufficient  condition  for  minimizer  X*â€™s  l1-column-flatness  is  the  same:  Î¦S
zero-column for z â‰  0. This fact will be used in sec.4.1.     
          In  fact,  the  first-order  optimization  condition  guarantees  there  is  M*  in  âˆ‚|||X*|||1  such  that  M*  + 
T(y  â€“  Î¦S(X*))  in  âˆ‚|||X*|||1.  Under  the  above  condition,  M*  has  no 
Î³Î¦S
0-column unless y â€“ Î¦S(X*) = 0. However, y â€“ Î¦S(X*) = 0 implies M* = O which cannot happen in âˆ‚|||X*|||1 
k|1 = |||X*|||1 for every j.   
unless X* = O. As a result, |x*

T(Î¦S(X*)  â€“  y)  =  O,  i.e.,  M*  =  Î³Î¦S

T(z)  doesnâ€™t  have  any 

j|1 = maxk |x*

3.3    Conditions on Î¦ For Robust Reconstruction From Noisy Measurements   

Now  consider  matrix  signal  reconstruction  from  noisy  measurements  by  solving  the  convex  optimization 
problem MPy, Î¦, Î· : inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦(Z)|2 â‰¤ Î· where Î· > 0.   

Theorem 3.4    Given positive integer s and linear operator Î¦ with the s-|||.|||1 Robust Null Space Property, 

8 

 
i.e., there exist constant 0 < Ï < 1 and Î² > 0 such that   

|||HS|||1 â‰¤ Ï |||H~S|||1 + Î²|Î¦(H)|2                                                                                (3.14) 
for any n-by-n matrix H and sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s. Let Z=(z1,â€¦,zn) be any feasible 
solution to the problem MPy, Î¦, Î· where y = Î¦(X) + e for some signal X = (x1,â€¦,xn) and |e|2 â‰¤ Î·, then       

|||Z â€“ X|||1 â‰¤ (1â€“Ï)-1(1+Ï)(2max j Ïƒs(xj)1 + 2Î²Î· + maxj (|zj|1 â€“ |xj|1))                          (3.15) 
In  particular,  for  the  minimizer  X*  of  MPy, Î¦, Î· where  the  real  signal  X  is  l1-column-flat,  there  is  the 

error-control inequality:         

|||X* â€“ X|||1 â‰¤ 2(1â€“Ï)â€“1(1+Ï)( maxj Ïƒs(xj)1 + Î²Î· )                                        (3.16) 

Proof    For  any  feasible  solution  Z=(z1,â€¦,zn)  to  problem  MPy,  Î¦,  Î·  where  y=Î¦(X)+e,  Let  Z  â€“  X  =  H  = 
(h1,â€¦,hn), apply (3.11) to each column vector  zj and xj we get | hj|~Sj |1 â‰¤ | zj |1 â€“ | xj |1 + | hj|Sj |1 + 2| xj|~Sj |1 
Hence  |||H~S|||1  â‰¡  maxj |hj|~Sj |1 â‰¤  maxj  (|zj|1  â€“  |xj|1)  +  |||HS|||1 +  2maxj |xj|~Sj|  â‰¤  maxj  (|zj|1  â€“  |xj|1)  +  Ï|||H~S|||1 + 
2maxj | xj|~Sj |1 + Î²|Î¦(H)|2 ( by (3.14)), namely : 
                                            |||H~S|||1 â‰¤ (1â€“Ï) â€“1(2max j | xj|~Sj |1 + maxj (|zj|1 â€“ |xj|1) + Î²|Î¦(H)|2)   
As  a result  |||H|||1 =  |||HS|||1 +  |||H~S|||1 â‰¤  (1+Ï)|||H~S|||1 + Î²|Î¦(H)|2  â‰¤  (1â€“Ï)â€“1(1+Ï)(2maxj |  xj|~Sj |1  +  maxj  (|zj|1 â€“ 
|xj|1)) + 2(1â€“Ï)â€“1Î²|Î¦(X)|2 ) for any s-sparsity pattern S, which implies (3.15) since minS max j | xj|~Sj |1 = max j 
Ïƒs(xj)1.   

In particular, if Z is a minimizer X* and X is l1-column-flat then |xj|1=|||X|||1 for any j so maxj (|x*

j|1 â€“ 
|xj|1) = |||X*|||1 â€“ |||X|||1 â‰¤ 0 for minimizer X*, which implies (3.16).                                                              â–¡   

Remark 3.6: (3.16) indicates that the error for the minimizer X* to approximate the flat but non-sparse signal 
X is up-bounded column-wisely by Xâ€™s non-sparsity (measured by maxj Ïƒs(xj)1) and the noise strength Î·. If 
the  signal  X  is  both  s-sparse  and  l1-column-flat,  the  column-wise  approximation  error  |||X*  â€“  X|||1  â‰¤  2(1â€“
Ï)-1(1+Ï)Î²Î· = O(Î·), i.e., with linear convergence rate.         

Remark 3.7: For any minimizer X* of problem MPy,Î¦, Î·, (3.13) is the first-order optimization condition with 
positive multiplier Î³* > 0 and M* in âˆ‚|||X*|||1. Then for any H = (h1,â€¦,hn) we have <M*, H> = Î³*<y â€“ Î¦(X*), 
*)), V=(Î»1Î¾1,â€¦, Î»nÎ¾n), |Î¾j|âˆ â‰¤ 1 and note that 
Î¦(H)> where by lemma 3.1 M*=E+V, E=(Î»1sgn(x1
supp(E) = supp(X*) = S = ~supp(V), so <E,HS> = <E,H> = â€“ <V,H> + Î³*<y â€“ Î¦(X*), Î¦(H)> = â€“ <V,H~S> + 
Î³*<y â€“ Î¦(X*), Î¦(H)>. Since for the left hand side   
|<E,HS>| = |âˆ‘ğ‘›

*)â€¦, Î»nsgn(xn

Î»j<hj|Sj, sgn(xj

*)> |   

ğ‘—=1

and for the right hand side 

  |<V,H~S>| + | Î³*<y â€“ Î¦(X*), Î¦(H)>| â‰¤  âˆ‘ğ‘›

ğ‘—=1

|Î»j<hj|~Sj, Î¾j>| + Î³*|y â€“ Î¦(X*)|2|Î¦(H)|2   

â‰¤  âˆ‘ğ‘›

ğ‘—=1

Î»j|hj|~Sj|1|Î¾j|âˆ + Î³*Î·|Î¦(H)|2 â‰¤ |||H~S|||1 + Î³*Î·|Î¦(H)|2 

we  obtain  a  non-uniform  necessary  condition,  namely,  for  given  operator  Î¦  and  unknown  signal  X  with 
unknown support S=S1âˆª â€¦ âˆªSn and y = Î¦(X)+e, if X* is the minimizer of MPy, Î¦, Î· with the correct support 
S and correct non-zero component signs as the real signal X, then there exist constants Î²(=Î³*Î·) > 0 and Î»j â‰¥ 
0 for all j=1,â€¦,n: Î»1+â€¦+Î»n =1 such that for any H=(h1,â€¦, hn) there holds the inequality     

|âˆ‘ğ‘›

ğ‘—=1

Î»j<hj|Sj, sgn(xj)> | â‰¤ |||H~S|||1 + Î²|Î¦(H)|2,                                                                          (3.17) 

3.4    M-Restricted Isometry Property     

Itâ€™s  well  known  that  RIP  with  appropriate  parameters  provide  powerful  sufficient  conditions  to  guarantee 

l1-min reconstruction for sparse vector signals. With our regularizer |||X|||1 we propose a similar but slightly 
stronger condition to guarantee reconstruction robustness by solving the convex programming MPy, Î¦, Î· .   

Theorem  3.5    Given  positive  integer  s  and  linear  operator  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š.  Suppose  there  exist  positive 

9 

 
constants 0 < Î´s < 1 and Î”s > 0 such that:   
2 â‰¤ (1 + Î´s )|Z|F
2 â‰¤ |Î¦(Z)|2
(1)    (1â€“ Î´s )|Z|F
(2)    |<Î¦(Z), Î¦(W)>| â‰¤    (Î”s/n)âˆ‘ |ğ‘§ğ‘—|2
ğ‘›
ğ‘—=1
  W=(w1,â€¦,wn)  âˆˆ âˆ‘ğ‘›Ã—ğ‘›
for  any  Z=(z1,â€¦,zn)  âˆˆ âˆ‘
conditions, there are constants Ï and Î² such that   

ğ‘›Ã—ğ‘›
ğ‘ 

ğ‘ğ‘›ğ‘‘

ğ‘ 

2    for any Z  âˆˆ âˆ‘ ;
                                                                                              (3.18) 
ğ‘›Ã—ğ‘›
ğ‘ 
|ğ‘¤ğ‘—|2                                                                                        (3.19) 
with  supp(Z)âˆ©supp(W)  =  âˆ….  Under  these  two 

|||HS|||1 â‰¤ Ï |||H~S|||1 + Î²|Î¦(H)|2                                                                                    (3.20) 

for any n-by-n matrix H and any s-sparsity pattern S, where the constants can be selected as 

Ï â‰¤ Î”s/(1â€“ Î´s â€“Î”s/4),    Î² â‰¤ s1/2(1+Î´s )1/2/(1â€“ Î´s â€“ Î”s/4)                              (3.21) 

          In particular, Î´s +5Î”s/4 <1 implies the robust null space condition: Ï < 1.   

Note: Condition (1) is the standard RIP which implies |<Î¦(Z),Î¦(W)>| â‰¤ Î´2s|Z|F|W|F for s-sparse matrices Z 
and W with separated supports, slightly weaker than condition (2).   

Proof    Let H = (h1,â€¦,hn) be any n-by-n matrix. For each j suppose |hj(i1)| â‰¥ |hj(i2)| â‰¥ â€¦â‰¥ |hj(in)|, let S0(j)   
= {(i1, j),â€¦,(is, j)}, i.e., the set of indices of s components in column hj with the largest absolute values, S1(j) 
= {(i1+s, j),â€¦,(i2s, j)} be the set of indices of s components in hj with the secondary largest absolute values, 
ğ‘› Sk(j), obviously H = âˆ‘kâ‰¥0 HSk. At first we note that (3.20) holds for 
etc., and for any k=0,1,2,â€¦ let Sk =  âˆªğ‘—=1
S as long as it holds for S0, so we try to prove this in the following. Start from condition (1):     
            (1â€“ Î´s )|HS0|F

2 = <Î¦(HS0), Î¦(H) â€“ âˆ‘kâ‰¥1Î¦(HSk)>   

2 â‰¤ |Î¦(HS0)|2
= <Î¦(HS0), Î¦(H)> â€“ âˆ‘kâ‰¥1<Î¦(HS0), Î¦(HSk)> 
â‰¤ |Î¦(HS0)|2|Î¦(H)|2 + (Î”s/n) âˆ‘nâ‰¥ j â‰¥1âˆ‘kâ‰¥1 |hj|S0(j) |2|hj|Sk(j)|2    (by condition (2))   
â‰¤ (1+ Î´s )1/2|HS0|F|Î¦(H)|2
â‰¤ (1+ Î´s )1/2|HS0|F|Î¦(H)|2
        ( by the inequality (âˆ‘sâ‰¥kâ‰¥1 ak

 + (Î”s/n) |HS0|F âˆ‘nâ‰¥ j â‰¥1âˆ‘kâ‰¥1|hj|Sk(j)|2    (by condition (1) and |hj|S0(j) |2 â‰¤ |HS0|F )   
 + (Î”s/n) |HS0|F âˆ‘nâ‰¥ j â‰¥1(sâ€“1/2|hj|~S0(j)|1 + (1/4)|hj|S0(j)|2)   

2) 1/2 â‰¤ sâ€“1/2 âˆ‘sâ‰¥kâ‰¥1 ak + (s1/2/4)(a1â€“as) for a1 â‰¥ a2 â‰¥ â€¦â‰¥ as â‰¥ 0   

and the fact min sâ‰¥iâ‰¥1 |hj|Sk(j)(i)| â‰¥ max sâ‰¥iâ‰¥1 |hj|Sk+1(j)(i)| for any j ) 

â‰¤ |HS0|F ((1+ Î´s )1/2 |Î¦(H)|2
â‰¤ |HS0|F ((1+ Î´s )1/2 |Î¦(H)|2
= |HS0|F ((1+ Î´s )1/2 |Î¦(H)|2

 + (sâ€“1/2Î”s/n)âˆ‘nâ‰¥ j â‰¥1|hj|~S0(j)|1 + (Î”s/4n)âˆ‘nâ‰¥ j â‰¥1|hj|S0(j)|2)   
 + sâ€“1/2Î”s maxj | hj|~S0(j) |1 + (Î”s/4n)n1/2 (âˆ‘nâ‰¥ j â‰¥1|hj|S0(j)|2
 + sâ€“1/2Î”s|||H~S0 |||1 + (Î”s/4n1/2) |HS0|F

 )     

2) 1/2   

Cancel    |HS0|F
hence   

    on  both  sides  we  get  (1â€“  Î´s )|HS0|F  â‰¤  (1+  Î´s )1/2  |Î¦(H)|2

 +  sâ€“1/2Î”s|||H~S0 |||1  +  (Î”s/4n1/2)  |HS0|F

|HS0|F â‰¤ (1â€“ Î´s â€“Î”s/4n1/2) â€“1((1+ Î´s )1/2 |Î¦(H)|2

 + sâ€“1/2Î”s|||H~S0 |||1)   

Note that |||HS0|||1 = max j | hj|S0(j) |1 â‰¤ s1/2max j | hj|S0(j) |2    â‰¤ s1/2|HS0|F and combine this with the above inequality, 
we obtain (3.20) and (3.21) for S0, which implies they hold for any S.                                                          â–¡ 

4    More Properties on Reconstruction from Noisy Measurements   

In this section we establish stronger conditions on the measurement operator Î¦ for some stronger results on 
sparse  and  flat  matrix  signal  reconstruction  from  noisy  measurements,  e.g.,  conditions  to  guarantee 

uniqueness, support and sign stability as well as value-error robustness.   

As  in  last  sections,  for  notation  simplification  this  section  only  deals  with  problem  MPy,Î¦,Î· but  all 
conclusions  can  be  straightforwardly  transformed  into  the  formulation  for  problem  MPY,A,B,Î·.  At  first  we 
note the basic fact that X* = arginf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |y â€“ Î¦(Z)|2 â‰¤ Î· if and only if their exists a multiplier 
Î³* > 0 (dependent on X* in general) such that X* is a minimizer of the unconstrained convex programming 
2 .  In  sec.  4.1  we  investigate  some  critical  properties  for  the  minimizer  of 
inf  |||Z|||1 +  (1/2)Î³*  |y  â€“  Î¦(Z)|2
unconstrained  optimization  (4.1),  then  on  basis  of  these  results  we  establish  conditions  for  robustness, 

10 

 
   
support and sign stability in signal reconstruction via solving MPy,Î¦,Î· in sec. 4.2.   

In the following for given positive integer s, sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s for all j and 
  we  denote  the 

â†’âˆ‘

(ğ‘†)

the  linear  operator  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š,  when  Î¦S
TÎ¦S)â€“1Î¦S
pseudo- inverse (Î¦S
(ğ‘†)

T: Rm â†’  âˆ‘

ğ‘›Ã—ğ‘›
ğ‘ 

TÎ¦S    is  a  bijection  for  âˆ‘ (ğ‘†)
  as Î¦S

*â€“1.   

ğ‘›Ã—ğ‘›
ğ‘ 

ğ‘›Ã—ğ‘›
ğ‘ 

4.1    Conditions on Minimizer Uniqueness and Robustness for MLPy,Î¦(Î³)     

Consider the convex programming (4.1) with given parameter Î³ > 0 (value of Î³ is independently set):   

Problem MLPy,Î¦(Î³)                          inf |||Z|||1 + (1/2)Î³ |y â€“ Î¦(Z)|2

2                                                                            (4.1) 

Lemma 4.1 indicates basic properties of its sparse minimizer under some sparsity-related conditions.   

Lemma 4.1    Given y, positive integer s and sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s for all j, suppose 
the linear measurement operator Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š  satisfies:   
T(z) does not have any 0-column for z â‰  0;   
TÎ¦S is a bijection;   

S=Arginf supp(S) in S|||Z|||1 +(1/2)Î³|y â€“ Î¦(Z)|2

T(Î¦SÎ¦S

*â€“1(y) â€“ y), H>: |||H|||1=1} + sup{<Î¦~S

T(Î¦S

*â€“1)TM, H>: |||H|||1=1, |||M|||1
2 be the minimizer of MLPy,Î¦(Î³) with support in S, i.e.,   
2                                                (4.3) 

* â‰¤ 1 } < 1 (4.2) 

 = Arg inf |||Z|||1 + (1/2)Î³ |y â€“ Î¦S(Z)|2
X*
S

(1)    Î¦S
(2)    Î¦S
(3)    Î³ sup{<Î¦~S
Let X*

Then there are the following conclusions:   

(1)  X*
(2)  X*
(3)  Let Y* = Î¦S
N((Î¦S

S is the unique minimizer of problem (4.3) and is l1-column-flat;         
S is also the unique minimizer of problem (4.1), i.e., the (global) minimizer of (4.1) is unique and is X*
*â†’|||.|||max) where 
TÎ¦S)â€“1â€™s operator norm and matrix norm |||M|||max:= maxij |Mij|.   

(ğ‘†)
*â†’|||.|||max) denotes (Î¦S

S,ij â‰  0 for all (i,j): |Y*

ij | > Î³â€“1N((Î¦S

TÎ¦S)â€“1: |||.|||1

TÎ¦S)â€“1: |||.|||1

*â€“1(y)  âˆˆ âˆ‘

, then X*

ğ‘›Ã—ğ‘›
ğ‘ 

S.   

(4)  With the same notations as the above, if     

then sgn(X*

S,ij) = sgn(Y*

min(i,j) in S |Y*
ij) for all (i,j) in S.   

ij | > Î³â€“1N((Î¦S

TÎ¦S)â€“1: |||.|||1

*â†’|||.|||max)                                          (4.4)   

TÎ¦S is  a  bijection,  (4.3)â€™s  objective  function  LS(Z)  =  |||Z|||1 +  (1/2)Î³  |y  â€“ 
. According to general convex programming theory, its 

S is also the global minimizer of (4.1), we prove its 
S) under the conditions 
S) for any Hâ‰ O with support in S and 

S+H) > L(X*

S+H) > L(X*

S+H with HS = O.   

S is the minimizer of (4.3), by first-order optimization condition there exists M* in âˆ‚|||X*

S|||1 
S) â€“ y) = O                                                        (4.5) 

ğ‘›Ã—ğ‘›
ğ‘ 

2.To prove X*

2 is strictly convex for variable Z  âˆˆ âˆ‘ (ğ‘†)

Proof  :  (1)  Observe  that  when  Î¦S
Î¦S(Z)|2
minimizer X*
S is unique.   
(2) Let L(Z) := |||Z|||1 + (1/2)Î³ |y â€“ Î¦(Z)|2
perturbation by  H  will always increase the  objectiveâ€™s  value, i.e., L(X*
specified by (1)(2)(3). Since conclusion (1) implies L(X*
L(Z) is convex, we only need to consider the perturbation X*
          Since X*
such that                                                M* + Î³ Î¦S
then    M* = Î³*Î¦S
                                                            X*
Now we compute L(X*
          = ||| X*
          = ||| X*
= ||| X*
â‰¥ ||| X*

S+H |||1 â€“ |||X*
S+H |||1 â€“ |||X*
S+H |||1 â€“ |||X*
S+H |||1 â€“ |||X*
          The first term    ||| X*

S|||1 + (1/2) Î³ (| Î¦(X*
S|||1 + Î³ < Î¦(X*
S|||1 + Î³ < Î¦(X*
S|||1 + Î³ < Î¦(X*
S+H |||1 â€“ ||| X*

2   
S) â€“ y, Î¦(H) > + (1/2)Î³ |Î¦(H)|2
S) â€“ y, Î¦~S(H) > + (1/2)Î³ |Î¦~S(H)|2
S) â€“ y, Î¦~S(H) >   
S |||1   
j|1    (supp(X*

S)) and in particular M*

j|1 + |hj|1 ) â€“ maxj |x*

S)                                 

~S = O. Equivalently: 

S)âˆ©supp(H) =  âˆ…)     

*â€“1(y) â€“ Î³â€“1(Î¦S

S+H) â€“ L(X*

T(y â€“ Î¦S(X*

= maxj ( |x*

2 + 2<Î¦(X*

T(Î¦S(X*

S) â€“ y |2

S = Î¦S

2   

TÎ¦S)â€“1(M*)                                                  (4.6)   

S) â€“ y, Î¦(H)>+ |Î¦(H)|2

2 â€“ |Î¦(X*

S) â€“ y |2

2)   

 |||1 + ||| H |||1 â€“ ||| X*
          = ||| X*
S

S |||1    ( condition (1) implies X*

Sâ€™s l1-column-flatness: remark 3.5 )   

11 

 
= ||| H |||1   
By replacing X*
Î³ <Î¦S(X*) â€“ y, Î¦~S(H)>   
T(Î¦SÎ¦S
          = Î³ <Î¦~S
          â‰¥ (â€“ Î³ sup{<Î¦~S
Therefore        L(X*

S with (4.6), note supp(Î¦~S

T) = ~S and |||M|||1

* â‰¤ 1, the second term   

*â€“1Î¦~S(H) >                                                                      (4.7)   

*â€“1(y) â€“ y), H> â€“ < M*, Î¦S
T(Î¦SÎ¦S
S+H) â€“ L(X*

*â€“1(y) â€“ y), H>: |||H|||1=1} â€“ sup{<Î¦~S
T(Î¦SÎ¦S

S) â‰¥ ||| H |||1(1â€“ Î³ sup{<Î¦~S

â€“ sup{<Î¦~S

T(Î¦S

*â€“1)TM, H>: |||H|||1 = 1and |||M|||1

*â€“1)TM, H>: |||H|||1=1, |||M|||1

T(Î¦S
*â€“1(y) â€“ y), H>: |||H|||1 = 1}   

*â‰¤ 1 })|||H|||1       

* â‰¤ 1 })                                  (4.8) 
S  is  the  minimizer  of  (4.1)  and  the 

and  condition  (3)  implies  the  right  hand  side  >  0.  This  proves  X*
minimizer is unique.     
(3) For Y* = Î¦S

*â€“1(y)  âˆˆ âˆ‘ (ğ‘†)
ğ‘›Ã—ğ‘›
ğ‘ 
S,ij | = | Y*
ij â€“ Î³â€“1(Î¦S
ij | â€“ Î³â€“1 max ij | (Î¦S
ij | â€“ N((Î¦S
ij | â€“ N((Î¦S

| X*
â‰¥ | Y*
          â‰¥ | Y*
          â‰¥ | Y*
          > 0    for those (i,j): | Y*
(4) Note that for any non-zero scalars u and v, sgn(u) = sgn(v) iff |u| > |u â€“ v |. Therefore   

  (then supp(Y*) in S) and by (4.6) we have 
ij | â€“ Î³â€“1 | (Î¦S
TÎ¦S)â€“1(M*) ij | â‰¥ | Y*
ij | â€“ Î³â€“1 | (Î¦S
TÎ¦S)â€“1(M*) ij | = | Y*
*â†’|||.|||max)|||M|||1
*     
*â†’|||.|||max) ( |||M|||1
TÎ¦S)â€“1: |||.|||1

TÎ¦S)â€“1(M*) ij |   
TÎ¦S)â€“1(M*) |max   

* â‰¤ 1 )   
*â†’|||.|||max)   

TÎ¦S)â€“1: |||.|||1
TÎ¦S)â€“1: |||.|||1

ij | > N((Î¦S

sgn(X*

In particular, if min(i,j) in S |Y*
S,ij) = sgn(Y*
sgn(X*

ij)    iff    | Y*

S,ij) = sgn(Y*

TÎ¦S)â€“1(M*) ij |                          (4.9) 
ij | = Î³â€“1 | (Î¦S
ij â€“ X*
TÎ¦S)â€“1(M*) ij | so 
ij | > Î³â€“1max(i,j) | (Î¦S
*â†’|||.|||max) then | Y*
ij) for all (i,j) in S.                                                                                                              â–¡ 

ij | > | Y*
TÎ¦S)â€“1: |||.|||1

ij | > Î³â€“1N((Î¦S

4.2    Conditions on Minimizer Uniqueness and Robustness for MPy,Î¦,Î·   

Now consider matrix signal reconstruction via solving the constrained convex programming:   

MPy, Î¦, Î· :                            X* = Arg inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦(Z)|2 â‰¤ Î·                                    (4.10) 

Lemma 4.2    Given y, positive integer s and sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s for all j, suppose 
the linear measurement operator Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š  satisfies:   
T(z) does not have any 0-column for z â‰  0;   
TÎ¦S is a bijection;   
T(Î¦SÎ¦S

(1)    Î¦S
(2)    Î¦S
(3)    sup{<Î¦~S

*â€“1(y) â€“ y), H>: |||H|||1=1} < Î· Î›min(Î¦S

T) (1 â€“ N(Î¦S

*â€“1Î¦~S: |||.|||1

*â†’|||.|||1

*))   

T) := inf { |||Î¦S
where Î›min(Î¦S
Then there are the following conclusions:   

T(z)|||1

*: |z|2 = 1 }.                                                                                                                                                   

(1)    As the minimizer of problem MPy, Î¦, Î·, X* is unique, S-sparse and l1-column-flat;         
(2)    Let Y* = Î¦S
, then for all (i,j) in S:   
ij) for all (i,j): |Y*
X*

*â€“1(y)  âˆˆ âˆ‘ (ğ‘†)

ij â‰  0 and sgn(X*

ij) = sgn(Y*

T: l2â†’|||.|||1

ij | > Î· N(Î¦S

*)N((Î¦S

ğ‘›Ã—ğ‘›
ğ‘ 

TÎ¦S)â€“1: |||.|||1

*â†’|||.|||max)   

(3)    For any given matrix norm |.|Î± there holds:     

|X* â€“ Y*|Î± â‰¤ Î· N(Î¦S

*â€“1: l2â†’|.|Î±)                                                    (4.11) 

Proof    (1) Let X*
S  âˆˆ  Arg inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦S(Z)|2 â‰¤ Î·. i.e., a minimizer with its support restricted 
on S. We first prove X*
S is the only minimizer of this support-restricted problem, then we prove X* is also 
the minimizer of problem MPy, Î¦, Î· (4.10),i.e., X*
S is the global minimizer and (4.10)â€™s minimizer is unique.   
          According to  general  convex  optimization theory,  there  exist  a  positive  multiplier Î³*  >  0 and  M* in 
âˆ‚|||X*
S|||1 such that                  M* + Î³*Î¦S
then equivalently                              X*

S)|2 = Î·                                (4.12) 
TÎ¦S)â€“1(M*)                                                (4.13) 

S) â€“ y) = O and |y â€“ Î¦S(X*

T(Î¦S(X*
S = Î¦S

*â€“1(y) â€“ Î³*â€“1(Î¦S

Suppose X0 is another minimizer of inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦S(Z)|2 â‰¤ Î·, then there exist a positive 

multiplier Î³0 > 0 and M0 in âˆ‚|||X0|||1 such that                       

12 

 
 
M0 + Î³0Î¦S
Equivalently, (4.12) shows that X*
a  strictly  convex  function  on  âˆ‘ (ğ‘†)
minimizer is unique. However, since |||X*
|||X*
minimizer of the support-restricted problem inf |||Z|||1 s.t. Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |yâ€“Î¦S(Z)|2 â‰¤ Î·.   
*â€™s l1-column-flatness is implied by condition (1) and theorem 3.3.   

T(Î¦S(X0) â€“ y) = O and |y â€“ Î¦S(X0)|2 = Î·                                    (4.14) 
S is also a minimizer of LS(Z) = |||Z|||1 + (1/2)Î³*|y â€“ Î¦S(Z)|2
2 which is 
  is  a  bijection  (condition(2)),  as  a  result  LS(Z)â€™s 
  since  Î¦S
2 = 
S|||1 = |||X0|||1 we have LS(X*
S  is  the  unique 

S) = |||X*
2 =  LS(X0),  which  implies  X*

S|||1 + (1/2)Î³*|y â€“ Î¦S(X*
S  =  X0,  i.e.,  X*

S|||1  +  Î³*Î·2/2  =  |||X0|||1 +  (Î³*/2)|y  â€“  Î¦S(X0)|2

TÎ¦S

ğ‘›Ã—ğ‘›
ğ‘ 

S)|2

XS

        Now prove X*
Again  we  start  with  the  fact  that  X*
multiplier Î³* > 0 (which value depends on X*
problem (without any restriction on solutionâ€™s support ) 

S (which is S-sparse and l1-column-flat ) is also a minimizer of problem MPy, Î¦, Î· (4.10). 
2  with  some 
S is the unique minimizer of the convex 

S  =  Arginf  LS(Z)  =  Arginf  |||Z|||1  +  (1/2)Î³*  |y  â€“  Î¦S(Z)|2

S) and by lemma 4.1, X*

under the condition   

inf |||Z|||1 + (1/2)Î³* |y â€“ Î¦(Z)|2

2                                                        (4.15) 

Î³* sup{<Î¦~S
+ sup{<Î¦~S

T(Î¦SÎ¦S
T(Î¦S

*â€“1(y) â€“ y), H)>: |||H|||1=1}   
*â€“1)T(M),H>: |||H|||1=1 and |||M|||1

T(Î¦S(X*

S) â€“ y) |||*

1 = Î³* |||Î¦S

1: |z|2 = 1}|Î¦S(X*

T(z)|||*
T))â€“1                                                              (4.17) 

* â‰¤ 1 } < 1                        (4.16) 
S (under condition (4.16)) being the unique minimizer of 
S is also  a minimizer  of  MPy,Î¦,Î·  (4.10),  which  furthermore  implies  that  MPy,Î¦,Î·â€™s 

          According to convex optimization theory, X*
problem  (4.15)  means  X*
minimizer is unique, S-sparse and l1-column-flat.   
          In  order  to  make  condition  (4.16)  more  meaningful,  we  need  to  replace  the  minimizer-dependent 
parameter Î³* with explicit information. From (4.15)â€™s first-order optimization condition (4.12) we obtain   
          1 â‰¥ |||M*|||*
T) 
1 â‰¥ Î³* min{ |||Î¦S
i.e.,                                                                Î³* â‰¤ (Î· Î›min(Î¦S
with this upper-bound of Î³*, (4.16) can be derived from a uniform condition 
*â€“1(y) â€“ y), H)>: |||H|||1=1}   
*â€“1)T(M), H>: |||H|||1=1and |||M|||1

T))â€“1 sup{<Î¦~S
+ sup{<Î¦~S
which is equivalent to condition (3).   
          From now on we denote X*
TÎ¦S)â€“1: 
(2) For Y* = Î¦S
*â€“1(y)  âˆˆ âˆ‘ (ğ‘†)
ij)  for  all  (i,j)  in  S.  To  replace  multiplier  Î³*  with  more  explicit 
|||.|||1
information in  this  condition,  we need  some  lower  bound  of  Î³*  which  can  be  derived  from the  first-order 
optimization condition M* = Î³*(y â€“ Î¦S
T(Î¦S(X*)) again. Note that X* is l1-column-flat implies every column 
of X* is not 0, further more M* has no 0-column so M* = ( Î»1u1,â€¦, Î»nun) with Î»j > 0 for all j, Î»1+â€¦+Î»n =1 
and |uj|âˆ = 1, as a result |||M*|||1

S as X*. 
  and by lemma 4.1â€™s conclusion (4), if min(i,j) in S |Y*

ğ‘›Ã—ğ‘›
ğ‘ 
*â†’|||.|||max)  then  sgn(X*

* â‰¤ 1 } < 1                          (4.18) 

S) â€“ y|2 =Î³*Î· Î›min(Î¦S

ij | > Î³*â€“1 N((Î¦S

S,ij)  =  sgn(Y*

(Î· Î›min(Î¦S

T(Î¦SÎ¦S

T(Î¦S

1 = |||M*|||1

 â‰¤ Î³* |||Î¦S
*

*
 = âˆ‘jÎ»j|uj|âˆ =1. Hence     
* â‰¤ Î³* N(Î¦S

T(Î¦S(X*) â€“ y) |||1

T: l2â†’|||.|||1

i.e.,                                                        Î³*â€“1    â‰¤ Î· N(Î¦S

T: l2â†’|||.|||1

Replace  Î³*â€“1  with  its  upper-bound  in  (4.19),  we  obtain  if  min(i,j) 
1)N((Î¦S

*â†’|||.|||max) then sgn(X*

ij) for all (i,j) in S.   

S,ij) = sgn(Y*

*)|Î¦S(X*) â€“ y|2 = Î³*Î· N(Î¦S

*)   
*)                                                        (4.19) 
in  S  |Y*
ij  |  >  Î·  N(Î¦S

T: l2â†’|||.|||1

T: 

TÎ¦S is a  bijection  for  âˆ‘ (ğ‘†)

  implies  Î¦S
ğ‘›Ã—ğ‘›
ğ‘ 

T(Î¦S(Y*)â€“y)  =  O  and  then  condition  (1)  leads  to  Î¦S(Y*)  =  y. 
,  so  for any  matrix 

  and notice  X*â€“Y*âˆˆ âˆ‘ (ğ‘†)

ğ‘›Ã—ğ‘›
â†’âˆ‘ (ğ‘†)
ğ‘ 

ğ‘›Ã—ğ‘›
ğ‘ 

TÎ¦S)â€“1: |||.|||1
*â€“1(y)  âˆˆ âˆ‘ (ğ‘†)
ğ‘›Ã—ğ‘›
ğ‘ 

l2â†’|||.|||*
(3)  Y*  =  Î¦S
Furthermore, Î¦S
norm |.|Î±:     

|X* â€“ Y*|Î± = |(Î¦S

TÎ¦S)â€“1(Î¦S
*â€“1: l2â†’|.|Î±)|Î¦S(X*) â€“ y|2 = Î· N(Î¦S

â‰¤ N(Î¦S

TÎ¦S)(X* â€“ Y*)|Î± = |Î¦S

*â€“1Î¦S(X* â€“ Y*)|Î± = |(Î¦S

*â€“1(Î¦S(X*) â€“ y))|Î±     

*â€“1: l2â†’|.|Î±)                                                                          â–¡   

Remark 4.1: Under the conditions specified in this lemma, the minimizer X* can have support stability, sign 
stability and component value-error robustness relative to any metric |.|Î±, e.g., we can take |.|Î± as |.|F, |||.|||1, 

13 

 
|||.|||max, etc. The error is linearly upper-bounded by the measurement error Î·. When Î· decreases as small as 
possible,  the  error  |X*  â€“  Y*|Î± seems  to  be  reduced  also  as  small  as  possible.  However,  this  is  not  true  in 
general because condition (3) may require Î· not be two small.     

        In real  world  applications,  the  support  S  of  the  signal is  of  course  unknown  so  lemma  4.2  cannot  be 

applied  directly.  However,  on  basis  of  lemma  4.2  a  stronger  and  uniform  sufficient  condition  can  be 

established to guarantee the uniqueness and robustness of the reconstructed signal from solving MPy, Î¦, Î·.   

T(z) does not have any 0-column for z â‰  0;   
TÎ¦S is a bijection;   

Theorem  4.1    Given  positive  integer  s  and  the  linear  measurement  operator  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š,  suppose  Î¦ 
satisfies the following conditions for any s-sparsity pattern S = S1âˆª â€¦ âˆªSn where |Sj| â‰¤ s for all j:   
(1)    Î¦S
(2)    Î¦S
(3)    N( Î¦~S
        N( ((Î¦S

T(Î¦SÎ¦S
*â€“1)TÎ¦S
where Î›min(Î¦S
Then for the minimizer X* of problem MPy, Î¦, Î· (4.10) where y = Î¦(X) + e with noise |e|2 â‰¤ Î· and a real 
  of some s-sparsity pattern R, there are the following conclusions:   

*â€“1Î¦~S: |||.|||1
 ) < Î›min(Î¦S
T(Î¦S
*: |z|2 = 1 } and IS := the identical mapping on  âˆ‘ (ğ‘†)

*â€“1 â€“ IS): l2â†’|||.|||1
T â€“ IS
T) := inf { |||Î¦S

*â€“1)T: |||.|||1â†’|||.|||1))   
ğ‘›Ã—ğ‘›
ğ‘ 

T)Î¦~S : |||.|||1â†’l2
T(z)|||1

flat signal Xâˆˆ âˆ‘ (ğ‘…)

*)) or equivalently   

T) (1 â€“ N(Î¦~S

T) (1 â€“ N(Î¦S

* ) < Î›min(Î¦S

*â†’|||.|||1

ğ‘›Ã—ğ‘›
ğ‘ 

.                                                                                                                                                   

(1)    Sparsity, flatness and support stability:   

X*âˆˆ âˆ‘ (ğ‘…)
ğ‘›Ã—ğ‘›
ğ‘ 

  and is l1-column-flat and the unique minimizer of MPy, Î¦, Î·;         

(2)    Robustness: For any given matrix norm |.|Î± there holds:     

(3)    Sign Stability: sgn(X*

|X* â€“ X|Î± â‰¤ 2Î· N(Î¦R
ij) = sgn(Xij) for (i,j) in R such that :     

*â€“1: l2â†’|.|Î±)                                                    (4.20) 

|Xij | > Î· ( N(Î¦R

*â€“1: l2â†’|||.|||max) + N(Î¦R

T: l2â†’|||.|||1

*)N((Î¦R

TÎ¦R)â€“1: |||.|||1

*â†’|||.|||max ))          (4.21) 

Proof    (1) Note that in case of X  âˆˆ âˆ‘ (ğ‘…)

  and y = Î¦(X) + e = Î¦R(X) + e, |e|2 â‰¤ Î·, we have 

ğ‘›Ã—ğ‘›
ğ‘ 
*â€“1(y) â€“ y = (Î¦RÎ¦R
    Î¦RÎ¦R

*â€“1 â€“ IR)e   

Itâ€™s  straightforward  to  verify  that  in  this  situation  condition  (3)  in  this  theorem  leads  to  condition  (3)  in 

*)) 

*â†’|||.|||1

TÎ¦R)â€“1Î¦R

*â€“1Î¦~R: |||.|||1

T) (1 â€“ N(Î¦R

TÎ¦R(Y*)  =  Î¦R

*â€“1(y) implies Î¦R

T(Î¦RÎ¦R
ğ‘›Ã—ğ‘›
ğ‘ 
ğ‘›Ã—ğ‘›
ğ‘ 

  and is l1-column-flat and the unique minimizer of MPy, Î¦, Î·. 

  and by lemma 4.2(4), we obtain  |X* â€“ Y*|Î± â‰¤ Î· N(Î¦R

*â€“1(y) â€“ y), H>: |||H|||1=1} < Î· Î›min(Î¦R
(ğ‘…)

lemma 4.2:    sup{<Î¦~R
for any Î·. As a result, X*âˆˆ âˆ‘
(2) For Y* = Î¦R
*â€“1(y)  âˆˆ âˆ‘ (ğ‘…)
given matrix norm |.|Î±. On the other hand, Y* = Î¦R
to  Î¦R(Y*)  =  y, hence  Î¦R(Y*)  =  y  =  Î¦(X) + e =  Î¦R(X) +  e,  namely  Î¦R
result:                                                  Y* â€“ X = (Î¦R
Since |e|2 â‰¤ Î·, we get |Y* â€“ X|Î± â‰¤ Î· N(Î¦R
Y*|Î± â‰¤ Î· N(Î¦R
(3)  By  the  first-order  optimization  condition  on  minimizer  X*  with  the  fact  supp(X*)  =  R,  we  have  the 
equation X* = Î¦R
                                                                  X* â€“ Y* = â€“ Î³*â€“1(Î¦R
Combining with (4.22), we get          X* â€“ X = Î¦R

*â€“1: l2â†’|.|Î±) for any 
T(Î¦R(Y*)â€“y) = O then condition (1) leads 
T(e),  as a 
*â€“1(e)                                          (4.22)   
*â€“1: l2â†’|.|Î±) for any given matrix norm |.|Î±. Combining with |X* â€“ 

*â€“1(e) â€“ Î³*â€“1(Î¦R
TÎ¦R)â€“1(M*)ij |, in particular, if 
ij| = |Î¦R
TÎ¦R)â€“1(M*)ij | then the former inequality is true and as 
ij)  =  sgn(Xij).  Itâ€™s  straightforward  to  verify  (by  using  (4.19))  that  the  condition  (4.21)  just 

Xij can satisfy |Xij| > maxij |Î¦R
a  result  sgn(X*
provides a guarantee for this.                                                                                                                        â–¡ 

TÎ¦R)â€“1(M*)                                              (4.23) 
TÎ¦R)â€“1(M*)                                        (4.24) 

ij) = sgn(Xij)    iff    |Xij| > |Xij â€“ X*
*â€“1(e) ij | +Î³*â€“1maxij | (Î¦R

*â€“1: l2â†’|.|Î±) we get the reconstruction error bound |X* â€“ X|Î± â‰¤ 2Î· N(Î¦R

TÎ¦R)â€“1(M*) where M* is in âˆ‚|||X*|||1, namely:   

TÎ¦R)â€“1(M*) = Y* â€“ Î³*â€“1(Î¦R

*â€“1(e) ij â€“ Î³*â€“1(Î¦R

*â€“1(y) â€“ Î³*â€“1(Î¦R

  Since sgn(X*

TÎ¦R(X) +Î¦R

*â€“1: l2â†’|.|Î±) 

T(e) â‰¡ Î¦R

Remark 4.2:  In  this  theorem  condition  (3)  is independent  with measurement  error  bound  Î·,  which  implies   
|X* â€“ X|Î± = O(Î·) can hold for any small value of Î·, and (4.21) indicates that with Î· small enough, all signalâ€™s 

14 

 
non-zero componentsâ€™ signs can be correctly recovered. For given Î¦, the associated value   
*â€“1: l2â†’|||.|||max) + N(Î¦S

T: l2â†’|||.|||1

  Max s-sparse pattern S N(Î¦S
or an enhancement 2Max s-sparse pattern S N(Î¦S
signal-to-noise ratio threshold for correct sign recovery.   

T: l2â†’|||.|||1

*)N((Î¦S

*)N((Î¦S
TÎ¦S)â€“1: |||.|||1

TÎ¦S)â€“1: |||.|||1
*â†’ |||.|||max ) can be regarded as a 

*â†’ |||.|||max )   

T)= 
T(z)|||1
            Finally we mention that in condition (3) Î›min(Î¦S
Y,A,B,Î· is one of critical quantities which value should be as large as possible 
inf{|||Î¦S
to  satisfy  this  condition.  Observe  that  there  is  a  counterpart  Î»min,Î± Î²(Î¦;K)=inf{|Î¦(Z)|Î±:  Z  in  K  and  |Z|Î²=1} 

*: |Z|F=1}for MP(F)

*: |z|2=1}for MP(2)

y,Î¦,Î· or Î›min(Î¦S

T)=inf{|||Î¦S

T(Z)|||1

(e.g., see FACT2.6) which is also critical for analysis in next sections in random measurement setting.   

5    Number of Measurements for Robust Reconstruction via Solving MP(2)

y, Î¦, Î·   

After  established  the  conditions  for  the  measurement  operator  to  guarantee  desired  properties  (e.g., 

uniqueness,  robustness,  etc.)  of  the  matrix  signal  reconstruction,  next  question  should  be  about  how  to 

construct  the  measurement  operator  to  satisfy  such  conditions  with  required  number  of  measurements  as 

few  as  possible.  This  and  next  sections  deal  with  this  problem  in  random  approach.  In  this  section  we 

establish  conditions  on  number  of  measurements  m  for  robustly  reconstructing  the  matrix  signal  X  by 
solving the convex programming problem MP(2)

y, Î¦, Î·:   

inf |||Z|||1    s.t Z âˆˆ ğ‘…ğ‘›Ã—ğ‘›, |y â€“ Î¦(Z)|2 â‰¤ Î·   
where  y  =  Î¦(X)  +  e  and  |e|2 â‰¤  Î·,  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š  is  a  linear  operator.  In  the  equivalent  component-wise 
formulation,  yi =  <Î¦i,X>  +  ei  where  for  i=1,â€¦,m,  Î¦iâˆˆ ğ‘…ğ‘›Ã—ğ‘›
  are  random  matrices  independent  each  other 
and Î¦iâ€™s entries are independently sampled under standard Gaussian N(0,1) or sub-Gaussian distribution. In 
section 5.3, a simple necessary condition on m is established.   

Instead of proving that M-RIP property (defined in sec. 3.5) can be satisfied with high probability in 

context of the distributions in consideration, which are quite involved particularly for problem MP(F)
Y,A,B,Î·, 
here we straightforwardly prove that the robustness in terms of Frobenius-norm error metric can be reached 

with  high  probability  when  number  of  measurements  exceeds  some  specific  bound.  The  investigation  on 

how those conditions established in last sections can be satisfied by the random measurement operator will 

be subjects in subsequent papers.   

5.1    Case 1: Gaussian Measurement Operator Î¦   

Based  upon  the  fundamental  facts  presented in  section  2.2,  one  of  the  critical  steps in this approach  is to 

estimate  the  width  w(D(|||X|||1,  X))â€™s  upper  bound  for  matrix  signal  X=(x1,â€¦,xn)  with  s-sparse  column 
vectors x1,â€¦,xn. This is done in lemma 5.1.     
          Based  on  lemma  3.1  and  FACT  2.4,  the  upper  bound  of  Gaussian  width  D(|||.|||1,  X)  with respect to 
Frobenius norm is estimated in the following lemma.   

Lemma  5.1    Given  n-by-n  matrix  X=(x1,â€¦,xn)  with  s-sparse  column  vectors  x1,â€¦,xn.  Let  r  (called 
l1-column-flatness  parameter  hereafter)  be  cardinality  of  the  set  {j:  |xj|1=maxk|xk|1},  i.e.,  the  number  of 
column vectors which have the maximum l1-norm. Then   

w2(D(|||.|||1, X)) â‰¤ 1+n2â€“r(nâ€“slog(Cn4r2))                                              (5.1) 

In particular, when r = n then   

where C is an absolute constant.   

w2(D(|||.|||1, X)) â‰¤ 1 + nslog(Cn6)                                                  (5.2) 

Remark 5.1: ns is the total sparsity of the matrix signal X. This estimate shows that the  signal complexity 

15 

 
(width) encoded by regularizer |||X|||1 is controlled by two structural parameters, the column-sparsity s and 
l1-column-flatness r. Complexity gets lower with smaller s and larger r.   

Proof    We  start  with  (FACT  2.4)  w2(D(|||.|||1,  X))  â‰¤  EG[inf{|Gâ€“tV|F
random matrix with entries Gij~iid N(0,1).   

2:  t>0,  V  in  âˆ‚|||X|||1}]  where  G  is  a 

Set G=(g1,â€¦,gn) where gj~iid N(0,In). By lemma 3.1, V=(Î»1Î¾1,â€¦, Î»nÎ¾n) where w.l.o.g. Î»jâ‰¥0 for j=1,â€¦,r, 
Î»1+â€¦+Î»r=1, Î»j=0 for jâ‰¥r+1; |xj|1=maxk|xk|1 for j=1,â€¦,r and |xj|1<maxk|xk|1 for jâ‰¥1+r; Î¾j(i)=sgn(Xij) for Xijâ‰ 0 
and |Î¾j(i)| â‰¤1 for all i and j. Then             

w2(D(|||.|||1, X)) â‰¤ EG[inf t>0, Î»j, Î¾j specified as the above  âˆ‘ğ‘Ÿ
ğ‘—=1
â‰¤ inf t>0, all Î»j specified as the above EG[inf all Î¾j specified as the above  âˆ‘ğ‘Ÿ
= inf t>0, all Î»j specified as the above EG[inf all Î¾j specified as the above  âˆ‘ğ‘Ÿ
= inf t>0, all Î»j specified as the above EG[âˆ‘ğ‘Ÿ

ğ‘—=1

ğ‘—=1

| gj â€“ tÎ»jÎ¾j |2

2    +  âˆ‘ğ‘›

| gj â€“ tÎ»jÎ¾j |2
| gj â€“ tÎ»jÎ¾j |2

ğ‘—=1
                                                      ( since Î¾j is unrelated each other and EG[|gj|2
        = inf t>0, all Î»j specified as the above  âˆ‘ğ‘Ÿ

ğ‘—=1 Egj[inf Î¾j specified as the above | gj â€“ tÎ»jÎ¾j |2

inf Î¾j specified as the above | gj â€“ tÎ»jÎ¾j |2

ğ‘—=ğ‘Ÿ+1

ğ‘—=ğ‘Ÿ+1
2    +  âˆ‘ğ‘›
2]+âˆ‘ğ‘›
2] + (nâ€“r)n 
2]=n )   
2] + (nâ€“r)n   

|gj|2

2]   
|gj|2

2 ]   

ğ‘—=ğ‘Ÿ+1 EG[|gj|2

2]     

2 + |gj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2

For each j=1,â€¦,r let S(j) be the support of xj (so |S(j)| â‰¤ s) and ~S(j) be its complimentary set, then |gj â€“
2. Notice that all components of Î¾j|S(j) are  Â±1 and all components 
2=|gj|S(j) â€“ tÎ»jÎ¾j|S(j)|2
tÎ»jÎ¾j|2
of Î¾j|~S(j)    can be any value in the interval [â€“1,+1]. Select Î»1=â€¦=Î»r =1/r, let Îµ>0 be arbitrarily small positive 
number and select t=t(Îµ) such that P[|g|>t(Îµ)/r]â‰¤Îµ where g is a standard scalar Gaussian random variable (i.e., 
g~N(0,1) and Îµ can be exp(â€“t(Îµ)2/2r2)). For each j and each i outside S(j), set Î¾jâ€™s component Î¾j(i) = rgj(i)/t(Îµ) 
if |gj(i)| â‰¤ t(Îµ)/r (in this case |gj(i) â€“ tÎ»jÎ¾j|(i)| = 0) and otherwise Î¾j(i)=sgn(gj(i)) (in this case |gj(i) â€“ tÎ»jÎ¾j|(i)| = 
2=0 when |gj|~S(j)|âˆ < t(Îµ)/r, hence:   
|gj(i)| â€“t(Îµ)/r), then |gj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2
âˆ
âˆ
2] =  âˆ« ğ‘‘ğ‘¢
E[ |gj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2
0
0

uP[ |gj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2>u]   

P[ |gj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2

2>u] = 2âˆ« ğ‘‘ğ‘¢

âˆ
â‰¤ 2 âˆ« ğ‘‘ğ‘¢
0

â‰¤ 2(nâ€“s) âˆ« ğ‘‘ğ‘¢

uP[There exists (gj|~S(j) â€“ tÎ»jÎ¾j|~S(j))â€™s component with magnitude > (nâ€“s)-1/2u ]   
âˆ
0

uP[ |g| â€“t(Îµ)/r > (nâ€“s)-1/2u ]   
uexp(â€“((t(Îµ)/r)+ (nâ€“s)-1/2u)2/2)   

âˆ
0

â‰¤ 2(n â€“ s) âˆ« ğ‘‘ğ‘¢
â‰¤ C0(nâ€“s)2exp(â€“t(Îµ)2/2r2 ) â‰¤ C0(nâ€“s)2Îµ   

where C0 is an absolute constant. On the other hand:   

            Egj[|gj|S(j) â€“tÎ»jÎ¾j|S(j)|2

2 = (1+ t(Îµ)2/r2)s=(1+2log (1/Îµ))s       
Hence          w2(D(|||.|||1, X)) â‰¤ (1+2log (1/Îµ))rs + (nâ€“r)n + r(nâ€“s)2Îµ â‰¤ n2 â€“ r(nâ€“slog(e/Îµ2)) + C0n2rÎµ   
In particular, let Îµ=1/C0n2r then we get w2(D(|||.|||1, X)) â‰¤ n2 â€“ r(n â€“ slog(Cn4r2)) + 1.                                  â–¡   

2] = Egj[|gj|S(j)|2] + (t(Îµ)2/r2)|Î¾j|S(j)|2

          Combing this lemma and FACT 2.3, we obtain the general result in the following:   

Theorem  5.1    Suppose  Î¦kij  ~iid N(0,1), let  Xâˆˆ âˆ‘ğ‘ 
signal,  ğ‘…ğ‘š âˆ‹y = Î¦(X) + e where |e|2 â‰¤ Î·, X* be the minimizer of the problem MP(2)
vector yâ€™s dimension   

ğ‘›Ã—ğ‘›  be  a  columnwise  s-sparse and  l1-column-flat matrix 
y, Î¦, Î·. If the measurement 

m â‰¥ (t + 2Î·/Î´ + (nslog(Cn6))1/2)2 
where  C is  an absolute  constant,  then  P[|X*â€“X|F    â‰¤ Î´  ]  â‰¥  1 â€“  exp(â€“t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*â€“X|F with high probability by solving MP(2)

y,Î¦,Î·.                        â–¡ 

5.2    Case 2: Sub-Gaussian Measurement Operator Î¦     

Combing lemma 5.1, FACT 2.1(2) and 2.5, the following result can be obtained straightforwardly:   
Theorem  5.2    Let  X  and  X*  be respectively  the  matrix  signal  and  the  minimizer  of  MP(2)
y, Î¦, Î·  where  the 
ğ‘›Ã—ğ‘›  is  columnwise  s-sparse  and  l1-column-flat,  yk=<Î¦k,X>+ek,  each  Î¦k  ~iid Î¦  where  Î¦  is  a 
signal  Xâˆˆ âˆ‘ğ‘ 
random  matrix  satisfying  the  conditions  (1)(2)(3)  in  FACT  2.5  with  parameters  Î±,  Ï  and  Ïƒ.  If  the 

16 

 
measurement vector yâ€™s dimension   

m â‰¥ (C1Ï4/Î±)( Î±t + 2Î·/Î´ + ÏƒC2(Ï6nslog(C3n6))1/2)2     
where  Ciâ€™s are  absolute  constants,  then  P[|X*â€“X|F â‰¤  Î´]  â‰¥  1 â€“  exp(â€“C4t2), i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*â€“X|F with high probability by solving MP(2)
y,Î¦,Î·.                          â–¡ 

5.3    Necessary Condition on Number of Measurements   

(Î¼)âˆ©Sj

(Î±)âˆªS2

(Î²)âˆªâ€¦âˆªSn

y,Î¦,Î· where Î·=0, then 

(Î¼)âˆ©Sj
(Î±)âˆªS2
Now we call the above S1

Theorem  5.3    Given  measurement  operator  Î¦:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘š,  if  any  X=(x1,â€¦,xn)  with  sparse  column 
vectors xj in âˆ‘2S for all j is always the unique solution to problem MP(Î±)
                                                                    m â‰¥ C1nslog(C2n/s))                                                              (5.3) 
where C1 and C2 are absolute constants.   
(Ï‰) in {(i,j): 1â‰¤i, jâ‰¤n } where 
Proof    For any s<n, there exist k â‰¥ (n/4s)ns/2 subsets S(Î±Î²â€¦Ï‰)=S1
(Î½)|  <  s/2  for  Î¼â‰ Î½.  This  fact  is  based  on  a 
(Î¼)={(i1,j),â€¦,  (is,j):  1â‰¤i1<i2<â€¦<  isâ‰¤n}  and  |Sj
each  Sj
combinatorial theorem[11] that for any s<n there exist l â‰¥ (n/4s)s/2 subsets R(Î¼) in {1,2,â€¦,n} where |R(Î¼)âˆ©R(Î½)| 
< s/2 for any Î¼â‰ Î½. For the n-by-n square {(i,j): 1â‰¤i, jâ‰¤n }, assign a R(Î¼) to each column, i.e., set Sj
(Î¼):={ (i, j): 
(Î½)| < s/2 for Î¼â‰ Î½ since |R(Î¼)âˆ©R(Î½)| < s/2 for Î¼â‰ Î½ and totally there can be k=ln such 
iâˆˆR(Î¼)}. As a result |Sj
(Î²)âˆªâ€¦âˆªSn
assignments S(Î±Î²â€¦Ï‰)=S1
(Î±)âˆªS2

(Ï‰) a configuration on the n-by-n square. Let m be the rank of 
linear operator Î¦. Consider the quotient space L:=ğ‘…ğ‘›Ã—ğ‘›/kerÎ¦, then dimL=n2â€“dimkerÎ¦=m. For any [X] in L 
define  the  norm  |[X]|:=inf{|||Xâ€“V|||1:  V  in  kerÎ¦}.  For  any  X=(x1,â€¦,xn)  with  xj  in  âˆ‘2S  for  all  j  ,  the 
assumption about Î¦ implies |[X]|=|||X|||1. Now for any configuration Î”=S1âˆªS2âˆªâ€¦âˆªSn on the n-by-n square, 
  and  0  otherwise,  then  |||X(Î”)|||1=1,  each  X(Î”)â€™s  column  xj(Î”)âˆˆâˆ‘S  and  each 
define  Xij(Î”):=1/s  if  (i,j)âˆˆSj
column of X(Î”â€™)â€“X(Î”â€) is in âˆ‘2S, furthermore |[X(Î”â€™)]â€“[X(Î”â€)]|=|||X(Î”â€™)â€“X(Î”â€)|||1>1 because of the property 
|Sjâ€™âˆ©Sjâ€|  <  s/2  for  Sjâ€™â‰ Sjâ€.  These  facts  imply  that  the  set  Î˜:={[X(Î”)]:  Î”  runs  over  all  configurations}  is  a 
subset on normed quotient space Lâ€™s unit sphere with distances between any pair of its members >1, i.e., a 
d-net  on  the  sphere  where  d>1.  The  cardinality  of  Î˜  =  number  of  configurations  k  â‰¥  (n/4s)ns/2  and  an 
elementary estimate derives k â‰¤ 3dimL=3m, hence m â‰¥ C1nslog(C2n/s)) where C1=1/2log3 and C2=1/4.        â–¡   

(Ï‰) on the square.         

(Î²)âˆªâ€¦âˆªSn

Remark 5.2: For the measurement operator Î¦A,B:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘šÃ—ğ‘š:Y=AXBT, the same result is true with m2 
â‰¥ C1nslog(C2n/s)).   

6    Number of Measurements for Robust Reconstruction via Solving MP(F)

Y, A, B, Î·   

Now we investigate the problem MP(F)
Y, A, B, Î· in which A and B are both Gaussian or sub-Gaussian m-by-n 
random  matrices.  Notice  that  in  these  cases  the  measurement  operator  Î¦A,B  is  neither  Gaussian  nor 
sub-Gaussian. The basis is FACT 2.6 and the critical step is also the widthâ€™s upper bound estimation.   

More  explicitly,  Î¦A,B:  ğ‘…ğ‘›Ã—ğ‘› â†’ ğ‘…ğ‘šÃ—ğ‘š :  Y=AZBT.  In  the  equivalent  component-wise  formulation, 
ykl=<Î¦kl,X>=âˆ‘ijAkiXijBlj  for  each  1â‰¤k,lâ‰¤m,  Aki  ~iidBlj ~iidN(0,1)  or  sub-Gaussian  distribution,  and  A,  B  are 
independent  each  other.  Let  Îµkl  ~iid  Rademacher  random  variable  Îµ  (P[Îµ= Â± 1]=1/2)  which  are  also 
independent of A and B. The width is defined as (FACT 2.6)   

                                      W(Î“X; Î¦A,B):=EH[sup{<H,U>: U in Î“X and |U|F=1]                                        (6.1) 
where                                              H:=m-1âˆ‘klÎµklÎ¦kl =m-1ATEB and E=[Îµkl]                                                (6.2)   
and Î“X = D(|||.|||1,X) in our applications.   

Let  Îµl:  l=1,â€¦,m  be  column  vectors  of  Rademacher  matrix  [Îµkl],  then  H=(h1,â€¦,hn)  has  its  column 

vectors hj=âˆ‘ ğ‘šâˆ’1

ğ‘š
ğ‘™=1

BljATÎµl  âˆˆ ğ‘…ğ‘›. Notice that in problem MP(F)

Y, A, B, Î·, m2 is the measurement dimension. 

17 

 
6.1    Case 1: A and B are both Gaussian     

Lemma 6.1 Given n-by-n matrix X=(x1,â€¦,xn) with s-sparse column vectors x1,â€¦,xn, r is cardinality of {j: 
|xj|1=maxk|xk|1}, Î¦A,B, Î“X, W(Î“X; Î¦A,B) are specified as the above and Aki ~iidBlj ~iidN(0,1), then   

W2(Î“X; Î¦A,B) â‰¤ 1+n2â€“r(nâ€“slog2(cn2r))   

where c is an absolute constant. Particularly, when r = n ( i.e., X is sparse and l1-column-flat ) then   
                                                                W2(Î“X; Î¦A,B) â‰¤ 1 + nslog2(cn3)                                                  (6.3) 

Proof    We  start  with  a  similar  inequality  as  that  in  FACT  2.4  (the  proof  is  also  similar)  W2(Î“X;  Î¦A,B)  â‰¤ 
2: t>0,  V  in âˆ‚|||X|||1}].  With the  same  specifications  for  V=(Î»1Î¾1,â€¦,  Î»nÎ¾n) as  those  in lemma 
EH[inf{|Hâ€“tV|F
5.1,  i.e.(w.l.o.g.)  Î»j  â‰¥  0  for  j=1,â€¦,r,  Î»1+â€¦+Î»r=1,  Î»j=0  for  jâ‰¥r+1;  |xj|1=maxk|xk|1  for  j=1,â€¦,r  and  |xj|1  < 
maxk|xk|1 for jâ‰¥1+r; Î¾j(i)=sgn(Xij) for Xijâ‰ 0 and |Î¾j(i)| â‰¤1 for all i and j. Let hj â‰¡  âˆ‘ ğ‘šâˆ’1
2]   

W2(Î“X; Î¦A,B) â‰¤ EA,B,E[inf t>0, Î»j, Î¾j specified as the above  âˆ‘ |

ğ‘š
ğ‘™=1

BljATÎµl , we have             

ğ‘—=1 âˆ‘ğ‘š
ğ‘›
2] + EA,B,E[inf t>0, Î»j, Î¾j specified as the above  âˆ‘

ğ‘™=1 m-1BljATÎµl â€“ tÎ»jÎ¾j |2
2]   
ğ‘Ÿ
ğ‘—=1   hj â€“ tÎ»jÎ¾j |2

|

=  âˆ‘ğ‘›

ğ‘—=ğ‘Ÿ+1 EA,B,E[| hj |2

= I + II 

The first and second terms are estimated respectively. The first term 
I =  âˆ‘ğ‘›

TAATÎµk] = m-2(nâ€“r) âˆ‘ğ‘š
To  estimate  II,  for  each  j=1,â€¦,r  let  S(j)  be  the  support  of  xj  (so  |S(j)|  â‰¤  s)  and  ~S(j)  be  its 

ğ‘™,ğ‘˜=1 EB[BljBkj]EA,E[Îµl

TAATÎµl] = (nâ€“r)n       

ğ‘—=ğ‘Ÿ+1 m-2âˆ‘ğ‘š

Î´lkEA,E[Îµl

ğ‘™,ğ‘˜=1

complimentary set, then 

âˆ‘

ğ‘Ÿ
ğ‘—=1   hj â€“ tÎ»jÎ¾j |2

|

2] =  âˆ‘ |

ğ‘Ÿ
ğ‘—=1   hj |S(j)    â€“ tÎ»jÎ¾j|S(j) |2

2] +  âˆ‘ |

ğ‘Ÿ
ğ‘—=1   hj |~S(j) â€“ tÎ»jÎ¾j|~S(j) |2

2]   

Notice  that  all  components  of  Î¾j|S(j)  are  Â±1  and  all  components  of  Î¾j|~S(j)    can  be  any  value  in  the 
interval  [â€“1,+1].  Select  Î»1=â€¦=Î»r  =1/r,  let Î´>0  be  arbitrarily  small  positive  number and  select  t=t(Î´)  such 
that  PA,B,E[|h|  > t(Î´)/r]  â‰¤ Î´  where  h is  a random  scalar  such  that  hj(i)~h and i indicates  the  vector  hjâ€™s  i-th 
component.  For  each j  and  i  outside  S(j),  set  Î¾jâ€™s  component Î¾j(i)=rhj(i)/t(Îµ) if  |hj(i)|  â‰¤ t(Î´)/r and  otherwise 
2=0  when  |hj|~S(j)|âˆ <  t(Î´)/r  and  notice  the  fact  that  for  independent 
Î¾j(i)=sgn(hj(i)),  then  |hj|~S(j) â€“  tÎ»jÎ¾j|~S(j)|2
standard  scalar  Gaussian  variables  al,  bl  and  Rademacher  variables  Îµl,  l=1,â€¦,m,  there  exists  absolute 
constant c such that for any Î· > 0:   

                                              P[| m-1âˆ‘ğ‘š

ğ‘™,ğ‘˜=1

blakÎµk | > Î·] < c exp(â€“Î·)                                            (6.4) 

as a result, in the above expression Î´ can be c exp(â€“t(Î´)/r) and:       
âˆ
2 ] =  âˆ« ğ‘‘ğ‘¢
0

E[ |hj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2

P[ |hj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2

2 > u] = 2âˆ« ğ‘‘ğ‘¢

âˆ
0

        â‰¤ 2 âˆ« ğ‘‘ğ‘¢

âˆ
0

â‰¤ 2(nâ€“s) âˆ« ğ‘‘ğ‘¢

uP[There exists (hj|~S(j) â€“ tÎ»jÎ¾j|~S(j))â€™s component with magnitude > (nâ€“s)-1/2u ] 
âˆ
0

uP[ |h| â€“t(Î´)/r > (nâ€“s)-1/2u ]   
uexp(â€“((t(Î´)/r)+ (nâ€“s)-1/2u))       

âˆ
0

â‰¤ 2(n â€“ s) âˆ« ğ‘‘ğ‘¢
â‰¤ C0(nâ€“s)2exp(â€“(t(Î´)/r)) â‰¤ C0(nâ€“s)2Î´   

uP[ |hj|~S(j) â€“ tÎ»jÎ¾j|~S(j)|2 > u]   

where C0 is an absolute constant. On the other hand |Î¾j|S(j)|2

2

 â‰¤ s for j â‰¥ 1+r so:     

ğ‘Ÿ
ğ‘—=1   hj |S(j)    â€“ tÎ»jÎ¾j|S(j) |2

2]   

ğ‘Ÿ
ğ‘—=1   hj |S(j)    â€“ t(Î´)Î¾j|S(j)/r |2

2]   

EA,B,E[inf t>0, Î»j, Î¾j  âˆ‘ |
|
ğ‘—=1 EA,B,E[ m-2| âˆ‘ğ‘š

â‰¤ EA,B,E[ âˆ‘
â‰¤  âˆ‘ğ‘Ÿ
= rs(1+t(Î´)2/r2)   

ğ‘™=1 Blj(ATÎµl)|S(j) |2

2 ] + rst(Î´)2/r2       

hence II â‰¤ rs(1+t(Î´)2/r2) + nrÎ´. Combine all the above estimates we have:   

W2(Î“X; Î¦A,B) â‰¤ I + II â‰¤ (nâ€“r)n + rs(1+t(Î´)2/r2) + C0n2rÎ´ = n2 â€“ r(nâ€“s(1+t(Î´)2/r2) + C0n2rÎ´   
Substitute t(Î´)/r with log(c/Î´) we get, for any Î´ > 0:   

W2(Î“X; Î¦A,B) â‰¤ n2â€“r(nâ€“s(1+log2(c/Î´)) + C0n2rÎ´   
In particular, let Î´=1/C0n2r then W2(Î“X; Î¦A,B) â‰¤ n2â€“r(nâ€“s(1+log2(cn2r)) + 1.                                                â–¡ 

To apply FACT 2.6 completely, now estimate the lower bound of QÎ¾(Î“X; Î¦A,B):= inf {P[|<Î¦kl,U>| â‰¥ Î¾]: 

18 

 
U in Î“X and |U|F=1} for some Î¾ > 0 where <Î¦kl,U>=âˆ‘ijAkiUijBlj for each 1â‰¤k,lâ‰¤m, Aki ~iidBlj ~iidN(0,1). The 
estimate is independent of the indices k and l, so we give a general and notational simplified statement on it.     

Lemma  6.2    Let  Mij =  aibj  where  ai ~iidbj ~iidN(0,1) and all random  variables are independent  each  other, 
there exists an positive absolute constant c such that inf {P[|<M,U>| â‰¥ 1/âˆš2]: U in Î“X and |U|F=1} â‰¥ c, as a 
result  ğ‘„1/âˆš2(Î“X; Î¦A,B) â‰¥ c.     

Proof    By the second moment inequality P[Z â‰¥ Î¾] â‰¥ (E[Z] â€“ Î¾) +
Î¾ > 0. Set Z = |<M,U>|2 and Î¾ = E[|<M,U>|2]/2, we get: 

2/E[Z2] for any non-negative r.v. Z and any 

P[|<M,U>|2 â‰¥ E[|<M,U>|2]/2] â‰¥ E[|<M,U>|2]2/4E[|<M,U>|4]                (6.5) 

To  estimate  the  upper  bound  of  E[|<M,U>|2],  let  U=âˆ‘jÎ»jujvj  be  Uâ€™s  singular  value  decomposition, 
Tvj=Î´ij,  Î»j>0  for  each  j.  Notice  that  M=abT  where  a~b~N(0,  In)  and  independent  each  other,  then 
Tb~N(0,1)  and  independent  each  other,  hence  E[|<M,U>|2]  = 

Tuj=vi

ui
<M,U>  =  aTUb  =  âˆ‘jÎ»jaTujvj
Tb|2] = âˆ‘jÎ»j
âˆ‘jÎ»j

2E[|aTuj|2]E|vj

Tb where  aTui~vj
2 = |U|F

2=1 for U in the assumption. 
On the other hand by Gaussian hypercontractivity we have 

In conclusion P[|<M,U>|2 â‰¥ 1/2] = P[|<M,U>|2 â‰¥ E[|<M,U>|2]/2] â‰¥ c for U: |U|F

2=1.      â–¡ 

(E[|<M,U>|4])1/4 â‰¤ C0(E[|<M,U>|2])1/2 = C0   

Combing lemma 6.1, 6.2 and FACT 2.6, we obtain the general result in the following.   

Theorem 6.1    Suppose Aki ~iidBlj ~iidN(0,1) and independent each other, Xâˆˆ âˆ‘ğ‘ 
and l1-column-flat signal, Y = AXBT + E  âˆˆ ğ‘…ğ‘šÃ—ğ‘š  with measurement errors bounded by |E|F
minimizer of the problem MP(F)

ğ‘›Ã—ğ‘›  is a columnwise s-sparse 
2 â‰¤ Î·, X* is the 

Y, A,B, Î·. If   

m â‰¥ t + 4âˆš2Î·/Î´ + C1(ns)1/2log(C2n3) 
where  Ciâ€™s  are  absolute  constants,  then  P[|X*â€“X|F â‰¤  Î´]  â‰¥  1  â€“  exp(â€“t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*â€“X|F with high probability by solving MP(F)
Y,A,B,Î·.                      â–¡ 

6.2    Case 2: A and B are both sub-Gaussian 

Lemma 6.3    Given n-by-n matrix X=(x1,â€¦,xn) with s-sparse column vectors x1,â€¦,xn, r is cardinality of {j: 
|xj|1=maxk|xk|1}, Î¦A,B, Î“X, W(Î“X; Î¦A,B) are specified as before, Aki ~iid Sub-Gaussian distribution and Blj ~iid 
Sub-Gaussian distribution with Ïˆ2-norms ÏƒA, ÏƒB respectively, then   

W2(Î“X; Î¦A,B) â‰¤ ÏƒA

2ÏƒB

2(1+n2â€“r(nâ€“slog2(Cn2r)))                                        (6.6) 

where C is an absolute constant. Particularly, when r = n then   

W2(Î“X; Î¦A,B) â‰¤ ÏƒA

2ÏƒB

2(1 + nslog2(Cn3))                                            (6.7) 

The proof  of this lemma is logically the same as the proof of lemma 6.1, the only difference is about 
ğ‘š
the distribution tail of the components of vectors hj â‰¡  âˆ‘ ğ‘šâˆ’1
blakÎµk with 
ğ‘™=1
independent scalar sub-Gaussian variables al, bl and Rademacher variables Îµl, l=1,â€¦,m. This auxiliary result 
is presented in the following lemma:   

BljATÎµl which ~iid h â‰¡ m-1âˆ‘ğ‘š

ğ‘™,ğ‘˜=1

Lemma 6.4    For independent scalar zero-mean sub-Gaussian variables al, bl and Rademacher variables Îµl, 
l=1,â€¦,m, let ÏƒAâ‰¡maxl|al|Ïˆ2, ÏƒBâ‰¡maxl|bl|Ïˆ2 (|.|Ïˆ2 denotes a sub-Gaussian variableâ€™s Ïˆ2-norm), then there exists 
absolute constant c such that for any Î· > 0:   

                                                P[| h | > Î·] < 2 exp(â€“cÎ·/ÏƒAÏƒB)                                                      (6.8)   

Proof    Notice  that  akÎµk  is  zero-mean  sub-Gaussian  variable  with  |akÎµk|Ïˆ2=|ak|Ïˆ2,  for  b=m-1/2âˆ‘1â‰¤lâ‰¤mbl  and 
a=m-1/2âˆ‘1â‰¤kâ‰¤makÎµk we have |b|Ïˆ2 â‰¤ Cm-1/2(âˆ‘l|bl|Ïˆ2
2)1/2 â‰¤ CÏƒA where C is an 
absolute  constant.  Furthermore,  because  the  product  of  two  sub-Gaussian  variables  a  and  b  is 
19 

2)1/2 â‰¤ CÏƒB and |a|Ïˆ2 â‰¤ Cm-1/2(âˆ‘l|ak|Ïˆ2

 
sub-Exponential and its Ïˆ1-norm |ba|Ïˆ1 â‰¤ |b|Ïˆ2 |a|Ïˆ2 â‰¤ C2ÏƒAÏƒB, h â‰¡ m-1âˆ‘ğ‘š
tail P[| h | > Î·] < 2exp(â€“cÎ·/ÏƒAÏƒB) where c is an absolute constant.   

ğ‘™,ğ‘˜=1

blakÎµk = ab has its distribution 

Proof  of  lemma  6.3    With  the  same  logic  as  in  the  proof  of  lemma  6.1  and  based-upon  lemma  6.4,  the 
auxiliary  parameter  Î´  in  the  argument  can  be  2exp(â€“ct(Î´)/rÏƒAÏƒB)  and  equivalently  t(Î´)/r  =  ÏƒAÏƒBlog(2/Î´) 
which derives the final result.    â–¡     

Lemma 6.5    Let Mij = aibj where ai ~iid Sub-Gaussian distribution, bj ~iid Sub-Gaussian distribution and all 
are independent each other, then  ğ‘„1/âˆš2(Î“X; Î¦A,B) â‰¥ c where c is a constant only dependent on ÏƒAÏƒB.          â–¡ 
        Lemma 6.5â€™s proof is almost the same as that of lemma 6.2. Now the following result can be obtained 

directly by combing lemmas 6.3-6.5 and FACT 2.6:   

Theorem 6.2    Suppose random matrices A, B are independent each other, Aki ~iid Sub-Gaussian distribution, 

Blj ~iid Sub-Gaussian distribution, each with Ïˆ2-norm ÏƒA and ÏƒB. Let Xâˆˆ âˆ‘ğ‘ 
l1-column-flat signal, Y = AXBT + E  âˆˆ ğ‘…ğ‘šÃ—ğ‘š with |E|F

2 â‰¤ Î·, X* be the minimizer of MP(F)

Y, A,B, Î·. If 

ğ‘›Ã—ğ‘›  be a columnwise s-sparse and 

                                                        m â‰¥ t + 4âˆš2Î·/Î´ + C1ÏƒAÏƒB(ns)1/2log(C2n3)   
where  Ciâ€™s  are  absolute  constants,  then  P[|X*â€“X|F â‰¤  Î´]  â‰¥  1  â€“  exp(â€“t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*â€“X|F with high probability by solving MP(F)
Y,A,B,Î·.                      â–¡ 

7    Conclusions, Some Extensions and Future Works   

In  this  paper  we  investigated the  problem  of  reconstructing  n-by-n  column-wise  sparse and  l1-column-flat 
matrix  signal  X=(x1,â€¦,xn)  via  convex  programming  with  the  regularizer  |||X|||1:=maxj|xj|1  where  |.|1  is  the 
l1-norm  in  vector  space.  In  the  first  part  (sec.3  and  sec.4),  the  most  important  conclusions  are  about  the 
general  conditions to  guarantee  uniqueness,  value-robustness,  support  stability  and  sign  stability  in  signal 

reconstruction.  In  the  second  part  (sec.5  and  sec.6)  we  took  the  convex  geometric  approach  in  random 

measurement  setting  and  established  bounds  on  dimensions  of  measurement  spaces  for  robust 

reconstruction in noise. For example, typical results show that the signal complexity (width) encoded by the 

regularizer |||X|||1 is determined by two structural parameters, i.e., the maximum number s of nonzero entries 
in each column (column-wise sparsity )and the number r of columns which l1-norms are maximum among 
all columns (l1-column-flatness). The signalâ€™s complexity decreases  with small s and large r. In particular, 
when r = n (i.e., sparse and flat signal) the condition reduces to m â‰¥ t + 4âˆš2Î·/Î´ + C1ÏƒAÏƒB(ns)1/2log(C2n3).   
        Based on the methods and results obtained in this paper, we can make some straightforward extensions 

to other similar problems. The first extension is about reconstructing row-wise sparse and l1-row-flat matrix 
signals via convex programming with regularizer |||XT|||1:= maxi|ğ’™ğ’Š|1 where  ğ’™ğ’Š  is matrix signal Xâ€™s i-th row. 
In  this  case  all  the  obtained  estimates  and  conclusions  remain  the  same,  e.g.,  the  signalâ€™s  complexity  is 

determined  by  the  maximum  number  s  of  nonzero  entries  in  each  row  and  the  number  r  of  rows  which 

l1-norms are maximum among all rows. The signalâ€™s complexity decreases with small s and large r.     

The second extension is for reconstructing the matrix signal which has both row-wise and column-wise 
sparsity  and  l1-flatness.  We  can  use  F(X):=max(|||X|||1,|||XT|||1)  as  the  regularizer.  Note  that  when  |||X|||1 â‰¥ 
|||XT|||1 then F(X) = |||X|||1 so in this case W(Î“X; Î¦) is the width determined by regularizer |||X|||1, otherwise 
W(Î“X;  Î¦)  is  the  width  determined  by  regularizer  |||XT|||1,  both  have  the  upper  bound  of  ğ‘Š(r1,s1,n)  and 
ğ‘Š(r2,s2,n) in the form as that in lemma 5.1 or lemma 6.1 where (r1,s1) and (r2,s2) are the matrix signalâ€™s row 
and  column  structural  parameters.  As  a  result,  the  sufficient  condition  for  robust  reconstruction  is  m  â‰¥ 
max(ğ‘Š2(r1,s1,n),  ğ‘Š2(r2,s2,n)) via MPy,Î¦,Î· or m2 â‰¥ max(ğ‘Š2(r1,s1,n),  ğ‘Š2(r2,s2,n)) via MPY,A,B,Î· .     

20 

 
The  third  extension  is  for  reconstructing  the  matrix  signal  with  the  general  linear  measurement  Y  = 
  where  HÎ¼  := 
ğ‘‹ğµğœ‡
(Î¼)] is the matrix with independent Rademacher entries.  Suppose matrices AÎ¼â€™s and 

Î¦A,B(X)  +  E  =  âˆ‘
m-1AÎ¼
BÎ¼â€™s are both Sub-Gaussian, in this case (see (4.1)) the width   

ğ‘‡  +  E  where  |E|Fâ‰¦Î·.  In  this  case  (see  (6.2))  H  =  âˆ‘

ğ´ğœ‡
TEÎ¼BÎ¼ and EÎ¼ = [Îµkl

ğ¿
ğœ‡=1

ğ¿
ğœ‡=1

ğ»ğœ‡

  W(Î“X; Î¦A,B) = EH[sup{âˆ‘ğ¿
â‰¤ C0L1/2 maxÎ¼EH[sup{<HÎ¼,U>: U in Î“X and |U|F=1]       

ğœ‡=1 <HÎ¼,U>: U in Î“X and |U|F=1]   

2(n2â€“r(nâ€“slog2(C1n2r)) 
2ÏƒB
and each EH[sup{|<HÎ¼,U>|2: U in Î“X and |U|F=1] has the upper bound estimate ÏƒA
according to lemma 4.3, finally we get the width estimate W2(Î“X; Î¦A,B) â‰¤C0LÏƒA
2(n2â€“r(nâ€“slog2(C1n2r)). As 
2ÏƒB
a result, a  sufficient  condition  for  column-wise  sparse and l1-column-flat  signal  X to  be  reconstructed  via 
solving  MP(F)
Y,A,B,Î·  robustly  with  respect  to  Frobenius  norm  |.|F  from  linear  measurement  Y  = 
âˆ‘
ğ´ğœ‡
This paper is only focused on basic theoretical analysis. In subsequent papers, the algorithms to solve 

ğ‘‡+E where |E|Fâ‰¦Î· is m â‰¥ t + 4âˆš2Î·/Î´ + C1LÏƒAÏƒB(ns)1/2log(C2n3).   

ğ‘‹ğµğœ‡

ğ¿
ğœ‡=1

the  |||.|||1-optimization  problems  (e.g.,  generalized  inverse  scale  space  algorithms,  etc.),  related  numeric 
investigations and applications (e.g.,in radar space-time waveform analysis)will be further investigated[14-15].     

REFERENCES     

[1]S.Foucart, H.Rauhut. A Mathematical Introduction to Compressive Sensing, Birkhaeusser, 2013. 

[2]Y.C.Eldar, G.Kutynoik(Ed). Compressed Sensing: Theory and Applications, Cambridge University Press,, 2012. 

[3]D.Cohen, Y.C.Eldar. Sub-Nyquist Radar Systems: Temporal, Spectral and Spatial Compression, IEEE Signal Processing Magazine, 

2018 Nov.,35-57. 

[4]M.A.Davenport, J.Romberg. An Overview of Low-Rank Matrix Recovery from Incomplete Observations, arXiv:1601.06422, 2016. 

[5]M.F.Duarte, R.G.Baraniuk. Kronecker Compressive Sensing, IEEE Transactions on Image Processing, 2012, 21(2):494-504. 

[6]G.Dasarathy, P.Shah, B.N.Bhaskar, R.Nowak. Sketching Sparse Matrices, arXiv:1303.6544, 2013. 

[7]V.Chandrasekaran, B.Recht, P.A.Parrio and A.S.Wilsky. The Convex Geometry of Linear Inverse Problems, Found. Comput. Math., 

2012, 12:805-849. 

[8]J.A.Tropp.  Convex  Recovery  of  a  Structured  Signal  from  Independent  Random  Linear  Measurements,  In:  Sampling  Theory:  A 

Renaissance: Compressive Sampling and Other Developments, Ed. by G.Pfander, Birkhaeusser, 2015. 

[9]S.Mendelson. Learning without Concentration. J. ACM., 2014, 62(3).   

[10]S.Mendelson,  A.Pajor,  N.Tomczak-Jaegermann.  Reconstruction  and  Subgaussian  Operators  in  Asymptotic  Geometric  Analysis, 

Geom. Func. Analysis, 2007, 17(4):1248-1282.   

[11]J.H.Van Lint, R.M.Wilson. A Course in Combinatorics, Springer-Verlag, 1995.   

[12]R.Vershynin. High-Dimensional Probability â€“ with Applications to Data Scienceï¼ŒOxford University Pressï¼Œ2015.     

[13]M.Ledoux, M.Talagrand. Probability in Banach Space: Isopermetry and Processes, Springer, 1991.   

[14]Yuan T, Convex Reconstruction of Structured Matrix Signals from Linear Measurements(II): Algorithms, to appear.   

[15]Yuan T, Convex Reconstruction of Structured Matrix Signals from Linear Measurements(III): Applications, to appear.   

21 

 
 
 
 
 
