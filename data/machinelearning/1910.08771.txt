Convex Reconstruction of Structured Matrix Signals   

from Linear Measurements (I): Theoretical Results     

Yuan Tian     

Software School, Dalian University of Technology, Dalian, P.R.China, 116620   

12 October, 2019   

Abstract    We investigate the problem of reconstructing n-by-n structured matrix signal X=(x1,…,xn) 
via  convex  programming,  where  each  column  xj is  a  vector  of  s-sparsity  and  all  columns  have  the  same 
l1-norm. The regularizer is matrix norm |||X|||1:=maxj|xj|1. The contribution in this paper has two parts. The 
first  part  is  about  conditions  for  stability  and  robustness  in  signal  reconstruction  via  solving  the  inf-|||.|||1 
convex  programming  from  noise-free  or  noisy  measurements.  We  establish  uniform  sufficient  conditions 

which are very close to necessary conditions and non-uniform conditions are also discussed. Similar as the 

inf-l1  compressive  sensing  theory  for  reconstructing  vector  signals,  a  |||.|||1-version  RIP  condition  is 
established. In addition, stronger conditions are investigated to guarantee the reconstructed signal’s support 

stability, sign stability and approximation-error robustness(e.g., with linear convergence rate relative to any 

matrix norm). The second part is about conditions on number of measurements for robust reconstruction in 

noise.  We  take  the  convex  geometric  approach  in  random  measurement  setting  and  one  of  the  critical 

ingredients in this approach is to estimate the related widths’ bounds in case of Gaussian and non-Gaussian 

distributions.  These  bounds  are  explicitly  controlled  by  signal’s  structural  parameters  r  and  s  which 

determine matrix signal’s column-wise sparsity and l1-column-flatness respectively.   
Keywords    Compressive Sensing, Structured Matrix Signal, Convex Optimization, Column-wise Sparsity, 

Flatness, Sign-Stability, Support-Stability, Robustness, Random Measurement.   

1    Introduction 

Compressive  sensing  develops  effective  methods  to  reconstruct  signals  accurately  or  approximately  from 

accurate  or  noisy  measurements  by  exploiting  a  priori  knowledge  about  the  signal,  e.g.,  the  signal’s 
structural  features[1-2].  So  far  in  most  investigations  the  signals  are  modeled  as  vectors  of  high  ambient 
dimension.  However,  there  are  lots  of  applications  in  which  the  signals  are  naturally  matrices  or  even 
tensors of high orders. For example, in modern radar  systems, e.g., MIMO radar [3], the measurements can 
be  naturally  represented  as  ykl =  ∑ijΦkl,ijXij +  ekl  where  each  ykl  is  the  echo  sampled  at  specific  time  k  and 
specific receiver element l in a linear or planar array; Φkl,ij is the coefficient of a linear processor determined 
by  system  transmitting/receiving  and  waveform  features;  ekl  is  the  intensity  of  noise  and  clutter;  Xij,  if 
nonzero,  is  the  scattering  intensity  of  a  target  detected  in  specific  state  cell  (i,j),  e,g,  a  target  at  specific 

distance and radial speed, or at specific distance and direction, etc. In another class of applications related to 

signal  sampling/  reconstruction,  multivariable  functions  (waveforms)  in  a  linear  space  spanned  by  given 

basis,  e.g.,  {ψμ(u)φν(v)}μ,ν,  are  sampled  as  s(ui,vj)=∑μν  ψμ(ui)φν(vj)χμ,ν  where  χμ,ν  are  the  signal’s  Fourier 
coefficients  to  be  recovered  from  the  samples  {s(ui,vj)}.  These  are  typical  examples  to reconstruct  matrix 
signals and many of them can be naturally extended to even more general tensor signal models.   
          So  far  typical  works  on  matrix  signal  compressive  sensing  include  low-rank  matrix  recovery[1,4], 

1 

 
 
 
 
 
matrix  completion,  Kronecker  compressive  sensing[5-6],  etc.  Low-rank  matrix  recovery  deals  with  how  to 
reconstruct the matrix signal with sparse singular values from linear measurements using nuclear norm (sum 

of  singular  values)  as  the  regularizer,  Kronecker  compressive  sensing  deals  with  how  to  reconstruct  the 

matrix  signal  from  matrix  measurements  via  matrix  L1-norm  ∑ij|Xij|  as  the  regularizer,  dealing  with  the 
measurement operator in tensor-product form.   

Matrix signals can have richer and more complicated structures than vector signals. When solving the 

reconstruction  problem  via  convex  programming,  it  is  important  to  select  the  appropriate  matrix  norm  or 

regularizer for specific signal structure. For example, L1-norm is suitable for general sparsity, nuclear norm 
is  suitable  for  singular-value-sparsity,  and  other  regularizers  are  needed  for  more  special  or  more 

fine-grained structures, e.g., column-wise sparsity, row-wise sparsity or some hybrid structure. Appropriate 

regularizer determines the reconstruction’s performance.   

Contributions and Paper Organization    In this paper we investigate the problem of reconstructing 

n-by-n  matrix  signal  X=(x1,…,xn)  by  convex  programming.  Signal’s  structural  features  in  concern  are 
sparsity  and  flatness,  i.e.,  each  column  xj is  a  vector  of  s-sparsity  and  all  columns have  the  same  l1-norm. 
Such  signals  naturally  appear  in  some  important  applications,  e.g.,  radar  waveform  space-time  analysis,   

which will be investigated as an application in subsequent papers. The regularizer to be used is matrix norm 

|||X|||1:=maxj|xj|1 where |.|1 is the l1-norm on column vector space.   

The  contribution  in  this  paper has  two  parts. The  first  part (sec.3  and  sec.4)  is about  conditions  for 

stability  and  robustness  in  signal  reconstruction  via  solving  the  inf-|||.|||1  convex  programming  from 
noise-free or noisy measurements. In sec. 3 we establish uniform sufficient conditions which are very close 

to  necessary  conditions  and  non-uniform  conditions  are  also  discussed.  Similar  as  the  inf-l1  compressive 
sensing theory for reconstructing vector signals, a |||.|||1-version RIP condition is investigated (theorem 3.5). 
In  sec.4  stronger  conditions  are  established  to  guarantee  the  reconstructed  signal’s  support  stability,  sign 

stability  and  approximation-error  robustness.  For  example,  linear  convergence  rate  relative  to  any  matrix 

norm metric is established under a general condition in Theorem 4.1.             

The  second  part  in  our  work  (sec.5  and  sec.6)  is  to  establish  conditions  on  number  of  linear 
measurements  for  robust  reconstruction  in  noise.  We  take  the  convex  geometric  approach [7-10]  in  random 
measurement  setting  and  one  of  the  critical  ingredients  in  this  approach  is  to  estimate  the  related  widths’ 

bounds  incase  of  Gaussian  and  non-Gaussian  distributions.  These  bounds  are  explicitly  controlled  by 

signal’s  structural  parameters  r  and  s  which  determine  matrix  signal’s  column-wise  sparsity  and 

l1-column-flatness respectively(e.g., lemma 5.1, 6.1 and 6.3).   

Foundations  for  works  in  the  first  part  is  mainly  general  theory  on  convex  optimization(e.g., 

first-order  optimization  conditions)  in  combination  with  the  information  on  subdifferential  ∂|||X|||1,  while 
foundations for the second part is mainly general theory on high-dimensional probability with some recent 

deep  extensions.  This  paper  is  only  focused  on  theoretical  analysis.  Algorithms,  numerical  investigations 

and applications will be the subjects in subsequent papers.   

2    Basic Problems, Related Concepts and Fundamental Facts         

Conventions and Notations: In this paper we only deal with vectors and matrices in real number field and 

only  deal  with  square  matrix  signals  for  notation  simplification,  but  all  results  are  also  true  for  rectangle 
matrix  signals in  complex  field.  Any  vector  x is regarded  as  column  vector,  xT  denotes  its transpose  (row 
vector).  For  a pair  of  vectors  x  and  y,  <x,y>  denotes their  scalar  product.  For  a  pair  of  matrices  X  and  Y, 
<X,Y> denotes the scalar product tr(XTY). In particular, the Frobenius norm <X,X>1/2 is denoted as |X|F.   

For  a  positive  integer  s,  ∑𝑛×𝑛

𝑠

denotes  the  set  of  n-by-n  matrices  which  column  vectors  are  all  of 

2 

 
sparsity s, i.e., the number of non-zero components of each column vector is at most s. Let S ≡ S1∪ … ∪Sn 
be  a  subset  of  {(i,j):  i,j=1,…,n}  where  each  Sj  is  a  subset  of  {(i,j):  i=1,…,n}  and  its  cardinality  |Sj|  ≤  s, 
𝑛×𝑛
∑ (𝑆)
  denotes  the  set  of  n-by-n  matrices  {M:  Mij=0  if  (i,j)  not  in  S}.  S  is  called  the  matrix  signal’s 
𝑠
s-sparsity pattern. Obviously  ∑ (𝑆)

  is a linear space for given S and  ∑

𝑛×𝑛
    ∪𝑆 ∑ (𝑆)
𝑠

=𝑛×𝑛
𝑠

𝑛×𝑛
𝑠

.   

A matrix M=(m1,…,mn) is called l1-column-flat if all its columns’ l1-norms |mj|1 have the same value.     
If Xk is a group of random variables and p(x) is some given probability distribution, then Xk ~iid p(x) 

denotes that all these Xk’s are identically and independently sampled under this distribution.   

2.1    Basic Problems   

In this paper we investigate the problem of reconstructing n-by-n matrix signal X=(x1,…,xn) with s-sparse 
and l1-flat column vectors x1,…,xn (i.e., |||X|||1 = |xj|1 for all j) by solving the following convex programming 
problems. The regularizer is matrix norm |||X|||1:=maxj|xj|1.     

y, Φ, η:                  inf |||Z|||1    s.t. Z ∈ 𝑅𝑛×𝑛, |y–Φ(Z)|α ≤ η                                        (2.1a) 

Problem MP(α)
In this setting y is a measurement vector in Rm with some vector norm |.|α defined on it, e.g., |.|α being 
the  l2-norm.  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚  is  a  linear  operator  and  there  is  a  matrix  X  (the  real  signal)  satisfying 
y=Φ(X)+e where |e|α≤η. In an equivalent component-wise formulation, yi=<Φi,X>+ei where Φi  ∈ 𝑅𝑛×𝑛
  for 
each i=1,…,m.     

y, A,B, η:                inf |||Z|||1    s.t. Z ∈ 𝑅𝑛×𝑛, |Y–AZBT|α ≤ η                                      (2.1b) 

Problem MP(α)
In this setting Y is a matrix in space  𝑅𝑚×𝑚  with some matrix norm |.|α defined on it, e.g., |.|α being the 
Frobenius-norm.  ΦA,B:  𝑅𝑛×𝑛 → 𝑅𝑚×𝑚:  Z→AZBT  is  a  linear  operator  and  there  is  a  matrix  signal  X 
satisfying  Y=  AXBT+E  and  |E|α≤η.  In  an  equivalent  component-wise  formulation,  ykl=<Φkl,X>+ekl= 
∑ijAkiXijBlj+ekl where for each 1≤k, l≤m Φkl is a n-by-n matrix with its (i,j)-entry as AkiBlj.   

Remark 2.1    Throughout this paper we only consider the case 0 ≤ η < |y|α for problem MP(α)
< |Y|α for problem MP(α)

y, A,B, η since otherwise the minimizers X* of these problems are trivially O.   

y, Φ, η and 0 ≤ η 

For  the  above  problems,  we  will  investigate  the  real  matrix  signal  X’s  reconstructability  and 

approximation error where the measurement operator Φ and ΦA,B (actually matrix A and B) are deterministic 
or at random. In some cases problem MP(α)
y, A,B, η are equivalent each other but in other cases 
some specific hypothesis is only suitable to one of them, so it’s appropriate to deal with them respectively.   

y, Φ, η and MP(α)

2.2    Related Concepts   

Some related concepts are presented in this subsection which are necessary and important to our work. For 

brevity all definitions are only presented in the form of vectors, however the generalization to the form of 

matrices is straightforward. 

A cone C is a subset in Rn such that tC is a subset of C for any t>0. For a subset K in Rn, its polar dual 

K*:={y: <x,y>≤0 for all x in K}. K* is always a convex cone.   

For a proper convex function F(x), there are two important and related sets:   

D(F,x):={v:F(x+tv) ≤ F(x) for some t>0}   

∂F(x)={u: F(y) ≥ F(x) + <y–x, u> for all y}   

and an important relation is D(F,x)* =  ∪𝑡>0t∂F(x).   

Let |.| be some vector norm and |.|* be its conjugate norm, i.e., |u|*:=max{<x,u>: |x| ≤ 1}(e.g., |||X|||1
* = 
∑j |xj|∞) then                                      ∂|x| = {u: |x| = <x,u> and |u|* ≤ 1}                                                    (2.2) 
Let K be a cone in a vector space L on which Φ is a linear operator, the minimum singular value of Φ 

with respect to K, norm |.|β on L and norm |.|α on Φ’s image space is defined as 

3 

 
                                        λmin,α,β(Φ;K):=inf{|Φu|α: u in K and |u|β=1}                                                (2.3) 

When both |.|β and |.|α are l2(or Frobenius) norms, λmin,α,β(Φ;K) is simply denoted as λmin(Φ;K).   

Let K be a cone (not necessarily convex) in normed space (L, |.|β), its conic Gaussian width is defined 
as                                            wβ(K):=Eg[sup{<g,u>: u in K and |u|β=1]                                                    (2.4) 
where  g  is  the  random  vector  on  L  sampled  under  standard  Gaussian  distribution.  When  |.|β  is  l2  or 
Frobenius norm on L, wβ(K) is simply denoted as w(K).   

2.3    Fundamental Facts 

Our  research  in  the  second  part  (sec.5  and  sec.6)  follows  the  convex  geometric  approach  built  upon  a 

sequence  of  important  results,  which  are  summarized  in  this  section  as  the  fundamental  facts.  Originally 
these facts were presented for vector rather than matrix signals[7-10]. We re-present them for matrix signals in 
consistency with the form of our problems. For brevity, all facts are only presented with respect to problem 
MP(α)

y, Φ, η except for FACT 2.6. 

y, Φ, η where η=0, then X* = X    iff    kerΦ∩D(|||.|||1,X)={O}. 

FACT 2.1 (1)Let X∈ 𝑅𝑛×𝑛  be any matrix signal and y=Φ(X), X* is the solution (minimizer) to the problem 
MP(α)
(2)  Let  X∈ 𝑅𝑛×𝑛  be  any  matrix  signal and y=Φ(X)+e  where  |e|α≤η,  X*  be  the  solution  (minimizer) to  the 
problem MP(α)

y, Φ, η where η > 0, |.|β be a norm on signal space to measure the reconstruction error, then   
|X*– X|β ≤2η/λmin,α,β(Φ;D(|||.|||1,X)) 

FACT 2.2    K is a cone in  𝑅𝑛×𝑛(not necessarily convex), Φ:  𝑅𝑛×𝑛 → 𝑅𝑚  is a linear operator with entries 
Φkij ~iid N(0,1), then for any t > 0: 

        Combining these two facts, the following quite useful corollary can be obtained. 

        P[λmin(Φ;K) ≥ (m–1)1/2 – w(K) – t] ≥ 1– exp(–t2/2)   

FACT  2.3    Let  X  and  X*  be  respectively  the  matrix  signal  and  the  solution  to  MP(2)
FACT 2.1(2), Φkij ~iid N(0,1), then for any t>0:   

y, Φ, η  as  specified  in 

P[|X*–X|F    ≤ 2η/((m–1)1/2 – w(D(|||.|||1,X)) –t)+] ≥ 1 – exp(–t2/2) 
where  (u)+:=max(u,0).  In  particular,  when  the  measurement  vector’s  dimension  m  ≥  w2(D(|||.|||1,X)  + 
Cw(D(|||.|||1, X) where C is some absolute constant, X can be reconstructed robustly with respect to the error 
norm |X*–X|F with high probability by solving MP(2)

y, Φ, η.   

FACT  2.4    Let  F  be  any  proper  convex  function  and  zero  matrix  is  not  in  ∂F(X),  then  wβ
EG[inf{|G–tV|β*
entries Gij~iid N(0,1). In particular, when |.|β is |.|F then w2(D(F, X)) ≤ EG[inf{|G–tV|F
          This fact is useful to estimate the squared Gaussian width wβ

2(D(F,  X))  ≤ 
2:  t>0,  V  in  ∂F(X)}]  where  |.|β*  is  the  norm  dual  to  |.|β and  G  is  the  random  matrix  with 
2: t>0, V in ∂F(X)}].     

2(D(F, X))’s upper bound. 

FACT  2.5    Let  X  and  X*  be  respectively  the  matrix  signal  and  the  solution  to  MP(2)
y, Φ, η  as  specified  in 
FACT 2.1(2), with the equivalent component-wise formulation yk=<Φk,X>+ek, each Φk ~iid Φ where Φ is a 
random matrix which satisfies the following conditions: (1)E[Φ]=0; (2)There exists a constant α>0 such that 
α ≤ E[<Φ,U>] for all U: |U|F=1; (3) There exists a constant σ>0 such that P[|<Φ,U>| ≥ t] ≤ 2exp(–t2/2σ2). Let 
ρ:= σ/α, then for any t>0: 

P[ |X*–X|F    ≤ 2η/(c1αρ-2m1/2 – c2σw(D(|||.|||1,X)) – αt)+ ] ≥ 1 – exp(–c3t2) 

where c1, c2, c3 are absolute constants.   

FACT 2.6    Γ is a subset in n-by-n matrix space. Define the linear operator ΦA,B:  𝑅𝑛×𝑛 → 𝑅𝑚×𝑚: Y=AXBT. 
In the equivalent component-wise formulation, ykl=<Φkl,X>=∑ijAkiXijBlj for each 1≤k,l≤m, Φkl ~iid Φ which is 

4 

 
sampled under some given distribution. For any parameter ξ>0, define 

Qξ(Γ; Φ):= inf {P[|<Φ,U>| ≥ ξ]: U in Γ and |U|F=1}   
Furthermore,  for  each  1≤k,l≤m,  let  εkl  ~iid Rademacher  random  variable  ε  (P[ε=±1]=1/2)  which  are  also 
independent of Φ, and define 

W(Γ; Φ):=EH[sup{<H,U>: U in Γ and |U|F=1}]    where H:=m-1∑klεklΦkl=m-1ATEB, E=[εkl]; 
λmin(Φ; Γ) := inf{(∑kl |UklΦkl|2 )1/2: U in Γ and |U|F=1 } 

Then for any ξ>0 and t>0: 

P[ λmin(Φ; Γ) ≥ ξmQ2ξ(Γ; Φ) – 2W(Γ; Φ) – ξt ] ≥ 1– exp(–t2/2) 
Remark: In Fact 2.6 the definition of λmin(Φ; Γ) is the matrix version of that in subsection 2.2 with respect to 
Frobenius  norm.  The  proof  of  FACT  2.5  and  2.6  (with  respect  to  vector  signals)  can  be  found  in  [8]’s 

Theorem 6.3 and Proposition 5.1.   

3    Basic Conditions on Matrix Signal Reconstruction   

In this and next section we investigate sufficient and necessary conditions on the measurement operator for 
accurate  and  approximate  signal  reconstruction  via  solving  problems  MP(α)
notation  simplicity,  we  only  deal  with  problem  MP(α)
transformed into problem MP(α)
        We present conditions for accurate, stable and robust reconstruction respectively. As will be seen, these 

y,  A,B,  η.  For 
y,  Φ,  η  and  the  formulation  can  be  straightforwardly 

y,  Φ,  η  and  MP(α)

y, A,B, η. 

conditions are similar as those related to the regularizers with so-called decomposable subdifferentials. The 

vector’s  l1-norm  and  matrix’s  nuclear  norm  are  such  examples.  However,  ∂|||X|||1  is  not  even 
weakly-decomposable (i.e., there is no W0 in ∂|||X|||1 such that <W0, W–W0> = 0 for all W in ∂|||X|||1). At first 
we prove a technical lemma 3.1 which describes ∂|||X|||1’s structure.     

Lemma 3.1    For n-by-n matrix X=(x1,…,xn) and matrix norm |||X|||1:=maxj|xj|1, the subdifferential   

∂|||X|||1 = {(λ1ξ1,…, λnξn): ξj in ∂|xj|1 and λj ≥ 0 for all j, λ1+…+λn =1 and λj=0 for j: |xj|1<maxk|xk|1}   

Proof    It’s easy to verify the set {(λ1ξ1,…, λnξn): ξj in ∂|xj|1 and λj≥0 for all j, λ1+…+λn=1 and λj=0 for j: 
|xj|1<maxk|xk|1} is contained in ∂|||X|||1: since for any M ≡ (λ1ξ1,…, λnξn) in this set, we have   

<M,X> = ∑j λj<ξj,xj> = ∑j λj|xj|1 = |||X|||1∑j λj = |||X|||1   

and |||.|||1’s conjugate norm |||M|||1
          Now  prove  that  any  M  in  ∂|||X|||1  has  the  form  specified  as  a  member  in  the  above  set.  Let  M  ≡ 
(η1,…,ηn), |||Y|||1 ≥ |||X|||1 + <Y–X,M> for all Y ≡ (y1,…, yn) implies: 

* = ∑j λj|ξj|∞ ≤ ∑j λj = 1, as a result M is in ∂|||X|||1.         

maxj|yj|1 ≥ maxj|xj|1 + ∑j <yj–xj, ηj>                                          (3.1)   

j, j=1,…,n, (3.1) implies 

j be such a vector with component e*

Let ηj=|ηj|∞ξj ( so |ξj|∞=1 if ηj≠0 ), then maxj|yj|1 ≥ maxj|xj|1 + ∑j |ηj|∞<yj–xj, ξj>. For each j: ηj≠0 we can select 
a ij such that |ξj(ij)| = 1 and let e*
j(i) = 0 for all i≠ij, 
then for yj = xj + e*
          1+ maxj|xj|1 ≥ maxj|yj|1 ≥ maxj|xj|1 + ∑j |ηj|∞<e*
As a result 1≥ ∑j |ηj|∞ . 
          Furthermore  for  any  given  i,  let  yj =  xj  for  all j  ≠  i  and  yi  be  any  vector  satisfying  |yi|1  ≤  |xi|1,  then 
substitute these y1,…,yn into (3.1) we obtain   

j, ξj> = maxj|xj|1 + ∑j |ηj|∞|ξj|∞ = maxj|xj|1 + ∑j |ηj|∞   

j(ij) = sgn ξ j(ij) and e*

maxj|xj|1 ≥ maxj|yj|1 ≥ maxj|xj|1 + ∑j <yj–xj, ηj> = maxj|xj|1 + |ηj|∞<yi–xi, ξi>     

i.e., <yi–xi,  ξi>  ≤  0.  As  a result, <xi,  ξi>  ≥  <yi, ξi>  for  all  yi:  |yi|1  ≤  |xi|1  so  <xi,  ξi>  ≥  |xi|1|ξi|∞ =  |xi|1, hence 
finally we get <xi, ξi> = |xi|1. This (together with |ξi|∞=1) implies ξi in ∂|xi|1 if ηi ≠ 0, for any i=1,…,n.   
          In summary, we have so far proved that for any M in  ∂|||X|||1, M always has the form (λ1ξ1,…, λnξn) 
where ξj in ∂|xj|1, λj ≥ 0 for all j and λ1+…+λn ≤ 1. Since |||X|||1 = <M,X> = ∑j λj<ξj,xj> = ∑j λj|xj|1 ≤ maxj|xj|1 

5 

 
∑j λj ≤ |||X|||1, as a result λ1+…+λn =1 and λj=0 for j: |xj|1<maxk|xk|1.                                                              □   

Remark  3.1:  More  explicitly,  ∂|||X|||1 =  {(λ1ξ1,…,  λnξn):  λj ≥  0  for  all  j,  λ1+…+λn=1  and  λj=0  for  j:  |xj|1 < 
maxk|xk|1; |ξj|∞ ≤ 1 for all j and ξj(i) = sgn(Xij) for Xij≠0 }.       

3.1    Conditions on Φ For Accurate Reconstruction From Noise-free Measurements   

At first we investigate the conditions for matrix signal reconstruction via solving the following problem:   

Problem MPy, Φ, 0:                          inf |||Z|||1    s.t. Z ∈ 𝑅𝑛×𝑛, y = Φ(Z)                                              (3.2)   

Theorem 3.1    Given positive integer s and linear operator Φ, the signal X∈  ∑𝑛×𝑛
minimizer of problem MPy, Φ, 0 where y=Φ(X) if and only if   

𝑠

is always the unique 

|||HS|||1 < |||H~S|||1                                                              (3.3) 

for any H=(h1,…,hn)  ∈  kerΦ\{O} and any s-sparsity pattern S.     

Proof    To prove the necessity, let S be a  s-sparsity pattern and H  ∈  kerΦ\{O}. Set y ≡ Φ(HS) = Φ(–H~S) 
and HS  ∈  ∑𝑛×𝑛
, HS should be the unique minimizer of MPy, Φ, 0 with –H~S as its feasible solution, hence 
|||HS|||1 < |||H~S|||1.   

𝑠

Now  prove  the  sufficiency.  Let  X=(x1,…,xn)  be  a  matrix  signal  with  its  support  S  =  S1∪ … ∪Sn  as  a 
s-sparsity  pattern  (where  Sj  =  supp(xj))  and  let  y  =  Φ(X)  .  For  any  feasible  solution  Z  (≠X)  of  MPy, Φ, 0, 
obviously there exists H=(h1,…, hn) in kerΦ\{O} such that Z=X+H. Since ∂|||Z|||1 ≥ ∂|||X|||1 + <H,M> for any 
M in ∂|||X|||1, we have     
              ∂|||Z|||1 – ∂|||X|||1 ≥ sup{<H,M> : for any M in ∂|||X|||1 }   
          = sup{<H,M> : M=E+V where E=(λ1sgn(x1)…, λnsgn(xn)) and V=(λ1ξ1,…, λnξn), |ξj|∞ ≤ 1,   

λj ≥ 0 for all j, λ1+…+λn =1 } ( by lemma 3.1 and notice supp(sgn(xj)) = Sj = ~ supp(ξj) ) 

          ≥ sup{ – | <H,E> | + <H,V>: E and V specified as the above }       
λj<hj|Sj, sgn(xj)> | +  ∑𝑛
          = sup{ – |  ∑𝑛
          ≥ – sup |  ∑𝑛
λj<hj|Sj, sgn(xj)> |: λj ≥ 0 for all j, λ1+…+λn =1} + sup {<H~S,V>: |||V|||1
* is |||.|||1’s conjugate norm)   

*=∑j |λjξj|∞ ≤ ∑j |ξj|∞ = 1 where |||.|||1

𝑗=1

𝑗=1

𝑗=1

λj<hj|~Sj, ξj> :    λj and ξj specified as the above }   

* ≤ 1 } 

λj<hj|Sj, sgn(xj)> |: λj ≥ 0 for all j, λ1+…+λn =1} + |||H~S|||1                                  (3.4)   

                      (note that |||V|||1
          = – sup |  ∑𝑛

𝑗=1

          = – max j | <hj|Sj, sgn(xj)> | + |||H~S|||1   
          ≥  –  max  j |  hj|Sj  |1 +  |||H~S|||1  =  –  |||HS|||1 +  |||H~S|||1 >  0 under  the  condition  (3.3).  As a result,  X  is  the 
unique minimizer of MPy, Φ, 0.                                                                                                                          □ 

Remark 3.2:    (3.3) provides the uniform condition for signal reconstruction which applies to all unknown 

column-wise sparse signals. By a similar proof, we can also obtain a (non-uniform) sufficient condition for 

individual signal reconstruction, namely, for given operator Φ and unknown signal X with unknown support 

S=S1∪ … ∪Sn, if  there  exist  λ1,  …,  λn ≥  0:  λ1+…+λn =1  such  that for any  H=(h1,…,  hn)  in kerΦ\{O}  there 
holds:                                              |  ∑𝑛

λj<hj|Sj, sgn(xj)> | < |||H~S|||1                                                                          (3.5) 

𝑗=1

then X will be the unique minimizer of MPy, Φ, 0 where y = Φ(X).   
          On the  other hand,  from  MPy, Φ, 0’s  first-order  optimization  condition,  i.e.,  for  its minimizer  X  there 
exist M in ∂|||X|||1 and a multiplier vector u such that     
                                                                                          M + ΦT(u) = O                                                                  (3.6)   
then for any H=(h1,…, hn) in ker Φ we have   

0 = <Φ(H),u > = <H, ΦT(u)> = – <H, M> = – <H,E+V>     
where E=(λ1sgn(x1)…, λnsgn(xn)) and V=(λ1ξ1,…, λnξn), |ξj|∞ ≤ 1, so – <H,E> = <H,V>. Note that for the left 
hand side |<H,E>| = |  ∑𝑛
|λj<hj|~Sj, ξj>| 

λj<hj|Sj, sgn(xj)> | and for the right hand side |<H,V>| ≤  ∑𝑛

𝑗=1

𝑗=1

6 

 
𝑗=1

≤  ∑𝑛

λj|hj|~Sj|1|ξj|∞  ≤  max j |hj|~Sj|1∑𝑛

λj |ξj|∞ ≤  max j |hj|~Sj|1  =  |||H~S|||1,  we  obtain  a  (relatively  weak) 
non-uniform  necessary  condition,  namely,  for  given  operator  Φ  and  unknown  signal  X  with  unknown 

𝑗=1

support S=S1∪ … ∪Sn, if X is the minimizer of MPy, Φ, 0 where y = Φ(X) then there exist λj ≥ 0 for j=1,…,n: 
λ1+…+λn =1 such that for any H=(h1,…, hn) in ker Φ there holds the inequality     

|∑𝑛

𝑗=1

λj<hj|Sj, sgn(xj)> | ≤ |||H~S|||1                                                                          (3.7) 

3.2    Conditions on Φ For Stable Reconstruction From Noise-free Measurements   

Now investigate the sufficient condition for reconstructing matrix signal via solving MPy, Φ, 0 where y=Φ(X) 
for some signal X which is unnecessarily sparse but l1-column-flat. The established condition guarantees the 
minimizer X* to be a good approximation to the real signal X.   

Theorem 3.2    Given positive integer s and linear operator Φ with the s-|||.|||1 Stable Null Space Property, 
i.e., there exists a constant 0 < ρ < 1 such that   

|||HS|||1 ≤ ρ |||H~S|||1                                                                                      (3.8)   
for  any  H  in  ker  Φ  and  sparsity  pattern  S  =  S1∪ … ∪Sn  where  |Sj|  ≤  s.  Let  Z=(z1,…,zn)  be  any  feasible 
solution to problem MPy, Φ, 0 where y = Φ(X) for some signal X = (x1,…,xn), then       

|||Z – X|||1 ≤ (1–ρ) –1(1+ρ)(2max j σs(xj)1 + maxj (|zj|1 – |xj|1))                      (3.9)   
where σs(v)1 := |v|1 – (|v(i1)|+…+|v(is)|), v(i1),…,v(is) are v’s s components with the largest absolute values. 

In  particular,  for  the  minimizer  X*  of  MPy, Φ, 0  where  the  real  signal  X  is  l1-column-flat,  there  is  the 

reconstruction-error bound:              |||X* – X|||1 ≤ 2(1–ρ)–1(1+ρ) maxj σs(xj)1                                        (3.10) 

The proof follows almost the same logic of proving l1-min reconstruction’s stability for vector signals 
under  the  l1  Null  Space  Property  assumption  (e.g.,  see  sec.  4.2  in  [1]).  For  presentation  completeness  we 
provide the simple proof here. The basic tool is an auxiliary inequality (which unfortunately does not hold 
for matrix norm |||.|||1): given index subset Δ and any vector x, z, then[1]   

                                            |(x – z)~Δ|1 ≤ | z |1 – | x |1 + | (x –z)Δ |1 + 2| x ~Δ |1                                                    (3.11) 

Proof of Theorem 3.2:    For any feasible solution Z=(z1,…,zn) to problem MPy, Φ, 0 where y=Φ(X), there is H 
= (h1,…,hn) in ker Φ such that Z = H + X. Apply (3.11) to each column vector zj and xj we get     

| hj|~Sj |1 ≤ | zj |1 – | xj |1 + | hj|Sj |1 + 2| xj|~Sj |1                                     

Hence |||H~S|||1 ≡ maxj |hj|~Sj |1 ≤ maxj (|zj|1 – |xj|1) + |||HS|||1 + 2maxj |xj|~Sj| ≤ maxj (|zj|1 – |xj|1) + ρ|||H~S|||1+2maxj 
| xj|~Sj |1 ( by (3.8)), namely : 
                                            |||H~S|||1 ≤ (1–ρ) –1(2max j | xj|~Sj |1 + maxj (|zj|1 – |xj|1))   
As  a result  |||H|||1 =  |||HS|||1 +  |||H~S|||1 ≤  (1+ρ)|||H~S|||1 ≤  (1–ρ) –1(1+ρ)(2max  j |  xj|~Sj |1  +  maxj  (|zj|1  –  |xj|1))  for 
any s-sparsity pattern S, which implies (3.9) since minS max j | xj|~Sj |1 = max j σs(xj)1.   

In particular, if Z is minimizer X* and X is l1-column-flat then |xj|1=|||X|||1 for any j so maxj (|x*
j|1 – |xj|1) 
= |||X*|||1 – |||X|||1 ≤ 0 for minimizer X*, which implies (3.10).                                                                      □   

Remark 3.3: For any flat and sparse signal X, condition (3.8) guarantees X can be uniquely reconstructed by 
solving MPy, Φ, 0 due to Theorem 3.1, while in this case the right hand side of (3.10) is zero, i.e., this theorem 
is  consisted  with  the  former  one.  In  addition,  (3.10)  indicates  that  the  error  for  the  minimizer  X*  to 
approximate the flat but non-sparse signal X is controlled column-wisely by X’s non-sparsity (measured by 

maxj σs(xj)1).       

Remark 3.4: Given positive integer s, sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s and linear operator Φ, 
Let ΦT denote the adjoint operator and ΦS denote the operator restricted on  ∑ (𝑆)
(so ΦS(X) = Φ(XS)). If 

𝑛×𝑛
𝑠

7 

 
ΦS

N((ΦS

TΦS)–1ΦS

TΦS is a bijection and there exists a constant 0 < ρ < 1 such that the operator norm satisfies the inequality 
TΦ~S: |||.|||1 →|||.|||1 ) ≤ ρ                                            (3.12a)   
TΦS)–1 
then  for  any  H  in  ker  Φ,  from  ΦS(HS) =  Φ(HS)  =  Φ(–H~S) =  Φ~S(–H~S) we  can  obtain  HS  =  –  (ΦS
TΦ~S: |||.|||1 →|||.|||1 )|||H~S|||1 ≤ ρ|||H~S|||1, therefore (3.12a) is a 
ΦS
uniformly  sufficient  condition  stronger  than  s-|||.|||1  Stable  Null  Space  Property  (3.8),  similar  as  Tropp’s 
exact recovery condition for vector signal’s l1-min reconstruction. By operator norm’s duality N(M: |.|α →|.|β ) 
= N(MT: |.|*

TΦ~S(H~S) and then |||H~S|||1 ≤ N((ΦS

TΦS)–1ΦS

β→|.|*

α) an equivalent sufficient condition is:   
TΦS)–1: |||.|||1
N(Φ~S

TΦS(ΦS

*
 ) ≤ ρ                                            (3.12b)   
The  condition  (3.12)  can  be  enhanced  to  provide  more  powerful  results  for  signal  reconstruction 

* →|||.|||1

(discussed in next section). Now we conclude this subsection with a simple condition  for problem MPy, Φ, 0 
and MPy, Φ, η    to guarantee their minimizers’ l1-column-flatness.   

Theorem 3.3(Condition for Minimizer’s l1-Column-Flatness) Given positive integer s, sparsity pattern S = 
S1∪ … ∪Sn  where  |Sj|  ≤  s  and  linear  operator  Φ,  let  ΦT  denote  the  adjoint  operator  and  ΦS  denote  the 
T(z) doesn’t have any zero-column for any z ≠ 0, 
operator restricted on  ∑ (𝑆)
then any minimizer X* of MPy, Φ, 0 or MPy, Φ, η with supp(X*) contained in S is l1-column-flat.     

  (so ΦS(X) = Φ(XS)). If ΦS

𝑛×𝑛
𝑠

Proof    Consider the problem MPy, Φ, η : inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–ΦS(Z)|2 ≤ η at first where η > 0. For any 
minimizer  X*  of  this  problem  with  both  its  objective  |||.|||1  and  constraint  function  |y–ΦS(.)|2  convex, 
according  to  the  general  convex  optimization  theory,  there  exist  a  positive  multiplier  γ*  >  0  and  M*  in 
∂|||X*|||1 such that   
                                                                            M* + γ*ΦS
then    M*  =  γ*ΦS
maxk|xk

T(ΦS(X*) – y) = O and |y – ΦS(X*)|2 = η                                  (3.13) 
j|1 = 

*|1 for every j according to lemma 3.1.   
Now consider the problem MPy, Φ, 0 : inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, y = ΦS(Z). For its minimizer X* there is a 
multiplier  vector  u  such  that    M* +  ΦS
T(u)  =  O.  If  u  ≠  0  then  M*  doesn’t  have  any  zero  column  which 
implies |xj|1=maxk|xk|1 for every j according to lemma 3.1. On the other hand, u = 0 implies M* = O which 
cannot happen according to lemma 3.1 unless X* = O.                                                                                  □ 

T(y  –  ΦS(X*))  can  not  have  any  zero  column  since  y–ΦS(X*)  ≠  0,  which  implies  |x*

𝑚
𝑘=1 Φk
𝑧𝑘

T(z)) is always a subset in S.   

TZ),  the  adjoint  operator  ΦT(z)  =  ∑

 :  𝑅𝑚 → 𝑅𝑛×𝑛.    For 
Note:  For  MPy,  Φ,  η  where  Φ(Z)k  =  tr(Φk
problem MPY, A,B, η where ΦA,B(Z) = AZBT, the adjoint operator ΦT(Y) = ATYB:  𝑅𝑚×𝑚 → 𝑅𝑛×𝑛. In addition, 
supp(ΦS
Remark 3.5: For the unconstrained convex optimization problem   
2                         
                                                        X* = Arg inf |||Z|||1 + (1/2)γ |y – ΦS(Z)|2
The  sufficient  condition  for  minimizer  X*’s  l1-column-flatness  is  the  same:  ΦS
zero-column for z ≠ 0. This fact will be used in sec.4.1.     
          In  fact,  the  first-order  optimization  condition  guarantees  there  is  M*  in  ∂|||X*|||1  such  that  M*  + 
T(y  –  ΦS(X*))  in  ∂|||X*|||1.  Under  the  above  condition,  M*  has  no 
γΦS
0-column unless y – ΦS(X*) = 0. However, y – ΦS(X*) = 0 implies M* = O which cannot happen in ∂|||X*|||1 
k|1 = |||X*|||1 for every j.   
unless X* = O. As a result, |x*

T(ΦS(X*)  –  y)  =  O,  i.e.,  M*  =  γΦS

T(z)  doesn’t  have  any 

j|1 = maxk |x*

3.3    Conditions on Φ For Robust Reconstruction From Noisy Measurements   

Now  consider  matrix  signal  reconstruction  from  noisy  measurements  by  solving  the  convex  optimization 
problem MPy, Φ, η : inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–Φ(Z)|2 ≤ η where η > 0.   

Theorem 3.4    Given positive integer s and linear operator Φ with the s-|||.|||1 Robust Null Space Property, 

8 

 
i.e., there exist constant 0 < ρ < 1 and β > 0 such that   

|||HS|||1 ≤ ρ |||H~S|||1 + β|Φ(H)|2                                                                                (3.14) 
for any n-by-n matrix H and sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s. Let Z=(z1,…,zn) be any feasible 
solution to the problem MPy, Φ, η where y = Φ(X) + e for some signal X = (x1,…,xn) and |e|2 ≤ η, then       

|||Z – X|||1 ≤ (1–ρ)-1(1+ρ)(2max j σs(xj)1 + 2βη + maxj (|zj|1 – |xj|1))                          (3.15) 
In  particular,  for  the  minimizer  X*  of  MPy, Φ, η where  the  real  signal  X  is  l1-column-flat,  there  is  the 

error-control inequality:         

|||X* – X|||1 ≤ 2(1–ρ)–1(1+ρ)( maxj σs(xj)1 + βη )                                        (3.16) 

Proof    For  any  feasible  solution  Z=(z1,…,zn)  to  problem  MPy,  Φ,  η  where  y=Φ(X)+e,  Let  Z  –  X  =  H  = 
(h1,…,hn), apply (3.11) to each column vector  zj and xj we get | hj|~Sj |1 ≤ | zj |1 – | xj |1 + | hj|Sj |1 + 2| xj|~Sj |1 
Hence  |||H~S|||1  ≡  maxj |hj|~Sj |1 ≤  maxj  (|zj|1  –  |xj|1)  +  |||HS|||1 +  2maxj |xj|~Sj|  ≤  maxj  (|zj|1  –  |xj|1)  +  ρ|||H~S|||1 + 
2maxj | xj|~Sj |1 + β|Φ(H)|2 ( by (3.14)), namely : 
                                            |||H~S|||1 ≤ (1–ρ) –1(2max j | xj|~Sj |1 + maxj (|zj|1 – |xj|1) + β|Φ(H)|2)   
As  a result  |||H|||1 =  |||HS|||1 +  |||H~S|||1 ≤  (1+ρ)|||H~S|||1 + β|Φ(H)|2  ≤  (1–ρ)–1(1+ρ)(2maxj |  xj|~Sj |1  +  maxj  (|zj|1 – 
|xj|1)) + 2(1–ρ)–1β|Φ(X)|2 ) for any s-sparsity pattern S, which implies (3.15) since minS max j | xj|~Sj |1 = max j 
σs(xj)1.   

In particular, if Z is a minimizer X* and X is l1-column-flat then |xj|1=|||X|||1 for any j so maxj (|x*

j|1 – 
|xj|1) = |||X*|||1 – |||X|||1 ≤ 0 for minimizer X*, which implies (3.16).                                                              □   

Remark 3.6: (3.16) indicates that the error for the minimizer X* to approximate the flat but non-sparse signal 
X is up-bounded column-wisely by X’s non-sparsity (measured by maxj σs(xj)1) and the noise strength η. If 
the  signal  X  is  both  s-sparse  and  l1-column-flat,  the  column-wise  approximation  error  |||X*  –  X|||1  ≤  2(1–
ρ)-1(1+ρ)βη = O(η), i.e., with linear convergence rate.         

Remark 3.7: For any minimizer X* of problem MPy,Φ, η, (3.13) is the first-order optimization condition with 
positive multiplier γ* > 0 and M* in ∂|||X*|||1. Then for any H = (h1,…,hn) we have <M*, H> = γ*<y – Φ(X*), 
*)), V=(λ1ξ1,…, λnξn), |ξj|∞ ≤ 1 and note that 
Φ(H)> where by lemma 3.1 M*=E+V, E=(λ1sgn(x1
supp(E) = supp(X*) = S = ~supp(V), so <E,HS> = <E,H> = – <V,H> + γ*<y – Φ(X*), Φ(H)> = – <V,H~S> + 
γ*<y – Φ(X*), Φ(H)>. Since for the left hand side   
|<E,HS>| = |∑𝑛

*)…, λnsgn(xn

λj<hj|Sj, sgn(xj

*)> |   

𝑗=1

and for the right hand side 

  |<V,H~S>| + | γ*<y – Φ(X*), Φ(H)>| ≤  ∑𝑛

𝑗=1

|λj<hj|~Sj, ξj>| + γ*|y – Φ(X*)|2|Φ(H)|2   

≤  ∑𝑛

𝑗=1

λj|hj|~Sj|1|ξj|∞ + γ*η|Φ(H)|2 ≤ |||H~S|||1 + γ*η|Φ(H)|2 

we  obtain  a  non-uniform  necessary  condition,  namely,  for  given  operator  Φ  and  unknown  signal  X  with 
unknown support S=S1∪ … ∪Sn and y = Φ(X)+e, if X* is the minimizer of MPy, Φ, η with the correct support 
S and correct non-zero component signs as the real signal X, then there exist constants β(=γ*η) > 0 and λj ≥ 
0 for all j=1,…,n: λ1+…+λn =1 such that for any H=(h1,…, hn) there holds the inequality     

|∑𝑛

𝑗=1

λj<hj|Sj, sgn(xj)> | ≤ |||H~S|||1 + β|Φ(H)|2,                                                                          (3.17) 

3.4    M-Restricted Isometry Property     

It’s  well  known  that  RIP  with  appropriate  parameters  provide  powerful  sufficient  conditions  to  guarantee 

l1-min reconstruction for sparse vector signals. With our regularizer |||X|||1 we propose a similar but slightly 
stronger condition to guarantee reconstruction robustness by solving the convex programming MPy, Φ, η .   

Theorem  3.5    Given  positive  integer  s  and  linear  operator  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚.  Suppose  there  exist  positive 

9 

 
constants 0 < δs < 1 and Δs > 0 such that:   
2 ≤ (1 + δs )|Z|F
2 ≤ |Φ(Z)|2
(1)    (1– δs )|Z|F
(2)    |<Φ(Z), Φ(W)>| ≤    (Δs/n)∑ |𝑧𝑗|2
𝑛
𝑗=1
  W=(w1,…,wn)  ∈ ∑𝑛×𝑛
for  any  Z=(z1,…,zn)  ∈ ∑
conditions, there are constants ρ and β such that   

𝑛×𝑛
𝑠

𝑎𝑛𝑑

𝑠

2    for any Z  ∈ ∑ ;
                                                                                              (3.18) 
𝑛×𝑛
𝑠
|𝑤𝑗|2                                                                                        (3.19) 
with  supp(Z)∩supp(W)  =  ∅.  Under  these  two 

|||HS|||1 ≤ ρ |||H~S|||1 + β|Φ(H)|2                                                                                    (3.20) 

for any n-by-n matrix H and any s-sparsity pattern S, where the constants can be selected as 

ρ ≤ Δs/(1– δs –Δs/4),    β ≤ s1/2(1+δs )1/2/(1– δs – Δs/4)                              (3.21) 

          In particular, δs +5Δs/4 <1 implies the robust null space condition: ρ < 1.   

Note: Condition (1) is the standard RIP which implies |<Φ(Z),Φ(W)>| ≤ δ2s|Z|F|W|F for s-sparse matrices Z 
and W with separated supports, slightly weaker than condition (2).   

Proof    Let H = (h1,…,hn) be any n-by-n matrix. For each j suppose |hj(i1)| ≥ |hj(i2)| ≥ …≥ |hj(in)|, let S0(j)   
= {(i1, j),…,(is, j)}, i.e., the set of indices of s components in column hj with the largest absolute values, S1(j) 
= {(i1+s, j),…,(i2s, j)} be the set of indices of s components in hj with the secondary largest absolute values, 
𝑛 Sk(j), obviously H = ∑k≥0 HSk. At first we note that (3.20) holds for 
etc., and for any k=0,1,2,… let Sk =  ∪𝑗=1
S as long as it holds for S0, so we try to prove this in the following. Start from condition (1):     
            (1– δs )|HS0|F

2 = <Φ(HS0), Φ(H) – ∑k≥1Φ(HSk)>   

2 ≤ |Φ(HS0)|2
= <Φ(HS0), Φ(H)> – ∑k≥1<Φ(HS0), Φ(HSk)> 
≤ |Φ(HS0)|2|Φ(H)|2 + (Δs/n) ∑n≥ j ≥1∑k≥1 |hj|S0(j) |2|hj|Sk(j)|2    (by condition (2))   
≤ (1+ δs )1/2|HS0|F|Φ(H)|2
≤ (1+ δs )1/2|HS0|F|Φ(H)|2
        ( by the inequality (∑s≥k≥1 ak

 + (Δs/n) |HS0|F ∑n≥ j ≥1∑k≥1|hj|Sk(j)|2    (by condition (1) and |hj|S0(j) |2 ≤ |HS0|F )   
 + (Δs/n) |HS0|F ∑n≥ j ≥1(s–1/2|hj|~S0(j)|1 + (1/4)|hj|S0(j)|2)   

2) 1/2 ≤ s–1/2 ∑s≥k≥1 ak + (s1/2/4)(a1–as) for a1 ≥ a2 ≥ …≥ as ≥ 0   

and the fact min s≥i≥1 |hj|Sk(j)(i)| ≥ max s≥i≥1 |hj|Sk+1(j)(i)| for any j ) 

≤ |HS0|F ((1+ δs )1/2 |Φ(H)|2
≤ |HS0|F ((1+ δs )1/2 |Φ(H)|2
= |HS0|F ((1+ δs )1/2 |Φ(H)|2

 + (s–1/2Δs/n)∑n≥ j ≥1|hj|~S0(j)|1 + (Δs/4n)∑n≥ j ≥1|hj|S0(j)|2)   
 + s–1/2Δs maxj | hj|~S0(j) |1 + (Δs/4n)n1/2 (∑n≥ j ≥1|hj|S0(j)|2
 + s–1/2Δs|||H~S0 |||1 + (Δs/4n1/2) |HS0|F

 )     

2) 1/2   

Cancel    |HS0|F
hence   

    on  both  sides  we  get  (1–  δs )|HS0|F  ≤  (1+  δs )1/2  |Φ(H)|2

 +  s–1/2Δs|||H~S0 |||1  +  (Δs/4n1/2)  |HS0|F

|HS0|F ≤ (1– δs –Δs/4n1/2) –1((1+ δs )1/2 |Φ(H)|2

 + s–1/2Δs|||H~S0 |||1)   

Note that |||HS0|||1 = max j | hj|S0(j) |1 ≤ s1/2max j | hj|S0(j) |2    ≤ s1/2|HS0|F and combine this with the above inequality, 
we obtain (3.20) and (3.21) for S0, which implies they hold for any S.                                                          □ 

4    More Properties on Reconstruction from Noisy Measurements   

In this section we establish stronger conditions on the measurement operator Φ for some stronger results on 
sparse  and  flat  matrix  signal  reconstruction  from  noisy  measurements,  e.g.,  conditions  to  guarantee 

uniqueness, support and sign stability as well as value-error robustness.   

As  in  last  sections,  for  notation  simplification  this  section  only  deals  with  problem  MPy,Φ,η but  all 
conclusions  can  be  straightforwardly  transformed  into  the  formulation  for  problem  MPY,A,B,η.  At  first  we 
note the basic fact that X* = arginf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y – Φ(Z)|2 ≤ η if and only if their exists a multiplier 
γ* > 0 (dependent on X* in general) such that X* is a minimizer of the unconstrained convex programming 
2 .  In  sec.  4.1  we  investigate  some  critical  properties  for  the  minimizer  of 
inf  |||Z|||1 +  (1/2)γ*  |y  –  Φ(Z)|2
unconstrained  optimization  (4.1),  then  on  basis  of  these  results  we  establish  conditions  for  robustness, 

10 

 
   
support and sign stability in signal reconstruction via solving MPy,Φ,η in sec. 4.2.   

In the following for given positive integer s, sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s for all j and 
  we  denote  the 

→∑

(𝑆)

the  linear  operator  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚,  when  ΦS
TΦS)–1ΦS
pseudo- inverse (ΦS
(𝑆)

T: Rm →  ∑

𝑛×𝑛
𝑠

TΦS    is  a  bijection  for  ∑ (𝑆)
  as ΦS

*–1.   

𝑛×𝑛
𝑠

𝑛×𝑛
𝑠

4.1    Conditions on Minimizer Uniqueness and Robustness for MLPy,Φ(γ)     

Consider the convex programming (4.1) with given parameter γ > 0 (value of γ is independently set):   

Problem MLPy,Φ(γ)                          inf |||Z|||1 + (1/2)γ |y – Φ(Z)|2

2                                                                            (4.1) 

Lemma 4.1 indicates basic properties of its sparse minimizer under some sparsity-related conditions.   

Lemma 4.1    Given y, positive integer s and sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s for all j, suppose 
the linear measurement operator Φ:  𝑅𝑛×𝑛 → 𝑅𝑚  satisfies:   
T(z) does not have any 0-column for z ≠ 0;   
TΦS is a bijection;   

S=Arginf supp(S) in S|||Z|||1 +(1/2)γ|y – Φ(Z)|2

T(ΦSΦS

*–1(y) – y), H>: |||H|||1=1} + sup{<Φ~S

T(ΦS

*–1)TM, H>: |||H|||1=1, |||M|||1
2 be the minimizer of MLPy,Φ(γ) with support in S, i.e.,   
2                                                (4.3) 

* ≤ 1 } < 1 (4.2) 

 = Arg inf |||Z|||1 + (1/2)γ |y – ΦS(Z)|2
X*
S

(1)    ΦS
(2)    ΦS
(3)    γ sup{<Φ~S
Let X*

Then there are the following conclusions:   

(1)  X*
(2)  X*
(3)  Let Y* = ΦS
N((ΦS

S is the unique minimizer of problem (4.3) and is l1-column-flat;         
S is also the unique minimizer of problem (4.1), i.e., the (global) minimizer of (4.1) is unique and is X*
*→|||.|||max) where 
TΦS)–1’s operator norm and matrix norm |||M|||max:= maxij |Mij|.   

(𝑆)
*→|||.|||max) denotes (ΦS

S,ij ≠ 0 for all (i,j): |Y*

ij | > γ–1N((ΦS

TΦS)–1: |||.|||1

TΦS)–1: |||.|||1

*–1(y)  ∈ ∑

, then X*

𝑛×𝑛
𝑠

S.   

(4)  With the same notations as the above, if     

then sgn(X*

S,ij) = sgn(Y*

min(i,j) in S |Y*
ij) for all (i,j) in S.   

ij | > γ–1N((ΦS

TΦS)–1: |||.|||1

*→|||.|||max)                                          (4.4)   

TΦS is  a  bijection,  (4.3)’s  objective  function  LS(Z)  =  |||Z|||1 +  (1/2)γ  |y  – 
. According to general convex programming theory, its 

S is also the global minimizer of (4.1), we prove its 
S) under the conditions 
S) for any H≠O with support in S and 

S+H) > L(X*

S+H) > L(X*

S+H with HS = O.   

S is the minimizer of (4.3), by first-order optimization condition there exists M* in ∂|||X*

S|||1 
S) – y) = O                                                        (4.5) 

𝑛×𝑛
𝑠

2.To prove X*

2 is strictly convex for variable Z  ∈ ∑ (𝑆)

Proof  :  (1)  Observe  that  when  ΦS
ΦS(Z)|2
minimizer X*
S is unique.   
(2) Let L(Z) := |||Z|||1 + (1/2)γ |y – Φ(Z)|2
perturbation by  H  will always increase the  objective’s  value, i.e., L(X*
specified by (1)(2)(3). Since conclusion (1) implies L(X*
L(Z) is convex, we only need to consider the perturbation X*
          Since X*
such that                                                M* + γ ΦS
then    M* = γ*ΦS
                                                            X*
Now we compute L(X*
          = ||| X*
          = ||| X*
= ||| X*
≥ ||| X*

S+H |||1 – |||X*
S+H |||1 – |||X*
S+H |||1 – |||X*
S+H |||1 – |||X*
          The first term    ||| X*

S|||1 + (1/2) γ (| Φ(X*
S|||1 + γ < Φ(X*
S|||1 + γ < Φ(X*
S|||1 + γ < Φ(X*
S+H |||1 – ||| X*

2   
S) – y, Φ(H) > + (1/2)γ |Φ(H)|2
S) – y, Φ~S(H) > + (1/2)γ |Φ~S(H)|2
S) – y, Φ~S(H) >   
S |||1   
j|1    (supp(X*

S)) and in particular M*

j|1 + |hj|1 ) – maxj |x*

S)                                 

~S = O. Equivalently: 

S)∩supp(H) =  ∅)     

*–1(y) – γ–1(ΦS

S+H) – L(X*

T(y – ΦS(X*

= maxj ( |x*

2 + 2<Φ(X*

T(ΦS(X*

S) – y |2

S = ΦS

2   

TΦS)–1(M*)                                                  (4.6)   

S) – y, Φ(H)>+ |Φ(H)|2

2 – |Φ(X*

S) – y |2

2)   

 |||1 + ||| H |||1 – ||| X*
          = ||| X*
S

S |||1    ( condition (1) implies X*

S’s l1-column-flatness: remark 3.5 )   

11 

 
= ||| H |||1   
By replacing X*
γ <ΦS(X*) – y, Φ~S(H)>   
T(ΦSΦS
          = γ <Φ~S
          ≥ (– γ sup{<Φ~S
Therefore        L(X*

S with (4.6), note supp(Φ~S

T) = ~S and |||M|||1

* ≤ 1, the second term   

*–1Φ~S(H) >                                                                      (4.7)   

*–1(y) – y), H> – < M*, ΦS
T(ΦSΦS
S+H) – L(X*

*–1(y) – y), H>: |||H|||1=1} – sup{<Φ~S
T(ΦSΦS

S) ≥ ||| H |||1(1– γ sup{<Φ~S

– sup{<Φ~S

T(ΦS

*–1)TM, H>: |||H|||1 = 1and |||M|||1

*–1)TM, H>: |||H|||1=1, |||M|||1

T(ΦS
*–1(y) – y), H>: |||H|||1 = 1}   

*≤ 1 })|||H|||1       

* ≤ 1 })                                  (4.8) 
S  is  the  minimizer  of  (4.1)  and  the 

and  condition  (3)  implies  the  right  hand  side  >  0.  This  proves  X*
minimizer is unique.     
(3) For Y* = ΦS

*–1(y)  ∈ ∑ (𝑆)
𝑛×𝑛
𝑠
S,ij | = | Y*
ij – γ–1(ΦS
ij | – γ–1 max ij | (ΦS
ij | – N((ΦS
ij | – N((ΦS

| X*
≥ | Y*
          ≥ | Y*
          ≥ | Y*
          > 0    for those (i,j): | Y*
(4) Note that for any non-zero scalars u and v, sgn(u) = sgn(v) iff |u| > |u – v |. Therefore   

  (then supp(Y*) in S) and by (4.6) we have 
ij | – γ–1 | (ΦS
TΦS)–1(M*) ij | ≥ | Y*
ij | – γ–1 | (ΦS
TΦS)–1(M*) ij | = | Y*
*→|||.|||max)|||M|||1
*     
*→|||.|||max) ( |||M|||1
TΦS)–1: |||.|||1

TΦS)–1(M*) ij |   
TΦS)–1(M*) |max   

* ≤ 1 )   
*→|||.|||max)   

TΦS)–1: |||.|||1
TΦS)–1: |||.|||1

ij | > N((ΦS

sgn(X*

In particular, if min(i,j) in S |Y*
S,ij) = sgn(Y*
sgn(X*

ij)    iff    | Y*

S,ij) = sgn(Y*

TΦS)–1(M*) ij |                          (4.9) 
ij | = γ–1 | (ΦS
ij – X*
TΦS)–1(M*) ij | so 
ij | > γ–1max(i,j) | (ΦS
*→|||.|||max) then | Y*
ij) for all (i,j) in S.                                                                                                              □ 

ij | > | Y*
TΦS)–1: |||.|||1

ij | > γ–1N((ΦS

4.2    Conditions on Minimizer Uniqueness and Robustness for MPy,Φ,η   

Now consider matrix signal reconstruction via solving the constrained convex programming:   

MPy, Φ, η :                            X* = Arg inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–Φ(Z)|2 ≤ η                                    (4.10) 

Lemma 4.2    Given y, positive integer s and sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s for all j, suppose 
the linear measurement operator Φ:  𝑅𝑛×𝑛 → 𝑅𝑚  satisfies:   
T(z) does not have any 0-column for z ≠ 0;   
TΦS is a bijection;   
T(ΦSΦS

(1)    ΦS
(2)    ΦS
(3)    sup{<Φ~S

*–1(y) – y), H>: |||H|||1=1} < η Λmin(ΦS

T) (1 – N(ΦS

*–1Φ~S: |||.|||1

*→|||.|||1

*))   

T) := inf { |||ΦS
where Λmin(ΦS
Then there are the following conclusions:   

T(z)|||1

*: |z|2 = 1 }.                                                                                                                                                   

(1)    As the minimizer of problem MPy, Φ, η, X* is unique, S-sparse and l1-column-flat;         
(2)    Let Y* = ΦS
, then for all (i,j) in S:   
ij) for all (i,j): |Y*
X*

*–1(y)  ∈ ∑ (𝑆)

ij ≠ 0 and sgn(X*

ij) = sgn(Y*

T: l2→|||.|||1

ij | > η N(ΦS

*)N((ΦS

𝑛×𝑛
𝑠

TΦS)–1: |||.|||1

*→|||.|||max)   

(3)    For any given matrix norm |.|α there holds:     

|X* – Y*|α ≤ η N(ΦS

*–1: l2→|.|α)                                                    (4.11) 

Proof    (1) Let X*
S  ∈  Arg inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–ΦS(Z)|2 ≤ η. i.e., a minimizer with its support restricted 
on S. We first prove X*
S is the only minimizer of this support-restricted problem, then we prove X* is also 
the minimizer of problem MPy, Φ, η (4.10),i.e., X*
S is the global minimizer and (4.10)’s minimizer is unique.   
          According to  general  convex  optimization theory,  there  exist  a  positive  multiplier γ*  >  0 and  M* in 
∂|||X*
S|||1 such that                  M* + γ*ΦS
then equivalently                              X*

S)|2 = η                                (4.12) 
TΦS)–1(M*)                                                (4.13) 

S) – y) = O and |y – ΦS(X*

T(ΦS(X*
S = ΦS

*–1(y) – γ*–1(ΦS

Suppose X0 is another minimizer of inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–ΦS(Z)|2 ≤ η, then there exist a positive 

multiplier γ0 > 0 and M0 in ∂|||X0|||1 such that                       

12 

 
 
M0 + γ0ΦS
Equivalently, (4.12) shows that X*
a  strictly  convex  function  on  ∑ (𝑆)
minimizer is unique. However, since |||X*
|||X*
minimizer of the support-restricted problem inf |||Z|||1 s.t. Z ∈ 𝑅𝑛×𝑛, |y–ΦS(Z)|2 ≤ η.   
*’s l1-column-flatness is implied by condition (1) and theorem 3.3.   

T(ΦS(X0) – y) = O and |y – ΦS(X0)|2 = η                                    (4.14) 
S is also a minimizer of LS(Z) = |||Z|||1 + (1/2)γ*|y – ΦS(Z)|2
2 which is 
  is  a  bijection  (condition(2)),  as  a  result  LS(Z)’s 
  since  ΦS
2 = 
S|||1 = |||X0|||1 we have LS(X*
S  is  the  unique 

S) = |||X*
2 =  LS(X0),  which  implies  X*

S|||1 + (1/2)γ*|y – ΦS(X*
S  =  X0,  i.e.,  X*

S|||1  +  γ*η2/2  =  |||X0|||1 +  (γ*/2)|y  –  ΦS(X0)|2

TΦS

𝑛×𝑛
𝑠

S)|2

XS

        Now prove X*
Again  we  start  with  the  fact  that  X*
multiplier γ* > 0 (which value depends on X*
problem (without any restriction on solution’s support ) 

S (which is S-sparse and l1-column-flat ) is also a minimizer of problem MPy, Φ, η (4.10). 
2  with  some 
S is the unique minimizer of the convex 

S  =  Arginf  LS(Z)  =  Arginf  |||Z|||1  +  (1/2)γ*  |y  –  ΦS(Z)|2

S) and by lemma 4.1, X*

under the condition   

inf |||Z|||1 + (1/2)γ* |y – Φ(Z)|2

2                                                        (4.15) 

γ* sup{<Φ~S
+ sup{<Φ~S

T(ΦSΦS
T(ΦS

*–1(y) – y), H)>: |||H|||1=1}   
*–1)T(M),H>: |||H|||1=1 and |||M|||1

T(ΦS(X*

S) – y) |||*

1 = γ* |||ΦS

1: |z|2 = 1}|ΦS(X*

T(z)|||*
T))–1                                                              (4.17) 

* ≤ 1 } < 1                        (4.16) 
S (under condition (4.16)) being the unique minimizer of 
S is also  a minimizer  of  MPy,Φ,η  (4.10),  which  furthermore  implies  that  MPy,Φ,η’s 

          According to convex optimization theory, X*
problem  (4.15)  means  X*
minimizer is unique, S-sparse and l1-column-flat.   
          In  order  to  make  condition  (4.16)  more  meaningful,  we  need  to  replace  the  minimizer-dependent 
parameter γ* with explicit information. From (4.15)’s first-order optimization condition (4.12) we obtain   
          1 ≥ |||M*|||*
T) 
1 ≥ γ* min{ |||ΦS
i.e.,                                                                γ* ≤ (η Λmin(ΦS
with this upper-bound of γ*, (4.16) can be derived from a uniform condition 
*–1(y) – y), H)>: |||H|||1=1}   
*–1)T(M), H>: |||H|||1=1and |||M|||1

T))–1 sup{<Φ~S
+ sup{<Φ~S
which is equivalent to condition (3).   
          From now on we denote X*
TΦS)–1: 
(2) For Y* = ΦS
*–1(y)  ∈ ∑ (𝑆)
ij)  for  all  (i,j)  in  S.  To  replace  multiplier  γ*  with  more  explicit 
|||.|||1
information in  this  condition,  we need  some  lower  bound  of  γ*  which  can  be  derived  from the  first-order 
optimization condition M* = γ*(y – ΦS
T(ΦS(X*)) again. Note that X* is l1-column-flat implies every column 
of X* is not 0, further more M* has no 0-column so M* = ( λ1u1,…, λnun) with λj > 0 for all j, λ1+…+λn =1 
and |uj|∞ = 1, as a result |||M*|||1

S as X*. 
  and by lemma 4.1’s conclusion (4), if min(i,j) in S |Y*

𝑛×𝑛
𝑠
*→|||.|||max)  then  sgn(X*

* ≤ 1 } < 1                          (4.18) 

S) – y|2 =γ*η Λmin(ΦS

ij | > γ*–1 N((ΦS

S,ij)  =  sgn(Y*

(η Λmin(ΦS

T(ΦSΦS

T(ΦS

1 = |||M*|||1

 ≤ γ* |||ΦS
*

*
 = ∑jλj|uj|∞ =1. Hence     
* ≤ γ* N(ΦS

T(ΦS(X*) – y) |||1

T: l2→|||.|||1

i.e.,                                                        γ*–1    ≤ η N(ΦS

T: l2→|||.|||1

Replace  γ*–1  with  its  upper-bound  in  (4.19),  we  obtain  if  min(i,j) 
1)N((ΦS

*→|||.|||max) then sgn(X*

ij) for all (i,j) in S.   

S,ij) = sgn(Y*

*)|ΦS(X*) – y|2 = γ*η N(ΦS

*)   
*)                                                        (4.19) 
in  S  |Y*
ij  |  >  η  N(ΦS

T: l2→|||.|||1

T: 

TΦS is a  bijection  for  ∑ (𝑆)

  implies  ΦS
𝑛×𝑛
𝑠

T(ΦS(Y*)–y)  =  O  and  then  condition  (1)  leads  to  ΦS(Y*)  =  y. 
,  so  for any  matrix 

  and notice  X*–Y*∈ ∑ (𝑆)

𝑛×𝑛
→∑ (𝑆)
𝑠

𝑛×𝑛
𝑠

TΦS)–1: |||.|||1
*–1(y)  ∈ ∑ (𝑆)
𝑛×𝑛
𝑠

l2→|||.|||*
(3)  Y*  =  ΦS
Furthermore, ΦS
norm |.|α:     

|X* – Y*|α = |(ΦS

TΦS)–1(ΦS
*–1: l2→|.|α)|ΦS(X*) – y|2 = η N(ΦS

≤ N(ΦS

TΦS)(X* – Y*)|α = |ΦS

*–1ΦS(X* – Y*)|α = |(ΦS

*–1(ΦS(X*) – y))|α     

*–1: l2→|.|α)                                                                          □   

Remark 4.1: Under the conditions specified in this lemma, the minimizer X* can have support stability, sign 
stability and component value-error robustness relative to any metric |.|α, e.g., we can take |.|α as |.|F, |||.|||1, 

13 

 
|||.|||max, etc. The error is linearly upper-bounded by the measurement error η. When η decreases as small as 
possible,  the  error  |X*  –  Y*|α seems  to  be  reduced  also  as  small  as  possible.  However,  this  is  not  true  in 
general because condition (3) may require η not be two small.     

        In real  world  applications,  the  support  S  of  the  signal is  of  course  unknown  so  lemma  4.2  cannot  be 

applied  directly.  However,  on  basis  of  lemma  4.2  a  stronger  and  uniform  sufficient  condition  can  be 

established to guarantee the uniqueness and robustness of the reconstructed signal from solving MPy, Φ, η.   

T(z) does not have any 0-column for z ≠ 0;   
TΦS is a bijection;   

Theorem  4.1    Given  positive  integer  s  and  the  linear  measurement  operator  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚,  suppose  Φ 
satisfies the following conditions for any s-sparsity pattern S = S1∪ … ∪Sn where |Sj| ≤ s for all j:   
(1)    ΦS
(2)    ΦS
(3)    N( Φ~S
        N( ((ΦS

T(ΦSΦS
*–1)TΦS
where Λmin(ΦS
Then for the minimizer X* of problem MPy, Φ, η (4.10) where y = Φ(X) + e with noise |e|2 ≤ η and a real 
  of some s-sparsity pattern R, there are the following conclusions:   

*–1Φ~S: |||.|||1
 ) < Λmin(ΦS
T(ΦS
*: |z|2 = 1 } and IS := the identical mapping on  ∑ (𝑆)

*–1 – IS): l2→|||.|||1
T – IS
T) := inf { |||ΦS

*–1)T: |||.|||1→|||.|||1))   
𝑛×𝑛
𝑠

T)Φ~S : |||.|||1→l2
T(z)|||1

flat signal X∈ ∑ (𝑅)

*)) or equivalently   

T) (1 – N(Φ~S

T) (1 – N(ΦS

* ) < Λmin(ΦS

*→|||.|||1

𝑛×𝑛
𝑠

.                                                                                                                                                   

(1)    Sparsity, flatness and support stability:   

X*∈ ∑ (𝑅)
𝑛×𝑛
𝑠

  and is l1-column-flat and the unique minimizer of MPy, Φ, η;         

(2)    Robustness: For any given matrix norm |.|α there holds:     

(3)    Sign Stability: sgn(X*

|X* – X|α ≤ 2η N(ΦR
ij) = sgn(Xij) for (i,j) in R such that :     

*–1: l2→|.|α)                                                    (4.20) 

|Xij | > η ( N(ΦR

*–1: l2→|||.|||max) + N(ΦR

T: l2→|||.|||1

*)N((ΦR

TΦR)–1: |||.|||1

*→|||.|||max ))          (4.21) 

Proof    (1) Note that in case of X  ∈ ∑ (𝑅)

  and y = Φ(X) + e = ΦR(X) + e, |e|2 ≤ η, we have 

𝑛×𝑛
𝑠
*–1(y) – y = (ΦRΦR
    ΦRΦR

*–1 – IR)e   

It’s  straightforward  to  verify  that  in  this  situation  condition  (3)  in  this  theorem  leads  to  condition  (3)  in 

*)) 

*→|||.|||1

TΦR)–1ΦR

*–1Φ~R: |||.|||1

T) (1 – N(ΦR

TΦR(Y*)  =  ΦR

*–1(y) implies ΦR

T(ΦRΦR
𝑛×𝑛
𝑠
𝑛×𝑛
𝑠

  and is l1-column-flat and the unique minimizer of MPy, Φ, η. 

  and by lemma 4.2(4), we obtain  |X* – Y*|α ≤ η N(ΦR

*–1(y) – y), H>: |||H|||1=1} < η Λmin(ΦR
(𝑅)

lemma 4.2:    sup{<Φ~R
for any η. As a result, X*∈ ∑
(2) For Y* = ΦR
*–1(y)  ∈ ∑ (𝑅)
given matrix norm |.|α. On the other hand, Y* = ΦR
to  ΦR(Y*)  =  y, hence  ΦR(Y*)  =  y  =  Φ(X) + e =  ΦR(X) +  e,  namely  ΦR
result:                                                  Y* – X = (ΦR
Since |e|2 ≤ η, we get |Y* – X|α ≤ η N(ΦR
Y*|α ≤ η N(ΦR
(3)  By  the  first-order  optimization  condition  on  minimizer  X*  with  the  fact  supp(X*)  =  R,  we  have  the 
equation X* = ΦR
                                                                  X* – Y* = – γ*–1(ΦR
Combining with (4.22), we get          X* – X = ΦR

*–1: l2→|.|α) for any 
T(ΦR(Y*)–y) = O then condition (1) leads 
T(e),  as a 
*–1(e)                                          (4.22)   
*–1: l2→|.|α) for any given matrix norm |.|α. Combining with |X* – 

*–1(e) – γ*–1(ΦR
TΦR)–1(M*)ij |, in particular, if 
ij| = |ΦR
TΦR)–1(M*)ij | then the former inequality is true and as 
ij)  =  sgn(Xij).  It’s  straightforward  to  verify  (by  using  (4.19))  that  the  condition  (4.21)  just 

Xij can satisfy |Xij| > maxij |ΦR
a  result  sgn(X*
provides a guarantee for this.                                                                                                                        □ 

TΦR)–1(M*)                                              (4.23) 
TΦR)–1(M*)                                        (4.24) 

ij) = sgn(Xij)    iff    |Xij| > |Xij – X*
*–1(e) ij | +γ*–1maxij | (ΦR

*–1: l2→|.|α) we get the reconstruction error bound |X* – X|α ≤ 2η N(ΦR

TΦR)–1(M*) where M* is in ∂|||X*|||1, namely:   

TΦR)–1(M*) = Y* – γ*–1(ΦR

*–1(e) ij – γ*–1(ΦR

*–1(y) – γ*–1(ΦR

  Since sgn(X*

TΦR(X) +ΦR

*–1: l2→|.|α) 

T(e) ≡ ΦR

Remark 4.2:  In  this  theorem  condition  (3)  is independent  with measurement  error  bound  η,  which  implies   
|X* – X|α = O(η) can hold for any small value of η, and (4.21) indicates that with η small enough, all signal’s 

14 

 
non-zero components’ signs can be correctly recovered. For given Φ, the associated value   
*–1: l2→|||.|||max) + N(ΦS

T: l2→|||.|||1

  Max s-sparse pattern S N(ΦS
or an enhancement 2Max s-sparse pattern S N(ΦS
signal-to-noise ratio threshold for correct sign recovery.   

T: l2→|||.|||1

*)N((ΦS

*)N((ΦS
TΦS)–1: |||.|||1

TΦS)–1: |||.|||1
*→ |||.|||max ) can be regarded as a 

*→ |||.|||max )   

T)= 
T(z)|||1
            Finally we mention that in condition (3) Λmin(ΦS
Y,A,B,η is one of critical quantities which value should be as large as possible 
inf{|||ΦS
to  satisfy  this  condition.  Observe  that  there  is  a  counterpart  λmin,α β(Φ;K)=inf{|Φ(Z)|α:  Z  in  K  and  |Z|β=1} 

*: |Z|F=1}for MP(F)

*: |z|2=1}for MP(2)

y,Φ,η or Λmin(ΦS

T)=inf{|||ΦS

T(Z)|||1

(e.g., see FACT2.6) which is also critical for analysis in next sections in random measurement setting.   

5    Number of Measurements for Robust Reconstruction via Solving MP(2)

y, Φ, η   

After  established  the  conditions  for  the  measurement  operator  to  guarantee  desired  properties  (e.g., 

uniqueness,  robustness,  etc.)  of  the  matrix  signal  reconstruction,  next  question  should  be  about  how  to 

construct  the  measurement  operator  to  satisfy  such  conditions  with  required  number  of  measurements  as 

few  as  possible.  This  and  next  sections  deal  with  this  problem  in  random  approach.  In  this  section  we 

establish  conditions  on  number  of  measurements  m  for  robustly  reconstructing  the  matrix  signal  X  by 
solving the convex programming problem MP(2)

y, Φ, η:   

inf |||Z|||1    s.t Z ∈ 𝑅𝑛×𝑛, |y – Φ(Z)|2 ≤ η   
where  y  =  Φ(X)  +  e  and  |e|2 ≤  η,  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚  is  a  linear  operator.  In  the  equivalent  component-wise 
formulation,  yi =  <Φi,X>  +  ei  where  for  i=1,…,m,  Φi∈ 𝑅𝑛×𝑛
  are  random  matrices  independent  each  other 
and Φi’s entries are independently sampled under standard Gaussian N(0,1) or sub-Gaussian distribution. In 
section 5.3, a simple necessary condition on m is established.   

Instead of proving that M-RIP property (defined in sec. 3.5) can be satisfied with high probability in 

context of the distributions in consideration, which are quite involved particularly for problem MP(F)
Y,A,B,η, 
here we straightforwardly prove that the robustness in terms of Frobenius-norm error metric can be reached 

with  high  probability  when  number  of  measurements  exceeds  some  specific  bound.  The  investigation  on 

how those conditions established in last sections can be satisfied by the random measurement operator will 

be subjects in subsequent papers.   

5.1    Case 1: Gaussian Measurement Operator Φ   

Based  upon  the  fundamental  facts  presented in  section  2.2,  one  of  the  critical  steps in this approach  is to 

estimate  the  width  w(D(|||X|||1,  X))’s  upper  bound  for  matrix  signal  X=(x1,…,xn)  with  s-sparse  column 
vectors x1,…,xn. This is done in lemma 5.1.     
          Based  on  lemma  3.1  and  FACT  2.4,  the  upper  bound  of  Gaussian  width  D(|||.|||1,  X)  with respect to 
Frobenius norm is estimated in the following lemma.   

Lemma  5.1    Given  n-by-n  matrix  X=(x1,…,xn)  with  s-sparse  column  vectors  x1,…,xn.  Let  r  (called 
l1-column-flatness  parameter  hereafter)  be  cardinality  of  the  set  {j:  |xj|1=maxk|xk|1},  i.e.,  the  number  of 
column vectors which have the maximum l1-norm. Then   

w2(D(|||.|||1, X)) ≤ 1+n2–r(n–slog(Cn4r2))                                              (5.1) 

In particular, when r = n then   

where C is an absolute constant.   

w2(D(|||.|||1, X)) ≤ 1 + nslog(Cn6)                                                  (5.2) 

Remark 5.1: ns is the total sparsity of the matrix signal X. This estimate shows that the  signal complexity 

15 

 
(width) encoded by regularizer |||X|||1 is controlled by two structural parameters, the column-sparsity s and 
l1-column-flatness r. Complexity gets lower with smaller s and larger r.   

Proof    We  start  with  (FACT  2.4)  w2(D(|||.|||1,  X))  ≤  EG[inf{|G–tV|F
random matrix with entries Gij~iid N(0,1).   

2:  t>0,  V  in  ∂|||X|||1}]  where  G  is  a 

Set G=(g1,…,gn) where gj~iid N(0,In). By lemma 3.1, V=(λ1ξ1,…, λnξn) where w.l.o.g. λj≥0 for j=1,…,r, 
λ1+…+λr=1, λj=0 for j≥r+1; |xj|1=maxk|xk|1 for j=1,…,r and |xj|1<maxk|xk|1 for j≥1+r; ξj(i)=sgn(Xij) for Xij≠0 
and |ξj(i)| ≤1 for all i and j. Then             

w2(D(|||.|||1, X)) ≤ EG[inf t>0, λj, ξj specified as the above  ∑𝑟
𝑗=1
≤ inf t>0, all λj specified as the above EG[inf all ξj specified as the above  ∑𝑟
= inf t>0, all λj specified as the above EG[inf all ξj specified as the above  ∑𝑟
= inf t>0, all λj specified as the above EG[∑𝑟

𝑗=1

𝑗=1

| gj – tλjξj |2

2    +  ∑𝑛

| gj – tλjξj |2
| gj – tλjξj |2

𝑗=1
                                                      ( since ξj is unrelated each other and EG[|gj|2
        = inf t>0, all λj specified as the above  ∑𝑟

𝑗=1 Egj[inf ξj specified as the above | gj – tλjξj |2

inf ξj specified as the above | gj – tλjξj |2

𝑗=𝑟+1

𝑗=𝑟+1
2    +  ∑𝑛
2]+∑𝑛
2] + (n–r)n 
2]=n )   
2] + (n–r)n   

|gj|2

2]   
|gj|2

2 ]   

𝑗=𝑟+1 EG[|gj|2

2]     

2 + |gj|~S(j) – tλjξj|~S(j)|2

For each j=1,…,r let S(j) be the support of xj (so |S(j)| ≤ s) and ~S(j) be its complimentary set, then |gj –
2. Notice that all components of ξj|S(j) are  ±1 and all components 
2=|gj|S(j) – tλjξj|S(j)|2
tλjξj|2
of ξj|~S(j)    can be any value in the interval [–1,+1]. Select λ1=…=λr =1/r, let ε>0 be arbitrarily small positive 
number and select t=t(ε) such that P[|g|>t(ε)/r]≤ε where g is a standard scalar Gaussian random variable (i.e., 
g~N(0,1) and ε can be exp(–t(ε)2/2r2)). For each j and each i outside S(j), set ξj’s component ξj(i) = rgj(i)/t(ε) 
if |gj(i)| ≤ t(ε)/r (in this case |gj(i) – tλjξj|(i)| = 0) and otherwise ξj(i)=sgn(gj(i)) (in this case |gj(i) – tλjξj|(i)| = 
2=0 when |gj|~S(j)|∞ < t(ε)/r, hence:   
|gj(i)| –t(ε)/r), then |gj|~S(j) – tλjξj|~S(j)|2
∞
∞
2] =  ∫ 𝑑𝑢
E[ |gj|~S(j) – tλjξj|~S(j)|2
0
0

uP[ |gj|~S(j) – tλjξj|~S(j)|2>u]   

P[ |gj|~S(j) – tλjξj|~S(j)|2

2>u] = 2∫ 𝑑𝑢

∞
≤ 2 ∫ 𝑑𝑢
0

≤ 2(n–s) ∫ 𝑑𝑢

uP[There exists (gj|~S(j) – tλjξj|~S(j))’s component with magnitude > (n–s)-1/2u ]   
∞
0

uP[ |g| –t(ε)/r > (n–s)-1/2u ]   
uexp(–((t(ε)/r)+ (n–s)-1/2u)2/2)   

∞
0

≤ 2(n – s) ∫ 𝑑𝑢
≤ C0(n–s)2exp(–t(ε)2/2r2 ) ≤ C0(n–s)2ε   

where C0 is an absolute constant. On the other hand:   

            Egj[|gj|S(j) –tλjξj|S(j)|2

2 = (1+ t(ε)2/r2)s=(1+2log (1/ε))s       
Hence          w2(D(|||.|||1, X)) ≤ (1+2log (1/ε))rs + (n–r)n + r(n–s)2ε ≤ n2 – r(n–slog(e/ε2)) + C0n2rε   
In particular, let ε=1/C0n2r then we get w2(D(|||.|||1, X)) ≤ n2 – r(n – slog(Cn4r2)) + 1.                                  □   

2] = Egj[|gj|S(j)|2] + (t(ε)2/r2)|ξj|S(j)|2

          Combing this lemma and FACT 2.3, we obtain the general result in the following:   

Theorem  5.1    Suppose  Φkij  ~iid N(0,1), let  X∈ ∑𝑠
signal,  𝑅𝑚 ∋y = Φ(X) + e where |e|2 ≤ η, X* be the minimizer of the problem MP(2)
vector y’s dimension   

𝑛×𝑛  be  a  columnwise  s-sparse and  l1-column-flat matrix 
y, Φ, η. If the measurement 

m ≥ (t + 2η/δ + (nslog(Cn6))1/2)2 
where  C is  an absolute  constant,  then  P[|X*–X|F    ≤ δ  ]  ≥  1 –  exp(–t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*–X|F with high probability by solving MP(2)

y,Φ,η.                        □ 

5.2    Case 2: Sub-Gaussian Measurement Operator Φ     

Combing lemma 5.1, FACT 2.1(2) and 2.5, the following result can be obtained straightforwardly:   
Theorem  5.2    Let  X  and  X*  be respectively  the  matrix  signal  and  the  minimizer  of  MP(2)
y, Φ, η  where  the 
𝑛×𝑛  is  columnwise  s-sparse  and  l1-column-flat,  yk=<Φk,X>+ek,  each  Φk  ~iid Φ  where  Φ  is  a 
signal  X∈ ∑𝑠
random  matrix  satisfying  the  conditions  (1)(2)(3)  in  FACT  2.5  with  parameters  α,  ρ  and  σ.  If  the 

16 

 
measurement vector y’s dimension   

m ≥ (C1ρ4/α)( αt + 2η/δ + σC2(ρ6nslog(C3n6))1/2)2     
where  Ci’s are  absolute  constants,  then  P[|X*–X|F ≤  δ]  ≥  1 –  exp(–C4t2), i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*–X|F with high probability by solving MP(2)
y,Φ,η.                          □ 

5.3    Necessary Condition on Number of Measurements   

(μ)∩Sj

(α)∪S2

(β)∪…∪Sn

y,Φ,η where η=0, then 

(μ)∩Sj
(α)∪S2
Now we call the above S1

Theorem  5.3    Given  measurement  operator  Φ:  𝑅𝑛×𝑛 → 𝑅𝑚,  if  any  X=(x1,…,xn)  with  sparse  column 
vectors xj in ∑2S for all j is always the unique solution to problem MP(α)
                                                                    m ≥ C1nslog(C2n/s))                                                              (5.3) 
where C1 and C2 are absolute constants.   
(ω) in {(i,j): 1≤i, j≤n } where 
Proof    For any s<n, there exist k ≥ (n/4s)ns/2 subsets S(αβ…ω)=S1
(ν)|  <  s/2  for  μ≠ν.  This  fact  is  based  on  a 
(μ)={(i1,j),…,  (is,j):  1≤i1<i2<…<  is≤n}  and  |Sj
each  Sj
combinatorial theorem[11] that for any s<n there exist l ≥ (n/4s)s/2 subsets R(μ) in {1,2,…,n} where |R(μ)∩R(ν)| 
< s/2 for any μ≠ν. For the n-by-n square {(i,j): 1≤i, j≤n }, assign a R(μ) to each column, i.e., set Sj
(μ):={ (i, j): 
(ν)| < s/2 for μ≠ν since |R(μ)∩R(ν)| < s/2 for μ≠ν and totally there can be k=ln such 
i∈R(μ)}. As a result |Sj
(β)∪…∪Sn
assignments S(αβ…ω)=S1
(α)∪S2

(ω) a configuration on the n-by-n square. Let m be the rank of 
linear operator Φ. Consider the quotient space L:=𝑅𝑛×𝑛/kerΦ, then dimL=n2–dimkerΦ=m. For any [X] in L 
define  the  norm  |[X]|:=inf{|||X–V|||1:  V  in  kerΦ}.  For  any  X=(x1,…,xn)  with  xj  in  ∑2S  for  all  j  ,  the 
assumption about Φ implies |[X]|=|||X|||1. Now for any configuration Δ=S1∪S2∪…∪Sn on the n-by-n square, 
  and  0  otherwise,  then  |||X(Δ)|||1=1,  each  X(Δ)’s  column  xj(Δ)∈∑S  and  each 
define  Xij(Δ):=1/s  if  (i,j)∈Sj
column of X(Δ’)–X(Δ”) is in ∑2S, furthermore |[X(Δ’)]–[X(Δ”)]|=|||X(Δ’)–X(Δ”)|||1>1 because of the property 
|Sj’∩Sj”|  <  s/2  for  Sj’≠Sj”.  These  facts  imply  that  the  set  Θ:={[X(Δ)]:  Δ  runs  over  all  configurations}  is  a 
subset on normed quotient space L’s unit sphere with distances between any pair of its members >1, i.e., a 
d-net  on  the  sphere  where  d>1.  The  cardinality  of  Θ  =  number  of  configurations  k  ≥  (n/4s)ns/2  and  an 
elementary estimate derives k ≤ 3dimL=3m, hence m ≥ C1nslog(C2n/s)) where C1=1/2log3 and C2=1/4.        □   

(ω) on the square.         

(β)∪…∪Sn

Remark 5.2: For the measurement operator ΦA,B:  𝑅𝑛×𝑛 → 𝑅𝑚×𝑚:Y=AXBT, the same result is true with m2 
≥ C1nslog(C2n/s)).   

6    Number of Measurements for Robust Reconstruction via Solving MP(F)

Y, A, B, η   

Now we investigate the problem MP(F)
Y, A, B, η in which A and B are both Gaussian or sub-Gaussian m-by-n 
random  matrices.  Notice  that  in  these  cases  the  measurement  operator  ΦA,B  is  neither  Gaussian  nor 
sub-Gaussian. The basis is FACT 2.6 and the critical step is also the width’s upper bound estimation.   

More  explicitly,  ΦA,B:  𝑅𝑛×𝑛 → 𝑅𝑚×𝑚 :  Y=AZBT.  In  the  equivalent  component-wise  formulation, 
ykl=<Φkl,X>=∑ijAkiXijBlj  for  each  1≤k,l≤m,  Aki  ~iidBlj ~iidN(0,1)  or  sub-Gaussian  distribution,  and  A,  B  are 
independent  each  other.  Let  εkl  ~iid  Rademacher  random  variable  ε  (P[ε= ± 1]=1/2)  which  are  also 
independent of A and B. The width is defined as (FACT 2.6)   

                                      W(ΓX; ΦA,B):=EH[sup{<H,U>: U in ΓX and |U|F=1]                                        (6.1) 
where                                              H:=m-1∑klεklΦkl =m-1ATEB and E=[εkl]                                                (6.2)   
and ΓX = D(|||.|||1,X) in our applications.   

Let  εl:  l=1,…,m  be  column  vectors  of  Rademacher  matrix  [εkl],  then  H=(h1,…,hn)  has  its  column 

vectors hj=∑ 𝑚−1

𝑚
𝑙=1

BljATεl  ∈ 𝑅𝑛. Notice that in problem MP(F)

Y, A, B, η, m2 is the measurement dimension. 

17 

 
6.1    Case 1: A and B are both Gaussian     

Lemma 6.1 Given n-by-n matrix X=(x1,…,xn) with s-sparse column vectors x1,…,xn, r is cardinality of {j: 
|xj|1=maxk|xk|1}, ΦA,B, ΓX, W(ΓX; ΦA,B) are specified as the above and Aki ~iidBlj ~iidN(0,1), then   

W2(ΓX; ΦA,B) ≤ 1+n2–r(n–slog2(cn2r))   

where c is an absolute constant. Particularly, when r = n ( i.e., X is sparse and l1-column-flat ) then   
                                                                W2(ΓX; ΦA,B) ≤ 1 + nslog2(cn3)                                                  (6.3) 

Proof    We  start  with  a  similar  inequality  as  that  in  FACT  2.4  (the  proof  is  also  similar)  W2(ΓX;  ΦA,B)  ≤ 
2: t>0,  V  in ∂|||X|||1}].  With the  same  specifications  for  V=(λ1ξ1,…,  λnξn) as  those  in lemma 
EH[inf{|H–tV|F
5.1,  i.e.(w.l.o.g.)  λj  ≥  0  for  j=1,…,r,  λ1+…+λr=1,  λj=0  for  j≥r+1;  |xj|1=maxk|xk|1  for  j=1,…,r  and  |xj|1  < 
maxk|xk|1 for j≥1+r; ξj(i)=sgn(Xij) for Xij≠0 and |ξj(i)| ≤1 for all i and j. Let hj ≡  ∑ 𝑚−1
2]   

W2(ΓX; ΦA,B) ≤ EA,B,E[inf t>0, λj, ξj specified as the above  ∑ |

𝑚
𝑙=1

BljATεl , we have             

𝑗=1 ∑𝑚
𝑛
2] + EA,B,E[inf t>0, λj, ξj specified as the above  ∑

𝑙=1 m-1BljATεl – tλjξj |2
2]   
𝑟
𝑗=1   hj – tλjξj |2

|

=  ∑𝑛

𝑗=𝑟+1 EA,B,E[| hj |2

= I + II 

The first and second terms are estimated respectively. The first term 
I =  ∑𝑛

TAATεk] = m-2(n–r) ∑𝑚
To  estimate  II,  for  each  j=1,…,r  let  S(j)  be  the  support  of  xj  (so  |S(j)|  ≤  s)  and  ~S(j)  be  its 

𝑙,𝑘=1 EB[BljBkj]EA,E[εl

TAATεl] = (n–r)n       

𝑗=𝑟+1 m-2∑𝑚

δlkEA,E[εl

𝑙,𝑘=1

complimentary set, then 

∑

𝑟
𝑗=1   hj – tλjξj |2

|

2] =  ∑ |

𝑟
𝑗=1   hj |S(j)    – tλjξj|S(j) |2

2] +  ∑ |

𝑟
𝑗=1   hj |~S(j) – tλjξj|~S(j) |2

2]   

Notice  that  all  components  of  ξj|S(j)  are  ±1  and  all  components  of  ξj|~S(j)    can  be  any  value  in  the 
interval  [–1,+1].  Select  λ1=…=λr  =1/r,  let δ>0  be  arbitrarily  small  positive  number and  select  t=t(δ)  such 
that  PA,B,E[|h|  > t(δ)/r]  ≤ δ  where  h is  a random  scalar  such  that  hj(i)~h and i indicates  the  vector  hj’s  i-th 
component.  For  each j  and  i  outside  S(j),  set  ξj’s  component ξj(i)=rhj(i)/t(ε) if  |hj(i)|  ≤ t(δ)/r and  otherwise 
2=0  when  |hj|~S(j)|∞ <  t(δ)/r  and  notice  the  fact  that  for  independent 
ξj(i)=sgn(hj(i)),  then  |hj|~S(j) –  tλjξj|~S(j)|2
standard  scalar  Gaussian  variables  al,  bl  and  Rademacher  variables  εl,  l=1,…,m,  there  exists  absolute 
constant c such that for any η > 0:   

                                              P[| m-1∑𝑚

𝑙,𝑘=1

blakεk | > η] < c exp(–η)                                            (6.4) 

as a result, in the above expression δ can be c exp(–t(δ)/r) and:       
∞
2 ] =  ∫ 𝑑𝑢
0

E[ |hj|~S(j) – tλjξj|~S(j)|2

P[ |hj|~S(j) – tλjξj|~S(j)|2

2 > u] = 2∫ 𝑑𝑢

∞
0

        ≤ 2 ∫ 𝑑𝑢

∞
0

≤ 2(n–s) ∫ 𝑑𝑢

uP[There exists (hj|~S(j) – tλjξj|~S(j))’s component with magnitude > (n–s)-1/2u ] 
∞
0

uP[ |h| –t(δ)/r > (n–s)-1/2u ]   
uexp(–((t(δ)/r)+ (n–s)-1/2u))       

∞
0

≤ 2(n – s) ∫ 𝑑𝑢
≤ C0(n–s)2exp(–(t(δ)/r)) ≤ C0(n–s)2δ   

uP[ |hj|~S(j) – tλjξj|~S(j)|2 > u]   

where C0 is an absolute constant. On the other hand |ξj|S(j)|2

2

 ≤ s for j ≥ 1+r so:     

𝑟
𝑗=1   hj |S(j)    – tλjξj|S(j) |2

2]   

𝑟
𝑗=1   hj |S(j)    – t(δ)ξj|S(j)/r |2

2]   

EA,B,E[inf t>0, λj, ξj  ∑ |
|
𝑗=1 EA,B,E[ m-2| ∑𝑚

≤ EA,B,E[ ∑
≤  ∑𝑟
= rs(1+t(δ)2/r2)   

𝑙=1 Blj(ATεl)|S(j) |2

2 ] + rst(δ)2/r2       

hence II ≤ rs(1+t(δ)2/r2) + nrδ. Combine all the above estimates we have:   

W2(ΓX; ΦA,B) ≤ I + II ≤ (n–r)n + rs(1+t(δ)2/r2) + C0n2rδ = n2 – r(n–s(1+t(δ)2/r2) + C0n2rδ   
Substitute t(δ)/r with log(c/δ) we get, for any δ > 0:   

W2(ΓX; ΦA,B) ≤ n2–r(n–s(1+log2(c/δ)) + C0n2rδ   
In particular, let δ=1/C0n2r then W2(ΓX; ΦA,B) ≤ n2–r(n–s(1+log2(cn2r)) + 1.                                                □ 

To apply FACT 2.6 completely, now estimate the lower bound of Qξ(ΓX; ΦA,B):= inf {P[|<Φkl,U>| ≥ ξ]: 

18 

 
U in ΓX and |U|F=1} for some ξ > 0 where <Φkl,U>=∑ijAkiUijBlj for each 1≤k,l≤m, Aki ~iidBlj ~iidN(0,1). The 
estimate is independent of the indices k and l, so we give a general and notational simplified statement on it.     

Lemma  6.2    Let  Mij =  aibj  where  ai ~iidbj ~iidN(0,1) and all random  variables are independent  each  other, 
there exists an positive absolute constant c such that inf {P[|<M,U>| ≥ 1/√2]: U in ΓX and |U|F=1} ≥ c, as a 
result  𝑄1/√2(ΓX; ΦA,B) ≥ c.     

Proof    By the second moment inequality P[Z ≥ ξ] ≥ (E[Z] – ξ) +
ξ > 0. Set Z = |<M,U>|2 and ξ = E[|<M,U>|2]/2, we get: 

2/E[Z2] for any non-negative r.v. Z and any 

P[|<M,U>|2 ≥ E[|<M,U>|2]/2] ≥ E[|<M,U>|2]2/4E[|<M,U>|4]                (6.5) 

To  estimate  the  upper  bound  of  E[|<M,U>|2],  let  U=∑jλjujvj  be  U’s  singular  value  decomposition, 
Tvj=δij,  λj>0  for  each  j.  Notice  that  M=abT  where  a~b~N(0,  In)  and  independent  each  other,  then 
Tb~N(0,1)  and  independent  each  other,  hence  E[|<M,U>|2]  = 

Tuj=vi

ui
<M,U>  =  aTUb  =  ∑jλjaTujvj
Tb|2] = ∑jλj
∑jλj

2E[|aTuj|2]E|vj

Tb where  aTui~vj
2 = |U|F

2=1 for U in the assumption. 
On the other hand by Gaussian hypercontractivity we have 

In conclusion P[|<M,U>|2 ≥ 1/2] = P[|<M,U>|2 ≥ E[|<M,U>|2]/2] ≥ c for U: |U|F

2=1.      □ 

(E[|<M,U>|4])1/4 ≤ C0(E[|<M,U>|2])1/2 = C0   

Combing lemma 6.1, 6.2 and FACT 2.6, we obtain the general result in the following.   

Theorem 6.1    Suppose Aki ~iidBlj ~iidN(0,1) and independent each other, X∈ ∑𝑠
and l1-column-flat signal, Y = AXBT + E  ∈ 𝑅𝑚×𝑚  with measurement errors bounded by |E|F
minimizer of the problem MP(F)

𝑛×𝑛  is a columnwise s-sparse 
2 ≤ η, X* is the 

Y, A,B, η. If   

m ≥ t + 4√2η/δ + C1(ns)1/2log(C2n3) 
where  Ci’s  are  absolute  constants,  then  P[|X*–X|F ≤  δ]  ≥  1  –  exp(–t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*–X|F with high probability by solving MP(F)
Y,A,B,η.                      □ 

6.2    Case 2: A and B are both sub-Gaussian 

Lemma 6.3    Given n-by-n matrix X=(x1,…,xn) with s-sparse column vectors x1,…,xn, r is cardinality of {j: 
|xj|1=maxk|xk|1}, ΦA,B, ΓX, W(ΓX; ΦA,B) are specified as before, Aki ~iid Sub-Gaussian distribution and Blj ~iid 
Sub-Gaussian distribution with ψ2-norms σA, σB respectively, then   

W2(ΓX; ΦA,B) ≤ σA

2σB

2(1+n2–r(n–slog2(Cn2r)))                                        (6.6) 

where C is an absolute constant. Particularly, when r = n then   

W2(ΓX; ΦA,B) ≤ σA

2σB

2(1 + nslog2(Cn3))                                            (6.7) 

The proof  of this lemma is logically the same as the proof of lemma 6.1, the only difference is about 
𝑚
the distribution tail of the components of vectors hj ≡  ∑ 𝑚−1
blakεk with 
𝑙=1
independent scalar sub-Gaussian variables al, bl and Rademacher variables εl, l=1,…,m. This auxiliary result 
is presented in the following lemma:   

BljATεl which ~iid h ≡ m-1∑𝑚

𝑙,𝑘=1

Lemma 6.4    For independent scalar zero-mean sub-Gaussian variables al, bl and Rademacher variables εl, 
l=1,…,m, let σA≡maxl|al|ψ2, σB≡maxl|bl|ψ2 (|.|ψ2 denotes a sub-Gaussian variable’s ψ2-norm), then there exists 
absolute constant c such that for any η > 0:   

                                                P[| h | > η] < 2 exp(–cη/σAσB)                                                      (6.8)   

Proof    Notice  that  akεk  is  zero-mean  sub-Gaussian  variable  with  |akεk|ψ2=|ak|ψ2,  for  b=m-1/2∑1≤l≤mbl  and 
a=m-1/2∑1≤k≤makεk we have |b|ψ2 ≤ Cm-1/2(∑l|bl|ψ2
2)1/2 ≤ CσA where C is an 
absolute  constant.  Furthermore,  because  the  product  of  two  sub-Gaussian  variables  a  and  b  is 
19 

2)1/2 ≤ CσB and |a|ψ2 ≤ Cm-1/2(∑l|ak|ψ2

 
sub-Exponential and its ψ1-norm |ba|ψ1 ≤ |b|ψ2 |a|ψ2 ≤ C2σAσB, h ≡ m-1∑𝑚
tail P[| h | > η] < 2exp(–cη/σAσB) where c is an absolute constant.   

𝑙,𝑘=1

blakεk = ab has its distribution 

Proof  of  lemma  6.3    With  the  same  logic  as  in  the  proof  of  lemma  6.1  and  based-upon  lemma  6.4,  the 
auxiliary  parameter  δ  in  the  argument  can  be  2exp(–ct(δ)/rσAσB)  and  equivalently  t(δ)/r  =  σAσBlog(2/δ) 
which derives the final result.    □     

Lemma 6.5    Let Mij = aibj where ai ~iid Sub-Gaussian distribution, bj ~iid Sub-Gaussian distribution and all 
are independent each other, then  𝑄1/√2(ΓX; ΦA,B) ≥ c where c is a constant only dependent on σAσB.          □ 
        Lemma 6.5’s proof is almost the same as that of lemma 6.2. Now the following result can be obtained 

directly by combing lemmas 6.3-6.5 and FACT 2.6:   

Theorem 6.2    Suppose random matrices A, B are independent each other, Aki ~iid Sub-Gaussian distribution, 

Blj ~iid Sub-Gaussian distribution, each with ψ2-norm σA and σB. Let X∈ ∑𝑠
l1-column-flat signal, Y = AXBT + E  ∈ 𝑅𝑚×𝑚 with |E|F

2 ≤ η, X* be the minimizer of MP(F)

Y, A,B, η. If 

𝑛×𝑛  be a columnwise s-sparse and 

                                                        m ≥ t + 4√2η/δ + C1σAσB(ns)1/2log(C2n3)   
where  Ci’s  are  absolute  constants,  then  P[|X*–X|F ≤  δ]  ≥  1  –  exp(–t2/2),  i.e.,  such  X  can  be  reconstructed 
robustly with respect to the error norm |X*–X|F with high probability by solving MP(F)
Y,A,B,η.                      □ 

7    Conclusions, Some Extensions and Future Works   

In  this  paper  we  investigated the  problem  of  reconstructing  n-by-n  column-wise  sparse and  l1-column-flat 
matrix  signal  X=(x1,…,xn)  via  convex  programming  with  the  regularizer  |||X|||1:=maxj|xj|1  where  |.|1  is  the 
l1-norm  in  vector  space.  In  the  first  part  (sec.3  and  sec.4),  the  most  important  conclusions  are  about  the 
general  conditions to  guarantee  uniqueness,  value-robustness,  support  stability  and  sign  stability  in  signal 

reconstruction.  In  the  second  part  (sec.5  and  sec.6)  we  took  the  convex  geometric  approach  in  random 

measurement  setting  and  established  bounds  on  dimensions  of  measurement  spaces  for  robust 

reconstruction in noise. For example, typical results show that the signal complexity (width) encoded by the 

regularizer |||X|||1 is determined by two structural parameters, i.e., the maximum number s of nonzero entries 
in each column (column-wise sparsity )and the number r of columns which l1-norms are maximum among 
all columns (l1-column-flatness). The signal’s complexity decreases  with small s and large r. In particular, 
when r = n (i.e., sparse and flat signal) the condition reduces to m ≥ t + 4√2η/δ + C1σAσB(ns)1/2log(C2n3).   
        Based on the methods and results obtained in this paper, we can make some straightforward extensions 

to other similar problems. The first extension is about reconstructing row-wise sparse and l1-row-flat matrix 
signals via convex programming with regularizer |||XT|||1:= maxi|𝒙𝒊|1 where  𝒙𝒊  is matrix signal X’s i-th row. 
In  this  case  all  the  obtained  estimates  and  conclusions  remain  the  same,  e.g.,  the  signal’s  complexity  is 

determined  by  the  maximum  number  s  of  nonzero  entries  in  each  row  and  the  number  r  of  rows  which 

l1-norms are maximum among all rows. The signal’s complexity decreases with small s and large r.     

The second extension is for reconstructing the matrix signal which has both row-wise and column-wise 
sparsity  and  l1-flatness.  We  can  use  F(X):=max(|||X|||1,|||XT|||1)  as  the  regularizer.  Note  that  when  |||X|||1 ≥ 
|||XT|||1 then F(X) = |||X|||1 so in this case W(ΓX; Φ) is the width determined by regularizer |||X|||1, otherwise 
W(ΓX;  Φ)  is  the  width  determined  by  regularizer  |||XT|||1,  both  have  the  upper  bound  of  𝑊(r1,s1,n)  and 
𝑊(r2,s2,n) in the form as that in lemma 5.1 or lemma 6.1 where (r1,s1) and (r2,s2) are the matrix signal’s row 
and  column  structural  parameters.  As  a  result,  the  sufficient  condition  for  robust  reconstruction  is  m  ≥ 
max(𝑊2(r1,s1,n),  𝑊2(r2,s2,n)) via MPy,Φ,η or m2 ≥ max(𝑊2(r1,s1,n),  𝑊2(r2,s2,n)) via MPY,A,B,η .     

20 

 
The  third  extension  is  for  reconstructing  the  matrix  signal  with  the  general  linear  measurement  Y  = 
  where  Hμ  := 
𝑋𝐵𝜇
(μ)] is the matrix with independent Rademacher entries.  Suppose matrices Aμ’s and 

ΦA,B(X)  +  E  =  ∑
m-1Aμ
Bμ’s are both Sub-Gaussian, in this case (see (4.1)) the width   

𝑇  +  E  where  |E|F≦η.  In  this  case  (see  (6.2))  H  =  ∑

𝐴𝜇
TEμBμ and Eμ = [εkl

𝐿
𝜇=1

𝐿
𝜇=1

𝐻𝜇

  W(ΓX; ΦA,B) = EH[sup{∑𝐿
≤ C0L1/2 maxμEH[sup{<Hμ,U>: U in ΓX and |U|F=1]       

𝜇=1 <Hμ,U>: U in ΓX and |U|F=1]   

2(n2–r(n–slog2(C1n2r)) 
2σB
and each EH[sup{|<Hμ,U>|2: U in ΓX and |U|F=1] has the upper bound estimate σA
according to lemma 4.3, finally we get the width estimate W2(ΓX; ΦA,B) ≤C0LσA
2(n2–r(n–slog2(C1n2r)). As 
2σB
a result, a  sufficient  condition  for  column-wise  sparse and l1-column-flat  signal  X to  be  reconstructed  via 
solving  MP(F)
Y,A,B,η  robustly  with  respect  to  Frobenius  norm  |.|F  from  linear  measurement  Y  = 
∑
𝐴𝜇
This paper is only focused on basic theoretical analysis. In subsequent papers, the algorithms to solve 

𝑇+E where |E|F≦η is m ≥ t + 4√2η/δ + C1LσAσB(ns)1/2log(C2n3).   

𝑋𝐵𝜇

𝐿
𝜇=1

the  |||.|||1-optimization  problems  (e.g.,  generalized  inverse  scale  space  algorithms,  etc.),  related  numeric 
investigations and applications (e.g.,in radar space-time waveform analysis)will be further investigated[14-15].     

REFERENCES     

[1]S.Foucart, H.Rauhut. A Mathematical Introduction to Compressive Sensing, Birkhaeusser, 2013. 

[2]Y.C.Eldar, G.Kutynoik(Ed). Compressed Sensing: Theory and Applications, Cambridge University Press,, 2012. 

[3]D.Cohen, Y.C.Eldar. Sub-Nyquist Radar Systems: Temporal, Spectral and Spatial Compression, IEEE Signal Processing Magazine, 

2018 Nov.,35-57. 

[4]M.A.Davenport, J.Romberg. An Overview of Low-Rank Matrix Recovery from Incomplete Observations, arXiv:1601.06422, 2016. 

[5]M.F.Duarte, R.G.Baraniuk. Kronecker Compressive Sensing, IEEE Transactions on Image Processing, 2012, 21(2):494-504. 

[6]G.Dasarathy, P.Shah, B.N.Bhaskar, R.Nowak. Sketching Sparse Matrices, arXiv:1303.6544, 2013. 

[7]V.Chandrasekaran, B.Recht, P.A.Parrio and A.S.Wilsky. The Convex Geometry of Linear Inverse Problems, Found. Comput. Math., 

2012, 12:805-849. 

[8]J.A.Tropp.  Convex  Recovery  of  a  Structured  Signal  from  Independent  Random  Linear  Measurements,  In:  Sampling  Theory:  A 

Renaissance: Compressive Sampling and Other Developments, Ed. by G.Pfander, Birkhaeusser, 2015. 

[9]S.Mendelson. Learning without Concentration. J. ACM., 2014, 62(3).   

[10]S.Mendelson,  A.Pajor,  N.Tomczak-Jaegermann.  Reconstruction  and  Subgaussian  Operators  in  Asymptotic  Geometric  Analysis, 

Geom. Func. Analysis, 2007, 17(4):1248-1282.   

[11]J.H.Van Lint, R.M.Wilson. A Course in Combinatorics, Springer-Verlag, 1995.   

[12]R.Vershynin. High-Dimensional Probability – with Applications to Data Science，Oxford University Press，2015.     

[13]M.Ledoux, M.Talagrand. Probability in Banach Space: Isopermetry and Processes, Springer, 1991.   

[14]Yuan T, Convex Reconstruction of Structured Matrix Signals from Linear Measurements(II): Algorithms, to appear.   

[15]Yuan T, Convex Reconstruction of Structured Matrix Signals from Linear Measurements(III): Applications, to appear.   

21 

 
 
 
 
 
