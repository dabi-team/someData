1
2
0
2
c
e
D
4
2

]
E
S
.
s
c
[

2
v
7
9
0
1
0
.
1
1
1
2
:
v
i
X
r
a

Code2Snapshot: Using Code Snapshots for
Learning Representations of Source Code

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

mrabin@uh.edu, maalipou@central.uh.edu
University of Houston, Houston, TX, USA

Abstract. There are several approaches to encode source code in the
input vectors of neural models. These approaches attempt to include var-
ious syntactic and semantic features of input programs in their encoding.
In this paper, we investigate Code2Snapshot, a novel representation
of the source code that is based on the snapshots of input programs. We
evaluate several variations of this representation and compare its perfor-
mance with state-of-the-art representations that utilize the rich syntactic
and semantic features of input programs.
Our preliminary study on the utility of Code2Snapshot in the code
summarization task suggests that simple snapshots of input programs
have comparable performance to the state-of-the-art representations. In-
terestingly, obscuring the input programs have insigniﬁcant impacts on
the Code2Snapshot performance, suggesting that, for some tasks, neu-
ral models may provide high performance by relying merely on the struc-
ture of input programs.

1

Introduction

Deep neural models are widely used to develop code intelligence systems [5, 27].
These models are powerful learning tools that provide a large hypothesis class
and a large capacity for learning almost any arbitrary function. However, the
representation of input programs as vectors can have a signiﬁcant impact on the
performance of the models.

Several representations for source code have been proposed in literature
[11, 25]. These approaches try to encode the various syntactic and semantic
features of input programs in the code embeddings. For example Code2Vec [9]
or Code2Seq [8] encode paths in the abstract syntax trees of programs, GGNN
[6] encodes the data and control dependencies of programs, and func2vec [14]
encodes paths in the extended call-graphs of programs.

The underpinning insight is that by encoding more information about the
program in the feature vectors, the code intelligence model would be able to
extract more patterns and insights, and are likely to perform better. Unfortu-
nately, due to the opacity of the neural models, it is unclear what features these
models capture from the input data. Studies suggest that models may rely on
variable names [12, 20] or few tokens [4, 21, 23] for their prediction, or they can
memorize data points for achieving high performance [2, 22].

 
 
 
 
 
 
2

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

In this paper, we explore a new source code representation, Code2Snapshot,
in which, instead of using the traditional syntactic or semantic program fea-
tures, we use program snapshots to represent any arbitrary code snippets.
Additionally, we study the redacted snapshots of input programs that merely
represent the structure of input programs. The main idea is that in real settings,
seasoned programmers may get a hunch by merely looking at the structure of
input programs.
We report

the result of our preliminary study on the utility of
Code2Snapshot for the method name prediction task
[4]. We compare
its performance with the complex state-of-the-art representations, Code2Vec
and Code2Seq, that use richer syntactic and semantic features based on
tokens and paths. Experiments on the most frequent labels suggest that the
simple snapshots of input programs can provide performance comparable to the
complex neural code embeddings. However, the gap between the performance of
Code2Snapshot and Code2Seq grows as we increase the number of labels.
Contributions. In this paper, we make the following contributions.

• We propose and evaluate several source code embeddings based on the snap-

shots and structure of the input programs.

• We provide a preliminary result on the evaluation of our proposed embed-

dings for the method name prediction task.

2 Related Work

Several source code embeddings have been proposed to encode source code as
an input vectors to neural networks [11]. For brevity, here, we mention the most
relevant to our work.

Allamanis et al. [3] introduce a framework that uses sequences of tokens to
represent the source code. Alon et al. [9] propose Code2Vec, a ﬁxed-length
representation of source code based on the bags of paths in the abstract syntax
tree. Allamanis et al. [6] propose GGNN that includes a richer feature space
including control-ﬂow and data-ﬂow for the source code.

Some works have been done in the area of using images of input programs for
overcoming feature extraction challenges. Dey et al. [15] propose an approach
for converting programs to images, where they use the intermediate representa-
tion of programs to generate a visual representation of input programs. Keller
et al. [18] apply diﬀerent visual representations of code and transfer learning for
code semantics learning. Bilgin [10] also uses a colored image of syntax trees for
representing input programs.

Unlike previous works, this work mainly focuses the model’s reliance merely

on the structure of input programs for intelligent code analysis.

3 Methodology

Figure 1 depicts a high-level view of our proposed approach. Given the dataset
of input programs, we reformat the original code and take snapshots from the

Code2Snapshot

3

Fig. 1: Workﬂow of the proposed Code2Snapshot approach.

reformatted code, and then train with Convolutional Neural Networks (CNNs)
for predicting target labels. Our approach contains following key steps.

3.1 Reformatting Code

The original input program is often poorly aligned and has severe indentation
problems such as extra spacing and newlines. It also includes random user com-
ments and documentations. As a result, the original input program is not suitable
for generating images, as we cannot get actual structural information from ran-
domly aligned and commented code. To address these problems, we reformat
the original code using JavaParser tool [13] that provides functions to analyze
Java code. We remove documentations and comments from input programs, and
adjust indentation of code by removing extra white spaces and line breaks.

3.2 Taking Snapshots

The Python Imaging Library (PIL) [1] provides image processing capabilities to
add text on image. After reformatting the original code, we use the PIL library
to take snapshots from the reformatted code. However, many programs contain
more than a hundred lines of code and very long statements. Thus, the generated
image from a larger program becomes extremely blurry when resized. For that
reason, we shorten the size of a program by limiting statements up to 30 lines
and a maximum of 120 characters per line. We simply ﬁlter out additional lines
and characters from programs. We follow the below steps to take snapshots.

• Given a sample program, we ﬁrst reformat the code and shorten the size of

a larger program by ﬁltering out extra lines and characters.

• Next, we create an image object with grayscale mode and white background
and set the size of the image as a ﬁxed window of 30 lines and 120 characters.
• We then adjust the spacing and font settings until ﬁnding a reasonable ﬁt
for the rendered text. In this work, we use ‘Times New Roman’ as default
font with 50 points as requested size and 50 pixels as line spacing.

• After that, we read the actual text from the reformatted program and write

the text on the image in black color.

• Finally, we convert the image object to PNG format and save it to a directory.

Now, instead of any complex learning of source code embeddings, we can feed
these images to models and train for a downstream task.

4

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

3.3 Training Models

We train multiple CNN models on the snapshots of input programs for predicting
target labels. Before training, we apply image transformation and resize each
image into “512 x 512” dimension in order to ensure that each image has the
same size. We also apply normalization on images to ensure that each pixel has
a similar data distribution. We train CNN models with the images of training
set and validation set, and compute the performance on the images of test set.
In this work, for method name prediction, we use the image of the method body
as input and the method name as target label to CNN models during training.

4 Experimental Settings

4.1 Task

We use the popular code summarization task [7], also known as method name
prediction (MethodName), to evaluate the Code2Snapshot representation.
The MethodName is a supervised learning task wherein the inputs are the body
of methods and the labels are the corresponding names. For example, given the
following code snippet: “void f(int a, int b) {int temp = a; a = b; b =
temp;}”, a trained model aims to predict the method’s name as “swap”.

4.2 Dataset

We use JavaTop10 and JavaTop50, subsets of popular Java-Large and Java-
Small datasets presented in [8]. The JavaTop10 dataset contains the top 10
most frequent methods from the Java-Large dataset. The training, validation,
and test set contains a total of 10000 (1000 instances per label), 4847, and
7100 examples, respectively. The JavaTop50 dataset contains the top 50 most
frequent methods from the Java-Small dataset. The training, validation, and
test set contains a total of 27119, 1803, and 4265 examples, respectively.

4.3

Input Types

Image. Using PIL [1], we generate three variations of the Code2Snapshot
representation: a) Original, b) Reformatted, and c) Redacted. Figure 2
depicts examples of these representations.

• Original: We use the original input programs as is to construct the vi-
sual representation of input programs. Figure 2a shows an example of this
representation where comments, extra lines and spaces remain unchanged.
• Reformatted: We reformat the input programs to adjust indentation, re-
move comments. Moreover, for long methods, we include the ﬁrst 30 lines of
code. Figure 2b shows an example of the reformatted input program derived
from the original input program of Figure 2a.

(a) Original

(b) Reformatted

(c) Redacted

Code2Snapshot

5

Fig. 2: Example of diﬀerent code snapshots.

• Redacted: We replace any alphanumeric characters in the reformatted in-
put programs with “x”. Punctuations and mathematical operators remain
intact. For example, “int i = 2;” becomes “xxx x = x;”. Figure 2c is the
example of Redacted on the reformatted code of Figure 2b.

Token. Using JavaParser [13], we traverse the abstract syntax tree of source
code and collect all tokens from the method body. We then represent a source
code as a sequence of tokens. For example, [int, i, =, 2, ;] is the list of
tokens for an expression “int i = 2;”.

Path. Using JavaExtractor [8, 9], we extract and preprocess a bag of path
contexts from Java ﬁles. For example, a path context for the expression “int i
= 2;” is “i, <NameExpr ↑ AssignExpr ↓ IntegerLiteralExpr>, 2”.

4.4 Models

Image-based Models. We use two classic convolutional networks: AlexNet
(an 8-layers deep convolutional neural networks [19] that consists of ﬁve convo-
lutional layers, then two fully-connected hidden layers, and one fully-connected
output layer) and ResNet (an 18-layer residual networks [16] that consists of
one convolutional input layer, then four modules made up of two residual blocks
with two convolution layers in each block, and a ﬁnal fully connected layer).

Token-based Models. We create a simple recurrent neural network as baseline
for learning method names from the sequence of tokens [3, 17] in method body.
The network includes an embedding layer followed by a 2-layers bi-directional
LSTMs and a ﬁnal fully connected linear layer with softmax activation function.
Path-based Models. We use two popular path-based models for Method-
Name task: Code2Vec [9] and Code2Seq [8]. The models use a bag of paths
between two terminal nodes in the abstract syntax tree (AST) for representing an
arbitrary source code as an embedding vectors. While Code2Vec considered an

6

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

entire path as single entry and learns monolithic path embeddings, Code2Seq
sub-tokenized each path and uses LSTMs to encode paths node-by-node.
Training Models. For each combination of dataset, model, and representation,
we train a model up to 100 epochs. For training image-based models and token-
based models, we use stochastic gradient descent optimizer and cross-entropy loss
function. We train both AlexNet and ResNet with the original conﬁgurations
and only changed the number of target classes in the output layer. For path-based
models, we train both Code2Vec and Code2Seq with the conﬁgurations used
in the original work, except for the batch size of 128.

5 Results

Table 1 shows the performance of diﬀerent models and code embeddings for both
JavaTop10 and JavaTop50 datasets. The underlined values denote the best
result in Code2Snapshot variations, and the values in bold denote the best
code embeddings overall.

5.1 Performance of the Code2Snapshot Variations

In this
section, we compare the eﬀectiveness of diﬀerent variations of
Code2Snapshot representation on the MethodName task. According to
the results in Table 1, Reformatted has consistently performed much better
than Original, more than 20 percentage points. Training CNN models with
the images of original code perform poorly, perhaps because the original input
programs often suﬀer from inconsistent indentation, random comments, large
code, etc. After reformatting the input programs and training with the images
of reformatted code, each CNN model signiﬁcantly increases its performance for
predicting correct method names. The result is consistent in both JavaTop10
and JavaTop50 datasets.

On average, in both datasets, the performance of the ResNet and AlexNet
models is between 20 and 30 percentage points higher on Reformatted images
compared to Original images. Comparing among CNN models, we can see
that ResNet model achieves higher accuracy than AlexNet models in both
datasets. For MethodName task, the ResNet model on Reformatted images
achieve more than 80% in JavaTop10 dataset, and more than 60% accuracy in
JavaTop50 dataset.

5.2 Comparison with State-of-the-art Code Embeddings

In this section, we compare Code2Snapshot with the baseline token-based em-
beddings and the state-of-the-art path-based embeddings for the MethodName
task. Table 1 shows Reformatted and Redacted consistently outperform
BiLSTM by 15 percentage points. Comparing visual embeddings with path-
based embeddings, in JavaTop10, ResNet model on Reformatted performs
very close to the Code2Vec, and Code2Seq’s accuracy is only 3 percentage

Table 1: Results for diﬀerent models and code embeddings.

Dataset

Model

Input Types Precision Recall F1-Score Accuracy

Code2Snapshot

7

Original

55.25

52.49

53.43

AlexNet

Reformatted 82.84

81.18

81.26

Redacted

Original

80.26

78.58

78.68

65.03

61.76

61.83

JavaTop10

ResNet

Reformatted 85.10

83.80

83.78

Redacted

83.75

83.06

83.00

BiLSTM

Token

69.72

67.87

67.52

Code2Vec

Code2Seq

Path

85.80

84.96

84.85

86.88 86.65 86.50

86.65

Original

33.19

26.10

27.63

AlexNet

Reformatted 56.88

51.72

51.30

Redacted

Original

55.67

51.09

50.75

58.09

45.25

48.96

JavaTop50

ResNet

Reformatted 62.38

62.88

61.44

Redacted

62.56

59.60

58.52

BiLSTM

Token

48.72

44.67

41.82

Code2Vec

Code2Seq

Path

69.12

68.11

67.39

73.73 71.40 71.59

71.40

52.49

81.18

78.58

61.76

83.80

83.06

67.87

84.96

26.10

51.72

51.09

45.25

62.88

59.60

44.67

68.11

points higher. In the JavaTop50 dataset, the gap between Reformatted mod-
els and path-based models grows, as the accuracy of Code2Vec and Code2Seq
models are 5 and 8 percentage points higher than the best accuracy of Refor-
matted or Redacted.

5.3

Impact of Obscuring Images

To get an intuition of what actually the Code2Snapshot models learn, we use
Redacted representation where we replace any alphanumeric character with
“x” (Figure 2c). In our experiments, Reformatted and Redacted perform
almost the same. The results may suggest that Code2Snapshot neural models
actually learn from the structure of the program, and for this MethodName
task and most frequent dataset, neural models can predict the correct method
names merely by identifying the structure of the input programs.

8

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

6 Discussion and Future Work

Neural models are opaque. It is hard to pinpoint what patterns do they learn
from the input data and what is the best representation of the inputs to optimize
their performance. We believe that our results show promising directions to
better understanding the neural code intelligence models in general, and the
representation of the source code in particular.

The results of Redacted may suggest that the neural models merely rely
on the structure of the code. The encoding uses only black and white images for
representing the source code. In the future, we plan to investigate if we enrich
the feature vectors by colors, e.g. choosing diﬀerent colors for diﬀerent program
constructs, or choosing the same colors for the same variable names, we can gain
meaningful improvements. The main insight is that might help the neural models
to extract more complex patterns, in addition to the structure of the code.
the good performance of

the
Code2Snapshot in the MethodName task, might be that in the real setting,
the visual structure of code can guide developers to guess the labels. For
example, a one line method is probably a getter-setter method. Therefore, it is
unclear if Code2Snapshot can provide similarly promising results for tasks
that require reasoning about the input programs, e.g. variable misuse. We plan
to explore the eﬀectiveness of Code2Snapshot in such tasks.

Moreover, a potential explanation for

Additionally, we have only evaluated the Code2Snapshot representation for
code summarization task on the most frequent labels. Despite our best eﬀort,
it is possible that experiments with diﬀerent models, sizes of images, and font
settings may produce diﬀerent results. Our further plan also includes a detailed
study with a variety of code intelligence tasks, embeddings, and datasets.

Lastly, the computer vision research community has made great advances in
the interpretability of neural models [24, 26]. Representing source code as an
image and using convolutional neural architectures would enable using the most
recent techniques devised by that community and translating the results to the
ﬁeld of code intelligence.

7 Conclusion

In this paper, we introduced Code2Snapshot that encoded the source code
as a simple black and white image and provided a preliminary evaluation of its
eﬀectiveness in the MethodName task. We observed that this simple visual en-
coding is surprisingly eﬀective in our study, even when replacing all alphanumeric
characters of programs with a random letter ‘x’. The results warrant further in-
vestigation on the applicability of Code2Snapshot for other tasks and datasets.
These initial results can enable further research to answer fundamental questions
about the neural code intelligence models, e.g., what do they actually learn? and
how they can be improved? We also plan to explore in such directions.

Bibliography

[1] Alex Clark and Contributors: Pillow (Fork of the Python Imaging Library).
https://github.com/python-pillow/Pillow/ (2010), [Online; accessed
31-July-2021]

[2] Allamanis, M.: The adverse eﬀects of code duplication in machine learning
models of code. In: Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reﬂections on Program-
ming and Software. pp. 143–153 (2019)

[3] Allamanis, M., Barr, E.T., Bird, C., Sutton, C.: Learning natural coding
conventions.
In: Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering. p. 281–293. FSE 2014,
Association for Computing Machinery, New York, NY, USA (2014), https:
//doi.org/10.1145/2635868.2635883

[4] Allamanis, M., Barr, E.T., Bird, C., Sutton, C.: Suggesting accurate method
In: Proceedings of the 2015 10th Joint Meeting on
and class names.
Foundations of Software Engineering. p. 38–49. ESEC/FSE 2015, Asso-
ciation for Computing Machinery, New York, NY, USA (2015), https:
//doi.org/10.1145/2786805.2786849

[5] Allamanis, M., Barr, E.T., Devanbu, P., Sutton, C.: A survey of machine
learning for big code and naturalness. ACM Computing Surveys 51(4)
(2018), https://doi.org/10.1145/3212695

[6] Allamanis, M., Brockschmidt, M., Khademi, M.: Learning to represent pro-
grams with graphs. In: International Conference on Learning Representa-
tions (2018), https://openreview.net/forum?id=BJOFETxR-

[7] Allamanis, M., Peng, H., Sutton, C.A.: A convolutional attention network
In: Proceedings of the 33nd

for extreme summarization of source code.
International Conference on Machine Learning, ICML (2016)

[8] Alon, U., Levy, O., Yahav, E.: code2seq: Generating sequences from struc-
In: International Conference on Learning

tured representations of code.
Representations (2019)

[9] Alon, U., Zilberstein, M., Levy, O., Yahav, E.: Code2vec: Learning dis-
tributed representations of code. Proc. ACM Program. Lang. 3(POPL),
40:1–40:29 (Jan 2019), http://doi.acm.org/10.1145/3290353

[10] Bilgin, Z.: Code2image: Intelligent code analysis by computer vision
arXiv preprint

techniques and application to vulnerability prediction.
arXiv:2105.03131 (2021)

[11] Chen, Z., Monperrus, M.: A literature study of embeddings on source code.

arXiv preprint arXiv:1904.03061 (2019)

[12] Compton, R., Frank, E., Patros, P., Koay, A.: Embedding java classes
with code2vec: Improvements from variable obfuscation.
In: Proceedings
of the 17th International Conference on Mining Software Repositories. p.
243–253. MSR ’20, Association for Computing Machinery, New York, NY,
USA (2020), https://doi.org/10.1145/3379597.3387445

10

Md Raﬁqul Islam Rabin, Mohammad Amin Alipour

[13] Danny van Bruggen and Contributors: JavaParser (Tools for Java code).
https://github.com/javaparser/javaparser/ (2011), [Online; accessed
31-July-2021]

[14] DeFreez, D., Thakur, A.V., Rubio-González, C.: Path-based function em-
beddings. In: Proceedings of the 40th International Conference on Software
Engineering: Companion Proceeedings. pp. 430–431. ICSE ’18, ACM, New
York, NY, USA (2018), http://doi.acm.org/10.1145/3183440.3195042
[15] Dey, S., Singh, A.K., Prasad, D.K., Mcdonald-Maier, K.D.: Socodecnn: Pro-
gram source code for visual cnn classiﬁcation using computer vision method-
ology. IEEE Access 7, 157158–157172 (2019)

[16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recog-
In: Proceedings of the IEEE conference on computer vision and

nition.
pattern recognition. pp. 770–778 (2016)

[17] Hellendoorn, V.J., Bird, C., Barr, E.T., Allamanis, M.: Deep learning
type inference.
In: Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. p. 152–162. ESEC/FSE 2018, As-
sociation for Computing Machinery, New York, NY, USA (2018), https:
//doi.org/10.1145/3236024.3236051

[18] Keller, P., Plein, L., Bissyandé, T.F., Klein, J., Traon, Y.L.: What you
see is what it means! semantic representation learning of code based on
visualization and transfer learning. arXiv preprint arXiv:2002.02650 (2020)
[19] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with
deep convolutional neural networks. vol. 60, p. 84–90. Association for Com-
puting Machinery, New York, NY, USA (May 2017), https://doi.org/
10.1145/3065386

[20] Rabin, M.R.I., Bui, N.D., Wang, K., Yu, Y., Jiang, L., Alipour, M.A.: On
the generalizability of neural program models with respect to semantic-
preserving program transformations. Information and Software Technology
p. 106552 (2021), https://www.sciencedirect.com/science/article/
pii/S0950584921000379

[21] Rabin, M.R.I., Hellendoorn, V.J., Alipour, M.A.: Understanding neural
code intelligence through program simpliﬁcation.
In: Proceedings of the
29th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. p. 441–452.
ESEC/FSE 2021, Association for Computing Machinery, New York, NY,
USA (2021), https://doi.org/10.1145/3468264.3468539

[22] Rabin, M.R.I., Hussain, A., Hellendoorn, V.J., Alipour, M.A.: Memoriza-
tion and generalization in neural code intelligence models (2021), https:
//arxiv.org/abs/2106.08704

[23] Rabin, M.R.I., Mukherjee, A., Gnawali, O., Alipour, M.A.: Towards de-
mystifying dimensions of source code embeddings. In: Proceedings of the
1st ACM SIGSOFT International Workshop on Representation Learning
for Software Engineering and Program Languages. p. 29–38. RL+SE&PL
2020, Association for Computing Machinery, New York, NY, USA (2020),
https://dl.acm.org/doi/10.1145/3416506.3423580

Code2Snapshot

11

[24] Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra,
D.: Grad-cam: Visual explanations from deep networks via gradient-based
localization. In: Proceedings of the IEEE international conference on com-
puter vision. pp. 618–626 (2017)

[25] Sharma, T., Kechagia, M., Georgiou, S., Tiwari, R., Sarro, F.: A survey
on machine learning techniques for source code analysis. arXiv preprint
arXiv:2110.09610 (2021)

[26] Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional net-
works. In: European conference on computer vision. pp. 818–833. Springer
(2014)

[27] Zhang, J.M., Harman, M., Ma, L., Liu, Y.: Machine learning testing: Survey,
landscapes and horizons. IEEE Transactions on Software Engineering pp.
1–1 (2020)

